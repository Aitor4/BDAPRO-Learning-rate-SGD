objc[3237]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1012ab4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10132f4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 09:47:54 INFO SparkContext: Running Spark version 2.0.0
18/02/27 09:47:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 09:47:55 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 09:47:55 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 09:47:55 INFO SecurityManager: Changing view acls groups to: 
18/02/27 09:47:55 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 09:47:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 09:47:56 INFO Utils: Successfully started service 'sparkDriver' on port 50652.
18/02/27 09:47:56 INFO SparkEnv: Registering MapOutputTracker
18/02/27 09:47:56 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 09:47:56 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-b1dcc0c9-0d29-4f1d-8cd2-bcbba61a8957
18/02/27 09:47:56 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 09:47:56 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 09:47:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 09:47:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 09:47:57 INFO Executor: Starting executor ID driver on host localhost
18/02/27 09:47:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50653.
18/02/27 09:47:57 INFO NettyBlockTransferService: Server created on 192.168.2.140:50653
18/02/27 09:47:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50653)
18/02/27 09:47:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50653 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50653)
18/02/27 09:47:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50653)
Loss in iteration 1 : 1.1488599803296142
Loss in iteration 2 : 31.25337483038936
Loss in iteration 3 : 18.899997665209618
Loss in iteration 4 : 6.683558234037278
Loss in iteration 5 : 2.657342003228115
Loss in iteration 6 : 2.3899751638579434
Loss in iteration 7 : 2.2088188963438258
Loss in iteration 8 : 2.0862561756089657
Loss in iteration 9 : 1.984943739966266
Loss in iteration 10 : 1.8886148409676484
Loss in iteration 11 : 1.7949862727626031
Loss in iteration 12 : 1.7021700190186548
Loss in iteration 13 : 1.6108028216421075
Loss in iteration 14 : 1.5210614528053166
Loss in iteration 15 : 1.4326046073324399
Loss in iteration 16 : 1.345375902786447
Loss in iteration 17 : 1.260226390826808
Loss in iteration 18 : 1.1793587697437218
Loss in iteration 19 : 1.1047697704174941
Loss in iteration 20 : 1.0700727099327703
Loss in iteration 21 : 1.4212381070278115
Loss in iteration 22 : 5.151245629530582
Loss in iteration 23 : 8.01916785567348
Loss in iteration 24 : 19.9020132211323
Loss in iteration 25 : 7.600192181587443
Loss in iteration 26 : 2.4016762303828108
Loss in iteration 27 : 2.34710994485025
Loss in iteration 28 : 2.026308187367129
Loss in iteration 29 : 2.078624464481202
Loss in iteration 30 : 1.9298556443129091
Loss in iteration 31 : 2.1417127658094985
Loss in iteration 32 : 2.1811923877836503
Loss in iteration 33 : 3.141638175808687
Loss in iteration 34 : 3.704738488998391
Loss in iteration 35 : 6.80023198251134
Loss in iteration 36 : 2.910948571149494
Loss in iteration 37 : 3.738672877898308
Loss in iteration 38 : 3.107395971571483
Loss in iteration 39 : 4.154597178905612
Loss in iteration 40 : 3.1468946347837377
Loss in iteration 41 : 3.9164652842873497
Loss in iteration 42 : 2.9169151673825335
Loss in iteration 43 : 3.346882589424721
Loss in iteration 44 : 2.4625640956010346
Loss in iteration 45 : 2.7833677475460012
Loss in iteration 46 : 2.267783465592726
Loss in iteration 47 : 2.695605342246314
Loss in iteration 48 : 2.3978802484545527
Loss in iteration 49 : 3.0958330249031523
Loss in iteration 50 : 2.9197862203910447
Loss in iteration 51 : 4.440357849323187
Loss in iteration 52 : 3.80926834839005
Loss in iteration 53 : 5.579811040160636
Loss in iteration 54 : 3.180598930141765
Loss in iteration 55 : 3.680981736431287
Loss in iteration 56 : 2.5830148898041094
Loss in iteration 57 : 2.781939922873983
Loss in iteration 58 : 2.1918401982007456
Loss in iteration 59 : 2.399693574302777
Loss in iteration 60 : 2.179991573510308
Loss in iteration 61 : 2.6485901346883796
Loss in iteration 62 : 2.450316237062589
Loss in iteration 63 : 3.5089885168508568
Loss in iteration 64 : 3.4627605804058854
Loss in iteration 65 : 5.640002305824014
Loss in iteration 66 : 3.3473979792632287
Loss in iteration 67 : 4.334860074088013
Loss in iteration 68 : 3.020096176107199
Loss in iteration 69 : 3.550109581354135
Loss in iteration 70 : 2.5149185351810663
Loss in iteration 71 : 2.792040825562124
Loss in iteration 72 : 2.240993881123238
Loss in iteration 73 : 2.4822033585349996
Loss in iteration 74 : 2.2469585224144346
Loss in iteration 75 : 2.8428616875167725
Loss in iteration 76 : 2.562061711369015
Loss in iteration 77 : 3.6959930558501877
Loss in iteration 78 : 3.460804347887667
Loss in iteration 79 : 5.4420458504057745
Loss in iteration 80 : 3.4007102548203667
Loss in iteration 81 : 4.278805752897185
Loss in iteration 82 : 2.8740371572110393
Loss in iteration 83 : 3.221521811158303
Loss in iteration 84 : 2.3455960857046905
Loss in iteration 85 : 2.569013652839543
Loss in iteration 86 : 2.2152239869743293
Loss in iteration 87 : 2.5218138026605548
Loss in iteration 88 : 2.3023775759725336
Loss in iteration 89 : 2.9565863184662278
Loss in iteration 90 : 2.726876857826944
Loss in iteration 91 : 4.254572092521499
Loss in iteration 92 : 3.738102533794053
Loss in iteration 93 : 5.73549790315185
Loss in iteration 94 : 3.153707411959437
Loss in iteration 95 : 3.791638731083391
Loss in iteration 96 : 2.626115645659609
Loss in iteration 97 : 2.845853261126736
Loss in iteration 98 : 2.2366559590349135
Loss in iteration 99 : 2.4210921696519376
Loss in iteration 100 : 2.1893466269281046
Loss in iteration 101 : 2.613516022693615
Loss in iteration 102 : 2.4218723700078706
Loss in iteration 103 : 3.4249257055582527
Loss in iteration 104 : 3.309628556594327
Loss in iteration 105 : 5.321955413313658
Loss in iteration 106 : 3.530241135974674
Loss in iteration 107 : 4.619678105961231
Loss in iteration 108 : 3.0914488689519604
Loss in iteration 109 : 3.6199079759090256
Loss in iteration 110 : 2.5443722593671554
Loss in iteration 111 : 2.738413935246912
Loss in iteration 112 : 2.201773728436576
Loss in iteration 113 : 2.4051296197363006
Loss in iteration 114 : 2.177182047763145
Loss in iteration 115 : 2.5983216890897527
Loss in iteration 116 : 2.4104512148470443
Loss in iteration 117 : 3.442254633484907
Loss in iteration 118 : 3.336584186804973
Loss in iteration 119 : 5.388534509239146
Loss in iteration 120 : 3.4770577176316713
Loss in iteration 121 : 4.586251657985008
Loss in iteration 122 : 3.0883657888650538
Loss in iteration 123 : 3.6122168459472648
Loss in iteration 124 : 2.5489731420146926
Loss in iteration 125 : 2.742369610188257
Loss in iteration 126 : 2.1961213872668632
Loss in iteration 127 : 2.408478904134768
Loss in iteration 128 : 2.171408989235406
Loss in iteration 129 : 2.60292930892092
Loss in iteration 130 : 2.4277478934690913
Loss in iteration 131 : 3.5186381015734116
Loss in iteration 132 : 3.368373226003645
Loss in iteration 133 : 5.401573339372413
Loss in iteration 134 : 3.4559197329165205
Loss in iteration 135 : 4.486324872304468
Loss in iteration 136 : 3.072782711841712
Loss in iteration 137 : 3.608663208050274
Loss in iteration 138 : 2.5428760110513275
Loss in iteration 139 : 2.740153350327144
Loss in iteration 140 : 2.1971555187273264
Loss in iteration 141 : 2.4063755091220136
Loss in iteration 142 : 2.1678754593374863
Loss in iteration 143 : 2.578551604606403
Loss in iteration 144 : 2.4204424866456637
Loss in iteration 145 : 3.5093035373382087
Loss in iteration 146 : 3.3752088887272538
Loss in iteration 147 : 5.376623578843593
Loss in iteration 148 : 3.4720845170345136
Loss in iteration 149 : 4.517597543152794
Loss in iteration 150 : 3.0619999046738804
Loss in iteration 151 : 3.6065563799362614
Loss in iteration 152 : 2.527670341643894
Loss in iteration 153 : 2.7426098232657714
Loss in iteration 154 : 2.206458372218614
Loss in iteration 155 : 2.4063813827991023
Loss in iteration 156 : 2.16955237940303
Loss in iteration 157 : 2.605571889002082
Loss in iteration 158 : 2.4240222483518985
Loss in iteration 159 : 3.5018859025241262
Loss in iteration 160 : 3.3592888392965534
Loss in iteration 161 : 5.376116402451455
Loss in iteration 162 : 3.4724929324339078
Loss in iteration 163 : 4.534751664129299
Loss in iteration 164 : 3.0463499802773772
Loss in iteration 165 : 3.586978730220015
Loss in iteration 166 : 2.458381459254705
Loss in iteration 167 : 2.6507375088924108
Loss in iteration 168 : 2.1978993010714674
Loss in iteration 169 : 2.4097645420214864
Loss in iteration 170 : 2.1915315058843317
Loss in iteration 171 : 2.6965222171790253
Loss in iteration 172 : 2.4366198996805606
Loss in iteration 173 : 3.5443095471034987
Loss in iteration 174 : 3.4343968747773213
Loss in iteration 175 : 5.482203689672777
Loss in iteration 176 : 3.432989792464236
Loss in iteration 177 : 4.438249592098959
Loss in iteration 178 : 2.9973473530422603
Loss in iteration 179 : 3.5168510376687894
Loss in iteration 180 : 2.4499817987077197
Loss in iteration 181 : 2.654903822119728
Loss in iteration 182 : 2.202121626289438
Loss in iteration 183 : 2.41583569314372
Loss in iteration 184 : 2.2064636431003923
Loss in iteration 185 : 2.7284262816203757
Loss in iteration 186 : 2.425979691835958
Loss in iteration 187 : 3.530920537245576
Loss in iteration 188 : 3.4003598471516656
Loss in iteration 189 : 5.4835635783169465
Loss in iteration 190 : 3.4359384217694813
Loss in iteration 191 : 4.455615485938305
Loss in iteration 192 : 3.034997325653159
Loss in iteration 193 : 3.5841722334627018
Loss in iteration 194 : 2.4606874767662945
Loss in iteration 195 : 2.675389081185681
Loss in iteration 196 : 2.1992802891525844
Loss in iteration 197 : 2.410171523005986
Loss in iteration 198 : 2.1871025082756117
Loss in iteration 199 : 2.694853023477211
Loss in iteration 200 : 2.439426198311958
Testing accuracy  of updater 0 on alg 1 with rate 0.011199999999999998 = 0.621, training accuracy 0.7241825833603108, time elapsed: 9855 millisecond.
Loss in iteration 1 : 1.072941390361511
Loss in iteration 2 : 21.963655995221295
Loss in iteration 3 : 13.336687971182007
Loss in iteration 4 : 4.78700199242291
Loss in iteration 5 : 1.9123245223486978
Loss in iteration 6 : 1.730557255254946
Loss in iteration 7 : 1.6003155697164446
Loss in iteration 8 : 1.5151034628320672
Loss in iteration 9 : 1.4454980415129637
Loss in iteration 10 : 1.3797732165125465
Loss in iteration 11 : 1.3162913695322922
Loss in iteration 12 : 1.2534575641589507
Loss in iteration 13 : 1.1912546947415243
Loss in iteration 14 : 1.1299270156827081
Loss in iteration 15 : 1.0700385598202848
Loss in iteration 16 : 1.0125903317838654
Loss in iteration 17 : 0.9573777574171751
Loss in iteration 18 : 0.9119464913860883
Loss in iteration 19 : 0.9189269396871432
Loss in iteration 20 : 1.3616136510534131
Loss in iteration 21 : 3.9999457055676717
Loss in iteration 22 : 11.224640350016264
Loss in iteration 23 : 2.7352512291648035
Loss in iteration 24 : 2.9517320621907017
Loss in iteration 25 : 4.787355544842321
Loss in iteration 26 : 2.0840464361255626
Loss in iteration 27 : 2.4126014736661228
Loss in iteration 28 : 1.9358167438394873
Loss in iteration 29 : 2.200518565942233
Loss in iteration 30 : 1.8214256263844972
Loss in iteration 31 : 2.1817692124116586
Loss in iteration 32 : 1.8843144962897662
Loss in iteration 33 : 2.4152623686316095
Loss in iteration 34 : 2.1424224415155924
Loss in iteration 35 : 3.0347142420758497
Loss in iteration 36 : 2.559755633090515
Loss in iteration 37 : 3.447575025985004
Loss in iteration 38 : 2.365173891656736
Loss in iteration 39 : 2.785397290325975
Loss in iteration 40 : 1.9755079576293153
Loss in iteration 41 : 2.1087278453940614
Loss in iteration 42 : 1.6419330197699276
Loss in iteration 43 : 1.7541612092153376
Loss in iteration 44 : 1.5762682293498995
Loss in iteration 45 : 1.8233958245055837
Loss in iteration 46 : 1.7151537579425085
Loss in iteration 47 : 2.269272877874745
Loss in iteration 48 : 2.3032170814155095
Loss in iteration 49 : 3.6860401133730374
Loss in iteration 50 : 2.6605768548492033
Loss in iteration 51 : 3.7000440951215072
Loss in iteration 52 : 2.2859761005009958
Loss in iteration 53 : 2.620719157016954
Loss in iteration 54 : 1.848546916466139
Loss in iteration 55 : 1.992803725700193
Loss in iteration 56 : 1.6045432635299621
Loss in iteration 57 : 1.7311346424725076
Loss in iteration 58 : 1.5924164395058258
Loss in iteration 59 : 1.8980956689956063
Loss in iteration 60 : 1.7492026462257548
Loss in iteration 61 : 2.37489867974082
Loss in iteration 62 : 2.3358362132079877
Loss in iteration 63 : 3.625657621274784
Loss in iteration 64 : 2.6188110504699313
Loss in iteration 65 : 3.5413218027945192
Loss in iteration 66 : 2.264838700176242
Loss in iteration 67 : 2.6152793156500547
Loss in iteration 68 : 1.8799023829015686
Loss in iteration 69 : 2.0113790903450455
Loss in iteration 70 : 1.6255399651438953
Loss in iteration 71 : 1.7485618592842338
Loss in iteration 72 : 1.5938213050392256
Loss in iteration 73 : 1.8621218627439113
Loss in iteration 74 : 1.7444592997931638
Loss in iteration 75 : 2.417893115056057
Loss in iteration 76 : 2.3481630976885084
Loss in iteration 77 : 3.6471668778980666
Loss in iteration 78 : 2.579309149179861
Loss in iteration 79 : 3.431822941257435
Loss in iteration 80 : 2.219205202942177
Loss in iteration 81 : 2.5459671223551226
Loss in iteration 82 : 1.7763478172002745
Loss in iteration 83 : 1.9079284939232277
Loss in iteration 84 : 1.61773871329006
Loss in iteration 85 : 1.7511705912527422
Loss in iteration 86 : 1.6145558715318906
Loss in iteration 87 : 1.9486331052824748
Loss in iteration 88 : 1.7745423383807684
Loss in iteration 89 : 2.487352869585976
Loss in iteration 90 : 2.422160352569551
Loss in iteration 91 : 3.810675247657096
Loss in iteration 92 : 2.4951140116112063
Loss in iteration 93 : 3.220876091719651
Loss in iteration 94 : 2.2119527102804346
Loss in iteration 95 : 2.5493561494861323
Loss in iteration 96 : 1.7701147147388239
Loss in iteration 97 : 1.913712198343481
Loss in iteration 98 : 1.6260789145687173
Loss in iteration 99 : 1.7761281336483214
Loss in iteration 100 : 1.6384975976766918
Loss in iteration 101 : 1.9809353144163842
Loss in iteration 102 : 1.7785307753369972
Loss in iteration 103 : 2.4958043890371857
Loss in iteration 104 : 2.3987239345291274
Loss in iteration 105 : 3.7175923400576885
Loss in iteration 106 : 2.5387021688240865
Loss in iteration 107 : 3.2849079652015045
Loss in iteration 108 : 2.192235760737576
Loss in iteration 109 : 2.527336599804388
Loss in iteration 110 : 1.7651727077182036
Loss in iteration 111 : 1.8803402968365912
Loss in iteration 112 : 1.6170014109476019
Loss in iteration 113 : 1.7618839142510518
Loss in iteration 114 : 1.6292426819093555
Loss in iteration 115 : 1.9716489461393405
Loss in iteration 116 : 1.7865040439484134
Loss in iteration 117 : 2.5246376587557817
Loss in iteration 118 : 2.4021868954781005
Loss in iteration 119 : 3.6985162826255182
Loss in iteration 120 : 2.479117684561471
Loss in iteration 121 : 3.190704845457381
Loss in iteration 122 : 2.193464047788465
Loss in iteration 123 : 2.5596365207733047
Loss in iteration 124 : 1.7744657364088519
Loss in iteration 125 : 1.8938119613118054
Loss in iteration 126 : 1.6185037556552924
Loss in iteration 127 : 1.783007658246966
Loss in iteration 128 : 1.640730526532325
Loss in iteration 129 : 1.990774126830328
Loss in iteration 130 : 1.7765908107857495
Loss in iteration 131 : 2.4940672838341698
Loss in iteration 132 : 2.391747492666588
Loss in iteration 133 : 3.6865997111812105
Loss in iteration 134 : 2.49187674447846
Loss in iteration 135 : 3.174609769985794
Loss in iteration 136 : 2.1765871731781057
Loss in iteration 137 : 2.5390990473640396
Loss in iteration 138 : 1.7821551540826832
Loss in iteration 139 : 1.917842369037971
Loss in iteration 140 : 1.6340019682377833
Loss in iteration 141 : 1.8101058712395528
Loss in iteration 142 : 1.666857890922158
Loss in iteration 143 : 2.0020010154805594
Loss in iteration 144 : 1.7711264656427312
Loss in iteration 145 : 2.4697313036444832
Loss in iteration 146 : 2.369566556598727
Loss in iteration 147 : 3.6617256926559922
Loss in iteration 148 : 2.466321426535179
Loss in iteration 149 : 3.1647264512625597
Loss in iteration 150 : 2.167858562801517
Loss in iteration 151 : 2.5379361414160684
Loss in iteration 152 : 1.7863685266446128
Loss in iteration 153 : 1.9328603792797288
Loss in iteration 154 : 1.6340102269533594
Loss in iteration 155 : 1.821191663136924
Loss in iteration 156 : 1.6588144737294568
Loss in iteration 157 : 1.9989567244276896
Loss in iteration 158 : 1.7739896661287886
Loss in iteration 159 : 2.463938071410051
Loss in iteration 160 : 2.3781876194379956
Loss in iteration 161 : 3.6791776154160916
Loss in iteration 162 : 2.4991157440062084
Loss in iteration 163 : 3.2222827243385592
Loss in iteration 164 : 2.208665122437367
Loss in iteration 165 : 2.572811583592119
Loss in iteration 166 : 1.813422818834775
Loss in iteration 167 : 1.944078452983562
Loss in iteration 168 : 1.625122629118432
Loss in iteration 169 : 1.7839241658716851
Loss in iteration 170 : 1.624215092082503
Loss in iteration 171 : 1.9371187954608575
Loss in iteration 172 : 1.7625769010916648
Loss in iteration 173 : 2.4437735110458854
Loss in iteration 174 : 2.3615047162922367
Loss in iteration 175 : 3.6501870107202503
Loss in iteration 176 : 2.4778728638347203
Loss in iteration 177 : 3.2021755126814715
Loss in iteration 178 : 2.2201877110893076
Loss in iteration 179 : 2.5775317816904377
Loss in iteration 180 : 1.825943364678202
Loss in iteration 181 : 1.968353958321948
Loss in iteration 182 : 1.6255199748640892
Loss in iteration 183 : 1.77711383578538
Loss in iteration 184 : 1.626492343654018
Loss in iteration 185 : 1.9344381530243382
Loss in iteration 186 : 1.7599869675376532
Loss in iteration 187 : 2.4467829072152525
Loss in iteration 188 : 2.358242784798655
Loss in iteration 189 : 3.642206614531944
Loss in iteration 190 : 2.4856622794796945
Loss in iteration 191 : 3.205735594147023
Loss in iteration 192 : 2.216911347045832
Loss in iteration 193 : 2.570980157777832
Loss in iteration 194 : 1.8135798844547562
Loss in iteration 195 : 1.9598307552372924
Loss in iteration 196 : 1.6207962730562426
Loss in iteration 197 : 1.7801589311334274
Loss in iteration 198 : 1.62646118814285
Loss in iteration 199 : 1.9482858741628257
Loss in iteration 200 : 1.77707372474649
Testing accuracy  of updater 0 on alg 1 with rate 0.00784 = 0.63825, training accuracy 0.7368080284881838, time elapsed: 4710 millisecond.
Loss in iteration 1 : 1.0238175968527383
Loss in iteration 2 : 12.690587902576139
Loss in iteration 3 : 7.772534752970244
Loss in iteration 4 : 2.8838645841350914
Loss in iteration 5 : 1.1998939863224267
Loss in iteration 6 : 1.0855352223404768
Loss in iteration 7 : 1.0023564346650344
Loss in iteration 8 : 0.9540229531947242
Loss in iteration 9 : 0.9161587743504485
Loss in iteration 10 : 0.8802995325349161
Loss in iteration 11 : 0.8456980269970725
Loss in iteration 12 : 0.8127304762567205
Loss in iteration 13 : 0.7795269424873568
Loss in iteration 14 : 0.748652062063315
Loss in iteration 15 : 0.7183085467042023
Loss in iteration 16 : 0.6984207251243575
Loss in iteration 17 : 0.7068680431180646
Loss in iteration 18 : 0.8795676534588344
Loss in iteration 19 : 1.6387771425146844
Loss in iteration 20 : 4.066474770739116
Loss in iteration 21 : 0.7562318235885743
Loss in iteration 22 : 0.8366262848606563
Loss in iteration 23 : 1.078053820895926
Loss in iteration 24 : 2.3245933821895592
Loss in iteration 25 : 2.124713352711724
Loss in iteration 26 : 3.764409300064777
Loss in iteration 27 : 0.8787117920541456
Loss in iteration 28 : 0.9005038001561345
Loss in iteration 29 : 0.8963434854553597
Loss in iteration 30 : 1.0466232347973372
Loss in iteration 31 : 1.1651041953941872
Loss in iteration 32 : 1.9332120274480473
Loss in iteration 33 : 1.9973729653201782
Loss in iteration 34 : 3.0747905027961364
Loss in iteration 35 : 1.1551607587549075
Loss in iteration 36 : 1.264541579233948
Loss in iteration 37 : 1.1029148203911994
Loss in iteration 38 : 1.2553133739421234
Loss in iteration 39 : 1.1360973492192552
Loss in iteration 40 : 1.367294495116829
Loss in iteration 41 : 1.2860540733898658
Loss in iteration 42 : 1.795531108065794
Loss in iteration 43 : 1.5752930137222128
Loss in iteration 44 : 2.084536333447847
Loss in iteration 45 : 1.460699592674599
Loss in iteration 46 : 1.7297542385001006
Loss in iteration 47 : 1.2829512156097453
Loss in iteration 48 : 1.3792349682311373
Loss in iteration 49 : 1.0836304740830582
Loss in iteration 50 : 1.1572588415375185
Loss in iteration 51 : 1.0359840559255942
Loss in iteration 52 : 1.1749719121130444
Loss in iteration 53 : 1.109564741597544
Loss in iteration 54 : 1.3683127206645167
Loss in iteration 55 : 1.2974025071160304
Loss in iteration 56 : 1.8285883638643006
Loss in iteration 57 : 1.5819800795827206
Loss in iteration 58 : 2.1275421816092726
Loss in iteration 59 : 1.4515286514598111
Loss in iteration 60 : 1.6836901032973495
Loss in iteration 61 : 1.2336768945387462
Loss in iteration 62 : 1.3018614284351582
Loss in iteration 63 : 1.0509927726607649
Loss in iteration 64 : 1.1220178212351788
Loss in iteration 65 : 1.0262392524446036
Loss in iteration 66 : 1.162016186079606
Loss in iteration 67 : 1.1137595076072797
Loss in iteration 68 : 1.4501712858810059
Loss in iteration 69 : 1.3966342618807144
Loss in iteration 70 : 2.0202885335305107
Loss in iteration 71 : 1.561329538002894
Loss in iteration 72 : 2.0362658902746644
Loss in iteration 73 : 1.3800601284817993
Loss in iteration 74 : 1.6017335476504193
Loss in iteration 75 : 1.1929682656104899
Loss in iteration 76 : 1.267900612876744
Loss in iteration 77 : 1.0588996485401871
Loss in iteration 78 : 1.1381833817079754
Loss in iteration 79 : 1.0502998002863335
Loss in iteration 80 : 1.2161227321183472
Loss in iteration 81 : 1.1166144232896469
Loss in iteration 82 : 1.4525976501592397
Loss in iteration 83 : 1.37238547120797
Loss in iteration 84 : 1.9007492109936164
Loss in iteration 85 : 1.5279756157746271
Loss in iteration 86 : 2.019647616485463
Loss in iteration 87 : 1.3833883748898956
Loss in iteration 88 : 1.6407266938825154
Loss in iteration 89 : 1.212387814125716
Loss in iteration 90 : 1.3130783353376498
Loss in iteration 91 : 1.0526515298426846
Loss in iteration 92 : 1.1304211459098241
Loss in iteration 93 : 1.048714855155438
Loss in iteration 94 : 1.1919867790852698
Loss in iteration 95 : 1.1166983151240313
Loss in iteration 96 : 1.465615810128372
Loss in iteration 97 : 1.3901059248010976
Loss in iteration 98 : 1.9481472177739725
Loss in iteration 99 : 1.5114966836451735
Loss in iteration 100 : 1.9722194258942796
Loss in iteration 101 : 1.388730194393301
Loss in iteration 102 : 1.6357259704324476
Loss in iteration 103 : 1.2032529888956665
Loss in iteration 104 : 1.3051293662170578
Loss in iteration 105 : 1.054244891944991
Loss in iteration 106 : 1.1296719385127392
Loss in iteration 107 : 1.0444990467211472
Loss in iteration 108 : 1.1744978117051534
Loss in iteration 109 : 1.0967220734690502
Loss in iteration 110 : 1.4229252981770497
Loss in iteration 111 : 1.3456219960957398
Loss in iteration 112 : 1.892725712360027
Loss in iteration 113 : 1.5330344985782938
Loss in iteration 114 : 2.0291756862967407
Loss in iteration 115 : 1.389791778388672
Loss in iteration 116 : 1.6299873707525943
Loss in iteration 117 : 1.2040471495410188
Loss in iteration 118 : 1.3190342892077491
Loss in iteration 119 : 1.0534839269258998
Loss in iteration 120 : 1.1289584227326317
Loss in iteration 121 : 1.0472423705813072
Loss in iteration 122 : 1.1890334753225034
Loss in iteration 123 : 1.1153561484105525
Loss in iteration 124 : 1.4648213710576174
Loss in iteration 125 : 1.3793749749547104
Loss in iteration 126 : 1.9263630052211367
Loss in iteration 127 : 1.5175163819844817
Loss in iteration 128 : 1.9774482984879076
Loss in iteration 129 : 1.3962655099688972
Loss in iteration 130 : 1.6201906610783556
Loss in iteration 131 : 1.1918733611768506
Loss in iteration 132 : 1.303206053669338
Loss in iteration 133 : 1.0535495720721242
Loss in iteration 134 : 1.1267713585321448
Loss in iteration 135 : 1.0312179161155102
Loss in iteration 136 : 1.1703055254741848
Loss in iteration 137 : 1.1015167106207813
Loss in iteration 138 : 1.428275964673325
Loss in iteration 139 : 1.3502533026587777
Loss in iteration 140 : 1.8990669523565427
Loss in iteration 141 : 1.5360286308833118
Loss in iteration 142 : 2.0135650479932066
Loss in iteration 143 : 1.3878091496045788
Loss in iteration 144 : 1.6195067553023643
Loss in iteration 145 : 1.1991039886700026
Loss in iteration 146 : 1.3186881929667602
Loss in iteration 147 : 1.0540520359377383
Loss in iteration 148 : 1.1250729104380475
Loss in iteration 149 : 1.0304498148451176
Loss in iteration 150 : 1.1708208753593974
Loss in iteration 151 : 1.1003423219860813
Loss in iteration 152 : 1.4353063538081954
Loss in iteration 153 : 1.3458742949407976
Loss in iteration 154 : 1.9044709485356217
Loss in iteration 155 : 1.546210580371112
Loss in iteration 156 : 2.0416454240742157
Loss in iteration 157 : 1.3811852363342416
Loss in iteration 158 : 1.6251828829046848
Loss in iteration 159 : 1.1943930679297712
Loss in iteration 160 : 1.3139650849287197
Loss in iteration 161 : 1.05362880426005
Loss in iteration 162 : 1.1266229091593667
Loss in iteration 163 : 1.0299526270779984
Loss in iteration 164 : 1.164725343025779
Loss in iteration 165 : 1.1011104560721792
Loss in iteration 166 : 1.4345725743825717
Loss in iteration 167 : 1.3464091953718693
Loss in iteration 168 : 1.8965456625262715
Loss in iteration 169 : 1.5339173909175543
Loss in iteration 170 : 2.009493938384846
Loss in iteration 171 : 1.382697320626861
Loss in iteration 172 : 1.6242495851849632
Loss in iteration 173 : 1.1987116761735979
Loss in iteration 174 : 1.3187528319632849
Loss in iteration 175 : 1.0536734564271235
Loss in iteration 176 : 1.1288441525748982
Loss in iteration 177 : 1.0386443059180948
Loss in iteration 178 : 1.1711163933574325
Loss in iteration 179 : 1.1029494758104827
Loss in iteration 180 : 1.4440303833484613
Loss in iteration 181 : 1.340099392489763
Loss in iteration 182 : 1.8915816137171833
Loss in iteration 183 : 1.533769817704399
Loss in iteration 184 : 2.0188037300018538
Loss in iteration 185 : 1.388039410144774
Loss in iteration 186 : 1.6192026580889514
Loss in iteration 187 : 1.1946711992300947
Loss in iteration 188 : 1.3204297262134155
Loss in iteration 189 : 1.0583769869004935
Loss in iteration 190 : 1.1473477702939883
Loss in iteration 191 : 1.052616039938569
Loss in iteration 192 : 1.1886416492390954
Loss in iteration 193 : 1.1027925029136667
Loss in iteration 194 : 1.4285444697047875
Loss in iteration 195 : 1.3396030918092652
Loss in iteration 196 : 1.8739575288329877
Loss in iteration 197 : 1.53366875792572
Loss in iteration 198 : 2.017436461703729
Loss in iteration 199 : 1.383417843020768
Loss in iteration 200 : 1.6231973701489515
Testing accuracy  of updater 0 on alg 1 with rate 0.00448 = 0.786, training accuracy 0.7853674328261573, time elapsed: 3695 millisecond.
Loss in iteration 1 : 1.001488599803296
Loss in iteration 2 : 3.434253280740304
Loss in iteration 3 : 2.2076476733525694
Loss in iteration 4 : 0.9825178000152008
Loss in iteration 5 : 0.5238163193198442
Loss in iteration 6 : 0.5159691124389573
Loss in iteration 7 : 0.5035602258414995
Loss in iteration 8 : 0.49819546835283246
Loss in iteration 9 : 0.49084154972052413
Loss in iteration 10 : 0.49317167820080665
Loss in iteration 11 : 0.49335472647268686
Loss in iteration 12 : 0.5129078158261102
Loss in iteration 13 : 0.534167413220405
Loss in iteration 14 : 0.5730780921078893
Loss in iteration 15 : 0.591035515141516
Loss in iteration 16 : 0.6426602339884219
Loss in iteration 17 : 0.6089864409237126
Loss in iteration 18 : 0.6293398908557485
Loss in iteration 19 : 0.5791669833300318
Loss in iteration 20 : 0.5823052582084711
Loss in iteration 21 : 0.5353155144657693
Loss in iteration 22 : 0.5326187001374847
Loss in iteration 23 : 0.5101050104043279
Loss in iteration 24 : 0.5049459654262417
Loss in iteration 25 : 0.4984087111947553
Loss in iteration 26 : 0.4996634627357693
Loss in iteration 27 : 0.5036973296942657
Loss in iteration 28 : 0.521847591171072
Loss in iteration 29 : 0.5279196117391757
Loss in iteration 30 : 0.55773511504785
Loss in iteration 31 : 0.5557576872676837
Loss in iteration 32 : 0.5978308925139738
Loss in iteration 33 : 0.5722657297642327
Loss in iteration 34 : 0.5998920597683395
Loss in iteration 35 : 0.5540212006032614
Loss in iteration 36 : 0.5704942707397247
Loss in iteration 37 : 0.534868948826727
Loss in iteration 38 : 0.5338388201161678
Loss in iteration 39 : 0.5157700001503976
Loss in iteration 40 : 0.5197019278905358
Loss in iteration 41 : 0.5069223643022388
Loss in iteration 42 : 0.5140927391300792
Loss in iteration 43 : 0.5086803213210483
Loss in iteration 44 : 0.523891666107041
Loss in iteration 45 : 0.5263496880897538
Loss in iteration 46 : 0.551924929564727
Loss in iteration 47 : 0.542950735567951
Loss in iteration 48 : 0.5691342695224478
Loss in iteration 49 : 0.5458881089892718
Loss in iteration 50 : 0.5676241205034595
Loss in iteration 51 : 0.5367265130568709
Loss in iteration 52 : 0.549092250914459
Loss in iteration 53 : 0.5269465604196512
Loss in iteration 54 : 0.5408995357813163
Loss in iteration 55 : 0.5233969621441406
Loss in iteration 56 : 0.5301460249794147
Loss in iteration 57 : 0.5164378319000137
Loss in iteration 58 : 0.5286389090417858
Loss in iteration 59 : 0.5170052212317793
Loss in iteration 60 : 0.5320426075705041
Loss in iteration 61 : 0.523655051164312
Loss in iteration 62 : 0.5432265317480359
Loss in iteration 63 : 0.5279903146391136
Loss in iteration 64 : 0.5437492621157219
Loss in iteration 65 : 0.5285528492485895
Loss in iteration 66 : 0.5436282867123549
Loss in iteration 67 : 0.5284606780153791
Loss in iteration 68 : 0.5435074132167432
Loss in iteration 69 : 0.5283698898202982
Loss in iteration 70 : 0.5427200438422235
Loss in iteration 71 : 0.5257750628688685
Loss in iteration 72 : 0.5402743279124641
Loss in iteration 73 : 0.5226535164627775
Loss in iteration 74 : 0.5371694104392446
Loss in iteration 75 : 0.5191033294484044
Loss in iteration 76 : 0.5358946174962549
Loss in iteration 77 : 0.5202219583393535
Loss in iteration 78 : 0.5386149645822654
Loss in iteration 79 : 0.5213775476868165
Loss in iteration 80 : 0.5387097311645753
Loss in iteration 81 : 0.5202353331228349
Loss in iteration 82 : 0.5359075360335229
Loss in iteration 83 : 0.5197944428704393
Loss in iteration 84 : 0.5357361850453751
Loss in iteration 85 : 0.5198786199801341
Loss in iteration 86 : 0.5345262999383672
Loss in iteration 87 : 0.5200033147163876
Loss in iteration 88 : 0.5358795277581235
Loss in iteration 89 : 0.5195740656581221
Loss in iteration 90 : 0.5362300922378606
Loss in iteration 91 : 0.5201154495530098
Loss in iteration 92 : 0.5372025989309335
Loss in iteration 93 : 0.5212749244364583
Loss in iteration 94 : 0.5407617191500346
Loss in iteration 95 : 0.5230540050638229
Loss in iteration 96 : 0.5428005678651315
Loss in iteration 97 : 0.5224032972769844
Loss in iteration 98 : 0.5418802326638615
Loss in iteration 99 : 0.5217898347692772
Loss in iteration 100 : 0.538763328380738
Loss in iteration 101 : 0.5207307448855711
Loss in iteration 102 : 0.5382270398349243
Loss in iteration 103 : 0.5203949991832967
Loss in iteration 104 : 0.537315120061336
Loss in iteration 105 : 0.5168448928534198
Loss in iteration 106 : 0.5304883345435005
Loss in iteration 107 : 0.5193566892069739
Loss in iteration 108 : 0.537062657830053
Loss in iteration 109 : 0.5204563186147307
Loss in iteration 110 : 0.5401223261059495
Loss in iteration 111 : 0.5212786000222808
Loss in iteration 112 : 0.5407500447218522
Loss in iteration 113 : 0.5206165844524739
Loss in iteration 114 : 0.5398920405056313
Loss in iteration 115 : 0.519587578589324
Loss in iteration 116 : 0.5361462675831843
Loss in iteration 117 : 0.5188382097389226
Loss in iteration 118 : 0.5356533516429889
Loss in iteration 119 : 0.5184175760050856
Loss in iteration 120 : 0.5350369086010192
Loss in iteration 121 : 0.5190062173769735
Loss in iteration 122 : 0.5378530797853744
Loss in iteration 123 : 0.5194721719539499
Loss in iteration 124 : 0.5384283250579358
Loss in iteration 125 : 0.5188784887952371
Loss in iteration 126 : 0.5360739848525904
Loss in iteration 127 : 0.5200573994359219
Loss in iteration 128 : 0.5382714352021387
Loss in iteration 129 : 0.5189986397466372
Loss in iteration 130 : 0.5375247861950716
Loss in iteration 131 : 0.5197287704675726
Loss in iteration 132 : 0.5380031717624182
Loss in iteration 133 : 0.5192323171245324
Loss in iteration 134 : 0.5384813471062313
Loss in iteration 135 : 0.5187378689897257
Loss in iteration 136 : 0.5355829048602021
Loss in iteration 137 : 0.5190588422530211
Loss in iteration 138 : 0.537608118210846
Loss in iteration 139 : 0.5195781432724639
Loss in iteration 140 : 0.5395719235722727
Loss in iteration 141 : 0.5205064536977732
Loss in iteration 142 : 0.5401085723594784
Loss in iteration 143 : 0.5209789811984874
Loss in iteration 144 : 0.5401514458646008
Loss in iteration 145 : 0.5198797485888329
Loss in iteration 146 : 0.5346311420808629
Loss in iteration 147 : 0.5165725260379445
Loss in iteration 148 : 0.5321663074827059
Loss in iteration 149 : 0.5135101806079472
Loss in iteration 150 : 0.5293525742848109
Loss in iteration 151 : 0.5133952616606061
Loss in iteration 152 : 0.5294121471657459
Loss in iteration 153 : 0.514235550913102
Loss in iteration 154 : 0.5340782990201689
Loss in iteration 155 : 0.5193328715226972
Loss in iteration 156 : 0.540049219969884
Loss in iteration 157 : 0.52515889520263
Loss in iteration 158 : 0.5411182837976335
Loss in iteration 159 : 0.5240540830973649
Loss in iteration 160 : 0.5387475384131746
Loss in iteration 161 : 0.5202069362925994
Loss in iteration 162 : 0.5391014139829885
Loss in iteration 163 : 0.5198391823816498
Loss in iteration 164 : 0.5384378635326669
Loss in iteration 165 : 0.5204899475087652
Loss in iteration 166 : 0.5387918927882319
Loss in iteration 167 : 0.5187165212376821
Loss in iteration 168 : 0.5368488195164968
Loss in iteration 169 : 0.5174821956805093
Loss in iteration 170 : 0.5358694570791633
Loss in iteration 171 : 0.5154347919008572
Loss in iteration 172 : 0.5311504190833128
Loss in iteration 173 : 0.513241413589904
Loss in iteration 174 : 0.5313812147388822
Loss in iteration 175 : 0.51296947231422
Loss in iteration 176 : 0.5316170459928534
Loss in iteration 177 : 0.5154654209541654
Loss in iteration 178 : 0.5395749466745238
Loss in iteration 179 : 0.5230585469083319
Loss in iteration 180 : 0.5392464889509069
Loss in iteration 181 : 0.5205524330858777
Loss in iteration 182 : 0.5386993498359576
Loss in iteration 183 : 0.5190198769142351
Loss in iteration 184 : 0.5392011492335628
Loss in iteration 185 : 0.5195306155726983
Loss in iteration 186 : 0.5386793612479633
Loss in iteration 187 : 0.5184984731990715
Loss in iteration 188 : 0.5397014243399149
Loss in iteration 189 : 0.5199757952498579
Loss in iteration 190 : 0.538217301286699
Loss in iteration 191 : 0.5180561929004187
Loss in iteration 192 : 0.537432476937764
Loss in iteration 193 : 0.5143630358868116
Loss in iteration 194 : 0.5306278057388377
Loss in iteration 195 : 0.5129849861811593
Loss in iteration 196 : 0.5311465538148292
Loss in iteration 197 : 0.5131367428123947
Loss in iteration 198 : 0.5328955193291587
Loss in iteration 199 : 0.5188911901229614
Loss in iteration 200 : 0.5393094641447516
Testing accuracy  of updater 0 on alg 1 with rate 0.00112 = 0.7895, training accuracy 0.7983166073162836, time elapsed: 3835 millisecond.
Loss in iteration 1 : 1.0007294139036151
Loss in iteration 2 : 2.5095433488227004
Loss in iteration 3 : 1.6511228433129341
Loss in iteration 4 : 0.794023679385984
Loss in iteration 5 : 0.47057057395955587
Loss in iteration 6 : 0.4789270108316736
Loss in iteration 7 : 0.4877338789926181
Loss in iteration 8 : 0.49440242948112806
Loss in iteration 9 : 0.500469681661989
Loss in iteration 10 : 0.5029509307452713
Loss in iteration 11 : 0.4941963494380171
Loss in iteration 12 : 0.4857967466755214
Loss in iteration 13 : 0.4717589717204402
Loss in iteration 14 : 0.4641652482568835
Loss in iteration 15 : 0.45829078107693993
Loss in iteration 16 : 0.4538930848701185
Loss in iteration 17 : 0.45167141944886785
Loss in iteration 18 : 0.45120486165462953
Loss in iteration 19 : 0.4490456853893618
Loss in iteration 20 : 0.45048348546940653
Loss in iteration 21 : 0.45173247954703033
Loss in iteration 22 : 0.45461843569860977
Loss in iteration 23 : 0.4604082196135853
Loss in iteration 24 : 0.4608214708926462
Loss in iteration 25 : 0.4615874630051043
Loss in iteration 26 : 0.465680322785678
Loss in iteration 27 : 0.4590752690229579
Loss in iteration 28 : 0.4575224934478895
Loss in iteration 29 : 0.45783555148280863
Loss in iteration 30 : 0.4572507562056297
Loss in iteration 31 : 0.45648850088087334
Loss in iteration 32 : 0.4554561091804792
Loss in iteration 33 : 0.4560097619896136
Loss in iteration 34 : 0.453028863024737
Loss in iteration 35 : 0.4537047513341672
Loss in iteration 36 : 0.45229443038343564
Loss in iteration 37 : 0.45318518635372285
Loss in iteration 38 : 0.4536628054906828
Loss in iteration 39 : 0.4549203232220386
Loss in iteration 40 : 0.457902481078425
Loss in iteration 41 : 0.4554233863427125
Loss in iteration 42 : 0.45747085184610037
Loss in iteration 43 : 0.45429304134212933
Loss in iteration 44 : 0.45717612668426155
Loss in iteration 45 : 0.45417029163115324
Loss in iteration 46 : 0.45758437782038597
Loss in iteration 47 : 0.4539555914109097
Loss in iteration 48 : 0.4567575861137235
Loss in iteration 49 : 0.45325031084689726
Loss in iteration 50 : 0.4562395988456834
Loss in iteration 51 : 0.45218319612526653
Loss in iteration 52 : 0.4560249966420456
Loss in iteration 53 : 0.45211964441085856
Loss in iteration 54 : 0.45492308206589827
Loss in iteration 55 : 0.45210554832314775
Loss in iteration 56 : 0.45468290503907816
Loss in iteration 57 : 0.4513611284652574
Loss in iteration 58 : 0.4544948116775123
Loss in iteration 59 : 0.4501230232983829
Loss in iteration 60 : 0.45450756485664173
Loss in iteration 61 : 0.45097990298360546
Loss in iteration 62 : 0.45646063370378964
Loss in iteration 63 : 0.45126164558184884
Loss in iteration 64 : 0.45535089550620333
Loss in iteration 65 : 0.45097333142899665
Loss in iteration 66 : 0.4550366139852092
Loss in iteration 67 : 0.4491315667821466
Loss in iteration 68 : 0.45207818786205584
Loss in iteration 69 : 0.44976863257781574
Loss in iteration 70 : 0.4529752974336849
Loss in iteration 71 : 0.44923076393371286
Loss in iteration 72 : 0.45337417434442184
Loss in iteration 73 : 0.44932829396002866
Loss in iteration 74 : 0.4531379595572903
Loss in iteration 75 : 0.44879200342115183
Loss in iteration 76 : 0.4529418921219829
Loss in iteration 77 : 0.4488517170851689
Loss in iteration 78 : 0.45274590753726096
Loss in iteration 79 : 0.4489114333864068
Loss in iteration 80 : 0.45255000576374543
Loss in iteration 81 : 0.4489722325035195
Loss in iteration 82 : 0.4530919421784616
Loss in iteration 83 : 0.44830990919792885
Loss in iteration 84 : 0.4517317969774493
Loss in iteration 85 : 0.4481025345178587
Loss in iteration 86 : 0.4520851818691722
Loss in iteration 87 : 0.4482934052338387
Loss in iteration 88 : 0.45179591811230435
Loss in iteration 89 : 0.4478416908353114
Loss in iteration 90 : 0.45215031213883916
Loss in iteration 91 : 0.44898645172486606
Loss in iteration 92 : 0.45246698153693937
Loss in iteration 93 : 0.44848823434591345
Loss in iteration 94 : 0.45287791634687086
Loss in iteration 95 : 0.44881199278946815
Loss in iteration 96 : 0.45284384215288614
Loss in iteration 97 : 0.44876315283617385
Loss in iteration 98 : 0.452066318822144
Loss in iteration 99 : 0.446955196565632
Loss in iteration 100 : 0.44924966970065916
Loss in iteration 101 : 0.4467975008732779
Loss in iteration 102 : 0.44855194464549675
Loss in iteration 103 : 0.44699873133119133
Loss in iteration 104 : 0.449098480957422
Loss in iteration 105 : 0.4472983642999225
Loss in iteration 106 : 0.44912357176236917
Loss in iteration 107 : 0.44776132277324615
Loss in iteration 108 : 0.44989127232897247
Loss in iteration 109 : 0.447446689940916
Loss in iteration 110 : 0.44942869471023605
Loss in iteration 111 : 0.4478700338209218
Loss in iteration 112 : 0.4497043406932667
Loss in iteration 113 : 0.44755776664753977
Loss in iteration 114 : 0.4484402276342393
Loss in iteration 115 : 0.4483410903419297
Loss in iteration 116 : 0.44917240806716974
Loss in iteration 117 : 0.4475752651622968
Loss in iteration 118 : 0.4499079576054928
Loss in iteration 119 : 0.4478562568795035
Loss in iteration 120 : 0.44959712557491094
Loss in iteration 121 : 0.44768626424877006
Loss in iteration 122 : 0.44973791970692795
Loss in iteration 123 : 0.447516207063522
Loss in iteration 124 : 0.4491405036847889
Loss in iteration 125 : 0.4473872938906924
Loss in iteration 126 : 0.44924276160241344
Loss in iteration 127 : 0.4472584253168953
Loss in iteration 128 : 0.44934499162819064
Loss in iteration 129 : 0.447129601325527
Loss in iteration 130 : 0.4494474535735411
Loss in iteration 131 : 0.4475517521690268
Loss in iteration 132 : 0.45126660971038796
Loss in iteration 133 : 0.44870393911041984
Loss in iteration 134 : 0.45080027707970716
Loss in iteration 135 : 0.44672386240326356
Loss in iteration 136 : 0.4492619896400136
Loss in iteration 137 : 0.44711384428825257
Loss in iteration 138 : 0.45036885009458655
Loss in iteration 139 : 0.4471073990979461
Loss in iteration 140 : 0.45035340371594673
Loss in iteration 141 : 0.4471009593600098
Loss in iteration 142 : 0.45033796561115275
Loss in iteration 143 : 0.4470962082723784
Loss in iteration 144 : 0.44972165137614634
Loss in iteration 145 : 0.44769461076999445
Loss in iteration 146 : 0.4511872980771127
Loss in iteration 147 : 0.44728034254100146
Loss in iteration 148 : 0.44950391823586155
Loss in iteration 149 : 0.4472277886422548
Loss in iteration 150 : 0.44899407345185544
Loss in iteration 151 : 0.44772432520171435
Loss in iteration 152 : 0.45058786169172294
Loss in iteration 153 : 0.44717912763282075
Loss in iteration 154 : 0.4498632676367221
Loss in iteration 155 : 0.44682533868962154
Loss in iteration 156 : 0.4487296842810829
Loss in iteration 157 : 0.44739581167951004
Loss in iteration 158 : 0.44961840994386854
Loss in iteration 159 : 0.4475341718445305
Loss in iteration 160 : 0.4501739676034812
Loss in iteration 161 : 0.4475375282405532
Loss in iteration 162 : 0.45015699047326707
Loss in iteration 163 : 0.44754088571918327
Loss in iteration 164 : 0.45014002080108484
Loss in iteration 165 : 0.4475441891354349
Loss in iteration 166 : 0.44879264183398604
Loss in iteration 167 : 0.44681266603345066
Loss in iteration 168 : 0.448677542794785
Loss in iteration 169 : 0.44691467612166513
Loss in iteration 170 : 0.4485624370574931
Loss in iteration 171 : 0.4465887791702847
Loss in iteration 172 : 0.4477487966659029
Loss in iteration 173 : 0.44667186806804104
Loss in iteration 174 : 0.4476555621182452
Loss in iteration 175 : 0.44675493250352244
Loss in iteration 176 : 0.4475623583985106
Loss in iteration 177 : 0.44683842170614696
Loss in iteration 178 : 0.44816917048592275
Loss in iteration 179 : 0.4469409986018471
Loss in iteration 180 : 0.4480569081860334
Loss in iteration 181 : 0.44632496364206853
Loss in iteration 182 : 0.4476136204051289
Loss in iteration 183 : 0.44675884859333304
Loss in iteration 184 : 0.4478701025636756
Loss in iteration 185 : 0.44698131159447163
Loss in iteration 186 : 0.44763900285168456
Loss in iteration 187 : 0.44671548896930086
Loss in iteration 188 : 0.4478965440372524
Loss in iteration 189 : 0.44748739001247
Loss in iteration 190 : 0.4495975706761584
Loss in iteration 191 : 0.447503545246582
Loss in iteration 192 : 0.44882096213088396
Loss in iteration 193 : 0.4478414127583084
Loss in iteration 194 : 0.4492276643839031
Loss in iteration 195 : 0.44742674928272164
Loss in iteration 196 : 0.4488825282400702
Loss in iteration 197 : 0.4477644840342672
Loss in iteration 198 : 0.44948833477236866
Loss in iteration 199 : 0.4471519901699466
Loss in iteration 200 : 0.44839727551225705
Testing accuracy  of updater 0 on alg 1 with rate 7.84E-4 = 0.78825, training accuracy 0.8080284881838783, time elapsed: 3514 millisecond.
Loss in iteration 1 : 1.0002381759685275
Loss in iteration 2 : 1.585001663181624
Loss in iteration 3 : 1.0945918875238143
Loss in iteration 4 : 0.6074214175844449
Loss in iteration 5 : 0.43812146588291323
Loss in iteration 6 : 0.47214736747141756
Loss in iteration 7 : 0.4602323037277708
Loss in iteration 8 : 0.4747133049569506
Loss in iteration 9 : 0.44667360681282936
Loss in iteration 10 : 0.44463594134260853
Loss in iteration 11 : 0.42968442059712386
Loss in iteration 12 : 0.4240669731971314
Loss in iteration 13 : 0.41221120168169806
Loss in iteration 14 : 0.4047578968167315
Loss in iteration 15 : 0.3970762609977489
Loss in iteration 16 : 0.39281310781060885
Loss in iteration 17 : 0.38964716920352865
Loss in iteration 18 : 0.38807942830008124
Loss in iteration 19 : 0.3863005221112078
Loss in iteration 20 : 0.3851356097549015
Loss in iteration 21 : 0.3837287520745182
Loss in iteration 22 : 0.3829045788205483
Testing accuracy  of updater 0 on alg 1 with rate 4.4799999999999994E-4 = 0.779, training accuracy 0.8313370022661055, time elapsed: 431 millisecond.
Loss in iteration 1 : 1.000014885998033
Loss in iteration 2 : 0.6613753506115296
Loss in iteration 3 : 0.5528462917524294
Loss in iteration 4 : 0.5233627123468951
Testing accuracy  of updater 0 on alg 1 with rate 1.119999999999999E-4 = 0.50225, training accuracy 0.6484299125930721, time elapsed: 85 millisecond.
Loss in iteration 1 : 1.0001453710745407
Loss in iteration 2 : 1.315694123094059
Loss in iteration 3 : 1.7993241037347505
Loss in iteration 4 : 1.8515126265648403
Loss in iteration 5 : 1.5154829465140123
Loss in iteration 6 : 0.8538500828476688
Loss in iteration 7 : 0.5249130519040286
Loss in iteration 8 : 0.8797201987701662
Loss in iteration 9 : 1.2189771528920754
Loss in iteration 10 : 1.142915465569065
Loss in iteration 11 : 0.8568569105157773
Loss in iteration 12 : 0.689333786882617
Loss in iteration 13 : 0.730121562493608
Loss in iteration 14 : 0.8713182222585704
Loss in iteration 15 : 0.9821129716686959
Loss in iteration 16 : 1.0012184068649839
Loss in iteration 17 : 0.936111295173126
Loss in iteration 18 : 0.8371983792698215
Loss in iteration 19 : 0.767385299352548
Loss in iteration 20 : 0.7611746128161962
Loss in iteration 21 : 0.8087259238372709
Loss in iteration 22 : 0.8607725585653927
Loss in iteration 23 : 0.8700080265489087
Loss in iteration 24 : 0.8275541191677688
Loss in iteration 25 : 0.7643575461723467
Loss in iteration 26 : 0.7211124421193672
Loss in iteration 27 : 0.7138375698425129
Loss in iteration 28 : 0.7299178722042602
Loss in iteration 29 : 0.7420700551457038
Loss in iteration 30 : 0.7294295965493671
Loss in iteration 31 : 0.6925679916286408
Loss in iteration 32 : 0.6504094281981739
Loss in iteration 33 : 0.6236032116321284
Loss in iteration 34 : 0.6197447140191228
Loss in iteration 35 : 0.6243567389258518
Loss in iteration 36 : 0.6132686183798761
Loss in iteration 37 : 0.5807937420592392
Loss in iteration 38 : 0.5455031420838546
Loss in iteration 39 : 0.5308964327270185
Loss in iteration 40 : 0.5298358891865224
Loss in iteration 41 : 0.5228461513696862
Loss in iteration 42 : 0.49841451902105394
Loss in iteration 43 : 0.46794179202182384
Loss in iteration 44 : 0.4562462128424255
Loss in iteration 45 : 0.4573425096712223
Loss in iteration 46 : 0.4433000676049154
Loss in iteration 47 : 0.41770983242533183
Loss in iteration 48 : 0.41234856419095817
Loss in iteration 49 : 0.41559060946550314
Loss in iteration 50 : 0.3982350923295655
Loss in iteration 51 : 0.39298182753412525
Loss in iteration 52 : 0.404323091863023
Loss in iteration 53 : 0.3896195628373274
Loss in iteration 54 : 0.40600440454082143
Loss in iteration 55 : 0.39939185964062046
Loss in iteration 56 : 0.4099690578856816
Loss in iteration 57 : 0.40130287151721994
Loss in iteration 58 : 0.4074511538164392
Loss in iteration 59 : 0.39737774141155746
Loss in iteration 60 : 0.3980679987941634
Loss in iteration 61 : 0.3888744736297536
Loss in iteration 62 : 0.38761206794946085
Loss in iteration 63 : 0.38530097489565573
Loss in iteration 64 : 0.3814529724850068
Loss in iteration 65 : 0.3842820923867892
Loss in iteration 66 : 0.3814609150941719
Loss in iteration 67 : 0.38238623737372507
Loss in iteration 68 : 0.3841215044915164
Loss in iteration 69 : 0.3818684391524614
Loss in iteration 70 : 0.38304946203976525
Loss in iteration 71 : 0.383596298514808
Loss in iteration 72 : 0.3815620328436999
Loss in iteration 73 : 0.3823035818445378
Loss in iteration 74 : 0.3819375939075747
Loss in iteration 75 : 0.37986517948982323
Loss in iteration 76 : 0.38021105267701333
Loss in iteration 77 : 0.3790714998607621
Loss in iteration 78 : 0.37766501054954715
Loss in iteration 79 : 0.3777432125904131
Loss in iteration 80 : 0.376421748772228
Loss in iteration 81 : 0.3764473000666241
Loss in iteration 82 : 0.37615570397148496
Loss in iteration 83 : 0.3757148084735901
Loss in iteration 84 : 0.37640629069588116
Loss in iteration 85 : 0.3757097336950435
Testing accuracy  of updater 1 on alg 1 with rate 3.4999999999999994E-4 = 0.78775, training accuracy 0.8378115895111686, time elapsed: 1723 millisecond.
Loss in iteration 1 : 1.000071231826525
Loss in iteration 2 : 1.0266621493919723
Loss in iteration 3 : 1.3651667955362807
Loss in iteration 4 : 1.401680104470399
Loss in iteration 5 : 1.1664399711594025
Loss in iteration 6 : 0.6977438789359811
Loss in iteration 7 : 0.4491108896995252
Loss in iteration 8 : 0.7171596736823349
Loss in iteration 9 : 0.9467637962265113
Loss in iteration 10 : 0.8664561720273822
Loss in iteration 11 : 0.6563304747915313
Loss in iteration 12 : 0.5501707392706621
Loss in iteration 13 : 0.5953907294242639
Loss in iteration 14 : 0.7016805239192355
Loss in iteration 15 : 0.7721495283774868
Loss in iteration 16 : 0.7705619567920119
Loss in iteration 17 : 0.7118450273254576
Loss in iteration 18 : 0.6419718523146969
Loss in iteration 19 : 0.6065256962543422
Loss in iteration 20 : 0.6178435311928959
Loss in iteration 21 : 0.6581014603615494
Loss in iteration 22 : 0.68698911284284
Loss in iteration 23 : 0.681258179410593
Loss in iteration 24 : 0.6456618390973546
Loss in iteration 25 : 0.605372786688251
Loss in iteration 26 : 0.5840898127351809
Loss in iteration 27 : 0.5869556680520568
Loss in iteration 28 : 0.6001451249866807
Loss in iteration 29 : 0.6050736454373915
Loss in iteration 30 : 0.5920729941493256
Loss in iteration 31 : 0.5655012652815692
Loss in iteration 32 : 0.5388037633470765
Loss in iteration 33 : 0.5241734861568658
Loss in iteration 34 : 0.5258061446778788
Loss in iteration 35 : 0.5291798409392897
Loss in iteration 36 : 0.51953194936138
Loss in iteration 37 : 0.49710390022880946
Loss in iteration 38 : 0.47438081065824367
Loss in iteration 39 : 0.4662867235125243
Loss in iteration 40 : 0.46624868848287254
Loss in iteration 41 : 0.461924479853994
Loss in iteration 42 : 0.44619547363481615
Loss in iteration 43 : 0.427042927017296
Loss in iteration 44 : 0.41945979998289246
Loss in iteration 45 : 0.42053531713905334
Loss in iteration 46 : 0.4151582726608119
Loss in iteration 47 : 0.3999984314883801
Loss in iteration 48 : 0.39180468464640195
Loss in iteration 49 : 0.394292943895323
Loss in iteration 50 : 0.39173421640771705
Loss in iteration 51 : 0.3822871025619935
Loss in iteration 52 : 0.3837653671860194
Loss in iteration 53 : 0.3893494918519021
Loss in iteration 54 : 0.3826441340753926
Loss in iteration 55 : 0.386411826562053
Loss in iteration 56 : 0.39160756171400346
Loss in iteration 57 : 0.38569362072708846
Loss in iteration 58 : 0.3926990736618617
Testing accuracy  of updater 1 on alg 1 with rate 2.45E-4 = 0.79275, training accuracy 0.8452573648429913, time elapsed: 1277 millisecond.
Loss in iteration 1 : 1.0000232593719265
Loss in iteration 2 : 0.7377870323044141
Loss in iteration 3 : 0.9315308041066218
Loss in iteration 4 : 0.952759524316992
Loss in iteration 5 : 0.8187293989412616
Loss in iteration 6 : 0.5531919156144441
Loss in iteration 7 : 0.3955398694282754
Loss in iteration 8 : 0.5610504340059129
Loss in iteration 9 : 0.6718790249272591
Loss in iteration 10 : 0.6044941663341914
Loss in iteration 11 : 0.4800537973573124
Loss in iteration 12 : 0.4300195139509463
Loss in iteration 13 : 0.4695901505084631
Loss in iteration 14 : 0.5333599084221117
Loss in iteration 15 : 0.5632843570214134
Loss in iteration 16 : 0.546194270635571
Loss in iteration 17 : 0.5038545841448085
Loss in iteration 18 : 0.47034582756304205
Loss in iteration 19 : 0.4659072270077134
Loss in iteration 20 : 0.4863048991272726
Loss in iteration 21 : 0.5094170334298223
Loss in iteration 22 : 0.5174444256397556
Loss in iteration 23 : 0.5054152935424915
Loss in iteration 24 : 0.4832037360227109
Loss in iteration 25 : 0.46572292228564577
Loss in iteration 26 : 0.4621642658165437
Loss in iteration 27 : 0.468868777934279
Loss in iteration 28 : 0.47638431944989623
Loss in iteration 29 : 0.47663094674056955
Loss in iteration 30 : 0.46680788751059105
Loss in iteration 31 : 0.4525118054829629
Loss in iteration 32 : 0.4413221789319523
Loss in iteration 33 : 0.438177840662808
Loss in iteration 34 : 0.44025643452906255
Loss in iteration 35 : 0.4415341675553192
Loss in iteration 36 : 0.43683730817506405
Loss in iteration 37 : 0.4268341808104422
Loss in iteration 38 : 0.41649962540966806
Loss in iteration 39 : 0.4108099596609089
Loss in iteration 40 : 0.4112447177729648
Loss in iteration 41 : 0.4113392091900138
Loss in iteration 42 : 0.406317730682813
Loss in iteration 43 : 0.397484496923394
Loss in iteration 44 : 0.3918055506962645
Loss in iteration 45 : 0.391121367596923
Loss in iteration 46 : 0.3915865709687301
Loss in iteration 47 : 0.3885352185616498
Loss in iteration 48 : 0.38297856866427366
Loss in iteration 49 : 0.3800641885832917
Loss in iteration 50 : 0.38099870690738963
Loss in iteration 51 : 0.38143338466149
Loss in iteration 52 : 0.3791133886282007
Loss in iteration 53 : 0.3762744020893331
Loss in iteration 54 : 0.3768509267617793
Loss in iteration 55 : 0.3786123195151328
Loss in iteration 56 : 0.37789608387800677
Loss in iteration 57 : 0.3764013328637024
Loss in iteration 58 : 0.37722766224506166
Testing accuracy  of updater 1 on alg 1 with rate 1.4E-4 = 0.78125, training accuracy 0.8387827775979282, time elapsed: 1037 millisecond.
Loss in iteration 1 : 1.0000014537107453
Loss in iteration 2 : 0.6267573706131185
Loss in iteration 3 : 0.5953205374977644
Loss in iteration 4 : 0.6861346954010957
Loss in iteration 5 : 0.7329312672184009
Loss in iteration 6 : 0.7371116623844585
Loss in iteration 7 : 0.7028385885239985
Loss in iteration 8 : 0.6346971965754539
Loss in iteration 9 : 0.5420225094948727
Loss in iteration 10 : 0.456017147573775
Loss in iteration 11 : 0.4180911506174496
Loss in iteration 12 : 0.4542879915851389
Loss in iteration 13 : 0.5048464845929898
Loss in iteration 14 : 0.5164847490928342
Loss in iteration 15 : 0.4856497389107294
Loss in iteration 16 : 0.4370393232995394
Loss in iteration 17 : 0.3996511102932486
Loss in iteration 18 : 0.38851083329084185
Loss in iteration 19 : 0.39859612550490986
Loss in iteration 20 : 0.41622405034478727
Loss in iteration 21 : 0.42773146713199317
Loss in iteration 22 : 0.42798781072267605
Loss in iteration 23 : 0.41868991999495536
Loss in iteration 24 : 0.40543791167785637
Loss in iteration 25 : 0.39408948756971574
Loss in iteration 26 : 0.3883193130112748
Loss in iteration 27 : 0.38916481937360425
Loss in iteration 28 : 0.39412960917853357
Testing accuracy  of updater 1 on alg 1 with rate 3.5E-5 = 0.78, training accuracy 0.8225963094852703, time elapsed: 558 millisecond.
Loss in iteration 1 : 1.0000007123182653
Loss in iteration 2 : 0.7148655299351966
Loss in iteration 3 : 0.5592189501343692
Loss in iteration 4 : 0.634731751027267
Loss in iteration 5 : 0.6924514044371387
Loss in iteration 6 : 0.7184115308705639
Loss in iteration 7 : 0.7152457186155332
Loss in iteration 8 : 0.6858988207906218
Loss in iteration 9 : 0.6335083155929647
Loss in iteration 10 : 0.5634776128092
Loss in iteration 11 : 0.49167860327282514
Loss in iteration 12 : 0.4404150831505172
Loss in iteration 13 : 0.42603511731159954
Loss in iteration 14 : 0.45279409845750523
Loss in iteration 15 : 0.48601559924482307
Loss in iteration 16 : 0.49700969152599334
Loss in iteration 17 : 0.4806736695924831
Loss in iteration 18 : 0.4479597368156497
Loss in iteration 19 : 0.41535314591237227
Loss in iteration 20 : 0.3955699552062394
Loss in iteration 21 : 0.3910127440562442
Loss in iteration 22 : 0.3983000554238323
Loss in iteration 23 : 0.4092954495945888
Loss in iteration 24 : 0.4165827382004747
Testing accuracy  of updater 1 on alg 1 with rate 2.45E-5 = 0.749, training accuracy 0.8164454516024603, time elapsed: 467 millisecond.
Loss in iteration 1 : 1.0000002325937192
Loss in iteration 2 : 0.8338635852058144
Loss in iteration 3 : 0.5944339794360496
Loss in iteration 4 : 0.5625167528201281
Loss in iteration 5 : 0.6135089398382029
Loss in iteration 6 : 0.6562283453340698
Loss in iteration 7 : 0.6803367536744745
Loss in iteration 8 : 0.6871192996083186
Loss in iteration 9 : 0.6781840619452965
Loss in iteration 10 : 0.6551763930857192
Loss in iteration 11 : 0.6198311474397201
Loss in iteration 12 : 0.575064331899367
Loss in iteration 13 : 0.5273700340397042
Loss in iteration 14 : 0.48647713216797833
Loss in iteration 15 : 0.4593338111859131
Loss in iteration 16 : 0.4490203916841818
Loss in iteration 17 : 0.4517799171114209
Loss in iteration 18 : 0.46442125041181115
Loss in iteration 19 : 0.4770832505286847
Loss in iteration 20 : 0.4803345388032734
Testing accuracy  of updater 1 on alg 1 with rate 1.3999999999999998E-5 = 0.7845, training accuracy 0.7944318549692457, time elapsed: 500 millisecond.
Loss in iteration 1 : 1.0000000145371075
Loss in iteration 2 : 0.9584655295311177
Testing accuracy  of updater 1 on alg 1 with rate 3.499999999999997E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 60 millisecond.
Loss in iteration 1 : 1.009303748770601
Loss in iteration 2 : 15.012463024755037
Loss in iteration 3 : 15.440303203401934
Loss in iteration 4 : 12.75697168939712
Loss in iteration 5 : 7.330131435590398
Loss in iteration 6 : 3.0671532537814987
Loss in iteration 7 : 3.3543631485852776
Loss in iteration 8 : 4.517737310558127
Loss in iteration 9 : 4.675799197939144
Loss in iteration 10 : 4.252581617660439
Loss in iteration 11 : 3.9323329969098
Loss in iteration 12 : 3.8965801935027216
Loss in iteration 13 : 3.9713205187229033
Loss in iteration 14 : 4.064633028639651
Loss in iteration 15 : 4.0901037374889855
Loss in iteration 16 : 4.046151267543398
Loss in iteration 17 : 3.971169164033492
Loss in iteration 18 : 3.8967432488302487
Loss in iteration 19 : 3.8192479013252503
Loss in iteration 20 : 3.7354687828896767
Loss in iteration 21 : 3.638022240399376
Loss in iteration 22 : 3.526000952004425
Loss in iteration 23 : 3.400575270022477
Loss in iteration 24 : 3.264814390329986
Loss in iteration 25 : 3.1216509403108765
Loss in iteration 26 : 2.9746287588018583
Loss in iteration 27 : 2.826280413557229
Loss in iteration 28 : 2.674325421829454
Loss in iteration 29 : 2.5175206692831797
Loss in iteration 30 : 2.358257658431927
Loss in iteration 31 : 2.1992483483155727
Loss in iteration 32 : 2.0421476607852824
Loss in iteration 33 : 1.8843405680548135
Loss in iteration 34 : 1.727398438753469
Loss in iteration 35 : 1.5741917791472557
Loss in iteration 36 : 1.4267171784504822
Loss in iteration 37 : 1.2861514253292037
Loss in iteration 38 : 1.1542468506295585
Loss in iteration 39 : 1.0330770337248305
Loss in iteration 40 : 0.9298230050984011
Loss in iteration 41 : 0.8554500943677998
Loss in iteration 42 : 0.8252899989625231
Loss in iteration 43 : 0.8641521734962984
Loss in iteration 44 : 1.0813501034750457
Loss in iteration 45 : 6.1577243721961334
Loss in iteration 46 : 14.481792719829436
Loss in iteration 47 : 15.197442247960515
Loss in iteration 48 : 12.771054825306196
Loss in iteration 49 : 7.52426789409802
Loss in iteration 50 : 2.446213041700235
Loss in iteration 51 : 2.987693091333188
Loss in iteration 52 : 4.020043148381524
Loss in iteration 53 : 3.5766340722290035
Loss in iteration 54 : 3.0457778118829375
Loss in iteration 55 : 2.9174384331608643
Loss in iteration 56 : 2.9901302214435974
Loss in iteration 57 : 3.07630147042173
Loss in iteration 58 : 3.087058556790203
Loss in iteration 59 : 3.038941346389764
Loss in iteration 60 : 2.9759997591734755
Loss in iteration 61 : 2.916986722796154
Loss in iteration 62 : 2.8501928747839935
Loss in iteration 63 : 2.7682508462493556
Loss in iteration 64 : 2.670190343432675
Loss in iteration 65 : 2.5594440354165666
Loss in iteration 66 : 2.4390653899170567
Loss in iteration 67 : 2.3120072999722643
Loss in iteration 68 : 2.1815068730045133
Loss in iteration 69 : 2.044771639610165
Loss in iteration 70 : 1.9029447603226624
Loss in iteration 71 : 1.7581617310403956
Loss in iteration 72 : 1.6144612016239108
Loss in iteration 73 : 1.468200116239567
Loss in iteration 74 : 1.3224443051868657
Loss in iteration 75 : 1.1805991190579732
Loss in iteration 76 : 1.046748899879623
Loss in iteration 77 : 0.9207799009025243
Loss in iteration 78 : 0.8064980498772419
Loss in iteration 79 : 0.7179287001296792
Loss in iteration 80 : 0.6753568108177376
Loss in iteration 81 : 0.7565695492603867
Loss in iteration 82 : 2.0712333090248785
Loss in iteration 83 : 8.429961669633393
Loss in iteration 84 : 14.658529216620678
Loss in iteration 85 : 15.508353843210285
Loss in iteration 86 : 13.203258279929635
Loss in iteration 87 : 8.070604610512314
Loss in iteration 88 : 3.013247129866995
Loss in iteration 89 : 2.9637840183030297
Loss in iteration 90 : 4.18688175911857
Loss in iteration 91 : 4.24135221088274
Loss in iteration 92 : 3.7520514572523833
Loss in iteration 93 : 3.4584467500927065
Loss in iteration 94 : 3.4478392202240054
Loss in iteration 95 : 3.5293872179799037
Loss in iteration 96 : 3.5993762149829753
Loss in iteration 97 : 3.594124906950296
Loss in iteration 98 : 3.5351188351141087
Loss in iteration 99 : 3.462600143273151
Loss in iteration 100 : 3.3909695411376357
Loss in iteration 101 : 3.3162058161859473
Loss in iteration 102 : 3.2279223836181576
Loss in iteration 103 : 3.1244852196622763
Loss in iteration 104 : 3.00639778115672
Loss in iteration 105 : 2.8784170880908064
Loss in iteration 106 : 2.743064327035565
Loss in iteration 107 : 2.6037291879135847
Loss in iteration 108 : 2.4622385766195523
Loss in iteration 109 : 2.315081657565303
Loss in iteration 110 : 2.1637332839276238
Loss in iteration 111 : 2.010343919454318
Loss in iteration 112 : 1.8594225244792475
Loss in iteration 113 : 1.7064381589557167
Loss in iteration 114 : 1.5538273751809577
Loss in iteration 115 : 1.404861914076772
Loss in iteration 116 : 1.2623177703127915
Loss in iteration 117 : 1.126907390060148
Loss in iteration 118 : 1.0004479317511894
Loss in iteration 119 : 0.8877592651400333
Loss in iteration 120 : 0.8009036208617124
Loss in iteration 121 : 0.7549603817940138
Loss in iteration 122 : 0.8104979683326546
Loss in iteration 123 : 1.8508348778298378
Loss in iteration 124 : 9.494699207470772
Loss in iteration 125 : 14.27051755133376
Loss in iteration 126 : 15.048627549289867
Loss in iteration 127 : 12.679076917993727
Loss in iteration 128 : 7.493922817120825
Loss in iteration 129 : 2.755427626608973
Loss in iteration 130 : 2.9857950026714075
Loss in iteration 131 : 4.108900143514348
Loss in iteration 132 : 3.986442514192259
Loss in iteration 133 : 3.504193334563845
Loss in iteration 134 : 3.271949221243028
Loss in iteration 135 : 3.3011486674500814
Loss in iteration 136 : 3.39286301477597
Loss in iteration 137 : 3.4388117189073295
Loss in iteration 138 : 3.4171125696000817
Loss in iteration 139 : 3.3570785104802554
Loss in iteration 140 : 3.2912938702653296
Loss in iteration 141 : 3.2242269324577877
Loss in iteration 142 : 3.1486770678503624
Loss in iteration 143 : 3.058655680814083
Loss in iteration 144 : 2.954044311538643
Loss in iteration 145 : 2.8368661525344105
Loss in iteration 146 : 2.7102845360135195
Loss in iteration 147 : 2.578925840991642
Loss in iteration 148 : 2.4437250435398483
Loss in iteration 149 : 2.3035845536820934
Loss in iteration 150 : 2.158613257552783
Loss in iteration 151 : 2.010824102118257
Loss in iteration 152 : 1.8637309526084846
Loss in iteration 153 : 1.7147254718480824
Loss in iteration 154 : 1.565772756889446
Loss in iteration 155 : 1.419576531522079
Loss in iteration 156 : 1.2790808274807535
Loss in iteration 157 : 1.1448568868567706
Loss in iteration 158 : 1.018845701885095
Loss in iteration 159 : 0.9051162321316865
Loss in iteration 160 : 0.8141567512177652
Loss in iteration 161 : 0.763403688672241
Loss in iteration 162 : 0.8697517149030672
Loss in iteration 163 : 4.7045793552772945
Loss in iteration 164 : 13.965045287324786
Loss in iteration 165 : 14.426057443818733
Loss in iteration 166 : 11.770238816717951
Loss in iteration 167 : 6.316447820759398
Loss in iteration 168 : 1.7572005062647857
Loss in iteration 169 : 3.3631093457934953
Loss in iteration 170 : 3.2093816171175047
Loss in iteration 171 : 2.4890022365421043
Loss in iteration 172 : 2.311714481231185
Loss in iteration 173 : 2.3854119325820182
Loss in iteration 174 : 2.4516254569538867
Loss in iteration 175 : 2.449897301487647
Loss in iteration 176 : 2.413772337099728
Loss in iteration 177 : 2.3731372053600324
Loss in iteration 178 : 2.324768880005489
Loss in iteration 179 : 2.260194586564806
Loss in iteration 180 : 2.1801875022760133
Loss in iteration 181 : 2.0867714758333955
Loss in iteration 182 : 1.9829434608904157
Loss in iteration 183 : 1.872948623366744
Loss in iteration 184 : 1.7550281090729185
Loss in iteration 185 : 1.6304500465529466
Loss in iteration 186 : 1.5024927250314515
Loss in iteration 187 : 1.3717812106869085
Loss in iteration 188 : 1.2383964756015111
Loss in iteration 189 : 1.1052583651172383
Loss in iteration 190 : 0.9777306961077334
Loss in iteration 191 : 0.8570758546284731
Loss in iteration 192 : 0.7457894155490608
Loss in iteration 193 : 0.6569204397027182
Loss in iteration 194 : 0.6221833721009851
Loss in iteration 195 : 1.9812240877642548
Loss in iteration 196 : 11.981735655329468
Loss in iteration 197 : 11.419113331730774
Loss in iteration 198 : 7.844047837151777
Loss in iteration 199 : 1.9580788820729895
Loss in iteration 200 : 3.0718353884407352
Testing accuracy  of updater 2 on alg 1 with rate 0.0027999999999999995 = 0.78175, training accuracy 0.830042084817093, time elapsed: 3730 millisecond.
Loss in iteration 1 : 1.0045588368975944
Loss in iteration 2 : 10.608980285679102
Loss in iteration 3 : 10.906155249398548
Loss in iteration 4 : 9.026645786802918
Loss in iteration 5 : 5.221535216219859
Loss in iteration 6 : 2.1845188550305132
Loss in iteration 7 : 2.392323439882397
Loss in iteration 8 : 3.2146879449005086
Loss in iteration 9 : 3.3022899159177785
Loss in iteration 10 : 2.9930278534486683
Loss in iteration 11 : 2.7702108845978337
Loss in iteration 12 : 2.74861317087525
Loss in iteration 13 : 2.80225949317357
Loss in iteration 14 : 2.867692848966881
Loss in iteration 15 : 2.8847296470290917
Loss in iteration 16 : 2.853811769398311
Loss in iteration 17 : 2.8018012325570294
Loss in iteration 18 : 2.7511894684134197
Loss in iteration 19 : 2.6985917301102718
Loss in iteration 20 : 2.640685108292717
Loss in iteration 21 : 2.5740417738712438
Loss in iteration 22 : 2.497810706019318
Loss in iteration 23 : 2.4123880889921185
Loss in iteration 24 : 2.319670632383464
Loss in iteration 25 : 2.2228063964968037
Loss in iteration 26 : 2.122978659466123
Loss in iteration 27 : 2.022136491195934
Loss in iteration 28 : 1.9179049456541366
Loss in iteration 29 : 1.811272075974557
Loss in iteration 30 : 1.7024028321221079
Loss in iteration 31 : 1.594093541069503
Loss in iteration 32 : 1.4862463331232794
Loss in iteration 33 : 1.377682353488395
Loss in iteration 34 : 1.2696243464828503
Loss in iteration 35 : 1.1639935793170006
Loss in iteration 36 : 1.0632482515627473
Loss in iteration 37 : 0.9670917049645453
Loss in iteration 38 : 0.8766699145134338
Loss in iteration 39 : 0.792655292592125
Loss in iteration 40 : 0.7206377000720616
Loss in iteration 41 : 0.6693086468584422
Loss in iteration 42 : 0.6469538236064057
Loss in iteration 43 : 0.7149390131909366
Loss in iteration 44 : 1.8157451096171568
Loss in iteration 45 : 3.9694235408440144
Loss in iteration 46 : 9.027773564961219
Loss in iteration 47 : 8.939971873700058
Loss in iteration 48 : 6.7139150873675435
Loss in iteration 49 : 2.8039581443389223
Loss in iteration 50 : 1.47334624015642
Loss in iteration 51 : 2.389417402904282
Loss in iteration 52 : 2.295059154126706
Loss in iteration 53 : 1.9373460639126423
Loss in iteration 54 : 1.846349468146665
Loss in iteration 55 : 1.8976645823051075
Loss in iteration 56 : 1.9489083648280496
Loss in iteration 57 : 1.9549128697165286
Loss in iteration 58 : 1.9301534076689946
Loss in iteration 59 : 1.8989094290613309
Loss in iteration 60 : 1.86481196950218
Loss in iteration 61 : 1.8225676636682155
Loss in iteration 62 : 1.768241973470188
Loss in iteration 63 : 1.703675396449919
Loss in iteration 64 : 1.6321036444965176
Loss in iteration 65 : 1.555999411300816
Loss in iteration 66 : 1.474578626191116
Loss in iteration 67 : 1.3881220871829605
Loss in iteration 68 : 1.298358073670418
Loss in iteration 69 : 1.2062845027033313
Loss in iteration 70 : 1.1123956063429985
Loss in iteration 71 : 1.0167977634377054
Loss in iteration 72 : 0.9216571152067967
Loss in iteration 73 : 0.8296076270736699
Loss in iteration 74 : 0.7421695543418338
Loss in iteration 75 : 0.6600369348311673
Loss in iteration 76 : 0.5899890510194629
Loss in iteration 77 : 0.5460968937631403
Loss in iteration 78 : 0.680781840266226
Loss in iteration 79 : 5.572750964220262
Loss in iteration 80 : 10.681775199754137
Loss in iteration 81 : 11.493119880796316
Loss in iteration 82 : 10.074759468152603
Loss in iteration 83 : 6.651862036881425
Loss in iteration 84 : 2.3146022104649058
Loss in iteration 85 : 1.8542388824997107
Loss in iteration 86 : 2.859088166287119
Loss in iteration 87 : 2.713305698670669
Loss in iteration 88 : 2.2373844499343836
Loss in iteration 89 : 2.062885419122499
Loss in iteration 90 : 2.0942912024630798
Loss in iteration 91 : 2.157446155116634
Loss in iteration 92 : 2.172964534407524
Loss in iteration 93 : 2.1421660917717147
Loss in iteration 94 : 2.095862488682909
Loss in iteration 95 : 2.053322498773121
Loss in iteration 96 : 2.008762414579265
Loss in iteration 97 : 1.9565277612724967
Loss in iteration 98 : 1.890545411383436
Loss in iteration 99 : 1.8161413338445116
Loss in iteration 100 : 1.7386465491291834
Loss in iteration 101 : 1.6565170392911401
Loss in iteration 102 : 1.5694258064504436
Loss in iteration 103 : 1.4781914705912715
Loss in iteration 104 : 1.3850076545793881
Loss in iteration 105 : 1.2900448687488963
Loss in iteration 106 : 1.1932567164026762
Loss in iteration 107 : 1.0957349099687415
Loss in iteration 108 : 0.9994849527472647
Loss in iteration 109 : 0.9070894187951681
Loss in iteration 110 : 0.8188418306411127
Loss in iteration 111 : 0.7354543407386942
Loss in iteration 112 : 0.6615998374409944
Loss in iteration 113 : 0.606982923389905
Loss in iteration 114 : 0.6126654246264445
Loss in iteration 115 : 1.8028358728625284
Loss in iteration 116 : 7.70938062363169
Loss in iteration 117 : 7.06668247129807
Loss in iteration 118 : 4.3417246702801755
Loss in iteration 119 : 0.8573837613173256
Loss in iteration 120 : 2.142500051525408
Loss in iteration 121 : 0.9635423900784088
Loss in iteration 122 : 1.008647878579765
Loss in iteration 123 : 1.0370804077667062
Loss in iteration 124 : 1.0504192997565562
Loss in iteration 125 : 1.0494525028011243
Loss in iteration 126 : 1.0354704073560357
Loss in iteration 127 : 1.0098970344484957
Loss in iteration 128 : 0.9739713119412343
Loss in iteration 129 : 0.9288181214551031
Loss in iteration 130 : 0.8760630436612098
Loss in iteration 131 : 0.8169259291484302
Loss in iteration 132 : 0.7524461296283912
Loss in iteration 133 : 0.6843748778397443
Loss in iteration 134 : 0.616012810649469
Loss in iteration 135 : 0.5488096519656355
Loss in iteration 136 : 0.4880097647351409
Loss in iteration 137 : 0.45428849659898124
Loss in iteration 138 : 0.9654577211924553
Loss in iteration 139 : 9.173135391434373
Loss in iteration 140 : 10.271234610956773
Loss in iteration 141 : 11.077426640764877
Loss in iteration 142 : 9.655041716081294
Loss in iteration 143 : 6.229809301940448
Loss in iteration 144 : 2.3588010812099314
Loss in iteration 145 : 2.018881385253013
Loss in iteration 146 : 2.9524453657764917
Loss in iteration 147 : 3.046404514241189
Loss in iteration 148 : 2.6716747202334794
Loss in iteration 149 : 2.428004019504484
Loss in iteration 150 : 2.4005908048833366
Loss in iteration 151 : 2.451299045645071
Loss in iteration 152 : 2.5033534475824375
Loss in iteration 153 : 2.503228713767631
Loss in iteration 154 : 2.462404631433272
Loss in iteration 155 : 2.408079511685674
Loss in iteration 156 : 2.356870061896317
Loss in iteration 157 : 2.3056618990609272
Loss in iteration 158 : 2.246923113808288
Loss in iteration 159 : 2.1768637485080613
Loss in iteration 160 : 2.0975884972900443
Loss in iteration 161 : 2.0126174795381333
Loss in iteration 162 : 1.9250681283400064
Loss in iteration 163 : 1.833904340281862
Loss in iteration 164 : 1.739001462483044
Loss in iteration 165 : 1.6408682020487715
Loss in iteration 166 : 1.5412089132351716
Loss in iteration 167 : 1.4407684226180149
Loss in iteration 168 : 1.3392454865385297
Loss in iteration 169 : 1.2372093763275918
Loss in iteration 170 : 1.1367146257503051
Loss in iteration 171 : 1.0401591279537281
Loss in iteration 172 : 0.9473224623724416
Loss in iteration 173 : 0.8597337362281804
Loss in iteration 174 : 0.7772720607979962
Loss in iteration 175 : 0.7057710651364196
Loss in iteration 176 : 0.6536705641225607
Loss in iteration 177 : 0.6344110554822585
Loss in iteration 178 : 0.7890650887135908
Loss in iteration 179 : 4.607766774632679
Loss in iteration 180 : 9.902212088222178
Loss in iteration 181 : 10.323648319071218
Loss in iteration 182 : 8.554888399122962
Loss in iteration 183 : 4.819421744073726
Loss in iteration 184 : 1.4769591913195312
Loss in iteration 185 : 2.2915925370041754
Loss in iteration 186 : 2.6056683671965186
Loss in iteration 187 : 2.1112596699601074
Loss in iteration 188 : 1.8631126432517633
Loss in iteration 189 : 1.8700385716884598
Loss in iteration 190 : 1.9296725284554368
Loss in iteration 191 : 1.9605604323262202
Loss in iteration 192 : 1.943901678031145
Loss in iteration 193 : 1.9116557672729078
Loss in iteration 194 : 1.8758105878546303
Loss in iteration 195 : 1.8368121423282842
Loss in iteration 196 : 1.7890887754496252
Loss in iteration 197 : 1.7285158974775612
Loss in iteration 198 : 1.6597292057886948
Loss in iteration 199 : 1.587005927667036
Loss in iteration 200 : 1.5082689371097737
Testing accuracy  of updater 2 on alg 1 with rate 0.00196 = 0.78025, training accuracy 0.8316607316283587, time elapsed: 3915 millisecond.
Loss in iteration 1 : 1.001488599803296
Loss in iteration 2 : 6.210215907719857
Loss in iteration 3 : 6.378705459242857
Loss in iteration 4 : 5.304023416937208
Loss in iteration 5 : 3.1232761634269854
Loss in iteration 6 : 1.3113335907540296
Loss in iteration 7 : 1.4652374259727587
Loss in iteration 8 : 1.9168676738216635
Loss in iteration 9 : 1.9397406514234123
Loss in iteration 10 : 1.761527231711469
Loss in iteration 11 : 1.6428256624817703
Loss in iteration 12 : 1.636104203677181
Loss in iteration 13 : 1.6678175196252567
Loss in iteration 14 : 1.7027201571366128
Loss in iteration 15 : 1.7112928727665941
Loss in iteration 16 : 1.694440134817147
Loss in iteration 17 : 1.666859587269362
Loss in iteration 18 : 1.6401848847848397
Loss in iteration 19 : 1.6121477155274195
Loss in iteration 20 : 1.5805614935792534
Loss in iteration 21 : 1.5441072683243795
Loss in iteration 22 : 1.5016144707499435
Loss in iteration 23 : 1.4534443288320829
Loss in iteration 24 : 1.4023473294911044
Loss in iteration 25 : 1.3498229884922532
Loss in iteration 26 : 1.2949899131333984
Loss in iteration 27 : 1.2387625796405688
Loss in iteration 28 : 1.1810360471158172
Loss in iteration 29 : 1.1224138757979059
Loss in iteration 30 : 1.0627817374268569
Loss in iteration 31 : 1.0026977722136903
Loss in iteration 32 : 0.942667518944544
Loss in iteration 33 : 0.8823190408631126
Loss in iteration 34 : 0.8225515092613674
Loss in iteration 35 : 0.7645886602830247
Loss in iteration 36 : 0.7092723375761786
Loss in iteration 37 : 0.6561526589339436
Loss in iteration 38 : 0.6060448864995941
Loss in iteration 39 : 0.5609181383970425
Loss in iteration 40 : 0.5225390402756166
Loss in iteration 41 : 0.4941131233085531
Loss in iteration 42 : 0.4815260446046415
Loss in iteration 43 : 0.5202515262731866
Loss in iteration 44 : 1.1911045899094734
Loss in iteration 45 : 2.2238880579469535
Loss in iteration 46 : 4.882859354844443
Loss in iteration 47 : 4.6499300518546205
Loss in iteration 48 : 3.2141081082505876
Loss in iteration 49 : 1.099086524633547
Loss in iteration 50 : 1.0304535740141405
Loss in iteration 51 : 1.2961866045836177
Loss in iteration 52 : 1.1012005149134658
Loss in iteration 53 : 1.0031711294388124
Loss in iteration 54 : 1.0192910540887477
Loss in iteration 55 : 1.0441734505150537
Loss in iteration 56 : 1.0528804787474821
Loss in iteration 57 : 1.0474132438669108
Loss in iteration 58 : 1.035780005488894
Loss in iteration 59 : 1.0198012948355415
Loss in iteration 60 : 0.9992405129993822
Loss in iteration 61 : 0.9727907016569695
Loss in iteration 62 : 0.941061299351362
Loss in iteration 63 : 0.9051405449487906
Loss in iteration 64 : 0.8659594120451626
Loss in iteration 65 : 0.8236906966640775
Loss in iteration 66 : 0.7789876499566327
Loss in iteration 67 : 0.7325980290495464
Loss in iteration 68 : 0.6848030992148019
Loss in iteration 69 : 0.6367429462373367
Loss in iteration 70 : 0.5897720792171945
Loss in iteration 71 : 0.5441127734296065
Loss in iteration 72 : 0.5013470661174604
Loss in iteration 73 : 0.46406066847017335
Loss in iteration 74 : 0.43453097626633846
Loss in iteration 75 : 0.4229911528731634
Loss in iteration 76 : 0.5454627166924605
Loss in iteration 77 : 2.2827879196343934
Loss in iteration 78 : 0.8943079644375255
Loss in iteration 79 : 4.274965382590379
Loss in iteration 80 : 5.02066215664124
Loss in iteration 81 : 4.973733210667515
Loss in iteration 82 : 3.7056480757241244
Loss in iteration 83 : 1.737292508623997
Loss in iteration 84 : 1.1526983733449505
Loss in iteration 85 : 1.4764797364242637
Loss in iteration 86 : 1.6920482570306195
Loss in iteration 87 : 1.6322411844088782
Loss in iteration 88 : 1.5283915340629752
Loss in iteration 89 : 1.4926886598896365
Loss in iteration 90 : 1.5091813851630531
Loss in iteration 91 : 1.5375289951893076
Loss in iteration 92 : 1.551637446487057
Loss in iteration 93 : 1.5418211225290543
Loss in iteration 94 : 1.5197363250007705
Loss in iteration 95 : 1.4925634153191583
Loss in iteration 96 : 1.4644289012398566
Loss in iteration 97 : 1.4331344796250616
Loss in iteration 98 : 1.397936238334897
Loss in iteration 99 : 1.3565192791933045
Loss in iteration 100 : 1.3105675758237838
Loss in iteration 101 : 1.2623671726365993
Loss in iteration 102 : 1.2127120790884232
Loss in iteration 103 : 1.160484525915653
Loss in iteration 104 : 1.1064373027055066
Loss in iteration 105 : 1.0509003464751372
Loss in iteration 106 : 0.9942926976326195
Loss in iteration 107 : 0.937291494798967
Loss in iteration 108 : 0.8794688036515623
Loss in iteration 109 : 0.8216855709323436
Loss in iteration 110 : 0.7650288829068744
Loss in iteration 111 : 0.7102896108437315
Loss in iteration 112 : 0.657781696655755
Loss in iteration 113 : 0.6073227103991078
Loss in iteration 114 : 0.5613171567906875
Loss in iteration 115 : 0.5214728440510947
Loss in iteration 116 : 0.48962812881225276
Loss in iteration 117 : 0.47228427967119
Loss in iteration 118 : 0.4758895603540888
Loss in iteration 119 : 0.5517959359733948
Loss in iteration 120 : 2.774036355363208
Loss in iteration 121 : 5.678617948721555
Loss in iteration 122 : 5.863715482807457
Loss in iteration 123 : 4.803711769842854
Loss in iteration 124 : 2.628025788786347
Loss in iteration 125 : 0.9079962269774623
Loss in iteration 126 : 1.4364165235488826
Loss in iteration 127 : 1.5594899900640624
Loss in iteration 128 : 1.2940682548544065
Loss in iteration 129 : 1.1628813891319858
Loss in iteration 130 : 1.173649088398471
Loss in iteration 131 : 1.2103907414083157
Loss in iteration 132 : 1.225845911179583
Loss in iteration 133 : 1.2170005360595495
Loss in iteration 134 : 1.1997335340681676
Loss in iteration 135 : 1.1813881688006704
Loss in iteration 136 : 1.1613338951379306
Loss in iteration 137 : 1.1356148335945404
Loss in iteration 138 : 1.1030491331792247
Loss in iteration 139 : 1.065644400153979
Loss in iteration 140 : 1.0254357608367937
Loss in iteration 141 : 0.9824164312877822
Loss in iteration 142 : 0.9365483297333138
Loss in iteration 143 : 0.888101803073416
Loss in iteration 144 : 0.8382813534459742
Loss in iteration 145 : 0.7871700114605789
Loss in iteration 146 : 0.7351624613939655
Loss in iteration 147 : 0.6833983210544139
Loss in iteration 148 : 0.632956027300481
Loss in iteration 149 : 0.5843730629467703
Loss in iteration 150 : 0.5380620492626932
Loss in iteration 151 : 0.49678358885101753
Loss in iteration 152 : 0.46238703294698685
Loss in iteration 153 : 0.44042231868257703
Loss in iteration 154 : 0.4439408283434225
Loss in iteration 155 : 0.6933136831334431
Loss in iteration 156 : 4.511888694344399
Loss in iteration 157 : 5.739267342960188
Loss in iteration 158 : 6.057036789402518
Loss in iteration 159 : 5.116478011249436
Loss in iteration 160 : 3.048913196018672
Loss in iteration 161 : 1.1578277733599807
Loss in iteration 162 : 1.3255238183186204
Loss in iteration 163 : 1.7404420980268114
Loss in iteration 164 : 1.6318292303632982
Loss in iteration 165 : 1.4400894856680315
Loss in iteration 166 : 1.3626455283725734
Loss in iteration 167 : 1.382189896907728
Loss in iteration 168 : 1.4177375831685222
Loss in iteration 169 : 1.4334408980149682
Loss in iteration 170 : 1.4202977570740758
Loss in iteration 171 : 1.3984201909115563
Loss in iteration 172 : 1.3736928452979915
Loss in iteration 173 : 1.3482355434632232
Loss in iteration 174 : 1.3214765247927094
Loss in iteration 175 : 1.2879111936509886
Loss in iteration 176 : 1.2473886725185868
Loss in iteration 177 : 1.2041750055316114
Loss in iteration 178 : 1.1593564248810402
Loss in iteration 179 : 1.1114995106024368
Loss in iteration 180 : 1.0613101532487148
Loss in iteration 181 : 1.0092047527051209
Loss in iteration 182 : 0.955999196320684
Loss in iteration 183 : 0.901803685486664
Loss in iteration 184 : 0.8466915293694144
Loss in iteration 185 : 0.7913462397171581
Loss in iteration 186 : 0.7370244379119599
Loss in iteration 187 : 0.6848215545953809
Loss in iteration 188 : 0.6340441521791524
Loss in iteration 189 : 0.5856606189049036
Loss in iteration 190 : 0.5419763071379408
Loss in iteration 191 : 0.5043580371898415
Loss in iteration 192 : 0.47678860632797315
Loss in iteration 193 : 0.47222623882739134
Loss in iteration 194 : 0.7613246666089962
Loss in iteration 195 : 3.0755696939917363
Loss in iteration 196 : 2.0633014448011653
Loss in iteration 197 : 0.46665657511175734
Loss in iteration 198 : 0.6181726649553799
Loss in iteration 199 : 0.7822313283153086
Loss in iteration 200 : 0.8659904204406056
Testing accuracy  of updater 2 on alg 1 with rate 0.00112 = 0.75425, training accuracy 0.8209776626740045, time elapsed: 3256 millisecond.
Loss in iteration 1 : 1.000093037487706
Loss in iteration 2 : 1.8161727058912918
Loss in iteration 3 : 1.8579631881779424
Loss in iteration 4 : 1.5891225591428197
Loss in iteration 5 : 1.0412376287012952
Loss in iteration 6 : 0.49279558174870886
Loss in iteration 7 : 0.5646544893358975
Loss in iteration 8 : 0.6650861617416978
Loss in iteration 9 : 0.6435012639438723
Loss in iteration 10 : 0.5914997969578374
Loss in iteration 11 : 0.5664689704385042
Loss in iteration 12 : 0.5711153393907626
Loss in iteration 13 : 0.5829968549842501
Loss in iteration 14 : 0.5910919257149627
Loss in iteration 15 : 0.591828317133693
Loss in iteration 16 : 0.5880188733960195
Loss in iteration 17 : 0.5828842887301595
Loss in iteration 18 : 0.5779987371670544
Loss in iteration 19 : 0.5730770515721677
Loss in iteration 20 : 0.5672346608256612
Loss in iteration 21 : 0.5597247079777056
Loss in iteration 22 : 0.5506838460395767
Loss in iteration 23 : 0.5410384179493332
Loss in iteration 24 : 0.5311290835312282
Loss in iteration 25 : 0.5207775064986708
Loss in iteration 26 : 0.5100171855215309
Loss in iteration 27 : 0.49893026629674847
Loss in iteration 28 : 0.48770522168097935
Loss in iteration 29 : 0.4764813521479164
Loss in iteration 30 : 0.4652245231649184
Loss in iteration 31 : 0.4540175716589018
Loss in iteration 32 : 0.442900171938045
Loss in iteration 33 : 0.43226530406824026
Loss in iteration 34 : 0.4223411938585126
Loss in iteration 35 : 0.4131889108935496
Loss in iteration 36 : 0.404753452552511
Loss in iteration 37 : 0.3972441063341163
Loss in iteration 38 : 0.390931318689545
Loss in iteration 39 : 0.38601370118697687
Loss in iteration 40 : 0.3824693329723791
Loss in iteration 41 : 0.38016633579717213
Loss in iteration 42 : 0.37959294369061
Loss in iteration 43 : 0.38016882377771527
Loss in iteration 44 : 0.38170712249775457
Loss in iteration 45 : 0.3837102218309951
Loss in iteration 46 : 0.38557028959435213
Loss in iteration 47 : 0.3866456321114205
Testing accuracy  of updater 2 on alg 1 with rate 2.8E-4 = 0.789, training accuracy 0.8429912593072192, time elapsed: 892 millisecond.
Loss in iteration 1 : 1.0000455883689758
Loss in iteration 2 : 1.3770281587905844
Loss in iteration 3 : 1.4062582349882977
Loss in iteration 4 : 1.2180578493887049
Loss in iteration 5 : 0.8349386319311094
Loss in iteration 6 : 0.4301687743813476
Loss in iteration 7 : 0.4859034544093462
Loss in iteration 8 : 0.5458026792028923
Loss in iteration 9 : 0.5231572895513427
Loss in iteration 10 : 0.4879487949693522
Loss in iteration 11 : 0.47384206282174934
Loss in iteration 12 : 0.47790707455590714
Loss in iteration 13 : 0.48667907677585465
Loss in iteration 14 : 0.49172336443430475
Loss in iteration 15 : 0.4922912181794312
Loss in iteration 16 : 0.4903565074499162
Loss in iteration 17 : 0.4879593806203178
Loss in iteration 18 : 0.48571708987081574
Loss in iteration 19 : 0.4832229103083733
Loss in iteration 20 : 0.48017728380520014
Loss in iteration 21 : 0.47604312219405526
Loss in iteration 22 : 0.47116180520404943
Loss in iteration 23 : 0.4658647052717438
Loss in iteration 24 : 0.4603495873400965
Loss in iteration 25 : 0.45458327087096817
Loss in iteration 26 : 0.4485016573398339
Loss in iteration 27 : 0.44222348261636685
Loss in iteration 28 : 0.4358294026120958
Loss in iteration 29 : 0.42932273363515466
Loss in iteration 30 : 0.42275408277494736
Loss in iteration 31 : 0.416247357381864
Loss in iteration 32 : 0.4099365781294041
Loss in iteration 33 : 0.4039373029886957
Loss in iteration 34 : 0.398570659433466
Loss in iteration 35 : 0.39369369364690443
Loss in iteration 36 : 0.38931855532425724
Loss in iteration 37 : 0.38552550390940404
Loss in iteration 38 : 0.38241099706488024
Loss in iteration 39 : 0.3800231598203745
Loss in iteration 40 : 0.3782165538533955
Loss in iteration 41 : 0.3770504518865855
Loss in iteration 42 : 0.37636936916104513
Loss in iteration 43 : 0.3763208903407007
Loss in iteration 44 : 0.3766374241667888
Loss in iteration 45 : 0.37723950074487006
Loss in iteration 46 : 0.3779776044594274
Loss in iteration 47 : 0.37869234217837955
Testing accuracy  of updater 2 on alg 1 with rate 1.96E-4 = 0.78925, training accuracy 0.838459048235675, time elapsed: 853 millisecond.
Loss in iteration 1 : 1.000014885998033
Loss in iteration 2 : 0.9379308544224217
Loss in iteration 3 : 0.9546204599655316
Loss in iteration 4 : 0.8470705527641507
Loss in iteration 5 : 0.6300268178186937
Loss in iteration 6 : 0.3996545761973756
Loss in iteration 7 : 0.43409371742164016
Loss in iteration 8 : 0.4536777338253164
Loss in iteration 9 : 0.4267953017933177
Loss in iteration 10 : 0.4048000862561258
Loss in iteration 11 : 0.4000094813217445
Loss in iteration 12 : 0.4038009902435823
Loss in iteration 13 : 0.4083433196075959
Loss in iteration 14 : 0.4101981947807138
Loss in iteration 15 : 0.4104524659017033
Loss in iteration 16 : 0.4107060042629945
Loss in iteration 17 : 0.4112609096561931
Loss in iteration 18 : 0.41166463102671297
Testing accuracy  of updater 2 on alg 1 with rate 1.1199999999999998E-4 = 0.779, training accuracy 0.8297183554548397, time elapsed: 356 millisecond.
Loss in iteration 1 : 1.0000009303748771
Loss in iteration 2 : 0.5574532090785335
Loss in iteration 3 : 0.5782067974887507
Loss in iteration 4 : 0.5887472865073675
Loss in iteration 5 : 0.573535132316424
Loss in iteration 6 : 0.5407225896876232
Loss in iteration 7 : 0.5082620669456319
Loss in iteration 8 : 0.48789185369019117
Loss in iteration 9 : 0.4766710753536235
Loss in iteration 10 : 0.46740709772033395
Loss in iteration 11 : 0.45704744387919644
Loss in iteration 12 : 0.44574739834344174
Loss in iteration 13 : 0.4342590300707417
Loss in iteration 14 : 0.4237405832037109
Loss in iteration 15 : 0.41483974453900996
Loss in iteration 16 : 0.4075843482454374
Loss in iteration 17 : 0.4018828910391055
Loss in iteration 18 : 0.39769765032742
Loss in iteration 19 : 0.3946260464388614
Loss in iteration 20 : 0.39214660700109477
Loss in iteration 21 : 0.39016458049581026
Loss in iteration 22 : 0.38868197665069565
Loss in iteration 23 : 0.3875429398044883
Loss in iteration 24 : 0.3866915620805973
Testing accuracy  of updater 2 on alg 1 with rate 2.7999999999999976E-5 = 0.77875, training accuracy 0.8319844609906119, time elapsed: 329 millisecond.
Loss in iteration 1 : 2.536637061158183
Loss in iteration 2 : 90.81036172480268
Loss in iteration 3 : 21.522153940442294
Loss in iteration 4 : 24.37175503389432
Loss in iteration 5 : 49.800419714468255
Loss in iteration 6 : 9.357131660519379
Loss in iteration 7 : 24.881590443130232
Loss in iteration 8 : 45.14317705630682
Loss in iteration 9 : 11.659736652885757
Loss in iteration 10 : 8.63338448844209
Loss in iteration 11 : 24.096233380265687
Loss in iteration 12 : 1.3480766728971658
Loss in iteration 13 : 7.225754731368823
Loss in iteration 14 : 36.60444055053442
Loss in iteration 15 : 9.606022549897887
Loss in iteration 16 : 9.382642169739047
Loss in iteration 17 : 28.680924020204348
Loss in iteration 18 : 5.963433061388024
Loss in iteration 19 : 14.322348720343173
Loss in iteration 20 : 32.01090242228144
Loss in iteration 21 : 9.152215301395444
Loss in iteration 22 : 4.117249575857563
Loss in iteration 23 : 13.723357786283017
Loss in iteration 24 : 1.1401058384332738
Loss in iteration 25 : 6.746351461039409
Loss in iteration 26 : 16.668819543591574
Loss in iteration 27 : 30.62965845759243
Loss in iteration 28 : 9.564844652037854
Loss in iteration 29 : 2.391490816936605
Loss in iteration 30 : 9.176390116537943
Loss in iteration 31 : 4.792304292892761
Loss in iteration 32 : 21.198968550733955
Loss in iteration 33 : 4.4009482091899494
Loss in iteration 34 : 12.53407814561806
Loss in iteration 35 : 26.314744911384196
Loss in iteration 36 : 8.05138679685878
Loss in iteration 37 : 2.317091151479384
Loss in iteration 38 : 8.208479775181392
Loss in iteration 39 : 3.918351130468853
Loss in iteration 40 : 17.58559765978488
Loss in iteration 41 : 3.1184955421347174
Loss in iteration 42 : 14.038904504026151
Loss in iteration 43 : 23.725664672328726
Loss in iteration 44 : 7.245945022290271
Loss in iteration 45 : 2.1841332268230995
Loss in iteration 46 : 7.689432081433391
Loss in iteration 47 : 3.2149524741038715
Loss in iteration 48 : 15.053118130663917
Loss in iteration 49 : 2.242070688228957
Loss in iteration 50 : 14.960215393635421
Loss in iteration 51 : 21.713599556275394
Loss in iteration 52 : 6.595127646297107
Loss in iteration 53 : 2.144287854475506
Loss in iteration 54 : 7.440522258507461
Loss in iteration 55 : 2.5072906071535694
Loss in iteration 56 : 12.259515505938792
Loss in iteration 57 : 1.0837244057321966
Loss in iteration 58 : 13.44938693603528
Loss in iteration 59 : 20.751262394318115
Loss in iteration 60 : 6.441741170242317
Loss in iteration 61 : 1.8876125690781926
Loss in iteration 62 : 6.88569084396667
Loss in iteration 63 : 2.623830183509427
Loss in iteration 64 : 12.666505700590319
Loss in iteration 65 : 1.698369096370357
Loss in iteration 66 : 13.90971668969956
Loss in iteration 67 : 19.138028812873284
Loss in iteration 68 : 5.835065020816912
Loss in iteration 69 : 1.9494446951879703
Loss in iteration 70 : 6.748167833378319
Loss in iteration 71 : 2.066532102128562
Loss in iteration 72 : 10.391677224007156
Loss in iteration 73 : 0.746195763387059
Loss in iteration 74 : 9.86794392438176
Loss in iteration 75 : 19.066889076951522
Loss in iteration 76 : 6.085306549138044
Loss in iteration 77 : 1.5187663361283885
Loss in iteration 78 : 5.710065714492453
Loss in iteration 79 : 3.398416836015931
Loss in iteration 80 : 14.623179360920908
Loss in iteration 81 : 3.533068794274159
Loss in iteration 82 : 6.174396097699354
Loss in iteration 83 : 16.552390055056875
Loss in iteration 84 : 4.840257548034316
Loss in iteration 85 : 2.4096407137514184
Loss in iteration 86 : 8.113052881245618
Loss in iteration 87 : 0.5076014947002715
Loss in iteration 88 : 0.9965243208472035
Loss in iteration 89 : 16.16088325648055
Loss in iteration 90 : 16.48292431155098
Loss in iteration 91 : 4.9906084272094615
Loss in iteration 92 : 1.9603424782245518
Loss in iteration 93 : 6.842041737841461
Loss in iteration 94 : 0.9097375543415955
Loss in iteration 95 : 5.283487282847613
Loss in iteration 96 : 4.222243662003404
Loss in iteration 97 : 17.03203274576411
Loss in iteration 98 : 5.497668640649973
Loss in iteration 99 : 1.3668574121961012
Loss in iteration 100 : 5.273482085470256
Loss in iteration 101 : 2.7809934459206316
Loss in iteration 102 : 12.539342171684618
Loss in iteration 103 : 2.867342761612524
Loss in iteration 104 : 6.6399266777231505
Loss in iteration 105 : 15.794134014318324
Loss in iteration 106 : 4.949348872480092
Loss in iteration 107 : 1.4778304625609504
Loss in iteration 108 : 5.155005525520123
Loss in iteration 109 : 2.3434617609855026
Loss in iteration 110 : 10.628234126156975
Loss in iteration 111 : 1.889956605684798
Loss in iteration 112 : 9.29759972309908
Loss in iteration 113 : 15.122149966927223
Loss in iteration 114 : 4.715102426992821
Loss in iteration 115 : 1.4859366959593052
Loss in iteration 116 : 5.119039809506366
Loss in iteration 117 : 2.0292711814966573
Loss in iteration 118 : 9.497572066440396
Loss in iteration 119 : 1.3650656550215292
Loss in iteration 120 : 10.469956327544772
Loss in iteration 121 : 14.485408669899932
Loss in iteration 122 : 4.480711934262534
Loss in iteration 123 : 1.5365308975072052
Loss in iteration 124 : 5.211014905925817
Loss in iteration 125 : 1.6089722459982776
Loss in iteration 126 : 7.89165735428533
Loss in iteration 127 : 0.6022113728140753
Loss in iteration 128 : 7.550564993584572
Loss in iteration 129 : 14.758018505129755
Loss in iteration 130 : 4.774629693276519
Loss in iteration 131 : 1.2256908180843393
Loss in iteration 132 : 4.560168364248516
Loss in iteration 133 : 2.5572770217670833
Loss in iteration 134 : 11.150701377972569
Loss in iteration 135 : 2.649377153034424
Loss in iteration 136 : 5.473317532237575
Loss in iteration 137 : 13.697015590752152
Loss in iteration 138 : 4.264856316448475
Loss in iteration 139 : 1.4615496383232454
Loss in iteration 140 : 4.982200801304162
Loss in iteration 141 : 1.4784751725555623
Loss in iteration 142 : 7.340924447581619
Loss in iteration 143 : 0.5366384474403622
Loss in iteration 144 : 6.245216418152119
Loss in iteration 145 : 14.118838892739143
Loss in iteration 146 : 4.62751636926194
Loss in iteration 147 : 1.1079767613278881
Loss in iteration 148 : 4.140173635130034
Loss in iteration 149 : 2.877041717754075
Loss in iteration 150 : 11.658810120095733
Loss in iteration 151 : 3.1943889840285156
Loss in iteration 152 : 3.2219872987528024
Loss in iteration 153 : 10.018489613412493
Loss in iteration 154 : 2.2389435787465617
Loss in iteration 155 : 5.387001468127639
Loss in iteration 156 : 12.728735997937505
Loss in iteration 157 : 3.941472725359973
Loss in iteration 158 : 1.4481383245919324
Loss in iteration 159 : 4.864877963450924
Loss in iteration 160 : 1.1834864362088473
Loss in iteration 161 : 6.0243715787745185
Loss in iteration 162 : 0.4516407957416168
Loss in iteration 163 : 1.8874687789600573
Loss in iteration 164 : 9.66887580532757
Loss in iteration 165 : 12.771675694678475
Loss in iteration 166 : 4.057538999803611
Loss in iteration 167 : 1.3191888372036633
Loss in iteration 168 : 4.678313422410119
Loss in iteration 169 : 1.3198027006486421
Loss in iteration 170 : 6.681835946156615
Loss in iteration 171 : 0.4857669573592406
Loss in iteration 172 : 5.232091982705975
Loss in iteration 173 : 13.109612460874482
Loss in iteration 174 : 4.344478950459338
Loss in iteration 175 : 1.0034591606045953
Loss in iteration 176 : 3.7629991546961112
Loss in iteration 177 : 2.8711529280241916
Loss in iteration 178 : 11.16966327992921
Loss in iteration 179 : 3.2154949148567717
Loss in iteration 180 : 2.4443457813195884
Loss in iteration 181 : 8.006847970787629
Loss in iteration 182 : 1.3210589684293494
Loss in iteration 183 : 7.530299887539998
Loss in iteration 184 : 11.934804127937584
Loss in iteration 185 : 3.7532796611690724
Loss in iteration 186 : 1.2653321309165866
Loss in iteration 187 : 4.209772486756914
Loss in iteration 188 : 1.5391678100236619
Loss in iteration 189 : 7.09349863485633
Loss in iteration 190 : 0.8478028335896786
Loss in iteration 191 : 8.818169961973455
Loss in iteration 192 : 11.658444066114429
Loss in iteration 193 : 3.6595697344698555
Loss in iteration 194 : 1.305215138709808
Loss in iteration 195 : 4.394292240947814
Loss in iteration 196 : 1.1780235512287869
Loss in iteration 197 : 5.825244900219585
Loss in iteration 198 : 0.39413088080898545
Loss in iteration 199 : 0.38026973947755965
Loss in iteration 200 : 0.40605792366894133
Testing accuracy  of updater 3 on alg 1 with rate 1.96 = 0.64075, training accuracy 0.5302686953706701, time elapsed: 3062 millisecond.
Loss in iteration 1 : 1.0104509077981318
Loss in iteration 2 : 6.990888478672557
Loss in iteration 3 : 2.7047470132313
Loss in iteration 4 : 0.5661975900520303
Loss in iteration 5 : 1.2198289536557705
Loss in iteration 6 : 3.7132524568558813
Loss in iteration 7 : 7.792171950490889
Loss in iteration 8 : 3.31676507875392
Loss in iteration 9 : 0.48666770020013633
Loss in iteration 10 : 0.4744461547107865
Loss in iteration 11 : 1.1705625624108684
Loss in iteration 12 : 4.575090784528186
Loss in iteration 13 : 7.875146452787309
Loss in iteration 14 : 3.4071641999845284
Loss in iteration 15 : 0.4983587705135218
Loss in iteration 16 : 0.5899595437506303
Loss in iteration 17 : 1.862162252562512
Loss in iteration 18 : 2.3343916175260957
Loss in iteration 19 : 6.977692261429347
Loss in iteration 20 : 2.7817093982905057
Loss in iteration 21 : 0.5047346369169323
Loss in iteration 22 : 1.0012246778994565
Loss in iteration 23 : 4.371555640038602
Loss in iteration 24 : 7.616155319358887
Loss in iteration 25 : 3.2727069046572073
Loss in iteration 26 : 0.4901346627143695
Loss in iteration 27 : 0.5131927404737947
Loss in iteration 28 : 1.4376932019673385
Loss in iteration 29 : 3.4332259821421687
Loss in iteration 30 : 7.694083895341497
Loss in iteration 31 : 3.3564358162341437
Loss in iteration 32 : 0.49770880832681175
Loss in iteration 33 : 0.6647924740624381
Loss in iteration 34 : 2.2011225206462397
Loss in iteration 35 : 1.3369813935837118
Loss in iteration 36 : 4.8733641790371784
Loss in iteration 37 : 1.3276460109480792
Loss in iteration 38 : 2.7706889192275392
Loss in iteration 39 : 6.536514616240292
Loss in iteration 40 : 2.552934352029532
Loss in iteration 41 : 0.5400397418845284
Loss in iteration 42 : 1.1130041335150012
Loss in iteration 43 : 3.5930435078998313
Loss in iteration 44 : 7.270352868872917
Loss in iteration 45 : 3.1095897908468264
Loss in iteration 46 : 0.47072324324205206
Loss in iteration 47 : 0.46187991461625894
Loss in iteration 48 : 1.1125562927147463
Loss in iteration 49 : 4.2632231520069235
Loss in iteration 50 : 7.346665679432931
Loss in iteration 51 : 3.189178153715251
Loss in iteration 52 : 0.47888822038041623
Loss in iteration 53 : 0.5710004823150631
Loss in iteration 54 : 1.774121281999251
Loss in iteration 55 : 2.1609289119290827
Loss in iteration 56 : 6.481839965178684
Loss in iteration 57 : 2.5823572474534853
Loss in iteration 58 : 0.504597708318737
Loss in iteration 59 : 1.0680849808110475
Loss in iteration 60 : 3.7803234304079636
Loss in iteration 61 : 7.144985506553157
Loss in iteration 62 : 3.0859471312427003
Loss in iteration 63 : 0.47190441877656664
Loss in iteration 64 : 0.507335678299549
Loss in iteration 65 : 1.4381482889043782
Loss in iteration 66 : 3.02775314749282
Loss in iteration 67 : 7.1772522924449955
Loss in iteration 68 : 3.131367850343931
Loss in iteration 69 : 0.47149253321964724
Loss in iteration 70 : 0.5924170042484697
Loss in iteration 71 : 1.9349557245216589
Loss in iteration 72 : 1.6169366267825989
Loss in iteration 73 : 5.4694225707565405
Loss in iteration 74 : 1.908884939730261
Loss in iteration 75 : 1.0881674727102628
Loss in iteration 76 : 3.17728334168657
Loss in iteration 77 : 0.4017385052772386
Loss in iteration 78 : 0.7866569631472616
Loss in iteration 79 : 3.461228241779064
Loss in iteration 80 : 0.4861434504514246
Loss in iteration 81 : 4.000108107174232
Loss in iteration 82 : 7.057505293728695
Loss in iteration 83 : 3.0909199303834165
Loss in iteration 84 : 0.4718267704642494
Loss in iteration 85 : 0.6284747546000435
Loss in iteration 86 : 2.037662567332348
Loss in iteration 87 : 1.2851210996991667
Loss in iteration 88 : 4.596583521391995
Loss in iteration 89 : 1.3193129922766953
Loss in iteration 90 : 2.3430290872485595
Loss in iteration 91 : 5.729608491425421
Loss in iteration 92 : 2.1547989382984967
Loss in iteration 93 : 0.658652736955264
Loss in iteration 94 : 1.7605548018390638
Loss in iteration 95 : 1.5251628450499854
Loss in iteration 96 : 4.629210479034418
Loss in iteration 97 : 1.369508346609437
Loss in iteration 98 : 1.9807201655730453
Loss in iteration 99 : 4.9701366683729615
Loss in iteration 100 : 1.6280074850793516
Loss in iteration 101 : 1.2632724564501128
Loss in iteration 102 : 3.2876443643734055
Loss in iteration 103 : 0.480475106588624
Loss in iteration 104 : 1.9945752367208198
Loss in iteration 105 : 5.813702275236216
Loss in iteration 106 : 2.2585780922003487
Loss in iteration 107 : 0.5766379306208519
Loss in iteration 108 : 1.5066012065618473
Loss in iteration 109 : 2.116093360334016
Loss in iteration 110 : 5.761067045514495
Loss in iteration 111 : 2.2355461696244032
Loss in iteration 112 : 0.573115103246313
Loss in iteration 113 : 1.4597631471672934
Loss in iteration 114 : 2.1634220145292566
Loss in iteration 115 : 5.7430164056803905
Loss in iteration 116 : 2.237317194791395
Loss in iteration 117 : 0.5594732205002816
Loss in iteration 118 : 1.388826117316044
Loss in iteration 119 : 2.303973760294052
Loss in iteration 120 : 5.890818460294569
Loss in iteration 121 : 2.3595839002265944
Loss in iteration 122 : 0.48671826995314416
Loss in iteration 123 : 0.9294676131604238
Loss in iteration 124 : 3.5676798212445897
Loss in iteration 125 : 6.498964567696613
Loss in iteration 126 : 2.8175161033053073
Loss in iteration 127 : 0.4517723751138608
Loss in iteration 128 : 0.46847841640556903
Loss in iteration 129 : 1.2152291162061715
Loss in iteration 130 : 3.1624865106641216
Loss in iteration 131 : 6.597546999886339
Loss in iteration 132 : 2.9056937516115036
Loss in iteration 133 : 0.4619759051530392
Loss in iteration 134 : 0.6445329388797086
Loss in iteration 135 : 2.0103159227755905
Loss in iteration 136 : 1.0419974177916183
Loss in iteration 137 : 3.7143182062281497
Loss in iteration 138 : 0.824229798525515
Loss in iteration 139 : 3.6902838598671637
Loss in iteration 140 : 6.344751765385808
Loss in iteration 141 : 2.7460755566268458
Loss in iteration 142 : 0.4463103183478643
Loss in iteration 143 : 0.45094732163879625
Loss in iteration 144 : 1.080264940639283
Loss in iteration 145 : 3.468163738414573
Loss in iteration 146 : 6.443401167705884
Loss in iteration 147 : 2.8333214292230013
Loss in iteration 148 : 0.45452734841265185
Loss in iteration 149 : 0.589211813896414
Loss in iteration 150 : 1.8215031569211395
Loss in iteration 151 : 1.3290040131018255
Loss in iteration 152 : 4.47271154237111
Loss in iteration 153 : 1.4145166146116148
Loss in iteration 154 : 1.6541436444883237
Loss in iteration 155 : 4.300573484184248
Loss in iteration 156 : 1.2977314859396634
Loss in iteration 157 : 1.7026056612603335
Loss in iteration 158 : 4.14794885495939
Loss in iteration 159 : 1.1948081566764364
Loss in iteration 160 : 1.8604663696902002
Loss in iteration 161 : 4.326506487457534
Loss in iteration 162 : 1.3324906880799288
Loss in iteration 163 : 1.5005596723722165
Loss in iteration 164 : 3.5695581132342546
Loss in iteration 165 : 0.7922652467837057
Loss in iteration 166 : 2.8186917888431497
Loss in iteration 167 : 5.622044572804943
Loss in iteration 168 : 2.289856669999892
Loss in iteration 169 : 0.45406449597877335
Loss in iteration 170 : 0.612233876219515
Loss in iteration 171 : 3.4293481152007828
Loss in iteration 172 : 6.105473598877226
Loss in iteration 173 : 2.654031942411973
Loss in iteration 174 : 0.43916495053028415
Loss in iteration 175 : 0.45251491587003967
Loss in iteration 176 : 1.1224538685564085
Loss in iteration 177 : 3.1129885784118985
Loss in iteration 178 : 6.215323306127889
Loss in iteration 179 : 2.7475459109344573
Loss in iteration 180 : 0.45010746871978774
Loss in iteration 181 : 0.6177018988534199
Loss in iteration 182 : 1.8896395924140534
Loss in iteration 183 : 1.038209789706914
Loss in iteration 184 : 3.581005177771218
Loss in iteration 185 : 0.8449834297673184
Loss in iteration 186 : 3.304899143883985
Loss in iteration 187 : 5.990818103135589
Loss in iteration 188 : 2.604480791586235
Loss in iteration 189 : 0.4352679871412344
Loss in iteration 190 : 0.44671917355987667
Loss in iteration 191 : 1.0963414389873773
Loss in iteration 192 : 3.103043503130033
Loss in iteration 193 : 6.108800406127163
Loss in iteration 194 : 2.7031407642248246
Loss in iteration 195 : 0.4465928710948578
Loss in iteration 196 : 0.6086129441224083
Loss in iteration 197 : 1.8428139380929611
Loss in iteration 198 : 1.0624905193945862
Loss in iteration 199 : 3.6332730901322123
Loss in iteration 200 : 0.9153883355779067
Testing accuracy  of updater 3 on alg 1 with rate 1.372 = 0.51525, training accuracy 0.3761735189381677, time elapsed: 2754 millisecond.
Loss in iteration 1 : 1.0018437311820416
Loss in iteration 2 : 3.1398447170505333
Loss in iteration 3 : 1.667188513542715
Loss in iteration 4 : 0.46471429380486223
Loss in iteration 5 : 0.7467761884842149
Loss in iteration 6 : 1.3748386591513666
Loss in iteration 7 : 0.377598655149943
Loss in iteration 8 : 0.39163965210032653
Loss in iteration 9 : 0.6054620912549228
Loss in iteration 10 : 1.4301623010766606
Loss in iteration 11 : 0.380122260037534
Loss in iteration 12 : 0.39826367238745913
Loss in iteration 13 : 0.7030727847888962
Loss in iteration 14 : 1.808734737711197
Loss in iteration 15 : 0.5551027564293628
Loss in iteration 16 : 1.4499813549592324
Loss in iteration 17 : 2.625044756579803
Loss in iteration 18 : 1.2439228129235693
Loss in iteration 19 : 0.3953849929418859
Loss in iteration 20 : 0.4322513908077937
Loss in iteration 21 : 0.7745848029630055
Loss in iteration 22 : 1.5793868251415095
Loss in iteration 23 : 0.413452310884875
Loss in iteration 24 : 0.7153310410998426
Loss in iteration 25 : 1.5604607582361252
Loss in iteration 26 : 0.40423979839224605
Loss in iteration 27 : 0.6738215645692813
Loss in iteration 28 : 1.5226220599756455
Loss in iteration 29 : 0.3910049034516501
Loss in iteration 30 : 0.570259497063678
Loss in iteration 31 : 1.3254619561592023
Loss in iteration 32 : 0.4036693086861121
Loss in iteration 33 : 0.6810419601861872
Loss in iteration 34 : 1.4817961681683105
Loss in iteration 35 : 2.8481340978383365
Loss in iteration 36 : 1.444101602794391
Loss in iteration 37 : 0.3948791759761639
Loss in iteration 38 : 0.42641664715330896
Loss in iteration 39 : 0.6752469968542533
Loss in iteration 40 : 1.1679906843907675
Loss in iteration 41 : 2.1805435039563204
Loss in iteration 42 : 0.8854425989835657
Loss in iteration 43 : 0.6409515256500515
Loss in iteration 44 : 1.1116416352962075
Loss in iteration 45 : 0.47710087109192384
Loss in iteration 46 : 0.8869985743996129
Loss in iteration 47 : 0.7958275754836808
Loss in iteration 48 : 1.6532115799272342
Loss in iteration 49 : 0.46523556819906764
Loss in iteration 50 : 1.046611528206355
Loss in iteration 51 : 1.9949331911979424
Loss in iteration 52 : 0.736613862072193
Loss in iteration 53 : 0.8474850344194997
Loss in iteration 54 : 1.4777987930515342
Loss in iteration 55 : 0.39246309636467663
Loss in iteration 56 : 0.49185928195392015
Loss in iteration 57 : 0.9906626154055681
Loss in iteration 58 : 0.6722862177247435
Loss in iteration 59 : 1.4735696620150291
Loss in iteration 60 : 0.3851289359342652
Loss in iteration 61 : 0.48976286308915395
Loss in iteration 62 : 1.0822873999135325
Loss in iteration 63 : 0.6046859480754422
Loss in iteration 64 : 1.391645625606274
Loss in iteration 65 : 0.3807348842387816
Loss in iteration 66 : 0.3972395126107259
Loss in iteration 67 : 0.6578136709361797
Loss in iteration 68 : 1.6601822336342817
Loss in iteration 69 : 0.475352101810206
Loss in iteration 70 : 1.3169973385224056
Loss in iteration 71 : 2.499498700704362
Loss in iteration 72 : 1.1734531573924918
Loss in iteration 73 : 0.4063898951442944
Loss in iteration 74 : 0.5055125341913079
Loss in iteration 75 : 1.0914246901486877
Loss in iteration 76 : 1.9821576408284314
Loss in iteration 77 : 0.7412536672950192
Loss in iteration 78 : 0.8009291710267176
Loss in iteration 79 : 1.3698974487433886
Loss in iteration 80 : 0.3769522658839457
Loss in iteration 81 : 0.38204984124102054
Loss in iteration 82 : 0.44510978520885014
Loss in iteration 83 : 1.2238906454427885
Loss in iteration 84 : 2.5035057269414915
Loss in iteration 85 : 1.1855603804991586
Loss in iteration 86 : 0.4017205756391129
Loss in iteration 87 : 0.5046957562881478
Loss in iteration 88 : 1.137603136118499
Loss in iteration 89 : 2.0582519264004158
Loss in iteration 90 : 0.8137851744167259
Loss in iteration 91 : 0.6959637971060578
Loss in iteration 92 : 1.1941713354361136
Loss in iteration 93 : 0.4075112597822505
Loss in iteration 94 : 0.6096455470936009
Loss in iteration 95 : 1.2796364397539322
Loss in iteration 96 : 2.3275423581078933
Loss in iteration 97 : 1.0458420493292004
Loss in iteration 98 : 0.4592609428547271
Loss in iteration 99 : 0.695999852336606
Loss in iteration 100 : 0.9519595120734271
Loss in iteration 101 : 1.682046216936211
Loss in iteration 102 : 0.5167723392881798
Loss in iteration 103 : 0.9627509680080418
Loss in iteration 104 : 1.6663480506197774
Loss in iteration 105 : 0.507952079078286
Loss in iteration 106 : 0.9293613043830832
Loss in iteration 107 : 1.6186641106039434
Loss in iteration 108 : 0.4769655716766281
Loss in iteration 109 : 0.8996163366733707
Loss in iteration 110 : 1.6076132680901782
Loss in iteration 111 : 0.4702266795315027
Loss in iteration 112 : 0.9061664211916541
Loss in iteration 113 : 1.6467299966436404
Loss in iteration 114 : 0.4978487332504875
Loss in iteration 115 : 0.959904290641724
Loss in iteration 116 : 1.6828495171486566
Loss in iteration 117 : 0.5261703947969627
Loss in iteration 118 : 0.9501156772080108
Loss in iteration 119 : 1.6306923303667886
Loss in iteration 120 : 0.49155437441459776
Loss in iteration 121 : 0.9011536806353302
Loss in iteration 122 : 1.5879171104573722
Loss in iteration 123 : 0.4641189353013232
Loss in iteration 124 : 0.8607848603372392
Loss in iteration 125 : 1.5768066170012804
Loss in iteration 126 : 0.45721168575034765
Loss in iteration 127 : 0.8849534333006234
Loss in iteration 128 : 1.6394148448579722
Loss in iteration 129 : 0.5002027911991188
Loss in iteration 130 : 0.9698005576431261
Loss in iteration 131 : 1.7015909483133207
Loss in iteration 132 : 0.5487457058465051
Loss in iteration 133 : 0.9488387293008818
Loss in iteration 134 : 1.6100040012041383
Loss in iteration 135 : 0.48555246554789366
Loss in iteration 136 : 0.878697403401242
Loss in iteration 137 : 1.5493937621431881
Loss in iteration 138 : 0.447316516807403
Loss in iteration 139 : 0.8071916338685552
Loss in iteration 140 : 1.5136074033431755
Loss in iteration 141 : 0.4264391889866168
Loss in iteration 142 : 0.7463695225100777
Loss in iteration 143 : 1.4804616924542005
Loss in iteration 144 : 0.4095797508738797
Loss in iteration 145 : 0.6714926993188945
Loss in iteration 146 : 1.407168926575848
Loss in iteration 147 : 0.3849885531678582
Loss in iteration 148 : 0.47952529516169057
Loss in iteration 149 : 0.9983799180568118
Loss in iteration 150 : 0.6421578913004803
Loss in iteration 151 : 1.4578375568537059
Loss in iteration 152 : 0.399880720520394
Loss in iteration 153 : 0.6632210939103876
Loss in iteration 154 : 1.492608611825393
Loss in iteration 155 : 0.415276954162956
Loss in iteration 156 : 0.8150647491040847
Loss in iteration 157 : 1.7227107584254784
Loss in iteration 158 : 0.5755357830734958
Loss in iteration 159 : 1.1236011522020326
Loss in iteration 160 : 1.957754981428854
Loss in iteration 161 : 0.7734842602696671
Loss in iteration 162 : 0.6916917703770133
Loss in iteration 163 : 1.158089251931104
Loss in iteration 164 : 0.40323208551495665
Loss in iteration 165 : 0.5815970852599097
Loss in iteration 166 : 1.2342838463153318
Loss in iteration 167 : 2.2203371273875856
Loss in iteration 168 : 0.9989985271752386
Loss in iteration 169 : 0.46430580576469355
Loss in iteration 170 : 0.7054370876799773
Loss in iteration 171 : 0.8840514146093558
Loss in iteration 172 : 1.5680451566726092
Loss in iteration 173 : 0.4717200400562719
Loss in iteration 174 : 0.8996736610505996
Loss in iteration 175 : 1.6019324347598942
Loss in iteration 176 : 0.49626982963859395
Loss in iteration 177 : 0.9313809976953746
Loss in iteration 178 : 1.6105634254383172
Loss in iteration 179 : 0.5044260001197186
Loss in iteration 180 : 0.9082014863601421
Loss in iteration 181 : 1.5654826802205375
Loss in iteration 182 : 0.47501139927420516
Loss in iteration 183 : 0.8815007239454806
Loss in iteration 184 : 1.547652971125346
Loss in iteration 185 : 0.4640330845625148
Loss in iteration 186 : 0.8640398111239106
Loss in iteration 187 : 1.5517661732277996
Loss in iteration 188 : 0.467310949331076
Loss in iteration 189 : 0.8803808909379708
Loss in iteration 190 : 1.584235296230813
Loss in iteration 191 : 0.49065191389452156
Loss in iteration 192 : 0.9309330057676937
Loss in iteration 193 : 1.6147001221558073
Loss in iteration 194 : 0.5144488516509159
Loss in iteration 195 : 0.9101863410108401
Loss in iteration 196 : 1.5623323648309109
Loss in iteration 197 : 0.4796984210753129
Loss in iteration 198 : 0.8769136215094558
Loss in iteration 199 : 1.5298650541379057
Loss in iteration 200 : 0.45919915373606085
Testing accuracy  of updater 3 on alg 1 with rate 0.784 = 0.71975, training accuracy 0.6633214632567174, time elapsed: 2875 millisecond.
Loss in iteration 1 : 1.0000936916461869
Loss in iteration 2 : 0.9725929230681525
Loss in iteration 3 : 0.7211866990970789
Loss in iteration 4 : 0.4993234720304099
Loss in iteration 5 : 0.46977612247773837
Loss in iteration 6 : 0.45505182366065233
Loss in iteration 7 : 0.4482014177772167
Loss in iteration 8 : 0.4408189896979389
Loss in iteration 9 : 0.43526401898753814
Loss in iteration 10 : 0.4296993947840273
Loss in iteration 11 : 0.42522054512221397
Loss in iteration 12 : 0.42119521583165914
Loss in iteration 13 : 0.417989219667018
Loss in iteration 14 : 0.4151105160849913
Loss in iteration 15 : 0.41270926879785974
Loss in iteration 16 : 0.4107470041172417
Loss in iteration 17 : 0.40918324775329457
Loss in iteration 18 : 0.40781714862961577
Loss in iteration 19 : 0.40679357359445134
Loss in iteration 20 : 0.40581059546178255
Loss in iteration 21 : 0.4050160428175502
Loss in iteration 22 : 0.4043420531858513
Loss in iteration 23 : 0.4037524267475648
Loss in iteration 24 : 0.4033255074189556
Loss in iteration 25 : 0.4028653353095953
Loss in iteration 26 : 0.4026075502076037
Loss in iteration 27 : 0.40227534698166334
Loss in iteration 28 : 0.40212458821201313
Loss in iteration 29 : 0.40182266850311876
Loss in iteration 30 : 0.4017101947326597
Loss in iteration 31 : 0.40149152844032066
Loss in iteration 32 : 0.4014228294403949
Loss in iteration 33 : 0.4013135675896151
Loss in iteration 34 : 0.40129020876762084
Loss in iteration 35 : 0.4011812357135597
Loss in iteration 36 : 0.4011943712537403
Testing accuracy  of updater 3 on alg 1 with rate 0.196 = 0.777, training accuracy 0.8355454839753965, time elapsed: 538 millisecond.
Loss in iteration 1 : 1.000045321600705
Loss in iteration 2 : 0.7847550981548492
Loss in iteration 3 : 0.6152905735978567
Loss in iteration 4 : 0.4979011839672434
Loss in iteration 5 : 0.4908648784399268
Loss in iteration 6 : 0.48438331787465533
Loss in iteration 7 : 0.47892302653887836
Loss in iteration 8 : 0.4735494703431206
Loss in iteration 9 : 0.46836877548891565
Loss in iteration 10 : 0.46335920349744486
Loss in iteration 11 : 0.4585240035075811
Loss in iteration 12 : 0.453813244409312
Loss in iteration 13 : 0.44934131867439814
Loss in iteration 14 : 0.4450155406889092
Loss in iteration 15 : 0.44079504436258365
Loss in iteration 16 : 0.436727322277004
Loss in iteration 17 : 0.43293747320489423
Loss in iteration 18 : 0.4294636101352576
Loss in iteration 19 : 0.42635026115944963
Loss in iteration 20 : 0.423517247127666
Loss in iteration 21 : 0.42095507787103426
Loss in iteration 22 : 0.41872533332616796
Loss in iteration 23 : 0.41669559500997466
Loss in iteration 24 : 0.41486295917274363
Loss in iteration 25 : 0.41327174887663504
Loss in iteration 26 : 0.41184505557944534
Loss in iteration 27 : 0.4106150233081202
Loss in iteration 28 : 0.40951737559298074
Loss in iteration 29 : 0.4085676996322609
Loss in iteration 30 : 0.4076910161854416
Loss in iteration 31 : 0.4069814376586171
Loss in iteration 32 : 0.40631711164504025
Loss in iteration 33 : 0.4057355235950781
Loss in iteration 34 : 0.4051965502083603
Loss in iteration 35 : 0.4047086968410881
Loss in iteration 36 : 0.40426825270222333
Loss in iteration 37 : 0.403894032747136
Loss in iteration 38 : 0.4035340439173676
Loss in iteration 39 : 0.4032654302808959
Loss in iteration 40 : 0.40297297476956884
Loss in iteration 41 : 0.4027913394939283
Loss in iteration 42 : 0.4025273194925215
Testing accuracy  of updater 3 on alg 1 with rate 0.13720000000000002 = 0.7775, training accuracy 0.8358692133376497, time elapsed: 591 millisecond.
Loss in iteration 1 : 1.0000146042423317
Loss in iteration 2 : 0.6004806993609829
Loss in iteration 3 : 0.5350356700920533
Loss in iteration 4 : 0.5260295862161054
Loss in iteration 5 : 0.5214985239359532
Loss in iteration 6 : 0.5173338663605579
Loss in iteration 7 : 0.5133057177031342
Loss in iteration 8 : 0.5094242803804014
Loss in iteration 9 : 0.5056602783900379
Loss in iteration 10 : 0.5019811301283712
Loss in iteration 11 : 0.4984165918080668
Loss in iteration 12 : 0.49497273390749846
Loss in iteration 13 : 0.4915937866086431
Loss in iteration 14 : 0.48829517133360373
Loss in iteration 15 : 0.48506463747518286
Loss in iteration 16 : 0.48190653276404144
Loss in iteration 17 : 0.4787992242938042
Loss in iteration 18 : 0.4757451152029054
Loss in iteration 19 : 0.47278087931902585
Loss in iteration 20 : 0.4698738700448616
Loss in iteration 21 : 0.46701373534002427
Loss in iteration 22 : 0.46420100395771857
Loss in iteration 23 : 0.4614460717556775
Loss in iteration 24 : 0.45873373658742994
Loss in iteration 25 : 0.45606764551007484
Loss in iteration 26 : 0.45347034814673165
Loss in iteration 27 : 0.45095929501218796
Loss in iteration 28 : 0.4484926747983627
Loss in iteration 29 : 0.44605157911848775
Loss in iteration 30 : 0.44365474861245885
Loss in iteration 31 : 0.4412970409830085
Loss in iteration 32 : 0.43898019591944937
Loss in iteration 33 : 0.43674787532048714
Loss in iteration 34 : 0.43460164357901976
Loss in iteration 35 : 0.432546885943351
Loss in iteration 36 : 0.43058401510930966
Loss in iteration 37 : 0.4287327662601448
Loss in iteration 38 : 0.4270080577941306
Loss in iteration 39 : 0.4253506503178846
Loss in iteration 40 : 0.4237661669648337
Loss in iteration 41 : 0.42228378735434835
Loss in iteration 42 : 0.4209180161672925
Loss in iteration 43 : 0.419650369767015
Loss in iteration 44 : 0.41845098368230044
Loss in iteration 45 : 0.41731918768179377
Loss in iteration 46 : 0.41625293313505196
Loss in iteration 47 : 0.4152483341611296
Loss in iteration 48 : 0.4143210959642132
Loss in iteration 49 : 0.41344350292474474
Loss in iteration 50 : 0.4126147757725868
Loss in iteration 51 : 0.41186252342121993
Loss in iteration 52 : 0.4111638660669737
Testing accuracy  of updater 3 on alg 1 with rate 0.07840000000000001 = 0.773, training accuracy 0.8348980252508903, time elapsed: 769 millisecond.
Loss in iteration 1 : 1.000000900059842
Loss in iteration 2 : 0.7216620395601606
Loss in iteration 3 : 0.5785007966709623
Loss in iteration 4 : 0.5556968642645724
Testing accuracy  of updater 3 on alg 1 with rate 0.019600000000000006 = 0.5, training accuracy 0.6474587245063127, time elapsed: 61 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.9478710272746188
Loss in iteration 3 : 76.22222007029666
Loss in iteration 4 : 2459221.1604211377
Loss in iteration 5 : 9.736560924754233E10
Loss in iteration 6 : 3.855754359440113E15
Loss in iteration 7 : 1.5269172751720484E20
Loss in iteration 8 : 6.046745100571968E24
Loss in iteration 9 : 2.394571527277159E29
Loss in iteration 10 : 9.482742705170248E33
Loss in iteration 11 : 3.75526093867447E38
Loss in iteration 12 : 1.4871208843244765E43
Loss in iteration 13 : 5.889147414013361E47
Loss in iteration 14 : 2.3321612674234308E52
Loss in iteration 15 : 9.235591835123528E56
Loss in iteration 16 : 3.657386722627269E61
Loss in iteration 17 : 1.4483617160276246E66
Loss in iteration 18 : 5.735657231640997E70
Loss in iteration 19 : 2.271377620302151E75
Loss in iteration 20 : 8.994882514158546E79
Loss in iteration 21 : 3.562063424431926E84
Loss in iteration 22 : 1.4106127367092875E89
Loss in iteration 23 : 5.586167498642447E93
Loss in iteration 24 : 2.2121781911373958E98
Loss in iteration 25 : 8.760446854723203E102
Loss in iteration 26 : 3.469224558938935E107
Loss in iteration 27 : 1.3738476175854075E112
Loss in iteration 28 : 5.4405739503999715E116
Loss in iteration 29 : 2.1545216900978933E121
Loss in iteration 30 : 8.532121344956667E125
Loss in iteration 31 : 3.3788053738162894E130
Loss in iteration 32 : 1.3380407160849885E135
Loss in iteration 33 : 5.298775039768164E139
Loss in iteration 34 : 2.09836790349859E144
Loss in iteration 35 : 8.309746734644767E148
Loss in iteration 36 : 3.2907428043866743E153
Loss in iteration 37 : 1.3031670579651668E158
Loss in iteration 38 : 5.1606718662478585E162
Loss in iteration 39 : 2.0436776657528143E167
Loss in iteration 40 : 8.093167924147717E171
Loss in iteration 41 : 3.2049754296417376E176
Loss in iteration 42 : 1.2692023198924247E181
Loss in iteration 43 : 5.026168107005992E185
Loss in iteration 44 : 1.9904128320554426E190
Loss in iteration 45 : 7.882233856222756E194
Loss in iteration 46 : 3.1214434294027744E199
Loss in iteration 47 : 1.2361228124777927E204
Loss in iteration 48 : 4.895169949693306E208
Loss in iteration 49 : 1.9385362517780464E213
Loss in iteration 50 : 7.676797410666239E217
Loss in iteration 51 : 3.0400885425979375E222
Loss in iteration 52 : 1.2039054637542094E227
Loss in iteration 53 : 4.7675860270130456E231
Loss in iteration 54 : 1.888011742557436E236
Loss in iteration 55 : 7.4767153017017E240
Loss in iteration 56 : 2.960854026626891E245
Loss in iteration 57 : 1.1725278030845151E250
Loss in iteration 58 : 4.643327352994988E254
Loss in iteration 59 : 1.838804065059545E259
Loss in iteration 60 : 7.281847978042303E263
Loss in iteration 61 : 2.8836846177845324E268
Loss in iteration 62 : 1.141967945488853E273
Loss in iteration 63 : 4.5223072609304065E277
Loss in iteration 64 : 1.7908788984010503E282
Loss in iteration 65 : 7.0920595255579975E286
Loss in iteration 66 : 2.808526492716223E291
Loss in iteration 67 : 1.1122045763805516E296
Loss in iteration 68 : 4.404441342924622E300
Loss in iteration 69 : 1.7442028162115797E305
Loss in iteration 70 : Infinity
Loss in iteration 71 : Infinity
Loss in iteration 72 : Infinity
Loss in iteration 73 : Infinity
Loss in iteration 74 : Infinity
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 2596 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.9470647673885166
Loss in iteration 3 : 24.761825122972695
Loss in iteration 4 : 284552.95828025823
Loss in iteration 5 : 5.492671792662567E9
Loss in iteration 6 : 1.0612040742139303E14
Loss in iteration 7 : 2.05035229191646925E18
Loss in iteration 8 : 3.961485656441205E22
Loss in iteration 9 : 7.653986436790756E26
Loss in iteration 10 : 1.4788267194522117E31
Loss in iteration 11 : 2.857241104653617E35
Loss in iteration 12 : 5.520475538301253E39
Loss in iteration 13 : 1.0666110787551854E44
Loss in iteration 14 : 2.0607992652628936E48
Loss in iteration 15 : 3.981670260414437E52
Loss in iteration 16 : 7.6929851101467315E56
Loss in iteration 17 : 1.4863616531314505E61
Loss in iteration 18 : 2.871799350015275E65
Loss in iteration 19 : 5.5486035241645144E69
Loss in iteration 20 : 1.072045686903826E74
Loss in iteration 21 : 2.071299471666882E78
Loss in iteration 22 : 4.0019577092075824E82
Loss in iteration 23 : 7.732182489959972E86
Loss in iteration 24 : 1.4939349788851656E91
Loss in iteration 25 : 2.8864317727040286E95
Loss in iteration 26 : 5.576874828041453E99
Loss in iteration 27 : 1.0775079855258892E104
Loss in iteration 28 : 2.081853178834571E108
Loss in iteration 29 : 4.022348526826275E112
Loss in iteration 30 : 7.771579588681044E116
Loss in iteration 31 : 1.501546892329065E121
Loss in iteration 32 : 2.901138750668986E125
Loss in iteration 33 : 5.605290180167548E129
Loss in iteration 34 : 1.0829981157101714E134
Loss in iteration 35 : 2.092460659363623E138
Loss in iteration 36 : 4.0428432399564555E142
Loss in iteration 37 : 7.811177423919868E146
Loss in iteration 38 : 1.5091975900755576E151
Loss in iteration 39 : 2.9159206637849845E155
Loss in iteration 40 : 5.633850314498971E159
Loss in iteration 41 : 1.0885162192643456E164
Loss in iteration 42 : 2.1031221872406423E168
Loss in iteration 43 : 4.063442377967645E172
Loss in iteration 44 : 7.850977018471287E176
Loss in iteration 45 : 1.5168872697388378E181
Loss in iteration 46 : 2.930777893862408E185
Loss in iteration 47 : 5.662555968731558E189
Loss in iteration 48 : 1.0940624387186245E194
Loss in iteration 49 : 2.113838037848254E198
Loss in iteration 50 : 4.084146472926611E202
Loss in iteration 51 : 7.890979400341506E206
Loss in iteration 52 : 1.5246161299399822E211
Loss in iteration 53 : 2.9457108246570403E215
Loss in iteration 54 : 5.691407884319868E219
Loss in iteration 55 : 1.0996369173294414E224
Loss in iteration 56 : 2.1246084879722132E228
Loss in iteration 57 : 4.104956059611114E232
Loss in iteration 58 : 7.931185602774635E236
Loss in iteration 59 : 1.532384370312087E241
Loss in iteration 60 : 2.9607198418799835E245
Loss in iteration 61 : 5.720406806496315E249
Loss in iteration 62 : 1.1052397990831532E254
Loss in iteration 63 : 2.13543381580856E258
Loss in iteration 64 : 4.1258716755237194E262
Loss in iteration 65 : 7.971596664279379E266
Loss in iteration 66 : 1.540192191505419E271
Loss in iteration 67 : 2.975805333207619E275
Loss in iteration 68 : 5.749553484290442E279
Loss in iteration 69 : 1.1108712286997562E284
Loss in iteration 70 : 2.1463143009707986E288
Loss in iteration 71 : 4.146893860905681E292
Loss in iteration 72 : 8.012213628655866E296
Loss in iteration 73 : 1.5480397951926E301
Loss in iteration 74 : 2.9909676882916223E305
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 2515 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.9465465019947603
Loss in iteration 3 : 7.478731131104393
Loss in iteration 4 : 9564.036508147976
Loss in iteration 5 : 5.916489684993966E7
Loss in iteration 6 : 3.69041696451435E11
Loss in iteration 7 : 2.3031859694000805E15
Loss in iteration 8 : 1.4374182346753954E19
Loss in iteration 9 : 8.970927200575731E22
Loss in iteration 10 : 5.598755665798914E26
Loss in iteration 11 : 3.494183411024975E30
Loss in iteration 12 : 2.180719866820682E34
Loss in iteration 13 : 1.3609872688827876E38
Loss in iteration 14 : 8.493921545097477E41
Loss in iteration 15 : 5.301056436295336E45
Loss in iteration 16 : 3.3083893218919193E49
Loss in iteration 17 : 2.0647657757927466E53
Loss in iteration 18 : 1.2886203206722532E57
Loss in iteration 19 : 8.042279421315532E60
Loss in iteration 20 : 5.019186586843024E64
Loss in iteration 21 : 3.132474348848731E68
Loss in iteration 22 : 1.9549772411164936E72
Loss in iteration 23 : 1.2201012961808033E76
Loss in iteration 24 : 7.614652189464395E79
Loss in iteration 25 : 4.752304431444728E83
Loss in iteration 26 : 2.9659131956646553E87
Loss in iteration 27 : 1.8510264254143112E91
Loss in iteration 28 : 1.1552255921010715E95
Loss in iteration 29 : 7.209762920302786E98
Loss in iteration 30 : 4.499613038560969E102
Loss in iteration 31 : 2.8082084973659013E106
Loss in iteration 32 : 1.7526029232060586E110
Loss in iteration 33 : 1.0937994843729012E114
Loss in iteration 34 : 6.826402581971276E117
Loss in iteration 35 : 4.260357851408274E121
Loss in iteration 36 : 2.6588893350639043E125
Loss in iteration 37 : 1.6594128340133827E129
Loss in iteration 38 : 1.035639549707752E133
Loss in iteration 39 : 6.463426429726081E136
Loss in iteration 40 : 4.033824434792047E140
Loss in iteration 41 : 2.5175098297537168E144
Loss in iteration 42 : 1.5711778847492947E148
Loss in iteration 43 : 9.805721178720349E151
Loss in iteration 44 : 6.119750587639369E155
Loss in iteration 45 : 3.81933634174573E159
Loss in iteration 46 : 2.3836478108835097E163
Loss in iteration 47 : 1.4876345987723986E167
Loss in iteration 48 : 9.28432753093854E170
Loss in iteration 49 : 5.794348812058742E174
Loss in iteration 50 : 3.6162530936058606E178
Loss in iteration 51 : 2.2569035557194178E182
Loss in iteration 52 : 1.4085335091244888E186
Loss in iteration 53 : 8.790657630445934E189
Loss in iteration 54 : 5.486249427161308E193
Loss in iteration 55 : 3.423968267491372E197
Loss in iteration 56 : 2.1368985957413644E201
Loss in iteration 57 : 1.3336384136021863E205
Loss in iteration 58 : 8.323237339291241E208
Loss in iteration 59 : 5.194532423451664E212
Loss in iteration 60 : 3.241907685476184E216
Loss in iteration 61 : 2.0232745865056863E220
Loss in iteration 62 : 1.2627256694381986E224
Loss in iteration 63 : 7.880670902963798E227
Loss in iteration 64 : 4.918326710539706E231
Loss in iteration 65 : 3.069527700047831E235
Loss in iteration 66 : 1.915692237599851E239
Loss in iteration 67 : 1.1955835254860672E243
Loss in iteration 68 : 7.461636782558544E246
Loss in iteration 69 : 4.6568075159947873E250
Loss in iteration 70 : 2.906313570732348E254
Loss in iteration 71 : 1.813830299494058E258
Loss in iteration 72 : 1.1320114899142416E262
Loss in iteration 73 : 7.06488370855478E265
Loss in iteration 74 : 4.409193922509039E269
Loss in iteration 75 : 2.751777927037892E273
Loss in iteration 76 : 1.717384604264348E277
Loss in iteration 77 : 1.0718197315213796E281
Loss in iteration 78 : 6.68922694442493E284
Loss in iteration 79 : 4.1747465360155977E288
Loss in iteration 80 : 2.6054593131273346E292
Loss in iteration 81 : 1.6260671573227702E296
Loss in iteration 82 : 1.0148285128851409E300
Loss in iteration 83 : 6.333544748916164E303
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 2800 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.94631623109335
Loss in iteration 3 : 1.9782660851906642
Loss in iteration 4 : 6.993542582984771
Loss in iteration 5 : 1055.6431372970912
Loss in iteration 6 : 222518.0380439028
Loss in iteration 7 : 7.984422158775921E7
Loss in iteration 8 : 2.8766603961296635E10
Loss in iteration 9 : 1.0384569066327578E13
Loss in iteration 10 : 3.7488087982560485E15
Loss in iteration 11 : 1.3533199130085865E18
Loss in iteration 12 : 4.885484811469774E20
Loss in iteration 13 : 1.7636600167125736E23
Loss in iteration 14 : 6.366812660063477E25
Loss in iteration 15 : 2.298419370282093E28
Loss in iteration 16 : 8.297293926717383E30
Loss in iteration 17 : 2.9953231075449723E33
Loss in iteration 18 : 1.081311641823735E36
Loss in iteration 19 : 3.9035350269836825E38
Loss in iteration 20 : 1.40917614474111E41
Loss in iteration 21 : 5.087125882515406E43
Loss in iteration 22 : 1.8364524435880609E46
Loss in iteration 23 : 6.6295933213529E48
Loss in iteration 24 : 2.3932831890083972E51
Loss in iteration 25 : 8.639752312320312E53
Loss in iteration 26 : 3.118950584747633E56
Loss in iteration 27 : 1.1259411610938955E59
Loss in iteration 28 : 4.0646475915489623E61
Loss in iteration 29 : 1.4673377805491753E64
Loss in iteration 30 : 5.2970893877825225E66
Loss in iteration 31 : 1.9122492689894913E69
Loss in iteration 32 : 6.903219861052063E71
Loss in iteration 33 : 2.4920623698397946E74
Loss in iteration 34 : 8.996345155121659E76
Loss in iteration 35 : 3.247680600998919E79
Loss in iteration 36 : 1.1724126969606097E82
Loss in iteration 37 : 4.232409836027802E84
Loss in iteration 38 : 1.5278999508060363E87
Loss in iteration 39 : 5.515718822409792E89
Loss in iteration 40 : 1.9911744948899344E92
Loss in iteration 41 : 7.188139926552663E94
Loss in iteration 42 : 2.5949185134855117E97
Loss in iteration 43 : 9.367655833682696E99
Loss in iteration 44 : 3.3817237559594535E102
Loss in iteration 45 : 1.220802275901363E105
Loss in iteration 46 : 4.4070962160039205E107
Loss in iteration 47 : 1.5909617339774152E110
Loss in iteration 48 : 5.743371859658468E112
Loss in iteration 49 : 2.073357241336707E115
Loss in iteration 50 : 7.484819641225511E117
Loss in iteration 51 : 2.7020198904824096E120
Loss in iteration 52 : 9.754291804641499E122
Loss in iteration 53 : 3.521299341475581E125
Loss in iteration 54 : 1.2711890622726848E128
Loss in iteration 55 : 4.588992514804392E130
Loss in iteration 56 : 1.6566262978443855E133
Loss in iteration 57 : 5.980420935218234E135
Loss in iteration 58 : 2.1589319576137814E138
Loss in iteration 59 : 7.793744366985752E140
Loss in iteration 60 : 2.8135417164818563E143
Loss in iteration 61 : 1.0156885596499503E146
Loss in iteration 62 : 3.66663570033632E148
Loss in iteration 63 : 1.3236554878214114E151
Loss in iteration 64 : 4.778396311035295E153
Loss in iteration 65 : 1.725001068283741E156
Loss in iteration 66 : 6.227253856504307E158
Loss in iteration 67 : 2.248038642198055E161
Loss in iteration 68 : 8.115419498334977E163
Loss in iteration 69 : 2.929666438898926E166
Loss in iteration 70 : 1.0576095844425124E169
Loss in iteration 71 : 3.8179705998374704E171
Loss in iteration 72 : 1.378287386541327E174
Loss in iteration 73 : 4.9756174654141897E176
Loss in iteration 74 : 1.7961979050145224E179
Loss in iteration 75 : 6.4842744371024276E181
Loss in iteration 76 : 2.3408230717939758E184
Loss in iteration 77 : 8.450371289176253E186
Loss in iteration 78 : 3.050584035392627E189
Loss in iteration 79 : 1.1012608367767387E192
Loss in iteration 80 : 3.975551620764025E194
Loss in iteration 81 : 1.4351741350958136E197
Loss in iteration 82 : 5.180978627695887E199
Loss in iteration 83 : 1.870333284598215E202
Loss in iteration 84 : 6.751903157399557E204
Loss in iteration 85 : 2.4374370398212398E207
Loss in iteration 86 : 8.799147713754676E209
Loss in iteration 87 : 3.176492324665437E212
Loss in iteration 88 : 1.1467137292042231E215
Loss in iteration 89 : 4.1396365624272454E217
Loss in iteration 90 : 1.4944087990362352E220
Loss in iteration 91 : 5.3948157645208114E222
Loss in iteration 92 : 1.9475284909920123E225
Loss in iteration 93 : 7.030577852481165E227
Loss in iteration 94 : 2.5380386047457E230
Loss in iteration 95 : 9.162319363131978E232
Loss in iteration 96 : 3.307597290090644E235
Loss in iteration 97 : 1.1940426217227223E238
Loss in iteration 98 : 4.310493864419028E240
Loss in iteration 99 : 1.556088285055269E243
Loss in iteration 100 : 5.617478709049522E245
Loss in iteration 101 : 2.027909813966877E248
Loss in iteration 102 : 7.320754428420427E250
Loss in iteration 103 : 2.6427923486597746E253
Loss in iteration 104 : 9.540480378661784E255
Loss in iteration 105 : 3.4441134166969056E258
Loss in iteration 106 : 1.2433249434275826E261
Loss in iteration 107 : 4.488403045773573E263
Loss in iteration 108 : 1.62031349952426E266
Loss in iteration 109 : 5.849331733282578E268
Loss in iteration 110 : 2.111608755715011E271
Loss in iteration 111 : 7.62290760813119E273
Loss in iteration 112 : 2.751869646535359E276
Loss in iteration 113 : 9.934249423992645E278
Loss in iteration 114 : 3.586264042061346E281
Loss in iteration 115 : 1.294641319184146E284
Loss in iteration 116 : 4.673655162254766E286
Loss in iteration 117 : 1.6871895135739708E289
Loss in iteration 118 : 6.090754144002034E291
Loss in iteration 119 : 2.1987622459847344E294
Loss in iteration 120 : 7.93753170800489E296
Loss in iteration 121 : 2.865448946589765E299
Loss in iteration 122 : 1.034427069718905E302
Loss in iteration 123 : 3.7342817216852477E304
Loss in iteration 124 : 1.3480757015283744E307
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 2927 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.946309043700288
Loss in iteration 3 : 1.644020819955163
Loss in iteration 4 : 2.7812913368744825
Loss in iteration 5 : 167.53255271047564
Loss in iteration 6 : 5123.896009482727
Loss in iteration 7 : 824697.0725863
Loss in iteration 8 : 1.3547297863899574E8
Loss in iteration 9 : 2.2887973485047165E10
Loss in iteration 10 : 3.8674082973994224E12
Loss in iteration 11 : 6.535908260465158E14
Loss in iteration 12 : 1.10456738193432928E17
Loss in iteration 13 : 1.8667188555910017E19
Loss in iteration 14 : 3.154754847120769E21
Loss in iteration 15 : 5.331535691298162E23
Loss in iteration 16 : 9.010295317975701E25
Loss in iteration 17 : 1.5227399087373255E28
Loss in iteration 18 : 2.5734304457655426E30
Loss in iteration 19 : 4.349097453343757E32
Loss in iteration 20 : 7.34997469615094E34
Loss in iteration 21 : 1.242145723649509E37
Loss in iteration 22 : 2.09922627296767E39
Loss in iteration 23 : 3.5476924013153625E41
Loss in iteration 24 : 5.995600158222962E43
Loss in iteration 25 : 1.0132564267396806E46
Loss in iteration 26 : 1.7124033611900604E48
Loss in iteration 27 : 2.8939616804112014E50
Loss in iteration 28 : 4.89079523989493E52
Loss in iteration 29 : 8.265443955422433E54
Loss in iteration 30 : 1.3968600284663912E57
Loss in iteration 31 : 2.3606934481082006E59
Loss in iteration 32 : 3.9895719273028605E61
Loss in iteration 33 : 6.742376557141834E63
Loss in iteration 34 : 1.1394616381569697E66
Loss in iteration 35 : 1.9256901684852794E68
Loss in iteration 36 : 3.254416384740123E70
Loss in iteration 37 : 5.499963690210805E72
Loss in iteration 38 : 9.29493863645626E74
Loss in iteration 39 : 1.570844629561108E77
Loss in iteration 40 : 2.654727423958272E79
Loss in iteration 41 : 4.486489346489481E81
Loss in iteration 42 : 7.582166995567222E83
Loss in iteration 43 : 1.2813862222508607E86
Loss in iteration 44 : 2.1655427156039543E88
Loss in iteration 45 : 3.659767189370684E90
Loss in iteration 46 : 6.185006550036455E92
Loss in iteration 47 : 1.0452661069561608E95
Loss in iteration 48 : 1.766499720755912E97
Loss in iteration 49 : 2.9853845280774913E99
Loss in iteration 50 : 5.0452998524509594E101
Loss in iteration 51 : 8.526556750642121E103
Loss in iteration 52 : 1.4409880908585185E106
Loss in iteration 53 : 2.435269873550897E108
Loss in iteration 54 : 4.1156060863010153E110
Loss in iteration 55 : 6.955374285848717E112
Loss in iteration 56 : 1.1754582543084327E115
Loss in iteration 57 : 1.9865244497812515E117
Loss in iteration 58 : 3.357226320130315E119
Loss in iteration 59 : 5.673712481020233E121
Loss in iteration 60 : 9.588574092924192E123
Loss in iteration 61 : 1.6204690217041887E126
Loss in iteration 62 : 2.7385926466800794E128
Loss in iteration 63 : 4.628221572889333E130
Loss in iteration 64 : 7.821694458182971E132
Loss in iteration 65 : 1.3218663634329225E135
Loss in iteration 66 : 2.2339541542016388E137
Loss in iteration 67 : 3.7753825206007707E139
Loss in iteration 68 : 6.3803964598153E141
Loss in iteration 69 : 1.078287001708786E144
Loss in iteration 70 : 1.8223050328878484E146
Loss in iteration 71 : 3.079695505580464E148
Loss in iteration 72 : 5.204685404430983E150
Loss in iteration 73 : 8.795918333488363E152
Loss in iteration 74 : 1.4865101983595336E155
Loss in iteration 75 : 2.5122022352276112E157
Loss in iteration 76 : 4.245621777534663E159
Loss in iteration 77 : 7.175100804033577E161
Loss in iteration 78 : 1.2125920358816747E164
Loss in iteration 79 : 2.0492805406400306E166
Loss in iteration 80 : 3.4632841136816515E168
Loss in iteration 81 : 5.852950152121989E170
Loss in iteration 82 : 9.891485757086164E172
Loss in iteration 83 : 1.671661092947562E175
Loss in iteration 84 : 2.825107247081379E177
Loss in iteration 85 : 4.774431247567532E179
Loss in iteration 86 : 8.068788808389126E181
Loss in iteration 87 : 1.3636253086177625E184
Loss in iteration 88 : 2.3045267715640185E186
Loss in iteration 89 : 3.894650243943191E188
Loss in iteration 90 : 6.5819589122639934E190
Loss in iteration 91 : 1.112351056172615E193
Loss in iteration 92 : 1.8798732849317192E195
Loss in iteration 93 : 3.1769858515346055E197
Loss in iteration 94 : 5.369106089093482E199
Loss in iteration 95 : 9.073789290567988E201
Loss in iteration 96 : 1.5334703901059895E204
Loss in iteration 97 : 2.5915649592791227E206
Loss in iteration 98 : 4.379744781181717E208
Loss in iteration 99 : 7.401768680197102E210
Loss in iteration 100 : 1.25089890695331E213
Loss in iteration 101 : 2.114019152751094E215
Loss in iteration 102 : 3.5726923681493485E217
Loss in iteration 103 : 6.0378501021724E219
Loss in iteration 104 : 1.0203966672671353E222
Loss in iteration 105 : 1.7244703676814593E224
Loss in iteration 106 : 2.9143549213816656E226
Loss in iteration 107 : 4.925259817135015E228
Loss in iteration 108 : 8.323689090958177E230
Loss in iteration 109 : 1.406703456371932E233
Loss in iteration 110 : 2.3773288412685652E235
Loss in iteration 111 : 4.0176857417438744E237
Loss in iteration 112 : 6.789888903547148E239
Loss in iteration 113 : 1.147491224699468E242
Loss in iteration 114 : 1.9392601697421008E244
Loss in iteration 115 : 3.277349686864151E246
Loss in iteration 116 : 5.538720970800415E248
Loss in iteration 117 : 9.360438440652702E250
Loss in iteration 118 : 1.5819140964703065E253
Loss in iteration 119 : 2.673434823034819E255
Loss in iteration 120 : 4.518104850928845E257
Loss in iteration 121 : 7.635597198069745E259
Loss in iteration 122 : 1.2904159264737872E262
Loss in iteration 123 : 2.1808029157406997E264
Loss in iteration 124 : 3.6855569276017825E266
Loss in iteration 125 : 6.228591207647013E268
Loss in iteration 126 : 1.0526319140923454E271
Loss in iteration 127 : 1.778947934816064E273
Loss in iteration 128 : 3.0064220098391474E275
Loss in iteration 129 : 5.08085319662816E277
Loss in iteration 130 : 8.586641902301591E279
Loss in iteration 131 : 1.4511424814889686E282
Loss in iteration 132 : 2.452430793716357E284
Loss in iteration 133 : 4.144608041380643E286
Loss in iteration 134 : 7.00438758993329E288
Loss in iteration 135 : 1.1837415026987259E291
Loss in iteration 136 : 2.0005231395608463E293
Loss in iteration 137 : 3.380884105857831E295
Loss in iteration 138 : 5.713694138899732E297
Loss in iteration 139 : 9.656143094740548E299
Loss in iteration 140 : 1.6318881830111526E302
Loss in iteration 141 : 2.7578910292888483E304
Loss in iteration 142 : 4.6608358394981544E306
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 70.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3481 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.9463047362521494
Loss in iteration 3 : 1.3208586806256373
Loss in iteration 4 : 0.9750372242200964
Loss in iteration 5 : 21.291840806846203
Loss in iteration 6 : 39.397420636367436
Loss in iteration 7 : 1427.9308588438594
Loss in iteration 8 : 22252.947795509437
Loss in iteration 9 : 1066565.244086445
Loss in iteration 10 : 4.992510469868553E7
Loss in iteration 11 : 2.4451632951218467E9
Loss in iteration 12 : 1.1969850991482076E11
Loss in iteration 13 : 5.865169811706155E12
Loss in iteration 14 : 2.8738771068930025E14
Loss in iteration 15 : 1.4081995022244836E16
Loss in iteration 16 : 6.9001748119586714E17
Loss in iteration 17 : 3.3810856441322484E19
Loss in iteration 18 : 1.6567319521549896E21
Loss in iteration 19 : 8.117986564886802E22
Loss in iteration 20 : 3.977813416134511E24
Loss in iteration 21 : 1.9491285739026145E26
Loss in iteration 22 : 9.550730012090471E27
Loss in iteration 23 : 4.67985770592417E29
Loss in iteration 24 : 2.2931302759026844E31
Loss in iteration 25 : 1.1236338351923144E33
Loss in iteration 26 : 5.505805792442333E34
Loss in iteration 27 : 2.697844838296743E36
Loss in iteration 28 : 1.3219439707654044E38
Loss in iteration 29 : 6.477525456750481E39
Loss in iteration 30 : 3.173987473807736E41
Loss in iteration 31 : 1.5552538621657906E43
Loss in iteration 32 : 7.620743924612374E44
Loss in iteration 33 : 3.734164523060063E46
Loss in iteration 34 : 1.8297406162994308E48
Loss in iteration 35 : 8.965729019867213E49
Loss in iteration 36 : 4.393207219734933E51
Loss in iteration 37 : 2.1526715376701186E53
Loss in iteration 38 : 1.0548090534583578E55
Loss in iteration 39 : 5.168564361945953E56
Loss in iteration 40 : 2.532596537353517E58
Loss in iteration 41 : 1.2409723033032232E60
Loss in iteration 42 : 6.080764286185794E61
Loss in iteration 43 : 2.9795745002310396E63
Loss in iteration 44 : 1.4599915051132094E65
Loss in iteration 45 : 7.153958375054725E66
Loss in iteration 46 : 3.5054396037768154E68
Loss in iteration 47 : 1.7176654058506394E70
Loss in iteration 48 : 8.416560488668136E71
Loss in iteration 49 : 4.124114639447386E73
Loss in iteration 50 : 2.0208161733292196E75
Loss in iteration 51 : 9.901999249313172E76
Loss in iteration 52 : 4.8519796321634535E78
Loss in iteration 53 : 2.3774700197600924E80
Loss in iteration 54 : 1.1649603096824452E82
Loss in iteration 55 : 5.708305517443983E83
Loss in iteration 56 : 2.7970697035475515E85
Loss in iteration 57 : 1.3705641547383004E87
Loss in iteration 58 : 6.715764358217673E88
Loss in iteration 59 : 3.29072453552666E90
Loss in iteration 60 : 1.6124550224080628E92
Loss in iteration 61 : 7.901029609799506E93
Loss in iteration 62 : 3.871504508801759E95
Loss in iteration 63 : 1.8970372093128615E97
Loss in iteration 64 : 9.29548232563302E98
Loss in iteration 65 : 4.554786339560179E100
Loss in iteration 66 : 2.2318453063844888E102
Loss in iteration 67 : 1.0936042001283991E104
Loss in iteration 68 : 5.358660580629156E105
Loss in iteration 69 : 2.6257436845082865E107
Loss in iteration 70 : 1.2866144054090604E109
Loss in iteration 71 : 6.304410586504396E110
Loss in iteration 72 : 3.0891611873871544E112
Loss in iteration 73 : 1.5136889818197052E114
Loss in iteration 74 : 7.417076010916559E115
Loss in iteration 75 : 3.634367245349114E117
Loss in iteration 76 : 1.7808399502210656E119
Loss in iteration 77 : 8.72611575608322E120
Loss in iteration 78 : 4.275796720480777E122
Loss in iteration 79 : 2.0951403930355818E124
Loss in iteration 80 : 1.0266187925874348E126
Loss in iteration 81 : 5.030432083678431E127
Loss in iteration 82 : 2.4649117210024315E129
Loss in iteration 83 : 1.2078067432911914E131
Loss in iteration 84 : 5.918253042126839E132
Loss in iteration 85 : 2.8999439906421514E134
Loss in iteration 86 : 1.4209725554146538E136
Loss in iteration 87 : 6.962765521531803E137
Loss in iteration 88 : 3.411755105550583E139
Loss in iteration 89 : 1.6717600017197856E141
Loss in iteration 90 : 8.191624008426951E142
Loss in iteration 91 : 4.013895764129206E144
Loss in iteration 92 : 1.966808924423311E146
Loss in iteration 93 : 9.637363729674222E147
Loss in iteration 94 : 4.722308227540368E149
Loss in iteration 95 : 2.313931031494781E151
Loss in iteration 96 : 1.1338262054324424E153
Loss in iteration 97 : 5.555748406618969E154
Loss in iteration 98 : 2.7223167192432943E156
Loss in iteration 99 : 1.3339351924292143E158
Loss in iteration 100 : 6.53628244290315E159
Loss in iteration 101 : 3.2027783970225433E161
Loss in iteration 102 : 1.5693614145410458E163
Loss in iteration 103 : 7.689870931251126E164
Loss in iteration 104 : 3.7680367563130516E166
Loss in iteration 105 : 1.8463380105933946E168
Loss in iteration 106 : 9.047056251907636E169
Loss in iteration 107 : 4.433057563434742E171
Loss in iteration 108 : 2.1721982060830237E173
Loss in iteration 109 : 1.064377120980681E175
Loss in iteration 110 : 5.215447892805339E176
Loss in iteration 111 : 2.5555694674746158E178
Loss in iteration 112 : 1.2522290390625618E180
Loss in iteration 113 : 6.135922291406553E181
Loss in iteration 114 : 3.006601922789211E183
Loss in iteration 115 : 1.473234942166713E185
Loss in iteration 116 : 7.218851216616894E186
Loss in iteration 117 : 3.5372370961422796E188
Loss in iteration 118 : 1.7332461771097169E190
Loss in iteration 119 : 8.492906267837611E191
Loss in iteration 120 : 4.16152407124043E193
Loss in iteration 121 : 2.0391467949078106E195
Loss in iteration 122 : 9.991819295048271E196
Loss in iteration 123 : 4.895991454573654E198
Loss in iteration 124 : 2.3990358127410905E200
Loss in iteration 125 : 1.1755275482431344E202
Loss in iteration 126 : 5.760084986391358E203
Loss in iteration 127 : 2.822441643331766E205
Loss in iteration 128 : 1.3829964052325653E207
Loss in iteration 129 : 6.776682385639568E208
Loss in iteration 130 : 3.3205743689633894E210
Loss in iteration 131 : 1.6270814407920608E212
Loss in iteration 132 : 7.972699059881097E213
Loss in iteration 133 : 3.9066225393417375E215
Loss in iteration 134 : 1.9142450442774518E217
Loss in iteration 135 : 9.379800716959516E218
Loss in iteration 136 : 4.596102351310161E220
Loss in iteration 137 : 2.252090152141979E222
Loss in iteration 138 : 1.1035241745495697E224
Loss in iteration 139 : 5.407268455292892E225
Loss in iteration 140 : 2.6495615430935168E227
Loss in iteration 141 : 1.2982851561158235E229
Loss in iteration 142 : 6.361597264967535E230
Loss in iteration 143 : 3.1171826598340924E232
Loss in iteration 144 : 1.5274195033187054E234
Loss in iteration 145 : 7.484355566261655E235
Loss in iteration 146 : 3.667334227468211E237
Loss in iteration 147 : 1.7969937714594232E239
Loss in iteration 148 : 8.805269480151175E240
Loss in iteration 149 : 4.314582045274075E242
Loss in iteration 150 : 2.114145202184297E244
Loss in iteration 151 : 1.0359311490703053E246
Loss in iteration 152 : 5.076062630444497E247
Loss in iteration 153 : 2.4872706889178037E249
Loss in iteration 154 : 1.218762637569724E251
Loss in iteration 155 : 5.9719369240916465E252
Loss in iteration 156 : 2.9262490928049073E254
Loss in iteration 157 : 1.4338620554744043E256
Loss in iteration 158 : 7.025924071824583E257
Loss in iteration 159 : 3.4427027951940454E259
Loss in iteration 160 : 1.6869243696450823E261
Loss in iteration 161 : 8.2659294112609E262
Loss in iteration 162 : 4.050305411517842E264
Loss in iteration 163 : 1.984649651643743E266
Loss in iteration 164 : 9.724783293054338E267
Loss in iteration 165 : 4.765143813596625E269
Loss in iteration 166 : 2.3349204686623465E271
Loss in iteration 167 : 1.14411102964455E273
Loss in iteration 168 : 5.606144045258295E274
Loss in iteration 169 : 2.7470105821765643E276
Loss in iteration 170 : 1.3460351852665167E278
Loss in iteration 171 : 6.595572407805931E279
Loss in iteration 172 : 3.231830479824906E281
Loss in iteration 173 : 1.5835969351142043E283
Loss in iteration 174 : 7.759624982059601E284
Loss in iteration 175 : 3.8022162412092043E286
Loss in iteration 176 : 1.8630859581925102E288
Loss in iteration 177 : 9.1291211951433E289
Loss in iteration 178 : 4.4732693856202174E291
Loss in iteration 179 : 2.1919019989539066E293
Loss in iteration 180 : 1.0740319794874141E295
Loss in iteration 181 : 5.26275669948833E296
Loss in iteration 182 : 2.578750782749282E298
Loss in iteration 183 : 1.2635878835471478E300
Loss in iteration 184 : 6.191580629381024E301
Loss in iteration 185 : 3.033874508396702E303
Loss in iteration 186 : 1.486598509114384E305
Loss in iteration 187 : 7.28433269466048E306
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 40.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3728 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.9463033087489342
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 60 millisecond.
Loss in iteration 1 : 1.0047058700041656
Loss in iteration 2 : 5.311123820583651
Loss in iteration 3 : 3.3098531289663664
Loss in iteration 4 : 1.4478512024995889
Loss in iteration 5 : 0.5333819102407276
Loss in iteration 6 : 0.6521456444012499
Loss in iteration 7 : 0.8518269183503401
Loss in iteration 8 : 1.244707854297864
Loss in iteration 9 : 0.5933206348340768
Loss in iteration 10 : 0.6897701447297593
Loss in iteration 11 : 0.691096750313995
Loss in iteration 12 : 0.9130797762356334
Loss in iteration 13 : 0.72420842943449
Loss in iteration 14 : 0.9079282834497492
Loss in iteration 15 : 0.6799450101797815
Loss in iteration 16 : 0.8026131270435497
Loss in iteration 17 : 0.6788990942186204
Loss in iteration 18 : 0.833953323260075
Loss in iteration 19 : 0.7034168227542146
Loss in iteration 20 : 0.8823112946016229
Loss in iteration 21 : 0.7067134162757341
Loss in iteration 22 : 0.8880740511817599
Loss in iteration 23 : 0.7111046235339085
Loss in iteration 24 : 0.9044359353382996
Loss in iteration 25 : 0.7146146759046881
Loss in iteration 26 : 0.9140156444137488
Loss in iteration 27 : 0.7186783076066798
Loss in iteration 28 : 0.9217649836987825
Loss in iteration 29 : 0.7273182098958303
Loss in iteration 30 : 0.9409749312013431
Loss in iteration 31 : 0.7284292351638417
Loss in iteration 32 : 0.9363390604527119
Loss in iteration 33 : 0.7375484878588007
Loss in iteration 34 : 0.9514282679279061
Loss in iteration 35 : 0.7427454670225908
Loss in iteration 36 : 0.9637151424096894
Loss in iteration 37 : 0.7492549756414053
Loss in iteration 38 : 0.9748960711515632
Loss in iteration 39 : 0.7533350577878378
Loss in iteration 40 : 0.9723325159046761
Loss in iteration 41 : 0.7656334974258934
Loss in iteration 42 : 1.0046462784622558
Loss in iteration 43 : 0.7631890919540251
Loss in iteration 44 : 0.9720299181047007
Loss in iteration 45 : 0.7751117870784981
Loss in iteration 46 : 1.0198875261944733
Loss in iteration 47 : 0.7729655221622467
Loss in iteration 48 : 0.9799858282018352
Loss in iteration 49 : 0.7806095886570164
Loss in iteration 50 : 1.0126845244041052
Loss in iteration 51 : 0.7945190439489171
Loss in iteration 52 : 1.0516910353835278
Loss in iteration 53 : 0.7901130106619859
Loss in iteration 54 : 0.9957651936485846
Loss in iteration 55 : 0.7944075631684723
Loss in iteration 56 : 1.0191113354222188
Loss in iteration 57 : 0.8154712347422476
Loss in iteration 58 : 1.089791913335215
Loss in iteration 59 : 0.8066704539414653
Loss in iteration 60 : 1.0109114231719936
Loss in iteration 61 : 0.802361124810806
Loss in iteration 62 : 1.0273034796988476
Loss in iteration 63 : 0.8300823719390648
Loss in iteration 64 : 1.1083126374224606
Loss in iteration 65 : 0.8226888609616405
Loss in iteration 66 : 1.032321769430142
Loss in iteration 67 : 0.8134519866233546
Loss in iteration 68 : 1.0387159761911071
Loss in iteration 69 : 0.8426776043962092
Loss in iteration 70 : 1.1204741578119
Loss in iteration 71 : 0.838178030578911
Loss in iteration 72 : 1.0550707166525832
Loss in iteration 73 : 0.8332128419261529
Loss in iteration 74 : 1.0574969497609321
Loss in iteration 75 : 0.8483368759068197
Loss in iteration 76 : 1.1014070310272013
Loss in iteration 77 : 0.8590185180990122
Loss in iteration 78 : 1.1106208750742728
Loss in iteration 79 : 0.8521137811571156
Loss in iteration 80 : 1.0699356440373107
Loss in iteration 81 : 0.8544031734820969
Loss in iteration 82 : 1.0870654666849977
Loss in iteration 83 : 0.866695826403666
Loss in iteration 84 : 1.1097333192198735
Loss in iteration 85 : 0.8704908866317556
Loss in iteration 86 : 1.1085066492100912
Loss in iteration 87 : 0.8732153083721987
Loss in iteration 88 : 1.1014483544161573
Loss in iteration 89 : 0.8743091479880309
Loss in iteration 90 : 1.1078933868246712
Loss in iteration 91 : 0.8865610893893532
Loss in iteration 92 : 1.1466774468770093
Loss in iteration 93 : 0.8852727459068148
Loss in iteration 94 : 1.1132398482342445
Loss in iteration 95 : 0.8816582381312977
Loss in iteration 96 : 1.1146435018131748
Loss in iteration 97 : 0.892003857035902
Loss in iteration 98 : 1.1448844399272387
Loss in iteration 99 : 0.9028284046873948
Loss in iteration 100 : 1.1555967780808236
Loss in iteration 101 : 0.8960207411073646
Loss in iteration 102 : 1.125022141280144
Loss in iteration 103 : 0.8999503841002134
Loss in iteration 104 : 1.133300386480344
Loss in iteration 105 : 0.9105156222238268
Loss in iteration 106 : 1.1695743927096376
Loss in iteration 107 : 0.9120275326473678
Loss in iteration 108 : 1.1456794288526275
Loss in iteration 109 : 0.9103538088675558
Loss in iteration 110 : 1.1450430637663174
Loss in iteration 111 : 0.9207557246783517
Loss in iteration 112 : 1.18243160903569
Loss in iteration 113 : 0.9237696771134021
Loss in iteration 114 : 1.1652329379436803
Loss in iteration 115 : 0.917884578402691
Loss in iteration 116 : 1.1487973300397505
Loss in iteration 117 : 0.9286566844210679
Loss in iteration 118 : 1.1815359576325495
Loss in iteration 119 : 0.9343714544290189
Loss in iteration 120 : 1.185864603251186
Loss in iteration 121 : 0.9336430249249394
Loss in iteration 122 : 1.1695948146496862
Loss in iteration 123 : 0.9352656928495511
Loss in iteration 124 : 1.1720634044639582
Loss in iteration 125 : 0.9421678094923672
Loss in iteration 126 : 1.1961189820872808
Loss in iteration 127 : 0.9452027856487097
Loss in iteration 128 : 1.1878649450652499
Loss in iteration 129 : 0.9399845723786603
Loss in iteration 130 : 1.1729219301628022
Loss in iteration 131 : 0.9486466271749614
Loss in iteration 132 : 1.2038288002602
Loss in iteration 133 : 0.9583599047631163
Loss in iteration 134 : 1.2152051426608868
Loss in iteration 135 : 0.9512030210428901
Loss in iteration 136 : 1.1817424791277007
Loss in iteration 137 : 0.9559058967082016
Loss in iteration 138 : 1.2025681854396857
Loss in iteration 139 : 0.9580802913021595
Loss in iteration 140 : 1.2055847539544744
Loss in iteration 141 : 0.9611281976834853
Loss in iteration 142 : 1.208473851614971
Loss in iteration 143 : 0.9656124167307547
Loss in iteration 144 : 1.2207518678390916
Loss in iteration 145 : 0.9653835701357516
Loss in iteration 146 : 1.2119705604682989
Loss in iteration 147 : 0.9680631156358416
Loss in iteration 148 : 1.21534045046131
Loss in iteration 149 : 0.9754659701844073
Loss in iteration 150 : 1.2287482325657877
Loss in iteration 151 : 0.9718789085448052
Loss in iteration 152 : 1.211519553977938
Loss in iteration 153 : 0.977711088006504
Loss in iteration 154 : 1.2307498807240902
Loss in iteration 155 : 0.9779583681928168
Loss in iteration 156 : 1.2247041259180362
Loss in iteration 157 : 0.9829033739075259
Loss in iteration 158 : 1.2374570018279956
Loss in iteration 159 : 0.9797906826745248
Loss in iteration 160 : 1.217803261817242
Loss in iteration 161 : 0.989210418036355
Loss in iteration 162 : 1.2524987280261521
Loss in iteration 163 : 0.9926118444006311
Loss in iteration 164 : 1.2404683352305743
Loss in iteration 165 : 0.9834093899985128
Loss in iteration 166 : 1.20651668510076
Loss in iteration 167 : 1.0016232978593556
Loss in iteration 168 : 1.2756645370732191
Loss in iteration 169 : 0.9983894172451944
Loss in iteration 170 : 1.2435147875952988
Loss in iteration 171 : 0.986615214491016
Loss in iteration 172 : 1.2074709124236989
Loss in iteration 173 : 0.9975691770041133
Loss in iteration 174 : 1.2545691283089617
Loss in iteration 175 : 1.0022085706476598
Loss in iteration 176 : 1.255643755272837
Loss in iteration 177 : 1.0010120698707323
Loss in iteration 178 : 1.2430343849602041
Loss in iteration 179 : 1.0055074504310422
Loss in iteration 180 : 1.25766778232836
Loss in iteration 181 : 1.001193605718558
Loss in iteration 182 : 1.237053864965746
Loss in iteration 183 : 1.009441322737831
Loss in iteration 184 : 1.2728914289192834
Loss in iteration 185 : 1.0085310174284683
Loss in iteration 186 : 1.2471807575478382
Loss in iteration 187 : 1.0071249686806447
Loss in iteration 188 : 1.2456970343490994
Loss in iteration 189 : 1.0137036921466194
Loss in iteration 190 : 1.278636773731807
Loss in iteration 191 : 1.011546307895564
Loss in iteration 192 : 1.2487079024899315
Loss in iteration 193 : 1.0130187482205126
Loss in iteration 194 : 1.2608936744678236
Loss in iteration 195 : 1.0141987420446825
Loss in iteration 196 : 1.2599112256936924
Loss in iteration 197 : 1.018718978093564
Loss in iteration 198 : 1.2738358147984352
Loss in iteration 199 : 1.0176703456155052
Loss in iteration 200 : 1.2565553095422766
Testing accuracy  of updater 5 on alg 1 with rate 0.0343 = 0.78675, training accuracy 0.7879572677241826, time elapsed: 3992 millisecond.
Loss in iteration 1 : 1.0014698258661427
Loss in iteration 2 : 3.0232507012474152
Loss in iteration 3 : 1.9607982187866468
Loss in iteration 4 : 0.9187327497331091
Loss in iteration 5 : 0.46843512690882483
Loss in iteration 6 : 0.5552417910885267
Loss in iteration 7 : 0.6236922462977307
Loss in iteration 8 : 0.7891233693306056
Loss in iteration 9 : 0.5317801628924611
Loss in iteration 10 : 0.595530318927303
Loss in iteration 11 : 0.5476255616242564
Loss in iteration 12 : 0.6284021965767634
Loss in iteration 13 : 0.5637938055423957
Loss in iteration 14 : 0.6489709885941439
Loss in iteration 15 : 0.5560779378384713
Loss in iteration 16 : 0.6337235029825729
Loss in iteration 17 : 0.5621733734017286
Loss in iteration 18 : 0.6485685167009013
Loss in iteration 19 : 0.5687300188839896
Loss in iteration 20 : 0.6550000127355512
Loss in iteration 21 : 0.5738603884544751
Loss in iteration 22 : 0.665586506580343
Loss in iteration 23 : 0.5788754556664264
Loss in iteration 24 : 0.6745761708269794
Loss in iteration 25 : 0.5800816262506788
Loss in iteration 26 : 0.6804101306928803
Loss in iteration 27 : 0.5832237370984059
Loss in iteration 28 : 0.6909543531549853
Loss in iteration 29 : 0.591632295322751
Loss in iteration 30 : 0.7035585510418105
Loss in iteration 31 : 0.594235418471536
Loss in iteration 32 : 0.7091678367324635
Loss in iteration 33 : 0.5984813456423422
Loss in iteration 34 : 0.7178002367840652
Loss in iteration 35 : 0.6031740909770296
Loss in iteration 36 : 0.7286556338828941
Loss in iteration 37 : 0.6050955146892802
Loss in iteration 38 : 0.7307191785813036
Loss in iteration 39 : 0.6108699526136205
Loss in iteration 40 : 0.74060335233777
Loss in iteration 41 : 0.6124629132956495
Loss in iteration 42 : 0.7391992837712348
Loss in iteration 43 : 0.6095997024906954
Loss in iteration 44 : 0.7309574285516267
Loss in iteration 45 : 0.6212066390961106
Loss in iteration 46 : 0.7687647517340341
Loss in iteration 47 : 0.6275036866894337
Loss in iteration 48 : 0.7703089903393826
Loss in iteration 49 : 0.623668305489679
Loss in iteration 50 : 0.7497282771215533
Loss in iteration 51 : 0.6280596808538308
Loss in iteration 52 : 0.7592909764284274
Loss in iteration 53 : 0.6378516758187464
Loss in iteration 54 : 0.7813461965174354
Loss in iteration 55 : 0.6355765662638805
Loss in iteration 56 : 0.7612521140020427
Loss in iteration 57 : 0.6424505622756924
Loss in iteration 58 : 0.7882259976333161
Loss in iteration 59 : 0.6432626680546316
Loss in iteration 60 : 0.7698959164724235
Loss in iteration 61 : 0.6514757277953583
Loss in iteration 62 : 0.7999781851037423
Loss in iteration 63 : 0.6481565589832783
Loss in iteration 64 : 0.7772494669296407
Loss in iteration 65 : 0.6573021512416504
Loss in iteration 66 : 0.7998166324778191
Loss in iteration 67 : 0.6612662003972416
Loss in iteration 68 : 0.7969325037151938
Loss in iteration 69 : 0.6638934330369818
Loss in iteration 70 : 0.7961526054243183
Loss in iteration 71 : 0.667793173167249
Loss in iteration 72 : 0.80032903977737
Loss in iteration 73 : 0.6715414714077061
Loss in iteration 74 : 0.8077488323282581
Loss in iteration 75 : 0.6740409266427129
Loss in iteration 76 : 0.8063062923469436
Loss in iteration 77 : 0.677834884607859
Loss in iteration 78 : 0.8171276735759244
Loss in iteration 79 : 0.6793659613197396
Loss in iteration 80 : 0.8134282547346249
Loss in iteration 81 : 0.6832542453351711
Loss in iteration 82 : 0.8213421477112026
Loss in iteration 83 : 0.6891239667490068
Loss in iteration 84 : 0.8328830981141835
Loss in iteration 85 : 0.6866586546696501
Loss in iteration 86 : 0.821302390377636
Loss in iteration 87 : 0.6935360958450627
Loss in iteration 88 : 0.8330420047678619
Loss in iteration 89 : 0.695490399683066
Loss in iteration 90 : 0.8340052768800776
Loss in iteration 91 : 0.6973379282099804
Loss in iteration 92 : 0.8392286967952147
Loss in iteration 93 : 0.701700509649062
Loss in iteration 94 : 0.8432219309828735
Loss in iteration 95 : 0.7042107991673014
Loss in iteration 96 : 0.845067086982545
Loss in iteration 97 : 0.707699071945886
Loss in iteration 98 : 0.8505925979984048
Loss in iteration 99 : 0.7097765686163162
Loss in iteration 100 : 0.8493451068321527
Loss in iteration 101 : 0.7152654631473088
Loss in iteration 102 : 0.8601869138111722
Loss in iteration 103 : 0.7176948888487859
Loss in iteration 104 : 0.8614542371093532
Loss in iteration 105 : 0.7176361284319834
Loss in iteration 106 : 0.853057528378178
Loss in iteration 107 : 0.7201529284173959
Loss in iteration 108 : 0.863829673909156
Loss in iteration 109 : 0.7302567354305273
Loss in iteration 110 : 0.8865635383698272
Loss in iteration 111 : 0.7232603091982729
Loss in iteration 112 : 0.8576006418516253
Loss in iteration 113 : 0.7320955319209429
Loss in iteration 114 : 0.878270997573894
Loss in iteration 115 : 0.7354881824022019
Loss in iteration 116 : 0.8795106531005747
Loss in iteration 117 : 0.7384118439735111
Loss in iteration 118 : 0.8796175021713613
Loss in iteration 119 : 0.7388661172552577
Loss in iteration 120 : 0.8814471512258083
Loss in iteration 121 : 0.7393643650056241
Loss in iteration 122 : 0.8789564585876042
Loss in iteration 123 : 0.7478866374637188
Loss in iteration 124 : 0.9039277335636369
Loss in iteration 125 : 0.7461191797532256
Loss in iteration 126 : 0.8886806008392583
Loss in iteration 127 : 0.7475602102319903
Loss in iteration 128 : 0.8883581156859831
Loss in iteration 129 : 0.7550642082961585
Loss in iteration 130 : 0.90595526176632
Loss in iteration 131 : 0.7579343468715936
Loss in iteration 132 : 0.9060030282285312
Loss in iteration 133 : 0.7565211869228132
Loss in iteration 134 : 0.8993099721554388
Loss in iteration 135 : 0.758682718784543
Loss in iteration 136 : 0.9037887136491415
Loss in iteration 137 : 0.7634680188204058
Loss in iteration 138 : 0.9165683040438509
Loss in iteration 139 : 0.7681297820886965
Loss in iteration 140 : 0.9132645953031262
Loss in iteration 141 : 0.7657703424304875
Loss in iteration 142 : 0.9080746605079081
Loss in iteration 143 : 0.7714425787937418
Loss in iteration 144 : 0.9217805723933066
Loss in iteration 145 : 0.7747884113281572
Loss in iteration 146 : 0.923172176298743
Loss in iteration 147 : 0.7760091494843651
Loss in iteration 148 : 0.9236409263268471
Loss in iteration 149 : 0.7751539233356598
Loss in iteration 150 : 0.9165486315714823
Loss in iteration 151 : 0.7788097656579673
Loss in iteration 152 : 0.9307771170788346
Loss in iteration 153 : 0.7847267288856783
Loss in iteration 154 : 0.935506326089864
Loss in iteration 155 : 0.7859025997443148
Loss in iteration 156 : 0.9316899007664479
Loss in iteration 157 : 0.7864271432933158
Loss in iteration 158 : 0.9358909544792782
Loss in iteration 159 : 0.7878962829686916
Loss in iteration 160 : 0.9358947329021696
Loss in iteration 161 : 0.79177370376349
Loss in iteration 162 : 0.939989870803366
Loss in iteration 163 : 0.7933669572003029
Loss in iteration 164 : 0.9414771178447905
Loss in iteration 165 : 0.7962942982849743
Loss in iteration 166 : 0.9450941932083265
Loss in iteration 167 : 0.7978033819547355
Loss in iteration 168 : 0.9414341515940016
Loss in iteration 169 : 0.801258073754439
Loss in iteration 170 : 0.9519077320597047
Loss in iteration 171 : 0.8036947100208263
Loss in iteration 172 : 0.952784826806468
Loss in iteration 173 : 0.8029427224141288
Loss in iteration 174 : 0.946818582154281
Loss in iteration 175 : 0.8052043537130052
Loss in iteration 176 : 0.950618422032133
Loss in iteration 177 : 0.809795096413808
Loss in iteration 178 : 0.9594764646767883
Loss in iteration 179 : 0.8102408512368562
Loss in iteration 180 : 0.9530116154460615
Loss in iteration 181 : 0.8138575714010384
Loss in iteration 182 : 0.9650700419117961
Loss in iteration 183 : 0.8130009853335587
Loss in iteration 184 : 0.9546085087077271
Loss in iteration 185 : 0.807930956472898
Loss in iteration 186 : 0.9428428388827144
Loss in iteration 187 : 0.8110129667760028
Loss in iteration 188 : 0.9673497932672859
Loss in iteration 189 : 0.8310421570319695
Loss in iteration 190 : 1.0040382677724855
Loss in iteration 191 : 0.816442474698616
Loss in iteration 192 : 0.9458551039234053
Loss in iteration 193 : 0.8119067288947415
Loss in iteration 194 : 0.9500601466095022
Loss in iteration 195 : 0.8205969649232749
Loss in iteration 196 : 0.9909885309753228
Loss in iteration 197 : 0.8346285330705758
Loss in iteration 198 : 1.002767882133489
Loss in iteration 199 : 0.8202376555124509
Loss in iteration 200 : 0.9473767033948957
Testing accuracy  of updater 5 on alg 1 with rate 0.02401 = 0.79075, training accuracy 0.7966979605050178, time elapsed: 4832 millisecond.
Loss in iteration 1 : 1.0004917045284347
Loss in iteration 2 : 1.9002966278447635
Loss in iteration 3 : 1.286162078618849
Loss in iteration 4 : 0.6843902006602677
Loss in iteration 5 : 0.4360578976184482
Loss in iteration 6 : 0.5311823608033907
Loss in iteration 7 : 0.5262176207327373
Loss in iteration 8 : 0.598354630389769
Loss in iteration 9 : 0.45108261956574264
Loss in iteration 10 : 0.48073931986722124
Loss in iteration 11 : 0.47009421825937
Loss in iteration 12 : 0.4883271935528825
Loss in iteration 13 : 0.4612538405251469
Loss in iteration 14 : 0.47741951118899667
Loss in iteration 15 : 0.45235729144964854
Loss in iteration 16 : 0.47062231218554323
Loss in iteration 17 : 0.44953828698929654
Loss in iteration 18 : 0.47532803852690925
Loss in iteration 19 : 0.45942237761701615
Loss in iteration 20 : 0.4929490464437864
Loss in iteration 21 : 0.4631762630647607
Loss in iteration 22 : 0.4957289035173605
Loss in iteration 23 : 0.4672439380739853
Loss in iteration 24 : 0.4992706100280469
Loss in iteration 25 : 0.4709310654440991
Loss in iteration 26 : 0.5036228741396918
Loss in iteration 27 : 0.4682141392997941
Loss in iteration 28 : 0.5009004904931359
Loss in iteration 29 : 0.4716867421459781
Loss in iteration 30 : 0.5090406828864318
Loss in iteration 31 : 0.47513117339104494
Loss in iteration 32 : 0.5160246545166889
Loss in iteration 33 : 0.4775136656096055
Loss in iteration 34 : 0.5176183732134829
Loss in iteration 35 : 0.4769884809364813
Loss in iteration 36 : 0.5173893618233311
Loss in iteration 37 : 0.481292251594018
Loss in iteration 38 : 0.5253084817181871
Loss in iteration 39 : 0.4821899862693639
Loss in iteration 40 : 0.5275072791581928
Loss in iteration 41 : 0.4862355050713363
Loss in iteration 42 : 0.5357723877524447
Loss in iteration 43 : 0.48746170861274396
Loss in iteration 44 : 0.5362737866452368
Loss in iteration 45 : 0.4880087624423072
Loss in iteration 46 : 0.5373803590678744
Loss in iteration 47 : 0.4882734250933154
Loss in iteration 48 : 0.5391797397748022
Loss in iteration 49 : 0.4909688032051187
Loss in iteration 50 : 0.5428633035876153
Loss in iteration 51 : 0.49385070772761647
Loss in iteration 52 : 0.545487158624063
Loss in iteration 53 : 0.4962198816715515
Loss in iteration 54 : 0.5469957226469562
Loss in iteration 55 : 0.4981245106304374
Loss in iteration 56 : 0.5496992366010048
Loss in iteration 57 : 0.4989844785040014
Loss in iteration 58 : 0.5487474532572655
Loss in iteration 59 : 0.4988202083985282
Loss in iteration 60 : 0.5458401275591332
Loss in iteration 61 : 0.5035348351795877
Loss in iteration 62 : 0.5608156463289643
Loss in iteration 63 : 0.5086288956348988
Loss in iteration 64 : 0.5641824600228234
Loss in iteration 65 : 0.5067521475869072
Loss in iteration 66 : 0.5553974494836278
Loss in iteration 67 : 0.5060953218996469
Loss in iteration 68 : 0.5528630440247931
Loss in iteration 69 : 0.5104177839500501
Loss in iteration 70 : 0.5634469852588349
Loss in iteration 71 : 0.5139418258614852
Loss in iteration 72 : 0.5688270273711111
Loss in iteration 73 : 0.5155196407667089
Loss in iteration 74 : 0.5662937375781675
Loss in iteration 75 : 0.5127060579900597
Loss in iteration 76 : 0.5591199288818943
Loss in iteration 77 : 0.5165332261942561
Loss in iteration 78 : 0.5716595578641824
Loss in iteration 79 : 0.523013279474751
Loss in iteration 80 : 0.5793937025369374
Loss in iteration 81 : 0.5220404082992709
Loss in iteration 82 : 0.5745624328461406
Loss in iteration 83 : 0.5205576410310164
Loss in iteration 84 : 0.5667085083214798
Loss in iteration 85 : 0.5213888235434102
Loss in iteration 86 : 0.5712085032428805
Loss in iteration 87 : 0.5301095524700373
Loss in iteration 88 : 0.588336795243081
Loss in iteration 89 : 0.5302028706548075
Loss in iteration 90 : 0.5893223294472807
Loss in iteration 91 : 0.5298948534498065
Loss in iteration 92 : 0.581136443328628
Loss in iteration 93 : 0.5294682173642153
Loss in iteration 94 : 0.5787221281421318
Loss in iteration 95 : 0.5339689840150741
Loss in iteration 96 : 0.5913713761179789
Loss in iteration 97 : 0.5361333592525028
Loss in iteration 98 : 0.5913180275588025
Loss in iteration 99 : 0.5369009922018855
Loss in iteration 100 : 0.5900628137004372
Loss in iteration 101 : 0.5374303622437075
Loss in iteration 102 : 0.5912095139214142
Loss in iteration 103 : 0.5399307170456505
Loss in iteration 104 : 0.5948651506789601
Loss in iteration 105 : 0.5428845035139533
Loss in iteration 106 : 0.6021159260074772
Loss in iteration 107 : 0.5448331369869621
Loss in iteration 108 : 0.6016441762029144
Loss in iteration 109 : 0.5469133349756445
Loss in iteration 110 : 0.6038779954537858
Loss in iteration 111 : 0.5462251456146281
Loss in iteration 112 : 0.5990239046175403
Loss in iteration 113 : 0.5508113038699767
Loss in iteration 114 : 0.6108130012454748
Loss in iteration 115 : 0.5516007421034885
Loss in iteration 116 : 0.6084110345020528
Loss in iteration 117 : 0.5510238898085481
Loss in iteration 118 : 0.6039817676059944
Loss in iteration 119 : 0.556515865620167
Loss in iteration 120 : 0.6183673651141042
Loss in iteration 121 : 0.5595796838496006
Loss in iteration 122 : 0.6187147289934222
Loss in iteration 123 : 0.5599974355999828
Loss in iteration 124 : 0.6156242399000493
Loss in iteration 125 : 0.5572463975934943
Loss in iteration 126 : 0.6099669082977981
Loss in iteration 127 : 0.5616618928130008
Loss in iteration 128 : 0.6200041795150552
Loss in iteration 129 : 0.5648988684512807
Loss in iteration 130 : 0.6271288059397004
Loss in iteration 131 : 0.5687851207036524
Loss in iteration 132 : 0.6312834551413266
Loss in iteration 133 : 0.5685653520142934
Loss in iteration 134 : 0.6203074119000137
Loss in iteration 135 : 0.5657767688814462
Loss in iteration 136 : 0.6201973768288015
Loss in iteration 137 : 0.5716762470120994
Loss in iteration 138 : 0.6306596467770414
Loss in iteration 139 : 0.5773687814058831
Loss in iteration 140 : 0.6420387117935091
Loss in iteration 141 : 0.5782638166657429
Loss in iteration 142 : 0.6362707756354771
Loss in iteration 143 : 0.5755935420976597
Loss in iteration 144 : 0.6291513122297796
Loss in iteration 145 : 0.5760131930270094
Loss in iteration 146 : 0.6330716509393696
Loss in iteration 147 : 0.5822030066268112
Loss in iteration 148 : 0.6435240992916098
Loss in iteration 149 : 0.5858996916746528
Loss in iteration 150 : 0.6489628783726579
Loss in iteration 151 : 0.5865285459118045
Loss in iteration 152 : 0.6401505953860774
Loss in iteration 153 : 0.5824362586686063
Loss in iteration 154 : 0.6364080242532848
Loss in iteration 155 : 0.5853515612683478
Loss in iteration 156 : 0.64416165410291
Loss in iteration 157 : 0.5903016823516279
Loss in iteration 158 : 0.6518616630881395
Loss in iteration 159 : 0.5981903721839117
Loss in iteration 160 : 0.6646459384802177
Loss in iteration 161 : 0.5932094151511054
Loss in iteration 162 : 0.642057902434352
Loss in iteration 163 : 0.588607602404346
Loss in iteration 164 : 0.6411214771677101
Loss in iteration 165 : 0.5940067574178579
Loss in iteration 166 : 0.6558473754901097
Loss in iteration 167 : 0.6046550214415394
Loss in iteration 168 : 0.6748261577147747
Loss in iteration 169 : 0.6013584387237051
Loss in iteration 170 : 0.6540798087811601
Loss in iteration 171 : 0.5938883840548164
Loss in iteration 172 : 0.6478765818957144
Loss in iteration 173 : 0.6017638166637373
Loss in iteration 174 : 0.6652136771276854
Loss in iteration 175 : 0.6120160933569739
Loss in iteration 176 : 0.6836869316320995
Loss in iteration 177 : 0.6069605204878482
Loss in iteration 178 : 0.6595764088984426
Loss in iteration 179 : 0.5984198154769864
Loss in iteration 180 : 0.6492156503521561
Loss in iteration 181 : 0.6086237500971676
Loss in iteration 182 : 0.6745888433340341
Loss in iteration 183 : 0.6198813963142394
Loss in iteration 184 : 0.6892353593497136
Loss in iteration 185 : 0.614240443252188
Loss in iteration 186 : 0.6664985522303792
Loss in iteration 187 : 0.6035326320556478
Loss in iteration 188 : 0.6516380690955026
Loss in iteration 189 : 0.6139618214879992
Loss in iteration 190 : 0.681922919150646
Loss in iteration 191 : 0.6272348928166951
Loss in iteration 192 : 0.698524787304177
Loss in iteration 193 : 0.617371882053279
Loss in iteration 194 : 0.6708866140454355
Loss in iteration 195 : 0.6111539021493684
Loss in iteration 196 : 0.6639762064463901
Loss in iteration 197 : 0.6190504677293287
Loss in iteration 198 : 0.6844452368239744
Loss in iteration 199 : 0.625749810044698
Loss in iteration 200 : 0.6940219901185207
Testing accuracy  of updater 5 on alg 1 with rate 0.01372 = 0.7955, training accuracy 0.8064098413726125, time elapsed: 4553 millisecond.
Loss in iteration 1 : 1.0000326243773245
Loss in iteration 2 : 0.7530795729986496
Loss in iteration 3 : 0.5961307871858744
Loss in iteration 4 : 0.5129479226185545
Loss in iteration 5 : 0.5047706542407473
Loss in iteration 6 : 0.49686184731637933
Loss in iteration 7 : 0.48881182333154255
Loss in iteration 8 : 0.48047600410898134
Loss in iteration 9 : 0.4718248506830296
Loss in iteration 10 : 0.46285342136670965
Loss in iteration 11 : 0.45350530035532965
Loss in iteration 12 : 0.4438403717798095
Loss in iteration 13 : 0.43382913040182997
Loss in iteration 14 : 0.4241714033531239
Loss in iteration 15 : 0.41551177846906484
Loss in iteration 16 : 0.4080202950545211
Loss in iteration 17 : 0.4019873444405336
Loss in iteration 18 : 0.39715186652113155
Loss in iteration 19 : 0.3933794542320015
Loss in iteration 20 : 0.39059188934935657
Loss in iteration 21 : 0.38835610290828254
Loss in iteration 22 : 0.38655835017268064
Loss in iteration 23 : 0.38491273983034674
Loss in iteration 24 : 0.38347327981233614
Loss in iteration 25 : 0.38223710962363666
Loss in iteration 26 : 0.3811564172626441
Loss in iteration 27 : 0.3802637866291788
Loss in iteration 28 : 0.37945100412631066
Loss in iteration 29 : 0.3787644464694628
Loss in iteration 30 : 0.378181165015758
Loss in iteration 31 : 0.37775809584388303
Loss in iteration 32 : 0.3774200718904397
Loss in iteration 33 : 0.377444605169267
Loss in iteration 34 : 0.3787849459728152
Loss in iteration 35 : 0.3856602840827266
Loss in iteration 36 : 0.4100692005005471
Loss in iteration 37 : 0.43923269375120855
Loss in iteration 38 : 0.4062741906901857
Loss in iteration 39 : 0.3958582712060119
Loss in iteration 40 : 0.3809978411404841
Loss in iteration 41 : 0.3784544287963664
Loss in iteration 42 : 0.37757305910083655
Loss in iteration 43 : 0.3768836897101843
Loss in iteration 44 : 0.3780525376894322
Loss in iteration 45 : 0.37831150034011746
Loss in iteration 46 : 0.38023779197539204
Loss in iteration 47 : 0.3824637350549028
Loss in iteration 48 : 0.38474531062757555
Loss in iteration 49 : 0.3907704406319763
Loss in iteration 50 : 0.39048532615835174
Loss in iteration 51 : 0.3956926149231468
Loss in iteration 52 : 0.38868714499271206
Loss in iteration 53 : 0.3919450077083228
Loss in iteration 54 : 0.38551597502102664
Loss in iteration 55 : 0.38739326529968005
Loss in iteration 56 : 0.3843854021345509
Loss in iteration 57 : 0.385910676833201
Loss in iteration 58 : 0.3852570959186114
Loss in iteration 59 : 0.3876988704174164
Loss in iteration 60 : 0.3862821749896862
Loss in iteration 61 : 0.3893877400531062
Loss in iteration 62 : 0.3866228366785125
Loss in iteration 63 : 0.38943899504044754
Loss in iteration 64 : 0.38652666198200514
Loss in iteration 65 : 0.38921468864973985
Loss in iteration 66 : 0.3867662273882241
Loss in iteration 67 : 0.389273714048712
Loss in iteration 68 : 0.3871455395207666
Loss in iteration 69 : 0.3894305940858305
Loss in iteration 70 : 0.38739517057161044
Loss in iteration 71 : 0.38986534543261986
Loss in iteration 72 : 0.38790320974224607
Loss in iteration 73 : 0.3905544395027436
Loss in iteration 74 : 0.38788959107024373
Loss in iteration 75 : 0.38984892762102213
Loss in iteration 76 : 0.3872643535504863
Loss in iteration 77 : 0.38832269784352824
Loss in iteration 78 : 0.3872011561258246
Loss in iteration 79 : 0.3886272325227739
Loss in iteration 80 : 0.3878480069753313
Loss in iteration 81 : 0.39047448353342235
Loss in iteration 82 : 0.3894075993463513
Loss in iteration 83 : 0.39426314401894114
Loss in iteration 84 : 0.38896597670501165
Loss in iteration 85 : 0.3912180628372023
Loss in iteration 86 : 0.38891646418873943
Loss in iteration 87 : 0.39130339944076703
Loss in iteration 88 : 0.38915462132818307
Loss in iteration 89 : 0.39154795331421016
Loss in iteration 90 : 0.3895555988619771
Loss in iteration 91 : 0.3920688828821618
Loss in iteration 92 : 0.3895247351606283
Loss in iteration 93 : 0.39184923896094237
Loss in iteration 94 : 0.38986310775421046
Loss in iteration 95 : 0.39212595834077313
Loss in iteration 96 : 0.39038738746060936
Loss in iteration 97 : 0.3930941343861349
Loss in iteration 98 : 0.390553200776614
Loss in iteration 99 : 0.3935240014350196
Loss in iteration 100 : 0.3905625836437503
Loss in iteration 101 : 0.39268706320908114
Loss in iteration 102 : 0.39113454754597166
Loss in iteration 103 : 0.39462768212780497
Loss in iteration 104 : 0.39146877689672216
Loss in iteration 105 : 0.3946102721079699
Loss in iteration 106 : 0.39112206562818475
Loss in iteration 107 : 0.3932353221066434
Loss in iteration 108 : 0.3909042511109259
Loss in iteration 109 : 0.39241742757543296
Loss in iteration 110 : 0.3914036399228127
Loss in iteration 111 : 0.3949021747787662
Loss in iteration 112 : 0.39289112159738193
Loss in iteration 113 : 0.3969460267399373
Loss in iteration 114 : 0.3929758605725495
Loss in iteration 115 : 0.39669132477653646
Loss in iteration 116 : 0.3932325385661529
Loss in iteration 117 : 0.39612256603091606
Loss in iteration 118 : 0.393184171455708
Loss in iteration 119 : 0.3959648814814654
Loss in iteration 120 : 0.3928533713028368
Loss in iteration 121 : 0.3954720756384898
Loss in iteration 122 : 0.3935933495506675
Loss in iteration 123 : 0.39705638416683475
Loss in iteration 124 : 0.39456572235176457
Loss in iteration 125 : 0.3985769458713081
Loss in iteration 126 : 0.39499850509999657
Loss in iteration 127 : 0.3987225864782941
Loss in iteration 128 : 0.39435487350433834
Loss in iteration 129 : 0.397020281268826
Loss in iteration 130 : 0.39437123994036966
Loss in iteration 131 : 0.3969939270458361
Loss in iteration 132 : 0.39528888758830943
Loss in iteration 133 : 0.398954220395359
Loss in iteration 134 : 0.3962407617164133
Loss in iteration 135 : 0.40057713315977994
Loss in iteration 136 : 0.39603832745335427
Loss in iteration 137 : 0.3991115347001088
Loss in iteration 138 : 0.3961582896848236
Loss in iteration 139 : 0.39969172919118373
Loss in iteration 140 : 0.3965023908261466
Loss in iteration 141 : 0.4005685653471523
Loss in iteration 142 : 0.39720605784336777
Loss in iteration 143 : 0.40134863275858534
Loss in iteration 144 : 0.3975232449061573
Loss in iteration 145 : 0.4013620557451883
Loss in iteration 146 : 0.3979471494533789
Loss in iteration 147 : 0.402025243420916
Loss in iteration 148 : 0.39827116351332265
Loss in iteration 149 : 0.4027927777250007
Loss in iteration 150 : 0.39880097779094104
Loss in iteration 151 : 0.4030745763720249
Loss in iteration 152 : 0.39922124958940514
Loss in iteration 153 : 0.4033915055474005
Loss in iteration 154 : 0.3994740308174657
Loss in iteration 155 : 0.40362861783427884
Loss in iteration 156 : 0.3999188747672762
Loss in iteration 157 : 0.40434009287357503
Loss in iteration 158 : 0.4005556876544106
Loss in iteration 159 : 0.4047546767864541
Loss in iteration 160 : 0.4007547771042411
Loss in iteration 161 : 0.4046886979027132
Loss in iteration 162 : 0.40130108204011006
Loss in iteration 163 : 0.40533371556213704
Loss in iteration 164 : 0.4019485591753162
Loss in iteration 165 : 0.4057561132959552
Loss in iteration 166 : 0.4021836967155493
Loss in iteration 167 : 0.40608408227257836
Loss in iteration 168 : 0.40232243398309175
Loss in iteration 169 : 0.406149907962537
Loss in iteration 170 : 0.4029217729143344
Loss in iteration 171 : 0.4071880155891103
Loss in iteration 172 : 0.40371477473278833
Loss in iteration 173 : 0.4074215335480512
Loss in iteration 174 : 0.4037295099348411
Loss in iteration 175 : 0.40829983812418236
Loss in iteration 176 : 0.40453308157128626
Loss in iteration 177 : 0.4085416108991164
Loss in iteration 178 : 0.40455529902101306
Loss in iteration 179 : 0.40844747719140584
Loss in iteration 180 : 0.4053943224521709
Loss in iteration 181 : 0.40983284848993407
Loss in iteration 182 : 0.4065064374965992
Loss in iteration 183 : 0.40992774163958406
Loss in iteration 184 : 0.40600612376127454
Loss in iteration 185 : 0.41023341665264934
Loss in iteration 186 : 0.4068123275324488
Loss in iteration 187 : 0.41073947484417217
Loss in iteration 188 : 0.4073904033321624
Loss in iteration 189 : 0.41124170065285975
Loss in iteration 190 : 0.4079006644069217
Loss in iteration 191 : 0.41181446111087683
Loss in iteration 192 : 0.4090380502839851
Loss in iteration 193 : 0.41331602386219257
Loss in iteration 194 : 0.4095844971717279
Loss in iteration 195 : 0.41353048547265514
Loss in iteration 196 : 0.4096769653529731
Loss in iteration 197 : 0.41350559662692027
Loss in iteration 198 : 0.41044406177236253
Loss in iteration 199 : 0.41400521986878086
Loss in iteration 200 : 0.41068569339568317
Testing accuracy  of updater 5 on alg 1 with rate 0.00343 = 0.771, training accuracy 0.8355454839753965, time elapsed: 3663 millisecond.
Loss in iteration 1 : 1.0000191564372658
Loss in iteration 2 : 0.6793998276185735
Loss in iteration 3 : 0.5598987611694066
Loss in iteration 4 : 0.5339229064222675
Loss in iteration 5 : 0.5248184264649249
Loss in iteration 6 : 0.5175149159824173
Loss in iteration 7 : 0.5106416755476649
Loss in iteration 8 : 0.5039006397135504
Loss in iteration 9 : 0.4969953530475493
Loss in iteration 10 : 0.4898723251217731
Loss in iteration 11 : 0.48251488848187263
Loss in iteration 12 : 0.47487339520939953
Loss in iteration 13 : 0.4669612930915266
Loss in iteration 14 : 0.4587188490267822
Loss in iteration 15 : 0.45015458261495317
Loss in iteration 16 : 0.4412972448617007
Loss in iteration 17 : 0.4321974578048571
Loss in iteration 18 : 0.4235557465280108
Loss in iteration 19 : 0.415815518695404
Loss in iteration 20 : 0.4090250607778152
Loss in iteration 21 : 0.40339147790561247
Loss in iteration 22 : 0.3987962365451168
Loss in iteration 23 : 0.39510432973500126
Loss in iteration 24 : 0.3921818121810641
Loss in iteration 25 : 0.3898009797610155
Loss in iteration 26 : 0.3880181594267209
Loss in iteration 27 : 0.3863446980215957
Loss in iteration 28 : 0.38481869984803174
Loss in iteration 29 : 0.38349373199829306
Loss in iteration 30 : 0.3823227304965807
Loss in iteration 31 : 0.3812992075908139
Loss in iteration 32 : 0.3804217087169482
Loss in iteration 33 : 0.3796016163643389
Loss in iteration 34 : 0.3788996328394157
Loss in iteration 35 : 0.3783371761931085
Loss in iteration 36 : 0.3779589585637651
Loss in iteration 37 : 0.3779489673388685
Loss in iteration 38 : 0.3788296977392079
Loss in iteration 39 : 0.38194702466176017
Loss in iteration 40 : 0.3928516719939492
Loss in iteration 41 : 0.40376911700690543
Loss in iteration 42 : 0.38969004450475414
Loss in iteration 43 : 0.3837443678109908
Loss in iteration 44 : 0.37904038379737665
Loss in iteration 45 : 0.37762097687830654
Loss in iteration 46 : 0.3769829212697673
Loss in iteration 47 : 0.37692442137362936
Loss in iteration 48 : 0.3766699229774701
Loss in iteration 49 : 0.37688236651086715
Loss in iteration 50 : 0.3775428133732765
Loss in iteration 51 : 0.37825642386253083
Loss in iteration 52 : 0.3801642332980703
Loss in iteration 53 : 0.3822173435406157
Loss in iteration 54 : 0.38339118691262614
Loss in iteration 55 : 0.38530800256085024
Loss in iteration 56 : 0.3836965092369002
Loss in iteration 57 : 0.38328122636616524
Loss in iteration 58 : 0.380233287405831
Loss in iteration 59 : 0.3799092949923905
Loss in iteration 60 : 0.37925160520593587
Loss in iteration 61 : 0.3793155172292878
Loss in iteration 62 : 0.3796754803865461
Loss in iteration 63 : 0.37993903723676425
Loss in iteration 64 : 0.3802386530082305
Loss in iteration 65 : 0.38106796856658476
Loss in iteration 66 : 0.3813952968093208
Loss in iteration 67 : 0.3822048375511589
Loss in iteration 68 : 0.38172003716567293
Loss in iteration 69 : 0.38150850080927395
Loss in iteration 70 : 0.38112492915626534
Loss in iteration 71 : 0.38122207853547924
Loss in iteration 72 : 0.3810854942611878
Loss in iteration 73 : 0.3810936648130008
Loss in iteration 74 : 0.3812071306764106
Loss in iteration 75 : 0.38132093816279117
Loss in iteration 76 : 0.38156659764431855
Loss in iteration 77 : 0.3816334784849865
Loss in iteration 78 : 0.3814265744214191
Loss in iteration 79 : 0.3816893832274324
Loss in iteration 80 : 0.38150018598070706
Loss in iteration 81 : 0.38184583013215057
Loss in iteration 82 : 0.38170213995062396
Loss in iteration 83 : 0.381925756992483
Loss in iteration 84 : 0.38181662147681866
Loss in iteration 85 : 0.382007775710603
Loss in iteration 86 : 0.38196783008638585
Loss in iteration 87 : 0.3821972494386927
Loss in iteration 88 : 0.3821837921900749
Loss in iteration 89 : 0.3822992025128499
Loss in iteration 90 : 0.38206935901084227
Loss in iteration 91 : 0.3823249127741616
Loss in iteration 92 : 0.38227216767999167
Loss in iteration 93 : 0.3824918577954878
Loss in iteration 94 : 0.38246769314512946
Loss in iteration 95 : 0.3827628043538349
Loss in iteration 96 : 0.3825419003514813
Loss in iteration 97 : 0.382712935258126
Loss in iteration 98 : 0.3825573969363937
Loss in iteration 99 : 0.38279218326001735
Loss in iteration 100 : 0.3824446887184077
Loss in iteration 101 : 0.3829471932148464
Loss in iteration 102 : 0.38260532100369876
Loss in iteration 103 : 0.38324072262245973
Loss in iteration 104 : 0.38300354005785897
Loss in iteration 105 : 0.3834562243036314
Loss in iteration 106 : 0.38301811508874095
Loss in iteration 107 : 0.3836797629275936
Loss in iteration 108 : 0.38304105034665986
Loss in iteration 109 : 0.38391133147871265
Loss in iteration 110 : 0.3833355739455688
Loss in iteration 111 : 0.38385899107608057
Loss in iteration 112 : 0.3830265289909062
Loss in iteration 113 : 0.38369522818710544
Loss in iteration 114 : 0.38330916525474157
Loss in iteration 115 : 0.38439915872349745
Loss in iteration 116 : 0.38378926491002113
Loss in iteration 117 : 0.3847410773721275
Loss in iteration 118 : 0.38432519968157325
Loss in iteration 119 : 0.38459510061455604
Loss in iteration 120 : 0.383754947885116
Loss in iteration 121 : 0.38408521752729713
Loss in iteration 122 : 0.3833958645680063
Loss in iteration 123 : 0.3840375053226293
Loss in iteration 124 : 0.38404973508580925
Loss in iteration 125 : 0.38504353561827903
Loss in iteration 126 : 0.38484485394129003
Loss in iteration 127 : 0.3857478551264308
Loss in iteration 128 : 0.3852651161714069
Loss in iteration 129 : 0.38558883072714994
Loss in iteration 130 : 0.38504907485683687
Loss in iteration 131 : 0.38562218663172054
Loss in iteration 132 : 0.38531790801667803
Loss in iteration 133 : 0.38594158579418286
Loss in iteration 134 : 0.3852950853446271
Loss in iteration 135 : 0.38587221308787256
Loss in iteration 136 : 0.3853108462594345
Loss in iteration 137 : 0.38576632831656177
Loss in iteration 138 : 0.3849097476204376
Loss in iteration 139 : 0.38636467251767787
Loss in iteration 140 : 0.38585567772806606
Loss in iteration 141 : 0.38645638696910173
Loss in iteration 142 : 0.3860852643953428
Loss in iteration 143 : 0.38664668358217785
Loss in iteration 144 : 0.38631766019292585
Loss in iteration 145 : 0.386991214895417
Loss in iteration 146 : 0.3865265278573061
Loss in iteration 147 : 0.38734349294845705
Loss in iteration 148 : 0.3867223096714615
Loss in iteration 149 : 0.3875114443667863
Loss in iteration 150 : 0.38679612784960216
Loss in iteration 151 : 0.38749297773541064
Loss in iteration 152 : 0.38709729007922294
Loss in iteration 153 : 0.3880388488826109
Loss in iteration 154 : 0.3882517538874069
Loss in iteration 155 : 0.38903189721624315
Loss in iteration 156 : 0.389248739606467
Loss in iteration 157 : 0.3886413844315797
Loss in iteration 158 : 0.38839434683639457
Loss in iteration 159 : 0.38790047455991356
Loss in iteration 160 : 0.3866904519762854
Loss in iteration 161 : 0.3875194863336359
Loss in iteration 162 : 0.38721910609852117
Loss in iteration 163 : 0.38822819344410336
Loss in iteration 164 : 0.3880806781650637
Loss in iteration 165 : 0.3888195223895562
Loss in iteration 166 : 0.3886418655584872
Loss in iteration 167 : 0.3896194212116834
Loss in iteration 168 : 0.39027270304972267
Loss in iteration 169 : 0.391354391526767
Loss in iteration 170 : 0.3912964504318007
Loss in iteration 171 : 0.39102166762345675
Loss in iteration 172 : 0.39037054217693984
Loss in iteration 173 : 0.38998906341013423
Loss in iteration 174 : 0.3891512000197306
Loss in iteration 175 : 0.389109087589625
Loss in iteration 176 : 0.388404601819638
Loss in iteration 177 : 0.38949360613078865
Loss in iteration 178 : 0.38928007474733217
Loss in iteration 179 : 0.390520888890319
Loss in iteration 180 : 0.3907984596721901
Loss in iteration 181 : 0.39237527143663425
Loss in iteration 182 : 0.3932445805761003
Loss in iteration 183 : 0.39406336118790647
Loss in iteration 184 : 0.3922439580133086
Loss in iteration 185 : 0.39155397528536795
Loss in iteration 186 : 0.39101130379132765
Loss in iteration 187 : 0.39080830366387986
Loss in iteration 188 : 0.39023533107558595
Loss in iteration 189 : 0.3909395004565994
Loss in iteration 190 : 0.3907823961545301
Loss in iteration 191 : 0.39177111257259817
Loss in iteration 192 : 0.39172070828644934
Loss in iteration 193 : 0.39286236054097345
Loss in iteration 194 : 0.39423657618292995
Loss in iteration 195 : 0.395350780586734
Loss in iteration 196 : 0.3947522280050089
Loss in iteration 197 : 0.39476265685459533
Loss in iteration 198 : 0.39426160181054154
Loss in iteration 199 : 0.3935561788633843
Loss in iteration 200 : 0.3931905408160124
Testing accuracy  of updater 5 on alg 1 with rate 0.002401 = 0.779, training accuracy 0.8352217546131434, time elapsed: 3221 millisecond.
Loss in iteration 1 : 1.0000063107015837
Loss in iteration 2 : 0.5647066020255778
Loss in iteration 3 : 0.5546357346187017
Loss in iteration 4 : 0.549749297854867
Loss in iteration 5 : 0.5451378886539007
Loss in iteration 6 : 0.5405599090721628
Loss in iteration 7 : 0.5359389602571307
Loss in iteration 8 : 0.5312209461029674
Loss in iteration 9 : 0.5264119633748826
Loss in iteration 10 : 0.521541805776883
Loss in iteration 11 : 0.5165195076758906
Loss in iteration 12 : 0.5113542164014835
Loss in iteration 13 : 0.5060896652428427
Loss in iteration 14 : 0.500676641111217
Loss in iteration 15 : 0.49506097614266426
Loss in iteration 16 : 0.4892874309147968
Loss in iteration 17 : 0.4833222690701916
Loss in iteration 18 : 0.4771573894299163
Loss in iteration 19 : 0.47076123041482354
Loss in iteration 20 : 0.4641518870269962
Loss in iteration 21 : 0.45729842282581246
Loss in iteration 22 : 0.4501908149321697
Loss in iteration 23 : 0.4428705675717424
Loss in iteration 24 : 0.4354012590216763
Loss in iteration 25 : 0.428107052984497
Loss in iteration 26 : 0.421436413344616
Loss in iteration 27 : 0.4153174455436237
Loss in iteration 28 : 0.4097700824701369
Loss in iteration 29 : 0.40499421901174926
Loss in iteration 30 : 0.40099108091838115
Loss in iteration 31 : 0.3975892025742096
Loss in iteration 32 : 0.39475475066199484
Loss in iteration 33 : 0.39241139565153715
Loss in iteration 34 : 0.39041389550739847
Loss in iteration 35 : 0.3888411602178807
Loss in iteration 36 : 0.3873747097911764
Loss in iteration 37 : 0.3860695939795029
Loss in iteration 38 : 0.3847980271205537
Loss in iteration 39 : 0.38365779448735843
Loss in iteration 40 : 0.3826576371276326
Loss in iteration 41 : 0.38178644481129265
Loss in iteration 42 : 0.38094885806118944
Loss in iteration 43 : 0.3802520463947109
Loss in iteration 44 : 0.37978959053156863
Loss in iteration 45 : 0.3796585915302079
Loss in iteration 46 : 0.3797362923728458
Loss in iteration 47 : 0.37944083976648285
Loss in iteration 48 : 0.38029798179162744
Loss in iteration 49 : 0.3813595982399279
Loss in iteration 50 : 0.38099095508917785
Loss in iteration 51 : 0.38037497715647195
Loss in iteration 52 : 0.37853457456655226
Loss in iteration 53 : 0.37733690657152247
Loss in iteration 54 : 0.3767906932045424
Loss in iteration 55 : 0.37629961125712635
Loss in iteration 56 : 0.3760225831308561
Loss in iteration 57 : 0.3756736478363527
Loss in iteration 58 : 0.37551683632691196
Loss in iteration 59 : 0.3754000000651472
Loss in iteration 60 : 0.3754093339969801
Loss in iteration 61 : 0.37576125473877103
Loss in iteration 62 : 0.3766877356679552
Loss in iteration 63 : 0.37783599059480033
Loss in iteration 64 : 0.3794847955497886
Loss in iteration 65 : 0.37960871842092075
Loss in iteration 66 : 0.3787391470022664
Loss in iteration 67 : 0.37710448191560153
Loss in iteration 68 : 0.37689404403800425
Loss in iteration 69 : 0.37569885771686695
Loss in iteration 70 : 0.3759482056623821
Loss in iteration 71 : 0.3758330501578755
Loss in iteration 72 : 0.37655620406995666
Loss in iteration 73 : 0.3761197133649096
Loss in iteration 74 : 0.3768359085423518
Loss in iteration 75 : 0.37636305210203097
Loss in iteration 76 : 0.3769975460898954
Loss in iteration 77 : 0.3762793056980001
Loss in iteration 78 : 0.37705247236601824
Loss in iteration 79 : 0.37648888768351624
Loss in iteration 80 : 0.3773878453857381
Loss in iteration 81 : 0.376618848628856
Loss in iteration 82 : 0.37737715075302736
Loss in iteration 83 : 0.37643612894695
Loss in iteration 84 : 0.37735425510356035
Loss in iteration 85 : 0.3762851595762211
Loss in iteration 86 : 0.3771285731643846
Loss in iteration 87 : 0.3763550873896501
Loss in iteration 88 : 0.37730426874115336
Loss in iteration 89 : 0.37651090337450965
Loss in iteration 90 : 0.3774516049471999
Loss in iteration 91 : 0.37646078159378105
Loss in iteration 92 : 0.3773654448251429
Loss in iteration 93 : 0.3764189576000252
Loss in iteration 94 : 0.37723712185923886
Loss in iteration 95 : 0.37654481248433674
Loss in iteration 96 : 0.3773303500881407
Loss in iteration 97 : 0.3764724862721703
Loss in iteration 98 : 0.377303928297548
Loss in iteration 99 : 0.3765397894296846
Loss in iteration 100 : 0.3772791617535417
Loss in iteration 101 : 0.3766086132750622
Loss in iteration 102 : 0.37754142812341085
Loss in iteration 103 : 0.3763714199517714
Loss in iteration 104 : 0.3771914883199319
Loss in iteration 105 : 0.37680817126350263
Loss in iteration 106 : 0.3774561324126456
Loss in iteration 107 : 0.37656244314821385
Loss in iteration 108 : 0.3773043927430852
Loss in iteration 109 : 0.37679023391837907
Loss in iteration 110 : 0.3775730529695058
Loss in iteration 111 : 0.3765533636848191
Loss in iteration 112 : 0.3772465273568629
Loss in iteration 113 : 0.3768896872928562
Loss in iteration 114 : 0.3776170775179394
Loss in iteration 115 : 0.3766515244892088
Loss in iteration 116 : 0.37750358543618556
Loss in iteration 117 : 0.3768464328895269
Loss in iteration 118 : 0.37757300648777564
Loss in iteration 119 : 0.3768412764782002
Loss in iteration 120 : 0.37756268112695446
Loss in iteration 121 : 0.3769281964036439
Loss in iteration 122 : 0.3775535923024994
Loss in iteration 123 : 0.37701645690883046
Loss in iteration 124 : 0.37771618717173633
Loss in iteration 125 : 0.3769906621159361
Loss in iteration 126 : 0.3776318388672763
Loss in iteration 127 : 0.37708352594604977
Loss in iteration 128 : 0.3777179045297278
Loss in iteration 129 : 0.37715139068692
Loss in iteration 130 : 0.3777261799083776
Loss in iteration 131 : 0.3771311478955739
Loss in iteration 132 : 0.3778369389591167
Loss in iteration 133 : 0.3772028257615081
Loss in iteration 134 : 0.37784880742217336
Loss in iteration 135 : 0.3771338182755499
Loss in iteration 136 : 0.3778305193949882
Loss in iteration 137 : 0.37719661092461404
Loss in iteration 138 : 0.3778956851065703
Loss in iteration 139 : 0.37725807813358286
Loss in iteration 140 : 0.37810552198321856
Loss in iteration 141 : 0.3775050679710184
Loss in iteration 142 : 0.3780911884379139
Loss in iteration 143 : 0.37741255633470555
Loss in iteration 144 : 0.3781141527054645
Loss in iteration 145 : 0.3774965426877371
Loss in iteration 146 : 0.37813870816143014
Loss in iteration 147 : 0.3775375327054865
Loss in iteration 148 : 0.3782137099990095
Loss in iteration 149 : 0.3775795652758522
Loss in iteration 150 : 0.378289879126938
Loss in iteration 151 : 0.37762248462711323
Loss in iteration 152 : 0.37831873613927
Loss in iteration 153 : 0.37772008110012967
Loss in iteration 154 : 0.3783483919270419
Loss in iteration 155 : 0.377819067846876
Loss in iteration 156 : 0.37852899935703854
Loss in iteration 157 : 0.3778044588724795
Loss in iteration 158 : 0.3786542656634334
Loss in iteration 159 : 0.37779490992110953
Loss in iteration 160 : 0.3784631959306003
Loss in iteration 161 : 0.37797615589470385
Loss in iteration 162 : 0.3787685299454347
Loss in iteration 163 : 0.3780442575107282
Loss in iteration 164 : 0.3788105710506912
Loss in iteration 165 : 0.3781163239255679
Loss in iteration 166 : 0.378856403475506
Loss in iteration 167 : 0.3781915807934362
Loss in iteration 168 : 0.37883493394225726
Loss in iteration 169 : 0.378162342635654
Loss in iteration 170 : 0.37885340923014016
Loss in iteration 171 : 0.3783146651652944
Loss in iteration 172 : 0.3788700576208993
Loss in iteration 173 : 0.3783926586047607
Loss in iteration 174 : 0.3789675324686386
Loss in iteration 175 : 0.378469600716196
Loss in iteration 176 : 0.379064005711667
Loss in iteration 177 : 0.37862537636420396
Loss in iteration 178 : 0.37940812556092895
Loss in iteration 179 : 0.3786800404763302
Loss in iteration 180 : 0.3794595364945883
Loss in iteration 181 : 0.3787415709580859
Loss in iteration 182 : 0.3794220489209887
Loss in iteration 183 : 0.3786505527160577
Loss in iteration 184 : 0.37919471388957693
Loss in iteration 185 : 0.378774342611616
Loss in iteration 186 : 0.3795351518702617
Loss in iteration 187 : 0.3789193384978491
Loss in iteration 188 : 0.37984101680369103
Loss in iteration 189 : 0.3790860298366732
Loss in iteration 190 : 0.3798013363767342
Loss in iteration 191 : 0.3790282625958536
Loss in iteration 192 : 0.3797328630925229
Loss in iteration 193 : 0.3791137200225548
Loss in iteration 194 : 0.3798575775519682
Loss in iteration 195 : 0.3793751361912989
Loss in iteration 196 : 0.37999247814310705
Loss in iteration 197 : 0.37940990889288606
Loss in iteration 198 : 0.37997931240688154
Loss in iteration 199 : 0.37941741059644535
Loss in iteration 200 : 0.3801266451471179
Testing accuracy  of updater 5 on alg 1 with rate 0.001372 = 0.78275, training accuracy 0.8391065069601813, time elapsed: 3386 millisecond.
Loss in iteration 1 : 1.0000003932413382
Loss in iteration 2 : 0.8171424526667732
Loss in iteration 3 : 0.6938703397049291
Loss in iteration 4 : 0.6277471353897609
Testing accuracy  of updater 5 on alg 1 with rate 3.43E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 92 millisecond.
Loss in iteration 1 : 1.0003403593083502
Loss in iteration 2 : 1.6854590226269426
Loss in iteration 3 : 2.172154226005889
Loss in iteration 4 : 2.18548859427568
Loss in iteration 5 : 1.8942091364071088
Loss in iteration 6 : 1.3901947398981438
Loss in iteration 7 : 0.7555172327522982
Loss in iteration 8 : 0.4599087732975045
Loss in iteration 9 : 0.7362997452300993
Loss in iteration 10 : 1.070514668074686
Loss in iteration 11 : 1.0794812156073055
Loss in iteration 12 : 0.8489248140755147
Loss in iteration 13 : 0.6290270997489037
Loss in iteration 14 : 0.5586221816131309
Loss in iteration 15 : 0.6178326858861437
Loss in iteration 16 : 0.7193570089885
Loss in iteration 17 : 0.7953351599909634
Loss in iteration 18 : 0.8158471815694691
Loss in iteration 19 : 0.7817447408158215
Loss in iteration 20 : 0.7131862915626567
Loss in iteration 21 : 0.6434899184556023
Loss in iteration 22 : 0.6032552110040339
Loss in iteration 23 : 0.610443394935636
Loss in iteration 24 : 0.6513305077546332
Loss in iteration 25 : 0.6881787227476616
Loss in iteration 26 : 0.6914231003460877
Loss in iteration 27 : 0.6583994883909374
Loss in iteration 28 : 0.6118874794334307
Loss in iteration 29 : 0.5807660891349748
Loss in iteration 30 : 0.5781242805179931
Loss in iteration 31 : 0.5938530840551192
Loss in iteration 32 : 0.6069538238932755
Loss in iteration 33 : 0.602340029785201
Loss in iteration 34 : 0.5777459754582009
Loss in iteration 35 : 0.5462474327538673
Loss in iteration 36 : 0.5278793565447816
Loss in iteration 37 : 0.530506867217449
Loss in iteration 38 : 0.540203124940906
Loss in iteration 39 : 0.5349506599413257
Loss in iteration 40 : 0.5098404193901515
Loss in iteration 41 : 0.48636899901043656
Loss in iteration 42 : 0.48112206617728526
Loss in iteration 43 : 0.4889037983047844
Loss in iteration 44 : 0.4834027310746196
Loss in iteration 45 : 0.46139912285513196
Loss in iteration 46 : 0.4485979186204312
Loss in iteration 47 : 0.45672102468364734
Loss in iteration 48 : 0.45730276682841525
Loss in iteration 49 : 0.4363168703911841
Loss in iteration 50 : 0.43823264081007
Loss in iteration 51 : 0.4490512335306719
Loss in iteration 52 : 0.4360545321983187
Loss in iteration 53 : 0.4441156876525977
Loss in iteration 54 : 0.4522218883847415
Loss in iteration 55 : 0.442769064043589
Loss in iteration 56 : 0.45985869607691365
Loss in iteration 57 : 0.44928507121217953
Loss in iteration 58 : 0.45848591127587257
Loss in iteration 59 : 0.4537890978610313
Loss in iteration 60 : 0.4535797246750881
Loss in iteration 61 : 0.45673314773324364
Loss in iteration 62 : 0.4509607936275465
Loss in iteration 63 : 0.4565252339505525
Loss in iteration 64 : 0.45426957425655357
Loss in iteration 65 : 0.45483162290042667
Loss in iteration 66 : 0.45897646691973215
Loss in iteration 67 : 0.45787361365636925
Loss in iteration 68 : 0.4599822575959304
Loss in iteration 69 : 0.4624711176351638
Loss in iteration 70 : 0.4617700972163345
Loss in iteration 71 : 0.46431420675133367
Loss in iteration 72 : 0.4650428786502605
Loss in iteration 73 : 0.4653140183855208
Loss in iteration 74 : 0.4672694795245931
Loss in iteration 75 : 0.4668674920076495
Loss in iteration 76 : 0.46762608551477475
Loss in iteration 77 : 0.4687489354859706
Loss in iteration 78 : 0.468665918478699
Loss in iteration 79 : 0.46958534251705836
Loss in iteration 80 : 0.47018746933351774
Loss in iteration 81 : 0.47019389557142954
Loss in iteration 82 : 0.47133068242912884
Loss in iteration 83 : 0.47162574923838396
Loss in iteration 84 : 0.472307660453957
Loss in iteration 85 : 0.472852507458444
Loss in iteration 86 : 0.47316828758035573
Loss in iteration 87 : 0.47385737858280075
Loss in iteration 88 : 0.474135505946526
Loss in iteration 89 : 0.47468009535318134
Loss in iteration 90 : 0.4748890581204518
Loss in iteration 91 : 0.47528179099421397
Loss in iteration 92 : 0.4756368958827449
Loss in iteration 93 : 0.47590141199668134
Loss in iteration 94 : 0.4761880129523826
Loss in iteration 95 : 0.47639077033010513
Loss in iteration 96 : 0.47676864843314504
Loss in iteration 97 : 0.47701951889886046
Loss in iteration 98 : 0.4773032422656521
Loss in iteration 99 : 0.4775240144317896
Testing accuracy  of updater 6 on alg 1 with rate 0.0392 = 0.7895, training accuracy 0.8452573648429913, time elapsed: 1727 millisecond.
Loss in iteration 1 : 1.0000917620069665
Loss in iteration 2 : 0.9710406986205121
Loss in iteration 3 : 1.2842920173122496
Loss in iteration 4 : 1.3337006752177116
Loss in iteration 5 : 1.1548529854848517
Loss in iteration 6 : 0.7797490449580264
Loss in iteration 7 : 0.4252390159203699
Loss in iteration 8 : 0.5409041399064396
Loss in iteration 9 : 0.7996095888966155
Loss in iteration 10 : 0.8198876524137999
Loss in iteration 11 : 0.6487789131026336
Loss in iteration 12 : 0.5179512487236332
Loss in iteration 13 : 0.5156860971569038
Loss in iteration 14 : 0.5942125009360566
Loss in iteration 15 : 0.6692603894400108
Loss in iteration 16 : 0.6959565669545141
Loss in iteration 17 : 0.6711742454841986
Loss in iteration 18 : 0.6174810121716561
Loss in iteration 19 : 0.5699828812790205
Loss in iteration 20 : 0.5546353074401662
Loss in iteration 21 : 0.5789826925818635
Loss in iteration 22 : 0.6149268331290735
Loss in iteration 23 : 0.6310925044029008
Loss in iteration 24 : 0.6155710524857158
Loss in iteration 25 : 0.581108291768238
Loss in iteration 26 : 0.552812896491357
Loss in iteration 27 : 0.5461887185106533
Loss in iteration 28 : 0.5580479513242749
Loss in iteration 29 : 0.5698628482414947
Loss in iteration 30 : 0.5679951819659871
Loss in iteration 31 : 0.5497113308898289
Loss in iteration 32 : 0.5252200234507951
Loss in iteration 33 : 0.5108922379779178
Loss in iteration 34 : 0.5118363783969893
Loss in iteration 35 : 0.5178123163544771
Loss in iteration 36 : 0.5138157026300836
Loss in iteration 37 : 0.4962709470770873
Loss in iteration 38 : 0.4782189537092484
Loss in iteration 39 : 0.471646975012447
Loss in iteration 40 : 0.4753163808980776
Loss in iteration 41 : 0.47388899317635597
Loss in iteration 42 : 0.4599829213915016
Loss in iteration 43 : 0.44640889812146084
Loss in iteration 44 : 0.44509448217775455
Loss in iteration 45 : 0.44908004075507557
Loss in iteration 46 : 0.440781034020204
Loss in iteration 47 : 0.4297673224640668
Loss in iteration 48 : 0.43330207255108355
Loss in iteration 49 : 0.43777985189881924
Loss in iteration 50 : 0.4300422257115978
Loss in iteration 51 : 0.4332189944115298
Loss in iteration 52 : 0.4411772362414546
Loss in iteration 53 : 0.43504215920566053
Loss in iteration 54 : 0.4423533475133549
Loss in iteration 55 : 0.4465153866560297
Loss in iteration 56 : 0.4422738666714914
Loss in iteration 57 : 0.4499605125198559
Loss in iteration 58 : 0.44664838999769174
Loss in iteration 59 : 0.44676603341866705
Loss in iteration 60 : 0.4505972375220788
Loss in iteration 61 : 0.4467527818281153
Loss in iteration 62 : 0.4485551090430461
Loss in iteration 63 : 0.4502531850796205
Loss in iteration 64 : 0.44848621536814715
Loss in iteration 65 : 0.4509756642268993
Loss in iteration 66 : 0.45268075974022715
Loss in iteration 67 : 0.4523886681078357
Loss in iteration 68 : 0.45432221578680754
Loss in iteration 69 : 0.45599128366932057
Loss in iteration 70 : 0.4561913514989611
Loss in iteration 71 : 0.4574604616145091
Loss in iteration 72 : 0.45911835717667604
Loss in iteration 73 : 0.45937157818658153
Loss in iteration 74 : 0.46036305040412207
Loss in iteration 75 : 0.46163450654383464
Loss in iteration 76 : 0.4618852032457962
Loss in iteration 77 : 0.4624798184585563
Loss in iteration 78 : 0.46348817333824444
Loss in iteration 79 : 0.46392294810222506
Loss in iteration 80 : 0.4643790230341549
Loss in iteration 81 : 0.465175750681461
Loss in iteration 82 : 0.4656942848553817
Loss in iteration 83 : 0.46603592329198074
Loss in iteration 84 : 0.4666987823219522
Loss in iteration 85 : 0.4673340023185596
Loss in iteration 86 : 0.4677492541682719
Loss in iteration 87 : 0.4684428469515759
Loss in iteration 88 : 0.4688594840075983
Loss in iteration 89 : 0.4692415490929558
Loss in iteration 90 : 0.46985663238109815
Loss in iteration 91 : 0.4702420427099825
Loss in iteration 92 : 0.47074189881057643
Loss in iteration 93 : 0.47117594836228305
Loss in iteration 94 : 0.4714387095774369
Loss in iteration 95 : 0.47194877022473436
Loss in iteration 96 : 0.4722655359476186
Loss in iteration 97 : 0.47260823594920054
Loss in iteration 98 : 0.47291781445150105
Loss in iteration 99 : 0.47316055764114473
Loss in iteration 100 : 0.47353713861557306
Loss in iteration 101 : 0.4738082792438824
Loss in iteration 102 : 0.4741010445338792
Loss in iteration 103 : 0.47437454870502144
Loss in iteration 104 : 0.47461525917334846
Loss in iteration 105 : 0.47488897746025927
Loss in iteration 106 : 0.47513740188649073
Loss in iteration 107 : 0.47538577253485753
Loss in iteration 108 : 0.47562131308656813
Loss in iteration 109 : 0.4758391906940839
Loss in iteration 110 : 0.4760703677159072
Loss in iteration 111 : 0.4762811766362545
Testing accuracy  of updater 6 on alg 1 with rate 0.02744 = 0.78925, training accuracy 0.8442861767562317, time elapsed: 1877 millisecond.
Loss in iteration 1 : 1.0000388955573787
Loss in iteration 2 : 0.7523334928498424
Loss in iteration 3 : 0.9574099541593313
Loss in iteration 4 : 0.9917951446876627
Loss in iteration 5 : 0.8766017502781694
Loss in iteration 6 : 0.6319241522534728
Loss in iteration 7 : 0.38678053693854986
Loss in iteration 8 : 0.4756339420318097
Loss in iteration 9 : 0.6395793014593328
Loss in iteration 10 : 0.639413154499378
Loss in iteration 11 : 0.5202983786223032
Loss in iteration 12 : 0.43272812548330203
Loss in iteration 13 : 0.4324382178410665
Loss in iteration 14 : 0.4882913075256568
Loss in iteration 15 : 0.537990772734318
Loss in iteration 16 : 0.5515985512845648
Loss in iteration 17 : 0.5303592915575991
Loss in iteration 18 : 0.4925290630696093
Loss in iteration 19 : 0.4642620489900912
Loss in iteration 20 : 0.46191414083746085
Loss in iteration 21 : 0.48092044141336804
Loss in iteration 22 : 0.5031070230074144
Loss in iteration 23 : 0.5116038275037703
Loss in iteration 24 : 0.5018780554173433
Loss in iteration 25 : 0.4820517679365373
Loss in iteration 26 : 0.46578666856940004
Loss in iteration 27 : 0.4611051459378479
Loss in iteration 28 : 0.46753266085372525
Loss in iteration 29 : 0.4763756401184667
Loss in iteration 30 : 0.47805926463589415
Loss in iteration 31 : 0.47005767597714826
Loss in iteration 32 : 0.4566313180631856
Loss in iteration 33 : 0.44690856173755117
Loss in iteration 34 : 0.44499242649310683
Loss in iteration 35 : 0.4474705110326977
Loss in iteration 36 : 0.44927958958423325
Loss in iteration 37 : 0.44487632236396
Loss in iteration 38 : 0.4357128906818676
Loss in iteration 39 : 0.4277260313194196
Loss in iteration 40 : 0.42512267743992066
Loss in iteration 41 : 0.42591709667477523
Loss in iteration 42 : 0.42597343446207697
Loss in iteration 43 : 0.42203053332161117
Loss in iteration 44 : 0.4157450753334072
Loss in iteration 45 : 0.4109205069599739
Loss in iteration 46 : 0.4104148536920856
Loss in iteration 47 : 0.4119075041753981
Loss in iteration 48 : 0.40990270298887443
Loss in iteration 49 : 0.4053242414335336
Loss in iteration 50 : 0.4038216876278005
Loss in iteration 51 : 0.4060662933384567
Loss in iteration 52 : 0.4068469647277829
Loss in iteration 53 : 0.40470567388792306
Loss in iteration 54 : 0.4048754795389012
Loss in iteration 55 : 0.4081605756150777
Loss in iteration 56 : 0.40925881195278846
Loss in iteration 57 : 0.4083019020903704
Loss in iteration 58 : 0.4105552647747422
Loss in iteration 59 : 0.41317689585451367
Loss in iteration 60 : 0.41371156689386485
Loss in iteration 61 : 0.41389348135565096
Loss in iteration 62 : 0.4161973631263873
Loss in iteration 63 : 0.41751376219956393
Loss in iteration 64 : 0.4171821315491261
Loss in iteration 65 : 0.4183847029323292
Loss in iteration 66 : 0.41995324492421904
Loss in iteration 67 : 0.420310593682502
Loss in iteration 68 : 0.42057427476918474
Loss in iteration 69 : 0.42181098614153767
Loss in iteration 70 : 0.42294436053553824
Loss in iteration 71 : 0.4233352310818395
Loss in iteration 72 : 0.4241573462204592
Loss in iteration 73 : 0.4254248563332591
Loss in iteration 74 : 0.4264547037871001
Loss in iteration 75 : 0.4271354393954964
Loss in iteration 76 : 0.42812135580669475
Loss in iteration 77 : 0.4293017227667444
Loss in iteration 78 : 0.43027317566892204
Loss in iteration 79 : 0.4310291543086196
Loss in iteration 80 : 0.4319367737272905
Loss in iteration 81 : 0.43296048845936924
Loss in iteration 82 : 0.43387749946078313
Loss in iteration 83 : 0.43466808159106834
Loss in iteration 84 : 0.43547100931487703
Loss in iteration 85 : 0.43638618629237497
Loss in iteration 86 : 0.43717115864725953
Loss in iteration 87 : 0.43784889675844446
Loss in iteration 88 : 0.43861628483270304
Loss in iteration 89 : 0.4394076330162651
Loss in iteration 90 : 0.4400965210111258
Loss in iteration 91 : 0.44077906744878625
Loss in iteration 92 : 0.4415102651430579
Loss in iteration 93 : 0.44218372620286306
Loss in iteration 94 : 0.44279984508051523
Loss in iteration 95 : 0.44346786947952077
Loss in iteration 96 : 0.44414756079348405
Loss in iteration 97 : 0.44478949979164023
Loss in iteration 98 : 0.4454329398667538
Loss in iteration 99 : 0.44610365880691877
Loss in iteration 100 : 0.446743357733675
Loss in iteration 101 : 0.44735117344265524
Loss in iteration 102 : 0.44798859359180876
Loss in iteration 103 : 0.44863091820642714
Loss in iteration 104 : 0.44924508404638025
Loss in iteration 105 : 0.4498579312348679
Loss in iteration 106 : 0.4504750699188011
Loss in iteration 107 : 0.4510647236680419
Loss in iteration 108 : 0.4516579739786845
Loss in iteration 109 : 0.4522571605299243
Loss in iteration 110 : 0.452843341161857
Loss in iteration 111 : 0.4534171949725824
Loss in iteration 112 : 0.4539829125345006
Loss in iteration 113 : 0.4545326566868726
Loss in iteration 114 : 0.4550697148743249
Loss in iteration 115 : 0.45560163115818797
Loss in iteration 116 : 0.45612870128794925
Loss in iteration 117 : 0.45664817864248897
Loss in iteration 118 : 0.45715421703615444
Loss in iteration 119 : 0.45764565170132704
Loss in iteration 120 : 0.4581234906661623
Loss in iteration 121 : 0.45858974407883724
Loss in iteration 122 : 0.4590471424484317
Loss in iteration 123 : 0.4594946640515142
Loss in iteration 124 : 0.4599334687320226
Loss in iteration 125 : 0.4603627147032877
Loss in iteration 126 : 0.4607832326390266
Loss in iteration 127 : 0.46119576240482785
Loss in iteration 128 : 0.46160116672468543
Loss in iteration 129 : 0.46199830900209216
Loss in iteration 130 : 0.46238659540300175
Loss in iteration 131 : 0.46276364182032537
Loss in iteration 132 : 0.46312948433052853
Loss in iteration 133 : 0.46348166131737406
Loss in iteration 134 : 0.4638259605159391
Loss in iteration 135 : 0.46416485808045116
Loss in iteration 136 : 0.4644979033249368
Loss in iteration 137 : 0.4648257500057989
Loss in iteration 138 : 0.46514898889728173
Loss in iteration 139 : 0.4654654475475575
Loss in iteration 140 : 0.4657756021932744
Loss in iteration 141 : 0.4660810630511022
Loss in iteration 142 : 0.46638323198156023
Loss in iteration 143 : 0.4666822206148179
Loss in iteration 144 : 0.46697842291930414
Loss in iteration 145 : 0.4672677105864616
Loss in iteration 146 : 0.4675506646674281
Loss in iteration 147 : 0.46782789710240763
Loss in iteration 148 : 0.4681000460296292
Loss in iteration 149 : 0.46836757603351936
Loss in iteration 150 : 0.46862921050237305
Loss in iteration 151 : 0.4688875318829653
Loss in iteration 152 : 0.4691412726740034
Loss in iteration 153 : 0.4693923120360307
Loss in iteration 154 : 0.4696414521806418
Loss in iteration 155 : 0.46988853791899043
Loss in iteration 156 : 0.4701326359512556
Loss in iteration 157 : 0.4703692296735736
Loss in iteration 158 : 0.4705994280736922
Loss in iteration 159 : 0.4708237288151233
Loss in iteration 160 : 0.4710427875187373
Loss in iteration 161 : 0.4712571961464201
Loss in iteration 162 : 0.4714674891025846
Testing accuracy  of updater 6 on alg 1 with rate 0.01568 = 0.7885, training accuracy 0.8429912593072192, time elapsed: 2612 millisecond.
Loss in iteration 1 : 1.0000031754700225
Loss in iteration 2 : 0.582294522378948
Loss in iteration 3 : 0.6123626630817766
Loss in iteration 4 : 0.6984280804365209
Loss in iteration 5 : 0.7343249959482636
Loss in iteration 6 : 0.7247755554774522
Loss in iteration 7 : 0.6752122499555269
Loss in iteration 8 : 0.5916712322157535
Loss in iteration 9 : 0.4901716771999773
Loss in iteration 10 : 0.4165680784120238
Loss in iteration 11 : 0.4215757100547221
Loss in iteration 12 : 0.4771738582032737
Loss in iteration 13 : 0.5050159776041667
Loss in iteration 14 : 0.4839899053520084
Loss in iteration 15 : 0.4353817919371681
Loss in iteration 16 : 0.3923369654991747
Loss in iteration 17 : 0.37674080728278375
Loss in iteration 18 : 0.3849115683009043
Loss in iteration 19 : 0.4022321664614621
Loss in iteration 20 : 0.41563302787242873
Loss in iteration 21 : 0.4184909468984963
Loss in iteration 22 : 0.41111867245352784
Loss in iteration 23 : 0.39836037483896136
Loss in iteration 24 : 0.3862011778338464
Loss in iteration 25 : 0.3795775829558113
Loss in iteration 26 : 0.3793961635992937
Loss in iteration 27 : 0.3843955288240438
Loss in iteration 28 : 0.3907212608728434
Loss in iteration 29 : 0.39509836819355054
Loss in iteration 30 : 0.3959366381776882
Loss in iteration 31 : 0.39342981759207113
Loss in iteration 32 : 0.3889829105406181
Loss in iteration 33 : 0.38459720765391164
Loss in iteration 34 : 0.38223337747023955
Loss in iteration 35 : 0.3822612196125785
Loss in iteration 36 : 0.3837865894485269
Loss in iteration 37 : 0.38580146088862943
Loss in iteration 38 : 0.38742302701916354
Loss in iteration 39 : 0.38784778759907473
Loss in iteration 40 : 0.38699169130158745
Loss in iteration 41 : 0.3853754531926845
Loss in iteration 42 : 0.38373930732935846
Loss in iteration 43 : 0.38247601468434356
Loss in iteration 44 : 0.38200293717130807
Loss in iteration 45 : 0.3821740768761204
Loss in iteration 46 : 0.3828580243766388
Loss in iteration 47 : 0.38353148324744446
Loss in iteration 48 : 0.38364632630358425
Loss in iteration 49 : 0.38312182493871205
Loss in iteration 50 : 0.382195533572291
Loss in iteration 51 : 0.38139513790452395
Loss in iteration 52 : 0.3808957158915797
Loss in iteration 53 : 0.38079422475460145
Loss in iteration 54 : 0.38096517469013685
Loss in iteration 55 : 0.38121396050332684
Loss in iteration 56 : 0.3812812144178757
Loss in iteration 57 : 0.38109829111576043
Loss in iteration 58 : 0.3807330218286607
Loss in iteration 59 : 0.3803259718035621
Loss in iteration 60 : 0.38002686444746825
Loss in iteration 61 : 0.3799653660155106
Loss in iteration 62 : 0.380087119966354
Loss in iteration 63 : 0.38025970044193363
Loss in iteration 64 : 0.3803359354379208
Loss in iteration 65 : 0.38026311015032543
Loss in iteration 66 : 0.380109568065329
Loss in iteration 67 : 0.380005991655526
Loss in iteration 68 : 0.37999463962343355
Loss in iteration 69 : 0.3800907705440506
Loss in iteration 70 : 0.3802535054154723
Loss in iteration 71 : 0.3803931239759782
Loss in iteration 72 : 0.3804680412309552
Loss in iteration 73 : 0.3805091453446211
Loss in iteration 74 : 0.38054412041669605
Loss in iteration 75 : 0.3806409845264032
Loss in iteration 76 : 0.3808128332680819
Loss in iteration 77 : 0.3809916622890525
Loss in iteration 78 : 0.38116394273104465
Loss in iteration 79 : 0.3813144658788193
Loss in iteration 80 : 0.38143948550855655
Loss in iteration 81 : 0.38157321796713584
Loss in iteration 82 : 0.3817257568429407
Loss in iteration 83 : 0.3818931761902157
Loss in iteration 84 : 0.38207975877783784
Loss in iteration 85 : 0.38227937084737024
Loss in iteration 86 : 0.3824755697365644
Loss in iteration 87 : 0.3826595536361433
Loss in iteration 88 : 0.3828401874935698
Loss in iteration 89 : 0.38302127698900135
Loss in iteration 90 : 0.38321808957913767
Loss in iteration 91 : 0.38343679606957415
Loss in iteration 92 : 0.38365276428092715
Loss in iteration 93 : 0.3838623370719201
Loss in iteration 94 : 0.3840657525345851
Loss in iteration 95 : 0.38426864914674935
Loss in iteration 96 : 0.3844727826824755
Loss in iteration 97 : 0.38468455412819214
Loss in iteration 98 : 0.38490277564247777
Loss in iteration 99 : 0.38512630324213815
Loss in iteration 100 : 0.3853501748861399
Loss in iteration 101 : 0.38556771594184375
Loss in iteration 102 : 0.38578430121561064
Loss in iteration 103 : 0.3860029517428336
Loss in iteration 104 : 0.3862227426735211
Loss in iteration 105 : 0.38644402181278215
Loss in iteration 106 : 0.3866725684873442
Loss in iteration 107 : 0.38690256369944265
Loss in iteration 108 : 0.38713095542546716
Loss in iteration 109 : 0.38735806750781404
Loss in iteration 110 : 0.38758456013809295
Loss in iteration 111 : 0.3878135206172956
Loss in iteration 112 : 0.38804507033351887
Loss in iteration 113 : 0.3882788191320272
Loss in iteration 114 : 0.3885150235278231
Loss in iteration 115 : 0.38875108491639093
Loss in iteration 116 : 0.3889861670693276
Loss in iteration 117 : 0.3892238283253982
Loss in iteration 118 : 0.3894630744070893
Loss in iteration 119 : 0.3897043878810199
Loss in iteration 120 : 0.3899456814842342
Loss in iteration 121 : 0.3901871048260769
Loss in iteration 122 : 0.3904289899564539
Loss in iteration 123 : 0.39067228311651
Loss in iteration 124 : 0.39091716292973133
Loss in iteration 125 : 0.39116229406766545
Loss in iteration 126 : 0.3914069602948291
Loss in iteration 127 : 0.3916532684382294
Loss in iteration 128 : 0.3919002484989193
Loss in iteration 129 : 0.39214813840193674
Loss in iteration 130 : 0.3923964372691896
Loss in iteration 131 : 0.3926454953837401
Loss in iteration 132 : 0.3928949310813385
Loss in iteration 133 : 0.3931447740730083
Loss in iteration 134 : 0.3933953124842532
Loss in iteration 135 : 0.3936463017753937
Loss in iteration 136 : 0.39389747139515385
Loss in iteration 137 : 0.39414916987936544
Loss in iteration 138 : 0.3944013537416818
Loss in iteration 139 : 0.3946536801796112
Loss in iteration 140 : 0.3949062034707583
Loss in iteration 141 : 0.39515892604017533
Loss in iteration 142 : 0.39541184690644676
Loss in iteration 143 : 0.39566496308591864
Loss in iteration 144 : 0.3959182400976862
Loss in iteration 145 : 0.396171371259506
Loss in iteration 146 : 0.3964244865566148
Loss in iteration 147 : 0.3966774214680361
Loss in iteration 148 : 0.3969300186261699
Loss in iteration 149 : 0.39718320206332136
Loss in iteration 150 : 0.39743652911861155
Loss in iteration 151 : 0.39768985529826084
Loss in iteration 152 : 0.3979426523386359
Loss in iteration 153 : 0.39819596690605935
Loss in iteration 154 : 0.3984476994955294
Loss in iteration 155 : 0.3986989397151801
Loss in iteration 156 : 0.3989519087968534
Loss in iteration 157 : 0.3992050880681127
Loss in iteration 158 : 0.399458390289159
Loss in iteration 159 : 0.39971154293822814
Loss in iteration 160 : 0.39996454595123015
Loss in iteration 161 : 0.40021696365930576
Loss in iteration 162 : 0.4004687883589475
Loss in iteration 163 : 0.40072065234699494
Loss in iteration 164 : 0.4009725661624632
Loss in iteration 165 : 0.40122459497679325
Loss in iteration 166 : 0.4014768240665606
Loss in iteration 167 : 0.40173067684823527
Loss in iteration 168 : 0.4019853330213331
Loss in iteration 169 : 0.4022407741846274
Loss in iteration 170 : 0.4024968942120547
Loss in iteration 171 : 0.40275374991024115
Loss in iteration 172 : 0.40301104550765443
Loss in iteration 173 : 0.40326871580400653
Loss in iteration 174 : 0.40352682124938444
Loss in iteration 175 : 0.4037852112510107
Loss in iteration 176 : 0.40404407341363113
Loss in iteration 177 : 0.40430280481326625
Loss in iteration 178 : 0.40456179884620713
Loss in iteration 179 : 0.40482063958916464
Loss in iteration 180 : 0.4050791541664117
Loss in iteration 181 : 0.40533746490359673
Loss in iteration 182 : 0.4055950203514718
Loss in iteration 183 : 0.4058519294727826
Loss in iteration 184 : 0.40610833809897384
Loss in iteration 185 : 0.406364378144015
Loss in iteration 186 : 0.40661998326222604
Loss in iteration 187 : 0.4068751537188438
Loss in iteration 188 : 0.40712866667992226
Loss in iteration 189 : 0.40738138475508817
Loss in iteration 190 : 0.40763495537235617
Loss in iteration 191 : 0.4078882239391561
Loss in iteration 192 : 0.40814127996353594
Loss in iteration 193 : 0.408394035381203
Loss in iteration 194 : 0.4086465691917966
Loss in iteration 195 : 0.40889897971387135
Loss in iteration 196 : 0.4091510235517671
Loss in iteration 197 : 0.4094029473672755
Loss in iteration 198 : 0.4096543803809797
Loss in iteration 199 : 0.4099049935714388
Loss in iteration 200 : 0.4101552488317596
Testing accuracy  of updater 6 on alg 1 with rate 0.00392 = 0.787, training accuracy 0.8413726124959534, time elapsed: 3038 millisecond.
Loss in iteration 1 : 1.0000020936689338
Loss in iteration 2 : 0.6405410645766048
Loss in iteration 3 : 0.576898342709624
Loss in iteration 4 : 0.662529593212654
Loss in iteration 5 : 0.7118438704268948
Loss in iteration 6 : 0.724652327677542
Loss in iteration 7 : 0.7048423262155905
Loss in iteration 8 : 0.6563957118942907
Loss in iteration 9 : 0.5838128348473041
Loss in iteration 10 : 0.4993032377861182
Loss in iteration 11 : 0.43290295990272465
Loss in iteration 12 : 0.41302913477482533
Loss in iteration 13 : 0.4470101515649717
Loss in iteration 14 : 0.48466453766653905
Loss in iteration 15 : 0.4918232903563111
Loss in iteration 16 : 0.46703931076565164
Loss in iteration 17 : 0.42684257146354393
Loss in iteration 18 : 0.3932328155646929
Loss in iteration 19 : 0.3788238232823787
Loss in iteration 20 : 0.3821222119338045
Loss in iteration 21 : 0.3942333695310005
Loss in iteration 22 : 0.40577129867416056
Loss in iteration 23 : 0.41113891793230933
Loss in iteration 24 : 0.40883155463716675
Loss in iteration 25 : 0.40069741574980944
Loss in iteration 26 : 0.390498411849068
Loss in iteration 27 : 0.381763719858476
Loss in iteration 28 : 0.377000114229406
Loss in iteration 29 : 0.37661222887411916
Loss in iteration 30 : 0.37983025389639796
Loss in iteration 31 : 0.38430250200499344
Loss in iteration 32 : 0.38778273638439825
Loss in iteration 33 : 0.38910167874244983
Loss in iteration 34 : 0.38804279452002305
Loss in iteration 35 : 0.3854582394488465
Loss in iteration 36 : 0.38219668633404175
Loss in iteration 37 : 0.37938695311530496
Loss in iteration 38 : 0.378052181507456
Loss in iteration 39 : 0.3782297278257797
Loss in iteration 40 : 0.3792975460821926
Loss in iteration 41 : 0.38069214494358344
Loss in iteration 42 : 0.38178669059975484
Loss in iteration 43 : 0.38226417637545085
Loss in iteration 44 : 0.38201435162519365
Loss in iteration 45 : 0.38120832159691687
Loss in iteration 46 : 0.3801010506936975
Loss in iteration 47 : 0.3791134134918236
Loss in iteration 48 : 0.37842149924698804
Loss in iteration 49 : 0.3781370892488626
Loss in iteration 50 : 0.37835421657492296
Loss in iteration 51 : 0.37883373588232083
Loss in iteration 52 : 0.3792828956302577
Loss in iteration 53 : 0.37946288748645574
Loss in iteration 54 : 0.3793283961429422
Loss in iteration 55 : 0.37892743885070856
Loss in iteration 56 : 0.3784198073568233
Loss in iteration 57 : 0.37799550526538905
Loss in iteration 58 : 0.3777322394853848
Loss in iteration 59 : 0.3777166483293744
Loss in iteration 60 : 0.37786321446812543
Loss in iteration 61 : 0.3780152977103901
Loss in iteration 62 : 0.37811554045998264
Loss in iteration 63 : 0.3780883771555741
Loss in iteration 64 : 0.3779532984052801
Loss in iteration 65 : 0.37775886872326525
Loss in iteration 66 : 0.37757250539821025
Loss in iteration 67 : 0.377477535281323
Loss in iteration 68 : 0.3774786929614061
Loss in iteration 69 : 0.3775730683679774
Loss in iteration 70 : 0.3776750873846978
Loss in iteration 71 : 0.3777362625778269
Loss in iteration 72 : 0.3777451768266965
Loss in iteration 73 : 0.37771757030293795
Loss in iteration 74 : 0.37766401409387607
Loss in iteration 75 : 0.3776217258734118
Loss in iteration 76 : 0.3776308390621566
Loss in iteration 77 : 0.37767482893609194
Loss in iteration 78 : 0.3777569048831058
Loss in iteration 79 : 0.3778480142436017
Loss in iteration 80 : 0.3779149470480126
Loss in iteration 81 : 0.37795819835931427
Loss in iteration 82 : 0.37799273743627015
Loss in iteration 83 : 0.37803123615104683
Loss in iteration 84 : 0.3780761336203066
Loss in iteration 85 : 0.3781415292661594
Loss in iteration 86 : 0.3782258719271812
Loss in iteration 87 : 0.3783199803702665
Loss in iteration 88 : 0.3784110093033359
Loss in iteration 89 : 0.3784953039497851
Loss in iteration 90 : 0.3785722376996783
Loss in iteration 91 : 0.3786469840596679
Loss in iteration 92 : 0.37872601719547905
Loss in iteration 93 : 0.37880880114680815
Loss in iteration 94 : 0.3789034288234857
Loss in iteration 95 : 0.37900744675267145
Loss in iteration 96 : 0.37911312666423524
Loss in iteration 97 : 0.37921600312606396
Loss in iteration 98 : 0.3793143942662062
Loss in iteration 99 : 0.37941262027136985
Loss in iteration 100 : 0.37951278439667724
Loss in iteration 101 : 0.3796174491290693
Loss in iteration 102 : 0.3797249731261805
Loss in iteration 103 : 0.37983599377806476
Loss in iteration 104 : 0.37994811323804817
Loss in iteration 105 : 0.3800605763706307
Loss in iteration 106 : 0.38017280926265745
Loss in iteration 107 : 0.3802846153399684
Loss in iteration 108 : 0.38039733829411887
Loss in iteration 109 : 0.3805113800339476
Loss in iteration 110 : 0.38062755975921886
Loss in iteration 111 : 0.38074585765165775
Loss in iteration 112 : 0.3808651425973123
Loss in iteration 113 : 0.38098591163109297
Loss in iteration 114 : 0.38110784775078504
Loss in iteration 115 : 0.38123091690072136
Loss in iteration 116 : 0.38135512825008366
Loss in iteration 117 : 0.381480668297415
Loss in iteration 118 : 0.3816072118090691
Loss in iteration 119 : 0.38173403504597087
Loss in iteration 120 : 0.38186178500724877
Loss in iteration 121 : 0.38199050747237984
Loss in iteration 122 : 0.382119876474629
Loss in iteration 123 : 0.3822498085578745
Loss in iteration 124 : 0.38238033525090787
Loss in iteration 125 : 0.38251139432694575
Loss in iteration 126 : 0.38264319215542325
Loss in iteration 127 : 0.38277582935200016
Loss in iteration 128 : 0.38290887538872326
Loss in iteration 129 : 0.3830425645490059
Loss in iteration 130 : 0.3831773156342525
Loss in iteration 131 : 0.38331237962947656
Loss in iteration 132 : 0.38344780681662094
Loss in iteration 133 : 0.38358364273463397
Loss in iteration 134 : 0.38371992861456017
Loss in iteration 135 : 0.38385670177568904
Loss in iteration 136 : 0.3839939959860735
Loss in iteration 137 : 0.38413184179048065
Loss in iteration 138 : 0.38427026680860293
Loss in iteration 139 : 0.3844093402917559
Loss in iteration 140 : 0.3845496202145168
Loss in iteration 141 : 0.38469024101275023
Loss in iteration 142 : 0.3848312479159281
Loss in iteration 143 : 0.384972681816065
Loss in iteration 144 : 0.3851145796744291
Loss in iteration 145 : 0.38525697489063
Loss in iteration 146 : 0.38539989763747334
Loss in iteration 147 : 0.385543375164683
Loss in iteration 148 : 0.38568743207432343
Loss in iteration 149 : 0.38583208045026385
Loss in iteration 150 : 0.385977716533059
Loss in iteration 151 : 0.38612395270859395
Loss in iteration 152 : 0.38627080779732526
Loss in iteration 153 : 0.3864185370998035
Loss in iteration 154 : 0.3865667775679849
Loss in iteration 155 : 0.38671553821890253
Loss in iteration 156 : 0.38686487564183925
Loss in iteration 157 : 0.3870146817542767
Loss in iteration 158 : 0.38716501851151075
Loss in iteration 159 : 0.38731594136321496
Loss in iteration 160 : 0.3874674380642893
Loss in iteration 161 : 0.3876194273923374
Loss in iteration 162 : 0.3877719343234176
Loss in iteration 163 : 0.3879249814452308
Loss in iteration 164 : 0.3880783468397228
Loss in iteration 165 : 0.38823203012857355
Loss in iteration 166 : 0.3883860720091052
Loss in iteration 167 : 0.38854088287017713
Loss in iteration 168 : 0.38869684641526747
Loss in iteration 169 : 0.3888531349355605
Loss in iteration 170 : 0.38900977469229686
Loss in iteration 171 : 0.3891667638656077
Loss in iteration 172 : 0.3893242111299883
Loss in iteration 173 : 0.3894823874833784
Loss in iteration 174 : 0.3896410093317748
Loss in iteration 175 : 0.3897996299618515
Loss in iteration 176 : 0.38995868664980543
Loss in iteration 177 : 0.3901179059035974
Loss in iteration 178 : 0.39027731572189833
Loss in iteration 179 : 0.39043679800019493
Loss in iteration 180 : 0.39059602786954806
Loss in iteration 181 : 0.3907551823513542
Loss in iteration 182 : 0.3909143030222999
Loss in iteration 183 : 0.39107368860836217
Loss in iteration 184 : 0.39123317807210717
Loss in iteration 185 : 0.39139297535214146
Loss in iteration 186 : 0.39155297804800543
Loss in iteration 187 : 0.3917132213556454
Loss in iteration 188 : 0.39187361993645703
Loss in iteration 189 : 0.3920342091774848
Loss in iteration 190 : 0.3921951457155562
Loss in iteration 191 : 0.3923565020495228
Loss in iteration 192 : 0.39251825222113024
Loss in iteration 193 : 0.3926803948082704
Loss in iteration 194 : 0.39284283549089527
Loss in iteration 195 : 0.393005491496402
Loss in iteration 196 : 0.39316833877127755
Loss in iteration 197 : 0.3933313890168522
Loss in iteration 198 : 0.39349472332078395
Loss in iteration 199 : 0.3936579920329802
Loss in iteration 200 : 0.39382138748290707
Testing accuracy  of updater 6 on alg 1 with rate 0.002744 = 0.78525, training accuracy 0.8394302363224344, time elapsed: 3351 millisecond.
Loss in iteration 1 : 1.0000007471622976
Loss in iteration 2 : 0.7683675727752703
Loss in iteration 3 : 0.5546926807100246
Loss in iteration 4 : 0.585263098575628
Loss in iteration 5 : 0.6426523986528749
Loss in iteration 6 : 0.6781850900011535
Loss in iteration 7 : 0.6920633454187971
Loss in iteration 8 : 0.6863899577934589
Loss in iteration 9 : 0.6634284206687553
Loss in iteration 10 : 0.6254014865375531
Loss in iteration 11 : 0.5750099759835501
Loss in iteration 12 : 0.51900169884836
Loss in iteration 13 : 0.47077838633374686
Loss in iteration 14 : 0.4407970134639665
Loss in iteration 15 : 0.4332563133399129
Loss in iteration 16 : 0.444461435097122
Loss in iteration 17 : 0.4637444824069516
Loss in iteration 18 : 0.47385530687015537
Loss in iteration 19 : 0.46854518447353877
Loss in iteration 20 : 0.4504052862127735
Loss in iteration 21 : 0.42668511157773814
Loss in iteration 22 : 0.4055404429264546
Loss in iteration 23 : 0.39161650040215235
Loss in iteration 24 : 0.3871330183933124
Loss in iteration 25 : 0.38908061830023094
Loss in iteration 26 : 0.39404141908660917
Loss in iteration 27 : 0.39834615445791266
Loss in iteration 28 : 0.3999686553063082
Loss in iteration 29 : 0.3983044645142461
Loss in iteration 30 : 0.39404911602619047
Loss in iteration 31 : 0.3886440564478938
Loss in iteration 32 : 0.3835011505827568
Loss in iteration 33 : 0.37951864593786344
Loss in iteration 34 : 0.3773070823148987
Loss in iteration 35 : 0.37681013617294623
Loss in iteration 36 : 0.3774690728362317
Loss in iteration 37 : 0.3789494462126648
Loss in iteration 38 : 0.38028289688676
Loss in iteration 39 : 0.381012037933019
Loss in iteration 40 : 0.38099369037046427
Loss in iteration 41 : 0.38026232124607684
Loss in iteration 42 : 0.3791093156574016
Loss in iteration 43 : 0.37777641385805216
Loss in iteration 44 : 0.3765960046902438
Loss in iteration 45 : 0.3756676115329248
Loss in iteration 46 : 0.3751965372745335
Loss in iteration 47 : 0.37522415262573966
Loss in iteration 48 : 0.3755971632967168
Loss in iteration 49 : 0.37608930644619853
Loss in iteration 50 : 0.37647169394434277
Loss in iteration 51 : 0.3766312077363521
Loss in iteration 52 : 0.3765496767645079
Loss in iteration 53 : 0.3762610342626925
Loss in iteration 54 : 0.3758756911885546
Loss in iteration 55 : 0.3754890963435952
Loss in iteration 56 : 0.37521386807150875
Loss in iteration 57 : 0.3750896341702109
Loss in iteration 58 : 0.37510258654487055
Loss in iteration 59 : 0.37520874216725997
Loss in iteration 60 : 0.3753510713693649
Loss in iteration 61 : 0.3754812557487115
Loss in iteration 62 : 0.37555609921391603
Loss in iteration 63 : 0.3755614792596418
Loss in iteration 64 : 0.3755031732365631
Loss in iteration 65 : 0.37540024749234197
Loss in iteration 66 : 0.3752908027132672
Loss in iteration 67 : 0.37521718175826607
Loss in iteration 68 : 0.37517275527858873
Loss in iteration 69 : 0.37517419043105266
Loss in iteration 70 : 0.3752081467135764
Loss in iteration 71 : 0.37524882613356897
Loss in iteration 72 : 0.3752946972188919
Loss in iteration 73 : 0.37533130354330685
Loss in iteration 74 : 0.375359200455538
Loss in iteration 75 : 0.3753721519986737
Loss in iteration 76 : 0.37537259387246535
Loss in iteration 77 : 0.3753639017461382
Loss in iteration 78 : 0.37535253560284226
Loss in iteration 79 : 0.37534502395788955
Loss in iteration 80 : 0.3753449877091074
Loss in iteration 81 : 0.3753558006481243
Loss in iteration 82 : 0.3753767828324368
Loss in iteration 83 : 0.375403567056979
Loss in iteration 84 : 0.37543173303647603
Loss in iteration 85 : 0.3754602459598878
Loss in iteration 86 : 0.3754868055317969
Loss in iteration 87 : 0.37551121605242854
Loss in iteration 88 : 0.375531960903302
Loss in iteration 89 : 0.3755497009768847
Loss in iteration 90 : 0.3755656478117322
Loss in iteration 91 : 0.37558093320895025
Loss in iteration 92 : 0.375599720600695
Loss in iteration 93 : 0.3756189063475937
Loss in iteration 94 : 0.3756386700866687
Loss in iteration 95 : 0.37566077766213174
Loss in iteration 96 : 0.37568652693525967
Loss in iteration 97 : 0.3757143326303484
Loss in iteration 98 : 0.3757442413432897
Loss in iteration 99 : 0.37577525180431137
Loss in iteration 100 : 0.3758041341140214
Loss in iteration 101 : 0.3758314614868756
Loss in iteration 102 : 0.37585900131267097
Loss in iteration 103 : 0.37588646246106505
Loss in iteration 104 : 0.37591622863726576
Loss in iteration 105 : 0.3759473217633121
Loss in iteration 106 : 0.37597932543460516
Loss in iteration 107 : 0.3760115840098648
Loss in iteration 108 : 0.3760440369799944
Loss in iteration 109 : 0.37607673589895013
Loss in iteration 110 : 0.37610969151946627
Loss in iteration 111 : 0.37614291365577046
Loss in iteration 112 : 0.376176411186054
Loss in iteration 113 : 0.376210192139245
Loss in iteration 114 : 0.3762443643491621
Loss in iteration 115 : 0.3762790831691765
Loss in iteration 116 : 0.3763142052866326
Loss in iteration 117 : 0.37634993362356733
Loss in iteration 118 : 0.37638610107133474
Loss in iteration 119 : 0.3764226573523705
Loss in iteration 120 : 0.37645962494562896
Loss in iteration 121 : 0.3764968848664418
Loss in iteration 122 : 0.37653444250211704
Loss in iteration 123 : 0.376572448935163
Loss in iteration 124 : 0.37661107318762
Loss in iteration 125 : 0.37665006579887456
Loss in iteration 126 : 0.37668942506649866
Loss in iteration 127 : 0.37672914946362845
Loss in iteration 128 : 0.37676923761897
Loss in iteration 129 : 0.37680968829906064
Loss in iteration 130 : 0.37685050039253076
Loss in iteration 131 : 0.37689167289614345
Loss in iteration 132 : 0.37693329522511115
Loss in iteration 133 : 0.3769752752805377
Loss in iteration 134 : 0.3770176702850758
Loss in iteration 135 : 0.37706056763870066
Loss in iteration 136 : 0.37710376525533196
Loss in iteration 137 : 0.3771472670504512
Loss in iteration 138 : 0.3771910765727229
Loss in iteration 139 : 0.3772351970363782
Loss in iteration 140 : 0.3772796313508185
Loss in iteration 141 : 0.377324382147671
Loss in iteration 142 : 0.3773694518054995
Loss in iteration 143 : 0.37741484247236584
Loss in iteration 144 : 0.37746055608642026
Loss in iteration 145 : 0.37750666878317374
Loss in iteration 146 : 0.37755310914465706
Loss in iteration 147 : 0.3776001229398004
Loss in iteration 148 : 0.3776473599437094
Loss in iteration 149 : 0.37769483105376256
Loss in iteration 150 : 0.3777425448495747
Loss in iteration 151 : 0.37779064255657047
Loss in iteration 152 : 0.3778389898374844
Loss in iteration 153 : 0.37788819247065597
Loss in iteration 154 : 0.3779376964924761
Loss in iteration 155 : 0.3779872446391337
Loss in iteration 156 : 0.37803683352651485
Loss in iteration 157 : 0.3780868276430914
Loss in iteration 158 : 0.37813729565814147
Loss in iteration 159 : 0.3781880962184788
Loss in iteration 160 : 0.378239323934179
Loss in iteration 161 : 0.37829121302822777
Loss in iteration 162 : 0.3783433604069747
Loss in iteration 163 : 0.3783955885977406
Loss in iteration 164 : 0.37844812159457863
Loss in iteration 165 : 0.3785010601678996
Loss in iteration 166 : 0.378554313755877
Loss in iteration 167 : 0.37860790974824277
Loss in iteration 168 : 0.37866198009923807
Loss in iteration 169 : 0.3787164355910597
Loss in iteration 170 : 0.37877112502904337
Loss in iteration 171 : 0.37882596096051996
Loss in iteration 172 : 0.3788813960727773
Loss in iteration 173 : 0.37893713355818703
Loss in iteration 174 : 0.3789931549112375
Loss in iteration 175 : 0.3790494451838282
Loss in iteration 176 : 0.3791060005456971
Loss in iteration 177 : 0.379162939151696
Loss in iteration 178 : 0.3792201168058261
Loss in iteration 179 : 0.3792775100319214
Loss in iteration 180 : 0.3793352265020063
Loss in iteration 181 : 0.37939322208737164
Loss in iteration 182 : 0.37945150033221003
Loss in iteration 183 : 0.3795100644358256
Loss in iteration 184 : 0.37956891728483033
Loss in iteration 185 : 0.3796280614823566
Loss in iteration 186 : 0.37968753288572893
Loss in iteration 187 : 0.3797474469516761
Loss in iteration 188 : 0.37980772008331015
Loss in iteration 189 : 0.3798682252979305
Loss in iteration 190 : 0.37992909604244884
Loss in iteration 191 : 0.37999034700989365
Loss in iteration 192 : 0.3800518134491392
Loss in iteration 193 : 0.38011350375304054
Loss in iteration 194 : 0.3801754255167799
Loss in iteration 195 : 0.3802375406566889
Loss in iteration 196 : 0.38030017908459796
Loss in iteration 197 : 0.38036292671492
Loss in iteration 198 : 0.38042579932521514
Loss in iteration 199 : 0.3804888132863419
Loss in iteration 200 : 0.3805521525232372
Testing accuracy  of updater 6 on alg 1 with rate 0.001568 = 0.78625, training accuracy 0.8400776950469407, time elapsed: 3659 millisecond.
Loss in iteration 1 : 1.0000000474112534
Loss in iteration 2 : 0.9426622412766195
Loss in iteration 3 : 0.8403799135200918
Loss in iteration 4 : 0.7102309368728248
Loss in iteration 5 : 0.6054379556728887
Loss in iteration 6 : 0.5566575911576412
Loss in iteration 7 : 0.5550053255093957
Loss in iteration 8 : 0.5719212810153287
Loss in iteration 9 : 0.5914312835395495
Loss in iteration 10 : 0.6075406974831418
Loss in iteration 11 : 0.6186470522458629
Testing accuracy  of updater 6 on alg 1 with rate 3.92E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 210 millisecond.
Loss in iteration 1 : 1.0055596675900278
Loss in iteration 2 : 5.749939441301445
Loss in iteration 3 : 7.276518003973823
Loss in iteration 4 : 6.932194835408638
Loss in iteration 5 : 5.530643530576198
Loss in iteration 6 : 3.4843078562475998
Loss in iteration 7 : 1.4254562533939545
Loss in iteration 8 : 1.0870593873559344
Loss in iteration 9 : 2.105027953244207
Loss in iteration 10 : 2.8359313616722117
Loss in iteration 11 : 2.5632868493699035
Loss in iteration 12 : 1.7463141490186491
Loss in iteration 13 : 1.164479534374949
Loss in iteration 14 : 1.1142873074641622
Loss in iteration 15 : 1.3394487383673364
Loss in iteration 16 : 1.5647331940482492
Loss in iteration 17 : 1.627916893539587
Loss in iteration 18 : 1.507986950923054
Loss in iteration 19 : 1.2783635950193664
Loss in iteration 20 : 1.0653216127698018
Loss in iteration 21 : 0.9644343195007118
Loss in iteration 22 : 1.0156423405860144
Loss in iteration 23 : 1.114778079641696
Loss in iteration 24 : 1.123649801564251
Loss in iteration 25 : 1.012240975076963
Loss in iteration 26 : 0.8657175720787811
Loss in iteration 27 : 0.7949205522542848
Loss in iteration 28 : 0.8141266776102793
Loss in iteration 29 : 0.8449209227386941
Loss in iteration 30 : 0.822300616374623
Loss in iteration 31 : 0.7423765381765528
Loss in iteration 32 : 0.6625585267363427
Loss in iteration 33 : 0.6511611327203713
Loss in iteration 34 : 0.6771308525998917
Loss in iteration 35 : 0.6463027371059912
Loss in iteration 36 : 0.5713052586362161
Loss in iteration 37 : 0.5476659358801287
Loss in iteration 38 : 0.5680048896270208
Loss in iteration 39 : 0.5345719039545609
Loss in iteration 40 : 0.48877915476023837
Loss in iteration 41 : 0.5146293383955463
Loss in iteration 42 : 0.48731980144764653
Loss in iteration 43 : 0.4667144410473975
Loss in iteration 44 : 0.49102170156825486
Loss in iteration 45 : 0.45338827336778337
Loss in iteration 46 : 0.4884307678843523
Loss in iteration 47 : 0.4646094384898467
Loss in iteration 48 : 0.476744547226835
Loss in iteration 49 : 0.48701139984639047
Loss in iteration 50 : 0.4571980105190675
Loss in iteration 51 : 0.4792116985809156
Loss in iteration 52 : 0.45707993352229737
Loss in iteration 53 : 0.45800210255421747
Loss in iteration 54 : 0.4538409078960083
Loss in iteration 55 : 0.4506285956771989
Loss in iteration 56 : 0.4489720193008173
Loss in iteration 57 : 0.4470860439883499
Loss in iteration 58 : 0.44432266780296625
Loss in iteration 59 : 0.4457788366830845
Loss in iteration 60 : 0.441685725525858
Loss in iteration 61 : 0.4438660746230708
Loss in iteration 62 : 0.43888345827197006
Loss in iteration 63 : 0.4410018943722332
Loss in iteration 64 : 0.43699033698828116
Loss in iteration 65 : 0.43757806133064003
Loss in iteration 66 : 0.4348634435368964
Loss in iteration 67 : 0.4344103722759048
Loss in iteration 68 : 0.4336644773325307
Loss in iteration 69 : 0.4325511133186747
Loss in iteration 70 : 0.43233549898949464
Loss in iteration 71 : 0.43138595868793184
Loss in iteration 72 : 0.43180479948787875
Loss in iteration 73 : 0.43096852669066826
Loss in iteration 74 : 0.43122858000624426
Loss in iteration 75 : 0.4308370678241719
Loss in iteration 76 : 0.4308973380092328
Testing accuracy  of updater 7 on alg 1 with rate 0.22400000000000003 = 0.78975, training accuracy 0.8429912593072192, time elapsed: 1305 millisecond.
Loss in iteration 1 : 1.0001760578447032
Loss in iteration 2 : 1.0530462320271203
Loss in iteration 3 : 1.3506563885189542
Loss in iteration 4 : 1.2958452443488455
Loss in iteration 5 : 0.9337775028454675
Loss in iteration 6 : 0.4178951224270767
Loss in iteration 7 : 0.6274828710090132
Loss in iteration 8 : 0.8752511268422437
Loss in iteration 9 : 0.6958103287970139
Loss in iteration 10 : 0.47708523808716546
Loss in iteration 11 : 0.5118964379341338
Loss in iteration 12 : 0.6431468779889289
Loss in iteration 13 : 0.6770398916021372
Loss in iteration 14 : 0.6013185851600574
Loss in iteration 15 : 0.5192801284925662
Loss in iteration 16 : 0.521713958328896
Loss in iteration 17 : 0.5801594435388329
Loss in iteration 18 : 0.6056189804715608
Loss in iteration 19 : 0.569053863622455
Loss in iteration 20 : 0.519121452924611
Loss in iteration 21 : 0.5111200518211295
Loss in iteration 22 : 0.5386177838300531
Loss in iteration 23 : 0.5476623645182708
Loss in iteration 24 : 0.5200347448565028
Loss in iteration 25 : 0.48875138550666053
Loss in iteration 26 : 0.49032164661709016
Loss in iteration 27 : 0.5031721712212077
Loss in iteration 28 : 0.4919713775022152
Loss in iteration 29 : 0.4647284201716688
Loss in iteration 30 : 0.4587240455133252
Loss in iteration 31 : 0.46879590518006803
Loss in iteration 32 : 0.45701393423932984
Loss in iteration 33 : 0.439063457416175
Loss in iteration 34 : 0.44580117433660343
Loss in iteration 35 : 0.44629245510740534
Loss in iteration 36 : 0.4290809405260992
Loss in iteration 37 : 0.4372667457698064
Loss in iteration 38 : 0.43813214817878976
Loss in iteration 39 : 0.42902428146009974
Loss in iteration 40 : 0.44265733504570187
Loss in iteration 41 : 0.433208710195905
Loss in iteration 42 : 0.4420956305546688
Loss in iteration 43 : 0.44050667970537083
Loss in iteration 44 : 0.4412462023676316
Loss in iteration 45 : 0.4434562071843113
Loss in iteration 46 : 0.4391476263447888
Loss in iteration 47 : 0.44290198195161207
Loss in iteration 48 : 0.4371810300933302
Loss in iteration 49 : 0.441021578593622
Loss in iteration 50 : 0.4359394855193462
Loss in iteration 51 : 0.4374169920486289
Loss in iteration 52 : 0.4366430074106661
Loss in iteration 53 : 0.43490509890331264
Loss in iteration 54 : 0.4363554063879274
Loss in iteration 55 : 0.4343983646803777
Loss in iteration 56 : 0.4347331687976585
Testing accuracy  of updater 7 on alg 1 with rate 0.15680000000000002 = 0.78225, training accuracy 0.8410488831337002, time elapsed: 1294 millisecond.
Loss in iteration 1 : 1.0000610225239261
Loss in iteration 2 : 0.7630421441026124
Loss in iteration 3 : 0.947511648839148
Loss in iteration 4 : 0.9316173758077942
Loss in iteration 5 : 0.7383718194757467
Loss in iteration 6 : 0.42311490648295197
Loss in iteration 7 : 0.492131063005301
Loss in iteration 8 : 0.6720544611520846
Loss in iteration 9 : 0.5976599033539496
Loss in iteration 10 : 0.43316118647350554
Loss in iteration 11 : 0.4060564875366625
Loss in iteration 12 : 0.49094429710728993
Loss in iteration 13 : 0.5404264057477308
Loss in iteration 14 : 0.5127932719384674
Loss in iteration 15 : 0.4521120003108228
Loss in iteration 16 : 0.4293245172645827
Loss in iteration 17 : 0.45625633834222334
Loss in iteration 18 : 0.49006388686238056
Loss in iteration 19 : 0.49597593943398083
Loss in iteration 20 : 0.4731586616594841
Loss in iteration 21 : 0.4470651653765513
Loss in iteration 22 : 0.4414730326682663
Loss in iteration 23 : 0.45686923348977415
Loss in iteration 24 : 0.4697456352546397
Loss in iteration 25 : 0.46360416857680875
Loss in iteration 26 : 0.44580987905740227
Loss in iteration 27 : 0.4366119431856593
Loss in iteration 28 : 0.4405788552886052
Loss in iteration 29 : 0.44856931617381
Loss in iteration 30 : 0.446116951949185
Loss in iteration 31 : 0.43396252802353513
Loss in iteration 32 : 0.42602850525790975
Loss in iteration 33 : 0.42782539186710955
Loss in iteration 34 : 0.4318687434936176
Loss in iteration 35 : 0.42882988961402924
Loss in iteration 36 : 0.42057756831072857
Loss in iteration 37 : 0.41692037677847416
Loss in iteration 38 : 0.42103199202548997
Loss in iteration 39 : 0.4213219293267649
Loss in iteration 40 : 0.4154994593580279
Loss in iteration 41 : 0.4130383234756111
Loss in iteration 42 : 0.41708911571761376
Loss in iteration 43 : 0.4171190828914143
Loss in iteration 44 : 0.4140095582709844
Loss in iteration 45 : 0.41604494464399483
Loss in iteration 46 : 0.41922189363053236
Loss in iteration 47 : 0.4175034641026999
Loss in iteration 48 : 0.41793643439226696
Loss in iteration 49 : 0.4209576431563253
Loss in iteration 50 : 0.4216330491993064
Loss in iteration 51 : 0.4211054805891182
Loss in iteration 52 : 0.42318809292158494
Loss in iteration 53 : 0.42404371604757174
Loss in iteration 54 : 0.4235214696408437
Loss in iteration 55 : 0.4248988042886704
Loss in iteration 56 : 0.42550205682222364
Loss in iteration 57 : 0.4251997796654203
Loss in iteration 58 : 0.42588263902111523
Loss in iteration 59 : 0.4264365508418028
Loss in iteration 60 : 0.4260953544654195
Loss in iteration 61 : 0.426537775813812
Loss in iteration 62 : 0.4270321971855023
Loss in iteration 63 : 0.4271124714608589
Loss in iteration 64 : 0.4273944568809671
Loss in iteration 65 : 0.42787186059852506
Loss in iteration 66 : 0.42800962439580004
Loss in iteration 67 : 0.42810714849132203
Loss in iteration 68 : 0.42844479762526605
Loss in iteration 69 : 0.42876195356506974
Loss in iteration 70 : 0.42891729205138907
Loss in iteration 71 : 0.42911420833082997
Loss in iteration 72 : 0.4293317825430786
Loss in iteration 73 : 0.4295626027147922
Loss in iteration 74 : 0.42964191147212366
Loss in iteration 75 : 0.4297084232106094
Testing accuracy  of updater 7 on alg 1 with rate 0.08960000000000001 = 0.786, training accuracy 0.8407251537714471, time elapsed: 1528 millisecond.
Loss in iteration 1 : 1.0000038763270067
Loss in iteration 2 : 0.6405953768978558
Loss in iteration 3 : 0.6160828994770453
Loss in iteration 4 : 0.714382978360934
Loss in iteration 5 : 0.759473052089772
Loss in iteration 6 : 0.7552301115962698
Loss in iteration 7 : 0.706594990500401
Loss in iteration 8 : 0.6187852701053119
Loss in iteration 9 : 0.5070998953889166
Loss in iteration 10 : 0.43315011950859433
Loss in iteration 11 : 0.44611960970092623
Loss in iteration 12 : 0.5081403978020587
Loss in iteration 13 : 0.5281112739846298
Loss in iteration 14 : 0.4902457070530749
Loss in iteration 15 : 0.4292096950088573
Loss in iteration 16 : 0.38809722017990267
Loss in iteration 17 : 0.3866590661910277
Loss in iteration 18 : 0.4086735605814001
Loss in iteration 19 : 0.4265062025423129
Loss in iteration 20 : 0.4270200265627815
Loss in iteration 21 : 0.4117701630175501
Loss in iteration 22 : 0.39274777644070286
Loss in iteration 23 : 0.38022446833222995
Loss in iteration 24 : 0.3786681280373915
Loss in iteration 25 : 0.3856298156464108
Loss in iteration 26 : 0.39441503929051935
Loss in iteration 27 : 0.3989208841938867
Loss in iteration 28 : 0.3973072249271022
Loss in iteration 29 : 0.39140414184899336
Loss in iteration 30 : 0.3848883981998262
Loss in iteration 31 : 0.3811325274490401
Loss in iteration 32 : 0.3815084066905646
Loss in iteration 33 : 0.3845981938295227
Loss in iteration 34 : 0.3881661199416723
Loss in iteration 35 : 0.3900993836368365
Loss in iteration 36 : 0.38969209344327577
Loss in iteration 37 : 0.387529380047771
Loss in iteration 38 : 0.3850339533975051
Loss in iteration 39 : 0.38353342259893364
Loss in iteration 40 : 0.3835990677595828
Loss in iteration 41 : 0.3850367567862972
Loss in iteration 42 : 0.38673696074896935
Loss in iteration 43 : 0.38768842159142214
Loss in iteration 44 : 0.38750797133281367
Loss in iteration 45 : 0.3865155589207157
Loss in iteration 46 : 0.38545836874173606
Loss in iteration 47 : 0.38484479227036916
Loss in iteration 48 : 0.3850739419540836
Loss in iteration 49 : 0.38583757418112796
Loss in iteration 50 : 0.3865875522807378
Loss in iteration 51 : 0.3870208917610314
Loss in iteration 52 : 0.3869551541055436
Loss in iteration 53 : 0.3866206480357834
Loss in iteration 54 : 0.3863475503854287
Loss in iteration 55 : 0.3864215914117204
Loss in iteration 56 : 0.38681828971734655
Loss in iteration 57 : 0.387360289395173
Loss in iteration 58 : 0.38780405502209775
Loss in iteration 59 : 0.38805886436588444
Loss in iteration 60 : 0.3881450474350112
Loss in iteration 61 : 0.3881281728990982
Loss in iteration 62 : 0.3881993692863944
Loss in iteration 63 : 0.38843199896131225
Loss in iteration 64 : 0.38880526171772967
Loss in iteration 65 : 0.3892219173452248
Loss in iteration 66 : 0.3895862396759232
Loss in iteration 67 : 0.38986027175418975
Loss in iteration 68 : 0.3900681067268045
Loss in iteration 69 : 0.39028360651793986
Loss in iteration 70 : 0.3905307109645858
Loss in iteration 71 : 0.3908702253340279
Loss in iteration 72 : 0.39126222657822946
Loss in iteration 73 : 0.39164594615907883
Loss in iteration 74 : 0.3919821509107344
Loss in iteration 75 : 0.3922673954310094
Loss in iteration 76 : 0.39253406292098947
Loss in iteration 77 : 0.3928281506558129
Loss in iteration 78 : 0.39315949582507637
Loss in iteration 79 : 0.39352108096859734
Loss in iteration 80 : 0.39388149340558515
Loss in iteration 81 : 0.3942282414730338
Loss in iteration 82 : 0.3945520836371893
Loss in iteration 83 : 0.39486416754340337
Loss in iteration 84 : 0.3951734472954195
Loss in iteration 85 : 0.39551050282567984
Loss in iteration 86 : 0.3958584904280069
Loss in iteration 87 : 0.3962037889591535
Loss in iteration 88 : 0.3965370135315463
Loss in iteration 89 : 0.39685770431038675
Loss in iteration 90 : 0.3971737567349202
Loss in iteration 91 : 0.39748745022676024
Loss in iteration 92 : 0.3978040387347478
Loss in iteration 93 : 0.398128782293892
Loss in iteration 94 : 0.3984531933168085
Loss in iteration 95 : 0.3987734517303022
Loss in iteration 96 : 0.39908487926397185
Loss in iteration 97 : 0.39939191270752356
Loss in iteration 98 : 0.39970163639506473
Loss in iteration 99 : 0.40001408950171685
Loss in iteration 100 : 0.4003274155919433
Loss in iteration 101 : 0.400637717730197
Loss in iteration 102 : 0.40094490892297174
Loss in iteration 103 : 0.4012493674209914
Loss in iteration 104 : 0.4015519444147995
Loss in iteration 105 : 0.4018541378336339
Loss in iteration 106 : 0.4021564969955427
Loss in iteration 107 : 0.4024588324303744
Loss in iteration 108 : 0.40276043328132216
Loss in iteration 109 : 0.40306045002887436
Loss in iteration 110 : 0.4033589826198264
Loss in iteration 111 : 0.40365619537066016
Loss in iteration 112 : 0.40395236159901604
Loss in iteration 113 : 0.404248373328061
Loss in iteration 114 : 0.40454315453460105
Loss in iteration 115 : 0.40483650088437684
Loss in iteration 116 : 0.4051287358893013
Loss in iteration 117 : 0.4054198438558822
Loss in iteration 118 : 0.405709594391321
Loss in iteration 119 : 0.4059979474465415
Loss in iteration 120 : 0.40628439600774274
Loss in iteration 121 : 0.40656885935088544
Loss in iteration 122 : 0.4068517943479129
Loss in iteration 123 : 0.4071332311294787
Loss in iteration 124 : 0.40741305079813456
Loss in iteration 125 : 0.4076904885091045
Loss in iteration 126 : 0.4079654501945789
Loss in iteration 127 : 0.4082378138317468
Loss in iteration 128 : 0.40850750206335407
Loss in iteration 129 : 0.4087747677088745
Loss in iteration 130 : 0.4090398670776709
Loss in iteration 131 : 0.4093021113981715
Loss in iteration 132 : 0.40956177659306064
Loss in iteration 133 : 0.409818418656159
Loss in iteration 134 : 0.4100722255452971
Loss in iteration 135 : 0.41032349388908346
Loss in iteration 136 : 0.4105715578745513
Loss in iteration 137 : 0.4108155794266761
Loss in iteration 138 : 0.41105648831868846
Loss in iteration 139 : 0.4112946306933721
Loss in iteration 140 : 0.411530166516932
Loss in iteration 141 : 0.41176260236966333
Loss in iteration 142 : 0.41199259840836194
Loss in iteration 143 : 0.41222001106031136
Loss in iteration 144 : 0.41244469364062153
Loss in iteration 145 : 0.412667117503679
Loss in iteration 146 : 0.41288718071737407
Loss in iteration 147 : 0.4131047504623409
Loss in iteration 148 : 0.4133200710944224
Loss in iteration 149 : 0.41353336867417245
Loss in iteration 150 : 0.4137448476969974
Loss in iteration 151 : 0.41395469312724437
Loss in iteration 152 : 0.41416191722994833
Loss in iteration 153 : 0.4143668679214673
Loss in iteration 154 : 0.41456925119346993
Loss in iteration 155 : 0.414769396997535
Loss in iteration 156 : 0.4149674562433334
Loss in iteration 157 : 0.41516363952208324
Loss in iteration 158 : 0.41535813723644344
Loss in iteration 159 : 0.41555112152186524
Loss in iteration 160 : 0.4157427479886792
Loss in iteration 161 : 0.4159331573009736
Loss in iteration 162 : 0.41612247660701907
Loss in iteration 163 : 0.41631082083475707
Loss in iteration 164 : 0.41649808568708896
Loss in iteration 165 : 0.4166846295603215
Loss in iteration 166 : 0.4168707959766258
Loss in iteration 167 : 0.4170559870478926
Loss in iteration 168 : 0.4172404957539732
Loss in iteration 169 : 0.4174243950099372
Loss in iteration 170 : 0.4176077530279517
Loss in iteration 171 : 0.4177906314964717
Loss in iteration 172 : 0.41797308620770457
Loss in iteration 173 : 0.4181551676252404
Loss in iteration 174 : 0.4183362952123896
Loss in iteration 175 : 0.41851665535992194
Loss in iteration 176 : 0.41869622721363053
Loss in iteration 177 : 0.41887509633889436
Loss in iteration 178 : 0.4190533401112908
Loss in iteration 179 : 0.41922993160798994
Loss in iteration 180 : 0.419405107881602
Loss in iteration 181 : 0.4195789082290593
Loss in iteration 182 : 0.41975274351445563
Loss in iteration 183 : 0.4199265515396401
Loss in iteration 184 : 0.42010029090798956
Loss in iteration 185 : 0.42027380788546603
Loss in iteration 186 : 0.4204475385515393
Loss in iteration 187 : 0.42062108840979656
Loss in iteration 188 : 0.42079449792566165
Loss in iteration 189 : 0.42096734622200127
Loss in iteration 190 : 0.4211389608275792
Loss in iteration 191 : 0.4213101052937445
Loss in iteration 192 : 0.42148037097128865
Loss in iteration 193 : 0.4216485373141719
Loss in iteration 194 : 0.4218146674786868
Loss in iteration 195 : 0.42197887884507956
Loss in iteration 196 : 0.4221422818643348
Loss in iteration 197 : 0.4223050897303585
Loss in iteration 198 : 0.42246565055169405
Loss in iteration 199 : 0.42262380681873324
Loss in iteration 200 : 0.4227800886725204
Testing accuracy  of updater 7 on alg 1 with rate 0.022400000000000003 = 0.78725, training accuracy 0.8407251537714471, time elapsed: 3664 millisecond.
Loss in iteration 1 : 1.000002256832453
Loss in iteration 2 : 0.7322205915174469
Loss in iteration 3 : 0.5691106776000496
Loss in iteration 4 : 0.6522519512592613
Loss in iteration 5 : 0.7089155271896452
Loss in iteration 6 : 0.7289570175208191
Loss in iteration 7 : 0.7156549210913042
Loss in iteration 8 : 0.6724138078713239
Loss in iteration 9 : 0.6031175311308088
Loss in iteration 10 : 0.5188403525724792
Loss in iteration 11 : 0.45593895959866043
Loss in iteration 12 : 0.4404350362649738
Loss in iteration 13 : 0.4690109745302818
Loss in iteration 14 : 0.5046831160183675
Loss in iteration 15 : 0.5082719753319555
Loss in iteration 16 : 0.4773490070254382
Loss in iteration 17 : 0.4330260356220494
Loss in iteration 18 : 0.39947602782399866
Loss in iteration 19 : 0.38942627274058284
Loss in iteration 20 : 0.39953187381122995
Loss in iteration 21 : 0.41375422378865556
Loss in iteration 22 : 0.4202065845037711
Loss in iteration 23 : 0.41606410993100323
Loss in iteration 24 : 0.4037348502712856
Loss in iteration 25 : 0.38986141497703863
Loss in iteration 26 : 0.3808515276011675
Loss in iteration 27 : 0.3784701000374514
Loss in iteration 28 : 0.38158563141889457
Loss in iteration 29 : 0.3867757143185313
Loss in iteration 30 : 0.3907477057212538
Loss in iteration 31 : 0.39165282698522724
Loss in iteration 32 : 0.3892918516550705
Loss in iteration 33 : 0.38516918568186775
Loss in iteration 34 : 0.3812637673962988
Loss in iteration 35 : 0.37880891511004694
Loss in iteration 36 : 0.37842577281343376
Loss in iteration 37 : 0.37995416102191953
Loss in iteration 38 : 0.3821193077311481
Loss in iteration 39 : 0.3836783426892069
Loss in iteration 40 : 0.38415296601088794
Loss in iteration 41 : 0.38350687686461604
Loss in iteration 42 : 0.38216468095556994
Loss in iteration 43 : 0.38080557628611683
Loss in iteration 44 : 0.3800142003049705
Loss in iteration 45 : 0.38018701319432086
Loss in iteration 46 : 0.3808977373470932
Loss in iteration 47 : 0.38184218322472596
Loss in iteration 48 : 0.3825625234881879
Loss in iteration 49 : 0.38286522937730827
Loss in iteration 50 : 0.38273687596336414
Loss in iteration 51 : 0.3823396967339515
Loss in iteration 52 : 0.38190036113684495
Loss in iteration 53 : 0.3816937821196191
Loss in iteration 54 : 0.3817147337539333
Loss in iteration 55 : 0.3820759565927472
Loss in iteration 56 : 0.3825659152138967
Loss in iteration 57 : 0.38301108761358
Loss in iteration 58 : 0.38324710594502587
Loss in iteration 59 : 0.38326028052869104
Loss in iteration 60 : 0.3831834517669429
Loss in iteration 61 : 0.38315280455903133
Loss in iteration 62 : 0.38326407765950987
Loss in iteration 63 : 0.38352057175033355
Loss in iteration 64 : 0.3838435692868095
Loss in iteration 65 : 0.3841567645408171
Loss in iteration 66 : 0.3844175418758434
Loss in iteration 67 : 0.38460447277703924
Loss in iteration 68 : 0.3847263403417518
Loss in iteration 69 : 0.3848290515034387
Loss in iteration 70 : 0.38495561832064995
Loss in iteration 71 : 0.38513629749613265
Loss in iteration 72 : 0.38537100390475965
Loss in iteration 73 : 0.3856341143022003
Loss in iteration 74 : 0.3859006122388197
Loss in iteration 75 : 0.3861382475979933
Loss in iteration 76 : 0.38634534366786716
Loss in iteration 77 : 0.38652767113715936
Loss in iteration 78 : 0.3867145982472676
Loss in iteration 79 : 0.38693196173709743
Loss in iteration 80 : 0.3871755053582277
Loss in iteration 81 : 0.387433472426029
Loss in iteration 82 : 0.3876838265536555
Loss in iteration 83 : 0.3879238976453883
Loss in iteration 84 : 0.3881543919174482
Loss in iteration 85 : 0.3883802836445396
Loss in iteration 86 : 0.388604087526992
Loss in iteration 87 : 0.388832243880441
Loss in iteration 88 : 0.38907050902091267
Loss in iteration 89 : 0.3893138202471972
Loss in iteration 90 : 0.3895603007281331
Loss in iteration 91 : 0.3898066722040776
Loss in iteration 92 : 0.390052624376937
Loss in iteration 93 : 0.39029634885503806
Loss in iteration 94 : 0.3905385483720417
Loss in iteration 95 : 0.3907810924202054
Loss in iteration 96 : 0.3910262275619356
Loss in iteration 97 : 0.39127347995534195
Loss in iteration 98 : 0.39152278840996874
Loss in iteration 99 : 0.3917711271871231
Loss in iteration 100 : 0.3920170742335616
Loss in iteration 101 : 0.3922599543161222
Loss in iteration 102 : 0.39250220149214765
Loss in iteration 103 : 0.3927445291975184
Loss in iteration 104 : 0.3929901583136524
Loss in iteration 105 : 0.39323438297537616
Loss in iteration 106 : 0.3934769428542786
Loss in iteration 107 : 0.3937167750344772
Loss in iteration 108 : 0.393953459790393
Loss in iteration 109 : 0.39418872232201846
Loss in iteration 110 : 0.3944255187507663
Loss in iteration 111 : 0.3946608756739722
Loss in iteration 112 : 0.3948942316966087
Loss in iteration 113 : 0.3951258372882309
Loss in iteration 114 : 0.39535591906372297
Loss in iteration 115 : 0.395584681999343
Loss in iteration 116 : 0.39581247812383286
Loss in iteration 117 : 0.3960392665393685
Loss in iteration 118 : 0.3962651418077495
Loss in iteration 119 : 0.39649079301025525
Loss in iteration 120 : 0.39671593069365174
Loss in iteration 121 : 0.39694066229915975
Loss in iteration 122 : 0.3971650853021771
Loss in iteration 123 : 0.39738928764312653
Loss in iteration 124 : 0.39761334863468656
Loss in iteration 125 : 0.39783733978377206
Loss in iteration 126 : 0.39806124628085005
Loss in iteration 127 : 0.3982848844866502
Loss in iteration 128 : 0.3985080947069706
Loss in iteration 129 : 0.3987309249077158
Loss in iteration 130 : 0.39895340614189817
Loss in iteration 131 : 0.3991753073794871
Loss in iteration 132 : 0.3993965249652202
Loss in iteration 133 : 0.39961738029736293
Loss in iteration 134 : 0.3998376868051994
Loss in iteration 135 : 0.4000573767349894
Loss in iteration 136 : 0.4002766251576285
Loss in iteration 137 : 0.40049547369406746
Loss in iteration 138 : 0.40071364752495814
Loss in iteration 139 : 0.4009315740030182
Loss in iteration 140 : 0.40114912069683917
Loss in iteration 141 : 0.4013663671173002
Loss in iteration 142 : 0.401583385153715
Loss in iteration 143 : 0.401800239786428
Loss in iteration 144 : 0.4020169897341232
Loss in iteration 145 : 0.40223364871752065
Loss in iteration 146 : 0.4024499617030422
Loss in iteration 147 : 0.4026659977777534
Loss in iteration 148 : 0.40288182533383626
Loss in iteration 149 : 0.40309750612701606
Loss in iteration 150 : 0.40331309590735953
Loss in iteration 151 : 0.40352864499083996
Loss in iteration 152 : 0.40374374799281987
Loss in iteration 153 : 0.40395848849966537
Loss in iteration 154 : 0.40417294763664563
Loss in iteration 155 : 0.40438709791412075
Loss in iteration 156 : 0.40460099874182803
Loss in iteration 157 : 0.40481471099721933
Loss in iteration 158 : 0.4050282897242066
Loss in iteration 159 : 0.4052417846809554
Loss in iteration 160 : 0.4054552408371126
Loss in iteration 161 : 0.40566832690589916
Loss in iteration 162 : 0.4058811696333157
Loss in iteration 163 : 0.4060936553307895
Loss in iteration 164 : 0.40630553331466607
Loss in iteration 165 : 0.4065170032556744
Loss in iteration 166 : 0.40672796697310065
Loss in iteration 167 : 0.4069382813370351
Loss in iteration 168 : 0.40714847855995945
Loss in iteration 169 : 0.40735753333260805
Loss in iteration 170 : 0.4075656565619463
Loss in iteration 171 : 0.4077728345739203
Loss in iteration 172 : 0.4079785124182919
Loss in iteration 173 : 0.4081830304336288
Loss in iteration 174 : 0.40838638582011977
Loss in iteration 175 : 0.40858871805962804
Loss in iteration 176 : 0.4087901532846785
Loss in iteration 177 : 0.40899019027031425
Loss in iteration 178 : 0.4091885524427673
Loss in iteration 179 : 0.4093853226274761
Loss in iteration 180 : 0.40958014468368187
Loss in iteration 181 : 0.40977316089745314
Loss in iteration 182 : 0.40996401575988894
Loss in iteration 183 : 0.4101526723187149
Loss in iteration 184 : 0.41033959351413807
Loss in iteration 185 : 0.4105244314818163
Loss in iteration 186 : 0.41070727410752406
Loss in iteration 187 : 0.4108883292702669
Loss in iteration 188 : 0.41106778974196845
Loss in iteration 189 : 0.4112455859244729
Loss in iteration 190 : 0.41142195888844757
Loss in iteration 191 : 0.41159701564298096
Loss in iteration 192 : 0.4117704851078124
Loss in iteration 193 : 0.41194317581688406
Loss in iteration 194 : 0.4121152287868609
Loss in iteration 195 : 0.41228626636485044
Loss in iteration 196 : 0.4124562871283237
Loss in iteration 197 : 0.4126254060098148
Loss in iteration 198 : 0.41279373018154647
Loss in iteration 199 : 0.4129611340089569
Loss in iteration 200 : 0.4131279763041655
Testing accuracy  of updater 7 on alg 1 with rate 0.015680000000000003 = 0.78575, training accuracy 0.840401424409194, time elapsed: 3128 millisecond.
Loss in iteration 1 : 1.000000807268918
Loss in iteration 2 : 0.8449571236896504
Loss in iteration 3 : 0.609735039408818
Loss in iteration 4 : 0.5733949709917213
Loss in iteration 5 : 0.631752941263549
Loss in iteration 6 : 0.67599308435268
Loss in iteration 7 : 0.698433797654643
Loss in iteration 8 : 0.7009141660114657
Loss in iteration 9 : 0.6853474856920483
Loss in iteration 10 : 0.6535933276499126
Loss in iteration 11 : 0.6077547537517058
Loss in iteration 12 : 0.5522261673745753
Loss in iteration 13 : 0.5010675383938172
Loss in iteration 14 : 0.4699360898252874
Loss in iteration 15 : 0.4630586736433788
Loss in iteration 16 : 0.4703997074166476
Loss in iteration 17 : 0.4839069675692154
Loss in iteration 18 : 0.4944121698858725
Loss in iteration 19 : 0.4915130390300828
Loss in iteration 20 : 0.4746943757624599
Loss in iteration 21 : 0.45058818301904247
Loss in iteration 22 : 0.4273841653751081
Loss in iteration 23 : 0.41213785264669384
Loss in iteration 24 : 0.406338123554558
Loss in iteration 25 : 0.4075671260625212
Loss in iteration 26 : 0.41092288339324706
Loss in iteration 27 : 0.4132726438657815
Loss in iteration 28 : 0.4125867271622892
Loss in iteration 29 : 0.40848888860731764
Loss in iteration 30 : 0.401864679553707
Loss in iteration 31 : 0.39441314817418643
Loss in iteration 32 : 0.3885613126293598
Loss in iteration 33 : 0.3857165495477851
Loss in iteration 34 : 0.38503354698136366
Loss in iteration 35 : 0.38590367885604443
Loss in iteration 36 : 0.3871195330169048
Loss in iteration 37 : 0.38802455664355523
Loss in iteration 38 : 0.38811433216444197
Loss in iteration 39 : 0.3872915080555678
Loss in iteration 40 : 0.38580040780618513
Loss in iteration 41 : 0.38399491916376316
Loss in iteration 42 : 0.38234132002125437
Loss in iteration 43 : 0.3811193160159122
Loss in iteration 44 : 0.3803843645096218
Loss in iteration 45 : 0.38018216642633673
Loss in iteration 46 : 0.38043136754083773
Loss in iteration 47 : 0.3808686996222217
Loss in iteration 48 : 0.3812461303813396
Loss in iteration 49 : 0.38143780099546143
Loss in iteration 50 : 0.3813723963822516
Loss in iteration 51 : 0.3810782375524433
Loss in iteration 52 : 0.38068848490711854
Loss in iteration 53 : 0.3803068720567979
Loss in iteration 54 : 0.3799828878230257
Loss in iteration 55 : 0.37976979632367314
Loss in iteration 56 : 0.3797276285267401
Loss in iteration 57 : 0.3798423357623748
Loss in iteration 58 : 0.38002519109013216
Loss in iteration 59 : 0.38022095742543827
Loss in iteration 60 : 0.38039126994936784
Loss in iteration 61 : 0.38048082211278267
Loss in iteration 62 : 0.3804823470875528
Loss in iteration 63 : 0.38043789768649927
Loss in iteration 64 : 0.3803714657995079
Loss in iteration 65 : 0.38031110795023404
Loss in iteration 66 : 0.38028905690536025
Loss in iteration 67 : 0.38033819657860324
Loss in iteration 68 : 0.3804486313000946
Loss in iteration 69 : 0.38059177001020295
Loss in iteration 70 : 0.3807287359173672
Loss in iteration 71 : 0.3808460957923859
Loss in iteration 72 : 0.38093736896972374
Loss in iteration 73 : 0.38100509097460017
Loss in iteration 74 : 0.3810580488581418
Loss in iteration 75 : 0.3811210192486455
Loss in iteration 76 : 0.3811948874646991
Loss in iteration 77 : 0.3812875422169683
Loss in iteration 78 : 0.38140485038220423
Loss in iteration 79 : 0.38152688067434065
Loss in iteration 80 : 0.3816498527555074
Loss in iteration 81 : 0.38177095407527684
Loss in iteration 82 : 0.38188486773001384
Loss in iteration 83 : 0.38199058009357667
Loss in iteration 84 : 0.38208977614398015
Loss in iteration 85 : 0.3821845892774428
Loss in iteration 86 : 0.38228561050432197
Loss in iteration 87 : 0.38239316147924063
Loss in iteration 88 : 0.3825086906642541
Loss in iteration 89 : 0.38263019255120345
Loss in iteration 90 : 0.3827534406439012
Loss in iteration 91 : 0.3828772985885246
Loss in iteration 92 : 0.38300002592776455
Loss in iteration 93 : 0.3831218206454724
Loss in iteration 94 : 0.38324286124063117
Loss in iteration 95 : 0.3833633086302929
Loss in iteration 96 : 0.3834847207346219
Loss in iteration 97 : 0.38360767301908105
Loss in iteration 98 : 0.3837328111951185
Loss in iteration 99 : 0.38385939758560444
Loss in iteration 100 : 0.38398845338254695
Loss in iteration 101 : 0.3841189549285741
Loss in iteration 102 : 0.3842497114684527
Loss in iteration 103 : 0.3843794021865807
Loss in iteration 104 : 0.38450779180642475
Loss in iteration 105 : 0.38463554799314237
Loss in iteration 106 : 0.3847628940570992
Loss in iteration 107 : 0.38489118883800855
Loss in iteration 108 : 0.38502122275626194
Loss in iteration 109 : 0.38515231088633484
Loss in iteration 110 : 0.38528456550562495
Loss in iteration 111 : 0.3854187283189415
Loss in iteration 112 : 0.38555362532939547
Loss in iteration 113 : 0.385688920785097
Loss in iteration 114 : 0.38582462952415114
Loss in iteration 115 : 0.3859608471209206
Loss in iteration 116 : 0.38609754585263323
Loss in iteration 117 : 0.3862346729299323
Loss in iteration 118 : 0.3863726483662312
Loss in iteration 119 : 0.3865114841724377
Loss in iteration 120 : 0.38665100280825576
Loss in iteration 121 : 0.386790750258372
Loss in iteration 122 : 0.38693101666639906
Loss in iteration 123 : 0.3870709181960999
Loss in iteration 124 : 0.38721044726183335
Loss in iteration 125 : 0.38734991725230494
Loss in iteration 126 : 0.38749004534615283
Loss in iteration 127 : 0.3876306189979917
Loss in iteration 128 : 0.38777163160284894
Loss in iteration 129 : 0.3879138442678529
Loss in iteration 130 : 0.3880566624705875
Loss in iteration 131 : 0.3882000426079576
Loss in iteration 132 : 0.38834418463903236
Loss in iteration 133 : 0.38848864247882325
Loss in iteration 134 : 0.38863329211372555
Loss in iteration 135 : 0.38877805468370624
Loss in iteration 136 : 0.38892297686616806
Loss in iteration 137 : 0.389068100976945
Loss in iteration 138 : 0.38921346536939
Loss in iteration 139 : 0.38935948748043797
Loss in iteration 140 : 0.38950607767543277
Loss in iteration 141 : 0.3896530170683173
Loss in iteration 142 : 0.3898005313394351
Loss in iteration 143 : 0.3899482012441308
Loss in iteration 144 : 0.39009586506731575
Loss in iteration 145 : 0.3902436193759178
Loss in iteration 146 : 0.39039132568082
Loss in iteration 147 : 0.3905389728461722
Loss in iteration 148 : 0.39068673619087857
Loss in iteration 149 : 0.39083420931593693
Loss in iteration 150 : 0.39098116184154347
Loss in iteration 151 : 0.3911273816659148
Loss in iteration 152 : 0.39127345005889536
Loss in iteration 153 : 0.3914193850655013
Loss in iteration 154 : 0.3915655121232304
Loss in iteration 155 : 0.39171182997610504
Loss in iteration 156 : 0.39185798369847175
Loss in iteration 157 : 0.39200318247736315
Loss in iteration 158 : 0.39214778016128315
Loss in iteration 159 : 0.39229264276006937
Loss in iteration 160 : 0.39243724540167
Loss in iteration 161 : 0.3925815630675311
Loss in iteration 162 : 0.3927255932236813
Loss in iteration 163 : 0.3928694361151917
Loss in iteration 164 : 0.39301309915215465
Loss in iteration 165 : 0.39315664055239197
Loss in iteration 166 : 0.3932999411376766
Loss in iteration 167 : 0.393442783396461
Loss in iteration 168 : 0.39358526385578896
Loss in iteration 169 : 0.3937271994952444
Loss in iteration 170 : 0.39386869828495924
Loss in iteration 171 : 0.3940097094176609
Loss in iteration 172 : 0.39415031761085045
Loss in iteration 173 : 0.39429041090570166
Loss in iteration 174 : 0.39443022437292136
Loss in iteration 175 : 0.3945695439307528
Loss in iteration 176 : 0.3947084331654723
Loss in iteration 177 : 0.3948468821473277
Loss in iteration 178 : 0.3949849674563337
Loss in iteration 179 : 0.3951227584193067
Loss in iteration 180 : 0.39526031779113013
Loss in iteration 181 : 0.39539770237332744
Loss in iteration 182 : 0.3955349635754729
Loss in iteration 183 : 0.3956721479245331
Loss in iteration 184 : 0.39580928960898676
Loss in iteration 185 : 0.39594629398336467
Loss in iteration 186 : 0.3960834172164913
Loss in iteration 187 : 0.3962200232932943
Loss in iteration 188 : 0.39635613305702744
Loss in iteration 189 : 0.396491987629333
Loss in iteration 190 : 0.3966283047028916
Loss in iteration 191 : 0.39676502061097996
Loss in iteration 192 : 0.39690216973638054
Loss in iteration 193 : 0.39703948730287275
Loss in iteration 194 : 0.3971767981136662
Loss in iteration 195 : 0.3973142779459416
Loss in iteration 196 : 0.3974518977092491
Loss in iteration 197 : 0.39758967720634664
Loss in iteration 198 : 0.39772750953939734
Loss in iteration 199 : 0.39786551189581465
Loss in iteration 200 : 0.39800363283244666
Testing accuracy  of updater 7 on alg 1 with rate 0.008960000000000001 = 0.78575, training accuracy 0.840401424409194, time elapsed: 3238 millisecond.
Loss in iteration 1 : 1.0000000611468502
Loss in iteration 2 : 0.9612780525961376
Loss in iteration 3 : 0.8884112033093282
Loss in iteration 4 : 0.7850644777270723
Loss in iteration 5 : 0.671293469467388
Loss in iteration 6 : 0.5903544685939217
Loss in iteration 7 : 0.5624428489999941
Loss in iteration 8 : 0.5720122800283074
Loss in iteration 9 : 0.5921288803015526
Loss in iteration 10 : 0.611394018073736
Loss in iteration 11 : 0.6260448298222394
Loss in iteration 12 : 0.6354316667245432
Loss in iteration 13 : 0.6396246493456638
Loss in iteration 14 : 0.6390436651877798
Loss in iteration 15 : 0.6341450428550844
Loss in iteration 16 : 0.6253625366905233
Loss in iteration 17 : 0.6132062789689622
Loss in iteration 18 : 0.5982770612333147
Loss in iteration 19 : 0.5812623174863327
Loss in iteration 20 : 0.5633512865707018
Loss in iteration 21 : 0.5458881854496471
Loss in iteration 22 : 0.5305046895433814
Loss in iteration 23 : 0.5186166019601298
Loss in iteration 24 : 0.5111365278030497
Loss in iteration 25 : 0.5074501516414571
Loss in iteration 26 : 0.5062999959510281
Loss in iteration 27 : 0.5069313072791699
Loss in iteration 28 : 0.508228683602692
Loss in iteration 29 : 0.5092971120898382
Loss in iteration 30 : 0.5096816623374103
Loss in iteration 31 : 0.5089950345216728
Loss in iteration 32 : 0.5071198300709872
Loss in iteration 33 : 0.5041795970573363
Loss in iteration 34 : 0.5004922148287011
Loss in iteration 35 : 0.4963825167580538
Loss in iteration 36 : 0.4920627549838611
Loss in iteration 37 : 0.48783776286409425
Loss in iteration 38 : 0.48395343365553956
Loss in iteration 39 : 0.48045847128688435
Loss in iteration 40 : 0.4775193271013621
Loss in iteration 41 : 0.4749794345819317
Loss in iteration 42 : 0.47281489327000037
Loss in iteration 43 : 0.4710340075166605
Loss in iteration 44 : 0.4694907908747869
Loss in iteration 45 : 0.4680623535870825
Loss in iteration 46 : 0.46668424212908505
Loss in iteration 47 : 0.4653142477469221
Loss in iteration 48 : 0.46388654651671746
Loss in iteration 49 : 0.46238088499864055
Loss in iteration 50 : 0.46079421020407024
Loss in iteration 51 : 0.4591259174652266
Loss in iteration 52 : 0.4573835220661162
Loss in iteration 53 : 0.4555841306809622
Loss in iteration 54 : 0.45374712622559554
Loss in iteration 55 : 0.4518838848316794
Loss in iteration 56 : 0.4500162525472971
Loss in iteration 57 : 0.44815860115209094
Loss in iteration 58 : 0.44632079653601203
Loss in iteration 59 : 0.44454828057290807
Loss in iteration 60 : 0.44285015913262427
Loss in iteration 61 : 0.44118495194554924
Loss in iteration 62 : 0.4395593743419768
Loss in iteration 63 : 0.4379947058731483
Loss in iteration 64 : 0.4364900753924551
Loss in iteration 65 : 0.4350296018462282
Loss in iteration 66 : 0.43360501437444726
Loss in iteration 67 : 0.4322132347340834
Loss in iteration 68 : 0.43083040064611217
Loss in iteration 69 : 0.42944967251523997
Loss in iteration 70 : 0.4280758438496273
Loss in iteration 71 : 0.42671948806995136
Loss in iteration 72 : 0.4253767411297752
Loss in iteration 73 : 0.4240419738004069
Loss in iteration 74 : 0.4227213517428802
Loss in iteration 75 : 0.4214236014955697
Loss in iteration 76 : 0.4201570041271277
Loss in iteration 77 : 0.4189151849524628
Loss in iteration 78 : 0.4176983864657486
Loss in iteration 79 : 0.4165200906070952
Loss in iteration 80 : 0.4153796330047925
Loss in iteration 81 : 0.4142654542951375
Loss in iteration 82 : 0.41318566593710204
Loss in iteration 83 : 0.4121415285129416
Loss in iteration 84 : 0.4111360887459139
Loss in iteration 85 : 0.4101599271063478
Loss in iteration 86 : 0.40922040676027366
Loss in iteration 87 : 0.4083122143823092
Loss in iteration 88 : 0.40743486080539454
Loss in iteration 89 : 0.40658163333411795
Loss in iteration 90 : 0.4057571042552439
Loss in iteration 91 : 0.4049622639511789
Loss in iteration 92 : 0.40419289362621474
Loss in iteration 93 : 0.40344812480094217
Loss in iteration 94 : 0.4027196114932077
Loss in iteration 95 : 0.4020123872802011
Loss in iteration 96 : 0.4013374272356847
Loss in iteration 97 : 0.40069322821734943
Loss in iteration 98 : 0.40007430927511134
Loss in iteration 99 : 0.39948071479571634
Loss in iteration 100 : 0.3989189580932496
Loss in iteration 101 : 0.39838530606310774
Loss in iteration 102 : 0.3978726336664006
Loss in iteration 103 : 0.39739426967864105
Loss in iteration 104 : 0.39694064197716683
Loss in iteration 105 : 0.3965011719403249
Loss in iteration 106 : 0.3960738767971538
Loss in iteration 107 : 0.39566606017639866
Loss in iteration 108 : 0.3952706559063618
Loss in iteration 109 : 0.39489043022465253
Loss in iteration 110 : 0.3945236921817721
Loss in iteration 111 : 0.39417288160902636
Loss in iteration 112 : 0.3938342403894644
Loss in iteration 113 : 0.39350681727380665
Loss in iteration 114 : 0.39319250452945803
Loss in iteration 115 : 0.39289390276822306
Loss in iteration 116 : 0.39261538374498695
Loss in iteration 117 : 0.39234656328931344
Loss in iteration 118 : 0.39209136259239763
Loss in iteration 119 : 0.39184916882514303
Loss in iteration 120 : 0.39161727987643546
Loss in iteration 121 : 0.391398730699112
Loss in iteration 122 : 0.391185072169108
Loss in iteration 123 : 0.39097676729183367
Loss in iteration 124 : 0.39077204267735366
Loss in iteration 125 : 0.3905721161140386
Loss in iteration 126 : 0.39037634418541733
Loss in iteration 127 : 0.3901863622293561
Loss in iteration 128 : 0.39000198426841676
Loss in iteration 129 : 0.38982136114521054
Loss in iteration 130 : 0.3896474662295179
Loss in iteration 131 : 0.38947938955998435
Loss in iteration 132 : 0.38931616380461725
Loss in iteration 133 : 0.38915869747225307
Loss in iteration 134 : 0.3890075789462072
Loss in iteration 135 : 0.38886046184076395
Loss in iteration 136 : 0.3887170627496076
Loss in iteration 137 : 0.3885766886155005
Loss in iteration 138 : 0.38843968746529345
Loss in iteration 139 : 0.3883089268858175
Loss in iteration 140 : 0.38818191797014395
Loss in iteration 141 : 0.388058850089272
Loss in iteration 142 : 0.3879395064488775
Loss in iteration 143 : 0.3878229739680168
Loss in iteration 144 : 0.3877079885029954
Loss in iteration 145 : 0.3875951632675963
Loss in iteration 146 : 0.38748579361144403
Loss in iteration 147 : 0.38737959688612267
Loss in iteration 148 : 0.3872751218053903
Loss in iteration 149 : 0.38717151634312763
Loss in iteration 150 : 0.38706998993379965
Loss in iteration 151 : 0.3869703639501933
Loss in iteration 152 : 0.3868726590304241
Loss in iteration 153 : 0.38677710057671744
Loss in iteration 154 : 0.38668487440047644
Loss in iteration 155 : 0.38659640990504307
Loss in iteration 156 : 0.38650979328567947
Loss in iteration 157 : 0.38642513472294965
Loss in iteration 158 : 0.3863442320145256
Loss in iteration 159 : 0.3862659307989326
Loss in iteration 160 : 0.3861895456947516
Loss in iteration 161 : 0.3861160236778932
Loss in iteration 162 : 0.3860455333237386
Loss in iteration 163 : 0.3859784012144152
Loss in iteration 164 : 0.3859138423331539
Loss in iteration 165 : 0.3858512551026877
Loss in iteration 166 : 0.3857897635043202
Loss in iteration 167 : 0.3857301628755562
Loss in iteration 168 : 0.3856725029941242
Loss in iteration 169 : 0.38561662127581553
Loss in iteration 170 : 0.3855625633106871
Loss in iteration 171 : 0.38550966376828366
Loss in iteration 172 : 0.3854583821677282
Loss in iteration 173 : 0.38540873439636986
Loss in iteration 174 : 0.3853604012260075
Testing accuracy  of updater 7 on alg 1 with rate 0.002239999999999999 = 0.77975, training accuracy 0.840401424409194, time elapsed: 3098 millisecond.
Loss in iteration 1 : 1.000905406461194
Loss in iteration 2 : 2.526582248968343
Loss in iteration 3 : 2.4255551633438754
Loss in iteration 4 : 2.07103760712691
Loss in iteration 5 : 1.529048926278824
Loss in iteration 6 : 0.8499233830086318
Loss in iteration 7 : 0.4440537400544689
Loss in iteration 8 : 0.5787319049400929
Loss in iteration 9 : 0.6692683016229741
Loss in iteration 10 : 0.6187406590189579
Loss in iteration 11 : 0.5530620925896224
Loss in iteration 12 : 0.5187986159437958
Loss in iteration 13 : 0.5157853077875546
Loss in iteration 14 : 0.5272328301211018
Loss in iteration 15 : 0.5364505086295688
Loss in iteration 16 : 0.5384092997234551
Loss in iteration 17 : 0.5350291454354505
Loss in iteration 18 : 0.5304521531513146
Loss in iteration 19 : 0.5267233412212343
Loss in iteration 20 : 0.524580939269373
Loss in iteration 21 : 0.5222331507446059
Loss in iteration 22 : 0.5177466640318673
Loss in iteration 23 : 0.5123829430602093
Loss in iteration 24 : 0.5067903491687524
Loss in iteration 25 : 0.5009056545996595
Loss in iteration 26 : 0.49455666161940626
Loss in iteration 27 : 0.4877390439140149
Loss in iteration 28 : 0.4806044688666419
Loss in iteration 29 : 0.47327334159751416
Loss in iteration 30 : 0.46586408739819574
Loss in iteration 31 : 0.458488077506218
Loss in iteration 32 : 0.45122473665926205
Loss in iteration 33 : 0.4441409868649944
Loss in iteration 34 : 0.4375053191712795
Loss in iteration 35 : 0.4315535024722882
Loss in iteration 36 : 0.42617768697737846
Loss in iteration 37 : 0.4214946883808265
Loss in iteration 38 : 0.41808176593265345
Loss in iteration 39 : 0.4163404870435812
Loss in iteration 40 : 0.41618762455037295
Loss in iteration 41 : 0.4177469209551532
Loss in iteration 42 : 0.4206906298671867
Loss in iteration 43 : 0.4243170763531538
Loss in iteration 44 : 0.42807282513955447
Loss in iteration 45 : 0.43136393450547317
Loss in iteration 46 : 0.433938378513237
Loss in iteration 47 : 0.4363187334705054
Loss in iteration 48 : 0.43881073562651207
Loss in iteration 49 : 0.4435366328010373
Loss in iteration 50 : 0.44984389368626904
Loss in iteration 51 : 0.4651858913282178
Loss in iteration 52 : 0.4697615502094802
Loss in iteration 53 : 0.47888084258840946
Loss in iteration 54 : 0.4645021328986233
Loss in iteration 55 : 0.4583674276300626
Loss in iteration 56 : 0.4522226052131185
Loss in iteration 57 : 0.45188104611537766
Loss in iteration 58 : 0.45312972283126707
Loss in iteration 59 : 0.4544874460668263
Loss in iteration 60 : 0.4560701496628109
Loss in iteration 61 : 0.4575320007562118
Loss in iteration 62 : 0.45863994341711983
Loss in iteration 63 : 0.45950094634457006
Loss in iteration 64 : 0.46047418747853663
Loss in iteration 65 : 0.46107078133404467
Loss in iteration 66 : 0.46179468209126223
Loss in iteration 67 : 0.46261346446230733
Loss in iteration 68 : 0.46343072770008203
Loss in iteration 69 : 0.4642503536758025
Loss in iteration 70 : 0.46517836803800616
Loss in iteration 71 : 0.4660080413611142
Loss in iteration 72 : 0.4669910157848727
Loss in iteration 73 : 0.4678960581397918
Loss in iteration 74 : 0.4689917950657508
Loss in iteration 75 : 0.4699190416322829
Loss in iteration 76 : 0.4714239153752745
Loss in iteration 77 : 0.4733617966072475
Loss in iteration 78 : 0.4784169308851865
Loss in iteration 79 : 0.4874848425280277
Loss in iteration 80 : 0.5097293629387842
Loss in iteration 81 : 0.516938937331728
Loss in iteration 82 : 0.5384943502052705
Loss in iteration 83 : 0.5201465305752471
Loss in iteration 84 : 0.5203038475843555
Loss in iteration 85 : 0.4994373030337207
Loss in iteration 86 : 0.49376433988477475
Loss in iteration 87 : 0.4888908113873887
Loss in iteration 88 : 0.4881740141234682
Loss in iteration 89 : 0.4879926218005038
Loss in iteration 90 : 0.4872867653792041
Loss in iteration 91 : 0.4880168166189884
Loss in iteration 92 : 0.4877350390625569
Loss in iteration 93 : 0.48889728166356206
Loss in iteration 94 : 0.4908725538475318
Loss in iteration 95 : 0.4911248250921166
Loss in iteration 96 : 0.4953048633024467
Loss in iteration 97 : 0.4983105645771537
Loss in iteration 98 : 0.5109246810646321
Loss in iteration 99 : 0.519566879577849
Loss in iteration 100 : 0.5563482640609689
Loss in iteration 101 : 0.5498587898258437
Loss in iteration 102 : 0.5766366849544959
Loss in iteration 103 : 0.5324977216046681
Loss in iteration 104 : 0.5259358477156908
Loss in iteration 105 : 0.5143999382059733
Loss in iteration 106 : 0.5093323523842734
Loss in iteration 107 : 0.5047899430984484
Loss in iteration 108 : 0.5029658029367207
Loss in iteration 109 : 0.5028565199693584
Loss in iteration 110 : 0.5019095734774477
Loss in iteration 111 : 0.5023847072408731
Loss in iteration 112 : 0.5022031283189633
Loss in iteration 113 : 0.5026401432989089
Loss in iteration 114 : 0.5048111261762833
Loss in iteration 115 : 0.5059394688528166
Loss in iteration 116 : 0.515865714956139
Loss in iteration 117 : 0.528239828892967
Loss in iteration 118 : 0.5586359222857211
Loss in iteration 119 : 0.5554114138212425
Loss in iteration 120 : 0.5927425503636856
Loss in iteration 121 : 0.5508259427179822
Loss in iteration 122 : 0.5508480098027179
Loss in iteration 123 : 0.5290079191490344
Loss in iteration 124 : 0.5214173345412385
Loss in iteration 125 : 0.5156505949737337
Loss in iteration 126 : 0.513170730042871
Loss in iteration 127 : 0.5125168867889154
Loss in iteration 128 : 0.5114326236533349
Loss in iteration 129 : 0.510844190881131
Loss in iteration 130 : 0.5102228041467916
Loss in iteration 131 : 0.5102738096738583
Loss in iteration 132 : 0.5120881603105714
Loss in iteration 133 : 0.5131656788065622
Loss in iteration 134 : 0.5219382047235781
Loss in iteration 135 : 0.5335508746383073
Loss in iteration 136 : 0.5670918583571338
Loss in iteration 137 : 0.5638498097584446
Loss in iteration 138 : 0.6031543654765363
Loss in iteration 139 : 0.5564988128285973
Loss in iteration 140 : 0.557386324875314
Loss in iteration 141 : 0.5349027159708069
Loss in iteration 142 : 0.5265941685272846
Loss in iteration 143 : 0.5212352274056781
Loss in iteration 144 : 0.5184240902865976
Loss in iteration 145 : 0.5176980642182686
Loss in iteration 146 : 0.5164252358225777
Loss in iteration 147 : 0.5156705284871809
Loss in iteration 148 : 0.5148236236185676
Loss in iteration 149 : 0.5142615588760259
Loss in iteration 150 : 0.5158727552601348
Loss in iteration 151 : 0.5163335487946545
Loss in iteration 152 : 0.5242436780113158
Loss in iteration 153 : 0.5393574647573844
Loss in iteration 154 : 0.578462145969242
Loss in iteration 155 : 0.5737484653911658
Loss in iteration 156 : 0.6151482108979623
Loss in iteration 157 : 0.5625902358831936
Loss in iteration 158 : 0.5633334647302443
Loss in iteration 159 : 0.5396002759516649
Loss in iteration 160 : 0.5299132172100394
Loss in iteration 161 : 0.5250691171333204
Loss in iteration 162 : 0.522727663660216
Loss in iteration 163 : 0.5219472110463295
Loss in iteration 164 : 0.5207018064183824
Loss in iteration 165 : 0.5199208083391926
Loss in iteration 166 : 0.5188763312743162
Loss in iteration 167 : 0.5182194729705879
Loss in iteration 168 : 0.5202445232513825
Loss in iteration 169 : 0.5211146439346788
Loss in iteration 170 : 0.5311155537247186
Loss in iteration 171 : 0.5456466734475308
Loss in iteration 172 : 0.590289781259767
Loss in iteration 173 : 0.5829790780999075
Loss in iteration 174 : 0.6183038870596315
Loss in iteration 175 : 0.5630142596803104
Loss in iteration 176 : 0.5648732485683197
Loss in iteration 177 : 0.5437087735812575
Loss in iteration 178 : 0.5344888930564806
Loss in iteration 179 : 0.529062626868361
Loss in iteration 180 : 0.5266398434995445
Loss in iteration 181 : 0.525594192910191
Loss in iteration 182 : 0.5242005817493428
Loss in iteration 183 : 0.5228588042898457
Loss in iteration 184 : 0.5216013047470806
Loss in iteration 185 : 0.5211383035493629
Loss in iteration 186 : 0.5228769321924193
Loss in iteration 187 : 0.5234469665213781
Loss in iteration 188 : 0.5339816875476538
Loss in iteration 189 : 0.5472958058789282
Loss in iteration 190 : 0.5921520683899282
Loss in iteration 191 : 0.5885412003046173
Loss in iteration 192 : 0.6316046599299389
Loss in iteration 193 : 0.5668884336149912
Loss in iteration 194 : 0.5670713518499276
Loss in iteration 195 : 0.5465152538326586
Loss in iteration 196 : 0.5380916969806265
Loss in iteration 197 : 0.5324409734416622
Loss in iteration 198 : 0.5289857223045875
Loss in iteration 199 : 0.5281780442913816
Loss in iteration 200 : 0.5265551253458329
Testing accuracy  of updater 8 on alg 1 with rate 0.0392 = 0.795, training accuracy 0.8442861767562317, time elapsed: 3329 millisecond.
Loss in iteration 1 : 1.000491849695663
Loss in iteration 2 : 1.7490461923899552
Loss in iteration 3 : 1.820192782646137
Loss in iteration 4 : 1.6213531885982446
Loss in iteration 5 : 1.1870901809140237
Loss in iteration 6 : 0.6242778162999554
Loss in iteration 7 : 0.46959382673882344
Loss in iteration 8 : 0.5938201707101174
Loss in iteration 9 : 0.643309176337177
Loss in iteration 10 : 0.6039206086512366
Loss in iteration 11 : 0.5640855622320218
Loss in iteration 12 : 0.5481100932485434
Loss in iteration 13 : 0.555089308250913
Loss in iteration 14 : 0.5673449797865879
Loss in iteration 15 : 0.5744063226191977
Loss in iteration 16 : 0.5749800127671936
Loss in iteration 17 : 0.5716982294230313
Loss in iteration 18 : 0.5681559289796738
Loss in iteration 19 : 0.5655731555725251
Loss in iteration 20 : 0.5631146392727434
Loss in iteration 21 : 0.5592350464727498
Loss in iteration 22 : 0.5536553122205976
Loss in iteration 23 : 0.5473164302678615
Loss in iteration 24 : 0.5406836731597101
Loss in iteration 25 : 0.5335576328059493
Loss in iteration 26 : 0.5259480554660606
Loss in iteration 27 : 0.5179340770425178
Loss in iteration 28 : 0.509684539135409
Loss in iteration 29 : 0.5012096353977616
Loss in iteration 30 : 0.4925376158349993
Loss in iteration 31 : 0.4839098325440734
Loss in iteration 32 : 0.47541854934761496
Loss in iteration 33 : 0.4671545220496164
Loss in iteration 34 : 0.4592643188470791
Loss in iteration 35 : 0.4519238487331678
Loss in iteration 36 : 0.44547638744854345
Loss in iteration 37 : 0.43974945171045887
Loss in iteration 38 : 0.4350081749725765
Loss in iteration 39 : 0.43174371185096017
Loss in iteration 40 : 0.43012632529704037
Loss in iteration 41 : 0.43026996258100714
Loss in iteration 42 : 0.4322210273221232
Loss in iteration 43 : 0.4355173556864228
Loss in iteration 44 : 0.43926922605062446
Loss in iteration 45 : 0.44294458175051
Loss in iteration 46 : 0.44589926792211787
Loss in iteration 47 : 0.448131920875209
Loss in iteration 48 : 0.4494917370117365
Loss in iteration 49 : 0.45025636870829105
Loss in iteration 50 : 0.4509400039941608
Loss in iteration 51 : 0.45121281738084684
Loss in iteration 52 : 0.45264365174811105
Loss in iteration 53 : 0.45294233195489103
Loss in iteration 54 : 0.45352070224127217
Loss in iteration 55 : 0.4547419263460249
Loss in iteration 56 : 0.4560811966932121
Loss in iteration 57 : 0.45748014653381597
Loss in iteration 58 : 0.45888611607521784
Loss in iteration 59 : 0.4602743530822043
Loss in iteration 60 : 0.4616145127440604
Loss in iteration 61 : 0.4629005755109291
Loss in iteration 62 : 0.4641221669860482
Loss in iteration 63 : 0.465281381840549
Loss in iteration 64 : 0.466343720238789
Loss in iteration 65 : 0.4673497682848049
Loss in iteration 66 : 0.46830361411023286
Loss in iteration 67 : 0.46918951406363213
Loss in iteration 68 : 0.4700224084628362
Loss in iteration 69 : 0.47080464247322784
Loss in iteration 70 : 0.4715444090903018
Loss in iteration 71 : 0.47223986359532794
Loss in iteration 72 : 0.4729203984737356
Loss in iteration 73 : 0.473566448895753
Loss in iteration 74 : 0.47418325425811036
Loss in iteration 75 : 0.4747659385142817
Loss in iteration 76 : 0.4753112616916684
Loss in iteration 77 : 0.47582944411976485
Loss in iteration 78 : 0.47637052445722944
Loss in iteration 79 : 0.4768422192715509
Loss in iteration 80 : 0.47735787817577646
Loss in iteration 81 : 0.4777656276773189
Loss in iteration 82 : 0.4782517867054631
Loss in iteration 83 : 0.47861115809837934
Loss in iteration 84 : 0.47899868311422344
Loss in iteration 85 : 0.4793113342689833
Loss in iteration 86 : 0.47968098689522076
Loss in iteration 87 : 0.47998811204497027
Loss in iteration 88 : 0.48022484125611586
Loss in iteration 89 : 0.48050391659820907
Loss in iteration 90 : 0.4807645657785335
Loss in iteration 91 : 0.48102630576312405
Loss in iteration 92 : 0.4813348522048763
Loss in iteration 93 : 0.481562813963547
Testing accuracy  of updater 8 on alg 1 with rate 0.02744 = 0.78825, training accuracy 0.8433149886694723, time elapsed: 1844 millisecond.
Loss in iteration 1 : 1.0001701052000127
Loss in iteration 2 : 1.192818965174486
Loss in iteration 3 : 1.236556037341118
Loss in iteration 4 : 1.1153790076743981
Loss in iteration 5 : 0.8495224597584956
Loss in iteration 6 : 0.4883971296713498
Loss in iteration 7 : 0.40590466802842995
Loss in iteration 8 : 0.48645565682126174
Loss in iteration 9 : 0.48989199987517607
Loss in iteration 10 : 0.457454802903345
Loss in iteration 11 : 0.43676125952286554
Loss in iteration 12 : 0.43288024635908257
Loss in iteration 13 : 0.4392413350595554
Loss in iteration 14 : 0.4469121724346597
Loss in iteration 15 : 0.4507327165337196
Loss in iteration 16 : 0.45137030441305875
Loss in iteration 17 : 0.4505682634831906
Loss in iteration 18 : 0.4502427996535369
Loss in iteration 19 : 0.4504865585790637
Loss in iteration 20 : 0.4502906484161226
Loss in iteration 21 : 0.4492621531823641
Loss in iteration 22 : 0.4475328898129606
Loss in iteration 23 : 0.44534334429674843
Loss in iteration 24 : 0.44293865886111566
Loss in iteration 25 : 0.44035734169867224
Loss in iteration 26 : 0.4375457973383296
Loss in iteration 27 : 0.434522865412595
Loss in iteration 28 : 0.4313127324578542
Loss in iteration 29 : 0.4279945705069576
Loss in iteration 30 : 0.42465477538779206
Loss in iteration 31 : 0.42140762068588
Loss in iteration 32 : 0.4182689609218113
Loss in iteration 33 : 0.4152683721265305
Loss in iteration 34 : 0.41244432189449975
Loss in iteration 35 : 0.4097285602303264
Loss in iteration 36 : 0.40718973968376376
Loss in iteration 37 : 0.4048723625707825
Loss in iteration 38 : 0.4028621122989057
Loss in iteration 39 : 0.4012742058832757
Loss in iteration 40 : 0.4001861038868046
Loss in iteration 41 : 0.3994974933475635
Loss in iteration 42 : 0.3993602635413131
Loss in iteration 43 : 0.3996264683371622
Loss in iteration 44 : 0.40034717090343686
Loss in iteration 45 : 0.4013468686510063
Loss in iteration 46 : 0.4026352145977365
Loss in iteration 47 : 0.40412352061725737
Loss in iteration 48 : 0.40571898944814105
Loss in iteration 49 : 0.40730065419902034
Loss in iteration 50 : 0.4088024115447685
Loss in iteration 51 : 0.41022923254884736
Loss in iteration 52 : 0.4115510376569504
Loss in iteration 53 : 0.41278059850478244
Loss in iteration 54 : 0.41391849895051885
Loss in iteration 55 : 0.41500307947842824
Loss in iteration 56 : 0.4160550626337406
Loss in iteration 57 : 0.4170754939356044
Loss in iteration 58 : 0.4180828236932824
Loss in iteration 59 : 0.4191136094624919
Loss in iteration 60 : 0.4201535438075491
Loss in iteration 61 : 0.4211966201175246
Loss in iteration 62 : 0.42226656580950256
Loss in iteration 63 : 0.423356427672539
Loss in iteration 64 : 0.42445313388482325
Loss in iteration 65 : 0.4255675744068649
Loss in iteration 66 : 0.4266823238465473
Loss in iteration 67 : 0.427792062630969
Loss in iteration 68 : 0.42890168512477
Loss in iteration 69 : 0.4299937195673438
Loss in iteration 70 : 0.43106206552688564
Loss in iteration 71 : 0.4321055066939086
Loss in iteration 72 : 0.4331299818577289
Loss in iteration 73 : 0.43413138411342406
Loss in iteration 74 : 0.43510548684138306
Loss in iteration 75 : 0.43604782840650747
Loss in iteration 76 : 0.43697258093040253
Loss in iteration 77 : 0.4378669672523562
Loss in iteration 78 : 0.43872966542919994
Loss in iteration 79 : 0.4395725908753533
Loss in iteration 80 : 0.44039950576773856
Loss in iteration 81 : 0.44121166570018977
Loss in iteration 82 : 0.4420184351853576
Loss in iteration 83 : 0.4428072127090854
Loss in iteration 84 : 0.443589287297245
Loss in iteration 85 : 0.44436103819366296
Loss in iteration 86 : 0.4451289711147032
Loss in iteration 87 : 0.4458834833669189
Loss in iteration 88 : 0.44662776104264085
Loss in iteration 89 : 0.44736167035360497
Loss in iteration 90 : 0.4480757320755078
Loss in iteration 91 : 0.44877879733440273
Loss in iteration 92 : 0.44946979094787726
Loss in iteration 93 : 0.45015947692813607
Loss in iteration 94 : 0.4508425459609585
Loss in iteration 95 : 0.4515113822234185
Loss in iteration 96 : 0.45218100874781114
Loss in iteration 97 : 0.4528365395899965
Loss in iteration 98 : 0.45347938352185635
Loss in iteration 99 : 0.454115473143602
Loss in iteration 100 : 0.45473906701621436
Loss in iteration 101 : 0.4553610679698622
Loss in iteration 102 : 0.45597165412665147
Loss in iteration 103 : 0.4565680385997256
Loss in iteration 104 : 0.4571540034859538
Loss in iteration 105 : 0.45772893380119006
Loss in iteration 106 : 0.45829325055925485
Loss in iteration 107 : 0.4588394073961224
Loss in iteration 108 : 0.4593779043889279
Loss in iteration 109 : 0.4599033743972193
Loss in iteration 110 : 0.4604208960553295
Loss in iteration 111 : 0.4609259959077052
Loss in iteration 112 : 0.46142207430145193
Loss in iteration 113 : 0.46190704249081654
Loss in iteration 114 : 0.46237504573971766
Loss in iteration 115 : 0.46283422815745534
Loss in iteration 116 : 0.46328218232147494
Loss in iteration 117 : 0.46372107368758586
Loss in iteration 118 : 0.4641455843069117
Loss in iteration 119 : 0.464566318194911
Loss in iteration 120 : 0.46497756482415425
Loss in iteration 121 : 0.46538539051601463
Loss in iteration 122 : 0.46577601115498113
Loss in iteration 123 : 0.46616279531881144
Loss in iteration 124 : 0.4665444548902635
Loss in iteration 125 : 0.4669209623945287
Loss in iteration 126 : 0.4672928897934616
Loss in iteration 127 : 0.4676566019711043
Loss in iteration 128 : 0.46801514598590127
Loss in iteration 129 : 0.46837341388219417
Loss in iteration 130 : 0.4687219022791662
Loss in iteration 131 : 0.4690677594842523
Loss in iteration 132 : 0.4694105571680994
Loss in iteration 133 : 0.46974435286231014
Loss in iteration 134 : 0.47007226074985897
Loss in iteration 135 : 0.4703950088712109
Loss in iteration 136 : 0.47070386280264465
Loss in iteration 137 : 0.47100540624706205
Loss in iteration 138 : 0.47129992535381304
Loss in iteration 139 : 0.47158817001490555
Loss in iteration 140 : 0.47187081761476923
Loss in iteration 141 : 0.472148479947259
Loss in iteration 142 : 0.47242170948925855
Loss in iteration 143 : 0.47268916144364626
Loss in iteration 144 : 0.47295032551157534
Loss in iteration 145 : 0.47320655184159566
Loss in iteration 146 : 0.47345843597257276
Loss in iteration 147 : 0.47370950081108987
Loss in iteration 148 : 0.47395338937517295
Loss in iteration 149 : 0.47419271546906655
Loss in iteration 150 : 0.47442874632861026
Loss in iteration 151 : 0.47466219949405486
Loss in iteration 152 : 0.47489303834874796
Loss in iteration 153 : 0.47511818678355183
Loss in iteration 154 : 0.47534330984354434
Loss in iteration 155 : 0.47556372405587993
Testing accuracy  of updater 8 on alg 1 with rate 0.01568 = 0.78875, training accuracy 0.842667529944966, time elapsed: 2284 millisecond.
Loss in iteration 1 : 1.0000127664429173
Loss in iteration 2 : 0.5901023430193061
Loss in iteration 3 : 0.6083383034716675
Loss in iteration 4 : 0.5804929053981713
Loss in iteration 5 : 0.5234265639448676
Loss in iteration 6 : 0.47404122664391146
Loss in iteration 7 : 0.4501316416177062
Loss in iteration 8 : 0.43601638388539227
Loss in iteration 9 : 0.42376984521550926
Loss in iteration 10 : 0.41110050492025946
Loss in iteration 11 : 0.39793476155760754
Loss in iteration 12 : 0.3887564361677407
Loss in iteration 13 : 0.3835711582317012
Loss in iteration 14 : 0.3805446013616636
Loss in iteration 15 : 0.37875751150248693
Loss in iteration 16 : 0.3775550232906125
Loss in iteration 17 : 0.37659226725253947
Loss in iteration 18 : 0.375816922252652
Loss in iteration 19 : 0.3753903029997054
Loss in iteration 20 : 0.37541883362511824
Loss in iteration 21 : 0.375582118824712
Loss in iteration 22 : 0.37581672542471983
Loss in iteration 23 : 0.37607200229498394
Loss in iteration 24 : 0.3763276836265592
Loss in iteration 25 : 0.3765796439068681
Loss in iteration 26 : 0.3768473312644839
Loss in iteration 27 : 0.37712093971136257
Loss in iteration 28 : 0.3773872070162647
Loss in iteration 29 : 0.3776261101004934
Loss in iteration 30 : 0.3778464512648412
Loss in iteration 31 : 0.37803394925813355
Loss in iteration 32 : 0.37819293281107585
Loss in iteration 33 : 0.3783278568699337
Loss in iteration 34 : 0.3784401049075636
Loss in iteration 35 : 0.378529847437994
Loss in iteration 36 : 0.37859967892860286
Loss in iteration 37 : 0.3786572205008658
Loss in iteration 38 : 0.3787016991356507
Loss in iteration 39 : 0.3787342086840371
Loss in iteration 40 : 0.3787578670586449
Loss in iteration 41 : 0.37877419222589925
Loss in iteration 42 : 0.37879212137800905
Loss in iteration 43 : 0.3788076015003987
Loss in iteration 44 : 0.3788256393229189
Loss in iteration 45 : 0.3788469019955434
Loss in iteration 46 : 0.37887301608967494
Loss in iteration 47 : 0.37891046910426
Loss in iteration 48 : 0.3789517934245811
Loss in iteration 49 : 0.37899866602815785
Loss in iteration 50 : 0.3790493126991438
Loss in iteration 51 : 0.3791037524816361
Loss in iteration 52 : 0.37917044741443523
Loss in iteration 53 : 0.3792461292155316
Loss in iteration 54 : 0.37933302136812114
Loss in iteration 55 : 0.3794297810190909
Loss in iteration 56 : 0.379532867226913
Loss in iteration 57 : 0.3796425390850348
Loss in iteration 58 : 0.37975799768021007
Loss in iteration 59 : 0.37988963791543473
Loss in iteration 60 : 0.38002724751767797
Loss in iteration 61 : 0.38017313238796735
Loss in iteration 62 : 0.38032613788186187
Loss in iteration 63 : 0.38048841470214645
Loss in iteration 64 : 0.3806587951212076
Loss in iteration 65 : 0.3808339552070015
Loss in iteration 66 : 0.3810121417820834
Loss in iteration 67 : 0.3811977226384642
Loss in iteration 68 : 0.3813869099851406
Loss in iteration 69 : 0.38158000358889066
Loss in iteration 70 : 0.381776302650163
Loss in iteration 71 : 0.38197693357992296
Loss in iteration 72 : 0.3821812057143191
Loss in iteration 73 : 0.38238803135392985
Loss in iteration 74 : 0.3825981752159894
Loss in iteration 75 : 0.3828124072380964
Loss in iteration 76 : 0.3830287686746879
Loss in iteration 77 : 0.383249973872737
Loss in iteration 78 : 0.3834746865316968
Loss in iteration 79 : 0.3837024536453664
Loss in iteration 80 : 0.38393278528145386
Loss in iteration 81 : 0.3841644821273273
Loss in iteration 82 : 0.38439734019558686
Loss in iteration 83 : 0.384632740425924
Loss in iteration 84 : 0.38487005152973774
Loss in iteration 85 : 0.38510986018424664
Loss in iteration 86 : 0.38535117638391975
Loss in iteration 87 : 0.3855937983178848
Loss in iteration 88 : 0.38583780342558593
Loss in iteration 89 : 0.38608326157310724
Loss in iteration 90 : 0.3863297443765756
Loss in iteration 91 : 0.3865780798350965
Loss in iteration 92 : 0.3868277992290226
Loss in iteration 93 : 0.3870789644566649
Loss in iteration 94 : 0.38733153803157794
Loss in iteration 95 : 0.38758532291625636
Loss in iteration 96 : 0.3878404982159113
Loss in iteration 97 : 0.3880966825056742
Loss in iteration 98 : 0.3883539936281545
Loss in iteration 99 : 0.3886130305989546
Loss in iteration 100 : 0.3888732118905104
Loss in iteration 101 : 0.38913398330142673
Loss in iteration 102 : 0.38939625176765974
Loss in iteration 103 : 0.38965953896013333
Loss in iteration 104 : 0.3899245067444477
Loss in iteration 105 : 0.3901902362356543
Loss in iteration 106 : 0.39045709499190173
Loss in iteration 107 : 0.39072480518999936
Loss in iteration 108 : 0.3909928113740794
Loss in iteration 109 : 0.39126193383797847
Loss in iteration 110 : 0.39153153096981475
Loss in iteration 111 : 0.39180192467925695
Loss in iteration 112 : 0.392073205262864
Loss in iteration 113 : 0.39234495378983636
Loss in iteration 114 : 0.39261750086233244
Loss in iteration 115 : 0.39289073220309706
Loss in iteration 116 : 0.39316444193123423
Loss in iteration 117 : 0.3934386134703095
Loss in iteration 118 : 0.3937135663046942
Loss in iteration 119 : 0.3939884923999402
Loss in iteration 120 : 0.3942639919399697
Loss in iteration 121 : 0.3945391967206776
Loss in iteration 122 : 0.39481476225033646
Loss in iteration 123 : 0.39509060162585474
Loss in iteration 124 : 0.39536671849078586
Loss in iteration 125 : 0.39564247105620376
Loss in iteration 126 : 0.39591873983845755
Loss in iteration 127 : 0.39619453915318437
Loss in iteration 128 : 0.3964703681989473
Loss in iteration 129 : 0.3967460621057719
Loss in iteration 130 : 0.3970223867325397
Loss in iteration 131 : 0.39729859891223257
Loss in iteration 132 : 0.3975751208647391
Loss in iteration 133 : 0.39785058488662006
Loss in iteration 134 : 0.3981259611499905
Loss in iteration 135 : 0.39840324572267
Loss in iteration 136 : 0.39867859955607526
Loss in iteration 137 : 0.39895365422693135
Loss in iteration 138 : 0.3992285963426122
Loss in iteration 139 : 0.39950372746900464
Loss in iteration 140 : 0.39977911151583106
Loss in iteration 141 : 0.40005388112990586
Loss in iteration 142 : 0.40032880424315764
Loss in iteration 143 : 0.40060372980923525
Loss in iteration 144 : 0.40088052647039746
Loss in iteration 145 : 0.40115650262918634
Loss in iteration 146 : 0.40143275136380824
Loss in iteration 147 : 0.4017093197690266
Loss in iteration 148 : 0.40198807831557853
Loss in iteration 149 : 0.4022662585440585
Loss in iteration 150 : 0.40254494390148116
Loss in iteration 151 : 0.40282425518810583
Loss in iteration 152 : 0.40310397643385554
Loss in iteration 153 : 0.4033842005182634
Loss in iteration 154 : 0.4036639905455876
Loss in iteration 155 : 0.40394387111725716
Loss in iteration 156 : 0.4042236813045471
Loss in iteration 157 : 0.4045044612901364
Loss in iteration 158 : 0.4047852404748822
Loss in iteration 159 : 0.40506551561862636
Loss in iteration 160 : 0.40534493583482556
Loss in iteration 161 : 0.4056240077510551
Loss in iteration 162 : 0.4059026548398358
Loss in iteration 163 : 0.4061804836222761
Loss in iteration 164 : 0.4064573076685077
Loss in iteration 165 : 0.4067334269932378
Loss in iteration 166 : 0.40700929980586553
Loss in iteration 167 : 0.4072850733124679
Loss in iteration 168 : 0.4075611462209032
Loss in iteration 169 : 0.4078362797917333
Loss in iteration 170 : 0.4081112404230932
Loss in iteration 171 : 0.4083860083358936
Loss in iteration 172 : 0.4086602584268268
Loss in iteration 173 : 0.4089344517801878
Loss in iteration 174 : 0.40920904460980456
Loss in iteration 175 : 0.40948250731947183
Loss in iteration 176 : 0.4097554632044284
Loss in iteration 177 : 0.41002800561737945
Loss in iteration 178 : 0.410299023160043
Loss in iteration 179 : 0.41057037338927177
Loss in iteration 180 : 0.4108406562329945
Loss in iteration 181 : 0.41111101900424696
Loss in iteration 182 : 0.4113799498221263
Loss in iteration 183 : 0.41164874401237256
Loss in iteration 184 : 0.4119161041091659
Loss in iteration 185 : 0.4121827489325489
Loss in iteration 186 : 0.412448735414379
Loss in iteration 187 : 0.4127142025856029
Loss in iteration 188 : 0.41297905335987123
Loss in iteration 189 : 0.41324366282290725
Loss in iteration 190 : 0.41350794973095484
Loss in iteration 191 : 0.41377219493179224
Loss in iteration 192 : 0.4140362376507978
Loss in iteration 193 : 0.4143005181084782
Loss in iteration 194 : 0.4145646264392394
Loss in iteration 195 : 0.41482829391513804
Loss in iteration 196 : 0.41509306254001027
Loss in iteration 197 : 0.4153561853063008
Loss in iteration 198 : 0.41561832985181013
Loss in iteration 199 : 0.4158809921558618
Loss in iteration 200 : 0.41614364697262446
Testing accuracy  of updater 8 on alg 1 with rate 0.00392 = 0.78775, training accuracy 0.8413726124959534, time elapsed: 3666 millisecond.
Loss in iteration 1 : 1.0000078807557964
Loss in iteration 2 : 0.5555498593939651
Loss in iteration 3 : 0.5837786047696047
Loss in iteration 4 : 0.5763254440749668
Loss in iteration 5 : 0.5420407420354864
Loss in iteration 6 : 0.502300411848501
Loss in iteration 7 : 0.47597661673908626
Loss in iteration 8 : 0.46156783750940983
Loss in iteration 9 : 0.4489410471154804
Loss in iteration 10 : 0.43581013636784005
Loss in iteration 11 : 0.42265009802963494
Loss in iteration 12 : 0.41050645395688773
Loss in iteration 13 : 0.40013494053271786
Loss in iteration 14 : 0.3925719302213423
Loss in iteration 15 : 0.3875292464173818
Loss in iteration 16 : 0.3840643399579513
Loss in iteration 17 : 0.3814704959747522
Loss in iteration 18 : 0.3796400119309605
Loss in iteration 19 : 0.37823259557083444
Loss in iteration 20 : 0.37719141083775143
Loss in iteration 21 : 0.3763607615403521
Loss in iteration 22 : 0.3757484254028419
Loss in iteration 23 : 0.3753388377954259
Loss in iteration 24 : 0.3751738162569114
Loss in iteration 25 : 0.3751406541574627
Loss in iteration 26 : 0.3751562180964593
Loss in iteration 27 : 0.3752047141501439
Loss in iteration 28 : 0.3752783811112261
Loss in iteration 29 : 0.37537957960174095
Loss in iteration 30 : 0.37549828422159204
Loss in iteration 31 : 0.375618847865059
Loss in iteration 32 : 0.3757393287442381
Loss in iteration 33 : 0.3758581819657073
Loss in iteration 34 : 0.37596579463674684
Loss in iteration 35 : 0.3760635231947311
Loss in iteration 36 : 0.37615137740237875
Loss in iteration 37 : 0.37623145769236094
Loss in iteration 38 : 0.37630497732291435
Loss in iteration 39 : 0.3763722203154382
Loss in iteration 40 : 0.37643154285821956
Loss in iteration 41 : 0.37648380574373846
Loss in iteration 42 : 0.37652930516725464
Loss in iteration 43 : 0.3765689110139591
Loss in iteration 44 : 0.3766032375539433
Loss in iteration 45 : 0.37663335415793814
Loss in iteration 46 : 0.37666186773720467
Loss in iteration 47 : 0.3766892821877224
Loss in iteration 48 : 0.37671421329944615
Loss in iteration 49 : 0.3767378057895237
Loss in iteration 50 : 0.3767627496489674
Loss in iteration 51 : 0.3767883331503161
Loss in iteration 52 : 0.37681703972566016
Loss in iteration 53 : 0.37684675696369235
Loss in iteration 54 : 0.3768772651558468
Loss in iteration 55 : 0.3769101882817373
Loss in iteration 56 : 0.37694691305012756
Loss in iteration 57 : 0.3769868548479875
Loss in iteration 58 : 0.3770277995755595
Loss in iteration 59 : 0.37707348059835055
Loss in iteration 60 : 0.3771221193684362
Loss in iteration 61 : 0.3771732335325439
Loss in iteration 62 : 0.37723017473425235
Loss in iteration 63 : 0.3772898246520433
Loss in iteration 64 : 0.37735195633340646
Loss in iteration 65 : 0.37741575881399414
Loss in iteration 66 : 0.3774812695189006
Loss in iteration 67 : 0.3775502229596361
Loss in iteration 68 : 0.3776226376311294
Loss in iteration 69 : 0.3776981621325862
Loss in iteration 70 : 0.3777784361314446
Loss in iteration 71 : 0.37786064395418006
Loss in iteration 72 : 0.37794442271214407
Loss in iteration 73 : 0.37803072014704314
Loss in iteration 74 : 0.37812015204994226
Loss in iteration 75 : 0.3782137086147196
Loss in iteration 76 : 0.37830896783652257
Loss in iteration 77 : 0.3784061330506609
Loss in iteration 78 : 0.3785051088008587
Loss in iteration 79 : 0.3786055233713168
Loss in iteration 80 : 0.3787076226716523
Loss in iteration 81 : 0.37881250742923117
Loss in iteration 82 : 0.3789190810080399
Loss in iteration 83 : 0.37902787507638047
Loss in iteration 84 : 0.3791379476149615
Loss in iteration 85 : 0.3792498272068358
Loss in iteration 86 : 0.3793630657326694
Loss in iteration 87 : 0.3794779956705923
Loss in iteration 88 : 0.37959398893737056
Loss in iteration 89 : 0.3797110523066347
Loss in iteration 90 : 0.37982998567903914
Loss in iteration 91 : 0.3799507865541011
Loss in iteration 92 : 0.38007289975851954
Loss in iteration 93 : 0.38019618594086213
Loss in iteration 94 : 0.38032054620228467
Loss in iteration 95 : 0.38044598596446666
Loss in iteration 96 : 0.380572845951437
Loss in iteration 97 : 0.3807014226766814
Loss in iteration 98 : 0.38083114925461653
Loss in iteration 99 : 0.3809619499005127
Loss in iteration 100 : 0.3810942512513776
Loss in iteration 101 : 0.3812265696651185
Loss in iteration 102 : 0.3813605483397683
Loss in iteration 103 : 0.3814955016078786
Loss in iteration 104 : 0.38163157346857407
Loss in iteration 105 : 0.38176839895013304
Loss in iteration 106 : 0.3819060191162562
Loss in iteration 107 : 0.38204477797466013
Loss in iteration 108 : 0.3821840171135042
Loss in iteration 109 : 0.3823242073438731
Loss in iteration 110 : 0.38246515538834797
Loss in iteration 111 : 0.38260668137795695
Loss in iteration 112 : 0.38274940321453305
Loss in iteration 113 : 0.3828922141793076
Loss in iteration 114 : 0.38303572287817855
Loss in iteration 115 : 0.3831797159185253
Loss in iteration 116 : 0.38332423887825673
Loss in iteration 117 : 0.3834693330212357
Loss in iteration 118 : 0.38361503568938093
Loss in iteration 119 : 0.38376138066009896
Loss in iteration 120 : 0.38390854456116263
Loss in iteration 121 : 0.384056826823227
Loss in iteration 122 : 0.384205899806638
Loss in iteration 123 : 0.38435578034632284
Loss in iteration 124 : 0.3845064836113361
Loss in iteration 125 : 0.38465792052209213
Loss in iteration 126 : 0.38480901224671943
Loss in iteration 127 : 0.3849618915681183
Loss in iteration 128 : 0.3851150322224521
Loss in iteration 129 : 0.385269832303595
Loss in iteration 130 : 0.38542450984471466
Loss in iteration 131 : 0.3855799876730774
Loss in iteration 132 : 0.38573607638479895
Loss in iteration 133 : 0.3858928044181942
Loss in iteration 134 : 0.38605019746899777
Loss in iteration 135 : 0.38620827874301317
Loss in iteration 136 : 0.38636669774227644
Loss in iteration 137 : 0.3865257038331241
Loss in iteration 138 : 0.38668569796880614
Loss in iteration 139 : 0.38684602916254945
Loss in iteration 140 : 0.38700681025946604
Loss in iteration 141 : 0.3871686391229164
Loss in iteration 142 : 0.3873308087227927
Loss in iteration 143 : 0.3874941212582935
Loss in iteration 144 : 0.3876577370694212
Loss in iteration 145 : 0.3878216734911704
Loss in iteration 146 : 0.3879865144278512
Loss in iteration 147 : 0.3881518669189649
Loss in iteration 148 : 0.3883179614596678
Loss in iteration 149 : 0.38848446408559034
Loss in iteration 150 : 0.3886513136051805
Loss in iteration 151 : 0.38881889791177815
Loss in iteration 152 : 0.3889869391838862
Loss in iteration 153 : 0.38915541183318303
Loss in iteration 154 : 0.3893241850253493
Loss in iteration 155 : 0.38949310533208353
Loss in iteration 156 : 0.3896623307385011
Loss in iteration 157 : 0.38983175731278336
Loss in iteration 158 : 0.39000098191199145
Loss in iteration 159 : 0.39017067099793995
Loss in iteration 160 : 0.3903407845474626
Loss in iteration 161 : 0.3905109008809568
Loss in iteration 162 : 0.3906811485611966
Loss in iteration 163 : 0.39085141787601585
Loss in iteration 164 : 0.3910220405521035
Loss in iteration 165 : 0.39119275387506613
Loss in iteration 166 : 0.3913637675433369
Loss in iteration 167 : 0.3915351834601717
Loss in iteration 168 : 0.39170698614093485
Loss in iteration 169 : 0.39187939334716576
Loss in iteration 170 : 0.39205195506230617
Loss in iteration 171 : 0.3922249930031947
Loss in iteration 172 : 0.39239831659027885
Loss in iteration 173 : 0.3925716438464058
Loss in iteration 174 : 0.3927453405760734
Loss in iteration 175 : 0.3929194506663694
Loss in iteration 176 : 0.3930938588142463
Loss in iteration 177 : 0.39326852770719295
Loss in iteration 178 : 0.39344361186207916
Loss in iteration 179 : 0.39361888019887326
Loss in iteration 180 : 0.39379454687913584
Loss in iteration 181 : 0.39397047213833536
Loss in iteration 182 : 0.3941465459048708
Loss in iteration 183 : 0.39432226735471915
Loss in iteration 184 : 0.3944984769097247
Loss in iteration 185 : 0.3946747544033954
Loss in iteration 186 : 0.39485115473577764
Loss in iteration 187 : 0.39502772753912285
Loss in iteration 188 : 0.39520451767635123
Loss in iteration 189 : 0.3953815065904032
Loss in iteration 190 : 0.3955587620412941
Loss in iteration 191 : 0.39573590521567614
Loss in iteration 192 : 0.3959133016295446
Loss in iteration 193 : 0.3960904436478925
Loss in iteration 194 : 0.3962674321629927
Loss in iteration 195 : 0.3964445768694371
Loss in iteration 196 : 0.3966221102508234
Loss in iteration 197 : 0.3967998501297792
Loss in iteration 198 : 0.3969774793319718
Loss in iteration 199 : 0.39715537654193667
Loss in iteration 200 : 0.3973333374083484
Testing accuracy  of updater 8 on alg 1 with rate 0.002744 = 0.785, training accuracy 0.8397539656846876, time elapsed: 3007 millisecond.
Loss in iteration 1 : 1.0000029176845098
Loss in iteration 2 : 0.5868159767327135
Loss in iteration 3 : 0.5695761755854315
Loss in iteration 4 : 0.5930546416337477
Loss in iteration 5 : 0.5940310391356504
Loss in iteration 6 : 0.575075852960073
Loss in iteration 7 : 0.5430855371820061
Loss in iteration 8 : 0.5104210260836445
Loss in iteration 9 : 0.4875334328782719
Loss in iteration 10 : 0.4759904866095229
Loss in iteration 11 : 0.46863861778198956
Loss in iteration 12 : 0.46119175782336075
Loss in iteration 13 : 0.4519076995077878
Loss in iteration 14 : 0.44151748536748037
Loss in iteration 15 : 0.4310452726409893
Loss in iteration 16 : 0.42119310578127167
Loss in iteration 17 : 0.4123819954477545
Loss in iteration 18 : 0.4049205124325573
Loss in iteration 19 : 0.39909962429148166
Loss in iteration 20 : 0.39454484524171074
Loss in iteration 21 : 0.39093537710413545
Loss in iteration 22 : 0.38810797295385213
Loss in iteration 23 : 0.38586015697725556
Loss in iteration 24 : 0.38395317688141734
Loss in iteration 25 : 0.382319797674525
Loss in iteration 26 : 0.3809822137833019
Loss in iteration 27 : 0.3799024393781555
Loss in iteration 28 : 0.37900564572210216
Loss in iteration 29 : 0.3782870176873378
Loss in iteration 30 : 0.3776936899916163
Loss in iteration 31 : 0.37718213709879134
Loss in iteration 32 : 0.37675579135291704
Loss in iteration 33 : 0.3764090319245611
Loss in iteration 34 : 0.376109523968742
Loss in iteration 35 : 0.3758435276758247
Loss in iteration 36 : 0.37563124479540055
Loss in iteration 37 : 0.37546369944562585
Loss in iteration 38 : 0.3753294973254532
Loss in iteration 39 : 0.375216662083042
Loss in iteration 40 : 0.3751351015184647
Loss in iteration 41 : 0.37507225313582165
Loss in iteration 42 : 0.375015107549884
Loss in iteration 43 : 0.3749675942012205
Loss in iteration 44 : 0.37493554619107133
Loss in iteration 45 : 0.3749182503909444
Loss in iteration 46 : 0.37491093879654486
Loss in iteration 47 : 0.3749071912982299
Loss in iteration 48 : 0.37490487672764794
Loss in iteration 49 : 0.37490697354792346
Loss in iteration 50 : 0.37491365067022364
Loss in iteration 51 : 0.37492228094370783
Loss in iteration 52 : 0.37493256557083393
Loss in iteration 53 : 0.37494409169558895
Loss in iteration 54 : 0.37495608821151694
Loss in iteration 55 : 0.37496829052363695
Loss in iteration 56 : 0.3749815398503152
Loss in iteration 57 : 0.374995132246112
Loss in iteration 58 : 0.37500859240967577
Loss in iteration 59 : 0.37502228971058044
Loss in iteration 60 : 0.3750360649758868
Loss in iteration 61 : 0.3750500432245767
Loss in iteration 62 : 0.37506483662445445
Loss in iteration 63 : 0.3750802725913014
Loss in iteration 64 : 0.37509614220282855
Loss in iteration 65 : 0.3751123490327083
Loss in iteration 66 : 0.37512890213148964
Loss in iteration 67 : 0.37514580980151035
Loss in iteration 68 : 0.37516307964927825
Loss in iteration 69 : 0.3751807186353461
Loss in iteration 70 : 0.37519873312156443
Loss in iteration 71 : 0.3752171289156679
Loss in iteration 72 : 0.37523591131319145
Loss in iteration 73 : 0.3752552834602446
Loss in iteration 74 : 0.37527504533509787
Loss in iteration 75 : 0.37529527186372713
Loss in iteration 76 : 0.3753158936949529
Loss in iteration 77 : 0.37533727351352897
Loss in iteration 78 : 0.3753591544535735
Loss in iteration 79 : 0.3753816001149299
Loss in iteration 80 : 0.3754049115145569
Loss in iteration 81 : 0.3754289738395068
Loss in iteration 82 : 0.37545387892196785
Loss in iteration 83 : 0.3754793147862207
Loss in iteration 84 : 0.37550525550740105
Loss in iteration 85 : 0.37553169484907434
Loss in iteration 86 : 0.37555862716382143
Loss in iteration 87 : 0.375586047333492
Loss in iteration 88 : 0.37561412335113503
Loss in iteration 89 : 0.37564331476848756
Loss in iteration 90 : 0.37567328185178295
Loss in iteration 91 : 0.3757036448351389
Loss in iteration 92 : 0.37573457866927523
Loss in iteration 93 : 0.3757660269303199
Loss in iteration 94 : 0.37579829384568686
Loss in iteration 95 : 0.3758310894182281
Loss in iteration 96 : 0.3758644316092411
Loss in iteration 97 : 0.3758981852761346
Loss in iteration 98 : 0.3759324838743443
Loss in iteration 99 : 0.37596722290905416
Loss in iteration 100 : 0.37600240092129017
Loss in iteration 101 : 0.3760379103072518
Loss in iteration 102 : 0.3760741941527396
Loss in iteration 103 : 0.3761108834986926
Loss in iteration 104 : 0.37614797908115993
Loss in iteration 105 : 0.3761854815722462
Loss in iteration 106 : 0.37622339158175727
Loss in iteration 107 : 0.3762618176952816
Loss in iteration 108 : 0.376300657633013
Loss in iteration 109 : 0.376339974377963
Loss in iteration 110 : 0.37637985450357414
Loss in iteration 111 : 0.3764200821694382
Loss in iteration 112 : 0.3764607346099453
Loss in iteration 113 : 0.3765017476219674
Loss in iteration 114 : 0.3765430416943693
Loss in iteration 115 : 0.37658470253079546
Loss in iteration 116 : 0.3766269650320754
Loss in iteration 117 : 0.3766696097926544
Loss in iteration 118 : 0.37671263797459587
Loss in iteration 119 : 0.3767560506289436
Loss in iteration 120 : 0.37679999350597093
Loss in iteration 121 : 0.3768444699165483
Loss in iteration 122 : 0.3768892710683462
Loss in iteration 123 : 0.37693451098893604
Loss in iteration 124 : 0.37698011778032836
Loss in iteration 125 : 0.37702629002974963
Loss in iteration 126 : 0.3770728589024992
Loss in iteration 127 : 0.37711980989572264
Loss in iteration 128 : 0.3771671438856851
Loss in iteration 129 : 0.37721494903484015
Loss in iteration 130 : 0.37726326479759653
Loss in iteration 131 : 0.37731188118797643
Loss in iteration 132 : 0.3773608061002674
Loss in iteration 133 : 0.3774100466578419
Loss in iteration 134 : 0.37745960928534084
Loss in iteration 135 : 0.3775094997741669
Loss in iteration 136 : 0.377559723341897
Loss in iteration 137 : 0.3776102381575891
Loss in iteration 138 : 0.37766135329422634
Loss in iteration 139 : 0.377712824021154
Loss in iteration 140 : 0.37776465251762914
Loss in iteration 141 : 0.37781694998863524
Loss in iteration 142 : 0.37786956834886837
Loss in iteration 143 : 0.37792235918995776
Loss in iteration 144 : 0.37797538872772624
Loss in iteration 145 : 0.3780291888235973
Loss in iteration 146 : 0.3780832910837611
Loss in iteration 147 : 0.3781377012055916
Loss in iteration 148 : 0.3781924243531036
Loss in iteration 149 : 0.37824756737495935
Loss in iteration 150 : 0.37830305931390773
Loss in iteration 151 : 0.37835887289862286
Loss in iteration 152 : 0.3784150122689614
Loss in iteration 153 : 0.37847148115399054
Loss in iteration 154 : 0.378528277469693
Loss in iteration 155 : 0.3785856933949289
Loss in iteration 156 : 0.3786434747246778
Loss in iteration 157 : 0.3787016065455119
Loss in iteration 158 : 0.378760058817252
Loss in iteration 159 : 0.37881879483521597
Loss in iteration 160 : 0.3788779854811545
Loss in iteration 161 : 0.37893753429663557
Loss in iteration 162 : 0.3789974235743959
Loss in iteration 163 : 0.37905772866084486
Loss in iteration 164 : 0.37911832205731383
Loss in iteration 165 : 0.3791792147980466
Loss in iteration 166 : 0.37924039308007446
Loss in iteration 167 : 0.37930186264894555
Loss in iteration 168 : 0.37936362870411633
Loss in iteration 169 : 0.37942578642667896
Loss in iteration 170 : 0.37948806765473886
Loss in iteration 171 : 0.37955080907980143
Loss in iteration 172 : 0.3796138299227251
Loss in iteration 173 : 0.379677162869033
Loss in iteration 174 : 0.37974083841527484
Loss in iteration 175 : 0.37980487870309104
Loss in iteration 176 : 0.3798691167098104
Loss in iteration 177 : 0.37993356491590535
Loss in iteration 178 : 0.3799982346153225
Loss in iteration 179 : 0.3800632351754653
Loss in iteration 180 : 0.3801285139312373
Loss in iteration 181 : 0.38019401681831494
Loss in iteration 182 : 0.3802597534106612
Loss in iteration 183 : 0.38032569229544955
Loss in iteration 184 : 0.3803919912873397
Loss in iteration 185 : 0.38045858993037956
Loss in iteration 186 : 0.38052533962483687
Loss in iteration 187 : 0.3805922915945891
Loss in iteration 188 : 0.38065959954782413
Loss in iteration 189 : 0.3807271603581649
Loss in iteration 190 : 0.3807949905155104
Loss in iteration 191 : 0.3808630503770921
Loss in iteration 192 : 0.3809312044243027
Loss in iteration 193 : 0.3809999339700485
Loss in iteration 194 : 0.3810688612923344
Loss in iteration 195 : 0.3811381285422437
Loss in iteration 196 : 0.38120756386846144
Loss in iteration 197 : 0.38127730635351353
Loss in iteration 198 : 0.38134718741205537
Loss in iteration 199 : 0.3814174089413831
Loss in iteration 200 : 0.381487809145043
Testing accuracy  of updater 8 on alg 1 with rate 0.001568 = 0.7865, training accuracy 0.8397539656846876, time elapsed: 2796 millisecond.
Loss in iteration 1 : 1.0000001935946468
Loss in iteration 2 : 0.870502734689463
Loss in iteration 3 : 0.7081578097951141
Loss in iteration 4 : 0.5930351230038978
Loss in iteration 5 : 0.5551196351464359
Loss in iteration 6 : 0.5582925761700674
Loss in iteration 7 : 0.5710376426521789
Loss in iteration 8 : 0.5822855824626257
Loss in iteration 9 : 0.5889916280897736
Testing accuracy  of updater 8 on alg 1 with rate 3.92E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 142 millisecond.
Loss in iteration 1 : 1.0030118011953153
Loss in iteration 2 : 4.324603682857374
Loss in iteration 3 : 6.0328734674397175
Loss in iteration 4 : 6.117215180506061
Loss in iteration 5 : 4.976267026899937
Loss in iteration 6 : 2.89982036093064
Loss in iteration 7 : 1.254119853498662
Loss in iteration 8 : 1.6635818581122914
Loss in iteration 9 : 3.0818785280100527
Loss in iteration 10 : 3.4008059741214813
Loss in iteration 11 : 2.6401247896316886
Loss in iteration 12 : 1.9012321030237949
Loss in iteration 13 : 1.7693113415775148
Loss in iteration 14 : 2.0760815290071903
Loss in iteration 15 : 2.458542454632709
Loss in iteration 16 : 2.7220751053989685
Loss in iteration 17 : 2.773772040081837
Loss in iteration 18 : 2.6395752941120083
Loss in iteration 19 : 2.418929011929369
Loss in iteration 20 : 2.219323335580656
Loss in iteration 21 : 2.125910661927868
Loss in iteration 22 : 2.195488964044471
Loss in iteration 23 : 2.3688095042663146
Loss in iteration 24 : 2.4920310701021062
Loss in iteration 25 : 2.461318640123279
Loss in iteration 26 : 2.312296613188451
Loss in iteration 27 : 2.165828474505563
Loss in iteration 28 : 2.107704638582769
Loss in iteration 29 : 2.1364750938778867
Loss in iteration 30 : 2.1897385260850735
Loss in iteration 31 : 2.211944296357234
Loss in iteration 32 : 2.179887840175638
Loss in iteration 33 : 2.100424906699282
Loss in iteration 34 : 2.0041290707154897
Loss in iteration 35 : 1.9369968868559029
Loss in iteration 36 : 1.9335651166674657
Loss in iteration 37 : 1.9525123997480653
Loss in iteration 38 : 1.9340056738338947
Loss in iteration 39 : 1.854877134065848
Loss in iteration 40 : 1.762049169001077
Loss in iteration 41 : 1.7135655062767177
Loss in iteration 42 : 1.7092021111340654
Loss in iteration 43 : 1.6931797531496144
Loss in iteration 44 : 1.637460427407756
Loss in iteration 45 : 1.5556595519042835
Loss in iteration 46 : 1.5043034346266295
Loss in iteration 47 : 1.4981232319639943
Loss in iteration 48 : 1.4546656400439493
Loss in iteration 49 : 1.3619582081836143
Loss in iteration 50 : 1.333173900240891
Loss in iteration 51 : 1.319430238245268
Loss in iteration 52 : 1.2373316894527455
Loss in iteration 53 : 1.2390323406396673
Loss in iteration 54 : 1.1636811909145544
Loss in iteration 55 : 1.2276627392778365
Loss in iteration 56 : 1.1499904605981435
Loss in iteration 57 : 1.1374017648555785
Loss in iteration 58 : 1.2164647119239804
Loss in iteration 59 : 1.252661826042144
Loss in iteration 60 : 1.4768128621091856
Loss in iteration 61 : 1.3848691071181358
Loss in iteration 62 : 1.0922827282295815
Loss in iteration 63 : 1.3770468995029006
Loss in iteration 64 : 1.142631861471962
Loss in iteration 65 : 1.1818575172840113
Loss in iteration 66 : 1.2942328418814304
Loss in iteration 67 : 1.2394959228222664
Loss in iteration 68 : 1.1636183544449916
Loss in iteration 69 : 1.2160147598029263
Loss in iteration 70 : 1.2516316234728373
Loss in iteration 71 : 1.1747251892923138
Loss in iteration 72 : 1.1458194329834892
Loss in iteration 73 : 1.1781584531800462
Loss in iteration 74 : 1.1625540759786293
Loss in iteration 75 : 1.1002715068721325
Loss in iteration 76 : 1.085480851043916
Loss in iteration 77 : 1.0953417434752915
Loss in iteration 78 : 1.0299538400608208
Loss in iteration 79 : 1.0082702254063178
Loss in iteration 80 : 1.0148963172232637
Loss in iteration 81 : 0.9515136627030976
Loss in iteration 82 : 0.9580512884187824
Loss in iteration 83 : 0.905733332735241
Loss in iteration 84 : 0.9204811875405126
Loss in iteration 85 : 0.8743715727189918
Loss in iteration 86 : 0.9459511570080825
Loss in iteration 87 : 0.9418402536801103
Loss in iteration 88 : 0.8463232288862842
Loss in iteration 89 : 0.998216648290941
Loss in iteration 90 : 0.8940245021903808
Loss in iteration 91 : 0.9076063181608995
Loss in iteration 92 : 0.8096193202312674
Loss in iteration 93 : 0.8931703419548134
Loss in iteration 94 : 0.79395116200103
Loss in iteration 95 : 0.853762022150276
Loss in iteration 96 : 0.8215573173736993
Loss in iteration 97 : 0.7942954953675314
Loss in iteration 98 : 0.8250445511661745
Loss in iteration 99 : 0.7712435730675657
Loss in iteration 100 : 0.7773690492055232
Loss in iteration 101 : 0.7719535508078365
Loss in iteration 102 : 0.7334825955037033
Loss in iteration 103 : 0.7497092612251512
Loss in iteration 104 : 0.7054190048295845
Loss in iteration 105 : 0.7157540127222726
Loss in iteration 106 : 0.6879096767859796
Loss in iteration 107 : 0.6984649365337975
Loss in iteration 108 : 0.6574665146022115
Loss in iteration 109 : 0.6791225944675736
Loss in iteration 110 : 0.6447814775637213
Loss in iteration 111 : 0.6479494116807281
Loss in iteration 112 : 0.6523159534356651
Loss in iteration 113 : 0.6181498605064366
Loss in iteration 114 : 0.625032059729177
Loss in iteration 115 : 0.6217218729699615
Loss in iteration 116 : 0.5935052148495867
Loss in iteration 117 : 0.6065693410034981
Loss in iteration 118 : 0.5845942624628914
Loss in iteration 119 : 0.5842279651419341
Loss in iteration 120 : 0.5789576961172936
Loss in iteration 121 : 0.5701409467891224
Loss in iteration 122 : 0.5690773022803173
Loss in iteration 123 : 0.5572403180025842
Loss in iteration 124 : 0.5619338343159427
Loss in iteration 125 : 0.5446593138546965
Loss in iteration 126 : 0.5516010455128902
Loss in iteration 127 : 0.5361521925170678
Loss in iteration 128 : 0.5402081538810766
Loss in iteration 129 : 0.535235929257009
Loss in iteration 130 : 0.5268531301579504
Loss in iteration 131 : 0.5347208843963682
Loss in iteration 132 : 0.5246385091680945
Loss in iteration 133 : 0.5200423866349245
Loss in iteration 134 : 0.5262373651773754
Loss in iteration 135 : 0.5180593851707868
Loss in iteration 136 : 0.5142582442290816
Loss in iteration 137 : 0.5180098641458459
Loss in iteration 138 : 0.5117545085639678
Loss in iteration 139 : 0.5118088776230365
Loss in iteration 140 : 0.5130845177411434
Loss in iteration 141 : 0.506865743648076
Loss in iteration 142 : 0.5089840201014697
Loss in iteration 143 : 0.5063661104581185
Loss in iteration 144 : 0.5054032954256301
Loss in iteration 145 : 0.505762366672922
Loss in iteration 146 : 0.5036132064889576
Loss in iteration 147 : 0.5052755750644261
Loss in iteration 148 : 0.5031149649623878
Loss in iteration 149 : 0.5035723434667309
Loss in iteration 150 : 0.5042500711335546
Loss in iteration 151 : 0.5018446199367713
Loss in iteration 152 : 0.5030329479638433
Loss in iteration 153 : 0.5026938429498121
Testing accuracy  of updater 9 on alg 1 with rate 0.02744 = 0.791, training accuracy 0.842667529944966, time elapsed: 2359 millisecond.
Loss in iteration 1 : 1.000345670858201
Loss in iteration 2 : 1.5463676493770229
Loss in iteration 3 : 2.1723642702170523
Loss in iteration 4 : 2.3035462560437203
Loss in iteration 5 : 2.003671379917316
Loss in iteration 6 : 1.334588964409182
Loss in iteration 7 : 0.6826861933482617
Loss in iteration 8 : 0.6805456977722266
Loss in iteration 9 : 1.155305110755743
Loss in iteration 10 : 1.4175858361890288
Loss in iteration 11 : 1.2392088572056394
Loss in iteration 12 : 0.9343735368486337
Loss in iteration 13 : 0.7987924298918597
Loss in iteration 14 : 0.862417069934464
Loss in iteration 15 : 0.9977766696866687
Loss in iteration 16 : 1.1091842904112226
Loss in iteration 17 : 1.149955318112376
Loss in iteration 18 : 1.1153849811797683
Loss in iteration 19 : 1.0356623245354177
Loss in iteration 20 : 0.9518623137426898
Loss in iteration 21 : 0.9020236239636252
Loss in iteration 22 : 0.9123693524361133
Loss in iteration 23 : 0.9683575855481574
Loss in iteration 24 : 1.0148088592460593
Loss in iteration 25 : 1.0098666132842518
Loss in iteration 26 : 0.9574053317924116
Loss in iteration 27 : 0.8963606778298784
Loss in iteration 28 : 0.8640986249711322
Loss in iteration 29 : 0.8684281041689736
Loss in iteration 30 : 0.8860341228771241
Loss in iteration 31 : 0.8935209882322299
Loss in iteration 32 : 0.8781254496322146
Loss in iteration 33 : 0.8423210694851349
Loss in iteration 34 : 0.8013878037011709
Loss in iteration 35 : 0.7754834307156155
Loss in iteration 36 : 0.7748037744076783
Loss in iteration 37 : 0.7801293108189173
Loss in iteration 38 : 0.7672546843334902
Loss in iteration 39 : 0.7315904941566039
Loss in iteration 40 : 0.6978509383952793
Loss in iteration 41 : 0.6832798955765039
Loss in iteration 42 : 0.6835663235668948
Loss in iteration 43 : 0.6747089949382362
Loss in iteration 44 : 0.6457206938963859
Loss in iteration 45 : 0.6172451673829814
Loss in iteration 46 : 0.6107074778132867
Loss in iteration 47 : 0.6113595871922411
Loss in iteration 48 : 0.587151847388085
Loss in iteration 49 : 0.5632749745952166
Loss in iteration 50 : 0.5686320539505096
Loss in iteration 51 : 0.5631112059527519
Loss in iteration 52 : 0.541713008455337
Loss in iteration 53 : 0.5634691179678828
Loss in iteration 54 : 0.5462030116053394
Loss in iteration 55 : 0.5686090851182078
Loss in iteration 56 : 0.5594482629536555
Loss in iteration 57 : 0.5834996844466536
Loss in iteration 58 : 0.5609661737207268
Loss in iteration 59 : 0.5731390976519027
Loss in iteration 60 : 0.5580856774945435
Loss in iteration 61 : 0.5612690454670531
Loss in iteration 62 : 0.5519709763183286
Loss in iteration 63 : 0.5565301328371335
Loss in iteration 64 : 0.5492082794018518
Loss in iteration 65 : 0.5565390850878044
Loss in iteration 66 : 0.5521909434095301
Loss in iteration 67 : 0.552555418256567
Loss in iteration 68 : 0.5559930316142675
Loss in iteration 69 : 0.5507384039892382
Loss in iteration 70 : 0.553382857774782
Loss in iteration 71 : 0.551896269739151
Loss in iteration 72 : 0.5475649455190954
Loss in iteration 73 : 0.5490441370806279
Loss in iteration 74 : 0.5451204271354984
Loss in iteration 75 : 0.5435178485060753
Loss in iteration 76 : 0.5424549984453693
Loss in iteration 77 : 0.5383919052921432
Loss in iteration 78 : 0.538485232920779
Loss in iteration 79 : 0.5359335332067134
Loss in iteration 80 : 0.534937663190608
Loss in iteration 81 : 0.5339454258096008
Loss in iteration 82 : 0.5317280150138133
Loss in iteration 83 : 0.5317772749559977
Loss in iteration 84 : 0.5291097956638435
Loss in iteration 85 : 0.5289633807041451
Loss in iteration 86 : 0.5259685979366991
Loss in iteration 87 : 0.5254797409086917
Loss in iteration 88 : 0.522944842183371
Loss in iteration 89 : 0.52191060201183
Loss in iteration 90 : 0.5200497817713396
Loss in iteration 91 : 0.5189782311387691
Loss in iteration 92 : 0.5178722448342724
Loss in iteration 93 : 0.5167292447260892
Loss in iteration 94 : 0.5159507895217521
Loss in iteration 95 : 0.5149100020350111
Loss in iteration 96 : 0.5143312221844837
Loss in iteration 97 : 0.5135376090745558
Loss in iteration 98 : 0.5130347066429879
Loss in iteration 99 : 0.5121344398567509
Loss in iteration 100 : 0.5116837437042798
Loss in iteration 101 : 0.5110700786656871
Loss in iteration 102 : 0.5105203131259732
Loss in iteration 103 : 0.5099956943515179
Loss in iteration 104 : 0.5094477535832912
Loss in iteration 105 : 0.5090916644931899
Loss in iteration 106 : 0.5086545589216063
Loss in iteration 107 : 0.5083747335567095
Loss in iteration 108 : 0.5079960499937072
Loss in iteration 109 : 0.5078545039890944
Testing accuracy  of updater 9 on alg 1 with rate 0.019208 = 0.79, training accuracy 0.8429912593072192, time elapsed: 1422 millisecond.
Loss in iteration 1 : 1.0000805293980564
Loss in iteration 2 : 0.9280859664482058
Loss in iteration 3 : 1.2289603610549968
Loss in iteration 4 : 1.2882080414146193
Loss in iteration 5 : 1.1349977251365406
Loss in iteration 6 : 0.7959927211738632
Loss in iteration 7 : 0.4410422326025602
Loss in iteration 8 : 0.497687079378686
Loss in iteration 9 : 0.7539469072538036
Loss in iteration 10 : 0.8271316378115764
Loss in iteration 11 : 0.6861397404028781
Loss in iteration 12 : 0.5387940581784535
Loss in iteration 13 : 0.5022935854358836
Loss in iteration 14 : 0.5585996742017632
Loss in iteration 15 : 0.6356476260807027
Loss in iteration 16 : 0.6802453641240851
Loss in iteration 17 : 0.6779237491622951
Loss in iteration 18 : 0.6392273499161618
Loss in iteration 19 : 0.5897909787174332
Loss in iteration 20 : 0.5579600938446775
Loss in iteration 21 : 0.5592802572367858
Loss in iteration 22 : 0.5871793301280566
Loss in iteration 23 : 0.6150973734239488
Loss in iteration 24 : 0.622086314377608
Loss in iteration 25 : 0.6042843661521143
Loss in iteration 26 : 0.5740418976894338
Loss in iteration 27 : 0.5523512043155072
Loss in iteration 28 : 0.5471775073882287
Loss in iteration 29 : 0.5560582246462354
Loss in iteration 30 : 0.5654772717659173
Loss in iteration 31 : 0.5657454477649762
Loss in iteration 32 : 0.5536354975623066
Loss in iteration 33 : 0.5345236467986365
Loss in iteration 34 : 0.518470595980665
Loss in iteration 35 : 0.5132411066118823
Loss in iteration 36 : 0.5155914391040783
Loss in iteration 37 : 0.5173801091203214
Loss in iteration 38 : 0.5107240813139244
Loss in iteration 39 : 0.4963441246459835
Loss in iteration 40 : 0.4831790761329313
Loss in iteration 41 : 0.4765066508335851
Loss in iteration 42 : 0.4764453770066248
Loss in iteration 43 : 0.4757499028175531
Loss in iteration 44 : 0.46805346307355705
Loss in iteration 45 : 0.4570143993802601
Loss in iteration 46 : 0.4499953149465704
Loss in iteration 47 : 0.4494797508554105
Loss in iteration 48 : 0.4494876051503828
Loss in iteration 49 : 0.44272518454868537
Loss in iteration 50 : 0.43547755947796
Loss in iteration 51 : 0.4357839086933143
Loss in iteration 52 : 0.43858309585608235
Loss in iteration 53 : 0.4356599907788662
Loss in iteration 54 : 0.43201436942022636
Loss in iteration 55 : 0.4373836254239422
Loss in iteration 56 : 0.43949807092402876
Loss in iteration 57 : 0.4368186428347631
Loss in iteration 58 : 0.4416575435301879
Loss in iteration 59 : 0.44523299205752087
Loss in iteration 60 : 0.4439977911904116
Loss in iteration 61 : 0.447276757009841
Loss in iteration 62 : 0.44937905454528076
Loss in iteration 63 : 0.44793583286804683
Loss in iteration 64 : 0.4503030125065795
Loss in iteration 65 : 0.45130328780085116
Loss in iteration 66 : 0.45057832292594613
Loss in iteration 67 : 0.4521873669530947
Loss in iteration 68 : 0.453552007891036
Loss in iteration 69 : 0.4534425222926689
Loss in iteration 70 : 0.45462080902250945
Loss in iteration 71 : 0.45654604082369066
Loss in iteration 72 : 0.4574495668921658
Loss in iteration 73 : 0.45826937506657744
Loss in iteration 74 : 0.459975415261487
Loss in iteration 75 : 0.4614303185368476
Loss in iteration 76 : 0.46212241884831234
Loss in iteration 77 : 0.46317618791609805
Loss in iteration 78 : 0.4646456419983264
Loss in iteration 79 : 0.46566261101740153
Loss in iteration 80 : 0.46644289194687527
Loss in iteration 81 : 0.4675684583906549
Loss in iteration 82 : 0.46866321220687845
Loss in iteration 83 : 0.46940059459168904
Loss in iteration 84 : 0.47007588931447297
Loss in iteration 85 : 0.47098774484809175
Loss in iteration 86 : 0.4718919664867952
Loss in iteration 87 : 0.47253362806637556
Loss in iteration 88 : 0.47323698664859015
Loss in iteration 89 : 0.47407363717555
Loss in iteration 90 : 0.47475371536553523
Loss in iteration 91 : 0.4753263731180835
Loss in iteration 92 : 0.47600519581303224
Loss in iteration 93 : 0.4767253315210007
Loss in iteration 94 : 0.4773592512835782
Loss in iteration 95 : 0.47792579739105456
Loss in iteration 96 : 0.47860315767657213
Loss in iteration 97 : 0.479205975990913
Loss in iteration 98 : 0.479662689851435
Loss in iteration 99 : 0.48027670341232975
Loss in iteration 100 : 0.4808805840139146
Loss in iteration 101 : 0.4813744647163513
Loss in iteration 102 : 0.4819250082281521
Loss in iteration 103 : 0.48243969200202463
Loss in iteration 104 : 0.48285807516123425
Loss in iteration 105 : 0.48334724519184247
Loss in iteration 106 : 0.483805187233749
Loss in iteration 107 : 0.484226879240918
Loss in iteration 108 : 0.4846589740955818
Loss in iteration 109 : 0.48506515638261094
Loss in iteration 110 : 0.4854616116933976
Loss in iteration 111 : 0.48586197603065906
Loss in iteration 112 : 0.48625105263891777
Loss in iteration 113 : 0.4866230180205272
Loss in iteration 114 : 0.4870036305857991
Loss in iteration 115 : 0.48737335877720067
Loss in iteration 116 : 0.4877373418513836
Loss in iteration 117 : 0.48810201505756134
Loss in iteration 118 : 0.4884586966393419
Loss in iteration 119 : 0.48881043957568077
Loss in iteration 120 : 0.4891604603398239
Loss in iteration 121 : 0.4895038731903774
Loss in iteration 122 : 0.4898402361079758
Loss in iteration 123 : 0.4901699165857468
Loss in iteration 124 : 0.4904858082319666
Loss in iteration 125 : 0.49079306752274865
Loss in iteration 126 : 0.49109727508679196
Loss in iteration 127 : 0.49139288241182777
Loss in iteration 128 : 0.4916857962365887
Loss in iteration 129 : 0.49196873289067217
Loss in iteration 130 : 0.49224765533529957
Loss in iteration 131 : 0.4925244123770418
Loss in iteration 132 : 0.4928005311932604
Loss in iteration 133 : 0.4930751913352843
Loss in iteration 134 : 0.49334461708503685
Loss in iteration 135 : 0.49360926082567647
Loss in iteration 136 : 0.49386884365624334
Testing accuracy  of updater 9 on alg 1 with rate 0.010976 = 0.7875, training accuracy 0.842667529944966, time elapsed: 1894 millisecond.
Loss in iteration 1 : 1.00000429507411
Loss in iteration 2 : 0.5582737711294437
Loss in iteration 3 : 0.6256585833347273
Loss in iteration 4 : 0.702468730019711
Loss in iteration 5 : 0.7240382295649187
Loss in iteration 6 : 0.6960015569657195
Loss in iteration 7 : 0.6247746812846071
Loss in iteration 8 : 0.5214090738571466
Loss in iteration 9 : 0.42861211998852927
Loss in iteration 10 : 0.4120326739111952
Loss in iteration 11 : 0.4688572309511799
Loss in iteration 12 : 0.5034018499415233
Loss in iteration 13 : 0.4826402068996284
Loss in iteration 14 : 0.4306083197408535
Loss in iteration 15 : 0.3876717327645267
Loss in iteration 16 : 0.37646810796116736
Loss in iteration 17 : 0.38889640558075034
Loss in iteration 18 : 0.4074693041365783
Loss in iteration 19 : 0.41888956828087026
Loss in iteration 20 : 0.4179140989294016
Loss in iteration 21 : 0.4072977743412099
Loss in iteration 22 : 0.39337628665385166
Loss in iteration 23 : 0.3832822449453161
Loss in iteration 24 : 0.38060050527262296
Loss in iteration 25 : 0.3841604845729422
Loss in iteration 26 : 0.39114876777903035
Loss in iteration 27 : 0.3969755827665661
Loss in iteration 28 : 0.39909288445986724
Loss in iteration 29 : 0.3973136934996275
Loss in iteration 30 : 0.39282406894404975
Loss in iteration 31 : 0.3879158277217021
Loss in iteration 32 : 0.3851067970045824
Loss in iteration 33 : 0.3849466410909473
Loss in iteration 34 : 0.38667630981925144
Loss in iteration 35 : 0.3889802256891153
Loss in iteration 36 : 0.390797520360432
Loss in iteration 37 : 0.391202065672317
Loss in iteration 38 : 0.3901527934429609
Loss in iteration 39 : 0.3881927545456187
Loss in iteration 40 : 0.3863289750516518
Loss in iteration 41 : 0.3850131534798211
Loss in iteration 42 : 0.3845921608226727
Loss in iteration 43 : 0.3850010962811624
Loss in iteration 44 : 0.3857805233999747
Loss in iteration 45 : 0.3863519150092185
Loss in iteration 46 : 0.3862187371002227
Loss in iteration 47 : 0.38538309331208565
Loss in iteration 48 : 0.38429975288472795
Loss in iteration 49 : 0.38343080046853584
Loss in iteration 50 : 0.3830577179635299
Loss in iteration 51 : 0.38308759253314156
Loss in iteration 52 : 0.3833209705424345
Loss in iteration 53 : 0.3835193190574892
Loss in iteration 54 : 0.3834781828142964
Loss in iteration 55 : 0.38312952099371783
Loss in iteration 56 : 0.3825934142931248
Loss in iteration 57 : 0.38207677385847216
Loss in iteration 58 : 0.3818586568227532
Loss in iteration 59 : 0.3819568044657155
Loss in iteration 60 : 0.3821802889160919
Loss in iteration 61 : 0.38233620468467927
Loss in iteration 62 : 0.38227074449092996
Loss in iteration 63 : 0.38203616325169765
Loss in iteration 64 : 0.38180323426373386
Loss in iteration 65 : 0.3817853942784673
Loss in iteration 66 : 0.38190915599872166
Loss in iteration 67 : 0.3821286068379896
Loss in iteration 68 : 0.3823188238846226
Loss in iteration 69 : 0.38239580468174866
Loss in iteration 70 : 0.3824022144812381
Loss in iteration 71 : 0.38245046543083194
Loss in iteration 72 : 0.3825817150051397
Loss in iteration 73 : 0.3828163151115661
Loss in iteration 74 : 0.38306138367597004
Loss in iteration 75 : 0.3832760208391955
Loss in iteration 76 : 0.3834478431419368
Loss in iteration 77 : 0.38358260271886774
Loss in iteration 78 : 0.38374331957520164
Loss in iteration 79 : 0.38394950871403044
Loss in iteration 80 : 0.38420331022629783
Loss in iteration 81 : 0.38446879588017313
Loss in iteration 82 : 0.3847199655623276
Loss in iteration 83 : 0.38494600807121876
Loss in iteration 84 : 0.3851598378132302
Loss in iteration 85 : 0.3853943152559949
Loss in iteration 86 : 0.3856629938597587
Loss in iteration 87 : 0.3859380522375051
Loss in iteration 88 : 0.3862160200054583
Loss in iteration 89 : 0.3864807125481239
Loss in iteration 90 : 0.386736337155641
Loss in iteration 91 : 0.3869962010628038
Loss in iteration 92 : 0.3872684605135542
Loss in iteration 93 : 0.3875492961530243
Loss in iteration 94 : 0.3878364496371343
Loss in iteration 95 : 0.388117259854388
Loss in iteration 96 : 0.3883908259596643
Loss in iteration 97 : 0.38866429509803424
Loss in iteration 98 : 0.3889463387625879
Loss in iteration 99 : 0.3892355565439531
Loss in iteration 100 : 0.38952518853848395
Loss in iteration 101 : 0.38981373817919357
Loss in iteration 102 : 0.39010115906902537
Loss in iteration 103 : 0.3903880581994649
Loss in iteration 104 : 0.3906767613205522
Loss in iteration 105 : 0.3909664952575215
Loss in iteration 106 : 0.39126236547989807
Loss in iteration 107 : 0.3915582835594078
Loss in iteration 108 : 0.3918517374894175
Loss in iteration 109 : 0.3921431473677864
Loss in iteration 110 : 0.392437978060888
Loss in iteration 111 : 0.39273675156565496
Loss in iteration 112 : 0.39303663320999144
Loss in iteration 113 : 0.3933364145280453
Loss in iteration 114 : 0.39363479207631
Loss in iteration 115 : 0.3939324921139045
Loss in iteration 116 : 0.39423339468643215
Loss in iteration 117 : 0.39453633474027827
Loss in iteration 118 : 0.3948397097533053
Loss in iteration 119 : 0.39514251705676384
Loss in iteration 120 : 0.3954445376839931
Loss in iteration 121 : 0.39574705634790713
Loss in iteration 122 : 0.3960497930404393
Loss in iteration 123 : 0.39635329774791145
Loss in iteration 124 : 0.3966556790252476
Loss in iteration 125 : 0.3969547304728531
Loss in iteration 126 : 0.3972531671241607
Loss in iteration 127 : 0.397554109249505
Loss in iteration 128 : 0.39785590771021057
Loss in iteration 129 : 0.39815772176755176
Loss in iteration 130 : 0.39845895973290846
Loss in iteration 131 : 0.39876010374655857
Loss in iteration 132 : 0.39906112052414633
Loss in iteration 133 : 0.3993612534629773
Loss in iteration 134 : 0.399660991041576
Loss in iteration 135 : 0.3999605228973407
Loss in iteration 136 : 0.40025988865802337
Loss in iteration 137 : 0.400559242918867
Loss in iteration 138 : 0.40085892973546944
Loss in iteration 139 : 0.40115891950512017
Loss in iteration 140 : 0.40145943529234257
Loss in iteration 141 : 0.40176001030031055
Loss in iteration 142 : 0.4020611061490072
Loss in iteration 143 : 0.4023626209882191
Loss in iteration 144 : 0.40266361013511565
Loss in iteration 145 : 0.40296429866927286
Loss in iteration 146 : 0.4032656347512578
Loss in iteration 147 : 0.40356773758021136
Loss in iteration 148 : 0.4038697181042396
Loss in iteration 149 : 0.4041713854176415
Loss in iteration 150 : 0.4044733095307153
Loss in iteration 151 : 0.4047753368287952
Loss in iteration 152 : 0.40507711879464936
Loss in iteration 153 : 0.4053789354063825
Loss in iteration 154 : 0.4056803139011361
Loss in iteration 155 : 0.4059808272394283
Loss in iteration 156 : 0.40628060724832243
Loss in iteration 157 : 0.40658121349277354
Loss in iteration 158 : 0.40688155172989204
Loss in iteration 159 : 0.40718134016591734
Loss in iteration 160 : 0.40748094525353173
Loss in iteration 161 : 0.4077801416044017
Loss in iteration 162 : 0.40807868884516096
Loss in iteration 163 : 0.408376889100069
Loss in iteration 164 : 0.40867451547773603
Loss in iteration 165 : 0.4089711998303286
Loss in iteration 166 : 0.40926741045157083
Loss in iteration 167 : 0.4095630584862973
Loss in iteration 168 : 0.4098582833965863
Loss in iteration 169 : 0.41015284207806846
Loss in iteration 170 : 0.41044718670475777
Loss in iteration 171 : 0.4107411046440578
Loss in iteration 172 : 0.4110347199508927
Loss in iteration 173 : 0.4113281446567968
Loss in iteration 174 : 0.4116214799129624
Loss in iteration 175 : 0.4119144258582213
Loss in iteration 176 : 0.4122065843420424
Loss in iteration 177 : 0.4124978956304777
Loss in iteration 178 : 0.4127885020962129
Loss in iteration 179 : 0.413078161227292
Loss in iteration 180 : 0.4133677218522788
Loss in iteration 181 : 0.41365676272576885
Loss in iteration 182 : 0.41394534753686985
Loss in iteration 183 : 0.414233356405613
Loss in iteration 184 : 0.4145212827210724
Loss in iteration 185 : 0.41480923749629484
Loss in iteration 186 : 0.4150972150652981
Loss in iteration 187 : 0.41538396408190276
Loss in iteration 188 : 0.41566986570091113
Loss in iteration 189 : 0.41595454403142007
Loss in iteration 190 : 0.41623736668979044
Loss in iteration 191 : 0.4165190224608687
Loss in iteration 192 : 0.416799498961468
Loss in iteration 193 : 0.41707889377349766
Loss in iteration 194 : 0.41735737366891446
Loss in iteration 195 : 0.41763471840242244
Loss in iteration 196 : 0.4179109692884447
Loss in iteration 197 : 0.4181866421836501
Loss in iteration 198 : 0.41846236155885913
Loss in iteration 199 : 0.4187379983664331
Loss in iteration 200 : 0.41901285396379884
Testing accuracy  of updater 9 on alg 1 with rate 0.002744 = 0.7875, training accuracy 0.8410488831337002, time elapsed: 2668 millisecond.
Loss in iteration 1 : 1.0000021583700454
Loss in iteration 2 : 0.635404713147822
Loss in iteration 3 : 0.5766569198802262
Loss in iteration 4 : 0.6618998927910416
Loss in iteration 5 : 0.710873250938262
Loss in iteration 6 : 0.7234823100581426
Loss in iteration 7 : 0.7035657893483458
Loss in iteration 8 : 0.6550823688017684
Loss in iteration 9 : 0.5825205625057457
Loss in iteration 10 : 0.49806452891817166
Loss in iteration 11 : 0.43132330122465856
Loss in iteration 12 : 0.4113108253987922
Loss in iteration 13 : 0.4452312031851216
Loss in iteration 14 : 0.4826615431029031
Loss in iteration 15 : 0.4900339512502142
Loss in iteration 16 : 0.4658365987538018
Loss in iteration 17 : 0.4262693469444913
Loss in iteration 18 : 0.39295712022665713
Loss in iteration 19 : 0.3785261542083274
Loss in iteration 20 : 0.3815406832739481
Loss in iteration 21 : 0.39330905339012767
Loss in iteration 22 : 0.4048752158429077
Loss in iteration 23 : 0.41063328220579876
Loss in iteration 24 : 0.40886156288097886
Loss in iteration 25 : 0.4012969388610361
Loss in iteration 26 : 0.39136510087635573
Loss in iteration 27 : 0.382524175719565
Loss in iteration 28 : 0.37749432028366453
Loss in iteration 29 : 0.3767172750862334
Loss in iteration 30 : 0.3795407439274079
Loss in iteration 31 : 0.38390166408185644
Loss in iteration 32 : 0.38746906260141134
Loss in iteration 33 : 0.38913571233466954
Loss in iteration 34 : 0.38845580002035085
Loss in iteration 35 : 0.3861274352392343
Loss in iteration 36 : 0.38299328559356066
Loss in iteration 37 : 0.38006642054552514
Loss in iteration 38 : 0.37854853906714053
Loss in iteration 39 : 0.3784873339195124
Loss in iteration 40 : 0.3793977309023284
Loss in iteration 41 : 0.38075154991792987
Loss in iteration 42 : 0.38190374724828957
Loss in iteration 43 : 0.3825203971413788
Loss in iteration 44 : 0.38242302616897145
Loss in iteration 45 : 0.3817452324967738
Loss in iteration 46 : 0.3807235186603381
Loss in iteration 47 : 0.3796825808498739
Loss in iteration 48 : 0.3789350481383552
Loss in iteration 49 : 0.3785553415222632
Loss in iteration 50 : 0.3786166358905402
Loss in iteration 51 : 0.3790272215530884
Loss in iteration 52 : 0.37949992677542543
Loss in iteration 53 : 0.3797655971572435
Loss in iteration 54 : 0.3797394515835081
Loss in iteration 55 : 0.3794361364480641
Loss in iteration 56 : 0.3789598047025364
Loss in iteration 57 : 0.37851384091019613
Loss in iteration 58 : 0.37820238461036193
Loss in iteration 59 : 0.3780839756671014
Loss in iteration 60 : 0.3781894434268553
Loss in iteration 61 : 0.3783324928495405
Loss in iteration 62 : 0.37846151880533363
Loss in iteration 63 : 0.37849565180073685
Loss in iteration 64 : 0.3784188991442559
Loss in iteration 65 : 0.3782582667384708
Loss in iteration 66 : 0.3780805902167275
Loss in iteration 67 : 0.3779430158024296
Loss in iteration 68 : 0.37790197968574313
Loss in iteration 69 : 0.3779474625481645
Loss in iteration 70 : 0.37804308622608085
Loss in iteration 71 : 0.3781288049398089
Loss in iteration 72 : 0.37816999674716173
Loss in iteration 73 : 0.37817258095054646
Loss in iteration 74 : 0.3781440969252322
Loss in iteration 75 : 0.3781000082464782
Loss in iteration 76 : 0.3780885246627667
Loss in iteration 77 : 0.37812666271155515
Loss in iteration 78 : 0.378192670905257
Loss in iteration 79 : 0.3782809733554295
Loss in iteration 80 : 0.37836720982166094
Loss in iteration 81 : 0.37843078725460033
Loss in iteration 82 : 0.3784774828877074
Loss in iteration 83 : 0.3785232648354087
Loss in iteration 84 : 0.37857276437367615
Loss in iteration 85 : 0.37863196517491715
Loss in iteration 86 : 0.37871721680198756
Loss in iteration 87 : 0.3788173118920614
Loss in iteration 88 : 0.37891874677403603
Loss in iteration 89 : 0.3790145692137492
Loss in iteration 90 : 0.3791023910340628
Loss in iteration 91 : 0.37918491250035724
Loss in iteration 92 : 0.37927060179774924
Loss in iteration 93 : 0.3793593280182569
Loss in iteration 94 : 0.37945371172249
Loss in iteration 95 : 0.37955581056551674
Loss in iteration 96 : 0.37966600101932296
Loss in iteration 97 : 0.37977786240892336
Loss in iteration 98 : 0.3798877395511502
Loss in iteration 99 : 0.3799947606446002
Loss in iteration 100 : 0.38010067920410956
Loss in iteration 101 : 0.38020728333156295
Loss in iteration 102 : 0.3803187220182716
Loss in iteration 103 : 0.3804341743529032
Loss in iteration 104 : 0.38055068590893054
Loss in iteration 105 : 0.38066957810373686
Loss in iteration 106 : 0.38078922214928496
Loss in iteration 107 : 0.38090935911393586
Loss in iteration 108 : 0.381030034396548
Loss in iteration 109 : 0.3811512888086805
Loss in iteration 110 : 0.38127315901556597
Loss in iteration 111 : 0.3813956779351548
Loss in iteration 112 : 0.3815189831278693
Loss in iteration 113 : 0.38164374571243087
Loss in iteration 114 : 0.38177054696558144
Loss in iteration 115 : 0.38189941184957366
Loss in iteration 116 : 0.38202911942776446
Loss in iteration 117 : 0.382160084305989
Loss in iteration 118 : 0.3822920568657435
Loss in iteration 119 : 0.38242478199596575
Loss in iteration 120 : 0.3825585518921869
Loss in iteration 121 : 0.3826934537640381
Loss in iteration 122 : 0.38282969258480337
Loss in iteration 123 : 0.38296672708930274
Loss in iteration 124 : 0.3831041358894318
Loss in iteration 125 : 0.38324196594119253
Loss in iteration 126 : 0.38338025956397626
Loss in iteration 127 : 0.38351928613358066
Loss in iteration 128 : 0.38365975637310495
Loss in iteration 129 : 0.3838006456864439
Loss in iteration 130 : 0.38394166190035683
Loss in iteration 131 : 0.38408287167593713
Loss in iteration 132 : 0.38422433514173343
Loss in iteration 133 : 0.3843661065151586
Loss in iteration 134 : 0.38450861225273597
Loss in iteration 135 : 0.3846523292234784
Loss in iteration 136 : 0.38479772001197465
Loss in iteration 137 : 0.38494311498024103
Loss in iteration 138 : 0.38508888420489407
Loss in iteration 139 : 0.3852348791929681
Loss in iteration 140 : 0.38538174386454094
Loss in iteration 141 : 0.3855296681194777
Loss in iteration 142 : 0.38567858342815947
Loss in iteration 143 : 0.38582792638037444
Loss in iteration 144 : 0.38597779458762926
Loss in iteration 145 : 0.3861279435246218
Loss in iteration 146 : 0.3862787237308184
Loss in iteration 147 : 0.38642978704151565
Loss in iteration 148 : 0.386581164376455
Loss in iteration 149 : 0.38673300345472544
Loss in iteration 150 : 0.38688534310404094
Loss in iteration 151 : 0.3870381594312449
Loss in iteration 152 : 0.3871913370634403
Loss in iteration 153 : 0.3873449390469168
Loss in iteration 154 : 0.3874991063830726
Loss in iteration 155 : 0.38765379829245294
Loss in iteration 156 : 0.387809006389748
Loss in iteration 157 : 0.387964750574207
Loss in iteration 158 : 0.38812104874477604
Loss in iteration 159 : 0.3882778092080962
Loss in iteration 160 : 0.38843525890904307
Loss in iteration 161 : 0.3885931544295523
Loss in iteration 162 : 0.3887515363561367
Loss in iteration 163 : 0.3889102804922871
Loss in iteration 164 : 0.3890693333628299
Loss in iteration 165 : 0.38922871614038185
Loss in iteration 166 : 0.3893881456289024
Loss in iteration 167 : 0.3895478394629426
Loss in iteration 168 : 0.38970759898076035
Loss in iteration 169 : 0.3898673976814333
Loss in iteration 170 : 0.3900273535748878
Loss in iteration 171 : 0.3901873540125832
Loss in iteration 172 : 0.39034745390580644
Loss in iteration 173 : 0.39050770288727127
Loss in iteration 174 : 0.39066830990862955
Loss in iteration 175 : 0.3908291012085506
Loss in iteration 176 : 0.39098962769331047
Loss in iteration 177 : 0.3911502319590611
Loss in iteration 178 : 0.3913114840183869
Loss in iteration 179 : 0.3914728855925218
Loss in iteration 180 : 0.39163447931656675
Loss in iteration 181 : 0.391796462106955
Loss in iteration 182 : 0.39195853600518726
Loss in iteration 183 : 0.39212064432727106
Loss in iteration 184 : 0.39228297333718226
Loss in iteration 185 : 0.39244539157322866
Loss in iteration 186 : 0.3926078400918682
Loss in iteration 187 : 0.39277050598362634
Loss in iteration 188 : 0.39293332197992303
Loss in iteration 189 : 0.3930962758114388
Loss in iteration 190 : 0.39325950321387076
Loss in iteration 191 : 0.39342296426419
Loss in iteration 192 : 0.39358664723112474
Loss in iteration 193 : 0.3937506002124834
Loss in iteration 194 : 0.39391484831362616
Loss in iteration 195 : 0.3940794380237903
Loss in iteration 196 : 0.39424434657370794
Loss in iteration 197 : 0.39440928253297003
Loss in iteration 198 : 0.39457439389057747
Loss in iteration 199 : 0.39473968534635684
Loss in iteration 200 : 0.3949049902048308
Testing accuracy  of updater 9 on alg 1 with rate 0.0019207999999999999 = 0.78575, training accuracy 0.8397539656846876, time elapsed: 2758 millisecond.
Loss in iteration 1 : 1.000000651401228
Loss in iteration 2 : 0.7841661486295582
Loss in iteration 3 : 0.5607226871377906
Loss in iteration 4 : 0.5750015054412767
Loss in iteration 5 : 0.6315014628309522
Loss in iteration 6 : 0.6694572306960838
Loss in iteration 7 : 0.687198142654376
Loss in iteration 8 : 0.6866095116932824
Loss in iteration 9 : 0.669716296304563
Loss in iteration 10 : 0.6385293778549861
Loss in iteration 11 : 0.5951931504824688
Loss in iteration 12 : 0.5435118229274111
Loss in iteration 13 : 0.49351325691947306
Loss in iteration 14 : 0.4557305820754394
Loss in iteration 15 : 0.43702911256245774
Loss in iteration 16 : 0.43665470863364025
Loss in iteration 17 : 0.450171056346493
Loss in iteration 18 : 0.4657954650022578
Loss in iteration 19 : 0.47130261498475834
Loss in iteration 20 : 0.4634081016494436
Loss in iteration 21 : 0.44565522112269745
Loss in iteration 22 : 0.424022088583179
Loss in iteration 23 : 0.40508665211259953
Loss in iteration 24 : 0.3927150637221071
Loss in iteration 25 : 0.3884872264651156
Loss in iteration 26 : 0.3898810976645887
Loss in iteration 27 : 0.3939036885083381
Loss in iteration 28 : 0.39749169701857745
Loss in iteration 29 : 0.39892073404201167
Loss in iteration 30 : 0.39760164162874545
Loss in iteration 31 : 0.3939888894376638
Loss in iteration 32 : 0.3892645306850232
Loss in iteration 33 : 0.38459944421580367
Loss in iteration 34 : 0.3807992008473666
Loss in iteration 35 : 0.3783450721834848
Loss in iteration 36 : 0.37742280811247986
Loss in iteration 37 : 0.37754443374395713
Loss in iteration 38 : 0.3783791739736287
Loss in iteration 39 : 0.37947799803407944
Loss in iteration 40 : 0.38029394029817726
Loss in iteration 41 : 0.38055877462240745
Loss in iteration 42 : 0.38021650095795556
Loss in iteration 43 : 0.379382568030149
Loss in iteration 44 : 0.37829331321000437
Loss in iteration 45 : 0.37717524215763854
Loss in iteration 46 : 0.37625487392160667
Loss in iteration 47 : 0.37557863892682763
Loss in iteration 48 : 0.37534210805236823
Loss in iteration 49 : 0.3754617338594502
Loss in iteration 50 : 0.37576965535196677
Loss in iteration 51 : 0.37610856143911087
Loss in iteration 52 : 0.3763521723801675
Loss in iteration 53 : 0.3764384653386827
Loss in iteration 54 : 0.37635099010235756
Loss in iteration 55 : 0.3761153076008525
Loss in iteration 56 : 0.3757856482046372
Loss in iteration 57 : 0.37546399402555924
Loss in iteration 58 : 0.3752495750765854
Loss in iteration 59 : 0.3751286528929074
Loss in iteration 60 : 0.375100435646678
Loss in iteration 61 : 0.3751774054481913
Loss in iteration 62 : 0.37528606299881273
Loss in iteration 63 : 0.3753764777546862
Loss in iteration 64 : 0.3754388982901209
Loss in iteration 65 : 0.37545555713492335
Loss in iteration 66 : 0.3754263698178684
Loss in iteration 67 : 0.3753636174878603
Loss in iteration 68 : 0.3752866830274083
Loss in iteration 69 : 0.3752168345545199
Loss in iteration 70 : 0.37517296489394836
Loss in iteration 71 : 0.37515444363202527
Loss in iteration 72 : 0.3751645972734677
Loss in iteration 73 : 0.3751952129341574
Loss in iteration 74 : 0.37523708395553823
Loss in iteration 75 : 0.3752744308318671
Loss in iteration 76 : 0.37530154259367343
Loss in iteration 77 : 0.37531744233757924
Loss in iteration 78 : 0.37532258856982087
Loss in iteration 79 : 0.3753220375275014
Loss in iteration 80 : 0.3753168004827596
Loss in iteration 81 : 0.3753091172405039
Loss in iteration 82 : 0.3753080103641469
Loss in iteration 83 : 0.3753224547113325
Loss in iteration 84 : 0.37534170390698485
Loss in iteration 85 : 0.375363872903725
Loss in iteration 86 : 0.37538724434427334
Loss in iteration 87 : 0.37541161246847066
Loss in iteration 88 : 0.3754356822196011
Loss in iteration 89 : 0.3754591191192351
Loss in iteration 90 : 0.3754791616248359
Loss in iteration 91 : 0.3754967361214034
Loss in iteration 92 : 0.375514183957928
Loss in iteration 93 : 0.37553076073155917
Loss in iteration 94 : 0.37554757767091174
Loss in iteration 95 : 0.3755661399067181
Loss in iteration 96 : 0.37558715817600985
Loss in iteration 97 : 0.37560899836601136
Loss in iteration 98 : 0.3756324175555347
Loss in iteration 99 : 0.3756569767332945
Loss in iteration 100 : 0.37568302195716613
Loss in iteration 101 : 0.37570969169771906
Loss in iteration 102 : 0.37573621463308027
Loss in iteration 103 : 0.37576218665708433
Loss in iteration 104 : 0.3757878496698329
Loss in iteration 105 : 0.3758139510013235
Loss in iteration 106 : 0.37584022626697106
Loss in iteration 107 : 0.37586662573906376
Loss in iteration 108 : 0.3758937268358052
Loss in iteration 109 : 0.3759216426450288
Loss in iteration 110 : 0.37595039098809335
Loss in iteration 111 : 0.3759794793486812
Loss in iteration 112 : 0.3760088683280137
Loss in iteration 113 : 0.37603846393563944
Loss in iteration 114 : 0.3760682748879004
Loss in iteration 115 : 0.37609830905020575
Loss in iteration 116 : 0.3761285735157518
Loss in iteration 117 : 0.37615907467709353
Loss in iteration 118 : 0.3761899844414561
Loss in iteration 119 : 0.3762214229562353
Loss in iteration 120 : 0.376253171603488
Loss in iteration 121 : 0.3762853506903739
Loss in iteration 122 : 0.3763178416209823
Loss in iteration 123 : 0.3763506165571255
Loss in iteration 124 : 0.3763836764397607
Loss in iteration 125 : 0.3764170221008702
Loss in iteration 126 : 0.37645065427195856
Loss in iteration 127 : 0.37648457359195875
Loss in iteration 128 : 0.37651878061458155
Loss in iteration 129 : 0.3765532758151391
Loss in iteration 130 : 0.37658805959686714
Loss in iteration 131 : 0.37662313229678357
Loss in iteration 132 : 0.3766585652456489
Loss in iteration 133 : 0.37669433497307964
Loss in iteration 134 : 0.3767303456672212
Loss in iteration 135 : 0.3767666014243413
Loss in iteration 136 : 0.3768031059301941
Loss in iteration 137 : 0.37683997955837456
Loss in iteration 138 : 0.3768774549867857
Loss in iteration 139 : 0.37691523649354475
Loss in iteration 140 : 0.3769533219129829
Loss in iteration 141 : 0.37699175745509356
Loss in iteration 142 : 0.3770305514819493
Loss in iteration 143 : 0.37706969764912635
Loss in iteration 144 : 0.37710904386709615
Loss in iteration 145 : 0.3771488122318892
Loss in iteration 146 : 0.37718885861049106
Loss in iteration 147 : 0.37722915739478124
Loss in iteration 148 : 0.37726971063856024
Loss in iteration 149 : 0.37731052018803046
Loss in iteration 150 : 0.3773516181775647
Loss in iteration 151 : 0.37739305453284144
Loss in iteration 152 : 0.3774347592724663
Loss in iteration 153 : 0.37747673298782736
Loss in iteration 154 : 0.3775189761903175
Loss in iteration 155 : 0.37756152030704376
Loss in iteration 156 : 0.37760439170946936
Loss in iteration 157 : 0.37764743637584536
Loss in iteration 158 : 0.3776906664192677
Loss in iteration 159 : 0.37773421563923415
Loss in iteration 160 : 0.3777779984371271
Loss in iteration 161 : 0.3778220178367208
Loss in iteration 162 : 0.3778663253759343
Loss in iteration 163 : 0.37791098053266203
Loss in iteration 164 : 0.37795585205907184
Loss in iteration 165 : 0.3780010153963715
Loss in iteration 166 : 0.378046332182712
Loss in iteration 167 : 0.37809181250278795
Loss in iteration 168 : 0.3781374654459711
Loss in iteration 169 : 0.3781832992014416
Loss in iteration 170 : 0.3782293211442971
Loss in iteration 171 : 0.37827553791348184
Loss in iteration 172 : 0.37832200806418864
Loss in iteration 173 : 0.3783689126348818
Loss in iteration 174 : 0.37841615026778874
Loss in iteration 175 : 0.37846362321215027
Loss in iteration 176 : 0.3785113657207228
Loss in iteration 177 : 0.37855943040376266
Loss in iteration 178 : 0.37860758238099096
Loss in iteration 179 : 0.37865611158300744
Loss in iteration 180 : 0.3787050050078297
Loss in iteration 181 : 0.37875437039217474
Loss in iteration 182 : 0.37880393929562084
Loss in iteration 183 : 0.3788535155974036
Loss in iteration 184 : 0.37890312304335705
Loss in iteration 185 : 0.37895294910320093
Loss in iteration 186 : 0.3790035382179104
Loss in iteration 187 : 0.37905431757992925
Loss in iteration 188 : 0.3791052364424037
Loss in iteration 189 : 0.3791563051426354
Loss in iteration 190 : 0.3792075329858227
Loss in iteration 191 : 0.3792590445089066
Loss in iteration 192 : 0.37931088735055374
Loss in iteration 193 : 0.3793629786669785
Loss in iteration 194 : 0.37941530963283987
Loss in iteration 195 : 0.3794678721817284
Loss in iteration 196 : 0.3795206886443122
Loss in iteration 197 : 0.3795736830580992
Loss in iteration 198 : 0.37962686114996314
Loss in iteration 199 : 0.3796802616052922
Loss in iteration 200 : 0.37973387360229605
Testing accuracy  of updater 9 on alg 1 with rate 0.0010976 = 0.7865, training accuracy 0.840401424409194, time elapsed: 3404 millisecond.
Loss in iteration 1 : 1.000000036336629
Loss in iteration 2 : 0.9503098610121972
Testing accuracy  of updater 9 on alg 1 with rate 2.7439999999999973E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 71 millisecond.
