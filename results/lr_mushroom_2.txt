objc[1145]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x108d434c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x108dc74e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 08:33:40 INFO SparkContext: Running Spark version 2.0.0
18/02/27 08:33:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 08:33:41 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 08:33:41 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 08:33:41 INFO SecurityManager: Changing view acls groups to: 
18/02/27 08:33:41 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 08:33:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 08:33:42 INFO Utils: Successfully started service 'sparkDriver' on port 50098.
18/02/27 08:33:42 INFO SparkEnv: Registering MapOutputTracker
18/02/27 08:33:42 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 08:33:42 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-84a8af5b-412d-4280-9280-13571f3d5aa7
18/02/27 08:33:42 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 08:33:42 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 08:33:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 08:33:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 08:33:43 INFO Executor: Starting executor ID driver on host localhost
18/02/27 08:33:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50099.
18/02/27 08:33:43 INFO NettyBlockTransferService: Server created on 192.168.2.140:50099
18/02/27 08:33:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50099)
18/02/27 08:33:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50099 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50099)
18/02/27 08:33:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50099)
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 8.686303150300049
Loss in iteration 3 : 23.112006856733757
Loss in iteration 4 : 154.09275435908629
Loss in iteration 5 : 4.995469249856768
Loss in iteration 6 : 3.088989532576581
Loss in iteration 7 : 2.3459691316000666
Loss in iteration 8 : 1.99605675734415
Loss in iteration 9 : 1.757953566430986
Loss in iteration 10 : 1.5504011381462448
Loss in iteration 11 : 1.3558253969904797
Loss in iteration 12 : 1.172688350311662
Loss in iteration 13 : 1.003480313231335
Loss in iteration 14 : 0.8469503962402908
Loss in iteration 15 : 0.7029869314948983
Loss in iteration 16 : 0.5993835551216864
Loss in iteration 17 : 0.5293889764151126
Loss in iteration 18 : 0.4729462032901965
Loss in iteration 19 : 0.42984623545745093
Loss in iteration 20 : 0.3968048471091929
Loss in iteration 21 : 0.3705342270591253
Loss in iteration 22 : 0.3482864048609649
Loss in iteration 23 : 0.32889602557302444
Loss in iteration 24 : 0.3112608910225166
Loss in iteration 25 : 0.29484164486604514
Loss in iteration 26 : 0.2777942723208297
Loss in iteration 27 : 0.2620652917772782
Loss in iteration 28 : 0.24483077114395904
Loss in iteration 29 : 0.22957398356605863
Loss in iteration 30 : 0.21477807392745452
Loss in iteration 31 : 0.20235499901299767
Loss in iteration 32 : 0.19094098845480545
Loss in iteration 33 : 0.1804974129962347
Loss in iteration 34 : 0.17029720045230215
Loss in iteration 35 : 0.1610934061060119
Loss in iteration 36 : 0.15254430398558236
Loss in iteration 37 : 0.14486708716656743
Loss in iteration 38 : 0.1376432866320908
Loss in iteration 39 : 0.13099189372388723
Loss in iteration 40 : 0.12470717387588971
Loss in iteration 41 : 0.11883882019246092
Loss in iteration 42 : 0.11335693781537243
Loss in iteration 43 : 0.1082911421804909
Loss in iteration 44 : 0.10364691129152158
Loss in iteration 45 : 0.09938664979360312
Loss in iteration 46 : 0.09546398522385928
Loss in iteration 47 : 0.09183279035773019
Loss in iteration 48 : 0.0884630829043554
Loss in iteration 49 : 0.08533086933948802
Loss in iteration 50 : 0.08240702000278431
Loss in iteration 51 : 0.07965825650928601
Loss in iteration 52 : 0.07705786289871004
Loss in iteration 53 : 0.07459029610870423
Loss in iteration 54 : 0.07224944764659162
Loss in iteration 55 : 0.07003114811491731
Loss in iteration 56 : 0.06792630685209651
Loss in iteration 57 : 0.0659221146017405
Loss in iteration 58 : 0.06400593232859521
Loss in iteration 59 : 0.06216581814263614
Loss in iteration 60 : 0.06039012374058476
Loss in iteration 61 : 0.05866842601990415
Loss in iteration 62 : 0.0569929196064398
Loss in iteration 63 : 0.05535936413850321
Loss in iteration 64 : 0.05376755591090295
Loss in iteration 65 : 0.05222119831524248
Loss in iteration 66 : 0.05072643459985572
Loss in iteration 67 : 0.049288614183151676
Loss in iteration 68 : 0.047908933027047074
Loss in iteration 69 : 0.0465837164418731
Loss in iteration 70 : 0.0453064935504024
Loss in iteration 71 : 0.04407045709567058
Loss in iteration 72 : 0.04286974989227098
Loss in iteration 73 : 0.041699695745255325
Loss in iteration 74 : 0.04055657795531648
Loss in iteration 75 : 0.039437356828786574
Loss in iteration 76 : 0.03833946640430672
Loss in iteration 77 : 0.03726069871332121
Loss in iteration 78 : 0.03619914155092119
Loss in iteration 79 : 0.03515313726112306
Loss in iteration 80 : 0.03412124486508555
Loss in iteration 81 : 0.033102200972540956
Loss in iteration 82 : 0.03209488201730305
Loss in iteration 83 : 0.031098272177172728
Loss in iteration 84 : 0.03011144001221012
Loss in iteration 85 : 0.029133524443183727
Loss in iteration 86 : 0.0281637286589324
Loss in iteration 87 : 0.02720131956839202
Loss in iteration 88 : 0.02624563048750848
Loss in iteration 89 : 0.025296065486418547
Loss in iteration 90 : 0.024352104801688965
Loss in iteration 91 : 0.023413311695012307
Loss in iteration 92 : 0.022479342075408228
Loss in iteration 93 : 0.021549959234437848
Loss in iteration 94 : 0.020625057476538353
Loss in iteration 95 : 0.01970470081191672
Loss in iteration 96 : 0.0187891872662379
Loss in iteration 97 : 0.01787915775590471
Loss in iteration 98 : 0.016975784261562234
Loss in iteration 99 : 0.016081098439879118
Loss in iteration 100 : 0.015198550917543964
Loss in iteration 101 : 0.014333864821030835
Loss in iteration 102 : 0.013495981786534928
Loss in iteration 103 : 0.012697167587151956
Loss in iteration 104 : 0.01195070424934671
Loss in iteration 105 : 0.011266166175309204
Loss in iteration 106 : 0.010645791870077083
Loss in iteration 107 : 0.010084983456692622
Loss in iteration 108 : 0.009575472326611162
Loss in iteration 109 : 0.009108331805559857
Loss in iteration 110 : 0.008675740593513446
Loss in iteration 111 : 0.008271676699951788
Loss in iteration 112 : 0.007891978991222137
Loss in iteration 113 : 0.0075340801699458355
Loss in iteration 114 : 0.00719657197678431
Loss in iteration 115 : 0.006878706947679172
Loss in iteration 116 : 0.006579941214860401
Loss in iteration 117 : 0.006299613719757072
Loss in iteration 118 : 0.006036803261917402
Loss in iteration 119 : 0.005790333844659291
Loss in iteration 120 : 0.005558860319293696
Testing accuracy  of updater 0 on alg 0 with rate 100.0 = 0.9786666666666667, training accuracy 0.9991426121749071, time elapsed: 8121 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 6.082956869845993
Loss in iteration 3 : 16.092478354011533
Loss in iteration 4 : 107.77290014200904
Loss in iteration 5 : 3.4762185101367913
Loss in iteration 6 : 2.1591911486674085
Loss in iteration 7 : 1.6399476428093467
Loss in iteration 8 : 1.3965527375402438
Loss in iteration 9 : 1.2307984022824883
Loss in iteration 10 : 1.0856418053098322
Loss in iteration 11 : 0.9493089785332137
Loss in iteration 12 : 0.8209254850718309
Loss in iteration 13 : 0.7024724424189441
Loss in iteration 14 : 0.5927043065591585
Loss in iteration 15 : 0.4928037036084912
Loss in iteration 16 : 0.4204189489677842
Loss in iteration 17 : 0.3710090753863241
Loss in iteration 18 : 0.33204929264871275
Loss in iteration 19 : 0.30223319574505053
Loss in iteration 20 : 0.2793161769730101
Loss in iteration 21 : 0.26094901423911543
Loss in iteration 22 : 0.24524137369862095
Loss in iteration 23 : 0.23130142668796347
Loss in iteration 24 : 0.21840948224918763
Loss in iteration 25 : 0.2061481231514911
Loss in iteration 26 : 0.19431783965800603
Loss in iteration 27 : 0.18282405931174617
Loss in iteration 28 : 0.17165361868860995
Loss in iteration 29 : 0.1609510940845584
Loss in iteration 30 : 0.1510874459744689
Loss in iteration 31 : 0.14225099928627086
Loss in iteration 32 : 0.1342409093015638
Loss in iteration 33 : 0.12682801267816327
Loss in iteration 34 : 0.11991475690340896
Loss in iteration 35 : 0.11349991422168712
Loss in iteration 36 : 0.1075957923948733
Loss in iteration 37 : 0.10217296646023608
Loss in iteration 38 : 0.09716783524721076
Loss in iteration 39 : 0.09251835250344033
Loss in iteration 40 : 0.08818336271783908
Loss in iteration 41 : 0.08414303983875959
Loss in iteration 42 : 0.08038978245613147
Loss in iteration 43 : 0.07691600894644743
Loss in iteration 44 : 0.07370647841421303
Loss in iteration 45 : 0.0707389279970783
Loss in iteration 46 : 0.06798918333398737
Loss in iteration 47 : 0.0654352003411598
Loss in iteration 48 : 0.06305762561009064
Loss in iteration 49 : 0.06083806179223811
Loss in iteration 50 : 0.058758117908473406
Loss in iteration 51 : 0.056800627777774226
Loss in iteration 52 : 0.05495151054730269
Loss in iteration 53 : 0.05320043709588383
Loss in iteration 54 : 0.05153984111634493
Loss in iteration 55 : 0.049963072730217405
Loss in iteration 56 : 0.048463050842668845
Loss in iteration 57 : 0.04703206940667591
Loss in iteration 58 : 0.04566229467422194
Loss in iteration 59 : 0.044346340851045454
Loss in iteration 60 : 0.04307774530719228
Loss in iteration 61 : 0.04185134864048428
Loss in iteration 62 : 0.04066354691995948
Loss in iteration 63 : 0.03951234851388778
Loss in iteration 64 : 0.03839716241859208
Loss in iteration 65 : 0.03731827413322635
Loss in iteration 66 : 0.03627608333943297
Loss in iteration 67 : 0.03527036073480641
Loss in iteration 68 : 0.0342998448213048
Loss in iteration 69 : 0.03336230546005617
Loss in iteration 70 : 0.03245491516374812
Loss in iteration 71 : 0.03157466459705434
Loss in iteration 72 : 0.030718658143496745
Loss in iteration 73 : 0.02988426233100379
Loss in iteration 74 : 0.029069149399079796
Loss in iteration 75 : 0.02827128588358209
Loss in iteration 76 : 0.02748889992134296
Loss in iteration 77 : 0.026720444496530514
Loss in iteration 78 : 0.025964563595581743
Loss in iteration 79 : 0.02522006329030851
Loss in iteration 80 : 0.024485887833808437
Loss in iteration 81 : 0.023761100267025555
Loss in iteration 82 : 0.023044866934282045
Loss in iteration 83 : 0.0223364453440863
Loss in iteration 84 : 0.0216351748809097
Loss in iteration 85 : 0.020940469957997244
Loss in iteration 86 : 0.02025181530600644
Loss in iteration 87 : 0.01956876322309746
Loss in iteration 88 : 0.018890932773733996
Loss in iteration 89 : 0.018218011124938083
Loss in iteration 90 : 0.017549757470613915
Loss in iteration 91 : 0.01688601035719918
Loss in iteration 92 : 0.016226699757397312
Loss in iteration 93 : 0.015571866051988523
Loss in iteration 94 : 0.014921689316670142
Loss in iteration 95 : 0.014276534096008755
Loss in iteration 96 : 0.013637017085108194
Loss in iteration 97 : 0.0130041069802477
Loss in iteration 98 : 0.012379264445584591
Loss in iteration 99 : 0.011764619254866169
Loss in iteration 100 : 0.011163150513559476
Loss in iteration 101 : 0.01057877666470581
Loss in iteration 102 : 0.010016195890214071
Loss in iteration 103 : 0.009480329315695308
Loss in iteration 104 : 0.008975415907221822
Loss in iteration 105 : 0.008504120402189237
Loss in iteration 106 : 0.00806711175038669
Loss in iteration 107 : 0.007663266494498404
Loss in iteration 108 : 0.007290265489060864
Loss in iteration 109 : 0.00694524487983645
Loss in iteration 110 : 0.006625292635652165
Loss in iteration 111 : 0.006327738612970757
Loss in iteration 112 : 0.006050272072099931
Loss in iteration 113 : 0.005790945338166785
Loss in iteration 114 : 0.0055481177647578915
Loss in iteration 115 : 0.005320380338046517
Loss in iteration 116 : 0.00510648626624834
Loss in iteration 117 : 0.004905299643530259
Loss in iteration 118 : 0.00471576430775378
Loss in iteration 119 : 0.00453688913039265
Loss in iteration 120 : 0.004367743880463383
Loss in iteration 121 : 0.004207460267937486
Testing accuracy  of updater 0 on alg 0 with rate 70.0 = 0.9795555555555555, training accuracy 0.9991426121749071, time elapsed: 4747 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 3.481498150701763
Loss in iteration 3 : 9.189309489281806
Loss in iteration 4 : 61.24538186375453
Loss in iteration 5 : 1.925974691359362
Loss in iteration 6 : 1.2291466630251022
Loss in iteration 7 : 0.9351990627098509
Loss in iteration 8 : 0.7989078346893953
Loss in iteration 9 : 0.7055278853338597
Loss in iteration 10 : 0.6225838690251739
Loss in iteration 11 : 0.5444422566823263
Loss in iteration 12 : 0.4708068733731148
Loss in iteration 13 : 0.4027541773640248
Loss in iteration 14 : 0.3398859547594745
Loss in iteration 15 : 0.2841017210108155
Loss in iteration 16 : 0.24274668394114662
Loss in iteration 17 : 0.213992923135475
Loss in iteration 18 : 0.19231761598755714
Loss in iteration 19 : 0.17569830102081652
Loss in iteration 20 : 0.1626900488476025
Loss in iteration 21 : 0.15203404461817835
Loss in iteration 22 : 0.14281987283162673
Loss in iteration 23 : 0.13453664711562008
Loss in iteration 24 : 0.12686921049797628
Loss in iteration 25 : 0.11962494073731623
Loss in iteration 26 : 0.11269835596736684
Loss in iteration 27 : 0.10604801435694237
Loss in iteration 28 : 0.09969079980000274
Loss in iteration 29 : 0.09369749940501722
Loss in iteration 30 : 0.08815064993657608
Loss in iteration 31 : 0.08307722233511136
Loss in iteration 32 : 0.07844064961000241
Loss in iteration 33 : 0.07418625051729395
Loss in iteration 34 : 0.07027024981128278
Loss in iteration 35 : 0.06666159368046366
Loss in iteration 36 : 0.06333549184667792
Loss in iteration 37 : 0.060268989118817874
Loss in iteration 38 : 0.057439850570781874
Loss in iteration 39 : 0.05482704085213441
Loss in iteration 40 : 0.052411209937525016
Loss in iteration 41 : 0.050174677735716704
Loss in iteration 42 : 0.048101155735922344
Loss in iteration 43 : 0.04617553601331863
Loss in iteration 44 : 0.044383850241398626
Loss in iteration 45 : 0.04271331176975936
Loss in iteration 46 : 0.041152323280140186
Loss in iteration 47 : 0.03969040669200845
Loss in iteration 48 : 0.0383180921695548
Loss in iteration 49 : 0.03702682639468036
Loss in iteration 50 : 0.035808925556223226
Loss in iteration 51 : 0.03465755016375339
Loss in iteration 52 : 0.03356666042869402
Loss in iteration 53 : 0.032530930107326175
Loss in iteration 54 : 0.03154563005394607
Loss in iteration 55 : 0.030606513404497344
Loss in iteration 56 : 0.02970973147735863
Loss in iteration 57 : 0.028851790630376167
Loss in iteration 58 : 0.028029540716526285
Loss in iteration 59 : 0.027240175888125853
Loss in iteration 60 : 0.026481229358203826
Loss in iteration 61 : 0.025750550838364042
Loss in iteration 62 : 0.025046264045858368
Loss in iteration 63 : 0.02436670913408482
Loss in iteration 64 : 0.02371037974346496
Loss in iteration 65 : 0.023075865764996838
Loss in iteration 66 : 0.022461810711561423
Loss in iteration 67 : 0.02186688782845158
Loss in iteration 68 : 0.021289793789967263
Loss in iteration 69 : 0.020729255064007867
Loss in iteration 70 : 0.02018404074387937
Loss in iteration 71 : 0.01965297649033756
Loss in iteration 72 : 0.01913495612369005
Loss in iteration 73 : 0.018628949336928616
Loss in iteration 74 : 0.0181340054074911
Loss in iteration 75 : 0.017649253556451545
Loss in iteration 76 : 0.017173900876916254
Loss in iteration 77 : 0.016707228728345947
Loss in iteration 78 : 0.016248588335209972
Loss in iteration 79 : 0.01579739614007453
Loss in iteration 80 : 0.015353129294326164
Loss in iteration 81 : 0.014915321542595637
Loss in iteration 82 : 0.01448355967085483
Loss in iteration 83 : 0.014057480637508423
Loss in iteration 84 : 0.01363676948445053
Loss in iteration 85 : 0.013221158124535788
Loss in iteration 86 : 0.012810425117820151
Loss in iteration 87 : 0.012404396576340376
Loss in iteration 88 : 0.01200294837041557
Loss in iteration 89 : 0.011606009840110882
Loss in iteration 90 : 0.011213569230045533
Loss in iteration 91 : 0.010825681042176849
Loss in iteration 92 : 0.010442475405670462
Loss in iteration 93 : 0.010064169347364312
Loss in iteration 94 : 0.009691079450492912
Loss in iteration 95 : 0.009323634754149861
Loss in iteration 96 : 0.008962387846879721
Loss in iteration 97 : 0.00860802101746542
Loss in iteration 98 : 0.008261343301282385
Loss in iteration 99 : 0.00792327381195562
Loss in iteration 100 : 0.007594807590783687
Loss in iteration 101 : 0.007276962964038363
Loss in iteration 102 : 0.006970714037628544
Loss in iteration 103 : 0.006676917255854373
Loss in iteration 104 : 0.006396244608101305
Loss in iteration 105 : 0.006129135878611749
Loss in iteration 106 : 0.005875777810916783
Loss in iteration 107 : 0.00563611107094965
Loss in iteration 108 : 0.0054098595262571745
Loss in iteration 109 : 0.005196572920325714
Loss in iteration 110 : 0.004995673920901552
Loss in iteration 111 : 0.004806502656828294
Loss in iteration 112 : 0.004628354721774857
Loss in iteration 113 : 0.004460511128927567
Loss in iteration 114 : 0.004302260396725549
Loss in iteration 115 : 0.004152913840484046
Loss in iteration 116 : 0.004011815451761498
Loss in iteration 117 : 0.0038783477069656534
Loss in iteration 118 : 0.0037519344457636245
Loss in iteration 119 : 0.003632041715572548
Loss in iteration 120 : 0.0035181772512590157
Loss in iteration 121 : 0.003409889072664876
Loss in iteration 122 : 0.003306763539959516
Loss in iteration 123 : 0.0032084231023661383
Loss in iteration 124 : 0.003114523901209385
Loss in iteration 125 : 0.003024753335616864
Testing accuracy  of updater 0 on alg 0 with rate 40.0 = 0.9813333333333333, training accuracy 0.9994284081166047, time elapsed: 4412 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.8982803999400666
Loss in iteration 3 : 2.6397664659256392
Loss in iteration 4 : 14.296968541226018
Loss in iteration 5 : 0.3761418477570714
Loss in iteration 6 : 0.28637675564248677
Loss in iteration 7 : 0.24138929855851085
Loss in iteration 8 : 0.21575303605153015
Loss in iteration 9 : 0.19241135504897794
Loss in iteration 10 : 0.17083126707562674
Loss in iteration 11 : 0.1508581329666001
Loss in iteration 12 : 0.1325822279211284
Loss in iteration 13 : 0.11628487031856097
Loss in iteration 14 : 0.10227179438113235
Loss in iteration 15 : 0.09068742074392488
Loss in iteration 16 : 0.08139732432145988
Loss in iteration 17 : 0.07402195244955276
Loss in iteration 18 : 0.06810128485381255
Loss in iteration 19 : 0.063234277122745
Loss in iteration 20 : 0.05912510060927892
Loss in iteration 21 : 0.05557218769162032
Loss in iteration 22 : 0.05244193929182017
Loss in iteration 23 : 0.049645478401115405
Loss in iteration 24 : 0.047122329837235764
Loss in iteration 25 : 0.044829927013082395
Loss in iteration 26 : 0.04273706775458136
Loss in iteration 27 : 0.04081985250280197
Loss in iteration 28 : 0.039059156412681866
Loss in iteration 29 : 0.037439057110186684
Loss in iteration 30 : 0.03594586467678787
Loss in iteration 31 : 0.034567530925041605
Loss in iteration 32 : 0.033293293079866185
Loss in iteration 33 : 0.032113457089285626
Loss in iteration 34 : 0.031019259945525502
Loss in iteration 35 : 0.030002774031808926
Loss in iteration 36 : 0.029056832404265116
Loss in iteration 37 : 0.02817496401941901
Loss in iteration 38 : 0.02735133387756726
Loss in iteration 39 : 0.026580686258139377
Loss in iteration 40 : 0.025858290741420216
Loss in iteration 41 : 0.025179891284621156
Loss in iteration 42 : 0.0245416587204907
Loss in iteration 43 : 0.02394014694664338
Loss in iteration 44 : 0.02337225291724563
Loss in iteration 45 : 0.022835180402094122
Loss in iteration 46 : 0.022326407365790314
Loss in iteration 47 : 0.021843656745223202
Loss in iteration 48 : 0.02138487036173102
Loss in iteration 49 : 0.02094818568733529
Loss in iteration 50 : 0.02053191518479242
Loss in iteration 51 : 0.02013452795267933
Loss in iteration 52 : 0.019754633424655388
Loss in iteration 53 : 0.019390966893122825
Loss in iteration 54 : 0.019042376649559975
Loss in iteration 55 : 0.0187078125554423
Loss in iteration 56 : 0.01838631587810799
Loss in iteration 57 : 0.0180770102447484
Loss in iteration 58 : 0.01777909358475839
Loss in iteration 59 : 0.017491830945956275
Loss in iteration 60 : 0.017214548083763728
Loss in iteration 61 : 0.01694662573444886
Loss in iteration 62 : 0.01668749449412827
Loss in iteration 63 : 0.01643663023454411
Loss in iteration 64 : 0.016193549994826112
Loss in iteration 65 : 0.015957808295643448
Loss in iteration 66 : 0.015728993828473346
Loss in iteration 67 : 0.015506726478263512
Loss in iteration 68 : 0.01529065464264206
Loss in iteration 69 : 0.015080452815114505
Loss in iteration 70 : 0.014875819403452489
Loss in iteration 71 : 0.014676474757793882
Loss in iteration 72 : 0.014482159385885696
Loss in iteration 73 : 0.014292632335468087
Loss in iteration 74 : 0.01410766972605613
Loss in iteration 75 : 0.01392706341436587
Loss in iteration 76 : 0.013750619779385931
Loss in iteration 77 : 0.013578158614644094
Loss in iteration 78 : 0.013409512116584074
Loss in iteration 79 : 0.013244523959175494
Loss in iteration 80 : 0.013083048445948986
Loss in iteration 81 : 0.012924949731593085
Loss in iteration 82 : 0.012770101106091187
Loss in iteration 83 : 0.01261838433512114
Loss in iteration 84 : 0.012469689051106746
Loss in iteration 85 : 0.012323912189902775
Loss in iteration 86 : 0.012180957468627825
Loss in iteration 87 : 0.012040734900636696
Loss in iteration 88 : 0.011903160344054803
Loss in iteration 89 : 0.011768155080685506
Loss in iteration 90 : 0.011635645422456046
Loss in iteration 91 : 0.011505562342887332
Loss in iteration 92 : 0.011377841131366999
Loss in iteration 93 : 0.0112524210682726
Loss in iteration 94 : 0.011129245119236601
Loss in iteration 95 : 0.01100825964706906
Loss in iteration 96 : 0.010889414140057539
Loss in iteration 97 : 0.010772660955550626
Loss in iteration 98 : 0.010657955077898329
Loss in iteration 99 : 0.01054525388997396
Loss in iteration 100 : 0.010434516957635866
Loss in iteration 101 : 0.010325705826603328
Loss in iteration 102 : 0.010218783831322287
Loss in iteration 103 : 0.010113715915479472
Loss in iteration 104 : 0.01001046846389062
Loss in iteration 105 : 0.009909009145541041
Loss in iteration 106 : 0.009809306767591904
Loss in iteration 107 : 0.00971133114018788
Loss in iteration 108 : 0.009615052951909944
Loss in iteration 109 : 0.0095204436557123
Loss in iteration 110 : 0.009427475365167972
Loss in iteration 111 : 0.009336120760822809
Loss in iteration 112 : 0.009246353006425397
Loss in iteration 113 : 0.00915814567476332
Loss in iteration 114 : 0.009071472682794065
Loss in iteration 115 : 0.00898630823571617
Loss in iteration 116 : 0.008902626779582753
Loss in iteration 117 : 0.00882040296201785
Loss in iteration 118 : 0.008739611600558148
Loss in iteration 119 : 0.00866022765810886
Loss in iteration 120 : 0.008582226224974277
Loss in iteration 121 : 0.008505582506903078
Loss in iteration 122 : 0.008430271818573145
Loss in iteration 123 : 0.008356269581934367
Loss in iteration 124 : 0.008283551328828265
Loss in iteration 125 : 0.008212092707310574
Loss in iteration 126 : 0.008141869491117677
Loss in iteration 127 : 0.008072857591737568
Loss in iteration 128 : 0.008005033072572724
Loss in iteration 129 : 0.007938372164711494
Loss in iteration 130 : 0.007872851283859623
Loss in iteration 131 : 0.007808447048019596
Loss in iteration 132 : 0.007745136295544508
Loss in iteration 133 : 0.007682896103232671
Loss in iteration 134 : 0.007621703804169099
Loss in iteration 135 : 0.007561537005059317
Loss in iteration 136 : 0.007502373602839449
Loss in iteration 137 : 0.00744419180038311
Loss in iteration 138 : 0.007386970121160086
Loss in iteration 139 : 0.007330687422734557
Loss in iteration 140 : 0.0072753229090197526
Loss in iteration 141 : 0.007220856141233142
Loss in iteration 142 : 0.007167267047520379
Loss in iteration 143 : 0.007114535931237203
Loss in iteration 144 : 0.007062643477897289
Loss in iteration 145 : 0.007011570760809775
Loss in iteration 146 : 0.006961299245443276
Loss in iteration 147 : 0.006911810792564296
Loss in iteration 148 : 0.006863087660206452
Loss in iteration 149 : 0.006815112504534077
Loss in iteration 150 : 0.006767868379667835
Loss in iteration 151 : 0.006721338736544178
Loss in iteration 152 : 0.0066755074208818595
Loss in iteration 153 : 0.006630358670329432
Loss in iteration 154 : 0.0065858771108677615
Loss in iteration 155 : 0.00654204775253976
Loss in iteration 156 : 0.0064988559845781235
Loss in iteration 157 : 0.0064562875699993435
Loss in iteration 158 : 0.006414328639729006
Loss in iteration 159 : 0.006372965686320521
Loss in iteration 160 : 0.006332185557325845
Loss in iteration 161 : 0.006291975448372963
Loss in iteration 162 : 0.0062523228960013185
Loss in iteration 163 : 0.0062132157703025884
Loss in iteration 164 : 0.006174642267410626
Loss in iteration 165 : 0.006136590901880558
Loss in iteration 166 : 0.006099050498993453
Loss in iteration 167 : 0.006062010187020049
Loss in iteration 168 : 0.006025459389472992
Loss in iteration 169 : 0.005989387817374824
Loss in iteration 170 : 0.005953785461565334
Loss in iteration 171 : 0.005918642585069563
Loss in iteration 172 : 0.0058839497155450775
Loss in iteration 173 : 0.005849697637824754
Loss in iteration 174 : 0.005815877386569242
Loss in iteration 175 : 0.005782480239041042
Loss in iteration 176 : 0.005749497708010391
Loss in iteration 177 : 0.005716921534801814
Loss in iteration 178 : 0.005684743682487873
Loss in iteration 179 : 0.005652956329236066
Loss in iteration 180 : 0.005621551861813073
Loss in iteration 181 : 0.0055905228692497986
Loss in iteration 182 : 0.005559862136669161
Loss in iteration 183 : 0.005529562639278072
Loss in iteration 184 : 0.005499617536524375
Loss in iteration 185 : 0.005470020166418239
Loss in iteration 186 : 0.005440764040017505
Loss in iteration 187 : 0.0054118428360754875
Loss in iteration 188 : 0.005383250395849772
Loss in iteration 189 : 0.00535498071806934
Loss in iteration 190 : 0.0053270279540581205
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.9848888888888889, training accuracy 0.9994284081166047, time elapsed: 6108 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6521746330115165
Loss in iteration 3 : 2.0384111931277533
Loss in iteration 4 : 9.526868504889167
Loss in iteration 5 : 0.2495798066649031
Loss in iteration 6 : 0.2072439107287139
Loss in iteration 7 : 0.18349311380158073
Loss in iteration 8 : 0.16500962748955367
Loss in iteration 9 : 0.14839661720693756
Loss in iteration 10 : 0.1333593089333123
Loss in iteration 11 : 0.11978548803111004
Loss in iteration 12 : 0.10767603465377759
Loss in iteration 13 : 0.09707589411800538
Loss in iteration 14 : 0.08799113909415834
Loss in iteration 15 : 0.08033737657119001
Loss in iteration 16 : 0.07394445226094722
Loss in iteration 17 : 0.06859873822671522
Loss in iteration 18 : 0.0640889410601578
Loss in iteration 19 : 0.06023405327662745
Loss in iteration 20 : 0.056891779621301125
Loss in iteration 21 : 0.05395532080370381
Loss in iteration 22 : 0.05134598873975971
Loss in iteration 23 : 0.049005761633961455
Loss in iteration 24 : 0.04689127541479081
Loss in iteration 25 : 0.04496943535757113
Loss in iteration 26 : 0.043214363888076035
Loss in iteration 27 : 0.04160532368047347
Loss in iteration 28 : 0.04012531276987156
Loss in iteration 29 : 0.038760110249005386
Loss in iteration 30 : 0.03749762078120532
Loss in iteration 31 : 0.03632741694640094
Loss in iteration 32 : 0.03524041308122425
Loss in iteration 33 : 0.03422862720834895
Loss in iteration 34 : 0.03328500264343113
Loss in iteration 35 : 0.03240327063090303
Loss in iteration 36 : 0.031577841715148186
Loss in iteration 37 : 0.03080371769424442
Loss in iteration 38 : 0.030076418703882984
Loss in iteration 39 : 0.029391921741918122
Loss in iteration 40 : 0.02874660809645177
Loss in iteration 41 : 0.028137217895820226
Loss in iteration 42 : 0.027560810496614248
Loss in iteration 43 : 0.027014729756697862
Loss in iteration 44 : 0.026496573463075566
Loss in iteration 45 : 0.026004166337562352
Loss in iteration 46 : 0.025535536151125793
Loss in iteration 47 : 0.025088892556258627
Loss in iteration 48 : 0.024662608305872684
Loss in iteration 49 : 0.024255202573388018
Loss in iteration 50 : 0.023865326125990457
Loss in iteration 51 : 0.02349174813403342
Loss in iteration 52 : 0.023133344425920417
Loss in iteration 53 : 0.022789087020586334
Loss in iteration 54 : 0.022458034789604564
Loss in iteration 55 : 0.022139325118466485
Loss in iteration 56 : 0.02183216645204431
Loss in iteration 57 : 0.021535831622926094
Loss in iteration 58 : 0.021249651873400402
Loss in iteration 59 : 0.020973011492544366
Loss in iteration 60 : 0.02070534299928428
Loss in iteration 61 : 0.020446122810584073
Loss in iteration 62 : 0.020194867341200323
Loss in iteration 63 : 0.01995112948783264
Loss in iteration 64 : 0.019714495456097197
Loss in iteration 65 : 0.019484581893655362
Loss in iteration 66 : 0.019261033297117064
Loss in iteration 67 : 0.01904351966409241
Loss in iteration 68 : 0.01883173436504632
Loss in iteration 69 : 0.018625392212485107
Loss in iteration 70 : 0.018424227707519936
Loss in iteration 71 : 0.018227993446059076
Loss in iteration 72 : 0.018036458668818905
Loss in iteration 73 : 0.017849407941047456
Loss in iteration 74 : 0.017666639949354853
Loss in iteration 75 : 0.01748796640437107
Loss in iteration 76 : 0.0173132110391211
Loss in iteration 77 : 0.017142208694046746
Loss in iteration 78 : 0.016974804480523985
Loss in iteration 79 : 0.016810853015545114
Loss in iteration 80 : 0.01665021772096348
Loss in iteration 81 : 0.016492770181350584
Loss in iteration 82 : 0.016338389555097324
Loss in iteration 83 : 0.016186962033912344
Loss in iteration 84 : 0.016038380346337703
Loss in iteration 85 : 0.015892543301322483
Loss in iteration 86 : 0.01574935536827082
Loss in iteration 87 : 0.015608726290321851
Loss in iteration 88 : 0.015470570727923717
Loss in iteration 89 : 0.015334807930040214
Loss in iteration 90 : 0.015201361430576904
Loss in iteration 91 : 0.015070158767838912
Loss in iteration 92 : 0.014941131225035053
Loss in iteration 93 : 0.01481421359002675
Loss in iteration 94 : 0.014689343932686666
Loss in iteration 95 : 0.014566463398381595
Loss in iteration 96 : 0.014445516016230958
Loss in iteration 97 : 0.0143264485209153
Loss in iteration 98 : 0.01420921018692107
Loss in iteration 99 : 0.014093752674209075
Loss in iteration 100 : 0.013980029884386147
Loss in iteration 101 : 0.013867997826542963
Loss in iteration 102 : 0.013757614491995925
Loss in iteration 103 : 0.013648839737240716
Loss in iteration 104 : 0.013541635174486179
Loss in iteration 105 : 0.013435964069194579
Loss in iteration 106 : 0.013331791244105039
Loss in iteration 107 : 0.013229082989263667
Loss in iteration 108 : 0.013127806977626133
Loss in iteration 109 : 0.013027932185836172
Loss in iteration 110 : 0.012929428819819554
Loss in iteration 111 : 0.012832268244862197
Loss in iteration 112 : 0.012736422919872564
Loss in iteration 113 : 0.012641866335551236
Loss in iteration 114 : 0.012548572956216482
Loss in iteration 115 : 0.012456518165053882
Loss in iteration 116 : 0.012365678212578236
Loss in iteration 117 : 0.012276030168112727
Loss in iteration 118 : 0.01218755187410602
Loss in iteration 119 : 0.012100221903121702
Loss in iteration 120 : 0.012014019517347532
Loss in iteration 121 : 0.011928924630482843
Loss in iteration 122 : 0.011844917771872783
Loss in iteration 123 : 0.01176198005276737
Loss in iteration 124 : 0.011680093134591273
Loss in iteration 125 : 0.011599239199117917
Loss in iteration 126 : 0.011519400920447577
Loss in iteration 127 : 0.011440561438695903
Loss in iteration 128 : 0.011362704335303758
Loss in iteration 129 : 0.011285813609884645
Loss in iteration 130 : 0.011209873658529919
Loss in iteration 131 : 0.011134869253496379
Loss in iteration 132 : 0.011060785524203743
Loss in iteration 133 : 0.010987607939473424
Loss in iteration 134 : 0.010915322290942178
Loss in iteration 135 : 0.010843914677588025
Loss in iteration 136 : 0.010773371491306884
Loss in iteration 137 : 0.010703679403482536
Loss in iteration 138 : 0.010634825352492933
Loss in iteration 139 : 0.010566796532099357
Loss in iteration 140 : 0.01049958038066645
Loss in iteration 141 : 0.010433164571162761
Loss in iteration 142 : 0.01036753700189405
Loss in iteration 143 : 0.010302685787922823
Loss in iteration 144 : 0.010238599253129683
Loss in iteration 145 : 0.010175265922874016
Loss in iteration 146 : 0.010112674517212579
Loss in iteration 147 : 0.010050813944637424
Loss in iteration 148 : 0.009989673296295062
Loss in iteration 149 : 0.00992924184065167
Loss in iteration 150 : 0.00986950901856973
Loss in iteration 151 : 0.009810464438764157
Loss in iteration 152 : 0.009752097873606813
Loss in iteration 153 : 0.009694399255250508
Loss in iteration 154 : 0.00963735867204476
Loss in iteration 155 : 0.00958096636521779
Loss in iteration 156 : 0.009525212725799809
Loss in iteration 157 : 0.009470088291765228
Loss in iteration 158 : 0.009415583745372182
Loss in iteration 159 : 0.009361689910679432
Loss in iteration 160 : 0.009308397751222225
Loss in iteration 161 : 0.009255698367829846
Loss in iteration 162 : 0.009203582996568808
Loss in iteration 163 : 0.009152043006797565
Loss in iteration 164 : 0.009101069899318586
Loss in iteration 165 : 0.00905065530461613
Loss in iteration 166 : 0.00900079098116818
Loss in iteration 167 : 0.008951468813822437
Loss in iteration 168 : 0.008902680812227214
Loss in iteration 169 : 0.008854419109309
Loss in iteration 170 : 0.008806675959789113
Loss in iteration 171 : 0.008759443738733155
Loss in iteration 172 : 0.008712714940126987
Loss in iteration 173 : 0.008666482175474645
Loss in iteration 174 : 0.00862073817241313
Loss in iteration 175 : 0.008575475773340746
Loss in iteration 176 : 0.008530687934055151
Loss in iteration 177 : 0.008486367722398809
Loss in iteration 178 : 0.00844250831690891
Loss in iteration 179 : 0.00839910300547042
Loss in iteration 180 : 0.008356145183970305
Loss in iteration 181 : 0.008313628354951768
Loss in iteration 182 : 0.008271546126267817
Loss in iteration 183 : 0.00822989220973328
Loss in iteration 184 : 0.008188660419774998
Loss in iteration 185 : 0.00814784467208007
Loss in iteration 186 : 0.008107438982242015
Loss in iteration 187 : 0.00806743746440521
Loss in iteration 188 : 0.00802783432990775
Loss in iteration 189 : 0.007988623885923278
Loss in iteration 190 : 0.007949800534102285
Loss in iteration 191 : 0.00791135876921346
Loss in iteration 192 : 0.007873293177785717
Loss in iteration 193 : 0.007835598436751946
Loss in iteration 194 : 0.007798269312094624
Loss in iteration 195 : 0.007761300657494792
Loss in iteration 196 : 0.007724687412984686
Loss in iteration 197 : 0.007688424603605029
Loss in iteration 198 : 0.007652507338067858
Loss in iteration 199 : 0.0076169308074254625
Loss in iteration 200 : 0.007581690283746562
Testing accuracy  of updater 0 on alg 0 with rate 7.0 = 0.9786666666666667, training accuracy 0.9988568162332095, time elapsed: 6677 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.4378553670076638
Loss in iteration 3 : 1.3177411918651418
Loss in iteration 4 : 4.5996016857579525
Loss in iteration 5 : 0.18007223761706925
Loss in iteration 6 : 0.15760689740496994
Loss in iteration 7 : 0.14213703958875595
Loss in iteration 8 : 0.13022235053112158
Loss in iteration 9 : 0.12008722467169826
Loss in iteration 10 : 0.11123659698041796
Loss in iteration 11 : 0.10346396110127216
Loss in iteration 12 : 0.09664427961755485
Loss in iteration 13 : 0.09067315171561077
Loss in iteration 14 : 0.08545000416656627
Loss in iteration 15 : 0.080875290743134
Loss in iteration 16 : 0.07685395330887024
Loss in iteration 17 : 0.07329973569506719
Loss in iteration 18 : 0.07013787703069677
Loss in iteration 19 : 0.06730573190850948
Loss in iteration 20 : 0.0647519167768448
Loss in iteration 21 : 0.06243474102573968
Loss in iteration 22 : 0.06032047318002247
Loss in iteration 23 : 0.05838173971553514
Loss in iteration 24 : 0.0565961736870308
Loss in iteration 25 : 0.05494533009443582
Loss in iteration 26 : 0.053413839806612774
Loss in iteration 27 : 0.051988759536652704
Loss in iteration 28 : 0.05065907566148928
Loss in iteration 29 : 0.04941532564088142
Loss in iteration 30 : 0.04824930804702412
Loss in iteration 31 : 0.04715385891400507
Loss in iteration 32 : 0.04612267765030455
Loss in iteration 33 : 0.04515019007667272
Loss in iteration 34 : 0.04423143941559019
Loss in iteration 35 : 0.043361998478840355
Loss in iteration 36 : 0.04253789807529139
Loss in iteration 37 : 0.04175556795642427
Loss in iteration 38 : 0.04101178756051383
Loss in iteration 39 : 0.040303644503850246
Loss in iteration 40 : 0.039628499269719855
Loss in iteration 41 : 0.0389839549144395
Loss in iteration 42 : 0.038367830881624564
Loss in iteration 43 : 0.03777814021770896
Loss in iteration 44 : 0.03721306963266234
Loss in iteration 45 : 0.03667096196360845
Loss in iteration 46 : 0.03615030068554048
Loss in iteration 47 : 0.03564969617970211
Loss in iteration 48 : 0.03516787352163809
Loss in iteration 49 : 0.03470366159118768
Loss in iteration 50 : 0.034255983338553245
Loss in iteration 51 : 0.03382384706605958
Loss in iteration 52 : 0.033406338605810416
Loss in iteration 53 : 0.033002614290276476
Loss in iteration 54 : 0.03261189462673176
Loss in iteration 55 : 0.032233458598027995
Loss in iteration 56 : 0.03186663852192685
Loss in iteration 57 : 0.03151081540946667
Loss in iteration 58 : 0.031165414769892118
Loss in iteration 59 : 0.03082990281575181
Loss in iteration 60 : 0.030503783027023886
Loss in iteration 61 : 0.0301865930377105
Loss in iteration 62 : 0.029877901812344614
Loss in iteration 63 : 0.029577307083373902
Loss in iteration 64 : 0.02928443302348207
Loss in iteration 65 : 0.02899892812965203
Loss in iteration 66 : 0.028720463298201954
Loss in iteration 67 : 0.028448730072182665
Loss in iteration 68 : 0.028183439044441892
Loss in iteration 69 : 0.027924318401372716
Loss in iteration 70 : 0.027671112593886122
Loss in iteration 71 : 0.027423581123511537
Loss in iteration 72 : 0.02718149743274508
Loss in iteration 73 : 0.026944647889856954
Loss in iteration 74 : 0.026712830859342257
Loss in iteration 75 : 0.026485855850075325
Loss in iteration 76 : 0.02626354273401077
Loss in iteration 77 : 0.026045721028976065
Loss in iteration 78 : 0.025832229239732465
Loss in iteration 79 : 0.025622914252046467
Loss in iteration 80 : 0.025417630775022303
Loss in iteration 81 : 0.025216240827403598
Loss in iteration 82 : 0.025018613263963114
Loss in iteration 83 : 0.024824623338467643
Loss in iteration 84 : 0.024634152300040055
Loss in iteration 85 : 0.024447087020036853
Loss in iteration 86 : 0.024263319646832197
Loss in iteration 87 : 0.024082747286139496
Loss in iteration 88 : 0.02390527170472246
Loss in iteration 89 : 0.02373079905554395
Loss in iteration 90 : 0.02355923962257995
Loss in iteration 91 : 0.023390507583685747
Loss in iteration 92 : 0.02322452079004809
Loss in iteration 93 : 0.023061200560886844
Loss in iteration 94 : 0.02290047149218996
Loss in iteration 95 : 0.022742261278371513
Loss in iteration 96 : 0.02258650054584105
Loss in iteration 97 : 0.022433122697559472
Loss in iteration 98 : 0.022282063767737597
Loss in iteration 99 : 0.022133262285904908
Loss in iteration 100 : 0.02198665914964178
Loss in iteration 101 : 0.021842197505329362
Loss in iteration 102 : 0.02169982263632347
Loss in iteration 103 : 0.021559481858009565
Loss in iteration 104 : 0.021421124419239924
Loss in iteration 105 : 0.021284701409695538
Loss in iteration 106 : 0.021150165672751155
Loss in iteration 107 : 0.02101747172345751
Loss in iteration 108 : 0.020886575671283752
Loss in iteration 109 : 0.020757435147292527
Loss in iteration 110 : 0.020630009235445498
Loss in iteration 111 : 0.020504258407760042
Loss in iteration 112 : 0.020380144463060662
Loss in iteration 113 : 0.02025763046908632
Loss in iteration 114 : 0.020136680707735048
Loss in iteration 115 : 0.020017260623242054
Loss in iteration 116 : 0.019899336773103096
Loss in iteration 117 : 0.0197828767815697
Loss in iteration 118 : 0.019667849295553545
Loss in iteration 119 : 0.019554223942791174
Loss in iteration 120 : 0.01944197129212951
Loss in iteration 121 : 0.01933106281580312
Loss in iteration 122 : 0.0192214708535827
Loss in iteration 123 : 0.019113168578684153
Loss in iteration 124 : 0.01900612996533286
Loss in iteration 125 : 0.018900329757887482
Loss in iteration 126 : 0.018795743441432467
Loss in iteration 127 : 0.018692347213755012
Loss in iteration 128 : 0.018590117958628194
Loss in iteration 129 : 0.018489033220326787
Loss in iteration 130 : 0.018389071179306878
Loss in iteration 131 : 0.01829021062898596
Loss in iteration 132 : 0.018192430953562294
Loss in iteration 133 : 0.018095712106818637
Loss in iteration 134 : 0.018000034591856715
Loss in iteration 135 : 0.017905379441713918
Loss in iteration 136 : 0.017811728200815138
Loss in iteration 137 : 0.017719062907217024
Loss in iteration 138 : 0.017627366075603536
Loss in iteration 139 : 0.017536620680994697
Loss in iteration 140 : 0.017446810143132017
Loss in iteration 141 : 0.017357918311507597
Loss in iteration 142 : 0.01726992945100406
Loss in iteration 143 : 0.017182828228116186
Loss in iteration 144 : 0.017096599697725208
Loss in iteration 145 : 0.01701122929039964
Loss in iteration 146 : 0.016926702800197174
Loss in iteration 147 : 0.01684300637294416
Loss in iteration 148 : 0.016760126494969982
Loss in iteration 149 : 0.01667804998227557
Loss in iteration 150 : 0.01659676397011558
Loss in iteration 151 : 0.01651625590297571
Loss in iteration 152 : 0.01643651352492715
Loss in iteration 153 : 0.016357524870341093
Loss in iteration 154 : 0.016279278254947536
Loss in iteration 155 : 0.016201762267223065
Loss in iteration 156 : 0.016124965760093087
Loss in iteration 157 : 0.01604887784293523
Loss in iteration 158 : 0.015973487873870554
Loss in iteration 159 : 0.015898785452330744
Loss in iteration 160 : 0.01582476041188905
Loss in iteration 161 : 0.01575140281334452
Loss in iteration 162 : 0.015678702938048645
Loss in iteration 163 : 0.015606651281464372
Loss in iteration 164 : 0.01553523854694821
Loss in iteration 165 : 0.015464455639746218
Loss in iteration 166 : 0.015394293661195343
Loss in iteration 167 : 0.015324743903121699
Loss in iteration 168 : 0.015255797842428405
Loss in iteration 169 : 0.015187447135865046
Loss in iteration 170 : 0.015119683614971973
Loss in iteration 171 : 0.015052499281192601
Loss in iteration 172 : 0.014985886301147187
Loss in iteration 173 : 0.014919837002062047
Loss in iteration 174 : 0.01485434386734816
Loss in iteration 175 : 0.014789399532323609
Loss in iteration 176 : 0.01472499678007469
Loss in iteration 177 : 0.014661128537450097
Loss in iteration 178 : 0.014597787871183782
Loss in iteration 179 : 0.014534967984141452
Loss in iteration 180 : 0.014472662211686327
Loss in iteration 181 : 0.014410864018159796
Loss in iteration 182 : 0.014349566993472989
Loss in iteration 183 : 0.014288764849805246
Loss in iteration 184 : 0.01422845141840551
Loss in iteration 185 : 0.014168620646493331
Loss in iteration 186 : 0.01410926659425596
Loss in iteration 187 : 0.01405038343193776
Loss in iteration 188 : 0.013991965437019438
Loss in iteration 189 : 0.013934006991483469
Loss in iteration 190 : 0.013876502579162925
Loss in iteration 191 : 0.01381944678317102
Loss in iteration 192 : 0.013762834283408382
Loss in iteration 193 : 0.013706659854145768
Loss in iteration 194 : 0.013650918361679195
Loss in iteration 195 : 0.01359560476205572
Loss in iteration 196 : 0.013540714098867068
Loss in iteration 197 : 0.013486241501108751
Loss in iteration 198 : 0.013432182181103256
Loss in iteration 199 : 0.013378531432484226
Loss in iteration 200 : 0.013325284628240463
Testing accuracy  of updater 0 on alg 0 with rate 4.0 = 0.9777777777777777, training accuracy 0.998428122320663, time elapsed: 5896 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.45859828352491205
Loss in iteration 3 : 0.36856470775773775
Loss in iteration 4 : 0.3163188041385286
Loss in iteration 5 : 0.2739225763498299
Loss in iteration 6 : 0.2469247483641216
Loss in iteration 7 : 0.2275369612957087
Loss in iteration 8 : 0.21301289123602263
Loss in iteration 9 : 0.2011217263498658
Loss in iteration 10 : 0.19103474225540862
Loss in iteration 11 : 0.18231552050529728
Loss in iteration 12 : 0.17468006333275374
Loss in iteration 13 : 0.16792270155413677
Loss in iteration 14 : 0.16188880622580817
Loss in iteration 15 : 0.1564592619195598
Loss in iteration 16 : 0.15154053125365097
Loss in iteration 17 : 0.14705796080665032
Loss in iteration 18 : 0.1429511354769423
Loss in iteration 19 : 0.13917056972347513
Loss in iteration 20 : 0.13567529867282746
Loss in iteration 21 : 0.13243109073792816
Loss in iteration 22 : 0.12940909922501828
Loss in iteration 23 : 0.12658483013995636
Loss in iteration 24 : 0.12393734171460241
Loss in iteration 25 : 0.12144861636716096
Loss in iteration 26 : 0.1191030627505009
Loss in iteration 27 : 0.11688711716137593
Loss in iteration 28 : 0.11478892169552665
Loss in iteration 29 : 0.11279806228872319
Loss in iteration 30 : 0.11090535392637374
Loss in iteration 31 : 0.10910266332578682
Loss in iteration 32 : 0.10738276162569632
Loss in iteration 33 : 0.10573920128262308
Loss in iteration 34 : 0.10416621262922353
Loss in iteration 35 : 0.10265861650561264
Loss in iteration 36 : 0.1012117501087221
Loss in iteration 37 : 0.09982140377315064
Loss in iteration 38 : 0.09848376684044043
Loss in iteration 39 : 0.09719538112223265
Loss in iteration 40 : 0.09595310073846676
Loss in iteration 41 : 0.09475405733133266
Loss in iteration 42 : 0.09359562983149619
Loss in iteration 43 : 0.09247541809476947
Loss in iteration 44 : 0.09139121984207668
Loss in iteration 45 : 0.0903410104289287
Loss in iteration 46 : 0.08932292504697563
Loss in iteration 47 : 0.08833524302293244
Loss in iteration 48 : 0.08737637393195744
Loss in iteration 49 : 0.08644484528545887
Loss in iteration 50 : 0.08553929158900829
Loss in iteration 51 : 0.08465844459585303
Loss in iteration 52 : 0.08380112460651748
Loss in iteration 53 : 0.08296623268600321
Loss in iteration 54 : 0.08215274368785351
Loss in iteration 55 : 0.08135969998937073
Loss in iteration 56 : 0.0805862058550455
Loss in iteration 57 : 0.07983142235613216
Loss in iteration 58 : 0.0790945627836036
Loss in iteration 59 : 0.07837488849967693
Loss in iteration 60 : 0.07767170517995091
Loss in iteration 61 : 0.07698435940408761
Loss in iteration 62 : 0.07631223555806002
Loss in iteration 63 : 0.07565475301539948
Loss in iteration 64 : 0.07501136356869322
Loss in iteration 65 : 0.07438154908591561
Loss in iteration 66 : 0.0737648193690699
Loss in iteration 67 : 0.07316071019515168
Loss in iteration 68 : 0.07256878152165915
Loss in iteration 69 : 0.07198861584082393
Loss in iteration 70 : 0.07141981666843615
Loss in iteration 71 : 0.07086200715464766
Loss in iteration 72 : 0.07031482880546018
Loss in iteration 73 : 0.06977794030477823
Loss in iteration 74 : 0.0692510164279416
Loss in iteration 75 : 0.06873374703857286
Loss in iteration 76 : 0.06822583616139352
Loss in iteration 77 : 0.06772700112438326
Loss in iteration 78 : 0.06723697176430844
Loss in iteration 79 : 0.06675548969022058
Loss in iteration 80 : 0.06628230760004025
Loss in iteration 81 : 0.06581718864580391
Loss in iteration 82 : 0.06535990584356453
Loss in iteration 83 : 0.06491024152430272
Loss in iteration 84 : 0.06446798682254734
Loss in iteration 85 : 0.06403294119969057
Loss in iteration 86 : 0.06360491199926303
Loss in iteration 87 : 0.06318371403167103
Loss in iteration 88 : 0.06276916918611801
Loss in iteration 89 : 0.06236110606763166
Loss in iteration 90 : 0.06195935965729487
Loss in iteration 91 : 0.06156377099394179
Loss in iteration 92 : 0.06117418687572715
Loss in iteration 93 : 0.060790459580109195
Loss in iteration 94 : 0.060412446600907985
Loss in iteration 95 : 0.060040010401210056
Loss in iteration 96 : 0.05967301818099195
Loss in iteration 97 : 0.05931134165842341
Loss in iteration 98 : 0.05895485686389535
Loss in iteration 99 : 0.05860344394589453
Loss in iteration 100 : 0.05825698698791284
Loss in iteration 101 : 0.05791537383564528
Loss in iteration 102 : 0.05757849593378445
Loss in iteration 103 : 0.057246248171777855
Loss in iteration 104 : 0.056918528737955214
Loss in iteration 105 : 0.056595238981484046
Loss in iteration 106 : 0.056276283281647646
Loss in iteration 107 : 0.055961568923979015
Loss in iteration 108 : 0.05565100598281815
Loss in iteration 109 : 0.05534450720989041
Loss in iteration 110 : 0.05504198792853375
Loss in iteration 111 : 0.054743365933229746
Loss in iteration 112 : 0.05444856139411424
Loss in iteration 113 : 0.05415749676617196
Loss in iteration 114 : 0.05387009670283359
Loss in iteration 115 : 0.0535862879737179
Loss in iteration 116 : 0.05330599938627758
Loss in iteration 117 : 0.05302916171112252
Loss in iteration 118 : 0.052755707610811275
Loss in iteration 119 : 0.052485571571914975
Loss in iteration 120 : 0.05221868984016918
Loss in iteration 121 : 0.05195500035854457
Loss in iteration 122 : 0.05169444270807413
Loss in iteration 123 : 0.05143695805128883
Loss in iteration 124 : 0.05118248907812084
Loss in iteration 125 : 0.05093097995414232
Loss in iteration 126 : 0.05068237627101771
Loss in iteration 127 : 0.050436624999053305
Loss in iteration 128 : 0.05019367444173534
Loss in iteration 129 : 0.049953474192155754
Loss in iteration 130 : 0.049715975091229034
Loss in iteration 131 : 0.04948112918761029
Loss in iteration 132 : 0.049248889699230546
Loss in iteration 133 : 0.04901921097636889
Loss in iteration 134 : 0.04879204846618663
Loss in iteration 135 : 0.048567358678653125
Loss in iteration 136 : 0.048345099153796464
Loss in iteration 137 : 0.04812522843021574
Loss in iteration 138 : 0.04790770601479619
Loss in iteration 139 : 0.04769249235357125
Loss in iteration 140 : 0.0474795488036772
Loss in iteration 141 : 0.04726883760635345
Loss in iteration 142 : 0.04706032186093694
Loss in iteration 143 : 0.04685396549981014
Loss in iteration 144 : 0.046649733264256824
Loss in iteration 145 : 0.04644759068118817
Loss in iteration 146 : 0.04624750404069922
Loss in iteration 147 : 0.04604944037442075
Loss in iteration 148 : 0.04585336743463182
Loss in iteration 149 : 0.04565925367410107
Loss in iteration 150 : 0.04546706822662559
Loss in iteration 151 : 0.04527678088823815
Loss in iteration 152 : 0.04508836209905471
Loss in iteration 153 : 0.04490178292573702
Loss in iteration 154 : 0.04471701504454341
Loss in iteration 155 : 0.04453403072494514
Loss in iteration 156 : 0.044352802813785355
Loss in iteration 157 : 0.044173304719958256
Loss in iteration 158 : 0.04399551039958902
Loss in iteration 159 : 0.043819394341693664
Loss in iteration 160 : 0.04364493155430076
Loss in iteration 161 : 0.043472097551017254
Loss in iteration 162 : 0.04330086833802031
Loss in iteration 163 : 0.04313122040146065
Loss in iteration 164 : 0.04296313069525959
Loss in iteration 165 : 0.042796576629287174
Loss in iteration 166 : 0.04263153605790564
Loss in iteration 167 : 0.04246798726886509
Loss in iteration 168 : 0.04230590897253876
Loss in iteration 169 : 0.042145280291485177
Loss in iteration 170 : 0.04198608075032505
Loss in iteration 171 : 0.041828290265922004
Loss in iteration 172 : 0.04167188913785631
Loss in iteration 173 : 0.041516858039180565
Loss in iteration 174 : 0.041363178007447876
Loss in iteration 175 : 0.0412108304360031
Loss in iteration 176 : 0.04105979706552743
Loss in iteration 177 : 0.04091005997582785
Loss in iteration 178 : 0.04076160157786298
Loss in iteration 179 : 0.04061440460599707
Loss in iteration 180 : 0.04046845211047521
Loss in iteration 181 : 0.0403237274501105
Loss in iteration 182 : 0.04018021428517775
Loss in iteration 183 : 0.04003789657050624
Loss in iteration 184 : 0.03989675854876473
Loss in iteration 185 : 0.03975678474393211
Loss in iteration 186 : 0.039617959954948594
Loss in iteration 187 : 0.0394802692495408
Loss in iteration 188 : 0.03934369795821458
Loss in iteration 189 : 0.03920823166841188
Loss in iteration 190 : 0.03907385621882441
Loss in iteration 191 : 0.038940557693860206
Loss in iteration 192 : 0.038808322418258365
Loss in iteration 193 : 0.038677136951846484
Loss in iteration 194 : 0.038546988084437156
Loss in iteration 195 : 0.03841786283085822
Loss in iteration 196 : 0.03828974842611391
Loss in iteration 197 : 0.038162632320671415
Loss in iteration 198 : 0.03803650217587017
Loss in iteration 199 : 0.037911345859449226
Loss in iteration 200 : 0.03778715144119009
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.9742222222222222, training accuracy 0.9958559588453844, time elapsed: 5987 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.8982803999400666
Loss in iteration 3 : 0.2566208671686087
Loss in iteration 4 : 0.5487744360808786
Loss in iteration 5 : 0.2910037485550582
Loss in iteration 6 : 0.5312696737303221
Loss in iteration 7 : 0.42336122887184974
Loss in iteration 8 : 0.27295466319696504
Loss in iteration 9 : 0.26044379283715785
Loss in iteration 10 : 0.2539042965028756
Loss in iteration 11 : 0.22494640252932363
Loss in iteration 12 : 0.15715971526066094
Loss in iteration 13 : 0.10384712269049588
Loss in iteration 14 : 0.09718466542060267
Loss in iteration 15 : 0.10158453458745985
Loss in iteration 16 : 0.09977026459950342
Loss in iteration 17 : 0.08969690911451188
Loss in iteration 18 : 0.08127747177910571
Loss in iteration 19 : 0.0847742251638798
Loss in iteration 20 : 0.08932033329330764
Loss in iteration 21 : 0.0779114166665056
Loss in iteration 22 : 0.06200878884367001
Loss in iteration 23 : 0.053685963111639964
Loss in iteration 24 : 0.0490018384001112
Loss in iteration 25 : 0.044866077495725035
Loss in iteration 26 : 0.04037291668597811
Loss in iteration 27 : 0.03636532773904414
Loss in iteration 28 : 0.03349417917379552
Loss in iteration 29 : 0.03170431998394584
Loss in iteration 30 : 0.030554664095829798
Loss in iteration 31 : 0.02969566486317357
Loss in iteration 32 : 0.028887425481532097
Loss in iteration 33 : 0.027932230466222133
Loss in iteration 34 : 0.026725998813481446
Loss in iteration 35 : 0.025307667426824262
Loss in iteration 36 : 0.023821831135876936
Loss in iteration 37 : 0.022415845517599807
Loss in iteration 38 : 0.021156449637484576
Loss in iteration 39 : 0.02002333180816635
Loss in iteration 40 : 0.018956164135236552
Loss in iteration 41 : 0.017898031814052983
Loss in iteration 42 : 0.016812026885926992
Loss in iteration 43 : 0.01568074311205263
Loss in iteration 44 : 0.014501387158491303
Loss in iteration 45 : 0.013281841440729076
Loss in iteration 46 : 0.012038545190719693
Loss in iteration 47 : 0.010795589715667955
Loss in iteration 48 : 0.009583886615378205
Loss in iteration 49 : 0.008438846322923113
Loss in iteration 50 : 0.007395250827335047
Loss in iteration 51 : 0.006480424876182228
Loss in iteration 52 : 0.005705778352069525
Loss in iteration 53 : 0.005048444478427856
Loss in iteration 54 : 0.0044466141893629175
Loss in iteration 55 : 0.0038485786109570073
Loss in iteration 56 : 0.0032548552460680055
Loss in iteration 57 : 0.0027015662592729727
Loss in iteration 58 : 0.002225740042869015
Loss in iteration 59 : 0.0018477913892515376
Loss in iteration 60 : 0.0015699588577579395
Loss in iteration 61 : 0.001380502759397379
Loss in iteration 62 : 0.001258707859456139
Loss in iteration 63 : 0.0011798508481100935
Loss in iteration 64 : 0.001120118423997928
Loss in iteration 65 : 0.001060995243967728
Loss in iteration 66 : 9.921020542821278E-4
Loss in iteration 67 : 9.115451653335153E-4
Loss in iteration 68 : 8.237540981006565E-4
Loss in iteration 69 : 7.359526240283831E-4
Loss in iteration 70 : 6.548928436135872E-4
Loss in iteration 71 : 5.84954913563249E-4
Loss in iteration 72 : 5.277429137093707E-4
Loss in iteration 73 : 4.826462853823822E-4
Loss in iteration 74 : 4.4773292078498846E-4
Loss in iteration 75 : 4.205612737201394E-4
Loss in iteration 76 : 3.987509191346327E-4
Loss in iteration 77 : 3.8030589882364913E-4
Loss in iteration 78 : 3.637460673210015E-4
Loss in iteration 79 : 3.4811021468207106E-4
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.9893333333333333, training accuracy 1.0, time elapsed: 1874 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6521746330115165
Loss in iteration 3 : 0.22766376375407565
Loss in iteration 4 : 0.3100856549984288
Loss in iteration 5 : 0.2128000599869434
Loss in iteration 6 : 0.2543598861835384
Loss in iteration 7 : 0.2627320687221719
Loss in iteration 8 : 0.19961338139602808
Loss in iteration 9 : 0.16237892015367472
Loss in iteration 10 : 0.13861911216473455
Loss in iteration 11 : 0.12093943577625439
Loss in iteration 12 : 0.10021583528871454
Loss in iteration 13 : 0.07662564205053754
Loss in iteration 14 : 0.06447997259915099
Loss in iteration 15 : 0.06209761798994704
Loss in iteration 16 : 0.06306403131277828
Loss in iteration 17 : 0.06150615964391859
Loss in iteration 18 : 0.05708429887103119
Loss in iteration 19 : 0.052915558246072955
Loss in iteration 20 : 0.050947184031746695
Loss in iteration 21 : 0.04874828698251266
Loss in iteration 22 : 0.04377994832888878
Loss in iteration 23 : 0.03784546963449252
Loss in iteration 24 : 0.03331177381016592
Loss in iteration 25 : 0.030378638216197275
Loss in iteration 26 : 0.028275738020003206
Loss in iteration 27 : 0.026450712435514973
Loss in iteration 28 : 0.02480470277061917
Loss in iteration 29 : 0.023381811585801975
Loss in iteration 30 : 0.022178524951381607
Loss in iteration 31 : 0.021138675833688177
Loss in iteration 32 : 0.020201460525675577
Loss in iteration 33 : 0.01932111132911063
Loss in iteration 34 : 0.018465336342515598
Loss in iteration 35 : 0.017612205062650127
Loss in iteration 36 : 0.016749660418949784
Loss in iteration 37 : 0.015875436273120484
Loss in iteration 38 : 0.014995243379261804
Loss in iteration 39 : 0.01411890137712465
Loss in iteration 40 : 0.013255821605137384
Loss in iteration 41 : 0.012411780178541753
Loss in iteration 42 : 0.011588029512792412
Loss in iteration 43 : 0.0107824049551145
Loss in iteration 44 : 0.009991343839700103
Loss in iteration 45 : 0.009211909659466205
Loss in iteration 46 : 0.008443452226334467
Loss in iteration 47 : 0.0076888904031162586
Loss in iteration 48 : 0.006955644369059665
Loss in iteration 49 : 0.00625607063155718
Loss in iteration 50 : 0.005606876492025545
Loss in iteration 51 : 0.005026270173618265
Loss in iteration 52 : 0.004527257109832977
Loss in iteration 53 : 0.004108718858233545
Loss in iteration 54 : 0.003752829607179703
Loss in iteration 55 : 0.003435156787437255
Loss in iteration 56 : 0.0031387677526597975
Loss in iteration 57 : 0.0028595104446362167
Loss in iteration 58 : 0.00260148193567991
Loss in iteration 59 : 0.002369689839915595
Loss in iteration 60 : 0.0021653917895443067
Loss in iteration 61 : 0.00198542032023857
Loss in iteration 62 : 0.0018241881032356494
Loss in iteration 63 : 0.001676360730093207
Loss in iteration 64 : 0.0015386649234117606
Loss in iteration 65 : 0.001410271852719723
Loss in iteration 66 : 0.001292063974667555
Loss in iteration 67 : 0.0011854899439146568
Loss in iteration 68 : 0.0010916197386604874
Loss in iteration 69 : 0.0010106759021977845
Loss in iteration 70 : 9.42007806035537E-4
Loss in iteration 71 : 8.843270249495807E-4
Loss in iteration 72 : 8.360196923246852E-4
Loss in iteration 73 : 7.95418768643062E-4
Loss in iteration 74 : 7.609897577733135E-4
Loss in iteration 75 : 7.31429140383894E-4
Loss in iteration 76 : 7.056949365243825E-4
Loss in iteration 77 : 6.829928256440811E-4
Loss in iteration 78 : 6.627380393564752E-4
Loss in iteration 79 : 6.445080265118699E-4
Loss in iteration 80 : 6.279959047425734E-4
Loss in iteration 81 : 6.129706139695544E-4
Loss in iteration 82 : 5.992465359754678E-4
Testing accuracy  of updater 1 on alg 0 with rate 7.0 = 0.9884444444444445, training accuracy 1.0, time elapsed: 2488 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.43785536700766375
Loss in iteration 3 : 0.226097494530056
Loss in iteration 4 : 0.16516953497248366
Loss in iteration 5 : 0.1532581554131522
Loss in iteration 6 : 0.1664244648940298
Loss in iteration 7 : 0.1528377962352313
Loss in iteration 8 : 0.13272573421693826
Loss in iteration 9 : 0.11718796797969157
Loss in iteration 10 : 0.10192680889315972
Loss in iteration 11 : 0.08642235710497657
Loss in iteration 12 : 0.07249135290607828
Loss in iteration 13 : 0.061844555779060846
Loss in iteration 14 : 0.05495193571607211
Loss in iteration 15 : 0.050710437388752964
Loss in iteration 16 : 0.04800580994823146
Loss in iteration 17 : 0.0459726811511487
Loss in iteration 18 : 0.0439024704421752
Loss in iteration 19 : 0.04154051066786784
Loss in iteration 20 : 0.03895388176278694
Loss in iteration 21 : 0.0362090098125028
Loss in iteration 22 : 0.03331705779412782
Loss in iteration 23 : 0.030370942901685016
Loss in iteration 24 : 0.02756775681488529
Loss in iteration 25 : 0.025086868523734377
Loss in iteration 26 : 0.023000544431266173
Loss in iteration 27 : 0.021286176846824812
Loss in iteration 28 : 0.019880146436665205
Loss in iteration 29 : 0.018715273874050184
Loss in iteration 30 : 0.01773327932408122
Loss in iteration 31 : 0.016886098371212972
Loss in iteration 32 : 0.016135608287645733
Loss in iteration 33 : 0.015453366687416257
Loss in iteration 34 : 0.014819594566908196
Loss in iteration 35 : 0.014221351511113683
Loss in iteration 36 : 0.013650460857529629
Loss in iteration 37 : 0.013101678903082907
Loss in iteration 38 : 0.012571332752328286
Loss in iteration 39 : 0.012056459244297008
Loss in iteration 40 : 0.011554384355597195
Loss in iteration 41 : 0.011062640250042029
Loss in iteration 42 : 0.010579094878355302
Loss in iteration 43 : 0.010102161410578063
Loss in iteration 44 : 0.009630968684664424
Loss in iteration 45 : 0.00916541382488434
Loss in iteration 46 : 0.00870607695801528
Loss in iteration 47 : 0.008254036247569128
Loss in iteration 48 : 0.007810656898163602
Loss in iteration 49 : 0.007377426896231464
Loss in iteration 50 : 0.006955878026035886
Loss in iteration 51 : 0.006547581630462795
Loss in iteration 52 : 0.006154170109690762
Loss in iteration 53 : 0.005777326994526042
Loss in iteration 54 : 0.00541871343417956
Loss in iteration 55 : 0.0050798402697184875
Loss in iteration 56 : 0.00476192617896087
Loss in iteration 57 : 0.004465786277263106
Loss in iteration 58 : 0.0041917757157615455
Loss in iteration 59 : 0.003939788014363541
Loss in iteration 60 : 0.003709294109365833
Loss in iteration 61 : 0.0034994074061327632
Loss in iteration 62 : 0.003308964690291442
Loss in iteration 63 : 0.003136615444603302
Loss in iteration 64 : 0.002980911648403828
Loss in iteration 65 : 0.002840389056831428
Loss in iteration 66 : 0.002713631765409336
Loss in iteration 67 : 0.0025993150817365802
Loss in iteration 68 : 0.00249622620381304
Loss in iteration 69 : 0.002403266300718233
Loss in iteration 70 : 0.002319440111559209
Loss in iteration 71 : 0.002243839747280555
Loss in iteration 72 : 0.0021756283344169574
Loss in iteration 73 : 0.002114027192343688
Loss in iteration 74 : 0.0020583081245830813
Loss in iteration 75 : 0.002007790666736217
Loss in iteration 76 : 0.001961843026498063
Loss in iteration 77 : 0.001919884992303587
Loss in iteration 78 : 0.001881391143849938
Loss in iteration 79 : 0.0018458930787512843
Loss in iteration 80 : 0.001812979889893333
Loss in iteration 81 : 0.0017822966430660028
Loss in iteration 82 : 0.0017535410207362774
Loss in iteration 83 : 0.001726458570239201
Loss in iteration 84 : 0.0017008371172136621
Loss in iteration 85 : 0.001676500899549781
Loss in iteration 86 : 0.0016533048812602503
Loss in iteration 87 : 0.001631129562599963
Loss in iteration 88 : 0.0016098764518191028
Loss in iteration 89 : 0.0015894642347207655
Loss in iteration 90 : 0.0015698255875994068
Loss in iteration 91 : 0.0015509045316038092
Loss in iteration 92 : 0.001532654216678853
Loss in iteration 93 : 0.0015150350396064475
Loss in iteration 94 : 0.0014980130299931717
Testing accuracy  of updater 1 on alg 0 with rate 4.0 = 0.9884444444444445, training accuracy 0.9998571020291512, time elapsed: 2479 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.45859828352491205
Loss in iteration 3 : 0.27029421044944957
Loss in iteration 4 : 0.1944299894927106
Loss in iteration 5 : 0.1586146713314852
Loss in iteration 6 : 0.13698566980742155
Loss in iteration 7 : 0.12289838066315058
Loss in iteration 8 : 0.11302625270262544
Loss in iteration 9 : 0.10528707288407152
Loss in iteration 10 : 0.09853401406914859
Loss in iteration 11 : 0.09223239841202352
Loss in iteration 12 : 0.08618761689509788
Loss in iteration 13 : 0.0803721606231997
Loss in iteration 14 : 0.07482475702474883
Loss in iteration 15 : 0.06959708373116444
Loss in iteration 16 : 0.06473414868198502
Loss in iteration 17 : 0.06027254085277619
Loss in iteration 18 : 0.05623904278654795
Loss in iteration 19 : 0.05264126653084795
Loss in iteration 20 : 0.04945667377138
Loss in iteration 21 : 0.046633094806205935
Loss in iteration 22 : 0.04410537514517343
Loss in iteration 23 : 0.041817440170124835
Loss in iteration 24 : 0.039733732745428345
Loss in iteration 25 : 0.03783417947902097
Loss in iteration 26 : 0.03610094024378791
Loss in iteration 27 : 0.0345095368036716
Loss in iteration 28 : 0.03303013422098589
Loss in iteration 29 : 0.03163551760672986
Loss in iteration 30 : 0.030308501633690636
Loss in iteration 31 : 0.029043943032812297
Loss in iteration 32 : 0.027845260832479755
Loss in iteration 33 : 0.02671856708553493
Loss in iteration 34 : 0.025667858385345656
Loss in iteration 35 : 0.024693076286354452
Loss in iteration 36 : 0.023790828386107857
Loss in iteration 37 : 0.022956364712293325
Loss in iteration 38 : 0.022185338091562142
Loss in iteration 39 : 0.02147455188976794
Loss in iteration 40 : 0.020821695668241148
Loss in iteration 41 : 0.020224553792567528
Loss in iteration 42 : 0.019680242192498823
Loss in iteration 43 : 0.0191848250835515
Loss in iteration 44 : 0.01873339036827823
Loss in iteration 45 : 0.018320460122010912
Loss in iteration 46 : 0.017940527811233134
Loss in iteration 47 : 0.017588529320284232
Loss in iteration 48 : 0.017260128344513844
Loss in iteration 49 : 0.0169517862092235
Loss in iteration 50 : 0.01666066008988489
Loss in iteration 51 : 0.01638441404990144
Loss in iteration 52 : 0.016121030521631788
Loss in iteration 53 : 0.01586868408700842
Loss in iteration 54 : 0.015625700059727386
Loss in iteration 55 : 0.01539058365656364
Loss in iteration 56 : 0.015162083058969284
Loss in iteration 57 : 0.014939245543812863
Loss in iteration 58 : 0.01472143709612112
Loss in iteration 59 : 0.014508315010900897
Loss in iteration 60 : 0.014299761543765664
Loss in iteration 61 : 0.014095798456614026
Loss in iteration 62 : 0.013896504902157512
Loss in iteration 63 : 0.013701955795773503
Loss in iteration 64 : 0.0135121882838443
Loss in iteration 65 : 0.01332719429004406
Loss in iteration 66 : 0.013146930526810811
Loss in iteration 67 : 0.01297133511564544
Loss in iteration 68 : 0.012800341631025386
Loss in iteration 69 : 0.012633885399404885
Loss in iteration 70 : 0.012471901359056015
Loss in iteration 71 : 0.012314316197299443
Loss in iteration 72 : 0.012161039049467111
Loss in iteration 73 : 0.012011954768183727
Loss in iteration 74 : 0.011866922218248465
Loss in iteration 75 : 0.011725778029714461
Loss in iteration 76 : 0.011588344493214022
Loss in iteration 77 : 0.011454439281808605
Loss in iteration 78 : 0.011323884567062212
Loss in iteration 79 : 0.011196513703327594
Loss in iteration 80 : 0.011072174647281831
Loss in iteration 81 : 0.010950730287794934
Loss in iteration 82 : 0.010832056593465192
Loss in iteration 83 : 0.010716039797250033
Loss in iteration 84 : 0.01060257373922567
Loss in iteration 85 : 0.010491558104464414
Loss in iteration 86 : 0.010382897800320201
Loss in iteration 87 : 0.010276503282114297
Loss in iteration 88 : 0.01017229136859499
Loss in iteration 89 : 0.010070186025023114
Loss in iteration 90 : 0.009970118702136425
Loss in iteration 91 : 0.009872028031144535
Loss in iteration 92 : 0.009775858903983988
Loss in iteration 93 : 0.009681561144517967
Loss in iteration 94 : 0.00958908806103688
Loss in iteration 95 : 0.009498395158616826
Loss in iteration 96 : 0.009409439204865934
Loss in iteration 97 : 0.009322177722785879
Loss in iteration 98 : 0.009236568869592107
Loss in iteration 99 : 0.009152571580156726
Loss in iteration 100 : 0.00907014582220858
Loss in iteration 101 : 0.008989252825343288
Loss in iteration 102 : 0.008909855193069
Loss in iteration 103 : 0.0088319168668294
Loss in iteration 104 : 0.008755402964508803
Loss in iteration 105 : 0.008680279550448013
Loss in iteration 106 : 0.008606513404571314
Loss in iteration 107 : 0.008534071847426328
Loss in iteration 108 : 0.008462922653477221
Loss in iteration 109 : 0.008393034056483716
Loss in iteration 110 : 0.008324374826789608
Loss in iteration 111 : 0.008256914386279221
Loss in iteration 112 : 0.008190622924253996
Loss in iteration 113 : 0.00812547148475196
Loss in iteration 114 : 0.008061432008936798
Loss in iteration 115 : 0.007998477330532383
Loss in iteration 116 : 0.007936581133967687
Loss in iteration 117 : 0.007875717891569278
Loss in iteration 118 : 0.007815862797233347
Loss in iteration 119 : 0.007756991710509206
Loss in iteration 120 : 0.007699081118852564
Loss in iteration 121 : 0.007642108119127639
Loss in iteration 122 : 0.007586050414051491
Loss in iteration 123 : 0.007530886316252692
Loss in iteration 124 : 0.007476594752189996
Loss in iteration 125 : 0.007423155259868715
Loss in iteration 126 : 0.007370547977205341
Loss in iteration 127 : 0.0073187536210168
Loss in iteration 128 : 0.0072677534591172755
Loss in iteration 129 : 0.0072175292793966155
Loss in iteration 130 : 0.0071680633599197924
Loss in iteration 131 : 0.007119338443244468
Loss in iteration 132 : 0.007071337716716026
Loss in iteration 133 : 0.007024044798938708
Loss in iteration 134 : 0.006977443731331147
Loss in iteration 135 : 0.006931518972900983
Loss in iteration 136 : 0.006886255396183568
Loss in iteration 137 : 0.006841638282600392
Loss in iteration 138 : 0.00679765331611967
Loss in iteration 139 : 0.00675428657483022
Loss in iteration 140 : 0.00671152452067868
Loss in iteration 141 : 0.006669353988043528
Loss in iteration 142 : 0.006627762171979407
Loss in iteration 143 : 0.006586736616887067
Loss in iteration 144 : 0.006546265206119838
Loss in iteration 145 : 0.006506336152725
Loss in iteration 146 : 0.006466937991224034
Loss in iteration 147 : 0.006428059570127541
Loss in iteration 148 : 0.006389690044785743
Loss in iteration 149 : 0.006351818870193754
Loss in iteration 150 : 0.006314435793473454
Loss in iteration 151 : 0.0062775308458996835
Loss in iteration 152 : 0.006241094334485851
Loss in iteration 153 : 0.006205116833260837
Loss in iteration 154 : 0.006169589174435183
Loss in iteration 155 : 0.006134502439666343
Loss in iteration 156 : 0.006099847951600768
Loss in iteration 157 : 0.006065617265808623
Loss in iteration 158 : 0.006031802163157433
Loss in iteration 159 : 0.005998394642607689
Loss in iteration 160 : 0.005965386914369814
Loss in iteration 161 : 0.005932771393342378
Loss in iteration 162 : 0.005900540692752999
Loss in iteration 163 : 0.005868687617943338
Loss in iteration 164 : 0.0058372051602659635
Loss in iteration 165 : 0.00580608649108954
Loss in iteration 166 : 0.005775324955930524
Loss in iteration 167 : 0.005744914068742166
Loss in iteration 168 : 0.005714847506394669
Loss in iteration 169 : 0.005685119103373452
Loss in iteration 170 : 0.0056557228467118215
Loss in iteration 171 : 0.005626652871159713
Loss in iteration 172 : 0.005597903454578605
Loss in iteration 173 : 0.005569469013542809
Loss in iteration 174 : 0.005541344099123946
Loss in iteration 175 : 0.005513523392834475
Loss in iteration 176 : 0.00548600170271043
Loss in iteration 177 : 0.005458773959518621
Loss in iteration 178 : 0.005431835213079837
Loss in iteration 179 : 0.005405180628705168
Loss in iteration 180 : 0.0053788054837457105
Loss in iteration 181 : 0.005352705164258007
Loss in iteration 182 : 0.005326875161787601
Loss in iteration 183 : 0.005301311070271097
Loss in iteration 184 : 0.005276008583055407
Loss in iteration 185 : 0.005250963490030347
Loss in iteration 186 : 0.005226171674868774
Loss in iteration 187 : 0.005201629112367516
Loss in iteration 188 : 0.005177331865881785
Loss in iteration 189 : 0.005153276084846522
Loss in iteration 190 : 0.005129458002378618
Loss in iteration 191 : 0.005105873932955693
Loss in iteration 192 : 0.005082520270168058
Loss in iteration 193 : 0.005059393484541428
Loss in iteration 194 : 0.005036490121429165
Loss in iteration 195 : 0.005013806798972305
Loss in iteration 196 : 0.004991340206126651
Loss in iteration 197 : 0.0049690871007550454
Loss in iteration 198 : 0.004947044307783236
Loss in iteration 199 : 0.004925208717417083
Loss in iteration 200 : 0.004903577283418609
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.9884444444444445, training accuracy 0.9997142040583024, time elapsed: 5668 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.5083277402811253
Loss in iteration 3 : 0.3329805655844772
Loss in iteration 4 : 0.23330852633872462
Loss in iteration 5 : 0.18479408797816774
Loss in iteration 6 : 0.15708584980102214
Loss in iteration 7 : 0.1389334371839717
Loss in iteration 8 : 0.12595413796729688
Loss in iteration 9 : 0.1160946282804368
Loss in iteration 10 : 0.10821418832639348
Loss in iteration 11 : 0.10157257412601145
Loss in iteration 12 : 0.0956783865453042
Loss in iteration 13 : 0.0902341454223458
Loss in iteration 14 : 0.08508851296907044
Loss in iteration 15 : 0.08018659986420296
Loss in iteration 16 : 0.07552698408262119
Loss in iteration 17 : 0.071131090742943
Loss in iteration 18 : 0.06702460046322146
Loss in iteration 19 : 0.06322715192329127
Loss in iteration 20 : 0.05974665179856797
Loss in iteration 21 : 0.05657659027157223
Loss in iteration 22 : 0.05369669697677426
Loss in iteration 23 : 0.05107739502861361
Loss in iteration 24 : 0.04868677759131179
Loss in iteration 25 : 0.04649699907150135
Loss in iteration 26 : 0.044487072853879955
Loss in iteration 27 : 0.04264134760029125
Loss in iteration 28 : 0.04094560683677136
Loss in iteration 29 : 0.03938379959868102
Loss in iteration 30 : 0.0379373891347896
Loss in iteration 31 : 0.03658729975273934
Loss in iteration 32 : 0.0353169234039533
Loss in iteration 33 : 0.03411438008823449
Loss in iteration 34 : 0.03297301123042444
Loss in iteration 35 : 0.03189019701189044
Loss in iteration 36 : 0.030865355267691044
Loss in iteration 37 : 0.029898119716501366
Loss in iteration 38 : 0.028987328917942545
Loss in iteration 39 : 0.028130912811346873
Loss in iteration 40 : 0.027326350470395337
Loss in iteration 41 : 0.02657123924756568
Loss in iteration 42 : 0.025863631079486464
Loss in iteration 43 : 0.025202020659177878
Loss in iteration 44 : 0.024585071553221358
Loss in iteration 45 : 0.024011261605862696
Loss in iteration 46 : 0.02347861500359769
Loss in iteration 47 : 0.022984609951857273
Loss in iteration 48 : 0.022526262282669025
Loss in iteration 49 : 0.022100323834072816
Loss in iteration 50 : 0.021703513002900292
Loss in iteration 51 : 0.02133270737814439
Loss in iteration 52 : 0.020985059616788378
Loss in iteration 53 : 0.020658031860331785
Loss in iteration 54 : 0.02034936948931854
Loss in iteration 55 : 0.020057046562253596
Loss in iteration 56 : 0.01977921335288118
Loss in iteration 57 : 0.019514165407354466
Loss in iteration 58 : 0.019260339461724726
Loss in iteration 59 : 0.019016329700299827
Loss in iteration 60 : 0.01878091139910543
Loss in iteration 61 : 0.018553058641443906
Loss in iteration 62 : 0.018331947076884344
Loss in iteration 63 : 0.01811693907678115
Loss in iteration 64 : 0.01790755454909402
Loss in iteration 65 : 0.017703434334262233
Loss in iteration 66 : 0.01750430388621813
Loss in iteration 67 : 0.017309943281918143
Loss in iteration 68 : 0.01712016658836741
Loss in iteration 69 : 0.01693481048578571
Loss in iteration 70 : 0.01675372976235867
Loss in iteration 71 : 0.016576796320925704
Loss in iteration 72 : 0.01640389861795254
Loss in iteration 73 : 0.016234939572824097
Loss in iteration 74 : 0.016069832380638957
Loss in iteration 75 : 0.015908494837346115
Loss in iteration 76 : 0.015750843440829297
Loss in iteration 77 : 0.015596788598122005
Loss in iteration 78 : 0.01544623187298172
Loss in iteration 79 : 0.015299065581105654
Loss in iteration 80 : 0.015155174424750808
Loss in iteration 81 : 0.01501443843450152
Loss in iteration 82 : 0.014876736334862703
Loss in iteration 83 : 0.014741948556278882
Loss in iteration 84 : 0.014609959394171871
Loss in iteration 85 : 0.014480658152481661
Loss in iteration 86 : 0.014353939402005163
Loss in iteration 87 : 0.014229702665845351
Loss in iteration 88 : 0.014107851894692422
Loss in iteration 89 : 0.013988295033138105
Loss in iteration 90 : 0.013870943849796452
Loss in iteration 91 : 0.013755714060655535
Loss in iteration 92 : 0.013642525660054196
Loss in iteration 93 : 0.013531303311937271
Loss in iteration 94 : 0.013421976650064778
Loss in iteration 95 : 0.01331448037738686
Loss in iteration 96 : 0.01320875412020089
Loss in iteration 97 : 0.013104742058945412
Loss in iteration 98 : 0.013002392406567153
Loss in iteration 99 : 0.012901656827931762
Loss in iteration 100 : 0.012802489889388395
Loss in iteration 101 : 0.012704848603039904
Loss in iteration 102 : 0.012608692095903372
Loss in iteration 103 : 0.01251398140053627
Loss in iteration 104 : 0.01242067933888074
Loss in iteration 105 : 0.012328750459061238
Loss in iteration 106 : 0.012238160985602674
Loss in iteration 107 : 0.012148878753808857
Loss in iteration 108 : 0.012060873114039404
Loss in iteration 109 : 0.011974114806479004
Loss in iteration 110 : 0.01188857581800926
Loss in iteration 111 : 0.011804229238088637
Loss in iteration 112 : 0.01172104913009044
Loss in iteration 113 : 0.011639010429753738
Loss in iteration 114 : 0.011558088875428262
Loss in iteration 115 : 0.011478260967855976
Loss in iteration 116 : 0.011399503952039503
Loss in iteration 117 : 0.011321795811206717
Loss in iteration 118 : 0.011245115263053046
Loss in iteration 119 : 0.011169441750725416
Loss in iteration 120 : 0.01109475542442946
Loss in iteration 121 : 0.01102103711306324
Loss in iteration 122 : 0.01094826828808073
Loss in iteration 123 : 0.01087643102338823
Loss in iteration 124 : 0.010805507955388299
Loss in iteration 125 : 0.010735482246535474
Loss in iteration 126 : 0.010666337554378404
Loss in iteration 127 : 0.010598058006509616
Loss in iteration 128 : 0.010530628180528925
Loss in iteration 129 : 0.010464033087310257
Loss in iteration 130 : 0.010398258155624033
Loss in iteration 131 : 0.010333289216441503
Loss in iteration 132 : 0.01026911248585869
Loss in iteration 133 : 0.010205714546314169
Loss in iteration 134 : 0.010143082326440725
Loss in iteration 135 : 0.01008120308034936
Loss in iteration 136 : 0.01002006436733409
Loss in iteration 137 : 0.009959654032920682
Loss in iteration 138 : 0.009899960191928089
Loss in iteration 139 : 0.009840971213862524
Loss in iteration 140 : 0.00978267571061548
Loss in iteration 141 : 0.00972506252616261
Loss in iteration 142 : 0.009668120727801523
Loss in iteration 143 : 0.009611839598432051
Loss in iteration 144 : 0.009556208629449633
Loss in iteration 145 : 0.009501217513955593
Loss in iteration 146 : 0.009446856140140876
Loss in iteration 147 : 0.009393114584837476
Loss in iteration 148 : 0.009339983107325622
Loss in iteration 149 : 0.00928745214352628
Loss in iteration 150 : 0.009235512300699963
Loss in iteration 151 : 0.0091841543527282
Loss in iteration 152 : 0.009133369235990722
Loss in iteration 153 : 0.009083148045789573
Loss in iteration 154 : 0.00903348203322257
Loss in iteration 155 : 0.008984362602383437
Loss in iteration 156 : 0.008935781307764235
Loss in iteration 157 : 0.008887729851754185
Loss in iteration 158 : 0.00884020008216076
Loss in iteration 159 : 0.008793183989714216
Loss in iteration 160 : 0.008746673705549678
Loss in iteration 161 : 0.008700661498684445
Loss in iteration 162 : 0.008655139773520983
Loss in iteration 163 : 0.008610101067407911
Loss in iteration 164 : 0.00856553804828458
Loss in iteration 165 : 0.008521443512422868
Loss in iteration 166 : 0.008477810382267083
Loss in iteration 167 : 0.008434631704361043
Loss in iteration 168 : 0.008391900647344394
Loss in iteration 169 : 0.00834961049999668
Loss in iteration 170 : 0.008307754669309969
Loss in iteration 171 : 0.008266326678574359
Loss in iteration 172 : 0.008225320165467755
Loss in iteration 173 : 0.008184728880147063
Loss in iteration 174 : 0.008144546683343313
Loss in iteration 175 : 0.008104767544466637
Loss in iteration 176 : 0.008065385539728104
Loss in iteration 177 : 0.008026394850284958
Loss in iteration 178 : 0.007987789760414135
Loss in iteration 179 : 0.007949564655715969
Loss in iteration 180 : 0.007911714021348057
Loss in iteration 181 : 0.00787423244028707
Loss in iteration 182 : 0.007837114591615037
Loss in iteration 183 : 0.007800355248826712
Loss in iteration 184 : 0.007763949278154667
Loss in iteration 185 : 0.007727891636910332
Loss in iteration 186 : 0.007692177371839663
Loss in iteration 187 : 0.007656801617494127
Loss in iteration 188 : 0.007621759594617982
Loss in iteration 189 : 0.007587046608553567
Loss in iteration 190 : 0.007552658047666746
Loss in iteration 191 : 0.007518589381794001
Loss in iteration 192 : 0.007484836160712587
Loss in iteration 193 : 0.0074513940126344335
Loss in iteration 194 : 0.007418258642724214
Loss in iteration 195 : 0.007385425831641151
Loss in iteration 196 : 0.007352891434104136
Loss in iteration 197 : 0.007320651377479738
Loss in iteration 198 : 0.007288701660392153
Loss in iteration 199 : 0.007257038351355031
Loss in iteration 200 : 0.007225657587424505
Testing accuracy  of updater 1 on alg 0 with rate 0.7 = 0.9813333333333333, training accuracy 0.9991426121749071, time elapsed: 5080 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.574937865432634
Loss in iteration 3 : 0.43298270672116146
Loss in iteration 4 : 0.3190951634414825
Loss in iteration 5 : 0.2466341806601714
Loss in iteration 6 : 0.2035830879041157
Loss in iteration 7 : 0.17618860033021908
Loss in iteration 8 : 0.15719868292823966
Loss in iteration 9 : 0.14310086708517392
Loss in iteration 10 : 0.13203079185571617
Loss in iteration 11 : 0.12297596377513094
Loss in iteration 12 : 0.11537544378703057
Loss in iteration 13 : 0.10887431620685904
Loss in iteration 14 : 0.10320542269576459
Loss in iteration 15 : 0.0981537934481562
Loss in iteration 16 : 0.09355393355395254
Loss in iteration 17 : 0.08929077181473834
Loss in iteration 18 : 0.08529458591153397
Loss in iteration 19 : 0.0815303303007898
Loss in iteration 20 : 0.07798494339918671
Loss in iteration 21 : 0.07465591802620879
Loss in iteration 22 : 0.07154295898518674
Loss in iteration 23 : 0.06864312941691739
Loss in iteration 24 : 0.06594898359951641
Loss in iteration 25 : 0.06344881145495095
Loss in iteration 26 : 0.061128083991171776
Loss in iteration 27 : 0.0589713011755861
Loss in iteration 28 : 0.056963617935833766
Loss in iteration 29 : 0.05509184982591811
Loss in iteration 30 : 0.05334472913855059
Loss in iteration 31 : 0.0517125442822389
Loss in iteration 32 : 0.050186472596950686
Loss in iteration 33 : 0.04875795479671914
Loss in iteration 34 : 0.04741836251138649
Loss in iteration 35 : 0.046159039239813474
Loss in iteration 36 : 0.04497163028287304
Loss in iteration 37 : 0.043848520929246196
Loss in iteration 38 : 0.04278319564434409
Loss in iteration 39 : 0.041770396013562326
Loss in iteration 40 : 0.04080605079077763
Loss in iteration 41 : 0.03988703519206295
Loss in iteration 42 : 0.039010860821657965
Loss in iteration 43 : 0.03817539583775096
Loss in iteration 44 : 0.0373786790469298
Loss in iteration 45 : 0.03661884292543273
Loss in iteration 46 : 0.03589411975153261
Loss in iteration 47 : 0.03520288460194514
Loss in iteration 48 : 0.03454369074418776
Loss in iteration 49 : 0.033915270223204376
Loss in iteration 50 : 0.033316494710699605
Loss in iteration 51 : 0.0327463096131119
Loss in iteration 52 : 0.03220366288811851
Loss in iteration 53 : 0.03168744868852728
Loss in iteration 54 : 0.03119647800154883
Loss in iteration 55 : 0.030729478358347936
Loss in iteration 56 : 0.03028511632063036
Loss in iteration 57 : 0.029862031981457825
Loss in iteration 58 : 0.029458874459393464
Loss in iteration 59 : 0.029074330250603582
Loss in iteration 60 : 0.028707140631391414
Loss in iteration 61 : 0.02835610843467936
Loss in iteration 62 : 0.02802009733970717
Loss in iteration 63 : 0.02769802788639571
Loss in iteration 64 : 0.027388873941128995
Loss in iteration 65 : 0.027091661875679934
Loss in iteration 66 : 0.026805472972349576
Loss in iteration 67 : 0.026529448124797544
Loss in iteration 68 : 0.02626279311040813
Loss in iteration 69 : 0.026004782635810488
Loss in iteration 70 : 0.025754761849511018
Loss in iteration 71 : 0.025512144799841513
Loss in iteration 72 : 0.025276410103898462
Loss in iteration 73 : 0.02504709466464422
Loss in iteration 74 : 0.02482378652199731
Loss in iteration 75 : 0.024606117856155785
Loss in iteration 76 : 0.02439375886761893
Loss in iteration 77 : 0.024186412866954565
Loss in iteration 78 : 0.02398381254148226
Loss in iteration 79 : 0.02378571711215145
Loss in iteration 80 : 0.023591909988014286
Loss in iteration 81 : 0.023402196556495967
Loss in iteration 82 : 0.023216401871195464
Loss in iteration 83 : 0.023034368158160356
Loss in iteration 84 : 0.022855952204396516
Loss in iteration 85 : 0.02268102278390525
Loss in iteration 86 : 0.02250945830384743
Loss in iteration 87 : 0.022341144823440136
Loss in iteration 88 : 0.022175974531061432
Loss in iteration 89 : 0.02201384468581962
Loss in iteration 90 : 0.021854656960933017
Loss in iteration 91 : 0.021698317082413855
Loss in iteration 92 : 0.02154473464297524
Loss in iteration 93 : 0.021393822984446934
Loss in iteration 94 : 0.02124549907322887
Loss in iteration 95 : 0.021099683331201654
Loss in iteration 96 : 0.020956299418883346
Loss in iteration 97 : 0.020815273991545296
Loss in iteration 98 : 0.020676536459670814
Loss in iteration 99 : 0.020540018783554315
Loss in iteration 100 : 0.020405655321627827
Loss in iteration 101 : 0.020273382738052938
Loss in iteration 102 : 0.020143139961751093
Loss in iteration 103 : 0.020014868179628602
Loss in iteration 104 : 0.019888510842819635
Loss in iteration 105 : 0.019764013666192657
Loss in iteration 106 : 0.019641324606760956
Loss in iteration 107 : 0.019520393814005752
Loss in iteration 108 : 0.019401173552433905
Loss in iteration 109 : 0.019283618102379965
Loss in iteration 110 : 0.019167683648261845
Loss in iteration 111 : 0.01905332816407763
Loss in iteration 112 : 0.018940511304366726
Loss in iteration 113 : 0.01882919430598045
Loss in iteration 114 : 0.018719339902732657
Loss in iteration 115 : 0.01861091225213036
Loss in iteration 116 : 0.0185038768714276
Loss in iteration 117 : 0.018398200579411758
Loss in iteration 118 : 0.018293851440521307
Loss in iteration 119 : 0.01819079870882396
Loss in iteration 120 : 0.01808901277066657
Loss in iteration 121 : 0.01798846508607789
Loss in iteration 122 : 0.017889128129978973
Loss in iteration 123 : 0.017790975334783427
Loss in iteration 124 : 0.017693981036029618
Loss in iteration 125 : 0.01759812042236319
Loss in iteration 126 : 0.017503369490629424
Loss in iteration 127 : 0.017409705006205683
Loss in iteration 128 : 0.017317104468147607
Loss in iteration 129 : 0.017225546078336623
Loss in iteration 130 : 0.017135008713643835
Loss in iteration 131 : 0.01704547190015084
Loss in iteration 132 : 0.016956915788648892
Loss in iteration 133 : 0.016869321130899943
Loss in iteration 134 : 0.0167826692564215
Loss in iteration 135 : 0.016696942049790692
Loss in iteration 136 : 0.016612121928620997
Loss in iteration 137 : 0.016528191822432023
Loss in iteration 138 : 0.016445135152619358
Loss in iteration 139 : 0.016362935813660974
Loss in iteration 140 : 0.01628157815559611
Loss in iteration 141 : 0.01620104696771261
Loss in iteration 142 : 0.01612132746329866
Loss in iteration 143 : 0.016042405265270426
Loss in iteration 144 : 0.015964266392476747
Loss in iteration 145 : 0.0158868972465071
Loss in iteration 146 : 0.015810284598872074
Loss in iteration 147 : 0.015734415578480678
Loss in iteration 148 : 0.015659277659390423
Loss in iteration 149 : 0.015584858648846404
Loss in iteration 150 : 0.015511146675651385
Loss in iteration 151 : 0.015438130178916992
Loss in iteration 152 : 0.015365797897240126
Loss in iteration 153 : 0.015294138858333573
Loss in iteration 154 : 0.015223142369119503
Loss in iteration 155 : 0.015152798006274606
Loss in iteration 156 : 0.015083095607200338
Loss in iteration 157 : 0.015014025261381951
Loss in iteration 158 : 0.014945577302097274
Loss in iteration 159 : 0.014877742298439111
Loss in iteration 160 : 0.01481051104762183
Loss in iteration 161 : 0.01474387456755177
Loss in iteration 162 : 0.014677824089649528
Loss in iteration 163 : 0.014612351051919326
Loss in iteration 164 : 0.014547447092265053
Loss in iteration 165 : 0.014483104042054341
Loss in iteration 166 : 0.014419313919931502
Loss in iteration 167 : 0.014356068925876698
Loss in iteration 168 : 0.014293361435506963
Loss in iteration 169 : 0.014231183994609509
Loss in iteration 170 : 0.0141695293138961
Loss in iteration 171 : 0.014108390263965037
Loss in iteration 172 : 0.014047759870456972
Loss in iteration 173 : 0.013987631309390892
Loss in iteration 174 : 0.01392799790266866
Loss in iteration 175 : 0.013868853113737542
Loss in iteration 176 : 0.013810190543402828
Loss in iteration 177 : 0.013752003925784202
Loss in iteration 178 : 0.0136942871244105
Loss in iteration 179 : 0.013637034128449157
Loss in iteration 180 : 0.01358023904906648
Loss in iteration 181 : 0.013523896115915224
Loss in iteration 182 : 0.013467999673745335
Loss in iteration 183 : 0.013412544179133932
Loss in iteration 184 : 0.013357524197329594
Loss in iteration 185 : 0.01330293439920639
Loss in iteration 186 : 0.013248769558322463
Loss in iteration 187 : 0.013195024548078305
Loss in iteration 188 : 0.013141694338970443
Loss in iteration 189 : 0.013088773995935846
Loss in iteration 190 : 0.01303625867578337
Loss in iteration 191 : 0.012984143624709258
Loss in iteration 192 : 0.012932424175892762
Loss in iteration 193 : 0.01288109574716997
Loss in iteration 194 : 0.01283015383878289
Loss in iteration 195 : 0.01277959403120121
Loss in iteration 196 : 0.012729411983014924
Loss in iteration 197 : 0.012679603428894818
Loss in iteration 198 : 0.012630164177618926
Loss in iteration 199 : 0.012581090110162488
Loss in iteration 200 : 0.012532377177848933
Testing accuracy  of updater 1 on alg 0 with rate 0.4 = 0.9768888888888889, training accuracy 0.998428122320663, time elapsed: 5211 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6603165680413241
Loss in iteration 3 : 0.6053888150136256
Loss in iteration 4 : 0.5406228435382018
Loss in iteration 5 : 0.4753068823098476
Loss in iteration 6 : 0.41493492086818357
Loss in iteration 7 : 0.3624273621125262
Loss in iteration 8 : 0.3189302089101973
Loss in iteration 9 : 0.2841089695944797
Loss in iteration 10 : 0.25666311387666957
Loss in iteration 11 : 0.23498617476105652
Loss in iteration 12 : 0.21761761806468527
Loss in iteration 13 : 0.20341114656005335
Loss in iteration 14 : 0.19153131464051193
Loss in iteration 15 : 0.18138965031308882
Loss in iteration 16 : 0.17257666151912623
Loss in iteration 17 : 0.16480736512662011
Loss in iteration 18 : 0.157881570174775
Loss in iteration 19 : 0.15165585718602204
Loss in iteration 20 : 0.14602418351380528
Loss in iteration 21 : 0.14090482852488975
Loss in iteration 22 : 0.13623198422294794
Loss in iteration 23 : 0.13195064741845494
Loss in iteration 24 : 0.12801371743818082
Loss in iteration 25 : 0.1243804360622528
Loss in iteration 26 : 0.12101554020232547
Loss in iteration 27 : 0.1178887151139068
Loss in iteration 28 : 0.1149741159464536
Loss in iteration 29 : 0.11224985745998961
Loss in iteration 30 : 0.10969745604184977
Loss in iteration 31 : 0.10730125226002397
Loss in iteration 32 : 0.10504785674074041
Loss in iteration 33 : 0.10292565796666187
Loss in iteration 34 : 0.10092441692402065
Loss in iteration 35 : 0.09903495735216819
Loss in iteration 36 : 0.09724894620805266
Loss in iteration 37 : 0.09555874920720171
Loss in iteration 38 : 0.09395734156812911
Loss in iteration 39 : 0.09243825380425867
Loss in iteration 40 : 0.09099553534837986
Loss in iteration 41 : 0.08962372351469328
Loss in iteration 42 : 0.08831781047098929
Loss in iteration 43 : 0.08707320549103435
Loss in iteration 44 : 0.08588569316573269
Loss in iteration 45 : 0.08475139024089466
Loss in iteration 46 : 0.08366670440539095
Loss in iteration 47 : 0.08262829797260249
Loss in iteration 48 : 0.08163305837593904
Loss in iteration 49 : 0.08067807613302022
Loss in iteration 50 : 0.07976062975138998
Loss in iteration 51 : 0.07887817617190168
Loss in iteration 52 : 0.0780283448772575
Loss in iteration 53 : 0.07720893373173568
Loss in iteration 54 : 0.07641790488714212
Loss in iteration 55 : 0.07565337957054584
Loss in iteration 56 : 0.07491363113246398
Loss in iteration 57 : 0.0741970762662818
Loss in iteration 58 : 0.07350226472884924
Loss in iteration 59 : 0.07282786815456195
Loss in iteration 60 : 0.07217266865360719
Loss in iteration 61 : 0.07153554784136337
Loss in iteration 62 : 0.0709154768007521
Loss in iteration 63 : 0.07031150728065233
Loss in iteration 64 : 0.0697227642270475
Loss in iteration 65 : 0.06914843956574661
Loss in iteration 66 : 0.06858778702947656
Loss in iteration 67 : 0.06804011775705547
Loss in iteration 68 : 0.06750479638514306
Loss in iteration 69 : 0.06698123739171996
Loss in iteration 70 : 0.06646890151811649
Loss in iteration 71 : 0.06596729217526569
Loss in iteration 72 : 0.06547595181432783
Loss in iteration 73 : 0.0649944583005963
Loss in iteration 74 : 0.06452242136652626
Loss in iteration 75 : 0.06405947923367342
Loss in iteration 76 : 0.06360529548716493
Loss in iteration 77 : 0.06315955626556086
Loss in iteration 78 : 0.06272196780023316
Loss in iteration 79 : 0.062292254308116184
Loss in iteration 80 : 0.06187015621509005
Loss in iteration 81 : 0.061455428667810286
Loss in iteration 82 : 0.06104784028103883
Loss in iteration 83 : 0.06064717206530217
Loss in iteration 84 : 0.06025321648453115
Loss in iteration 85 : 0.05986577660300011
Loss in iteration 86 : 0.05948466529290301
Loss in iteration 87 : 0.059109704486010446
Loss in iteration 88 : 0.05874072446326933
Loss in iteration 89 : 0.05837756318383677
Loss in iteration 90 : 0.058020065659440435
Loss in iteration 91 : 0.05766808338129624
Loss in iteration 92 : 0.0573214738056763
Loss in iteration 93 : 0.05698009990142175
Loss in iteration 94 : 0.056643829759139376
Loss in iteration 95 : 0.05631253625828673
Loss in iteration 96 : 0.05598609678545971
Loss in iteration 97 : 0.05566439299530896
Loss in iteration 98 : 0.05534731060476911
Loss in iteration 99 : 0.05503473921160267
Loss in iteration 100 : 0.054726572129427525
Loss in iteration 101 : 0.05442270623310361
Loss in iteration 102 : 0.05412304181028524
Loss in iteration 103 : 0.05382748241681007
Loss in iteration 104 : 0.05353593473517131
Loss in iteration 105 : 0.05324830843648641
Loss in iteration 106 : 0.052964516047058686
Loss in iteration 107 : 0.052684472820885425
Loss in iteration 108 : 0.052408096619350586
Loss in iteration 109 : 0.05213530779897744
Loss in iteration 110 : 0.05186602910762319
Loss in iteration 111 : 0.05160018558897948
Loss in iteration 112 : 0.05133770449479337
Loss in iteration 113 : 0.051078515203893934
Loss in iteration 114 : 0.05082254914692538
Loss in iteration 115 : 0.050569739735658516
Loss in iteration 116 : 0.05032002229583781
Loss in iteration 117 : 0.05007333400270303
Loss in iteration 118 : 0.049829613818552554
Loss in iteration 119 : 0.049588802431954127
Loss in iteration 120 : 0.049350842198424935
Loss in iteration 121 : 0.04911567708257607
Loss in iteration 122 : 0.04888325260183525
Loss in iteration 123 : 0.04865351577191639
Loss in iteration 124 : 0.04842641505421889
Loss in iteration 125 : 0.048201900305301144
Loss in iteration 126 : 0.047979922728519836
Loss in iteration 127 : 0.04776043482785421
Loss in iteration 128 : 0.047543390363868974
Loss in iteration 129 : 0.047328744311709664
Loss in iteration 130 : 0.04711645282098683
Loss in iteration 131 : 0.046906473177381926
Loss in iteration 132 : 0.046698763765805895
Loss in iteration 133 : 0.04649328403495391
Loss in iteration 134 : 0.046289994463121376
Loss in iteration 135 : 0.04608885652517892
Loss in iteration 136 : 0.04588983266062795
Loss in iteration 137 : 0.045692886242694054
Loss in iteration 138 : 0.045497981548429295
Loss in iteration 139 : 0.04530508372981631
Loss in iteration 140 : 0.04511415878586997
Loss in iteration 141 : 0.044925173535739646
Loss in iteration 142 : 0.0447380955928044
Loss in iteration 143 : 0.04455289333975464
Loss in iteration 144 : 0.0443695359046376
Loss in iteration 145 : 0.04418799313784095
Loss in iteration 146 : 0.04400823558997835
Loss in iteration 147 : 0.043830234490635174
Loss in iteration 148 : 0.043653961727931505
Loss in iteration 149 : 0.043479389828855255
Loss in iteration 150 : 0.04330649194032365
Loss in iteration 151 : 0.04313524181092985
Loss in iteration 152 : 0.042965613773339004
Loss in iteration 153 : 0.04279758272730115
Loss in iteration 154 : 0.042631124123251
Loss in iteration 155 : 0.04246621394647276
Loss in iteration 156 : 0.04230282870180681
Loss in iteration 157 : 0.04214094539888016
Loss in iteration 158 : 0.04198054153784425
Loss in iteration 159 : 0.04182159509560297
Loss in iteration 160 : 0.04166408451251497
Loss in iteration 161 : 0.04150798867955486
Loss in iteration 162 : 0.04135328692591613
Loss in iteration 163 : 0.041199959007039634
Loss in iteration 164 : 0.04104798509305105
Loss in iteration 165 : 0.04089734575758944
Loss in iteration 166 : 0.04074802196701197
Loss in iteration 167 : 0.04059999506995719
Loss in iteration 168 : 0.04045324678725251
Loss in iteration 169 : 0.040307759202151364
Loss in iteration 170 : 0.04016351475088535
Loss in iteration 171 : 0.040020496213520496
Loss in iteration 172 : 0.039878686705104006
Loss in iteration 173 : 0.03973806966709222
Loss in iteration 174 : 0.039598628859048725
Loss in iteration 175 : 0.03946034835060266
Loss in iteration 176 : 0.03932321251365974
Loss in iteration 177 : 0.0391872060148558
Loss in iteration 178 : 0.03905231380824463
Loss in iteration 179 : 0.03891852112821355
Loss in iteration 180 : 0.038785813482616394
Loss in iteration 181 : 0.03865417664611863
Loss in iteration 182 : 0.03852359665374477
Loss in iteration 183 : 0.03839405979462313
Loss in iteration 184 : 0.03826555260591857
Loss in iteration 185 : 0.038138061866947605
Loss in iteration 186 : 0.03801157459346944
Loss in iteration 187 : 0.0378860780321451
Loss in iteration 188 : 0.03776155965516045
Loss in iteration 189 : 0.03763800715500572
Loss in iteration 190 : 0.03751540843940668
Loss in iteration 191 : 0.037393751626401864
Loss in iteration 192 : 0.03727302503956156
Loss in iteration 193 : 0.03715321720334174
Loss in iteration 194 : 0.03703431683857138
Loss in iteration 195 : 0.036916312858065146
Loss in iteration 196 : 0.03679919436236013
Loss in iteration 197 : 0.03668295063557126
Loss in iteration 198 : 0.03656757114136072
Loss in iteration 199 : 0.036453045519019456
Loss in iteration 200 : 0.0363393635796552
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.9715555555555555, training accuracy 0.9955701629036867, time elapsed: 4656 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 1.66603220189524
Loss in iteration 3 : 1.2178879602564268
Loss in iteration 4 : 3.223712439428168
Loss in iteration 5 : 0.8616268489875162
Loss in iteration 6 : 0.5155683052177406
Loss in iteration 7 : 0.5724973651256393
Loss in iteration 8 : 0.5072359890021206
Loss in iteration 9 : 0.3896119542625313
Loss in iteration 10 : 0.2942998221642528
Loss in iteration 11 : 0.23783706339609445
Loss in iteration 12 : 0.20554507431017546
Loss in iteration 13 : 0.1794082808864253
Loss in iteration 14 : 0.15527211563696897
Loss in iteration 15 : 0.1349262839478539
Loss in iteration 16 : 0.12236027471795898
Loss in iteration 17 : 0.11553419625614553
Loss in iteration 18 : 0.10933800587802323
Loss in iteration 19 : 0.10165265885309888
Loss in iteration 20 : 0.09349312144704403
Loss in iteration 21 : 0.08717399954856778
Loss in iteration 22 : 0.08212313484382718
Loss in iteration 23 : 0.07727991157949869
Loss in iteration 24 : 0.07189074633512896
Loss in iteration 25 : 0.06607046856893857
Loss in iteration 26 : 0.06033268545928196
Loss in iteration 27 : 0.05504471186821342
Loss in iteration 28 : 0.05029778171041121
Loss in iteration 29 : 0.04620993403393771
Loss in iteration 30 : 0.042846532332297356
Loss in iteration 31 : 0.040173803389736876
Loss in iteration 32 : 0.038052251841869715
Loss in iteration 33 : 0.03631126551910909
Loss in iteration 34 : 0.03481485490168966
Loss in iteration 35 : 0.03346506962023668
Loss in iteration 36 : 0.03219367720124202
Loss in iteration 37 : 0.030958359464379175
Loss in iteration 38 : 0.02973623467126201
Loss in iteration 39 : 0.02851623844245359
Loss in iteration 40 : 0.02729368992584201
Loss in iteration 41 : 0.02606717762112733
Loss in iteration 42 : 0.024836876493147753
Loss in iteration 43 : 0.023603648740435444
Loss in iteration 44 : 0.02236858155326389
Loss in iteration 45 : 0.021132774599865872
Loss in iteration 46 : 0.019897264937042202
Loss in iteration 47 : 0.01866302026697534
Loss in iteration 48 : 0.017430967056965157
Loss in iteration 49 : 0.016202058268604852
Loss in iteration 50 : 0.014977436890992856
Loss in iteration 51 : 0.013758839944172561
Loss in iteration 52 : 0.01254955877453691
Loss in iteration 53 : 0.01135654557818893
Loss in iteration 54 : 0.010194261336440929
Loss in iteration 55 : 0.009088660423860882
Loss in iteration 56 : 0.008073038629623911
Loss in iteration 57 : 0.007169550301787301
Loss in iteration 58 : 0.006377941612342015
Loss in iteration 59 : 0.005687113070367889
Loss in iteration 60 : 0.005085285997329408
Loss in iteration 61 : 0.004560190973790312
Loss in iteration 62 : 0.004099280336600509
Loss in iteration 63 : 0.003691892207054196
Loss in iteration 64 : 0.003330474981799483
Loss in iteration 65 : 0.003010352461873001
Loss in iteration 66 : 0.002728972822444169
Loss in iteration 67 : 0.0024850180603359934
Loss in iteration 68 : 0.002277268349548325
Loss in iteration 69 : 0.00210336181292064
Loss in iteration 70 : 0.001959031930653416
Loss in iteration 71 : 0.00183833246741033
Loss in iteration 72 : 0.0017347076223217888
Loss in iteration 73 : 0.0016422412866577195
Loss in iteration 74 : 0.0015564964970314831
Loss in iteration 75 : 0.001474757207171852
Loss in iteration 76 : 0.0013957967747354565
Loss in iteration 77 : 0.0013194082652963181
Loss in iteration 78 : 0.0012459074100456637
Loss in iteration 79 : 0.0011757397869493287
Loss in iteration 80 : 0.0011092411254358678
Loss in iteration 81 : 0.0010465397708475915
Loss in iteration 82 : 9.875601808181835E-4
Loss in iteration 83 : 9.320805388968256E-4
Loss in iteration 84 : 8.79806187130706E-4
Loss in iteration 85 : 8.304344164672051E-4
Loss in iteration 86 : 7.8369919798752E-4
Loss in iteration 87 : 7.393939392286191E-4
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.9848888888888889, training accuracy 0.9998571020291512, time elapsed: 1934 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 1.1774865263236816
Loss in iteration 3 : 0.9464411540233595
Loss in iteration 4 : 2.374760258860678
Loss in iteration 5 : 0.6667999288538451
Loss in iteration 6 : 0.36534402929080256
Loss in iteration 7 : 0.41383053986330964
Loss in iteration 8 : 0.374930000403285
Loss in iteration 9 : 0.29116912081969376
Loss in iteration 10 : 0.21920922482431832
Loss in iteration 11 : 0.17646069917107224
Loss in iteration 12 : 0.15384335351554718
Loss in iteration 13 : 0.13567700462296797
Loss in iteration 14 : 0.11785474432111373
Loss in iteration 15 : 0.10228769423215729
Loss in iteration 16 : 0.09205650050468638
Loss in iteration 17 : 0.08691282978875839
Loss in iteration 18 : 0.08263446627448567
Loss in iteration 19 : 0.07716179337317174
Loss in iteration 20 : 0.07102122307173665
Loss in iteration 21 : 0.06584165652962091
Loss in iteration 22 : 0.06173071071376703
Loss in iteration 23 : 0.05787495104338642
Loss in iteration 24 : 0.05375257991385288
Loss in iteration 25 : 0.049388784156856894
Loss in iteration 26 : 0.045084105366807056
Loss in iteration 27 : 0.04112208038172821
Loss in iteration 28 : 0.037616917377608874
Loss in iteration 29 : 0.03459498232653877
Loss in iteration 30 : 0.03205174791417116
Loss in iteration 31 : 0.029951988884066815
Loss in iteration 32 : 0.028230422583780532
Loss in iteration 33 : 0.02680105660553004
Loss in iteration 34 : 0.025578953279325557
Loss in iteration 35 : 0.024494624610389558
Loss in iteration 36 : 0.02349693782643145
Loss in iteration 37 : 0.02255077791139291
Loss in iteration 38 : 0.021633381027589788
Loss in iteration 39 : 0.02073077946603722
Loss in iteration 40 : 0.0198349345932332
Loss in iteration 41 : 0.01894165100814112
Loss in iteration 42 : 0.018049124846379848
Loss in iteration 43 : 0.017156952429534612
Loss in iteration 44 : 0.01626547500937075
Loss in iteration 45 : 0.0153753788093605
Loss in iteration 46 : 0.01448749181687035
Loss in iteration 47 : 0.013602730803976757
Loss in iteration 48 : 0.012722166105143675
Loss in iteration 49 : 0.011847193949827266
Loss in iteration 50 : 0.010979836551332013
Loss in iteration 51 : 0.010123220063148995
Loss in iteration 52 : 0.00928227848513715
Loss in iteration 53 : 0.0084646103601943
Loss in iteration 54 : 0.0076810262519917145
Loss in iteration 55 : 0.006944716634311837
Loss in iteration 56 : 0.0062680881220577595
Loss in iteration 57 : 0.005658502137404126
Loss in iteration 58 : 0.005116333888148607
Loss in iteration 59 : 0.004636619033142913
Loss in iteration 60 : 0.0042120153013123765
Loss in iteration 61 : 0.0038350677494096022
Loss in iteration 62 : 0.0034994538935421296
Loss in iteration 63 : 0.00320039877302276
Loss in iteration 64 : 0.0029345006181507576
Loss in iteration 65 : 0.002699262158485799
Loss in iteration 66 : 0.0024925684352483744
Loss in iteration 67 : 0.002312244331545932
Loss in iteration 68 : 0.0021557630328027113
Loss in iteration 69 : 0.002020152075942029
Loss in iteration 70 : 0.0019021056704219943
Loss in iteration 71 : 0.0017982497969172555
Loss in iteration 72 : 0.0017054563371838305
Loss in iteration 73 : 0.0016210991803073517
Loss in iteration 74 : 0.001543186653882044
Loss in iteration 75 : 0.001470360768243552
Loss in iteration 76 : 0.0014017962468071115
Loss in iteration 77 : 0.0013370498590465857
Loss in iteration 78 : 0.0012759066559626091
Loss in iteration 79 : 0.0012182541112210317
Loss in iteration 80 : 0.0011639973450217045
Loss in iteration 81 : 0.0011130145687528203
Loss in iteration 82 : 0.0010651438302573654
Loss in iteration 83 : 0.0010201895238952415
Loss in iteration 84 : 9.779381604355676E-4
Loss in iteration 85 : 9.38175666369211E-4
Loss in iteration 86 : 9.007015835020266E-4
Loss in iteration 87 : 8.653381667222582E-4
Loss in iteration 88 : 8.319342268468786E-4
Loss in iteration 89 : 8.003646412063473E-4
Testing accuracy  of updater 2 on alg 0 with rate 7.0 = 0.9848888888888889, training accuracy 0.9997142040583024, time elapsed: 2094 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.7002356274945285
Loss in iteration 3 : 0.7334569640192863
Loss in iteration 4 : 1.6419199788125534
Loss in iteration 5 : 0.5280100666874492
Loss in iteration 6 : 0.21851549591933425
Loss in iteration 7 : 0.2604016862816613
Loss in iteration 8 : 0.2541544655032399
Loss in iteration 9 : 0.21228842175767185
Loss in iteration 10 : 0.1659068213234434
Loss in iteration 11 : 0.13164396145355975
Loss in iteration 12 : 0.1140246344066603
Loss in iteration 13 : 0.10305341043220406
Loss in iteration 14 : 0.0912405511455937
Loss in iteration 15 : 0.07949341479029126
Loss in iteration 16 : 0.06997895217069847
Loss in iteration 17 : 0.06407415363066948
Loss in iteration 18 : 0.060905357863718534
Loss in iteration 19 : 0.05790372410383836
Loss in iteration 20 : 0.05404900034000272
Loss in iteration 21 : 0.04995637919961193
Loss in iteration 22 : 0.04638087114193991
Loss in iteration 23 : 0.04337555371141706
Loss in iteration 24 : 0.04058632524364299
Loss in iteration 25 : 0.03776702194355313
Loss in iteration 26 : 0.03490589388459015
Loss in iteration 27 : 0.03212166533673882
Loss in iteration 28 : 0.029540636206957235
Loss in iteration 29 : 0.02723429718561595
Loss in iteration 30 : 0.025214250030879782
Loss in iteration 31 : 0.023458378579679834
Loss in iteration 32 : 0.021934989062130962
Loss in iteration 33 : 0.020612907602314316
Loss in iteration 34 : 0.01946238897873492
Loss in iteration 35 : 0.01845418635496986
Loss in iteration 36 : 0.01756013509714834
Loss in iteration 37 : 0.016754669883681687
Loss in iteration 38 : 0.016016014830423024
Loss in iteration 39 : 0.015326610529906269
Loss in iteration 40 : 0.014672945528286463
Loss in iteration 41 : 0.01404507948972995
Loss in iteration 42 : 0.013436056036426143
Loss in iteration 43 : 0.012841305815872832
Loss in iteration 44 : 0.012258087642691744
Loss in iteration 45 : 0.011684995057027034
Loss in iteration 46 : 0.011121546737403344
Loss in iteration 47 : 0.010567870837178448
Loss in iteration 48 : 0.01002448332445393
Loss in iteration 49 : 0.009492150764766316
Loss in iteration 50 : 0.00897182098018832
Loss in iteration 51 : 0.008464601418131388
Loss in iteration 52 : 0.007971763950089507
Loss in iteration 53 : 0.007494754522398072
Loss in iteration 54 : 0.007035185688858211
Loss in iteration 55 : 0.006594790907040578
Loss in iteration 56 : 0.006175325548105082
Loss in iteration 57 : 0.005778415188729974
Loss in iteration 58 : 0.005405376169286178
Loss in iteration 59 : 0.005057055911224155
Loss in iteration 60 : 0.004733744151423465
Loss in iteration 61 : 0.004435182469259461
Loss in iteration 62 : 0.004160659884031736
Loss in iteration 63 : 0.003909151379677558
Loss in iteration 64 : 0.0036794504768895976
Loss in iteration 65 : 0.0034702636799082325
Loss in iteration 66 : 0.0032802590466474795
Loss in iteration 67 : 0.0031080799071884217
Loss in iteration 68 : 0.0029523425483523693
Loss in iteration 69 : 0.0028116349920505076
Loss in iteration 70 : 0.002684527160114891
Loss in iteration 71 : 0.0025695948650169024
Loss in iteration 72 : 0.002465453883614366
Loss in iteration 73 : 0.002370797055230066
Loss in iteration 74 : 0.002284426946556834
Loss in iteration 75 : 0.0022052784451181266
Loss in iteration 76 : 0.002132428610436677
Loss in iteration 77 : 0.002065094144959124
Loss in iteration 78 : 0.002002619136307177
Loss in iteration 79 : 0.001944456882222468
Loss in iteration 80 : 0.0018901496641114707
Loss in iteration 81 : 0.0018393095887483365
Loss in iteration 82 : 0.0017916024812422594
Loss in iteration 83 : 0.0017467356568804308
Loss in iteration 84 : 0.0017044494740150833
Loss in iteration 85 : 0.0016645119843988474
Loss in iteration 86 : 0.0016267157476984508
Loss in iteration 87 : 0.0015908758907664583
Loss in iteration 88 : 0.0015568286706436972
Loss in iteration 89 : 0.0015244300487745911
Loss in iteration 90 : 0.0014935540298451966
Loss in iteration 91 : 0.001464090716797144
Loss in iteration 92 : 0.0014359441643156511
Loss in iteration 93 : 0.001409030176470469
Loss in iteration 94 : 0.0013832742026315247
Loss in iteration 95 : 0.0013586094573392856
Testing accuracy  of updater 2 on alg 0 with rate 4.0 = 0.9848888888888889, training accuracy 0.9997142040583024, time elapsed: 3553 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.38662499419280294
Loss in iteration 3 : 0.3054893986847428
Loss in iteration 4 : 0.23935035529819632
Loss in iteration 5 : 0.14284643918196732
Loss in iteration 6 : 0.11982297701420226
Loss in iteration 7 : 0.10927266284116101
Loss in iteration 8 : 0.09947178714949753
Loss in iteration 9 : 0.09111934606004504
Loss in iteration 10 : 0.08414481363502871
Loss in iteration 11 : 0.07811252860595264
Loss in iteration 12 : 0.07267524496979495
Loss in iteration 13 : 0.06765633784016267
Loss in iteration 14 : 0.0630050228194952
Loss in iteration 15 : 0.05873047510768985
Loss in iteration 16 : 0.05485052137638002
Loss in iteration 17 : 0.051363475839909356
Loss in iteration 18 : 0.04824238109914791
Loss in iteration 19 : 0.04544503354165413
Loss in iteration 20 : 0.04292812988665515
Loss in iteration 21 : 0.04065544868035632
Loss in iteration 22 : 0.03859796310332099
Loss in iteration 23 : 0.03673026692488725
Loss in iteration 24 : 0.035028069854409004
Loss in iteration 25 : 0.03346827657974026
Loss in iteration 26 : 0.032030497713665555
Loss in iteration 27 : 0.03069833080520054
Loss in iteration 28 : 0.02945959862036445
Loss in iteration 29 : 0.028305670456941354
Loss in iteration 30 : 0.027230399278815157
Loss in iteration 31 : 0.026229133560919517
Loss in iteration 32 : 0.025298008766113163
Loss in iteration 33 : 0.024433517696552783
Loss in iteration 34 : 0.023632272421614604
Loss in iteration 35 : 0.022890872353016838
Loss in iteration 36 : 0.022205828139283564
Loss in iteration 37 : 0.021573522262984413
Loss in iteration 38 : 0.02099020240228272
Loss in iteration 39 : 0.020452005820233207
Loss in iteration 40 : 0.019955009572873595
Loss in iteration 41 : 0.019495297712285916
Loss in iteration 42 : 0.019069035274881706
Loss in iteration 43 : 0.018672539783016848
Loss in iteration 44 : 0.01830234335604792
Loss in iteration 45 : 0.017955241326532336
Loss in iteration 46 : 0.017628325774666807
Loss in iteration 47 : 0.017319004271921266
Loss in iteration 48 : 0.0170250052929011
Loss in iteration 49 : 0.016744372322327648
Loss in iteration 50 : 0.016475448840106134
Loss in iteration 51 : 0.016216856300824693
Loss in iteration 52 : 0.01596746707564259
Loss in iteration 53 : 0.015726374169231543
Loss in iteration 54 : 0.015492859378880506
Loss in iteration 55 : 0.015266361408886528
Loss in iteration 56 : 0.015046445264435548
Loss in iteration 57 : 0.014832774010242725
Loss in iteration 58 : 0.014625083694917818
Loss in iteration 59 : 0.014423161934415918
Loss in iteration 60 : 0.014226830347798808
Loss in iteration 61 : 0.014035930775370275
Loss in iteration 62 : 0.013850315003851664
Loss in iteration 63 : 0.013669837584523862
Loss in iteration 64 : 0.013494351255947643
Loss in iteration 65 : 0.013323704462870591
Loss in iteration 66 : 0.01315774048332467
Loss in iteration 67 : 0.012996297722507861
Loss in iteration 68 : 0.012839210792656704
Loss in iteration 69 : 0.012686312063633635
Loss in iteration 70 : 0.01253743343329688
Loss in iteration 71 : 0.01239240812631237
Loss in iteration 72 : 0.012251072383053493
Loss in iteration 73 : 0.012113266945840674
Loss in iteration 74 : 0.011978838287825935
Loss in iteration 75 : 0.011847639560491285
Loss in iteration 76 : 0.011719531259395765
Loss in iteration 77 : 0.011594381625041818
Loss in iteration 78 : 0.01147206680727175
Loss in iteration 79 : 0.011352470828301138
Loss in iteration 80 : 0.011235485382276374
Loss in iteration 81 : 0.011121009509033008
Loss in iteration 82 : 0.01100894917739672
Loss in iteration 83 : 0.010899216809666198
Loss in iteration 84 : 0.010791730774461387
Loss in iteration 85 : 0.010686414870390855
Loss in iteration 86 : 0.010583197818326026
Loss in iteration 87 : 0.010482012775702544
Loss in iteration 88 : 0.010382796882340613
Loss in iteration 89 : 0.010285490843867168
Loss in iteration 90 : 0.01019003855596155
Loss in iteration 91 : 0.010096386770333247
Loss in iteration 92 : 0.010004484801546488
Loss in iteration 93 : 0.009914284272489233
Loss in iteration 94 : 0.009825738895389317
Loss in iteration 95 : 0.009738804284744412
Loss in iteration 96 : 0.009653437798291847
Loss in iteration 97 : 0.00956959840213479
Loss in iteration 98 : 0.009487246556304195
Loss in iteration 99 : 0.009406344117317058
Loss in iteration 100 : 0.00932685425464723
Loss in iteration 101 : 0.009248741378417996
Loss in iteration 102 : 0.009171971076027624
Loss in iteration 103 : 0.00909651005580794
Loss in iteration 104 : 0.009022326096179338
Loss in iteration 105 : 0.008949387999091146
Loss in iteration 106 : 0.008877665546821298
Loss in iteration 107 : 0.0088071294614501
Loss in iteration 108 : 0.008737751366521463
Loss in iteration 109 : 0.008669503750561849
Loss in iteration 110 : 0.008602359932248864
Loss in iteration 111 : 0.008536294027108772
Loss in iteration 112 : 0.008471280915682916
Loss in iteration 113 : 0.008407296213141097
Loss in iteration 114 : 0.008344316240338925
Loss in iteration 115 : 0.008282317996322893
Loss in iteration 116 : 0.008221279132283049
Loss in iteration 117 : 0.00816117792694319
Loss in iteration 118 : 0.00810199326336526
Loss in iteration 119 : 0.008043704607130088
Loss in iteration 120 : 0.007986291985841647
Loss in iteration 121 : 0.007929735969889941
Loss in iteration 122 : 0.007874017654396338
Loss in iteration 123 : 0.00781911864225778
Loss in iteration 124 : 0.007765021028201041
Loss in iteration 125 : 0.007711707383755831
Loss in iteration 126 : 0.007659160743056152
Loss in iteration 127 : 0.007607364589381445
Loss in iteration 128 : 0.007556302842353852
Loss in iteration 129 : 0.007505959845713245
Loss in iteration 130 : 0.007456320355599121
Loss in iteration 131 : 0.007407369529275428
Loss in iteration 132 : 0.0073590929142427946
Loss in iteration 133 : 0.0073114764376898105
Loss in iteration 134 : 0.007264506396243444
Loss in iteration 135 : 0.0072181694459850405
Loss in iteration 136 : 0.0071724525927056365
Loss in iteration 137 : 0.007127343182379833
Loss in iteration 138 : 0.007082828891842831
Loss in iteration 139 : 0.007038897719659679
Loss in iteration 140 : 0.006995537977179514
Loss in iteration 141 : 0.006952738279770575
Loss in iteration 142 : 0.0069104875382339755
Loss in iteration 143 : 0.006868774950396215
Loss in iteration 144 : 0.006827589992881623
Loss in iteration 145 : 0.0067869224130662304
Loss in iteration 146 : 0.00674676222121584
Loss in iteration 147 : 0.006707099682810148
Loss in iteration 148 : 0.0066679253110553905
Loss in iteration 149 : 0.00662922985958726
Loss in iteration 150 : 0.006591004315365343
Loss in iteration 151 : 0.0065532398917599165
Loss in iteration 152 : 0.006515928021831264
Loss in iteration 153 : 0.006479060351800812
Loss in iteration 154 : 0.006442628734713196
Loss in iteration 155 : 0.006406625224287366
Loss in iteration 156 : 0.006371042068954465
Loss in iteration 157 : 0.006335871706079699
Loss in iteration 158 : 0.006301106756364896
Loss in iteration 159 : 0.006266740018428344
Loss in iteration 160 : 0.00623276446355749
Loss in iteration 161 : 0.006199173230631036
Loss in iteration 162 : 0.0061659596212052924
Loss in iteration 163 : 0.006133117094760807
Loss in iteration 164 : 0.00610063926410429
Loss in iteration 165 : 0.006068519890921415
Loss in iteration 166 : 0.006036752881475639
Loss in iteration 167 : 0.006005332282448345
Loss in iteration 168 : 0.0059742522769159236
Loss in iteration 169 : 0.005943507180458849
Loss in iteration 170 : 0.005913091437398666
Loss in iteration 171 : 0.0058829996171582445
Loss in iteration 172 : 0.0058532264107411655
Loss in iteration 173 : 0.00582376662732603
Loss in iteration 174 : 0.005794615190971767
Loss in iteration 175 : 0.005765767137429947
Loss in iteration 176 : 0.005737217611060413
Loss in iteration 177 : 0.005708961861846503
Loss in iteration 178 : 0.005680995242506498
Loss in iteration 179 : 0.005653313205697734
Loss in iteration 180 : 0.005625911301310316
Loss in iteration 181 : 0.005598785173847009
Loss in iteration 182 : 0.0055719305598866216
Loss in iteration 183 : 0.005545343285627648
Loss in iteration 184 : 0.005519019264509517
Loss in iteration 185 : 0.005492954494908732
Loss in iteration 186 : 0.005467145057907197
Loss in iteration 187 : 0.0054415871151302815
Loss in iteration 188 : 0.005416276906652238
Loss in iteration 189 : 0.005391210748966475
Loss in iteration 190 : 0.005366385033018636
Loss in iteration 191 : 0.005341796222300089
Loss in iteration 192 : 0.005317440850999958
Loss in iteration 193 : 0.005293315522213503
Loss in iteration 194 : 0.005269416906205002
Loss in iteration 195 : 0.005245741738723202
Loss in iteration 196 : 0.005222286819367574
Loss in iteration 197 : 0.0051990490100036054
Loss in iteration 198 : 0.0051760252332254475
Loss in iteration 199 : 0.0051532124708643726
Loss in iteration 200 : 0.005130607762541315
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.9893333333333333, training accuracy 0.9998571020291512, time elapsed: 5149 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.4205390011707472
Loss in iteration 3 : 0.27974701642253186
Loss in iteration 4 : 0.2068482107587762
Loss in iteration 5 : 0.16588875273504194
Loss in iteration 6 : 0.1410596633549694
Loss in iteration 7 : 0.12454409323747295
Loss in iteration 8 : 0.11240057639806451
Loss in iteration 9 : 0.10284991955278464
Loss in iteration 10 : 0.095002405734875
Loss in iteration 11 : 0.08834132658025623
Loss in iteration 12 : 0.08254180750842768
Loss in iteration 13 : 0.07739395994107566
Loss in iteration 14 : 0.07276090489168656
Loss in iteration 15 : 0.06855285218939071
Loss in iteration 16 : 0.06471039605393974
Loss in iteration 17 : 0.061193204483044855
Loss in iteration 18 : 0.05797196094436703
Loss in iteration 19 : 0.05502270611237386
Loss in iteration 20 : 0.052323371761492155
Loss in iteration 21 : 0.049852295310980496
Loss in iteration 22 : 0.04758817276282537
Loss in iteration 23 : 0.045510666180261464
Loss in iteration 24 : 0.04360097660032519
Loss in iteration 25 : 0.04184205918566894
Loss in iteration 26 : 0.04021852989608882
Loss in iteration 27 : 0.03871648212959719
Loss in iteration 28 : 0.03732338433627666
Loss in iteration 29 : 0.03602809101816248
Loss in iteration 30 : 0.03482089661515798
Loss in iteration 31 : 0.03369354249989893
Loss in iteration 32 : 0.03263912818903211
Loss in iteration 33 : 0.03165193179034649
Loss in iteration 34 : 0.03072717882015004
Loss in iteration 35 : 0.02986080503067999
Loss in iteration 36 : 0.029049246723198856
Loss in iteration 37 : 0.028289274250597133
Loss in iteration 38 : 0.027577870128020033
Loss in iteration 39 : 0.026912145525839547
Loss in iteration 40 : 0.02628928689243292
Loss in iteration 41 : 0.02570652548026889
Loss in iteration 42 : 0.02516112437887311
Loss in iteration 43 : 0.024650379067045737
Loss in iteration 44 : 0.02417162818702053
Loss in iteration 45 : 0.02372227143625025
Loss in iteration 46 : 0.023299791522599575
Loss in iteration 47 : 0.022901777298342207
Loss in iteration 48 : 0.022525945579019123
Loss in iteration 49 : 0.022170159733512246
Loss in iteration 50 : 0.02183244380107216
Loss in iteration 51 : 0.02151099153942797
Loss in iteration 52 : 0.02120417035350043
Loss in iteration 53 : 0.02091052045652962
Loss in iteration 54 : 0.02062874987237407
Loss in iteration 55 : 0.02035772602186767
Loss in iteration 56 : 0.02009646467996914
Loss in iteration 57 : 0.019844117075130424
Loss in iteration 58 : 0.019599955851406545
Loss in iteration 59 : 0.01936336054205534
Loss in iteration 60 : 0.01913380311859269
Loss in iteration 61 : 0.018910834085604532
Loss in iteration 62 : 0.01869406949222621
Loss in iteration 63 : 0.01848317912985774
Loss in iteration 64 : 0.018277876087078128
Loss in iteration 65 : 0.018077907741874805
Loss in iteration 66 : 0.0178830481926673
Loss in iteration 67 : 0.017693092066254933
Loss in iteration 68 : 0.01750784959408914
Loss in iteration 69 : 0.017327142817769436
Loss in iteration 70 : 0.0171508027686544
Loss in iteration 71 : 0.016978667462407834
Loss in iteration 72 : 0.01681058055430615
Loss in iteration 73 : 0.016646390512464398
Loss in iteration 74 : 0.016485950181409623
Loss in iteration 75 : 0.016329116625719845
Loss in iteration 76 : 0.016175751161315373
Loss in iteration 77 : 0.016025719499398208
Loss in iteration 78 : 0.01587889194428667
Loss in iteration 79 : 0.01573514360103519
Loss in iteration 80 : 0.015594354561504887
Loss in iteration 81 : 0.015456410048348143
Loss in iteration 82 : 0.015321200505193002
Loss in iteration 83 : 0.015188621628251135
Loss in iteration 84 : 0.01505857433978714
Loss in iteration 85 : 0.014930964707581654
Loss in iteration 86 : 0.01480570381692643
Loss in iteration 87 : 0.014682707603042482
Loss in iteration 88 : 0.014561896652343647
Loss in iteration 89 : 0.014443195980881007
Loss in iteration 90 : 0.014326534797784696
Loss in iteration 91 : 0.014211846260715977
Loss in iteration 92 : 0.01409906722937857
Loss in iteration 93 : 0.013988138022106666
Loss in iteration 94 : 0.013879002179519609
Loss in iteration 95 : 0.013771606238259678
Loss in iteration 96 : 0.013665899516943665
Loss in iteration 97 : 0.013561833915682086
Loss in iteration 98 : 0.013459363729860131
Loss in iteration 99 : 0.013358445478335947
Loss in iteration 100 : 0.013259037745788217
Loss in iteration 101 : 0.013161101038626817
Loss in iteration 102 : 0.013064597653658928
Loss in iteration 103 : 0.012969491558560569
Loss in iteration 104 : 0.012875748283130347
Loss in iteration 105 : 0.012783334820281574
Loss in iteration 106 : 0.012692219535750658
Loss in iteration 107 : 0.012602372085550779
Loss in iteration 108 : 0.012513763340271513
Loss in iteration 109 : 0.012426365315409917
Loss in iteration 110 : 0.012340151107007305
Loss in iteration 111 : 0.012255094831958144
Loss in iteration 112 : 0.012171171572443592
Loss in iteration 113 : 0.012088357324025412
Loss in iteration 114 : 0.01200662894700931
Loss in iteration 115 : 0.011925964120753366
Loss in iteration 116 : 0.011846341300653335
Loss in iteration 117 : 0.01176773967758527
Loss in iteration 118 : 0.011690139139625464
Loss in iteration 119 : 0.011613520235899243
Loss in iteration 120 : 0.011537864142435723
Loss in iteration 121 : 0.011463152629924513
Loss in iteration 122 : 0.011389368033284056
Loss in iteration 123 : 0.01131649322296195
Loss in iteration 124 : 0.011244511577893562
Loss in iteration 125 : 0.011173406960049825
Loss in iteration 126 : 0.01110316369050695
Loss in iteration 127 : 0.011033766526972733
Loss in iteration 128 : 0.01096520064270303
Loss in iteration 129 : 0.010897451606743156
Loss in iteration 130 : 0.010830505365428057
Loss in iteration 131 : 0.01076434822507474
Loss in iteration 132 : 0.010698966835801087
Loss in iteration 133 : 0.010634348176405246
Loss in iteration 134 : 0.010570479540240818
Loss in iteration 135 : 0.010507348522024057
Loss in iteration 136 : 0.010444943005511348
Loss in iteration 137 : 0.010383251151986803
Loss in iteration 138 : 0.010322261389502442
Loss in iteration 139 : 0.010261962402816021
Loss in iteration 140 : 0.010202343123973954
Loss in iteration 141 : 0.010143392723490418
Loss in iteration 142 : 0.010085100602075884
Loss in iteration 143 : 0.01002745638287209
Loss in iteration 144 : 0.009970449904153066
Loss in iteration 145 : 0.009914071212454828
Loss in iteration 146 : 0.009858310556099458
Loss in iteration 147 : 0.00980315837908167
Loss in iteration 148 : 0.009748605315289002
Loss in iteration 149 : 0.009694642183028755
Loss in iteration 150 : 0.009641259979837712
Loss in iteration 151 : 0.009588449877552032
Loss in iteration 152 : 0.009536203217617589
Loss in iteration 153 : 0.009484511506621912
Loss in iteration 154 : 0.009433366412031494
Loss in iteration 155 : 0.009382759758118791
Loss in iteration 156 : 0.009332683522065446
Loss in iteration 157 : 0.009283129830228961
Loss in iteration 158 : 0.00923409095456137
Loss in iteration 159 : 0.009185559309169568
Loss in iteration 160 : 0.009137527447007621
Loss in iteration 161 : 0.009089988056692382
Loss in iteration 162 : 0.009042933959434545
Loss in iteration 163 : 0.008996358106077674
Loss in iteration 164 : 0.008950253574238618
Loss in iteration 165 : 0.008904613565543054
Loss in iteration 166 : 0.00885943140295074
Loss in iteration 167 : 0.008814700528164942
Loss in iteration 168 : 0.008770414499121382
Loss in iteration 169 : 0.008726566987552456
Loss in iteration 170 : 0.00868315177662237
Loss in iteration 171 : 0.008640162758629555
Loss in iteration 172 : 0.008597593932772858
Loss in iteration 173 : 0.008555439402978374
Loss in iteration 174 : 0.008513693375783704
Loss in iteration 175 : 0.008472350158277207
Loss in iteration 176 : 0.008431404156089506
Loss in iteration 177 : 0.00839084987143482
Loss in iteration 178 : 0.008350681901200146
Loss in iteration 179 : 0.00831089493508024
Loss in iteration 180 : 0.008271483753756245
Loss in iteration 181 : 0.00823244322711669
Loss in iteration 182 : 0.008193768312518807
Loss in iteration 183 : 0.008155454053089053
Loss in iteration 184 : 0.008117495576061142
Loss in iteration 185 : 0.008079888091150508
Loss in iteration 186 : 0.00804262688896392
Loss in iteration 187 : 0.008005707339443117
Loss in iteration 188 : 0.007969124890341477
Loss in iteration 189 : 0.007932875065732651
Loss in iteration 190 : 0.007896953464550436
Loss in iteration 191 : 0.007861355759158714
Loss in iteration 192 : 0.00782607769395109
Loss in iteration 193 : 0.007791115083978954
Loss in iteration 194 : 0.007756463813607757
Loss in iteration 195 : 0.007722119835200419
Loss in iteration 196 : 0.007688079167827465
Loss in iteration 197 : 0.007654337896003172
Loss in iteration 198 : 0.007620892168447159
Loss in iteration 199 : 0.007587738196870872
Loss in iteration 200 : 0.007554872254788471
Testing accuracy  of updater 2 on alg 0 with rate 0.7 = 0.9831111111111112, training accuracy 0.9989997142040583, time elapsed: 4286 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.49710343037019455
Loss in iteration 3 : 0.35208894993014245
Loss in iteration 4 : 0.26972530971533126
Loss in iteration 5 : 0.22014352814556865
Loss in iteration 6 : 0.18738869560410015
Loss in iteration 7 : 0.16393213092753386
Loss in iteration 8 : 0.1463677177029596
Loss in iteration 9 : 0.1328560016132462
Loss in iteration 10 : 0.122176330895674
Loss in iteration 11 : 0.11347578524869822
Loss in iteration 12 : 0.10617907462438955
Loss in iteration 13 : 0.09991070317193108
Loss in iteration 14 : 0.09442562437417408
Loss in iteration 15 : 0.089558886097827
Loss in iteration 16 : 0.08519415881683842
Loss in iteration 17 : 0.08124574859080169
Loss in iteration 18 : 0.07764865877599725
Loss in iteration 19 : 0.07435289261864686
Loss in iteration 20 : 0.07131976778619108
Loss in iteration 21 : 0.06851913220662872
Loss in iteration 22 : 0.06592705307293421
Loss in iteration 23 : 0.06352390089197933
Loss in iteration 24 : 0.06129287180211201
Loss in iteration 25 : 0.059218979396343165
Loss in iteration 26 : 0.05728848126705647
Loss in iteration 27 : 0.05548864107268042
Loss in iteration 28 : 0.05380769419140956
Loss in iteration 29 : 0.05223489143466921
Loss in iteration 30 : 0.050760531478345756
Loss in iteration 31 : 0.049375940411644496
Loss in iteration 32 : 0.048073397947861266
Loss in iteration 33 : 0.046846033337863635
Loss in iteration 34 : 0.0456877184579388
Loss in iteration 35 : 0.044592976906581074
Loss in iteration 36 : 0.043556914912842894
Loss in iteration 37 : 0.04257516943906017
Loss in iteration 38 : 0.04164386426612299
Loss in iteration 39 : 0.04075956571470003
Loss in iteration 40 : 0.03991923360687193
Loss in iteration 41 : 0.03912016745982696
Loss in iteration 42 : 0.03835995093157401
Loss in iteration 43 : 0.037636398601653645
Loss in iteration 44 : 0.03694750859604105
Loss in iteration 45 : 0.03629142312337001
Loss in iteration 46 : 0.03566639743941746
Loss in iteration 47 : 0.03507077659247846
Loss in iteration 48 : 0.034502978695508925
Loss in iteration 49 : 0.03396148334954279
Loss in iteration 50 : 0.03344482401834914
Loss in iteration 51 : 0.03295158343298359
Loss in iteration 52 : 0.0324803913525955
Loss in iteration 53 : 0.032029924166387114
Loss in iteration 54 : 0.03159890589316611
Loss in iteration 55 : 0.031186110153649804
Loss in iteration 56 : 0.030790362696353428
Loss in iteration 57 : 0.030410544079316898
Loss in iteration 58 : 0.030045592159041568
Loss in iteration 59 : 0.0296945041120349
Loss in iteration 60 : 0.02935633780186718
Loss in iteration 61 : 0.02903021239198019
Loss in iteration 62 : 0.028715308180533545
Loss in iteration 63 : 0.028410865691966047
Loss in iteration 64 : 0.0281161840992133
Loss in iteration 65 : 0.027830619072771037
Loss in iteration 66 : 0.02755358016203223
Loss in iteration 67 : 0.027284527814857993
Loss in iteration 68 : 0.027022970136767358
Loss in iteration 69 : 0.026768459483898818
Loss in iteration 70 : 0.026520588975358306
Loss in iteration 71 : 0.026278989001283833
Loss in iteration 72 : 0.026043323793073194
Loss in iteration 73 : 0.025813288111797113
Loss in iteration 74 : 0.025588604100005873
Loss in iteration 75 : 0.025369018331229395
Loss in iteration 76 : 0.025154299080881723
Loss in iteration 77 : 0.02494423383244094
Loss in iteration 78 : 0.02473862702406102
Loss in iteration 79 : 0.024537298033434744
Loss in iteration 80 : 0.024340079392874598
Loss in iteration 81 : 0.024146815222195524
Loss in iteration 82 : 0.02395735986393752
Loss in iteration 83 : 0.023771576703582403
Loss in iteration 84 : 0.02358933715648569
Loss in iteration 85 : 0.023410519803064696
Loss in iteration 86 : 0.023235009654176242
Loss in iteration 87 : 0.02306269752943392
Loss in iteration 88 : 0.022893479532334754
Loss in iteration 89 : 0.022727256607388235
Loss in iteration 90 : 0.022563934165887225
Loss in iteration 91 : 0.022403421768466797
Loss in iteration 92 : 0.02224563285410317
Loss in iteration 93 : 0.022090484506668645
Loss in iteration 94 : 0.021937897251539343
Loss in iteration 95 : 0.021787794876026324
Loss in iteration 96 : 0.021640104268546432
Loss in iteration 97 : 0.0214947552724582
Loss in iteration 98 : 0.02135168055135877
Loss in iteration 99 : 0.02121081546337416
Loss in iteration 100 : 0.021072097942586663
Loss in iteration 101 : 0.020935468386238307
Loss in iteration 102 : 0.02080086954674832
Loss in iteration 103 : 0.020668246427889312
Loss in iteration 104 : 0.02053754618470404
Loss in iteration 105 : 0.020408718026916725
Loss in iteration 106 : 0.020281713125716173
Loss in iteration 107 : 0.02015648452386904
Loss in iteration 108 : 0.020032987049171034
Loss in iteration 109 : 0.01991117723126717
Loss in iteration 110 : 0.019791013221879025
Loss in iteration 111 : 0.01967245471846741
Loss in iteration 112 : 0.019555462891344837
Loss in iteration 113 : 0.019440000314228344
Loss in iteration 114 : 0.019326030898202333
Loss in iteration 115 : 0.019213519829035266
Loss in iteration 116 : 0.019102433507773468
Loss in iteration 117 : 0.018992739494515926
Loss in iteration 118 : 0.018884406455256495
Loss in iteration 119 : 0.01877740411166825
Loss in iteration 120 : 0.018671703193694454
Loss in iteration 121 : 0.0185672753948048
Loss in iteration 122 : 0.01846409332977156
Loss in iteration 123 : 0.018362130494820068
Loss in iteration 124 : 0.018261361230009723
Loss in iteration 125 : 0.018161760683703666
Loss in iteration 126 : 0.01806330477899184
Loss in iteration 127 : 0.017965970181936543
Loss in iteration 128 : 0.01786973427151744
Loss in iteration 129 : 0.017774575111159315
Loss in iteration 130 : 0.01768047142173426
Loss in iteration 131 : 0.017587402555936976
Loss in iteration 132 : 0.017495348473939795
Loss in iteration 133 : 0.017404289720241068
Loss in iteration 134 : 0.017314207401628257
Loss in iteration 135 : 0.017225083166182466
Loss in iteration 136 : 0.017136899183258638
Loss in iteration 137 : 0.01704963812438075
Loss in iteration 138 : 0.01696328314499609
Loss in iteration 139 : 0.01687781786703849
Loss in iteration 140 : 0.016793226362253933
Loss in iteration 141 : 0.016709493136245996
Loss in iteration 142 : 0.01662660311320213
Loss in iteration 143 : 0.016544541621264695
Loss in iteration 144 : 0.016463294378513852
Loss in iteration 145 : 0.016382847479530876
Loss in iteration 146 : 0.016303187382514177
Loss in iteration 147 : 0.01622430089692039
Loss in iteration 148 : 0.016146175171606245
Loss in iteration 149 : 0.01606879768344749
Loss in iteration 150 : 0.01599215622641267
Loss in iteration 151 : 0.015916238901071463
Loss in iteration 152 : 0.015841034104517107
Loss in iteration 153 : 0.015766530520685138
Loss in iteration 154 : 0.015692717111049793
Loss in iteration 155 : 0.015619583105682108
Loss in iteration 156 : 0.015547117994653038
Loss in iteration 157 : 0.01547531151976665
Loss in iteration 158 : 0.015404153666608774
Loss in iteration 159 : 0.01533363465689709
Loss in iteration 160 : 0.015263744941119824
Loss in iteration 161 : 0.0151944751914498
Loss in iteration 162 : 0.015125816294922514
Loss in iteration 163 : 0.015057759346866165
Loss in iteration 164 : 0.014990295644573033
Loss in iteration 165 : 0.014923416681201892
Loss in iteration 166 : 0.01485711413990096
Loss in iteration 167 : 0.014791379888142707
Loss in iteration 168 : 0.01472620597226073
Loss in iteration 169 : 0.014661584612180926
Loss in iteration 170 : 0.014597508196337898
Loss in iteration 171 : 0.014533969276769707
Loss in iteration 172 : 0.014470960564382854
Loss in iteration 173 : 0.014408474924380958
Loss in iteration 174 : 0.014346505371850239
Loss in iteration 175 : 0.014285045067495278
Loss in iteration 176 : 0.014224087313519373
Loss in iteration 177 : 0.0141636255496433
Loss in iteration 178 : 0.014103653349257369
Loss in iteration 179 : 0.014044164415701286
Loss in iteration 180 : 0.013985152578666865
Loss in iteration 181 : 0.013926611790719147
Loss in iteration 182 : 0.013868536123930826
Loss in iteration 183 : 0.013810919766626252
Loss in iteration 184 : 0.013753757020230642
Loss in iteration 185 : 0.013697042296220494
Loss in iteration 186 : 0.01364077011317164
Loss in iteration 187 : 0.013584935093901252
Loss in iteration 188 : 0.013529531962700441
Loss in iteration 189 : 0.01347455554265405
Loss in iteration 190 : 0.013420000753044796
Loss in iteration 191 : 0.013365862606838455
Loss in iteration 192 : 0.013312136208247387
Loss in iteration 193 : 0.013258816750369688
Loss in iteration 194 : 0.013205899512901232
Loss in iteration 195 : 0.013153379859918164
Loss in iteration 196 : 0.013101253237727484
Loss in iteration 197 : 0.013049515172783093
Loss in iteration 198 : 0.012998161269665565
Loss in iteration 199 : 0.012947187209123067
Loss in iteration 200 : 0.012896588746171651
Testing accuracy  of updater 2 on alg 0 with rate 0.4 = 0.9768888888888889, training accuracy 0.998428122320663, time elapsed: 3727 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6326531461336024
Loss in iteration 3 : 0.5635345628267657
Loss in iteration 4 : 0.4947542419364412
Loss in iteration 5 : 0.431754206901754
Loss in iteration 6 : 0.3775444932806589
Loss in iteration 7 : 0.3329463937417748
Loss in iteration 8 : 0.2971533414226449
Loss in iteration 9 : 0.2686120931526104
Loss in iteration 10 : 0.24570082410543123
Loss in iteration 11 : 0.22704612408202415
Loss in iteration 12 : 0.21159478298953666
Loss in iteration 13 : 0.1985770410653671
Loss in iteration 14 : 0.18744055076689167
Loss in iteration 15 : 0.17778837933054426
Loss in iteration 16 : 0.1693309099749352
Loss in iteration 17 : 0.16185187854148006
Loss in iteration 18 : 0.15518567224050447
Loss in iteration 19 : 0.14920258776951656
Loss in iteration 20 : 0.14379928313813378
Loss in iteration 21 : 0.13889242911420668
Loss in iteration 22 : 0.13441426965918413
Loss in iteration 23 : 0.13030933096212108
Loss in iteration 24 : 0.12653186699520805
Loss in iteration 25 : 0.12304383051047654
Loss in iteration 26 : 0.11981325876791478
Loss in iteration 27 : 0.11681300484757588
Loss in iteration 28 : 0.11401975843359345
Loss in iteration 29 : 0.11141330290803937
Loss in iteration 30 : 0.10897595769367308
Loss in iteration 31 : 0.10669215933912757
Loss in iteration 32 : 0.10454814197007428
Loss in iteration 33 : 0.10253168626498134
Loss in iteration 34 : 0.10063191468281556
Loss in iteration 35 : 0.09883911821346306
Loss in iteration 36 : 0.09714460584762519
Loss in iteration 37 : 0.09554057211000715
Loss in iteration 38 : 0.09401998052002762
Loss in iteration 39 : 0.09257646207127593
Loss in iteration 40 : 0.09120422815304574
Loss in iteration 41 : 0.08989799715375607
Loss in iteration 42 : 0.08865293359575577
Loss in iteration 43 : 0.08746459827027776
Loss in iteration 44 : 0.08632890759437369
Loss in iteration 45 : 0.08524210034509201
Loss in iteration 46 : 0.08420071003035579
Loss in iteration 47 : 0.08320154138808117
Loss in iteration 48 : 0.08224164980882932
Loss in iteration 49 : 0.08131832279831264
Loss in iteration 50 : 0.08042906289123047
Loss in iteration 51 : 0.0795715716695227
Loss in iteration 52 : 0.07874373471411168
Loss in iteration 53 : 0.07794360743042396
Loss in iteration 54 : 0.07716940174417833
Loss in iteration 55 : 0.07641947367955162
Loss in iteration 56 : 0.07569231182220894
Loss in iteration 57 : 0.07498652664845973
Loss in iteration 58 : 0.07430084067926704
Loss in iteration 59 : 0.0736340794004881
Loss in iteration 60 : 0.07298516288150574
Loss in iteration 61 : 0.07235309802347603
Loss in iteration 62 : 0.07173697137410284
Loss in iteration 63 : 0.07113594245564839
Loss in iteration 64 : 0.07054923756416737
Loss in iteration 65 : 0.06997614400856306
Loss in iteration 66 : 0.0694160047665245
Loss in iteration 67 : 0.06886821354004394
Loss in iteration 68 : 0.06833221019600187
Loss in iteration 69 : 0.06780747657771506
Loss in iteration 70 : 0.06729353267208732
Loss in iteration 71 : 0.06678993311489304
Loss in iteration 72 : 0.06629626401445768
Loss in iteration 73 : 0.065812140072133
Loss in iteration 74 : 0.06533720197680733
Loss in iteration 75 : 0.06487111405038089
Loss in iteration 76 : 0.06441356212161925
Loss in iteration 77 : 0.06396425160693346
Loss in iteration 78 : 0.06352290577820294
Loss in iteration 79 : 0.06308926419955725
Loss in iteration 80 : 0.06266308131685899
Loss in iteration 81 : 0.06224412518536064
Loss in iteration 82 : 0.06183217632253998
Loss in iteration 83 : 0.06142702667443005
Loss in iteration 84 : 0.06102847868484289
Loss in iteration 85 : 0.060636344457785846
Loss in iteration 86 : 0.06025044500411892
Loss in iteration 87 : 0.05987060956415397
Loss in iteration 88 : 0.059496674998497576
Loss in iteration 89 : 0.05912848524000719
Loss in iteration 90 : 0.058765890800296976
Loss in iteration 91 : 0.05840874832479413
Loss in iteration 92 : 0.058056920190905026
Loss in iteration 93 : 0.05771027414440721
Loss in iteration 94 : 0.05736868296971031
Loss in iteration 95 : 0.057032024190131037
Loss in iteration 96 : 0.05670017979478991
Loss in iteration 97 : 0.056373035989150215
Loss in iteration 98 : 0.056050482966592825
Loss in iteration 99 : 0.055732414698739456
Loss in iteration 100 : 0.05541872874251811
Loss in iteration 101 : 0.05510932606220512
Loss in iteration 102 : 0.054804110864882835
Loss in iteration 103 : 0.05450299044793583
Loss in iteration 104 : 0.05420587505735945
Loss in iteration 105 : 0.05391267775579857
Loss in iteration 106 : 0.053623314299350626
Loss in iteration 107 : 0.05333770302228377
Loss in iteration 108 : 0.05305576472891327
Loss in iteration 109 : 0.0527774225919726
Loss in iteration 110 : 0.05250260205689343
Loss in iteration 111 : 0.052231230751477134
Loss in iteration 112 : 0.051963238400507164
Loss in iteration 113 : 0.05169855674490216
Loss in iteration 114 : 0.051437119465056635
Loss in iteration 115 : 0.0511788621080604
Loss in iteration 116 : 0.05092372201851678
Loss in iteration 117 : 0.050671638272713836
Loss in iteration 118 : 0.05042255161592455
Loss in iteration 119 : 0.0501764044026347
Loss in iteration 120 : 0.04993314053951578
Loss in iteration 121 : 0.0496927054309742
Loss in iteration 122 : 0.049455045927125005
Loss in iteration 123 : 0.04922011027404786
Loss in iteration 124 : 0.04898784806619594
Loss in iteration 125 : 0.04875821020083664
Loss in iteration 126 : 0.04853114883441551
Loss in iteration 127 : 0.0483066173407369
Loss in iteration 128 : 0.04808457027086991
Loss in iteration 129 : 0.04786496331468788
Loss in iteration 130 : 0.04764775326396047
Loss in iteration 131 : 0.04743289797692136
Loss in iteration 132 : 0.04722035634423887
Loss in iteration 133 : 0.047010088256322656
Loss in iteration 134 : 0.046802054571903076
Loss in iteration 135 : 0.0465962170878233
Loss in iteration 136 : 0.046392538509989
Loss in iteration 137 : 0.04619098242542157
Loss in iteration 138 : 0.0459915132753655
Loss in iteration 139 : 0.04579409632940157
Loss in iteration 140 : 0.04559869766052335
Loss in iteration 141 : 0.0454052841211312
Loss in iteration 142 : 0.0452138233199067
Loss in iteration 143 : 0.04502428359952697
Loss in iteration 144 : 0.044836634015183736
Loss in iteration 145 : 0.04465084431387307
Loss in iteration 146 : 0.04446688491442207
Loss in iteration 147 : 0.04428472688822251
Loss in iteration 148 : 0.044104341940640945
Loss in iteration 149 : 0.04392570239307859
Loss in iteration 150 : 0.04374878116565236
Loss in iteration 151 : 0.043573551760474144
Loss in iteration 152 : 0.043399988245501085
Loss in iteration 153 : 0.043228065238937144
Loss in iteration 154 : 0.043057757894160525
Loss in iteration 155 : 0.042889041885158616
Loss in iteration 156 : 0.04272189339244931
Loss in iteration 157 : 0.0425562890894694
Loss in iteration 158 : 0.04239220612941184
Loss in iteration 159 : 0.0422296221324953
Loss in iteration 160 : 0.04206851517364784
Loss in iteration 161 : 0.041908863770589944
Loss in iteration 162 : 0.04175064687230121
Loss in iteration 163 : 0.04159384384785594
Loss in iteration 164 : 0.04143843447561449
Loss in iteration 165 : 0.0412843989327555
Loss in iteration 166 : 0.04113171778513814
Loss in iteration 167 : 0.040980371977480674
Loss in iteration 168 : 0.04083034282384427
Loss in iteration 169 : 0.040681611998410784
Loss in iteration 170 : 0.0405341615265436
Loss in iteration 171 : 0.040387973776121994
Loss in iteration 172 : 0.04024303144913748
Loss in iteration 173 : 0.04009931757354497
Loss in iteration 174 : 0.0399568154953575
Loss in iteration 175 : 0.03981550887097774
Loss in iteration 176 : 0.039675381659755815
Loss in iteration 177 : 0.03953641811676841
Loss in iteration 178 : 0.03939860278580795
Loss in iteration 179 : 0.03926192049257747
Loss in iteration 180 : 0.039126356338082355
Loss in iteration 181 : 0.03899189569221314
Loss in iteration 182 : 0.03885852418751191
Loss in iteration 183 : 0.0387262277131169
Loss in iteration 184 : 0.03859499240887901
Loss in iteration 185 : 0.03846480465964376
Loss in iteration 186 : 0.03833565108969447
Loss in iteration 187 : 0.038207518557350224
Loss in iteration 188 : 0.03808039414971351
Loss in iteration 189 : 0.03795426517756329
Loss in iteration 190 : 0.03782911917038851
Loss in iteration 191 : 0.03770494387155648
Loss in iteration 192 : 0.037581727233613416
Loss in iteration 193 : 0.037459457413711435
Loss in iteration 194 : 0.03733812276915873
Loss in iteration 195 : 0.03721771185308894
Loss in iteration 196 : 0.03709821341024512
Loss in iteration 197 : 0.03697961637287586
Loss in iteration 198 : 0.03686190985673941
Loss in iteration 199 : 0.036745083157211936
Loss in iteration 200 : 0.03662912574549806
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.9724444444444444, training accuracy 0.9955701629036867, time elapsed: 3895 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 12.017962449375089
Loss in iteration 3 : 4.297431790821699
Loss in iteration 4 : 5.636614846185139
Loss in iteration 5 : 0.5583257759467926
Loss in iteration 6 : 0.31852801893412547
Loss in iteration 7 : 0.22073767509609313
Loss in iteration 8 : 0.1681725154416492
Loss in iteration 9 : 0.1339140193509196
Loss in iteration 10 : 0.11177970745978388
Loss in iteration 11 : 0.09642413084760489
Loss in iteration 12 : 0.08447624362922036
Loss in iteration 13 : 0.07499010463638739
Loss in iteration 14 : 0.06766351665721809
Loss in iteration 15 : 0.061719529082004336
Loss in iteration 16 : 0.056571881176764684
Loss in iteration 17 : 0.05200128690015168
Loss in iteration 18 : 0.04791508780847073
Loss in iteration 19 : 0.04426310687385435
Loss in iteration 20 : 0.04102278440961568
Loss in iteration 21 : 0.03819088346568029
Loss in iteration 22 : 0.03575361787475777
Loss in iteration 23 : 0.033661696786658354
Loss in iteration 24 : 0.03185381138472356
Loss in iteration 25 : 0.030272622993705516
Loss in iteration 26 : 0.02886912846894471
Loss in iteration 27 : 0.027606869186400887
Loss in iteration 28 : 0.02646038569492388
Loss in iteration 29 : 0.025410844400050495
Loss in iteration 30 : 0.024443627891616963
Loss in iteration 31 : 0.023548185110950134
Loss in iteration 32 : 0.022718085128983552
Loss in iteration 33 : 0.021950109010591287
Loss in iteration 34 : 0.021242633332441745
Loss in iteration 35 : 0.020594114260376176
Loss in iteration 36 : 0.020002080174679658
Loss in iteration 37 : 0.019462554488314746
Loss in iteration 38 : 0.018970049131354496
Loss in iteration 39 : 0.018518262999459772
Loss in iteration 40 : 0.018101004175091955
Loss in iteration 41 : 0.017712773926718706
Loss in iteration 42 : 0.01734893377150495
Loss in iteration 43 : 0.017005646649945375
Loss in iteration 44 : 0.016679752139288156
Loss in iteration 45 : 0.01636864497605186
Loss in iteration 46 : 0.01607017449519124
Loss in iteration 47 : 0.015782564216081263
Loss in iteration 48 : 0.015504346839636042
Loss in iteration 49 : 0.015234310446743017
Loss in iteration 50 : 0.01497145307382017
Loss in iteration 51 : 0.014714944005366394
Loss in iteration 52 : 0.014464090863605953
Loss in iteration 53 : 0.014218311959849395
Loss in iteration 54 : 0.013977113518004363
Loss in iteration 55 : 0.013740071384800647
Loss in iteration 56 : 0.01350681677411886
Loss in iteration 57 : 0.013277025506868252
Loss in iteration 58 : 0.013050410142294525
Loss in iteration 59 : 0.012826714375498363
Loss in iteration 60 : 0.012605709106111578
Loss in iteration 61 : 0.012387189655869518
Loss in iteration 62 : 0.012170973710357934
Loss in iteration 63 : 0.01195689966307949
Loss in iteration 64 : 0.011744825133162613
Loss in iteration 65 : 0.011534625503550647
Loss in iteration 66 : 0.011326192383106207
Loss in iteration 67 : 0.011119431937154927
Loss in iteration 68 : 0.010914263062198566
Loss in iteration 69 : 0.010710615407489008
Loss in iteration 70 : 0.010508427272911352
Loss in iteration 71 : 0.010307643440541752
Loss in iteration 72 : 0.01010821302457992
Loss in iteration 73 : 0.009910087446578633
Loss in iteration 74 : 0.009713218653931752
Loss in iteration 75 : 0.009517557693912902
Loss in iteration 76 : 0.009323053730578523
Loss in iteration 77 : 0.009129653549693298
Loss in iteration 78 : 0.008937301544597898
Loss in iteration 79 : 0.008745940124005508
Loss in iteration 80 : 0.008555510441636391
Loss in iteration 81 : 0.008365953324860783
Loss in iteration 82 : 0.00817721027750218
Loss in iteration 83 : 0.00798922444790049
Loss in iteration 84 : 0.007801941480997585
Loss in iteration 85 : 0.007615310205256567
Loss in iteration 86 : 0.007429283135362196
Loss in iteration 87 : 0.007243816795800982
Loss in iteration 88 : 0.007058871886923627
Loss in iteration 89 : 0.006874413324204807
Loss in iteration 90 : 0.006690410184525625
Loss in iteration 91 : 0.006506835592230227
Loss in iteration 92 : 0.006323666574189925
Loss in iteration 93 : 0.006140883908553558
Loss in iteration 94 : 0.005958471987274827
Loss in iteration 95 : 0.005776418708530796
Loss in iteration 96 : 0.005594715412163171
Loss in iteration 97 : 0.0054133568695201705
Loss in iteration 98 : 0.0052323413387458375
Loss in iteration 99 : 0.005051670697917725
Loss in iteration 100 : 0.004871350671907329
Loss in iteration 101 : 0.004691391175167726
Loss in iteration 102 : 0.004511806803041005
Loss in iteration 103 : 0.0043326175205111285
Loss in iteration 104 : 0.0041538496224843725
Loss in iteration 105 : 0.003975537077914889
Loss in iteration 106 : 0.003797723427514601
Loss in iteration 107 : 0.0036204644897797488
Loss in iteration 108 : 0.0034438322533935643
Loss in iteration 109 : 0.0032679205078443617
Loss in iteration 110 : 0.003092852997932298
Loss in iteration 111 : 0.0029187951783991076
Loss in iteration 112 : 0.0027459709518902935
Loss in iteration 113 : 0.0025746859698489465
Loss in iteration 114 : 0.002405358857739638
Loss in iteration 115 : 0.0022385604799911936
Loss in iteration 116 : 0.002075058061282628
Loss in iteration 117 : 0.0019158544035552212
Loss in iteration 118 : 0.0017622023412669269
Loss in iteration 119 : 0.0016155645941089466
Loss in iteration 120 : 0.0014774901663613226
Loss in iteration 121 : 0.001349405413831555
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.9893333333333333, training accuracy 0.9995713060874536, time elapsed: 2339 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 2.387956065677602
Loss in iteration 3 : 2.3573146745523164
Loss in iteration 4 : 8.693734049657841
Loss in iteration 5 : 0.7082980623374131
Loss in iteration 6 : 0.312905768601658
Loss in iteration 7 : 0.17905059732966952
Loss in iteration 8 : 0.1199531749440062
Loss in iteration 9 : 0.09372350602803324
Loss in iteration 10 : 0.07620127293824604
Loss in iteration 11 : 0.06338880082561052
Loss in iteration 12 : 0.0537452757897336
Loss in iteration 13 : 0.04643031679505693
Loss in iteration 14 : 0.04065854597871219
Loss in iteration 15 : 0.0360091931391386
Loss in iteration 16 : 0.032244779404292434
Loss in iteration 17 : 0.02917418151099841
Loss in iteration 18 : 0.026632021049307626
Loss in iteration 19 : 0.024490032703717697
Loss in iteration 20 : 0.022658408162050638
Loss in iteration 21 : 0.021075333473555332
Loss in iteration 22 : 0.019696484299112374
Loss in iteration 23 : 0.018488318723791388
Loss in iteration 24 : 0.01742425803424701
Loss in iteration 25 : 0.01648255894354333
Loss in iteration 26 : 0.0156451514256925
Loss in iteration 27 : 0.014896955512904098
Loss in iteration 28 : 0.014225379532194552
Loss in iteration 29 : 0.013619888537649803
Loss in iteration 30 : 0.013071633332369196
Loss in iteration 31 : 0.012573148147254984
Loss in iteration 32 : 0.012118111775561975
Loss in iteration 33 : 0.011701157004412068
Loss in iteration 34 : 0.011317713177079216
Loss in iteration 35 : 0.010963871971199003
Loss in iteration 36 : 0.010636271962786237
Loss in iteration 37 : 0.010332000885557161
Loss in iteration 38 : 0.010048515447058412
Loss in iteration 39 : 0.00978357792727798
Loss in iteration 40 : 0.009535207601764803
Loss in iteration 41 : 0.009301644110291923
Loss in iteration 42 : 0.00908131961069101
Loss in iteration 43 : 0.008872836885493783
Loss in iteration 44 : 0.008674951247327862
Loss in iteration 45 : 0.00848655483077763
Loss in iteration 46 : 0.008306662472681144
Loss in iteration 47 : 0.008134398799540844
Loss in iteration 48 : 0.007968986375168487
Loss in iteration 49 : 0.00780973486586995
Loss in iteration 50 : 0.0076560312084868664
Loss in iteration 51 : 0.007507330759003424
Loss in iteration 52 : 0.0073631493808328456
Loss in iteration 53 : 0.007223056414543434
Loss in iteration 54 : 0.007086668459151988
Loss in iteration 55 : 0.006953643889669859
Loss in iteration 56 : 0.006823678035150037
Loss in iteration 57 : 0.006696498944490132
Loss in iteration 58 : 0.006571863672301484
Loss in iteration 59 : 0.006449555023214478
Loss in iteration 60 : 0.006329378699348998
Loss in iteration 61 : 0.006211160801902237
Loss in iteration 62 : 0.006094745643653889
Loss in iteration 63 : 0.005979993834548249
Loss in iteration 64 : 0.005866780607347475
Loss in iteration 65 : 0.005754994354672187
Loss in iteration 66 : 0.005644535352586026
Loss in iteration 67 : 0.005535314649284033
Loss in iteration 68 : 0.005427253100455895
Loss in iteration 69 : 0.005320280535557019
Loss in iteration 70 : 0.005214335041570312
Loss in iteration 71 : 0.005109362352911149
Loss in iteration 72 : 0.005005315337937005
Loss in iteration 73 : 0.004902153574085322
Loss in iteration 74 : 0.004799843004976317
Loss in iteration 75 : 0.004698355673868195
Loss in iteration 76 : 0.00459766952860898
Loss in iteration 77 : 0.004497768293644665
Loss in iteration 78 : 0.004398641404648036
Loss in iteration 79 : 0.004300284000839744
Loss in iteration 80 : 0.004202696968979658
Loss in iteration 81 : 0.004105887031200123
Loss in iteration 82 : 0.004009866866229756
Loss in iteration 83 : 0.0039146552500445506
Loss in iteration 84 : 0.0038202771975815604
Loss in iteration 85 : 0.0037267640819810847
Loss in iteration 86 : 0.0036341537021938597
Loss in iteration 87 : 0.0035424902642654773
Loss in iteration 88 : 0.0034518242370638976
Loss in iteration 89 : 0.003362212040846188
Loss in iteration 90 : 0.0032737155283247246
Loss in iteration 91 : 0.0031864012243158035
Loss in iteration 92 : 0.00310033930289793
Loss in iteration 93 : 0.003015602300798748
Loss in iteration 94 : 0.002932263591741548
Loss in iteration 95 : 0.002850395676332194
Loss in iteration 96 : 0.002770068371657528
Loss in iteration 97 : 0.0026913470087664188
Loss in iteration 98 : 0.002614290759133051
Loss in iteration 99 : 0.002538951208913136
Loss in iteration 100 : 0.0024653712809067987
Loss in iteration 101 : 0.0023935845708072064
Loss in iteration 102 : 0.002323615122004606
Loss in iteration 103 : 0.0022554776193576885
Loss in iteration 104 : 0.0021891779443670555
Loss in iteration 105 : 0.002124714007642189
Loss in iteration 106 : 0.0020620767620016145
Loss in iteration 107 : 0.002001251300429905
Loss in iteration 108 : 0.0019422179545446042
Loss in iteration 109 : 0.0018849533271607728
Testing accuracy  of updater 3 on alg 0 with rate 7.0 = 0.9875555555555555, training accuracy 0.9991426121749071, time elapsed: 2302 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.8963406702139608
Loss in iteration 3 : 0.7085239629719199
Loss in iteration 4 : 2.1193025779502452
Loss in iteration 5 : 0.28821178979692536
Loss in iteration 6 : 0.060804221649623014
Loss in iteration 7 : 0.04134226398876103
Loss in iteration 8 : 0.03141624130213153
Loss in iteration 9 : 0.0253614541537981
Loss in iteration 10 : 0.0212838264428794
Loss in iteration 11 : 0.018372170259330425
Loss in iteration 12 : 0.016203374097132118
Loss in iteration 13 : 0.014532198663900845
Loss in iteration 14 : 0.013207708181861007
Loss in iteration 15 : 0.012133064546425395
Loss in iteration 16 : 0.01124386591575437
Loss in iteration 17 : 0.010495829453084205
Loss in iteration 18 : 0.009857566843203998
Loss in iteration 19 : 0.009306227091007174
Loss in iteration 20 : 0.008824788562285704
Loss in iteration 21 : 0.008400322267862293
Loss in iteration 22 : 0.008022843194148933
Loss in iteration 23 : 0.0076845285954600185
Loss in iteration 24 : 0.007379172361043443
Loss in iteration 25 : 0.007101795695655603
Loss in iteration 26 : 0.006848364039107656
Loss in iteration 27 : 0.00661557786942469
Loss in iteration 28 : 0.0064007159203967184
Loss in iteration 29 : 0.0062015162254847695
Loss in iteration 30 : 0.00601608486759304
Loss in iteration 31 : 0.005842825286006537
Loss in iteration 32 : 0.005680383011592737
Loss in iteration 33 : 0.005527602100178286
Loss in iteration 34 : 0.0053834905187139116
Loss in iteration 35 : 0.005247192441955797
Loss in iteration 36 : 0.005117965925746469
Loss in iteration 37 : 0.0049951647946080355
Loss in iteration 38 : 0.004878223855752903
Loss in iteration 39 : 0.0047666467560499425
Loss in iteration 40 : 0.004659995952070742
Loss in iteration 41 : 0.004557884379621009
Loss in iteration 42 : 0.004459968497844554
Loss in iteration 43 : 0.00436594245108664
Loss in iteration 44 : 0.00427553314434466
Loss in iteration 45 : 0.004188496069081395
Loss in iteration 46 : 0.00410461174822282
Loss in iteration 47 : 0.004023682694389696
Loss in iteration 48 : 0.0039455307953856645
Loss in iteration 49 : 0.0038699950568662858
Loss in iteration 50 : 0.003796929644841071
Loss in iteration 51 : 0.003726202180902081
Loss in iteration 52 : 0.0036576922513552553
Loss in iteration 53 : 0.0035912900981608788
Loss in iteration 54 : 0.0035268954650840498
Loss in iteration 55 : 0.003464416576960103
Loss in iteration 56 : 0.003403769233685031
Loss in iteration 57 : 0.0033448760035992755
Loss in iteration 58 : 0.0032876655034608394
Loss in iteration 59 : 0.0032320717542989584
Loss in iteration 60 : 0.0031780336041704227
Loss in iteration 61 : 0.0031254942102760834
Loss in iteration 62 : 0.003074400574074688
Loss in iteration 63 : 0.003024703124001776
Loss in iteration 64 : 0.00297635534119214
Loss in iteration 65 : 0.0029293134242453877
Loss in iteration 66 : 0.0028835359895879866
Loss in iteration 67 : 0.002838983804394508
Loss in iteration 68 : 0.002795619549352562
Loss in iteration 69 : 0.0027534076088077818
Loss in iteration 70 : 0.0027123138860207256
Loss in iteration 71 : 0.0026723056414207193
Loss in iteration 72 : 0.002633351351863184
Loss in iteration 73 : 0.002595420588995647
Loss in iteration 74 : 0.0025584839149245704
Loss in iteration 75 : 0.0025225127934518556
Loss in iteration 76 : 0.0024874795152274734
Loss in iteration 77 : 0.002453357135241241
Loss in iteration 78 : 0.002420119421158709
Loss in iteration 79 : 0.0023877408110918627
Loss in iteration 80 : 0.002356196379487097
Loss in iteration 81 : 0.0023254618099096645
Loss in iteration 82 : 0.00229551337360281
Loss in iteration 83 : 0.002266327912803316
Loss in iteration 84 : 0.0022378828278973835
Loss in iteration 85 : 0.002210156067603621
Loss in iteration 86 : 0.0021831261214689925
Loss in iteration 87 : 0.0021567720140599016
Loss in iteration 88 : 0.0021310733003205933
Loss in iteration 89 : 0.002106010061655938
Loss in iteration 90 : 0.002081562902373081
Loss in iteration 91 : 0.002057712946186947
Loss in iteration 92 : 0.002034441832557264
Loss in iteration 93 : 0.0020117317126805414
Loss in iteration 94 : 0.001989565245008775
Loss in iteration 95 : 0.0019679255902072257
Loss in iteration 96 : 0.0019467964054994348
Loss in iteration 97 : 0.001926161838375791
Loss in iteration 98 : 0.0019060065196658147
Loss in iteration 99 : 0.0018863155559925634
Loss in iteration 100 : 0.0018670745216425044
Loss in iteration 101 : 0.0018482694498945281
Loss in iteration 102 : 0.00182988682385904
Loss in iteration 103 : 0.0018119135668841718
Loss in iteration 104 : 0.0017943370325870223
Loss in iteration 105 : 0.0017771449945706394
Loss in iteration 106 : 0.001760325635885396
Loss in iteration 107 : 0.0017438675382927516
Loss in iteration 108 : 0.0017277596713862352
Loss in iteration 109 : 0.001711991381621878
Loss in iteration 110 : 0.0016965523813063732
Testing accuracy  of updater 3 on alg 0 with rate 4.0 = 0.992, training accuracy 0.9998571020291512, time elapsed: 2129 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.2846431509069894
Loss in iteration 3 : 0.29788743167203435
Loss in iteration 4 : 0.41510342413335716
Loss in iteration 5 : 0.15310430974494194
Loss in iteration 6 : 0.10490570533794194
Loss in iteration 7 : 0.07450031507352767
Loss in iteration 8 : 0.06708685874788775
Loss in iteration 9 : 0.062238340449377455
Loss in iteration 10 : 0.058297902635436306
Loss in iteration 11 : 0.05495938167069612
Loss in iteration 12 : 0.052073387933898
Loss in iteration 13 : 0.0495403106768309
Loss in iteration 14 : 0.04729011388468741
Loss in iteration 15 : 0.04527176721078388
Loss in iteration 16 : 0.04344692997430165
Loss in iteration 17 : 0.04178603199592027
Loss in iteration 18 : 0.040265755904783526
Loss in iteration 19 : 0.03886736625587851
Loss in iteration 20 : 0.0375755669284125
Loss in iteration 21 : 0.03637769823919521
Loss in iteration 22 : 0.035263158790947456
Loss in iteration 23 : 0.03422297993752367
Loss in iteration 24 : 0.03324950641115238
Loss in iteration 25 : 0.0323361524327275
Loss in iteration 26 : 0.031477212569827884
Loss in iteration 27 : 0.030667713024323234
Loss in iteration 28 : 0.029903293265521314
Loss in iteration 29 : 0.029180110776940823
Loss in iteration 30 : 0.028494763643418818
Loss in iteration 31 : 0.027844227074607666
Loss in iteration 32 : 0.027225800934286407
Loss in iteration 33 : 0.026637066047510302
Loss in iteration 34 : 0.026075847572009273
Loss in iteration 35 : 0.02554018410179319
Loss in iteration 36 : 0.02502830145737396
Loss in iteration 37 : 0.024538590334475058
Loss in iteration 38 : 0.02406958714989838
Loss in iteration 39 : 0.023619957552375365
Loss in iteration 40 : 0.023188482167128415
Loss in iteration 41 : 0.02277404422233731
Loss in iteration 42 : 0.02237561876877315
Loss in iteration 43 : 0.02199226325426913
Loss in iteration 44 : 0.021623109255253417
Loss in iteration 45 : 0.021267355200399733
Loss in iteration 46 : 0.020924259948183553
Loss in iteration 47 : 0.0205931371020175
Loss in iteration 48 : 0.02027334996464773
Loss in iteration 49 : 0.019964307048384658
Loss in iteration 50 : 0.019665458070110196
Loss in iteration 51 : 0.01937629037032296
Loss in iteration 52 : 0.01909632570412624
Loss in iteration 53 : 0.018825117359333052
Loss in iteration 54 : 0.018562247562997843
Loss in iteration 55 : 0.018307325142880818
Loss in iteration 56 : 0.018059983414769127
Loss in iteration 57 : 0.017819878270343724
Loss in iteration 58 : 0.017586686443502805
Loss in iteration 59 : 0.01736010393581598
Loss in iteration 60 : 0.0171398445841585
Loss in iteration 61 : 0.01692563875562768
Loss in iteration 62 : 0.01671723215661491
Loss in iteration 63 : 0.01651438474444463
Loss in iteration 64 : 0.016316869731328128
Loss in iteration 65 : 0.016124472671543516
Loss in iteration 66 : 0.01593699062377034
Loss in iteration 67 : 0.015754231381395603
Loss in iteration 68 : 0.015576012764388731
Loss in iteration 69 : 0.015402161967027986
Loss in iteration 70 : 0.015232514956363723
Loss in iteration 71 : 0.01506691591683646
Loss in iteration 72 : 0.014905216736937815
Loss in iteration 73 : 0.014747276534218607
Loss in iteration 74 : 0.014592961215317462
Loss in iteration 75 : 0.014442143068011889
Loss in iteration 76 : 0.014294700382584517
Loss in iteration 77 : 0.01415051710005816
Loss in iteration 78 : 0.014009482485085125
Loss in iteration 79 : 0.013871490821483555
Loss in iteration 80 : 0.013736441128599706
Loss in iteration 81 : 0.013604236896841182
Loss in iteration 82 : 0.01347478584087595
Loss in iteration 83 : 0.013347999669126212
Loss in iteration 84 : 0.01322379386830714
Loss in iteration 85 : 0.013102087501869763
Loss in iteration 86 : 0.012982803021305249
Loss in iteration 87 : 0.012865866089357062
Loss in iteration 88 : 0.012751205414268097
Loss in iteration 89 : 0.012638752594261737
Loss in iteration 90 : 0.012528441971523295
Loss in iteration 91 : 0.012420210495006842
Loss in iteration 92 : 0.012313997591447792
Loss in iteration 93 : 0.012209745044010656
Loss in iteration 94 : 0.012107396878046978
Loss in iteration 95 : 0.012006899253478745
Loss in iteration 96 : 0.011908200363361258
Loss in iteration 97 : 0.011811250338212573
Loss in iteration 98 : 0.011716001155728491
Loss in iteration 99 : 0.011622406555531175
Loss in iteration 100 : 0.011530421958624198
Loss in iteration 101 : 0.011440004391253053
Loss in iteration 102 : 0.011351112412890278
Loss in iteration 103 : 0.011263706048085579
Loss in iteration 104 : 0.011177746721939996
Loss in iteration 105 : 0.011093197198979828
Loss in iteration 106 : 0.011010021525222121
Loss in iteration 107 : 0.010928184973237996
Loss in iteration 108 : 0.010847653990033481
Loss in iteration 109 : 0.010768396147579549
Loss in iteration 110 : 0.010690380095835271
Loss in iteration 111 : 0.010613575518117523
Loss in iteration 112 : 0.01053795308868125
Loss in iteration 113 : 0.010463484432382725
Loss in iteration 114 : 0.010390142086307012
Loss in iteration 115 : 0.010317899463248391
Loss in iteration 116 : 0.010246730816939676
Loss in iteration 117 : 0.010176611208932889
Loss in iteration 118 : 0.010107516477040277
Loss in iteration 119 : 0.01003942320525005
Loss in iteration 120 : 0.00997230869503675
Loss in iteration 121 : 0.009906150937990901
Loss in iteration 122 : 0.009840928589697756
Loss in iteration 123 : 0.009776620944798278
Loss in iteration 124 : 0.009713207913170648
Loss in iteration 125 : 0.009650669997173367
Loss in iteration 126 : 0.009588988269895153
Loss in iteration 127 : 0.00952814435435959
Loss in iteration 128 : 0.009468120403636076
Loss in iteration 129 : 0.009408899081810803
Loss in iteration 130 : 0.009350463545774729
Loss in iteration 131 : 0.009292797427787559
Loss in iteration 132 : 0.009235884818779619
Loss in iteration 133 : 0.009179710252354669
Loss in iteration 134 : 0.009124258689460117
Loss in iteration 135 : 0.00906951550369177
Loss in iteration 136 : 0.009015466467202717
Loss in iteration 137 : 0.008962097737187449
Loss in iteration 138 : 0.008909395842913928
Loss in iteration 139 : 0.008857347673277722
Loss in iteration 140 : 0.008805940464853595
Loss in iteration 141 : 0.00875516179042184
Loss in iteration 142 : 0.008704999547946866
Loss in iteration 143 : 0.008655441949987708
Loss in iteration 144 : 0.008606477513520623
Loss in iteration 145 : 0.008558095050155071
Loss in iteration 146 : 0.00851028365672542
Loss in iteration 147 : 0.008463032706241666
Loss in iteration 148 : 0.008416331839183026
Loss in iteration 149 : 0.008370170955119527
Loss in iteration 150 : 0.008324540204647068
Loss in iteration 151 : 0.00827942998162232
Loss in iteration 152 : 0.00823483091568452
Loss in iteration 153 : 0.00819073386505177
Loss in iteration 154 : 0.008147129909580275
Loss in iteration 155 : 0.008104010344075084
Loss in iteration 156 : 0.008061366671842034
Loss in iteration 157 : 0.008019190598470454
Loss in iteration 158 : 0.00797747402583738
Loss in iteration 159 : 0.007936209046323663
Loss in iteration 160 : 0.00789538793723378
Loss in iteration 161 : 0.007855003155410482
Loss in iteration 162 : 0.007815047332036681
Loss in iteration 163 : 0.007775513267616973
Loss in iteration 164 : 0.007736393927131378
Loss in iteration 165 : 0.007697682435354782
Loss in iteration 166 : 0.0076593720723349586
Loss in iteration 167 : 0.0076214562690235
Loss in iteration 168 : 0.0075839286030532265
Loss in iteration 169 : 0.00754678279465655
Loss in iteration 170 : 0.007510012702719215
Loss in iteration 171 : 0.0074736123209643065
Loss in iteration 172 : 0.007437575774261422
Loss in iteration 173 : 0.007401897315056291
Loss in iteration 174 : 0.007366571319916238
Loss in iteration 175 : 0.007331592286187077
Loss in iteration 176 : 0.007296954828757439
Loss in iteration 177 : 0.007262653676926241
Loss in iteration 178 : 0.0072286836713696895
Loss in iteration 179 : 0.007195039761204034
Loss in iteration 180 : 0.007161717001140614
Loss in iteration 181 : 0.00712871054872982
Loss in iteration 182 : 0.0070960156616906005
Loss in iteration 183 : 0.0070636276953227
Loss in iteration 184 : 0.007031542099998383
Loss in iteration 185 : 0.006999754418730928
Loss in iteration 186 : 0.0069682602848172
Loss in iteration 187 : 0.0069370554195515655
Loss in iteration 188 : 0.0069061356300087265
Loss in iteration 189 : 0.006875496806892972
Loss in iteration 190 : 0.006845134922451762
Loss in iteration 191 : 0.006815046028450954
Loss in iteration 192 : 0.006785226254210166
Loss in iteration 193 : 0.006755671804695623
Loss in iteration 194 : 0.006726378958668845
Loss in iteration 195 : 0.0066973440668893925
Loss in iteration 196 : 0.00666856355036938
Loss in iteration 197 : 0.006640033898678506
Loss in iteration 198 : 0.0066117516682976316
Loss in iteration 199 : 0.006583713481019424
Loss in iteration 200 : 0.006555916022394348
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.9982222222222222, training accuracy 0.9995713060874536, time elapsed: 4706 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.3084051832474993
Loss in iteration 3 : 0.2616960100052396
Loss in iteration 4 : 0.2318428676098275
Loss in iteration 5 : 0.16958044827965574
Loss in iteration 6 : 0.14165657275064314
Loss in iteration 7 : 0.12268036489804178
Loss in iteration 8 : 0.1116080492196357
Loss in iteration 9 : 0.10308564174173576
Loss in iteration 10 : 0.09599647529372736
Loss in iteration 11 : 0.08994046518704421
Loss in iteration 12 : 0.08469599977771884
Loss in iteration 13 : 0.08010435098523641
Loss in iteration 14 : 0.07604635316047778
Loss in iteration 15 : 0.0724306025937193
Loss in iteration 16 : 0.06918576236639852
Loss in iteration 17 : 0.06625530108717093
Loss in iteration 18 : 0.06359381129220151
Loss in iteration 19 : 0.06116437911559726
Loss in iteration 20 : 0.05893667001779452
Loss in iteration 21 : 0.056885512394619926
Loss in iteration 22 : 0.05498983382748564
Loss in iteration 23 : 0.053231851309657706
Loss in iteration 24 : 0.051596447182549764
Loss in iteration 25 : 0.05007068275505691
Loss in iteration 26 : 0.0486434152993637
Loss in iteration 27 : 0.0473049935729119
Loss in iteration 28 : 0.046047013633170174
Loss in iteration 29 : 0.04486212140719708
Loss in iteration 30 : 0.04374385185312647
Loss in iteration 31 : 0.04268649700608953
Loss in iteration 32 : 0.04168499700739291
Loss in iteration 33 : 0.04073484955846142
Loss in iteration 34 : 0.03983203424886568
Loss in iteration 35 : 0.03897294897112652
Loss in iteration 36 : 0.03815435621816334
Loss in iteration 37 : 0.03737333750836697
Loss in iteration 38 : 0.03662725453176681
Loss in iteration 39 : 0.03591371588311535
Loss in iteration 40 : 0.03523054846200651
Loss in iteration 41 : 0.03457577278982682
Loss in iteration 42 : 0.03394758162852725
Loss in iteration 43 : 0.03334432139451666
Loss in iteration 44 : 0.032764475948247634
Loss in iteration 45 : 0.03220665241073548
Loss in iteration 46 : 0.031669568715773026
Loss in iteration 47 : 0.031152042653636976
Loss in iteration 48 : 0.03065298220072062
Loss in iteration 49 : 0.030171376961402045
Loss in iteration 50 : 0.029706290574862973
Loss in iteration 51 : 0.02925685396153644
Loss in iteration 52 : 0.028822259302194352
Loss in iteration 53 : 0.028401754658051715
Loss in iteration 54 : 0.027994639153181514
Loss in iteration 55 : 0.027600258651431935
Loss in iteration 56 : 0.027218001869265124
Loss in iteration 57 : 0.02684729687376675
Loss in iteration 58 : 0.026487607921747543
Loss in iteration 59 : 0.02613843260155496
Loss in iteration 60 : 0.025799299244090718
Loss in iteration 61 : 0.02546976457372302
Loss in iteration 62 : 0.025149411573387792
Loss in iteration 63 : 0.024837847541288562
Loss in iteration 64 : 0.02453470231929975
Loss in iteration 65 : 0.02423962667551488
Loss in iteration 66 : 0.023952290825413128
Loss in iteration 67 : 0.02367238307788813
Loss in iteration 68 : 0.02339960859392886
Loss in iteration 69 : 0.023133688247093616
Loss in iteration 70 : 0.02287435757610486
Loss in iteration 71 : 0.022621365820933714
Loss in iteration 72 : 0.02237447503465868
Loss in iteration 73 : 0.022133459264192155
Loss in iteration 74 : 0.0218981037936817
Loss in iteration 75 : 0.02166820444502445
Loss in iteration 76 : 0.021443566930492697
Loss in iteration 77 : 0.02122400625296596
Loss in iteration 78 : 0.02100934614970635
Loss in iteration 79 : 0.02079941857600781
Loss in iteration 80 : 0.020594063225400666
Loss in iteration 81 : 0.020393127083406303
Loss in iteration 82 : 0.020196464012117613
Loss in iteration 83 : 0.02000393436313187
Loss in iteration 84 : 0.0198154046165881
Loss in iteration 85 : 0.019630747044263267
Loss in iteration 86 : 0.019449839394864412
Loss in iteration 87 : 0.019272564599816747
Loss in iteration 88 : 0.01909881049799645
Loss in iteration 89 : 0.018928469577990287
Loss in iteration 90 : 0.018761438736584316
Loss in iteration 91 : 0.0185976190522934
Loss in iteration 92 : 0.01843691557284302
Loss in iteration 93 : 0.018279237115602396
Loss in iteration 94 : 0.018124496080052413
Loss in iteration 95 : 0.017972608271442703
Loss in iteration 96 : 0.017823492734862296
Loss in iteration 97 : 0.017677071599007634
Loss in iteration 98 : 0.017533269928988866
Loss in iteration 99 : 0.01739201558756604
Loss in iteration 100 : 0.01725323910425272
Loss in iteration 101 : 0.01711687355176869
Loss in iteration 102 : 0.016982854429360855
Loss in iteration 103 : 0.016851119552548026
Loss in iteration 104 : 0.016721608948878065
Loss in iteration 105 : 0.016594264759315237
Loss in iteration 106 : 0.01646903114490418
Loss in iteration 107 : 0.016345854198381325
Loss in iteration 108 : 0.016224681860428598
Loss in iteration 109 : 0.016105463840285283
Loss in iteration 110 : 0.015988151540454125
Loss in iteration 111 : 0.01587269798525539
Loss in iteration 112 : 0.01575905775300023
Loss in iteration 113 : 0.015647186911569355
Loss in iteration 114 : 0.015537042957198174
Loss in iteration 115 : 0.015428584756282185
Loss in iteration 116 : 0.015321772490029341
Loss in iteration 117 : 0.015216567601796653
Loss in iteration 118 : 0.015112932746959804
Loss in iteration 119 : 0.01501083174517349
Loss in iteration 120 : 0.014910229534889671
Loss in iteration 121 : 0.014811092130009515
Loss in iteration 122 : 0.014713386578551877
Loss in iteration 123 : 0.014617080923229634
Loss in iteration 124 : 0.014522144163830648
Loss in iteration 125 : 0.014428546221307372
Loss in iteration 126 : 0.01433625790348461
Loss in iteration 127 : 0.01424525087230028
Loss in iteration 128 : 0.014155497612499597
Loss in iteration 129 : 0.014066971401707089
Loss in iteration 130 : 0.013979646281806155
Loss in iteration 131 : 0.013893497031559216
Loss in iteration 132 : 0.01380849914040603
Loss in iteration 133 : 0.01372462878338101
Loss in iteration 134 : 0.013641862797093502
Loss in iteration 135 : 0.013560178656719097
Loss in iteration 136 : 0.013479554453951903
Loss in iteration 137 : 0.013399968875870914
Loss in iteration 138 : 0.01332140118467673
Loss in iteration 139 : 0.013243831198256395
Loss in iteration 140 : 0.013167239271537167
Loss in iteration 141 : 0.013091606278591687
Loss in iteration 142 : 0.013016913595459294
Loss in iteration 143 : 0.012943143083650094
Loss in iteration 144 : 0.012870277074300183
Loss in iteration 145 : 0.012798298352947938
Loss in iteration 146 : 0.01272719014490286
Loss in iteration 147 : 0.01265693610118059
Loss in iteration 148 : 0.012587520284977805
Loss in iteration 149 : 0.012518927158663533
Loss in iteration 150 : 0.012451141571263197
Loss in iteration 151 : 0.01238414874641441
Loss in iteration 152 : 0.012317934270773016
Loss in iteration 153 : 0.012252484082850165
Loss in iteration 154 : 0.012187784462261736
Loss in iteration 155 : 0.012123822019372147
Loss in iteration 156 : 0.012060583685315828
Loss in iteration 157 : 0.011998056702380192
Loss in iteration 158 : 0.011936228614734914
Loss in iteration 159 : 0.011875087259492916
Loss in iteration 160 : 0.011814620758089178
Loss in iteration 161 : 0.01175481750796438
Loss in iteration 162 : 0.011695666174540466
Loss in iteration 163 : 0.011637155683476492
Loss in iteration 164 : 0.011579275213193208
Loss in iteration 165 : 0.01152201418765549
Loss in iteration 166 : 0.011465362269402195
Loss in iteration 167 : 0.011409309352813687
Loss in iteration 168 : 0.011353845557607437
Loss in iteration 169 : 0.011298961222552866
Loss in iteration 170 : 0.011244646899396462
Loss in iteration 171 : 0.011190893346989398
Loss in iteration 172 : 0.011137691525609424
Loss in iteration 173 : 0.011085032591469774
Loss in iteration 174 : 0.011032907891407784
Loss in iteration 175 : 0.010981308957746237
Loss in iteration 176 : 0.010930227503321253
Loss in iteration 177 : 0.01087965541667003
Loss in iteration 178 : 0.010829584757372607
Loss in iteration 179 : 0.010780007751542005
Loss in iteration 180 : 0.010730916787457013
Loss in iteration 181 : 0.010682304411332463
Loss in iteration 182 : 0.010634163323222093
Loss in iteration 183 : 0.010586486373048726
Loss in iteration 184 : 0.010539266556757692
Loss in iteration 185 : 0.010492497012588604
Loss in iteration 186 : 0.010446171017461389
Loss in iteration 187 : 0.010400281983472622
Loss in iteration 188 : 0.01035482345449793
Loss in iteration 189 : 0.010309789102897027
Loss in iteration 190 : 0.01026517272631757
Loss in iteration 191 : 0.010220968244594404
Loss in iteration 192 : 0.010177169696741007
Loss in iteration 193 : 0.01013377123802977
Loss in iteration 194 : 0.010090767137158061
Loss in iteration 195 : 0.010048151773497356
Loss in iteration 196 : 0.01000591963442221
Loss in iteration 197 : 0.00996406531271676
Loss in iteration 198 : 0.009922583504055919
Loss in iteration 199 : 0.009881469004558663
Loss in iteration 200 : 0.009840716708411334
Testing accuracy  of updater 3 on alg 0 with rate 0.7 = 0.9928888888888889, training accuracy 0.9994284081166047, time elapsed: 4833 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.412118935164681
Loss in iteration 3 : 0.3196112455319564
Loss in iteration 4 : 0.26913795130894985
Loss in iteration 5 : 0.23601082448325456
Loss in iteration 6 : 0.21207577432850352
Loss in iteration 7 : 0.19341044819178593
Loss in iteration 8 : 0.17826088892859712
Loss in iteration 9 : 0.16564925567039718
Loss in iteration 10 : 0.1549514046344481
Loss in iteration 11 : 0.1457410278721769
Loss in iteration 12 : 0.13771412880161923
Loss in iteration 13 : 0.1306467247791924
Loss in iteration 14 : 0.12436946053541441
Loss in iteration 15 : 0.11875154454908182
Loss in iteration 16 : 0.11369015600780351
Loss in iteration 17 : 0.10910322313511932
Loss in iteration 18 : 0.10492436300403249
Loss in iteration 19 : 0.10109925295271799
Loss in iteration 20 : 0.09758297639674797
Loss in iteration 21 : 0.09433804747251387
Loss in iteration 22 : 0.09133291825037212
Loss in iteration 23 : 0.08854083514445667
Loss in iteration 24 : 0.08593895202807374
Loss in iteration 25 : 0.08350763475294067
Loss in iteration 26 : 0.08122991022099883
Loss in iteration 27 : 0.07909102590464918
Loss in iteration 28 : 0.07707809466141344
Loss in iteration 29 : 0.0751798060659471
Loss in iteration 30 : 0.07338619008694826
Loss in iteration 31 : 0.07168842230243781
Loss in iteration 32 : 0.07007866233531519
Loss in iteration 33 : 0.0685499190501561
Loss in iteration 34 : 0.06709593745464694
Loss in iteration 35 : 0.06571110331665357
Loss in iteration 36 : 0.06439036232753081
Loss in iteration 37 : 0.06312915127652549
Loss in iteration 38 : 0.06192333919559605
Loss in iteration 39 : 0.0607691768221757
Loss in iteration 40 : 0.05966325303421192
Loss in iteration 41 : 0.058602457155806864
Loss in iteration 42 : 0.05758394622695897
Loss in iteration 43 : 0.056605116487933524
Loss in iteration 44 : 0.055663578455771426
Loss in iteration 45 : 0.054757135073671435
Loss in iteration 46 : 0.053883762498292805
Loss in iteration 47 : 0.05304159315920014
Loss in iteration 48 : 0.052228900781686904
Loss in iteration 49 : 0.051444087111398105
Loss in iteration 50 : 0.050685670118373535
Loss in iteration 51 : 0.049952273490837716
Loss in iteration 52 : 0.04924261725643651
Loss in iteration 53 : 0.04855550939162021
Loss in iteration 54 : 0.04788983829926267
Loss in iteration 55 : 0.047244566051003065
Loss in iteration 56 : 0.0466187223047063
Loss in iteration 57 : 0.04601139881928199
Loss in iteration 58 : 0.04542174449919821
Loss in iteration 59 : 0.0448489609096779
Loss in iteration 60 : 0.044292298210984675
Loss in iteration 61 : 0.04375105146658891
Loss in iteration 62 : 0.04322455728551179
Loss in iteration 63 : 0.04271219076390858
Loss in iteration 64 : 0.042213362695077125
Loss in iteration 65 : 0.04172751702066702
Loss in iteration 66 : 0.041254128498981336
Loss in iteration 67 : 0.04079270056899268
Loss in iteration 68 : 0.040342763391071
Loss in iteration 69 : 0.03990387204751178
Loss in iteration 70 : 0.039475604887782116
Loss in iteration 71 : 0.039057562005013625
Loss in iteration 72 : 0.03864936383169059
Loss in iteration 73 : 0.03825064984373539
Loss in iteration 74 : 0.03786107736329926
Loss in iteration 75 : 0.03748032045154973
Loss in iteration 76 : 0.03710806888361492
Loss in iteration 77 : 0.036744027198620556
Loss in iteration 78 : 0.03638791381844155
Loss in iteration 79 : 0.036039460229406496
Loss in iteration 80 : 0.03569841022173881
Loss in iteration 81 : 0.03536451918201064
Loss in iteration 82 : 0.03503755343432184
Loss in iteration 83 : 0.03471728962631108
Loss in iteration 84 : 0.03440351415645853
Loss in iteration 85 : 0.03409602263945688
Loss in iteration 86 : 0.03379461940671264
Loss in iteration 87 : 0.03349911703929804
Loss in iteration 88 : 0.03320933593090416
Loss in iteration 89 : 0.0329251038785579
Loss in iteration 90 : 0.03264625569905326
Loss in iteration 91 : 0.0323726328692198
Loss in iteration 92 : 0.03210408318830677
Loss in iteration 93 : 0.03184046046090339
Loss in iteration 94 : 0.0315816241989416
Loss in iteration 95 : 0.03132743934144801
Loss in iteration 96 : 0.03107777599081432
Loss in iteration 97 : 0.030832509164454795
Loss in iteration 98 : 0.030591518560806066
Loss in iteration 99 : 0.030354688338706068
Loss in iteration 100 : 0.030121906909261843
Loss in iteration 101 : 0.02989306673938416
Loss in iteration 102 : 0.02966806416622685
Loss in iteration 103 : 0.02944679922182758
Loss in iteration 104 : 0.029229175467296423
Loss in iteration 105 : 0.029015099835947213
Loss in iteration 106 : 0.02880448248481033
Loss in iteration 107 : 0.02859723665400524
Loss in iteration 108 : 0.02839327853348836
Loss in iteration 109 : 0.02819252713672547
Loss in iteration 110 : 0.02799490418087024
Loss in iteration 111 : 0.02780033397305713
Loss in iteration 112 : 0.027608743302447312
Loss in iteration 113 : 0.027420061337685684
Loss in iteration 114 : 0.02723421952945531
Loss in iteration 115 : 0.02705115151783278
Loss in iteration 116 : 0.026870793044168888
Loss in iteration 117 : 0.02669308186723698
Loss in iteration 118 : 0.026517957683408505
Loss in iteration 119 : 0.026345362050629456
Loss in iteration 120 : 0.026175238315987153
Loss in iteration 121 : 0.026007531546669658
Loss in iteration 122 : 0.025842188464132415
Loss in iteration 123 : 0.025679157381298112
Loss in iteration 124 : 0.025518388142627052
Loss in iteration 125 : 0.025359832066904935
Loss in iteration 126 : 0.025203441892604154
Loss in iteration 127 : 0.025049171725683512
Loss in iteration 128 : 0.024896976989699768
Loss in iteration 129 : 0.024746814378110975
Loss in iteration 130 : 0.024598641808659658
Loss in iteration 131 : 0.024452418379729713
Loss in iteration 132 : 0.02430810432857765
Loss in iteration 133 : 0.024165660991343825
Loss in iteration 134 : 0.02402505076475558
Loss in iteration 135 : 0.023886237069438033
Loss in iteration 136 : 0.023749184314754812
Loss in iteration 137 : 0.02361385786510295
Loss in iteration 138 : 0.023480224007592504
Loss in iteration 139 : 0.023348249921044794
Loss in iteration 140 : 0.023217903646245163
Loss in iteration 141 : 0.023089154057392324
Loss in iteration 142 : 0.022961970834687314
Loss in iteration 143 : 0.022836324438009193
Loss in iteration 144 : 0.022712186081626916
Loss in iteration 145 : 0.022589527709900005
Loss in iteration 146 : 0.022468321973922924
Loss in iteration 147 : 0.02234854220906973
Loss in iteration 148 : 0.022230162413399302
Loss in iteration 149 : 0.022113157226882108
Loss in iteration 150 : 0.02199750191141175
Loss in iteration 151 : 0.021883172331567436
Loss in iteration 152 : 0.02177014493609372
Loss in iteration 153 : 0.02165839674006642
Loss in iteration 154 : 0.021547905307715214
Loss in iteration 155 : 0.02143864873587432
Loss in iteration 156 : 0.021330605638034933
Loss in iteration 157 : 0.021223755128973
Loss in iteration 158 : 0.021118076809929165
Loss in iteration 159 : 0.021013550754316657
Loss in iteration 160 : 0.0209101574939357
Loss in iteration 161 : 0.020807878005673595
Loss in iteration 162 : 0.02070669369866988
Loss in iteration 163 : 0.02060658640192815
Loss in iteration 164 : 0.020507538352355956
Loss in iteration 165 : 0.02040953218321582
Loss in iteration 166 : 0.020312550912970354
Loss in iteration 167 : 0.020216577934506492
Loss in iteration 168 : 0.020121597004722956
Loss in iteration 169 : 0.02002759223446726
Loss in iteration 170 : 0.019934548078808182
Loss in iteration 171 : 0.019842449327630858
Loss in iteration 172 : 0.01975128109654169
Loss in iteration 173 : 0.019661028818071493
Loss in iteration 174 : 0.019571678233165014
Loss in iteration 175 : 0.01948321538294653
Loss in iteration 176 : 0.019395626600750216
Loss in iteration 177 : 0.019308898504406233
Loss in iteration 178 : 0.019223017988772198
Loss in iteration 179 : 0.0191379722185013
Loss in iteration 180 : 0.01905374862103818
Loss in iteration 181 : 0.01897033487983422
Loss in iteration 182 : 0.01888771892777411
Loss in iteration 183 : 0.018805888940806207
Loss in iteration 184 : 0.018724833331768855
Loss in iteration 185 : 0.018644540744406326
Loss in iteration 186 : 0.01856500004756668
Loss in iteration 187 : 0.018486200329576017
Loss in iteration 188 : 0.01840813089278191
Loss in iteration 189 : 0.01833078124826083
Loss in iteration 190 : 0.018254141110683447
Loss in iteration 191 : 0.018178200393332254
Loss in iteration 192 : 0.018102949203266307
Loss in iteration 193 : 0.01802837783662822
Loss in iteration 194 : 0.017954476774087954
Loss in iteration 195 : 0.01788123667641937
Loss in iteration 196 : 0.017808648380204593
Loss in iteration 197 : 0.0177367028936621
Loss in iteration 198 : 0.017665391392594074
Loss in iteration 199 : 0.01759470521644953
Loss in iteration 200 : 0.0175246358644987
Testing accuracy  of updater 3 on alg 0 with rate 0.4 = 0.9875555555555555, training accuracy 0.999285510145756, time elapsed: 4518 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6084270461202975
Loss in iteration 3 : 0.5460630743998026
Loss in iteration 4 : 0.4979545758281866
Loss in iteration 5 : 0.45971693165827776
Loss in iteration 6 : 0.4285895864951363
Loss in iteration 7 : 0.40272712584991555
Loss in iteration 8 : 0.3808550351801402
Loss in iteration 9 : 0.3620708561436443
Loss in iteration 10 : 0.34572143361818813
Loss in iteration 11 : 0.3313245522348525
Loss in iteration 12 : 0.3185176644842933
Loss in iteration 13 : 0.30702361227717445
Loss in iteration 14 : 0.29662724939728613
Loss in iteration 15 : 0.28715920116157523
Loss in iteration 16 : 0.27848438733959757
Loss in iteration 17 : 0.27049378145641273
Loss in iteration 18 : 0.26309840613299196
Loss in iteration 19 : 0.25622489757703304
Loss in iteration 20 : 0.24981218731717156
Loss in iteration 21 : 0.24380899023221866
Loss in iteration 22 : 0.2381718818368374
Loss in iteration 23 : 0.23286381129543232
Loss in iteration 24 : 0.22785294019687588
Loss in iteration 25 : 0.22311172739780602
Loss in iteration 26 : 0.21861620154615122
Loss in iteration 27 : 0.2143453780630271
Loss in iteration 28 : 0.21028078827700755
Loss in iteration 29 : 0.2064060963424809
Loss in iteration 30 : 0.20270678540207715
Loss in iteration 31 : 0.19916989877185456
Loss in iteration 32 : 0.19578382515583684
Loss in iteration 33 : 0.19253811932893772
Loss in iteration 34 : 0.18942335157460596
Loss in iteration 35 : 0.18643098057686097
Loss in iteration 36 : 0.18355324555529778
Loss in iteration 37 : 0.18078307427631327
Loss in iteration 38 : 0.17811400423316798
Loss in iteration 39 : 0.17554011480544573
Loss in iteration 40 : 0.17305596861769246
Loss in iteration 41 : 0.17065656064219134
Loss in iteration 42 : 0.1683372738506363
Loss in iteration 43 : 0.16609384042811162
Loss in iteration 44 : 0.16392230773122204
Loss in iteration 45 : 0.16181900830883963
Loss in iteration 46 : 0.15978053341527693
Loss in iteration 47 : 0.15780370953685724
Loss in iteration 48 : 0.1558855775278149
Loss in iteration 49 : 0.1540233740133578
Loss in iteration 50 : 0.15221451476905606
Loss in iteration 51 : 0.15045657982846147
Loss in iteration 52 : 0.14874730010656795
Loss in iteration 53 : 0.14708454535668944
Loss in iteration 54 : 0.14546631330355397
Loss in iteration 55 : 0.14389071981672064
Loss in iteration 56 : 0.142355990006497
Loss in iteration 57 : 0.14086045013989093
Loss in iteration 58 : 0.13940252028725203
Loss in iteration 59 : 0.13798070762146694
Loss in iteration 60 : 0.13659360030121426
Loss in iteration 61 : 0.13523986187807036
Loss in iteration 62 : 0.13391822617442178
Loss in iteration 63 : 0.13262749258533246
Loss in iteration 64 : 0.13136652176289973
Loss in iteration 65 : 0.13013423164630492
Loss in iteration 66 : 0.12892959380485772
Loss in iteration 67 : 0.1277516300648981
Loss in iteration 68 : 0.1265994093945474
Loss in iteration 69 : 0.12547204502305395
Loss in iteration 70 : 0.1243686917738978
Loss in iteration 71 : 0.12328854359294836
Loss in iteration 72 : 0.12223083125486214
Loss in iteration 73 : 0.12119482023257597
Loss in iteration 74 : 0.1201798087162378
Loss in iteration 75 : 0.11918512576922734
Loss in iteration 76 : 0.1182101296101054
Loss in iteration 77 : 0.11725420601036822
Loss in iteration 78 : 0.11631676679882749
Loss in iteration 79 : 0.11539724846426733
Loss in iteration 80 : 0.11449511084879009
Loss in iteration 81 : 0.11360983592492963
Loss in iteration 82 : 0.11274092665022517
Loss in iteration 83 : 0.11188790589348797
Loss in iteration 84 : 0.11105031542749505
Loss in iteration 85 : 0.11022771498328113
Loss in iteration 86 : 0.10941968136160875
Loss in iteration 87 : 0.10862580759755998
Loss in iteration 88 : 0.10784570217452058
Loss in iteration 89 : 0.10707898828413379
Loss in iteration 90 : 0.10632530312906818
Loss in iteration 91 : 0.1055842972656959
Loss in iteration 92 : 0.10485563398400251
Loss in iteration 93 : 0.10413898872225649
Loss in iteration 94 : 0.10343404851415319
Loss in iteration 95 : 0.10274051146632512
Loss in iteration 96 : 0.10205808626425981
Loss in iteration 97 : 0.10138649170481882
Loss in iteration 98 : 0.10072545625368068
Loss in iteration 99 : 0.10007471762614704
Loss in iteration 100 : 0.0994340223898731
Loss in iteration 101 : 0.09880312558817612
Loss in iteration 102 : 0.09818179038267381
Loss in iteration 103 : 0.09756978771409464
Loss in iteration 104 : 0.09696689598017402
Loss in iteration 105 : 0.09637290072963095
Loss in iteration 106 : 0.09578759437128734
Loss in iteration 107 : 0.09521077589744968
Loss in iteration 108 : 0.0946422506207365
Loss in iteration 109 : 0.09408182992358764
Loss in iteration 110 : 0.09352933101973868
Loss in iteration 111 : 0.0929845767269917
Loss in iteration 112 : 0.09244739525066004
Loss in iteration 113 : 0.09191761997709426
Loss in iteration 114 : 0.09139508927674714
Loss in iteration 115 : 0.09087964631625658
Loss in iteration 116 : 0.09037113887906786
Loss in iteration 117 : 0.08986941919413761
Loss in iteration 118 : 0.08937434377229689
Loss in iteration 119 : 0.08888577324987117
Loss in iteration 120 : 0.08840357223918092
Loss in iteration 121 : 0.08792760918556906
Loss in iteration 122 : 0.08745775623062321
Loss in iteration 123 : 0.0869938890812762
Loss in iteration 124 : 0.08653588688449261
Loss in iteration 125 : 0.08608363210726074
Loss in iteration 126 : 0.08563701042162745
Loss in iteration 127 : 0.08519591059452887
Loss in iteration 128 : 0.08476022438218359
Loss in iteration 129 : 0.08432984642882496
Loss in iteration 130 : 0.08390467416956704
Loss in iteration 131 : 0.08348460773720658
Loss in iteration 132 : 0.08306954987277146
Loss in iteration 133 : 0.0826594058396433
Loss in iteration 134 : 0.0822540833410868
Loss in iteration 135 : 0.08185349244102481
Loss in iteration 136 : 0.08145754548791415
Loss in iteration 137 : 0.08106615704157609
Loss in iteration 138 : 0.08067924380285225
Loss in iteration 139 : 0.0802967245459524
Loss in iteration 140 : 0.07991852005338038
Loss in iteration 141 : 0.07954455305331787
Loss in iteration 142 : 0.07917474815936094
Loss in iteration 143 : 0.07880903181250375
Loss in iteration 144 : 0.0784473322252735
Loss in iteration 145 : 0.07808957932792267
Loss in iteration 146 : 0.07773570471658897
Loss in iteration 147 : 0.07738564160333998
Loss in iteration 148 : 0.07703932476802301
Loss in iteration 149 : 0.07669669051184236
Loss in iteration 150 : 0.0763576766125932
Loss in iteration 151 : 0.07602222228148076
Loss in iteration 152 : 0.07569026812146239
Loss in iteration 153 : 0.07536175608704695
Loss in iteration 154 : 0.07503662944549302
Loss in iteration 155 : 0.07471483273934973
Loss in iteration 156 : 0.07439631175028417
Loss in iteration 157 : 0.07408101346414621
Loss in iteration 158 : 0.07376888603721836
Loss in iteration 159 : 0.0734598787636065
Loss in iteration 160 : 0.07315394204372458
Loss in iteration 161 : 0.07285102735383053
Loss in iteration 162 : 0.07255108721657434
Loss in iteration 163 : 0.07225407517251564
Loss in iteration 164 : 0.07195994575257646
Loss in iteration 165 : 0.07166865445139275
Loss in iteration 166 : 0.07138015770152811
Loss in iteration 167 : 0.07109441284852071
Loss in iteration 168 : 0.0708113781267293
Loss in iteration 169 : 0.07053101263594871
Loss in iteration 170 : 0.07025327631876814
Loss in iteration 171 : 0.06997812993864194
Loss in iteration 172 : 0.06970553505864822
Loss in iteration 173 : 0.06943545402091114
Loss in iteration 174 : 0.06916784992665877
Loss in iteration 175 : 0.06890268661689937
Loss in iteration 176 : 0.06863992865368776
Loss in iteration 177 : 0.06837954130196516
Loss in iteration 178 : 0.0681214905119492
Loss in iteration 179 : 0.06786574290205694
Loss in iteration 180 : 0.06761226574233882
Loss in iteration 181 : 0.06736102693841044
Loss in iteration 182 : 0.06711199501585924
Loss in iteration 183 : 0.06686513910511421
Loss in iteration 184 : 0.0666204289267593
Loss in iteration 185 : 0.06637783477727836
Loss in iteration 186 : 0.06613732751521355
Loss in iteration 187 : 0.06589887854772572
Loss in iteration 188 : 0.06566245981754235
Loss in iteration 189 : 0.06542804379027915
Loss in iteration 190 : 0.06519560344212419
Loss in iteration 191 : 0.06496511224787119
Loss in iteration 192 : 0.06473654416929174
Loss in iteration 193 : 0.06450987364383401
Loss in iteration 194 : 0.06428507557363741
Loss in iteration 195 : 0.06406212531485389
Loss in iteration 196 : 0.06384099866726498
Loss in iteration 197 : 0.06362167186418434
Loss in iteration 198 : 0.0634041215626388
Loss in iteration 199 : 0.06318832483381699
Loss in iteration 200 : 0.0629742591537772
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.9733333333333334, training accuracy 0.9925693055158616, time elapsed: 5078 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224037
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632758
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439016
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043774
Loss in iteration 15 : 0.6751379721466285
Loss in iteration 16 : 0.6738340200990942
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694718
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873453
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050735
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.654327743708557
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663436
Loss in iteration 37 : 0.6465682744294412
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538787
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680166
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889296
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056247
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658474
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426644
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535707
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033768
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.5832800218249878
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708514
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261589
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677672
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153798
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211118
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190974
Loss in iteration 131 : 0.5306307670254145
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961832
Loss in iteration 134 : 0.5271766840349438
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.5237415434680152
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056484
Loss in iteration 146 : 0.5135520100559262
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333071
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.49914911381687194
Loss in iteration 160 : 0.4980569551084796
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.4947941542526308
Loss in iteration 164 : 0.49371112713244425
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.49155195800730855
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.48833047684132436
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.48619438167066215
Loss in iteration 172 : 0.4851298124112099
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967204
Loss in iteration 175 : 0.48195005758540466
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.476697156505787
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.47253704845175315
Loss in iteration 185 : 0.4715029036334387
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.46636756901018045
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465956
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.4612913582360449
Loss in iteration 196 : 0.4602832245926189
Loss in iteration 197 : 0.45927746265434266
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890788
Loss in iteration 200 : 0.4562744140691076
Testing accuracy  of updater 4 on alg 0 with rate 100.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4432 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224037
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632759
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439014
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043774
Loss in iteration 15 : 0.6751379721466285
Loss in iteration 16 : 0.6738340200990941
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694719
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873453
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050736
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534878
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663437
Loss in iteration 37 : 0.6465682744294412
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680166
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889296
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153971
Loss in iteration 55 : 0.6234700115106355
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658474
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426643
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708373
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033768
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213936
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088277
Loss in iteration 87 : 0.583280021824988
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960756
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708515
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879716
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677673
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190326
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211118
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846017
Loss in iteration 122 : 0.5411041561722849
Loss in iteration 123 : 0.5399323592268604
Loss in iteration 124 : 0.5387625598561839
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723462
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190975
Loss in iteration 131 : 0.5306307670254145
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961832
Loss in iteration 134 : 0.5271766840349436
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.5237415434680152
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609136
Loss in iteration 145 : 0.5146754836056485
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.505749596933307
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303146
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.50024354407835
Loss in iteration 159 : 0.49914911381687194
Loss in iteration 160 : 0.4980569551084795
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.4947941542526308
Loss in iteration 164 : 0.49371112713244425
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.4915519580073085
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.4883304768413243
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.4861943816706622
Loss in iteration 172 : 0.48512981241120995
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.48195005758540466
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.4766971565057869
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.47357354825509423
Loss in iteration 184 : 0.4725370484517532
Loss in iteration 185 : 0.47150290363343866
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.46841461808560453
Loss in iteration 189 : 0.4673899116905835
Loss in iteration 190 : 0.46636756901018045
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.46129135823604484
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.4592774626543427
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.4562744140691076
Testing accuracy  of updater 4 on alg 0 with rate 70.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 3944 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224037
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632758
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439014
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043772
Loss in iteration 15 : 0.6751379721466284
Loss in iteration 16 : 0.6738340200990942
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694719
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873453
Loss in iteration 25 : 0.6621138616377291
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050736
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534878
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663436
Loss in iteration 37 : 0.6465682744294412
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260154
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680166
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889295
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658473
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426643
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114671
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033768
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098036
Loss in iteration 74 : 0.5994493156213936
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088277
Loss in iteration 87 : 0.583280021824988
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.5710135726721919
Loss in iteration 98 : 0.5697958068708514
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677673
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623554
Loss in iteration 111 : 0.5541225442190326
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211117
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.537594768690609
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723462
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190974
Loss in iteration 131 : 0.5306307670254145
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961833
Loss in iteration 134 : 0.5271766840349437
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035022
Loss in iteration 137 : 0.5237415434680152
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507401
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056485
Loss in iteration 146 : 0.5135520100559262
Loss in iteration 147 : 0.5124307344248353
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333072
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.49914911381687194
Loss in iteration 160 : 0.4980569551084795
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.49479415425263085
Loss in iteration 164 : 0.4937111271324443
Loss in iteration 165 : 0.4926303936870237
Loss in iteration 166 : 0.49155195800730855
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.48833047684132436
Loss in iteration 170 : 0.4872612709605178
Loss in iteration 171 : 0.48619438167066215
Loss in iteration 172 : 0.48512981241120995
Loss in iteration 173 : 0.4840675665132492
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.48195005758540466
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.4798418793763408
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.476697156505787
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.4746124009886095
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.47253704845175315
Loss in iteration 185 : 0.47150290363343866
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905835
Loss in iteration 190 : 0.4663675690101804
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.4633147371593365
Loss in iteration 194 : 0.4623018627393149
Loss in iteration 195 : 0.4612913582360449
Loss in iteration 196 : 0.4602832245926189
Loss in iteration 197 : 0.4592774626543426
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.45727305678907876
Loss in iteration 200 : 0.45627441406910757
Testing accuracy  of updater 4 on alg 0 with rate 40.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 3904 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224037
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632759
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439016
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043772
Loss in iteration 15 : 0.6751379721466285
Loss in iteration 16 : 0.6738340200990941
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057905
Loss in iteration 19 : 0.6699232506694718
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053065
Loss in iteration 24 : 0.6634139578873454
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050735
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085573
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663437
Loss in iteration 37 : 0.6465682744294413
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260154
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680169
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889295
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658474
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426643
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033768
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.5869900323068191
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.5832800218249878
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848324
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.5710135726721919
Loss in iteration 98 : 0.5697958068708515
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677672
Loss in iteration 109 : 0.556514135314575
Loss in iteration 110 : 0.5553174181623554
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211118
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190975
Loss in iteration 131 : 0.5306307670254145
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961833
Loss in iteration 134 : 0.5271766840349438
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.5237415434680152
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056485
Loss in iteration 146 : 0.5135520100559262
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049848
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333071
Loss in iteration 154 : 0.5046438831894594
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.4991491138168719
Loss in iteration 160 : 0.4980569551084795
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.49479415425263085
Loss in iteration 164 : 0.4937111271324443
Loss in iteration 165 : 0.4926303936870238
Loss in iteration 166 : 0.49155195800730855
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.4883304768413243
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.4861943816706622
Loss in iteration 172 : 0.48512981241120995
Loss in iteration 173 : 0.4840675665132492
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.48195005758540466
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.4787912964742022
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.4766971565057869
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.4746124009886094
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.4725370484517532
Loss in iteration 185 : 0.47150290363343866
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536213
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.46636756901018045
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.4633147371593364
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.46129135823604484
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.45927746265434266
Loss in iteration 198 : 0.4582740731693264
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.4562744140691076
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4052 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224036
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257244
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632759
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439016
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043772
Loss in iteration 15 : 0.6751379721466284
Loss in iteration 16 : 0.6738340200990941
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694719
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873456
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050736
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663435
Loss in iteration 37 : 0.6465682744294412
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680169
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889295
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658473
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426643
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708373
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739225
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033768
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182817
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.583280021824988
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615477
Loss in iteration 90 : 0.5795835666695709
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960756
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708514
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677672
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211117
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.54581110087663
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723462
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190974
Loss in iteration 131 : 0.5306307670254143
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961833
Loss in iteration 134 : 0.5271766840349438
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.523741543468015
Loss in iteration 138 : 0.5226007462549447
Loss in iteration 139 : 0.5214620876507401
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056485
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049848
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.505749596933307
Loss in iteration 154 : 0.5046438831894594
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.49914911381687194
Loss in iteration 160 : 0.4980569551084795
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.49479415425263085
Loss in iteration 164 : 0.49371112713244425
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.49155195800730855
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.4883304768413243
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.48619438167066215
Loss in iteration 172 : 0.48512981241120995
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.48195005758540466
Loss in iteration 176 : 0.4808948006776775
Loss in iteration 177 : 0.4798418793763408
Loss in iteration 178 : 0.4787912964742022
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.47669715650578703
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.47253704845175315
Loss in iteration 185 : 0.4715029036334388
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.4663675690101804
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.4612913582360449
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.45927746265434266
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.4562744140691076
Testing accuracy  of updater 4 on alg 0 with rate 7.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4017 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224036
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632759
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439016
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043772
Loss in iteration 15 : 0.6751379721466285
Loss in iteration 16 : 0.6738340200990942
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694719
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873454
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050735
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534878
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663435
Loss in iteration 37 : 0.6465682744294412
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680166
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889295
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658473
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426643
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708373
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033768
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088277
Loss in iteration 87 : 0.583280021824988
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708514
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879716
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677673
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231537
Loss in iteration 114 : 0.5505491101153798
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211117
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.537594768690609
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723462
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190975
Loss in iteration 131 : 0.5306307670254145
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961833
Loss in iteration 134 : 0.5271766840349437
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.5237415434680152
Loss in iteration 138 : 0.5226007462549447
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552454
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056485
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333072
Loss in iteration 154 : 0.5046438831894594
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.49914911381687194
Loss in iteration 160 : 0.4980569551084795
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.4947941542526308
Loss in iteration 164 : 0.49371112713244436
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.4915519580073085
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.48833047684132436
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.48619438167066215
Loss in iteration 172 : 0.4851298124112099
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.48195005758540466
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.47669715650578703
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.4746124009886094
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.47253704845175315
Loss in iteration 185 : 0.4715029036334387
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536213
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.4663675690101804
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.4623018627393149
Loss in iteration 195 : 0.46129135823604495
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.4592774626543426
Loss in iteration 198 : 0.4582740731693265
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.45627441406910757
Testing accuracy  of updater 4 on alg 0 with rate 4.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 3880 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224036
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632758
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439014
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043774
Loss in iteration 15 : 0.6751379721466284
Loss in iteration 16 : 0.6738340200990942
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694718
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873454
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050736
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663436
Loss in iteration 37 : 0.6465682744294413
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680166
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889295
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658473
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426643
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739225
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033767
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213936
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.5832800218249878
Loss in iteration 88 : 0.5820463432644792
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.5795835666695709
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708514
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677673
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231537
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211118
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722849
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190974
Loss in iteration 131 : 0.5306307670254143
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961833
Loss in iteration 134 : 0.5271766840349437
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035022
Loss in iteration 137 : 0.523741543468015
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609136
Loss in iteration 145 : 0.5146754836056485
Loss in iteration 146 : 0.5135520100559262
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333072
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.4991491138168719
Loss in iteration 160 : 0.49805695510847947
Loss in iteration 161 : 0.4969670726066896
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.4947941542526308
Loss in iteration 164 : 0.49371112713244425
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.4915519580073085
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.4883304768413243
Loss in iteration 170 : 0.4872612709605178
Loss in iteration 171 : 0.4861943816706622
Loss in iteration 172 : 0.48512981241120995
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.4819500575854046
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.4766971565057869
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.47253704845175315
Loss in iteration 185 : 0.4715029036334387
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.46636756901018045
Loss in iteration 191 : 0.4653475914848989
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.4612913582360449
Loss in iteration 196 : 0.4602832245926189
Loss in iteration 197 : 0.4592774626543426
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.45727305678907876
Loss in iteration 200 : 0.45627441406910757
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 3700 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 38.02310763522986
Loss in iteration 3 : 14.887005509999248
Loss in iteration 4 : 21.606474474301464
Loss in iteration 5 : 1.6576291684181068
Loss in iteration 6 : 0.9670692878857601
Loss in iteration 7 : 0.6311853425716035
Loss in iteration 8 : 0.45944929315132615
Loss in iteration 9 : 0.3601509001479763
Loss in iteration 10 : 0.2979979601956787
Loss in iteration 11 : 0.24722823036692687
Loss in iteration 12 : 0.20996148267621453
Loss in iteration 13 : 0.1818838380426416
Loss in iteration 14 : 0.1567426524148298
Loss in iteration 15 : 0.1352205950713526
Loss in iteration 16 : 0.11703418490144389
Loss in iteration 17 : 0.10268845948004189
Loss in iteration 18 : 0.0914080917459776
Loss in iteration 19 : 0.08268618271948033
Loss in iteration 20 : 0.07500659801805643
Loss in iteration 21 : 0.06835292933521621
Loss in iteration 22 : 0.06192145698401974
Loss in iteration 23 : 0.057040070808683695
Loss in iteration 24 : 0.053329731385423074
Loss in iteration 25 : 0.05008399386175958
Loss in iteration 26 : 0.04704572506989722
Loss in iteration 27 : 0.04464927638376722
Loss in iteration 28 : 0.04189738705053139
Loss in iteration 29 : 0.03973161882289194
Loss in iteration 30 : 0.03644216306777371
Loss in iteration 31 : 0.03397659865396088
Loss in iteration 32 : 0.031212357550845053
Loss in iteration 33 : 0.02861253692851269
Loss in iteration 34 : 0.025567275508468063
Loss in iteration 35 : 0.023078865921906982
Loss in iteration 36 : 0.01983957787875962
Loss in iteration 37 : 0.0180906292216495
Loss in iteration 38 : 0.01284948847058867
Loss in iteration 39 : 0.01032429721138684
Loss in iteration 40 : 0.007078175445368785
Loss in iteration 41 : 0.006759468568423596
Loss in iteration 42 : 0.0017462077115349842
Loss in iteration 43 : 0.010331223001240779
Loss in iteration 44 : 0.009268050321059335
Loss in iteration 45 : 0.05948228673496621
Loss in iteration 46 : 0.15372785369044664
Loss in iteration 47 : 0.41856503370556725
Loss in iteration 48 : 10.769433207850245
Loss in iteration 49 : 66.06358604832289
Loss in iteration 50 : 0.510345237907012
Loss in iteration 51 : 0.18973477534895525
Loss in iteration 52 : 0.08594630169959804
Loss in iteration 53 : 0.05551248693778378
Loss in iteration 54 : 0.041831490665091946
Loss in iteration 55 : 0.03108013062789706
Loss in iteration 56 : 0.024044095253380096
Loss in iteration 57 : 0.020230457662590396
Loss in iteration 58 : 0.017450340220756786
Loss in iteration 59 : 0.014942154619879397
Loss in iteration 60 : 0.012813307416882745
Loss in iteration 61 : 0.011256237824019341
Loss in iteration 62 : 0.009691197841637786
Loss in iteration 63 : 0.008067337565681893
Loss in iteration 64 : 0.006530093822911467
Loss in iteration 65 : 0.005301325922500782
Loss in iteration 66 : 0.00412423128266355
Loss in iteration 67 : 0.0029183526513507636
Loss in iteration 68 : 0.0016925674636708219
Loss in iteration 69 : 8.451417674728583E-4
Loss in iteration 70 : 4.099709636282074E-4
Loss in iteration 71 : 1.2756715221336591E-4
Loss in iteration 72 : 5.643072539694344E-5
Testing accuracy  of updater 5 on alg 0 with rate 10.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1443 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 15.662613531788876
Loss in iteration 3 : 9.544082302935292
Loss in iteration 4 : 21.65687241605563
Loss in iteration 5 : 1.4292530255253315
Loss in iteration 6 : 0.6227864654695227
Loss in iteration 7 : 0.3910904257634994
Loss in iteration 8 : 0.28196345349652496
Loss in iteration 9 : 0.21997408832450674
Loss in iteration 10 : 0.1771641394560861
Loss in iteration 11 : 0.14397718775845836
Loss in iteration 12 : 0.11583245588470564
Loss in iteration 13 : 0.09654604861837417
Loss in iteration 14 : 0.0826201011208357
Loss in iteration 15 : 0.06977800021572812
Loss in iteration 16 : 0.05964583360998751
Loss in iteration 17 : 0.05240678252936614
Loss in iteration 18 : 0.046569594540225676
Loss in iteration 19 : 0.04119066031757895
Loss in iteration 20 : 0.036750544203877314
Loss in iteration 21 : 0.03323683810148526
Loss in iteration 22 : 0.030583600691155868
Loss in iteration 23 : 0.02834056594512578
Loss in iteration 24 : 0.026150509286071485
Loss in iteration 25 : 0.023987666750119534
Loss in iteration 26 : 0.02196687785917313
Loss in iteration 27 : 0.020098380346325044
Loss in iteration 28 : 0.01827799790497094
Loss in iteration 29 : 0.01645211364576135
Loss in iteration 30 : 0.014641447175134149
Loss in iteration 31 : 0.01284948652909061
Loss in iteration 32 : 0.01102817598666811
Loss in iteration 33 : 0.009140033551117845
Loss in iteration 34 : 0.007166521362312136
Loss in iteration 35 : 0.005101050002505357
Loss in iteration 36 : 0.003083650453430027
Loss in iteration 37 : 0.0016862613351520275
Loss in iteration 38 : 7.813555276650535E-4
Loss in iteration 39 : 7.363746064895556E-4
Loss in iteration 40 : 0.0012390573365929738
Loss in iteration 41 : 9.449057889462432E-4
Loss in iteration 42 : 5.298022467748178E-4
Loss in iteration 43 : 3.378790543108443E-5
Testing accuracy  of updater 5 on alg 0 with rate 7.0 = 0.9955555555555555, training accuracy 1.0, time elapsed: 852 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 11.336676669672627
Loss in iteration 3 : 7.995528402039113
Loss in iteration 4 : 12.072719527100674
Loss in iteration 5 : 0.6919160476460285
Loss in iteration 6 : 0.4324986100168862
Loss in iteration 7 : 0.2898221360790744
Loss in iteration 8 : 0.20639639789818254
Loss in iteration 9 : 0.16153786011127774
Loss in iteration 10 : 0.1297290384380916
Loss in iteration 11 : 0.10530423747976958
Loss in iteration 12 : 0.08732131543928795
Loss in iteration 13 : 0.07424028767676696
Loss in iteration 14 : 0.0643732266475063
Loss in iteration 15 : 0.056276195753210134
Loss in iteration 16 : 0.04870828269900886
Loss in iteration 17 : 0.042201005595840044
Loss in iteration 18 : 0.037429350840820025
Loss in iteration 19 : 0.033636849449099684
Loss in iteration 20 : 0.030481234401955333
Loss in iteration 21 : 0.027739231048789347
Loss in iteration 22 : 0.02521942053089351
Loss in iteration 23 : 0.022976914487050307
Loss in iteration 24 : 0.02119949806419115
Loss in iteration 25 : 0.0197319878110061
Loss in iteration 26 : 0.018402845221473955
Loss in iteration 27 : 0.017177170065170503
Loss in iteration 28 : 0.01605039708096444
Loss in iteration 29 : 0.01499019890221261
Loss in iteration 30 : 0.013960679392045048
Loss in iteration 31 : 0.01293668751103236
Loss in iteration 32 : 0.011900501468714334
Loss in iteration 33 : 0.010839429873362411
Loss in iteration 34 : 0.009744863098751432
Loss in iteration 35 : 0.008611553052052068
Loss in iteration 36 : 0.007436802418292707
Loss in iteration 37 : 0.006218819712765885
Loss in iteration 38 : 0.004954462765481448
Loss in iteration 39 : 0.003640463723228728
Loss in iteration 40 : 0.0023032367502567558
Loss in iteration 41 : 0.0012015438265902214
Loss in iteration 42 : 6.510634001377079E-4
Loss in iteration 43 : 4.5151057179320536E-4
Loss in iteration 44 : 4.1607053351772375E-4
Loss in iteration 45 : 4.678694402648701E-4
Loss in iteration 46 : 5.375435712830337E-4
Loss in iteration 47 : 5.002679161806282E-4
Loss in iteration 48 : 3.0684467536373024E-4
Loss in iteration 49 : 9.078568083436158E-5
Loss in iteration 50 : 6.171374555844962E-5
Testing accuracy  of updater 5 on alg 0 with rate 4.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 1277 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 3.108869697259835
Loss in iteration 3 : 1.9199108934707352
Loss in iteration 4 : 2.793850216321727
Loss in iteration 5 : 0.17756504449764834
Loss in iteration 6 : 0.1071173243993115
Loss in iteration 7 : 0.07319977518243999
Loss in iteration 8 : 0.05484013751162413
Loss in iteration 9 : 0.043508108832394754
Loss in iteration 10 : 0.03561918998226584
Loss in iteration 11 : 0.0297552040293749
Loss in iteration 12 : 0.025242424861993476
Loss in iteration 13 : 0.021654381753456537
Loss in iteration 14 : 0.01875217973588756
Loss in iteration 15 : 0.016387458241849003
Loss in iteration 16 : 0.014449737706387824
Loss in iteration 17 : 0.012856879394520132
Loss in iteration 18 : 0.01154224299482073
Loss in iteration 19 : 0.010449058166677072
Loss in iteration 20 : 0.009531204094468075
Loss in iteration 21 : 0.008752852713057145
Loss in iteration 22 : 0.008086100229448796
Loss in iteration 23 : 0.007508436085309795
Loss in iteration 24 : 0.007001151202606277
Loss in iteration 25 : 0.006548621287566273
Loss in iteration 26 : 0.0061379770643674655
Loss in iteration 27 : 0.0057588210301774325
Loss in iteration 28 : 0.005402890841974771
Loss in iteration 29 : 0.005063693073327042
Loss in iteration 30 : 0.004736154942742743
Loss in iteration 31 : 0.004416327442006113
Loss in iteration 32 : 0.004101156888722026
Loss in iteration 33 : 0.0037883349492464123
Loss in iteration 34 : 0.0034762417406608123
Loss in iteration 35 : 0.0031640146486738353
Loss in iteration 36 : 0.0028518106695232135
Loss in iteration 37 : 0.0025413744439899016
Loss in iteration 38 : 0.002236997739096776
Loss in iteration 39 : 0.001946567722033613
Loss in iteration 40 : 0.001681258801160974
Loss in iteration 41 : 0.0014517089330737072
Loss in iteration 42 : 0.0012621732834803346
Loss in iteration 43 : 0.001109031352833785
Loss in iteration 44 : 9.84889457634415E-4
Loss in iteration 45 : 8.826906715297215E-4
Loss in iteration 46 : 7.970999070842289E-4
Loss in iteration 47 : 7.243352822038127E-4
Loss in iteration 48 : 6.617005853879365E-4
Loss in iteration 49 : 6.072224277630945E-4
Loss in iteration 50 : 5.59417929675663E-4
Loss in iteration 51 : 5.171485794231527E-4
Loss in iteration 52 : 4.795249912834726E-4
Loss in iteration 53 : 4.4584212956420735E-4
Loss in iteration 54 : 4.155337135245245E-4
Loss in iteration 55 : 3.8813933121600706E-4
Loss in iteration 56 : 3.632803123020373E-4
Loss in iteration 57 : 3.406418043226956E-4
Loss in iteration 58 : 3.1995932463917565E-4
Loss in iteration 59 : 3.0100858405010994E-4
Loss in iteration 60 : 2.8359772570746153E-4
Loss in iteration 61 : 2.6756136119790154E-4
Loss in iteration 62 : 2.5275595235517515E-4
Loss in iteration 63 : 2.3905620569674376E-4
Loss in iteration 64 : 2.2635223138891574E-4
Loss in iteration 65 : 2.14547280491467E-4
Loss in iteration 66 : 2.0355591990314447E-4
Loss in iteration 67 : 1.9330253873008627E-4
Loss in iteration 68 : 1.837201060048548E-4
Loss in iteration 69 : 1.747491199803625E-4
Loss in iteration 70 : 1.663367050395532E-4
Loss in iteration 71 : 1.5843582453056376E-4
Loss in iteration 72 : 1.5100458719718583E-4
Loss in iteration 73 : 1.4400563179686995E-4
Loss in iteration 74 : 1.3740557939255767E-4
Loss in iteration 75 : 1.3117454605024892E-4
Loss in iteration 76 : 1.2528571063751386E-4
Loss in iteration 77 : 1.1971493344489066E-4
Loss in iteration 78 : 1.1444042175406186E-4
Loss in iteration 79 : 1.0944243851222695E-4
Loss in iteration 80 : 1.0470305014633884E-4
Loss in iteration 81 : 1.0020590940914489E-4
Loss in iteration 82 : 9.593606908109687E-5
Loss in iteration 83 : 9.187982240632523E-5
Loss in iteration 84 : 8.802456632832643E-5
Loss in iteration 85 : 8.435868389611625E-5
Loss in iteration 86 : 8.087144260865633E-5
Loss in iteration 87 : 7.755290591556982E-5
Loss in iteration 88 : 7.439385555917085E-5
Loss in iteration 89 : 7.138572289385106E-5
Loss in iteration 90 : 6.852052772709476E-5
Loss in iteration 91 : 6.579082357533421E-5
Loss in iteration 92 : 6.318964850866385E-5
Loss in iteration 93 : 6.0710480970086824E-5
Loss in iteration 94 : 5.834720010529972E-5
Loss in iteration 95 : 5.6094050234711676E-5
Loss in iteration 96 : 5.394560915458497E-5
Loss in iteration 97 : 5.1896759979458446E-5
Loss in iteration 98 : 4.994266624484403E-5
Loss in iteration 99 : 4.807874998580641E-5
Loss in iteration 100 : 4.630067250094819E-5
Loss in iteration 101 : 4.460431750674399E-5
Loss in iteration 102 : 4.298577638690139E-5
Loss in iteration 103 : 4.14413352480236E-5
Loss in iteration 104 : 3.996746350386252E-5
Loss in iteration 105 : 3.8560803729065646E-5
Loss in iteration 106 : 3.7218162546193304E-5
Loss in iteration 107 : 3.593650233553932E-5
Loss in iteration 108 : 3.471293358753944E-5
Loss in iteration 109 : 3.354470774697721E-5
Loss in iteration 110 : 3.242921042936152E-5
Loss in iteration 111 : 3.136395491893509E-5
Loss in iteration 112 : 3.0346575884919572E-5
Loss in iteration 113 : 2.9374823277621315E-5
Loss in iteration 114 : 2.8446556386499334E-5
Loss in iteration 115 : 2.755973805926335E-5
Loss in iteration 116 : 2.671242909426128E-5
Loss in iteration 117 : 2.590278282678489E-5
Loss in iteration 118 : 2.5129039934980676E-5
Loss in iteration 119 : 2.4389523491632326E-5
Loss in iteration 120 : 2.3682634287037652E-5
Loss in iteration 121 : 2.300684644257746E-5
Loss in iteration 122 : 2.2360703329603684E-5
Loss in iteration 123 : 2.174281379971628E-5
Loss in iteration 124 : 2.1151848725520424E-5
Loss in iteration 125 : 2.058653784331907E-5
Loss in iteration 126 : 2.0045666882092475E-5
Loss in iteration 127 : 1.9528074957763355E-5
Loss in iteration 128 : 1.9032652208187156E-5
Loss in iteration 129 : 1.855833764057074E-5
Loss in iteration 130 : 1.810411716355428E-5
Loss in iteration 131 : 1.766902177554872E-5
Loss in iteration 132 : 1.7252125882948983E-5
Loss in iteration 133 : 1.6852545724333833E-5
Loss in iteration 134 : 1.6469437879658437E-5
Loss in iteration 135 : 1.610199784635447E-5
Testing accuracy  of updater 5 on alg 0 with rate 1.0 = 0.9982222222222222, training accuracy 1.0, time elapsed: 3188 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 2.6763099994271347
Loss in iteration 3 : 1.148855974526317
Loss in iteration 4 : 1.549668664054991
Loss in iteration 5 : 0.12405555221059233
Loss in iteration 6 : 0.08008802386490861
Loss in iteration 7 : 0.05590257289409145
Loss in iteration 8 : 0.0426321931252182
Loss in iteration 9 : 0.03417814234450328
Loss in iteration 10 : 0.028328869767154822
Loss in iteration 11 : 0.02397127959886068
Loss in iteration 12 : 0.020569321512668944
Loss in iteration 13 : 0.01784113135846166
Loss in iteration 14 : 0.015620886188841995
Loss in iteration 15 : 0.013800690061834332
Loss in iteration 16 : 0.01230083991319355
Loss in iteration 17 : 0.01105840869047674
Loss in iteration 18 : 0.010022942823695128
Loss in iteration 19 : 0.009153690705201924
Loss in iteration 20 : 0.008417669682054844
Loss in iteration 21 : 0.007788241066479901
Loss in iteration 22 : 0.007243953216341525
Loss in iteration 23 : 0.006767576889284497
Loss in iteration 24 : 0.006345305549097877
Loss in iteration 25 : 0.005966095093130463
Loss in iteration 26 : 0.00562111899250658
Loss in iteration 27 : 0.005303319852251271
Loss in iteration 28 : 0.00500704298691306
Loss in iteration 29 : 0.00472774067518819
Loss in iteration 30 : 0.004461737958208597
Loss in iteration 31 : 0.0042060529030552764
Loss in iteration 32 : 0.00395826653125349
Loss in iteration 33 : 0.0037164402974857974
Loss in iteration 34 : 0.0034790822831551648
Loss in iteration 35 : 0.003245167136473899
Loss in iteration 36 : 0.0030142180572519206
Loss in iteration 37 : 0.0027864569935472524
Loss in iteration 38 : 0.002563009199872069
Loss in iteration 39 : 0.002346088917487932
Loss in iteration 40 : 0.002138985143759599
Loss in iteration 41 : 0.0019455884396984262
Loss in iteration 42 : 0.0017693732518246907
Loss in iteration 43 : 0.001612268605055319
Loss in iteration 44 : 0.0014741769620847907
Loss in iteration 45 : 0.0013534137927087363
Loss in iteration 46 : 0.00124757794425605
Loss in iteration 47 : 0.0011542541670388963
Loss in iteration 48 : 0.001071349911710465
Loss in iteration 49 : 9.971678986311534E-4
Loss in iteration 50 : 9.303617486865866E-4
Loss in iteration 51 : 8.698632888246906E-4
Loss in iteration 52 : 8.148164078832262E-4
Loss in iteration 53 : 7.645255675803372E-4
Loss in iteration 54 : 7.184177428687097E-4
Loss in iteration 55 : 6.760146039329842E-4
Loss in iteration 56 : 6.369120765142584E-4
Loss in iteration 57 : 6.007651557985263E-4
Loss in iteration 58 : 5.672764830465054E-4
Loss in iteration 59 : 5.361876487219103E-4
Loss in iteration 60 : 5.072724954992852E-4
Loss in iteration 61 : 4.803319038771834E-4
Loss in iteration 62 : 4.551896860732494E-4
Loss in iteration 63 : 4.31689313195714E-4
Loss in iteration 64 : 4.0969127119167725E-4
Loss in iteration 65 : 3.890708923825864E-4
Loss in iteration 66 : 3.697165477131712E-4
Loss in iteration 67 : 3.5152811410182785E-4
Loss in iteration 68 : 3.344156539336599E-4
Loss in iteration 69 : 3.1829826127860927E-4
Loss in iteration 70 : 3.0310304279099827E-4
Loss in iteration 71 : 2.8876421112402067E-4
Loss in iteration 72 : 2.7522227564694303E-4
Loss in iteration 73 : 2.624233198337608E-4
Loss in iteration 74 : 2.5031835745192675E-4
Loss in iteration 75 : 2.3886276114265127E-4
Loss in iteration 76 : 2.280157576148943E-4
Loss in iteration 77 : 2.1773998385112527E-4
Loss in iteration 78 : 2.0800109871818323E-4
Loss in iteration 79 : 1.987674443693197E-4
Loss in iteration 80 : 1.9000975190738473E-4
Loss in iteration 81 : 1.817008859874532E-4
Loss in iteration 82 : 1.7381562336309038E-4
Loss in iteration 83 : 1.6633046079660366E-4
Loss in iteration 84 : 1.5922344822258053E-4
Loss in iteration 85 : 1.5247404354497228E-4
Loss in iteration 86 : 1.4606298592811766E-4
Loss in iteration 87 : 1.3997218489511938E-4
Loss in iteration 88 : 1.3418462295547955E-4
Loss in iteration 89 : 1.2868426984422905E-4
Loss in iteration 90 : 1.234560067632534E-4
Loss in iteration 91 : 1.1848555927284005E-4
Loss in iteration 92 : 1.1375943769517753E-4
Loss in iteration 93 : 1.0926488406192627E-4
Loss in iteration 94 : 1.0498982477556917E-4
Loss in iteration 95 : 1.0092282826219094E-4
Loss in iteration 96 : 9.705306697891488E-5
Loss in iteration 97 : 9.3370283207798E-5
Loss in iteration 98 : 8.986475812354789E-5
Loss in iteration 99 : 8.652728367025744E-5
Loss in iteration 100 : 8.334913682541698E-5
Loss in iteration 101 : 8.032205586854613E-5
Loss in iteration 102 : 7.743821831251962E-5
Loss in iteration 103 : 7.469022019403624E-5
Loss in iteration 104 : 7.207105646041262E-5
Loss in iteration 105 : 6.95741022301175E-5
Loss in iteration 106 : 6.719309474411063E-5
Loss in iteration 107 : 6.492211586576994E-5
Loss in iteration 108 : 6.275557502261758E-5
Loss in iteration 109 : 6.068819252003169E-5
Loss in iteration 110 : 5.871498318746564E-5
Loss in iteration 111 : 5.683124034570685E-5
Loss in iteration 112 : 5.5032520107710566E-5
Loss in iteration 113 : 5.3314626043612856E-5
Loss in iteration 114 : 5.167359425576378E-5
Loss in iteration 115 : 5.010567891814233E-5
Loss in iteration 116 : 4.860733834126587E-5
Loss in iteration 117 : 4.717522162273456E-5
Loss in iteration 118 : 4.58061559423894E-5
Loss in iteration 119 : 4.449713455294579E-5
Loss in iteration 120 : 4.3245305508670354E-5
Loss in iteration 121 : 4.204796116221633E-5
Loss in iteration 122 : 4.090252844619373E-5
Loss in iteration 123 : 3.980655994297486E-5
Loss in iteration 124 : 3.875772573062183E-5
Loss in iteration 125 : 3.775380598119365E-5
Loss in iteration 126 : 3.6792684274914586E-5
Loss in iteration 127 : 3.58723415847685E-5
Loss in iteration 128 : 3.499085087811336E-5
Loss in iteration 129 : 3.414637227783401E-5
Loss in iteration 130 : 3.333714872257481E-5
Loss in iteration 131 : 3.256150206576274E-5
Loss in iteration 132 : 3.1817829554106414E-5
Loss in iteration 133 : 3.110460063021002E-5
Loss in iteration 134 : 3.0420354007307653E-5
Loss in iteration 135 : 2.976369496923768E-5
Loss in iteration 136 : 2.913329285407268E-5
Loss in iteration 137 : 2.8527878685226034E-5
Loss in iteration 138 : 2.79462429189168E-5
Loss in iteration 139 : 2.7387233282379813E-5
Loss in iteration 140 : 2.684975268194038E-5
Loss in iteration 141 : 2.6332757164555888E-5
Loss in iteration 142 : 2.5835253920498586E-5
Loss in iteration 143 : 2.5356299318827305E-5
Loss in iteration 144 : 2.4894996969849364E-5
Loss in iteration 145 : 2.4450495812229734E-5
Loss in iteration 146 : 2.4021988223952474E-5
Loss in iteration 147 : 2.3608708158544793E-5
Loss in iteration 148 : 2.3209929308822218E-5
Loss in iteration 149 : 2.2824963301917414E-5
Loss in iteration 150 : 2.2453157929060793E-5
Loss in iteration 151 : 2.209389541457651E-5
Loss in iteration 152 : 2.174659072802939E-5
Loss in iteration 153 : 2.1410689943344305E-5
Loss in iteration 154 : 2.1085668648435806E-5
Loss in iteration 155 : 2.077103040807185E-5
Loss in iteration 156 : 2.0466305282753995E-5
Loss in iteration 157 : 2.0171048404997482E-5
Testing accuracy  of updater 5 on alg 0 with rate 0.7 = 1.0, training accuracy 1.0, time elapsed: 3240 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 1.5588570204121273
Loss in iteration 3 : 0.7647729889338459
Loss in iteration 4 : 0.9245054465659857
Loss in iteration 5 : 0.0872513468268666
Loss in iteration 6 : 0.06023319194065519
Loss in iteration 7 : 0.04461825526317237
Loss in iteration 8 : 0.03542027178111441
Loss in iteration 9 : 0.029353900495911946
Loss in iteration 10 : 0.02504202932408291
Loss in iteration 11 : 0.021807283184712884
Loss in iteration 12 : 0.01927845579603323
Loss in iteration 13 : 0.017241757891380865
Loss in iteration 14 : 0.015566367920211573
Loss in iteration 15 : 0.014166777937724355
Loss in iteration 16 : 0.0129835565658793
Loss in iteration 17 : 0.011973200728198229
Loss in iteration 18 : 0.011102601772609801
Loss in iteration 19 : 0.010345895399219356
Loss in iteration 20 : 0.009682549914694033
Loss in iteration 21 : 0.009096112394257503
Loss in iteration 22 : 0.008573323364050148
Loss in iteration 23 : 0.008103454853659238
Loss in iteration 24 : 0.00767779676390891
Loss in iteration 25 : 0.007289250433498127
Loss in iteration 26 : 0.006932004736844324
Loss in iteration 27 : 0.006601278113355891
Loss in iteration 28 : 0.006293114093055509
Loss in iteration 29 : 0.006004220321171363
Loss in iteration 30 : 0.0057318428057361644
Loss in iteration 31 : 0.00547366854346638
Loss in iteration 32 : 0.005227750964953056
Loss in iteration 33 : 0.004992453799820024
Loss in iteration 34 : 0.004766409964972173
Loss in iteration 35 : 0.00454849287267302
Loss in iteration 36 : 0.004337798072520535
Loss in iteration 37 : 0.004133633294198554
Loss in iteration 38 : 0.003935514638742774
Loss in iteration 39 : 0.00374316578211964
Loss in iteration 40 : 0.0035565156296635996
Loss in iteration 41 : 0.0033756882291238874
Loss in iteration 42 : 0.003200977800335457
Loss in iteration 43 : 0.0030328029476478766
Loss in iteration 44 : 0.0028716389464110968
Loss in iteration 45 : 0.0027179353768733905
Loss in iteration 46 : 0.0025720351987331275
Loss in iteration 47 : 0.0024341150523220566
Loss in iteration 48 : 0.0023041611295314254
Loss in iteration 49 : 0.0021819823334492824
Loss in iteration 50 : 0.0020672499846249577
Loss in iteration 51 : 0.001959547769076204
Loss in iteration 52 : 0.0018584180083981712
Loss in iteration 53 : 0.0017633967978738984
Loss in iteration 54 : 0.001674036629464979
Loss in iteration 55 : 0.0015899186251011421
Loss in iteration 56 : 0.0015106575661248808
Loss in iteration 57 : 0.001435902561570103
Loss in iteration 58 : 0.0013653353897901527
Loss in iteration 59 : 0.0012986677845639157
Loss in iteration 60 : 0.0012356383849247263
Loss in iteration 61 : 0.00117600972236069
Loss in iteration 62 : 0.0011195654216071625
Loss in iteration 63 : 0.0010661076853512484
Loss in iteration 64 : 0.0010154550789067578
Loss in iteration 65 : 9.674406042171299E-4
Loss in iteration 66 : 9.219100401299467E-4
Loss in iteration 67 : 8.787205208364668E-4
Loss in iteration 68 : 8.377393231529802E-4
Loss in iteration 69 : 7.988428341282349E-4
Loss in iteration 70 : 7.619156723300248E-4
Loss in iteration 71 : 7.268499385558786E-4
Loss in iteration 72 : 6.935445743167165E-4
Loss in iteration 73 : 6.619048090752294E-4
Loss in iteration 74 : 6.318416797811714E-4
Loss in iteration 75 : 6.032716086784478E-4
Loss in iteration 76 : 5.761160276390462E-4
Loss in iteration 77 : 5.503010393912986E-4
Loss in iteration 78 : 5.25757107945676E-4
Loss in iteration 79 : 5.024187722681092E-4
Loss in iteration 80 : 4.802243787943845E-4
Loss in iteration 81 : 4.591158297064979E-4
Loss in iteration 82 : 4.390383449903547E-4
Loss in iteration 83 : 4.1994023716104123E-4
Loss in iteration 84 : 4.017726981805992E-4
Loss in iteration 85 : 3.844895985191917E-4
Loss in iteration 86 : 3.6804729854824134E-4
Loss in iteration 87 : 3.5240447253466863E-4
Loss in iteration 88 : 3.375219454649727E-4
Loss in iteration 89 : 3.233625428020531E-4
Loss in iteration 90 : 3.09890953102485E-4
Loss in iteration 91 : 2.9707360322439434E-4
Loss in iteration 92 : 2.848785456623397E-4
Loss in iteration 93 : 2.73275357370602E-4
Loss in iteration 94 : 2.622350492919922E-4
Loss in iteration 95 : 2.5172998570116594E-4
Loss in iteration 96 : 2.4173381239934647E-4
Loss in iteration 97 : 2.3222139276159529E-4
Loss in iteration 98 : 2.2316875063209187E-4
Loss in iteration 99 : 2.1455301908508465E-4
Loss in iteration 100 : 2.063523941120962E-4
Loss in iteration 101 : 1.9854609235690483E-4
Loss in iteration 102 : 1.9111431209317166E-4
Loss in iteration 103 : 1.8403819672237548E-4
Loss in iteration 104 : 1.7729980015870813E-4
Loss in iteration 105 : 1.7088205356018517E-4
Loss in iteration 106 : 1.647687329583153E-4
Loss in iteration 107 : 1.589444274315285E-4
Loss in iteration 108 : 1.5339450755659598E-4
Loss in iteration 109 : 1.4810509395673228E-4
Loss in iteration 110 : 1.4306302584341874E-4
Loss in iteration 111 : 1.3825582951901272E-4
Loss in iteration 112 : 1.3367168686866598E-4
Loss in iteration 113 : 1.2929940392183284E-4
Loss in iteration 114 : 1.2512837960447848E-4
Loss in iteration 115 : 1.2114857483391704E-4
Loss in iteration 116 : 1.1735048212760338E-4
Loss in iteration 117 : 1.1372509590600487E-4
Loss in iteration 118 : 1.1026388366963157E-4
Loss in iteration 119 : 1.0695875821833877E-4
Loss in iteration 120 : 1.0380205106569346E-4
Loss in iteration 121 : 1.007864871725568E-4
Loss in iteration 122 : 9.79051611011935E-5
Loss in iteration 123 : 9.515151464936899E-5
Loss in iteration 124 : 9.251931600693272E-5
Loss in iteration 125 : 9.000264042064768E-5
Loss in iteration 126 : 8.759585236393036E-5
Loss in iteration 127 : 8.529358911969926E-5
Loss in iteration 128 : 8.309074576423328E-5
Loss in iteration 129 : 8.098246136517483E-5
Loss in iteration 130 : 7.896410644838907E-5
Loss in iteration 131 : 7.70312713840541E-5
Loss in iteration 132 : 7.517975596566643E-5
Loss in iteration 133 : 7.340555948008667E-5
Loss in iteration 134 : 7.170487208845508E-5
Loss in iteration 135 : 7.007406600841178E-5
Loss in iteration 136 : 6.850968857415976E-5
Loss in iteration 137 : 6.700845386737194E-5
Loss in iteration 138 : 6.556723775323755E-5
Loss in iteration 139 : 6.418306924161153E-5
Loss in iteration 140 : 6.285312877659806E-5
Loss in iteration 141 : 6.157473901064371E-5
Loss in iteration 142 : 6.034537002851006E-5
Loss in iteration 143 : 5.916263124249954E-5
Loss in iteration 144 : 5.8024292424241857E-5
Loss in iteration 145 : 5.6928283173027474E-5
Loss in iteration 146 : 5.5872745485247084E-5
Loss in iteration 147 : 5.485604954138389E-5
Loss in iteration 148 : 5.387688867254988E-5
Loss in iteration 149 : 5.293429651942888E-5
Loss in iteration 150 : 5.2027748283926035E-5
Loss in iteration 151 : 5.1157082979740265E-5
Loss in iteration 152 : 5.0322490565939755E-5
Loss in iteration 153 : 4.9524146716722114E-5
Loss in iteration 154 : 4.8762002411926085E-5
Loss in iteration 155 : 4.8035005769353396E-5
Loss in iteration 156 : 4.7341010158451106E-5
Loss in iteration 157 : 4.667574502713981E-5
Loss in iteration 158 : 4.6033741191623835E-5
Loss in iteration 159 : 4.540730732067175E-5
Loss in iteration 160 : 4.478933184490183E-5
Loss in iteration 161 : 4.417202288596794E-5
Loss in iteration 162 : 4.355127364697269E-5
Loss in iteration 163 : 4.292427530366105E-5
Loss in iteration 164 : 4.2293649203039366E-5
Loss in iteration 165 : 4.166335218025312E-5
Loss in iteration 166 : 4.1040925065154795E-5
Loss in iteration 167 : 4.043287281463572E-5
Loss in iteration 168 : 3.984554159014564E-5
Loss in iteration 169 : 3.9282297450783684E-5
Loss in iteration 170 : 3.874454282407037E-5
Loss in iteration 171 : 3.823128587170068E-5
Loss in iteration 172 : 3.774037975992918E-5
Loss in iteration 173 : 3.726903088210383E-5
Loss in iteration 174 : 3.681454174762145E-5
Loss in iteration 175 : 3.63745906721383E-5
Loss in iteration 176 : 3.594737797395479E-5
Loss in iteration 177 : 3.5531582775008693E-5
Loss in iteration 178 : 3.5126264728322504E-5
Loss in iteration 179 : 3.473074920325311E-5
Loss in iteration 180 : 3.434453180624752E-5
Loss in iteration 181 : 3.39672107483778E-5
Loss in iteration 182 : 3.3598444058407496E-5
Loss in iteration 183 : 3.32379253692852E-5
Loss in iteration 184 : 3.288537101908022E-5
Loss in iteration 185 : 3.254051368174414E-5
Loss in iteration 186 : 3.2203099202124505E-5
Loss in iteration 187 : 3.187288498478521E-5
Testing accuracy  of updater 5 on alg 0 with rate 0.4 = 1.0, training accuracy 1.0, time elapsed: 3679 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.5268180900808944
Loss in iteration 3 : 0.43754545005056916
Loss in iteration 4 : 0.27986871573554783
Loss in iteration 5 : 0.11545340425758958
Loss in iteration 6 : 0.10294768472865182
Loss in iteration 7 : 0.09338730030878864
Loss in iteration 8 : 0.0855545671697385
Loss in iteration 9 : 0.0789717724517851
Loss in iteration 10 : 0.0733232447610071
Loss in iteration 11 : 0.0683918460919035
Loss in iteration 12 : 0.06402550799446834
Loss in iteration 13 : 0.060115464981303775
Loss in iteration 14 : 0.05658204418935898
Loss in iteration 15 : 0.053365413333828035
Loss in iteration 16 : 0.05041953289928414
Loss in iteration 17 : 0.047708162435127745
Loss in iteration 18 : 0.04520218447959585
Loss in iteration 19 : 0.042877779098717386
Loss in iteration 20 : 0.04071515350827349
Loss in iteration 21 : 0.03869763910078919
Loss in iteration 22 : 0.036811035779026474
Loss in iteration 23 : 0.03504312588920268
Loss in iteration 24 : 0.03338330677473529
Loss in iteration 25 : 0.031822307965018316
Loss in iteration 26 : 0.030351969940863165
Loss in iteration 27 : 0.02896506854083766
Loss in iteration 28 : 0.027655173787706194
Loss in iteration 29 : 0.026416535087980492
Loss in iteration 30 : 0.025243986931540372
Loss in iteration 31 : 0.024132870733822566
Loss in iteration 32 : 0.023078969538082486
Loss in iteration 33 : 0.022078453070575738
Loss in iteration 34 : 0.021127831209588867
Loss in iteration 35 : 0.020223914351596633
Loss in iteration 36 : 0.01936377947606422
Loss in iteration 37 : 0.01854474095314445
Loss in iteration 38 : 0.01776432532570146
Loss in iteration 39 : 0.017020249442865742
Loss in iteration 40 : 0.016310401436846513
Loss in iteration 41 : 0.015632824125391416
Loss in iteration 42 : 0.014985700494536618
Loss in iteration 43 : 0.014367340974178872
Loss in iteration 44 : 0.013776172265564853
Loss in iteration 45 : 0.01321072751732028
Loss in iteration 46 : 0.012669637676872311
Loss in iteration 47 : 0.012151623868374546
Loss in iteration 48 : 0.011655490667542224
Loss in iteration 49 : 0.01118012015893264
Loss in iteration 50 : 0.010724466672798659
Loss in iteration 51 : 0.010287552107240963
Loss in iteration 52 : 0.00986846174749513
Loss in iteration 53 : 0.009466340498344218
Loss in iteration 54 : 0.00908038944845493
Loss in iteration 55 : 0.008709862687632138
Loss in iteration 56 : 0.008354064300449793
Loss in iteration 57 : 0.008012345463468299
Loss in iteration 58 : 0.007684101579375625
Loss in iteration 59 : 0.007368769390916434
Loss in iteration 60 : 0.007065824031154654
Loss in iteration 61 : 0.006774775984672951
Loss in iteration 62 : 0.0064951679561918475
Loss in iteration 63 : 0.006226571667274345
Loss in iteration 64 : 0.005968584625820531
Loss in iteration 65 : 0.005720826933825242
Loss in iteration 66 : 0.00548293821313179
Loss in iteration 67 : 0.0052545747340233965
Loss in iteration 68 : 0.005035406826129256
Loss in iteration 69 : 0.00482511663575246
Loss in iteration 70 : 0.004623396270635177
Loss in iteration 71 : 0.004429946346015632
Loss in iteration 72 : 0.004244474918785615
Loss in iteration 73 : 0.004066696773420011
Loss in iteration 74 : 0.003896333006764291
Loss in iteration 75 : 0.003733110849857431
Loss in iteration 76 : 0.0035767636633997272
Loss in iteration 77 : 0.0034270310478363075
Loss in iteration 78 : 0.003283659017361089
Loss in iteration 79 : 0.0031464001974582283
Loss in iteration 80 : 0.003015014016209342
Loss in iteration 81 : 0.0028892668693202847
Loss in iteration 82 : 0.0027689322469849373
Loss in iteration 83 : 0.0026537908170592415
Loss in iteration 84 : 0.002543630463618966
Loss in iteration 85 : 0.0024382462830488554
Loss in iteration 86 : 0.00233744054166168
Loss in iteration 87 : 0.0022410225997799298
Loss in iteration 88 : 0.0021488088075044148
Loss in iteration 89 : 0.002060622377263215
Loss in iteration 90 : 0.001976293237851141
Loss in iteration 91 : 0.0018956578741535918
Loss in iteration 92 : 0.0018185591561825942
Loss in iteration 93 : 0.0017448461604890988
Loss in iteration 94 : 0.0016743739864874064
Loss in iteration 95 : 0.0016070035697492307
Loss in iteration 96 : 0.0015426014939112752
Loss in iteration 97 : 0.001481039802480314
Loss in iteration 98 : 0.0014221958115343387
Loss in iteration 99 : 0.0013659519240642928
Loss in iteration 100 : 0.001312195446543741
Loss in iteration 101 : 0.0012608184081154817
Loss in iteration 102 : 0.0012117173827881046
Loss in iteration 103 : 0.0011647933147666942
Loss in iteration 104 : 0.0011199513473564172
Loss in iteration 105 : 0.0010771006552648256
Loss in iteration 106 : 0.0010361542810954634
Loss in iteration 107 : 9.970289764672327E-4
Loss in iteration 108 : 9.596450507094889E-4
Loss in iteration 109 : 9.239262604337598E-4
Loss in iteration 110 : 8.897998704979314E-4
Loss in iteration 111 : 8.571980583063738E-4
Loss in iteration 112 : 8.260673248828123E-4
Loss in iteration 113 : 7.964340067315494E-4
Loss in iteration 114 : 7.688402180769686E-4
Loss in iteration 115 : 7.473349074342361E-4
Loss in iteration 116 : 7.599979093118357E-4
Loss in iteration 117 : 0.0010065882502055558
Loss in iteration 118 : 0.00288283125678587
Loss in iteration 119 : 0.006935399802995322
Loss in iteration 120 : 0.0030684762366210584
Loss in iteration 121 : 5.211403178092054E-4
Loss in iteration 122 : 5.075557198471985E-4
Loss in iteration 123 : 4.990082183019035E-4
Loss in iteration 124 : 4.91839311122516E-4
Loss in iteration 125 : 4.850394370386641E-4
Loss in iteration 126 : 4.7833812652352443E-4
Loss in iteration 127 : 4.716614734108074E-4
Loss in iteration 128 : 4.649858599615625E-4
Loss in iteration 129 : 4.583013543605834E-4
Loss in iteration 130 : 4.5160311470275925E-4
Loss in iteration 131 : 4.448893042182247E-4
Loss in iteration 132 : 4.3816038189259225E-4
Loss in iteration 133 : 4.314186704777376E-4
Loss in iteration 134 : 4.2466799733219653E-4
Loss in iteration 135 : 4.1791337711971475E-4
Loss in iteration 136 : 4.111607329851068E-4
Loss in iteration 137 : 4.044166549544223E-4
Loss in iteration 138 : 3.9768819315524586E-4
Loss in iteration 139 : 3.9098268247958587E-4
Loss in iteration 140 : 3.8430759487123644E-4
Loss in iteration 141 : 3.7767041541997845E-4
Loss in iteration 142 : 3.710785387429528E-4
Loss in iteration 143 : 3.645391825979031E-4
Loss in iteration 144 : 3.5805931619489636E-4
Loss in iteration 145 : 3.5164560117581945E-4
Loss in iteration 146 : 3.453043436667347E-4
Loss in iteration 147 : 3.390414561525894E-4
Loss in iteration 148 : 3.328624281745923E-4
Loss in iteration 149 : 3.267723050147353E-4
Loss in iteration 150 : 3.207756736272899E-4
Loss in iteration 151 : 3.1487665512135946E-4
Loss in iteration 152 : 3.0907890311165356E-4
Loss in iteration 153 : 3.0338560725302465E-4
Loss in iteration 154 : 2.9779950127081437E-4
Loss in iteration 155 : 2.923228748035913E-4
Loss in iteration 156 : 2.8695758839294196E-4
Loss in iteration 157 : 2.817050909886081E-4
Loss in iteration 158 : 2.7656643938528896E-4
Loss in iteration 159 : 2.715423190682818E-4
Loss in iteration 160 : 2.666330660130411E-4
Loss in iteration 161 : 2.618386890555118E-4
Loss in iteration 162 : 2.5715889252023486E-4
Loss in iteration 163 : 2.5259309885739554E-4
Loss in iteration 164 : 2.4814047109713097E-4
Loss in iteration 165 : 2.4379993497447772E-4
Loss in iteration 166 : 2.395702006143529E-4
Loss in iteration 167 : 2.3544978369037232E-4
Loss in iteration 168 : 2.3143702598750018E-4
Loss in iteration 169 : 2.275301153080281E-4
Loss in iteration 170 : 2.237271046642624E-4
Loss in iteration 171 : 2.20025930704334E-4
Loss in iteration 172 : 2.1642443131820787E-4
Loss in iteration 173 : 2.129203623741494E-4
Loss in iteration 174 : 2.0951141353952983E-4
Loss in iteration 175 : 2.0619522314616447E-4
Loss in iteration 176 : 2.0296939206862057E-4
Loss in iteration 177 : 1.9983149659319092E-4
Loss in iteration 178 : 1.967791002660168E-4
Loss in iteration 179 : 1.9380976471935078E-4
Loss in iteration 180 : 1.9092105948558295E-4
Loss in iteration 181 : 1.8811057081824978E-4
Loss in iteration 182 : 1.853759095479469E-4
Loss in iteration 183 : 1.8271471800812672E-4
Loss in iteration 184 : 1.801246760725332E-4
Loss in iteration 185 : 1.7760350634997413E-4
Loss in iteration 186 : 1.7514897858660812E-4
Loss in iteration 187 : 1.7275891332796194E-4
Loss in iteration 188 : 1.704311848949061E-4
Loss in iteration 189 : 1.681637237284008E-4
Loss in iteration 190 : 1.659545181580009E-4
Loss in iteration 191 : 1.6380161564847476E-4
Loss in iteration 192 : 1.6170312357763374E-4
Loss in iteration 193 : 1.5965720959667544E-4
Loss in iteration 194 : 1.5766210162237407E-4
Loss in iteration 195 : 1.5571608750772077E-4
Loss in iteration 196 : 1.5381751443470972E-4
Loss in iteration 197 : 1.519647880702799E-4
Loss in iteration 198 : 1.5015637152283496E-4
Loss in iteration 199 : 1.483907841338252E-4
Loss in iteration 200 : 1.4666660013542326E-4
Testing accuracy  of updater 5 on alg 0 with rate 0.09999999999999998 = 1.0, training accuracy 1.0, time elapsed: 3680 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 1.8184721435293538
Loss in iteration 3 : 0.24000812053250836
Loss in iteration 4 : 0.7362726309832311
Loss in iteration 5 : 0.5845545117052736
Loss in iteration 6 : 0.30455545183816296
Loss in iteration 7 : 0.16964949575636726
Loss in iteration 8 : 0.14012982791874848
Loss in iteration 9 : 0.1910827029294971
Loss in iteration 10 : 0.21470576651306034
Loss in iteration 11 : 0.16961833349640226
Loss in iteration 12 : 0.10486135104777997
Loss in iteration 13 : 0.06019098153660873
Loss in iteration 14 : 0.03878518288473785
Loss in iteration 15 : 0.032152472385260956
Loss in iteration 16 : 0.03322574431591284
Loss in iteration 17 : 0.037159543889967755
Loss in iteration 18 : 0.04036573287358335
Loss in iteration 19 : 0.041474385498560806
Loss in iteration 20 : 0.04023437554579095
Loss in iteration 21 : 0.03696052663429016
Loss in iteration 22 : 0.03254813718781299
Loss in iteration 23 : 0.028095301126597756
Loss in iteration 24 : 0.024423583404554527
Loss in iteration 25 : 0.02185705198008547
Loss in iteration 26 : 0.020298725801289434
Loss in iteration 27 : 0.019541153210996295
Loss in iteration 28 : 0.019273356296834587
Loss in iteration 29 : 0.019207287259594204
Loss in iteration 30 : 0.01914559070631298
Loss in iteration 31 : 0.018978482886231395
Loss in iteration 32 : 0.018662985795222655
Loss in iteration 33 : 0.018197569315400147
Loss in iteration 34 : 0.017607405671834695
Loss in iteration 35 : 0.016931900807766707
Loss in iteration 36 : 0.016210252610171137
Loss in iteration 37 : 0.015468618968170018
Loss in iteration 38 : 0.014716895026483072
Loss in iteration 39 : 0.01395495753027233
Loss in iteration 40 : 0.013180259912761517
Loss in iteration 41 : 0.012392187580103865
Loss in iteration 42 : 0.011594397602618798
Loss in iteration 43 : 0.010797775448884338
Loss in iteration 44 : 0.010023045473413336
Loss in iteration 45 : 0.009293235543900432
Loss in iteration 46 : 0.008618245867414472
Loss in iteration 47 : 0.008000140437211315
Loss in iteration 48 : 0.0074462033302584146
Loss in iteration 49 : 0.006965253211028798
Loss in iteration 50 : 0.0065553988293453914
Loss in iteration 51 : 0.006199652058946647
Loss in iteration 52 : 0.005875820538350594
Loss in iteration 53 : 0.005566569799030813
Loss in iteration 54 : 0.005262048746900591
Loss in iteration 55 : 0.0049588273630394334
Loss in iteration 56 : 0.004657344323086499
Loss in iteration 57 : 0.004358676678153409
Loss in iteration 58 : 0.004062555698405395
Loss in iteration 59 : 0.0037679982197185878
Loss in iteration 60 : 0.0034752239244713193
Loss in iteration 61 : 0.0031865775226179102
Loss in iteration 62 : 0.0029058423142932746
Loss in iteration 63 : 0.0026370199319597338
Loss in iteration 64 : 0.0023836960225852516
Loss in iteration 65 : 0.002148944301429194
Loss in iteration 66 : 0.0019349680479781302
Loss in iteration 67 : 0.0017422608265692827
Loss in iteration 68 : 0.0015690948872083275
Loss in iteration 69 : 0.0014120791197852597
Loss in iteration 70 : 0.0012674322781087313
Loss in iteration 71 : 0.0011320767522108185
Loss in iteration 72 : 0.0010041221342617743
Loss in iteration 73 : 8.828387718604893E-4
Loss in iteration 74 : 7.683363774518431E-4
Loss in iteration 75 : 6.61101976223903E-4
Loss in iteration 76 : 5.615665545382993E-4
Loss in iteration 77 : 4.6992096173412374E-4
Loss in iteration 78 : 3.86294449758341E-4
Loss in iteration 79 : 3.1113302061379876E-4
Loss in iteration 80 : 2.4540946536777517E-4
Loss in iteration 81 : 1.9034705660572174E-4
Loss in iteration 82 : 1.4668416107743026E-4
Loss in iteration 83 : 1.1400102573716718E-4
Loss in iteration 84 : 9.070229813873984E-5
Loss in iteration 85 : 7.461763825185188E-5
Loss in iteration 86 : 6.366121377186491E-5
Loss in iteration 87 : 5.6175038474327094E-5
Loss in iteration 88 : 5.0976052703571834E-5
Loss in iteration 89 : 4.7267440636469525E-5
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.9946666666666667, training accuracy 1.0, time elapsed: 1644 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.54493172798401
Loss in iteration 3 : 0.12871496134399402
Loss in iteration 4 : 0.24628810181078228
Loss in iteration 5 : 0.1668198303156406
Loss in iteration 6 : 0.07975396329988288
Loss in iteration 7 : 0.05935736518484233
Loss in iteration 8 : 0.07909945362995564
Loss in iteration 9 : 0.0784086624204547
Loss in iteration 10 : 0.055743973420043
Loss in iteration 11 : 0.03451831876692093
Loss in iteration 12 : 0.023674714925178596
Loss in iteration 13 : 0.020814612651677174
Loss in iteration 14 : 0.022222658898471206
Loss in iteration 15 : 0.02425318223478021
Loss in iteration 16 : 0.024901697215673
Loss in iteration 17 : 0.023581980658713565
Loss in iteration 18 : 0.020789444214918407
Loss in iteration 19 : 0.017526303497469215
Loss in iteration 20 : 0.01466104428897264
Loss in iteration 21 : 0.012641876005264466
Loss in iteration 22 : 0.011461220029958969
Loss in iteration 23 : 0.010770511079867113
Loss in iteration 24 : 0.010271824259670563
Loss in iteration 25 : 0.009824614938755888
Loss in iteration 26 : 0.009365218120509188
Loss in iteration 27 : 0.008868890408531522
Loss in iteration 28 : 0.00833611301207069
Loss in iteration 29 : 0.007780829076227831
Loss in iteration 30 : 0.007221320714658395
Loss in iteration 31 : 0.006673623216640052
Loss in iteration 32 : 0.006146762726172401
Loss in iteration 33 : 0.005641528717844279
Loss in iteration 34 : 0.005154284057630163
Loss in iteration 35 : 0.00468313014366368
Loss in iteration 36 : 0.004232665924242884
Loss in iteration 37 : 0.0038161200233714576
Loss in iteration 38 : 0.0034539554398076693
Loss in iteration 39 : 0.003165734639045533
Loss in iteration 40 : 0.0029558209457528346
Loss in iteration 41 : 0.0028070356585560086
Loss in iteration 42 : 0.0026909234582908077
Loss in iteration 43 : 0.002581361257657571
Loss in iteration 44 : 0.002460901935159547
Loss in iteration 45 : 0.0023211113425286884
Loss in iteration 46 : 0.0021606505475290443
Loss in iteration 47 : 0.001983177339370969
Loss in iteration 48 : 0.0017955627538250194
Loss in iteration 49 : 0.001606240025071972
Loss in iteration 50 : 0.0014235117245743466
Loss in iteration 51 : 0.001253978917666982
Loss in iteration 52 : 0.00110152709158003
Loss in iteration 53 : 9.671983263680727E-4
Loss in iteration 54 : 8.498708816941057E-4
Loss in iteration 55 : 7.473173313532263E-4
Loss in iteration 56 : 6.571583207558315E-4
Loss in iteration 57 : 5.774313860691622E-4
Loss in iteration 58 : 5.067567075367621E-4
Loss in iteration 59 : 4.4423783030718823E-4
Loss in iteration 60 : 3.892598228495482E-4
Loss in iteration 61 : 3.413019738223852E-4
Loss in iteration 62 : 2.9982157132889996E-4
Loss in iteration 63 : 2.6421471660145247E-4
Loss in iteration 64 : 2.3383041767259793E-4
Loss in iteration 65 : 2.080072389585399E-4
Loss in iteration 66 : 1.861098665392134E-4
Loss in iteration 67 : 1.6755498155441703E-4
Loss in iteration 68 : 1.5182472349194536E-4
Loss in iteration 69 : 1.3847026187828004E-4
Loss in iteration 70 : 1.271089815584543E-4
Loss in iteration 71 : 1.1741826645959265E-4
Testing accuracy  of updater 6 on alg 0 with rate 1.4000000000000001 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1376 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.3763973202904008
Loss in iteration 3 : 0.12196067835355306
Loss in iteration 4 : 0.1796722933029131
Loss in iteration 5 : 0.1046077610132229
Loss in iteration 6 : 0.06019941604008839
Loss in iteration 7 : 0.05882945588346231
Loss in iteration 8 : 0.06261623318676766
Loss in iteration 9 : 0.047656288311490234
Loss in iteration 10 : 0.030598347487580637
Loss in iteration 11 : 0.02161266969832695
Loss in iteration 12 : 0.018980381813207575
Loss in iteration 13 : 0.01945505349152224
Loss in iteration 14 : 0.020432772561913188
Loss in iteration 15 : 0.02036356949854079
Loss in iteration 16 : 0.018877027627679194
Loss in iteration 17 : 0.016446156956183934
Loss in iteration 18 : 0.013880276983273493
Loss in iteration 19 : 0.011808716556380645
Loss in iteration 20 : 0.010448894102659312
Loss in iteration 21 : 0.009687975003339497
Loss in iteration 22 : 0.009266747124542174
Loss in iteration 23 : 0.008957992803010214
Loss in iteration 24 : 0.008633047105783796
Loss in iteration 25 : 0.008240615592778285
Loss in iteration 26 : 0.00777637269552207
Loss in iteration 27 : 0.007261437128273679
Loss in iteration 28 : 0.0067265537607450205
Loss in iteration 29 : 0.006200125204725441
Loss in iteration 30 : 0.0057011508228203955
Loss in iteration 31 : 0.00523805193530627
Loss in iteration 32 : 0.004812118031988018
Loss in iteration 33 : 0.004422499978147964
Loss in iteration 34 : 0.004069494926912077
Loss in iteration 35 : 0.0037549568336451263
Loss in iteration 36 : 0.0034806729160236417
Loss in iteration 37 : 0.0032462395015319255
Loss in iteration 38 : 0.0030479866877359597
Loss in iteration 39 : 0.0028795145778882417
Loss in iteration 40 : 0.002733026523501883
Loss in iteration 41 : 0.002600540581994056
Loss in iteration 42 : 0.0024748262708905464
Loss in iteration 43 : 0.0023501715353153314
Loss in iteration 44 : 0.002222887191777852
Loss in iteration 45 : 0.0020914229198419525
Loss in iteration 46 : 0.0019561266463069323
Loss in iteration 47 : 0.0018187827511894573
Loss in iteration 48 : 0.0016820556632775546
Loss in iteration 49 : 0.001548921820240796
Loss in iteration 50 : 0.0014221518985350692
Loss in iteration 51 : 0.0013039064453752645
Loss in iteration 52 : 0.0011955025594406247
Loss in iteration 53 : 0.0010973763524858863
Loss in iteration 54 : 0.001009214126292251
Loss in iteration 55 : 9.30183353659079E-4
Loss in iteration 56 : 8.591844182496973E-4
Loss in iteration 57 : 7.950640214131032E-4
Loss in iteration 58 : 7.367632150303806E-4
Loss in iteration 59 : 6.833995896532929E-4
Loss in iteration 60 : 6.342968695153891E-4
Loss in iteration 61 : 5.889779888216847E-4
Loss in iteration 62 : 5.471351835814304E-4
Loss in iteration 63 : 5.085872728841429E-4
Loss in iteration 64 : 4.7323210934773277E-4
Loss in iteration 65 : 4.4100107290659504E-4
Loss in iteration 66 : 4.118214785992054E-4
Loss in iteration 67 : 3.855910103723373E-4
Loss in iteration 68 : 3.6216566163082506E-4
Loss in iteration 69 : 3.413598160387003E-4
Loss in iteration 70 : 3.2295498466722826E-4
Loss in iteration 71 : 3.067129310548964E-4
Loss in iteration 72 : 2.9238941273928393E-4
Loss in iteration 73 : 2.797460049806336E-4
Loss in iteration 74 : 2.685588190522853E-4
Loss in iteration 75 : 2.5862396766538845E-4
Loss in iteration 76 : 2.497602334648642E-4
Testing accuracy  of updater 6 on alg 0 with rate 0.8 = 0.9928888888888889, training accuracy 1.0, time elapsed: 1403 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.3811258298154555
Loss in iteration 3 : 0.20914764881680029
Loss in iteration 4 : 0.14581486150804795
Loss in iteration 5 : 0.11026038423834345
Loss in iteration 6 : 0.0870801539109578
Loss in iteration 7 : 0.07014474412969614
Loss in iteration 8 : 0.05683070474094252
Loss in iteration 9 : 0.0463686011230056
Loss in iteration 10 : 0.038528039902787775
Loss in iteration 11 : 0.03283322400594134
Loss in iteration 12 : 0.028634773468554423
Loss in iteration 13 : 0.0254184006099986
Loss in iteration 14 : 0.022875483752948216
Loss in iteration 15 : 0.020828269401225223
Loss in iteration 16 : 0.019157184127649027
Loss in iteration 17 : 0.017766640047330876
Loss in iteration 18 : 0.01657632772377693
Loss in iteration 19 : 0.015521870508500155
Loss in iteration 20 : 0.014556564074971079
Loss in iteration 21 : 0.013651391229612762
Loss in iteration 22 : 0.012792711588326388
Loss in iteration 23 : 0.011978038199128592
Loss in iteration 24 : 0.011211020752962356
Loss in iteration 25 : 0.010496956595643038
Loss in iteration 26 : 0.009839740793805934
Loss in iteration 27 : 0.009240481999683636
Loss in iteration 28 : 0.00869745808292213
Loss in iteration 29 : 0.008206845759625167
Loss in iteration 30 : 0.007763692819739081
Loss in iteration 31 : 0.007362777534512015
Loss in iteration 32 : 0.006999194862903358
Loss in iteration 33 : 0.006668654111714505
Loss in iteration 34 : 0.0063675503580223475
Loss in iteration 35 : 0.006092895128520758
Loss in iteration 36 : 0.005842182727515694
Loss in iteration 37 : 0.005613246427700328
Loss in iteration 38 : 0.0054041357381098675
Loss in iteration 39 : 0.00521302793103305
Loss in iteration 40 : 0.005038175297149643
Loss in iteration 41 : 0.004877883230435674
Loss in iteration 42 : 0.004730511527994036
Loss in iteration 43 : 0.004594490733193738
Loss in iteration 44 : 0.004468345963535791
Loss in iteration 45 : 0.0043507218881432
Loss in iteration 46 : 0.004240404043684441
Loss in iteration 47 : 0.004136333302447686
Loss in iteration 48 : 0.004037611878853028
Loss in iteration 49 : 0.003943500656835026
Loss in iteration 50 : 0.0038534087461950506
Loss in iteration 51 : 0.0037668769710320093
Loss in iteration 52 : 0.0036835574344561803
Loss in iteration 53 : 0.0036031914055217017
Loss in iteration 54 : 0.003525587586051467
Loss in iteration 55 : 0.0034506024129668405
Loss in iteration 56 : 0.0033781235265464287
Loss in iteration 57 : 0.003308056977632222
Loss in iteration 58 : 0.003240318235361635
Loss in iteration 59 : 0.0031748266478840246
Loss in iteration 60 : 0.0031115027322772236
Loss in iteration 61 : 0.003050267532250957
Loss in iteration 62 : 0.0029910432688388436
Loss in iteration 63 : 0.002933754592380991
Loss in iteration 64 : 0.0028783298896760414
Loss in iteration 65 : 0.0028247022742350528
Loss in iteration 66 : 0.0027728100607756173
Loss in iteration 67 : 0.0027225966755993385
Loss in iteration 68 : 0.002674010068931203
Loss in iteration 69 : 0.0026270017683770863
Loss in iteration 70 : 0.00258152574576551
Loss in iteration 71 : 0.0025375372689225824
Loss in iteration 72 : 0.0024949918842622093
Loss in iteration 73 : 0.0024538446351983984
Loss in iteration 74 : 0.002414049574480192
Loss in iteration 75 : 0.0023755595832097463
Loss in iteration 76 : 0.002338326471036728
Loss in iteration 77 : 0.0023023013041006133
Loss in iteration 78 : 0.0022674348909393024
Loss in iteration 79 : 0.002233678351330731
Loss in iteration 80 : 0.0022009836972022396
Loss in iteration 81 : 0.0021693043659177444
Loss in iteration 82 : 0.0021385956617468167
Loss in iteration 83 : 0.002108815078533146
Loss in iteration 84 : 0.002079922493279204
Loss in iteration 85 : 0.0020518802348475015
Loss in iteration 86 : 0.002024653043154083
Loss in iteration 87 : 0.001998207941589875
Loss in iteration 88 : 0.0019725140489739923
Loss in iteration 89 : 0.0019475423575369133
Loss in iteration 90 : 0.0019232655009493952
Loss in iteration 91 : 0.0018996575320794806
Loss in iteration 92 : 0.0018766937248261522
Loss in iteration 93 : 0.0018543504088064057
Loss in iteration 94 : 0.0018326048404909117
Loss in iteration 95 : 0.0018114351100332366
Loss in iteration 96 : 0.0017908200797796567
Loss in iteration 97 : 0.0017707393483649893
Loss in iteration 98 : 0.0017511732333405341
Loss in iteration 99 : 0.0017321027652800443
Loss in iteration 100 : 0.0017135096870462814
Loss in iteration 101 : 0.0016953764531227695
Loss in iteration 102 : 0.0016776862253797822
Loss in iteration 103 : 0.00166042286313912
Loss in iteration 104 : 0.0016435709067597745
Loss in iteration 105 : 0.0016271155550749775
Loss in iteration 106 : 0.0016110426378100188
Loss in iteration 107 : 0.0015953385845834174
Loss in iteration 108 : 0.0015799903922696747
Loss in iteration 109 : 0.0015649855924276756
Loss in iteration 110 : 0.001550312220241787
Loss in iteration 111 : 0.0015359587860523307
Loss in iteration 112 : 0.0015219142501362644
Loss in iteration 113 : 0.0015081680009901366
Loss in iteration 114 : 0.0014947098370133647
Loss in iteration 115 : 0.0014815299512121432
Loss in iteration 116 : 0.0014686189183606695
Loss in iteration 117 : 0.001455967683963933
Loss in iteration 118 : 0.0014435675543563564
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.9937777777777778, training accuracy 1.0, time elapsed: 2249 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.42921664529181425
Loss in iteration 3 : 0.24586328054164075
Loss in iteration 4 : 0.17028535016490695
Loss in iteration 5 : 0.12861550897504853
Loss in iteration 6 : 0.10180780187374411
Loss in iteration 7 : 0.08309745741205424
Loss in iteration 8 : 0.0687590159253709
Loss in iteration 9 : 0.05716045826183405
Loss in iteration 10 : 0.04782477784656872
Loss in iteration 11 : 0.04059055458982521
Loss in iteration 12 : 0.03514698797232679
Loss in iteration 13 : 0.03103688885750735
Loss in iteration 14 : 0.02784297811307998
Loss in iteration 15 : 0.02527951003617852
Loss in iteration 16 : 0.023174449991082938
Loss in iteration 17 : 0.021420649076386297
Loss in iteration 18 : 0.01994000012427801
Loss in iteration 19 : 0.018667293080474844
Loss in iteration 20 : 0.017546666417166183
Loss in iteration 21 : 0.016532983694484627
Loss in iteration 22 : 0.015593553854661428
Loss in iteration 23 : 0.014708283514367213
Loss in iteration 24 : 0.013867937561853822
Loss in iteration 25 : 0.01307104237980496
Loss in iteration 26 : 0.012320339503634876
Loss in iteration 27 : 0.011619665110020822
Loss in iteration 28 : 0.010971808359188366
Loss in iteration 29 : 0.010377485713912216
Loss in iteration 30 : 0.009835239752992166
Loss in iteration 31 : 0.009341913725046839
Loss in iteration 32 : 0.008893352836324017
Loss in iteration 33 : 0.00848507517410147
Loss in iteration 34 : 0.008112772415072352
Loss in iteration 35 : 0.007772599675192597
Loss in iteration 36 : 0.007461278069111722
Loss in iteration 37 : 0.007176063811308811
Loss in iteration 38 : 0.006914643104739969
Loss in iteration 39 : 0.006675002975392842
Loss in iteration 40 : 0.006455313040416544
Loss in iteration 41 : 0.0062538375880646134
Loss in iteration 42 : 0.0060688844145155366
Loss in iteration 43 : 0.005898787807558916
Loss in iteration 44 : 0.005741917854845093
Loss in iteration 45 : 0.005596706202832306
Loss in iteration 46 : 0.005461678603517438
Loss in iteration 47 : 0.005335486200372982
Loss in iteration 48 : 0.005216929815530833
Loss in iteration 49 : 0.0051049739767932405
Loss in iteration 50 : 0.004998749696437892
Loss in iteration 51 : 0.004897546850527697
Loss in iteration 52 : 0.004800798286392841
Loss in iteration 53 : 0.004708058479262609
Loss in iteration 54 : 0.00461897971423401
Loss in iteration 55 : 0.004533288490084767
Loss in iteration 56 : 0.004450764263274745
Loss in iteration 57 : 0.004371221919906143
Loss in iteration 58 : 0.00429449861477684
Loss in iteration 59 : 0.004220444956421509
Loss in iteration 60 : 0.004148920014769685
Loss in iteration 61 : 0.004079789314839693
Loss in iteration 62 : 0.004012924852853223
Loss in iteration 63 : 0.003948206202024825
Loss in iteration 64 : 0.003885521920654935
Loss in iteration 65 : 0.003824770686478689
Loss in iteration 66 : 0.003765861812966376
Loss in iteration 67 : 0.0037087150186233316
Loss in iteration 68 : 0.0036532594941432665
Loss in iteration 69 : 0.0035994324315630154
Loss in iteration 70 : 0.003547177242208604
Loss in iteration 71 : 0.0034964417023714083
Loss in iteration 72 : 0.003447176238729982
Loss in iteration 73 : 0.003399332513277284
Loss in iteration 74 : 0.00335286240350382
Loss in iteration 75 : 0.003307717409476924
Loss in iteration 76 : 0.0032638484639356227
Loss in iteration 77 : 0.0032212060799743257
Loss in iteration 78 : 0.003179740745563225
Loss in iteration 79 : 0.003139403464681451
Loss in iteration 80 : 0.003100146348954033
Loss in iteration 81 : 0.003061923177964345
Loss in iteration 82 : 0.0030246898669887053
Loss in iteration 83 : 0.0029884048040172795
Loss in iteration 84 : 0.0029530290403709026
Loss in iteration 85 : 0.0029185263385793506
Loss in iteration 86 : 0.0028848630959265177
Loss in iteration 87 : 0.0028520081715400925
Loss in iteration 88 : 0.0028199326492043433
Loss in iteration 89 : 0.0027886095678715046
Loss in iteration 90 : 0.0027580136481746786
Loss in iteration 91 : 0.0027281210372962546
Loss in iteration 92 : 0.002698909087508979
Loss in iteration 93 : 0.0026703561766177825
Loss in iteration 94 : 0.0026424415722009287
Loss in iteration 95 : 0.002615145336511797
Loss in iteration 96 : 0.002588448265427018
Loss in iteration 97 : 0.002562331852938267
Loss in iteration 98 : 0.0025367782722244014
Loss in iteration 99 : 0.002511770365024849
Loss in iteration 100 : 0.002487291632513168
Loss in iteration 101 : 0.002463326222784986
Loss in iteration 102 : 0.0024398589121009165
Loss in iteration 103 : 0.0024168750789011497
Loss in iteration 104 : 0.0023943606711479314
Loss in iteration 105 : 0.002372302168653613
Loss in iteration 106 : 0.0023506865426840247
Loss in iteration 107 : 0.0023295012153218115
Loss in iteration 108 : 0.002308734020906129
Loss in iteration 109 : 0.002288373171433202
Loss in iteration 110 : 0.0022684072272141217
Loss in iteration 111 : 0.0022488250734441833
Loss in iteration 112 : 0.0022296159027268736
Loss in iteration 113 : 0.0022107692030803048
Loss in iteration 114 : 0.0021922747505719654
Loss in iteration 115 : 0.002174122605497141
Loss in iteration 116 : 0.002156303110931009
Loss in iteration 117 : 0.0021388068925269807
Loss in iteration 118 : 0.002121624858573558
Loss in iteration 119 : 0.0021047481995264444
Loss in iteration 120 : 0.002088168386468093
Loss in iteration 121 : 0.0020718771681831915
Loss in iteration 122 : 0.0020558665667531934
Loss in iteration 123 : 0.0020401288717484724
Loss in iteration 124 : 0.002024656633224889
Loss in iteration 125 : 0.002009442653810149
Loss in iteration 126 : 0.001994479980197383
Loss in iteration 127 : 0.0019797618943566612
Loss in iteration 128 : 0.001965281904739051
Loss in iteration 129 : 0.0019510337376923222
Loss in iteration 130 : 0.0019370113292435493
Loss in iteration 131 : 0.0019232088173391378
Loss in iteration 132 : 0.0019096205345746039
Testing accuracy  of updater 6 on alg 0 with rate 0.14 = 0.9928888888888889, training accuracy 0.9998571020291512, time elapsed: 2565 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.5083009596139715
Loss in iteration 3 : 0.33296258577023147
Loss in iteration 4 : 0.230420267277213
Loss in iteration 5 : 0.17534592428485982
Loss in iteration 6 : 0.1402249834733627
Loss in iteration 7 : 0.11564659283650207
Loss in iteration 8 : 0.09755586035014471
Loss in iteration 9 : 0.08344019476071282
Loss in iteration 10 : 0.07188834821646849
Loss in iteration 11 : 0.062242210687032264
Loss in iteration 12 : 0.05422173411134561
Loss in iteration 13 : 0.047660151328725225
Loss in iteration 14 : 0.04236608737300055
Loss in iteration 15 : 0.038102959456901134
Loss in iteration 16 : 0.03463436781315604
Loss in iteration 17 : 0.0317675541413458
Loss in iteration 18 : 0.029364876040614885
Loss in iteration 19 : 0.027331777701474377
Loss in iteration 20 : 0.025599041305753918
Loss in iteration 21 : 0.024109819702299976
Loss in iteration 22 : 0.022813704762761337
Loss in iteration 23 : 0.02166588755596791
Loss in iteration 24 : 0.020628577262115977
Loss in iteration 25 : 0.019672516056897755
Loss in iteration 26 : 0.01877746636824645
Loss in iteration 27 : 0.0179314172773773
Loss in iteration 28 : 0.01712880685691267
Loss in iteration 29 : 0.016368296483622143
Loss in iteration 30 : 0.01565062903124086
Loss in iteration 31 : 0.014976947427090607
Loss in iteration 32 : 0.014347739750668243
Loss in iteration 33 : 0.013762389679616698
Loss in iteration 34 : 0.01321919014060057
Loss in iteration 35 : 0.012715631830569089
Loss in iteration 36 : 0.012248791080137414
Loss in iteration 37 : 0.011815688042407764
Loss in iteration 38 : 0.011413542812052426
Loss in iteration 39 : 0.011039907448046562
Loss in iteration 40 : 0.01069268770940641
Loss in iteration 41 : 0.010370087768318461
Loss in iteration 42 : 0.01007051646398577
Loss in iteration 43 : 0.009792488983871443
Loss in iteration 44 : 0.009534547772216193
Loss in iteration 45 : 0.009295214908826593
Loss in iteration 46 : 0.009072977970893174
Loss in iteration 47 : 0.008866304023235969
Loss in iteration 48 : 0.00867367232129774
Loss in iteration 49 : 0.008493615226122232
Loss in iteration 50 : 0.00832475796555334
Loss in iteration 51 : 0.00816585034233746
Loss in iteration 52 : 0.008015786467327172
Loss in iteration 53 : 0.007873611442567385
Loss in iteration 54 : 0.007738516210697867
Loss in iteration 55 : 0.007609823311139416
Loss in iteration 56 : 0.007486967005193086
Loss in iteration 57 : 0.007369471248224685
Loss in iteration 58 : 0.007256928475428217
Loss in iteration 59 : 0.007148981339654963
Loss in iteration 60 : 0.0070453086004089276
Loss in iteration 61 : 0.006945615481958877
Loss in iteration 62 : 0.006849628113435747
Loss in iteration 63 : 0.0067570911962250135
Loss in iteration 64 : 0.0066677678240082494
Loss in iteration 65 : 0.006581440378646021
Loss in iteration 66 : 0.006497911585604716
Loss in iteration 67 : 0.00641700507008981
Loss in iteration 68 : 0.0063385650457327474
Loss in iteration 69 : 0.006262455038981375
Loss in iteration 70 : 0.00618855576792166
Loss in iteration 71 : 0.0061167624354103185
Loss in iteration 72 : 0.006046981760403249
Loss in iteration 73 : 0.005979129067817215
Loss in iteration 74 : 0.0059131257034617845
Loss in iteration 75 : 0.005848896956955622
Loss in iteration 76 : 0.005786370581800637
Loss in iteration 77 : 0.005725475914184857
Loss in iteration 78 : 0.0056661435219345635
Loss in iteration 79 : 0.005608305268347999
Loss in iteration 80 : 0.005551894653618251
Loss in iteration 81 : 0.005496847296642454
Loss in iteration 82 : 0.0054431014372210975
Loss in iteration 83 : 0.005390598366830708
Loss in iteration 84 : 0.0053392827291404025
Loss in iteration 85 : 0.005289102663885139
Loss in iteration 86 : 0.005240009795584978
Loss in iteration 87 : 0.005191959089433291
Loss in iteration 88 : 0.005144908609484337
Loss in iteration 89 : 0.005098819219339289
Loss in iteration 90 : 0.005053654264071618
Loss in iteration 91 : 0.005009379265910207
Loss in iteration 92 : 0.004965961657164856
Loss in iteration 93 : 0.004923370563881975
Loss in iteration 94 : 0.00488157664428773
Loss in iteration 95 : 0.004840551978305338
Loss in iteration 96 : 0.004800269998942341
Loss in iteration 97 : 0.0047607054533121956
Loss in iteration 98 : 0.004721834380307359
Loss in iteration 99 : 0.00468363409304806
Loss in iteration 100 : 0.00464608315662957
Loss in iteration 101 : 0.004609161354781494
Loss in iteration 102 : 0.004572849642287546
Loss in iteration 103 : 0.004537130082951888
Loss in iteration 104 : 0.004501985775233704
Loss in iteration 105 : 0.004467400769245403
Loss in iteration 106 : 0.004433359979590778
Loss in iteration 107 : 0.00439984909858177
Loss in iteration 108 : 0.004366854513861951
Loss in iteration 109 : 0.004334363233565014
Loss in iteration 110 : 0.004302362821036023
Loss in iteration 111 : 0.004270841340016664
Loss in iteration 112 : 0.004239787310178531
Loss in iteration 113 : 0.004209189672078243
Loss in iteration 114 : 0.004179037760052907
Loss in iteration 115 : 0.004149321281284421
Loss in iteration 116 : 0.004120030299213327
Loss in iteration 117 : 0.004091155219630292
Loss in iteration 118 : 0.004062686778058048
Loss in iteration 119 : 0.004034616027397041
Loss in iteration 120 : 0.004006934325188189
Loss in iteration 121 : 0.003979633320198636
Loss in iteration 122 : 0.003952704938330147
Loss in iteration 123 : 0.003926141368063888
Loss in iteration 124 : 0.003899935045785719
Loss in iteration 125 : 0.0038740786413864374
Loss in iteration 126 : 0.0038485650445159347
Loss in iteration 127 : 0.0038233873518043623
Loss in iteration 128 : 0.003798538855269088
Loss in iteration 129 : 0.003774013032018207
Loss in iteration 130 : 0.0037498035352584035
Loss in iteration 131 : 0.0037259041865262887
Loss in iteration 132 : 0.003702308968995792
Loss in iteration 133 : 0.0036790120216740464
Loss in iteration 134 : 0.003656007634282104
Loss in iteration 135 : 0.003633290242623146
Loss in iteration 136 : 0.0036108544242637926
Loss in iteration 137 : 0.003588694894388327
Loss in iteration 138 : 0.003566806501725217
Loss in iteration 139 : 0.0035451842244847657
Loss in iteration 140 : 0.003523823166282834
Loss in iteration 141 : 0.003502718552054485
Loss in iteration 142 : 0.0034818657239824424
Loss in iteration 143 : 0.003461260137477649
Loss in iteration 144 : 0.0034408973572538156
Loss in iteration 145 : 0.0034207730535358804
Loss in iteration 146 : 0.003400882998435342
Loss in iteration 147 : 0.003381223062516042
Loss in iteration 148 : 0.0033617892115624334
Loss in iteration 149 : 0.003342577503552243
Loss in iteration 150 : 0.003323584085825449
Loss in iteration 151 : 0.003304805192434784
Loss in iteration 152 : 0.0032862371416579753
Loss in iteration 153 : 0.0032678763336496924
Loss in iteration 154 : 0.0032497192482110132
Loss in iteration 155 : 0.0032317624426561877
Loss in iteration 156 : 0.0032140025497591116
Loss in iteration 157 : 0.003196436275766087
Loss in iteration 158 : 0.0031790603984646368
Loss in iteration 159 : 0.00316187176530264
Loss in iteration 160 : 0.0031448672915544472
Loss in iteration 161 : 0.0031280439585337173
Loss in iteration 162 : 0.0031113988118538507
Loss in iteration 163 : 0.0030949289597380822
Loss in iteration 164 : 0.0030786315713814505
Testing accuracy  of updater 6 on alg 0 with rate 0.08000000000000002 = 0.992, training accuracy 0.9997142040583024, time elapsed: 3158 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6354768835339416
Loss in iteration 3 : 0.5510957954736928
Loss in iteration 4 : 0.46338808933060893
Loss in iteration 5 : 0.3847602614884552
Loss in iteration 6 : 0.32088875003252043
Loss in iteration 7 : 0.27231425530376296
Loss in iteration 8 : 0.2360532968770293
Loss in iteration 9 : 0.208372550021142
Loss in iteration 10 : 0.1864053757725279
Loss in iteration 11 : 0.168344323734304
Loss in iteration 12 : 0.1531155804374867
Loss in iteration 13 : 0.14006244037459792
Loss in iteration 14 : 0.1287508534836689
Loss in iteration 15 : 0.1188692257465417
Loss in iteration 16 : 0.11018037377549103
Loss in iteration 17 : 0.10249806579744802
Loss in iteration 18 : 0.09567407151778437
Loss in iteration 19 : 0.089589563937775
Loss in iteration 20 : 0.08414852643495783
Loss in iteration 21 : 0.07927239739217119
Loss in iteration 22 : 0.07489574178985982
Loss in iteration 23 : 0.07096288353117716
Loss in iteration 24 : 0.06742542681668925
Loss in iteration 25 : 0.06424054643240563
Loss in iteration 26 : 0.061369882926140804
Loss in iteration 27 : 0.05877886196038007
Loss in iteration 28 : 0.05643627276026226
Loss in iteration 29 : 0.05431397972141422
Loss in iteration 30 : 0.052386688396363144
Loss in iteration 31 : 0.05063172823622637
Loss in iteration 32 : 0.04902884204910768
Loss in iteration 33 : 0.04755998540292589
Loss in iteration 34 : 0.04620914178059278
Loss in iteration 35 : 0.0449621562000802
Loss in iteration 36 : 0.04380658551468749
Loss in iteration 37 : 0.04273156034464231
Loss in iteration 38 : 0.04172765256254527
Loss in iteration 39 : 0.04078674329248882
Loss in iteration 40 : 0.03990188870573029
Loss in iteration 41 : 0.03906718358162649
Loss in iteration 42 : 0.03827762488923427
Loss in iteration 43 : 0.03752897907357322
Loss in iteration 44 : 0.03681765716124008
Loss in iteration 45 : 0.03614060134882719
Loss in iteration 46 : 0.03549518567653104
Loss in iteration 47 : 0.034879132044863335
Loss in iteration 48 : 0.03429044150348465
Loss in iteration 49 : 0.0337273396481673
Loss in iteration 50 : 0.03318823422617261
Loss in iteration 51 : 0.032671682699554214
Loss in iteration 52 : 0.032176367505337734
Loss in iteration 53 : 0.03170107699244197
Loss in iteration 54 : 0.031244690402813555
Loss in iteration 55 : 0.030806165700135366
Loss in iteration 56 : 0.030384529456261654
Loss in iteration 57 : 0.029978868333570805
Loss in iteration 58 : 0.02958832192875307
Loss in iteration 59 : 0.0292120768712978
Loss in iteration 60 : 0.028849362115217336
Loss in iteration 61 : 0.028499445350816534
Loss in iteration 62 : 0.028161630421660172
Loss in iteration 63 : 0.02783525558396881
Loss in iteration 64 : 0.02751969240873958
Loss in iteration 65 : 0.027214345110722134
Loss in iteration 66 : 0.02691865009575703
Loss in iteration 67 : 0.026632075546182098
Loss in iteration 68 : 0.026354120906781273
Loss in iteration 69 : 0.026084316183350064
Loss in iteration 70 : 0.025822221014873113
Loss in iteration 71 : 0.025567423522490928
Loss in iteration 72 : 0.02531953896996249
Loss in iteration 73 : 0.02507820828957232
Loss in iteration 74 : 0.02484309653480502
Loss in iteration 75 : 0.024613891318559008
Loss in iteration 76 : 0.024390301286048886
Loss in iteration 77 : 0.024172054657986258
Loss in iteration 78 : 0.023958897865038693
Loss in iteration 79 : 0.023750594281271034
Loss in iteration 80 : 0.023546923053807026
Loss in iteration 81 : 0.023347678019059143
Loss in iteration 82 : 0.023152666692581977
Loss in iteration 83 : 0.02296170931940576
Loss in iteration 84 : 0.02277463797376314
Loss in iteration 85 : 0.022591295700477503
Loss in iteration 86 : 0.022411535694028435
Loss in iteration 87 : 0.022235220514711034
Loss in iteration 88 : 0.022062221343861807
Loss in iteration 89 : 0.02189241728159353
Loss in iteration 90 : 0.021725694690842092
Loss in iteration 91 : 0.021561946590944812
Loss in iteration 92 : 0.02140107210270305
Loss in iteration 93 : 0.02124297594525237
Loss in iteration 94 : 0.021087567983367427
Loss in iteration 95 : 0.020934762822315933
Loss in iteration 96 : 0.020784479446219004
Loss in iteration 97 : 0.020636640895160995
Loss in iteration 98 : 0.0204911739760319
Loss in iteration 99 : 0.020348009002226233
Loss in iteration 100 : 0.020207079557775162
Loss in iteration 101 : 0.020068322282136324
Loss in iteration 102 : 0.019931676672600995
Loss in iteration 103 : 0.01979708490200032
Loss in iteration 104 : 0.019664491650027146
Loss in iteration 105 : 0.019533843946994558
Loss in iteration 106 : 0.01940509102920566
Loss in iteration 107 : 0.01927818420531734
Loss in iteration 108 : 0.01915307673316633
Loss in iteration 109 : 0.019029723706520948
Loss in iteration 110 : 0.018908081951161147
Loss in iteration 111 : 0.0187881099296084
Loss in iteration 112 : 0.018669767653751505
Loss in iteration 113 : 0.01855301660456433
Loss in iteration 114 : 0.018437819658099333
Loss in iteration 115 : 0.01832414101696513
Loss in iteration 116 : 0.018211946146557922
Loss in iteration 117 : 0.018101201715403095
Loss in iteration 118 : 0.017991875539065443
Loss in iteration 119 : 0.01788393652719532
Loss in iteration 120 : 0.01777735463337835
Loss in iteration 121 : 0.017672100807547362
Loss in iteration 122 : 0.017568146950786193
Loss in iteration 123 : 0.017465465872408294
Loss in iteration 124 : 0.01736403124922541
Loss in iteration 125 : 0.017263817586939435
Loss in iteration 126 : 0.017164800183593242
Loss in iteration 127 : 0.017066955095011088
Loss in iteration 128 : 0.01697025910214868
Loss in iteration 129 : 0.016874689680261606
Loss in iteration 130 : 0.016780224969789776
Loss in iteration 131 : 0.016686843748849337
Loss in iteration 132 : 0.016594525407220048
Loss in iteration 133 : 0.01650324992171847
Loss in iteration 134 : 0.016412997832851457
Loss in iteration 135 : 0.01632375022265463
Loss in iteration 136 : 0.016235488693628952
Loss in iteration 137 : 0.016148195348700375
Loss in iteration 138 : 0.016061852772138268
Loss in iteration 139 : 0.015976444011376763
Loss in iteration 140 : 0.015891952559692477
Loss in iteration 141 : 0.015808362339696905
Loss in iteration 142 : 0.015725657687606694
Loss in iteration 143 : 0.01564382333825775
Loss in iteration 144 : 0.01556284441082998
Loss in iteration 145 : 0.015482706395250104
Loss in iteration 146 : 0.01540339513924023
Loss in iteration 147 : 0.015324896835979266
Loss in iteration 148 : 0.015247198012344214
Loss in iteration 149 : 0.015170285517698482
Loss in iteration 150 : 0.015094146513195312
Loss in iteration 151 : 0.015018768461565083
Loss in iteration 152 : 0.014944139117356967
Loss in iteration 153 : 0.014870246517606852
Loss in iteration 154 : 0.014797078972906087
Loss in iteration 155 : 0.01472462505884711
Loss in iteration 156 : 0.014652873607824128
Loss in iteration 157 : 0.014581813701169613
Loss in iteration 158 : 0.014511434661608183
Loss in iteration 159 : 0.014441726046012078
Loss in iteration 160 : 0.014372677638442738
Loss in iteration 161 : 0.01430427944346506
Loss in iteration 162 : 0.014236521679721205
Loss in iteration 163 : 0.014169394773751542
Loss in iteration 164 : 0.014102889354051488
Loss in iteration 165 : 0.01403699624535296
Loss in iteration 166 : 0.013971706463119737
Loss in iteration 167 : 0.013907011208246804
Loss in iteration 168 : 0.013842901861953777
Loss in iteration 169 : 0.013779369980863094
Loss in iteration 170 : 0.013716407292254295
Loss in iteration 171 : 0.013654005689485884
Loss in iteration 172 : 0.013592157227576803
Loss in iteration 173 : 0.013530854118940547
Loss in iteration 174 : 0.013470088729264303
Loss in iteration 175 : 0.013409853573527234
Loss in iteration 176 : 0.013350141312151535
Loss in iteration 177 : 0.013290944747280434
Loss in iteration 178 : 0.013232256819178417
Loss in iteration 179 : 0.013174070602747662
Loss in iteration 180 : 0.01311637930415718
Loss in iteration 181 : 0.013059176257579104
Loss in iteration 182 : 0.013002454922028596
Loss in iteration 183 : 0.012946208878302988
Loss in iteration 184 : 0.012890431826016539
Loss in iteration 185 : 0.012835117580726875
Loss in iteration 186 : 0.012780260071149751
Loss in iteration 187 : 0.012725853336458816
Loss in iteration 188 : 0.012671891523667024
Loss in iteration 189 : 0.012618368885086563
Loss in iteration 190 : 0.012565279775864645
Loss in iteration 191 : 0.012512618651591867
Loss in iteration 192 : 0.012460380065981068
Loss in iteration 193 : 0.012408558668613328
Loss in iteration 194 : 0.012357149202749555
Loss in iteration 195 : 0.012306146503204543
Loss in iteration 196 : 0.012255545494281716
Loss in iteration 197 : 0.012205341187766375
Loss in iteration 198 : 0.01215552868097521
Loss in iteration 199 : 0.012106103154860584
Loss in iteration 200 : 0.012057059872166953
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.9848888888888889, training accuracy 0.999285510145756, time elapsed: 3881 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 12.648730475199672
Loss in iteration 3 : 2.055630161094476
Loss in iteration 4 : 3.3946705368507692
Loss in iteration 5 : 5.199504424964679
Loss in iteration 6 : 2.869943660810852
Loss in iteration 7 : 1.554424804863215
Loss in iteration 8 : 0.8698107688906965
Loss in iteration 9 : 0.7844340305229388
Loss in iteration 10 : 1.0406213991043967
Loss in iteration 11 : 1.2112397023188373
Loss in iteration 12 : 1.0801962754530574
Loss in iteration 13 : 0.7738182304863926
Loss in iteration 14 : 0.48545488035143963
Loss in iteration 15 : 0.30396209707904565
Loss in iteration 16 : 0.2086491166348495
Loss in iteration 17 : 0.17632375443553402
Loss in iteration 18 : 0.1701801432822565
Loss in iteration 19 : 0.18174594368094682
Loss in iteration 20 : 0.1939610328703869
Loss in iteration 21 : 0.20150396519053554
Loss in iteration 22 : 0.20114991471780289
Loss in iteration 23 : 0.19292607074242754
Loss in iteration 24 : 0.1779308468703368
Loss in iteration 25 : 0.16050965383175858
Loss in iteration 26 : 0.1442342052543157
Loss in iteration 27 : 0.13012267385178353
Loss in iteration 28 : 0.11993041589699985
Loss in iteration 29 : 0.11166668197503538
Loss in iteration 30 : 0.10382985246044134
Loss in iteration 31 : 0.09782555829295997
Loss in iteration 32 : 0.09422207392671755
Loss in iteration 33 : 0.09259024869804686
Loss in iteration 34 : 0.09237404391606263
Loss in iteration 35 : 0.09216872037998253
Loss in iteration 36 : 0.0917867868983219
Loss in iteration 37 : 0.09126732367251346
Loss in iteration 38 : 0.0906327972071967
Loss in iteration 39 : 0.08985970943576199
Loss in iteration 40 : 0.088906681831963
Loss in iteration 41 : 0.08775745786848936
Loss in iteration 42 : 0.08642679293144281
Loss in iteration 43 : 0.08494611827162735
Loss in iteration 44 : 0.0833466596628936
Loss in iteration 45 : 0.08165305470197919
Loss in iteration 46 : 0.07988854088920792
Loss in iteration 47 : 0.07807688366232458
Loss in iteration 48 : 0.07623682389376224
Loss in iteration 49 : 0.07438072280024767
Loss in iteration 50 : 0.07254095886493417
Loss in iteration 51 : 0.07081212935057803
Loss in iteration 52 : 0.06919226670641282
Loss in iteration 53 : 0.06760043848712477
Loss in iteration 54 : 0.06601273898893399
Loss in iteration 55 : 0.06442531033497832
Loss in iteration 56 : 0.06283737211067843
Loss in iteration 57 : 0.0612486671248818
Loss in iteration 58 : 0.05965911793967381
Loss in iteration 59 : 0.05806892432828529
Loss in iteration 60 : 0.056479281381901766
Loss in iteration 61 : 0.05489531733047785
Loss in iteration 62 : 0.05333507450156025
Loss in iteration 63 : 0.05183696003869266
Loss in iteration 64 : 0.050427018422393936
Loss in iteration 65 : 0.04910793666305956
Loss in iteration 66 : 0.0478908442753768
Loss in iteration 67 : 0.046751705339526814
Loss in iteration 68 : 0.045640218107639546
Loss in iteration 69 : 0.04453225490277036
Loss in iteration 70 : 0.043443909758905844
Loss in iteration 71 : 0.04240901149857391
Loss in iteration 72 : 0.04141282113411759
Loss in iteration 73 : 0.04042213808173918
Loss in iteration 74 : 0.0394252966206103
Loss in iteration 75 : 0.03842279231131073
Loss in iteration 76 : 0.03741953336102715
Loss in iteration 77 : 0.036422928005590736
Loss in iteration 78 : 0.03544305164246473
Loss in iteration 79 : 0.034493549362682105
Loss in iteration 80 : 0.03359136186797673
Loss in iteration 81 : 0.03274882322893415
Loss in iteration 82 : 0.03195874765229195
Loss in iteration 83 : 0.031200060984756434
Loss in iteration 84 : 0.030455910521954063
Loss in iteration 85 : 0.02971744125003536
Loss in iteration 86 : 0.028980513224954086
Loss in iteration 87 : 0.028243268302230132
Loss in iteration 88 : 0.02750497507066389
Loss in iteration 89 : 0.026765461090606135
Loss in iteration 90 : 0.026024810927835224
Loss in iteration 91 : 0.025283214847372636
Loss in iteration 92 : 0.02454090159409183
Loss in iteration 93 : 0.023798104461135044
Loss in iteration 94 : 0.02305503310863517
Loss in iteration 95 : 0.022311846329166055
Loss in iteration 96 : 0.021568632138958533
Loss in iteration 97 : 0.020825401404615786
Loss in iteration 98 : 0.020082095861983708
Loss in iteration 99 : 0.019338606399931294
Loss in iteration 100 : 0.01859479541020078
Loss in iteration 101 : 0.0178505177546046
Loss in iteration 102 : 0.017105637118836642
Loss in iteration 103 : 0.016360036928990296
Loss in iteration 104 : 0.01561362734689118
Loss in iteration 105 : 0.01486635435921426
Loss in iteration 106 : 0.014118234834935978
Loss in iteration 107 : 0.013369523754424871
Loss in iteration 108 : 0.012621492295898285
Loss in iteration 109 : 0.01187977127814451
Loss in iteration 110 : 0.011164866226849625
Loss in iteration 111 : 0.010516769218752638
Loss in iteration 112 : 0.009943385982595544
Loss in iteration 113 : 0.009409429674772058
Loss in iteration 114 : 0.008892801505112787
Loss in iteration 115 : 0.008388617518180581
Loss in iteration 116 : 0.00789995115921823
Loss in iteration 117 : 0.00743624184674124
Loss in iteration 118 : 0.007012094600877048
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.992, training accuracy 0.9995713060874536, time elapsed: 2396 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.9786773385587937
Loss in iteration 3 : 0.212744637523873
Loss in iteration 4 : 0.42450304906785563
Loss in iteration 5 : 0.25625085226434624
Loss in iteration 6 : 0.10603892056645027
Loss in iteration 7 : 0.10015294502175195
Loss in iteration 8 : 0.1412790603557383
Loss in iteration 9 : 0.1141338809380434
Loss in iteration 10 : 0.06420161551026493
Loss in iteration 11 : 0.0364910070843148
Loss in iteration 12 : 0.03149587826328503
Loss in iteration 13 : 0.035026939246449784
Loss in iteration 14 : 0.039962661490943914
Loss in iteration 15 : 0.04094515967999868
Loss in iteration 16 : 0.03709724400332861
Loss in iteration 17 : 0.03083198077959653
Loss in iteration 18 : 0.02499060269993791
Loss in iteration 19 : 0.020562182323391156
Loss in iteration 20 : 0.0180518819223825
Loss in iteration 21 : 0.01734864589588333
Loss in iteration 22 : 0.01702437705520549
Loss in iteration 23 : 0.016557619889113653
Loss in iteration 24 : 0.01588153880728855
Loss in iteration 25 : 0.015012041800790632
Loss in iteration 26 : 0.013996054507258722
Loss in iteration 27 : 0.012897949484725028
Loss in iteration 28 : 0.011800042517972528
Loss in iteration 29 : 0.010787662299790298
Loss in iteration 30 : 0.009886133974098292
Loss in iteration 31 : 0.00904917465073542
Loss in iteration 32 : 0.008229495386150804
Loss in iteration 33 : 0.007409735629570083
Loss in iteration 34 : 0.006599272930262588
Loss in iteration 35 : 0.005837533962149343
Loss in iteration 36 : 0.005198318637765174
Loss in iteration 37 : 0.004747588133507134
Loss in iteration 38 : 0.004477608574092967
Loss in iteration 39 : 0.004318270385846476
Loss in iteration 40 : 0.004184608865010035
Loss in iteration 41 : 0.00401516752405393
Loss in iteration 42 : 0.003782072257443216
Loss in iteration 43 : 0.003483447283399787
Loss in iteration 44 : 0.0031341952740742947
Loss in iteration 45 : 0.0027607508164067198
Loss in iteration 46 : 0.0023954021076749757
Loss in iteration 47 : 0.0020663555836156655
Loss in iteration 48 : 0.0017875204645617026
Loss in iteration 49 : 0.0015568615064112448
Loss in iteration 50 : 0.001363416140483427
Loss in iteration 51 : 0.001194994169832675
Loss in iteration 52 : 0.001042402400714848
Loss in iteration 53 : 9.003843954646023E-4
Loss in iteration 54 : 7.669630575220903E-4
Loss in iteration 55 : 6.425570032978918E-4
Loss in iteration 56 : 5.292454755888273E-4
Loss in iteration 57 : 4.299461518225812E-4
Loss in iteration 58 : 3.472332506733884E-4
Loss in iteration 59 : 2.82019941328722E-4
Loss in iteration 60 : 2.32908029614309E-4
Loss in iteration 61 : 1.9673386445580524E-4
Loss in iteration 62 : 1.6981206615767882E-4
Loss in iteration 63 : 1.489628861053673E-4
Loss in iteration 64 : 1.3192807862458657E-4
Loss in iteration 65 : 1.1732454173931223E-4
Testing accuracy  of updater 7 on alg 0 with rate 14.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1265 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.576491888341324
Loss in iteration 3 : 0.15824668272115577
Loss in iteration 4 : 0.27794804964837105
Loss in iteration 5 : 0.14311638066255944
Loss in iteration 6 : 0.06971476955652355
Loss in iteration 7 : 0.08846544011104308
Loss in iteration 8 : 0.09340718142434863
Loss in iteration 9 : 0.05656889417430409
Loss in iteration 10 : 0.03068830263496271
Loss in iteration 11 : 0.02391990646345626
Loss in iteration 12 : 0.02554675828266626
Loss in iteration 13 : 0.02848557650560685
Loss in iteration 14 : 0.02924148477183664
Loss in iteration 15 : 0.02695327217190645
Loss in iteration 16 : 0.022730375049398233
Loss in iteration 17 : 0.01850590745188091
Loss in iteration 18 : 0.015430025828013174
Loss in iteration 19 : 0.013815119528031987
Loss in iteration 20 : 0.01338996346956102
Loss in iteration 21 : 0.013328500085234994
Loss in iteration 22 : 0.013137203209561861
Loss in iteration 23 : 0.012686612425909887
Loss in iteration 24 : 0.01199473816234231
Loss in iteration 25 : 0.011143943872411407
Loss in iteration 26 : 0.010229034937718704
Loss in iteration 27 : 0.009323018578572416
Loss in iteration 28 : 0.008477448286848327
Loss in iteration 29 : 0.007718289869254355
Loss in iteration 30 : 0.007037382929375589
Loss in iteration 31 : 0.006415127076665345
Loss in iteration 32 : 0.00584300182705255
Loss in iteration 33 : 0.005325350224705348
Loss in iteration 34 : 0.0048730782725434595
Loss in iteration 35 : 0.004494661572912219
Loss in iteration 36 : 0.004186894045878233
Loss in iteration 37 : 0.0039335879780035225
Loss in iteration 38 : 0.0037144191163854473
Loss in iteration 39 : 0.003512419422170388
Loss in iteration 40 : 0.0033149678290036247
Loss in iteration 41 : 0.003112904083090811
Loss in iteration 42 : 0.0029005150984450835
Loss in iteration 43 : 0.0026760115632802643
Loss in iteration 44 : 0.0024417364813756276
Loss in iteration 45 : 0.0022036960216708598
Loss in iteration 46 : 0.001970239556754619
Loss in iteration 47 : 0.001749959170517306
Loss in iteration 48 : 0.0015493860710578805
Loss in iteration 49 : 0.0013715109776153214
Loss in iteration 50 : 0.0012158151764358066
Loss in iteration 51 : 0.001079500378258244
Loss in iteration 52 : 9.589976025137255E-4
Loss in iteration 53 : 8.510557103051243E-4
Loss in iteration 54 : 7.53234358711907E-4
Loss in iteration 55 : 6.639522798491938E-4
Loss in iteration 56 : 5.82311225998118E-4
Loss in iteration 57 : 5.078637520800465E-4
Loss in iteration 58 : 4.404196839390195E-4
Loss in iteration 59 : 3.799210555778687E-4
Loss in iteration 60 : 3.2636508903123595E-4
Loss in iteration 61 : 2.797341273687645E-4
Loss in iteration 62 : 2.3991012483611977E-4
Loss in iteration 63 : 2.0659181321801032E-4
Loss in iteration 64 : 1.7925647034098326E-4
Loss in iteration 65 : 1.5719138219683868E-4
Loss in iteration 66 : 1.395796364422306E-4
Loss in iteration 67 : 1.2559906949541905E-4
Loss in iteration 68 : 1.1449872507375013E-4
Loss in iteration 69 : 1.0563944637340881E-4
Testing accuracy  of updater 7 on alg 0 with rate 8.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1269 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.338629798166236
Loss in iteration 3 : 0.17618532914598764
Loss in iteration 4 : 0.13059911181382405
Loss in iteration 5 : 0.09196896128913495
Loss in iteration 6 : 0.07096083938471937
Loss in iteration 7 : 0.055463619500065196
Loss in iteration 8 : 0.04274684717287934
Loss in iteration 9 : 0.033415272144736545
Loss in iteration 10 : 0.027247051856188316
Loss in iteration 11 : 0.02328939025836533
Loss in iteration 12 : 0.020629980819783726
Loss in iteration 13 : 0.018672541564548825
Loss in iteration 14 : 0.017097579331822103
Loss in iteration 15 : 0.015758271334757063
Loss in iteration 16 : 0.014594011882660761
Loss in iteration 17 : 0.013575110769547953
Loss in iteration 18 : 0.012675945746164412
Loss in iteration 19 : 0.01186842264288839
Loss in iteration 20 : 0.01112561691093933
Loss in iteration 21 : 0.010427213754261467
Loss in iteration 22 : 0.00976230626060973
Loss in iteration 23 : 0.009128712212109523
Loss in iteration 24 : 0.008529982383837408
Loss in iteration 25 : 0.00797178885955201
Loss in iteration 26 : 0.007458995507352021
Loss in iteration 27 : 0.006994016959370793
Loss in iteration 28 : 0.006576438531992632
Loss in iteration 29 : 0.006203490074675626
Loss in iteration 30 : 0.00587089188664155
Loss in iteration 31 : 0.00557371119285014
Loss in iteration 32 : 0.005307031589281111
Loss in iteration 33 : 0.0050663706510085844
Loss in iteration 34 : 0.004847868825932431
Loss in iteration 35 : 0.004648316017893572
Loss in iteration 36 : 0.0044650876613349065
Loss in iteration 37 : 0.00429604515341701
Loss in iteration 38 : 0.004139433078508552
Loss in iteration 39 : 0.003993788060469948
Loss in iteration 40 : 0.003857863979533394
Loss in iteration 41 : 0.003730573912999498
Loss in iteration 42 : 0.003610947708179668
Loss in iteration 43 : 0.0034981035830024833
Loss in iteration 44 : 0.0033912317215798087
Loss in iteration 45 : 0.0032895873866865175
Loss in iteration 46 : 0.0031924907761144436
Loss in iteration 47 : 0.003099330841202761
Loss in iteration 48 : 0.003009570572967628
Loss in iteration 49 : 0.0029227517574676553
Loss in iteration 50 : 0.0028384977908032277
Loss in iteration 51 : 0.002756513727672396
Loss in iteration 52 : 0.002676583254142005
Loss in iteration 53 : 0.00259856269579976
Loss in iteration 54 : 0.002522372487865773
Loss in iteration 55 : 0.0024479867460783845
Loss in iteration 56 : 0.0023754216934506827
Loss in iteration 57 : 0.002304723728650674
Loss in iteration 58 : 0.0022359578794329087
Loss in iteration 59 : 0.002169197283646654
Loss in iteration 60 : 0.0021045141964863836
Loss in iteration 61 : 0.002041972852203335
Loss in iteration 62 : 0.001981624328235871
Loss in iteration 63 : 0.0019235033863305732
Loss in iteration 64 : 0.0018676271143298825
Loss in iteration 65 : 0.001813995076917998
Loss in iteration 66 : 0.001762590612342891
Loss in iteration 67 : 0.0017133828877262415
Loss in iteration 68 : 0.0016663293444825228
Loss in iteration 69 : 0.0016213782185876002
Loss in iteration 70 : 0.0015784708954684245
Loss in iteration 71 : 0.0015375439426737474
Loss in iteration 72 : 0.0014985307432075028
Loss in iteration 73 : 0.0014613627195646658
Loss in iteration 74 : 0.0014259701880514522
Loss in iteration 75 : 0.0013922829135394478
Loss in iteration 76 : 0.0013602304479254461
Loss in iteration 77 : 0.00132974233463109
Loss in iteration 78 : 0.0013007482506592804
Loss in iteration 79 : 0.0012731781412120377
Loss in iteration 80 : 0.001246962383278995
Loss in iteration 81 : 0.0012220319967148876
Loss in iteration 82 : 0.0011983189060138211
Loss in iteration 83 : 0.0011757562443020896
Loss in iteration 84 : 0.001154278683357572
Loss in iteration 85 : 0.0011338227695563308
Loss in iteration 86 : 0.0011143272450257675
Loss in iteration 87 : 0.0010957333352362012
Loss in iteration 88 : 0.0010779849879928784
Loss in iteration 89 : 0.0010610290535325532
Loss in iteration 90 : 0.0010448154004876063
Loss in iteration 91 : 0.001029296967301242
Loss in iteration 92 : 0.001014429752843554
Loss in iteration 93 : 0.0010001727532259548
Loss in iteration 94 : 9.864878540226578E-4
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.9928888888888889, training accuracy 1.0, time elapsed: 1780 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.3847114432358738
Loss in iteration 3 : 0.20534019271644968
Loss in iteration 4 : 0.14361239867213257
Loss in iteration 5 : 0.10464377386684844
Loss in iteration 6 : 0.07973450845746186
Loss in iteration 7 : 0.061901026995332675
Loss in iteration 8 : 0.04846891219007021
Loss in iteration 9 : 0.038636893204506806
Loss in iteration 10 : 0.031671116327736656
Loss in iteration 11 : 0.026764740697454013
Loss in iteration 12 : 0.02325875940390485
Loss in iteration 13 : 0.020687531590118952
Loss in iteration 14 : 0.018737260074999106
Loss in iteration 15 : 0.01719926857956368
Loss in iteration 16 : 0.015935273373263583
Loss in iteration 17 : 0.014854327584279127
Loss in iteration 18 : 0.01389783001282152
Loss in iteration 19 : 0.013029333633737962
Loss in iteration 20 : 0.012227281629051288
Loss in iteration 21 : 0.011479769587486931
Loss in iteration 22 : 0.010780809043722242
Loss in iteration 23 : 0.010127720692414017
Loss in iteration 24 : 0.009519387784329671
Loss in iteration 25 : 0.008955163688042776
Loss in iteration 26 : 0.008434261711904797
Loss in iteration 27 : 0.007955479715834607
Loss in iteration 28 : 0.007517137040117012
Loss in iteration 29 : 0.007117128926922152
Loss in iteration 30 : 0.0067530324706242695
Loss in iteration 31 : 0.006422224210610604
Loss in iteration 32 : 0.006121988798473688
Loss in iteration 33 : 0.005849609825737101
Loss in iteration 34 : 0.0056024399570579475
Loss in iteration 35 : 0.005377950682199439
Loss in iteration 36 : 0.005173763931555051
Loss in iteration 37 : 0.004987668996509488
Loss in iteration 38 : 0.004817628665975597
Loss in iteration 39 : 0.00466177827357306
Loss in iteration 40 : 0.004518420654496433
Loss in iteration 41 : 0.004386019122345845
Loss in iteration 42 : 0.004263189734604088
Loss in iteration 43 : 0.004148693458611012
Loss in iteration 44 : 0.004041428417291795
Loss in iteration 45 : 0.003940422161492893
Loss in iteration 46 : 0.003844823831814668
Loss in iteration 47 : 0.00375389608239443
Loss in iteration 48 : 0.0036670066964053895
Loss in iteration 49 : 0.0035836198952346643
Loss in iteration 50 : 0.0035032874097720283
Loss in iteration 51 : 0.0034256394321595532
Loss in iteration 52 : 0.003350375595918234
Loss in iteration 53 : 0.0032772561423089013
Loss in iteration 54 : 0.003206093424366282
Loss in iteration 55 : 0.0031367438816278814
Loss in iteration 56 : 0.003069100592623472
Loss in iteration 57 : 0.0030030864826809376
Loss in iteration 58 : 0.00293864823473582
Loss in iteration 59 : 0.0028757509229420746
Loss in iteration 60 : 0.0028143733644755134
Loss in iteration 61 : 0.0027545041648097735
Loss in iteration 62 : 0.002696138416181335
Loss in iteration 63 : 0.0026392749978118852
Loss in iteration 64 : 0.0025839144193239793
Loss in iteration 65 : 0.002530057145131148
Loss in iteration 66 : 0.0024777023368071404
Loss in iteration 67 : 0.002426846951937192
Loss in iteration 68 : 0.0023774851411668086
Loss in iteration 69 : 0.002329607889581835
Loss in iteration 70 : 0.0022832028537467493
Loss in iteration 71 : 0.0022382543513282454
Loss in iteration 72 : 0.0021947434659473366
Loss in iteration 73 : 0.002152648235506289
Loss in iteration 74 : 0.00211194389755314
Loss in iteration 75 : 0.0020726031701615915
Loss in iteration 76 : 0.002034596551233919
Loss in iteration 77 : 0.0019978926230433468
Loss in iteration 78 : 0.0019624583522030383
Loss in iteration 79 : 0.001928259378091463
Loss in iteration 80 : 0.00189526028510201
Loss in iteration 81 : 0.0018634248559563752
Loss in iteration 82 : 0.0018327163047691998
Loss in iteration 83 : 0.0018030974896261537
Loss in iteration 84 : 0.0017745311051916538
Loss in iteration 85 : 0.0017469798563475058
Loss in iteration 86 : 0.0017204066141334087
Loss in iteration 87 : 0.0016947745553620504
Loss in iteration 88 : 0.001670047287263842
Loss in iteration 89 : 0.0016461889584164144
Loss in iteration 90 : 0.0016231643570689385
Loss in iteration 91 : 0.0016009389978094474
Loss in iteration 92 : 0.0015794791973633392
Loss in iteration 93 : 0.001558752140172285
Loss in iteration 94 : 0.0015387259342901504
Loss in iteration 95 : 0.0015193696580526846
Loss in iteration 96 : 0.001500653397929793
Loss in iteration 97 : 0.0014825482779490108
Loss in iteration 98 : 0.0014650264810831554
Loss in iteration 99 : 0.0014480612630166265
Loss in iteration 100 : 0.0014316269587363172
Loss in iteration 101 : 0.00141569898243115
Loss in iteration 102 : 0.00140025382122097
Loss in iteration 103 : 0.0013852690232682352
Loss in iteration 104 : 0.001370723180851216
Testing accuracy  of updater 7 on alg 0 with rate 1.4 = 0.9928888888888889, training accuracy 1.0, time elapsed: 2010 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.47319878124990084
Loss in iteration 3 : 0.2835507775082208
Loss in iteration 4 : 0.19250058862399738
Loss in iteration 5 : 0.14291389395230578
Loss in iteration 6 : 0.11054900892389913
Loss in iteration 7 : 0.08799737841890237
Loss in iteration 8 : 0.07124125422024054
Loss in iteration 9 : 0.05815095818275611
Loss in iteration 10 : 0.047862034285720965
Loss in iteration 11 : 0.03997790398214629
Loss in iteration 12 : 0.03407693176192856
Loss in iteration 13 : 0.02967002114768816
Loss in iteration 14 : 0.026320720479299592
Loss in iteration 15 : 0.023705871651133668
Loss in iteration 16 : 0.021608753066437652
Loss in iteration 17 : 0.01988891200420948
Loss in iteration 18 : 0.018453279264932614
Loss in iteration 19 : 0.01723607659404927
Loss in iteration 20 : 0.01618741290413358
Loss in iteration 21 : 0.015267897520498524
Loss in iteration 22 : 0.014446472592656604
Loss in iteration 23 : 0.013699500099130832
Loss in iteration 24 : 0.013010040803924842
Loss in iteration 25 : 0.012366907371973794
Loss in iteration 26 : 0.011763454220436608
Loss in iteration 27 : 0.011196249031401023
Loss in iteration 28 : 0.01066381604095559
Loss in iteration 29 : 0.010165602667142639
Loss in iteration 30 : 0.009701246246667673
Loss in iteration 31 : 0.009270143100037095
Loss in iteration 32 : 0.00887126912076962
Loss in iteration 33 : 0.008503176112430016
Loss in iteration 34 : 0.008164087210396851
Loss in iteration 35 : 0.007852029039868124
Loss in iteration 36 : 0.007564958623268466
Loss in iteration 37 : 0.007300862683699896
Loss in iteration 38 : 0.00705782223034661
Loss in iteration 39 : 0.006834045098711714
Loss in iteration 40 : 0.006627874030347405
Loss in iteration 41 : 0.00643777920758718
Loss in iteration 42 : 0.00626234328849588
Loss in iteration 43 : 0.006100245054672998
Loss in iteration 44 : 0.0059502455850221295
Loss in iteration 45 : 0.005811178877446909
Loss in iteration 46 : 0.005681947287229464
Loss in iteration 47 : 0.005561521100062905
Loss in iteration 48 : 0.00544894097565952
Loss in iteration 49 : 0.00534332180091489
Loss in iteration 50 : 0.005243856577775208
Loss in iteration 51 : 0.005149819239018336
Loss in iteration 52 : 0.005060565645473877
Loss in iteration 53 : 0.0049755323980869995
Loss in iteration 54 : 0.004894233443941687
Loss in iteration 55 : 0.004816254731940411
Loss in iteration 56 : 0.004741247363491872
Loss in iteration 57 : 0.004668919783062463
Loss in iteration 58 : 0.0045990295709122985
Loss in iteration 59 : 0.004531375351630926
Loss in iteration 60 : 0.004465789237250704
Loss in iteration 61 : 0.004402130103679889
Loss in iteration 62 : 0.004340277873121293
Loss in iteration 63 : 0.0042801288585284
Loss in iteration 64 : 0.004221592129826733
Loss in iteration 65 : 0.0041645867915276165
Loss in iteration 66 : 0.004109040018970702
Loss in iteration 67 : 0.004054885683668268
Loss in iteration 68 : 0.004002063402560871
Loss in iteration 69 : 0.003950517865598166
Loss in iteration 70 : 0.003900198324813093
Loss in iteration 71 : 0.0038510581603618797
Loss in iteration 72 : 0.0038030544702981526
Loss in iteration 73 : 0.0037561476578919996
Loss in iteration 74 : 0.00371030101120111
Loss in iteration 75 : 0.0036654802836733062
Loss in iteration 76 : 0.003621653292107373
Loss in iteration 77 : 0.003578789550298804
Loss in iteration 78 : 0.003536859954512806
Loss in iteration 79 : 0.0034958365320473723
Loss in iteration 80 : 0.003455692257981304
Loss in iteration 81 : 0.0034164009389348066
Loss in iteration 82 : 0.0033779371571932286
Loss in iteration 83 : 0.003340276264425885
Loss in iteration 84 : 0.0033033944117393343
Loss in iteration 85 : 0.003267268601945162
Loss in iteration 86 : 0.003231876750515322
Loss in iteration 87 : 0.0031971977434308084
Loss in iteration 88 : 0.0031632114826313507
Loss in iteration 89 : 0.00312989891266324
Loss in iteration 90 : 0.0030972420250550167
Loss in iteration 91 : 0.0030652238396435038
Loss in iteration 92 : 0.003033828364323941
Loss in iteration 93 : 0.0030030405363865188
Loss in iteration 94 : 0.0029728461496860833
Loss in iteration 95 : 0.002943231772395883
Loss in iteration 96 : 0.0029141846600918763
Loss in iteration 97 : 0.00288569266850847
Loss in iteration 98 : 0.002857744169618267
Loss in iteration 99 : 0.002830327973840245
Loss in iteration 100 : 0.0028034332602817303
Loss in iteration 101 : 0.002777049516059939
Loss in iteration 102 : 0.002751166484997316
Loss in iteration 103 : 0.0027257741253806033
Loss in iteration 104 : 0.0027008625760389527
Loss in iteration 105 : 0.0026764221297262396
Loss in iteration 106 : 0.002652443212673897
Loss in iteration 107 : 0.0026289163691840182
Loss in iteration 108 : 0.0026058322502268147
Loss in iteration 109 : 0.00258318160515926
Loss in iteration 110 : 0.0025609552758601487
Loss in iteration 111 : 0.002539144192758954
Loss in iteration 112 : 0.002517739372397074
Loss in iteration 113 : 0.0024967319162940774
Loss in iteration 114 : 0.002476113010985402
Loss in iteration 115 : 0.0024558739291576565
Loss in iteration 116 : 0.0024360060318313415
Loss in iteration 117 : 0.0024165007715392654
Loss in iteration 118 : 0.002397349696427197
Loss in iteration 119 : 0.0023785444551723254
Loss in iteration 120 : 0.002360076802580731
Loss in iteration 121 : 0.0023419386056948927
Loss in iteration 122 : 0.0023241218502200855
Loss in iteration 123 : 0.002306618647068234
Loss in iteration 124 : 0.00228942123881835
Loss in iteration 125 : 0.0022725220059058755
Loss in iteration 126 : 0.0022559134723753307
Loss in iteration 127 : 0.00223958831106025
Testing accuracy  of updater 7 on alg 0 with rate 0.8 = 0.9937777777777778, training accuracy 0.9997142040583024, time elapsed: 2420 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.624416056558966
Loss in iteration 3 : 0.5221163944693042
Loss in iteration 4 : 0.4183121888427827
Loss in iteration 5 : 0.33158101205537577
Loss in iteration 6 : 0.26761434096438474
Loss in iteration 7 : 0.22222426974002868
Loss in iteration 8 : 0.18896081325291172
Loss in iteration 9 : 0.16330604425836437
Loss in iteration 10 : 0.14274386909637551
Loss in iteration 11 : 0.1258689902101988
Loss in iteration 12 : 0.11179501784157973
Loss in iteration 13 : 0.099903636903325
Loss in iteration 14 : 0.08974501949004982
Loss in iteration 15 : 0.08099487836337935
Loss in iteration 16 : 0.0734222587848917
Loss in iteration 17 : 0.06685959692088263
Loss in iteration 18 : 0.061177362041774506
Loss in iteration 19 : 0.056266098864973205
Loss in iteration 20 : 0.05202654190316473
Loss in iteration 21 : 0.04836627884427298
Loss in iteration 22 : 0.045200223038279204
Loss in iteration 23 : 0.042452254410648343
Loss in iteration 24 : 0.040056367952862534
Loss in iteration 25 : 0.03795675962762811
Loss in iteration 26 : 0.03610699067420092
Loss in iteration 27 : 0.03446863988736812
Loss in iteration 28 : 0.033009836660415605
Loss in iteration 29 : 0.031703936204973755
Loss in iteration 30 : 0.030528458103633443
Loss in iteration 31 : 0.029464304641112382
Loss in iteration 32 : 0.028495214630132718
Loss in iteration 33 : 0.027607384378920498
Loss in iteration 34 : 0.02678918820189861
Loss in iteration 35 : 0.0260309452069009
Loss in iteration 36 : 0.025324698257284586
Loss in iteration 37 : 0.024663989181123266
Loss in iteration 38 : 0.024043628301210523
Loss in iteration 39 : 0.023459465074138694
Loss in iteration 40 : 0.022908170348213883
Loss in iteration 41 : 0.022387040573581374
Loss in iteration 42 : 0.02189383162779893
Loss in iteration 43 : 0.021426626115071484
Loss in iteration 44 : 0.02098373415318933
Loss in iteration 45 : 0.02056362451395853
Loss in iteration 46 : 0.02016488090173805
Loss in iteration 47 : 0.01978617720736772
Loss in iteration 48 : 0.019426265614216134
Loss in iteration 49 : 0.01908397219377287
Loss in iteration 50 : 0.018758195808346836
Loss in iteration 51 : 0.01844790745997589
Loss in iteration 52 : 0.018152148469317043
Loss in iteration 53 : 0.01787002689231242
Loss in iteration 54 : 0.01760071231529749
Loss in iteration 55 : 0.01734342960191999
Loss in iteration 56 : 0.01709745233326774
Loss in iteration 57 : 0.016862096647804335
Loss in iteration 58 : 0.016636716021821835
Loss in iteration 59 : 0.016420697302844907
Loss in iteration 60 : 0.016213458074707504
Loss in iteration 61 : 0.016014445234518043
Loss in iteration 62 : 0.015823134521835976
Loss in iteration 63 : 0.01563903066692981
Loss in iteration 64 : 0.01546166781361851
Loss in iteration 65 : 0.015290609910415852
Loss in iteration 66 : 0.015125450834916638
Loss in iteration 67 : 0.014965814103362299
Loss in iteration 68 : 0.014811352104841507
Loss in iteration 69 : 0.014661744875931498
Loss in iteration 70 : 0.01451669848934991
Loss in iteration 71 : 0.014375943166166683
Loss in iteration 72 : 0.014239231235709802
Loss in iteration 73 : 0.014106335063559823
Loss in iteration 74 : 0.013977045050646559
Loss in iteration 75 : 0.01385116778074991
Loss in iteration 76 : 0.013728524364762055
Loss in iteration 77 : 0.013608949002185443
Loss in iteration 78 : 0.013492287756652599
Loss in iteration 79 : 0.013378397524627545
Loss in iteration 80 : 0.013267145165548009
Loss in iteration 81 : 0.01315840675716822
Loss in iteration 82 : 0.01305206694072913
Loss in iteration 83 : 0.012948018325372412
Loss in iteration 84 : 0.012846160928351127
Loss in iteration 85 : 0.012746401635598165
Loss in iteration 86 : 0.012648653674868521
Loss in iteration 87 : 0.012552836100099123
Loss in iteration 88 : 0.01245887329031803
Loss in iteration 89 : 0.01236669446921852
Loss in iteration 90 : 0.012276233252502326
Loss in iteration 91 : 0.012187427229598789
Loss in iteration 92 : 0.012100217584802904
Loss in iteration 93 : 0.012014548760698174
Loss in iteration 94 : 0.011930368164359693
Loss in iteration 95 : 0.011847625914620153
Loss in iteration 96 : 0.01176627462687216
Loss in iteration 97 : 0.011686269230618466
Loss in iteration 98 : 0.011607566814313124
Loss in iteration 99 : 0.011530126491924628
Loss in iteration 100 : 0.01145390928600548
Loss in iteration 101 : 0.011378878022744213
Loss in iteration 102 : 0.011304997235368397
Loss in iteration 103 : 0.011232233073231823
Loss in iteration 104 : 0.011160553214845341
Loss in iteration 105 : 0.011089926783917195
Loss in iteration 106 : 0.01102032426810659
Loss in iteration 107 : 0.010951717440639334
Loss in iteration 108 : 0.010884079285194012
Loss in iteration 109 : 0.01081738392456066
Loss in iteration 110 : 0.010751606553539064
Loss in iteration 111 : 0.010686723376416941
Loss in iteration 112 : 0.010622711549194284
Loss in iteration 113 : 0.010559549126528596
Loss in iteration 114 : 0.010497215013200424
Loss in iteration 115 : 0.010435688919755598
Loss in iteration 116 : 0.010374951321882842
Loss in iteration 117 : 0.010314983423034598
Loss in iteration 118 : 0.010255767119795046
Loss in iteration 119 : 0.010197284969530677
Loss in iteration 120 : 0.01013952015991884
Loss in iteration 121 : 0.010082456480026621
Loss in iteration 122 : 0.010026078292693877
Loss in iteration 123 : 0.00997037050805448
Loss in iteration 124 : 0.0099153185580992
Loss in iteration 125 : 0.009860908372240034
Loss in iteration 126 : 0.009807126353874861
Loss in iteration 127 : 0.009753959357975793
Loss in iteration 128 : 0.009701394669733656
Loss in iteration 129 : 0.009649419984288025
Loss in iteration 130 : 0.009598023387561598
Loss in iteration 131 : 0.00954719333820118
Loss in iteration 132 : 0.009496918650608265
Loss in iteration 133 : 0.009447188479024966
Loss in iteration 134 : 0.009397992302624511
Loss in iteration 135 : 0.009349319911543698
Loss in iteration 136 : 0.009301161393787097
Loss in iteration 137 : 0.009253507122929104
Loss in iteration 138 : 0.009206347746540635
Loss in iteration 139 : 0.009159674175271372
Loss in iteration 140 : 0.00911347757252382
Loss in iteration 141 : 0.009067749344664231
Loss in iteration 142 : 0.00902248113172261
Loss in iteration 143 : 0.00897766479854293
Loss in iteration 144 : 0.008933292426351287
Loss in iteration 145 : 0.008889356304716765
Loss in iteration 146 : 0.008845848923884113
Loss in iteration 147 : 0.0088027629674611
Loss in iteration 148 : 0.00876009130544621
Loss in iteration 149 : 0.008717826987582838
Loss in iteration 150 : 0.008675963237027502
Loss in iteration 151 : 0.008634493444318755
Loss in iteration 152 : 0.008593411161633462
Loss in iteration 153 : 0.008552710097316459
Loss in iteration 154 : 0.008512384110668987
Loss in iteration 155 : 0.008472427206981275
Loss in iteration 156 : 0.008432833532794148
Loss in iteration 157 : 0.008393597371375279
Loss in iteration 158 : 0.00835471313839585
Loss in iteration 159 : 0.00831617537779418
Loss in iteration 160 : 0.008277978757814015
Loss in iteration 161 : 0.008240118067206016
Loss in iteration 162 : 0.008202588211582025
Loss in iteration 163 : 0.008165384209913132
Loss in iteration 164 : 0.008128501191163087
Loss in iteration 165 : 0.00809193439105005
Loss in iteration 166 : 0.008055679148929976
Loss in iteration 167 : 0.00801973090479625
Loss in iteration 168 : 0.007984085196389901
Loss in iteration 169 : 0.007948737656416117
Loss in iteration 170 : 0.007913684009862258
Loss in iteration 171 : 0.007878920071413394
Loss in iteration 172 : 0.007844441742961221
Loss in iteration 173 : 0.007810245011202647
Loss in iteration 174 : 0.007776325945324088
Loss in iteration 175 : 0.007742680694767953
Loss in iteration 176 : 0.007709305487077744
Loss in iteration 177 : 0.007676196625818072
Loss in iteration 178 : 0.007643350488566688
Loss in iteration 179 : 0.007610763524974804
Loss in iteration 180 : 0.007578432254893055
Loss in iteration 181 : 0.007546353266559899
Loss in iteration 182 : 0.007514523214849775
Loss in iteration 183 : 0.00748293881957856
Loss in iteration 184 : 0.007451596863863578
Loss in iteration 185 : 0.0074204941925362
Loss in iteration 186 : 0.00738962771060466
Loss in iteration 187 : 0.007358994381765169
Loss in iteration 188 : 0.007328591226959362
Loss in iteration 189 : 0.0072984153229764545
Loss in iteration 190 : 0.007268463801098181
Loss in iteration 191 : 0.0072387338457851925
Loss in iteration 192 : 0.0072092226934030315
Loss in iteration 193 : 0.007179927630986666
Loss in iteration 194 : 0.007150845995041804
Loss in iteration 195 : 0.007121975170381797
Loss in iteration 196 : 0.007093312588998886
Loss in iteration 197 : 0.007064855728968477
Loss in iteration 198 : 0.007036602113385297
Loss in iteration 199 : 0.007008549309330131
Loss in iteration 200 : 0.006980694926866393
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.9937777777777778, training accuracy 0.9995713060874536, time elapsed: 4474 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 2.9287772595038857
Loss in iteration 3 : 0.3464394268633959
Loss in iteration 4 : 0.33520262589623734
Loss in iteration 5 : 0.2705934284097816
Loss in iteration 6 : 0.2010539504290384
Loss in iteration 7 : 0.14253050849489973
Loss in iteration 8 : 0.1036182340774732
Loss in iteration 9 : 0.08338654390508876
Loss in iteration 10 : 0.07129047688230176
Loss in iteration 11 : 0.06043280079325397
Loss in iteration 12 : 0.050319224314898256
Loss in iteration 13 : 0.04195026193433671
Loss in iteration 14 : 0.03557664228270098
Loss in iteration 15 : 0.03079088911476712
Loss in iteration 16 : 0.02731239766997257
Loss in iteration 17 : 0.024884608474464942
Loss in iteration 18 : 0.023102527748399902
Loss in iteration 19 : 0.021680636567146104
Loss in iteration 20 : 0.0205267492060084
Loss in iteration 21 : 0.019606332880948617
Loss in iteration 22 : 0.018858396052738625
Loss in iteration 23 : 0.018223909182948844
Loss in iteration 24 : 0.01765942369210867
Loss in iteration 25 : 0.017136272261403997
Loss in iteration 26 : 0.01663864864051356
Loss in iteration 27 : 0.016159175992483267
Loss in iteration 28 : 0.015692644286069285
Loss in iteration 29 : 0.015232227058548693
Loss in iteration 30 : 0.014769720235534196
Loss in iteration 31 : 0.01429741260870671
Loss in iteration 32 : 0.01380978406304383
Loss in iteration 33 : 0.013304278851542086
Loss in iteration 34 : 0.012781234251511243
Loss in iteration 35 : 0.012243440205745517
Loss in iteration 36 : 0.011695420891143957
Loss in iteration 37 : 0.01114216362970474
Loss in iteration 38 : 0.010587447383281287
Loss in iteration 39 : 0.01003298360753202
Loss in iteration 40 : 0.009479560599101253
Loss in iteration 41 : 0.008929614397158678
Loss in iteration 42 : 0.008389389461270975
Loss in iteration 43 : 0.007868977999284548
Loss in iteration 44 : 0.007379117640135525
Loss in iteration 45 : 0.006925790995522383
Loss in iteration 46 : 0.0065069708511194275
Loss in iteration 47 : 0.006114577706745775
Loss in iteration 48 : 0.005738996102440523
Loss in iteration 49 : 0.005372293209599411
Loss in iteration 50 : 0.005009699860317538
Loss in iteration 51 : 0.004650644006619984
Loss in iteration 52 : 0.0042996693022478326
Loss in iteration 53 : 0.003965311258077548
Loss in iteration 54 : 0.003654282017502509
Loss in iteration 55 : 0.0033650661419709627
Loss in iteration 56 : 0.0030900329172271063
Loss in iteration 57 : 0.002822707334010825
Loss in iteration 58 : 0.0025609365784671537
Loss in iteration 59 : 0.0023059364945678925
Loss in iteration 60 : 0.0020606523642684458
Loss in iteration 61 : 0.0018285914458400292
Loss in iteration 62 : 0.001612862107215683
Loss in iteration 63 : 0.001415283158295712
Loss in iteration 64 : 0.0012359553191572774
Loss in iteration 65 : 0.0010736388280836
Loss in iteration 66 : 9.266461763273528E-4
Loss in iteration 67 : 7.935687821037532E-4
Loss in iteration 68 : 6.734478391712078E-4
Loss in iteration 69 : 5.655971946807056E-4
Loss in iteration 70 : 4.695746965581421E-4
Loss in iteration 71 : 3.854360491456743E-4
Loss in iteration 72 : 3.138052179017584E-4
Loss in iteration 73 : 2.553042148096469E-4
Loss in iteration 74 : 2.0964969344215446E-4
Loss in iteration 75 : 1.7528898892485637E-4
Loss in iteration 76 : 1.4988719697042955E-4
Loss in iteration 77 : 1.310843213688607E-4
Loss in iteration 78 : 1.1695082924328019E-4
Loss in iteration 79 : 1.0608209668609369E-4
Loss in iteration 80 : 9.751242929524381E-5
Loss in iteration 81 : 9.059141103769277E-5
Loss in iteration 82 : 8.488028009698101E-5
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 1981 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 1.1299788964772204
Loss in iteration 3 : 0.3676989662362676
Loss in iteration 4 : 0.12252574073940285
Loss in iteration 5 : 0.08665336156735386
Loss in iteration 6 : 0.06183754004985398
Loss in iteration 7 : 0.048113439701127685
Loss in iteration 8 : 0.03966887066537016
Loss in iteration 9 : 0.034039233940045
Loss in iteration 10 : 0.029903757129660567
Loss in iteration 11 : 0.026591449739269902
Loss in iteration 12 : 0.02382735002257259
Loss in iteration 13 : 0.02153458454725582
Loss in iteration 14 : 0.01965270182831324
Loss in iteration 15 : 0.01809058923941764
Loss in iteration 16 : 0.016734314562992292
Loss in iteration 17 : 0.015508423064131287
Loss in iteration 18 : 0.014384532915788758
Loss in iteration 19 : 0.013359981286961186
Loss in iteration 20 : 0.012436452876506148
Loss in iteration 21 : 0.011610012232038627
Loss in iteration 22 : 0.01087010967892487
Loss in iteration 23 : 0.010201785798317573
Loss in iteration 24 : 0.009588615900330893
Loss in iteration 25 : 0.009015589412195052
Loss in iteration 26 : 0.008471136617662758
Loss in iteration 27 : 0.007947852336341037
Loss in iteration 28 : 0.007442069328684162
Loss in iteration 29 : 0.006952762412949298
Loss in iteration 30 : 0.006480294863064817
Loss in iteration 31 : 0.006025427780522193
Loss in iteration 32 : 0.005588818323062692
Loss in iteration 33 : 0.005170855314468948
Loss in iteration 34 : 0.004771363730563779
Loss in iteration 35 : 0.004389155155790719
Loss in iteration 36 : 0.004022241276073176
Loss in iteration 37 : 0.003669036728031413
Loss in iteration 38 : 0.003329595735195008
Loss in iteration 39 : 0.0030059041230936848
Loss in iteration 40 : 0.0027010913390829337
Loss in iteration 41 : 0.002418051052857687
Loss in iteration 42 : 0.002158264656378647
Loss in iteration 43 : 0.0019214700332585912
Loss in iteration 44 : 0.0017061652103147484
Loss in iteration 45 : 0.001510398675362734
Loss in iteration 46 : 0.0013323555493535357
Loss in iteration 47 : 0.0011706186948996983
Loss in iteration 48 : 0.0010242143651100141
Loss in iteration 49 : 8.925596951787134E-4
Loss in iteration 50 : 7.753553616768455E-4
Loss in iteration 51 : 6.724199376237876E-4
Loss in iteration 52 : 5.834707146402271E-4
Loss in iteration 53 : 5.079047841230502E-4
Loss in iteration 54 : 4.4467675844626014E-4
Loss in iteration 55 : 3.9234243616411346E-4
Loss in iteration 56 : 3.4924112320279353E-4
Loss in iteration 57 : 3.1371382987956104E-4
Loss in iteration 58 : 2.842657949817917E-4
Loss in iteration 59 : 2.596435637557278E-4
Loss in iteration 60 : 2.38844281839294E-4
Loss in iteration 61 : 2.2108798938607924E-4
Loss in iteration 62 : 2.0577689651176865E-4
Loss in iteration 63 : 1.9245494844933293E-4
Loss in iteration 64 : 1.8077320837662174E-4
Loss in iteration 65 : 1.7046236135859867E-4
Loss in iteration 66 : 1.6131179439654526E-4
Testing accuracy  of updater 8 on alg 0 with rate 1.4000000000000001 = 0.9928888888888889, training accuracy 1.0, time elapsed: 1448 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.681166773848561
Loss in iteration 3 : 0.3155706495076682
Loss in iteration 4 : 0.14479117597171767
Loss in iteration 5 : 0.06405102432801497
Loss in iteration 6 : 0.046622554415775654
Loss in iteration 7 : 0.03646270153313805
Loss in iteration 8 : 0.03055244237829671
Loss in iteration 9 : 0.026666372365910306
Loss in iteration 10 : 0.02378123043914823
Loss in iteration 11 : 0.021407826147835433
Loss in iteration 12 : 0.019355145664378133
Loss in iteration 13 : 0.01755609585703915
Loss in iteration 14 : 0.015973834273184873
Loss in iteration 15 : 0.014573935635335504
Loss in iteration 16 : 0.013324709055726384
Loss in iteration 17 : 0.01220197444705524
Loss in iteration 18 : 0.011191258583204071
Loss in iteration 19 : 0.010284539781178803
Loss in iteration 20 : 0.009476054230690076
Loss in iteration 21 : 0.00875925383146144
Loss in iteration 22 : 0.008125438110627313
Loss in iteration 23 : 0.007563955252308888
Loss in iteration 24 : 0.007063363287009973
Loss in iteration 25 : 0.006612756598949577
Loss in iteration 26 : 0.006202677885815808
Loss in iteration 27 : 0.005825440480708037
Loss in iteration 28 : 0.005474998984881128
Loss in iteration 29 : 0.005146617648471452
Loss in iteration 30 : 0.004836548068020921
Loss in iteration 31 : 0.004541825457063562
Loss in iteration 32 : 0.004260189346176111
Loss in iteration 33 : 0.00399006939184838
Loss in iteration 34 : 0.0037305613761533705
Loss in iteration 35 : 0.0034813415292445494
Loss in iteration 36 : 0.0032425117154984305
Loss in iteration 37 : 0.003014413490129462
Loss in iteration 38 : 0.0027974683699490124
Loss in iteration 39 : 0.002592078318338158
Loss in iteration 40 : 0.0023985752918853132
Loss in iteration 41 : 0.002217185159987874
Loss in iteration 42 : 0.002047986103123402
Loss in iteration 43 : 0.001890870460026651
Loss in iteration 44 : 0.001745531456526272
Loss in iteration 45 : 0.0016114860326597383
Loss in iteration 46 : 0.0014881265816652845
Loss in iteration 47 : 0.0013747832752508224
Loss in iteration 48 : 0.0012707792089887916
Loss in iteration 49 : 0.0011754681486801748
Loss in iteration 50 : 0.0010882528120936881
Loss in iteration 51 : 0.0010085869718153082
Loss in iteration 52 : 9.359669379628557E-4
Loss in iteration 53 : 8.699182302004006E-4
Loss in iteration 54 : 8.099824261520248E-4
Loss in iteration 55 : 7.557077339026837E-4
Loss in iteration 56 : 7.066450234384403E-4
Loss in iteration 57 : 6.623492166720581E-4
Loss in iteration 58 : 6.223845402384517E-4
Loss in iteration 59 : 5.863315202485413E-4
Loss in iteration 60 : 5.537937485500253E-4
Loss in iteration 61 : 5.244030918636466E-4
Loss in iteration 62 : 4.978227776972132E-4
Loss in iteration 63 : 4.737484030580612E-4
Loss in iteration 64 : 4.51907268110685E-4
Loss in iteration 65 : 4.320565596602016E-4
Loss in iteration 66 : 4.139808779133766E-4
Loss in iteration 67 : 3.9748949511812196E-4
Loss in iteration 68 : 3.82413613679951E-4
Loss in iteration 69 : 3.6860378508829094E-4
Loss in iteration 70 : 3.5592757079192086E-4
Loss in iteration 71 : 3.442674721566505E-4
Testing accuracy  of updater 8 on alg 0 with rate 0.8 = 0.992, training accuracy 1.0, time elapsed: 1589 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.3104803068373541
Loss in iteration 3 : 0.24252567193950333
Loss in iteration 4 : 0.16457622104343714
Loss in iteration 5 : 0.08712193709009584
Loss in iteration 6 : 0.06493930379924567
Loss in iteration 7 : 0.05283155493796723
Loss in iteration 8 : 0.0431470827216794
Loss in iteration 9 : 0.03556240363263373
Loss in iteration 10 : 0.029997436593985898
Loss in iteration 11 : 0.026007603590497544
Loss in iteration 12 : 0.023084951988723224
Loss in iteration 13 : 0.02083764761986989
Loss in iteration 14 : 0.01901262711720757
Loss in iteration 15 : 0.017462247304846797
Loss in iteration 16 : 0.016105644205121224
Loss in iteration 17 : 0.01489946898604242
Loss in iteration 18 : 0.013819231867369722
Loss in iteration 19 : 0.012848863951659791
Loss in iteration 20 : 0.011975719812731092
Loss in iteration 21 : 0.011188707783433427
Loss in iteration 22 : 0.010477888322526066
Loss in iteration 23 : 0.009834538341588815
Loss in iteration 24 : 0.009251210597048243
Loss in iteration 25 : 0.008721652317222814
Loss in iteration 26 : 0.008240607646828591
Loss in iteration 27 : 0.007803576152473684
Loss in iteration 28 : 0.0074065903244924045
Loss in iteration 29 : 0.007046046508924014
Loss in iteration 30 : 0.00671859705855473
Loss in iteration 31 : 0.006421094754335483
Loss in iteration 32 : 0.006150573790587143
Loss in iteration 33 : 0.005904251645695918
Loss in iteration 34 : 0.005679539512815237
Loss in iteration 35 : 0.005474053159987984
Loss in iteration 36 : 0.005285619784309309
Loss in iteration 37 : 0.005112279129670658
Loss in iteration 38 : 0.004952278849265519
Loss in iteration 39 : 0.0048040650165260355
Loss in iteration 40 : 0.004666269065964244
Loss in iteration 41 : 0.004537692487032916
Loss in iteration 42 : 0.0044172904511421175
Loss in iteration 43 : 0.004304155324208392
Loss in iteration 44 : 0.00419750076617469
Loss in iteration 45 : 0.004096646880981313
Loss in iteration 46 : 0.004001006675452304
Loss in iteration 47 : 0.003910073922992498
Loss in iteration 48 : 0.003823412410285612
Loss in iteration 49 : 0.003740646469782036
Loss in iteration 50 : 0.0036614526617061256
Loss in iteration 51 : 0.0035855524586395644
Loss in iteration 52 : 0.0035127057947826382
Loss in iteration 53 : 0.003442705362480406
Loss in iteration 54 : 0.003375371563471055
Loss in iteration 55 : 0.0033105480462393704
Loss in iteration 56 : 0.003248097780399157
Loss in iteration 57 : 0.0031878996325330447
Loss in iteration 58 : 0.0031298454152023356
Loss in iteration 59 : 0.0030738373827694683
Loss in iteration 60 : 0.0030197861457264227
Loss in iteration 61 : 0.0029676089710727396
Loss in iteration 62 : 0.0029172284315133086
Loss in iteration 63 : 0.00286857136211636
Loss in iteration 64 : 0.0028215680804412435
Loss in iteration 65 : 0.00277615182544809
Loss in iteration 66 : 0.0027322583717966087
Loss in iteration 67 : 0.0026898257792050805
Loss in iteration 68 : 0.0026487942409628806
Loss in iteration 69 : 0.0026091060009793767
Loss in iteration 70 : 0.002570705314408614
Loss in iteration 71 : 0.002533538432474591
Loss in iteration 72 : 0.002497553597286835
Loss in iteration 73 : 0.0024627010369418475
Loss in iteration 74 : 0.0024289329549231327
Loss in iteration 75 : 0.0023962035107030454
Loss in iteration 76 : 0.0023644687905529428
Loss in iteration 77 : 0.002333686768976539
Loss in iteration 78 : 0.0023038172620130667
Loss in iteration 79 : 0.00227482187404742
Loss in iteration 80 : 0.0022466639398394375
Loss in iteration 81 : 0.002219308463359292
Loss in iteration 82 : 0.0021927220547820706
Loss in iteration 83 : 0.002166872866722728
Loss in iteration 84 : 0.0021417305305302115
Loss in iteration 85 : 0.0021172660932334106
Loss in iteration 86 : 0.0020934519555551264
Loss in iteration 87 : 0.0020702618112833754
Loss in iteration 88 : 0.002047670588204843
Loss in iteration 89 : 0.002025654390754314
Loss in iteration 90 : 0.0020041904445039513
Loss in iteration 91 : 0.001983257042597903
Loss in iteration 92 : 0.0019628334942228434
Loss in iteration 93 : 0.001942900075189363
Loss in iteration 94 : 0.0019234379806782407
Loss in iteration 95 : 0.00190442928018145
Loss in iteration 96 : 0.001885856874639464
Loss in iteration 97 : 0.0018677044557474218
Loss in iteration 98 : 0.0018499564673742218
Loss in iteration 99 : 0.0018325980690139735
Loss in iteration 100 : 0.001815615101169211
Loss in iteration 101 : 0.0017989940525511843
Loss in iteration 102 : 0.0017827220289750526
Loss in iteration 103 : 0.0017667867238262114
Loss in iteration 104 : 0.0017511763899778286
Loss in iteration 105 : 0.00173587981304729
Loss in iteration 106 : 0.0017208862858907855
Loss in iteration 107 : 0.0017061855842471546
Loss in iteration 108 : 0.0016917679434556448
Loss in iteration 109 : 0.0016776240361845768
Loss in iteration 110 : 0.0016637449511195937
Loss in iteration 111 : 0.0016501221725695203
Loss in iteration 112 : 0.0016367475609561483
Loss in iteration 113 : 0.0016236133341595723
Loss in iteration 114 : 0.0016107120496951976
Loss in iteration 115 : 0.0015980365877006765
Loss in iteration 116 : 0.0015855801347123357
Loss in iteration 117 : 0.0015733361682103847
Loss in iteration 118 : 0.0015612984419123342
Loss in iteration 119 : 0.0015494609717923882
Loss in iteration 120 : 0.001537818022803866
Loss in iteration 121 : 0.0015263640962805921
Loss in iteration 122 : 0.001515093917992162
Loss in iteration 123 : 0.001504002426827354
Loss in iteration 124 : 0.0014930847640792137
Loss in iteration 125 : 0.001482336263305821
Loss in iteration 126 : 0.001471752440739817
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.9946666666666667, training accuracy 1.0, time elapsed: 2682 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.32910383798767767
Loss in iteration 3 : 0.2265080710968357
Loss in iteration 4 : 0.15254630055633997
Loss in iteration 5 : 0.10315466142707634
Loss in iteration 6 : 0.07934674517270857
Loss in iteration 7 : 0.0640123303917992
Loss in iteration 8 : 0.052484488253527015
Loss in iteration 9 : 0.043651367601445265
Loss in iteration 10 : 0.03698470818437425
Loss in iteration 11 : 0.03198677032576642
Loss in iteration 12 : 0.02819643541169253
Loss in iteration 13 : 0.025249173400447803
Loss in iteration 14 : 0.022886681435414193
Loss in iteration 15 : 0.02093617711492445
Loss in iteration 16 : 0.01928459047727368
Loss in iteration 17 : 0.01785775493652693
Loss in iteration 18 : 0.01660607889733924
Loss in iteration 19 : 0.015495390753794172
Loss in iteration 20 : 0.014501311062917877
Loss in iteration 21 : 0.013605854436530786
Loss in iteration 22 : 0.01279537232211608
Loss in iteration 23 : 0.012059268259806756
Loss in iteration 24 : 0.011389145909930919
Loss in iteration 25 : 0.010778204583268683
Loss in iteration 26 : 0.010220791291004656
Loss in iteration 27 : 0.00971206725878689
Loss in iteration 28 : 0.00924776694489464
Loss in iteration 29 : 0.008824033209105643
Loss in iteration 30 : 0.008437312794324605
Loss in iteration 31 : 0.008084296390156153
Loss in iteration 32 : 0.007761888821057166
Loss in iteration 33 : 0.0074671973298510354
Loss in iteration 34 : 0.007197528921808633
Loss in iteration 35 : 0.006950390681636534
Loss in iteration 36 : 0.00672348946658224
Loss in iteration 37 : 0.0065147292423036055
Loss in iteration 38 : 0.006322205571268931
Loss in iteration 39 : 0.006144197490884013
Loss in iteration 40 : 0.0059791573670859586
Loss in iteration 41 : 0.005825699408089715
Loss in iteration 42 : 0.005682587476379865
Loss in iteration 43 : 0.005548722720029719
Loss in iteration 44 : 0.005423131406457827
Loss in iteration 45 : 0.005304953212186697
Loss in iteration 46 : 0.005193430115953882
Loss in iteration 47 : 0.00508789596473189
Loss in iteration 48 : 0.004987766731733931
Loss in iteration 49 : 0.004892531457719155
Loss in iteration 50 : 0.004801743855754548
Loss in iteration 51 : 0.004715014558816766
Loss in iteration 52 : 0.004632003993799525
Loss in iteration 53 : 0.0045524158705013435
Loss in iteration 54 : 0.004475991277347329
Loss in iteration 55 : 0.004402503375664839
Loss in iteration 56 : 0.004331752681135825
Loss in iteration 57 : 0.0042635629152194526
Loss in iteration 58 : 0.0041977774019449346
Loss in iteration 59 : 0.004134255977708014
Loss in iteration 60 : 0.004072872374644677
Loss in iteration 61 : 0.0040135120326234815
Loss in iteration 62 : 0.003956070291386028
Loss in iteration 63 : 0.0039004509130447364
Loss in iteration 64 : 0.0038465648859109516
Loss in iteration 65 : 0.0037943294631680034
Loss in iteration 66 : 0.003743667393797948
Loss in iteration 67 : 0.0036945063079483766
Loss in iteration 68 : 0.0036467782241382395
Loss in iteration 69 : 0.0036004191509609743
Loss in iteration 70 : 0.003555368760946387
Loss in iteration 71 : 0.003511570118780043
Loss in iteration 72 : 0.003468969450030146
Loss in iteration 73 : 0.003427515939848867
Loss in iteration 74 : 0.003387161553811853
Loss in iteration 75 : 0.0033478608751876804
Loss in iteration 76 : 0.0033095709545635614
Loss in iteration 77 : 0.0032722511689803188
Loss in iteration 78 : 0.0032358630886313573
Loss in iteration 79 : 0.003200370349833678
Loss in iteration 80 : 0.0031657385334460525
Loss in iteration 81 : 0.0031319350482430933
Loss in iteration 82 : 0.003098929018988243
Loss in iteration 83 : 0.003066691179114639
Loss in iteration 84 : 0.003035193768034496
Loss in iteration 85 : 0.0030044104331698016
Loss in iteration 86 : 0.002974316136835987
Loss in iteration 87 : 0.0029448870681216504
Loss in iteration 88 : 0.0029161005598955002
Loss in iteration 89 : 0.0028879350110414276
Loss in iteration 90 : 0.0028603698139768974
Loss in iteration 91 : 0.0028333852874544655
Loss in iteration 92 : 0.0028069626145849767
Loss in iteration 93 : 0.002781083785958861
Loss in iteration 94 : 0.002755731547682264
Loss in iteration 95 : 0.0027308893540918095
Loss in iteration 96 : 0.002706541324866839
Loss in iteration 97 : 0.0026826722062246715
Loss in iteration 98 : 0.002659267335860647
Loss in iteration 99 : 0.002636312611283583
Loss in iteration 100 : 0.002613794461195562
Loss in iteration 101 : 0.0025916998195726335
Loss in iteration 102 : 0.0025700161021188876
Loss in iteration 103 : 0.002548731184786709
Loss in iteration 104 : 0.0025278333840823898
Loss in iteration 105 : 0.0025073114389034634
Loss in iteration 106 : 0.0024871544936836665
Loss in iteration 107 : 0.002467352082649722
Loss in iteration 108 : 0.002447894115022697
Loss in iteration 109 : 0.0024287708610219116
Loss in iteration 110 : 0.0024099729385535836
Loss in iteration 111 : 0.0023914913004875094
Loss in iteration 112 : 0.002373317222443775
Loss in iteration 113 : 0.002355442291027298
Loss in iteration 114 : 0.0023378583924621604
Loss in iteration 115 : 0.002320557701588247
Loss in iteration 116 : 0.002303532671193056
Loss in iteration 117 : 0.002286776021658114
Loss in iteration 118 : 0.002270280730906094
Loss in iteration 119 : 0.0022540400246389342
Loss in iteration 120 : 0.0022380473668609895
Loss in iteration 121 : 0.002222296450683581
Loss in iteration 122 : 0.0022067811894090563
Loss in iteration 123 : 0.0021914957078936195
Loss in iteration 124 : 0.002176434334188472
Loss in iteration 125 : 0.002161591591459153
Loss in iteration 126 : 0.0021469621901826145
Loss in iteration 127 : 0.0021325410206208886
Loss in iteration 128 : 0.002118323145570006
Loss in iteration 129 : 0.0021043037933816205
Loss in iteration 130 : 0.002090478351254271
Loss in iteration 131 : 0.0020768423587901865
Loss in iteration 132 : 0.002063391501812991
Loss in iteration 133 : 0.002050121606440631
Loss in iteration 134 : 0.0020370286334075384
Loss in iteration 135 : 0.0020241086726289557
Loss in iteration 136 : 0.0020113579380005163
Loss in iteration 137 : 0.0019987727624251536
Loss in iteration 138 : 0.0019863495930595733
Loss in iteration 139 : 0.0019740849867721393
Loss in iteration 140 : 0.001961975605803875
Loss in iteration 141 : 0.0019500182136243703
Loss in iteration 142 : 0.0019382096709743965
Loss in iteration 143 : 0.0019265469320870626
Testing accuracy  of updater 8 on alg 0 with rate 0.14 = 0.9946666666666667, training accuracy 1.0, time elapsed: 3815 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.4008055345072845
Loss in iteration 3 : 0.260764295480876
Loss in iteration 4 : 0.1898973870186047
Loss in iteration 5 : 0.14632396764782635
Loss in iteration 6 : 0.11615022283507262
Loss in iteration 7 : 0.09486867173253455
Loss in iteration 8 : 0.07920771787585842
Loss in iteration 9 : 0.06717994441926016
Loss in iteration 10 : 0.0576976142133262
Loss in iteration 11 : 0.050142482089764795
Loss in iteration 12 : 0.044104333711439146
Loss in iteration 13 : 0.03926400378618968
Loss in iteration 14 : 0.035357471251865884
Loss in iteration 15 : 0.03216959827040564
Loss in iteration 16 : 0.029531457158656637
Loss in iteration 17 : 0.027314546316408363
Loss in iteration 18 : 0.025422987514104864
Loss in iteration 19 : 0.02378580140037975
Loss in iteration 20 : 0.02235041457078218
Loss in iteration 21 : 0.021077672548773164
Loss in iteration 22 : 0.019938188095156392
Loss in iteration 23 : 0.018909720251967615
Loss in iteration 24 : 0.017975294899734002
Loss in iteration 25 : 0.017121841905471138
Loss in iteration 26 : 0.01633919054289116
Loss in iteration 27 : 0.015619317748304998
Loss in iteration 28 : 0.014955780700705948
Loss in iteration 29 : 0.014343288867624402
Loss in iteration 30 : 0.013777384787982261
Loss in iteration 31 : 0.013254210867372246
Loss in iteration 32 : 0.012770343966874292
Loss in iteration 33 : 0.012322682375439747
Loss in iteration 34 : 0.011908371939502213
Loss in iteration 35 : 0.011524760190665815
Loss in iteration 36 : 0.011169369401519611
Loss in iteration 37 : 0.010839881553825618
Loss in iteration 38 : 0.010534130100616504
Loss in iteration 39 : 0.010250095035677063
Loss in iteration 40 : 0.009985899089441712
Loss in iteration 41 : 0.009739803839919483
Loss in iteration 42 : 0.009510205191173586
Loss in iteration 43 : 0.00929562808397382
Loss in iteration 44 : 0.009094720525796362
Loss in iteration 45 : 0.008906247120083748
Loss in iteration 46 : 0.008729082288151188
Loss in iteration 47 : 0.008562203349934981
Loss in iteration 48 : 0.008404683588447035
Loss in iteration 49 : 0.008255685383307624
Loss in iteration 50 : 0.008114453468817508
Loss in iteration 51 : 0.00798030835368248
Loss in iteration 52 : 0.007852639931206962
Loss in iteration 53 : 0.00773090130735201
Loss in iteration 54 : 0.007614602876000501
Loss in iteration 55 : 0.007503306673086462
Loss in iteration 56 : 0.007396621041849502
Loss in iteration 57 : 0.007294195639313299
Loss in iteration 58 : 0.007195716809001262
Loss in iteration 59 : 0.007100903337401756
Loss in iteration 60 : 0.0070095026026717
Loss in iteration 61 : 0.006921287114546112
Loss in iteration 62 : 0.006836051435355668
Loss in iteration 63 : 0.0067536094641963244
Loss in iteration 64 : 0.006673792060129624
Loss in iteration 65 : 0.00659644497603445
Loss in iteration 66 : 0.006521427072359423
Loss in iteration 67 : 0.00644860877934038
Loss in iteration 68 : 0.0063778707769423576
Loss in iteration 69 : 0.006309102863502354
Loss in iteration 70 : 0.0062422029864361165
Loss in iteration 71 : 0.006177076411116984
Loss in iteration 72 : 0.006113635006884432
Loss in iteration 73 : 0.006051796631911374
Loss in iteration 74 : 0.0059914846012331545
Loss in iteration 75 : 0.005932627224553721
Loss in iteration 76 : 0.00587515740247578
Loss in iteration 77 : 0.005819012271559296
Loss in iteration 78 : 0.005764132890123001
Loss in iteration 79 : 0.005710463957998386
Loss in iteration 80 : 0.005657953564559518
Loss in iteration 81 : 0.005606552960313111
Loss in iteration 82 : 0.005556216348165717
Loss in iteration 83 : 0.005506900691207136
Loss in iteration 84 : 0.005458565534471677
Loss in iteration 85 : 0.0054111728386731514
Loss in iteration 86 : 0.0053646868243593035
Loss in iteration 87 : 0.0053190738253049395
Loss in iteration 88 : 0.005274302150263438
Loss in iteration 89 : 0.00523034195243259
Loss in iteration 90 : 0.00518716510616727
Loss in iteration 91 : 0.005144745090598553
Loss in iteration 92 : 0.005103056879902651
Loss in iteration 93 : 0.005062076840013998
Loss in iteration 94 : 0.0050217826316006015
Loss in iteration 95 : 0.004982153119125945
Loss in iteration 96 : 0.004943168285814755
Loss in iteration 97 : 0.004904809154326407
Loss in iteration 98 : 0.004867057712922779
Loss in iteration 99 : 0.004829896846900791
Loss in iteration 100 : 0.00479331027504487
Loss in iteration 101 : 0.004757282490843458
Loss in iteration 102 : 0.004721798708205269
Loss in iteration 103 : 0.004686844811407704
Loss in iteration 104 : 0.004652407309009167
Loss in iteration 105 : 0.004618473291459962
Loss in iteration 106 : 0.004585030392151949
Loss in iteration 107 : 0.004552066751654928
Loss in iteration 108 : 0.004519570984896881
Loss in iteration 109 : 0.004487532151055997
Loss in iteration 110 : 0.00445593972594376
Loss in iteration 111 : 0.004424783576670464
Loss in iteration 112 : 0.004394053938397153
Loss in iteration 113 : 0.0043637413929903815
Loss in iteration 114 : 0.004333836849409115
Loss in iteration 115 : 0.004304331525665408
Loss in iteration 116 : 0.004275216932213081
Loss in iteration 117 : 0.004246484856630488
Loss in iteration 118 : 0.0042181273494749216
Loss in iteration 119 : 0.004190136711197686
Loss in iteration 120 : 0.004162505480018775
Loss in iteration 121 : 0.004135226420670533
Loss in iteration 122 : 0.004108292513928329
Loss in iteration 123 : 0.004081696946855163
Loss in iteration 124 : 0.004055433103694477
Loss in iteration 125 : 0.004029494557352891
Loss in iteration 126 : 0.004003875061420442
Loss in iteration 127 : 0.00397856854268203
Loss in iteration 128 : 0.003953569094078589
Loss in iteration 129 : 0.003928870968080877
Loss in iteration 130 : 0.0039044685704430334
Loss in iteration 131 : 0.0038803564543062616
Loss in iteration 132 : 0.003856529314626031
Loss in iteration 133 : 0.003832981982899158
Loss in iteration 134 : 0.0038097094221689522
Loss in iteration 135 : 0.0037867067222892036
Loss in iteration 136 : 0.0037639690954291926
Loss in iteration 137 : 0.003741491871803631
Loss in iteration 138 : 0.0037192704956128123
Loss in iteration 139 : 0.003697300521179564
Loss in iteration 140 : 0.0036755776092705097
Loss in iteration 141 : 0.0036540975235902777
Loss in iteration 142 : 0.003632856127438067
Loss in iteration 143 : 0.0036118493805168887
Loss in iteration 144 : 0.0035910733358863476
Loss in iteration 145 : 0.003570524137050516
Loss in iteration 146 : 0.0035501980151731293
Loss in iteration 147 : 0.003530091286412746
Loss in iteration 148 : 0.0035102003493709425
Loss in iteration 149 : 0.0034905216826474406
Loss in iteration 150 : 0.003471051842495673
Loss in iteration 151 : 0.0034517874605738213
Loss in iteration 152 : 0.0034327252417854837
Loss in iteration 153 : 0.003413861962205528
Loss in iteration 154 : 0.0033951944670862068
Loss in iteration 155 : 0.003376719668939325
Loss in iteration 156 : 0.0033584345456905778
Loss in iteration 157 : 0.003340336138901916
Loss in iteration 158 : 0.0033224215520587437
Loss in iteration 159 : 0.0033046879489184618
Loss in iteration 160 : 0.0032871325519172427
Loss in iteration 161 : 0.0032697526406321336
Loss in iteration 162 : 0.003252545550295885
Loss in iteration 163 : 0.0032355086703617533
Loss in iteration 164 : 0.0032186394431161004
Loss in iteration 165 : 0.0032019353623363557
Loss in iteration 166 : 0.00318539397199247
Loss in iteration 167 : 0.0031690128649896915
Loss in iteration 168 : 0.003152789681950997
Loss in iteration 169 : 0.0031367221100373735
Loss in iteration 170 : 0.003120807881804334
Loss in iteration 171 : 0.003105044774093205
Loss in iteration 172 : 0.003089430606955697
Loss in iteration 173 : 0.0030739632426105127
Loss in iteration 174 : 0.0030586405844305995
Loss in iteration 175 : 0.0030434605759600097
Loss in iteration 176 : 0.003028421199959094
Testing accuracy  of updater 8 on alg 0 with rate 0.08000000000000002 = 0.9946666666666667, training accuracy 0.9998571020291512, time elapsed: 4758 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.5860728736678696
Loss in iteration 3 : 0.48340568705908166
Loss in iteration 4 : 0.396746652302464
Loss in iteration 5 : 0.32945355395895265
Loss in iteration 6 : 0.27899119496191016
Loss in iteration 7 : 0.24095649971734462
Loss in iteration 8 : 0.21150901424997542
Loss in iteration 9 : 0.18796914957304703
Loss in iteration 10 : 0.16863850203225828
Loss in iteration 11 : 0.1524621648082617
Loss in iteration 12 : 0.13875435485632995
Loss in iteration 13 : 0.12702994466299908
Loss in iteration 14 : 0.11691945229116675
Loss in iteration 15 : 0.10813172490729073
Loss in iteration 16 : 0.10043695688820113
Loss in iteration 17 : 0.09365564218775223
Loss in iteration 18 : 0.08764847007584173
Loss in iteration 19 : 0.08230667868027784
Loss in iteration 20 : 0.07754362446373089
Loss in iteration 21 : 0.07328816059268918
Loss in iteration 22 : 0.0694799390631623
Loss in iteration 23 : 0.06606638653349836
Loss in iteration 24 : 0.06300093250267731
Loss in iteration 25 : 0.060242050367559696
Loss in iteration 26 : 0.05775274443228452
Loss in iteration 27 : 0.055500225165149755
Loss in iteration 28 : 0.05345562278752955
Loss in iteration 29 : 0.051593674149011406
Loss in iteration 30 : 0.0498923726407582
Loss in iteration 31 : 0.048332598341042345
Loss in iteration 32 : 0.04689775322568381
Loss in iteration 33 : 0.045573422474217494
Loss in iteration 34 : 0.044347074490057765
Loss in iteration 35 : 0.04320780370394779
Loss in iteration 36 : 0.042146113812983575
Loss in iteration 37 : 0.04115373547018016
Loss in iteration 38 : 0.04022347126415824
Loss in iteration 39 : 0.03934906137147901
Loss in iteration 40 : 0.038525064721538896
Loss in iteration 41 : 0.037746752223624655
Loss in iteration 42 : 0.037010010125147874
Loss in iteration 43 : 0.03631125267389928
Loss in iteration 44 : 0.0356473438924416
Loss in iteration 45 : 0.03501552849959569
Loss in iteration 46 : 0.03441337194897986
Loss in iteration 47 : 0.0338387093267155
Loss in iteration 48 : 0.03328960257216155
Loss in iteration 49 : 0.0327643052382192
Loss in iteration 50 : 0.03226123383827769
Loss in iteration 51 : 0.031778944751829734
Loss in iteration 52 : 0.03131611567429742
Loss in iteration 53 : 0.030871530679235388
Loss in iteration 54 : 0.03044406808778534
Loss in iteration 55 : 0.03003269048644369
Loss in iteration 56 : 0.029636436379573458
Loss in iteration 57 : 0.029254413093400336
Loss in iteration 58 : 0.028885790655529972
Loss in iteration 59 : 0.02852979645576348
Loss in iteration 60 : 0.028185710551662017
Loss in iteration 61 : 0.027852861519868256
Loss in iteration 62 : 0.02753062277668679
Loss in iteration 63 : 0.02721840930390666
Loss in iteration 64 : 0.026915674722659464
Loss in iteration 65 : 0.026621908662519105
Loss in iteration 66 : 0.026336634377175545
Loss in iteration 67 : 0.02605940656294014
Loss in iteration 68 : 0.025789809342334005
Loss in iteration 69 : 0.025527454381826836
Loss in iteration 70 : 0.025271979119898057
Loss in iteration 71 : 0.025023045088399056
Loss in iteration 72 : 0.02478033631619245
Loss in iteration 73 : 0.02454355780888975
Loss in iteration 74 : 0.024312434102040845
Loss in iteration 75 : 0.024086707887371054
Loss in iteration 76 : 0.023866138712762094
Loss in iteration 77 : 0.023650501756871525
Loss in iteration 78 : 0.02343958667885035
Loss in iteration 79 : 0.02323319654281884
Loss in iteration 80 : 0.02303114681581572
Loss in iteration 81 : 0.022833264437028513
Loss in iteration 82 : 0.022639386955349467
Loss in iteration 83 : 0.022449361731746182
Loss in iteration 84 : 0.022263045202601143
Loss in iteration 85 : 0.022080302200041826
Loss in iteration 86 : 0.021901005325313
Loss in iteration 87 : 0.021725034371389843
Loss in iteration 88 : 0.021552275791246883
Loss in iteration 89 : 0.02138262220844494
Loss in iteration 90 : 0.021215971966943914
Loss in iteration 91 : 0.021052228717278434
Loss in iteration 92 : 0.020891301036430734
Loss in iteration 93 : 0.020733102078904636
Loss in iteration 94 : 0.0205775492566462
Loss in iteration 95 : 0.020424563945578858
Loss in iteration 96 : 0.02027407121663037
Loss in iteration 97 : 0.02012599958923442
Loss in iteration 98 : 0.019980280805394754
Loss in iteration 99 : 0.01983684962250788
Loss in iteration 100 : 0.01969564362325648
Loss in iteration 101 : 0.019556603041002547
Loss in iteration 102 : 0.019419670599233423
Loss in iteration 103 : 0.019284791363736044
Loss in iteration 104 : 0.019151912606295472
Loss in iteration 105 : 0.019020983678831666
Loss in iteration 106 : 0.018891955896997017
Loss in iteration 107 : 0.018764782432360308
Loss in iteration 108 : 0.01863941821239396
Loss in iteration 109 : 0.018515819827565613
Loss in iteration 110 : 0.018393945444908445
Loss in iteration 111 : 0.018273754727510003
Loss in iteration 112 : 0.018155208759415804
Loss in iteration 113 : 0.018038269975495253
Loss in iteration 114 : 0.017922902095860175
Loss in iteration 115 : 0.01780907006446703
Loss in iteration 116 : 0.01769673999156712
Loss in iteration 117 : 0.017585879099701215
Loss in iteration 118 : 0.017476455672962996
Loss in iteration 119 : 0.017368439009280513
Loss in iteration 120 : 0.017261799375489162
Loss in iteration 121 : 0.017156507964988753
Loss in iteration 122 : 0.017052536857798443
Loss in iteration 123 : 0.016949858982837896
Loss in iteration 124 : 0.01684844808228101
Loss in iteration 125 : 0.016748278677839838
Loss in iteration 126 : 0.016649326038850703
Loss in iteration 127 : 0.0165515661520444
Loss in iteration 128 : 0.016454975692891907
Loss in iteration 129 : 0.016359531998427164
Loss in iteration 130 : 0.016265213041453886
Loss in iteration 131 : 0.0161719974060529
Loss in iteration 132 : 0.01607986426431023
Loss in iteration 133 : 0.015988793354193467
Loss in iteration 134 : 0.015898764958508216
Loss in iteration 135 : 0.01580975988487104
Loss in iteration 136 : 0.015721759446639786
Loss in iteration 137 : 0.015634745444745704
Loss in iteration 138 : 0.015548700150376151
Loss in iteration 139 : 0.015463606288458614
Loss in iteration 140 : 0.01537944702190168
Loss in iteration 141 : 0.01529620593654978
Loss in iteration 142 : 0.015213867026812591
Loss in iteration 143 : 0.01513241468193147
Loss in iteration 144 : 0.01505183367284856
Loss in iteration 145 : 0.014972109139645345
Loss in iteration 146 : 0.01489322657952083
Loss in iteration 147 : 0.01481517183527976
Loss in iteration 148 : 0.014737931084304825
Loss in iteration 149 : 0.014661490827986835
Loss in iteration 150 : 0.014585837881589549
Loss in iteration 151 : 0.014510959364526791
Loss in iteration 152 : 0.01443684269103061
Loss in iteration 153 : 0.01436347556119115
Loss in iteration 154 : 0.014290845952349278
Loss in iteration 155 : 0.014218942110824717
Loss in iteration 156 : 0.01414775254396329
Loss in iteration 157 : 0.014077266012487419
Loss in iteration 158 : 0.0140074715231358
Loss in iteration 159 : 0.013938358321578083
Loss in iteration 160 : 0.013869915885591753
Loss in iteration 161 : 0.013802133918489038
Loss in iteration 162 : 0.013735002342782118
Loss in iteration 163 : 0.013668511294075867
Loss in iteration 164 : 0.013602651115178164
Loss in iteration 165 : 0.01353741235041716
Loss in iteration 166 : 0.01347278574015744
Loss in iteration 167 : 0.01340876221550559
Loss in iteration 168 : 0.01334533289319726
Loss in iteration 169 : 0.013282489070657915
Loss in iteration 170 : 0.013220222221229955
Loss in iteration 171 : 0.013158523989559101
Loss in iteration 172 : 0.01309738618713351
Loss in iteration 173 : 0.013036800787969285
Loss in iteration 174 : 0.012976759924436488
Loss in iteration 175 : 0.012917255883219876
Loss in iteration 176 : 0.012858281101409106
Loss in iteration 177 : 0.012799828162713129
Loss in iteration 178 : 0.012741889793794028
Loss in iteration 179 : 0.012684458860715632
Loss in iteration 180 : 0.012627528365502336
Loss in iteration 181 : 0.012571091442804157
Loss in iteration 182 : 0.012515141356663885
Loss in iteration 183 : 0.012459671497382379
Loss in iteration 184 : 0.012404675378478558
Loss in iteration 185 : 0.012350146633740573
Loss in iteration 186 : 0.012296079014364561
Loss in iteration 187 : 0.012242466386178086
Loss in iteration 188 : 0.012189302726945241
Loss in iteration 189 : 0.01213658212375011
Loss in iteration 190 : 0.012084298770456296
Loss in iteration 191 : 0.01203244696523945
Loss in iteration 192 : 0.011981021108190534
Loss in iteration 193 : 0.011930015698987056
Loss in iteration 194 : 0.011879425334630336
Loss in iteration 195 : 0.011829244707246115
Loss in iteration 196 : 0.011779468601946633
Loss in iteration 197 : 0.011730091894752088
Loss in iteration 198 : 0.011681109550569314
Loss in iteration 199 : 0.01163251662122595
Loss in iteration 200 : 0.011584308243558123
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.9848888888888889, training accuracy 0.999285510145756, time elapsed: 5892 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 7.599951025738028
Loss in iteration 3 : 0.8718756950006896
Loss in iteration 4 : 3.509201546608929
Loss in iteration 5 : 1.8407103502175406
Loss in iteration 6 : 0.9655536547669195
Loss in iteration 7 : 0.699458269255078
Loss in iteration 8 : 0.9795323013910049
Loss in iteration 9 : 1.021890094468266
Loss in iteration 10 : 0.6459168838248488
Loss in iteration 11 : 0.3494470662898382
Loss in iteration 12 : 0.21063737121657927
Loss in iteration 13 : 0.173467416718564
Loss in iteration 14 : 0.18180454256623232
Loss in iteration 15 : 0.20285624052936474
Loss in iteration 16 : 0.21494879357238234
Loss in iteration 17 : 0.2116574864251264
Loss in iteration 18 : 0.19614954552777286
Loss in iteration 19 : 0.17392394851457157
Loss in iteration 20 : 0.15099961317168795
Loss in iteration 21 : 0.1314404554386002
Loss in iteration 22 : 0.11896960084913222
Loss in iteration 23 : 0.11100880116019149
Loss in iteration 24 : 0.10746852438650868
Loss in iteration 25 : 0.10691578400300875
Loss in iteration 26 : 0.1071851138383063
Loss in iteration 27 : 0.10733353004454613
Loss in iteration 28 : 0.10685619802856323
Loss in iteration 29 : 0.1054450589584096
Loss in iteration 30 : 0.10306571233365226
Loss in iteration 31 : 0.10002201183418802
Loss in iteration 32 : 0.0968184062738999
Loss in iteration 33 : 0.09364853167153207
Loss in iteration 34 : 0.09038399646387575
Loss in iteration 35 : 0.0869727198145357
Loss in iteration 36 : 0.08342392193017446
Loss in iteration 37 : 0.07975083055792098
Loss in iteration 38 : 0.07596562043757402
Loss in iteration 39 : 0.07207926531404239
Loss in iteration 40 : 0.06810164622530489
Loss in iteration 41 : 0.06404166349005612
Loss in iteration 42 : 0.05990795073659251
Loss in iteration 43 : 0.05579510567876846
Loss in iteration 44 : 0.052214425042963426
Loss in iteration 45 : 0.048751833445901005
Loss in iteration 46 : 0.04542224088873032
Loss in iteration 47 : 0.042465662848713344
Loss in iteration 48 : 0.03995511047129543
Loss in iteration 49 : 0.03777209576684477
Loss in iteration 50 : 0.0357578145883223
Loss in iteration 51 : 0.03407139409621475
Loss in iteration 52 : 0.0324952622663961
Loss in iteration 53 : 0.03096220266730478
Loss in iteration 54 : 0.029487334985368204
Loss in iteration 55 : 0.028049178029467096
Loss in iteration 56 : 0.026571541165597456
Loss in iteration 57 : 0.025022248552349646
Loss in iteration 58 : 0.02341212047124501
Loss in iteration 59 : 0.02178246679638241
Loss in iteration 60 : 0.0202578005854662
Loss in iteration 61 : 0.019046540064642622
Loss in iteration 62 : 0.018031158105531093
Loss in iteration 63 : 0.017038620415776962
Loss in iteration 64 : 0.01602823176694055
Loss in iteration 65 : 0.014994512492604008
Loss in iteration 66 : 0.013938370289163185
Loss in iteration 67 : 0.01286266031386748
Loss in iteration 68 : 0.01177773744627179
Loss in iteration 69 : 0.010747822727150504
Loss in iteration 70 : 0.009954675402159217
Loss in iteration 71 : 0.009411090456802473
Loss in iteration 72 : 0.00894233027495946
Loss in iteration 73 : 0.008480964715216125
Loss in iteration 74 : 0.008016914541742817
Loss in iteration 75 : 0.0075489025423283295
Loss in iteration 76 : 0.007076783562457972
Loss in iteration 77 : 0.006600608420410269
Loss in iteration 78 : 0.006120565549937863
Loss in iteration 79 : 0.005636996223934323
Loss in iteration 80 : 0.005150375457199071
Loss in iteration 81 : 0.004661357881634505
Loss in iteration 82 : 0.0041713982902022995
Loss in iteration 83 : 0.003686295224678001
Loss in iteration 84 : 0.0032295824485945464
Loss in iteration 85 : 0.0028478593347107694
Loss in iteration 86 : 0.0025363491336252457
Loss in iteration 87 : 0.0022449815466806054
Loss in iteration 88 : 0.0019504183487767093
Loss in iteration 89 : 0.0016473335694351097
Loss in iteration 90 : 0.0013351650981547195
Loss in iteration 91 : 0.0010144995429837275
Loss in iteration 92 : 6.869395586413291E-4
Loss in iteration 93 : 3.621532473516505E-4
Loss in iteration 94 : 1.0507410385521492E-4
Loss in iteration 95 : 1.5408571328702005E-5
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.9928888888888889, training accuracy 1.0, time elapsed: 2794 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 1.8330309118084653
Loss in iteration 3 : 0.372021220679807
Loss in iteration 4 : 0.7370382211657639
Loss in iteration 5 : 0.5579171822625764
Loss in iteration 6 : 0.26566947420077364
Loss in iteration 7 : 0.16830012670936395
Loss in iteration 8 : 0.23717990166553302
Loss in iteration 9 : 0.26313632871772413
Loss in iteration 10 : 0.199979524701209
Loss in iteration 11 : 0.12316839768051827
Loss in iteration 12 : 0.07737019904179854
Loss in iteration 13 : 0.06263485793905575
Loss in iteration 14 : 0.06602188321764679
Loss in iteration 15 : 0.07471112759568782
Loss in iteration 16 : 0.07919015445659847
Loss in iteration 17 : 0.07589677309758033
Loss in iteration 18 : 0.06659408938551954
Loss in iteration 19 : 0.05576701755310565
Loss in iteration 20 : 0.04628096558811387
Loss in iteration 21 : 0.03895716625613092
Loss in iteration 22 : 0.03472863336148983
Loss in iteration 23 : 0.032378703405995114
Loss in iteration 24 : 0.030469070434627942
Loss in iteration 25 : 0.02886119454976031
Loss in iteration 26 : 0.0274800759200776
Loss in iteration 27 : 0.02613467663826446
Loss in iteration 28 : 0.02470217485665153
Loss in iteration 29 : 0.023187566390028624
Loss in iteration 30 : 0.021684647168235573
Loss in iteration 31 : 0.020386635868128562
Loss in iteration 32 : 0.019246560918145583
Loss in iteration 33 : 0.018088090701403024
Loss in iteration 34 : 0.016864211887501035
Loss in iteration 35 : 0.015570743708955812
Loss in iteration 36 : 0.014211232574603052
Loss in iteration 37 : 0.01279622152107696
Loss in iteration 38 : 0.01136135457356573
Loss in iteration 39 : 0.010027468822417864
Loss in iteration 40 : 0.0090818316125707
Loss in iteration 41 : 0.008608202428885036
Loss in iteration 42 : 0.00831250454105234
Loss in iteration 43 : 0.008019977907707688
Loss in iteration 44 : 0.007662450685458966
Loss in iteration 45 : 0.0072147512846942406
Loss in iteration 46 : 0.006678230187490606
Loss in iteration 47 : 0.0060732954121429875
Loss in iteration 48 : 0.005429082332555755
Loss in iteration 49 : 0.004774205784311814
Loss in iteration 50 : 0.004140935283696494
Loss in iteration 51 : 0.0035714404667563306
Loss in iteration 52 : 0.0030881726753639216
Loss in iteration 53 : 0.00267669303692416
Loss in iteration 54 : 0.0023191377842533153
Loss in iteration 55 : 0.002002617421459886
Loss in iteration 56 : 0.001714339057057513
Loss in iteration 57 : 0.001449566765016113
Loss in iteration 58 : 0.0012136920734661584
Loss in iteration 59 : 0.001012047118947837
Loss in iteration 60 : 8.399520680237906E-4
Loss in iteration 61 : 6.869139262438945E-4
Loss in iteration 62 : 5.45115160302709E-4
Loss in iteration 63 : 4.120805479809522E-4
Loss in iteration 64 : 2.90933089500053E-4
Loss in iteration 65 : 1.9002940510687882E-4
Loss in iteration 66 : 1.1794836210597222E-4
Loss in iteration 67 : 7.447548806894592E-5
Loss in iteration 68 : 5.0753371279632344E-5
Testing accuracy  of updater 9 on alg 0 with rate 1.4000000000000001 = 0.9928888888888889, training accuracy 1.0, time elapsed: 1804 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.8999873385987354
Loss in iteration 3 : 0.19718337436554934
Loss in iteration 4 : 0.39938765705967355
Loss in iteration 5 : 0.2627325322008173
Loss in iteration 6 : 0.12161052734234687
Loss in iteration 7 : 0.09786719685453535
Loss in iteration 8 : 0.1369902027219265
Loss in iteration 9 : 0.12733035986849503
Loss in iteration 10 : 0.08359431997032454
Loss in iteration 11 : 0.04971034401081738
Loss in iteration 12 : 0.03619828123418415
Loss in iteration 13 : 0.035471091851816856
Loss in iteration 14 : 0.03963682955285895
Loss in iteration 15 : 0.04325377694571203
Loss in iteration 16 : 0.04259425481761494
Loss in iteration 17 : 0.03783609914945571
Loss in iteration 18 : 0.0312943088789922
Loss in iteration 19 : 0.02531146387223749
Loss in iteration 20 : 0.02106084153901774
Loss in iteration 21 : 0.018751780578482334
Loss in iteration 22 : 0.017605817460565654
Loss in iteration 23 : 0.016793854675519155
Loss in iteration 24 : 0.016060365834613646
Loss in iteration 25 : 0.015326702161208095
Loss in iteration 26 : 0.01453275109067721
Loss in iteration 27 : 0.013658671239988754
Loss in iteration 28 : 0.012730364034096496
Loss in iteration 29 : 0.01178983669634824
Loss in iteration 30 : 0.010877809748945175
Loss in iteration 31 : 0.010022296540224053
Loss in iteration 32 : 0.009224487928738987
Loss in iteration 33 : 0.008461783685455443
Loss in iteration 34 : 0.007709774045591359
Loss in iteration 35 : 0.006959825837116248
Loss in iteration 36 : 0.0062251353120209305
Loss in iteration 37 : 0.005542022685416189
Loss in iteration 38 : 0.004967963685934678
Loss in iteration 39 : 0.004563814057662971
Loss in iteration 40 : 0.004331543854174574
Loss in iteration 41 : 0.004196854761800284
Loss in iteration 42 : 0.00407928656673133
Loss in iteration 43 : 0.003927696077555289
Loss in iteration 44 : 0.003720317917504963
Loss in iteration 45 : 0.0034556570602769275
Loss in iteration 46 : 0.003144864775031149
Loss in iteration 47 : 0.0028077482258842203
Loss in iteration 48 : 0.0024694992689466466
Loss in iteration 49 : 0.0021551534735779116
Loss in iteration 50 : 0.0018818762420638898
Loss in iteration 51 : 0.0016539170340386415
Loss in iteration 52 : 0.0014647230936887694
Loss in iteration 53 : 0.001303283087201836
Loss in iteration 54 : 0.0011594733975126288
Loss in iteration 55 : 0.001026488752107464
Loss in iteration 56 : 9.009416630843682E-4
Loss in iteration 57 : 7.819923571203078E-4
Loss in iteration 58 : 6.703754947180403E-4
Loss in iteration 59 : 5.675495244586137E-4
Loss in iteration 60 : 4.7498514831229043E-4
Loss in iteration 61 : 3.9367049261722326E-4
Loss in iteration 62 : 3.2392390964061353E-4
Loss in iteration 63 : 2.6545688793535026E-4
Loss in iteration 64 : 2.175123450944722E-4
Loss in iteration 65 : 1.7897699793547115E-4
Loss in iteration 66 : 1.484942517055283E-4
Loss in iteration 67 : 1.2461498132727622E-4
Loss in iteration 68 : 1.05957744355528E-4
Loss in iteration 69 : 9.132408095103207E-5
Testing accuracy  of updater 9 on alg 0 with rate 0.8 = 0.9928888888888889, training accuracy 1.0, time elapsed: 1691 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.30980705927148205
Loss in iteration 3 : 0.1477745471107953
Loss in iteration 4 : 0.1296317975729193
Loss in iteration 5 : 0.0842105395133867
Loss in iteration 6 : 0.06459721467389586
Loss in iteration 7 : 0.05486635440423912
Loss in iteration 8 : 0.044439512952774665
Loss in iteration 9 : 0.03358075524809325
Loss in iteration 10 : 0.0259521535663595
Loss in iteration 11 : 0.021930285137706354
Loss in iteration 12 : 0.020078716832692235
Loss in iteration 13 : 0.019070029041211894
Loss in iteration 14 : 0.018107303743880386
Loss in iteration 15 : 0.01686915450833737
Loss in iteration 16 : 0.015387086726348689
Loss in iteration 17 : 0.013867154173953174
Loss in iteration 18 : 0.012513109988869986
Loss in iteration 19 : 0.011424994857067723
Loss in iteration 20 : 0.010591417751893156
Loss in iteration 21 : 0.00993653959138142
Loss in iteration 22 : 0.009373030599672986
Loss in iteration 23 : 0.008836034948482686
Loss in iteration 24 : 0.008293976543715833
Loss in iteration 25 : 0.0077433347586772635
Loss in iteration 26 : 0.007197155937639514
Loss in iteration 27 : 0.006673944369122422
Loss in iteration 28 : 0.006189792954856104
Loss in iteration 29 : 0.005754327601320238
Loss in iteration 30 : 0.005370072251882181
Loss in iteration 31 : 0.005034211259437559
Loss in iteration 32 : 0.004741175488324797
Loss in iteration 33 : 0.004484676485392142
Loss in iteration 34 : 0.004258713944737973
Loss in iteration 35 : 0.004057838132825584
Loss in iteration 36 : 0.0038771361852654976
Loss in iteration 37 : 0.0037122312150749477
Loss in iteration 38 : 0.0035593559234160387
Loss in iteration 39 : 0.0034154462307113263
Loss in iteration 40 : 0.003278192862483159
Loss in iteration 41 : 0.003146024456843653
Loss in iteration 42 : 0.0030180269557941917
Loss in iteration 43 : 0.002893818628806712
Loss in iteration 44 : 0.0027734031867616463
Loss in iteration 45 : 0.002657021406898598
Loss in iteration 46 : 0.002545017578542134
Loss in iteration 47 : 0.002437731939828033
Loss in iteration 48 : 0.0023354247698073385
Loss in iteration 49 : 0.002238232731016586
Loss in iteration 50 : 0.0021461541265375976
Loss in iteration 51 : 0.002059057248608378
Loss in iteration 52 : 0.0019767048585626694
Loss in iteration 53 : 0.0018987877548741704
Loss in iteration 54 : 0.0018249610491487436
Loss in iteration 55 : 0.001754877932661664
Loss in iteration 56 : 0.0016882171815470815
Loss in iteration 57 : 0.0016247022349777395
Loss in iteration 58 : 0.0015641112055149158
Loss in iteration 59 : 0.0015062784750536193
Loss in iteration 60 : 0.001451089465722115
Loss in iteration 61 : 0.0013984706921378082
Loss in iteration 62 : 0.0013483773149319425
Loss in iteration 63 : 0.001300780204550481
Loss in iteration 64 : 0.0012556541042905744
Loss in iteration 65 : 0.0012129679716208126
Loss in iteration 66 : 0.001172678074545824
Loss in iteration 67 : 0.0011347239879145037
Loss in iteration 68 : 0.0010990273017014612
Loss in iteration 69 : 0.0010654926231666517
Loss in iteration 70 : 0.001034010318221445
Loss in iteration 71 : 0.0010044603812349511
Loss in iteration 72 : 9.767168339752856E-4
Loss in iteration 73 : 9.506521205300674E-4
Loss in iteration 74 : 9.261410714719224E-4
Loss in iteration 75 : 9.030641405544048E-4
Loss in iteration 76 : 8.813097530459217E-4
Loss in iteration 77 : 8.607757299868774E-4
Loss in iteration 78 : 8.413698546434949E-4
Loss in iteration 79 : 8.230097191344708E-4
Loss in iteration 80 : 8.056220291227466E-4
Loss in iteration 81 : 7.891415556553191E-4
Loss in iteration 82 : 7.735099116751258E-4
Loss in iteration 83 : 7.58674303626727E-4
Loss in iteration 84 : 7.445863730029974E-4
Loss in iteration 85 : 7.312012046861874E-4
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.992, training accuracy 0.9998571020291512, time elapsed: 2247 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.3376594319014914
Loss in iteration 3 : 0.1758113000544377
Loss in iteration 4 : 0.1301545399599511
Loss in iteration 5 : 0.09475232784020221
Loss in iteration 6 : 0.07353358137649521
Loss in iteration 7 : 0.058778580534558804
Loss in iteration 8 : 0.04698099478915628
Loss in iteration 9 : 0.037809621306141336
Loss in iteration 10 : 0.031127614020641625
Loss in iteration 11 : 0.026474804366413616
Loss in iteration 12 : 0.023276792175534893
Loss in iteration 13 : 0.021016334470045275
Loss in iteration 14 : 0.019304759075514894
Loss in iteration 15 : 0.01788911384942035
Loss in iteration 16 : 0.016627988442283716
Loss in iteration 17 : 0.01545764309497189
Loss in iteration 18 : 0.014359582581714502
Loss in iteration 19 : 0.013334963148786067
Loss in iteration 20 : 0.012388260175107869
Loss in iteration 21 : 0.011519932257401739
Loss in iteration 22 : 0.010725542710404564
Loss in iteration 23 : 0.009997960725411566
Loss in iteration 24 : 0.009329939455831491
Loss in iteration 25 : 0.008715711039986482
Loss in iteration 26 : 0.0081513864657414
Loss in iteration 27 : 0.007634539428207645
Loss in iteration 28 : 0.00716346540547683
Loss in iteration 29 : 0.006736481779823406
Loss in iteration 30 : 0.006351462313479013
Loss in iteration 31 : 0.0060056583333978585
Loss in iteration 32 : 0.005695761707911934
Loss in iteration 33 : 0.005418111804722763
Loss in iteration 34 : 0.0051689423439187
Loss in iteration 35 : 0.004944594305748041
Loss in iteration 36 : 0.004741663573005279
Loss in iteration 37 : 0.004557084442038383
Loss in iteration 38 : 0.00438816457575629
Loss in iteration 39 : 0.004232587884457193
Loss in iteration 40 : 0.004088397273101823
Loss in iteration 41 : 0.0039539646892649185
Loss in iteration 42 : 0.0038279532756803195
Loss in iteration 43 : 0.0037092753464548823
Loss in iteration 44 : 0.0035970494922972418
Loss in iteration 45 : 0.003490559749282244
Loss in iteration 46 : 0.0033892191876871824
Loss in iteration 47 : 0.0032925394943589532
Loss in iteration 48 : 0.0032001072471834614
Loss in iteration 49 : 0.0031115667499910563
Loss in iteration 50 : 0.0030266086254340037
Loss in iteration 51 : 0.0029449629240347135
Loss in iteration 52 : 0.00286639532216549
Loss in iteration 53 : 0.002790705027351971
Loss in iteration 54 : 0.002717723230595404
Loss in iteration 55 : 0.0026473112710200027
Loss in iteration 56 : 0.002579358036649754
Loss in iteration 57 : 0.00251377645689288
Loss in iteration 58 : 0.0024504992061835647
Loss in iteration 59 : 0.0023894739131769204
Loss in iteration 60 : 0.002330658253077577
Loss in iteration 61 : 0.0022740153032195223
Loss in iteration 62 : 0.002219509483763655
Loss in iteration 63 : 0.0021671033098122692
Loss in iteration 64 : 0.0021167550711069815
Loss in iteration 65 : 0.00206841744972586
Loss in iteration 66 : 0.0020220369982540815
Loss in iteration 67 : 0.001977554338252365
Loss in iteration 68 : 0.0019349049035911165
Loss in iteration 69 : 0.0018940200432808504
Loss in iteration 70 : 0.0018548283090235576
Loss in iteration 71 : 0.0018172567777554184
Loss in iteration 72 : 0.0017812322927447063
Loss in iteration 73 : 0.0017466825428488946
Loss in iteration 74 : 0.001713536933987501
Loss in iteration 75 : 0.0016817272367588513
Loss in iteration 76 : 0.001651188017709711
Loss in iteration 77 : 0.001621856878449486
Loss in iteration 78 : 0.0015936745368231745
Loss in iteration 79 : 0.0015665847885441213
Loss in iteration 80 : 0.0015405343871995564
Loss in iteration 81 : 0.0015154728766743036
Loss in iteration 82 : 0.0014913524040634253
Loss in iteration 83 : 0.0014681275341915348
Loss in iteration 84 : 0.001445755079853414
Loss in iteration 85 : 0.0014241939555211319
Loss in iteration 86 : 0.001403405056974533
Loss in iteration 87 : 0.0013833511653296574
Loss in iteration 88 : 0.0013639968713063425
Loss in iteration 89 : 0.0013453085141907723
Loss in iteration 90 : 0.001327254129617648
Loss in iteration 91 : 0.0013098034007693413
Loss in iteration 92 : 0.0012929276086029447
Loss in iteration 93 : 0.0012765995780180993
Loss in iteration 94 : 0.001260793618252565
Loss in iteration 95 : 0.0012454854570679206
Loss in iteration 96 : 0.0012306521693484802
Loss in iteration 97 : 0.0012162721015127052
Loss in iteration 98 : 0.0012023247936104526
Loss in iteration 99 : 0.0011887909011633986
Loss in iteration 100 : 0.0011756521187430901
Testing accuracy  of updater 9 on alg 0 with rate 0.14 = 0.9902222222222222, training accuracy 0.9998571020291512, time elapsed: 2736 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.4336042530669468
Loss in iteration 3 : 0.24980761945054455
Loss in iteration 4 : 0.17287530744890084
Loss in iteration 5 : 0.1305011751350032
Loss in iteration 6 : 0.10325694427816288
Loss in iteration 7 : 0.08433207461766289
Loss in iteration 8 : 0.06987137989722597
Loss in iteration 9 : 0.05815289544442977
Loss in iteration 10 : 0.04867670890734146
Loss in iteration 11 : 0.041303909857517855
Loss in iteration 12 : 0.03575412004966857
Loss in iteration 13 : 0.031576105858088246
Loss in iteration 14 : 0.028337975926644287
Loss in iteration 15 : 0.025738066354721152
Loss in iteration 16 : 0.023596975163447616
Loss in iteration 17 : 0.021807990550040336
Loss in iteration 18 : 0.02029675362051226
Loss in iteration 19 : 0.019001418584571363
Loss in iteration 20 : 0.017867443425838955
Loss in iteration 21 : 0.01684885584003903
Loss in iteration 22 : 0.015910628752300177
Loss in iteration 23 : 0.015029717098917292
Loss in iteration 24 : 0.014194127347129866
Loss in iteration 25 : 0.013400377859016223
Loss in iteration 26 : 0.012650181751822056
Loss in iteration 27 : 0.011947253463377808
Loss in iteration 28 : 0.011294896475938307
Loss in iteration 29 : 0.010694641268561324
Loss in iteration 30 : 0.0101458511192643
Loss in iteration 31 : 0.009646006536449092
Loss in iteration 32 : 0.009191329314627364
Loss in iteration 33 : 0.008777465904239183
Loss in iteration 34 : 0.00840005362117907
Loss in iteration 35 : 0.008055094092208422
Loss in iteration 36 : 0.0077391320937985186
Loss in iteration 37 : 0.007449279676306444
Loss in iteration 38 : 0.0071831407208147405
Loss in iteration 39 : 0.006938688749191195
Loss in iteration 40 : 0.006714139202471938
Loss in iteration 41 : 0.006507842747298459
Loss in iteration 42 : 0.006318212439266168
Loss in iteration 43 : 0.006143686827230768
Loss in iteration 44 : 0.005982723974895141
Loss in iteration 45 : 0.005833817659505465
Loss in iteration 46 : 0.0056955259818706454
Loss in iteration 47 : 0.005566503454408135
Loss in iteration 48 : 0.005445529551353522
Loss in iteration 49 : 0.00533152908047093
Loss in iteration 50 : 0.00522358210670643
Loss in iteration 51 : 0.005120923215845982
Loss in iteration 52 : 0.005022931470597445
Loss in iteration 53 : 0.00492911340950986
Loss in iteration 54 : 0.004839081883021219
Loss in iteration 55 : 0.004752533487108818
Loss in iteration 56 : 0.0046692269617916666
Loss in iteration 57 : 0.00458896430632943
Loss in iteration 58 : 0.004511575660118989
Loss in iteration 59 : 0.004436908322944353
Loss in iteration 60 : 0.0043648197243157895
Loss in iteration 61 : 0.004295173747641211
Loss in iteration 62 : 0.004227839585693529
Loss in iteration 63 : 0.004162692236534822
Loss in iteration 64 : 0.004099613812119284
Loss in iteration 65 : 0.004038494983904147
Loss in iteration 66 : 0.003979236088117996
Loss in iteration 67 : 0.003921747619803097
Loss in iteration 68 : 0.0038659500300275585
Loss in iteration 69 : 0.0038117728858696174
Loss in iteration 70 : 0.003759153549283971
Loss in iteration 71 : 0.003708035578865129
Loss in iteration 72 : 0.0036583670643074804
Loss in iteration 73 : 0.003610099077339562
Loss in iteration 74 : 0.00356318437682676
Loss in iteration 75 : 0.0035175764508607204
Loss in iteration 76 : 0.0034732289244336634
Loss in iteration 77 : 0.0034300953146513538
Loss in iteration 78 : 0.0033881290805538892
Loss in iteration 79 : 0.0033472838931465384
Loss in iteration 80 : 0.003307514042781561
Loss in iteration 81 : 0.00326877490368542
Loss in iteration 82 : 0.003231023386418934
Loss in iteration 83 : 0.003194218325293432
Loss in iteration 84 : 0.0031583207662516308
Loss in iteration 85 : 0.0031232941389006583
Loss in iteration 86 : 0.0030891043123188644
Loss in iteration 87 : 0.00305571954670113
Loss in iteration 88 : 0.0030231103612880852
Loss in iteration 89 : 0.0029912493433476343
Loss in iteration 90 : 0.002960110923696678
Loss in iteration 91 : 0.002929671142107934
Loss in iteration 92 : 0.0028999074218300325
Loss in iteration 93 : 0.002870798367253462
Loss in iteration 94 : 0.0028423235932894765
Loss in iteration 95 : 0.002814463589942365
Loss in iteration 96 : 0.0027871996213036116
Loss in iteration 97 : 0.0027605136550412916
Loss in iteration 98 : 0.0027343883164775516
Loss in iteration 99 : 0.0027088068604766767
Loss in iteration 100 : 0.0026837531544368757
Loss in iteration 101 : 0.002659211666456269
Loss in iteration 102 : 0.0026351674539774857
Loss in iteration 103 : 0.0026116061496597918
Loss in iteration 104 : 0.002588513942676553
Loss in iteration 105 : 0.002565877554927051
Loss in iteration 106 : 0.002543684212682219
Loss in iteration 107 : 0.0025219216148981397
Loss in iteration 108 : 0.0025005778998264967
Loss in iteration 109 : 0.002479641611654584
Loss in iteration 110 : 0.0024591016687756685
Loss in iteration 111 : 0.0024389473349901154
Loss in iteration 112 : 0.002419168194539149
Loss in iteration 113 : 0.002399754131443026
Loss in iteration 114 : 0.0023806953132045248
Loss in iteration 115 : 0.002361982178592656
Loss in iteration 116 : 0.002343605428957428
Loss in iteration 117 : 0.002325556022362063
Loss in iteration 118 : 0.0023078251697470262
Loss in iteration 119 : 0.0022904043323512357
Loss in iteration 120 : 0.0022732852196930492
Loss in iteration 121 : 0.0022564597875348595
Loss in iteration 122 : 0.002239920235402092
Loss in iteration 123 : 0.0022236590033790204
Loss in iteration 124 : 0.0022076687680472047
Loss in iteration 125 : 0.0021919424375546536
Loss in iteration 126 : 0.0021764731458986013
Loss in iteration 127 : 0.0021612542465708857
Loss in iteration 128 : 0.0021462793057500837
Loss in iteration 129 : 0.0021315420952352747
Testing accuracy  of updater 9 on alg 0 with rate 0.08000000000000002 = 0.9911111111111112, training accuracy 0.9997142040583024, time elapsed: 3351 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6125687694902314
Loss in iteration 3 : 0.5026683874788205
Loss in iteration 4 : 0.3989293845834503
Loss in iteration 5 : 0.3166749513984747
Loss in iteration 6 : 0.25791492042197095
Loss in iteration 7 : 0.2168392744878645
Loss in iteration 8 : 0.186863902188307
Loss in iteration 9 : 0.16373310200197586
Loss in iteration 10 : 0.14516052825504114
Loss in iteration 11 : 0.12987822271224836
Loss in iteration 12 : 0.11707511989505932
Loss in iteration 13 : 0.10617687967426027
Loss in iteration 14 : 0.09677151355523746
Loss in iteration 15 : 0.08857087600189048
Loss in iteration 16 : 0.08137608686296277
Loss in iteration 17 : 0.07504639818924282
Loss in iteration 18 : 0.06947530230379885
Loss in iteration 19 : 0.06457478733227477
Loss in iteration 20 : 0.06026651481084661
Loss in iteration 21 : 0.05647805784534093
Loss in iteration 22 : 0.05314230771964725
Loss in iteration 23 : 0.05019832546647651
Loss in iteration 24 : 0.047592304840361685
Loss in iteration 25 : 0.045277898562225215
Loss in iteration 26 : 0.04321574772374419
Loss in iteration 27 : 0.04137244902140017
Loss in iteration 28 : 0.03971933312607674
Loss in iteration 29 : 0.038231371833327164
Loss in iteration 30 : 0.03688638838048365
Loss in iteration 31 : 0.035664601672745745
Loss in iteration 32 : 0.0345484366023404
Loss in iteration 33 : 0.03352248910448524
Loss in iteration 34 : 0.03257353567992985
Loss in iteration 35 : 0.031690505244084344
Loss in iteration 36 : 0.030864369517978177
Loss in iteration 37 : 0.030087944271362084
Loss in iteration 38 : 0.02935562027105673
Loss in iteration 39 : 0.028663057131068734
Loss in iteration 40 : 0.028006876221817256
Loss in iteration 41 : 0.027384383278721395
Loss in iteration 42 : 0.0267933410583545
Loss in iteration 43 : 0.026231800812755363
Loss in iteration 44 : 0.02569799113199564
Loss in iteration 45 : 0.02519025538713254
Loss in iteration 46 : 0.02470702509428789
Loss in iteration 47 : 0.024246815723553763
Loss in iteration 48 : 0.02380823302197905
Loss in iteration 49 : 0.023389980855682963
Loss in iteration 50 : 0.02299086499880648
Loss in iteration 51 : 0.02260979049499468
Loss in iteration 52 : 0.022245752732323563
Loss in iteration 53 : 0.02189782399895698
Loss in iteration 54 : 0.02156513802729228
Loss in iteration 55 : 0.021246875037162945
Loss in iteration 56 : 0.020942249279550094
Loss in iteration 57 : 0.020650500304666253
Loss in iteration 58 : 0.020370888348645977
Loss in iteration 59 : 0.020102693513658794
Loss in iteration 60 : 0.01984521790465508
Loss in iteration 61 : 0.01959778961796196
Loss in iteration 62 : 0.01935976743839457
Loss in iteration 63 : 0.01913054524452614
Loss in iteration 64 : 0.018909555381533216
Loss in iteration 65 : 0.01869627057073938
Loss in iteration 66 : 0.0184902042266255
Loss in iteration 67 : 0.018290909303105545
Loss in iteration 68 : 0.018097975966703393
Loss in iteration 69 : 0.017911028487395016
Loss in iteration 70 : 0.017729721754708074
Loss in iteration 71 : 0.017553737783083483
Loss in iteration 72 : 0.01738278248722057
Loss in iteration 73 : 0.01721658290647438
Loss in iteration 74 : 0.017054884956107835
Loss in iteration 75 : 0.016897451696423944
Loss in iteration 76 : 0.016744062047132465
Loss in iteration 77 : 0.016594509836981183
Loss in iteration 78 : 0.016448603066448554
Loss in iteration 79 : 0.01630616326966753
Loss in iteration 80 : 0.016167024884452742
Loss in iteration 81 : 0.016031034569575442
Loss in iteration 82 : 0.015898050440101612
Loss in iteration 83 : 0.015767941219812964
Loss in iteration 84 : 0.015640585331267706
Loss in iteration 85 : 0.01551586995743532
Loss in iteration 86 : 0.015393690114060745
Loss in iteration 87 : 0.015273947770120134
Loss in iteration 88 : 0.015156551046763765
Loss in iteration 89 : 0.015041413515128355
Loss in iteration 90 : 0.014928453602382645
Loss in iteration 91 : 0.01481759410506137
Loss in iteration 92 : 0.014708761800356882
Loss in iteration 93 : 0.01460188714026551
Loss in iteration 94 : 0.014496904010504966
Loss in iteration 95 : 0.014393749535710692
Loss in iteration 96 : 0.014292363914074197
Loss in iteration 97 : 0.01419269026765395
Loss in iteration 98 : 0.014094674498381312
Loss in iteration 99 : 0.013998265143685325
Loss in iteration 100 : 0.013903413229177694
Loss in iteration 101 : 0.01381007211864559
Loss in iteration 102 : 0.01371819736353149
Loss in iteration 103 : 0.013627746555115866
Loss in iteration 104 : 0.013538679182859122
Loss in iteration 105 : 0.013450956501973381
Loss in iteration 106 : 0.01336454141249187
Loss in iteration 107 : 0.013279398351092484
Loss in iteration 108 : 0.013195493195897464
Loss in iteration 109 : 0.013112793183558452
Loss in iteration 110 : 0.013031266837238275
Loss in iteration 111 : 0.012950883903663672
Loss in iteration 112 : 0.012871615297248708
Loss in iteration 113 : 0.012793433049346124
Loss in iteration 114 : 0.012716310260919676
Loss in iteration 115 : 0.012640221057282092
Loss in iteration 116 : 0.012565140543947693
Loss in iteration 117 : 0.012491044763050513
Loss in iteration 118 : 0.012417910650132116
Loss in iteration 119 : 0.01234571599138516
Loss in iteration 120 : 0.012274439381628643
Loss in iteration 121 : 0.012204060183394115
Loss in iteration 122 : 0.012134558487523792
Loss in iteration 123 : 0.012065915075641278
Loss in iteration 124 : 0.011998111384770667
Loss in iteration 125 : 0.011931129474271702
Loss in iteration 126 : 0.011864951995145045
Loss in iteration 127 : 0.011799562161657154
Loss in iteration 128 : 0.011734943725148746
Loss in iteration 129 : 0.011671080949830924
Loss in iteration 130 : 0.011607958590338672
Loss in iteration 131 : 0.011545561870803092
Loss in iteration 132 : 0.011483876465214857
Loss in iteration 133 : 0.011422888478878264
Loss in iteration 134 : 0.011362584430791836
Loss in iteration 135 : 0.011302951236829874
Loss in iteration 136 : 0.011243976193639219
Loss in iteration 137 : 0.011185646963197772
Loss in iteration 138 : 0.011127951558008699
Loss in iteration 139 : 0.011070878326922695
Loss in iteration 140 : 0.011014415941590939
Loss in iteration 141 : 0.010958553383554894
Loss in iteration 142 : 0.010903279931977322
Loss in iteration 143 : 0.010848585152012136
Loss in iteration 144 : 0.010794458883802928
Loss in iteration 145 : 0.010740891232090766
Loss in iteration 146 : 0.010687872556403836
Loss in iteration 147 : 0.010635393461794476
Loss in iteration 148 : 0.010583444790085227
Loss in iteration 149 : 0.010532017611582357
Loss in iteration 150 : 0.010481103217216039
Loss in iteration 151 : 0.010430693111068078
Loss in iteration 152 : 0.010380779003250719
Loss in iteration 153 : 0.010331352803105444
Loss in iteration 154 : 0.010282406612694067
Loss in iteration 155 : 0.010233932720560118
Loss in iteration 156 : 0.01018592359574221
Loss in iteration 157 : 0.010138371882025123
Loss in iteration 158 : 0.010091270392416885
Loss in iteration 159 : 0.010044612103843071
Loss in iteration 160 : 0.009998390152049825
Loss in iteration 161 : 0.009952597826709283
Loss in iteration 162 : 0.00990722856672007
Loss in iteration 163 : 0.009862275955696397
Loss in iteration 164 : 0.009817733717638447
Loss in iteration 165 : 0.009773595712776747
Loss in iteration 166 : 0.009729855933582322
Loss in iteration 167 : 0.009686508500935127
Loss in iteration 168 : 0.009643547660441896
Loss in iteration 169 : 0.009600967778895856
Loss in iteration 170 : 0.00955876334087008
Loss in iteration 171 : 0.00951692894543709
Loss in iteration 172 : 0.009475459303007803
Loss in iteration 173 : 0.009434349232282948
Loss in iteration 174 : 0.009393593657311563
Loss in iteration 175 : 0.009353187604650811
Loss in iteration 176 : 0.009313126200622647
Loss in iteration 177 : 0.009273404668662766
Loss in iteration 178 : 0.009234018326758192
Loss in iteration 179 : 0.00919496258496972
Loss in iteration 180 : 0.009156232943036314
Loss in iteration 181 : 0.00911782498805798
Loss in iteration 182 : 0.009079734392254561
Loss in iteration 183 : 0.009041956910797533
Loss in iteration 184 : 0.00900448837971203
Loss in iteration 185 : 0.00896732471384681
Loss in iteration 186 : 0.008930461904909075
Loss in iteration 187 : 0.008893896019562057
Loss in iteration 188 : 0.008857623197582787
Loss in iteration 189 : 0.00882163965007752
Loss in iteration 190 : 0.008785941657752806
Loss in iteration 191 : 0.008750525569239656
Loss in iteration 192 : 0.008715387799468944
Loss in iteration 193 : 0.008680524828095929
Loss in iteration 194 : 0.008645933197971934
Loss in iteration 195 : 0.008611609513661436
Loss in iteration 196 : 0.00857755044000289
Loss in iteration 197 : 0.008543752700711522
Loss in iteration 198 : 0.008510213077022825
Loss in iteration 199 : 0.008476928406374927
Loss in iteration 200 : 0.008443895581128865
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.9857777777777778, training accuracy 0.9994284081166047, time elapsed: 5712 millisecond.
