objc[3299]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x102baa4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x1043ef4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 09:53:30 INFO SparkContext: Running Spark version 2.0.0
18/02/27 09:53:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 09:53:31 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 09:53:31 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 09:53:31 INFO SecurityManager: Changing view acls groups to: 
18/02/27 09:53:31 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 09:53:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 09:53:31 INFO Utils: Successfully started service 'sparkDriver' on port 50680.
18/02/27 09:53:31 INFO SparkEnv: Registering MapOutputTracker
18/02/27 09:53:31 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 09:53:31 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-1ed17236-1bed-4fe6-9148-b924821ac42f
18/02/27 09:53:31 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 09:53:32 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 09:53:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 09:53:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 09:53:32 INFO Executor: Starting executor ID driver on host localhost
18/02/27 09:53:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50681.
18/02/27 09:53:32 INFO NettyBlockTransferService: Server created on 192.168.2.140:50681
18/02/27 09:53:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50681)
18/02/27 09:53:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50681 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50681)
18/02/27 09:53:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50681)
Loss in iteration 1 : 1.7442999016480714
Loss in iteration 2 : 31.615482834423165
Loss in iteration 3 : 18.870934776457887
Loss in iteration 4 : 6.50504293639505
Loss in iteration 5 : 2.942698146276232
Loss in iteration 6 : 2.5980376483357763
Loss in iteration 7 : 2.3026642711331373
Loss in iteration 8 : 2.1428805710769927
Loss in iteration 9 : 2.0105633350984564
Loss in iteration 10 : 1.882642443492791
Loss in iteration 11 : 1.7589156124118386
Loss in iteration 12 : 1.6396296414062783
Loss in iteration 13 : 1.525666529251293
Loss in iteration 14 : 1.4244733018621756
Loss in iteration 15 : 1.345751115401195
Loss in iteration 16 : 1.4054550887513546
Loss in iteration 17 : 2.1219296504289824
Loss in iteration 18 : 6.875277282661501
Loss in iteration 19 : 4.250873623267082
Loss in iteration 20 : 10.811199603511406
Loss in iteration 21 : 1.5258588478434647
Loss in iteration 22 : 1.4195684027871167
Loss in iteration 23 : 1.3205286347439722
Loss in iteration 24 : 1.249055059248123
Loss in iteration 25 : 1.3638980257117497
Loss in iteration 26 : 2.767258190034115
Loss in iteration 27 : 10.493069717848138
Loss in iteration 28 : 22.251075639051848
Loss in iteration 29 : 9.713974242229115
Loss in iteration 30 : 2.2299942909313915
Loss in iteration 31 : 2.100293972600831
Loss in iteration 32 : 1.971540599407444
Loss in iteration 33 : 1.855394269578165
Loss in iteration 34 : 1.74000146442157
Loss in iteration 35 : 1.637314143111434
Loss in iteration 36 : 1.5420602780418446
Loss in iteration 37 : 1.4853874494467618
Loss in iteration 38 : 1.5616248052650195
Loss in iteration 39 : 2.327962990032464
Loss in iteration 40 : 4.793488470253206
Loss in iteration 41 : 12.131499729120263
Loss in iteration 42 : 1.7327647788065688
Loss in iteration 43 : 1.801704542852979
Loss in iteration 44 : 2.4131872912843773
Loss in iteration 45 : 3.4998677921329633
Loss in iteration 46 : 8.170380377587977
Loss in iteration 47 : 2.3709540906262583
Loss in iteration 48 : 3.5816135341017303
Loss in iteration 49 : 4.50078315948839
Loss in iteration 50 : 8.098506340131143
Loss in iteration 51 : 2.3814476208894364
Loss in iteration 52 : 2.7752996233089027
Loss in iteration 53 : 2.5401933982169345
Loss in iteration 54 : 3.4904510164975178
Loss in iteration 55 : 3.7738164547952335
Loss in iteration 56 : 6.461346666048698
Loss in iteration 57 : 3.4155794760412315
Loss in iteration 58 : 4.560979078442125
Loss in iteration 59 : 3.581205848429521
Loss in iteration 60 : 4.625157196635289
Loss in iteration 61 : 3.4373653076668873
Loss in iteration 62 : 4.2103522510780556
Loss in iteration 63 : 3.087510160203962
Loss in iteration 64 : 3.5083440149018927
Loss in iteration 65 : 2.6417485750200287
Loss in iteration 66 : 2.99888748469896
Loss in iteration 67 : 2.5517479454472363
Loss in iteration 68 : 3.168141300755566
Loss in iteration 69 : 2.8787546261689285
Loss in iteration 70 : 4.317309316152908
Loss in iteration 71 : 4.053901100364409
Loss in iteration 72 : 6.463669140092328
Loss in iteration 73 : 3.3217024463151574
Loss in iteration 74 : 4.027761702195254
Loss in iteration 75 : 3.0748067612274
Loss in iteration 76 : 3.5530209226402283
Loss in iteration 77 : 2.818375715967954
Loss in iteration 78 : 3.36075224101675
Loss in iteration 79 : 2.765382634688091
Loss in iteration 80 : 3.577325682507947
Loss in iteration 81 : 3.1922437356768256
Loss in iteration 82 : 4.616847712770155
Loss in iteration 83 : 3.85803142634529
Loss in iteration 84 : 5.517905312723372
Loss in iteration 85 : 3.505989392621969
Loss in iteration 86 : 4.246652467322939
Loss in iteration 87 : 3.0495136296441543
Loss in iteration 88 : 3.4268942108163567
Loss in iteration 89 : 2.5889065898500143
Loss in iteration 90 : 2.9983543038250473
Loss in iteration 91 : 2.5415042601698774
Loss in iteration 92 : 3.1563584206790116
Loss in iteration 93 : 2.9015150415737456
Loss in iteration 94 : 4.440390991764407
Loss in iteration 95 : 4.093818627639058
Loss in iteration 96 : 6.4957760763815
Loss in iteration 97 : 3.2638272512322106
Loss in iteration 98 : 3.886020729432687
Loss in iteration 99 : 2.890347535515589
Loss in iteration 100 : 3.3545319797527635
Loss in iteration 101 : 2.678088185655585
Loss in iteration 102 : 3.2046857326481546
Loss in iteration 103 : 2.756175687319982
Loss in iteration 104 : 3.6800086191681305
Loss in iteration 105 : 3.549211226674897
Loss in iteration 106 : 5.419290141501181
Loss in iteration 107 : 3.7922469720537237
Loss in iteration 108 : 5.036563267331883
Loss in iteration 109 : 3.4515824052186455
Loss in iteration 110 : 4.166869265120216
Loss in iteration 111 : 3.066118204246578
Loss in iteration 112 : 3.4504890276604825
Loss in iteration 113 : 2.620018032877307
Loss in iteration 114 : 3.0000598457316365
Loss in iteration 115 : 2.5263250485339337
Loss in iteration 116 : 3.1343928097941367
Loss in iteration 117 : 2.906410883902624
Loss in iteration 118 : 4.43699764550456
Loss in iteration 119 : 4.077998125964341
Loss in iteration 120 : 6.474947205933892
Loss in iteration 121 : 3.2623509948207734
Loss in iteration 122 : 3.9293040684755827
Loss in iteration 123 : 2.964901405420122
Loss in iteration 124 : 3.4493092633126814
Loss in iteration 125 : 2.653065216297566
Loss in iteration 126 : 3.1217946735766278
Loss in iteration 127 : 2.684408375627398
Loss in iteration 128 : 3.57623985066015
Loss in iteration 129 : 3.401281153216652
Loss in iteration 130 : 5.267048318936582
Loss in iteration 131 : 3.8654943160557247
Loss in iteration 132 : 5.36559328799002
Loss in iteration 133 : 3.468433271565528
Loss in iteration 134 : 4.171144830372907
Loss in iteration 135 : 3.03507079481495
Loss in iteration 136 : 3.4200309634196704
Loss in iteration 137 : 2.5680154605717282
Loss in iteration 138 : 2.9700313552034117
Loss in iteration 139 : 2.520362645291247
Loss in iteration 140 : 3.1374509853716672
Loss in iteration 141 : 2.909321073339381
Loss in iteration 142 : 4.47243213371017
Loss in iteration 143 : 4.109479143090563
Loss in iteration 144 : 6.530525380580943
Loss in iteration 145 : 3.2465408988149336
Loss in iteration 146 : 3.850800558948795
Loss in iteration 147 : 2.86533209191422
Loss in iteration 148 : 3.2710324798632793
Loss in iteration 149 : 2.6148596025129205
Loss in iteration 150 : 3.124032655374913
Loss in iteration 151 : 2.72411035101086
Loss in iteration 152 : 3.7439746483995924
Loss in iteration 153 : 3.6207441188688616
Loss in iteration 154 : 5.65823655028556
Loss in iteration 155 : 3.702737309223838
Loss in iteration 156 : 4.982632185903323
Loss in iteration 157 : 3.424824438709545
Loss in iteration 158 : 4.146326785052452
Loss in iteration 159 : 3.0558651452189314
Loss in iteration 160 : 3.463733973982736
Loss in iteration 161 : 2.6470738465349317
Loss in iteration 162 : 3.0524445769685786
Loss in iteration 163 : 2.5401560364086326
Loss in iteration 164 : 3.1209480967459866
Loss in iteration 165 : 2.869897509487622
Loss in iteration 166 : 4.353295552649423
Loss in iteration 167 : 4.02078650861814
Loss in iteration 168 : 6.335921982998527
Loss in iteration 169 : 3.3408871552644985
Loss in iteration 170 : 4.097702103546601
Loss in iteration 171 : 3.0973235371722145
Loss in iteration 172 : 3.647693964290855
Loss in iteration 173 : 2.840406426623102
Loss in iteration 174 : 3.359943992821512
Loss in iteration 175 : 2.718325784631197
Loss in iteration 176 : 3.3822011271484795
Loss in iteration 177 : 3.0318036364134353
Loss in iteration 178 : 4.380888468406127
Loss in iteration 179 : 3.8511018250351996
Loss in iteration 180 : 5.637680135008771
Loss in iteration 181 : 3.5206117896634885
Loss in iteration 182 : 4.275817520069863
Loss in iteration 183 : 3.0942844736739326
Loss in iteration 184 : 3.48721365923228
Loss in iteration 185 : 2.6407014397156927
Loss in iteration 186 : 3.0374444298465444
Loss in iteration 187 : 2.541913910852585
Loss in iteration 188 : 3.140062542496057
Loss in iteration 189 : 2.8951605648623326
Loss in iteration 190 : 4.4192361288545206
Loss in iteration 191 : 4.032086321586719
Loss in iteration 192 : 6.2993856957844985
Loss in iteration 193 : 3.3448179061933394
Loss in iteration 194 : 4.0937886075593015
Loss in iteration 195 : 3.1014454667223648
Loss in iteration 196 : 3.6446276613558357
Loss in iteration 197 : 2.804415673895112
Loss in iteration 198 : 3.2894390223372345
Loss in iteration 199 : 2.6799918621439884
Loss in iteration 200 : 3.358504438328731
Testing accuracy  of updater 0 on alg 1 with rate 0.011199999999999998 = 0.77225, training accuracy 0.7549368727743606, time elapsed: 7954 millisecond.
Loss in iteration 1 : 1.364706951807555
Loss in iteration 2 : 22.142873207474683
Loss in iteration 3 : 13.3247629653279
Loss in iteration 4 : 4.700816241120716
Loss in iteration 5 : 2.0630867031322104
Loss in iteration 6 : 1.8291140625444786
Loss in iteration 7 : 1.6546969441871806
Loss in iteration 8 : 1.5480242086283025
Loss in iteration 9 : 1.4634446934070786
Loss in iteration 10 : 1.384571935861153
Loss in iteration 11 : 1.3061651632175537
Loss in iteration 12 : 1.2339059391964124
Loss in iteration 13 : 1.1649489184921262
Loss in iteration 14 : 1.1073009352650387
Loss in iteration 15 : 1.09613848125479
Loss in iteration 16 : 1.3035128987624491
Loss in iteration 17 : 2.1978279964026406
Loss in iteration 18 : 6.082889157687985
Loss in iteration 19 : 1.5718629779273314
Loss in iteration 20 : 2.816642175118541
Loss in iteration 21 : 4.383811243688947
Loss in iteration 22 : 8.101374379957237
Loss in iteration 23 : 1.3859754488180032
Loss in iteration 24 : 1.3421203557373524
Loss in iteration 25 : 1.3501875900966476
Loss in iteration 26 : 1.3533864677342797
Loss in iteration 27 : 1.6486292701348648
Loss in iteration 28 : 2.1025772273023104
Loss in iteration 29 : 4.437526415174428
Loss in iteration 30 : 2.8578065569758504
Loss in iteration 31 : 5.08447978177356
Loss in iteration 32 : 2.084974116951919
Loss in iteration 33 : 2.6300126291727
Loss in iteration 34 : 2.370356777504433
Loss in iteration 35 : 3.300575104277053
Loss in iteration 36 : 2.6079661139175063
Loss in iteration 37 : 3.443220721643274
Loss in iteration 38 : 2.415586404618294
Loss in iteration 39 : 2.8773299623783672
Loss in iteration 40 : 2.1432197487268976
Loss in iteration 41 : 2.370130807002134
Loss in iteration 42 : 1.8604601319980918
Loss in iteration 43 : 2.0845005377403343
Loss in iteration 44 : 1.8026437508702897
Loss in iteration 45 : 2.1879964176779985
Loss in iteration 46 : 2.0275416520632423
Loss in iteration 47 : 2.9026232280266098
Loss in iteration 48 : 2.839846956680085
Loss in iteration 49 : 4.360748839887287
Loss in iteration 50 : 2.387148655742484
Loss in iteration 51 : 2.9444451806421674
Loss in iteration 52 : 2.210985616911325
Loss in iteration 53 : 2.485454693790965
Loss in iteration 54 : 1.9099236032339153
Loss in iteration 55 : 2.19684475697545
Loss in iteration 56 : 1.8545957874565315
Loss in iteration 57 : 2.2239388768304034
Loss in iteration 58 : 1.9750294825852621
Loss in iteration 59 : 2.7236736849862604
Loss in iteration 60 : 2.616491900130462
Loss in iteration 61 : 4.025592748908944
Loss in iteration 62 : 2.5754367451171447
Loss in iteration 63 : 3.316181119835048
Loss in iteration 64 : 2.3911112282240605
Loss in iteration 65 : 2.8012785717119657
Loss in iteration 66 : 2.033553785979409
Loss in iteration 67 : 2.224272449627722
Loss in iteration 68 : 1.7871976567357857
Loss in iteration 69 : 2.0528868808143965
Loss in iteration 70 : 1.7857543272585241
Loss in iteration 71 : 2.2419938638835393
Loss in iteration 72 : 2.10029823063261
Loss in iteration 73 : 3.2197550085675757
Loss in iteration 74 : 2.8769360684058154
Loss in iteration 75 : 4.353102265086086
Loss in iteration 76 : 2.365440047648361
Loss in iteration 77 : 2.818134095780945
Loss in iteration 78 : 2.1052197349867576
Loss in iteration 79 : 2.382248123667942
Loss in iteration 80 : 1.8352361378189492
Loss in iteration 81 : 2.109032581410536
Loss in iteration 82 : 1.8202900433272124
Loss in iteration 83 : 2.2386067599411232
Loss in iteration 84 : 2.04548105066591
Loss in iteration 85 : 2.988895692376801
Loss in iteration 86 : 2.8079872866931965
Loss in iteration 87 : 4.262124311815219
Loss in iteration 88 : 2.3980614191907486
Loss in iteration 89 : 2.904768575624019
Loss in iteration 90 : 2.180722938199652
Loss in iteration 91 : 2.461171388860882
Loss in iteration 92 : 1.8843228078245071
Loss in iteration 93 : 2.138246986367943
Loss in iteration 94 : 1.8173135956210138
Loss in iteration 95 : 2.1879627398262538
Loss in iteration 96 : 1.959199792641935
Loss in iteration 97 : 2.7777180934994585
Loss in iteration 98 : 2.609995972501488
Loss in iteration 99 : 4.021738971833646
Loss in iteration 100 : 2.5636636130621415
Loss in iteration 101 : 3.2917217511716688
Loss in iteration 102 : 2.3850578196879666
Loss in iteration 103 : 2.8121816342726893
Loss in iteration 104 : 2.0243473913069994
Loss in iteration 105 : 2.2424854410224486
Loss in iteration 106 : 1.7718763324015105
Loss in iteration 107 : 2.025178295752164
Loss in iteration 108 : 1.790218942899676
Loss in iteration 109 : 2.2097297569211425
Loss in iteration 110 : 2.062221881761625
Loss in iteration 111 : 3.1685206351190063
Loss in iteration 112 : 2.8296598905547476
Loss in iteration 113 : 4.303396019236589
Loss in iteration 114 : 2.394729738813508
Loss in iteration 115 : 2.898734258355148
Loss in iteration 116 : 2.1587585537743155
Loss in iteration 117 : 2.4562405595571204
Loss in iteration 118 : 1.8862008062419795
Loss in iteration 119 : 2.1356665952515583
Loss in iteration 120 : 1.8070134139597378
Loss in iteration 121 : 2.1749077115451616
Loss in iteration 122 : 1.9596547503910176
Loss in iteration 123 : 2.7767558299389186
Loss in iteration 124 : 2.611926786307304
Loss in iteration 125 : 4.03054936252286
Loss in iteration 126 : 2.55447149204839
Loss in iteration 127 : 3.247867074269046
Loss in iteration 128 : 2.376150931905888
Loss in iteration 129 : 2.8205719089295456
Loss in iteration 130 : 2.045467090266141
Loss in iteration 131 : 2.286166464280558
Loss in iteration 132 : 1.7966185054303707
Loss in iteration 133 : 2.0699908082413505
Loss in iteration 134 : 1.7862645621955324
Loss in iteration 135 : 2.181614701580235
Loss in iteration 136 : 2.035967427868957
Loss in iteration 137 : 3.05395618010869
Loss in iteration 138 : 2.8494212729897295
Loss in iteration 139 : 4.3856341987959055
Loss in iteration 140 : 2.351949671008841
Loss in iteration 141 : 2.840030111413688
Loss in iteration 142 : 2.135761592025286
Loss in iteration 143 : 2.4559608829501935
Loss in iteration 144 : 1.8827561920566682
Loss in iteration 145 : 2.1270899511473
Loss in iteration 146 : 1.811559102541479
Loss in iteration 147 : 2.1781513181346472
Loss in iteration 148 : 1.9807218177821475
Loss in iteration 149 : 2.892415302079789
Loss in iteration 150 : 2.752172232307148
Loss in iteration 151 : 4.2696765055379675
Loss in iteration 152 : 2.4282524701966968
Loss in iteration 153 : 2.916050676680384
Loss in iteration 154 : 2.1912467517096976
Loss in iteration 155 : 2.527259719027531
Loss in iteration 156 : 1.9096699635822387
Loss in iteration 157 : 2.151350008456054
Loss in iteration 158 : 1.8056642182527862
Loss in iteration 159 : 2.1517238419613687
Loss in iteration 160 : 1.9643145735268834
Loss in iteration 161 : 2.7938882262007048
Loss in iteration 162 : 2.589341346208423
Loss in iteration 163 : 3.999390952147087
Loss in iteration 164 : 2.581751913269481
Loss in iteration 165 : 3.32151261310255
Loss in iteration 166 : 2.3656007238625585
Loss in iteration 167 : 2.783495268742115
Loss in iteration 168 : 2.0284326395919683
Loss in iteration 169 : 2.2628464874647714
Loss in iteration 170 : 1.7801386903141285
Loss in iteration 171 : 2.0395654262264125
Loss in iteration 172 : 1.7780403954172395
Loss in iteration 173 : 2.1599991508023932
Loss in iteration 174 : 2.005802188231177
Loss in iteration 175 : 3.0078985332078756
Loss in iteration 176 : 2.883908712866106
Loss in iteration 177 : 4.478384771619207
Loss in iteration 178 : 2.331605334187467
Loss in iteration 179 : 2.812978421456184
Loss in iteration 180 : 2.104805791041576
Loss in iteration 181 : 2.397597949990058
Loss in iteration 182 : 1.8486367447678356
Loss in iteration 183 : 2.1201251899764824
Loss in iteration 184 : 1.811877351143389
Loss in iteration 185 : 2.2779638582873356
Loss in iteration 186 : 2.0885109110846067
Loss in iteration 187 : 3.1356905749339585
Loss in iteration 188 : 2.744555000711224
Loss in iteration 189 : 4.102591871413085
Loss in iteration 190 : 2.46216887564998
Loss in iteration 191 : 2.913937748668601
Loss in iteration 192 : 2.144919413801426
Loss in iteration 193 : 2.4504040346409743
Loss in iteration 194 : 1.8775399363417038
Loss in iteration 195 : 2.1159146102007274
Loss in iteration 196 : 1.7978912497239947
Loss in iteration 197 : 2.1483165460426243
Loss in iteration 198 : 1.9669027192189996
Loss in iteration 199 : 2.81492340563151
Loss in iteration 200 : 2.6244848236359224
Testing accuracy  of updater 0 on alg 1 with rate 0.00784 = 0.5305, training accuracy 0.6668824862415021, time elapsed: 5143 millisecond.
Loss in iteration 1 : 1.1190879842636914
Loss in iteration 2 : 12.749693101414696
Loss in iteration 3 : 7.769408138068917
Loss in iteration 4 : 2.8559078401669997
Loss in iteration 5 : 1.2520545957690543
Loss in iteration 6 : 1.1140371152579902
Loss in iteration 7 : 1.022762109713091
Loss in iteration 8 : 0.9693590301693282
Loss in iteration 9 : 0.9244801418986308
Loss in iteration 10 : 0.8835540395192493
Loss in iteration 11 : 0.8438515719151084
Loss in iteration 12 : 0.8063618283251586
Loss in iteration 13 : 0.7695486879265394
Loss in iteration 14 : 0.7341187846286181
Loss in iteration 15 : 0.7046877834327927
Loss in iteration 16 : 0.6933305643565015
Loss in iteration 17 : 0.7733445175404777
Loss in iteration 18 : 1.3733039889459773
Loss in iteration 19 : 3.428682610532608
Loss in iteration 20 : 6.5292160543760875
Loss in iteration 21 : 1.7246539758363837
Loss in iteration 22 : 1.3852410787450529
Loss in iteration 23 : 1.5740964020300345
Loss in iteration 24 : 1.2856866663944186
Loss in iteration 25 : 1.4323960011322923
Loss in iteration 26 : 1.2134218255374964
Loss in iteration 27 : 1.3675510698937057
Loss in iteration 28 : 1.230032016424231
Loss in iteration 29 : 1.519769782477793
Loss in iteration 30 : 1.4368446708908071
Loss in iteration 31 : 1.9646265816168789
Loss in iteration 32 : 1.6043004058691201
Loss in iteration 33 : 2.096778836008827
Loss in iteration 34 : 1.4956549072347531
Loss in iteration 35 : 1.7559462494021232
Loss in iteration 36 : 1.3166686232908906
Loss in iteration 37 : 1.391108838926182
Loss in iteration 38 : 1.1375795858570033
Loss in iteration 39 : 1.2449770010764851
Loss in iteration 40 : 1.1105855305511592
Loss in iteration 41 : 1.2882917636632565
Loss in iteration 42 : 1.2041885998912796
Loss in iteration 43 : 1.5586824498720389
Loss in iteration 44 : 1.526296071319555
Loss in iteration 45 : 2.1890288414759107
Loss in iteration 46 : 1.6144690734595288
Loss in iteration 47 : 2.0381477245716506
Loss in iteration 48 : 1.4311292930652828
Loss in iteration 49 : 1.6495786944882964
Loss in iteration 50 : 1.249028285994123
Loss in iteration 51 : 1.324255757643872
Loss in iteration 52 : 1.0986203244970878
Loss in iteration 53 : 1.2177634122834486
Loss in iteration 54 : 1.1250798284172319
Loss in iteration 55 : 1.3372060388558862
Loss in iteration 56 : 1.2637793197886453
Loss in iteration 57 : 1.7958386501133206
Loss in iteration 58 : 1.6558813984414054
Loss in iteration 59 : 2.328551095095646
Loss in iteration 60 : 1.5036028973859599
Loss in iteration 61 : 1.7575819192125155
Loss in iteration 62 : 1.3186509825191712
Loss in iteration 63 : 1.432468200684533
Loss in iteration 64 : 1.152598289410739
Loss in iteration 65 : 1.2707010953259423
Loss in iteration 66 : 1.109966210333513
Loss in iteration 67 : 1.3061812882208161
Loss in iteration 68 : 1.195975477555503
Loss in iteration 69 : 1.5455969784324282
Loss in iteration 70 : 1.4727224201518818
Loss in iteration 71 : 2.0885578525334876
Loss in iteration 72 : 1.618761184363229
Loss in iteration 73 : 2.0996461975574787
Loss in iteration 74 : 1.4122991050048703
Loss in iteration 75 : 1.6384607588902378
Loss in iteration 76 : 1.2351722179375302
Loss in iteration 77 : 1.3188027889534748
Loss in iteration 78 : 1.0940986752574937
Loss in iteration 79 : 1.1968569751172333
Loss in iteration 80 : 1.0935083907849028
Loss in iteration 81 : 1.295166894958151
Loss in iteration 82 : 1.226146763832749
Loss in iteration 83 : 1.728175795372674
Loss in iteration 84 : 1.6006729827864525
Loss in iteration 85 : 2.3150539556761003
Loss in iteration 86 : 1.5344237783640768
Loss in iteration 87 : 1.8506349832691187
Loss in iteration 88 : 1.34112916362456
Loss in iteration 89 : 1.5030722776380527
Loss in iteration 90 : 1.1640161617959806
Loss in iteration 91 : 1.2699725381470799
Loss in iteration 92 : 1.1109247951926784
Loss in iteration 93 : 1.2667026896687437
Loss in iteration 94 : 1.1533284332323874
Loss in iteration 95 : 1.4758874983527575
Loss in iteration 96 : 1.4087586827669552
Loss in iteration 97 : 1.9954945324538438
Loss in iteration 98 : 1.5748865131740997
Loss in iteration 99 : 2.0840952632159313
Loss in iteration 100 : 1.4426219155563178
Loss in iteration 101 : 1.7101760513909356
Loss in iteration 102 : 1.2835688641311964
Loss in iteration 103 : 1.4374055814588178
Loss in iteration 104 : 1.154171536664976
Loss in iteration 105 : 1.2678413068036676
Loss in iteration 106 : 1.1159403710690021
Loss in iteration 107 : 1.2951245685893746
Loss in iteration 108 : 1.179288683922756
Loss in iteration 109 : 1.557833802201169
Loss in iteration 110 : 1.4803419219751541
Loss in iteration 111 : 2.1172448152848857
Loss in iteration 112 : 1.608628516917936
Loss in iteration 113 : 2.054828576947063
Loss in iteration 114 : 1.4094122572740422
Loss in iteration 115 : 1.6325308164734265
Loss in iteration 116 : 1.232352890745369
Loss in iteration 117 : 1.3401121244978962
Loss in iteration 118 : 1.1014154457940222
Loss in iteration 119 : 1.2260642338259058
Loss in iteration 120 : 1.115961455843675
Loss in iteration 121 : 1.3125086245575184
Loss in iteration 122 : 1.229075412854744
Loss in iteration 123 : 1.7185197614971166
Loss in iteration 124 : 1.5551959225183
Loss in iteration 125 : 2.1913325072273224
Loss in iteration 126 : 1.5629454877730538
Loss in iteration 127 : 1.9066639107695624
Loss in iteration 128 : 1.3494822514766527
Loss in iteration 129 : 1.5187564390923793
Loss in iteration 130 : 1.1719912348420858
Loss in iteration 131 : 1.2612488009134142
Loss in iteration 132 : 1.1030301044847546
Loss in iteration 133 : 1.245504081573221
Loss in iteration 134 : 1.1355920806174684
Loss in iteration 135 : 1.414391035562192
Loss in iteration 136 : 1.3533685307669354
Loss in iteration 137 : 1.9237793322727528
Loss in iteration 138 : 1.6001970631087905
Loss in iteration 139 : 2.1773767927975225
Loss in iteration 140 : 1.4944543745093481
Loss in iteration 141 : 1.759953777595097
Loss in iteration 142 : 1.29831423334575
Loss in iteration 143 : 1.4299358603483665
Loss in iteration 144 : 1.1417093243588599
Loss in iteration 145 : 1.236314598943187
Loss in iteration 146 : 1.1025006299919606
Loss in iteration 147 : 1.260680379822193
Loss in iteration 148 : 1.1461506756592752
Loss in iteration 149 : 1.4920315655873448
Loss in iteration 150 : 1.4357098451756
Loss in iteration 151 : 2.0618333055844684
Loss in iteration 152 : 1.616624430045088
Loss in iteration 153 : 2.1094386359225816
Loss in iteration 154 : 1.415622497361697
Loss in iteration 155 : 1.6795241796114881
Loss in iteration 156 : 1.2585002125659042
Loss in iteration 157 : 1.3750082758255988
Loss in iteration 158 : 1.099243676077179
Loss in iteration 159 : 1.2191249426077064
Loss in iteration 160 : 1.1096065333635083
Loss in iteration 161 : 1.3099211973552083
Loss in iteration 162 : 1.2274481113400004
Loss in iteration 163 : 1.700013849505558
Loss in iteration 164 : 1.537199385644478
Loss in iteration 165 : 2.1761458598595924
Loss in iteration 166 : 1.5716740819323725
Loss in iteration 167 : 1.9391573905441315
Loss in iteration 168 : 1.3749671828972587
Loss in iteration 169 : 1.567279591097667
Loss in iteration 170 : 1.1991901552172899
Loss in iteration 171 : 1.2886474372850067
Loss in iteration 172 : 1.0981761757591784
Loss in iteration 173 : 1.2308353878613898
Loss in iteration 174 : 1.1269567082752308
Loss in iteration 175 : 1.3996043272704728
Loss in iteration 176 : 1.3197415164467097
Loss in iteration 177 : 1.826819556961374
Loss in iteration 178 : 1.606999540326451
Loss in iteration 179 : 2.192806904738825
Loss in iteration 180 : 1.509674639885705
Loss in iteration 181 : 1.784054271828715
Loss in iteration 182 : 1.2956925156944312
Loss in iteration 183 : 1.4328882741108595
Loss in iteration 184 : 1.139867407559017
Loss in iteration 185 : 1.2391738013420646
Loss in iteration 186 : 1.1004507677732256
Loss in iteration 187 : 1.2638030971373335
Loss in iteration 188 : 1.1435280415195859
Loss in iteration 189 : 1.4771688483031078
Loss in iteration 190 : 1.4116866081659643
Loss in iteration 191 : 1.9959994623643886
Loss in iteration 192 : 1.5696249243180698
Loss in iteration 193 : 2.0973706719994483
Loss in iteration 194 : 1.4394091104687325
Loss in iteration 195 : 1.7045972807734928
Loss in iteration 196 : 1.284922298852069
Loss in iteration 197 : 1.4346154878636717
Loss in iteration 198 : 1.143567994623761
Loss in iteration 199 : 1.2504158634039544
Loss in iteration 200 : 1.1144526565959059
Testing accuracy  of updater 0 on alg 1 with rate 0.00448 = 0.7015, training accuracy 0.7785691162188411, time elapsed: 4165 millisecond.
Loss in iteration 1 : 1.0074429990164808
Loss in iteration 2 : 3.4379841031800114
Loss in iteration 3 : 2.2075009125097074
Loss in iteration 4 : 0.9807241602271816
Loss in iteration 5 : 0.5291983121947859
Loss in iteration 6 : 0.5193151164485037
Loss in iteration 7 : 0.504781542544487
Loss in iteration 8 : 0.5001494469813992
Loss in iteration 9 : 0.49361921753369703
Loss in iteration 10 : 0.499785041083027
Loss in iteration 11 : 0.5075177748808435
Loss in iteration 12 : 0.5280225522775585
Loss in iteration 13 : 0.5593358200792677
Loss in iteration 14 : 0.6170666630030547
Loss in iteration 15 : 0.6171484511716208
Loss in iteration 16 : 0.660832068459779
Loss in iteration 17 : 0.6060746493583885
Loss in iteration 18 : 0.6167281721468145
Loss in iteration 19 : 0.5582366767179443
Loss in iteration 20 : 0.5495584073652486
Loss in iteration 21 : 0.5263656662036431
Loss in iteration 22 : 0.5254641939388661
Loss in iteration 23 : 0.5094134214618027
Loss in iteration 24 : 0.5088857445407465
Loss in iteration 25 : 0.506512821215627
Loss in iteration 26 : 0.5181855931769676
Loss in iteration 27 : 0.5195555154375765
Loss in iteration 28 : 0.5463760730985577
Loss in iteration 29 : 0.5552837013841453
Loss in iteration 30 : 0.6004168294531783
Loss in iteration 31 : 0.5835438290918644
Loss in iteration 32 : 0.6182712180712174
Loss in iteration 33 : 0.5708835554851279
Loss in iteration 34 : 0.58581601013471
Loss in iteration 35 : 0.5385254614925659
Loss in iteration 36 : 0.5340958948810587
Loss in iteration 37 : 0.511845050237616
Loss in iteration 38 : 0.5134120608708237
Loss in iteration 39 : 0.506916358895852
Loss in iteration 40 : 0.5167929200824587
Loss in iteration 41 : 0.516242605108886
Loss in iteration 42 : 0.5436272252538504
Loss in iteration 43 : 0.5418731100073327
Loss in iteration 44 : 0.5745805705220495
Loss in iteration 45 : 0.5545351660850001
Loss in iteration 46 : 0.5795267100896171
Loss in iteration 47 : 0.550287862344326
Loss in iteration 48 : 0.5694029124613007
Loss in iteration 49 : 0.5382772349145861
Loss in iteration 50 : 0.5496564400002117
Loss in iteration 51 : 0.5316536436323578
Loss in iteration 52 : 0.5466098012515349
Loss in iteration 53 : 0.5299481566662976
Loss in iteration 54 : 0.5476175652049341
Loss in iteration 55 : 0.5306654079604379
Loss in iteration 56 : 0.5468279182664741
Loss in iteration 57 : 0.5311823347549302
Loss in iteration 58 : 0.5487708674808204
Loss in iteration 59 : 0.5303650870310694
Loss in iteration 60 : 0.5467867070582133
Loss in iteration 61 : 0.5308004256909178
Loss in iteration 62 : 0.5467181631707633
Loss in iteration 63 : 0.5306968651221866
Loss in iteration 64 : 0.5466499649410324
Loss in iteration 65 : 0.5305954170218006
Loss in iteration 66 : 0.5444752786641811
Loss in iteration 67 : 0.5305252741936863
Loss in iteration 68 : 0.5458196673636607
Loss in iteration 69 : 0.5311223297561448
Loss in iteration 70 : 0.5463229168532775
Loss in iteration 71 : 0.5304708443276426
Loss in iteration 72 : 0.546828426643155
Loss in iteration 73 : 0.5311209720287388
Loss in iteration 74 : 0.547210092441785
Loss in iteration 75 : 0.5306072497369463
Loss in iteration 76 : 0.5442276161810683
Loss in iteration 77 : 0.526114177412657
Loss in iteration 78 : 0.540590899411658
Loss in iteration 79 : 0.5255404906535477
Loss in iteration 80 : 0.5453001275781736
Loss in iteration 81 : 0.5302975316693176
Loss in iteration 82 : 0.5500845562768146
Loss in iteration 83 : 0.5319252809498834
Loss in iteration 84 : 0.5525886883597673
Loss in iteration 85 : 0.5293150960439363
Loss in iteration 86 : 0.5490599424765529
Loss in iteration 87 : 0.5316149267902169
Loss in iteration 88 : 0.5526999875654867
Loss in iteration 89 : 0.5290097212452672
Loss in iteration 90 : 0.5457039527142957
Loss in iteration 91 : 0.5260325055959952
Loss in iteration 92 : 0.5421971802149372
Loss in iteration 93 : 0.5227340742625793
Loss in iteration 94 : 0.5377908472740882
Loss in iteration 95 : 0.522086753893711
Loss in iteration 96 : 0.5383494856892918
Loss in iteration 97 : 0.5232735653205273
Loss in iteration 98 : 0.5398779674279122
Loss in iteration 99 : 0.5248816801830197
Loss in iteration 100 : 0.5473415231406772
Loss in iteration 101 : 0.5322554377412435
Loss in iteration 102 : 0.5578421938341984
Loss in iteration 103 : 0.5321933851972477
Loss in iteration 104 : 0.5535088136101582
Loss in iteration 105 : 0.5318529145228048
Loss in iteration 106 : 0.5510169137531818
Loss in iteration 107 : 0.5293641526931382
Loss in iteration 108 : 0.5469959914896857
Loss in iteration 109 : 0.5240067952285357
Loss in iteration 110 : 0.5375702172067387
Loss in iteration 111 : 0.5211438015851558
Loss in iteration 112 : 0.5362620088542742
Loss in iteration 113 : 0.5223835253625964
Loss in iteration 114 : 0.5428309154140221
Loss in iteration 115 : 0.5250210051789334
Loss in iteration 116 : 0.546759638171431
Loss in iteration 117 : 0.5289309639469011
Loss in iteration 118 : 0.548494539170713
Loss in iteration 119 : 0.5317573563874244
Loss in iteration 120 : 0.5527188332798043
Loss in iteration 121 : 0.5295810228350212
Loss in iteration 122 : 0.5457229857145068
Loss in iteration 123 : 0.5240824992073053
Loss in iteration 124 : 0.5428154285338999
Loss in iteration 125 : 0.5239425247967551
Loss in iteration 126 : 0.5429414150478038
Loss in iteration 127 : 0.5238028792104554
Loss in iteration 128 : 0.5430682022094423
Loss in iteration 129 : 0.5246975216067221
Loss in iteration 130 : 0.5447416960651013
Loss in iteration 131 : 0.5249192337882572
Loss in iteration 132 : 0.5445118048263422
Loss in iteration 133 : 0.52514068476634
Loss in iteration 134 : 0.5447520723291371
Loss in iteration 135 : 0.5248927805811421
Loss in iteration 136 : 0.5449937946358578
Loss in iteration 137 : 0.5266361365730422
Loss in iteration 138 : 0.5432438386445686
Loss in iteration 139 : 0.5244792549216181
Loss in iteration 140 : 0.5453970096240636
Loss in iteration 141 : 0.5273339208261184
Loss in iteration 142 : 0.5443351618714753
Loss in iteration 143 : 0.5252799238049898
Loss in iteration 144 : 0.544577359086399
Loss in iteration 145 : 0.5260457898684116
Loss in iteration 146 : 0.5438025923295579
Loss in iteration 147 : 0.5247654904016987
Loss in iteration 148 : 0.545082000497891
Loss in iteration 149 : 0.5276226473385245
Loss in iteration 150 : 0.5445612144749871
Loss in iteration 151 : 0.5260425478098786
Loss in iteration 152 : 0.5442008664383375
Loss in iteration 153 : 0.5263958556831259
Loss in iteration 154 : 0.5438402121033084
Loss in iteration 155 : 0.5257170188873184
Loss in iteration 156 : 0.5445146290557208
Loss in iteration 157 : 0.5270528373527521
Loss in iteration 158 : 0.5451087788936851
Loss in iteration 159 : 0.5275661398163803
Loss in iteration 160 : 0.5445888669622984
Loss in iteration 161 : 0.526966673507417
Loss in iteration 162 : 0.5451810735394178
Loss in iteration 163 : 0.5263673518093751
Loss in iteration 164 : 0.543200507309683
Loss in iteration 165 : 0.5254959593111065
Loss in iteration 166 : 0.5422057851643618
Loss in iteration 167 : 0.5225547688699064
Loss in iteration 168 : 0.5415938344283037
Loss in iteration 169 : 0.5231625836885345
Loss in iteration 170 : 0.543559997737046
Loss in iteration 171 : 0.526994982449943
Loss in iteration 172 : 0.5438035788349427
Loss in iteration 173 : 0.5267474108180114
Loss in iteration 174 : 0.542866504070204
Loss in iteration 175 : 0.5258131772671174
Loss in iteration 176 : 0.5424091077879181
Loss in iteration 177 : 0.5252352964061817
Loss in iteration 178 : 0.5429846634869087
Loss in iteration 179 : 0.5256914820408132
Loss in iteration 180 : 0.5439213291201864
Loss in iteration 181 : 0.5287200354214909
Loss in iteration 182 : 0.545073183846189
Loss in iteration 183 : 0.5275597911514736
Loss in iteration 184 : 0.5420449787706213
Loss in iteration 185 : 0.5245705258384371
Loss in iteration 186 : 0.5430744877136249
Loss in iteration 187 : 0.5255918540527478
Loss in iteration 188 : 0.5426173928603693
Loss in iteration 189 : 0.5260486848105066
Loss in iteration 190 : 0.543549592259449
Loss in iteration 191 : 0.52698180931905
Loss in iteration 192 : 0.5426135761683478
Loss in iteration 193 : 0.5260485708164104
Loss in iteration 194 : 0.5435472415223613
Loss in iteration 195 : 0.5280923930444823
Loss in iteration 196 : 0.5431632898155405
Loss in iteration 197 : 0.5254932468126373
Loss in iteration 198 : 0.5421385449470456
Loss in iteration 199 : 0.5254844716238476
Loss in iteration 200 : 0.5435362258302912
Testing accuracy  of updater 0 on alg 1 with rate 0.00112 = 0.78975, training accuracy 0.7986403366785367, time elapsed: 3807 millisecond.
Loss in iteration 1 : 1.0036470695180755
Loss in iteration 2 : 2.511373256939162
Loss in iteration 3 : 1.651053333415161
Loss in iteration 4 : 0.793148417717214
Loss in iteration 5 : 0.4734889608546624
Loss in iteration 6 : 0.4811357497297464
Loss in iteration 7 : 0.4937275745654575
Loss in iteration 8 : 0.5037523694010242
Loss in iteration 9 : 0.5065488564879448
Loss in iteration 10 : 0.5063616774070201
Loss in iteration 11 : 0.49993922796472096
Loss in iteration 12 : 0.4942508229705629
Loss in iteration 13 : 0.47821637702253855
Loss in iteration 14 : 0.4677375195257403
Loss in iteration 15 : 0.4593848803677012
Loss in iteration 16 : 0.4553945248134551
Loss in iteration 17 : 0.45279182228544723
Loss in iteration 18 : 0.4539487087043058
Loss in iteration 19 : 0.453840570235797
Loss in iteration 20 : 0.45359411555056117
Loss in iteration 21 : 0.4564651100423535
Loss in iteration 22 : 0.4586436607998693
Loss in iteration 23 : 0.4631098483228397
Loss in iteration 24 : 0.4638587503105356
Loss in iteration 25 : 0.46170566248666733
Loss in iteration 26 : 0.4643965416255428
Loss in iteration 27 : 0.46060939798882194
Loss in iteration 28 : 0.462033884101705
Loss in iteration 29 : 0.46079410758649975
Loss in iteration 30 : 0.462898303029296
Loss in iteration 31 : 0.45883863861387875
Loss in iteration 32 : 0.45945630668251886
Loss in iteration 33 : 0.4573218338433148
Loss in iteration 34 : 0.45451783604479745
Loss in iteration 35 : 0.4554140462192818
Loss in iteration 36 : 0.4550331330157832
Loss in iteration 37 : 0.45660331700438195
Loss in iteration 38 : 0.458526513377966
Loss in iteration 39 : 0.45799070133679676
Loss in iteration 40 : 0.460206256324814
Loss in iteration 41 : 0.4564906229411935
Loss in iteration 42 : 0.45997507870940635
Loss in iteration 43 : 0.4570943423237479
Loss in iteration 44 : 0.46023730659938317
Loss in iteration 45 : 0.45762585453576066
Loss in iteration 46 : 0.46157477151134435
Loss in iteration 47 : 0.4570641181138099
Loss in iteration 48 : 0.4606340437304921
Loss in iteration 49 : 0.4551859322078431
Loss in iteration 50 : 0.45877965729357095
Loss in iteration 51 : 0.4561579538538086
Loss in iteration 52 : 0.4592750527048191
Loss in iteration 53 : 0.4553802856594674
Loss in iteration 54 : 0.45760709874276284
Loss in iteration 55 : 0.45439765245801883
Loss in iteration 56 : 0.45745485731534613
Loss in iteration 57 : 0.45284700034396763
Loss in iteration 58 : 0.4580966279859291
Loss in iteration 59 : 0.45362222992901247
Loss in iteration 60 : 0.4575826204790346
Loss in iteration 61 : 0.45395084499359384
Loss in iteration 62 : 0.4578225396347511
Loss in iteration 63 : 0.4541011330962608
Loss in iteration 64 : 0.45832395962500727
Loss in iteration 65 : 0.4540136443193885
Loss in iteration 66 : 0.4591071316146463
Loss in iteration 67 : 0.4536177562327517
Loss in iteration 68 : 0.45893178863787254
Loss in iteration 69 : 0.45362919593134077
Loss in iteration 70 : 0.4581002936542068
Loss in iteration 71 : 0.45430940602920233
Loss in iteration 72 : 0.45768491251357274
Loss in iteration 73 : 0.45335817303388215
Loss in iteration 74 : 0.4574186766932543
Loss in iteration 75 : 0.4527786324200356
Loss in iteration 76 : 0.4563499605685535
Loss in iteration 77 : 0.45083030560580667
Loss in iteration 78 : 0.4558090737093045
Loss in iteration 79 : 0.451261941949598
Loss in iteration 80 : 0.4570072935385492
Loss in iteration 81 : 0.4527823638157204
Loss in iteration 82 : 0.4560410779122059
Loss in iteration 83 : 0.4524007758472499
Loss in iteration 84 : 0.45632351470051385
Loss in iteration 85 : 0.45252619153529305
Loss in iteration 86 : 0.45610007189491586
Loss in iteration 87 : 0.4526534282088857
Loss in iteration 88 : 0.45670894128419803
Loss in iteration 89 : 0.4519541918348256
Loss in iteration 90 : 0.4551497463861564
Loss in iteration 91 : 0.4525659505129236
Loss in iteration 92 : 0.4564274798970802
Loss in iteration 93 : 0.4517775443416339
Loss in iteration 94 : 0.4540794013417106
Loss in iteration 95 : 0.45276054242116154
Loss in iteration 96 : 0.4552828775282041
Loss in iteration 97 : 0.45278173825278367
Loss in iteration 98 : 0.4545890836665811
Loss in iteration 99 : 0.4521148113932303
Loss in iteration 100 : 0.4540894962642986
Loss in iteration 101 : 0.4517252400552081
Loss in iteration 102 : 0.45441837233528526
Loss in iteration 103 : 0.45272489903179386
Loss in iteration 104 : 0.4547859554777972
Loss in iteration 105 : 0.45229985398596034
Loss in iteration 106 : 0.4535177872214444
Loss in iteration 107 : 0.4508651755059415
Loss in iteration 108 : 0.45491152401571533
Loss in iteration 109 : 0.45282326546615326
Loss in iteration 110 : 0.45607694213730865
Loss in iteration 111 : 0.45281796658488205
Loss in iteration 112 : 0.45604062274940244
Loss in iteration 113 : 0.4528120671195333
Loss in iteration 114 : 0.45537648359660604
Loss in iteration 115 : 0.45148664278268236
Loss in iteration 116 : 0.4522724174764429
Loss in iteration 117 : 0.4487676016915516
Loss in iteration 118 : 0.4517832692622273
Loss in iteration 119 : 0.4492214446745844
Loss in iteration 120 : 0.452036635725467
Loss in iteration 121 : 0.45039954390352005
Loss in iteration 122 : 0.452460408015449
Loss in iteration 123 : 0.45176261895727593
Loss in iteration 124 : 0.4542151026934572
Loss in iteration 125 : 0.4512024437929007
Loss in iteration 126 : 0.45387632508643777
Loss in iteration 127 : 0.45093925914476735
Loss in iteration 128 : 0.4533027960471405
Loss in iteration 129 : 0.4508612809840405
Loss in iteration 130 : 0.45336538998840564
Loss in iteration 131 : 0.4507834370355707
Loss in iteration 132 : 0.4534278977924812
Loss in iteration 133 : 0.45070525401103445
Loss in iteration 134 : 0.4526739141009016
Loss in iteration 135 : 0.4514498871001678
Loss in iteration 136 : 0.4551457124922291
Loss in iteration 137 : 0.4528371238335218
Loss in iteration 138 : 0.45539318533594386
Loss in iteration 139 : 0.45257200665826314
Loss in iteration 140 : 0.4548642796817321
Loss in iteration 141 : 0.4530848490146973
Loss in iteration 142 : 0.45511146427769755
Loss in iteration 143 : 0.45208704701422237
Loss in iteration 144 : 0.4544462097282529
Loss in iteration 145 : 0.4512743252620137
Loss in iteration 146 : 0.4520428085017774
Loss in iteration 147 : 0.44889874889734305
Loss in iteration 148 : 0.4509021651633991
Loss in iteration 149 : 0.4493180302377611
Loss in iteration 150 : 0.45164128409315707
Loss in iteration 151 : 0.44910727519947724
Loss in iteration 152 : 0.4518324832962688
Loss in iteration 153 : 0.4488985527003893
Loss in iteration 154 : 0.4528922795542521
Loss in iteration 155 : 0.4501336219933442
Loss in iteration 156 : 0.45565304472119567
Loss in iteration 157 : 0.4564418061289762
Loss in iteration 158 : 0.45751981647443024
Loss in iteration 159 : 0.4564491983993096
Loss in iteration 160 : 0.45683423855765704
Loss in iteration 161 : 0.4553029047314588
Loss in iteration 162 : 0.45559833240254993
Loss in iteration 163 : 0.4519252716216649
Loss in iteration 164 : 0.45303733640953087
Loss in iteration 165 : 0.44826156134265466
Loss in iteration 166 : 0.45004338886822143
Loss in iteration 167 : 0.4494582329356539
Loss in iteration 168 : 0.4517684263065529
Loss in iteration 169 : 0.4494823881622255
Loss in iteration 170 : 0.45172821338460334
Loss in iteration 171 : 0.4495087958678234
Loss in iteration 172 : 0.45231494227688523
Loss in iteration 173 : 0.4489119191148649
Loss in iteration 174 : 0.4511251727556911
Loss in iteration 175 : 0.44944300784682745
Loss in iteration 176 : 0.45322300725685133
Loss in iteration 177 : 0.44970613215846705
Loss in iteration 178 : 0.4544743413998776
Loss in iteration 179 : 0.4539547161015562
Loss in iteration 180 : 0.4559345290243721
Loss in iteration 181 : 0.4566732270902916
Loss in iteration 182 : 0.457553402884639
Loss in iteration 183 : 0.45501214658442596
Loss in iteration 184 : 0.4558033650601079
Loss in iteration 185 : 0.4517036694251573
Loss in iteration 186 : 0.4546893794595265
Loss in iteration 187 : 0.4481947223872841
Loss in iteration 188 : 0.44888040368820026
Loss in iteration 189 : 0.448409546481404
Loss in iteration 190 : 0.4492956802297719
Loss in iteration 191 : 0.44894749917253673
Loss in iteration 192 : 0.4519041881196971
Loss in iteration 193 : 0.44911854838896087
Loss in iteration 194 : 0.4524347245838147
Loss in iteration 195 : 0.44915579663043914
Loss in iteration 196 : 0.45239166820917265
Loss in iteration 197 : 0.44919299105947336
Loss in iteration 198 : 0.4523486838832019
Loss in iteration 199 : 0.44923013174609305
Loss in iteration 200 : 0.45230577147868195
Testing accuracy  of updater 0 on alg 1 with rate 7.84E-4 = 0.787, training accuracy 0.807381029459372, time elapsed: 4037 millisecond.
Loss in iteration 1 : 1.001190879842637
Loss in iteration 2 : 1.5855997738932512
Loss in iteration 3 : 1.0945699761678966
Loss in iteration 4 : 0.6071546928275156
Loss in iteration 5 : 0.439285748685509
Loss in iteration 6 : 0.47385236381682355
Loss in iteration 7 : 0.4613081700170899
Loss in iteration 8 : 0.4761796436023827
Loss in iteration 9 : 0.4474877867450823
Loss in iteration 10 : 0.44528169030151776
Loss in iteration 11 : 0.4305517288740352
Loss in iteration 12 : 0.4252924637622423
Loss in iteration 13 : 0.413776886024006
Loss in iteration 14 : 0.4062087646525824
Loss in iteration 15 : 0.39822941279976076
Loss in iteration 16 : 0.3948143913096599
Loss in iteration 17 : 0.3917958010057704
Loss in iteration 18 : 0.38942350996290154
Loss in iteration 19 : 0.3871953571949418
Loss in iteration 20 : 0.38606879450591913
Loss in iteration 21 : 0.38448810297285374
Loss in iteration 22 : 0.383602219458493
Testing accuracy  of updater 0 on alg 1 with rate 4.4799999999999994E-4 = 0.779, training accuracy 0.8332793784396245, time elapsed: 401 millisecond.
Loss in iteration 1 : 1.0000744299901647
Loss in iteration 2 : 0.6614130444633041
Loss in iteration 3 : 0.552857862074749
Loss in iteration 4 : 0.5234112484799033
Testing accuracy  of updater 0 on alg 1 with rate 1.119999999999999E-4 = 0.50225, training accuracy 0.6484299125930721, time elapsed: 94 millisecond.
Loss in iteration 1 : 1.0007268553727031
Loss in iteration 2 : 1.3173339091567493
Loss in iteration 3 : 1.8016547284619921
Loss in iteration 4 : 1.8541968315618034
Loss in iteration 5 : 1.5185339406961227
Loss in iteration 6 : 0.8579202513081134
Loss in iteration 7 : 0.5319061288540239
Loss in iteration 8 : 0.8895006617265317
Loss in iteration 9 : 1.2300040929201281
Loss in iteration 10 : 1.1542617448446233
Loss in iteration 11 : 0.8684240044031245
Loss in iteration 12 : 0.7024780196843852
Loss in iteration 13 : 0.7452141815990367
Loss in iteration 14 : 0.8873812814167479
Loss in iteration 15 : 0.9981061378491004
Loss in iteration 16 : 1.016269412878276
Loss in iteration 17 : 0.9507615684493673
Loss in iteration 18 : 0.8520890216093768
Loss in iteration 19 : 0.783571597623729
Loss in iteration 20 : 0.7782378639439913
Loss in iteration 21 : 0.8256913892384962
Loss in iteration 22 : 0.8760422278367267
Loss in iteration 23 : 0.8831379871805379
Loss in iteration 24 : 0.8395091214359924
Loss in iteration 25 : 0.7762527098928441
Loss in iteration 26 : 0.7340907014711114
Loss in iteration 27 : 0.727841542041485
Loss in iteration 28 : 0.7439980247842295
Loss in iteration 29 : 0.7549999865590873
Loss in iteration 30 : 0.7406283341251588
Loss in iteration 31 : 0.7019067425100262
Loss in iteration 32 : 0.6585375722793435
Loss in iteration 33 : 0.6316034742596652
Loss in iteration 34 : 0.6285888563097446
Loss in iteration 35 : 0.6329948624947392
Loss in iteration 36 : 0.6197016614878648
Loss in iteration 37 : 0.5847390694906193
Loss in iteration 38 : 0.5490176386016704
Loss in iteration 39 : 0.5358830762027472
Loss in iteration 40 : 0.5352581927876089
Loss in iteration 41 : 0.5256926606072491
Loss in iteration 42 : 0.4985558611804499
Loss in iteration 43 : 0.4688047887165898
Loss in iteration 44 : 0.45985997078344093
Loss in iteration 45 : 0.46036840019595054
Loss in iteration 46 : 0.44287519952310744
Loss in iteration 47 : 0.4179132365294872
Loss in iteration 48 : 0.41588080590815224
Loss in iteration 49 : 0.41770043996421524
Loss in iteration 50 : 0.39788506136976703
Loss in iteration 51 : 0.3980103917904829
Loss in iteration 52 : 0.4082916881725781
Loss in iteration 53 : 0.3914745400840084
Loss in iteration 54 : 0.4129675418672825
Loss in iteration 55 : 0.3990957896661829
Loss in iteration 56 : 0.41834234372560497
Loss in iteration 57 : 0.4009109304894024
Loss in iteration 58 : 0.4127549597650471
Loss in iteration 59 : 0.3966670434913812
Loss in iteration 60 : 0.4031900664402304
Loss in iteration 61 : 0.3901637001393356
Loss in iteration 62 : 0.3903590639481222
Loss in iteration 63 : 0.3881388715185909
Loss in iteration 64 : 0.3832885462556992
Loss in iteration 65 : 0.3884460261250839
Loss in iteration 66 : 0.3840688458049732
Loss in iteration 67 : 0.38466093712199867
Loss in iteration 68 : 0.3880788426563874
Loss in iteration 69 : 0.3847010837818139
Loss in iteration 70 : 0.38490919701087795
Loss in iteration 71 : 0.38698394356059423
Loss in iteration 72 : 0.38357974076907303
Loss in iteration 73 : 0.3838774339160742
Loss in iteration 74 : 0.38429815605108686
Loss in iteration 75 : 0.38154276047597563
Loss in iteration 76 : 0.38135569967657784
Loss in iteration 77 : 0.38083221727085836
Loss in iteration 78 : 0.3786537351671043
Loss in iteration 79 : 0.3790394771015833
Loss in iteration 80 : 0.37783409931548273
Loss in iteration 81 : 0.37718731226272423
Loss in iteration 82 : 0.3774118768364373
Loss in iteration 83 : 0.3765813755099567
Loss in iteration 84 : 0.37741203761364084
Loss in iteration 85 : 0.3766970267035673
Testing accuracy  of updater 1 on alg 1 with rate 3.4999999999999994E-4 = 0.787, training accuracy 0.838459048235675, time elapsed: 1586 millisecond.
Loss in iteration 1 : 1.0003561591326247
Loss in iteration 2 : 1.0274657643206857
Loss in iteration 3 : 1.366309199824049
Loss in iteration 4 : 1.4029961305250975
Loss in iteration 5 : 1.1679361176565264
Loss in iteration 6 : 0.6997333927369224
Loss in iteration 7 : 0.45255644351537017
Loss in iteration 8 : 0.7213411541280071
Loss in iteration 9 : 0.95030061954324
Loss in iteration 10 : 0.8696774717844633
Loss in iteration 11 : 0.6608755038915373
Loss in iteration 12 : 0.5567069223402793
Loss in iteration 13 : 0.6033450534499493
Loss in iteration 14 : 0.7098588006138374
Loss in iteration 15 : 0.779763733123274
Loss in iteration 16 : 0.7775208642635907
Loss in iteration 17 : 0.718534974362851
Loss in iteration 18 : 0.6492992783740359
Loss in iteration 19 : 0.6149670200055368
Loss in iteration 20 : 0.6273654976958747
Loss in iteration 21 : 0.6675822675995281
Loss in iteration 22 : 0.6955230789981248
Loss in iteration 23 : 0.6883203561447647
Loss in iteration 24 : 0.6519220129578288
Loss in iteration 25 : 0.6117806609680618
Loss in iteration 26 : 0.5913810226342088
Loss in iteration 27 : 0.5949108343689092
Loss in iteration 28 : 0.6077245547859584
Loss in iteration 29 : 0.611517162532626
Loss in iteration 30 : 0.5970777627354257
Loss in iteration 31 : 0.5696683014692268
Loss in iteration 32 : 0.5430125449328865
Loss in iteration 33 : 0.5293791395243111
Loss in iteration 34 : 0.5316209978886354
Loss in iteration 35 : 0.5337752826534055
Loss in iteration 36 : 0.5223668468568612
Loss in iteration 37 : 0.4989816300471372
Loss in iteration 38 : 0.4768818723272953
Loss in iteration 39 : 0.46966072937017783
Loss in iteration 40 : 0.4692846905731231
Loss in iteration 41 : 0.46350872355936285
Loss in iteration 42 : 0.4466229189254664
Loss in iteration 43 : 0.428114535053443
Loss in iteration 44 : 0.4213562541797527
Loss in iteration 45 : 0.42134704640103277
Loss in iteration 46 : 0.4143169277934122
Loss in iteration 47 : 0.39991531822268556
Loss in iteration 48 : 0.3933434872592984
Loss in iteration 49 : 0.3953958788084145
Loss in iteration 50 : 0.3915029071192342
Loss in iteration 51 : 0.383084205427146
Loss in iteration 52 : 0.38631756982606325
Loss in iteration 53 : 0.38996697074012027
Loss in iteration 54 : 0.3834568634952151
Loss in iteration 55 : 0.38999044285856654
Loss in iteration 56 : 0.39161890648559894
Loss in iteration 57 : 0.38775929081214777
Loss in iteration 58 : 0.394700794932908
Loss in iteration 59 : 0.3885553033781861
Loss in iteration 60 : 0.38985905075681226
Loss in iteration 61 : 0.38886141474876396
Loss in iteration 62 : 0.383800712055566
Loss in iteration 63 : 0.3855819924480723
Loss in iteration 64 : 0.38182308978845636
Loss in iteration 65 : 0.3805150264203886
Loss in iteration 66 : 0.3816136040456253
Loss in iteration 67 : 0.37926886177970603
Loss in iteration 68 : 0.3794498207008132
Loss in iteration 69 : 0.3806649235695497
Loss in iteration 70 : 0.379256781521078
Loss in iteration 71 : 0.3793569900809827
Testing accuracy  of updater 1 on alg 1 with rate 2.45E-4 = 0.782, training accuracy 0.8391065069601813, time elapsed: 1306 millisecond.
Loss in iteration 1 : 1.0001162968596324
Loss in iteration 2 : 0.7380496876085274
Loss in iteration 3 : 0.9319044062413931
Loss in iteration 4 : 0.9531900069588005
Loss in iteration 5 : 0.8192190867901884
Loss in iteration 6 : 0.5538488152851738
Loss in iteration 7 : 0.3967320414563552
Loss in iteration 8 : 0.562581013700434
Loss in iteration 9 : 0.6734381507565863
Loss in iteration 10 : 0.6061574316339801
Loss in iteration 11 : 0.48200268326558765
Loss in iteration 12 : 0.43246647101240615
Loss in iteration 13 : 0.47253149593203697
Loss in iteration 14 : 0.5366454439662052
Loss in iteration 15 : 0.5667723914867612
Loss in iteration 16 : 0.5494259987093923
Loss in iteration 17 : 0.5068833106940526
Loss in iteration 18 : 0.47356647592613077
Loss in iteration 19 : 0.46947844923094983
Loss in iteration 20 : 0.49026657555680475
Loss in iteration 21 : 0.5135062227534937
Loss in iteration 22 : 0.5211974109361933
Loss in iteration 23 : 0.50866036492767
Loss in iteration 24 : 0.4862606503131547
Loss in iteration 25 : 0.468954091367458
Loss in iteration 26 : 0.46562506049577534
Loss in iteration 27 : 0.47243936809919296
Loss in iteration 28 : 0.4799334161972742
Loss in iteration 29 : 0.47989038612144636
Loss in iteration 30 : 0.4697069858479436
Loss in iteration 31 : 0.4551730753646521
Loss in iteration 32 : 0.4439428194752328
Loss in iteration 33 : 0.440883600322202
Loss in iteration 34 : 0.4430320167897803
Loss in iteration 35 : 0.4441648576712262
Loss in iteration 36 : 0.43908801831355504
Loss in iteration 37 : 0.4288128577686431
Loss in iteration 38 : 0.41837740249424016
Loss in iteration 39 : 0.4128332007438604
Loss in iteration 40 : 0.41337716396535257
Loss in iteration 41 : 0.4132542566258964
Loss in iteration 42 : 0.4078270044375066
Loss in iteration 43 : 0.3988103571099975
Loss in iteration 44 : 0.39334817646579234
Loss in iteration 45 : 0.39291000969870743
Loss in iteration 46 : 0.3932692022832494
Loss in iteration 47 : 0.3899133074527346
Loss in iteration 48 : 0.38410736108051474
Loss in iteration 49 : 0.3814331495514686
Loss in iteration 50 : 0.3826734454485623
Loss in iteration 51 : 0.3830895992765397
Loss in iteration 52 : 0.38033845553612144
Loss in iteration 53 : 0.37735014572195646
Loss in iteration 54 : 0.3783334467454324
Loss in iteration 55 : 0.38014647026608345
Loss in iteration 56 : 0.3790050776045726
Loss in iteration 57 : 0.3774522669064647
Loss in iteration 58 : 0.3786451414266661
Testing accuracy  of updater 1 on alg 1 with rate 1.4E-4 = 0.7805, training accuracy 0.8387827775979282, time elapsed: 1583 millisecond.
Loss in iteration 1 : 1.0000072685537271
Loss in iteration 2 : 0.6267924622763749
Loss in iteration 3 : 0.5953922875089654
Loss in iteration 4 : 0.6862360118586857
Loss in iteration 5 : 0.7330518114668456
Loss in iteration 6 : 0.7372415155669636
Loss in iteration 7 : 0.7029718738520632
Loss in iteration 8 : 0.6348345071329659
Loss in iteration 9 : 0.5421781922903008
Loss in iteration 10 : 0.45628771822434805
Loss in iteration 11 : 0.41837539842128135
Loss in iteration 12 : 0.454452997373961
Loss in iteration 13 : 0.504900563103014
Loss in iteration 14 : 0.5165744005307884
Loss in iteration 15 : 0.48592833157509696
Loss in iteration 16 : 0.4375218988319828
Loss in iteration 17 : 0.4002551530975767
Loss in iteration 18 : 0.3891301373072481
Loss in iteration 19 : 0.39917031859825364
Loss in iteration 20 : 0.4167364491073014
Loss in iteration 21 : 0.4281974302004802
Loss in iteration 22 : 0.42849001167065726
Loss in iteration 23 : 0.4192855638654792
Loss in iteration 24 : 0.40614911457667885
Loss in iteration 25 : 0.39493033350141127
Loss in iteration 26 : 0.3892275041534195
Loss in iteration 27 : 0.39005828337386483
Loss in iteration 28 : 0.3949666030807804
Testing accuracy  of updater 1 on alg 1 with rate 3.5E-5 = 0.77975, training accuracy 0.8232437682097766, time elapsed: 532 millisecond.
Loss in iteration 1 : 1.0000035615913263
Loss in iteration 2 : 0.7148871204288201
Loss in iteration 3 : 0.5592696099290299
Loss in iteration 4 : 0.6348087378575187
Loss in iteration 5 : 0.692549477512378
Loss in iteration 6 : 0.718523449195424
Loss in iteration 7 : 0.7153649415468658
Loss in iteration 8 : 0.6860212068248651
Loss in iteration 9 : 0.6336332048847174
Loss in iteration 10 : 0.563611204188327
Loss in iteration 11 : 0.49183795432325184
Loss in iteration 12 : 0.4406190507530659
Loss in iteration 13 : 0.426302221822339
Loss in iteration 14 : 0.453129207326406
Loss in iteration 15 : 0.4864036334424506
Loss in iteration 16 : 0.49743398509131753
Loss in iteration 17 : 0.4811217314529115
Loss in iteration 18 : 0.44842130665137
Loss in iteration 19 : 0.41582877867408663
Loss in iteration 20 : 0.3960626387853318
Loss in iteration 21 : 0.39152420134145144
Loss in iteration 22 : 0.398849443414745
Loss in iteration 23 : 0.4098949455873171
Loss in iteration 24 : 0.41720840395018044
Testing accuracy  of updater 1 on alg 1 with rate 2.45E-5 = 0.749, training accuracy 0.8164454516024603, time elapsed: 607 millisecond.
Loss in iteration 1 : 1.0000011629685963
Loss in iteration 2 : 0.833871409567875
Loss in iteration 3 : 0.5944582165872848
Loss in iteration 4 : 0.5625754282275287
Loss in iteration 5 : 0.613620866470885
Loss in iteration 6 : 0.656380680141533
Loss in iteration 7 : 0.6805211736297282
Loss in iteration 8 : 0.6873293238537165
Loss in iteration 9 : 0.6784143230448471
Loss in iteration 10 : 0.6554218962228916
Loss in iteration 11 : 0.6200886544959351
Loss in iteration 12 : 0.5753233100110825
Loss in iteration 13 : 0.5276015345842933
Loss in iteration 14 : 0.4866648551969451
Loss in iteration 15 : 0.4594880585684543
Loss in iteration 16 : 0.4491631731692776
Loss in iteration 17 : 0.451938045822973
Loss in iteration 18 : 0.46463399201834177
Loss in iteration 19 : 0.4773499084736685
Loss in iteration 20 : 0.4806498503600749
Testing accuracy  of updater 1 on alg 1 with rate 1.3999999999999998E-5 = 0.78425, training accuracy 0.7944318549692457, time elapsed: 349 millisecond.
Loss in iteration 1 : 1.0000000726855374
Loss in iteration 2 : 0.9584660185579961
Testing accuracy  of updater 1 on alg 1 with rate 3.499999999999997E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 67 millisecond.
Loss in iteration 1 : 1.0465187438530044
Loss in iteration 2 : 15.117044644886738
Loss in iteration 3 : 15.588252950828247
Loss in iteration 4 : 12.92643871818493
Loss in iteration 5 : 7.524111546163741
Loss in iteration 6 : 3.359539183621366
Loss in iteration 7 : 3.7693482316924065
Loss in iteration 8 : 5.007855246802903
Loss in iteration 9 : 5.192146966523648
Loss in iteration 10 : 4.800509017639741
Loss in iteration 11 : 4.527781733559585
Loss in iteration 12 : 4.530113270579794
Loss in iteration 13 : 4.6253742127131625
Loss in iteration 14 : 4.724997205771397
Loss in iteration 15 : 4.744202028296501
Loss in iteration 16 : 4.685710323608559
Loss in iteration 17 : 4.592205633265024
Loss in iteration 18 : 4.493058070472678
Loss in iteration 19 : 4.385245706225373
Loss in iteration 20 : 4.265497392947797
Loss in iteration 21 : 4.128172181361823
Loss in iteration 22 : 3.973580414849008
Loss in iteration 23 : 3.8040193897267343
Loss in iteration 24 : 3.623983868914716
Loss in iteration 25 : 3.4363879094457856
Loss in iteration 26 : 3.245542237238543
Loss in iteration 27 : 3.0545247728492315
Loss in iteration 28 : 2.8600581283794324
Loss in iteration 29 : 2.6624468382807716
Loss in iteration 30 : 2.4650863609561102
Loss in iteration 31 : 2.272263325749251
Loss in iteration 32 : 2.0816777607670587
Loss in iteration 33 : 1.8942174605442825
Loss in iteration 34 : 1.7118402887389166
Loss in iteration 35 : 1.5381753085595473
Loss in iteration 36 : 1.3745865369445505
Loss in iteration 37 : 1.2223871529295147
Loss in iteration 38 : 1.083877335464951
Loss in iteration 39 : 0.9668435580353987
Loss in iteration 40 : 0.882698256346879
Loss in iteration 41 : 0.8528740820805509
Loss in iteration 42 : 0.9251973972336114
Loss in iteration 43 : 3.217621525743148
Loss in iteration 44 : 12.772232335387345
Loss in iteration 45 : 12.538257629332323
Loss in iteration 46 : 9.238338643573922
Loss in iteration 47 : 3.388331037095333
Loss in iteration 48 : 2.2549319531040672
Loss in iteration 49 : 2.9903575634450843
Loss in iteration 50 : 2.276186439763375
Loss in iteration 51 : 2.107856581966308
Loss in iteration 52 : 2.1768401651703706
Loss in iteration 53 : 2.231803125720697
Loss in iteration 54 : 2.2357877538727413
Loss in iteration 55 : 2.208425511975761
Loss in iteration 56 : 2.165305196617972
Loss in iteration 57 : 2.104904358101117
Loss in iteration 58 : 2.025484500153754
Loss in iteration 59 : 1.9288033404403435
Loss in iteration 60 : 1.8189474847948812
Loss in iteration 61 : 1.6999680945632454
Loss in iteration 62 : 1.5725622420068042
Loss in iteration 63 : 1.438645971273302
Loss in iteration 64 : 1.3006541332005679
Loss in iteration 65 : 1.1608891434870254
Loss in iteration 66 : 1.0199836985628723
Loss in iteration 67 : 0.8842696896782173
Loss in iteration 68 : 0.7581379232162814
Loss in iteration 69 : 0.6456897349865209
Loss in iteration 70 : 0.5777738028949577
Loss in iteration 71 : 2.101095380018197
Loss in iteration 72 : 12.807619367944353
Loss in iteration 73 : 12.611412773204899
Loss in iteration 74 : 9.342455275406866
Loss in iteration 75 : 3.3828839250077167
Loss in iteration 76 : 2.862284912224473
Loss in iteration 77 : 2.2177898036771575
Loss in iteration 78 : 1.7648929907777386
Loss in iteration 79 : 1.7906692337920116
Loss in iteration 80 : 1.8481397678390743
Loss in iteration 81 : 1.8726147133798607
Loss in iteration 82 : 1.8646138122234606
Loss in iteration 83 : 1.8347213010528869
Loss in iteration 84 : 1.7838750106101766
Loss in iteration 85 : 1.7147164494232099
Loss in iteration 86 : 1.62942422570473
Loss in iteration 87 : 1.5309850213084686
Loss in iteration 88 : 1.4234319969429017
Loss in iteration 89 : 1.3071564996109306
Loss in iteration 90 : 1.1849979954669227
Loss in iteration 91 : 1.059419808960001
Loss in iteration 92 : 0.9317697683082804
Loss in iteration 93 : 0.8084980518315689
Loss in iteration 94 : 0.6941560914910315
Loss in iteration 95 : 0.6004623617410528
Loss in iteration 96 : 0.769343113538151
Loss in iteration 97 : 4.5763818464769015
Loss in iteration 98 : 0.8648191543180084
Loss in iteration 99 : 15.689256932939681
Loss in iteration 100 : 13.041849108722124
Loss in iteration 101 : 13.5602365069134
Loss in iteration 102 : 10.943233053771026
Loss in iteration 103 : 5.793295452415656
Loss in iteration 104 : 2.9845447078533267
Loss in iteration 105 : 3.6865373868882414
Loss in iteration 106 : 4.603840245547946
Loss in iteration 107 : 4.5363259605197515
Loss in iteration 108 : 4.217407187399305
Loss in iteration 109 : 4.086311365416446
Loss in iteration 110 : 4.1342980789745845
Loss in iteration 111 : 4.227244093510877
Loss in iteration 112 : 4.278603658476765
Loss in iteration 113 : 4.254755023606291
Loss in iteration 114 : 4.183224997680305
Loss in iteration 115 : 4.094940277858246
Loss in iteration 116 : 3.9974092401419776
Loss in iteration 117 : 3.8900686850491257
Loss in iteration 118 : 3.7656863890659533
Loss in iteration 119 : 3.6244929122482863
Loss in iteration 120 : 3.4690380160177927
Loss in iteration 121 : 3.302433660713138
Loss in iteration 122 : 3.1302726072406903
Loss in iteration 123 : 2.9540229630084864
Loss in iteration 124 : 2.774058110378422
Loss in iteration 125 : 2.5909142285353086
Loss in iteration 126 : 2.4062732246416494
Loss in iteration 127 : 2.223688375881817
Loss in iteration 128 : 2.0420522230316047
Loss in iteration 129 : 1.8622882791059847
Loss in iteration 130 : 1.687648010451181
Loss in iteration 131 : 1.520772451451036
Loss in iteration 132 : 1.3624549672796442
Loss in iteration 133 : 1.2146194337783274
Loss in iteration 134 : 1.0795863441340323
Loss in iteration 135 : 0.9649927128253538
Loss in iteration 136 : 0.8816579549357316
Loss in iteration 137 : 0.8507507207415221
Loss in iteration 138 : 1.1861221882674307
Loss in iteration 139 : 6.073469517053542
Loss in iteration 140 : 2.908951112146195
Loss in iteration 141 : 3.940091705181669
Loss in iteration 142 : 7.216028612811145
Loss in iteration 143 : 4.5351927120879525
Loss in iteration 144 : 1.5282343767335065
Loss in iteration 145 : 2.1016471368530767
Loss in iteration 146 : 2.017275063457418
Loss in iteration 147 : 1.940407840685753
Loss in iteration 148 : 1.9944622827089304
Loss in iteration 149 : 2.0440323997292382
Loss in iteration 150 : 2.0581439696909003
Loss in iteration 151 : 2.041546604977465
Loss in iteration 152 : 2.0038493195051803
Loss in iteration 153 : 1.9461307620043649
Loss in iteration 154 : 1.8701566268712992
Loss in iteration 155 : 1.7779105611702068
Loss in iteration 156 : 1.6723618630228385
Loss in iteration 157 : 1.5574401880622275
Loss in iteration 158 : 1.4341592247685055
Loss in iteration 159 : 1.304167640213922
Loss in iteration 160 : 1.1706281675782901
Loss in iteration 161 : 1.034848856083436
Loss in iteration 162 : 0.899466499834484
Loss in iteration 163 : 0.7714214314998877
Loss in iteration 164 : 0.65453059690718
Loss in iteration 165 : 0.5796730778351867
Loss in iteration 166 : 1.8969518997942838
Loss in iteration 167 : 11.917083327529049
Loss in iteration 168 : 11.369360483126622
Loss in iteration 169 : 7.788779517254618
Loss in iteration 170 : 1.8786663044859542
Loss in iteration 171 : 3.4737757255120836
Loss in iteration 172 : 1.4366362937060742
Loss in iteration 173 : 1.480881079767838
Loss in iteration 174 : 1.532893384282926
Loss in iteration 175 : 1.5618438485039052
Loss in iteration 176 : 1.5634869864416394
Loss in iteration 177 : 1.5406491534547393
Loss in iteration 178 : 1.4966921438895953
Loss in iteration 179 : 1.4347315275156298
Loss in iteration 180 : 1.356790243227385
Loss in iteration 181 : 1.2671750732608988
Loss in iteration 182 : 1.1670789684773721
Loss in iteration 183 : 1.0589863727957287
Loss in iteration 184 : 0.9460327157842467
Loss in iteration 185 : 0.8298078064905698
Loss in iteration 186 : 0.7173113705635514
Loss in iteration 187 : 0.615608067126736
Loss in iteration 188 : 0.569234988944882
Loss in iteration 189 : 2.7623532013345713
Loss in iteration 190 : 14.11326063038241
Loss in iteration 191 : 14.561974059679068
Loss in iteration 192 : 11.866263953510275
Loss in iteration 193 : 6.369263928844299
Loss in iteration 194 : 1.6866959523680314
Loss in iteration 195 : 3.6081067641034683
Loss in iteration 196 : 2.7860255023742857
Loss in iteration 197 : 2.243175310524691
Loss in iteration 198 : 2.237219047910845
Loss in iteration 199 : 2.311015956003632
Loss in iteration 200 : 2.3304706994162876
Testing accuracy  of updater 2 on alg 1 with rate 0.0027999999999999995 = 0.779, training accuracy 0.8303658141793461, time elapsed: 3557 millisecond.
Loss in iteration 1 : 1.0227941844879722
Loss in iteration 2 : 10.660286516527314
Loss in iteration 3 : 10.97885360245524
Loss in iteration 4 : 9.11007356018159
Loss in iteration 5 : 5.317130220616847
Loss in iteration 6 : 2.3286580553087566
Loss in iteration 7 : 2.596221146764882
Loss in iteration 8 : 3.4375063071680514
Loss in iteration 9 : 3.5422201052533824
Loss in iteration 10 : 3.2622957784545705
Loss in iteration 11 : 3.067772346280829
Loss in iteration 12 : 3.063920912825063
Loss in iteration 13 : 3.128404919015965
Loss in iteration 14 : 3.197081734045473
Loss in iteration 15 : 3.2116368612329818
Loss in iteration 16 : 3.1746650451859053
Loss in iteration 17 : 3.1138726858461303
Loss in iteration 18 : 3.052015158946787
Loss in iteration 19 : 2.985098140568384
Loss in iteration 20 : 2.9101274736605127
Loss in iteration 21 : 2.824747082796261
Loss in iteration 22 : 2.7277186109765235
Loss in iteration 23 : 2.6204073894827924
Loss in iteration 24 : 2.5057872478804515
Loss in iteration 25 : 2.387169126334891
Loss in iteration 26 : 2.2657723921980577
Loss in iteration 27 : 2.1435428570491895
Loss in iteration 28 : 2.018407971501024
Loss in iteration 29 : 1.8914772706559444
Loss in iteration 30 : 1.7637831561607171
Loss in iteration 31 : 1.6381455823131696
Loss in iteration 32 : 1.5133025080799554
Loss in iteration 33 : 1.3893370143516746
Loss in iteration 34 : 1.2680175426498608
Loss in iteration 35 : 1.1518401676174295
Loss in iteration 36 : 1.042535430126986
Loss in iteration 37 : 0.9401446371888587
Loss in iteration 38 : 0.844416947836967
Loss in iteration 39 : 0.7611635040515506
Loss in iteration 40 : 0.6991592194302689
Loss in iteration 41 : 0.6703771058225501
Loss in iteration 42 : 0.9420185211513438
Loss in iteration 43 : 4.2406571281394445
Loss in iteration 44 : 2.02867202729308
Loss in iteration 45 : 3.5151716764864296
Loss in iteration 46 : 6.295214606408574
Loss in iteration 47 : 4.949544061572843
Loss in iteration 48 : 2.0343823793146947
Loss in iteration 49 : 1.3776423096712893
Loss in iteration 50 : 1.8617558547883795
Loss in iteration 51 : 1.7735665976086967
Loss in iteration 52 : 1.679193809564787
Loss in iteration 53 : 1.7041918276066859
Loss in iteration 54 : 1.742812750001326
Loss in iteration 55 : 1.7591424343942128
Loss in iteration 56 : 1.7503399193509572
Loss in iteration 57 : 1.7239061358286376
Loss in iteration 58 : 1.6861914840409422
Loss in iteration 59 : 1.6366633473084362
Loss in iteration 60 : 1.5763521141573413
Loss in iteration 61 : 1.5053995103136166
Loss in iteration 62 : 1.4279726311873218
Loss in iteration 63 : 1.3441802572520762
Loss in iteration 64 : 1.25501935434397
Loss in iteration 65 : 1.1617574359510494
Loss in iteration 66 : 1.066316955972228
Loss in iteration 67 : 0.9689457369635318
Loss in iteration 68 : 0.8711454804938116
Loss in iteration 69 : 0.776498594704025
Loss in iteration 70 : 0.6870624603343267
Loss in iteration 71 : 0.6054466152119524
Loss in iteration 72 : 0.5472972275413778
Loss in iteration 73 : 0.6859268496963687
Loss in iteration 74 : 5.231276092980719
Loss in iteration 75 : 10.719182811964235
Loss in iteration 76 : 11.559432511986197
Loss in iteration 77 : 10.152106607205235
Loss in iteration 78 : 6.732985835707437
Loss in iteration 79 : 2.4318851104913146
Loss in iteration 80 : 2.0017151591366935
Loss in iteration 81 : 3.0213949066908343
Loss in iteration 82 : 2.8911916262389106
Loss in iteration 83 : 2.4392014091443595
Loss in iteration 84 : 2.2704863159472386
Loss in iteration 85 : 2.3065171312229555
Loss in iteration 86 : 2.372231692228177
Loss in iteration 87 : 2.384354986832894
Loss in iteration 88 : 2.346098081533727
Loss in iteration 89 : 2.2910763980164313
Loss in iteration 90 : 2.2376884121924236
Loss in iteration 91 : 2.180723716249308
Loss in iteration 92 : 2.114129809085573
Loss in iteration 93 : 2.0323004923547527
Loss in iteration 94 : 1.94251163657403
Loss in iteration 95 : 1.8488702066949423
Loss in iteration 96 : 1.750551516995605
Loss in iteration 97 : 1.6475144794183347
Loss in iteration 98 : 1.540905690361833
Loss in iteration 99 : 1.433366806919558
Loss in iteration 100 : 1.3245498585298896
Loss in iteration 101 : 1.2148473135230466
Loss in iteration 102 : 1.1060685579208234
Loss in iteration 103 : 1.0008876186033961
Loss in iteration 104 : 0.9011564607440402
Loss in iteration 105 : 0.807185434224554
Loss in iteration 106 : 0.7207561439690828
Loss in iteration 107 : 0.6477787099943296
Loss in iteration 108 : 0.5985278074736411
Loss in iteration 109 : 0.6022264144300228
Loss in iteration 110 : 1.2229795163200854
Loss in iteration 111 : 7.086747305631918
Loss in iteration 112 : 10.223081937728267
Loss in iteration 113 : 10.84910649147347
Loss in iteration 114 : 9.25266063057394
Loss in iteration 115 : 5.67098738498646
Loss in iteration 116 : 2.125737815105726
Loss in iteration 117 : 2.251048994234668
Loss in iteration 118 : 3.0915302037886314
Loss in iteration 119 : 2.9873151076965083
Loss in iteration 120 : 2.6418751617686067
Loss in iteration 121 : 2.5013360484016136
Loss in iteration 122 : 2.5368923857717527
Loss in iteration 123 : 2.606962725841347
Loss in iteration 124 : 2.633807865374618
Loss in iteration 125 : 2.6114029303597164
Loss in iteration 126 : 2.5609267494784165
Loss in iteration 127 : 2.5065867124248316
Loss in iteration 128 : 2.4506408896157774
Loss in iteration 129 : 2.3862927882963407
Loss in iteration 130 : 2.3087163851608823
Loss in iteration 131 : 2.2182917772749153
Loss in iteration 132 : 2.1210238012606557
Loss in iteration 133 : 2.0198209883259497
Loss in iteration 134 : 1.9151210991956973
Loss in iteration 135 : 1.8059725908362443
Loss in iteration 136 : 1.6930979105233637
Loss in iteration 137 : 1.5786285733436913
Loss in iteration 138 : 1.4643289512892643
Loss in iteration 139 : 1.3495276311206459
Loss in iteration 140 : 1.234949811722987
Loss in iteration 141 : 1.1227375004676903
Loss in iteration 142 : 1.0153753632814393
Loss in iteration 143 : 0.9137145074782245
Loss in iteration 144 : 0.8185370694198515
Loss in iteration 145 : 0.7319165898584771
Loss in iteration 146 : 0.6608508763079093
Loss in iteration 147 : 0.6148105614967236
Loss in iteration 148 : 0.6182915130075651
Loss in iteration 149 : 1.0560475695908822
Loss in iteration 150 : 8.006151236296775
Loss in iteration 151 : 10.032098161763653
Loss in iteration 152 : 10.65059859575105
Loss in iteration 153 : 9.048020896303763
Loss in iteration 154 : 5.464136450988731
Loss in iteration 155 : 2.092714241032123
Loss in iteration 156 : 2.2849587189986575
Loss in iteration 157 : 3.0911008132888127
Loss in iteration 158 : 2.985612667545228
Loss in iteration 159 : 2.6764021054881635
Loss in iteration 160 : 2.5304294783613726
Loss in iteration 161 : 2.5604774412059323
Loss in iteration 162 : 2.6266769564328056
Loss in iteration 163 : 2.656021925840035
Loss in iteration 164 : 2.636561752918448
Loss in iteration 165 : 2.5880044453751445
Loss in iteration 166 : 2.534379089391357
Loss in iteration 167 : 2.478450998594846
Loss in iteration 168 : 2.413875662104243
Loss in iteration 169 : 2.336091617210849
Loss in iteration 170 : 2.2463925124131703
Loss in iteration 171 : 2.149269610776479
Loss in iteration 172 : 2.0479179130722445
Loss in iteration 173 : 1.9429354848562368
Loss in iteration 174 : 1.8335072967626478
Loss in iteration 175 : 1.7204700670146253
Loss in iteration 176 : 1.6056216691091325
Loss in iteration 177 : 1.4909353872554343
Loss in iteration 178 : 1.3756067027631116
Loss in iteration 179 : 1.2604089176344728
Loss in iteration 180 : 1.1473057230585866
Loss in iteration 181 : 1.0387838928189137
Loss in iteration 182 : 0.9360068679346749
Loss in iteration 183 : 0.8395614516560483
Loss in iteration 184 : 0.7509766922724543
Loss in iteration 185 : 0.676096876058337
Loss in iteration 186 : 0.6249501086513324
Loss in iteration 187 : 0.6107965678046541
Loss in iteration 188 : 0.6672305035528464
Loss in iteration 189 : 2.07583985873853
Loss in iteration 190 : 8.977339308587421
Loss in iteration 191 : 8.764943274074762
Loss in iteration 192 : 6.417025209694129
Loss in iteration 193 : 2.305722439194623
Loss in iteration 194 : 1.7049240960617436
Loss in iteration 195 : 2.120233345184126
Loss in iteration 196 : 1.6061108345843236
Loss in iteration 197 : 1.505991881712202
Loss in iteration 198 : 1.560245847867757
Loss in iteration 199 : 1.5995864000437219
Loss in iteration 200 : 1.6027742653494155
Testing accuracy  of updater 2 on alg 1 with rate 0.00196 = 0.77875, training accuracy 0.8313370022661055, time elapsed: 3573 millisecond.
Loss in iteration 1 : 1.0074429990164808
Loss in iteration 2 : 6.226988968860006
Loss in iteration 3 : 6.402510091838371
Loss in iteration 4 : 5.331392538601967
Loss in iteration 5 : 3.1545665230380187
Loss in iteration 6 : 1.3588385776746073
Loss in iteration 7 : 1.531721169243135
Loss in iteration 8 : 1.9942247083394713
Loss in iteration 9 : 2.018715491706383
Loss in iteration 10 : 1.8523081824069814
Loss in iteration 11 : 1.7431497910508327
Loss in iteration 12 : 1.7418408250423123
Loss in iteration 13 : 1.7768873894471158
Loss in iteration 14 : 1.8132271546415086
Loss in iteration 15 : 1.821146396566957
Loss in iteration 16 : 1.802593720720477
Loss in iteration 17 : 1.7729368392699751
Loss in iteration 18 : 1.7427709303614267
Loss in iteration 19 : 1.7101910935623683
Loss in iteration 20 : 1.6732022502357846
Loss in iteration 21 : 1.6306092845572442
Loss in iteration 22 : 1.581291278042909
Loss in iteration 23 : 1.5263573252795197
Loss in iteration 24 : 1.468378847161542
Loss in iteration 25 : 1.4087602029424746
Loss in iteration 26 : 1.3467250435325064
Loss in iteration 27 : 1.283417922355692
Loss in iteration 28 : 1.2188586362113765
Loss in iteration 29 : 1.1535047181038616
Loss in iteration 30 : 1.0874064947597148
Loss in iteration 31 : 1.021386743668397
Loss in iteration 32 : 0.9557664333226932
Loss in iteration 33 : 0.8901541227817946
Loss in iteration 34 : 0.8260561142828511
Loss in iteration 35 : 0.7645897617100026
Loss in iteration 36 : 0.7061857031730419
Loss in iteration 37 : 0.6504695374905853
Loss in iteration 38 : 0.5995697428771588
Loss in iteration 39 : 0.5543627587831361
Loss in iteration 40 : 0.5173170837409066
Loss in iteration 41 : 0.4937356429191059
Loss in iteration 42 : 0.4891290667239651
Loss in iteration 43 : 0.534196777132419
Loss in iteration 44 : 2.053121775079306
Loss in iteration 45 : 5.286853896917523
Loss in iteration 46 : 5.257579491196324
Loss in iteration 47 : 4.001487299375733
Loss in iteration 48 : 1.6976069652439578
Loss in iteration 49 : 0.9550344825906375
Loss in iteration 50 : 1.4350607319728834
Loss in iteration 51 : 1.1212820266061951
Loss in iteration 52 : 1.0059507237407221
Loss in iteration 53 : 1.0229993182462345
Loss in iteration 54 : 1.0518917487453245
Loss in iteration 55 : 1.0629434957794255
Loss in iteration 56 : 1.0580803609420892
Loss in iteration 57 : 1.0467918280069926
Loss in iteration 58 : 1.0306724553481394
Loss in iteration 59 : 1.0090480746323245
Loss in iteration 60 : 0.9808665904459393
Loss in iteration 61 : 0.9469923303688583
Loss in iteration 62 : 0.9086786487023233
Loss in iteration 63 : 0.8668251596137773
Loss in iteration 64 : 0.8217887435514124
Loss in iteration 65 : 0.7743080831554444
Loss in iteration 66 : 0.7251513694904451
Loss in iteration 67 : 0.6748607244842268
Loss in iteration 68 : 0.6250475557094544
Loss in iteration 69 : 0.5764731824155324
Loss in iteration 70 : 0.5296982609172142
Loss in iteration 71 : 0.4870124742551972
Loss in iteration 72 : 0.451034401555445
Loss in iteration 73 : 0.42579531019023764
Loss in iteration 74 : 0.42469412593503597
Loss in iteration 75 : 0.90744666532812
Loss in iteration 76 : 4.365455244433186
Loss in iteration 77 : 3.882699727177026
Loss in iteration 78 : 2.2205213651854856
Loss in iteration 79 : 0.5991255817241009
Loss in iteration 80 : 0.9406390162772675
Loss in iteration 81 : 0.6251740477263644
Loss in iteration 82 : 0.6460131958354489
Loss in iteration 83 : 0.6575341328258046
Loss in iteration 84 : 0.6604000032651136
Loss in iteration 85 : 0.6556158811207913
Loss in iteration 86 : 0.6439219500359441
Loss in iteration 87 : 0.6261645085018086
Loss in iteration 88 : 0.6032640461682132
Loss in iteration 89 : 0.575868109383861
Loss in iteration 90 : 0.5451431381057098
Loss in iteration 91 : 0.5124530689467567
Loss in iteration 92 : 0.479391383010662
Loss in iteration 93 : 0.4478301080915227
Loss in iteration 94 : 0.4209031593425493
Loss in iteration 95 : 0.41471742021970415
Loss in iteration 96 : 0.7519746635377272
Loss in iteration 97 : 2.934134230868417
Loss in iteration 98 : 1.8823544470556894
Loss in iteration 99 : 0.5457710424459713
Loss in iteration 100 : 0.5552993447354411
Loss in iteration 101 : 1.2045062517029868
Loss in iteration 102 : 1.8013446299144897
Loss in iteration 103 : 0.6341399235524146
Loss in iteration 104 : 0.7134457535845391
Loss in iteration 105 : 0.5889429911114947
Loss in iteration 106 : 0.5969098684176369
Loss in iteration 107 : 0.5992971431304392
Loss in iteration 108 : 0.5940036910332848
Loss in iteration 109 : 0.5822652492650034
Loss in iteration 110 : 0.564936104244827
Loss in iteration 111 : 0.542967978481665
Loss in iteration 112 : 0.5174610537862655
Loss in iteration 113 : 0.4897134645876061
Loss in iteration 114 : 0.461672607204313
Loss in iteration 115 : 0.43505614202236925
Loss in iteration 116 : 0.4157506410955421
Loss in iteration 117 : 0.45130598100923014
Loss in iteration 118 : 1.6572235555583028
Loss in iteration 119 : 4.8407792195175
Loss in iteration 120 : 4.68394411288427
Loss in iteration 121 : 3.3136353240030236
Loss in iteration 122 : 1.070950221119864
Loss in iteration 123 : 1.1344113463952725
Loss in iteration 124 : 1.114337223720301
Loss in iteration 125 : 0.8971565348218964
Loss in iteration 126 : 0.8789867960093134
Loss in iteration 127 : 0.9003818921291677
Loss in iteration 128 : 0.9082753302912918
Loss in iteration 129 : 0.9012265445601265
Loss in iteration 130 : 0.8877423727888705
Loss in iteration 131 : 0.8698605239343611
Loss in iteration 132 : 0.8453858843143905
Loss in iteration 133 : 0.8156672573391829
Loss in iteration 134 : 0.781114798242225
Loss in iteration 135 : 0.7428797675176672
Loss in iteration 136 : 0.7017036557751
Loss in iteration 137 : 0.6581680163264597
Loss in iteration 138 : 0.6132766721959235
Loss in iteration 139 : 0.5692023675001799
Loss in iteration 140 : 0.5259062467319945
Loss in iteration 141 : 0.48492534075710136
Loss in iteration 142 : 0.4496609593487184
Loss in iteration 143 : 0.4237514906697398
Loss in iteration 144 : 0.448451720851798
Loss in iteration 145 : 1.8130270119614496
Loss in iteration 146 : 5.369323899728923
Loss in iteration 147 : 5.412542688835036
Loss in iteration 148 : 4.221200892786404
Loss in iteration 149 : 1.9395630364682388
Loss in iteration 150 : 0.920272099924222
Loss in iteration 151 : 1.4745304977528753
Loss in iteration 152 : 1.13755098630952
Loss in iteration 153 : 0.9997143732355313
Loss in iteration 154 : 1.0110875909917116
Loss in iteration 155 : 1.0383746110579135
Loss in iteration 156 : 1.0478378406944564
Loss in iteration 157 : 1.039898162089541
Loss in iteration 158 : 1.0255372521534265
Loss in iteration 159 : 1.0072938077144988
Loss in iteration 160 : 0.9836654957483087
Loss in iteration 161 : 0.9533708454542267
Loss in iteration 162 : 0.9180890471773948
Loss in iteration 163 : 0.8786713269593457
Loss in iteration 164 : 0.835915941683956
Loss in iteration 165 : 0.790297918684042
Loss in iteration 166 : 0.7425677609268179
Loss in iteration 167 : 0.6934079953139246
Loss in iteration 168 : 0.6437494541680964
Loss in iteration 169 : 0.5953353754939674
Loss in iteration 170 : 0.5484616740338
Loss in iteration 171 : 0.5045042574882388
Loss in iteration 172 : 0.46630179009054196
Loss in iteration 173 : 0.43636185126877725
Loss in iteration 174 : 0.4263401351013288
Loss in iteration 175 : 0.6421986138262118
Loss in iteration 176 : 2.968118983026251
Loss in iteration 177 : 1.880824241585484
Loss in iteration 178 : 0.6227210863437002
Loss in iteration 179 : 0.85668804237996
Loss in iteration 180 : 1.5162809736383265
Loss in iteration 181 : 1.7901593041111203
Loss in iteration 182 : 0.713391654060936
Loss in iteration 183 : 0.7042034227841069
Loss in iteration 184 : 0.6878819619601606
Loss in iteration 185 : 0.6936121055034455
Loss in iteration 186 : 0.7058763460925007
Loss in iteration 187 : 0.7095654109589709
Loss in iteration 188 : 0.704919287056278
Loss in iteration 189 : 0.6931200446073228
Loss in iteration 190 : 0.6751348439711374
Loss in iteration 191 : 0.6516276355379588
Loss in iteration 192 : 0.6234343129212572
Loss in iteration 193 : 0.5914262856440164
Loss in iteration 194 : 0.556873370853134
Loss in iteration 195 : 0.5213451798121237
Loss in iteration 196 : 0.485905411620344
Loss in iteration 197 : 0.45185687816742826
Loss in iteration 198 : 0.4232235405929109
Loss in iteration 199 : 0.4109915153348014
Loss in iteration 200 : 0.5986508849292641
Testing accuracy  of updater 2 on alg 1 with rate 0.00112 = 0.67875, training accuracy 0.5998705082550987, time elapsed: 3964 millisecond.
Loss in iteration 1 : 1.00046518743853
Loss in iteration 2 : 1.8172222732493528
Loss in iteration 3 : 1.85945513469777
Loss in iteration 4 : 1.5908411169472731
Loss in iteration 5 : 1.043195095764895
Loss in iteration 6 : 0.49580838594249177
Loss in iteration 7 : 0.5685877788786619
Loss in iteration 8 : 0.6694339579708959
Loss in iteration 9 : 0.6484247542779106
Loss in iteration 10 : 0.597326370385407
Loss in iteration 11 : 0.5730425734663143
Loss in iteration 12 : 0.5782744623361621
Loss in iteration 13 : 0.590539843791647
Loss in iteration 14 : 0.598846017400579
Loss in iteration 15 : 0.5996451365605481
Loss in iteration 16 : 0.5957578780512206
Loss in iteration 17 : 0.5904922319926739
Loss in iteration 18 : 0.5854513604636569
Loss in iteration 19 : 0.5803051390410591
Loss in iteration 20 : 0.5741952537218132
Loss in iteration 21 : 0.5663725499598941
Loss in iteration 22 : 0.5569801531215073
Loss in iteration 23 : 0.5469545176546154
Loss in iteration 24 : 0.5366574977891327
Loss in iteration 25 : 0.5259419200456893
Loss in iteration 26 : 0.514763567395304
Loss in iteration 27 : 0.5032691721043712
Loss in iteration 28 : 0.49166465053975306
Loss in iteration 29 : 0.48007474781872717
Loss in iteration 30 : 0.46844903580787073
Loss in iteration 31 : 0.4569043921154577
Loss in iteration 32 : 0.4454802158076021
Loss in iteration 33 : 0.4345813055667333
Loss in iteration 34 : 0.4243708696050817
Loss in iteration 35 : 0.4150262072144504
Loss in iteration 36 : 0.4064482931342373
Loss in iteration 37 : 0.39877072274126707
Loss in iteration 38 : 0.39240800664438436
Loss in iteration 39 : 0.38749453014511204
Loss in iteration 40 : 0.38394009062145984
Loss in iteration 41 : 0.38170882097286946
Loss in iteration 42 : 0.38128320945971206
Loss in iteration 43 : 0.38185525852349844
Loss in iteration 44 : 0.383378173412573
Loss in iteration 45 : 0.38535538100790784
Loss in iteration 46 : 0.38706662831658634
Loss in iteration 47 : 0.3879822816270664
Testing accuracy  of updater 2 on alg 1 with rate 2.8E-4 = 0.78875, training accuracy 0.8429912593072192, time elapsed: 903 millisecond.
Loss in iteration 1 : 1.0002279418448796
Loss in iteration 2 : 1.3775425081143882
Loss in iteration 3 : 1.4069894926721611
Loss in iteration 4 : 1.2189003347910028
Loss in iteration 5 : 0.8358984083296395
Loss in iteration 6 : 0.4316659837221085
Loss in iteration 7 : 0.48808108283502044
Loss in iteration 8 : 0.5479917662140156
Loss in iteration 9 : 0.5255809879064924
Loss in iteration 10 : 0.4907737797113298
Loss in iteration 11 : 0.47716234988992834
Loss in iteration 12 : 0.481628031625978
Loss in iteration 13 : 0.4906373867387333
Loss in iteration 14 : 0.49573797483662274
Loss in iteration 15 : 0.4963446364705091
Loss in iteration 16 : 0.4944242856414457
Loss in iteration 17 : 0.49203275272345304
Loss in iteration 18 : 0.4897716501346401
Loss in iteration 19 : 0.48721754906440196
Loss in iteration 20 : 0.48405085033544265
Loss in iteration 21 : 0.47977692783390413
Loss in iteration 22 : 0.47474575573656574
Loss in iteration 23 : 0.46930356542040863
Loss in iteration 24 : 0.4636335657905171
Loss in iteration 25 : 0.45768302419708173
Loss in iteration 26 : 0.4514057067861472
Loss in iteration 27 : 0.4449370785331391
Loss in iteration 28 : 0.43835323089016276
Loss in iteration 29 : 0.43166659988753253
Loss in iteration 30 : 0.4249190366299209
Loss in iteration 31 : 0.41824659077826937
Loss in iteration 32 : 0.4117778281964549
Loss in iteration 33 : 0.40566417751977174
Loss in iteration 34 : 0.4001791361088196
Loss in iteration 35 : 0.3952057724816117
Loss in iteration 36 : 0.39074449583442716
Loss in iteration 37 : 0.3869124886510471
Loss in iteration 38 : 0.3837425676485514
Loss in iteration 39 : 0.38132074694655244
Loss in iteration 40 : 0.37952603536291296
Loss in iteration 41 : 0.3783187574260459
Loss in iteration 42 : 0.3776137622637162
Loss in iteration 43 : 0.3775822881600456
Loss in iteration 44 : 0.377924822064713
Loss in iteration 45 : 0.3785207529946802
Loss in iteration 46 : 0.37924571504860655
Loss in iteration 47 : 0.37992728183853663
Testing accuracy  of updater 2 on alg 1 with rate 1.96E-4 = 0.7895, training accuracy 0.838459048235675, time elapsed: 795 millisecond.
Loss in iteration 1 : 1.0000744299901647
Loss in iteration 2 : 0.9380988252453926
Loss in iteration 3 : 0.9548593045725021
Loss in iteration 4 : 0.847345778104337
Loss in iteration 5 : 0.6303436421697947
Loss in iteration 6 : 0.4002146935790756
Loss in iteration 7 : 0.4349177958767821
Loss in iteration 8 : 0.4547209391118445
Loss in iteration 9 : 0.4277959054476919
Loss in iteration 10 : 0.40592228617732234
Loss in iteration 11 : 0.40128213151201336
Loss in iteration 12 : 0.4052256997572177
Loss in iteration 13 : 0.4097917608436089
Loss in iteration 14 : 0.41173559491275274
Loss in iteration 15 : 0.41205941869571705
Loss in iteration 16 : 0.4123967971626398
Loss in iteration 17 : 0.4129921949807834
Loss in iteration 18 : 0.413417391180938
Testing accuracy  of updater 2 on alg 1 with rate 1.1199999999999998E-4 = 0.779, training accuracy 0.8297183554548397, time elapsed: 350 millisecond.
Loss in iteration 1 : 1.0000046518743853
Loss in iteration 2 : 0.5574681167623411
Loss in iteration 3 : 0.578233274813846
Loss in iteration 4 : 0.5887815907687854
Loss in iteration 5 : 0.5735739635853446
Loss in iteration 6 : 0.5407670404740234
Loss in iteration 7 : 0.5083174732820274
Loss in iteration 8 : 0.4879764891056076
Loss in iteration 9 : 0.4767678048270553
Loss in iteration 10 : 0.467525206082603
Loss in iteration 11 : 0.45719157118151393
Loss in iteration 12 : 0.4459193008006782
Loss in iteration 13 : 0.4344611445336029
Loss in iteration 14 : 0.42397344713784363
Loss in iteration 15 : 0.41510419629395984
Loss in iteration 16 : 0.40788173587945276
Loss in iteration 17 : 0.40221213681304385
Loss in iteration 18 : 0.39805559825448394
Loss in iteration 19 : 0.395010749235892
Loss in iteration 20 : 0.3925592017402549
Loss in iteration 21 : 0.39061616188831233
Loss in iteration 22 : 0.3891645507931855
Loss in iteration 23 : 0.38805357119123585
Loss in iteration 24 : 0.38722885176702515
Testing accuracy  of updater 2 on alg 1 with rate 2.7999999999999976E-5 = 0.77875, training accuracy 0.8319844609906119, time elapsed: 376 millisecond.
Loss in iteration 1 : 8.683185305790914
Loss in iteration 2 : 98.60194314003876
Loss in iteration 3 : 543.2453370345656
Loss in iteration 4 : 211.0612012967548
Loss in iteration 5 : 900.0237129267346
Loss in iteration 6 : 286.9038592607962
Loss in iteration 7 : 1146.7393275790723
Loss in iteration 8 : 344.11218553408804
Loss in iteration 9 : 1328.7716563767947
Loss in iteration 10 : 388.38640894087257
Loss in iteration 11 : 1466.369201879791
Loss in iteration 12 : 422.80642871885937
Loss in iteration 13 : 1571.0855438614112
Loss in iteration 14 : 449.3986252218088
Loss in iteration 15 : 1650.4345705574685
Loss in iteration 16 : 469.6368595208018
Loss in iteration 17 : 1709.708117985108
Loss in iteration 18 : 484.65581384644594
Loss in iteration 19 : 1752.8326536184404
Loss in iteration 20 : 495.3595193685515
Loss in iteration 21 : 1782.8278251242302
Loss in iteration 22 : 502.48399204767804
Loss in iteration 23 : 1802.075777512661
Loss in iteration 24 : 506.6369718733322
Loss in iteration 25 : 1812.4909581679121
Loss in iteration 26 : 508.32502612211715
Loss in iteration 27 : 1815.6333253150533
Loss in iteration 28 : 507.97308563387674
Loss in iteration 29 : 1812.787278216833
Loss in iteration 30 : 505.93913229619267
Loss in iteration 31 : 1805.0187238072303
Loss in iteration 32 : 502.52559919853036
Loss in iteration 33 : 1793.21758086692
Loss in iteration 34 : 497.98843533740296
Loss in iteration 35 : 1778.1302222832514
Loss in iteration 36 : 492.5444461563095
Loss in iteration 37 : 1760.3847439289957
Loss in iteration 38 : 486.37732088129553
Loss in iteration 39 : 1740.5109802962597
Loss in iteration 40 : 479.64263425893364
Loss in iteration 41 : 1718.9565831773548
Loss in iteration 42 : 472.4720311175997
Loss in iteration 43 : 1696.100090572124
Loss in iteration 44 : 464.9767493745118
Loss in iteration 45 : 1672.2616548100514
Loss in iteration 46 : 457.25060066899675
Loss in iteration 47 : 1647.711922995212
Loss in iteration 48 : 449.37250185073515
Loss in iteration 49 : 1622.6794401985578
Loss in iteration 50 : 441.4086315454527
Loss in iteration 51 : 1597.3568583890021
Loss in iteration 52 : 433.4142717515222
Loss in iteration 53 : 1571.9061705582014
Loss in iteration 54 : 425.43538347389455
Loss in iteration 55 : 1546.4631424981071
Loss in iteration 56 : 417.5099568447275
Loss in iteration 57 : 1521.141079365753
Loss in iteration 58 : 409.66916938358213
Loss in iteration 59 : 1496.034037227026
Loss in iteration 60 : 401.9383805782668
Loss in iteration 61 : 1471.2195689472615
Loss in iteration 62 : 394.3379865117275
Loss in iteration 63 : 1446.7610775072042
Loss in iteration 64 : 386.8841545973147
Loss in iteration 65 : 1422.7098369413181
Loss in iteration 66 : 379.58945544919084
Loss in iteration 67 : 1399.1067308080076
Loss in iteration 68 : 372.46340638231817
Loss in iteration 69 : 1375.9837498120805
Loss in iteration 70 : 365.51293891213277
Loss in iteration 71 : 1353.3652834668771
Loss in iteration 72 : 358.7428008336657
Loss in iteration 73 : 1331.2692351747917
Loss in iteration 74 : 352.1559019450661
Loss in iteration 75 : 1309.7079855682575
Loss in iteration 76 : 345.7536111945973
Loss in iteration 77 : 1288.6892251948864
Loss in iteration 78 : 339.5360119355705
Loss in iteration 79 : 1268.2166745002442
Loss in iteration 80 : 333.5021210396767
Loss in iteration 81 : 1248.290706441918
Loss in iteration 82 : 327.65007682060394
Loss in iteration 83 : 1228.908884866324
Loss in iteration 84 : 321.97730003583683
Loss in iteration 85 : 1210.066429921184
Loss in iteration 86 : 316.48063164779734
Loss in iteration 87 : 1191.7566202023197
Loss in iteration 88 : 311.1564505215251
Loss in iteration 89 : 1173.9711399956636
Loss in iteration 90 : 306.0007738026891
Loss in iteration 91 : 1156.7003788350892
Loss in iteration 92 : 301.0093423466538
Loss in iteration 93 : 1139.9336896220686
Loss in iteration 94 : 296.17769324785473
Loss in iteration 95 : 1123.6596107180994
Loss in iteration 96 : 291.50122124153825
Loss in iteration 97 : 1107.866056703643
Loss in iteration 98 : 286.9752305106928
Loss in iteration 99 : 1092.5404818800814
Loss in iteration 100 : 282.5949782244125
Loss in iteration 101 : 1077.6700200589678
Loss in iteration 102 : 278.3557109554269
Loss in iteration 103 : 1063.241603723054
Loss in iteration 104 : 274.25269497019804
Loss in iteration 105 : 1049.2420652457433
Loss in iteration 106 : 270.2812412514816
Loss in iteration 107 : 1035.658222510866
Loss in iteration 108 : 266.4367259977198
Loss in iteration 109 : 1022.4769509754921
Loss in iteration 110 : 262.7146072436071
Loss in iteration 111 : 1009.6852439585011
Loss in iteration 112 : 259.11043815953127
Loss in iteration 113 : 997.2702627115417
Loss in iteration 114 : 255.61987751253474
Loss in iteration 115 : 985.2193776319701
Loss in iteration 116 : 252.23869770638518
Loss in iteration 117 : 973.5202018057134
Loss in iteration 118 : 248.96279076194625
Loss in iteration 119 : 962.1606179181354
Loss in iteration 120 : 245.78817255013098
Loss in iteration 121 : 951.1287994401597
Loss in iteration 122 : 242.71098554731233
Loss in iteration 123 : 940.4132268825397
Loss in iteration 124 : 239.72750034626296
Loss in iteration 125 : 930.0026998111377
Loss in iteration 126 : 236.8341161237799
Loss in iteration 127 : 919.8863452285801
Loss in iteration 128 : 234.02736023845503
Loss in iteration 129 : 910.0536228509687
Loss in iteration 130 : 231.3038871080173
Loss in iteration 131 : 900.494327741263
Loss in iteration 132 : 228.660476494826
Loss in iteration 133 : 891.1985907020368
Loss in iteration 134 : 226.0940313100045
Loss in iteration 135 : 882.156876778799
Loss in iteration 136 : 223.60157503101124
Loss in iteration 137 : 873.359982179797
Loss in iteration 138 : 221.18024881383292
Loss in iteration 139 : 864.7990298785828
Loss in iteration 140 : 218.82730836918708
Loss in iteration 141 : 856.4654641307991
Loss in iteration 142 : 216.54012066188972
Loss in iteration 143 : 848.3510441061378
Loss in iteration 144 : 214.31616048367903
Loss in iteration 145 : 840.4478368096617
Loss in iteration 146 : 212.15300694211658
Loss in iteration 147 : 832.7482094431479
Loss in iteration 148 : 210.04833990153546
Loss in iteration 149 : 825.2448213365814
Loss in iteration 150 : 207.99993640626917
Loss in iteration 151 : 817.9306155618247
Loss in iteration 152 : 206.00566711142582
Loss in iteration 153 : 810.7988103246973
Loss in iteration 154 : 204.06349274218513
Loss in iteration 155 : 803.8428902178282
Loss in iteration 156 : 202.17146059890695
Loss in iteration 157 : 797.0565974045134
Loss in iteration 158 : 200.32770112215226
Loss in iteration 159 : 790.433922793214
Loss in iteration 160 : 198.53042452898555
Loss in iteration 161 : 783.9690972530254
Loss in iteration 162 : 196.77791752957842
Loss in iteration 163 : 777.6565829123806
Loss in iteration 164 : 195.0685401311251
Loss in iteration 165 : 771.4910645761612
Loss in iteration 166 : 193.4007225343605
Loss in iteration 167 : 765.4674412902438
Loss in iteration 168 : 191.77296212651012
Loss in iteration 169 : 759.5808180771639
Loss in iteration 170 : 190.18382057325718
Loss in iteration 171 : 753.8264978619081
Loss in iteration 172 : 188.63192101125895
Loss in iteration 173 : 748.1999736028481
Loss in iteration 174 : 187.11594534186185
Loss in iteration 175 : 742.6969206393095
Loss in iteration 176 : 185.6346316259187
Loss in iteration 177 : 737.3131892642958
Loss in iteration 178 : 184.186771578997
Loss in iteration 179 : 732.0447975282834
Loss in iteration 180 : 182.77120816575126
Loss in iteration 181 : 726.8879242777913
Loss in iteration 182 : 181.38683329182092
Loss in iteration 183 : 721.838902430546
Loss in iteration 184 : 180.03258559126994
Loss in iteration 185 : 716.8942124874377
Loss in iteration 186 : 178.7074483073192
Loss in iteration 187 : 712.0504762801298
Loss in iteration 188 : 177.41044726390874
Loss in iteration 189 : 707.3044509520173
Loss in iteration 190 : 176.14064892546958
Loss in iteration 191 : 702.6530231692894
Loss in iteration 192 : 174.89715854216163
Loss in iteration 193 : 698.0932035580778
Loss in iteration 194 : 173.679118377762
Loss in iteration 195 : 693.6221213630032
Loss in iteration 196 : 172.48570601732754
Loss in iteration 197 : 689.237019321949
Loss in iteration 198 : 171.31613275174337
Loss in iteration 199 : 684.9352487514607
Loss in iteration 200 : 170.1696420362566
Testing accuracy  of updater 3 on alg 1 with rate 1.96 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3680 millisecond.
Loss in iteration 1 : 1.021666530773821
Loss in iteration 2 : 4.6636474780915185
Loss in iteration 3 : 15.12929625096529
Loss in iteration 4 : 7.572213512114659
Loss in iteration 5 : 19.2819014142543
Loss in iteration 6 : 7.942436919523027
Loss in iteration 7 : 19.774885772612855
Loss in iteration 8 : 7.961489184983598
Loss in iteration 9 : 19.76257838592565
Loss in iteration 10 : 7.9324052190177055
Loss in iteration 11 : 19.681509494296762
Loss in iteration 12 : 7.897115322306471
Loss in iteration 13 : 19.59206077302539
Loss in iteration 14 : 7.861411183689683
Loss in iteration 15 : 19.502561225621662
Loss in iteration 16 : 7.826083831607589
Loss in iteration 17 : 19.414137506102826
Loss in iteration 18 : 7.7912326375157175
Loss in iteration 19 : 19.326920426339996
Loss in iteration 20 : 7.756861586292353
Loss in iteration 21 : 19.240903797249956
Loss in iteration 22 : 7.7229617751322825
Loss in iteration 23 : 19.156063244867415
Loss in iteration 24 : 7.689522817096187
Loss in iteration 25 : 19.072372621084785
Loss in iteration 26 : 7.656534407071707
Loss in iteration 27 : 18.989806244766875
Loss in iteration 28 : 7.623986526992755
Loss in iteration 29 : 18.908339183483406
Loss in iteration 30 : 7.591869463314175
Loss in iteration 31 : 18.827947265467134
Loss in iteration 32 : 7.560173799030569
Loss in iteration 33 : 18.748607055533792
Loss in iteration 34 : 7.528890402667229
Loss in iteration 35 : 18.670295827235996
Loss in iteration 36 : 7.498010417319329
Loss in iteration 37 : 18.59299153564938
Loss in iteration 38 : 7.467525250139196
Loss in iteration 39 : 18.516672791336088
Loss in iteration 40 : 7.4374265623027584
Loss in iteration 41 : 18.441318835501185
Loss in iteration 42 : 7.407706259436838
Loss in iteration 43 : 18.366909516288626
Loss in iteration 44 : 7.378356482483542
Loss in iteration 45 : 18.29342526615683
Loss in iteration 46 : 7.349369598978338
Loss in iteration 47 : 18.220847080276453
Loss in iteration 48 : 7.32073819472008
Loss in iteration 49 : 18.149156495896246
Loss in iteration 50 : 7.292455065812162
Loss in iteration 51 : 18.078335572626212
Loss in iteration 52 : 7.264513211055576
Loss in iteration 53 : 18.00836687359005
Loss in iteration 54 : 7.236905824675454
Loss in iteration 55 : 17.93923344740234
Loss in iteration 56 : 7.209626289364152
Loss in iteration 57 : 17.87091881092768
Loss in iteration 58 : 7.182668169624552
Loss in iteration 59 : 17.80340693278246
Loss in iteration 60 : 7.156025205398493
Loss in iteration 61 : 17.736682217541347
Loss in iteration 62 : 7.129691305965993
Loss in iteration 63 : 17.67072949061353
Loss in iteration 64 : 7.103660544101734
Loss in iteration 65 : 17.605533983755144
Loss in iteration 66 : 7.077927150476108
Loss in iteration 67 : 17.541081321186727
Loss in iteration 68 : 7.052485508288819
Loss in iteration 69 : 17.477357506285827
Loss in iteration 70 : 7.027330148123744
Loss in iteration 71 : 17.41434890882708
Loss in iteration 72 : 7.002455743014278
Loss in iteration 73 : 17.352042252743043
Loss in iteration 74 : 6.9778571037091
Loss in iteration 75 : 17.290424604381155
Loss in iteration 76 : 6.95352917412889
Loss in iteration 77 : 17.229483361232987
Loss in iteration 78 : 6.929467027004786
Loss in iteration 79 : 17.16920624111363
Loss in iteration 80 : 6.905665859690238
Loss in iteration 81 : 17.109581271770086
Loss in iteration 82 : 6.882120990138033
Loss in iteration 83 : 17.05059678089874
Loss in iteration 84 : 6.8588278530349145
Loss in iteration 85 : 16.992241386552877
Loss in iteration 86 : 6.83578199608658
Loss in iteration 87 : 16.93450398792273
Loss in iteration 88 : 6.812979076446087
Loss in iteration 89 : 16.87737375647045
Loss in iteration 90 : 6.790414857279272
Loss in iteration 91 : 16.820840127404708
Loss in iteration 92 : 6.768085204460989
Loss in iteration 93 : 16.76489279147904
Loss in iteration 94 : 6.745986083396261
Loss in iteration 95 : 16.709521687099862
Loss in iteration 96 : 6.7241135559609155
Loss in iteration 97 : 16.654716992730332
Loss in iteration 98 : 6.7024637775562885
Loss in iteration 99 : 16.60046911957698
Loss in iteration 100 : 6.681032994273171
Loss in iteration 101 : 16.546768704546814
Loss in iteration 102 : 6.65981754016006
Loss in iteration 103 : 16.493606603463224
Loss in iteration 104 : 6.638813834591376
Loss in iteration 105 : 16.440973884529377
Loss in iteration 106 : 6.618018379731244
Loss in iteration 107 : 16.388861822028566
Loss in iteration 108 : 6.59742775808882
Loss in iteration 109 : 16.337261890251483
Loss in iteration 110 : 6.577038630161231
Loss in iteration 111 : 16.286165757640745
Loss in iteration 112 : 6.55684773216053
Loss in iteration 113 : 16.235565281143558
Loss in iteration 114 : 6.536851873821049
Loss in iteration 115 : 16.185452500763923
Loss in iteration 116 : 6.517047936283854
Loss in iteration 117 : 16.135819634305903
Loss in iteration 118 : 6.497432870055179
Loss in iteration 119 : 16.086659072300346
Loss in iteration 120 : 6.478003693035659
Loss in iteration 121 : 16.03796337310732
Loss in iteration 122 : 6.458757488617652
Loss in iteration 123 : 15.989725258187235
Loss in iteration 124 : 6.43969140384771
Loss in iteration 125 : 15.941937607533754
Loss in iteration 126 : 6.420802647651754
Loss in iteration 127 : 15.894593455262129
Loss in iteration 128 : 6.402088489120289
Loss in iteration 129 : 15.84768598534654
Loss in iteration 130 : 6.383546255851354
Loss in iteration 131 : 15.801208527500764
Loss in iteration 132 : 6.365173332348955
Loss in iteration 133 : 15.755154553196181
Loss in iteration 134 : 6.346967158474658
Loss in iteration 135 : 15.70951767181231
Loss in iteration 136 : 6.328925227950463
Loss in iteration 137 : 15.664291626913874
Loss in iteration 138 : 6.311045086910808
Loss in iteration 139 : 15.61947029265036
Loss in iteration 140 : 6.293324332501861
Loss in iteration 141 : 15.575047670272708
Loss in iteration 142 : 6.2757606115263265
Loss in iteration 143 : 15.531017884762994
Loss in iteration 144 : 6.258351619131964
Loss in iteration 145 : 15.487375181572643
Loss in iteration 146 : 6.241095097542195
Loss in iteration 147 : 15.444113923465189
Loss in iteration 148 : 6.22398883482723
Loss in iteration 149 : 15.401228587459478
Loss in iteration 150 : 6.207030663714125
Loss in iteration 151 : 15.358713761869753
Loss in iteration 152 : 6.190218460434432
Loss in iteration 153 : 15.316564143438843
Loss in iteration 154 : 6.173550143607912
Loss in iteration 155 : 15.274774534561088
Loss in iteration 156 : 6.157023673161123
Loss in iteration 157 : 15.233339840591812
Loss in iteration 158 : 6.140637049279474
Loss in iteration 159 : 15.192255067239907
Loss in iteration 160 : 6.124388311391639
Loss in iteration 161 : 15.151515318040746
Loss in iteration 162 : 6.108275537185063
Loss in iteration 163 : 15.111115791906464
Loss in iteration 164 : 6.092296841651519
Loss in iteration 165 : 15.071051780750729
Loss in iteration 166 : 6.076450376161588
Loss in iteration 167 : 15.031318667185433
Loss in iteration 168 : 6.060734327567055
Loss in iteration 169 : 14.991911922286741
Loss in iteration 170 : 6.045146917330203
Loss in iteration 171 : 14.952827103427996
Loss in iteration 172 : 6.029686400679104
Loss in iteration 173 : 14.914059852177191
Loss in iteration 174 : 6.014351065787962
Loss in iteration 175 : 14.875605892256603
Loss in iteration 176 : 5.999139232981639
Loss in iteration 177 : 14.837461027562705
Loss in iteration 178 : 5.984049253963524
Loss in iteration 179 : 14.799621140243934
Loss in iteration 180 : 5.969079511065943
Loss in iteration 181 : 14.76208218883452
Loss in iteration 182 : 5.954228416522374
Loss in iteration 183 : 14.724840206442568
Loss in iteration 184 : 5.939494411760613
Loss in iteration 185 : 14.68789129899018
Loss in iteration 186 : 5.924875966716323
Loss in iteration 187 : 14.651231643504168
Loss in iteration 188 : 5.9103715791661715
Loss in iteration 189 : 14.614857486455575
Loss in iteration 190 : 5.895979774079941
Loss in iteration 191 : 14.5787651421463
Loss in iteration 192 : 5.881699102990969
Loss in iteration 193 : 14.542950991141316
Loss in iteration 194 : 5.867528143384309
Loss in iteration 195 : 14.50741147874499
Loss in iteration 196 : 5.853465498102035
Loss in iteration 197 : 14.472143113519817
Loss in iteration 198 : 5.839509794765108
Loss in iteration 199 : 14.437142465846712
Loss in iteration 200 : 5.825659685211265
Testing accuracy  of updater 3 on alg 1 with rate 1.372 = 0.5, training accuracy 0.3525412754936873, time elapsed: 4993 millisecond.
Loss in iteration 1 : 1.0035486670469298
Loss in iteration 2 : 2.0941119651900646
Loss in iteration 3 : 1.1957665227857814
Loss in iteration 4 : 2.423956583133869
Loss in iteration 5 : 0.9778569120034606
Loss in iteration 6 : 2.2471011338898066
Loss in iteration 7 : 1.1248082019218046
Loss in iteration 8 : 2.376385379297718
Loss in iteration 9 : 1.0030284395229252
Loss in iteration 10 : 2.275996278323061
Loss in iteration 11 : 1.0921561476878598
Loss in iteration 12 : 2.351977126680246
Loss in iteration 13 : 1.017651570730036
Loss in iteration 14 : 2.2875950061601986
Loss in iteration 15 : 1.0739364854746627
Loss in iteration 16 : 2.330229148377322
Loss in iteration 17 : 1.0309705225190413
Loss in iteration 18 : 2.29743193056157
Loss in iteration 19 : 1.056686478906181
Loss in iteration 20 : 2.3149955363168293
Loss in iteration 21 : 1.0389110346954318
Loss in iteration 22 : 2.2942387550671333
Loss in iteration 23 : 1.0528535214899755
Loss in iteration 24 : 2.3035675944504277
Loss in iteration 25 : 1.0412288931172262
Loss in iteration 26 : 2.2891958823330834
Loss in iteration 27 : 1.05024631430107
Loss in iteration 28 : 2.294094995556393
Loss in iteration 29 : 1.0425671151208777
Loss in iteration 30 : 2.284466963498868
Loss in iteration 31 : 1.0474534818520955
Loss in iteration 32 : 2.2836233976422426
Loss in iteration 33 : 1.044678720314838
Loss in iteration 34 : 2.276856453159724
Loss in iteration 35 : 1.047620535043889
Loss in iteration 36 : 2.2754272972852396
Loss in iteration 37 : 1.0452189723126122
Loss in iteration 38 : 2.2721773536825554
Loss in iteration 39 : 1.0449628507989461
Loss in iteration 40 : 2.2682002786430497
Loss in iteration 41 : 1.0451581356269948
Loss in iteration 42 : 2.2630640329266996
Loss in iteration 43 : 1.0462220547120922
Loss in iteration 44 : 2.260868802818984
Loss in iteration 45 : 1.0452606188035365
Loss in iteration 46 : 2.252671125678095
Loss in iteration 47 : 1.0482354604823105
Loss in iteration 48 : 2.2535172114490947
Loss in iteration 49 : 1.0454460372208805
Loss in iteration 50 : 2.243077050551899
Loss in iteration 51 : 1.0498430162701289
Loss in iteration 52 : 2.2455827309434766
Loss in iteration 53 : 1.0460093118649114
Loss in iteration 54 : 2.235958437827069
Loss in iteration 55 : 1.049962202286837
Loss in iteration 56 : 2.2369400897406293
Loss in iteration 57 : 1.0470949920606711
Loss in iteration 58 : 2.229562607179395
Loss in iteration 59 : 1.0497243361924846
Loss in iteration 60 : 2.226250190990145
Loss in iteration 61 : 1.049718649900814
Loss in iteration 62 : 2.225189561020157
Loss in iteration 63 : 1.0472088647577855
Loss in iteration 64 : 2.2178137669233777
Loss in iteration 65 : 1.0507474884756083
Loss in iteration 66 : 2.2189258688707003
Loss in iteration 67 : 1.04699846336054
Loss in iteration 68 : 2.208737947043064
Loss in iteration 69 : 1.0522163017696855
Loss in iteration 70 : 2.213514287732968
Loss in iteration 71 : 1.0450741147416733
Loss in iteration 72 : 2.201928167657745
Loss in iteration 73 : 1.052350618305795
Loss in iteration 74 : 2.2045424549795567
Loss in iteration 75 : 1.046525158949581
Loss in iteration 76 : 2.1963199543589007
Loss in iteration 77 : 1.0507280347188586
Loss in iteration 78 : 2.1976455197009486
Loss in iteration 79 : 1.0468575422310904
Loss in iteration 80 : 2.189627015728783
Loss in iteration 81 : 1.0509225046421198
Loss in iteration 82 : 2.1914725219557405
Loss in iteration 83 : 1.045635213693424
Loss in iteration 84 : 2.1823082788676
Loss in iteration 85 : 1.0514574292940868
Loss in iteration 86 : 2.18483495188493
Loss in iteration 87 : 1.0458580593507487
Loss in iteration 88 : 2.17579366260751
Loss in iteration 89 : 1.0516012661957903
Loss in iteration 90 : 2.178327471012488
Loss in iteration 91 : 1.046024928155651
Loss in iteration 92 : 2.169252193339301
Loss in iteration 93 : 1.0518368610119289
Loss in iteration 94 : 2.171141589381415
Loss in iteration 95 : 1.046643484464188
Loss in iteration 96 : 2.1649139778337125
Loss in iteration 97 : 1.0496062896071703
Loss in iteration 98 : 2.1647688512630285
Loss in iteration 99 : 1.0467915753795287
Loss in iteration 100 : 2.1588605575083526
Loss in iteration 101 : 1.0485522639993288
Loss in iteration 102 : 2.158369044949524
Loss in iteration 103 : 1.0469977459065463
Loss in iteration 104 : 2.1516919699083608
Loss in iteration 105 : 1.049293001033177
Loss in iteration 106 : 2.150527604961575
Loss in iteration 107 : 1.048125408095178
Loss in iteration 108 : 2.146084522299505
Loss in iteration 109 : 1.049120530212003
Loss in iteration 110 : 2.1432765027428493
Loss in iteration 111 : 1.049057408602906
Loss in iteration 112 : 2.1397376667283488
Loss in iteration 113 : 1.0494599884353755
Loss in iteration 114 : 2.136346185627158
Loss in iteration 115 : 1.0497161709522072
Loss in iteration 116 : 2.133627966294958
Loss in iteration 117 : 1.0496183166811777
Loss in iteration 118 : 2.128335128507583
Loss in iteration 119 : 1.051218683867137
Loss in iteration 120 : 2.129453413468661
Loss in iteration 121 : 1.0475833104490129
Loss in iteration 122 : 2.121008941843919
Loss in iteration 123 : 1.0520711522735013
Loss in iteration 124 : 2.123427594605099
Loss in iteration 125 : 1.0477637084111833
Loss in iteration 126 : 2.114415150990752
Loss in iteration 127 : 1.0525933656450814
Loss in iteration 128 : 2.12009142979003
Loss in iteration 129 : 1.0440979915210666
Loss in iteration 130 : 2.1071525698714577
Loss in iteration 131 : 1.0534539105917196
Loss in iteration 132 : 2.1134438898201773
Loss in iteration 133 : 1.0447058618391905
Loss in iteration 134 : 2.1019482721253233
Loss in iteration 135 : 1.0532739585261144
Loss in iteration 136 : 2.1076094956209968
Loss in iteration 137 : 1.0449048021701894
Loss in iteration 138 : 2.0955210375257414
Loss in iteration 139 : 1.0537795219493535
Loss in iteration 140 : 2.1029146505322327
Loss in iteration 141 : 1.0433727052117965
Loss in iteration 142 : 2.0904989584994853
Loss in iteration 143 : 1.0535047724200728
Loss in iteration 144 : 2.0971820252419575
Loss in iteration 145 : 1.043568502614446
Loss in iteration 146 : 2.082899993119724
Loss in iteration 147 : 1.054972123697205
Loss in iteration 148 : 2.0914352651828776
Loss in iteration 149 : 1.043801014255386
Loss in iteration 150 : 2.079142232670081
Loss in iteration 151 : 1.0529498541268758
Loss in iteration 152 : 2.0858743221501492
Loss in iteration 153 : 1.0439036017116683
Loss in iteration 154 : 2.07157608929109
Loss in iteration 155 : 1.0541756122321424
Loss in iteration 156 : 2.080848683792103
Loss in iteration 157 : 1.0437991678478844
Loss in iteration 158 : 2.0651021070056976
Loss in iteration 159 : 1.0549855406322697
Loss in iteration 160 : 2.075916988684409
Loss in iteration 161 : 1.043677388776097
Loss in iteration 162 : 2.0587135924295255
Loss in iteration 163 : 1.0557591531208899
Loss in iteration 164 : 2.070317122293561
Loss in iteration 165 : 1.0439862256035886
Loss in iteration 166 : 2.057523587346922
Loss in iteration 167 : 1.05105429086444
Loss in iteration 168 : 2.06045509024835
Loss in iteration 169 : 1.0467408269937932
Loss in iteration 170 : 2.05514847891791
Loss in iteration 171 : 1.0484253617745742
Loss in iteration 172 : 2.0542399587691715
Loss in iteration 173 : 1.0474589640138754
Loss in iteration 174 : 2.04967971964353
Loss in iteration 175 : 1.0487025209692526
Loss in iteration 176 : 2.0479605779507812
Loss in iteration 177 : 1.0482777326145964
Loss in iteration 178 : 2.0436471174605217
Loss in iteration 179 : 1.0492855815009725
Loss in iteration 180 : 2.041252098621468
Loss in iteration 181 : 1.0492537468276921
Loss in iteration 182 : 2.038955573685
Loss in iteration 183 : 1.049171529548061
Loss in iteration 184 : 2.0352871112640933
Loss in iteration 185 : 1.049862010879564
Loss in iteration 186 : 2.0329534256126913
Loss in iteration 187 : 1.049816596653046
Loss in iteration 188 : 2.0299430964185126
Loss in iteration 189 : 1.0501801851088572
Loss in iteration 190 : 2.027007612197692
Loss in iteration 191 : 1.050466921705441
Loss in iteration 192 : 2.023939900018762
Loss in iteration 193 : 1.0508963280977803
Loss in iteration 194 : 2.021675978413939
Loss in iteration 195 : 1.0508386603563078
Loss in iteration 196 : 2.0194580573351524
Loss in iteration 197 : 1.050758131678749
Loss in iteration 198 : 2.017250724298399
Loss in iteration 199 : 1.0506764043484338
Loss in iteration 200 : 2.0150522706562697
Testing accuracy  of updater 3 on alg 1 with rate 0.784 = 0.58225, training accuracy 0.4541922952411784, time elapsed: 5300 millisecond.
Loss in iteration 1 : 1.0001581446053034
Loss in iteration 2 : 0.7248868075703988
Loss in iteration 3 : 0.532023796826284
Loss in iteration 4 : 0.5507382512012035
Loss in iteration 5 : 0.5319052476473566
Loss in iteration 6 : 0.5518907201073548
Loss in iteration 7 : 0.5320130674498501
Loss in iteration 8 : 0.5538283731490399
Loss in iteration 9 : 0.5320314407244648
Loss in iteration 10 : 0.5535239676301763
Loss in iteration 11 : 0.5310767912581184
Loss in iteration 12 : 0.5522779217196736
Loss in iteration 13 : 0.529656854111851
Loss in iteration 14 : 0.5507596598449863
Loss in iteration 15 : 0.5291921404279581
Loss in iteration 16 : 0.5492518801038571
Loss in iteration 17 : 0.52838383947361
Loss in iteration 18 : 0.5468995940816256
Loss in iteration 19 : 0.5273085555951172
Loss in iteration 20 : 0.5441300336784795
Loss in iteration 21 : 0.5267886100396755
Loss in iteration 22 : 0.5428705119723738
Loss in iteration 23 : 0.5270301681805893
Loss in iteration 24 : 0.5426811391424747
Loss in iteration 25 : 0.5275025327980017
Loss in iteration 26 : 0.5432302931220161
Loss in iteration 27 : 0.5279604873219449
Loss in iteration 28 : 0.5441052217929379
Loss in iteration 29 : 0.5283105189188552
Loss in iteration 30 : 0.5446726054747981
Loss in iteration 31 : 0.5285396254363188
Loss in iteration 32 : 0.5450131216895594
Loss in iteration 33 : 0.5284884966471781
Loss in iteration 34 : 0.5450527393234768
Loss in iteration 35 : 0.5284399248932106
Loss in iteration 36 : 0.5449524965207889
Loss in iteration 37 : 0.5284239185113531
Loss in iteration 38 : 0.545021557235707
Loss in iteration 39 : 0.5283980125180802
Loss in iteration 40 : 0.5449403692285483
Loss in iteration 41 : 0.528396661950467
Loss in iteration 42 : 0.5450217299834644
Loss in iteration 43 : 0.528380258353219
Loss in iteration 44 : 0.5449485155302471
Loss in iteration 45 : 0.5283850115987375
Loss in iteration 46 : 0.5450350128356808
Loss in iteration 47 : 0.5283722880864391
Loss in iteration 48 : 0.5447953038896014
Loss in iteration 49 : 0.5284349001855091
Loss in iteration 50 : 0.5449439970093914
Loss in iteration 51 : 0.5284046236174113
Loss in iteration 52 : 0.5449143519248443
Loss in iteration 53 : 0.5284005005116521
Loss in iteration 54 : 0.5450289769212192
Loss in iteration 55 : 0.5283820383430569
Loss in iteration 56 : 0.5448075428336209
Loss in iteration 57 : 0.5284408899438882
Loss in iteration 58 : 0.5449679642744492
Loss in iteration 59 : 0.5284082817257977
Loss in iteration 60 : 0.5449459446033026
Loss in iteration 61 : 0.5284021126559308
Loss in iteration 62 : 0.5449357382768161
Loss in iteration 63 : 0.5283980676445944
Loss in iteration 64 : 0.5449331690471648
Loss in iteration 65 : 0.5283953968758203
Loss in iteration 66 : 0.5449355373246889
Loss in iteration 67 : 0.5283936156582023
Loss in iteration 68 : 0.5449410980450764
Loss in iteration 69 : 0.5283924106732144
Loss in iteration 70 : 0.5449487231585807
Loss in iteration 71 : 0.5283915793758635
Loss in iteration 72 : 0.5449576834600754
Loss in iteration 73 : 0.5283909908241882
Loss in iteration 74 : 0.54496750756195
Loss in iteration 75 : 0.5283905603599658
Loss in iteration 76 : 0.5449778907328061
Loss in iteration 77 : 0.5283902332426099
Loss in iteration 78 : 0.5449886359698716
Loss in iteration 79 : 0.5283896704527387
Loss in iteration 80 : 0.5448299715439636
Loss in iteration 81 : 0.528445023759359
Loss in iteration 82 : 0.5450307121258109
Loss in iteration 83 : 0.5284101149726759
Loss in iteration 84 : 0.5448652641064672
Loss in iteration 85 : 0.528457634348283
Loss in iteration 86 : 0.5449319734164314
Loss in iteration 87 : 0.5284333656134202
Loss in iteration 88 : 0.5449791318048779
Loss in iteration 89 : 0.528417635606783
Loss in iteration 90 : 0.5450136527367782
Loss in iteration 91 : 0.5284071486448184
Loss in iteration 92 : 0.5448704327260363
Loss in iteration 93 : 0.5284555269334109
Loss in iteration 94 : 0.5449514646877068
Loss in iteration 95 : 0.5284318417627413
Loss in iteration 96 : 0.5450078835273845
Loss in iteration 97 : 0.5284162053429651
Loss in iteration 98 : 0.5448788560276642
Loss in iteration 99 : 0.5284612652968406
Loss in iteration 100 : 0.5449690277610241
Loss in iteration 101 : 0.5284354406562051
Loss in iteration 102 : 0.5450313560728957
Loss in iteration 103 : 0.528418786236388
Loss in iteration 104 : 0.5446020548732409
Loss in iteration 105 : 0.5283334934265117
Loss in iteration 106 : 0.5448150106068901
Loss in iteration 107 : 0.5285424961719635
Loss in iteration 108 : 0.5449438561606609
Loss in iteration 109 : 0.5284878228622133
Loss in iteration 110 : 0.5450311853284305
Loss in iteration 111 : 0.5284532424595563
Loss in iteration 112 : 0.5449446317477676
Loss in iteration 113 : 0.5284633920895023
Loss in iteration 114 : 0.5450397707524255
Loss in iteration 115 : 0.5284367423604578
Loss in iteration 116 : 0.5446316556976438
Loss in iteration 117 : 0.5283451336030067
Loss in iteration 118 : 0.5448584722275096
Loss in iteration 119 : 0.5285496341923412
Loss in iteration 120 : 0.5449961419993706
Loss in iteration 121 : 0.5284922692809034
Loss in iteration 122 : 0.5450894689500642
Loss in iteration 123 : 0.5284827412259928
Loss in iteration 124 : 0.545011885929655
Loss in iteration 125 : 0.5284495642043926
Loss in iteration 126 : 0.5447911098161016
Loss in iteration 127 : 0.5285156357890468
Loss in iteration 128 : 0.54496879169327
Loss in iteration 129 : 0.5284710086775489
Loss in iteration 130 : 0.5449407328105572
Loss in iteration 131 : 0.5284754003971553
Loss in iteration 132 : 0.5449266497930711
Loss in iteration 133 : 0.5284782243534308
Loss in iteration 134 : 0.5449216013230771
Loss in iteration 135 : 0.5284800357284554
Loss in iteration 136 : 0.5449223932105913
Loss in iteration 137 : 0.5284811933594532
Loss in iteration 138 : 0.5449269606398169
Loss in iteration 139 : 0.5284819291689639
Loss in iteration 140 : 0.5449339688469074
Loss in iteration 141 : 0.5284823930465216
Loss in iteration 142 : 0.5449425549913458
Loss in iteration 143 : 0.5284826818597848
Loss in iteration 144 : 0.5449521612961634
Loss in iteration 145 : 0.5284828582063735
Loss in iteration 146 : 0.5449624271858031
Loss in iteration 147 : 0.528482962534144
Loss in iteration 148 : 0.5449731195612796
Loss in iteration 149 : 0.5284830209750294
Loss in iteration 150 : 0.5449840877281825
Loss in iteration 151 : 0.5284830504083167
Loss in iteration 152 : 0.5449952342609029
Loss in iteration 153 : 0.5284823169331048
Loss in iteration 154 : 0.5446806754304079
Loss in iteration 155 : 0.5284131769492132
Loss in iteration 156 : 0.5449899342583597
Loss in iteration 157 : 0.5285340479780625
Loss in iteration 158 : 0.5450111354361813
Loss in iteration 159 : 0.5285160139635212
Loss in iteration 160 : 0.5450288967282186
Loss in iteration 161 : 0.5285036219651419
Loss in iteration 162 : 0.5447184388806723
Loss in iteration 163 : 0.5284270107908251
Loss in iteration 164 : 0.5450306457585823
Loss in iteration 165 : 0.5285429090941637
Loss in iteration 166 : 0.5450536304084609
Loss in iteration 167 : 0.5285212823368616
Loss in iteration 168 : 0.5449152690200787
Loss in iteration 169 : 0.5285502624122806
Loss in iteration 170 : 0.5449871632700991
Loss in iteration 171 : 0.5285264784693974
Loss in iteration 172 : 0.5450376933952112
Loss in iteration 173 : 0.5285104650285395
Loss in iteration 174 : 0.54474845121693
Loss in iteration 175 : 0.528431541986752
Loss in iteration 176 : 0.5450743302941519
Loss in iteration 177 : 0.5285452596622231
Loss in iteration 178 : 0.5449489418585043
Loss in iteration 179 : 0.5285657470802506
Loss in iteration 180 : 0.5450291653505034
Loss in iteration 181 : 0.5285360434925189
Loss in iteration 182 : 0.5449278825245478
Loss in iteration 183 : 0.5285597893832655
Loss in iteration 184 : 0.5450236657089881
Loss in iteration 185 : 0.5285319579440964
Loss in iteration 186 : 0.5447642989104247
Loss in iteration 187 : 0.5284961179270852
Loss in iteration 188 : 0.5449437239135309
Loss in iteration 189 : 0.5285879289998725
Loss in iteration 190 : 0.5450460426729394
Loss in iteration 191 : 0.5285504261681672
Loss in iteration 192 : 0.5449590981732088
Loss in iteration 193 : 0.5285690728265545
Loss in iteration 194 : 0.5450643975428229
Loss in iteration 195 : 0.5285658723233535
Loss in iteration 196 : 0.5449949055402947
Loss in iteration 197 : 0.5285359164192127
Loss in iteration 198 : 0.5447701704089284
Loss in iteration 199 : 0.5284988013167495
Loss in iteration 200 : 0.5449718285161737
Testing accuracy  of updater 3 on alg 1 with rate 0.196 = 0.5, training accuracy 0.6474587245063127, time elapsed: 4766 millisecond.
Loss in iteration 1 : 1.0000727047452425
Loss in iteration 2 : 0.6126898423679528
Loss in iteration 3 : 0.5409319364804959
Loss in iteration 4 : 0.5403332039954191
Loss in iteration 5 : 0.5387351502325618
Loss in iteration 6 : 0.5383500897298051
Loss in iteration 7 : 0.5370927589728943
Loss in iteration 8 : 0.5368986684192463
Loss in iteration 9 : 0.5360695019537
Loss in iteration 10 : 0.5358517821336376
Loss in iteration 11 : 0.5353081199282588
Loss in iteration 12 : 0.5350650363051426
Testing accuracy  of updater 3 on alg 1 with rate 0.13720000000000002 = 0.5, training accuracy 0.6474587245063127, time elapsed: 287 millisecond.
Loss in iteration 1 : 1.000023629942639
Loss in iteration 2 : 0.5572959317361302
Loss in iteration 3 : 0.5540622087675654
Loss in iteration 4 : 0.5521387406444833
Loss in iteration 5 : 0.5503838805346648
Loss in iteration 6 : 0.5487580816053046
Loss in iteration 7 : 0.5471991239281397
Loss in iteration 8 : 0.545897100011318
Loss in iteration 9 : 0.5447381065359734
Loss in iteration 10 : 0.5437108961096495
Loss in iteration 11 : 0.5428034244619366
Loss in iteration 12 : 0.5419486504239829
Loss in iteration 13 : 0.5410927188030554
Loss in iteration 14 : 0.540464754907949
Testing accuracy  of updater 3 on alg 1 with rate 0.07840000000000001 = 0.5, training accuracy 0.6474587245063127, time elapsed: 403 millisecond.
Loss in iteration 1 : 1.00000146971072
Loss in iteration 2 : 0.8437899473087854
Loss in iteration 3 : 0.7006138669753806
Loss in iteration 4 : 0.620014802794156
Testing accuracy  of updater 3 on alg 1 with rate 0.019600000000000006 = 0.5, training accuracy 0.6474587245063127, time elapsed: 118 millisecond.
Loss in iteration 1 : 1.0000001999961752
Loss in iteration 2 : 1.145489929124024
Loss in iteration 3 : 198857.393034138
Loss in iteration 4 : 1.9839138327165024E11
Loss in iteration 5 : 1.97994784597425632E17
Loss in iteration 6 : 1.9759899295530334E23
Loss in iteration 7 : 1.9720399256837132E29
Loss in iteration 8 : 1.968097817872271E35
Loss in iteration 9 : 1.9641635903343436E41
Loss in iteration 10 : 1.9602372273172663E47
Loss in iteration 11 : 1.9563187130998593E53
Loss in iteration 12 : 1.9524080319923717E59
Loss in iteration 13 : 1.9485051683364194E65
Loss in iteration 14 : 1.9446101065049147E71
Loss in iteration 15 : 1.9407228309020115E77
Loss in iteration 16 : 1.9368433259630383E83
Loss in iteration 17 : 1.9329715761544385E89
Loss in iteration 18 : 1.9291075659737057E95
Loss in iteration 19 : 1.9252512799493246E101
Loss in iteration 20 : 1.9214027026407052E107
Loss in iteration 21 : 1.917561818638127E113
Loss in iteration 22 : 1.9137286125626693E119
Loss in iteration 23 : 1.909903069066156E125
Loss in iteration 24 : 1.9060851728310934E131
Loss in iteration 25 : 1.9022749085706046E137
Loss in iteration 26 : 1.898472261028372E143
Loss in iteration 27 : 1.8946772149785755E149
Loss in iteration 28 : 1.8908897552258333E155
Loss in iteration 29 : 1.8871098666051376E161
Loss in iteration 30 : 1.8833375339817936E167
Loss in iteration 31 : 1.8795727422513631E173
Loss in iteration 32 : 1.8758154763396033E179
Loss in iteration 33 : 1.8720657212024007E185
Loss in iteration 34 : 1.8683234618257168E191
Loss in iteration 35 : 1.8645886832255274E197
Loss in iteration 36 : 1.8608613704477598E203
Loss in iteration 37 : 1.857141508568235E209
Loss in iteration 38 : 1.8534290826926058E215
Loss in iteration 39 : 1.8497240779563035E221
Loss in iteration 40 : 1.8460264795244693E227
Loss in iteration 41 : 1.8423362725918998E233
Loss in iteration 42 : 1.8386534423829883E239
Loss in iteration 43 : 1.8349779741516648E245
Loss in iteration 44 : 1.8313098531813356E251
Loss in iteration 45 : 1.8276490647848266E257
Loss in iteration 46 : 1.823995594304321E263
Loss in iteration 47 : 1.820349427111307E269
Loss in iteration 48 : 1.816710548606512E275
Loss in iteration 49 : 1.8130789442198466E281
Loss in iteration 50 : 1.8094545994103512E287
Loss in iteration 51 : 1.8058374996661304E293
Loss in iteration 52 : 1.802227630504298E299
Loss in iteration 53 : 1.7986249774709197E305
Loss in iteration 54 : Infinity
Loss in iteration 55 : Infinity
Loss in iteration 56 : Infinity
Loss in iteration 57 : Infinity
Loss in iteration 58 : Infinity
Loss in iteration 59 : Infinity
Loss in iteration 60 : Infinity
Loss in iteration 61 : Infinity
Loss in iteration 62 : Infinity
Loss in iteration 63 : Infinity
Loss in iteration 64 : Infinity
Loss in iteration 65 : Infinity
Loss in iteration 66 : Infinity
Loss in iteration 67 : Infinity
Loss in iteration 68 : Infinity
Loss in iteration 69 : Infinity
Loss in iteration 70 : Infinity
Loss in iteration 71 : Infinity
Loss in iteration 72 : Infinity
Loss in iteration 73 : Infinity
Loss in iteration 74 : Infinity
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 5149 millisecond.
Loss in iteration 1 : 1.0000001999961752
Loss in iteration 2 : 1.0437349924734927
Loss in iteration 3 : 47653.570159792
Loss in iteration 4 : 2.326003624270933E10
Loss in iteration 5 : 1.1364873549303888E16
Loss in iteration 6 : 5.552888569713471E21
Loss in iteration 7 : 2.713146908048901E27
Loss in iteration 8 : 1.3256462924195952E33
Loss in iteration 9 : 6.477121041225069E38
Loss in iteration 10 : 3.164727817863609E44
Loss in iteration 11 : 1.5462891765359776E50
Loss in iteration 12 : 7.55518437944655E55
Loss in iteration 13 : 3.6914706429819646E61
Loss in iteration 14 : 1.80365624763163E67
Loss in iteration 15 : 8.812682462490623E72
Loss in iteration 16 : 4.305885463855382E78
Loss in iteration 17 : 2.103859943525203E84
Loss in iteration 18 : 1.027948072266358E90
Loss in iteration 19 : 5.0225645605741457E95
Loss in iteration 20 : 2.4540300668610883E101
Loss in iteration 21 : 1.1990415446983944E107
Loss in iteration 22 : 5.858528977811803E112
Loss in iteration 23 : 2.862483117087825E118
Loss in iteration 24 : 1.3986121134922285E124
Loss in iteration 25 : 6.833632772644163E129
Loss in iteration 26 : 3.3389198063467105E135
Loss in iteration 27 : 1.6313995563008094E141
Loss in iteration 28 : 7.971034546081316E146
Loss in iteration 29 : 3.894655450249877E152
Loss in iteration 30 : 1.9029325476475403E158
Loss in iteration 31 : 9.297747457131356E163
Loss in iteration 32 : 4.542888705301839E169
Loss in iteration 33 : 2.2196599642991834E175
Loss in iteration 34 : 1.0845280782165453E181
Loss in iteration 35 : 5.299015035446825E186
Loss in iteration 36 : 2.589104045334352E192
Loss in iteration 37 : 1.2650388256544103E198
Loss in iteration 38 : 6.1809923525357056E203
Loss in iteration 39 : 3.0200390444412972E209
Loss in iteration 40 : 1.4755940971530627E215
Loss in iteration 41 : 7.209767514630836E220
Loss in iteration 42 : 3.5226996174161406E226
Loss in iteration 43 : 1.7211945557691437E232
Loss in iteration 44 : 8.409773811433592E237
Loss in iteration 45 : 4.1090238940402645E243
Loss in iteration 46 : 2.0076731836519678E249
Loss in iteration 47 : 9.809511252055352E254
Loss in iteration 48 : 4.792937007265496E260
Loss in iteration 49 : 2.3418338146869282E266
Loss in iteration 50 : 1.1442223436898479E272
Loss in iteration 51 : 5.590681813492034E277
Loss in iteration 52 : 2.731612724754021E283
Loss in iteration 53 : 1.3346687089275397E289
Loss in iteration 54 : 6.521204658507048E294
Loss in iteration 55 : 3.1862671173512024E300
Loss in iteration 56 : 1.5568132998049144E306
Loss in iteration 57 : Infinity
Loss in iteration 58 : Infinity
Loss in iteration 59 : Infinity
Loss in iteration 60 : Infinity
Loss in iteration 61 : Infinity
Loss in iteration 62 : Infinity
Loss in iteration 63 : Infinity
Loss in iteration 64 : Infinity
Loss in iteration 65 : Infinity
Loss in iteration 66 : Infinity
Loss in iteration 67 : Infinity
Loss in iteration 68 : Infinity
Loss in iteration 69 : Infinity
Loss in iteration 70 : Infinity
Loss in iteration 71 : Infinity
Loss in iteration 72 : Infinity
Loss in iteration 73 : Infinity
Loss in iteration 74 : Infinity
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 5053 millisecond.
Loss in iteration 1 : 1.0000001999961752
Loss in iteration 2 : 0.9779793673661877
Loss in iteration 3 : 5070.672013200395
Loss in iteration 4 : 8.028383187601839E8
Loss in iteration 5 : 1.2781230242816206E14
Loss in iteration 6 : 2.0347845671962178E19
Loss in iteration 7 : 3.239397378764626E24
Loss in iteration 8 : 5.15715302096598E29
Loss in iteration 9 : 8.210239180908049E34
Loss in iteration 10 : 1.3070782878397422E40
Loss in iteration 11 : 2.080881705023748E45
Loss in iteration 12 : 3.3127844832148573E50
Loss in iteration 13 : 5.273986025122884E55
Loss in iteration 14 : 8.396238491855885E60
Loss in iteration 15 : 1.3366895641419487E66
Loss in iteration 16 : 2.1280231530096231E71
Loss in iteration 17 : 3.387834139822851E76
Loss in iteration 18 : 5.393465828939377E81
Loss in iteration 19 : 8.586451534329776E86
Loss in iteration 20 : 1.3669716707168344E92
Loss in iteration 21 : 2.176232569497908E97
Loss in iteration 22 : 3.4645840129663646E102
Loss in iteration 23 : 5.5156523944825814E107
Loss in iteration 24 : 8.780973768540215E112
Loss in iteration 25 : 1.3979398049253709E118
Loss in iteration 26 : 2.2255341488392397E123
Loss in iteration 27 : 3.543072620293559E128
Loss in iteration 28 : 5.640607042233548E133
Loss in iteration 29 : 8.97990281730623E138
Loss in iteration 30 : 1.429609508417969E144
Loss in iteration 31 : 2.2759526334964913E149
Loss in iteration 32 : 3.6233393520527486E154
Loss in iteration 33 : 5.768392481861498E159
Loss in iteration 34 : 9.183338515048323E164
Loss in iteration 35 : 1.4619966749342077E170
Loss in iteration 36 : 2.327513326462009E175
Loss in iteration 37 : 3.705424490860782E180
Loss in iteration 38 : 5.899072843695273E185
Loss in iteration 39 : 9.391382957891312E190
Loss in iteration 40 : 1.4951175582792553E196
Loss in iteration 41 : 2.3802421039561563E201
Loss in iteration 42 : 3.7893692319192405E206
Loss in iteration 43 : 6.032713710907751E211
Loss in iteration 44 : 9.604140554902248E216
Loss in iteration 45 : 1.5289887804809926E222
Loss in iteration 46 : 2.4341654284135447E227
Loss in iteration 47 : 3.8752157036886483E232
Loss in iteration 48 : 6.169382152429365E237
Loss in iteration 49 : 9.821718080489076E242
Loss in iteration 50 : 1.563627340131941E248
Loss in iteration 51 : 2.489310361763451E253
Loss in iteration 52 : 3.96300698903103E258
Loss in iteration 53 : 6.309146756607293E263
Loss in iteration 54 : 1.0044224727986377E269
Loss in iteration 55 : 1.5990506209201592E274
Loss in iteration 56 : 2.545704579011103E279
Loss in iteration 57 : 4.052787146831466E284
Loss in iteration 58 : 6.452077665627162E289
Loss in iteration 59 : 1.0271772164455098E295
Loss in iteration 60 : 1.6352764003534164E300
Loss in iteration 61 : 2.6033763821266418E305
Loss in iteration 62 : Infinity
Loss in iteration 63 : Infinity
Loss in iteration 64 : Infinity
Loss in iteration 65 : Infinity
Loss in iteration 66 : Infinity
Loss in iteration 67 : Infinity
Loss in iteration 68 : Infinity
Loss in iteration 69 : Infinity
Loss in iteration 70 : Infinity
Loss in iteration 71 : Infinity
Loss in iteration 72 : Infinity
Loss in iteration 73 : Infinity
Loss in iteration 74 : Infinity
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 4446 millisecond.
Loss in iteration 1 : 1.0000001999961752
Loss in iteration 2 : 0.948223053802109
Loss in iteration 3 : 26.141508163881614
Loss in iteration 4 : 184579.03357859785
Loss in iteration 5 : 1.8077482802689342E9
Loss in iteration 6 : 1.771710092998075E13
Loss in iteration 7 : 1.7364529340117104E17
Loss in iteration 8 : 1.701897514352643E21
Loss in iteration 9 : 1.668029753804467E25
Loss in iteration 10 : 1.6348359617031432E29
Loss in iteration 11 : 1.6023027260652496E33
Loss in iteration 12 : 1.5704169018165512E37
Loss in iteration 13 : 1.539165605470402E41
Loss in iteration 14 : 1.508536209921541E45
Loss in iteration 15 : 1.478516339344102E49
Loss in iteration 16 : 1.4490938641911543E53
Loss in iteration 17 : 1.4202568962937502E57
Loss in iteration 18 : 1.391993784057505E61
Loss in iteration 19 : 1.3642931077547605E65
Loss in iteration 20 : 1.3371436749104403E69
Loss in iteration 21 : 1.3105345157797226E73
Loss in iteration 22 : 1.2844548789157066E77
Loss in iteration 23 : 1.2588942268252841E81
Loss in iteration 24 : 1.2338422317114608E85
Loss in iteration 25 : 1.2092887713004027E89
Loss in iteration 26 : 1.185223924751525E93
Loss in iteration 27 : 1.1616379686489694E97
Loss in iteration 28 : 1.138521373072855E101
Loss in iteration 29 : 1.115864797748705E105
Loss in iteration 30 : 1.093659088273506E109
Loss in iteration 31 : 1.071895272416863E113
Loss in iteration 32 : 1.0505645564957675E117
Loss in iteration 33 : 1.0296583218215019E121
Loss in iteration 34 : 1.0091681212172542E125
Loss in iteration 35 : 9.890856756050309E128
Loss in iteration 36 : 9.694028706604906E132
Loss in iteration 37 : 9.501117535343467E136
Loss in iteration 38 : 9.312045296390133E140
Loss in iteration 39 : 9.126735594991967E144
Loss in iteration 40 : 8.945113556651628E148
Loss in iteration 41 : 8.767105796874262E152
Loss in iteration 42 : 8.592640391516464E156
Loss in iteration 43 : 8.421646847725287E160
Loss in iteration 44 : 8.254056075455552E164
Loss in iteration 45 : 8.089800359553986E168
Loss in iteration 46 : 7.928813332398862E172
Loss in iteration 47 : 7.771029947084125E176
Loss in iteration 48 : 7.616386451137151E180
Loss in iteration 49 : 7.464820360759521E184
Loss in iteration 50 : 7.316270435580406E188
Loss in iteration 51 : 7.170676653912356E192
Loss in iteration 52 : 7.0279801884995E196
Loss in iteration 53 : 6.888123382748362E200
Loss in iteration 54 : 6.751049727431666E204
Loss in iteration 55 : 6.616703837855778E208
Loss in iteration 56 : 6.485031431482447E212
Loss in iteration 57 : 6.355979305995949E216
Loss in iteration 58 : 6.229495317806628E220
Loss in iteration 59 : 6.1055283609822755E224
Loss in iteration 60 : 5.9840283465987284E228
Loss in iteration 61 : 5.864946182501413E232
Loss in iteration 62 : 5.748233753469636E236
Loss in iteration 63 : 5.633843901775588E240
Loss in iteration 64 : 5.521730408130257E244
Loss in iteration 65 : 5.411847973008463E248
Loss in iteration 66 : 5.304152198345596E252
Loss in iteration 67 : 5.198599569598518E256
Loss in iteration 68 : 5.095147438163507E260
Loss in iteration 69 : 4.993754004144052E264
Loss in iteration 70 : 4.8943782994615854E268
Loss in iteration 71 : 4.7969801713022996E272
Loss in iteration 72 : 4.701520265893384E276
Loss in iteration 73 : 4.607960012602106E280
Loss in iteration 74 : 4.516261608351325E284
Loss in iteration 75 : 4.426388002345133E288
Loss in iteration 76 : 4.338302881098465E292
Loss in iteration 77 : 4.251970653764606E296
Loss in iteration 78 : 4.167356437754688E300
Loss in iteration 79 : 4.084426044643371E304
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3395 millisecond.
Loss in iteration 1 : 1.0000001999961752
Loss in iteration 2 : 0.9472273845805785
Loss in iteration 3 : 9.680391133534759
Loss in iteration 4 : 21020.96423918197
Loss in iteration 5 : 9.977867852471623E7
Loss in iteration 6 : 4.7494159857679425E11
Loss in iteration 7 : 2.261195520357235E15
Loss in iteration 8 : 1.0765551374006903E19
Loss in iteration 9 : 5.1254790084836414E22
Loss in iteration 10 : 2.440240555915332E26
Loss in iteration 11 : 1.1617985286712574E30
Loss in iteration 12 : 5.531322795003844E33
Loss in iteration 13 : 2.6334627827013307E37
Loss in iteration 14 : 1.2537916308441032E41
Loss in iteration 15 : 5.969301954448776E44
Loss in iteration 16 : 2.8419846605130623E48
Loss in iteration 17 : 1.3530688968702685E52
Loss in iteration 18 : 6.441961017999348E55
Loss in iteration 19 : 3.06701764066949E59
Loss in iteration 20 : 1.4602070987227445E63
Loss in iteration 21 : 6.952045997018985E66
Loss in iteration 22 : 3.309869099180739E70
Loss in iteration 23 : 1.5758286781199498E74
Loss in iteration 24 : 7.502520336529083E77
Loss in iteration 25 : 3.571949932221496E81
Loss in iteration 26 : 1.7006053627306545E85
Loss in iteration 27 : 8.096582131960642E88
Loss in iteration 28 : 3.8547827530264625E92
Loss in iteration 29 : 1.8352620687158985E96
Loss in iteration 30 : 8.737682709156394E99
Loss in iteration 31 : 4.16001073782936E103
Loss in iteration 32 : 1.9805811122805577E107
Loss in iteration 33 : 9.429546675567733E110
Loss in iteration 34 : 4.489407172237798E114
Loss in iteration 35 : 2.1374067547024156E118
Loss in iteration 36 : 1.0176193559138202E122
Loss in iteration 37 : 4.844885753505698E125
Loss in iteration 38 : 2.3066501072440624E129
Loss in iteration 39 : 1.0981961160588983E133
Loss in iteration 40 : 5.228511708556415E136
Loss in iteration 41 : 2.489294424443709E140
Loss in iteration 42 : 1.1851530754776499E144
Loss in iteration 43 : 5.6425137923490916E147
Loss in iteration 44 : 2.6864008165374033E151
Loss in iteration 45 : 1.2789954287534571E155
Loss in iteration 46 : 6.08929723629521E158
Loss in iteration 47 : 2.89911441420015E162
Loss in iteration 48 : 1.3802683726006912E166
Loss in iteration 49 : 6.571457721951892E169
Loss in iteration 50 : 3.128671021421295E173
Loss in iteration 51 : 1.4895602732986788E177
Loss in iteration 52 : 7.091796461175008E180
Loss in iteration 53 : 3.376404295165422E184
Loss in iteration 54 : 1.6075060849282573E188
Loss in iteration 55 : 7.653336470343436E191
Loss in iteration 56 : 3.6437534935305087E195
Loss in iteration 57 : 1.7347910382698753E199
Loss in iteration 58 : 8.259340133202876E202
Loss in iteration 59 : 3.9322718374178895E206
Loss in iteration 60 : 1.8721546217946566E210
Loss in iteration 61 : 8.913328154364362E213
Loss in iteration 62 : 4.243635534292873E217
Loss in iteration 63 : 2.0203948778768366E221
Loss in iteration 64 : 9.619100013571619E224
Loss in iteration 65 : 4.5796535164614484E228
Loss in iteration 66 : 2.1803730391872955E232
Loss in iteration 67 : 1.0380756039570712E236
Loss in iteration 68 : 4.942277950439616E239
Loss in iteration 69 : 2.353018532204302E243
Loss in iteration 70 : 1.120272123182468E247
Loss in iteration 71 : 5.3336155784717296E250
Loss in iteration 72 : 2.53933437691039E254
Loss in iteration 73 : 1.2089770968470369E258
Loss in iteration 74 : 5.7559399580887415E261
Loss in iteration 75 : 2.7404030140460506E265
Loss in iteration 76 : 1.3047058749873246E269
Loss in iteration 77 : 6.211704670814654E272
Loss in iteration 78 : 2.957392593774856E276
Loss in iteration 79 : 1.408014613896209E280
Loss in iteration 80 : 6.703557576759851E283
Loss in iteration 81 : 3.1915637622953636E287
Loss in iteration 82 : 1.5195035072288234E291
Loss in iteration 83 : 7.23435619791643E294
Loss in iteration 84 : 3.444276985828011E298
Loss in iteration 85 : 1.6398202729527162E302
Loss in iteration 86 : 7.807184319527882E305
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 70.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3617 millisecond.
Loss in iteration 1 : 1.0000001999961752
Loss in iteration 2 : 0.9465917084744805
Loss in iteration 3 : 3.6854751841339213
Loss in iteration 4 : 689.6712302477353
Loss in iteration 5 : 1019840.6104484512
Loss in iteration 6 : 1.5452185007801266E9
Loss in iteration 7 : 2.350233817209782E12
Loss in iteration 8 : 3.5746965737250825E15
Loss in iteration 9 : 5.4371134224389939E18
Loss in iteration 10 : 8.269849501746029E21
Loss in iteration 11 : 1.257844109205503E25
Loss in iteration 12 : 1.913180890099473E28
Loss in iteration 13 : 2.9099481338412834E31
Loss in iteration 14 : 4.42603111157259E34
Loss in iteration 15 : 6.731993320701906E37
Loss in iteration 16 : 1.0239361840787604E41
Loss in iteration 17 : 1.5574069359837942E44
Loss in iteration 18 : 2.3688159496313508E47
Loss in iteration 19 : 3.6029690593892857E50
Loss in iteration 20 : 5.480115939331103E53
Loss in iteration 21 : 8.335256343722606E56
Loss in iteration 22 : 1.2677924898802084E60
Loss in iteration 23 : 1.9283123771077974E63
Loss in iteration 24 : 2.9329631255809598E66
Loss in iteration 25 : 4.46103691400864E69
Loss in iteration 26 : 6.7852371462071405E72
Loss in iteration 27 : 1.0320345699381062E76
Loss in iteration 28 : 1.56972458087586E79
Loss in iteration 29 : 2.3875510875121827E82
Loss in iteration 30 : 3.63146520410603E85
Loss in iteration 31 : 5.52345857544527E88
Loss in iteration 32 : 8.401180493252257E91
Loss in iteration 33 : 1.2778195530236683E95
Loss in iteration 34 : 1.9435635401489993E98
Loss in iteration 35 : 2.9561601445666286E101
Loss in iteration 36 : 4.4963195798858414E104
Loss in iteration 37 : 6.838902081006363E107
Loss in iteration 38 : 1.0401970065210679E111
Loss in iteration 39 : 1.5821396469185443E114
Loss in iteration 40 : 2.4064344029631062E117
Loss in iteration 41 : 3.6601867269068847E120
Loss in iteration 42 : 5.56714401162537E123
Loss in iteration 43 : 8.467626041682188E126
Loss in iteration 44 : 1.2879259209398608E130
Loss in iteration 45 : 1.958935325749528E133
Loss in iteration 46 : 2.979540630465032E136
Loss in iteration 47 : 4.531881298937315E139
Loss in iteration 48 : 6.892991455683655E142
Loss in iteration 49 : 1.048424000409484E146
Loss in iteration 50 : 1.594652904622825E149
Loss in iteration 51 : 2.425467067931317E152
Loss in iteration 52 : 3.689135410323533E155
Loss in iteration 53 : 5.611174959102095E158
Loss in iteration 54 : 8.534597112794286E161
Loss in iteration 55 : 1.2981122208560109E165
Loss in iteration 56 : 1.974428687921992E168
Loss in iteration 57 : 3.0031060343293497E171
Loss in iteration 58 : 4.56772427821494E174
Loss in iteration 59 : 6.947508627164926E177
Loss in iteration 60 : 1.0567160621917849E181
Loss in iteration 61 : 1.6072651305937049E184
Loss in iteration 62 : 2.444650263633026E187
Loss in iteration 63 : 3.718313050985831E190
Loss in iteration 64 : 5.655554150549453E193
Loss in iteration 65 : 8.602097862985716E196
Loss in iteration 66 : 1.3083790849601276E200
Loss in iteration 67 : 1.9900445882243533E203
Loss in iteration 68 : 3.0268578186892417E206
Loss in iteration 69 : 4.6038507422263366E209
Loss in iteration 70 : 7.002456978926259E212
Loss in iteration 71 : 1.0650737064946839E216
Loss in iteration 72 : 1.6199771075784143E219
Loss in iteration 73 : 2.463985180626768E222
Loss in iteration 74 : 3.7477214597333134E225
Loss in iteration 75 : 5.7002843402543707E228
Loss in iteration 76 : 8.670132481526899E231
Loss in iteration 77 : 1.3187271504402414E235
Loss in iteration 78 : 2.0057839958196069E238
Loss in iteration 79 : 3.050797457641622E241
Loss in iteration 80 : 4.6402629330729074E244
Loss in iteration 81 : 7.05783992120389E247
Loss in iteration 82 : 1.0734974520151121E251
Loss in iteration 83 : 1.632789624514985E254
Loss in iteration 84 : 2.4834730188872926E257
Loss in iteration 85 : 3.7773624617275726E260
Loss in iteration 86 : 5.745368304287638E263
Loss in iteration 87 : 8.738705190821496E266
Loss in iteration 88 : 1.32915705952395E270
Loss in iteration 89 : 2.0216478875359276E273
Loss in iteration 90 : 3.074926436942145E276
Loss in iteration 91 : 4.676963110589003E279
Loss in iteration 92 : 7.113660891205875E282
Loss in iteration 93 : 1.0819878215524132E286
Loss in iteration 94 : 1.6457034765812207E289
Loss in iteration 95 : 2.503114987880037E292
Loss in iteration 96 : 3.807237896565536E295
Loss in iteration 97 : 5.79080884067618E298
Loss in iteration 98 : 8.80782024666847E301
Loss in iteration 99 : 1.3396694595182744E305
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 40.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3024 millisecond.
Loss in iteration 1 : 1.0000001999961752
Loss in iteration 2 : 0.9463160254838147
Loss in iteration 3 : 1.4292388177223374
Loss in iteration 4 : 1.4897266576311994
Loss in iteration 5 : 52.277607968541886
Loss in iteration 6 : 649.0712273243262
Loss in iteration 7 : 49261.633756494324
Loss in iteration 8 : 3703877.0486791637
Loss in iteration 9 : 2.9974790392361456E8
Loss in iteration 10 : 2.4256392781761276E10
Loss in iteration 11 : 1.964746260459029E12
Loss in iteration 12 : 1.5914256891892788E14
Loss in iteration 13 : 1.2890546336491956E16
Loss in iteration 14 : 1.04413410112341197E18
Loss in iteration 15 : 8.457486204957519E19
Loss in iteration 16 : 6.850563813692864E21
Loss in iteration 17 : 5.548956688976668E23
Loss in iteration 18 : 4.494654917971287E25
Loss in iteration 19 : 3.640670483555814E27
Loss in iteration 20 : 2.9489430916794003E29
Loss in iteration 21 : 2.3886439042603076E31
Loss in iteration 22 : 1.9348015624508426E33
Loss in iteration 23 : 1.567189265585182E35
Loss in iteration 24 : 1.2694233051239978E37
Loss in iteration 25 : 1.0282328771504383E39
Loss in iteration 26 : 8.328686304918548E40
Loss in iteration 27 : 6.746235906984024E42
Loss in iteration 28 : 5.464451084657058E44
Loss in iteration 29 : 4.426205378572219E46
Loss in iteration 30 : 3.585226356643497E48
Loss in iteration 31 : 2.904033348881232E50
Loss in iteration 32 : 2.3522670125937985E52
Loss in iteration 33 : 1.9053362802009767E54
Loss in iteration 34 : 1.5433223869627914E56
Loss in iteration 35 : 1.2500911334398609E58
Loss in iteration 36 : 1.0125738180862874E60
Loss in iteration 37 : 8.201847926498927E61
Loss in iteration 38 : 6.643496820464132E63
Loss in iteration 39 : 5.3812324245759455E65
Loss in iteration 40 : 4.358798263906516E67
Loss in iteration 41 : 3.530626593764278E69
Loss in iteration 42 : 2.8598075409490654E71
Loss in iteration 43 : 2.3164441081687422E73
Loss in iteration 44 : 1.876319727616682E75
Loss in iteration 45 : 1.5198189793695122E77
Loss in iteration 46 : 1.2310533732893047E79
Loss in iteration 47 : 9.971532323643372E80
Loss in iteration 48 : 8.076941182151128E82
Loss in iteration 49 : 6.542322357542414E84
Loss in iteration 50 : 5.299281109609354E86
Loss in iteration 51 : 4.292417698783578E88
Loss in iteration 52 : 3.476858336014697E90
Loss in iteration 53 : 2.8162552521719054E92
Loss in iteration 54 : 2.281166754259243E94
Loss in iteration 55 : 1.8477450709499865E96
Loss in iteration 56 : 1.4966735074694894E98
Loss in iteration 57 : 1.212305541050286E100
Loss in iteration 58 : 9.819674882507319E101
Loss in iteration 59 : 7.953936654830929E103
Loss in iteration 60 : 6.442688690413053E105
Loss in iteration 61 : 5.218577839234573E107
Loss in iteration 62 : 4.2270480497800034E109
Loss in iteration 63 : 3.423908920321803E111
Loss in iteration 64 : 2.77336622546066E113
Loss in iteration 65 : 2.246426642623135E115
Loss in iteration 66 : 1.8196055805247395E117
Loss in iteration 67 : 1.4738805202250388E119
Loss in iteration 68 : 1.1938432213822818E121
Loss in iteration 69 : 9.670130093196481E122
Loss in iteration 70 : 7.832805375489148E124
Loss in iteration 71 : 6.344572354146211E126
Loss in iteration 72 : 5.139103606858431E128
Loss in iteration 73 : 4.162673921555328E130
Loss in iteration 74 : 3.3717658764598164E132
Loss in iteration 75 : 2.731130359932451E134
Loss in iteration 76 : 2.212215591545286E136
Loss in iteration 77 : 1.7918946291516813E138
Loss in iteration 78 : 1.4514346496128618E140
Loss in iteration 79 : 1.1756620661864183E142
Loss in iteration 80 : 9.522862736109987E143
Loss in iteration 81 : 7.713518816249088E145
Loss in iteration 82 : 6.247950241161762E147
Loss in iteration 83 : 5.060839695341028E149
Loss in iteration 84 : 4.0992801532262324E151
Loss in iteration 85 : 3.3204169241132484E153
Loss in iteration 86 : 2.689537708531731E155
Loss in iteration 87 : 2.1785255439107026E157
Loss in iteration 88 : 1.7646056905676694E159
Loss in iteration 89 : 1.429330609359812E161
Loss in iteration 90 : 1.1577577935814473E163
Loss in iteration 91 : 9.377838128009723E164
Loss in iteration 92 : 7.596048883687876E166
Loss in iteration 93 : 6.152799595787182E168
Loss in iteration 94 : 4.983767672587615E170
Loss in iteration 95 : 4.036851814795969E172
Loss in iteration 96 : 3.2698499699847355E174
Loss in iteration 97 : 2.648578475687635E176
Loss in iteration 98 : 2.1453485653069843E178
Loss in iteration 99 : 1.7377323378986572E180
Loss in iteration 100 : 1.4075631936979123E182
Loss in iteration 101 : 1.140126186895309E184
Loss in iteration 102 : 9.235022113852005E185
Loss in iteration 103 : 7.480367912220122E187
Loss in iteration 104 : 6.0590980088983E189
Loss in iteration 105 : 4.907869387207623E191
Loss in iteration 106 : 3.975374203638175E193
Loss in iteration 107 : 3.220053104946921E195
Loss in iteration 108 : 2.6082430150070068E197
Loss in iteration 109 : 2.1126768421556755E199
Loss in iteration 110 : 1.7112682421460976E201
Loss in iteration 111 : 1.3861272761383387E203
Loss in iteration 112 : 1.1227630936720542E205
Loss in iteration 113 : 9.094381058743639E206
Loss in iteration 114 : 7.3664486575823495E208
Loss in iteration 115 : 5.966823412641703E210
Loss in iteration 116 : 4.833126964239779E212
Loss in iteration 117 : 3.91483284103422E214
Loss in iteration 118 : 3.171014601237719E216
Loss in iteration 119 : 2.5685218270025522E218
Loss in iteration 120 : 2.0805026798720677E220
Loss in iteration 121 : 1.685207170696374E222
Loss in iteration 122 : 1.3650178082640634E224
Loss in iteration 123 : 1.1056644246938914E226
Loss in iteration 124 : 8.955881840020522E227
Loss in iteration 125 : 7.254264290416622E229
Loss in iteration 126 : 5.8759540752374625E231
Loss in iteration 127 : 4.759522800942345E233
Loss in iteration 128 : 3.8552134687632997E235
Loss in iteration 129 : 3.122722909698273E237
Loss in iteration 130 : 2.529405556855601E239
Loss in iteration 131 : 2.048818501053037E241
Loss in iteration 132 : 1.6595429858529603E243
Loss in iteration 133 : 1.3442298185408975E245
Loss in iteration 134 : 1.088826153018127E247
Loss in iteration 135 : 8.819491839446829E248
Loss in iteration 136 : 7.143788389951932E250
Loss in iteration 137 : 5.786468595861064E252
Loss in iteration 138 : 4.6870395626474615E254
Loss in iteration 139 : 3.796502045744444E256
Loss in iteration 140 : 3.0751666570529997E258
Loss in iteration 141 : 2.49088499221293E260
Loss in iteration 142 : 2.0176168436924737E262
Loss in iteration 143 : 1.6342696433909034E264
Loss in iteration 144 : 1.3237584111466314E266
Loss in iteration 145 : 1.0722443130287718E268
Loss in iteration 146 : 8.685178935533051E269
Loss in iteration 147 : 7.03499493778177E271
Loss in iteration 148 : 5.698345899603236E273
Loss in iteration 149 : 4.6156601786786196E275
Loss in iteration 150 : 3.7386847447296824E277
Loss in iteration 151 : 3.028334643231043E279
Loss in iteration 152 : 2.452951061017145E281
Loss in iteration 153 : 1.9868903594238865E283
Loss in iteration 154 : 1.6093811911333486E285
Loss in iteration 155 : 1.3035987648180124E287
Loss in iteration 156 : 1.05591499950259E289
Loss in iteration 157 : 8.55291149597098E290
Loss in iteration 158 : 6.927858311736493E292
Loss in iteration 159 : 5.61156523250656E294
Loss in iteration 160 : 4.5453678383303136E296
Loss in iteration 161 : 3.6817479490475545E298
Loss in iteration 162 : 2.9822158387285187E300
Loss in iteration 163 : 2.4155948293701003E302
Loss in iteration 164 : 1.9566318117897817E304
Loss in iteration 165 : 1.5848717675497226E306
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3477 millisecond.
Loss in iteration 1 : 1.0235293500208278
Loss in iteration 2 : 5.333448884279096
Loss in iteration 3 : 3.204433931407522
Loss in iteration 4 : 1.2771860850668746
Loss in iteration 5 : 0.771161147655971
Loss in iteration 6 : 1.2244966192737539
Loss in iteration 7 : 0.7253545507264225
Loss in iteration 8 : 1.0231107582377739
Loss in iteration 9 : 0.8278574677501432
Loss in iteration 10 : 1.168093236794557
Loss in iteration 11 : 0.6913477470059297
Loss in iteration 12 : 0.8541982588957145
Loss in iteration 13 : 0.8059337056456469
Loss in iteration 14 : 1.138067856011982
Loss in iteration 15 : 0.7244920857269299
Loss in iteration 16 : 0.9480003569171908
Loss in iteration 17 : 0.795816494977461
Loss in iteration 18 : 1.106099603807362
Loss in iteration 19 : 0.7490252393908425
Loss in iteration 20 : 0.9958280112713687
Loss in iteration 21 : 0.7957170420602743
Loss in iteration 22 : 1.0922574145014528
Loss in iteration 23 : 0.7682818827732042
Loss in iteration 24 : 1.0365620265009081
Loss in iteration 25 : 0.7962816302259039
Loss in iteration 26 : 1.1028117671811644
Loss in iteration 27 : 0.7819721571161692
Loss in iteration 28 : 1.0651582652903153
Loss in iteration 29 : 0.8041197098415832
Loss in iteration 30 : 1.1181437574689463
Loss in iteration 31 : 0.7819745897463733
Loss in iteration 32 : 1.076182947024297
Loss in iteration 33 : 0.8179057726800403
Loss in iteration 34 : 1.152400941656403
Loss in iteration 35 : 0.7783128154865535
Loss in iteration 36 : 1.0585379094298812
Loss in iteration 37 : 0.8314629290620074
Loss in iteration 38 : 1.1850944807395212
Loss in iteration 39 : 0.7929992003620855
Loss in iteration 40 : 1.080514883971098
Loss in iteration 41 : 0.8345405618535181
Loss in iteration 42 : 1.1842588355039598
Loss in iteration 43 : 0.7926203256505546
Loss in iteration 44 : 1.071626122679041
Loss in iteration 45 : 0.8476138757590196
Loss in iteration 46 : 1.210836013881826
Loss in iteration 47 : 0.8006387164671204
Loss in iteration 48 : 1.08509896044823
Loss in iteration 49 : 0.8473195624448318
Loss in iteration 50 : 1.2103618840607875
Loss in iteration 51 : 0.802370832308373
Loss in iteration 52 : 1.0817996178492884
Loss in iteration 53 : 0.8582872576794404
Loss in iteration 54 : 1.2286167015124436
Loss in iteration 55 : 0.8085360318878801
Loss in iteration 56 : 1.091577677011227
Loss in iteration 57 : 0.8575494564757089
Loss in iteration 58 : 1.2272885562631821
Loss in iteration 59 : 0.8120483978698271
Loss in iteration 60 : 1.0965206445727627
Loss in iteration 61 : 0.862095269098287
Loss in iteration 62 : 1.2321120467549747
Loss in iteration 63 : 0.8142432355107996
Loss in iteration 64 : 1.0974522898979777
Loss in iteration 65 : 0.8697040841541301
Loss in iteration 66 : 1.2437939524984247
Loss in iteration 67 : 0.8142749932559091
Loss in iteration 68 : 1.0820324430923351
Loss in iteration 69 : 0.8758664039333491
Loss in iteration 70 : 1.251990711597128
Loss in iteration 71 : 0.8184984784839313
Loss in iteration 72 : 1.0890361591443636
Loss in iteration 73 : 0.8765432504102795
Loss in iteration 74 : 1.2570890981480531
Loss in iteration 75 : 0.819242282919246
Loss in iteration 76 : 1.0913445159442872
Loss in iteration 77 : 0.8795838345762372
Loss in iteration 78 : 1.2626965269428332
Loss in iteration 79 : 0.8199587142626057
Loss in iteration 80 : 1.092518877701749
Loss in iteration 81 : 0.8824749378407393
Loss in iteration 82 : 1.2636140373277842
Loss in iteration 83 : 0.8210341368839497
Loss in iteration 84 : 1.0903519924368434
Loss in iteration 85 : 0.8853669686777002
Loss in iteration 86 : 1.2656586702307109
Loss in iteration 87 : 0.8232806423217636
Loss in iteration 88 : 1.0967556593116043
Loss in iteration 89 : 0.8855807947856906
Loss in iteration 90 : 1.267747892793818
Loss in iteration 91 : 0.8237581953240232
Loss in iteration 92 : 1.0940103656733522
Loss in iteration 93 : 0.8901618733286483
Loss in iteration 94 : 1.270488992358277
Loss in iteration 95 : 0.8254380083899249
Loss in iteration 96 : 1.094732491527804
Loss in iteration 97 : 0.8894461289780438
Loss in iteration 98 : 1.2697826520239825
Loss in iteration 99 : 0.8269724950511347
Loss in iteration 100 : 1.1005055478229493
Loss in iteration 101 : 0.8889695464295382
Loss in iteration 102 : 1.2712539466703296
Loss in iteration 103 : 0.826878564191726
Loss in iteration 104 : 1.0971725614674503
Loss in iteration 105 : 0.8907195608376675
Loss in iteration 106 : 1.272141173588866
Loss in iteration 107 : 0.8281998076753139
Loss in iteration 108 : 1.1026171332753487
Loss in iteration 109 : 0.8900392801120274
Loss in iteration 110 : 1.2732799736611962
Loss in iteration 111 : 0.827931823117688
Loss in iteration 112 : 1.0989989861619969
Loss in iteration 113 : 0.8916265652838553
Loss in iteration 114 : 1.2738875194151813
Loss in iteration 115 : 0.8291046716938382
Loss in iteration 116 : 1.104201968230932
Loss in iteration 117 : 0.8908123781011509
Loss in iteration 118 : 1.2747884226437503
Loss in iteration 119 : 0.8287140224810398
Loss in iteration 120 : 1.100433006513579
Loss in iteration 121 : 0.8945284002341329
Loss in iteration 122 : 1.2742641672626116
Loss in iteration 123 : 0.8301925545612242
Loss in iteration 124 : 1.1024180768555283
Loss in iteration 125 : 0.8946381246992567
Loss in iteration 126 : 1.2742865910039092
Loss in iteration 127 : 0.830448093233668
Loss in iteration 128 : 1.1000649049022062
Loss in iteration 129 : 0.8940469458217333
Loss in iteration 130 : 1.2747824601018787
Loss in iteration 131 : 0.8312944364150314
Loss in iteration 132 : 1.1051390621381123
Loss in iteration 133 : 0.892916480339873
Loss in iteration 134 : 1.2755794237619176
Loss in iteration 135 : 0.8306165872814462
Loss in iteration 136 : 1.1011998697210528
Loss in iteration 137 : 0.8941193742675738
Loss in iteration 138 : 1.2758899300307591
Loss in iteration 139 : 0.8314360766516528
Loss in iteration 140 : 1.1061187850801462
Loss in iteration 141 : 0.8928967873197639
Loss in iteration 142 : 1.2745942112337554
Loss in iteration 143 : 0.832435606749653
Loss in iteration 144 : 1.105094411762968
Loss in iteration 145 : 0.8940282654900646
Loss in iteration 146 : 1.2754470697421467
Loss in iteration 147 : 0.8316133236710714
Loss in iteration 148 : 1.1011852256061525
Loss in iteration 149 : 0.8950917207809463
Loss in iteration 150 : 1.2757955291212464
Loss in iteration 151 : 0.8323075219775333
Loss in iteration 152 : 1.1061172253796734
Loss in iteration 153 : 0.8938228476868679
Loss in iteration 154 : 1.2764628104099702
Loss in iteration 155 : 0.831498790485157
Loss in iteration 156 : 1.1020547409328314
Loss in iteration 157 : 0.894906569252836
Loss in iteration 158 : 1.276660040953867
Loss in iteration 159 : 0.832222776297014
Loss in iteration 160 : 1.1045994863718396
Loss in iteration 161 : 0.8965104419083303
Loss in iteration 162 : 1.2763358753311036
Loss in iteration 163 : 0.8321976030836709
Loss in iteration 164 : 1.1019452893251427
Loss in iteration 165 : 0.8956524689910009
Loss in iteration 166 : 1.2765430036796133
Loss in iteration 167 : 0.8328051752073627
Loss in iteration 168 : 1.106764563568629
Loss in iteration 169 : 0.8942974302103813
Loss in iteration 170 : 1.2770959819242456
Loss in iteration 171 : 0.831924016967576
Loss in iteration 172 : 1.1026066662272096
Loss in iteration 173 : 0.8953110046632721
Loss in iteration 174 : 1.277199786497302
Loss in iteration 175 : 0.8325880306995023
Loss in iteration 176 : 1.105073903937476
Loss in iteration 177 : 0.8968566532632489
Loss in iteration 178 : 1.2767979805981504
Loss in iteration 179 : 0.832512119712685
Loss in iteration 180 : 1.1023528847045772
Loss in iteration 181 : 0.8959499676704203
Loss in iteration 182 : 1.2769402358654685
Loss in iteration 183 : 0.8330768616836874
Loss in iteration 184 : 1.1071168799543984
Loss in iteration 185 : 0.8945537305355105
Loss in iteration 186 : 1.2774383157489095
Loss in iteration 187 : 0.8321588652267323
Loss in iteration 188 : 1.102910753199903
Loss in iteration 189 : 0.8955324196391811
Loss in iteration 190 : 1.2774955363366267
Loss in iteration 191 : 0.8327718726789166
Loss in iteration 192 : 1.1076017779973444
Loss in iteration 193 : 0.894118234750193
Loss in iteration 194 : 1.2758595343350307
Loss in iteration 195 : 0.831639944523836
Loss in iteration 196 : 1.1037421682158541
Loss in iteration 197 : 0.899059077015874
Loss in iteration 198 : 1.2776748180685504
Loss in iteration 199 : 0.8321054337072791
Loss in iteration 200 : 1.1031071355589979
Testing accuracy  of updater 5 on alg 1 with rate 0.0343 = 0.7765, training accuracy 0.7630301068306895, time elapsed: 3461 millisecond.
Loss in iteration 1 : 1.0065137691950432
Loss in iteration 2 : 2.8413601460116955
Loss in iteration 3 : 1.8186560229808015
Loss in iteration 4 : 0.8253511842210653
Loss in iteration 5 : 0.526112914293536
Loss in iteration 6 : 0.6754725822823879
Loss in iteration 7 : 0.6383330793423485
Loss in iteration 8 : 0.8146181809657574
Loss in iteration 9 : 0.5251274880008492
Loss in iteration 10 : 0.6170788708579868
Loss in iteration 11 : 0.6153317436843044
Loss in iteration 12 : 0.771396576782241
Loss in iteration 13 : 0.5653583118597381
Loss in iteration 14 : 0.667577518806009
Loss in iteration 15 : 0.6079073544292057
Loss in iteration 16 : 0.749876823158273
Loss in iteration 17 : 0.5889973222815849
Loss in iteration 18 : 0.7035604292259456
Loss in iteration 19 : 0.6085221580250723
Loss in iteration 20 : 0.7352135700364427
Loss in iteration 21 : 0.6156201764025961
Loss in iteration 22 : 0.749501119613952
Loss in iteration 23 : 0.6204733642844726
Loss in iteration 24 : 0.7585335392779973
Loss in iteration 25 : 0.6259741452307708
Loss in iteration 26 : 0.7683436778142329
Loss in iteration 27 : 0.6297163258085267
Loss in iteration 28 : 0.7698540929316318
Loss in iteration 29 : 0.6379431569152373
Loss in iteration 30 : 0.7874233131082566
Loss in iteration 31 : 0.6418462147429975
Loss in iteration 32 : 0.790225045903102
Loss in iteration 33 : 0.6493890570936147
Loss in iteration 34 : 0.8041268195747615
Loss in iteration 35 : 0.6557761971538066
Loss in iteration 36 : 0.8138987334695099
Loss in iteration 37 : 0.6580252195630503
Loss in iteration 38 : 0.8216292842246172
Loss in iteration 39 : 0.6608061205867005
Loss in iteration 40 : 0.8250421862640733
Loss in iteration 41 : 0.6669481236032915
Loss in iteration 42 : 0.8373710803325948
Loss in iteration 43 : 0.6721020019800248
Loss in iteration 44 : 0.842861925408334
Loss in iteration 45 : 0.6757415837191192
Loss in iteration 46 : 0.8461787499682234
Loss in iteration 47 : 0.6793113419601785
Loss in iteration 48 : 0.8524428610381413
Loss in iteration 49 : 0.6853131794804961
Loss in iteration 50 : 0.8610606259836066
Loss in iteration 51 : 0.6890532869035138
Loss in iteration 52 : 0.8639109281566063
Loss in iteration 53 : 0.6914192422758216
Loss in iteration 54 : 0.8632791312243117
Loss in iteration 55 : 0.6916333227599777
Loss in iteration 56 : 0.862110126419735
Loss in iteration 57 : 0.6965573148161122
Loss in iteration 58 : 0.881831369686133
Loss in iteration 59 : 0.7075760158220067
Loss in iteration 60 : 0.8893160679817509
Loss in iteration 61 : 0.7048149120011266
Loss in iteration 62 : 0.8780380426736569
Loss in iteration 63 : 0.7015276258861685
Loss in iteration 64 : 0.8653026182584207
Loss in iteration 65 : 0.7206464524858932
Loss in iteration 66 : 0.9132113011826954
Loss in iteration 67 : 0.7176196256595959
Loss in iteration 68 : 0.9007690630883479
Loss in iteration 69 : 0.7150221269337248
Loss in iteration 70 : 0.874442841248519
Loss in iteration 71 : 0.7244805105348837
Loss in iteration 72 : 0.9106108028932269
Loss in iteration 73 : 0.7198539359449461
Loss in iteration 74 : 0.8851546584892371
Loss in iteration 75 : 0.7240034064421396
Loss in iteration 76 : 0.905848287492412
Loss in iteration 77 : 0.7294089946649244
Loss in iteration 78 : 0.9145775191490912
Loss in iteration 79 : 0.7267320234082564
Loss in iteration 80 : 0.8913975892468404
Loss in iteration 81 : 0.7304080044673544
Loss in iteration 82 : 0.9131975265519604
Loss in iteration 83 : 0.73370641002274
Loss in iteration 84 : 0.9164369218626692
Loss in iteration 85 : 0.7307973557631533
Loss in iteration 86 : 0.8944975445976362
Loss in iteration 87 : 0.7390228005488427
Loss in iteration 88 : 0.9282854933549727
Loss in iteration 89 : 0.7348796307815935
Loss in iteration 90 : 0.8982373594808134
Loss in iteration 91 : 0.7419563777302842
Loss in iteration 92 : 0.9298237031260334
Loss in iteration 93 : 0.7386470138276677
Loss in iteration 94 : 0.9133451687347153
Loss in iteration 95 : 0.7370567698668301
Loss in iteration 96 : 0.9051355911433704
Loss in iteration 97 : 0.7447770173229215
Loss in iteration 98 : 0.9357723774753091
Loss in iteration 99 : 0.7431888964118858
Loss in iteration 100 : 0.9237694774901446
Loss in iteration 101 : 0.7435518028616325
Loss in iteration 102 : 0.918615375104173
Loss in iteration 103 : 0.744046141939537
Loss in iteration 104 : 0.9236980646603339
Loss in iteration 105 : 0.7436478224068763
Loss in iteration 106 : 0.9206011195480034
Loss in iteration 107 : 0.7468660766396485
Loss in iteration 108 : 0.9320625167577428
Loss in iteration 109 : 0.7472363221694306
Loss in iteration 110 : 0.9270610782498634
Loss in iteration 111 : 0.7478706667612598
Loss in iteration 112 : 0.9260127907538598
Loss in iteration 113 : 0.7460234683455107
Loss in iteration 114 : 0.9185176392402702
Loss in iteration 115 : 0.7517395095400531
Loss in iteration 116 : 0.941094982653186
Loss in iteration 117 : 0.7515523343420689
Loss in iteration 118 : 0.9298666605898732
Loss in iteration 119 : 0.7464786777814002
Loss in iteration 120 : 0.9168773937096443
Loss in iteration 121 : 0.7536703767844898
Loss in iteration 122 : 0.9446215431563828
Loss in iteration 123 : 0.7532564411244383
Loss in iteration 124 : 0.9301548166615323
Loss in iteration 125 : 0.7511293311998485
Loss in iteration 126 : 0.9259709670008038
Loss in iteration 127 : 0.7537173920034462
Loss in iteration 128 : 0.936737606646205
Loss in iteration 129 : 0.755919526670179
Loss in iteration 130 : 0.9385790508811145
Loss in iteration 131 : 0.753487224439937
Loss in iteration 132 : 0.9303401407083176
Loss in iteration 133 : 0.7534921667828741
Loss in iteration 134 : 0.9319289364988546
Loss in iteration 135 : 0.7545860870153654
Loss in iteration 136 : 0.9336799560220799
Loss in iteration 137 : 0.7553022543433536
Loss in iteration 138 : 0.9354960440785578
Loss in iteration 139 : 0.75867669036905
Loss in iteration 140 : 0.9456686104745637
Loss in iteration 141 : 0.7541066647323995
Loss in iteration 142 : 0.9213206432032337
Loss in iteration 143 : 0.758872938150359
Loss in iteration 144 : 0.9429528159535046
Loss in iteration 145 : 0.7577178456789384
Loss in iteration 146 : 0.9343203048712982
Loss in iteration 147 : 0.756950776326025
Loss in iteration 148 : 0.9359127666079066
Loss in iteration 149 : 0.7561778522205772
Loss in iteration 150 : 0.935900092447753
Loss in iteration 151 : 0.7571202014159404
Loss in iteration 152 : 0.9375506545521479
Loss in iteration 153 : 0.7619553808286706
Loss in iteration 154 : 0.9504196534678472
Loss in iteration 155 : 0.7568970636836094
Loss in iteration 156 : 0.9243887028901809
Loss in iteration 157 : 0.7597874675384324
Loss in iteration 158 : 0.9435575911014998
Loss in iteration 159 : 0.7596329403239149
Loss in iteration 160 : 0.936459220149789
Loss in iteration 161 : 0.7587734477976471
Loss in iteration 162 : 0.9379482300284964
Loss in iteration 163 : 0.7579129399056599
Loss in iteration 164 : 0.9378892734624809
Loss in iteration 165 : 0.7600989256762265
Loss in iteration 166 : 0.9412603847892653
Loss in iteration 167 : 0.7623584811451432
Loss in iteration 168 : 0.9466891179306238
Loss in iteration 169 : 0.7582437647322727
Loss in iteration 170 : 0.9284803514719809
Loss in iteration 171 : 0.7610070758590207
Loss in iteration 172 : 0.9456046949047738
Loss in iteration 173 : 0.7599262199160542
Loss in iteration 174 : 0.9369067986570596
Loss in iteration 175 : 0.7607229065826401
Loss in iteration 176 : 0.938384327360408
Loss in iteration 177 : 0.7598090378624435
Loss in iteration 178 : 0.9399403134742373
Loss in iteration 179 : 0.7644787376808273
Loss in iteration 180 : 0.9503058612537026
Loss in iteration 181 : 0.7590534995828973
Loss in iteration 182 : 0.929266180031718
Loss in iteration 183 : 0.7617306131408427
Loss in iteration 184 : 0.9449139267751282
Loss in iteration 185 : 0.7621897846206955
Loss in iteration 186 : 0.9417991846826967
Loss in iteration 187 : 0.7609534139851705
Loss in iteration 188 : 0.9371217713774811
Loss in iteration 189 : 0.7617284725563285
Loss in iteration 190 : 0.938615173314613
Loss in iteration 191 : 0.7607900697316354
Loss in iteration 192 : 0.9400699668956831
Loss in iteration 193 : 0.7611812434352696
Loss in iteration 194 : 0.9400443674241816
Loss in iteration 195 : 0.7615754672659079
Loss in iteration 196 : 0.9433394218295189
Loss in iteration 197 : 0.7637525089787244
Loss in iteration 198 : 0.9464252347586372
Loss in iteration 199 : 0.7604598289578365
Loss in iteration 200 : 0.9356187635888231
Testing accuracy  of updater 5 on alg 1 with rate 0.02401 = 0.785, training accuracy 0.7831013272903853, time elapsed: 3027 millisecond.
Loss in iteration 1 : 1.0022066559926377
Loss in iteration 2 : 1.8118298166298554
Loss in iteration 3 : 1.2232632776932777
Loss in iteration 4 : 0.6482572621499769
Loss in iteration 5 : 0.45950506802767305
Loss in iteration 6 : 0.5709499709968696
Loss in iteration 7 : 0.5021989728351807
Loss in iteration 8 : 0.5873471283952751
Loss in iteration 9 : 0.4715694757228812
Loss in iteration 10 : 0.5274185596053454
Loss in iteration 11 : 0.48186902340732096
Loss in iteration 12 : 0.5250539203884196
Loss in iteration 13 : 0.47660216177106346
Loss in iteration 14 : 0.5035401653032282
Loss in iteration 15 : 0.477601242341465
Loss in iteration 16 : 0.5008760837771871
Loss in iteration 17 : 0.4806080933268189
Loss in iteration 18 : 0.5122962939495715
Loss in iteration 19 : 0.4848333932010751
Loss in iteration 20 : 0.5226467364200749
Loss in iteration 21 : 0.4848127651469574
Loss in iteration 22 : 0.5251717259396531
Loss in iteration 23 : 0.48538791338612325
Loss in iteration 24 : 0.5308811386585267
Loss in iteration 25 : 0.4890167131372968
Loss in iteration 26 : 0.5359019512092801
Loss in iteration 27 : 0.4913079490338538
Loss in iteration 28 : 0.5391145658126745
Loss in iteration 29 : 0.49592568177230145
Loss in iteration 30 : 0.5487064245856941
Loss in iteration 31 : 0.5013800938146737
Loss in iteration 32 : 0.5579614640639646
Loss in iteration 33 : 0.5074869766616246
Loss in iteration 34 : 0.5663878807157484
Loss in iteration 35 : 0.5116023379837209
Loss in iteration 36 : 0.5683571743547697
Loss in iteration 37 : 0.5132606593701735
Loss in iteration 38 : 0.5690446615459549
Loss in iteration 39 : 0.5175688184039223
Loss in iteration 40 : 0.5758837277938392
Loss in iteration 41 : 0.522327734121927
Loss in iteration 42 : 0.583019110057654
Loss in iteration 43 : 0.5278583546409608
Loss in iteration 44 : 0.5880367011529817
Loss in iteration 45 : 0.5302892464383842
Loss in iteration 46 : 0.5915774034170416
Loss in iteration 47 : 0.5339934013303669
Loss in iteration 48 : 0.5917689837933942
Loss in iteration 49 : 0.5372034743398612
Loss in iteration 50 : 0.5958955023912689
Loss in iteration 51 : 0.5391284510818136
Loss in iteration 52 : 0.5975271946318027
Loss in iteration 53 : 0.5415523373375561
Loss in iteration 54 : 0.6005580779838271
Loss in iteration 55 : 0.5463360040010943
Loss in iteration 56 : 0.6084909982946312
Loss in iteration 57 : 0.5517814422444217
Loss in iteration 58 : 0.6133520872937884
Loss in iteration 59 : 0.5536995501760137
Loss in iteration 60 : 0.6116126645527231
Loss in iteration 61 : 0.5516241139416678
Loss in iteration 62 : 0.6145486338047709
Loss in iteration 63 : 0.5570990770343589
Loss in iteration 64 : 0.6227444907045188
Loss in iteration 65 : 0.5623958439708165
Loss in iteration 66 : 0.6274351519055943
Loss in iteration 67 : 0.5657900288673345
Loss in iteration 68 : 0.6279025066250742
Loss in iteration 69 : 0.5660363851691499
Loss in iteration 70 : 0.6285582814940347
Loss in iteration 71 : 0.5705737026126085
Loss in iteration 72 : 0.6354831894469816
Loss in iteration 73 : 0.5736036769567069
Loss in iteration 74 : 0.637899170018702
Loss in iteration 75 : 0.5762827770862047
Loss in iteration 76 : 0.64151299068644
Loss in iteration 77 : 0.5788956984418507
Loss in iteration 78 : 0.6450603747219662
Loss in iteration 79 : 0.5820227413166336
Loss in iteration 80 : 0.6484692751845256
Loss in iteration 81 : 0.5843964266557173
Loss in iteration 82 : 0.6518020072277823
Loss in iteration 83 : 0.5873106085207404
Loss in iteration 84 : 0.6546046817394976
Loss in iteration 85 : 0.5899294658085726
Loss in iteration 86 : 0.6573701794980893
Loss in iteration 87 : 0.5924840002022406
Loss in iteration 88 : 0.6587867662436823
Loss in iteration 89 : 0.5940361626906937
Loss in iteration 90 : 0.6602252584671268
Loss in iteration 91 : 0.5975663462069886
Loss in iteration 92 : 0.6647402570816383
Loss in iteration 93 : 0.6007726073969543
Loss in iteration 94 : 0.6699542399058687
Loss in iteration 95 : 0.6017767489998402
Loss in iteration 96 : 0.6666258961512453
Loss in iteration 97 : 0.6011707465949367
Loss in iteration 98 : 0.6664378845226144
Loss in iteration 99 : 0.6072536083584245
Loss in iteration 100 : 0.6809033221367848
Loss in iteration 101 : 0.6132895578707023
Loss in iteration 102 : 0.6829400192744304
Loss in iteration 103 : 0.6069239903329626
Loss in iteration 104 : 0.6672859995379862
Loss in iteration 105 : 0.611288703234289
Loss in iteration 106 : 0.6773285421296651
Loss in iteration 107 : 0.6156547467556641
Loss in iteration 108 : 0.6905942134365068
Loss in iteration 109 : 0.6240674097985293
Loss in iteration 110 : 0.6902248847130011
Loss in iteration 111 : 0.6146600224795289
Loss in iteration 112 : 0.6735591604165697
Loss in iteration 113 : 0.6168846687228092
Loss in iteration 114 : 0.6842207278666119
Loss in iteration 115 : 0.6222967954277538
Loss in iteration 116 : 0.6956160318921173
Loss in iteration 117 : 0.6262425150215312
Loss in iteration 118 : 0.6987858671838441
Loss in iteration 119 : 0.6253643426947283
Loss in iteration 120 : 0.6923931997455695
Loss in iteration 121 : 0.6221957895856822
Loss in iteration 122 : 0.6838589607778764
Loss in iteration 123 : 0.6253379835287202
Loss in iteration 124 : 0.6944743829461524
Loss in iteration 125 : 0.6343910491655562
Loss in iteration 126 : 0.7080461946528953
Loss in iteration 127 : 0.6384439556066985
Loss in iteration 128 : 0.7053384462165466
Loss in iteration 129 : 0.6301215600359548
Loss in iteration 130 : 0.6895064797534662
Loss in iteration 131 : 0.6299163360528197
Loss in iteration 132 : 0.6952160737174964
Loss in iteration 133 : 0.634849080766584
Loss in iteration 134 : 0.7096286832422403
Loss in iteration 135 : 0.6406759856335065
Loss in iteration 136 : 0.7125174187220513
Loss in iteration 137 : 0.6401481448264229
Loss in iteration 138 : 0.7076534484302175
Loss in iteration 139 : 0.6355960081621218
Loss in iteration 140 : 0.6973726397486604
Loss in iteration 141 : 0.6362322308314755
Loss in iteration 142 : 0.7034220400996106
Loss in iteration 143 : 0.6442790867234787
Loss in iteration 144 : 0.7205504709672015
Loss in iteration 145 : 0.6531109983600738
Loss in iteration 146 : 0.724064590509673
Loss in iteration 147 : 0.6455610313988616
Loss in iteration 148 : 0.7060152239745654
Loss in iteration 149 : 0.6342860216160335
Loss in iteration 150 : 0.6909177834990864
Loss in iteration 151 : 0.6461739566341689
Loss in iteration 152 : 0.7223335829365635
Loss in iteration 153 : 0.6552812379245085
Loss in iteration 154 : 0.7277705271403037
Loss in iteration 155 : 0.6513960162020331
Loss in iteration 156 : 0.7155254462053127
Loss in iteration 157 : 0.6416635576544416
Loss in iteration 158 : 0.6986861302699415
Loss in iteration 159 : 0.648079666363511
Loss in iteration 160 : 0.7219355908823433
Loss in iteration 161 : 0.653588071516448
Loss in iteration 162 : 0.7253260309130923
Loss in iteration 163 : 0.6535394560556809
Loss in iteration 164 : 0.7227890869280897
Loss in iteration 165 : 0.6505163525338609
Loss in iteration 166 : 0.7139982889645606
Loss in iteration 167 : 0.6487357217062464
Loss in iteration 168 : 0.7156982235121543
Loss in iteration 169 : 0.6534117855222065
Loss in iteration 170 : 0.7305739520559099
Loss in iteration 171 : 0.662819276336801
Loss in iteration 172 : 0.7346981510274828
Loss in iteration 173 : 0.658275657622299
Loss in iteration 174 : 0.7240483352441623
Loss in iteration 175 : 0.6475842899971249
Loss in iteration 176 : 0.7056845333014325
Loss in iteration 177 : 0.656066577832774
Loss in iteration 178 : 0.7317293477812733
Loss in iteration 179 : 0.6623447193686272
Loss in iteration 180 : 0.7325729681390275
Loss in iteration 181 : 0.6581055481214955
Loss in iteration 182 : 0.7241394851357043
Loss in iteration 183 : 0.6532925151877035
Loss in iteration 184 : 0.7172566538784864
Loss in iteration 185 : 0.6566666739434078
Loss in iteration 186 : 0.731151949587071
Loss in iteration 187 : 0.6657770359897007
Loss in iteration 188 : 0.7412402095766126
Loss in iteration 189 : 0.6659058272984353
Loss in iteration 190 : 0.7326539081323388
Loss in iteration 191 : 0.6570568441173323
Loss in iteration 192 : 0.7162351707100858
Loss in iteration 193 : 0.6568762956017151
Loss in iteration 194 : 0.7238161168592308
Loss in iteration 195 : 0.661811232797625
Loss in iteration 196 : 0.7385080693742271
Loss in iteration 197 : 0.6718629228667325
Loss in iteration 198 : 0.7428296115175101
Loss in iteration 199 : 0.6663695220934407
Loss in iteration 200 : 0.7313540733919697
Testing accuracy  of updater 5 on alg 1 with rate 0.01372 = 0.79275, training accuracy 0.8022013596633215, time elapsed: 3031 millisecond.
Loss in iteration 1 : 1.000149215744508
Loss in iteration 2 : 0.7374395100538566
Loss in iteration 3 : 0.5870580608809726
Loss in iteration 4 : 0.5140641247746474
Loss in iteration 5 : 0.5059493733525542
Loss in iteration 6 : 0.4984038795925965
Loss in iteration 7 : 0.4907971565844401
Loss in iteration 8 : 0.4829794946407397
Loss in iteration 9 : 0.4748679548495101
Loss in iteration 10 : 0.4664712052431177
Loss in iteration 11 : 0.457734557182088
Loss in iteration 12 : 0.44867503348396043
Loss in iteration 13 : 0.4392787683797886
Loss in iteration 14 : 0.4298806870675476
Loss in iteration 15 : 0.4212702523629245
Loss in iteration 16 : 0.41372771746957765
Loss in iteration 17 : 0.4074389849052576
Loss in iteration 18 : 0.40243014084454326
Loss in iteration 19 : 0.39849626749832784
Loss in iteration 20 : 0.39540492332212696
Loss in iteration 21 : 0.39305387930612184
Loss in iteration 22 : 0.39131253501018454
Loss in iteration 23 : 0.3898568939182721
Loss in iteration 24 : 0.38850321389677084
Loss in iteration 25 : 0.3873761488870181
Loss in iteration 26 : 0.386387545063461
Loss in iteration 27 : 0.38557080014608863
Loss in iteration 28 : 0.3849232023809922
Loss in iteration 29 : 0.38445812302598287
Loss in iteration 30 : 0.3842698200462033
Loss in iteration 31 : 0.3846647631293927
Loss in iteration 32 : 0.3857186998382852
Loss in iteration 33 : 0.39141195359975867
Loss in iteration 34 : 0.40761514815532124
Loss in iteration 35 : 0.42398085303170496
Loss in iteration 36 : 0.43014857015462915
Loss in iteration 37 : 0.40290417982178695
Loss in iteration 38 : 0.39628965271099265
Loss in iteration 39 : 0.38824834675176684
Loss in iteration 40 : 0.38658965674454115
Loss in iteration 41 : 0.38579733004150474
Loss in iteration 42 : 0.3856814139696691
Loss in iteration 43 : 0.38596083689566
Loss in iteration 44 : 0.3869203402462425
Loss in iteration 45 : 0.38864841546134354
Loss in iteration 46 : 0.3901902565127465
Loss in iteration 47 : 0.3920873358953373
Loss in iteration 48 : 0.3983184598304027
Loss in iteration 49 : 0.4012432253401137
Loss in iteration 50 : 0.4096898501150753
Loss in iteration 51 : 0.4024473522955502
Loss in iteration 52 : 0.40471665069848256
Loss in iteration 53 : 0.396862002636465
Loss in iteration 54 : 0.39765940758497226
Loss in iteration 55 : 0.39457000912601997
Loss in iteration 56 : 0.39484162028959346
Loss in iteration 57 : 0.39425910303014716
Loss in iteration 58 : 0.3958285033840377
Loss in iteration 59 : 0.3961961393937951
Loss in iteration 60 : 0.3988582503059488
Loss in iteration 61 : 0.40036153311403405
Loss in iteration 62 : 0.40775841139486496
Loss in iteration 63 : 0.40412905609820254
Loss in iteration 64 : 0.4078671153900664
Loss in iteration 65 : 0.4013945515044823
Loss in iteration 66 : 0.40489715144397126
Loss in iteration 67 : 0.4009496929593679
Loss in iteration 68 : 0.4032146807326298
Loss in iteration 69 : 0.4013005303440201
Loss in iteration 70 : 0.40401758316126823
Loss in iteration 71 : 0.402690254855558
Loss in iteration 72 : 0.40589848263227635
Loss in iteration 73 : 0.4042605003750287
Loss in iteration 74 : 0.40934211593807446
Loss in iteration 75 : 0.4055994065221423
Loss in iteration 76 : 0.41090839220613623
Loss in iteration 77 : 0.4058445406742886
Loss in iteration 78 : 0.40989034242332545
Loss in iteration 79 : 0.4068604887000854
Loss in iteration 80 : 0.4111502119116586
Loss in iteration 81 : 0.407563916380725
Loss in iteration 82 : 0.4112751937628326
Loss in iteration 83 : 0.40848710657035775
Loss in iteration 84 : 0.41234422452670677
Loss in iteration 85 : 0.4098390341435407
Loss in iteration 86 : 0.41455626210906443
Loss in iteration 87 : 0.41098780648672506
Loss in iteration 88 : 0.41537320935207683
Loss in iteration 89 : 0.41185553934178
Loss in iteration 90 : 0.4163298182642534
Loss in iteration 91 : 0.41298287386219884
Loss in iteration 92 : 0.41740942129792546
Loss in iteration 93 : 0.41401680457317064
Loss in iteration 94 : 0.41823110136572184
Loss in iteration 95 : 0.41491522747463183
Loss in iteration 96 : 0.41930583390386134
Loss in iteration 97 : 0.4159723440527346
Loss in iteration 98 : 0.42002667891328194
Loss in iteration 99 : 0.41730080031128747
Loss in iteration 100 : 0.4222675037606995
Loss in iteration 101 : 0.4182768125929865
Loss in iteration 102 : 0.4226333025170405
Loss in iteration 103 : 0.4195378381166861
Loss in iteration 104 : 0.424386518327161
Loss in iteration 105 : 0.4203616703384384
Loss in iteration 106 : 0.42411474318605036
Loss in iteration 107 : 0.42173948246489973
Loss in iteration 108 : 0.4269605180499424
Loss in iteration 109 : 0.42268173530948233
Loss in iteration 110 : 0.42699357489155565
Loss in iteration 111 : 0.4240467778370273
Loss in iteration 112 : 0.4297535999869093
Loss in iteration 113 : 0.4251976487819579
Loss in iteration 114 : 0.43049880730105605
Loss in iteration 115 : 0.4260971522945799
Loss in iteration 116 : 0.43026303986555214
Loss in iteration 117 : 0.42732650879870027
Loss in iteration 118 : 0.4321894537790657
Loss in iteration 119 : 0.42879811339929885
Loss in iteration 120 : 0.4345943836672027
Loss in iteration 121 : 0.42966013422648186
Loss in iteration 122 : 0.43387683069531513
Loss in iteration 123 : 0.4311306700856708
Loss in iteration 124 : 0.4369450988183717
Loss in iteration 125 : 0.4324856545592405
Loss in iteration 126 : 0.4381662740894441
Loss in iteration 127 : 0.4335577349200262
Loss in iteration 128 : 0.43857940446022803
Loss in iteration 129 : 0.4345995878877546
Loss in iteration 130 : 0.4390635407626555
Loss in iteration 131 : 0.43566241559206087
Loss in iteration 132 : 0.43976128314027046
Loss in iteration 133 : 0.437411154188005
Loss in iteration 134 : 0.4430837612676927
Loss in iteration 135 : 0.4390680941567117
Loss in iteration 136 : 0.44454789837961467
Loss in iteration 137 : 0.4402210759687215
Loss in iteration 138 : 0.44497889585049505
Loss in iteration 139 : 0.44091235963253805
Loss in iteration 140 : 0.44568451403750764
Loss in iteration 141 : 0.44209194304091914
Loss in iteration 142 : 0.4472227075825022
Loss in iteration 143 : 0.443825447025553
Loss in iteration 144 : 0.4486893981234544
Loss in iteration 145 : 0.44510551525705994
Loss in iteration 146 : 0.4499605612281938
Loss in iteration 147 : 0.4464008553031165
Loss in iteration 148 : 0.4512462732215549
Loss in iteration 149 : 0.4476946635117834
Loss in iteration 150 : 0.45257014849316857
Loss in iteration 151 : 0.44895094048701367
Loss in iteration 152 : 0.4537229196212892
Loss in iteration 153 : 0.4504236751628403
Loss in iteration 154 : 0.4550701762864162
Loss in iteration 155 : 0.4516996444712669
Loss in iteration 156 : 0.45623136065549674
Loss in iteration 157 : 0.4529629162993136
Loss in iteration 158 : 0.4576326075051078
Loss in iteration 159 : 0.45423421918155726
Loss in iteration 160 : 0.45903555000979857
Loss in iteration 161 : 0.45562983714640676
Loss in iteration 162 : 0.4606754952562947
Loss in iteration 163 : 0.4568187062626169
Loss in iteration 164 : 0.4616031930918279
Loss in iteration 165 : 0.4583069118028958
Loss in iteration 166 : 0.4636515123228264
Loss in iteration 167 : 0.45937333573958494
Loss in iteration 168 : 0.4640072010825367
Loss in iteration 169 : 0.46093330970875535
Loss in iteration 170 : 0.46624237253024037
Loss in iteration 171 : 0.46180398478289586
Loss in iteration 172 : 0.4666225179776333
Loss in iteration 173 : 0.46361176416063155
Loss in iteration 174 : 0.469098868573451
Loss in iteration 175 : 0.46441027771011684
Loss in iteration 176 : 0.46960956687022387
Loss in iteration 177 : 0.46600428856034704
Loss in iteration 178 : 0.47085127188778364
Loss in iteration 179 : 0.4674247883375261
Loss in iteration 180 : 0.4725850485165029
Loss in iteration 181 : 0.4684568583981168
Loss in iteration 182 : 0.47355775570099956
Loss in iteration 183 : 0.47003486108567966
Loss in iteration 184 : 0.47558952609082195
Loss in iteration 185 : 0.4707423792437649
Loss in iteration 186 : 0.4758151315359569
Loss in iteration 187 : 0.4727508339151723
Loss in iteration 188 : 0.47793287175810995
Loss in iteration 189 : 0.47374609181747224
Loss in iteration 190 : 0.478960046126225
Loss in iteration 191 : 0.47522656675322794
Loss in iteration 192 : 0.48068945725861467
Loss in iteration 193 : 0.4762336555880394
Loss in iteration 194 : 0.48172309746638153
Loss in iteration 195 : 0.4777218153665893
Loss in iteration 196 : 0.48345757689244506
Loss in iteration 197 : 0.4787330812630747
Loss in iteration 198 : 0.4842704062475758
Loss in iteration 199 : 0.48047352943697696
Loss in iteration 200 : 0.4860071939016948
Testing accuracy  of updater 5 on alg 1 with rate 0.00343 = 0.78875, training accuracy 0.8352217546131434, time elapsed: 3037 millisecond.
Loss in iteration 1 : 1.0000926832048156
Loss in iteration 2 : 0.6780009001599505
Loss in iteration 3 : 0.5589671614205798
Loss in iteration 4 : 0.5340368633077439
Loss in iteration 5 : 0.5251640644277756
Loss in iteration 6 : 0.5181565302084317
Loss in iteration 7 : 0.5114118109999629
Loss in iteration 8 : 0.5048603596410239
Loss in iteration 9 : 0.4981588097536937
Loss in iteration 10 : 0.4912579520730961
Loss in iteration 11 : 0.48413465065221556
Loss in iteration 12 : 0.47674596721133033
Loss in iteration 13 : 0.4691066885290657
Loss in iteration 14 : 0.46116247952525347
Loss in iteration 15 : 0.4529092160263647
Loss in iteration 16 : 0.4443739911190669
Loss in iteration 17 : 0.4355781075960694
Loss in iteration 18 : 0.42705749755679967
Loss in iteration 19 : 0.4194287229066441
Loss in iteration 20 : 0.41262020115546977
Loss in iteration 21 : 0.4069066587631742
Loss in iteration 22 : 0.4022639712552115
Loss in iteration 23 : 0.398527419901091
Loss in iteration 24 : 0.39550618902928
Loss in iteration 25 : 0.3932282276931169
Loss in iteration 26 : 0.3914375173796416
Loss in iteration 27 : 0.3899378981845058
Loss in iteration 28 : 0.38851753171619874
Loss in iteration 29 : 0.3873909067889206
Loss in iteration 30 : 0.3862210244767889
Loss in iteration 31 : 0.3853059603486528
Loss in iteration 32 : 0.3845475205103347
Loss in iteration 33 : 0.3840037529844545
Loss in iteration 34 : 0.3840853010265026
Loss in iteration 35 : 0.3842721983856728
Loss in iteration 36 : 0.3867515014543931
Loss in iteration 37 : 0.3921678823945405
Loss in iteration 38 : 0.3977861214584225
Loss in iteration 39 : 0.40125676044499986
Loss in iteration 40 : 0.3921105996532781
Loss in iteration 41 : 0.3872137061225935
Loss in iteration 42 : 0.38446419863920184
Loss in iteration 43 : 0.38365817494674487
Loss in iteration 44 : 0.38295302390873054
Loss in iteration 45 : 0.38271859929205954
Loss in iteration 46 : 0.38287852439153247
Loss in iteration 47 : 0.38359012245701124
Loss in iteration 48 : 0.3844287396416815
Loss in iteration 49 : 0.38561656310851533
Loss in iteration 50 : 0.38726258710734396
Loss in iteration 51 : 0.389235343866153
Loss in iteration 52 : 0.3893920218285012
Loss in iteration 53 : 0.3908771651363185
Loss in iteration 54 : 0.3889663658690559
Loss in iteration 55 : 0.38929655631451027
Loss in iteration 56 : 0.38770682174610566
Loss in iteration 57 : 0.3870821440811713
Loss in iteration 58 : 0.38675447198035057
Loss in iteration 59 : 0.38714231465849275
Loss in iteration 60 : 0.3873326141040275
Loss in iteration 61 : 0.3882881405700131
Loss in iteration 62 : 0.3885707845914514
Loss in iteration 63 : 0.3890316262815669
Loss in iteration 64 : 0.3887913020914791
Loss in iteration 65 : 0.38958377435980096
Loss in iteration 66 : 0.3897053843876842
Loss in iteration 67 : 0.3907626504336111
Loss in iteration 68 : 0.3917882863334657
Loss in iteration 69 : 0.39351238119710313
Loss in iteration 70 : 0.3919690891561149
Loss in iteration 71 : 0.39227989645247885
Loss in iteration 72 : 0.3919206140415995
Loss in iteration 73 : 0.39163493828420737
Loss in iteration 74 : 0.3909691675555321
Loss in iteration 75 : 0.391289595452417
Loss in iteration 76 : 0.3914954787506616
Loss in iteration 77 : 0.39235378040148455
Loss in iteration 78 : 0.3929426058294156
Loss in iteration 79 : 0.3938898534112004
Loss in iteration 80 : 0.39390961865220386
Loss in iteration 81 : 0.3943537273086876
Loss in iteration 82 : 0.3945109378621185
Loss in iteration 83 : 0.39473833462521485
Loss in iteration 84 : 0.3952445854297445
Loss in iteration 85 : 0.3954956519126118
Loss in iteration 86 : 0.39560223772790754
Loss in iteration 87 : 0.39605600788971235
Loss in iteration 88 : 0.3962183755379094
Loss in iteration 89 : 0.3966339214048029
Loss in iteration 90 : 0.39684326432994643
Loss in iteration 91 : 0.39719452223115403
Loss in iteration 92 : 0.3975192702959534
Loss in iteration 93 : 0.397770944181437
Loss in iteration 94 : 0.3982032993656489
Loss in iteration 95 : 0.39822816974724884
Loss in iteration 96 : 0.3984456269210776
Loss in iteration 97 : 0.39910851922986107
Loss in iteration 98 : 0.3994249598879464
Loss in iteration 99 : 0.39982497038558545
Loss in iteration 100 : 0.4001643136416213
Loss in iteration 101 : 0.4005977320448262
Loss in iteration 102 : 0.401265055617213
Loss in iteration 103 : 0.40170733277176884
Loss in iteration 104 : 0.40134009165596535
Loss in iteration 105 : 0.4015989621972964
Loss in iteration 106 : 0.4016251049606613
Loss in iteration 107 : 0.402070347239128
Loss in iteration 108 : 0.4022474064205847
Loss in iteration 109 : 0.4029341074365349
Loss in iteration 110 : 0.40324995100558414
Loss in iteration 111 : 0.40390689600838037
Loss in iteration 112 : 0.4046348834049307
Loss in iteration 113 : 0.4056474079234293
Loss in iteration 114 : 0.4054515826238291
Loss in iteration 115 : 0.40602611068850164
Loss in iteration 116 : 0.4059631438649767
Loss in iteration 117 : 0.4063068922267287
Loss in iteration 118 : 0.40628860508990516
Loss in iteration 119 : 0.40652366562934517
Loss in iteration 120 : 0.4068352640157888
Loss in iteration 121 : 0.40739570565372757
Loss in iteration 122 : 0.40765074591278394
Loss in iteration 123 : 0.40826366941296083
Loss in iteration 124 : 0.40867138348536414
Loss in iteration 125 : 0.40922490290334085
Loss in iteration 126 : 0.4095985954769842
Loss in iteration 127 : 0.4103140555290182
Loss in iteration 128 : 0.4105822427809381
Loss in iteration 129 : 0.41112393926741836
Loss in iteration 130 : 0.41127426027235914
Loss in iteration 131 : 0.41175958426630604
Loss in iteration 132 : 0.4120147972398832
Loss in iteration 133 : 0.4126094297050044
Loss in iteration 134 : 0.4127655237475798
Loss in iteration 135 : 0.4133004528189399
Loss in iteration 136 : 0.4133898799508487
Loss in iteration 137 : 0.41410615234657316
Loss in iteration 138 : 0.41429238377590766
Loss in iteration 139 : 0.41516535847107855
Loss in iteration 140 : 0.41536417591469255
Loss in iteration 141 : 0.41605622218563054
Loss in iteration 142 : 0.41610641034632223
Loss in iteration 143 : 0.41664945225004485
Loss in iteration 144 : 0.41687889289098795
Loss in iteration 145 : 0.41760505755241695
Loss in iteration 146 : 0.41764596449555846
Loss in iteration 147 : 0.41817485511112246
Loss in iteration 148 : 0.41844920099862976
Loss in iteration 149 : 0.41917615264328173
Loss in iteration 150 : 0.4195372943190884
Loss in iteration 151 : 0.42021724104569774
Loss in iteration 152 : 0.4203658978591084
Loss in iteration 153 : 0.4210402781855501
Loss in iteration 154 : 0.4212721638779138
Loss in iteration 155 : 0.42213088752490846
Loss in iteration 156 : 0.4222005697744589
Loss in iteration 157 : 0.4229017661350172
Loss in iteration 158 : 0.4228501571333417
Loss in iteration 159 : 0.4236621004289484
Loss in iteration 160 : 0.4238969682531595
Loss in iteration 161 : 0.42477346247310654
Loss in iteration 162 : 0.4247462142425478
Loss in iteration 163 : 0.4256816413334296
Loss in iteration 164 : 0.42560011064547093
Loss in iteration 165 : 0.42634512246835604
Loss in iteration 166 : 0.4265559884747047
Loss in iteration 167 : 0.42748107783231304
Loss in iteration 168 : 0.4275583675409918
Loss in iteration 169 : 0.4282609539058799
Loss in iteration 170 : 0.42825127766442656
Loss in iteration 171 : 0.42892466818052166
Loss in iteration 172 : 0.42873916575706694
Loss in iteration 173 : 0.4297792347148479
Loss in iteration 174 : 0.4298685589638444
Loss in iteration 175 : 0.4308094741286506
Loss in iteration 176 : 0.4309162325627019
Loss in iteration 177 : 0.4320191635031654
Loss in iteration 178 : 0.43242054249302303
Loss in iteration 179 : 0.4333153278347368
Loss in iteration 180 : 0.4338634977327922
Loss in iteration 181 : 0.4343810314958696
Loss in iteration 182 : 0.43498204798318374
Loss in iteration 183 : 0.4348135985351377
Loss in iteration 184 : 0.43431403283194403
Loss in iteration 185 : 0.4348903795581225
Loss in iteration 186 : 0.43487481208784573
Loss in iteration 187 : 0.4362244083345519
Loss in iteration 188 : 0.43665618229726966
Loss in iteration 189 : 0.4379623663514585
Loss in iteration 190 : 0.4382791156933664
Loss in iteration 191 : 0.43866515181096155
Loss in iteration 192 : 0.4388267389258991
Loss in iteration 193 : 0.4393751726380448
Loss in iteration 194 : 0.43950605116397223
Loss in iteration 195 : 0.4404618806655311
Loss in iteration 196 : 0.44039489906077367
Loss in iteration 197 : 0.44154428221416653
Loss in iteration 198 : 0.4420000898125218
Loss in iteration 199 : 0.44241084123428787
Loss in iteration 200 : 0.44261566902798166
Testing accuracy  of updater 5 on alg 1 with rate 0.002401 = 0.7755, training accuracy 0.8345742958886371, time elapsed: 3238 millisecond.
Loss in iteration 1 : 1.0000310966404597
Loss in iteration 2 : 0.5648615657035891
Loss in iteration 3 : 0.5547606612452158
Loss in iteration 4 : 0.5498902805168957
Loss in iteration 5 : 0.545333000170546
Loss in iteration 6 : 0.5408009557518345
Loss in iteration 7 : 0.5362407999021949
Loss in iteration 8 : 0.5315868339766728
Loss in iteration 9 : 0.5268488546138708
Loss in iteration 10 : 0.5220471166707472
Loss in iteration 11 : 0.5171055338515986
Loss in iteration 12 : 0.512031131978762
Loss in iteration 13 : 0.5068495265793223
Loss in iteration 14 : 0.5015403704630343
Loss in iteration 15 : 0.49603843045224527
Loss in iteration 16 : 0.490369219843898
Loss in iteration 17 : 0.48452488734510135
Loss in iteration 18 : 0.47849710737766055
Loss in iteration 19 : 0.47224637010399506
Loss in iteration 20 : 0.46578127960208804
Loss in iteration 21 : 0.4590955492520491
Loss in iteration 22 : 0.4521608454700013
Loss in iteration 23 : 0.4450120249230154
Loss in iteration 24 : 0.43769021586213275
Loss in iteration 25 : 0.4305109131676601
Loss in iteration 26 : 0.4238284690360936
Loss in iteration 27 : 0.4177856507462749
Loss in iteration 28 : 0.41226403633774705
Loss in iteration 29 : 0.40745743731840517
Loss in iteration 30 : 0.40342313782040323
Loss in iteration 31 : 0.3999817772953062
Loss in iteration 32 : 0.3971085309671269
Loss in iteration 33 : 0.39467539556629455
Loss in iteration 34 : 0.39265799029523973
Loss in iteration 35 : 0.3909754039577729
Loss in iteration 36 : 0.3896293954913626
Loss in iteration 37 : 0.3883737465411595
Loss in iteration 38 : 0.38718775269007716
Loss in iteration 39 : 0.3860959915294862
Loss in iteration 40 : 0.3851343575450833
Loss in iteration 41 : 0.3842679881875083
Loss in iteration 42 : 0.3835274644206376
Loss in iteration 43 : 0.3828565599174333
Loss in iteration 44 : 0.38220856511699025
Loss in iteration 45 : 0.3816092767091489
Loss in iteration 46 : 0.381141403277407
Loss in iteration 47 : 0.3809719272829431
Loss in iteration 48 : 0.38111165533119296
Loss in iteration 49 : 0.3822264895579253
Loss in iteration 50 : 0.3841627746191869
Loss in iteration 51 : 0.3862464557228094
Loss in iteration 52 : 0.385593371028969
Loss in iteration 53 : 0.38230195109879433
Loss in iteration 54 : 0.38038029861238515
Loss in iteration 55 : 0.3797570843626595
Loss in iteration 56 : 0.3792417820544419
Loss in iteration 57 : 0.3789868729685138
Loss in iteration 58 : 0.3788541419998462
Loss in iteration 59 : 0.37880351204175344
Loss in iteration 60 : 0.3787856018142893
Loss in iteration 61 : 0.37880339561315013
Loss in iteration 62 : 0.37882075393814185
Loss in iteration 63 : 0.3790541309234591
Loss in iteration 64 : 0.3798453182016695
Loss in iteration 65 : 0.381953611555463
Loss in iteration 66 : 0.383697942659055
Loss in iteration 67 : 0.38463499117392086
Loss in iteration 68 : 0.3836979162809726
Loss in iteration 69 : 0.38198590093463547
Loss in iteration 70 : 0.3806387088763658
Loss in iteration 71 : 0.3807669011965034
Loss in iteration 72 : 0.3800438621291845
Loss in iteration 73 : 0.38029316892882614
Loss in iteration 74 : 0.38047852922735487
Loss in iteration 75 : 0.381296086704329
Loss in iteration 76 : 0.3807538969207296
Loss in iteration 77 : 0.3816283837355757
Loss in iteration 78 : 0.3811704427087415
Loss in iteration 79 : 0.3821524567208851
Loss in iteration 80 : 0.3817046135559206
Loss in iteration 81 : 0.38267683364373256
Loss in iteration 82 : 0.38189773518901654
Loss in iteration 83 : 0.3828054739789489
Loss in iteration 84 : 0.3821461242262385
Loss in iteration 85 : 0.38317415394983273
Loss in iteration 86 : 0.38215837122254026
Loss in iteration 87 : 0.38303989284277734
Loss in iteration 88 : 0.3821832095042844
Loss in iteration 89 : 0.38299401915478476
Loss in iteration 90 : 0.382257796366292
Loss in iteration 91 : 0.38339903093264405
Loss in iteration 92 : 0.38266593624437256
Loss in iteration 93 : 0.38375938223073863
Loss in iteration 94 : 0.3831761836443688
Loss in iteration 95 : 0.3844885698921705
Loss in iteration 96 : 0.38418453065420954
Loss in iteration 97 : 0.38511981585268895
Loss in iteration 98 : 0.3839358671236053
Loss in iteration 99 : 0.38454647056861674
Loss in iteration 100 : 0.38340875425498644
Loss in iteration 101 : 0.3841183065084823
Loss in iteration 102 : 0.38359172920822315
Loss in iteration 103 : 0.38450736695292276
Loss in iteration 104 : 0.38413241356592487
Loss in iteration 105 : 0.3851956750699961
Loss in iteration 106 : 0.38439053097645715
Loss in iteration 107 : 0.3855016815067715
Loss in iteration 108 : 0.38465135309287063
Loss in iteration 109 : 0.38571637447348683
Loss in iteration 110 : 0.38501563893687735
Loss in iteration 111 : 0.38624219426387457
Loss in iteration 112 : 0.3855384005346637
Loss in iteration 113 : 0.38667863407493147
Loss in iteration 114 : 0.38622120335714866
Loss in iteration 115 : 0.3871539417815363
Loss in iteration 116 : 0.3863699012166811
Loss in iteration 117 : 0.38705213467594524
Loss in iteration 118 : 0.3859580609948369
Loss in iteration 119 : 0.38665359453107384
Loss in iteration 120 : 0.38618237106548947
Loss in iteration 121 : 0.38690757628461353
Loss in iteration 122 : 0.38659344426746295
Loss in iteration 123 : 0.38771465370222086
Loss in iteration 124 : 0.38708181349640225
Loss in iteration 125 : 0.38821925935315665
Loss in iteration 126 : 0.3874564155988372
Loss in iteration 127 : 0.3885860250795389
Loss in iteration 128 : 0.3881599040719648
Loss in iteration 129 : 0.38930974639022203
Loss in iteration 130 : 0.3886034424323008
Loss in iteration 131 : 0.3892366872043275
Loss in iteration 132 : 0.3883828229293926
Loss in iteration 133 : 0.3891127566633118
Loss in iteration 134 : 0.3883389161678361
Loss in iteration 135 : 0.3890159422601402
Loss in iteration 136 : 0.3885032875279534
Loss in iteration 137 : 0.38933145468961217
Loss in iteration 138 : 0.3890494479043256
Loss in iteration 139 : 0.39043695524927113
Loss in iteration 140 : 0.38990630662775705
Loss in iteration 141 : 0.3912072204406861
Loss in iteration 142 : 0.39085854685938515
Loss in iteration 143 : 0.39154242255132016
Loss in iteration 144 : 0.3907798279285897
Loss in iteration 145 : 0.3913397727196978
Loss in iteration 146 : 0.390444772480927
Loss in iteration 147 : 0.3913374511469958
Loss in iteration 148 : 0.39092519564860656
Loss in iteration 149 : 0.39202247985381083
Loss in iteration 150 : 0.3913270462910208
Loss in iteration 151 : 0.3924673745774672
Loss in iteration 152 : 0.3918426094126965
Loss in iteration 153 : 0.3929038188615699
Loss in iteration 154 : 0.3923745195285927
Loss in iteration 155 : 0.3933387035174125
Loss in iteration 156 : 0.3927399064525882
Loss in iteration 157 : 0.39364926879115547
Loss in iteration 158 : 0.39295105939873576
Loss in iteration 159 : 0.3938910209004626
Loss in iteration 160 : 0.39321299808373544
Loss in iteration 161 : 0.3940231202879096
Loss in iteration 162 : 0.39360131248494695
Loss in iteration 163 : 0.3945329995805062
Loss in iteration 164 : 0.3941546552783105
Loss in iteration 165 : 0.39512855206666037
Loss in iteration 166 : 0.3947137119330761
Loss in iteration 167 : 0.3955981773760389
Loss in iteration 168 : 0.3949648838409955
Loss in iteration 169 : 0.39581284149818985
Loss in iteration 170 : 0.3953656238068829
Loss in iteration 171 : 0.39620463088660995
Loss in iteration 172 : 0.39577136302736887
Loss in iteration 173 : 0.3966010542029676
Loss in iteration 174 : 0.3961818005051536
Loss in iteration 175 : 0.39699881380969226
Loss in iteration 176 : 0.39656368069429365
Loss in iteration 177 : 0.3973530750913278
Loss in iteration 178 : 0.39685167633891477
Loss in iteration 179 : 0.3977544139170173
Loss in iteration 180 : 0.39732039891098936
Loss in iteration 181 : 0.39824173881083763
Loss in iteration 182 : 0.39788581251371463
Loss in iteration 183 : 0.3985973969619096
Loss in iteration 184 : 0.39837095331433103
Loss in iteration 185 : 0.39913696022865747
Loss in iteration 186 : 0.398763097352173
Loss in iteration 187 : 0.39938994093648306
Loss in iteration 188 : 0.3989453752156313
Loss in iteration 189 : 0.3999339623232528
Loss in iteration 190 : 0.3996225478177982
Loss in iteration 191 : 0.40029865820262
Loss in iteration 192 : 0.39988758066150826
Loss in iteration 193 : 0.40084209173022484
Loss in iteration 194 : 0.40057520965897364
Loss in iteration 195 : 0.4011961046791878
Loss in iteration 196 : 0.40074243915558344
Loss in iteration 197 : 0.40174237378999883
Loss in iteration 198 : 0.4012963383142189
Loss in iteration 199 : 0.40207846235617223
Loss in iteration 200 : 0.40165939356699315
Testing accuracy  of updater 5 on alg 1 with rate 0.001372 = 0.789, training accuracy 0.8391065069601813, time elapsed: 3093 millisecond.
Loss in iteration 1 : 1.0000019533755926
Loss in iteration 2 : 0.8172264353871345
Loss in iteration 3 : 0.6940084657681541
Loss in iteration 4 : 0.6279102388507317
Testing accuracy  of updater 5 on alg 1 with rate 3.43E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 73 millisecond.
Loss in iteration 1 : 1.0017017965417514
Loss in iteration 2 : 1.6891871633796376
Loss in iteration 3 : 2.1368061578550726
Loss in iteration 4 : 2.09783483833275
Loss in iteration 5 : 1.7565611413918456
Loss in iteration 6 : 1.2144981292553545
Loss in iteration 7 : 0.601019632565689
Loss in iteration 8 : 0.5143163433690616
Loss in iteration 9 : 0.8934334795325567
Loss in iteration 10 : 1.0592961302690858
Loss in iteration 11 : 0.8777657355920826
Loss in iteration 12 : 0.6227862673858597
Loss in iteration 13 : 0.5325755637653244
Loss in iteration 14 : 0.5969411151761461
Loss in iteration 15 : 0.6977773032728021
Loss in iteration 16 : 0.7476805507413091
Loss in iteration 17 : 0.7234746066154473
Loss in iteration 18 : 0.6487752307859931
Loss in iteration 19 : 0.5717247761911481
Loss in iteration 20 : 0.5453402238856184
Loss in iteration 21 : 0.5769844800987813
Loss in iteration 22 : 0.6172421570750455
Loss in iteration 23 : 0.6161210657052838
Loss in iteration 24 : 0.5702571794742995
Loss in iteration 25 : 0.5236148732087671
Loss in iteration 26 : 0.5137293431251363
Loss in iteration 27 : 0.534547949110124
Loss in iteration 28 : 0.5437547059930888
Loss in iteration 29 : 0.5201590413127977
Loss in iteration 30 : 0.4867356746833029
Loss in iteration 31 : 0.4831167936355602
Loss in iteration 32 : 0.4995751259004376
Loss in iteration 33 : 0.49268743566377593
Loss in iteration 34 : 0.46515833095124426
Loss in iteration 35 : 0.46686794110011215
Loss in iteration 36 : 0.48291590234088977
Loss in iteration 37 : 0.4697079853466563
Loss in iteration 38 : 0.46281335731834655
Loss in iteration 39 : 0.4840364179183928
Loss in iteration 40 : 0.4699110528939413
Loss in iteration 41 : 0.47783990384382163
Loss in iteration 42 : 0.487356070646953
Loss in iteration 43 : 0.4794330434922313
Loss in iteration 44 : 0.4940720188927262
Loss in iteration 45 : 0.48605448670616325
Loss in iteration 46 : 0.49209258421995605
Loss in iteration 47 : 0.49431901974097037
Loss in iteration 48 : 0.49293479460952566
Loss in iteration 49 : 0.50027412479524
Loss in iteration 50 : 0.4977897304609912
Loss in iteration 51 : 0.5017006418433285
Loss in iteration 52 : 0.5057787866910228
Loss in iteration 53 : 0.505440694863987
Loss in iteration 54 : 0.5100750360472823
Loss in iteration 55 : 0.5112895298052565
Loss in iteration 56 : 0.5124153197518857
Loss in iteration 57 : 0.5159494074756904
Loss in iteration 58 : 0.5165053070715133
Loss in iteration 59 : 0.518410737806714
Loss in iteration 60 : 0.5202288251767353
Loss in iteration 61 : 0.5205177084203731
Loss in iteration 62 : 0.522390079125671
Loss in iteration 63 : 0.5233920892849264
Loss in iteration 64 : 0.5242669905983601
Loss in iteration 65 : 0.52577485125754
Loss in iteration 66 : 0.5264978648499153
Loss in iteration 67 : 0.5278803276029851
Loss in iteration 68 : 0.5288851485529992
Loss in iteration 69 : 0.5298359054708464
Loss in iteration 70 : 0.5310712017915207
Loss in iteration 71 : 0.5317304297847536
Loss in iteration 72 : 0.5328551880392959
Loss in iteration 73 : 0.5337097729445466
Loss in iteration 74 : 0.5345725047143128
Loss in iteration 75 : 0.535584578805413
Loss in iteration 76 : 0.5362466758472706
Loss in iteration 77 : 0.5371310886751041
Loss in iteration 78 : 0.5378854122974576
Loss in iteration 79 : 0.5386040203517527
Loss in iteration 80 : 0.5393592800975424
Loss in iteration 81 : 0.539947280894739
Loss in iteration 82 : 0.5406380258032197
Loss in iteration 83 : 0.5412006318774911
Loss in iteration 84 : 0.5418174670787967
Loss in iteration 85 : 0.5423469487866277
Testing accuracy  of updater 6 on alg 1 with rate 0.0392 = 0.7865, training accuracy 0.8400776950469407, time elapsed: 1299 millisecond.
Loss in iteration 1 : 1.0005274017242323
Loss in iteration 2 : 0.9278193190370203
Loss in iteration 3 : 1.2063573703133372
Loss in iteration 4 : 1.2335069641288723
Loss in iteration 5 : 1.047885914372402
Loss in iteration 6 : 0.6862280619927248
Loss in iteration 7 : 0.41611965632090875
Loss in iteration 8 : 0.5876341362367075
Loss in iteration 9 : 0.7835250322970851
Loss in iteration 10 : 0.7310289043397528
Loss in iteration 11 : 0.5675484716840923
Loss in iteration 12 : 0.49686307759786846
Loss in iteration 13 : 0.5400101557153534
Loss in iteration 14 : 0.6183147666074899
Loss in iteration 15 : 0.6572001986936414
Loss in iteration 16 : 0.6401303617001497
Loss in iteration 17 : 0.588497806721621
Loss in iteration 18 : 0.5447973274105793
Loss in iteration 19 : 0.5431379093452895
Loss in iteration 20 : 0.5742310166840617
Loss in iteration 21 : 0.5990052696529148
Loss in iteration 22 : 0.5902687096779992
Loss in iteration 23 : 0.5565492438711055
Loss in iteration 24 : 0.5288601273411776
Loss in iteration 25 : 0.5267318037550452
Loss in iteration 26 : 0.5417951282908986
Loss in iteration 27 : 0.5460852457048444
Loss in iteration 28 : 0.5284781885118274
Loss in iteration 29 : 0.504643367742634
Loss in iteration 30 : 0.49799283339595396
Loss in iteration 31 : 0.5072543870893691
Loss in iteration 32 : 0.5071203512463581
Loss in iteration 33 : 0.48915284568440287
Loss in iteration 34 : 0.47672493321206727
Loss in iteration 35 : 0.48371451307814345
Loss in iteration 36 : 0.48794374013083924
Loss in iteration 37 : 0.4765575212848889
Loss in iteration 38 : 0.47301410412413425
Loss in iteration 39 : 0.4861438404515802
Loss in iteration 40 : 0.48012245018071315
Loss in iteration 41 : 0.4790323243525696
Loss in iteration 42 : 0.4924654234943434
Loss in iteration 43 : 0.48770626732686845
Loss in iteration 44 : 0.4926692761264695
Loss in iteration 45 : 0.5000210696426562
Loss in iteration 46 : 0.49520424169973587
Loss in iteration 47 : 0.502430761865971
Loss in iteration 48 : 0.5033400767013524
Loss in iteration 49 : 0.5030341421857762
Loss in iteration 50 : 0.5086185127398906
Loss in iteration 51 : 0.5078943984879533
Loss in iteration 52 : 0.5094635166437027
Loss in iteration 53 : 0.5136903042063357
Loss in iteration 54 : 0.5144978171815445
Loss in iteration 55 : 0.5166997732110623
Loss in iteration 56 : 0.5201794790128588
Loss in iteration 57 : 0.5208867558821714
Loss in iteration 58 : 0.5225352469120945
Loss in iteration 59 : 0.5252176416320122
Loss in iteration 60 : 0.5264019419457888
Loss in iteration 61 : 0.5277149705962314
Loss in iteration 62 : 0.5297193452320909
Loss in iteration 63 : 0.5305230138945206
Loss in iteration 64 : 0.5312318832611524
Loss in iteration 65 : 0.5327728994116613
Loss in iteration 66 : 0.5334850088163534
Loss in iteration 67 : 0.5341971686601252
Loss in iteration 68 : 0.5353634640554414
Loss in iteration 69 : 0.5359753887747559
Loss in iteration 70 : 0.5365804521225295
Loss in iteration 71 : 0.537530545416342
Loss in iteration 72 : 0.5381433992049498
Loss in iteration 73 : 0.5387968449187136
Loss in iteration 74 : 0.5395805984392721
Loss in iteration 75 : 0.5401092204723099
Loss in iteration 76 : 0.5406975947617315
Testing accuracy  of updater 6 on alg 1 with rate 0.02744 = 0.78625, training accuracy 0.8397539656846876, time elapsed: 1173 millisecond.
Loss in iteration 1 : 1.000195894722208
Loss in iteration 2 : 0.705079814088538
Loss in iteration 3 : 0.8821677292030523
Loss in iteration 4 : 0.9070470181640269
Loss in iteration 5 : 0.7999823508489241
Loss in iteration 6 : 0.5816107266435444
Loss in iteration 7 : 0.38584512081519284
Loss in iteration 8 : 0.4818761515681181
Loss in iteration 9 : 0.608254672619679
Loss in iteration 10 : 0.5861297171958694
Loss in iteration 11 : 0.48028996060069806
Loss in iteration 12 : 0.4199862308952359
Loss in iteration 13 : 0.438580036685961
Loss in iteration 14 : 0.49232161438691924
Loss in iteration 15 : 0.5258265950386649
Loss in iteration 16 : 0.5208586868058946
Loss in iteration 17 : 0.4903317729733329
Loss in iteration 18 : 0.4595964264681593
Loss in iteration 19 : 0.453301455840661
Loss in iteration 20 : 0.46930163562074523
Loss in iteration 21 : 0.48993728002893167
Loss in iteration 22 : 0.4978081084790043
Loss in iteration 23 : 0.4882544836518913
Loss in iteration 24 : 0.4705521643670785
Loss in iteration 25 : 0.4584595423365519
Loss in iteration 26 : 0.4583709687869838
Loss in iteration 27 : 0.46732443111254923
Loss in iteration 28 : 0.4734143998523111
Loss in iteration 29 : 0.4690862142461303
Loss in iteration 30 : 0.4581522290688263
Loss in iteration 31 : 0.4505461424019277
Loss in iteration 32 : 0.4504100129085834
Loss in iteration 33 : 0.4552482748589652
Loss in iteration 34 : 0.4565151370553976
Loss in iteration 35 : 0.45080415177850985
Loss in iteration 36 : 0.44356883906749145
Loss in iteration 37 : 0.44242464969406703
Loss in iteration 38 : 0.44608474788810765
Loss in iteration 39 : 0.4479531352128988
Loss in iteration 40 : 0.4449882358152445
Loss in iteration 41 : 0.44160133880715546
Loss in iteration 42 : 0.4439758995650036
Loss in iteration 43 : 0.4485687477490107
Loss in iteration 44 : 0.4491637951110196
Loss in iteration 45 : 0.44741239228184915
Loss in iteration 46 : 0.44947123053853055
Loss in iteration 47 : 0.4544122391860038
Loss in iteration 48 : 0.4560956611242444
Loss in iteration 49 : 0.4566165773907789
Loss in iteration 50 : 0.4597548618275248
Loss in iteration 51 : 0.4635911351663089
Loss in iteration 52 : 0.46478547024621997
Loss in iteration 53 : 0.466077385177568
Loss in iteration 54 : 0.4689940370146106
Loss in iteration 55 : 0.47179848510116273
Loss in iteration 56 : 0.4732259037337783
Loss in iteration 57 : 0.47481039232330646
Loss in iteration 58 : 0.4773078332015015
Loss in iteration 59 : 0.4796617655373366
Loss in iteration 60 : 0.48100176711319687
Loss in iteration 61 : 0.4825840566293851
Loss in iteration 62 : 0.48482196219789614
Loss in iteration 63 : 0.4868793509063808
Loss in iteration 64 : 0.48839397396137596
Loss in iteration 65 : 0.49017335307010723
Loss in iteration 66 : 0.4922264597042415
Loss in iteration 67 : 0.49398399801229437
Loss in iteration 68 : 0.4955052097763517
Loss in iteration 69 : 0.49715476738924724
Loss in iteration 70 : 0.49891578070816944
Loss in iteration 71 : 0.5004872657501565
Loss in iteration 72 : 0.5019054017790516
Loss in iteration 73 : 0.5033347168183582
Loss in iteration 74 : 0.5048419989483591
Loss in iteration 75 : 0.5061978487151495
Loss in iteration 76 : 0.5073983699133544
Loss in iteration 77 : 0.5086392694963807
Loss in iteration 78 : 0.509909609214449
Loss in iteration 79 : 0.5111019527097342
Loss in iteration 80 : 0.5121945123996579
Loss in iteration 81 : 0.5132996123250636
Loss in iteration 82 : 0.514435700312076
Loss in iteration 83 : 0.5154681353528157
Loss in iteration 84 : 0.5164371194446575
Loss in iteration 85 : 0.517436238348757
Loss in iteration 86 : 0.5184217686970704
Loss in iteration 87 : 0.5193328312659079
Loss in iteration 88 : 0.5202434042035096
Loss in iteration 89 : 0.5211480208498512
Loss in iteration 90 : 0.5220080225922286
Loss in iteration 91 : 0.5228287364975445
Loss in iteration 92 : 0.5236438424979952
Loss in iteration 93 : 0.5244544568232239
Loss in iteration 94 : 0.5252215064938356
Loss in iteration 95 : 0.5259758842559286
Loss in iteration 96 : 0.5267054253695705
Loss in iteration 97 : 0.5274171186601578
Loss in iteration 98 : 0.5280904963156758
Loss in iteration 99 : 0.5287570747335633
Loss in iteration 100 : 0.5294166201266302
Loss in iteration 101 : 0.530058803906275
Loss in iteration 102 : 0.5306752077539358
Loss in iteration 103 : 0.5312844702300836
Loss in iteration 104 : 0.5318817151143769
Loss in iteration 105 : 0.5324505516775266
Loss in iteration 106 : 0.5330065525296037
Loss in iteration 107 : 0.5335496287821978
Testing accuracy  of updater 6 on alg 1 with rate 0.01568 = 0.78625, training accuracy 0.8407251537714471, time elapsed: 1650 millisecond.
Loss in iteration 1 : 1.0000143899840124
Loss in iteration 2 : 0.6139312538401905
Loss in iteration 3 : 0.588178156593956
Loss in iteration 4 : 0.6735000935023171
Loss in iteration 5 : 0.7166884709256014
Loss in iteration 6 : 0.7200155367738196
Loss in iteration 7 : 0.6880921368331252
Loss in iteration 8 : 0.625685354408719
Loss in iteration 9 : 0.5401119398261468
Loss in iteration 10 : 0.4567876631224898
Loss in iteration 11 : 0.4126614198096037
Loss in iteration 12 : 0.4352718664917956
Loss in iteration 13 : 0.48132991906796746
Loss in iteration 14 : 0.49857978388107943
Loss in iteration 15 : 0.4771007893616673
Loss in iteration 16 : 0.4349211795385607
Loss in iteration 17 : 0.39767779374227036
Loss in iteration 18 : 0.38239856643705555
Loss in iteration 19 : 0.3875267881845783
Loss in iteration 20 : 0.40182123238107387
Loss in iteration 21 : 0.41434053359149153
Loss in iteration 22 : 0.4188647828421587
Loss in iteration 23 : 0.41475692927269664
Loss in iteration 24 : 0.404997723111841
Loss in iteration 25 : 0.3942827498632893
Loss in iteration 26 : 0.3866307822911541
Loss in iteration 27 : 0.38437034523821323
Loss in iteration 28 : 0.3870206155696139
Loss in iteration 29 : 0.3923704301827294
Loss in iteration 30 : 0.39726192312106845
Loss in iteration 31 : 0.3997553439245636
Loss in iteration 32 : 0.39916856400878903
Loss in iteration 33 : 0.39639580094934734
Loss in iteration 34 : 0.39285519965555293
Loss in iteration 35 : 0.3900155709790085
Loss in iteration 36 : 0.38906856003007245
Loss in iteration 37 : 0.39004417746177833
Loss in iteration 38 : 0.39187880992848373
Loss in iteration 39 : 0.39385350095568444
Loss in iteration 40 : 0.39516525217285836
Loss in iteration 41 : 0.3955183338724154
Loss in iteration 42 : 0.39498001960243523
Loss in iteration 43 : 0.3939023888110023
Loss in iteration 44 : 0.39290543428334274
Loss in iteration 45 : 0.3923143516373551
Loss in iteration 46 : 0.3923799843706234
Loss in iteration 47 : 0.39319740422444494
Loss in iteration 48 : 0.39422920211813195
Loss in iteration 49 : 0.3950671445678854
Loss in iteration 50 : 0.3954314495602102
Loss in iteration 51 : 0.39535656781735773
Loss in iteration 52 : 0.39500384203013167
Loss in iteration 53 : 0.3947609690320935
Loss in iteration 54 : 0.39485562327933366
Loss in iteration 55 : 0.39525610018500545
Loss in iteration 56 : 0.39588525719843265
Loss in iteration 57 : 0.39652498334303304
Loss in iteration 58 : 0.3970175156663009
Loss in iteration 59 : 0.39730199505274766
Loss in iteration 60 : 0.397452788827199
Loss in iteration 61 : 0.3976317781517658
Loss in iteration 62 : 0.39796616790509226
Loss in iteration 63 : 0.39847247365054234
Loss in iteration 64 : 0.3990497603117248
Loss in iteration 65 : 0.39963032291075423
Loss in iteration 66 : 0.400159253783927
Loss in iteration 67 : 0.4006091389881605
Loss in iteration 68 : 0.4009979840886944
Loss in iteration 69 : 0.40138309192447974
Loss in iteration 70 : 0.40180817524491125
Loss in iteration 71 : 0.4022995126971072
Loss in iteration 72 : 0.40285901873600366
Loss in iteration 73 : 0.403463970919652
Loss in iteration 74 : 0.404057866554443
Loss in iteration 75 : 0.40460369341350333
Loss in iteration 76 : 0.4051293826888573
Loss in iteration 77 : 0.4056481650873316
Loss in iteration 78 : 0.4061811531632067
Loss in iteration 79 : 0.40676261821014825
Loss in iteration 80 : 0.4073786552025974
Loss in iteration 81 : 0.40799928827401444
Loss in iteration 82 : 0.4086006332824645
Loss in iteration 83 : 0.40917838339668916
Loss in iteration 84 : 0.4097416445803234
Loss in iteration 85 : 0.4103101202568847
Loss in iteration 86 : 0.41089381878401826
Loss in iteration 87 : 0.41149097824447356
Loss in iteration 88 : 0.412099041908241
Loss in iteration 89 : 0.4127109567702869
Loss in iteration 90 : 0.41332384584620674
Loss in iteration 91 : 0.41393283429562155
Loss in iteration 92 : 0.4145409509908195
Loss in iteration 93 : 0.4151509685969119
Loss in iteration 94 : 0.41576571943664353
Loss in iteration 95 : 0.4163813040424932
Loss in iteration 96 : 0.41699905757124894
Loss in iteration 97 : 0.4176195360250552
Loss in iteration 98 : 0.41824087853262965
Loss in iteration 99 : 0.41886019434934596
Loss in iteration 100 : 0.41947784854284853
Loss in iteration 101 : 0.42009419159501293
Loss in iteration 102 : 0.4207095374609451
Loss in iteration 103 : 0.4213249155086545
Loss in iteration 104 : 0.42194093196556465
Loss in iteration 105 : 0.4225599875628202
Loss in iteration 106 : 0.42318193893954253
Loss in iteration 107 : 0.4238060522830294
Loss in iteration 108 : 0.4244320846402007
Loss in iteration 109 : 0.4250590631269944
Loss in iteration 110 : 0.4256872634389391
Loss in iteration 111 : 0.42631633818066816
Loss in iteration 112 : 0.42694570354567773
Loss in iteration 113 : 0.42757520180041947
Loss in iteration 114 : 0.4282046820196165
Loss in iteration 115 : 0.42883412449039815
Loss in iteration 116 : 0.42946078299753343
Loss in iteration 117 : 0.4300848768471506
Loss in iteration 118 : 0.43070593155782144
Loss in iteration 119 : 0.4313249262505459
Loss in iteration 120 : 0.43194312005370433
Loss in iteration 121 : 0.43256109353859035
Loss in iteration 122 : 0.4331788287217657
Loss in iteration 123 : 0.4337962942227098
Loss in iteration 124 : 0.43441272070308545
Loss in iteration 125 : 0.43502778496478245
Loss in iteration 126 : 0.43564105769610806
Loss in iteration 127 : 0.43625287995194245
Loss in iteration 128 : 0.43686195544339174
Loss in iteration 129 : 0.437468393130771
Loss in iteration 130 : 0.43807325710403805
Loss in iteration 131 : 0.4386776343406401
Loss in iteration 132 : 0.43928097542173195
Loss in iteration 133 : 0.43988352808115
Loss in iteration 134 : 0.4404858051325258
Loss in iteration 135 : 0.4410876899892902
Loss in iteration 136 : 0.4416891499307556
Loss in iteration 137 : 0.44228862197304997
Loss in iteration 138 : 0.4428854689193819
Loss in iteration 139 : 0.44348131820187325
Loss in iteration 140 : 0.4440757867604908
Loss in iteration 141 : 0.44466807340566705
Loss in iteration 142 : 0.4452578363261232
Loss in iteration 143 : 0.44584638867125337
Loss in iteration 144 : 0.4464335620784226
Loss in iteration 145 : 0.4470186815242514
Loss in iteration 146 : 0.4476020975165522
Loss in iteration 147 : 0.4481833355748686
Loss in iteration 148 : 0.44876327561595175
Loss in iteration 149 : 0.4493411481228233
Loss in iteration 150 : 0.4499173780085325
Loss in iteration 151 : 0.4504921505096482
Loss in iteration 152 : 0.4510656413045584
Loss in iteration 153 : 0.4516380083185737
Loss in iteration 154 : 0.4522081319436751
Loss in iteration 155 : 0.45277581055210814
Loss in iteration 156 : 0.4533410576090946
Loss in iteration 157 : 0.4539039662554739
Loss in iteration 158 : 0.45446478785938865
Loss in iteration 159 : 0.45502374892633907
Loss in iteration 160 : 0.45558105344967537
Loss in iteration 161 : 0.45613647240588534
Loss in iteration 162 : 0.4566903929651889
Loss in iteration 163 : 0.4572417904546588
Loss in iteration 164 : 0.4577900262644011
Loss in iteration 165 : 0.45833629262211206
Loss in iteration 166 : 0.4588806382477063
Loss in iteration 167 : 0.4594226869172463
Loss in iteration 168 : 0.45996168140309623
Loss in iteration 169 : 0.46049919818411045
Loss in iteration 170 : 0.46103379449766674
Loss in iteration 171 : 0.46156600345200477
Loss in iteration 172 : 0.4620960615337189
Loss in iteration 173 : 0.4626252554959319
Loss in iteration 174 : 0.46315232474433526
Loss in iteration 175 : 0.46367673891727673
Loss in iteration 176 : 0.46419817416489256
Loss in iteration 177 : 0.4647174866497433
Loss in iteration 178 : 0.465235387025676
Loss in iteration 179 : 0.4657516992603835
Loss in iteration 180 : 0.46626640722641577
Loss in iteration 181 : 0.4667787480433533
Loss in iteration 182 : 0.4672876130341258
Loss in iteration 183 : 0.4677945085258759
Loss in iteration 184 : 0.46830040043720733
Loss in iteration 185 : 0.4688058386000955
Loss in iteration 186 : 0.4693099973884039
Loss in iteration 187 : 0.4698115635103192
Loss in iteration 188 : 0.4703107690924407
Loss in iteration 189 : 0.4708082063051999
Loss in iteration 190 : 0.4713036268841076
Loss in iteration 191 : 0.4717974595222605
Loss in iteration 192 : 0.47228831594044474
Loss in iteration 193 : 0.4727764509145402
Loss in iteration 194 : 0.47326186412343324
Loss in iteration 195 : 0.47374663170651443
Loss in iteration 196 : 0.47422987586112664
Loss in iteration 197 : 0.4747103931237846
Loss in iteration 198 : 0.4751883331792303
Loss in iteration 199 : 0.4756638745786422
Loss in iteration 200 : 0.47613665545076955
Testing accuracy  of updater 6 on alg 1 with rate 0.00392 = 0.785, training accuracy 0.8397539656846876, time elapsed: 3304 millisecond.
Loss in iteration 1 : 1.0000092266028557
Loss in iteration 2 : 0.6640132163356238
Loss in iteration 3 : 0.5645820906045967
Loss in iteration 4 : 0.6448623944104328
Loss in iteration 5 : 0.6959275672609809
Loss in iteration 6 : 0.7134130958673741
Loss in iteration 7 : 0.7008180420535154
Loss in iteration 8 : 0.6617597367080118
Loss in iteration 9 : 0.6000670140172452
Loss in iteration 10 : 0.523601237849385
Loss in iteration 11 : 0.4552990721905433
Loss in iteration 12 : 0.4187018427847163
Loss in iteration 13 : 0.42994500581261125
Loss in iteration 14 : 0.4669122014696898
Loss in iteration 15 : 0.48878021792885346
Loss in iteration 16 : 0.48055024257008133
Loss in iteration 17 : 0.45007516617085636
Loss in iteration 18 : 0.41443939000794927
Loss in iteration 19 : 0.3898410115469819
Loss in iteration 20 : 0.3829388104487216
Loss in iteration 21 : 0.3890006476776905
Loss in iteration 22 : 0.4001953462979521
Loss in iteration 23 : 0.4089736079791733
Loss in iteration 24 : 0.41153764425961137
Loss in iteration 25 : 0.40779643589168746
Loss in iteration 26 : 0.399980745954915
Loss in iteration 27 : 0.3913450586248507
Loss in iteration 28 : 0.38466383309371954
Loss in iteration 29 : 0.38126858138485353
Loss in iteration 30 : 0.38197222373281015
Loss in iteration 31 : 0.3851349454726753
Loss in iteration 32 : 0.38895515510901224
Loss in iteration 33 : 0.3918774942819348
Loss in iteration 34 : 0.3927793278386368
Loss in iteration 35 : 0.3916014536453782
Loss in iteration 36 : 0.38924522305416026
Loss in iteration 37 : 0.3867182843761632
Loss in iteration 38 : 0.38471437958409993
Loss in iteration 39 : 0.3840386450881401
Loss in iteration 40 : 0.38456369490884007
Loss in iteration 41 : 0.38584380574047633
Loss in iteration 42 : 0.3871991870341762
Loss in iteration 43 : 0.388257293662342
Loss in iteration 44 : 0.3886943319175495
Loss in iteration 45 : 0.3884903126746205
Loss in iteration 46 : 0.3878751567751971
Loss in iteration 47 : 0.3871719762547239
Loss in iteration 48 : 0.38658132144242985
Loss in iteration 49 : 0.38638598864308926
Loss in iteration 50 : 0.3866760987528575
Loss in iteration 51 : 0.38724792955935006
Loss in iteration 52 : 0.38794614233676217
Loss in iteration 53 : 0.3885014687220951
Loss in iteration 54 : 0.3887854295685418
Loss in iteration 55 : 0.3887966669512886
Loss in iteration 56 : 0.38862601904282407
Loss in iteration 57 : 0.38848702449673345
Loss in iteration 58 : 0.3884739310845902
Loss in iteration 59 : 0.3886169628339918
Loss in iteration 60 : 0.3889360047574121
Loss in iteration 61 : 0.3893562177291523
Loss in iteration 62 : 0.389768887962545
Loss in iteration 63 : 0.39010477391886483
Loss in iteration 64 : 0.3903328754696576
Loss in iteration 65 : 0.39047445201859815
Loss in iteration 66 : 0.3906032766841907
Loss in iteration 67 : 0.39079275047695383
Loss in iteration 68 : 0.39106517520383427
Loss in iteration 69 : 0.3913965938193745
Loss in iteration 70 : 0.3917604950946091
Loss in iteration 71 : 0.39212962894910325
Loss in iteration 72 : 0.3924745472134064
Loss in iteration 73 : 0.3927728871747156
Loss in iteration 74 : 0.3930387888950687
Loss in iteration 75 : 0.3933072846350171
Loss in iteration 76 : 0.3935848813881401
Loss in iteration 77 : 0.3938904950606873
Loss in iteration 78 : 0.39423107324879497
Loss in iteration 79 : 0.3945884105025516
Loss in iteration 80 : 0.3949519990945113
Loss in iteration 81 : 0.39531917813583045
Loss in iteration 82 : 0.39567945612759103
Loss in iteration 83 : 0.396030375348637
Loss in iteration 84 : 0.39637268832756717
Loss in iteration 85 : 0.3967132848472796
Loss in iteration 86 : 0.3970655766311966
Loss in iteration 87 : 0.3974408206235511
Loss in iteration 88 : 0.3978230283645944
Loss in iteration 89 : 0.39821093588194356
Loss in iteration 90 : 0.39859750338925287
Loss in iteration 91 : 0.39897874517004744
Loss in iteration 92 : 0.3993562461103737
Loss in iteration 93 : 0.3997348790267875
Loss in iteration 94 : 0.40011970734481545
Loss in iteration 95 : 0.40050913625665796
Loss in iteration 96 : 0.4009040202777383
Loss in iteration 97 : 0.40130115191976773
Loss in iteration 98 : 0.40169937482948515
Loss in iteration 99 : 0.4020991150491969
Loss in iteration 100 : 0.40250010572957584
Loss in iteration 101 : 0.4029023241103564
Loss in iteration 102 : 0.40330529331444426
Loss in iteration 103 : 0.40370908951314977
Loss in iteration 104 : 0.40411510698915537
Loss in iteration 105 : 0.4045225568319779
Loss in iteration 106 : 0.4049323561236735
Loss in iteration 107 : 0.40534474080158417
Loss in iteration 108 : 0.40575740617043565
Loss in iteration 109 : 0.4061704663071894
Loss in iteration 110 : 0.4065829161137114
Loss in iteration 111 : 0.4069949200742674
Loss in iteration 112 : 0.4074075652655169
Loss in iteration 113 : 0.4078206696926991
Loss in iteration 114 : 0.4082352960793463
Loss in iteration 115 : 0.40865026889551426
Loss in iteration 116 : 0.40906523086329355
Loss in iteration 117 : 0.409480288223262
Loss in iteration 118 : 0.40989559440569373
Loss in iteration 119 : 0.4103110516244256
Loss in iteration 120 : 0.41072671508043346
Loss in iteration 121 : 0.41114252903501686
Loss in iteration 122 : 0.41155860094395424
Loss in iteration 123 : 0.4119750269152323
Loss in iteration 124 : 0.4123918927736408
Loss in iteration 125 : 0.4128091382327293
Loss in iteration 126 : 0.41322669923707234
Loss in iteration 127 : 0.41364462583609624
Loss in iteration 128 : 0.41406335741156464
Loss in iteration 129 : 0.41448284741157826
Loss in iteration 130 : 0.41490313103655097
Loss in iteration 131 : 0.4153240084769333
Loss in iteration 132 : 0.4157452406066481
Loss in iteration 133 : 0.41616574955946195
Loss in iteration 134 : 0.4165861804415889
Loss in iteration 135 : 0.41700673179438036
Loss in iteration 136 : 0.4174280742124485
Loss in iteration 137 : 0.4178501682047407
Loss in iteration 138 : 0.4182723850291006
Loss in iteration 139 : 0.4186941900461977
Loss in iteration 140 : 0.4191156125476399
Loss in iteration 141 : 0.4195365833539727
Loss in iteration 142 : 0.41995673853587057
Loss in iteration 143 : 0.4203757723698049
Loss in iteration 144 : 0.42079352894543615
Loss in iteration 145 : 0.42120940594865663
Loss in iteration 146 : 0.42162416870295183
Loss in iteration 147 : 0.42203943747131595
Loss in iteration 148 : 0.4224561056174536
Loss in iteration 149 : 0.42287238513474584
Loss in iteration 150 : 0.42328787571835075
Loss in iteration 151 : 0.4237029089455326
Loss in iteration 152 : 0.4241173868195083
Loss in iteration 153 : 0.4245309139472573
Loss in iteration 154 : 0.4249434750227196
Loss in iteration 155 : 0.42535520861181864
Loss in iteration 156 : 0.4257662550588098
Loss in iteration 157 : 0.4261767408115435
Loss in iteration 158 : 0.4265867797152455
Loss in iteration 159 : 0.4269964741889583
Loss in iteration 160 : 0.427405916294786
Loss in iteration 161 : 0.42781518870931157
Loss in iteration 162 : 0.4282243656057992
Loss in iteration 163 : 0.4286335134551058
Loss in iteration 164 : 0.42904269175257004
Loss in iteration 165 : 0.42945195367753464
Loss in iteration 166 : 0.4298613466915956
Loss in iteration 167 : 0.4302709130811432
Loss in iteration 168 : 0.4306806904492833
Loss in iteration 169 : 0.43109071216177064
Loss in iteration 170 : 0.4315010077511853
Loss in iteration 171 : 0.4319116032832023
Loss in iteration 172 : 0.43232252168845886
Loss in iteration 173 : 0.4327334053739182
Loss in iteration 174 : 0.43314417782487546
Loss in iteration 175 : 0.43355486481063843
Loss in iteration 176 : 0.4339655217543121
Loss in iteration 177 : 0.4343761981881754
Loss in iteration 178 : 0.43478693831513887
Loss in iteration 179 : 0.4351973261537316
Loss in iteration 180 : 0.43560748193070287
Loss in iteration 181 : 0.43601786712139984
Loss in iteration 182 : 0.4364285840251245
Loss in iteration 183 : 0.43683958318938426
Loss in iteration 184 : 0.43725088013633856
Loss in iteration 185 : 0.43766235816460863
Loss in iteration 186 : 0.4380738350107815
Loss in iteration 187 : 0.4384852471433057
Loss in iteration 188 : 0.4388966407737721
Loss in iteration 189 : 0.43930768544806686
Loss in iteration 190 : 0.4397185432163811
Loss in iteration 191 : 0.4401291996042095
Loss in iteration 192 : 0.4405397103181071
Loss in iteration 193 : 0.4409501251831752
Loss in iteration 194 : 0.44136048870868533
Loss in iteration 195 : 0.441770840599927
Loss in iteration 196 : 0.44218121622133266
Loss in iteration 197 : 0.44259164701546816
Loss in iteration 198 : 0.44300216088205546
Loss in iteration 199 : 0.44341222992717155
Loss in iteration 200 : 0.4438217799204054
Testing accuracy  of updater 6 on alg 1 with rate 0.002744 = 0.78475, training accuracy 0.8400776950469407, time elapsed: 2885 millisecond.
Loss in iteration 1 : 1.0000033760355995
Loss in iteration 2 : 0.7828327673887561
Loss in iteration 3 : 0.560413985809913
Loss in iteration 4 : 0.5761769147460598
Loss in iteration 5 : 0.6327763353280158
Loss in iteration 6 : 0.6701950299029861
Loss in iteration 7 : 0.6870175632242996
Loss in iteration 8 : 0.6851945417875459
Loss in iteration 9 : 0.6668329938848316
Loss in iteration 10 : 0.6340026392136805
Loss in iteration 11 : 0.5890259047808165
Loss in iteration 12 : 0.5364106099638227
Loss in iteration 13 : 0.48732689589079026
Loss in iteration 14 : 0.4522282073112976
Loss in iteration 15 : 0.43727622467273103
Loss in iteration 16 : 0.4401505730234032
Loss in iteration 17 : 0.45589812406843633
Loss in iteration 18 : 0.4706476321047336
Loss in iteration 19 : 0.4733768935755426
Loss in iteration 20 : 0.46248466881542094
Loss in iteration 21 : 0.4427892250325828
Loss in iteration 22 : 0.4208768507049247
Loss in iteration 23 : 0.40324963655213436
Loss in iteration 24 : 0.3933126264777439
Loss in iteration 25 : 0.39136357280454603
Loss in iteration 26 : 0.3944514884563772
Loss in iteration 27 : 0.3990288722182333
Loss in iteration 28 : 0.40228170810098834
Loss in iteration 29 : 0.4026876638770594
Loss in iteration 30 : 0.40015364441662543
Loss in iteration 31 : 0.3956390742014526
Loss in iteration 32 : 0.39054699444600255
Loss in iteration 33 : 0.38611090214235194
Loss in iteration 34 : 0.38293375154299747
Loss in iteration 35 : 0.3815451665780397
Loss in iteration 36 : 0.3815204507158169
Loss in iteration 37 : 0.3823947776649535
Loss in iteration 38 : 0.3836338988672799
Loss in iteration 39 : 0.38475596789102706
Loss in iteration 40 : 0.38530826046407657
Loss in iteration 41 : 0.3851334562906041
Loss in iteration 42 : 0.38439522357601913
Loss in iteration 43 : 0.3833306384966703
Loss in iteration 44 : 0.3822044564404225
Loss in iteration 45 : 0.3812839212976225
Loss in iteration 46 : 0.38074915384697783
Loss in iteration 47 : 0.38064218980508446
Loss in iteration 48 : 0.38088350160933504
Loss in iteration 49 : 0.3813001283029338
Loss in iteration 50 : 0.3817384379975831
Loss in iteration 51 : 0.38210011319437825
Loss in iteration 52 : 0.38228906466906476
Loss in iteration 53 : 0.38229213366081893
Loss in iteration 54 : 0.38214190810492643
Loss in iteration 55 : 0.3819340279579881
Loss in iteration 56 : 0.3817360140457729
Loss in iteration 57 : 0.38160052851219506
Loss in iteration 58 : 0.3815949764699852
Loss in iteration 59 : 0.38173464699508175
Loss in iteration 60 : 0.38195162490500884
Loss in iteration 61 : 0.3821937678674594
Loss in iteration 62 : 0.38242298212300185
Loss in iteration 63 : 0.382601231239049
Loss in iteration 64 : 0.38271760773996233
Loss in iteration 65 : 0.3827833544066076
Loss in iteration 66 : 0.3828140486029437
Loss in iteration 67 : 0.38283485549660656
Loss in iteration 68 : 0.38287526390677534
Loss in iteration 69 : 0.38296595592661214
Loss in iteration 70 : 0.38308239003754624
Loss in iteration 71 : 0.383236495097219
Loss in iteration 72 : 0.3834113641197897
Loss in iteration 73 : 0.38359291107501625
Loss in iteration 74 : 0.383757175626558
Loss in iteration 75 : 0.3838991681429605
Loss in iteration 76 : 0.3840220388034197
Loss in iteration 77 : 0.384137069488946
Loss in iteration 78 : 0.3842526850945389
Loss in iteration 79 : 0.38437497127423437
Loss in iteration 80 : 0.3845125431439431
Loss in iteration 81 : 0.3846661628055987
Loss in iteration 82 : 0.38482809530952583
Loss in iteration 83 : 0.38499730712925534
Loss in iteration 84 : 0.3851658033819552
Loss in iteration 85 : 0.3853271448363004
Loss in iteration 86 : 0.385480536969109
Loss in iteration 87 : 0.3856277241241886
Loss in iteration 88 : 0.38577298898091306
Loss in iteration 89 : 0.3859232457819005
Loss in iteration 90 : 0.38608006025077646
Loss in iteration 91 : 0.3862447234066773
Loss in iteration 92 : 0.3864131308386342
Loss in iteration 93 : 0.386582713846512
Loss in iteration 94 : 0.38675168697098555
Loss in iteration 95 : 0.3869204335499672
Loss in iteration 96 : 0.387088085138535
Loss in iteration 97 : 0.38725488002659914
Loss in iteration 98 : 0.3874220891017251
Loss in iteration 99 : 0.38759050437113707
Loss in iteration 100 : 0.3877605952699336
Loss in iteration 101 : 0.3879327716153308
Loss in iteration 102 : 0.3881077382225744
Loss in iteration 103 : 0.3882837648187722
Loss in iteration 104 : 0.38846040918487534
Loss in iteration 105 : 0.38863723804090233
Loss in iteration 106 : 0.38881430069234235
Loss in iteration 107 : 0.38899151682219074
Loss in iteration 108 : 0.38916820179945666
Loss in iteration 109 : 0.38934521455493865
Loss in iteration 110 : 0.3895237437646871
Loss in iteration 111 : 0.38970369326044163
Loss in iteration 112 : 0.3898847618950701
Loss in iteration 113 : 0.3900668056371013
Loss in iteration 114 : 0.3902499526105827
Loss in iteration 115 : 0.39043373517051977
Loss in iteration 116 : 0.39061773677357
Loss in iteration 117 : 0.39080202078021015
Loss in iteration 118 : 0.39098664399197475
Loss in iteration 119 : 0.3911719110112687
Loss in iteration 120 : 0.3913583594061594
Loss in iteration 121 : 0.39154729257654763
Loss in iteration 122 : 0.39173628201007193
Loss in iteration 123 : 0.39192508239529006
Loss in iteration 124 : 0.39211366978914286
Loss in iteration 125 : 0.3923024655556107
Loss in iteration 126 : 0.39249235478715333
Loss in iteration 127 : 0.3926837554468891
Loss in iteration 128 : 0.3928760397752116
Loss in iteration 129 : 0.393068450377761
Loss in iteration 130 : 0.3932611441422162
Loss in iteration 131 : 0.3934542209849137
Loss in iteration 132 : 0.3936475369661938
Loss in iteration 133 : 0.3938415431563706
Loss in iteration 134 : 0.39403618263903567
Loss in iteration 135 : 0.3942308850466663
Loss in iteration 136 : 0.3944259339728641
Loss in iteration 137 : 0.3946215123918541
Loss in iteration 138 : 0.39481762826895517
Loss in iteration 139 : 0.3950142220772404
Loss in iteration 140 : 0.39521131613158894
Loss in iteration 141 : 0.395408930343511
Loss in iteration 142 : 0.39560708244529985
Loss in iteration 143 : 0.3958057881936425
Loss in iteration 144 : 0.39600506155453025
Loss in iteration 145 : 0.3962049148711416
Loss in iteration 146 : 0.3964053590162404
Loss in iteration 147 : 0.39660640353048493
Testing accuracy  of updater 6 on alg 1 with rate 0.001568 = 0.78325, training accuracy 0.8397539656846876, time elapsed: 2138 millisecond.
Loss in iteration 1 : 1.0000002051660268
Loss in iteration 2 : 0.9476379497537284
Loss in iteration 3 : 0.854224313103523
Loss in iteration 4 : 0.7321342988093641
Loss in iteration 5 : 0.6247082620891804
Loss in iteration 6 : 0.5644197650569943
Loss in iteration 7 : 0.5521420365054435
Loss in iteration 8 : 0.5634818201006233
Loss in iteration 9 : 0.5814231632464805
Loss in iteration 10 : 0.59785760714281
Loss in iteration 11 : 0.6101324479509556
Testing accuracy  of updater 6 on alg 1 with rate 3.92E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 170 millisecond.
Loss in iteration 1 : 1.0277983379501385
Loss in iteration 2 : 5.792035898406908
Loss in iteration 3 : 6.369673095624089
Loss in iteration 4 : 5.021904095275305
Loss in iteration 5 : 2.9112169604429123
Loss in iteration 6 : 0.8995874095725237
Loss in iteration 7 : 1.3445997348275003
Loss in iteration 8 : 1.8153791843199611
Loss in iteration 9 : 0.9741098410546216
Loss in iteration 10 : 0.6460234128165113
Loss in iteration 11 : 0.8977067265725964
Loss in iteration 12 : 0.822031531418568
Loss in iteration 13 : 0.5489882031703628
Loss in iteration 14 : 0.600638560856053
Loss in iteration 15 : 0.6014483033894891
Loss in iteration 16 : 0.46116144963614725
Loss in iteration 17 : 0.5227225593145972
Loss in iteration 18 : 0.452791479085541
Loss in iteration 19 : 0.4715849166881607
Loss in iteration 20 : 0.43369945746402855
Loss in iteration 21 : 0.4609075402976286
Loss in iteration 22 : 0.4374641003103306
Loss in iteration 23 : 0.4495825297749484
Loss in iteration 24 : 0.46192743895171584
Loss in iteration 25 : 0.45147233753231264
Loss in iteration 26 : 0.44788409624033
Loss in iteration 27 : 0.4526725733782219
Loss in iteration 28 : 0.45476178024622294
Loss in iteration 29 : 0.4521908304451393
Loss in iteration 30 : 0.45112421816588916
Loss in iteration 31 : 0.45277520058261633
Loss in iteration 32 : 0.45243963865383685
Loss in iteration 33 : 0.45214423986118785
Loss in iteration 34 : 0.4528838205072674
Testing accuracy  of updater 7 on alg 1 with rate 0.22400000000000003 = 0.77975, training accuracy 0.8394302363224344, time elapsed: 527 millisecond.
Loss in iteration 1 : 1.0022687281895528
Loss in iteration 2 : 1.0920131359154048
Loss in iteration 3 : 1.3261322430075002
Loss in iteration 4 : 1.1761835362158815
Loss in iteration 5 : 0.7390857441705969
Loss in iteration 6 : 0.4581637395808266
Loss in iteration 7 : 0.7439120898877946
Loss in iteration 8 : 0.6641727665450222
Loss in iteration 9 : 0.46574932726475493
Loss in iteration 10 : 0.5219186107424776
Loss in iteration 11 : 0.5740414892778809
Loss in iteration 12 : 0.49291933877906396
Loss in iteration 13 : 0.4639663963004587
Loss in iteration 14 : 0.5020098199852465
Loss in iteration 15 : 0.4711419928957443
Loss in iteration 16 : 0.44102881281572026
Loss in iteration 17 : 0.4600518771670046
Loss in iteration 18 : 0.44380139083418346
Loss in iteration 19 : 0.43636504004481713
Loss in iteration 20 : 0.4461639718590611
Loss in iteration 21 : 0.43209688845825855
Loss in iteration 22 : 0.44391076650744976
Loss in iteration 23 : 0.438022201699829
Loss in iteration 24 : 0.4470761628751863
Loss in iteration 25 : 0.4432461668055141
Loss in iteration 26 : 0.44981999589987265
Loss in iteration 27 : 0.44816180206754846
Loss in iteration 28 : 0.45133437021764
Loss in iteration 29 : 0.4502358686184839
Loss in iteration 30 : 0.4517308437399561
Loss in iteration 31 : 0.45178112321754926
Loss in iteration 32 : 0.4524082034634718
Loss in iteration 33 : 0.4525649136831884
Loss in iteration 34 : 0.45327474618865554
Loss in iteration 35 : 0.45369952794256263
Loss in iteration 36 : 0.4538935174090889
Loss in iteration 37 : 0.45421983217361117
Loss in iteration 38 : 0.45465293211166047
Loss in iteration 39 : 0.45507218685269263
Loss in iteration 40 : 0.4554588846750499
Testing accuracy  of updater 7 on alg 1 with rate 0.15680000000000002 = 0.781, training accuracy 0.840401424409194, time elapsed: 620 millisecond.
Loss in iteration 1 : 1.0007582614876036
Loss in iteration 2 : 0.76099926353857
Loss in iteration 3 : 0.9153805029064956
Loss in iteration 4 : 0.8607998154962054
Loss in iteration 5 : 0.6365560979352572
Loss in iteration 6 : 0.4016969205333288
Loss in iteration 7 : 0.554376829221059
Loss in iteration 8 : 0.6043207851681606
Loss in iteration 9 : 0.4775040416851812
Loss in iteration 10 : 0.4175687541183509
Loss in iteration 11 : 0.4795789253189593
Loss in iteration 12 : 0.5051931940819543
Loss in iteration 13 : 0.4592714658023359
Loss in iteration 14 : 0.43062432181165744
Loss in iteration 15 : 0.4551415300010089
Loss in iteration 16 : 0.47019213150255973
Loss in iteration 17 : 0.446589825933967
Loss in iteration 18 : 0.4295227620064436
Loss in iteration 19 : 0.4411747507107729
Loss in iteration 20 : 0.4464163525681871
Loss in iteration 21 : 0.43417780604910533
Loss in iteration 22 : 0.4306488708809562
Loss in iteration 23 : 0.43960040556684904
Loss in iteration 24 : 0.43684360941850847
Loss in iteration 25 : 0.431393914767749
Loss in iteration 26 : 0.4376237601692238
Loss in iteration 27 : 0.43986889983427174
Loss in iteration 28 : 0.4377044165924607
Loss in iteration 29 : 0.443089812445306
Loss in iteration 30 : 0.4440178574813163
Loss in iteration 31 : 0.4438231502398674
Loss in iteration 32 : 0.44833911620902783
Loss in iteration 33 : 0.447913537498156
Loss in iteration 34 : 0.4496516208747618
Loss in iteration 35 : 0.4510208472049288
Loss in iteration 36 : 0.4502659978323484
Loss in iteration 37 : 0.4518733029575934
Loss in iteration 38 : 0.4522171578803512
Loss in iteration 39 : 0.45230710119796774
Loss in iteration 40 : 0.4533684966754753
Loss in iteration 41 : 0.4530665855779628
Loss in iteration 42 : 0.45331314351145846
Loss in iteration 43 : 0.4540753458099418
Loss in iteration 44 : 0.4542039195659308
Loss in iteration 45 : 0.45468016312091086
Loss in iteration 46 : 0.45501571950184805
Loss in iteration 47 : 0.4550942086197596
Loss in iteration 48 : 0.45549070532282737
Testing accuracy  of updater 7 on alg 1 with rate 0.08960000000000001 = 0.78125, training accuracy 0.840401424409194, time elapsed: 714 millisecond.
Loss in iteration 1 : 1.0000484737848663
Loss in iteration 2 : 0.6282959345103251
Loss in iteration 3 : 0.6055675426089482
Loss in iteration 4 : 0.6928973849060464
Loss in iteration 5 : 0.7267088684133959
Loss in iteration 6 : 0.7114013452989096
Loss in iteration 7 : 0.6530002171233503
Loss in iteration 8 : 0.5597684051023183
Loss in iteration 9 : 0.4680079910424537
Loss in iteration 10 : 0.4362875030986326
Loss in iteration 11 : 0.47721798635523605
Loss in iteration 12 : 0.5199638282128998
Loss in iteration 13 : 0.5094582912141086
Loss in iteration 14 : 0.4598474015407611
Loss in iteration 15 : 0.41354998875719357
Loss in iteration 16 : 0.4005174063987795
Loss in iteration 17 : 0.4175469589461637
Loss in iteration 18 : 0.43598082998099436
Loss in iteration 19 : 0.4394196608210117
Loss in iteration 20 : 0.4275138324424306
Loss in iteration 21 : 0.4100517697570291
Loss in iteration 22 : 0.40070519528620396
Loss in iteration 23 : 0.40232070501282996
Loss in iteration 24 : 0.4105672116984943
Loss in iteration 25 : 0.4180476487257825
Loss in iteration 26 : 0.4194954135927448
Loss in iteration 27 : 0.4151678660523176
Loss in iteration 28 : 0.4094748204758167
Loss in iteration 29 : 0.40622853847385476
Loss in iteration 30 : 0.4072214156931014
Loss in iteration 31 : 0.41068613460577
Loss in iteration 32 : 0.4141454027433964
Loss in iteration 33 : 0.415480690125956
Loss in iteration 34 : 0.4146232438351907
Loss in iteration 35 : 0.4129805198939405
Loss in iteration 36 : 0.41244536657581293
Loss in iteration 37 : 0.41363071359738235
Loss in iteration 38 : 0.4157972767402396
Loss in iteration 39 : 0.41787356514828083
Loss in iteration 40 : 0.4188811410493554
Loss in iteration 41 : 0.41886253942445156
Loss in iteration 42 : 0.41856561462893016
Loss in iteration 43 : 0.4186573008218205
Loss in iteration 44 : 0.419451548962663
Loss in iteration 45 : 0.42077743769358517
Loss in iteration 46 : 0.42224217027407657
Loss in iteration 47 : 0.4232074560846577
Loss in iteration 48 : 0.423703656824557
Loss in iteration 49 : 0.4242447824170056
Loss in iteration 50 : 0.42514258372501273
Loss in iteration 51 : 0.426294769328701
Loss in iteration 52 : 0.42747553844305036
Loss in iteration 53 : 0.4284675045045785
Loss in iteration 54 : 0.42920083271074416
Loss in iteration 55 : 0.4298219836062595
Loss in iteration 56 : 0.43052607359750983
Loss in iteration 57 : 0.43142246205842677
Loss in iteration 58 : 0.43243801251651154
Loss in iteration 59 : 0.4333778957481785
Loss in iteration 60 : 0.43420220181922814
Loss in iteration 61 : 0.4349527839736347
Loss in iteration 62 : 0.4357134892284151
Loss in iteration 63 : 0.43654706340904015
Loss in iteration 64 : 0.4373794560969274
Loss in iteration 65 : 0.4380913829203138
Loss in iteration 66 : 0.43868610827618343
Loss in iteration 67 : 0.4392654565048115
Loss in iteration 68 : 0.4399222229797593
Loss in iteration 69 : 0.4405964701673685
Loss in iteration 70 : 0.4412454302330621
Loss in iteration 71 : 0.4418620024527514
Loss in iteration 72 : 0.44245479779269814
Loss in iteration 73 : 0.443046435692235
Loss in iteration 74 : 0.4436254246135244
Loss in iteration 75 : 0.44415261772530845
Loss in iteration 76 : 0.4446270564024145
Loss in iteration 77 : 0.44509267482976944
Loss in iteration 78 : 0.44557933075683187
Loss in iteration 79 : 0.4460706651577446
Loss in iteration 80 : 0.44655413251690745
Loss in iteration 81 : 0.44702254257751434
Loss in iteration 82 : 0.44748016364555027
Loss in iteration 83 : 0.4479299059836999
Loss in iteration 84 : 0.44836660996733674
Loss in iteration 85 : 0.4487846351430431
Loss in iteration 86 : 0.44918700984537446
Loss in iteration 87 : 0.4495802945418958
Loss in iteration 88 : 0.44996536558226535
Loss in iteration 89 : 0.4503419720481765
Loss in iteration 90 : 0.45071456977375773
Loss in iteration 91 : 0.4510803223050407
Loss in iteration 92 : 0.4514340929866276
Loss in iteration 93 : 0.4517798167768602
Loss in iteration 94 : 0.45211843276167923
Testing accuracy  of updater 7 on alg 1 with rate 0.022400000000000003 = 0.781, training accuracy 0.8413726124959534, time elapsed: 1594 millisecond.
Loss in iteration 1 : 1.0000258566063296
Loss in iteration 2 : 0.7181881751546563
Loss in iteration 3 : 0.5648796270369999
Loss in iteration 4 : 0.6426815294244942
Loss in iteration 5 : 0.6938081829356346
Loss in iteration 6 : 0.7080541332582105
Loss in iteration 7 : 0.6891651470398047
Loss in iteration 8 : 0.6411121255317382
Loss in iteration 9 : 0.5692356532921207
Loss in iteration 10 : 0.4936691414517211
Loss in iteration 11 : 0.4503272013829466
Loss in iteration 12 : 0.4503147912851142
Loss in iteration 13 : 0.48203294263594837
Loss in iteration 14 : 0.505197761372215
Loss in iteration 15 : 0.4960170475073009
Loss in iteration 16 : 0.4624538065949723
Loss in iteration 17 : 0.4262795321437828
Loss in iteration 18 : 0.40548671607265946
Loss in iteration 19 : 0.40680192916204366
Loss in iteration 20 : 0.41853451557641985
Loss in iteration 21 : 0.4276403234508306
Loss in iteration 22 : 0.4280817619945288
Loss in iteration 23 : 0.4200663782869141
Loss in iteration 24 : 0.4084137052341894
Loss in iteration 25 : 0.40044498921278426
Loss in iteration 26 : 0.39912992043423173
Loss in iteration 27 : 0.4024775920393112
Loss in iteration 28 : 0.40741623255632853
Loss in iteration 29 : 0.4111034611548514
Loss in iteration 30 : 0.41137852597153646
Loss in iteration 31 : 0.40882205359096263
Loss in iteration 32 : 0.4054861918534272
Loss in iteration 33 : 0.40341379612628814
Loss in iteration 34 : 0.4031011194744321
Loss in iteration 35 : 0.4044700004031244
Loss in iteration 36 : 0.40667436582848643
Loss in iteration 37 : 0.40861206534083344
Loss in iteration 38 : 0.40949408089870143
Loss in iteration 39 : 0.40931927258269835
Loss in iteration 40 : 0.4087566358118758
Loss in iteration 41 : 0.40848141671732774
Loss in iteration 42 : 0.40897132105084033
Loss in iteration 43 : 0.4100802585771839
Loss in iteration 44 : 0.41144385390545113
Loss in iteration 45 : 0.41269530312412334
Loss in iteration 46 : 0.4135842423954631
Loss in iteration 47 : 0.4140842814426119
Loss in iteration 48 : 0.4143297385754963
Loss in iteration 49 : 0.4145525847801888
Loss in iteration 50 : 0.41494014290131903
Loss in iteration 51 : 0.4155918150774048
Loss in iteration 52 : 0.41646257158873673
Loss in iteration 53 : 0.417426974540078
Loss in iteration 54 : 0.4183372049267716
Loss in iteration 55 : 0.4190751486686359
Loss in iteration 56 : 0.4196661074764727
Loss in iteration 57 : 0.4202396805231956
Loss in iteration 58 : 0.42090331186261953
Loss in iteration 59 : 0.4217041550095886
Loss in iteration 60 : 0.42253988402698905
Loss in iteration 61 : 0.42336435247338045
Loss in iteration 62 : 0.42413502824318694
Loss in iteration 63 : 0.42482912352680235
Loss in iteration 64 : 0.42546261185891004
Loss in iteration 65 : 0.4260842471907087
Loss in iteration 66 : 0.4267123545256051
Loss in iteration 67 : 0.42738072045484
Loss in iteration 68 : 0.42810280910348647
Loss in iteration 69 : 0.4288392470626306
Loss in iteration 70 : 0.42954221059236464
Loss in iteration 71 : 0.43020139795295304
Loss in iteration 72 : 0.43083323778343696
Loss in iteration 73 : 0.43147692715117986
Loss in iteration 74 : 0.4321338094601596
Loss in iteration 75 : 0.4327858151418557
Loss in iteration 76 : 0.433413930405175
Loss in iteration 77 : 0.43399480033975907
Loss in iteration 78 : 0.43453809325104414
Loss in iteration 79 : 0.4350778353714609
Loss in iteration 80 : 0.4356218970024249
Loss in iteration 81 : 0.43618655096634545
Loss in iteration 82 : 0.4367477614586883
Loss in iteration 83 : 0.4372957330889733
Loss in iteration 84 : 0.43783121333229225
Loss in iteration 85 : 0.4383543109776065
Loss in iteration 86 : 0.43887516283256595
Loss in iteration 87 : 0.4393897316387097
Loss in iteration 88 : 0.4398874850115243
Loss in iteration 89 : 0.44036450968605007
Loss in iteration 90 : 0.44082139111090424
Loss in iteration 91 : 0.4412692152581164
Loss in iteration 92 : 0.44171538667035537
Loss in iteration 93 : 0.4421593987775267
Loss in iteration 94 : 0.44259861062129546
Loss in iteration 95 : 0.44303112249112825
Loss in iteration 96 : 0.44345606653892367
Loss in iteration 97 : 0.4438741154576622
Loss in iteration 98 : 0.44428300477457516
Loss in iteration 99 : 0.4446839501341189
Loss in iteration 100 : 0.44507609298408896
Loss in iteration 101 : 0.4454581528960503
Loss in iteration 102 : 0.44583083182735983
Loss in iteration 103 : 0.44619540183638035
Loss in iteration 104 : 0.44655238333384456
Loss in iteration 105 : 0.44690227235560026
Loss in iteration 106 : 0.44724665623314996
Loss in iteration 107 : 0.4475864222744103
Loss in iteration 108 : 0.4479236066678708
Loss in iteration 109 : 0.44825580721243063
Testing accuracy  of updater 7 on alg 1 with rate 0.015680000000000003 = 0.78025, training accuracy 0.8413726124959534, time elapsed: 1683 millisecond.
Loss in iteration 1 : 1.0000087383664082
Loss in iteration 2 : 0.8350563744950419
Loss in iteration 3 : 0.602085807985098
Loss in iteration 4 : 0.5702394983370352
Loss in iteration 5 : 0.6255610967356542
Loss in iteration 6 : 0.6666434006456262
Loss in iteration 7 : 0.6858181176601759
Loss in iteration 8 : 0.6850570922933308
Loss in iteration 9 : 0.6664345664802793
Loss in iteration 10 : 0.6320942644079038
Loss in iteration 11 : 0.5846424690228197
Loss in iteration 12 : 0.5314579628293462
Loss in iteration 13 : 0.4884780732712685
Loss in iteration 14 : 0.4679102980970707
Loss in iteration 15 : 0.46732134739269643
Loss in iteration 16 : 0.4771980185963619
Loss in iteration 17 : 0.48967766221133674
Loss in iteration 18 : 0.49572002249838704
Loss in iteration 19 : 0.48863376597337177
Loss in iteration 20 : 0.4704323324836268
Loss in iteration 21 : 0.4485357078454337
Loss in iteration 22 : 0.43019137405959745
Loss in iteration 23 : 0.420151095880533
Loss in iteration 24 : 0.4177432232038976
Loss in iteration 25 : 0.42014000665367435
Loss in iteration 26 : 0.42337954722471527
Loss in iteration 27 : 0.4248011321872126
Loss in iteration 28 : 0.4231771068054372
Loss in iteration 29 : 0.41870628648104063
Loss in iteration 30 : 0.4127963903028346
Loss in iteration 31 : 0.40706266025489884
Loss in iteration 32 : 0.40323419043268355
Loss in iteration 33 : 0.40223969124878145
Loss in iteration 34 : 0.40311613508040395
Loss in iteration 35 : 0.40490846662735835
Loss in iteration 36 : 0.4064383518364188
Loss in iteration 37 : 0.4069795676115331
Loss in iteration 38 : 0.40644591072693265
Loss in iteration 39 : 0.4051427263991805
Loss in iteration 40 : 0.4037238251184856
Loss in iteration 41 : 0.4025735927050263
Loss in iteration 42 : 0.40204480744888993
Loss in iteration 43 : 0.40213297752684884
Loss in iteration 44 : 0.40271052412596486
Loss in iteration 45 : 0.40351039678116485
Loss in iteration 46 : 0.40428565475275524
Loss in iteration 47 : 0.4048693120919292
Loss in iteration 48 : 0.4051840223434329
Loss in iteration 49 : 0.4052903305413261
Loss in iteration 50 : 0.40529440084863017
Loss in iteration 51 : 0.4053594846209696
Loss in iteration 52 : 0.4055734509934166
Loss in iteration 53 : 0.4059767496909263
Loss in iteration 54 : 0.40653121035142975
Loss in iteration 55 : 0.40714278834038264
Loss in iteration 56 : 0.40774494128473887
Loss in iteration 57 : 0.40829809501614095
Loss in iteration 58 : 0.40876791364596626
Loss in iteration 59 : 0.40916071472454574
Loss in iteration 60 : 0.40949815928519184
Loss in iteration 61 : 0.409827612847147
Loss in iteration 62 : 0.4101700229700973
Loss in iteration 63 : 0.41054356307715706
Loss in iteration 64 : 0.4109632183212546
Loss in iteration 65 : 0.4114192250504405
Loss in iteration 66 : 0.4119207468414759
Loss in iteration 67 : 0.4124409339280617
Loss in iteration 68 : 0.4129478748815535
Loss in iteration 69 : 0.4134257126919121
Loss in iteration 70 : 0.4138748895718025
Loss in iteration 71 : 0.4143244427726309
Loss in iteration 72 : 0.4147927904597982
Loss in iteration 73 : 0.41527270064729027
Loss in iteration 74 : 0.4157609237582107
Loss in iteration 75 : 0.4162527328278586
Loss in iteration 76 : 0.4167392815662115
Loss in iteration 77 : 0.41721837072002826
Loss in iteration 78 : 0.4176891353925286
Loss in iteration 79 : 0.41815052075157233
Loss in iteration 80 : 0.41860125438962636
Loss in iteration 81 : 0.419045596873802
Loss in iteration 82 : 0.4194857314837787
Loss in iteration 83 : 0.4199231768416028
Loss in iteration 84 : 0.4203636514374039
Loss in iteration 85 : 0.4208107105603722
Loss in iteration 86 : 0.4212642263022267
Loss in iteration 87 : 0.4217185606544218
Loss in iteration 88 : 0.42216951615099696
Loss in iteration 89 : 0.4226162785986261
Loss in iteration 90 : 0.4230595654280543
Loss in iteration 91 : 0.4234993047732092
Loss in iteration 92 : 0.42393793770821186
Loss in iteration 93 : 0.42437768775612006
Loss in iteration 94 : 0.42481702529827947
Loss in iteration 95 : 0.425253353614342
Loss in iteration 96 : 0.4256860899868522
Loss in iteration 97 : 0.4261152636038677
Loss in iteration 98 : 0.42654104552281275
Loss in iteration 99 : 0.4269631507957711
Loss in iteration 100 : 0.4273810636973154
Loss in iteration 101 : 0.427795012624366
Loss in iteration 102 : 0.4282069153117502
Loss in iteration 103 : 0.4286166736436477
Loss in iteration 104 : 0.4290233385608582
Loss in iteration 105 : 0.4294264424958335
Loss in iteration 106 : 0.42982643216648425
Loss in iteration 107 : 0.43022392623095734
Loss in iteration 108 : 0.4306185304700444
Loss in iteration 109 : 0.43100933791656987
Loss in iteration 110 : 0.43139705335196
Loss in iteration 111 : 0.4317815375055902
Loss in iteration 112 : 0.43216177357463026
Loss in iteration 113 : 0.432538470140615
Loss in iteration 114 : 0.4329112000160189
Loss in iteration 115 : 0.43328045810258675
Loss in iteration 116 : 0.43364646274569957
Loss in iteration 117 : 0.4340088685277683
Loss in iteration 118 : 0.4343676067005192
Loss in iteration 119 : 0.43472245678613836
Loss in iteration 120 : 0.435073210277146
Loss in iteration 121 : 0.43542012441394784
Loss in iteration 122 : 0.43576346631887136
Loss in iteration 123 : 0.43610347811542055
Loss in iteration 124 : 0.43644037929450313
Loss in iteration 125 : 0.436774368861222
Loss in iteration 126 : 0.43710562728184293
Loss in iteration 127 : 0.4374343182489373
Loss in iteration 128 : 0.43776059028117753
Loss in iteration 129 : 0.4380845781728625
Loss in iteration 130 : 0.43840640430695116
Loss in iteration 131 : 0.43872617984416873
Loss in iteration 132 : 0.43904400579965036
Loss in iteration 133 : 0.43935997401755905
Loss in iteration 134 : 0.4396734887599072
Testing accuracy  of updater 7 on alg 1 with rate 0.008960000000000001 = 0.781, training accuracy 0.8429912593072192, time elapsed: 2017 millisecond.
Loss in iteration 1 : 1.0000005909953837
Loss in iteration 2 : 0.9588439708914336
Loss in iteration 3 : 0.8838636331098798
Loss in iteration 4 : 0.7789779418504708
Loss in iteration 5 : 0.6661078851334653
Loss in iteration 6 : 0.5875859131863054
Loss in iteration 7 : 0.5610805504672889
Loss in iteration 8 : 0.5702232073701059
Loss in iteration 9 : 0.5896115286204442
Loss in iteration 10 : 0.6081313052515448
Loss in iteration 11 : 0.622073483270727
Loss in iteration 12 : 0.630805146871788
Loss in iteration 13 : 0.6343912004994374
Loss in iteration 14 : 0.6332372615660099
Loss in iteration 15 : 0.6277996895290016
Loss in iteration 16 : 0.618553058988177
Loss in iteration 17 : 0.6060630227633984
Loss in iteration 18 : 0.5909591326823572
Loss in iteration 19 : 0.57411681048613
Loss in iteration 20 : 0.5566676915997747
Loss in iteration 21 : 0.5402423425255933
Loss in iteration 22 : 0.526272048877619
Loss in iteration 23 : 0.5161519389271232
Loss in iteration 24 : 0.5102138052500729
Loss in iteration 25 : 0.5075892629360601
Loss in iteration 26 : 0.507244191034228
Loss in iteration 27 : 0.5083013461748883
Loss in iteration 28 : 0.5097472255920963
Loss in iteration 29 : 0.5107998606492777
Loss in iteration 30 : 0.5110652317005749
Loss in iteration 31 : 0.5102238555798416
Loss in iteration 32 : 0.5082298004605136
Loss in iteration 33 : 0.5052684630549259
Loss in iteration 34 : 0.5016677555070527
Loss in iteration 35 : 0.49771305907084945
Loss in iteration 36 : 0.4936481439610869
Loss in iteration 37 : 0.48976695059287795
Loss in iteration 38 : 0.4862365254070197
Loss in iteration 39 : 0.48313505833155906
Loss in iteration 40 : 0.48057351356425787
Loss in iteration 41 : 0.47837494759409693
Loss in iteration 42 : 0.47657844199697413
Loss in iteration 43 : 0.47510488980729565
Loss in iteration 44 : 0.47385513100946536
Loss in iteration 45 : 0.4726772239201367
Loss in iteration 46 : 0.4715387521764526
Loss in iteration 47 : 0.4703940262511059
Loss in iteration 48 : 0.4691947398193051
Loss in iteration 49 : 0.46792381592898874
Loss in iteration 50 : 0.4665804972155011
Loss in iteration 51 : 0.4651724486951635
Loss in iteration 52 : 0.46370902933007013
Loss in iteration 53 : 0.46220928435928704
Loss in iteration 54 : 0.46068475624356603
Loss in iteration 55 : 0.4591592640056562
Loss in iteration 56 : 0.457641730251895
Loss in iteration 57 : 0.45614582729033293
Loss in iteration 58 : 0.45471105534717043
Loss in iteration 59 : 0.45335637741641416
Loss in iteration 60 : 0.45204387851151256
Loss in iteration 61 : 0.450764728712572
Loss in iteration 62 : 0.4495320071926595
Loss in iteration 63 : 0.4483436878495347
Loss in iteration 64 : 0.44718403405581225
Loss in iteration 65 : 0.44604959249803217
Loss in iteration 66 : 0.4449244268411927
Loss in iteration 67 : 0.4438126547183218
Loss in iteration 68 : 0.44270214989556284
Loss in iteration 69 : 0.4415984174515411
Loss in iteration 70 : 0.4405022192613531
Loss in iteration 71 : 0.4394168914157664
Loss in iteration 72 : 0.4383347822218872
Loss in iteration 73 : 0.43725823504157313
Loss in iteration 74 : 0.43618991693579234
Loss in iteration 75 : 0.4351305011931207
Loss in iteration 76 : 0.4340888125907256
Loss in iteration 77 : 0.4330665862665331
Loss in iteration 78 : 0.4320744322572456
Loss in iteration 79 : 0.43111337446245623
Loss in iteration 80 : 0.4301812489818456
Loss in iteration 81 : 0.42927611504842506
Loss in iteration 82 : 0.4284034985378606
Loss in iteration 83 : 0.42756132196207564
Loss in iteration 84 : 0.4267408721694346
Loss in iteration 85 : 0.42595214275783017
Loss in iteration 86 : 0.42519096851035526
Loss in iteration 87 : 0.42444660948254725
Loss in iteration 88 : 0.4237254246146355
Loss in iteration 89 : 0.4230305924541096
Loss in iteration 90 : 0.4223614907268468
Loss in iteration 91 : 0.4217173430063747
Loss in iteration 92 : 0.42109922469125294
Loss in iteration 93 : 0.420503300657939
Loss in iteration 94 : 0.41992585603909
Loss in iteration 95 : 0.4193724286824219
Loss in iteration 96 : 0.418841393663044
Loss in iteration 97 : 0.4183377799477504
Loss in iteration 98 : 0.41785228417329795
Loss in iteration 99 : 0.4173862580694619
Loss in iteration 100 : 0.41693974058995503
Loss in iteration 101 : 0.41651201253737646
Loss in iteration 102 : 0.41610773175174764
Loss in iteration 103 : 0.4157190858149056
Loss in iteration 104 : 0.415341803692159
Loss in iteration 105 : 0.41497783878444694
Loss in iteration 106 : 0.41463410727962163
Loss in iteration 107 : 0.4143096773868534
Loss in iteration 108 : 0.4140048111748354
Loss in iteration 109 : 0.41371361448002225
Loss in iteration 110 : 0.4134451175371687
Loss in iteration 111 : 0.4131901192606405
Loss in iteration 112 : 0.41294705440731966
Loss in iteration 113 : 0.41271764180099463
Loss in iteration 114 : 0.4125048501556722
Loss in iteration 115 : 0.4123061682912789
Loss in iteration 116 : 0.4121212788151286
Loss in iteration 117 : 0.4119478525932304
Loss in iteration 118 : 0.4117852691829154
Loss in iteration 119 : 0.4116369222237849
Loss in iteration 120 : 0.41150221492207417
Loss in iteration 121 : 0.4113787137331952
Loss in iteration 122 : 0.4112663053194219
Loss in iteration 123 : 0.41116511257562266
Loss in iteration 124 : 0.4110716304886809
Loss in iteration 125 : 0.410986700770912
Loss in iteration 126 : 0.410912984156765
Loss in iteration 127 : 0.4108492548156723
Loss in iteration 128 : 0.41079300855160633
Loss in iteration 129 : 0.41074204502043266
Testing accuracy  of updater 7 on alg 1 with rate 0.002239999999999999 = 0.7795, training accuracy 0.8371641307866623, time elapsed: 1831 millisecond.
Loss in iteration 1 : 1.0045270323059696
Loss in iteration 2 : 2.532815853866265
Loss in iteration 3 : 2.366826648587889
Loss in iteration 4 : 1.9538434369888338
Loss in iteration 5 : 1.3677119968002238
Loss in iteration 6 : 0.6803477821208967
Loss in iteration 7 : 0.4789240384350865
Loss in iteration 8 : 0.6338399550262878
Loss in iteration 9 : 0.6192222178182502
Loss in iteration 10 : 0.5444547871447001
Loss in iteration 11 : 0.5059379834992855
Loss in iteration 12 : 0.5001513202194124
Loss in iteration 13 : 0.5075580025221494
Loss in iteration 14 : 0.5106752737560438
Loss in iteration 15 : 0.507353331888998
Loss in iteration 16 : 0.5017113918227026
Loss in iteration 17 : 0.49713842995417024
Loss in iteration 18 : 0.493333730549945
Loss in iteration 19 : 0.48855728593607295
Loss in iteration 20 : 0.4827611266463673
Loss in iteration 21 : 0.4766001405750686
Loss in iteration 22 : 0.47091548078300416
Loss in iteration 23 : 0.4653881015803292
Loss in iteration 24 : 0.46011652527785424
Loss in iteration 25 : 0.4553431614390705
Loss in iteration 26 : 0.45141544410220524
Loss in iteration 27 : 0.4487476755448585
Loss in iteration 28 : 0.44745729951436697
Loss in iteration 29 : 0.4478513248742458
Loss in iteration 30 : 0.4498368317503008
Loss in iteration 31 : 0.453269964594063
Loss in iteration 32 : 0.4578481220281466
Loss in iteration 33 : 0.46286629369700555
Loss in iteration 34 : 0.4678723461469824
Loss in iteration 35 : 0.4725620033965645
Loss in iteration 36 : 0.47697389988812644
Loss in iteration 37 : 0.4807391499539265
Loss in iteration 38 : 0.48408975028777523
Loss in iteration 39 : 0.4872487426063068
Loss in iteration 40 : 0.4903697512955747
Loss in iteration 41 : 0.4933130451767651
Loss in iteration 42 : 0.4964517810885684
Loss in iteration 43 : 0.4995217413121251
Loss in iteration 44 : 0.5025393369266143
Loss in iteration 45 : 0.5054501354197896
Loss in iteration 46 : 0.5084052885039188
Loss in iteration 47 : 0.5111036363185893
Loss in iteration 48 : 0.5136386024428634
Loss in iteration 49 : 0.5158960875562371
Loss in iteration 50 : 0.518219233407902
Loss in iteration 51 : 0.5202311125624144
Loss in iteration 52 : 0.5222260279052537
Loss in iteration 53 : 0.5237830628679905
Loss in iteration 54 : 0.5256401043881337
Loss in iteration 55 : 0.5271732930920091
Loss in iteration 56 : 0.5288764094894927
Loss in iteration 57 : 0.5302124300301683
Loss in iteration 58 : 0.5321719544554828
Loss in iteration 59 : 0.5336754603703987
Loss in iteration 60 : 0.536448524177529
Loss in iteration 61 : 0.5400511363182577
Loss in iteration 62 : 0.5501781620920705
Loss in iteration 63 : 0.5582268410918951
Loss in iteration 64 : 0.5923218828362526
Loss in iteration 65 : 0.613348240965957
Loss in iteration 66 : 0.6412922355128345
Loss in iteration 67 : 0.6025612820716524
Loss in iteration 68 : 0.5914050233013028
Loss in iteration 69 : 0.570829671226669
Loss in iteration 70 : 0.5625803739504773
Loss in iteration 71 : 0.5587158663776328
Loss in iteration 72 : 0.558231031299534
Loss in iteration 73 : 0.5571848108684324
Loss in iteration 74 : 0.5566404684252585
Loss in iteration 75 : 0.5558224214374272
Loss in iteration 76 : 0.5548453150503249
Loss in iteration 77 : 0.5546117612197924
Loss in iteration 78 : 0.555000006962957
Loss in iteration 79 : 0.5557805101216731
Loss in iteration 80 : 0.561434585578076
Loss in iteration 81 : 0.5687281872154659
Loss in iteration 82 : 0.60640024592928
Loss in iteration 83 : 0.6473209103480476
Loss in iteration 84 : 0.7284181419613398
Loss in iteration 85 : 0.6150695326292034
Loss in iteration 86 : 0.6035349390353439
Loss in iteration 87 : 0.5886009994151675
Loss in iteration 88 : 0.5795838570697246
Loss in iteration 89 : 0.573391801878167
Loss in iteration 90 : 0.5718298978385934
Loss in iteration 91 : 0.5701703690979095
Loss in iteration 92 : 0.5694517155229778
Loss in iteration 93 : 0.5680230263303613
Loss in iteration 94 : 0.5665141266029606
Loss in iteration 95 : 0.565483533815343
Loss in iteration 96 : 0.5657015847560772
Loss in iteration 97 : 0.5658517865190023
Loss in iteration 98 : 0.5710708719857384
Loss in iteration 99 : 0.5806643021612294
Loss in iteration 100 : 0.6231598312586137
Loss in iteration 101 : 0.6663902740578218
Loss in iteration 102 : 0.7506663953666937
Loss in iteration 103 : 0.6160820435167274
Loss in iteration 104 : 0.6066308914972725
Loss in iteration 105 : 0.5933327902172042
Loss in iteration 106 : 0.5851452905288104
Loss in iteration 107 : 0.5807664303831495
Loss in iteration 108 : 0.5793608238449216
Loss in iteration 109 : 0.5774006242043452
Loss in iteration 110 : 0.5761657033246291
Loss in iteration 111 : 0.5746078926636023
Loss in iteration 112 : 0.5739642832017633
Loss in iteration 113 : 0.574664882458394
Loss in iteration 114 : 0.5793320830448807
Loss in iteration 115 : 0.583105364191313
Loss in iteration 116 : 0.6077566312396476
Loss in iteration 117 : 0.6302890367788991
Loss in iteration 118 : 0.6996514938914455
Loss in iteration 119 : 0.6510477328948469
Loss in iteration 120 : 0.6590506288174319
Loss in iteration 121 : 0.6156480443877974
Loss in iteration 122 : 0.6046818711005733
Loss in iteration 123 : 0.5913801953945026
Loss in iteration 124 : 0.5861235071074231
Loss in iteration 125 : 0.5834561610248775
Loss in iteration 126 : 0.5812121747688845
Loss in iteration 127 : 0.5786857943875732
Loss in iteration 128 : 0.5770699129949362
Loss in iteration 129 : 0.5755530927320804
Loss in iteration 130 : 0.5743926260648828
Loss in iteration 131 : 0.5749944060428407
Loss in iteration 132 : 0.581353852268047
Loss in iteration 133 : 0.5872674939006719
Loss in iteration 134 : 0.6317341815256363
Loss in iteration 135 : 0.676964670972281
Loss in iteration 136 : 0.771965460606133
Loss in iteration 137 : 0.6132012116394101
Loss in iteration 138 : 0.6044308952810842
Loss in iteration 139 : 0.5967159380682752
Loss in iteration 140 : 0.5906121077596571
Loss in iteration 141 : 0.5869973293193644
Loss in iteration 142 : 0.5852624596964743
Loss in iteration 143 : 0.5831095287591764
Loss in iteration 144 : 0.5815032331325085
Loss in iteration 145 : 0.5797686037240215
Loss in iteration 146 : 0.5788936201690776
Loss in iteration 147 : 0.5795942909417262
Loss in iteration 148 : 0.5844052083992772
Loss in iteration 149 : 0.5885710307001295
Loss in iteration 150 : 0.6232276634256717
Loss in iteration 151 : 0.65275737606029
Loss in iteration 152 : 0.73336217164875
Loss in iteration 153 : 0.6411808429437718
Loss in iteration 154 : 0.6380659728904824
Loss in iteration 155 : 0.6139549043789347
Loss in iteration 156 : 0.6041371320591484
Loss in iteration 157 : 0.5922061914870922
Loss in iteration 158 : 0.5890253351681859
Loss in iteration 159 : 0.5864726343651128
Loss in iteration 160 : 0.5856520999578066
Loss in iteration 161 : 0.5840855051276688
Loss in iteration 162 : 0.5837402215443899
Loss in iteration 163 : 0.5825218922157959
Loss in iteration 164 : 0.5877022240465166
Loss in iteration 165 : 0.5885950139617959
Loss in iteration 166 : 0.6116411674344484
Loss in iteration 167 : 0.6343542482194331
Loss in iteration 168 : 0.7124014037557854
Loss in iteration 169 : 0.6551558519967934
Loss in iteration 170 : 0.6683500264593876
Loss in iteration 171 : 0.6215399090026554
Loss in iteration 172 : 0.6104007198408705
Loss in iteration 173 : 0.5969667951488073
Loss in iteration 174 : 0.5905679252997592
Loss in iteration 175 : 0.5872991124423667
Loss in iteration 176 : 0.5866401073940095
Loss in iteration 177 : 0.5841693718525944
Loss in iteration 178 : 0.5822239002391469
Loss in iteration 179 : 0.5811655327479525
Loss in iteration 180 : 0.5823497884923631
Loss in iteration 181 : 0.5839255632311477
Loss in iteration 182 : 0.5973238899007662
Loss in iteration 183 : 0.6011747341442943
Loss in iteration 184 : 0.65393636875604
Loss in iteration 185 : 0.6867328781617794
Loss in iteration 186 : 0.7674874868750043
Loss in iteration 187 : 0.6145962039886723
Loss in iteration 188 : 0.602134076781194
Loss in iteration 189 : 0.5952179428375
Loss in iteration 190 : 0.5917454101584813
Loss in iteration 191 : 0.5906163877165032
Loss in iteration 192 : 0.5893358271194569
Loss in iteration 193 : 0.5870072874010865
Loss in iteration 194 : 0.5861639871499182
Loss in iteration 195 : 0.5839713394179112
Loss in iteration 196 : 0.5839125125255404
Loss in iteration 197 : 0.5844686800001373
Loss in iteration 198 : 0.5957619473003387
Loss in iteration 199 : 0.598301292184205
Loss in iteration 200 : 0.6436597516692106
Testing accuracy  of updater 8 on alg 1 with rate 0.0392 = 0.78925, training accuracy 0.7921657494334736, time elapsed: 2806 millisecond.
Loss in iteration 1 : 1.002334693933758
Loss in iteration 2 : 1.6075571903384982
Loss in iteration 3 : 1.600256037800066
Loss in iteration 4 : 1.3486037214998698
Loss in iteration 5 : 0.890889500057239
Loss in iteration 6 : 0.48091821418970093
Loss in iteration 7 : 0.5185234534800865
Loss in iteration 8 : 0.606357448529426
Loss in iteration 9 : 0.5935084176014669
Loss in iteration 10 : 0.5572210157227457
Loss in iteration 11 : 0.5415038364404398
Loss in iteration 12 : 0.5451475466903509
Loss in iteration 13 : 0.5544998554427767
Loss in iteration 14 : 0.559683092347209
Loss in iteration 15 : 0.5593785841202994
Loss in iteration 16 : 0.5561805295357826
Loss in iteration 17 : 0.5520912328509331
Loss in iteration 18 : 0.5484236877761558
Loss in iteration 19 : 0.5439073688499213
Loss in iteration 20 : 0.5378482373457492
Loss in iteration 21 : 0.530708215798947
Loss in iteration 22 : 0.523367377957788
Loss in iteration 23 : 0.5159153587191811
Loss in iteration 24 : 0.5083064743526382
Loss in iteration 25 : 0.5007004900502394
Loss in iteration 26 : 0.4935175902590361
Loss in iteration 27 : 0.4870551798647025
Loss in iteration 28 : 0.48128690430799714
Loss in iteration 29 : 0.47639347125563036
Loss in iteration 30 : 0.473019333571337
Loss in iteration 31 : 0.4714597168420894
Loss in iteration 32 : 0.4718832869495214
Loss in iteration 33 : 0.47425023393321614
Loss in iteration 34 : 0.4782121129728266
Loss in iteration 35 : 0.48322911543520847
Loss in iteration 36 : 0.48866902587414557
Loss in iteration 37 : 0.4937644439077199
Loss in iteration 38 : 0.4985986788254434
Loss in iteration 39 : 0.5026588116560283
Loss in iteration 40 : 0.5063322179600102
Loss in iteration 41 : 0.5096522460951185
Loss in iteration 42 : 0.5128798237635867
Loss in iteration 43 : 0.5146175518057514
Loss in iteration 44 : 0.516907075992095
Loss in iteration 45 : 0.5195786592149175
Loss in iteration 46 : 0.5224110974670539
Loss in iteration 47 : 0.5251583366967336
Loss in iteration 48 : 0.5278139070259792
Loss in iteration 49 : 0.5303025243524779
Loss in iteration 50 : 0.532672097598645
Loss in iteration 51 : 0.5348196887967934
Loss in iteration 52 : 0.5367734855590556
Loss in iteration 53 : 0.5385692497833322
Loss in iteration 54 : 0.5402289038740353
Loss in iteration 55 : 0.5417253169279428
Loss in iteration 56 : 0.5430985888774043
Loss in iteration 57 : 0.5443693528047819
Loss in iteration 58 : 0.5455490201247375
Loss in iteration 59 : 0.5465884196304819
Loss in iteration 60 : 0.5476101693034435
Loss in iteration 61 : 0.548522893878086
Loss in iteration 62 : 0.5493588191542098
Loss in iteration 63 : 0.5500923515876291
Loss in iteration 64 : 0.5507394243811286
Loss in iteration 65 : 0.5513679449720217
Testing accuracy  of updater 8 on alg 1 with rate 0.02744 = 0.7865, training accuracy 0.8407251537714471, time elapsed: 965 millisecond.
Loss in iteration 1 : 1.0007737357884732
Loss in iteration 2 : 1.1163500754664752
Loss in iteration 3 : 1.1497032852000373
Loss in iteration 4 : 1.0341716255325797
Loss in iteration 5 : 0.7896316029885293
Loss in iteration 6 : 0.4672228131913769
Loss in iteration 7 : 0.4102300239404047
Loss in iteration 8 : 0.47845510124651
Loss in iteration 9 : 0.4758708014094326
Loss in iteration 10 : 0.44818693844133645
Loss in iteration 11 : 0.4328063424658104
Loss in iteration 12 : 0.43190091077153325
Loss in iteration 13 : 0.43867761208779477
Loss in iteration 14 : 0.4451750793431823
Loss in iteration 15 : 0.44797143933550154
Loss in iteration 16 : 0.4482895047169322
Loss in iteration 17 : 0.44846770973363215
Loss in iteration 18 : 0.44903730537695896
Loss in iteration 19 : 0.44944021227887415
Loss in iteration 20 : 0.44923027487640504
Loss in iteration 21 : 0.44841424118636775
Loss in iteration 22 : 0.4471082828731
Loss in iteration 23 : 0.44562590145979664
Loss in iteration 24 : 0.4440586697670162
Loss in iteration 25 : 0.44243877181221697
Loss in iteration 26 : 0.44074224799741957
Loss in iteration 27 : 0.4390278709861874
Loss in iteration 28 : 0.4373695520046221
Loss in iteration 29 : 0.4359035941304618
Loss in iteration 30 : 0.4347639103623389
Loss in iteration 31 : 0.43399067994158325
Loss in iteration 32 : 0.43358467321206023
Loss in iteration 33 : 0.433584447996947
Loss in iteration 34 : 0.43423417696380234
Loss in iteration 35 : 0.4352499180299808
Loss in iteration 36 : 0.43681759241827206
Loss in iteration 37 : 0.4387915082687935
Loss in iteration 38 : 0.44115204848748524
Loss in iteration 39 : 0.44377674010651535
Loss in iteration 40 : 0.44660658055555197
Loss in iteration 41 : 0.44956278079652656
Loss in iteration 42 : 0.4524945749829287
Loss in iteration 43 : 0.4554080663910137
Loss in iteration 44 : 0.4582325084089589
Loss in iteration 45 : 0.46097674665386923
Loss in iteration 46 : 0.46364350874601024
Loss in iteration 47 : 0.46623231670403
Loss in iteration 48 : 0.4687582907736116
Loss in iteration 49 : 0.4712211075852376
Loss in iteration 50 : 0.4736284678097913
Loss in iteration 51 : 0.47598944870245785
Loss in iteration 52 : 0.478356122580172
Loss in iteration 53 : 0.48070143735407955
Loss in iteration 54 : 0.48303166823655846
Loss in iteration 55 : 0.4853148162132985
Loss in iteration 56 : 0.48755969121723236
Loss in iteration 57 : 0.4897690338424649
Loss in iteration 58 : 0.4919297867733554
Loss in iteration 59 : 0.49406520856017383
Loss in iteration 60 : 0.496151468609492
Loss in iteration 61 : 0.4981832524320334
Loss in iteration 62 : 0.5001648392923221
Loss in iteration 63 : 0.5020852673294527
Loss in iteration 64 : 0.503954249180316
Loss in iteration 65 : 0.5057736063697693
Loss in iteration 66 : 0.5075379230682904
Loss in iteration 67 : 0.5092578774443156
Loss in iteration 68 : 0.5109268661244867
Loss in iteration 69 : 0.5125582664187259
Loss in iteration 70 : 0.51415918278533
Loss in iteration 71 : 0.5157136114426502
Loss in iteration 72 : 0.5172294302813562
Loss in iteration 73 : 0.51867813331382
Loss in iteration 74 : 0.5200717285904328
Loss in iteration 75 : 0.5214105968848929
Loss in iteration 76 : 0.5227024595677038
Loss in iteration 77 : 0.5239471618954135
Loss in iteration 78 : 0.5251466622732718
Loss in iteration 79 : 0.5263081452709192
Loss in iteration 80 : 0.5274225017103351
Loss in iteration 81 : 0.5285045895225904
Loss in iteration 82 : 0.5295381872423341
Loss in iteration 83 : 0.530532550393251
Loss in iteration 84 : 0.5314976025263403
Loss in iteration 85 : 0.53242733568876
Loss in iteration 86 : 0.5333206810299189
Loss in iteration 87 : 0.5341886865501806
Loss in iteration 88 : 0.5350129806521626
Loss in iteration 89 : 0.5358098285299243
Loss in iteration 90 : 0.536596417079955
Loss in iteration 91 : 0.5373587950325169
Loss in iteration 92 : 0.5380970439393613
Loss in iteration 93 : 0.5388223909951053
Loss in iteration 94 : 0.539533644109019
Loss in iteration 95 : 0.5402221344871171
Loss in iteration 96 : 0.5408897842638489
Loss in iteration 97 : 0.5415413735008519
Loss in iteration 98 : 0.5421778230329394
Loss in iteration 99 : 0.5428089801911649
Loss in iteration 100 : 0.5434132796055879
Loss in iteration 101 : 0.5440030548137507
Loss in iteration 102 : 0.5445668315995108
Testing accuracy  of updater 8 on alg 1 with rate 0.01568 = 0.78675, training accuracy 0.8400776950469407, time elapsed: 1412 millisecond.
Loss in iteration 1 : 1.0000527576236597
Loss in iteration 2 : 0.5646884715785017
Loss in iteration 3 : 0.587815234249194
Loss in iteration 4 : 0.5720573880987597
Loss in iteration 5 : 0.530355763101605
Loss in iteration 6 : 0.4883246660258165
Loss in iteration 7 : 0.4629738413726193
Loss in iteration 8 : 0.4485999685911849
Loss in iteration 9 : 0.43607531073108113
Loss in iteration 10 : 0.42379466048724834
Loss in iteration 11 : 0.41220451306646727
Loss in iteration 12 : 0.4014870845429702
Loss in iteration 13 : 0.39368428797496635
Loss in iteration 14 : 0.38902290111037025
Loss in iteration 15 : 0.3861888531919584
Loss in iteration 16 : 0.3844536218674278
Loss in iteration 17 : 0.38337521585293094
Loss in iteration 18 : 0.3826661145356469
Loss in iteration 19 : 0.38220160919143004
Loss in iteration 20 : 0.3819628053093731
Loss in iteration 21 : 0.38192775280942776
Loss in iteration 22 : 0.382177230953408
Loss in iteration 23 : 0.3826101369496306
Loss in iteration 24 : 0.38310673974720316
Loss in iteration 25 : 0.38361193966615365
Loss in iteration 26 : 0.3841396533837498
Loss in iteration 27 : 0.3846697227357915
Loss in iteration 28 : 0.38520883464347067
Loss in iteration 29 : 0.3857557988654051
Loss in iteration 30 : 0.38629395972004577
Loss in iteration 31 : 0.38682534329736346
Loss in iteration 32 : 0.38734571594225764
Loss in iteration 33 : 0.3878532713291043
Loss in iteration 34 : 0.3883456283849049
Loss in iteration 35 : 0.38882504383855604
Loss in iteration 36 : 0.3892966366846273
Loss in iteration 37 : 0.38976512590314966
Loss in iteration 38 : 0.39023562764778996
Loss in iteration 39 : 0.39070255656569874
Loss in iteration 40 : 0.39116648186162356
Loss in iteration 41 : 0.391631894719051
Loss in iteration 42 : 0.3920949204672095
Loss in iteration 43 : 0.3925562399449126
Loss in iteration 44 : 0.3930230903525254
Loss in iteration 45 : 0.3934962554221957
Loss in iteration 46 : 0.39397165792907574
Loss in iteration 47 : 0.3944546110641589
Loss in iteration 48 : 0.39494665865499956
Loss in iteration 49 : 0.39544420369888406
Loss in iteration 50 : 0.39594948570185506
Loss in iteration 51 : 0.3964610845890358
Loss in iteration 52 : 0.39698599621102904
Loss in iteration 53 : 0.39751867461091955
Loss in iteration 54 : 0.3980582000576084
Loss in iteration 55 : 0.3986037432110337
Loss in iteration 56 : 0.399159865626211
Loss in iteration 57 : 0.399720599027933
Loss in iteration 58 : 0.40028684906645706
Loss in iteration 59 : 0.40086176453555145
Loss in iteration 60 : 0.40144407743157495
Loss in iteration 61 : 0.40203103316045846
Loss in iteration 62 : 0.40262210382776037
Loss in iteration 63 : 0.40321839072896815
Loss in iteration 64 : 0.4038222090781196
Loss in iteration 65 : 0.4044328091906223
Loss in iteration 66 : 0.4050511086414637
Loss in iteration 67 : 0.4056758588565524
Loss in iteration 68 : 0.406311623850866
Loss in iteration 69 : 0.4069552749958444
Loss in iteration 70 : 0.40760385043075237
Loss in iteration 71 : 0.40825609044668393
Loss in iteration 72 : 0.4089133881522438
Loss in iteration 73 : 0.40957379183199494
Loss in iteration 74 : 0.4102347455533686
Loss in iteration 75 : 0.4108979667114056
Loss in iteration 76 : 0.41156181872632175
Loss in iteration 77 : 0.4122252986287044
Loss in iteration 78 : 0.41289063722866826
Loss in iteration 79 : 0.4135579615227957
Loss in iteration 80 : 0.41422589699772594
Loss in iteration 81 : 0.41489400898573664
Loss in iteration 82 : 0.4155635940458013
Loss in iteration 83 : 0.41623295739739613
Loss in iteration 84 : 0.416903055945085
Loss in iteration 85 : 0.41757395919343615
Loss in iteration 86 : 0.41824483957091063
Loss in iteration 87 : 0.4189155224100796
Loss in iteration 88 : 0.4195867599476428
Loss in iteration 89 : 0.42025851911793816
Loss in iteration 90 : 0.4209263702193957
Loss in iteration 91 : 0.42159212738766844
Loss in iteration 92 : 0.42225630953925153
Loss in iteration 93 : 0.42292136287828913
Loss in iteration 94 : 0.42358509670014666
Loss in iteration 95 : 0.4242484327446284
Loss in iteration 96 : 0.42491135750230413
Loss in iteration 97 : 0.42557321015747585
Loss in iteration 98 : 0.4262326422835714
Loss in iteration 99 : 0.4268906763780411
Loss in iteration 100 : 0.4275476641108349
Loss in iteration 101 : 0.4282036934644847
Loss in iteration 102 : 0.4288582965421617
Loss in iteration 103 : 0.4295126979767706
Loss in iteration 104 : 0.4301666428042749
Loss in iteration 105 : 0.4308202898419111
Loss in iteration 106 : 0.4314737811971294
Loss in iteration 107 : 0.4321253779105887
Loss in iteration 108 : 0.4327769548562857
Loss in iteration 109 : 0.43342809022926615
Loss in iteration 110 : 0.43407892682678645
Loss in iteration 111 : 0.4347295922159439
Loss in iteration 112 : 0.43538020011567274
Loss in iteration 113 : 0.43602956708716833
Loss in iteration 114 : 0.4366784806121523
Loss in iteration 115 : 0.43732808649113786
Loss in iteration 116 : 0.4379757722441463
Loss in iteration 117 : 0.4386232078113383
Loss in iteration 118 : 0.4392702561205938
Loss in iteration 119 : 0.439917033188084
Loss in iteration 120 : 0.4405636422951241
Loss in iteration 121 : 0.4412101751743544
Loss in iteration 122 : 0.44185671308921487
Loss in iteration 123 : 0.44250015307363677
Loss in iteration 124 : 0.44314229617789064
Loss in iteration 125 : 0.44378470873498327
Loss in iteration 126 : 0.4444265895577257
Loss in iteration 127 : 0.44506811050639583
Loss in iteration 128 : 0.4457085549976384
Loss in iteration 129 : 0.4463487800168031
Loss in iteration 130 : 0.4469886007315297
Loss in iteration 131 : 0.44762625366937003
Loss in iteration 132 : 0.4482630782291879
Loss in iteration 133 : 0.4488946972874276
Loss in iteration 134 : 0.44952456275367647
Loss in iteration 135 : 0.4501522694725934
Loss in iteration 136 : 0.4507770135935981
Loss in iteration 137 : 0.45139976590478115
Loss in iteration 138 : 0.4520206556047725
Loss in iteration 139 : 0.4526387233186391
Loss in iteration 140 : 0.4532545816485339
Loss in iteration 141 : 0.4538663673972113
Loss in iteration 142 : 0.45447572570469874
Loss in iteration 143 : 0.4550810281982909
Loss in iteration 144 : 0.4556852905883217
Loss in iteration 145 : 0.45628605806609007
Loss in iteration 146 : 0.45688482380053985
Loss in iteration 147 : 0.4574816233327298
Loss in iteration 148 : 0.45807666120175733
Loss in iteration 149 : 0.45866911554066514
Loss in iteration 150 : 0.4592614388061026
Loss in iteration 151 : 0.45985255742415065
Loss in iteration 152 : 0.4604426945879361
Loss in iteration 153 : 0.46103195711778316
Loss in iteration 154 : 0.46162164749542417
Loss in iteration 155 : 0.4622107779792369
Loss in iteration 156 : 0.462798472309799
Loss in iteration 157 : 0.4633839559391158
Loss in iteration 158 : 0.46396938897775136
Loss in iteration 159 : 0.46455403714363463
Loss in iteration 160 : 0.46513542432961075
Loss in iteration 161 : 0.4657199438041296
Loss in iteration 162 : 0.46630170268441595
Loss in iteration 163 : 0.46688140945880097
Loss in iteration 164 : 0.46745781354334964
Loss in iteration 165 : 0.46803238382999374
Loss in iteration 166 : 0.4686051358995283
Loss in iteration 167 : 0.4691762421412882
Loss in iteration 168 : 0.4697442128695981
Loss in iteration 169 : 0.4703060758748012
Loss in iteration 170 : 0.4708662577971085
Loss in iteration 171 : 0.47142366349338655
Loss in iteration 172 : 0.4719769548042406
Loss in iteration 173 : 0.4725292961801024
Loss in iteration 174 : 0.4730796015708824
Loss in iteration 175 : 0.47362640935858086
Loss in iteration 176 : 0.4741729284999457
Loss in iteration 177 : 0.4747167083212247
Loss in iteration 178 : 0.47525845072283357
Loss in iteration 179 : 0.4757970903749446
Loss in iteration 180 : 0.4763339406479312
Loss in iteration 181 : 0.47687081997713476
Loss in iteration 182 : 0.4774059444328902
Loss in iteration 183 : 0.4779360474979959
Loss in iteration 184 : 0.47846493806315493
Loss in iteration 185 : 0.4789917589200668
Loss in iteration 186 : 0.479516730648362
Loss in iteration 187 : 0.480040015943061
Loss in iteration 188 : 0.48056176124032535
Loss in iteration 189 : 0.48108209828959514
Loss in iteration 190 : 0.48160114557514727
Loss in iteration 191 : 0.4821190096013205
Loss in iteration 192 : 0.4826357860543319
Loss in iteration 193 : 0.48315156085241273
Loss in iteration 194 : 0.4836654630319164
Loss in iteration 195 : 0.48417842025357427
Loss in iteration 196 : 0.4846902508177422
Loss in iteration 197 : 0.4851998489235724
Loss in iteration 198 : 0.485708345965802
Loss in iteration 199 : 0.4862156078191717
Loss in iteration 200 : 0.486720740041347
Testing accuracy  of updater 8 on alg 1 with rate 0.00392 = 0.785, training accuracy 0.8394302363224344, time elapsed: 2841 millisecond.
Loss in iteration 1 : 1.000034036731371
Loss in iteration 2 : 0.5496956913951837
Loss in iteration 3 : 0.5778788570664184
Loss in iteration 4 : 0.5779598043475915
Loss in iteration 5 : 0.5516357075548705
Loss in iteration 6 : 0.5140877430227593
Loss in iteration 7 : 0.4844610704955623
Loss in iteration 8 : 0.4686511599904331
Loss in iteration 9 : 0.4574104435589683
Loss in iteration 10 : 0.44576365863130196
Loss in iteration 11 : 0.4335761664169559
Loss in iteration 12 : 0.4216151912517144
Loss in iteration 13 : 0.4106706754544627
Loss in iteration 14 : 0.40165040217974
Loss in iteration 15 : 0.3952890074185257
Loss in iteration 16 : 0.3908947408587779
Loss in iteration 17 : 0.38788109707534946
Loss in iteration 18 : 0.3856315017454079
Loss in iteration 19 : 0.3840045036212085
Loss in iteration 20 : 0.38287580156733386
Loss in iteration 21 : 0.38204545346995206
Loss in iteration 22 : 0.38148758291668355
Loss in iteration 23 : 0.3811047194391286
Loss in iteration 24 : 0.38089496989406635
Loss in iteration 25 : 0.3807993631391874
Loss in iteration 26 : 0.380854155695535
Loss in iteration 27 : 0.38103390978371904
Loss in iteration 28 : 0.38126236773641003
Loss in iteration 29 : 0.38151031989996004
Loss in iteration 30 : 0.3817669307559881
Loss in iteration 31 : 0.38205288138573984
Loss in iteration 32 : 0.38236122044985826
Loss in iteration 33 : 0.3826803574330954
Loss in iteration 34 : 0.38300012211303675
Loss in iteration 35 : 0.3833167002026735
Loss in iteration 36 : 0.3836279809088671
Loss in iteration 37 : 0.38393879755533133
Loss in iteration 38 : 0.38424560631889704
Loss in iteration 39 : 0.3845488828545951
Loss in iteration 40 : 0.3848493639529117
Loss in iteration 41 : 0.38514706652319697
Loss in iteration 42 : 0.3854427384726785
Loss in iteration 43 : 0.3857370815487681
Loss in iteration 44 : 0.3860304946441832
Loss in iteration 45 : 0.3863230517721992
Loss in iteration 46 : 0.38661587595880376
Loss in iteration 47 : 0.3869086938192648
Loss in iteration 48 : 0.38720211694942785
Loss in iteration 49 : 0.3874985881823455
Loss in iteration 50 : 0.38779782006344327
Loss in iteration 51 : 0.38809971355307327
Loss in iteration 52 : 0.38840361513713884
Loss in iteration 53 : 0.38871237254493696
Loss in iteration 54 : 0.3890221590672787
Loss in iteration 55 : 0.3893367017881395
Loss in iteration 56 : 0.38965440088003456
Loss in iteration 57 : 0.38997530821869364
Loss in iteration 58 : 0.3902999494965736
Loss in iteration 59 : 0.3906273114313147
Loss in iteration 60 : 0.39096250561728557
Loss in iteration 61 : 0.3913042398758865
Loss in iteration 62 : 0.39165043137686795
Loss in iteration 63 : 0.39199894796168927
Loss in iteration 64 : 0.3923494881159796
Loss in iteration 65 : 0.392702108210992
Loss in iteration 66 : 0.39305730467520544
Loss in iteration 67 : 0.39341542897157616
Loss in iteration 68 : 0.39378074755469317
Loss in iteration 69 : 0.39414955671319557
Loss in iteration 70 : 0.39451978348497174
Loss in iteration 71 : 0.39489448678349526
Loss in iteration 72 : 0.3952746479160843
Loss in iteration 73 : 0.3956590878814545
Loss in iteration 74 : 0.3960465066848309
Loss in iteration 75 : 0.39643773672836335
Loss in iteration 76 : 0.3968332582004199
Loss in iteration 77 : 0.3972334564430019
Loss in iteration 78 : 0.3976368922428566
Loss in iteration 79 : 0.3980450181257593
Loss in iteration 80 : 0.39845751389790324
Loss in iteration 81 : 0.39887294187555117
Loss in iteration 82 : 0.39929141574749755
Loss in iteration 83 : 0.3997132082393213
Loss in iteration 84 : 0.4001376779080134
Loss in iteration 85 : 0.4005647248630949
Loss in iteration 86 : 0.4009935339419661
Loss in iteration 87 : 0.4014241410911729
Loss in iteration 88 : 0.4018561374610673
Loss in iteration 89 : 0.4022902686077392
Loss in iteration 90 : 0.4027260510131186
Loss in iteration 91 : 0.4031635273121783
Loss in iteration 92 : 0.4036021354166238
Loss in iteration 93 : 0.40404123654338414
Loss in iteration 94 : 0.40448161355263507
Loss in iteration 95 : 0.404923121208256
Loss in iteration 96 : 0.4053658363559453
Loss in iteration 97 : 0.405809826677069
Loss in iteration 98 : 0.40625515155639175
Loss in iteration 99 : 0.4067018628698737
Loss in iteration 100 : 0.4071497027557993
Loss in iteration 101 : 0.40759861468968267
Loss in iteration 102 : 0.4080487074664069
Loss in iteration 103 : 0.40850003656452544
Loss in iteration 104 : 0.4089526504578708
Loss in iteration 105 : 0.4094052716070979
Loss in iteration 106 : 0.4098561053027131
Loss in iteration 107 : 0.4103066930728626
Loss in iteration 108 : 0.41075684187871553
Loss in iteration 109 : 0.4112066396399871
Loss in iteration 110 : 0.41165625290530383
Loss in iteration 111 : 0.41210583145093593
Loss in iteration 112 : 0.41255589633299067
Loss in iteration 113 : 0.4130066403955251
Loss in iteration 114 : 0.4134572154372802
Loss in iteration 115 : 0.4139063267373122
Loss in iteration 116 : 0.41435562641648904
Loss in iteration 117 : 0.4148039620519146
Loss in iteration 118 : 0.4152506307534606
Loss in iteration 119 : 0.4156970165741919
Loss in iteration 120 : 0.4161427747494135
Loss in iteration 121 : 0.41658834898409103
Loss in iteration 122 : 0.41703361559335056
Loss in iteration 123 : 0.41747870462427106
Loss in iteration 124 : 0.41792373300896996
Loss in iteration 125 : 0.4183688057569791
Loss in iteration 126 : 0.41881280361706325
Loss in iteration 127 : 0.4192574533916109
Loss in iteration 128 : 0.4197015096002547
Loss in iteration 129 : 0.420146210271044
Loss in iteration 130 : 0.42059084024972815
Loss in iteration 131 : 0.42103542694242974
Loss in iteration 132 : 0.42148090954340756
Loss in iteration 133 : 0.42192574343982625
Loss in iteration 134 : 0.4223707270652386
Loss in iteration 135 : 0.42281494475017173
Loss in iteration 136 : 0.42325879566065605
Loss in iteration 137 : 0.42370241034230627
Loss in iteration 138 : 0.42414557924810403
Loss in iteration 139 : 0.42458836888325985
Loss in iteration 140 : 0.4250309566577174
Loss in iteration 141 : 0.4254734382006269
Loss in iteration 142 : 0.42591589914376976
Loss in iteration 143 : 0.4263584160674081
Loss in iteration 144 : 0.4268010573582724
Loss in iteration 145 : 0.427243883987661
Loss in iteration 146 : 0.4276869502169283
Loss in iteration 147 : 0.42813030423701565
Loss in iteration 148 : 0.4285727833793964
Loss in iteration 149 : 0.4290142330542824
Loss in iteration 150 : 0.42945548666250566
Loss in iteration 151 : 0.4298960448419029
Loss in iteration 152 : 0.4303357414361171
Loss in iteration 153 : 0.43077523164945236
Loss in iteration 154 : 0.4312141001531169
Loss in iteration 155 : 0.4316524394196885
Loss in iteration 156 : 0.4320910555715917
Loss in iteration 157 : 0.4325291659042747
Loss in iteration 158 : 0.4329676004384003
Loss in iteration 159 : 0.4334057955391597
Loss in iteration 160 : 0.433843802729245
Loss in iteration 161 : 0.4342808161558317
Loss in iteration 162 : 0.4347170301560085
Loss in iteration 163 : 0.4351513849963853
Loss in iteration 164 : 0.43558447078592316
Loss in iteration 165 : 0.4360171161584296
Loss in iteration 166 : 0.4364492424758829
Loss in iteration 167 : 0.4368803377230821
Loss in iteration 168 : 0.4373112320524344
Loss in iteration 169 : 0.43773997747680665
Loss in iteration 170 : 0.43816787633027543
Loss in iteration 171 : 0.43859488860149226
Loss in iteration 172 : 0.4390211378851271
Loss in iteration 173 : 0.43944594814934
Loss in iteration 174 : 0.43986967809734356
Loss in iteration 175 : 0.4402918670322968
Loss in iteration 176 : 0.44071394342568293
Loss in iteration 177 : 0.44113416558714114
Loss in iteration 178 : 0.44155356129957035
Loss in iteration 179 : 0.44197208345148736
Loss in iteration 180 : 0.4423890547728359
Loss in iteration 181 : 0.4428052519818847
Loss in iteration 182 : 0.4432200002676383
Loss in iteration 183 : 0.44363441906776324
Loss in iteration 184 : 0.44404674600150396
Loss in iteration 185 : 0.4444596677368606
Loss in iteration 186 : 0.4448705981752039
Loss in iteration 187 : 0.44528067888342615
Loss in iteration 188 : 0.4456916608390749
Loss in iteration 189 : 0.44610242227152597
Loss in iteration 190 : 0.44651152408747014
Loss in iteration 191 : 0.44692241753578016
Loss in iteration 192 : 0.4473311581174
Loss in iteration 193 : 0.447740162403383
Loss in iteration 194 : 0.4481476246905892
Loss in iteration 195 : 0.44855412621129487
Loss in iteration 196 : 0.4489598257612577
Loss in iteration 197 : 0.44936641978677916
Loss in iteration 198 : 0.44976976458395446
Loss in iteration 199 : 0.45017227667591975
Loss in iteration 200 : 0.4505732122203998
Testing accuracy  of updater 8 on alg 1 with rate 0.002744 = 0.78675, training accuracy 0.8391065069601813, time elapsed: 2737 millisecond.
Loss in iteration 1 : 1.000013029751275
Loss in iteration 2 : 0.5986389957092242
Loss in iteration 3 : 0.5636028659292036
Loss in iteration 4 : 0.5881328435832619
Loss in iteration 5 : 0.593215437340191
Loss in iteration 6 : 0.5790619557372767
Loss in iteration 7 : 0.5512913079407322
Loss in iteration 8 : 0.5191677408210605
Loss in iteration 9 : 0.4933992610685905
Loss in iteration 10 : 0.4791009726688242
Loss in iteration 11 : 0.4711592186499494
Loss in iteration 12 : 0.4647150695621741
Loss in iteration 13 : 0.4571397779797096
Loss in iteration 14 : 0.4480195573258541
Loss in iteration 15 : 0.4383137154078076
Loss in iteration 16 : 0.42865524951587697
Loss in iteration 17 : 0.4197410542911243
Loss in iteration 18 : 0.41193090665722726
Loss in iteration 19 : 0.4054408706778896
Loss in iteration 20 : 0.4004590232173535
Loss in iteration 21 : 0.39646794418648307
Loss in iteration 22 : 0.3932477293115579
Loss in iteration 23 : 0.3907287299740023
Loss in iteration 24 : 0.388718305293701
Loss in iteration 25 : 0.38706897655337513
Loss in iteration 26 : 0.38565236418003906
Loss in iteration 27 : 0.3844621923272196
Loss in iteration 28 : 0.38353607617093
Loss in iteration 29 : 0.38279086976419835
Loss in iteration 30 : 0.3821883817999188
Loss in iteration 31 : 0.3817267396239789
Loss in iteration 32 : 0.3813573758182711
Loss in iteration 33 : 0.3810571580448365
Loss in iteration 34 : 0.3808168589904919
Loss in iteration 35 : 0.38063537845653966
Loss in iteration 36 : 0.38050567206893343
Loss in iteration 37 : 0.38040897037968224
Loss in iteration 38 : 0.38034102117568763
Loss in iteration 39 : 0.38029672117848573
Loss in iteration 40 : 0.380273571971307
Loss in iteration 41 : 0.38027824148949557
Loss in iteration 42 : 0.38030380593889135
Loss in iteration 43 : 0.38034853387814443
Loss in iteration 44 : 0.3804061794152843
Loss in iteration 45 : 0.3804766087639661
Loss in iteration 46 : 0.3805560972028107
Loss in iteration 47 : 0.38064102537545635
Loss in iteration 48 : 0.3807336561022501
Loss in iteration 49 : 0.3808353838526495
Loss in iteration 50 : 0.3809437180138968
Loss in iteration 51 : 0.38105797781880957
Loss in iteration 52 : 0.38117679513530245
Loss in iteration 53 : 0.3813008020721911
Loss in iteration 54 : 0.3814285340607671
Loss in iteration 55 : 0.3815587835169705
Loss in iteration 56 : 0.3816919632038792
Loss in iteration 57 : 0.38182624944015314
Loss in iteration 58 : 0.3819615040957063
Loss in iteration 59 : 0.38209674126353077
Loss in iteration 60 : 0.3822331598249156
Loss in iteration 61 : 0.38237135435677555
Loss in iteration 62 : 0.3825117916456613
Loss in iteration 63 : 0.38265369057308574
Loss in iteration 64 : 0.38279748032018746
Loss in iteration 65 : 0.38294328263259575
Loss in iteration 66 : 0.3830910566977019
Loss in iteration 67 : 0.38324027919382614
Loss in iteration 68 : 0.3833902656466836
Loss in iteration 69 : 0.3835408787116479
Loss in iteration 70 : 0.3836932887219576
Loss in iteration 71 : 0.38384770769465054
Loss in iteration 72 : 0.3840035736761847
Loss in iteration 73 : 0.38416077250128583
Loss in iteration 74 : 0.3843190776734886
Loss in iteration 75 : 0.3844789052706611
Loss in iteration 76 : 0.38463969002637394
Loss in iteration 77 : 0.3848017014510605
Loss in iteration 78 : 0.38496476831342885
Loss in iteration 79 : 0.38512901392953625
Loss in iteration 80 : 0.38529428173561786
Loss in iteration 81 : 0.3854609622687111
Loss in iteration 82 : 0.3856294057123758
Loss in iteration 83 : 0.38579895742296416
Loss in iteration 84 : 0.38596985373082
Loss in iteration 85 : 0.38614162327570917
Loss in iteration 86 : 0.3863148188233927
Loss in iteration 87 : 0.38648883362808
Loss in iteration 88 : 0.38666375126198704
Loss in iteration 89 : 0.3868399532289661
Loss in iteration 90 : 0.38701721214715296
Loss in iteration 91 : 0.3871952276500696
Loss in iteration 92 : 0.38737428269797547
Loss in iteration 93 : 0.38755416573193335
Loss in iteration 94 : 0.3877352626266082
Loss in iteration 95 : 0.38791673773603624
Loss in iteration 96 : 0.38809906794401405
Loss in iteration 97 : 0.38828216737496113
Loss in iteration 98 : 0.38846605867893746
Loss in iteration 99 : 0.3886507619951104
Loss in iteration 100 : 0.38883629517776813
Loss in iteration 101 : 0.38902267400265955
Loss in iteration 102 : 0.3892096715879528
Loss in iteration 103 : 0.3893974605029052
Loss in iteration 104 : 0.3895863146639945
Loss in iteration 105 : 0.38977598351035897
Loss in iteration 106 : 0.3899664658750443
Loss in iteration 107 : 0.3901577761019694
Loss in iteration 108 : 0.39034992674014024
Loss in iteration 109 : 0.3905431667451946
Loss in iteration 110 : 0.3907375496340028
Loss in iteration 111 : 0.3909328370851999
Loss in iteration 112 : 0.3911290335586368
Loss in iteration 113 : 0.3913260631794106
Loss in iteration 114 : 0.39152389539322424
Loss in iteration 115 : 0.39172187331858277
Loss in iteration 116 : 0.39192099962249916
Loss in iteration 117 : 0.3921208735652818
Loss in iteration 118 : 0.39232164723162916
Loss in iteration 119 : 0.39252329834437577
Loss in iteration 120 : 0.3927259221916306
Loss in iteration 121 : 0.39292903176723093
Loss in iteration 122 : 0.3931327401764636
Loss in iteration 123 : 0.3933369783863843
Loss in iteration 124 : 0.3935420534013958
Loss in iteration 125 : 0.39374754321023864
Loss in iteration 126 : 0.3939536134859829
Loss in iteration 127 : 0.3941605606082579
Loss in iteration 128 : 0.3943678257510991
Loss in iteration 129 : 0.3945756237065625
Loss in iteration 130 : 0.39478398226322414
Loss in iteration 131 : 0.39499330515244074
Loss in iteration 132 : 0.39520319380397534
Loss in iteration 133 : 0.3954137230836995
Loss in iteration 134 : 0.39562490953649637
Loss in iteration 135 : 0.3958367677067232
Loss in iteration 136 : 0.3960493103325948
Loss in iteration 137 : 0.39626254852186865
Loss in iteration 138 : 0.3964764919106242
Loss in iteration 139 : 0.39669114880674344
Loss in iteration 140 : 0.39690632159923556
Loss in iteration 141 : 0.39712240813443944
Loss in iteration 142 : 0.3973391740903561
Loss in iteration 143 : 0.397556627855747
Loss in iteration 144 : 0.3977743654346199
Loss in iteration 145 : 0.3979926362438857
Loss in iteration 146 : 0.3982114104466816
Loss in iteration 147 : 0.3984307102939304
Loss in iteration 148 : 0.39865079548220644
Loss in iteration 149 : 0.39887135436848487
Loss in iteration 150 : 0.3990923196521002
Loss in iteration 151 : 0.39931367847846233
Loss in iteration 152 : 0.399535506723111
Loss in iteration 153 : 0.3997578271272113
Loss in iteration 154 : 0.3999806599035281
Loss in iteration 155 : 0.400204022978792
Loss in iteration 156 : 0.40042793221306155
Loss in iteration 157 : 0.40065266085858
Loss in iteration 158 : 0.4008781409917791
Loss in iteration 159 : 0.4011041296542998
Loss in iteration 160 : 0.4013306209289437
Loss in iteration 161 : 0.40155768654815693
Loss in iteration 162 : 0.4017852489871929
Loss in iteration 163 : 0.4020136329466401
Loss in iteration 164 : 0.4022421812683806
Loss in iteration 165 : 0.40247125935221045
Loss in iteration 166 : 0.4027008316984556
Loss in iteration 167 : 0.4029313155185857
Loss in iteration 168 : 0.4031620081183224
Loss in iteration 169 : 0.4033936764954597
Loss in iteration 170 : 0.40362555363777647
Testing accuracy  of updater 8 on alg 1 with rate 0.001568 = 0.783, training accuracy 0.8400776950469407, time elapsed: 2380 millisecond.
Loss in iteration 1 : 1.0000008534377987
Loss in iteration 2 : 0.8789295998388655
Loss in iteration 3 : 0.7244460670377759
Loss in iteration 4 : 0.6050158271808671
Loss in iteration 5 : 0.5572497879812716
Loss in iteration 6 : 0.5547402922964462
Loss in iteration 7 : 0.5654597996865771
Loss in iteration 8 : 0.5768254340487345
Testing accuracy  of updater 8 on alg 1 with rate 3.92E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 133 millisecond.
Loss in iteration 1 : 1.015059005976577
Loss in iteration 2 : 4.363102421588543
Loss in iteration 3 : 6.021302259646218
Loss in iteration 4 : 6.033191489508592
Loss in iteration 5 : 4.837819987700516
Loss in iteration 6 : 2.7740350029612024
Loss in iteration 7 : 1.469276942067113
Loss in iteration 8 : 2.3511898098075736
Loss in iteration 9 : 3.6748164646869963
Loss in iteration 10 : 3.5530151754300476
Loss in iteration 11 : 2.6951292588399465
Loss in iteration 12 : 2.266540360024665
Loss in iteration 13 : 2.473252858336634
Loss in iteration 14 : 2.8835470988986955
Loss in iteration 15 : 3.2056497469105096
Loss in iteration 16 : 3.2959442424049046
Loss in iteration 17 : 3.17370478331548
Loss in iteration 18 : 2.968090228976536
Loss in iteration 19 : 2.8092886540518816
Loss in iteration 20 : 2.8052064779856183
Loss in iteration 21 : 2.9637876614776255
Loss in iteration 22 : 3.098450377411886
Loss in iteration 23 : 3.0548964077776417
Loss in iteration 24 : 2.8682300184033185
Loss in iteration 25 : 2.7095079808257143
Loss in iteration 26 : 2.6792646729107625
Loss in iteration 27 : 2.715651969404535
Loss in iteration 28 : 2.725719099754283
Loss in iteration 29 : 2.6604878651970196
Loss in iteration 30 : 2.537765827432001
Loss in iteration 31 : 2.4283818079838855
Loss in iteration 32 : 2.4080777842554277
Loss in iteration 33 : 2.413180719732326
Loss in iteration 34 : 2.3105144688236443
Loss in iteration 35 : 2.1644753813843547
Loss in iteration 36 : 2.12194958641167
Loss in iteration 37 : 2.115187526189783
Loss in iteration 38 : 2.019881984107797
Loss in iteration 39 : 1.9125683839868062
Loss in iteration 40 : 1.9332041044252204
Loss in iteration 41 : 1.7756689900758609
Loss in iteration 42 : 1.8193051689558057
Loss in iteration 43 : 1.698676617889018
Loss in iteration 44 : 1.989354985610856
Loss in iteration 45 : 2.424470458084432
Loss in iteration 46 : 2.7260063736044104
Loss in iteration 47 : 2.2285399640428976
Loss in iteration 48 : 2.045125580223406
Loss in iteration 49 : 2.2061733863555917
Loss in iteration 50 : 1.853739272594842
Loss in iteration 51 : 2.119599096072128
Loss in iteration 52 : 2.140142003827948
Loss in iteration 53 : 1.9858901263909543
Loss in iteration 54 : 2.0219483327858985
Loss in iteration 55 : 2.123712297269394
Loss in iteration 56 : 2.0001593590780247
Loss in iteration 57 : 1.935465127145293
Loss in iteration 58 : 1.9847975747817221
Loss in iteration 59 : 1.9672120765613592
Loss in iteration 60 : 1.872186670897904
Loss in iteration 61 : 1.8443703294790053
Loss in iteration 62 : 1.849624205656498
Loss in iteration 63 : 1.7435779072714377
Loss in iteration 64 : 1.6906425477187457
Loss in iteration 65 : 1.6868717441318783
Loss in iteration 66 : 1.5933413653558017
Loss in iteration 67 : 1.570279627927635
Loss in iteration 68 : 1.4968189902036562
Loss in iteration 69 : 1.478585238114536
Loss in iteration 70 : 1.4175308499758121
Loss in iteration 71 : 1.4663111541491645
Loss in iteration 72 : 1.418950843489487
Loss in iteration 73 : 1.3309552824353283
Loss in iteration 74 : 1.4368226629525724
Loss in iteration 75 : 1.3468985714088515
Loss in iteration 76 : 1.3184378329726179
Loss in iteration 77 : 1.296361259317162
Loss in iteration 78 : 1.2561722064658063
Loss in iteration 79 : 1.2347291758032364
Loss in iteration 80 : 1.2350477117968692
Loss in iteration 81 : 1.187042774977089
Loss in iteration 82 : 1.194468698765792
Loss in iteration 83 : 1.1414201665322694
Loss in iteration 84 : 1.1381998123365296
Loss in iteration 85 : 1.1041437063001192
Loss in iteration 86 : 1.0846102770028256
Loss in iteration 87 : 1.0579917762484403
Loss in iteration 88 : 1.0329939710115452
Loss in iteration 89 : 1.0147449442966696
Loss in iteration 90 : 1.0013018295051763
Loss in iteration 91 : 0.9698184301173826
Loss in iteration 92 : 0.9687807551127111
Loss in iteration 93 : 0.9531247610358147
Loss in iteration 94 : 0.9312914522958264
Loss in iteration 95 : 0.9218416262881828
Loss in iteration 96 : 0.9126923291424037
Loss in iteration 97 : 0.898400659550048
Loss in iteration 98 : 0.8874824579277647
Loss in iteration 99 : 0.8806461503264666
Loss in iteration 100 : 0.8690095404280067
Loss in iteration 101 : 0.8629922768788412
Loss in iteration 102 : 0.8558067536824627
Loss in iteration 103 : 0.8461455045947631
Loss in iteration 104 : 0.8420694603460765
Loss in iteration 105 : 0.8343159329379068
Loss in iteration 106 : 0.8287013975899222
Loss in iteration 107 : 0.822912433489645
Loss in iteration 108 : 0.8183688610547585
Loss in iteration 109 : 0.8137745422500212
Loss in iteration 110 : 0.8087631409515496
Loss in iteration 111 : 0.806555569254575
Loss in iteration 112 : 0.8032640920993765
Loss in iteration 113 : 0.8007739044894485
Loss in iteration 114 : 0.7993477508619959
Loss in iteration 115 : 0.7977598406761417
Loss in iteration 116 : 0.7965661477545103
Loss in iteration 117 : 0.796454509171865
Testing accuracy  of updater 9 on alg 1 with rate 0.02744 = 0.78925, training accuracy 0.842667529944966, time elapsed: 1616 millisecond.
Loss in iteration 1 : 1.001887135298984
Loss in iteration 2 : 1.5012436648642282
Loss in iteration 3 : 2.085392380551121
Loss in iteration 4 : 2.1853940571938257
Loss in iteration 5 : 1.8705614429395787
Loss in iteration 6 : 1.2152461456621853
Loss in iteration 7 : 0.6759370308963827
Loss in iteration 8 : 0.7988995483298315
Loss in iteration 9 : 1.2684998074029117
Loss in iteration 10 : 1.4184468773033905
Loss in iteration 11 : 1.1859030399540629
Loss in iteration 12 : 0.9446044256291088
Loss in iteration 13 : 0.9113196890881293
Loss in iteration 14 : 1.02991962678825
Loss in iteration 15 : 1.1650864155832525
Loss in iteration 16 : 1.2408913839075713
Loss in iteration 17 : 1.2310120706020908
Loss in iteration 18 : 1.1629680500056783
Loss in iteration 19 : 1.085564941026771
Loss in iteration 20 : 1.0469325403449896
Loss in iteration 21 : 1.0754455426579592
Loss in iteration 22 : 1.1389976093132088
Loss in iteration 23 : 1.1682711553809453
Loss in iteration 24 : 1.1361087109088712
Loss in iteration 25 : 1.0698701608477343
Loss in iteration 26 : 1.0257406346133975
Loss in iteration 27 : 1.0262659244355707
Loss in iteration 28 : 1.045149526011631
Loss in iteration 29 : 1.049851013734852
Loss in iteration 30 : 1.0249484861510152
Loss in iteration 31 : 0.9815633321338059
Loss in iteration 32 : 0.9494685978086848
Loss in iteration 33 : 0.9491776003487722
Loss in iteration 34 : 0.9570704770714649
Loss in iteration 35 : 0.9330775001351042
Loss in iteration 36 : 0.8893271643377938
Loss in iteration 37 : 0.8714355501254156
Loss in iteration 38 : 0.8799239409098032
Loss in iteration 39 : 0.8654447323486447
Loss in iteration 40 : 0.8311304879638168
Loss in iteration 41 : 0.831920030687861
Loss in iteration 42 : 0.834189201988871
Loss in iteration 43 : 0.8020659550129805
Loss in iteration 44 : 0.8284871532189235
Loss in iteration 45 : 0.8136614131463419
Loss in iteration 46 : 0.8369814198650739
Loss in iteration 47 : 0.8209370801613352
Loss in iteration 48 : 0.8540652948270246
Loss in iteration 49 : 0.8312353954098453
Loss in iteration 50 : 0.849467675351854
Loss in iteration 51 : 0.8337408830571291
Loss in iteration 52 : 0.84377300444115
Loss in iteration 53 : 0.8370538136197947
Loss in iteration 54 : 0.8463831379101752
Loss in iteration 55 : 0.840120289736463
Loss in iteration 56 : 0.8458074840280982
Loss in iteration 57 : 0.8468054860462606
Loss in iteration 58 : 0.8458124426409546
Loss in iteration 59 : 0.8472789688203944
Loss in iteration 60 : 0.8423906926002843
Loss in iteration 61 : 0.8430131747733828
Loss in iteration 62 : 0.8402149802544405
Loss in iteration 63 : 0.8371587667292899
Loss in iteration 64 : 0.8345382930288763
Loss in iteration 65 : 0.8288163179378065
Loss in iteration 66 : 0.8271628697724821
Loss in iteration 67 : 0.8226449891924656
Loss in iteration 68 : 0.8197515426816149
Loss in iteration 69 : 0.8157462571579526
Loss in iteration 70 : 0.811702209482055
Loss in iteration 71 : 0.8100015034399304
Loss in iteration 72 : 0.8065907461051671
Loss in iteration 73 : 0.8043372569248701
Loss in iteration 74 : 0.8004852455256151
Loss in iteration 75 : 0.79868440028976
Loss in iteration 76 : 0.7961007696362059
Loss in iteration 77 : 0.7940491162544168
Loss in iteration 78 : 0.7915208792391001
Loss in iteration 79 : 0.7893474700702319
Loss in iteration 80 : 0.7880022155665041
Loss in iteration 81 : 0.7862677188902154
Loss in iteration 82 : 0.7846709133205978
Loss in iteration 83 : 0.7826955360178192
Loss in iteration 84 : 0.7809305828975638
Loss in iteration 85 : 0.7795291335592096
Loss in iteration 86 : 0.7778022637494179
Loss in iteration 87 : 0.7762056395161228
Loss in iteration 88 : 0.7743016782437476
Loss in iteration 89 : 0.7726588665450902
Loss in iteration 90 : 0.7711420901653037
Loss in iteration 91 : 0.7695508210904219
Loss in iteration 92 : 0.7679133574952366
Loss in iteration 93 : 0.7663337757252623
Loss in iteration 94 : 0.7650789557502997
Loss in iteration 95 : 0.763922988929385
Loss in iteration 96 : 0.7628653254580761
Loss in iteration 97 : 0.7617768805782348
Loss in iteration 98 : 0.7607823184444951
Loss in iteration 99 : 0.7602156840898955
Testing accuracy  of updater 9 on alg 1 with rate 0.019208 = 0.7875, training accuracy 0.842667529944966, time elapsed: 1308 millisecond.
Loss in iteration 1 : 1.0004557934108784
Loss in iteration 2 : 0.9159597743660794
Loss in iteration 3 : 1.2066419901596723
Loss in iteration 4 : 1.2595166132835147
Loss in iteration 5 : 1.1052143469689235
Loss in iteration 6 : 0.7723036328507472
Loss in iteration 7 : 0.4453198027003299
Loss in iteration 8 : 0.5281477960139223
Loss in iteration 9 : 0.7715697111130247
Loss in iteration 10 : 0.8209292641918435
Loss in iteration 11 : 0.6808005533693432
Loss in iteration 12 : 0.5554219031841092
Loss in iteration 13 : 0.5422699967387415
Loss in iteration 14 : 0.6097383606633063
Loss in iteration 15 : 0.6838155741876895
Loss in iteration 16 : 0.7183133507919442
Loss in iteration 17 : 0.7060275934721227
Loss in iteration 18 : 0.6643434425362209
Loss in iteration 19 : 0.6239236037755526
Loss in iteration 20 : 0.6102732569806575
Loss in iteration 21 : 0.6323823315131519
Loss in iteration 22 : 0.6670292399097392
Loss in iteration 23 : 0.686074338671006
Loss in iteration 24 : 0.6769224196441244
Loss in iteration 25 : 0.6488563154515705
Loss in iteration 26 : 0.625110732259395
Loss in iteration 27 : 0.6198238483603513
Loss in iteration 28 : 0.6310876847555196
Loss in iteration 29 : 0.6434117654920416
Loss in iteration 30 : 0.6436638143820842
Loss in iteration 31 : 0.6297622661896487
Loss in iteration 32 : 0.6115680947185894
Loss in iteration 33 : 0.604167354931206
Loss in iteration 34 : 0.6076618481074831
Loss in iteration 35 : 0.6138187180734662
Loss in iteration 36 : 0.6101588837572418
Loss in iteration 37 : 0.5962445797972021
Loss in iteration 38 : 0.5852315155996229
Loss in iteration 39 : 0.5850065964428328
Loss in iteration 40 : 0.5899558805165745
Loss in iteration 41 : 0.5887241011515024
Loss in iteration 42 : 0.5800239944437748
Loss in iteration 43 : 0.5744266311366725
Loss in iteration 44 : 0.5803127837620277
Loss in iteration 45 : 0.5848963383541446
Loss in iteration 46 : 0.5782384017989042
Loss in iteration 47 : 0.5777873698622676
Loss in iteration 48 : 0.588322644478819
Loss in iteration 49 : 0.5909977997458802
Loss in iteration 50 : 0.590675760959951
Loss in iteration 51 : 0.6014348470399592
Loss in iteration 52 : 0.6062759679278182
Loss in iteration 53 : 0.6055020192580935
Loss in iteration 54 : 0.6144636683874378
Loss in iteration 55 : 0.6197055636201689
Loss in iteration 56 : 0.6203696730649191
Loss in iteration 57 : 0.6276829827779544
Loss in iteration 58 : 0.631214548814413
Loss in iteration 59 : 0.632156229053785
Loss in iteration 60 : 0.638020444234594
Loss in iteration 61 : 0.6424176018937737
Loss in iteration 62 : 0.6448990880875328
Loss in iteration 63 : 0.6494579836188232
Loss in iteration 64 : 0.6542043781528224
Loss in iteration 65 : 0.6565780128344614
Loss in iteration 66 : 0.6597757756720917
Loss in iteration 67 : 0.6640326630593347
Loss in iteration 68 : 0.6672719891878058
Loss in iteration 69 : 0.6698352521277626
Loss in iteration 70 : 0.6730440097964665
Loss in iteration 71 : 0.6759094575042002
Loss in iteration 72 : 0.6778423407951669
Loss in iteration 73 : 0.6797545681092262
Loss in iteration 74 : 0.6820609302691595
Loss in iteration 75 : 0.6840227781724546
Loss in iteration 76 : 0.6855157165071917
Loss in iteration 77 : 0.6871073985109474
Loss in iteration 78 : 0.6886889077982759
Loss in iteration 79 : 0.6897486934795942
Loss in iteration 80 : 0.690766008579411
Loss in iteration 81 : 0.6920442883143869
Loss in iteration 82 : 0.693190712781045
Loss in iteration 83 : 0.6942071652276424
Loss in iteration 84 : 0.6952433925776954
Loss in iteration 85 : 0.6962868750827506
Loss in iteration 86 : 0.6971642729103044
Loss in iteration 87 : 0.6979832449091176
Loss in iteration 88 : 0.6989432367921489
Loss in iteration 89 : 0.6999155838321516
Loss in iteration 90 : 0.7007805082988615
Loss in iteration 91 : 0.7016247705333319
Loss in iteration 92 : 0.702422392919415
Testing accuracy  of updater 9 on alg 1 with rate 0.010976 = 0.78875, training accuracy 0.8416963418582065, time elapsed: 1178 millisecond.
Loss in iteration 1 : 1.0000238104231236
Loss in iteration 2 : 0.5612549968443798
Loss in iteration 3 : 0.6205878287697587
Loss in iteration 4 : 0.6986432773711486
Loss in iteration 5 : 0.723042209571252
Loss in iteration 6 : 0.6991681180435767
Loss in iteration 7 : 0.6332392289806275
Loss in iteration 8 : 0.5346933219454454
Loss in iteration 9 : 0.43980970707944456
Loss in iteration 10 : 0.4095459217354671
Loss in iteration 11 : 0.46108802502428253
Loss in iteration 12 : 0.5036143122601965
Loss in iteration 13 : 0.4933351590590477
Loss in iteration 14 : 0.44621351161015255
Loss in iteration 15 : 0.39975638934564256
Loss in iteration 16 : 0.3824077451622588
Loss in iteration 17 : 0.39172403285421054
Loss in iteration 18 : 0.4109198740598491
Loss in iteration 19 : 0.4252238103940935
Loss in iteration 20 : 0.4277658429713255
Loss in iteration 21 : 0.4195463624376239
Loss in iteration 22 : 0.4066200093033942
Loss in iteration 23 : 0.39566133493338557
Loss in iteration 24 : 0.3917166872991103
Loss in iteration 25 : 0.3945601891036452
Loss in iteration 26 : 0.4019676146606314
Loss in iteration 27 : 0.40905617878914746
Loss in iteration 28 : 0.412830565013091
Loss in iteration 29 : 0.4123701292164454
Loss in iteration 30 : 0.4088295478235083
Loss in iteration 31 : 0.40423917920606417
Loss in iteration 32 : 0.4012629889316581
Loss in iteration 33 : 0.40117395303069814
Loss in iteration 34 : 0.40322152610034656
Loss in iteration 35 : 0.40617250934740173
Loss in iteration 36 : 0.40885183202998227
Loss in iteration 37 : 0.4101378018945673
Loss in iteration 38 : 0.40984652526875304
Loss in iteration 39 : 0.4085318898860969
Loss in iteration 40 : 0.40719487822728706
Loss in iteration 41 : 0.40639270399307614
Loss in iteration 42 : 0.40659811040899724
Loss in iteration 43 : 0.407692759865502
Loss in iteration 44 : 0.4093703164179338
Loss in iteration 45 : 0.41068126996601095
Loss in iteration 46 : 0.41112922023015613
Loss in iteration 47 : 0.410778071816036
Loss in iteration 48 : 0.41021500915531545
Loss in iteration 49 : 0.4099908667263725
Loss in iteration 50 : 0.4103363195635682
Loss in iteration 51 : 0.41128419350030326
Loss in iteration 52 : 0.41237891116931835
Loss in iteration 53 : 0.41328997318566374
Loss in iteration 54 : 0.4138326132363266
Loss in iteration 55 : 0.41410091231589696
Loss in iteration 56 : 0.4143310175080878
Loss in iteration 57 : 0.4147699067442034
Loss in iteration 58 : 0.415579514533152
Loss in iteration 59 : 0.4166498557374638
Loss in iteration 60 : 0.4177153101151479
Loss in iteration 61 : 0.4185872302612775
Loss in iteration 62 : 0.4192536709770387
Loss in iteration 63 : 0.41980503294930704
Loss in iteration 64 : 0.42045008686143903
Loss in iteration 65 : 0.4212916932381198
Loss in iteration 66 : 0.42232338344187687
Loss in iteration 67 : 0.4234014075158195
Loss in iteration 68 : 0.4244062495560471
Loss in iteration 69 : 0.4253158824622407
Loss in iteration 70 : 0.42617790782165255
Loss in iteration 71 : 0.42710905513568287
Loss in iteration 72 : 0.42811992373986735
Loss in iteration 73 : 0.42923053231991304
Loss in iteration 74 : 0.4303521792862702
Loss in iteration 75 : 0.4314177623137563
Loss in iteration 76 : 0.4324387216055747
Loss in iteration 77 : 0.4334464035249213
Loss in iteration 78 : 0.43448020206433113
Loss in iteration 79 : 0.4355879683063347
Loss in iteration 80 : 0.436730847165496
Loss in iteration 81 : 0.4378613656108512
Loss in iteration 82 : 0.4389538714344062
Loss in iteration 83 : 0.44002619238922996
Loss in iteration 84 : 0.4411275813613113
Loss in iteration 85 : 0.4422638196703342
Loss in iteration 86 : 0.4434021691430165
Loss in iteration 87 : 0.4445321223781038
Loss in iteration 88 : 0.4456507910427623
Loss in iteration 89 : 0.44675826656923534
Loss in iteration 90 : 0.4478601008137002
Loss in iteration 91 : 0.4489663522192152
Loss in iteration 92 : 0.45008600134625526
Loss in iteration 93 : 0.45121971473687184
Loss in iteration 94 : 0.45236009272413213
Loss in iteration 95 : 0.45349420569876436
Loss in iteration 96 : 0.4546211388790225
Loss in iteration 97 : 0.4557509935131435
Loss in iteration 98 : 0.45688424012298756
Loss in iteration 99 : 0.45801868765887765
Loss in iteration 100 : 0.45915251783168104
Loss in iteration 101 : 0.460281584688341
Loss in iteration 102 : 0.461402844385
Loss in iteration 103 : 0.46251785939826645
Loss in iteration 104 : 0.4636308847871
Loss in iteration 105 : 0.46474313665402067
Loss in iteration 106 : 0.46585621045055264
Loss in iteration 107 : 0.4669706028933588
Loss in iteration 108 : 0.46808323189007894
Loss in iteration 109 : 0.4691934110395033
Loss in iteration 110 : 0.4703015752875986
Loss in iteration 111 : 0.47140751086931865
Loss in iteration 112 : 0.47251348619879247
Loss in iteration 113 : 0.47361913976836456
Loss in iteration 114 : 0.4747228853236169
Loss in iteration 115 : 0.47582549030441795
Loss in iteration 116 : 0.47692639496386985
Loss in iteration 117 : 0.478025284233321
Loss in iteration 118 : 0.47912312577581806
Loss in iteration 119 : 0.4802199688488728
Loss in iteration 120 : 0.4813160470186596
Loss in iteration 121 : 0.4824110711603693
Loss in iteration 122 : 0.4835052460864942
Loss in iteration 123 : 0.48459778221367006
Loss in iteration 124 : 0.48568866081790457
Loss in iteration 125 : 0.4867764787741259
Loss in iteration 126 : 0.48786195528467713
Loss in iteration 127 : 0.4889452233677768
Loss in iteration 128 : 0.4900265883733377
Loss in iteration 129 : 0.49110632372449264
Loss in iteration 130 : 0.49218313475857217
Loss in iteration 131 : 0.49325605055444754
Loss in iteration 132 : 0.4943256135637875
Loss in iteration 133 : 0.49539206410529857
Loss in iteration 134 : 0.49645576607972364
Loss in iteration 135 : 0.4975170461872518
Loss in iteration 136 : 0.49857467310557874
Loss in iteration 137 : 0.49962888234118463
Loss in iteration 138 : 0.5006798650317406
Loss in iteration 139 : 0.5017279787248408
Loss in iteration 140 : 0.5027723886161364
Loss in iteration 141 : 0.5038122254532676
Loss in iteration 142 : 0.5048476399580627
Loss in iteration 143 : 0.5058790639314865
Loss in iteration 144 : 0.5069069095444604
Loss in iteration 145 : 0.507930455659571
Loss in iteration 146 : 0.5089501847416533
Loss in iteration 147 : 0.5099649216076605
Loss in iteration 148 : 0.5109745256090708
Loss in iteration 149 : 0.5119800661596169
Loss in iteration 150 : 0.5129820796896897
Loss in iteration 151 : 0.5139822124945
Loss in iteration 152 : 0.514980840449376
Loss in iteration 153 : 0.5159780004313057
Loss in iteration 154 : 0.5169753548116194
Loss in iteration 155 : 0.5179719000112714
Loss in iteration 156 : 0.5189652011994272
Loss in iteration 157 : 0.5199536647855605
Loss in iteration 158 : 0.5209377692579259
Loss in iteration 159 : 0.521918278694608
Loss in iteration 160 : 0.522895681469354
Loss in iteration 161 : 0.5238697937553707
Loss in iteration 162 : 0.5248409173287503
Loss in iteration 163 : 0.5258093231941334
Loss in iteration 164 : 0.5267697440134393
Loss in iteration 165 : 0.5277222538749782
Loss in iteration 166 : 0.5286660894178518
Loss in iteration 167 : 0.5296009327270039
Loss in iteration 168 : 0.5305242371994459
Loss in iteration 169 : 0.5314343143631963
Loss in iteration 170 : 0.5323324167367599
Loss in iteration 171 : 0.5332189823966202
Loss in iteration 172 : 0.5340951176553295
Loss in iteration 173 : 0.5349617270371301
Loss in iteration 174 : 0.5358188469719584
Loss in iteration 175 : 0.5366676356460065
Loss in iteration 176 : 0.5375078028746205
Loss in iteration 177 : 0.5383361381291125
Loss in iteration 178 : 0.5391552818097305
Loss in iteration 179 : 0.5399656070993403
Loss in iteration 180 : 0.5407664292276572
Loss in iteration 181 : 0.5415601919522337
Loss in iteration 182 : 0.542347672450828
Loss in iteration 183 : 0.5431284942260174
Loss in iteration 184 : 0.543903350999905
Loss in iteration 185 : 0.5446736133554884
Loss in iteration 186 : 0.5454404551103673
Loss in iteration 187 : 0.5462007895210241
Loss in iteration 188 : 0.5469554318629016
Loss in iteration 189 : 0.547706310776074
Loss in iteration 190 : 0.5484537325821159
Loss in iteration 191 : 0.5491965294088401
Loss in iteration 192 : 0.5499349852598595
Loss in iteration 193 : 0.5506678340617839
Loss in iteration 194 : 0.5513955362331522
Loss in iteration 195 : 0.5521184509077663
Loss in iteration 196 : 0.5528345585571921
Loss in iteration 197 : 0.5535435345793006
Loss in iteration 198 : 0.5542451631687241
Loss in iteration 199 : 0.5549396223409786
Loss in iteration 200 : 0.5556275209090935
Testing accuracy  of updater 9 on alg 1 with rate 0.002744 = 0.786, training accuracy 0.840401424409194, time elapsed: 2641 millisecond.
Loss in iteration 1 : 1.0000109633490653
Loss in iteration 2 : 0.6402547339288243
Loss in iteration 3 : 0.572657942510188
Loss in iteration 4 : 0.6564643349429228
Loss in iteration 5 : 0.7057190737134843
Loss in iteration 6 : 0.719438606873524
Loss in iteration 7 : 0.701396918391076
Loss in iteration 8 : 0.655473012522008
Loss in iteration 9 : 0.5859717083707643
Loss in iteration 10 : 0.5039601246855997
Loss in iteration 11 : 0.43698182994732154
Loss in iteration 12 : 0.4123596701097924
Loss in iteration 13 : 0.4415924515393406
Loss in iteration 14 : 0.47997498974731806
Loss in iteration 15 : 0.491677001930384
Loss in iteration 16 : 0.47181183400831134
Loss in iteration 17 : 0.4345370691801545
Loss in iteration 18 : 0.4004790445652518
Loss in iteration 19 : 0.3836649030505688
Loss in iteration 20 : 0.38450015163512474
Loss in iteration 21 : 0.3952618368713159
Loss in iteration 22 : 0.4070985884586816
Loss in iteration 23 : 0.4139440057744164
Loss in iteration 24 : 0.4136564530931741
Loss in iteration 25 : 0.4073214186051712
Loss in iteration 26 : 0.3983085247220834
Loss in iteration 27 : 0.3898289908816353
Loss in iteration 28 : 0.38457848205949036
Loss in iteration 29 : 0.38357589477538195
Loss in iteration 30 : 0.3860902599183868
Loss in iteration 31 : 0.39035512331182987
Loss in iteration 32 : 0.3942248019611155
Loss in iteration 33 : 0.3963438877234885
Loss in iteration 34 : 0.3962273191846901
Loss in iteration 35 : 0.3943956058995166
Loss in iteration 36 : 0.39176141931800473
Loss in iteration 37 : 0.3892814001574669
Loss in iteration 38 : 0.38800426591938836
Loss in iteration 39 : 0.3881200075815463
Loss in iteration 40 : 0.38927189102071713
Loss in iteration 41 : 0.3907438786383908
Loss in iteration 42 : 0.3921493363504914
Loss in iteration 43 : 0.39301659585658455
Loss in iteration 44 : 0.3932408367685534
Loss in iteration 45 : 0.3928929659774371
Loss in iteration 46 : 0.39224964973635235
Loss in iteration 47 : 0.391663050150122
Loss in iteration 48 : 0.39129926893102124
Loss in iteration 49 : 0.3912902079531558
Loss in iteration 50 : 0.3918052237251226
Loss in iteration 51 : 0.3925856518496835
Loss in iteration 52 : 0.3933379840696349
Loss in iteration 53 : 0.39388674592710293
Loss in iteration 54 : 0.3941393178934432
Loss in iteration 55 : 0.39413504410362943
Loss in iteration 56 : 0.3940240050174468
Loss in iteration 57 : 0.39397627586423084
Loss in iteration 58 : 0.3941113192129009
Loss in iteration 59 : 0.3944510666492205
Loss in iteration 60 : 0.39492448917509687
Loss in iteration 61 : 0.39546791241707907
Loss in iteration 62 : 0.39594318843257187
Loss in iteration 63 : 0.39631403654789094
Loss in iteration 64 : 0.39658581279279065
Loss in iteration 65 : 0.39680577426484703
Loss in iteration 66 : 0.3970456804449684
Loss in iteration 67 : 0.39736104165520597
Loss in iteration 68 : 0.3977690170130318
Loss in iteration 69 : 0.3982455405500043
Loss in iteration 70 : 0.3987242654261879
Loss in iteration 71 : 0.39920454807062383
Loss in iteration 72 : 0.39964407158885235
Loss in iteration 73 : 0.40004605912166424
Loss in iteration 74 : 0.40043211758719993
Loss in iteration 75 : 0.4008230488202299
Loss in iteration 76 : 0.4012319840934054
Loss in iteration 77 : 0.40167475543691017
Loss in iteration 78 : 0.4021571795199931
Loss in iteration 79 : 0.40266394177055875
Loss in iteration 80 : 0.40317047660243654
Loss in iteration 81 : 0.40366634236664023
Loss in iteration 82 : 0.4041473646976727
Loss in iteration 83 : 0.40462531308264393
Loss in iteration 84 : 0.4051126067931779
Loss in iteration 85 : 0.4056253301747428
Loss in iteration 86 : 0.4061483218038231
Loss in iteration 87 : 0.40667527411690185
Loss in iteration 88 : 0.40720278436558316
Loss in iteration 89 : 0.4077303651712975
Loss in iteration 90 : 0.4082580285772608
Loss in iteration 91 : 0.4087839207339038
Loss in iteration 92 : 0.40930898008677596
Loss in iteration 93 : 0.40983383632162057
Loss in iteration 94 : 0.4103631182026255
Loss in iteration 95 : 0.410898581609532
Loss in iteration 96 : 0.41143852996279845
Loss in iteration 97 : 0.4119880217647283
Loss in iteration 98 : 0.4125426220166558
Loss in iteration 99 : 0.4130997426561337
Loss in iteration 100 : 0.41365755897992057
Loss in iteration 101 : 0.41421609805262055
Loss in iteration 102 : 0.4147760900508454
Loss in iteration 103 : 0.4153381428261609
Loss in iteration 104 : 0.4159018458307663
Loss in iteration 105 : 0.416469278588109
Loss in iteration 106 : 0.4170396158800215
Loss in iteration 107 : 0.417611154500313
Loss in iteration 108 : 0.41818419267280094
Loss in iteration 109 : 0.41875632534616775
Loss in iteration 110 : 0.4193290483113449
Loss in iteration 111 : 0.4199017540482052
Loss in iteration 112 : 0.4204742699372308
Loss in iteration 113 : 0.42104739794442597
Loss in iteration 114 : 0.4216209261385109
Loss in iteration 115 : 0.4221949945223721
Loss in iteration 116 : 0.4227703241754828
Loss in iteration 117 : 0.4233466189065267
Loss in iteration 118 : 0.4239246705110062
Loss in iteration 119 : 0.42450424574242507
Loss in iteration 120 : 0.42508530038114006
Loss in iteration 121 : 0.4256678685107874
Loss in iteration 122 : 0.42625197898074446
Loss in iteration 123 : 0.42683710092069027
Loss in iteration 124 : 0.42742267310747145
Loss in iteration 125 : 0.42800868868856634
Loss in iteration 126 : 0.4285945464123684
Loss in iteration 127 : 0.4291799426859946
Loss in iteration 128 : 0.42976489887406355
Loss in iteration 129 : 0.4303490415486884
Loss in iteration 130 : 0.430934014907041
Loss in iteration 131 : 0.4315185439473389
Loss in iteration 132 : 0.4321022321714169
Loss in iteration 133 : 0.4326859555217522
Loss in iteration 134 : 0.4332707000751064
Loss in iteration 135 : 0.4338552748504765
Loss in iteration 136 : 0.4344387378314687
Loss in iteration 137 : 0.43502134052968333
Loss in iteration 138 : 0.43560318461793474
Loss in iteration 139 : 0.4361850366201865
Loss in iteration 140 : 0.43676643821848515
Loss in iteration 141 : 0.4373470932255353
Loss in iteration 142 : 0.43792715261754755
Loss in iteration 143 : 0.43850643033223813
Loss in iteration 144 : 0.43908490449047055
Loss in iteration 145 : 0.43966148268968014
Loss in iteration 146 : 0.4402358643685064
Loss in iteration 147 : 0.4408083561846582
Loss in iteration 148 : 0.4413802157019847
Loss in iteration 149 : 0.44195163897130674
Loss in iteration 150 : 0.4425222104774813
Loss in iteration 151 : 0.4430917999603379
Loss in iteration 152 : 0.4436606770161986
Loss in iteration 153 : 0.44422866286438134
Loss in iteration 154 : 0.4447958754287538
Loss in iteration 155 : 0.4453616655927164
Loss in iteration 156 : 0.4459264848922756
Loss in iteration 157 : 0.44649034648376634
Loss in iteration 158 : 0.44705341893036915
Loss in iteration 159 : 0.4476152936865172
Loss in iteration 160 : 0.44817614357042473
Loss in iteration 161 : 0.4487364511584293
Loss in iteration 162 : 0.44929609828577854
Loss in iteration 163 : 0.44985528337273323
Loss in iteration 164 : 0.45041372445089223
Loss in iteration 165 : 0.45097167991093023
Loss in iteration 166 : 0.45152916883258515
Loss in iteration 167 : 0.45208630103077685
Loss in iteration 168 : 0.4526431748875412
Loss in iteration 169 : 0.45319987844612153
Loss in iteration 170 : 0.4537564904018724
Loss in iteration 171 : 0.454312418506504
Loss in iteration 172 : 0.45486767379715215
Loss in iteration 173 : 0.4554222839339983
Loss in iteration 174 : 0.4559763677502798
Loss in iteration 175 : 0.4565295501192357
Loss in iteration 176 : 0.45708165260771905
Loss in iteration 177 : 0.4576327683017938
Loss in iteration 178 : 0.4581830437997013
Loss in iteration 179 : 0.4587326107633344
Loss in iteration 180 : 0.45928234534128365
Loss in iteration 181 : 0.4598327087171085
Loss in iteration 182 : 0.4603824129362766
Loss in iteration 183 : 0.46093145201756514
Loss in iteration 184 : 0.46147989946824114
Loss in iteration 185 : 0.46202781720540625
Loss in iteration 186 : 0.4625757268246061
Loss in iteration 187 : 0.4631236188057526
Loss in iteration 188 : 0.463671135217982
Loss in iteration 189 : 0.46421845410738355
Loss in iteration 190 : 0.464765562078721
Loss in iteration 191 : 0.4653125194178342
Loss in iteration 192 : 0.46585937985433334
Loss in iteration 193 : 0.46640619119805715
Loss in iteration 194 : 0.46695299591442607
Loss in iteration 195 : 0.4674998316445066
Loss in iteration 196 : 0.4680467316750561
Loss in iteration 197 : 0.46859372536332655
Loss in iteration 198 : 0.469140838520953
Loss in iteration 199 : 0.4696880937608382
Loss in iteration 200 : 0.4702355108105858
Testing accuracy  of updater 9 on alg 1 with rate 0.0019207999999999999 = 0.78525, training accuracy 0.8394302363224344, time elapsed: 2797 millisecond.
Loss in iteration 1 : 1.0000032914022194
Loss in iteration 2 : 0.786759189653491
Loss in iteration 3 : 0.5622679463820213
Loss in iteration 4 : 0.5721261950757813
Loss in iteration 5 : 0.6279244823573318
Loss in iteration 6 : 0.6661301544272241
Loss in iteration 7 : 0.684406402949796
Loss in iteration 8 : 0.6846202740121984
Loss in iteration 9 : 0.6687756781319494
Loss in iteration 10 : 0.6388479520049984
Loss in iteration 11 : 0.5969189830929681
Loss in iteration 12 : 0.546540356728078
Loss in iteration 13 : 0.49703679830148295
Loss in iteration 14 : 0.4588530578969335
Loss in iteration 15 : 0.43902509895737024
Loss in iteration 16 : 0.43723469695041534
Loss in iteration 17 : 0.44914026910798627
Loss in iteration 18 : 0.4648434745351582
Loss in iteration 19 : 0.47172590552610966
Loss in iteration 20 : 0.4657195174364153
Loss in iteration 21 : 0.44947162873478447
Loss in iteration 22 : 0.42862511935211917
Loss in iteration 23 : 0.40973150629789035
Loss in iteration 24 : 0.39666653167892896
Loss in iteration 25 : 0.39150520942867184
Loss in iteration 26 : 0.3922782245658075
Loss in iteration 27 : 0.3961134990468108
Loss in iteration 28 : 0.3998721619275739
Loss in iteration 29 : 0.4017863181676379
Loss in iteration 30 : 0.40101400667232223
Loss in iteration 31 : 0.39794651519256824
Loss in iteration 32 : 0.3935600104922926
Loss in iteration 33 : 0.3890491335907765
Loss in iteration 34 : 0.38534405529741483
Loss in iteration 35 : 0.3828196630999974
Loss in iteration 36 : 0.38189056688871126
Loss in iteration 37 : 0.3820482569439763
Loss in iteration 38 : 0.38293956538045404
Loss in iteration 39 : 0.38408938110700536
Loss in iteration 40 : 0.38509259970027815
Loss in iteration 41 : 0.3855838841597113
Loss in iteration 42 : 0.38546552160283665
Loss in iteration 43 : 0.38483525866539686
Loss in iteration 44 : 0.383911480304613
Loss in iteration 45 : 0.38289576449453877
Loss in iteration 46 : 0.38205033242525294
Loss in iteration 47 : 0.3815226553105115
Loss in iteration 48 : 0.38138199341730034
Loss in iteration 49 : 0.38158510913852123
Loss in iteration 50 : 0.381977230100524
Loss in iteration 51 : 0.38239484116865946
Loss in iteration 52 : 0.3827789621449961
Loss in iteration 53 : 0.3830243586912035
Loss in iteration 54 : 0.38310226652667767
Loss in iteration 55 : 0.38302743541517525
Loss in iteration 56 : 0.3828838135694666
Loss in iteration 57 : 0.3827301463998529
Loss in iteration 58 : 0.382618843055328
Loss in iteration 59 : 0.3825915914094464
Loss in iteration 60 : 0.38270039205825845
Loss in iteration 61 : 0.3828950333292226
Loss in iteration 62 : 0.3831310529471002
Loss in iteration 63 : 0.3833760118891854
Loss in iteration 64 : 0.38358723497291475
Loss in iteration 65 : 0.3837506912790525
Loss in iteration 66 : 0.38386949069873255
Loss in iteration 67 : 0.3839517636116417
Loss in iteration 68 : 0.38400821884857883
Loss in iteration 69 : 0.384059337331576
Loss in iteration 70 : 0.3841575949482834
Loss in iteration 71 : 0.3842819672021623
Loss in iteration 72 : 0.38443535221903713
Loss in iteration 73 : 0.3846137463302519
Loss in iteration 74 : 0.38480009150189787
Loss in iteration 75 : 0.38498709820048777
Loss in iteration 76 : 0.3851612285316515
Loss in iteration 77 : 0.38531955771308846
Loss in iteration 78 : 0.38546560068221214
Loss in iteration 79 : 0.38560766209358166
Loss in iteration 80 : 0.3857489767582586
Loss in iteration 81 : 0.3858960456152953
Loss in iteration 82 : 0.3860560750815285
Loss in iteration 83 : 0.3862287606507636
Loss in iteration 84 : 0.3864069715597686
Loss in iteration 85 : 0.386589834000869
Loss in iteration 86 : 0.3867729997280659
Loss in iteration 87 : 0.38695375444022234
Loss in iteration 88 : 0.387131548604445
Loss in iteration 89 : 0.38730552808367524
Loss in iteration 90 : 0.3874772341565454
Loss in iteration 91 : 0.3876474953945845
Loss in iteration 92 : 0.38781794559687466
Loss in iteration 93 : 0.387992142056128
Loss in iteration 94 : 0.38817044337040013
Loss in iteration 95 : 0.3883559391997764
Loss in iteration 96 : 0.3885439149475777
Loss in iteration 97 : 0.3887342603998558
Loss in iteration 98 : 0.38892481406896
Loss in iteration 99 : 0.3891146854487766
Loss in iteration 100 : 0.38930388946272143
Loss in iteration 101 : 0.38949311368695183
Loss in iteration 102 : 0.3896826639797068
Loss in iteration 103 : 0.38987384396520325
Loss in iteration 104 : 0.3900692881492922
Loss in iteration 105 : 0.390265654513392
Loss in iteration 106 : 0.3904627695812646
Loss in iteration 107 : 0.39066049207948017
Loss in iteration 108 : 0.3908585719891677
Loss in iteration 109 : 0.39105706617128405
Loss in iteration 110 : 0.39125602568209567
Loss in iteration 111 : 0.39145549631567195
Loss in iteration 112 : 0.39165551909684654
Loss in iteration 113 : 0.3918561307289877
Loss in iteration 114 : 0.39205768627748977
Loss in iteration 115 : 0.3922600709520086
Loss in iteration 116 : 0.3924638409484169
Loss in iteration 117 : 0.3926687255879319
Loss in iteration 118 : 0.39287444559734264
Loss in iteration 119 : 0.39308100032146764
Loss in iteration 120 : 0.39328829016410843
Loss in iteration 121 : 0.39349623400091743
Loss in iteration 122 : 0.39370463389695415
Loss in iteration 123 : 0.39391373128061
Loss in iteration 124 : 0.39412369901100175
Loss in iteration 125 : 0.39433437618264267
Loss in iteration 126 : 0.3945455677464636
Loss in iteration 127 : 0.39475726537703454
Loss in iteration 128 : 0.3949695015333908
Loss in iteration 129 : 0.3951823051721818
Loss in iteration 130 : 0.39539560234494586
Loss in iteration 131 : 0.39560958708746313
Loss in iteration 132 : 0.39582417744768617
Loss in iteration 133 : 0.39603939347737427
Loss in iteration 134 : 0.3962552529414851
Loss in iteration 135 : 0.3964717715345195
Loss in iteration 136 : 0.3966889630767066
Loss in iteration 137 : 0.39690683969188445
Loss in iteration 138 : 0.39712541196875695
Loss in iteration 139 : 0.397344689107057
Loss in iteration 140 : 0.3975646790500169
Loss in iteration 141 : 0.39778538860440743
Loss in iteration 142 : 0.39800682354930467
Loss in iteration 143 : 0.39822881519158
Loss in iteration 144 : 0.39845125045600804
Loss in iteration 145 : 0.39867398187689507
Loss in iteration 146 : 0.3988972790198809
Loss in iteration 147 : 0.39912117949228565
Loss in iteration 148 : 0.3993456799899596
Loss in iteration 149 : 0.3995710268179414
Loss in iteration 150 : 0.3997972394948142
Loss in iteration 151 : 0.4000239611080341
Loss in iteration 152 : 0.400251253620475
Loss in iteration 153 : 0.40047910530295205
Loss in iteration 154 : 0.400707459669299
Loss in iteration 155 : 0.4009362459948698
Loss in iteration 156 : 0.40116544686589045
Loss in iteration 157 : 0.4013950588762652
Loss in iteration 158 : 0.4016251083796864
Loss in iteration 159 : 0.40185561887751364
Loss in iteration 160 : 0.4020867756563895
Loss in iteration 161 : 0.40231874851934024
Loss in iteration 162 : 0.4025513954769774
Loss in iteration 163 : 0.402784685953742
Loss in iteration 164 : 0.40301862336467276
Loss in iteration 165 : 0.40325321043522083
Loss in iteration 166 : 0.40348844927051075
Loss in iteration 167 : 0.4037243414177749
Loss in iteration 168 : 0.403960887922639
Loss in iteration 169 : 0.4041980538102478
Loss in iteration 170 : 0.4044356223481483
Loss in iteration 171 : 0.4046734841010645
Loss in iteration 172 : 0.4049116681788408
Testing accuracy  of updater 9 on alg 1 with rate 0.0010976 = 0.78275, training accuracy 0.8397539656846876, time elapsed: 2211 millisecond.
Loss in iteration 1 : 1.000000181892725
Loss in iteration 2 : 0.9511293172936165
Testing accuracy  of updater 9 on alg 1 with rate 2.7439999999999973E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 42 millisecond.
