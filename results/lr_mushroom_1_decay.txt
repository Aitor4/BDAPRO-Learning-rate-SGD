objc[3402]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10adce4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10bdf34e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 10:04:28 INFO SparkContext: Running Spark version 2.0.0
18/02/27 10:04:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 10:04:29 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 10:04:29 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 10:04:29 INFO SecurityManager: Changing view acls groups to: 
18/02/27 10:04:29 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 10:04:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 10:04:30 INFO Utils: Successfully started service 'sparkDriver' on port 50738.
18/02/27 10:04:30 INFO SparkEnv: Registering MapOutputTracker
18/02/27 10:04:30 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 10:04:30 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-a98dc6c1-2673-4630-9c48-dde3b227c634
18/02/27 10:04:30 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 10:04:30 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 10:04:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 10:04:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 10:04:30 INFO Executor: Starting executor ID driver on host localhost
18/02/27 10:04:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50739.
18/02/27 10:04:31 INFO NettyBlockTransferService: Server created on 192.168.2.140:50739
18/02/27 10:04:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50739)
18/02/27 10:04:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50739 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50739)
18/02/27 10:04:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50739)
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 3.4814981507017624
Loss in iteration 3 : 1.755027494250378
Loss in iteration 4 : 5.167609065703567
Loss in iteration 5 : 0.4640935090454568
Loss in iteration 6 : 0.38653018935575023
Loss in iteration 7 : 0.34355097055793227
Loss in iteration 8 : 0.30719618636751467
Loss in iteration 9 : 0.27510877471294115
Loss in iteration 10 : 0.24686007087168402
Loss in iteration 11 : 0.2221950903922793
Loss in iteration 12 : 0.20068309330989642
Loss in iteration 13 : 0.18180459998261264
Loss in iteration 14 : 0.165107305527046
Loss in iteration 15 : 0.15023454739892475
Loss in iteration 16 : 0.1369572354197227
Loss in iteration 17 : 0.1251932990518128
Loss in iteration 18 : 0.11494803620847088
Loss in iteration 19 : 0.10618583066032035
Loss in iteration 20 : 0.09876232646299543
Loss in iteration 21 : 0.09246613262626527
Loss in iteration 22 : 0.08708444639977429
Loss in iteration 23 : 0.08243747466389685
Loss in iteration 24 : 0.07838423848478762
Loss in iteration 25 : 0.07481600143377386
Loss in iteration 26 : 0.07164780772099064
Loss in iteration 27 : 0.06881191829151369
Loss in iteration 28 : 0.06625348622555308
Loss in iteration 29 : 0.06392776725947723
Loss in iteration 30 : 0.061798187248170634
Loss in iteration 31 : 0.059834868316982946
Loss in iteration 32 : 0.0580134311321487
Loss in iteration 33 : 0.056313998806792956
Loss in iteration 34 : 0.054720368556791806
Loss in iteration 35 : 0.05321932850583429
Loss in iteration 36 : 0.05180009910274911
Loss in iteration 37 : 0.05045387919734303
Loss in iteration 38 : 0.049173478027944466
Loss in iteration 39 : 0.04795301634193956
Loss in iteration 40 : 0.04678768227163229
Loss in iteration 41 : 0.04567353011004452
Loss in iteration 42 : 0.044607312574384786
Loss in iteration 43 : 0.04358633939604695
Loss in iteration 44 : 0.04260835706698148
Loss in iteration 45 : 0.04167144624946988
Loss in iteration 46 : 0.040773934670848305
Loss in iteration 47 : 0.039914324240230115
Loss in iteration 48 : 0.03909123163384323
Loss in iteration 49 : 0.038303341736590585
Loss in iteration 50 : 0.03754937318442063
Loss in iteration 51 : 0.03682805494379285
Loss in iteration 52 : 0.036138112518413416
Loss in iteration 53 : 0.03547826209750749
Loss in iteration 54 : 0.03484721082216966
Loss in iteration 55 : 0.0342436613680155
Loss in iteration 56 : 0.033666319204838924
Loss in iteration 57 : 0.03311390115411577
Loss in iteration 58 : 0.032585144172176346
Loss in iteration 59 : 0.032078813595384954
Loss in iteration 60 : 0.03159371036112651
Loss in iteration 61 : 0.031128676946619128
Loss in iteration 62 : 0.03068260194071288
Loss in iteration 63 : 0.030254423284752013
Loss in iteration 64 : 0.029843130295011778
Loss in iteration 65 : 0.02944776462079558
Loss in iteration 66 : 0.02906742030846435
Loss in iteration 67 : 0.02870124314069274
Loss in iteration 68 : 0.028348429408657075
Loss in iteration 69 : 0.028008224257523133
Loss in iteration 70 : 0.02767991972591714
Loss in iteration 71 : 0.027362852580249064
Loss in iteration 72 : 0.0270564020261524
Loss in iteration 73 : 0.026759987362622667
Loss in iteration 74 : 0.02647306562996664
Loss in iteration 75 : 0.026195129290442855
Loss in iteration 76 : 0.02592570397034723
Loss in iteration 77 : 0.02566434628406029
Loss in iteration 78 : 0.025410641753981975
Loss in iteration 79 : 0.025164202835082264
Loss in iteration 80 : 0.02492466704876214
Loss in iteration 81 : 0.024691695227637235
Loss in iteration 82 : 0.02446496987054745
Loss in iteration 83 : 0.02424419360540247
Loss in iteration 84 : 0.024029087756267758
Loss in iteration 85 : 0.023819391010275935
Loss in iteration 86 : 0.023614858179421227
Loss in iteration 87 : 0.0234152590519988
Loss in iteration 88 : 0.023220377328324167
Loss in iteration 89 : 0.02303000963537125
Loss in iteration 90 : 0.0228439646150633
Loss in iteration 91 : 0.022662062081115396
Loss in iteration 92 : 0.022484132239533838
Loss in iteration 93 : 0.02231001496811881
Loss in iteration 94 : 0.02213955915057007
Loss in iteration 95 : 0.02197262206106115
Loss in iteration 96 : 0.021809068795411286
Testing accuracy  of updater 0 on alg 0 with rate 40.0 = 0.9493333333333334, training accuracy 0.9948556730494427, time elapsed: 6624 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.43785536700766375
Loss in iteration 3 : 0.5219774620319565
Loss in iteration 4 : 0.7423669393109441
Loss in iteration 5 : 0.1424947952305236
Loss in iteration 6 : 0.13468801124658739
Loss in iteration 7 : 0.12885963084035337
Loss in iteration 8 : 0.12411309531906405
Loss in iteration 9 : 0.1201319600215107
Loss in iteration 10 : 0.11672449940566518
Loss in iteration 11 : 0.11376083723439871
Loss in iteration 12 : 0.11114912031414173
Loss in iteration 13 : 0.10882219282938173
Loss in iteration 14 : 0.10672961571409693
Loss in iteration 15 : 0.1048326506264781
Loss in iteration 16 : 0.10310097997592742
Loss in iteration 17 : 0.10151049020247163
Loss in iteration 18 : 0.10004173123124639
Loss in iteration 19 : 0.09867881993109259
Loss in iteration 20 : 0.09740864326551658
Loss in iteration 21 : 0.0962202686276548
Loss in iteration 22 : 0.09510450044245536
Loss in iteration 23 : 0.09405354196262448
Loss in iteration 24 : 0.09306073397820698
Loss in iteration 25 : 0.09212035060111644
Loss in iteration 26 : 0.09122743797244978
Loss in iteration 27 : 0.09037768564312214
Loss in iteration 28 : 0.08956732310226159
Loss in iteration 29 : 0.08879303585823499
Loss in iteration 30 : 0.0880518968644802
Loss in iteration 31 : 0.08734131009210855
Loss in iteration 32 : 0.08665896379493748
Loss in iteration 33 : 0.08600279156628825
Loss in iteration 34 : 0.08537093970327139
Loss in iteration 35 : 0.08476173971033328
Loss in iteration 36 : 0.08417368501584904
Loss in iteration 37 : 0.08360541116236446
Loss in iteration 38 : 0.08305567887640082
Loss in iteration 39 : 0.08252335953758796
Loss in iteration 40 : 0.08200742265668515
Loss in iteration 41 : 0.08150692504332545
Loss in iteration 42 : 0.08102100140123826
Loss in iteration 43 : 0.0805488561344222
Loss in iteration 44 : 0.0800897561846467
Loss in iteration 45 : 0.07964302475062654
Loss in iteration 46 : 0.07920803576364349
Loss in iteration 47 : 0.07878420901441627
Loss in iteration 48 : 0.07837100584249833
Loss in iteration 49 : 0.07796792531310671
Loss in iteration 50 : 0.07757450081759111
Loss in iteration 51 : 0.07719029704317228
Loss in iteration 52 : 0.07681490726545658
Loss in iteration 53 : 0.07644795092384901
Loss in iteration 54 : 0.07608907144555122
Loss in iteration 55 : 0.0757379342885414
Loss in iteration 56 : 0.07539422517792008
Loss in iteration 57 : 0.07505764851339818
Loss in iteration 58 : 0.07472792592859746
Loss in iteration 59 : 0.07440479498530618
Loss in iteration 60 : 0.0740880079879576
Loss in iteration 61 : 0.073777330905421
Loss in iteration 62 : 0.07347254238876962
Loss in iteration 63 : 0.07317343287504918
Loss in iteration 64 : 0.07287980376824928
Loss in iteration 65 : 0.0725914666897029
Loss in iteration 66 : 0.07230824279102853
Loss in iteration 67 : 0.07202996212350765
Loss in iteration 68 : 0.07175646305846847
Loss in iteration 69 : 0.07148759175384233
Loss in iteration 70 : 0.0712232016625793
Loss in iteration 71 : 0.0709631530790707
Loss in iteration 72 : 0.07070731272012952
Loss in iteration 73 : 0.07045555333743658
Loss in iteration 74 : 0.07020775335867722
Loss in iteration 75 : 0.06996379655487149
Loss in iteration 76 : 0.06972357173164843
Loss in iteration 77 : 0.06948697244243927
Loss in iteration 78 : 0.06925389672175494
Loss in iteration 79 : 0.06902424683689383
Loss in iteration 80 : 0.06879792905657803
Loss in iteration 81 : 0.068574853435158
Loss in iteration 82 : 0.06835493361115111
Loss in iteration 83 : 0.06813808661898955
Loss in iteration 84 : 0.06792423271295646
Loss in iteration 85 : 0.0677132952023792
Loss in iteration 86 : 0.06750520029722695
Loss in iteration 87 : 0.06729987696334074
Loss in iteration 88 : 0.06709725678658184
Loss in iteration 89 : 0.06689727384525161
Loss in iteration 90 : 0.06669986459018494
Loss in iteration 91 : 0.06650496773197331
Loss in iteration 92 : 0.06631252413481364
Loss in iteration 93 : 0.06612247671652446
Loss in iteration 94 : 0.06593477035430223
Loss in iteration 95 : 0.06574935179582957
Loss in iteration 96 : 0.06556616957537438
Loss in iteration 97 : 0.06538517393454754
Loss in iteration 98 : 0.06520631674741277
Loss in iteration 99 : 0.06502955144966517
Loss in iteration 100 : 0.06485483297161573
Loss in iteration 101 : 0.06468211767473957
Loss in iteration 102 : 0.06451136329156276
Loss in iteration 103 : 0.06434252886867917
Loss in iteration 104 : 0.06417557471270383
Loss in iteration 105 : 0.06401046233898224
Loss in iteration 106 : 0.06384715442289078
Loss in iteration 107 : 0.0636856147535698
Loss in iteration 108 : 0.06352580818994714
Loss in iteration 109 : 0.06336770061891653
Loss in iteration 110 : 0.06321125891554459
Loss in iteration 111 : 0.06305645090519085
Loss in iteration 112 : 0.06290324532743011
Loss in iteration 113 : 0.06275161180167516
Loss in iteration 114 : 0.0626015207944063
Loss in iteration 115 : 0.06245294358791474
Loss in iteration 116 : 0.06230585225048032
Loss in iteration 117 : 0.06216021960790319
Loss in iteration 118 : 0.06201601921631569
Loss in iteration 119 : 0.061873225336208246
Loss in iteration 120 : 0.061731812907601706
Loss in iteration 121 : 0.06159175752630849
Loss in iteration 122 : 0.06145303542122371
Loss in iteration 123 : 0.06131562343259386
Loss in iteration 124 : 0.061179498991213395
Loss in iteration 125 : 0.06104464009850063
Loss in iteration 126 : 0.06091102530740953
Loss in iteration 127 : 0.06077863370413594
Loss in iteration 128 : 0.06064744489057756
Loss in iteration 129 : 0.06051743896751187
Loss in iteration 130 : 0.06038859651845623
Loss in iteration 131 : 0.060260898594177444
Loss in iteration 132 : 0.06013432669781935
Loss in iteration 133 : 0.06000886277061934
Loss in iteration 134 : 0.05988448917818565
Loss in iteration 135 : 0.05976118869730959
Loss in iteration 136 : 0.05963894450328711
Loss in iteration 137 : 0.05951774015772704
Loss in iteration 138 : 0.0593975595968227
Loss in iteration 139 : 0.05927838712006768
Loss in iteration 140 : 0.059160207379392545
Loss in iteration 141 : 0.05904300536870798
Loss in iteration 142 : 0.05892676641383168
Loss in iteration 143 : 0.05881147616278571
Loss in iteration 144 : 0.058697120576446285
Loss in iteration 145 : 0.05858368591953128
Loss in iteration 146 : 0.05847115875191061
Loss in iteration 147 : 0.05835952592022558
Loss in iteration 148 : 0.05824877454980454
Loss in iteration 149 : 0.05813889203686204
Loss in iteration 150 : 0.05802986604096972
Loss in iteration 151 : 0.05792168447778709
Loss in iteration 152 : 0.057814335512042576
Loss in iteration 153 : 0.05770780755075318
Loss in iteration 154 : 0.05760208923667466
Loss in iteration 155 : 0.05749716944197076
Loss in iteration 156 : 0.057393037262095284
Loss in iteration 157 : 0.0572896820098758
Loss in iteration 158 : 0.05718709320979378
Loss in iteration 159 : 0.05708526059245033
Loss in iteration 160 : 0.05698417408921347
Loss in iteration 161 : 0.05688382382703727
Loss in iteration 162 : 0.05678420012344764
Loss in iteration 163 : 0.05668529348168824
Loss in iteration 164 : 0.056587094586019585
Loss in iteration 165 : 0.05648959429716705
Loss in iteration 166 : 0.05639278364790977
Loss in iteration 167 : 0.05629665383880866
Loss in iteration 168 : 0.056201196234064575
Loss in iteration 169 : 0.056106402357504526
Loss in iteration 170 : 0.05601226388869014
Loss in iteration 171 : 0.055918772659144064
Loss in iteration 172 : 0.05582592064869041
Loss in iteration 173 : 0.05573369998190378
Loss in iteration 174 : 0.055642102924665454
Loss in iteration 175 : 0.05555112188082027
Loss in iteration 176 : 0.0554607493889324
Loss in iteration 177 : 0.05537097811913612
Loss in iteration 178 : 0.055281800870077245
Loss in iteration 179 : 0.05519321056594419
Loss in iteration 180 : 0.05510520025358328
Loss in iteration 181 : 0.05501776309969716
Loss in iteration 182 : 0.05493089238812203
Loss in iteration 183 : 0.05484458151718259
Loss in iteration 184 : 0.054758823997120586
Loss in iteration 185 : 0.05467361344759542
Loss in iteration 186 : 0.05458894359525421
Testing accuracy  of updater 0 on alg 0 with rate 4.0 = 0.976, training accuracy 0.987710774507002, time elapsed: 6164 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.574937865432634
Loss in iteration 3 : 0.5187916309722154
Loss in iteration 4 : 0.48229841309430804
Loss in iteration 5 : 0.4557462464473422
Loss in iteration 6 : 0.43514075094352633
Loss in iteration 7 : 0.41846030157005326
Loss in iteration 8 : 0.40454614497099434
Loss in iteration 9 : 0.39267574449132703
Loss in iteration 10 : 0.3823701190773697
Loss in iteration 11 : 0.37329656265959205
Loss in iteration 12 : 0.365215286166343
Loss in iteration 13 : 0.3579482142172627
Loss in iteration 14 : 0.3513597783959523
Loss in iteration 15 : 0.345344587827569
Loss in iteration 16 : 0.3398192318333732
Loss in iteration 17 : 0.3347166649815133
Loss in iteration 18 : 0.32998226110893236
Loss in iteration 19 : 0.3255709776643292
Loss in iteration 20 : 0.3214452776106061
Loss in iteration 21 : 0.3175735798241961
Loss in iteration 22 : 0.31392908553988125
Loss in iteration 23 : 0.3104888771294235
Loss in iteration 24 : 0.3072332172610861
Loss in iteration 25 : 0.3041449976300697
Loss in iteration 26 : 0.30120930080064734
Loss in iteration 27 : 0.29841304861401846
Loss in iteration 28 : 0.29574471757424964
Loss in iteration 29 : 0.2931941065810532
Loss in iteration 30 : 0.29075214595640525
Loss in iteration 31 : 0.2884107393276161
Loss in iteration 32 : 0.2861626318635234
Loss in iteration 33 : 0.28400129980591143
Loss in iteration 34 : 0.2819208573292943
Loss in iteration 35 : 0.2799159775934096
Loss in iteration 36 : 0.27798182549152795
Loss in iteration 37 : 0.27611400009257225
Loss in iteration 38 : 0.2743084851613805
Loss in iteration 39 : 0.27256160644525357
Loss in iteration 40 : 0.27086999465541595
Loss in iteration 41 : 0.26923055326366313
Loss in iteration 42 : 0.267640430388047
Loss in iteration 43 : 0.26609699416531035
Loss in iteration 44 : 0.2645978111081532
Loss in iteration 45 : 0.2631406270272119
Loss in iteration 46 : 0.2617233501646097
Loss in iteration 47 : 0.26034403624104124
Loss in iteration 48 : 0.25900087516389153
Loss in iteration 49 : 0.2576921791816753
Loss in iteration 50 : 0.2564163723015854
Loss in iteration 51 : 0.25517198081327963
Loss in iteration 52 : 0.2539576247841647
Loss in iteration 53 : 0.25277201041007596
Loss in iteration 54 : 0.2516139231210312
Loss in iteration 55 : 0.25048222135510073
Loss in iteration 56 : 0.24937583092483595
Loss in iteration 57 : 0.24829373991042164
Loss in iteration 58 : 0.24723499402202556
Loss in iteration 59 : 0.24619869238100037
Loss in iteration 60 : 0.24518398367571154
Loss in iteration 61 : 0.24419006265311624
Loss in iteration 62 : 0.24321616691178763
Loss in iteration 63 : 0.24226157396609896
Loss in iteration 64 : 0.2413255985547306
Loss in iteration 65 : 0.2404075901697018
Loss in iteration 66 : 0.2395069307847816
Loss in iteration 67 : 0.23862303276442348
Loss in iteration 68 : 0.2377553369364301
Loss in iteration 69 : 0.23690331081332203
Loss in iteration 70 : 0.2360664469489602
Loss in iteration 71 : 0.2352442614183665
Loss in iteration 72 : 0.23443629240991473
Loss in iteration 73 : 0.23364209892014462
Loss in iteration 74 : 0.2328612595424253
Loss in iteration 75 : 0.2320933713415471
Loss in iteration 76 : 0.23133804880708983
Loss in iteration 77 : 0.23059492287909245
Loss in iteration 78 : 0.22986364004015716
Loss in iteration 79 : 0.2291438614686727
Loss in iteration 80 : 0.2284352622483169
Loss in iteration 81 : 0.22773753062944974
Loss in iteration 82 : 0.22705036733838904
Loss in iteration 83 : 0.2263734849309208
Loss in iteration 84 : 0.2257066071867195
Loss in iteration 85 : 0.22504946854162297
Loss in iteration 86 : 0.22440181355499056
Loss in iteration 87 : 0.22376339640958626
Loss in iteration 88 : 0.2231339804416535
Loss in iteration 89 : 0.22251333769903792
Loss in iteration 90 : 0.22190124852538365
Loss in iteration 91 : 0.22129750116859634
Loss in iteration 92 : 0.22070189141190363
Loss in iteration 93 : 0.22011422222597837
Loss in iteration 94 : 0.21953430344070485
Loss in iteration 95 : 0.2189619514352898
Loss in iteration 96 : 0.21839698884549918
Loss in iteration 97 : 0.2178392442869168
Loss in iteration 98 : 0.21728855209318146
Loss in iteration 99 : 0.2167447520682572
Loss in iteration 100 : 0.21620768925183848
Loss in iteration 101 : 0.21567721369708368
Loss in iteration 102 : 0.21515318025989413
Loss in iteration 103 : 0.21463544839905377
Loss in iteration 104 : 0.2141238819865489
Loss in iteration 105 : 0.21361834912746963
Loss in iteration 106 : 0.21311872198891585
Loss in iteration 107 : 0.21262487663738072
Loss in iteration 108 : 0.21213669288411002
Loss in iteration 109 : 0.21165405413798327
Loss in iteration 110 : 0.21117684726547878
Loss in iteration 111 : 0.21070496245732215
Loss in iteration 112 : 0.21023829310144307
Loss in iteration 113 : 0.20977673566188704
Loss in iteration 114 : 0.2093201895633479
Loss in iteration 115 : 0.2088685570810246
Loss in iteration 116 : 0.20842174323549845
Loss in iteration 117 : 0.20797965569237112
Loss in iteration 118 : 0.20754220466640277
Loss in iteration 119 : 0.2071093028299154
Loss in iteration 120 : 0.20668086522523324
Loss in iteration 121 : 0.20625680918095585
Loss in iteration 122 : 0.20583705423185963
Loss in iteration 123 : 0.20542152204224456
Loss in iteration 124 : 0.2050101363325535
Loss in iteration 125 : 0.20460282280909317
Loss in iteration 126 : 0.20419950909670767
Loss in iteration 127 : 0.2038001246742529
Loss in iteration 128 : 0.20340460081273845
Loss in iteration 129 : 0.20301287051600486
Loss in iteration 130 : 0.2026248684638106
Loss in iteration 131 : 0.20224053095721742
Loss in iteration 132 : 0.20185979586616304
Loss in iteration 133 : 0.20148260257911538
Loss in iteration 134 : 0.20110889195471263
Loss in iteration 135 : 0.20073860627529633
Loss in iteration 136 : 0.20037168920224885
Loss in iteration 137 : 0.2000080857330533
Loss in iteration 138 : 0.19964774215999778
Loss in iteration 139 : 0.19929060603044638
Loss in iteration 140 : 0.19893662610861093
Loss in iteration 141 : 0.1985857523387545
Loss in iteration 142 : 0.19823793580975999
Loss in iteration 143 : 0.19789312872100975
Loss in iteration 144 : 0.19755128434951594
Loss in iteration 145 : 0.19721235701824494
Loss in iteration 146 : 0.1968763020655888
Loss in iteration 147 : 0.19654307581593258
Loss in iteration 148 : 0.19621263555127
Loss in iteration 149 : 0.19588493948382493
Loss in iteration 150 : 0.19555994672963356
Loss in iteration 151 : 0.1952376172830556
Loss in iteration 152 : 0.1949179119921618
Loss in iteration 153 : 0.19460079253497511
Loss in iteration 154 : 0.19428622139652535
Loss in iteration 155 : 0.19397416184668065
Loss in iteration 156 : 0.193664577918734
Loss in iteration 157 : 0.1933574343887021
Loss in iteration 158 : 0.19305269675532152
Loss in iteration 159 : 0.1927503312207033
Loss in iteration 160 : 0.19245030467162594
Loss in iteration 161 : 0.19215258466144225
Loss in iteration 162 : 0.19185713939257393
Loss in iteration 163 : 0.1915639376995703
Loss in iteration 164 : 0.19127294903271438
Loss in iteration 165 : 0.19098414344215017
Loss in iteration 166 : 0.19069749156251334
Loss in iteration 167 : 0.1904129645980498
Loss in iteration 168 : 0.1901305343081975
Loss in iteration 169 : 0.18985017299361745
Loss in iteration 170 : 0.18957185348266095
Loss in iteration 171 : 0.18929554911824972
Loss in iteration 172 : 0.18902123374516042
Loss in iteration 173 : 0.1887488816976936
Loss in iteration 174 : 0.18847846778771846
Loss in iteration 175 : 0.1882099672930741
Loss in iteration 176 : 0.1879433559463213
Loss in iteration 177 : 0.1876786099238251
Loss in iteration 178 : 0.18741570583516337
Loss in iteration 179 : 0.18715462071284403
Loss in iteration 180 : 0.1868953320023248
Loss in iteration 181 : 0.1866378175523226
Loss in iteration 182 : 0.1863820556054015
Loss in iteration 183 : 0.1861280247888342
Loss in iteration 184 : 0.18587570410572213
Loss in iteration 185 : 0.18562507292636957
Loss in iteration 186 : 0.18537611097990067
Loss in iteration 187 : 0.18512879834611257
Loss in iteration 188 : 0.18488311544755767
Loss in iteration 189 : 0.18463904304184395
Loss in iteration 190 : 0.1843965622141497
Loss in iteration 191 : 0.18415565436994408
Loss in iteration 192 : 0.183916301227907
Loss in iteration 193 : 0.18367848481304247
Loss in iteration 194 : 0.18344218744997673
Loss in iteration 195 : 0.18320739175643883
Loss in iteration 196 : 0.18297408063691598
Loss in iteration 197 : 0.1827422372764765
Loss in iteration 198 : 0.1825118451347583
Loss in iteration 199 : 0.18228288794011552
Loss in iteration 200 : 0.18205534968392004
Testing accuracy  of updater 0 on alg 0 with rate 0.3999999999999999 = 0.9875555555555555, training accuracy 0.9604172620748785, time elapsed: 5883 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.8982803999400666
Loss in iteration 3 : 0.26327358395595857
Loss in iteration 4 : 0.3978049418744733
Loss in iteration 5 : 0.6258334600480191
Loss in iteration 6 : 0.4529595050575439
Loss in iteration 7 : 0.39329624543988323
Loss in iteration 8 : 0.3712284743731969
Loss in iteration 9 : 0.3851128710890327
Loss in iteration 10 : 0.42835493361858334
Loss in iteration 11 : 0.4562814866704872
Loss in iteration 12 : 0.4470021029480078
Loss in iteration 13 : 0.4093135859217738
Loss in iteration 14 : 0.36347105815758307
Loss in iteration 15 : 0.3244517137771751
Loss in iteration 16 : 0.29509667871970896
Loss in iteration 17 : 0.2711244402768022
Loss in iteration 18 : 0.24974969076979872
Loss in iteration 19 : 0.23059833638780894
Loss in iteration 20 : 0.21445378758923433
Loss in iteration 21 : 0.200622166590612
Loss in iteration 22 : 0.18760235456902036
Loss in iteration 23 : 0.1748844956857392
Loss in iteration 24 : 0.16219964796799088
Loss in iteration 25 : 0.14951259037608405
Loss in iteration 26 : 0.13739870073623328
Loss in iteration 27 : 0.12671132271811217
Loss in iteration 28 : 0.11791458494980464
Loss in iteration 29 : 0.11107433577419665
Loss in iteration 30 : 0.10590276672442013
Loss in iteration 31 : 0.1019165123212025
Loss in iteration 32 : 0.09870013753843956
Loss in iteration 33 : 0.09596962637634059
Loss in iteration 34 : 0.09354766792022366
Loss in iteration 35 : 0.09131558007028208
Loss in iteration 36 : 0.08917634073260157
Loss in iteration 37 : 0.08705569065688683
Loss in iteration 38 : 0.0849123010671087
Loss in iteration 39 : 0.08273784828404318
Loss in iteration 40 : 0.08054920109484508
Loss in iteration 41 : 0.07837732771796052
Loss in iteration 42 : 0.07625688122775919
Loss in iteration 43 : 0.0742188465737313
Loss in iteration 44 : 0.07228632309591294
Loss in iteration 45 : 0.07047239974731177
Loss in iteration 46 : 0.06877926542168802
Loss in iteration 47 : 0.06719840945287155
Loss in iteration 48 : 0.0657122467141359
Loss in iteration 49 : 0.06429736337331902
Loss in iteration 50 : 0.0629289827983839
Loss in iteration 51 : 0.061585655740150014
Loss in iteration 52 : 0.060252963994323436
Loss in iteration 53 : 0.05892532262169441
Loss in iteration 54 : 0.057605624796062155
Loss in iteration 55 : 0.05630316671600103
Loss in iteration 56 : 0.05503070923376598
Loss in iteration 57 : 0.05380155233829181
Loss in iteration 58 : 0.05262721419728567
Loss in iteration 59 : 0.05151594178497421
Loss in iteration 60 : 0.0504720257797295
Loss in iteration 61 : 0.0494957969001829
Loss in iteration 62 : 0.04858416909754515
Loss in iteration 63 : 0.04773157606700356
Loss in iteration 64 : 0.046931101520765295
Loss in iteration 65 : 0.0461755762372451
Loss in iteration 66 : 0.04545844892346562
Loss in iteration 67 : 0.0447743264812019
Loss in iteration 68 : 0.04411917892949681
Loss in iteration 69 : 0.04349027552804508
Loss in iteration 70 : 0.04288594842271134
Loss in iteration 71 : 0.042305276989369686
Loss in iteration 72 : 0.041747765716456144
Loss in iteration 73 : 0.04121306371841188
Loss in iteration 74 : 0.04070075181768364
Loss in iteration 75 : 0.04021020603329025
Loss in iteration 76 : 0.03974053429272454
Loss in iteration 77 : 0.03929057540453934
Loss in iteration 78 : 0.03885894491197582
Loss in iteration 79 : 0.03844411072595285
Loss in iteration 80 : 0.03804448190682195
Loss in iteration 81 : 0.03765849615794476
Loss in iteration 82 : 0.03728469497971398
Loss in iteration 83 : 0.03692177941584952
Loss in iteration 84 : 0.03656864331169002
Loss in iteration 85 : 0.03622438449239825
Loss in iteration 86 : 0.03588829691087361
Loss in iteration 87 : 0.03555984845190906
Loss in iteration 88 : 0.035238649729128387
Loss in iteration 89 : 0.034924419029101005
Loss in iteration 90 : 0.034616947777494515
Loss in iteration 91 : 0.03431606978362831
Loss in iteration 92 : 0.03402163629855659
Loss in iteration 93 : 0.03373349778228792
Loss in iteration 94 : 0.03345149233815679
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.9448888888888889, training accuracy 0.9959988568162332, time elapsed: 3084 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.45859828352491205
Loss in iteration 3 : 0.2922871568937091
Loss in iteration 4 : 0.21266962266158246
Loss in iteration 5 : 0.17527190071610932
Loss in iteration 6 : 0.15557030925121798
Loss in iteration 7 : 0.14425122440683705
Loss in iteration 8 : 0.13662225010514978
Loss in iteration 9 : 0.13058674996356667
Loss in iteration 10 : 0.12546535204478962
Loss in iteration 11 : 0.12108147421025924
Loss in iteration 12 : 0.11733779051394373
Loss in iteration 13 : 0.11410883837072018
Loss in iteration 14 : 0.11124898735724188
Loss in iteration 15 : 0.10861955244921093
Loss in iteration 16 : 0.10610731639991013
Loss in iteration 17 : 0.10363198963996921
Loss in iteration 18 : 0.10114584934513789
Loss in iteration 19 : 0.09862888181015457
Loss in iteration 20 : 0.09608191376427508
Loss in iteration 21 : 0.09351945325521259
Loss in iteration 22 : 0.09096332908466805
Loss in iteration 23 : 0.0884376887457009
Loss in iteration 24 : 0.08596549967771898
Loss in iteration 25 : 0.08356641907800354
Loss in iteration 26 : 0.08125575375666547
Loss in iteration 27 : 0.07904419766237614
Loss in iteration 28 : 0.07693807054257815
Loss in iteration 29 : 0.07493984711275833
Loss in iteration 30 : 0.07304883284663508
Loss in iteration 31 : 0.0712618942696274
Loss in iteration 32 : 0.06957418430886196
Loss in iteration 33 : 0.0679798203070336
Loss in iteration 34 : 0.06647248054040346
Loss in iteration 35 : 0.06504589114569986
Loss in iteration 36 : 0.06369418362554632
Loss in iteration 37 : 0.062412114900355094
Loss in iteration 38 : 0.0611951559247581
Loss in iteration 39 : 0.06003946844634256
Loss in iteration 40 : 0.05894179969471556
Loss in iteration 41 : 0.05789932973127328
Loss in iteration 42 : 0.05690950538752103
Loss in iteration 43 : 0.055969889063619764
Loss in iteration 44 : 0.05507804193629953
Loss in iteration 45 : 0.054231451403631406
Loss in iteration 46 : 0.0534275036849611
Loss in iteration 47 : 0.052663495626928424
Loss in iteration 48 : 0.051936675494402305
Loss in iteration 49 : 0.05124430081303652
Loss in iteration 50 : 0.05058370173827255
Loss in iteration 51 : 0.049952340313490336
Loss in iteration 52 : 0.04934785867713521
Loss in iteration 53 : 0.04876811219959576
Loss in iteration 54 : 0.04821118623401106
Loss in iteration 55 : 0.04767539736864435
Loss in iteration 56 : 0.047159281634872145
Loss in iteration 57 : 0.046661573033762095
Loss in iteration 58 : 0.046181176058546075
Loss in iteration 59 : 0.04571713572410238
Loss in iteration 60 : 0.045268608105625434
Loss in iteration 61 : 0.044834833676778145
Loss in iteration 62 : 0.04441511494913556
Loss in iteration 63 : 0.04400879915263166
Loss in iteration 64 : 0.04361526603582729
Loss in iteration 65 : 0.04323392035094349
Loss in iteration 66 : 0.04286418824091378
Loss in iteration 67 : 0.0425055165611442
Loss in iteration 68 : 0.042157374127655474
Loss in iteration 69 : 0.04181925395570825
Loss in iteration 70 : 0.04149067570398902
Loss in iteration 71 : 0.04117118773413138
Loss in iteration 72 : 0.04086036840272505
Loss in iteration 73 : 0.040557826398189445
Loss in iteration 74 : 0.040263200100494376
Loss in iteration 75 : 0.039976156067688805
Loss in iteration 76 : 0.0396963868362105
Loss in iteration 77 : 0.03942360826404778
Loss in iteration 78 : 0.03915755665285243
Loss in iteration 79 : 0.03889798586510927
Loss in iteration 80 : 0.038644664614292695
Loss in iteration 81 : 0.03839737405805533
Loss in iteration 82 : 0.038155905774217305
Loss in iteration 83 : 0.03792006015234243
Loss in iteration 84 : 0.03768964519391396
Loss in iteration 85 : 0.03746447568372351
Loss in iteration 86 : 0.03724437267477158
Loss in iteration 87 : 0.03702916321826262
Loss in iteration 88 : 0.03681868026787789
Loss in iteration 89 : 0.036612762691670395
Loss in iteration 90 : 0.03641125533370743
Loss in iteration 91 : 0.03621400907910915
Loss in iteration 92 : 0.03602088088873037
Loss in iteration 93 : 0.03583173378205887
Loss in iteration 94 : 0.03564643675796941
Loss in iteration 95 : 0.03546486465214398
Loss in iteration 96 : 0.03528689793693148
Loss in iteration 97 : 0.03511242247413266
Loss in iteration 98 : 0.03494132923381471
Loss in iteration 99 : 0.03477351399307793
Loss in iteration 100 : 0.0346088770280939
Loss in iteration 101 : 0.03444732281110686
Loss in iteration 102 : 0.03428875972182168
Loss in iteration 103 : 0.03413309978004609
Loss in iteration 104 : 0.03398025840388294
Loss in iteration 105 : 0.03383015419540688
Loss in iteration 106 : 0.033682708753745766
Loss in iteration 107 : 0.033537846513907406
Loss in iteration 108 : 0.033395494608570817
Loss in iteration 109 : 0.03325558274938466
Loss in iteration 110 : 0.033118043124031916
Loss in iteration 111 : 0.032982810305365394
Loss in iteration 112 : 0.032849821169211826
Loss in iteration 113 : 0.032719014817908815
Loss in iteration 114 : 0.03259033250719994
Loss in iteration 115 : 0.03246371757470929
Loss in iteration 116 : 0.032339115368795426
Loss in iteration 117 : 0.03221647317710384
Loss in iteration 118 : 0.032095740154580055
Loss in iteration 119 : 0.03197686725104901
Loss in iteration 120 : 0.03185980713871592
Loss in iteration 121 : 0.03174451414009923
Loss in iteration 122 : 0.03163094415697968
Loss in iteration 123 : 0.03151905460095379
Loss in iteration 124 : 0.03140880432613424
Loss in iteration 125 : 0.03130015356445286
Loss in iteration 126 : 0.031193063863918336
Loss in iteration 127 : 0.03108749803006213
Loss in iteration 128 : 0.030983420070695055
Loss in iteration 129 : 0.03088079514398972
Loss in iteration 130 : 0.03077958950981523
Loss in iteration 131 : 0.030679770484179605
Loss in iteration 132 : 0.030581306396580663
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.9502222222222222, training accuracy 0.9918548156616176, time elapsed: 4373 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6603165680413241
Loss in iteration 3 : 0.613202650650183
Loss in iteration 4 : 0.5621722859398044
Loss in iteration 5 : 0.5127542390785343
Loss in iteration 6 : 0.46745181970861777
Loss in iteration 7 : 0.4271328371191829
Loss in iteration 8 : 0.39189375098378865
Loss in iteration 9 : 0.36148163278587275
Loss in iteration 10 : 0.33547342609456676
Loss in iteration 11 : 0.31336263515185303
Loss in iteration 12 : 0.29461743612952807
Loss in iteration 13 : 0.2787240214663541
Loss in iteration 14 : 0.26521478326489684
Loss in iteration 15 : 0.25368196804865467
Loss in iteration 16 : 0.24378029835108853
Loss in iteration 17 : 0.23522289908467658
Loss in iteration 18 : 0.2277741117507698
Loss in iteration 19 : 0.22124153226677726
Loss in iteration 20 : 0.21546851992284383
Loss in iteration 21 : 0.21032769052484412
Loss in iteration 22 : 0.2057154923479171
Loss in iteration 23 : 0.20154776976678848
Loss in iteration 24 : 0.19775615438562796
Loss in iteration 25 : 0.19428512269668846
Loss in iteration 26 : 0.1910895841240766
Loss in iteration 27 : 0.18813289350302095
Loss in iteration 28 : 0.18538520864456157
Loss in iteration 29 : 0.18282213386410887
Loss in iteration 30 : 0.18042360440218386
Loss in iteration 31 : 0.17817297586516637
Loss in iteration 32 : 0.1760562886845038
Loss in iteration 33 : 0.17406168143304557
Loss in iteration 34 : 0.1721789295934166
Loss in iteration 35 : 0.170399088651932
Loss in iteration 36 : 0.16871422252717602
Loss in iteration 37 : 0.1671172004791934
Loss in iteration 38 : 0.16560154780895106
Loss in iteration 39 : 0.16416133781144115
Loss in iteration 40 : 0.1627911145282763
Loss in iteration 41 : 0.16148583779552544
Loss in iteration 42 : 0.16024084385054463
Loss in iteration 43 : 0.1590518163159176
Loss in iteration 44 : 0.15791476370484075
Loss in iteration 45 : 0.15682600069107352
Loss in iteration 46 : 0.15578213127005508
Loss in iteration 47 : 0.15478003262599496
Loss in iteration 48 : 0.15381683903714
Loss in iteration 49 : 0.15288992552435407
Loss in iteration 50 : 0.15199689120265036
Loss in iteration 51 : 0.15113554245584884
Loss in iteration 52 : 0.15030387614301177
Loss in iteration 53 : 0.14950006308071437
Loss in iteration 54 : 0.14872243204342755
Loss in iteration 55 : 0.14796945449823726
Loss in iteration 56 : 0.14723973024991932
Loss in iteration 57 : 0.14653197412572905
Loss in iteration 58 : 0.14584500378173368
Loss in iteration 59 : 0.14517772866792203
Loss in iteration 60 : 0.14452914015012294
Loss in iteration 61 : 0.14389830275422452
Loss in iteration 62 : 0.14328434647281432
Loss in iteration 63 : 0.14268646005600524
Loss in iteration 64 : 0.14210388519635367
Loss in iteration 65 : 0.14153591151161032
Loss in iteration 66 : 0.1409818722276468
Loss in iteration 67 : 0.14044114046631903
Loss in iteration 68 : 0.13991312604832187
Loss in iteration 69 : 0.13939727272838712
Loss in iteration 70 : 0.13889305578875696
Loss in iteration 71 : 0.13839997992603856
Loss in iteration 72 : 0.13791757737584032
Loss in iteration 73 : 0.13744540622857523
Loss in iteration 74 : 0.1369830488982177
Loss in iteration 75 : 0.13653011071339907
Loss in iteration 76 : 0.13608621860694425
Loss in iteration 77 : 0.1356510198856958
Loss in iteration 78 : 0.13522418106728762
Loss in iteration 79 : 0.1348053867744369
Loss in iteration 80 : 0.13439433868040376
Loss in iteration 81 : 0.13399075450161135
Loss in iteration 82 : 0.13359436703512656
Loss in iteration 83 : 0.13320492323986363
Loss in iteration 84 : 0.13282218336110838
Loss in iteration 85 : 0.13244592009834222
Loss in iteration 86 : 0.13207591781646671
Loss in iteration 87 : 0.1317119718004631
Loss in iteration 88 : 0.13135388755332633
Loss in iteration 89 : 0.1310014801368435
Loss in iteration 90 : 0.1306545735544804
Loss in iteration 91 : 0.1303130001753342
Loss in iteration 92 : 0.12997660019781399
Loss in iteration 93 : 0.12964522115146757
Loss in iteration 94 : 0.12931871743514922
Loss in iteration 95 : 0.12899694988957613
Loss in iteration 96 : 0.12867978540219902
Loss in iteration 97 : 0.12836709654225967
Loss in iteration 98 : 0.12805876122388057
Loss in iteration 99 : 0.12775466239506456
Loss in iteration 100 : 0.12745468775052152
Loss in iteration 101 : 0.12715872946633222
Loss in iteration 102 : 0.12686668395455494
Loss in iteration 103 : 0.1265784516359934
Loss in iteration 104 : 0.12629393672947567
Loss in iteration 105 : 0.12601304705611935
Loss in iteration 106 : 0.12573569385719216
Loss in iteration 107 : 0.12546179162430168
Loss in iteration 108 : 0.12519125794078034
Loss in iteration 109 : 0.12492401333323976
Loss in iteration 110 : 0.12465998113238604
Loss in iteration 111 : 0.12439908734228251
Loss in iteration 112 : 0.12414126051734498
Loss in iteration 113 : 0.1238864316464312
Loss in iteration 114 : 0.12363453404346142
Loss in iteration 115 : 0.12338550324407538
Loss in iteration 116 : 0.1231392769078816
Loss in iteration 117 : 0.12289579472590947
Loss in iteration 118 : 0.12265499833291418
Loss in iteration 119 : 0.12241683122422317
Loss in iteration 120 : 0.12218123867684137
Loss in iteration 121 : 0.12194816767455913
Loss in iteration 122 : 0.12171756683683577
Loss in iteration 123 : 0.12148938635123825
Loss in iteration 124 : 0.12126357790924384
Loss in iteration 125 : 0.12104009464522297
Loss in iteration 126 : 0.12081889107842701
Loss in iteration 127 : 0.12059992305782852
Loss in iteration 128 : 0.12038314770965612
Loss in iteration 129 : 0.1201685233874853
Loss in iteration 130 : 0.1199560096247485
Loss in iteration 131 : 0.11974556708953654
Loss in iteration 132 : 0.11953715754156974
Loss in iteration 133 : 0.11933074379122247
Loss in iteration 134 : 0.1191262896604917
Loss in iteration 135 : 0.11892375994580358
Loss in iteration 136 : 0.11872312038256037
Loss in iteration 137 : 0.11852433761133264
Loss in iteration 138 : 0.11832737914560645
Loss in iteration 139 : 0.11813221334100253
Loss in iteration 140 : 0.11793880936588597
Loss in iteration 141 : 0.11774713717329144
Loss in iteration 142 : 0.11755716747409255
Loss in iteration 143 : 0.11736887171134872
Loss in iteration 144 : 0.11718222203576355
Loss in iteration 145 : 0.11699719128219858
Loss in iteration 146 : 0.116813752947184
Loss in iteration 147 : 0.11663188116737455
Loss in iteration 148 : 0.11645155069890087
Loss in iteration 149 : 0.11627273689756984
Loss in iteration 150 : 0.11609541569987072
Loss in iteration 151 : 0.11591956360474394
Loss in iteration 152 : 0.11574515765608019
Loss in iteration 153 : 0.11557217542590413
Loss in iteration 154 : 0.11540059499821903
Loss in iteration 155 : 0.11523039495347218
Loss in iteration 156 : 0.11506155435361708
Loss in iteration 157 : 0.11489405272774049
Loss in iteration 158 : 0.11472787005823024
Loss in iteration 159 : 0.11456298676745663
Loss in iteration 160 : 0.1143993837049473
Loss in iteration 161 : 0.11423704213502772
Loss in iteration 162 : 0.11407594372491406
Loss in iteration 163 : 0.11391607053323141
Loss in iteration 164 : 0.11375740499894195
Loss in iteration 165 : 0.11359992993066687
Loss in iteration 166 : 0.1134436284963816
Loss in iteration 167 : 0.11328848421347076
Loss in iteration 168 : 0.11313448093912717
Loss in iteration 169 : 0.11298160286108155
Loss in iteration 170 : 0.11282983448864621
Loss in iteration 171 : 0.11267916064406452
Loss in iteration 172 : 0.11252956645414833
Loss in iteration 173 : 0.11238103734219726
Loss in iteration 174 : 0.11223355902018335
Loss in iteration 175 : 0.11208711748119539
Loss in iteration 176 : 0.11194169899212747
Loss in iteration 177 : 0.11179729008660716
Loss in iteration 178 : 0.11165387755815166
Loss in iteration 179 : 0.11151144845354065
Loss in iteration 180 : 0.11136999006640393
Loss in iteration 181 : 0.11122948993100823
Loss in iteration 182 : 0.11108993581624114
Loss in iteration 183 : 0.11095131571978215
Loss in iteration 184 : 0.11081361786245443
Loss in iteration 185 : 0.11067683068274989
Loss in iteration 186 : 0.11054094283152373
Loss in iteration 187 : 0.11040594316684683
Loss in iteration 188 : 0.1102718207490174
Loss in iteration 189 : 0.1101385648357199
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.976, training accuracy 0.9732780794512718, time elapsed: 4936 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 1.66603220189524
Loss in iteration 3 : 0.2918042516089046
Loss in iteration 4 : 0.3563367520100534
Loss in iteration 5 : 0.3732161966746686
Loss in iteration 6 : 0.36901694381739975
Loss in iteration 7 : 0.3582115562795647
Loss in iteration 8 : 0.34389899516132966
Loss in iteration 9 : 0.3282091245425314
Loss in iteration 10 : 0.31313895423499727
Loss in iteration 11 : 0.2981172509864634
Loss in iteration 12 : 0.2817923159529146
Loss in iteration 13 : 0.2634316350067403
Loss in iteration 14 : 0.24316106675876334
Loss in iteration 15 : 0.22231907859156683
Loss in iteration 16 : 0.20291260479804035
Loss in iteration 17 : 0.18609175456484658
Loss in iteration 18 : 0.1720218870216901
Loss in iteration 19 : 0.16122030892096362
Loss in iteration 20 : 0.1539231786179749
Loss in iteration 21 : 0.1489838999888949
Loss in iteration 22 : 0.1450040687052124
Loss in iteration 23 : 0.14119165286493857
Loss in iteration 24 : 0.13725029880970205
Loss in iteration 25 : 0.133151016904005
Loss in iteration 26 : 0.12897946752739825
Loss in iteration 27 : 0.12482752631300999
Loss in iteration 28 : 0.12075961062598156
Loss in iteration 29 : 0.11681690031902414
Loss in iteration 30 : 0.11302200149365753
Loss in iteration 31 : 0.10938046395476696
Loss in iteration 32 : 0.10588354950109374
Loss in iteration 33 : 0.10251395363679283
Loss in iteration 34 : 0.09925264812623108
Loss in iteration 35 : 0.09608480783844203
Loss in iteration 36 : 0.09300341108208322
Loss in iteration 37 : 0.09000971909077968
Loss in iteration 38 : 0.08711124301422071
Loss in iteration 39 : 0.08431864150776315
Loss in iteration 40 : 0.08164264147511688
Loss in iteration 41 : 0.07909147907716468
Loss in iteration 42 : 0.07666922033220175
Loss in iteration 43 : 0.0743753560414831
Loss in iteration 44 : 0.07220567384564464
Loss in iteration 45 : 0.0701537582687769
Loss in iteration 46 : 0.0682123160372427
Loss in iteration 47 : 0.06637397805409224
Loss in iteration 48 : 0.06463169818087201
Loss in iteration 49 : 0.06297898850921699
Loss in iteration 50 : 0.06141010911174243
Loss in iteration 51 : 0.059920192350732794
Loss in iteration 52 : 0.05850524073138264
Loss in iteration 53 : 0.057161981493037084
Loss in iteration 54 : 0.05588762246864309
Loss in iteration 55 : 0.05467958213794423
Loss in iteration 56 : 0.053535257854719784
Loss in iteration 57 : 0.052451869602461114
Loss in iteration 58 : 0.051426389651776
Loss in iteration 59 : 0.050455548691610175
Loss in iteration 60 : 0.04953589775716623
Loss in iteration 61 : 0.04866390155617988
Loss in iteration 62 : 0.04783604067147275
Loss in iteration 63 : 0.04704890532725285
Loss in iteration 64 : 0.046299269840233866
Loss in iteration 65 : 0.04558414295825103
Loss in iteration 66 : 0.0449007941072172
Loss in iteration 67 : 0.04424675879142569
Loss in iteration 68 : 0.04361982809172836
Loss in iteration 69 : 0.043018027667996755
Loss in iteration 70 : 0.042439591262045946
Loss in iteration 71 : 0.041882932770474966
Loss in iteration 72 : 0.041346619812262286
Loss in iteration 73 : 0.04082935057648696
Loss in iteration 74 : 0.04032993474751823
Loss in iteration 75 : 0.03984727854638592
Loss in iteration 76 : 0.03938037342063825
Loss in iteration 77 : 0.038928287643536556
Loss in iteration 78 : 0.03849016000450024
Loss in iteration 79 : 0.03806519483216299
Loss in iteration 80 : 0.03765265773425246
Loss in iteration 81 : 0.03725187161662089
Loss in iteration 82 : 0.036862212720202785
Loss in iteration 83 : 0.03648310656518059
Loss in iteration 84 : 0.03611402380375349
Loss in iteration 85 : 0.03575447605378336
Loss in iteration 86 : 0.03540401181913721
Loss in iteration 87 : 0.035062212606550604
Loss in iteration 88 : 0.0347286893325778
Loss in iteration 89 : 0.03440307908661228
Loss in iteration 90 : 0.03408504228458062
Loss in iteration 91 : 0.03377426021826294
Loss in iteration 92 : 0.03347043298078218
Loss in iteration 93 : 0.03317327773130613
Loss in iteration 94 : 0.03288252725168056
Loss in iteration 95 : 0.032597928743826485
Loss in iteration 96 : 0.03231924281799429
Loss in iteration 97 : 0.03204624262685727
Loss in iteration 98 : 0.03177871310746282
Loss in iteration 99 : 0.03151645030096451
Loss in iteration 100 : 0.03125926072783749
Loss in iteration 101 : 0.031006960803249165
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.9422222222222222, training accuracy 0.996141754787082, time elapsed: 2370 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.386624994192803
Loss in iteration 3 : 0.23760897535544287
Loss in iteration 4 : 0.18371482452368865
Loss in iteration 5 : 0.15877092580507715
Loss in iteration 6 : 0.14221084464572967
Loss in iteration 7 : 0.13044132976742448
Loss in iteration 8 : 0.12183891805454997
Loss in iteration 9 : 0.1153379752607689
Loss in iteration 10 : 0.11018718034147655
Loss in iteration 11 : 0.10588195451469715
Loss in iteration 12 : 0.10210557386524223
Loss in iteration 13 : 0.09867161659161579
Loss in iteration 14 : 0.09547676721106303
Loss in iteration 15 : 0.09246688130531437
Loss in iteration 16 : 0.08961471421531636
Loss in iteration 17 : 0.08690635211243182
Loss in iteration 18 : 0.08433356535169752
Loss in iteration 19 : 0.08188994003602579
Loss in iteration 20 : 0.07956927095482691
Loss in iteration 21 : 0.07736518964097441
Loss in iteration 22 : 0.07527135864331382
Loss in iteration 23 : 0.07328182142114592
Loss in iteration 24 : 0.07139128430173627
Loss in iteration 25 : 0.06959523854260317
Loss in iteration 26 : 0.06788991600784523
Loss in iteration 27 : 0.06627211941810028
Loss in iteration 28 : 0.06473898653282581
Loss in iteration 29 : 0.06328774629639422
Loss in iteration 30 : 0.0619155123458673
Loss in iteration 31 : 0.06061914196268839
Loss in iteration 32 : 0.05939517116062079
Loss in iteration 33 : 0.05823982197325198
Loss in iteration 34 : 0.05714906759922134
Loss in iteration 35 : 0.0561187353318676
Loss in iteration 36 : 0.055144625846197594
Loss in iteration 37 : 0.05422262961043449
Loss in iteration 38 : 0.0533488257762684
Loss in iteration 39 : 0.05251955461950296
Loss in iteration 40 : 0.05173146029563877
Loss in iteration 41 : 0.050981505460791825
Loss in iteration 42 : 0.05026696266780112
Loss in iteration 43 : 0.04958538922787764
Loss in iteration 44 : 0.0489345925773569
Loss in iteration 45 : 0.04831259244818116
Loss in iteration 46 : 0.0477175847301807
Loss in iteration 47 : 0.04714791023732033
Loss in iteration 48 : 0.046602029973371796
Loss in iteration 49 : 0.046078507150833545
Loss in iteration 50 : 0.045575995257100176
Loss in iteration 51 : 0.045093230900017114
Loss in iteration 52 : 0.04462902995382968
Loss in iteration 53 : 0.044182285583826655
Loss in iteration 54 : 0.04375196696018647
Loss in iteration 55 : 0.04333711779127745
Loss in iteration 56 : 0.04293685414303614
Loss in iteration 57 : 0.04255036131391844
Loss in iteration 58 : 0.04217688977482307
Loss in iteration 59 : 0.04181575034860882
Loss in iteration 60 : 0.04146630889604131
Loss in iteration 61 : 0.041127980804706164
Loss in iteration 62 : 0.04080022555970401
Loss in iteration 63 : 0.04048254162610846
Loss in iteration 64 : 0.04017446180835694
Loss in iteration 65 : 0.03987554918341215
Loss in iteration 66 : 0.03958539364180836
Loss in iteration 67 : 0.03930360901932314
Loss in iteration 68 : 0.039029830764693275
Loss in iteration 69 : 0.03876371406582893
Loss in iteration 70 : 0.03850493234694327
Loss in iteration 71 : 0.03825317604945452
Loss in iteration 72 : 0.038008151617524924
Loss in iteration 73 : 0.03776958062178793
Loss in iteration 74 : 0.03753719896961726
Loss in iteration 75 : 0.037310756165158915
Loss in iteration 76 : 0.03709001459581498
Loss in iteration 77 : 0.036874748833038144
Loss in iteration 78 : 0.03666474494376179
Loss in iteration 79 : 0.03645979981454974
Loss in iteration 80 : 0.03625972049385842
Loss in iteration 81 : 0.03606432355910623
Loss in iteration 82 : 0.035873434515034816
Loss in iteration 83 : 0.03568688722864176
Loss in iteration 84 : 0.035504523404207525
Loss in iteration 85 : 0.03532619210001145
Loss in iteration 86 : 0.03515174928650223
Loss in iteration 87 : 0.03498105744414484
Loss in iteration 88 : 0.03481398519800437
Loss in iteration 89 : 0.03465040698537611
Loss in iteration 90 : 0.03449020275241104
Loss in iteration 91 : 0.034333257675659735
Loss in iteration 92 : 0.03417946190468916
Loss in iteration 93 : 0.0340287103223414
Loss in iteration 94 : 0.033880902319720664
Loss in iteration 95 : 0.03373594158355004
Loss in iteration 96 : 0.033593735894083585
Loss in iteration 97 : 0.03345419693224769
Loss in iteration 98 : 0.03331724009510564
Loss in iteration 99 : 0.03318278431906552
Loss in iteration 100 : 0.033050751910499276
Loss in iteration 101 : 0.03292106838360018
Loss in iteration 102 : 0.032793662305402385
Loss in iteration 103 : 0.03266846514792092
Loss in iteration 104 : 0.03254541114737005
Loss in iteration 105 : 0.0324244371703838
Loss in iteration 106 : 0.03230548258711746
Loss in iteration 107 : 0.0321884891510562
Loss in iteration 108 : 0.032073400885305206
Loss in iteration 109 : 0.03196016397509555
Loss in iteration 110 : 0.03184872666620305
Loss in iteration 111 : 0.03173903916895912
Loss in iteration 112 : 0.031631053567520696
Loss in iteration 113 : 0.03152472373406703
Loss in iteration 114 : 0.03142000524759965
Loss in iteration 115 : 0.031316855317038046
Loss in iteration 116 : 0.031215232708321924
Loss in iteration 117 : 0.031115097675255617
Loss in iteration 118 : 0.031016411893851632
Loss in iteration 119 : 0.030919138399955533
Loss in iteration 120 : 0.030823241529954474
Loss in iteration 121 : 0.030728686864392306
Loss in iteration 122 : 0.03063544117433152
Loss in iteration 123 : 0.03054347237031664
Loss in iteration 124 : 0.030452749453806566
Loss in iteration 125 : 0.030363242470952526
Loss in iteration 126 : 0.030274922468608315
Loss in iteration 127 : 0.030187761452463244
Loss in iteration 128 : 0.030101732347196312
Loss in iteration 129 : 0.030016808958553402
Loss in iteration 130 : 0.029932965937252143
Loss in iteration 131 : 0.029850178744624718
Loss in iteration 132 : 0.02976842361990969
Loss in iteration 133 : 0.02968767754910922
Loss in iteration 134 : 0.029607918235330176
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.9537777777777777, training accuracy 0.9934266933409546, time elapsed: 3186 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6326531461336024
Loss in iteration 3 : 0.5764503589779023
Loss in iteration 4 : 0.5243047399139797
Loss in iteration 5 : 0.4772655934001421
Loss in iteration 6 : 0.43579349864183076
Loss in iteration 7 : 0.3998501437993068
Loss in iteration 8 : 0.36907123665959785
Loss in iteration 9 : 0.342909023760911
Loss in iteration 10 : 0.320743276265047
Loss in iteration 11 : 0.301959314239452
Loss in iteration 12 : 0.28599447377225856
Loss in iteration 13 : 0.2723593857326625
Loss in iteration 14 : 0.26064239453462135
Loss in iteration 15 : 0.25050448688956517
Loss in iteration 16 : 0.24166996889801468
Loss in iteration 17 : 0.23391606982866006
Loss in iteration 18 : 0.22706313313038964
Loss in iteration 19 : 0.2209660993899013
Loss in iteration 20 : 0.21550745309190794
Loss in iteration 21 : 0.21059154405296543
Loss in iteration 22 : 0.2061400903196585
Loss in iteration 23 : 0.2020886474260607
Loss in iteration 24 : 0.19838384517240099
Loss in iteration 25 : 0.19498122355233358
Loss in iteration 26 : 0.19184353212292432
Loss in iteration 27 : 0.1889393866586515
Loss in iteration 28 : 0.18624220153324694
Loss in iteration 29 : 0.18372933579840472
Loss in iteration 30 : 0.18138140596349125
Loss in iteration 31 : 0.17918172983649758
Loss in iteration 32 : 0.1771158742641824
Loss in iteration 33 : 0.17517128590287648
Loss in iteration 34 : 0.173336988822202
Loss in iteration 35 : 0.17160333622626148
Loss in iteration 36 : 0.16996180619638737
Loss in iteration 37 : 0.16840483335496004
Loss in iteration 38 : 0.16692566989288982
Loss in iteration 39 : 0.16551827061612331
Loss in iteration 40 : 0.16417719763486793
Loss in iteration 41 : 0.1628975411033438
Loss in iteration 42 : 0.16167485306005464
Loss in iteration 43 : 0.16050509194879065
Loss in iteration 44 : 0.15938457584052426
Loss in iteration 45 : 0.15830994274200874
Loss in iteration 46 : 0.15727811668057867
Loss in iteration 47 : 0.15628627850607923
Loss in iteration 48 : 0.15533184055803892
Loss in iteration 49 : 0.15441242451586054
Loss in iteration 50 : 0.15352584188774546
Loss in iteration 51 : 0.15267007670532567
Loss in iteration 52 : 0.1518432700799339
Loss in iteration 53 : 0.1510437063469585
Loss in iteration 54 : 0.15026980058011216
Loss in iteration 55 : 0.14952008730056154
Loss in iteration 56 : 0.14879321023916803
Loss in iteration 57 : 0.14808791303561902
Loss in iteration 58 : 0.1474030307776964
Loss in iteration 59 : 0.14673748229874525
Loss in iteration 60 : 0.14609026316268825
Loss in iteration 61 : 0.14546043927459484
Loss in iteration 62 : 0.14484714106155885
Loss in iteration 63 : 0.14424955817399326
Loss in iteration 64 : 0.1436669346618581
Loss in iteration 65 : 0.14309856458403916
Loss in iteration 66 : 0.14254378801236547
Loss in iteration 67 : 0.14200198739468695
Loss in iteration 68 : 0.1414725842441602
Loss in iteration 69 : 0.1409550361244492
Loss in iteration 70 : 0.14044883390297974
Loss in iteration 71 : 0.13995349924670922
Loss in iteration 72 : 0.13946858233707968
Loss in iteration 73 : 0.13899365978292733
Loss in iteration 74 : 0.13852833271209153
Loss in iteration 75 : 0.13807222502434055
Loss in iteration 76 : 0.13762498178994173
Loss in iteration 77 : 0.13718626777981818
Loss in iteration 78 : 0.13675576611468834
Loss in iteration 79 : 0.13633317702192202
Loss in iteration 80 : 0.13591821669005125
Loss in iteration 81 : 0.13551061621196203
Loss in iteration 82 : 0.13511012060875238
Loss in iteration 83 : 0.13471648792711044
Loss in iteration 84 : 0.13432948840382547
Loss in iteration 85 : 0.13394890369171367
Loss in iteration 86 : 0.133574526141831
Loss in iteration 87 : 0.13320615813737213
Loss in iteration 88 : 0.1328436114750936
Loss in iteration 89 : 0.1324867067905128
Loss in iteration 90 : 0.13213527302347527
Loss in iteration 91 : 0.13178914692099625
Loss in iteration 92 : 0.13144817257454702
Loss in iteration 93 : 0.13111220098920826
Loss in iteration 94 : 0.13078108968231245
Loss in iteration 95 : 0.13045470230940573
Loss in iteration 96 : 0.1301329083155114
Loss in iteration 97 : 0.1298155826098553
Loss in iteration 98 : 0.12950260526233154
Loss in iteration 99 : 0.12919386122013596
Loss in iteration 100 : 0.12888924004309665
Loss in iteration 101 : 0.1285886356563498
Loss in iteration 102 : 0.12829194611910552
Loss in iteration 103 : 0.12799907340833802
Loss in iteration 104 : 0.12770992321632707
Loss in iteration 105 : 0.12742440476105144
Loss in iteration 106 : 0.12714243060851102
Loss in iteration 107 : 0.12686391650612486
Loss in iteration 108 : 0.12658878122641423
Loss in iteration 109 : 0.1263169464202375
Loss in iteration 110 : 0.12604833647890681
Loss in iteration 111 : 0.12578287840455474
Loss in iteration 112 : 0.1255205016881801
Loss in iteration 113 : 0.12526113819483556
Loss in iteration 114 : 0.1250047220554667
Loss in iteration 115 : 0.12475118956494613
Loss in iteration 116 : 0.12450047908588319
Loss in iteration 117 : 0.12425253095782059
Loss in iteration 118 : 0.12400728741145749
Loss in iteration 119 : 0.1237646924875694
Loss in iteration 120 : 0.12352469196031554
Loss in iteration 121 : 0.12328723326465227
Loss in iteration 122 : 0.12305226542758453
Loss in iteration 123 : 0.1228197390030202
Loss in iteration 124 : 0.12258960600999037
Loss in iteration 125 : 0.12236181987403642
Loss in iteration 126 : 0.12213633537156249
Loss in iteration 127 : 0.12191310857697639
Loss in iteration 128 : 0.12169209681244922
Loss in iteration 129 : 0.1214732586001381
Loss in iteration 130 : 0.12125655361672763
Loss in iteration 131 : 0.12104194265015254
Loss in iteration 132 : 0.12082938755837673
Loss in iteration 133 : 0.12061885123011182
Loss in iteration 134 : 0.12041029754736142
Loss in iteration 135 : 0.1202036913496945
Loss in iteration 136 : 0.11999899840014563
Loss in iteration 137 : 0.11979618535265481
Loss in iteration 138 : 0.11959521972096543
Loss in iteration 139 : 0.11939606984889677
Loss in iteration 140 : 0.11919870488191951
Loss in iteration 141 : 0.119003094739967
Loss in iteration 142 : 0.11880921009141261
Loss in iteration 143 : 0.11861702232815663
Loss in iteration 144 : 0.11842650354176429
Loss in iteration 145 : 0.11823762650059802
Loss in iteration 146 : 0.1180503646278973
Loss in iteration 147 : 0.11786469198075823
Loss in iteration 148 : 0.11768058322996419
Loss in iteration 149 : 0.11749801364062953
Loss in iteration 150 : 0.11731695905361524
Loss in iteration 151 : 0.11713739586767687
Loss in iteration 152 : 0.11695930102231439
Loss in iteration 153 : 0.11678265198128528
Loss in iteration 154 : 0.1166074267167518
Loss in iteration 155 : 0.11643360369403494
Loss in iteration 156 : 0.11626116185694169
Loss in iteration 157 : 0.11609008061364413
Loss in iteration 158 : 0.11592033982308338
Loss in iteration 159 : 0.11575191978187303
Loss in iteration 160 : 0.11558480121168187
Loss in iteration 161 : 0.11541896524707536
Loss in iteration 162 : 0.11525439342379143
Loss in iteration 163 : 0.11509106766743685
Loss in iteration 164 : 0.11492897028258144
Loss in iteration 165 : 0.11476808394223731
Loss in iteration 166 : 0.11460839167770231
Loss in iteration 167 : 0.11444987686875557
Loss in iteration 168 : 0.1142925232341893
Loss in iteration 169 : 0.11413631482266233
Loss in iteration 170 : 0.1139812360038634
Loss in iteration 171 : 0.11382727145996988
Loss in iteration 172 : 0.11367440617739084
Loss in iteration 173 : 0.113522625438785
Loss in iteration 174 : 0.11337191481533661
Loss in iteration 175 : 0.11322226015928703
Loss in iteration 176 : 0.11307364759670506
Loss in iteration 177 : 0.11292606352049195
Loss in iteration 178 : 0.11277949458360627
Loss in iteration 179 : 0.11263392769250712
Loss in iteration 180 : 0.11248935000080089
Loss in iteration 181 : 0.11234574890308827
Loss in iteration 182 : 0.11220311202900117
Loss in iteration 183 : 0.11206142723742496
Loss in iteration 184 : 0.11192068261089595
Loss in iteration 185 : 0.11178086645017152
Loss in iteration 186 : 0.11164196726896239
Loss in iteration 187 : 0.11150397378882558
Loss in iteration 188 : 0.1113668749342076
Loss in iteration 189 : 0.11123065982763654
Loss in iteration 190 : 0.1110953177850554
Loss in iteration 191 : 0.1109608383112916
Loss in iteration 192 : 0.11082721109565977
Loss in iteration 193 : 0.11069442600769082
Loss in iteration 194 : 0.11056247309298453
Loss in iteration 195 : 0.11043134256918154
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.9777777777777777, training accuracy 0.9739925693055158, time elapsed: 4465 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 48.071824095478505
Loss in iteration 3 : 4.061516071378867
Loss in iteration 4 : 2.6362641426802833
Loss in iteration 5 : 2.0585549202562547
Loss in iteration 6 : 1.6306055612800694
Loss in iteration 7 : 1.2858970328766124
Loss in iteration 8 : 1.0220945725867139
Loss in iteration 9 : 0.8538990270900415
Loss in iteration 10 : 0.7462125030304726
Loss in iteration 11 : 0.671545608821947
Loss in iteration 12 : 0.61014731045422
Loss in iteration 13 : 0.5560923950614769
Loss in iteration 14 : 0.5122783849438003
Loss in iteration 15 : 0.47618196099936416
Loss in iteration 16 : 0.4439388009402044
Loss in iteration 17 : 0.41493504116290747
Loss in iteration 18 : 0.3906734016665345
Loss in iteration 19 : 0.37079725917606676
Loss in iteration 20 : 0.3534221984996479
Loss in iteration 21 : 0.337643053794143
Loss in iteration 22 : 0.3234085002600236
Loss in iteration 23 : 0.31060775427135745
Loss in iteration 24 : 0.2988044029975925
Loss in iteration 25 : 0.2879679993983571
Loss in iteration 26 : 0.2778910724352052
Loss in iteration 27 : 0.26846964078828633
Loss in iteration 28 : 0.2597170782590288
Loss in iteration 29 : 0.2516265764121471
Loss in iteration 30 : 0.24412104907101057
Loss in iteration 31 : 0.23712145835489462
Loss in iteration 32 : 0.23059626138971073
Loss in iteration 33 : 0.22454839895237794
Loss in iteration 34 : 0.2189628354190231
Loss in iteration 35 : 0.213736295115004
Loss in iteration 36 : 0.20877565666493528
Loss in iteration 37 : 0.20403744243635205
Loss in iteration 38 : 0.1994957602853279
Loss in iteration 39 : 0.1951287702163972
Loss in iteration 40 : 0.1909262223207916
Loss in iteration 41 : 0.1868945464475677
Loss in iteration 42 : 0.18305236738510844
Loss in iteration 43 : 0.17943039331888458
Loss in iteration 44 : 0.1760684424755059
Loss in iteration 45 : 0.17298449545617622
Loss in iteration 46 : 0.1701507614956436
Loss in iteration 47 : 0.1675145355386399
Loss in iteration 48 : 0.16502607374220218
Loss in iteration 49 : 0.1626487639879761
Loss in iteration 50 : 0.16035882170456797
Loss in iteration 51 : 0.15814226038448634
Loss in iteration 52 : 0.15599201702137563
Loss in iteration 53 : 0.153905677083069
Loss in iteration 54 : 0.1518838663066415
Loss in iteration 55 : 0.14992972819637604
Loss in iteration 56 : 0.1480493961051711
Loss in iteration 57 : 0.14625129122372044
Loss in iteration 58 : 0.14454144282195477
Loss in iteration 59 : 0.142917468896447
Testing accuracy  of updater 3 on alg 0 with rate 40.0 = 0.9733333333333334, training accuracy 0.9955701629036867, time elapsed: 1550 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 1.8480788335699778
Loss in iteration 3 : 0.3919079477908707
Loss in iteration 4 : 0.17108672180373474
Loss in iteration 5 : 0.09208125781530721
Loss in iteration 6 : 0.061244002351333204
Loss in iteration 7 : 0.05003762063270052
Loss in iteration 8 : 0.04330897876031293
Loss in iteration 9 : 0.038662305189987485
Loss in iteration 10 : 0.03521646905504491
Loss in iteration 11 : 0.03252651411598873
Loss in iteration 12 : 0.030345228468404626
Loss in iteration 13 : 0.028525337949983517
Loss in iteration 14 : 0.02697380295543823
Loss in iteration 15 : 0.02562881763265097
Loss in iteration 16 : 0.024447511161258706
Loss in iteration 17 : 0.023399016602724875
Loss in iteration 18 : 0.022460382562112552
Loss in iteration 19 : 0.021614064007011862
Loss in iteration 20 : 0.020846327126615403
Loss in iteration 21 : 0.020146202444511842
Loss in iteration 22 : 0.019504777346225806
Loss in iteration 23 : 0.018914704882994432
Loss in iteration 24 : 0.018369854121863188
Loss in iteration 25 : 0.01786505546182613
Loss in iteration 26 : 0.017395911146364282
Loss in iteration 27 : 0.016958651502564464
Loss in iteration 28 : 0.01655002390462797
Loss in iteration 29 : 0.01616720561306532
Loss in iteration 30 : 0.015807734360639803
Loss in iteration 31 : 0.015469452366626647
Loss in iteration 32 : 0.015150460683623235
Loss in iteration 33 : 0.014849081618176867
Loss in iteration 34 : 0.014563827547697215
Loss in iteration 35 : 0.014293374865960528
Loss in iteration 36 : 0.014036542083424903
Loss in iteration 37 : 0.01379227132314112
Loss in iteration 38 : 0.013559612612528783
Loss in iteration 39 : 0.013337710491906043
Loss in iteration 40 : 0.013125792553355402
Loss in iteration 41 : 0.012923159595758153
Loss in iteration 42 : 0.012729177138835944
Loss in iteration 43 : 0.012543268084483649
Loss in iteration 44 : 0.012364906350217097
Loss in iteration 45 : 0.012193611329152792
Loss in iteration 46 : 0.01202894305504862
Loss in iteration 47 : 0.01187049797068176
Loss in iteration 48 : 0.011717905214085477
Loss in iteration 49 : 0.011570823350585832
Loss in iteration 50 : 0.011428937489702688
Loss in iteration 51 : 0.011291956735231893
Loss in iteration 52 : 0.01115961192454601
Loss in iteration 53 : 0.011031653619612693
Loss in iteration 54 : 0.010907850317652564
Loss in iteration 55 : 0.010787986853925312
Loss in iteration 56 : 0.010671862972984378
Loss in iteration 57 : 0.010559292048003963
Loss in iteration 58 : 0.010450099930548329
Loss in iteration 59 : 0.01034412391550866
Loss in iteration 60 : 0.0102412118079418
Loss in iteration 61 : 0.010141221080261142
Loss in iteration 62 : 0.010044018109703932
Loss in iteration 63 : 0.00994947748726292
Loss in iteration 64 : 0.009857481390360237
Loss in iteration 65 : 0.009767919012481925
Loss in iteration 66 : 0.009680686043805065
Loss in iteration 67 : 0.009595684197555073
Loss in iteration 68 : 0.009512820777443741
Loss in iteration 69 : 0.009432008282072777
Loss in iteration 70 : 0.009353164042652781
Loss in iteration 71 : 0.009276209890795832
Loss in iteration 72 : 0.009201071853495606
Loss in iteration 73 : 0.009127679872722937
Loss in iteration 74 : 0.009055967547339755
Loss in iteration 75 : 0.008985871895276843
Loss in iteration 76 : 0.008917333134135218
Testing accuracy  of updater 3 on alg 0 with rate 4.0 = 0.9697777777777777, training accuracy 0.9972849385538726, time elapsed: 1752 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.32959557793956007
Loss in iteration 3 : 0.2956665410525547
Loss in iteration 4 : 0.23227202499118665
Loss in iteration 5 : 0.192428810309048
Loss in iteration 6 : 0.18028999385382155
Loss in iteration 7 : 0.17087219490174319
Loss in iteration 8 : 0.16316534509263414
Loss in iteration 9 : 0.15669124124118305
Loss in iteration 10 : 0.15114236685445215
Loss in iteration 11 : 0.14631026199011132
Loss in iteration 12 : 0.14204766400665056
Loss in iteration 13 : 0.13824702813755038
Loss in iteration 14 : 0.13482763776405435
Loss in iteration 15 : 0.13172750729171082
Loss in iteration 16 : 0.12889809833205118
Loss in iteration 17 : 0.12630075890254863
Loss in iteration 18 : 0.1239042566087545
Loss in iteration 19 : 0.12168302828877287
Loss in iteration 20 : 0.11961591170443805
Loss in iteration 21 : 0.11768520933514447
Loss in iteration 22 : 0.11587598582188904
Loss in iteration 23 : 0.11417553290056232
Loss in iteration 24 : 0.1125729564318578
Loss in iteration 25 : 0.11105885379774619
Loss in iteration 26 : 0.10962505910735436
Loss in iteration 27 : 0.10826443992837231
Loss in iteration 28 : 0.10697073362298468
Loss in iteration 29 : 0.1057384144485218
Loss in iteration 30 : 0.10456258478995399
Loss in iteration 31 : 0.10343888549271568
Loss in iteration 32 : 0.10236342144035548
Loss in iteration 33 : 0.10133269939478735
Loss in iteration 34 : 0.10034357577216187
Loss in iteration 35 : 0.0993932125237613
Loss in iteration 36 : 0.0984790396707949
Loss in iteration 37 : 0.0975987233344958
Loss in iteration 38 : 0.09675013833024518
Loss in iteration 39 : 0.09593134457239888
Loss in iteration 40 : 0.09514056667678913
Loss in iteration 41 : 0.09437617625922119
Loss in iteration 42 : 0.09363667651719455
Loss in iteration 43 : 0.09292068875352112
Loss in iteration 44 : 0.09222694055821491
Loss in iteration 45 : 0.09155425541190948
Loss in iteration 46 : 0.09090154351231512
Loss in iteration 47 : 0.0902677936566283
Loss in iteration 48 : 0.08965206603866396
Loss in iteration 49 : 0.08905348584090071
Loss in iteration 50 : 0.08847123751942348
Loss in iteration 51 : 0.08790455969460206
Loss in iteration 52 : 0.0873527405727918
Loss in iteration 53 : 0.08681511383479978
Loss in iteration 54 : 0.08629105493570123
Loss in iteration 55 : 0.08577997776805506
Loss in iteration 56 : 0.08528133164692901
Loss in iteration 57 : 0.08479459858055256
Loss in iteration 58 : 0.08431929079504369
Loss in iteration 59 : 0.08385494848562061
Loss in iteration 60 : 0.0834011377701176
Loss in iteration 61 : 0.08295744882356616
Loss in iteration 62 : 0.08252349417513812
Loss in iteration 63 : 0.08209890715094853
Loss in iteration 64 : 0.08168334044812735
Loss in iteration 65 : 0.08127646482723229
Loss in iteration 66 : 0.08087796791152524
Loss in iteration 67 : 0.08048755308290438
Loss in iteration 68 : 0.08010493846539478
Loss in iteration 69 : 0.07972985598807955
Loss in iteration 70 : 0.07936205052020877
Loss in iteration 71 : 0.0790012790719823
Loss in iteration 72 : 0.0786473100551731
Loss in iteration 73 : 0.0782999225983465
Loss in iteration 74 : 0.07795890591195588
Loss in iteration 75 : 0.07762405869906387
Loss in iteration 76 : 0.07729518860784816
Loss in iteration 77 : 0.07697211172242259
Loss in iteration 78 : 0.07665465208883238
Loss in iteration 79 : 0.07634264127337799
Loss in iteration 80 : 0.07603591795068278
Loss in iteration 81 : 0.07573432751915686
Loss in iteration 82 : 0.0754377217417245
Loss in iteration 83 : 0.07514595840986509
Loss in iteration 84 : 0.07485890102919714
Loss in iteration 85 : 0.07457641852498467
Loss in iteration 86 : 0.07429838496608555
Loss in iteration 87 : 0.07402467930598633
Loss in iteration 88 : 0.07375518513968436
Loss in iteration 89 : 0.07348979047527912
Loss in iteration 90 : 0.07322838751922739
Loss in iteration 91 : 0.07297087247430498
Loss in iteration 92 : 0.0727171453493906
Loss in iteration 93 : 0.07246710978026122
Loss in iteration 94 : 0.07222067286065073
Loss in iteration 95 : 0.07197774498288026
Loss in iteration 96 : 0.07173823968742557
Loss in iteration 97 : 0.07150207352083075
Loss in iteration 98 : 0.07126916590142662
Loss in iteration 99 : 0.07103943899234794
Loss in iteration 100 : 0.07081281758138586
Loss in iteration 101 : 0.07058922896724093
Loss in iteration 102 : 0.07036860285177816
Loss in iteration 103 : 0.07015087123791089
Loss in iteration 104 : 0.06993596833276737
Loss in iteration 105 : 0.06972383045582094
Loss in iteration 106 : 0.06951439595168198
Loss in iteration 107 : 0.06930760510727595
Loss in iteration 108 : 0.06910340007314686
Loss in iteration 109 : 0.0689017247886454
Loss in iteration 110 : 0.06870252491077491
Loss in iteration 111 : 0.0685057477464869
Loss in iteration 112 : 0.06831134218822767
Loss in iteration 113 : 0.06811925865255332
Loss in iteration 114 : 0.06792944902164137
Loss in iteration 115 : 0.06774186658753743
Loss in iteration 116 : 0.06755646599898768
Loss in iteration 117 : 0.06737320321071481
Loss in iteration 118 : 0.06719203543500679
Loss in iteration 119 : 0.06701292109549256
Loss in iteration 120 : 0.06683581978298972
Loss in iteration 121 : 0.06666069221331489
Loss in iteration 122 : 0.06648750018695321
Loss in iteration 123 : 0.06631620655049134
Loss in iteration 124 : 0.06614677515972296
Loss in iteration 125 : 0.06597917084434136
Loss in iteration 126 : 0.06581335937413851
Loss in iteration 127 : 0.06564930742663563
Loss in iteration 128 : 0.065486982556073
Loss in iteration 129 : 0.06532635316369255
Loss in iteration 130 : 0.06516738846924955
Loss in iteration 131 : 0.06501005848369297
Loss in iteration 132 : 0.06485433398295896
Loss in iteration 133 : 0.06470018648282398
Loss in iteration 134 : 0.06454758821476579
Loss in iteration 135 : 0.06439651210278625
Loss in iteration 136 : 0.0642469317411507
Loss in iteration 137 : 0.06409882137299963
Loss in iteration 138 : 0.06395215586979448
Loss in iteration 139 : 0.06380691071155788
Loss in iteration 140 : 0.06366306196787266
Loss in iteration 141 : 0.06352058627960501
Loss in iteration 142 : 0.06337946084131928
Loss in iteration 143 : 0.06323966338435437
Loss in iteration 144 : 0.06310117216053122
Loss in iteration 145 : 0.06296396592646422
Loss in iteration 146 : 0.06282802392845051
Loss in iteration 147 : 0.06269332588791114
Loss in iteration 148 : 0.06255985198736165
Loss in iteration 149 : 0.06242758285688757
Loss in iteration 150 : 0.062296499561105674
Loss in iteration 151 : 0.06216658358658828
Loss in iteration 152 : 0.0620378168297328
Loss in iteration 153 : 0.06191018158505671
Loss in iteration 154 : 0.06178366053390171
Loss in iteration 155 : 0.061658236733528576
Loss in iteration 156 : 0.06153389360658841
Loss in iteration 157 : 0.06141061493095363
Loss in iteration 158 : 0.06128838482989555
Loss in iteration 159 : 0.061167187762592654
Loss in iteration 160 : 0.061047008514958874
Loss in iteration 161 : 0.06092783219077692
Loss in iteration 162 : 0.06080964420312647
Loss in iteration 163 : 0.060692430266094534
Loss in iteration 164 : 0.06057617638675715
Loss in iteration 165 : 0.06046086885742246
Loss in iteration 166 : 0.06034649424812455
Loss in iteration 167 : 0.06023303939935946
Loss in iteration 168 : 0.06012049141505239
Loss in iteration 169 : 0.060008837655749166
Loss in iteration 170 : 0.05989806573202269
Loss in iteration 171 : 0.05978816349808659
Loss in iteration 172 : 0.05967911904560803
Loss in iteration 173 : 0.05957092069771297
Loss in iteration 174 : 0.059463557003176334
Loss in iteration 175 : 0.059357016730790385
Loss in iteration 176 : 0.05925128886390527
Loss in iteration 177 : 0.05914636259513465
Loss in iteration 178 : 0.05904222732122215
Loss in iteration 179 : 0.05893887263806056
Loss in iteration 180 : 0.05883628833586035
Loss in iteration 181 : 0.058734464394461845
Loss in iteration 182 : 0.058633390978784845
Loss in iteration 183 : 0.05853305843441272
Loss in iteration 184 : 0.058433457283304686
Loss in iteration 185 : 0.05833457821963291
Loss in iteration 186 : 0.05823641210573966
Loss in iteration 187 : 0.058138949968210854
Loss in iteration 188 : 0.058042182994060984
Testing accuracy  of updater 3 on alg 0 with rate 0.3999999999999999 = 0.9715555555555555, training accuracy 0.9925693055158616, time elapsed: 4097 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224036
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632759
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439016
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043772
Loss in iteration 15 : 0.6751379721466284
Loss in iteration 16 : 0.6738340200990941
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694719
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873456
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050736
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663436
Loss in iteration 37 : 0.6465682744294413
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680169
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889296
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153971
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658474
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426644
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708373
Loss in iteration 64 : 0.6120394510085111
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535707
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114671
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033768
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098036
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895768
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.5832800218249878
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708515
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562977
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879716
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677673
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190326
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153798
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.548176269221112
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268604
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190975
Loss in iteration 131 : 0.5306307670254143
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961832
Loss in iteration 134 : 0.5271766840349437
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.523741543468015
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056484
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333072
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.4991491138168719
Loss in iteration 160 : 0.4980569551084796
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.49479415425263074
Loss in iteration 164 : 0.4937111271324443
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.49155195800730855
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.4883304768413243
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.48619438167066215
Loss in iteration 172 : 0.4851298124112099
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.4819500575854046
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.476697156505787
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.47357354825509423
Loss in iteration 184 : 0.4725370484517531
Loss in iteration 185 : 0.47150290363343866
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536213
Loss in iteration 188 : 0.46841461808560453
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.46636756901018045
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.4633147371593365
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.4612913582360449
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.45927746265434266
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.4562744140691075
Testing accuracy  of updater 4 on alg 0 with rate 1000.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4699 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224037
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257244
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632759
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439014
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043774
Loss in iteration 15 : 0.6751379721466285
Loss in iteration 16 : 0.6738340200990941
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694719
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053065
Loss in iteration 24 : 0.6634139578873454
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050735
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663436
Loss in iteration 37 : 0.6465682744294413
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260154
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.638837287894918
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680169
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889296
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106355
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658474
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426643
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033768
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895764
Loss in iteration 84 : 0.5869900323068191
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.583280021824988
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708515
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261589
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562977
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677672
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211118
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268604
Loss in iteration 124 : 0.5387625598561838
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190975
Loss in iteration 131 : 0.5306307670254145
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961832
Loss in iteration 134 : 0.5271766840349436
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.523741543468015
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056485
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.505749596933307
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.49914911381687194
Loss in iteration 160 : 0.4980569551084795
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.4947941542526308
Loss in iteration 164 : 0.49371112713244425
Loss in iteration 165 : 0.4926303936870237
Loss in iteration 166 : 0.4915519580073085
Loss in iteration 167 : 0.49047582407548224
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.4883304768413243
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.4861943816706622
Loss in iteration 172 : 0.4851298124112099
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.4819500575854046
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.4798418793763408
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.47669715650578703
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.47357354825509423
Loss in iteration 184 : 0.4725370484517532
Loss in iteration 185 : 0.4715029036334387
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.46636756901018045
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.4612913582360449
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.4592774626543426
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.4562744140691076
Testing accuracy  of updater 4 on alg 0 with rate 100.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4902 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224037
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632759
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439014
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043772
Loss in iteration 15 : 0.6751379721466284
Loss in iteration 16 : 0.6738340200990942
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694719
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642098
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873456
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050735
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663437
Loss in iteration 37 : 0.6465682744294413
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.638837287894918
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680166
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889295
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106355
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658473
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426644
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739225
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033767
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213936
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.5832800218249878
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708514
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261589
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562977
Loss in iteration 105 : 0.5613191712872166
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677673
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.548176269221112
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.54581110087663
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.5387625598561839
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190974
Loss in iteration 131 : 0.5306307670254143
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961832
Loss in iteration 134 : 0.5271766840349437
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.5237415434680152
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056484
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333071
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.49914911381687194
Loss in iteration 160 : 0.4980569551084796
Loss in iteration 161 : 0.4969670726066896
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.49479415425263085
Loss in iteration 164 : 0.49371112713244436
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.4915519580073085
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.48833047684132425
Loss in iteration 170 : 0.4872612709605178
Loss in iteration 171 : 0.4861943816706622
Loss in iteration 172 : 0.4851298124112099
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.48195005758540466
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.4777430546573994
Loss in iteration 180 : 0.476697156505787
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.47253704845175315
Loss in iteration 185 : 0.47150290363343866
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905835
Loss in iteration 190 : 0.46636756901018045
Loss in iteration 191 : 0.4653475914848989
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.4623018627393149
Loss in iteration 195 : 0.46129135823604495
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.45927746265434266
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.4562744140691076
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4349 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 15.209256382864378
Loss in iteration 3 : 1.3325829487318877
Loss in iteration 4 : 0.8319496014039717
Loss in iteration 5 : 0.6213140578592693
Loss in iteration 6 : 0.4663907500247813
Loss in iteration 7 : 0.347102099830223
Loss in iteration 8 : 0.2684794803455803
Loss in iteration 9 : 0.221602767524273
Loss in iteration 10 : 0.18891249277853003
Loss in iteration 11 : 0.16407578640417098
Loss in iteration 12 : 0.14488127571080925
Loss in iteration 13 : 0.1291244336528069
Loss in iteration 14 : 0.11622598171616748
Loss in iteration 15 : 0.10528055831758835
Loss in iteration 16 : 0.0958161393134366
Loss in iteration 17 : 0.08741163519862379
Loss in iteration 18 : 0.07984970027044919
Loss in iteration 19 : 0.07303430807690502
Loss in iteration 20 : 0.06688582168855048
Loss in iteration 21 : 0.061419651879460885
Loss in iteration 22 : 0.05665630279775749
Loss in iteration 23 : 0.05253067159909581
Loss in iteration 24 : 0.04893712885690978
Loss in iteration 25 : 0.045774979823133825
Loss in iteration 26 : 0.04295089442087444
Loss in iteration 27 : 0.040391339045216496
Loss in iteration 28 : 0.038060372452618986
Loss in iteration 29 : 0.035926346195888206
Loss in iteration 30 : 0.033948591241454475
Loss in iteration 31 : 0.03210104741620453
Loss in iteration 32 : 0.030374804928053032
Loss in iteration 33 : 0.028774362059527697
Loss in iteration 34 : 0.027306969275110875
Loss in iteration 35 : 0.02597298799950362
Loss in iteration 36 : 0.024763644635444212
Loss in iteration 37 : 0.023662843303587203
Loss in iteration 38 : 0.02265072341617129
Loss in iteration 39 : 0.021707459409930463
Loss in iteration 40 : 0.020816105648251773
Loss in iteration 41 : 0.019964377994077263
Loss in iteration 42 : 0.019146462195152314
Loss in iteration 43 : 0.018364554224968667
Loss in iteration 44 : 0.01762567546217353
Loss in iteration 45 : 0.016931163263930536
Loss in iteration 46 : 0.016271170114859096
Loss in iteration 47 : 0.015631105661804718
Loss in iteration 48 : 0.01499827139484703
Loss in iteration 49 : 0.01436363350537336
Loss in iteration 50 : 0.013721233255943948
Loss in iteration 51 : 0.013067203098836388
Loss in iteration 52 : 0.012399030319140202
Loss in iteration 53 : 0.011715155577252981
Loss in iteration 54 : 0.011014901572516217
Loss in iteration 55 : 0.010298831949874862
Loss in iteration 56 : 0.009569823730966995
Loss in iteration 57 : 0.008835010724565476
Loss in iteration 58 : 0.008106672117864661
Loss in iteration 59 : 0.0073959678264005095
Loss in iteration 60 : 0.006701933089022525
Loss in iteration 61 : 0.0060141355778249896
Loss in iteration 62 : 0.0053232003528344545
Loss in iteration 63 : 0.0046236402024528
Loss in iteration 64 : 0.003912517521410206
Loss in iteration 65 : 0.0031892539213649335
Loss in iteration 66 : 0.00246092878611177
Loss in iteration 67 : 0.0017662501071396457
Loss in iteration 68 : 0.0011981536021282554
Loss in iteration 69 : 8.08677794826248E-4
Loss in iteration 70 : 5.67452468692968E-4
Loss in iteration 71 : 4.230269949397758E-4
Loss in iteration 72 : 3.33561786184254E-4
Loss in iteration 73 : 2.7422866186598005E-4
Loss in iteration 74 : 2.3244120990643312E-4
Loss in iteration 75 : 2.0152493178362578E-4
Loss in iteration 76 : 1.7775158778937783E-4
Testing accuracy  of updater 5 on alg 0 with rate 4.0 = 0.992, training accuracy 1.0, time elapsed: 1511 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 1.5489102093646068
Loss in iteration 3 : 0.17114439167215584
Loss in iteration 4 : 0.11522612817264616
Loss in iteration 5 : 0.08886800848680794
Loss in iteration 6 : 0.07324321864307648
Loss in iteration 7 : 0.06196963691131447
Loss in iteration 8 : 0.05366926680030061
Loss in iteration 9 : 0.04750859492123037
Loss in iteration 10 : 0.04280942855283087
Loss in iteration 11 : 0.039100088065772975
Loss in iteration 12 : 0.036079108361264906
Loss in iteration 13 : 0.03355386809893633
Loss in iteration 14 : 0.03139743578093879
Loss in iteration 15 : 0.029523296800976616
Loss in iteration 16 : 0.027870718347782757
Loss in iteration 17 : 0.02639596279730646
Loss in iteration 18 : 0.02506680104523074
Loss in iteration 19 : 0.02385897626863279
Loss in iteration 20 : 0.022753867266245505
Loss in iteration 21 : 0.021736911875075995
Loss in iteration 22 : 0.020796523192836574
Loss in iteration 23 : 0.019923331564823787
Loss in iteration 24 : 0.019109645721212126
Loss in iteration 25 : 0.018349063794266013
Loss in iteration 26 : 0.01763618846510267
Loss in iteration 27 : 0.016966415546726444
Loss in iteration 28 : 0.016335775097078592
Loss in iteration 29 : 0.015740810612517437
Loss in iteration 30 : 0.015178486174639147
Loss in iteration 31 : 0.01464611435892395
Loss in iteration 32 : 0.014141299734883829
Loss in iteration 33 : 0.013661894196762888
Loss in iteration 34 : 0.013205961357752203
Loss in iteration 35 : 0.012771747948452442
Loss in iteration 36 : 0.012357660668568407
Loss in iteration 37 : 0.011962247308629976
Loss in iteration 38 : 0.011584181226985898
Loss in iteration 39 : 0.01122224846549378
Loss in iteration 40 : 0.010875336936102055
Loss in iteration 41 : 0.010542427224946046
Loss in iteration 42 : 0.01022258465141584
Loss in iteration 43 : 0.009914952294352346
Loss in iteration 44 : 0.009618744760937388
Loss in iteration 45 : 0.009333242528759732
Loss in iteration 46 : 0.009057786739214166
Loss in iteration 47 : 0.008791774361038446
Loss in iteration 48 : 0.00853465367608114
Loss in iteration 49 : 0.008285920064896547
Loss in iteration 50 : 0.00804511208731719
Loss in iteration 51 : 0.007811807863072418
Loss in iteration 52 : 0.007585621760642624
Loss in iteration 53 : 0.007366201400136663
Loss in iteration 54 : 0.007153224969579718
Loss in iteration 55 : 0.00694639884514582
Loss in iteration 56 : 0.006745455495950636
Loss in iteration 57 : 0.006550151644142985
Loss in iteration 58 : 0.00636026664199683
Loss in iteration 59 : 0.006175601020060602
Loss in iteration 60 : 0.005995975154564877
Loss in iteration 61 : 0.005821227998602938
Loss in iteration 62 : 0.005651215820532041
Loss in iteration 63 : 0.00548581089517596
Loss in iteration 64 : 0.005324900099411735
Loss in iteration 65 : 0.0051683833742258226
Loss in iteration 66 : 0.00501617203069871
Loss in iteration 67 : 0.0048681868974369494
Loss in iteration 68 : 0.004724356330699857
Loss in iteration 69 : 0.004584614133814988
Loss in iteration 70 : 0.004448897456399143
Loss in iteration 71 : 0.004317144762696557
Loss in iteration 72 : 0.004189293968303433
Loss in iteration 73 : 0.004065280842832095
Loss in iteration 74 : 0.003945037761571928
Loss in iteration 75 : 0.0038284928631123504
Loss in iteration 76 : 0.003715569635690966
Loss in iteration 77 : 0.0036061869178770816
Loss in iteration 78 : 0.0035002592648240446
Loss in iteration 79 : 0.0033976976047351234
Loss in iteration 80 : 0.003298410094642999
Loss in iteration 81 : 0.003202303081074896
Loss in iteration 82 : 0.0031092820784486227
Loss in iteration 83 : 0.0030192526933119387
Loss in iteration 84 : 0.0029321214422429577
Loss in iteration 85 : 0.0028477964318999295
Loss in iteration 86 : 0.002766187888557035
Loss in iteration 87 : 0.0026872085396910466
Loss in iteration 88 : 0.002610773861005713
Loss in iteration 89 : 0.0025368022087445758
Loss in iteration 90 : 0.0024652148598571866
Loss in iteration 91 : 0.0023959359824433694
Loss in iteration 92 : 0.0023288925568545234
Loss in iteration 93 : 0.0022640142647332704
Loss in iteration 94 : 0.0022012333597942494
Loss in iteration 95 : 0.0021404845307620044
Loss in iteration 96 : 0.0020817047638721337
Loss in iteration 97 : 0.0020248332098463705
Loss in iteration 98 : 0.0019698110582998395
Loss in iteration 99 : 0.0019165814210954659
Loss in iteration 100 : 0.0018650892251501796
Loss in iteration 101 : 0.0018152811145381099
Loss in iteration 102 : 0.0017671053613420294
Loss in iteration 103 : 0.0017205117845025355
Loss in iteration 104 : 0.001675451675845106
Loss in iteration 105 : 0.0016318777324790775
Loss in iteration 106 : 0.0015897439948270927
Loss in iteration 107 : 0.001549005789631564
Loss in iteration 108 : 0.0015096196773809226
Loss in iteration 109 : 0.0014715434036915104
Loss in iteration 110 : 0.0014347358542645928
Loss in iteration 111 : 0.001399157013109974
Loss in iteration 112 : 0.0013647679237863452
Loss in iteration 113 : 0.0013315306534546518
Loss in iteration 114 : 0.0012994082595767164
Loss in iteration 115 : 0.0012683647591160213
Loss in iteration 116 : 0.0012383651001179473
Loss in iteration 117 : 0.0012093751355540592
Loss in iteration 118 : 0.0011813615993312944
Loss in iteration 119 : 0.0011542920843486465
Loss in iteration 120 : 0.0011281350225276938
Loss in iteration 121 : 0.001102859666642315
Loss in iteration 122 : 0.0010784360739737227
Loss in iteration 123 : 0.001054835091364186
Loss in iteration 124 : 0.0010320283421768046
Loss in iteration 125 : 0.0010099882136549029
Loss in iteration 126 : 9.886878473863345E-4
Loss in iteration 127 : 9.681011267269539E-4
Loss in iteration 128 : 9.482026734263223E-4
Loss in iteration 129 : 9.289678278941201E-4
Loss in iteration 130 : 9.103726649382967E-4
Loss in iteration 131 : 8.923939419938898E-4
Loss in iteration 132 : 8.750091890219836E-4
Loss in iteration 133 : 8.581965491446583E-4
Loss in iteration 134 : 8.419351804816222E-4
Loss in iteration 135 : 8.262048581523733E-4
Loss in iteration 136 : 8.109878167655951E-4
Loss in iteration 137 : 7.962686401338757E-4
Loss in iteration 138 : 7.820432934719967E-4
Loss in iteration 139 : 7.683262436562563E-4
Loss in iteration 140 : 7.551942816357857E-4
Loss in iteration 141 : 7.42838012095926E-4
Loss in iteration 142 : 7.31734050866428E-4
Loss in iteration 143 : 7.227405646418384E-4
Loss in iteration 144 : 7.171824266349094E-4
Loss in iteration 145 : 7.150658543380001E-4
Loss in iteration 146 : 7.133886605576931E-4
Loss in iteration 147 : 7.047573637500049E-4
Loss in iteration 148 : 6.888765558222485E-4
Loss in iteration 149 : 6.695040931558084E-4
Loss in iteration 150 : 6.524612165946441E-4
Loss in iteration 151 : 6.383346034133387E-4
Loss in iteration 152 : 6.267362915707475E-4
Loss in iteration 153 : 6.166274482636186E-4
Loss in iteration 154 : 6.074490271391282E-4
Loss in iteration 155 : 5.988302102757894E-4
Loss in iteration 156 : 5.906006193788193E-4
Loss in iteration 157 : 5.826592564604922E-4
Loss in iteration 158 : 5.749594134771632E-4
Loss in iteration 159 : 5.674708205613793E-4
Loss in iteration 160 : 5.601782476497453E-4
Loss in iteration 161 : 5.530697220979054E-4
Loss in iteration 162 : 5.461379540796926E-4
Loss in iteration 163 : 5.393760792039839E-4
Loss in iteration 164 : 5.327788872421901E-4
Loss in iteration 165 : 5.263411195325131E-4
Loss in iteration 166 : 5.200581770605988E-4
Loss in iteration 167 : 5.139254151675806E-4
Loss in iteration 168 : 5.079384943576754E-4
Loss in iteration 169 : 5.020930925652503E-4
Loss in iteration 170 : 4.963850630112738E-4
Loss in iteration 171 : 4.90810322446763E-4
Loss in iteration 172 : 4.8536491656541234E-4
Loss in iteration 173 : 4.800449795404801E-4
Loss in iteration 174 : 4.748467588522964E-4
Loss in iteration 175 : 4.6976660161011675E-4
Loss in iteration 176 : 4.6480096280325614E-4
Loss in iteration 177 : 4.599464005755018E-4
Loss in iteration 178 : 4.551995780418882E-4
Loss in iteration 179 : 4.505572609940717E-4
Loss in iteration 180 : 4.4601631723218804E-4
Loss in iteration 181 : 4.4157371461224563E-4
Loss in iteration 182 : 4.372265193485056E-4
Loss in iteration 183 : 4.3297189387975384E-4
Loss in iteration 184 : 4.2880709468773E-4
Loss in iteration 185 : 4.247294699403729E-4
Loss in iteration 186 : 4.207364570631403E-4
Loss in iteration 187 : 4.1682558023352394E-4
Loss in iteration 188 : 4.129944478357142E-4
Loss in iteration 189 : 4.092407498899297E-4
Loss in iteration 190 : 4.05562255476417E-4
Loss in iteration 191 : 4.01956810167841E-4
Loss in iteration 192 : 3.9842233348313905E-4
Loss in iteration 193 : 3.949568163732079E-4
Loss in iteration 194 : 3.915583187472012E-4
Loss in iteration 195 : 3.8822496704655904E-4
Loss in iteration 196 : 3.8495495187249057E-4
Loss in iteration 197 : 3.817465256714936E-4
Loss in iteration 198 : 3.7859800048220603E-4
Loss in iteration 199 : 3.7550774574635244E-4
Loss in iteration 200 : 3.724741861852004E-4
Testing accuracy  of updater 5 on alg 0 with rate 0.4 = 1.0, training accuracy 1.0, time elapsed: 3472 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.4491489988989485
Loss in iteration 3 : 0.33243107467393757
Loss in iteration 4 : 0.2897085443141558
Loss in iteration 5 : 0.2688715300200639
Loss in iteration 6 : 0.2533343927006715
Loss in iteration 7 : 0.24086907578534017
Loss in iteration 8 : 0.23047957546784492
Loss in iteration 9 : 0.22158053248380669
Loss in iteration 10 : 0.2138002297369454
Loss in iteration 11 : 0.20688876869557515
Loss in iteration 12 : 0.20067056776593728
Loss in iteration 13 : 0.1950178399090423
Loss in iteration 14 : 0.1898348677836839
Loss in iteration 15 : 0.18504821634729626
Loss in iteration 16 : 0.18060039111941562
Loss in iteration 17 : 0.1764455876357184
Loss in iteration 18 : 0.17254675902361963
Loss in iteration 19 : 0.1688735418198561
Loss in iteration 20 : 0.16540075654648975
Loss in iteration 21 : 0.1621073028040769
Loss in iteration 22 : 0.15897533112561896
Loss in iteration 23 : 0.1559896127838699
Loss in iteration 24 : 0.1531370536662093
Loss in iteration 25 : 0.15040631465464782
Loss in iteration 26 : 0.14778751186728906
Loss in iteration 27 : 0.1452719775613754
Loss in iteration 28 : 0.14285206766128702
Loss in iteration 29 : 0.14052100551319882
Loss in iteration 30 : 0.13827275406919848
Loss in iteration 31 : 0.1361019105880469
Loss in iteration 32 : 0.13400361932165517
Loss in iteration 33 : 0.13197349868113253
Loss in iteration 34 : 0.13000758014405153
Loss in iteration 35 : 0.12810225674524073
Loss in iteration 36 : 0.12625423943634675
Loss in iteration 37 : 0.12446051993999911
Loss in iteration 38 : 0.12271833898829884
Loss in iteration 39 : 0.12102515904145855
Loss in iteration 40 : 0.11937864074478971
Loss in iteration 41 : 0.11777662251153538
Loss in iteration 42 : 0.11621710272337338
Loss in iteration 43 : 0.11469822412596109
Loss in iteration 44 : 0.11321826006828863
Loss in iteration 45 : 0.11177560229522589
Loss in iteration 46 : 0.11036875005475202
Loss in iteration 47 : 0.10899630032633473
Loss in iteration 48 : 0.10765693901549805
Loss in iteration 49 : 0.10634943299208378
Loss in iteration 50 : 0.10507262287621175
Loss in iteration 51 : 0.10382541649671133
Loss in iteration 52 : 0.10260678296223615
Loss in iteration 53 : 0.10141574729607715
Loss in iteration 54 : 0.10025138559271882
Loss in iteration 55 : 0.09911282065842943
Loss in iteration 56 : 0.0979992181005331
Loss in iteration 57 : 0.09690978283131016
Loss in iteration 58 : 0.09584375595326788
Loss in iteration 59 : 0.09480041199320208
Loss in iteration 60 : 0.09377905645323174
Loss in iteration 61 : 0.09277902364787091
Loss in iteration 62 : 0.09179967479719421
Loss in iteration 63 : 0.09084039634721466
Loss in iteration 64 : 0.08990059848966976
Loss in iteration 65 : 0.08897971385454401
Loss in iteration 66 : 0.08807719634986454
Loss in iteration 67 : 0.08719252012471164
Loss in iteration 68 : 0.0863251786330953
Loss in iteration 69 : 0.08547468377846618
Loss in iteration 70 : 0.0846405651212246
Loss in iteration 71 : 0.08382236913465708
Loss in iteration 72 : 0.08301965849813067
Loss in iteration 73 : 0.0822320114199534
Loss in iteration 74 : 0.08145902098572885
Loss in iteration 75 : 0.08070029453104344
Loss in iteration 76 : 0.07995545303958715
Loss in iteration 77 : 0.07922413056917008
Loss in iteration 78 : 0.07850597370841003
Loss in iteration 79 : 0.07780064106622417
Loss in iteration 80 : 0.07710780279481061
Loss in iteration 81 : 0.07642714014485597
Loss in iteration 82 : 0.07575834504963175
Loss in iteration 83 : 0.07510111973282482
Loss in iteration 84 : 0.0744551763337553
Loss in iteration 85 : 0.07382023654329871
Loss in iteration 86 : 0.07319603124444296
Loss in iteration 87 : 0.07258230015285473
Loss in iteration 88 : 0.07197879145485567
Loss in iteration 89 : 0.07138526144243966
Loss in iteration 90 : 0.07080147414705097
Loss in iteration 91 : 0.0702272009754963
Loss in iteration 92 : 0.06966222035242892
Loss in iteration 93 : 0.06910631737427612
Loss in iteration 94 : 0.06855928347934359
Loss in iteration 95 : 0.06802091613822853
Loss in iteration 96 : 0.06749101856772673
Loss in iteration 97 : 0.06696939947026959
Loss in iteration 98 : 0.06645587279967327
Loss in iteration 99 : 0.06595025755274468
Loss in iteration 100 : 0.06545237758515761
Loss in iteration 101 : 0.06496206144905793
Loss in iteration 102 : 0.06447914224913276
Loss in iteration 103 : 0.06400345751344876
Loss in iteration 104 : 0.06353484907513537
Loss in iteration 105 : 0.0630731629612642
Loss in iteration 106 : 0.06261824928507685
Loss in iteration 107 : 0.0621699621400486
Loss in iteration 108 : 0.06172815948887346
Loss in iteration 109 : 0.06129270306304845
Loss in iteration 110 : 0.06086345821421862
Loss in iteration 111 : 0.06044029401954867
Loss in iteration 112 : 0.060023083872400726
Loss in iteration 113 : 0.05961173195007337
Loss in iteration 114 : 0.059206629010022854
Loss in iteration 115 : 0.058817412965889944
Loss in iteration 116 : 0.05849385325007836
Loss in iteration 117 : 0.05810974765345807
Loss in iteration 118 : 0.05768138926400552
Loss in iteration 119 : 0.05728556030254191
Loss in iteration 120 : 0.05690992452129636
Loss in iteration 121 : 0.05654313962314859
Loss in iteration 122 : 0.05618211338788321
Loss in iteration 123 : 0.05582579319897924
Loss in iteration 124 : 0.05547389387453779
Loss in iteration 125 : 0.05512623185338778
Loss in iteration 126 : 0.0547827351845203
Loss in iteration 127 : 0.05444331398425713
Loss in iteration 128 : 0.05410791788109042
Loss in iteration 129 : 0.05377647645797458
Loss in iteration 130 : 0.05344894865486275
Loss in iteration 131 : 0.05312528235313366
Loss in iteration 132 : 0.05280547258049837
Loss in iteration 133 : 0.05248954840201749
Loss in iteration 134 : 0.05217771515012827
Loss in iteration 135 : 0.05187048992206387
Loss in iteration 136 : 0.0515691181430719
Loss in iteration 137 : 0.05127486172921829
Loss in iteration 138 : 0.05098572826526223
Loss in iteration 139 : 0.05069355228564249
Loss in iteration 140 : 0.050398392658788094
Loss in iteration 141 : 0.05010500466033674
Loss in iteration 142 : 0.049817197363379466
Loss in iteration 143 : 0.04953421639426912
Loss in iteration 144 : 0.04925551792383506
Loss in iteration 145 : 0.04898030198758667
Loss in iteration 146 : 0.048708352790628545
Loss in iteration 147 : 0.0484393830328566
Loss in iteration 148 : 0.04817337522822651
Loss in iteration 149 : 0.04791021024207466
Loss in iteration 150 : 0.04764996027711985
Loss in iteration 151 : 0.04739259303055386
Loss in iteration 152 : 0.04713829600449928
Loss in iteration 153 : 0.046887066902930166
Loss in iteration 154 : 0.046639101881832415
Loss in iteration 155 : 0.046393847727889674
Loss in iteration 156 : 0.04615092571637175
Loss in iteration 157 : 0.04590907945990451
Loss in iteration 158 : 0.04566876878335986
Loss in iteration 159 : 0.04542996630919611
Loss in iteration 160 : 0.04519373444412929
Loss in iteration 161 : 0.044959927545960815
Loss in iteration 162 : 0.04472885772124011
Loss in iteration 163 : 0.04450017839039062
Loss in iteration 164 : 0.04427397797782767
Loss in iteration 165 : 0.04404999737756984
Loss in iteration 166 : 0.04382833237416128
Loss in iteration 167 : 0.04360879656045795
Loss in iteration 168 : 0.0433915236708612
Loss in iteration 169 : 0.043176329589768976
Loss in iteration 170 : 0.04296337135805835
Loss in iteration 171 : 0.04275235061208356
Loss in iteration 172 : 0.042543399976397264
Loss in iteration 173 : 0.0423360346260629
Loss in iteration 174 : 0.04213045530653609
Loss in iteration 175 : 0.041926228273621864
Loss in iteration 176 : 0.04172377010549075
Loss in iteration 177 : 0.04152281807234912
Loss in iteration 178 : 0.04132377294357066
Loss in iteration 179 : 0.04112639040691801
Loss in iteration 180 : 0.04093092018041628
Loss in iteration 181 : 0.04073712005417931
Loss in iteration 182 : 0.04054515909714842
Loss in iteration 183 : 0.04035481976928424
Loss in iteration 184 : 0.04016625170530151
Loss in iteration 185 : 0.03997924824714845
Loss in iteration 186 : 0.039793960890904
Loss in iteration 187 : 0.03961016147327863
Loss in iteration 188 : 0.03942800680756524
Loss in iteration 189 : 0.03924722601573227
Loss in iteration 190 : 0.039067993620639306
Loss in iteration 191 : 0.03889002455604051
Loss in iteration 192 : 0.038713535519210855
Loss in iteration 193 : 0.03853827792222366
Loss in iteration 194 : 0.038364492632396024
Loss in iteration 195 : 0.0381919694334893
Loss in iteration 196 : 0.038020923203336755
Loss in iteration 197 : 0.037851161382526095
Loss in iteration 198 : 0.03768285711591913
Loss in iteration 199 : 0.03751582871391059
Loss in iteration 200 : 0.03735022215329922
Testing accuracy  of updater 5 on alg 0 with rate 0.03999999999999998 = 0.944, training accuracy 0.9942840811660475, time elapsed: 3617 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 7.153773178702859
Loss in iteration 3 : 1.8475234194324377
Loss in iteration 4 : 1.0669890519615148
Loss in iteration 5 : 2.3542733697062035
Loss in iteration 6 : 2.1765921683802745
Loss in iteration 7 : 1.3900310369835471
Loss in iteration 8 : 0.8929474524024157
Loss in iteration 9 : 0.5816284844000086
Loss in iteration 10 : 0.3916612928548665
Loss in iteration 11 : 0.3242999243670561
Loss in iteration 12 : 0.36394381544998916
Loss in iteration 13 : 0.4305592948317077
Loss in iteration 14 : 0.47054562655515775
Loss in iteration 15 : 0.46642220040862103
Loss in iteration 16 : 0.4178669987621431
Loss in iteration 17 : 0.3428664483254921
Loss in iteration 18 : 0.27002402915949714
Loss in iteration 19 : 0.20605553061675366
Loss in iteration 20 : 0.15536605588097313
Loss in iteration 21 : 0.11922594046447084
Loss in iteration 22 : 0.09986084696922469
Loss in iteration 23 : 0.08916726568244851
Loss in iteration 24 : 0.08327944393900888
Loss in iteration 25 : 0.08020430397212015
Loss in iteration 26 : 0.08028544448625113
Loss in iteration 27 : 0.08119375202305897
Loss in iteration 28 : 0.08235434056960884
Loss in iteration 29 : 0.08271922046239444
Loss in iteration 30 : 0.08158116211697046
Loss in iteration 31 : 0.0789774515854737
Loss in iteration 32 : 0.07525457201620156
Loss in iteration 33 : 0.07083612705548166
Loss in iteration 34 : 0.0661522285783102
Loss in iteration 35 : 0.06148048294298387
Loss in iteration 36 : 0.05694839417650742
Loss in iteration 37 : 0.05267877058939963
Loss in iteration 38 : 0.048900001247582585
Loss in iteration 39 : 0.045847338798293176
Loss in iteration 40 : 0.04354820186395092
Loss in iteration 41 : 0.041796811570845256
Loss in iteration 42 : 0.04036985055851258
Loss in iteration 43 : 0.03914276642858325
Loss in iteration 44 : 0.038122369130136335
Loss in iteration 45 : 0.03733734485631126
Loss in iteration 46 : 0.036752241673657246
Loss in iteration 47 : 0.03630103586829783
Loss in iteration 48 : 0.03591183714362559
Loss in iteration 49 : 0.0355344783293455
Loss in iteration 50 : 0.03514769709098809
Loss in iteration 51 : 0.0347456921723068
Loss in iteration 52 : 0.03432705395773063
Loss in iteration 53 : 0.03389043388903438
Loss in iteration 54 : 0.03343489122117686
Loss in iteration 55 : 0.03296119995522421
Loss in iteration 56 : 0.03247220728295047
Loss in iteration 57 : 0.03197251247521926
Loss in iteration 58 : 0.03146791883152891
Loss in iteration 59 : 0.030964705636726667
Loss in iteration 60 : 0.030468747008542314
Loss in iteration 61 : 0.029984686357396678
Loss in iteration 62 : 0.029515412283472552
Loss in iteration 63 : 0.029061933589103896
Loss in iteration 64 : 0.02862361276153884
Loss in iteration 65 : 0.028198662366461156
Loss in iteration 66 : 0.02778475515646778
Loss in iteration 67 : 0.027379565200197974
Loss in iteration 68 : 0.026981118978944
Loss in iteration 69 : 0.026587948812401126
Loss in iteration 70 : 0.026199113105589655
Loss in iteration 71 : 0.02581415492088068
Loss in iteration 72 : 0.025433043551280468
Loss in iteration 73 : 0.025056113640531136
Loss in iteration 74 : 0.02468399563253543
Loss in iteration 75 : 0.02431752355752283
Loss in iteration 76 : 0.023957613724097875
Loss in iteration 77 : 0.023605129223476046
Loss in iteration 78 : 0.023260766885796008
Loss in iteration 79 : 0.022925003569555843
Loss in iteration 80 : 0.022598109787389818
Loss in iteration 81 : 0.02228020304090427
Loss in iteration 82 : 0.021971301283177348
Loss in iteration 83 : 0.021671352821402874
Loss in iteration 84 : 0.021380241182937736
Loss in iteration 85 : 0.021097773269700682
Testing accuracy  of updater 6 on alg 0 with rate 8.0 = 0.9831111111111112, training accuracy 0.9988568162332095, time elapsed: 1528 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.333998250852078
Loss in iteration 3 : 0.12931272372263747
Loss in iteration 4 : 0.13844917644646296
Loss in iteration 5 : 0.13224057266737874
Loss in iteration 6 : 0.08956016133349819
Loss in iteration 7 : 0.05960900065528943
Loss in iteration 8 : 0.04682824326129163
Loss in iteration 9 : 0.04493871210568846
Loss in iteration 10 : 0.044701594213256444
Loss in iteration 11 : 0.041024311219101335
Loss in iteration 12 : 0.034439501224119956
Loss in iteration 13 : 0.027521059197165798
Loss in iteration 14 : 0.022065120211597806
Loss in iteration 15 : 0.018539895073523036
Loss in iteration 16 : 0.016635256463582715
Loss in iteration 17 : 0.01581178478790452
Loss in iteration 18 : 0.015561634842757154
Loss in iteration 19 : 0.015490797478248462
Loss in iteration 20 : 0.015338440442324401
Loss in iteration 21 : 0.014971872310603256
Loss in iteration 22 : 0.014364911494703855
Loss in iteration 23 : 0.013564959566554154
Loss in iteration 24 : 0.01265656945712799
Loss in iteration 25 : 0.011729772178535556
Loss in iteration 26 : 0.010858651425011707
Loss in iteration 27 : 0.010091388713660167
Loss in iteration 28 : 0.009449596320053162
Loss in iteration 29 : 0.008933285461697481
Loss in iteration 30 : 0.00852801713183535
Loss in iteration 31 : 0.00821183661967105
Loss in iteration 32 : 0.007960767571120462
Loss in iteration 33 : 0.007752532750378558
Loss in iteration 34 : 0.00756867396936788
Loss in iteration 35 : 0.007395436187620524
Loss in iteration 36 : 0.007223790860540623
Loss in iteration 37 : 0.007048912462323007
Loss in iteration 38 : 0.006869352224877959
Loss in iteration 39 : 0.006686094754948434
Loss in iteration 40 : 0.006501635196350729
Loss in iteration 41 : 0.006319171643769574
Loss in iteration 42 : 0.00614196751477751
Loss in iteration 43 : 0.0059729031389738185
Loss in iteration 44 : 0.005814207814641178
Loss in iteration 45 : 0.005667344987452118
Loss in iteration 46 : 0.005533014109206311
Loss in iteration 47 : 0.005411231511665109
Loss in iteration 48 : 0.005301456727334336
Loss in iteration 49 : 0.00520273748408164
Loss in iteration 50 : 0.005113854007726963
Loss in iteration 51 : 0.005033449975612588
Loss in iteration 52 : 0.004960142881844596
Loss in iteration 53 : 0.0048926106124143245
Loss in iteration 54 : 0.004829653862860585
Loss in iteration 55 : 0.004770235923676341
Loss in iteration 56 : 0.004713502551026191
Loss in iteration 57 : 0.00465878531668888
Loss in iteration 58 : 0.004605592119400111
Loss in iteration 59 : 0.004553588529028769
Loss in iteration 60 : 0.004502573393984344
Loss in iteration 61 : 0.0044524517318922235
Loss in iteration 62 : 0.0044032074026032755
Loss in iteration 63 : 0.004354877487469562
Loss in iteration 64 : 0.004307529720732847
Loss in iteration 65 : 0.004261243780243128
Loss in iteration 66 : 0.004216096776560779
Loss in iteration 67 : 0.004172152900470631
Loss in iteration 68 : 0.004129456906128337
Loss in iteration 69 : 0.0040880309179225135
Loss in iteration 70 : 0.0040478739441368984
Loss in iteration 71 : 0.004008963445816764
Loss in iteration 72 : 0.003971258329218424
Loss in iteration 73 : 0.003934702789199987
Loss in iteration 74 : 0.003899230514587879
Loss in iteration 75 : 0.0038647688626957228
Loss in iteration 76 : 0.003831242708976637
Loss in iteration 77 : 0.0037985777718835766
Loss in iteration 78 : 0.0037667032972909112
Loss in iteration 79 : 0.003735554058162361
Loss in iteration 80 : 0.00370507168205481
Loss in iteration 81 : 0.003675205361348795
Loss in iteration 82 : 0.0036459120296027463
Loss in iteration 83 : 0.0036171561036346484
Testing accuracy  of updater 6 on alg 0 with rate 0.8 = 0.9831111111111112, training accuracy 0.999285510145756, time elapsed: 1369 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.535142602803342
Loss in iteration 3 : 0.41189928285476035
Loss in iteration 4 : 0.32708045390140933
Loss in iteration 5 : 0.27070657153719707
Loss in iteration 6 : 0.23241068514699942
Loss in iteration 7 : 0.2048862678629745
Loss in iteration 8 : 0.18390021271425452
Loss in iteration 9 : 0.16715167023485392
Loss in iteration 10 : 0.1533589325753414
Loss in iteration 11 : 0.141754186774478
Loss in iteration 12 : 0.13183800155975695
Loss in iteration 13 : 0.12326348261538149
Loss in iteration 14 : 0.11577904357696947
Loss in iteration 15 : 0.10919697810761199
Loss in iteration 16 : 0.10337402044082399
Loss in iteration 17 : 0.0981983116607739
Loss in iteration 18 : 0.09358038976475454
Loss in iteration 19 : 0.08944697358603342
Loss in iteration 20 : 0.0857367373800884
Loss in iteration 21 : 0.08239747899533535
Loss in iteration 22 : 0.07938422917694031
Loss in iteration 23 : 0.07665796978764253
Loss in iteration 24 : 0.07418472858031437
Loss in iteration 25 : 0.0719348970312708
Loss in iteration 26 : 0.06988267663460655
Loss in iteration 27 : 0.06800560050673657
Loss in iteration 28 : 0.06628410434756969
Loss in iteration 29 : 0.06470113701048846
Loss in iteration 30 : 0.06324180918750304
Loss in iteration 31 : 0.06189308167720749
Loss in iteration 32 : 0.06064349454601585
Loss in iteration 33 : 0.05948293685953401
Loss in iteration 34 : 0.05840245467213308
Loss in iteration 35 : 0.05739409328429081
Loss in iteration 36 : 0.05645076871645335
Loss in iteration 37 : 0.055566162961973684
Loss in iteration 38 : 0.05473463777575986
Loss in iteration 39 : 0.05395116236376266
Loss in iteration 40 : 0.05321125118092535
Loss in iteration 41 : 0.05251090896188963
Loss in iteration 42 : 0.05184658097794321
Loss in iteration 43 : 0.051215107256684184
Loss in iteration 44 : 0.05061368007841285
Loss in iteration 45 : 0.05003980446778854
Loss in iteration 46 : 0.0494912616455698
Loss in iteration 47 : 0.0489660755213234
Loss in iteration 48 : 0.0484624823274863
Loss in iteration 49 : 0.047978903451625206
Loss in iteration 50 : 0.04751392144668202
Loss in iteration 51 : 0.04706625911193714
Loss in iteration 52 : 0.046634761457122274
Loss in iteration 53 : 0.04621838029890138
Loss in iteration 54 : 0.04581616119767383
Loss in iteration 55 : 0.04542723242394186
Loss in iteration 56 : 0.045050795645010584
Loss in iteration 57 : 0.04468611804050984
Loss in iteration 58 : 0.0443325255843806
Loss in iteration 59 : 0.043989397266823674
Loss in iteration 60 : 0.043656160068077585
Loss in iteration 61 : 0.04333228453341867
Loss in iteration 62 : 0.043017280833037685
Loss in iteration 63 : 0.042710695219943376
Loss in iteration 64 : 0.04241210682307271
Loss in iteration 65 : 0.042121124731288005
Loss in iteration 66 : 0.04183738533733043
Loss in iteration 67 : 0.04156054991978382
Loss in iteration 68 : 0.04129030244653811
Loss in iteration 69 : 0.04102634758602461
Loss in iteration 70 : 0.040768408913455345
Loss in iteration 71 : 0.040516227299160976
Loss in iteration 72 : 0.04026955946546559
Loss in iteration 73 : 0.040028176697792996
Loss in iteration 74 : 0.03979186369515752
Loss in iteration 75 : 0.039560417545011245
Loss in iteration 76 : 0.03933364680767091
Loss in iteration 77 : 0.039111370696209936
Loss in iteration 78 : 0.03889341833871777
Loss in iteration 79 : 0.038679628111107124
Loss in iteration 80 : 0.03846984703008826
Loss in iteration 81 : 0.038263930197428374
Loss in iteration 82 : 0.03806174028808721
Loss in iteration 83 : 0.037863147076199304
Loss in iteration 84 : 0.03766802699410445
Loss in iteration 85 : 0.0374762627206892
Loss in iteration 86 : 0.037287742796169025
Loss in iteration 87 : 0.037102361261128265
Loss in iteration 88 : 0.03692001731814687
Loss in iteration 89 : 0.03674061501470365
Loss in iteration 90 : 0.036564062946281096
Loss in iteration 91 : 0.03639027397873343
Loss in iteration 92 : 0.03621916498904008
Loss in iteration 93 : 0.036050656623580535
Loss in iteration 94 : 0.035884673073050356
Loss in iteration 95 : 0.03572114186310762
Loss in iteration 96 : 0.03555999365981014
Loss in iteration 97 : 0.03540116208888247
Loss in iteration 98 : 0.03524458356784261
Loss in iteration 99 : 0.03509019715002812
Loss in iteration 100 : 0.034937944379582225
Loss in iteration 101 : 0.03478776915650178
Loss in iteration 102 : 0.03463961761089632
Loss in iteration 103 : 0.03449343798566792
Loss in iteration 104 : 0.03434918052688587
Loss in iteration 105 : 0.034206797381196086
Loss in iteration 106 : 0.03406624249967288
Loss in iteration 107 : 0.03392747154758414
Loss in iteration 108 : 0.0337904418196011
Loss in iteration 109 : 0.03365511216003742
Loss in iteration 110 : 0.033521442887751635
Loss in iteration 111 : 0.03338939572538768
Loss in iteration 112 : 0.033258933732664274
Loss in iteration 113 : 0.033130021243454814
Loss in iteration 114 : 0.03300262380642154
Loss in iteration 115 : 0.032876708128990614
Loss in iteration 116 : 0.03275224202446977
Loss in iteration 117 : 0.03262919436212355
Loss in iteration 118 : 0.032507535020033035
Loss in iteration 119 : 0.03238723484057549
Loss in iteration 120 : 0.03226826558836876
Loss in iteration 121 : 0.03215059991053137
Loss in iteration 122 : 0.032034211299118355
Loss in iteration 123 : 0.03191907405559648
Loss in iteration 124 : 0.03180516325723354
Loss in iteration 125 : 0.031692454725279186
Loss in iteration 126 : 0.03158092499482392
Loss in iteration 127 : 0.03147055128622919
Loss in iteration 128 : 0.03136131147802749
Loss in iteration 129 : 0.03125318408119925
Loss in iteration 130 : 0.031146148214738465
Loss in iteration 131 : 0.03104018358242658
Loss in iteration 132 : 0.030935270450738968
Loss in iteration 133 : 0.030831389627814853
Loss in iteration 134 : 0.03072852244342603
Loss in iteration 135 : 0.03062665072988577
Loss in iteration 136 : 0.03052575680384234
Loss in iteration 137 : 0.03042582344890722
Loss in iteration 138 : 0.030326833899070395
Loss in iteration 139 : 0.03022877182285992
Loss in iteration 140 : 0.030131621308204928
Loss in iteration 141 : 0.03003536684796476
Loss in iteration 142 : 0.029939993326088585
Loss in iteration 143 : 0.029845486004373322
Loss in iteration 144 : 0.029751830509788295
Loss in iteration 145 : 0.029659012822338335
Loss in iteration 146 : 0.029567019263437674
Loss in iteration 147 : 0.029475836484768696
Loss in iteration 148 : 0.029385451457602223
Loss in iteration 149 : 0.029295851462555103
Loss in iteration 150 : 0.02920702407976453
Loss in iteration 151 : 0.029118957179458154
Loss in iteration 152 : 0.029031638912900583
Loss in iteration 153 : 0.02894505770369843
Loss in iteration 154 : 0.028859202239445648
Loss in iteration 155 : 0.02877406146369387
Loss in iteration 156 : 0.028689624568231986
Loss in iteration 157 : 0.02860588098565957
Loss in iteration 158 : 0.028522820382241604
Loss in iteration 159 : 0.028440432651030426
Loss in iteration 160 : 0.02835870790524327
Loss in iteration 161 : 0.028277636471883392
Loss in iteration 162 : 0.028197208885593652
Loss in iteration 163 : 0.028117415882733074
Loss in iteration 164 : 0.028038248395664895
Loss in iteration 165 : 0.027959697547248478
Loss in iteration 166 : 0.02788175464552554
Loss in iteration 167 : 0.027804411178591965
Loss in iteration 168 : 0.027727658809648024
Loss in iteration 169 : 0.027651489372219222
Loss in iteration 170 : 0.02757589486554035
Loss in iteration 171 : 0.027500867450096445
Loss in iteration 172 : 0.027426399443314126
Loss in iteration 173 : 0.027352483315396765
Loss in iteration 174 : 0.02727911168529824
Loss in iteration 175 : 0.027206277316829338
Loss in iteration 176 : 0.027133973114891814
Loss in iteration 177 : 0.027062192121834812
Loss in iteration 178 : 0.02699092751392908
Loss in iteration 179 : 0.02692017259795402
Loss in iteration 180 : 0.026849920807893744
Loss in iteration 181 : 0.026780165701737385
Loss in iteration 182 : 0.026710900958380353
Loss in iteration 183 : 0.02664212037462192
Loss in iteration 184 : 0.026573817862256155
Loss in iteration 185 : 0.026505987445252316
Loss in iteration 186 : 0.026438623257021565
Loss in iteration 187 : 0.02637171953776727
Loss in iteration 188 : 0.0263052706319142
Loss in iteration 189 : 0.02623927098561619
Loss in iteration 190 : 0.026173715144337365
Loss in iteration 191 : 0.026108597750505252
Loss in iteration 192 : 0.026043913541233078
Loss in iteration 193 : 0.025979657346108583
Loss in iteration 194 : 0.025915824085047414
Loss in iteration 195 : 0.025852408766208392
Loss in iteration 196 : 0.02578940648396873
Loss in iteration 197 : 0.02572681241695726
Loss in iteration 198 : 0.025664621826143232
Loss in iteration 199 : 0.025602830052979513
Loss in iteration 200 : 0.02554143251759737
Testing accuracy  of updater 6 on alg 0 with rate 0.07999999999999996 = 0.9733333333333334, training accuracy 0.997713632466419, time elapsed: 3359 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 12.648730475199674
Loss in iteration 3 : 4.0241112159188495
Loss in iteration 4 : 1.4873468158566692
Loss in iteration 5 : 2.842319918722274
Loss in iteration 6 : 3.4558391104281645
Loss in iteration 7 : 2.8747964369804406
Loss in iteration 8 : 2.0728229366269746
Loss in iteration 9 : 1.4600237786675814
Loss in iteration 10 : 1.0269316644520234
Loss in iteration 11 : 0.7331120693263644
Loss in iteration 12 : 0.5523851585656638
Loss in iteration 13 : 0.47117283250160324
Loss in iteration 14 : 0.46323340282681263
Loss in iteration 15 : 0.4859226908013028
Loss in iteration 16 : 0.511513600620139
Loss in iteration 17 : 0.5286070101205264
Loss in iteration 18 : 0.5267996584358497
Loss in iteration 19 : 0.5067276215541582
Loss in iteration 20 : 0.4719548444975826
Loss in iteration 21 : 0.4290822026488733
Loss in iteration 22 : 0.3818096771380983
Loss in iteration 23 : 0.3339173904002106
Loss in iteration 24 : 0.29156512780678023
Loss in iteration 25 : 0.25408743802655254
Loss in iteration 26 : 0.2195260177458724
Loss in iteration 27 : 0.19329712897504855
Loss in iteration 28 : 0.17404589107574744
Loss in iteration 29 : 0.1591657946182296
Loss in iteration 30 : 0.1476385659828743
Loss in iteration 31 : 0.139297408258372
Loss in iteration 32 : 0.1335000153603474
Loss in iteration 33 : 0.12936069736414918
Loss in iteration 34 : 0.12630630760443218
Loss in iteration 35 : 0.12405969181111795
Loss in iteration 36 : 0.12214848711601754
Loss in iteration 37 : 0.1204702816955265
Loss in iteration 38 : 0.11881482010594198
Loss in iteration 39 : 0.11704494295886038
Loss in iteration 40 : 0.11514486313069662
Loss in iteration 41 : 0.11314570608352954
Loss in iteration 42 : 0.1110724316175779
Loss in iteration 43 : 0.10894163596133569
Loss in iteration 44 : 0.10677525067266957
Loss in iteration 45 : 0.1046003807310107
Loss in iteration 46 : 0.1024406595226429
Loss in iteration 47 : 0.10030865369128489
Loss in iteration 48 : 0.09820530208799902
Loss in iteration 49 : 0.09612618373750705
Loss in iteration 50 : 0.09406758137243354
Loss in iteration 51 : 0.09202901558945902
Loss in iteration 52 : 0.0900145483302639
Loss in iteration 53 : 0.08803550979292932
Loss in iteration 54 : 0.08611396773393792
Loss in iteration 55 : 0.08427942610883828
Loss in iteration 56 : 0.08255366666766242
Loss in iteration 57 : 0.08094302042967501
Loss in iteration 58 : 0.07944559458565517
Loss in iteration 59 : 0.07805146618890701
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.9715555555555555, training accuracy 0.9964275507287796, time elapsed: 1008 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.36940791685006336
Loss in iteration 3 : 0.19139619839737693
Loss in iteration 4 : 0.15676180712530116
Loss in iteration 5 : 0.12502178051009033
Loss in iteration 6 : 0.09444391740515656
Loss in iteration 7 : 0.07541936130019272
Loss in iteration 8 : 0.06391594287616365
Loss in iteration 9 : 0.05536654868847566
Loss in iteration 10 : 0.047912020963676104
Loss in iteration 11 : 0.041349561068495015
Loss in iteration 12 : 0.03591771518821984
Loss in iteration 13 : 0.03170805155886359
Loss in iteration 14 : 0.02858305784711473
Loss in iteration 15 : 0.026287770248656937
Loss in iteration 16 : 0.024560741098537453
Loss in iteration 17 : 0.0231916818454112
Loss in iteration 18 : 0.02203631599814433
Loss in iteration 19 : 0.021008888079668595
Loss in iteration 20 : 0.020066547813496132
Loss in iteration 21 : 0.01919323296984214
Loss in iteration 22 : 0.018386518536203877
Loss in iteration 23 : 0.017648527507891566
Loss in iteration 24 : 0.01698068442543871
Loss in iteration 25 : 0.016381462435044326
Loss in iteration 26 : 0.015846106081766877
Loss in iteration 27 : 0.015367420619295639
Loss in iteration 28 : 0.014936950174765748
Loss in iteration 29 : 0.014546115137203273
Loss in iteration 30 : 0.01418708703018306
Loss in iteration 31 : 0.01385332786336224
Loss in iteration 32 : 0.01353981314399114
Loss in iteration 33 : 0.013243005070313311
Loss in iteration 34 : 0.012960658326860705
Loss in iteration 35 : 0.012691537115939676
Loss in iteration 36 : 0.012435107702594551
Loss in iteration 37 : 0.012191252331188732
Loss in iteration 38 : 0.01196003217636805
Loss in iteration 39 : 0.01174151151737097
Loss in iteration 40 : 0.011535643701033932
Loss in iteration 41 : 0.011342211839097125
Loss in iteration 42 : 0.011160813108092868
Loss in iteration 43 : 0.010990874213714606
Loss in iteration 44 : 0.010831686196364957
Loss in iteration 45 : 0.010682448514145045
Loss in iteration 46 : 0.010542314619653376
Loss in iteration 47 : 0.010410433598730779
Loss in iteration 48 : 0.010285984576691783
Loss in iteration 49 : 0.010168202367180398
Loss in iteration 50 : 0.010056394184551707
Loss in iteration 51 : 0.009949948171569599
Loss in iteration 52 : 0.009848335057857222
Loss in iteration 53 : 0.00975110452770373
Loss in iteration 54 : 0.009657877910899646
Loss in iteration 55 : 0.0095683386865091
Loss in iteration 56 : 0.009482222067882224
Loss in iteration 57 : 0.009399304667892367
Loss in iteration 58 : 0.009319394964819903
Loss in iteration 59 : 0.009242325028742988
Loss in iteration 60 : 0.00916794374298049
Loss in iteration 61 : 0.009096111573969258
Loss in iteration 62 : 0.009026696808354292
Loss in iteration 63 : 0.008959573085744335
Loss in iteration 64 : 0.008894618004209901
Loss in iteration 65 : 0.008831712556186365
Loss in iteration 66 : 0.008770741157434175
Loss in iteration 67 : 0.008711592053774907
Loss in iteration 68 : 0.008654157922900202
Loss in iteration 69 : 0.00859833652613923
Loss in iteration 70 : 0.008544031303356552
Loss in iteration 71 : 0.00849115184000205
Loss in iteration 72 : 0.008439614166690262
Loss in iteration 73 : 0.008389340877418211
Loss in iteration 74 : 0.008340261072249588
Loss in iteration 75 : 0.00829231014417332
Loss in iteration 76 : 0.00824542943844792
Loss in iteration 77 : 0.008199565816867099
Loss in iteration 78 : 0.008154671159936766
Loss in iteration 79 : 0.008110701837859082
Loss in iteration 80 : 0.00806761817733882
Loss in iteration 81 : 0.008025383946316994
Loss in iteration 82 : 0.007983965873424119
Loss in iteration 83 : 0.007943333213713166
Loss in iteration 84 : 0.007903457367432924
Loss in iteration 85 : 0.007864311554456884
Loss in iteration 86 : 0.00782587054361498
Loss in iteration 87 : 0.0077881104336146745
Loss in iteration 88 : 0.007751008480456819
Loss in iteration 89 : 0.007714542965173244
Loss in iteration 90 : 0.007678693095234166
Loss in iteration 91 : 0.007643438932978883
Loss in iteration 92 : 0.007608761344794013
Loss in iteration 93 : 0.0075746419653887785
Loss in iteration 94 : 0.007541063172297336
Loss in iteration 95 : 0.007508008066590183
Loss in iteration 96 : 0.007475460456632508
Loss in iteration 97 : 0.007443404842538032
Loss in iteration 98 : 0.00741182639969611
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.9848888888888889, training accuracy 0.999285510145756, time elapsed: 1843 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6263844603792799
Loss in iteration 3 : 0.5526934763070834
Loss in iteration 4 : 0.48432778691079353
Loss in iteration 5 : 0.42531770514345857
Loss in iteration 6 : 0.37632218503144255
Loss in iteration 7 : 0.33650697078661723
Loss in iteration 8 : 0.30441245036937103
Loss in iteration 9 : 0.278464095109217
Loss in iteration 10 : 0.2572557514468516
Loss in iteration 11 : 0.2396557568437759
Loss in iteration 12 : 0.2248058005197158
Loss in iteration 13 : 0.2120746199303175
Loss in iteration 14 : 0.20100325752341314
Loss in iteration 15 : 0.19125748229092998
Loss in iteration 16 : 0.18259132650364868
Loss in iteration 17 : 0.17482083894517392
Loss in iteration 18 : 0.16780578641888091
Loss in iteration 19 : 0.16143707617438324
Loss in iteration 20 : 0.15562813341335532
Loss in iteration 21 : 0.15030895801369556
Loss in iteration 22 : 0.14542198197503717
Loss in iteration 23 : 0.14091913835718645
Loss in iteration 24 : 0.13675975167540338
Loss in iteration 25 : 0.13290899258115257
Loss in iteration 26 : 0.1293367266436447
Loss in iteration 27 : 0.12601664345170363
Loss in iteration 28 : 0.12292558875161666
Loss in iteration 29 : 0.12004304607271493
Loss in iteration 30 : 0.11735072992307324
Loss in iteration 31 : 0.11483226313228212
Loss in iteration 32 : 0.1124729181420751
Loss in iteration 33 : 0.11025940715782367
Loss in iteration 34 : 0.10817970979110698
Loss in iteration 35 : 0.10622292958949166
Loss in iteration 36 : 0.1043791729426807
Loss in iteration 37 : 0.10263944545323353
Loss in iteration 38 : 0.10099556208488454
Loss in iteration 39 : 0.09944006833631835
Loss in iteration 40 : 0.09796617039552007
Loss in iteration 41 : 0.0965676727581015
Loss in iteration 42 : 0.0952389221812365
Loss in iteration 43 : 0.09397475712487639
Loss in iteration 44 : 0.09277046202984875
Loss in iteration 45 : 0.09162172591947264
Loss in iteration 46 : 0.09052460490449181
Loss in iteration 47 : 0.08947548823372173
Loss in iteration 48 : 0.08847106757503254
Loss in iteration 49 : 0.08750830924063067
Loss in iteration 50 : 0.08658442909239977
Loss in iteration 51 : 0.0856968698808879
Loss in iteration 52 : 0.08484328078764462
Loss in iteration 53 : 0.0840214989562821
Loss in iteration 54 : 0.0832295328134201
Loss in iteration 55 : 0.08246554699672977
Loss in iteration 56 : 0.08172784872342488
Loss in iteration 57 : 0.08101487544853232
Loss in iteration 58 : 0.08032518367774204
Loss in iteration 59 : 0.07965743881433773
Loss in iteration 60 : 0.07901040593337354
Loss in iteration 61 : 0.07838294138875194
Loss in iteration 62 : 0.07777398517007436
Loss in iteration 63 : 0.07718255393605732
Loss in iteration 64 : 0.07660773465998262
Loss in iteration 65 : 0.07604867883014425
Loss in iteration 66 : 0.07550459715468835
Loss in iteration 67 : 0.07497475472573065
Loss in iteration 68 : 0.07445846660231255
Loss in iteration 69 : 0.07395509377574447
Loss in iteration 70 : 0.0734640394843078
Loss in iteration 71 : 0.07298474584724005
Loss in iteration 72 : 0.07251669079051075
Loss in iteration 73 : 0.07205938523918021
Loss in iteration 74 : 0.07161237055317946
Loss in iteration 75 : 0.071175216185209
Loss in iteration 76 : 0.07074751754115571
Loss in iteration 77 : 0.07032889402500934
Loss in iteration 78 : 0.06991898725172221
Loss in iteration 79 : 0.06951745941283989
Loss in iteration 80 : 0.06912399178100338
Loss in iteration 81 : 0.06873828334063943
Loss in iteration 82 : 0.0683600495332695
Loss in iteration 83 : 0.06798902110691848
Loss in iteration 84 : 0.0676249430600761
Loss in iteration 85 : 0.06726757367155463
Loss in iteration 86 : 0.06691668360841688
Loss in iteration 87 : 0.066572055104898
Loss in iteration 88 : 0.06623348120593583
Loss in iteration 89 : 0.06590076506954311
Loss in iteration 90 : 0.06557371932282668
Loss in iteration 91 : 0.06525216546695893
Loss in iteration 92 : 0.06493593332687073
Loss in iteration 93 : 0.06462486054184119
Loss in iteration 94 : 0.06431879209352694
Loss in iteration 95 : 0.0640175798683038
Loss in iteration 96 : 0.06372108225108446
Loss in iteration 97 : 0.06342916374804172
Loss in iteration 98 : 0.06314169463589732
Loss in iteration 99 : 0.06285855063565153
Loss in iteration 100 : 0.06257961260881312
Loss in iteration 101 : 0.06230476627436245
Loss in iteration 102 : 0.062033901944831146
Loss in iteration 103 : 0.061766914280019826
Loss in iteration 104 : 0.06150370205700249
Loss in iteration 105 : 0.06124416795517653
Loss in iteration 106 : 0.06098821835522038
Loss in iteration 107 : 0.060735763150918935
Loss in iteration 108 : 0.060486715572895096
Loss in iteration 109 : 0.060240992023369966
Loss in iteration 110 : 0.05999851192114242
Loss in iteration 111 : 0.059759197556045064
Loss in iteration 112 : 0.059522973952192
Loss in iteration 113 : 0.059289768739392894
Loss in iteration 114 : 0.05905951203215006
Loss in iteration 115 : 0.05883213631571046
Loss in iteration 116 : 0.058607576338679995
Loss in iteration 117 : 0.05838576901174908
Loss in iteration 118 : 0.058166653312112494
Loss in iteration 119 : 0.057950170193201034
Loss in iteration 120 : 0.057736262499369716
Loss in iteration 121 : 0.05752487488521528
Loss in iteration 122 : 0.05731595373922254
Loss in iteration 123 : 0.057109447111458586
Loss in iteration 124 : 0.05690530464505825
Loss in iteration 125 : 0.05670347751125997
Loss in iteration 126 : 0.05650391834777218
Loss in iteration 127 : 0.05630658120026376
Loss in iteration 128 : 0.05611142146678787
Loss in iteration 129 : 0.05591839584496305
Loss in iteration 130 : 0.055727462281745875
Loss in iteration 131 : 0.05553857992564242
Loss in iteration 132 : 0.05535170908121605
Loss in iteration 133 : 0.0551668111657584
Loss in iteration 134 : 0.0549838486680005
Loss in iteration 135 : 0.05480278510874595
Loss in iteration 136 : 0.05462358500332197
Loss in iteration 137 : 0.05444621382574322
Loss in iteration 138 : 0.0542706379744972
Loss in iteration 139 : 0.05409682473986119
Loss in iteration 140 : 0.05392474227266772
Loss in iteration 141 : 0.053754359554442785
Loss in iteration 142 : 0.05358564636884205
Loss in iteration 143 : 0.05341857327431795
Loss in iteration 144 : 0.05325311157795315
Loss in iteration 145 : 0.05308923331040126
Loss in iteration 146 : 0.052926911201875954
Loss in iteration 147 : 0.05276611865913859
Loss in iteration 148 : 0.052606829743430845
Loss in iteration 149 : 0.05244901914930869
Loss in iteration 150 : 0.05229266218432998
Loss in iteration 151 : 0.05213773474955712
Loss in iteration 152 : 0.0519842133208333
Loss in iteration 153 : 0.05183207493079532
Loss in iteration 154 : 0.05168129715158962
Loss in iteration 155 : 0.05153185807825613
Loss in iteration 156 : 0.05138373631275056
Loss in iteration 157 : 0.051236910948574375
Loss in iteration 158 : 0.05109136155598597
Loss in iteration 159 : 0.05094706816776397
Loss in iteration 160 : 0.05080401126550129
Loss in iteration 161 : 0.050662171766403044
Loss in iteration 162 : 0.05052153101056813
Loss in iteration 163 : 0.05038207074873109
Loss in iteration 164 : 0.05024377313044736
Loss in iteration 165 : 0.050106620692698
Loss in iteration 166 : 0.04997059634890159
Loss in iteration 167 : 0.04983568337831015
Loss in iteration 168 : 0.049701865415777896
Loss in iteration 169 : 0.04956912644188333
Loss in iteration 170 : 0.04943745077339233
Loss in iteration 171 : 0.04930682305404721
Loss in iteration 172 : 0.049177228245668295
Loss in iteration 173 : 0.04904865161955575
Loss in iteration 174 : 0.048921078748179254
Loss in iteration 175 : 0.04879449549714392
Loss in iteration 176 : 0.04866888801742149
Loss in iteration 177 : 0.048544242737836404
Loss in iteration 178 : 0.04842054635779665
Loss in iteration 179 : 0.048297785840259885
Loss in iteration 180 : 0.04817594840492611
Loss in iteration 181 : 0.048055021521646646
Loss in iteration 182 : 0.04793499290404358
Loss in iteration 183 : 0.0478158505033298
Loss in iteration 184 : 0.04769758250232237
Loss in iteration 185 : 0.04758017730964278
Loss in iteration 186 : 0.04746362355409644
Loss in iteration 187 : 0.0473479100792249
Loss in iteration 188 : 0.04723302593802463
Loss in iteration 189 : 0.047118960387825856
Loss in iteration 190 : 0.0470057028853268
Loss in iteration 191 : 0.04689324308177585
Loss in iteration 192 : 0.046781570818297924
Loss in iteration 193 : 0.04667067612135976
Loss in iteration 194 : 0.04656054919836786
Loss in iteration 195 : 0.04645118043339645
Loss in iteration 196 : 0.04634256038303913
Loss in iteration 197 : 0.0462346797723812
Loss in iteration 198 : 0.04612752949108708
Loss in iteration 199 : 0.04602110058960117
Loss in iteration 200 : 0.045915384275455426
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.9777777777777777, training accuracy 0.9952843669619892, time elapsed: 3460 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 11.667191196957885
Loss in iteration 3 : 1.4732812683076952
Loss in iteration 4 : 1.2000037823182836
Loss in iteration 5 : 1.276914469635924
Loss in iteration 6 : 1.1652050753470207
Loss in iteration 7 : 0.961212221448147
Loss in iteration 8 : 0.7523236045500009
Loss in iteration 9 : 0.5790113115149018
Loss in iteration 10 : 0.43284436451740904
Loss in iteration 11 : 0.33373616095753833
Loss in iteration 12 : 0.270843050336355
Loss in iteration 13 : 0.2431955894494156
Loss in iteration 14 : 0.23055744079845839
Loss in iteration 15 : 0.21998883542661282
Loss in iteration 16 : 0.20742563596652108
Loss in iteration 17 : 0.1939347493822655
Loss in iteration 18 : 0.17707877688562604
Loss in iteration 19 : 0.15803713719621812
Loss in iteration 20 : 0.13952154612361625
Loss in iteration 21 : 0.12377014037609156
Loss in iteration 22 : 0.11006068722005016
Loss in iteration 23 : 0.09886480170155955
Loss in iteration 24 : 0.08988285323383587
Loss in iteration 25 : 0.08190535414578302
Loss in iteration 26 : 0.07531093495443432
Loss in iteration 27 : 0.07041042934389155
Loss in iteration 28 : 0.06698488006012422
Loss in iteration 29 : 0.06457590151197008
Loss in iteration 30 : 0.06238962013393211
Loss in iteration 31 : 0.06019250485907865
Loss in iteration 32 : 0.05796904662604949
Loss in iteration 33 : 0.05579977437810197
Loss in iteration 34 : 0.05376130458857404
Loss in iteration 35 : 0.05187052635630473
Loss in iteration 36 : 0.05013047736432959
Loss in iteration 37 : 0.04853936898679482
Loss in iteration 38 : 0.04709068048962129
Loss in iteration 39 : 0.04577798769211107
Loss in iteration 40 : 0.04459703167153858
Loss in iteration 41 : 0.04354026921029266
Loss in iteration 42 : 0.042590323868453794
Loss in iteration 43 : 0.04172655307659382
Loss in iteration 44 : 0.040933963854265856
Loss in iteration 45 : 0.04020173867378141
Loss in iteration 46 : 0.03951850494372968
Loss in iteration 47 : 0.03887211838136784
Loss in iteration 48 : 0.03825215229411919
Loss in iteration 49 : 0.03765116677897334
Loss in iteration 50 : 0.037064356354092685
Loss in iteration 51 : 0.036488695365594584
Loss in iteration 52 : 0.035922222842276814
Loss in iteration 53 : 0.03536358540322842
Loss in iteration 54 : 0.034811778882026335
Loss in iteration 55 : 0.03426601401233991
Loss in iteration 56 : 0.03372565384595691
Loss in iteration 57 : 0.03319019201908162
Loss in iteration 58 : 0.03265925553469235
Loss in iteration 59 : 0.03213262472253849
Loss in iteration 60 : 0.031610268004851866
Loss in iteration 61 : 0.031092390212840715
Loss in iteration 62 : 0.030579488960703363
Loss in iteration 63 : 0.030072401403380876
Loss in iteration 64 : 0.029572303746902107
Loss in iteration 65 : 0.029080610855140066
Loss in iteration 66 : 0.028598746457292403
Loss in iteration 67 : 0.028127843431735505
Loss in iteration 68 : 0.02766853697803096
Loss in iteration 69 : 0.02722099798208297
Loss in iteration 70 : 0.02678518641723079
Loss in iteration 71 : 0.026361152823161066
Loss in iteration 72 : 0.025949218025855973
Loss in iteration 73 : 0.025549957045665234
Loss in iteration 74 : 0.025163999484922886
Loss in iteration 75 : 0.02479172634940411
Loss in iteration 76 : 0.02443300617054043
Loss in iteration 77 : 0.02408711670713707
Loss in iteration 78 : 0.023752888934647148
Loss in iteration 79 : 0.02342896718892747
Loss in iteration 80 : 0.02311403963930993
Loss in iteration 81 : 0.0228069638703154
Loss in iteration 82 : 0.02250679494418585
Loss in iteration 83 : 0.022212757891668856
Loss in iteration 84 : 0.021924202291379057
Loss in iteration 85 : 0.021640560599286766
Loss in iteration 86 : 0.02136131879524934
Loss in iteration 87 : 0.02108600045593077
Loss in iteration 88 : 0.020814161977202636
Loss in iteration 89 : 0.020545395547802343
Loss in iteration 90 : 0.020279336501890167
Loss in iteration 91 : 0.020015672243932667
Testing accuracy  of updater 8 on alg 0 with rate 8.0 = 0.9857777777777778, training accuracy 0.9989997142040583, time elapsed: 1555 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6682848557365284
Loss in iteration 3 : 0.12239740947836461
Loss in iteration 4 : 0.11674772796337461
Loss in iteration 5 : 0.08970516528489898
Loss in iteration 6 : 0.06710467154088312
Loss in iteration 7 : 0.05108709132150233
Loss in iteration 8 : 0.04122620979445086
Loss in iteration 9 : 0.035187500334620085
Loss in iteration 10 : 0.03088275441580288
Loss in iteration 11 : 0.02739133144923824
Loss in iteration 12 : 0.024455055900773293
Loss in iteration 13 : 0.022010041776392323
Loss in iteration 14 : 0.02000662620993639
Loss in iteration 15 : 0.01837608395130325
Loss in iteration 16 : 0.01704059995749814
Loss in iteration 17 : 0.01592677499230522
Loss in iteration 18 : 0.014973524342004252
Loss in iteration 19 : 0.014134837023368445
Loss in iteration 20 : 0.013379209742488987
Loss in iteration 21 : 0.01268715521019687
Loss in iteration 22 : 0.01204787957978882
Loss in iteration 23 : 0.011456007176589386
Loss in iteration 24 : 0.010908923787698624
Loss in iteration 25 : 0.010404962035612695
Loss in iteration 26 : 0.009942381360603537
Loss in iteration 27 : 0.00951895266764384
Loss in iteration 28 : 0.009131925420186184
Loss in iteration 29 : 0.008778187562739261
Loss in iteration 30 : 0.008454485331377314
Loss in iteration 31 : 0.008157625291384765
Loss in iteration 32 : 0.00788462348014591
Loss in iteration 33 : 0.007632794140478988
Loss in iteration 34 : 0.0073997855168290785
Loss in iteration 35 : 0.007183576273380075
Loss in iteration 36 : 0.006982446778369658
Loss in iteration 37 : 0.00679493743717558
Loss in iteration 38 : 0.0066198031892054626
Loss in iteration 39 : 0.006455970208495982
Loss in iteration 40 : 0.006302498246543866
Loss in iteration 41 : 0.006158550089967011
Loss in iteration 42 : 0.006023368260966047
Loss in iteration 43 : 0.005896258268863378
Loss in iteration 44 : 0.00577657730348254
Loss in iteration 45 : 0.00566372712691957
Loss in iteration 46 : 0.005557149968443861
Loss in iteration 47 : 0.0054563263794404696
Loss in iteration 48 : 0.005360774204798353
Loss in iteration 49 : 0.005270048035823261
Loss in iteration 50 : 0.005183738704004756
Loss in iteration 51 : 0.005101472541893643
Loss in iteration 52 : 0.005022910271516662
Loss in iteration 53 : 0.004947745481654219
Loss in iteration 54 : 0.004875702725471954
Loss in iteration 55 : 0.004806535313641201
Loss in iteration 56 : 0.004740022900180376
Loss in iteration 57 : 0.004675968963867521
Loss in iteration 58 : 0.0046141982819779765
Loss in iteration 59 : 0.0045545544794776665
Loss in iteration 60 : 0.004496897719143761
Loss in iteration 61 : 0.004441102579119398
Loss in iteration 62 : 0.004387056146154023
Loss in iteration 63 : 0.004334656336574095
Loss in iteration 64 : 0.004283810443651632
Loss in iteration 65 : 0.004234433899807587
Loss in iteration 66 : 0.004186449234974067
Loss in iteration 67 : 0.004139785208170931
Loss in iteration 68 : 0.004094376087508024
Loss in iteration 69 : 0.00405016105390667
Loss in iteration 70 : 0.004007083705337326
Loss in iteration 71 : 0.0039650916408179105
Loss in iteration 72 : 0.0039241361063900365
Loss in iteration 73 : 0.003884171688451137
Loss in iteration 74 : 0.0038451560429011255
Loss in iteration 75 : 0.0038070496513854636
Loss in iteration 76 : 0.003769815598361936
Loss in iteration 77 : 0.0037334193647356245
Loss in iteration 78 : 0.0036978286353819842
Loss in iteration 79 : 0.0036630131190388228
Loss in iteration 80 : 0.0036289443798413665
Loss in iteration 81 : 0.0035955956802585097
Loss in iteration 82 : 0.003562941835429968
Loss in iteration 83 : 0.003530959078964501
Loss in iteration 84 : 0.003499624940197078
Loss in iteration 85 : 0.003468918132767878
Loss in iteration 86 : 0.003438818454216509
Loss in iteration 87 : 0.0034093066961131836
Loss in iteration 88 : 0.0033803645640943024
Loss in iteration 89 : 0.0033519746070480643
Testing accuracy  of updater 8 on alg 0 with rate 0.8 = 0.9831111111111112, training accuracy 0.9991426121749071, time elapsed: 1520 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.4233732639646418
Loss in iteration 3 : 0.30886432611216574
Loss in iteration 4 : 0.2514415853970715
Loss in iteration 5 : 0.21428586643596878
Loss in iteration 6 : 0.18758046800966566
Loss in iteration 7 : 0.16707952271905901
Loss in iteration 8 : 0.15072530671989007
Loss in iteration 9 : 0.13737598795157066
Loss in iteration 10 : 0.1263062594349715
Loss in iteration 11 : 0.11700841460913251
Loss in iteration 12 : 0.10910897296215485
Loss in iteration 13 : 0.10232794410145393
Loss in iteration 14 : 0.09645388778878657
Loss in iteration 15 : 0.09132594766305935
Loss in iteration 16 : 0.08682023202252727
Loss in iteration 17 : 0.08283962427580367
Loss in iteration 18 : 0.0793064086979969
Loss in iteration 19 : 0.07615710673142256
Loss in iteration 20 : 0.07333894600098183
Loss in iteration 21 : 0.07080746683045082
Loss in iteration 22 : 0.06852488148732376
Loss in iteration 23 : 0.06645891087519745
Loss in iteration 24 : 0.06458191570568048
Loss in iteration 25 : 0.06287020887336034
Loss in iteration 26 : 0.061303483831378576
Loss in iteration 27 : 0.05986432417908546
Loss in iteration 28 : 0.0585377771463203
Loss in iteration 29 : 0.05731098250735889
Loss in iteration 30 : 0.05617285210709282
Loss in iteration 31 : 0.05511379607848445
Loss in iteration 32 : 0.05412549155524436
Loss in iteration 33 : 0.05320068914868311
Loss in iteration 34 : 0.05233305210634153
Loss in iteration 35 : 0.05151702305060877
Loss in iteration 36 : 0.05074771349247609
Loss in iteration 37 : 0.05002081183964005
Loss in iteration 38 : 0.04933250626251524
Loss in iteration 39 : 0.048679419452118525
Loss in iteration 40 : 0.04805855293139541
Loss in iteration 41 : 0.047467239124854885
Loss in iteration 42 : 0.04690309983247193
Loss in iteration 43 : 0.04636401009231049
Loss in iteration 44 : 0.04584806666300445
Loss in iteration 45 : 0.04535356052861958
Loss in iteration 46 : 0.04487895294261164
Loss in iteration 47 : 0.04442285460154783
Loss in iteration 48 : 0.04398400758724314
Loss in iteration 49 : 0.04356126974894546
Loss in iteration 50 : 0.04315360122288811
Loss in iteration 51 : 0.04276005280974914
Loss in iteration 52 : 0.04237975595391502
Loss in iteration 53 : 0.042011914092877044
Loss in iteration 54 : 0.04165579517052916
Loss in iteration 55 : 0.04131072513390957
Loss in iteration 56 : 0.04097608225817152
Loss in iteration 57 : 0.040651292168440246
Loss in iteration 58 : 0.04033582344902145
Loss in iteration 59 : 0.040029183749737324
Loss in iteration 60 : 0.03973091631575207
Loss in iteration 61 : 0.03944059688111149
Loss in iteration 62 : 0.03915783087751717
Loss in iteration 63 : 0.0388822509188484
Loss in iteration 64 : 0.038613514528969224
Loss in iteration 65 : 0.03835130208575829
Loss in iteration 66 : 0.038095314958419196
Loss in iteration 67 : 0.0378452738182649
Loss in iteration 68 : 0.037600917105594635
Loss in iteration 69 : 0.037361999637190835
Loss in iteration 70 : 0.03712829134052744
Loss in iteration 71 : 0.03689957610211632
Loss in iteration 72 : 0.0366756507185948
Loss in iteration 73 : 0.03645632394023783
Loss in iteration 74 : 0.03624141559757923
Loss in iteration 75 : 0.03603075580276632
Loss in iteration 76 : 0.035824184218150694
Loss in iteration 77 : 0.03562154938543709
Loss in iteration 78 : 0.03542270810946214
Loss in iteration 79 : 0.03522752489135731
Loss in iteration 80 : 0.035035871406463165
Loss in iteration 81 : 0.03484762602290018
Loss in iteration 82 : 0.03466267335717709
Loss in iteration 83 : 0.03448090386362331
Loss in iteration 84 : 0.03430221345478692
Loss in iteration 85 : 0.034126503150235986
Loss in iteration 86 : 0.0339536787514603
Loss in iteration 87 : 0.033783650540786136
Loss in iteration 88 : 0.0336163330024066
Loss in iteration 89 : 0.033451644563791585
Loss in iteration 90 : 0.03328950735588392
Loss in iteration 91 : 0.03312984699061605
Loss in iteration 92 : 0.032972592354394356
Loss in iteration 93 : 0.03281767541630256
Loss in iteration 94 : 0.03266503104987204
Loss in iteration 95 : 0.03251459686735484
Loss in iteration 96 : 0.032366313065518834
Loss in iteration 97 : 0.03222012228206077
Loss in iteration 98 : 0.03207596946180767
Loss in iteration 99 : 0.031933801731941915
Loss in iteration 100 : 0.03179356828555022
Loss in iteration 101 : 0.0316552202728549
Loss in iteration 102 : 0.031518710699538856
Loss in iteration 103 : 0.0313839943316267
Loss in iteration 104 : 0.031251027606428554
Loss in iteration 105 : 0.03111976854909587
Loss in iteration 106 : 0.030990176694375995
Loss in iteration 107 : 0.030862213013186677
Loss in iteration 108 : 0.030735839843662526
Loss in iteration 109 : 0.030611020826355282
Loss in iteration 110 : 0.030487720843293344
Loss in iteration 111 : 0.030365905960631455
Loss in iteration 112 : 0.030245543374640933
Loss in iteration 113 : 0.030126601360811692
Loss in iteration 114 : 0.030009049225853346
Loss in iteration 115 : 0.02989285726240025
Loss in iteration 116 : 0.02977799670623895
Loss in iteration 117 : 0.029664439695890728
Loss in iteration 118 : 0.029552159234394335
Loss in iteration 119 : 0.029441129153144383
Loss in iteration 120 : 0.02933132407765337
Loss in iteration 121 : 0.029222719395113183
Loss in iteration 122 : 0.029115291223642075
Loss in iteration 123 : 0.029009016383110514
Loss in iteration 124 : 0.028903872367447854
Loss in iteration 125 : 0.028799837318338538
Loss in iteration 126 : 0.028696890000222568
Loss in iteration 127 : 0.02859500977652146
Loss in iteration 128 : 0.028494176587016806
Loss in iteration 129 : 0.02839437092631291
Loss in iteration 130 : 0.028295573823320194
Loss in iteration 131 : 0.02819776682170054
Loss in iteration 132 : 0.028100931961219232
Loss in iteration 133 : 0.028005051759952455
Loss in iteration 134 : 0.027910109197302396
Loss in iteration 135 : 0.02781608769777509
Loss in iteration 136 : 0.027722971115479795
Loss in iteration 137 : 0.02763074371931017
Loss in iteration 138 : 0.02753939017877071
Loss in iteration 139 : 0.027448895550414944
Loss in iteration 140 : 0.027359245264862535
Loss in iteration 141 : 0.027270425114365265
Loss in iteration 142 : 0.027182421240894095
Loss in iteration 143 : 0.027095220124720203
Loss in iteration 144 : 0.027008808573465267
Loss in iteration 145 : 0.02692317371159787
Loss in iteration 146 : 0.026838302970353035
Loss in iteration 147 : 0.02675418407805553
Loss in iteration 148 : 0.02667080505082564
Loss in iteration 149 : 0.02658815418365047
Loss in iteration 150 : 0.026506220041802416
Loss in iteration 151 : 0.026424991452588878
Loss in iteration 152 : 0.026344457497417366
Loss in iteration 153 : 0.02626460750416177
Loss in iteration 154 : 0.02618543103981535
Loss in iteration 155 : 0.026106917903418212
Loss in iteration 156 : 0.026029058119245788
Loss in iteration 157 : 0.025951841930247833
Loss in iteration 158 : 0.025875259791725704
Loss in iteration 159 : 0.025799302365238353
Loss in iteration 160 : 0.025723960512726365
Loss in iteration 161 : 0.025649225290844818
Loss in iteration 162 : 0.025575087945496486
Loss in iteration 163 : 0.025501539906555945
Loss in iteration 164 : 0.025428572782777235
Loss in iteration 165 : 0.025356178356877066
Loss in iteration 166 : 0.02528434858078666
Loss in iteration 167 : 0.025213075571064333
Loss in iteration 168 : 0.025142351604463695
Loss in iteration 169 : 0.025072169113649644
Loss in iteration 170 : 0.025002520683057163
Loss in iteration 171 : 0.024933399044887113
Loss in iteration 172 : 0.02486479707523303
Loss in iteration 173 : 0.024796707790334365
Loss in iteration 174 : 0.024729124342950905
Loss in iteration 175 : 0.024662040018853755
Loss in iteration 176 : 0.024595448233428498
Loss in iteration 177 : 0.02452934252838568
Loss in iteration 178 : 0.024463716568575496
Loss in iteration 179 : 0.02439856413890196
Loss in iteration 180 : 0.024333879141332875
Loss in iteration 181 : 0.024269655592002556
Loss in iteration 182 : 0.0242058876184034
Loss in iteration 183 : 0.024142569456663102
Loss in iteration 184 : 0.024079695448904533
Loss in iteration 185 : 0.024017260040685218
Loss in iteration 186 : 0.023955257778513362
Loss in iteration 187 : 0.023893683307438178
Loss in iteration 188 : 0.023832531368711003
Loss in iteration 189 : 0.02377179679751604
Loss in iteration 190 : 0.023711474520766353
Loss in iteration 191 : 0.023651559554965056
Loss in iteration 192 : 0.023592047004127505
Loss in iteration 193 : 0.02353293205776358
Loss in iteration 194 : 0.023474209988917676
Loss in iteration 195 : 0.023415876152263985
Loss in iteration 196 : 0.023357925982256004
Loss in iteration 197 : 0.023300354991327633
Loss in iteration 198 : 0.023243158768144815
Loss in iteration 199 : 0.023186332975905676
Loss in iteration 200 : 0.023129873350687304
Testing accuracy  of updater 8 on alg 0 with rate 0.07999999999999996 = 0.9751111111111112, training accuracy 0.9987139182623607, time elapsed: 4167 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 7.599951025738028
Loss in iteration 3 : 1.3973908840144431
Loss in iteration 4 : 1.7919128003972642
Loss in iteration 5 : 3.261631137552927
Loss in iteration 6 : 1.9450081955073097
Loss in iteration 7 : 1.10783637869316
Loss in iteration 8 : 0.6506258193128326
Loss in iteration 9 : 0.43652212341657903
Loss in iteration 10 : 0.47316885322942237
Loss in iteration 11 : 0.6117105029221608
Loss in iteration 12 : 0.6975273412284198
Loss in iteration 13 : 0.6544850280029127
Loss in iteration 14 : 0.5184969268086536
Loss in iteration 15 : 0.37632511873503194
Loss in iteration 16 : 0.25745246830486346
Loss in iteration 17 : 0.17794704757781152
Loss in iteration 18 : 0.13269076778826083
Loss in iteration 19 : 0.11247608656198163
Loss in iteration 20 : 0.10456267485750868
Loss in iteration 21 : 0.10453145144403406
Loss in iteration 22 : 0.10730854711824547
Loss in iteration 23 : 0.11116085160325652
Loss in iteration 24 : 0.11443152546265081
Loss in iteration 25 : 0.11475374400777659
Loss in iteration 26 : 0.11206651126830836
Loss in iteration 27 : 0.106939027544704
Loss in iteration 28 : 0.10020314045871023
Loss in iteration 29 : 0.09276904785480775
Loss in iteration 30 : 0.08542049552176585
Loss in iteration 31 : 0.07848478136343581
Loss in iteration 32 : 0.07196978332976732
Loss in iteration 33 : 0.066155397487239
Loss in iteration 34 : 0.061494295559419904
Loss in iteration 35 : 0.05804476131880828
Loss in iteration 36 : 0.05555244328151644
Loss in iteration 37 : 0.053596953620586814
Loss in iteration 38 : 0.05192348104291382
Loss in iteration 39 : 0.05058617270930413
Loss in iteration 40 : 0.04964350596661201
Loss in iteration 41 : 0.049015378853694884
Loss in iteration 42 : 0.048562002369195355
Loss in iteration 43 : 0.04817833365114937
Loss in iteration 44 : 0.04782831438158801
Loss in iteration 45 : 0.04749405475348901
Loss in iteration 46 : 0.047152841936460806
Loss in iteration 47 : 0.046785837310877806
Loss in iteration 48 : 0.04638161836681898
Loss in iteration 49 : 0.04593439884644319
Loss in iteration 50 : 0.04544432146049003
Loss in iteration 51 : 0.044916895975814525
Loss in iteration 52 : 0.04436122629876127
Loss in iteration 53 : 0.04378823150146867
Loss in iteration 54 : 0.04320925673688304
Loss in iteration 55 : 0.042635067449380774
Loss in iteration 56 : 0.042075127204050815
Loss in iteration 57 : 0.04153681370091523
Loss in iteration 58 : 0.04102426021486265
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.9786666666666667, training accuracy 0.9989997142040583, time elapsed: 1132 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.304747616635278
Loss in iteration 3 : 0.1333762197755013
Loss in iteration 4 : 0.13148963858718998
Loss in iteration 5 : 0.11851223599148879
Loss in iteration 6 : 0.08299571548236749
Loss in iteration 7 : 0.058959390443576655
Loss in iteration 8 : 0.04809745884354901
Loss in iteration 9 : 0.04459635595814958
Loss in iteration 10 : 0.04218761870832431
Loss in iteration 11 : 0.03789284029600358
Loss in iteration 12 : 0.032184320613976344
Loss in iteration 13 : 0.02663463493050813
Loss in iteration 14 : 0.022287800934053352
Loss in iteration 15 : 0.019384081451760595
Loss in iteration 16 : 0.017685181470303634
Loss in iteration 17 : 0.01680016971301942
Loss in iteration 18 : 0.016357837218867837
Loss in iteration 19 : 0.016071318111777068
Loss in iteration 20 : 0.015753571059862474
Loss in iteration 21 : 0.015310967671637336
Loss in iteration 22 : 0.014724583126544953
Loss in iteration 23 : 0.014025031862564544
Loss in iteration 24 : 0.013266857663480753
Loss in iteration 25 : 0.012507621643298451
Loss in iteration 26 : 0.011794413280019025
Loss in iteration 27 : 0.011157823112787428
Loss in iteration 28 : 0.010611623336628717
Loss in iteration 29 : 0.010155816509277734
Loss in iteration 30 : 0.009781002878486638
Loss in iteration 31 : 0.009472680545533437
Loss in iteration 32 : 0.009214754471744064
Loss in iteration 33 : 0.008992015751676439
Loss in iteration 34 : 0.008791636751846153
Loss in iteration 35 : 0.008603857310054362
Loss in iteration 36 : 0.008422073025404372
Loss in iteration 37 : 0.008242526016570837
Loss in iteration 38 : 0.008063769152063019
Loss in iteration 39 : 0.007886039636824594
Loss in iteration 40 : 0.007710642161441925
Loss in iteration 41 : 0.007539408017146681
Loss in iteration 42 : 0.007374266555880225
Loss in iteration 43 : 0.0072169408171913405
Loss in iteration 44 : 0.0070687611275618125
Loss in iteration 45 : 0.006930579149346246
Loss in iteration 46 : 0.0068027594633034575
Loss in iteration 47 : 0.006685224963694143
Loss in iteration 48 : 0.006577534581264269
Loss in iteration 49 : 0.006478975709499594
Loss in iteration 50 : 0.0063886580956666805
Loss in iteration 51 : 0.006305600145989062
Loss in iteration 52 : 0.006228802190631342
Loss in iteration 53 : 0.006157304110039115
Loss in iteration 54 : 0.006090226843309258
Loss in iteration 55 : 0.006026798763960997
Loss in iteration 56 : 0.005966368830257052
Loss in iteration 57 : 0.005908408909286533
Loss in iteration 58 : 0.005852507839100449
Loss in iteration 59 : 0.005798359720075224
Loss in iteration 60 : 0.0057457486905157775
Loss in iteration 61 : 0.005694532105067539
Loss in iteration 62 : 0.005644623649218738
Loss in iteration 63 : 0.005595977529766186
Loss in iteration 64 : 0.005548574510096826
Loss in iteration 65 : 0.005502410231398074
Loss in iteration 66 : 0.005457485988691217
Loss in iteration 67 : 0.005413801918757304
Loss in iteration 68 : 0.0053713524046437285
Loss in iteration 69 : 0.005330123403361272
Loss in iteration 70 : 0.005290091351827725
Loss in iteration 71 : 0.005251223292019963
Loss in iteration 72 : 0.005213477870390688
Loss in iteration 73 : 0.005176806900182943
Loss in iteration 74 : 0.0051411572206952715
Loss in iteration 75 : 0.005106472638493476
Loss in iteration 76 : 0.005072695787108322
Loss in iteration 77 : 0.005039769790316332
Loss in iteration 78 : 0.005007639657286762
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.9813333333333333, training accuracy 0.9989997142040583, time elapsed: 1683 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.5876793654598029
Loss in iteration 3 : 0.4916549294741767
Loss in iteration 4 : 0.41475015125747594
Loss in iteration 5 : 0.35558279826434636
Loss in iteration 6 : 0.3107549006781367
Loss in iteration 7 : 0.27672548352978776
Loss in iteration 8 : 0.2504692947703929
Loss in iteration 9 : 0.22970433029042595
Loss in iteration 10 : 0.21283633292991516
Loss in iteration 11 : 0.19879237840190814
Loss in iteration 12 : 0.1868561583832255
Loss in iteration 13 : 0.17654359752064136
Loss in iteration 14 : 0.16751938721050802
Loss in iteration 15 : 0.1595439478769466
Loss in iteration 16 : 0.1524404138704651
Loss in iteration 17 : 0.14607408454260806
Loss in iteration 18 : 0.14033946717794737
Loss in iteration 19 : 0.13515193170553846
Loss in iteration 20 : 0.13044219557191275
Loss in iteration 21 : 0.126152577836799
Loss in iteration 22 : 0.12223438532290325
Loss in iteration 23 : 0.1186460415252319
Loss in iteration 24 : 0.11535171497729853
Loss in iteration 25 : 0.11232029115884962
Loss in iteration 26 : 0.10952458554610116
Loss in iteration 27 : 0.10694072906089924
Loss in iteration 28 : 0.1045476789202832
Loss in iteration 29 : 0.10232682228679153
Loss in iteration 30 : 0.10026164984636664
Loss in iteration 31 : 0.09833748310579078
Loss in iteration 32 : 0.09654124380874876
Loss in iteration 33 : 0.09486125706666049
Loss in iteration 34 : 0.09328708201996277
Loss in iteration 35 : 0.09180936538483966
Loss in iteration 36 : 0.09041971430792844
Loss in iteration 37 : 0.0891105856950613
Loss in iteration 38 : 0.08787518970442103
Loss in iteration 39 : 0.08670740547326068
Loss in iteration 40 : 0.08560170743135563
Loss in iteration 41 : 0.08455310077765604
Loss in iteration 42 : 0.08355706488109847
Loss in iteration 43 : 0.08260950352557873
Loss in iteration 44 : 0.0817067010602352
Loss in iteration 45 : 0.08084528364328225
Loss in iteration 46 : 0.08002218488229748
Loss in iteration 47 : 0.07923461527662788
Loss in iteration 48 : 0.07848003495852926
Loss in iteration 49 : 0.07775612930889142
Loss in iteration 50 : 0.07706078709126843
Loss in iteration 51 : 0.0763920808050357
Loss in iteration 52 : 0.07574824900574788
Loss in iteration 53 : 0.07512768037926915
Loss in iteration 54 : 0.07452889938721971
Loss in iteration 55 : 0.07395055332598137
Loss in iteration 56 : 0.0733914006611264
Loss in iteration 57 : 0.07285030051478612
Loss in iteration 58 : 0.07232620319609938
Loss in iteration 59 : 0.0718181416752843
Loss in iteration 60 : 0.07132522391067765
Loss in iteration 61 : 0.07084662594577533
Loss in iteration 62 : 0.07038158570021717
Loss in iteration 63 : 0.06992939738503685
Loss in iteration 64 : 0.06948940647846778
Loss in iteration 65 : 0.0690610052042563
Loss in iteration 66 : 0.06864362845979488
Loss in iteration 67 : 0.06823675014646854
Loss in iteration 68 : 0.06783987985938826
Loss in iteration 69 : 0.06745255989814117
Loss in iteration 70 : 0.06707436256431686
Loss in iteration 71 : 0.06670488771534183
Loss in iteration 72 : 0.06634376054758709
Loss in iteration 73 : 0.06599062958479075
Loss in iteration 74 : 0.06564516485058988
Loss in iteration 75 : 0.06530705620638888
Loss in iteration 76 : 0.06497601183793371
Loss in iteration 77 : 0.06465175687584351
Loss in iteration 78 : 0.06433403213698619
Loss in iteration 79 : 0.06402259297502477
Loss in iteration 80 : 0.06371720822970345
Loss in iteration 81 : 0.06341765926554115
Loss in iteration 82 : 0.06312373909155365
Loss in iteration 83 : 0.0628352515544698
Loss in iteration 84 : 0.06255201059865072
Loss in iteration 85 : 0.06227383958658602
Loss in iteration 86 : 0.062000570674426374
Loss in iteration 87 : 0.0617320442375467
Loss in iteration 88 : 0.061468108341608
Loss in iteration 89 : 0.06120861825501966
Loss in iteration 90 : 0.06095343599909053
Loss in iteration 91 : 0.060702429932515324
Loss in iteration 92 : 0.06045547436716024
Loss in iteration 93 : 0.060212449212404526
Loss in iteration 94 : 0.059973239645557806
Loss in iteration 95 : 0.05973773580611313
Loss in iteration 96 : 0.059505832511807424
Loss in iteration 97 : 0.059277428994661444
Loss in iteration 98 : 0.05905242865534121
Loss in iteration 99 : 0.058830738834343074
Loss in iteration 100 : 0.05861227059864514
Loss in iteration 101 : 0.05839693854259514
Loss in iteration 102 : 0.058184660601916516
Loss in iteration 103 : 0.0579753578798196
Loss in iteration 104 : 0.05776895448429078
Loss in iteration 105 : 0.05756537737571984
Loss in iteration 106 : 0.05736455622409428
Loss in iteration 107 : 0.05716642327505725
Loss in iteration 108 : 0.05697091322418549
Loss in iteration 109 : 0.05677796309889464
Loss in iteration 110 : 0.056587512147429755
Loss in iteration 111 : 0.056399501734442144
Loss in iteration 112 : 0.05621387524269179
Loss in iteration 113 : 0.05603057798045284
Loss in iteration 114 : 0.05584955709423035
Loss in iteration 115 : 0.055670761486428424
Loss in iteration 116 : 0.05549414173763699
Loss in iteration 117 : 0.05531965003322765
Loss in iteration 118 : 0.05514724009397644
Loss in iteration 119 : 0.054976867110447164
Loss in iteration 120 : 0.05480848768089393
Loss in iteration 121 : 0.05464205975245547
Loss in iteration 122 : 0.05447754256543254
Loss in iteration 123 : 0.05431489660045396
Loss in iteration 124 : 0.05415408352835281
Loss in iteration 125 : 0.053995066162582546
Loss in iteration 126 : 0.05383780841402189
Loss in iteration 127 : 0.053682275248021494
Loss in iteration 128 : 0.05352843264355939
Loss in iteration 129 : 0.053376247554380696
Loss in iteration 130 : 0.05322568787200523
Loss in iteration 131 : 0.05307672239049541
Loss in iteration 132 : 0.052929320772882295
Loss in iteration 133 : 0.052783453519157836
Loss in iteration 134 : 0.0526390919357433
Loss in iteration 135 : 0.052496208106353325
Loss in iteration 136 : 0.052354774864178354
Loss in iteration 137 : 0.05221476576531331
Loss in iteration 138 : 0.05207615506336578
Loss in iteration 139 : 0.05193891768518048
Loss in iteration 140 : 0.051803029207621025
Loss in iteration 141 : 0.05166846583535294
Loss in iteration 142 : 0.05153520437957738
Loss in iteration 143 : 0.051403222237664435
Loss in iteration 144 : 0.051272497373642215
Loss in iteration 145 : 0.05114300829949696
Loss in iteration 146 : 0.05101473405724466
Loss in iteration 147 : 0.05088765420173465
Loss in iteration 148 : 0.05076174878415017
Loss in iteration 149 : 0.05063699833617129
Loss in iteration 150 : 0.050513383854768674
Loss in iteration 151 : 0.0503908867875966
Loss in iteration 152 : 0.05026948901895923
Loss in iteration 153 : 0.0501491728563197
Loss in iteration 154 : 0.05002992101733045
Loss in iteration 155 : 0.049911716617356834
Loss in iteration 156 : 0.049794543157474454
Loss in iteration 157 : 0.049678384512917
Loss in iteration 158 : 0.04956322492195403
Loss in iteration 159 : 0.04944904897518103
Loss in iteration 160 : 0.04933584160520113
Loss in iteration 161 : 0.0492235880766829
Loss in iteration 162 : 0.049112273976778316
Loss in iteration 163 : 0.04900188520588183
Loss in iteration 164 : 0.04889240796872067
Loss in iteration 165 : 0.04878382876575857
Loss in iteration 166 : 0.04867613438490063
Loss in iteration 167 : 0.048569311893487704
Loss in iteration 168 : 0.04846334863056697
Loss in iteration 169 : 0.048358232199427226
Loss in iteration 170 : 0.04825395046038942
Loss in iteration 171 : 0.04815049152383989
Loss in iteration 172 : 0.0480478437434988
Loss in iteration 173 : 0.04794599570991177
Loss in iteration 174 : 0.047844936244158545
Loss in iteration 175 : 0.04774465439176737
Loss in iteration 176 : 0.047645139416829435
Loss in iteration 177 : 0.047546380796303314
Loss in iteration 178 : 0.047448368214503975
Loss in iteration 179 : 0.047351091557767774
Loss in iteration 180 : 0.04725454090928773
Loss in iteration 181 : 0.04715870654411186
Loss in iteration 182 : 0.04706357892429868
Loss in iteration 183 : 0.046969148694224104
Loss in iteration 184 : 0.04687540667603416
Loss in iteration 185 : 0.04678234386523736
Loss in iteration 186 : 0.04668995142643285
Loss in iteration 187 : 0.04659822068916807
Loss in iteration 188 : 0.04650714314392211
Loss in iteration 189 : 0.04641671043821003
Loss in iteration 190 : 0.04632691437280329
Loss in iteration 191 : 0.046237746898062905
Loss in iteration 192 : 0.04614920011038098
Loss in iteration 193 : 0.046061266248726286
Loss in iteration 194 : 0.04597393769129139
Loss in iteration 195 : 0.04588720695223691
Loss in iteration 196 : 0.04580106667852931
Loss in iteration 197 : 0.0457155096468704
Loss in iteration 198 : 0.045630528760713406
Loss in iteration 199 : 0.04554611704736473
Loss in iteration 200 : 0.045462267655166706
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.9662222222222222, training accuracy 0.993855387253501, time elapsed: 4488 millisecond.
