objc[2575]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1017714c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x1017f54e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 21:06:02 INFO SparkContext: Running Spark version 2.0.0
18/02/26 21:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 21:06:03 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 21:06:03 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 21:06:03 INFO SecurityManager: Changing view acls groups to: 
18/02/26 21:06:03 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 21:06:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 21:06:03 INFO Utils: Successfully started service 'sparkDriver' on port 51437.
18/02/26 21:06:03 INFO SparkEnv: Registering MapOutputTracker
18/02/26 21:06:03 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 21:06:03 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-23fbf987-70eb-4074-87fb-e44a699554d7
18/02/26 21:06:04 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 21:06:04 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 21:06:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 21:06:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 21:06:04 INFO Executor: Starting executor ID driver on host localhost
18/02/26 21:06:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51438.
18/02/26 21:06:04 INFO NettyBlockTransferService: Server created on 192.168.2.140:51438
18/02/26 21:06:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 51438)
18/02/26 21:06:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:51438 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 51438)
18/02/26 21:06:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 51438)
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.3896664660825948
Loss in iteration 3 : 0.4713776023236357
Loss in iteration 4 : 0.41990645578428715
Loss in iteration 5 : 0.46278734838873964
Loss in iteration 6 : 0.3927738077420322
Loss in iteration 7 : 0.4025152188589516
Loss in iteration 8 : 0.36895536196077505
Loss in iteration 9 : 0.3677005591725164
Loss in iteration 10 : 0.353368905328423
Loss in iteration 11 : 0.3505896443801199
Loss in iteration 12 : 0.3447685653092544
Loss in iteration 13 : 0.34280612641325336
Loss in iteration 14 : 0.3404370987953765
Loss in iteration 15 : 0.3393256544364279
Loss in iteration 16 : 0.3382993212044811
Loss in iteration 17 : 0.33768784590532996
Loss in iteration 18 : 0.3371767052405078
Loss in iteration 19 : 0.3368088249733965
Testing accuracy  of updater 0 on alg 0 with rate 0.001 = 0.78, training accuracy 0.832308190352865, time elapsed: 1495 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.9961903707673733
Loss in iteration 3 : 0.44215993013868515
Loss in iteration 4 : 0.3870963146042838
Loss in iteration 5 : 0.3783608645767582
Loss in iteration 6 : 0.3649236947047112
Loss in iteration 7 : 0.35934664547400486
Loss in iteration 8 : 0.3549348350332629
Loss in iteration 9 : 0.35217499762012416
Loss in iteration 10 : 0.35007229853927585
Loss in iteration 11 : 0.34840481058086786
Loss in iteration 12 : 0.34700602025689925
Testing accuracy  of updater 0 on alg 0 with rate 7.000000000000001E-4 = 0.7735, training accuracy 0.830042084817093, time elapsed: 369 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6427414632225813
Loss in iteration 3 : 0.44551441542822173
Loss in iteration 4 : 0.4124851389141893
Loss in iteration 5 : 0.4019085679906379
Loss in iteration 6 : 0.3935753727922455
Loss in iteration 7 : 0.3867835762303029
Loss in iteration 8 : 0.38114803989698465
Loss in iteration 9 : 0.3764072801340163
Loss in iteration 10 : 0.3723721756684631
Loss in iteration 11 : 0.3689027694958427
Loss in iteration 12 : 0.3658933403566717
Loss in iteration 13 : 0.36326259977980596
Testing accuracy  of updater 0 on alg 0 with rate 4.0E-4 = 0.762, training accuracy 0.828099708643574, time elapsed: 363 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5138915865062822
Loss in iteration 3 : 0.4934760387861219
Loss in iteration 4 : 0.4827481535422838
Testing accuracy  of updater 0 on alg 0 with rate 1.0E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 146 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.545475733585198
Loss in iteration 3 : 0.5111620708418756
Loss in iteration 4 : 0.4961942157337223
Testing accuracy  of updater 0 on alg 0 with rate 7.000000000000001E-5 = 0.5, training accuracy 0.6474587245063127, time elapsed: 104 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5946891169439639
Loss in iteration 3 : 0.5497500527966266
Loss in iteration 4 : 0.5259849471409118
Testing accuracy  of updater 0 on alg 0 with rate 4.0E-5 = 0.5, training accuracy 0.6474587245063127, time elapsed: 143 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6647626738851746
Testing accuracy  of updater 0 on alg 0 with rate 9.999999999999999E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 100 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.3896664660825948
Loss in iteration 3 : 1.5577100708616933
Loss in iteration 4 : 0.7481982349911901
Loss in iteration 5 : 0.5481028613768388
Loss in iteration 6 : 1.1874776289745916
Loss in iteration 7 : 0.845234944769576
Loss in iteration 8 : 0.5904954308956555
Loss in iteration 9 : 0.7982429986485692
Loss in iteration 10 : 1.0009136847918068
Loss in iteration 11 : 0.9724511533420018
Loss in iteration 12 : 0.8103593933762574
Loss in iteration 13 : 0.7272238362104204
Loss in iteration 14 : 0.8190432770076564
Loss in iteration 15 : 0.9250906480831307
Loss in iteration 16 : 0.8663339535243548
Loss in iteration 17 : 0.7405987224719383
Loss in iteration 18 : 0.7149674410930664
Loss in iteration 19 : 0.7689025558927276
Loss in iteration 20 : 0.7864723928382138
Loss in iteration 21 : 0.7189455906013639
Loss in iteration 22 : 0.6240146490859185
Loss in iteration 23 : 0.600164102336776
Loss in iteration 24 : 0.6360737052766785
Loss in iteration 25 : 0.596405396866853
Loss in iteration 26 : 0.502592009410427
Loss in iteration 27 : 0.488665330993169
Loss in iteration 28 : 0.5082301609648968
Loss in iteration 29 : 0.45777481991915464
Loss in iteration 30 : 0.39246785473561285
Loss in iteration 31 : 0.4317518900330115
Loss in iteration 32 : 0.3960625947732433
Loss in iteration 33 : 0.37223195092787337
Loss in iteration 34 : 0.42114274754636627
Loss in iteration 35 : 0.3679852585247164
Loss in iteration 36 : 0.4577800883115486
Loss in iteration 37 : 0.39276928906918973
Loss in iteration 38 : 0.42605464950977073
Loss in iteration 39 : 0.3662924699353081
Loss in iteration 40 : 0.3985247037371278
Loss in iteration 41 : 0.35146159867422494
Loss in iteration 42 : 0.37495262117295436
Loss in iteration 43 : 0.34177912394488047
Loss in iteration 44 : 0.3633890473377066
Loss in iteration 45 : 0.3529882844437886
Loss in iteration 46 : 0.3503077943858269
Loss in iteration 47 : 0.36399918359175487
Loss in iteration 48 : 0.34950212716318285
Loss in iteration 49 : 0.35552875818979707
Loss in iteration 50 : 0.3558854626273872
Loss in iteration 51 : 0.3445640344555903
Loss in iteration 52 : 0.35181221122030143
Loss in iteration 53 : 0.341739459462673
Loss in iteration 54 : 0.34024219231297265
Loss in iteration 55 : 0.34079686410241544
Loss in iteration 56 : 0.33337510830864847
Loss in iteration 57 : 0.3394500666833576
Loss in iteration 58 : 0.33223012364288584
Loss in iteration 59 : 0.34015386368990114
Loss in iteration 60 : 0.3341429383599296
Loss in iteration 61 : 0.3406039836033181
Loss in iteration 62 : 0.3354621469296663
Loss in iteration 63 : 0.33996737177608616
Loss in iteration 64 : 0.3344837882420348
Loss in iteration 65 : 0.3371947671658648
Loss in iteration 66 : 0.3330534963866218
Loss in iteration 67 : 0.33509101062338836
Loss in iteration 68 : 0.33249714274502407
Loss in iteration 69 : 0.3338971387637286
Loss in iteration 70 : 0.33315080485407483
Loss in iteration 71 : 0.33337451632211795
Loss in iteration 72 : 0.3338860005260433
Loss in iteration 73 : 0.33306496756828047
Loss in iteration 74 : 0.33402310222839193
Loss in iteration 75 : 0.3327107313949606
Loss in iteration 76 : 0.33345774906258113
Loss in iteration 77 : 0.33239695124632074
Loss in iteration 78 : 0.3326274291440093
Loss in iteration 79 : 0.33222882325678665
Loss in iteration 80 : 0.33205405941569716
Loss in iteration 81 : 0.3322346252365986
Loss in iteration 82 : 0.3319099697430569
Testing accuracy  of updater 1 on alg 0 with rate 0.001 = 0.78025, training accuracy 0.8358692133376497, time elapsed: 1820 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.9961903707673733
Loss in iteration 3 : 1.140712959485568
Loss in iteration 4 : 0.6400512763367884
Loss in iteration 5 : 0.40488631998447455
Loss in iteration 6 : 0.8340489195466435
Loss in iteration 7 : 0.69824333721801
Loss in iteration 8 : 0.46851184520993816
Loss in iteration 9 : 0.5618862760484417
Loss in iteration 10 : 0.7150993810726302
Loss in iteration 11 : 0.7327963951165893
Loss in iteration 12 : 0.6365283232641423
Loss in iteration 13 : 0.5595266966611131
Loss in iteration 14 : 0.5921122961775336
Loss in iteration 15 : 0.6736696369394742
Loss in iteration 16 : 0.6719919551748136
Loss in iteration 17 : 0.5928823596846662
Loss in iteration 18 : 0.5481130990803046
Loss in iteration 19 : 0.5703543952385
Loss in iteration 20 : 0.5985656737038929
Loss in iteration 21 : 0.5785566496535487
Loss in iteration 22 : 0.5195102016648092
Loss in iteration 23 : 0.47892545884555604
Loss in iteration 24 : 0.49022137118045395
Loss in iteration 25 : 0.49931053932049857
Loss in iteration 26 : 0.45310133070225334
Loss in iteration 27 : 0.40978034171217037
Loss in iteration 28 : 0.41560584268212153
Loss in iteration 29 : 0.42042128783977795
Loss in iteration 30 : 0.3842035963838497
Loss in iteration 31 : 0.3567341618686861
Loss in iteration 32 : 0.38030320382737937
Loss in iteration 33 : 0.36495214483620797
Loss in iteration 34 : 0.34445916779300956
Loss in iteration 35 : 0.3737951198466599
Loss in iteration 36 : 0.36068496367444997
Loss in iteration 37 : 0.3605077234828342
Loss in iteration 38 : 0.3800736369486823
Loss in iteration 39 : 0.35688142510525434
Loss in iteration 40 : 0.3752581702562215
Loss in iteration 41 : 0.3540204671965455
Loss in iteration 42 : 0.35427813630080707
Loss in iteration 43 : 0.3520948771808088
Loss in iteration 44 : 0.338159191310624
Loss in iteration 45 : 0.34607344967881626
Loss in iteration 46 : 0.3387995276533605
Loss in iteration 47 : 0.3361026688305956
Loss in iteration 48 : 0.34290506466257653
Loss in iteration 49 : 0.3375982919782269
Loss in iteration 50 : 0.3382012593624913
Loss in iteration 51 : 0.3424233641081972
Loss in iteration 52 : 0.3387300477471445
Loss in iteration 53 : 0.33827094541084357
Loss in iteration 54 : 0.34085940277661925
Loss in iteration 55 : 0.33718930363597527
Loss in iteration 56 : 0.33639889962932656
Loss in iteration 57 : 0.33755568058435736
Loss in iteration 58 : 0.33433730631187153
Loss in iteration 59 : 0.33385916661386855
Loss in iteration 60 : 0.3345916097859396
Loss in iteration 61 : 0.3321254277037676
Loss in iteration 62 : 0.33303715038641235
Loss in iteration 63 : 0.33321541303696356
Loss in iteration 64 : 0.332084825024101
Loss in iteration 65 : 0.33368496294494454
Loss in iteration 66 : 0.3330086474234743
Loss in iteration 67 : 0.3331740710908006
Testing accuracy  of updater 1 on alg 0 with rate 7.000000000000001E-4 = 0.77525, training accuracy 0.8355454839753965, time elapsed: 1245 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6427414632225813
Loss in iteration 3 : 0.7694098989238926
Loss in iteration 4 : 0.5838697526922739
Loss in iteration 5 : 0.3472022966056726
Loss in iteration 6 : 0.46385209249417386
Loss in iteration 7 : 0.5654563128955586
Loss in iteration 8 : 0.4364987073012429
Loss in iteration 9 : 0.3730478869025397
Loss in iteration 10 : 0.43184654602876305
Loss in iteration 11 : 0.49556504806334317
Loss in iteration 12 : 0.4962345727012181
Loss in iteration 13 : 0.451151432867469
Loss in iteration 14 : 0.41733677164087135
Loss in iteration 15 : 0.42998855478059
Loss in iteration 16 : 0.46649166839444456
Loss in iteration 17 : 0.4756759663237052
Loss in iteration 18 : 0.4474472942632273
Loss in iteration 19 : 0.4191175299664114
Loss in iteration 20 : 0.41764458886495603
Loss in iteration 21 : 0.4320066726309164
Loss in iteration 22 : 0.43668936084353044
Loss in iteration 23 : 0.4208933219188838
Loss in iteration 24 : 0.3967014504676716
Loss in iteration 25 : 0.384822007561187
Loss in iteration 26 : 0.38997751462796715
Loss in iteration 27 : 0.39283343245728713
Loss in iteration 28 : 0.37821143571237137
Loss in iteration 29 : 0.359673262168233
Loss in iteration 30 : 0.3555244828912857
Loss in iteration 31 : 0.3601761795810064
Loss in iteration 32 : 0.35681452749048614
Loss in iteration 33 : 0.34390881399672046
Loss in iteration 34 : 0.3372323111570368
Loss in iteration 35 : 0.34308537255875626
Loss in iteration 36 : 0.3443872274257142
Loss in iteration 37 : 0.33587294335896883
Loss in iteration 38 : 0.33528857560700387
Loss in iteration 39 : 0.3422342294316783
Loss in iteration 40 : 0.34176059342445286
Loss in iteration 41 : 0.33712696626252026
Loss in iteration 42 : 0.3406308725552818
Loss in iteration 43 : 0.344073351949684
Loss in iteration 44 : 0.33964432874110306
Loss in iteration 45 : 0.33887719747248307
Loss in iteration 46 : 0.34151133332756256
Loss in iteration 47 : 0.33923523761748897
Loss in iteration 48 : 0.33585967378444986
Loss in iteration 49 : 0.3367409237897584
Loss in iteration 50 : 0.3367722365244471
Loss in iteration 51 : 0.33397762401237757
Loss in iteration 52 : 0.3332897834862446
Loss in iteration 53 : 0.3343101615674221
Loss in iteration 54 : 0.3336458636146798
Loss in iteration 55 : 0.3323655043867913
Loss in iteration 56 : 0.3327973803708596
Loss in iteration 57 : 0.3335701999148935
Loss in iteration 58 : 0.3330253802069149
Loss in iteration 59 : 0.33255466634767816
Loss in iteration 60 : 0.3331186708039071
Testing accuracy  of updater 1 on alg 0 with rate 4.0E-4 = 0.782, training accuracy 0.8365166720621561, time elapsed: 881 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5138915865062822
Loss in iteration 3 : 0.49434035906106377
Loss in iteration 4 : 0.5262617944756769
Loss in iteration 5 : 0.5310164705656222
Loss in iteration 6 : 0.49454496979716656
Loss in iteration 7 : 0.4326058380108367
Loss in iteration 8 : 0.37766734429665266
Loss in iteration 9 : 0.35940383977349166
Loss in iteration 10 : 0.376581787809734
Loss in iteration 11 : 0.3974117548542574
Loss in iteration 12 : 0.3957421178801419
Loss in iteration 13 : 0.37407393288753177
Loss in iteration 14 : 0.3510383203020607
Loss in iteration 15 : 0.3401035822952443
Loss in iteration 16 : 0.342323356154168
Loss in iteration 17 : 0.35111368056175596
Loss in iteration 18 : 0.35903483958121185
Loss in iteration 19 : 0.3616820906004428
Loss in iteration 20 : 0.35856395682024544
Loss in iteration 21 : 0.35212238894522374
Loss in iteration 22 : 0.34591270520900386
Loss in iteration 23 : 0.3427428736405778
Loss in iteration 24 : 0.3434457051420556
Loss in iteration 25 : 0.3467421963283963
Loss in iteration 26 : 0.3502042422801864
Testing accuracy  of updater 1 on alg 0 with rate 1.0E-4 = 0.78175, training accuracy 0.821948850760764, time elapsed: 363 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.545475733585198
Loss in iteration 3 : 0.48754296144682874
Loss in iteration 4 : 0.5019975355377355
Loss in iteration 5 : 0.5210100330761144
Loss in iteration 6 : 0.5173678999227783
Loss in iteration 7 : 0.48776829706353514
Loss in iteration 8 : 0.4416172858539855
Loss in iteration 9 : 0.3958217642357256
Loss in iteration 10 : 0.36784030528804007
Loss in iteration 11 : 0.36493834065668
Loss in iteration 12 : 0.37815073348165557
Loss in iteration 13 : 0.38988270512445794
Loss in iteration 14 : 0.388433049117929
Loss in iteration 15 : 0.374706343353524
Loss in iteration 16 : 0.35744371593832647
Loss in iteration 17 : 0.3449106747341535
Loss in iteration 18 : 0.3403887037616103
Loss in iteration 19 : 0.34246952864348507
Loss in iteration 20 : 0.34753313420788456
Loss in iteration 21 : 0.3520556886940927
Loss in iteration 22 : 0.3538826118886722
Loss in iteration 23 : 0.35253732334261384
Loss in iteration 24 : 0.3488971271669193
Loss in iteration 25 : 0.34454696351373026
Loss in iteration 26 : 0.341062506699828
Loss in iteration 27 : 0.33943191344713913
Loss in iteration 28 : 0.33977483332482616
Loss in iteration 29 : 0.3414264373296753
Testing accuracy  of updater 1 on alg 0 with rate 7.000000000000001E-5 = 0.77875, training accuracy 0.8271285205568145, time elapsed: 450 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5946891169439639
Loss in iteration 3 : 0.5113135669884309
Loss in iteration 4 : 0.48634427757675425
Loss in iteration 5 : 0.4934095588274048
Loss in iteration 6 : 0.5055682080534609
Loss in iteration 7 : 0.5092136521459733
Loss in iteration 8 : 0.4998923316248943
Loss in iteration 9 : 0.4784947922933867
Loss in iteration 10 : 0.4492037513385785
Loss in iteration 11 : 0.41816646561944903
Loss in iteration 12 : 0.3919826202687343
Loss in iteration 13 : 0.37564401592168406
Loss in iteration 14 : 0.3705195468445878
Loss in iteration 15 : 0.3737246669204451
Loss in iteration 16 : 0.37967664727953576
Loss in iteration 17 : 0.3830082501761054
Loss in iteration 18 : 0.3809563719240826
Loss in iteration 19 : 0.37392756350568035
Loss in iteration 20 : 0.36440093031246845
Loss in iteration 21 : 0.35525618846534873
Loss in iteration 22 : 0.3485251575613081
Loss in iteration 23 : 0.34494198468953824
Loss in iteration 24 : 0.34413412718298064
Loss in iteration 25 : 0.3451029610142578
Loss in iteration 26 : 0.34670663225654474
Loss in iteration 27 : 0.3480022125361593
Testing accuracy  of updater 1 on alg 0 with rate 4.0E-5 = 0.7545, training accuracy 0.8235674975720297, time elapsed: 388 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6647626738851746
Testing accuracy  of updater 1 on alg 0 with rate 9.999999999999999E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 48 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 2.6150117376811233
Loss in iteration 3 : 1.662906834288797
Loss in iteration 4 : 0.43918411292716625
Loss in iteration 5 : 0.6651983365211819
Loss in iteration 6 : 0.5449589907767509
Loss in iteration 7 : 0.5473150671154264
Loss in iteration 8 : 0.568734754593046
Loss in iteration 9 : 0.5841711092390561
Loss in iteration 10 : 0.5911623522099207
Loss in iteration 11 : 0.5916230706303085
Loss in iteration 12 : 0.586571569874536
Loss in iteration 13 : 0.5764379261020084
Loss in iteration 14 : 0.5618573082825885
Loss in iteration 15 : 0.5436275564126073
Loss in iteration 16 : 0.5225156768408554
Loss in iteration 17 : 0.4992679945407664
Loss in iteration 18 : 0.47466522045845694
Loss in iteration 19 : 0.449529176942373
Loss in iteration 20 : 0.4247269624174817
Loss in iteration 21 : 0.40121841062840125
Loss in iteration 22 : 0.38012945888383615
Loss in iteration 23 : 0.3627657748857423
Loss in iteration 24 : 0.35050744661779587
Loss in iteration 25 : 0.34453396989798024
Loss in iteration 26 : 0.3452473642930284
Loss in iteration 27 : 0.35132349763836623
Loss in iteration 28 : 0.35902529962921464
Loss in iteration 29 : 0.3634775738955459
Loss in iteration 30 : 0.3621078709422554
Loss in iteration 31 : 0.3562657462002593
Loss in iteration 32 : 0.34902455485580997
Loss in iteration 33 : 0.3426944718858658
Loss in iteration 34 : 0.3381978483512791
Loss in iteration 35 : 0.33556085761603605
Loss in iteration 36 : 0.33434721932304606
Loss in iteration 37 : 0.33410072643328864
Loss in iteration 38 : 0.33432249049467594
Loss in iteration 39 : 0.33471641185862455
Loss in iteration 40 : 0.33502679538706553
Loss in iteration 41 : 0.3351594482434434
Loss in iteration 42 : 0.33512081602382554
Loss in iteration 43 : 0.33494143889338046
Loss in iteration 44 : 0.33487659153384763
Loss in iteration 45 : 0.33502356931592886
Loss in iteration 46 : 0.3362364116510043
Loss in iteration 47 : 0.33880385524289586
Loss in iteration 48 : 0.3473693296960269
Loss in iteration 49 : 0.3620280930229549
Loss in iteration 50 : 0.41739969579408437
Loss in iteration 51 : 0.4581426648884221
Loss in iteration 52 : 0.6774353861831395
Loss in iteration 53 : 0.44973640792234254
Loss in iteration 54 : 0.4790597200304583
Loss in iteration 55 : 0.4018802474852281
Loss in iteration 56 : 0.3822278444704536
Loss in iteration 57 : 0.36258797877965954
Loss in iteration 58 : 0.3547514326059475
Loss in iteration 59 : 0.3512074011306845
Loss in iteration 60 : 0.3477982167376714
Loss in iteration 61 : 0.344379091038123
Loss in iteration 62 : 0.3408470239836077
Loss in iteration 63 : 0.33760073795472584
Loss in iteration 64 : 0.33496355196481953
Loss in iteration 65 : 0.3332291694221724
Loss in iteration 66 : 0.3324812805555893
Loss in iteration 67 : 0.3327400915648856
Loss in iteration 68 : 0.3339933318680327
Loss in iteration 69 : 0.3369446238359508
Loss in iteration 70 : 0.3460100068297869
Loss in iteration 71 : 0.3713829060418213
Loss in iteration 72 : 0.48101935357742215
Loss in iteration 73 : 0.5467510736929784
Loss in iteration 74 : 0.9426332275724819
Loss in iteration 75 : 0.35787367569930895
Loss in iteration 76 : 0.34908417538775643
Loss in iteration 77 : 0.35342777320192825
Loss in iteration 78 : 0.355481474888475
Loss in iteration 79 : 0.35624725746705976
Testing accuracy  of updater 2 on alg 0 with rate 0.001 = 0.78, training accuracy 0.8306895435415992, time elapsed: 1113 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.8355255346364894
Loss in iteration 3 : 1.1911326987836386
Loss in iteration 4 : 0.38071874377918796
Loss in iteration 5 : 0.5157181093404665
Loss in iteration 6 : 0.4446395078624231
Loss in iteration 7 : 0.438567822937381
Loss in iteration 8 : 0.45240785413169166
Loss in iteration 9 : 0.4644521455990182
Loss in iteration 10 : 0.4711372217914584
Loss in iteration 11 : 0.4736571022439513
Loss in iteration 12 : 0.4728487886488813
Loss in iteration 13 : 0.4689208828506785
Loss in iteration 14 : 0.46215948599779
Loss in iteration 15 : 0.4530442270773925
Loss in iteration 16 : 0.4420735903515149
Loss in iteration 17 : 0.42971261452517373
Loss in iteration 18 : 0.41643184955949475
Loss in iteration 19 : 0.40272135839072015
Loss in iteration 20 : 0.3890807490585324
Loss in iteration 21 : 0.3760171899216768
Loss in iteration 22 : 0.36404831387991615
Loss in iteration 23 : 0.35369497486185014
Loss in iteration 24 : 0.3454522076231719
Loss in iteration 25 : 0.33972758641685546
Loss in iteration 26 : 0.3367422797943782
Loss in iteration 27 : 0.3364049351080535
Loss in iteration 28 : 0.33819786398753704
Loss in iteration 29 : 0.34116192241609333
Loss in iteration 30 : 0.3440844956418785
Loss in iteration 31 : 0.34589290325652045
Loss in iteration 32 : 0.3460497272080055
Loss in iteration 33 : 0.3446732220044445
Loss in iteration 34 : 0.3423243609928325
Loss in iteration 35 : 0.3396661675166723
Loss in iteration 36 : 0.33721807679212706
Loss in iteration 37 : 0.3352717076937479
Loss in iteration 38 : 0.33391706027037604
Loss in iteration 39 : 0.33310939672209106
Loss in iteration 40 : 0.3327343011058896
Loss in iteration 41 : 0.3326555063476533
Loss in iteration 42 : 0.33274451184554343
Loss in iteration 43 : 0.3328960516678184
Loss in iteration 44 : 0.3330340182101188
Loss in iteration 45 : 0.33311150187403427
Loss in iteration 46 : 0.33310746337643654
Testing accuracy  of updater 2 on alg 0 with rate 7.000000000000001E-4 = 0.7805, training accuracy 0.834250566526384, time elapsed: 613 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.073201657954043
Loss in iteration 3 : 0.7656956767849009
Loss in iteration 4 : 0.37019571078504465
Loss in iteration 5 : 0.38678579735363133
Loss in iteration 6 : 0.3771680291845718
Loss in iteration 7 : 0.36014464678033875
Loss in iteration 8 : 0.36194679933267004
Loss in iteration 9 : 0.3687061078873937
Loss in iteration 10 : 0.37414562877282725
Loss in iteration 11 : 0.3776417127171067
Loss in iteration 12 : 0.37981733677134544
Loss in iteration 13 : 0.3808934423416699
Loss in iteration 14 : 0.38080794786121486
Loss in iteration 15 : 0.37958473220505035
Loss in iteration 16 : 0.3773851785690078
Loss in iteration 17 : 0.3744022734630092
Loss in iteration 18 : 0.370806393517998
Loss in iteration 19 : 0.36675688118453714
Loss in iteration 20 : 0.36241762764794966
Loss in iteration 21 : 0.3579533748914332
Loss in iteration 22 : 0.3535203443484475
Loss in iteration 23 : 0.3492623054598184
Loss in iteration 24 : 0.345309369032657
Loss in iteration 25 : 0.3417747649230768
Loss in iteration 26 : 0.3387497983408277
Loss in iteration 27 : 0.3362987583565974
Loss in iteration 28 : 0.33445425298143605
Loss in iteration 29 : 0.33321329959354634
Loss in iteration 30 : 0.332535338939804
Loss in iteration 31 : 0.33234359409925357
Loss in iteration 32 : 0.33253083575698017
Loss in iteration 33 : 0.33296989259643583
Loss in iteration 34 : 0.33352804095351407
Loss in iteration 35 : 0.3340829882024189
Loss in iteration 36 : 0.3345372100093449
Testing accuracy  of updater 2 on alg 0 with rate 4.0E-4 = 0.78575, training accuracy 0.8391065069601813, time elapsed: 513 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4929761023111823
Loss in iteration 3 : 0.49584581971414426
Loss in iteration 4 : 0.475228892298021
Loss in iteration 5 : 0.43657625863305505
Loss in iteration 6 : 0.3992385368454529
Loss in iteration 7 : 0.3769740358109618
Loss in iteration 8 : 0.36801216396615166
Loss in iteration 9 : 0.36312394633762024
Loss in iteration 10 : 0.35742250392266395
Loss in iteration 11 : 0.35124457674951853
Loss in iteration 12 : 0.3461492299976222
Loss in iteration 13 : 0.3428058749205513
Loss in iteration 14 : 0.3409648025989394
Loss in iteration 15 : 0.3400381441680323
Loss in iteration 16 : 0.33952682963871766
Loss in iteration 17 : 0.3391631428336356
Loss in iteration 18 : 0.3388700311154351
Loss in iteration 19 : 0.33866022764276865
Loss in iteration 20 : 0.33855357509347306
Loss in iteration 21 : 0.33854037353630745
Loss in iteration 22 : 0.33858363545421016
Loss in iteration 23 : 0.33863880379776296
Testing accuracy  of updater 2 on alg 0 with rate 1.0E-4 = 0.77875, training accuracy 0.832308190352865, time elapsed: 273 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4954413300215582
Loss in iteration 3 : 0.48553748610697905
Loss in iteration 4 : 0.4817693533839836
Loss in iteration 5 : 0.4652726837176546
Loss in iteration 6 : 0.43858000869209707
Loss in iteration 7 : 0.41040239576035786
Loss in iteration 8 : 0.3887126303335827
Loss in iteration 9 : 0.37611775141743437
Loss in iteration 10 : 0.3698084067937395
Loss in iteration 11 : 0.36552909635789377
Loss in iteration 12 : 0.3609140959881073
Loss in iteration 13 : 0.35580799669223684
Loss in iteration 14 : 0.3509975714392001
Loss in iteration 15 : 0.347147632397907
Loss in iteration 16 : 0.34444683402431303
Loss in iteration 17 : 0.342710089152994
Loss in iteration 18 : 0.3416069092263843
Loss in iteration 19 : 0.3408375507782329
Loss in iteration 20 : 0.34021183995516674
Loss in iteration 21 : 0.3396507114961059
Loss in iteration 22 : 0.3391481466758621
Loss in iteration 23 : 0.33872596111192965
Loss in iteration 24 : 0.3384008817407916
Loss in iteration 25 : 0.33817029295905127
Testing accuracy  of updater 2 on alg 0 with rate 7.000000000000001E-5 = 0.7795, training accuracy 0.8329556490773713, time elapsed: 376 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5378608758802431
Loss in iteration 3 : 0.4907605997543612
Loss in iteration 4 : 0.48161517104602763
Loss in iteration 5 : 0.47946051888966035
Loss in iteration 6 : 0.47346689992959606
Loss in iteration 7 : 0.4611073428995746
Loss in iteration 8 : 0.44373478189539606
Loss in iteration 9 : 0.424435670687333
Loss in iteration 10 : 0.4065446984650425
Loss in iteration 11 : 0.39244757437823913
Loss in iteration 12 : 0.38288289440945333
Loss in iteration 13 : 0.3770342816894408
Loss in iteration 14 : 0.3732976512645113
Loss in iteration 15 : 0.3701833546826216
Loss in iteration 16 : 0.3668456731041453
Loss in iteration 17 : 0.36311862449150306
Loss in iteration 18 : 0.3592550058891403
Loss in iteration 19 : 0.3556218472012967
Loss in iteration 20 : 0.3524980218421005
Loss in iteration 21 : 0.35000016852707505
Loss in iteration 22 : 0.3480991320702236
Loss in iteration 23 : 0.34667673899172735
Loss in iteration 24 : 0.3455857269150479
Loss in iteration 25 : 0.34469373325447134
Testing accuracy  of updater 2 on alg 0 with rate 4.0E-5 = 0.77025, training accuracy 0.8303658141793461, time elapsed: 303 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6413977969667161
Testing accuracy  of updater 2 on alg 0 with rate 9.999999999999999E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 41 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 4.566306556930491
Loss in iteration 3 : 1.8558920568926884
Loss in iteration 4 : 0.3438950363161287
Loss in iteration 5 : 0.35567863218706436
Loss in iteration 6 : 0.3890842824632514
Loss in iteration 7 : 0.5213377513215014
Loss in iteration 8 : 0.6069782172858637
Loss in iteration 9 : 0.9743256880604787
Loss in iteration 10 : 0.3981899318560972
Loss in iteration 11 : 0.40832572689764435
Loss in iteration 12 : 0.3768061978056493
Loss in iteration 13 : 0.3816340478636038
Loss in iteration 14 : 0.36583332400559654
Loss in iteration 15 : 0.3702145224240373
Loss in iteration 16 : 0.36060835158454496
Loss in iteration 17 : 0.3662556822445905
Loss in iteration 18 : 0.35926961371325206
Loss in iteration 19 : 0.3674464927932927
Loss in iteration 20 : 0.3611434186679194
Loss in iteration 21 : 0.37315409459625926
Loss in iteration 22 : 0.36555691830910503
Loss in iteration 23 : 0.3822623721160365
Loss in iteration 24 : 0.3707611439513415
Loss in iteration 25 : 0.39130685771745144
Loss in iteration 26 : 0.37387877039121264
Loss in iteration 27 : 0.39513865955554306
Loss in iteration 28 : 0.3730022069299697
Loss in iteration 29 : 0.39158065356427746
Loss in iteration 30 : 0.3689045280765899
Loss in iteration 31 : 0.38348641599726735
Loss in iteration 32 : 0.3636692073096097
Loss in iteration 33 : 0.374818028517056
Loss in iteration 34 : 0.35881176658274094
Loss in iteration 35 : 0.36764463154559407
Loss in iteration 36 : 0.3549295330444738
Loss in iteration 37 : 0.362396463760359
Loss in iteration 38 : 0.35208290017045724
Loss in iteration 39 : 0.35883333065803164
Loss in iteration 40 : 0.35012509378230255
Loss in iteration 41 : 0.35656685751127826
Loss in iteration 42 : 0.3488552963166013
Loss in iteration 43 : 0.35522573577721084
Loss in iteration 44 : 0.34806956862329885
Loss in iteration 45 : 0.354481178019414
Loss in iteration 46 : 0.34757451344223567
Loss in iteration 47 : 0.35404207502797685
Loss in iteration 48 : 0.3471956121383419
Loss in iteration 49 : 0.35365882174578916
Loss in iteration 50 : 0.34679019356687707
Loss in iteration 51 : 0.3531410600746288
Loss in iteration 52 : 0.3462619623066671
Loss in iteration 53 : 0.352377435818449
Loss in iteration 54 : 0.34556867000540475
Loss in iteration 55 : 0.3513413500098117
Loss in iteration 56 : 0.3447172637082748
Loss in iteration 57 : 0.35007555511649313
Loss in iteration 58 : 0.34374821135273154
Loss in iteration 59 : 0.34866216398044747
Loss in iteration 60 : 0.342716138534634
Loss in iteration 61 : 0.34719163734892783
Loss in iteration 62 : 0.34167384360256436
Loss in iteration 63 : 0.3457411203060626
Loss in iteration 64 : 0.3406630020819793
Loss in iteration 65 : 0.34436483953617475
Loss in iteration 66 : 0.3397111386599459
Loss in iteration 67 : 0.3430937009753633
Loss in iteration 68 : 0.33883267175329296
Loss in iteration 69 : 0.3419396715341126
Loss in iteration 70 : 0.338031802001993
Loss in iteration 71 : 0.3409014972026876
Loss in iteration 72 : 0.3373057393074492
Loss in iteration 73 : 0.33996988807667095
Loss in iteration 74 : 0.3366475163996812
Loss in iteration 75 : 0.3391315021655992
Loss in iteration 76 : 0.3360481447014244
Loss in iteration 77 : 0.33837170576370246
Loss in iteration 78 : 0.3354981277937646
Loss in iteration 79 : 0.3376763343964457
Loss in iteration 80 : 0.3349884443173773
Loss in iteration 81 : 0.3370327163459722
Loss in iteration 82 : 0.33451112632590996
Loss in iteration 83 : 0.33643017942981246
Loss in iteration 84 : 0.33405954216075656
Loss in iteration 85 : 0.3358602059873073
Loss in iteration 86 : 0.3336284701739656
Loss in iteration 87 : 0.33531635515867564
Loss in iteration 88 : 0.3332140303686663
Loss in iteration 89 : 0.33479403962166426
Loss in iteration 90 : 0.33281352663202673
Loss in iteration 91 : 0.33429022253967233
Loss in iteration 92 : 0.33242524124935663
Loss in iteration 93 : 0.3338030849494862
Loss in iteration 94 : 0.33204821410714014
Loss in iteration 95 : 0.33333170090518366
Loss in iteration 96 : 0.3316820304402074
Loss in iteration 97 : 0.3328757459490682
Loss in iteration 98 : 0.3313266329735265
Loss in iteration 99 : 0.3324352537971495
Loss in iteration 100 : 0.3309821671546835
Loss in iteration 101 : 0.33201042701658795
Loss in iteration 102 : 0.33064886228534734
Loss in iteration 103 : 0.33160150046277653
Loss in iteration 104 : 0.33032694701386367
Loss in iteration 105 : 0.3312086515582509
Loss in iteration 106 : 0.33001659488162977
Loss in iteration 107 : 0.33083194899234875
Loss in iteration 108 : 0.32971789424119774
Loss in iteration 109 : 0.3304713306884112
Loss in iteration 110 : 0.32943083657052347
Loss in iteration 111 : 0.33012660239482644
Loss in iteration 112 : 0.32915531764255435
Loss in iteration 113 : 0.3297974494904056
Loss in iteration 114 : 0.32889114685109866
Loss in iteration 115 : 0.32948345613149715
Loss in iteration 116 : 0.3286380609907802
Loss in iteration 117 : 0.3291841274109061
Loss in iteration 118 : 0.32839573976903824
Loss in iteration 119 : 0.32889891157019663
Loss in iteration 120 : 0.32816382119155113
Loss in iteration 121 : 0.32862722042571463
Loss in iteration 122 : 0.3279419156648514
Loss in iteration 123 : 0.3283684470186607
Loss in iteration 124 : 0.32772961819429436
Loss in iteration 125 : 0.32812198010366456
Loss in iteration 126 : 0.327526518437096
Loss in iteration 127 : 0.3278872154895881
Loss in iteration 128 : 0.3273322086243307
Loss in iteration 129 : 0.32766356448696243
Loss in iteration 130 : 0.3271462895205301
Loss in iteration 131 : 0.32745045984125515
Loss in iteration 132 : 0.32696837467130146
Loss in iteration 133 : 0.3272473595766228
Loss in iteration 134 : 0.3267980932203267
Loss in iteration 135 : 0.32705374916922436
Loss in iteration 136 : 0.3266350915750219
Loss in iteration 137 : 0.32686914243388027
Loss in iteration 138 : 0.32647903417830826
Loss in iteration 139 : 0.32669308145759757
Loss in iteration 140 : 0.3263296036118521
Loss in iteration 141 : 0.32652513585833676
Loss in iteration 142 : 0.32618650022029516
Loss in iteration 143 : 0.3263649015934014
Loss in iteration 144 : 0.3260494414104696
Loss in iteration 145 : 0.326211999492725
Loss in iteration 146 : 0.3259181607469103
Loss in iteration 147 : 0.32606607364960444
Loss in iteration 148 : 0.3257924069362384
Loss in iteration 149 : 0.32592678976580813
Loss in iteration 150 : 0.3256719427688329
Loss in iteration 151 : 0.32579383351907243
Loss in iteration 152 : 0.3255565440663598
Loss in iteration 153 : 0.3256669089982973
Loss in iteration 154 : 0.3254459986680315
Loss in iteration 155 : 0.32554573723433916
Loss in iteration 156 : 0.32534010547631964
Loss in iteration 157 : 0.32543005484139176
Loss in iteration 158 : 0.3252386735736939
Loss in iteration 159 : 0.3253196127746376
Loss in iteration 160 : 0.3251415214153647
Loss in iteration 161 : 0.3252141752034228
Loss in iteration 162 : 0.3250484760983048
Loss in iteration 163 : 0.32511351849498027
Loss in iteration 164 : 0.3249593727037546
Loss in iteration 165 : 0.3250174303011841
Loss in iteration 166 : 0.32487405370848094
Loss in iteration 167 : 0.3249257087394557
Loss in iteration 168 : 0.3247923684589741
Loss in iteration 169 : 0.324838161658467
Loss in iteration 170 : 0.3247141727023628
Loss in iteration 171 : 0.3247546059793806
Loss in iteration 172 : 0.3246393281677977
Loss in iteration 173 : 0.3246748671038281
Loss in iteration 174 : 0.32456770219231545
Loss in iteration 175 : 0.32459877838053514
Loss in iteration 176 : 0.32449916738566875
Loss in iteration 177 : 0.3245261806232846
Loss in iteration 178 : 0.3244336013290966
Loss in iteration 179 : 0.3244569216737438
Loss in iteration 180 : 0.32437088630361155
Loss in iteration 181 : 0.3243908560035021
Loss in iteration 182 : 0.32431090904390214
Loss in iteration 183 : 0.3243278443504137
Loss in iteration 184 : 0.3242535605145085
Loss in iteration 185 : 0.32426775338503366
Loss in iteration 186 : 0.32419873570538765
Loss in iteration 187 : 0.3242104554035646
Loss in iteration 188 : 0.32414633344442545
Loss in iteration 189 : 0.3241558280442524
Loss in iteration 190 : 0.32409625622483207
Loss in iteration 191 : 0.3241037540246351
Loss in iteration 192 : 0.32404841004567336
Loss in iteration 193 : 0.32405412089744157
Loss in iteration 194 : 0.3240027042640734
Loss in iteration 195 : 0.32400682082327475
Loss in iteration 196 : 0.3239590514578534
Loss in iteration 197 : 0.32396175035847574
Loss in iteration 198 : 0.32391736729755666
Loss in iteration 199 : 0.3239188102567953
Loss in iteration 200 : 0.32387757042697746
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999999 = 0.7855, training accuracy 0.8413726124959534, time elapsed: 2524 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.186530291692375
Loss in iteration 3 : 0.5302184653939052
Loss in iteration 4 : 0.3564016351641912
Loss in iteration 5 : 0.3509681476611616
Loss in iteration 6 : 0.34556594098572413
Loss in iteration 7 : 0.3426576698436021
Loss in iteration 8 : 0.3404827887079026
Loss in iteration 9 : 0.3389336340903768
Loss in iteration 10 : 0.33770750486700074
Loss in iteration 11 : 0.33670195744575554
Loss in iteration 12 : 0.3358533227786099
Loss in iteration 13 : 0.33512671157597146
Loss in iteration 14 : 0.33449896464835516
Loss in iteration 15 : 0.33395304923099517
Loss in iteration 16 : 0.33347567906786196
Loss in iteration 17 : 0.3330561466781727
Loss in iteration 18 : 0.33268570347790855
Loss in iteration 19 : 0.3323571307154461
Loss in iteration 20 : 0.33206443152707854
Loss in iteration 21 : 0.3318025937058408
Loss in iteration 22 : 0.33156740559919096
Loss in iteration 23 : 0.3313553107907055
Loss in iteration 24 : 0.33116329261083766
Loss in iteration 25 : 0.3309887815626118
Loss in iteration 26 : 0.33082958061440776
Loss in iteration 27 : 0.3306838045098268
Loss in iteration 28 : 0.3305498301676712
Loss in iteration 29 : 0.33042625591568714
Loss in iteration 30 : 0.33031186780832106
Loss in iteration 31 : 0.3302056116602958
Loss in iteration 32 : 0.3301065697189433
Loss in iteration 33 : 0.3300139411217266
Loss in iteration 34 : 0.32992702545830493
Loss in iteration 35 : 0.3298452088911899
Loss in iteration 36 : 0.3297679523946738
Loss in iteration 37 : 0.32969478175501116
Loss in iteration 38 : 0.3296252790410022
Loss in iteration 39 : 0.32955907530683004
Loss in iteration 40 : 0.32949584433138185
Loss in iteration 41 : 0.3294352972323655
Loss in iteration 42 : 0.3293771778212025
Loss in iteration 43 : 0.32932125858712985
Loss in iteration 44 : 0.32926733721735824
Loss in iteration 45 : 0.329215233575179
Loss in iteration 46 : 0.32916478707036073
Loss in iteration 47 : 0.3291158543664313
Loss in iteration 48 : 0.32906830737797477
Loss in iteration 49 : 0.3290220315181653
Loss in iteration 50 : 0.3289769241626894
Loss in iteration 51 : 0.32893289330119413
Loss in iteration 52 : 0.3288898563515481
Loss in iteration 53 : 0.32884773911575077
Loss in iteration 54 : 0.32880647485929965
Loss in iteration 55 : 0.32876600349833085
Loss in iteration 56 : 0.32872627088102413
Loss in iteration 57 : 0.32868722815156437
Loss in iteration 58 : 0.32864883118653276
Loss in iteration 59 : 0.3286110400949264
Loss in iteration 60 : 0.32857381877415054
Loss in iteration 61 : 0.32853713451533534
Loss in iteration 62 : 0.32850095765213244
Loss in iteration 63 : 0.3284652612479392
Loss in iteration 64 : 0.3284300208170776
Loss in iteration 65 : 0.3283952140760377
Loss in iteration 66 : 0.3283608207213656
Loss in iteration 67 : 0.32832682223117354
Loss in iteration 68 : 0.3282932016876411
Loss in iteration 69 : 0.32825994361815736
Loss in iteration 70 : 0.32822703385305385
Loss in iteration 71 : 0.3281944593981092
Loss in iteration 72 : 0.32816220832021353
Loss in iteration 73 : 0.32813026964476893
Loss in iteration 74 : 0.32809863326357264
Loss in iteration 75 : 0.3280672898520517
Loss in iteration 76 : 0.3280362307948626
Loss in iteration 77 : 0.3280054481189774
Loss in iteration 78 : 0.32797493443346065
Loss in iteration 79 : 0.32794468287524514
Loss in iteration 80 : 0.32791468706028865
Loss in iteration 81 : 0.3278849410395397
Loss in iteration 82 : 0.32785543925924054
Loss in iteration 83 : 0.3278261765251028
Loss in iteration 84 : 0.32779714796997916
Loss in iteration 85 : 0.3277683490246634
Loss in iteration 86 : 0.32773977539151006
Loss in iteration 87 : 0.3277114230205945
Loss in iteration 88 : 0.3276832880881481
Loss in iteration 89 : 0.3276553669770563
Loss in iteration 90 : 0.32762765625919804
Loss in iteration 91 : 0.32760015267946446
Loss in iteration 92 : 0.3275728531412764
Loss in iteration 93 : 0.3275457546934568
Loss in iteration 94 : 0.32751885451833296
Loss in iteration 95 : 0.3274921499209428
Loss in iteration 96 : 0.3274656383192358
Loss in iteration 97 : 0.32743931723518166
Loss in iteration 98 : 0.32741318428669014
Loss in iteration 99 : 0.3273872371802668
Loss in iteration 100 : 0.3273614737043428
Loss in iteration 101 : 0.3273358917231967
Loss in iteration 102 : 0.3273104891714299
Loss in iteration 103 : 0.32728526404893493
Loss in iteration 104 : 0.32726021441631054
Loss in iteration 105 : 0.32723533839067914
Loss in iteration 106 : 0.32721063414187523
Loss in iteration 107 : 0.3271860998889692
Loss in iteration 108 : 0.3271617338970856
Loss in iteration 109 : 0.3271375344745028
Loss in iteration 110 : 0.32711349996999584
Loss in iteration 111 : 0.3270896287704103
Loss in iteration 112 : 0.32706591929843426
Loss in iteration 113 : 0.3270423700105648
Loss in iteration 114 : 0.3270189793952338
Loss in iteration 115 : 0.32699574597109643
Loss in iteration 116 : 0.3269726682854557
Loss in iteration 117 : 0.3269497449128117
Loss in iteration 118 : 0.326926974453531
Loss in iteration 119 : 0.3269043555326172
Loss in iteration 120 : 0.32688188679858277
Loss in iteration 121 : 0.3268595669224004
Loss in iteration 122 : 0.32683739459654165
Loss in iteration 123 : 0.3268153685340836
Loss in iteration 124 : 0.326793487467883
Loss in iteration 125 : 0.32677175014981247
Loss in iteration 126 : 0.3267501553500454
Loss in iteration 127 : 0.32672870185640107
Loss in iteration 128 : 0.3267073884737253
Loss in iteration 129 : 0.326686214023324
Loss in iteration 130 : 0.3266651773424245
Loss in iteration 131 : 0.3266442772836792
Loss in iteration 132 : 0.32662351271469875
Loss in iteration 133 : 0.326602882517615
Loss in iteration 134 : 0.3265823855886716
Loss in iteration 135 : 0.3265620208378379
Loss in iteration 136 : 0.32654178718844756
Loss in iteration 137 : 0.32652168357685835
Loss in iteration 138 : 0.3265017089521242
Loss in iteration 139 : 0.3264818622757016
Loss in iteration 140 : 0.3264621425211477
Loss in iteration 141 : 0.3264425486738585
Loss in iteration 142 : 0.32642307973080303
Loss in iteration 143 : 0.3264037347002788
Loss in iteration 144 : 0.3263845126016777
Loss in iteration 145 : 0.3263654124652593
Loss in iteration 146 : 0.32634643333193925
Loss in iteration 147 : 0.3263275742530836
Loss in iteration 148 : 0.32630883429031454
Loss in iteration 149 : 0.3262902125153189
Loss in iteration 150 : 0.32627170800967287
Loss in iteration 151 : 0.3262533198646611
Loss in iteration 152 : 0.326235047181119
Loss in iteration 153 : 0.326216889069265
Loss in iteration 154 : 0.32619884464854815
Loss in iteration 155 : 0.3261809130474957
Loss in iteration 156 : 0.3261630934035717
Loss in iteration 157 : 0.3261453848630325
Loss in iteration 158 : 0.3261277865807946
Loss in iteration 159 : 0.3261102977203001
Loss in iteration 160 : 0.3260929174533881
Loss in iteration 161 : 0.3260756449601743
Loss in iteration 162 : 0.32605847942892374
Loss in iteration 163 : 0.32604142005593845
Loss in iteration 164 : 0.3260244660454401
Loss in iteration 165 : 0.32600761660945793
Loss in iteration 166 : 0.3259908709677216
Loss in iteration 167 : 0.32597422834755263
Loss in iteration 168 : 0.3259576879837618
Loss in iteration 169 : 0.32594124911854633
Loss in iteration 170 : 0.3259249110013928
Loss in iteration 171 : 0.32590867288897885
Loss in iteration 172 : 0.32589253404507607
Loss in iteration 173 : 0.32587649374046007
Loss in iteration 174 : 0.32586055125281915
Loss in iteration 175 : 0.3258447058666631
Loss in iteration 176 : 0.3258289568732367
Loss in iteration 177 : 0.3258133035704344
Loss in iteration 178 : 0.3257977452627155
Loss in iteration 179 : 0.3257822812610213
Loss in iteration 180 : 0.3257669108826954
Loss in iteration 181 : 0.325751633451403
Loss in iteration 182 : 0.3257364482970532
Loss in iteration 183 : 0.32572135475572106
Loss in iteration 184 : 0.3257063521695751
Loss in iteration 185 : 0.3256914398867994
Loss in iteration 186 : 0.3256766172615238
Loss in iteration 187 : 0.3256618836537506
Loss in iteration 188 : 0.32564723842928567
Loss in iteration 189 : 0.32563268095966647
Loss in iteration 190 : 0.32561821062209645
Loss in iteration 191 : 0.3256038267993782
Loss in iteration 192 : 0.32558952887984555
Loss in iteration 193 : 0.32557531625729946
Loss in iteration 194 : 0.32556118833094444
Loss in iteration 195 : 0.32554714450532507
Loss in iteration 196 : 0.3255331841902668
Loss in iteration 197 : 0.3255193068008089
Loss in iteration 198 : 0.32550551175715114
Loss in iteration 199 : 0.3254917984845908
Loss in iteration 200 : 0.325478166413465
Testing accuracy  of updater 3 on alg 0 with rate 0.06999999999999999 = 0.78625, training accuracy 0.8400776950469407, time elapsed: 2732 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6695416433334421
Loss in iteration 3 : 0.4538509138246102
Loss in iteration 4 : 0.4013680585619697
Loss in iteration 5 : 0.3915651398955334
Loss in iteration 6 : 0.38368484526681795
Loss in iteration 7 : 0.37721118217672406
Loss in iteration 8 : 0.3718174548223831
Loss in iteration 9 : 0.3672691939159538
Loss in iteration 10 : 0.36339397512064153
Loss in iteration 11 : 0.3600623431404239
Loss in iteration 12 : 0.3571753660049812
Loss in iteration 13 : 0.3546562274279466
Loss in iteration 14 : 0.35244443036224604
Loss in iteration 15 : 0.35049172010088475
Loss in iteration 16 : 0.3487591641165507
Loss in iteration 17 : 0.34721502528514486
Loss in iteration 18 : 0.34583318914326394
Loss in iteration 19 : 0.3445919845061043
Loss in iteration 20 : 0.34347328767151303
Loss in iteration 21 : 0.34246183397433144
Loss in iteration 22 : 0.3415446829370235
Loss in iteration 23 : 0.3407107985767345
Loss in iteration 24 : 0.33995071701795837
Loss in iteration 25 : 0.33925628098491084
Loss in iteration 26 : 0.3386204260223876
Loss in iteration 27 : 0.33803700708694506
Loss in iteration 28 : 0.3375006569092206
Loss in iteration 29 : 0.33700666955653685
Loss in iteration 30 : 0.33655090413114697
Loss in iteration 31 : 0.3361297046685331
Loss in iteration 32 : 0.33573983315407596
Loss in iteration 33 : 0.33537841322758344
Loss in iteration 34 : 0.33504288264567583
Loss in iteration 35 : 0.3347309529596372
Loss in iteration 36 : 0.33444057516856107
Loss in iteration 37 : 0.3341699103449212
Loss in iteration 38 : 0.3339173044171158
Loss in iteration 39 : 0.333681266442542
Loss in iteration 40 : 0.3334604498238027
Loss in iteration 41 : 0.3332536360163509
Loss in iteration 42 : 0.33305972035317716
Loss in iteration 43 : 0.33287769967488207
Loss in iteration 44 : 0.33270666150466777
Loss in iteration 45 : 0.33254577454969286
Loss in iteration 46 : 0.332394280344755
Loss in iteration 47 : 0.33225148588273445
Loss in iteration 48 : 0.33211675709990623
Loss in iteration 49 : 0.3319895131038832
Loss in iteration 50 : 0.3318692210484045
Loss in iteration 51 : 0.33175539157299117
Loss in iteration 52 : 0.3316475747370558
Loss in iteration 53 : 0.3315453563878834
Loss in iteration 54 : 0.33144835491016106
Loss in iteration 55 : 0.33135621831178974
Loss in iteration 56 : 0.3312686216067036
Loss in iteration 57 : 0.33118526446052465
Loss in iteration 58 : 0.33110586906930206
Loss in iteration 59 : 0.3310301782452981
Loss in iteration 60 : 0.33095795368706593
Loss in iteration 61 : 0.330888974413833
Loss in iteration 62 : 0.33082303534661334
Loss in iteration 63 : 0.33075994602060427
Loss in iteration 64 : 0.3306995294151852
Loss in iteration 65 : 0.3306416208894669
Loss in iteration 66 : 0.33058606721269346
Loss in iteration 67 : 0.3305327256800108
Loss in iteration 68 : 0.3304814633051808
Loss in iteration 69 : 0.3304321560827245
Loss in iteration 70 : 0.3303846883128279
Loss in iteration 71 : 0.33033895198301155
Loss in iteration 72 : 0.33029484620124994
Loss in iteration 73 : 0.3302522766757457
Loss in iteration 74 : 0.33021115523706696
Loss in iteration 75 : 0.33017139939881646
Loss in iteration 76 : 0.3301329319533453
Loss in iteration 77 : 0.33009568059942174
Loss in iteration 78 : 0.3300595775990277
Loss in iteration 79 : 0.3300245594607544
Loss in iteration 80 : 0.3299905666475056
Loss in iteration 81 : 0.32995754330643867
Loss in iteration 82 : 0.32992543701926674
Loss in iteration 83 : 0.3298941985712184
Loss in iteration 84 : 0.32986378173711145
Loss in iteration 85 : 0.329834143083142
Loss in iteration 86 : 0.32980524178311127
Loss in iteration 87 : 0.3297770394479228
Loss in iteration 88 : 0.3297494999673061
Loss in iteration 89 : 0.3297225893627855
Loss in iteration 90 : 0.3296962756510245
Loss in iteration 91 : 0.329670528716742
Loss in iteration 92 : 0.3296453201944451
Loss in iteration 93 : 0.3296206233583369
Loss in iteration 94 : 0.3295964130197515
Loss in iteration 95 : 0.32957266543157004
Loss in iteration 96 : 0.32954935819908304
Loss in iteration 97 : 0.32952647019684445
Loss in iteration 98 : 0.32950398149105087
Loss in iteration 99 : 0.32948187326706535
Loss in iteration 100 : 0.32946012776171146
Loss in iteration 101 : 0.3294387281999798
Loss in iteration 102 : 0.3294176587358557
Loss in iteration 103 : 0.3293969043969603
Loss in iteration 104 : 0.3293764510327462
Loss in iteration 105 : 0.3293562852659962
Loss in iteration 106 : 0.32933639444739926
Loss in iteration 107 : 0.32931676661298714
Loss in iteration 108 : 0.32929739044424655
Loss in iteration 109 : 0.3292782552307121
Loss in iteration 110 : 0.3292593508348885
Loss in iteration 111 : 0.3292406676593246
Loss in iteration 112 : 0.3292221966157248
Loss in iteration 113 : 0.3292039290959347
Loss in iteration 114 : 0.3291858569446942
Loss in iteration 115 : 0.32916797243404283
Loss in iteration 116 : 0.32915026823925747
Loss in iteration 117 : 0.329132737416243
Loss in iteration 118 : 0.3291153733802572
Loss in iteration 119 : 0.32909816988591345
Loss in iteration 120 : 0.3290811210083534
Loss in iteration 121 : 0.32906422112553446
Loss in iteration 122 : 0.3290474649015562
Loss in iteration 123 : 0.3290308472709498
Loss in iteration 124 : 0.329014363423895
Loss in iteration 125 : 0.3289980087922796
Loss in iteration 126 : 0.32898177903657044
Loss in iteration 127 : 0.32896567003343563
Loss in iteration 128 : 0.32894967786407425
Loss in iteration 129 : 0.3289337988032116
Loss in iteration 130 : 0.32891802930872077
Loss in iteration 131 : 0.32890236601182643
Loss in iteration 132 : 0.32888680570786843
Loss in iteration 133 : 0.32887134534757645
Loss in iteration 134 : 0.32885598202883687
Loss in iteration 135 : 0.3288407129889202
Loss in iteration 136 : 0.328825535597137
Loss in iteration 137 : 0.3288104473479027
Loss in iteration 138 : 0.3287954458541861
Loss in iteration 139 : 0.3287805288413153
Loss in iteration 140 : 0.3287656941411308
Loss in iteration 141 : 0.32875093968644936
Loss in iteration 142 : 0.3287362635058357
Loss in iteration 143 : 0.32872166371865846
Loss in iteration 144 : 0.32870713853040917
Loss in iteration 145 : 0.3286926862282822
Loss in iteration 146 : 0.3286783051769886
Loss in iteration 147 : 0.32866399381479294
Loss in iteration 148 : 0.32864975064976626
Loss in iteration 149 : 0.3286355742562402
Loss in iteration 150 : 0.3286214632714443
Loss in iteration 151 : 0.3286074163923305
Loss in iteration 152 : 0.32859343237255983
Loss in iteration 153 : 0.32857951001965113
Loss in iteration 154 : 0.32856564819227935
Loss in iteration 155 : 0.328551845797717
Loss in iteration 156 : 0.3285381017894096
Loss in iteration 157 : 0.32852441516467895
Loss in iteration 158 : 0.32851078496254404
Loss in iteration 159 : 0.3284972102616584
Loss in iteration 160 : 0.32848369017835555
Loss in iteration 161 : 0.3284702238647914
Loss in iteration 162 : 0.3284568105071862
Loss in iteration 163 : 0.3284434493241566
Loss in iteration 164 : 0.3284301395651383
Loss in iteration 165 : 0.3284168805088775
Loss in iteration 166 : 0.3284036714620153
Loss in iteration 167 : 0.32839051175773315
Loss in iteration 168 : 0.3283774007544729
Loss in iteration 169 : 0.3283643378347209
Loss in iteration 170 : 0.32835132240385384
Loss in iteration 171 : 0.328338353889045
Loss in iteration 172 : 0.3283254317382197
Loss in iteration 173 : 0.32831255541907484
Loss in iteration 174 : 0.32829972441813743
Loss in iteration 175 : 0.32828693823987554
Loss in iteration 176 : 0.3282741964058531
Loss in iteration 177 : 0.3282614984539271
Loss in iteration 178 : 0.3282488439374857
Loss in iteration 179 : 0.3282362324247241
Loss in iteration 180 : 0.32822366349795334
Loss in iteration 181 : 0.32821113675295155
Loss in iteration 182 : 0.3281986517983381
Loss in iteration 183 : 0.32818620825498673
Loss in iteration 184 : 0.3281738057554599
Loss in iteration 185 : 0.3281614439434793
Loss in iteration 186 : 0.32814912247341765
Loss in iteration 187 : 0.32813684100981594
Loss in iteration 188 : 0.3281245992269246
Loss in iteration 189 : 0.3281123968082692
Loss in iteration 190 : 0.3281002334462354
Loss in iteration 191 : 0.3280881088416726
Loss in iteration 192 : 0.32807602270352276
Loss in iteration 193 : 0.3280639747484595
Loss in iteration 194 : 0.32805196470055176
Loss in iteration 195 : 0.3280399922909404
Loss in iteration 196 : 0.3280280572575296
Loss in iteration 197 : 0.32801615934469525
Loss in iteration 198 : 0.3280042983030076
Loss in iteration 199 : 0.3279924738889633
Loss in iteration 200 : 0.327980685864739
Testing accuracy  of updater 3 on alg 0 with rate 0.04 = 0.7845, training accuracy 0.838459048235675, time elapsed: 2759 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5064727734233964
Loss in iteration 3 : 0.4869499937676805
Loss in iteration 4 : 0.47675864233522725
Loss in iteration 5 : 0.46910486630406295
Loss in iteration 6 : 0.4625231382892608
Loss in iteration 7 : 0.45656774210438594
Loss in iteration 8 : 0.4510697897034528
Loss in iteration 9 : 0.4459501133651346
Loss in iteration 10 : 0.44116241811443974
Loss in iteration 11 : 0.4366739822862908
Loss in iteration 12 : 0.4324585893323987
Loss in iteration 13 : 0.42849376336875067
Loss in iteration 14 : 0.4247595987040863
Loss in iteration 15 : 0.42123820254421573
Loss in iteration 16 : 0.4179133824197836
Loss in iteration 17 : 0.41477043662571456
Loss in iteration 18 : 0.4117959923396245
Loss in iteration 19 : 0.40897786951709514
Testing accuracy  of updater 3 on alg 0 with rate 0.01 = 0.64675, training accuracy 0.7416639689219813, time elapsed: 213 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5483514960786048
Loss in iteration 3 : 0.512129561546811
Loss in iteration 4 : 0.49617234900181667
Loss in iteration 5 : 0.4866616405110179
Loss in iteration 6 : 0.479824682875653
Testing accuracy  of updater 3 on alg 0 with rate 0.007 = 0.5, training accuracy 0.6474587245063127, time elapsed: 99 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6042457479373602
Loss in iteration 3 : 0.5602692353696495
Loss in iteration 4 : 0.5354944345059637
Testing accuracy  of updater 3 on alg 0 with rate 0.004 = 0.5, training accuracy 0.6474587245063127, time elapsed: 64 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6701898931037181
Testing accuracy  of updater 3 on alg 0 with rate 9.999999999999992E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 35 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 46 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 7.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 37 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 4.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 34 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 30 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 0.7 = 0.5, training accuracy 0.6474587245063127, time elapsed: 63 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 0.4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 45 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6674072936956095
Testing accuracy  of updater 4 on alg 0 with rate 0.09999999999999998 = 0.5, training accuracy 0.6474587245063127, time elapsed: 36 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.4573948681767481
Loss in iteration 3 : 0.6549313409854222
Loss in iteration 4 : 0.38184125752162285
Loss in iteration 5 : 0.37595115190208306
Loss in iteration 6 : 0.3673619470511759
Loss in iteration 7 : 0.3658117722991448
Loss in iteration 8 : 0.35930714401473757
Loss in iteration 9 : 0.36073048868292523
Loss in iteration 10 : 0.3555359303655162
Loss in iteration 11 : 0.35993047192600597
Loss in iteration 12 : 0.3551857067368805
Loss in iteration 13 : 0.3630339102014079
Loss in iteration 14 : 0.3571827955364543
Loss in iteration 15 : 0.3683676470690956
Loss in iteration 16 : 0.3592556709098529
Loss in iteration 17 : 0.37204046065955915
Loss in iteration 18 : 0.3592206523535124
Loss in iteration 19 : 0.37148397645614256
Loss in iteration 20 : 0.3575359443577001
Loss in iteration 21 : 0.3688850406323172
Loss in iteration 22 : 0.35612403066724124
Loss in iteration 23 : 0.36751050259399154
Loss in iteration 24 : 0.3560735315497049
Loss in iteration 25 : 0.3685482175508889
Loss in iteration 26 : 0.35728920686190896
Loss in iteration 27 : 0.371319657070488
Loss in iteration 28 : 0.35885073249832955
Loss in iteration 29 : 0.3740182333537004
Loss in iteration 30 : 0.3597095653039918
Loss in iteration 31 : 0.37508741550418806
Loss in iteration 32 : 0.3596362385401285
Loss in iteration 33 : 0.374692320822024
Loss in iteration 34 : 0.35925079683693933
Loss in iteration 35 : 0.3741576328522898
Loss in iteration 36 : 0.35917346795841193
Loss in iteration 37 : 0.37436787360471824
Loss in iteration 38 : 0.35951647087156136
Loss in iteration 39 : 0.37522386096154864
Loss in iteration 40 : 0.359980001719077
Loss in iteration 41 : 0.3760675841522467
Loss in iteration 42 : 0.3602277885114409
Loss in iteration 43 : 0.37639887211645084
Loss in iteration 44 : 0.3601942729079201
Loss in iteration 45 : 0.3762815843013099
Loss in iteration 46 : 0.3600588358662821
Loss in iteration 47 : 0.37611016317548884
Loss in iteration 48 : 0.36000276498904493
Loss in iteration 49 : 0.3761469704391211
Loss in iteration 50 : 0.36005482627796187
Loss in iteration 51 : 0.3763501242549723
Loss in iteration 52 : 0.36012461268995705
Loss in iteration 53 : 0.3765235483722209
Loss in iteration 54 : 0.3601250099393065
Loss in iteration 55 : 0.37654509042722056
Loss in iteration 56 : 0.3600497110007014
Loss in iteration 57 : 0.37645238444440865
Loss in iteration 58 : 0.3599516470233764
Loss in iteration 59 : 0.37635522236922464
Loss in iteration 60 : 0.35987604361917885
Loss in iteration 61 : 0.37631445544573316
Loss in iteration 62 : 0.3598248501342546
Loss in iteration 63 : 0.3763086549000825
Loss in iteration 64 : 0.35977207713707926
Loss in iteration 65 : 0.3762848391278621
Loss in iteration 66 : 0.35969790108067984
Loss in iteration 67 : 0.37621822907844377
Loss in iteration 68 : 0.35960484128320563
Loss in iteration 69 : 0.3761252479639342
Loss in iteration 70 : 0.3595080807327864
Loss in iteration 71 : 0.37603492199036787
Loss in iteration 72 : 0.3594179922827207
Loss in iteration 73 : 0.3759594266326869
Loss in iteration 74 : 0.35933330607702535
Loss in iteration 75 : 0.3758903254308503
Loss in iteration 76 : 0.35924702730598146
Loss in iteration 77 : 0.37581431704165996
Loss in iteration 78 : 0.3591551735805099
Loss in iteration 79 : 0.37572739525492205
Loss in iteration 80 : 0.3590595611776149
Loss in iteration 81 : 0.37563545989001945
Loss in iteration 82 : 0.3589643589029685
Loss in iteration 83 : 0.37554582196482605
Loss in iteration 84 : 0.3588717831698853
Loss in iteration 85 : 0.3754605356302427
Loss in iteration 86 : 0.3587810842198493
Loss in iteration 87 : 0.3753768158287061
Loss in iteration 88 : 0.35869050768888083
Loss in iteration 89 : 0.3752915524024355
Loss in iteration 90 : 0.3585993923386121
Loss in iteration 91 : 0.37520440355839557
Loss in iteration 92 : 0.3585084937559999
Loss in iteration 93 : 0.37511725046523525
Loss in iteration 94 : 0.35841890670594595
Loss in iteration 95 : 0.37503184948266405
Loss in iteration 96 : 0.35833106711515095
Loss in iteration 97 : 0.374948442254689
Loss in iteration 98 : 0.3582446938884441
Loss in iteration 99 : 0.37486620748780936
Loss in iteration 100 : 0.3581593754680279
Loss in iteration 101 : 0.37478447246561397
Loss in iteration 102 : 0.3580750422956307
Loss in iteration 103 : 0.3747033280083324
Loss in iteration 104 : 0.3579919491307698
Loss in iteration 105 : 0.37462332348734817
Loss in iteration 106 : 0.35791036641892265
Loss in iteration 107 : 0.3745448563762787
Loss in iteration 108 : 0.35783036451990596
Loss in iteration 109 : 0.37446791424483344
Loss in iteration 110 : 0.3577518492263144
Loss in iteration 111 : 0.374392269083183
Loss in iteration 112 : 0.3576747245054049
Loss in iteration 113 : 0.37431778428088786
Loss in iteration 114 : 0.35759899125215766
Loss in iteration 115 : 0.3742445203853632
Loss in iteration 116 : 0.3575247187089978
Loss in iteration 117 : 0.37417262116091754
Loss in iteration 118 : 0.35745196191317075
Loss in iteration 119 : 0.3741021636359612
Loss in iteration 120 : 0.3573807191801611
Loss in iteration 121 : 0.3740331195063499
Loss in iteration 122 : 0.3573109529913094
Loss in iteration 123 : 0.3739654213584191
Loss in iteration 124 : 0.3572426325103197
Loss in iteration 125 : 0.37389903629717597
Loss in iteration 126 : 0.3571757522027547
Loss in iteration 127 : 0.37383397912266453
Loss in iteration 128 : 0.35711031917686403
Loss in iteration 129 : 0.37377027613817065
Loss in iteration 130 : 0.35704633239242367
Loss in iteration 131 : 0.3737079304209203
Loss in iteration 132 : 0.35698377547253135
Loss in iteration 133 : 0.3736469191467596
Loss in iteration 134 : 0.3569226246384673
Loss in iteration 135 : 0.3735872136393833
Loss in iteration 136 : 0.35686285924633254
Loss in iteration 137 : 0.37352879604169736
Loss in iteration 138 : 0.3568044647281797
Loss in iteration 139 : 0.3734716591938807
Loss in iteration 140 : 0.35674742827269923
Loss in iteration 141 : 0.3734157962086423
Loss in iteration 142 : 0.3566917338979992
Loss in iteration 143 : 0.3733611929666515
Loss in iteration 144 : 0.35663736162026066
Loss in iteration 145 : 0.37330782922394035
Loss in iteration 146 : 0.356584290016641
Loss in iteration 147 : 0.37325568423238675
Loss in iteration 148 : 0.35653249871287684
Loss in iteration 149 : 0.3732047402734908
Loss in iteration 150 : 0.356481968675743
Loss in iteration 151 : 0.3731549818072033
Loss in iteration 152 : 0.35643268092459474
Loss in iteration 153 : 0.3731063926846495
Loss in iteration 154 : 0.3563846154472302
Loss in iteration 155 : 0.3730589546775361
Loss in iteration 156 : 0.3563377512534696
Loss in iteration 157 : 0.3730126481895708
Loss in iteration 158 : 0.3562920671356034
Loss in iteration 159 : 0.37296745374710993
Loss in iteration 160 : 0.3562475422323546
Loss in iteration 161 : 0.37292335269055255
Loss in iteration 162 : 0.3562041560003789
Loss in iteration 163 : 0.37288032678224525
Loss in iteration 164 : 0.3561618878685207
Loss in iteration 165 : 0.3728383575127635
Loss in iteration 166 : 0.35612071702696746
Loss in iteration 167 : 0.37279742586179737
Loss in iteration 168 : 0.3560806225127014
Loss in iteration 169 : 0.3727575125815753
Loss in iteration 170 : 0.35604158342689834
Loss in iteration 171 : 0.37271859857841405
Loss in iteration 172 : 0.35600357906185426
Loss in iteration 173 : 0.3726806650366847
Loss in iteration 174 : 0.3559665888766143
Loss in iteration 175 : 0.37264369328917707
Loss in iteration 176 : 0.3559305924161946
Loss in iteration 177 : 0.37260766466039297
Loss in iteration 178 : 0.35589556928278626
Loss in iteration 179 : 0.37257256044769327
Loss in iteration 180 : 0.3558614991795426
Loss in iteration 181 : 0.3725383620200192
Loss in iteration 182 : 0.3558283619742427
Loss in iteration 183 : 0.3725050509157691
Loss in iteration 184 : 0.35579613773105295
Loss in iteration 185 : 0.3724726088652134
Loss in iteration 186 : 0.35576480670536964
Loss in iteration 187 : 0.3724410177568258
Loss in iteration 188 : 0.35573434933056836
Loss in iteration 189 : 0.372410259608719
Loss in iteration 190 : 0.3557047462211792
Loss in iteration 191 : 0.37238031657829906
Loss in iteration 192 : 0.3556759781925478
Loss in iteration 193 : 0.37235117099622517
Loss in iteration 194 : 0.35564802628166414
Loss in iteration 195 : 0.37232280539366214
Loss in iteration 196 : 0.35562087175788476
Loss in iteration 197 : 0.37229520250867204
Loss in iteration 198 : 0.35559449612481986
Loss in iteration 199 : 0.3722683452808508
Loss in iteration 200 : 0.3555688811215024
Testing accuracy  of updater 5 on alg 0 with rate 0.010000000000000002 = 0.74675, training accuracy 0.8190352865004856, time elapsed: 2288 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.8579854942107102
Loss in iteration 3 : 0.4794103282789223
Loss in iteration 4 : 0.39955582978327736
Loss in iteration 5 : 0.38647575499151676
Loss in iteration 6 : 0.3764342026396625
Loss in iteration 7 : 0.3687157410053907
Loss in iteration 8 : 0.362484421905449
Loss in iteration 9 : 0.35736112506893225
Loss in iteration 10 : 0.3530979576853804
Loss in iteration 11 : 0.3495171267148891
Loss in iteration 12 : 0.3464940566664671
Loss in iteration 13 : 0.3439292278319888
Loss in iteration 14 : 0.3417477343167511
Loss in iteration 15 : 0.3398875554616111
Loss in iteration 16 : 0.3383000053470624
Loss in iteration 17 : 0.3369438255234368
Loss in iteration 18 : 0.3357858744923151
Loss in iteration 19 : 0.33479759701528816
Loss in iteration 20 : 0.3339562187473708
Loss in iteration 21 : 0.33324284788963227
Loss in iteration 22 : 0.3326460328489844
Loss in iteration 23 : 0.3321679423704555
Loss in iteration 24 : 0.33185247466733303
Loss in iteration 25 : 0.33190974517028454
Loss in iteration 26 : 0.3331367294992031
Loss in iteration 27 : 0.3393487083725832
Loss in iteration 28 : 0.3606911510442563
Loss in iteration 29 : 0.4188031288386047
Loss in iteration 30 : 0.38650826053616066
Loss in iteration 31 : 0.39701444796818
Loss in iteration 32 : 0.34462968066861693
Loss in iteration 33 : 0.34182593065462585
Loss in iteration 34 : 0.3357396706676975
Loss in iteration 35 : 0.33547304353783636
Loss in iteration 36 : 0.3342122795363868
Loss in iteration 37 : 0.33546382113965495
Loss in iteration 38 : 0.33602676563911654
Loss in iteration 39 : 0.34033915015721145
Loss in iteration 40 : 0.34341373443223616
Loss in iteration 41 : 0.35574460417884873
Loss in iteration 42 : 0.3567142841017831
Loss in iteration 43 : 0.3736442389595139
Loss in iteration 44 : 0.35512901727232493
Loss in iteration 45 : 0.3617067519466619
Loss in iteration 46 : 0.34605228153709605
Loss in iteration 47 : 0.348614592684528
Loss in iteration 48 : 0.3416349372576468
Loss in iteration 49 : 0.34465833192291334
Loss in iteration 50 : 0.34139603649120015
Loss in iteration 51 : 0.3463721921835709
Loss in iteration 52 : 0.34428981182280977
Loss in iteration 53 : 0.3521719660718735
Loss in iteration 54 : 0.3483907023724662
Loss in iteration 55 : 0.3578235071400581
Loss in iteration 56 : 0.3494603820220329
Loss in iteration 57 : 0.357080666148567
Loss in iteration 58 : 0.34707111974726385
Loss in iteration 59 : 0.35254298020922265
Loss in iteration 60 : 0.34469512499197286
Loss in iteration 61 : 0.34966562789194083
Loss in iteration 62 : 0.3439790020983811
Loss in iteration 63 : 0.34963451073074514
Loss in iteration 64 : 0.3448060115669884
Loss in iteration 65 : 0.35157746419855473
Loss in iteration 66 : 0.34617318418255605
Loss in iteration 67 : 0.35358001711464787
Loss in iteration 68 : 0.34674092603756645
Loss in iteration 69 : 0.35379603489170647
Loss in iteration 70 : 0.34613815996117847
Loss in iteration 71 : 0.3524538620790409
Loss in iteration 72 : 0.34519247681743015
Loss in iteration 73 : 0.3511497931395452
Loss in iteration 74 : 0.3447030439980644
Loss in iteration 75 : 0.35081712473386767
Loss in iteration 76 : 0.34483272704872997
Loss in iteration 77 : 0.3513450615597616
Loss in iteration 78 : 0.34524235834247474
Loss in iteration 79 : 0.35202501215868987
Loss in iteration 80 : 0.3454535626762924
Loss in iteration 81 : 0.3521862430946345
Loss in iteration 82 : 0.34527243148564907
Loss in iteration 83 : 0.351769216396277
Loss in iteration 84 : 0.3448968331269906
Loss in iteration 85 : 0.3512284989733278
Loss in iteration 86 : 0.344622089755188
Loss in iteration 87 : 0.35097052336596346
Loss in iteration 88 : 0.34456774871384477
Loss in iteration 89 : 0.35104784970272257
Loss in iteration 90 : 0.34464481217350124
Loss in iteration 91 : 0.3512357382994161
Loss in iteration 92 : 0.34468476599159603
Loss in iteration 93 : 0.351282861882278
Loss in iteration 94 : 0.344596708298661
Loss in iteration 95 : 0.3511239453532753
Loss in iteration 96 : 0.3444225681989406
Loss in iteration 97 : 0.3508841880755689
Loss in iteration 98 : 0.34426277380115083
Loss in iteration 99 : 0.3507171610078444
Loss in iteration 100 : 0.34417601071641046
Loss in iteration 101 : 0.3506713369003315
Loss in iteration 102 : 0.34414542457719
Loss in iteration 103 : 0.3506842655891622
Loss in iteration 104 : 0.34411453322314156
Loss in iteration 105 : 0.35066486886390286
Loss in iteration 106 : 0.34404420557015636
Loss in iteration 107 : 0.3505754233406356
Loss in iteration 108 : 0.34393976875483717
Loss in iteration 109 : 0.3504473144962365
Loss in iteration 110 : 0.34383356940632753
Loss in iteration 111 : 0.3503348117373594
Loss in iteration 112 : 0.34375048142703934
Loss in iteration 113 : 0.3502640095757622
Loss in iteration 114 : 0.3436900722645127
Loss in iteration 115 : 0.3502203006240033
Loss in iteration 116 : 0.3436344987507873
Loss in iteration 117 : 0.35017222678028315
Loss in iteration 118 : 0.34356806897742326
Loss in iteration 119 : 0.3501019074385454
Loss in iteration 120 : 0.3434893869855243
Loss in iteration 121 : 0.3500156456171833
Loss in iteration 122 : 0.3434082763829273
Loss in iteration 123 : 0.3499316468124092
Loss in iteration 124 : 0.3433344497016092
Loss in iteration 125 : 0.3498616690802939
Loss in iteration 126 : 0.3432696649904708
Loss in iteration 127 : 0.349803419213399
Loss in iteration 128 : 0.3432086675299635
Loss in iteration 129 : 0.34974658159462196
Loss in iteration 130 : 0.3431455243415615
Loss in iteration 131 : 0.34968355338856033
Loss in iteration 132 : 0.34307864251582737
Loss in iteration 133 : 0.34961482262701343
Loss in iteration 134 : 0.34301077820205916
Loss in iteration 135 : 0.34954617618529
Loss in iteration 136 : 0.3429455030442136
Loss in iteration 137 : 0.34948242200251134
Loss in iteration 138 : 0.34288405898732977
Loss in iteration 139 : 0.3494237749285871
Loss in iteration 140 : 0.34282503903586287
Loss in iteration 141 : 0.3493670422243733
Loss in iteration 142 : 0.34276631039495736
Loss in iteration 143 : 0.34930922026706496
Loss in iteration 144 : 0.34270695686461866
Loss in iteration 145 : 0.3492498739374725
Loss in iteration 146 : 0.3426476577813377
Loss in iteration 147 : 0.34919072493219444
Loss in iteration 148 : 0.34258965776658185
Loss in iteration 149 : 0.3491336089145653
Loss in iteration 150 : 0.3425335829691535
Loss in iteration 151 : 0.34907894633271747
Loss in iteration 152 : 0.34247911085502064
Loss in iteration 153 : 0.34902581583343495
Loss in iteration 154 : 0.34242550775584235
Loss in iteration 155 : 0.34897309465276316
Loss in iteration 156 : 0.3423723405135549
Loss in iteration 157 : 0.3489204243692362
Loss in iteration 158 : 0.3423197329019822
Loss in iteration 159 : 0.34886826955986966
Loss in iteration 160 : 0.34226809347693193
Loss in iteration 161 : 0.34881729041588116
Loss in iteration 162 : 0.3422176927743482
Loss in iteration 163 : 0.34876774248500664
Loss in iteration 164 : 0.3421684779611395
Loss in iteration 165 : 0.3487193829383212
Loss in iteration 166 : 0.3421202048891032
Loss in iteration 167 : 0.34867180910121387
Loss in iteration 168 : 0.34207268613919717
Loss in iteration 169 : 0.3486248262176894
Loss in iteration 170 : 0.3420259192676647
Loss in iteration 171 : 0.3485785371870965
Loss in iteration 172 : 0.3419800266656272
Loss in iteration 173 : 0.3485331638485139
Loss in iteration 174 : 0.3419351117907324
Loss in iteration 175 : 0.3484888245962239
Loss in iteration 176 : 0.3418911733481036
Loss in iteration 177 : 0.348445461872019
Loss in iteration 178 : 0.34184812989527197
Loss in iteration 179 : 0.3484029339270191
Loss in iteration 180 : 0.34180590211417305
Loss in iteration 181 : 0.3483611476127828
Loss in iteration 182 : 0.3417644688400285
Loss in iteration 183 : 0.3483201122166593
Loss in iteration 184 : 0.34172385914970915
Loss in iteration 185 : 0.3482798940579989
Loss in iteration 186 : 0.34168410592103426
Loss in iteration 187 : 0.34824053816566536
Loss in iteration 188 : 0.3416452100154143
Loss in iteration 189 : 0.3482020304067515
Loss in iteration 190 : 0.3416071410143618
Loss in iteration 191 : 0.3481643188136583
Loss in iteration 192 : 0.3415698630618778
Loss in iteration 193 : 0.34812735922828675
Loss in iteration 194 : 0.34153335738107055
Loss in iteration 195 : 0.3480911409911861
Loss in iteration 196 : 0.34149762417534263
Loss in iteration 197 : 0.348055677744046
Loss in iteration 198 : 0.34146266849630497
Loss in iteration 199 : 0.3480209811719431
Loss in iteration 200 : 0.3414284863179777
Testing accuracy  of updater 5 on alg 0 with rate 0.007 = 0.761, training accuracy 0.8264810618323082, time elapsed: 2284 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6102455703211856
Loss in iteration 3 : 0.46685232191137727
Loss in iteration 4 : 0.4342179655425946
Loss in iteration 5 : 0.4199511460729228
Loss in iteration 6 : 0.4079972187551526
Loss in iteration 7 : 0.3978905847413133
Loss in iteration 8 : 0.389275368931115
Loss in iteration 9 : 0.3818780508864729
Loss in iteration 10 : 0.3754862543112979
Loss in iteration 11 : 0.36993332567151443
Loss in iteration 12 : 0.36508702079123195
Loss in iteration 13 : 0.36084122585008505
Loss in iteration 14 : 0.35710986440544984
Loss in iteration 15 : 0.35382238039183356
Loss in iteration 16 : 0.35092035988735015
Loss in iteration 17 : 0.3483549808962407
Loss in iteration 18 : 0.3460850699143112
Loss in iteration 19 : 0.34407560708572177
Loss in iteration 20 : 0.3422965660804599
Loss in iteration 21 : 0.34072200607958514
Loss in iteration 22 : 0.339329355423551
Loss in iteration 23 : 0.3380988423087617
Loss in iteration 24 : 0.3370130393095324
Loss in iteration 25 : 0.3360564967664027
Loss in iteration 26 : 0.33521544612519094
Loss in iteration 27 : 0.33447755876549246
Loss in iteration 28 : 0.33383174916524433
Loss in iteration 29 : 0.3332680137216626
Loss in iteration 30 : 0.33277729841058007
Loss in iteration 31 : 0.3323513898753059
Loss in iteration 32 : 0.3319828256079987
Loss in iteration 33 : 0.33166481970566014
Loss in iteration 34 : 0.3313912013116336
Loss in iteration 35 : 0.331156363338783
Loss in iteration 36 : 0.3309552194482664
Loss in iteration 37 : 0.330783167555377
Loss in iteration 38 : 0.3306360583739877
Loss in iteration 39 : 0.3305101677101683
Loss in iteration 40 : 0.3304021713902729
Loss in iteration 41 : 0.33030912186614003
Loss in iteration 42 : 0.33022842569918665
Loss in iteration 43 : 0.33015782126934523
Loss in iteration 44 : 0.33009535624698066
Loss in iteration 45 : 0.33003936446397053
Loss in iteration 46 : 0.3299884425112457
Loss in iteration 47 : 0.32994143042954327
Loss in iteration 48 : 0.32989749951480574
Loss in iteration 49 : 0.32985860807432493
Loss in iteration 50 : 0.32989214530539945
Loss in iteration 51 : 0.33185222886275423
Loss in iteration 52 : 0.37493807362553916
Loss in iteration 53 : 0.4017809992812065
Loss in iteration 54 : 0.335489329525058
Loss in iteration 55 : 0.3324219186260386
Loss in iteration 56 : 0.33072633036735155
Loss in iteration 57 : 0.33029845711735895
Loss in iteration 58 : 0.3300422113188191
Loss in iteration 59 : 0.32996647015578606
Loss in iteration 60 : 0.3299396400734704
Loss in iteration 61 : 0.33005455146155854
Loss in iteration 62 : 0.33031724418747754
Loss in iteration 63 : 0.3311263609059477
Loss in iteration 64 : 0.33280614550708904
Loss in iteration 65 : 0.3375663008938745
Loss in iteration 66 : 0.3433760544392078
Loss in iteration 67 : 0.3529223886032239
Loss in iteration 68 : 0.34324696791766024
Loss in iteration 69 : 0.34247024775353546
Loss in iteration 70 : 0.3352087656249618
Loss in iteration 71 : 0.3341874264830954
Loss in iteration 72 : 0.3322678387715356
Loss in iteration 73 : 0.33219247828878157
Loss in iteration 74 : 0.331741899881381
Loss in iteration 75 : 0.332383613006018
Loss in iteration 76 : 0.3327057117422853
Loss in iteration 77 : 0.33457604821577935
Loss in iteration 78 : 0.3355997301936227
Loss in iteration 79 : 0.33921192357066593
Loss in iteration 80 : 0.33872894417116184
Loss in iteration 81 : 0.341524252376559
Loss in iteration 82 : 0.33752036385350626
Loss in iteration 83 : 0.33808122067768304
Loss in iteration 84 : 0.334784001076931
Loss in iteration 85 : 0.3349884094845985
Loss in iteration 86 : 0.3333550232901883
Loss in iteration 87 : 0.33395839437783875
Loss in iteration 88 : 0.333262364205677
Loss in iteration 89 : 0.3344940420054743
Loss in iteration 90 : 0.3342286558353789
Loss in iteration 91 : 0.33619074441604513
Loss in iteration 92 : 0.33565199649058225
Loss in iteration 93 : 0.33783542385053233
Loss in iteration 94 : 0.3361503894885425
Loss in iteration 95 : 0.3376786164150383
Loss in iteration 96 : 0.33530631402147154
Loss in iteration 97 : 0.33622241284318016
Loss in iteration 98 : 0.3342472998801362
Loss in iteration 99 : 0.33508511887151693
Loss in iteration 100 : 0.33374612339759685
Loss in iteration 101 : 0.334829601072186
Loss in iteration 102 : 0.33389468173550263
Loss in iteration 103 : 0.33533266611933976
Loss in iteration 104 : 0.33444309166928815
Loss in iteration 105 : 0.33611487381685756
Loss in iteration 106 : 0.33488369873178603
Loss in iteration 107 : 0.33646599485010065
Loss in iteration 108 : 0.334803579366024
Loss in iteration 109 : 0.33609949528416894
Loss in iteration 110 : 0.33434067638517656
Loss in iteration 111 : 0.33545677047126427
Loss in iteration 112 : 0.33391647519064616
Loss in iteration 113 : 0.3350540367105825
Loss in iteration 114 : 0.3337749372788665
Loss in iteration 115 : 0.3350575310416121
Loss in iteration 116 : 0.3339030776029635
Loss in iteration 117 : 0.3353331086549996
Loss in iteration 118 : 0.3341126187571741
Loss in iteration 119 : 0.3355831601402532
Loss in iteration 120 : 0.3341796606050575
Loss in iteration 121 : 0.3355671120622665
Loss in iteration 122 : 0.3340328055970535
Loss in iteration 123 : 0.33530910080603266
Loss in iteration 124 : 0.3337925957774522
Loss in iteration 125 : 0.3350241150945982
Loss in iteration 126 : 0.3336185425820709
Loss in iteration 127 : 0.3348855688006498
Loss in iteration 128 : 0.33357783046621736
Loss in iteration 129 : 0.3349158289168008
Loss in iteration 130 : 0.3336276381756249
Loss in iteration 131 : 0.3350136658987304
Loss in iteration 132 : 0.3336683064637894
Loss in iteration 133 : 0.3350474915448619
Loss in iteration 134 : 0.3336264568134997
Loss in iteration 135 : 0.33496092665319344
Loss in iteration 136 : 0.33350917096690835
Loss in iteration 137 : 0.33480500551993847
Loss in iteration 138 : 0.3333805998424742
Loss in iteration 139 : 0.33467174189297716
Loss in iteration 140 : 0.3332974180432839
Loss in iteration 141 : 0.3346131095083337
Loss in iteration 142 : 0.3332693240849289
Loss in iteration 143 : 0.3346143641290023
Loss in iteration 144 : 0.33326368629619074
Loss in iteration 145 : 0.33462023132437435
Loss in iteration 146 : 0.33323839312013903
Loss in iteration 147 : 0.33458396557895703
Loss in iteration 148 : 0.33317570681185155
Loss in iteration 149 : 0.334501575815181
Loss in iteration 150 : 0.33309085504159935
Loss in iteration 151 : 0.33440559417580185
Loss in iteration 152 : 0.33301258078239593
Loss in iteration 153 : 0.33433132655988695
Loss in iteration 154 : 0.33295814188662165
Loss in iteration 155 : 0.33428978149146593
Loss in iteration 156 : 0.3329233627057505
Loss in iteration 157 : 0.3342654342598087
Loss in iteration 158 : 0.3328906602133882
Loss in iteration 159 : 0.33423368301749484
Loss in iteration 160 : 0.3328451274952529
Loss in iteration 161 : 0.33418130872332713
Loss in iteration 162 : 0.3327852608752993
Loss in iteration 163 : 0.33411405301926334
Loss in iteration 164 : 0.3327210790076896
Loss in iteration 165 : 0.33404790003856233
Loss in iteration 166 : 0.3326638325134636
Loss in iteration 167 : 0.33399458216296163
Loss in iteration 168 : 0.33261727623758436
Loss in iteration 169 : 0.33395382172806765
Loss in iteration 170 : 0.33257662202550775
Loss in iteration 171 : 0.33391632162783724
Loss in iteration 172 : 0.3325340623226679
Loss in iteration 173 : 0.33387276010765526
Loss in iteration 174 : 0.3324853209716792
Loss in iteration 175 : 0.33382082517812933
Loss in iteration 176 : 0.33243208823618026
Loss in iteration 177 : 0.33376537534178824
Loss in iteration 178 : 0.33237937255150735
Loss in iteration 179 : 0.33371311322037656
Loss in iteration 180 : 0.332330957008224
Loss in iteration 181 : 0.3336671158387537
Loss in iteration 182 : 0.3322868463688951
Loss in iteration 183 : 0.3336253702307131
Loss in iteration 184 : 0.3322441335347151
Loss in iteration 185 : 0.3335834490658933
Loss in iteration 186 : 0.33219984597851976
Loss in iteration 187 : 0.33353838645406625
Loss in iteration 188 : 0.3321531982587583
Loss in iteration 189 : 0.33349058015134775
Loss in iteration 190 : 0.3321056836307565
Loss in iteration 191 : 0.33344272061385766
Loss in iteration 192 : 0.33205942397595395
Loss in iteration 193 : 0.333397235952568
Loss in iteration 194 : 0.3320154286443652
Loss in iteration 195 : 0.33335452755843076
Loss in iteration 196 : 0.3319730925745499
Loss in iteration 197 : 0.3333131348900197
Loss in iteration 198 : 0.33193102080976705
Loss in iteration 199 : 0.33327125871093805
Loss in iteration 200 : 0.33188825755829526
Testing accuracy  of updater 5 on alg 0 with rate 0.004 = 0.77075, training accuracy 0.8326319197151182, time elapsed: 2656 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5293298190102812
Loss in iteration 3 : 0.507242816105538
Loss in iteration 4 : 0.4958919664388501
Loss in iteration 5 : 0.48701172017179095
Loss in iteration 6 : 0.47900975087015674
Loss in iteration 7 : 0.4714908608315363
Loss in iteration 8 : 0.46434372598746526
Loss in iteration 9 : 0.45753032441602565
Loss in iteration 10 : 0.4510316400757436
Loss in iteration 11 : 0.4448334088217974
Loss in iteration 12 : 0.4389225603376879
Loss in iteration 13 : 0.4332864611026779
Loss in iteration 14 : 0.4279128307527543
Loss in iteration 15 : 0.42278978516672566
Loss in iteration 16 : 0.4179058819040476
Loss in iteration 17 : 0.4132501483172043
Loss in iteration 18 : 0.40881209447065553
Loss in iteration 19 : 0.4045817153696759
Loss in iteration 20 : 0.40054948589099204
Loss in iteration 21 : 0.39670635065163457
Loss in iteration 22 : 0.3930437103088167
Loss in iteration 23 : 0.38955340534583444
Loss in iteration 24 : 0.3862276981335445
Loss in iteration 25 : 0.3830592538814731
Loss in iteration 26 : 0.38004112096463627
Loss in iteration 27 : 0.3771667110118195
Loss in iteration 28 : 0.3744297790590051
Loss in iteration 29 : 0.3718244040034746
Loss in iteration 30 : 0.36934496953743573
Loss in iteration 31 : 0.36698614569307875
Loss in iteration 32 : 0.3647428710925338
Loss in iteration 33 : 0.36261033596497194
Loss in iteration 34 : 0.36058396596813425
Loss in iteration 35 : 0.3586594068317732
Loss in iteration 36 : 0.3568325098251108
Loss in iteration 37 : 0.3550993180386537
Loss in iteration 38 : 0.35345605346192704
Loss in iteration 39 : 0.3518991048323048
Loss in iteration 40 : 0.35042501622577354
Loss in iteration 41 : 0.3490304763576283
Loss in iteration 42 : 0.34771230855958424
Loss in iteration 43 : 0.3464674613992648
Loss in iteration 44 : 0.3452929999082596
Loss in iteration 45 : 0.34418609738589795
Loss in iteration 46 : 0.34314402774720504
Loss in iteration 47 : 0.3421641583853214
Loss in iteration 48 : 0.34124394352072634
Loss in iteration 49 : 0.3403809180119118
Loss in iteration 50 : 0.33957269160467146
Loss in iteration 51 : 0.33881694359983566
Loss in iteration 52 : 0.3381114179220484
Loss in iteration 53 : 0.3374539185751312
Loss in iteration 54 : 0.33684230547249716
Loss in iteration 55 : 0.3362744906341954
Loss in iteration 56 : 0.335748434745231
Loss in iteration 57 : 0.3352621440729125
Loss in iteration 58 : 0.3348136677440995
Loss in iteration 59 : 0.3344010953862142
Loss in iteration 60 : 0.3340225551387547
Loss in iteration 61 : 0.3336762120446872
Loss in iteration 62 : 0.33336026683334935
Loss in iteration 63 : 0.33307295510828205
Loss in iteration 64 : 0.33281254695444923
Loss in iteration 65 : 0.3325773469794872
Loss in iteration 66 : 0.3323656948025942
Loss in iteration 67 : 0.33217596600218036
Loss in iteration 68 : 0.3320065735291832
Loss in iteration 69 : 0.33185596958656416
Loss in iteration 70 : 0.3317226479667887
Loss in iteration 71 : 0.3316051468276988
Loss in iteration 72 : 0.33150205187303916
Loss in iteration 73 : 0.3314119998869387
Loss in iteration 74 : 0.33133368255221207
Loss in iteration 75 : 0.33126585046094176
Testing accuracy  of updater 5 on alg 0 with rate 0.001 = 0.78075, training accuracy 0.8355454839753965, time elapsed: 857 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5576357016383112
Loss in iteration 3 : 0.5260151919559689
Loss in iteration 4 : 0.5117079241945321
Loss in iteration 5 : 0.5026144292571507
Loss in iteration 6 : 0.4954707680582311
Loss in iteration 7 : 0.48915823417246157
Loss in iteration 8 : 0.483255409199314
Loss in iteration 9 : 0.47759874120048756
Loss in iteration 10 : 0.47212501286620684
Loss in iteration 11 : 0.4668103191782613
Loss in iteration 12 : 0.46164578556778696
Loss in iteration 13 : 0.45662794093737075
Loss in iteration 14 : 0.4517549992347871
Loss in iteration 15 : 0.4470254954467376
Loss in iteration 16 : 0.4424378332753447
Loss in iteration 17 : 0.43799016954396014
Testing accuracy  of updater 5 on alg 0 with rate 7.000000000000001E-4 = 0.53675, training accuracy 0.6733570734865653, time elapsed: 164 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6137276833136656
Loss in iteration 3 : 0.5762863467178808
Testing accuracy  of updater 5 on alg 0 with rate 4.000000000000001E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 35 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6766506404334911
Testing accuracy  of updater 5 on alg 0 with rate 9.999999999999994E-5 = 0.5, training accuracy 0.6474587245063127, time elapsed: 35 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 6.795654913061559
Loss in iteration 3 : 7.2920116754838045
Loss in iteration 4 : 5.190899703597021
Loss in iteration 5 : 1.8868136829842077
Loss in iteration 6 : 1.1219627222417548
Loss in iteration 7 : 3.389173191398797
Loss in iteration 8 : 3.4014322480454564
Loss in iteration 9 : 1.9469739973610565
Loss in iteration 10 : 1.4009173287041476
Loss in iteration 11 : 1.780350486978087
Loss in iteration 12 : 2.32499282331594
Loss in iteration 13 : 2.6539678216636244
Loss in iteration 14 : 2.6378029180016385
Loss in iteration 15 : 2.350495971389602
Loss in iteration 16 : 1.974262586792545
Loss in iteration 17 : 1.7029813392016322
Loss in iteration 18 : 1.7072349699215685
Loss in iteration 19 : 1.9749469949299587
Loss in iteration 20 : 2.1743109949332986
Loss in iteration 21 : 2.073325688781951
Loss in iteration 22 : 1.7718849044748872
Loss in iteration 23 : 1.5603105573501934
Loss in iteration 24 : 1.5641440398629374
Loss in iteration 25 : 1.6650668076757147
Loss in iteration 26 : 1.7210876479424533
Loss in iteration 27 : 1.6554492198452075
Loss in iteration 28 : 1.4784002207529607
Loss in iteration 29 : 1.2799346697632192
Loss in iteration 30 : 1.1901367325315435
Loss in iteration 31 : 1.248460941157019
Loss in iteration 32 : 1.2565225854936817
Loss in iteration 33 : 1.071332624177779
Loss in iteration 34 : 0.8945342688839416
Loss in iteration 35 : 0.8849820684982813
Loss in iteration 36 : 0.904789639457781
Loss in iteration 37 : 0.7983735119517377
Loss in iteration 38 : 0.6069380273129541
Loss in iteration 39 : 0.5940797941222072
Loss in iteration 40 : 0.6161395871644237
Loss in iteration 41 : 0.40957019699359565
Loss in iteration 42 : 0.5455002291751796
Loss in iteration 43 : 0.3961527098603815
Loss in iteration 44 : 1.0752198075040842
Loss in iteration 45 : 2.034727583721084
Loss in iteration 46 : 3.040484690150654
Loss in iteration 47 : 2.535722605548116
Loss in iteration 48 : 0.9591031933677636
Loss in iteration 49 : 0.8264163345838281
Loss in iteration 50 : 2.08987427417067
Loss in iteration 51 : 1.1650663807299868
Loss in iteration 52 : 0.762301896881357
Loss in iteration 53 : 1.1795843222762465
Loss in iteration 54 : 1.5348086498017037
Loss in iteration 55 : 1.5136322026432314
Loss in iteration 56 : 1.225006718347441
Loss in iteration 57 : 0.9694563342298409
Loss in iteration 58 : 1.0293539034594776
Loss in iteration 59 : 1.3068385758943661
Loss in iteration 60 : 1.3056575276752242
Loss in iteration 61 : 1.0389259868603444
Loss in iteration 62 : 0.9304820724981295
Loss in iteration 63 : 1.0315803359684343
Loss in iteration 64 : 1.1194394782307575
Loss in iteration 65 : 1.0657798655557762
Loss in iteration 66 : 0.8972417588964814
Loss in iteration 67 : 0.7712138740343946
Loss in iteration 68 : 0.8315607650294399
Loss in iteration 69 : 0.8722525312204903
Loss in iteration 70 : 0.6895307816125285
Loss in iteration 71 : 0.6087700781678299
Loss in iteration 72 : 0.6709965104501169
Loss in iteration 73 : 0.6384214181584612
Loss in iteration 74 : 0.48015834121288625
Loss in iteration 75 : 0.5031539226172868
Loss in iteration 76 : 0.5029205343774419
Loss in iteration 77 : 0.41666319055661855
Loss in iteration 78 : 0.5289588648936733
Loss in iteration 79 : 0.3737245589616567
Loss in iteration 80 : 0.72985535857637
Loss in iteration 81 : 1.2084560414565109
Loss in iteration 82 : 1.4333660895118223
Loss in iteration 83 : 0.5570333033505547
Loss in iteration 84 : 0.9225173255349317
Loss in iteration 85 : 0.7871001113836396
Loss in iteration 86 : 0.5287591005585526
Loss in iteration 87 : 0.8376029674146139
Loss in iteration 88 : 0.8948694450299008
Loss in iteration 89 : 0.6908299042685319
Loss in iteration 90 : 0.6181847659704358
Loss in iteration 91 : 0.8239529100429069
Loss in iteration 92 : 0.7725225973860089
Loss in iteration 93 : 0.6108788773681785
Loss in iteration 94 : 0.6783579713316423
Loss in iteration 95 : 0.7427572643581537
Loss in iteration 96 : 0.6552211496267557
Loss in iteration 97 : 0.5389813641723104
Loss in iteration 98 : 0.6018925977418671
Loss in iteration 99 : 0.5832961095817638
Loss in iteration 100 : 0.45491920693559285
Loss in iteration 101 : 0.5133897930684468
Loss in iteration 102 : 0.48760645164107436
Loss in iteration 103 : 0.3788192659134984
Loss in iteration 104 : 0.5116676857552912
Loss in iteration 105 : 0.3770742264933102
Loss in iteration 106 : 0.47624494156790914
Loss in iteration 107 : 0.356045492782621
Loss in iteration 108 : 0.5028559327056815
Loss in iteration 109 : 0.6915953923171072
Loss in iteration 110 : 0.5791705426821875
Loss in iteration 111 : 0.4748766955371263
Loss in iteration 112 : 0.4525138630487912
Loss in iteration 113 : 0.4449621507921497
Loss in iteration 114 : 0.5348644772830924
Loss in iteration 115 : 0.4071157616448413
Loss in iteration 116 : 0.49466878449392004
Loss in iteration 117 : 0.4548395422523394
Loss in iteration 118 : 0.42308941594137056
Loss in iteration 119 : 0.4833684864426787
Loss in iteration 120 : 0.41033074641614864
Loss in iteration 121 : 0.4164741430565979
Loss in iteration 122 : 0.4176244662085091
Loss in iteration 123 : 0.3744895311736814
Loss in iteration 124 : 0.4126376675754728
Loss in iteration 125 : 0.34021651465143044
Loss in iteration 126 : 0.425161261474725
Loss in iteration 127 : 0.38920217142650676
Loss in iteration 128 : 0.38911737455577505
Loss in iteration 129 : 0.4255138218696198
Loss in iteration 130 : 0.3623634044754995
Loss in iteration 131 : 0.35866394295260456
Loss in iteration 132 : 0.37827821443481796
Loss in iteration 133 : 0.33017136540402575
Loss in iteration 134 : 0.35977458804044943
Loss in iteration 135 : 0.3337042522375115
Loss in iteration 136 : 0.3567574750418786
Loss in iteration 137 : 0.33948702414122506
Loss in iteration 138 : 0.3451448445762115
Loss in iteration 139 : 0.3361775926738135
Loss in iteration 140 : 0.3323174325039925
Loss in iteration 141 : 0.33982526935057905
Loss in iteration 142 : 0.3262933720501677
Loss in iteration 143 : 0.3461471965801578
Loss in iteration 144 : 0.33728510082582613
Loss in iteration 145 : 0.3293141509561806
Loss in iteration 146 : 0.3493652585864255
Loss in iteration 147 : 0.34040603749496245
Loss in iteration 148 : 0.3263269710783975
Loss in iteration 149 : 0.35031830543287007
Loss in iteration 150 : 0.33891292556476493
Loss in iteration 151 : 0.3305957621054957
Loss in iteration 152 : 0.35053986068306514
Loss in iteration 153 : 0.33039532608262595
Loss in iteration 154 : 0.3358837087648183
Loss in iteration 155 : 0.3368886958953818
Loss in iteration 156 : 0.3244819895172373
Loss in iteration 157 : 0.3336046094895603
Loss in iteration 158 : 0.32537794478203436
Loss in iteration 159 : 0.32635795561264785
Loss in iteration 160 : 0.33200761277241786
Loss in iteration 161 : 0.3238011947478761
Loss in iteration 162 : 0.3286579643251684
Loss in iteration 163 : 0.33321229562591487
Loss in iteration 164 : 0.3236988329224001
Loss in iteration 165 : 0.3281124778398156
Loss in iteration 166 : 0.33257548453323243
Loss in iteration 167 : 0.323250087407907
Loss in iteration 168 : 0.3290119345115685
Loss in iteration 169 : 0.33209181782775454
Loss in iteration 170 : 0.3235065902216477
Loss in iteration 171 : 0.3313569827036711
Loss in iteration 172 : 0.3318219960012574
Loss in iteration 173 : 0.3234632432252537
Loss in iteration 174 : 0.3330865467256323
Loss in iteration 175 : 0.3331425329218194
Loss in iteration 176 : 0.3231957117579112
Loss in iteration 177 : 0.3347824520245113
Loss in iteration 178 : 0.3380199570843641
Loss in iteration 179 : 0.32307666514592825
Loss in iteration 180 : 0.336772077187172
Loss in iteration 181 : 0.34468609141421136
Loss in iteration 182 : 0.32311279891870137
Loss in iteration 183 : 0.34192546027471626
Loss in iteration 184 : 0.35305716157988004
Loss in iteration 185 : 0.3236276827373921
Loss in iteration 186 : 0.3560025199722633
Loss in iteration 187 : 0.3686049397001079
Loss in iteration 188 : 0.3265990661897487
Loss in iteration 189 : 0.3888545974661495
Loss in iteration 190 : 0.40103487566348067
Loss in iteration 191 : 0.34167862481716627
Loss in iteration 192 : 0.45221553577224455
Loss in iteration 193 : 0.4287478715750116
Loss in iteration 194 : 0.3936276420889454
Loss in iteration 195 : 0.45012662315597674
Loss in iteration 196 : 0.3443075217611643
Loss in iteration 197 : 0.4003756017474523
Loss in iteration 198 : 0.34195657728332646
Loss in iteration 199 : 0.3925793447671935
Loss in iteration 200 : 0.3758745085180873
Testing accuracy  of updater 6 on alg 0 with rate 0.19999999999999998 = 0.76325, training accuracy 0.8293946260925866, time elapsed: 2027 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.9804646205578473
Loss in iteration 3 : 2.4178399653048728
Loss in iteration 4 : 1.5362174915177251
Loss in iteration 5 : 0.5539233710988994
Loss in iteration 6 : 1.2768593410253568
Loss in iteration 7 : 1.6341877544462462
Loss in iteration 8 : 0.971965734110653
Loss in iteration 9 : 0.8821003373171856
Loss in iteration 10 : 1.1949505197581811
Loss in iteration 11 : 1.4286733434848429
Loss in iteration 12 : 1.4210819935182437
Loss in iteration 13 : 1.2380633968189105
Loss in iteration 14 : 1.0587633291633214
Loss in iteration 15 : 1.06063685013147
Loss in iteration 16 : 1.23228478282548
Loss in iteration 17 : 1.2877246876111734
Loss in iteration 18 : 1.130819997986928
Loss in iteration 19 : 0.9854200353947895
Loss in iteration 20 : 0.9945950373399262
Loss in iteration 21 : 1.0575680796965
Loss in iteration 22 : 1.0540190572404369
Loss in iteration 23 : 0.9506844370758911
Loss in iteration 24 : 0.8161341035357023
Loss in iteration 25 : 0.7706623100846112
Loss in iteration 26 : 0.8150787316562155
Loss in iteration 27 : 0.7543047958766561
Loss in iteration 28 : 0.6097641043165732
Loss in iteration 29 : 0.5841380590163444
Loss in iteration 30 : 0.6030499302399908
Loss in iteration 31 : 0.5196925035628273
Loss in iteration 32 : 0.4096973309256468
Loss in iteration 33 : 0.4873203636646316
Loss in iteration 34 : 0.3561321177704607
Loss in iteration 35 : 0.42538543721898514
Loss in iteration 36 : 0.3907799434350613
Loss in iteration 37 : 0.6155914556128952
Loss in iteration 38 : 0.9614325465874696
Loss in iteration 39 : 0.9735245589750222
Loss in iteration 40 : 0.3449235713911439
Loss in iteration 41 : 1.0772441372234682
Loss in iteration 42 : 0.40030090008544633
Loss in iteration 43 : 0.6980177542374965
Loss in iteration 44 : 0.7860066117734316
Loss in iteration 45 : 0.5878818359661144
Loss in iteration 46 : 0.540594339556879
Loss in iteration 47 : 0.7406293033514351
Loss in iteration 48 : 0.6646340829067794
Loss in iteration 49 : 0.5494179533038788
Loss in iteration 50 : 0.6282081990309771
Loss in iteration 51 : 0.6767051169642533
Loss in iteration 52 : 0.5964190745503292
Loss in iteration 53 : 0.5098673324004387
Loss in iteration 54 : 0.5658932847720826
Loss in iteration 55 : 0.5563152258880131
Loss in iteration 56 : 0.44685340844516735
Loss in iteration 57 : 0.47604148314414374
Loss in iteration 58 : 0.48204140514727706
Loss in iteration 59 : 0.38768278126122674
Loss in iteration 60 : 0.42126664273063746
Loss in iteration 61 : 0.3830210598583217
Loss in iteration 62 : 0.3827708191924755
Loss in iteration 63 : 0.4058543601095907
Loss in iteration 64 : 0.3662341252888311
Loss in iteration 65 : 0.37315357645662156
Loss in iteration 66 : 0.41682720593066014
Loss in iteration 67 : 0.35597813275955703
Loss in iteration 68 : 0.4352054530009415
Loss in iteration 69 : 0.3601163603355405
Loss in iteration 70 : 0.3926635920269532
Loss in iteration 71 : 0.34181580413636586
Loss in iteration 72 : 0.3955078379270837
Loss in iteration 73 : 0.3454335905200628
Loss in iteration 74 : 0.3816907891497547
Loss in iteration 75 : 0.35635186047284445
Loss in iteration 76 : 0.3593638949603902
Loss in iteration 77 : 0.3626842330805861
Loss in iteration 78 : 0.34456210473739846
Loss in iteration 79 : 0.3605591841261872
Loss in iteration 80 : 0.33457902122784117
Loss in iteration 81 : 0.35493144452134007
Loss in iteration 82 : 0.3297999866417236
Loss in iteration 83 : 0.3467117872966246
Loss in iteration 84 : 0.3286484039518895
Loss in iteration 85 : 0.3482352720195178
Loss in iteration 86 : 0.3303587167879794
Loss in iteration 87 : 0.33747297786989405
Loss in iteration 88 : 0.3313286430784513
Loss in iteration 89 : 0.3277974494690375
Loss in iteration 90 : 0.33307067595623
Loss in iteration 91 : 0.32596448711755144
Loss in iteration 92 : 0.333859219884965
Loss in iteration 93 : 0.32540306251204326
Loss in iteration 94 : 0.3311651754782451
Loss in iteration 95 : 0.3248932092376084
Loss in iteration 96 : 0.32943478814221583
Loss in iteration 97 : 0.32447816209383123
Loss in iteration 98 : 0.32732500830191735
Loss in iteration 99 : 0.32410813011255435
Loss in iteration 100 : 0.32637756490999636
Loss in iteration 101 : 0.32487965439372174
Loss in iteration 102 : 0.3255093120959578
Loss in iteration 103 : 0.3254390331114657
Loss in iteration 104 : 0.32433780369277954
Loss in iteration 105 : 0.3255588331239131
Loss in iteration 106 : 0.3234769296300162
Loss in iteration 107 : 0.32513086722140666
Loss in iteration 108 : 0.32318024306593207
Loss in iteration 109 : 0.3242114974644377
Loss in iteration 110 : 0.3235418902381678
Loss in iteration 111 : 0.3237324770148672
Loss in iteration 112 : 0.3240791460156877
Loss in iteration 113 : 0.32349679566993184
Loss in iteration 114 : 0.32417254233700227
Loss in iteration 115 : 0.32322535172132116
Loss in iteration 116 : 0.3237886723751117
Loss in iteration 117 : 0.32309464716274516
Loss in iteration 118 : 0.32342522143369595
Loss in iteration 119 : 0.32322202942886624
Loss in iteration 120 : 0.323210950511283
Loss in iteration 121 : 0.323418953723382
Loss in iteration 122 : 0.3231245212687565
Loss in iteration 123 : 0.32345211431190735
Loss in iteration 124 : 0.32309865134241633
Loss in iteration 125 : 0.3233014406397709
Loss in iteration 126 : 0.32309672109303494
Loss in iteration 127 : 0.3230893552706061
Loss in iteration 128 : 0.32311965601838655
Loss in iteration 129 : 0.3229846570927992
Loss in iteration 130 : 0.32314634126365904
Loss in iteration 131 : 0.3230005338224533
Loss in iteration 132 : 0.3231316815548771
Loss in iteration 133 : 0.32304557859195066
Loss in iteration 134 : 0.3230542149574073
Loss in iteration 135 : 0.3230579857537632
Loss in iteration 136 : 0.32297750195596153
Loss in iteration 137 : 0.3230409170370582
Loss in iteration 138 : 0.32295939601917933
Loss in iteration 139 : 0.32301708263977197
Loss in iteration 140 : 0.32298886025715773
Loss in iteration 141 : 0.3229917004749726
Loss in iteration 142 : 0.32301408063145937
Loss in iteration 143 : 0.3229723887827014
Loss in iteration 144 : 0.32300662956826265
Loss in iteration 145 : 0.32296665474066244
Loss in iteration 146 : 0.32298147814023265
Testing accuracy  of updater 6 on alg 0 with rate 0.13999999999999999 = 0.78975, training accuracy 0.8439624473939786, time elapsed: 1532 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.284553356995398
Loss in iteration 3 : 1.5890715240230948
Loss in iteration 4 : 1.0839278153282128
Loss in iteration 5 : 0.44679998505077484
Loss in iteration 6 : 0.7475194867127458
Loss in iteration 7 : 1.111776252144613
Loss in iteration 8 : 0.7679239105343719
Loss in iteration 9 : 0.5987777171223347
Loss in iteration 10 : 0.7473384868925657
Loss in iteration 11 : 0.9185260146247256
Loss in iteration 12 : 0.963859991691944
Loss in iteration 13 : 0.8819796602840269
Loss in iteration 14 : 0.7621753502624776
Loss in iteration 15 : 0.714649110450579
Loss in iteration 16 : 0.7869745857551199
Loss in iteration 17 : 0.8662616850991706
Loss in iteration 18 : 0.8251459214152728
Loss in iteration 19 : 0.7188547786685819
Loss in iteration 20 : 0.6751760451956643
Loss in iteration 21 : 0.7020165746944729
Loss in iteration 22 : 0.7274010264889998
Loss in iteration 23 : 0.7000491379187253
Loss in iteration 24 : 0.625638315005641
Loss in iteration 25 : 0.5602731868219801
Loss in iteration 26 : 0.5575982665452099
Loss in iteration 27 : 0.574660909173288
Loss in iteration 28 : 0.520308599455869
Loss in iteration 29 : 0.44904751165326057
Loss in iteration 30 : 0.4443584889720172
Loss in iteration 31 : 0.4522282928908852
Loss in iteration 32 : 0.4057667463540338
Loss in iteration 33 : 0.3538156651401574
Loss in iteration 34 : 0.38818382861588624
Loss in iteration 35 : 0.3535462134432093
Loss in iteration 36 : 0.3442329444277155
Loss in iteration 37 : 0.3829504055178352
Loss in iteration 38 : 0.34152025655697377
Loss in iteration 39 : 0.4159267319050504
Loss in iteration 40 : 0.36370641666314435
Loss in iteration 41 : 0.38921484488506136
Loss in iteration 42 : 0.33567281633678087
Loss in iteration 43 : 0.3772251761210048
Loss in iteration 44 : 0.32900040814243625
Loss in iteration 45 : 0.3545689240558259
Loss in iteration 46 : 0.34208281875618995
Loss in iteration 47 : 0.3340845158292604
Loss in iteration 48 : 0.3524454921286289
Loss in iteration 49 : 0.3363074989102487
Loss in iteration 50 : 0.34100414691444475
Loss in iteration 51 : 0.3477124757126702
Loss in iteration 52 : 0.3360955799862662
Loss in iteration 53 : 0.338636027898132
Loss in iteration 54 : 0.3415670072241093
Loss in iteration 55 : 0.33100458811096467
Loss in iteration 56 : 0.3352987389389617
Loss in iteration 57 : 0.3335790911857679
Loss in iteration 58 : 0.3268646186407729
Loss in iteration 59 : 0.33252881479495733
Loss in iteration 60 : 0.32668473279800037
Loss in iteration 61 : 0.3280695875712716
Loss in iteration 62 : 0.3293448690844331
Loss in iteration 63 : 0.32528671402600373
Loss in iteration 64 : 0.3296654557614553
Loss in iteration 65 : 0.32497536774145347
Loss in iteration 66 : 0.32760552523757624
Loss in iteration 67 : 0.32557634138266034
Loss in iteration 68 : 0.3252499956443393
Loss in iteration 69 : 0.3260547141899552
Loss in iteration 70 : 0.324076012256981
Loss in iteration 71 : 0.32597135502690205
Loss in iteration 72 : 0.32413002843780786
Loss in iteration 73 : 0.32505503883397063
Loss in iteration 74 : 0.32469711692722675
Loss in iteration 75 : 0.32398114032368075
Loss in iteration 76 : 0.32478487016909907
Loss in iteration 77 : 0.32356839406554416
Loss in iteration 78 : 0.32412437299620045
Loss in iteration 79 : 0.3237740169636219
Loss in iteration 80 : 0.323428114859091
Loss in iteration 81 : 0.3239263257175683
Loss in iteration 82 : 0.3232647212827341
Loss in iteration 83 : 0.32374018535952986
Loss in iteration 84 : 0.32348317314957364
Loss in iteration 85 : 0.3234340948164052
Loss in iteration 86 : 0.32364091977414133
Loss in iteration 87 : 0.3232317189953631
Loss in iteration 88 : 0.3235235147352653
Loss in iteration 89 : 0.32317263735403845
Loss in iteration 90 : 0.3232495385671588
Loss in iteration 91 : 0.32318294374760564
Loss in iteration 92 : 0.32303768948478506
Loss in iteration 93 : 0.32319258729585654
Loss in iteration 94 : 0.32299885508009474
Loss in iteration 95 : 0.3231692455130735
Loss in iteration 96 : 0.3230750686799737
Loss in iteration 97 : 0.3231153334344744
Loss in iteration 98 : 0.3231412992021866
Loss in iteration 99 : 0.3230545284287548
Loss in iteration 100 : 0.3231279858396079
Loss in iteration 101 : 0.32301315336837455
Loss in iteration 102 : 0.32305471335492925
Loss in iteration 103 : 0.323001905378284
Loss in iteration 104 : 0.3229856277907217
Loss in iteration 105 : 0.3230087932826069
Loss in iteration 106 : 0.3229621869491791
Loss in iteration 107 : 0.3230145328213751
Loss in iteration 108 : 0.32297812474462145
Loss in iteration 109 : 0.32300802118374106
Testing accuracy  of updater 6 on alg 0 with rate 0.08 = 0.7905, training accuracy 0.8439624473939786, time elapsed: 1109 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.505978112153966
Loss in iteration 3 : 0.6178449838142137
Loss in iteration 4 : 0.6098747786883977
Loss in iteration 5 : 0.4854162712411377
Loss in iteration 6 : 0.35469176541926994
Loss in iteration 7 : 0.3531073939398307
Loss in iteration 8 : 0.44019291376344416
Loss in iteration 9 : 0.447784056520602
Loss in iteration 10 : 0.38170759888466144
Loss in iteration 11 : 0.343913153881469
Loss in iteration 12 : 0.35809475446771266
Loss in iteration 13 : 0.39113325625464784
Loss in iteration 14 : 0.4119666730170433
Loss in iteration 15 : 0.4098206611197065
Loss in iteration 16 : 0.3915886522851428
Loss in iteration 17 : 0.372905250925594
Loss in iteration 18 : 0.36748093335625753
Loss in iteration 19 : 0.37762689969953755
Loss in iteration 20 : 0.3919783288084471
Loss in iteration 21 : 0.39602027487889585
Loss in iteration 22 : 0.3865224979529846
Loss in iteration 23 : 0.372973400143138
Loss in iteration 24 : 0.36589429797285344
Loss in iteration 25 : 0.3674360808013176
Loss in iteration 26 : 0.3722908766593664
Loss in iteration 27 : 0.3738683439100024
Loss in iteration 28 : 0.3692185079692631
Loss in iteration 29 : 0.36039822305766844
Loss in iteration 30 : 0.3524789091221898
Loss in iteration 31 : 0.3494775218633576
Loss in iteration 32 : 0.35077683378334756
Loss in iteration 33 : 0.35156013498141175
Loss in iteration 34 : 0.34796532810876074
Loss in iteration 35 : 0.34141258283960496
Loss in iteration 36 : 0.3364968760196632
Loss in iteration 37 : 0.3354866159965095
Loss in iteration 38 : 0.3364156646416697
Loss in iteration 39 : 0.33593374588902575
Loss in iteration 40 : 0.33291081060737965
Loss in iteration 41 : 0.3293427289983358
Loss in iteration 42 : 0.32799303784728323
Loss in iteration 43 : 0.3290772267367097
Loss in iteration 44 : 0.32992602250391256
Loss in iteration 45 : 0.3286695715045811
Loss in iteration 46 : 0.32678649442207686
Loss in iteration 47 : 0.3265536354369494
Loss in iteration 48 : 0.3277578973538242
Loss in iteration 49 : 0.3284416315824569
Loss in iteration 50 : 0.32773732768425334
Loss in iteration 51 : 0.32677966064708736
Loss in iteration 52 : 0.32689557414529513
Loss in iteration 53 : 0.3276892894593665
Loss in iteration 54 : 0.3277809265148158
Loss in iteration 55 : 0.3269983763595985
Loss in iteration 56 : 0.32643030215586377
Loss in iteration 57 : 0.3265829925860024
Loss in iteration 58 : 0.32681638675766017
Loss in iteration 59 : 0.32650370710569004
Loss in iteration 60 : 0.3258816181824631
Loss in iteration 61 : 0.3255680787651539
Loss in iteration 62 : 0.3256556927764537
Loss in iteration 63 : 0.3256784007467811
Loss in iteration 64 : 0.3253727327019007
Loss in iteration 65 : 0.32501060569977447
Loss in iteration 66 : 0.3249074038067685
Loss in iteration 67 : 0.32498536131840905
Loss in iteration 68 : 0.324967431153806
Loss in iteration 69 : 0.3247800093923613
Loss in iteration 70 : 0.32460150350325523
Loss in iteration 71 : 0.32457780811877146
Loss in iteration 72 : 0.3246336381120813
Loss in iteration 73 : 0.32460979964550307
Loss in iteration 74 : 0.3244887464842674
Loss in iteration 75 : 0.32438834755937834
Loss in iteration 76 : 0.324377466166427
Loss in iteration 77 : 0.3243956406725953
Loss in iteration 78 : 0.3243574568181771
Loss in iteration 79 : 0.32426692236690335
Loss in iteration 80 : 0.32419515874588933
Loss in iteration 81 : 0.32417613110671406
Loss in iteration 82 : 0.32416886837696535
Loss in iteration 83 : 0.3241249437611199
Loss in iteration 84 : 0.324054465360274
Loss in iteration 85 : 0.3240022290498299
Loss in iteration 86 : 0.3239821435670888
Loss in iteration 87 : 0.3239650921850927
Loss in iteration 88 : 0.32392561285442795
Loss in iteration 89 : 0.3238748055132957
Loss in iteration 90 : 0.32383906702334087
Loss in iteration 91 : 0.323822205605324
Loss in iteration 92 : 0.32380413701930055
Loss in iteration 93 : 0.3237720147111348
Loss in iteration 94 : 0.3237361572336121
Loss in iteration 95 : 0.32371164712316625
Loss in iteration 96 : 0.3236968721982739
Loss in iteration 97 : 0.3236786789198181
Loss in iteration 98 : 0.3236521139496861
Loss in iteration 99 : 0.32362545953346944
Loss in iteration 100 : 0.3236064961351522
Loss in iteration 101 : 0.32359192184988533
Loss in iteration 102 : 0.3235738017447802
Loss in iteration 103 : 0.32355124839953814
Loss in iteration 104 : 0.3235303174171838
Loss in iteration 105 : 0.32351440040266183
Loss in iteration 106 : 0.3235000947333825
Loss in iteration 107 : 0.32348315002059264
Loss in iteration 108 : 0.32346443824033766
Loss in iteration 109 : 0.32344780403315493
Loss in iteration 110 : 0.32343424135985915
Loss in iteration 111 : 0.3234210458249092
Loss in iteration 112 : 0.32340620749981264
Loss in iteration 113 : 0.3233910300821408
Loss in iteration 114 : 0.32337768613761303
Loss in iteration 115 : 0.323366034880391
Loss in iteration 116 : 0.32335422638692074
Loss in iteration 117 : 0.32334153728112397
Loss in iteration 118 : 0.3233291220971103
Loss in iteration 119 : 0.32331804360164645
Loss in iteration 120 : 0.3233077984011069
Loss in iteration 121 : 0.3232972804103688
Loss in iteration 122 : 0.32328640245837037
Loss in iteration 123 : 0.32327600872678586
Loss in iteration 124 : 0.3232665174533697
Loss in iteration 125 : 0.3232574115146389
Loss in iteration 126 : 0.323248120715132
Loss in iteration 127 : 0.3232388127772841
Loss in iteration 128 : 0.32323001600732515
Loss in iteration 129 : 0.3232218129211845
Loss in iteration 130 : 0.3232138022254834
Loss in iteration 131 : 0.3232057405281387
Loss in iteration 132 : 0.32319784478817
Loss in iteration 133 : 0.3231903963023152
Loss in iteration 134 : 0.32318332832732816
Loss in iteration 135 : 0.3231763755516448
Loss in iteration 136 : 0.3231694719986497
Loss in iteration 137 : 0.3231627960240405
Loss in iteration 138 : 0.32315646523904024
Loss in iteration 139 : 0.3231503728920734
Loss in iteration 140 : 0.3231443691291882
Loss in iteration 141 : 0.32313846947859226
Loss in iteration 142 : 0.32313279269748885
Loss in iteration 143 : 0.32312736556122756
Loss in iteration 144 : 0.32312209399471115
Loss in iteration 145 : 0.3231169102309786
Loss in iteration 146 : 0.32311185606063036
Loss in iteration 147 : 0.32310699671407783
Loss in iteration 148 : 0.3231023181554513
Loss in iteration 149 : 0.32309775586320194
Loss in iteration 150 : 0.32309329005569837
Loss in iteration 151 : 0.3230889601661328
Loss in iteration 152 : 0.32308479293197045
Loss in iteration 153 : 0.3230807620487057
Loss in iteration 154 : 0.32307683070340754
Loss in iteration 155 : 0.3230730011485505
Loss in iteration 156 : 0.3230692995602329
Loss in iteration 157 : 0.3230657297027569
Loss in iteration 158 : 0.3230622671280226
Loss in iteration 159 : 0.32305889467987214
Loss in iteration 160 : 0.3230556210215419
Loss in iteration 161 : 0.32305245889741274
Loss in iteration 162 : 0.3230494016834859
Loss in iteration 163 : 0.32304643208841477
Loss in iteration 164 : 0.32304354459711243
Loss in iteration 165 : 0.32304074699000435
Loss in iteration 166 : 0.3230380426596307
Loss in iteration 167 : 0.3230354226558493
Loss in iteration 168 : 0.32303287708176687
Loss in iteration 169 : 0.3230304058784403
Loss in iteration 170 : 0.32302801359612404
Loss in iteration 171 : 0.32302569864414
Loss in iteration 172 : 0.3230234535217915
Loss in iteration 173 : 0.323021273646878
Loss in iteration 174 : 0.3230191604377226
Loss in iteration 175 : 0.32301711525494897
Loss in iteration 176 : 0.3230151346438315
Loss in iteration 177 : 0.32301321350980045
Loss in iteration 178 : 0.32301135015128957
Loss in iteration 179 : 0.32300954550558725
Loss in iteration 180 : 0.32300779877045127
Loss in iteration 181 : 0.323006106426627
Loss in iteration 182 : 0.3230044654478099
Loss in iteration 183 : 0.3230028752527788
Loss in iteration 184 : 0.3230013357924777
Loss in iteration 185 : 0.3229998452755308
Loss in iteration 186 : 0.3229984009157359
Loss in iteration 187 : 0.32299700105145973
Loss in iteration 188 : 0.3229956452880379
Loss in iteration 189 : 0.32299433282206874
Loss in iteration 190 : 0.32299306173613834
Loss in iteration 191 : 0.3229918301158455
Loss in iteration 192 : 0.3229906370132836
Loss in iteration 193 : 0.32298948187136206
Loss in iteration 194 : 0.3229883635378584
Loss in iteration 195 : 0.32298728039187036
Loss in iteration 196 : 0.3229862311884425
Loss in iteration 197 : 0.32298521524309587
Loss in iteration 198 : 0.3229842318047953
Loss in iteration 199 : 0.32298327969008517
Testing accuracy  of updater 6 on alg 0 with rate 0.02 = 0.78975, training accuracy 0.8442861767562317, time elapsed: 2213 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4856639882100534
Loss in iteration 3 : 0.5729304742220778
Loss in iteration 4 : 0.5875038066091989
Loss in iteration 5 : 0.5043970840509616
Loss in iteration 6 : 0.38648934788020867
Loss in iteration 7 : 0.3374439506860034
Loss in iteration 8 : 0.3885125728075986
Loss in iteration 9 : 0.4337817822304461
Loss in iteration 10 : 0.40393473119538725
Loss in iteration 11 : 0.35352780831172853
Loss in iteration 12 : 0.33771305130298834
Loss in iteration 13 : 0.3547163519211861
Loss in iteration 14 : 0.3790974093238019
Loss in iteration 15 : 0.3918247890817478
Loss in iteration 16 : 0.38788363781105484
Loss in iteration 17 : 0.3734884356817555
Loss in iteration 18 : 0.3598086323402815
Loss in iteration 19 : 0.3559000269455029
Loss in iteration 20 : 0.36301078465147735
Loss in iteration 21 : 0.3737450753015085
Loss in iteration 22 : 0.37838180640561897
Loss in iteration 23 : 0.373455997571489
Loss in iteration 24 : 0.36380931672455974
Loss in iteration 25 : 0.3567719301409277
Loss in iteration 26 : 0.3557102730102329
Loss in iteration 27 : 0.35870833366801785
Loss in iteration 28 : 0.36145515945590106
Loss in iteration 29 : 0.3606956975622972
Loss in iteration 30 : 0.3560591107767305
Loss in iteration 31 : 0.34984268785891165
Loss in iteration 32 : 0.3452217318618527
Loss in iteration 33 : 0.34391292609923496
Loss in iteration 34 : 0.3448310626408616
Loss in iteration 35 : 0.3450734374239062
Loss in iteration 36 : 0.3426888451917543
Loss in iteration 37 : 0.33849144642433826
Loss in iteration 38 : 0.3349580531397732
Loss in iteration 39 : 0.33363671450282284
Loss in iteration 40 : 0.33392062294471925
Loss in iteration 41 : 0.3340210004552352
Loss in iteration 42 : 0.3327485748760157
Loss in iteration 43 : 0.3304675993253933
Loss in iteration 44 : 0.32859281291788933
Loss in iteration 45 : 0.32815256531655024
Loss in iteration 46 : 0.32874193353180414
Loss in iteration 47 : 0.32901193912244187
Loss in iteration 48 : 0.3282475936207532
Loss in iteration 49 : 0.3271131095824933
Loss in iteration 50 : 0.3266699499771719
Loss in iteration 51 : 0.32708739034023127
Loss in iteration 52 : 0.3276136470131654
Loss in iteration 53 : 0.32755715047799533
Loss in iteration 54 : 0.3270089814567971
Loss in iteration 55 : 0.32660744030392125
Loss in iteration 56 : 0.3267331966074306
Loss in iteration 57 : 0.3270872660759561
Loss in iteration 58 : 0.32713075028346106
Loss in iteration 59 : 0.32677485983154786
Loss in iteration 60 : 0.3264110022320391
Loss in iteration 61 : 0.32634614760260827
Loss in iteration 62 : 0.32646256009618196
Loss in iteration 63 : 0.3264508967984334
Loss in iteration 64 : 0.3262055381267071
Loss in iteration 65 : 0.32590521790415944
Loss in iteration 66 : 0.32575945969882986
Loss in iteration 67 : 0.3257651167978143
Loss in iteration 68 : 0.3257494199926686
Loss in iteration 69 : 0.32560464558629665
Loss in iteration 70 : 0.32540278526056665
Loss in iteration 71 : 0.3252757064349438
Loss in iteration 72 : 0.3252516031441477
Loss in iteration 73 : 0.32524589015987926
Loss in iteration 74 : 0.3251792530126005
Loss in iteration 75 : 0.3250632058752351
Loss in iteration 76 : 0.3249682489350699
Loss in iteration 77 : 0.32493362185642566
Loss in iteration 78 : 0.32492846971020684
Loss in iteration 79 : 0.3248985144276258
Loss in iteration 80 : 0.32483072893531845
Loss in iteration 81 : 0.32475933856326816
Loss in iteration 82 : 0.3247176225079205
Loss in iteration 83 : 0.3247007495030316
Loss in iteration 84 : 0.32467851901802885
Loss in iteration 85 : 0.32463332165545694
Loss in iteration 86 : 0.32457681519665965
Loss in iteration 87 : 0.324531520718886
Loss in iteration 88 : 0.32450395450340824
Loss in iteration 89 : 0.32448030095970976
Loss in iteration 90 : 0.324445541271285
Loss in iteration 91 : 0.3244005447382796
Loss in iteration 92 : 0.32435821109137186
Loss in iteration 93 : 0.3243268440714509
Loss in iteration 94 : 0.3243018363456672
Loss in iteration 95 : 0.32427331841121415
Loss in iteration 96 : 0.324238166932479
Loss in iteration 97 : 0.3242022454590348
Loss in iteration 98 : 0.32417221125694223
Loss in iteration 99 : 0.3241478898510687
Loss in iteration 100 : 0.3241236766559764
Loss in iteration 101 : 0.32409582456835895
Loss in iteration 102 : 0.32406631154743254
Loss in iteration 103 : 0.324039490862609
Loss in iteration 104 : 0.32401666212695723
Loss in iteration 105 : 0.32399512548065784
Loss in iteration 106 : 0.3239719438007922
Loss in iteration 107 : 0.32394726897259646
Loss in iteration 108 : 0.32392355544261414
Loss in iteration 109 : 0.32390230937564496
Loss in iteration 110 : 0.3238825143632132
Loss in iteration 111 : 0.32386223054059604
Loss in iteration 112 : 0.3238409658112291
Loss in iteration 113 : 0.3238199542833573
Loss in iteration 114 : 0.3238004051632809
Loss in iteration 115 : 0.3237821120689854
Loss in iteration 116 : 0.3237639491530759
Loss in iteration 117 : 0.3237453272220221
Loss in iteration 118 : 0.32372676184160176
Loss in iteration 119 : 0.323709069608994
Loss in iteration 120 : 0.32369236819681085
Loss in iteration 121 : 0.32367606462027887
Loss in iteration 122 : 0.32365966355913134
Loss in iteration 123 : 0.32364331213092545
Loss in iteration 124 : 0.32362750082034203
Loss in iteration 125 : 0.3236124238544083
Loss in iteration 126 : 0.3235978015076675
Loss in iteration 127 : 0.3235832818684119
Loss in iteration 128 : 0.3235688496151109
Loss in iteration 129 : 0.32355476977100434
Loss in iteration 130 : 0.323541218659825
Loss in iteration 131 : 0.32352808649468795
Loss in iteration 132 : 0.3235151481064956
Loss in iteration 133 : 0.32350233593340827
Loss in iteration 134 : 0.32348977983909005
Loss in iteration 135 : 0.32347761110326734
Loss in iteration 136 : 0.3234658021661928
Loss in iteration 137 : 0.3234542212357956
Loss in iteration 138 : 0.3234427979459087
Loss in iteration 139 : 0.3234315870337213
Loss in iteration 140 : 0.32342067519395556
Loss in iteration 141 : 0.323410068478538
Loss in iteration 142 : 0.3233996947352646
Loss in iteration 143 : 0.32338949684235746
Loss in iteration 144 : 0.32337949115173653
Loss in iteration 145 : 0.32336972938613245
Loss in iteration 146 : 0.3233602264403094
Loss in iteration 147 : 0.3233509447394716
Loss in iteration 148 : 0.32334184282522593
Loss in iteration 149 : 0.32333291938588066
Loss in iteration 150 : 0.32332420211591134
Loss in iteration 151 : 0.3233157047273487
Loss in iteration 152 : 0.32330740839099803
Loss in iteration 153 : 0.32329928484494386
Loss in iteration 154 : 0.32329132633168506
Loss in iteration 155 : 0.3232835459448108
Loss in iteration 156 : 0.32327595349758365
Loss in iteration 157 : 0.32326853986783427
Loss in iteration 158 : 0.32326128666578124
Loss in iteration 159 : 0.32325418512287407
Loss in iteration 160 : 0.3232472402366997
Loss in iteration 161 : 0.3232404580352423
Loss in iteration 162 : 0.3232338341255998
Loss in iteration 163 : 0.32322735680691317
Loss in iteration 164 : 0.32322101835670997
Loss in iteration 165 : 0.3232148196147707
Loss in iteration 166 : 0.3232087636790535
Loss in iteration 167 : 0.32320284820166645
Loss in iteration 168 : 0.3231970656684007
Loss in iteration 169 : 0.32319140983365663
Loss in iteration 170 : 0.3231858795213744
Loss in iteration 171 : 0.3231804757649859
Loss in iteration 172 : 0.3231751969210567
Loss in iteration 173 : 0.3231700379407589
Loss in iteration 174 : 0.3231649938897706
Loss in iteration 175 : 0.32316006273183123
Loss in iteration 176 : 0.32315524419427155
Loss in iteration 177 : 0.32315053679781225
Loss in iteration 178 : 0.323145936936983
Loss in iteration 179 : 0.3231414407265783
Loss in iteration 180 : 0.3231370458921258
Loss in iteration 181 : 0.32313275143616
Loss in iteration 182 : 0.32312855589443484
Loss in iteration 183 : 0.3231244565329812
Loss in iteration 184 : 0.3231204502730877
Loss in iteration 185 : 0.3231165349130978
Loss in iteration 186 : 0.32311270912518264
Loss in iteration 187 : 0.3231089714585437
Loss in iteration 188 : 0.3231053197311212
Loss in iteration 189 : 0.323101751467674
Loss in iteration 190 : 0.3230982646607684
Loss in iteration 191 : 0.3230948578769168
Loss in iteration 192 : 0.323091529700655
Loss in iteration 193 : 0.32308827830886194
Loss in iteration 194 : 0.3230851016633668
Loss in iteration 195 : 0.3230819979727952
Loss in iteration 196 : 0.3230789658141166
Loss in iteration 197 : 0.3230760038290346
Loss in iteration 198 : 0.3230731104406144
Loss in iteration 199 : 0.32307028392834936
Loss in iteration 200 : 0.3230675227028296
Testing accuracy  of updater 6 on alg 0 with rate 0.014 = 0.78975, training accuracy 0.8433149886694723, time elapsed: 2345 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4808210496491019
Loss in iteration 3 : 0.5057160805215256
Loss in iteration 4 : 0.5443131591350605
Loss in iteration 5 : 0.5309383945599035
Loss in iteration 6 : 0.46902852738523476
Loss in iteration 7 : 0.3920775802740692
Loss in iteration 8 : 0.3466377588129881
Loss in iteration 9 : 0.3561231567996288
Loss in iteration 10 : 0.3901839025666458
Loss in iteration 11 : 0.3995664997596063
Loss in iteration 12 : 0.3757968947834805
Loss in iteration 13 : 0.3459944587567077
Loss in iteration 14 : 0.3322969719219175
Loss in iteration 15 : 0.3365204751416846
Loss in iteration 16 : 0.349025915467601
Loss in iteration 17 : 0.35974922132759485
Loss in iteration 18 : 0.36324896903151027
Loss in iteration 19 : 0.3592726491246191
Loss in iteration 20 : 0.3512170144908487
Loss in iteration 21 : 0.34379361822178844
Loss in iteration 22 : 0.34065747807196045
Loss in iteration 23 : 0.34270032804850364
Loss in iteration 24 : 0.3477757823150811
Loss in iteration 25 : 0.3522013573663852
Loss in iteration 26 : 0.35320009172035566
Loss in iteration 27 : 0.35053015933745774
Loss in iteration 28 : 0.34617649222215585
Loss in iteration 29 : 0.3426428287339362
Loss in iteration 30 : 0.34133970919838896
Loss in iteration 31 : 0.3420754734142075
Loss in iteration 32 : 0.3435891400161041
Loss in iteration 33 : 0.3444770762472528
Loss in iteration 34 : 0.3439305314827539
Loss in iteration 35 : 0.34201684683590605
Loss in iteration 36 : 0.3395010367900965
Loss in iteration 37 : 0.3373623624163163
Loss in iteration 38 : 0.33624970915312463
Loss in iteration 39 : 0.33614772842556356
Loss in iteration 40 : 0.33644522777320013
Loss in iteration 41 : 0.33637456863512194
Loss in iteration 42 : 0.335528188216656
Loss in iteration 43 : 0.3340845425857485
Loss in iteration 44 : 0.332604438499551
Loss in iteration 45 : 0.3315988469183154
Loss in iteration 46 : 0.33120859550890525
Loss in iteration 47 : 0.3311876928065978
Loss in iteration 48 : 0.3311330509127438
Loss in iteration 49 : 0.33076406440786743
Loss in iteration 50 : 0.33007568074944155
Loss in iteration 51 : 0.3292983373186026
Loss in iteration 52 : 0.32871612377340254
Loss in iteration 53 : 0.3284700192092905
Loss in iteration 54 : 0.32847593102118217
Loss in iteration 55 : 0.32850840123978386
Loss in iteration 56 : 0.32837955575015965
Loss in iteration 57 : 0.3280701423787577
Loss in iteration 58 : 0.32771559639038705
Loss in iteration 59 : 0.3274771464193601
Loss in iteration 60 : 0.3274161185729194
Loss in iteration 61 : 0.32746588314475056
Loss in iteration 62 : 0.32750241605917013
Loss in iteration 63 : 0.3274424105405327
Loss in iteration 64 : 0.3272941553108888
Loss in iteration 65 : 0.3271342464332754
Loss in iteration 66 : 0.3270389316600272
Loss in iteration 67 : 0.3270263144885077
Loss in iteration 68 : 0.3270508382965856
Loss in iteration 69 : 0.3270475240414273
Loss in iteration 70 : 0.3269847706238818
Loss in iteration 71 : 0.32688260802400837
Loss in iteration 72 : 0.3267874442430611
Loss in iteration 73 : 0.3267311112471182
Loss in iteration 74 : 0.32670932077845716
Loss in iteration 75 : 0.3266921118500335
Loss in iteration 76 : 0.3266516012324032
Loss in iteration 77 : 0.3265826382726438
Loss in iteration 78 : 0.3265024672188194
Loss in iteration 79 : 0.3264337022792057
Loss in iteration 80 : 0.3263863631087612
Loss in iteration 81 : 0.3263526606698923
Loss in iteration 82 : 0.32631631815180895
Loss in iteration 83 : 0.3262667451194401
Loss in iteration 84 : 0.3262062077498751
Loss in iteration 85 : 0.32614569941110444
Loss in iteration 86 : 0.3260948774501985
Loss in iteration 87 : 0.3260550039582867
Loss in iteration 88 : 0.32601967195507064
Loss in iteration 89 : 0.3259811601273135
Loss in iteration 90 : 0.3259364878729238
Loss in iteration 91 : 0.32588871208048265
Loss in iteration 92 : 0.32584344099435253
Loss in iteration 93 : 0.3258040876685864
Loss in iteration 94 : 0.3257696600354079
Loss in iteration 95 : 0.32573629054635844
Loss in iteration 96 : 0.3257006646615029
Loss in iteration 97 : 0.3256623901192704
Loss in iteration 98 : 0.3256236971459682
Loss in iteration 99 : 0.3255872050628934
Loss in iteration 100 : 0.32555382861766696
Loss in iteration 101 : 0.32552238428430413
Loss in iteration 102 : 0.3254908592114786
Loss in iteration 103 : 0.32545806161430985
Loss in iteration 104 : 0.3254243400542858
Loss in iteration 105 : 0.32539101126251413
Loss in iteration 106 : 0.32535916991599423
Loss in iteration 107 : 0.3253288905282392
Loss in iteration 108 : 0.3252993512090311
Loss in iteration 109 : 0.32526961625233586
Loss in iteration 110 : 0.3252393618573013
Loss in iteration 111 : 0.32520900265323244
Loss in iteration 112 : 0.32517923374677815
Loss in iteration 113 : 0.32515044658142445
Loss in iteration 114 : 0.32512248676144223
Loss in iteration 115 : 0.32509487270126375
Loss in iteration 116 : 0.32506722139967753
Loss in iteration 117 : 0.3250395190830679
Loss in iteration 118 : 0.3250120605651418
Loss in iteration 119 : 0.3249851651434548
Loss in iteration 120 : 0.32495892641001495
Loss in iteration 121 : 0.324933180350777
Loss in iteration 122 : 0.3249076760166276
Loss in iteration 123 : 0.3248822809487777
Loss in iteration 124 : 0.3248570574735761
Loss in iteration 125 : 0.3248321752103363
Loss in iteration 126 : 0.32480775746054935
Loss in iteration 127 : 0.3247837918297623
Loss in iteration 128 : 0.3247601617030546
Loss in iteration 129 : 0.32473675177252775
Loss in iteration 130 : 0.3247135327665338
Loss in iteration 131 : 0.32469056424053133
Loss in iteration 132 : 0.32466792857742144
Loss in iteration 133 : 0.3246456597634794
Loss in iteration 134 : 0.3246237227948932
Loss in iteration 135 : 0.324602050259501
Loss in iteration 136 : 0.3245805972697334
Loss in iteration 137 : 0.32455936885810055
Loss in iteration 138 : 0.3245384037330482
Loss in iteration 139 : 0.32451773509961207
Loss in iteration 140 : 0.3244973631849685
Loss in iteration 141 : 0.3244772585701684
Loss in iteration 142 : 0.3244573879699918
Loss in iteration 143 : 0.32443773825882405
Loss in iteration 144 : 0.32441832042272933
Loss in iteration 145 : 0.3243991541090739
Loss in iteration 146 : 0.3243802483857509
Loss in iteration 147 : 0.3243615943717222
Loss in iteration 148 : 0.3243431732625514
Loss in iteration 149 : 0.3243249706294396
Loss in iteration 150 : 0.3243069846675045
Loss in iteration 151 : 0.32428922307846986
Loss in iteration 152 : 0.3242716931392285
Loss in iteration 153 : 0.324254393985923
Loss in iteration 154 : 0.3242373167274284
Loss in iteration 155 : 0.32422045083241047
Loss in iteration 156 : 0.3242037906190242
Loss in iteration 157 : 0.3241873367491424
Loss in iteration 158 : 0.32417109249091414
Loss in iteration 159 : 0.32415505865560085
Loss in iteration 160 : 0.3241392314272591
Loss in iteration 161 : 0.3241236042502173
Loss in iteration 162 : 0.32410817153649635
Loss in iteration 163 : 0.3240929309408218
Loss in iteration 164 : 0.32407788268202664
Loss in iteration 165 : 0.32406302699695066
Loss in iteration 166 : 0.3240483620808465
Loss in iteration 167 : 0.3240338840458593
Loss in iteration 168 : 0.3240195885528768
Loss in iteration 169 : 0.3240054725169715
Loss in iteration 170 : 0.32399153452702495
Loss in iteration 171 : 0.3239777738883942
Loss in iteration 172 : 0.3239641893018346
Loss in iteration 173 : 0.32395077829071617
Loss in iteration 174 : 0.3239375376897755
Loss in iteration 175 : 0.32392446461014746
Loss in iteration 176 : 0.32391155702695856
Loss in iteration 177 : 0.32389881359368783
Loss in iteration 178 : 0.3238862329743702
Loss in iteration 179 : 0.3238738133130415
Loss in iteration 180 : 0.323861552235449
Loss in iteration 181 : 0.3238494472815969
Loss in iteration 182 : 0.32383749634422837
Loss in iteration 183 : 0.3238256977628686
Loss in iteration 184 : 0.32381405006252095
Loss in iteration 185 : 0.3238025516111501
Loss in iteration 186 : 0.3237912004838259
Loss in iteration 187 : 0.3237799946031146
Loss in iteration 188 : 0.3237689319919082
Loss in iteration 189 : 0.3237580109156544
Loss in iteration 190 : 0.32374722982075943
Loss in iteration 191 : 0.32373658715627646
Loss in iteration 192 : 0.3237260812430993
Loss in iteration 193 : 0.32371571028628887
Loss in iteration 194 : 0.3237054724937248
Loss in iteration 195 : 0.32369536618593103
Loss in iteration 196 : 0.3236853898104971
Loss in iteration 197 : 0.3236755418671923
Loss in iteration 198 : 0.32366582082046297
Loss in iteration 199 : 0.3236562250718805
Loss in iteration 200 : 0.3236467530033814
Testing accuracy  of updater 6 on alg 0 with rate 0.008 = 0.78775, training accuracy 0.8429912593072192, time elapsed: 2158 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5910311444634104
Loss in iteration 3 : 0.5052899454454637
Loss in iteration 4 : 0.47595764625612214
Loss in iteration 5 : 0.4796050546307442
Loss in iteration 6 : 0.4918666735688161
Loss in iteration 7 : 0.4996481309177854
Loss in iteration 8 : 0.4977234014398786
Loss in iteration 9 : 0.4853535958805088
Loss in iteration 10 : 0.4644018879469916
Loss in iteration 11 : 0.4383114525497283
Loss in iteration 12 : 0.4113792228944914
Loss in iteration 13 : 0.38796568309107976
Loss in iteration 14 : 0.37150538378912684
Loss in iteration 15 : 0.36349178269751803
Loss in iteration 16 : 0.362909909258349
Loss in iteration 17 : 0.36660622051100383
Loss in iteration 18 : 0.3706224782698019
Loss in iteration 19 : 0.3718419635692373
Loss in iteration 20 : 0.36903984716324806
Loss in iteration 21 : 0.3628833917534155
Loss in iteration 22 : 0.3551395556893431
Loss in iteration 23 : 0.34770122532091335
Loss in iteration 24 : 0.3419099759341852
Loss in iteration 25 : 0.3383202791828062
Loss in iteration 26 : 0.33680217745432006
Loss in iteration 27 : 0.3368010119364883
Loss in iteration 28 : 0.3376085021709245
Loss in iteration 29 : 0.33856899857006767
Loss in iteration 30 : 0.3392005273716458
Loss in iteration 31 : 0.33924036158166626
Loss in iteration 32 : 0.33863534511113
Loss in iteration 33 : 0.3374974915944681
Loss in iteration 34 : 0.3360421173029518
Loss in iteration 35 : 0.33452216127505247
Loss in iteration 36 : 0.33316931982835857
Loss in iteration 37 : 0.33214997374345456
Loss in iteration 38 : 0.3315410657941752
Loss in iteration 39 : 0.33132769562131437
Loss in iteration 40 : 0.3314203038088855
Loss in iteration 41 : 0.3316855544658107
Loss in iteration 42 : 0.3319824167163033
Loss in iteration 43 : 0.3321944242870996
Loss in iteration 44 : 0.33225097043539137
Loss in iteration 45 : 0.332134194086952
Loss in iteration 46 : 0.33187226177264645
Loss in iteration 47 : 0.3315232493453361
Loss in iteration 48 : 0.33115544464452507
Loss in iteration 49 : 0.33082956845589556
Loss in iteration 50 : 0.3305866734281029
Loss in iteration 51 : 0.3304431855176297
Loss in iteration 52 : 0.3303924927663839
Loss in iteration 53 : 0.3304111427369204
Loss in iteration 54 : 0.33046720270511704
Loss in iteration 55 : 0.33052851991913346
Loss in iteration 56 : 0.33056922649931614
Loss in iteration 57 : 0.33057359885426535
Loss in iteration 58 : 0.33053710509005996
Loss in iteration 59 : 0.3304650383643553
Loss in iteration 60 : 0.3303694911623522
Loss in iteration 61 : 0.3302655734806855
Loss in iteration 62 : 0.33016774279696703
Loss in iteration 63 : 0.33008693673172856
Loss in iteration 64 : 0.33002893090453306
Loss in iteration 65 : 0.32999404047014963
Loss in iteration 66 : 0.3299780006651538
Loss in iteration 67 : 0.32997364923700845
Loss in iteration 68 : 0.32997292614667306
Loss in iteration 69 : 0.32996871452767523
Loss in iteration 70 : 0.3299561566127154
Loss in iteration 71 : 0.3299332523085175
Loss in iteration 72 : 0.32990073788971275
Loss in iteration 73 : 0.32986140146648973
Loss in iteration 74 : 0.32981908805360227
Loss in iteration 75 : 0.3297776685200528
Loss in iteration 76 : 0.32974020218155276
Loss in iteration 77 : 0.3297084355038996
Loss in iteration 78 : 0.3296826780681418
Loss in iteration 79 : 0.32966200747111046
Loss in iteration 80 : 0.3296446945066036
Loss in iteration 81 : 0.32962871552564477
Loss in iteration 82 : 0.32961222759072817
Loss in iteration 83 : 0.3295939147831104
Loss in iteration 84 : 0.32957315883592286
Loss in iteration 85 : 0.32955003252199633
Loss in iteration 86 : 0.32952515081328637
Loss in iteration 87 : 0.3294994372688634
Loss in iteration 88 : 0.32947386973665743
Loss in iteration 89 : 0.3294492619264131
Loss in iteration 90 : 0.329426119771806
Loss in iteration 91 : 0.3294045890339245
Loss in iteration 92 : 0.3293844886600774
Loss in iteration 93 : 0.3293654073975533
Loss in iteration 94 : 0.32934683178538415
Loss in iteration 95 : 0.32932827262211767
Loss in iteration 96 : 0.3293093631740585
Loss in iteration 97 : 0.3292899132228679
Loss in iteration 98 : 0.3292699154157665
Loss in iteration 99 : 0.3292495113146444
Loss in iteration 100 : 0.32922893189663494
Loss in iteration 101 : 0.3292084300618714
Loss in iteration 102 : 0.3291882211678901
Loss in iteration 103 : 0.3291684428767296
Loss in iteration 104 : 0.3291491393087421
Loss in iteration 105 : 0.329130268316953
Loss in iteration 106 : 0.3291117259490935
Loss in iteration 107 : 0.3290933796010707
Loss in iteration 108 : 0.32907510111472277
Loss in iteration 109 : 0.3290567927406186
Loss in iteration 110 : 0.32903840174796034
Loss in iteration 111 : 0.3290199226816027
Loss in iteration 112 : 0.3290013890909351
Loss in iteration 113 : 0.32898285846347
Loss in iteration 114 : 0.32896439485497736
Loss in iteration 115 : 0.3289460533699483
Loss in iteration 116 : 0.32892786948396735
Loss in iteration 117 : 0.3289098546101227
Loss in iteration 118 : 0.32889199771119404
Loss in iteration 119 : 0.32887427149230125
Loss in iteration 120 : 0.3288566409875508
Loss in iteration 121 : 0.3288390722365915
Loss in iteration 122 : 0.32882153915010487
Loss in iteration 123 : 0.32880402740604636
Loss in iteration 124 : 0.32878653507784084
Loss in iteration 125 : 0.3287690704629582
Loss in iteration 126 : 0.3287516481043933
Loss in iteration 127 : 0.32873428420809125
Loss in iteration 128 : 0.32871699256798875
Loss in iteration 129 : 0.32869978179264986
Loss in iteration 130 : 0.32868265419523124
Loss in iteration 131 : 0.32866560627762786
Loss in iteration 132 : 0.32864863040426434
Loss in iteration 133 : 0.3286317170765903
Loss in iteration 134 : 0.3286148571981054
Loss in iteration 135 : 0.3285980438357088
Loss in iteration 136 : 0.328581273185513
Loss in iteration 137 : 0.3285645446796774
Loss in iteration 138 : 0.3285478603705986
Loss in iteration 139 : 0.32853122386170674
Loss in iteration 140 : 0.3285146391034617
Loss in iteration 141 : 0.32849810934363505
Loss in iteration 142 : 0.3284816364335046
Loss in iteration 143 : 0.3284652205760848
Loss in iteration 144 : 0.3284488604893595
Loss in iteration 145 : 0.3284325538711296
Loss in iteration 146 : 0.3284162980066342
Loss in iteration 147 : 0.32840009035799644
Loss in iteration 148 : 0.32838392900840796
Loss in iteration 149 : 0.32836781288959915
Loss in iteration 150 : 0.32835174178237403
Loss in iteration 151 : 0.32833571613212337
Loss in iteration 152 : 0.32831973675447085
Loss in iteration 153 : 0.32830380451661095
Loss in iteration 154 : 0.32828792006941526
Loss in iteration 155 : 0.32827208368013633
Loss in iteration 156 : 0.3282562951839695
Loss in iteration 157 : 0.3282405540429025
Loss in iteration 158 : 0.3282248594786018
Loss in iteration 159 : 0.328209210635802
Loss in iteration 160 : 0.3281936067340129
Loss in iteration 161 : 0.3281780471757832
Loss in iteration 162 : 0.32816253159539355
Loss in iteration 163 : 0.32814705984812936
Loss in iteration 164 : 0.32813163195350764
Loss in iteration 165 : 0.3281162480136833
Loss in iteration 166 : 0.3281009081297907
Loss in iteration 167 : 0.32808561233530187
Loss in iteration 168 : 0.32807036055806427
Loss in iteration 169 : 0.32805515261414153
Loss in iteration 170 : 0.32803998822885344
Loss in iteration 171 : 0.32802486707518247
Loss in iteration 172 : 0.32800978881763504
Loss in iteration 173 : 0.3279947531506432
Loss in iteration 174 : 0.3279797598238369
Loss in iteration 175 : 0.32796480865090955
Loss in iteration 176 : 0.3279498995030972
Loss in iteration 177 : 0.327935032291561
Loss in iteration 178 : 0.32792020694469215
Loss in iteration 179 : 0.3279054233863509
Loss in iteration 180 : 0.32789068151976386
Loss in iteration 181 : 0.3278759812196127
Loss in iteration 182 : 0.3278613223325588
Loss in iteration 183 : 0.32784670468446525
Loss in iteration 184 : 0.3278321280914285
Loss in iteration 185 : 0.327817592371357
Loss in iteration 186 : 0.32780309735337043
Loss in iteration 187 : 0.3277886428832638
Loss in iteration 188 : 0.32777422882450713
Loss in iteration 189 : 0.32775985505538213
Loss in iteration 190 : 0.3277455214636001
Loss in iteration 191 : 0.3277312279400908
Loss in iteration 192 : 0.32771697437351144
Loss in iteration 193 : 0.32770276064658566
Loss in iteration 194 : 0.3276885866347576
Loss in iteration 195 : 0.32767445220701585
Loss in iteration 196 : 0.32766035722830045
Loss in iteration 197 : 0.32764630156263114
Loss in iteration 198 : 0.3276322850760944
Loss in iteration 199 : 0.32761830763903965
Loss in iteration 200 : 0.3276043691271008
Testing accuracy  of updater 6 on alg 0 with rate 0.0019999999999999983 = 0.7845, training accuracy 0.8387827775979282, time elapsed: 2453 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 4.806587259903958
Loss in iteration 3 : 4.991969479442522
Loss in iteration 4 : 3.0984730392456847
Loss in iteration 5 : 0.7682986467837825
Loss in iteration 6 : 1.4426781655094987
Loss in iteration 7 : 2.5824259728989523
Loss in iteration 8 : 1.9202999038395139
Loss in iteration 9 : 1.0392282186858466
Loss in iteration 10 : 0.9344697202585254
Loss in iteration 11 : 1.253119538415422
Loss in iteration 12 : 1.5656581093950317
Loss in iteration 13 : 1.6818145466843029
Loss in iteration 14 : 1.583322409595971
Loss in iteration 15 : 1.361894527949007
Loss in iteration 16 : 1.1420692455481056
Loss in iteration 17 : 1.0220625402047332
Loss in iteration 18 : 1.0516337664841022
Loss in iteration 19 : 1.1856676073088133
Loss in iteration 20 : 1.2899635367051245
Loss in iteration 21 : 1.2747232772268704
Loss in iteration 22 : 1.1576265993236907
Loss in iteration 23 : 1.0287854175873838
Loss in iteration 24 : 0.9665119132196904
Loss in iteration 25 : 0.9772148033740691
Loss in iteration 26 : 1.0189790225310402
Loss in iteration 27 : 1.0486119208431732
Loss in iteration 28 : 1.040594951853552
Loss in iteration 29 : 0.9924994866906895
Loss in iteration 30 : 0.9221035644720619
Loss in iteration 31 : 0.8575983956490406
Loss in iteration 32 : 0.8238492120007189
Loss in iteration 33 : 0.8263197465009258
Loss in iteration 34 : 0.8401787648562322
Loss in iteration 35 : 0.8302201648793669
Loss in iteration 36 : 0.7855182391462915
Loss in iteration 37 : 0.7283595784642081
Loss in iteration 38 : 0.6890994153230017
Loss in iteration 39 : 0.6759465334978614
Loss in iteration 40 : 0.673357815540104
Loss in iteration 41 : 0.6611023305163174
Loss in iteration 42 : 0.6297981069617454
Loss in iteration 43 : 0.5866564424983951
Loss in iteration 44 : 0.5498818274606125
Loss in iteration 45 : 0.5327571401978695
Loss in iteration 46 : 0.5267986244190881
Loss in iteration 47 : 0.5087478862997518
Loss in iteration 48 : 0.4729981591931643
Loss in iteration 49 : 0.44088000091832774
Loss in iteration 50 : 0.42743110049806676
Loss in iteration 51 : 0.42070906945887315
Loss in iteration 52 : 0.40202175093048936
Loss in iteration 53 : 0.3735975508050556
Loss in iteration 54 : 0.35764487142078877
Loss in iteration 55 : 0.35839626293143956
Loss in iteration 56 : 0.347280734866949
Loss in iteration 57 : 0.32903239440431575
Loss in iteration 58 : 0.33293991896342356
Loss in iteration 59 : 0.33723685402893705
Loss in iteration 60 : 0.32810527871540635
Loss in iteration 61 : 0.3389867650671854
Loss in iteration 62 : 0.3455290592636015
Loss in iteration 63 : 0.3417406931800927
Loss in iteration 64 : 0.3539045898486216
Loss in iteration 65 : 0.34689370313831247
Loss in iteration 66 : 0.3465492579583023
Loss in iteration 67 : 0.34517172493954795
Loss in iteration 68 : 0.33535027358944924
Loss in iteration 69 : 0.3362204811898132
Loss in iteration 70 : 0.33018917003055137
Loss in iteration 71 : 0.32655495432111314
Loss in iteration 72 : 0.3283106965576378
Loss in iteration 73 : 0.3250878323394733
Loss in iteration 74 : 0.3247895813345185
Loss in iteration 75 : 0.32702228609543377
Loss in iteration 76 : 0.3261240711570346
Loss in iteration 77 : 0.32605545482677556
Loss in iteration 78 : 0.3280270166583139
Loss in iteration 79 : 0.3278100851456286
Loss in iteration 80 : 0.3271795085595701
Loss in iteration 81 : 0.3281440706399798
Loss in iteration 82 : 0.32803684842316566
Loss in iteration 83 : 0.32689387792732566
Loss in iteration 84 : 0.326910752479506
Loss in iteration 85 : 0.32677251638037885
Loss in iteration 86 : 0.3255711812413969
Loss in iteration 87 : 0.32516265659230265
Loss in iteration 88 : 0.325068737826277
Loss in iteration 89 : 0.3241891301871666
Loss in iteration 90 : 0.3238160424817589
Loss in iteration 91 : 0.32393621175353704
Loss in iteration 92 : 0.32345623593542444
Loss in iteration 93 : 0.32335754986096577
Loss in iteration 94 : 0.32363770649768253
Loss in iteration 95 : 0.323433750231103
Loss in iteration 96 : 0.3235060045155882
Loss in iteration 97 : 0.32377051734504986
Loss in iteration 98 : 0.32360775054486923
Loss in iteration 99 : 0.32368397635515395
Loss in iteration 100 : 0.3237785179422918
Loss in iteration 101 : 0.32356878515028825
Loss in iteration 102 : 0.3235757257421569
Loss in iteration 103 : 0.3235343662508978
Loss in iteration 104 : 0.32332602186618575
Loss in iteration 105 : 0.32331724268133827
Loss in iteration 106 : 0.32324696122739377
Loss in iteration 107 : 0.3231105776233775
Loss in iteration 108 : 0.32313311523084043
Loss in iteration 109 : 0.3230963644280551
Loss in iteration 110 : 0.3230361117534491
Loss in iteration 111 : 0.3230848186601885
Loss in iteration 112 : 0.3230753703221335
Loss in iteration 113 : 0.32305294902801857
Loss in iteration 114 : 0.3230983497781935
Loss in iteration 115 : 0.3230892858329492
Loss in iteration 116 : 0.3230711795382523
Loss in iteration 117 : 0.32309547630299273
Loss in iteration 118 : 0.3230751674041544
Loss in iteration 119 : 0.3230516973581501
Loss in iteration 120 : 0.32305850552946785
Loss in iteration 121 : 0.3230327740788868
Loss in iteration 122 : 0.3230113482511247
Loss in iteration 123 : 0.3230130775219222
Loss in iteration 124 : 0.32299248893802235
Loss in iteration 125 : 0.3229807989588263
Loss in iteration 126 : 0.3229855802562606
Loss in iteration 127 : 0.3229738823141075
Loss in iteration 128 : 0.3229720101288775
Loss in iteration 129 : 0.3229790612296398
Loss in iteration 130 : 0.32297291689586216
Loss in iteration 131 : 0.32297515818327277
Loss in iteration 132 : 0.3229801255306563
Loss in iteration 133 : 0.32297475512999035
Loss in iteration 134 : 0.3229763470994642
Loss in iteration 135 : 0.32297710809545604
Loss in iteration 136 : 0.32297109789051126
Loss in iteration 137 : 0.32297116247894203
Loss in iteration 138 : 0.3229692528723561
Loss in iteration 139 : 0.32296401014692994
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999998 = 0.7895, training accuracy 0.8449336354807381, time elapsed: 1592 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6761591185004534
Loss in iteration 3 : 0.786723946339388
Loss in iteration 4 : 0.5421460804800143
Loss in iteration 5 : 0.33759082176415905
Loss in iteration 6 : 0.5506820599528993
Loss in iteration 7 : 0.5002949372848793
Loss in iteration 8 : 0.349718404130148
Loss in iteration 9 : 0.3929796220960851
Loss in iteration 10 : 0.4767062341012954
Loss in iteration 11 : 0.47923640760230013
Loss in iteration 12 : 0.4222923988324405
Loss in iteration 13 : 0.38968657571690896
Loss in iteration 14 : 0.4213129436681173
Loss in iteration 15 : 0.4649712294722369
Loss in iteration 16 : 0.45740844849048
Loss in iteration 17 : 0.4203389885187268
Loss in iteration 18 : 0.4085130753978096
Loss in iteration 19 : 0.42712190365372976
Loss in iteration 20 : 0.4441009762750715
Loss in iteration 21 : 0.4371070120181809
Loss in iteration 22 : 0.4134914697081381
Loss in iteration 23 : 0.3980451227480094
Loss in iteration 24 : 0.4033313450596634
Loss in iteration 25 : 0.41226477256977956
Loss in iteration 26 : 0.4027414429803385
Loss in iteration 27 : 0.38246249371776475
Loss in iteration 28 : 0.37363475611915387
Loss in iteration 29 : 0.37747384408905754
Loss in iteration 30 : 0.3771902647992138
Loss in iteration 31 : 0.36455971678525356
Loss in iteration 32 : 0.35074429294908
Loss in iteration 33 : 0.34896404176889406
Loss in iteration 34 : 0.3517356867277313
Loss in iteration 35 : 0.3438876981510786
Loss in iteration 36 : 0.3331858104276753
Loss in iteration 37 : 0.3332224918943634
Loss in iteration 38 : 0.3358603196558247
Loss in iteration 39 : 0.3302442456104535
Loss in iteration 40 : 0.32490612103002314
Loss in iteration 41 : 0.32885708036293115
Loss in iteration 42 : 0.33008327859045616
Loss in iteration 43 : 0.32535434492557364
Loss in iteration 44 : 0.3274545815155179
Loss in iteration 45 : 0.3309442924015854
Loss in iteration 46 : 0.3281957409170807
Loss in iteration 47 : 0.3280792669057348
Loss in iteration 48 : 0.33123022115570816
Loss in iteration 49 : 0.3290321355279144
Loss in iteration 50 : 0.3278279081278214
Loss in iteration 51 : 0.3295169750882561
Loss in iteration 52 : 0.32791523927163274
Loss in iteration 53 : 0.3260078340046366
Loss in iteration 54 : 0.3268489473212892
Loss in iteration 55 : 0.32605506112203425
Loss in iteration 56 : 0.32435163944449624
Loss in iteration 57 : 0.3247131126398075
Loss in iteration 58 : 0.324777425273307
Loss in iteration 59 : 0.323676226724136
Loss in iteration 60 : 0.3236364676263196
Loss in iteration 61 : 0.3241909866027155
Loss in iteration 62 : 0.32375463576964747
Loss in iteration 63 : 0.323454844343018
Loss in iteration 64 : 0.3239361160548491
Loss in iteration 65 : 0.324015166539369
Loss in iteration 66 : 0.32365104447895265
Loss in iteration 67 : 0.32377264603205186
Loss in iteration 68 : 0.32403514520614835
Loss in iteration 69 : 0.32380875554985755
Loss in iteration 70 : 0.3236337759593579
Loss in iteration 71 : 0.32379394642021386
Loss in iteration 72 : 0.3237493300307704
Loss in iteration 73 : 0.3234891132758674
Loss in iteration 74 : 0.3234667473010194
Loss in iteration 75 : 0.32351784422796837
Loss in iteration 76 : 0.3233419664570226
Loss in iteration 77 : 0.3232162233065499
Loss in iteration 78 : 0.3232682648611226
Loss in iteration 79 : 0.32321903812684266
Loss in iteration 80 : 0.32309238964859727
Loss in iteration 81 : 0.3231083471940845
Loss in iteration 82 : 0.32314142195473944
Loss in iteration 83 : 0.32306781855242256
Loss in iteration 84 : 0.32305135402199087
Loss in iteration 85 : 0.323104914700424
Loss in iteration 86 : 0.32308438242406023
Loss in iteration 87 : 0.32305007948606823
Loss in iteration 88 : 0.3230852994322449
Loss in iteration 89 : 0.3230930928893634
Loss in iteration 90 : 0.32305613892767115
Loss in iteration 91 : 0.32306214419832296
Loss in iteration 92 : 0.3230763895712376
Loss in iteration 93 : 0.3230473903682969
Loss in iteration 94 : 0.3230321250242923
Loss in iteration 95 : 0.32304230745722734
Loss in iteration 96 : 0.3230262607771696
Loss in iteration 97 : 0.32300433699581166
Loss in iteration 98 : 0.323008015897722
Loss in iteration 99 : 0.32300437893570644
Loss in iteration 100 : 0.322986572328094
Loss in iteration 101 : 0.32298514767850794
Loss in iteration 102 : 0.3229891197373549
Loss in iteration 103 : 0.3229795143227537
Loss in iteration 104 : 0.3229752384712013
Loss in iteration 105 : 0.32298091902279447
Loss in iteration 106 : 0.32297835430419314
Loss in iteration 107 : 0.3229731082794497
Loss in iteration 108 : 0.322976515448941
Loss in iteration 109 : 0.322977614825471
Loss in iteration 110 : 0.3229728472504562
Loss in iteration 111 : 0.32297285232254735
Loss in iteration 112 : 0.32297465760412186
Loss in iteration 113 : 0.3229712332050821
Loss in iteration 114 : 0.32296883260981524
Loss in iteration 115 : 0.3229698651628505
Loss in iteration 116 : 0.3229680362155034
Loss in iteration 117 : 0.32296496727934776
Loss in iteration 118 : 0.322965000720524
Loss in iteration 119 : 0.3229645181392494
Testing accuracy  of updater 7 on alg 0 with rate 0.13999999999999999 = 0.79, training accuracy 0.8442861767562317, time elapsed: 1223 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5367744424872787
Loss in iteration 3 : 0.6294789126264102
Loss in iteration 4 : 0.5597343272612697
Loss in iteration 5 : 0.39289060726982994
Loss in iteration 6 : 0.37435336922127443
Loss in iteration 7 : 0.46301714509836844
Loss in iteration 8 : 0.41103387696561067
Loss in iteration 9 : 0.3356635769082291
Loss in iteration 10 : 0.34637833509462745
Loss in iteration 11 : 0.3874723517545432
Loss in iteration 12 : 0.3977658306074135
Loss in iteration 13 : 0.37389726395187284
Loss in iteration 14 : 0.3493321148508145
Loss in iteration 15 : 0.35145699881112513
Loss in iteration 16 : 0.3734723425152435
Loss in iteration 17 : 0.386459114358329
Loss in iteration 18 : 0.3777418366019231
Loss in iteration 19 : 0.3625513926609484
Loss in iteration 20 : 0.3589070860996152
Loss in iteration 21 : 0.3672986745478455
Loss in iteration 22 : 0.3759886073040168
Loss in iteration 23 : 0.3757120270797134
Loss in iteration 24 : 0.36732272324876375
Loss in iteration 25 : 0.3590526636103887
Loss in iteration 26 : 0.3577313929162282
Loss in iteration 27 : 0.36189385119689016
Loss in iteration 28 : 0.36392671128409715
Loss in iteration 29 : 0.3594530310245282
Loss in iteration 30 : 0.3521848400838375
Loss in iteration 31 : 0.34829666584112623
Loss in iteration 32 : 0.3488956939745806
Loss in iteration 33 : 0.3498842014908311
Loss in iteration 34 : 0.3474905300387106
Loss in iteration 35 : 0.3423575664160789
Loss in iteration 36 : 0.33830862281153873
Loss in iteration 37 : 0.33758292294059644
Loss in iteration 38 : 0.33810830233491934
Loss in iteration 39 : 0.3365150803324568
Loss in iteration 40 : 0.33291283722545334
Loss in iteration 41 : 0.3303674890966163
Loss in iteration 42 : 0.33022030093461485
Loss in iteration 43 : 0.3305332906406991
Loss in iteration 44 : 0.32927586547808546
Loss in iteration 45 : 0.3271381474875112
Loss in iteration 46 : 0.32623305019675686
Loss in iteration 47 : 0.3268158457029075
Loss in iteration 48 : 0.3270088946712391
Loss in iteration 49 : 0.3259841097118524
Loss in iteration 50 : 0.3251600117907806
Loss in iteration 51 : 0.3255349589460909
Loss in iteration 52 : 0.3261277845254661
Loss in iteration 53 : 0.3258646050827962
Loss in iteration 54 : 0.325283545072398
Loss in iteration 55 : 0.3253942561388095
Loss in iteration 56 : 0.3259068342380564
Loss in iteration 57 : 0.3258828012291094
Loss in iteration 58 : 0.3254394663824823
Loss in iteration 59 : 0.32535730622941766
Loss in iteration 60 : 0.32561941984705417
Loss in iteration 61 : 0.32560261912897875
Loss in iteration 62 : 0.32523856228571857
Loss in iteration 63 : 0.325025158199504
Loss in iteration 64 : 0.32509536007686335
Loss in iteration 65 : 0.3250632647937131
Loss in iteration 66 : 0.32478216579033353
Loss in iteration 67 : 0.3245535108221232
Loss in iteration 68 : 0.3245340955264548
Loss in iteration 69 : 0.3245125118316084
Loss in iteration 70 : 0.3243378559301245
Loss in iteration 71 : 0.32415485030824254
Loss in iteration 72 : 0.32411050600932223
Loss in iteration 73 : 0.32411376004252485
Loss in iteration 74 : 0.3240299625094633
Loss in iteration 75 : 0.3239089218816494
Loss in iteration 76 : 0.32386501247846755
Loss in iteration 77 : 0.3238774742349294
Loss in iteration 78 : 0.3238501515188892
Loss in iteration 79 : 0.3237782653369029
Loss in iteration 80 : 0.32373602655810135
Loss in iteration 81 : 0.32374146275878535
Loss in iteration 82 : 0.3237367440669594
Loss in iteration 83 : 0.32369402888353294
Loss in iteration 84 : 0.32365334715973537
Loss in iteration 85 : 0.3236445965627285
Loss in iteration 86 : 0.3236413883187077
Loss in iteration 87 : 0.323613610392825
Loss in iteration 88 : 0.3235756880186661
Loss in iteration 89 : 0.32355519702788293
Loss in iteration 90 : 0.3235464970027881
Loss in iteration 91 : 0.3235259697903388
Loss in iteration 92 : 0.3234934219495293
Loss in iteration 93 : 0.3234682546963591
Loss in iteration 94 : 0.3234548607991682
Loss in iteration 95 : 0.3234386467827568
Loss in iteration 96 : 0.3234132152648456
Loss in iteration 97 : 0.3233892921866629
Loss in iteration 98 : 0.3233744548774405
Loss in iteration 99 : 0.32336155975181047
Loss in iteration 100 : 0.3233430645586794
Loss in iteration 101 : 0.32332338576065184
Loss in iteration 102 : 0.3233094992172489
Loss in iteration 103 : 0.3232990806377279
Loss in iteration 104 : 0.32328596175657626
Loss in iteration 105 : 0.32327080554002136
Loss in iteration 106 : 0.32325866730917246
Loss in iteration 107 : 0.32324981956110577
Loss in iteration 108 : 0.32324007476206473
Loss in iteration 109 : 0.32322837858257075
Loss in iteration 110 : 0.3232178655305383
Loss in iteration 111 : 0.3232098326407452
Loss in iteration 112 : 0.32320187117778315
Loss in iteration 113 : 0.32319245618834513
Loss in iteration 114 : 0.3231832356276005
Loss in iteration 115 : 0.32317567743509984
Loss in iteration 116 : 0.3231686441547102
Loss in iteration 117 : 0.32316073326190203
Loss in iteration 118 : 0.32315263153907214
Loss in iteration 119 : 0.3231455757845256
Loss in iteration 120 : 0.32313919584108497
Loss in iteration 121 : 0.3231324287929239
Loss in iteration 122 : 0.32312541035229414
Loss in iteration 123 : 0.32311902579221236
Loss in iteration 124 : 0.32311330153838214
Loss in iteration 125 : 0.32310752216089655
Loss in iteration 126 : 0.32310155741174457
Loss in iteration 127 : 0.32309596578192845
Loss in iteration 128 : 0.3230909307447939
Loss in iteration 129 : 0.32308601947927784
Loss in iteration 130 : 0.32308101068207806
Loss in iteration 131 : 0.32307621295281613
Loss in iteration 132 : 0.3230718396766051
Loss in iteration 133 : 0.32306765872256216
Loss in iteration 134 : 0.32306345044134116
Loss in iteration 135 : 0.32305935889398063
Loss in iteration 136 : 0.3230555696101313
Loss in iteration 137 : 0.32305197884866665
Loss in iteration 138 : 0.3230484090959118
Loss in iteration 139 : 0.3230449081258467
Loss in iteration 140 : 0.32304161576520307
Loss in iteration 141 : 0.32303850222237807
Loss in iteration 142 : 0.3230354405038512
Loss in iteration 143 : 0.32303242926986075
Loss in iteration 144 : 0.3230295638336912
Loss in iteration 145 : 0.32302685089669997
Loss in iteration 146 : 0.32302420757825656
Loss in iteration 147 : 0.3230216118699107
Loss in iteration 148 : 0.3230191229523093
Loss in iteration 149 : 0.3230167612999341
Loss in iteration 150 : 0.32301447669905914
Loss in iteration 151 : 0.3230122417666276
Loss in iteration 152 : 0.323010089621537
Loss in iteration 153 : 0.32300804233954894
Loss in iteration 154 : 0.3230060715178664
Loss in iteration 155 : 0.3230041517095085
Loss in iteration 156 : 0.3230022989050249
Loss in iteration 157 : 0.3230005315949855
Loss in iteration 158 : 0.32299883487137343
Loss in iteration 159 : 0.32299718799048166
Loss in iteration 160 : 0.3229955967726341
Loss in iteration 161 : 0.32299407482160536
Loss in iteration 162 : 0.32299261507018523
Loss in iteration 163 : 0.32299120195448794
Loss in iteration 164 : 0.3229898359127707
Loss in iteration 165 : 0.32298852611954926
Loss in iteration 166 : 0.3229872697766475
Loss in iteration 167 : 0.32298605586649437
Loss in iteration 168 : 0.32298488240710804
Loss in iteration 169 : 0.3229837551527314
Loss in iteration 170 : 0.32298267341292614
Loss in iteration 171 : 0.3229816297019176
Loss in iteration 172 : 0.3229806212416255
Loss in iteration 173 : 0.3229796513503147
Loss in iteration 174 : 0.32297872020309515
Loss in iteration 175 : 0.32297782282633936
Loss in iteration 176 : 0.322976956427036
Loss in iteration 177 : 0.32297612268508624
Testing accuracy  of updater 7 on alg 0 with rate 0.08 = 0.78975, training accuracy 0.8439624473939786, time elapsed: 1799 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5663395200846489
Loss in iteration 3 : 0.5040880148577208
Loss in iteration 4 : 0.5133490845819867
Loss in iteration 5 : 0.5321059771556826
Loss in iteration 6 : 0.5298074391112354
Loss in iteration 7 : 0.5008697417313344
Loss in iteration 8 : 0.45398659929881596
Loss in iteration 9 : 0.40675933500234374
Loss in iteration 10 : 0.3782344526981591
Loss in iteration 11 : 0.37597357017950067
Loss in iteration 12 : 0.3885323954849082
Loss in iteration 13 : 0.39602530206892445
Loss in iteration 14 : 0.388044635443281
Loss in iteration 15 : 0.36909647973754994
Loss in iteration 16 : 0.3502822187704092
Loss in iteration 17 : 0.3394224216465347
Loss in iteration 18 : 0.33759374249019114
Loss in iteration 19 : 0.34123236924292927
Loss in iteration 20 : 0.34568411960528095
Loss in iteration 21 : 0.3476330054380035
Loss in iteration 22 : 0.34596264600386295
Loss in iteration 23 : 0.34149947717378765
Loss in iteration 24 : 0.33616307936251816
Loss in iteration 25 : 0.3319442803134756
Loss in iteration 26 : 0.33006130516108506
Loss in iteration 27 : 0.3305648259306308
Loss in iteration 28 : 0.3324997204354907
Loss in iteration 29 : 0.3344948075356975
Loss in iteration 30 : 0.33545411231332356
Loss in iteration 31 : 0.33499940838476755
Loss in iteration 32 : 0.3334892146488574
Loss in iteration 33 : 0.3316903560117093
Loss in iteration 34 : 0.33033944704857154
Loss in iteration 35 : 0.329827205850979
Loss in iteration 36 : 0.33011471203631354
Loss in iteration 37 : 0.3308517890786736
Loss in iteration 38 : 0.33159022013062345
Loss in iteration 39 : 0.33198121733740005
Loss in iteration 40 : 0.33188759810710095
Loss in iteration 41 : 0.33139173969955155
Loss in iteration 42 : 0.3307202948212619
Loss in iteration 43 : 0.33012922994251764
Loss in iteration 44 : 0.3297978444097023
Loss in iteration 45 : 0.3297699816230345
Loss in iteration 46 : 0.3299585332724715
Loss in iteration 47 : 0.3302028387567765
Loss in iteration 48 : 0.3303479036235873
Loss in iteration 49 : 0.3303085520205878
Loss in iteration 50 : 0.3300930076329673
Loss in iteration 51 : 0.32978228599023623
Loss in iteration 52 : 0.3294822460008648
Loss in iteration 53 : 0.3292743061818384
Loss in iteration 54 : 0.3291862611082153
Loss in iteration 55 : 0.32919139205901493
Loss in iteration 56 : 0.3292302757939806
Loss in iteration 57 : 0.3292415663716302
Loss in iteration 58 : 0.3291874557273249
Loss in iteration 59 : 0.329064708901702
Loss in iteration 60 : 0.3288996675438512
Loss in iteration 61 : 0.3287321518464155
Loss in iteration 62 : 0.32859665152094897
Loss in iteration 63 : 0.32850889392767835
Loss in iteration 64 : 0.3284624681679925
Loss in iteration 65 : 0.3284353477642284
Loss in iteration 66 : 0.3284019962452902
Loss in iteration 67 : 0.32834495784240053
Loss in iteration 68 : 0.32826096929547016
Loss in iteration 69 : 0.3281598266114515
Loss in iteration 70 : 0.32805775442136487
Loss in iteration 71 : 0.32796917329789677
Loss in iteration 72 : 0.32790077020040725
Loss in iteration 73 : 0.327850032100659
Loss in iteration 74 : 0.3278080361304071
Loss in iteration 75 : 0.3277644826047512
Loss in iteration 76 : 0.32771238146594844
Loss in iteration 77 : 0.3276504457971606
Loss in iteration 78 : 0.327582586991654
Loss in iteration 79 : 0.3275152505080337
Loss in iteration 80 : 0.3274541230408071
Loss in iteration 81 : 0.32740174953723017
Loss in iteration 82 : 0.3273569463366767
Loss in iteration 83 : 0.32731596406007074
Loss in iteration 84 : 0.32727458748819965
Loss in iteration 85 : 0.3272300750236126
Loss in iteration 86 : 0.3271820950955932
Loss in iteration 87 : 0.3271324132267641
Loss in iteration 88 : 0.3270836928627419
Loss in iteration 89 : 0.3270381113305966
Loss in iteration 90 : 0.32699644970548647
Loss in iteration 91 : 0.32695798002184456
Loss in iteration 92 : 0.32692105319506426
Loss in iteration 93 : 0.3268839933581109
Loss in iteration 94 : 0.3268458399346887
Loss in iteration 95 : 0.32680663189691633
Loss in iteration 96 : 0.3267671899750214
Loss in iteration 97 : 0.32672858600787524
Loss in iteration 98 : 0.32669159553757254
Loss in iteration 99 : 0.3266563848172457
Loss in iteration 100 : 0.326622532864467
Loss in iteration 101 : 0.3265893188425354
Loss in iteration 102 : 0.3265560957553141
Loss in iteration 103 : 0.3265225631946336
Loss in iteration 104 : 0.32648883155195696
Loss in iteration 105 : 0.32645528573093047
Loss in iteration 106 : 0.3264223478264824
Loss in iteration 107 : 0.3263902670154283
Loss in iteration 108 : 0.32635902883263473
Loss in iteration 109 : 0.3263284037348902
Loss in iteration 110 : 0.3262980866883628
Loss in iteration 111 : 0.32626784631793143
Loss in iteration 112 : 0.3262376127771887
Loss in iteration 113 : 0.3262074748971078
Loss in iteration 114 : 0.326177604579637
Loss in iteration 115 : 0.326148156856376
Loss in iteration 116 : 0.3261191965348365
Loss in iteration 117 : 0.3260906807536748
Loss in iteration 118 : 0.3260624953799912
Loss in iteration 119 : 0.32603451837134295
Loss in iteration 120 : 0.326006675445138
Loss in iteration 121 : 0.3259789632948331
Loss in iteration 122 : 0.3259514353663823
Loss in iteration 123 : 0.32592416375826366
Loss in iteration 124 : 0.32589719958248703
Loss in iteration 125 : 0.32587055074379384
Loss in iteration 126 : 0.32584418437962065
Loss in iteration 127 : 0.32581804822564714
Loss in iteration 128 : 0.32579209729745273
Loss in iteration 129 : 0.3257663122845963
Loss in iteration 130 : 0.32574070248279435
Loss in iteration 131 : 0.3257152947255155
Loss in iteration 132 : 0.3256901161355931
Loss in iteration 133 : 0.32566518001042727
Loss in iteration 134 : 0.32564048095949166
Loss in iteration 135 : 0.3256159998949159
Loss in iteration 136 : 0.3255917146767519
Loss in iteration 137 : 0.3255676102891523
Loss in iteration 138 : 0.32554368377126613
Loss in iteration 139 : 0.32551994251068145
Loss in iteration 140 : 0.3254963979386747
Loss in iteration 141 : 0.32547305849256425
Loss in iteration 142 : 0.32544992534887585
Loss in iteration 143 : 0.32542699245590334
Loss in iteration 144 : 0.32540425004973766
Loss in iteration 145 : 0.32538168931718775
Loss in iteration 146 : 0.3253593057505641
Loss in iteration 147 : 0.32533709982201997
Loss in iteration 148 : 0.3253150751480652
Loss in iteration 149 : 0.3252932354917713
Loss in iteration 150 : 0.32527158226195924
Loss in iteration 151 : 0.32525011362327205
Loss in iteration 152 : 0.32522882534807523
Loss in iteration 153 : 0.32520771267512
Loss in iteration 154 : 0.3251867720881493
Loss in iteration 155 : 0.3251660021668109
Loss in iteration 156 : 0.3251454032684385
Loss in iteration 157 : 0.32512497641132837
Loss in iteration 158 : 0.3251047220496912
Loss in iteration 159 : 0.32508463935568516
Loss in iteration 160 : 0.3250647262635343
Loss in iteration 161 : 0.3250449801126039
Loss in iteration 162 : 0.32502539846463574
Loss in iteration 163 : 0.3250059796640189
Loss in iteration 164 : 0.32498672291506797
Loss in iteration 165 : 0.324967627928015
Loss in iteration 166 : 0.3249486943867137
Loss in iteration 167 : 0.3249299215316139
Loss in iteration 168 : 0.3249113080409465
Loss in iteration 169 : 0.3248928522131041
Loss in iteration 170 : 0.32487455230488316
Loss in iteration 171 : 0.3248564068304995
Loss in iteration 172 : 0.32483841468160063
Loss in iteration 173 : 0.3248205750423742
Loss in iteration 174 : 0.32480288717954037
Loss in iteration 175 : 0.3247853502339675
Loss in iteration 176 : 0.32476796311642114
Loss in iteration 177 : 0.32475072453959714
Loss in iteration 178 : 0.3247336331453244
Loss in iteration 179 : 0.32471668764632783
Loss in iteration 180 : 0.32469988690967044
Loss in iteration 181 : 0.3246832299513077
Loss in iteration 182 : 0.32466671586096707
Loss in iteration 183 : 0.32465034370754065
Loss in iteration 184 : 0.32463411247562557
Loss in iteration 185 : 0.3246180210590029
Loss in iteration 186 : 0.3246020683037493
Loss in iteration 187 : 0.3245862530703158
Loss in iteration 188 : 0.3245705742800854
Loss in iteration 189 : 0.3245550309260678
Loss in iteration 190 : 0.3245396220490136
Loss in iteration 191 : 0.32452434669730124
Loss in iteration 192 : 0.3245092038936595
Loss in iteration 193 : 0.3244941926241105
Loss in iteration 194 : 0.3244793118505362
Loss in iteration 195 : 0.32446456053617073
Loss in iteration 196 : 0.3244499376688048
Loss in iteration 197 : 0.3244354422704435
Loss in iteration 198 : 0.3244210733910699
Loss in iteration 199 : 0.3244068300925749
Loss in iteration 200 : 0.32439271143277837
Testing accuracy  of updater 7 on alg 0 with rate 0.02 = 0.78725, training accuracy 0.8429912593072192, time elapsed: 2230 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5956662228278868
Loss in iteration 3 : 0.5184441929590073
Loss in iteration 4 : 0.5019715629803556
Loss in iteration 5 : 0.5141553241058939
Loss in iteration 6 : 0.5252826427644856
Loss in iteration 7 : 0.5218629226830122
Loss in iteration 8 : 0.5013955855557668
Loss in iteration 9 : 0.468106507534794
Loss in iteration 10 : 0.4305428897226751
Loss in iteration 11 : 0.39905626176135833
Loss in iteration 12 : 0.38169350217443987
Loss in iteration 13 : 0.37963662205842913
Loss in iteration 14 : 0.38621030103810366
Loss in iteration 15 : 0.39152465742291126
Loss in iteration 16 : 0.38902639946207485
Loss in iteration 17 : 0.3785351713523202
Loss in iteration 18 : 0.3644656988319368
Loss in iteration 19 : 0.3519851535815986
Loss in iteration 20 : 0.3441959217588081
Loss in iteration 21 : 0.34145606751921764
Loss in iteration 22 : 0.3422141976621363
Loss in iteration 23 : 0.3442697373435121
Loss in iteration 24 : 0.3457518506685675
Loss in iteration 25 : 0.345609740294673
Loss in iteration 26 : 0.34368439776277476
Loss in iteration 27 : 0.34050795560130903
Loss in iteration 28 : 0.3369673638097514
Loss in iteration 29 : 0.333946028344361
Loss in iteration 30 : 0.33203717909855124
Loss in iteration 31 : 0.33139671430806694
Loss in iteration 32 : 0.33176169277002976
Loss in iteration 33 : 0.3326076870744093
Loss in iteration 34 : 0.33337218248469236
Loss in iteration 35 : 0.3336538062231945
Loss in iteration 36 : 0.33331780379024395
Loss in iteration 37 : 0.33248565258765417
Loss in iteration 38 : 0.33143605847319896
Loss in iteration 39 : 0.33047306942603744
Loss in iteration 40 : 0.3298164118040415
Loss in iteration 41 : 0.3295468826005055
Loss in iteration 42 : 0.3296110553398721
Loss in iteration 43 : 0.32986803238146983
Loss in iteration 44 : 0.3301522603485895
Loss in iteration 45 : 0.33032900529155435
Loss in iteration 46 : 0.3303279197772728
Loss in iteration 47 : 0.3301502798580021
Loss in iteration 48 : 0.3298537753406168
Loss in iteration 49 : 0.3295238576822548
Loss in iteration 50 : 0.32924239200558925
Loss in iteration 51 : 0.3290631187542226
Loss in iteration 52 : 0.3289999582991938
Loss in iteration 53 : 0.3290295610334486
Loss in iteration 54 : 0.32910503322558643
Loss in iteration 55 : 0.32917474646832673
Loss in iteration 56 : 0.3291994093936603
Loss in iteration 57 : 0.3291621852572637
Loss in iteration 58 : 0.32906977319434105
Loss in iteration 59 : 0.3289457302817885
Loss in iteration 60 : 0.3288196634706847
Loss in iteration 61 : 0.32871659859276625
Loss in iteration 62 : 0.328649917454854
Loss in iteration 63 : 0.32861939438421633
Loss in iteration 64 : 0.32861390467089857
Loss in iteration 65 : 0.32861699319827503
Loss in iteration 66 : 0.3286129901075989
Loss in iteration 67 : 0.3285916834734775
Loss in iteration 68 : 0.32855041036992216
Loss in iteration 69 : 0.32849343396939734
Loss in iteration 70 : 0.3284293124305669
Loss in iteration 71 : 0.32836743336732555
Loss in iteration 72 : 0.32831492501918963
Loss in iteration 73 : 0.32827482767852884
Loss in iteration 74 : 0.3282458712140684
Loss in iteration 75 : 0.3282236485055002
Loss in iteration 76 : 0.32820257226225313
Loss in iteration 77 : 0.32817785732448185
Loss in iteration 78 : 0.32814689086075394
Loss in iteration 79 : 0.32810965972242956
Loss in iteration 80 : 0.3280682672855253
Loss in iteration 81 : 0.32802586164539355
Loss in iteration 82 : 0.32798542923348534
Loss in iteration 83 : 0.32794886708056414
Loss in iteration 84 : 0.3279165789372405
Loss in iteration 85 : 0.3278876251043678
Loss in iteration 86 : 0.3278602727029521
Loss in iteration 87 : 0.3278326948804503
Loss in iteration 88 : 0.3278035684025788
Loss in iteration 89 : 0.3277723990455029
Loss in iteration 90 : 0.3277395224762118
Loss in iteration 91 : 0.3277058405991977
Loss in iteration 92 : 0.3276724259151349
Loss in iteration 93 : 0.3276401440823838
Loss in iteration 94 : 0.3276094119809231
Loss in iteration 95 : 0.3275801443391349
Loss in iteration 96 : 0.3275518720878339
Loss in iteration 97 : 0.3275239632833202
Loss in iteration 98 : 0.32749585653828495
Loss in iteration 99 : 0.3274672288749311
Loss in iteration 100 : 0.32743805525901015
Loss in iteration 101 : 0.32740856015772196
Loss in iteration 102 : 0.32737909673534965
Loss in iteration 103 : 0.3273500064940255
Loss in iteration 104 : 0.3273215088602535
Loss in iteration 105 : 0.32729365125406323
Loss in iteration 106 : 0.32726632470895706
Loss in iteration 107 : 0.32723932772818254
Loss in iteration 108 : 0.3272124485257506
Loss in iteration 109 : 0.3271855353547059
Loss in iteration 110 : 0.3271585341442621
Loss in iteration 111 : 0.32713148719069196
Loss in iteration 112 : 0.3271045005251133
Loss in iteration 113 : 0.32707769642425155
Loss in iteration 114 : 0.3270511693817455
Loss in iteration 115 : 0.32702495938009407
Loss in iteration 116 : 0.3269990480965802
Loss in iteration 117 : 0.3269733750959924
Loss in iteration 118 : 0.3269478649973104
Loss in iteration 119 : 0.32692245460631425
Loss in iteration 120 : 0.3268971110189502
Loss in iteration 121 : 0.3268718363385257
Loss in iteration 122 : 0.3268466598989505
Loss in iteration 123 : 0.326821622884555
Loss in iteration 124 : 0.32679676187443174
Loss in iteration 125 : 0.3267720969971025
Loss in iteration 126 : 0.3267476277943454
Loss in iteration 127 : 0.3267233367465583
Loss in iteration 128 : 0.3266991978614087
Loss in iteration 129 : 0.3266751865155001
Loss in iteration 130 : 0.326651287023698
Loss in iteration 131 : 0.32662749582170725
Loss in iteration 132 : 0.3266038200059812
Loss in iteration 133 : 0.3265802725778948
Loss in iteration 134 : 0.3265568665969749
Loss in iteration 135 : 0.3265336104094778
Loss in iteration 136 : 0.3265105053595784
Loss in iteration 137 : 0.3264875462965525
Loss in iteration 138 : 0.3264647241911103
Loss in iteration 139 : 0.3264420295877655
Loss in iteration 140 : 0.3264194555707838
Loss in iteration 141 : 0.32639699932849087
Loss in iteration 142 : 0.3263746620437908
Loss in iteration 143 : 0.32635244745670533
Loss in iteration 144 : 0.32633035983223035
Loss in iteration 145 : 0.32630840213402335
Loss in iteration 146 : 0.3262865749869744
Loss in iteration 147 : 0.3262648766356424
Loss in iteration 148 : 0.32624330372802507
Loss in iteration 149 : 0.32622185250415364
Loss in iteration 150 : 0.32620051990841903
Loss in iteration 151 : 0.32617930425931924
Loss in iteration 152 : 0.3261582053291688
Loss in iteration 153 : 0.32613722391536476
Loss in iteration 154 : 0.3261163611439494
Loss in iteration 155 : 0.3260956177936516
Loss in iteration 156 : 0.3260749938685579
Loss in iteration 157 : 0.3260544885201781
Loss in iteration 158 : 0.32603410028098107
Loss in iteration 159 : 0.32601382747123164
Loss in iteration 160 : 0.3259936686066476
Loss in iteration 161 : 0.3259736226658524
Loss in iteration 162 : 0.3259536891509843
Loss in iteration 163 : 0.32593386795861307
Loss in iteration 164 : 0.325914159140601
Loss in iteration 165 : 0.325894562657992
Loss in iteration 166 : 0.32587507821447115
Loss in iteration 167 : 0.32585570521233
Loss in iteration 168 : 0.32583644282346147
Loss in iteration 169 : 0.32581729012928645
Loss in iteration 170 : 0.32579824626804094
Loss in iteration 171 : 0.3257793105366609
Loss in iteration 172 : 0.32576048242005645
Loss in iteration 173 : 0.32574176155091034
Loss in iteration 174 : 0.32572314762684373
Loss in iteration 175 : 0.32570464032173035
Loss in iteration 176 : 0.3256862392232253
Loss in iteration 177 : 0.325667943813489
Loss in iteration 178 : 0.32564975349182873
Loss in iteration 179 : 0.3256316676234884
Loss in iteration 180 : 0.32561368559258225
Loss in iteration 181 : 0.32559580683972356
Loss in iteration 182 : 0.32557803087390225
Loss in iteration 183 : 0.32556035725914956
Loss in iteration 184 : 0.3255427855853193
Loss in iteration 185 : 0.32552531543620017
Loss in iteration 186 : 0.325507946366688
Loss in iteration 187 : 0.32549067789537406
Loss in iteration 188 : 0.32547350951228854
Loss in iteration 189 : 0.3254564406962173
Loss in iteration 190 : 0.3254394709336483
Loss in iteration 191 : 0.32542259973229953
Loss in iteration 192 : 0.32540582662539197
Loss in iteration 193 : 0.3253891511668501
Loss in iteration 194 : 0.32537257292079164
Loss in iteration 195 : 0.32535609145008926
Loss in iteration 196 : 0.3253397063082492
Loss in iteration 197 : 0.32532341703685713
Loss in iteration 198 : 0.32530722316847965
Loss in iteration 199 : 0.32529112423295053
Loss in iteration 200 : 0.32527511976414547
Testing accuracy  of updater 7 on alg 0 with rate 0.014 = 0.7855, training accuracy 0.8400776950469407, time elapsed: 2212 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6322867784213156
Loss in iteration 3 : 0.55964495903368
Loss in iteration 4 : 0.51471797433318
Loss in iteration 5 : 0.5010010641281456
Loss in iteration 6 : 0.5045144738260927
Loss in iteration 7 : 0.5119870605367175
Loss in iteration 8 : 0.5153861469894309
Loss in iteration 9 : 0.5111318809539391
Loss in iteration 10 : 0.49867508159239077
Loss in iteration 11 : 0.4794773337903896
Loss in iteration 12 : 0.456322318917444
Loss in iteration 13 : 0.43272405880493586
Loss in iteration 14 : 0.41222291571260583
Loss in iteration 15 : 0.39751739754311943
Loss in iteration 16 : 0.3896422598546926
Loss in iteration 17 : 0.3876148433799716
Loss in iteration 18 : 0.3888680548168266
Loss in iteration 19 : 0.3903413958882392
Loss in iteration 20 : 0.3896736089854077
Loss in iteration 21 : 0.3859121122199244
Loss in iteration 22 : 0.37950926742509095
Loss in iteration 23 : 0.3717863631908041
Loss in iteration 24 : 0.36424015158440975
Loss in iteration 25 : 0.3580090312180006
Loss in iteration 26 : 0.3536342075516362
Loss in iteration 27 : 0.3510857119883827
Loss in iteration 28 : 0.34994216521053195
Loss in iteration 29 : 0.3496113431255007
Loss in iteration 30 : 0.3495169704676238
Loss in iteration 31 : 0.34922000084439314
Loss in iteration 32 : 0.3484724954261003
Loss in iteration 33 : 0.34721703830106837
Loss in iteration 34 : 0.345549179745227
Loss in iteration 35 : 0.3436599213355237
Loss in iteration 36 : 0.3417730145489925
Loss in iteration 37 : 0.34008908649111125
Loss in iteration 38 : 0.33874541217856263
Loss in iteration 39 : 0.33779632019281175
Loss in iteration 40 : 0.33721485198144907
Loss in iteration 41 : 0.3369119954792796
Loss in iteration 42 : 0.33676650329017654
Loss in iteration 43 : 0.33665682814439385
Loss in iteration 44 : 0.3364874212539841
Loss in iteration 45 : 0.3362042136694218
Loss in iteration 46 : 0.33579763356892495
Loss in iteration 47 : 0.3352948849686781
Loss in iteration 48 : 0.3347455091713295
Loss in iteration 49 : 0.334205038342498
Loss in iteration 50 : 0.33372093786259427
Loss in iteration 51 : 0.33332351339522975
Loss in iteration 52 : 0.33302265857892327
Loss in iteration 53 : 0.33280977503235415
Loss in iteration 54 : 0.3326632184197102
Loss in iteration 55 : 0.3325552808808973
Loss in iteration 56 : 0.33245890044672327
Loss in iteration 57 : 0.3323527952664682
Loss in iteration 58 : 0.3322243521179103
Loss in iteration 59 : 0.33207019512603536
Loss in iteration 60 : 0.3318948217342818
Loss in iteration 61 : 0.33170797377232086
Loss in iteration 62 : 0.33152150901020727
Loss in iteration 63 : 0.3313464782237946
Loss in iteration 64 : 0.3311909370744109
Loss in iteration 65 : 0.33105878231927544
Loss in iteration 66 : 0.3309496507809134
Loss in iteration 67 : 0.33085970427213085
Loss in iteration 68 : 0.3307829791307855
Loss in iteration 69 : 0.33071292253694284
Loss in iteration 70 : 0.3306437672770837
Loss in iteration 71 : 0.3305714926518388
Loss in iteration 72 : 0.33049425096438173
Loss in iteration 73 : 0.3304122724800274
Loss in iteration 74 : 0.33032736795131895
Loss in iteration 75 : 0.3302422087796073
Loss in iteration 76 : 0.3301595759987674
Loss in iteration 77 : 0.3300817378844422
Loss in iteration 78 : 0.33001005742765704
Loss in iteration 79 : 0.3299448632634769
Loss in iteration 80 : 0.32988555708910977
Loss in iteration 81 : 0.3298308882283455
Loss in iteration 82 : 0.3297793065718654
Loss in iteration 83 : 0.3297293075948621
Loss in iteration 84 : 0.32967970227864757
Loss in iteration 85 : 0.3296297731775056
Loss in iteration 86 : 0.32957930805113034
Loss in iteration 87 : 0.32952852822017775
Loss in iteration 88 : 0.3294779460673782
Loss in iteration 89 : 0.32942819330261325
Loss in iteration 90 : 0.32937985934550845
Loss in iteration 91 : 0.3293333697098796
Loss in iteration 92 : 0.3292889207777823
Loss in iteration 93 : 0.32924647317623634
Loss in iteration 94 : 0.32920579398951244
Loss in iteration 95 : 0.32916653019420655
Loss in iteration 96 : 0.32912829278809247
Loss in iteration 97 : 0.32909073278418716
Loss in iteration 98 : 0.32905359538454654
Loss in iteration 99 : 0.3290167455783067
Loss in iteration 100 : 0.32898016540456865
Loss in iteration 101 : 0.3289439287593047
Loss in iteration 102 : 0.32890816301950143
Loss in iteration 103 : 0.32887300764250604
Loss in iteration 104 : 0.32883857857091203
Loss in iteration 105 : 0.32880494442465696
Loss in iteration 106 : 0.3287721169597438
Loss in iteration 107 : 0.328740054948583
Loss in iteration 108 : 0.3287086781229647
Loss in iteration 109 : 0.3286778864622173
Loss in iteration 110 : 0.3286475799520939
Loss in iteration 111 : 0.3286176747815901
Loss in iteration 112 : 0.3285881134245559
Loss in iteration 113 : 0.3285588677543881
Loss in iteration 114 : 0.328529935884752
Loss in iteration 115 : 0.32850133454255637
Loss in iteration 116 : 0.32847308932424457
Loss in iteration 117 : 0.3284452251631976
Loss in iteration 118 : 0.3284177588564045
Loss in iteration 119 : 0.32839069474277194
Loss in iteration 120 : 0.3283640237944788
Loss in iteration 121 : 0.3283377256550832
Loss in iteration 122 : 0.3283117726587546
Loss in iteration 123 : 0.3282861346509988
Loss in iteration 124 : 0.3282607834922293
Loss in iteration 125 : 0.3282356963984987
Loss in iteration 126 : 0.32821085766480185
Loss in iteration 127 : 0.3281862587254098
Loss in iteration 128 : 0.32816189684748237
Loss in iteration 129 : 0.32813777297268903
Loss in iteration 130 : 0.3281138892959591
Loss in iteration 131 : 0.3280902471127453
Loss in iteration 132 : 0.32806684531275476
Loss in iteration 133 : 0.328043679697572
Loss in iteration 134 : 0.32802074310123835
Loss in iteration 135 : 0.32799802613587803
Loss in iteration 136 : 0.32797551829271676
Loss in iteration 137 : 0.3279532091083614
Loss in iteration 138 : 0.3279310891477335
Loss in iteration 139 : 0.32790915063866244
Loss in iteration 140 : 0.32788738769410064
Loss in iteration 141 : 0.32786579615284817
Loss in iteration 142 : 0.3278443731406277
Loss in iteration 143 : 0.32782311649055906
Loss in iteration 144 : 0.32780202416448095
Loss in iteration 145 : 0.32778109379000847
Loss in iteration 146 : 0.32776032238358627
Loss in iteration 147 : 0.32773970627923626
Loss in iteration 148 : 0.3277192412376674
Loss in iteration 149 : 0.3276989226787798
Loss in iteration 150 : 0.32767874596637
Loss in iteration 151 : 0.32765870667661773
Loss in iteration 152 : 0.32763880079800706
Loss in iteration 153 : 0.3276190248340179
Loss in iteration 154 : 0.3275993758049876
Loss in iteration 155 : 0.3275798511666876
Loss in iteration 156 : 0.3275604486768193
Loss in iteration 157 : 0.3275411662454157
Loss in iteration 158 : 0.32752200180174024
Loss in iteration 159 : 0.3275029532008743
Loss in iteration 160 : 0.3274840181808487
Loss in iteration 161 : 0.3274651943688903
Loss in iteration 162 : 0.3274464793256793
Loss in iteration 163 : 0.3274278706108543
Loss in iteration 164 : 0.3274093658518513
Loss in iteration 165 : 0.3273909628008182
Loss in iteration 166 : 0.32737265936966603
Loss in iteration 167 : 0.32735445363964355
Loss in iteration 168 : 0.32733634384770655
Loss in iteration 169 : 0.32731832835632485
Loss in iteration 170 : 0.3273004056154882
Loss in iteration 171 : 0.32728257412572037
Loss in iteration 172 : 0.3272648324090314
Loss in iteration 173 : 0.32724717899187095
Loss in iteration 174 : 0.32722961240094994
Loss in iteration 175 : 0.3272121311699876
Loss in iteration 176 : 0.32719473385360653
Loss in iteration 177 : 0.3271774190438166
Loss in iteration 178 : 0.32716018538487857
Loss in iteration 179 : 0.3271430315834516
Loss in iteration 180 : 0.32712595641249365
Loss in iteration 181 : 0.32710895870897133
Loss in iteration 182 : 0.3270920373666953
Loss in iteration 183 : 0.32707519132640406
Loss in iteration 184 : 0.32705841956536286
Loss in iteration 185 : 0.3270417210884684
Loss in iteration 186 : 0.3270250949221615
Loss in iteration 187 : 0.3270085401116401
Loss in iteration 188 : 0.32699205572110324
Loss in iteration 189 : 0.3269756408361805
Loss in iteration 190 : 0.32695929456742456
Loss in iteration 191 : 0.32694301605372167
Loss in iteration 192 : 0.32692680446472877
Loss in iteration 193 : 0.3269106590018265
Loss in iteration 194 : 0.3268945788974485
Loss in iteration 195 : 0.3268785634130783
Loss in iteration 196 : 0.32686261183638915
Loss in iteration 197 : 0.3268467234781066
Loss in iteration 198 : 0.326830897669151
Loss in iteration 199 : 0.32681513375844534
Loss in iteration 200 : 0.32679943111156406
Testing accuracy  of updater 7 on alg 0 with rate 0.008 = 0.78525, training accuracy 0.8381353188734219, time elapsed: 2743 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.676563395880249
Testing accuracy  of updater 7 on alg 0 with rate 0.0019999999999999983 = 0.5, training accuracy 0.6474587245063127, time elapsed: 37 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 11.083637371780485
Loss in iteration 3 : 7.964006013102107
Loss in iteration 4 : 3.7872205227208866
Loss in iteration 5 : 0.8714953759107757
Loss in iteration 6 : 1.8328701924770905
Loss in iteration 7 : 1.4230514889708459
Loss in iteration 8 : 1.2049032646753783
Loss in iteration 9 : 1.1926202774223127
Loss in iteration 10 : 1.2358534356780129
Loss in iteration 11 : 1.2655658664003082
Loss in iteration 12 : 1.272854700347998
Loss in iteration 13 : 1.2654978803398156
Loss in iteration 14 : 1.2506109436571407
Loss in iteration 15 : 1.229256620363588
Loss in iteration 16 : 1.1994115318361596
Loss in iteration 17 : 1.160561797617523
Loss in iteration 18 : 1.114109991034251
Loss in iteration 19 : 1.0613069815249947
Loss in iteration 20 : 1.0026403399940425
Loss in iteration 21 : 0.938518686676679
Loss in iteration 22 : 0.8696387919992794
Loss in iteration 23 : 0.7968051472419506
Loss in iteration 24 : 0.7208623733088746
Loss in iteration 25 : 0.6428995281840724
Loss in iteration 26 : 0.5644825590813042
Loss in iteration 27 : 0.4880626312868437
Loss in iteration 28 : 0.41793674535624353
Loss in iteration 29 : 0.3624059733818162
Loss in iteration 30 : 0.33834776201433664
Loss in iteration 31 : 0.37089982189940673
Loss in iteration 32 : 0.4198153150598352
Loss in iteration 33 : 0.8568358336079207
Loss in iteration 34 : 5.990033611574791
Loss in iteration 35 : 5.537295470077332
Loss in iteration 36 : 3.633712867678002
Loss in iteration 37 : 1.061333898490143
Loss in iteration 38 : 0.9741929222173715
Loss in iteration 39 : 1.2332070032686206
Loss in iteration 40 : 0.9579964881243672
Loss in iteration 41 : 0.9020849795118505
Loss in iteration 42 : 0.9321688589610931
Loss in iteration 43 : 0.9557922263952077
Loss in iteration 44 : 0.9595008367171164
Loss in iteration 45 : 0.9517863916964705
Loss in iteration 46 : 0.9383902770434376
Loss in iteration 47 : 0.9182441888440757
Loss in iteration 48 : 0.8900091395293899
Loss in iteration 49 : 0.8547443070321747
Loss in iteration 50 : 0.8137251238140718
Loss in iteration 51 : 0.7676120945229749
Loss in iteration 52 : 0.7170770617379044
Loss in iteration 53 : 0.6630340559493911
Loss in iteration 54 : 0.606539449661043
Loss in iteration 55 : 0.5488930355799996
Loss in iteration 56 : 0.4918869191928333
Loss in iteration 57 : 0.4381536690755287
Loss in iteration 58 : 0.3917818730702651
Loss in iteration 59 : 0.35925647218879725
Loss in iteration 60 : 0.34980136043935517
Loss in iteration 61 : 0.3688578234374094
Loss in iteration 62 : 0.39475663807535866
Loss in iteration 63 : 0.3899965438547251
Loss in iteration 64 : 0.3703502549914665
Loss in iteration 65 : 0.3752057106386074
Loss in iteration 66 : 0.7225580579234056
Loss in iteration 67 : 2.4268877117321033
Loss in iteration 68 : 6.065418999504812
Loss in iteration 69 : 6.223756681731854
Loss in iteration 70 : 4.927778535327322
Loss in iteration 71 : 2.586451408797185
Loss in iteration 72 : 1.216942222887619
Loss in iteration 73 : 1.293717851591032
Loss in iteration 74 : 1.740780885468364
Loss in iteration 75 : 1.725040143104477
Loss in iteration 76 : 1.5364601663444262
Loss in iteration 77 : 1.4413402337405252
Loss in iteration 78 : 1.4536673322776246
Loss in iteration 79 : 1.4905612989837447
Loss in iteration 80 : 1.5078265129489312
Loss in iteration 81 : 1.4985122519497158
Loss in iteration 82 : 1.4725880610113555
Loss in iteration 83 : 1.4420710772048648
Loss in iteration 84 : 1.4118566716413559
Loss in iteration 85 : 1.3789790762996779
Loss in iteration 86 : 1.3386551365616213
Loss in iteration 87 : 1.2904259657497537
Loss in iteration 88 : 1.2370045279209028
Loss in iteration 89 : 1.180237043853475
Loss in iteration 90 : 1.1202261859863052
Loss in iteration 91 : 1.0567249288429188
Loss in iteration 92 : 0.990115959866944
Loss in iteration 93 : 0.9211791219774585
Loss in iteration 94 : 0.8505559108333945
Loss in iteration 95 : 0.7787665963587733
Loss in iteration 96 : 0.7065195064554559
Loss in iteration 97 : 0.6348180771988466
Loss in iteration 98 : 0.565023454864788
Loss in iteration 99 : 0.4991477464450943
Loss in iteration 100 : 0.4403641903794344
Loss in iteration 101 : 0.39381686765092483
Loss in iteration 102 : 0.3676561624339654
Loss in iteration 103 : 0.37157917557286424
Loss in iteration 104 : 0.4033821584908192
Loss in iteration 105 : 0.4272073026936243
Loss in iteration 106 : 0.47238160202884344
Loss in iteration 107 : 2.385633099867767
Loss in iteration 108 : 8.052472492913163
Loss in iteration 109 : 8.98501440484149
Loss in iteration 110 : 8.397359137439857
Loss in iteration 111 : 6.469723981168784
Loss in iteration 112 : 3.526191772050527
Loss in iteration 113 : 1.6007962678114078
Loss in iteration 114 : 1.4850994931912285
Loss in iteration 115 : 2.1931576074816594
Loss in iteration 116 : 2.4317350450826
Loss in iteration 117 : 2.1911510012331474
Loss in iteration 118 : 1.904188748241034
Loss in iteration 119 : 1.7793425078220886
Loss in iteration 120 : 1.7954477716875787
Loss in iteration 121 : 1.8496614135938567
Loss in iteration 122 : 1.8829437515522092
Loss in iteration 123 : 1.8798312524328906
Loss in iteration 124 : 1.848174878840275
Loss in iteration 125 : 1.804904523801982
Loss in iteration 126 : 1.76451363526794
Loss in iteration 127 : 1.7307442470191627
Loss in iteration 128 : 1.6977441057228388
Loss in iteration 129 : 1.657424498715493
Loss in iteration 130 : 1.607445551950885
Loss in iteration 131 : 1.5513806230531875
Loss in iteration 132 : 1.492940090930668
Loss in iteration 133 : 1.4330432329848357
Loss in iteration 134 : 1.3708697405194512
Loss in iteration 135 : 1.3056954798659417
Loss in iteration 136 : 1.2377387931128954
Loss in iteration 137 : 1.1678314493420465
Loss in iteration 138 : 1.0966417532156203
Loss in iteration 139 : 1.0244043292914728
Loss in iteration 140 : 0.9512934503912812
Loss in iteration 141 : 0.8777253698700408
Loss in iteration 142 : 0.8042752237371987
Loss in iteration 143 : 0.7315743470260389
Loss in iteration 144 : 0.6604226647694792
Loss in iteration 145 : 0.5919814452899202
Loss in iteration 146 : 0.5279563777176971
Loss in iteration 147 : 0.47089220793842007
Loss in iteration 148 : 0.4246828431682892
Loss in iteration 149 : 0.3950411631338051
Loss in iteration 150 : 0.3883327534348389
Loss in iteration 151 : 0.40627644705527166
Loss in iteration 152 : 0.4345323374993127
Loss in iteration 153 : 0.4412932646078472
Loss in iteration 154 : 0.42526967090019063
Loss in iteration 155 : 0.43205837319840906
Loss in iteration 156 : 0.8100275605743353
Loss in iteration 157 : 3.38725955531723
Loss in iteration 158 : 2.010373295421544
Loss in iteration 159 : 0.4764009922886283
Loss in iteration 160 : 0.8189866144879281
Loss in iteration 161 : 0.5622320833185159
Loss in iteration 162 : 0.5896992389470748
Loss in iteration 163 : 0.6079987704810373
Loss in iteration 164 : 0.616908987543977
Loss in iteration 165 : 0.6172578013410395
Loss in iteration 166 : 0.6098874420335293
Loss in iteration 167 : 0.5956532989206343
Loss in iteration 168 : 0.5754537567637369
Loss in iteration 169 : 0.5502338274800173
Loss in iteration 170 : 0.521018483666054
Loss in iteration 171 : 0.4889657158198389
Loss in iteration 172 : 0.4554447615801078
Loss in iteration 173 : 0.42215120668118006
Loss in iteration 174 : 0.39126204766115846
Loss in iteration 175 : 0.3655980391029239
Loss in iteration 176 : 0.3485972470415523
Loss in iteration 177 : 0.34341655812054006
Loss in iteration 178 : 0.34992951195777566
Loss in iteration 179 : 0.360521853493806
Loss in iteration 180 : 0.3636048555303264
Loss in iteration 181 : 0.35744607593574
Loss in iteration 182 : 0.3530290931489215
Loss in iteration 183 : 0.4381148955886018
Loss in iteration 184 : 1.4233154625108506
Loss in iteration 185 : 0.5531555918808376
Loss in iteration 186 : 0.9681054022489547
Loss in iteration 187 : 0.5584268281013848
Loss in iteration 188 : 0.5311845273292054
Loss in iteration 189 : 0.4565877431946642
Loss in iteration 190 : 0.4513128313661231
Loss in iteration 191 : 0.45345325324280655
Loss in iteration 192 : 0.44846461793125203
Loss in iteration 193 : 0.43797385704697617
Loss in iteration 194 : 0.4228217557550566
Loss in iteration 195 : 0.4043167760908497
Loss in iteration 196 : 0.38402900661955924
Loss in iteration 197 : 0.36387237833886105
Loss in iteration 198 : 0.3461633634478447
Loss in iteration 199 : 0.33352460638634923
Loss in iteration 200 : 0.32825186094255393
Testing accuracy  of updater 8 on alg 0 with rate 0.19999999999999998 = 0.783, training accuracy 0.8420200712204597, time elapsed: 2159 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 3.16863434526221
Loss in iteration 3 : 2.301567786253802
Loss in iteration 4 : 0.6906496011019474
Loss in iteration 5 : 0.6918800253451005
Loss in iteration 6 : 0.6125041569321861
Loss in iteration 7 : 0.5793782699515373
Loss in iteration 8 : 0.5979770932866281
Loss in iteration 9 : 0.6173585724299078
Loss in iteration 10 : 0.6276422551028706
Loss in iteration 11 : 0.6300704722479179
Loss in iteration 12 : 0.6262564598872397
Loss in iteration 13 : 0.6166868178607209
Loss in iteration 14 : 0.6018032459416996
Loss in iteration 15 : 0.5823144673013806
Loss in iteration 16 : 0.5589476454365486
Loss in iteration 17 : 0.5323947106353728
Loss in iteration 18 : 0.5033944521738405
Loss in iteration 19 : 0.47277528259232005
Loss in iteration 20 : 0.44149302538174817
Loss in iteration 21 : 0.410706838027967
Loss in iteration 22 : 0.3818832719382562
Loss in iteration 23 : 0.35691887764934294
Loss in iteration 24 : 0.3382226911920428
Loss in iteration 25 : 0.32850998789565744
Loss in iteration 26 : 0.329613649869337
Loss in iteration 27 : 0.33942710606996523
Loss in iteration 28 : 0.3494744317068199
Loss in iteration 29 : 0.35118858552259163
Loss in iteration 30 : 0.345270053452823
Loss in iteration 31 : 0.3375678341953772
Loss in iteration 32 : 0.33320047245306733
Loss in iteration 33 : 0.34254192403724953
Loss in iteration 34 : 0.407148484023054
Loss in iteration 35 : 0.74290484999217
Loss in iteration 36 : 0.5207854976132791
Loss in iteration 37 : 0.7063344419529201
Loss in iteration 38 : 0.3915322864493076
Loss in iteration 39 : 0.37251777060910624
Loss in iteration 40 : 0.3793240185802021
Loss in iteration 41 : 0.3795444688006404
Loss in iteration 42 : 0.3780271488149735
Loss in iteration 43 : 0.37351680076090626
Loss in iteration 44 : 0.36665535341008565
Loss in iteration 45 : 0.35822096130817815
Loss in iteration 46 : 0.34911172711962074
Loss in iteration 47 : 0.3403067642248318
Loss in iteration 48 : 0.33280638513237243
Loss in iteration 49 : 0.3274994104677361
Loss in iteration 50 : 0.32492936081538026
Loss in iteration 51 : 0.3249886672663527
Loss in iteration 52 : 0.3267486258558928
Loss in iteration 53 : 0.3287407914539984
Loss in iteration 54 : 0.32974983748943487
Loss in iteration 55 : 0.32948176740633006
Loss in iteration 56 : 0.3289541578701038
Loss in iteration 57 : 0.33303347178065357
Loss in iteration 58 : 0.3672467721905135
Loss in iteration 59 : 0.5935596998699486
Loss in iteration 60 : 0.7174013952484662
Loss in iteration 61 : 1.6430298948141884
Loss in iteration 62 : 0.5278058295874432
Loss in iteration 63 : 0.5296359446684912
Loss in iteration 64 : 0.42016043950035115
Loss in iteration 65 : 0.4278345648897276
Loss in iteration 66 : 0.4326513317393582
Loss in iteration 67 : 0.4329664432978767
Loss in iteration 68 : 0.42870755685151224
Loss in iteration 69 : 0.42027501746234924
Loss in iteration 70 : 0.4084862704218169
Loss in iteration 71 : 0.3942568770693364
Loss in iteration 72 : 0.3786336427924295
Loss in iteration 73 : 0.3628193920486573
Loss in iteration 74 : 0.34818866579828417
Loss in iteration 75 : 0.336250963944746
Loss in iteration 76 : 0.3284698518708858
Loss in iteration 77 : 0.325793946659378
Loss in iteration 78 : 0.32784881643410757
Loss in iteration 79 : 0.33231479312967965
Loss in iteration 80 : 0.3357756574849104
Loss in iteration 81 : 0.33613225177438893
Loss in iteration 82 : 0.33383520307562936
Loss in iteration 83 : 0.3305780427664147
Loss in iteration 84 : 0.32769375899758746
Loss in iteration 85 : 0.3257454899794681
Loss in iteration 86 : 0.3247820474272565
Loss in iteration 87 : 0.32489943174196045
Loss in iteration 88 : 0.32714460529848305
Loss in iteration 89 : 0.3358624487692661
Loss in iteration 90 : 0.37388146758360397
Loss in iteration 91 : 0.451192641728684
Loss in iteration 92 : 0.841110334361657
Loss in iteration 93 : 0.4623747830652579
Loss in iteration 94 : 0.5354052546188832
Loss in iteration 95 : 0.41708848373195456
Loss in iteration 96 : 0.4024590458610411
Loss in iteration 97 : 0.37922243029459235
Loss in iteration 98 : 0.3692564361882797
Loss in iteration 99 : 0.3625239253703937
Loss in iteration 100 : 0.35526837563981845
Loss in iteration 101 : 0.34781772127782207
Loss in iteration 102 : 0.3405564955736036
Loss in iteration 103 : 0.334575861488826
Loss in iteration 104 : 0.3323565767278579
Loss in iteration 105 : 0.33965882472258185
Loss in iteration 106 : 0.39893927500463233
Loss in iteration 107 : 0.6218343991071981
Loss in iteration 108 : 1.9593176738431581
Loss in iteration 109 : 0.6484671587581501
Loss in iteration 110 : 0.8279839146544958
Loss in iteration 111 : 0.8691076169782791
Loss in iteration 112 : 0.4171679131142648
Loss in iteration 113 : 0.4644408035122416
Loss in iteration 114 : 0.4424627835954838
Loss in iteration 115 : 0.4460176039340261
Loss in iteration 116 : 0.4438676455268543
Loss in iteration 117 : 0.4366858165944176
Loss in iteration 118 : 0.4253090804497739
Loss in iteration 119 : 0.41068074955294015
Loss in iteration 120 : 0.3938722949103952
Loss in iteration 121 : 0.37611810268811735
Loss in iteration 122 : 0.35885574306859525
Loss in iteration 123 : 0.3437415678622698
Loss in iteration 124 : 0.3325650395159126
Loss in iteration 125 : 0.32689807759621436
Loss in iteration 126 : 0.3272621004413457
Loss in iteration 127 : 0.33196633090494115
Loss in iteration 128 : 0.33701575046000787
Loss in iteration 129 : 0.33871811475451713
Loss in iteration 130 : 0.33662161959080467
Loss in iteration 131 : 0.3327640417164027
Loss in iteration 132 : 0.3291617137170093
Loss in iteration 133 : 0.326866584445334
Loss in iteration 134 : 0.32726807455066825
Loss in iteration 135 : 0.337138204992857
Loss in iteration 136 : 0.3815702712878586
Loss in iteration 137 : 0.6162574006813046
Loss in iteration 138 : 0.626724502764013
Loss in iteration 139 : 1.228785157164628
Loss in iteration 140 : 0.3735953188020722
Loss in iteration 141 : 0.44603853537235316
Loss in iteration 142 : 0.40973476242028695
Loss in iteration 143 : 0.4025338866167082
Loss in iteration 144 : 0.3998739509507459
Loss in iteration 145 : 0.3950641428549346
Loss in iteration 146 : 0.3867508695772751
Loss in iteration 147 : 0.3758888132807777
Loss in iteration 148 : 0.3635667439874749
Loss in iteration 149 : 0.35103923739864473
Loss in iteration 150 : 0.33969844296747603
Loss in iteration 151 : 0.3309709828738856
Loss in iteration 152 : 0.32603497486006394
Loss in iteration 153 : 0.32528543926797104
Loss in iteration 154 : 0.3277144720927559
Loss in iteration 155 : 0.33097169209438865
Loss in iteration 156 : 0.33273188048219154
Loss in iteration 157 : 0.3328564179334242
Loss in iteration 158 : 0.33870093613419006
Loss in iteration 159 : 0.4147747868017538
Loss in iteration 160 : 1.0461645618195083
Loss in iteration 161 : 0.5434146744252691
Loss in iteration 162 : 0.9288873880135984
Loss in iteration 163 : 0.420699587931821
Loss in iteration 164 : 0.391142520331371
Loss in iteration 165 : 0.40304581419624697
Loss in iteration 166 : 0.4023661397580885
Loss in iteration 167 : 0.40175100348505777
Loss in iteration 168 : 0.3968933547547064
Loss in iteration 169 : 0.38845421262744717
Loss in iteration 170 : 0.37733902697765165
Loss in iteration 171 : 0.36468738704658926
Loss in iteration 172 : 0.35180829954261594
Loss in iteration 173 : 0.340160734227166
Loss in iteration 174 : 0.33124282689892187
Loss in iteration 175 : 0.3262905011499081
Loss in iteration 176 : 0.3256853281706562
Loss in iteration 177 : 0.3282946108622769
Loss in iteration 178 : 0.33155244796154654
Loss in iteration 179 : 0.33318505567868567
Loss in iteration 180 : 0.33413899998763846
Loss in iteration 181 : 0.35437628694673085
Loss in iteration 182 : 0.5787078206815053
Loss in iteration 183 : 1.14904026004723
Loss in iteration 184 : 3.5605290436864574
Loss in iteration 185 : 2.7375091788552584
Loss in iteration 186 : 1.0024309570339962
Loss in iteration 187 : 0.6401039957464986
Loss in iteration 188 : 0.8469629582284807
Loss in iteration 189 : 0.7099772245616985
Loss in iteration 190 : 0.6984895743167651
Loss in iteration 191 : 0.7152047137795478
Loss in iteration 192 : 0.7223072956106001
Loss in iteration 193 : 0.7184244337711335
Loss in iteration 194 : 0.7076481332864734
Loss in iteration 195 : 0.6910312058059468
Loss in iteration 196 : 0.6684642074701594
Loss in iteration 197 : 0.6406711393008854
Loss in iteration 198 : 0.6086521820794637
Loss in iteration 199 : 0.5732450239967265
Loss in iteration 200 : 0.5353332925393163
Testing accuracy  of updater 8 on alg 0 with rate 0.13999999999999999 = 0.7915, training accuracy 0.8420200712204597, time elapsed: 2488 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 2.146417483296328
Loss in iteration 3 : 1.6300299830900213
Loss in iteration 4 : 0.6021752380258728
Loss in iteration 5 : 0.4884868549544973
Loss in iteration 6 : 0.47090699087129756
Loss in iteration 7 : 0.4295705582226966
Loss in iteration 8 : 0.4375659605143966
Loss in iteration 9 : 0.44984107903135107
Loss in iteration 10 : 0.4572683942933583
Loss in iteration 11 : 0.460154220051685
Loss in iteration 12 : 0.4596193128886129
Loss in iteration 13 : 0.45597360197592407
Loss in iteration 14 : 0.4494175956492076
Loss in iteration 15 : 0.4403754821477285
Loss in iteration 16 : 0.4293333610030662
Loss in iteration 17 : 0.41675480463052894
Loss in iteration 18 : 0.403122172902136
Loss in iteration 19 : 0.3889616814164455
Loss in iteration 20 : 0.3748415508480488
Loss in iteration 21 : 0.36137549089535537
Loss in iteration 22 : 0.3492222973422444
Loss in iteration 23 : 0.33906164050445914
Loss in iteration 24 : 0.33152967406468253
Loss in iteration 25 : 0.3270906548942929
Loss in iteration 26 : 0.3258328377608831
Loss in iteration 27 : 0.3272410615646975
Loss in iteration 28 : 0.33012262307886137
Loss in iteration 29 : 0.33291954074605584
Loss in iteration 30 : 0.3343783000790448
Loss in iteration 31 : 0.33409363435305867
Loss in iteration 32 : 0.33247044819158755
Loss in iteration 33 : 0.330249648780585
Loss in iteration 34 : 0.3280706456659736
Loss in iteration 35 : 0.32630028951989776
Loss in iteration 36 : 0.3250587985302117
Loss in iteration 37 : 0.32431060840933834
Loss in iteration 38 : 0.3239466699872739
Loss in iteration 39 : 0.3238393712791128
Loss in iteration 40 : 0.3238727646897274
Loss in iteration 41 : 0.323956229039369
Loss in iteration 42 : 0.3240280457492072
Loss in iteration 43 : 0.32405352672113474
Loss in iteration 44 : 0.3240203675781481
Loss in iteration 45 : 0.3239328303135968
Loss in iteration 46 : 0.32380566787601267
Loss in iteration 47 : 0.3236584157610527
Loss in iteration 48 : 0.3235105281211655
Loss in iteration 49 : 0.3233777490473631
Loss in iteration 50 : 0.32326998391535083
Loss in iteration 51 : 0.32319074694937416
Loss in iteration 52 : 0.3231380135998117
Loss in iteration 53 : 0.3231060555482801
Loss in iteration 54 : 0.3230876630676531
Loss in iteration 55 : 0.32307614008480773
Loss in iteration 56 : 0.3230666143944366
Loss in iteration 57 : 0.32305648602904474
Loss in iteration 58 : 0.32304513062874574
Loss in iteration 59 : 0.32303317105110435
Loss in iteration 60 : 0.32302167398629605
Loss in iteration 61 : 0.32301153881435046
Loss in iteration 62 : 0.32300319383233717
Loss in iteration 63 : 0.32299657595775294
Loss in iteration 64 : 0.3229912895306542
Loss in iteration 65 : 0.32298682415223856
Loss in iteration 66 : 0.3229827401938924
Loss in iteration 67 : 0.32297877588861623
Loss in iteration 68 : 0.3229748704475042
Loss in iteration 69 : 0.3229711233279239
Testing accuracy  of updater 8 on alg 0 with rate 0.08 = 0.789, training accuracy 0.8442861767562317, time elapsed: 827 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.6462942408413641
Loss in iteration 3 : 0.5935185065339555
Loss in iteration 4 : 0.4582539661694344
Loss in iteration 5 : 0.3553496898424721
Loss in iteration 6 : 0.3443824851147055
Loss in iteration 7 : 0.35165476049947714
Loss in iteration 8 : 0.34349723170787055
Loss in iteration 9 : 0.3351615797530695
Loss in iteration 10 : 0.33277050960131865
Loss in iteration 11 : 0.3341799419394582
Loss in iteration 12 : 0.3365267395235933
Loss in iteration 13 : 0.33846168278051586
Loss in iteration 14 : 0.3398434106191036
Loss in iteration 15 : 0.34094090667212684
Loss in iteration 16 : 0.3419249150738952
Loss in iteration 17 : 0.3427646961229572
Loss in iteration 18 : 0.34334865320299157
Loss in iteration 19 : 0.34361083993045133
Loss in iteration 20 : 0.34356365187375415
Loss in iteration 21 : 0.343260828135688
Loss in iteration 22 : 0.34275407984612083
Loss in iteration 23 : 0.34207730867956393
Loss in iteration 24 : 0.34125391940706556
Loss in iteration 25 : 0.3403085443848417
Loss in iteration 26 : 0.3392714269920926
Loss in iteration 27 : 0.3381754034556615
Loss in iteration 28 : 0.3370511112046327
Loss in iteration 29 : 0.335924691597251
Loss in iteration 30 : 0.3348182776348429
Loss in iteration 31 : 0.33375124388913296
Loss in iteration 32 : 0.3327406137775175
Loss in iteration 33 : 0.3318005650095813
Loss in iteration 34 : 0.33094182371464675
Loss in iteration 35 : 0.33017149415856517
Loss in iteration 36 : 0.3294932759906531
Loss in iteration 37 : 0.32890777551015105
Loss in iteration 38 : 0.328412767158564
Loss in iteration 39 : 0.3280034656306813
Loss in iteration 40 : 0.32767290344847316
Loss in iteration 41 : 0.3274124221322203
Loss in iteration 42 : 0.3272122185809645
Loss in iteration 43 : 0.3270618918038364
Loss in iteration 44 : 0.3269509642248147
Loss in iteration 45 : 0.32686936187600896
Loss in iteration 46 : 0.32680783045784556
Loss in iteration 47 : 0.326758260964143
Loss in iteration 48 : 0.3267139072788151
Loss in iteration 49 : 0.32666949178141524
Loss in iteration 50 : 0.3266212061746598
Loss in iteration 51 : 0.3265666222366721
Loss in iteration 52 : 0.32650453236887694
Loss in iteration 53 : 0.3264347429462096
Loss in iteration 54 : 0.3263578440058199
Loss in iteration 55 : 0.3262749767076204
Loss in iteration 56 : 0.32618761602576524
Loss in iteration 57 : 0.326097381249959
Loss in iteration 58 : 0.3260058818613518
Loss in iteration 59 : 0.3259146017508054
Loss in iteration 60 : 0.3258248209508159
Loss in iteration 61 : 0.3257375712674254
Loss in iteration 62 : 0.3256536204556306
Loss in iteration 63 : 0.32557347877006454
Loss in iteration 64 : 0.32549742165376777
Loss in iteration 65 : 0.32542552279127734
Loss in iteration 66 : 0.32535769254920494
Loss in iteration 67 : 0.3252937177884007
Loss in iteration 68 : 0.32523330002488476
Loss in iteration 69 : 0.32517608984974367
Loss in iteration 70 : 0.325121716334606
Loss in iteration 71 : 0.3250698108217663
Loss in iteration 72 : 0.32502002502039085
Loss in iteration 73 : 0.3249720437107061
Loss in iteration 74 : 0.3249255926130735
Loss in iteration 75 : 0.3248804421289327
Loss in iteration 76 : 0.32483640772698735
Loss in iteration 77 : 0.32479334775084395
Loss in iteration 78 : 0.3247511593815355
Loss in iteration 79 : 0.32470977341504376
Loss in iteration 80 : 0.3246691484234734
Loss in iteration 81 : 0.32462926476861587
Loss in iteration 82 : 0.3245901188356331
Loss in iteration 83 : 0.32455171775796754
Loss in iteration 84 : 0.32451407481598676
Loss in iteration 85 : 0.3244772056138989
Loss in iteration 86 : 0.3244411250733959
Loss in iteration 87 : 0.32440584522892185
Loss in iteration 88 : 0.32437137376831926
Loss in iteration 89 : 0.32433771323324634
Loss in iteration 90 : 0.32430486077521775
Loss in iteration 91 : 0.3242728083541528
Loss in iteration 92 : 0.32424154326537946
Loss in iteration 93 : 0.32421104888669927
Loss in iteration 94 : 0.32418130554767655
Loss in iteration 95 : 0.32415229143728813
Loss in iteration 96 : 0.3241239834820165
Loss in iteration 97 : 0.32409635814309046
Loss in iteration 98 : 0.32406939209781477
Loss in iteration 99 : 0.32404306278487316
Loss in iteration 100 : 0.32401734880658645
Loss in iteration 101 : 0.32399223019182366
Loss in iteration 102 : 0.3239676885315682
Loss in iteration 103 : 0.32394370700486647
Loss in iteration 104 : 0.3239202703163061
Loss in iteration 105 : 0.32389736456746093
Loss in iteration 106 : 0.3238749770843539
Loss in iteration 107 : 0.32385309622116193
Loss in iteration 108 : 0.3238317111576828
Loss in iteration 109 : 0.32381081170476966
Loss in iteration 110 : 0.3237903881283725
Loss in iteration 111 : 0.32377043099935227
Loss in iteration 112 : 0.3237509310729938
Loss in iteration 113 : 0.32373187919932656
Loss in iteration 114 : 0.32371326626311
Loss in iteration 115 : 0.323695083150614
Loss in iteration 116 : 0.3236773207391499
Loss in iteration 117 : 0.3236599699046984
Loss in iteration 118 : 0.3236430215427561
Loss in iteration 119 : 0.32362646659772304
Loss in iteration 120 : 0.3236102960965647
Loss in iteration 121 : 0.3235945011831512
Loss in iteration 122 : 0.323579073150397
Loss in iteration 123 : 0.32356400346808906
Loss in iteration 124 : 0.3235492838050702
Loss in iteration 125 : 0.3235349060450784
Loss in iteration 126 : 0.3235208622961747
Loss in iteration 127 : 0.32350714489410226
Loss in iteration 128 : 0.3234937464003154
Loss in iteration 129 : 0.3234806595955881
Loss in iteration 130 : 0.3234678774702724
Loss in iteration 131 : 0.3234553932122609
Loss in iteration 132 : 0.32344320019368605
Loss in iteration 133 : 0.32343129195724235
Loss in iteration 134 : 0.3234196622029089
Loss in iteration 135 : 0.32340830477564975
Loss in iteration 136 : 0.32339721365450963
Loss in iteration 137 : 0.3233863829433645
Loss in iteration 138 : 0.3233758068634189
Loss in iteration 139 : 0.3233654797474225
Loss in iteration 140 : 0.3233553960354903
Loss in iteration 141 : 0.3233455502723137
Loss in iteration 142 : 0.32333593710552233
Loss in iteration 143 : 0.323326551284931
Loss in iteration 144 : 0.32331738766240004
Loss in iteration 145 : 0.323308441192053
Loss in iteration 146 : 0.3232997069306347
Loss in iteration 147 : 0.3232911800378222
Loss in iteration 148 : 0.3232828557763396
Loss in iteration 149 : 0.3232747295117796
Loss in iteration 150 : 0.32326679671206815
Loss in iteration 151 : 0.3232590529465492
Loss in iteration 152 : 0.3232514938846914
Loss in iteration 153 : 0.32324411529445574
Loss in iteration 154 : 0.32323691304036306
Loss in iteration 155 : 0.32322988308133105
Loss in iteration 156 : 0.3232230214683426
Loss in iteration 157 : 0.32321632434199965
Loss in iteration 158 : 0.3232097879300503
Loss in iteration 159 : 0.3232034085449188
Loss in iteration 160 : 0.3231971825812955
Loss in iteration 161 : 0.32319110651382005
Loss in iteration 162 : 0.3231851768948884
Loss in iteration 163 : 0.32317939035258153
Loss in iteration 164 : 0.32317374358874684
Loss in iteration 165 : 0.32316823337720507
Loss in iteration 166 : 0.32316285656209887
Loss in iteration 167 : 0.32315761005634075
Loss in iteration 168 : 0.32315249084018527
Loss in iteration 169 : 0.3231474959598624
Loss in iteration 170 : 0.3231426225263004
Loss in iteration 171 : 0.3231378677138822
Loss in iteration 172 : 0.323133228759253
Loss in iteration 173 : 0.32312870296014473
Loss in iteration 174 : 0.3231242876742207
Loss in iteration 175 : 0.32311998031792527
Loss in iteration 176 : 0.3231157783653399
Loss in iteration 177 : 0.32311167934703855
Loss in iteration 178 : 0.3231076808489461
Loss in iteration 179 : 0.3231037805111923
Loss in iteration 180 : 0.32309997602697604
Loss in iteration 181 : 0.3230962651414324
Loss in iteration 182 : 0.3230926456505081
Loss in iteration 183 : 0.32308911539985435
Loss in iteration 184 : 0.32308567228372903
Loss in iteration 185 : 0.32308231424392486
Loss in iteration 186 : 0.32307903926871623
Loss in iteration 187 : 0.32307584539182366
Loss in iteration 188 : 0.3230727306914128
Loss in iteration 189 : 0.3230696932891101
Loss in iteration 190 : 0.3230667313490499
Loss in iteration 191 : 0.3230638430769397
Loss in iteration 192 : 0.3230610267191567
Loss in iteration 193 : 0.32305828056186114
Loss in iteration 194 : 0.3230556029301387
Loss in iteration 195 : 0.32305299218715855
Loss in iteration 196 : 0.3230504467333518
Loss in iteration 197 : 0.32304796500561367
Loss in iteration 198 : 0.3230455454765141
Loss in iteration 199 : 0.32304318665353626
Loss in iteration 200 : 0.32304088707831924
Testing accuracy  of updater 8 on alg 0 with rate 0.02 = 0.78975, training accuracy 0.8442861767562317, time elapsed: 2412 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5868165359664868
Loss in iteration 3 : 0.558358356144777
Loss in iteration 4 : 0.4611427225027545
Loss in iteration 5 : 0.3721759674599902
Loss in iteration 6 : 0.3464800576799905
Loss in iteration 7 : 0.3503395126679316
Loss in iteration 8 : 0.3459794845816904
Loss in iteration 9 : 0.3373970655169743
Loss in iteration 10 : 0.3323368018935426
Loss in iteration 11 : 0.3313880205699715
Loss in iteration 12 : 0.3324759030925091
Loss in iteration 13 : 0.33392758115384785
Loss in iteration 14 : 0.3350995351149946
Loss in iteration 15 : 0.33600302682012056
Loss in iteration 16 : 0.33681496187790005
Loss in iteration 17 : 0.3376110742748154
Loss in iteration 18 : 0.3383382713364182
Loss in iteration 19 : 0.3389025547243589
Loss in iteration 20 : 0.33924801318063075
Loss in iteration 21 : 0.33937554327755315
Loss in iteration 22 : 0.3393181078603031
Loss in iteration 23 : 0.3391105284912368
Loss in iteration 24 : 0.3387759346130238
Loss in iteration 25 : 0.3383284807457228
Loss in iteration 26 : 0.3377814946421237
Loss in iteration 27 : 0.3371523060668057
Loss in iteration 28 : 0.3364617204709827
Loss in iteration 29 : 0.33573079126263133
Loss in iteration 30 : 0.33497818865139284
Loss in iteration 31 : 0.33421955465183284
Loss in iteration 32 : 0.33346822795357794
Loss in iteration 33 : 0.33273605881349183
Loss in iteration 34 : 0.3320335964085521
Loss in iteration 35 : 0.3313697655255907
Loss in iteration 36 : 0.3307515101186871
Loss in iteration 37 : 0.33018370800103264
Loss in iteration 38 : 0.3296693326313486
Loss in iteration 39 : 0.32920968361397174
Loss in iteration 40 : 0.3288045677028951
Loss in iteration 41 : 0.32845243631936183
Loss in iteration 42 : 0.3281505410455658
Loss in iteration 43 : 0.32789514112649587
Loss in iteration 44 : 0.3276817486450701
Loss in iteration 45 : 0.32750537825731224
Loss in iteration 46 : 0.32736078032695265
Loss in iteration 47 : 0.3272426532123154
Loss in iteration 48 : 0.3271458353516709
Loss in iteration 49 : 0.32706547329693864
Loss in iteration 50 : 0.32699715806746854
Loss in iteration 51 : 0.32693702377073103
Loss in iteration 52 : 0.326881807032644
Loss in iteration 53 : 0.32682886966812047
Loss in iteration 54 : 0.3267761889831583
Loss in iteration 55 : 0.3267223209811465
Loss in iteration 56 : 0.32666634243195825
Loss in iteration 57 : 0.32660777830249893
Loss in iteration 58 : 0.32654652110448223
Loss in iteration 59 : 0.32648274816241235
Loss in iteration 60 : 0.32641684181757874
Loss in iteration 61 : 0.32634931641511195
Loss in iteration 62 : 0.32628075473676577
Loss in iteration 63 : 0.32621175542218794
Loss in iteration 64 : 0.3261428919201139
Loss in iteration 65 : 0.3260746826794237
Loss in iteration 66 : 0.32600757166372185
Loss in iteration 67 : 0.3259419178579069
Loss in iteration 68 : 0.32587799221043723
Loss in iteration 69 : 0.3258159803873819
Loss in iteration 70 : 0.3257559897669857
Loss in iteration 71 : 0.3256980592419334
Loss in iteration 72 : 0.3256421705890108
Loss in iteration 73 : 0.3255882603853223
Loss in iteration 74 : 0.3255362316747174
Loss in iteration 75 : 0.3254859648015296
Loss in iteration 76 : 0.32543732702074896
Loss in iteration 77 : 0.3253901806582599
Loss in iteration 78 : 0.3253443897295098
Loss in iteration 79 : 0.32529982503011434
Loss in iteration 80 : 0.32525636778939176
Loss in iteration 81 : 0.32521391203081185
Loss in iteration 82 : 0.32517236581547215
Loss in iteration 83 : 0.3251316515597467
Loss in iteration 84 : 0.3250917056199073
Loss in iteration 85 : 0.3250524773281061
Loss in iteration 86 : 0.3250139276486425
Loss in iteration 87 : 0.32497602760339406
Loss in iteration 88 : 0.3249387565927707
Loss in iteration 89 : 0.32490210071523096
Loss in iteration 90 : 0.3248660511654898
Loss in iteration 91 : 0.324830602770085
Loss in iteration 92 : 0.3247957526995262
Loss in iteration 93 : 0.32476149937925763
Loss in iteration 94 : 0.3247278416074008
Loss in iteration 95 : 0.32469477787563084
Loss in iteration 96 : 0.3246623058806767
Loss in iteration 97 : 0.32463042220744615
Loss in iteration 98 : 0.3245991221606793
Loss in iteration 99 : 0.3245683997197704
Loss in iteration 100 : 0.32453824759091
Loss in iteration 101 : 0.3245086573314763
Loss in iteration 102 : 0.32447961952348314
Loss in iteration 103 : 0.32445112397543846
Loss in iteration 104 : 0.32442315993505005
Loss in iteration 105 : 0.32439571629846803
Loss in iteration 106 : 0.3243687818050654
Loss in iteration 107 : 0.3243423452099067
Loss in iteration 108 : 0.32431639542895196
Loss in iteration 109 : 0.3242909216545835
Loss in iteration 110 : 0.3242659134411487
Loss in iteration 111 : 0.3242413607619246
Loss in iteration 112 : 0.324217254040162
Loss in iteration 113 : 0.3241935841577109
Loss in iteration 114 : 0.3241703424452281
Loss in iteration 115 : 0.32414752065814356
Loss in iteration 116 : 0.3241251109424366
Loss in iteration 117 : 0.32410310579402996
Loss in iteration 118 : 0.32408149801513497
Loss in iteration 119 : 0.3240602806703691
Loss in iteration 120 : 0.3240394470449092
Loss in iteration 121 : 0.32401899060633504
Loss in iteration 122 : 0.3239989049713226
Loss in iteration 123 : 0.32397918387780533
Loss in iteration 124 : 0.32395982116282707
Loss in iteration 125 : 0.3239408107459494
Loss in iteration 126 : 0.3239221466178012
Loss in iteration 127 : 0.3239038228331948
Loss in iteration 128 : 0.3238858335080626
Loss in iteration 129 : 0.3238681728194798
Loss in iteration 130 : 0.32385083500796685
Loss in iteration 131 : 0.32383381438136016
Loss in iteration 132 : 0.3238171053195818
Loss in iteration 133 : 0.32380070227973645
Loss in iteration 134 : 0.32378459980107344
Loss in iteration 135 : 0.3237687925094564
Loss in iteration 136 : 0.32375327512108365
Loss in iteration 137 : 0.3237380424453034
Loss in iteration 138 : 0.3237230893864616
Loss in iteration 139 : 0.32370841094476255
Loss in iteration 140 : 0.32369400221622135
Loss in iteration 141 : 0.32367985839179375
Loss in iteration 142 : 0.32366597475580605
Loss in iteration 143 : 0.3236523466838389
Loss in iteration 144 : 0.32363896964020455
Loss in iteration 145 : 0.3236258391751521
Loss in iteration 146 : 0.32361295092194503
Loss in iteration 147 : 0.3236003005939103
Loss in iteration 148 : 0.32358788398156135
Loss in iteration 149 : 0.32357569694986876
Loss in iteration 150 : 0.3235637354357269
Loss in iteration 151 : 0.32355199544565366
Loss in iteration 152 : 0.32354047305374345
Loss in iteration 153 : 0.3235291643998626
Loss in iteration 154 : 0.3235180656880936
Loss in iteration 155 : 0.3235071731853889
Loss in iteration 156 : 0.3234964832204232
Loss in iteration 157 : 0.32348599218260676
Loss in iteration 158 : 0.3234756965212262
Loss in iteration 159 : 0.32346559274468195
Loss in iteration 160 : 0.3234556774197996
Loss in iteration 161 : 0.32344594717117353
Loss in iteration 162 : 0.3234363986805366
Loss in iteration 163 : 0.3234270286861221
Loss in iteration 164 : 0.323417833982016
Loss in iteration 165 : 0.3234088114174862
Loss in iteration 166 : 0.3233999578962716
Loss in iteration 167 : 0.32339127037584403
Loss in iteration 168 : 0.3233827458666448
Loss in iteration 169 : 0.3233743814312712
Loss in iteration 170 : 0.32336617418365715
Loss in iteration 171 : 0.3233581212882216
Loss in iteration 172 : 0.32335021995900476
Loss in iteration 173 : 0.3233424674588041
Loss in iteration 174 : 0.32333486109829557
Loss in iteration 175 : 0.32332739823517104
Loss in iteration 176 : 0.3233200762732799
Loss in iteration 177 : 0.3233128926617859
Loss in iteration 178 : 0.3233058448943368
Loss in iteration 179 : 0.32329893050826136
Loss in iteration 180 : 0.32329214708377757
Loss in iteration 181 : 0.3232854922432332
Loss in iteration 182 : 0.3232789636503576
Loss in iteration 183 : 0.32327255900954255
Loss in iteration 184 : 0.32326627606514435
Loss in iteration 185 : 0.323260112600794
Loss in iteration 186 : 0.3232540664387439
Loss in iteration 187 : 0.3232481354392119
Loss in iteration 188 : 0.323242317499756
Loss in iteration 189 : 0.32323661055464875
Loss in iteration 190 : 0.3232310125742758
Loss in iteration 191 : 0.32322552156453827
Loss in iteration 192 : 0.3232201355662677
Loss in iteration 193 : 0.32321485265464817
Loss in iteration 194 : 0.3232096709386481
Loss in iteration 195 : 0.3232045885604594
Loss in iteration 196 : 0.32319960369494494
Loss in iteration 197 : 0.3231947145490931
Loss in iteration 198 : 0.323189919361476
Loss in iteration 199 : 0.3231852164017242
Loss in iteration 200 : 0.3231806039699958
Testing accuracy  of updater 8 on alg 0 with rate 0.014 = 0.78725, training accuracy 0.842667529944966, time elapsed: 3078 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.498917744173116
Loss in iteration 3 : 0.5041662373390942
Loss in iteration 4 : 0.4715018335837526
Loss in iteration 5 : 0.41979570131578164
Loss in iteration 6 : 0.37727302196588924
Loss in iteration 7 : 0.3582662238310588
Loss in iteration 8 : 0.3537283774855472
Loss in iteration 9 : 0.350061401057817
Loss in iteration 10 : 0.3439416631486519
Loss in iteration 11 : 0.33785019941987066
Loss in iteration 12 : 0.3337455269306594
Loss in iteration 13 : 0.33178124242869583
Loss in iteration 14 : 0.3312071162550132
Loss in iteration 15 : 0.33123641500465095
Loss in iteration 16 : 0.33140201159810695
Loss in iteration 17 : 0.3315530817854954
Loss in iteration 18 : 0.3317118712671315
Loss in iteration 19 : 0.3319345700390418
Loss in iteration 20 : 0.3322369879182296
Loss in iteration 21 : 0.332585797132
Loss in iteration 22 : 0.3329258309158798
Loss in iteration 23 : 0.3332107849295545
Loss in iteration 24 : 0.3334184744764636
Loss in iteration 25 : 0.3335488146064959
Loss in iteration 26 : 0.33361286273739005
Loss in iteration 27 : 0.33362230577258106
Loss in iteration 28 : 0.33358433618105043
Loss in iteration 29 : 0.33350186697354217
Loss in iteration 30 : 0.3333763601298182
Loss in iteration 31 : 0.3332104716402966
Loss in iteration 32 : 0.33300908085909103
Loss in iteration 33 : 0.33277874807166835
Loss in iteration 34 : 0.3325264583767037
Loss in iteration 35 : 0.3322585436469071
Loss in iteration 36 : 0.33198023315058395
Loss in iteration 37 : 0.3316957820382757
Loss in iteration 38 : 0.3314088489632456
Loss in iteration 39 : 0.3311228002261131
Loss in iteration 40 : 0.3308407953463269
Loss in iteration 41 : 0.33056569468503977
Loss in iteration 42 : 0.3302999177889046
Loss in iteration 43 : 0.33004536077561564
Loss in iteration 44 : 0.32980340693014676
Loss in iteration 45 : 0.32957500044678517
Loss in iteration 46 : 0.3293607317516088
Loss in iteration 47 : 0.3291608992711797
Loss in iteration 48 : 0.32897554208254653
Loss in iteration 49 : 0.32880445806931197
Loss in iteration 50 : 0.3286472249958325
Loss in iteration 51 : 0.3285032327524361
Loss in iteration 52 : 0.3283717242985704
Loss in iteration 53 : 0.3282518376843866
Loss in iteration 54 : 0.32814264284386657
Loss in iteration 55 : 0.3280431712033725
Loss in iteration 56 : 0.32795243965341253
Loss in iteration 57 : 0.3278694713344817
Loss in iteration 58 : 0.3277933145847001
Loss in iteration 59 : 0.32772305988358924
Loss in iteration 60 : 0.3276578538763213
Loss in iteration 61 : 0.3275969097652041
Loss in iteration 62 : 0.3275395140136607
Loss in iteration 63 : 0.3274850298488773
Loss in iteration 64 : 0.3274328982174274
Loss in iteration 65 : 0.32738263672891615
Loss in iteration 66 : 0.32733383693664864
Loss in iteration 67 : 0.3272861602064511
Loss in iteration 68 : 0.32723933243496944
Loss in iteration 69 : 0.32719313793121274
Loss in iteration 70 : 0.32714741280066806
Loss in iteration 71 : 0.32710203814544697
Loss in iteration 72 : 0.32705693333349145
Loss in iteration 73 : 0.32701204952466983
Loss in iteration 74 : 0.3269673635903604
Loss in iteration 75 : 0.3269228725274288
Loss in iteration 76 : 0.3268785884397648
Loss in iteration 77 : 0.3268345341335318
Loss in iteration 78 : 0.32679073934447755
Loss in iteration 79 : 0.3267472375894226
Loss in iteration 80 : 0.3267040636127185
Loss in iteration 81 : 0.32666125138352337
Loss in iteration 82 : 0.3266188325905298
Loss in iteration 83 : 0.3265768355758331
Loss in iteration 84 : 0.32653528464754916
Loss in iteration 85 : 0.32649419971078797
Loss in iteration 86 : 0.32645359615850655
Loss in iteration 87 : 0.32641348496722744
Loss in iteration 88 : 0.32637387294750475
Loss in iteration 89 : 0.3263347631046771
Loss in iteration 90 : 0.32629615507152143
Loss in iteration 91 : 0.3262580455805019
Loss in iteration 92 : 0.3262204289490988
Loss in iteration 93 : 0.3261832975572037
Loss in iteration 94 : 0.32614664230048884
Loss in iteration 95 : 0.32611045300816344
Loss in iteration 96 : 0.32607471881737554
Loss in iteration 97 : 0.3260394284998391
Loss in iteration 98 : 0.32600457073894445
Loss in iteration 99 : 0.3259701343577996
Loss in iteration 100 : 0.32593610850028815
Loss in iteration 101 : 0.3259024827684666
Loss in iteration 102 : 0.3258692473204501
Loss in iteration 103 : 0.32583639293344135
Loss in iteration 104 : 0.3258039110367937
Loss in iteration 105 : 0.3257717937200065
Loss in iteration 106 : 0.32574003372040894
Loss in iteration 107 : 0.3257086243949465
Loss in iteration 108 : 0.32567755968017126
Loss in iteration 109 : 0.32564683404405076
Loss in iteration 110 : 0.32561644243273546
Loss in iteration 111 : 0.3255863802149998
Loss in iteration 112 : 0.32555664312653637
Loss in iteration 113 : 0.3255272272158878
Loss in iteration 114 : 0.32549812879335993
Loss in iteration 115 : 0.32546934438391606
Loss in iteration 116 : 0.32544087068468464
Loss in iteration 117 : 0.32541270452747867
Loss in iteration 118 : 0.32538484284644037
Loss in iteration 119 : 0.325357282650767
Loss in iteration 120 : 0.32533002100231223
Loss in iteration 121 : 0.3253030549977262
Loss in iteration 122 : 0.3252763817547458
Loss in iteration 123 : 0.32524999840216506
Loss in iteration 124 : 0.3252239020730276
Loss in iteration 125 : 0.32519808990052496
Loss in iteration 126 : 0.3251725590161599
Loss in iteration 127 : 0.32514730654969587
Loss in iteration 128 : 0.3251223296305082
Loss in iteration 129 : 0.32509762538993175
Loss in iteration 130 : 0.3250731909643045
Loss in iteration 131 : 0.3250490234984139
Loss in iteration 132 : 0.3250251201491038
Loss in iteration 133 : 0.3250014780888658
Loss in iteration 134 : 0.32497809450925613
Loss in iteration 135 : 0.32495496662403245
Loss in iteration 136 : 0.3249320916719279
Loss in iteration 137 : 0.32490946691902584
Loss in iteration 138 : 0.3248870896607021
Loss in iteration 139 : 0.32486495722314285
Loss in iteration 140 : 0.32484306696444565
Loss in iteration 141 : 0.32482141627533767
Loss in iteration 142 : 0.3248000025795402
Loss in iteration 143 : 0.32477882333382885
Loss in iteration 144 : 0.32475787602783335
Loss in iteration 145 : 0.3247371581836125
Loss in iteration 146 : 0.32471666735506655
Loss in iteration 147 : 0.32469640112721204
Loss in iteration 148 : 0.3246763571153725
Loss in iteration 149 : 0.32465653296430963
Loss in iteration 150 : 0.32463692634732955
Loss in iteration 151 : 0.324617534965389
Loss in iteration 152 : 0.3245983565462228
Loss in iteration 153 : 0.3245793888435037
Loss in iteration 154 : 0.3245606296360576
Loss in iteration 155 : 0.3245420767271313
Loss in iteration 156 : 0.32452372794371687
Loss in iteration 157 : 0.32450558113595324
Loss in iteration 158 : 0.3244876341765747
Loss in iteration 159 : 0.3244698849604312
Loss in iteration 160 : 0.3244523314040583
Loss in iteration 161 : 0.32443497144530675
Loss in iteration 162 : 0.32441780304300655
Loss in iteration 163 : 0.32440082417668575
Loss in iteration 164 : 0.3243840328463109
Loss in iteration 165 : 0.3243674270720638
Loss in iteration 166 : 0.32435100489413704
Loss in iteration 167 : 0.32433476437255127
Loss in iteration 168 : 0.32431870358698195
Loss in iteration 169 : 0.3243028206365938
Loss in iteration 170 : 0.32428711363989116
Loss in iteration 171 : 0.3242715807345541
Loss in iteration 172 : 0.32425622007729493
Loss in iteration 173 : 0.3242410298436947
Loss in iteration 174 : 0.3242260082280511
Loss in iteration 175 : 0.32421115344321766
Loss in iteration 176 : 0.3241964637204361
Loss in iteration 177 : 0.3241819373091756
Loss in iteration 178 : 0.32416757247695366
Loss in iteration 179 : 0.32415336750916723
Loss in iteration 180 : 0.3241393207089098
Loss in iteration 181 : 0.3241254303967991
Loss in iteration 182 : 0.32411169491078523
Loss in iteration 183 : 0.324098112605979
Loss in iteration 184 : 0.32408468185446115
Loss in iteration 185 : 0.3240714010451023
Loss in iteration 186 : 0.32405826858338166
Loss in iteration 187 : 0.32404528289120504
Loss in iteration 188 : 0.32403244240672496
Loss in iteration 189 : 0.3240197455841672
Loss in iteration 190 : 0.32400719089365165
Loss in iteration 191 : 0.32399477682102507
Loss in iteration 192 : 0.3239825018676855
Loss in iteration 193 : 0.3239703645504221
Loss in iteration 194 : 0.32395836340124756
Loss in iteration 195 : 0.3239464969672365
Loss in iteration 196 : 0.3239347638103716
Loss in iteration 197 : 0.32392316250738185
Loss in iteration 198 : 0.3239116916495966
Loss in iteration 199 : 0.3239003498427901
Loss in iteration 200 : 0.32388913570703554
Testing accuracy  of updater 8 on alg 0 with rate 0.008 = 0.788, training accuracy 0.8433149886694723, time elapsed: 2884 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5409429640918185
Loss in iteration 3 : 0.48689864199571464
Loss in iteration 4 : 0.4733556899958963
Loss in iteration 5 : 0.47151664656752584
Loss in iteration 6 : 0.4696129101519972
Loss in iteration 7 : 0.4633658736952669
Loss in iteration 8 : 0.4521417271594458
Loss in iteration 9 : 0.43717880054617664
Loss in iteration 10 : 0.4205832383171114
Loss in iteration 11 : 0.40460304821041837
Loss in iteration 12 : 0.39103439454472744
Loss in iteration 13 : 0.3808063395142406
Loss in iteration 14 : 0.37386369812171993
Loss in iteration 15 : 0.369394607264017
Loss in iteration 16 : 0.3662840866001983
Loss in iteration 17 : 0.3635632534087274
Loss in iteration 18 : 0.36066544083251023
Loss in iteration 19 : 0.3574461371288571
Loss in iteration 20 : 0.3540479390325699
Loss in iteration 21 : 0.3507257660936822
Loss in iteration 22 : 0.34771186210063776
Loss in iteration 23 : 0.34514729846088427
Loss in iteration 24 : 0.3430704404980955
Loss in iteration 25 : 0.34143928989996225
Loss in iteration 26 : 0.3401660550187766
Loss in iteration 27 : 0.33914958337505635
Loss in iteration 28 : 0.33829872495746993
Loss in iteration 29 : 0.3375450452111857
Loss in iteration 30 : 0.3368463491765538
Loss in iteration 31 : 0.33618376219776885
Loss in iteration 32 : 0.33555528865596373
Loss in iteration 33 : 0.33496834538905473
Loss in iteration 34 : 0.33443307524309707
Loss in iteration 35 : 0.3339574948146955
Loss in iteration 36 : 0.33354484608954615
Loss in iteration 37 : 0.3331929832799173
Loss in iteration 38 : 0.3328952766840451
Loss in iteration 39 : 0.3326423640574657
Loss in iteration 40 : 0.33242410354698604
Loss in iteration 41 : 0.33223123124571874
Loss in iteration 42 : 0.33205643765581744
Loss in iteration 43 : 0.3318947892280823
Loss in iteration 44 : 0.3317435872677684
Loss in iteration 45 : 0.33160185303598955
Loss in iteration 46 : 0.3314696536474057
Loss in iteration 47 : 0.3313474539060199
Loss in iteration 48 : 0.3312356181355607
Loss in iteration 49 : 0.3311341168395895
Loss in iteration 50 : 0.3310424334846273
Loss in iteration 51 : 0.3309596266812645
Loss in iteration 52 : 0.33088448485283645
Loss in iteration 53 : 0.3308157109697037
Loss in iteration 54 : 0.3307520880363154
Loss in iteration 55 : 0.33069259508047744
Loss in iteration 56 : 0.33063646270946917
Loss in iteration 57 : 0.3305831729462319
Loss in iteration 58 : 0.33053241807595735
Loss in iteration 59 : 0.3304840373506933
Loss in iteration 60 : 0.33043794956139777
Loss in iteration 61 : 0.3303940953035094
Loss in iteration 62 : 0.3303523970141443
Loss in iteration 63 : 0.33031273911812475
Loss in iteration 64 : 0.3302749660144527
Loss in iteration 65 : 0.3302388927591235
Loss in iteration 66 : 0.3302043222469432
Loss in iteration 67 : 0.33017106317198375
Loss in iteration 68 : 0.3301389445366834
Loss in iteration 69 : 0.3301078244044195
Loss in iteration 70 : 0.33007759244143103
Loss in iteration 71 : 0.33004816721692304
Loss in iteration 72 : 0.33001949005540854
Loss in iteration 73 : 0.32999151746166744
Loss in iteration 74 : 0.32996421388765595
Loss in iteration 75 : 0.32993754606717146
Loss in iteration 76 : 0.3299114795010245
Loss in iteration 77 : 0.32988597709190964
Loss in iteration 78 : 0.3298609995061098
Loss in iteration 79 : 0.3298365066192716
Loss in iteration 80 : 0.3298124593736851
Loss in iteration 81 : 0.32978882148776406
Loss in iteration 82 : 0.3297655606520368
Loss in iteration 83 : 0.32974264905844214
Loss in iteration 84 : 0.3297200632932065
Loss in iteration 85 : 0.32969778374922054
Loss in iteration 86 : 0.3296757937724496
Loss in iteration 87 : 0.32965407875548813
Loss in iteration 88 : 0.3296326253467086
Loss in iteration 89 : 0.3296114208767412
Loss in iteration 90 : 0.3295904530347303
Loss in iteration 91 : 0.32956970976976435
Loss in iteration 92 : 0.3295491793562353
Loss in iteration 93 : 0.3295288505474863
Loss in iteration 94 : 0.3295087127468327
Loss in iteration 95 : 0.3294887561425802
Loss in iteration 96 : 0.3294689717771284
Loss in iteration 97 : 0.32944935154311067
Loss in iteration 98 : 0.32942988811740564
Loss in iteration 99 : 0.32941057485449976
Loss in iteration 100 : 0.32939140566389985
Loss in iteration 101 : 0.3293723748935147
Loss in iteration 102 : 0.32935347723435826
Loss in iteration 103 : 0.32933470765397355
Loss in iteration 104 : 0.3293160613586372
Loss in iteration 105 : 0.3292975337790308
Loss in iteration 106 : 0.3292791205711827
Loss in iteration 107 : 0.32926081762405623
Loss in iteration 108 : 0.3292426210665311
Loss in iteration 109 : 0.3292245272690007
Loss in iteration 110 : 0.32920653283750456
Loss in iteration 111 : 0.32918863460072556
Loss in iteration 112 : 0.32917082959173205
Loss in iteration 113 : 0.32915311502719424
Loss in iteration 114 : 0.3291354882867192
Loss in iteration 115 : 0.3291179468943821
Loss in iteration 116 : 0.3291004885037088
Loss in iteration 117 : 0.32908311088640635
Loss in iteration 118 : 0.3290658119244627
Loss in iteration 119 : 0.3290485896047657
Loss in iteration 120 : 0.32903144201520573
Loss in iteration 121 : 0.3290143673413291
Loss in iteration 122 : 0.3289973638628409
Loss in iteration 123 : 0.32898042994956916
Loss in iteration 124 : 0.32896356405679195
Loss in iteration 125 : 0.32894676472008505
Loss in iteration 126 : 0.32893003054993114
Loss in iteration 127 : 0.3289133602264237
Loss in iteration 128 : 0.328896752494307
Loss in iteration 129 : 0.32888020615852703
Loss in iteration 130 : 0.3288637200803666
Loss in iteration 131 : 0.3288472931741335
Loss in iteration 132 : 0.3288309244043151
Loss in iteration 133 : 0.32881461278307045
Loss in iteration 134 : 0.3287983573679461
Loss in iteration 135 : 0.32878215725970567
Loss in iteration 136 : 0.32876601160021707
Loss in iteration 137 : 0.3287499195703591
Loss in iteration 138 : 0.32873388038795504
Loss in iteration 139 : 0.3287178933057503
Loss in iteration 140 : 0.32870195760947013
Loss in iteration 141 : 0.3286860726159773
Loss in iteration 142 : 0.328670237671548
Loss in iteration 143 : 0.3286544521502879
Loss in iteration 144 : 0.32863871545266465
Loss in iteration 145 : 0.3286230270041624
Loss in iteration 146 : 0.3286073862540267
Loss in iteration 147 : 0.32859179267409533
Loss in iteration 148 : 0.32857624575769123
Loss in iteration 149 : 0.3285607450185657
Loss in iteration 150 : 0.3285452899898902
Loss in iteration 151 : 0.328529880223289
Loss in iteration 152 : 0.32851451528791575
Loss in iteration 153 : 0.32849919476956696
Loss in iteration 154 : 0.3284839182698498
Loss in iteration 155 : 0.3284686854053839
Loss in iteration 156 : 0.3284534958070612
Loss in iteration 157 : 0.32843834911933606
Loss in iteration 158 : 0.3284232449995655
Loss in iteration 159 : 0.3284081831173844
Loss in iteration 160 : 0.3283931631541094
Loss in iteration 161 : 0.3283781848021814
Loss in iteration 162 : 0.32836324776463494
Loss in iteration 163 : 0.3283483517545851
Loss in iteration 164 : 0.3283334964947512
Loss in iteration 165 : 0.3283186817169932
Loss in iteration 166 : 0.32830390716188
Loss in iteration 167 : 0.3282891725782686
Loss in iteration 168 : 0.3282744777229149
Loss in iteration 169 : 0.3282598223600979
Loss in iteration 170 : 0.32824520626126424
Loss in iteration 171 : 0.32823062920469476
Loss in iteration 172 : 0.32821609097517906
Loss in iteration 173 : 0.32820159136371857
Loss in iteration 174 : 0.32818713016723294
Loss in iteration 175 : 0.3281727071882903
Loss in iteration 176 : 0.3281583222348397
Loss in iteration 177 : 0.32814397511997195
Loss in iteration 178 : 0.3281296656616734
Loss in iteration 179 : 0.328115393682608
Loss in iteration 180 : 0.32810115900990017
Loss in iteration 181 : 0.32808696147493216
Loss in iteration 182 : 0.32807280091315005
Loss in iteration 183 : 0.3280586771638786
Loss in iteration 184 : 0.32804459007014947
Loss in iteration 185 : 0.3280305394785296
Loss in iteration 186 : 0.3280165252389649
Loss in iteration 187 : 0.3280025472046331
Loss in iteration 188 : 0.32798860523179274
Loss in iteration 189 : 0.3279746991796524
Loss in iteration 190 : 0.32796082891023826
Loss in iteration 191 : 0.32794699428826946
Loss in iteration 192 : 0.3279331951810435
Loss in iteration 193 : 0.3279194314583191
Loss in iteration 194 : 0.32790570299221367
Loss in iteration 195 : 0.32789200965709747
Loss in iteration 196 : 0.3278783513295034
Loss in iteration 197 : 0.32786472788802606
Loss in iteration 198 : 0.3278511392132392
Loss in iteration 199 : 0.3278375851876111
Loss in iteration 200 : 0.32782406569542605
Testing accuracy  of updater 8 on alg 0 with rate 0.0019999999999999983 = 0.78475, training accuracy 0.8387827775979282, time elapsed: 2962 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 28.878615836293676
Loss in iteration 3 : 31.33546952003798
Loss in iteration 4 : 19.90080840501129
Loss in iteration 5 : 4.97047022834106
Loss in iteration 6 : 11.83399546970483
Loss in iteration 7 : 17.237256069856077
Loss in iteration 8 : 9.81340826519011
Loss in iteration 9 : 7.104452592691641
Loss in iteration 10 : 9.755233938517868
Loss in iteration 11 : 12.665264273472037
Loss in iteration 12 : 13.690843744796286
Loss in iteration 13 : 12.635299955664948
Loss in iteration 14 : 10.623634859622735
Loss in iteration 15 : 9.073338652046735
Loss in iteration 16 : 9.230614434149274
Loss in iteration 17 : 10.861618496007894
Loss in iteration 18 : 11.573216571039136
Loss in iteration 19 : 10.471656934505969
Loss in iteration 20 : 8.86334720054003
Loss in iteration 21 : 8.413392039729985
Loss in iteration 22 : 8.884676368898512
Loss in iteration 23 : 9.324682637052964
Loss in iteration 24 : 9.12548017878824
Loss in iteration 25 : 8.254809552002262
Loss in iteration 26 : 7.14373993331787
Loss in iteration 27 : 6.519186185883354
Loss in iteration 28 : 6.7456650889151515
Loss in iteration 29 : 6.834039307868925
Loss in iteration 30 : 5.868344313071846
Loss in iteration 31 : 4.882353712510856
Loss in iteration 32 : 4.752667680260148
Loss in iteration 33 : 4.784715384722783
Loss in iteration 34 : 4.257102761073783
Loss in iteration 35 : 3.2538744642465507
Loss in iteration 36 : 2.847271153627015
Loss in iteration 37 : 3.080171699055066
Loss in iteration 38 : 1.8215887195867373
Loss in iteration 39 : 2.020002857490404
Loss in iteration 40 : 1.6887040366964592
Loss in iteration 41 : 2.256704386717688
Loss in iteration 42 : 5.771454800187332
Loss in iteration 43 : 5.754522273745509
Loss in iteration 44 : 0.9645160883215222
Loss in iteration 45 : 10.028874831106709
Loss in iteration 46 : 8.193327472956463
Loss in iteration 47 : 16.06001776043865
Loss in iteration 48 : 17.65872389985621
Loss in iteration 49 : 13.934626640289451
Loss in iteration 50 : 7.907098968821624
Loss in iteration 51 : 4.7232561456822415
Loss in iteration 52 : 5.90143843760851
Loss in iteration 53 : 9.68941337068057
Loss in iteration 54 : 11.249560207935584
Loss in iteration 55 : 9.514241841066854
Loss in iteration 56 : 7.216931183074236
Loss in iteration 57 : 6.401154599710318
Loss in iteration 58 : 7.117123082165264
Loss in iteration 59 : 8.230464841591557
Loss in iteration 60 : 9.02841993856807
Loss in iteration 61 : 9.215319061935578
Loss in iteration 62 : 8.781504626842011
Loss in iteration 63 : 7.990100946216077
Loss in iteration 64 : 7.163217916360531
Loss in iteration 65 : 6.670811948446979
Loss in iteration 66 : 6.767763301723343
Loss in iteration 67 : 7.253724264034192
Loss in iteration 68 : 7.496124277771807
Loss in iteration 69 : 7.153678630450502
Loss in iteration 70 : 6.441405762457516
Loss in iteration 71 : 5.855730225310955
Loss in iteration 72 : 5.711875012055367
Loss in iteration 73 : 5.789295295146064
Loss in iteration 74 : 5.831723490715666
Loss in iteration 75 : 5.666389176311909
Loss in iteration 76 : 5.268066979701295
Loss in iteration 77 : 4.754409189963209
Loss in iteration 78 : 4.323479545766635
Loss in iteration 79 : 4.183390248047396
Loss in iteration 80 : 4.190336967019912
Loss in iteration 81 : 3.99217028786402
Loss in iteration 82 : 3.511027984271003
Loss in iteration 83 : 3.0901448605072543
Loss in iteration 84 : 2.9392177440858402
Loss in iteration 85 : 2.863722170143563
Loss in iteration 86 : 2.6203344402827233
Loss in iteration 87 : 2.188890691906806
Loss in iteration 88 : 1.8848304095929576
Loss in iteration 89 : 1.9067777950918237
Loss in iteration 90 : 1.632657614825697
Loss in iteration 91 : 1.315309841811763
Loss in iteration 92 : 1.4448229114906932
Loss in iteration 93 : 1.2504011230678513
Loss in iteration 94 : 1.3957064116180558
Loss in iteration 95 : 1.151288918168945
Loss in iteration 96 : 1.338198880451365
Loss in iteration 97 : 1.636666544226168
Loss in iteration 98 : 4.032012427331557
Loss in iteration 99 : 3.5708539895648914
Loss in iteration 100 : 1.1440667943232876
Loss in iteration 101 : 3.624112797316124
Loss in iteration 102 : 1.7913757942867072
Loss in iteration 103 : 1.8433498979510703
Loss in iteration 104 : 2.754208495366721
Loss in iteration 105 : 2.752948092953724
Loss in iteration 106 : 2.156063292645438
Loss in iteration 107 : 1.9845199054251677
Loss in iteration 108 : 2.575896179118263
Loss in iteration 109 : 2.5481885194799525
Loss in iteration 110 : 2.0219304492269248
Loss in iteration 111 : 2.0748159098775525
Loss in iteration 112 : 2.317365336631173
Loss in iteration 113 : 2.2357688227544226
Loss in iteration 114 : 1.855696632789809
Loss in iteration 115 : 1.6724821220407375
Loss in iteration 116 : 1.8753076560785231
Loss in iteration 117 : 1.6162695945156156
Loss in iteration 118 : 1.3092996759312856
Loss in iteration 119 : 1.4235735462639627
Loss in iteration 120 : 1.3057659510816708
Loss in iteration 121 : 0.9382331523520505
Loss in iteration 122 : 1.2348871185353953
Loss in iteration 123 : 0.7642569877117378
Loss in iteration 124 : 1.0222014348553856
Loss in iteration 125 : 0.6970922846248078
Loss in iteration 126 : 1.7055726816888752
Loss in iteration 127 : 5.932966254370626
Loss in iteration 128 : 8.180355198860637
Loss in iteration 129 : 5.95752279771371
Loss in iteration 130 : 1.5480366069276064
Loss in iteration 131 : 3.88795429290777
Loss in iteration 132 : 4.874194068622522
Loss in iteration 133 : 2.1318310110498255
Loss in iteration 134 : 2.561371976101085
Loss in iteration 135 : 3.840771809486441
Loss in iteration 136 : 4.242754291269118
Loss in iteration 137 : 3.6328564176740867
Loss in iteration 138 : 2.799437812112838
Loss in iteration 139 : 2.6234116529497906
Loss in iteration 140 : 3.3557488718526245
Loss in iteration 141 : 3.642691743533685
Loss in iteration 142 : 2.990062103071524
Loss in iteration 143 : 2.5029281976510287
Loss in iteration 144 : 2.6984290599634977
Loss in iteration 145 : 2.966365669775736
Loss in iteration 146 : 2.902035794124599
Loss in iteration 147 : 2.4858384016178614
Loss in iteration 148 : 2.067134298186496
Loss in iteration 149 : 2.1081902220503954
Loss in iteration 150 : 2.3065003214471904
Loss in iteration 151 : 1.8818474846945317
Loss in iteration 152 : 1.5251858872518542
Loss in iteration 153 : 1.637906330061909
Loss in iteration 154 : 1.6193951974939569
Loss in iteration 155 : 1.2070490662255244
Loss in iteration 156 : 1.0246722859163349
Loss in iteration 157 : 1.2598418362895063
Loss in iteration 158 : 0.7777809026227973
Loss in iteration 159 : 1.1314942031904605
Loss in iteration 160 : 0.6304402003730603
Loss in iteration 161 : 2.8943768054800283
Loss in iteration 162 : 8.11596586722269
Loss in iteration 163 : 13.325700988753427
Loss in iteration 164 : 14.344280930687068
Loss in iteration 165 : 11.658536479648772
Loss in iteration 166 : 5.945065820833746
Loss in iteration 167 : 2.2570895857237825
Loss in iteration 168 : 4.986029944324988
Loss in iteration 169 : 8.81649793319978
Loss in iteration 170 : 7.146803688580861
Loss in iteration 171 : 4.049547787429903
Loss in iteration 172 : 3.4729690748022906
Loss in iteration 173 : 4.632035895855249
Loss in iteration 174 : 5.896507185953842
Loss in iteration 175 : 6.50147186239898
Loss in iteration 176 : 6.248537861543415
Loss in iteration 177 : 5.417395103759678
Loss in iteration 178 : 4.510580831245515
Loss in iteration 179 : 3.982255716501026
Loss in iteration 180 : 4.213032121103427
Loss in iteration 181 : 4.908746584566583
Loss in iteration 182 : 5.214713622673794
Loss in iteration 183 : 4.782995309595329
Loss in iteration 184 : 4.03961693232547
Loss in iteration 185 : 3.649895617429579
Loss in iteration 186 : 3.751234224543311
Loss in iteration 187 : 3.980069810038935
Loss in iteration 188 : 4.056828981128105
Loss in iteration 189 : 3.864614031998085
Loss in iteration 190 : 3.4564538985512145
Loss in iteration 191 : 3.0214261591082745
Loss in iteration 192 : 2.801630072346818
Loss in iteration 193 : 2.903930948088516
Loss in iteration 194 : 2.952341018059618
Loss in iteration 195 : 2.628584822399689
Loss in iteration 196 : 2.201921888318065
Loss in iteration 197 : 2.0610458194533865
Loss in iteration 198 : 2.093982762513094
Loss in iteration 199 : 2.0145279916025833
Loss in iteration 200 : 1.7005536554481973
Testing accuracy  of updater 9 on alg 0 with rate 0.19999999999999998 = 0.796, training accuracy 0.8420200712204597, time elapsed: 2550 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 2.85033521911561
Loss in iteration 3 : 3.030536390227881
Loss in iteration 4 : 0.989027706304096
Loss in iteration 5 : 1.970514424739268
Loss in iteration 6 : 1.287667064962069
Loss in iteration 7 : 0.8842163067892783
Loss in iteration 8 : 1.5387226321847225
Loss in iteration 9 : 1.7140193105135642
Loss in iteration 10 : 1.3848815877756544
Loss in iteration 11 : 1.1248250222837644
Loss in iteration 12 : 1.3822450611267134
Loss in iteration 13 : 1.6323502027554
Loss in iteration 14 : 1.3724030312775348
Loss in iteration 15 : 1.1853186061640548
Loss in iteration 16 : 1.3152424916329597
Loss in iteration 17 : 1.4218493420892262
Loss in iteration 18 : 1.3171494710444072
Loss in iteration 19 : 1.096223311405132
Loss in iteration 20 : 1.0311802179265561
Loss in iteration 21 : 1.1425350982443974
Loss in iteration 22 : 0.9961248299684635
Loss in iteration 23 : 0.8001512531130653
Loss in iteration 24 : 0.8426193469252194
Loss in iteration 25 : 0.816060991528294
Loss in iteration 26 : 0.599053996749996
Loss in iteration 27 : 0.5976927538346231
Loss in iteration 28 : 0.5351615092613865
Loss in iteration 29 : 0.4880966738240189
Loss in iteration 30 : 0.5208827532585883
Loss in iteration 31 : 0.795167897557019
Loss in iteration 32 : 2.017032409673213
Loss in iteration 33 : 2.258966159341468
Loss in iteration 34 : 0.5952104925886665
Loss in iteration 35 : 2.344047226498648
Loss in iteration 36 : 0.5605523702200957
Loss in iteration 37 : 1.205218196452456
Loss in iteration 38 : 1.462164866126598
Loss in iteration 39 : 1.1092955721255766
Loss in iteration 40 : 0.8989741576678092
Loss in iteration 41 : 1.2561549098809477
Loss in iteration 42 : 1.3623845601982665
Loss in iteration 43 : 1.051579886037613
Loss in iteration 44 : 1.0449991318884762
Loss in iteration 45 : 1.2176964271828712
Loss in iteration 46 : 1.2334661084239111
Loss in iteration 47 : 1.056570626745423
Loss in iteration 48 : 0.9095981441074634
Loss in iteration 49 : 1.0051865482780482
Loss in iteration 50 : 1.0025871630597316
Loss in iteration 51 : 0.7801732253578111
Loss in iteration 52 : 0.7653933003850364
Loss in iteration 53 : 0.8103081609857479
Loss in iteration 54 : 0.6715408248416641
Loss in iteration 55 : 0.5361613100010276
Loss in iteration 56 : 0.6623099319506068
Loss in iteration 57 : 0.42686500424364565
Loss in iteration 58 : 0.552240606305471
Loss in iteration 59 : 0.40567339030465294
Loss in iteration 60 : 0.9350351872662511
Loss in iteration 61 : 2.2641331668833438
Loss in iteration 62 : 2.9421408462058563
Loss in iteration 63 : 1.5069141693849115
Loss in iteration 64 : 0.8036369911907604
Loss in iteration 65 : 2.2249048670538762
Loss in iteration 66 : 0.7198380718139061
Loss in iteration 67 : 1.1781796059194634
Loss in iteration 68 : 1.6895657624306013
Loss in iteration 69 : 1.5361065119176416
Loss in iteration 70 : 1.1015660699016285
Loss in iteration 71 : 1.037511962129042
Loss in iteration 72 : 1.449952916122259
Loss in iteration 73 : 1.4212650010641936
Loss in iteration 74 : 1.062973170801371
Loss in iteration 75 : 1.0795232509945718
Loss in iteration 76 : 1.2600664486298305
Loss in iteration 77 : 1.2704190382898708
Loss in iteration 78 : 1.067028246913172
Loss in iteration 79 : 0.8847970086442828
Loss in iteration 80 : 0.9831643859843765
Loss in iteration 81 : 1.0157676368728086
Loss in iteration 82 : 0.7561802342243495
Loss in iteration 83 : 0.7340299371602766
Loss in iteration 84 : 0.8094290935619575
Loss in iteration 85 : 0.6740307020167771
Loss in iteration 86 : 0.5097630635100799
Loss in iteration 87 : 0.6933943880132233
Loss in iteration 88 : 0.42018801095592273
Loss in iteration 89 : 0.5815249228295729
Loss in iteration 90 : 0.4249142872056639
Loss in iteration 91 : 0.9256435005798732
Loss in iteration 92 : 1.9095770477519605
Loss in iteration 93 : 2.40707901174399
Loss in iteration 94 : 1.031908675213485
Loss in iteration 95 : 0.9799792228751599
Loss in iteration 96 : 1.5860590471754017
Loss in iteration 97 : 0.6561221710668379
Loss in iteration 98 : 1.1210536142901364
Loss in iteration 99 : 1.4224218387807896
Loss in iteration 100 : 1.189052507916046
Loss in iteration 101 : 0.8784561886109399
Loss in iteration 102 : 1.0290796339935966
Loss in iteration 103 : 1.2885251710381413
Loss in iteration 104 : 1.0293391647060568
Loss in iteration 105 : 0.8779643235752304
Loss in iteration 106 : 1.0229478877592313
Loss in iteration 107 : 1.0896309976961747
Loss in iteration 108 : 0.9395980931158598
Loss in iteration 109 : 0.7582330302564735
Loss in iteration 110 : 0.8311759660932553
Loss in iteration 111 : 0.8590707294948347
Loss in iteration 112 : 0.6272262731950192
Loss in iteration 113 : 0.6526330232139298
Loss in iteration 114 : 0.6937613030548866
Loss in iteration 115 : 0.5184604024998285
Loss in iteration 116 : 0.5206716610664736
Loss in iteration 117 : 0.5005657275271094
Loss in iteration 118 : 0.4934678414594169
Loss in iteration 119 : 0.5306734609452619
Loss in iteration 120 : 0.5365019275148757
Loss in iteration 121 : 0.5049475759555678
Loss in iteration 122 : 0.40464965566705846
Loss in iteration 123 : 0.6256558259939407
Loss in iteration 124 : 0.6820962616454845
Loss in iteration 125 : 0.6432949328734625
Loss in iteration 126 : 0.44756573332288574
Loss in iteration 127 : 0.6862413282620776
Loss in iteration 128 : 0.46240927201691734
Loss in iteration 129 : 0.6554781884086162
Loss in iteration 130 : 0.5608933836446488
Loss in iteration 131 : 0.4848634310282387
Loss in iteration 132 : 0.6325353003987915
Loss in iteration 133 : 0.46213587400397227
Loss in iteration 134 : 0.5342331983372113
Loss in iteration 135 : 0.5279313359606608
Loss in iteration 136 : 0.41596363301401584
Loss in iteration 137 : 0.52603501400573
Loss in iteration 138 : 0.3800763617580554
Loss in iteration 139 : 0.46183267486453233
Loss in iteration 140 : 0.3605850848292383
Loss in iteration 141 : 0.5102187373679721
Loss in iteration 142 : 0.5234115030622091
Loss in iteration 143 : 0.43577170780556285
Loss in iteration 144 : 0.6862069954378289
Loss in iteration 145 : 0.7642539319921948
Loss in iteration 146 : 0.7308266197700987
Loss in iteration 147 : 0.4265592402668191
Loss in iteration 148 : 0.8057298001879258
Loss in iteration 149 : 0.4682809402742197
Loss in iteration 150 : 0.7376693324048652
Loss in iteration 151 : 0.6429961435517064
Loss in iteration 152 : 0.49599432190121434
Loss in iteration 153 : 0.7163875347452339
Loss in iteration 154 : 0.5200283605580212
Loss in iteration 155 : 0.5490026648456559
Loss in iteration 156 : 0.6198378251820137
Loss in iteration 157 : 0.4715170546038159
Loss in iteration 158 : 0.5080128153776085
Loss in iteration 159 : 0.4713247769432021
Loss in iteration 160 : 0.4273092106278461
Loss in iteration 161 : 0.4801023919563673
Loss in iteration 162 : 0.34898273952720166
Loss in iteration 163 : 0.5067835263460273
Loss in iteration 164 : 0.6556020123949561
Loss in iteration 165 : 0.504733993227951
Loss in iteration 166 : 0.8658225597772103
Loss in iteration 167 : 0.9379818834939637
Loss in iteration 168 : 1.0662349884433937
Loss in iteration 169 : 0.4506524424732071
Loss in iteration 170 : 1.012197222800237
Loss in iteration 171 : 0.6656495275202124
Loss in iteration 172 : 0.645510579958793
Loss in iteration 173 : 0.8962350317301946
Loss in iteration 174 : 0.8324819117233795
Loss in iteration 175 : 0.6387148828271677
Loss in iteration 176 : 0.7481305305324856
Loss in iteration 177 : 0.8154632177603184
Loss in iteration 178 : 0.6079738401901882
Loss in iteration 179 : 0.6608153928707634
Loss in iteration 180 : 0.7144248591157838
Loss in iteration 181 : 0.575141694984474
Loss in iteration 182 : 0.5108751072084993
Loss in iteration 183 : 0.6049234461755149
Loss in iteration 184 : 0.4157613117431773
Loss in iteration 185 : 0.5050797789339445
Loss in iteration 186 : 0.4222504223282972
Loss in iteration 187 : 0.4586336563974098
Loss in iteration 188 : 0.3437494945083568
Loss in iteration 189 : 0.4147022423277219
Loss in iteration 190 : 0.4110840002767591
Loss in iteration 191 : 0.4314244750840083
Loss in iteration 192 : 0.3409558579859932
Loss in iteration 193 : 0.39959660304473155
Loss in iteration 194 : 0.4595006844347381
Loss in iteration 195 : 0.3658928913818044
Loss in iteration 196 : 0.47271535677456555
Loss in iteration 197 : 0.36094810047575077
Loss in iteration 198 : 0.4300430687766972
Loss in iteration 199 : 0.36443007624577184
Loss in iteration 200 : 0.42869899232113773
Testing accuracy  of updater 9 on alg 0 with rate 0.13999999999999999 = 0.7955, training accuracy 0.8452573648429913, time elapsed: 2007 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 1.7201230137838754
Loss in iteration 3 : 2.2115499074169924
Loss in iteration 4 : 1.6283123128056989
Loss in iteration 5 : 0.6090761623250028
Loss in iteration 6 : 0.7924676731050395
Loss in iteration 7 : 1.4861078669403864
Loss in iteration 8 : 1.08133832207743
Loss in iteration 9 : 0.7281931305179618
Loss in iteration 10 : 0.8892129165212825
Loss in iteration 11 : 1.1460180400487305
Loss in iteration 12 : 1.2514444016606476
Loss in iteration 13 : 1.1679750993205997
Loss in iteration 14 : 0.9970976307644231
Loss in iteration 15 : 0.8886396989823554
Loss in iteration 16 : 0.9482883836873356
Loss in iteration 17 : 1.0841286129448315
Loss in iteration 18 : 1.0822831271997688
Loss in iteration 19 : 0.9403200158707588
Loss in iteration 20 : 0.842175294366818
Loss in iteration 21 : 0.8594981109095634
Loss in iteration 22 : 0.9081397642266418
Loss in iteration 23 : 0.9018054402311495
Loss in iteration 24 : 0.8197993502617287
Loss in iteration 25 : 0.714492582178927
Loss in iteration 26 : 0.6722411587702974
Loss in iteration 27 : 0.7031580101628969
Loss in iteration 28 : 0.6784758229292632
Loss in iteration 29 : 0.5688201601291045
Loss in iteration 30 : 0.5178948528980194
Loss in iteration 31 : 0.5346239631862462
Loss in iteration 32 : 0.508374114195254
Loss in iteration 33 : 0.41694095246992857
Loss in iteration 34 : 0.39458685283651385
Loss in iteration 35 : 0.42859098329863454
Loss in iteration 36 : 0.33400753606167466
Loss in iteration 37 : 0.3976481252850763
Loss in iteration 38 : 0.3639461964677982
Loss in iteration 39 : 0.43836704191524134
Loss in iteration 40 : 0.37703548111186747
Loss in iteration 41 : 0.3921698382227375
Loss in iteration 42 : 0.3648773881202254
Loss in iteration 43 : 0.346905762260973
Loss in iteration 44 : 0.349161233058641
Loss in iteration 45 : 0.34436461724053996
Loss in iteration 46 : 0.33480878413083476
Loss in iteration 47 : 0.34874622372670827
Loss in iteration 48 : 0.33440440967503365
Loss in iteration 49 : 0.3461724105129659
Loss in iteration 50 : 0.3427975730831072
Loss in iteration 51 : 0.3379893914611159
Loss in iteration 52 : 0.34562956061977984
Loss in iteration 53 : 0.3361403024536022
Loss in iteration 54 : 0.3378543597520404
Loss in iteration 55 : 0.33718210550288286
Loss in iteration 56 : 0.33001201998325963
Loss in iteration 57 : 0.33441766577369764
Loss in iteration 58 : 0.3276213048550491
Loss in iteration 59 : 0.3302194196877708
Loss in iteration 60 : 0.3282999539424953
Loss in iteration 61 : 0.3278323085505842
Loss in iteration 62 : 0.32859112753264896
Loss in iteration 63 : 0.3269403674643374
Loss in iteration 64 : 0.32810329754283407
Loss in iteration 65 : 0.3255480418369507
Loss in iteration 66 : 0.327256404322823
Loss in iteration 67 : 0.32485000407264036
Loss in iteration 68 : 0.3266035544377557
Loss in iteration 69 : 0.32450173971042817
Loss in iteration 70 : 0.32623197071063137
Loss in iteration 71 : 0.3244051908382661
Loss in iteration 72 : 0.32541038110190185
Loss in iteration 73 : 0.3243620757266579
Loss in iteration 74 : 0.3244462665715494
Loss in iteration 75 : 0.32429146998902575
Loss in iteration 76 : 0.32374430193852266
Loss in iteration 77 : 0.3242151892824471
Loss in iteration 78 : 0.3234553800420627
Loss in iteration 79 : 0.32410763116019614
Loss in iteration 80 : 0.32349874434774956
Loss in iteration 81 : 0.3239234707320574
Loss in iteration 82 : 0.3236162336513249
Loss in iteration 83 : 0.3236838739861323
Loss in iteration 84 : 0.3235901723827767
Loss in iteration 85 : 0.32339817424548445
Loss in iteration 86 : 0.32344318636940306
Loss in iteration 87 : 0.3231340499616428
Loss in iteration 88 : 0.32329689070771656
Loss in iteration 89 : 0.3230216692038761
Loss in iteration 90 : 0.323240320146108
Loss in iteration 91 : 0.32305627744009224
Loss in iteration 92 : 0.3232542008484827
Loss in iteration 93 : 0.32313158088178806
Loss in iteration 94 : 0.32322996910924545
Loss in iteration 95 : 0.32315034033859463
Loss in iteration 96 : 0.3231374772583584
Loss in iteration 97 : 0.3231020649326138
Loss in iteration 98 : 0.32303021567250934
Loss in iteration 99 : 0.32304790024689506
Loss in iteration 100 : 0.32297021308448587
Loss in iteration 101 : 0.3230302308879702
Loss in iteration 102 : 0.3229726643112267
Loss in iteration 103 : 0.32303973544517894
Loss in iteration 104 : 0.32300162092599627
Testing accuracy  of updater 9 on alg 0 with rate 0.08 = 0.791, training accuracy 0.8439624473939786, time elapsed: 973 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5306893858674613
Loss in iteration 3 : 0.6584140669675012
Loss in iteration 4 : 0.6254335887556823
Loss in iteration 5 : 0.46352399090139546
Loss in iteration 6 : 0.33667222273405584
Loss in iteration 7 : 0.3892753055287278
Loss in iteration 8 : 0.4807764244496328
Loss in iteration 9 : 0.44040451395894453
Loss in iteration 10 : 0.3644511223373895
Loss in iteration 11 : 0.3534449278050637
Loss in iteration 12 : 0.3896131473326521
Loss in iteration 13 : 0.4248966321605574
Loss in iteration 14 : 0.433484698831617
Loss in iteration 15 : 0.41634665017758593
Loss in iteration 16 : 0.39121271309602657
Loss in iteration 17 : 0.378479232208432
Loss in iteration 18 : 0.38687131449184015
Loss in iteration 19 : 0.4054709381017003
Loss in iteration 20 : 0.41368597053501616
Loss in iteration 21 : 0.403400060420575
Loss in iteration 22 : 0.385903950966669
Loss in iteration 23 : 0.37627412215789896
Loss in iteration 24 : 0.3781606761246017
Loss in iteration 25 : 0.38438554295581084
Loss in iteration 26 : 0.3859945202377496
Loss in iteration 27 : 0.3792983652113686
Loss in iteration 28 : 0.36761006293464066
Loss in iteration 29 : 0.3581037288993748
Loss in iteration 30 : 0.35568522302647576
Loss in iteration 31 : 0.3580421638439088
Loss in iteration 32 : 0.35772467475176734
Loss in iteration 33 : 0.35098859907184765
Loss in iteration 34 : 0.34232407154107075
Loss in iteration 35 : 0.3380652569338701
Loss in iteration 36 : 0.3386755453547636
Loss in iteration 37 : 0.3394546547289824
Loss in iteration 38 : 0.3366551419181419
Loss in iteration 39 : 0.331480675805268
Loss in iteration 40 : 0.3283888184544432
Loss in iteration 41 : 0.32941781924026714
Loss in iteration 42 : 0.33114548230220786
Loss in iteration 43 : 0.329719701186967
Loss in iteration 44 : 0.32692636156136273
Loss in iteration 45 : 0.32668046128396694
Loss in iteration 46 : 0.32861270963545774
Loss in iteration 47 : 0.3294209880156248
Loss in iteration 48 : 0.3281121078774571
Loss in iteration 49 : 0.3270173120234749
Loss in iteration 50 : 0.3278377944751216
Loss in iteration 51 : 0.3289069435736206
Loss in iteration 52 : 0.3282718290768529
Loss in iteration 53 : 0.32704090888431353
Loss in iteration 54 : 0.3269590647499469
Loss in iteration 55 : 0.32749456513645403
Loss in iteration 56 : 0.32725539831151457
Loss in iteration 57 : 0.3262937264995492
Loss in iteration 58 : 0.325740337191235
Loss in iteration 59 : 0.3259151614884165
Loss in iteration 60 : 0.3259843696056852
Loss in iteration 61 : 0.32548035875847026
Loss in iteration 62 : 0.32495941622029967
Loss in iteration 63 : 0.32492235339255887
Loss in iteration 64 : 0.3250896098355269
Loss in iteration 65 : 0.3249932437636414
Loss in iteration 66 : 0.3246738305559222
Loss in iteration 67 : 0.3245071502569647
Loss in iteration 68 : 0.32460060541561775
Loss in iteration 69 : 0.3246826025672588
Loss in iteration 70 : 0.3245604976662543
Loss in iteration 71 : 0.32438001073372097
Loss in iteration 72 : 0.3243429572996812
Loss in iteration 73 : 0.3244079152448926
Loss in iteration 74 : 0.3243996625354297
Loss in iteration 75 : 0.324278416892163
Loss in iteration 76 : 0.3241677412591631
Loss in iteration 77 : 0.32415283425926955
Loss in iteration 78 : 0.3241676781943275
Loss in iteration 79 : 0.3241156991138756
Loss in iteration 80 : 0.32401427861618537
Loss in iteration 81 : 0.3239495959397214
Loss in iteration 82 : 0.32394136941052437
Loss in iteration 83 : 0.32392857996052116
Loss in iteration 84 : 0.3238715675702467
Loss in iteration 85 : 0.3238042379661011
Loss in iteration 86 : 0.32377280428847616
Loss in iteration 87 : 0.3237673376062809
Loss in iteration 88 : 0.32374552111461413
Loss in iteration 89 : 0.32369956057109844
Loss in iteration 90 : 0.32366098381726466
Loss in iteration 91 : 0.3236470998348515
Loss in iteration 92 : 0.3236382755576947
Loss in iteration 93 : 0.3236130229647411
Loss in iteration 94 : 0.32357908666464297
Loss in iteration 95 : 0.32355659771533063
Loss in iteration 96 : 0.3235463384623524
Loss in iteration 97 : 0.32353193268110964
Loss in iteration 98 : 0.3235069193538092
Loss in iteration 99 : 0.3234823694327215
Loss in iteration 100 : 0.32346731672323004
Loss in iteration 101 : 0.3234559608828525
Loss in iteration 102 : 0.3234390249082148
Loss in iteration 103 : 0.3234179589609465
Loss in iteration 104 : 0.32340081605715876
Loss in iteration 105 : 0.3233892711833848
Loss in iteration 106 : 0.32337735815266955
Loss in iteration 107 : 0.3233616328898206
Loss in iteration 108 : 0.32334572172692955
Loss in iteration 109 : 0.32333362367989965
Loss in iteration 110 : 0.32332375619975684
Loss in iteration 111 : 0.3233122966540523
Loss in iteration 112 : 0.3232991286173263
Loss in iteration 113 : 0.3232872543436509
Loss in iteration 114 : 0.32327778396874524
Loss in iteration 115 : 0.3232686305246524
Loss in iteration 116 : 0.3232581262683125
Loss in iteration 117 : 0.32324735236959473
Loss in iteration 118 : 0.32323801337746777
Loss in iteration 119 : 0.3232298140693766
Loss in iteration 120 : 0.32322126111639127
Loss in iteration 121 : 0.32321205862990254
Loss in iteration 122 : 0.32320330584559875
Loss in iteration 123 : 0.3231956262508391
Loss in iteration 124 : 0.32318834214056386
Loss in iteration 125 : 0.32318070483095934
Loss in iteration 126 : 0.32317299757100826
Loss in iteration 127 : 0.32316591915656023
Loss in iteration 128 : 0.32315947468324613
Loss in iteration 129 : 0.3231530898352943
Loss in iteration 130 : 0.3231465488740798
Loss in iteration 131 : 0.32314022990299096
Loss in iteration 132 : 0.32313442648511437
Loss in iteration 133 : 0.3231289197451426
Loss in iteration 134 : 0.3231233819057962
Loss in iteration 135 : 0.32311786463145237
Loss in iteration 136 : 0.32311263917865307
Loss in iteration 137 : 0.3231077450717118
Loss in iteration 138 : 0.3231029653382755
Loss in iteration 139 : 0.3230981840187172
Loss in iteration 140 : 0.32309352891515203
Loss in iteration 141 : 0.3230891303751994
Loss in iteration 142 : 0.32308492248886594
Loss in iteration 143 : 0.323080771091558
Loss in iteration 144 : 0.32307667884825375
Loss in iteration 145 : 0.3230727506104931
Loss in iteration 146 : 0.323069015009069
Loss in iteration 147 : 0.32306539218866365
Loss in iteration 148 : 0.3230618274939689
Loss in iteration 149 : 0.3230583627149846
Loss in iteration 150 : 0.3230550516349261
Loss in iteration 151 : 0.3230518736699457
Loss in iteration 152 : 0.32304877339517907
Loss in iteration 153 : 0.32304574400240865
Loss in iteration 154 : 0.323042823177285
Loss in iteration 155 : 0.32304002401709303
Loss in iteration 156 : 0.3230373153797674
Loss in iteration 157 : 0.32303467141464054
Loss in iteration 158 : 0.32303210405544197
Loss in iteration 159 : 0.323029633692831
Loss in iteration 160 : 0.3230272530620026
Loss in iteration 161 : 0.32302493907441215
Loss in iteration 162 : 0.3230226860745158
Loss in iteration 163 : 0.32302050722679604
Loss in iteration 164 : 0.32301840796373005
Loss in iteration 165 : 0.3230163760938285
Loss in iteration 166 : 0.3230144000706032
Loss in iteration 167 : 0.32301248300717794
Loss in iteration 168 : 0.32301063243264555
Loss in iteration 169 : 0.32300884553126935
Loss in iteration 170 : 0.32300711260641507
Loss in iteration 171 : 0.3230054301858687
Loss in iteration 172 : 0.32300380247238314
Loss in iteration 173 : 0.32300223116659876
Loss in iteration 174 : 0.3230007109766466
Loss in iteration 175 : 0.32299923641934025
Loss in iteration 176 : 0.32299780767006075
Loss in iteration 177 : 0.3229964269374602
Loss in iteration 178 : 0.32299509254500514
Loss in iteration 179 : 0.3229937999692242
Loss in iteration 180 : 0.3229925469845744
Loss in iteration 181 : 0.3229913344769298
Loss in iteration 182 : 0.32299016252938445
Loss in iteration 183 : 0.32298902851035954
Loss in iteration 184 : 0.32298792964084816
Loss in iteration 185 : 0.3229868653654471
Loss in iteration 186 : 0.32298583602226055
Loss in iteration 187 : 0.3229848404924115
Loss in iteration 188 : 0.3229838765311088
Testing accuracy  of updater 9 on alg 0 with rate 0.02 = 0.78975, training accuracy 0.8442861767562317, time elapsed: 1933 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4878677533622165
Loss in iteration 3 : 0.5792519958118347
Loss in iteration 4 : 0.5908699049036794
Loss in iteration 5 : 0.501341603042175
Loss in iteration 6 : 0.38033874859518974
Loss in iteration 7 : 0.33810070883671955
Loss in iteration 8 : 0.39694506424612264
Loss in iteration 9 : 0.43778388726370177
Loss in iteration 10 : 0.4003126604480564
Loss in iteration 11 : 0.3501955720505743
Loss in iteration 12 : 0.33954226366424317
Loss in iteration 13 : 0.3603254194495455
Loss in iteration 14 : 0.3850102347814579
Loss in iteration 15 : 0.3955042365068199
Loss in iteration 16 : 0.38878055726328087
Loss in iteration 17 : 0.37290257716582825
Loss in iteration 18 : 0.35993754224427227
Loss in iteration 19 : 0.3583642413165339
Loss in iteration 20 : 0.36752550513117915
Loss in iteration 21 : 0.37819579562260486
Loss in iteration 22 : 0.380725555778032
Loss in iteration 23 : 0.37360255608287213
Loss in iteration 24 : 0.3634316957170303
Loss in iteration 25 : 0.35752813615031515
Loss in iteration 26 : 0.3579308747057073
Loss in iteration 27 : 0.36149198535300475
Loss in iteration 28 : 0.3635731295146706
Loss in iteration 29 : 0.3614556910448352
Loss in iteration 30 : 0.355687570662439
Loss in iteration 31 : 0.34927388461906883
Loss in iteration 32 : 0.34540904807346656
Loss in iteration 33 : 0.3450559481538701
Loss in iteration 34 : 0.3461610631933751
Loss in iteration 35 : 0.3455667387612593
Loss in iteration 36 : 0.34210186530819875
Loss in iteration 37 : 0.3375694988844972
Loss in iteration 38 : 0.3346017599730745
Loss in iteration 39 : 0.33403254109977465
Loss in iteration 40 : 0.33449956053658964
Loss in iteration 41 : 0.33409913818265213
Loss in iteration 42 : 0.3321660009230802
Loss in iteration 43 : 0.3297039934750125
Loss in iteration 44 : 0.3282872755482767
Loss in iteration 45 : 0.3284440886603107
Loss in iteration 46 : 0.32911662135887076
Loss in iteration 47 : 0.32891690150756236
Loss in iteration 48 : 0.32775897045563507
Loss in iteration 49 : 0.3267843748046623
Loss in iteration 50 : 0.32681470415589137
Loss in iteration 51 : 0.32745835156490893
Loss in iteration 52 : 0.32777074340820067
Loss in iteration 53 : 0.3273799506028449
Loss in iteration 54 : 0.32678385478679656
Loss in iteration 55 : 0.32664970763766676
Loss in iteration 56 : 0.3270030124992135
Loss in iteration 57 : 0.32726632164384645
Loss in iteration 58 : 0.32704863424896374
Loss in iteration 59 : 0.32659953961081434
Loss in iteration 60 : 0.3263817144258699
Loss in iteration 61 : 0.3264705379059153
Loss in iteration 62 : 0.32655563249880415
Loss in iteration 63 : 0.32638844244164195
Loss in iteration 64 : 0.32605613722235477
Loss in iteration 65 : 0.32581930256456576
Loss in iteration 66 : 0.3257825825531103
Loss in iteration 67 : 0.32580243899118994
Loss in iteration 68 : 0.3257031341859715
Loss in iteration 69 : 0.32549229146126
Loss in iteration 70 : 0.32531565793160333
Loss in iteration 71 : 0.32525876811133164
Loss in iteration 72 : 0.325262730846469
Loss in iteration 73 : 0.3252220294212288
Loss in iteration 74 : 0.32511103396391844
Loss in iteration 75 : 0.32499486149708906
Loss in iteration 76 : 0.3249382959077176
Loss in iteration 77 : 0.3249325263599781
Loss in iteration 78 : 0.3249177461224205
Loss in iteration 79 : 0.32485930884420705
Loss in iteration 80 : 0.32478118567890624
Loss in iteration 81 : 0.32472673424253384
Loss in iteration 82 : 0.32470590456939935
Loss in iteration 83 : 0.3246907092847047
Loss in iteration 84 : 0.32465339967334095
Loss in iteration 85 : 0.3245966039543299
Loss in iteration 86 : 0.32454448329588775
Loss in iteration 87 : 0.3245120776366717
Loss in iteration 88 : 0.3244903256477949
Loss in iteration 89 : 0.3244605581494099
Loss in iteration 90 : 0.3244171679435203
Loss in iteration 91 : 0.3243715210956318
Loss in iteration 92 : 0.32433612721386235
Loss in iteration 93 : 0.32431057072318836
Loss in iteration 94 : 0.32428453166961135
Loss in iteration 95 : 0.32425116591685493
Loss in iteration 96 : 0.3242140627767345
Loss in iteration 97 : 0.32418133457150633
Loss in iteration 98 : 0.32415574011855897
Loss in iteration 99 : 0.3241325461097323
Loss in iteration 100 : 0.32410617662493296
Loss in iteration 101 : 0.3240766110519662
Loss in iteration 102 : 0.3240483307339436
Loss in iteration 103 : 0.3240243730076319
Loss in iteration 104 : 0.32400313554345334
Loss in iteration 105 : 0.3239810084756892
Loss in iteration 106 : 0.32395676240855353
Loss in iteration 107 : 0.323932439771876
Loss in iteration 108 : 0.32391040564469603
Loss in iteration 109 : 0.32389058279806565
Loss in iteration 110 : 0.3238709606306844
Loss in iteration 111 : 0.3238502092864747
Loss in iteration 112 : 0.3238290426729914
Loss in iteration 113 : 0.32380900815072083
Loss in iteration 114 : 0.32379056163066194
Loss in iteration 115 : 0.3237727494821513
Loss in iteration 116 : 0.3237545490231895
Loss in iteration 117 : 0.32373603266960155
Loss in iteration 118 : 0.32371807296065697
Loss in iteration 119 : 0.3237011969567338
Loss in iteration 120 : 0.3236850464877153
Loss in iteration 121 : 0.323668944289084
Loss in iteration 122 : 0.32365271652840033
Loss in iteration 123 : 0.3236367870944521
Loss in iteration 124 : 0.32362157335775804
Loss in iteration 125 : 0.3236070037668522
Loss in iteration 126 : 0.32359268236030075
Loss in iteration 127 : 0.3235783896999307
Loss in iteration 128 : 0.32356428803046605
Loss in iteration 129 : 0.3235506544689993
Loss in iteration 130 : 0.32353753428234455
Loss in iteration 131 : 0.3235247232502261
Loss in iteration 132 : 0.32351203760342
Loss in iteration 133 : 0.3234995103306053
Loss in iteration 134 : 0.32348730410375504
Loss in iteration 135 : 0.3234754935710713
Loss in iteration 136 : 0.32346398967404927
Loss in iteration 137 : 0.3234526650611107
Loss in iteration 138 : 0.3234414996769232
Loss in iteration 139 : 0.3234305770887942
Loss in iteration 140 : 0.3234199638767856
Loss in iteration 141 : 0.32340963133797174
Loss in iteration 142 : 0.3233995007237249
Loss in iteration 143 : 0.32338953720826596
Loss in iteration 144 : 0.3233797761559088
Loss in iteration 145 : 0.32337026439484284
Loss in iteration 146 : 0.3233609995511963
Loss in iteration 147 : 0.3233519370524644
Loss in iteration 148 : 0.3233430440011418
Loss in iteration 149 : 0.32333432956148983
Loss in iteration 150 : 0.3233258215953978
Loss in iteration 151 : 0.32331752635997985
Loss in iteration 152 : 0.32330942053320394
Loss in iteration 153 : 0.32330147864200093
Loss in iteration 154 : 0.323293697866965
Loss in iteration 155 : 0.32328609231740935
Loss in iteration 156 : 0.3232786690876101
Loss in iteration 157 : 0.3232714169721783
Loss in iteration 158 : 0.3232643183935501
Loss in iteration 159 : 0.3232573664986704
Loss in iteration 160 : 0.3232505668527798
Loss in iteration 161 : 0.3232439246867418
Loss in iteration 162 : 0.32323743504507035
Loss in iteration 163 : 0.3232310865982879
Loss in iteration 164 : 0.3232248721704552
Loss in iteration 165 : 0.323218792615499
Loss in iteration 166 : 0.32321285082928686
Loss in iteration 167 : 0.32320704463337274
Loss in iteration 168 : 0.3232013669221006
Loss in iteration 169 : 0.3231958115290327
Loss in iteration 170 : 0.3231903769783424
Loss in iteration 171 : 0.3231850641958487
Loss in iteration 172 : 0.3231798719481605
Loss in iteration 173 : 0.3231747956888042
Loss in iteration 174 : 0.3231698304987337
Loss in iteration 175 : 0.3231649739587901
Loss in iteration 176 : 0.3231602256516762
Loss in iteration 177 : 0.3231555845013588
Loss in iteration 178 : 0.3231510474277188
Loss in iteration 179 : 0.3231466106218973
Loss in iteration 180 : 0.32314227147563457
Loss in iteration 181 : 0.32313802881236126
Loss in iteration 182 : 0.3231338814725833
Loss in iteration 183 : 0.323129827198818
Loss in iteration 184 : 0.32312586305356217
Loss in iteration 185 : 0.3231219865981906
Loss in iteration 186 : 0.32311819632449285
Loss in iteration 187 : 0.32311449097899614
Loss in iteration 188 : 0.32311086877028333
Loss in iteration 189 : 0.3231073273988993
Loss in iteration 190 : 0.3231038647192069
Loss in iteration 191 : 0.3231004791455536
Loss in iteration 192 : 0.3230971693784303
Loss in iteration 193 : 0.32309393389506125
Loss in iteration 194 : 0.3230907708388691
Loss in iteration 195 : 0.32308767836000163
Loss in iteration 196 : 0.32308465492526606
Loss in iteration 197 : 0.32308169924182323
Loss in iteration 198 : 0.32307880995484917
Loss in iteration 199 : 0.32307598551226274
Loss in iteration 200 : 0.3230732243214612
Testing accuracy  of updater 9 on alg 0 with rate 0.014 = 0.78975, training accuracy 0.8433149886694723, time elapsed: 2049 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.4813741741639844
Loss in iteration 3 : 0.5022202616034025
Loss in iteration 4 : 0.5414010097788108
Loss in iteration 5 : 0.5317343872315082
Loss in iteration 6 : 0.4739936914603424
Loss in iteration 7 : 0.3983186649630815
Loss in iteration 8 : 0.3490043201800967
Loss in iteration 9 : 0.35245874938485866
Loss in iteration 10 : 0.38535492459282833
Loss in iteration 11 : 0.3994688874256281
Loss in iteration 12 : 0.3799342177747113
Loss in iteration 13 : 0.3499344813286629
Loss in iteration 14 : 0.33318592966984834
Loss in iteration 15 : 0.33450257793810145
Loss in iteration 16 : 0.3458168741416214
Loss in iteration 17 : 0.3571061757305865
Loss in iteration 18 : 0.3621657901358962
Loss in iteration 19 : 0.35976870092180235
Loss in iteration 20 : 0.35252198049575656
Loss in iteration 21 : 0.3447942026257614
Loss in iteration 22 : 0.3404673564040198
Loss in iteration 23 : 0.34114248186837975
Loss in iteration 24 : 0.34549604482352214
Loss in iteration 25 : 0.35025624814668205
Loss in iteration 26 : 0.35235211076656436
Loss in iteration 27 : 0.3507807800839693
Loss in iteration 28 : 0.3468756792926719
Loss in iteration 29 : 0.3430109939700368
Loss in iteration 30 : 0.3409570974056158
Loss in iteration 31 : 0.3410402275488734
Loss in iteration 32 : 0.34235428860031536
Loss in iteration 33 : 0.34354928502974535
Loss in iteration 34 : 0.34360872038476625
Loss in iteration 35 : 0.3422722146967497
Loss in iteration 36 : 0.3400251189352023
Loss in iteration 37 : 0.33774899138743836
Loss in iteration 38 : 0.33622439082019995
Loss in iteration 39 : 0.33572390325145607
Loss in iteration 40 : 0.33590603470899516
Loss in iteration 41 : 0.3360773187380068
Loss in iteration 42 : 0.33566068250435493
Loss in iteration 43 : 0.33455098135895434
Loss in iteration 44 : 0.33311819065382464
Loss in iteration 45 : 0.33189458103213687
Loss in iteration 46 : 0.33120910640632506
Loss in iteration 47 : 0.3310221949996028
Loss in iteration 48 : 0.33102300065065887
Loss in iteration 49 : 0.3308700468111456
Loss in iteration 50 : 0.3304022002020837
Loss in iteration 51 : 0.32970541519911645
Loss in iteration 52 : 0.32902182328150686
Loss in iteration 53 : 0.32857496268367964
Loss in iteration 54 : 0.32842562985420565
Loss in iteration 55 : 0.32844907025634407
Loss in iteration 56 : 0.3284417419561033
Loss in iteration 57 : 0.32827274981633314
Loss in iteration 58 : 0.3279644326409733
Loss in iteration 59 : 0.32764983732294056
Loss in iteration 60 : 0.32745661694871253
Loss in iteration 61 : 0.3274157717803672
Loss in iteration 62 : 0.3274565141521439
Loss in iteration 63 : 0.3274732868098255
Loss in iteration 64 : 0.32740379676261694
Loss in iteration 65 : 0.3272628999577938
Loss in iteration 66 : 0.3271176213186098
Loss in iteration 67 : 0.32703016789065764
Loss in iteration 68 : 0.32701331808682443
Loss in iteration 69 : 0.32702826861933854
Loss in iteration 70 : 0.32702082133890253
Loss in iteration 71 : 0.32696354941330924
Loss in iteration 72 : 0.32687107871128407
Loss in iteration 73 : 0.3267812531649201
Loss in iteration 74 : 0.32672260953965127
Loss in iteration 75 : 0.3266952121270242
Loss in iteration 76 : 0.32667625388519966
Loss in iteration 77 : 0.32664112293488373
Loss in iteration 78 : 0.3265815495872683
Loss in iteration 79 : 0.3265085024632844
Loss in iteration 80 : 0.32644076190722754
Loss in iteration 81 : 0.32638998873249064
Loss in iteration 82 : 0.3263536453018716
Loss in iteration 83 : 0.3263195382545667
Loss in iteration 84 : 0.32627662592378154
Loss in iteration 85 : 0.3262229866403754
Loss in iteration 86 : 0.32616554874391473
Loss in iteration 87 : 0.3261133437498249
Loss in iteration 88 : 0.326070536796022
Loss in iteration 89 : 0.3260343858174816
Loss in iteration 90 : 0.3259985986683085
Loss in iteration 91 : 0.32595855848202016
Loss in iteration 92 : 0.32591429863747745
Loss in iteration 93 : 0.3258695592042344
Loss in iteration 94 : 0.3258283648848839
Loss in iteration 95 : 0.3257920430805249
Loss in iteration 96 : 0.32575874079809053
Loss in iteration 97 : 0.32572532853229963
Loss in iteration 98 : 0.32568987284176387
Loss in iteration 99 : 0.32565277644211493
Loss in iteration 100 : 0.3256160087213974
Loss in iteration 101 : 0.3255813521999433
Loss in iteration 102 : 0.3255491269611977
Loss in iteration 103 : 0.325518219767746
Loss in iteration 104 : 0.32548713946781394
Loss in iteration 105 : 0.32545514065888714
Loss in iteration 106 : 0.32542259917928124
Loss in iteration 107 : 0.3253905154260955
Loss in iteration 108 : 0.3253596661633879
Loss in iteration 109 : 0.3253300780407613
Loss in iteration 110 : 0.32530114082025274
Loss in iteration 111 : 0.325272160048372
Loss in iteration 112 : 0.3252428684819146
Loss in iteration 113 : 0.3252135338681461
Loss in iteration 114 : 0.3251846605732614
Loss in iteration 115 : 0.325156577481099
Loss in iteration 116 : 0.32512923143363925
Loss in iteration 117 : 0.32510229585038064
Loss in iteration 118 : 0.32507545586520226
Loss in iteration 119 : 0.3250486309517241
Loss in iteration 120 : 0.3250219866296912
Loss in iteration 121 : 0.32499576654843787
Loss in iteration 122 : 0.32497009956735723
Loss in iteration 123 : 0.32494492571401745
Loss in iteration 124 : 0.3249200724799093
Loss in iteration 125 : 0.3248953983066675
Loss in iteration 126 : 0.3248708867048483
Loss in iteration 127 : 0.32484663187483553
Loss in iteration 128 : 0.32482274659429367
Loss in iteration 129 : 0.3247992739143479
Loss in iteration 130 : 0.32477616609605425
Loss in iteration 131 : 0.3247533333956959
Loss in iteration 132 : 0.3247307141174399
Loss in iteration 133 : 0.3247083111067035
Loss in iteration 134 : 0.3246861745296013
Loss in iteration 135 : 0.32466435353660644
Loss in iteration 136 : 0.32464285777706436
Loss in iteration 137 : 0.32462165493777895
Loss in iteration 138 : 0.3246006993865143
Loss in iteration 139 : 0.32457996523592386
Loss in iteration 140 : 0.3245594588756187
Loss in iteration 141 : 0.3245392056660734
Loss in iteration 142 : 0.3245192253603263
Loss in iteration 143 : 0.3244995163239201
Loss in iteration 144 : 0.3244800584180226
Loss in iteration 145 : 0.3244608289165523
Loss in iteration 146 : 0.3244418173685479
Loss in iteration 147 : 0.32442302874112905
Loss in iteration 148 : 0.32440447474567263
Loss in iteration 149 : 0.3243861619048247
Loss in iteration 150 : 0.3243680856989628
Loss in iteration 151 : 0.3243502339347708
Loss in iteration 152 : 0.3243325951361443
Loss in iteration 153 : 0.32431516486119044
Loss in iteration 154 : 0.32429794569855763
Loss in iteration 155 : 0.32428094213116254
Loss in iteration 156 : 0.3242641550006326
Loss in iteration 157 : 0.32424757970942597
Loss in iteration 158 : 0.3242312087750225
Loss in iteration 159 : 0.3242150360395484
Loss in iteration 160 : 0.32419905913065616
Loss in iteration 161 : 0.32418327869476016
Loss in iteration 162 : 0.32416769557349184
Loss in iteration 163 : 0.32415230838637354
Loss in iteration 164 : 0.32413711322206346
Loss in iteration 165 : 0.3241221052697695
Loss in iteration 166 : 0.3241072808254852
Loss in iteration 167 : 0.32409263812502426
Loss in iteration 168 : 0.32407817661073485
Loss in iteration 169 : 0.32406389546725684
Loss in iteration 170 : 0.3240497926436577
Loss in iteration 171 : 0.3240358649915734
Loss in iteration 172 : 0.32402210920716584
Loss in iteration 173 : 0.32400852273021086
Loss in iteration 174 : 0.3239951039445038
Loss in iteration 175 : 0.3239818516628712
Loss in iteration 176 : 0.32396876441164607
Loss in iteration 177 : 0.32395584008097206
Loss in iteration 178 : 0.32394307613055756
Loss in iteration 179 : 0.32393047009406345
Loss in iteration 180 : 0.3239180199524499
Loss in iteration 181 : 0.32390572412420615
Loss in iteration 182 : 0.3238935811513873
Loss in iteration 183 : 0.3238815893708835
Loss in iteration 184 : 0.32386974681692415
Loss in iteration 185 : 0.3238580513831916
Loss in iteration 186 : 0.32384650107510016
Loss in iteration 187 : 0.323835094147792
Loss in iteration 188 : 0.3238238290486226
Loss in iteration 189 : 0.32381270424336056
Loss in iteration 190 : 0.3238017180768017
Loss in iteration 191 : 0.32379086876470653
Loss in iteration 192 : 0.3237801544981957
Loss in iteration 193 : 0.32376957356165204
Loss in iteration 194 : 0.3237591243737235
Loss in iteration 195 : 0.3237488054351802
Loss in iteration 196 : 0.3237386152397718
Loss in iteration 197 : 0.323728552221245
Loss in iteration 198 : 0.3237186147690041
Loss in iteration 199 : 0.3237088012876518
Loss in iteration 200 : 0.3236991102476106
Testing accuracy  of updater 9 on alg 0 with rate 0.008 = 0.78775, training accuracy 0.8429912593072192, time elapsed: 2051 millisecond.
Loss in iteration 1 : 0.6931471805599175
Loss in iteration 2 : 0.5953570400216083
Loss in iteration 3 : 0.5086717668249109
Loss in iteration 4 : 0.47584518989889296
Loss in iteration 5 : 0.4769117846426318
Loss in iteration 6 : 0.4884420180508669
Loss in iteration 7 : 0.49719162950403145
Loss in iteration 8 : 0.49745381722426335
Loss in iteration 9 : 0.48790256356161193
Loss in iteration 10 : 0.4697677710692251
Loss in iteration 11 : 0.4458501445490714
Loss in iteration 12 : 0.41987271611830457
Loss in iteration 13 : 0.3958397867924659
Loss in iteration 14 : 0.3772413752878504
Loss in iteration 15 : 0.36616313880305057
Loss in iteration 16 : 0.36261419598113354
Loss in iteration 17 : 0.3644978843364139
Loss in iteration 18 : 0.368429895315389
Loss in iteration 19 : 0.3711015157176738
Loss in iteration 20 : 0.370489580649913
Loss in iteration 21 : 0.3663192874274433
Loss in iteration 22 : 0.35970308447726324
Loss in iteration 23 : 0.3523426001361796
Loss in iteration 24 : 0.34577552531317607
Loss in iteration 25 : 0.34094665457680173
Loss in iteration 26 : 0.3381278955016704
Loss in iteration 27 : 0.3370661309323142
Loss in iteration 28 : 0.3372152480013519
Loss in iteration 29 : 0.33795220219343053
Loss in iteration 30 : 0.33873122021092705
Loss in iteration 31 : 0.3391685036874737
Loss in iteration 32 : 0.3390686365319098
Loss in iteration 33 : 0.338409523476084
Loss in iteration 34 : 0.33730193270549264
Loss in iteration 35 : 0.3359369529652475
Loss in iteration 36 : 0.3345318745107837
Loss in iteration 37 : 0.333282642386541
Loss in iteration 38 : 0.33232889615746447
Loss in iteration 39 : 0.33173531758967373
Loss in iteration 40 : 0.3314903238678134
Loss in iteration 41 : 0.33152017529651545
Loss in iteration 42 : 0.33171379758854824
Loss in iteration 43 : 0.3319517595800412
Loss in iteration 44 : 0.3321325149272904
Loss in iteration 45 : 0.33219039744893913
Loss in iteration 46 : 0.33210254832366704
Loss in iteration 47 : 0.3318850806437958
Loss in iteration 48 : 0.33158138052530994
Loss in iteration 49 : 0.33124681861769867
Loss in iteration 50 : 0.33093412484305035
Loss in iteration 51 : 0.3306825670675431
Loss in iteration 52 : 0.3305124418942415
Loss in iteration 53 : 0.3304248070434025
Loss in iteration 54 : 0.3304052501925046
Loss in iteration 55 : 0.33042994385255925
Loss in iteration 56 : 0.3304722231314164
Loss in iteration 57 : 0.3305082758984566
Loss in iteration 58 : 0.3305210627206325
Loss in iteration 59 : 0.3305021296605089
Loss in iteration 60 : 0.33045143746902933
Loss in iteration 61 : 0.33037565431424604
Loss in iteration 62 : 0.33028553281311757
Loss in iteration 63 : 0.3301930269542437
Loss in iteration 64 : 0.33010872560717536
Loss in iteration 65 : 0.33004001953031115
Loss in iteration 66 : 0.32999021513437937
Loss in iteration 67 : 0.32995859899821395
Loss in iteration 68 : 0.329941277968041
Loss in iteration 69 : 0.3299324988529445
Loss in iteration 70 : 0.3299261051542134
Loss in iteration 71 : 0.32991681630772574
Loss in iteration 72 : 0.3299011025854131
Loss in iteration 73 : 0.32987754976387756
Loss in iteration 74 : 0.32984673122283625
Loss in iteration 75 : 0.32981070402187895
Loss in iteration 76 : 0.329772302145125
Loss in iteration 77 : 0.32973440952902044
Loss in iteration 78 : 0.3296993644622532
Loss in iteration 79 : 0.329668590151069
Loss in iteration 80 : 0.3296424811586243
Loss in iteration 81 : 0.3296205176148989
Loss in iteration 82 : 0.3296015390669173
Loss in iteration 83 : 0.3295840919286723
Loss in iteration 84 : 0.3295667674420178
Loss in iteration 85 : 0.3295484657070856
Loss in iteration 86 : 0.3295285486389629
Loss in iteration 87 : 0.3295068735746482
Loss in iteration 88 : 0.32948372390488756
Loss in iteration 89 : 0.32945966978617836
Loss in iteration 90 : 0.3294353991755346
Loss in iteration 91 : 0.3294115576153232
Loss in iteration 92 : 0.3293886263952007
Loss in iteration 93 : 0.3293668558480238
Loss in iteration 94 : 0.32934625674712736
Loss in iteration 95 : 0.32932664085441354
Loss in iteration 96 : 0.32930769360105394
Loss in iteration 97 : 0.3292890585851205
Loss in iteration 98 : 0.3292704148829832
Loss in iteration 99 : 0.32925153304535376
Loss in iteration 100 : 0.32923230249116436
Loss in iteration 101 : 0.3292127300948425
Loss in iteration 102 : 0.32919291560157693
Loss in iteration 103 : 0.32917301313187025
Loss in iteration 104 : 0.32915318911323566
Loss in iteration 105 : 0.3291335857553385
Loss in iteration 106 : 0.3291142963466244
Loss in iteration 107 : 0.3290953550814705
Loss in iteration 108 : 0.32907674069282944
Loss in iteration 109 : 0.3290583905355317
Loss in iteration 110 : 0.32904022031415986
Loss in iteration 111 : 0.32902214443877653
Loss in iteration 112 : 0.32900409282295295
Loss in iteration 113 : 0.3289860214438213
Loss in iteration 114 : 0.3289679157346178
Loss in iteration 115 : 0.32894978748261045
Loss in iteration 116 : 0.32893166707115523
Loss in iteration 117 : 0.32891359348634813
Loss in iteration 118 : 0.3288956045011888
Loss in iteration 119 : 0.3288777289647093
Loss in iteration 120 : 0.3288599823446617
Loss in iteration 121 : 0.32884236580874154
Loss in iteration 122 : 0.32882486836796543
Loss in iteration 123 : 0.32880747108016506
Loss in iteration 124 : 0.3287901520834592
Loss in iteration 125 : 0.3287728912907493
Loss in iteration 126 : 0.3287556738615118
Loss in iteration 127 : 0.3287384919779971
Loss in iteration 128 : 0.3287213448833499
Loss in iteration 129 : 0.3287042374983511
Loss in iteration 130 : 0.3286871781618465
Loss in iteration 131 : 0.3286701761153196
Loss in iteration 132 : 0.32865323928768203
Loss in iteration 133 : 0.32863637277137536
Loss in iteration 134 : 0.3286195781675083
Loss in iteration 135 : 0.32860285376855913
Loss in iteration 136 : 0.32858619538357337
Loss in iteration 137 : 0.3285695975174948
Loss in iteration 138 : 0.3285530545988687
Loss in iteration 139 : 0.3285365619978771
Loss in iteration 140 : 0.3285201166676146
Loss in iteration 141 : 0.32850371734916406
Loss in iteration 142 : 0.3284873643805395
Loss in iteration 143 : 0.328471059222273
Loss in iteration 144 : 0.3284548038487618
Loss in iteration 145 : 0.32843860015377746
Loss in iteration 146 : 0.3284224494879743
Loss in iteration 147 : 0.328406352397383
Loss in iteration 148 : 0.3283903085782356
Loss in iteration 149 : 0.3283743170165884
Loss in iteration 150 : 0.3283583762495645
Loss in iteration 151 : 0.32834248467205635
Loss in iteration 152 : 0.32832664081771534
Loss in iteration 153 : 0.32831084356158496
Loss in iteration 154 : 0.3282950922175729
Loss in iteration 155 : 0.32827938653042255
Loss in iteration 156 : 0.3282637265834659
Loss in iteration 157 : 0.32824811265667014
Loss in iteration 158 : 0.3282325450731976
Loss in iteration 159 : 0.3282170240678956
Loss in iteration 160 : 0.32820154970040083
Loss in iteration 161 : 0.32818612182216006
Loss in iteration 162 : 0.32817074009390274
Loss in iteration 163 : 0.32815540404043064
Loss in iteration 164 : 0.32814011312439023
Loss in iteration 165 : 0.32812486682027114
Loss in iteration 166 : 0.32810966467334024
Loss in iteration 167 : 0.3280945063341648
Loss in iteration 168 : 0.3280793915661727
Loss in iteration 169 : 0.3280643202296815
Loss in iteration 170 : 0.3280492922500437
Loss in iteration 171 : 0.32803430757938906
Loss in iteration 172 : 0.3280193661609702
Loss in iteration 173 : 0.32800446790293064
Loss in iteration 174 : 0.3279896126650761
Loss in iteration 175 : 0.3279748002588992
Loss in iteration 176 : 0.32796003045833305
Loss in iteration 177 : 0.32794530301696856
Loss in iteration 178 : 0.3279306176869382
Loss in iteration 179 : 0.3279159742352289
Loss in iteration 180 : 0.3279013724544977
Loss in iteration 181 : 0.3278868121671729
Loss in iteration 182 : 0.3278722932232025
Loss in iteration 183 : 0.32785781549310083
Loss in iteration 184 : 0.3278433788585675
Loss in iteration 185 : 0.3278289832030639
Loss in iteration 186 : 0.32781462840427394
Loss in iteration 187 : 0.32780031432963497
Loss in iteration 188 : 0.3277860408352652
Loss in iteration 189 : 0.3277718077678671
Loss in iteration 190 : 0.32775761496859623
Loss in iteration 191 : 0.3277434622777593
Loss in iteration 192 : 0.3277293495391437
Loss in iteration 193 : 0.3277152766031624
Loss in iteration 194 : 0.32770124332835276
Loss in iteration 195 : 0.32768724958123097
Loss in iteration 196 : 0.3276732952348007
Loss in iteration 197 : 0.32765938016629553
Loss in iteration 198 : 0.32764550425473365
Loss in iteration 199 : 0.32763166737884064
Loss in iteration 200 : 0.32761786941567306
Testing accuracy  of updater 9 on alg 0 with rate 0.0019999999999999983 = 0.7845, training accuracy 0.8387827775979282, time elapsed: 1993 millisecond.
