objc[1660]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x109f444c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10af694e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 19:37:46 INFO SparkContext: Running Spark version 2.0.0
18/02/26 19:37:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 19:37:47 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 19:37:47 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 19:37:47 INFO SecurityManager: Changing view acls groups to: 
18/02/26 19:37:47 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 19:37:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 19:37:48 INFO Utils: Successfully started service 'sparkDriver' on port 50575.
18/02/26 19:37:48 INFO SparkEnv: Registering MapOutputTracker
18/02/26 19:37:48 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 19:37:48 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-2f78062e-b3bc-4bbe-950d-9bf560794445
18/02/26 19:37:48 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 19:37:48 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 19:37:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 19:37:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 19:37:49 INFO Executor: Starting executor ID driver on host localhost
18/02/26 19:37:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50576.
18/02/26 19:37:49 INFO NettyBlockTransferService: Server created on 192.168.2.140:50576
18/02/26 19:37:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50576)
18/02/26 19:37:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50576 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50576)
18/02/26 19:37:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50576)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0004925100680415
Loss in iteration 3 : 1.8713901205952803
Loss in iteration 4 : 1.9119880475500086
Loss in iteration 5 : 1.2520403570475873
Loss in iteration 6 : 2.1852011019027135
Loss in iteration 7 : 0.8596258100324305
Loss in iteration 8 : 1.7721198456322778
Loss in iteration 9 : 1.1777353020709829
Loss in iteration 10 : 2.0225043278343557
Loss in iteration 11 : 0.8407361852396762
Loss in iteration 12 : 1.754172740998644
Loss in iteration 13 : 1.030388872004367
Loss in iteration 14 : 1.8113729182665674
Loss in iteration 15 : 0.9143429687935397
Loss in iteration 16 : 1.6387628355474018
Loss in iteration 17 : 1.0120698429925923
Loss in iteration 18 : 1.582446795181391
Loss in iteration 19 : 1.001307276508493
Loss in iteration 20 : 1.4928154306456027
Loss in iteration 21 : 1.0232831470054922
Loss in iteration 22 : 1.4030287236623307
Loss in iteration 23 : 1.0419446901612044
Loss in iteration 24 : 1.323526128898755
Loss in iteration 25 : 1.0572317357874668
Loss in iteration 26 : 1.2516400781261832
Loss in iteration 27 : 1.055513251602917
Loss in iteration 28 : 1.1905146889031708
Loss in iteration 29 : 1.0586073111438168
Loss in iteration 30 : 1.1319685329611735
Loss in iteration 31 : 1.055327134345227
Loss in iteration 32 : 1.090040830151232
Loss in iteration 33 : 1.0341038422327415
Loss in iteration 34 : 1.0412020236900492
Loss in iteration 35 : 1.0146848895526432
Loss in iteration 36 : 1.0082259266968676
Loss in iteration 37 : 0.9916609642996843
Loss in iteration 38 : 0.9844889532171096
Loss in iteration 39 : 0.9744100418874161
Loss in iteration 40 : 0.9554011049807685
Loss in iteration 41 : 0.9569852748824796
Loss in iteration 42 : 0.9302547767867451
Loss in iteration 43 : 0.9425594427049363
Loss in iteration 44 : 0.9067112724589906
Loss in iteration 45 : 0.9244971841935824
Loss in iteration 46 : 0.890305168857069
Loss in iteration 47 : 0.9149620110943999
Loss in iteration 48 : 0.877158254347853
Loss in iteration 49 : 0.9067780522155895
Loss in iteration 50 : 0.8665155702980942
Loss in iteration 51 : 0.8988476355321668
Loss in iteration 52 : 0.8536317534469678
Loss in iteration 53 : 0.8850842498281487
Loss in iteration 54 : 0.8416640309963002
Loss in iteration 55 : 0.874700134244194
Loss in iteration 56 : 0.827794990484818
Loss in iteration 57 : 0.8646725126013611
Loss in iteration 58 : 0.8164309906582419
Loss in iteration 59 : 0.8549030162814933
Loss in iteration 60 : 0.8105281440021285
Loss in iteration 61 : 0.8458975197082156
Loss in iteration 62 : 0.8040649216378509
Loss in iteration 63 : 0.8398390317995617
Loss in iteration 64 : 0.7959726482514509
Loss in iteration 65 : 0.8256270427714201
Loss in iteration 66 : 0.7907640190667657
Loss in iteration 67 : 0.8209671387219292
Loss in iteration 68 : 0.7849826418722684
Loss in iteration 69 : 0.8172470809114527
Loss in iteration 70 : 0.7800313961829543
Loss in iteration 71 : 0.8145541384555394
Loss in iteration 72 : 0.7783995552811322
Loss in iteration 73 : 0.8105655736506019
Loss in iteration 74 : 0.7728580630501738
Loss in iteration 75 : 0.8051225182339585
Loss in iteration 76 : 0.7709144497669121
Loss in iteration 77 : 0.8035499701355427
Loss in iteration 78 : 0.769752169025809
Loss in iteration 79 : 0.8007463192511923
Loss in iteration 80 : 0.7656337360407709
Loss in iteration 81 : 0.7965185382620404
Loss in iteration 82 : 0.7626827758014524
Loss in iteration 83 : 0.792840380975734
Loss in iteration 84 : 0.7606685316676514
Loss in iteration 85 : 0.7899732254253055
Loss in iteration 86 : 0.7574014665486013
Loss in iteration 87 : 0.7862671243833094
Loss in iteration 88 : 0.7521987114320718
Loss in iteration 89 : 0.7842953436666285
Loss in iteration 90 : 0.7497583637838838
Loss in iteration 91 : 0.7802639824746188
Loss in iteration 92 : 0.7486017065220694
Loss in iteration 93 : 0.780030998719143
Loss in iteration 94 : 0.7480103130259431
Loss in iteration 95 : 0.7789397197913215
Loss in iteration 96 : 0.7463601942328316
Loss in iteration 97 : 0.7774063951949857
Loss in iteration 98 : 0.7444168823093235
Loss in iteration 99 : 0.7764577424834836
Loss in iteration 100 : 0.7426586969745831
Loss in iteration 101 : 0.7733681043176125
Loss in iteration 102 : 0.7409923961586499
Loss in iteration 103 : 0.7729065430368871
Loss in iteration 104 : 0.738933493550683
Loss in iteration 105 : 0.7689185813397391
Loss in iteration 106 : 0.7396468843847195
Loss in iteration 107 : 0.7688079274373671
Loss in iteration 108 : 0.7381042597349491
Loss in iteration 109 : 0.7652765244856516
Loss in iteration 110 : 0.7390215697553927
Loss in iteration 111 : 0.7654445372668143
Loss in iteration 112 : 0.7368683722581499
Loss in iteration 113 : 0.7641890687922623
Loss in iteration 114 : 0.7367331642971631
Loss in iteration 115 : 0.7629764821717925
Loss in iteration 116 : 0.7357667126959684
Loss in iteration 117 : 0.7607690675216611
Loss in iteration 118 : 0.7358138788919262
Loss in iteration 119 : 0.7587459489517073
Loss in iteration 120 : 0.7355628413117281
Loss in iteration 121 : 0.7582659345991958
Loss in iteration 122 : 0.7342661035421384
Loss in iteration 123 : 0.7544348541934146
Loss in iteration 124 : 0.7320388147804051
Loss in iteration 125 : 0.7512644573671976
Loss in iteration 126 : 0.7307708629978611
Loss in iteration 127 : 0.7502417748217137
Loss in iteration 128 : 0.7296083744859045
Loss in iteration 129 : 0.7491611383661408
Loss in iteration 130 : 0.7255438709450348
Loss in iteration 131 : 0.7469924482815654
Loss in iteration 132 : 0.7252574210639605
Loss in iteration 133 : 0.7460889921502804
Loss in iteration 134 : 0.7257549275459663
Loss in iteration 135 : 0.7460877630605796
Loss in iteration 136 : 0.7255557953272596
Loss in iteration 137 : 0.7433481839564738
Loss in iteration 138 : 0.7238613749357967
Loss in iteration 139 : 0.7422950936049313
Loss in iteration 140 : 0.7229828328084343
Loss in iteration 141 : 0.7405962325639641
Loss in iteration 142 : 0.7221823234754002
Loss in iteration 143 : 0.741195306558644
Loss in iteration 144 : 0.72124371301299
Loss in iteration 145 : 0.7391476799589214
Loss in iteration 146 : 0.7207577372204428
Loss in iteration 147 : 0.739132393480869
Loss in iteration 148 : 0.7201701728448222
Loss in iteration 149 : 0.7386926693225642
Loss in iteration 150 : 0.7192851756710844
Loss in iteration 151 : 0.7374362033338269
Loss in iteration 152 : 0.7178497456597269
Loss in iteration 153 : 0.7340725044282141
Loss in iteration 154 : 0.717785224267388
Loss in iteration 155 : 0.7340066539127262
Loss in iteration 156 : 0.7162617368211517
Loss in iteration 157 : 0.7326715771368397
Loss in iteration 158 : 0.713269769108226
Loss in iteration 159 : 0.7307835346700957
Loss in iteration 160 : 0.7129857720834422
Loss in iteration 161 : 0.7292616320044787
Loss in iteration 162 : 0.7123627329320044
Loss in iteration 163 : 0.7286799522140941
Loss in iteration 164 : 0.7117084420316458
Loss in iteration 165 : 0.728453010745233
Loss in iteration 166 : 0.7120163005342507
Loss in iteration 167 : 0.7282370389362173
Loss in iteration 168 : 0.7121982709593776
Loss in iteration 169 : 0.7285113345069275
Loss in iteration 170 : 0.7117914121657156
Loss in iteration 171 : 0.7281394428436194
Loss in iteration 172 : 0.7123503559233629
Loss in iteration 173 : 0.7275154405643156
Loss in iteration 174 : 0.7115052603118829
Loss in iteration 175 : 0.7268022181908962
Loss in iteration 176 : 0.7096309504796691
Loss in iteration 177 : 0.7256181924832024
Loss in iteration 178 : 0.7092160408330579
Loss in iteration 179 : 0.7250597116042249
Loss in iteration 180 : 0.7087014807820452
Loss in iteration 181 : 0.7247232338795588
Loss in iteration 182 : 0.7084803051471172
Loss in iteration 183 : 0.7245551697149875
Loss in iteration 184 : 0.7079237988743846
Loss in iteration 185 : 0.7227068305544272
Loss in iteration 186 : 0.7089680565050387
Loss in iteration 187 : 0.7228883228742953
Loss in iteration 188 : 0.7087828984097446
Loss in iteration 189 : 0.7225038315850991
Loss in iteration 190 : 0.7085847504114572
Loss in iteration 191 : 0.7218285852594587
Loss in iteration 192 : 0.7093241970933906
Loss in iteration 193 : 0.7218143438270023
Loss in iteration 194 : 0.7096159658616411
Loss in iteration 195 : 0.721798306010899
Loss in iteration 196 : 0.7092242140181788
Loss in iteration 197 : 0.7210188220482028
Loss in iteration 198 : 0.7091317368755472
Loss in iteration 199 : 0.7206232795959229
Loss in iteration 200 : 0.7088359100970554
Testing accuracy  of updater 0 on alg 1 with rate 1.0 = 0.752, training accuracy 0.744125, time elapsed: 6183 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.927212173608185
Loss in iteration 3 : 0.9596167183822104
Loss in iteration 4 : 0.9146257478703564
Loss in iteration 5 : 1.0500459028170452
Loss in iteration 6 : 1.4648847504883848
Loss in iteration 7 : 0.9365378966782881
Loss in iteration 8 : 1.4050828352509763
Loss in iteration 9 : 0.9087933333662707
Loss in iteration 10 : 1.3866077015565783
Loss in iteration 11 : 0.8570427623634198
Loss in iteration 12 : 1.3116091937891745
Loss in iteration 13 : 0.8558186706996841
Loss in iteration 14 : 1.2793747849660262
Loss in iteration 15 : 0.8352953410557552
Loss in iteration 16 : 1.2190298763597134
Loss in iteration 17 : 0.8400552252533926
Loss in iteration 18 : 1.1754011616179292
Loss in iteration 19 : 0.8344359972305891
Loss in iteration 20 : 1.1239980801354321
Loss in iteration 21 : 0.8365624074424017
Loss in iteration 22 : 1.0831167254327254
Loss in iteration 23 : 0.8371544759302424
Loss in iteration 24 : 1.040509376666529
Loss in iteration 25 : 0.8338834235473935
Loss in iteration 26 : 1.003443949168916
Loss in iteration 27 : 0.8238062470810521
Loss in iteration 28 : 0.9712385173695989
Loss in iteration 29 : 0.817559479796384
Loss in iteration 30 : 0.9392725492898203
Loss in iteration 31 : 0.8126535431300401
Loss in iteration 32 : 0.905303996684686
Loss in iteration 33 : 0.8076900469072125
Loss in iteration 34 : 0.8790115272334958
Loss in iteration 35 : 0.7972938170036022
Loss in iteration 36 : 0.8546280703856982
Loss in iteration 37 : 0.7884992761387366
Loss in iteration 38 : 0.8281005394137031
Loss in iteration 39 : 0.7715160532027037
Loss in iteration 40 : 0.8074276975363055
Loss in iteration 41 : 0.7587670911254909
Loss in iteration 42 : 0.7860360323613012
Loss in iteration 43 : 0.7449730587135696
Loss in iteration 44 : 0.7686891191625332
Loss in iteration 45 : 0.7369653100898708
Loss in iteration 46 : 0.7562551687959623
Loss in iteration 47 : 0.726856620255807
Loss in iteration 48 : 0.7459963379670091
Loss in iteration 49 : 0.7186620334887258
Loss in iteration 50 : 0.7340871347918553
Loss in iteration 51 : 0.7106919059891702
Loss in iteration 52 : 0.7281787231180239
Loss in iteration 53 : 0.7038778105131193
Loss in iteration 54 : 0.7187180871779132
Loss in iteration 55 : 0.6976285366747741
Loss in iteration 56 : 0.7063573903628653
Loss in iteration 57 : 0.6944901661423696
Loss in iteration 58 : 0.7012659798684174
Loss in iteration 59 : 0.6878287086819045
Loss in iteration 60 : 0.6923436812777377
Loss in iteration 61 : 0.6818742633381941
Loss in iteration 62 : 0.6872393341721951
Loss in iteration 63 : 0.6756416205228623
Loss in iteration 64 : 0.6778691914468136
Loss in iteration 65 : 0.6688820587816195
Loss in iteration 66 : 0.6735057842538258
Loss in iteration 67 : 0.6632179232764595
Loss in iteration 68 : 0.670360371970668
Loss in iteration 69 : 0.6601047142383996
Loss in iteration 70 : 0.665850399147032
Loss in iteration 71 : 0.6590795310895732
Loss in iteration 72 : 0.6629755233305412
Loss in iteration 73 : 0.6546943295341183
Loss in iteration 74 : 0.6597066504506462
Loss in iteration 75 : 0.6518574744792572
Loss in iteration 76 : 0.6575603702849199
Loss in iteration 77 : 0.6508738966459175
Loss in iteration 78 : 0.6553164098312013
Loss in iteration 79 : 0.6488119386534976
Loss in iteration 80 : 0.6538251017636769
Loss in iteration 81 : 0.6461282325343746
Loss in iteration 82 : 0.6532371191455196
Loss in iteration 83 : 0.6438842498411689
Loss in iteration 84 : 0.6514833784381254
Loss in iteration 85 : 0.6425477164170877
Loss in iteration 86 : 0.6494129405900924
Loss in iteration 87 : 0.6403692185776488
Loss in iteration 88 : 0.6456435135296495
Loss in iteration 89 : 0.6398796339743926
Loss in iteration 90 : 0.6443019429148019
Loss in iteration 91 : 0.6371693885389879
Loss in iteration 92 : 0.6423449666270877
Loss in iteration 93 : 0.6360037247581598
Loss in iteration 94 : 0.6417999171116355
Loss in iteration 95 : 0.6350018687366372
Loss in iteration 96 : 0.6397249155863852
Loss in iteration 97 : 0.6323840605635944
Loss in iteration 98 : 0.6386066953076709
Loss in iteration 99 : 0.6314030943448888
Loss in iteration 100 : 0.6379928855645051
Loss in iteration 101 : 0.631359702373831
Loss in iteration 102 : 0.637672116960912
Loss in iteration 103 : 0.6301340005783088
Loss in iteration 104 : 0.6366022229702687
Loss in iteration 105 : 0.6287443502783852
Loss in iteration 106 : 0.6339648194467095
Loss in iteration 107 : 0.626197568445373
Loss in iteration 108 : 0.6313899816995354
Loss in iteration 109 : 0.626414357661831
Loss in iteration 110 : 0.6310728514409993
Loss in iteration 111 : 0.624859334461642
Loss in iteration 112 : 0.6306321700658227
Loss in iteration 113 : 0.6235272720399989
Loss in iteration 114 : 0.6292441605429424
Loss in iteration 115 : 0.6205345125716667
Loss in iteration 116 : 0.6256985633500194
Loss in iteration 117 : 0.6178764129018871
Loss in iteration 118 : 0.6242721490759986
Loss in iteration 119 : 0.6159224853967742
Loss in iteration 120 : 0.6222515223294465
Loss in iteration 121 : 0.6152310752380864
Loss in iteration 122 : 0.6221479913502528
Loss in iteration 123 : 0.6144450133598097
Loss in iteration 124 : 0.6217699244818918
Loss in iteration 125 : 0.6140820234808914
Loss in iteration 126 : 0.6212395349931069
Loss in iteration 127 : 0.6142250208969071
Loss in iteration 128 : 0.6201293040285271
Loss in iteration 129 : 0.6129309025853983
Loss in iteration 130 : 0.6201359809307958
Loss in iteration 131 : 0.6119609976065854
Loss in iteration 132 : 0.6179827933391532
Loss in iteration 133 : 0.6097579110900763
Loss in iteration 134 : 0.6156935710593765
Loss in iteration 135 : 0.6091634717637368
Loss in iteration 136 : 0.6150881350733547
Loss in iteration 137 : 0.608878608640816
Loss in iteration 138 : 0.6140126777244056
Loss in iteration 139 : 0.6077816371764568
Loss in iteration 140 : 0.6126082537472969
Loss in iteration 141 : 0.6073956858980013
Loss in iteration 142 : 0.611753225837163
Loss in iteration 143 : 0.6071736469875655
Loss in iteration 144 : 0.6112974516639682
Loss in iteration 145 : 0.6061403291152125
Loss in iteration 146 : 0.6085949703620104
Loss in iteration 147 : 0.6044250705332307
Loss in iteration 148 : 0.6076387891664747
Loss in iteration 149 : 0.6033069644089049
Loss in iteration 150 : 0.6071864009406966
Loss in iteration 151 : 0.6025152795343931
Loss in iteration 152 : 0.6074931793422906
Loss in iteration 153 : 0.6026343617157154
Loss in iteration 154 : 0.6072148414531441
Loss in iteration 155 : 0.602211544602475
Loss in iteration 156 : 0.6064957099994367
Loss in iteration 157 : 0.6015463106600704
Loss in iteration 158 : 0.6055711825006496
Loss in iteration 159 : 0.6010009701004195
Loss in iteration 160 : 0.60520487947226
Loss in iteration 161 : 0.6006454656568474
Loss in iteration 162 : 0.6036881438298897
Loss in iteration 163 : 0.5986261417145331
Loss in iteration 164 : 0.6015277335875427
Loss in iteration 165 : 0.5993230138430798
Loss in iteration 166 : 0.6021934010944802
Loss in iteration 167 : 0.5987630538872186
Loss in iteration 168 : 0.6012387496944641
Loss in iteration 169 : 0.5977876950643374
Loss in iteration 170 : 0.6010539774915475
Loss in iteration 171 : 0.5970988429423804
Loss in iteration 172 : 0.5995855397852321
Loss in iteration 173 : 0.5975969824027427
Loss in iteration 174 : 0.6004677470621839
Loss in iteration 175 : 0.5973128955961843
Loss in iteration 176 : 0.5997344158006924
Loss in iteration 177 : 0.5970639131004647
Loss in iteration 178 : 0.5992596699213802
Loss in iteration 179 : 0.5967167259943775
Loss in iteration 180 : 0.5988203027807784
Loss in iteration 181 : 0.5962290308288752
Loss in iteration 182 : 0.5978252256184153
Loss in iteration 183 : 0.5961772226818339
Loss in iteration 184 : 0.5975270511135111
Loss in iteration 185 : 0.5961352278456983
Loss in iteration 186 : 0.5970859698915196
Loss in iteration 187 : 0.5953566811859291
Loss in iteration 188 : 0.5962662384812545
Loss in iteration 189 : 0.5939667768447067
Loss in iteration 190 : 0.5956417564713121
Loss in iteration 191 : 0.592541039347037
Loss in iteration 192 : 0.5950915702735172
Loss in iteration 193 : 0.5917942078267031
Loss in iteration 194 : 0.5954192099697279
Loss in iteration 195 : 0.5916927228685998
Loss in iteration 196 : 0.5952879776400025
Loss in iteration 197 : 0.5913310698914401
Loss in iteration 198 : 0.5952465522283048
Loss in iteration 199 : 0.5913382514368484
Loss in iteration 200 : 0.595033472813961
Testing accuracy  of updater 0 on alg 1 with rate 0.7000000000000001 = 0.764, training accuracy 0.757, time elapsed: 4276 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9197754624762481
Loss in iteration 3 : 0.8973186601138853
Loss in iteration 4 : 0.8751812694395799
Loss in iteration 5 : 0.8539362309299418
Loss in iteration 6 : 0.8331143255874642
Loss in iteration 7 : 0.8125525215874536
Loss in iteration 8 : 0.7921354712003108
Loss in iteration 9 : 0.7718178397544425
Loss in iteration 10 : 0.7516032257790431
Loss in iteration 11 : 0.7316324548102863
Loss in iteration 12 : 0.712889316052432
Loss in iteration 13 : 0.6965503533527687
Loss in iteration 14 : 0.6822371005848585
Loss in iteration 15 : 0.6697698982672827
Loss in iteration 16 : 0.658929940493326
Loss in iteration 17 : 0.6495184850302427
Loss in iteration 18 : 0.6412180931636244
Loss in iteration 19 : 0.6340271483955408
Loss in iteration 20 : 0.6277812840604363
Loss in iteration 21 : 0.622221565063804
Loss in iteration 22 : 0.6171888078546443
Loss in iteration 23 : 0.6126410574447954
Loss in iteration 24 : 0.608516355320814
Loss in iteration 25 : 0.6047470986024186
Loss in iteration 26 : 0.6012679177569487
Loss in iteration 27 : 0.5980624755102074
Loss in iteration 28 : 0.5951428244553981
Loss in iteration 29 : 0.5924539620534884
Loss in iteration 30 : 0.5899345793097487
Loss in iteration 31 : 0.587551112695435
Loss in iteration 32 : 0.5852593981724297
Loss in iteration 33 : 0.583084440165648
Loss in iteration 34 : 0.5810369490373697
Loss in iteration 35 : 0.5791072000830023
Loss in iteration 36 : 0.5772879829416521
Loss in iteration 37 : 0.5755666038088514
Loss in iteration 38 : 0.5739097350229883
Loss in iteration 39 : 0.5723152533818796
Loss in iteration 40 : 0.5707798648226808
Loss in iteration 41 : 0.5693095230621728
Loss in iteration 42 : 0.5679176476109836
Loss in iteration 43 : 0.5665918532730816
Loss in iteration 44 : 0.5653245377937497
Loss in iteration 45 : 0.5641214210898863
Loss in iteration 46 : 0.5629650453241514
Loss in iteration 47 : 0.5618708399567626
Loss in iteration 48 : 0.5608077353915537
Loss in iteration 49 : 0.5597846328563294
Loss in iteration 50 : 0.5588122506524162
Loss in iteration 51 : 0.5578755397094556
Loss in iteration 52 : 0.5569655095322557
Loss in iteration 53 : 0.5560802517471156
Loss in iteration 54 : 0.5552355397502567
Loss in iteration 55 : 0.5544201339569998
Loss in iteration 56 : 0.5536343520943299
Loss in iteration 57 : 0.5528844271018498
Loss in iteration 58 : 0.5521738010704692
Loss in iteration 59 : 0.5514814044137247
Loss in iteration 60 : 0.5508007569361659
Loss in iteration 61 : 0.5501360495287215
Loss in iteration 62 : 0.5494949655065838
Loss in iteration 63 : 0.5488811363528087
Loss in iteration 64 : 0.5482834512749624
Loss in iteration 65 : 0.5476949097932615
Loss in iteration 66 : 0.547119043064318
Loss in iteration 67 : 0.5465626426967172
Loss in iteration 68 : 0.546025205076549
Loss in iteration 69 : 0.5454980201288511
Loss in iteration 70 : 0.5449780666861757
Loss in iteration 71 : 0.5444698237166505
Loss in iteration 72 : 0.5439750721566204
Loss in iteration 73 : 0.5434966502717786
Loss in iteration 74 : 0.5430296823999543
Loss in iteration 75 : 0.5425782637329344
Loss in iteration 76 : 0.5421470590395174
Loss in iteration 77 : 0.541736928924339
Loss in iteration 78 : 0.5413509092941262
Loss in iteration 79 : 0.5409593820977275
Loss in iteration 80 : 0.5406309928465752
Loss in iteration 81 : 0.5402446596096125
Loss in iteration 82 : 0.5399484311485502
Loss in iteration 83 : 0.5394873210001315
Loss in iteration 84 : 0.5392169341783699
Loss in iteration 85 : 0.5387809976960782
Loss in iteration 86 : 0.5385047437521304
Loss in iteration 87 : 0.5380969174152948
Loss in iteration 88 : 0.5377844198804249
Loss in iteration 89 : 0.5373902099505121
Loss in iteration 90 : 0.5370723629455101
Loss in iteration 91 : 0.536720874696032
Loss in iteration 92 : 0.5363947688443038
Loss in iteration 93 : 0.5360434773178477
Loss in iteration 94 : 0.5357194736988637
Loss in iteration 95 : 0.5354209265532363
Loss in iteration 96 : 0.5351340420202266
Loss in iteration 97 : 0.5348592490766982
Loss in iteration 98 : 0.5345846767912475
Loss in iteration 99 : 0.5343340459576821
Loss in iteration 100 : 0.5340644298636259
Loss in iteration 101 : 0.5338032370866537
Loss in iteration 102 : 0.5335467781530628
Loss in iteration 103 : 0.5332930855079863
Loss in iteration 104 : 0.5330373087417758
Loss in iteration 105 : 0.5327913062125638
Loss in iteration 106 : 0.5325554817861007
Loss in iteration 107 : 0.5323286213980314
Loss in iteration 108 : 0.5320972534401659
Loss in iteration 109 : 0.5318662633065746
Loss in iteration 110 : 0.5316417180007958
Loss in iteration 111 : 0.5314264231752008
Loss in iteration 112 : 0.5312159666183102
Loss in iteration 113 : 0.5310063542641223
Loss in iteration 114 : 0.5307940147486996
Loss in iteration 115 : 0.5305858966002104
Loss in iteration 116 : 0.5303847995864589
Loss in iteration 117 : 0.5301893206287678
Loss in iteration 118 : 0.5299897401004889
Loss in iteration 119 : 0.5297977094511893
Loss in iteration 120 : 0.529606012805153
Loss in iteration 121 : 0.5294167157160233
Loss in iteration 122 : 0.5292300549522352
Loss in iteration 123 : 0.5290449150702956
Loss in iteration 124 : 0.5288623902901383
Loss in iteration 125 : 0.5286815272318718
Loss in iteration 126 : 0.5285045978651634
Loss in iteration 127 : 0.5283300816785188
Loss in iteration 128 : 0.5281567905554109
Loss in iteration 129 : 0.527984479072866
Loss in iteration 130 : 0.5278143480566501
Loss in iteration 131 : 0.5276475006464816
Loss in iteration 132 : 0.5274863539035937
Loss in iteration 133 : 0.5273189477400372
Loss in iteration 134 : 0.5271571583109061
Loss in iteration 135 : 0.526992555989209
Loss in iteration 136 : 0.5268327623808812
Loss in iteration 137 : 0.5266732591731619
Loss in iteration 138 : 0.5265178436462242
Loss in iteration 139 : 0.5263670740746613
Loss in iteration 140 : 0.5262161530109581
Loss in iteration 141 : 0.5260717043689033
Loss in iteration 142 : 0.5259245502831477
Loss in iteration 143 : 0.5257735646629569
Loss in iteration 144 : 0.5256283196198432
Loss in iteration 145 : 0.5254858997868712
Loss in iteration 146 : 0.5253378155903193
Loss in iteration 147 : 0.5251902599068294
Loss in iteration 148 : 0.525049005642513
Loss in iteration 149 : 0.524908724793989
Loss in iteration 150 : 0.5247776127443101
Loss in iteration 151 : 0.5246497464169061
Loss in iteration 152 : 0.5245149569302485
Loss in iteration 153 : 0.524380201160005
Loss in iteration 154 : 0.5242481354732768
Loss in iteration 155 : 0.5241179422882831
Loss in iteration 156 : 0.5239854223835316
Loss in iteration 157 : 0.5238569267539355
Loss in iteration 158 : 0.5237316920479728
Loss in iteration 159 : 0.5236065086693524
Loss in iteration 160 : 0.5234843764814652
Loss in iteration 161 : 0.5233606213026096
Loss in iteration 162 : 0.523237556263068
Loss in iteration 163 : 0.5231154714017996
Loss in iteration 164 : 0.5229943502100703
Loss in iteration 165 : 0.5228754712934547
Loss in iteration 166 : 0.5227572671547106
Loss in iteration 167 : 0.5226395062954645
Loss in iteration 168 : 0.5225224070929653
Loss in iteration 169 : 0.5224072457114856
Loss in iteration 170 : 0.5222934726804519
Loss in iteration 171 : 0.5221819056048969
Loss in iteration 172 : 0.5220734191864185
Loss in iteration 173 : 0.521967590187504
Loss in iteration 174 : 0.5218638884830307
Loss in iteration 175 : 0.5217600976580379
Loss in iteration 176 : 0.5216566309725952
Loss in iteration 177 : 0.5215522187100676
Loss in iteration 178 : 0.521451934272269
Loss in iteration 179 : 0.5213522742792124
Loss in iteration 180 : 0.5212543338737587
Loss in iteration 181 : 0.5211581294391856
Loss in iteration 182 : 0.5210617948818197
Loss in iteration 183 : 0.5209665365584697
Loss in iteration 184 : 0.520872701241775
Loss in iteration 185 : 0.5207788087057084
Loss in iteration 186 : 0.5206856347432777
Loss in iteration 187 : 0.5205928777355143
Loss in iteration 188 : 0.5205013351810777
Loss in iteration 189 : 0.5204106532933824
Loss in iteration 190 : 0.5203219121310879
Loss in iteration 191 : 0.5202340989812273
Loss in iteration 192 : 0.5201468109928328
Loss in iteration 193 : 0.5200611960780493
Loss in iteration 194 : 0.5199752874460948
Loss in iteration 195 : 0.5198905899028398
Loss in iteration 196 : 0.519809525872112
Loss in iteration 197 : 0.519725383877545
Loss in iteration 198 : 0.5196424812867567
Loss in iteration 199 : 0.5195577824342136
Loss in iteration 200 : 0.5194750687343078
Testing accuracy  of updater 0 on alg 1 with rate 0.4 = 0.7765, training accuracy 0.785, time elapsed: 3588 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.976881816203008
Loss in iteration 3 : 0.9540353271130189
Loss in iteration 4 : 0.9344581318744213
Loss in iteration 5 : 0.9221497410136664
Loss in iteration 6 : 0.9143530761802388
Loss in iteration 7 : 0.9082381952924468
Loss in iteration 8 : 0.9026218690588094
Loss in iteration 9 : 0.8971800762964935
Loss in iteration 10 : 0.8917851439620087
Loss in iteration 11 : 0.8864230045702859
Loss in iteration 12 : 0.88109634892903
Loss in iteration 13 : 0.8758051471196118
Loss in iteration 14 : 0.8705334109506994
Loss in iteration 15 : 0.8652807885480788
Loss in iteration 16 : 0.8600421729291213
Loss in iteration 17 : 0.854823364922182
Loss in iteration 18 : 0.8496257479882563
Loss in iteration 19 : 0.8444419733592461
Loss in iteration 20 : 0.8392667990722289
Loss in iteration 21 : 0.834109664422836
Loss in iteration 22 : 0.82895743935068
Loss in iteration 23 : 0.8238127159923615
Loss in iteration 24 : 0.8186811650058063
Loss in iteration 25 : 0.8135548376893412
Loss in iteration 26 : 0.8084335701575625
Loss in iteration 27 : 0.803320886686303
Loss in iteration 28 : 0.7982216660665351
Loss in iteration 29 : 0.7931277302443953
Loss in iteration 30 : 0.7880391315578832
Loss in iteration 31 : 0.782954929559514
Loss in iteration 32 : 0.7778796071107058
Loss in iteration 33 : 0.7728117112657525
Loss in iteration 34 : 0.7677498742027626
Loss in iteration 35 : 0.7626969619725378
Loss in iteration 36 : 0.7576452340070586
Loss in iteration 37 : 0.7525944502048141
Loss in iteration 38 : 0.7475515560015035
Loss in iteration 39 : 0.7425395443519981
Loss in iteration 40 : 0.7375833282478741
Loss in iteration 41 : 0.7326778353343925
Loss in iteration 42 : 0.7278543569345289
Loss in iteration 43 : 0.7231104001753054
Loss in iteration 44 : 0.7185275717167301
Loss in iteration 45 : 0.7141020116082805
Loss in iteration 46 : 0.7098355798282466
Loss in iteration 47 : 0.7057244290310136
Loss in iteration 48 : 0.7017284722007326
Loss in iteration 49 : 0.6978711137199116
Loss in iteration 50 : 0.6941418606970855
Loss in iteration 51 : 0.6905048517424704
Loss in iteration 52 : 0.6869937516422737
Loss in iteration 53 : 0.6836154290269817
Loss in iteration 54 : 0.6803796762048855
Loss in iteration 55 : 0.677228807119533
Loss in iteration 56 : 0.6741642505388665
Loss in iteration 57 : 0.671224733713188
Loss in iteration 58 : 0.6683890578257629
Loss in iteration 59 : 0.6656646454200995
Loss in iteration 60 : 0.6630202227900445
Loss in iteration 61 : 0.6604474562241838
Loss in iteration 62 : 0.6579705425385018
Loss in iteration 63 : 0.6556048405159804
Loss in iteration 64 : 0.6533087785352082
Loss in iteration 65 : 0.6510666394916205
Loss in iteration 66 : 0.6488957642680073
Loss in iteration 67 : 0.646789963033411
Loss in iteration 68 : 0.6447520701279327
Loss in iteration 69 : 0.6427742717960464
Loss in iteration 70 : 0.6408656923805017
Loss in iteration 71 : 0.639032407327177
Loss in iteration 72 : 0.6372668349740594
Loss in iteration 73 : 0.635566089943357
Loss in iteration 74 : 0.6339232291830386
Loss in iteration 75 : 0.6323269190871124
Loss in iteration 76 : 0.6307808279922487
Loss in iteration 77 : 0.6292691265812248
Loss in iteration 78 : 0.6278096985531643
Loss in iteration 79 : 0.6263931802206945
Loss in iteration 80 : 0.6250102893642138
Loss in iteration 81 : 0.6236607304122588
Loss in iteration 82 : 0.6223348983981567
Loss in iteration 83 : 0.6210496535826032
Loss in iteration 84 : 0.6197881130619778
Loss in iteration 85 : 0.6185554486912231
Loss in iteration 86 : 0.6173534137994834
Loss in iteration 87 : 0.6161906925620485
Loss in iteration 88 : 0.6150557722981612
Loss in iteration 89 : 0.6139459197216028
Loss in iteration 90 : 0.6128648432678294
Loss in iteration 91 : 0.6118055613920707
Loss in iteration 92 : 0.6107690968381484
Loss in iteration 93 : 0.6097575631501634
Loss in iteration 94 : 0.6087664638095617
Loss in iteration 95 : 0.6077895598116523
Loss in iteration 96 : 0.6068414488302794
Loss in iteration 97 : 0.6059175059536204
Loss in iteration 98 : 0.605013469097144
Loss in iteration 99 : 0.6041213249735559
Loss in iteration 100 : 0.6032476192202546
Loss in iteration 101 : 0.6023941882691292
Loss in iteration 102 : 0.6015541980526111
Loss in iteration 103 : 0.6007292268735476
Loss in iteration 104 : 0.5999213757054558
Loss in iteration 105 : 0.5991273903416612
Loss in iteration 106 : 0.5983529472777716
Loss in iteration 107 : 0.5976002157681316
Loss in iteration 108 : 0.596861834345326
Loss in iteration 109 : 0.5961395588836698
Loss in iteration 110 : 0.5954332584134133
Loss in iteration 111 : 0.5947413961732358
Loss in iteration 112 : 0.594062814137143
Loss in iteration 113 : 0.5933959656808233
Loss in iteration 114 : 0.592745198844615
Loss in iteration 115 : 0.5921017972571607
Loss in iteration 116 : 0.5914702192279029
Loss in iteration 117 : 0.5908469220832121
Loss in iteration 118 : 0.5902346623988055
Loss in iteration 119 : 0.5896288756688193
Loss in iteration 120 : 0.58903094152789
Loss in iteration 121 : 0.5884385173958556
Loss in iteration 122 : 0.5878501178690433
Loss in iteration 123 : 0.5872657016380113
Loss in iteration 124 : 0.5866904441041235
Loss in iteration 125 : 0.5861210277759517
Loss in iteration 126 : 0.5855580672800508
Loss in iteration 127 : 0.5850005428105156
Loss in iteration 128 : 0.584449768151603
Loss in iteration 129 : 0.5839086785084401
Loss in iteration 130 : 0.5833757297284778
Loss in iteration 131 : 0.5828536464309696
Loss in iteration 132 : 0.5823377883671274
Loss in iteration 133 : 0.5818275035365075
Loss in iteration 134 : 0.5813254380179333
Loss in iteration 135 : 0.5808266925158719
Loss in iteration 136 : 0.5803354579837913
Loss in iteration 137 : 0.579855052258014
Loss in iteration 138 : 0.579384264040464
Loss in iteration 139 : 0.5789213963604203
Loss in iteration 140 : 0.578461339432818
Loss in iteration 141 : 0.5780064283685847
Loss in iteration 142 : 0.5775577675861148
Loss in iteration 143 : 0.5771154489502065
Loss in iteration 144 : 0.5766836342513529
Loss in iteration 145 : 0.5762550796307689
Loss in iteration 146 : 0.5758313884091637
Loss in iteration 147 : 0.5754123071326208
Loss in iteration 148 : 0.5749964275751157
Loss in iteration 149 : 0.5745824004267944
Loss in iteration 150 : 0.5741724170063303
Loss in iteration 151 : 0.5737641833053696
Loss in iteration 152 : 0.5733614980507667
Loss in iteration 153 : 0.5729646939763403
Loss in iteration 154 : 0.5725708318268394
Loss in iteration 155 : 0.5721821111010841
Loss in iteration 156 : 0.5717965387496619
Loss in iteration 157 : 0.5714151337766938
Loss in iteration 158 : 0.5710356545720437
Loss in iteration 159 : 0.5706590610174801
Loss in iteration 160 : 0.5702859704304021
Loss in iteration 161 : 0.5699189782785122
Loss in iteration 162 : 0.5695591279320286
Loss in iteration 163 : 0.5692033588248764
Loss in iteration 164 : 0.568852513986146
Loss in iteration 165 : 0.5685051955112499
Loss in iteration 166 : 0.5681612904615734
Loss in iteration 167 : 0.5678228890298123
Loss in iteration 168 : 0.5674892425105554
Loss in iteration 169 : 0.5671598261841657
Loss in iteration 170 : 0.5668328719735729
Loss in iteration 171 : 0.5665086672379422
Loss in iteration 172 : 0.5661882267158198
Loss in iteration 173 : 0.5658710031046774
Loss in iteration 174 : 0.5655581303415607
Loss in iteration 175 : 0.5652506228883595
Loss in iteration 176 : 0.5649455808335483
Loss in iteration 177 : 0.564645477219681
Loss in iteration 178 : 0.5643483484485323
Loss in iteration 179 : 0.5640527271512826
Loss in iteration 180 : 0.5637598120321955
Loss in iteration 181 : 0.563471747505615
Loss in iteration 182 : 0.5631870259776178
Loss in iteration 183 : 0.5629066020969927
Loss in iteration 184 : 0.562630565819872
Loss in iteration 185 : 0.5623572095127584
Loss in iteration 186 : 0.5620858443576899
Loss in iteration 187 : 0.5618155878515385
Loss in iteration 188 : 0.5615468836052728
Loss in iteration 189 : 0.5612808029366282
Loss in iteration 190 : 0.5610158959352952
Loss in iteration 191 : 0.5607538627099131
Loss in iteration 192 : 0.5604938551616355
Loss in iteration 193 : 0.5602374878308592
Loss in iteration 194 : 0.5599861607145095
Loss in iteration 195 : 0.5597377303166584
Loss in iteration 196 : 0.5594914884703726
Loss in iteration 197 : 0.559247475090399
Loss in iteration 198 : 0.5590067737584142
Loss in iteration 199 : 0.5587681573814879
Loss in iteration 200 : 0.5585317191084163
Testing accuracy  of updater 0 on alg 1 with rate 0.1 = 0.767, training accuracy 0.77075, time elapsed: 3898 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.983817271342106
Loss in iteration 3 : 0.9676685448863576
Loss in iteration 4 : 0.9519532238795125
Loss in iteration 5 : 0.9382809326685448
Loss in iteration 6 : 0.9281793595189126
Loss in iteration 7 : 0.9210340714607917
Loss in iteration 8 : 0.9154795755396082
Loss in iteration 9 : 0.9109193801751594
Loss in iteration 10 : 0.9067664806591043
Loss in iteration 11 : 0.9028045924962935
Loss in iteration 12 : 0.8989631625700065
Loss in iteration 13 : 0.8951601824268136
Loss in iteration 14 : 0.8913875808099377
Loss in iteration 15 : 0.887633542523671
Loss in iteration 16 : 0.883896273836625
Loss in iteration 17 : 0.8801785318664733
Loss in iteration 18 : 0.876475541250551
Loss in iteration 19 : 0.8727820133176825
Loss in iteration 20 : 0.8690983287888336
Loss in iteration 21 : 0.8654212056449717
Loss in iteration 22 : 0.8617533057541423
Loss in iteration 23 : 0.8580949245631887
Loss in iteration 24 : 0.8544472874998441
Loss in iteration 25 : 0.8508079429588838
Loss in iteration 26 : 0.847175992429289
Loss in iteration 27 : 0.8435497447727744
Loss in iteration 28 : 0.8399269685734356
Loss in iteration 29 : 0.8363152703630887
Loss in iteration 30 : 0.8327065014538505
Loss in iteration 31 : 0.8291008477718673
Loss in iteration 32 : 0.82549764178901
Loss in iteration 33 : 0.8219010206019761
Loss in iteration 34 : 0.8183099553185946
Loss in iteration 35 : 0.8147212651676701
Loss in iteration 36 : 0.811134321146819
Loss in iteration 37 : 0.8075530591940052
Loss in iteration 38 : 0.8039740640517202
Loss in iteration 39 : 0.8004019422893047
Loss in iteration 40 : 0.7968342385298219
Loss in iteration 41 : 0.7932685618647963
Loss in iteration 42 : 0.789706105232028
Loss in iteration 43 : 0.7861454957506626
Loss in iteration 44 : 0.7825875769428469
Loss in iteration 45 : 0.7790344032990378
Loss in iteration 46 : 0.7754836673586148
Loss in iteration 47 : 0.771938016967681
Loss in iteration 48 : 0.7683947963830219
Loss in iteration 49 : 0.7648552016924307
Loss in iteration 50 : 0.761318733303476
Loss in iteration 51 : 0.7577830051423293
Loss in iteration 52 : 0.7542472769811841
Loss in iteration 53 : 0.7507136388599888
Loss in iteration 54 : 0.7471883036287814
Loss in iteration 55 : 0.743678077045882
Loss in iteration 56 : 0.7402006628665619
Loss in iteration 57 : 0.7367444090423005
Loss in iteration 58 : 0.7333172069577955
Loss in iteration 59 : 0.7299303007339065
Loss in iteration 60 : 0.7265721008497699
Loss in iteration 61 : 0.7232631301593656
Loss in iteration 62 : 0.7200439628457804
Loss in iteration 63 : 0.7169052142799781
Loss in iteration 64 : 0.713838114948182
Loss in iteration 65 : 0.7108473569901489
Loss in iteration 66 : 0.7079292555476615
Loss in iteration 67 : 0.7050840683278418
Loss in iteration 68 : 0.7022905245605751
Loss in iteration 69 : 0.6995682478082347
Loss in iteration 70 : 0.6969136940189106
Loss in iteration 71 : 0.6943118338898701
Loss in iteration 72 : 0.6917557147649822
Loss in iteration 73 : 0.6892537334379392
Loss in iteration 74 : 0.6868248228013274
Loss in iteration 75 : 0.684455611543552
Loss in iteration 76 : 0.6821538791307412
Loss in iteration 77 : 0.6799108393223243
Loss in iteration 78 : 0.677706691502658
Loss in iteration 79 : 0.6755429704393284
Loss in iteration 80 : 0.6734454635790974
Loss in iteration 81 : 0.6714070965326401
Loss in iteration 82 : 0.6694149178438361
Loss in iteration 83 : 0.6674665921877495
Loss in iteration 84 : 0.6655728353517033
Loss in iteration 85 : 0.6637184745879144
Loss in iteration 86 : 0.6619007582388416
Loss in iteration 87 : 0.6601209046239733
Loss in iteration 88 : 0.6583884247623296
Loss in iteration 89 : 0.6567143814340254
Loss in iteration 90 : 0.6550837546277969
Loss in iteration 91 : 0.6534810007500289
Loss in iteration 92 : 0.6519031355791762
Loss in iteration 93 : 0.6503598473948452
Loss in iteration 94 : 0.6488542847076547
Loss in iteration 95 : 0.6473766356697811
Loss in iteration 96 : 0.6459337893600564
Loss in iteration 97 : 0.6445247207931653
Loss in iteration 98 : 0.6431413760458493
Loss in iteration 99 : 0.6417916908785073
Loss in iteration 100 : 0.6404785807436324
Loss in iteration 101 : 0.6391989303312914
Loss in iteration 102 : 0.6379523035442951
Loss in iteration 103 : 0.6367432090879487
Loss in iteration 104 : 0.6355623589964622
Loss in iteration 105 : 0.6344093776659004
Loss in iteration 106 : 0.6332768153467278
Loss in iteration 107 : 0.6321721367550335
Loss in iteration 108 : 0.6310916604931412
Loss in iteration 109 : 0.6300239165835637
Loss in iteration 110 : 0.6289834945590578
Loss in iteration 111 : 0.6279670409323109
Loss in iteration 112 : 0.626969546397743
Loss in iteration 113 : 0.6259913938467827
Loss in iteration 114 : 0.6250291206426705
Loss in iteration 115 : 0.6240822192640683
Loss in iteration 116 : 0.6231484617741002
Loss in iteration 117 : 0.6222252029916852
Loss in iteration 118 : 0.6213246844776247
Loss in iteration 119 : 0.6204376325300165
Loss in iteration 120 : 0.6195617936349764
Loss in iteration 121 : 0.6187003420525443
Loss in iteration 122 : 0.6178538288874661
Loss in iteration 123 : 0.6170256005638096
Loss in iteration 124 : 0.616217201120827
Loss in iteration 125 : 0.6154213211557905
Loss in iteration 126 : 0.6146375458217308
Loss in iteration 127 : 0.6138674505924111
Loss in iteration 128 : 0.6131099084485168
Loss in iteration 129 : 0.6123639142547631
Loss in iteration 130 : 0.6116290198674282
Loss in iteration 131 : 0.6109050586741585
Loss in iteration 132 : 0.6101928325601982
Loss in iteration 133 : 0.6094922407834984
Loss in iteration 134 : 0.6088006388090452
Loss in iteration 135 : 0.6081143058927755
Loss in iteration 136 : 0.6074420401069107
Loss in iteration 137 : 0.6067819508882799
Loss in iteration 138 : 0.6061345169141537
Loss in iteration 139 : 0.6054982894912228
Loss in iteration 140 : 0.6048672958569309
Loss in iteration 141 : 0.6042430110930926
Loss in iteration 142 : 0.6036286427345993
Loss in iteration 143 : 0.6030232069248226
Loss in iteration 144 : 0.602429774559523
Loss in iteration 145 : 0.6018407260726824
Loss in iteration 146 : 0.6012589449726676
Loss in iteration 147 : 0.6006845051700288
Loss in iteration 148 : 0.6001190839881828
Loss in iteration 149 : 0.5995597063467433
Loss in iteration 150 : 0.5990083951126446
Loss in iteration 151 : 0.5984670142990653
Loss in iteration 152 : 0.597936475254453
Loss in iteration 153 : 0.5974149895271047
Loss in iteration 154 : 0.5968986121958825
Loss in iteration 155 : 0.5963918894712226
Loss in iteration 156 : 0.5958921912357223
Loss in iteration 157 : 0.5953995878620029
Loss in iteration 158 : 0.5949160398081343
Loss in iteration 159 : 0.5944380797095837
Loss in iteration 160 : 0.5939665107163994
Loss in iteration 161 : 0.5935007788424268
Loss in iteration 162 : 0.5930440512113392
Loss in iteration 163 : 0.5925903048246874
Loss in iteration 164 : 0.5921413073661903
Loss in iteration 165 : 0.5916986456144256
Loss in iteration 166 : 0.5912595054278843
Loss in iteration 167 : 0.590825557819312
Loss in iteration 168 : 0.5903966401562414
Loss in iteration 169 : 0.5899701860720912
Loss in iteration 170 : 0.5895472165436105
Loss in iteration 171 : 0.5891285494422814
Loss in iteration 172 : 0.5887129753470118
Loss in iteration 173 : 0.5882992556360062
Loss in iteration 174 : 0.58788765632252
Loss in iteration 175 : 0.5874774653749886
Loss in iteration 176 : 0.587071718671263
Loss in iteration 177 : 0.5866697936551367
Loss in iteration 178 : 0.5862709156719552
Loss in iteration 179 : 0.5858752558044237
Loss in iteration 180 : 0.5854826568750401
Loss in iteration 181 : 0.5850923145900498
Loss in iteration 182 : 0.5847051667867258
Loss in iteration 183 : 0.5843217980728405
Loss in iteration 184 : 0.5839436928125405
Loss in iteration 185 : 0.5835697484478066
Loss in iteration 186 : 0.5831996804589471
Loss in iteration 187 : 0.5828356403220039
Loss in iteration 188 : 0.5824744070063715
Loss in iteration 189 : 0.5821156837878568
Loss in iteration 190 : 0.5817612694370812
Loss in iteration 191 : 0.5814097158493434
Loss in iteration 192 : 0.5810596214905072
Loss in iteration 193 : 0.5807128546622068
Loss in iteration 194 : 0.5803691966565184
Loss in iteration 195 : 0.5800314499801753
Loss in iteration 196 : 0.5796985479315255
Loss in iteration 197 : 0.579371002070744
Loss in iteration 198 : 0.5790468044893248
Loss in iteration 199 : 0.5787239022712283
Loss in iteration 200 : 0.5784030742040667
Testing accuracy  of updater 0 on alg 1 with rate 0.07 = 0.7655, training accuracy 0.765875, time elapsed: 4538 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9907527264812034
Loss in iteration 3 : 0.9815054529624048
Loss in iteration 4 : 0.9722619591580626
Loss in iteration 5 : 0.9631313266358689
Loss in iteration 6 : 0.9542154659870289
Loss in iteration 7 : 0.9458759064729215
Loss in iteration 8 : 0.938630429092765
Loss in iteration 9 : 0.9325240100819927
Loss in iteration 10 : 0.927449268871314
Loss in iteration 11 : 0.9233114433868461
Loss in iteration 12 : 0.9197246497280694
Loss in iteration 13 : 0.9165483628156794
Loss in iteration 14 : 0.9137290929119658
Loss in iteration 15 : 0.9111450641809006
Loss in iteration 16 : 0.9087003484586484
Loss in iteration 17 : 0.9063457081765095
Loss in iteration 18 : 0.9040471539305134
Loss in iteration 19 : 0.9018096909346164
Loss in iteration 20 : 0.8996071860644593
Loss in iteration 21 : 0.8974226520924924
Loss in iteration 22 : 0.8952489500373738
Loss in iteration 23 : 0.893089363725645
Loss in iteration 24 : 0.8909371907212219
Loss in iteration 25 : 0.8887908375953971
Loss in iteration 26 : 0.88664957416215
Loss in iteration 27 : 0.8845125750648413
Loss in iteration 28 : 0.8823852193628593
Loss in iteration 29 : 0.8802622363638961
Loss in iteration 30 : 0.8781443684680491
Loss in iteration 31 : 0.876032231623758
Loss in iteration 32 : 0.8739208786184286
Loss in iteration 33 : 0.8718122942481125
Loss in iteration 34 : 0.8697072156703876
Loss in iteration 35 : 0.8676051105627051
Loss in iteration 36 : 0.8655041454024579
Loss in iteration 37 : 0.8634071436637653
Loss in iteration 38 : 0.8613153503496469
Loss in iteration 39 : 0.8592241871337897
Loss in iteration 40 : 0.8571357490663488
Loss in iteration 41 : 0.8550519509563651
Loss in iteration 42 : 0.8529712122314379
Loss in iteration 43 : 0.8508926824857801
Loss in iteration 44 : 0.8488166272753546
Loss in iteration 45 : 0.8467427713954462
Loss in iteration 46 : 0.844670675065021
Loss in iteration 47 : 0.8425989196783242
Loss in iteration 48 : 0.8405304012252314
Loss in iteration 49 : 0.8384641778593211
Loss in iteration 50 : 0.8364006485132527
Loss in iteration 51 : 0.8343384220975821
Loss in iteration 52 : 0.8322765918703173
Loss in iteration 53 : 0.8302169551304204
Loss in iteration 54 : 0.82815731839052
Loss in iteration 55 : 0.8260982233389359
Loss in iteration 56 : 0.8240421329757402
Loss in iteration 57 : 0.8219884627359534
Loss in iteration 58 : 0.8199361029709584
Loss in iteration 59 : 0.817885044783653
Loss in iteration 60 : 0.8158341644258439
Loss in iteration 61 : 0.8137837705248582
Loss in iteration 62 : 0.8117335036922424
Loss in iteration 63 : 0.8096848316928082
Loss in iteration 64 : 0.8076383533097337
Loss in iteration 65 : 0.805592591104612
Loss in iteration 66 : 0.8035485693269584
Loss in iteration 67 : 0.8015071669610226
Loss in iteration 68 : 0.7994673164162869
Loss in iteration 69 : 0.7974286004487592
Loss in iteration 70 : 0.7953900015915883
Loss in iteration 71 : 0.793352975337644
Loss in iteration 72 : 0.7913170843425541
Loss in iteration 73 : 0.7892814820401546
Loss in iteration 74 : 0.7872466192554134
Loss in iteration 75 : 0.785212880674884
Loss in iteration 76 : 0.7831798364596739
Loss in iteration 77 : 0.7811484456309786
Loss in iteration 78 : 0.7791180414920459
Loss in iteration 79 : 0.7770885593525664
Loss in iteration 80 : 0.7750613377736258
Loss in iteration 81 : 0.7730352591985522
Loss in iteration 82 : 0.7710098588283529
Loss in iteration 83 : 0.7689855061055759
Loss in iteration 84 : 0.7669616346226386
Loss in iteration 85 : 0.7649396500436381
Loss in iteration 86 : 0.7629188088897242
Loss in iteration 87 : 0.7608981016966305
Loss in iteration 88 : 0.7588776881242362
Loss in iteration 89 : 0.756857289200851
Loss in iteration 90 : 0.7548368911437653
Loss in iteration 91 : 0.7528164905669843
Loss in iteration 92 : 0.7507977358415194
Loss in iteration 93 : 0.7487823624726438
Loss in iteration 94 : 0.746769304186138
Loss in iteration 95 : 0.7447647834905244
Loss in iteration 96 : 0.7427680938058658
Loss in iteration 97 : 0.7407818037993641
Loss in iteration 98 : 0.7388032802155099
Loss in iteration 99 : 0.7368295384785704
Loss in iteration 100 : 0.7348690012159252
Loss in iteration 101 : 0.7329187285863238
Loss in iteration 102 : 0.730982932301952
Loss in iteration 103 : 0.729054320469995
Loss in iteration 104 : 0.7271415644144706
Loss in iteration 105 : 0.7252409829879845
Loss in iteration 106 : 0.7233549033547332
Loss in iteration 107 : 0.7215066901894236
Loss in iteration 108 : 0.7196897691790856
Loss in iteration 109 : 0.7178959106693497
Loss in iteration 110 : 0.716121723367052
Loss in iteration 111 : 0.7143750065661065
Loss in iteration 112 : 0.7126489018800823
Loss in iteration 113 : 0.7109519166716034
Loss in iteration 114 : 0.7092774371911716
Loss in iteration 115 : 0.7076304724586343
Loss in iteration 116 : 0.7060048700830891
Loss in iteration 117 : 0.7043942377402912
Loss in iteration 118 : 0.7028035602836115
Loss in iteration 119 : 0.7012353785948592
Loss in iteration 120 : 0.6996844939001702
Loss in iteration 121 : 0.6981615493211516
Loss in iteration 122 : 0.6966530982498252
Loss in iteration 123 : 0.6951641928913493
Loss in iteration 124 : 0.6936922072024138
Loss in iteration 125 : 0.6922336837502118
Loss in iteration 126 : 0.6907889865151147
Loss in iteration 127 : 0.6893694274618714
Loss in iteration 128 : 0.687974750768898
Loss in iteration 129 : 0.6866010159741915
Loss in iteration 130 : 0.6852446777175589
Loss in iteration 131 : 0.6839100101495794
Loss in iteration 132 : 0.6826011011032862
Loss in iteration 133 : 0.6813135666141734
Loss in iteration 134 : 0.6800399669238115
Loss in iteration 135 : 0.6787780061271504
Loss in iteration 136 : 0.6775257557704414
Loss in iteration 137 : 0.6762857645340679
Loss in iteration 138 : 0.6750680573141222
Loss in iteration 139 : 0.6738707444434149
Loss in iteration 140 : 0.6726946261248535
Loss in iteration 141 : 0.6715365121006791
Loss in iteration 142 : 0.6703948758737913
Loss in iteration 143 : 0.6692656048164936
Loss in iteration 144 : 0.668151563694993
Loss in iteration 145 : 0.6670610602268969
Loss in iteration 146 : 0.665979726984357
Loss in iteration 147 : 0.6649142896532714
Loss in iteration 148 : 0.6638581658142373
Loss in iteration 149 : 0.6628155945280872
Loss in iteration 150 : 0.6617817855576321
Loss in iteration 151 : 0.660762365955152
Loss in iteration 152 : 0.6597587993019037
Loss in iteration 153 : 0.6587700399969826
Loss in iteration 154 : 0.657802887416269
Loss in iteration 155 : 0.6568523810940795
Loss in iteration 156 : 0.655917725878295
Loss in iteration 157 : 0.6549943958042065
Loss in iteration 158 : 0.6540774149901215
Loss in iteration 159 : 0.6531686177915276
Loss in iteration 160 : 0.6522685948810059
Loss in iteration 161 : 0.6513794674433324
Loss in iteration 162 : 0.6505010687048116
Loss in iteration 163 : 0.6496367334072813
Loss in iteration 164 : 0.6487826200897968
Loss in iteration 165 : 0.6479372746160824
Loss in iteration 166 : 0.6471028757034891
Loss in iteration 167 : 0.6462796005979533
Loss in iteration 168 : 0.6454686176152816
Loss in iteration 169 : 0.6446658422670035
Loss in iteration 170 : 0.6438709038010141
Loss in iteration 171 : 0.6430861621477885
Loss in iteration 172 : 0.6423153896296545
Loss in iteration 173 : 0.6415528329532375
Loss in iteration 174 : 0.6408039848534761
Loss in iteration 175 : 0.6400667908040284
Loss in iteration 176 : 0.6393397614432278
Loss in iteration 177 : 0.638623454376255
Loss in iteration 178 : 0.6379191202139168
Loss in iteration 179 : 0.6372268151281292
Loss in iteration 180 : 0.6365436048290222
Loss in iteration 181 : 0.6358683554189445
Loss in iteration 182 : 0.6352034232528411
Loss in iteration 183 : 0.6345464370646835
Loss in iteration 184 : 0.6338955726612906
Loss in iteration 185 : 0.6332534365027059
Loss in iteration 186 : 0.6326200758386286
Loss in iteration 187 : 0.6319979429695846
Loss in iteration 188 : 0.6313807120342667
Loss in iteration 189 : 0.6307661607640033
Loss in iteration 190 : 0.6301575950173078
Loss in iteration 191 : 0.6295593540507898
Loss in iteration 192 : 0.6289686564372187
Loss in iteration 193 : 0.6283867952644032
Loss in iteration 194 : 0.6278117625855857
Loss in iteration 195 : 0.6272427318220819
Loss in iteration 196 : 0.6266801302426815
Loss in iteration 197 : 0.6261215196310292
Loss in iteration 198 : 0.6255686490029189
Loss in iteration 199 : 0.6250208954458985
Loss in iteration 200 : 0.6244790759989264
Testing accuracy  of updater 0 on alg 1 with rate 0.04000000000000001 = 0.7565, training accuracy 0.75125, time elapsed: 3964 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9976881816203013
Loss in iteration 3 : 0.9953763632406024
Loss in iteration 4 : 0.9930645448609035
Loss in iteration 5 : 0.9907527264812034
Loss in iteration 6 : 0.9884409081015044
Loss in iteration 7 : 0.9861290897218049
Loss in iteration 8 : 0.983817271342106
Loss in iteration 9 : 0.9815054529624048
Loss in iteration 10 : 0.9791936345827075
Loss in iteration 11 : 0.976881816203008
Loss in iteration 12 : 0.9745699978233086
Loss in iteration 13 : 0.9722619591580626
Loss in iteration 14 : 0.9699715321843873
Loss in iteration 15 : 0.9676897603573344
Loss in iteration 16 : 0.9654182847667475
Loss in iteration 17 : 0.9631500688989645
Loss in iteration 18 : 0.9609098123075003
Loss in iteration 19 : 0.9586766970916585
Loss in iteration 20 : 0.9564558880678208
Loss in iteration 21 : 0.9542609849068372
Loss in iteration 22 : 0.952112090507517
Loss in iteration 23 : 0.9500465454640541
Loss in iteration 24 : 0.9480636490633402
Loss in iteration 25 : 0.9461332838809831
Loss in iteration 26 : 0.9442544569456638
Loss in iteration 27 : 0.9424321278711372
Loss in iteration 28 : 0.9406933137856401
Loss in iteration 29 : 0.9390111372041502
Loss in iteration 30 : 0.9374135381686763
Loss in iteration 31 : 0.9358937633903817
Loss in iteration 32 : 0.9344463041983365
Loss in iteration 33 : 0.933033263600153
Loss in iteration 34 : 0.9316537875668234
Loss in iteration 35 : 0.9303618597386216
Loss in iteration 36 : 0.9291366311502167
Loss in iteration 37 : 0.9279822617995197
Loss in iteration 38 : 0.9268833619393742
Loss in iteration 39 : 0.9258270135912214
Loss in iteration 40 : 0.9248079873113183
Loss in iteration 41 : 0.9238193123498513
Loss in iteration 42 : 0.9228623250327632
Loss in iteration 43 : 0.9219355804813378
Loss in iteration 44 : 0.9210544508862261
Loss in iteration 45 : 0.9201962648058459
Loss in iteration 46 : 0.9193527426661456
Loss in iteration 47 : 0.9185275372460208
Loss in iteration 48 : 0.9177280881395277
Loss in iteration 49 : 0.9169485217749088
Loss in iteration 50 : 0.9161971238955848
Loss in iteration 51 : 0.9154642181333066
Loss in iteration 52 : 0.9147511450332357
Loss in iteration 53 : 0.9140594381578346
Loss in iteration 54 : 0.9133839807582781
Loss in iteration 55 : 0.9127203523686697
Loss in iteration 56 : 0.9120688078742948
Loss in iteration 57 : 0.9114281855641102
Loss in iteration 58 : 0.9107929917617257
Loss in iteration 59 : 0.9101666085343322
Loss in iteration 60 : 0.9095464992174166
Loss in iteration 61 : 0.9089347378764339
Loss in iteration 62 : 0.9083321828660539
Loss in iteration 63 : 0.9077339919880433
Loss in iteration 64 : 0.9071425250742756
Loss in iteration 65 : 0.9065510594978748
Loss in iteration 66 : 0.9059616191952184
Loss in iteration 67 : 0.9053762019400465
Loss in iteration 68 : 0.9047958113197292
Loss in iteration 69 : 0.9042200924897607
Loss in iteration 70 : 0.9036481182740406
Loss in iteration 71 : 0.903077785919243
Loss in iteration 72 : 0.9025131726475549
Loss in iteration 73 : 0.901951826972614
Loss in iteration 74 : 0.901393169348374
Loss in iteration 75 : 0.9008367470312496
Loss in iteration 76 : 0.9002818924549564
Loss in iteration 77 : 0.8997309208464649
Loss in iteration 78 : 0.899181572285393
Loss in iteration 79 : 0.8986340300475492
Loss in iteration 80 : 0.8980870756204579
Loss in iteration 81 : 0.89754064077616
Loss in iteration 82 : 0.8969952223060292
Loss in iteration 83 : 0.8964500368601914
Loss in iteration 84 : 0.8959055462836708
Loss in iteration 85 : 0.8953621339271395
Loss in iteration 86 : 0.8948195244525832
Loss in iteration 87 : 0.8942779493058082
Loss in iteration 88 : 0.8937372145442531
Loss in iteration 89 : 0.8931970030767542
Loss in iteration 90 : 0.8926577462343258
Loss in iteration 91 : 0.8921186770329536
Loss in iteration 92 : 0.8915799960474815
Loss in iteration 93 : 0.8910421155684699
Loss in iteration 94 : 0.890504269116014
Loss in iteration 95 : 0.8899665552115175
Loss in iteration 96 : 0.8894294563531306
Loss in iteration 97 : 0.8888925885054285
Loss in iteration 98 : 0.8883560752247834
Loss in iteration 99 : 0.8878204713969446
Loss in iteration 100 : 0.8872848679082154
Loss in iteration 101 : 0.8867493752112455
Loss in iteration 102 : 0.8862139002601643
Loss in iteration 103 : 0.8856791318963858
Loss in iteration 104 : 0.885144520650298
Loss in iteration 105 : 0.884610425172139
Loss in iteration 106 : 0.8840768457788714
Loss in iteration 107 : 0.8835449292609747
Loss in iteration 108 : 0.8830130593267354
Loss in iteration 109 : 0.882481717373521
Loss in iteration 110 : 0.8819504761295514
Loss in iteration 111 : 0.8814194335908697
Loss in iteration 112 : 0.8808886140346973
Loss in iteration 113 : 0.8803579310432376
Loss in iteration 114 : 0.8798279118561116
Loss in iteration 115 : 0.8792985437534722
Loss in iteration 116 : 0.878769318489875
Loss in iteration 117 : 0.8782405078576956
Loss in iteration 118 : 0.8777116972255198
Loss in iteration 119 : 0.8771831891324942
Loss in iteration 120 : 0.8766551549214217
Loss in iteration 121 : 0.8761271207103505
Loss in iteration 122 : 0.8755990864992775
Loss in iteration 123 : 0.8750711628382062
Loss in iteration 124 : 0.8745435348125211
Loss in iteration 125 : 0.8740159372267619
Loss in iteration 126 : 0.8734884956749209
Loss in iteration 127 : 0.8729610880853498
Loss in iteration 128 : 0.8724338998916248
Loss in iteration 129 : 0.8719068417314284
Loss in iteration 130 : 0.8713803029420732
Loss in iteration 131 : 0.8708539087393178
Loss in iteration 132 : 0.8703277369745795
Loss in iteration 133 : 0.8698015732092392
Loss in iteration 134 : 0.869275651994125
Loss in iteration 135 : 0.8687501017627958
Loss in iteration 136 : 0.8682246530442664
Loss in iteration 137 : 0.8676992043257384
Loss in iteration 138 : 0.8671737556072082
Loss in iteration 139 : 0.8666483081975023
Loss in iteration 140 : 0.8661228755073302
Loss in iteration 141 : 0.8655974886903713
Loss in iteration 142 : 0.8650725942999061
Loss in iteration 143 : 0.8645481018885458
Loss in iteration 144 : 0.8640243809492996
Loss in iteration 145 : 0.8635011206710297
Loss in iteration 146 : 0.8629781653428295
Loss in iteration 147 : 0.8624552417456305
Loss in iteration 148 : 0.8619323601802339
Loss in iteration 149 : 0.8614094786148403
Loss in iteration 150 : 0.8608865970494448
Loss in iteration 151 : 0.8603637154840523
Loss in iteration 152 : 0.8598408991839187
Loss in iteration 153 : 0.8593181845868323
Loss in iteration 154 : 0.8587955259266608
Loss in iteration 155 : 0.8582731666040476
Loss in iteration 156 : 0.8577511799504329
Loss in iteration 157 : 0.8572299799278001
Loss in iteration 158 : 0.8567087994646131
Loss in iteration 159 : 0.8561876390749068
Loss in iteration 160 : 0.8556665123306543
Loss in iteration 161 : 0.8551456033820496
Loss in iteration 162 : 0.8546249446184968
Loss in iteration 163 : 0.854104762654329
Loss in iteration 164 : 0.8535848631720557
Loss in iteration 165 : 0.8530649636897792
Loss in iteration 166 : 0.852545064207505
Loss in iteration 167 : 0.8520251857251497
Loss in iteration 168 : 0.8515056620936605
Loss in iteration 169 : 0.8509862795895522
Loss in iteration 170 : 0.8504671974876647
Loss in iteration 171 : 0.8499481153857774
Loss in iteration 172 : 0.8494290332838929
Loss in iteration 173 : 0.8489100222947947
Loss in iteration 174 : 0.8483912793388202
Loss in iteration 175 : 0.8478727733262401
Loss in iteration 176 : 0.8473542673136569
Loss in iteration 177 : 0.8468357613010775
Loss in iteration 178 : 0.8463172552884958
Loss in iteration 179 : 0.8457989060070684
Loss in iteration 180 : 0.8452808562321527
Loss in iteration 181 : 0.8447628175663067
Loss in iteration 182 : 0.8442447934837
Loss in iteration 183 : 0.8437267694010937
Loss in iteration 184 : 0.8432089469669166
Loss in iteration 185 : 0.8426916686864868
Loss in iteration 186 : 0.8421743904060571
Loss in iteration 187 : 0.841657247311691
Loss in iteration 188 : 0.8411403534782148
Loss in iteration 189 : 0.8406234596447396
Loss in iteration 190 : 0.8401065658112612
Loss in iteration 191 : 0.8395896719777859
Loss in iteration 192 : 0.8390731198876895
Loss in iteration 193 : 0.8385572537528986
Loss in iteration 194 : 0.8380413876181088
Loss in iteration 195 : 0.8375255214833205
Loss in iteration 196 : 0.8370096553485318
Loss in iteration 197 : 0.8364938997086915
Loss in iteration 198 : 0.8359783190576815
Loss in iteration 199 : 0.8354627504952024
Loss in iteration 200 : 0.8349472677504047
Testing accuracy  of updater 0 on alg 1 with rate 0.009999999999999995 = 0.6455, training accuracy 0.663375, time elapsed: 3240 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0004925100680415
Loss in iteration 3 : 1.2941204768357544
Loss in iteration 4 : 0.9869333759951523
Loss in iteration 5 : 0.6279215581784685
Loss in iteration 6 : 0.9669243396348883
Loss in iteration 7 : 0.7506677840090303
Loss in iteration 8 : 0.5807722048770447
Loss in iteration 9 : 0.80079120898707
Loss in iteration 10 : 0.5797757439926047
Loss in iteration 11 : 0.7613239690791646
Loss in iteration 12 : 0.5990487646239733
Loss in iteration 13 : 0.7215683046466114
Loss in iteration 14 : 0.6128323791985686
Loss in iteration 15 : 0.7020067403734149
Loss in iteration 16 : 0.6257380716081998
Loss in iteration 17 : 0.6734535018716433
Loss in iteration 18 : 0.6407173742476837
Loss in iteration 19 : 0.6391008925753726
Loss in iteration 20 : 0.6450098827148335
Loss in iteration 21 : 0.6116940975356475
Loss in iteration 22 : 0.6355327057543004
Loss in iteration 23 : 0.5915596420721986
Loss in iteration 24 : 0.6216931028532577
Loss in iteration 25 : 0.5780843959379723
Loss in iteration 26 : 0.6022288914744495
Loss in iteration 27 : 0.5638436930433156
Loss in iteration 28 : 0.583177973221154
Loss in iteration 29 : 0.5564606507872395
Loss in iteration 30 : 0.5621221414329337
Loss in iteration 31 : 0.5550539263016312
Loss in iteration 32 : 0.5407641891124879
Loss in iteration 33 : 0.5538893047661586
Loss in iteration 34 : 0.5309323559725353
Loss in iteration 35 : 0.5477819095212355
Loss in iteration 36 : 0.5424085326720218
Loss in iteration 37 : 0.5286521707939135
Loss in iteration 38 : 0.5507845839484271
Loss in iteration 39 : 0.5403750222870712
Loss in iteration 40 : 0.525618813866677
Loss in iteration 41 : 0.5427454663006522
Loss in iteration 42 : 0.5465949274197861
Loss in iteration 43 : 0.5262500365373608
Loss in iteration 44 : 0.5265489616269472
Loss in iteration 45 : 0.5402802587087517
Loss in iteration 46 : 0.5345808077033746
Loss in iteration 47 : 0.518947714469766
Loss in iteration 48 : 0.5196290041124025
Loss in iteration 49 : 0.5280865260535832
Loss in iteration 50 : 0.5233489003169282
Loss in iteration 51 : 0.5129999399401804
Loss in iteration 52 : 0.5158373111738019
Loss in iteration 53 : 0.5228165285562438
Loss in iteration 54 : 0.5165511570401122
Loss in iteration 55 : 0.5104449923545228
Loss in iteration 56 : 0.5106460729604464
Loss in iteration 57 : 0.5143668759754292
Loss in iteration 58 : 0.5160173236045666
Loss in iteration 59 : 0.5101039220951792
Loss in iteration 60 : 0.5092287714886349
Loss in iteration 61 : 0.5125556447434624
Loss in iteration 62 : 0.5117122291270024
Loss in iteration 63 : 0.5096033221871044
Loss in iteration 64 : 0.5080081037062807
Loss in iteration 65 : 0.508636783304695
Loss in iteration 66 : 0.5104960626474698
Loss in iteration 67 : 0.510879026501632
Loss in iteration 68 : 0.5105998819743242
Loss in iteration 69 : 0.5084032817627879
Loss in iteration 70 : 0.5072521432865742
Loss in iteration 71 : 0.5075260547233387
Loss in iteration 72 : 0.5084062243476072
Loss in iteration 73 : 0.509147714976146
Loss in iteration 74 : 0.5085963175756893
Loss in iteration 75 : 0.5079420091868232
Loss in iteration 76 : 0.5073018892374094
Loss in iteration 77 : 0.5072093595726443
Loss in iteration 78 : 0.507655887882906
Loss in iteration 79 : 0.5081630069990708
Loss in iteration 80 : 0.5092139040307132
Loss in iteration 81 : 0.5087033809755792
Loss in iteration 82 : 0.508532085078392
Loss in iteration 83 : 0.507314109721098
Loss in iteration 84 : 0.5070850402550414
Loss in iteration 85 : 0.5079273310211038
Loss in iteration 86 : 0.5082591644842733
Loss in iteration 87 : 0.5086062463960392
Loss in iteration 88 : 0.5075710963596113
Loss in iteration 89 : 0.507049450856526
Loss in iteration 90 : 0.5067927400496992
Loss in iteration 91 : 0.506874223669715
Loss in iteration 92 : 0.5070502949566913
Loss in iteration 93 : 0.507188889802531
Loss in iteration 94 : 0.507603889613731
Loss in iteration 95 : 0.5075802583899349
Loss in iteration 96 : 0.5072597278169395
Loss in iteration 97 : 0.5067189755440775
Loss in iteration 98 : 0.506774975385833
Loss in iteration 99 : 0.5073323963149995
Loss in iteration 100 : 0.5078930635218597
Loss in iteration 101 : 0.5080272574496011
Loss in iteration 102 : 0.5073975459802429
Loss in iteration 103 : 0.5068968376518495
Loss in iteration 104 : 0.5065987579341747
Loss in iteration 105 : 0.5067942045855243
Loss in iteration 106 : 0.507309443656994
Loss in iteration 107 : 0.5076270600754297
Loss in iteration 108 : 0.5074608772724924
Loss in iteration 109 : 0.5068912171888923
Loss in iteration 110 : 0.5065821209886998
Loss in iteration 111 : 0.5069016771597012
Loss in iteration 112 : 0.5071770765341479
Loss in iteration 113 : 0.5074094437682111
Loss in iteration 114 : 0.5071950829491869
Loss in iteration 115 : 0.5068737336064637
Loss in iteration 116 : 0.5065667926509024
Loss in iteration 117 : 0.5065079898196476
Loss in iteration 118 : 0.5066815473952303
Loss in iteration 119 : 0.5068127311727966
Loss in iteration 120 : 0.506791069943819
Loss in iteration 121 : 0.5065934847624014
Loss in iteration 122 : 0.506474670562981
Loss in iteration 123 : 0.5064905789743274
Loss in iteration 124 : 0.5065372455325201
Loss in iteration 125 : 0.5065223851103634
Loss in iteration 126 : 0.5064654117578865
Loss in iteration 127 : 0.506431094339356
Loss in iteration 128 : 0.5064816300001824
Loss in iteration 129 : 0.5065736463436304
Loss in iteration 130 : 0.5066024471216517
Loss in iteration 131 : 0.5066028499756152
Loss in iteration 132 : 0.5065189260666949
Loss in iteration 133 : 0.5064131568478372
Loss in iteration 134 : 0.5064188241176903
Loss in iteration 135 : 0.5065625076068785
Loss in iteration 136 : 0.5067104507148472
Loss in iteration 137 : 0.5066922799775285
Loss in iteration 138 : 0.5065613946208984
Loss in iteration 139 : 0.5064156277244121
Loss in iteration 140 : 0.5063900422841597
Loss in iteration 141 : 0.5064207643124754
Loss in iteration 142 : 0.5064708104002221
Loss in iteration 143 : 0.5064873507836267
Loss in iteration 144 : 0.5065210700562855
Loss in iteration 145 : 0.5065118844236355
Loss in iteration 146 : 0.5064747319661196
Loss in iteration 147 : 0.5063646894957289
Loss in iteration 148 : 0.506466401006958
Loss in iteration 149 : 0.506865532134009
Loss in iteration 150 : 0.5073085012377629
Loss in iteration 151 : 0.5076965170496895
Loss in iteration 152 : 0.507013625291557
Loss in iteration 153 : 0.5065066718826103
Loss in iteration 154 : 0.5064272018827987
Loss in iteration 155 : 0.5068306656111295
Loss in iteration 156 : 0.5072833767188879
Loss in iteration 157 : 0.5071297973607936
Loss in iteration 158 : 0.5068109714984527
Loss in iteration 159 : 0.50645209476562
Loss in iteration 160 : 0.5065010878866912
Loss in iteration 161 : 0.5070344418411347
Loss in iteration 162 : 0.5073191616618453
Loss in iteration 163 : 0.5071845210008296
Loss in iteration 164 : 0.5066377422350575
Loss in iteration 165 : 0.5063813750006521
Loss in iteration 166 : 0.5064889074869815
Loss in iteration 167 : 0.5068023208876073
Loss in iteration 168 : 0.5070199730738517
Loss in iteration 169 : 0.5068256374635011
Loss in iteration 170 : 0.5065513151437704
Loss in iteration 171 : 0.5063422967775874
Loss in iteration 172 : 0.5063877416312308
Loss in iteration 173 : 0.5066355298386688
Loss in iteration 174 : 0.5069756625988244
Loss in iteration 175 : 0.5070560642861912
Loss in iteration 176 : 0.5066486933360962
Loss in iteration 177 : 0.5063847854966292
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.7825, training accuracy 0.788875, time elapsed: 2650 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.927212173608185
Loss in iteration 3 : 0.8230541546166088
Loss in iteration 4 : 0.759612197326236
Loss in iteration 5 : 0.7048773164236966
Loss in iteration 6 : 0.5817969357096262
Loss in iteration 7 : 0.6279900256895043
Loss in iteration 8 : 0.5932213419834869
Loss in iteration 9 : 0.5753793269020809
Loss in iteration 10 : 0.597912269254813
Loss in iteration 11 : 0.5707781286893933
Loss in iteration 12 : 0.5913802428754984
Loss in iteration 13 : 0.5749457933934322
Loss in iteration 14 : 0.5831286145163953
Loss in iteration 15 : 0.5848273556477487
Loss in iteration 16 : 0.5737212434857454
Loss in iteration 17 : 0.5885102318037924
Loss in iteration 18 : 0.5689806834305435
Loss in iteration 19 : 0.5813597470269318
Loss in iteration 20 : 0.5670252792247474
Loss in iteration 21 : 0.5698835660040563
Loss in iteration 22 : 0.5641538647098847
Loss in iteration 23 : 0.5587048230601193
Loss in iteration 24 : 0.5591251620010506
Loss in iteration 25 : 0.5470418807245057
Loss in iteration 26 : 0.5499022680863807
Loss in iteration 27 : 0.5377151203216943
Loss in iteration 28 : 0.5403860204620461
Loss in iteration 29 : 0.5302587097458982
Loss in iteration 30 : 0.5319478721963314
Loss in iteration 31 : 0.5245034716737457
Loss in iteration 32 : 0.5255780686954882
Loss in iteration 33 : 0.5206614546168187
Loss in iteration 34 : 0.5222000970852232
Loss in iteration 35 : 0.5182081563471468
Loss in iteration 36 : 0.5194462274283224
Loss in iteration 37 : 0.5171323852150538
Loss in iteration 38 : 0.5175718586941286
Loss in iteration 39 : 0.5177411548747134
Loss in iteration 40 : 0.51622821863345
Loss in iteration 41 : 0.5180047806393625
Loss in iteration 42 : 0.515979186087094
Loss in iteration 43 : 0.5173819388582512
Loss in iteration 44 : 0.5162654113069534
Loss in iteration 45 : 0.5160015396918215
Loss in iteration 46 : 0.5161290590609003
Loss in iteration 47 : 0.5146166073450233
Loss in iteration 48 : 0.515380984379155
Loss in iteration 49 : 0.5136180099913193
Loss in iteration 50 : 0.5133106448199912
Loss in iteration 51 : 0.5127193044007184
Loss in iteration 52 : 0.5116960712562137
Loss in iteration 53 : 0.5116488343714485
Loss in iteration 54 : 0.5107831329727255
Loss in iteration 55 : 0.5104608731438245
Loss in iteration 56 : 0.5102304916330347
Loss in iteration 57 : 0.5096711064537016
Loss in iteration 58 : 0.5096542728929376
Loss in iteration 59 : 0.5091552881746404
Loss in iteration 60 : 0.5091263600779807
Loss in iteration 61 : 0.5088733731638541
Loss in iteration 62 : 0.5087491709898474
Loss in iteration 63 : 0.5087410572264344
Loss in iteration 64 : 0.5085796067612199
Loss in iteration 65 : 0.508696860609578
Loss in iteration 66 : 0.5084779988496373
Loss in iteration 67 : 0.5084421657068593
Loss in iteration 68 : 0.5083487567883551
Loss in iteration 69 : 0.5082163268634275
Loss in iteration 70 : 0.5081941400793981
Loss in iteration 71 : 0.5080410489493228
Loss in iteration 72 : 0.5080362807296555
Loss in iteration 73 : 0.5079758983349167
Loss in iteration 74 : 0.5078311953341788
Loss in iteration 75 : 0.5078874435810918
Loss in iteration 76 : 0.5077315913483297
Loss in iteration 77 : 0.5076517361496002
Loss in iteration 78 : 0.5077213460713135
Loss in iteration 79 : 0.5075687125840438
Loss in iteration 80 : 0.5075723947783849
Loss in iteration 81 : 0.507608281206697
Loss in iteration 82 : 0.507472910540036
Loss in iteration 83 : 0.5075264172513789
Loss in iteration 84 : 0.5074593672864414
Loss in iteration 85 : 0.5074408071808093
Loss in iteration 86 : 0.5074821399914883
Loss in iteration 87 : 0.5073844575571518
Loss in iteration 88 : 0.5074685702099196
Loss in iteration 89 : 0.5073871491998283
Loss in iteration 90 : 0.5073699506545081
Loss in iteration 91 : 0.5073792369354388
Loss in iteration 92 : 0.5072658446806011
Loss in iteration 93 : 0.5073475955692631
Loss in iteration 94 : 0.5072322283867249
Loss in iteration 95 : 0.5072535063772549
Loss in iteration 96 : 0.5072616265526307
Loss in iteration 97 : 0.507155450060362
Loss in iteration 98 : 0.507203794689942
Loss in iteration 99 : 0.507122082931394
Loss in iteration 100 : 0.5070900992244566
Loss in iteration 101 : 0.5070941890029939
Loss in iteration 102 : 0.5070563753859187
Loss in iteration 103 : 0.5070520391888329
Loss in iteration 104 : 0.5070466561049188
Loss in iteration 105 : 0.5070106149296773
Loss in iteration 106 : 0.5070034042095213
Loss in iteration 107 : 0.5070043491624101
Loss in iteration 108 : 0.5069662234244833
Loss in iteration 109 : 0.5069771565364768
Loss in iteration 110 : 0.5069874189848249
Loss in iteration 111 : 0.5069393180259595
Loss in iteration 112 : 0.5069312749341821
Loss in iteration 113 : 0.5069347329722774
Loss in iteration 114 : 0.5068908605118675
Loss in iteration 115 : 0.5068869712076477
Loss in iteration 116 : 0.5068758192029796
Loss in iteration 117 : 0.5068507761934866
Loss in iteration 118 : 0.5068467605310242
Loss in iteration 119 : 0.5068411933788575
Loss in iteration 120 : 0.5068146850330592
Loss in iteration 121 : 0.5068094952764913
Loss in iteration 122 : 0.5067980560817881
Loss in iteration 123 : 0.5067856653707818
Loss in iteration 124 : 0.5067724908668058
Loss in iteration 125 : 0.5067685528652529
Loss in iteration 126 : 0.5067534637270191
Loss in iteration 127 : 0.5067437887388467
Loss in iteration 128 : 0.506734648062725
Loss in iteration 129 : 0.5067246390430957
Loss in iteration 130 : 0.5067180133684809
Loss in iteration 131 : 0.5067062085620321
Loss in iteration 132 : 0.5066983758840238
Loss in iteration 133 : 0.5066935531062279
Loss in iteration 134 : 0.5066841733552379
Loss in iteration 135 : 0.5066737846789817
Loss in iteration 136 : 0.5066692943345047
Loss in iteration 137 : 0.5066607646170856
Loss in iteration 138 : 0.5066500886084088
Loss in iteration 139 : 0.5066461406080894
Loss in iteration 140 : 0.5066388863217212
Loss in iteration 141 : 0.5066312744026464
Loss in iteration 142 : 0.5066212938537669
Loss in iteration 143 : 0.506616339302351
Loss in iteration 144 : 0.50660918526416
Loss in iteration 145 : 0.5066017161996273
Loss in iteration 146 : 0.5065945167790813
Loss in iteration 147 : 0.5065883186867296
Loss in iteration 148 : 0.5065825232217003
Loss in iteration 149 : 0.5065792810006012
Loss in iteration 150 : 0.5065796015649563
Loss in iteration 151 : 0.5065665057566942
Loss in iteration 152 : 0.5065764487427025
Loss in iteration 153 : 0.5065666764609358
Loss in iteration 154 : 0.5065531884751578
Loss in iteration 155 : 0.5065700686269394
Loss in iteration 156 : 0.5065465880445397
Loss in iteration 157 : 0.5065414350870548
Loss in iteration 158 : 0.5065596816756832
Loss in iteration 159 : 0.5065301814922862
Loss in iteration 160 : 0.5065279173164794
Loss in iteration 161 : 0.5065496907794956
Loss in iteration 162 : 0.5065149702401676
Loss in iteration 163 : 0.5065228918430642
Loss in iteration 164 : 0.5065165542108893
Loss in iteration 165 : 0.506496553216982
Loss in iteration 166 : 0.506505451595
Loss in iteration 167 : 0.5064883220626522
Loss in iteration 168 : 0.5064929177620207
Loss in iteration 169 : 0.5065125829576165
Loss in iteration 170 : 0.5064731304856903
Loss in iteration 171 : 0.5065023450067235
Loss in iteration 172 : 0.5064979787248578
Loss in iteration 173 : 0.5064734713811124
Loss in iteration 174 : 0.5064981796278614
Loss in iteration 175 : 0.5064631438820586
Loss in iteration 176 : 0.5064769668763491
Loss in iteration 177 : 0.5064618142425503
Loss in iteration 178 : 0.506450293102105
Loss in iteration 179 : 0.5064699669491423
Loss in iteration 180 : 0.5064401041946006
Loss in iteration 181 : 0.5064526472136304
Loss in iteration 182 : 0.506484561225319
Loss in iteration 183 : 0.5064378385129235
Loss in iteration 184 : 0.5064929808828107
Loss in iteration 185 : 0.5064523371128189
Testing accuracy  of updater 1 on alg 1 with rate 0.7000000000000001 = 0.7815, training accuracy 0.788, time elapsed: 3149 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9197754624762481
Loss in iteration 3 : 0.9371350062521381
Loss in iteration 4 : 0.8073830741055
Loss in iteration 5 : 0.7801035832472947
Loss in iteration 6 : 0.6740840405388142
Loss in iteration 7 : 0.6456596466343952
Loss in iteration 8 : 0.5965060733799148
Loss in iteration 9 : 0.5818390000645987
Loss in iteration 10 : 0.5717609096267007
Loss in iteration 11 : 0.559047567521266
Loss in iteration 12 : 0.5693493118862518
Loss in iteration 13 : 0.5522934435245486
Loss in iteration 14 : 0.5608636482808808
Loss in iteration 15 : 0.558143116360869
Loss in iteration 16 : 0.5508866322920012
Loss in iteration 17 : 0.55901960296831
Loss in iteration 18 : 0.553330587514363
Loss in iteration 19 : 0.5513078046185194
Loss in iteration 20 : 0.5556914679121776
Loss in iteration 21 : 0.5499771650496821
Loss in iteration 22 : 0.5486264421330018
Loss in iteration 23 : 0.550533268536134
Loss in iteration 24 : 0.5448901020473552
Loss in iteration 25 : 0.5445638451576061
Loss in iteration 26 : 0.5442462255907183
Loss in iteration 27 : 0.5396776248070053
Loss in iteration 28 : 0.5388115681889246
Loss in iteration 29 : 0.5373348342426245
Loss in iteration 30 : 0.5327624358799717
Loss in iteration 31 : 0.5318327034270253
Loss in iteration 32 : 0.5290321271312416
Loss in iteration 33 : 0.525627524301698
Loss in iteration 34 : 0.524860785127508
Loss in iteration 35 : 0.5216670973859288
Loss in iteration 36 : 0.5202218046691779
Loss in iteration 37 : 0.5192811409095359
Loss in iteration 38 : 0.516830973382428
Loss in iteration 39 : 0.5166832559937087
Loss in iteration 40 : 0.5151371143218259
Loss in iteration 41 : 0.5143826650446972
Loss in iteration 42 : 0.513988007141674
Loss in iteration 43 : 0.5127450160031889
Loss in iteration 44 : 0.5129413931022113
Loss in iteration 45 : 0.5119815038587644
Loss in iteration 46 : 0.5121712766889229
Loss in iteration 47 : 0.5115775215371827
Loss in iteration 48 : 0.5116936699705394
Loss in iteration 49 : 0.5115375299627297
Loss in iteration 50 : 0.5113479910508557
Loss in iteration 51 : 0.5115816801255973
Loss in iteration 52 : 0.5112890549700584
Loss in iteration 53 : 0.5115703431214356
Loss in iteration 54 : 0.5113238696267722
Loss in iteration 55 : 0.5112991167755607
Loss in iteration 56 : 0.511298247671157
Loss in iteration 57 : 0.5110706631732861
Loss in iteration 58 : 0.5111733833428751
Loss in iteration 59 : 0.5108980690715517
Loss in iteration 60 : 0.5109062373474946
Loss in iteration 61 : 0.5107237584222266
Loss in iteration 62 : 0.5105991637713934
Loss in iteration 63 : 0.5105279775502333
Loss in iteration 64 : 0.5103262512570809
Loss in iteration 65 : 0.5102583135913015
Loss in iteration 66 : 0.5101051332035036
Loss in iteration 67 : 0.5100016081573207
Loss in iteration 68 : 0.5099295486976426
Loss in iteration 69 : 0.5097804661226231
Loss in iteration 70 : 0.5097123794477452
Loss in iteration 71 : 0.5096143649401254
Loss in iteration 72 : 0.5094936911065805
Loss in iteration 73 : 0.509455115109319
Loss in iteration 74 : 0.5093504245481019
Loss in iteration 75 : 0.5093183375941396
Loss in iteration 76 : 0.5092465810000414
Loss in iteration 77 : 0.5091954225954749
Loss in iteration 78 : 0.5091504232219821
Loss in iteration 79 : 0.5090928273806994
Loss in iteration 80 : 0.5090612123134498
Loss in iteration 81 : 0.509006757358848
Loss in iteration 82 : 0.5089719596428405
Loss in iteration 83 : 0.5089302813552501
Loss in iteration 84 : 0.5088807211654797
Loss in iteration 85 : 0.5088502994389686
Loss in iteration 86 : 0.5088028669301804
Loss in iteration 87 : 0.5087757752681069
Loss in iteration 88 : 0.5087270339857486
Loss in iteration 89 : 0.5087023833807006
Loss in iteration 90 : 0.5086581892003509
Loss in iteration 91 : 0.5086330400063008
Loss in iteration 92 : 0.5085903846099912
Loss in iteration 93 : 0.5085638409269387
Loss in iteration 94 : 0.5085285790115613
Loss in iteration 95 : 0.5084960750253344
Loss in iteration 96 : 0.508469544670319
Loss in iteration 97 : 0.5084282763678886
Loss in iteration 98 : 0.5084070254884983
Loss in iteration 99 : 0.5083687570972094
Loss in iteration 100 : 0.5083466438461729
Loss in iteration 101 : 0.5083166582219957
Loss in iteration 102 : 0.5082858293543161
Loss in iteration 103 : 0.5082617873371615
Loss in iteration 104 : 0.5082336906145541
Loss in iteration 105 : 0.5082137122242301
Loss in iteration 106 : 0.5081823749828248
Loss in iteration 107 : 0.5081631843587696
Loss in iteration 108 : 0.5081345748708526
Loss in iteration 109 : 0.5081141038183127
Loss in iteration 110 : 0.508090130205328
Loss in iteration 111 : 0.5080693635795994
Loss in iteration 112 : 0.5080478410526136
Loss in iteration 113 : 0.5080225425229303
Loss in iteration 114 : 0.5080028754200859
Loss in iteration 115 : 0.5079805105644036
Loss in iteration 116 : 0.5079611810248138
Loss in iteration 117 : 0.5079404555745175
Loss in iteration 118 : 0.5079189364139135
Loss in iteration 119 : 0.5079000798111527
Loss in iteration 120 : 0.5078774545946458
Loss in iteration 121 : 0.5078591309738145
Loss in iteration 122 : 0.5078379330563335
Loss in iteration 123 : 0.507819468811007
Loss in iteration 124 : 0.5077976781558488
Loss in iteration 125 : 0.5077803847531758
Loss in iteration 126 : 0.5077589586733067
Loss in iteration 127 : 0.5077400719298473
Loss in iteration 128 : 0.507722378205144
Loss in iteration 129 : 0.5077027040772378
Loss in iteration 130 : 0.5076858306804553
Loss in iteration 131 : 0.5076671136426102
Loss in iteration 132 : 0.5076501004222965
Loss in iteration 133 : 0.5076315655047802
Loss in iteration 134 : 0.5076141003202183
Loss in iteration 135 : 0.5075991315634885
Loss in iteration 136 : 0.5075790423642921
Loss in iteration 137 : 0.5075646767653156
Loss in iteration 138 : 0.5075474144969868
Loss in iteration 139 : 0.5075323989198993
Loss in iteration 140 : 0.5075155760949969
Loss in iteration 141 : 0.5075007528667189
Loss in iteration 142 : 0.5074851565322188
Loss in iteration 143 : 0.5074708557461395
Loss in iteration 144 : 0.5074565466055663
Loss in iteration 145 : 0.5074424089510438
Loss in iteration 146 : 0.5074283046271639
Loss in iteration 147 : 0.5074144281242354
Loss in iteration 148 : 0.5074003452349706
Loss in iteration 149 : 0.5073875426110072
Loss in iteration 150 : 0.507375326667467
Loss in iteration 151 : 0.5073620904325284
Loss in iteration 152 : 0.5073495467105107
Loss in iteration 153 : 0.5073365333651331
Loss in iteration 154 : 0.5073245404219474
Loss in iteration 155 : 0.5073131192725435
Loss in iteration 156 : 0.5073014377253297
Loss in iteration 157 : 0.507289128118647
Loss in iteration 158 : 0.5072771426328753
Loss in iteration 159 : 0.5072662513667245
Loss in iteration 160 : 0.5072546718444231
Loss in iteration 161 : 0.5072439993440542
Loss in iteration 162 : 0.5072337843925957
Loss in iteration 163 : 0.5072223679918355
Loss in iteration 164 : 0.5072116138889423
Loss in iteration 165 : 0.5072008805225598
Loss in iteration 166 : 0.5071907519772539
Loss in iteration 167 : 0.5071810157736377
Loss in iteration 168 : 0.5071706627926463
Loss in iteration 169 : 0.507159714510539
Loss in iteration 170 : 0.5071506978782349
Loss in iteration 171 : 0.5071428783072988
Loss in iteration 172 : 0.5071327971523238
Loss in iteration 173 : 0.5071250009244112
Loss in iteration 174 : 0.5071162028943225
Loss in iteration 175 : 0.5071070330532598
Loss in iteration 176 : 0.5070996801001282
Loss in iteration 177 : 0.507091138099485
Loss in iteration 178 : 0.5070826922478694
Loss in iteration 179 : 0.5070752378872881
Loss in iteration 180 : 0.5070671790277683
Loss in iteration 181 : 0.5070589996905559
Loss in iteration 182 : 0.5070512511917271
Loss in iteration 183 : 0.5070437415175163
Loss in iteration 184 : 0.5070355868556368
Loss in iteration 185 : 0.5070283326174593
Loss in iteration 186 : 0.5070208948873834
Loss in iteration 187 : 0.5070133323671139
Loss in iteration 188 : 0.5070058310966642
Loss in iteration 189 : 0.5069985972171246
Loss in iteration 190 : 0.5069910668801831
Loss in iteration 191 : 0.5069838326951356
Loss in iteration 192 : 0.5069764696162112
Loss in iteration 193 : 0.5069688659143659
Loss in iteration 194 : 0.5069621714018012
Loss in iteration 195 : 0.5069550731070417
Loss in iteration 196 : 0.5069476079703048
Loss in iteration 197 : 0.5069411482399746
Loss in iteration 198 : 0.5069354004524705
Loss in iteration 199 : 0.5069274400636422
Loss in iteration 200 : 0.5069231661585134
Testing accuracy  of updater 1 on alg 1 with rate 0.4 = 0.783, training accuracy 0.788, time elapsed: 2878 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.976881816203008
Loss in iteration 3 : 0.9356914387655068
Loss in iteration 4 : 0.9118945900533192
Loss in iteration 5 : 0.9137583144775939
Loss in iteration 6 : 0.8870448132840141
Loss in iteration 7 : 0.8429505789818151
Loss in iteration 8 : 0.8069669450114523
Loss in iteration 9 : 0.7779934522664058
Loss in iteration 10 : 0.7538595475209124
Loss in iteration 11 : 0.7383310836441096
Loss in iteration 12 : 0.7035617414134797
Loss in iteration 13 : 0.6618248637977693
Loss in iteration 14 : 0.6406558394247837
Loss in iteration 15 : 0.6316318372492505
Loss in iteration 16 : 0.6171921963174081
Loss in iteration 17 : 0.5970656732583002
Loss in iteration 18 : 0.5840599181380626
Loss in iteration 19 : 0.5816541615321486
Loss in iteration 20 : 0.5798135917096922
Loss in iteration 21 : 0.5717031420861949
Loss in iteration 22 : 0.5633752403047251
Loss in iteration 23 : 0.5610869722928308
Loss in iteration 24 : 0.5617294982070873
Loss in iteration 25 : 0.5595358890657883
Loss in iteration 26 : 0.5545461156644517
Loss in iteration 27 : 0.5506166910042721
Loss in iteration 28 : 0.5500289237521467
Loss in iteration 29 : 0.5500566778729478
Loss in iteration 30 : 0.5481488047672374
Loss in iteration 31 : 0.5449878226399112
Loss in iteration 32 : 0.542854362785902
Loss in iteration 33 : 0.5425921156233078
Loss in iteration 34 : 0.5424061035815965
Loss in iteration 35 : 0.5409248306601872
Loss in iteration 36 : 0.5388872365484634
Loss in iteration 37 : 0.5378430468893448
Loss in iteration 38 : 0.5376280551586717
Loss in iteration 39 : 0.5372360283986422
Loss in iteration 40 : 0.5360200029800762
Loss in iteration 41 : 0.5346109733948733
Loss in iteration 42 : 0.5338398653953543
Loss in iteration 43 : 0.5335936315334536
Loss in iteration 44 : 0.5330940146361557
Loss in iteration 45 : 0.5321101215152491
Loss in iteration 46 : 0.5311355842255252
Loss in iteration 47 : 0.5306263181185146
Loss in iteration 48 : 0.5304025871888743
Loss in iteration 49 : 0.5298591391006245
Loss in iteration 50 : 0.5290228827269586
Loss in iteration 51 : 0.5284254692534928
Loss in iteration 52 : 0.5280927855142997
Loss in iteration 53 : 0.5277085918039942
Loss in iteration 54 : 0.5270999818608841
Loss in iteration 55 : 0.5264641262675768
Loss in iteration 56 : 0.5259901996146207
Loss in iteration 57 : 0.5256328547693906
Loss in iteration 58 : 0.5251853053862837
Loss in iteration 59 : 0.5246363087431132
Loss in iteration 60 : 0.5241641720334891
Loss in iteration 61 : 0.5238278413828216
Loss in iteration 62 : 0.5234912060010786
Loss in iteration 63 : 0.5230791411973098
Loss in iteration 64 : 0.5226689013868611
Loss in iteration 65 : 0.52233287885761
Loss in iteration 66 : 0.5220444162193921
Loss in iteration 67 : 0.5217247447340494
Loss in iteration 68 : 0.521375872875094
Loss in iteration 69 : 0.5210449187990711
Loss in iteration 70 : 0.5207569364242894
Loss in iteration 71 : 0.5204934227913238
Loss in iteration 72 : 0.5202114988026854
Loss in iteration 73 : 0.5199185473974524
Loss in iteration 74 : 0.5196558186208108
Loss in iteration 75 : 0.5194263913757092
Loss in iteration 76 : 0.519205699782339
Loss in iteration 77 : 0.5189767232641758
Loss in iteration 78 : 0.5187489010466197
Loss in iteration 79 : 0.5185279597002425
Loss in iteration 80 : 0.5183285564975517
Loss in iteration 81 : 0.5181414708771676
Loss in iteration 82 : 0.5179484290408805
Loss in iteration 83 : 0.5177514612005636
Loss in iteration 84 : 0.5175636245621219
Loss in iteration 85 : 0.5173900629331267
Loss in iteration 86 : 0.5172207704329207
Loss in iteration 87 : 0.5170522486004127
Loss in iteration 88 : 0.5168857317618962
Loss in iteration 89 : 0.5167301520244031
Loss in iteration 90 : 0.5165802634470975
Loss in iteration 91 : 0.5164316938747536
Loss in iteration 92 : 0.5162869447497831
Loss in iteration 93 : 0.5161476630115578
Loss in iteration 94 : 0.5160139611916218
Loss in iteration 95 : 0.5158812663749293
Loss in iteration 96 : 0.5157522341064699
Loss in iteration 97 : 0.5156282845463818
Loss in iteration 98 : 0.5155066290880963
Loss in iteration 99 : 0.5153880852851007
Loss in iteration 100 : 0.5152727041671321
Loss in iteration 101 : 0.5151591522951883
Loss in iteration 102 : 0.5150480154956076
Loss in iteration 103 : 0.5149388938285979
Loss in iteration 104 : 0.5148318408623977
Loss in iteration 105 : 0.5147278278915917
Loss in iteration 106 : 0.5146269827971115
Loss in iteration 107 : 0.5145279504063902
Loss in iteration 108 : 0.5144299691796375
Loss in iteration 109 : 0.5143344821589194
Loss in iteration 110 : 0.5142409068327458
Loss in iteration 111 : 0.5141488256537434
Loss in iteration 112 : 0.5140586358855697
Loss in iteration 113 : 0.5139705551439766
Loss in iteration 114 : 0.5138851376669682
Loss in iteration 115 : 0.5138020826615147
Loss in iteration 116 : 0.5137214074037921
Loss in iteration 117 : 0.5136427997467614
Loss in iteration 118 : 0.5135653231987396
Loss in iteration 119 : 0.5134896571216024
Loss in iteration 120 : 0.5134153236654052
Loss in iteration 121 : 0.5133417392161332
Loss in iteration 122 : 0.5132689812616515
Loss in iteration 123 : 0.5131975423806407
Loss in iteration 124 : 0.5131272297795272
Loss in iteration 125 : 0.5130583414601958
Loss in iteration 126 : 0.5129902493149718
Loss in iteration 127 : 0.5129233980628934
Loss in iteration 128 : 0.5128574485353694
Loss in iteration 129 : 0.5127925384387861
Loss in iteration 130 : 0.5127279787547174
Loss in iteration 131 : 0.5126643248452981
Loss in iteration 132 : 0.5126022477673436
Loss in iteration 133 : 0.5125407886648995
Loss in iteration 134 : 0.5124806698107043
Loss in iteration 135 : 0.5124218557527768
Loss in iteration 136 : 0.5123645700880353
Loss in iteration 137 : 0.5123086180863088
Loss in iteration 138 : 0.5122534931524257
Loss in iteration 139 : 0.5121997278821794
Loss in iteration 140 : 0.5121475734698834
Loss in iteration 141 : 0.512096045948283
Loss in iteration 142 : 0.5120455452958929
Loss in iteration 143 : 0.5119963640018148
Loss in iteration 144 : 0.5119476996184433
Loss in iteration 145 : 0.5118999886614661
Loss in iteration 146 : 0.5118530805534997
Loss in iteration 147 : 0.5118072104086518
Loss in iteration 148 : 0.5117623715376299
Loss in iteration 149 : 0.511718139485072
Loss in iteration 150 : 0.5116744444167968
Loss in iteration 151 : 0.511631140137996
Loss in iteration 152 : 0.5115893230926531
Loss in iteration 153 : 0.5115490116619424
Loss in iteration 154 : 0.5115098419634111
Loss in iteration 155 : 0.5114718652234451
Loss in iteration 156 : 0.5114346690057588
Loss in iteration 157 : 0.5113976931252474
Loss in iteration 158 : 0.5113614402830572
Loss in iteration 159 : 0.5113265085397333
Loss in iteration 160 : 0.5112920422147635
Loss in iteration 161 : 0.5112580364476333
Loss in iteration 162 : 0.5112249815228986
Loss in iteration 163 : 0.5111926290925303
Loss in iteration 164 : 0.5111608248312969
Loss in iteration 165 : 0.5111292346707929
Loss in iteration 166 : 0.5110981254517781
Loss in iteration 167 : 0.511068687321561
Loss in iteration 168 : 0.5110402221606718
Loss in iteration 169 : 0.5110117690614437
Loss in iteration 170 : 0.5109841201322196
Loss in iteration 171 : 0.5109570071405125
Loss in iteration 172 : 0.5109300788876555
Loss in iteration 173 : 0.5109034570448473
Loss in iteration 174 : 0.510876979266086
Loss in iteration 175 : 0.5108506412190572
Loss in iteration 176 : 0.5108246195330841
Loss in iteration 177 : 0.5107987067506907
Loss in iteration 178 : 0.5107728682503521
Loss in iteration 179 : 0.5107472270844597
Loss in iteration 180 : 0.5107221209434534
Loss in iteration 181 : 0.5106972508596846
Loss in iteration 182 : 0.5106725229620912
Loss in iteration 183 : 0.5106484382644443
Loss in iteration 184 : 0.5106249306814741
Loss in iteration 185 : 0.5106019416315283
Loss in iteration 186 : 0.5105793725652109
Loss in iteration 187 : 0.510556741595176
Loss in iteration 188 : 0.5105347740823052
Loss in iteration 189 : 0.5105135812055048
Loss in iteration 190 : 0.5104923169512378
Loss in iteration 191 : 0.510471462638957
Loss in iteration 192 : 0.5104508203077848
Loss in iteration 193 : 0.5104301502043035
Loss in iteration 194 : 0.510409721712599
Loss in iteration 195 : 0.5103894860285
Loss in iteration 196 : 0.5103695804694949
Loss in iteration 197 : 0.5103498813011421
Loss in iteration 198 : 0.5103304099765967
Loss in iteration 199 : 0.5103111835235554
Loss in iteration 200 : 0.5102920331198268
Testing accuracy  of updater 1 on alg 1 with rate 0.1 = 0.781, training accuracy 0.7875, time elapsed: 3823 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.983817271342106
Loss in iteration 3 : 0.953365011277132
Loss in iteration 4 : 0.920800675103947
Loss in iteration 5 : 0.9117018780413215
Loss in iteration 6 : 0.9106034516897696
Loss in iteration 7 : 0.8894136854793316
Loss in iteration 8 : 0.8553954473165588
Loss in iteration 9 : 0.8248462890238893
Loss in iteration 10 : 0.8002574124657512
Loss in iteration 11 : 0.7779126983389822
Loss in iteration 12 : 0.7570103880415688
Loss in iteration 13 : 0.7412579511225098
Loss in iteration 14 : 0.7203469851767975
Loss in iteration 15 : 0.6901491507134657
Loss in iteration 16 : 0.6622702486427186
Loss in iteration 17 : 0.645409859464635
Loss in iteration 18 : 0.6353839894041607
Loss in iteration 19 : 0.6249232552672959
Loss in iteration 20 : 0.6110603337382703
Loss in iteration 21 : 0.5977486169911655
Loss in iteration 22 : 0.5894699175775157
Loss in iteration 23 : 0.5859179615683074
Loss in iteration 24 : 0.5830452773196443
Loss in iteration 25 : 0.5779099261708066
Loss in iteration 26 : 0.5716900324600245
Loss in iteration 27 : 0.5670782066303253
Loss in iteration 28 : 0.5649828698051079
Loss in iteration 29 : 0.5639341541466727
Loss in iteration 30 : 0.5619586343939289
Loss in iteration 31 : 0.5587731154984891
Loss in iteration 32 : 0.5556350354997417
Loss in iteration 33 : 0.5536454785236902
Loss in iteration 34 : 0.5528056393324801
Loss in iteration 35 : 0.5519715706703505
Loss in iteration 36 : 0.5503014016217176
Loss in iteration 37 : 0.5482101400918639
Loss in iteration 38 : 0.5465496647718907
Loss in iteration 39 : 0.5456202929079964
Loss in iteration 40 : 0.5450230161115294
Loss in iteration 41 : 0.5441879938090339
Loss in iteration 42 : 0.5429770101589573
Loss in iteration 43 : 0.5416885503674483
Loss in iteration 44 : 0.5407679761002426
Loss in iteration 45 : 0.5402228499294749
Loss in iteration 46 : 0.5396892485273781
Loss in iteration 47 : 0.5389311113791107
Loss in iteration 48 : 0.5380373572031263
Loss in iteration 49 : 0.5372539132438874
Loss in iteration 50 : 0.536742654607638
Loss in iteration 51 : 0.5363158658250842
Loss in iteration 52 : 0.5358042914840915
Loss in iteration 53 : 0.5351717628659494
Loss in iteration 54 : 0.5345368183235192
Loss in iteration 55 : 0.5340171727606037
Loss in iteration 56 : 0.53359333049869
Loss in iteration 57 : 0.5331957000506216
Loss in iteration 58 : 0.5327513890950306
Loss in iteration 59 : 0.5322602754091283
Loss in iteration 60 : 0.5317867126691473
Loss in iteration 61 : 0.5313759493754544
Loss in iteration 62 : 0.5310124374611465
Loss in iteration 63 : 0.5306568364000537
Loss in iteration 64 : 0.5302756801610349
Loss in iteration 65 : 0.5298765464402717
Loss in iteration 66 : 0.5294893529338897
Loss in iteration 67 : 0.5291262718156916
Loss in iteration 68 : 0.5287882387457039
Loss in iteration 69 : 0.5284498723277745
Loss in iteration 70 : 0.5280958232430514
Loss in iteration 71 : 0.5277378584779108
Loss in iteration 72 : 0.5273944167074293
Loss in iteration 73 : 0.5270767334439221
Loss in iteration 74 : 0.5267755472046233
Loss in iteration 75 : 0.5264731413031293
Loss in iteration 76 : 0.5261669965238913
Loss in iteration 77 : 0.5258696907939545
Loss in iteration 78 : 0.5255853219615886
Loss in iteration 79 : 0.5253156413332508
Loss in iteration 80 : 0.5250511662579076
Loss in iteration 81 : 0.5247904243418816
Loss in iteration 82 : 0.5245326309137445
Loss in iteration 83 : 0.5242801953525527
Loss in iteration 84 : 0.5240332397179405
Loss in iteration 85 : 0.5237935291725204
Loss in iteration 86 : 0.5235603528974032
Loss in iteration 87 : 0.5233297901564251
Loss in iteration 88 : 0.5231011698977922
Loss in iteration 89 : 0.5228777778053926
Loss in iteration 90 : 0.5226600972633098
Loss in iteration 91 : 0.5224488661494691
Loss in iteration 92 : 0.5222430570171099
Loss in iteration 93 : 0.5220402582023137
Loss in iteration 94 : 0.5218405800198402
Loss in iteration 95 : 0.5216433926224611
Loss in iteration 96 : 0.5214517428388405
Loss in iteration 97 : 0.5212648589597573
Loss in iteration 98 : 0.5210818715383864
Loss in iteration 99 : 0.5209014339953719
Loss in iteration 100 : 0.5207243965707575
Loss in iteration 101 : 0.5205491411233518
Loss in iteration 102 : 0.5203781988601889
Loss in iteration 103 : 0.5202115322085126
Loss in iteration 104 : 0.5200472719157894
Loss in iteration 105 : 0.5198861078081354
Loss in iteration 106 : 0.5197315900589587
Loss in iteration 107 : 0.5195810455866537
Loss in iteration 108 : 0.519433551151733
Loss in iteration 109 : 0.5192893542896502
Loss in iteration 110 : 0.5191474842389721
Loss in iteration 111 : 0.5190072897883711
Loss in iteration 112 : 0.5188692814410651
Loss in iteration 113 : 0.5187333640952866
Loss in iteration 114 : 0.5186007817951616
Loss in iteration 115 : 0.5184705312918172
Loss in iteration 116 : 0.51834200768954
Loss in iteration 117 : 0.5182150538285929
Loss in iteration 118 : 0.5180909420562119
Loss in iteration 119 : 0.5179697451742054
Loss in iteration 120 : 0.5178501451255241
Loss in iteration 121 : 0.5177329095421602
Loss in iteration 122 : 0.5176179922705311
Loss in iteration 123 : 0.5175052624611366
Loss in iteration 124 : 0.5173948759031218
Loss in iteration 125 : 0.5172868627595187
Loss in iteration 126 : 0.5171795996451358
Loss in iteration 127 : 0.517073300811411
Loss in iteration 128 : 0.5169702310676221
Loss in iteration 129 : 0.5168691834974517
Loss in iteration 130 : 0.5167699802170563
Loss in iteration 131 : 0.5166716658736085
Loss in iteration 132 : 0.5165735492974266
Loss in iteration 133 : 0.5164766987034438
Loss in iteration 134 : 0.5163819426878212
Loss in iteration 135 : 0.5162890300920953
Loss in iteration 136 : 0.516198417920275
Loss in iteration 137 : 0.5161083966470749
Loss in iteration 138 : 0.5160182858326707
Loss in iteration 139 : 0.5159303691238507
Loss in iteration 140 : 0.5158449876313442
Loss in iteration 141 : 0.5157614790412435
Loss in iteration 142 : 0.515677722037751
Loss in iteration 143 : 0.5155946279940454
Loss in iteration 144 : 0.5155138058022878
Loss in iteration 145 : 0.5154351428201016
Loss in iteration 146 : 0.5153574852781705
Loss in iteration 147 : 0.515280841199391
Loss in iteration 148 : 0.5152048927491926
Loss in iteration 149 : 0.5151298165262084
Loss in iteration 150 : 0.5150562677472679
Loss in iteration 151 : 0.5149835864817389
Loss in iteration 152 : 0.5149117760686909
Loss in iteration 153 : 0.5148410262445606
Loss in iteration 154 : 0.5147711366300604
Loss in iteration 155 : 0.5147025072829047
Loss in iteration 156 : 0.5146345237353434
Loss in iteration 157 : 0.514567665276184
Loss in iteration 158 : 0.5145020345585479
Loss in iteration 159 : 0.514437220377144
Loss in iteration 160 : 0.5143724511874768
Loss in iteration 161 : 0.5143086602727226
Loss in iteration 162 : 0.5142463184598263
Loss in iteration 163 : 0.5141851416761148
Loss in iteration 164 : 0.5141245373644798
Loss in iteration 165 : 0.514064169409973
Loss in iteration 166 : 0.5140045346987061
Loss in iteration 167 : 0.5139454597930591
Loss in iteration 168 : 0.5138868988846337
Loss in iteration 169 : 0.5138293732893329
Loss in iteration 170 : 0.5137727641587828
Loss in iteration 171 : 0.5137172175342526
Loss in iteration 172 : 0.5136615633346484
Loss in iteration 173 : 0.5136064030641483
Loss in iteration 174 : 0.5135529782953838
Loss in iteration 175 : 0.5135007837440403
Loss in iteration 176 : 0.5134490482100262
Loss in iteration 177 : 0.513397384516445
Loss in iteration 178 : 0.5133468556319003
Loss in iteration 179 : 0.5132981440744374
Loss in iteration 180 : 0.5132504116931367
Loss in iteration 181 : 0.5132029282252473
Loss in iteration 182 : 0.5131556625263425
Loss in iteration 183 : 0.513108881363987
Loss in iteration 184 : 0.5130624729391343
Loss in iteration 185 : 0.51301634842834
Loss in iteration 186 : 0.512970526270886
Loss in iteration 187 : 0.512925244472661
Loss in iteration 188 : 0.5128804913525291
Loss in iteration 189 : 0.5128359171303107
Loss in iteration 190 : 0.512791796957618
Loss in iteration 191 : 0.5127482152635846
Loss in iteration 192 : 0.5127050391297798
Loss in iteration 193 : 0.5126621586540563
Loss in iteration 194 : 0.512619471195965
Loss in iteration 195 : 0.512577219877965
Loss in iteration 196 : 0.5125352358370175
Loss in iteration 197 : 0.5124939279430953
Loss in iteration 198 : 0.5124531402993534
Loss in iteration 199 : 0.5124132555985945
Loss in iteration 200 : 0.5123737719991768
Testing accuracy  of updater 1 on alg 1 with rate 0.07 = 0.7825, training accuracy 0.78725, time elapsed: 2939 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9907527264812034
Loss in iteration 3 : 0.97318290679549
Loss in iteration 4 : 0.9487201040434611
Loss in iteration 5 : 0.9246980432178099
Loss in iteration 6 : 0.9121935954278103
Loss in iteration 7 : 0.9107429723450591
Loss in iteration 8 : 0.9071030565727931
Loss in iteration 9 : 0.8932365707760368
Loss in iteration 10 : 0.8717378043899948
Loss in iteration 11 : 0.8495400082375133
Loss in iteration 12 : 0.8300933456794738
Loss in iteration 13 : 0.8135478265729591
Loss in iteration 14 : 0.7986053133938792
Loss in iteration 15 : 0.7836678617428782
Loss in iteration 16 : 0.7681605142636079
Loss in iteration 17 : 0.7531465103130371
Loss in iteration 18 : 0.7388276903242232
Loss in iteration 19 : 0.7237302610570487
Loss in iteration 20 : 0.7067812775304578
Loss in iteration 21 : 0.6891394811677921
Loss in iteration 22 : 0.6731888471894583
Loss in iteration 23 : 0.6602091083500539
Loss in iteration 24 : 0.6499506976950228
Loss in iteration 25 : 0.6411807355966755
Loss in iteration 26 : 0.6326409772557869
Loss in iteration 27 : 0.6239241667536908
Loss in iteration 28 : 0.615580081574625
Loss in iteration 29 : 0.6083545240213611
Loss in iteration 30 : 0.6025908689259373
Loss in iteration 31 : 0.5981296237855147
Loss in iteration 32 : 0.5943919910417287
Loss in iteration 33 : 0.5908295753536861
Loss in iteration 34 : 0.587238490665639
Loss in iteration 35 : 0.5837247087157426
Loss in iteration 36 : 0.5804996629744787
Loss in iteration 37 : 0.5777767589455114
Loss in iteration 38 : 0.5755374483796362
Loss in iteration 39 : 0.5735858739034388
Loss in iteration 40 : 0.5717027351140077
Loss in iteration 41 : 0.5697411411699008
Loss in iteration 42 : 0.5677640240091659
Loss in iteration 43 : 0.565897942237837
Loss in iteration 44 : 0.5642377279641039
Loss in iteration 45 : 0.5627858215271417
Loss in iteration 46 : 0.5614443100390525
Loss in iteration 47 : 0.560176330777746
Loss in iteration 48 : 0.558927541480887
Loss in iteration 49 : 0.5576854878142031
Loss in iteration 50 : 0.5564889847931386
Loss in iteration 51 : 0.5553932838448274
Loss in iteration 52 : 0.5544127480267754
Loss in iteration 53 : 0.5535186827641664
Loss in iteration 54 : 0.5526645708346978
Loss in iteration 55 : 0.5518378752737758
Loss in iteration 56 : 0.5510182783860087
Loss in iteration 57 : 0.5502217796861423
Loss in iteration 58 : 0.5494817940265696
Loss in iteration 59 : 0.5488019476898703
Loss in iteration 60 : 0.5481779141546085
Loss in iteration 61 : 0.547563865444905
Loss in iteration 62 : 0.5469511085371593
Loss in iteration 63 : 0.5463468393643844
Loss in iteration 64 : 0.545761130148051
Loss in iteration 65 : 0.545208375553598
Loss in iteration 66 : 0.5446921684498675
Loss in iteration 67 : 0.5441994847845056
Loss in iteration 68 : 0.5437112070986995
Loss in iteration 69 : 0.5432238585599765
Loss in iteration 70 : 0.5427412473859855
Loss in iteration 71 : 0.5422803803114035
Loss in iteration 72 : 0.5418493331779665
Loss in iteration 73 : 0.5414374985378246
Loss in iteration 74 : 0.5410352051860227
Loss in iteration 75 : 0.5406391532810166
Loss in iteration 76 : 0.5402466448659011
Loss in iteration 77 : 0.5398615893311787
Loss in iteration 78 : 0.5394840742333283
Loss in iteration 79 : 0.5391187834375214
Loss in iteration 80 : 0.5387624636697226
Loss in iteration 81 : 0.5384141158395764
Loss in iteration 82 : 0.5380718883371054
Loss in iteration 83 : 0.5377348727900446
Loss in iteration 84 : 0.5374034855213666
Loss in iteration 85 : 0.5370795448178486
Loss in iteration 86 : 0.5367629861998744
Loss in iteration 87 : 0.5364546414377586
Loss in iteration 88 : 0.5361525914684585
Loss in iteration 89 : 0.5358567798097769
Loss in iteration 90 : 0.5355662653317922
Loss in iteration 91 : 0.5352795372374628
Loss in iteration 92 : 0.5349965066164091
Loss in iteration 93 : 0.5347184940704258
Loss in iteration 94 : 0.5344458029067591
Loss in iteration 95 : 0.5341773231467439
Loss in iteration 96 : 0.5339121418642312
Loss in iteration 97 : 0.53365061890917
Loss in iteration 98 : 0.53339459809385
Loss in iteration 99 : 0.5331423959140706
Loss in iteration 100 : 0.532896589082935
Loss in iteration 101 : 0.5326567746026198
Loss in iteration 102 : 0.5324203995568291
Loss in iteration 103 : 0.5321880493120311
Loss in iteration 104 : 0.5319596089340333
Loss in iteration 105 : 0.5317350208457692
Loss in iteration 106 : 0.5315140166806883
Loss in iteration 107 : 0.5312962523042228
Loss in iteration 108 : 0.5310812460208036
Loss in iteration 109 : 0.5308696642664494
Loss in iteration 110 : 0.5306615803495816
Loss in iteration 111 : 0.5304565748955279
Loss in iteration 112 : 0.530255137723912
Loss in iteration 113 : 0.5300564375866852
Loss in iteration 114 : 0.5298596023297366
Loss in iteration 115 : 0.5296644035410696
Loss in iteration 116 : 0.5294712305809565
Loss in iteration 117 : 0.5292803488470189
Loss in iteration 118 : 0.529091572046114
Loss in iteration 119 : 0.5289048613299668
Loss in iteration 120 : 0.5287196838527162
Loss in iteration 121 : 0.5285353916464752
Loss in iteration 122 : 0.528352534157116
Loss in iteration 123 : 0.5281720144190644
Loss in iteration 124 : 0.5279935946500529
Loss in iteration 125 : 0.5278168315320204
Loss in iteration 126 : 0.5276415173186005
Loss in iteration 127 : 0.5274680413108025
Loss in iteration 128 : 0.5272973114386482
Loss in iteration 129 : 0.527128708845476
Loss in iteration 130 : 0.5269624036543632
Loss in iteration 131 : 0.5267977340321605
Loss in iteration 132 : 0.5266349880557338
Loss in iteration 133 : 0.5264738612795614
Loss in iteration 134 : 0.5263138648438174
Loss in iteration 135 : 0.5261559936608606
Loss in iteration 136 : 0.5260004003537018
Loss in iteration 137 : 0.5258473840291458
Loss in iteration 138 : 0.525696443871163
Loss in iteration 139 : 0.525547510513039
Loss in iteration 140 : 0.5254005043306511
Loss in iteration 141 : 0.5252557720253923
Loss in iteration 142 : 0.5251129897442293
Loss in iteration 143 : 0.5249712671445175
Loss in iteration 144 : 0.5248312314757662
Loss in iteration 145 : 0.5246929031332415
Loss in iteration 146 : 0.524556820513272
Loss in iteration 147 : 0.5244221256049699
Loss in iteration 148 : 0.5242886228538661
Loss in iteration 149 : 0.5241567274225142
Loss in iteration 150 : 0.524026285768266
Loss in iteration 151 : 0.5238966295388086
Loss in iteration 152 : 0.5237678036019511
Loss in iteration 153 : 0.5236404097764287
Loss in iteration 154 : 0.5235149171700125
Loss in iteration 155 : 0.5233909821432338
Loss in iteration 156 : 0.5232684054271844
Loss in iteration 157 : 0.5231479953276387
Loss in iteration 158 : 0.5230290527922625
Loss in iteration 159 : 0.5229116993038072
Loss in iteration 160 : 0.5227952161686679
Loss in iteration 161 : 0.5226800400702257
Loss in iteration 162 : 0.5225663820532535
Loss in iteration 163 : 0.5224540149459135
Loss in iteration 164 : 0.5223423552269463
Loss in iteration 165 : 0.5222311911700044
Loss in iteration 166 : 0.5221212726161362
Loss in iteration 167 : 0.5220123178233407
Loss in iteration 168 : 0.5219041606581802
Loss in iteration 169 : 0.5217968128648064
Loss in iteration 170 : 0.5216909934097178
Loss in iteration 171 : 0.521586782693326
Loss in iteration 172 : 0.5214834637453931
Loss in iteration 173 : 0.5213813413224966
Loss in iteration 174 : 0.5212803357157427
Loss in iteration 175 : 0.5211807733874293
Loss in iteration 176 : 0.5210818535321028
Loss in iteration 177 : 0.5209834411692016
Loss in iteration 178 : 0.5208856059886391
Loss in iteration 179 : 0.520788395878101
Loss in iteration 180 : 0.5206924593281035
Loss in iteration 181 : 0.5205972328269425
Loss in iteration 182 : 0.5205027766406576
Loss in iteration 183 : 0.5204088630336356
Loss in iteration 184 : 0.5203158476063044
Loss in iteration 185 : 0.5202242154268933
Loss in iteration 186 : 0.5201340910023533
Loss in iteration 187 : 0.5200456927277491
Loss in iteration 188 : 0.5199585349503786
Loss in iteration 189 : 0.5198719638887774
Loss in iteration 190 : 0.5197865189944838
Loss in iteration 191 : 0.5197016925838537
Loss in iteration 192 : 0.5196176006599219
Loss in iteration 193 : 0.5195341206372812
Loss in iteration 194 : 0.5194515504777205
Loss in iteration 195 : 0.5193695763419337
Loss in iteration 196 : 0.5192880686897476
Loss in iteration 197 : 0.5192070942821997
Loss in iteration 198 : 0.5191268653155353
Loss in iteration 199 : 0.5190476480986222
Loss in iteration 200 : 0.5189699150580027
Testing accuracy  of updater 1 on alg 1 with rate 0.04000000000000001 = 0.7775, training accuracy 0.7845, time elapsed: 3329 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9976881816203013
Loss in iteration 3 : 0.9932957266988724
Loss in iteration 4 : 0.9870306988898876
Loss in iteration 5 : 0.9790803554821018
Loss in iteration 6 : 0.9696298675600558
Loss in iteration 7 : 0.9589517975731401
Loss in iteration 8 : 0.9475106053555417
Loss in iteration 9 : 0.9364172393224336
Loss in iteration 10 : 0.9267408591807352
Loss in iteration 11 : 0.9191717350138261
Loss in iteration 12 : 0.9140312723702098
Loss in iteration 13 : 0.9108221909042955
Loss in iteration 14 : 0.9086511539243497
Loss in iteration 15 : 0.906753961929015
Loss in iteration 16 : 0.9041595855553618
Loss in iteration 17 : 0.9003167462469543
Loss in iteration 18 : 0.895221050254984
Loss in iteration 19 : 0.8891740548914536
Loss in iteration 20 : 0.8825695615986329
Loss in iteration 21 : 0.8758498109331843
Loss in iteration 22 : 0.8693243799447673
Loss in iteration 23 : 0.8631114675210632
Loss in iteration 24 : 0.8572790232302043
Loss in iteration 25 : 0.8517745842836312
Loss in iteration 26 : 0.8465617143314488
Loss in iteration 27 : 0.841586512759671
Loss in iteration 28 : 0.8367583280768781
Loss in iteration 29 : 0.8319950063813906
Loss in iteration 30 : 0.8272156973412843
Loss in iteration 31 : 0.8223784221734788
Loss in iteration 32 : 0.8174665683493716
Loss in iteration 33 : 0.8124760949134286
Loss in iteration 34 : 0.8074126655841328
Loss in iteration 35 : 0.8022920629634422
Loss in iteration 36 : 0.7971244668697923
Loss in iteration 37 : 0.7919192030490934
Loss in iteration 38 : 0.7866917515074863
Loss in iteration 39 : 0.7814489347783268
Loss in iteration 40 : 0.77621089838549
Loss in iteration 41 : 0.7709943196410479
Loss in iteration 42 : 0.7658053778233873
Loss in iteration 43 : 0.7606505288160265
Loss in iteration 44 : 0.7555356047649798
Loss in iteration 45 : 0.7504561203508822
Loss in iteration 46 : 0.7454134602844728
Loss in iteration 47 : 0.7404080302610924
Loss in iteration 48 : 0.7354467103242623
Loss in iteration 49 : 0.7305283448980759
Loss in iteration 50 : 0.7256498569711906
Loss in iteration 51 : 0.7208379617710268
Loss in iteration 52 : 0.7161499379403397
Loss in iteration 53 : 0.7115857806366764
Loss in iteration 54 : 0.7071566153682165
Loss in iteration 55 : 0.7028497959668789
Loss in iteration 56 : 0.6987246923948865
Loss in iteration 57 : 0.6947996665047556
Loss in iteration 58 : 0.6909994117760977
Loss in iteration 59 : 0.6873149752451472
Loss in iteration 60 : 0.6837688639649415
Loss in iteration 61 : 0.6803251002926249
Loss in iteration 62 : 0.6769488910021523
Loss in iteration 63 : 0.6736407872827785
Loss in iteration 64 : 0.6704013206042828
Loss in iteration 65 : 0.6672364552559331
Loss in iteration 66 : 0.6641460888824146
Loss in iteration 67 : 0.6611563329440876
Loss in iteration 68 : 0.6582675149685403
Loss in iteration 69 : 0.6554764472161914
Loss in iteration 70 : 0.6527818303342088
Loss in iteration 71 : 0.6501716829477159
Loss in iteration 72 : 0.6476653568844428
Loss in iteration 73 : 0.6452485910417183
Loss in iteration 74 : 0.6429198252828658
Loss in iteration 75 : 0.6406772666385795
Loss in iteration 76 : 0.6385184173914795
Loss in iteration 77 : 0.6364429580744881
Loss in iteration 78 : 0.6344521170643613
Loss in iteration 79 : 0.6325310816005016
Loss in iteration 80 : 0.6306789457159278
Loss in iteration 81 : 0.628891252746494
Loss in iteration 82 : 0.6271627940957012
Loss in iteration 83 : 0.6254931414444868
Loss in iteration 84 : 0.6238810129727521
Loss in iteration 85 : 0.622326886702197
Loss in iteration 86 : 0.6208348558583996
Loss in iteration 87 : 0.6193951789985777
Loss in iteration 88 : 0.6179949610315353
Loss in iteration 89 : 0.6166332187825465
Loss in iteration 90 : 0.6153153531971518
Loss in iteration 91 : 0.6140375894365729
Loss in iteration 92 : 0.6128025979757165
Loss in iteration 93 : 0.6116102967316228
Loss in iteration 94 : 0.6104460341594863
Loss in iteration 95 : 0.6093173430752487
Loss in iteration 96 : 0.6082175263973624
Loss in iteration 97 : 0.6071498886525936
Loss in iteration 98 : 0.6061105930253048
Loss in iteration 99 : 0.6050985566786955
Loss in iteration 100 : 0.6041118482892089
Loss in iteration 101 : 0.6031506240402734
Loss in iteration 102 : 0.6022124216941129
Loss in iteration 103 : 0.6012962963695613
Loss in iteration 104 : 0.6004041120822362
Loss in iteration 105 : 0.5995318203043414
Loss in iteration 106 : 0.5986829699461548
Loss in iteration 107 : 0.5978552004166962
Loss in iteration 108 : 0.5970476394159013
Loss in iteration 109 : 0.596258928238232
Loss in iteration 110 : 0.5954862235999915
Loss in iteration 111 : 0.5947323306846765
Loss in iteration 112 : 0.593999780894993
Loss in iteration 113 : 0.5932836106038663
Loss in iteration 114 : 0.5925821590224957
Loss in iteration 115 : 0.591891949550803
Loss in iteration 116 : 0.5912121102011324
Loss in iteration 117 : 0.5905461887848403
Loss in iteration 118 : 0.589890552631384
Loss in iteration 119 : 0.5892442379704799
Loss in iteration 120 : 0.5886058337609408
Loss in iteration 121 : 0.5879776219024374
Loss in iteration 122 : 0.5873611777407787
Loss in iteration 123 : 0.5867545676358191
Loss in iteration 124 : 0.586158596662777
Loss in iteration 125 : 0.5855706486309703
Loss in iteration 126 : 0.5849931739920897
Loss in iteration 127 : 0.5844229318278715
Loss in iteration 128 : 0.5838605322789585
Loss in iteration 129 : 0.583305301572319
Loss in iteration 130 : 0.582759048985148
Loss in iteration 131 : 0.5822226852243553
Loss in iteration 132 : 0.5816929480269796
Loss in iteration 133 : 0.5811716235005745
Loss in iteration 134 : 0.5806592590222673
Loss in iteration 135 : 0.5801546863991185
Loss in iteration 136 : 0.5796583112640913
Loss in iteration 137 : 0.5791686156721194
Loss in iteration 138 : 0.5786875824872999
Loss in iteration 139 : 0.5782120384801483
Loss in iteration 140 : 0.5777424603230413
Loss in iteration 141 : 0.5772786503027585
Loss in iteration 142 : 0.5768211524335433
Loss in iteration 143 : 0.5763697825357046
Loss in iteration 144 : 0.5759237725732919
Loss in iteration 145 : 0.5754831051560297
Loss in iteration 146 : 0.5750471923655912
Loss in iteration 147 : 0.5746164223914559
Loss in iteration 148 : 0.5741915368099387
Loss in iteration 149 : 0.5737732931321506
Loss in iteration 150 : 0.5733596476762315
Loss in iteration 151 : 0.5729498642520413
Loss in iteration 152 : 0.572545080206476
Loss in iteration 153 : 0.5721444446598185
Loss in iteration 154 : 0.5717487297381181
Loss in iteration 155 : 0.5713567053394742
Loss in iteration 156 : 0.5709696774757093
Loss in iteration 157 : 0.5705873032701356
Loss in iteration 158 : 0.5702083421469936
Loss in iteration 159 : 0.5698338817681539
Loss in iteration 160 : 0.5694649758661532
Loss in iteration 161 : 0.5691010532635787
Loss in iteration 162 : 0.5687425707105066
Loss in iteration 163 : 0.5683893703319216
Loss in iteration 164 : 0.5680395857859171
Loss in iteration 165 : 0.5676930739192226
Loss in iteration 166 : 0.5673502493949492
Loss in iteration 167 : 0.5670114544222533
Loss in iteration 168 : 0.5666778097318755
Loss in iteration 169 : 0.5663490690216436
Loss in iteration 170 : 0.5660239471481614
Loss in iteration 171 : 0.5657036876655506
Loss in iteration 172 : 0.5653872155307074
Loss in iteration 173 : 0.565074938313103
Loss in iteration 174 : 0.5647656244447483
Loss in iteration 175 : 0.5644593278113125
Loss in iteration 176 : 0.5641566776097142
Loss in iteration 177 : 0.5638576918333802
Loss in iteration 178 : 0.5635615394324557
Loss in iteration 179 : 0.5632701304532447
Loss in iteration 180 : 0.5629817493973646
Loss in iteration 181 : 0.5626964520722847
Loss in iteration 182 : 0.5624135643612825
Loss in iteration 183 : 0.5621339770888307
Loss in iteration 184 : 0.5618580415516312
Loss in iteration 185 : 0.5615854423418286
Loss in iteration 186 : 0.5613156501639187
Loss in iteration 187 : 0.5610484362279649
Loss in iteration 188 : 0.5607839525944233
Loss in iteration 189 : 0.5605219840158112
Loss in iteration 190 : 0.560262842226349
Loss in iteration 191 : 0.5600060668588153
Loss in iteration 192 : 0.5597524714433127
Loss in iteration 193 : 0.5595014922103346
Loss in iteration 194 : 0.5592530270522656
Loss in iteration 195 : 0.5590076663122007
Loss in iteration 196 : 0.5587648379742788
Loss in iteration 197 : 0.5585240556178813
Loss in iteration 198 : 0.5582847326182165
Loss in iteration 199 : 0.5580472135072843
Loss in iteration 200 : 0.5578119627742258
Testing accuracy  of updater 1 on alg 1 with rate 0.009999999999999995 = 0.7695, training accuracy 0.770875, time elapsed: 3228 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3464210846018327
Loss in iteration 3 : 3.73545723363827
Loss in iteration 4 : 1.1015662955488137
Loss in iteration 5 : 2.4103402991671796
Loss in iteration 6 : 2.0750304424941897
Loss in iteration 7 : 1.0367438584063136
Loss in iteration 8 : 1.506062757812067
Loss in iteration 9 : 1.5129532821095226
Loss in iteration 10 : 0.962946016049446
Loss in iteration 11 : 1.0419816040657284
Loss in iteration 12 : 0.9077084580175715
Loss in iteration 13 : 0.8789252821355418
Loss in iteration 14 : 0.8062029833960266
Loss in iteration 15 : 0.7790282617310742
Loss in iteration 16 : 0.7490426125583877
Loss in iteration 17 : 0.7363790230028867
Loss in iteration 18 : 0.7298830657895318
Loss in iteration 19 : 0.7227081670322479
Loss in iteration 20 : 0.7150003070837585
Loss in iteration 21 : 0.7075906651297396
Loss in iteration 22 : 0.6991166036476597
Loss in iteration 23 : 0.6898936258726432
Loss in iteration 24 : 0.6800997132850607
Loss in iteration 25 : 0.6696282618429329
Loss in iteration 26 : 0.658556867887071
Loss in iteration 27 : 0.6469302589344407
Loss in iteration 28 : 0.6350865495863613
Loss in iteration 29 : 0.6235430390655182
Loss in iteration 30 : 0.6122647811939546
Loss in iteration 31 : 0.6017724398501557
Loss in iteration 32 : 0.5925039168776313
Loss in iteration 33 : 0.5861162244422093
Loss in iteration 34 : 0.5877844087561037
Loss in iteration 35 : 0.6320191282174369
Loss in iteration 36 : 0.8461005593770822
Loss in iteration 37 : 1.2977153549721927
Loss in iteration 38 : 1.1561055718633055
Loss in iteration 39 : 1.2930330058239348
Loss in iteration 40 : 1.1333441035432883
Loss in iteration 41 : 1.2462246835013138
Loss in iteration 42 : 1.0734983142441474
Loss in iteration 43 : 1.130882013213547
Loss in iteration 44 : 1.0483396424750957
Loss in iteration 45 : 1.0291064813087447
Loss in iteration 46 : 0.9498052420240642
Loss in iteration 47 : 0.9440975140625927
Loss in iteration 48 : 0.894406650322532
Loss in iteration 49 : 0.888782851804398
Loss in iteration 50 : 0.8478106301688815
Loss in iteration 51 : 0.8403980569769626
Loss in iteration 52 : 0.8183180555001275
Loss in iteration 53 : 0.8176232589709086
Loss in iteration 54 : 0.7929841611424492
Loss in iteration 55 : 0.8045955543442945
Loss in iteration 56 : 0.7965702562400802
Loss in iteration 57 : 0.8288591733202163
Loss in iteration 58 : 0.7957538403582748
Loss in iteration 59 : 0.840497605588017
Loss in iteration 60 : 0.8005865589918885
Loss in iteration 61 : 0.8574803251408118
Loss in iteration 62 : 0.8117099867770111
Loss in iteration 63 : 0.8713698266169511
Loss in iteration 64 : 0.8164380039629165
Loss in iteration 65 : 0.8817951722053601
Loss in iteration 66 : 0.8260197077037417
Loss in iteration 67 : 0.8874642451515884
Loss in iteration 68 : 0.8276313662370055
Loss in iteration 69 : 0.88058556460988
Loss in iteration 70 : 0.8281406283232507
Loss in iteration 71 : 0.8710511999581984
Loss in iteration 72 : 0.8229715088554267
Loss in iteration 73 : 0.8586018459527087
Loss in iteration 74 : 0.8232193714405602
Loss in iteration 75 : 0.8599308838807481
Loss in iteration 76 : 0.8246491367985557
Loss in iteration 77 : 0.861629426993466
Loss in iteration 78 : 0.8250096829567262
Loss in iteration 79 : 0.8691968705340333
Loss in iteration 80 : 0.8367290539265937
Loss in iteration 81 : 0.877976457663047
Loss in iteration 82 : 0.842393375094076
Loss in iteration 83 : 0.8851550279855209
Loss in iteration 84 : 0.8486560210788591
Loss in iteration 85 : 0.8865895559226905
Loss in iteration 86 : 0.846340291032814
Loss in iteration 87 : 0.8844419408487949
Loss in iteration 88 : 0.8456767535903363
Loss in iteration 89 : 0.880697482174486
Loss in iteration 90 : 0.8366672180586678
Loss in iteration 91 : 0.8709596694703596
Loss in iteration 92 : 0.8370202431684943
Loss in iteration 93 : 0.8738118571760999
Loss in iteration 94 : 0.8352332588096781
Loss in iteration 95 : 0.8703157747006581
Loss in iteration 96 : 0.8282785761161765
Loss in iteration 97 : 0.8694486040939701
Loss in iteration 98 : 0.829431161512136
Loss in iteration 99 : 0.8688021534683666
Loss in iteration 100 : 0.8278649716849465
Loss in iteration 101 : 0.8698639998474696
Loss in iteration 102 : 0.8307467309680572
Loss in iteration 103 : 0.8712890702763291
Loss in iteration 104 : 0.8298883450297425
Loss in iteration 105 : 0.8713960895246892
Loss in iteration 106 : 0.8298121686829127
Loss in iteration 107 : 0.8718198964549067
Loss in iteration 108 : 0.832688429651201
Loss in iteration 109 : 0.8744753479483404
Loss in iteration 110 : 0.8353587701827039
Loss in iteration 111 : 0.8759256478932355
Loss in iteration 112 : 0.8349514571128022
Loss in iteration 113 : 0.8746665579571761
Loss in iteration 114 : 0.832716460386563
Loss in iteration 115 : 0.8734262155021493
Loss in iteration 116 : 0.8331274761693644
Loss in iteration 117 : 0.8724763108704693
Loss in iteration 118 : 0.833159498818756
Loss in iteration 119 : 0.8725260143214234
Loss in iteration 120 : 0.8340741001378535
Loss in iteration 121 : 0.8750291441014726
Loss in iteration 122 : 0.8344826507037139
Loss in iteration 123 : 0.8742614556948033
Loss in iteration 124 : 0.8332100323444943
Loss in iteration 125 : 0.8702923049812873
Loss in iteration 126 : 0.8274449743447726
Loss in iteration 127 : 0.8667555176005586
Loss in iteration 128 : 0.8242938825022322
Loss in iteration 129 : 0.8667687662370028
Loss in iteration 130 : 0.827149113332563
Loss in iteration 131 : 0.8710408530493768
Loss in iteration 132 : 0.83711806470846
Loss in iteration 133 : 0.8777557195449308
Loss in iteration 134 : 0.8399936346806152
Loss in iteration 135 : 0.8769933236712373
Loss in iteration 136 : 0.8382147183113022
Loss in iteration 137 : 0.8772258614245553
Loss in iteration 138 : 0.8362809647187216
Loss in iteration 139 : 0.8710229664684459
Loss in iteration 140 : 0.8309785241008133
Loss in iteration 141 : 0.8675844873080537
Loss in iteration 142 : 0.8221885935351565
Loss in iteration 143 : 0.8619041853441809
Loss in iteration 144 : 0.8176964172695425
Loss in iteration 145 : 0.8638550131215984
Loss in iteration 146 : 0.8247000560763782
Loss in iteration 147 : 0.8678257246215351
Loss in iteration 148 : 0.8323697122163539
Loss in iteration 149 : 0.8725136716969337
Loss in iteration 150 : 0.8377051463150529
Loss in iteration 151 : 0.878937592167601
Loss in iteration 152 : 0.839728966392096
Loss in iteration 153 : 0.8776089391225878
Loss in iteration 154 : 0.835673655040446
Loss in iteration 155 : 0.8702178964512202
Loss in iteration 156 : 0.8290775225721654
Loss in iteration 157 : 0.8637350341403298
Loss in iteration 158 : 0.8227463591929004
Loss in iteration 159 : 0.8618569710620779
Loss in iteration 160 : 0.8217967656174696
Loss in iteration 161 : 0.8639193659009097
Loss in iteration 162 : 0.826227062691155
Loss in iteration 163 : 0.8682703210578888
Loss in iteration 164 : 0.8295197527432093
Loss in iteration 165 : 0.869663131394361
Loss in iteration 166 : 0.8355317185943142
Loss in iteration 167 : 0.8772723008613262
Loss in iteration 168 : 0.8365980394142335
Loss in iteration 169 : 0.8729789016337443
Loss in iteration 170 : 0.835130599414519
Loss in iteration 171 : 0.8699208760583363
Loss in iteration 172 : 0.8303875751429932
Loss in iteration 173 : 0.8673178662319706
Loss in iteration 174 : 0.8248593714804151
Loss in iteration 175 : 0.8654015693757405
Loss in iteration 176 : 0.825133468461031
Loss in iteration 177 : 0.865596136355724
Loss in iteration 178 : 0.8262226764658301
Loss in iteration 179 : 0.8698404596418065
Loss in iteration 180 : 0.8342436598160994
Loss in iteration 181 : 0.873334351411888
Loss in iteration 182 : 0.8386240759010349
Loss in iteration 183 : 0.8746401355309569
Loss in iteration 184 : 0.8334417857536064
Loss in iteration 185 : 0.8684360010330232
Loss in iteration 186 : 0.8279967534216685
Loss in iteration 187 : 0.8676860642966926
Loss in iteration 188 : 0.8268462313564824
Loss in iteration 189 : 0.8667319521967729
Loss in iteration 190 : 0.8271485136904273
Loss in iteration 191 : 0.8662770095107198
Loss in iteration 192 : 0.8279225496536644
Loss in iteration 193 : 0.871021187602517
Loss in iteration 194 : 0.8352023200482362
Loss in iteration 195 : 0.8712157538654471
Loss in iteration 196 : 0.833305248076441
Loss in iteration 197 : 0.8693873960873654
Loss in iteration 198 : 0.829747574851693
Loss in iteration 199 : 0.8676126071877297
Loss in iteration 200 : 0.8255845848790385
Testing accuracy  of updater 2 on alg 1 with rate 1.0 = 0.79, training accuracy 0.784125, time elapsed: 2807 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.117670059666827
Loss in iteration 3 : 2.4899576746280605
Loss in iteration 4 : 1.236540752899759
Loss in iteration 5 : 1.585768250077945
Loss in iteration 6 : 1.7407970354987994
Loss in iteration 7 : 0.8005412852808674
Loss in iteration 8 : 1.0454761666719146
Loss in iteration 9 : 1.2864485057067518
Loss in iteration 10 : 0.8526607111982687
Loss in iteration 11 : 0.8829073847934795
Loss in iteration 12 : 0.8061262563837247
Loss in iteration 13 : 0.7548443738604792
Loss in iteration 14 : 0.6925542906072428
Loss in iteration 15 : 0.6655647792869839
Loss in iteration 16 : 0.6468543936649969
Loss in iteration 17 : 0.6377330849525835
Loss in iteration 18 : 0.6335353991025554
Loss in iteration 19 : 0.6286586397315214
Loss in iteration 20 : 0.6236030457436847
Loss in iteration 21 : 0.6191705597079533
Loss in iteration 22 : 0.614516789706514
Loss in iteration 23 : 0.60932263709071
Loss in iteration 24 : 0.6037280356345386
Loss in iteration 25 : 0.5976792497616183
Loss in iteration 26 : 0.5912943348974122
Loss in iteration 27 : 0.5846162009747777
Loss in iteration 28 : 0.5777785448601885
Loss in iteration 29 : 0.5709122126499825
Loss in iteration 30 : 0.5641635715785769
Loss in iteration 31 : 0.5576998712219201
Loss in iteration 32 : 0.5516240828579444
Loss in iteration 33 : 0.5460009509570989
Loss in iteration 34 : 0.5408350388634064
Loss in iteration 35 : 0.5361528816323854
Loss in iteration 36 : 0.5320827861981166
Loss in iteration 37 : 0.5286545926161902
Loss in iteration 38 : 0.5261651104947671
Loss in iteration 39 : 0.5258699516067142
Loss in iteration 40 : 0.5353091102026608
Loss in iteration 41 : 0.5961014293635187
Loss in iteration 42 : 0.8129353704423353
Loss in iteration 43 : 1.0602517924488384
Loss in iteration 44 : 0.9341733730601838
Loss in iteration 45 : 0.9635333561558433
Loss in iteration 46 : 0.9233890870874696
Loss in iteration 47 : 0.9203934679852059
Loss in iteration 48 : 0.8624712095615894
Loss in iteration 49 : 0.8393421957563335
Loss in iteration 50 : 0.8196997263171375
Loss in iteration 51 : 0.779086813876627
Loss in iteration 52 : 0.7503850720718345
Loss in iteration 53 : 0.7248008084855051
Loss in iteration 54 : 0.7034493163597424
Loss in iteration 55 : 0.6887519093940118
Loss in iteration 56 : 0.6763360145171179
Loss in iteration 57 : 0.6688989168070032
Loss in iteration 58 : 0.6656368023908822
Loss in iteration 59 : 0.6614527915397825
Loss in iteration 60 : 0.6611597695738519
Loss in iteration 61 : 0.6643728715296069
Loss in iteration 62 : 0.6595631723899219
Loss in iteration 63 : 0.6640296227543028
Loss in iteration 64 : 0.6594623693538656
Loss in iteration 65 : 0.6734001925208278
Loss in iteration 66 : 0.6626696387218834
Loss in iteration 67 : 0.6785870965782627
Loss in iteration 68 : 0.6728893122747804
Loss in iteration 69 : 0.6881637752005115
Loss in iteration 70 : 0.6833112184130378
Loss in iteration 71 : 0.6959810729429251
Loss in iteration 72 : 0.6902258336409531
Loss in iteration 73 : 0.6943844229798664
Loss in iteration 74 : 0.6865883541364016
Loss in iteration 75 : 0.6940504354988438
Loss in iteration 76 : 0.6862501980621353
Loss in iteration 77 : 0.6894651195278221
Loss in iteration 78 : 0.6827730175228105
Loss in iteration 79 : 0.6872479115231557
Loss in iteration 80 : 0.6836254595085675
Loss in iteration 81 : 0.6903132450013741
Loss in iteration 82 : 0.6892739541717886
Loss in iteration 83 : 0.6975912442402907
Loss in iteration 84 : 0.6946658963913109
Loss in iteration 85 : 0.6997152661661701
Loss in iteration 86 : 0.6960126075476467
Loss in iteration 87 : 0.7018725359171347
Loss in iteration 88 : 0.697919863038069
Loss in iteration 89 : 0.7013942304138329
Loss in iteration 90 : 0.6970870325872941
Loss in iteration 91 : 0.6969826739242795
Loss in iteration 92 : 0.6927957238051963
Loss in iteration 93 : 0.6942228255255749
Loss in iteration 94 : 0.6910778410009412
Loss in iteration 95 : 0.6944269619888805
Loss in iteration 96 : 0.6903700484365525
Loss in iteration 97 : 0.6959328613044027
Loss in iteration 98 : 0.6928883480463636
Loss in iteration 99 : 0.6976952180845776
Loss in iteration 100 : 0.6963291087048884
Loss in iteration 101 : 0.6976582759971911
Loss in iteration 102 : 0.6928776425098953
Loss in iteration 103 : 0.6958647143371388
Loss in iteration 104 : 0.6882161210843124
Loss in iteration 105 : 0.6943739973387199
Loss in iteration 106 : 0.6873223999673214
Loss in iteration 107 : 0.6950743142032292
Loss in iteration 108 : 0.6893142288495594
Loss in iteration 109 : 0.6956122053994436
Loss in iteration 110 : 0.6882420402183921
Loss in iteration 111 : 0.6946420175236976
Loss in iteration 112 : 0.6877911909598857
Loss in iteration 113 : 0.6950158178835941
Loss in iteration 114 : 0.6892069822722106
Loss in iteration 115 : 0.6946561279062005
Loss in iteration 116 : 0.6883666880584731
Loss in iteration 117 : 0.693214048985816
Loss in iteration 118 : 0.687691906250736
Loss in iteration 119 : 0.6936613890578864
Loss in iteration 120 : 0.68834141060932
Loss in iteration 121 : 0.692813593264558
Loss in iteration 122 : 0.6871463873920506
Loss in iteration 123 : 0.6922377526680958
Loss in iteration 124 : 0.687986937329694
Loss in iteration 125 : 0.6924420858143119
Loss in iteration 126 : 0.6884150190686931
Loss in iteration 127 : 0.6943028798630264
Loss in iteration 128 : 0.6915184848047782
Loss in iteration 129 : 0.6957740986012193
Loss in iteration 130 : 0.6906692379421537
Loss in iteration 131 : 0.6945374552706332
Loss in iteration 132 : 0.6899851738242393
Loss in iteration 133 : 0.6934231215780133
Loss in iteration 134 : 0.6891020671914319
Loss in iteration 135 : 0.6920345360985755
Loss in iteration 136 : 0.687939575883768
Loss in iteration 137 : 0.6924652696236737
Loss in iteration 138 : 0.6881365913646387
Loss in iteration 139 : 0.6923489600439566
Loss in iteration 140 : 0.6871394811562909
Loss in iteration 141 : 0.6926283980568193
Loss in iteration 142 : 0.6877042320695207
Loss in iteration 143 : 0.6928513996931863
Loss in iteration 144 : 0.6878963403440244
Loss in iteration 145 : 0.6930063095787038
Loss in iteration 146 : 0.6896326344673018
Loss in iteration 147 : 0.6933822456152211
Loss in iteration 148 : 0.6885948841800498
Loss in iteration 149 : 0.6929754560082888
Loss in iteration 150 : 0.6866267688769568
Loss in iteration 151 : 0.6920006357229623
Loss in iteration 152 : 0.6875931599383109
Loss in iteration 153 : 0.692718191992469
Loss in iteration 154 : 0.6883245707836799
Loss in iteration 155 : 0.693603159887631
Loss in iteration 156 : 0.6890623586736982
Loss in iteration 157 : 0.6932331020869549
Loss in iteration 158 : 0.6884842758901668
Loss in iteration 159 : 0.6923973376858671
Loss in iteration 160 : 0.6873087078189066
Loss in iteration 161 : 0.6919790894682025
Loss in iteration 162 : 0.6863692754685846
Loss in iteration 163 : 0.6917814420055113
Loss in iteration 164 : 0.6869165925066424
Loss in iteration 165 : 0.6929505006041634
Loss in iteration 166 : 0.6890015549762707
Loss in iteration 167 : 0.6922967541014523
Loss in iteration 168 : 0.6878645397862585
Loss in iteration 169 : 0.693240480751938
Loss in iteration 170 : 0.6882741540276779
Loss in iteration 171 : 0.6929235392315044
Loss in iteration 172 : 0.6877075303366157
Loss in iteration 173 : 0.6926282250386983
Loss in iteration 174 : 0.6882686390574775
Loss in iteration 175 : 0.6928685148215056
Loss in iteration 176 : 0.6877743089459484
Loss in iteration 177 : 0.6931910226533364
Loss in iteration 178 : 0.688509982466486
Loss in iteration 179 : 0.6927462640693506
Loss in iteration 180 : 0.686815215638177
Loss in iteration 181 : 0.6910177616025506
Loss in iteration 182 : 0.6850757794017825
Loss in iteration 183 : 0.6890823712013705
Loss in iteration 184 : 0.680941692738878
Loss in iteration 185 : 0.6887192578185364
Loss in iteration 186 : 0.6840484027333148
Loss in iteration 187 : 0.6914846599682662
Loss in iteration 188 : 0.6892498036452851
Loss in iteration 189 : 0.6938069762152889
Loss in iteration 190 : 0.6888347285892682
Loss in iteration 191 : 0.692189857358832
Loss in iteration 192 : 0.6877012301597576
Loss in iteration 193 : 0.692314496517017
Loss in iteration 194 : 0.6871438234579521
Loss in iteration 195 : 0.6911965225085378
Loss in iteration 196 : 0.685610902390275
Loss in iteration 197 : 0.6890609411470139
Loss in iteration 198 : 0.6823920935218531
Loss in iteration 199 : 0.6888153339813747
Loss in iteration 200 : 0.6827031118846683
Testing accuracy  of updater 2 on alg 1 with rate 0.7000000000000001 = 0.7885, training accuracy 0.786875, time elapsed: 2556 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9377828636054183
Loss in iteration 3 : 0.9269954514537124
Loss in iteration 4 : 0.8727246771428249
Loss in iteration 5 : 0.8358977328125622
Loss in iteration 6 : 1.0773340277245285
Loss in iteration 7 : 0.7312612498065711
Loss in iteration 8 : 0.8403404170807344
Loss in iteration 9 : 0.7432523346028325
Loss in iteration 10 : 0.7407779690353814
Loss in iteration 11 : 0.658144089573437
Loss in iteration 12 : 0.6256040447677204
Loss in iteration 13 : 0.5957390892900281
Loss in iteration 14 : 0.5726965388969208
Loss in iteration 15 : 0.5610700213449977
Loss in iteration 16 : 0.5540662413066688
Loss in iteration 17 : 0.5507304125501202
Loss in iteration 18 : 0.5491147049253073
Loss in iteration 19 : 0.5479508388732808
Loss in iteration 20 : 0.5466742042421782
Loss in iteration 21 : 0.5452801516307549
Loss in iteration 22 : 0.5437798006665571
Loss in iteration 23 : 0.542274632197636
Loss in iteration 24 : 0.5406237031640804
Loss in iteration 25 : 0.5388356484550104
Loss in iteration 26 : 0.5369051113837227
Loss in iteration 27 : 0.5348753186168779
Loss in iteration 28 : 0.5327686707821695
Loss in iteration 29 : 0.5306396554641277
Loss in iteration 30 : 0.5285343793608263
Loss in iteration 31 : 0.5264493125048548
Loss in iteration 32 : 0.5244522832273892
Loss in iteration 33 : 0.5225196547687869
Loss in iteration 34 : 0.5207211795529758
Loss in iteration 35 : 0.5191101646859797
Loss in iteration 36 : 0.5177075687609713
Loss in iteration 37 : 0.5164238832196045
Loss in iteration 38 : 0.5153526358526125
Loss in iteration 39 : 0.5146337858386997
Loss in iteration 40 : 0.5142319433559365
Loss in iteration 41 : 0.5137796816836621
Loss in iteration 42 : 0.5141823617740459
Loss in iteration 43 : 0.5141705892647478
Loss in iteration 44 : 0.5151098372311131
Loss in iteration 45 : 0.514524583554926
Loss in iteration 46 : 0.515272663391737
Loss in iteration 47 : 0.5151534639556531
Loss in iteration 48 : 0.516269155053751
Loss in iteration 49 : 0.516390762570566
Loss in iteration 50 : 0.518445719202172
Loss in iteration 51 : 0.5195879560349951
Loss in iteration 52 : 0.5228907217274914
Loss in iteration 53 : 0.5233307458354188
Loss in iteration 54 : 0.5266646524018112
Loss in iteration 55 : 0.5289280482142091
Loss in iteration 56 : 0.5303408467606814
Loss in iteration 57 : 0.530714814669077
Loss in iteration 58 : 0.5323644744681032
Loss in iteration 59 : 0.5323327710011326
Loss in iteration 60 : 0.5344014527764241
Loss in iteration 61 : 0.5337916688450047
Loss in iteration 62 : 0.5341384129322556
Loss in iteration 63 : 0.5338589377826566
Loss in iteration 64 : 0.5330527186135319
Loss in iteration 65 : 0.530602961884992
Loss in iteration 66 : 0.5301158502624127
Loss in iteration 67 : 0.5258622346682539
Loss in iteration 68 : 0.526191832583479
Loss in iteration 69 : 0.5230403545570552
Loss in iteration 70 : 0.523863425856433
Loss in iteration 71 : 0.5214464314592747
Loss in iteration 72 : 0.5223911595142006
Loss in iteration 73 : 0.5211388124727278
Loss in iteration 74 : 0.5225127267937262
Loss in iteration 75 : 0.5214145024004662
Loss in iteration 76 : 0.5232255287684054
Loss in iteration 77 : 0.5219194869101669
Loss in iteration 78 : 0.5235161300277632
Loss in iteration 79 : 0.5222169815696334
Loss in iteration 80 : 0.5237943500454701
Loss in iteration 81 : 0.5222927303400324
Loss in iteration 82 : 0.5235137711584478
Loss in iteration 83 : 0.5219411851487015
Loss in iteration 84 : 0.5230649156101541
Loss in iteration 85 : 0.5216945979033114
Loss in iteration 86 : 0.5230661866188756
Loss in iteration 87 : 0.5216696844787692
Loss in iteration 88 : 0.5227244139374175
Loss in iteration 89 : 0.521715551594258
Loss in iteration 90 : 0.5230367645606339
Loss in iteration 91 : 0.5216872328276839
Loss in iteration 92 : 0.522994091759289
Loss in iteration 93 : 0.5214451475555048
Loss in iteration 94 : 0.5229163218321212
Loss in iteration 95 : 0.5215052139995126
Loss in iteration 96 : 0.5227494406062226
Loss in iteration 97 : 0.5215324824180775
Loss in iteration 98 : 0.5232830427494853
Loss in iteration 99 : 0.521639058161362
Loss in iteration 100 : 0.5234394235575799
Loss in iteration 101 : 0.5216230686732961
Loss in iteration 102 : 0.5233817952073047
Loss in iteration 103 : 0.521605733547813
Loss in iteration 104 : 0.5236683149491795
Loss in iteration 105 : 0.5218160758850786
Loss in iteration 106 : 0.5233653074532617
Loss in iteration 107 : 0.5211406569275083
Loss in iteration 108 : 0.522972458030245
Loss in iteration 109 : 0.5211481443805632
Loss in iteration 110 : 0.5230953268345511
Loss in iteration 111 : 0.5212946867352717
Loss in iteration 112 : 0.5232059730176072
Loss in iteration 113 : 0.5215535854237177
Loss in iteration 114 : 0.5235336838605263
Loss in iteration 115 : 0.5218864811347046
Loss in iteration 116 : 0.5237554258634666
Loss in iteration 117 : 0.5219676592687974
Loss in iteration 118 : 0.5234156095690359
Loss in iteration 119 : 0.5211763442810519
Loss in iteration 120 : 0.5225946223918456
Loss in iteration 121 : 0.520643022254926
Loss in iteration 122 : 0.5220467424140008
Loss in iteration 123 : 0.5202664979538428
Loss in iteration 124 : 0.5220132897597134
Loss in iteration 125 : 0.520290254868182
Loss in iteration 126 : 0.5223709755927043
Loss in iteration 127 : 0.5208168488178839
Loss in iteration 128 : 0.5227600265566927
Loss in iteration 129 : 0.5215763190684131
Loss in iteration 130 : 0.5231077725619789
Loss in iteration 131 : 0.5214012869215853
Loss in iteration 132 : 0.5227899202126366
Loss in iteration 133 : 0.5212379099386147
Loss in iteration 134 : 0.5226599388200404
Loss in iteration 135 : 0.520711869545886
Loss in iteration 136 : 0.5220347483505566
Loss in iteration 137 : 0.520288046026639
Loss in iteration 138 : 0.5213927343254716
Loss in iteration 139 : 0.5199407532499527
Loss in iteration 140 : 0.5215072092025145
Loss in iteration 141 : 0.5199441707122275
Loss in iteration 142 : 0.5212611741334939
Loss in iteration 143 : 0.5194257312924672
Loss in iteration 144 : 0.5207840695868262
Loss in iteration 145 : 0.5187666172265986
Loss in iteration 146 : 0.5196503831707951
Loss in iteration 147 : 0.5186445663093134
Loss in iteration 148 : 0.5195664080330863
Loss in iteration 149 : 0.5184714724137316
Loss in iteration 150 : 0.5195593904143935
Loss in iteration 151 : 0.5188121545352551
Loss in iteration 152 : 0.5202915032441483
Loss in iteration 153 : 0.5188041358431815
Loss in iteration 154 : 0.5200087591703748
Loss in iteration 155 : 0.5186605310968011
Loss in iteration 156 : 0.5195781574790879
Loss in iteration 157 : 0.5186545684700593
Loss in iteration 158 : 0.5196658183899876
Loss in iteration 159 : 0.518522643108844
Loss in iteration 160 : 0.5194999145081386
Loss in iteration 161 : 0.5180430569145215
Loss in iteration 162 : 0.5194582163285009
Loss in iteration 163 : 0.5182069413372784
Loss in iteration 164 : 0.5195499154069257
Loss in iteration 165 : 0.5183862549253595
Loss in iteration 166 : 0.5195027076714366
Loss in iteration 167 : 0.5182685934079738
Loss in iteration 168 : 0.5194050496569966
Loss in iteration 169 : 0.5181125717089873
Loss in iteration 170 : 0.5193663695975244
Loss in iteration 171 : 0.5180782111560331
Loss in iteration 172 : 0.5193993239911376
Loss in iteration 173 : 0.5181322631736208
Loss in iteration 174 : 0.5194991311937331
Loss in iteration 175 : 0.5185401318839767
Loss in iteration 176 : 0.519831077986873
Loss in iteration 177 : 0.5186448944892772
Loss in iteration 178 : 0.5198437884134448
Loss in iteration 179 : 0.5185441971050045
Loss in iteration 180 : 0.5196405563182511
Loss in iteration 181 : 0.518361788730142
Loss in iteration 182 : 0.5195232736126439
Loss in iteration 183 : 0.5182334317137267
Loss in iteration 184 : 0.5191469751252844
Loss in iteration 185 : 0.5176741870811138
Loss in iteration 186 : 0.5185897357755166
Loss in iteration 187 : 0.5173577398102437
Loss in iteration 188 : 0.5180566584857065
Loss in iteration 189 : 0.5165653867277683
Loss in iteration 190 : 0.5178823165612744
Loss in iteration 191 : 0.5162483835453393
Loss in iteration 192 : 0.517657381184913
Loss in iteration 193 : 0.5165648530568876
Loss in iteration 194 : 0.5179210741954421
Loss in iteration 195 : 0.5170852497805022
Loss in iteration 196 : 0.5182306744350458
Loss in iteration 197 : 0.5175302272023196
Loss in iteration 198 : 0.5184987187579024
Loss in iteration 199 : 0.5175310853052543
Loss in iteration 200 : 0.5186702036125639
Testing accuracy  of updater 2 on alg 1 with rate 0.4 = 0.784, training accuracy 0.789, time elapsed: 2766 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9562833339385256
Loss in iteration 3 : 0.9151041458516825
Loss in iteration 4 : 0.9031458434782358
Loss in iteration 5 : 0.8756135068483786
Loss in iteration 6 : 0.8464201034407033
Loss in iteration 7 : 0.8168193527516882
Loss in iteration 8 : 0.7862626647888489
Loss in iteration 9 : 0.7541128418533996
Loss in iteration 10 : 0.7214528212914179
Loss in iteration 11 : 0.6935804189682547
Loss in iteration 12 : 0.6679083150732179
Loss in iteration 13 : 0.6442895948804946
Loss in iteration 14 : 0.6250960080791541
Loss in iteration 15 : 0.6101215133606694
Loss in iteration 16 : 0.5981960898365524
Loss in iteration 17 : 0.58858787732707
Loss in iteration 18 : 0.5808346356886059
Loss in iteration 19 : 0.5745479929062344
Loss in iteration 20 : 0.5693767377336548
Loss in iteration 21 : 0.5650182243441499
Loss in iteration 22 : 0.561335049164717
Loss in iteration 23 : 0.5581914818118894
Loss in iteration 24 : 0.5554844156707133
Loss in iteration 25 : 0.5531237585654804
Loss in iteration 26 : 0.5509658150612174
Loss in iteration 27 : 0.5489913212494533
Loss in iteration 28 : 0.5472016207794879
Loss in iteration 29 : 0.5455856887609407
Loss in iteration 30 : 0.5441170948437498
Loss in iteration 31 : 0.5427484664836966
Loss in iteration 32 : 0.5414626074222827
Loss in iteration 33 : 0.5403038467162744
Loss in iteration 34 : 0.5392375215597603
Loss in iteration 35 : 0.5382536369908166
Loss in iteration 36 : 0.5373666280655288
Loss in iteration 37 : 0.536541176079028
Loss in iteration 38 : 0.5357506529036141
Loss in iteration 39 : 0.5349892395359332
Loss in iteration 40 : 0.5342613055504877
Loss in iteration 41 : 0.5335609316952008
Loss in iteration 42 : 0.5328791821819124
Loss in iteration 43 : 0.5322211793826275
Loss in iteration 44 : 0.5315984527887246
Loss in iteration 45 : 0.5309975987515999
Loss in iteration 46 : 0.5304247045075984
Loss in iteration 47 : 0.5298777944603968
Loss in iteration 48 : 0.5293476465479107
Loss in iteration 49 : 0.5288338013007164
Loss in iteration 50 : 0.5283343921824617
Loss in iteration 51 : 0.5278435240709317
Loss in iteration 52 : 0.5273634671845503
Loss in iteration 53 : 0.5268917211834448
Loss in iteration 54 : 0.5264307720131595
Loss in iteration 55 : 0.5259821891588071
Loss in iteration 56 : 0.5255463471063619
Loss in iteration 57 : 0.5251274432256464
Loss in iteration 58 : 0.5247223325506545
Loss in iteration 59 : 0.5243301628067814
Loss in iteration 60 : 0.5239502994208501
Loss in iteration 61 : 0.5235851477323936
Loss in iteration 62 : 0.5232342104141386
Loss in iteration 63 : 0.5228943625936185
Loss in iteration 64 : 0.5225665950413078
Loss in iteration 65 : 0.5222476836097101
Loss in iteration 66 : 0.5219370081744762
Loss in iteration 67 : 0.5216328552512256
Loss in iteration 68 : 0.5213372295978648
Loss in iteration 69 : 0.5210488001542297
Loss in iteration 70 : 0.5207704829168756
Loss in iteration 71 : 0.5205010082489713
Loss in iteration 72 : 0.5202412028281092
Loss in iteration 73 : 0.5199903724343076
Loss in iteration 74 : 0.5197500026898333
Loss in iteration 75 : 0.5195184289227381
Loss in iteration 76 : 0.5192945001053136
Loss in iteration 77 : 0.5190765190359147
Loss in iteration 78 : 0.518864944095257
Loss in iteration 79 : 0.5186589814073534
Loss in iteration 80 : 0.5184575507668285
Loss in iteration 81 : 0.5182619470261944
Loss in iteration 82 : 0.5180737541888061
Loss in iteration 83 : 0.5178907089904562
Loss in iteration 84 : 0.5177126878499383
Loss in iteration 85 : 0.5175398979067749
Loss in iteration 86 : 0.5173711936878098
Loss in iteration 87 : 0.5172069549616396
Loss in iteration 88 : 0.5170462010376484
Loss in iteration 89 : 0.516888596880037
Loss in iteration 90 : 0.5167354161603687
Loss in iteration 91 : 0.5165863314951791
Loss in iteration 92 : 0.51644235561225
Loss in iteration 93 : 0.5163029250753683
Loss in iteration 94 : 0.5161676742423505
Loss in iteration 95 : 0.5160355886394398
Loss in iteration 96 : 0.5159068691124978
Loss in iteration 97 : 0.5157814365612896
Loss in iteration 98 : 0.5156592343090137
Loss in iteration 99 : 0.515540188970517
Loss in iteration 100 : 0.5154230509278062
Loss in iteration 101 : 0.5153085334907404
Loss in iteration 102 : 0.5151966750956896
Loss in iteration 103 : 0.5150872211198386
Loss in iteration 104 : 0.5149797805083951
Loss in iteration 105 : 0.514874995149283
Loss in iteration 106 : 0.5147725435339733
Loss in iteration 107 : 0.5146717737542186
Loss in iteration 108 : 0.5145731738231846
Loss in iteration 109 : 0.514476307076311
Loss in iteration 110 : 0.5143810899057123
Loss in iteration 111 : 0.5142875618897955
Loss in iteration 112 : 0.5141955429872153
Loss in iteration 113 : 0.5141053698857949
Loss in iteration 114 : 0.5140177184402654
Loss in iteration 115 : 0.5139320179715272
Loss in iteration 116 : 0.5138485180450766
Loss in iteration 117 : 0.5137670108233108
Loss in iteration 118 : 0.5136864615770491
Loss in iteration 119 : 0.513607166042709
Loss in iteration 120 : 0.5135303166944966
Loss in iteration 121 : 0.5134564420786283
Loss in iteration 122 : 0.513383969044208
Loss in iteration 123 : 0.5133125698448634
Loss in iteration 124 : 0.5132418171982429
Loss in iteration 125 : 0.5131721263318937
Loss in iteration 126 : 0.513103575795147
Loss in iteration 127 : 0.5130363413816431
Loss in iteration 128 : 0.5129696326084843
Loss in iteration 129 : 0.5129037187216503
Loss in iteration 130 : 0.5128382963857143
Loss in iteration 131 : 0.5127736722463774
Loss in iteration 132 : 0.5127100612133874
Loss in iteration 133 : 0.5126473703116414
Loss in iteration 134 : 0.5125854661926451
Loss in iteration 135 : 0.5125247875075425
Loss in iteration 136 : 0.512465680363952
Loss in iteration 137 : 0.5124078153171882
Loss in iteration 138 : 0.5123514806729833
Loss in iteration 139 : 0.5122961710615959
Loss in iteration 140 : 0.5122417290643178
Loss in iteration 141 : 0.5121877573057368
Loss in iteration 142 : 0.5121355225584361
Loss in iteration 143 : 0.5120843667661804
Loss in iteration 144 : 0.5120344377432137
Loss in iteration 145 : 0.5119849126599052
Loss in iteration 146 : 0.5119360044381188
Loss in iteration 147 : 0.5118879054936578
Loss in iteration 148 : 0.5118405129287321
Loss in iteration 149 : 0.5117941965001244
Loss in iteration 150 : 0.5117486495060333
Loss in iteration 151 : 0.5117037451103725
Loss in iteration 152 : 0.5116601578530449
Loss in iteration 153 : 0.5116180422217782
Loss in iteration 154 : 0.511576650494478
Loss in iteration 155 : 0.5115358627770701
Loss in iteration 156 : 0.5114965332797671
Loss in iteration 157 : 0.5114583321741847
Loss in iteration 158 : 0.5114209142746666
Loss in iteration 159 : 0.5113844404623379
Loss in iteration 160 : 0.5113485527327956
Loss in iteration 161 : 0.5113130580419625
Loss in iteration 162 : 0.511278209373588
Loss in iteration 163 : 0.5112436735700809
Loss in iteration 164 : 0.5112102225007448
Loss in iteration 165 : 0.5111779921959471
Loss in iteration 166 : 0.5111468159087655
Loss in iteration 167 : 0.5111162456995653
Loss in iteration 168 : 0.5110861877272568
Loss in iteration 169 : 0.5110564696462476
Loss in iteration 170 : 0.5110277330255028
Loss in iteration 171 : 0.5109993143552236
Loss in iteration 172 : 0.5109713301654752
Loss in iteration 173 : 0.5109437129066434
Loss in iteration 174 : 0.510916745086902
Loss in iteration 175 : 0.5108899577930198
Loss in iteration 176 : 0.5108634189551202
Loss in iteration 177 : 0.5108371009600499
Loss in iteration 178 : 0.5108110342987573
Loss in iteration 179 : 0.5107852116846722
Loss in iteration 180 : 0.5107599463547147
Loss in iteration 181 : 0.5107349103882687
Loss in iteration 182 : 0.5107101540891066
Loss in iteration 183 : 0.5106859547899284
Loss in iteration 184 : 0.510662289370379
Loss in iteration 185 : 0.51063886247692
Loss in iteration 186 : 0.5106158415483403
Loss in iteration 187 : 0.510592903435241
Loss in iteration 188 : 0.5105701828953872
Loss in iteration 189 : 0.5105477189568396
Loss in iteration 190 : 0.5105255469678736
Loss in iteration 191 : 0.5105039186046715
Loss in iteration 192 : 0.5104828496046954
Loss in iteration 193 : 0.5104621524222174
Loss in iteration 194 : 0.5104414843204208
Loss in iteration 195 : 0.5104210909490896
Loss in iteration 196 : 0.51040097778258
Loss in iteration 197 : 0.5103810764723461
Loss in iteration 198 : 0.5103614552783097
Loss in iteration 199 : 0.510341990479319
Loss in iteration 200 : 0.5103227624443522
Testing accuracy  of updater 2 on alg 1 with rate 0.1 = 0.781, training accuracy 0.787, time elapsed: 3177 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9692726176420401
Loss in iteration 3 : 0.9302409853516811
Loss in iteration 4 : 0.9106543858728405
Loss in iteration 5 : 0.9005325631614257
Loss in iteration 6 : 0.8795616417334152
Loss in iteration 7 : 0.855631918747328
Loss in iteration 8 : 0.831859752797943
Loss in iteration 9 : 0.8080384168625909
Loss in iteration 10 : 0.7838652685481279
Loss in iteration 11 : 0.7587968362617039
Loss in iteration 12 : 0.7330847079910767
Loss in iteration 13 : 0.7095460864878773
Loss in iteration 14 : 0.6886544917863499
Loss in iteration 15 : 0.66866782550679
Loss in iteration 16 : 0.6501468255423436
Loss in iteration 17 : 0.6342644965260393
Loss in iteration 18 : 0.6212814701326221
Loss in iteration 19 : 0.6104908960780693
Loss in iteration 20 : 0.6014415121107424
Loss in iteration 21 : 0.5937751882150385
Loss in iteration 22 : 0.5873145600895149
Loss in iteration 23 : 0.5818473801445934
Loss in iteration 24 : 0.5771474704919444
Loss in iteration 25 : 0.5730935571757905
Loss in iteration 26 : 0.5695269958427143
Loss in iteration 27 : 0.5663692593494969
Loss in iteration 28 : 0.5635377705705323
Loss in iteration 29 : 0.561006212339127
Loss in iteration 30 : 0.5587202856113623
Loss in iteration 31 : 0.5566255056216275
Loss in iteration 32 : 0.5547235859810208
Loss in iteration 33 : 0.553009842220707
Loss in iteration 34 : 0.5514193201308142
Loss in iteration 35 : 0.5499422681957972
Loss in iteration 36 : 0.5485712696956528
Loss in iteration 37 : 0.5472942690309984
Loss in iteration 38 : 0.54610887098523
Loss in iteration 39 : 0.545019332018607
Loss in iteration 40 : 0.544008203522857
Loss in iteration 41 : 0.5430562215430397
Loss in iteration 42 : 0.5421565535738885
Loss in iteration 43 : 0.5413107553814781
Loss in iteration 44 : 0.5405157392305345
Loss in iteration 45 : 0.5397665327706136
Loss in iteration 46 : 0.539061051030358
Loss in iteration 47 : 0.538389999863531
Loss in iteration 48 : 0.5377537051643355
Loss in iteration 49 : 0.537151427774392
Loss in iteration 50 : 0.5365748690565008
Loss in iteration 51 : 0.5360192052112038
Loss in iteration 52 : 0.5354854711466632
Loss in iteration 53 : 0.5349714455658962
Loss in iteration 54 : 0.5344725315121808
Loss in iteration 55 : 0.533986276763025
Loss in iteration 56 : 0.5335174424498298
Loss in iteration 57 : 0.5330625451803376
Loss in iteration 58 : 0.5326169921765818
Loss in iteration 59 : 0.5321840087637563
Loss in iteration 60 : 0.5317650084019465
Loss in iteration 61 : 0.5313602734620665
Loss in iteration 62 : 0.5309671763957036
Loss in iteration 63 : 0.5305832940483128
Loss in iteration 64 : 0.5302084717915451
Loss in iteration 65 : 0.5298439909775341
Loss in iteration 66 : 0.5294872642020771
Loss in iteration 67 : 0.529134274841375
Loss in iteration 68 : 0.5287864062592532
Loss in iteration 69 : 0.5284450738720959
Loss in iteration 70 : 0.5281099490311532
Loss in iteration 71 : 0.5277827293908092
Loss in iteration 72 : 0.527463563802415
Loss in iteration 73 : 0.5271512166161201
Loss in iteration 74 : 0.5268460109184273
Loss in iteration 75 : 0.5265491721861895
Loss in iteration 76 : 0.5262591843417308
Loss in iteration 77 : 0.5259734007397756
Loss in iteration 78 : 0.5256943963622538
Loss in iteration 79 : 0.5254228976235611
Loss in iteration 80 : 0.5251575688427179
Loss in iteration 81 : 0.5248980549643968
Loss in iteration 82 : 0.5246456893995223
Loss in iteration 83 : 0.524399803714585
Loss in iteration 84 : 0.5241583376052972
Loss in iteration 85 : 0.5239218203930278
Loss in iteration 86 : 0.5236889456844749
Loss in iteration 87 : 0.5234620769882254
Loss in iteration 88 : 0.5232396310976435
Loss in iteration 89 : 0.5230228010197036
Loss in iteration 90 : 0.5228106418543544
Loss in iteration 91 : 0.5226016036966139
Loss in iteration 92 : 0.5223967237704156
Loss in iteration 93 : 0.522195212901244
Loss in iteration 94 : 0.5219976148347235
Loss in iteration 95 : 0.5218032429239363
Loss in iteration 96 : 0.5216112009026352
Loss in iteration 97 : 0.5214218888196409
Loss in iteration 98 : 0.5212359338397425
Loss in iteration 99 : 0.5210548502714604
Loss in iteration 100 : 0.520878216698281
Loss in iteration 101 : 0.520705098189503
Loss in iteration 102 : 0.5205348632117696
Loss in iteration 103 : 0.5203670961771076
Loss in iteration 104 : 0.5202020994162995
Loss in iteration 105 : 0.5200405432815736
Loss in iteration 106 : 0.5198840038664859
Loss in iteration 107 : 0.5197305508257117
Loss in iteration 108 : 0.5195797322084695
Loss in iteration 109 : 0.5194332669083803
Loss in iteration 110 : 0.5192895266469293
Loss in iteration 111 : 0.5191475447430717
Loss in iteration 112 : 0.5190078172350725
Loss in iteration 113 : 0.5188706346596208
Loss in iteration 114 : 0.5187363842331675
Loss in iteration 115 : 0.5186064978656243
Loss in iteration 116 : 0.5184786171415416
Loss in iteration 117 : 0.5183522995829294
Loss in iteration 118 : 0.5182268290839372
Loss in iteration 119 : 0.5181029938120266
Loss in iteration 120 : 0.5179814658398547
Loss in iteration 121 : 0.5178622659727237
Loss in iteration 122 : 0.5177458516957982
Loss in iteration 123 : 0.5176317608036777
Loss in iteration 124 : 0.5175202861924721
Loss in iteration 125 : 0.5174104089881614
Loss in iteration 126 : 0.517302147481892
Loss in iteration 127 : 0.5171956709284786
Loss in iteration 128 : 0.5170907960422289
Loss in iteration 129 : 0.5169888712160423
Loss in iteration 130 : 0.5168877090299141
Loss in iteration 131 : 0.5167874219280261
Loss in iteration 132 : 0.5166888015798069
Loss in iteration 133 : 0.5165914254644736
Loss in iteration 134 : 0.5164961540400821
Loss in iteration 135 : 0.5164017256032001
Loss in iteration 136 : 0.5163090314942999
Loss in iteration 137 : 0.5162172986234986
Loss in iteration 138 : 0.5161265349642472
Loss in iteration 139 : 0.5160374920459773
Loss in iteration 140 : 0.5159501952498788
Loss in iteration 141 : 0.5158639226486482
Loss in iteration 142 : 0.5157790433411048
Loss in iteration 143 : 0.5156955229865259
Loss in iteration 144 : 0.5156137080523207
Loss in iteration 145 : 0.5155331452951184
Loss in iteration 146 : 0.5154539713634806
Loss in iteration 147 : 0.5153761265755761
Loss in iteration 148 : 0.5152992743966283
Loss in iteration 149 : 0.5152232373061317
Loss in iteration 150 : 0.5151482031714443
Loss in iteration 151 : 0.5150740290000024
Loss in iteration 152 : 0.5150009306009309
Loss in iteration 153 : 0.5149286992505276
Loss in iteration 154 : 0.5148576729707746
Loss in iteration 155 : 0.5147877120894994
Loss in iteration 156 : 0.5147189686448577
Loss in iteration 157 : 0.5146516785639137
Loss in iteration 158 : 0.5145853787981568
Loss in iteration 159 : 0.5145195606745817
Loss in iteration 160 : 0.5144543056239672
Loss in iteration 161 : 0.5143903501121498
Loss in iteration 162 : 0.5143268020115422
Loss in iteration 163 : 0.5142643065158771
Loss in iteration 164 : 0.5142028433342835
Loss in iteration 165 : 0.5141421904160483
Loss in iteration 166 : 0.514082277513125
Loss in iteration 167 : 0.514022868017044
Loss in iteration 168 : 0.5139638610727079
Loss in iteration 169 : 0.51390592932155
Loss in iteration 170 : 0.513848480266086
Loss in iteration 171 : 0.5137919907914807
Loss in iteration 172 : 0.5137362412495521
Loss in iteration 173 : 0.5136810249174599
Loss in iteration 174 : 0.5136264117217202
Loss in iteration 175 : 0.5135722843510074
Loss in iteration 176 : 0.5135191259742291
Loss in iteration 177 : 0.5134673777457373
Loss in iteration 178 : 0.5134160473294009
Loss in iteration 179 : 0.5133656438401127
Loss in iteration 180 : 0.5133161681736876
Loss in iteration 181 : 0.5132677737617134
Loss in iteration 182 : 0.5132203614342759
Loss in iteration 183 : 0.5131732040296599
Loss in iteration 184 : 0.5131264496576837
Loss in iteration 185 : 0.5130802334880619
Loss in iteration 186 : 0.5130343286657321
Loss in iteration 187 : 0.5129888455137513
Loss in iteration 188 : 0.5129437420556949
Loss in iteration 189 : 0.5128991839731991
Loss in iteration 190 : 0.5128549245949833
Loss in iteration 191 : 0.5128110429322942
Loss in iteration 192 : 0.5127675982605799
Loss in iteration 193 : 0.5127244433860428
Loss in iteration 194 : 0.512681654968899
Loss in iteration 195 : 0.5126391370046506
Loss in iteration 196 : 0.5125972169778313
Loss in iteration 197 : 0.5125555048159638
Loss in iteration 198 : 0.5125141507071982
Loss in iteration 199 : 0.5124731546065268
Loss in iteration 200 : 0.5124330553622212
Testing accuracy  of updater 2 on alg 1 with rate 0.07 = 0.783, training accuracy 0.787, time elapsed: 3135 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9824301803142858
Loss in iteration 3 : 0.9575483218733266
Loss in iteration 4 : 0.9304323366122109
Loss in iteration 5 : 0.9138608797782899
Loss in iteration 6 : 0.9065302608015732
Loss in iteration 7 : 0.8980880122014375
Loss in iteration 8 : 0.8843946305950792
Loss in iteration 9 : 0.867950529681738
Loss in iteration 10 : 0.8512679333191295
Loss in iteration 11 : 0.8348764556131753
Loss in iteration 12 : 0.8187799090053334
Loss in iteration 13 : 0.8028263681512838
Loss in iteration 14 : 0.7867457588428577
Loss in iteration 15 : 0.7702692399053682
Loss in iteration 16 : 0.7533719170356508
Loss in iteration 17 : 0.736385054271708
Loss in iteration 18 : 0.7202305356464928
Loss in iteration 19 : 0.7053702208358389
Loss in iteration 20 : 0.6913332729093077
Loss in iteration 21 : 0.6779353411599188
Loss in iteration 22 : 0.6653214148046195
Loss in iteration 23 : 0.6537176518141378
Loss in iteration 24 : 0.6433380954457725
Loss in iteration 25 : 0.6340910648421323
Loss in iteration 26 : 0.626000067115567
Loss in iteration 27 : 0.6188460752634644
Loss in iteration 28 : 0.6124392657831278
Loss in iteration 29 : 0.6067182138952646
Loss in iteration 30 : 0.6016204248521715
Loss in iteration 31 : 0.597119802597525
Loss in iteration 32 : 0.5931338578285054
Loss in iteration 33 : 0.5895811985093697
Loss in iteration 34 : 0.5863710104314163
Loss in iteration 35 : 0.5834033970078103
Loss in iteration 36 : 0.5806633072969021
Loss in iteration 37 : 0.5781374593705468
Loss in iteration 38 : 0.57578547201753
Loss in iteration 39 : 0.5735886063332862
Loss in iteration 40 : 0.5715599948464073
Loss in iteration 41 : 0.5696708118263893
Loss in iteration 42 : 0.5679110221785179
Loss in iteration 43 : 0.5662620925743458
Loss in iteration 44 : 0.5647024004906218
Loss in iteration 45 : 0.5632315585501866
Loss in iteration 46 : 0.5618401299988353
Loss in iteration 47 : 0.5605309232976754
Loss in iteration 48 : 0.5592883453429013
Loss in iteration 49 : 0.5581165483017138
Loss in iteration 50 : 0.5570091205876325
Loss in iteration 51 : 0.5559673798198919
Loss in iteration 52 : 0.554994673810071
Loss in iteration 53 : 0.5540697942549271
Loss in iteration 54 : 0.5531893555793759
Loss in iteration 55 : 0.5523445613667224
Loss in iteration 56 : 0.551532483237977
Loss in iteration 57 : 0.5507571199435428
Loss in iteration 58 : 0.5500234448161261
Loss in iteration 59 : 0.5493219370168022
Loss in iteration 60 : 0.5486513392900074
Loss in iteration 61 : 0.5480050777357509
Loss in iteration 62 : 0.5473781558335702
Loss in iteration 63 : 0.5467730463356739
Loss in iteration 64 : 0.5461896449755856
Loss in iteration 65 : 0.5456328167667934
Loss in iteration 66 : 0.5450955400401835
Loss in iteration 67 : 0.5445756317631666
Loss in iteration 68 : 0.5440728058265784
Loss in iteration 69 : 0.5435854400998389
Loss in iteration 70 : 0.5431154288751736
Loss in iteration 71 : 0.5426599788826147
Loss in iteration 72 : 0.542217205101473
Loss in iteration 73 : 0.5417858621017566
Loss in iteration 74 : 0.5413673191191745
Loss in iteration 75 : 0.5409590297654172
Loss in iteration 76 : 0.5405584536442846
Loss in iteration 77 : 0.5401678375276194
Loss in iteration 78 : 0.5397874311380666
Loss in iteration 79 : 0.5394169531861547
Loss in iteration 80 : 0.5390538965807
Loss in iteration 81 : 0.5386981183137995
Loss in iteration 82 : 0.5383493231601
Loss in iteration 83 : 0.5380065295515835
Loss in iteration 84 : 0.5376710284498643
Loss in iteration 85 : 0.5373425244292463
Loss in iteration 86 : 0.5370216855060901
Loss in iteration 87 : 0.5367082762599751
Loss in iteration 88 : 0.5364027243256505
Loss in iteration 89 : 0.5361012639851078
Loss in iteration 90 : 0.5358035616652926
Loss in iteration 91 : 0.5355107361509414
Loss in iteration 92 : 0.53522317830593
Loss in iteration 93 : 0.5349401660694061
Loss in iteration 94 : 0.5346606985621079
Loss in iteration 95 : 0.5343850061156548
Loss in iteration 96 : 0.534115319576234
Loss in iteration 97 : 0.5338503978871182
Loss in iteration 98 : 0.5335910661668571
Loss in iteration 99 : 0.5333372239633253
Loss in iteration 100 : 0.5330882128133546
Loss in iteration 101 : 0.5328437527821374
Loss in iteration 102 : 0.5326029084212511
Loss in iteration 103 : 0.5323661685213195
Loss in iteration 104 : 0.5321349519066007
Loss in iteration 105 : 0.5319083551168751
Loss in iteration 106 : 0.5316852282392435
Loss in iteration 107 : 0.5314661202213786
Loss in iteration 108 : 0.5312509119447486
Loss in iteration 109 : 0.5310377101671937
Loss in iteration 110 : 0.530827729635214
Loss in iteration 111 : 0.5306209797870842
Loss in iteration 112 : 0.5304162421785183
Loss in iteration 113 : 0.5302138284009534
Loss in iteration 114 : 0.5300136558933825
Loss in iteration 115 : 0.5298171235787357
Loss in iteration 116 : 0.5296233837612616
Loss in iteration 117 : 0.5294329736661599
Loss in iteration 118 : 0.52924397549034
Loss in iteration 119 : 0.5290562610171241
Loss in iteration 120 : 0.5288697498187284
Loss in iteration 121 : 0.5286841920533241
Loss in iteration 122 : 0.5284998332826351
Loss in iteration 123 : 0.5283166538264215
Loss in iteration 124 : 0.5281366555967157
Loss in iteration 125 : 0.5279588935397874
Loss in iteration 126 : 0.5277826868470175
Loss in iteration 127 : 0.527609026245619
Loss in iteration 128 : 0.5274374715678124
Loss in iteration 129 : 0.5272674306356702
Loss in iteration 130 : 0.5270990145673926
Loss in iteration 131 : 0.5269319912342201
Loss in iteration 132 : 0.5267679410289864
Loss in iteration 133 : 0.5266063194487699
Loss in iteration 134 : 0.5264455640390848
Loss in iteration 135 : 0.5262857753909957
Loss in iteration 136 : 0.5261274612344061
Loss in iteration 137 : 0.5259714974849853
Loss in iteration 138 : 0.5258185392889054
Loss in iteration 139 : 0.5256684681331124
Loss in iteration 140 : 0.5255203769624498
Loss in iteration 141 : 0.5253739266288329
Loss in iteration 142 : 0.5252287852225941
Loss in iteration 143 : 0.525085338628737
Loss in iteration 144 : 0.524943000907899
Loss in iteration 145 : 0.524802240451346
Loss in iteration 146 : 0.5246640058503951
Loss in iteration 147 : 0.5245269220678836
Loss in iteration 148 : 0.5243920464269989
Loss in iteration 149 : 0.5242584143848732
Loss in iteration 150 : 0.5241260130154964
Loss in iteration 151 : 0.5239954610743702
Loss in iteration 152 : 0.5238670007009832
Loss in iteration 153 : 0.5237398279035376
Loss in iteration 154 : 0.5236137785585966
Loss in iteration 155 : 0.5234885703038886
Loss in iteration 156 : 0.5233648629261846
Loss in iteration 157 : 0.5232432773077484
Loss in iteration 158 : 0.5231237866274264
Loss in iteration 159 : 0.5230053099545731
Loss in iteration 160 : 0.5228876152160035
Loss in iteration 161 : 0.5227713147870202
Loss in iteration 162 : 0.522656056387755
Loss in iteration 163 : 0.5225423308862784
Loss in iteration 164 : 0.5224299852889824
Loss in iteration 165 : 0.5223189002708898
Loss in iteration 166 : 0.5222087967688227
Loss in iteration 167 : 0.5220991472248825
Loss in iteration 168 : 0.5219900637358289
Loss in iteration 169 : 0.5218824083198651
Loss in iteration 170 : 0.5217761584499324
Loss in iteration 171 : 0.521671343372735
Loss in iteration 172 : 0.5215672008304119
Loss in iteration 173 : 0.5214644698678175
Loss in iteration 174 : 0.5213630286981958
Loss in iteration 175 : 0.5212626862530182
Loss in iteration 176 : 0.5211630901690587
Loss in iteration 177 : 0.5210644633984177
Loss in iteration 178 : 0.5209667022801627
Loss in iteration 179 : 0.5208698328952317
Loss in iteration 180 : 0.5207735392502832
Loss in iteration 181 : 0.5206778410454211
Loss in iteration 182 : 0.520582676737497
Loss in iteration 183 : 0.520488178112712
Loss in iteration 184 : 0.5203947593425469
Loss in iteration 185 : 0.5203021957324317
Loss in iteration 186 : 0.5202106327016729
Loss in iteration 187 : 0.5201207782790153
Loss in iteration 188 : 0.5200322104285002
Loss in iteration 189 : 0.5199452062626265
Loss in iteration 190 : 0.519858838500529
Loss in iteration 191 : 0.519773213067673
Loss in iteration 192 : 0.519688338035991
Loss in iteration 193 : 0.5196038448263341
Loss in iteration 194 : 0.5195199058995345
Loss in iteration 195 : 0.5194370743481546
Loss in iteration 196 : 0.5193552573506374
Loss in iteration 197 : 0.5192740975335497
Loss in iteration 198 : 0.5191934576454056
Loss in iteration 199 : 0.51911366407049
Loss in iteration 200 : 0.5190351705154859
Testing accuracy  of updater 2 on alg 1 with rate 0.04000000000000001 = 0.7775, training accuracy 0.7845, time elapsed: 2531 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9956075450785709
Loss in iteration 3 : 0.9893425172695873
Loss in iteration 4 : 0.9813921738618009
Loss in iteration 5 : 0.9719303725592876
Loss in iteration 6 : 0.9612211601170195
Loss in iteration 7 : 0.94964798285596
Loss in iteration 8 : 0.9383571298657392
Loss in iteration 9 : 0.9284349402451295
Loss in iteration 10 : 0.9205656807440232
Loss in iteration 11 : 0.9148128267489485
Loss in iteration 12 : 0.910870283434604
Loss in iteration 13 : 0.9076971534592891
Loss in iteration 14 : 0.9046944508343955
Loss in iteration 15 : 0.901320584548271
Loss in iteration 16 : 0.897243679654323
Loss in iteration 17 : 0.892426028057897
Loss in iteration 18 : 0.8870531538538203
Loss in iteration 19 : 0.88136190638791
Loss in iteration 20 : 0.8755833927168353
Loss in iteration 21 : 0.8698502862429066
Loss in iteration 22 : 0.8642574696603773
Loss in iteration 23 : 0.8588069952733394
Loss in iteration 24 : 0.8534885106011141
Loss in iteration 25 : 0.848287243327165
Loss in iteration 26 : 0.8432000247342387
Loss in iteration 27 : 0.8381914728792108
Loss in iteration 28 : 0.83323227484796
Loss in iteration 29 : 0.8282914034464331
Loss in iteration 30 : 0.8233528216129364
Loss in iteration 31 : 0.8183923850895303
Loss in iteration 32 : 0.8134020981236023
Loss in iteration 33 : 0.8083826558993191
Loss in iteration 34 : 0.8033363400487951
Loss in iteration 35 : 0.7982663855756396
Loss in iteration 36 : 0.7931797046233671
Loss in iteration 37 : 0.7880913265898112
Loss in iteration 38 : 0.7830072143197251
Loss in iteration 39 : 0.7779265498810548
Loss in iteration 40 : 0.7728467491740303
Loss in iteration 41 : 0.7677741822163
Loss in iteration 42 : 0.7627124050276511
Loss in iteration 43 : 0.7576598903635827
Loss in iteration 44 : 0.7526172862902343
Loss in iteration 45 : 0.7475853440971757
Loss in iteration 46 : 0.7425797091610519
Loss in iteration 47 : 0.737601140992137
Loss in iteration 48 : 0.7326673432086583
Loss in iteration 49 : 0.7277779248174235
Loss in iteration 50 : 0.7229549901303695
Loss in iteration 51 : 0.7182649962765857
Loss in iteration 52 : 0.7137114173621515
Loss in iteration 53 : 0.7092985621667447
Loss in iteration 54 : 0.7049988961739739
Loss in iteration 55 : 0.7008535882745734
Loss in iteration 56 : 0.6969023609765134
Loss in iteration 57 : 0.693060673298914
Loss in iteration 58 : 0.6893185069028079
Loss in iteration 59 : 0.68567907180161
Loss in iteration 60 : 0.6821559043409315
Loss in iteration 61 : 0.678713319724335
Loss in iteration 62 : 0.6753486199289294
Loss in iteration 63 : 0.6720774711073284
Loss in iteration 64 : 0.6688866474341594
Loss in iteration 65 : 0.6657944209184222
Loss in iteration 66 : 0.6628090667448361
Loss in iteration 67 : 0.659930504326845
Loss in iteration 68 : 0.6571449723048566
Loss in iteration 69 : 0.6544602352221379
Loss in iteration 70 : 0.6518584087595715
Loss in iteration 71 : 0.6493460922093168
Loss in iteration 72 : 0.6469183698703784
Loss in iteration 73 : 0.64457189639693
Loss in iteration 74 : 0.6423042963670641
Loss in iteration 75 : 0.6401198508330176
Loss in iteration 76 : 0.6380182151413513
Loss in iteration 77 : 0.6360066095465524
Loss in iteration 78 : 0.6340751323088889
Loss in iteration 79 : 0.63222073315267
Loss in iteration 80 : 0.6304263189851197
Loss in iteration 81 : 0.6286883977609516
Loss in iteration 82 : 0.6270140568880794
Loss in iteration 83 : 0.6253947539451056
Loss in iteration 84 : 0.6238227488269943
Loss in iteration 85 : 0.6223028019120171
Loss in iteration 86 : 0.620843276915811
Loss in iteration 87 : 0.619429394393147
Loss in iteration 88 : 0.6180507386211498
Loss in iteration 89 : 0.6167072417880873
Loss in iteration 90 : 0.6154049930059114
Loss in iteration 91 : 0.6141432887191768
Loss in iteration 92 : 0.6129282950670404
Loss in iteration 93 : 0.6117463873451362
Loss in iteration 94 : 0.6105954490368497
Loss in iteration 95 : 0.609477854837547
Loss in iteration 96 : 0.6083887711795135
Loss in iteration 97 : 0.6073300656659417
Loss in iteration 98 : 0.6062987022467784
Loss in iteration 99 : 0.6052918467982973
Loss in iteration 100 : 0.6043113112010037
Loss in iteration 101 : 0.6033537743067707
Loss in iteration 102 : 0.6024184610381356
Loss in iteration 103 : 0.6015051673809629
Loss in iteration 104 : 0.6006175500022433
Loss in iteration 105 : 0.5997495511233571
Loss in iteration 106 : 0.5989038706226766
Loss in iteration 107 : 0.5980782732595831
Loss in iteration 108 : 0.5972728708490137
Loss in iteration 109 : 0.5964871752871915
Loss in iteration 110 : 0.5957174757441781
Loss in iteration 111 : 0.5949634236305503
Loss in iteration 112 : 0.5942307381357659
Loss in iteration 113 : 0.5935159453205494
Loss in iteration 114 : 0.5928158138335825
Loss in iteration 115 : 0.5921266453654443
Loss in iteration 116 : 0.5914480601647829
Loss in iteration 117 : 0.5907820210124244
Loss in iteration 118 : 0.5901273271606865
Loss in iteration 119 : 0.5894818455644525
Loss in iteration 120 : 0.5888451857829354
Loss in iteration 121 : 0.5882180100341605
Loss in iteration 122 : 0.587600464420243
Loss in iteration 123 : 0.5869930235916339
Loss in iteration 124 : 0.5863949290387132
Loss in iteration 125 : 0.5858073387744631
Loss in iteration 126 : 0.5852293080502113
Loss in iteration 127 : 0.5846583495541404
Loss in iteration 128 : 0.5840951262428241
Loss in iteration 129 : 0.5835389710628024
Loss in iteration 130 : 0.5829907944338643
Loss in iteration 131 : 0.582452913943192
Loss in iteration 132 : 0.5819226619184842
Loss in iteration 133 : 0.5814003641085949
Loss in iteration 134 : 0.580887236892338
Loss in iteration 135 : 0.5803818611105825
Loss in iteration 136 : 0.5798835028997916
Loss in iteration 137 : 0.5793927002705238
Loss in iteration 138 : 0.5789102132141123
Loss in iteration 139 : 0.5784345341979659
Loss in iteration 140 : 0.5779637896513011
Loss in iteration 141 : 0.577498001143438
Loss in iteration 142 : 0.5770391778931221
Loss in iteration 143 : 0.576586310117229
Loss in iteration 144 : 0.5761402013609368
Loss in iteration 145 : 0.5756995762897437
Loss in iteration 146 : 0.5752634658942127
Loss in iteration 147 : 0.5748309043941872
Loss in iteration 148 : 0.5744040567947399
Loss in iteration 149 : 0.5739837893782588
Loss in iteration 150 : 0.5735696912801079
Loss in iteration 151 : 0.5731602415889129
Loss in iteration 152 : 0.5727539935599475
Loss in iteration 153 : 0.5723519518137806
Loss in iteration 154 : 0.5719547984953288
Loss in iteration 155 : 0.5715615613660268
Loss in iteration 156 : 0.571172673948625
Loss in iteration 157 : 0.5707883051099027
Loss in iteration 158 : 0.5704090132766034
Loss in iteration 159 : 0.5700328501679884
Loss in iteration 160 : 0.5696608441212339
Loss in iteration 161 : 0.5692948098092125
Loss in iteration 162 : 0.5689335972715819
Loss in iteration 163 : 0.5685781220703736
Loss in iteration 164 : 0.5682273150995734
Loss in iteration 165 : 0.5678797274555684
Loss in iteration 166 : 0.5675352821567531
Loss in iteration 167 : 0.5671945427330802
Loss in iteration 168 : 0.5668596687962291
Loss in iteration 169 : 0.5665295485945578
Loss in iteration 170 : 0.5662028024272913
Loss in iteration 171 : 0.5658800395556324
Loss in iteration 172 : 0.5655621150261236
Loss in iteration 173 : 0.5652480925612783
Loss in iteration 174 : 0.564937851173197
Loss in iteration 175 : 0.5646307812929936
Loss in iteration 176 : 0.564326791825811
Loss in iteration 177 : 0.5640266258715237
Loss in iteration 178 : 0.563729440664556
Loss in iteration 179 : 0.5634355660277572
Loss in iteration 180 : 0.5631463737403061
Loss in iteration 181 : 0.5628598707910386
Loss in iteration 182 : 0.5625762284440787
Loss in iteration 183 : 0.562295148612782
Loss in iteration 184 : 0.5620172907835888
Loss in iteration 185 : 0.5617436403957956
Loss in iteration 186 : 0.5614727138441576
Loss in iteration 187 : 0.561204501729715
Loss in iteration 188 : 0.5609386847663875
Loss in iteration 189 : 0.5606752310716691
Loss in iteration 190 : 0.560414826177263
Loss in iteration 191 : 0.5601570650224024
Loss in iteration 192 : 0.5599018348833382
Loss in iteration 193 : 0.5596495550132715
Loss in iteration 194 : 0.5593998180020824
Loss in iteration 195 : 0.5591526120446316
Loss in iteration 196 : 0.558908809366994
Loss in iteration 197 : 0.558667706555844
Loss in iteration 198 : 0.5584282229464151
Loss in iteration 199 : 0.5581901328918677
Loss in iteration 200 : 0.5579534174725085
Testing accuracy  of updater 2 on alg 1 with rate 0.009999999999999995 = 0.7695, training accuracy 0.77075, time elapsed: 2487 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 20.37732908901907
Loss in iteration 3 : 25.287366070202793
Loss in iteration 4 : 11.168884937204485
Loss in iteration 5 : 11.6784906900165
Loss in iteration 6 : 7.350552661719799
Loss in iteration 7 : 7.762225700903877
Loss in iteration 8 : 6.32013226186656
Loss in iteration 9 : 6.340921394894471
Loss in iteration 10 : 5.4520296409480595
Loss in iteration 11 : 5.33377793124557
Loss in iteration 12 : 4.767078403366352
Loss in iteration 13 : 4.647737190252947
Loss in iteration 14 : 4.271200710520914
Loss in iteration 15 : 4.189880425630291
Loss in iteration 16 : 3.777859235508905
Loss in iteration 17 : 3.817027300148857
Loss in iteration 18 : 3.3929439800858185
Loss in iteration 19 : 3.4839866173252148
Loss in iteration 20 : 3.1075214095075236
Loss in iteration 21 : 3.278737428036524
Loss in iteration 22 : 2.8955764294197324
Loss in iteration 23 : 3.11895623164707
Loss in iteration 24 : 2.796890261406155
Loss in iteration 25 : 3.078568865605482
Loss in iteration 26 : 2.6691947509800924
Loss in iteration 27 : 3.0350723374253175
Loss in iteration 28 : 2.488808211776199
Loss in iteration 29 : 2.811912752103233
Loss in iteration 30 : 2.290016793936932
Loss in iteration 31 : 2.540316316626215
Loss in iteration 32 : 2.222048607136492
Loss in iteration 33 : 2.4972018230670634
Loss in iteration 34 : 2.269598651278266
Loss in iteration 35 : 2.553689105753858
Loss in iteration 36 : 2.351304993973123
Loss in iteration 37 : 2.6405132400325173
Loss in iteration 38 : 2.4774894825254727
Loss in iteration 39 : 2.796378462882507
Loss in iteration 40 : 2.4950328096823524
Loss in iteration 41 : 2.7472021765997496
Loss in iteration 42 : 2.4373392677924235
Loss in iteration 43 : 2.6396004740931485
Loss in iteration 44 : 2.342377434466678
Loss in iteration 45 : 2.4921293254847465
Loss in iteration 46 : 2.2988065104151625
Loss in iteration 47 : 2.4287923681263837
Loss in iteration 48 : 2.2310171139916575
Loss in iteration 49 : 2.344514610565474
Loss in iteration 50 : 2.16578533885449
Loss in iteration 51 : 2.272322763005821
Loss in iteration 52 : 2.1320045079311756
Loss in iteration 53 : 2.248051530445119
Loss in iteration 54 : 2.105302061487764
Loss in iteration 55 : 2.2095529662394147
Loss in iteration 56 : 2.0775645976600576
Loss in iteration 57 : 2.187791519455366
Loss in iteration 58 : 2.0451971519311463
Loss in iteration 59 : 2.152560100152242
Loss in iteration 60 : 2.015986034121985
Loss in iteration 61 : 2.1294860395648567
Loss in iteration 62 : 2.0012179889014963
Loss in iteration 63 : 2.122708045273517
Loss in iteration 64 : 1.9805129497851182
Loss in iteration 65 : 2.101263472928125
Loss in iteration 66 : 1.959394448210694
Loss in iteration 67 : 2.076501749893059
Loss in iteration 68 : 1.9514101830440367
Loss in iteration 69 : 2.084914708842053
Loss in iteration 70 : 1.928894735027179
Loss in iteration 71 : 2.0475498850841856
Loss in iteration 72 : 1.9205984870163566
Loss in iteration 73 : 2.0595547646113235
Loss in iteration 74 : 1.8958175307185807
Loss in iteration 75 : 2.0262887028798096
Loss in iteration 76 : 1.887623874793267
Loss in iteration 77 : 2.0143611110725947
Loss in iteration 78 : 1.8785311613782192
Loss in iteration 79 : 1.9998813615697228
Loss in iteration 80 : 1.8684294088536149
Loss in iteration 81 : 1.9847595934732962
Loss in iteration 82 : 1.850735254709397
Loss in iteration 83 : 1.9721781099377471
Loss in iteration 84 : 1.8342405315602985
Loss in iteration 85 : 1.9584170635280889
Loss in iteration 86 : 1.8207103826181037
Loss in iteration 87 : 1.9394424025852186
Loss in iteration 88 : 1.8151300906435608
Loss in iteration 89 : 1.930336990418779
Loss in iteration 90 : 1.78372258003892
Loss in iteration 91 : 1.896043090206903
Loss in iteration 92 : 1.7845123018770488
Loss in iteration 93 : 1.896473088109551
Loss in iteration 94 : 1.766639433714873
Loss in iteration 95 : 1.869718067870824
Loss in iteration 96 : 1.766503008617947
Loss in iteration 97 : 1.8752982959795628
Loss in iteration 98 : 1.753121340339268
Loss in iteration 99 : 1.8537668509831842
Loss in iteration 100 : 1.7369839289586653
Loss in iteration 101 : 1.8342307438431924
Loss in iteration 102 : 1.7257022652771439
Loss in iteration 103 : 1.8228782808871788
Loss in iteration 104 : 1.7105798189422243
Loss in iteration 105 : 1.8064713118694216
Loss in iteration 106 : 1.6992908988317295
Loss in iteration 107 : 1.7953393631475973
Loss in iteration 108 : 1.6854915924568556
Loss in iteration 109 : 1.7807423586972921
Loss in iteration 110 : 1.6815108551883569
Loss in iteration 111 : 1.7796695363713253
Loss in iteration 112 : 1.670589385902886
Loss in iteration 113 : 1.7690288944149715
Loss in iteration 114 : 1.6577188524512518
Loss in iteration 115 : 1.7539862678691365
Loss in iteration 116 : 1.6485113698021305
Loss in iteration 117 : 1.7463895711740935
Loss in iteration 118 : 1.6396747083228762
Loss in iteration 119 : 1.7366525753314446
Loss in iteration 120 : 1.6275367817959583
Loss in iteration 121 : 1.7346866141451553
Loss in iteration 122 : 1.61975581278652
Loss in iteration 123 : 1.718179071899761
Loss in iteration 124 : 1.6138485498604613
Loss in iteration 125 : 1.7148470642522107
Loss in iteration 126 : 1.601545359353319
Loss in iteration 127 : 1.7079213280545193
Loss in iteration 128 : 1.5913303173583022
Loss in iteration 129 : 1.7046142293373883
Loss in iteration 130 : 1.5784724934952292
Loss in iteration 131 : 1.6841281946860935
Loss in iteration 132 : 1.576730761221078
Loss in iteration 133 : 1.680174281193655
Loss in iteration 134 : 1.5652652738521637
Loss in iteration 135 : 1.6667885464333787
Loss in iteration 136 : 1.5561943554028799
Loss in iteration 137 : 1.6648555994957754
Loss in iteration 138 : 1.5475422643163534
Loss in iteration 139 : 1.6572296395303252
Loss in iteration 140 : 1.5373487304668398
Loss in iteration 141 : 1.6467372699682843
Loss in iteration 142 : 1.529735129890627
Loss in iteration 143 : 1.636027962782095
Loss in iteration 144 : 1.522113858183323
Loss in iteration 145 : 1.6270071868847036
Loss in iteration 146 : 1.5127781949071253
Loss in iteration 147 : 1.6165133830899572
Loss in iteration 148 : 1.5033634360201558
Loss in iteration 149 : 1.6062087817646649
Loss in iteration 150 : 1.4932748020769824
Loss in iteration 151 : 1.5973769470806747
Loss in iteration 152 : 1.4860839959334593
Loss in iteration 153 : 1.5908183318564293
Loss in iteration 154 : 1.4725471509133319
Loss in iteration 155 : 1.581552631618283
Loss in iteration 156 : 1.4672470418529369
Loss in iteration 157 : 1.574727424793487
Loss in iteration 158 : 1.4605706886015868
Loss in iteration 159 : 1.5626740118799503
Loss in iteration 160 : 1.457366699242572
Loss in iteration 161 : 1.562472758191955
Loss in iteration 162 : 1.4490247281747841
Loss in iteration 163 : 1.5524718799275647
Loss in iteration 164 : 1.454251308787513
Loss in iteration 165 : 1.5613282792933492
Loss in iteration 166 : 1.4451792156714516
Loss in iteration 167 : 1.5438504168739073
Loss in iteration 168 : 1.4464940555270946
Loss in iteration 169 : 1.5493754151944286
Loss in iteration 170 : 1.4383181953977022
Loss in iteration 171 : 1.5310353221870818
Loss in iteration 172 : 1.4329806090597867
Loss in iteration 173 : 1.526241352072294
Loss in iteration 174 : 1.4257532447545413
Loss in iteration 175 : 1.5218535959242558
Loss in iteration 176 : 1.4323957647438967
Loss in iteration 177 : 1.5256940181178962
Loss in iteration 178 : 1.4346679859415221
Loss in iteration 179 : 1.5283548502935855
Loss in iteration 180 : 1.426735163331838
Loss in iteration 181 : 1.518156297836681
Loss in iteration 182 : 1.4237953887765682
Loss in iteration 183 : 1.5138552878629539
Loss in iteration 184 : 1.41606364545525
Loss in iteration 185 : 1.5069336406231841
Loss in iteration 186 : 1.3797982175358543
Loss in iteration 187 : 1.4819210306195405
Loss in iteration 188 : 1.363644632041734
Loss in iteration 189 : 1.5572141786007256
Loss in iteration 190 : 1.3886981555989357
Loss in iteration 191 : 1.7100707819971603
Loss in iteration 192 : 1.3923167082414616
Loss in iteration 193 : 1.4790467306071942
Loss in iteration 194 : 1.2565894607126407
Loss in iteration 195 : 1.2351014007354124
Loss in iteration 196 : 1.116821687189522
Loss in iteration 197 : 1.1106368637260302
Loss in iteration 198 : 1.0382780064651882
Loss in iteration 199 : 1.0668448454071267
Loss in iteration 200 : 1.0424252678853534
Testing accuracy  of updater 3 on alg 1 with rate 10.0 = 0.753, training accuracy 0.7615, time elapsed: 2439 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.5538424106485653
Loss in iteration 3 : 4.960685113392788
Loss in iteration 4 : 2.540236868646691
Loss in iteration 5 : 3.3611948261102342
Loss in iteration 6 : 3.1317002257540074
Loss in iteration 7 : 2.3585531892889686
Loss in iteration 8 : 3.2513651038245697
Loss in iteration 9 : 1.8895266661952415
Loss in iteration 10 : 3.4664352515466357
Loss in iteration 11 : 1.393006760828509
Loss in iteration 12 : 3.1463704131680883
Loss in iteration 13 : 1.511856344476197
Loss in iteration 14 : 2.9503830979123404
Loss in iteration 15 : 1.5121864726160843
Loss in iteration 16 : 2.6560031416485526
Loss in iteration 17 : 1.610779385704912
Loss in iteration 18 : 2.3949374463959265
Loss in iteration 19 : 1.6591040546988058
Loss in iteration 20 : 2.163894006555005
Loss in iteration 21 : 1.6844296041251114
Loss in iteration 22 : 1.9743544036096528
Loss in iteration 23 : 1.6744893765141606
Loss in iteration 24 : 1.8009349555107408
Loss in iteration 25 : 1.6518213115639917
Loss in iteration 26 : 1.6485118160232766
Loss in iteration 27 : 1.6024608511247806
Loss in iteration 28 : 1.5445608171523795
Loss in iteration 29 : 1.5526274697314661
Loss in iteration 30 : 1.4485086715255486
Loss in iteration 31 : 1.5108218673377396
Loss in iteration 32 : 1.3636469978109995
Loss in iteration 33 : 1.4560619541997226
Loss in iteration 34 : 1.3057997543302653
Loss in iteration 35 : 1.4149927298388087
Loss in iteration 36 : 1.2483991905685254
Loss in iteration 37 : 1.3740180478439146
Loss in iteration 38 : 1.2133634135607145
Loss in iteration 39 : 1.3356106133407195
Loss in iteration 40 : 1.1885928644645358
Loss in iteration 41 : 1.296722693017617
Loss in iteration 42 : 1.1530362375720746
Loss in iteration 43 : 1.2669795327510098
Loss in iteration 44 : 1.120480986273779
Loss in iteration 45 : 1.2418413356226423
Loss in iteration 46 : 1.0869095748405782
Loss in iteration 47 : 1.208847245825796
Loss in iteration 48 : 1.0660769826479755
Loss in iteration 49 : 1.1965024670641438
Loss in iteration 50 : 1.0520142006695241
Loss in iteration 51 : 1.187087640727713
Loss in iteration 52 : 1.0346954527281031
Loss in iteration 53 : 1.1710299604220271
Loss in iteration 54 : 1.0188752755655035
Loss in iteration 55 : 1.1573255562751619
Loss in iteration 56 : 1.0027427804984295
Loss in iteration 57 : 1.1426253991209046
Loss in iteration 58 : 0.9905478146819449
Loss in iteration 59 : 1.1275165760288224
Loss in iteration 60 : 0.9819719463009572
Loss in iteration 61 : 1.1212025023172616
Loss in iteration 62 : 0.9723516581183619
Loss in iteration 63 : 1.1089750271838543
Loss in iteration 64 : 0.9622798040190592
Loss in iteration 65 : 1.094959168492519
Loss in iteration 66 : 0.941108850664175
Loss in iteration 67 : 1.0842714431754137
Loss in iteration 68 : 0.916931925657479
Loss in iteration 69 : 1.0831274407625906
Loss in iteration 70 : 0.9041687317020599
Loss in iteration 71 : 1.0803833126041809
Loss in iteration 72 : 0.8935915892168841
Loss in iteration 73 : 0.9748871985158473
Loss in iteration 74 : 0.8572222416870428
Loss in iteration 75 : 0.8774574600709887
Loss in iteration 76 : 0.8167519509592558
Loss in iteration 77 : 0.8429026160524247
Loss in iteration 78 : 0.8233891148787684
Loss in iteration 79 : 0.8724481947814704
Loss in iteration 80 : 0.862553796180043
Loss in iteration 81 : 0.9265942000854611
Loss in iteration 82 : 0.8945749988715365
Loss in iteration 83 : 0.8924940505044169
Loss in iteration 84 : 0.8615357759599476
Loss in iteration 85 : 0.8217098324335828
Loss in iteration 86 : 0.8007071009458716
Loss in iteration 87 : 0.7686528778742947
Loss in iteration 88 : 0.7555074461225572
Loss in iteration 89 : 0.730342982660548
Loss in iteration 90 : 0.728484196456913
Loss in iteration 91 : 0.7077212820096197
Loss in iteration 92 : 0.7294907430274653
Loss in iteration 93 : 0.7024268825935073
Loss in iteration 94 : 0.7423808327794661
Loss in iteration 95 : 0.7074082865809098
Loss in iteration 96 : 0.7718909902492853
Loss in iteration 97 : 0.7362911275075048
Loss in iteration 98 : 0.8642448009882078
Loss in iteration 99 : 0.8870206143321819
Loss in iteration 100 : 1.1532822519787396
Loss in iteration 101 : 1.1265012061007655
Loss in iteration 102 : 1.27423717090639
Loss in iteration 103 : 1.12109710933001
Loss in iteration 104 : 1.2435379053820053
Loss in iteration 105 : 1.084523929878228
Loss in iteration 106 : 1.1955360062834726
Loss in iteration 107 : 1.0429375728516612
Loss in iteration 108 : 1.152584575040777
Loss in iteration 109 : 1.033461970283804
Loss in iteration 110 : 1.1319691301541095
Loss in iteration 111 : 1.007291751613512
Loss in iteration 112 : 1.1028701141712778
Loss in iteration 113 : 0.9880747099095182
Loss in iteration 114 : 1.0778908608104762
Loss in iteration 115 : 0.9729773299000734
Loss in iteration 116 : 1.0572472847257086
Loss in iteration 117 : 0.9644745911157089
Loss in iteration 118 : 1.0423038088061731
Loss in iteration 119 : 0.9575785140530301
Loss in iteration 120 : 1.026742426937899
Loss in iteration 121 : 0.945440086329539
Loss in iteration 122 : 1.0158709909548722
Loss in iteration 123 : 0.9341628486852745
Loss in iteration 124 : 1.0076492495519622
Loss in iteration 125 : 0.9259251238132498
Loss in iteration 126 : 0.9969037540773553
Loss in iteration 127 : 0.9186374305350083
Loss in iteration 128 : 0.9882928541748577
Loss in iteration 129 : 0.9147153474603368
Loss in iteration 130 : 0.9817018606436512
Loss in iteration 131 : 0.9080007089787154
Loss in iteration 132 : 0.9740842091972898
Loss in iteration 133 : 0.9044710267206124
Loss in iteration 134 : 0.9674823374067362
Loss in iteration 135 : 0.8988686535753033
Loss in iteration 136 : 0.961037065388969
Loss in iteration 137 : 0.894539251941165
Loss in iteration 138 : 0.9539670014442634
Loss in iteration 139 : 0.8918347106353123
Loss in iteration 140 : 0.9495754745858118
Loss in iteration 141 : 0.8863165448515279
Loss in iteration 142 : 0.9419860979973455
Loss in iteration 143 : 0.8835961347344409
Loss in iteration 144 : 0.9375144141429717
Loss in iteration 145 : 0.8766506031854364
Loss in iteration 146 : 0.9325985152087403
Loss in iteration 147 : 0.8687267413886276
Loss in iteration 148 : 0.9255709415315609
Loss in iteration 149 : 0.863281354956421
Loss in iteration 150 : 0.9198363589140006
Loss in iteration 151 : 0.8587670718590273
Loss in iteration 152 : 0.9129743023080075
Loss in iteration 153 : 0.8568384363406962
Loss in iteration 154 : 0.9090935516736202
Loss in iteration 155 : 0.8542831979714447
Loss in iteration 156 : 0.9069375661671999
Loss in iteration 157 : 0.8501686171521471
Loss in iteration 158 : 0.9026833157425236
Loss in iteration 159 : 0.8495065747234214
Loss in iteration 160 : 0.9013608638299195
Loss in iteration 161 : 0.8476022440063314
Loss in iteration 162 : 0.8976783393057235
Loss in iteration 163 : 0.8462429284770047
Loss in iteration 164 : 0.8962477410837596
Loss in iteration 165 : 0.8425952215181196
Loss in iteration 166 : 0.8908869087661099
Loss in iteration 167 : 0.8412605878531078
Loss in iteration 168 : 0.889482051802858
Loss in iteration 169 : 0.8389861994655484
Loss in iteration 170 : 0.8866806979512013
Loss in iteration 171 : 0.8382526308270671
Loss in iteration 172 : 0.8861988036195101
Loss in iteration 173 : 0.8329317611573533
Loss in iteration 174 : 0.8805038835050837
Loss in iteration 175 : 0.8280866102975601
Loss in iteration 176 : 0.8755455938057444
Loss in iteration 177 : 0.8278210480293701
Loss in iteration 178 : 0.8769130450560798
Loss in iteration 179 : 0.8275425410724802
Loss in iteration 180 : 0.8763049552081474
Loss in iteration 181 : 0.8271793744594627
Loss in iteration 182 : 0.8732602983804884
Loss in iteration 183 : 0.8256062333202164
Loss in iteration 184 : 0.8719183009345314
Loss in iteration 185 : 0.8247167897176046
Loss in iteration 186 : 0.871124045907798
Loss in iteration 187 : 0.8241073638598376
Loss in iteration 188 : 0.8678951165620052
Loss in iteration 189 : 0.8216848066987748
Loss in iteration 190 : 0.8645870932256373
Loss in iteration 191 : 0.819035885358194
Loss in iteration 192 : 0.8621084770112731
Loss in iteration 193 : 0.8182983594572161
Loss in iteration 194 : 0.8608301770187385
Loss in iteration 195 : 0.81691460414272
Loss in iteration 196 : 0.8607282901430601
Loss in iteration 197 : 0.8173661338720538
Loss in iteration 198 : 0.8596711599591611
Loss in iteration 199 : 0.8147459178692644
Loss in iteration 200 : 0.8535576955024083
Testing accuracy  of updater 3 on alg 1 with rate 7.0 = 0.7135, training accuracy 0.724375, time elapsed: 2446 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9553448558290163
Loss in iteration 3 : 1.1752017616958184
Loss in iteration 4 : 1.6347848388087416
Loss in iteration 5 : 1.0849050942571226
Loss in iteration 6 : 1.494851695034165
Loss in iteration 7 : 1.0788190492363747
Loss in iteration 8 : 1.5803814497328459
Loss in iteration 9 : 0.9085966507596119
Loss in iteration 10 : 1.4445447300710634
Loss in iteration 11 : 0.9293662765241293
Loss in iteration 12 : 1.4795928960583802
Loss in iteration 13 : 0.8314052078786556
Loss in iteration 14 : 1.304826403544277
Loss in iteration 15 : 0.9027400983954789
Loss in iteration 16 : 1.3388623050112947
Loss in iteration 17 : 0.8266971436397546
Loss in iteration 18 : 1.2161927185503159
Loss in iteration 19 : 0.8723822736881223
Loss in iteration 20 : 1.1887619292482263
Loss in iteration 21 : 0.8503769030961519
Loss in iteration 22 : 1.1102248886614925
Loss in iteration 23 : 0.8614795879726133
Loss in iteration 24 : 1.0581303353880527
Loss in iteration 25 : 0.858781681891251
Loss in iteration 26 : 1.011838990280967
Loss in iteration 27 : 0.846844999350231
Loss in iteration 28 : 0.9706225430733069
Loss in iteration 29 : 0.8364866936773147
Loss in iteration 30 : 0.930733272524168
Loss in iteration 31 : 0.825011017339424
Loss in iteration 32 : 0.8928650328248842
Loss in iteration 33 : 0.8142397928366018
Loss in iteration 34 : 0.8643132823065341
Loss in iteration 35 : 0.7952370247847964
Loss in iteration 36 : 0.8341046378620011
Loss in iteration 37 : 0.7758724637693148
Loss in iteration 38 : 0.8048126167581998
Loss in iteration 39 : 0.7620078871651608
Loss in iteration 40 : 0.7784705540215894
Loss in iteration 41 : 0.7494002411017627
Loss in iteration 42 : 0.7621683102491825
Loss in iteration 43 : 0.7351662139221831
Loss in iteration 44 : 0.7453197185757774
Loss in iteration 45 : 0.7230891233677503
Loss in iteration 46 : 0.7343932192234622
Loss in iteration 47 : 0.7137452981372143
Loss in iteration 48 : 0.7249034300361337
Loss in iteration 49 : 0.7043671191383836
Loss in iteration 50 : 0.7123004353019647
Loss in iteration 51 : 0.6980746621030967
Loss in iteration 52 : 0.7028315425071134
Loss in iteration 53 : 0.690186303488332
Loss in iteration 54 : 0.6944808638242848
Loss in iteration 55 : 0.6830696345536421
Loss in iteration 56 : 0.6865429616328586
Loss in iteration 57 : 0.6753651654178932
Loss in iteration 58 : 0.6802989570220977
Loss in iteration 59 : 0.6694180673669686
Loss in iteration 60 : 0.6713537293905453
Loss in iteration 61 : 0.6634702628750678
Loss in iteration 62 : 0.664961261316507
Loss in iteration 63 : 0.6590817176973448
Loss in iteration 64 : 0.659495016002347
Loss in iteration 65 : 0.6556028040252081
Loss in iteration 66 : 0.6549046000077783
Loss in iteration 67 : 0.6499774044644685
Loss in iteration 68 : 0.6522792648407207
Loss in iteration 69 : 0.645887862424695
Loss in iteration 70 : 0.6475262903567396
Loss in iteration 71 : 0.6416644712722759
Loss in iteration 72 : 0.642857414385912
Loss in iteration 73 : 0.6393786836782415
Loss in iteration 74 : 0.641462719991552
Loss in iteration 75 : 0.6382771503668494
Loss in iteration 76 : 0.6370004716889237
Loss in iteration 77 : 0.6332193015739207
Loss in iteration 78 : 0.6313890278129681
Loss in iteration 79 : 0.628443967911859
Loss in iteration 80 : 0.6252891273240881
Loss in iteration 81 : 0.6264102956559053
Loss in iteration 82 : 0.6231780699986935
Loss in iteration 83 : 0.6251672022441006
Loss in iteration 84 : 0.6217091276351812
Loss in iteration 85 : 0.6238169284387947
Loss in iteration 86 : 0.6204463769622467
Loss in iteration 87 : 0.6218657992491508
Loss in iteration 88 : 0.6193178498477412
Loss in iteration 89 : 0.619195736890252
Loss in iteration 90 : 0.6175957096340244
Loss in iteration 91 : 0.6175683791456676
Loss in iteration 92 : 0.6159523919232369
Loss in iteration 93 : 0.6160977057283821
Loss in iteration 94 : 0.6136226074143228
Loss in iteration 95 : 0.6155936339366922
Loss in iteration 96 : 0.6118370458660147
Loss in iteration 97 : 0.6130275917254366
Loss in iteration 98 : 0.6086429043399132
Loss in iteration 99 : 0.6091406405458886
Loss in iteration 100 : 0.6050518188873646
Loss in iteration 101 : 0.6085401145898731
Loss in iteration 102 : 0.602629936872233
Loss in iteration 103 : 0.6057660241067707
Loss in iteration 104 : 0.6011645788086414
Loss in iteration 105 : 0.6049866751904961
Loss in iteration 106 : 0.6005689815035492
Loss in iteration 107 : 0.6038632651751418
Loss in iteration 108 : 0.5996107486754014
Loss in iteration 109 : 0.6027407734137546
Loss in iteration 110 : 0.5994419668374094
Loss in iteration 111 : 0.6024610287824701
Loss in iteration 112 : 0.5987034335821552
Loss in iteration 113 : 0.6008881844270463
Loss in iteration 114 : 0.59859822373702
Loss in iteration 115 : 0.6004293524139249
Loss in iteration 116 : 0.5988673156035641
Loss in iteration 117 : 0.5996376685567282
Loss in iteration 118 : 0.5992992103941414
Loss in iteration 119 : 0.5984866700752255
Loss in iteration 120 : 0.5987086911248998
Loss in iteration 121 : 0.5973649241771525
Loss in iteration 122 : 0.5985818155392135
Loss in iteration 123 : 0.5971801598341993
Loss in iteration 124 : 0.5972683463636286
Loss in iteration 125 : 0.5975113756841143
Loss in iteration 126 : 0.5969510818172616
Loss in iteration 127 : 0.5973796172694196
Loss in iteration 128 : 0.5974035170087937
Loss in iteration 129 : 0.5966164480430888
Loss in iteration 130 : 0.5966364193099815
Loss in iteration 131 : 0.5953632326378063
Loss in iteration 132 : 0.5970140850902931
Loss in iteration 133 : 0.5965249740797012
Loss in iteration 134 : 0.5981420893621623
Loss in iteration 135 : 0.5971474201801812
Loss in iteration 136 : 0.6002258912271616
Loss in iteration 137 : 0.5968951263894633
Loss in iteration 138 : 0.5996011196642732
Loss in iteration 139 : 0.5977379496248154
Loss in iteration 140 : 0.5994626539333063
Loss in iteration 141 : 0.5972811920282021
Loss in iteration 142 : 0.5997723827248936
Loss in iteration 143 : 0.5964636417006615
Loss in iteration 144 : 0.5976749491195911
Loss in iteration 145 : 0.5953596768634066
Loss in iteration 146 : 0.5965000566815126
Loss in iteration 147 : 0.5941705653655197
Loss in iteration 148 : 0.5937290211518552
Loss in iteration 149 : 0.5906725817684314
Loss in iteration 150 : 0.5927675889238914
Loss in iteration 151 : 0.5896690859014145
Loss in iteration 152 : 0.590136907481937
Loss in iteration 153 : 0.5875943341308678
Loss in iteration 154 : 0.5873851352746342
Loss in iteration 155 : 0.5876326631876441
Loss in iteration 156 : 0.5861500316790285
Loss in iteration 157 : 0.5861362991030746
Loss in iteration 158 : 0.5844055234210569
Loss in iteration 159 : 0.5841887564565541
Loss in iteration 160 : 0.5826248602324116
Loss in iteration 161 : 0.5831008488651708
Loss in iteration 162 : 0.5810311013229769
Loss in iteration 163 : 0.5827389076758036
Loss in iteration 164 : 0.5800315639323115
Loss in iteration 165 : 0.5817748056385911
Loss in iteration 166 : 0.5788989674504416
Loss in iteration 167 : 0.5802686517417033
Loss in iteration 168 : 0.5787446016106276
Loss in iteration 169 : 0.5788004694552369
Loss in iteration 170 : 0.5773216085380659
Loss in iteration 171 : 0.577363995749811
Loss in iteration 172 : 0.5759980819128163
Loss in iteration 173 : 0.576177315348963
Loss in iteration 174 : 0.5755039462615584
Loss in iteration 175 : 0.5757192148805526
Loss in iteration 176 : 0.5743085843944427
Loss in iteration 177 : 0.5754132026032496
Loss in iteration 178 : 0.5743051708784824
Loss in iteration 179 : 0.5748302923307456
Loss in iteration 180 : 0.5739692216501355
Loss in iteration 181 : 0.5750388990439367
Loss in iteration 182 : 0.573730524788232
Loss in iteration 183 : 0.5741729903914823
Loss in iteration 184 : 0.5731755391601929
Loss in iteration 185 : 0.5737863339434739
Loss in iteration 186 : 0.5731301399217469
Loss in iteration 187 : 0.5734626599051891
Loss in iteration 188 : 0.572702508495899
Loss in iteration 189 : 0.5726963737405999
Loss in iteration 190 : 0.5719905903196338
Loss in iteration 191 : 0.5719151574268908
Loss in iteration 192 : 0.5706516937102772
Loss in iteration 193 : 0.5714874120214716
Loss in iteration 194 : 0.570216924681433
Loss in iteration 195 : 0.5705863895578066
Loss in iteration 196 : 0.5701466102381108
Loss in iteration 197 : 0.570243710073938
Loss in iteration 198 : 0.5692555276550931
Loss in iteration 199 : 0.5699356943797044
Loss in iteration 200 : 0.5687103263934198
Testing accuracy  of updater 3 on alg 1 with rate 4.0 = 0.7665, training accuracy 0.763875, time elapsed: 2585 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9553031168517377
Loss in iteration 3 : 0.9234856159145782
Loss in iteration 4 : 0.9094413480726692
Loss in iteration 5 : 0.9000350920709427
Loss in iteration 6 : 0.8908986327606171
Loss in iteration 7 : 0.8818634182129167
Loss in iteration 8 : 0.8728813485378958
Loss in iteration 9 : 0.8639487773100156
Loss in iteration 10 : 0.8550868698925592
Loss in iteration 11 : 0.8462751772509687
Loss in iteration 12 : 0.8375063044737829
Loss in iteration 13 : 0.8287649121221473
Loss in iteration 14 : 0.8200545160007016
Loss in iteration 15 : 0.8113778763360598
Loss in iteration 16 : 0.8027295248576851
Loss in iteration 17 : 0.7941095641988816
Loss in iteration 18 : 0.7855286803483514
Loss in iteration 19 : 0.7770502081214847
Loss in iteration 20 : 0.7686836628855453
Loss in iteration 21 : 0.7604861943392467
Loss in iteration 22 : 0.752539014342397
Loss in iteration 23 : 0.7447891124092371
Loss in iteration 24 : 0.7371870862540527
Loss in iteration 25 : 0.7297407906865043
Loss in iteration 26 : 0.7224348747243193
Loss in iteration 27 : 0.7152984448558402
Loss in iteration 28 : 0.7083332228948471
Loss in iteration 29 : 0.701534232621224
Loss in iteration 30 : 0.6948987759730144
Loss in iteration 31 : 0.6885805916319963
Loss in iteration 32 : 0.6825258908012488
Loss in iteration 33 : 0.6768104079754844
Loss in iteration 34 : 0.6714031203325121
Loss in iteration 35 : 0.6663791212742495
Loss in iteration 36 : 0.6616497847523495
Loss in iteration 37 : 0.6571808762192376
Loss in iteration 38 : 0.6529308227983417
Loss in iteration 39 : 0.6489382296417053
Loss in iteration 40 : 0.6452040599946369
Loss in iteration 41 : 0.6417112649822284
Loss in iteration 42 : 0.638406503506379
Loss in iteration 43 : 0.6352730582990793
Loss in iteration 44 : 0.6323344769879079
Loss in iteration 45 : 0.6295628307809729
Loss in iteration 46 : 0.6269168827025874
Loss in iteration 47 : 0.6244186514801687
Loss in iteration 48 : 0.6220610595830096
Loss in iteration 49 : 0.6198160790708537
Loss in iteration 50 : 0.6176687219019553
Loss in iteration 51 : 0.6156114137943856
Loss in iteration 52 : 0.6136177005267643
Loss in iteration 53 : 0.6116972164587601
Loss in iteration 54 : 0.6098577362492702
Loss in iteration 55 : 0.6081113212236744
Loss in iteration 56 : 0.6064348763267703
Loss in iteration 57 : 0.6048343352313602
Loss in iteration 58 : 0.6032755987226894
Loss in iteration 59 : 0.6017762855480766
Loss in iteration 60 : 0.6003289469773808
Loss in iteration 61 : 0.5989295501269207
Loss in iteration 62 : 0.5975772128446986
Loss in iteration 63 : 0.5962588134778147
Loss in iteration 64 : 0.5949746297288278
Loss in iteration 65 : 0.5937382262668529
Loss in iteration 66 : 0.5925393101753068
Loss in iteration 67 : 0.5913765412534676
Loss in iteration 68 : 0.5902445772979166
Loss in iteration 69 : 0.5891390965570225
Loss in iteration 70 : 0.5880685560238974
Loss in iteration 71 : 0.5870368593904162
Loss in iteration 72 : 0.5860326850710387
Loss in iteration 73 : 0.58506277847124
Loss in iteration 74 : 0.5841141835820647
Loss in iteration 75 : 0.5831839788137678
Loss in iteration 76 : 0.5822693241793063
Loss in iteration 77 : 0.5813726940634324
Loss in iteration 78 : 0.5804966652701836
Loss in iteration 79 : 0.579635264318158
Loss in iteration 80 : 0.5787979109743422
Loss in iteration 81 : 0.5779820112771629
Loss in iteration 82 : 0.5771830102866568
Loss in iteration 83 : 0.5763994116761967
Loss in iteration 84 : 0.5756359371066682
Loss in iteration 85 : 0.5749003537792814
Loss in iteration 86 : 0.5741886197715576
Loss in iteration 87 : 0.5734945788148895
Loss in iteration 88 : 0.5728080027700014
Loss in iteration 89 : 0.5721330185230797
Loss in iteration 90 : 0.5714691136050285
Loss in iteration 91 : 0.570828708544234
Loss in iteration 92 : 0.5702013115582931
Loss in iteration 93 : 0.5695822785378821
Loss in iteration 94 : 0.5689759655919663
Loss in iteration 95 : 0.5683871245717186
Loss in iteration 96 : 0.5678072884505564
Loss in iteration 97 : 0.5672404788607655
Loss in iteration 98 : 0.5666845803071178
Loss in iteration 99 : 0.5661427064535756
Loss in iteration 100 : 0.5656124733779877
Loss in iteration 101 : 0.5650908678958029
Loss in iteration 102 : 0.5645792036167616
Loss in iteration 103 : 0.564075352017383
Loss in iteration 104 : 0.5635805579312525
Loss in iteration 105 : 0.5630935796206322
Loss in iteration 106 : 0.5626147241191365
Loss in iteration 107 : 0.5621484674196775
Loss in iteration 108 : 0.5616946505492885
Loss in iteration 109 : 0.5612481603351189
Loss in iteration 110 : 0.5608086184555615
Loss in iteration 111 : 0.5603743816875284
Loss in iteration 112 : 0.5599455513627031
Loss in iteration 113 : 0.5595212961508225
Loss in iteration 114 : 0.5591035642750216
Loss in iteration 115 : 0.5586930228793019
Loss in iteration 116 : 0.5582887015898123
Loss in iteration 117 : 0.557888900174848
Loss in iteration 118 : 0.5574936418579712
Loss in iteration 119 : 0.5571064052001669
Loss in iteration 120 : 0.5567250101011741
Loss in iteration 121 : 0.5563515310146887
Loss in iteration 122 : 0.555985677307874
Loss in iteration 123 : 0.5556289172836298
Loss in iteration 124 : 0.5552771168094841
Loss in iteration 125 : 0.554931145165553
Loss in iteration 126 : 0.5545883807986683
Loss in iteration 127 : 0.554250617245498
Loss in iteration 128 : 0.5539163011507188
Loss in iteration 129 : 0.5535879295430794
Loss in iteration 130 : 0.5532654280488345
Loss in iteration 131 : 0.5529491305818204
Loss in iteration 132 : 0.5526377439916723
Loss in iteration 133 : 0.5523307441271689
Loss in iteration 134 : 0.5520296227681314
Loss in iteration 135 : 0.5517339783698066
Loss in iteration 136 : 0.5514441085818828
Loss in iteration 137 : 0.5511608253947663
Loss in iteration 138 : 0.5508806182730414
Loss in iteration 139 : 0.5506035888866679
Loss in iteration 140 : 0.5503280696973857
Loss in iteration 141 : 0.5500549620065974
Loss in iteration 142 : 0.5497843026327498
Loss in iteration 143 : 0.549515684107563
Loss in iteration 144 : 0.5492476331565045
Loss in iteration 145 : 0.5489806572688274
Loss in iteration 146 : 0.5487176027340682
Loss in iteration 147 : 0.5484578668426366
Loss in iteration 148 : 0.5482028841142702
Loss in iteration 149 : 0.5479519955815804
Loss in iteration 150 : 0.5477039977198049
Loss in iteration 151 : 0.5474588004671227
Loss in iteration 152 : 0.5472151339163899
Loss in iteration 153 : 0.5469726490141233
Loss in iteration 154 : 0.5467320128726655
Loss in iteration 155 : 0.5464931542334035
Loss in iteration 156 : 0.5462578043151469
Loss in iteration 157 : 0.5460258487170908
Loss in iteration 158 : 0.5457966861451025
Loss in iteration 159 : 0.5455718499072143
Loss in iteration 160 : 0.5453491164791138
Loss in iteration 161 : 0.5451269638302494
Loss in iteration 162 : 0.5449059398868449
Loss in iteration 163 : 0.5446885208966683
Loss in iteration 164 : 0.5444738999558991
Loss in iteration 165 : 0.5442600304085495
Loss in iteration 166 : 0.5440480686876067
Loss in iteration 167 : 0.5438370345935152
Loss in iteration 168 : 0.5436280931150554
Loss in iteration 169 : 0.543421011083342
Loss in iteration 170 : 0.543215337576944
Loss in iteration 171 : 0.5430127295728949
Loss in iteration 172 : 0.5428123038399951
Loss in iteration 173 : 0.5426145437741802
Loss in iteration 174 : 0.5424186545845898
Loss in iteration 175 : 0.5422239429638895
Loss in iteration 176 : 0.5420314495472005
Loss in iteration 177 : 0.5418417392465599
Loss in iteration 178 : 0.5416551717952184
Loss in iteration 179 : 0.5414719727013557
Loss in iteration 180 : 0.5412905119564974
Loss in iteration 181 : 0.541110971354939
Loss in iteration 182 : 0.540933236606663
Loss in iteration 183 : 0.5407570149018961
Loss in iteration 184 : 0.5405816013913666
Loss in iteration 185 : 0.5404074457754573
Loss in iteration 186 : 0.5402348637369706
Loss in iteration 187 : 0.540064542645786
Loss in iteration 188 : 0.539895639209192
Loss in iteration 189 : 0.539728620532263
Loss in iteration 190 : 0.5395631231547254
Loss in iteration 191 : 0.5393989079876466
Loss in iteration 192 : 0.5392346182167573
Loss in iteration 193 : 0.5390723028529387
Loss in iteration 194 : 0.5389115562067793
Loss in iteration 195 : 0.5387522809067269
Loss in iteration 196 : 0.5385943442491073
Loss in iteration 197 : 0.5384397338800236
Loss in iteration 198 : 0.538286544051188
Loss in iteration 199 : 0.5381342452400406
Loss in iteration 200 : 0.5379831112277257
Testing accuracy  of updater 3 on alg 1 with rate 1.0 = 0.773, training accuracy 0.776125, time elapsed: 2063 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.968529636123105
Loss in iteration 3 : 0.9406101009641166
Loss in iteration 4 : 0.9226658309393156
Loss in iteration 5 : 0.9123099550649553
Loss in iteration 6 : 0.9052381759791408
Loss in iteration 7 : 0.898724908256848
Loss in iteration 8 : 0.8923779033338495
Loss in iteration 9 : 0.886093942749411
Loss in iteration 10 : 0.8798402918051764
Loss in iteration 11 : 0.873605006285071
Loss in iteration 12 : 0.8673934066524746
Loss in iteration 13 : 0.8612242047113248
Loss in iteration 14 : 0.8550809821703477
Loss in iteration 15 : 0.848962177465171
Loss in iteration 16 : 0.8428674164196464
Loss in iteration 17 : 0.8367933989638688
Loss in iteration 18 : 0.8307276385488991
Loss in iteration 19 : 0.8246768515685915
Loss in iteration 20 : 0.8186391528583947
Loss in iteration 21 : 0.8126210242594418
Loss in iteration 22 : 0.8066154310815395
Loss in iteration 23 : 0.8006265554848878
Loss in iteration 24 : 0.7946486260393562
Loss in iteration 25 : 0.7886896728446362
Loss in iteration 26 : 0.7827687443929585
Loss in iteration 27 : 0.7769130711658047
Loss in iteration 28 : 0.7711028758269372
Loss in iteration 29 : 0.7653711690564524
Loss in iteration 30 : 0.7597309440737259
Loss in iteration 31 : 0.7542161957926831
Loss in iteration 32 : 0.7488061383885751
Loss in iteration 33 : 0.7434734137313833
Loss in iteration 34 : 0.7382113028145572
Loss in iteration 35 : 0.7330306276847153
Loss in iteration 36 : 0.7279157010027998
Loss in iteration 37 : 0.7228626336610499
Loss in iteration 38 : 0.7178993651685803
Loss in iteration 39 : 0.7130147188440453
Loss in iteration 40 : 0.7082259415746742
Loss in iteration 41 : 0.7035093707288163
Loss in iteration 42 : 0.6988566600240751
Loss in iteration 43 : 0.6943261978686686
Loss in iteration 44 : 0.6899502004170395
Loss in iteration 45 : 0.6857022890921817
Loss in iteration 46 : 0.6815712097997362
Loss in iteration 47 : 0.6776309188635681
Loss in iteration 48 : 0.6738348400448083
Loss in iteration 49 : 0.6702048444086541
Loss in iteration 50 : 0.6667595216123693
Loss in iteration 51 : 0.663450829317049
Loss in iteration 52 : 0.6602670607140536
Loss in iteration 53 : 0.6571985177746316
Loss in iteration 54 : 0.6542411625944132
Loss in iteration 55 : 0.651403426588628
Loss in iteration 56 : 0.6486968699551676
Loss in iteration 57 : 0.6461115344499702
Loss in iteration 58 : 0.6436360110280845
Loss in iteration 59 : 0.6412648187415062
Loss in iteration 60 : 0.6389785796240736
Loss in iteration 61 : 0.6367690252061716
Loss in iteration 62 : 0.6346597814508848
Loss in iteration 63 : 0.6326396127163902
Loss in iteration 64 : 0.6307027957189243
Loss in iteration 65 : 0.6288249032786136
Loss in iteration 66 : 0.6270168578843768
Loss in iteration 67 : 0.6252746950999818
Loss in iteration 68 : 0.6236055693786817
Loss in iteration 69 : 0.6219936455234714
Loss in iteration 70 : 0.6204365948636669
Loss in iteration 71 : 0.6189249972252361
Loss in iteration 72 : 0.6174669039574433
Loss in iteration 73 : 0.6160425654845961
Loss in iteration 74 : 0.6146462627024817
Loss in iteration 75 : 0.6132865470085779
Loss in iteration 76 : 0.6119611996368747
Loss in iteration 77 : 0.6106731059198337
Loss in iteration 78 : 0.6094339029674998
Loss in iteration 79 : 0.6082333475599305
Loss in iteration 80 : 0.6070633948285127
Loss in iteration 81 : 0.6059347129368202
Loss in iteration 82 : 0.6048333496546509
Loss in iteration 83 : 0.6037529581000137
Loss in iteration 84 : 0.602696033256568
Loss in iteration 85 : 0.6016708074375824
Loss in iteration 86 : 0.6006685304035604
Loss in iteration 87 : 0.5996869273383096
Loss in iteration 88 : 0.5987344216486531
Loss in iteration 89 : 0.5978000167397546
Loss in iteration 90 : 0.5968815918952021
Loss in iteration 91 : 0.5959778590765185
Loss in iteration 92 : 0.5950921605821405
Loss in iteration 93 : 0.5942271573456976
Loss in iteration 94 : 0.5933830104892879
Loss in iteration 95 : 0.592557934891033
Loss in iteration 96 : 0.5917541912824356
Loss in iteration 97 : 0.5909650536790384
Loss in iteration 98 : 0.5901880527713248
Loss in iteration 99 : 0.5894240898591394
Loss in iteration 100 : 0.5886781192328847
Loss in iteration 101 : 0.5879502879741619
Loss in iteration 102 : 0.587235322480819
Loss in iteration 103 : 0.5865303079728683
Loss in iteration 104 : 0.585841011856882
Loss in iteration 105 : 0.5851681930485343
Loss in iteration 106 : 0.58450902593859
Loss in iteration 107 : 0.5838592159020466
Loss in iteration 108 : 0.5832173369272671
Loss in iteration 109 : 0.5825808326596015
Loss in iteration 110 : 0.5819533660551256
Loss in iteration 111 : 0.5813361538398276
Loss in iteration 112 : 0.5807279979162775
Loss in iteration 113 : 0.5801282446043277
Loss in iteration 114 : 0.5795356240037336
Loss in iteration 115 : 0.5789608653030323
Loss in iteration 116 : 0.5783945605570581
Loss in iteration 117 : 0.577833876366379
Loss in iteration 118 : 0.5772844257924773
Loss in iteration 119 : 0.5767397424092691
Loss in iteration 120 : 0.5762024814066958
Loss in iteration 121 : 0.5756766437891523
Loss in iteration 122 : 0.5751651968915797
Loss in iteration 123 : 0.5746674161170243
Loss in iteration 124 : 0.5741788512975547
Loss in iteration 125 : 0.573698610603537
Loss in iteration 126 : 0.5732214010684608
Loss in iteration 127 : 0.5727480255487162
Loss in iteration 128 : 0.5722783009343784
Loss in iteration 129 : 0.571815211540988
Loss in iteration 130 : 0.5713604578647963
Loss in iteration 131 : 0.5709173528265121
Loss in iteration 132 : 0.5704815683833678
Loss in iteration 133 : 0.5700527244080149
Loss in iteration 134 : 0.5696262021368792
Loss in iteration 135 : 0.569207304141257
Loss in iteration 136 : 0.5687926851874212
Loss in iteration 137 : 0.5683840031457512
Loss in iteration 138 : 0.5679805579478716
Loss in iteration 139 : 0.5675867581530633
Loss in iteration 140 : 0.5671979450991502
Loss in iteration 141 : 0.5668128236969996
Loss in iteration 142 : 0.5664313130284195
Loss in iteration 143 : 0.5660577899871376
Loss in iteration 144 : 0.5656892483798985
Loss in iteration 145 : 0.5653245721301189
Loss in iteration 146 : 0.5649659901267267
Loss in iteration 147 : 0.5646106419557538
Loss in iteration 148 : 0.56425930920953
Loss in iteration 149 : 0.5639135011189998
Loss in iteration 150 : 0.56357150530927
Loss in iteration 151 : 0.5632352352917126
Loss in iteration 152 : 0.5629013798942792
Loss in iteration 153 : 0.5625706402000937
Loss in iteration 154 : 0.5622476510415408
Loss in iteration 155 : 0.5619295552274391
Loss in iteration 156 : 0.5616159197316749
Loss in iteration 157 : 0.561307039561601
Loss in iteration 158 : 0.5610016759619247
Loss in iteration 159 : 0.5606991669384613
Loss in iteration 160 : 0.560397868383451
Loss in iteration 161 : 0.5601002707316781
Loss in iteration 162 : 0.5598052058205087
Loss in iteration 163 : 0.559513275074929
Loss in iteration 164 : 0.5592240532898668
Loss in iteration 165 : 0.558937455985253
Loss in iteration 166 : 0.5586537607488806
Loss in iteration 167 : 0.5583736916562402
Loss in iteration 168 : 0.5580959903313543
Loss in iteration 169 : 0.5578218648073976
Loss in iteration 170 : 0.5575490332760926
Loss in iteration 171 : 0.5572783672364366
Loss in iteration 172 : 0.5570113268602566
Loss in iteration 173 : 0.5567495719632007
Loss in iteration 174 : 0.5564910434391119
Loss in iteration 175 : 0.5562356455216323
Loss in iteration 176 : 0.5559842571719141
Loss in iteration 177 : 0.5557359955925258
Loss in iteration 178 : 0.5554903489258363
Loss in iteration 179 : 0.5552477964991209
Loss in iteration 180 : 0.5550082380209667
Loss in iteration 181 : 0.5547705556787552
Loss in iteration 182 : 0.5545350535255918
Loss in iteration 183 : 0.5543018084348389
Loss in iteration 184 : 0.5540697646089198
Loss in iteration 185 : 0.5538419164432518
Loss in iteration 186 : 0.5536170644913823
Loss in iteration 187 : 0.5533946547158749
Loss in iteration 188 : 0.5531747066161472
Loss in iteration 189 : 0.552956354278027
Loss in iteration 190 : 0.55273978083844
Loss in iteration 191 : 0.552526423616397
Loss in iteration 192 : 0.5523150772443152
Loss in iteration 193 : 0.5521053722992271
Loss in iteration 194 : 0.5518986909191358
Loss in iteration 195 : 0.5516946470254145
Loss in iteration 196 : 0.5514930005143683
Loss in iteration 197 : 0.5512958039955694
Loss in iteration 198 : 0.5511011192048558
Loss in iteration 199 : 0.5509078063706863
Loss in iteration 200 : 0.5507154596680526
Testing accuracy  of updater 3 on alg 1 with rate 0.7 = 0.7695, training accuracy 0.77075, time elapsed: 2440 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9821257102416162
Loss in iteration 3 : 0.9644601939016167
Loss in iteration 4 : 0.9484409306915909
Loss in iteration 5 : 0.9356506853009189
Loss in iteration 6 : 0.9259160218348391
Loss in iteration 7 : 0.9186585683863812
Loss in iteration 8 : 0.9133201026379869
Loss in iteration 9 : 0.9089566077756063
Loss in iteration 10 : 0.9050255480353114
Loss in iteration 11 : 0.9012726064751898
Loss in iteration 12 : 0.8976249165564306
Loss in iteration 13 : 0.8940269031597174
Loss in iteration 14 : 0.8904572715506515
Loss in iteration 15 : 0.8869101243918951
Loss in iteration 16 : 0.8833692082357015
Loss in iteration 17 : 0.8798415303511592
Loss in iteration 18 : 0.8763207518582383
Loss in iteration 19 : 0.8728047989283074
Loss in iteration 20 : 0.8692960182398538
Loss in iteration 21 : 0.8658067083087615
Loss in iteration 22 : 0.8623287643817051
Loss in iteration 23 : 0.8588591779942265
Loss in iteration 24 : 0.8553943337285012
Loss in iteration 25 : 0.851939321638779
Loss in iteration 26 : 0.8484896850485432
Loss in iteration 27 : 0.8450492521026981
Loss in iteration 28 : 0.8416185268646134
Loss in iteration 29 : 0.8381921057828249
Loss in iteration 30 : 0.8347687682816348
Loss in iteration 31 : 0.8313477957752992
Loss in iteration 32 : 0.8279315836144895
Loss in iteration 33 : 0.8245210562980344
Loss in iteration 34 : 0.8211147478508203
Loss in iteration 35 : 0.817715436770539
Loss in iteration 36 : 0.8143228261545028
Loss in iteration 37 : 0.8109339862587999
Loss in iteration 38 : 0.8075477354226549
Loss in iteration 39 : 0.804169401225831
Loss in iteration 40 : 0.8007957754721288
Loss in iteration 41 : 0.7974245155559588
Loss in iteration 42 : 0.7940581644046267
Loss in iteration 43 : 0.7906988718627765
Loss in iteration 44 : 0.7873484452313639
Loss in iteration 45 : 0.7840260679442781
Loss in iteration 46 : 0.7807157283777328
Loss in iteration 47 : 0.7774176933518979
Loss in iteration 48 : 0.7741394806840235
Loss in iteration 49 : 0.770885779916284
Loss in iteration 50 : 0.7676527356722387
Loss in iteration 51 : 0.7644557165700017
Loss in iteration 52 : 0.7612778630880784
Loss in iteration 53 : 0.7581445484750643
Loss in iteration 54 : 0.7550516350335841
Loss in iteration 55 : 0.7519876935646161
Loss in iteration 56 : 0.7489599766225444
Loss in iteration 57 : 0.745950277437412
Loss in iteration 58 : 0.7429642158835038
Loss in iteration 59 : 0.7399970856780822
Loss in iteration 60 : 0.7370648412296255
Loss in iteration 61 : 0.7341524806843642
Loss in iteration 62 : 0.7312593444419523
Loss in iteration 63 : 0.7283920581137187
Loss in iteration 64 : 0.7255395303093818
Loss in iteration 65 : 0.7227134058652159
Loss in iteration 66 : 0.7199137618920878
Loss in iteration 67 : 0.7171423232551547
Loss in iteration 68 : 0.7143966077728685
Loss in iteration 69 : 0.7116746059174236
Loss in iteration 70 : 0.7089826581871513
Loss in iteration 71 : 0.7063191639672943
Loss in iteration 72 : 0.7036706032250304
Loss in iteration 73 : 0.7010477764915691
Loss in iteration 74 : 0.6984525670886971
Loss in iteration 75 : 0.6959056320639336
Loss in iteration 76 : 0.6934103695444159
Loss in iteration 77 : 0.6909620980060621
Loss in iteration 78 : 0.688551290847406
Loss in iteration 79 : 0.6861711417150012
Loss in iteration 80 : 0.6838284388417201
Loss in iteration 81 : 0.6815511206367889
Loss in iteration 82 : 0.679328732676634
Loss in iteration 83 : 0.6771515846650504
Loss in iteration 84 : 0.6750213119557347
Loss in iteration 85 : 0.6729337632662636
Loss in iteration 86 : 0.6709141944678436
Loss in iteration 87 : 0.6689553687024185
Loss in iteration 88 : 0.6670414335406729
Loss in iteration 89 : 0.6651674757477416
Loss in iteration 90 : 0.6633282811158967
Loss in iteration 91 : 0.6615311378982105
Loss in iteration 92 : 0.6597756841444598
Loss in iteration 93 : 0.6580467759481664
Loss in iteration 94 : 0.6563647202031048
Loss in iteration 95 : 0.6547159158328337
Loss in iteration 96 : 0.6531043715784696
Loss in iteration 97 : 0.6515297914657476
Loss in iteration 98 : 0.6499974338648483
Loss in iteration 99 : 0.6485001209967592
Loss in iteration 100 : 0.6470444851735474
Loss in iteration 101 : 0.6456294747145215
Loss in iteration 102 : 0.6442405657062297
Loss in iteration 103 : 0.6428856819299985
Loss in iteration 104 : 0.6415710236951034
Loss in iteration 105 : 0.6402792029332425
Loss in iteration 106 : 0.6390032143827458
Loss in iteration 107 : 0.6377551004040876
Loss in iteration 108 : 0.6365387990060558
Loss in iteration 109 : 0.6353514973692654
Loss in iteration 110 : 0.6341962420003253
Loss in iteration 111 : 0.6330686030425011
Loss in iteration 112 : 0.6319631924609962
Loss in iteration 113 : 0.6308766900494179
Loss in iteration 114 : 0.6298147211955849
Loss in iteration 115 : 0.6287712000110305
Loss in iteration 116 : 0.6277525209553908
Loss in iteration 117 : 0.6267593794555064
Loss in iteration 118 : 0.6257834452135252
Loss in iteration 119 : 0.6248244314228403
Loss in iteration 120 : 0.6238865045635352
Loss in iteration 121 : 0.6229722851888867
Loss in iteration 122 : 0.6220733102544505
Loss in iteration 123 : 0.6211921422179698
Loss in iteration 124 : 0.6203299540115863
Loss in iteration 125 : 0.6194794146358972
Loss in iteration 126 : 0.6186449456432247
Loss in iteration 127 : 0.617829868743941
Loss in iteration 128 : 0.6170265976490454
Loss in iteration 129 : 0.6162302614845255
Loss in iteration 130 : 0.6154421239135565
Loss in iteration 131 : 0.6146647732693027
Loss in iteration 132 : 0.6138996189568038
Loss in iteration 133 : 0.6131473760016254
Loss in iteration 134 : 0.6124049814132944
Loss in iteration 135 : 0.6116731032762577
Loss in iteration 136 : 0.6109541061363991
Loss in iteration 137 : 0.6102504865543998
Loss in iteration 138 : 0.6095600730164056
Loss in iteration 139 : 0.6088832787461449
Loss in iteration 140 : 0.6082126038490635
Loss in iteration 141 : 0.6075518332581499
Loss in iteration 142 : 0.6069051269005843
Loss in iteration 143 : 0.606273770892328
Loss in iteration 144 : 0.6056485910696864
Loss in iteration 145 : 0.6050299571253214
Loss in iteration 146 : 0.604420357459676
Loss in iteration 147 : 0.6038165363988722
Loss in iteration 148 : 0.6032196501365013
Loss in iteration 149 : 0.6026314657386139
Loss in iteration 150 : 0.60205414600526
Loss in iteration 151 : 0.6014845102967858
Loss in iteration 152 : 0.6009231472022764
Loss in iteration 153 : 0.6003679578631739
Loss in iteration 154 : 0.5998217599603938
Loss in iteration 155 : 0.5992837746885248
Loss in iteration 156 : 0.5987521572051553
Loss in iteration 157 : 0.5982254143109744
Loss in iteration 158 : 0.5977038260791908
Loss in iteration 159 : 0.5971884632862807
Loss in iteration 160 : 0.5966776322046594
Loss in iteration 161 : 0.5961739347849538
Loss in iteration 162 : 0.5956757425762842
Loss in iteration 163 : 0.5951817415232051
Loss in iteration 164 : 0.5946927213572214
Loss in iteration 165 : 0.5942101287544883
Loss in iteration 166 : 0.5937364059559836
Loss in iteration 167 : 0.5932702383876312
Loss in iteration 168 : 0.5928089898434377
Loss in iteration 169 : 0.5923546614922062
Loss in iteration 170 : 0.5919066850517722
Loss in iteration 171 : 0.5914640250149036
Loss in iteration 172 : 0.5910247020946741
Loss in iteration 173 : 0.590588624630197
Loss in iteration 174 : 0.5901546665876729
Loss in iteration 175 : 0.5897279829750436
Loss in iteration 176 : 0.5893065462757181
Loss in iteration 177 : 0.5888902127277093
Loss in iteration 178 : 0.588479567374899
Loss in iteration 179 : 0.5880734207785538
Loss in iteration 180 : 0.5876696764460461
Loss in iteration 181 : 0.5872687557234729
Loss in iteration 182 : 0.5868725550690376
Loss in iteration 183 : 0.5864797350857367
Loss in iteration 184 : 0.5860899517267618
Loss in iteration 185 : 0.5857058269678221
Loss in iteration 186 : 0.5853252563396406
Loss in iteration 187 : 0.5849488452281083
Loss in iteration 188 : 0.5845780234054878
Loss in iteration 189 : 0.5842119850297499
Loss in iteration 190 : 0.5838487179120116
Loss in iteration 191 : 0.5834886341860206
Loss in iteration 192 : 0.5831302961441884
Loss in iteration 193 : 0.5827728598416568
Loss in iteration 194 : 0.5824172628040151
Loss in iteration 195 : 0.5820648332663894
Loss in iteration 196 : 0.5817152473550274
Loss in iteration 197 : 0.5813695674057608
Loss in iteration 198 : 0.5810278379755135
Loss in iteration 199 : 0.580690392243178
Loss in iteration 200 : 0.5803561389474265
Testing accuracy  of updater 3 on alg 1 with rate 0.4 = 0.763, training accuracy 0.764625, time elapsed: 2688 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9955934447394867
Loss in iteration 3 : 0.9911909775297634
Loss in iteration 4 : 0.9867925819351175
Loss in iteration 5 : 0.9823982416439726
Loss in iteration 6 : 0.9780079404675474
Loss in iteration 7 : 0.9736448147077835
Loss in iteration 8 : 0.9693445017373867
Loss in iteration 9 : 0.9651363687552458
Loss in iteration 10 : 0.9610366899351618
Loss in iteration 11 : 0.957095044587082
Loss in iteration 12 : 0.9533304377228162
Loss in iteration 13 : 0.9497745897986783
Loss in iteration 14 : 0.9463630315922912
Loss in iteration 15 : 0.9431739964887754
Loss in iteration 16 : 0.9401863253194221
Loss in iteration 17 : 0.9373325168484362
Loss in iteration 18 : 0.9346614147091814
Loss in iteration 19 : 0.9321942857630778
Loss in iteration 20 : 0.9298668405367718
Loss in iteration 21 : 0.9276767544505998
Loss in iteration 22 : 0.9256267948971931
Loss in iteration 23 : 0.9237013146183797
Loss in iteration 24 : 0.9219350012640518
Loss in iteration 25 : 0.9203234542426053
Loss in iteration 26 : 0.9188246968753997
Loss in iteration 27 : 0.9174004129263091
Loss in iteration 28 : 0.916050817311833
Loss in iteration 29 : 0.9147740272978244
Loss in iteration 30 : 0.9135772045283393
Loss in iteration 31 : 0.9124316614104673
Loss in iteration 32 : 0.9113264454437815
Loss in iteration 33 : 0.91026237149488
Loss in iteration 34 : 0.909235398364079
Loss in iteration 35 : 0.9082330541956193
Loss in iteration 36 : 0.9072516339751996
Loss in iteration 37 : 0.9062927442454368
Loss in iteration 38 : 0.9053411454663556
Loss in iteration 39 : 0.9043996280079474
Loss in iteration 40 : 0.9034632499147038
Loss in iteration 41 : 0.9025371993290732
Loss in iteration 42 : 0.9016212347499093
Loss in iteration 43 : 0.9007086686374312
Loss in iteration 44 : 0.8998036478592387
Loss in iteration 45 : 0.8989047736227578
Loss in iteration 46 : 0.8980125423371628
Loss in iteration 47 : 0.8971242579658919
Loss in iteration 48 : 0.8962411894121278
Loss in iteration 49 : 0.8953618995164409
Loss in iteration 50 : 0.8944843817857313
Loss in iteration 51 : 0.8936077478963648
Loss in iteration 52 : 0.8927346180024158
Loss in iteration 53 : 0.8918639690052996
Loss in iteration 54 : 0.8909944291416141
Loss in iteration 55 : 0.8901267925408747
Loss in iteration 56 : 0.8892594730498554
Loss in iteration 57 : 0.8883930215587785
Loss in iteration 58 : 0.8875283050225239
Loss in iteration 59 : 0.8866646137130886
Loss in iteration 60 : 0.8858017490968069
Loss in iteration 61 : 0.884939426423972
Loss in iteration 62 : 0.8840778973213365
Loss in iteration 63 : 0.8832167643916196
Loss in iteration 64 : 0.8823568039314059
Loss in iteration 65 : 0.8814981822561652
Loss in iteration 66 : 0.880640475918088
Loss in iteration 67 : 0.8797833682653018
Loss in iteration 68 : 0.8789266580598483
Loss in iteration 69 : 0.8780702015752049
Loss in iteration 70 : 0.8772139762076285
Loss in iteration 71 : 0.8763578779194706
Loss in iteration 72 : 0.875501906635911
Loss in iteration 73 : 0.8746462497746584
Loss in iteration 74 : 0.8737911577192745
Loss in iteration 75 : 0.8729362529981938
Loss in iteration 76 : 0.8720824244728191
Loss in iteration 77 : 0.8712289888854081
Loss in iteration 78 : 0.8703757217633397
Loss in iteration 79 : 0.8695230456008101
Loss in iteration 80 : 0.8686728448518084
Loss in iteration 81 : 0.8678243492025436
Loss in iteration 82 : 0.8669761827830909
Loss in iteration 83 : 0.8661290919308856
Loss in iteration 84 : 0.8652832044703814
Loss in iteration 85 : 0.8644383778780951
Loss in iteration 86 : 0.863593917249098
Loss in iteration 87 : 0.8627496481281292
Loss in iteration 88 : 0.8619056049681458
Loss in iteration 89 : 0.8610622296730617
Loss in iteration 90 : 0.8602195172332663
Loss in iteration 91 : 0.8593776049772545
Loss in iteration 92 : 0.8585359221148667
Loss in iteration 93 : 0.8576948630316327
Loss in iteration 94 : 0.8568539288378869
Loss in iteration 95 : 0.8560131570253
Loss in iteration 96 : 0.85517339051047
Loss in iteration 97 : 0.8543339623908854
Loss in iteration 98 : 0.8534950848904908
Loss in iteration 99 : 0.8526576773617692
Loss in iteration 100 : 0.8518204919921512
Loss in iteration 101 : 0.8509839791896554
Loss in iteration 102 : 0.8501480334419272
Loss in iteration 103 : 0.8493125347026259
Loss in iteration 104 : 0.8484774154473925
Loss in iteration 105 : 0.8476431258647421
Loss in iteration 106 : 0.8468097464838263
Loss in iteration 107 : 0.8459765451519986
Loss in iteration 108 : 0.8451440346558301
Loss in iteration 109 : 0.8443121451495489
Loss in iteration 110 : 0.8434804719939935
Loss in iteration 111 : 0.8426495108920341
Loss in iteration 112 : 0.8418186898607746
Loss in iteration 113 : 0.8409879920418672
Loss in iteration 114 : 0.840157532843496
Loss in iteration 115 : 0.839327944361608
Loss in iteration 116 : 0.838498478692897
Loss in iteration 117 : 0.8376691861701406
Loss in iteration 118 : 0.8368403750240601
Loss in iteration 119 : 0.8360119824141543
Loss in iteration 120 : 0.8351839905150953
Loss in iteration 121 : 0.8343564464607881
Loss in iteration 122 : 0.8335291088556558
Loss in iteration 123 : 0.8327019278663509
Loss in iteration 124 : 0.8318752532510106
Loss in iteration 125 : 0.8310491862527335
Loss in iteration 126 : 0.830223241224031
Loss in iteration 127 : 0.8293974382445974
Loss in iteration 128 : 0.8285719423897127
Loss in iteration 129 : 0.8277469277202006
Loss in iteration 130 : 0.8269230556018938
Loss in iteration 131 : 0.8260994999958411
Loss in iteration 132 : 0.8252760701830016
Loss in iteration 133 : 0.8244531295276988
Loss in iteration 134 : 0.8236309697435032
Loss in iteration 135 : 0.8228092605582938
Loss in iteration 136 : 0.8219876819854744
Loss in iteration 137 : 0.8211663515605101
Loss in iteration 138 : 0.8203455328552568
Loss in iteration 139 : 0.8195248350954066
Loss in iteration 140 : 0.8187042582097657
Loss in iteration 141 : 0.8178839715273787
Loss in iteration 142 : 0.817064300578879
Loss in iteration 143 : 0.8162460182333259
Loss in iteration 144 : 0.8154278562483435
Loss in iteration 145 : 0.8146098485961475
Loss in iteration 146 : 0.8137921742405713
Loss in iteration 147 : 0.8129747585319078
Loss in iteration 148 : 0.8121575062346883
Loss in iteration 149 : 0.8113403737488247
Loss in iteration 150 : 0.8105233610043556
Loss in iteration 151 : 0.8097066908132645
Loss in iteration 152 : 0.8088903573451859
Loss in iteration 153 : 0.8080743205162539
Loss in iteration 154 : 0.8072589486556301
Loss in iteration 155 : 0.8064441979757814
Loss in iteration 156 : 0.8056308107277044
Loss in iteration 157 : 0.8048177328171751
Loss in iteration 158 : 0.8040047735777457
Loss in iteration 159 : 0.8031919329406002
Loss in iteration 160 : 0.8023793812727813
Loss in iteration 161 : 0.8015673755778623
Loss in iteration 162 : 0.8007554879654858
Loss in iteration 163 : 0.7999437183676125
Loss in iteration 164 : 0.7991322950624145
Loss in iteration 165 : 0.798321280912575
Loss in iteration 166 : 0.7975107380299219
Loss in iteration 167 : 0.7967015886249187
Loss in iteration 168 : 0.7958925564098694
Loss in iteration 169 : 0.7950836413175646
Loss in iteration 170 : 0.7942748432808688
Loss in iteration 171 : 0.7934661622327144
Loss in iteration 172 : 0.7926577851168028
Loss in iteration 173 : 0.7918499926303314
Loss in iteration 174 : 0.7910424062121517
Loss in iteration 175 : 0.7902357889461351
Loss in iteration 176 : 0.789431056142391
Loss in iteration 177 : 0.7886280697436675
Loss in iteration 178 : 0.7878260804117786
Loss in iteration 179 : 0.7870272504204954
Loss in iteration 180 : 0.7862285354703709
Loss in iteration 181 : 0.7854306300971641
Loss in iteration 182 : 0.7846336037422311
Loss in iteration 183 : 0.7838374831487245
Loss in iteration 184 : 0.7830445529420863
Loss in iteration 185 : 0.7822517358335935
Loss in iteration 186 : 0.7814590317596884
Loss in iteration 187 : 0.780666440656891
Loss in iteration 188 : 0.7798739624617795
Loss in iteration 189 : 0.7790817422694163
Loss in iteration 190 : 0.7782918853468159
Loss in iteration 191 : 0.7775039102546855
Loss in iteration 192 : 0.7767164013155674
Loss in iteration 193 : 0.7759302121381658
Loss in iteration 194 : 0.7751452074689997
Loss in iteration 195 : 0.7743631784826368
Loss in iteration 196 : 0.7735822210890038
Loss in iteration 197 : 0.7728023138400814
Loss in iteration 198 : 0.7720237270110838
Loss in iteration 199 : 0.7712460530660973
Loss in iteration 200 : 0.7704710879097365
Testing accuracy  of updater 3 on alg 1 with rate 0.09999999999999998 = 0.6835, training accuracy 0.69925, time elapsed: 2347 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.595, training accuracy 0.6015, time elapsed: 2715 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.595, training accuracy 0.6015, time elapsed: 2830 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.595, training accuracy 0.6015, time elapsed: 2433 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.595, training accuracy 0.6015, time elapsed: 2390 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 70.0 = 0.595, training accuracy 0.6015, time elapsed: 2440 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 40.0 = 0.595, training accuracy 0.6015, time elapsed: 2924 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.595, training accuracy 0.6015, time elapsed: 3993 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 6.879454003846271
Loss in iteration 3 : 8.329824449921343
Loss in iteration 4 : 4.0609660797562865
Loss in iteration 5 : 4.461488505610107
Loss in iteration 6 : 3.094821482494691
Loss in iteration 7 : 3.287080609232515
Loss in iteration 8 : 2.841539655567315
Loss in iteration 9 : 2.807302119303015
Loss in iteration 10 : 2.6458467911980974
Loss in iteration 11 : 2.6609314398582247
Loss in iteration 12 : 2.469272766660651
Loss in iteration 13 : 2.416257784154345
Loss in iteration 14 : 2.235106404339777
Loss in iteration 15 : 2.308528384755071
Loss in iteration 16 : 2.1061255292010026
Loss in iteration 17 : 2.2408029668214566
Loss in iteration 18 : 2.032757710042035
Loss in iteration 19 : 2.2134209529976943
Loss in iteration 20 : 2.012253049093152
Loss in iteration 21 : 2.17967643237966
Loss in iteration 22 : 1.9596576192408282
Loss in iteration 23 : 2.1820663604087907
Loss in iteration 24 : 1.8186907512638433
Loss in iteration 25 : 2.1039398626712678
Loss in iteration 26 : 1.6656517365046095
Loss in iteration 27 : 2.1250749963609192
Loss in iteration 28 : 1.9141529471235095
Loss in iteration 29 : 2.2499038571107457
Loss in iteration 30 : 1.9786981982456227
Loss in iteration 31 : 2.1955075189726223
Loss in iteration 32 : 1.9019530011586008
Loss in iteration 33 : 2.1253340342161247
Loss in iteration 34 : 1.8473204520802446
Loss in iteration 35 : 2.0853222421044317
Loss in iteration 36 : 1.8153091597199082
Loss in iteration 37 : 2.0906086506652577
Loss in iteration 38 : 1.812306932284347
Loss in iteration 39 : 2.0810360683903038
Loss in iteration 40 : 1.792122003172029
Loss in iteration 41 : 2.0798227727538983
Loss in iteration 42 : 1.7888341250111848
Loss in iteration 43 : 2.1009763288064147
Loss in iteration 44 : 1.7327463337790634
Loss in iteration 45 : 2.045233222639319
Loss in iteration 46 : 1.6179877940561809
Loss in iteration 47 : 2.0244670647745693
Loss in iteration 48 : 1.8290224070576946
Loss in iteration 49 : 2.191004927802621
Loss in iteration 50 : 1.8828288691292439
Loss in iteration 51 : 2.1311482885836686
Loss in iteration 52 : 1.8189649802662298
Loss in iteration 53 : 2.0778438905592864
Loss in iteration 54 : 1.7866858656125792
Loss in iteration 55 : 2.051518587083206
Loss in iteration 56 : 1.771697336183376
Loss in iteration 57 : 2.0512684022697036
Loss in iteration 58 : 1.7593701411500562
Loss in iteration 59 : 2.0520221421524405
Loss in iteration 60 : 1.7224616985224128
Loss in iteration 61 : 2.0601317805144364
Loss in iteration 62 : 1.6790771757884095
Loss in iteration 63 : 2.0905284989866484
Loss in iteration 64 : 1.6029854903500431
Loss in iteration 65 : 1.974799277425675
Loss in iteration 66 : 1.6161999964689704
Loss in iteration 67 : 2.08189385300403
Loss in iteration 68 : 1.825341704602333
Loss in iteration 69 : 2.2112952746459262
Loss in iteration 70 : 1.8726909936596459
Loss in iteration 71 : 2.137768827706612
Loss in iteration 72 : 1.7951124270061694
Loss in iteration 73 : 2.0695779475885736
Loss in iteration 74 : 1.76128481062225
Loss in iteration 75 : 2.0402129426124422
Loss in iteration 76 : 1.7413625208845676
Loss in iteration 77 : 2.031029289177844
Loss in iteration 78 : 1.7171864546054256
Loss in iteration 79 : 2.03031368400632
Loss in iteration 80 : 1.6924041897937885
Loss in iteration 81 : 2.0528471512332946
Loss in iteration 82 : 1.591445426370818
Loss in iteration 83 : 1.9518394127843934
Loss in iteration 84 : 1.5829112044422209
Loss in iteration 85 : 2.048705094399923
Loss in iteration 86 : 1.8358099623236817
Loss in iteration 87 : 2.2003122279483924
Loss in iteration 88 : 1.8512322937792554
Loss in iteration 89 : 2.1299006365556843
Loss in iteration 90 : 1.7772575109721611
Loss in iteration 91 : 2.0400589677237035
Loss in iteration 92 : 1.734184850516906
Loss in iteration 93 : 2.014680251601767
Loss in iteration 94 : 1.718619444841434
Loss in iteration 95 : 2.0192026338534235
Loss in iteration 96 : 1.7202839915031352
Loss in iteration 97 : 2.0238453267229306
Loss in iteration 98 : 1.6795235797447654
Loss in iteration 99 : 2.0420335723186334
Loss in iteration 100 : 1.5951971157954927
Loss in iteration 101 : 1.9601697259988695
Loss in iteration 102 : 1.6384746635001757
Loss in iteration 103 : 2.0598175996704975
Loss in iteration 104 : 1.8019023500823963
Loss in iteration 105 : 2.136634318710674
Loss in iteration 106 : 1.8112859343498944
Loss in iteration 107 : 2.091237683473683
Loss in iteration 108 : 1.7666479386429288
Loss in iteration 109 : 2.03208559092943
Loss in iteration 110 : 1.7310188964737534
Loss in iteration 111 : 1.9954314250839211
Loss in iteration 112 : 1.7172034503680953
Loss in iteration 113 : 1.997292719772152
Loss in iteration 114 : 1.7092586897943083
Loss in iteration 115 : 2.055896805240879
Loss in iteration 116 : 1.5692587681424406
Loss in iteration 117 : 1.9060193807355694
Loss in iteration 118 : 1.6015391163331048
Loss in iteration 119 : 2.018354550093148
Loss in iteration 120 : 1.8274838640397661
Loss in iteration 121 : 2.1518181982748303
Loss in iteration 122 : 1.8226386358852096
Loss in iteration 123 : 2.0846452175059804
Loss in iteration 124 : 1.7595129220990555
Loss in iteration 125 : 2.020018170534568
Loss in iteration 126 : 1.7420306804522867
Loss in iteration 127 : 1.9943010048120309
Loss in iteration 128 : 1.706431478438869
Loss in iteration 129 : 1.9756163364040595
Loss in iteration 130 : 1.6896790505098307
Loss in iteration 131 : 2.029866850097306
Loss in iteration 132 : 1.6400231460548897
Loss in iteration 133 : 1.9575277801900957
Loss in iteration 134 : 1.5244645992726804
Loss in iteration 135 : 1.9357876481833518
Loss in iteration 136 : 1.779631443243865
Loss in iteration 137 : 2.1419235235646035
Loss in iteration 138 : 1.845405512233741
Loss in iteration 139 : 2.1035907801706486
Loss in iteration 140 : 1.780387401072428
Loss in iteration 141 : 2.0372581938940617
Loss in iteration 142 : 1.7380898335193247
Loss in iteration 143 : 1.9639881873510927
Loss in iteration 144 : 1.727147903241778
Loss in iteration 145 : 1.9799073072163051
Loss in iteration 146 : 1.6961851367330427
Loss in iteration 147 : 1.9657039646385648
Loss in iteration 148 : 1.6842808197961088
Loss in iteration 149 : 2.012002676297542
Loss in iteration 150 : 1.6138951063288687
Loss in iteration 151 : 1.9633597962992113
Loss in iteration 152 : 1.5448695126375795
Loss in iteration 153 : 1.9327048217982792
Loss in iteration 154 : 1.7963446635866611
Loss in iteration 155 : 2.1421034140620914
Loss in iteration 156 : 1.847341906001232
Loss in iteration 157 : 2.0774999853945313
Loss in iteration 158 : 1.769462487166298
Loss in iteration 159 : 1.9989165947876986
Loss in iteration 160 : 1.7128330667183618
Loss in iteration 161 : 1.9651132547118915
Loss in iteration 162 : 1.7265204281090738
Loss in iteration 163 : 1.9863454814859218
Loss in iteration 164 : 1.6966753658964053
Loss in iteration 165 : 1.9554596553908143
Loss in iteration 166 : 1.7165324643016242
Loss in iteration 167 : 1.98980964418499
Loss in iteration 168 : 1.665255563615422
Loss in iteration 169 : 1.9666290288548096
Loss in iteration 170 : 1.6073870368957008
Loss in iteration 171 : 1.9373236794233109
Loss in iteration 172 : 1.7048385842440097
Loss in iteration 173 : 2.0576511938688355
Loss in iteration 174 : 1.8178168745008725
Loss in iteration 175 : 2.073904323828049
Loss in iteration 176 : 1.7790191736154586
Loss in iteration 177 : 1.9979470175251524
Loss in iteration 178 : 1.717955151177132
Loss in iteration 179 : 1.9690208172525931
Loss in iteration 180 : 1.716506825580058
Loss in iteration 181 : 1.9702597934838915
Loss in iteration 182 : 1.6753206457391248
Loss in iteration 183 : 1.9796934523371024
Loss in iteration 184 : 1.6173218676678216
Loss in iteration 185 : 1.9725462029680736
Loss in iteration 186 : 1.595616019736232
Loss in iteration 187 : 1.9342115932623225
Loss in iteration 188 : 1.6957349894265208
Loss in iteration 189 : 2.0574899525139236
Loss in iteration 190 : 1.8182815099097593
Loss in iteration 191 : 2.0776861775441566
Loss in iteration 192 : 1.821746888022227
Loss in iteration 193 : 2.0160453020576146
Loss in iteration 194 : 1.7424330036314324
Loss in iteration 195 : 1.9441118878286396
Loss in iteration 196 : 1.7415762578519507
Loss in iteration 197 : 1.972571257166496
Loss in iteration 198 : 1.6816437688549557
Loss in iteration 199 : 1.9106305653232407
Loss in iteration 200 : 1.6821071703695634
Testing accuracy  of updater 5 on alg 1 with rate 1.0 = 0.7315, training accuracy 0.721, time elapsed: 2898 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.7704123749332288
Loss in iteration 3 : 5.280196907219424
Loss in iteration 4 : 2.2346145375493123
Loss in iteration 5 : 3.50928834370268
Loss in iteration 6 : 2.421790898600075
Loss in iteration 7 : 2.621847680119472
Loss in iteration 8 : 2.5172634616530964
Loss in iteration 9 : 2.0524032811288744
Loss in iteration 10 : 2.67381801421561
Loss in iteration 11 : 1.5315315447350353
Loss in iteration 12 : 2.63104805049718
Loss in iteration 13 : 1.3574877885868717
Loss in iteration 14 : 2.467643531027565
Loss in iteration 15 : 1.3782623453873406
Loss in iteration 16 : 2.22608173865277
Loss in iteration 17 : 1.5534414507639858
Loss in iteration 18 : 1.8672188173144437
Loss in iteration 19 : 1.333424593257082
Loss in iteration 20 : 1.842547430218896
Loss in iteration 21 : 1.7790568277906067
Loss in iteration 22 : 1.9920666229815949
Loss in iteration 23 : 1.5671294811318048
Loss in iteration 24 : 1.8445795933364866
Loss in iteration 25 : 1.5693360691270246
Loss in iteration 26 : 1.734992568513319
Loss in iteration 27 : 1.566157400204906
Loss in iteration 28 : 1.6337107275294527
Loss in iteration 29 : 1.569890385248118
Loss in iteration 30 : 1.5481515646098452
Loss in iteration 31 : 1.5673185625742672
Loss in iteration 32 : 1.4939395001585203
Loss in iteration 33 : 1.5722783069998636
Loss in iteration 34 : 1.44958053228988
Loss in iteration 35 : 1.5720636065094953
Loss in iteration 36 : 1.4265001454675432
Loss in iteration 37 : 1.5868613282014703
Loss in iteration 38 : 1.3759816267071614
Loss in iteration 39 : 1.5088108459238663
Loss in iteration 40 : 1.270978636921335
Loss in iteration 41 : 1.5712210950027212
Loss in iteration 42 : 1.4437996075443758
Loss in iteration 43 : 1.668724608868753
Loss in iteration 44 : 1.4745500529890088
Loss in iteration 45 : 1.6105771689661366
Loss in iteration 46 : 1.4217118639546107
Loss in iteration 47 : 1.587017518411507
Loss in iteration 48 : 1.3821063687490394
Loss in iteration 49 : 1.5669558821140646
Loss in iteration 50 : 1.370991426652903
Loss in iteration 51 : 1.5602040723864912
Loss in iteration 52 : 1.358092060787484
Loss in iteration 53 : 1.556042100057607
Loss in iteration 54 : 1.3458760873797526
Loss in iteration 55 : 1.5528201895628424
Loss in iteration 56 : 1.3246227263341652
Loss in iteration 57 : 1.5624562090461518
Loss in iteration 58 : 1.3297443630059305
Loss in iteration 59 : 1.5832485427114826
Loss in iteration 60 : 1.2256520000142166
Loss in iteration 61 : 1.5158222791373768
Loss in iteration 62 : 1.2552785310780086
Loss in iteration 63 : 1.609122668362179
Loss in iteration 64 : 1.4153224073130724
Loss in iteration 65 : 1.6342722433462018
Loss in iteration 66 : 1.369683194445001
Loss in iteration 67 : 1.5789454004758456
Loss in iteration 68 : 1.3452602141843144
Loss in iteration 69 : 1.5652264511357994
Loss in iteration 70 : 1.331746624628267
Loss in iteration 71 : 1.547507907783614
Loss in iteration 72 : 1.3183585635206478
Loss in iteration 73 : 1.5500388406805623
Loss in iteration 74 : 1.3177568278126315
Loss in iteration 75 : 1.5404973992520676
Loss in iteration 76 : 1.3150631153353967
Loss in iteration 77 : 1.5542453884124143
Loss in iteration 78 : 1.2904641573185325
Loss in iteration 79 : 1.5711637323082959
Loss in iteration 80 : 1.1859240785654426
Loss in iteration 81 : 1.4910812969612985
Loss in iteration 82 : 1.2490964553045
Loss in iteration 83 : 1.5902220392182331
Loss in iteration 84 : 1.3931342899866967
Loss in iteration 85 : 1.6244271827956698
Loss in iteration 86 : 1.3441268525969117
Loss in iteration 87 : 1.5666486120107046
Loss in iteration 88 : 1.3387326780140638
Loss in iteration 89 : 1.551452332464432
Loss in iteration 90 : 1.3090478415519808
Loss in iteration 91 : 1.5364752183378152
Loss in iteration 92 : 1.2985733521494744
Loss in iteration 93 : 1.536216586293419
Loss in iteration 94 : 1.2914731209211783
Loss in iteration 95 : 1.5463773292508853
Loss in iteration 96 : 1.250654717761423
Loss in iteration 97 : 1.5438004704829913
Loss in iteration 98 : 1.1722307707791182
Loss in iteration 99 : 1.5036537336969755
Loss in iteration 100 : 1.2846952704485226
Loss in iteration 101 : 1.6066437544389054
Loss in iteration 102 : 1.3902751579429682
Loss in iteration 103 : 1.6039845755777082
Loss in iteration 104 : 1.3382845806602943
Loss in iteration 105 : 1.5537531502174438
Loss in iteration 106 : 1.3247240283049904
Loss in iteration 107 : 1.527736279236688
Loss in iteration 108 : 1.3073074764243102
Loss in iteration 109 : 1.5229832135234165
Loss in iteration 110 : 1.2907330103434151
Loss in iteration 111 : 1.5076939996575285
Loss in iteration 112 : 1.28105166474106
Loss in iteration 113 : 1.5397111923831834
Loss in iteration 114 : 1.238001911096221
Loss in iteration 115 : 1.5356300034929686
Loss in iteration 116 : 1.1468733333346772
Loss in iteration 117 : 1.4657769523145934
Loss in iteration 118 : 1.3294453617797266
Loss in iteration 119 : 1.6266718475769313
Loss in iteration 120 : 1.388724352918483
Loss in iteration 121 : 1.5938460438562296
Loss in iteration 122 : 1.3387335263969118
Loss in iteration 123 : 1.536947119326436
Loss in iteration 124 : 1.3186148952330272
Loss in iteration 125 : 1.520353116197162
Loss in iteration 126 : 1.2979205723704612
Loss in iteration 127 : 1.5009034560096275
Loss in iteration 128 : 1.2872438824906292
Loss in iteration 129 : 1.5004403562321718
Loss in iteration 130 : 1.2660939768150927
Loss in iteration 131 : 1.487514063534711
Loss in iteration 132 : 1.277006688564431
Loss in iteration 133 : 1.5357193120046857
Loss in iteration 134 : 1.2554533768160954
Loss in iteration 135 : 1.5402393294821646
Loss in iteration 136 : 1.1669842886827162
Loss in iteration 137 : 1.4674662826984586
Loss in iteration 138 : 1.305242179902034
Loss in iteration 139 : 1.6119843088367785
Loss in iteration 140 : 1.3889838001838402
Loss in iteration 141 : 1.5854209081226518
Loss in iteration 142 : 1.339840017562247
Loss in iteration 143 : 1.529991566136459
Loss in iteration 144 : 1.3088486572127664
Loss in iteration 145 : 1.5040423428036638
Loss in iteration 146 : 1.2921347097414546
Loss in iteration 147 : 1.5031463042081756
Loss in iteration 148 : 1.2892512032765369
Loss in iteration 149 : 1.4958256705556519
Loss in iteration 150 : 1.2913850826747044
Loss in iteration 151 : 1.5091855543745583
Loss in iteration 152 : 1.2792485384783685
Loss in iteration 153 : 1.5171780535457655
Loss in iteration 154 : 1.2230502019256146
Loss in iteration 155 : 1.4649795507723025
Loss in iteration 156 : 1.2443685225099088
Loss in iteration 157 : 1.5245327211847113
Loss in iteration 158 : 1.3616649898107585
Loss in iteration 159 : 1.563116480152361
Loss in iteration 160 : 1.348423276020987
Loss in iteration 161 : 1.5262547183953656
Loss in iteration 162 : 1.3132619623685498
Loss in iteration 163 : 1.498070833797657
Loss in iteration 164 : 1.2972038476624626
Loss in iteration 165 : 1.4909731036051481
Loss in iteration 166 : 1.2852844928374656
Loss in iteration 167 : 1.4872123604269587
Loss in iteration 168 : 1.2652416279914935
Loss in iteration 169 : 1.4931013029578595
Loss in iteration 170 : 1.2543141652455059
Loss in iteration 171 : 1.5260777898072582
Loss in iteration 172 : 1.227508824884164
Loss in iteration 173 : 1.4612878292472784
Loss in iteration 174 : 1.2468758748736621
Loss in iteration 175 : 1.533809259915777
Loss in iteration 176 : 1.3716405901200213
Loss in iteration 177 : 1.5928922818928963
Loss in iteration 178 : 1.3661825762596373
Loss in iteration 179 : 1.5295422588835181
Loss in iteration 180 : 1.319736907885696
Loss in iteration 181 : 1.485943809646531
Loss in iteration 182 : 1.2937038801417362
Loss in iteration 183 : 1.479585834927831
Loss in iteration 184 : 1.289286273490613
Loss in iteration 185 : 1.489537532460547
Loss in iteration 186 : 1.3000207077056103
Loss in iteration 187 : 1.4946272903389568
Loss in iteration 188 : 1.2572615424640907
Loss in iteration 189 : 1.482198097499803
Loss in iteration 190 : 1.225803318321893
Loss in iteration 191 : 1.4692228638464948
Loss in iteration 192 : 1.2704936177299009
Loss in iteration 193 : 1.5240806333359307
Loss in iteration 194 : 1.3633346672608926
Loss in iteration 195 : 1.5390103975183742
Loss in iteration 196 : 1.3413024797670046
Loss in iteration 197 : 1.5060703894928242
Loss in iteration 198 : 1.308442672502903
Loss in iteration 199 : 1.4832476898075901
Loss in iteration 200 : 1.2910123768465156
Testing accuracy  of updater 5 on alg 1 with rate 0.7000000000000001 = 0.735, training accuracy 0.7235, time elapsed: 3001 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.2540073881384683
Loss in iteration 3 : 2.9813493400603677
Loss in iteration 4 : 1.729086469259084
Loss in iteration 5 : 1.977110325465311
Loss in iteration 6 : 1.7871828534816971
Loss in iteration 7 : 1.5154949263001591
Loss in iteration 8 : 1.819967881347392
Loss in iteration 9 : 1.2045766019400779
Loss in iteration 10 : 1.8722270198706041
Loss in iteration 11 : 0.9611162882864809
Loss in iteration 12 : 1.7900011154773396
Loss in iteration 13 : 0.9022892983437611
Loss in iteration 14 : 1.6606775495809871
Loss in iteration 15 : 0.9637828892170311
Loss in iteration 16 : 1.5593716653701621
Loss in iteration 17 : 0.9878222592084515
Loss in iteration 18 : 1.4678018590775876
Loss in iteration 19 : 1.0326621447621254
Loss in iteration 20 : 1.2907460228249716
Loss in iteration 21 : 0.9358178681028393
Loss in iteration 22 : 1.251050048842123
Loss in iteration 23 : 1.1384165190222548
Loss in iteration 24 : 1.346866612413439
Loss in iteration 25 : 1.0405812867667639
Loss in iteration 26 : 1.2654069043876075
Loss in iteration 27 : 1.051016578802284
Loss in iteration 28 : 1.2056895426449417
Loss in iteration 29 : 1.0478156749155518
Loss in iteration 30 : 1.1623826162164532
Loss in iteration 31 : 1.0434353054282555
Loss in iteration 32 : 1.1148314185226067
Loss in iteration 33 : 1.040618606712262
Loss in iteration 34 : 1.0772474882085974
Loss in iteration 35 : 1.0431729822152669
Loss in iteration 36 : 1.04917840209524
Loss in iteration 37 : 1.0453160971744258
Loss in iteration 38 : 1.0217769774076901
Loss in iteration 39 : 1.0552981498733434
Loss in iteration 40 : 0.9907666483759949
Loss in iteration 41 : 1.0692535250213882
Loss in iteration 42 : 0.9702408215558239
Loss in iteration 43 : 1.0340453929405913
Loss in iteration 44 : 0.969516458657496
Loss in iteration 45 : 1.0840394349754863
Loss in iteration 46 : 1.0301989540995113
Loss in iteration 47 : 1.0810642884458443
Loss in iteration 48 : 1.0254995692236228
Loss in iteration 49 : 1.0595528536790275
Loss in iteration 50 : 1.0075517290994593
Loss in iteration 51 : 1.046324892368766
Loss in iteration 52 : 0.9846787005709193
Loss in iteration 53 : 1.0349915423936675
Loss in iteration 54 : 0.9693576114775313
Loss in iteration 55 : 1.0361470698684108
Loss in iteration 56 : 0.9599157232577247
Loss in iteration 57 : 1.0378036706027944
Loss in iteration 58 : 0.9616387811607648
Loss in iteration 59 : 1.0250217752452861
Loss in iteration 60 : 0.9195294586534988
Loss in iteration 61 : 1.0259101509485284
Loss in iteration 62 : 0.9547280761403096
Loss in iteration 63 : 1.0696253085867347
Loss in iteration 64 : 0.996306497506594
Loss in iteration 65 : 1.0656896309042794
Loss in iteration 66 : 0.9637319490196312
Loss in iteration 67 : 1.0397332733055717
Loss in iteration 68 : 0.955212390018686
Loss in iteration 69 : 1.0349917048835686
Loss in iteration 70 : 0.9481955465242695
Loss in iteration 71 : 1.0365363067630238
Loss in iteration 72 : 0.9387005694699713
Loss in iteration 73 : 1.0328847525230105
Loss in iteration 74 : 0.9285459733440221
Loss in iteration 75 : 1.0383486300145068
Loss in iteration 76 : 0.900836118583774
Loss in iteration 77 : 1.0204462099917597
Loss in iteration 78 : 0.8868656461340406
Loss in iteration 79 : 1.0254768734767452
Loss in iteration 80 : 0.9466098597444227
Loss in iteration 81 : 1.0775033203574724
Loss in iteration 82 : 0.9663683560233246
Loss in iteration 83 : 1.0651553009470527
Loss in iteration 84 : 0.9563534703207432
Loss in iteration 85 : 1.0472102179852882
Loss in iteration 86 : 0.9381935617678283
Loss in iteration 87 : 1.0265965671274908
Loss in iteration 88 : 0.9258904407040708
Loss in iteration 89 : 1.0201515345559333
Loss in iteration 90 : 0.9163761499073219
Loss in iteration 91 : 1.0255357190806096
Loss in iteration 92 : 0.9168920473771965
Loss in iteration 93 : 1.029044034517671
Loss in iteration 94 : 0.910658952533445
Loss in iteration 95 : 1.031977990659919
Loss in iteration 96 : 0.8704295630753642
Loss in iteration 97 : 1.0117952543944024
Loss in iteration 98 : 0.9152765001146096
Loss in iteration 99 : 1.060493461164589
Loss in iteration 100 : 0.9574558112069785
Loss in iteration 101 : 1.0517370883011945
Loss in iteration 102 : 0.9335304718860092
Loss in iteration 103 : 1.0315472253807856
Loss in iteration 104 : 0.9232923517268485
Loss in iteration 105 : 1.03284033657465
Loss in iteration 106 : 0.917986154302032
Loss in iteration 107 : 1.029093005797722
Loss in iteration 108 : 0.9079073681957502
Loss in iteration 109 : 1.0254823856823372
Loss in iteration 110 : 0.9047131989820035
Loss in iteration 111 : 1.0311365212901964
Loss in iteration 112 : 0.8760023690450527
Loss in iteration 113 : 1.0150611968870311
Loss in iteration 114 : 0.8926623108780557
Loss in iteration 115 : 1.0387597193046927
Loss in iteration 116 : 0.939950783888995
Loss in iteration 117 : 1.0556138664130554
Loss in iteration 118 : 0.9361861975049022
Loss in iteration 119 : 1.034602156986323
Loss in iteration 120 : 0.9257600672897675
Loss in iteration 121 : 1.022877278048978
Loss in iteration 122 : 0.9067385069525054
Loss in iteration 123 : 1.016641764470022
Loss in iteration 124 : 0.905887099849868
Loss in iteration 125 : 1.0219022411625736
Loss in iteration 126 : 0.9043959730949462
Loss in iteration 127 : 1.0338750335763975
Loss in iteration 128 : 0.8769001352464099
Loss in iteration 129 : 1.0106452014585563
Loss in iteration 130 : 0.8862650156412255
Loss in iteration 131 : 1.0244261508057655
Loss in iteration 132 : 0.9307156159021589
Loss in iteration 133 : 1.0451304213540238
Loss in iteration 134 : 0.9352805263725072
Loss in iteration 135 : 1.0267069918365865
Loss in iteration 136 : 0.923755592199404
Loss in iteration 137 : 1.0190526713890993
Loss in iteration 138 : 0.91202141801019
Loss in iteration 139 : 1.0135114231171276
Loss in iteration 140 : 0.9069760903793846
Loss in iteration 141 : 1.0072739111968876
Loss in iteration 142 : 0.9145271475846025
Loss in iteration 143 : 1.0153989781656827
Loss in iteration 144 : 0.9026254494547923
Loss in iteration 145 : 1.0222885933622927
Loss in iteration 146 : 0.8986924712353231
Loss in iteration 147 : 1.0176193352759118
Loss in iteration 148 : 0.8651450966252942
Loss in iteration 149 : 0.9983385610737212
Loss in iteration 150 : 0.9149414504545302
Loss in iteration 151 : 1.0416820837482978
Loss in iteration 152 : 0.936870880223111
Loss in iteration 153 : 1.027119163339202
Loss in iteration 154 : 0.9241277431235676
Loss in iteration 155 : 1.0134895248456004
Loss in iteration 156 : 0.9154927091468276
Loss in iteration 157 : 1.0074861223629552
Loss in iteration 158 : 0.9124869959124753
Loss in iteration 159 : 1.0043061137580394
Loss in iteration 160 : 0.9076775826393542
Loss in iteration 161 : 1.0019445355684613
Loss in iteration 162 : 0.9050861407480789
Loss in iteration 163 : 1.0020200995154054
Loss in iteration 164 : 0.9034216700137122
Loss in iteration 165 : 1.0056433820230002
Loss in iteration 166 : 0.9031892437826483
Loss in iteration 167 : 1.0086104549036186
Loss in iteration 168 : 0.9034378186048126
Loss in iteration 169 : 1.0072742340967706
Loss in iteration 170 : 0.8996462709817131
Loss in iteration 171 : 1.005710039634199
Loss in iteration 172 : 0.8939805858611075
Loss in iteration 173 : 1.0009002199457933
Loss in iteration 174 : 0.9061532041835944
Loss in iteration 175 : 1.0120930509048047
Loss in iteration 176 : 0.9218056773682008
Loss in iteration 177 : 1.0197644810021054
Loss in iteration 178 : 0.9219848080592772
Loss in iteration 179 : 1.0164564956703235
Loss in iteration 180 : 0.8774374936547485
Loss in iteration 181 : 0.984222537641765
Loss in iteration 182 : 0.8818726500774662
Loss in iteration 183 : 1.0047011755709392
Loss in iteration 184 : 0.9235098914317749
Loss in iteration 185 : 1.0253856178510028
Loss in iteration 186 : 0.9388987956170761
Loss in iteration 187 : 1.0184199487238978
Loss in iteration 188 : 0.9290634155157971
Loss in iteration 189 : 0.9984123708632676
Loss in iteration 190 : 0.9116121902419239
Loss in iteration 191 : 0.9939378269640975
Loss in iteration 192 : 0.9097030717635342
Loss in iteration 193 : 0.9941190885477874
Loss in iteration 194 : 0.9058303270562057
Loss in iteration 195 : 1.011687991147724
Loss in iteration 196 : 0.8747807465371041
Loss in iteration 197 : 0.9921147804407615
Loss in iteration 198 : 0.8652466929027042
Loss in iteration 199 : 0.9890616690993727
Loss in iteration 200 : 0.8969212575421764
Testing accuracy  of updater 5 on alg 1 with rate 0.4 = 0.7325, training accuracy 0.72625, time elapsed: 3671 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9137638237278501
Loss in iteration 3 : 0.8877394959915709
Loss in iteration 4 : 0.8622917503424748
Loss in iteration 5 : 0.8363931191844058
Loss in iteration 6 : 0.809836246086531
Loss in iteration 7 : 0.7826448957149155
Loss in iteration 8 : 0.75580046512493
Loss in iteration 9 : 0.7299011010174961
Loss in iteration 10 : 0.7053585485753351
Loss in iteration 11 : 0.684702175377232
Loss in iteration 12 : 0.6728358122316788
Loss in iteration 13 : 0.7080312856749104
Loss in iteration 14 : 0.7633867637807911
Loss in iteration 15 : 0.9530912095343012
Loss in iteration 16 : 0.6880761875389813
Loss in iteration 17 : 0.7485742655278079
Loss in iteration 18 : 0.669329650215152
Loss in iteration 19 : 0.7113159202901345
Loss in iteration 20 : 0.6512675616441085
Loss in iteration 21 : 0.6838900004344005
Loss in iteration 22 : 0.6442826705318297
Loss in iteration 23 : 0.6774031654973739
Loss in iteration 24 : 0.644629230557175
Loss in iteration 25 : 0.6795454374516974
Loss in iteration 26 : 0.6411611077887183
Loss in iteration 27 : 0.6697735441945404
Loss in iteration 28 : 0.6334365111692675
Loss in iteration 29 : 0.6573965078781748
Loss in iteration 30 : 0.6288415511323775
Loss in iteration 31 : 0.6495845824666249
Loss in iteration 32 : 0.6247672885011202
Loss in iteration 33 : 0.6415043568670995
Loss in iteration 34 : 0.6178520805335511
Loss in iteration 35 : 0.6334649704344922
Loss in iteration 36 : 0.6159658121471836
Loss in iteration 37 : 0.6316067261017179
Loss in iteration 38 : 0.6171590823401131
Loss in iteration 39 : 0.6325515232898294
Loss in iteration 40 : 0.6176916877032619
Loss in iteration 41 : 0.6334524150646009
Loss in iteration 42 : 0.6141313897452982
Loss in iteration 43 : 0.6265529035091614
Loss in iteration 44 : 0.6097347638838657
Loss in iteration 45 : 0.6213972758443977
Loss in iteration 46 : 0.6065466373561597
Loss in iteration 47 : 0.6165458721597673
Loss in iteration 48 : 0.6031872486563318
Loss in iteration 49 : 0.6131122351853182
Loss in iteration 50 : 0.6009531444753224
Loss in iteration 51 : 0.6115031244666148
Loss in iteration 52 : 0.5993532267640644
Loss in iteration 53 : 0.6128775080475145
Loss in iteration 54 : 0.6012132659537379
Loss in iteration 55 : 0.6148840581136972
Loss in iteration 56 : 0.6019621390532955
Loss in iteration 57 : 0.6118294803033257
Loss in iteration 58 : 0.5990149401136666
Loss in iteration 59 : 0.6066388487897135
Loss in iteration 60 : 0.5946719916268013
Loss in iteration 61 : 0.6020726422322658
Loss in iteration 62 : 0.5915656413789157
Loss in iteration 63 : 0.5996357830140052
Loss in iteration 64 : 0.59074841678863
Loss in iteration 65 : 0.5951042726634155
Loss in iteration 66 : 0.5868120317163298
Loss in iteration 67 : 0.5951481291358839
Loss in iteration 68 : 0.590717314375068
Loss in iteration 69 : 0.6009907581834899
Loss in iteration 70 : 0.5941040338921034
Loss in iteration 71 : 0.6040200247194915
Loss in iteration 72 : 0.592421038322804
Loss in iteration 73 : 0.6005873566580503
Loss in iteration 74 : 0.5882462735540198
Loss in iteration 75 : 0.5961807998688354
Loss in iteration 76 : 0.5863515209976179
Loss in iteration 77 : 0.5968961908034743
Loss in iteration 78 : 0.5885111518717919
Loss in iteration 79 : 0.5943476663847103
Loss in iteration 80 : 0.5859402052306937
Loss in iteration 81 : 0.5933801603812525
Loss in iteration 82 : 0.5865218047474218
Loss in iteration 83 : 0.5948661827395877
Loss in iteration 84 : 0.587164104086825
Loss in iteration 85 : 0.5921490848327892
Loss in iteration 86 : 0.584055179082854
Loss in iteration 87 : 0.5910855820333063
Loss in iteration 88 : 0.5841312233183953
Loss in iteration 89 : 0.5908224580334885
Loss in iteration 90 : 0.584042116065378
Loss in iteration 91 : 0.5899677969515996
Loss in iteration 92 : 0.5816030375268231
Loss in iteration 93 : 0.5875771424973218
Loss in iteration 94 : 0.5799739318526747
Loss in iteration 95 : 0.5862187669975356
Loss in iteration 96 : 0.5810306327822652
Loss in iteration 97 : 0.5912125385058927
Loss in iteration 98 : 0.5823689335152541
Loss in iteration 99 : 0.5862956133587225
Loss in iteration 100 : 0.5798334568363548
Loss in iteration 101 : 0.5880293784955666
Loss in iteration 102 : 0.5798990421345742
Loss in iteration 103 : 0.5873114573698849
Loss in iteration 104 : 0.5785510520040072
Loss in iteration 105 : 0.5856165051899607
Loss in iteration 106 : 0.5785476038199779
Loss in iteration 107 : 0.5845091533777195
Loss in iteration 108 : 0.577008920363774
Loss in iteration 109 : 0.5837274278888379
Loss in iteration 110 : 0.5767462928729824
Loss in iteration 111 : 0.583316143467104
Loss in iteration 112 : 0.576744221923157
Loss in iteration 113 : 0.5836115390507722
Loss in iteration 114 : 0.5781103741936845
Loss in iteration 115 : 0.5854690935716592
Loss in iteration 116 : 0.5805606531209389
Loss in iteration 117 : 0.5829701453037719
Loss in iteration 118 : 0.5764594725979545
Loss in iteration 119 : 0.5815087003331579
Loss in iteration 120 : 0.5750718548513704
Loss in iteration 121 : 0.5795023167925257
Loss in iteration 122 : 0.5731154709031375
Loss in iteration 123 : 0.5771207410997576
Loss in iteration 124 : 0.5730471030133704
Loss in iteration 125 : 0.5779610724777968
Loss in iteration 126 : 0.5740809444630948
Loss in iteration 127 : 0.5782099949586836
Loss in iteration 128 : 0.5736888042595655
Loss in iteration 129 : 0.576857110087686
Loss in iteration 130 : 0.5729953632955267
Loss in iteration 131 : 0.5777957770625632
Loss in iteration 132 : 0.5744196784564884
Loss in iteration 133 : 0.5781560019371925
Loss in iteration 134 : 0.5750054222129525
Loss in iteration 135 : 0.579520030517243
Loss in iteration 136 : 0.5755603412679277
Loss in iteration 137 : 0.5780822630797768
Loss in iteration 138 : 0.574870090359298
Loss in iteration 139 : 0.5769042549025328
Loss in iteration 140 : 0.5722050764717026
Loss in iteration 141 : 0.5746847366833819
Loss in iteration 142 : 0.5716191090154111
Loss in iteration 143 : 0.5743809681145206
Loss in iteration 144 : 0.572030007457957
Loss in iteration 145 : 0.5747389071933761
Loss in iteration 146 : 0.5722287755184905
Loss in iteration 147 : 0.5745805786227561
Loss in iteration 148 : 0.5719298659868282
Loss in iteration 149 : 0.5741334948089757
Loss in iteration 150 : 0.5712698290923265
Loss in iteration 151 : 0.5736923184721159
Loss in iteration 152 : 0.5702903723133492
Loss in iteration 153 : 0.5722090124720118
Loss in iteration 154 : 0.5698683666776985
Loss in iteration 155 : 0.5732282238161774
Loss in iteration 156 : 0.5702213138805647
Loss in iteration 157 : 0.5731031090551512
Loss in iteration 158 : 0.5694155154990365
Loss in iteration 159 : 0.5725285005304906
Loss in iteration 160 : 0.5697393284807879
Loss in iteration 161 : 0.5718825027350504
Loss in iteration 162 : 0.5691751783063855
Loss in iteration 163 : 0.5719595374598598
Loss in iteration 164 : 0.5694001046645031
Loss in iteration 165 : 0.5719308481471187
Loss in iteration 166 : 0.5691936300747009
Loss in iteration 167 : 0.5709442859703608
Loss in iteration 168 : 0.568165550421322
Loss in iteration 169 : 0.5701613550339698
Loss in iteration 170 : 0.5682813850670955
Loss in iteration 171 : 0.5705883246904301
Loss in iteration 172 : 0.5686918552825303
Loss in iteration 173 : 0.5705859347054133
Loss in iteration 174 : 0.5678878266689514
Loss in iteration 175 : 0.5706898037130259
Loss in iteration 176 : 0.5671249124238998
Loss in iteration 177 : 0.5690779810154792
Loss in iteration 178 : 0.5661786762741843
Loss in iteration 179 : 0.5694137098882277
Loss in iteration 180 : 0.5677013810188871
Loss in iteration 181 : 0.5706976523206315
Loss in iteration 182 : 0.5681895421049472
Loss in iteration 183 : 0.5699399304317644
Loss in iteration 184 : 0.5667227878839417
Loss in iteration 185 : 0.56867064498479
Loss in iteration 186 : 0.5671416042503695
Loss in iteration 187 : 0.5672040884944445
Loss in iteration 188 : 0.5657752750479225
Loss in iteration 189 : 0.5678096346920434
Loss in iteration 190 : 0.5677582372806047
Loss in iteration 191 : 0.5699972264338171
Loss in iteration 192 : 0.5674643107247607
Loss in iteration 193 : 0.5685722684075918
Loss in iteration 194 : 0.5654763650767338
Loss in iteration 195 : 0.5673927640800963
Loss in iteration 196 : 0.5646768787580104
Loss in iteration 197 : 0.5687535720338189
Loss in iteration 198 : 0.5651096358610105
Loss in iteration 199 : 0.5674549236396067
Loss in iteration 200 : 0.5649071527675142
Testing accuracy  of updater 5 on alg 1 with rate 0.1 = 0.763, training accuracy 0.77025, time elapsed: 3082 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9141669358646614
Loss in iteration 3 : 0.8877385363421306
Loss in iteration 4 : 0.8622181201589355
Loss in iteration 5 : 0.8367013289556987
Loss in iteration 6 : 0.8108337706142281
Loss in iteration 7 : 0.7845455994183805
Loss in iteration 8 : 0.758584973686714
Loss in iteration 9 : 0.7339541758163022
Loss in iteration 10 : 0.7103023421550082
Loss in iteration 11 : 0.6887866888754827
Loss in iteration 12 : 0.6733695811931056
Loss in iteration 13 : 0.6685664039369894
Loss in iteration 14 : 0.7275709236538137
Loss in iteration 15 : 0.7575825592455049
Loss in iteration 16 : 0.8013305822988479
Loss in iteration 17 : 0.6407771356275762
Loss in iteration 18 : 0.644201583428568
Loss in iteration 19 : 0.6285564145487142
Loss in iteration 20 : 0.6353126173905528
Loss in iteration 21 : 0.6248056859100987
Loss in iteration 22 : 0.635083767857429
Loss in iteration 23 : 0.6242933405486756
Loss in iteration 24 : 0.6403121930222337
Loss in iteration 25 : 0.6292690363455449
Loss in iteration 26 : 0.6439351390581708
Loss in iteration 27 : 0.6227311647810957
Loss in iteration 28 : 0.6279515545317823
Loss in iteration 29 : 0.6116745889250196
Loss in iteration 30 : 0.6155924652649003
Loss in iteration 31 : 0.6061690548502705
Loss in iteration 32 : 0.609823058426887
Loss in iteration 33 : 0.6011776339597897
Loss in iteration 34 : 0.6038777769166134
Loss in iteration 35 : 0.597895624333116
Loss in iteration 36 : 0.6016227119491493
Loss in iteration 37 : 0.5985290011361267
Loss in iteration 38 : 0.6033193210470056
Loss in iteration 39 : 0.596518395883415
Loss in iteration 40 : 0.5996471224271399
Loss in iteration 41 : 0.5928964953881394
Loss in iteration 42 : 0.596131942221294
Loss in iteration 43 : 0.5891764274054917
Loss in iteration 44 : 0.5923781474879181
Loss in iteration 45 : 0.5865313310629913
Loss in iteration 46 : 0.5914025090551918
Loss in iteration 47 : 0.5849735459123205
Loss in iteration 48 : 0.5897102775631207
Loss in iteration 49 : 0.5824848098441828
Loss in iteration 50 : 0.5875712014268493
Loss in iteration 51 : 0.5820217825872301
Loss in iteration 52 : 0.5865513757630634
Loss in iteration 53 : 0.5795921346247126
Loss in iteration 54 : 0.581480573934228
Loss in iteration 55 : 0.5762975622198937
Loss in iteration 56 : 0.5791363887068265
Loss in iteration 57 : 0.5752055153600301
Loss in iteration 58 : 0.5780250098850142
Loss in iteration 59 : 0.5747206244111817
Loss in iteration 60 : 0.5770876350355486
Loss in iteration 61 : 0.5732924484003629
Loss in iteration 62 : 0.5760937997821094
Loss in iteration 63 : 0.5730723126586913
Loss in iteration 64 : 0.5759920717359185
Loss in iteration 65 : 0.5724859232624683
Loss in iteration 66 : 0.5746373633045961
Loss in iteration 67 : 0.5719289347018084
Loss in iteration 68 : 0.5736331844950332
Loss in iteration 69 : 0.5703872759064543
Loss in iteration 70 : 0.5728486123312436
Loss in iteration 71 : 0.5692269794774935
Loss in iteration 72 : 0.5706676110675383
Loss in iteration 73 : 0.5672187053137794
Loss in iteration 74 : 0.5693684809425709
Loss in iteration 75 : 0.5669152967907092
Loss in iteration 76 : 0.568853465647689
Loss in iteration 77 : 0.5659990357891079
Loss in iteration 78 : 0.5677318648856381
Loss in iteration 79 : 0.5642862094657471
Loss in iteration 80 : 0.5679944564364395
Loss in iteration 81 : 0.5637060518153832
Loss in iteration 82 : 0.5668494160510704
Loss in iteration 83 : 0.5622927153244579
Loss in iteration 84 : 0.5654269773211913
Loss in iteration 85 : 0.5621394510854876
Loss in iteration 86 : 0.5653699474253874
Loss in iteration 87 : 0.5630142000263979
Loss in iteration 88 : 0.564347212918256
Loss in iteration 89 : 0.5615481469773274
Loss in iteration 90 : 0.5628427087188752
Loss in iteration 91 : 0.5599725009868207
Loss in iteration 92 : 0.5615168757901001
Loss in iteration 93 : 0.5587385545065131
Loss in iteration 94 : 0.5611650770559443
Loss in iteration 95 : 0.5588991239515763
Loss in iteration 96 : 0.5621933198752425
Loss in iteration 97 : 0.558893172221072
Loss in iteration 98 : 0.5622424238112169
Loss in iteration 99 : 0.558124859021694
Loss in iteration 100 : 0.5610367852454542
Loss in iteration 101 : 0.5576293878370081
Loss in iteration 102 : 0.5593888043993546
Loss in iteration 103 : 0.5559766937025556
Loss in iteration 104 : 0.556277840237944
Loss in iteration 105 : 0.553929154930648
Loss in iteration 106 : 0.5546330723263792
Loss in iteration 107 : 0.5547830467991632
Loss in iteration 108 : 0.5566860539320125
Loss in iteration 109 : 0.555951012396184
Loss in iteration 110 : 0.5574845619299236
Loss in iteration 111 : 0.5556339109471917
Loss in iteration 112 : 0.5574668498349504
Loss in iteration 113 : 0.5557381071488958
Loss in iteration 114 : 0.5570155515419295
Loss in iteration 115 : 0.5547332278641239
Loss in iteration 116 : 0.5557343607899066
Loss in iteration 117 : 0.5540365104672967
Loss in iteration 118 : 0.5560119051932745
Loss in iteration 119 : 0.5529515088093435
Loss in iteration 120 : 0.5552460686853931
Loss in iteration 121 : 0.5516129881870195
Loss in iteration 122 : 0.5529422982118815
Loss in iteration 123 : 0.5506791888440266
Loss in iteration 124 : 0.5510429780695336
Loss in iteration 125 : 0.5502017910040272
Loss in iteration 126 : 0.5511903823903601
Loss in iteration 127 : 0.5508242485125637
Loss in iteration 128 : 0.5533823448084004
Loss in iteration 129 : 0.5512873752346628
Loss in iteration 130 : 0.5530093844630298
Loss in iteration 131 : 0.5505065451594805
Loss in iteration 132 : 0.5522439611917946
Loss in iteration 133 : 0.5494439182875671
Loss in iteration 134 : 0.551018863186213
Loss in iteration 135 : 0.5488991748815711
Loss in iteration 136 : 0.5496764547455534
Loss in iteration 137 : 0.5491952746514769
Loss in iteration 138 : 0.5490578526282818
Loss in iteration 139 : 0.5496088164614673
Loss in iteration 140 : 0.55022908271845
Loss in iteration 141 : 0.5475194168965875
Loss in iteration 142 : 0.5484141745272243
Loss in iteration 143 : 0.5484767347959637
Loss in iteration 144 : 0.5504826466440484
Loss in iteration 145 : 0.5485377742892986
Loss in iteration 146 : 0.5499235710589526
Loss in iteration 147 : 0.5475900510128605
Loss in iteration 148 : 0.5485733745709953
Loss in iteration 149 : 0.5464965336239116
Loss in iteration 150 : 0.5473910272112883
Loss in iteration 151 : 0.5452742079035798
Loss in iteration 152 : 0.5457870724605253
Loss in iteration 153 : 0.5463029188169105
Loss in iteration 154 : 0.5469573429128601
Loss in iteration 155 : 0.5454615961177335
Loss in iteration 156 : 0.544971810862146
Loss in iteration 157 : 0.5457253165875336
Loss in iteration 158 : 0.5476984412715369
Loss in iteration 159 : 0.5481431547764637
Loss in iteration 160 : 0.5516209622292906
Loss in iteration 161 : 0.5489665750400231
Loss in iteration 162 : 0.5501376539651792
Loss in iteration 163 : 0.5461148079342488
Loss in iteration 164 : 0.546975218558241
Loss in iteration 165 : 0.545873226766122
Loss in iteration 166 : 0.5468259225328722
Loss in iteration 167 : 0.5446803675244767
Loss in iteration 168 : 0.5437288609566984
Loss in iteration 169 : 0.5413787257961121
Loss in iteration 170 : 0.5417827609693889
Loss in iteration 171 : 0.5419997918986658
Loss in iteration 172 : 0.5436810417140919
Loss in iteration 173 : 0.5437949917592407
Loss in iteration 174 : 0.5456723840897361
Loss in iteration 175 : 0.5463621834716241
Loss in iteration 176 : 0.5477711721500618
Loss in iteration 177 : 0.5464124129132417
Loss in iteration 178 : 0.5478173682826817
Loss in iteration 179 : 0.5452185295811356
Loss in iteration 180 : 0.545206694949914
Loss in iteration 181 : 0.5427278673874175
Loss in iteration 182 : 0.5423218756200412
Loss in iteration 183 : 0.5420928418961722
Loss in iteration 184 : 0.5425251643184181
Loss in iteration 185 : 0.5422889635462325
Loss in iteration 186 : 0.5432727409114686
Loss in iteration 187 : 0.5434145483237374
Loss in iteration 188 : 0.5443170984313835
Loss in iteration 189 : 0.5438541733018917
Loss in iteration 190 : 0.5441357704687741
Loss in iteration 191 : 0.5443476276271954
Loss in iteration 192 : 0.5447562922667273
Loss in iteration 193 : 0.5440217472940463
Loss in iteration 194 : 0.5444234781543503
Loss in iteration 195 : 0.5433897686245377
Loss in iteration 196 : 0.5433394266284169
Loss in iteration 197 : 0.5418883272845153
Loss in iteration 198 : 0.54189271018151
Loss in iteration 199 : 0.5405447333825407
Loss in iteration 200 : 0.5405420453306963
Testing accuracy  of updater 5 on alg 1 with rate 0.07 = 0.7695, training accuracy 0.768875, time elapsed: 2289 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9266785288945044
Loss in iteration 3 : 0.905383492109994
Loss in iteration 4 : 0.8866819602475257
Loss in iteration 5 : 0.8672275654700173
Loss in iteration 6 : 0.8491616303497801
Loss in iteration 7 : 0.8310796552564945
Loss in iteration 8 : 0.8130201749673468
Loss in iteration 9 : 0.7948791269240613
Loss in iteration 10 : 0.7768397229994104
Loss in iteration 11 : 0.7593588485944296
Loss in iteration 12 : 0.7423930448114929
Loss in iteration 13 : 0.725697025779027
Loss in iteration 14 : 0.7094468392186041
Loss in iteration 15 : 0.6941385857933228
Loss in iteration 16 : 0.6800568441390552
Loss in iteration 17 : 0.6679917958907562
Loss in iteration 18 : 0.6601512949790426
Loss in iteration 19 : 0.6540197422481772
Loss in iteration 20 : 0.665809134929371
Loss in iteration 21 : 0.6582258996804834
Loss in iteration 22 : 0.6709632866821833
Loss in iteration 23 : 0.6335168298777591
Loss in iteration 24 : 0.6317909145389201
Loss in iteration 25 : 0.6210354956850165
Loss in iteration 26 : 0.6188808270409238
Loss in iteration 27 : 0.6130438528405354
Loss in iteration 28 : 0.6117135826313929
Loss in iteration 29 : 0.6052801937097331
Loss in iteration 30 : 0.6041223052633584
Loss in iteration 31 : 0.6007851011285904
Loss in iteration 32 : 0.6019024193763234
Loss in iteration 33 : 0.6008332007367501
Loss in iteration 34 : 0.6039183550710385
Loss in iteration 35 : 0.6003127728256953
Loss in iteration 36 : 0.6015634543754882
Loss in iteration 37 : 0.5948355642265717
Loss in iteration 38 : 0.5925713084083907
Loss in iteration 39 : 0.586481943700007
Loss in iteration 40 : 0.585045805561077
Loss in iteration 41 : 0.581288805107459
Loss in iteration 42 : 0.5802922101311331
Loss in iteration 43 : 0.5783416843242474
Loss in iteration 44 : 0.5784553214954427
Loss in iteration 45 : 0.5772189856564467
Loss in iteration 46 : 0.5781225913238278
Loss in iteration 47 : 0.5772013200410507
Loss in iteration 48 : 0.5782293679959534
Loss in iteration 49 : 0.5753992240540018
Loss in iteration 50 : 0.5756487356357332
Loss in iteration 51 : 0.5723671823048717
Loss in iteration 52 : 0.5723098126041959
Loss in iteration 53 : 0.5702240016578484
Loss in iteration 54 : 0.5702367829788972
Loss in iteration 55 : 0.5680451512949064
Loss in iteration 56 : 0.5677847585594955
Loss in iteration 57 : 0.5658792988592346
Loss in iteration 58 : 0.56557698083986
Loss in iteration 59 : 0.5638140525548897
Loss in iteration 60 : 0.5636530795372205
Loss in iteration 61 : 0.562858854478247
Loss in iteration 62 : 0.5632990468998611
Loss in iteration 63 : 0.562029348208204
Loss in iteration 64 : 0.5627899793193589
Loss in iteration 65 : 0.5611818152282253
Loss in iteration 66 : 0.5614506202702871
Loss in iteration 67 : 0.5593410677007499
Loss in iteration 68 : 0.5594977327812487
Loss in iteration 69 : 0.558123259119543
Loss in iteration 70 : 0.5581165067042525
Loss in iteration 71 : 0.5559605851847461
Loss in iteration 72 : 0.555553842733209
Loss in iteration 73 : 0.5541853266028587
Loss in iteration 74 : 0.5547433805606153
Loss in iteration 75 : 0.5541476129159014
Loss in iteration 76 : 0.5549941249811893
Loss in iteration 77 : 0.5546490327716178
Loss in iteration 78 : 0.5552456069240984
Loss in iteration 79 : 0.5535369518154843
Loss in iteration 80 : 0.5531923231737635
Loss in iteration 81 : 0.5516191112532791
Loss in iteration 82 : 0.5516156984624996
Loss in iteration 83 : 0.5505197247226888
Loss in iteration 84 : 0.5506296631033708
Loss in iteration 85 : 0.5496838947661836
Loss in iteration 86 : 0.550152283842985
Loss in iteration 87 : 0.5490429392975916
Loss in iteration 88 : 0.5491211611426066
Loss in iteration 89 : 0.54826392926464
Loss in iteration 90 : 0.5481100055820189
Loss in iteration 91 : 0.547563476846847
Loss in iteration 92 : 0.5475901305340191
Loss in iteration 93 : 0.5469133427211811
Loss in iteration 94 : 0.5470339610902172
Loss in iteration 95 : 0.5458167706169839
Loss in iteration 96 : 0.545511234763056
Loss in iteration 97 : 0.5449456409892326
Loss in iteration 98 : 0.545212370111027
Loss in iteration 99 : 0.5445933883199284
Loss in iteration 100 : 0.5450646328471005
Loss in iteration 101 : 0.5442153884432864
Loss in iteration 102 : 0.5450356260577806
Loss in iteration 103 : 0.5435899767921984
Loss in iteration 104 : 0.5438581058993474
Loss in iteration 105 : 0.5428736253285531
Loss in iteration 106 : 0.5432900086723235
Loss in iteration 107 : 0.5423876724814906
Loss in iteration 108 : 0.5428365947826715
Loss in iteration 109 : 0.5415232020141011
Loss in iteration 110 : 0.5416388707461616
Loss in iteration 111 : 0.540452231546378
Loss in iteration 112 : 0.5408208757725631
Loss in iteration 113 : 0.5401836093466227
Loss in iteration 114 : 0.5401940736007805
Loss in iteration 115 : 0.5399549519533346
Loss in iteration 116 : 0.5393490420768239
Loss in iteration 117 : 0.5392968039816918
Loss in iteration 118 : 0.5385334236379816
Loss in iteration 119 : 0.5387771686823195
Loss in iteration 120 : 0.5386894675549706
Loss in iteration 121 : 0.5386314672369469
Loss in iteration 122 : 0.5390860621587182
Loss in iteration 123 : 0.5383277154396867
Loss in iteration 124 : 0.5389425917688145
Loss in iteration 125 : 0.5376772036358679
Loss in iteration 126 : 0.5378126139860444
Loss in iteration 127 : 0.5370226979571273
Loss in iteration 128 : 0.5373353847777
Loss in iteration 129 : 0.5366021613963803
Loss in iteration 130 : 0.5365842814075472
Loss in iteration 131 : 0.5358093949111401
Loss in iteration 132 : 0.5357153915512798
Loss in iteration 133 : 0.535139502408433
Loss in iteration 134 : 0.5349763764642033
Loss in iteration 135 : 0.5347047835676567
Loss in iteration 136 : 0.5347647188486655
Loss in iteration 137 : 0.535208757782732
Loss in iteration 138 : 0.5352521792605376
Loss in iteration 139 : 0.5352708987753785
Loss in iteration 140 : 0.5349731945242625
Loss in iteration 141 : 0.5350272802686478
Loss in iteration 142 : 0.5346136859340646
Loss in iteration 143 : 0.5343416233083684
Loss in iteration 144 : 0.5337285988053038
Loss in iteration 145 : 0.5335643494003499
Loss in iteration 146 : 0.5329340704679207
Loss in iteration 147 : 0.5330475189062365
Loss in iteration 148 : 0.5327395106022371
Loss in iteration 149 : 0.5328676922754021
Loss in iteration 150 : 0.5326829913005722
Loss in iteration 151 : 0.5328450534378844
Loss in iteration 152 : 0.5325475788370361
Loss in iteration 153 : 0.5326644683486466
Loss in iteration 154 : 0.5324826679357907
Loss in iteration 155 : 0.5323910522070217
Loss in iteration 156 : 0.531976140352311
Loss in iteration 157 : 0.5316998633385572
Loss in iteration 158 : 0.5313715909438262
Loss in iteration 159 : 0.531451736641428
Loss in iteration 160 : 0.5314788876374211
Loss in iteration 161 : 0.5314504407010441
Loss in iteration 162 : 0.5312214375859903
Loss in iteration 163 : 0.5311242833263207
Loss in iteration 164 : 0.5310495559671641
Loss in iteration 165 : 0.5305850814820333
Loss in iteration 166 : 0.5306978261829639
Loss in iteration 167 : 0.5303180410489164
Loss in iteration 168 : 0.5305090507132759
Loss in iteration 169 : 0.530313970940826
Loss in iteration 170 : 0.5299878602257654
Loss in iteration 171 : 0.5296623863659423
Loss in iteration 172 : 0.5297937126118305
Loss in iteration 173 : 0.5295443535148813
Loss in iteration 174 : 0.5298480734151769
Loss in iteration 175 : 0.5293403117695231
Loss in iteration 176 : 0.5292889986646003
Loss in iteration 177 : 0.5295094868301107
Loss in iteration 178 : 0.5292283764553835
Loss in iteration 179 : 0.5288780178391214
Loss in iteration 180 : 0.5282867863026166
Loss in iteration 181 : 0.5283350624060472
Loss in iteration 182 : 0.5280585999449283
Loss in iteration 183 : 0.5283039863867939
Loss in iteration 184 : 0.5281068836964481
Loss in iteration 185 : 0.5283367170430263
Loss in iteration 186 : 0.5277886362988239
Loss in iteration 187 : 0.5285721108358794
Loss in iteration 188 : 0.52803725906093
Loss in iteration 189 : 0.5286896493522848
Loss in iteration 190 : 0.5281716862925259
Loss in iteration 191 : 0.5283521521455559
Loss in iteration 192 : 0.5274235296903239
Loss in iteration 193 : 0.5272285051115236
Loss in iteration 194 : 0.5266959574826101
Loss in iteration 195 : 0.5271271303719932
Loss in iteration 196 : 0.5263610502970154
Loss in iteration 197 : 0.5269664429920761
Loss in iteration 198 : 0.5266505004080225
Loss in iteration 199 : 0.5271442266258698
Loss in iteration 200 : 0.526852363549918
Testing accuracy  of updater 5 on alg 1 with rate 0.04000000000000001 = 0.7785, training accuracy 0.776, time elapsed: 2417 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9729313415099531
Loss in iteration 3 : 0.9505007947559566
Loss in iteration 4 : 0.9336652100207903
Loss in iteration 5 : 0.9227767332792007
Loss in iteration 6 : 0.9152939903304157
Loss in iteration 7 : 0.9090160997208298
Loss in iteration 8 : 0.9030497026101406
Loss in iteration 9 : 0.897184430603113
Loss in iteration 10 : 0.891385838777839
Loss in iteration 11 : 0.8856468433560404
Loss in iteration 12 : 0.8799309107109113
Loss in iteration 13 : 0.8742450667816265
Loss in iteration 14 : 0.8685784138991235
Loss in iteration 15 : 0.862951019905696
Loss in iteration 16 : 0.8573187806502923
Loss in iteration 17 : 0.8516910963054835
Loss in iteration 18 : 0.8460557911671969
Loss in iteration 19 : 0.8404139099405702
Loss in iteration 20 : 0.8347713020567965
Loss in iteration 21 : 0.8291406221980872
Loss in iteration 22 : 0.8234967754540342
Loss in iteration 23 : 0.8178386352456107
Loss in iteration 24 : 0.8121697752011822
Loss in iteration 25 : 0.8064985765832293
Loss in iteration 26 : 0.8008462108519966
Loss in iteration 27 : 0.795233747267134
Loss in iteration 28 : 0.7896564988842825
Loss in iteration 29 : 0.7841408547115748
Loss in iteration 30 : 0.778663805955439
Loss in iteration 31 : 0.7732152913251487
Loss in iteration 32 : 0.7678045556936195
Loss in iteration 33 : 0.7624391923265782
Loss in iteration 34 : 0.757117749753492
Loss in iteration 35 : 0.7518276465502165
Loss in iteration 36 : 0.7466226119284306
Loss in iteration 37 : 0.7414970054424455
Loss in iteration 38 : 0.7364089725262939
Loss in iteration 39 : 0.7313190561319259
Loss in iteration 40 : 0.7262440428894854
Loss in iteration 41 : 0.7211940062617678
Loss in iteration 42 : 0.7162003162507972
Loss in iteration 43 : 0.7112827099154825
Loss in iteration 44 : 0.7064621895063989
Loss in iteration 45 : 0.7016749814478261
Loss in iteration 46 : 0.6970355224317639
Loss in iteration 47 : 0.6924828298311291
Loss in iteration 48 : 0.6880776637656415
Loss in iteration 49 : 0.6837301675406181
Loss in iteration 50 : 0.679635502430645
Loss in iteration 51 : 0.6756285180635236
Loss in iteration 52 : 0.6716602227810812
Loss in iteration 53 : 0.6678387340540562
Loss in iteration 54 : 0.664303662100466
Loss in iteration 55 : 0.6610646611977958
Loss in iteration 56 : 0.6577051523617711
Loss in iteration 57 : 0.6546110260320837
Loss in iteration 58 : 0.6511613792819986
Loss in iteration 59 : 0.6480927941846324
Loss in iteration 60 : 0.6451354661208493
Loss in iteration 61 : 0.6423378692992285
Loss in iteration 62 : 0.6395989571543335
Loss in iteration 63 : 0.6369697468178653
Loss in iteration 64 : 0.6344297817749156
Loss in iteration 65 : 0.6319480924686008
Loss in iteration 66 : 0.6295007565778549
Loss in iteration 67 : 0.6271486094373433
Loss in iteration 68 : 0.6250052630198527
Loss in iteration 69 : 0.6229968742544179
Loss in iteration 70 : 0.621107762868829
Loss in iteration 71 : 0.6193853198328922
Loss in iteration 72 : 0.6171362385794225
Loss in iteration 73 : 0.6151042768462596
Loss in iteration 74 : 0.6130047792201558
Loss in iteration 75 : 0.6110728407591345
Loss in iteration 76 : 0.6091251340047701
Loss in iteration 77 : 0.607345677998612
Loss in iteration 78 : 0.6056028371599189
Loss in iteration 79 : 0.6039156234247435
Loss in iteration 80 : 0.6022705736307538
Loss in iteration 81 : 0.6006846045175567
Loss in iteration 82 : 0.5991571463300882
Loss in iteration 83 : 0.5976841992254669
Loss in iteration 84 : 0.5962793144567121
Loss in iteration 85 : 0.5952293084576138
Loss in iteration 86 : 0.5937010388044142
Loss in iteration 87 : 0.5924899105473194
Loss in iteration 88 : 0.590973274140525
Loss in iteration 89 : 0.5898493806632653
Loss in iteration 90 : 0.5882727646561388
Loss in iteration 91 : 0.5871463002789703
Loss in iteration 92 : 0.5856971946454452
Loss in iteration 93 : 0.5846010098721812
Loss in iteration 94 : 0.5833726531197863
Loss in iteration 95 : 0.5823209723360108
Loss in iteration 96 : 0.5811381047056922
Loss in iteration 97 : 0.5802317450007588
Loss in iteration 98 : 0.5789862186969151
Loss in iteration 99 : 0.5780978308442337
Loss in iteration 100 : 0.5770222614867971
Loss in iteration 101 : 0.5762031469932467
Loss in iteration 102 : 0.5751635107424871
Loss in iteration 103 : 0.5744358421053897
Loss in iteration 104 : 0.5733263767162102
Loss in iteration 105 : 0.5725418332665415
Loss in iteration 106 : 0.571619871888988
Loss in iteration 107 : 0.5708830866136502
Loss in iteration 108 : 0.5701247980299895
Loss in iteration 109 : 0.569484451654783
Loss in iteration 110 : 0.5686495741538297
Loss in iteration 111 : 0.5679348944921084
Loss in iteration 112 : 0.5670951231353111
Loss in iteration 113 : 0.5664796321949032
Loss in iteration 114 : 0.5656945258111368
Loss in iteration 115 : 0.5650657122155069
Loss in iteration 116 : 0.5644147924637802
Loss in iteration 117 : 0.563861114845887
Loss in iteration 118 : 0.5631275068746174
Loss in iteration 119 : 0.5626748226954755
Loss in iteration 120 : 0.5621075164460851
Loss in iteration 121 : 0.5613816336380401
Loss in iteration 122 : 0.5606865164164813
Loss in iteration 123 : 0.5600764062389101
Loss in iteration 124 : 0.5594899461944843
Loss in iteration 125 : 0.5589588926282584
Loss in iteration 126 : 0.558471241482004
Loss in iteration 127 : 0.5578971463906623
Loss in iteration 128 : 0.5575825981343839
Loss in iteration 129 : 0.5569488962475369
Loss in iteration 130 : 0.5566198138452368
Loss in iteration 131 : 0.5560674763988794
Loss in iteration 132 : 0.5556009906067475
Loss in iteration 133 : 0.555049862794028
Loss in iteration 134 : 0.5545859139599169
Loss in iteration 135 : 0.5540149838405233
Loss in iteration 136 : 0.5536095498049997
Loss in iteration 137 : 0.5529355232734516
Loss in iteration 138 : 0.5525135368768338
Loss in iteration 139 : 0.5520186962505891
Loss in iteration 140 : 0.5516186889301223
Loss in iteration 141 : 0.5510943936919582
Loss in iteration 142 : 0.5507916944312709
Loss in iteration 143 : 0.55028612796134
Loss in iteration 144 : 0.5499892324864962
Loss in iteration 145 : 0.5495607283561419
Loss in iteration 146 : 0.5493594974140258
Loss in iteration 147 : 0.5489480487095134
Loss in iteration 148 : 0.5488104111298634
Loss in iteration 149 : 0.5482727210638831
Loss in iteration 150 : 0.548013180147331
Loss in iteration 151 : 0.547413341585074
Loss in iteration 152 : 0.5470317293399024
Loss in iteration 153 : 0.5464591906009064
Loss in iteration 154 : 0.5461686518774591
Loss in iteration 155 : 0.5456610631873172
Loss in iteration 156 : 0.5452714133357974
Loss in iteration 157 : 0.5448604659125503
Loss in iteration 158 : 0.5445353621419922
Loss in iteration 159 : 0.5441582697718871
Loss in iteration 160 : 0.5439134910838151
Loss in iteration 161 : 0.5434987734737331
Loss in iteration 162 : 0.5434917294448232
Loss in iteration 163 : 0.5432884878193275
Loss in iteration 164 : 0.543456630458598
Loss in iteration 165 : 0.5430358901804462
Loss in iteration 166 : 0.5429543164002207
Loss in iteration 167 : 0.5421689248582263
Loss in iteration 168 : 0.5418956440913485
Loss in iteration 169 : 0.5411504318019481
Loss in iteration 170 : 0.5409204442991113
Loss in iteration 171 : 0.5404640766762043
Loss in iteration 172 : 0.5402401308445497
Loss in iteration 173 : 0.5398954800650164
Loss in iteration 174 : 0.5396962906017909
Loss in iteration 175 : 0.5393702673701609
Loss in iteration 176 : 0.5392052086442582
Loss in iteration 177 : 0.5389116243583221
Loss in iteration 178 : 0.5388125743094645
Loss in iteration 179 : 0.538481396705826
Loss in iteration 180 : 0.5385011811863376
Loss in iteration 181 : 0.5382660602016115
Loss in iteration 182 : 0.5383848194132498
Loss in iteration 183 : 0.5378174055601693
Loss in iteration 184 : 0.5376494227827106
Loss in iteration 185 : 0.5370210758290334
Loss in iteration 186 : 0.5367984606653
Loss in iteration 187 : 0.5364362939827478
Loss in iteration 188 : 0.5362321047175173
Loss in iteration 189 : 0.535971129011614
Loss in iteration 190 : 0.5357506770841646
Loss in iteration 191 : 0.5355807520945387
Loss in iteration 192 : 0.5354348450013947
Loss in iteration 193 : 0.5351248932810639
Loss in iteration 194 : 0.5350089824372904
Loss in iteration 195 : 0.5347105067836307
Loss in iteration 196 : 0.5345965880471834
Loss in iteration 197 : 0.5342787789136073
Loss in iteration 198 : 0.5341488795717748
Loss in iteration 199 : 0.5338398944631358
Loss in iteration 200 : 0.5336692808351677
Testing accuracy  of updater 5 on alg 1 with rate 0.009999999999999995 = 0.78, training accuracy 0.777875, time elapsed: 1963 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.460044302182312
Loss in iteration 3 : 2.2244825109156827
Loss in iteration 4 : 2.0088085444956767
Loss in iteration 5 : 1.234012072936522
Loss in iteration 6 : 1.3590801679122244
Loss in iteration 7 : 0.9807856666172464
Loss in iteration 8 : 1.466392703898998
Loss in iteration 9 : 1.1323154234982136
Loss in iteration 10 : 1.1628364404795293
Loss in iteration 11 : 1.2684883983739539
Loss in iteration 12 : 0.9119434604656899
Loss in iteration 13 : 0.9922206494129455
Loss in iteration 14 : 0.9980408901016481
Loss in iteration 15 : 0.8784500343724315
Loss in iteration 16 : 1.0564179973084709
Loss in iteration 17 : 0.9513930005350312
Loss in iteration 18 : 0.8689812880881195
Loss in iteration 19 : 0.9681013683195133
Loss in iteration 20 : 0.8596155148976407
Loss in iteration 21 : 0.8165591262843809
Loss in iteration 22 : 0.8346791088194923
Loss in iteration 23 : 0.7586721605025527
Loss in iteration 24 : 0.8305721900733606
Loss in iteration 25 : 0.8018700881761608
Loss in iteration 26 : 0.7611599883645147
Loss in iteration 27 : 0.7547368263687115
Loss in iteration 28 : 0.6626662050745735
Loss in iteration 29 : 0.6993242528426133
Loss in iteration 30 : 0.6783497624572121
Loss in iteration 31 : 0.6994341542406785
Loss in iteration 32 : 0.638747905945967
Loss in iteration 33 : 0.645862597020047
Loss in iteration 34 : 0.6219730364717657
Loss in iteration 35 : 0.6293912854012867
Loss in iteration 36 : 0.6017394520929934
Loss in iteration 37 : 0.5998166665795431
Loss in iteration 38 : 0.6115250355575771
Loss in iteration 39 : 0.5901698082327739
Loss in iteration 40 : 0.5797088419335801
Loss in iteration 41 : 0.5933429438777924
Loss in iteration 42 : 0.5947884889895049
Loss in iteration 43 : 0.580198983315943
Loss in iteration 44 : 0.5589713758298138
Loss in iteration 45 : 0.5702359413359221
Loss in iteration 46 : 0.5934146724079488
Loss in iteration 47 : 0.5983104356360306
Loss in iteration 48 : 0.5911466030632161
Loss in iteration 49 : 0.5985790152084179
Loss in iteration 50 : 0.5981369224492716
Loss in iteration 51 : 0.5781464077956558
Loss in iteration 52 : 0.5644237436266051
Loss in iteration 53 : 0.5443724594566106
Loss in iteration 54 : 0.5358226708106725
Loss in iteration 55 : 0.5288357931675152
Loss in iteration 56 : 0.5299581743561714
Loss in iteration 57 : 0.522023021049818
Loss in iteration 58 : 0.5292982132137922
Loss in iteration 59 : 0.5485692425780592
Loss in iteration 60 : 0.5898345899975175
Loss in iteration 61 : 0.6613812040040666
Loss in iteration 62 : 0.7649324649301298
Loss in iteration 63 : 0.6748476529164428
Loss in iteration 64 : 0.5595616187976074
Loss in iteration 65 : 0.5246154861289274
Loss in iteration 66 : 0.5650351200840176
Loss in iteration 67 : 0.635856299411628
Loss in iteration 68 : 0.6505866524952175
Loss in iteration 69 : 0.5870332738712054
Loss in iteration 70 : 0.529021912976193
Loss in iteration 71 : 0.5234227977741439
Loss in iteration 72 : 0.5378841438571494
Loss in iteration 73 : 0.6002567880194135
Loss in iteration 74 : 0.697488665024244
Loss in iteration 75 : 0.7098542314065905
Loss in iteration 76 : 0.5906480738276662
Loss in iteration 77 : 0.5235858813235678
Loss in iteration 78 : 0.5379541991130161
Loss in iteration 79 : 0.6090959574736944
Loss in iteration 80 : 0.6552447617383241
Loss in iteration 81 : 0.5987266359743126
Loss in iteration 82 : 0.5313910676617819
Loss in iteration 83 : 0.5218746626266477
Loss in iteration 84 : 0.5576252106654901
Loss in iteration 85 : 0.6098909302945326
Loss in iteration 86 : 0.6472641375026095
Loss in iteration 87 : 0.5897965655392174
Loss in iteration 88 : 0.5305719737790896
Loss in iteration 89 : 0.5180207281105931
Loss in iteration 90 : 0.5343684219244946
Loss in iteration 91 : 0.560753230734058
Loss in iteration 92 : 0.6098028746816685
Loss in iteration 93 : 0.6226470501984774
Loss in iteration 94 : 0.5787838131211835
Loss in iteration 95 : 0.5340279244146697
Loss in iteration 96 : 0.5221908764852647
Loss in iteration 97 : 0.5250820006982939
Loss in iteration 98 : 0.5584170828625769
Loss in iteration 99 : 0.604641186746093
Loss in iteration 100 : 0.61789934548921
Loss in iteration 101 : 0.5929759320320246
Loss in iteration 102 : 0.5517778027780389
Loss in iteration 103 : 0.5391477256456624
Loss in iteration 104 : 0.5227751261856838
Loss in iteration 105 : 0.5187683160023617
Loss in iteration 106 : 0.518687677827196
Loss in iteration 107 : 0.5156997358619342
Loss in iteration 108 : 0.5276293690530388
Loss in iteration 109 : 0.533003769420651
Loss in iteration 110 : 0.5541977526270035
Loss in iteration 111 : 0.6387049836653135
Loss in iteration 112 : 0.7094939597692056
Loss in iteration 113 : 0.6842476084756904
Loss in iteration 114 : 0.5633673858611089
Loss in iteration 115 : 0.5176237902228368
Loss in iteration 116 : 0.5838709811152543
Loss in iteration 117 : 0.6301766942522297
Loss in iteration 118 : 0.6367182450314405
Loss in iteration 119 : 0.557532607260791
Loss in iteration 120 : 0.5268219748414273
Loss in iteration 121 : 0.6082978140945977
Loss in iteration 122 : 0.6060467558765703
Loss in iteration 123 : 0.6187869295173299
Loss in iteration 124 : 0.5299328530967734
Loss in iteration 125 : 0.5376141167720124
Loss in iteration 126 : 0.5408754812197759
Loss in iteration 127 : 0.5664754303163936
Loss in iteration 128 : 0.5777501360344659
Loss in iteration 129 : 0.5626520944828549
Loss in iteration 130 : 0.5396682239088013
Loss in iteration 131 : 0.522000761595393
Loss in iteration 132 : 0.5193611712996767
Loss in iteration 133 : 0.5142134137978951
Loss in iteration 134 : 0.5134936049749622
Loss in iteration 135 : 0.5143459310575629
Loss in iteration 136 : 0.5098535864685715
Loss in iteration 137 : 0.514523202128903
Loss in iteration 138 : 0.5301639808719082
Loss in iteration 139 : 0.5859829451429552
Loss in iteration 140 : 0.76591326950274
Loss in iteration 141 : 0.7943534493912764
Loss in iteration 142 : 0.6169039273695335
Loss in iteration 143 : 0.5232493663114495
Loss in iteration 144 : 0.5472544347102218
Loss in iteration 145 : 0.650156532099219
Loss in iteration 146 : 0.6558302923966471
Loss in iteration 147 : 0.5590097388589492
Loss in iteration 148 : 0.5206290252692655
Loss in iteration 149 : 0.5621518502880879
Loss in iteration 150 : 0.6369823843598131
Loss in iteration 151 : 0.6062500591100707
Loss in iteration 152 : 0.5415468769224507
Loss in iteration 153 : 0.5203152306909292
Loss in iteration 154 : 0.5458768658678064
Loss in iteration 155 : 0.5925882529038937
Loss in iteration 156 : 0.594957703791293
Loss in iteration 157 : 0.5516104868727517
Loss in iteration 158 : 0.5274709477563622
Loss in iteration 159 : 0.5238858305536748
Loss in iteration 160 : 0.5536843500489321
Loss in iteration 161 : 0.5919487554306033
Loss in iteration 162 : 0.5865144904679109
Loss in iteration 163 : 0.5703676362349939
Loss in iteration 164 : 0.5484446573062278
Loss in iteration 165 : 0.5424833890604357
Loss in iteration 166 : 0.5297349360312857
Loss in iteration 167 : 0.5367681376289372
Loss in iteration 168 : 0.5303289399997425
Loss in iteration 169 : 0.5670655014427025
Loss in iteration 170 : 0.6011124551323199
Loss in iteration 171 : 0.6507944120302105
Loss in iteration 172 : 0.6681204205707124
Loss in iteration 173 : 0.5952593448429399
Loss in iteration 174 : 0.5361997129528343
Loss in iteration 175 : 0.5300583176103492
Loss in iteration 176 : 0.5374091694563629
Loss in iteration 177 : 0.5974322411474317
Loss in iteration 178 : 0.6063697981699611
Loss in iteration 179 : 0.6053069001553117
Loss in iteration 180 : 0.5425658245671191
Loss in iteration 181 : 0.5296016099590358
Loss in iteration 182 : 0.5210424956115806
Loss in iteration 183 : 0.5331724806969254
Loss in iteration 184 : 0.561566743806332
Loss in iteration 185 : 0.5875239556515776
Loss in iteration 186 : 0.5990399798620217
Loss in iteration 187 : 0.5679030013620384
Loss in iteration 188 : 0.5361298440893466
Loss in iteration 189 : 0.521608736088573
Loss in iteration 190 : 0.5169791286832445
Loss in iteration 191 : 0.5133696361907063
Loss in iteration 192 : 0.5151353031896888
Loss in iteration 193 : 0.5189152527297927
Loss in iteration 194 : 0.5220096243834517
Loss in iteration 195 : 0.5597040437248463
Loss in iteration 196 : 0.665353322279056
Loss in iteration 197 : 0.7650102622666299
Loss in iteration 198 : 0.6765454692753355
Loss in iteration 199 : 0.5529176044337725
Loss in iteration 200 : 0.5160601753893997
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.769, training accuracy 0.775625, time elapsed: 2014 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.1028551039406946
Loss in iteration 3 : 1.2039632000044054
Loss in iteration 4 : 0.7123524077176446
Loss in iteration 5 : 0.7211460955284639
Loss in iteration 6 : 0.8819913957340297
Loss in iteration 7 : 0.6043399628236102
Loss in iteration 8 : 0.6673052822055385
Loss in iteration 9 : 0.6427636628286191
Loss in iteration 10 : 0.6183039491926445
Loss in iteration 11 : 0.6434912848563153
Loss in iteration 12 : 0.6195358706268264
Loss in iteration 13 : 0.6339817202903443
Loss in iteration 14 : 0.6333206367638449
Loss in iteration 15 : 0.6113944807238968
Loss in iteration 16 : 0.6356347528591145
Loss in iteration 17 : 0.6019088406781675
Loss in iteration 18 : 0.6341271696542558
Loss in iteration 19 : 0.5954355907085124
Loss in iteration 20 : 0.6163609528969559
Loss in iteration 21 : 0.5839016521958076
Loss in iteration 22 : 0.5968980586143152
Loss in iteration 23 : 0.5750100696609988
Loss in iteration 24 : 0.5826307176867304
Loss in iteration 25 : 0.5620909802461671
Loss in iteration 26 : 0.5691501299828834
Loss in iteration 27 : 0.5493278508570686
Loss in iteration 28 : 0.5587683736709734
Loss in iteration 29 : 0.5384819353955616
Loss in iteration 30 : 0.5510848933001867
Loss in iteration 31 : 0.532892374162525
Loss in iteration 32 : 0.5394015208260748
Loss in iteration 33 : 0.5359516573890271
Loss in iteration 34 : 0.5300895515128975
Loss in iteration 35 : 0.5369636933104034
Loss in iteration 36 : 0.5311372604191674
Loss in iteration 37 : 0.5271228789603681
Loss in iteration 38 : 0.5329723311116741
Loss in iteration 39 : 0.5305886533063985
Loss in iteration 40 : 0.5242731515722145
Loss in iteration 41 : 0.5258683156796516
Loss in iteration 42 : 0.5276529482593175
Loss in iteration 43 : 0.5238954831007977
Loss in iteration 44 : 0.5195692460235583
Loss in iteration 45 : 0.5191207052332888
Loss in iteration 46 : 0.5198578827594674
Loss in iteration 47 : 0.5182819282179307
Loss in iteration 48 : 0.5141727465584247
Loss in iteration 49 : 0.5134010541006937
Loss in iteration 50 : 0.5144708862124064
Loss in iteration 51 : 0.5114873492582643
Loss in iteration 52 : 0.5111819469707317
Loss in iteration 53 : 0.5104965244625638
Loss in iteration 54 : 0.5091171488777317
Loss in iteration 55 : 0.5101191682149229
Loss in iteration 56 : 0.509840076641786
Loss in iteration 57 : 0.5088760908476861
Loss in iteration 58 : 0.5094187584512679
Loss in iteration 59 : 0.5093399425959252
Loss in iteration 60 : 0.5087944226826312
Loss in iteration 61 : 0.5088534872831929
Loss in iteration 62 : 0.508383163419598
Loss in iteration 63 : 0.5082232216501209
Loss in iteration 64 : 0.5083247384272926
Loss in iteration 65 : 0.5078019253414057
Loss in iteration 66 : 0.5077708352609541
Loss in iteration 67 : 0.5078728892132919
Loss in iteration 68 : 0.5077599401047387
Loss in iteration 69 : 0.5078121926524476
Loss in iteration 70 : 0.5076475054088013
Loss in iteration 71 : 0.5075753546553431
Loss in iteration 72 : 0.5077271139452381
Loss in iteration 73 : 0.507547481145622
Loss in iteration 74 : 0.5074513850107706
Loss in iteration 75 : 0.5077112919350834
Loss in iteration 76 : 0.5079376603900377
Loss in iteration 77 : 0.5081665748278085
Loss in iteration 78 : 0.5083734273221835
Loss in iteration 79 : 0.5086877043853211
Loss in iteration 80 : 0.5082003025926721
Loss in iteration 81 : 0.5074291717055739
Loss in iteration 82 : 0.5077078941830614
Loss in iteration 83 : 0.5076471515097477
Loss in iteration 84 : 0.507443501341392
Loss in iteration 85 : 0.5075984320649103
Loss in iteration 86 : 0.5075606882667668
Loss in iteration 87 : 0.5075840532835932
Loss in iteration 88 : 0.5074316932311211
Loss in iteration 89 : 0.5075678046367404
Loss in iteration 90 : 0.5074297777411687
Loss in iteration 91 : 0.5074778586622117
Loss in iteration 92 : 0.5073913809805729
Loss in iteration 93 : 0.5072876442416743
Loss in iteration 94 : 0.5073145267294747
Loss in iteration 95 : 0.5067669501147118
Loss in iteration 96 : 0.5070597589020273
Loss in iteration 97 : 0.5076016348479959
Loss in iteration 98 : 0.507584712517426
Loss in iteration 99 : 0.5074553799693023
Loss in iteration 100 : 0.5071697774583722
Loss in iteration 101 : 0.5072419100857529
Loss in iteration 102 : 0.5079803306269867
Loss in iteration 103 : 0.5078366832391393
Loss in iteration 104 : 0.5078322219765807
Loss in iteration 105 : 0.5075402699996151
Loss in iteration 106 : 0.5078451905095658
Loss in iteration 107 : 0.5075569970349979
Loss in iteration 108 : 0.5077185241416162
Loss in iteration 109 : 0.507677290645492
Loss in iteration 110 : 0.5075582208978344
Loss in iteration 111 : 0.5070658005631474
Loss in iteration 112 : 0.5071030430137553
Loss in iteration 113 : 0.5073234498039257
Loss in iteration 114 : 0.5078690472502332
Loss in iteration 115 : 0.5078182582291029
Loss in iteration 116 : 0.5082269443600732
Loss in iteration 117 : 0.5078742221554419
Loss in iteration 118 : 0.5085491226975348
Loss in iteration 119 : 0.5090957010551361
Loss in iteration 120 : 0.5112903982775607
Loss in iteration 121 : 0.5132769571794668
Loss in iteration 122 : 0.5181999986978729
Loss in iteration 123 : 0.52417993928332
Loss in iteration 124 : 0.5337274434565991
Loss in iteration 125 : 0.5410418162705499
Loss in iteration 126 : 0.5396510605664696
Loss in iteration 127 : 0.5314166240383242
Loss in iteration 128 : 0.5198906642397999
Loss in iteration 129 : 0.5092454914210514
Loss in iteration 130 : 0.5078318126502285
Loss in iteration 131 : 0.510347690100001
Loss in iteration 132 : 0.5129377277881119
Loss in iteration 133 : 0.5221914574036539
Loss in iteration 134 : 0.5316071910526325
Loss in iteration 135 : 0.5382137403955626
Loss in iteration 136 : 0.5397393370274183
Loss in iteration 137 : 0.5366489643937478
Loss in iteration 138 : 0.527164068004659
Loss in iteration 139 : 0.521032138384066
Loss in iteration 140 : 0.5122670150300708
Loss in iteration 141 : 0.5095022022477709
Loss in iteration 142 : 0.5123191687193507
Loss in iteration 143 : 0.5204713710655908
Loss in iteration 144 : 0.5297072092389965
Loss in iteration 145 : 0.5301879537034769
Loss in iteration 146 : 0.5345122947148632
Loss in iteration 147 : 0.5364543008280298
Loss in iteration 148 : 0.5350644841296066
Loss in iteration 149 : 0.5283229496511829
Loss in iteration 150 : 0.5222606222502693
Loss in iteration 151 : 0.5097301456547189
Loss in iteration 152 : 0.5091021274040073
Loss in iteration 153 : 0.5111247881831785
Loss in iteration 154 : 0.5139557330354553
Loss in iteration 155 : 0.5224698707321074
Loss in iteration 156 : 0.5318295870774945
Loss in iteration 157 : 0.5446126039328008
Loss in iteration 158 : 0.5476907409288255
Loss in iteration 159 : 0.5417029419148421
Loss in iteration 160 : 0.5248876692703075
Loss in iteration 161 : 0.5147631543597984
Loss in iteration 162 : 0.507783191973263
Loss in iteration 163 : 0.5098182089522003
Loss in iteration 164 : 0.5139320803897267
Loss in iteration 165 : 0.522791946986697
Loss in iteration 166 : 0.5354346505319076
Loss in iteration 167 : 0.5405095791638265
Loss in iteration 168 : 0.5406824438270869
Loss in iteration 169 : 0.5309546812878225
Loss in iteration 170 : 0.5268786321851799
Loss in iteration 171 : 0.5149599715655955
Loss in iteration 172 : 0.5145011436500074
Loss in iteration 173 : 0.5086817536402883
Loss in iteration 174 : 0.5085910432434705
Loss in iteration 175 : 0.5091918297890301
Loss in iteration 176 : 0.5080011677928362
Loss in iteration 177 : 0.5108343893571167
Loss in iteration 178 : 0.5084515358103531
Loss in iteration 179 : 0.5093579085301322
Loss in iteration 180 : 0.5099610829004471
Loss in iteration 181 : 0.5113692517119273
Loss in iteration 182 : 0.5180695492207421
Loss in iteration 183 : 0.5295120971510724
Loss in iteration 184 : 0.5478050886072449
Loss in iteration 185 : 0.5668074475108574
Loss in iteration 186 : 0.5781417864081041
Loss in iteration 187 : 0.5567749388865636
Loss in iteration 188 : 0.5303585097285048
Loss in iteration 189 : 0.5187348823670933
Loss in iteration 190 : 0.5087707288880938
Loss in iteration 191 : 0.5123492667412155
Loss in iteration 192 : 0.5177987812147719
Loss in iteration 193 : 0.5252441486623494
Loss in iteration 194 : 0.5360453352135615
Loss in iteration 195 : 0.5437466039806548
Loss in iteration 196 : 0.5480784858890168
Loss in iteration 197 : 0.5409225342931692
Loss in iteration 198 : 0.533761965434883
Loss in iteration 199 : 0.5176903122112237
Loss in iteration 200 : 0.5171241971481286
Testing accuracy  of updater 6 on alg 1 with rate 1.4000000000000001 = 0.783, training accuracy 0.78975, time elapsed: 2128 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9268148942140014
Loss in iteration 3 : 0.8306842658164385
Loss in iteration 4 : 0.7511838443714082
Loss in iteration 5 : 0.6537982970573812
Loss in iteration 6 : 0.5971709807659697
Loss in iteration 7 : 0.5918564073822759
Loss in iteration 8 : 0.5651296230614464
Loss in iteration 9 : 0.5642529130451494
Loss in iteration 10 : 0.5641940875567905
Loss in iteration 11 : 0.5617517388357051
Loss in iteration 12 : 0.5693701022492931
Loss in iteration 13 : 0.564287507495246
Loss in iteration 14 : 0.5668669235325391
Loss in iteration 15 : 0.5706785940802143
Loss in iteration 16 : 0.5668967782474085
Loss in iteration 17 : 0.5721205550390767
Loss in iteration 18 : 0.5664241846286627
Loss in iteration 19 : 0.5647588327271077
Loss in iteration 20 : 0.561770754578969
Loss in iteration 21 : 0.5564981270143817
Loss in iteration 22 : 0.5542206073521374
Loss in iteration 23 : 0.5496409504648911
Loss in iteration 24 : 0.5460070277548346
Loss in iteration 25 : 0.5416186484143456
Loss in iteration 26 : 0.5378400071340034
Loss in iteration 27 : 0.5333261299428823
Loss in iteration 28 : 0.531310422401111
Loss in iteration 29 : 0.5274785857716181
Loss in iteration 30 : 0.5261511458982839
Loss in iteration 31 : 0.5223866345119457
Loss in iteration 32 : 0.5214530703504241
Loss in iteration 33 : 0.5201026641770445
Loss in iteration 34 : 0.519287238962864
Loss in iteration 35 : 0.5186512995875505
Loss in iteration 36 : 0.5175949863766043
Loss in iteration 37 : 0.5181199172897861
Loss in iteration 38 : 0.5172455038251226
Loss in iteration 39 : 0.5175251843713782
Loss in iteration 40 : 0.5171117031730144
Loss in iteration 41 : 0.5164715829009002
Loss in iteration 42 : 0.516358458358152
Loss in iteration 43 : 0.5158035968833939
Loss in iteration 44 : 0.515547781841507
Loss in iteration 45 : 0.5148688627407064
Loss in iteration 46 : 0.514278629689761
Loss in iteration 47 : 0.5136596378124381
Loss in iteration 48 : 0.5130876595475211
Loss in iteration 49 : 0.51253212820316
Loss in iteration 50 : 0.511641111440091
Loss in iteration 51 : 0.5111839928821852
Loss in iteration 52 : 0.5104627247616383
Loss in iteration 53 : 0.5101132034187245
Loss in iteration 54 : 0.5097704934642746
Loss in iteration 55 : 0.5093666799084949
Loss in iteration 56 : 0.5091821600521425
Loss in iteration 57 : 0.508988220679296
Loss in iteration 58 : 0.5088368940787341
Loss in iteration 59 : 0.5087220601463014
Loss in iteration 60 : 0.5085348519542944
Loss in iteration 61 : 0.5085289279178586
Loss in iteration 62 : 0.5084089696064694
Loss in iteration 63 : 0.5084394408269984
Loss in iteration 64 : 0.5082708679175808
Loss in iteration 65 : 0.508272693180041
Loss in iteration 66 : 0.5081603721897063
Loss in iteration 67 : 0.5081268399944461
Loss in iteration 68 : 0.5079857119555696
Loss in iteration 69 : 0.5079428949173347
Loss in iteration 70 : 0.5078261655232157
Loss in iteration 71 : 0.5077874198831163
Loss in iteration 72 : 0.5077103790065287
Loss in iteration 73 : 0.5076654973026582
Loss in iteration 74 : 0.5075948865917906
Loss in iteration 75 : 0.5075513860778741
Loss in iteration 76 : 0.5075310023319886
Loss in iteration 77 : 0.5076124226105692
Loss in iteration 78 : 0.507485797773003
Loss in iteration 79 : 0.5075559005157115
Loss in iteration 80 : 0.5075110710673754
Loss in iteration 81 : 0.5074484395888819
Loss in iteration 82 : 0.5075634155438667
Loss in iteration 83 : 0.5073803391755383
Loss in iteration 84 : 0.5073849417565153
Loss in iteration 85 : 0.5073665238591002
Loss in iteration 86 : 0.5072765356009603
Loss in iteration 87 : 0.5073155407210218
Loss in iteration 88 : 0.5072770957998186
Loss in iteration 89 : 0.5071873121247237
Loss in iteration 90 : 0.5071949768638816
Loss in iteration 91 : 0.507144750263837
Loss in iteration 92 : 0.5071206469725964
Loss in iteration 93 : 0.5070870370364677
Loss in iteration 94 : 0.507105093777155
Loss in iteration 95 : 0.5070465036509154
Loss in iteration 96 : 0.5070390490037195
Loss in iteration 97 : 0.5070545972422117
Loss in iteration 98 : 0.5069969434408322
Loss in iteration 99 : 0.5069827025944149
Loss in iteration 100 : 0.5069773649542092
Loss in iteration 101 : 0.5069728616462136
Loss in iteration 102 : 0.506918240909926
Loss in iteration 103 : 0.5069386761980027
Loss in iteration 104 : 0.5069158697399122
Loss in iteration 105 : 0.5068860536045275
Loss in iteration 106 : 0.506892207473402
Loss in iteration 107 : 0.506870796114625
Loss in iteration 108 : 0.5068269263355591
Loss in iteration 109 : 0.5068626383459209
Loss in iteration 110 : 0.506866932251943
Loss in iteration 111 : 0.5067921740683846
Loss in iteration 112 : 0.5068207342142341
Loss in iteration 113 : 0.506816531791789
Loss in iteration 114 : 0.506749148841941
Loss in iteration 115 : 0.5067639091124913
Loss in iteration 116 : 0.5067812564038968
Loss in iteration 117 : 0.5067300201808166
Loss in iteration 118 : 0.5067338201064896
Loss in iteration 119 : 0.506764462541738
Loss in iteration 120 : 0.5067148683406161
Loss in iteration 121 : 0.5066920180784904
Loss in iteration 122 : 0.506714655449871
Loss in iteration 123 : 0.5066604291774525
Loss in iteration 124 : 0.5067012066215365
Loss in iteration 125 : 0.5066954428228089
Loss in iteration 126 : 0.5066584215441493
Loss in iteration 127 : 0.5066334375767941
Loss in iteration 128 : 0.5066797191791955
Loss in iteration 129 : 0.5066421292751329
Loss in iteration 130 : 0.5066607973008846
Loss in iteration 131 : 0.5066384456951907
Loss in iteration 132 : 0.5066117984031393
Loss in iteration 133 : 0.5065922302726206
Loss in iteration 134 : 0.5065880404386265
Loss in iteration 135 : 0.5065907765293447
Loss in iteration 136 : 0.506555007382224
Loss in iteration 137 : 0.5065629248727423
Loss in iteration 138 : 0.5065429600637492
Loss in iteration 139 : 0.5065364163561337
Loss in iteration 140 : 0.50654513083353
Loss in iteration 141 : 0.5065459546781717
Loss in iteration 142 : 0.5065237023806803
Loss in iteration 143 : 0.5065505474976615
Loss in iteration 144 : 0.5065093817903916
Loss in iteration 145 : 0.5065135986725168
Loss in iteration 146 : 0.5065045300088723
Loss in iteration 147 : 0.5064837484600819
Loss in iteration 148 : 0.5065056237236258
Loss in iteration 149 : 0.506489655689078
Loss in iteration 150 : 0.5064801922096372
Loss in iteration 151 : 0.5064665047028117
Loss in iteration 152 : 0.5064672896040423
Loss in iteration 153 : 0.5064680262434913
Loss in iteration 154 : 0.5064536141796463
Loss in iteration 155 : 0.5064806900162285
Loss in iteration 156 : 0.5064813548676144
Loss in iteration 157 : 0.5064497120806932
Loss in iteration 158 : 0.5064782384205562
Loss in iteration 159 : 0.5064818239669785
Loss in iteration 160 : 0.5064456517359854
Loss in iteration 161 : 0.5064769547895366
Loss in iteration 162 : 0.5064727869478539
Loss in iteration 163 : 0.5064346367101457
Loss in iteration 164 : 0.5064551171196255
Loss in iteration 165 : 0.506472550063577
Loss in iteration 166 : 0.506462529210875
Loss in iteration 167 : 0.5064389099136687
Loss in iteration 168 : 0.506497175573817
Loss in iteration 169 : 0.5064856285211868
Loss in iteration 170 : 0.5064394436991576
Loss in iteration 171 : 0.506443413034945
Loss in iteration 172 : 0.5064783063504115
Loss in iteration 173 : 0.5064858147661923
Loss in iteration 174 : 0.5064082407469864
Loss in iteration 175 : 0.5065475002003247
Loss in iteration 176 : 0.5065579986012219
Loss in iteration 177 : 0.506455094953711
Loss in iteration 178 : 0.5064364923097862
Loss in iteration 179 : 0.5065740038902322
Loss in iteration 180 : 0.5065238844157657
Loss in iteration 181 : 0.5064260697400661
Loss in iteration 182 : 0.5064646934180942
Loss in iteration 183 : 0.5065380776093055
Loss in iteration 184 : 0.5065214744372205
Loss in iteration 185 : 0.5064876449424567
Loss in iteration 186 : 0.5065042169052281
Loss in iteration 187 : 0.5065297982819177
Loss in iteration 188 : 0.5064687201516126
Loss in iteration 189 : 0.5065336183533373
Loss in iteration 190 : 0.5066695728845843
Loss in iteration 191 : 0.5064506067264183
Loss in iteration 192 : 0.5064902784503158
Loss in iteration 193 : 0.5065111377752329
Loss in iteration 194 : 0.5064533544891677
Loss in iteration 195 : 0.5064632561385516
Loss in iteration 196 : 0.5063864296029859
Loss in iteration 197 : 0.5063722248507208
Loss in iteration 198 : 0.5064334620972208
Loss in iteration 199 : 0.5064001336972114
Loss in iteration 200 : 0.5064011154592676
Testing accuracy  of updater 6 on alg 1 with rate 0.8 = 0.7815, training accuracy 0.7885, time elapsed: 2069 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9455829241446756
Loss in iteration 3 : 0.9140709103146221
Loss in iteration 4 : 0.9039673076768088
Loss in iteration 5 : 0.8339531928202861
Loss in iteration 6 : 0.7810082541006264
Loss in iteration 7 : 0.758647690574997
Loss in iteration 8 : 0.6974104492171534
Loss in iteration 9 : 0.6386978137311149
Loss in iteration 10 : 0.6324243028495564
Loss in iteration 11 : 0.6070936749198788
Loss in iteration 12 : 0.5786726380285182
Loss in iteration 13 : 0.5820844091255137
Loss in iteration 14 : 0.5718361438945166
Loss in iteration 15 : 0.5557807598507185
Loss in iteration 16 : 0.5576074321727383
Loss in iteration 17 : 0.5568649320469765
Loss in iteration 18 : 0.5495000606623613
Loss in iteration 19 : 0.5500010842402123
Loss in iteration 20 : 0.5517426226092543
Loss in iteration 21 : 0.5471198683306397
Loss in iteration 22 : 0.543929068012307
Loss in iteration 23 : 0.5452077747602369
Loss in iteration 24 : 0.5438198635662875
Loss in iteration 25 : 0.5405474919974118
Loss in iteration 26 : 0.5405167809497771
Loss in iteration 27 : 0.5411994722642894
Loss in iteration 28 : 0.5390620028418474
Loss in iteration 29 : 0.536918382734221
Loss in iteration 30 : 0.5370473961204332
Loss in iteration 31 : 0.5361390114593819
Loss in iteration 32 : 0.5332462418987605
Loss in iteration 33 : 0.5319283837175276
Loss in iteration 34 : 0.5316842766997849
Loss in iteration 35 : 0.5297046832046122
Loss in iteration 36 : 0.5275322057441422
Loss in iteration 37 : 0.5270397963330087
Loss in iteration 38 : 0.526154604000985
Loss in iteration 39 : 0.5242250524748782
Loss in iteration 40 : 0.5230206541542757
Loss in iteration 41 : 0.5222944067994387
Loss in iteration 42 : 0.5209367596670368
Loss in iteration 43 : 0.5197629236630819
Loss in iteration 44 : 0.5190783872286276
Loss in iteration 45 : 0.5182416727388912
Loss in iteration 46 : 0.5173510883806312
Loss in iteration 47 : 0.5168607382191344
Loss in iteration 48 : 0.5161362390859446
Loss in iteration 49 : 0.5152564705039587
Loss in iteration 50 : 0.5148914882339638
Loss in iteration 51 : 0.5145342227406177
Loss in iteration 52 : 0.5139142483064781
Loss in iteration 53 : 0.5136121821155696
Loss in iteration 54 : 0.5134621883955658
Loss in iteration 55 : 0.5130652547332646
Loss in iteration 56 : 0.5127570805707942
Loss in iteration 57 : 0.5126629104514648
Loss in iteration 58 : 0.51241456724811
Loss in iteration 59 : 0.5121794484136969
Loss in iteration 60 : 0.5121009357857809
Loss in iteration 61 : 0.5119699347980008
Loss in iteration 62 : 0.5117831997311842
Loss in iteration 63 : 0.511727364411296
Loss in iteration 64 : 0.5116640498258284
Loss in iteration 65 : 0.511520099943637
Loss in iteration 66 : 0.5114284961554951
Loss in iteration 67 : 0.5113505259589775
Loss in iteration 68 : 0.5112452476050741
Loss in iteration 69 : 0.5111750308245748
Loss in iteration 70 : 0.5110862027473454
Loss in iteration 71 : 0.5110004147322137
Loss in iteration 72 : 0.5109615212123029
Loss in iteration 73 : 0.5108827340771572
Loss in iteration 74 : 0.5107933634969969
Loss in iteration 75 : 0.5107296602875016
Loss in iteration 76 : 0.5106674913599852
Loss in iteration 77 : 0.510592928467731
Loss in iteration 78 : 0.5105288850858475
Loss in iteration 79 : 0.5104717917428974
Loss in iteration 80 : 0.5104077642721618
Loss in iteration 81 : 0.5103447622982294
Loss in iteration 82 : 0.5102896291859343
Loss in iteration 83 : 0.5102386104498002
Loss in iteration 84 : 0.5101779933413485
Loss in iteration 85 : 0.5101233633576734
Loss in iteration 86 : 0.5100731978945721
Loss in iteration 87 : 0.5100306937210471
Loss in iteration 88 : 0.5099763830588673
Loss in iteration 89 : 0.5099316574098184
Loss in iteration 90 : 0.5098922585180986
Loss in iteration 91 : 0.5098509614887462
Loss in iteration 92 : 0.5098060228633258
Loss in iteration 93 : 0.5097665407416665
Loss in iteration 94 : 0.5097290386871227
Loss in iteration 95 : 0.5096916596621671
Loss in iteration 96 : 0.509654606631069
Loss in iteration 97 : 0.5096229426443841
Loss in iteration 98 : 0.5095865076925578
Loss in iteration 99 : 0.5095530017973122
Loss in iteration 100 : 0.5095237444857791
Loss in iteration 101 : 0.5094896592687099
Loss in iteration 102 : 0.5094557464752238
Loss in iteration 103 : 0.5094259211327538
Loss in iteration 104 : 0.5093967294071972
Loss in iteration 105 : 0.5093688848688213
Loss in iteration 106 : 0.5093402360258268
Loss in iteration 107 : 0.5093121727830666
Loss in iteration 108 : 0.5092850691666337
Loss in iteration 109 : 0.5092578950830235
Loss in iteration 110 : 0.5092311671957779
Loss in iteration 111 : 0.5092047207332235
Loss in iteration 112 : 0.5091795235982273
Loss in iteration 113 : 0.5091557889703909
Loss in iteration 114 : 0.5091310178982464
Loss in iteration 115 : 0.5091061488689059
Loss in iteration 116 : 0.5090841311061064
Loss in iteration 117 : 0.5090615242177435
Loss in iteration 118 : 0.5090375886887647
Loss in iteration 119 : 0.509016743947945
Loss in iteration 120 : 0.508996910750956
Loss in iteration 121 : 0.5089742789881384
Loss in iteration 122 : 0.5089527353685677
Loss in iteration 123 : 0.5089320150267196
Loss in iteration 124 : 0.5089113948166822
Loss in iteration 125 : 0.5088906558285572
Loss in iteration 126 : 0.5088695792358625
Loss in iteration 127 : 0.5088498179457461
Loss in iteration 128 : 0.5088301616908767
Loss in iteration 129 : 0.5088099578297939
Loss in iteration 130 : 0.5087902762674457
Loss in iteration 131 : 0.5087717028949003
Loss in iteration 132 : 0.5087527679903893
Loss in iteration 133 : 0.5087330314384171
Loss in iteration 134 : 0.5087154538156351
Loss in iteration 135 : 0.5086965186686844
Loss in iteration 136 : 0.5086787906820325
Loss in iteration 137 : 0.5086597387457041
Loss in iteration 138 : 0.5086425263044094
Loss in iteration 139 : 0.508624330280345
Loss in iteration 140 : 0.5086063729453529
Loss in iteration 141 : 0.5085884865827703
Loss in iteration 142 : 0.5085717241039229
Loss in iteration 143 : 0.5085529249317519
Loss in iteration 144 : 0.5085377443161448
Loss in iteration 145 : 0.5085195048750115
Loss in iteration 146 : 0.5085049801327802
Loss in iteration 147 : 0.5084884209158846
Loss in iteration 148 : 0.5084725427577059
Loss in iteration 149 : 0.5084565912287476
Loss in iteration 150 : 0.508440134280189
Loss in iteration 151 : 0.5084242905978319
Loss in iteration 152 : 0.508409440350652
Loss in iteration 153 : 0.508394090135966
Loss in iteration 154 : 0.5083789212064024
Loss in iteration 155 : 0.508363937147163
Loss in iteration 156 : 0.5083491413088135
Loss in iteration 157 : 0.5083352102073249
Loss in iteration 158 : 0.5083205636312843
Loss in iteration 159 : 0.508306799002517
Loss in iteration 160 : 0.5082924503470702
Loss in iteration 161 : 0.5082783884247077
Loss in iteration 162 : 0.5082638330680473
Loss in iteration 163 : 0.5082503847155364
Loss in iteration 164 : 0.5082347912098368
Loss in iteration 165 : 0.508221266270552
Loss in iteration 166 : 0.5082067015449674
Loss in iteration 167 : 0.5081927260558625
Loss in iteration 168 : 0.5081787111254444
Loss in iteration 169 : 0.5081660480002302
Loss in iteration 170 : 0.5081503835357444
Loss in iteration 171 : 0.5081373342847207
Loss in iteration 172 : 0.5081245911663057
Loss in iteration 173 : 0.5081095037004382
Loss in iteration 174 : 0.5080957197299237
Loss in iteration 175 : 0.5080828384332834
Loss in iteration 176 : 0.5080682101963313
Loss in iteration 177 : 0.5080565701352032
Loss in iteration 178 : 0.5080425170958036
Loss in iteration 179 : 0.508031499092306
Loss in iteration 180 : 0.5080179705713215
Loss in iteration 181 : 0.5080043909248674
Loss in iteration 182 : 0.5079916548867665
Loss in iteration 183 : 0.5079788448675677
Loss in iteration 184 : 0.5079665757335684
Loss in iteration 185 : 0.5079547476430022
Loss in iteration 186 : 0.5079418407013624
Loss in iteration 187 : 0.507929586160206
Loss in iteration 188 : 0.5079179784486121
Loss in iteration 189 : 0.5079051058258337
Loss in iteration 190 : 0.507892726742366
Loss in iteration 191 : 0.507880785949107
Loss in iteration 192 : 0.5078685177114761
Loss in iteration 193 : 0.5078563352828601
Loss in iteration 194 : 0.5078445381648855
Loss in iteration 195 : 0.5078333820451431
Loss in iteration 196 : 0.5078218439733512
Loss in iteration 197 : 0.5078091475535715
Loss in iteration 198 : 0.5077970745523268
Loss in iteration 199 : 0.5077863544568716
Loss in iteration 200 : 0.5077727682820078
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.7835, training accuracy 0.787875, time elapsed: 2079 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9545878235931409
Loss in iteration 3 : 0.9111929159993387
Loss in iteration 4 : 0.9168642091730894
Loss in iteration 5 : 0.8635582333381976
Loss in iteration 6 : 0.8080013062514247
Loss in iteration 7 : 0.7671205364771231
Loss in iteration 8 : 0.7469617656872731
Loss in iteration 9 : 0.6869233914895999
Loss in iteration 10 : 0.6397042590962942
Loss in iteration 11 : 0.6309271189952176
Loss in iteration 12 : 0.6116340866586449
Loss in iteration 13 : 0.5837605585892857
Loss in iteration 14 : 0.5789922267627571
Loss in iteration 15 : 0.5770950860953167
Loss in iteration 16 : 0.5622331568220018
Loss in iteration 17 : 0.5559755966649498
Loss in iteration 18 : 0.5579180838780344
Loss in iteration 19 : 0.5539770836902641
Loss in iteration 20 : 0.5482139857134828
Loss in iteration 21 : 0.5491451365796729
Loss in iteration 22 : 0.5503953445255403
Loss in iteration 23 : 0.5463600890801699
Loss in iteration 24 : 0.5426510938763365
Loss in iteration 25 : 0.5431935035220067
Loss in iteration 26 : 0.543199619792589
Loss in iteration 27 : 0.5402794599369334
Loss in iteration 28 : 0.5383739919636371
Loss in iteration 29 : 0.5389521754854306
Loss in iteration 30 : 0.5384888556266871
Loss in iteration 31 : 0.5362356560241504
Loss in iteration 32 : 0.5349992576427647
Loss in iteration 33 : 0.5348878544959832
Loss in iteration 34 : 0.5336527699543702
Loss in iteration 35 : 0.5315050631771583
Loss in iteration 36 : 0.5305971468758471
Loss in iteration 37 : 0.5301619886298361
Loss in iteration 38 : 0.5286686948302413
Loss in iteration 39 : 0.5269304727709777
Loss in iteration 40 : 0.5262753129464327
Loss in iteration 41 : 0.5256484692364541
Loss in iteration 42 : 0.5242438691253057
Loss in iteration 43 : 0.5229665839358896
Loss in iteration 44 : 0.5224020203737811
Loss in iteration 45 : 0.521637424723571
Loss in iteration 46 : 0.5204962664278608
Loss in iteration 47 : 0.5196099715942866
Loss in iteration 48 : 0.5191069358404112
Loss in iteration 49 : 0.5184850050343838
Loss in iteration 50 : 0.5177551993591205
Loss in iteration 51 : 0.5171991983057367
Loss in iteration 52 : 0.5167374461140559
Loss in iteration 53 : 0.5162201821934727
Loss in iteration 54 : 0.5157238271584704
Loss in iteration 55 : 0.5153892088012336
Loss in iteration 56 : 0.5150539790092782
Loss in iteration 57 : 0.5146557352755503
Loss in iteration 58 : 0.5143758542072372
Loss in iteration 59 : 0.5141609836603898
Loss in iteration 60 : 0.5138732859556069
Loss in iteration 61 : 0.5135910298644059
Loss in iteration 62 : 0.5134274940758059
Loss in iteration 63 : 0.5132543943099837
Loss in iteration 64 : 0.5130209174776664
Loss in iteration 65 : 0.5128601594257934
Loss in iteration 66 : 0.5127612674977297
Loss in iteration 67 : 0.5125862864530781
Loss in iteration 68 : 0.5124429342171617
Loss in iteration 69 : 0.5123592292118173
Loss in iteration 70 : 0.5122421177413928
Loss in iteration 71 : 0.5120983597895908
Loss in iteration 72 : 0.5120058932855489
Loss in iteration 73 : 0.5119198083739571
Loss in iteration 74 : 0.5118033223052734
Loss in iteration 75 : 0.511713832332999
Loss in iteration 76 : 0.5116480032418872
Loss in iteration 77 : 0.5115622042173524
Loss in iteration 78 : 0.5114726173337464
Loss in iteration 79 : 0.511418382883559
Loss in iteration 80 : 0.5113512604990068
Loss in iteration 81 : 0.5112676120127678
Loss in iteration 82 : 0.511205038768894
Loss in iteration 83 : 0.5111374915712077
Loss in iteration 84 : 0.5110695111396747
Loss in iteration 85 : 0.5110137976197436
Loss in iteration 86 : 0.51095697084489
Loss in iteration 87 : 0.5108945288866172
Loss in iteration 88 : 0.5108392569306475
Loss in iteration 89 : 0.510788233663508
Loss in iteration 90 : 0.5107251207398364
Loss in iteration 91 : 0.5106694927031856
Loss in iteration 92 : 0.5106218258050601
Loss in iteration 93 : 0.5105729300817053
Loss in iteration 94 : 0.5105211850364607
Loss in iteration 95 : 0.5104715902957886
Loss in iteration 96 : 0.5104290035921432
Loss in iteration 97 : 0.5103848410033622
Loss in iteration 98 : 0.510340636694198
Loss in iteration 99 : 0.5103017453072063
Loss in iteration 100 : 0.5102593008509909
Loss in iteration 101 : 0.5102169257323793
Loss in iteration 102 : 0.5101791929833072
Loss in iteration 103 : 0.5101400154659788
Loss in iteration 104 : 0.5101009320066315
Loss in iteration 105 : 0.5100646714967888
Loss in iteration 106 : 0.5100291610390792
Loss in iteration 107 : 0.5099938026889809
Loss in iteration 108 : 0.5099578985521653
Loss in iteration 109 : 0.5099234966551828
Loss in iteration 110 : 0.5098914165227837
Loss in iteration 111 : 0.5098581454989964
Loss in iteration 112 : 0.509824712875568
Loss in iteration 113 : 0.5097955661637275
Loss in iteration 114 : 0.5097648722581679
Loss in iteration 115 : 0.5097339044699054
Loss in iteration 116 : 0.509705797929807
Loss in iteration 117 : 0.5096780076854774
Loss in iteration 118 : 0.5096488583533245
Loss in iteration 119 : 0.5096215863288148
Loss in iteration 120 : 0.5095944101241437
Loss in iteration 121 : 0.5095668767028977
Loss in iteration 122 : 0.509540091365915
Loss in iteration 123 : 0.5095143795635325
Loss in iteration 124 : 0.5094876690867912
Loss in iteration 125 : 0.5094619404598696
Loss in iteration 126 : 0.5094370908384458
Loss in iteration 127 : 0.5094114818490166
Loss in iteration 128 : 0.5093862072634925
Loss in iteration 129 : 0.5093623811737944
Loss in iteration 130 : 0.5093378874373328
Loss in iteration 131 : 0.5093142757387861
Loss in iteration 132 : 0.509291004209209
Loss in iteration 133 : 0.509269301815434
Loss in iteration 134 : 0.5092465354732618
Loss in iteration 135 : 0.5092250112857315
Loss in iteration 136 : 0.5092027797305472
Loss in iteration 137 : 0.5091829937737612
Loss in iteration 138 : 0.5091619763134715
Loss in iteration 139 : 0.5091425434044793
Loss in iteration 140 : 0.5091224585568116
Loss in iteration 141 : 0.5091036421090714
Loss in iteration 142 : 0.509084495045601
Loss in iteration 143 : 0.5090657612750185
Loss in iteration 144 : 0.5090479878402542
Loss in iteration 145 : 0.509029516220736
Loss in iteration 146 : 0.5090126504415386
Loss in iteration 147 : 0.508994701118581
Loss in iteration 148 : 0.5089757743104414
Loss in iteration 149 : 0.5089593539700297
Loss in iteration 150 : 0.5089423381322579
Loss in iteration 151 : 0.5089248290337293
Loss in iteration 152 : 0.508907038456863
Loss in iteration 153 : 0.5088906904258851
Loss in iteration 154 : 0.5088751120434571
Loss in iteration 155 : 0.5088578625305132
Loss in iteration 156 : 0.508840143676178
Loss in iteration 157 : 0.5088244582425446
Loss in iteration 158 : 0.5088089104949187
Loss in iteration 159 : 0.5087933158618366
Loss in iteration 160 : 0.5087769169297114
Loss in iteration 161 : 0.5087611640030208
Loss in iteration 162 : 0.5087457301250796
Loss in iteration 163 : 0.5087305563782957
Loss in iteration 164 : 0.5087151803731386
Loss in iteration 165 : 0.5086999996110039
Loss in iteration 166 : 0.508684942677508
Loss in iteration 167 : 0.5086698946870342
Loss in iteration 168 : 0.5086553870255924
Loss in iteration 169 : 0.5086405869105336
Loss in iteration 170 : 0.5086260271233913
Loss in iteration 171 : 0.5086119543145643
Loss in iteration 172 : 0.5085971302779031
Loss in iteration 173 : 0.5085838018191288
Loss in iteration 174 : 0.5085697527660946
Loss in iteration 175 : 0.5085557266921445
Loss in iteration 176 : 0.5085420657025568
Loss in iteration 177 : 0.5085278095225346
Loss in iteration 178 : 0.508514228894914
Loss in iteration 179 : 0.5085003511055545
Loss in iteration 180 : 0.508486826963084
Loss in iteration 181 : 0.5084731853718034
Loss in iteration 182 : 0.508459936189729
Loss in iteration 183 : 0.5084465350534687
Loss in iteration 184 : 0.508433357841861
Loss in iteration 185 : 0.508419971394726
Loss in iteration 186 : 0.5084075275773751
Loss in iteration 187 : 0.5083939881241129
Loss in iteration 188 : 0.5083818489545308
Loss in iteration 189 : 0.5083688710992394
Loss in iteration 190 : 0.5083566927657359
Loss in iteration 191 : 0.5083444244037869
Loss in iteration 192 : 0.5083316866711702
Loss in iteration 193 : 0.5083200109031237
Loss in iteration 194 : 0.5083082359739153
Loss in iteration 195 : 0.5082958859325165
Loss in iteration 196 : 0.5082843106719732
Loss in iteration 197 : 0.5082727966156788
Loss in iteration 198 : 0.5082609194485461
Loss in iteration 199 : 0.508249494049474
Loss in iteration 200 : 0.5082380889675892
Testing accuracy  of updater 6 on alg 1 with rate 0.14 = 0.7815, training accuracy 0.788, time elapsed: 1984 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9702904905591863
Loss in iteration 3 : 0.9238507640728876
Loss in iteration 4 : 0.9117919028815884
Loss in iteration 5 : 0.9096502177976024
Loss in iteration 6 : 0.8700546732560254
Loss in iteration 7 : 0.8258952880840862
Loss in iteration 8 : 0.7917278187390032
Loss in iteration 9 : 0.7638711618664242
Loss in iteration 10 : 0.7442262864862634
Loss in iteration 11 : 0.7042797565791216
Loss in iteration 12 : 0.660089113696976
Loss in iteration 13 : 0.6398942787390038
Loss in iteration 14 : 0.6293685447764152
Loss in iteration 15 : 0.6132236502664836
Loss in iteration 16 : 0.5935937157941956
Loss in iteration 17 : 0.5824527362332143
Loss in iteration 18 : 0.5801440714062996
Loss in iteration 19 : 0.5748794028370117
Loss in iteration 20 : 0.5653046827026077
Loss in iteration 21 : 0.5596570124233985
Loss in iteration 22 : 0.55878446607785
Loss in iteration 23 : 0.5568938706570523
Loss in iteration 24 : 0.552700309895562
Loss in iteration 25 : 0.5498317234047952
Loss in iteration 26 : 0.5493537475478831
Loss in iteration 27 : 0.5488647279043836
Loss in iteration 28 : 0.5466052473114111
Loss in iteration 29 : 0.5439985643488285
Loss in iteration 30 : 0.5427814099857455
Loss in iteration 31 : 0.542352462187685
Loss in iteration 32 : 0.5414101696391939
Loss in iteration 33 : 0.5397331490835912
Loss in iteration 34 : 0.5383699498703416
Loss in iteration 35 : 0.5377814277782753
Loss in iteration 36 : 0.5373883446617541
Loss in iteration 37 : 0.5364906412477047
Loss in iteration 38 : 0.5351965041856317
Loss in iteration 39 : 0.5342109706805146
Loss in iteration 40 : 0.5336248773031991
Loss in iteration 41 : 0.5329814904226977
Loss in iteration 42 : 0.5320219716302427
Loss in iteration 43 : 0.5309956867795023
Loss in iteration 44 : 0.5302577546141058
Loss in iteration 45 : 0.5296815917798219
Loss in iteration 46 : 0.5289039082476805
Loss in iteration 47 : 0.527982487753663
Loss in iteration 48 : 0.5272162702808338
Loss in iteration 49 : 0.5266975781722658
Loss in iteration 50 : 0.526145257543628
Loss in iteration 51 : 0.5254239469919362
Loss in iteration 52 : 0.5247098670852051
Loss in iteration 53 : 0.524168925859989
Loss in iteration 54 : 0.5236974629633026
Loss in iteration 55 : 0.5231094458762382
Loss in iteration 56 : 0.52251360623209
Loss in iteration 57 : 0.5220665634493448
Loss in iteration 58 : 0.5216997226478844
Loss in iteration 59 : 0.5212799303955882
Loss in iteration 60 : 0.5208067269542092
Loss in iteration 61 : 0.5203816413273711
Loss in iteration 62 : 0.5200346514771013
Loss in iteration 63 : 0.5197021969445782
Loss in iteration 64 : 0.5193273829731522
Loss in iteration 65 : 0.5189610845027751
Loss in iteration 66 : 0.5186508957593948
Loss in iteration 67 : 0.5183820627492695
Loss in iteration 68 : 0.5181154335988005
Loss in iteration 69 : 0.5178268733616894
Loss in iteration 70 : 0.5175544572342705
Loss in iteration 71 : 0.5173193356715396
Loss in iteration 72 : 0.5171107054609282
Loss in iteration 73 : 0.5168960005561841
Loss in iteration 74 : 0.5166800556915256
Loss in iteration 75 : 0.5164775047767468
Loss in iteration 76 : 0.5162915146903942
Loss in iteration 77 : 0.5161109837183232
Loss in iteration 78 : 0.5159310790844911
Loss in iteration 79 : 0.5157625633614927
Loss in iteration 80 : 0.5156039458508489
Loss in iteration 81 : 0.5154492914337551
Loss in iteration 82 : 0.5152948417350696
Loss in iteration 83 : 0.5151465674916048
Loss in iteration 84 : 0.5150130466211668
Loss in iteration 85 : 0.5148781793475775
Loss in iteration 86 : 0.514745251814736
Loss in iteration 87 : 0.5146163676830738
Loss in iteration 88 : 0.5144923889085679
Loss in iteration 89 : 0.5143724296928773
Loss in iteration 90 : 0.5142538606146901
Loss in iteration 91 : 0.5141356020696329
Loss in iteration 92 : 0.5140201897785442
Loss in iteration 93 : 0.5139088728482762
Loss in iteration 94 : 0.513802597511335
Loss in iteration 95 : 0.5136982791146367
Loss in iteration 96 : 0.5135967272904306
Loss in iteration 97 : 0.513496694966408
Loss in iteration 98 : 0.5133988438409774
Loss in iteration 99 : 0.5133058438889275
Loss in iteration 100 : 0.5132156410303907
Loss in iteration 101 : 0.5131247972414171
Loss in iteration 102 : 0.5130356812945945
Loss in iteration 103 : 0.5129493442750879
Loss in iteration 104 : 0.5128662951134634
Loss in iteration 105 : 0.5127856244612873
Loss in iteration 106 : 0.5127049777467128
Loss in iteration 107 : 0.5126244780831737
Loss in iteration 108 : 0.5125474132967539
Loss in iteration 109 : 0.5124726708600131
Loss in iteration 110 : 0.5123980251125854
Loss in iteration 111 : 0.5123259230175524
Loss in iteration 112 : 0.5122571137569605
Loss in iteration 113 : 0.5121895125294295
Loss in iteration 114 : 0.5121220899550983
Loss in iteration 115 : 0.5120581244570849
Loss in iteration 116 : 0.5119951352965075
Loss in iteration 117 : 0.5119346157096227
Loss in iteration 118 : 0.5118767942199562
Loss in iteration 119 : 0.511820870506051
Loss in iteration 120 : 0.5117650953487116
Loss in iteration 121 : 0.5117105144370415
Loss in iteration 122 : 0.511657130713647
Loss in iteration 123 : 0.511603938773136
Loss in iteration 124 : 0.511551871032763
Loss in iteration 125 : 0.5115024087162142
Loss in iteration 126 : 0.5114533289317604
Loss in iteration 127 : 0.5114052517199734
Loss in iteration 128 : 0.5113585862242087
Loss in iteration 129 : 0.5113140592668698
Loss in iteration 130 : 0.5112705065686344
Loss in iteration 131 : 0.5112286760029906
Loss in iteration 132 : 0.5111871004840645
Loss in iteration 133 : 0.5111446931041346
Loss in iteration 134 : 0.5111036068384556
Loss in iteration 135 : 0.5110644366634517
Loss in iteration 136 : 0.5110265567580323
Loss in iteration 137 : 0.5109888087546187
Loss in iteration 138 : 0.5109509335944823
Loss in iteration 139 : 0.5109136971249537
Loss in iteration 140 : 0.510878274046641
Loss in iteration 141 : 0.5108440675007374
Loss in iteration 142 : 0.5108098218751191
Loss in iteration 143 : 0.510776364318736
Loss in iteration 144 : 0.5107428896251964
Loss in iteration 145 : 0.510709729947412
Loss in iteration 146 : 0.5106774914098026
Loss in iteration 147 : 0.5106455089309261
Loss in iteration 148 : 0.5106139033086604
Loss in iteration 149 : 0.510582613446402
Loss in iteration 150 : 0.5105525030867889
Loss in iteration 151 : 0.5105244946615749
Loss in iteration 152 : 0.5104962972355142
Loss in iteration 153 : 0.5104685271384237
Loss in iteration 154 : 0.5104407094316562
Loss in iteration 155 : 0.5104126806813991
Loss in iteration 156 : 0.5103856401185726
Loss in iteration 157 : 0.5103587162616542
Loss in iteration 158 : 0.5103321119293537
Loss in iteration 159 : 0.5103058988909941
Loss in iteration 160 : 0.5102805135993089
Loss in iteration 161 : 0.5102548478062645
Loss in iteration 162 : 0.5102305789820952
Loss in iteration 163 : 0.5102060886590677
Loss in iteration 164 : 0.5101824541215103
Loss in iteration 165 : 0.5101591972931273
Loss in iteration 166 : 0.5101359140652205
Loss in iteration 167 : 0.510113143391702
Loss in iteration 168 : 0.5100910839165175
Loss in iteration 169 : 0.5100695010472334
Loss in iteration 170 : 0.5100474935073658
Loss in iteration 171 : 0.5100261133862328
Loss in iteration 172 : 0.5100052080575018
Loss in iteration 173 : 0.5099836691076376
Loss in iteration 174 : 0.5099621307792416
Loss in iteration 175 : 0.5099416891857883
Loss in iteration 176 : 0.5099199201726451
Loss in iteration 177 : 0.5098991318659535
Loss in iteration 178 : 0.5098784805469966
Loss in iteration 179 : 0.5098574242267612
Loss in iteration 180 : 0.5098373041582216
Loss in iteration 181 : 0.5098169085364062
Loss in iteration 182 : 0.5097962451993494
Loss in iteration 183 : 0.5097757285338049
Loss in iteration 184 : 0.5097559939138245
Loss in iteration 185 : 0.5097358930010167
Loss in iteration 186 : 0.509716726715588
Loss in iteration 187 : 0.5096975432942776
Loss in iteration 188 : 0.5096787856109002
Loss in iteration 189 : 0.5096602341774477
Loss in iteration 190 : 0.5096410792823132
Loss in iteration 191 : 0.5096226754950469
Loss in iteration 192 : 0.5096046979972724
Loss in iteration 193 : 0.5095867998919906
Loss in iteration 194 : 0.5095693988442628
Loss in iteration 195 : 0.5095524009154324
Loss in iteration 196 : 0.509534720976664
Loss in iteration 197 : 0.5095176689036964
Loss in iteration 198 : 0.5095008513513619
Loss in iteration 199 : 0.5094840577425853
Loss in iteration 200 : 0.5094672849901573
Testing accuracy  of updater 6 on alg 1 with rate 0.08000000000000002 = 0.7805, training accuracy 0.787875, time elapsed: 2282 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.99179322524404
Loss in iteration 3 : 0.9764231857889257
Loss in iteration 4 : 0.9551827176099159
Loss in iteration 5 : 0.9324170488476204
Loss in iteration 6 : 0.9161220852404782
Loss in iteration 7 : 0.9098773126326056
Loss in iteration 8 : 0.9093114589141277
Loss in iteration 9 : 0.9057464122648068
Loss in iteration 10 : 0.8948092279817516
Loss in iteration 11 : 0.8780755363680528
Loss in iteration 12 : 0.8600255131239221
Loss in iteration 13 : 0.843387385943605
Loss in iteration 14 : 0.8289976284898912
Loss in iteration 15 : 0.8160499936972749
Loss in iteration 16 : 0.8035762716362749
Loss in iteration 17 : 0.7910567477830227
Loss in iteration 18 : 0.7782983859597337
Loss in iteration 19 : 0.7658448164813931
Loss in iteration 20 : 0.7534620980112283
Loss in iteration 21 : 0.7405698594374668
Loss in iteration 22 : 0.7268415184755036
Loss in iteration 23 : 0.7125114375282781
Loss in iteration 24 : 0.6987912587466962
Loss in iteration 25 : 0.6865230006785079
Loss in iteration 26 : 0.6758581317163882
Loss in iteration 27 : 0.6663456828988303
Loss in iteration 28 : 0.6573467515260227
Loss in iteration 29 : 0.6487137697431893
Loss in iteration 30 : 0.6406167080817365
Loss in iteration 31 : 0.6331124413198578
Loss in iteration 32 : 0.6263322959422521
Loss in iteration 33 : 0.6203498147647593
Loss in iteration 34 : 0.6151441264242454
Loss in iteration 35 : 0.6105351504673404
Loss in iteration 36 : 0.6062800079515203
Loss in iteration 37 : 0.60227206029708
Loss in iteration 38 : 0.5984852529255853
Loss in iteration 39 : 0.5949864187528704
Loss in iteration 40 : 0.5918645707586511
Loss in iteration 41 : 0.5891334321920081
Loss in iteration 42 : 0.5867313376736131
Loss in iteration 43 : 0.5844744044315685
Loss in iteration 44 : 0.5822846721492506
Loss in iteration 45 : 0.5801474598643627
Loss in iteration 46 : 0.5780687248132236
Loss in iteration 47 : 0.5760879973519805
Loss in iteration 48 : 0.574284258789849
Loss in iteration 49 : 0.5726754879813059
Loss in iteration 50 : 0.5712204508930248
Loss in iteration 51 : 0.569848061269417
Loss in iteration 52 : 0.5685199113482168
Loss in iteration 53 : 0.5672317558503327
Loss in iteration 54 : 0.5659856503088015
Loss in iteration 55 : 0.5648108023228993
Loss in iteration 56 : 0.5637417599762652
Loss in iteration 57 : 0.5627576262801262
Loss in iteration 58 : 0.5618305405799537
Loss in iteration 59 : 0.5609529653946075
Loss in iteration 60 : 0.5601002500626926
Loss in iteration 61 : 0.5592657850430193
Loss in iteration 62 : 0.5584405600543153
Loss in iteration 63 : 0.5576363935255119
Loss in iteration 64 : 0.5568717707754465
Loss in iteration 65 : 0.5561402683614727
Loss in iteration 66 : 0.5554581338941382
Loss in iteration 67 : 0.5548010714907533
Loss in iteration 68 : 0.5541523173243681
Loss in iteration 69 : 0.5535236922210823
Loss in iteration 70 : 0.5529185902857827
Loss in iteration 71 : 0.5523370986400089
Loss in iteration 72 : 0.5517747650032345
Loss in iteration 73 : 0.5512282402605847
Loss in iteration 74 : 0.5507001729273671
Loss in iteration 75 : 0.5501861203497072
Loss in iteration 76 : 0.5496782364000053
Loss in iteration 77 : 0.5491796380116665
Loss in iteration 78 : 0.5486924304190843
Loss in iteration 79 : 0.5482182269334734
Loss in iteration 80 : 0.5477588694282428
Loss in iteration 81 : 0.5473142185248718
Loss in iteration 82 : 0.5468788974192276
Loss in iteration 83 : 0.546451406748286
Loss in iteration 84 : 0.5460294517977656
Loss in iteration 85 : 0.545614250987923
Loss in iteration 86 : 0.5452063662663251
Loss in iteration 87 : 0.5448089023310398
Loss in iteration 88 : 0.5444236205353301
Loss in iteration 89 : 0.5440469851995718
Loss in iteration 90 : 0.5436789247597464
Loss in iteration 91 : 0.5433137994359913
Loss in iteration 92 : 0.5429515425447922
Loss in iteration 93 : 0.5425943163173897
Loss in iteration 94 : 0.5422448991153694
Loss in iteration 95 : 0.5419028892545625
Loss in iteration 96 : 0.5415683555800002
Loss in iteration 97 : 0.5412405668655622
Loss in iteration 98 : 0.5409168622978561
Loss in iteration 99 : 0.5405956593841225
Loss in iteration 100 : 0.5402786049606841
Loss in iteration 101 : 0.5399675628065017
Loss in iteration 102 : 0.5396617501419871
Loss in iteration 103 : 0.5393625441171376
Loss in iteration 104 : 0.5390712630799348
Loss in iteration 105 : 0.5387861045080933
Loss in iteration 106 : 0.5385048919507169
Loss in iteration 107 : 0.5382260447394163
Loss in iteration 108 : 0.5379501738057907
Loss in iteration 109 : 0.5376780202101552
Loss in iteration 110 : 0.5374100666210572
Loss in iteration 111 : 0.5371468352930725
Loss in iteration 112 : 0.5368897456756319
Loss in iteration 113 : 0.536637045086429
Loss in iteration 114 : 0.536386351989717
Loss in iteration 115 : 0.5361372064496807
Loss in iteration 116 : 0.5358917266132992
Loss in iteration 117 : 0.5356521882933173
Loss in iteration 118 : 0.5354171653133355
Loss in iteration 119 : 0.5351860884817292
Loss in iteration 120 : 0.5349576484568954
Loss in iteration 121 : 0.5347320647929359
Loss in iteration 122 : 0.5345081773780729
Loss in iteration 123 : 0.5342872332260351
Loss in iteration 124 : 0.5340689565894945
Loss in iteration 125 : 0.533852644468345
Loss in iteration 126 : 0.5336419798521818
Loss in iteration 127 : 0.5334337846293927
Loss in iteration 128 : 0.5332278236030151
Loss in iteration 129 : 0.5330244783119975
Loss in iteration 130 : 0.5328244013528582
Loss in iteration 131 : 0.5326292992936219
Loss in iteration 132 : 0.5324366770276133
Loss in iteration 133 : 0.5322455347533444
Loss in iteration 134 : 0.5320567354189202
Loss in iteration 135 : 0.5318699086999777
Loss in iteration 136 : 0.5316853553840888
Loss in iteration 137 : 0.5315040936218733
Loss in iteration 138 : 0.5313249641740309
Loss in iteration 139 : 0.5311477286514347
Loss in iteration 140 : 0.5309727831237878
Loss in iteration 141 : 0.5307989784508216
Loss in iteration 142 : 0.5306267242636492
Loss in iteration 143 : 0.5304553598605733
Loss in iteration 144 : 0.5302848160664665
Loss in iteration 145 : 0.5301150638344254
Loss in iteration 146 : 0.5299468799350625
Loss in iteration 147 : 0.529780385853336
Loss in iteration 148 : 0.5296150289224636
Loss in iteration 149 : 0.5294512935668424
Loss in iteration 150 : 0.529289397514211
Loss in iteration 151 : 0.5291287464069032
Loss in iteration 152 : 0.5289698531332812
Loss in iteration 153 : 0.5288127583722557
Loss in iteration 154 : 0.5286564078949657
Loss in iteration 155 : 0.528502191426106
Loss in iteration 156 : 0.5283498271412231
Loss in iteration 157 : 0.5281984283813351
Loss in iteration 158 : 0.5280492167449888
Loss in iteration 159 : 0.5279023440293499
Loss in iteration 160 : 0.5277576201487714
Loss in iteration 161 : 0.527614045799736
Loss in iteration 162 : 0.5274709291246629
Loss in iteration 163 : 0.5273287475403572
Loss in iteration 164 : 0.5271879542200676
Loss in iteration 165 : 0.5270483248031294
Loss in iteration 166 : 0.5269117261733111
Loss in iteration 167 : 0.5267770408960136
Loss in iteration 168 : 0.5266437084559735
Loss in iteration 169 : 0.5265120290283744
Loss in iteration 170 : 0.5263806745445743
Loss in iteration 171 : 0.5262496252444655
Loss in iteration 172 : 0.5261197312661836
Loss in iteration 173 : 0.5259911222741641
Loss in iteration 174 : 0.5258638997603057
Loss in iteration 175 : 0.5257381283021975
Loss in iteration 176 : 0.5256137770309788
Loss in iteration 177 : 0.5254907445039088
Loss in iteration 178 : 0.5253689868728136
Loss in iteration 179 : 0.525248031928626
Loss in iteration 180 : 0.5251278446268571
Loss in iteration 181 : 0.5250083496234383
Loss in iteration 182 : 0.5248896175535424
Loss in iteration 183 : 0.5247718656955677
Loss in iteration 184 : 0.5246555556032177
Loss in iteration 185 : 0.524539839629728
Loss in iteration 186 : 0.5244248973127246
Loss in iteration 187 : 0.5243103300748327
Loss in iteration 188 : 0.5241963266461029
Loss in iteration 189 : 0.5240830277743433
Loss in iteration 190 : 0.5239709694074999
Loss in iteration 191 : 0.5238595132966736
Loss in iteration 192 : 0.523749476924859
Loss in iteration 193 : 0.5236400387709569
Loss in iteration 194 : 0.5235313960442655
Loss in iteration 195 : 0.5234240682211649
Loss in iteration 196 : 0.5233173074145724
Loss in iteration 197 : 0.5232110477448096
Loss in iteration 198 : 0.523105398961226
Loss in iteration 199 : 0.5230003066173713
Loss in iteration 200 : 0.5228963904563977
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.777, training accuracy 0.783375, time elapsed: 2298 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 21.385250216909082
Loss in iteration 3 : 10.12475362748073
Loss in iteration 4 : 11.207942337773282
Loss in iteration 5 : 5.711671050904876
Loss in iteration 6 : 6.785650977398408
Loss in iteration 7 : 3.63435823185863
Loss in iteration 8 : 6.221338542391411
Loss in iteration 9 : 6.346555094729656
Loss in iteration 10 : 5.71551391524631
Loss in iteration 11 : 5.759177408470966
Loss in iteration 12 : 4.469657742779541
Loss in iteration 13 : 3.7057073511137006
Loss in iteration 14 : 4.281324775730311
Loss in iteration 15 : 4.328738072010546
Loss in iteration 16 : 4.141671538302031
Loss in iteration 17 : 4.0919041276255905
Loss in iteration 18 : 3.7501647955578377
Loss in iteration 19 : 3.6031593684272245
Loss in iteration 20 : 3.8302117667971824
Loss in iteration 21 : 3.7389073561929607
Loss in iteration 22 : 3.297896118699141
Loss in iteration 23 : 2.950818866908053
Loss in iteration 24 : 2.8342248262308596
Loss in iteration 25 : 2.851573846452566
Loss in iteration 26 : 2.976126714671964
Loss in iteration 27 : 2.9719485538869166
Loss in iteration 28 : 2.7014937312761758
Loss in iteration 29 : 2.4530618816033103
Loss in iteration 30 : 2.3166870215930238
Loss in iteration 31 : 2.2518336661054263
Loss in iteration 32 : 2.2402688198232603
Loss in iteration 33 : 2.130757877766546
Loss in iteration 34 : 1.9997790199060939
Loss in iteration 35 : 2.0112534172736614
Loss in iteration 36 : 1.9426148522819178
Loss in iteration 37 : 1.8746624082658305
Loss in iteration 38 : 1.710501901622516
Loss in iteration 39 : 1.675766404550031
Loss in iteration 40 : 1.6483643052109107
Loss in iteration 41 : 1.660402642970369
Loss in iteration 42 : 1.583505321045811
Loss in iteration 43 : 1.5307562359880726
Loss in iteration 44 : 1.4392075166394545
Loss in iteration 45 : 1.4337885573545577
Loss in iteration 46 : 1.3831217134083482
Loss in iteration 47 : 1.3740292703797212
Loss in iteration 48 : 1.3464774017603818
Loss in iteration 49 : 1.2723032851136669
Loss in iteration 50 : 1.2429934003988978
Loss in iteration 51 : 1.1871771617398508
Loss in iteration 52 : 1.1883961803909264
Loss in iteration 53 : 1.1996578977927401
Loss in iteration 54 : 1.074255036444661
Loss in iteration 55 : 1.0676177359081758
Loss in iteration 56 : 1.05188774526563
Loss in iteration 57 : 1.107637360026485
Loss in iteration 58 : 1.580958817131649
Loss in iteration 59 : 2.61309257717875
Loss in iteration 60 : 1.660064664154791
Loss in iteration 61 : 0.8635142462096406
Loss in iteration 62 : 1.2722946365068846
Loss in iteration 63 : 1.9969351864955363
Loss in iteration 64 : 1.5552023222605076
Loss in iteration 65 : 1.165823640326692
Loss in iteration 66 : 0.985626687518311
Loss in iteration 67 : 1.4077817104548802
Loss in iteration 68 : 1.5874720205736252
Loss in iteration 69 : 1.032032518113201
Loss in iteration 70 : 1.1065960738318943
Loss in iteration 71 : 0.9639236923000286
Loss in iteration 72 : 0.8588683247575878
Loss in iteration 73 : 1.0685123007758652
Loss in iteration 74 : 1.1345245342039862
Loss in iteration 75 : 2.0177772987786216
Loss in iteration 76 : 2.856070875568699
Loss in iteration 77 : 1.1904888769458366
Loss in iteration 78 : 0.8062196533418552
Loss in iteration 79 : 1.5637132016452306
Loss in iteration 80 : 2.321898677444902
Loss in iteration 81 : 1.2110140559928473
Loss in iteration 82 : 0.8681268331307145
Loss in iteration 83 : 1.0033051436527423
Loss in iteration 84 : 1.6634611983324856
Loss in iteration 85 : 2.095586689602664
Loss in iteration 86 : 1.3025635281905656
Loss in iteration 87 : 0.838798847524954
Loss in iteration 88 : 0.8233396282836966
Loss in iteration 89 : 1.0041195179117026
Loss in iteration 90 : 1.753520525673015
Loss in iteration 91 : 2.5103411612378173
Loss in iteration 92 : 1.0790971609470112
Loss in iteration 93 : 0.8353336929620679
Loss in iteration 94 : 1.6074193245352384
Loss in iteration 95 : 1.9498118033310456
Loss in iteration 96 : 1.1801699465249456
Loss in iteration 97 : 0.9208119279373252
Loss in iteration 98 : 0.9106764433003011
Loss in iteration 99 : 1.145257331160545
Loss in iteration 100 : 1.657921299719665
Loss in iteration 101 : 1.5357570513753034
Loss in iteration 102 : 1.1334448043179213
Loss in iteration 103 : 0.9471616119984781
Loss in iteration 104 : 0.9180180187308535
Loss in iteration 105 : 1.068415271544221
Loss in iteration 106 : 1.5892562485764696
Loss in iteration 107 : 2.311246986495408
Loss in iteration 108 : 1.2927399034062805
Loss in iteration 109 : 0.9074236557979523
Loss in iteration 110 : 0.8552787878340133
Loss in iteration 111 : 0.8610839779869097
Loss in iteration 112 : 0.9612253676104499
Loss in iteration 113 : 1.4419442021453848
Loss in iteration 114 : 2.0864520492573257
Loss in iteration 115 : 1.9999645105629256
Loss in iteration 116 : 1.0317919434047917
Loss in iteration 117 : 0.8179042522974311
Loss in iteration 118 : 0.77406725817238
Loss in iteration 119 : 0.7712338453859054
Loss in iteration 120 : 0.7905120628098938
Loss in iteration 121 : 0.9914765048565805
Loss in iteration 122 : 2.1678986734020254
Loss in iteration 123 : 2.3846308065710637
Loss in iteration 124 : 1.4772838153526153
Loss in iteration 125 : 0.9234164279042911
Loss in iteration 126 : 0.8266838643783019
Loss in iteration 127 : 0.8286372730710259
Loss in iteration 128 : 0.8766176823413473
Loss in iteration 129 : 1.1804062387345287
Loss in iteration 130 : 1.9648529011774845
Loss in iteration 131 : 1.6727551284066626
Loss in iteration 132 : 1.3492173186176062
Loss in iteration 133 : 1.067931620529894
Loss in iteration 134 : 1.086472282979274
Loss in iteration 135 : 1.188404701642189
Loss in iteration 136 : 1.4642383807955324
Loss in iteration 137 : 1.357416204380847
Loss in iteration 138 : 1.2530789747889057
Loss in iteration 139 : 1.0107451147231281
Loss in iteration 140 : 0.9233783667619945
Loss in iteration 141 : 0.9252605116126187
Loss in iteration 142 : 1.0707328040558663
Loss in iteration 143 : 1.4436118763049108
Loss in iteration 144 : 2.0871167075211807
Loss in iteration 145 : 1.3429345045831633
Loss in iteration 146 : 1.0569459350024204
Loss in iteration 147 : 1.0214383189486198
Loss in iteration 148 : 1.240686675113362
Loss in iteration 149 : 1.483006107255022
Loss in iteration 150 : 1.6955220294548352
Loss in iteration 151 : 1.2270705421864327
Loss in iteration 152 : 1.0023381310163344
Loss in iteration 153 : 0.9396217080870632
Loss in iteration 154 : 0.9644073957955134
Loss in iteration 155 : 1.1779743708928732
Loss in iteration 156 : 1.7927597913221185
Loss in iteration 157 : 1.577434991613583
Loss in iteration 158 : 1.3976537190881777
Loss in iteration 159 : 1.1717526969002876
Loss in iteration 160 : 1.2303184868829002
Loss in iteration 161 : 1.240214619933061
Loss in iteration 162 : 1.3325833798231557
Loss in iteration 163 : 1.2004362794185761
Loss in iteration 164 : 1.1497816838971124
Loss in iteration 165 : 1.0671267617693148
Loss in iteration 166 : 1.0703165615403099
Loss in iteration 167 : 1.170195661531976
Loss in iteration 168 : 1.5020943825528497
Loss in iteration 169 : 1.5520494288627185
Loss in iteration 170 : 1.5747328981210587
Loss in iteration 171 : 1.2566181441847182
Loss in iteration 172 : 1.1782448960919147
Loss in iteration 173 : 1.1303930321075586
Loss in iteration 174 : 1.2666598648556973
Loss in iteration 175 : 1.324292622440422
Loss in iteration 176 : 1.4652503479862697
Loss in iteration 177 : 1.2877459875836863
Loss in iteration 178 : 1.2293138327214297
Loss in iteration 179 : 1.086665565786655
Loss in iteration 180 : 1.0871271006624557
Loss in iteration 181 : 1.161089210088872
Loss in iteration 182 : 1.547364865174156
Loss in iteration 183 : 1.580926173734155
Loss in iteration 184 : 1.5935113007131503
Loss in iteration 185 : 1.1927437256806366
Loss in iteration 186 : 1.1087759172245282
Loss in iteration 187 : 1.097213793324043
Loss in iteration 188 : 1.2064100437776188
Loss in iteration 189 : 1.2894789524914643
Loss in iteration 190 : 1.4811949800309303
Loss in iteration 191 : 1.3814824097890992
Loss in iteration 192 : 1.3847824760529028
Loss in iteration 193 : 1.2072210099023992
Loss in iteration 194 : 1.2099648902890723
Loss in iteration 195 : 1.1837329451754859
Loss in iteration 196 : 1.3406196585055181
Loss in iteration 197 : 1.3807832665467332
Loss in iteration 198 : 1.5127997298803055
Loss in iteration 199 : 1.33744016689723
Loss in iteration 200 : 1.3002391922858363
Testing accuracy  of updater 7 on alg 1 with rate 20.0 = 0.708, training accuracy 0.71775, time elapsed: 2758 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.00176606559238
Loss in iteration 3 : 0.9480960955655451
Loss in iteration 4 : 0.6412815137044363
Loss in iteration 5 : 0.6667791809763897
Loss in iteration 6 : 1.0963662858118215
Loss in iteration 7 : 1.863740667220231
Loss in iteration 8 : 0.9907068698322713
Loss in iteration 9 : 0.8411022987760458
Loss in iteration 10 : 1.348493060885999
Loss in iteration 11 : 1.0199438643033885
Loss in iteration 12 : 0.9216091284620334
Loss in iteration 13 : 1.2393133196235235
Loss in iteration 14 : 0.8676034943989328
Loss in iteration 15 : 0.9925533516764483
Loss in iteration 16 : 1.012259229108512
Loss in iteration 17 : 0.8126353181967264
Loss in iteration 18 : 0.9519225564751387
Loss in iteration 19 : 0.8908531816262749
Loss in iteration 20 : 0.7669900634151231
Loss in iteration 21 : 0.8486811295107377
Loss in iteration 22 : 0.8646654762402566
Loss in iteration 23 : 0.7808207114062905
Loss in iteration 24 : 0.7137821799612201
Loss in iteration 25 : 0.7018614470501382
Loss in iteration 26 : 0.7733268825657124
Loss in iteration 27 : 0.903939215594347
Loss in iteration 28 : 1.230185326012996
Loss in iteration 29 : 1.3214773653749938
Loss in iteration 30 : 1.0556239685687483
Loss in iteration 31 : 0.9422183088480963
Loss in iteration 32 : 0.9869584150325755
Loss in iteration 33 : 1.1587260311344725
Loss in iteration 34 : 1.305205333120443
Loss in iteration 35 : 1.0010851474247269
Loss in iteration 36 : 0.8020409823984185
Loss in iteration 37 : 0.7395298548321817
Loss in iteration 38 : 0.6942259190184005
Loss in iteration 39 : 0.7121630501328484
Loss in iteration 40 : 0.7542810039036186
Loss in iteration 41 : 0.9669988723589897
Loss in iteration 42 : 1.3377149364431467
Loss in iteration 43 : 1.1098419801103838
Loss in iteration 44 : 0.8716684720548765
Loss in iteration 45 : 0.7682822550966516
Loss in iteration 46 : 0.7432791684644464
Loss in iteration 47 : 0.7899411844461299
Loss in iteration 48 : 0.849049977711004
Loss in iteration 49 : 0.9819545406703757
Loss in iteration 50 : 1.0903364604553636
Loss in iteration 51 : 1.0422383904266066
Loss in iteration 52 : 0.9260038856076528
Loss in iteration 53 : 0.9117508389281015
Loss in iteration 54 : 0.9731645531786721
Loss in iteration 55 : 1.1986763900985935
Loss in iteration 56 : 1.162080972768773
Loss in iteration 57 : 1.0576365965555843
Loss in iteration 58 : 0.9205950823317784
Loss in iteration 59 : 0.9119604296069242
Loss in iteration 60 : 0.8923969536168345
Loss in iteration 61 : 0.9709068111510237
Loss in iteration 62 : 1.02541615746238
Loss in iteration 63 : 1.019291870740771
Loss in iteration 64 : 0.9282805064001459
Loss in iteration 65 : 0.8728245291114148
Loss in iteration 66 : 0.8567083052775069
Loss in iteration 67 : 0.9587385950880518
Loss in iteration 68 : 1.0243056665193666
Loss in iteration 69 : 1.0632772403973767
Loss in iteration 70 : 0.9838077721880661
Loss in iteration 71 : 0.9565524054696474
Loss in iteration 72 : 0.8869903671184187
Loss in iteration 73 : 0.9291051410892128
Loss in iteration 74 : 0.9965317528451558
Loss in iteration 75 : 1.080613430294139
Loss in iteration 76 : 1.0412522996792795
Loss in iteration 77 : 0.9737629779150786
Loss in iteration 78 : 0.9234270373117763
Loss in iteration 79 : 0.9062032769068961
Loss in iteration 80 : 0.8913453367474417
Loss in iteration 81 : 0.9382324214747498
Loss in iteration 82 : 1.0546773037452895
Loss in iteration 83 : 1.120493172133527
Loss in iteration 84 : 1.0870223580763378
Loss in iteration 85 : 0.9408632560823696
Loss in iteration 86 : 0.8644972891097079
Loss in iteration 87 : 0.8635912876452655
Loss in iteration 88 : 0.8734817472561509
Loss in iteration 89 : 0.9504921227036256
Loss in iteration 90 : 1.0479374643514174
Loss in iteration 91 : 1.0840257779301903
Loss in iteration 92 : 1.0274294820760006
Loss in iteration 93 : 0.934632309375289
Loss in iteration 94 : 0.9004470949403267
Loss in iteration 95 : 0.9172927065670107
Loss in iteration 96 : 0.9451771402626951
Loss in iteration 97 : 1.0210907594794751
Loss in iteration 98 : 1.1106751890467523
Loss in iteration 99 : 1.0833026406705097
Loss in iteration 100 : 1.006797010458753
Loss in iteration 101 : 0.9294511815699291
Loss in iteration 102 : 0.9037032401340116
Loss in iteration 103 : 0.911424524878934
Loss in iteration 104 : 0.9112568346175468
Loss in iteration 105 : 0.9801890636998357
Loss in iteration 106 : 1.0777615364470072
Loss in iteration 107 : 1.1265855999126515
Loss in iteration 108 : 1.0424350482206315
Loss in iteration 109 : 0.9762816412390617
Loss in iteration 110 : 0.9127735598394784
Loss in iteration 111 : 0.9292605125140231
Loss in iteration 112 : 0.912717252748795
Loss in iteration 113 : 0.9889847865403977
Loss in iteration 114 : 1.0423068063654741
Loss in iteration 115 : 1.0904703781575453
Loss in iteration 116 : 1.0074285837121444
Loss in iteration 117 : 0.9768495248346296
Loss in iteration 118 : 0.9496652315541071
Loss in iteration 119 : 0.9865883676828469
Loss in iteration 120 : 0.9874205481929051
Loss in iteration 121 : 1.0490157585625581
Loss in iteration 122 : 1.0736964055303944
Loss in iteration 123 : 1.0717589047686382
Loss in iteration 124 : 1.011519510793567
Loss in iteration 125 : 0.9525246415229619
Loss in iteration 126 : 0.9288442817278458
Loss in iteration 127 : 0.9240964173332353
Loss in iteration 128 : 0.9189034105233684
Loss in iteration 129 : 0.9664264998611295
Loss in iteration 130 : 1.0773876282642696
Loss in iteration 131 : 1.1548787986752205
Loss in iteration 132 : 1.1235515164850183
Loss in iteration 133 : 0.9891274268067617
Loss in iteration 134 : 0.9055854113070042
Loss in iteration 135 : 0.8994984614216033
Loss in iteration 136 : 0.8917728601986399
Loss in iteration 137 : 0.9960592499692816
Loss in iteration 138 : 1.1269434672176102
Loss in iteration 139 : 1.1675717467555364
Loss in iteration 140 : 1.0662506186473042
Loss in iteration 141 : 0.9621664742719277
Loss in iteration 142 : 0.9237473926594203
Loss in iteration 143 : 0.9511313808604455
Loss in iteration 144 : 0.992618343595793
Loss in iteration 145 : 1.0614758460609703
Loss in iteration 146 : 1.1204590214335135
Loss in iteration 147 : 1.079198615088606
Loss in iteration 148 : 1.0116407377116188
Loss in iteration 149 : 0.9629743850700679
Loss in iteration 150 : 0.9412294689257799
Loss in iteration 151 : 0.9564383083416276
Loss in iteration 152 : 0.9470085261097744
Loss in iteration 153 : 0.9978472491019243
Loss in iteration 154 : 1.0890147885342472
Loss in iteration 155 : 1.1922393377708098
Loss in iteration 156 : 1.1155517674630406
Loss in iteration 157 : 1.014824910598741
Loss in iteration 158 : 0.9472427605901869
Loss in iteration 159 : 0.943215480322408
Loss in iteration 160 : 0.9362752188062654
Loss in iteration 161 : 0.988594226000463
Loss in iteration 162 : 1.1030773637600564
Loss in iteration 163 : 1.1383193437805001
Loss in iteration 164 : 1.1159733070163138
Loss in iteration 165 : 1.0180525205096498
Loss in iteration 166 : 0.9641222743111668
Loss in iteration 167 : 0.9357041564867737
Loss in iteration 168 : 0.9142062883332325
Loss in iteration 169 : 0.9978530256388449
Loss in iteration 170 : 1.1492305359100508
Loss in iteration 171 : 1.222810363921521
Loss in iteration 172 : 1.0863861973723326
Loss in iteration 173 : 0.9590765814628749
Loss in iteration 174 : 0.9048338232949005
Loss in iteration 175 : 0.9505412560515604
Loss in iteration 176 : 0.9984859199445403
Loss in iteration 177 : 1.0980709831513766
Loss in iteration 178 : 1.1764210079051167
Loss in iteration 179 : 1.1119502810063902
Loss in iteration 180 : 1.0242093182881564
Loss in iteration 181 : 0.977046025322458
Loss in iteration 182 : 0.9970609539137493
Loss in iteration 183 : 1.0150477470872212
Loss in iteration 184 : 1.037616445547938
Loss in iteration 185 : 1.0592588436797536
Loss in iteration 186 : 1.0947973849423178
Loss in iteration 187 : 1.1053278087103628
Loss in iteration 188 : 1.0736345538819938
Loss in iteration 189 : 1.0362323807402793
Loss in iteration 190 : 0.9720053255030536
Loss in iteration 191 : 0.9700645036391904
Loss in iteration 192 : 0.9160429709530047
Loss in iteration 193 : 1.0208973867103937
Loss in iteration 194 : 1.1397743364021675
Loss in iteration 195 : 1.2595764733593382
Loss in iteration 196 : 1.1167259771903528
Loss in iteration 197 : 1.0157060926316563
Loss in iteration 198 : 0.9534269322788205
Loss in iteration 199 : 0.9680952045994088
Loss in iteration 200 : 0.9982881730466132
Testing accuracy  of updater 7 on alg 1 with rate 14.0 = 0.691, training accuracy 0.697, time elapsed: 4113 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.69042057266785
Loss in iteration 3 : 0.724383044461136
Loss in iteration 4 : 1.806309112229789
Loss in iteration 5 : 1.4026436501252175
Loss in iteration 6 : 0.9954745144524014
Loss in iteration 7 : 1.7197024815412578
Loss in iteration 8 : 1.0131399922674256
Loss in iteration 9 : 1.5324831071394176
Loss in iteration 10 : 1.1165973343599147
Loss in iteration 11 : 1.2245104079587577
Loss in iteration 12 : 1.4481905260410803
Loss in iteration 13 : 1.038320543080369
Loss in iteration 14 : 1.2580251437674965
Loss in iteration 15 : 0.8853805232940397
Loss in iteration 16 : 1.0997238521933395
Loss in iteration 17 : 1.2101349344435985
Loss in iteration 18 : 0.973964593019994
Loss in iteration 19 : 0.9983779115343266
Loss in iteration 20 : 0.9207007721812528
Loss in iteration 21 : 0.9830372060911264
Loss in iteration 22 : 1.0551904009090904
Loss in iteration 23 : 0.8589321839387203
Loss in iteration 24 : 0.8820349449118846
Loss in iteration 25 : 0.880559446315365
Loss in iteration 26 : 0.8753176584655694
Loss in iteration 27 : 0.8753569321966832
Loss in iteration 28 : 0.7414865411060856
Loss in iteration 29 : 0.8442148315995042
Loss in iteration 30 : 0.7357667627633386
Loss in iteration 31 : 0.7894469903710184
Loss in iteration 32 : 0.6825099876382992
Loss in iteration 33 : 0.7664615434596689
Loss in iteration 34 : 0.6596239106987314
Loss in iteration 35 : 0.7200780372117729
Loss in iteration 36 : 0.6502353051244365
Loss in iteration 37 : 0.6829399438680771
Loss in iteration 38 : 0.6480157413722417
Loss in iteration 39 : 0.6367429332318031
Loss in iteration 40 : 0.6589946119908884
Loss in iteration 41 : 0.608360683322395
Loss in iteration 42 : 0.6240348717775658
Loss in iteration 43 : 0.6283240670672527
Loss in iteration 44 : 0.6151828496716658
Loss in iteration 45 : 0.6032587085245951
Loss in iteration 46 : 0.579603147270014
Loss in iteration 47 : 0.6062673056891945
Loss in iteration 48 : 0.5863242681622585
Loss in iteration 49 : 0.5884485676812223
Loss in iteration 50 : 0.5867127860809256
Loss in iteration 51 : 0.5779794912510033
Loss in iteration 52 : 0.6107256872980829
Loss in iteration 53 : 0.6100334838061428
Loss in iteration 54 : 0.6914639997431792
Loss in iteration 55 : 0.6744527604685268
Loss in iteration 56 : 0.6803040748718172
Loss in iteration 57 : 0.6029056336873412
Loss in iteration 58 : 0.5576075721704978
Loss in iteration 59 : 0.5411080466400476
Loss in iteration 60 : 0.5292535564702959
Loss in iteration 61 : 0.5433801670586117
Loss in iteration 62 : 0.5477903242688376
Loss in iteration 63 : 0.6122011980794033
Loss in iteration 64 : 0.7245729177185988
Loss in iteration 65 : 0.7767517066165229
Loss in iteration 66 : 0.7300487463198528
Loss in iteration 67 : 0.6204790005898256
Loss in iteration 68 : 0.5675039352702049
Loss in iteration 69 : 0.5509753995674241
Loss in iteration 70 : 0.5438199389452352
Loss in iteration 71 : 0.5489287727135312
Loss in iteration 72 : 0.5696923084912175
Loss in iteration 73 : 0.6481339115074006
Loss in iteration 74 : 0.8142320735782969
Loss in iteration 75 : 0.8476864130744605
Loss in iteration 76 : 0.6965094221687411
Loss in iteration 77 : 0.6334178839722945
Loss in iteration 78 : 0.6088866258617438
Loss in iteration 79 : 0.6459803693903075
Loss in iteration 80 : 0.683802371718717
Loss in iteration 81 : 0.6786878403947613
Loss in iteration 82 : 0.613800178085742
Loss in iteration 83 : 0.6105500676878799
Loss in iteration 84 : 0.5701856579874088
Loss in iteration 85 : 0.5637399825723047
Loss in iteration 86 : 0.579462282886891
Loss in iteration 87 : 0.534649948659464
Loss in iteration 88 : 0.5966338932996997
Loss in iteration 89 : 0.5362551650092872
Loss in iteration 90 : 0.6695232045414478
Loss in iteration 91 : 0.7373220929599513
Loss in iteration 92 : 0.8911016298213423
Loss in iteration 93 : 0.7579402517696173
Loss in iteration 94 : 0.5837376806451641
Loss in iteration 95 : 0.5507743949508085
Loss in iteration 96 : 0.5492246676788672
Loss in iteration 97 : 0.6355033311236732
Loss in iteration 98 : 0.7364007079659464
Loss in iteration 99 : 0.7408693155551278
Loss in iteration 100 : 0.6742695593864639
Loss in iteration 101 : 0.6244460612517865
Loss in iteration 102 : 0.6095796066140676
Loss in iteration 103 : 0.6091292544148635
Loss in iteration 104 : 0.6327646990167666
Loss in iteration 105 : 0.6541870366882495
Loss in iteration 106 : 0.6558997936376115
Loss in iteration 107 : 0.6279122337148237
Loss in iteration 108 : 0.5989695020524483
Loss in iteration 109 : 0.5826837247870456
Loss in iteration 110 : 0.5759670787759608
Loss in iteration 111 : 0.5878220018076882
Loss in iteration 112 : 0.6208223157451953
Loss in iteration 113 : 0.6932938482437568
Loss in iteration 114 : 0.73485271750955
Loss in iteration 115 : 0.7051601676089626
Loss in iteration 116 : 0.6495706585656758
Loss in iteration 117 : 0.6362702074212083
Loss in iteration 118 : 0.6282423494649056
Loss in iteration 119 : 0.6546314003064782
Loss in iteration 120 : 0.6556881826006209
Loss in iteration 121 : 0.6690804231170971
Loss in iteration 122 : 0.6476592199715517
Loss in iteration 123 : 0.6417868303494647
Loss in iteration 124 : 0.6262939411104711
Loss in iteration 125 : 0.6398708670130683
Loss in iteration 126 : 0.6448079291711192
Loss in iteration 127 : 0.669896484211263
Loss in iteration 128 : 0.6840500548649457
Loss in iteration 129 : 0.6934697387122363
Loss in iteration 130 : 0.6616453846793993
Loss in iteration 131 : 0.6371464425975377
Loss in iteration 132 : 0.6002708561828627
Loss in iteration 133 : 0.5903736279460677
Loss in iteration 134 : 0.583978909431891
Loss in iteration 135 : 0.6042659232932528
Loss in iteration 136 : 0.6290388701338585
Loss in iteration 137 : 0.6977078302409898
Loss in iteration 138 : 0.7466272231220514
Loss in iteration 139 : 0.7342007682055248
Loss in iteration 140 : 0.6438340885452887
Loss in iteration 141 : 0.6421821958486532
Loss in iteration 142 : 0.5959878953052423
Loss in iteration 143 : 0.6312629069978809
Loss in iteration 144 : 0.6177789017468333
Loss in iteration 145 : 0.6797023216214582
Loss in iteration 146 : 0.6637710511192649
Loss in iteration 147 : 0.711244786342558
Loss in iteration 148 : 0.6375119914385685
Loss in iteration 149 : 0.6902998316094904
Loss in iteration 150 : 0.6194426506424536
Loss in iteration 151 : 0.6991226546460189
Loss in iteration 152 : 0.6513082127637575
Loss in iteration 153 : 0.7305836220279687
Loss in iteration 154 : 0.6594693777810134
Loss in iteration 155 : 0.6783386153889256
Loss in iteration 156 : 0.6250999717849136
Loss in iteration 157 : 0.6409120939731543
Loss in iteration 158 : 0.6177273550650039
Loss in iteration 159 : 0.6547201269470727
Loss in iteration 160 : 0.6742611231698927
Loss in iteration 161 : 0.7117519251432834
Loss in iteration 162 : 0.7009766152230449
Loss in iteration 163 : 0.6837775610926708
Loss in iteration 164 : 0.650673705673222
Loss in iteration 165 : 0.6315831076189836
Loss in iteration 166 : 0.6288666241733604
Loss in iteration 167 : 0.6495114542972895
Loss in iteration 168 : 0.6802009334350878
Loss in iteration 169 : 0.7059222719310084
Loss in iteration 170 : 0.6826120831068493
Loss in iteration 171 : 0.6610915650827021
Loss in iteration 172 : 0.6390210280500654
Loss in iteration 173 : 0.6457959991883925
Loss in iteration 174 : 0.6570834132747684
Loss in iteration 175 : 0.6902019450764971
Loss in iteration 176 : 0.6924491756901885
Loss in iteration 177 : 0.6857595597022703
Loss in iteration 178 : 0.6629918240046221
Loss in iteration 179 : 0.6466153948156068
Loss in iteration 180 : 0.6465578579059054
Loss in iteration 181 : 0.6672367789425018
Loss in iteration 182 : 0.6978947619626538
Loss in iteration 183 : 0.7051522628848687
Loss in iteration 184 : 0.6767358810172225
Loss in iteration 185 : 0.6469096026133685
Loss in iteration 186 : 0.6453913457127662
Loss in iteration 187 : 0.6534180060513718
Loss in iteration 188 : 0.6820623883467803
Loss in iteration 189 : 0.6971707413692205
Loss in iteration 190 : 0.6895993424638824
Loss in iteration 191 : 0.6667849460814043
Loss in iteration 192 : 0.66007927722354
Loss in iteration 193 : 0.6588253416678956
Loss in iteration 194 : 0.6698728455814509
Loss in iteration 195 : 0.6759504471119094
Loss in iteration 196 : 0.6805526295167024
Loss in iteration 197 : 0.683882265110024
Loss in iteration 198 : 0.6847111488843245
Loss in iteration 199 : 0.6796449226358595
Loss in iteration 200 : 0.6724051241731367
Testing accuracy  of updater 7 on alg 1 with rate 8.0 = 0.737, training accuracy 0.747, time elapsed: 2784 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9903636984331358
Loss in iteration 3 : 0.8762773106066986
Loss in iteration 4 : 0.8250432800608524
Loss in iteration 5 : 0.731976367183275
Loss in iteration 6 : 0.6636742313099441
Loss in iteration 7 : 0.6104582850320763
Loss in iteration 8 : 0.6066799676665245
Loss in iteration 9 : 0.5688945942406116
Loss in iteration 10 : 0.574733705118266
Loss in iteration 11 : 0.561905919796294
Loss in iteration 12 : 0.5643428023511281
Loss in iteration 13 : 0.5670290034293702
Loss in iteration 14 : 0.5602297007388525
Loss in iteration 15 : 0.567157455764362
Loss in iteration 16 : 0.5604541931582907
Loss in iteration 17 : 0.5626911100521579
Loss in iteration 18 : 0.5654518070403436
Loss in iteration 19 : 0.5601680010926886
Loss in iteration 20 : 0.5620874997009263
Loss in iteration 21 : 0.5587564875142277
Loss in iteration 22 : 0.5548514739233735
Loss in iteration 23 : 0.5546030718712882
Loss in iteration 24 : 0.5489257714514818
Loss in iteration 25 : 0.5461669405751789
Loss in iteration 26 : 0.5441276977215261
Loss in iteration 27 : 0.5388773970775148
Loss in iteration 28 : 0.5372082569367744
Loss in iteration 29 : 0.5335638691974298
Loss in iteration 30 : 0.5300250664432751
Loss in iteration 31 : 0.5279179407982311
Loss in iteration 32 : 0.5239970018396952
Loss in iteration 33 : 0.5225206797552054
Loss in iteration 34 : 0.5204761193345471
Loss in iteration 35 : 0.5186800071906436
Loss in iteration 36 : 0.5172727082654597
Loss in iteration 37 : 0.5155647373143115
Loss in iteration 38 : 0.5151932539379311
Loss in iteration 39 : 0.5145194862655992
Loss in iteration 40 : 0.5146515712635048
Loss in iteration 41 : 0.514055481014628
Loss in iteration 42 : 0.5140665640357849
Loss in iteration 43 : 0.513518356446892
Loss in iteration 44 : 0.5136776628029901
Loss in iteration 45 : 0.513622114587921
Loss in iteration 46 : 0.5137540751749211
Loss in iteration 47 : 0.5134200037593476
Loss in iteration 48 : 0.5133735844339908
Loss in iteration 49 : 0.5130399488434837
Loss in iteration 50 : 0.513056400560212
Loss in iteration 51 : 0.512799637047054
Loss in iteration 52 : 0.512797396539102
Loss in iteration 53 : 0.5123426016509022
Loss in iteration 54 : 0.5120970717961275
Loss in iteration 55 : 0.5117893956134691
Loss in iteration 56 : 0.5115812017885155
Loss in iteration 57 : 0.5112964083973812
Loss in iteration 58 : 0.511036148413579
Loss in iteration 59 : 0.5108284672978761
Loss in iteration 60 : 0.510498268635161
Loss in iteration 61 : 0.5104299052026141
Loss in iteration 62 : 0.5102021978224626
Loss in iteration 63 : 0.51009833187032
Loss in iteration 64 : 0.5098506350170388
Loss in iteration 65 : 0.509830602000476
Loss in iteration 66 : 0.5095943639572585
Loss in iteration 67 : 0.5095603777669777
Loss in iteration 68 : 0.5094476405312015
Loss in iteration 69 : 0.5093652917917678
Loss in iteration 70 : 0.5092688892213288
Loss in iteration 71 : 0.5092159038994241
Loss in iteration 72 : 0.5091262190689747
Loss in iteration 73 : 0.5090759684238019
Loss in iteration 74 : 0.509004792306007
Loss in iteration 75 : 0.508955758451335
Loss in iteration 76 : 0.5088859296333824
Loss in iteration 77 : 0.5088283114644792
Loss in iteration 78 : 0.5087846925699502
Loss in iteration 79 : 0.5087261979216491
Loss in iteration 80 : 0.5086652473008985
Loss in iteration 81 : 0.5086308940924615
Loss in iteration 82 : 0.5085582775592469
Loss in iteration 83 : 0.508528935053271
Loss in iteration 84 : 0.5084570311455419
Loss in iteration 85 : 0.5084302286795185
Loss in iteration 86 : 0.5083693832937246
Loss in iteration 87 : 0.5083380522667739
Loss in iteration 88 : 0.5082829069605488
Loss in iteration 89 : 0.5082497535830967
Loss in iteration 90 : 0.508206473503524
Loss in iteration 91 : 0.5081783402172633
Loss in iteration 92 : 0.508139892427036
Loss in iteration 93 : 0.5081144325836274
Loss in iteration 94 : 0.5080761251431786
Loss in iteration 95 : 0.5080531211815601
Loss in iteration 96 : 0.5080213694948355
Loss in iteration 97 : 0.5079965290822298
Loss in iteration 98 : 0.5079698281768468
Loss in iteration 99 : 0.5079464249210198
Loss in iteration 100 : 0.5079192847209597
Loss in iteration 101 : 0.507894127660155
Loss in iteration 102 : 0.5078693165361542
Loss in iteration 103 : 0.5078473814329709
Loss in iteration 104 : 0.5078183273965905
Loss in iteration 105 : 0.5077960248602227
Loss in iteration 106 : 0.5077712768963492
Loss in iteration 107 : 0.5077499327460318
Loss in iteration 108 : 0.5077290406075228
Loss in iteration 109 : 0.5077018430261802
Loss in iteration 110 : 0.5076781531630289
Loss in iteration 111 : 0.5076587779406712
Loss in iteration 112 : 0.507637056111167
Loss in iteration 113 : 0.5076114509069326
Loss in iteration 114 : 0.5075877440000806
Loss in iteration 115 : 0.5075730301573808
Loss in iteration 116 : 0.5075494962012638
Loss in iteration 117 : 0.5075332016746983
Loss in iteration 118 : 0.5075096535659972
Loss in iteration 119 : 0.5074939099907203
Loss in iteration 120 : 0.5074761260775889
Loss in iteration 121 : 0.5074584279314118
Loss in iteration 122 : 0.507440696516257
Loss in iteration 123 : 0.5074242134684094
Loss in iteration 124 : 0.5074090677005272
Loss in iteration 125 : 0.5073919651643134
Loss in iteration 126 : 0.5073737221982735
Loss in iteration 127 : 0.5073583609556658
Loss in iteration 128 : 0.5073439177224657
Loss in iteration 129 : 0.5073273489507382
Loss in iteration 130 : 0.5073132278485828
Loss in iteration 131 : 0.5072973316932367
Loss in iteration 132 : 0.5072854400976108
Loss in iteration 133 : 0.5072706598391462
Loss in iteration 134 : 0.507256533537809
Loss in iteration 135 : 0.5072445192943887
Loss in iteration 136 : 0.507232664773456
Loss in iteration 137 : 0.5072153137224781
Loss in iteration 138 : 0.5072052734911984
Loss in iteration 139 : 0.5071894328561865
Loss in iteration 140 : 0.5071768648122627
Loss in iteration 141 : 0.5071661733238303
Loss in iteration 142 : 0.5071523667356345
Loss in iteration 143 : 0.5071403755616601
Loss in iteration 144 : 0.5071295655981577
Loss in iteration 145 : 0.5071176334796822
Loss in iteration 146 : 0.5071059583474674
Loss in iteration 147 : 0.507096854583432
Loss in iteration 148 : 0.507084499478479
Loss in iteration 149 : 0.5070738910776635
Loss in iteration 150 : 0.5070637768816956
Loss in iteration 151 : 0.5070533176454862
Loss in iteration 152 : 0.5070423322482254
Loss in iteration 153 : 0.5070335778472672
Loss in iteration 154 : 0.5070229629389927
Loss in iteration 155 : 0.5070136436641663
Loss in iteration 156 : 0.5070038157688358
Loss in iteration 157 : 0.5069934658606731
Loss in iteration 158 : 0.5069827545108146
Loss in iteration 159 : 0.5069741707900127
Loss in iteration 160 : 0.5069649020414579
Loss in iteration 161 : 0.5069549548452427
Loss in iteration 162 : 0.5069499001888386
Loss in iteration 163 : 0.506944133580183
Loss in iteration 164 : 0.5069310643313794
Loss in iteration 165 : 0.5069315547424585
Loss in iteration 166 : 0.5069101420429365
Loss in iteration 167 : 0.5069069915219365
Loss in iteration 168 : 0.5068944163470585
Loss in iteration 169 : 0.5068813216417225
Loss in iteration 170 : 0.5068724101524319
Loss in iteration 171 : 0.5068662505692242
Loss in iteration 172 : 0.5068666406935296
Loss in iteration 173 : 0.5068493417812403
Loss in iteration 174 : 0.5068567276353346
Loss in iteration 175 : 0.5068348477767309
Loss in iteration 176 : 0.5068301569821152
Loss in iteration 177 : 0.5068245152687734
Loss in iteration 178 : 0.5068049452122678
Loss in iteration 179 : 0.5068109998024285
Loss in iteration 180 : 0.5067939250185579
Loss in iteration 181 : 0.5067861181786506
Loss in iteration 182 : 0.5068073095485672
Loss in iteration 183 : 0.506768466256052
Loss in iteration 184 : 0.5067956046823584
Loss in iteration 185 : 0.506769068920671
Loss in iteration 186 : 0.5067484761944183
Loss in iteration 187 : 0.5067633702093299
Loss in iteration 188 : 0.5067335979208456
Loss in iteration 189 : 0.5067357334580584
Loss in iteration 190 : 0.5067480766757781
Loss in iteration 191 : 0.5067134181691128
Loss in iteration 192 : 0.5067528718835982
Loss in iteration 193 : 0.5067054517149068
Loss in iteration 194 : 0.5067400204733099
Loss in iteration 195 : 0.506694952738135
Loss in iteration 196 : 0.506704825692429
Loss in iteration 197 : 0.5066914293930057
Loss in iteration 198 : 0.5066675075223904
Loss in iteration 199 : 0.5066698094489323
Loss in iteration 200 : 0.5066619479714767
Testing accuracy  of updater 7 on alg 1 with rate 2.0 = 0.782, training accuracy 0.788, time elapsed: 3017 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9197864519709049
Loss in iteration 3 : 0.9350987380695042
Loss in iteration 4 : 0.8159118056297662
Loss in iteration 5 : 0.7921278958167377
Loss in iteration 6 : 0.6726942173434493
Loss in iteration 7 : 0.6456060309069551
Loss in iteration 8 : 0.6019114603056872
Loss in iteration 9 : 0.5944846948183709
Loss in iteration 10 : 0.572909527760024
Loss in iteration 11 : 0.5680699067488156
Loss in iteration 12 : 0.5693924389058516
Loss in iteration 13 : 0.554966640424375
Loss in iteration 14 : 0.5642417839993862
Loss in iteration 15 : 0.5591723437032917
Loss in iteration 16 : 0.5580623154587306
Loss in iteration 17 : 0.5612179185932059
Loss in iteration 18 : 0.5542073506056749
Loss in iteration 19 : 0.556556166046614
Loss in iteration 20 : 0.5576839415891011
Loss in iteration 21 : 0.5518351074973392
Loss in iteration 22 : 0.5532942382252588
Loss in iteration 23 : 0.551649281537497
Loss in iteration 24 : 0.5469573932012164
Loss in iteration 25 : 0.5468735374825529
Loss in iteration 26 : 0.5423482514863618
Loss in iteration 27 : 0.5387384484349892
Loss in iteration 28 : 0.5381317362390792
Loss in iteration 29 : 0.5333897166162767
Loss in iteration 30 : 0.5313384217849523
Loss in iteration 31 : 0.5299975332566058
Loss in iteration 32 : 0.5258787844655732
Loss in iteration 33 : 0.5242677483628456
Loss in iteration 34 : 0.5219063859083093
Loss in iteration 35 : 0.5195474871729594
Loss in iteration 36 : 0.5190860963494
Loss in iteration 37 : 0.5167696793864602
Loss in iteration 38 : 0.5163794596784551
Loss in iteration 39 : 0.5151351294809153
Loss in iteration 40 : 0.5139951654861717
Loss in iteration 41 : 0.5140529004959906
Loss in iteration 42 : 0.5130197154736075
Loss in iteration 43 : 0.5134505951806189
Loss in iteration 44 : 0.5127431722040919
Loss in iteration 45 : 0.512878523369746
Loss in iteration 46 : 0.5124994354498529
Loss in iteration 47 : 0.5124024754560572
Loss in iteration 48 : 0.5125652335698415
Loss in iteration 49 : 0.5123106967934864
Loss in iteration 50 : 0.5125802196581964
Loss in iteration 51 : 0.5121144570773662
Loss in iteration 52 : 0.512279145919547
Loss in iteration 53 : 0.5118787138760026
Loss in iteration 54 : 0.5119957697119195
Loss in iteration 55 : 0.511710816991661
Loss in iteration 56 : 0.5116437618098068
Loss in iteration 57 : 0.5114435320051635
Loss in iteration 58 : 0.5112415286934795
Loss in iteration 59 : 0.5111623698364951
Loss in iteration 60 : 0.5108532319367991
Loss in iteration 61 : 0.5107891445251929
Loss in iteration 62 : 0.5105171656783376
Loss in iteration 63 : 0.5104642329490253
Loss in iteration 64 : 0.5102304369290118
Loss in iteration 65 : 0.510141922908702
Loss in iteration 66 : 0.5099806880963837
Loss in iteration 67 : 0.5098753347728533
Loss in iteration 68 : 0.5097874528543225
Loss in iteration 69 : 0.5096689820273439
Loss in iteration 70 : 0.5096147222317056
Loss in iteration 71 : 0.5095064595589819
Loss in iteration 72 : 0.509471814378186
Loss in iteration 73 : 0.5093763355364739
Loss in iteration 74 : 0.5093260105610882
Loss in iteration 75 : 0.5092810740812634
Loss in iteration 76 : 0.5092060267560593
Loss in iteration 77 : 0.5091780142579038
Loss in iteration 78 : 0.5091082259030305
Loss in iteration 79 : 0.5090904148935069
Loss in iteration 80 : 0.5090282101328782
Loss in iteration 81 : 0.5090070655620956
Loss in iteration 82 : 0.5089439445319113
Loss in iteration 83 : 0.5089175487246723
Loss in iteration 84 : 0.5088592650536279
Loss in iteration 85 : 0.5088397306022789
Loss in iteration 86 : 0.5087967896199447
Loss in iteration 87 : 0.5087288221222085
Loss in iteration 88 : 0.5087078333431536
Loss in iteration 89 : 0.5086510596287751
Loss in iteration 90 : 0.5086251708450165
Loss in iteration 91 : 0.5085624051622247
Loss in iteration 92 : 0.5085505939558265
Loss in iteration 93 : 0.5084927439248127
Loss in iteration 94 : 0.508474687267891
Loss in iteration 95 : 0.5084270576096632
Loss in iteration 96 : 0.5083983408877665
Loss in iteration 97 : 0.5083692292755156
Loss in iteration 98 : 0.5083263711403702
Loss in iteration 99 : 0.5083021015411777
Loss in iteration 100 : 0.5082742519122131
Loss in iteration 101 : 0.5082436654530857
Loss in iteration 102 : 0.5082236511621702
Loss in iteration 103 : 0.5082019289494186
Loss in iteration 104 : 0.5081675053444579
Loss in iteration 105 : 0.5081456844471247
Loss in iteration 106 : 0.5081211485831335
Loss in iteration 107 : 0.508100494123603
Loss in iteration 108 : 0.5080721361418233
Loss in iteration 109 : 0.5080493160701345
Loss in iteration 110 : 0.5080265450001388
Loss in iteration 111 : 0.5080053340940307
Loss in iteration 112 : 0.5079812902680377
Loss in iteration 113 : 0.5079613449202153
Loss in iteration 114 : 0.5079352281998564
Loss in iteration 115 : 0.5079197005703499
Loss in iteration 116 : 0.5078927198302763
Loss in iteration 117 : 0.5078754826288128
Loss in iteration 118 : 0.5078513561589244
Loss in iteration 119 : 0.5078335278383572
Loss in iteration 120 : 0.5078088262646154
Loss in iteration 121 : 0.5077924423615168
Loss in iteration 122 : 0.5077698151730206
Loss in iteration 123 : 0.5077502974941761
Loss in iteration 124 : 0.5077306358749117
Loss in iteration 125 : 0.5077112574532248
Loss in iteration 126 : 0.5076907602598199
Loss in iteration 127 : 0.5076713054179313
Loss in iteration 128 : 0.5076521107693555
Loss in iteration 129 : 0.5076325114327426
Loss in iteration 130 : 0.5076144636076286
Loss in iteration 131 : 0.5075959790715432
Loss in iteration 132 : 0.5075796871977138
Loss in iteration 133 : 0.5075600166333891
Loss in iteration 134 : 0.5075427403172433
Loss in iteration 135 : 0.5075309863329202
Loss in iteration 136 : 0.50751070598024
Loss in iteration 137 : 0.5074955572728578
Loss in iteration 138 : 0.5074810873471695
Loss in iteration 139 : 0.5074664390004036
Loss in iteration 140 : 0.5074491871739549
Loss in iteration 141 : 0.507432886224514
Loss in iteration 142 : 0.5074198051223716
Loss in iteration 143 : 0.5074042709828818
Loss in iteration 144 : 0.5073890806324766
Loss in iteration 145 : 0.5073744907399788
Loss in iteration 146 : 0.5073625787569003
Loss in iteration 147 : 0.5073464958484275
Loss in iteration 148 : 0.5073355141222653
Loss in iteration 149 : 0.5073208362089234
Loss in iteration 150 : 0.5073109425697019
Loss in iteration 151 : 0.5072979176352558
Loss in iteration 152 : 0.5072881165435466
Loss in iteration 153 : 0.5072728485668593
Loss in iteration 154 : 0.5072640161469865
Loss in iteration 155 : 0.5072501898445846
Loss in iteration 156 : 0.5072410047433715
Loss in iteration 157 : 0.5072354447014444
Loss in iteration 158 : 0.5072192720491778
Loss in iteration 159 : 0.5072090923369115
Loss in iteration 160 : 0.5071928621056392
Loss in iteration 161 : 0.5071884492430985
Loss in iteration 162 : 0.5071713804130275
Loss in iteration 163 : 0.507167883549346
Loss in iteration 164 : 0.5071510168590557
Loss in iteration 165 : 0.5071445087370414
Loss in iteration 166 : 0.5071316934327224
Loss in iteration 167 : 0.5071261858279138
Loss in iteration 168 : 0.5071141587408459
Loss in iteration 169 : 0.5071050923205302
Loss in iteration 170 : 0.5070978700940384
Loss in iteration 171 : 0.507084092555805
Loss in iteration 172 : 0.507079653431848
Loss in iteration 173 : 0.5070679495183001
Loss in iteration 174 : 0.5070591067071379
Loss in iteration 175 : 0.5070522382887912
Loss in iteration 176 : 0.5070392130659674
Loss in iteration 177 : 0.5070348048141189
Loss in iteration 178 : 0.5070231230960913
Loss in iteration 179 : 0.5070142752312036
Loss in iteration 180 : 0.5070076090336572
Loss in iteration 181 : 0.5069995735740082
Loss in iteration 182 : 0.5069915790645071
Loss in iteration 183 : 0.5069825145114442
Loss in iteration 184 : 0.506976359596883
Loss in iteration 185 : 0.5069665867347888
Loss in iteration 186 : 0.5069589181167474
Loss in iteration 187 : 0.5069470774745021
Loss in iteration 188 : 0.5069404082156702
Loss in iteration 189 : 0.5069352977071877
Loss in iteration 190 : 0.5069245622385757
Loss in iteration 191 : 0.5069164459295866
Loss in iteration 192 : 0.5069102310712781
Loss in iteration 193 : 0.5068997813381559
Loss in iteration 194 : 0.5068916381148302
Loss in iteration 195 : 0.5068862476173057
Loss in iteration 196 : 0.5068784299637411
Loss in iteration 197 : 0.5068719139518313
Loss in iteration 198 : 0.5068621343513
Loss in iteration 199 : 0.5068528738921078
Loss in iteration 200 : 0.5068483928740274
Testing accuracy  of updater 7 on alg 1 with rate 1.4 = 0.7835, training accuracy 0.787875, time elapsed: 3293 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9371649398778443
Loss in iteration 3 : 0.926168440194178
Loss in iteration 4 : 0.8692071154837493
Loss in iteration 5 : 0.799155856225545
Loss in iteration 6 : 0.7655760791912222
Loss in iteration 7 : 0.7018948753733614
Loss in iteration 8 : 0.6376756323407392
Loss in iteration 9 : 0.6299756039153364
Loss in iteration 10 : 0.5909133145085982
Loss in iteration 11 : 0.5881739835206088
Loss in iteration 12 : 0.5774717683047595
Loss in iteration 13 : 0.5618330900341846
Loss in iteration 14 : 0.5686133431220979
Loss in iteration 15 : 0.5607885684322335
Loss in iteration 16 : 0.5529139415768384
Loss in iteration 17 : 0.5590588229890081
Loss in iteration 18 : 0.5562055420436245
Loss in iteration 19 : 0.551292400053471
Loss in iteration 20 : 0.5542824179923704
Loss in iteration 21 : 0.5518533055718167
Loss in iteration 22 : 0.5472954208769488
Loss in iteration 23 : 0.5490127699528813
Loss in iteration 24 : 0.5487356791599343
Loss in iteration 25 : 0.5440683516963108
Loss in iteration 26 : 0.5436407466491894
Loss in iteration 27 : 0.5441633890958218
Loss in iteration 28 : 0.5402919498515357
Loss in iteration 29 : 0.53848513592874
Loss in iteration 30 : 0.5374839067843945
Loss in iteration 31 : 0.5338341895544684
Loss in iteration 32 : 0.5317892600213632
Loss in iteration 33 : 0.530830216367088
Loss in iteration 34 : 0.5278122071500541
Loss in iteration 35 : 0.5260772645066437
Loss in iteration 36 : 0.5253320427112094
Loss in iteration 37 : 0.523038107107531
Loss in iteration 38 : 0.5211806808668512
Loss in iteration 39 : 0.5203690177881845
Loss in iteration 40 : 0.5185548818355419
Loss in iteration 41 : 0.5175722629882251
Loss in iteration 42 : 0.5169873670258983
Loss in iteration 43 : 0.5154940218979696
Loss in iteration 44 : 0.5151194092900436
Loss in iteration 45 : 0.5145559063382265
Loss in iteration 46 : 0.5134757365758708
Loss in iteration 47 : 0.5133575416969615
Loss in iteration 48 : 0.5129585361638642
Loss in iteration 49 : 0.5125177718401642
Loss in iteration 50 : 0.5124950416940164
Loss in iteration 51 : 0.5122413682489659
Loss in iteration 52 : 0.5120752567100535
Loss in iteration 53 : 0.5120370875139039
Loss in iteration 54 : 0.5117903136559269
Loss in iteration 55 : 0.5116714743618261
Loss in iteration 56 : 0.5116957766952344
Loss in iteration 57 : 0.5115286609884689
Loss in iteration 58 : 0.5114446978880887
Loss in iteration 59 : 0.5114562199363119
Loss in iteration 60 : 0.5113078661183835
Loss in iteration 61 : 0.5112459755593002
Loss in iteration 62 : 0.5111699609977656
Loss in iteration 63 : 0.5110431772886408
Loss in iteration 64 : 0.5110076059916582
Loss in iteration 65 : 0.5109062433242929
Loss in iteration 66 : 0.51081905650534
Loss in iteration 67 : 0.5107835453880193
Loss in iteration 68 : 0.5106805210572053
Loss in iteration 69 : 0.51059922818083
Loss in iteration 70 : 0.5105213543140262
Loss in iteration 71 : 0.510436138274843
Loss in iteration 72 : 0.5103603337023628
Loss in iteration 73 : 0.5102815690654889
Loss in iteration 74 : 0.5102150544585534
Loss in iteration 75 : 0.510143267298157
Loss in iteration 76 : 0.5100696036485999
Loss in iteration 77 : 0.5100208216479064
Loss in iteration 78 : 0.5099504362986302
Loss in iteration 79 : 0.509900370576251
Loss in iteration 80 : 0.5098580820963813
Loss in iteration 81 : 0.5098029568058707
Loss in iteration 82 : 0.5097579392112772
Loss in iteration 83 : 0.5097230740668198
Loss in iteration 84 : 0.509687806592393
Loss in iteration 85 : 0.5096452793014211
Loss in iteration 86 : 0.5096103270306447
Loss in iteration 87 : 0.5095793222470162
Loss in iteration 88 : 0.5095452478853092
Loss in iteration 89 : 0.5095083031829936
Loss in iteration 90 : 0.5094766377670626
Loss in iteration 91 : 0.5094480155059639
Loss in iteration 92 : 0.5094131455876126
Loss in iteration 93 : 0.509383251137473
Loss in iteration 94 : 0.5093553464589596
Loss in iteration 95 : 0.5093231676514723
Loss in iteration 96 : 0.5092901795061283
Loss in iteration 97 : 0.5092631915594544
Loss in iteration 98 : 0.5092334620184252
Loss in iteration 99 : 0.5092024841352564
Loss in iteration 100 : 0.5091738724092677
Loss in iteration 101 : 0.5091435179228411
Loss in iteration 102 : 0.5091135740051108
Loss in iteration 103 : 0.509085223166211
Loss in iteration 104 : 0.509059778448855
Loss in iteration 105 : 0.5090302337972429
Loss in iteration 106 : 0.5090064085266304
Loss in iteration 107 : 0.5089795698811447
Loss in iteration 108 : 0.5089533409353738
Loss in iteration 109 : 0.5089267028283396
Loss in iteration 110 : 0.5089019294625394
Loss in iteration 111 : 0.5088768720551603
Loss in iteration 112 : 0.5088538804593572
Loss in iteration 113 : 0.5088271684680827
Loss in iteration 114 : 0.5088047991459432
Loss in iteration 115 : 0.5087812652139422
Loss in iteration 116 : 0.5087583502500412
Loss in iteration 117 : 0.5087355322564278
Loss in iteration 118 : 0.5087132215153423
Loss in iteration 119 : 0.508691513246557
Loss in iteration 120 : 0.5086692567210657
Loss in iteration 121 : 0.5086489275274446
Loss in iteration 122 : 0.5086264082993633
Loss in iteration 123 : 0.508604428905998
Loss in iteration 124 : 0.5085836666097951
Loss in iteration 125 : 0.5085621815500381
Loss in iteration 126 : 0.5085407070135599
Loss in iteration 127 : 0.5085205051908169
Loss in iteration 128 : 0.5085001973221883
Loss in iteration 129 : 0.5084793913274034
Loss in iteration 130 : 0.5084592322558213
Loss in iteration 131 : 0.50843839118127
Loss in iteration 132 : 0.5084189905727562
Loss in iteration 133 : 0.5083991788731054
Loss in iteration 134 : 0.5083791454355134
Loss in iteration 135 : 0.508361569312032
Loss in iteration 136 : 0.5083425836094965
Loss in iteration 137 : 0.5083249137069193
Loss in iteration 138 : 0.5083058206788696
Loss in iteration 139 : 0.5082883386638892
Loss in iteration 140 : 0.5082715336706561
Loss in iteration 141 : 0.5082541128813355
Loss in iteration 142 : 0.5082365935174062
Loss in iteration 143 : 0.5082189696282132
Loss in iteration 144 : 0.5082022254847163
Loss in iteration 145 : 0.5081841630863423
Loss in iteration 146 : 0.5081689009622326
Loss in iteration 147 : 0.5081507997522239
Loss in iteration 148 : 0.5081358774407041
Loss in iteration 149 : 0.5081192662739767
Loss in iteration 150 : 0.5081016952515582
Loss in iteration 151 : 0.5080861865109096
Loss in iteration 152 : 0.5080695712888964
Loss in iteration 153 : 0.5080549247248536
Loss in iteration 154 : 0.5080394538657305
Loss in iteration 155 : 0.5080244702497165
Loss in iteration 156 : 0.5080089590457604
Loss in iteration 157 : 0.507994197670583
Loss in iteration 158 : 0.5079799484886256
Loss in iteration 159 : 0.5079642338563155
Loss in iteration 160 : 0.5079505669381077
Loss in iteration 161 : 0.5079355996793787
Loss in iteration 162 : 0.5079205558126253
Loss in iteration 163 : 0.5079064890118156
Loss in iteration 164 : 0.5078915540470653
Loss in iteration 165 : 0.5078773981918252
Loss in iteration 166 : 0.5078630770309283
Loss in iteration 167 : 0.5078489665864001
Loss in iteration 168 : 0.507833554339982
Loss in iteration 169 : 0.5078189685027478
Loss in iteration 170 : 0.5078054946997254
Loss in iteration 171 : 0.5077910049383237
Loss in iteration 172 : 0.5077761257877929
Loss in iteration 173 : 0.5077613104341787
Loss in iteration 174 : 0.5077477496211181
Loss in iteration 175 : 0.5077326806700957
Loss in iteration 176 : 0.5077182858925348
Loss in iteration 177 : 0.5077039812507571
Loss in iteration 178 : 0.5076897134371725
Loss in iteration 179 : 0.507675186404152
Loss in iteration 180 : 0.5076615588809429
Loss in iteration 181 : 0.5076477877580087
Loss in iteration 182 : 0.5076327587709519
Loss in iteration 183 : 0.5076186320257333
Loss in iteration 184 : 0.5076064782357511
Loss in iteration 185 : 0.5075910132307531
Loss in iteration 186 : 0.5075775090896927
Loss in iteration 187 : 0.507565443832669
Loss in iteration 188 : 0.5075517277743447
Loss in iteration 189 : 0.5075379484710106
Loss in iteration 190 : 0.5075269113084437
Loss in iteration 191 : 0.5075145218170987
Loss in iteration 192 : 0.5075032385870081
Loss in iteration 193 : 0.5074925196016707
Loss in iteration 194 : 0.5074819840610354
Loss in iteration 195 : 0.5074679369190384
Loss in iteration 196 : 0.5074584392463751
Loss in iteration 197 : 0.5074468277875175
Loss in iteration 198 : 0.5074371325463495
Loss in iteration 199 : 0.5074252483163817
Loss in iteration 200 : 0.5074140013451212
Testing accuracy  of updater 7 on alg 1 with rate 0.8 = 0.784, training accuracy 0.787, time elapsed: 2315 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9814087988663309
Loss in iteration 3 : 0.9471414236655974
Loss in iteration 4 : 0.9166295061345984
Loss in iteration 5 : 0.9129709061576577
Loss in iteration 6 : 0.9063205755027223
Loss in iteration 7 : 0.8767588272586386
Loss in iteration 8 : 0.8413470963829314
Loss in iteration 9 : 0.8122022837272764
Loss in iteration 10 : 0.7868851330898349
Loss in iteration 11 : 0.7666831040449709
Loss in iteration 12 : 0.7502021839796319
Loss in iteration 13 : 0.7210242015870273
Loss in iteration 14 : 0.6838522155053465
Loss in iteration 15 : 0.6590944753343408
Loss in iteration 16 : 0.6465108956250583
Loss in iteration 17 : 0.6354524795111797
Loss in iteration 18 : 0.6211166199913141
Loss in iteration 19 : 0.6068170558634625
Loss in iteration 20 : 0.5978541803771324
Loss in iteration 21 : 0.5930853290343627
Loss in iteration 22 : 0.5878120208257146
Loss in iteration 23 : 0.5802189440689189
Loss in iteration 24 : 0.5738898691606472
Loss in iteration 25 : 0.5708312499493738
Loss in iteration 26 : 0.569644540876272
Loss in iteration 27 : 0.5674116049529748
Loss in iteration 28 : 0.5634961508026347
Loss in iteration 29 : 0.5598561640783034
Loss in iteration 30 : 0.5582302236096
Loss in iteration 31 : 0.5576523248775541
Loss in iteration 32 : 0.5563574344440262
Loss in iteration 33 : 0.55417800151985
Loss in iteration 34 : 0.5521966451326654
Loss in iteration 35 : 0.5509806455500056
Loss in iteration 36 : 0.5503137319013959
Loss in iteration 37 : 0.5493207988846237
Loss in iteration 38 : 0.5479056577268744
Loss in iteration 39 : 0.5465966275651243
Loss in iteration 40 : 0.5457059191773466
Loss in iteration 41 : 0.5451164528712756
Loss in iteration 42 : 0.5443679733180296
Loss in iteration 43 : 0.5432673943903126
Loss in iteration 44 : 0.5421349481253844
Loss in iteration 45 : 0.5412794994535373
Loss in iteration 46 : 0.5406745802432694
Loss in iteration 47 : 0.5400399726022496
Loss in iteration 48 : 0.5392194385023094
Loss in iteration 49 : 0.5383302560862485
Loss in iteration 50 : 0.5375194360212819
Loss in iteration 51 : 0.536852350691987
Loss in iteration 52 : 0.536234348927587
Loss in iteration 53 : 0.5355636487902358
Loss in iteration 54 : 0.5348454283216254
Loss in iteration 55 : 0.5341490571495094
Loss in iteration 56 : 0.5335170540551125
Loss in iteration 57 : 0.5329446345054236
Loss in iteration 58 : 0.5323825073000293
Loss in iteration 59 : 0.5317940514101774
Loss in iteration 60 : 0.5312135699341933
Loss in iteration 61 : 0.5306794574324313
Loss in iteration 62 : 0.5301968828492307
Loss in iteration 63 : 0.5297234464278504
Loss in iteration 64 : 0.5292223955182719
Loss in iteration 65 : 0.5287143532743473
Loss in iteration 66 : 0.5282526189759067
Loss in iteration 67 : 0.5278394189895911
Loss in iteration 68 : 0.5274306280585777
Loss in iteration 69 : 0.5270137244713952
Loss in iteration 70 : 0.5266157593370983
Loss in iteration 71 : 0.5262558478159424
Loss in iteration 72 : 0.52591145552858
Loss in iteration 73 : 0.5255631109751295
Loss in iteration 74 : 0.5252155321057126
Loss in iteration 75 : 0.5248861584428071
Loss in iteration 76 : 0.5245724279199463
Loss in iteration 77 : 0.5242741596365388
Loss in iteration 78 : 0.5239804844173572
Loss in iteration 79 : 0.5236883334024545
Loss in iteration 80 : 0.5234033721174515
Loss in iteration 81 : 0.5231383878452914
Loss in iteration 82 : 0.5228786252923564
Loss in iteration 83 : 0.5226199365025838
Loss in iteration 84 : 0.5223687094589622
Loss in iteration 85 : 0.5221294534581786
Loss in iteration 86 : 0.5218992845505216
Loss in iteration 87 : 0.5216758242417207
Loss in iteration 88 : 0.5214549209828744
Loss in iteration 89 : 0.5212389265820575
Loss in iteration 90 : 0.5210283189309433
Loss in iteration 91 : 0.5208210289168776
Loss in iteration 92 : 0.5206168529100559
Loss in iteration 93 : 0.5204174036539893
Loss in iteration 94 : 0.5202255450292875
Loss in iteration 95 : 0.5200400718186591
Loss in iteration 96 : 0.5198575466635932
Loss in iteration 97 : 0.5196790180988737
Loss in iteration 98 : 0.5195070438961871
Loss in iteration 99 : 0.519336618856968
Loss in iteration 100 : 0.5191704554453798
Loss in iteration 101 : 0.5190078199625768
Loss in iteration 102 : 0.5188480256232137
Loss in iteration 103 : 0.5186907387949703
Loss in iteration 104 : 0.5185359355716355
Loss in iteration 105 : 0.5183846845409675
Loss in iteration 106 : 0.518236311716639
Loss in iteration 107 : 0.5180891572569145
Loss in iteration 108 : 0.5179483074871095
Loss in iteration 109 : 0.5178087692787927
Loss in iteration 110 : 0.517671883252191
Loss in iteration 111 : 0.5175363501818943
Loss in iteration 112 : 0.5174038973021149
Loss in iteration 113 : 0.5172751548136794
Loss in iteration 114 : 0.5171491205601519
Loss in iteration 115 : 0.5170246883802029
Loss in iteration 116 : 0.5169043271199796
Loss in iteration 117 : 0.5167867460592669
Loss in iteration 118 : 0.5166727579012884
Loss in iteration 119 : 0.5165607378707064
Loss in iteration 120 : 0.5164484942469021
Loss in iteration 121 : 0.5163391114364945
Loss in iteration 122 : 0.5162313413426152
Loss in iteration 123 : 0.5161265229154365
Loss in iteration 124 : 0.5160236216020275
Loss in iteration 125 : 0.5159220914226089
Loss in iteration 126 : 0.5158227044103058
Loss in iteration 127 : 0.5157257244960144
Loss in iteration 128 : 0.5156304871811758
Loss in iteration 129 : 0.5155370832683444
Loss in iteration 130 : 0.5154454967918832
Loss in iteration 131 : 0.5153551104196507
Loss in iteration 132 : 0.5152654960225344
Loss in iteration 133 : 0.5151777026261245
Loss in iteration 134 : 0.5150916649179323
Loss in iteration 135 : 0.5150076961635329
Loss in iteration 136 : 0.514926202363932
Loss in iteration 137 : 0.514847345919106
Loss in iteration 138 : 0.5147686886035368
Loss in iteration 139 : 0.5146912892750584
Loss in iteration 140 : 0.5146136629025998
Loss in iteration 141 : 0.5145372893875834
Loss in iteration 142 : 0.5144634333307099
Loss in iteration 143 : 0.5143919071398959
Loss in iteration 144 : 0.5143208481077872
Loss in iteration 145 : 0.514250664189732
Loss in iteration 146 : 0.5141807912186984
Loss in iteration 147 : 0.514112011397045
Loss in iteration 148 : 0.5140442573584951
Loss in iteration 149 : 0.5139775642487662
Loss in iteration 150 : 0.513912686025257
Loss in iteration 151 : 0.5138492871323815
Loss in iteration 152 : 0.5137859943057894
Loss in iteration 153 : 0.5137229077434997
Loss in iteration 154 : 0.5136614937466559
Loss in iteration 155 : 0.5136021363447751
Loss in iteration 156 : 0.5135437529818379
Loss in iteration 157 : 0.5134873655240867
Loss in iteration 158 : 0.51343164952102
Loss in iteration 159 : 0.5133763431370962
Loss in iteration 160 : 0.5133211839788928
Loss in iteration 161 : 0.5132675060649837
Loss in iteration 162 : 0.5132149774460256
Loss in iteration 163 : 0.5131624685695517
Loss in iteration 164 : 0.5131117259392168
Loss in iteration 165 : 0.5130620243469949
Loss in iteration 166 : 0.5130138915072594
Loss in iteration 167 : 0.5129658120232962
Loss in iteration 168 : 0.5129192747532451
Loss in iteration 169 : 0.5128739374767419
Loss in iteration 170 : 0.512829439299781
Loss in iteration 171 : 0.5127850712069931
Loss in iteration 172 : 0.5127409996233916
Loss in iteration 173 : 0.5126979779940676
Loss in iteration 174 : 0.5126551297427375
Loss in iteration 175 : 0.5126129768334227
Loss in iteration 176 : 0.512571202065119
Loss in iteration 177 : 0.5125298652434741
Loss in iteration 178 : 0.5124888710803722
Loss in iteration 179 : 0.5124483301746586
Loss in iteration 180 : 0.5124081187790854
Loss in iteration 181 : 0.5123686568637134
Loss in iteration 182 : 0.5123305181238286
Loss in iteration 183 : 0.512293231169207
Loss in iteration 184 : 0.5122562691010509
Loss in iteration 185 : 0.5122194752462161
Loss in iteration 186 : 0.5121832762701207
Loss in iteration 187 : 0.5121475282453786
Loss in iteration 188 : 0.5121122247823849
Loss in iteration 189 : 0.5120778100959116
Loss in iteration 190 : 0.5120429804154853
Loss in iteration 191 : 0.5120089861074514
Loss in iteration 192 : 0.5119758296043967
Loss in iteration 193 : 0.5119427713492148
Loss in iteration 194 : 0.5119101727015574
Loss in iteration 195 : 0.5118783070650865
Loss in iteration 196 : 0.5118473376311116
Loss in iteration 197 : 0.511816044661624
Loss in iteration 198 : 0.5117854478574226
Loss in iteration 199 : 0.5117559892164051
Loss in iteration 200 : 0.5117261008443451
Testing accuracy  of updater 7 on alg 1 with rate 0.19999999999999996 = 0.7795, training accuracy 0.78625, time elapsed: 2935 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 5.317080503371858
Loss in iteration 3 : 4.4022018535258765
Loss in iteration 4 : 1.719849137863844
Loss in iteration 5 : 1.2885613304599701
Loss in iteration 6 : 0.9719300710097416
Loss in iteration 7 : 0.8834084793480049
Loss in iteration 8 : 0.9601726007635646
Loss in iteration 9 : 0.9941139015206363
Loss in iteration 10 : 0.9789862838117378
Loss in iteration 11 : 0.9551068136753127
Loss in iteration 12 : 0.9818275955241995
Loss in iteration 13 : 1.0100109605359642
Loss in iteration 14 : 1.1280754358561558
Loss in iteration 15 : 0.9887421333369939
Loss in iteration 16 : 1.0101170442287228
Loss in iteration 17 : 0.90545390318794
Loss in iteration 18 : 0.8894371009770757
Loss in iteration 19 : 0.850282250715574
Loss in iteration 20 : 0.8538981909709233
Loss in iteration 21 : 0.8326321592882296
Loss in iteration 22 : 0.8598196535824011
Loss in iteration 23 : 0.84290697097177
Loss in iteration 24 : 0.88737912635849
Loss in iteration 25 : 0.8684708688705759
Loss in iteration 26 : 0.9176858983953893
Loss in iteration 27 : 0.9031681989710817
Loss in iteration 28 : 0.9554580641854085
Loss in iteration 29 : 0.8851154082615726
Loss in iteration 30 : 0.9285139127424795
Loss in iteration 31 : 0.8754802109546993
Loss in iteration 32 : 0.9072765667462896
Loss in iteration 33 : 0.8682240647011795
Loss in iteration 34 : 0.9085874518381053
Loss in iteration 35 : 0.868397646892454
Loss in iteration 36 : 0.9024840892811835
Loss in iteration 37 : 0.8723344936789638
Loss in iteration 38 : 0.9031257322734547
Loss in iteration 39 : 0.8599191106433071
Loss in iteration 40 : 0.8866847655311637
Loss in iteration 41 : 0.8447364603668968
Loss in iteration 42 : 0.8698669331266895
Loss in iteration 43 : 0.8272258983704692
Loss in iteration 44 : 0.8499887957556443
Loss in iteration 45 : 0.8169664336504489
Loss in iteration 46 : 0.8340543739506189
Loss in iteration 47 : 0.8012072206852751
Loss in iteration 48 : 0.8305549655514495
Loss in iteration 49 : 0.7936699468133904
Loss in iteration 50 : 0.8488620296051524
Loss in iteration 51 : 0.7512484885533705
Loss in iteration 52 : 0.8046961199815446
Loss in iteration 53 : 0.7145272994261264
Loss in iteration 54 : 0.760418336192736
Loss in iteration 55 : 0.7261337255855558
Loss in iteration 56 : 0.7942959359070921
Loss in iteration 57 : 0.7848140278930328
Loss in iteration 58 : 0.8897585042237128
Loss in iteration 59 : 0.9238160572093138
Loss in iteration 60 : 0.9507152818053901
Loss in iteration 61 : 0.8965023752713643
Loss in iteration 62 : 0.8960290818461988
Loss in iteration 63 : 0.8492593165557998
Loss in iteration 64 : 0.8552551624336595
Loss in iteration 65 : 0.8193866588253386
Loss in iteration 66 : 0.8336315899598453
Loss in iteration 67 : 0.8111585665840275
Loss in iteration 68 : 0.8126576253781559
Loss in iteration 69 : 0.7969740898202505
Loss in iteration 70 : 0.8200977174890833
Loss in iteration 71 : 0.7997294929532317
Loss in iteration 72 : 0.8213935400511415
Loss in iteration 73 : 0.7937843898103091
Loss in iteration 74 : 0.8230565182011621
Loss in iteration 75 : 0.804382297815191
Loss in iteration 76 : 0.8283846689967364
Loss in iteration 77 : 0.7967618369661832
Loss in iteration 78 : 0.8290118519257045
Loss in iteration 79 : 0.8114861551798931
Loss in iteration 80 : 0.8407591680182036
Loss in iteration 81 : 0.8015983652770542
Loss in iteration 82 : 0.8290130712745304
Loss in iteration 83 : 0.7985643584505971
Loss in iteration 84 : 0.8195309753208164
Loss in iteration 85 : 0.7900212650646559
Loss in iteration 86 : 0.8246964046319237
Loss in iteration 87 : 0.7959047573945184
Loss in iteration 88 : 0.8141556861431181
Loss in iteration 89 : 0.784628082760894
Loss in iteration 90 : 0.8219587705709639
Loss in iteration 91 : 0.801934990888482
Loss in iteration 92 : 0.8221075854811372
Loss in iteration 93 : 0.7973076287647137
Loss in iteration 94 : 0.8339990729621015
Loss in iteration 95 : 0.8085943419996717
Loss in iteration 96 : 0.830252276491305
Loss in iteration 97 : 0.7985133850961653
Loss in iteration 98 : 0.8301954515829295
Loss in iteration 99 : 0.8086832506489193
Loss in iteration 100 : 0.8264514362900602
Loss in iteration 101 : 0.7977585944982531
Loss in iteration 102 : 0.8258730791648633
Loss in iteration 103 : 0.7991480999775231
Loss in iteration 104 : 0.8199858754608269
Loss in iteration 105 : 0.7919288736356206
Loss in iteration 106 : 0.8195144226859156
Loss in iteration 107 : 0.7913131721065831
Loss in iteration 108 : 0.8167105440301653
Loss in iteration 109 : 0.7926523205789963
Loss in iteration 110 : 0.8192164972998605
Loss in iteration 111 : 0.7898464074440567
Loss in iteration 112 : 0.8186049041002833
Loss in iteration 113 : 0.8054952526794367
Loss in iteration 114 : 0.8359996776588379
Loss in iteration 115 : 0.803745117294944
Loss in iteration 116 : 0.8269243538397147
Loss in iteration 117 : 0.8036668959518948
Loss in iteration 118 : 0.8244707969731258
Loss in iteration 119 : 0.7914250575494596
Loss in iteration 120 : 0.8117912166785511
Loss in iteration 121 : 0.7910726216757284
Loss in iteration 122 : 0.8140986743302492
Loss in iteration 123 : 0.7811291442096169
Loss in iteration 124 : 0.8092212902069875
Loss in iteration 125 : 0.7864027943833747
Loss in iteration 126 : 0.8114093864676847
Loss in iteration 127 : 0.7826239468193356
Loss in iteration 128 : 0.8123991423815631
Loss in iteration 129 : 0.7938625692135921
Loss in iteration 130 : 0.8280499097922452
Loss in iteration 131 : 0.800061157678168
Loss in iteration 132 : 0.8258363740948156
Loss in iteration 133 : 0.803152080812088
Loss in iteration 134 : 0.8246583842222881
Loss in iteration 135 : 0.7870828351145019
Loss in iteration 136 : 0.8080119259798904
Loss in iteration 137 : 0.779491310248751
Loss in iteration 138 : 0.8082267953141824
Loss in iteration 139 : 0.7757445683441649
Loss in iteration 140 : 0.8040777643566526
Loss in iteration 141 : 0.771917277499535
Loss in iteration 142 : 0.8073383516776369
Loss in iteration 143 : 0.7841547909396275
Loss in iteration 144 : 0.82227024494529
Loss in iteration 145 : 0.7868609582991186
Loss in iteration 146 : 0.8286665037080362
Loss in iteration 147 : 0.7979576104033592
Loss in iteration 148 : 0.8337248807236134
Loss in iteration 149 : 0.7950389959830013
Loss in iteration 150 : 0.8283589108748065
Loss in iteration 151 : 0.7955656658824605
Loss in iteration 152 : 0.8267407901350557
Loss in iteration 153 : 0.792116593034653
Loss in iteration 154 : 0.8164761527903918
Loss in iteration 155 : 0.7786846683532048
Loss in iteration 156 : 0.7982231659557117
Loss in iteration 157 : 0.779210181603687
Loss in iteration 158 : 0.8098061663923074
Loss in iteration 159 : 0.7739497392117048
Loss in iteration 160 : 0.805621333143521
Loss in iteration 161 : 0.7810804236438695
Loss in iteration 162 : 0.815488910629155
Loss in iteration 163 : 0.7818198059156698
Loss in iteration 164 : 0.8194376894122553
Loss in iteration 165 : 0.7810372392637261
Loss in iteration 166 : 0.8308089452885371
Loss in iteration 167 : 0.7624411873531254
Loss in iteration 168 : 0.8531777079577435
Loss in iteration 169 : 0.7582943495209113
Loss in iteration 170 : 0.841275933265277
Loss in iteration 171 : 0.7482427078467174
Loss in iteration 172 : 0.7120268478435514
Loss in iteration 173 : 0.7031135601663087
Loss in iteration 174 : 0.6927098876018433
Loss in iteration 175 : 0.6866582996351911
Loss in iteration 176 : 0.6944947556184656
Loss in iteration 177 : 0.6558598630716301
Loss in iteration 178 : 0.6630872490769321
Loss in iteration 179 : 0.6372870632520826
Loss in iteration 180 : 0.6728226519181968
Loss in iteration 181 : 0.6855627017800059
Loss in iteration 182 : 0.8443504169511029
Loss in iteration 183 : 0.9904156862656489
Loss in iteration 184 : 1.117535908955716
Loss in iteration 185 : 1.0102130318237155
Loss in iteration 186 : 0.9848270272860666
Loss in iteration 187 : 0.9453585415462973
Loss in iteration 188 : 0.9457325925546579
Loss in iteration 189 : 0.8880121677920165
Loss in iteration 190 : 0.8734514455639993
Loss in iteration 191 : 0.8258651437300863
Loss in iteration 192 : 0.8136433759742713
Loss in iteration 193 : 0.7945675720877288
Loss in iteration 194 : 0.8001681463176494
Loss in iteration 195 : 0.7730033573039893
Loss in iteration 196 : 0.7877381112643017
Loss in iteration 197 : 0.7773594719720652
Loss in iteration 198 : 0.7982688331856234
Loss in iteration 199 : 0.7715614575317242
Loss in iteration 200 : 0.7985127540119711
Testing accuracy  of updater 8 on alg 1 with rate 2.0 = 0.7215, training accuracy 0.7295, time elapsed: 3151 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3698606849755566
Loss in iteration 3 : 1.8253351872252233
Loss in iteration 4 : 1.5581855923304906
Loss in iteration 5 : 1.1382572958935162
Loss in iteration 6 : 1.3539475034903696
Loss in iteration 7 : 1.0515730373849892
Loss in iteration 8 : 1.2052634457848344
Loss in iteration 9 : 0.8757367534442073
Loss in iteration 10 : 0.8536427164377545
Loss in iteration 11 : 0.7987249869615649
Loss in iteration 12 : 0.7064429787279051
Loss in iteration 13 : 0.6687373295571256
Loss in iteration 14 : 0.6290434177331954
Loss in iteration 15 : 0.6168820489732383
Loss in iteration 16 : 0.6086615036700732
Loss in iteration 17 : 0.6037404833441946
Loss in iteration 18 : 0.6008891004125723
Loss in iteration 19 : 0.5985104379081008
Loss in iteration 20 : 0.5959312094930461
Loss in iteration 21 : 0.5928423813585537
Loss in iteration 22 : 0.589125135330153
Loss in iteration 23 : 0.5846950084284733
Loss in iteration 24 : 0.5796637774486203
Loss in iteration 25 : 0.5741258139186451
Loss in iteration 26 : 0.5684266318007285
Loss in iteration 27 : 0.5627016203368465
Loss in iteration 28 : 0.5571017165961502
Loss in iteration 29 : 0.5518593246621335
Loss in iteration 30 : 0.5466493975425994
Loss in iteration 31 : 0.5419081671617538
Loss in iteration 32 : 0.5397885171488388
Loss in iteration 33 : 0.5420312210937305
Loss in iteration 34 : 0.5527902282724824
Loss in iteration 35 : 0.592203827325044
Loss in iteration 36 : 0.6197638617191313
Loss in iteration 37 : 0.6754950740169445
Loss in iteration 38 : 0.6199994220794875
Loss in iteration 39 : 0.6785005739750191
Loss in iteration 40 : 0.7383492804806171
Loss in iteration 41 : 0.8292733922935738
Loss in iteration 42 : 0.8577114300681783
Loss in iteration 43 : 0.8272420917086617
Loss in iteration 44 : 0.803170196740853
Loss in iteration 45 : 0.7832437260995967
Loss in iteration 46 : 0.7673676144903763
Loss in iteration 47 : 0.7344263220733585
Loss in iteration 48 : 0.7180935985557475
Loss in iteration 49 : 0.7002006291045104
Loss in iteration 50 : 0.6861018741881054
Loss in iteration 51 : 0.6803750754900152
Loss in iteration 52 : 0.6652862877428618
Loss in iteration 53 : 0.659297558997459
Loss in iteration 54 : 0.6539930555696823
Loss in iteration 55 : 0.6510734458970416
Loss in iteration 56 : 0.6423572185899024
Loss in iteration 57 : 0.6454274885269446
Loss in iteration 58 : 0.6402617251083675
Loss in iteration 59 : 0.6479283548167782
Loss in iteration 60 : 0.6424189172680204
Loss in iteration 61 : 0.6490379435030522
Loss in iteration 62 : 0.6404240193322667
Loss in iteration 63 : 0.6461040670871664
Loss in iteration 64 : 0.6410346342931812
Loss in iteration 65 : 0.6436264756238494
Loss in iteration 66 : 0.6382445551516763
Loss in iteration 67 : 0.643315907021437
Loss in iteration 68 : 0.6375268473093681
Loss in iteration 69 : 0.6437856210285197
Loss in iteration 70 : 0.6393665875302266
Loss in iteration 71 : 0.646174927369763
Loss in iteration 72 : 0.6400627659109517
Loss in iteration 73 : 0.6496002850376332
Loss in iteration 74 : 0.6460588095507028
Loss in iteration 75 : 0.6589888401710894
Loss in iteration 76 : 0.6587941240238852
Loss in iteration 77 : 0.6682770889847789
Loss in iteration 78 : 0.6712769910012769
Loss in iteration 79 : 0.6798295762213352
Loss in iteration 80 : 0.6764718807879282
Loss in iteration 81 : 0.6765730623281367
Loss in iteration 82 : 0.677895922199073
Loss in iteration 83 : 0.6772482671253554
Loss in iteration 84 : 0.6736416684540707
Loss in iteration 85 : 0.6741324936825237
Loss in iteration 86 : 0.6713694339981567
Loss in iteration 87 : 0.6726816895086464
Loss in iteration 88 : 0.6703713907653972
Loss in iteration 89 : 0.6697807886321068
Loss in iteration 90 : 0.6613163327621281
Loss in iteration 91 : 0.6639073530673179
Loss in iteration 92 : 0.6580164721496747
Loss in iteration 93 : 0.6605052192072906
Loss in iteration 94 : 0.6538174067553721
Loss in iteration 95 : 0.6571520858693644
Loss in iteration 96 : 0.6590288700148882
Loss in iteration 97 : 0.6632575972304915
Loss in iteration 98 : 0.6593549048862222
Loss in iteration 99 : 0.6623556148501353
Loss in iteration 100 : 0.6636795281495342
Loss in iteration 101 : 0.6669008684152042
Loss in iteration 102 : 0.6634999315417415
Loss in iteration 103 : 0.6646744811493722
Loss in iteration 104 : 0.6597883569282085
Loss in iteration 105 : 0.6641951161445718
Loss in iteration 106 : 0.6593388500601631
Loss in iteration 107 : 0.663066092167466
Loss in iteration 108 : 0.6595170724745992
Loss in iteration 109 : 0.6649408355197013
Loss in iteration 110 : 0.6592565094666675
Loss in iteration 111 : 0.6623756085618038
Loss in iteration 112 : 0.6591241995282321
Loss in iteration 113 : 0.6624004949387752
Loss in iteration 114 : 0.6597098987498814
Loss in iteration 115 : 0.6651773973797543
Loss in iteration 116 : 0.6627648259537356
Loss in iteration 117 : 0.6686904060760105
Loss in iteration 118 : 0.6670998871866107
Loss in iteration 119 : 0.6680988362300176
Loss in iteration 120 : 0.665191394194662
Loss in iteration 121 : 0.6689496148260421
Loss in iteration 122 : 0.6645826949287533
Loss in iteration 123 : 0.6668220724631323
Loss in iteration 124 : 0.6640427972419418
Loss in iteration 125 : 0.6651986880312074
Loss in iteration 126 : 0.6575210996158335
Loss in iteration 127 : 0.6599309975571365
Loss in iteration 128 : 0.6577964664610874
Loss in iteration 129 : 0.6626938065629073
Loss in iteration 130 : 0.6608435192423989
Loss in iteration 131 : 0.6649446249752105
Loss in iteration 132 : 0.6615745037339075
Loss in iteration 133 : 0.663677552553723
Loss in iteration 134 : 0.6600941212636624
Loss in iteration 135 : 0.662326838115837
Loss in iteration 136 : 0.6602744695830379
Loss in iteration 137 : 0.6636148628398421
Loss in iteration 138 : 0.6607986469165108
Loss in iteration 139 : 0.66397936051309
Loss in iteration 140 : 0.6615576840132609
Loss in iteration 141 : 0.6658876178535309
Loss in iteration 142 : 0.6634012403243508
Loss in iteration 143 : 0.6661367478187976
Loss in iteration 144 : 0.6625284720043867
Loss in iteration 145 : 0.6644019727599675
Loss in iteration 146 : 0.66144382741726
Loss in iteration 147 : 0.6644453964940147
Loss in iteration 148 : 0.6627080545917565
Loss in iteration 149 : 0.664980424569369
Loss in iteration 150 : 0.662295994547689
Loss in iteration 151 : 0.6654717659006796
Loss in iteration 152 : 0.6623360077918248
Loss in iteration 153 : 0.6652163394375655
Loss in iteration 154 : 0.6622638103166965
Loss in iteration 155 : 0.6668240935036258
Loss in iteration 156 : 0.6659239196587655
Loss in iteration 157 : 0.6690433191855208
Loss in iteration 158 : 0.6630189432240602
Loss in iteration 159 : 0.6669602798990408
Loss in iteration 160 : 0.6638983077930696
Loss in iteration 161 : 0.6651330260711786
Loss in iteration 162 : 0.6615152022829184
Loss in iteration 163 : 0.6655713903487575
Loss in iteration 164 : 0.6623162926820473
Loss in iteration 165 : 0.6664369624740369
Loss in iteration 166 : 0.6613895657794255
Loss in iteration 167 : 0.6648936764302632
Loss in iteration 168 : 0.6639854681204803
Loss in iteration 169 : 0.6685795871779612
Loss in iteration 170 : 0.6625591426224636
Loss in iteration 171 : 0.666908978744037
Loss in iteration 172 : 0.6637395972081994
Loss in iteration 173 : 0.667035125556308
Loss in iteration 174 : 0.6615962269963392
Loss in iteration 175 : 0.6657079167713679
Loss in iteration 176 : 0.6631214774011847
Loss in iteration 177 : 0.6660234607043803
Loss in iteration 178 : 0.6607117900057327
Loss in iteration 179 : 0.6658231735468149
Loss in iteration 180 : 0.6630697349455436
Loss in iteration 181 : 0.6671829793283723
Loss in iteration 182 : 0.659833625586065
Loss in iteration 183 : 0.6648599396055849
Loss in iteration 184 : 0.664347217875289
Loss in iteration 185 : 0.6670593772516156
Loss in iteration 186 : 0.6612612516613845
Loss in iteration 187 : 0.6659186049408593
Loss in iteration 188 : 0.6633298707731452
Loss in iteration 189 : 0.6656138043403376
Loss in iteration 190 : 0.6599563730994664
Loss in iteration 191 : 0.6669619180873705
Loss in iteration 192 : 0.6618156867728828
Loss in iteration 193 : 0.6673137111063354
Loss in iteration 194 : 0.662982374682775
Loss in iteration 195 : 0.6681937692326437
Loss in iteration 196 : 0.6638610117704259
Loss in iteration 197 : 0.6681508644258581
Loss in iteration 198 : 0.662375428809444
Loss in iteration 199 : 0.666425739504544
Loss in iteration 200 : 0.6609851983459512
Testing accuracy  of updater 8 on alg 1 with rate 1.4000000000000001 = 0.76, training accuracy 0.754125, time elapsed: 2881 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8971386810130486
Loss in iteration 3 : 0.8240371194036993
Loss in iteration 4 : 0.7503457865928849
Loss in iteration 5 : 0.6749147496869486
Loss in iteration 6 : 0.6268850976033848
Loss in iteration 7 : 0.6063847267960327
Loss in iteration 8 : 0.6190351020774404
Loss in iteration 9 : 0.6640196114342705
Loss in iteration 10 : 0.6265706985745004
Loss in iteration 11 : 0.6103200212742917
Loss in iteration 12 : 0.5815104231130472
Loss in iteration 13 : 0.5638994194592458
Loss in iteration 14 : 0.5538300621287822
Loss in iteration 15 : 0.5485512731347668
Loss in iteration 16 : 0.547947094840253
Loss in iteration 17 : 0.5467946690850081
Loss in iteration 18 : 0.5460484380897735
Loss in iteration 19 : 0.5443698475112301
Loss in iteration 20 : 0.5428613163607535
Loss in iteration 21 : 0.5410874636917024
Loss in iteration 22 : 0.5397499346940065
Loss in iteration 23 : 0.5379631013993464
Loss in iteration 24 : 0.536217508972468
Loss in iteration 25 : 0.5342010808705128
Loss in iteration 26 : 0.5322472667384464
Loss in iteration 27 : 0.5302185145785784
Loss in iteration 28 : 0.5281478474466542
Loss in iteration 29 : 0.5261895283983236
Loss in iteration 30 : 0.5242933655280174
Loss in iteration 31 : 0.5224470299596824
Loss in iteration 32 : 0.5207531925566802
Loss in iteration 33 : 0.5191309334640116
Loss in iteration 34 : 0.517638937165913
Loss in iteration 35 : 0.5163716503452871
Loss in iteration 36 : 0.5153753932912938
Loss in iteration 37 : 0.5148151980990586
Loss in iteration 38 : 0.5154782924528613
Loss in iteration 39 : 0.5159341833417144
Loss in iteration 40 : 0.517527598323974
Loss in iteration 41 : 0.5201053429458209
Loss in iteration 42 : 0.5256046141532897
Loss in iteration 43 : 0.5290961059900262
Loss in iteration 44 : 0.5356345611639093
Loss in iteration 45 : 0.5383880529342495
Loss in iteration 46 : 0.5437020207092514
Loss in iteration 47 : 0.5439722170749623
Loss in iteration 48 : 0.5435367622997904
Loss in iteration 49 : 0.5414528144952201
Loss in iteration 50 : 0.5382177513579252
Loss in iteration 51 : 0.5411238545374949
Loss in iteration 52 : 0.5384733630086419
Loss in iteration 53 : 0.5378091612340461
Loss in iteration 54 : 0.5339702797444698
Loss in iteration 55 : 0.5333288276539369
Loss in iteration 56 : 0.5293340837001375
Loss in iteration 57 : 0.5269307994379185
Loss in iteration 58 : 0.526155554813204
Loss in iteration 59 : 0.5233449994235015
Loss in iteration 60 : 0.5218555337945313
Loss in iteration 61 : 0.5207468683331018
Loss in iteration 62 : 0.5194293335805913
Loss in iteration 63 : 0.518061732986637
Loss in iteration 64 : 0.5177219636498086
Loss in iteration 65 : 0.516582757230964
Loss in iteration 66 : 0.5167893441576866
Loss in iteration 67 : 0.5162136388115112
Loss in iteration 68 : 0.5171278050991912
Loss in iteration 69 : 0.5165992346366004
Loss in iteration 70 : 0.5169195427785221
Loss in iteration 71 : 0.5167351483541475
Loss in iteration 72 : 0.5168981931497408
Loss in iteration 73 : 0.516563919371574
Loss in iteration 74 : 0.5169150524034615
Loss in iteration 75 : 0.5168291432988562
Loss in iteration 76 : 0.5164592245698496
Loss in iteration 77 : 0.5162236266616408
Loss in iteration 78 : 0.5165781098715432
Loss in iteration 79 : 0.5166258476390613
Loss in iteration 80 : 0.5171366238047654
Loss in iteration 81 : 0.5171992736619662
Loss in iteration 82 : 0.5177339598796081
Loss in iteration 83 : 0.5187041597115953
Loss in iteration 84 : 0.5191240478684784
Loss in iteration 85 : 0.5204788928501614
Loss in iteration 86 : 0.5205777002287219
Loss in iteration 87 : 0.5218184792103473
Loss in iteration 88 : 0.5220839417099012
Loss in iteration 89 : 0.5236414970591415
Loss in iteration 90 : 0.5244385100727214
Loss in iteration 91 : 0.5256652233235402
Loss in iteration 92 : 0.5254085653873555
Loss in iteration 93 : 0.5269300717874313
Loss in iteration 94 : 0.5280932254298841
Loss in iteration 95 : 0.5309230879738155
Loss in iteration 96 : 0.5331796198525313
Loss in iteration 97 : 0.5357912769547108
Loss in iteration 98 : 0.5369405168948338
Loss in iteration 99 : 0.5393249769804604
Loss in iteration 100 : 0.5398212598334017
Loss in iteration 101 : 0.5417218439721548
Loss in iteration 102 : 0.5432071443357253
Loss in iteration 103 : 0.5441577579124964
Loss in iteration 104 : 0.5467101768242891
Loss in iteration 105 : 0.5455737242615715
Loss in iteration 106 : 0.5477840301599697
Loss in iteration 107 : 0.5464466361763818
Loss in iteration 108 : 0.5471786838661435
Loss in iteration 109 : 0.5444900748004124
Loss in iteration 110 : 0.5473727335242109
Loss in iteration 111 : 0.5452551964666706
Loss in iteration 112 : 0.5468674491439388
Loss in iteration 113 : 0.5455002898406294
Loss in iteration 114 : 0.5450360707362588
Loss in iteration 115 : 0.5434004145345769
Loss in iteration 116 : 0.543156698127495
Loss in iteration 117 : 0.5415292754876236
Loss in iteration 118 : 0.5422019793125604
Loss in iteration 119 : 0.541122755545069
Loss in iteration 120 : 0.5399383069406324
Loss in iteration 121 : 0.5379906551967135
Loss in iteration 122 : 0.5377167370437832
Loss in iteration 123 : 0.5372822641395751
Loss in iteration 124 : 0.5367451068859596
Loss in iteration 125 : 0.537084515621839
Loss in iteration 126 : 0.5372001543533315
Loss in iteration 127 : 0.5373271108086535
Loss in iteration 128 : 0.5367551952660372
Loss in iteration 129 : 0.5372445418643215
Loss in iteration 130 : 0.5377106092172027
Loss in iteration 131 : 0.5382075855786262
Loss in iteration 132 : 0.5388862386885278
Loss in iteration 133 : 0.5391136865107655
Loss in iteration 134 : 0.5408946657389851
Loss in iteration 135 : 0.5407298295911138
Loss in iteration 136 : 0.5414267988048111
Loss in iteration 137 : 0.5424600616979228
Loss in iteration 138 : 0.5435650544727394
Loss in iteration 139 : 0.5440788678461891
Loss in iteration 140 : 0.5454299871458479
Loss in iteration 141 : 0.5464864147500617
Loss in iteration 142 : 0.5469007950323226
Loss in iteration 143 : 0.547315394683958
Loss in iteration 144 : 0.5460748801370581
Loss in iteration 145 : 0.5468106375151583
Loss in iteration 146 : 0.5460287186412479
Loss in iteration 147 : 0.546045299464382
Loss in iteration 148 : 0.5450115801657253
Loss in iteration 149 : 0.5458910643850522
Loss in iteration 150 : 0.5457686193433738
Loss in iteration 151 : 0.5460578819783468
Loss in iteration 152 : 0.5455725659474531
Loss in iteration 153 : 0.5462072864179663
Loss in iteration 154 : 0.5451628097822888
Loss in iteration 155 : 0.5455864473302305
Loss in iteration 156 : 0.5456775013449544
Loss in iteration 157 : 0.5460688662621319
Loss in iteration 158 : 0.5455707940669597
Loss in iteration 159 : 0.5457449918894031
Loss in iteration 160 : 0.5457497603875956
Loss in iteration 161 : 0.5462121954769725
Loss in iteration 162 : 0.5458779103748223
Loss in iteration 163 : 0.546390548932655
Loss in iteration 164 : 0.546142786716724
Loss in iteration 165 : 0.5467166092317257
Loss in iteration 166 : 0.5456336899063243
Loss in iteration 167 : 0.5460710441153892
Loss in iteration 168 : 0.5460494812674436
Loss in iteration 169 : 0.5470383488641295
Loss in iteration 170 : 0.5457078726775334
Loss in iteration 171 : 0.5461114868159107
Loss in iteration 172 : 0.54659649022857
Loss in iteration 173 : 0.547397804380047
Loss in iteration 174 : 0.5459200223026476
Loss in iteration 175 : 0.5461178028732023
Loss in iteration 176 : 0.5462247017260429
Loss in iteration 177 : 0.5475175840854467
Loss in iteration 178 : 0.5460097238392665
Loss in iteration 179 : 0.5459021601028483
Loss in iteration 180 : 0.5465749193124209
Loss in iteration 181 : 0.5484773116782402
Loss in iteration 182 : 0.5475816203893046
Loss in iteration 183 : 0.5478217651453015
Loss in iteration 184 : 0.5467539884153059
Loss in iteration 185 : 0.5471876125559153
Loss in iteration 186 : 0.5473901768127203
Loss in iteration 187 : 0.5485001802419112
Loss in iteration 188 : 0.5474511432073628
Loss in iteration 189 : 0.5475446789959071
Loss in iteration 190 : 0.5466438504364229
Loss in iteration 191 : 0.5471143269241089
Loss in iteration 192 : 0.5465467551056732
Loss in iteration 193 : 0.5475428472712258
Loss in iteration 194 : 0.5465538600014349
Loss in iteration 195 : 0.5475334127569831
Loss in iteration 196 : 0.5468517343499997
Loss in iteration 197 : 0.5480953082665868
Loss in iteration 198 : 0.5466434506398141
Loss in iteration 199 : 0.5483243065840362
Loss in iteration 200 : 0.5477134438825868
Testing accuracy  of updater 8 on alg 1 with rate 0.8 = 0.774, training accuracy 0.7725, time elapsed: 3220 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9493228902396034
Loss in iteration 3 : 0.9063065682740775
Loss in iteration 4 : 0.8887598151446634
Loss in iteration 5 : 0.8597016856452739
Loss in iteration 6 : 0.8252422794787253
Loss in iteration 7 : 0.7904360535656105
Loss in iteration 8 : 0.7551430076764584
Loss in iteration 9 : 0.7232403785857968
Loss in iteration 10 : 0.6904762063514185
Loss in iteration 11 : 0.6595248796250531
Loss in iteration 12 : 0.6360225860976634
Loss in iteration 13 : 0.6159770581773877
Loss in iteration 14 : 0.6003125824522997
Loss in iteration 15 : 0.5884974453474238
Loss in iteration 16 : 0.5793980521829479
Loss in iteration 17 : 0.5720457266437322
Loss in iteration 18 : 0.5660071161138052
Loss in iteration 19 : 0.5610329874809074
Loss in iteration 20 : 0.556985807431475
Loss in iteration 21 : 0.5536751159035103
Loss in iteration 22 : 0.5509675720312597
Loss in iteration 23 : 0.548681016205876
Loss in iteration 24 : 0.5467178580107993
Loss in iteration 25 : 0.5449688316610214
Loss in iteration 26 : 0.5433678530516463
Loss in iteration 27 : 0.5419249624115906
Loss in iteration 28 : 0.5406378569265605
Loss in iteration 29 : 0.5394644949865738
Loss in iteration 30 : 0.5383618532021612
Loss in iteration 31 : 0.5373564483631746
Loss in iteration 32 : 0.5364184697282125
Loss in iteration 33 : 0.5355123179281834
Loss in iteration 34 : 0.5346353393776054
Loss in iteration 35 : 0.5337855131659337
Loss in iteration 36 : 0.5329698694958952
Loss in iteration 37 : 0.5321736381243567
Loss in iteration 38 : 0.5314025084437735
Loss in iteration 39 : 0.5306446894163073
Loss in iteration 40 : 0.5299038249003061
Loss in iteration 41 : 0.5291750924510543
Loss in iteration 42 : 0.5284660583429719
Loss in iteration 43 : 0.5277880142904238
Loss in iteration 44 : 0.5271227293081554
Loss in iteration 45 : 0.5264719476339824
Loss in iteration 46 : 0.5258401233595807
Loss in iteration 47 : 0.5252314524847576
Loss in iteration 48 : 0.5246485706580958
Loss in iteration 49 : 0.5240980089764933
Loss in iteration 50 : 0.5235762806928697
Loss in iteration 51 : 0.5230794079462043
Loss in iteration 52 : 0.522608963085773
Loss in iteration 53 : 0.5221583992613834
Loss in iteration 54 : 0.5217227835180874
Loss in iteration 55 : 0.5213015555253501
Loss in iteration 56 : 0.5208914164465748
Loss in iteration 57 : 0.5204984979668148
Loss in iteration 58 : 0.520119516307021
Loss in iteration 59 : 0.5197544018904896
Loss in iteration 60 : 0.5194009944223518
Loss in iteration 61 : 0.5190604130801946
Loss in iteration 62 : 0.5187370320349715
Loss in iteration 63 : 0.5184294430745834
Loss in iteration 64 : 0.5181409370376716
Loss in iteration 65 : 0.5178713853774067
Loss in iteration 66 : 0.517614466471378
Loss in iteration 67 : 0.5173714786354824
Loss in iteration 68 : 0.5171349314103483
Loss in iteration 69 : 0.5169065870440543
Loss in iteration 70 : 0.5166884231851641
Loss in iteration 71 : 0.516481373604652
Loss in iteration 72 : 0.5162864236716995
Loss in iteration 73 : 0.5160960248591162
Loss in iteration 74 : 0.5159134256625005
Loss in iteration 75 : 0.515736869793862
Loss in iteration 76 : 0.5155653857842251
Loss in iteration 77 : 0.5153980492648784
Loss in iteration 78 : 0.5152374686922676
Loss in iteration 79 : 0.5150821426369095
Loss in iteration 80 : 0.5149328972975941
Loss in iteration 81 : 0.5147893027092911
Loss in iteration 82 : 0.5146507786551631
Loss in iteration 83 : 0.5145159779792604
Loss in iteration 84 : 0.5143843583271155
Loss in iteration 85 : 0.51425638714455
Loss in iteration 86 : 0.5141316256731935
Loss in iteration 87 : 0.5140096485579605
Loss in iteration 88 : 0.5138922454653501
Loss in iteration 89 : 0.5137764769510431
Loss in iteration 90 : 0.5136639006188412
Loss in iteration 91 : 0.5135534230467157
Loss in iteration 92 : 0.5134471270872436
Loss in iteration 93 : 0.5133429001868589
Loss in iteration 94 : 0.513241128593345
Loss in iteration 95 : 0.5131428315341963
Loss in iteration 96 : 0.5130472575521255
Loss in iteration 97 : 0.5129522447513685
Loss in iteration 98 : 0.5128584070928435
Loss in iteration 99 : 0.5127680703666072
Loss in iteration 100 : 0.5126788840409984
Loss in iteration 101 : 0.5125915403253566
Loss in iteration 102 : 0.5125040502747752
Loss in iteration 103 : 0.5124193040822377
Loss in iteration 104 : 0.5123371401062874
Loss in iteration 105 : 0.5122577167486064
Loss in iteration 106 : 0.5121797824864787
Loss in iteration 107 : 0.5121035019530371
Loss in iteration 108 : 0.5120285972264079
Loss in iteration 109 : 0.5119557721094111
Loss in iteration 110 : 0.511883952664391
Loss in iteration 111 : 0.5118154575266041
Loss in iteration 112 : 0.5117496297153987
Loss in iteration 113 : 0.5116854226805561
Loss in iteration 114 : 0.5116236394587087
Loss in iteration 115 : 0.5115630652519537
Loss in iteration 116 : 0.5115032391179739
Loss in iteration 117 : 0.5114452232971332
Loss in iteration 118 : 0.5113885876180088
Loss in iteration 119 : 0.5113324206560599
Loss in iteration 120 : 0.5112775103908972
Loss in iteration 121 : 0.5112250201016824
Loss in iteration 122 : 0.5111731836288562
Loss in iteration 123 : 0.5111230309902818
Loss in iteration 124 : 0.5110734805148847
Loss in iteration 125 : 0.5110249562825117
Loss in iteration 126 : 0.510976838541368
Loss in iteration 127 : 0.5109302244469888
Loss in iteration 128 : 0.5108855640351614
Loss in iteration 129 : 0.5108419671077262
Loss in iteration 130 : 0.5107976555474083
Loss in iteration 131 : 0.5107553081011259
Loss in iteration 132 : 0.5107130610495011
Loss in iteration 133 : 0.5106732804290608
Loss in iteration 134 : 0.5106332787823923
Loss in iteration 135 : 0.5105948227168535
Loss in iteration 136 : 0.510556644621763
Loss in iteration 137 : 0.5105201239086787
Loss in iteration 138 : 0.5104833770136852
Loss in iteration 139 : 0.510448832578054
Loss in iteration 140 : 0.510415050854247
Loss in iteration 141 : 0.5103816784093169
Loss in iteration 142 : 0.510349705802832
Loss in iteration 143 : 0.5103171713898347
Loss in iteration 144 : 0.5102866238392814
Loss in iteration 145 : 0.5102557625151757
Loss in iteration 146 : 0.5102245933664624
Loss in iteration 147 : 0.5101951693585444
Loss in iteration 148 : 0.510165025855948
Loss in iteration 149 : 0.510136461663586
Loss in iteration 150 : 0.5101084553712953
Loss in iteration 151 : 0.510081954060434
Loss in iteration 152 : 0.5100559814474565
Loss in iteration 153 : 0.5100285853125084
Loss in iteration 154 : 0.5100016207511424
Loss in iteration 155 : 0.5099766552329841
Loss in iteration 156 : 0.5099519400611969
Loss in iteration 157 : 0.5099266910953675
Loss in iteration 158 : 0.5099021900106094
Loss in iteration 159 : 0.5098780696484996
Loss in iteration 160 : 0.5098536770204519
Loss in iteration 161 : 0.5098298290562868
Loss in iteration 162 : 0.509805894002962
Loss in iteration 163 : 0.5097825205020661
Loss in iteration 164 : 0.5097593829877094
Loss in iteration 165 : 0.509736475123013
Loss in iteration 166 : 0.5097135105594306
Loss in iteration 167 : 0.5096908085872077
Loss in iteration 168 : 0.5096690832197392
Loss in iteration 169 : 0.5096474509770561
Loss in iteration 170 : 0.5096261551401892
Loss in iteration 171 : 0.5096041431478823
Loss in iteration 172 : 0.5095827122512202
Loss in iteration 173 : 0.5095620827436723
Loss in iteration 174 : 0.5095417093336159
Loss in iteration 175 : 0.5095216976604208
Loss in iteration 176 : 0.5095006490218277
Loss in iteration 177 : 0.5094819407894369
Loss in iteration 178 : 0.5094615821558842
Loss in iteration 179 : 0.5094436592390996
Loss in iteration 180 : 0.5094258101811421
Loss in iteration 181 : 0.5094059037540164
Loss in iteration 182 : 0.5093880666689826
Loss in iteration 183 : 0.509370033324237
Loss in iteration 184 : 0.5093522970993662
Loss in iteration 185 : 0.5093352563393233
Loss in iteration 186 : 0.5093182453358381
Loss in iteration 187 : 0.5093014663902767
Loss in iteration 188 : 0.509284867461478
Loss in iteration 189 : 0.5092683616157643
Loss in iteration 190 : 0.509252518159835
Loss in iteration 191 : 0.5092370082884573
Loss in iteration 192 : 0.5092208079726676
Loss in iteration 193 : 0.5092052360165652
Loss in iteration 194 : 0.5091897099901257
Loss in iteration 195 : 0.5091743549314619
Loss in iteration 196 : 0.509159227182701
Loss in iteration 197 : 0.5091441058763964
Loss in iteration 198 : 0.5091291167682107
Loss in iteration 199 : 0.5091144435434499
Loss in iteration 200 : 0.5090999360768248
Testing accuracy  of updater 8 on alg 1 with rate 0.2 = 0.781, training accuracy 0.7875, time elapsed: 4281 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9515280203013272
Loss in iteration 3 : 0.9123980030047101
Loss in iteration 4 : 0.9017500050443841
Loss in iteration 5 : 0.8767499882386388
Loss in iteration 6 : 0.8467722986376444
Loss in iteration 7 : 0.8169992164430646
Loss in iteration 8 : 0.7863533186627769
Loss in iteration 9 : 0.7554285737475691
Loss in iteration 10 : 0.7267474221617706
Loss in iteration 11 : 0.697675180173487
Loss in iteration 12 : 0.6690044549624241
Loss in iteration 13 : 0.6456153774102156
Loss in iteration 14 : 0.6265986882578265
Loss in iteration 15 : 0.6104698910736767
Loss in iteration 16 : 0.5977511408917385
Loss in iteration 17 : 0.5878766467741489
Loss in iteration 18 : 0.5800352960587386
Loss in iteration 19 : 0.5734964066370817
Loss in iteration 20 : 0.567911308172939
Loss in iteration 21 : 0.5632646750606993
Loss in iteration 22 : 0.5593793160840516
Loss in iteration 23 : 0.5561285739298362
Loss in iteration 24 : 0.5534073409231726
Loss in iteration 25 : 0.5510936653598638
Loss in iteration 26 : 0.5490557145859418
Loss in iteration 27 : 0.5472614635341259
Loss in iteration 28 : 0.5456486728564266
Loss in iteration 29 : 0.5441641917305192
Loss in iteration 30 : 0.5427932858663567
Loss in iteration 31 : 0.5415320899409322
Loss in iteration 32 : 0.5403889159882629
Loss in iteration 33 : 0.539341513385639
Loss in iteration 34 : 0.538348836992082
Loss in iteration 35 : 0.5374227009596186
Loss in iteration 36 : 0.5365566567786528
Loss in iteration 37 : 0.5357353725116498
Loss in iteration 38 : 0.5349447163930091
Loss in iteration 39 : 0.5341819682650322
Loss in iteration 40 : 0.5334419326070077
Loss in iteration 41 : 0.5327206903008165
Loss in iteration 42 : 0.5320233133986622
Loss in iteration 43 : 0.5313497438066124
Loss in iteration 44 : 0.5306949881336375
Loss in iteration 45 : 0.5300584860164276
Loss in iteration 46 : 0.5294350345831572
Loss in iteration 47 : 0.5288291349292485
Loss in iteration 48 : 0.5282353763148429
Loss in iteration 49 : 0.5276592400255717
Loss in iteration 50 : 0.5271052921280054
Loss in iteration 51 : 0.5265679133715021
Loss in iteration 52 : 0.5260506117899981
Loss in iteration 53 : 0.5255484653864284
Loss in iteration 54 : 0.5250664809358241
Loss in iteration 55 : 0.524602299531935
Loss in iteration 56 : 0.5241564870008337
Loss in iteration 57 : 0.5237265400649853
Loss in iteration 58 : 0.5233197346464447
Loss in iteration 59 : 0.5229280599350309
Loss in iteration 60 : 0.5225539564769963
Loss in iteration 61 : 0.5221937071169253
Loss in iteration 62 : 0.5218471633710969
Loss in iteration 63 : 0.5215094994566102
Loss in iteration 64 : 0.5211812401354923
Loss in iteration 65 : 0.5208637308176278
Loss in iteration 66 : 0.5205546935593237
Loss in iteration 67 : 0.5202560000172225
Loss in iteration 68 : 0.519963851233637
Loss in iteration 69 : 0.5196823255979365
Loss in iteration 70 : 0.5194137882338394
Loss in iteration 71 : 0.5191574441591524
Loss in iteration 72 : 0.5189147262411159
Loss in iteration 73 : 0.5186830957237257
Loss in iteration 74 : 0.5184582666498578
Loss in iteration 75 : 0.5182425527143724
Loss in iteration 76 : 0.5180317695365797
Loss in iteration 77 : 0.517827602511359
Loss in iteration 78 : 0.517631901695154
Loss in iteration 79 : 0.5174406527023988
Loss in iteration 80 : 0.5172535527896112
Loss in iteration 81 : 0.5170707837558086
Loss in iteration 82 : 0.5168941426661667
Loss in iteration 83 : 0.5167229581150711
Loss in iteration 84 : 0.5165566662490698
Loss in iteration 85 : 0.5163957439925484
Loss in iteration 86 : 0.5162413310316389
Loss in iteration 87 : 0.5160910650793115
Loss in iteration 88 : 0.5159451275708986
Loss in iteration 89 : 0.5158006619185419
Loss in iteration 90 : 0.515660318476672
Loss in iteration 91 : 0.5155252727575628
Loss in iteration 92 : 0.5153932350108226
Loss in iteration 93 : 0.5152638665137906
Loss in iteration 94 : 0.5151367718021389
Loss in iteration 95 : 0.5150126317827233
Loss in iteration 96 : 0.5148910413316345
Loss in iteration 97 : 0.5147731683293902
Loss in iteration 98 : 0.5146591883146386
Loss in iteration 99 : 0.5145478070295509
Loss in iteration 100 : 0.5144391755951049
Loss in iteration 101 : 0.5143326601455542
Loss in iteration 102 : 0.5142285125956709
Loss in iteration 103 : 0.5141267552145373
Loss in iteration 104 : 0.5140266724881302
Loss in iteration 105 : 0.5139288133406594
Loss in iteration 106 : 0.5138327686591905
Loss in iteration 107 : 0.5137379892163796
Loss in iteration 108 : 0.5136455635346724
Loss in iteration 109 : 0.513554644980071
Loss in iteration 110 : 0.5134657580290712
Loss in iteration 111 : 0.5133789920230367
Loss in iteration 112 : 0.5132934093358978
Loss in iteration 113 : 0.5132091739578237
Loss in iteration 114 : 0.513126125473769
Loss in iteration 115 : 0.5130443340621287
Loss in iteration 116 : 0.5129634193958912
Loss in iteration 117 : 0.5128841013917517
Loss in iteration 118 : 0.5128064175591676
Loss in iteration 119 : 0.5127295774059296
Loss in iteration 120 : 0.5126538217488179
Loss in iteration 121 : 0.5125792038131688
Loss in iteration 122 : 0.5125065778935245
Loss in iteration 123 : 0.5124335706017573
Loss in iteration 124 : 0.5123628279915639
Loss in iteration 125 : 0.5122949223252069
Loss in iteration 126 : 0.5122276642980085
Loss in iteration 127 : 0.5121615195915006
Loss in iteration 128 : 0.512097088810777
Loss in iteration 129 : 0.512033179299089
Loss in iteration 130 : 0.5119717664381318
Loss in iteration 131 : 0.5119120247005091
Loss in iteration 132 : 0.5118537322216075
Loss in iteration 133 : 0.5117970702082719
Loss in iteration 134 : 0.5117414289181895
Loss in iteration 135 : 0.5116868217962675
Loss in iteration 136 : 0.5116327606441068
Loss in iteration 137 : 0.5115801927288587
Loss in iteration 138 : 0.5115282138725169
Loss in iteration 139 : 0.5114775739434351
Loss in iteration 140 : 0.5114280866519851
Loss in iteration 141 : 0.5113795716774396
Loss in iteration 142 : 0.5113316427543051
Loss in iteration 143 : 0.5112843541101054
Loss in iteration 144 : 0.5112379558453917
Loss in iteration 145 : 0.511193121288429
Loss in iteration 146 : 0.5111487431840174
Loss in iteration 147 : 0.5111050383419106
Loss in iteration 148 : 0.5110625041361831
Loss in iteration 149 : 0.5110217486208949
Loss in iteration 150 : 0.5109809788105101
Loss in iteration 151 : 0.5109415320703243
Loss in iteration 152 : 0.5109021354132127
Loss in iteration 153 : 0.5108634051184106
Loss in iteration 154 : 0.510826075096587
Loss in iteration 155 : 0.5107891824217093
Loss in iteration 156 : 0.5107527828351908
Loss in iteration 157 : 0.5107178115705497
Loss in iteration 158 : 0.5106822316158092
Loss in iteration 159 : 0.5106480549056256
Loss in iteration 160 : 0.5106148880652044
Loss in iteration 161 : 0.5105822486073675
Loss in iteration 162 : 0.5105503940607331
Loss in iteration 163 : 0.5105189563172192
Loss in iteration 164 : 0.5104884488934632
Loss in iteration 165 : 0.5104593033536539
Loss in iteration 166 : 0.5104310281180374
Loss in iteration 167 : 0.5104020607634258
Loss in iteration 168 : 0.5103751837918657
Loss in iteration 169 : 0.510348468569039
Loss in iteration 170 : 0.5103197994063273
Loss in iteration 171 : 0.510294104674467
Loss in iteration 172 : 0.5102675431424443
Loss in iteration 173 : 0.5102417402574154
Loss in iteration 174 : 0.5102163675811098
Loss in iteration 175 : 0.5101915090199504
Loss in iteration 176 : 0.5101673643980262
Loss in iteration 177 : 0.5101435935302016
Loss in iteration 178 : 0.5101204922999987
Loss in iteration 179 : 0.5100968991707644
Loss in iteration 180 : 0.5100730980052249
Loss in iteration 181 : 0.5100498808163185
Loss in iteration 182 : 0.5100272108862544
Loss in iteration 183 : 0.5100050274497642
Loss in iteration 184 : 0.509982926568955
Loss in iteration 185 : 0.5099611002111919
Loss in iteration 186 : 0.5099395958729701
Loss in iteration 187 : 0.5099184687693582
Loss in iteration 188 : 0.5098979602100107
Loss in iteration 189 : 0.5098770132039354
Loss in iteration 190 : 0.5098571941128082
Loss in iteration 191 : 0.5098370556206505
Loss in iteration 192 : 0.509816437601382
Loss in iteration 193 : 0.5097962180701834
Loss in iteration 194 : 0.5097764512899493
Loss in iteration 195 : 0.5097569863490995
Loss in iteration 196 : 0.5097385773495682
Loss in iteration 197 : 0.5097181548851377
Loss in iteration 198 : 0.5096997300068089
Loss in iteration 199 : 0.5096805582205294
Loss in iteration 200 : 0.5096619934543339
Testing accuracy  of updater 8 on alg 1 with rate 0.14 = 0.7805, training accuracy 0.787625, time elapsed: 3159 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9668973955441557
Loss in iteration 3 : 0.9278297090637156
Loss in iteration 4 : 0.9087303485101025
Loss in iteration 5 : 0.9006102004168397
Loss in iteration 6 : 0.8830971082534171
Loss in iteration 7 : 0.8601233404600132
Loss in iteration 8 : 0.8364897033439108
Loss in iteration 9 : 0.8133788979565463
Loss in iteration 10 : 0.7900648209249567
Loss in iteration 11 : 0.7664737391808782
Loss in iteration 12 : 0.7441419338153552
Loss in iteration 13 : 0.7227276057518436
Loss in iteration 14 : 0.7006276233635674
Loss in iteration 15 : 0.6787346765460431
Loss in iteration 16 : 0.6592698051623598
Loss in iteration 17 : 0.6431498386385045
Loss in iteration 18 : 0.6292238920603874
Loss in iteration 19 : 0.6169906506349032
Loss in iteration 20 : 0.6066677716762335
Loss in iteration 21 : 0.5980782316810699
Loss in iteration 22 : 0.5909563288997136
Loss in iteration 23 : 0.5850072614692231
Loss in iteration 24 : 0.5799011158678745
Loss in iteration 25 : 0.5752851273814158
Loss in iteration 26 : 0.5711119823843026
Loss in iteration 27 : 0.5674734528627242
Loss in iteration 28 : 0.5642955492540389
Loss in iteration 29 : 0.5615480816903453
Loss in iteration 30 : 0.5591296271310332
Loss in iteration 31 : 0.5569926981376948
Loss in iteration 32 : 0.5551120429788151
Loss in iteration 33 : 0.5534198509298571
Loss in iteration 34 : 0.551860152021098
Loss in iteration 35 : 0.5504107235315023
Loss in iteration 36 : 0.5490595764554521
Loss in iteration 37 : 0.5478039392439639
Loss in iteration 38 : 0.5466249524583359
Loss in iteration 39 : 0.5455246649135058
Loss in iteration 40 : 0.544512117031836
Loss in iteration 41 : 0.5435665623578468
Loss in iteration 42 : 0.5426770354549382
Loss in iteration 43 : 0.5418389590101471
Loss in iteration 44 : 0.5410338554223629
Loss in iteration 45 : 0.5402564705650875
Loss in iteration 46 : 0.5395203697762797
Loss in iteration 47 : 0.5388276467756182
Loss in iteration 48 : 0.538171896177383
Loss in iteration 49 : 0.5375437423867838
Loss in iteration 50 : 0.5369357434669607
Loss in iteration 51 : 0.5363463933823425
Loss in iteration 52 : 0.5357730991644882
Loss in iteration 53 : 0.5352192208104743
Loss in iteration 54 : 0.5346793078141793
Loss in iteration 55 : 0.5341494302861215
Loss in iteration 56 : 0.5336407511831104
Loss in iteration 57 : 0.5331488921063194
Loss in iteration 58 : 0.5326747126376418
Loss in iteration 59 : 0.5322129405167753
Loss in iteration 60 : 0.5317616471651305
Loss in iteration 61 : 0.5313247941151731
Loss in iteration 62 : 0.5308994366592535
Loss in iteration 63 : 0.5304865886680591
Loss in iteration 64 : 0.5300842658815919
Loss in iteration 65 : 0.5296940073480809
Loss in iteration 66 : 0.5293117045360707
Loss in iteration 67 : 0.5289377966660772
Loss in iteration 68 : 0.5285732146459654
Loss in iteration 69 : 0.5282188024318434
Loss in iteration 70 : 0.5278727764330831
Loss in iteration 71 : 0.5275326821526634
Loss in iteration 72 : 0.5271992850248371
Loss in iteration 73 : 0.526874583529978
Loss in iteration 74 : 0.5265579822108786
Loss in iteration 75 : 0.5262494789339848
Loss in iteration 76 : 0.5259498206514865
Loss in iteration 77 : 0.5256570486678873
Loss in iteration 78 : 0.5253708362489209
Loss in iteration 79 : 0.5250914683664841
Loss in iteration 80 : 0.5248179259579929
Loss in iteration 81 : 0.524547279385315
Loss in iteration 82 : 0.5242832548099144
Loss in iteration 83 : 0.5240252932753577
Loss in iteration 84 : 0.5237719710290094
Loss in iteration 85 : 0.5235246783753992
Loss in iteration 86 : 0.5232843811525258
Loss in iteration 87 : 0.5230500765478482
Loss in iteration 88 : 0.5228208681818642
Loss in iteration 89 : 0.5225962795049642
Loss in iteration 90 : 0.5223774714109839
Loss in iteration 91 : 0.5221617549752062
Loss in iteration 92 : 0.5219517744347296
Loss in iteration 93 : 0.5217476932723949
Loss in iteration 94 : 0.5215469689467442
Loss in iteration 95 : 0.5213502610812507
Loss in iteration 96 : 0.5211583612886043
Loss in iteration 97 : 0.5209703308272878
Loss in iteration 98 : 0.5207845939313643
Loss in iteration 99 : 0.5206014673179228
Loss in iteration 100 : 0.5204214711190818
Loss in iteration 101 : 0.5202452803908675
Loss in iteration 102 : 0.520072139921386
Loss in iteration 103 : 0.5199032537323246
Loss in iteration 104 : 0.519737490667455
Loss in iteration 105 : 0.5195740844277391
Loss in iteration 106 : 0.5194145967422552
Loss in iteration 107 : 0.5192583051735455
Loss in iteration 108 : 0.5191051036026403
Loss in iteration 109 : 0.5189556421877458
Loss in iteration 110 : 0.5188101649814271
Loss in iteration 111 : 0.5186682751482888
Loss in iteration 112 : 0.5185277139690851
Loss in iteration 113 : 0.5183900468715179
Loss in iteration 114 : 0.5182548384119495
Loss in iteration 115 : 0.5181217151162475
Loss in iteration 116 : 0.5179904038927562
Loss in iteration 117 : 0.5178612852468926
Loss in iteration 118 : 0.5177349579294899
Loss in iteration 119 : 0.5176110214119927
Loss in iteration 120 : 0.5174892140143603
Loss in iteration 121 : 0.5173688906308582
Loss in iteration 122 : 0.5172503343005027
Loss in iteration 123 : 0.5171338342273395
Loss in iteration 124 : 0.5170190880215554
Loss in iteration 125 : 0.516905780534152
Loss in iteration 126 : 0.516795153624173
Loss in iteration 127 : 0.516686667503619
Loss in iteration 128 : 0.5165809825503092
Loss in iteration 129 : 0.5164767564340822
Loss in iteration 130 : 0.5163744542388922
Loss in iteration 131 : 0.5162740373058416
Loss in iteration 132 : 0.5161750500755798
Loss in iteration 133 : 0.5160775322866213
Loss in iteration 134 : 0.5159813352242538
Loss in iteration 135 : 0.5158868667466443
Loss in iteration 136 : 0.5157934596666153
Loss in iteration 137 : 0.5157007037890516
Loss in iteration 138 : 0.5156086947081643
Loss in iteration 139 : 0.5155181902492947
Loss in iteration 140 : 0.5154291115471438
Loss in iteration 141 : 0.5153423301496846
Loss in iteration 142 : 0.5152573952323201
Loss in iteration 143 : 0.5151741567196564
Loss in iteration 144 : 0.5150918640002153
Loss in iteration 145 : 0.5150103186214487
Loss in iteration 146 : 0.5149301535736925
Loss in iteration 147 : 0.5148512519570516
Loss in iteration 148 : 0.5147726671705001
Loss in iteration 149 : 0.514695131255211
Loss in iteration 150 : 0.5146184032825101
Loss in iteration 151 : 0.5145424033175454
Loss in iteration 152 : 0.5144679162654665
Loss in iteration 153 : 0.5143949701133955
Loss in iteration 154 : 0.5143228105988291
Loss in iteration 155 : 0.5142518788248821
Loss in iteration 156 : 0.5141829388831879
Loss in iteration 157 : 0.5141147009450738
Loss in iteration 158 : 0.5140473860625883
Loss in iteration 159 : 0.5139806236808333
Loss in iteration 160 : 0.5139144942216576
Loss in iteration 161 : 0.5138492198755862
Loss in iteration 162 : 0.5137848617039388
Loss in iteration 163 : 0.5137211444629277
Loss in iteration 164 : 0.5136583639332482
Loss in iteration 165 : 0.513596198637198
Loss in iteration 166 : 0.5135345219511687
Loss in iteration 167 : 0.5134738145880918
Loss in iteration 168 : 0.5134140780434666
Loss in iteration 169 : 0.5133554062033441
Loss in iteration 170 : 0.5132971760858275
Loss in iteration 171 : 0.5132397992909784
Loss in iteration 172 : 0.5131835264045628
Loss in iteration 173 : 0.5131282940345198
Loss in iteration 174 : 0.5130733117303667
Loss in iteration 175 : 0.5130192687162048
Loss in iteration 176 : 0.5129662547354231
Loss in iteration 177 : 0.5129143798190184
Loss in iteration 178 : 0.5128626940481581
Loss in iteration 179 : 0.5128111455045508
Loss in iteration 180 : 0.5127599490195877
Loss in iteration 181 : 0.512709213354326
Loss in iteration 182 : 0.5126592211878976
Loss in iteration 183 : 0.5126096386293302
Loss in iteration 184 : 0.5125605830266553
Loss in iteration 185 : 0.5125118553577865
Loss in iteration 186 : 0.5124635460650075
Loss in iteration 187 : 0.5124158265599876
Loss in iteration 188 : 0.5123684226214104
Loss in iteration 189 : 0.5123215558666583
Loss in iteration 190 : 0.5122754222054434
Loss in iteration 191 : 0.5122299836791628
Loss in iteration 192 : 0.5121852560863969
Loss in iteration 193 : 0.5121409822810754
Loss in iteration 194 : 0.5120967656608115
Loss in iteration 195 : 0.5120536920187849
Loss in iteration 196 : 0.5120104543069679
Loss in iteration 197 : 0.5119676780969106
Loss in iteration 198 : 0.5119262858106587
Loss in iteration 199 : 0.5118851088126896
Loss in iteration 200 : 0.5118449142913727
Testing accuracy  of updater 8 on alg 1 with rate 0.08000000000000002 = 0.783, training accuracy 0.7875, time elapsed: 3129 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9904364196785138
Loss in iteration 3 : 0.9769122469188601
Loss in iteration 4 : 0.9601452451786116
Loss in iteration 5 : 0.9423086677902991
Loss in iteration 6 : 0.9265552554574221
Loss in iteration 7 : 0.9153726718962076
Loss in iteration 8 : 0.9088432002232775
Loss in iteration 9 : 0.9046631260435826
Loss in iteration 10 : 0.9006530352178904
Loss in iteration 11 : 0.8951300781402918
Loss in iteration 12 : 0.8874473160433384
Loss in iteration 13 : 0.8783369796272801
Loss in iteration 14 : 0.8684503034927186
Loss in iteration 15 : 0.8584995452217025
Loss in iteration 16 : 0.8488952847655484
Loss in iteration 17 : 0.8396547462712406
Loss in iteration 18 : 0.8306288698788867
Loss in iteration 19 : 0.8218374807740408
Loss in iteration 20 : 0.8131835643658817
Loss in iteration 21 : 0.8044995300923479
Loss in iteration 22 : 0.7957254661477345
Loss in iteration 23 : 0.7868728045300516
Loss in iteration 24 : 0.7779912795836632
Loss in iteration 25 : 0.7691569315887302
Loss in iteration 26 : 0.7605248796579694
Loss in iteration 27 : 0.7521971530088382
Loss in iteration 28 : 0.7440096002909476
Loss in iteration 29 : 0.735926004786238
Loss in iteration 30 : 0.7278918563429968
Loss in iteration 31 : 0.7198848774616329
Loss in iteration 32 : 0.7119413416449643
Loss in iteration 33 : 0.7041072175296222
Loss in iteration 34 : 0.6964317394520081
Loss in iteration 35 : 0.6889930695185658
Loss in iteration 36 : 0.6818285284909295
Loss in iteration 37 : 0.6750106703323732
Loss in iteration 38 : 0.6685308302523437
Loss in iteration 39 : 0.6624282708998063
Loss in iteration 40 : 0.65670846573771
Loss in iteration 41 : 0.6514004095910795
Loss in iteration 42 : 0.6464110215944168
Loss in iteration 43 : 0.6416993790387265
Loss in iteration 44 : 0.6372602996838439
Loss in iteration 45 : 0.6329995300864407
Loss in iteration 46 : 0.6289868059908685
Loss in iteration 47 : 0.6252252731371646
Loss in iteration 48 : 0.6217369269524522
Loss in iteration 49 : 0.6185386994198269
Loss in iteration 50 : 0.6155609237698761
Loss in iteration 51 : 0.6128009194999342
Loss in iteration 52 : 0.6101920018074627
Loss in iteration 53 : 0.6077217832884043
Loss in iteration 54 : 0.6053789303881827
Loss in iteration 55 : 0.6031662578066836
Loss in iteration 56 : 0.6010729724942311
Loss in iteration 57 : 0.599083779776241
Loss in iteration 58 : 0.5971765417229332
Loss in iteration 59 : 0.5953620095770777
Loss in iteration 60 : 0.5936261955342129
Loss in iteration 61 : 0.5919719198890167
Loss in iteration 62 : 0.5903930634831044
Loss in iteration 63 : 0.5888844922367732
Loss in iteration 64 : 0.5874410938012202
Loss in iteration 65 : 0.5860540341736626
Loss in iteration 66 : 0.5847246348764427
Loss in iteration 67 : 0.583445847558356
Loss in iteration 68 : 0.5822193040749041
Loss in iteration 69 : 0.5810448783995551
Loss in iteration 70 : 0.5799176897782561
Loss in iteration 71 : 0.5788305758502268
Loss in iteration 72 : 0.5777813793997744
Loss in iteration 73 : 0.5767648213793434
Loss in iteration 74 : 0.5757831178297408
Loss in iteration 75 : 0.5748336076604387
Loss in iteration 76 : 0.5739209669690891
Loss in iteration 77 : 0.5730372000367204
Loss in iteration 78 : 0.5721782753885917
Loss in iteration 79 : 0.5713428591060697
Loss in iteration 80 : 0.5705340429129926
Loss in iteration 81 : 0.5697490060077205
Loss in iteration 82 : 0.5689879845235407
Loss in iteration 83 : 0.5682494747101378
Loss in iteration 84 : 0.5675321720129757
Loss in iteration 85 : 0.5668346098890105
Loss in iteration 86 : 0.5661555269853056
Loss in iteration 87 : 0.56549404147584
Loss in iteration 88 : 0.5648487793873634
Loss in iteration 89 : 0.5642215223248108
Loss in iteration 90 : 0.5636121151701945
Loss in iteration 91 : 0.5630194878062955
Loss in iteration 92 : 0.5624404831320614
Loss in iteration 93 : 0.5618732603601853
Loss in iteration 94 : 0.5613192604158321
Loss in iteration 95 : 0.560778741110782
Loss in iteration 96 : 0.5602496296145933
Loss in iteration 97 : 0.5597339315540437
Loss in iteration 98 : 0.5592304772065227
Loss in iteration 99 : 0.5587368205467235
Loss in iteration 100 : 0.5582540195563723
Loss in iteration 101 : 0.5577795899125053
Loss in iteration 102 : 0.5573134500571133
Loss in iteration 103 : 0.5568581095348573
Loss in iteration 104 : 0.5564136235088789
Loss in iteration 105 : 0.5559801162784154
Loss in iteration 106 : 0.5555535898280515
Loss in iteration 107 : 0.5551337567600019
Loss in iteration 108 : 0.5547237806159548
Loss in iteration 109 : 0.5543232848332968
Loss in iteration 110 : 0.5539301703501568
Loss in iteration 111 : 0.5535418696481196
Loss in iteration 112 : 0.553161429207305
Loss in iteration 113 : 0.5527887859707046
Loss in iteration 114 : 0.55242282012116
Loss in iteration 115 : 0.5520660658553131
Loss in iteration 116 : 0.5517148804598462
Loss in iteration 117 : 0.5513694468006929
Loss in iteration 118 : 0.5510285532582888
Loss in iteration 119 : 0.5506913881724947
Loss in iteration 120 : 0.5503581796605437
Loss in iteration 121 : 0.5500288181020376
Loss in iteration 122 : 0.5497045902415783
Loss in iteration 123 : 0.5493853461581183
Loss in iteration 124 : 0.5490710136568829
Loss in iteration 125 : 0.5487615853814032
Loss in iteration 126 : 0.5484559170602437
Loss in iteration 127 : 0.5481536986414085
Loss in iteration 128 : 0.547854735197538
Loss in iteration 129 : 0.547559218631423
Loss in iteration 130 : 0.5472676524917222
Loss in iteration 131 : 0.5469802871763846
Loss in iteration 132 : 0.5466972327155319
Loss in iteration 133 : 0.5464183108999134
Loss in iteration 134 : 0.5461420740501396
Loss in iteration 135 : 0.545869106400164
Loss in iteration 136 : 0.5455983299061316
Loss in iteration 137 : 0.5453304417304041
Loss in iteration 138 : 0.5450668087162476
Loss in iteration 139 : 0.5448064103147879
Loss in iteration 140 : 0.5445497158713799
Loss in iteration 141 : 0.544297327073094
Loss in iteration 142 : 0.5440474191140277
Loss in iteration 143 : 0.5437993851405258
Loss in iteration 144 : 0.5435544110389672
Loss in iteration 145 : 0.5433119323018339
Loss in iteration 146 : 0.5430720399707175
Loss in iteration 147 : 0.5428336793439096
Loss in iteration 148 : 0.5425972120401096
Loss in iteration 149 : 0.5423642602043406
Loss in iteration 150 : 0.5421337256250667
Loss in iteration 151 : 0.5419050491267878
Loss in iteration 152 : 0.5416790341993367
Loss in iteration 153 : 0.5414559888940378
Loss in iteration 154 : 0.5412365847789115
Loss in iteration 155 : 0.5410209216010455
Loss in iteration 156 : 0.54080771887059
Loss in iteration 157 : 0.5405967154775684
Loss in iteration 158 : 0.5403871333279323
Loss in iteration 159 : 0.5401797720442019
Loss in iteration 160 : 0.5399743353643973
Loss in iteration 161 : 0.5397710866446944
Loss in iteration 162 : 0.5395702590841293
Loss in iteration 163 : 0.5393712901694471
Loss in iteration 164 : 0.5391738410781447
Loss in iteration 165 : 0.5389781701162542
Loss in iteration 166 : 0.538783888177915
Loss in iteration 167 : 0.5385919788868262
Loss in iteration 168 : 0.5384022507701283
Loss in iteration 169 : 0.5382145103158996
Loss in iteration 170 : 0.5380284171562429
Loss in iteration 171 : 0.5378434423124795
Loss in iteration 172 : 0.5376617880756817
Loss in iteration 173 : 0.5374826040390751
Loss in iteration 174 : 0.5373058667482185
Loss in iteration 175 : 0.5371308257589794
Loss in iteration 176 : 0.5369574555443007
Loss in iteration 177 : 0.5367865491762361
Loss in iteration 178 : 0.536617421381824
Loss in iteration 179 : 0.5364489751507678
Loss in iteration 180 : 0.536281635947754
Loss in iteration 181 : 0.536115587567679
Loss in iteration 182 : 0.535950705879483
Loss in iteration 183 : 0.5357876779376859
Loss in iteration 184 : 0.5356261609196427
Loss in iteration 185 : 0.5354664234577312
Loss in iteration 186 : 0.5353086747497421
Loss in iteration 187 : 0.5351526223534816
Loss in iteration 188 : 0.5349983247032631
Loss in iteration 189 : 0.5348451625457242
Loss in iteration 190 : 0.5346939795826564
Loss in iteration 191 : 0.5345440120309595
Loss in iteration 192 : 0.5343950046317232
Loss in iteration 193 : 0.5342476449792739
Loss in iteration 194 : 0.5341015807729299
Loss in iteration 195 : 0.5339564005835502
Loss in iteration 196 : 0.5338123105105683
Loss in iteration 197 : 0.5336695416430538
Loss in iteration 198 : 0.5335281422502806
Loss in iteration 199 : 0.5333880481007025
Loss in iteration 200 : 0.5332490333372586
Testing accuracy  of updater 8 on alg 1 with rate 0.01999999999999999 = 0.7725, training accuracy 0.777875, time elapsed: 2889 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 13.047744702165827
Loss in iteration 3 : 10.216536773725506
Loss in iteration 4 : 6.486844360594467
Loss in iteration 5 : 7.671989240130641
Loss in iteration 6 : 4.3204167355236605
Loss in iteration 7 : 6.255170096458657
Loss in iteration 8 : 6.199987679365051
Loss in iteration 9 : 4.239830259317618
Loss in iteration 10 : 6.324036980703771
Loss in iteration 11 : 3.9984511808048526
Loss in iteration 12 : 3.885736332985842
Loss in iteration 13 : 4.965166516276351
Loss in iteration 14 : 3.522618090273562
Loss in iteration 15 : 4.197415808326391
Loss in iteration 16 : 4.1164348960709045
Loss in iteration 17 : 3.2533089002784545
Loss in iteration 18 : 4.066106106371875
Loss in iteration 19 : 3.7307224031030475
Loss in iteration 20 : 3.0849693553754616
Loss in iteration 21 : 3.3757666768431287
Loss in iteration 22 : 2.8246755029094737
Loss in iteration 23 : 2.9882170593981483
Loss in iteration 24 : 3.257030203975566
Loss in iteration 25 : 2.7825086141365523
Loss in iteration 26 : 2.797003033311104
Loss in iteration 27 : 2.364418591610278
Loss in iteration 28 : 2.208893830858278
Loss in iteration 29 : 2.389167973950283
Loss in iteration 30 : 2.325971815316354
Loss in iteration 31 : 2.22055467525548
Loss in iteration 32 : 1.9057445661228116
Loss in iteration 33 : 1.8604344867977307
Loss in iteration 34 : 1.8593102477276862
Loss in iteration 35 : 1.8781288002645333
Loss in iteration 36 : 1.712800276993076
Loss in iteration 37 : 1.6298570302826154
Loss in iteration 38 : 1.5948465238542426
Loss in iteration 39 : 1.585317051923322
Loss in iteration 40 : 1.5557943180752931
Loss in iteration 41 : 1.438606809126518
Loss in iteration 42 : 1.4392819256459484
Loss in iteration 43 : 1.4645576820493575
Loss in iteration 44 : 1.3684031749970873
Loss in iteration 45 : 1.2792734505394292
Loss in iteration 46 : 1.2688840102727896
Loss in iteration 47 : 1.3238129931114606
Loss in iteration 48 : 1.4364383276836539
Loss in iteration 49 : 1.7562121069424745
Loss in iteration 50 : 2.130847022959369
Loss in iteration 51 : 1.5670263263978645
Loss in iteration 52 : 1.1901650744451553
Loss in iteration 53 : 1.1704749847738263
Loss in iteration 54 : 1.792762166720469
Loss in iteration 55 : 1.6055172781553892
Loss in iteration 56 : 1.1777918737255881
Loss in iteration 57 : 1.0799764840578558
Loss in iteration 58 : 1.5099725138717186
Loss in iteration 59 : 1.5202693960389846
Loss in iteration 60 : 1.1309928454984937
Loss in iteration 61 : 0.9185439435976864
Loss in iteration 62 : 1.192066476184243
Loss in iteration 63 : 1.4698941512311277
Loss in iteration 64 : 1.3309982399795224
Loss in iteration 65 : 0.9732056242103226
Loss in iteration 66 : 1.0135806355046606
Loss in iteration 67 : 1.1826674186340846
Loss in iteration 68 : 1.566012424409449
Loss in iteration 69 : 1.2858803530507148
Loss in iteration 70 : 0.9643549408081958
Loss in iteration 71 : 0.9382117566153001
Loss in iteration 72 : 0.8484046911365287
Loss in iteration 73 : 0.8914149571067653
Loss in iteration 74 : 0.7664403417910671
Loss in iteration 75 : 1.2032621193207669
Loss in iteration 76 : 2.3513822360444663
Loss in iteration 77 : 2.5715657099126004
Loss in iteration 78 : 0.9993505299868861
Loss in iteration 79 : 2.227372276572423
Loss in iteration 80 : 3.0498000902749043
Loss in iteration 81 : 2.1248609562834773
Loss in iteration 82 : 3.1097963302796616
Loss in iteration 83 : 1.3259888920868215
Loss in iteration 84 : 2.8663181722249926
Loss in iteration 85 : 1.2361776177221655
Loss in iteration 86 : 2.6989742468504856
Loss in iteration 87 : 1.6271777022947937
Loss in iteration 88 : 2.5398856843711783
Loss in iteration 89 : 1.509780164903629
Loss in iteration 90 : 2.0204630769032845
Loss in iteration 91 : 2.00191473735274
Loss in iteration 92 : 2.1144653651822805
Loss in iteration 93 : 1.8909001932564533
Loss in iteration 94 : 1.1913864715268452
Loss in iteration 95 : 1.7132183906765186
Loss in iteration 96 : 1.81389473217584
Loss in iteration 97 : 1.5941511508348813
Loss in iteration 98 : 1.1082747148088015
Loss in iteration 99 : 1.2726703128467443
Loss in iteration 100 : 1.5650665361306089
Loss in iteration 101 : 1.4073840914561202
Loss in iteration 102 : 1.0939783677360102
Loss in iteration 103 : 1.0724306642889678
Loss in iteration 104 : 1.2810235994589212
Loss in iteration 105 : 1.2140477659973985
Loss in iteration 106 : 0.9916260470657662
Loss in iteration 107 : 1.0279904434918912
Loss in iteration 108 : 1.0110462120733048
Loss in iteration 109 : 1.101038199657839
Loss in iteration 110 : 0.8825045603645226
Loss in iteration 111 : 0.8603176336789929
Loss in iteration 112 : 1.0558008857383383
Loss in iteration 113 : 0.9245549458257312
Loss in iteration 114 : 0.7750443913650585
Loss in iteration 115 : 0.8605281930620456
Loss in iteration 116 : 0.9449280802903879
Loss in iteration 117 : 1.0009605086456395
Loss in iteration 118 : 1.2988790577753904
Loss in iteration 119 : 1.176308470911717
Loss in iteration 120 : 0.84906110318741
Loss in iteration 121 : 1.0412400383253624
Loss in iteration 122 : 0.9672049557672153
Loss in iteration 123 : 1.4949369109957555
Loss in iteration 124 : 0.9175514766636765
Loss in iteration 125 : 0.9176348559758444
Loss in iteration 126 : 0.7003902379480837
Loss in iteration 127 : 0.769715018794104
Loss in iteration 128 : 0.700987958113466
Loss in iteration 129 : 0.7382218623011391
Loss in iteration 130 : 0.6932431832282503
Loss in iteration 131 : 0.7629789672461353
Loss in iteration 132 : 1.0606775262244352
Loss in iteration 133 : 1.6889793751804196
Loss in iteration 134 : 1.139248801420346
Loss in iteration 135 : 0.7161143441664278
Loss in iteration 136 : 0.6852198511057014
Loss in iteration 137 : 0.8268948572441379
Loss in iteration 138 : 1.0301453443224555
Loss in iteration 139 : 1.0614197534668475
Loss in iteration 140 : 0.9839767796963076
Loss in iteration 141 : 0.8346994417090725
Loss in iteration 142 : 0.7587202370432538
Loss in iteration 143 : 0.7493005417769443
Loss in iteration 144 : 0.8355872918493067
Loss in iteration 145 : 0.8053262665837956
Loss in iteration 146 : 0.9909158176029849
Loss in iteration 147 : 0.9639389041720711
Loss in iteration 148 : 0.8874307188893583
Loss in iteration 149 : 0.932992009124941
Loss in iteration 150 : 0.685423648784685
Loss in iteration 151 : 0.8505861289873755
Loss in iteration 152 : 0.644933367663198
Loss in iteration 153 : 0.9917465114606707
Loss in iteration 154 : 1.1686797904122317
Loss in iteration 155 : 1.7389217940760893
Loss in iteration 156 : 0.8640526446251079
Loss in iteration 157 : 0.686948680037783
Loss in iteration 158 : 1.4338688424321513
Loss in iteration 159 : 1.2360544725780522
Loss in iteration 160 : 0.7536619321321741
Loss in iteration 161 : 0.6738415854468139
Loss in iteration 162 : 0.9865291721060291
Loss in iteration 163 : 1.1018414801772665
Loss in iteration 164 : 0.7586452543174025
Loss in iteration 165 : 0.7400910085363066
Loss in iteration 166 : 0.6609147399221079
Loss in iteration 167 : 0.7836095073918307
Loss in iteration 168 : 0.7691151077542042
Loss in iteration 169 : 0.7818604220862451
Loss in iteration 170 : 0.8351333889006288
Loss in iteration 171 : 0.7172487316455082
Loss in iteration 172 : 0.9665911086674125
Loss in iteration 173 : 0.8040184331297299
Loss in iteration 174 : 1.0822070866173228
Loss in iteration 175 : 0.8948641983621701
Loss in iteration 176 : 0.7923780350368386
Loss in iteration 177 : 0.7444011006213195
Loss in iteration 178 : 0.7613415629361284
Loss in iteration 179 : 0.8924707985939991
Loss in iteration 180 : 1.053074670225178
Loss in iteration 181 : 0.9955098613241478
Loss in iteration 182 : 1.0177176769686045
Loss in iteration 183 : 0.7227770241358499
Loss in iteration 184 : 0.7778458777470432
Loss in iteration 185 : 0.681445880618169
Loss in iteration 186 : 0.7981644656026767
Loss in iteration 187 : 0.7721794049530801
Loss in iteration 188 : 0.9125177698658834
Loss in iteration 189 : 0.8376733974513272
Loss in iteration 190 : 0.8659625406556336
Loss in iteration 191 : 0.674625391963777
Loss in iteration 192 : 0.7491623924131402
Loss in iteration 193 : 0.6393788027739673
Loss in iteration 194 : 0.7791996292211264
Loss in iteration 195 : 0.7749563787116011
Loss in iteration 196 : 1.106118794844254
Loss in iteration 197 : 0.9410899141039344
Loss in iteration 198 : 0.8194259408857588
Loss in iteration 199 : 0.6975562280833472
Loss in iteration 200 : 0.7029659205213218
Testing accuracy  of updater 9 on alg 1 with rate 2.0 = 0.766, training accuracy 0.756, time elapsed: 2766 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8758173327161641
Loss in iteration 3 : 0.7688598222147071
Loss in iteration 4 : 2.041754164190926
Loss in iteration 5 : 0.7951625345362865
Loss in iteration 6 : 1.6459212715571052
Loss in iteration 7 : 0.8719226387124702
Loss in iteration 8 : 0.8625439269061085
Loss in iteration 9 : 1.1035050198070457
Loss in iteration 10 : 0.9248205536539779
Loss in iteration 11 : 1.0163720241621261
Loss in iteration 12 : 0.8403942989025075
Loss in iteration 13 : 0.9458476628785866
Loss in iteration 14 : 0.9592243002341022
Loss in iteration 15 : 0.8787543447528908
Loss in iteration 16 : 0.9667083551712632
Loss in iteration 17 : 0.8235924885261177
Loss in iteration 18 : 0.9230103997396817
Loss in iteration 19 : 0.8002860432476417
Loss in iteration 20 : 0.8811193078503162
Loss in iteration 21 : 0.8141848669948035
Loss in iteration 22 : 0.8302748957757
Loss in iteration 23 : 0.7865561904443584
Loss in iteration 24 : 0.7638262061347626
Loss in iteration 25 : 0.7576013441259443
Loss in iteration 26 : 0.7314998312378841
Loss in iteration 27 : 0.7323789648272384
Loss in iteration 28 : 0.7050946091147104
Loss in iteration 29 : 0.6703622713570864
Loss in iteration 30 : 0.6847314168467294
Loss in iteration 31 : 0.6370690470116473
Loss in iteration 32 : 0.6663768924015739
Loss in iteration 33 : 0.6381447971999494
Loss in iteration 34 : 0.607052634991417
Loss in iteration 35 : 0.6441267159194022
Loss in iteration 36 : 0.6403313579269575
Loss in iteration 37 : 0.6003173201486959
Loss in iteration 38 : 0.5857721774494599
Loss in iteration 39 : 0.6007302566770718
Loss in iteration 40 : 0.6247337128754478
Loss in iteration 41 : 0.6544264596930393
Loss in iteration 42 : 0.6598899402744528
Loss in iteration 43 : 0.6838906384833104
Loss in iteration 44 : 0.7218183168696483
Loss in iteration 45 : 0.6998471313705023
Loss in iteration 46 : 0.6715129739651879
Loss in iteration 47 : 0.6109809464689858
Loss in iteration 48 : 0.593352249583392
Loss in iteration 49 : 0.5644244060314801
Loss in iteration 50 : 0.5614789160136677
Loss in iteration 51 : 0.5474251485254515
Loss in iteration 52 : 0.5644771476391882
Loss in iteration 53 : 0.5856932333062534
Loss in iteration 54 : 0.6451254611403001
Loss in iteration 55 : 0.6858905926352936
Loss in iteration 56 : 0.7144377963406201
Loss in iteration 57 : 0.6339143611950724
Loss in iteration 58 : 0.5988023943142005
Loss in iteration 59 : 0.5459987454682664
Loss in iteration 60 : 0.5459739645559355
Loss in iteration 61 : 0.5313635415141729
Loss in iteration 62 : 0.5356239942153457
Loss in iteration 63 : 0.5339333114534417
Loss in iteration 64 : 0.5697404824814879
Loss in iteration 65 : 0.7174590112544795
Loss in iteration 66 : 1.011271427826985
Loss in iteration 67 : 0.784001976535862
Loss in iteration 68 : 0.6093344709212993
Loss in iteration 69 : 0.565018246281195
Loss in iteration 70 : 0.5548748944172035
Loss in iteration 71 : 0.5698670446035042
Loss in iteration 72 : 0.600611152260476
Loss in iteration 73 : 0.6700147676877116
Loss in iteration 74 : 0.7071960034830149
Loss in iteration 75 : 0.7007779310025323
Loss in iteration 76 : 0.6122712339900803
Loss in iteration 77 : 0.5995731783897185
Loss in iteration 78 : 0.5504541633865185
Loss in iteration 79 : 0.5658482509419618
Loss in iteration 80 : 0.5521918051686848
Loss in iteration 81 : 0.5812604943145031
Loss in iteration 82 : 0.5900880638376981
Loss in iteration 83 : 0.6476073156835455
Loss in iteration 84 : 0.6462068823718771
Loss in iteration 85 : 0.697231290233357
Loss in iteration 86 : 0.5947666928189447
Loss in iteration 87 : 0.6092061560824997
Loss in iteration 88 : 0.5484083136397978
Loss in iteration 89 : 0.5871910584691997
Loss in iteration 90 : 0.5529971661373774
Loss in iteration 91 : 0.6095619559116551
Loss in iteration 92 : 0.6120260171851741
Loss in iteration 93 : 0.7151042807774448
Loss in iteration 94 : 0.6647660109802674
Loss in iteration 95 : 0.676996581430255
Loss in iteration 96 : 0.5901033593749427
Loss in iteration 97 : 0.5856098898265177
Loss in iteration 98 : 0.5426918094782006
Loss in iteration 99 : 0.5549273417718055
Loss in iteration 100 : 0.5400076403796095
Loss in iteration 101 : 0.5687567398787937
Loss in iteration 102 : 0.5837726974060836
Loss in iteration 103 : 0.6550832727001675
Loss in iteration 104 : 0.6851051031613017
Loss in iteration 105 : 0.6805384363975652
Loss in iteration 106 : 0.6106338331493678
Loss in iteration 107 : 0.5754283285911581
Loss in iteration 108 : 0.554882680128617
Loss in iteration 109 : 0.5481238157164747
Loss in iteration 110 : 0.543964992100474
Loss in iteration 111 : 0.5388523938680294
Loss in iteration 112 : 0.5576588345795735
Loss in iteration 113 : 0.5745741662561159
Loss in iteration 114 : 0.6440053954945959
Loss in iteration 115 : 0.716032735243191
Loss in iteration 116 : 0.7052358954241422
Loss in iteration 117 : 0.6441000462308306
Loss in iteration 118 : 0.562361858474478
Loss in iteration 119 : 0.5417330876307301
Loss in iteration 120 : 0.5585643722127522
Loss in iteration 121 : 0.5718780609640985
Loss in iteration 122 : 0.6383306295476766
Loss in iteration 123 : 0.6503089694757993
Loss in iteration 124 : 0.6442714979686984
Loss in iteration 125 : 0.5954687001219561
Loss in iteration 126 : 0.5680489033801431
Loss in iteration 127 : 0.5443085045511148
Loss in iteration 128 : 0.5420450145885299
Loss in iteration 129 : 0.5455887005357342
Loss in iteration 130 : 0.5685346121972147
Loss in iteration 131 : 0.6021854888322352
Loss in iteration 132 : 0.6593607102032334
Loss in iteration 133 : 0.662137588986291
Loss in iteration 134 : 0.6414030489634395
Loss in iteration 135 : 0.5763449434645154
Loss in iteration 136 : 0.5480786423870964
Loss in iteration 137 : 0.5274965420746878
Loss in iteration 138 : 0.5267753742775196
Loss in iteration 139 : 0.5238933809619383
Loss in iteration 140 : 0.5314262089258893
Loss in iteration 141 : 0.5680403372918659
Loss in iteration 142 : 0.6601059385794849
Loss in iteration 143 : 0.7405768142132823
Loss in iteration 144 : 0.7224430143724014
Loss in iteration 145 : 0.5928425709324828
Loss in iteration 146 : 0.5555342321717869
Loss in iteration 147 : 0.5232375320168638
Loss in iteration 148 : 0.5327279572673896
Loss in iteration 149 : 0.5395096834699648
Loss in iteration 150 : 0.5691189598005854
Loss in iteration 151 : 0.6148163906827496
Loss in iteration 152 : 0.6490874780413228
Loss in iteration 153 : 0.6357162894135588
Loss in iteration 154 : 0.5993324446549972
Loss in iteration 155 : 0.5740829519192339
Loss in iteration 156 : 0.5564640970275057
Loss in iteration 157 : 0.546543130994163
Loss in iteration 158 : 0.5382346042545537
Loss in iteration 159 : 0.5318698744658023
Loss in iteration 160 : 0.5345618981169901
Loss in iteration 161 : 0.5300530999912539
Loss in iteration 162 : 0.5508219317862922
Loss in iteration 163 : 0.5618219836725725
Loss in iteration 164 : 0.6345204497848475
Loss in iteration 165 : 0.7203245110642973
Loss in iteration 166 : 0.6928850181725598
Loss in iteration 167 : 0.6608514885204013
Loss in iteration 168 : 0.5441415239083699
Loss in iteration 169 : 0.5473580168644893
Loss in iteration 170 : 0.5248905434448364
Loss in iteration 171 : 0.5629539574723909
Loss in iteration 172 : 0.5696756200809477
Loss in iteration 173 : 0.615869725339132
Loss in iteration 174 : 0.607794982195831
Loss in iteration 175 : 0.596555362785258
Loss in iteration 176 : 0.5847073755415284
Loss in iteration 177 : 0.5561122803708788
Loss in iteration 178 : 0.5511952492178819
Loss in iteration 179 : 0.5364868350711449
Loss in iteration 180 : 0.534670647704719
Loss in iteration 181 : 0.5288407050924513
Loss in iteration 182 : 0.5255399649320489
Loss in iteration 183 : 0.5303106939376167
Loss in iteration 184 : 0.5234436286236719
Loss in iteration 185 : 0.5529317613331696
Loss in iteration 186 : 0.587846604455923
Loss in iteration 187 : 0.7222466079681722
Loss in iteration 188 : 0.7591447945005677
Loss in iteration 189 : 0.6617123479316979
Loss in iteration 190 : 0.5695068141911602
Loss in iteration 191 : 0.5245286325879226
Loss in iteration 192 : 0.5218247724570118
Loss in iteration 193 : 0.5399784227147403
Loss in iteration 194 : 0.5950209122844679
Loss in iteration 195 : 0.6523059600401065
Loss in iteration 196 : 0.6480616129757718
Loss in iteration 197 : 0.5856868691766517
Loss in iteration 198 : 0.540664748951785
Loss in iteration 199 : 0.521975195594063
Loss in iteration 200 : 0.516012228036597
Testing accuracy  of updater 9 on alg 1 with rate 1.4000000000000001 = 0.7835, training accuracy 0.78825, time elapsed: 3033 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9635603861586374
Loss in iteration 3 : 0.8951807545130457
Loss in iteration 4 : 0.6994659032473612
Loss in iteration 5 : 0.6322976536480139
Loss in iteration 6 : 0.644004351456538
Loss in iteration 7 : 0.611937782102041
Loss in iteration 8 : 0.555385718588941
Loss in iteration 9 : 0.5990438629932553
Loss in iteration 10 : 0.5660754502690499
Loss in iteration 11 : 0.5961725857240117
Loss in iteration 12 : 0.5717511265187996
Loss in iteration 13 : 0.591442891710627
Loss in iteration 14 : 0.5743119802385143
Loss in iteration 15 : 0.5918329454784655
Loss in iteration 16 : 0.5783932703142408
Loss in iteration 17 : 0.5873894135841045
Loss in iteration 18 : 0.5766158776067095
Loss in iteration 19 : 0.5759627327685148
Loss in iteration 20 : 0.5706474485538867
Loss in iteration 21 : 0.5661734827980386
Loss in iteration 22 : 0.5631550696308709
Loss in iteration 23 : 0.5550650070814588
Loss in iteration 24 : 0.5523957148664013
Loss in iteration 25 : 0.5437644042620347
Loss in iteration 26 : 0.5428276517812755
Loss in iteration 27 : 0.5366649390344992
Loss in iteration 28 : 0.5350034361101883
Loss in iteration 29 : 0.5311086574026251
Loss in iteration 30 : 0.526919581592987
Loss in iteration 31 : 0.5282282432538736
Loss in iteration 32 : 0.5229224355196653
Loss in iteration 33 : 0.5249984121090235
Loss in iteration 34 : 0.522670460849664
Loss in iteration 35 : 0.5212185380162785
Loss in iteration 36 : 0.5228884362560493
Loss in iteration 37 : 0.5205358613556593
Loss in iteration 38 : 0.5206335194857633
Loss in iteration 39 : 0.5205657979087244
Loss in iteration 40 : 0.518809398458452
Loss in iteration 41 : 0.5194749195136533
Loss in iteration 42 : 0.5190979217804477
Loss in iteration 43 : 0.5170817971661524
Loss in iteration 44 : 0.5169556056333253
Loss in iteration 45 : 0.5162675346990739
Loss in iteration 46 : 0.513978947971077
Loss in iteration 47 : 0.5140317917262637
Loss in iteration 48 : 0.5130451340965915
Loss in iteration 49 : 0.5111402058142376
Loss in iteration 50 : 0.5114011795116401
Loss in iteration 51 : 0.5108566480034633
Loss in iteration 52 : 0.5094631876946104
Loss in iteration 53 : 0.5099396784841411
Loss in iteration 54 : 0.5095540578459882
Loss in iteration 55 : 0.5087587649423575
Loss in iteration 56 : 0.5096926004106089
Loss in iteration 57 : 0.5093942021305972
Loss in iteration 58 : 0.5085119851633453
Loss in iteration 59 : 0.5095294733497626
Loss in iteration 60 : 0.5093615779667152
Loss in iteration 61 : 0.5084059365348945
Loss in iteration 62 : 0.5095408016010675
Loss in iteration 63 : 0.5096185432621793
Loss in iteration 64 : 0.507937106969765
Loss in iteration 65 : 0.509536541116948
Loss in iteration 66 : 0.5105160669682892
Loss in iteration 67 : 0.5078547024410363
Loss in iteration 68 : 0.5088164682684334
Loss in iteration 69 : 0.5102994008851651
Loss in iteration 70 : 0.5077330472993058
Loss in iteration 71 : 0.5081013407611662
Loss in iteration 72 : 0.5095380040187661
Loss in iteration 73 : 0.5077658259778023
Loss in iteration 74 : 0.5076462387317502
Loss in iteration 75 : 0.5086987198416465
Loss in iteration 76 : 0.5077243212499134
Loss in iteration 77 : 0.5073602109022521
Loss in iteration 78 : 0.508082323051496
Loss in iteration 79 : 0.5075370450592763
Loss in iteration 80 : 0.5072085005935042
Loss in iteration 81 : 0.5075139306457502
Loss in iteration 82 : 0.5074351176586571
Loss in iteration 83 : 0.5071220019644451
Loss in iteration 84 : 0.5073407001294842
Loss in iteration 85 : 0.5072982994060942
Loss in iteration 86 : 0.5070233797854561
Loss in iteration 87 : 0.5070904958020357
Loss in iteration 88 : 0.5070461685672156
Loss in iteration 89 : 0.5070136089932074
Loss in iteration 90 : 0.5069072343510295
Loss in iteration 91 : 0.5069572060680002
Loss in iteration 92 : 0.5069435284202854
Loss in iteration 93 : 0.5068425267140363
Loss in iteration 94 : 0.5069457797568692
Loss in iteration 95 : 0.5070043022788018
Loss in iteration 96 : 0.5068195009405989
Loss in iteration 97 : 0.5067969973433543
Loss in iteration 98 : 0.5068487307497157
Loss in iteration 99 : 0.5067907056358482
Loss in iteration 100 : 0.5067546275291045
Loss in iteration 101 : 0.506786810060893
Loss in iteration 102 : 0.50677087786787
Loss in iteration 103 : 0.5067273650113875
Loss in iteration 104 : 0.5067085550318051
Loss in iteration 105 : 0.5067383872581762
Loss in iteration 106 : 0.5066666271543416
Loss in iteration 107 : 0.5066761127648964
Loss in iteration 108 : 0.5066497500500065
Loss in iteration 109 : 0.506646823623068
Loss in iteration 110 : 0.5066305473077588
Loss in iteration 111 : 0.5066153603532806
Loss in iteration 112 : 0.5066196571005416
Loss in iteration 113 : 0.5065950228222754
Loss in iteration 114 : 0.5066063236883054
Loss in iteration 115 : 0.5065946721948734
Loss in iteration 116 : 0.5065945909787255
Loss in iteration 117 : 0.5065729179038914
Loss in iteration 118 : 0.5065790809891684
Loss in iteration 119 : 0.506569594888265
Loss in iteration 120 : 0.5065491186813731
Loss in iteration 121 : 0.506551768711618
Loss in iteration 122 : 0.5065683188151533
Loss in iteration 123 : 0.5065485680447636
Loss in iteration 124 : 0.5065167616500986
Loss in iteration 125 : 0.5065875485650018
Loss in iteration 126 : 0.5065922135163387
Loss in iteration 127 : 0.5065362242190582
Loss in iteration 128 : 0.5065054457125459
Loss in iteration 129 : 0.506526807099017
Loss in iteration 130 : 0.5065372427700335
Loss in iteration 131 : 0.5064759240827038
Loss in iteration 132 : 0.5065372692376371
Loss in iteration 133 : 0.5065935551871793
Loss in iteration 134 : 0.5065447767947595
Loss in iteration 135 : 0.5064784882653958
Loss in iteration 136 : 0.5066325737441724
Loss in iteration 137 : 0.5065879488444027
Loss in iteration 138 : 0.5064681685980197
Loss in iteration 139 : 0.506489676416953
Loss in iteration 140 : 0.5065329411174664
Loss in iteration 141 : 0.506457469519223
Loss in iteration 142 : 0.5064489035192478
Loss in iteration 143 : 0.5064980764482309
Loss in iteration 144 : 0.5065326974219797
Loss in iteration 145 : 0.5064576359162508
Loss in iteration 146 : 0.5064367278519338
Loss in iteration 147 : 0.5064730774581416
Loss in iteration 148 : 0.5064183840559451
Loss in iteration 149 : 0.5064113773040279
Loss in iteration 150 : 0.5064159285329994
Loss in iteration 151 : 0.506422150203169
Loss in iteration 152 : 0.5063993012606776
Loss in iteration 153 : 0.5063979808513717
Loss in iteration 154 : 0.5063892714726635
Loss in iteration 155 : 0.5063985769234638
Loss in iteration 156 : 0.506378871301212
Loss in iteration 157 : 0.5063881913529551
Loss in iteration 158 : 0.5063876761176025
Loss in iteration 159 : 0.5064085627788413
Loss in iteration 160 : 0.5064175943909515
Loss in iteration 161 : 0.5063803340520201
Loss in iteration 162 : 0.506408604214173
Loss in iteration 163 : 0.5065217143561722
Loss in iteration 164 : 0.5065274551110076
Loss in iteration 165 : 0.5064096246179675
Loss in iteration 166 : 0.5063980212616731
Loss in iteration 167 : 0.5065157081268379
Loss in iteration 168 : 0.5065360897703627
Loss in iteration 169 : 0.5063764042990129
Loss in iteration 170 : 0.506522440578466
Loss in iteration 171 : 0.5067191448950081
Loss in iteration 172 : 0.5065686956859452
Loss in iteration 173 : 0.5063644971829209
Loss in iteration 174 : 0.506512793144637
Loss in iteration 175 : 0.5065745291434786
Loss in iteration 176 : 0.5064618633331319
Loss in iteration 177 : 0.5063747885091339
Loss in iteration 178 : 0.5065601797059259
Loss in iteration 179 : 0.5065437014873444
Loss in iteration 180 : 0.5064549121029808
Loss in iteration 181 : 0.5063768824445071
Loss in iteration 182 : 0.5066778315972079
Loss in iteration 183 : 0.506735487738519
Loss in iteration 184 : 0.5065272848734967
Loss in iteration 185 : 0.5063862538536985
Loss in iteration 186 : 0.5067791869806402
Loss in iteration 187 : 0.506748518850685
Loss in iteration 188 : 0.5064614007125958
Loss in iteration 189 : 0.5066409612965439
Loss in iteration 190 : 0.5072470230287155
Loss in iteration 191 : 0.5066354249469974
Loss in iteration 192 : 0.5064565409720654
Loss in iteration 193 : 0.5066902137999304
Loss in iteration 194 : 0.5067232851547921
Loss in iteration 195 : 0.5065035517863706
Loss in iteration 196 : 0.5063664449902308
Loss in iteration 197 : 0.5067176566185432
Loss in iteration 198 : 0.5069599711692072
Loss in iteration 199 : 0.506648463030744
Loss in iteration 200 : 0.5063655780028267
Testing accuracy  of updater 9 on alg 1 with rate 0.8 = 0.7835, training accuracy 0.788625, time elapsed: 2921 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9442494910532079
Loss in iteration 3 : 0.9134275066509047
Loss in iteration 4 : 0.9008240503148506
Loss in iteration 5 : 0.8292293528711799
Loss in iteration 6 : 0.7738794444380415
Loss in iteration 7 : 0.7523050516805663
Loss in iteration 8 : 0.6865955626039879
Loss in iteration 9 : 0.631543406794437
Loss in iteration 10 : 0.6293012532101875
Loss in iteration 11 : 0.6016424680082615
Loss in iteration 12 : 0.577126733567037
Loss in iteration 13 : 0.5786851668473373
Loss in iteration 14 : 0.5643457184726777
Loss in iteration 15 : 0.5548141586273595
Loss in iteration 16 : 0.5598666467882377
Loss in iteration 17 : 0.5560800912126095
Loss in iteration 18 : 0.5473207194456167
Loss in iteration 19 : 0.5496157977970593
Loss in iteration 20 : 0.5524628765722256
Loss in iteration 21 : 0.5466545606002485
Loss in iteration 22 : 0.5431885943581003
Loss in iteration 23 : 0.5451256508990818
Loss in iteration 24 : 0.5444172207280442
Loss in iteration 25 : 0.5404587673583648
Loss in iteration 26 : 0.5399865745174934
Loss in iteration 27 : 0.5414022896819943
Loss in iteration 28 : 0.5392754255975803
Loss in iteration 29 : 0.5367241888381362
Loss in iteration 30 : 0.5366990459669049
Loss in iteration 31 : 0.5359150217028255
Loss in iteration 32 : 0.5329005784668143
Loss in iteration 33 : 0.5314458809004234
Loss in iteration 34 : 0.5312167434585711
Loss in iteration 35 : 0.5291021592885925
Loss in iteration 36 : 0.527172442333111
Loss in iteration 37 : 0.5268951783814767
Loss in iteration 38 : 0.5256431684570955
Loss in iteration 39 : 0.5233963667420471
Loss in iteration 40 : 0.5224968876871408
Loss in iteration 41 : 0.5218848082231475
Loss in iteration 42 : 0.5203377491042991
Loss in iteration 43 : 0.5191951511116648
Loss in iteration 44 : 0.5187430822241187
Loss in iteration 45 : 0.5178139353977395
Loss in iteration 46 : 0.5168096111698872
Loss in iteration 47 : 0.5163359855929077
Loss in iteration 48 : 0.5155998524363381
Loss in iteration 49 : 0.5147447826258549
Loss in iteration 50 : 0.5144268851195629
Loss in iteration 51 : 0.5141006882251542
Loss in iteration 52 : 0.5134708412850751
Loss in iteration 53 : 0.5131909685375177
Loss in iteration 54 : 0.5130618947872769
Loss in iteration 55 : 0.5126640116191192
Loss in iteration 56 : 0.512427077231504
Loss in iteration 57 : 0.5123411474750922
Loss in iteration 58 : 0.512092565499881
Loss in iteration 59 : 0.5119180273241681
Loss in iteration 60 : 0.5118456741693633
Loss in iteration 61 : 0.5116754997185115
Loss in iteration 62 : 0.5115522815557079
Loss in iteration 63 : 0.511512742722426
Loss in iteration 64 : 0.5114114194649526
Loss in iteration 65 : 0.5112717315408057
Loss in iteration 66 : 0.5111986706268224
Loss in iteration 67 : 0.5111280555108517
Loss in iteration 68 : 0.5110122162210432
Loss in iteration 69 : 0.5109540801666055
Loss in iteration 70 : 0.5108914453608928
Loss in iteration 71 : 0.5108000816900521
Loss in iteration 72 : 0.5107567915504686
Loss in iteration 73 : 0.5106843756660099
Loss in iteration 74 : 0.510597935242383
Loss in iteration 75 : 0.5105523255465396
Loss in iteration 76 : 0.5104801828798288
Loss in iteration 77 : 0.5104011734290648
Loss in iteration 78 : 0.5103539444911015
Loss in iteration 79 : 0.510296391334412
Loss in iteration 80 : 0.5102356576680721
Loss in iteration 81 : 0.5101859289380155
Loss in iteration 82 : 0.5101235108890929
Loss in iteration 83 : 0.5100715470518619
Loss in iteration 84 : 0.5100194805562522
Loss in iteration 85 : 0.5099644693567997
Loss in iteration 86 : 0.5099172247190521
Loss in iteration 87 : 0.509877435679272
Loss in iteration 88 : 0.5098292363532849
Loss in iteration 89 : 0.5097891053749816
Loss in iteration 90 : 0.5097482585857315
Loss in iteration 91 : 0.5097056037928791
Loss in iteration 92 : 0.5096664657467842
Loss in iteration 93 : 0.5096291772625268
Loss in iteration 94 : 0.5095962868989481
Loss in iteration 95 : 0.5095585874896355
Loss in iteration 96 : 0.509522820569042
Loss in iteration 97 : 0.5094926201884159
Loss in iteration 98 : 0.5094585943050007
Loss in iteration 99 : 0.5094255518025874
Loss in iteration 100 : 0.5093962911523913
Loss in iteration 101 : 0.509368913194187
Loss in iteration 102 : 0.509339867786238
Loss in iteration 103 : 0.5093119647044013
Loss in iteration 104 : 0.5092848270494836
Loss in iteration 105 : 0.509258219970693
Loss in iteration 106 : 0.5092318278301967
Loss in iteration 107 : 0.5092060363072894
Loss in iteration 108 : 0.509180553280316
Loss in iteration 109 : 0.509154818549819
Loss in iteration 110 : 0.5091293348749533
Loss in iteration 111 : 0.5091054828668489
Loss in iteration 112 : 0.5090829683192
Loss in iteration 113 : 0.5090592256191022
Loss in iteration 114 : 0.5090361779127636
Loss in iteration 115 : 0.5090163845407822
Loss in iteration 116 : 0.5089934167323776
Loss in iteration 117 : 0.5089719038167603
Loss in iteration 118 : 0.5089518599858227
Loss in iteration 119 : 0.5089312209261395
Loss in iteration 120 : 0.5089110145728502
Loss in iteration 121 : 0.5088899255590694
Loss in iteration 122 : 0.5088700851934267
Loss in iteration 123 : 0.5088498994614559
Loss in iteration 124 : 0.5088302851366665
Loss in iteration 125 : 0.5088113474657944
Loss in iteration 126 : 0.5087924798084469
Loss in iteration 127 : 0.5087739503285055
Loss in iteration 128 : 0.508754090959502
Loss in iteration 129 : 0.508737306337371
Loss in iteration 130 : 0.5087177942962742
Loss in iteration 131 : 0.5086997000975958
Loss in iteration 132 : 0.5086824946256758
Loss in iteration 133 : 0.5086643000888622
Loss in iteration 134 : 0.5086469825045776
Loss in iteration 135 : 0.5086289860674174
Loss in iteration 136 : 0.508611821871431
Loss in iteration 137 : 0.5085957816540858
Loss in iteration 138 : 0.5085776703252186
Loss in iteration 139 : 0.5085607746673891
Loss in iteration 140 : 0.508544819091219
Loss in iteration 141 : 0.508528415857291
Loss in iteration 142 : 0.5085116247187235
Loss in iteration 143 : 0.5084960104401651
Loss in iteration 144 : 0.5084803859604764
Loss in iteration 145 : 0.5084638079065424
Loss in iteration 146 : 0.5084492628218303
Loss in iteration 147 : 0.5084335196695641
Loss in iteration 148 : 0.5084191531086946
Loss in iteration 149 : 0.5084036660716421
Loss in iteration 150 : 0.508389195571167
Loss in iteration 151 : 0.5083743405080686
Loss in iteration 152 : 0.5083600858584036
Loss in iteration 153 : 0.5083461561125684
Loss in iteration 154 : 0.5083322107959567
Loss in iteration 155 : 0.5083181582425149
Loss in iteration 156 : 0.5083044507398568
Loss in iteration 157 : 0.5082901287196802
Loss in iteration 158 : 0.5082759273914178
Loss in iteration 159 : 0.5082625383997718
Loss in iteration 160 : 0.5082482791470808
Loss in iteration 161 : 0.5082352958687018
Loss in iteration 162 : 0.5082211813589488
Loss in iteration 163 : 0.5082082225649601
Loss in iteration 164 : 0.508194109129346
Loss in iteration 165 : 0.5081808533144554
Loss in iteration 166 : 0.5081669635192076
Loss in iteration 167 : 0.5081538063803648
Loss in iteration 168 : 0.5081404392117697
Loss in iteration 169 : 0.5081273656094487
Loss in iteration 170 : 0.5081147633144748
Loss in iteration 171 : 0.5081011243064241
Loss in iteration 172 : 0.5080880800754729
Loss in iteration 173 : 0.5080750278936317
Loss in iteration 174 : 0.5080623218520094
Loss in iteration 175 : 0.5080495002316956
Loss in iteration 176 : 0.5080369392747248
Loss in iteration 177 : 0.5080250597413812
Loss in iteration 178 : 0.5080123539195663
Loss in iteration 179 : 0.5080001189292356
Loss in iteration 180 : 0.5079879511169524
Loss in iteration 181 : 0.5079759468023722
Loss in iteration 182 : 0.507964068236045
Loss in iteration 183 : 0.5079522871963634
Loss in iteration 184 : 0.5079403460845621
Loss in iteration 185 : 0.5079287931235188
Loss in iteration 186 : 0.507917090808067
Loss in iteration 187 : 0.5079053980638047
Loss in iteration 188 : 0.5078939884275973
Loss in iteration 189 : 0.507882249000515
Loss in iteration 190 : 0.5078708022664011
Loss in iteration 191 : 0.5078596274360574
Loss in iteration 192 : 0.5078477679239164
Loss in iteration 193 : 0.5078364635993983
Loss in iteration 194 : 0.5078253032093502
Loss in iteration 195 : 0.5078138710845687
Loss in iteration 196 : 0.5078021984357143
Loss in iteration 197 : 0.5077910527894718
Loss in iteration 198 : 0.5077793510016202
Loss in iteration 199 : 0.5077686650668664
Loss in iteration 200 : 0.5077568965443625
Testing accuracy  of updater 9 on alg 1 with rate 0.2 = 0.7835, training accuracy 0.787875, time elapsed: 2853 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9560756138662316
Loss in iteration 3 : 0.9113422519374688
Loss in iteration 4 : 0.9173952823735828
Loss in iteration 5 : 0.8706304759710107
Loss in iteration 6 : 0.8135380353544659
Loss in iteration 7 : 0.770784672258881
Loss in iteration 8 : 0.7502210934984722
Loss in iteration 9 : 0.7031428605312972
Loss in iteration 10 : 0.6432196715028042
Loss in iteration 11 : 0.6292461527271012
Loss in iteration 12 : 0.6180254118093539
Loss in iteration 13 : 0.591481212583591
Loss in iteration 14 : 0.5766719488963529
Loss in iteration 15 : 0.5757093454566257
Loss in iteration 16 : 0.5652121666323069
Loss in iteration 17 : 0.556143048310478
Loss in iteration 18 : 0.5568758101541773
Loss in iteration 19 : 0.5564737236965038
Loss in iteration 20 : 0.5497008615286015
Loss in iteration 21 : 0.5462670167552546
Loss in iteration 22 : 0.5481543049412195
Loss in iteration 23 : 0.5480240175668746
Loss in iteration 24 : 0.5436071939980776
Loss in iteration 25 : 0.5412189685095883
Loss in iteration 26 : 0.5418440044581516
Loss in iteration 27 : 0.5413746165702616
Loss in iteration 28 : 0.5388169928357585
Loss in iteration 29 : 0.5372253827313184
Loss in iteration 30 : 0.5376120582545659
Loss in iteration 31 : 0.5371476537297105
Loss in iteration 32 : 0.5349343839475666
Loss in iteration 33 : 0.533548626803546
Loss in iteration 34 : 0.5333681079164692
Loss in iteration 35 : 0.5324294827837205
Loss in iteration 36 : 0.5305796483005029
Loss in iteration 37 : 0.5295596218815158
Loss in iteration 38 : 0.5289969770065464
Loss in iteration 39 : 0.5277762958021146
Loss in iteration 40 : 0.5262546835376
Loss in iteration 41 : 0.5254040316228983
Loss in iteration 42 : 0.5249440096827446
Loss in iteration 43 : 0.5237934561175102
Loss in iteration 44 : 0.5224992919166555
Loss in iteration 45 : 0.5218026746547776
Loss in iteration 46 : 0.5212494827190226
Loss in iteration 47 : 0.5203455354929987
Loss in iteration 48 : 0.5194077560586594
Loss in iteration 49 : 0.5188520066945815
Loss in iteration 50 : 0.5184104044383651
Loss in iteration 51 : 0.5177482597159808
Loss in iteration 52 : 0.5171159637949484
Loss in iteration 53 : 0.5166611890367915
Loss in iteration 54 : 0.5162192964768154
Loss in iteration 55 : 0.5157353276581389
Loss in iteration 56 : 0.5153344323996296
Loss in iteration 57 : 0.5150460238393726
Loss in iteration 58 : 0.5147341101617865
Loss in iteration 59 : 0.514419087278653
Loss in iteration 60 : 0.5141649216952012
Loss in iteration 61 : 0.5139533106435676
Loss in iteration 62 : 0.5136954576802321
Loss in iteration 63 : 0.5134474726957335
Loss in iteration 64 : 0.513282190585719
Loss in iteration 65 : 0.5131282478925648
Loss in iteration 66 : 0.512941831546621
Loss in iteration 67 : 0.5127762670769771
Loss in iteration 68 : 0.5126727950740431
Loss in iteration 69 : 0.5125425930357276
Loss in iteration 70 : 0.5123888482175162
Loss in iteration 71 : 0.512275080211388
Loss in iteration 72 : 0.5121716004448574
Loss in iteration 73 : 0.51205676249871
Loss in iteration 74 : 0.5119468979809114
Loss in iteration 75 : 0.5118536167706231
Loss in iteration 76 : 0.511773043945265
Loss in iteration 77 : 0.5116984257941041
Loss in iteration 78 : 0.5116171739365343
Loss in iteration 79 : 0.5115354682582178
Loss in iteration 80 : 0.5114580265042827
Loss in iteration 81 : 0.5113827863207934
Loss in iteration 82 : 0.511304586591761
Loss in iteration 83 : 0.5112455362085341
Loss in iteration 84 : 0.511179870552573
Loss in iteration 85 : 0.5111094388108997
Loss in iteration 86 : 0.5110503913106836
Loss in iteration 87 : 0.5109866836142724
Loss in iteration 88 : 0.5109274572772506
Loss in iteration 89 : 0.5108739837885612
Loss in iteration 90 : 0.510818433770471
Loss in iteration 91 : 0.5107628340193909
Loss in iteration 92 : 0.5107119039140487
Loss in iteration 93 : 0.5106603523907796
Loss in iteration 94 : 0.5106045763978923
Loss in iteration 95 : 0.5105538154278323
Loss in iteration 96 : 0.5105061609508689
Loss in iteration 97 : 0.5104619711901325
Loss in iteration 98 : 0.510420692910507
Loss in iteration 99 : 0.5103764865664159
Loss in iteration 100 : 0.5103337626968751
Loss in iteration 101 : 0.5102934870858052
Loss in iteration 102 : 0.5102536695805786
Loss in iteration 103 : 0.5102139087990453
Loss in iteration 104 : 0.5101748708520496
Loss in iteration 105 : 0.5101358596519826
Loss in iteration 106 : 0.5100998390652749
Loss in iteration 107 : 0.5100649095190932
Loss in iteration 108 : 0.5100291001898952
Loss in iteration 109 : 0.5099932005998236
Loss in iteration 110 : 0.5099589767211108
Loss in iteration 111 : 0.5099255535581043
Loss in iteration 112 : 0.5098923185177828
Loss in iteration 113 : 0.5098590838255631
Loss in iteration 114 : 0.5098271144465036
Loss in iteration 115 : 0.5097953034366232
Loss in iteration 116 : 0.509766576579005
Loss in iteration 117 : 0.5097361757985455
Loss in iteration 118 : 0.5097075976569504
Loss in iteration 119 : 0.5096797833624661
Loss in iteration 120 : 0.5096527715307625
Loss in iteration 121 : 0.5096258260414629
Loss in iteration 122 : 0.5095994177397297
Loss in iteration 123 : 0.5095739468868474
Loss in iteration 124 : 0.5095482483938699
Loss in iteration 125 : 0.50952264313488
Loss in iteration 126 : 0.5094974037902674
Loss in iteration 127 : 0.5094724564601305
Loss in iteration 128 : 0.5094481565380639
Loss in iteration 129 : 0.50942403793593
Loss in iteration 130 : 0.5094008262556154
Loss in iteration 131 : 0.5093768836688889
Loss in iteration 132 : 0.5093537365112174
Loss in iteration 133 : 0.5093305678672921
Loss in iteration 134 : 0.5093086830544262
Loss in iteration 135 : 0.5092867333660338
Loss in iteration 136 : 0.5092655754287543
Loss in iteration 137 : 0.5092435052597227
Loss in iteration 138 : 0.5092224780281545
Loss in iteration 139 : 0.5092027663265875
Loss in iteration 140 : 0.5091823762543066
Loss in iteration 141 : 0.5091636638182451
Loss in iteration 142 : 0.5091453631759697
Loss in iteration 143 : 0.509126648430844
Loss in iteration 144 : 0.5091083398274024
Loss in iteration 145 : 0.5090909841287606
Loss in iteration 146 : 0.5090735873511656
Loss in iteration 147 : 0.5090565022685609
Loss in iteration 148 : 0.5090391797648695
Loss in iteration 149 : 0.5090222541361746
Loss in iteration 150 : 0.5090061407458208
Loss in iteration 151 : 0.5089899530533447
Loss in iteration 152 : 0.5089737626119066
Loss in iteration 153 : 0.5089581334702683
Loss in iteration 154 : 0.5089425466784433
Loss in iteration 155 : 0.508926863497105
Loss in iteration 156 : 0.5089113474171293
Loss in iteration 157 : 0.508896041540113
Loss in iteration 158 : 0.5088806445567859
Loss in iteration 159 : 0.5088653751616736
Loss in iteration 160 : 0.5088502219777857
Loss in iteration 161 : 0.5088349257042225
Loss in iteration 162 : 0.5088200890500477
Loss in iteration 163 : 0.5088051078283725
Loss in iteration 164 : 0.5087900488971571
Loss in iteration 165 : 0.508775338314361
Loss in iteration 166 : 0.5087607311372324
Loss in iteration 167 : 0.5087465064124499
Loss in iteration 168 : 0.5087326181302747
Loss in iteration 169 : 0.5087182856280642
Loss in iteration 170 : 0.5087038896793703
Loss in iteration 171 : 0.5086900416368326
Loss in iteration 172 : 0.5086760754823485
Loss in iteration 173 : 0.5086623940097147
Loss in iteration 174 : 0.5086488713747794
Loss in iteration 175 : 0.5086353705889591
Loss in iteration 176 : 0.508621917742719
Loss in iteration 177 : 0.5086085915403828
Loss in iteration 178 : 0.5085955415227427
Loss in iteration 179 : 0.5085828927981302
Loss in iteration 180 : 0.5085699874491705
Loss in iteration 181 : 0.508557020033956
Loss in iteration 182 : 0.5085444219169845
Loss in iteration 183 : 0.5085317999403963
Loss in iteration 184 : 0.5085194132686268
Loss in iteration 185 : 0.5085067690252294
Loss in iteration 186 : 0.5084942654223318
Loss in iteration 187 : 0.5084821172015253
Loss in iteration 188 : 0.5084696313848714
Loss in iteration 189 : 0.5084571871870865
Loss in iteration 190 : 0.5084451620987713
Loss in iteration 191 : 0.508433232385791
Loss in iteration 192 : 0.5084210127010322
Loss in iteration 193 : 0.5084087441553805
Loss in iteration 194 : 0.5083967966694274
Loss in iteration 195 : 0.5083852493527169
Loss in iteration 196 : 0.5083732919986828
Loss in iteration 197 : 0.5083616343494519
Loss in iteration 198 : 0.5083501934002872
Loss in iteration 199 : 0.508338796039397
Loss in iteration 200 : 0.5083278794927113
Testing accuracy  of updater 9 on alg 1 with rate 0.14 = 0.781, training accuracy 0.787625, time elapsed: 2815 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9727284749657314
Loss in iteration 3 : 0.928063546829843
Loss in iteration 4 : 0.9102057941993855
Loss in iteration 5 : 0.9126851437410307
Loss in iteration 6 : 0.8830959720329383
Loss in iteration 7 : 0.8384404987517649
Loss in iteration 8 : 0.802388815694987
Loss in iteration 9 : 0.7719140256870827
Loss in iteration 10 : 0.7510822901146537
Loss in iteration 11 : 0.7275710666109876
Loss in iteration 12 : 0.6835091817015811
Loss in iteration 13 : 0.6483789554136488
Loss in iteration 14 : 0.6344339955599777
Loss in iteration 15 : 0.6250971082240395
Loss in iteration 16 : 0.6095148922406175
Loss in iteration 17 : 0.5921435261934483
Loss in iteration 18 : 0.5815288297964809
Loss in iteration 19 : 0.5776991652093543
Loss in iteration 20 : 0.5721557400995225
Loss in iteration 21 : 0.5641468053137246
Loss in iteration 22 : 0.5596090359600234
Loss in iteration 23 : 0.558813629146269
Loss in iteration 24 : 0.5576657519721974
Loss in iteration 25 : 0.553703046068778
Loss in iteration 26 : 0.5493517575110322
Loss in iteration 27 : 0.5478972412873456
Loss in iteration 28 : 0.5480774207546196
Loss in iteration 29 : 0.5471058176827257
Loss in iteration 30 : 0.5445699112196203
Loss in iteration 31 : 0.5422958275542185
Loss in iteration 32 : 0.5414964999711522
Loss in iteration 33 : 0.5411433950728215
Loss in iteration 34 : 0.540153244517327
Loss in iteration 35 : 0.5386364514957988
Loss in iteration 36 : 0.5373976730331048
Loss in iteration 37 : 0.5369224156975034
Loss in iteration 38 : 0.5365930902392032
Loss in iteration 39 : 0.535747799017387
Loss in iteration 40 : 0.5344562948195708
Loss in iteration 41 : 0.5333449263174276
Loss in iteration 42 : 0.5328225320270079
Loss in iteration 43 : 0.5324139215688465
Loss in iteration 44 : 0.5316540224566267
Loss in iteration 45 : 0.5306532543860129
Loss in iteration 46 : 0.5298038268620693
Loss in iteration 47 : 0.5292387499800871
Loss in iteration 48 : 0.5287042413656118
Loss in iteration 49 : 0.5279651489823168
Loss in iteration 50 : 0.5271902202369283
Loss in iteration 51 : 0.5266079971479068
Loss in iteration 52 : 0.5261253658228177
Loss in iteration 53 : 0.5255711291549321
Loss in iteration 54 : 0.5249246090943159
Loss in iteration 55 : 0.5243156894313628
Loss in iteration 56 : 0.5238772668083514
Loss in iteration 57 : 0.5234810255150674
Loss in iteration 58 : 0.5229826440327567
Loss in iteration 59 : 0.5224927524222885
Loss in iteration 60 : 0.5220847207239381
Loss in iteration 61 : 0.5217249101258167
Loss in iteration 62 : 0.5213497074652476
Loss in iteration 63 : 0.520957213232562
Loss in iteration 64 : 0.5205977874896522
Loss in iteration 65 : 0.5202728717722455
Loss in iteration 66 : 0.519954726969042
Loss in iteration 67 : 0.5196250138375997
Loss in iteration 68 : 0.5193047631345368
Loss in iteration 69 : 0.5190101930458749
Loss in iteration 70 : 0.5187376926299394
Loss in iteration 71 : 0.5184783508339545
Loss in iteration 72 : 0.5182191891500861
Loss in iteration 73 : 0.5179648870664292
Loss in iteration 74 : 0.5177404493257175
Loss in iteration 75 : 0.517533649597855
Loss in iteration 76 : 0.5173351392790284
Loss in iteration 77 : 0.5171381962193254
Loss in iteration 78 : 0.5169454508059639
Loss in iteration 79 : 0.5167620147678365
Loss in iteration 80 : 0.5165856242282221
Loss in iteration 81 : 0.5164099994673893
Loss in iteration 82 : 0.5162399497547697
Loss in iteration 83 : 0.5160806284111927
Loss in iteration 84 : 0.5159317474074236
Loss in iteration 85 : 0.5157833115911985
Loss in iteration 86 : 0.5156364800984621
Loss in iteration 87 : 0.5154906944383827
Loss in iteration 88 : 0.5153506720295284
Loss in iteration 89 : 0.5152209398601005
Loss in iteration 90 : 0.5150933615583221
Loss in iteration 91 : 0.5149650177616221
Loss in iteration 92 : 0.5148418042553786
Loss in iteration 93 : 0.514721286503841
Loss in iteration 94 : 0.5146049222983001
Loss in iteration 95 : 0.514492174156004
Loss in iteration 96 : 0.5143811071399341
Loss in iteration 97 : 0.5142714782229004
Loss in iteration 98 : 0.5141638832814661
Loss in iteration 99 : 0.5140593701410618
Loss in iteration 100 : 0.5139594527709014
Loss in iteration 101 : 0.513861391672101
Loss in iteration 102 : 0.513764260838092
Loss in iteration 103 : 0.5136701087177159
Loss in iteration 104 : 0.5135775752525529
Loss in iteration 105 : 0.5134872962422654
Loss in iteration 106 : 0.5133981669587326
Loss in iteration 107 : 0.513312031039292
Loss in iteration 108 : 0.5132278141102715
Loss in iteration 109 : 0.5131444066050003
Loss in iteration 110 : 0.5130620399675497
Loss in iteration 111 : 0.5129813039046566
Loss in iteration 112 : 0.5129013597288381
Loss in iteration 113 : 0.512822486431183
Loss in iteration 114 : 0.5127452732523057
Loss in iteration 115 : 0.5126696885725864
Loss in iteration 116 : 0.5125957196130656
Loss in iteration 117 : 0.5125215868142479
Loss in iteration 118 : 0.5124492951287996
Loss in iteration 119 : 0.5123798887059745
Loss in iteration 120 : 0.5123117568838944
Loss in iteration 121 : 0.5122446355698947
Loss in iteration 122 : 0.5121789175743481
Loss in iteration 123 : 0.5121157762447129
Loss in iteration 124 : 0.5120546359317865
Loss in iteration 125 : 0.511995202997189
Loss in iteration 126 : 0.5119367158400309
Loss in iteration 127 : 0.5118815074923089
Loss in iteration 128 : 0.5118273245285918
Loss in iteration 129 : 0.5117738415894568
Loss in iteration 130 : 0.5117210715149987
Loss in iteration 131 : 0.5116684489865044
Loss in iteration 132 : 0.5116164283651932
Loss in iteration 133 : 0.5115655378960736
Loss in iteration 134 : 0.5115159993960102
Loss in iteration 135 : 0.5114681951022071
Loss in iteration 136 : 0.5114215411132017
Loss in iteration 137 : 0.5113751029247421
Loss in iteration 138 : 0.5113291452673898
Loss in iteration 139 : 0.5112848292245057
Loss in iteration 140 : 0.5112414496418205
Loss in iteration 141 : 0.5111990874305721
Loss in iteration 142 : 0.5111589024180151
Loss in iteration 143 : 0.5111195823528728
Loss in iteration 144 : 0.5110795871557504
Loss in iteration 145 : 0.5110405897465046
Loss in iteration 146 : 0.5110030591505663
Loss in iteration 147 : 0.5109665185628971
Loss in iteration 148 : 0.5109302613468285
Loss in iteration 149 : 0.5108940444473442
Loss in iteration 150 : 0.5108586129830205
Loss in iteration 151 : 0.5108239077197163
Loss in iteration 152 : 0.5107905947976291
Loss in iteration 153 : 0.5107579870310001
Loss in iteration 154 : 0.5107255879651278
Loss in iteration 155 : 0.510693495214417
Loss in iteration 156 : 0.5106617951654878
Loss in iteration 157 : 0.5106304231218871
Loss in iteration 158 : 0.5105997692379699
Loss in iteration 159 : 0.5105700460640965
Loss in iteration 160 : 0.5105405407229004
Loss in iteration 161 : 0.5105116780590584
Loss in iteration 162 : 0.5104833807756587
Loss in iteration 163 : 0.5104559587206544
Loss in iteration 164 : 0.5104288689770543
Loss in iteration 165 : 0.5104020188131776
Loss in iteration 166 : 0.5103755726354066
Loss in iteration 167 : 0.5103496902073551
Loss in iteration 168 : 0.5103242757746542
Loss in iteration 169 : 0.5102994641035903
Loss in iteration 170 : 0.5102748792767484
Loss in iteration 171 : 0.5102506319381029
Loss in iteration 172 : 0.5102268828917239
Loss in iteration 173 : 0.5102032796237228
Loss in iteration 174 : 0.51017960195978
Loss in iteration 175 : 0.5101560858101464
Loss in iteration 176 : 0.5101325480089436
Loss in iteration 177 : 0.5101100250850314
Loss in iteration 178 : 0.5100880835692497
Loss in iteration 179 : 0.5100666702723703
Loss in iteration 180 : 0.5100460545738011
Loss in iteration 181 : 0.5100248943548744
Loss in iteration 182 : 0.510004205498615
Loss in iteration 183 : 0.5099841956273146
Loss in iteration 184 : 0.5099641728271278
Loss in iteration 185 : 0.5099439547565366
Loss in iteration 186 : 0.5099241618778779
Loss in iteration 187 : 0.5099048055188043
Loss in iteration 188 : 0.509885214773895
Loss in iteration 189 : 0.5098657054458843
Loss in iteration 190 : 0.5098467499842795
Loss in iteration 191 : 0.5098276676008918
Loss in iteration 192 : 0.5098088186078872
Loss in iteration 193 : 0.5097900894095535
Loss in iteration 194 : 0.5097713328384824
Loss in iteration 195 : 0.5097526223053936
Loss in iteration 196 : 0.5097336428101917
Loss in iteration 197 : 0.5097154120190236
Loss in iteration 198 : 0.5096971884968995
Loss in iteration 199 : 0.509678919729517
Loss in iteration 200 : 0.5096613440175342
Testing accuracy  of updater 9 on alg 1 with rate 0.08000000000000002 = 0.78, training accuracy 0.787625, time elapsed: 4048 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9926370365309083
Loss in iteration 3 : 0.9788202624810135
Loss in iteration 4 : 0.9595272744484321
Loss in iteration 5 : 0.9377901601222308
Loss in iteration 6 : 0.9199097015189358
Loss in iteration 7 : 0.9107537496615864
Loss in iteration 8 : 0.90853372492533
Loss in iteration 9 : 0.9074315165184935
Loss in iteration 10 : 0.9013848138377596
Loss in iteration 11 : 0.8891314011218965
Loss in iteration 12 : 0.8730310853155753
Loss in iteration 13 : 0.8564308306474623
Loss in iteration 14 : 0.8411747692183226
Loss in iteration 15 : 0.8278326138747406
Loss in iteration 16 : 0.8157343309458913
Loss in iteration 17 : 0.804039758793587
Loss in iteration 18 : 0.7923786397940271
Loss in iteration 19 : 0.7804260431209723
Loss in iteration 20 : 0.7683527561056964
Loss in iteration 21 : 0.7564957715857683
Loss in iteration 22 : 0.744675725502361
Loss in iteration 23 : 0.7326469033661801
Loss in iteration 24 : 0.7202534044851755
Loss in iteration 25 : 0.707725494867816
Loss in iteration 26 : 0.6956921599816949
Loss in iteration 27 : 0.6845942134195352
Loss in iteration 28 : 0.674601372767884
Loss in iteration 29 : 0.6656217817845921
Loss in iteration 30 : 0.6572391924377582
Loss in iteration 31 : 0.6492546367105433
Loss in iteration 32 : 0.6417106641053406
Loss in iteration 33 : 0.6347965337008187
Loss in iteration 34 : 0.6285442978883189
Loss in iteration 35 : 0.6230259270059069
Loss in iteration 36 : 0.6180916921256654
Loss in iteration 37 : 0.6136899002095796
Loss in iteration 38 : 0.6095940948925163
Loss in iteration 39 : 0.605654368715736
Loss in iteration 40 : 0.6018484866237522
Loss in iteration 41 : 0.5982417912640529
Loss in iteration 42 : 0.5949619633256941
Loss in iteration 43 : 0.5920683842366045
Loss in iteration 44 : 0.5895363052851926
Loss in iteration 45 : 0.5872617322329116
Loss in iteration 46 : 0.5851854259231507
Loss in iteration 47 : 0.5832102927581182
Loss in iteration 48 : 0.5812874976742517
Loss in iteration 49 : 0.579395233410097
Loss in iteration 50 : 0.5775789979279032
Loss in iteration 51 : 0.5758693716549748
Loss in iteration 52 : 0.574292311895515
Loss in iteration 53 : 0.5728392144813702
Loss in iteration 54 : 0.5714953240886481
Loss in iteration 55 : 0.5702208762379576
Loss in iteration 56 : 0.5689896437593722
Loss in iteration 57 : 0.5677924042505896
Loss in iteration 58 : 0.5666365970264535
Loss in iteration 59 : 0.5655415690186858
Loss in iteration 60 : 0.5645237735630616
Loss in iteration 61 : 0.563569880425907
Loss in iteration 62 : 0.5626642575646984
Loss in iteration 63 : 0.561802217078555
Loss in iteration 64 : 0.5609681531844612
Loss in iteration 65 : 0.5601631974601508
Loss in iteration 66 : 0.5593876256449387
Loss in iteration 67 : 0.5586480824318361
Loss in iteration 68 : 0.5579400317770182
Loss in iteration 69 : 0.5572629895147274
Loss in iteration 70 : 0.5566056288385438
Loss in iteration 71 : 0.5559657605609084
Loss in iteration 72 : 0.5553440143552316
Loss in iteration 73 : 0.5547404107937373
Loss in iteration 74 : 0.5541585264666699
Loss in iteration 75 : 0.5535915757973789
Loss in iteration 76 : 0.5530357789405745
Loss in iteration 77 : 0.5524904261153651
Loss in iteration 78 : 0.5519563535973269
Loss in iteration 79 : 0.5514443779792937
Loss in iteration 80 : 0.5509473123450498
Loss in iteration 81 : 0.5504647810001033
Loss in iteration 82 : 0.5499941900013239
Loss in iteration 83 : 0.5495357277576839
Loss in iteration 84 : 0.549084642173744
Loss in iteration 85 : 0.5486384994474367
Loss in iteration 86 : 0.5481974893321717
Loss in iteration 87 : 0.5477648681537061
Loss in iteration 88 : 0.5473432850794159
Loss in iteration 89 : 0.546936839142691
Loss in iteration 90 : 0.546543872246385
Loss in iteration 91 : 0.5461582735455923
Loss in iteration 92 : 0.5457776565537397
Loss in iteration 93 : 0.545401176286133
Loss in iteration 94 : 0.5450300528586284
Loss in iteration 95 : 0.5446644598567046
Loss in iteration 96 : 0.544303353726921
Loss in iteration 97 : 0.5439475608035692
Loss in iteration 98 : 0.5436004905637114
Loss in iteration 99 : 0.5432611375072594
Loss in iteration 100 : 0.5429283196101896
Loss in iteration 101 : 0.5426025929900095
Loss in iteration 102 : 0.5422821670826493
Loss in iteration 103 : 0.5419673664544614
Loss in iteration 104 : 0.5416579794794134
Loss in iteration 105 : 0.5413525083261754
Loss in iteration 106 : 0.5410506819805477
Loss in iteration 107 : 0.5407511068162719
Loss in iteration 108 : 0.540456746974347
Loss in iteration 109 : 0.5401677246892788
Loss in iteration 110 : 0.5398829003645228
Loss in iteration 111 : 0.5396030168687446
Loss in iteration 112 : 0.539329140369206
Loss in iteration 113 : 0.539060465932743
Loss in iteration 114 : 0.5387951564563079
Loss in iteration 115 : 0.5385357110651808
Loss in iteration 116 : 0.5382792622886992
Loss in iteration 117 : 0.5380270161502081
Loss in iteration 118 : 0.5377777311898772
Loss in iteration 119 : 0.537531838666647
Loss in iteration 120 : 0.5372892904919916
Loss in iteration 121 : 0.5370492175931841
Loss in iteration 122 : 0.5368134596156188
Loss in iteration 123 : 0.5365808381931916
Loss in iteration 124 : 0.5363506004796336
Loss in iteration 125 : 0.5361234472764067
Loss in iteration 126 : 0.5359000920982585
Loss in iteration 127 : 0.53568047654917
Loss in iteration 128 : 0.5354648305681322
Loss in iteration 129 : 0.5352514939775826
Loss in iteration 130 : 0.5350409475675418
Loss in iteration 131 : 0.5348327857536744
Loss in iteration 132 : 0.5346268665304856
Loss in iteration 133 : 0.5344225846319279
Loss in iteration 134 : 0.5342210655931624
Loss in iteration 135 : 0.5340207374984216
Loss in iteration 136 : 0.5338232658825828
Loss in iteration 137 : 0.5336282772629277
Loss in iteration 138 : 0.5334356393556823
Loss in iteration 139 : 0.5332471130444983
Loss in iteration 140 : 0.5330615736734694
Loss in iteration 141 : 0.5328774639589956
Loss in iteration 142 : 0.532694447948007
Loss in iteration 143 : 0.5325128031355255
Loss in iteration 144 : 0.5323345230548957
Loss in iteration 145 : 0.5321599000335473
Loss in iteration 146 : 0.531987711339243
Loss in iteration 147 : 0.5318182298984293
Loss in iteration 148 : 0.5316507755877321
Loss in iteration 149 : 0.5314853078835506
Loss in iteration 150 : 0.5313221134091787
Loss in iteration 151 : 0.5311602551569797
Loss in iteration 152 : 0.5309995452005358
Loss in iteration 153 : 0.5308398144063943
Loss in iteration 154 : 0.530681678029179
Loss in iteration 155 : 0.5305249347041331
Loss in iteration 156 : 0.5303698237212464
Loss in iteration 157 : 0.530216828244328
Loss in iteration 158 : 0.5300651586834031
Loss in iteration 159 : 0.5299153205833055
Loss in iteration 160 : 0.5297667383834977
Loss in iteration 161 : 0.5296187246817663
Loss in iteration 162 : 0.5294713379853513
Loss in iteration 163 : 0.5293244982201561
Loss in iteration 164 : 0.5291784106456864
Loss in iteration 165 : 0.5290333002539291
Loss in iteration 166 : 0.5288887792896592
Loss in iteration 167 : 0.5287456012459228
Loss in iteration 168 : 0.5286041434347646
Loss in iteration 169 : 0.5284644874050229
Loss in iteration 170 : 0.5283260471015313
Loss in iteration 171 : 0.5281889646048186
Loss in iteration 172 : 0.5280530874599917
Loss in iteration 173 : 0.5279187096937911
Loss in iteration 174 : 0.5277856833907772
Loss in iteration 175 : 0.5276545915868673
Loss in iteration 176 : 0.5275251212361318
Loss in iteration 177 : 0.5273969779336773
Loss in iteration 178 : 0.5272695701720128
Loss in iteration 179 : 0.5271428827136205
Loss in iteration 180 : 0.5270163039082356
Loss in iteration 181 : 0.5268910999827326
Loss in iteration 182 : 0.5267667059562425
Loss in iteration 183 : 0.5266433350704233
Loss in iteration 184 : 0.5265218773739148
Loss in iteration 185 : 0.5264020220006246
Loss in iteration 186 : 0.526283783325213
Loss in iteration 187 : 0.5261664250383999
Loss in iteration 188 : 0.5260500199832668
Loss in iteration 189 : 0.5259343276136406
Loss in iteration 190 : 0.5258193783205385
Loss in iteration 191 : 0.5257052357787508
Loss in iteration 192 : 0.5255922302133349
Loss in iteration 193 : 0.5254800408907571
Loss in iteration 194 : 0.5253683929210365
Loss in iteration 195 : 0.5252576446524128
Loss in iteration 196 : 0.52514844660425
Loss in iteration 197 : 0.5250411893866894
Loss in iteration 198 : 0.5249345592033787
Loss in iteration 199 : 0.5248285666060062
Loss in iteration 200 : 0.524723564473569
Testing accuracy  of updater 9 on alg 1 with rate 0.01999999999999999 = 0.777, training accuracy 0.782125, time elapsed: 4044 millisecond.
