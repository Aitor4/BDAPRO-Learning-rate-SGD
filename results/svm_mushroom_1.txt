objc[2560]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1037b24c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x104fef4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 08:47:01 INFO SparkContext: Running Spark version 2.0.0
18/02/27 08:47:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 08:47:02 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 08:47:02 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 08:47:02 INFO SecurityManager: Changing view acls groups to: 
18/02/27 08:47:02 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 08:47:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 08:47:02 INFO Utils: Successfully started service 'sparkDriver' on port 50183.
18/02/27 08:47:03 INFO SparkEnv: Registering MapOutputTracker
18/02/27 08:47:03 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 08:47:03 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-222bdab4-8f18-4cd3-84f4-dbd97b81df86
18/02/27 08:47:03 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 08:47:03 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 08:47:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 08:47:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 08:47:03 INFO Executor: Starting executor ID driver on host localhost
18/02/27 08:47:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50184.
18/02/27 08:47:03 INFO NettyBlockTransferService: Server created on 192.168.2.140:50184
18/02/27 08:47:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50184)
18/02/27 08:47:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50184 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50184)
18/02/27 08:47:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50184)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 17.50765817139115
Loss in iteration 3 : 2.1470878341021122
Loss in iteration 4 : 1.6624074032175602
Loss in iteration 5 : 1.400689463310507
Loss in iteration 6 : 1.1662349370060492
Loss in iteration 7 : 0.963366865689221
Loss in iteration 8 : 0.7769547678710468
Loss in iteration 9 : 0.60833046570853
Loss in iteration 10 : 0.4622969891123916
Loss in iteration 11 : 0.3706725989812462
Loss in iteration 12 : 0.320461543701581
Loss in iteration 13 : 0.29063393035743434
Loss in iteration 14 : 0.2767953298378395
Loss in iteration 15 : 0.25514467408766844
Loss in iteration 16 : 0.23427548523438
Loss in iteration 17 : 0.20420409995882538
Loss in iteration 18 : 0.1846312027582126
Loss in iteration 19 : 0.1705833398200326
Loss in iteration 20 : 0.14754352303001514
Loss in iteration 21 : 0.13443901540153413
Loss in iteration 22 : 0.12109800530115108
Loss in iteration 23 : 0.11291896488450812
Loss in iteration 24 : 0.1029858202249594
Loss in iteration 25 : 0.10100824136173789
Loss in iteration 26 : 0.08895780536161017
Loss in iteration 27 : 0.08577741682778584
Loss in iteration 28 : 0.07792954521526205
Loss in iteration 29 : 0.07524923831991835
Loss in iteration 30 : 0.0678836014143916
Loss in iteration 31 : 0.06548467977744989
Loss in iteration 32 : 0.06270064014533683
Loss in iteration 33 : 0.06432895823499471
Loss in iteration 34 : 0.054803352544037114
Loss in iteration 35 : 0.05241631524819764
Loss in iteration 36 : 0.0501150003990034
Loss in iteration 37 : 0.04800367164880562
Loss in iteration 38 : 0.04597181687725085
Loss in iteration 39 : 0.04425479504575708
Loss in iteration 40 : 0.04254716633609674
Loss in iteration 41 : 0.04264158763035296
Loss in iteration 42 : 0.03891096635538941
Loss in iteration 43 : 0.03730396656832741
Loss in iteration 44 : 0.035798943412648485
Loss in iteration 45 : 0.03431458512500317
Loss in iteration 46 : 0.03309927851839586
Loss in iteration 47 : 0.03225426511032702
Loss in iteration 48 : 0.03076333163739831
Loss in iteration 49 : 0.030880092225754042
Loss in iteration 50 : 0.02836632946448334
Loss in iteration 51 : 0.027222818980411664
Loss in iteration 52 : 0.026273419401011158
Loss in iteration 53 : 0.025713548500077635
Loss in iteration 54 : 0.02497271706503977
Loss in iteration 55 : 0.02311659535109072
Loss in iteration 56 : 0.022138485490607988
Loss in iteration 57 : 0.02116037563012525
Loss in iteration 58 : 0.0201822657696425
Loss in iteration 59 : 0.01923482649392899
Loss in iteration 60 : 0.018336231451749372
Loss in iteration 61 : 0.017456218454935935
Loss in iteration 62 : 0.016626928316023092
Loss in iteration 63 : 0.015902351065723117
Loss in iteration 64 : 0.014836272577287136
Loss in iteration 65 : 0.013819487558646695
Loss in iteration 66 : 0.012916849390112685
Loss in iteration 67 : 0.012045045164988484
Loss in iteration 68 : 0.011117862360707078
Loss in iteration 69 : 0.010231764254531966
Loss in iteration 70 : 0.009363594759160664
Loss in iteration 71 : 0.00891472605450242
Loss in iteration 72 : 0.007683246942477501
Loss in iteration 73 : 0.006799190819309671
Loss in iteration 74 : 0.006255900820395199
Loss in iteration 75 : 0.0060764921933763915
Loss in iteration 76 : 0.00520656659261885
Loss in iteration 77 : 0.004798210830824834
Loss in iteration 78 : 0.004424527940494268
Loss in iteration 79 : 0.0040508450501637
Loss in iteration 80 : 0.0037160823559517324
Loss in iteration 81 : 0.0034873394194772423
Loss in iteration 82 : 0.003156619851619666
Loss in iteration 83 : 0.0030340600315232587
Loss in iteration 84 : 0.0028361101987984436
Loss in iteration 85 : 0.0026135340510059094
Loss in iteration 86 : 0.0024644692914751395
Loss in iteration 87 : 0.0023338232186699607
Loss in iteration 88 : 0.0022193904909425023
Loss in iteration 89 : 0.0020887852577974584
Loss in iteration 90 : 0.0019458464472884996
Loss in iteration 91 : 0.0018130767121557363
Loss in iteration 92 : 0.001678346673336017
Loss in iteration 93 : 0.0015597891399338605
Loss in iteration 94 : 0.0015046964383977012
Loss in iteration 95 : 0.0012924118849618272
Loss in iteration 96 : 0.0011901902156178374
Loss in iteration 97 : 0.0010657517711547618
Loss in iteration 98 : 9.228129606458001E-4
Loss in iteration 99 : 7.901249048333461E-4
Loss in iteration 100 : 6.778158394334498E-4
Loss in iteration 101 : 6.328513736133519E-4
Loss in iteration 102 : 6.10348720873226E-4
Loss in iteration 103 : 5.512128929826659E-4
Testing accuracy  of updater 0 on alg 1 with rate 100.0 = 0.9911111111111112, training accuracy 0.9997142040583024, time elapsed: 6980 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.8826394770367163
Loss in iteration 3 : 0.278181427903175
Loss in iteration 4 : 0.22172063859179625
Loss in iteration 5 : 0.19341609953311278
Loss in iteration 6 : 0.15912757011128237
Loss in iteration 7 : 0.13717902988001063
Loss in iteration 8 : 0.11205181638064071
Loss in iteration 9 : 0.09268789572099191
Loss in iteration 10 : 0.07693256743179223
Loss in iteration 11 : 0.06837694450894839
Loss in iteration 12 : 0.08559559866081856
Loss in iteration 13 : 0.1399742595790034
Loss in iteration 14 : 0.8829616202759435
Loss in iteration 15 : 0.4459558567380659
Loss in iteration 16 : 1.2495978722863783
Loss in iteration 17 : 0.1355307411965415
Loss in iteration 18 : 0.06734116904834037
Loss in iteration 19 : 0.05704675675514525
Loss in iteration 20 : 0.049236498469615426
Loss in iteration 21 : 0.04330163821762328
Loss in iteration 22 : 0.03868344860871938
Loss in iteration 23 : 0.034856854132414114
Loss in iteration 24 : 0.031584290485641556
Loss in iteration 25 : 0.028826714953302714
Loss in iteration 26 : 0.026567219916097378
Loss in iteration 27 : 0.024420401501233247
Loss in iteration 28 : 0.02253136302120698
Loss in iteration 29 : 0.020776646183399
Loss in iteration 30 : 0.019228986422528273
Loss in iteration 31 : 0.017922525694476384
Loss in iteration 32 : 0.01689414221235465
Loss in iteration 33 : 0.01598015061830021
Loss in iteration 34 : 0.01523874742802031
Loss in iteration 35 : 0.01452278734601101
Loss in iteration 36 : 0.013849668067494244
Loss in iteration 37 : 0.013199623196959645
Loss in iteration 38 : 0.012587150813758828
Loss in iteration 39 : 0.012047413865276994
Loss in iteration 40 : 0.011634157344265513
Loss in iteration 41 : 0.011352649566883156
Loss in iteration 42 : 0.011088049408800997
Loss in iteration 43 : 0.0106897810430629
Loss in iteration 44 : 0.010367147727914097
Loss in iteration 45 : 0.009974964471537662
Loss in iteration 46 : 0.009695784554783588
Loss in iteration 47 : 0.009410723726968577
Loss in iteration 48 : 0.009149268222717617
Loss in iteration 49 : 0.008898267671463884
Loss in iteration 50 : 0.008652453757048623
Loss in iteration 51 : 0.008472840931729076
Loss in iteration 52 : 0.008335537994320194
Loss in iteration 53 : 0.008224209080763779
Loss in iteration 54 : 0.007886383412040895
Loss in iteration 55 : 0.007727353775434636
Loss in iteration 56 : 0.007514946703018327
Loss in iteration 57 : 0.007298823221528777
Loss in iteration 58 : 0.007142979078413871
Loss in iteration 59 : 0.006958424654216726
Loss in iteration 60 : 0.006744833231656201
Loss in iteration 61 : 0.006590990231888411
Loss in iteration 62 : 0.006487053296818329
Loss in iteration 63 : 0.006342235861942673
Loss in iteration 64 : 0.006178509664419702
Loss in iteration 65 : 0.0060399406975463
Loss in iteration 66 : 0.005920157974339792
Loss in iteration 67 : 0.00579906838200863
Loss in iteration 68 : 0.005679244819141979
Loss in iteration 69 : 0.005559584614915907
Loss in iteration 70 : 0.005441884714376814
Loss in iteration 71 : 0.00532663519344645
Loss in iteration 72 : 0.005204728807912379
Loss in iteration 73 : 0.00508698806771314
Loss in iteration 74 : 0.004976394268039353
Loss in iteration 75 : 0.004851996663236413
Loss in iteration 76 : 0.00474311812928873
Loss in iteration 77 : 0.004638282721695448
Loss in iteration 78 : 0.004508167564472147
Loss in iteration 79 : 0.004394469950627308
Loss in iteration 80 : 0.004294984538513068
Loss in iteration 81 : 0.004221268951950596
Loss in iteration 82 : 0.004094094250257762
Loss in iteration 83 : 0.0039840722058260124
Loss in iteration 84 : 0.0038680058916927343
Loss in iteration 85 : 0.003772767804233626
Loss in iteration 86 : 0.0036564156124793304
Loss in iteration 87 : 0.0035554599725998596
Loss in iteration 88 : 0.003457118070969697
Loss in iteration 89 : 0.0033532628152199003
Loss in iteration 90 : 0.003257207934557882
Loss in iteration 91 : 0.003146940852165256
Loss in iteration 92 : 0.0030470062037894186
Loss in iteration 93 : 0.002949358576381727
Loss in iteration 94 : 0.0028473411053384742
Loss in iteration 95 : 0.002758596523842482
Loss in iteration 96 : 0.0026537202765890525
Loss in iteration 97 : 0.0025765741585743627
Loss in iteration 98 : 0.002496773462650214
Loss in iteration 99 : 0.00240129033723023
Loss in iteration 100 : 0.002280690820820814
Loss in iteration 101 : 0.0022058725634344074
Loss in iteration 102 : 0.002119129125285544
Loss in iteration 103 : 0.002011720819103097
Loss in iteration 104 : 0.0019251407395948106
Loss in iteration 105 : 0.0018461568368735779
Loss in iteration 106 : 0.0017428324967056737
Loss in iteration 107 : 0.0016568241724394218
Loss in iteration 108 : 0.0015741647003051009
Loss in iteration 109 : 0.0014708403601371974
Loss in iteration 110 : 0.0013905904279514487
Loss in iteration 111 : 0.0013155679722643232
Loss in iteration 112 : 0.0012450378791931816
Loss in iteration 113 : 0.0011620108501175564
Loss in iteration 114 : 0.001082414352494127
Loss in iteration 115 : 0.001072122758137492
Loss in iteration 116 : 9.442537822221881E-4
Loss in iteration 117 : 8.820141401605744E-4
Loss in iteration 118 : 8.248386159569903E-4
Loss in iteration 119 : 7.742791166969644E-4
Loss in iteration 120 : 7.316425115051437E-4
Loss in iteration 121 : 6.852486575799539E-4
Loss in iteration 122 : 6.403658710801405E-4
Loss in iteration 123 : 6.010372783601042E-4
Loss in iteration 124 : 5.98055983169485E-4
Loss in iteration 125 : 5.865391990084819E-4
Loss in iteration 126 : 4.942007274196925E-4
Loss in iteration 127 : 4.724331885621856E-4
Loss in iteration 128 : 4.4645916470970086E-4
Loss in iteration 129 : 4.367393255950912E-4
Loss in iteration 130 : 4.0165805753017707E-4
Loss in iteration 131 : 3.80829830856014E-4
Loss in iteration 132 : 3.6171686990795984E-4
Loss in iteration 133 : 3.448909299280507E-4
Loss in iteration 134 : 3.296168970336633E-4
Loss in iteration 135 : 3.1658904544727364E-4
Loss in iteration 136 : 3.104630964254619E-4
Loss in iteration 137 : 3.0286691963841284E-4
Loss in iteration 138 : 2.7044022948295146E-4
Loss in iteration 139 : 2.691742000184465E-4
Loss in iteration 140 : 2.8481578985413924E-4
Loss in iteration 141 : 2.3801353932749114E-4
Loss in iteration 142 : 2.2322958235485293E-4
Loss in iteration 143 : 2.094257772256988E-4
Loss in iteration 144 : 2.0166624179807142E-4
Loss in iteration 145 : 2.026463936415598E-4
Loss in iteration 146 : 1.8145061002608766E-4
Loss in iteration 147 : 1.6368535786283228E-4
Loss in iteration 148 : 1.4947315613222668E-4
Loss in iteration 149 : 1.4081514818139806E-4
Loss in iteration 150 : 1.273788999935601E-4
Loss in iteration 151 : 1.1937432660505451E-4
Loss in iteration 152 : 1.053254835150332E-4
Loss in iteration 153 : 9.695335318521983E-5
Testing accuracy  of updater 0 on alg 1 with rate 10.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 4874 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38032923872178076
Loss in iteration 3 : 0.7167107353826075
Loss in iteration 4 : 1.2735218064590532
Loss in iteration 5 : 0.1622033682754743
Loss in iteration 6 : 0.15664131286111962
Loss in iteration 7 : 0.21922974604020687
Loss in iteration 8 : 0.11009134933502009
Loss in iteration 9 : 0.10928799238029954
Loss in iteration 10 : 0.13191867745498023
Loss in iteration 11 : 0.101416862581323
Loss in iteration 12 : 0.12214970822921606
Loss in iteration 13 : 0.09000354569929384
Loss in iteration 14 : 0.09156151747435128
Loss in iteration 15 : 0.08039822099173233
Loss in iteration 16 : 0.07939405542807683
Loss in iteration 17 : 0.0722108492844197
Loss in iteration 18 : 0.0689087177237019
Loss in iteration 19 : 0.065221529427303
Loss in iteration 20 : 0.06276596318173952
Loss in iteration 21 : 0.060688694708103015
Loss in iteration 22 : 0.06012931388309124
Loss in iteration 23 : 0.05847473589195984
Loss in iteration 24 : 0.059086514000938166
Loss in iteration 25 : 0.057446331990008
Loss in iteration 26 : 0.057871411592631594
Loss in iteration 27 : 0.05494986482480886
Loss in iteration 28 : 0.0535022826511245
Loss in iteration 29 : 0.049839945287924084
Loss in iteration 30 : 0.04726379994578128
Loss in iteration 31 : 0.04477791025255981
Loss in iteration 32 : 0.042754590969975415
Loss in iteration 33 : 0.04130331280704788
Loss in iteration 34 : 0.03994788532648162
Loss in iteration 35 : 0.03891019040184675
Loss in iteration 36 : 0.03791760288184244
Loss in iteration 37 : 0.03697763726393553
Loss in iteration 38 : 0.03607781703195155
Loss in iteration 39 : 0.03515408517895245
Loss in iteration 40 : 0.03426870176682988
Loss in iteration 41 : 0.033384278086720724
Loss in iteration 42 : 0.03251320897547913
Loss in iteration 43 : 0.031676914834851355
Loss in iteration 44 : 0.030890996415012962
Loss in iteration 45 : 0.03013099075953681
Loss in iteration 46 : 0.02947520791675179
Loss in iteration 47 : 0.029029504285754775
Loss in iteration 48 : 0.028621720279202793
Loss in iteration 49 : 0.02826672153338875
Loss in iteration 50 : 0.02773051721550951
Loss in iteration 51 : 0.02794198497574246
Loss in iteration 52 : 0.027681754661295882
Loss in iteration 53 : 0.02844272004878542
Loss in iteration 54 : 0.029640424761870104
Loss in iteration 55 : 0.029217366722423695
Loss in iteration 56 : 0.030797759470900982
Loss in iteration 57 : 0.02796350747663909
Loss in iteration 58 : 0.027050291836127453
Loss in iteration 59 : 0.024382236838827346
Loss in iteration 60 : 0.022553396017855436
Loss in iteration 61 : 0.021318077977776857
Loss in iteration 62 : 0.02068726858717076
Loss in iteration 63 : 0.02012065914231324
Loss in iteration 64 : 0.019597788973471468
Loss in iteration 65 : 0.01908604761201932
Loss in iteration 66 : 0.01858692570555211
Loss in iteration 67 : 0.01810638584445106
Loss in iteration 68 : 0.01764181429046687
Loss in iteration 69 : 0.017200929739367023
Loss in iteration 70 : 0.016789143446120786
Loss in iteration 71 : 0.016380726424836543
Loss in iteration 72 : 0.016003776361694345
Loss in iteration 73 : 0.015639629532007735
Loss in iteration 74 : 0.015289919522182527
Loss in iteration 75 : 0.014985582374778882
Loss in iteration 76 : 0.01465212654969155
Loss in iteration 77 : 0.014353445695218046
Loss in iteration 78 : 0.014090867100313084
Loss in iteration 79 : 0.013869965378586526
Loss in iteration 80 : 0.013687555036547018
Loss in iteration 81 : 0.013585966381935299
Loss in iteration 82 : 0.013500631912061451
Loss in iteration 83 : 0.013766763557399066
Loss in iteration 84 : 0.013392815209277546
Loss in iteration 85 : 0.014236399229241295
Loss in iteration 86 : 0.013477884221360446
Loss in iteration 87 : 0.014936186805833016
Loss in iteration 88 : 0.013113022697621289
Loss in iteration 89 : 0.013668972991180871
Loss in iteration 90 : 0.011913112642888794
Loss in iteration 91 : 0.011796311214872911
Loss in iteration 92 : 0.01122594452128201
Loss in iteration 93 : 0.011161295339271826
Loss in iteration 94 : 0.010897430295072274
Loss in iteration 95 : 0.01091062150529926
Loss in iteration 96 : 0.010557113407080512
Loss in iteration 97 : 0.010632707618009702
Loss in iteration 98 : 0.010309237089827904
Loss in iteration 99 : 0.010308093579343851
Loss in iteration 100 : 0.009956852082263174
Loss in iteration 101 : 0.009903495066283203
Loss in iteration 102 : 0.009686452692440369
Loss in iteration 103 : 0.009576287709198128
Loss in iteration 104 : 0.009387465540515772
Loss in iteration 105 : 0.00928626486267544
Loss in iteration 106 : 0.009068140237838758
Loss in iteration 107 : 0.008894877979671837
Loss in iteration 108 : 0.008692558303311435
Loss in iteration 109 : 0.008529342601540285
Loss in iteration 110 : 0.008377337386479031
Loss in iteration 111 : 0.00824863119753076
Loss in iteration 112 : 0.008117678827274473
Loss in iteration 113 : 0.008079105768267132
Loss in iteration 114 : 0.00799185183436644
Loss in iteration 115 : 0.007971513683614033
Loss in iteration 116 : 0.00788125803469265
Loss in iteration 117 : 0.007885342000707207
Loss in iteration 118 : 0.008039082901324605
Loss in iteration 119 : 0.00847226917648707
Loss in iteration 120 : 0.007987481990730872
Loss in iteration 121 : 0.008053111324584581
Loss in iteration 122 : 0.007536101646973657
Loss in iteration 123 : 0.007123886537295908
Loss in iteration 124 : 0.006919320679627503
Loss in iteration 125 : 0.006796720019870962
Loss in iteration 126 : 0.00668377793973881
Loss in iteration 127 : 0.006586334510631845
Loss in iteration 128 : 0.006491096423172733
Loss in iteration 129 : 0.006405292297207211
Loss in iteration 130 : 0.006326328814316048
Loss in iteration 131 : 0.006259474290658
Loss in iteration 132 : 0.006201604492231941
Loss in iteration 133 : 0.006158784108569476
Loss in iteration 134 : 0.006109061822342426
Loss in iteration 135 : 0.0060773906658996586
Loss in iteration 136 : 0.006077472345219946
Loss in iteration 137 : 0.006158089834347004
Loss in iteration 138 : 0.0060274437615418066
Loss in iteration 139 : 0.0062088535319077594
Loss in iteration 140 : 0.005976884262281781
Loss in iteration 141 : 0.006139262751019972
Loss in iteration 142 : 0.005774789204052188
Loss in iteration 143 : 0.005758187882203081
Loss in iteration 144 : 0.005585701577578911
Loss in iteration 145 : 0.005538429670960594
Loss in iteration 146 : 0.005494016540552448
Loss in iteration 147 : 0.005476557585840286
Loss in iteration 148 : 0.0054315727001901055
Loss in iteration 149 : 0.005456954548970485
Loss in iteration 150 : 0.0053321689673961606
Loss in iteration 151 : 0.005323715157746062
Loss in iteration 152 : 0.005269561768393234
Loss in iteration 153 : 0.005271277034119347
Loss in iteration 154 : 0.005192864886640143
Loss in iteration 155 : 0.005199113354642396
Loss in iteration 156 : 0.005108367629799277
Loss in iteration 157 : 0.005114575258141383
Loss in iteration 158 : 0.0050476390551630395
Loss in iteration 159 : 0.005054540957727623
Loss in iteration 160 : 0.004974005147920853
Loss in iteration 161 : 0.0049596500073797465
Loss in iteration 162 : 0.0049099277211526945
Loss in iteration 163 : 0.004910356537584229
Loss in iteration 164 : 0.00484239934310225
Loss in iteration 165 : 0.0048305558416600865
Loss in iteration 166 : 0.004776810848908711
Loss in iteration 167 : 0.0047578204069410965
Loss in iteration 168 : 0.004704177513340089
Loss in iteration 169 : 0.004676059407329972
Loss in iteration 170 : 0.004642407527370146
Loss in iteration 171 : 0.004612553735803848
Loss in iteration 172 : 0.004581801471714351
Loss in iteration 173 : 0.004556378783273829
Loss in iteration 174 : 0.004524789306151347
Loss in iteration 175 : 0.0045004284488746075
Loss in iteration 176 : 0.004471268931530778
Loss in iteration 177 : 0.004455974478806321
Loss in iteration 178 : 0.0044142363461377055
Loss in iteration 179 : 0.004393755256574781
Loss in iteration 180 : 0.004357530478025794
Loss in iteration 181 : 0.004349648423617731
Loss in iteration 182 : 0.004332883743128032
Loss in iteration 183 : 0.004384974729643519
Loss in iteration 184 : 0.0042714813140994
Loss in iteration 185 : 0.004290900572498551
Loss in iteration 186 : 0.004203891676558734
Loss in iteration 187 : 0.004188944360945514
Loss in iteration 188 : 0.004146634473034862
Loss in iteration 189 : 0.004128950900191899
Loss in iteration 190 : 0.004098464093893342
Loss in iteration 191 : 0.004110123816864861
Loss in iteration 192 : 0.004060891606559557
Loss in iteration 193 : 0.004102445960757524
Loss in iteration 194 : 0.004010270847809315
Loss in iteration 195 : 0.004018377520348183
Loss in iteration 196 : 0.003962406766118886
Loss in iteration 197 : 0.003971228132710298
Loss in iteration 198 : 0.003911683908218279
Loss in iteration 199 : 0.0039025562441757783
Loss in iteration 200 : 0.003858592350029238
Testing accuracy  of updater 0 on alg 1 with rate 1.0 = 0.9937777777777778, training accuracy 0.9987139182623607, time elapsed: 5769 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.8826394770367167
Loss in iteration 3 : 1.1824377862911226
Loss in iteration 4 : 0.5964445767830943
Loss in iteration 5 : 1.0803368524596217
Loss in iteration 6 : 1.0575533146652523
Loss in iteration 7 : 0.885816542588292
Loss in iteration 8 : 0.7708676135910225
Loss in iteration 9 : 0.7260956320408695
Loss in iteration 10 : 0.7773871189506336
Loss in iteration 11 : 0.8171065438033128
Loss in iteration 12 : 0.7544954756130122
Loss in iteration 13 : 0.626221351985912
Loss in iteration 14 : 0.5086298190882191
Loss in iteration 15 : 0.42362721903850087
Loss in iteration 16 : 0.361504903304012
Loss in iteration 17 : 0.328613060716517
Loss in iteration 18 : 0.32097325162394436
Loss in iteration 19 : 0.2977726364772756
Loss in iteration 20 : 0.25115079583094985
Loss in iteration 21 : 0.2136666537000986
Loss in iteration 22 : 0.19155267297242584
Loss in iteration 23 : 0.18111841055973152
Loss in iteration 24 : 0.17960860616631413
Loss in iteration 25 : 0.1806758229817065
Loss in iteration 26 : 0.17615378648388708
Loss in iteration 27 : 0.16511282468673327
Loss in iteration 28 : 0.15049897747860738
Loss in iteration 29 : 0.13662904467423564
Loss in iteration 30 : 0.12404493813657688
Loss in iteration 31 : 0.11609987110252934
Loss in iteration 32 : 0.11484833305353327
Loss in iteration 33 : 0.1127067372692075
Loss in iteration 34 : 0.10363397833372495
Loss in iteration 35 : 0.09422341938386784
Loss in iteration 36 : 0.08604099932307528
Loss in iteration 37 : 0.07783909915039314
Loss in iteration 38 : 0.07430911274700044
Loss in iteration 39 : 0.0726310511487284
Loss in iteration 40 : 0.07076457599483203
Loss in iteration 41 : 0.06860029062395176
Loss in iteration 42 : 0.06606282533733862
Loss in iteration 43 : 0.06344706380568296
Loss in iteration 44 : 0.06098931218634246
Loss in iteration 45 : 0.05867243900579189
Loss in iteration 46 : 0.05670426964757239
Loss in iteration 47 : 0.054971688960231586
Loss in iteration 48 : 0.05343736091053446
Loss in iteration 49 : 0.05217087678902586
Loss in iteration 50 : 0.05088712876841853
Loss in iteration 51 : 0.049472477312325645
Loss in iteration 52 : 0.047882760140798175
Loss in iteration 53 : 0.046121253183280926
Loss in iteration 54 : 0.044493736034812444
Loss in iteration 55 : 0.04287916576100503
Loss in iteration 56 : 0.041387903345015874
Loss in iteration 57 : 0.04004064263629802
Loss in iteration 58 : 0.03873410544116984
Loss in iteration 59 : 0.03744027485461391
Loss in iteration 60 : 0.036098174805081026
Loss in iteration 61 : 0.03471263223886883
Loss in iteration 62 : 0.0333008453127964
Loss in iteration 63 : 0.03185627479887246
Loss in iteration 64 : 0.030430541044908804
Loss in iteration 65 : 0.029090027028424108
Loss in iteration 66 : 0.027812745764906884
Loss in iteration 67 : 0.026574177262094316
Loss in iteration 68 : 0.02542556017100291
Loss in iteration 69 : 0.02427821704906467
Loss in iteration 70 : 0.02316954163550421
Loss in iteration 71 : 0.022058618471241652
Loss in iteration 72 : 0.02095932018399585
Loss in iteration 73 : 0.019912068958810272
Loss in iteration 74 : 0.01890003908993163
Loss in iteration 75 : 0.017906800606343556
Loss in iteration 76 : 0.016943267490579717
Loss in iteration 77 : 0.01600072537341061
Loss in iteration 78 : 0.015054013887007962
Loss in iteration 79 : 0.014103549968295112
Loss in iteration 80 : 0.013149708860503092
Loss in iteration 81 : 0.012192828282539831
Loss in iteration 82 : 0.011241000132098113
Loss in iteration 83 : 0.010298975575714605
Loss in iteration 84 : 0.00935130050591391
Loss in iteration 85 : 0.008412970547116268
Loss in iteration 86 : 0.007525255134683362
Loss in iteration 87 : 0.0066859707855328595
Loss in iteration 88 : 0.006048154324092071
Loss in iteration 89 : 0.005487596831382453
Loss in iteration 90 : 0.004987112403082613
Loss in iteration 91 : 0.004540543084371612
Loss in iteration 92 : 0.004119853642551964
Loss in iteration 93 : 0.0037561363316816294
Loss in iteration 94 : 0.0033949921901029836
Loss in iteration 95 : 0.003088225618508238
Loss in iteration 96 : 0.0028242873694960522
Loss in iteration 97 : 0.002607998462151948
Loss in iteration 98 : 0.00239864710321394
Loss in iteration 99 : 0.002250042697453493
Loss in iteration 100 : 0.0021020048512181942
Loss in iteration 101 : 0.0019544769085555348
Loss in iteration 102 : 0.0018074078791082441
Loss in iteration 103 : 0.0016607518715547828
Loss in iteration 104 : 0.0015144675837057723
Loss in iteration 105 : 0.0013685178435907679
Loss in iteration 106 : 0.001222869196436368
Loss in iteration 107 : 0.0010831027381937993
Loss in iteration 108 : 9.776014329718754E-4
Loss in iteration 109 : 8.702276336764591E-4
Loss in iteration 110 : 7.933598156437043E-4
Loss in iteration 111 : 7.154409702863785E-4
Testing accuracy  of updater 1 on alg 1 with rate 10.0 = 0.9866666666666667, training accuracy 0.9997142040583024, time elapsed: 3325 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38032923872178076
Loss in iteration 3 : 0.16258107387232926
Loss in iteration 4 : 0.326088268717776
Loss in iteration 5 : 0.14181329563315398
Loss in iteration 6 : 0.19715923835585736
Loss in iteration 7 : 0.19136511362389824
Loss in iteration 8 : 0.1488623986017807
Loss in iteration 9 : 0.13082066461996122
Loss in iteration 10 : 0.13425179828196596
Loss in iteration 11 : 0.13580389235362586
Loss in iteration 12 : 0.13217917907493548
Loss in iteration 13 : 0.12330805276847047
Loss in iteration 14 : 0.11108442122421905
Loss in iteration 15 : 0.098393296289161
Loss in iteration 16 : 0.08755668495476158
Loss in iteration 17 : 0.08041847796279517
Loss in iteration 18 : 0.07655394981892984
Loss in iteration 19 : 0.07333336569772489
Loss in iteration 20 : 0.06899019835483589
Loss in iteration 21 : 0.06283928146199334
Loss in iteration 22 : 0.05638784429644379
Loss in iteration 23 : 0.051959421746334375
Loss in iteration 24 : 0.050832334470968056
Loss in iteration 25 : 0.050070646445938116
Loss in iteration 26 : 0.046978698802786024
Loss in iteration 27 : 0.041559933942462476
Loss in iteration 28 : 0.03616955794276013
Loss in iteration 29 : 0.03295513746741535
Loss in iteration 30 : 0.03113721355027461
Loss in iteration 31 : 0.029423240588280872
Loss in iteration 32 : 0.02743641016951555
Loss in iteration 33 : 0.025192560434794845
Loss in iteration 34 : 0.022840509318262345
Loss in iteration 35 : 0.020702366695046475
Loss in iteration 36 : 0.01895401980009804
Loss in iteration 37 : 0.017714614877576244
Loss in iteration 38 : 0.016655735035967043
Loss in iteration 39 : 0.015749950724734278
Loss in iteration 40 : 0.014924000736312759
Loss in iteration 41 : 0.01417312765283592
Loss in iteration 42 : 0.013554204574498983
Loss in iteration 43 : 0.012985286387194952
Loss in iteration 44 : 0.01245823561843371
Loss in iteration 45 : 0.011976063444632137
Loss in iteration 46 : 0.011555601637078486
Loss in iteration 47 : 0.011178569309616639
Loss in iteration 48 : 0.010817830882497145
Loss in iteration 49 : 0.010479862675132902
Loss in iteration 50 : 0.010174161576167524
Loss in iteration 51 : 0.009888514185775178
Loss in iteration 52 : 0.009615010849534977
Loss in iteration 53 : 0.00936291329504558
Loss in iteration 54 : 0.009138705998446565
Loss in iteration 55 : 0.008925574003149537
Loss in iteration 56 : 0.008711621398192074
Loss in iteration 57 : 0.008499950342299103
Loss in iteration 58 : 0.008287149536388032
Loss in iteration 59 : 0.00809020640483004
Loss in iteration 60 : 0.007910866779092815
Loss in iteration 61 : 0.007733714544217559
Loss in iteration 62 : 0.0075577732211896806
Loss in iteration 63 : 0.007375695561550129
Loss in iteration 64 : 0.007182286702781469
Loss in iteration 65 : 0.0069966484172230715
Loss in iteration 66 : 0.006832366522164713
Loss in iteration 67 : 0.006682242460558773
Loss in iteration 68 : 0.006533388603102084
Loss in iteration 69 : 0.006384807522359156
Loss in iteration 70 : 0.006239803737357143
Loss in iteration 71 : 0.006102352599248908
Loss in iteration 72 : 0.005974296158170026
Loss in iteration 73 : 0.005847858090912075
Loss in iteration 74 : 0.005725889915850967
Loss in iteration 75 : 0.005604571000940698
Loss in iteration 76 : 0.005484129441281638
Loss in iteration 77 : 0.005364015992688495
Loss in iteration 78 : 0.005244032053741268
Loss in iteration 79 : 0.005124419307830471
Loss in iteration 80 : 0.005005128533369308
Loss in iteration 81 : 0.004886127533212819
Loss in iteration 82 : 0.004767387329930535
Loss in iteration 83 : 0.004648881843835034
Loss in iteration 84 : 0.0045311417740154574
Loss in iteration 85 : 0.004419818796336922
Loss in iteration 86 : 0.004310208037880121
Loss in iteration 87 : 0.0042087961151827755
Loss in iteration 88 : 0.0041092021100284926
Loss in iteration 89 : 0.00401753088182345
Loss in iteration 90 : 0.003925346501509381
Loss in iteration 91 : 0.0038308433552356365
Loss in iteration 92 : 0.0037342533195981876
Loss in iteration 93 : 0.003637267110778683
Loss in iteration 94 : 0.0035470150159325697
Loss in iteration 95 : 0.003455578215534714
Loss in iteration 96 : 0.00336307518014029
Loss in iteration 97 : 0.0032705251310098864
Loss in iteration 98 : 0.0031774716186440933
Loss in iteration 99 : 0.003082698029487455
Loss in iteration 100 : 0.0029859908077823644
Loss in iteration 101 : 0.0028879226262501557
Loss in iteration 102 : 0.002790656764925276
Loss in iteration 103 : 0.0026952039295677407
Loss in iteration 104 : 0.00260002804577175
Loss in iteration 105 : 0.0025051609411288786
Loss in iteration 106 : 0.002410921890528443
Loss in iteration 107 : 0.002316366486043369
Loss in iteration 108 : 0.002223012692046168
Loss in iteration 109 : 0.002129474787314035
Loss in iteration 110 : 0.0020354700917876905
Loss in iteration 111 : 0.0019429841939802533
Loss in iteration 112 : 0.001849787397037304
Loss in iteration 113 : 0.001757220640310623
Loss in iteration 114 : 0.001663966102011201
Loss in iteration 115 : 0.001570092560296312
Loss in iteration 116 : 0.001475797192824111
Loss in iteration 117 : 0.001383078591946978
Loss in iteration 118 : 0.0012902578872001414
Loss in iteration 119 : 0.0011972281749322972
Loss in iteration 120 : 0.0011037228367301612
Loss in iteration 121 : 0.0010196150876353217
Loss in iteration 122 : 9.39829514330749E-4
Loss in iteration 123 : 8.734444647352973E-4
Loss in iteration 124 : 8.139353224343702E-4
Loss in iteration 125 : 7.568956841342973E-4
Loss in iteration 126 : 7.139150980035243E-4
Loss in iteration 127 : 6.75806988882644E-4
Loss in iteration 128 : 6.401471498890737E-4
Loss in iteration 129 : 6.059494707723353E-4
Loss in iteration 130 : 5.741929353346014E-4
Loss in iteration 131 : 5.456665334246765E-4
Loss in iteration 132 : 5.177873516242745E-4
Loss in iteration 133 : 4.911535952133796E-4
Loss in iteration 134 : 4.6471241500477625E-4
Loss in iteration 135 : 4.4122426518001205E-4
Loss in iteration 136 : 4.2332557452018517E-4
Loss in iteration 137 : 4.0401024952612324E-4
Loss in iteration 138 : 3.809266202898356E-4
Loss in iteration 139 : 3.596120947765857E-4
Loss in iteration 140 : 3.39513009299406E-4
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.992, training accuracy 1.0, time elapsed: 3709 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8642561778766499
Loss in iteration 3 : 0.6063429158422842
Loss in iteration 4 : 0.4059050616552262
Loss in iteration 5 : 0.2617863986962837
Loss in iteration 6 : 0.20918030163172407
Loss in iteration 7 : 0.18624151727401242
Loss in iteration 8 : 0.17111295857390688
Loss in iteration 9 : 0.16033436855827457
Loss in iteration 10 : 0.14620481028971444
Loss in iteration 11 : 0.13018154793234904
Loss in iteration 12 : 0.1208372113012009
Loss in iteration 13 : 0.11562572415307917
Loss in iteration 14 : 0.11171603232471544
Loss in iteration 15 : 0.10843099073462258
Loss in iteration 16 : 0.10531626494123597
Loss in iteration 17 : 0.10234425429075848
Loss in iteration 18 : 0.09949210537593345
Loss in iteration 19 : 0.09677037703386146
Loss in iteration 20 : 0.09419722681325281
Loss in iteration 21 : 0.09169070599950761
Loss in iteration 22 : 0.08921294565228258
Loss in iteration 23 : 0.0867100788148634
Loss in iteration 24 : 0.08416940847411869
Loss in iteration 25 : 0.08162757754090254
Loss in iteration 26 : 0.07920069758406442
Loss in iteration 27 : 0.07688114878077898
Loss in iteration 28 : 0.0746894271041305
Loss in iteration 29 : 0.07260135813019745
Loss in iteration 30 : 0.07058397132959243
Loss in iteration 31 : 0.06859153996044014
Loss in iteration 32 : 0.06663963974816109
Loss in iteration 33 : 0.0646991359116289
Loss in iteration 34 : 0.06285192922141791
Loss in iteration 35 : 0.06103768872677061
Loss in iteration 36 : 0.05931111176270477
Loss in iteration 37 : 0.057674993185150444
Loss in iteration 38 : 0.05609978092629534
Loss in iteration 39 : 0.054628611018654405
Loss in iteration 40 : 0.053246887832442615
Loss in iteration 41 : 0.051982503635794916
Loss in iteration 42 : 0.05081332220130532
Loss in iteration 43 : 0.049712453886980805
Loss in iteration 44 : 0.04868087915375325
Loss in iteration 45 : 0.04768679155283613
Loss in iteration 46 : 0.04675172575680978
Loss in iteration 47 : 0.04584432534765278
Loss in iteration 48 : 0.04497514836247068
Loss in iteration 49 : 0.044124509289357405
Loss in iteration 50 : 0.043286294171716415
Loss in iteration 51 : 0.042440576555540704
Loss in iteration 52 : 0.04158425719848226
Loss in iteration 53 : 0.04071503613443036
Loss in iteration 54 : 0.03987014176908591
Loss in iteration 55 : 0.039031286094879034
Loss in iteration 56 : 0.038183856252870386
Loss in iteration 57 : 0.037328900295786255
Loss in iteration 58 : 0.03647117692742889
Loss in iteration 59 : 0.03561819738990076
Loss in iteration 60 : 0.0347723580027691
Loss in iteration 61 : 0.03394500589834645
Loss in iteration 62 : 0.03314016731625736
Loss in iteration 63 : 0.032355960529197716
Loss in iteration 64 : 0.031579195117284284
Loss in iteration 65 : 0.030820471486229902
Loss in iteration 66 : 0.030089362023977592
Loss in iteration 67 : 0.029397692575831882
Loss in iteration 68 : 0.02872517691206639
Loss in iteration 69 : 0.028068449586034053
Loss in iteration 70 : 0.027431970477274963
Loss in iteration 71 : 0.026828202981833384
Loss in iteration 72 : 0.02624907109783564
Loss in iteration 73 : 0.025675858199342136
Loss in iteration 74 : 0.0251033616924225
Loss in iteration 75 : 0.024532612295808248
Loss in iteration 76 : 0.023961302307837908
Loss in iteration 77 : 0.0233933360260768
Loss in iteration 78 : 0.022830419069357327
Loss in iteration 79 : 0.022270356079732434
Loss in iteration 80 : 0.02171159722416133
Loss in iteration 81 : 0.02115265582149471
Loss in iteration 82 : 0.02059625951517058
Loss in iteration 83 : 0.020047462649047425
Loss in iteration 84 : 0.01951439912351132
Loss in iteration 85 : 0.018999837980050928
Loss in iteration 86 : 0.01850980374407596
Loss in iteration 87 : 0.0180470202528293
Loss in iteration 88 : 0.017604064142348094
Loss in iteration 89 : 0.017180234032860852
Loss in iteration 90 : 0.016779079793215003
Loss in iteration 91 : 0.01640208055786216
Loss in iteration 92 : 0.01604870314744102
Loss in iteration 93 : 0.01573522309841142
Loss in iteration 94 : 0.015428770403479352
Loss in iteration 95 : 0.01515242343146793
Loss in iteration 96 : 0.01489417656730261
Loss in iteration 97 : 0.014650336203318107
Loss in iteration 98 : 0.014413261564811354
Loss in iteration 99 : 0.014183857981893836
Loss in iteration 100 : 0.013966405648652551
Loss in iteration 101 : 0.013757837089122609
Loss in iteration 102 : 0.013555183591921389
Loss in iteration 103 : 0.013360381389714392
Loss in iteration 104 : 0.013176029135712334
Loss in iteration 105 : 0.01299813852280319
Loss in iteration 106 : 0.012825833823099098
Loss in iteration 107 : 0.01265586163799484
Loss in iteration 108 : 0.012487722090654426
Loss in iteration 109 : 0.012319138472605183
Loss in iteration 110 : 0.012151582955248515
Loss in iteration 111 : 0.011987172516616637
Loss in iteration 112 : 0.011830460560861083
Loss in iteration 113 : 0.01167943257987926
Loss in iteration 114 : 0.01153114141403211
Loss in iteration 115 : 0.011383309800989385
Loss in iteration 116 : 0.011235915687469894
Loss in iteration 117 : 0.011090919086050014
Loss in iteration 118 : 0.010948372715103346
Loss in iteration 119 : 0.010807714238009122
Loss in iteration 120 : 0.010668049811869163
Loss in iteration 121 : 0.010528852766038748
Loss in iteration 122 : 0.010390655989758961
Loss in iteration 123 : 0.010255646526905337
Loss in iteration 124 : 0.010122220166125422
Loss in iteration 125 : 0.009990517892875576
Loss in iteration 126 : 0.009861203841908871
Loss in iteration 127 : 0.009734606522884536
Loss in iteration 128 : 0.009611703764318138
Loss in iteration 129 : 0.009490058039386443
Loss in iteration 130 : 0.00936906270108781
Loss in iteration 131 : 0.009249235322326698
Loss in iteration 132 : 0.009130770503549412
Loss in iteration 133 : 0.009018233587989382
Loss in iteration 134 : 0.00891298388348163
Loss in iteration 135 : 0.008808060358521426
Loss in iteration 136 : 0.008707547423897073
Loss in iteration 137 : 0.008609305504301843
Loss in iteration 138 : 0.008513825036317292
Loss in iteration 139 : 0.008419976763709095
Loss in iteration 140 : 0.008332036554344401
Loss in iteration 141 : 0.008247095110111286
Loss in iteration 142 : 0.00816499917364902
Loss in iteration 143 : 0.008087776745916953
Loss in iteration 144 : 0.008016114387880724
Loss in iteration 145 : 0.007945420556630212
Loss in iteration 146 : 0.007876182023468375
Loss in iteration 147 : 0.007806471882214724
Loss in iteration 148 : 0.007737191998096088
Loss in iteration 149 : 0.00767050491511085
Loss in iteration 150 : 0.007604238487899029
Loss in iteration 151 : 0.007538614792682964
Loss in iteration 152 : 0.00747480797339695
Loss in iteration 153 : 0.007413131320465432
Loss in iteration 154 : 0.0073532927034994245
Loss in iteration 155 : 0.0072943086827328765
Loss in iteration 156 : 0.007236428253459643
Loss in iteration 157 : 0.007180302433383255
Loss in iteration 158 : 0.007124694646500706
Loss in iteration 159 : 0.0070695742256022456
Loss in iteration 160 : 0.007014687320719835
Loss in iteration 161 : 0.006960224580598534
Loss in iteration 162 : 0.006908032229126692
Loss in iteration 163 : 0.006858137551425589
Loss in iteration 164 : 0.006809633179699806
Loss in iteration 165 : 0.006761462017835994
Loss in iteration 166 : 0.006713624051194644
Loss in iteration 167 : 0.006666411421870448
Loss in iteration 168 : 0.006619453832745061
Loss in iteration 169 : 0.00657402042850418
Loss in iteration 170 : 0.006528800807267544
Loss in iteration 171 : 0.006483858915920284
Loss in iteration 172 : 0.0064403679471448545
Loss in iteration 173 : 0.006397329633113332
Loss in iteration 174 : 0.006353952961880633
Loss in iteration 175 : 0.006310349594384537
Loss in iteration 176 : 0.006266951410164002
Loss in iteration 177 : 0.006224059568551342
Loss in iteration 178 : 0.006181914704881536
Loss in iteration 179 : 0.006140019679593039
Loss in iteration 180 : 0.0060977014842643175
Loss in iteration 181 : 0.006055004006114192
Loss in iteration 182 : 0.006013611258465875
Loss in iteration 183 : 0.005972370492534965
Loss in iteration 184 : 0.005931138870382226
Loss in iteration 185 : 0.005889666692758222
Loss in iteration 186 : 0.005848574210203467
Loss in iteration 187 : 0.005807844001130582
Loss in iteration 188 : 0.005766935404343849
Loss in iteration 189 : 0.005725866258614648
Loss in iteration 190 : 0.005684733090499535
Loss in iteration 191 : 0.005644308077242625
Loss in iteration 192 : 0.005604090869113611
Loss in iteration 193 : 0.005565530959540386
Loss in iteration 194 : 0.005527599870723351
Loss in iteration 195 : 0.005489767438404656
Loss in iteration 196 : 0.005452023602247526
Loss in iteration 197 : 0.00541428768550419
Loss in iteration 198 : 0.005378583999373583
Loss in iteration 199 : 0.005343012579564708
Loss in iteration 200 : 0.005307754811460914
Testing accuracy  of updater 1 on alg 1 with rate 0.09999999999999998 = 0.9786666666666667, training accuracy 0.9985710202915119, time elapsed: 4742 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.4437439194850996
Loss in iteration 3 : 0.8831775742728438
Loss in iteration 4 : 0.8273652125814579
Loss in iteration 5 : 0.9480216160237186
Loss in iteration 6 : 0.8814463429411631
Loss in iteration 7 : 0.7848374331669175
Loss in iteration 8 : 0.712577399729364
Loss in iteration 9 : 0.6432121477284963
Loss in iteration 10 : 0.5859097930158815
Loss in iteration 11 : 0.5444547408485861
Loss in iteration 12 : 0.49282691334067313
Loss in iteration 13 : 0.4283480375942148
Loss in iteration 14 : 0.3626709469155019
Loss in iteration 15 : 0.31141532259672094
Loss in iteration 16 : 0.28353028195902535
Loss in iteration 17 : 0.2707505974765086
Loss in iteration 18 : 0.26077768686227926
Loss in iteration 19 : 0.2445350694359468
Loss in iteration 20 : 0.22981016185416986
Loss in iteration 21 : 0.2149679181804924
Loss in iteration 22 : 0.198296577246585
Loss in iteration 23 : 0.18079947001140312
Loss in iteration 24 : 0.1684213471622158
Loss in iteration 25 : 0.15728511271049642
Loss in iteration 26 : 0.14693039908973202
Loss in iteration 27 : 0.1344987926420939
Loss in iteration 28 : 0.12333352312015185
Loss in iteration 29 : 0.11295217396002848
Loss in iteration 30 : 0.10365937405844122
Loss in iteration 31 : 0.09570981880545044
Loss in iteration 32 : 0.08903845681180887
Loss in iteration 33 : 0.0841369325518683
Loss in iteration 34 : 0.07991315544471948
Loss in iteration 35 : 0.07645406058915195
Loss in iteration 36 : 0.0734056815185441
Loss in iteration 37 : 0.0702961125743953
Loss in iteration 38 : 0.06734673583154986
Loss in iteration 39 : 0.06456740891600607
Loss in iteration 40 : 0.061944105906499465
Loss in iteration 41 : 0.05958287410471352
Loss in iteration 42 : 0.05734235498135406
Loss in iteration 43 : 0.055275676922061014
Loss in iteration 44 : 0.05327988426650352
Loss in iteration 45 : 0.05138557754808104
Loss in iteration 46 : 0.04955451536233321
Loss in iteration 47 : 0.047726466984851564
Loss in iteration 48 : 0.045875187359684454
Loss in iteration 49 : 0.044028259675611574
Loss in iteration 50 : 0.04228203985854601
Loss in iteration 51 : 0.04066659622374072
Loss in iteration 52 : 0.03923067008192779
Loss in iteration 53 : 0.03786294060903373
Loss in iteration 54 : 0.03654851236834937
Loss in iteration 55 : 0.03526445658860708
Loss in iteration 56 : 0.03402936906979717
Loss in iteration 57 : 0.03280629803067126
Loss in iteration 58 : 0.031610940455276154
Loss in iteration 59 : 0.0304918579461378
Loss in iteration 60 : 0.029403838175058764
Loss in iteration 61 : 0.02833227224585594
Loss in iteration 62 : 0.027260727553723337
Loss in iteration 63 : 0.026203086181648773
Loss in iteration 64 : 0.025166053771668733
Loss in iteration 65 : 0.024159785029112125
Loss in iteration 66 : 0.023132028253863922
Loss in iteration 67 : 0.022095146487342097
Loss in iteration 68 : 0.021075230180428933
Loss in iteration 69 : 0.020111046119885106
Loss in iteration 70 : 0.01919593060439347
Loss in iteration 71 : 0.018292812181177467
Loss in iteration 72 : 0.01738974995136169
Loss in iteration 73 : 0.01649032837063809
Loss in iteration 74 : 0.015606377430297979
Loss in iteration 75 : 0.014725067587893796
Loss in iteration 76 : 0.013850040078675772
Loss in iteration 77 : 0.012983676464206579
Loss in iteration 78 : 0.012112197617353928
Loss in iteration 79 : 0.011242499184207532
Loss in iteration 80 : 0.010381116893528308
Loss in iteration 81 : 0.009514033532932446
Loss in iteration 82 : 0.008659287544141686
Loss in iteration 83 : 0.007804600551117813
Loss in iteration 84 : 0.006959968798360557
Loss in iteration 85 : 0.006188896753380576
Loss in iteration 86 : 0.005518120863039
Loss in iteration 87 : 0.005000527695525649
Loss in iteration 88 : 0.004502124848242966
Loss in iteration 89 : 0.004016193996655485
Loss in iteration 90 : 0.0035414879411936944
Loss in iteration 91 : 0.003120974140371781
Loss in iteration 92 : 0.0027276058026452273
Loss in iteration 93 : 0.002353394010240028
Loss in iteration 94 : 0.0020977037773530897
Loss in iteration 95 : 0.0018766482343403274
Loss in iteration 96 : 0.0016885354491523383
Loss in iteration 97 : 0.0015405754743674966
Loss in iteration 98 : 0.0013984267718291513
Loss in iteration 99 : 0.001261508214312648
Loss in iteration 100 : 0.001148181465950646
Loss in iteration 101 : 0.0010325138544245564
Loss in iteration 102 : 9.18667403790687E-4
Loss in iteration 103 : 8.53134076273383E-4
Loss in iteration 104 : 7.900701154932672E-4
Loss in iteration 105 : 7.292285847766229E-4
Loss in iteration 106 : 6.703872411171016E-4
Testing accuracy  of updater 2 on alg 1 with rate 10.0 = 0.9893333333333333, training accuracy 0.9997142040583024, time elapsed: 2567 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5009066282033305
Loss in iteration 3 : 0.13945594895401872
Loss in iteration 4 : 0.20702487837745007
Loss in iteration 5 : 0.23068228073942
Loss in iteration 6 : 0.13686249024585556
Loss in iteration 7 : 0.12810766704887144
Loss in iteration 8 : 0.12619627081869061
Loss in iteration 9 : 0.12112996591329324
Loss in iteration 10 : 0.11438757412942752
Loss in iteration 11 : 0.10668682977842951
Loss in iteration 12 : 0.09835515054152012
Loss in iteration 13 : 0.09044711506451664
Loss in iteration 14 : 0.08308063192213236
Loss in iteration 15 : 0.07599451598614856
Loss in iteration 16 : 0.06870545694962397
Loss in iteration 17 : 0.06130900262790829
Loss in iteration 18 : 0.05437880185620423
Loss in iteration 19 : 0.04875428924478039
Loss in iteration 20 : 0.045109102272008685
Loss in iteration 21 : 0.04238709013505198
Loss in iteration 22 : 0.03900493394684469
Loss in iteration 23 : 0.03549849953192093
Loss in iteration 24 : 0.032320910827068665
Loss in iteration 25 : 0.0294678541266812
Loss in iteration 26 : 0.027073607933345647
Loss in iteration 27 : 0.02526298268752436
Loss in iteration 28 : 0.02369262254725149
Loss in iteration 29 : 0.02226835017055276
Loss in iteration 30 : 0.02092199168399479
Loss in iteration 31 : 0.019496619684169443
Loss in iteration 32 : 0.01813490963822116
Loss in iteration 33 : 0.017068834136367835
Loss in iteration 34 : 0.01607064801160084
Loss in iteration 35 : 0.01511671218571472
Loss in iteration 36 : 0.014172609820475373
Loss in iteration 37 : 0.013314302603063908
Loss in iteration 38 : 0.012499127948630267
Loss in iteration 39 : 0.01184573602484942
Loss in iteration 40 : 0.011316942420370091
Loss in iteration 41 : 0.010800386528056231
Loss in iteration 42 : 0.01039518184601986
Loss in iteration 43 : 0.010041575963475187
Loss in iteration 44 : 0.009703145238582124
Loss in iteration 45 : 0.009387931603073676
Loss in iteration 46 : 0.009092470736069677
Loss in iteration 47 : 0.008809317199840561
Loss in iteration 48 : 0.008540137444418432
Loss in iteration 49 : 0.008294725131438585
Loss in iteration 50 : 0.008051036283590517
Loss in iteration 51 : 0.007807779592769044
Loss in iteration 52 : 0.007596488340485405
Loss in iteration 53 : 0.0073979148499463505
Loss in iteration 54 : 0.007203837197695826
Loss in iteration 55 : 0.0070043572171320165
Loss in iteration 56 : 0.00681386660093895
Loss in iteration 57 : 0.006641325150717084
Loss in iteration 58 : 0.006470623535471328
Loss in iteration 59 : 0.0063080849335916825
Loss in iteration 60 : 0.006160095909374621
Loss in iteration 61 : 0.006017924624319082
Loss in iteration 62 : 0.0058915379640354275
Loss in iteration 63 : 0.005769996121098057
Loss in iteration 64 : 0.005652694635042582
Loss in iteration 65 : 0.005542107688817449
Loss in iteration 66 : 0.005432273133308683
Loss in iteration 67 : 0.005322623577902214
Loss in iteration 68 : 0.005209521242452516
Loss in iteration 69 : 0.0051016296088500415
Loss in iteration 70 : 0.004993253774448098
Loss in iteration 71 : 0.004890174546835246
Loss in iteration 72 : 0.004787978101151068
Loss in iteration 73 : 0.00468515283832333
Loss in iteration 74 : 0.0045825327503526
Loss in iteration 75 : 0.0044838058789328505
Loss in iteration 76 : 0.004388301224177016
Loss in iteration 77 : 0.004292830544431907
Loss in iteration 78 : 0.004197365808106671
Loss in iteration 79 : 0.004102085958879192
Loss in iteration 80 : 0.004009003880625098
Loss in iteration 81 : 0.003916538899861825
Loss in iteration 82 : 0.0038257778434223444
Loss in iteration 83 : 0.0037340131530266027
Loss in iteration 84 : 0.0036422242908920564
Loss in iteration 85 : 0.0035503329675505347
Loss in iteration 86 : 0.0034587719149816146
Loss in iteration 87 : 0.0033667030013721836
Loss in iteration 88 : 0.003274304918479739
Loss in iteration 89 : 0.0031825621200196386
Loss in iteration 90 : 0.003089839877725521
Loss in iteration 91 : 0.0029974471828617
Loss in iteration 92 : 0.0029063771853929166
Loss in iteration 93 : 0.0028137250886731634
Loss in iteration 94 : 0.0027219370779563595
Loss in iteration 95 : 0.002629648594451294
Loss in iteration 96 : 0.0025378704107531813
Loss in iteration 97 : 0.002446983493329092
Loss in iteration 98 : 0.0023558861578527497
Loss in iteration 99 : 0.002265213680995413
Loss in iteration 100 : 0.0021751610574557256
Loss in iteration 101 : 0.0020868381980074465
Loss in iteration 102 : 0.0019942062481295357
Loss in iteration 103 : 0.0019044611826006177
Loss in iteration 104 : 0.001815057988171582
Loss in iteration 105 : 0.0017265313940222026
Loss in iteration 106 : 0.001636342147168059
Loss in iteration 107 : 0.001548050967684005
Loss in iteration 108 : 0.001458898378591482
Loss in iteration 109 : 0.0013696143516768878
Loss in iteration 110 : 0.0012803493938519292
Loss in iteration 111 : 0.0011912770652005588
Loss in iteration 112 : 0.0011023359005745566
Loss in iteration 113 : 0.0010145576821193262
Loss in iteration 114 : 9.303761008441811E-4
Loss in iteration 115 : 8.552266651167053E-4
Loss in iteration 116 : 7.792477086239621E-4
Loss in iteration 117 : 7.100435277091646E-4
Loss in iteration 118 : 6.482862017816814E-4
Loss in iteration 119 : 5.971266158541842E-4
Loss in iteration 120 : 5.490833261523458E-4
Loss in iteration 121 : 5.025008974375705E-4
Loss in iteration 122 : 4.6290682846727267E-4
Loss in iteration 123 : 4.33779336065967E-4
Loss in iteration 124 : 4.1007317281445925E-4
Loss in iteration 125 : 3.853734574161788E-4
Loss in iteration 126 : 3.602805277944315E-4
Loss in iteration 127 : 3.3608156585851164E-4
Loss in iteration 128 : 3.200907677014452E-4
Loss in iteration 129 : 3.04373676152242E-4
Loss in iteration 130 : 2.896698144447705E-4
Loss in iteration 131 : 2.7966191359382354E-4
Loss in iteration 132 : 2.698931409665628E-4
Loss in iteration 133 : 2.606854148972208E-4
Loss in iteration 134 : 2.517440993221095E-4
Testing accuracy  of updater 2 on alg 1 with rate 1.0 = 0.9946666666666667, training accuracy 1.0, time elapsed: 3332 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7420867379656343
Loss in iteration 3 : 0.4400620845738721
Loss in iteration 4 : 0.24176183539681162
Loss in iteration 5 : 0.20460925585871487
Loss in iteration 6 : 0.1855337607896749
Loss in iteration 7 : 0.1683459971488996
Loss in iteration 8 : 0.15023220885599314
Loss in iteration 9 : 0.1353935179368286
Loss in iteration 10 : 0.12385205717462917
Loss in iteration 11 : 0.11444974270526909
Loss in iteration 12 : 0.10696352770835756
Loss in iteration 13 : 0.10136303711364357
Loss in iteration 14 : 0.09714987330670392
Loss in iteration 15 : 0.0937821165273916
Loss in iteration 16 : 0.09076389340477077
Loss in iteration 17 : 0.08791025724222028
Loss in iteration 18 : 0.08514303083368804
Loss in iteration 19 : 0.08242375098676129
Loss in iteration 20 : 0.07974662373756054
Loss in iteration 21 : 0.07703343436981673
Loss in iteration 22 : 0.07438289938880246
Loss in iteration 23 : 0.07177822437127024
Loss in iteration 24 : 0.06928014057022099
Loss in iteration 25 : 0.06688850350699065
Loss in iteration 26 : 0.0646010566175226
Loss in iteration 27 : 0.06244257283184505
Loss in iteration 28 : 0.06040104512847664
Loss in iteration 29 : 0.05849726026777411
Loss in iteration 30 : 0.05669192132245924
Loss in iteration 31 : 0.05501865499783076
Loss in iteration 32 : 0.05352812242405151
Loss in iteration 33 : 0.05214625917631481
Loss in iteration 34 : 0.050884215806385184
Loss in iteration 35 : 0.04975462074727202
Loss in iteration 36 : 0.048685838090506535
Loss in iteration 37 : 0.047664555081910524
Loss in iteration 38 : 0.04668986982771943
Loss in iteration 39 : 0.0457510346001822
Loss in iteration 40 : 0.04481759365544242
Loss in iteration 41 : 0.043897863547115064
Loss in iteration 42 : 0.04298732917651691
Loss in iteration 43 : 0.04207679976961035
Loss in iteration 44 : 0.04116504458984792
Loss in iteration 45 : 0.04024802211692122
Loss in iteration 46 : 0.03932830878900816
Loss in iteration 47 : 0.03842515621796039
Loss in iteration 48 : 0.03752239854584722
Loss in iteration 49 : 0.03661572165823601
Loss in iteration 50 : 0.035711235254049546
Loss in iteration 51 : 0.03483184286717081
Loss in iteration 52 : 0.03398385966501436
Loss in iteration 53 : 0.033153289541343195
Loss in iteration 54 : 0.03233744710412786
Loss in iteration 55 : 0.031533812229561496
Loss in iteration 56 : 0.03074630107635769
Loss in iteration 57 : 0.029986855535886474
Loss in iteration 58 : 0.029255607842706307
Loss in iteration 59 : 0.028549365871837793
Loss in iteration 60 : 0.027863277721287855
Loss in iteration 61 : 0.02720436989927178
Loss in iteration 62 : 0.026563533737114692
Loss in iteration 63 : 0.02594294223011263
Loss in iteration 64 : 0.025339423817171594
Loss in iteration 65 : 0.02474205256593059
Loss in iteration 66 : 0.024155757284313486
Loss in iteration 67 : 0.02357878547853001
Loss in iteration 68 : 0.02301038819846323
Loss in iteration 69 : 0.022447703890063325
Loss in iteration 70 : 0.021892914974108564
Loss in iteration 71 : 0.021348885528045917
Loss in iteration 72 : 0.02081177609964266
Loss in iteration 73 : 0.020278693945208124
Loss in iteration 74 : 0.019747959690005045
Loss in iteration 75 : 0.01921973450788083
Loss in iteration 76 : 0.018706806040295048
Loss in iteration 77 : 0.01821401018584906
Loss in iteration 78 : 0.017732239167726288
Loss in iteration 79 : 0.01727977077294704
Loss in iteration 80 : 0.01684586961713951
Loss in iteration 81 : 0.016440071184087933
Loss in iteration 82 : 0.016077068146492467
Loss in iteration 83 : 0.015728266561095463
Loss in iteration 84 : 0.015387048189247035
Loss in iteration 85 : 0.015088149177054687
Loss in iteration 86 : 0.01481582091675188
Loss in iteration 87 : 0.014547223697004744
Loss in iteration 88 : 0.014282903537456744
Loss in iteration 89 : 0.014038022590740183
Loss in iteration 90 : 0.013808833991862996
Loss in iteration 91 : 0.013583031184528053
Loss in iteration 92 : 0.013361764132173482
Loss in iteration 93 : 0.013150296002282192
Loss in iteration 94 : 0.01294036910101598
Loss in iteration 95 : 0.012731132633484344
Loss in iteration 96 : 0.012525510891616498
Loss in iteration 97 : 0.012327737085048487
Loss in iteration 98 : 0.01213558225747541
Loss in iteration 99 : 0.011948359238001513
Loss in iteration 100 : 0.011769782770946475
Loss in iteration 101 : 0.011594291761991947
Loss in iteration 102 : 0.011425187337923547
Loss in iteration 103 : 0.011258603548512885
Loss in iteration 104 : 0.011092711872909436
Loss in iteration 105 : 0.010932187431326155
Loss in iteration 106 : 0.010776465172202881
Loss in iteration 107 : 0.010623670700806578
Loss in iteration 108 : 0.010474745065116841
Loss in iteration 109 : 0.010330406525084735
Loss in iteration 110 : 0.010192111581647036
Loss in iteration 111 : 0.010055881881735777
Loss in iteration 112 : 0.009921321118068647
Loss in iteration 113 : 0.009787662145323032
Loss in iteration 114 : 0.009655794577904492
Loss in iteration 115 : 0.009526930432206263
Loss in iteration 116 : 0.00940105989512161
Loss in iteration 117 : 0.009276936590051662
Loss in iteration 118 : 0.00915597942527984
Loss in iteration 119 : 0.009035571075734156
Loss in iteration 120 : 0.008918015175029173
Loss in iteration 121 : 0.00880398365314547
Loss in iteration 122 : 0.008691898968200178
Loss in iteration 123 : 0.008581225304450796
Loss in iteration 124 : 0.008470702565919478
Loss in iteration 125 : 0.008360315660084423
Loss in iteration 126 : 0.008250051003676004
Loss in iteration 127 : 0.008140323077352652
Loss in iteration 128 : 0.008035818500329622
Loss in iteration 129 : 0.00793582493761191
Loss in iteration 130 : 0.007839504944739699
Loss in iteration 131 : 0.007745143811216537
Loss in iteration 132 : 0.007654988117649741
Loss in iteration 133 : 0.007566288600224411
Loss in iteration 134 : 0.007479891303340691
Loss in iteration 135 : 0.007394933993447391
Loss in iteration 136 : 0.007313694539417368
Loss in iteration 137 : 0.007237453481278517
Loss in iteration 138 : 0.007164530501836118
Loss in iteration 139 : 0.007094091583299026
Loss in iteration 140 : 0.0070252583171746515
Loss in iteration 141 : 0.006957577651278868
Loss in iteration 142 : 0.006890150778396067
Loss in iteration 143 : 0.00682342929706353
Loss in iteration 144 : 0.006757884487360596
Loss in iteration 145 : 0.006692576263203461
Loss in iteration 146 : 0.006627480966037546
Loss in iteration 147 : 0.006562777010742308
Loss in iteration 148 : 0.00650013240707528
Loss in iteration 149 : 0.006439261040545899
Loss in iteration 150 : 0.0063810029929073885
Loss in iteration 151 : 0.006324345405115342
Loss in iteration 152 : 0.006268498126858927
Loss in iteration 153 : 0.006213575558480577
Loss in iteration 154 : 0.006159741848350505
Loss in iteration 155 : 0.006107384257607432
Loss in iteration 156 : 0.006055564788048639
Loss in iteration 157 : 0.006005161948109247
Loss in iteration 158 : 0.005956036541854818
Loss in iteration 159 : 0.005907171841361713
Loss in iteration 160 : 0.005860444132471016
Loss in iteration 161 : 0.005813883590267063
Loss in iteration 162 : 0.0057676197297619036
Loss in iteration 163 : 0.005722743650354857
Loss in iteration 164 : 0.005677559546042563
Loss in iteration 165 : 0.005633832119758809
Loss in iteration 166 : 0.0055906021168324925
Loss in iteration 167 : 0.0055476661645549544
Loss in iteration 168 : 0.005505376956037937
Loss in iteration 169 : 0.0054626031335020635
Loss in iteration 170 : 0.005422352738587807
Loss in iteration 171 : 0.0053850267778344845
Loss in iteration 172 : 0.005347955051452127
Loss in iteration 173 : 0.005310584017952667
Loss in iteration 174 : 0.005275727944837336
Loss in iteration 175 : 0.005241144146324779
Loss in iteration 176 : 0.0052074016724407025
Loss in iteration 177 : 0.00517500791330561
Loss in iteration 178 : 0.005143055277673085
Loss in iteration 179 : 0.005111536281946084
Loss in iteration 180 : 0.005081493798206201
Loss in iteration 181 : 0.005052445331403622
Loss in iteration 182 : 0.005021873960091208
Loss in iteration 183 : 0.004992507257943416
Loss in iteration 184 : 0.004963646438199026
Loss in iteration 185 : 0.0049349317883335695
Loss in iteration 186 : 0.004906391228021577
Loss in iteration 187 : 0.00487807721093226
Loss in iteration 188 : 0.00485133535326685
Loss in iteration 189 : 0.004824243493034818
Loss in iteration 190 : 0.004797979693370201
Loss in iteration 191 : 0.004770962964576446
Loss in iteration 192 : 0.004745063795014154
Loss in iteration 193 : 0.004718935664636257
Loss in iteration 194 : 0.004693278537205697
Loss in iteration 195 : 0.004667659501282333
Loss in iteration 196 : 0.004642246658652961
Loss in iteration 197 : 0.004616955427095248
Loss in iteration 198 : 0.004591762913998534
Loss in iteration 199 : 0.0045665611032019255
Loss in iteration 200 : 0.004541768277653227
Testing accuracy  of updater 2 on alg 1 with rate 0.09999999999999998 = 0.9902222222222222, training accuracy 0.9987139182623607, time elapsed: 4759 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 12.19132066449727
Loss in iteration 3 : 1.524238975627231
Loss in iteration 4 : 0.8521449497021388
Loss in iteration 5 : 0.6222899706240723
Loss in iteration 6 : 0.2886561036142424
Loss in iteration 7 : 0.22040147692301784
Loss in iteration 8 : 0.17882326777300225
Loss in iteration 9 : 0.14770898226238685
Loss in iteration 10 : 0.12106209099250766
Loss in iteration 11 : 0.10135575890790359
Loss in iteration 12 : 0.08739482344860752
Loss in iteration 13 : 0.07718182284263249
Loss in iteration 14 : 0.06896821348951519
Loss in iteration 15 : 0.06198924145799381
Loss in iteration 16 : 0.056137692450405174
Loss in iteration 17 : 0.05158142141808707
Loss in iteration 18 : 0.04786620853054855
Loss in iteration 19 : 0.04490797851509921
Loss in iteration 20 : 0.04204480720392817
Loss in iteration 21 : 0.03963914508965118
Loss in iteration 22 : 0.0378447012653273
Loss in iteration 23 : 0.03624968510769803
Loss in iteration 24 : 0.03488602019737779
Loss in iteration 25 : 0.033527510257240774
Loss in iteration 26 : 0.03237511148393116
Loss in iteration 27 : 0.03143009821916349
Loss in iteration 28 : 0.030485187438791278
Loss in iteration 29 : 0.02954748569246493
Loss in iteration 30 : 0.028647984210952132
Loss in iteration 31 : 0.027818057166636413
Loss in iteration 32 : 0.026993366130226536
Loss in iteration 33 : 0.026162204180343457
Loss in iteration 34 : 0.025339037427231956
Loss in iteration 35 : 0.024514469569303535
Loss in iteration 36 : 0.02369539410488537
Loss in iteration 37 : 0.02285937595146411
Loss in iteration 38 : 0.02216279147075145
Loss in iteration 39 : 0.021579482149172286
Loss in iteration 40 : 0.021045200803849064
Loss in iteration 41 : 0.0205188433595718
Loss in iteration 42 : 0.019984764736762844
Loss in iteration 43 : 0.019479271399588174
Loss in iteration 44 : 0.01901742196203564
Loss in iteration 45 : 0.018596036056485184
Loss in iteration 46 : 0.018164301380707706
Loss in iteration 47 : 0.017884370286871447
Loss in iteration 48 : 0.01769156908587116
Loss in iteration 49 : 0.017338986258309836
Loss in iteration 50 : 0.01711760427117643
Loss in iteration 51 : 0.016888708625503745
Loss in iteration 52 : 0.01664932116178235
Loss in iteration 53 : 0.016450590675771405
Loss in iteration 54 : 0.01625322650741447
Loss in iteration 55 : 0.016036529945048558
Loss in iteration 56 : 0.015851383053329008
Loss in iteration 57 : 0.015697971186026775
Loss in iteration 58 : 0.015445534655430575
Loss in iteration 59 : 0.015246398935837312
Loss in iteration 60 : 0.01504092252078925
Loss in iteration 61 : 0.014834663009362169
Loss in iteration 62 : 0.014637711777123143
Loss in iteration 63 : 0.014453538246103334
Loss in iteration 64 : 0.014244143542348466
Loss in iteration 65 : 0.01403464674175117
Loss in iteration 66 : 0.013825245632413717
Loss in iteration 67 : 0.013639670054830362
Loss in iteration 68 : 0.013454895146456263
Loss in iteration 69 : 0.013238970462551715
Loss in iteration 70 : 0.013029505301726804
Loss in iteration 71 : 0.012828182629820457
Loss in iteration 72 : 0.012625399660541353
Loss in iteration 73 : 0.01243092845429653
Loss in iteration 74 : 0.012241648441249179
Loss in iteration 75 : 0.012038450987240143
Loss in iteration 76 : 0.011830520009540319
Loss in iteration 77 : 0.011626915628131024
Loss in iteration 78 : 0.011426133862053408
Loss in iteration 79 : 0.01121672549250357
Loss in iteration 80 : 0.01102838600234597
Loss in iteration 81 : 0.010847153673754866
Loss in iteration 82 : 0.01063709589967117
Loss in iteration 83 : 0.010431676037084859
Loss in iteration 84 : 0.010225648059212021
Loss in iteration 85 : 0.01002737754999571
Loss in iteration 86 : 0.00983297609176071
Loss in iteration 87 : 0.009676689469305068
Loss in iteration 88 : 0.009539396040309055
Loss in iteration 89 : 0.009429859600705984
Loss in iteration 90 : 0.009267013776648863
Loss in iteration 91 : 0.0090591417063044
Loss in iteration 92 : 0.008899200843243771
Loss in iteration 93 : 0.008794956831240108
Loss in iteration 94 : 0.00860571255145634
Loss in iteration 95 : 0.008419324904779896
Loss in iteration 96 : 0.008260915327904687
Loss in iteration 97 : 0.008118386285166413
Loss in iteration 98 : 0.007931078023000993
Loss in iteration 99 : 0.007777060979360808
Loss in iteration 100 : 0.007636681147410402
Loss in iteration 101 : 0.00746825039203857
Loss in iteration 102 : 0.007337236514214378
Loss in iteration 103 : 0.007161671547618326
Loss in iteration 104 : 0.00699748664155477
Loss in iteration 105 : 0.00684260525423104
Loss in iteration 106 : 0.006670009010581751
Loss in iteration 107 : 0.006519247592704774
Loss in iteration 108 : 0.006395677639307291
Loss in iteration 109 : 0.00619574969797077
Loss in iteration 110 : 0.006031249474147138
Loss in iteration 111 : 0.005886435639203545
Loss in iteration 112 : 0.00572503348470805
Loss in iteration 113 : 0.005589032612915948
Loss in iteration 114 : 0.005436375626597491
Loss in iteration 115 : 0.0052773206613282685
Loss in iteration 116 : 0.005108354182488529
Loss in iteration 117 : 0.0049471427309734865
Loss in iteration 118 : 0.004777305050109452
Loss in iteration 119 : 0.004627229565962221
Loss in iteration 120 : 0.004453928406108888
Loss in iteration 121 : 0.004300726568745993
Loss in iteration 122 : 0.004189996006730959
Loss in iteration 123 : 0.004061089335782835
Loss in iteration 124 : 0.003869171263668643
Loss in iteration 125 : 0.0036730877212477263
Loss in iteration 126 : 0.0035280564250425206
Loss in iteration 127 : 0.003387003363928538
Loss in iteration 128 : 0.003216495454108717
Loss in iteration 129 : 0.0030678560486130033
Loss in iteration 130 : 0.002894162626672141
Loss in iteration 131 : 0.002723477817115519
Loss in iteration 132 : 0.002579194721451823
Loss in iteration 133 : 0.0024443486897240607
Loss in iteration 134 : 0.002252928878126019
Loss in iteration 135 : 0.0020867559356844254
Loss in iteration 136 : 0.001962299837771554
Loss in iteration 137 : 0.001834494922651696
Loss in iteration 138 : 0.001720141240655849
Loss in iteration 139 : 0.0014949011847139998
Loss in iteration 140 : 0.0013059434736961532
Loss in iteration 141 : 0.0011948719222573752
Loss in iteration 142 : 0.0010398033020899925
Loss in iteration 143 : 8.500747372649835E-4
Loss in iteration 144 : 6.81761170513374E-4
Loss in iteration 145 : 5.297723320295748E-4
Testing accuracy  of updater 3 on alg 1 with rate 10.0 = 0.9928888888888889, training accuracy 0.9998571020291512, time elapsed: 3630 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7011598426875671
Loss in iteration 3 : 0.26413434028722
Loss in iteration 4 : 0.7298155868417944
Loss in iteration 5 : 0.09387583003551021
Loss in iteration 6 : 0.06385286575804772
Loss in iteration 7 : 0.045919563229844286
Loss in iteration 8 : 0.03442055076890586
Loss in iteration 9 : 0.026862878070999525
Loss in iteration 10 : 0.02217760244519393
Loss in iteration 11 : 0.019048183130214245
Loss in iteration 12 : 0.016569251452583866
Loss in iteration 13 : 0.014509154098602534
Loss in iteration 14 : 0.012848733007461166
Loss in iteration 15 : 0.011566671217925859
Loss in iteration 16 : 0.01039699404466268
Loss in iteration 17 : 0.009373324316472318
Loss in iteration 18 : 0.008548662419022246
Loss in iteration 19 : 0.007752320988754532
Loss in iteration 20 : 0.007007844109990771
Loss in iteration 21 : 0.006414015168604848
Loss in iteration 22 : 0.005905759108999285
Loss in iteration 23 : 0.005439894983307128
Loss in iteration 24 : 0.005019106538606329
Loss in iteration 25 : 0.004650573127255792
Loss in iteration 26 : 0.004353888347250722
Loss in iteration 27 : 0.004115803269472126
Loss in iteration 28 : 0.003882075846854341
Loss in iteration 29 : 0.0036636348469889658
Loss in iteration 30 : 0.0034929801958075834
Loss in iteration 31 : 0.003353765059508094
Loss in iteration 32 : 0.0032249612675006602
Loss in iteration 33 : 0.003102570727498142
Loss in iteration 34 : 0.002994760401524059
Loss in iteration 35 : 0.002893228905573015
Loss in iteration 36 : 0.002782941967004644
Loss in iteration 37 : 0.0026821844718983422
Loss in iteration 38 : 0.0026013543300689914
Loss in iteration 39 : 0.002530183284967113
Loss in iteration 40 : 0.002447558712467418
Loss in iteration 41 : 0.002387803161107907
Loss in iteration 42 : 0.0023276444538000373
Loss in iteration 43 : 0.0022714354765387018
Loss in iteration 44 : 0.002218626198066837
Loss in iteration 45 : 0.0021739546150387388
Loss in iteration 46 : 0.0021319785147742272
Loss in iteration 47 : 0.002093466092933013
Loss in iteration 48 : 0.002059391087493054
Loss in iteration 49 : 0.0020147050978817876
Loss in iteration 50 : 0.001976949201817325
Loss in iteration 51 : 0.001942093062041859
Loss in iteration 52 : 0.001913790700198425
Loss in iteration 53 : 0.0018853134030458868
Loss in iteration 54 : 0.0018340575722534066
Loss in iteration 55 : 0.0017996255016014089
Loss in iteration 56 : 0.0017685984027052376
Loss in iteration 57 : 0.0017375274866978394
Loss in iteration 58 : 0.001716726224917379
Loss in iteration 59 : 0.0017149512281069872
Loss in iteration 60 : 0.0016974724537395206
Loss in iteration 61 : 0.0016715419898116893
Loss in iteration 62 : 0.0016367778542000083
Loss in iteration 63 : 0.0016180038405665725
Loss in iteration 64 : 0.0015984409547821902
Loss in iteration 65 : 0.0015804920715919045
Loss in iteration 66 : 0.0015628863724387254
Loss in iteration 67 : 0.0015434612090964139
Loss in iteration 68 : 0.0015255944543476232
Loss in iteration 69 : 0.0015081143783166149
Loss in iteration 70 : 0.0014941764684484567
Loss in iteration 71 : 0.0014744447439599595
Loss in iteration 72 : 0.0014591836675473461
Loss in iteration 73 : 0.0014403023890161477
Loss in iteration 74 : 0.001426892438030569
Loss in iteration 75 : 0.0014135908120939827
Loss in iteration 76 : 0.0014036044955169466
Loss in iteration 77 : 0.0014125440774604603
Loss in iteration 78 : 0.0013824947749362265
Loss in iteration 79 : 0.0013737210603607128
Loss in iteration 80 : 0.001356232858907351
Loss in iteration 81 : 0.001342900046417885
Loss in iteration 82 : 0.0013295674332287528
Loss in iteration 83 : 0.0013162350193208151
Loss in iteration 84 : 0.0013033336388984915
Loss in iteration 85 : 0.0012903861360464386
Loss in iteration 86 : 0.0012772967187859397
Loss in iteration 87 : 0.0012659163376039237
Loss in iteration 88 : 0.001252568967996851
Loss in iteration 89 : 0.001241931606566146
Loss in iteration 90 : 0.0012298440756741168
Loss in iteration 91 : 0.0012208578412899026
Loss in iteration 92 : 0.0012046060061300652
Loss in iteration 93 : 0.0011912755474662552
Loss in iteration 94 : 0.001178014317959202
Loss in iteration 95 : 0.0011664108792312684
Loss in iteration 96 : 0.0011532797520592854
Loss in iteration 97 : 0.0011441198880528343
Loss in iteration 98 : 0.0011317583653915092
Loss in iteration 99 : 0.0011196664338645205
Loss in iteration 100 : 0.0011059305362759533
Loss in iteration 101 : 0.0010933511782810867
Loss in iteration 102 : 0.0010800224760619728
Loss in iteration 103 : 0.0010669657034047028
Loss in iteration 104 : 0.0010569057554626249
Loss in iteration 105 : 0.0010432876031825767
Loss in iteration 106 : 0.0010309659106166773
Loss in iteration 107 : 0.0010176381676806822
Loss in iteration 108 : 0.0010043786554237066
Loss in iteration 109 : 9.937963934022905E-4
Loss in iteration 110 : 9.820324804160432E-4
Loss in iteration 111 : 9.729491550535139E-4
Loss in iteration 112 : 9.563950847934936E-4
Loss in iteration 113 : 9.430685029340031E-4
Loss in iteration 114 : 9.302757333404167E-4
Loss in iteration 115 : 9.187540946571848E-4
Loss in iteration 116 : 9.073507696253434E-4
Loss in iteration 117 : 9.038015424505987E-4
Loss in iteration 118 : 9.060906474036827E-4
Loss in iteration 119 : 8.818810542941232E-4
Loss in iteration 120 : 8.760352878229145E-4
Loss in iteration 121 : 8.574637752933787E-4
Loss in iteration 122 : 8.429111014629822E-4
Loss in iteration 123 : 8.299692541035796E-4
Loss in iteration 124 : 8.176591687568424E-4
Loss in iteration 125 : 8.07532333841884E-4
Loss in iteration 126 : 7.992406878349932E-4
Loss in iteration 127 : 8.198492854233082E-4
Loss in iteration 128 : 7.785842776889286E-4
Loss in iteration 129 : 7.646714545641842E-4
Loss in iteration 130 : 7.523215187634503E-4
Loss in iteration 131 : 7.415432982192078E-4
Loss in iteration 132 : 7.31183972400827E-4
Loss in iteration 133 : 7.186161218994171E-4
Loss in iteration 134 : 7.248044899454258E-4
Loss in iteration 135 : 8.083709828670703E-4
Loss in iteration 136 : 6.984474408844581E-4
Loss in iteration 137 : 6.842355805937532E-4
Loss in iteration 138 : 6.723408573475132E-4
Loss in iteration 139 : 6.606960454844654E-4
Loss in iteration 140 : 6.498404212703332E-4
Loss in iteration 141 : 6.487910318968283E-4
Loss in iteration 142 : 6.356936501537244E-4
Loss in iteration 143 : 6.499948359141008E-4
Loss in iteration 144 : 6.178888988405736E-4
Loss in iteration 145 : 6.45668723133211E-4
Loss in iteration 146 : 5.924607128829003E-4
Loss in iteration 147 : 5.804768855323979E-4
Loss in iteration 148 : 5.686743696460307E-4
Loss in iteration 149 : 5.588442268011613E-4
Loss in iteration 150 : 5.516560679099544E-4
Loss in iteration 151 : 5.736204459603833E-4
Loss in iteration 152 : 5.329044885552793E-4
Loss in iteration 153 : 5.238647121197541E-4
Loss in iteration 154 : 5.057764605104479E-4
Loss in iteration 155 : 4.932698140981597E-4
Loss in iteration 156 : 4.894023174360761E-4
Loss in iteration 157 : 4.933685060963998E-4
Loss in iteration 158 : 4.8135979688612387E-4
Loss in iteration 159 : 5.39378973696198E-4
Loss in iteration 160 : 4.5600483985486174E-4
Loss in iteration 161 : 4.464062653909532E-4
Loss in iteration 162 : 4.292448825815562E-4
Loss in iteration 163 : 4.189339550763949E-4
Loss in iteration 164 : 4.1529910675284603E-4
Loss in iteration 165 : 4.2669022185444955E-4
Loss in iteration 166 : 3.933873677223401E-4
Loss in iteration 167 : 4.04336990662087E-4
Loss in iteration 168 : 3.700845590164341E-4
Loss in iteration 169 : 3.568316755673975E-4
Loss in iteration 170 : 3.441207084626397E-4
Loss in iteration 171 : 3.3287732910902E-4
Loss in iteration 172 : 3.2884158372764816E-4
Loss in iteration 173 : 3.4063785681912875E-4
Loss in iteration 174 : 3.581800855778991E-4
Loss in iteration 175 : 4.2915326738247124E-4
Loss in iteration 176 : 2.986358494019964E-4
Loss in iteration 177 : 2.8865069470319487E-4
Loss in iteration 178 : 2.8250528322589966E-4
Loss in iteration 179 : 2.801566480087233E-4
Loss in iteration 180 : 3.2880249434956633E-4
Loss in iteration 181 : 2.539602909244356E-4
Loss in iteration 182 : 2.4461506576279246E-4
Loss in iteration 183 : 2.3591756955832136E-4
Loss in iteration 184 : 2.3099541825391756E-4
Loss in iteration 185 : 2.2415771847515917E-4
Loss in iteration 186 : 2.377763876978187E-4
Loss in iteration 187 : 2.2873431919986868E-4
Loss in iteration 188 : 3.03362155334332E-4
Loss in iteration 189 : 2.002646544257511E-4
Loss in iteration 190 : 1.966321860597858E-4
Loss in iteration 191 : 1.8156569919309528E-4
Loss in iteration 192 : 1.714503959240181E-4
Loss in iteration 193 : 1.5741649734069484E-4
Loss in iteration 194 : 1.6171711615071618E-4
Loss in iteration 195 : 1.5589926476211222E-4
Loss in iteration 196 : 1.8563250241722943E-4
Loss in iteration 197 : 1.3120914016733357E-4
Testing accuracy  of updater 3 on alg 1 with rate 1.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 4061 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6083366053928189
Loss in iteration 3 : 0.36566545066876466
Loss in iteration 4 : 0.4336181328826235
Loss in iteration 5 : 0.25453968775436203
Loss in iteration 6 : 0.2079822777603907
Loss in iteration 7 : 0.19063094288023918
Loss in iteration 8 : 0.16522380935686967
Loss in iteration 9 : 0.15455163816218734
Loss in iteration 10 : 0.14461997315442401
Loss in iteration 11 : 0.13525259979214307
Loss in iteration 12 : 0.1263847127948202
Loss in iteration 13 : 0.11801142137591238
Loss in iteration 14 : 0.11009280077034313
Loss in iteration 15 : 0.10271645204590399
Loss in iteration 16 : 0.09600447059627915
Loss in iteration 17 : 0.09002081911447994
Loss in iteration 18 : 0.0847024969646227
Loss in iteration 19 : 0.07997820548908612
Loss in iteration 20 : 0.07579220000834311
Loss in iteration 21 : 0.07194682984777351
Loss in iteration 22 : 0.06851476536388386
Loss in iteration 23 : 0.06540863397877818
Loss in iteration 24 : 0.06256156695671758
Loss in iteration 25 : 0.0599436678283133
Loss in iteration 26 : 0.05755398758324664
Loss in iteration 27 : 0.05537583882078813
Loss in iteration 28 : 0.053338845717610654
Loss in iteration 29 : 0.051424630339844
Loss in iteration 30 : 0.0496272612712901
Loss in iteration 31 : 0.04792015872583034
Loss in iteration 32 : 0.046336701960685574
Loss in iteration 33 : 0.044890406846869645
Loss in iteration 34 : 0.04354823374336019
Loss in iteration 35 : 0.042256770799417975
Loss in iteration 36 : 0.04100725474762061
Loss in iteration 37 : 0.039816850130350245
Loss in iteration 38 : 0.03868947951329571
Loss in iteration 39 : 0.037606750906600796
Loss in iteration 40 : 0.036574836468286936
Loss in iteration 41 : 0.0356017870130748
Loss in iteration 42 : 0.03467645182419445
Loss in iteration 43 : 0.03378000032143208
Loss in iteration 44 : 0.03290524353651575
Loss in iteration 45 : 0.03207108624597007
Loss in iteration 46 : 0.03127585156775754
Loss in iteration 47 : 0.03051931399893335
Loss in iteration 48 : 0.029790505717514168
Loss in iteration 49 : 0.02909438873556981
Loss in iteration 50 : 0.028427694117732258
Loss in iteration 51 : 0.02779383906967963
Loss in iteration 52 : 0.027186157216583486
Loss in iteration 53 : 0.02659756267201089
Loss in iteration 54 : 0.026034266402715344
Loss in iteration 55 : 0.025496542602613814
Loss in iteration 56 : 0.024979453142331343
Loss in iteration 57 : 0.024484351000208522
Loss in iteration 58 : 0.024006059240271178
Loss in iteration 59 : 0.023544507493971717
Loss in iteration 60 : 0.023098890294180222
Loss in iteration 61 : 0.022670865521602586
Loss in iteration 62 : 0.022265046002764955
Loss in iteration 63 : 0.021878940947391037
Loss in iteration 64 : 0.021502462938225917
Loss in iteration 65 : 0.021135728835565165
Loss in iteration 66 : 0.020779722382364182
Loss in iteration 67 : 0.020433179584939096
Loss in iteration 68 : 0.020096557309974273
Loss in iteration 69 : 0.019772925332958418
Loss in iteration 70 : 0.019459271169134578
Loss in iteration 71 : 0.01915310096614718
Loss in iteration 72 : 0.01884958435899357
Loss in iteration 73 : 0.018552351212902454
Loss in iteration 74 : 0.018262799931090295
Loss in iteration 75 : 0.01797689660755935
Loss in iteration 76 : 0.01769558099715913
Loss in iteration 77 : 0.017421572474548265
Loss in iteration 78 : 0.017155642500916328
Loss in iteration 79 : 0.016895656290200275
Loss in iteration 80 : 0.01664113831537925
Loss in iteration 81 : 0.016388341338129518
Loss in iteration 82 : 0.016139432427923716
Loss in iteration 83 : 0.015895876775119056
Loss in iteration 84 : 0.015657761787048294
Loss in iteration 85 : 0.015430014477973215
Loss in iteration 86 : 0.015213143052773489
Loss in iteration 87 : 0.014999693171849281
Loss in iteration 88 : 0.01478990044603062
Loss in iteration 89 : 0.014583228100698077
Loss in iteration 90 : 0.01438073754148591
Loss in iteration 91 : 0.014181100553511595
Loss in iteration 92 : 0.013988249158789741
Loss in iteration 93 : 0.01380445995110821
Loss in iteration 94 : 0.013627349680762572
Loss in iteration 95 : 0.013459442083292042
Loss in iteration 96 : 0.013298945741472896
Loss in iteration 97 : 0.013143175111317237
Loss in iteration 98 : 0.012992170378341245
Loss in iteration 99 : 0.012845011823622392
Loss in iteration 100 : 0.012700557954148326
Loss in iteration 101 : 0.012557733040564628
Loss in iteration 102 : 0.012420427001754792
Loss in iteration 103 : 0.012288276103297786
Loss in iteration 104 : 0.01216196864001929
Loss in iteration 105 : 0.012039077153196272
Loss in iteration 106 : 0.011921023827662133
Loss in iteration 107 : 0.011809639194894389
Loss in iteration 108 : 0.01170070891558803
Loss in iteration 109 : 0.01159845727662547
Loss in iteration 110 : 0.011483993355190222
Loss in iteration 111 : 0.01137801683625069
Loss in iteration 112 : 0.011275510769976897
Loss in iteration 113 : 0.011173219203316487
Loss in iteration 114 : 0.011075115834900002
Loss in iteration 115 : 0.010976392732376675
Loss in iteration 116 : 0.010881557912578346
Loss in iteration 117 : 0.01078906027215779
Loss in iteration 118 : 0.010696892559721673
Loss in iteration 119 : 0.010607481487871254
Loss in iteration 120 : 0.010516730580093083
Loss in iteration 121 : 0.010430590532382598
Loss in iteration 122 : 0.010344483414101715
Loss in iteration 123 : 0.010272668975587628
Loss in iteration 124 : 0.01019069927055612
Loss in iteration 125 : 0.01011704475348233
Loss in iteration 126 : 0.010021013384258927
Loss in iteration 127 : 0.009935338329218675
Loss in iteration 128 : 0.009857224868398268
Loss in iteration 129 : 0.009780464579946684
Loss in iteration 130 : 0.009703454662269965
Loss in iteration 131 : 0.009627360533210868
Loss in iteration 132 : 0.00955494832114059
Loss in iteration 133 : 0.009489067925402796
Loss in iteration 134 : 0.009442389653082478
Loss in iteration 135 : 0.009357736901149402
Loss in iteration 136 : 0.009295061235396175
Loss in iteration 137 : 0.009212362215637811
Loss in iteration 138 : 0.009149417700185795
Loss in iteration 139 : 0.009072121601094364
Loss in iteration 140 : 0.009007836261259079
Loss in iteration 141 : 0.008945483669902487
Loss in iteration 142 : 0.008886538900358372
Loss in iteration 143 : 0.008813205591771945
Loss in iteration 144 : 0.008752398990352663
Loss in iteration 145 : 0.008684055631434724
Loss in iteration 146 : 0.008622781863698234
Loss in iteration 147 : 0.008553893220433453
Loss in iteration 148 : 0.008491221457249459
Loss in iteration 149 : 0.008425942786665886
Loss in iteration 150 : 0.008366295726575903
Loss in iteration 151 : 0.008312216580618538
Loss in iteration 152 : 0.008279877729302781
Loss in iteration 153 : 0.008198755166049081
Loss in iteration 154 : 0.008145079250299848
Loss in iteration 155 : 0.008074580810241302
Loss in iteration 156 : 0.008029097783903039
Loss in iteration 157 : 0.007960678684504698
Loss in iteration 158 : 0.007907795037394172
Loss in iteration 159 : 0.00784112042527779
Loss in iteration 160 : 0.00778481733427581
Loss in iteration 161 : 0.007720995841162843
Loss in iteration 162 : 0.007661323479352025
Loss in iteration 163 : 0.007604681776418868
Loss in iteration 164 : 0.007549478322410706
Loss in iteration 165 : 0.007495357596800599
Loss in iteration 166 : 0.0074404617582694565
Loss in iteration 167 : 0.007384599543864103
Loss in iteration 168 : 0.007332261573861704
Loss in iteration 169 : 0.007278901999849374
Loss in iteration 170 : 0.007240259837369906
Loss in iteration 171 : 0.007194364801498553
Loss in iteration 172 : 0.007155357493158961
Loss in iteration 173 : 0.007097126886351951
Loss in iteration 174 : 0.007038182540720885
Loss in iteration 175 : 0.006969332961474032
Loss in iteration 176 : 0.006910174946351884
Loss in iteration 177 : 0.006854128641191237
Loss in iteration 178 : 0.00680083835543239
Loss in iteration 179 : 0.006748409024536472
Loss in iteration 180 : 0.006696985533749581
Loss in iteration 181 : 0.0066453047931014485
Loss in iteration 182 : 0.006596058874786875
Loss in iteration 183 : 0.0065474211586783
Loss in iteration 184 : 0.006504540348615235
Loss in iteration 185 : 0.006450212516722654
Loss in iteration 186 : 0.006413887697651119
Loss in iteration 187 : 0.0063591434371684805
Loss in iteration 188 : 0.006317774613802305
Loss in iteration 189 : 0.00626233138542362
Loss in iteration 190 : 0.006223519926527257
Loss in iteration 191 : 0.006170864961935976
Loss in iteration 192 : 0.006132435655301562
Loss in iteration 193 : 0.006081158314557329
Loss in iteration 194 : 0.006042323964792183
Loss in iteration 195 : 0.005986313410590295
Loss in iteration 196 : 0.005943818039394187
Loss in iteration 197 : 0.005899638705570207
Loss in iteration 198 : 0.005863103014829423
Loss in iteration 199 : 0.005813083474999847
Loss in iteration 200 : 0.005771846600849
Testing accuracy  of updater 3 on alg 1 with rate 0.09999999999999998 = 0.9982222222222222, training accuracy 0.9995713060874536, time elapsed: 4454 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845752
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486289
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614217
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311626
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.9341560838258172
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310218
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543957
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694353
Loss in iteration 37 : 0.8984001208826744
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149773
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830087
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987295
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743325
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.819301480225692
Loss in iteration 63 : 0.8160147255223987
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712587
Loss in iteration 67 : 0.8027762269894211
Loss in iteration 68 : 0.7994438633952063
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048011
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505614
Loss in iteration 86 : 0.7379381485702151
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134936
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166519
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256584
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.662655282008631
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407875
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934181
Loss in iteration 118 : 0.6217649050146481
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423256
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265787
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234753
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.43866690280212967
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614924984
Loss in iteration 179 : 0.4224809016575409
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.4094159654317907
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.3995523073557874
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.39295876891020853
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.3832241331850308
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 4849 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614217
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468237
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540818
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778983
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807446
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.9312352176477111
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487195
Loss in iteration 31 : 0.9164678049543957
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694353
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830087
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160542
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987295
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712588
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454701
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753548
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256582
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372487
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451954
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249592
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748412
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562493
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986777
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036678
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234755
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.49874963817009316
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550484
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.4578840198263232
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.4450968060893727
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.4094159654317907
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.3995523073557874
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.39295876891020853
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.3832241331850308
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 4341 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433591
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.937065804358194
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.9312352176477111
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826744
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793263
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209122
Loss in iteration 51 : 0.8548422260830087
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142405
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987295
Loss in iteration 59 : 0.8291064249630267
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.819301480225692
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626255
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134936
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001563
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955337
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.6404730705088589
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934181
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782388
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954033
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892694
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979853
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296424
Loss in iteration 149 : 0.5173760647036678
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234752
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.5018535958418039
Loss in iteration 155 : 0.49874963817009316
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.48627209050369236
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472434
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550484
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.454696771363955
Loss in iteration 170 : 0.4515028637989074
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.4450968060893727
Loss in iteration 173 : 0.4418848318575792
Loss in iteration 174 : 0.43866690280212967
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.4257311461492499
Loss in iteration 179 : 0.4224809016575409
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.402844836490377
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.3962532440560076
Loss in iteration 188 : 0.39295876891020853
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.3832241331850308
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.3736917204558962
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 1.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 3763 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 38.18326792354314
Loss in iteration 3 : 5.079728485450062
Loss in iteration 4 : 2.946146465549762
Loss in iteration 5 : 2.1044577315653963
Loss in iteration 6 : 0.7799164036211272
Loss in iteration 7 : 0.5699182088368433
Loss in iteration 8 : 0.4462818695204205
Loss in iteration 9 : 0.34939422068906806
Loss in iteration 10 : 0.2832451780488801
Loss in iteration 11 : 0.23778927929851915
Loss in iteration 12 : 0.19593680183615045
Loss in iteration 13 : 0.1620469155359943
Loss in iteration 14 : 0.13976373967471814
Loss in iteration 15 : 0.12082333416826768
Loss in iteration 16 : 0.10792992261350604
Loss in iteration 17 : 0.09903413973216678
Loss in iteration 18 : 0.09189422450647987
Loss in iteration 19 : 0.0844948211586036
Loss in iteration 20 : 0.07798961012246121
Loss in iteration 21 : 0.07165202603698709
Loss in iteration 22 : 0.06578740824873971
Loss in iteration 23 : 0.06097753180105627
Loss in iteration 24 : 0.05681741528144134
Loss in iteration 25 : 0.053625675070608045
Loss in iteration 26 : 0.05255785908309639
Loss in iteration 27 : 0.04936480314128857
Loss in iteration 28 : 0.046888910548757046
Loss in iteration 29 : 0.043973378831628204
Loss in iteration 30 : 0.0413800254902825
Loss in iteration 31 : 0.038492479914813436
Loss in iteration 32 : 0.0356262260555147
Loss in iteration 33 : 0.03297528522552146
Loss in iteration 34 : 0.03634297769035232
Loss in iteration 35 : 0.03495089983004165
Loss in iteration 36 : 0.03403793504265499
Loss in iteration 37 : 0.0234332141286683
Loss in iteration 38 : 0.018538098435349565
Loss in iteration 39 : 0.013962566448182034
Loss in iteration 40 : 0.010994041001918385
Loss in iteration 41 : 0.006905532478647497
Loss in iteration 42 : 0.009349829850456926
Loss in iteration 43 : 0.021447766525152102
Loss in iteration 44 : 0.09168863711633769
Loss in iteration 45 : 8.527944443018558
Loss in iteration 46 : 115.76986114266849
Loss in iteration 47 : 5.483335576848177
Loss in iteration 48 : 0.08326299540770908
Loss in iteration 49 : 0.06535402598090259
Loss in iteration 50 : 0.050881131278821906
Loss in iteration 51 : 0.044579484170048096
Loss in iteration 52 : 0.03966943746701989
Loss in iteration 53 : 0.03480829385038508
Loss in iteration 54 : 0.031595658435527266
Loss in iteration 55 : 0.02959746178941851
Loss in iteration 56 : 0.02749297490801513
Loss in iteration 57 : 0.02527674373295737
Loss in iteration 58 : 0.022943066242160845
Loss in iteration 59 : 0.020485986105300173
Loss in iteration 60 : 0.017899286938754092
Loss in iteration 61 : 0.015176487285471916
Loss in iteration 62 : 0.01231083645209262
Loss in iteration 63 : 0.009410273140446361
Loss in iteration 64 : 0.006352902190668533
Loss in iteration 65 : 0.0039253048008166406
Loss in iteration 66 : 0.002809207131388662
Loss in iteration 67 : 0.0018814736840005513
Loss in iteration 68 : 0.0013913883601622673
Loss in iteration 69 : 9.332803942859048E-4
Loss in iteration 70 : 4.895292675555268E-4
Loss in iteration 71 : 2.2662881067823308E-5
Loss in iteration 72 : 4.0487985198968363E-4
Loss in iteration 73 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 10.0 = 0.9982222222222222, training accuracy 1.0, time elapsed: 1431 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.4937723850902334
Loss in iteration 3 : 0.47289310707808674
Loss in iteration 4 : 0.3338520020846372
Loss in iteration 5 : 0.22550228629114663
Loss in iteration 6 : 0.075768517508738
Loss in iteration 7 : 0.05485603735781666
Loss in iteration 8 : 0.03922728825825357
Loss in iteration 9 : 0.030682284502842723
Loss in iteration 10 : 0.024282910569257354
Loss in iteration 11 : 0.02022239111178669
Loss in iteration 12 : 0.017095788648187133
Loss in iteration 13 : 0.01458743927426655
Loss in iteration 14 : 0.012798144396898145
Loss in iteration 15 : 0.011441405441812609
Loss in iteration 16 : 0.010212072918210276
Loss in iteration 17 : 0.009438341475118005
Loss in iteration 18 : 0.0086473764436798
Loss in iteration 19 : 0.007943935072154004
Loss in iteration 20 : 0.007283191064882206
Loss in iteration 21 : 0.006632090806337669
Loss in iteration 22 : 0.005997958976973922
Loss in iteration 23 : 0.005507023916401445
Loss in iteration 24 : 0.0050461993614036695
Loss in iteration 25 : 0.0048811595430511435
Loss in iteration 26 : 0.004776572514870389
Loss in iteration 27 : 0.004626976188703535
Loss in iteration 28 : 0.003879949038081908
Loss in iteration 29 : 0.0033826367224573614
Loss in iteration 30 : 0.0031163706654991344
Loss in iteration 31 : 0.0028426255532567205
Loss in iteration 32 : 0.0026643019624888508
Loss in iteration 33 : 0.002358688741022823
Loss in iteration 34 : 0.002192180488628188
Loss in iteration 35 : 0.0018434438979060886
Loss in iteration 36 : 0.0012971604454054357
Loss in iteration 37 : 8.667517215166189E-4
Loss in iteration 38 : 6.285852234070797E-4
Loss in iteration 39 : 2.277815501786263E-4
Loss in iteration 40 : 3.534359024975345E-4
Loss in iteration 41 : 0.001831802667831089
Loss in iteration 42 : 0.008473211233951186
Loss in iteration 43 : 0.12547510052375527
Loss in iteration 44 : 10.325615520768348
Loss in iteration 45 : 2.217208192787088
Loss in iteration 46 : 0.02909464201038031
Loss in iteration 47 : 0.01595882089625291
Loss in iteration 48 : 0.007983934485088818
Loss in iteration 49 : 0.005081871126626207
Loss in iteration 50 : 0.004090952376236628
Loss in iteration 51 : 0.0031645208911953585
Loss in iteration 52 : 0.002588851908810331
Loss in iteration 53 : 0.002244787109208902
Loss in iteration 54 : 0.0018856480133100856
Loss in iteration 55 : 0.0017682268564158084
Loss in iteration 56 : 0.0016432464605709042
Loss in iteration 57 : 0.0015115265548172223
Loss in iteration 58 : 0.001373236313901394
Loss in iteration 59 : 0.0012452056066805548
Loss in iteration 60 : 0.0011694906376654452
Loss in iteration 61 : 0.0010714597496208525
Loss in iteration 62 : 9.700387160307484E-4
Loss in iteration 63 : 8.631471048265627E-4
Loss in iteration 64 : 7.504916435311427E-4
Loss in iteration 65 : 6.317636437448693E-4
Loss in iteration 66 : 5.308240391476103E-4
Loss in iteration 67 : 4.43394295511283E-4
Loss in iteration 68 : 3.104376848309426E-4
Loss in iteration 69 : 1.7479261407478161E-4
Loss in iteration 70 : 1.1530083986571159E-4
Loss in iteration 71 : 6.962632481390352E-5
Loss in iteration 72 : 3.907095292599031E-5
Loss in iteration 73 : 7.055255573072541E-6
Loss in iteration 74 : 1.7894700048833062E-5
Loss in iteration 75 : 2.5815580104172492E-5
Loss in iteration 76 : 6.275507628225779E-5
Loss in iteration 77 : 1.163378297831765E-5
Loss in iteration 78 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 1.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 1444 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4268109242858221
Loss in iteration 3 : 0.392879136524169
Loss in iteration 4 : 0.509872700417326
Loss in iteration 5 : 0.13930338729976316
Loss in iteration 6 : 0.06914568663761078
Loss in iteration 7 : 0.05420635018945935
Loss in iteration 8 : 0.04364541889238796
Loss in iteration 9 : 0.03518267651360039
Loss in iteration 10 : 0.028669875852142285
Loss in iteration 11 : 0.024266859583196604
Loss in iteration 12 : 0.020722146676488553
Loss in iteration 13 : 0.018089424175215484
Loss in iteration 14 : 0.016117156759581988
Loss in iteration 15 : 0.014468542247643537
Loss in iteration 16 : 0.013117058982891724
Loss in iteration 17 : 0.011888489581809117
Loss in iteration 18 : 0.01150283366278113
Loss in iteration 19 : 0.010120920262606432
Loss in iteration 20 : 0.009873475074213389
Loss in iteration 21 : 0.008962228148469182
Loss in iteration 22 : 0.008243422307314868
Loss in iteration 23 : 0.007848159096543617
Loss in iteration 24 : 0.00710585989710755
Loss in iteration 25 : 0.0063175521305708915
Loss in iteration 26 : 0.006116801842102133
Loss in iteration 27 : 0.005115652730928455
Loss in iteration 28 : 0.004273201876220603
Loss in iteration 29 : 0.0035458251740680432
Loss in iteration 30 : 0.002643947861448248
Loss in iteration 31 : 0.0023098567418228535
Loss in iteration 32 : 0.002068375325263412
Loss in iteration 33 : 0.002044087073563761
Loss in iteration 34 : 0.0018708078760505979
Loss in iteration 35 : 0.002234890923366909
Loss in iteration 36 : 0.0033743001946973535
Loss in iteration 37 : 0.018356252701572456
Loss in iteration 38 : 0.19525878952083137
Loss in iteration 39 : 0.41098591304233345
Loss in iteration 40 : 0.1664559373673012
Loss in iteration 41 : 0.007395167864666779
Loss in iteration 42 : 0.003745626814953144
Loss in iteration 43 : 0.0028674688902402246
Loss in iteration 44 : 0.002595125045387583
Loss in iteration 45 : 0.0023880074678767683
Loss in iteration 46 : 0.0022366067491201972
Loss in iteration 47 : 0.002094199971638838
Loss in iteration 48 : 0.0019661514492834006
Loss in iteration 49 : 0.0018252508296529597
Loss in iteration 50 : 0.0017624595522705761
Loss in iteration 51 : 0.0016740298954579646
Loss in iteration 52 : 0.0016048794479790011
Loss in iteration 53 : 0.0015507067862170047
Loss in iteration 54 : 0.001511219925583465
Loss in iteration 55 : 0.0014715048707600823
Loss in iteration 56 : 0.001433132274950981
Loss in iteration 57 : 0.0014147392572558771
Loss in iteration 58 : 0.0014084974873688385
Loss in iteration 59 : 0.0013694523543699283
Loss in iteration 60 : 0.0013410140207313658
Loss in iteration 61 : 0.0013153350458132052
Loss in iteration 62 : 0.001286109621007539
Loss in iteration 63 : 0.001258923422356032
Loss in iteration 64 : 0.0012372831598715002
Loss in iteration 65 : 0.0012193758751345212
Loss in iteration 66 : 0.0011837964526224942
Loss in iteration 67 : 0.0011703965343367933
Loss in iteration 68 : 0.0012052184009252096
Loss in iteration 69 : 0.001220864439032367
Loss in iteration 70 : 0.0012382097978092172
Loss in iteration 71 : 0.0010476352638851482
Loss in iteration 72 : 0.0010001182557514978
Loss in iteration 73 : 9.612020607771568E-4
Loss in iteration 74 : 9.017580133069141E-4
Loss in iteration 75 : 8.66715325962799E-4
Loss in iteration 76 : 8.730375945243977E-4
Loss in iteration 77 : 7.695899081989841E-4
Loss in iteration 78 : 7.271359975672513E-4
Loss in iteration 79 : 7.126713033094526E-4
Loss in iteration 80 : 6.646311523275052E-4
Loss in iteration 81 : 7.466726025941963E-4
Loss in iteration 82 : 6.223080845320089E-4
Loss in iteration 83 : 6.523128567642143E-4
Loss in iteration 84 : 4.89000274484833E-4
Loss in iteration 85 : 4.333393375231888E-4
Loss in iteration 86 : 4.2623297821438256E-4
Loss in iteration 87 : 3.1825019051510853E-4
Loss in iteration 88 : 2.8457916718935305E-4
Loss in iteration 89 : 2.3170840169565505E-4
Loss in iteration 90 : 2.305316155233053E-4
Loss in iteration 91 : 0.0010021433802156672
Loss in iteration 92 : 0.0010690183741218664
Loss in iteration 93 : 0.006011874471242654
Loss in iteration 94 : 0.11782712176699275
Loss in iteration 95 : 0.3305184905872858
Loss in iteration 96 : 0.013039096228537068
Loss in iteration 97 : 0.005296804111681773
Loss in iteration 98 : 0.0027101669340286374
Loss in iteration 99 : 0.0014756105934726415
Loss in iteration 100 : 8.678857290886141E-4
Loss in iteration 101 : 7.384228353403312E-4
Loss in iteration 102 : 6.431793241368084E-4
Loss in iteration 103 : 5.428536813491453E-4
Loss in iteration 104 : 4.440181576800283E-4
Loss in iteration 105 : 3.9209945283754695E-4
Loss in iteration 106 : 3.536952933242272E-4
Loss in iteration 107 : 3.246807994578978E-4
Loss in iteration 108 : 2.943646757729989E-4
Loss in iteration 109 : 2.624209465211453E-4
Loss in iteration 110 : 2.2892137537317778E-4
Loss in iteration 111 : 1.9388726956792311E-4
Loss in iteration 112 : 1.6043171210494684E-4
Loss in iteration 113 : 1.2797709645956834E-4
Loss in iteration 114 : 9.527672889904117E-5
Loss in iteration 115 : 6.383622483727991E-5
Loss in iteration 116 : 5.576535221863321E-5
Loss in iteration 117 : 4.2163211390793145E-5
Loss in iteration 118 : 3.2838904630950925E-5
Loss in iteration 119 : 2.4041022644679313E-5
Loss in iteration 120 : 1.5073048091790015E-5
Loss in iteration 121 : 9.584421150669738E-6
Loss in iteration 122 : 3.6261771797558075E-6
Loss in iteration 123 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 0.09999999999999998 = 1.0, training accuracy 1.0, time elapsed: 2602 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.9628308837434816
Loss in iteration 3 : 0.7066223149120525
Loss in iteration 4 : 0.37764325050534925
Loss in iteration 5 : 0.7882389292438781
Loss in iteration 6 : 0.6919547874511415
Loss in iteration 7 : 0.446185328544377
Loss in iteration 8 : 0.29450815941292907
Loss in iteration 9 : 0.20870746305396282
Loss in iteration 10 : 0.19659015327009036
Loss in iteration 11 : 0.23180605846614663
Loss in iteration 12 : 0.25325973093035153
Loss in iteration 13 : 0.21796950998857537
Loss in iteration 14 : 0.15746336121050017
Loss in iteration 15 : 0.11000925013648195
Loss in iteration 16 : 0.07803920441453485
Loss in iteration 17 : 0.06270984293640805
Loss in iteration 18 : 0.05634847463578715
Loss in iteration 19 : 0.056052442476452725
Loss in iteration 20 : 0.05748484215100547
Loss in iteration 21 : 0.05805732429866309
Loss in iteration 22 : 0.05702520619949462
Loss in iteration 23 : 0.0535928862458369
Loss in iteration 24 : 0.04795031607335442
Loss in iteration 25 : 0.042037135565603526
Loss in iteration 26 : 0.03652683960272321
Loss in iteration 27 : 0.03227196957157908
Loss in iteration 28 : 0.0293669280188867
Loss in iteration 29 : 0.0280810439441369
Loss in iteration 30 : 0.027676196333675938
Loss in iteration 31 : 0.027720393090367924
Loss in iteration 32 : 0.027785179112732847
Loss in iteration 33 : 0.02766724491793696
Loss in iteration 34 : 0.02728690308616776
Loss in iteration 35 : 0.02666894231681894
Loss in iteration 36 : 0.02574569650398175
Loss in iteration 37 : 0.024567796846463794
Loss in iteration 38 : 0.023278083635314296
Loss in iteration 39 : 0.022199672904732723
Loss in iteration 40 : 0.021302381750812414
Loss in iteration 41 : 0.02048608470593926
Loss in iteration 42 : 0.019819188185919857
Loss in iteration 43 : 0.01928991043625233
Loss in iteration 44 : 0.018758397815488503
Loss in iteration 45 : 0.018224396920983234
Loss in iteration 46 : 0.01768767984946969
Loss in iteration 47 : 0.017148041924470794
Loss in iteration 48 : 0.016605299622593644
Loss in iteration 49 : 0.016059288680512998
Loss in iteration 50 : 0.015564559532404933
Loss in iteration 51 : 0.015098161129638759
Loss in iteration 52 : 0.014666155391181717
Loss in iteration 53 : 0.014289525127297063
Loss in iteration 54 : 0.013959895355835129
Loss in iteration 55 : 0.013660602308756799
Loss in iteration 56 : 0.013326850411046759
Loss in iteration 57 : 0.012961298381650325
Loss in iteration 58 : 0.012566370310537844
Loss in iteration 59 : 0.012144276172288044
Loss in iteration 60 : 0.011697030580574532
Loss in iteration 61 : 0.011226469931713678
Loss in iteration 62 : 0.010734268072942761
Loss in iteration 63 : 0.010221950619727069
Loss in iteration 64 : 0.009690908036024452
Loss in iteration 65 : 0.009298216177077986
Loss in iteration 66 : 0.00895254873375368
Loss in iteration 67 : 0.00861792520667556
Loss in iteration 68 : 0.008281717179623917
Loss in iteration 69 : 0.007943858033208054
Loss in iteration 70 : 0.007616843477195142
Loss in iteration 71 : 0.007301104329016634
Loss in iteration 72 : 0.007033611688373932
Loss in iteration 73 : 0.006747910057655717
Loss in iteration 74 : 0.006445424119061135
Loss in iteration 75 : 0.006127449032104986
Loss in iteration 76 : 0.005801858998940648
Loss in iteration 77 : 0.005489736737056971
Loss in iteration 78 : 0.00517423300508288
Loss in iteration 79 : 0.004855473087675238
Loss in iteration 80 : 0.004561031136711989
Loss in iteration 81 : 0.004267912055994588
Loss in iteration 82 : 0.003970746609742676
Loss in iteration 83 : 0.003669737010104151
Loss in iteration 84 : 0.003365067500474135
Loss in iteration 85 : 0.003056905994305263
Loss in iteration 86 : 0.0027454055655859433
Loss in iteration 87 : 0.0024307058042001266
Loss in iteration 88 : 0.0021429454163818815
Loss in iteration 89 : 0.0019143998865996203
Loss in iteration 90 : 0.0016838426170744261
Loss in iteration 91 : 0.0014513403224388744
Loss in iteration 92 : 0.0012481611520814262
Loss in iteration 93 : 0.0010700268285293985
Loss in iteration 94 : 8.815670703590406E-4
Loss in iteration 95 : 6.981327344914842E-4
Loss in iteration 96 : 5.808776957893771E-4
Loss in iteration 97 : 4.7108965719176054E-4
Loss in iteration 98 : 3.6684085086821817E-4
Loss in iteration 99 : 3.0131046376621754E-4
Loss in iteration 100 : 2.3296604569764438E-4
Loss in iteration 101 : 1.6203138097259934E-4
Loss in iteration 102 : 8.870931142680228E-5
Loss in iteration 103 : 1.3183696180290847E-5
Loss in iteration 104 : 0.0
Loss in iteration 105 : 0.0
Loss in iteration 106 : 0.0
Loss in iteration 107 : 0.0
Loss in iteration 108 : 0.0
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1936 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3319956272069027
Loss in iteration 3 : 0.23849793088545027
Loss in iteration 4 : 0.12035880676989828
Loss in iteration 5 : 0.10294121489744194
Loss in iteration 6 : 0.0834170642664459
Loss in iteration 7 : 0.07231148029811939
Loss in iteration 8 : 0.0652319337203504
Loss in iteration 9 : 0.05740546560395618
Loss in iteration 10 : 0.048715035951432735
Loss in iteration 11 : 0.03991919775995102
Loss in iteration 12 : 0.0319404214468556
Loss in iteration 13 : 0.026041326213266883
Loss in iteration 14 : 0.021968607897369513
Loss in iteration 15 : 0.019774317194060406
Loss in iteration 16 : 0.018228726842439626
Loss in iteration 17 : 0.016777001338855773
Loss in iteration 18 : 0.015459495630498789
Loss in iteration 19 : 0.014245988356169711
Loss in iteration 20 : 0.013071336100427744
Loss in iteration 21 : 0.011962469814775795
Loss in iteration 22 : 0.010864876284672916
Loss in iteration 23 : 0.00988129323500427
Loss in iteration 24 : 0.009160457302619503
Loss in iteration 25 : 0.008647948608809877
Loss in iteration 26 : 0.008264544256675376
Loss in iteration 27 : 0.007890284233630272
Loss in iteration 28 : 0.007505343305817015
Loss in iteration 29 : 0.007085928543072653
Loss in iteration 30 : 0.006695391599341291
Loss in iteration 31 : 0.0063229916937686415
Loss in iteration 32 : 0.005920279637883851
Loss in iteration 33 : 0.005568898819602178
Loss in iteration 34 : 0.005226023333955827
Loss in iteration 35 : 0.004942972512901777
Loss in iteration 36 : 0.0047567311222706295
Loss in iteration 37 : 0.0046065439143191484
Loss in iteration 38 : 0.004475362744711655
Loss in iteration 39 : 0.004370624126905868
Loss in iteration 40 : 0.004281107247658678
Loss in iteration 41 : 0.004198481050080562
Loss in iteration 42 : 0.004113278265429593
Loss in iteration 43 : 0.004045412224963292
Loss in iteration 44 : 0.003986262499607165
Loss in iteration 45 : 0.003923250606731116
Loss in iteration 46 : 0.0038595235654426767
Loss in iteration 47 : 0.0037945107606180265
Loss in iteration 48 : 0.0037283122446550073
Loss in iteration 49 : 0.0036626229034382417
Loss in iteration 50 : 0.003597663972648788
Loss in iteration 51 : 0.0035325166006228946
Loss in iteration 52 : 0.0034730509624440174
Loss in iteration 53 : 0.003422637741648237
Loss in iteration 54 : 0.003373456665081096
Loss in iteration 55 : 0.003324438521231827
Loss in iteration 56 : 0.0032755531902308233
Loss in iteration 57 : 0.003226773495556569
Loss in iteration 58 : 0.0031780749197147505
Loss in iteration 59 : 0.0031294353472949748
Loss in iteration 60 : 0.003080834832776094
Loss in iteration 61 : 0.003032255390702806
Loss in iteration 62 : 0.002983680806083885
Loss in iteration 63 : 0.002935096463068387
Loss in iteration 64 : 0.0028864891901423813
Loss in iteration 65 : 0.002837847120257335
Loss in iteration 66 : 0.002789159564453604
Loss in iteration 67 : 0.0027404168976803483
Loss in iteration 68 : 0.002691610455637792
Loss in iteration 69 : 0.002642732441580499
Loss in iteration 70 : 0.0025982230559529846
Loss in iteration 71 : 0.002560986456438742
Loss in iteration 72 : 0.002523273637868744
Loss in iteration 73 : 0.002485119133575487
Loss in iteration 74 : 0.0024470723261866886
Loss in iteration 75 : 0.0024101443673567673
Loss in iteration 76 : 0.002373450651851645
Loss in iteration 77 : 0.0023369589847531988
Loss in iteration 78 : 0.0023006403138753047
Loss in iteration 79 : 0.0022644684250069406
Loss in iteration 80 : 0.0022284196666447634
Loss in iteration 81 : 0.002192472701367256
Loss in iteration 82 : 0.0021566082812770077
Loss in iteration 83 : 0.002120809045185718
Loss in iteration 84 : 0.0020850593354407167
Loss in iteration 85 : 0.0020493450324944374
Loss in iteration 86 : 0.002013653405501411
Loss in iteration 87 : 0.001978967054332967
Loss in iteration 88 : 0.0019449124314436658
Loss in iteration 89 : 0.001910361977418932
Loss in iteration 90 : 0.0018753538719005754
Loss in iteration 91 : 0.001839922598027245
Loss in iteration 92 : 0.0018043273075776067
Loss in iteration 93 : 0.0017684455318292475
Loss in iteration 94 : 0.0017328860138814107
Loss in iteration 95 : 0.0016979504361762032
Loss in iteration 96 : 0.0016641976461782965
Loss in iteration 97 : 0.001630798653247377
Loss in iteration 98 : 0.001596925147298411
Loss in iteration 99 : 0.0015634044998240383
Loss in iteration 100 : 0.0015304171749767315
Loss in iteration 101 : 0.001496771564221331
Loss in iteration 102 : 0.0014661910311696642
Loss in iteration 103 : 0.0014361997597059856
Loss in iteration 104 : 0.0014055865373094106
Loss in iteration 105 : 0.0013744032152082691
Loss in iteration 106 : 0.001343371858656563
Loss in iteration 107 : 0.001313275839233524
Loss in iteration 108 : 0.001283009922074971
Loss in iteration 109 : 0.0012529068185508578
Loss in iteration 110 : 0.0012221756630326774
Loss in iteration 111 : 0.0011916082432755497
Loss in iteration 112 : 0.0011609394740391588
Loss in iteration 113 : 0.0011301187711334518
Loss in iteration 114 : 0.0010991534208650373
Loss in iteration 115 : 0.0010680500097018024
Loss in iteration 116 : 0.0010368144923968026
Loss in iteration 117 : 0.0010054522534809242
Loss in iteration 118 : 9.73968162769087E-4
Loss in iteration 119 : 9.423666254621627E-4
Loss in iteration 120 : 9.106516273702118E-4
Loss in iteration 121 : 8.78826775731634E-4
Loss in iteration 122 : 8.468953360566775E-4
Loss in iteration 123 : 8.148602653821417E-4
Loss in iteration 124 : 7.827242422865261E-4
Loss in iteration 125 : 7.51110460309585E-4
Loss in iteration 126 : 7.187743404533538E-4
Loss in iteration 127 : 6.860765928492083E-4
Loss in iteration 128 : 6.544270754139266E-4
Loss in iteration 129 : 6.223159784256226E-4
Loss in iteration 130 : 5.896075517001071E-4
Loss in iteration 131 : 5.603706409182879E-4
Loss in iteration 132 : 5.320638365743937E-4
Loss in iteration 133 : 5.017119772432822E-4
Loss in iteration 134 : 4.692686210753489E-4
Loss in iteration 135 : 4.363713556410134E-4
Loss in iteration 136 : 4.048154695585207E-4
Loss in iteration 137 : 3.7279712225702684E-4
Loss in iteration 138 : 3.396295365927859E-4
Loss in iteration 139 : 3.119201125181118E-4
Loss in iteration 140 : 2.904721826630249E-4
Loss in iteration 141 : 2.689552789398588E-4
Loss in iteration 142 : 2.4713611049191744E-4
Loss in iteration 143 : 2.3260196183497262E-4
Loss in iteration 144 : 2.1913308687855905E-4
Loss in iteration 145 : 2.06949162396002E-4
Loss in iteration 146 : 1.9526724080794525E-4
Loss in iteration 147 : 1.837745101685712E-4
Loss in iteration 148 : 1.7326503730371958E-4
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.9955555555555555, training accuracy 1.0, time elapsed: 2520 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8831269502943879
Loss in iteration 3 : 0.686010354930663
Loss in iteration 4 : 0.4401405368094748
Loss in iteration 5 : 0.309101318348147
Loss in iteration 6 : 0.22554231782562448
Loss in iteration 7 : 0.19492020339402377
Loss in iteration 8 : 0.1759731823231562
Loss in iteration 9 : 0.15863937346954726
Loss in iteration 10 : 0.14353832904666947
Loss in iteration 11 : 0.13116217906143054
Loss in iteration 12 : 0.12016262276210121
Loss in iteration 13 : 0.10829665879633697
Loss in iteration 14 : 0.09625923360381387
Loss in iteration 15 : 0.08599794016408097
Loss in iteration 16 : 0.07789163542937873
Loss in iteration 17 : 0.07193603689762888
Loss in iteration 18 : 0.06705554573841888
Loss in iteration 19 : 0.06256260475052146
Loss in iteration 20 : 0.058413312664855006
Loss in iteration 21 : 0.05452381592149178
Loss in iteration 22 : 0.05074600410349432
Loss in iteration 23 : 0.04713786669876946
Loss in iteration 24 : 0.043752525003002994
Loss in iteration 25 : 0.04051590628191842
Loss in iteration 26 : 0.03755528112293715
Loss in iteration 27 : 0.034805460569330156
Loss in iteration 28 : 0.0322738691568802
Loss in iteration 29 : 0.030042873971539263
Loss in iteration 30 : 0.027963808919280343
Loss in iteration 31 : 0.026084223941577864
Loss in iteration 32 : 0.024439253538048218
Loss in iteration 33 : 0.023028849213830162
Loss in iteration 34 : 0.02178627656823191
Loss in iteration 35 : 0.020700458660976858
Loss in iteration 36 : 0.019761227275279177
Loss in iteration 37 : 0.018956923894944525
Loss in iteration 38 : 0.01821203449146415
Loss in iteration 39 : 0.017523110824255016
Loss in iteration 40 : 0.016878100944079058
Loss in iteration 41 : 0.016263976764586168
Loss in iteration 42 : 0.01569815240424694
Loss in iteration 43 : 0.015173168990994941
Loss in iteration 44 : 0.014673455580574899
Loss in iteration 45 : 0.014211753488127326
Loss in iteration 46 : 0.013773514759562407
Loss in iteration 47 : 0.013350808079199966
Loss in iteration 48 : 0.01295355033209119
Loss in iteration 49 : 0.012571818858389658
Loss in iteration 50 : 0.012214191148999735
Loss in iteration 51 : 0.011874579263775345
Loss in iteration 52 : 0.011541831891194877
Loss in iteration 53 : 0.011221958976517694
Loss in iteration 54 : 0.010911071851665066
Loss in iteration 55 : 0.010606958355328143
Loss in iteration 56 : 0.010305542414654504
Loss in iteration 57 : 0.010006637846241598
Loss in iteration 58 : 0.009710108242163793
Loss in iteration 59 : 0.009420104541534581
Loss in iteration 60 : 0.009136357526960319
Loss in iteration 61 : 0.008860856116915648
Loss in iteration 62 : 0.008590706964648885
Loss in iteration 63 : 0.008333264650765214
Loss in iteration 64 : 0.008081370386195118
Loss in iteration 65 : 0.007838495331244684
Loss in iteration 66 : 0.007602433686251293
Loss in iteration 67 : 0.007370508920366388
Loss in iteration 68 : 0.007144874433909476
Loss in iteration 69 : 0.006928200627982436
Loss in iteration 70 : 0.006720089047448727
Loss in iteration 71 : 0.006517337818817268
Loss in iteration 72 : 0.006323353649374861
Loss in iteration 73 : 0.0061385385940451445
Loss in iteration 74 : 0.005963203117051016
Loss in iteration 75 : 0.00579677362137194
Loss in iteration 76 : 0.005639366353036447
Loss in iteration 77 : 0.005496612915158028
Loss in iteration 78 : 0.0053632958634462925
Loss in iteration 79 : 0.005233533914325433
Loss in iteration 80 : 0.0051109208458137165
Loss in iteration 81 : 0.004994843346180885
Loss in iteration 82 : 0.004880404494034494
Loss in iteration 83 : 0.00477014052957207
Loss in iteration 84 : 0.004665196061761897
Loss in iteration 85 : 0.004563030980184489
Loss in iteration 86 : 0.004464857253175674
Loss in iteration 87 : 0.004369922688060283
Loss in iteration 88 : 0.00427674166464737
Loss in iteration 89 : 0.0041863933941952685
Loss in iteration 90 : 0.004098416314251371
Loss in iteration 91 : 0.004012907772575099
Loss in iteration 92 : 0.00393375301933746
Loss in iteration 93 : 0.0038629299453477855
Loss in iteration 94 : 0.003792741688977909
Loss in iteration 95 : 0.0037230647697675374
Loss in iteration 96 : 0.0036538387086642995
Loss in iteration 97 : 0.0035850960164233456
Loss in iteration 98 : 0.0035185157522280262
Loss in iteration 99 : 0.0034533698376304415
Loss in iteration 100 : 0.003392222679932836
Loss in iteration 101 : 0.003332253551246811
Loss in iteration 102 : 0.003276167784865023
Loss in iteration 103 : 0.0032219861823344
Loss in iteration 104 : 0.003169527347624757
Loss in iteration 105 : 0.0031202329973614774
Loss in iteration 106 : 0.003072229024973928
Loss in iteration 107 : 0.0030251581714757646
Loss in iteration 108 : 0.0029801693723896286
Loss in iteration 109 : 0.0029360366159457978
Loss in iteration 110 : 0.0028940699933389176
Loss in iteration 111 : 0.0028540047985268205
Loss in iteration 112 : 0.0028144053091545
Loss in iteration 113 : 0.002775481520053644
Loss in iteration 114 : 0.0027389641849530437
Loss in iteration 115 : 0.0027040610978970405
Loss in iteration 116 : 0.00266933343868626
Loss in iteration 117 : 0.0026344928215231264
Loss in iteration 118 : 0.0026009594097605782
Loss in iteration 119 : 0.0025679014045324076
Loss in iteration 120 : 0.0025362210188043352
Loss in iteration 121 : 0.002506255796029378
Loss in iteration 122 : 0.002476309582652575
Loss in iteration 123 : 0.0024478877452027005
Loss in iteration 124 : 0.002419467739577003
Loss in iteration 125 : 0.0023915991079743594
Loss in iteration 126 : 0.002365110033223022
Loss in iteration 127 : 0.002340689688595822
Loss in iteration 128 : 0.002316730315147196
Loss in iteration 129 : 0.00229374520315104
Loss in iteration 130 : 0.0022713239253378107
Loss in iteration 131 : 0.002248525376236783
Loss in iteration 132 : 0.0022253828794536845
Loss in iteration 133 : 0.0022019264769090247
Loss in iteration 134 : 0.0021782562231048306
Loss in iteration 135 : 0.002156345544854307
Loss in iteration 136 : 0.0021358287510177174
Loss in iteration 137 : 0.0021154992536707427
Loss in iteration 138 : 0.002095260890073413
Loss in iteration 139 : 0.0020750319941262763
Loss in iteration 140 : 0.0020548087182328126
Loss in iteration 141 : 0.0020345875959010503
Loss in iteration 142 : 0.002014428251784829
Loss in iteration 143 : 0.001995610560947296
Loss in iteration 144 : 0.0019774099499807437
Loss in iteration 145 : 0.0019597169076851053
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.9866666666666667, training accuracy 0.9994284081166047, time elapsed: 2602 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 12.82320512627665
Loss in iteration 3 : 4.575331828105403
Loss in iteration 4 : 2.0607134843027333
Loss in iteration 5 : 4.00158522851858
Loss in iteration 6 : 4.436831662214039
Loss in iteration 7 : 3.227517190728091
Loss in iteration 8 : 2.1327430909611516
Loss in iteration 9 : 1.3618923122360937
Loss in iteration 10 : 0.9169045005347088
Loss in iteration 11 : 0.7803864058280233
Loss in iteration 12 : 0.8267135603507789
Loss in iteration 13 : 0.9184397414752796
Loss in iteration 14 : 0.9889612779074004
Loss in iteration 15 : 0.9494286270042726
Loss in iteration 16 : 0.8246031657393071
Loss in iteration 17 : 0.6700727982027013
Loss in iteration 18 : 0.5180862145154211
Loss in iteration 19 : 0.38271919512539915
Loss in iteration 20 : 0.27565813147201595
Loss in iteration 21 : 0.23014951826287738
Loss in iteration 22 : 0.2082456307413757
Loss in iteration 23 : 0.20012911329009114
Loss in iteration 24 : 0.19564398622607662
Loss in iteration 25 : 0.19493986818994957
Loss in iteration 26 : 0.1933548422982237
Loss in iteration 27 : 0.19035417706098198
Loss in iteration 28 : 0.18629752839258737
Loss in iteration 29 : 0.1819797049391542
Loss in iteration 30 : 0.17745037468610872
Loss in iteration 31 : 0.1711889211825577
Loss in iteration 32 : 0.16454820815666302
Loss in iteration 33 : 0.15791166731584413
Loss in iteration 34 : 0.15091587372693405
Loss in iteration 35 : 0.14376966430683974
Loss in iteration 36 : 0.1377102202610208
Loss in iteration 37 : 0.1324350817384161
Loss in iteration 38 : 0.12745970924572098
Loss in iteration 39 : 0.12313275235218325
Loss in iteration 40 : 0.11903635218981526
Loss in iteration 41 : 0.11598029925982305
Loss in iteration 42 : 0.1139927052196788
Loss in iteration 43 : 0.11265245038119749
Loss in iteration 44 : 0.1112653230312593
Loss in iteration 45 : 0.1098674268581769
Loss in iteration 46 : 0.1090439066540181
Loss in iteration 47 : 0.10820215291886692
Loss in iteration 48 : 0.10756439833243119
Loss in iteration 49 : 0.10668082343036053
Loss in iteration 50 : 0.10557574786693535
Loss in iteration 51 : 0.10427103004179868
Loss in iteration 52 : 0.10278632073783436
Loss in iteration 53 : 0.1011392897551266
Loss in iteration 54 : 0.09934582857817888
Loss in iteration 55 : 0.09767289236962924
Loss in iteration 56 : 0.09623754869690053
Loss in iteration 57 : 0.09485698136683093
Loss in iteration 58 : 0.09383710635398596
Loss in iteration 59 : 0.0928499557056293
Loss in iteration 60 : 0.0918569193207505
Loss in iteration 61 : 0.09085847833182643
Loss in iteration 62 : 0.08985506577137034
Testing accuracy  of updater 7 on alg 1 with rate 20.0 = 0.9804444444444445, training accuracy 0.9989997142040583, time elapsed: 1286 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3429992496848125
Loss in iteration 3 : 0.13605731452735476
Loss in iteration 4 : 0.19659625752011503
Loss in iteration 5 : 0.09140567991343543
Loss in iteration 6 : 0.08925175417060525
Loss in iteration 7 : 0.08284608137502071
Loss in iteration 8 : 0.06224191532514252
Loss in iteration 9 : 0.042430545942940406
Loss in iteration 10 : 0.0320116054256558
Loss in iteration 11 : 0.027568688019133956
Loss in iteration 12 : 0.024637142760731034
Loss in iteration 13 : 0.021690538860263647
Loss in iteration 14 : 0.018907883288885267
Loss in iteration 15 : 0.015877713421030813
Loss in iteration 16 : 0.013667506871053958
Loss in iteration 17 : 0.011874278700564242
Loss in iteration 18 : 0.010743865178329233
Loss in iteration 19 : 0.010113906461108733
Loss in iteration 20 : 0.009824013008873879
Loss in iteration 21 : 0.009792290339981028
Loss in iteration 22 : 0.009968809000919414
Loss in iteration 23 : 0.00997605416206516
Loss in iteration 24 : 0.00983768914642943
Loss in iteration 25 : 0.00959454446849785
Loss in iteration 26 : 0.009216502872775745
Loss in iteration 27 : 0.008730207381918861
Loss in iteration 28 : 0.008158879973762452
Loss in iteration 29 : 0.007644473546763609
Loss in iteration 30 : 0.007359768774371382
Loss in iteration 31 : 0.0071562563780936395
Loss in iteration 32 : 0.00695122964272721
Loss in iteration 33 : 0.006745443665867533
Loss in iteration 34 : 0.00653663917870049
Loss in iteration 35 : 0.006318349393455165
Loss in iteration 36 : 0.00612991255709427
Loss in iteration 37 : 0.0059834699689841776
Loss in iteration 38 : 0.005881746574882445
Loss in iteration 39 : 0.005816237287892187
Loss in iteration 40 : 0.005748026644264658
Loss in iteration 41 : 0.005677373153245495
Loss in iteration 42 : 0.0056045096950085175
Loss in iteration 43 : 0.005530683437720793
Loss in iteration 44 : 0.0054562646558926255
Loss in iteration 45 : 0.005377672288100167
Loss in iteration 46 : 0.005295308381886291
Loss in iteration 47 : 0.0052095351289954865
Loss in iteration 48 : 0.005120749856916952
Loss in iteration 49 : 0.005034287857496335
Loss in iteration 50 : 0.004947108503317125
Loss in iteration 51 : 0.004859273589443678
Loss in iteration 52 : 0.004770838777648447
Loss in iteration 53 : 0.004681854204235216
Loss in iteration 54 : 0.004592365027622121
Loss in iteration 55 : 0.004508772785720914
Loss in iteration 56 : 0.004426996425507799
Loss in iteration 57 : 0.004350967158205534
Loss in iteration 58 : 0.004274310009936856
Loss in iteration 59 : 0.004197007952240581
Loss in iteration 60 : 0.004119438676760336
Loss in iteration 61 : 0.004044087331824838
Loss in iteration 62 : 0.0039665376265860235
Loss in iteration 63 : 0.00388699774205337
Loss in iteration 64 : 0.0038056552188560004
Loss in iteration 65 : 0.003727651608822001
Loss in iteration 66 : 0.0036525654428764803
Loss in iteration 67 : 0.0035872180990677784
Loss in iteration 68 : 0.0035296333738783184
Loss in iteration 69 : 0.0034735048093243676
Loss in iteration 70 : 0.0034196786168729913
Loss in iteration 71 : 0.003365707492557324
Loss in iteration 72 : 0.0033116003144637537
Loss in iteration 73 : 0.003257365075375863
Loss in iteration 74 : 0.003203008970502103
Loss in iteration 75 : 0.0031485384765096327
Loss in iteration 76 : 0.0030939594227259175
Loss in iteration 77 : 0.0030405589733298005
Loss in iteration 78 : 0.0029884521865940024
Loss in iteration 79 : 0.002936388658792212
Loss in iteration 80 : 0.002884358977394982
Loss in iteration 81 : 0.002832354657516401
Loss in iteration 82 : 0.002780368049980124
Loss in iteration 83 : 0.002728392258495991
Loss in iteration 84 : 0.0026779750218912544
Loss in iteration 85 : 0.002635176664370277
Loss in iteration 86 : 0.00259189984067356
Loss in iteration 87 : 0.002548187241911349
Loss in iteration 88 : 0.002504462607178031
Loss in iteration 89 : 0.0024628954500842304
Loss in iteration 90 : 0.002421045387091341
Loss in iteration 91 : 0.002378936047343372
Loss in iteration 92 : 0.0023365887139455963
Loss in iteration 93 : 0.0022940225564506245
Loss in iteration 94 : 0.0022512548403052766
Loss in iteration 95 : 0.0022083011155414403
Loss in iteration 96 : 0.002165175386767798
Loss in iteration 97 : 0.002121890266315515
Loss in iteration 98 : 0.0020784571122073048
Loss in iteration 99 : 0.002034886152453873
Loss in iteration 100 : 0.0019911865970326788
Loss in iteration 101 : 0.001947366738769691
Loss in iteration 102 : 0.0019034340442238484
Loss in iteration 103 : 0.0018593952355649322
Loss in iteration 104 : 0.0018152563643374165
Loss in iteration 105 : 0.001771022877914378
Loss in iteration 106 : 0.0017266996793658697
Loss in iteration 107 : 0.0016822911813943965
Loss in iteration 108 : 0.00163908555446845
Loss in iteration 109 : 0.0015954194324694858
Loss in iteration 110 : 0.0015499622349122134
Loss in iteration 111 : 0.0015061090382167525
Loss in iteration 112 : 0.0014621878122916925
Loss in iteration 113 : 0.0014181144914049658
Loss in iteration 114 : 0.0013738996388723932
Loss in iteration 115 : 0.0013295527666810079
Loss in iteration 116 : 0.0012850824396702782
Loss in iteration 117 : 0.0012404963693890653
Loss in iteration 118 : 0.0011958014986514244
Loss in iteration 119 : 0.0011510040777130094
Loss in iteration 120 : 0.0011061097328984469
Loss in iteration 121 : 0.0010621942176237715
Loss in iteration 122 : 0.0010178652733379334
Loss in iteration 123 : 9.731080280880127E-4
Loss in iteration 124 : 9.279601000284272E-4
Loss in iteration 125 : 8.825126888644383E-4
Loss in iteration 126 : 8.378701395862653E-4
Loss in iteration 127 : 7.950983656268128E-4
Loss in iteration 128 : 7.517532784764836E-4
Loss in iteration 129 : 7.07210372595242E-4
Loss in iteration 130 : 6.622209025226261E-4
Loss in iteration 131 : 6.167057804280704E-4
Loss in iteration 132 : 5.72246171742062E-4
Loss in iteration 133 : 5.317523594926864E-4
Loss in iteration 134 : 4.904720858299097E-4
Loss in iteration 135 : 4.488353225832917E-4
Loss in iteration 136 : 4.065127518316524E-4
Loss in iteration 137 : 3.646357180574887E-4
Loss in iteration 138 : 3.2635895345482064E-4
Loss in iteration 139 : 2.896860176917035E-4
Loss in iteration 140 : 2.6225754250924776E-4
Loss in iteration 141 : 2.3284378464543532E-4
Loss in iteration 142 : 2.016367756976281E-4
Loss in iteration 143 : 1.7373035758408068E-4
Loss in iteration 144 : 1.5091820366352696E-4
Testing accuracy  of updater 7 on alg 1 with rate 2.0 = 1.0, training accuracy 1.0, time elapsed: 3897 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8520783829040017
Loss in iteration 3 : 0.5706703574848419
Loss in iteration 4 : 0.352107223116218
Loss in iteration 5 : 0.24007910811779684
Loss in iteration 6 : 0.19899579666986536
Loss in iteration 7 : 0.16582718845826672
Loss in iteration 8 : 0.12950279605983556
Loss in iteration 9 : 0.1050262981083495
Loss in iteration 10 : 0.10128446288096438
Loss in iteration 11 : 0.09784293780881985
Loss in iteration 12 : 0.08314003333040339
Loss in iteration 13 : 0.06670948925418146
Loss in iteration 14 : 0.05449163562010466
Loss in iteration 15 : 0.04754083102597291
Loss in iteration 16 : 0.04352065617835447
Loss in iteration 17 : 0.040378717652975085
Loss in iteration 18 : 0.037107610132409916
Loss in iteration 19 : 0.03371334391029146
Loss in iteration 20 : 0.03040617890058141
Loss in iteration 21 : 0.027263139151599704
Loss in iteration 22 : 0.024457003184073896
Loss in iteration 23 : 0.022001986475742755
Loss in iteration 24 : 0.019798480174998842
Loss in iteration 25 : 0.017843871335513406
Loss in iteration 26 : 0.016191218715875148
Loss in iteration 27 : 0.014875223608029835
Loss in iteration 28 : 0.01375403090018586
Loss in iteration 29 : 0.012829917068966407
Loss in iteration 30 : 0.012001911050091643
Loss in iteration 31 : 0.011223823525585057
Loss in iteration 32 : 0.01049617289258982
Loss in iteration 33 : 0.00988415935953457
Loss in iteration 34 : 0.009313886828987887
Loss in iteration 35 : 0.008782319261330884
Loss in iteration 36 : 0.008285701774050531
Loss in iteration 37 : 0.007816012647454938
Loss in iteration 38 : 0.007414401251916919
Loss in iteration 39 : 0.0070983289093567315
Loss in iteration 40 : 0.006806921631126894
Loss in iteration 41 : 0.006537837012102197
Loss in iteration 42 : 0.006292162579537374
Loss in iteration 43 : 0.006056453141287969
Loss in iteration 44 : 0.005852530160042209
Loss in iteration 45 : 0.005661253628999409
Loss in iteration 46 : 0.005482159500823343
Loss in iteration 47 : 0.005314291157782726
Loss in iteration 48 : 0.005168807433240037
Loss in iteration 49 : 0.005029156486080501
Loss in iteration 50 : 0.004890521966186624
Loss in iteration 51 : 0.004760881642957881
Loss in iteration 52 : 0.0046341483056724034
Loss in iteration 53 : 0.004506915128461822
Loss in iteration 54 : 0.004379789618753908
Loss in iteration 55 : 0.004256161877622818
Loss in iteration 56 : 0.004139682881177309
Loss in iteration 57 : 0.0040307562427576785
Loss in iteration 58 : 0.00392462739030034
Loss in iteration 59 : 0.00382135110341223
Loss in iteration 60 : 0.0037214544826111103
Loss in iteration 61 : 0.003622060746971924
Loss in iteration 62 : 0.003532076370521887
Loss in iteration 63 : 0.0034506220518986243
Loss in iteration 64 : 0.0033717107714249287
Loss in iteration 65 : 0.0032997857615295425
Loss in iteration 66 : 0.003232118654253716
Loss in iteration 67 : 0.003174126868957998
Loss in iteration 68 : 0.0031188189073542544
Loss in iteration 69 : 0.003063728832507204
Loss in iteration 70 : 0.0030088297836847917
Loss in iteration 71 : 0.0029540975567556834
Loss in iteration 72 : 0.0029001289391972
Loss in iteration 73 : 0.0028549439340437364
Loss in iteration 74 : 0.002813338525214189
Loss in iteration 75 : 0.0027724409969753804
Loss in iteration 76 : 0.0027351214862193396
Loss in iteration 77 : 0.002698467707372493
Loss in iteration 78 : 0.0026623158935492144
Loss in iteration 79 : 0.002626712045221408
Loss in iteration 80 : 0.002591725116287039
Loss in iteration 81 : 0.0025582083751055336
Loss in iteration 82 : 0.0025250320869109623
Loss in iteration 83 : 0.0024923453030136215
Loss in iteration 84 : 0.002459989121857836
Loss in iteration 85 : 0.002428929168760457
Loss in iteration 86 : 0.0023988310908814427
Loss in iteration 87 : 0.0023687803549972885
Loss in iteration 88 : 0.0023389208193202995
Loss in iteration 89 : 0.002310467199873403
Loss in iteration 90 : 0.0022837620998082415
Loss in iteration 91 : 0.0022581498185099205
Loss in iteration 92 : 0.0022342421609151766
Loss in iteration 93 : 0.0022124378457298323
Loss in iteration 94 : 0.0021913813948834264
Loss in iteration 95 : 0.0021710729109616096
Loss in iteration 96 : 0.0021542226561425426
Loss in iteration 97 : 0.0021381035196045943
Loss in iteration 98 : 0.002124837298423333
Testing accuracy  of updater 7 on alg 1 with rate 0.19999999999999996 = 0.9911111111111112, training accuracy 0.9994284081166047, time elapsed: 2604 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.0912746877606057
Loss in iteration 3 : 0.42826660979751713
Loss in iteration 4 : 0.4071873580487533
Loss in iteration 5 : 0.38116025367364764
Loss in iteration 6 : 0.31356792052214316
Loss in iteration 7 : 0.24932889828984867
Loss in iteration 8 : 0.1984320185698494
Loss in iteration 9 : 0.15705544828098184
Loss in iteration 10 : 0.13344594966752626
Loss in iteration 11 : 0.11767574191311127
Loss in iteration 12 : 0.1044353857435006
Loss in iteration 13 : 0.09157943450611931
Loss in iteration 14 : 0.07901934409058341
Loss in iteration 15 : 0.06852340091339322
Loss in iteration 16 : 0.060121978078172385
Loss in iteration 17 : 0.05289494710897352
Loss in iteration 18 : 0.04747483286044441
Loss in iteration 19 : 0.04341160971030872
Loss in iteration 20 : 0.03968237873712408
Loss in iteration 21 : 0.036160725798380075
Loss in iteration 22 : 0.033270337183662756
Loss in iteration 23 : 0.031137431778622375
Loss in iteration 24 : 0.029187141481932247
Loss in iteration 25 : 0.02754303543380148
Loss in iteration 26 : 0.025887886075724575
Loss in iteration 27 : 0.024956497582414677
Loss in iteration 28 : 0.024086681041314305
Loss in iteration 29 : 0.023258381252666764
Loss in iteration 30 : 0.022626834940223703
Loss in iteration 31 : 0.02213015520035737
Loss in iteration 32 : 0.021752872175031846
Loss in iteration 33 : 0.02137610799597303
Loss in iteration 34 : 0.02097525487886239
Loss in iteration 35 : 0.02055180206781351
Loss in iteration 36 : 0.020107114798887254
Loss in iteration 37 : 0.01964244511247526
Loss in iteration 38 : 0.019158941644320977
Loss in iteration 39 : 0.018720661665055736
Loss in iteration 40 : 0.018269206690080732
Loss in iteration 41 : 0.017812994487662243
Loss in iteration 42 : 0.017342481633488865
Loss in iteration 43 : 0.01685841836093174
Loss in iteration 44 : 0.016369894875868266
Loss in iteration 45 : 0.01590356708580519
Loss in iteration 46 : 0.01542117528474713
Loss in iteration 47 : 0.01493353559946099
Loss in iteration 48 : 0.014477328855515927
Loss in iteration 49 : 0.014013361760056943
Loss in iteration 50 : 0.01354191944589713
Loss in iteration 51 : 0.013063264772971308
Loss in iteration 52 : 0.012580976472835568
Loss in iteration 53 : 0.012087317873996853
Loss in iteration 54 : 0.011633545184252875
Loss in iteration 55 : 0.011177326534315257
Loss in iteration 56 : 0.010714978861237604
Loss in iteration 57 : 0.010254080448488004
Loss in iteration 58 : 0.00984268888921838
Loss in iteration 59 : 0.009497856297395496
Loss in iteration 60 : 0.00915995618737071
Loss in iteration 61 : 0.008812759463062137
Loss in iteration 62 : 0.008460374155563484
Loss in iteration 63 : 0.008108198389005202
Loss in iteration 64 : 0.007750070428920299
Loss in iteration 65 : 0.007386277396867169
Loss in iteration 66 : 0.007017081862141714
Loss in iteration 67 : 0.006642724007276344
Loss in iteration 68 : 0.006313697723752854
Loss in iteration 69 : 0.005996012724148811
Loss in iteration 70 : 0.005675343301338581
Loss in iteration 71 : 0.005351758726061913
Loss in iteration 72 : 0.005025323034656502
Loss in iteration 73 : 0.004696095481348354
Loss in iteration 74 : 0.0043641309510825425
Loss in iteration 75 : 0.004029480336278484
Loss in iteration 76 : 0.0036921908806122872
Loss in iteration 77 : 0.003352306492669139
Loss in iteration 78 : 0.003009868032070534
Loss in iteration 79 : 0.0026654716937072128
Loss in iteration 80 : 0.002324683260715009
Loss in iteration 81 : 0.002062025981474503
Loss in iteration 82 : 0.001795008386050731
Loss in iteration 83 : 0.0015293608371371786
Loss in iteration 84 : 0.0012630495804065555
Loss in iteration 85 : 0.0010072573689336635
Loss in iteration 86 : 7.835932299633016E-4
Loss in iteration 87 : 5.910793473100439E-4
Loss in iteration 88 : 3.8749912512566295E-4
Loss in iteration 89 : 2.1498376445524772E-4
Loss in iteration 90 : 1.1505627506547847E-4
Loss in iteration 91 : 2.7961652812919873E-5
Loss in iteration 92 : 1.335951678780276E-5
Loss in iteration 93 : 0.0
Loss in iteration 94 : 0.0
Loss in iteration 95 : 0.0
Loss in iteration 96 : 0.0
Loss in iteration 97 : 0.0
Loss in iteration 98 : 0.0
Testing accuracy  of updater 8 on alg 1 with rate 2.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 2632 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3816167251383839
Loss in iteration 3 : 0.18184476835140156
Loss in iteration 4 : 0.21057740351549423
Loss in iteration 5 : 0.09196794406234458
Loss in iteration 6 : 0.06301007500967451
Loss in iteration 7 : 0.05597512998096073
Loss in iteration 8 : 0.04690396610198414
Loss in iteration 9 : 0.03616676576619736
Loss in iteration 10 : 0.02794167087560484
Loss in iteration 11 : 0.0224431545892683
Loss in iteration 12 : 0.01900243316067038
Loss in iteration 13 : 0.01691540676813451
Loss in iteration 14 : 0.015264656682876401
Loss in iteration 15 : 0.014004210614555174
Loss in iteration 16 : 0.012907652075445296
Loss in iteration 17 : 0.011792199980993304
Loss in iteration 18 : 0.010701567453133316
Loss in iteration 19 : 0.009760473260252657
Loss in iteration 20 : 0.008956933713891488
Loss in iteration 21 : 0.00822372313126969
Loss in iteration 22 : 0.007549475783000304
Loss in iteration 23 : 0.0070135115974403655
Loss in iteration 24 : 0.006498547788886279
Loss in iteration 25 : 0.006002152764245284
Loss in iteration 26 : 0.005576252663174538
Loss in iteration 27 : 0.005185127080746421
Loss in iteration 28 : 0.004844617888447433
Loss in iteration 29 : 0.004537916588550601
Loss in iteration 30 : 0.004286679350119694
Loss in iteration 31 : 0.004066475846460641
Loss in iteration 32 : 0.0038976949167334927
Loss in iteration 33 : 0.0037535945593621437
Loss in iteration 34 : 0.003638109017131337
Loss in iteration 35 : 0.0035488847657060693
Loss in iteration 36 : 0.00345996970042801
Loss in iteration 37 : 0.0033734960265825813
Loss in iteration 38 : 0.003305607289067505
Loss in iteration 39 : 0.003242128647288962
Loss in iteration 40 : 0.0031789147888894925
Loss in iteration 41 : 0.00311591917114131
Loss in iteration 42 : 0.0030553212873161706
Loss in iteration 43 : 0.0030017278103053567
Loss in iteration 44 : 0.0029482047566074957
Loss in iteration 45 : 0.0028947274601690163
Loss in iteration 46 : 0.002841273672676453
Loss in iteration 47 : 0.00278782333119289
Loss in iteration 48 : 0.0027343583480161754
Loss in iteration 49 : 0.0026808624206423166
Loss in iteration 50 : 0.00262732085991816
Loss in iteration 51 : 0.002574349191089848
Loss in iteration 52 : 0.002523109231637847
Loss in iteration 53 : 0.0024715498059879586
Loss in iteration 54 : 0.002419684246423315
Loss in iteration 55 : 0.0023675246415017673
Loss in iteration 56 : 0.002316978083988266
Loss in iteration 57 : 0.0022677245061025367
Loss in iteration 58 : 0.0022178806140745974
Loss in iteration 59 : 0.0021670653807414693
Loss in iteration 60 : 0.002117533006230449
Loss in iteration 61 : 0.0020681001532749637
Loss in iteration 62 : 0.0020278119957668783
Loss in iteration 63 : 0.0019880432984899386
Loss in iteration 64 : 0.0019477377805941897
Loss in iteration 65 : 0.001907364829226425
Loss in iteration 66 : 0.00186850935632466
Loss in iteration 67 : 0.0018309400492532034
Loss in iteration 68 : 0.0017934009985943276
Loss in iteration 69 : 0.0017559488398097838
Loss in iteration 70 : 0.0017185415572523448
Loss in iteration 71 : 0.0016811639145736458
Loss in iteration 72 : 0.0016438021727190517
Loss in iteration 73 : 0.0016097968748004553
Loss in iteration 74 : 0.0015757709157430118
Loss in iteration 75 : 0.001541332792415614
Loss in iteration 76 : 0.001508083540441336
Loss in iteration 77 : 0.0014741190354181435
Loss in iteration 78 : 0.0014396440465505261
Loss in iteration 79 : 0.0014046970211950823
Loss in iteration 80 : 0.0013693127015486526
Loss in iteration 81 : 0.0013335224830060845
Loss in iteration 82 : 0.0012973547379077875
Loss in iteration 83 : 0.0012619775227099346
Loss in iteration 84 : 0.0012295479493287678
Loss in iteration 85 : 0.0011969833521827864
Loss in iteration 86 : 0.001166431848366147
Loss in iteration 87 : 0.0011356648072541255
Loss in iteration 88 : 0.0011044071443876995
Loss in iteration 89 : 0.0010726968847078666
Loss in iteration 90 : 0.0010412293667223856
Loss in iteration 91 : 0.0010096367389393305
Loss in iteration 92 : 9.778417698106995E-4
Loss in iteration 93 : 9.458552053787964E-4
Loss in iteration 94 : 9.136867633309681E-4
Loss in iteration 95 : 8.818978313999581E-4
Loss in iteration 96 : 8.492573363079944E-4
Loss in iteration 97 : 8.170277214335587E-4
Loss in iteration 98 : 7.844978104674007E-4
Loss in iteration 99 : 7.519703642394666E-4
Loss in iteration 100 : 7.19339254771862E-4
Loss in iteration 101 : 6.866856996234902E-4
Loss in iteration 102 : 6.544318840215007E-4
Loss in iteration 103 : 6.227960949677492E-4
Loss in iteration 104 : 5.907725009130211E-4
Loss in iteration 105 : 5.583924632204417E-4
Loss in iteration 106 : 5.261516372373394E-4
Loss in iteration 107 : 4.930502658528909E-4
Loss in iteration 108 : 4.6087650101386894E-4
Loss in iteration 109 : 4.288171342556776E-4
Loss in iteration 110 : 3.963923419656229E-4
Loss in iteration 111 : 3.6399255506161266E-4
Loss in iteration 112 : 3.322276921807696E-4
Loss in iteration 113 : 2.990520479565266E-4
Loss in iteration 114 : 2.6657879140184267E-4
Loss in iteration 115 : 2.33633227731037E-4
Loss in iteration 116 : 2.06655248111465E-4
Loss in iteration 117 : 1.8693137189446983E-4
Loss in iteration 118 : 1.7325100694030366E-4
Loss in iteration 119 : 1.6012372233992716E-4
Loss in iteration 120 : 1.472133944306333E-4
Loss in iteration 121 : 1.345524255746982E-4
Loss in iteration 122 : 1.2192043612658264E-4
Loss in iteration 123 : 1.0931167804364747E-4
Loss in iteration 124 : 9.712752113494361E-5
Loss in iteration 125 : 8.448205160584392E-5
Loss in iteration 126 : 7.538934375243502E-5
Loss in iteration 127 : 7.001493114998811E-5
Testing accuracy  of updater 8 on alg 1 with rate 0.2 = 0.9955555555555555, training accuracy 1.0, time elapsed: 3042 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7747888291792298
Loss in iteration 3 : 0.49524530276340656
Loss in iteration 4 : 0.3302215474069816
Loss in iteration 5 : 0.21202236750513015
Loss in iteration 6 : 0.18901446682275638
Loss in iteration 7 : 0.17267967787571825
Loss in iteration 8 : 0.155176731735353
Loss in iteration 9 : 0.13667948836943192
Loss in iteration 10 : 0.11868914702192644
Loss in iteration 11 : 0.10361073797187202
Loss in iteration 12 : 0.09178373852828253
Loss in iteration 13 : 0.08254885097919341
Loss in iteration 14 : 0.0741032999120887
Loss in iteration 15 : 0.06719618484841118
Loss in iteration 16 : 0.061314175666934395
Loss in iteration 17 : 0.05616412482002879
Loss in iteration 18 : 0.051741321472442973
Loss in iteration 19 : 0.047730562185433076
Loss in iteration 20 : 0.044107651913816145
Loss in iteration 21 : 0.0408314127007436
Loss in iteration 22 : 0.03776460912899556
Loss in iteration 23 : 0.03486794291967123
Loss in iteration 24 : 0.032223379526552805
Loss in iteration 25 : 0.029849353728554517
Loss in iteration 26 : 0.027635530586982114
Loss in iteration 27 : 0.02574927919091808
Loss in iteration 28 : 0.024087211677269748
Loss in iteration 29 : 0.02259778962550292
Loss in iteration 30 : 0.021275143081307773
Loss in iteration 31 : 0.020210119007342604
Loss in iteration 32 : 0.019267896681781777
Loss in iteration 33 : 0.01840303546000089
Loss in iteration 34 : 0.01762510404997316
Loss in iteration 35 : 0.016883490929148582
Loss in iteration 36 : 0.01621832917182958
Loss in iteration 37 : 0.0156013014937705
Loss in iteration 38 : 0.01504469182733257
Loss in iteration 39 : 0.014527904653148432
Loss in iteration 40 : 0.014051405926774837
Loss in iteration 41 : 0.013601785001986173
Loss in iteration 42 : 0.013176113020897606
Loss in iteration 43 : 0.012762840498781628
Loss in iteration 44 : 0.012355914336910797
Loss in iteration 45 : 0.01195653970484953
Loss in iteration 46 : 0.011566621907632697
Loss in iteration 47 : 0.011185299508491744
Loss in iteration 48 : 0.010810370920493003
Loss in iteration 49 : 0.010455360037205848
Loss in iteration 50 : 0.010118149295061154
Loss in iteration 51 : 0.009788147545525469
Loss in iteration 52 : 0.009464508449283855
Loss in iteration 53 : 0.009150989352886947
Loss in iteration 54 : 0.008851577277535043
Loss in iteration 55 : 0.008566917564664848
Loss in iteration 56 : 0.00828830404705907
Loss in iteration 57 : 0.00801109124094725
Loss in iteration 58 : 0.007738072268009428
Loss in iteration 59 : 0.007478130919515936
Loss in iteration 60 : 0.007226783285185699
Loss in iteration 61 : 0.006980200840072152
Loss in iteration 62 : 0.006749043386781224
Loss in iteration 63 : 0.006538411054007688
Loss in iteration 64 : 0.006336739980512163
Loss in iteration 65 : 0.0061410999845591405
Loss in iteration 66 : 0.005955591166091644
Loss in iteration 67 : 0.0057722622055563074
Loss in iteration 68 : 0.005598024341304237
Loss in iteration 69 : 0.005431768337269305
Loss in iteration 70 : 0.0052759063915512865
Loss in iteration 71 : 0.005125143853680112
Loss in iteration 72 : 0.004986365353224913
Loss in iteration 73 : 0.004856575387109187
Loss in iteration 74 : 0.004734342152394989
Loss in iteration 75 : 0.0046167310142081345
Loss in iteration 76 : 0.0045020389496599885
Loss in iteration 77 : 0.00439000922443137
Loss in iteration 78 : 0.004283298807028876
Loss in iteration 79 : 0.004183838483778193
Loss in iteration 80 : 0.004089895506023359
Loss in iteration 81 : 0.003997730961006492
Loss in iteration 82 : 0.003906356128988435
Loss in iteration 83 : 0.0038161834228492876
Loss in iteration 84 : 0.003727772404704805
Loss in iteration 85 : 0.00364216774381349
Loss in iteration 86 : 0.003558005205278125
Loss in iteration 87 : 0.0034744608717433498
Loss in iteration 88 : 0.0033914648624182838
Loss in iteration 89 : 0.003310057767198561
Loss in iteration 90 : 0.0032327868808885185
Loss in iteration 91 : 0.0031590855519969953
Loss in iteration 92 : 0.00308796106425713
Loss in iteration 93 : 0.003019241161266184
Loss in iteration 94 : 0.002951190152163968
Loss in iteration 95 : 0.0028869559565407065
Loss in iteration 96 : 0.0028255420672672815
Loss in iteration 97 : 0.002767159617790131
Loss in iteration 98 : 0.00271256413564671
Loss in iteration 99 : 0.002660408708092953
Loss in iteration 100 : 0.002612538751808735
Loss in iteration 101 : 0.002569219201802612
Loss in iteration 102 : 0.002526570421815904
Loss in iteration 103 : 0.002484422474852878
Loss in iteration 104 : 0.002442970378510582
Loss in iteration 105 : 0.002402806637734706
Loss in iteration 106 : 0.002363272870837092
Loss in iteration 107 : 0.0023247064267795737
Loss in iteration 108 : 0.0022889800287506975
Loss in iteration 109 : 0.0022569944079312634
Loss in iteration 110 : 0.002227987243647483
Loss in iteration 111 : 0.0021995891705084126
Loss in iteration 112 : 0.002171647970953543
Loss in iteration 113 : 0.0021440298120017393
Loss in iteration 114 : 0.002116914560487011
Loss in iteration 115 : 0.002090053201354262
Loss in iteration 116 : 0.0020632875162415352
Loss in iteration 117 : 0.0020372942596957907
Loss in iteration 118 : 0.0020126882692630263
Loss in iteration 119 : 0.0019896883137123307
Loss in iteration 120 : 0.0019678322503263253
Loss in iteration 121 : 0.0019478680698129054
Loss in iteration 122 : 0.0019287038290978957
Loss in iteration 123 : 0.0019107425995207924
Loss in iteration 124 : 0.0018925912199796579
Loss in iteration 125 : 0.001874265201809626
Loss in iteration 126 : 0.0018564099972875068
Loss in iteration 127 : 0.0018398354442102506
Loss in iteration 128 : 0.0018237633009278399
Loss in iteration 129 : 0.0018080589148064088
Loss in iteration 130 : 0.0017936286343533576
Loss in iteration 131 : 0.0017792085307818558
Loss in iteration 132 : 0.0017644569233050166
Loss in iteration 133 : 0.0017501686171583672
Testing accuracy  of updater 8 on alg 1 with rate 0.01999999999999999 = 0.9875555555555555, training accuracy 0.9994284081166047, time elapsed: 3233 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.77421276813093
Loss in iteration 3 : 2.1406087551333473
Loss in iteration 4 : 2.198250367433063
Loss in iteration 5 : 4.264163876953011
Loss in iteration 6 : 2.754888715499706
Loss in iteration 7 : 1.7061935640336705
Loss in iteration 8 : 1.0681048199112477
Loss in iteration 9 : 0.9259038202285844
Loss in iteration 10 : 1.1758721543042967
Loss in iteration 11 : 1.365766179297021
Loss in iteration 12 : 1.1529277173116597
Loss in iteration 13 : 0.794298890936367
Loss in iteration 14 : 0.5251126860863572
Loss in iteration 15 : 0.36826316168628415
Loss in iteration 16 : 0.3092765847493408
Loss in iteration 17 : 0.29809272599789155
Loss in iteration 18 : 0.31137322341099727
Loss in iteration 19 : 0.318419399947519
Loss in iteration 20 : 0.31410787394559725
Loss in iteration 21 : 0.3005985665724599
Loss in iteration 22 : 0.2725500868779413
Loss in iteration 23 : 0.239817678249702
Loss in iteration 24 : 0.21077758844030092
Loss in iteration 25 : 0.18135184800863588
Loss in iteration 26 : 0.16119670445164314
Loss in iteration 27 : 0.14969383624058438
Loss in iteration 28 : 0.14469142676976443
Loss in iteration 29 : 0.14194204787282927
Loss in iteration 30 : 0.14231527840595504
Loss in iteration 31 : 0.14306247101550693
Loss in iteration 32 : 0.14314047879532776
Loss in iteration 33 : 0.14245604725620803
Loss in iteration 34 : 0.14040552853768554
Loss in iteration 35 : 0.1370563235177466
Loss in iteration 36 : 0.13317459203351695
Loss in iteration 37 : 0.1291500803665544
Loss in iteration 38 : 0.12461702873804455
Loss in iteration 39 : 0.11969387457708185
Loss in iteration 40 : 0.11545181025723808
Loss in iteration 41 : 0.11241054015596597
Loss in iteration 42 : 0.11010892838749389
Loss in iteration 43 : 0.10779519928013018
Loss in iteration 44 : 0.10547044743093957
Loss in iteration 45 : 0.10313565838186792
Loss in iteration 46 : 0.10079171948041585
Loss in iteration 47 : 0.0984394296587115
Loss in iteration 48 : 0.09607950823869894
Loss in iteration 49 : 0.09371260286042869
Loss in iteration 50 : 0.0913392966207796
Loss in iteration 51 : 0.08896011450124429
Loss in iteration 52 : 0.08657552915557905
Loss in iteration 53 : 0.08418596612106811
Loss in iteration 54 : 0.0817918085108043
Loss in iteration 55 : 0.07968877976513815
Loss in iteration 56 : 0.07764094139318446
Loss in iteration 57 : 0.0756273733701916
Loss in iteration 58 : 0.07385283266492733
Loss in iteration 59 : 0.07227635151521052
Loss in iteration 60 : 0.07066321026837755
Loss in iteration 61 : 0.06904387952412332
Loss in iteration 62 : 0.06750361871450417
Loss in iteration 63 : 0.06584934093445999
Loss in iteration 64 : 0.06409227218209125
Loss in iteration 65 : 0.06224252073730401
Loss in iteration 66 : 0.06030918844226083
Loss in iteration 67 : 0.05830047090271601
Loss in iteration 68 : 0.056223747713274624
Loss in iteration 69 : 0.05408566369979544
Loss in iteration 70 : 0.05206076519828192
Loss in iteration 71 : 0.05014725839413617
Loss in iteration 72 : 0.048802135434535154
Loss in iteration 73 : 0.047535355734835985
Loss in iteration 74 : 0.04624184587603087
Loss in iteration 75 : 0.044924196242569096
Loss in iteration 76 : 0.043584739288297966
Loss in iteration 77 : 0.04222557521634183
Loss in iteration 78 : 0.040848595102263276
Loss in iteration 79 : 0.03946154492508885
Loss in iteration 80 : 0.03815356842743012
Loss in iteration 81 : 0.03684644450855181
Loss in iteration 82 : 0.03554002842214648
Loss in iteration 83 : 0.03423418981146168
Loss in iteration 84 : 0.03294393205489081
Loss in iteration 85 : 0.03182643432351703
Loss in iteration 86 : 0.030698985000518507
Loss in iteration 87 : 0.029617077896335758
Loss in iteration 88 : 0.02855300267719138
Loss in iteration 89 : 0.027480477122477684
Loss in iteration 90 : 0.026400290546581086
Loss in iteration 91 : 0.025313153640267143
Loss in iteration 92 : 0.024219706300365584
Loss in iteration 93 : 0.023120524679742904
Loss in iteration 94 : 0.022016127535209307
Loss in iteration 95 : 0.020906981943274335
Loss in iteration 96 : 0.019793508446703287
Loss in iteration 97 : 0.018676085688557626
Loss in iteration 98 : 0.01755505458475769
Loss in iteration 99 : 0.01643072208112354
Loss in iteration 100 : 0.01530336453627317
Loss in iteration 101 : 0.014173230767637095
Loss in iteration 102 : 0.013040544794136993
Loss in iteration 103 : 0.011905508305736424
Loss in iteration 104 : 0.01098243285981398
Loss in iteration 105 : 0.01019706615895801
Loss in iteration 106 : 0.009438661424431512
Loss in iteration 107 : 0.008714419070237199
Loss in iteration 108 : 0.007982899382066604
Loss in iteration 109 : 0.007244788642798023
Loss in iteration 110 : 0.00651485442040067
Loss in iteration 111 : 0.0058163949765237585
Loss in iteration 112 : 0.005415370474543722
Loss in iteration 113 : 0.004929681506756699
Loss in iteration 114 : 0.00444390556040978
Loss in iteration 115 : 0.004048961605996391
Loss in iteration 116 : 0.0036554582049765296
Loss in iteration 117 : 0.0032632336918913896
Loss in iteration 118 : 0.0028721424882462566
Loss in iteration 119 : 0.002482053500918749
Loss in iteration 120 : 0.0020928486800183407
Loss in iteration 121 : 0.0017044217203228853
Loss in iteration 122 : 0.0013166768919974457
Loss in iteration 123 : 0.001051698302729964
Testing accuracy  of updater 9 on alg 1 with rate 2.0 = 0.992, training accuracy 0.9998571020291512, time elapsed: 2768 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4774967690967379
Loss in iteration 3 : 0.21330223706898307
Loss in iteration 4 : 0.18515797150445804
Loss in iteration 5 : 0.2434116717933124
Loss in iteration 6 : 0.1448672733643349
Loss in iteration 7 : 0.09251647407209802
Loss in iteration 8 : 0.07460793295116283
Loss in iteration 9 : 0.08434340491749029
Loss in iteration 10 : 0.08893194166041132
Loss in iteration 11 : 0.0701552601001973
Loss in iteration 12 : 0.048938746659490114
Loss in iteration 13 : 0.033975683151090834
Loss in iteration 14 : 0.02684120338087679
Loss in iteration 15 : 0.02534581757664192
Loss in iteration 16 : 0.02499332911974209
Loss in iteration 17 : 0.02557283238672547
Loss in iteration 18 : 0.025319512384839635
Loss in iteration 19 : 0.023923064677079945
Loss in iteration 20 : 0.021559065960500088
Loss in iteration 21 : 0.01883880092537857
Loss in iteration 22 : 0.016520155783522072
Loss in iteration 23 : 0.014636645624533073
Loss in iteration 24 : 0.013321346601269027
Loss in iteration 25 : 0.012602062333454811
Loss in iteration 26 : 0.012210030520751028
Loss in iteration 27 : 0.011927124054926967
Loss in iteration 28 : 0.011684332561494691
Loss in iteration 29 : 0.01141180435836146
Loss in iteration 30 : 0.011041063835486111
Loss in iteration 31 : 0.010586052022832651
Loss in iteration 32 : 0.010025200127784299
Loss in iteration 33 : 0.009465586564409934
Loss in iteration 34 : 0.008894437630590953
Loss in iteration 35 : 0.008372511864266288
Loss in iteration 36 : 0.007979108961071353
Loss in iteration 37 : 0.007640650704949216
Loss in iteration 38 : 0.007378988211081612
Loss in iteration 39 : 0.007172340371530507
Loss in iteration 40 : 0.00700621194034173
Loss in iteration 41 : 0.006873101718146373
Loss in iteration 42 : 0.006738454164440958
Loss in iteration 43 : 0.006602415236561904
Loss in iteration 44 : 0.00646569703237897
Loss in iteration 45 : 0.006355148651159417
Loss in iteration 46 : 0.00624036085972131
Loss in iteration 47 : 0.006114052134572195
Loss in iteration 48 : 0.005977358366612127
Loss in iteration 49 : 0.005831302357650797
Loss in iteration 50 : 0.005676805079032404
Loss in iteration 51 : 0.005514695809404501
Loss in iteration 52 : 0.005345721263217925
Loss in iteration 53 : 0.005187504714482282
Loss in iteration 54 : 0.005030925699157365
Loss in iteration 55 : 0.004879544182975652
Loss in iteration 56 : 0.004731880907665016
Loss in iteration 57 : 0.004582963531299607
Loss in iteration 58 : 0.004432909231974779
Loss in iteration 59 : 0.004291450259129846
Loss in iteration 60 : 0.004162610156382859
Loss in iteration 61 : 0.004032851604340054
Loss in iteration 62 : 0.003901056794954657
Loss in iteration 63 : 0.0037738960434062557
Loss in iteration 64 : 0.003657957775715121
Loss in iteration 65 : 0.0035545565348641893
Loss in iteration 66 : 0.0034512409554607283
Loss in iteration 67 : 0.003346002155399591
Loss in iteration 68 : 0.0032390257506568873
Loss in iteration 69 : 0.003131956452059218
Loss in iteration 70 : 0.0030299973609157144
Loss in iteration 71 : 0.0029284450152731595
Loss in iteration 72 : 0.002827254256013554
Loss in iteration 73 : 0.0027263844179770363
Loss in iteration 74 : 0.0026257988825466377
Loss in iteration 75 : 0.0025254646747772612
Loss in iteration 76 : 0.0024253521006341308
Loss in iteration 77 : 0.0023254344203474425
Loss in iteration 78 : 0.002225687554287516
Loss in iteration 79 : 0.0021260898181227578
Loss in iteration 80 : 0.0020266216843450526
Loss in iteration 81 : 0.001930746039221856
Loss in iteration 82 : 0.0018416449949896566
Loss in iteration 83 : 0.0017625097321871497
Loss in iteration 84 : 0.0016825276148109518
Loss in iteration 85 : 0.0016017788321084005
Loss in iteration 86 : 0.0015203355862966764
Loss in iteration 87 : 0.0014382628879102753
Loss in iteration 88 : 0.0013556192719480446
Loss in iteration 89 : 0.0012724574427064883
Loss in iteration 90 : 0.0011888248544007505
Loss in iteration 91 : 0.0011047642339674908
Loss in iteration 92 : 0.0010203140518071567
Loss in iteration 93 : 9.363449343833264E-4
Loss in iteration 94 : 8.527742646819953E-4
Loss in iteration 95 : 7.674791076268006E-4
Loss in iteration 96 : 6.8062635026178E-4
Loss in iteration 97 : 5.954955559535103E-4
Loss in iteration 98 : 5.33056154794483E-4
Loss in iteration 99 : 4.7496353967808554E-4
Loss in iteration 100 : 4.1856936441849666E-4
Loss in iteration 101 : 3.631587231858287E-4
Loss in iteration 102 : 3.1412136632436716E-4
Loss in iteration 103 : 2.660882748711498E-4
Loss in iteration 104 : 2.1788086408948827E-4
Loss in iteration 105 : 1.8400420094890268E-4
Loss in iteration 106 : 1.570259359940438E-4
Loss in iteration 107 : 1.3112368735597097E-4
Loss in iteration 108 : 1.0766092294453006E-4
Loss in iteration 109 : 8.545771354674787E-5
Loss in iteration 110 : 6.414777148250423E-5
Loss in iteration 111 : 4.2608112166047097E-5
Loss in iteration 112 : 2.0860461671327317E-5
Loss in iteration 113 : 0.0
Testing accuracy  of updater 9 on alg 1 with rate 0.2 = 0.9937777777777778, training accuracy 1.0, time elapsed: 2693 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7330193412532204
Loss in iteration 3 : 0.38413591761572646
Loss in iteration 4 : 0.20437066707848547
Loss in iteration 5 : 0.17834787584736986
Loss in iteration 6 : 0.152109339337498
Loss in iteration 7 : 0.1255227234412045
Loss in iteration 8 : 0.10470094932828147
Loss in iteration 9 : 0.08959050665856201
Loss in iteration 10 : 0.07836293484272554
Loss in iteration 11 : 0.06985846261650788
Loss in iteration 12 : 0.06297597745023309
Loss in iteration 13 : 0.05689859556481585
Loss in iteration 14 : 0.05146738874037517
Loss in iteration 15 : 0.04614969906268104
Loss in iteration 16 : 0.04094732486410834
Loss in iteration 17 : 0.03632225131251161
Loss in iteration 18 : 0.032347981303044226
Loss in iteration 19 : 0.02897893749073407
Loss in iteration 20 : 0.026155851890786085
Loss in iteration 21 : 0.02373100491440524
Loss in iteration 22 : 0.021721064671395385
Loss in iteration 23 : 0.01999648294728749
Loss in iteration 24 : 0.018464971418862285
Loss in iteration 25 : 0.01718419504210839
Loss in iteration 26 : 0.016092731263341174
Loss in iteration 27 : 0.01516643370461306
Loss in iteration 28 : 0.014398169952109229
Loss in iteration 29 : 0.01369409938032105
Loss in iteration 30 : 0.013021647803161953
Loss in iteration 31 : 0.012411802609165993
Loss in iteration 32 : 0.01185209287468693
Loss in iteration 33 : 0.011308804873747644
Loss in iteration 34 : 0.010776404558507839
Loss in iteration 35 : 0.010244432868908011
Loss in iteration 36 : 0.009710759228859691
Loss in iteration 37 : 0.009175024953437848
Loss in iteration 38 : 0.008664508538158141
Loss in iteration 39 : 0.008189823488537805
Loss in iteration 40 : 0.007772399643949866
Loss in iteration 41 : 0.0073674939025631745
Loss in iteration 42 : 0.007015749518296042
Loss in iteration 43 : 0.006680508787649134
Loss in iteration 44 : 0.006352325299122612
Loss in iteration 45 : 0.006051335532477839
Loss in iteration 46 : 0.00576996300611673
Loss in iteration 47 : 0.005517165986476297
Loss in iteration 48 : 0.0052766829640705564
Loss in iteration 49 : 0.005075647979227228
Loss in iteration 50 : 0.004909494424865373
Loss in iteration 51 : 0.0047462572846471325
Loss in iteration 52 : 0.004585128863338121
Loss in iteration 53 : 0.004432823648969008
Loss in iteration 54 : 0.00429735330617954
Loss in iteration 55 : 0.004170761483809298
Loss in iteration 56 : 0.004043496274422058
Loss in iteration 57 : 0.003919322312417835
Loss in iteration 58 : 0.0038020144021433575
Loss in iteration 59 : 0.0036869114859872684
Loss in iteration 60 : 0.0035986113077393304
Loss in iteration 61 : 0.0035186660297461757
Loss in iteration 62 : 0.0034420672322789518
Loss in iteration 63 : 0.003369162146815392
Loss in iteration 64 : 0.0032948841457173766
Loss in iteration 65 : 0.003220049619209977
Loss in iteration 66 : 0.003146368579479006
Loss in iteration 67 : 0.0030741222782457596
Loss in iteration 68 : 0.0030035891292646685
Loss in iteration 69 : 0.002934555818601397
Loss in iteration 70 : 0.0028692633207088843
Loss in iteration 71 : 0.0028104571317531858
Loss in iteration 72 : 0.0027545675717869597
Loss in iteration 73 : 0.002704367213163633
Loss in iteration 74 : 0.0026591389734281616
Loss in iteration 75 : 0.0026167373263885677
Loss in iteration 76 : 0.0025786064229970183
Loss in iteration 77 : 0.002541513143084914
Loss in iteration 78 : 0.0025069441321724393
Loss in iteration 79 : 0.002473676927684764
Loss in iteration 80 : 0.0024408455977293197
Loss in iteration 81 : 0.002413617648597065
Loss in iteration 82 : 0.002389204544729892
Loss in iteration 83 : 0.002365585047609964
Loss in iteration 84 : 0.0023433300637081293
Loss in iteration 85 : 0.0023210116919149927
Loss in iteration 86 : 0.002298635123201034
Loss in iteration 87 : 0.0022762050313897006
Loss in iteration 88 : 0.002253725624625215
Loss in iteration 89 : 0.0022313861903777666
Loss in iteration 90 : 0.002209874861684223
Loss in iteration 91 : 0.002188788459883797
Loss in iteration 92 : 0.002167951448959219
Loss in iteration 93 : 0.0021471651254437014
Loss in iteration 94 : 0.002126010849872429
Loss in iteration 95 : 0.0021050379036703787
Loss in iteration 96 : 0.002084017170476849
Loss in iteration 97 : 0.0020629523593961277
Loss in iteration 98 : 0.0020420978883168893
Loss in iteration 99 : 0.0020217019546004433
Loss in iteration 100 : 0.0020018593875029116
Testing accuracy  of updater 9 on alg 1 with rate 0.01999999999999999 = 0.9848888888888889, training accuracy 0.999285510145756, time elapsed: 2227 millisecond.
