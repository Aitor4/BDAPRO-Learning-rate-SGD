objc[880]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x107c644c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x107ce84e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 18:21:51 INFO SparkContext: Running Spark version 2.0.0
18/02/26 18:21:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 18:21:51 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 18:21:51 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 18:21:51 INFO SecurityManager: Changing view acls groups to: 
18/02/26 18:21:51 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 18:21:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 18:21:52 INFO Utils: Successfully started service 'sparkDriver' on port 49887.
18/02/26 18:21:52 INFO SparkEnv: Registering MapOutputTracker
18/02/26 18:21:52 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 18:21:52 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-7f6ba50c-d98a-41ca-bc29-07c916d20185
18/02/26 18:21:52 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 18:21:52 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 18:21:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 18:21:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 18:21:53 INFO Executor: Starting executor ID driver on host localhost
18/02/26 18:21:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49888.
18/02/26 18:21:53 INFO NettyBlockTransferService: Server created on 192.168.2.140:49888
18/02/26 18:21:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 49888)
18/02/26 18:21:53 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:49888 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 49888)
18/02/26 18:21:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 49888)
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 2.141549710106488
Loss in iteration 3 : 22.853611911924023
Loss in iteration 4 : 7.21335341680556
Loss in iteration 5 : 16.31191673601085
Loss in iteration 6 : 11.442165808524548
Loss in iteration 7 : 9.853829878915363
Loss in iteration 8 : 14.82284181691413
Loss in iteration 9 : 5.208611633251837
Loss in iteration 10 : 14.418368881409089
Loss in iteration 11 : 5.027456977489407
Loss in iteration 12 : 13.16121341794933
Loss in iteration 13 : 5.731375784753711
Loss in iteration 14 : 12.185442371765923
Loss in iteration 15 : 6.159044453681352
Loss in iteration 16 : 11.112710431471651
Loss in iteration 17 : 6.636997211211101
Loss in iteration 18 : 10.102649849043829
Loss in iteration 19 : 6.9678892811581665
Loss in iteration 20 : 9.149213153490695
Loss in iteration 21 : 7.157181689747246
Loss in iteration 22 : 8.298333441163864
Loss in iteration 23 : 7.163533922187165
Loss in iteration 24 : 7.5925123345224526
Loss in iteration 25 : 7.02406964037165
Loss in iteration 26 : 7.001492905922772
Loss in iteration 27 : 6.8358197263641465
Loss in iteration 28 : 6.46063891285564
Loss in iteration 29 : 6.638765314460299
Loss in iteration 30 : 6.033601126197938
Loss in iteration 31 : 6.4398244301918455
Loss in iteration 32 : 5.710873207592277
Loss in iteration 33 : 6.245350939543627
Loss in iteration 34 : 5.443091679953739
Loss in iteration 35 : 6.05940758122028
Loss in iteration 36 : 5.200301852161027
Loss in iteration 37 : 5.885788329523017
Loss in iteration 38 : 5.004616699027921
Loss in iteration 39 : 5.736806682123505
Loss in iteration 40 : 4.851267538800062
Loss in iteration 41 : 5.613576091966786
Loss in iteration 42 : 4.729931568486472
Loss in iteration 43 : 5.512083071690393
Loss in iteration 44 : 4.630276631813032
Loss in iteration 45 : 5.427013934654445
Loss in iteration 46 : 4.545162113838059
Loss in iteration 47 : 5.353566656837553
Loss in iteration 48 : 4.469785388997436
Loss in iteration 49 : 5.2881638570347524
Loss in iteration 50 : 4.4014101221878414
Loss in iteration 51 : 5.2290228656008715
Loss in iteration 52 : 4.338588314309602
Loss in iteration 53 : 5.174854409074381
Loss in iteration 54 : 4.280386792480614
Loss in iteration 55 : 5.124470984959149
Loss in iteration 56 : 4.226226279031746
Loss in iteration 57 : 5.077069142260412
Loss in iteration 58 : 4.175914895832702
Loss in iteration 59 : 5.032397573236789
Loss in iteration 60 : 4.129519093742629
Loss in iteration 61 : 4.990574392627231
Loss in iteration 62 : 4.0871386719404335
Loss in iteration 63 : 4.9520648480287015
Loss in iteration 64 : 4.048840182490215
Loss in iteration 65 : 4.9162612516531645
Loss in iteration 66 : 4.014409499607035
Loss in iteration 67 : 4.874381138066329
Loss in iteration 68 : 3.981769288192354
Loss in iteration 69 : 4.834482774596378
Loss in iteration 70 : 3.952872989644929
Loss in iteration 71 : 4.80226914262368
Loss in iteration 72 : 3.928628877456003
Loss in iteration 73 : 4.773430204168923
Loss in iteration 74 : 3.907532614081969
Loss in iteration 75 : 4.747192746517256
Loss in iteration 76 : 3.8890180694357186
Loss in iteration 77 : 4.723103048137378
Loss in iteration 78 : 3.8726655838956083
Loss in iteration 79 : 4.700407225364924
Loss in iteration 80 : 3.8580813833123844
Loss in iteration 81 : 4.678727245758651
Loss in iteration 82 : 3.84499079525849
Loss in iteration 83 : 4.658333625544983
Loss in iteration 84 : 3.8332396266691875
Loss in iteration 85 : 4.639642696867704
Loss in iteration 86 : 3.822704852484356
Loss in iteration 87 : 4.622707303622456
Loss in iteration 88 : 3.8132326192603303
Loss in iteration 89 : 4.607271357472206
Loss in iteration 90 : 3.804653931589664
Loss in iteration 91 : 4.59300364378205
Loss in iteration 92 : 3.7968150376857235
Loss in iteration 93 : 4.579613750471645
Loss in iteration 94 : 3.7895880692322192
Loss in iteration 95 : 4.566871186531451
Loss in iteration 96 : 3.782868997575615
Loss in iteration 97 : 4.554595405577422
Loss in iteration 98 : 3.7765727914623675
Loss in iteration 99 : 4.54264740050369
Loss in iteration 100 : 3.7706296498440652
Loss in iteration 101 : 4.530928520818157
Loss in iteration 102 : 3.7649828747257943
Loss in iteration 103 : 4.519380758973132
Loss in iteration 104 : 3.759587608339996
Loss in iteration 105 : 4.50798127566556
Loss in iteration 106 : 3.7544094130874313
Loss in iteration 107 : 4.496731860714972
Loss in iteration 108 : 3.7494225376554624
Loss in iteration 109 : 4.485651338605162
Loss in iteration 110 : 3.74460872698373
Loss in iteration 111 : 4.474774423623616
Loss in iteration 112 : 3.739957026183869
Loss in iteration 113 : 4.464150638117977
Loss in iteration 114 : 3.7354635858278264
Loss in iteration 115 : 4.453836230867908
Loss in iteration 116 : 3.7311300825786446
Loss in iteration 117 : 4.4438806884904185
Loss in iteration 118 : 3.72696067226151
Loss in iteration 119 : 4.434315458348565
Loss in iteration 120 : 3.7229587921521317
Loss in iteration 121 : 4.425150136896224
Loss in iteration 122 : 3.7191251801056007
Loss in iteration 123 : 4.416375643569683
Loss in iteration 124 : 3.7154575145051947
Loss in iteration 125 : 4.407970755457076
Loss in iteration 126 : 3.7119512108707644
Loss in iteration 127 : 4.399908642092712
Loss in iteration 128 : 3.70860064494953
Loss in iteration 129 : 4.39216167437689
Loss in iteration 130 : 3.705400237130026
Loss in iteration 131 : 4.3847041783818055
Loss in iteration 132 : 3.70234511522714
Loss in iteration 133 : 4.37751351939515
Loss in iteration 134 : 3.6994313007040422
Loss in iteration 135 : 4.370570095459711
Loss in iteration 136 : 3.6966555014015703
Loss in iteration 137 : 4.363856757756025
Loss in iteration 138 : 3.6940146601231763
Loss in iteration 139 : 4.357358037644811
Loss in iteration 140 : 3.691505424864636
Loss in iteration 141 : 4.351059427891867
Loss in iteration 142 : 3.6891236862679007
Loss in iteration 143 : 4.34494685813272
Loss in iteration 144 : 3.68686428136045
Loss in iteration 145 : 4.339006419114514
Loss in iteration 146 : 3.6847209036573214
Loss in iteration 147 : 4.333224323761524
Loss in iteration 148 : 3.6826862048130953
Loss in iteration 149 : 4.327587047880171
Loss in iteration 150 : 3.680752035595314
Loss in iteration 151 : 4.32208157221888
Loss in iteration 152 : 3.6789097592330062
Loss in iteration 153 : 4.316695649062863
Loss in iteration 154 : 3.677150574991686
Loss in iteration 155 : 4.3114180337981995
Loss in iteration 156 : 3.675465806223042
Loss in iteration 157 : 4.306238645869296
Loss in iteration 158 : 3.6738471269411055
Loss in iteration 159 : 4.301148646516094
Loss in iteration 160 : 3.672286718494077
Loss in iteration 161 : 4.296140438085905
Loss in iteration 162 : 3.670777360526271
Loss in iteration 163 : 4.291207600256283
Loss in iteration 164 : 3.6693124678706273
Loss in iteration 165 : 4.286344782975205
Loss in iteration 166 : 3.6678860881866866
Loss in iteration 167 : 4.281547575994307
Loss in iteration 168 : 3.6664928753025214
Loss in iteration 169 : 4.27681237224576
Loss in iteration 170 : 3.6651280514464997
Loss in iteration 171 : 4.272136238343614
Loss in iteration 172 : 3.6637873687050666
Loss in iteration 173 : 4.267516801100013
Loss in iteration 174 : 3.662467076706321
Loss in iteration 175 : 4.262952154707882
Loss in iteration 176 : 3.6611639001111995
Loss in iteration 177 : 4.258440789492557
Loss in iteration 178 : 3.6598750263065214
Loss in iteration 179 : 4.253981540076023
Loss in iteration 180 : 3.65859810099108
Loss in iteration 181 : 4.249573548557088
Loss in iteration 182 : 3.6573312273540695
Loss in iteration 183 : 4.24521623697674
Loss in iteration 184 : 3.656072963451229
Loss in iteration 185 : 4.240909282953672
Loss in iteration 186 : 3.6548223122986694
Loss in iteration 187 : 4.236652592923428
Loss in iteration 188 : 3.6535787001259092
Loss in iteration 189 : 4.232446268784105
Loss in iteration 190 : 3.652341939997897
Loss in iteration 191 : 4.228290565720401
Loss in iteration 192 : 3.6511121803217663
Loss in iteration 193 : 4.224185841217948
Loss in iteration 194 : 3.64988984016604
Loss in iteration 195 : 4.2201324974019
Loss in iteration 196 : 3.648675535383165
Loss in iteration 197 : 4.2161309204633275
Loss in iteration 198 : 3.647470000862182
Loss in iteration 199 : 4.212181421802627
Loss in iteration 200 : 3.6462740146408303
Loss in iteration 201 : 4.208284185517716
Loss in iteration 202 : 3.645088329089257
Loss in iteration 203 : 4.2044392260842285
Loss in iteration 204 : 3.643913613149095
Loss in iteration 205 : 4.200646358759137
Loss in iteration 206 : 3.6427504080024726
Loss in iteration 207 : 4.19690518370865
Loss in iteration 208 : 3.6415990969083123
Loss in iteration 209 : 4.193215083425861
Loss in iteration 210 : 3.6404598885573534
Loss in iteration 211 : 4.189575231890601
Loss in iteration 212 : 3.639332812329759
Loss in iteration 213 : 4.185984613238528
Loss in iteration 214 : 3.6382177233236415
Loss in iteration 215 : 4.182442047447535
Loss in iteration 216 : 3.637114314900761
Loss in iteration 217 : 4.178946220629478
Loss in iteration 218 : 3.6360221366512095
Loss in iteration 219 : 4.175495717817943
Loss in iteration 220 : 3.6349406159878637
Loss in iteration 221 : 4.172089056545523
Loss in iteration 222 : 3.6338690819380908
Loss in iteration 223 : 4.168724719918424
Loss in iteration 224 : 3.632806790032263
Loss in iteration 225 : 4.165401188261345
Loss in iteration 226 : 3.6317529474594896
Loss in iteration 227 : 4.162116968696684
Loss in iteration 228 : 3.630706737860289
Loss in iteration 229 : 4.158870622236607
Loss in iteration 230 : 3.6296673452604393
Loss in iteration 231 : 4.155660788114266
Loss in iteration 232 : 3.6286339767366007
Loss in iteration 233 : 4.152486205177093
Loss in iteration 234 : 3.627605883456734
Loss in iteration 235 : 4.149345730225275
Loss in iteration 236 : 3.626582379772638
Loss in iteration 237 : 4.146238353213257
Loss in iteration 238 : 3.625562860067961
Loss in iteration 239 : 4.1431632092502
Loss in iteration 240 : 3.62454681308838
Loss in iteration 241 : 4.140119587341929
Loss in iteration 242 : 3.6235338335084717
Loss in iteration 243 : 4.137106935816736
Loss in iteration 244 : 3.6225236305219704
Loss in iteration 245 : 4.134124864375086
Loss in iteration 246 : 3.6215160332843324
Loss in iteration 247 : 4.131173142704252
Loss in iteration 248 : 3.620510993087797
Loss in iteration 249 : 4.128251695608596
Loss in iteration 250 : 3.619508582213916
Loss in iteration 251 : 4.125360594632132
Loss in iteration 252 : 3.6185089894852775
Loss in iteration 253 : 4.122500046197112
Loss in iteration 254 : 3.6175125126280316
Loss in iteration 255 : 4.1196703763553595
Loss in iteration 256 : 3.6165195476567407
Loss in iteration 257 : 4.116872012348837
Loss in iteration 258 : 3.615530575599816
Loss in iteration 259 : 4.114105461300891
Loss in iteration 260 : 3.614546146990018
Loss in iteration 261 : 4.1113712865012335
Loss in iteration 262 : 3.6135668646445964
Loss in iteration 263 : 4.108670081896606
Loss in iteration 264 : 3.6125933653426654
Loss in iteration 265 : 4.10600244553923
Loss in iteration 266 : 3.6116263010680427
Loss in iteration 267 : 4.1033689528623345
Loss in iteration 268 : 3.610666320513792
Loss in iteration 269 : 4.100770130729162
Loss in iteration 270 : 3.6097140515380155
Loss in iteration 271 : 4.098206433228411
Loss in iteration 272 : 3.6087700852150215
Loss in iteration 273 : 4.095678220156644
Loss in iteration 274 : 3.6078349620455175
Loss in iteration 275 : 4.09318573903621
Loss in iteration 276 : 3.6069091607774664
Loss in iteration 277 : 4.090729111371614
Loss in iteration 278 : 3.6059930901551014
Loss in iteration 279 : 4.088308323658588
Loss in iteration 280 : 3.60508708376735
Loss in iteration 281 : 4.085923223444066
Loss in iteration 282 : 3.6041913980177798
Loss in iteration 283 : 4.083573520507995
Loss in iteration 284 : 3.60330621309908
Loss in iteration 285 : 4.0812587930167314
Loss in iteration 286 : 3.6024316367312235
Loss in iteration 287 : 4.078978498294361
Loss in iteration 288 : 3.6015677103221706
Loss in iteration 289 : 4.0767319876843855
Loss in iteration 290 : 3.600714417135504
Loss in iteration 291 : 4.074518524833543
Loss in iteration 292 : 3.5998716920001854
Loss in iteration 293 : 4.072337306625073
Loss in iteration 294 : 3.5990394320742314
Loss in iteration 295 : 4.070187485917973
Loss in iteration 296 : 3.5982175081705314
Loss in iteration 297 : 4.068068195210206
Loss in iteration 298 : 3.5974057761680105
Loss in iteration 299 : 4.0659785703332325
Loss in iteration 300 : 3.5966040880592574
Loss in iteration 301 : 4.0639177733005
Loss in iteration 302 : 3.595812302224891
Loss in iteration 303 : 4.06188501347323
Loss in iteration 304 : 3.5950302925715585
Loss in iteration 305 : 4.059879566271955
Loss in iteration 306 : 3.594257956225869
Loss in iteration 307 : 4.057900788754301
Loss in iteration 308 : 3.5934952195374654
Loss in iteration 309 : 4.055948131498661
Loss in iteration 310 : 3.5927420422131497
Loss in iteration 311 : 4.054021146379425
Loss in iteration 312 : 3.591998419478875
Loss in iteration 313 : 4.052119489991428
Loss in iteration 314 : 3.591264382246869
Loss in iteration 315 : 4.05024292267123
Loss in iteration 316 : 3.5905399953490953
Loss in iteration 317 : 4.0483913032647845
Loss in iteration 318 : 3.5898253539823237
Loss in iteration 319 : 4.046564579990998
Loss in iteration 320 : 3.5891205785893905
Loss in iteration 321 : 4.044762777935828
Loss in iteration 322 : 3.588425808471211
Loss in iteration 323 : 4.0429859838678945
Loss in iteration 324 : 3.5877411944789275
Loss in iteration 325 : 4.041234329180948
Loss in iteration 326 : 3.5870668911705654
Loss in iteration 327 : 4.039507971831803
Loss in iteration 328 : 3.58640304882903
Loss in iteration 329 : 4.037807078150109
Loss in iteration 330 : 3.585749805726824
Loss in iteration 331 : 4.036131805348677
Loss in iteration 332 : 3.585107280988026
Loss in iteration 333 : 4.0344822854664155
Loss in iteration 334 : 3.5844755683435543
Loss in iteration 335 : 4.032858611340443
Loss in iteration 336 : 3.5838547310072046
Loss in iteration 337 : 4.031260825042988
Loss in iteration 338 : 3.583244797821594
Loss in iteration 339 : 4.029688909046982
Loss in iteration 340 : 3.5826457607438615
Loss in iteration 341 : 4.028142780215783
Loss in iteration 342 : 3.5820575736651916
Loss in iteration 343 : 4.026622286559125
Loss in iteration 344 : 3.5814801524914164
Loss in iteration 345 : 4.0251272065688735
Loss in iteration 346 : 3.580913376358629
Loss in iteration 347 : 4.023657250848922
Loss in iteration 348 : 3.5803570898184858
Loss in iteration 349 : 4.022212065687053
Loss in iteration 350 : 3.5798111058044007
Loss in iteration 351 : 4.020791238180441
Loss in iteration 352 : 3.579275209180416
Loss in iteration 353 : 4.019394302518752
Loss in iteration 354 : 3.5787491606780866
Loss in iteration 355 : 4.018020747043091
Loss in iteration 356 : 3.5782327010391626
Loss in iteration 357 : 4.016670021731621
Loss in iteration 358 : 3.5777255552033234
Loss in iteration 359 : 4.015341545806286
Loss in iteration 360 : 3.577227436403763
Loss in iteration 361 : 4.014034715204965
Loss in iteration 362 : 3.576738050061459
Loss in iteration 363 : 4.012748909716323
Loss in iteration 364 : 3.576257097394713
Loss in iteration 365 : 4.011483499624715
Loss in iteration 366 : 3.5757842786868888
Loss in iteration 367 : 4.010237851759911
Loss in iteration 368 : 3.575319296177059
Loss in iteration 369 : 4.009011334887304
Loss in iteration 370 : 3.574861856558182
Loss in iteration 371 : 4.00780332440939
Loss in iteration 372 : 3.574411673083468
Loss in iteration 373 : 4.006613206377575
Loss in iteration 374 : 3.5739684672937266
Loss in iteration 375 : 4.005440380835369
Loss in iteration 376 : 3.573531970388024
Loss in iteration 377 : 4.004284264529793
Loss in iteration 378 : 3.5731019242661617
Loss in iteration 379 : 4.0031442930394805
Loss in iteration 380 : 3.57267808227548
Loss in iteration 381 : 4.002019922374044
Loss in iteration 382 : 3.572260209696241
Loss in iteration 383 : 4.000910630103179
Loss in iteration 384 : 3.571848084000234
Loss in iteration 385 : 3.999815916073753
Loss in iteration 386 : 3.5714414949158626
Loss in iteration 387 : 3.998735302772431
Loss in iteration 388 : 3.5710402443319462
Loss in iteration 389 : 3.997668335387827
Loss in iteration 390 : 3.570644146069061
Loss in iteration 391 : 3.996614581622478
Loss in iteration 392 : 3.5702530255453016
Loss in iteration 393 : 3.995573631300329
Loss in iteration 394 : 3.5698667193600615
Loss in iteration 395 : 3.9945450958108384
Loss in iteration 396 : 3.5694850748164937
Loss in iteration 397 : 3.993528607425822
Loss in iteration 398 : 3.5691079494007165
Loss in iteration 399 : 3.9925238185204144
Loss in iteration 400 : 3.5687352102329815
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.722375, training accuracy 0.722375, time elapsed: 8936 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 1.54457676584438
Loss in iteration 3 : 14.976574100308936
Loss in iteration 4 : 5.913120927477442
Loss in iteration 5 : 10.391293331019405
Loss in iteration 6 : 8.877626532625086
Loss in iteration 7 : 5.916936722393808
Loss in iteration 8 : 10.872142003493897
Loss in iteration 9 : 3.1978435354988117
Loss in iteration 10 : 10.012385159536603
Loss in iteration 11 : 3.6505761133236585
Loss in iteration 12 : 9.390779742550432
Loss in iteration 13 : 3.8860799421792245
Loss in iteration 14 : 8.59985114252457
Loss in iteration 15 : 4.286850914166156
Loss in iteration 16 : 7.890998143262184
Loss in iteration 17 : 4.577237825248421
Loss in iteration 18 : 7.1740600643498205
Loss in iteration 19 : 4.819044478524913
Loss in iteration 20 : 6.507773728571595
Loss in iteration 21 : 4.953327112631867
Loss in iteration 22 : 5.90810193229686
Loss in iteration 23 : 4.973404411267034
Loss in iteration 24 : 5.398866955426425
Loss in iteration 25 : 4.895580400853669
Loss in iteration 26 : 4.972469618724163
Loss in iteration 27 : 4.772651629739904
Loss in iteration 28 : 4.601967088588724
Loss in iteration 29 : 4.636315122418397
Loss in iteration 30 : 4.299755494125269
Loss in iteration 31 : 4.495588621551491
Loss in iteration 32 : 4.059916448554175
Loss in iteration 33 : 4.35824068261466
Loss in iteration 34 : 3.8609313082420145
Loss in iteration 35 : 4.229221189274453
Loss in iteration 36 : 3.691053477174858
Loss in iteration 37 : 4.1115077898703145
Loss in iteration 38 : 3.5529460518408986
Loss in iteration 39 : 4.009075690547205
Loss in iteration 40 : 3.4421653604228633
Loss in iteration 41 : 3.9222492597739564
Loss in iteration 42 : 3.3521483256497167
Loss in iteration 43 : 3.848948567135856
Loss in iteration 44 : 3.277271961008926
Loss in iteration 45 : 3.786524401878396
Loss in iteration 46 : 3.213333133858253
Loss in iteration 47 : 3.7324395792870435
Loss in iteration 48 : 3.157366383364545
Loss in iteration 49 : 3.684683654974607
Loss in iteration 50 : 3.107408088550962
Loss in iteration 51 : 3.641895346823997
Loss in iteration 52 : 3.0622299340862402
Loss in iteration 53 : 3.603106562863248
Loss in iteration 54 : 3.0210801332888497
Loss in iteration 55 : 3.5675735647347304
Loss in iteration 56 : 2.983488126751706
Loss in iteration 57 : 3.534748485175493
Loss in iteration 58 : 2.9491386295071065
Loss in iteration 59 : 3.504269981406661
Loss in iteration 60 : 2.917794594553521
Loss in iteration 61 : 3.475940304760524
Loss in iteration 62 : 2.8892580539532347
Loss in iteration 63 : 3.4495604017963712
Loss in iteration 64 : 2.863332944535673
Loss in iteration 65 : 3.4240125355585014
Loss in iteration 66 : 2.8396645043461093
Loss in iteration 67 : 3.3955514318517293
Loss in iteration 68 : 2.8173648045625588
Loss in iteration 69 : 3.3677857600832573
Loss in iteration 70 : 2.7971720157548576
Loss in iteration 71 : 3.344751051283556
Loss in iteration 72 : 2.7797842928249366
Loss in iteration 73 : 3.3245131629900584
Loss in iteration 74 : 2.7645050905331017
Loss in iteration 75 : 3.305986918473196
Loss in iteration 76 : 2.7508884339950894
Loss in iteration 77 : 3.2887843011656135
Loss in iteration 78 : 2.7386812268418415
Loss in iteration 79 : 3.2726829323269184
Loss in iteration 80 : 2.7276919056105697
Loss in iteration 81 : 3.2575598204532756
Loss in iteration 82 : 2.717767928569785
Loss in iteration 83 : 3.2434000121463717
Loss in iteration 84 : 2.708791402244165
Loss in iteration 85 : 3.230230068013828
Loss in iteration 86 : 2.7006635193479576
Loss in iteration 87 : 3.218034175800677
Loss in iteration 88 : 2.6932887555416594
Loss in iteration 89 : 3.206728465237322
Loss in iteration 90 : 2.6865707545071227
Loss in iteration 91 : 3.1961861822367434
Loss in iteration 92 : 2.680416935124791
Loss in iteration 93 : 3.1862716607797563
Loss in iteration 94 : 2.6747438674159163
Loss in iteration 95 : 3.176861888982969
Loss in iteration 96 : 2.669479910165653
Loss in iteration 97 : 3.1678555738534486
Loss in iteration 98 : 2.6645654592048906
Loss in iteration 99 : 3.1591751704162836
Loss in iteration 100 : 2.6599520111903634
Loss in iteration 101 : 3.150765699847211
Loss in iteration 102 : 2.6556008223179663
Loss in iteration 103 : 3.142592044322154
Loss in iteration 104 : 2.6514814811301295
Loss in iteration 105 : 3.134635455559511
Loss in iteration 106 : 2.6475705093246833
Loss in iteration 107 : 3.1268897902822808
Loss in iteration 108 : 2.6438500532156533
Loss in iteration 109 : 3.1193578251592347
Loss in iteration 110 : 2.6403066954087198
Loss in iteration 111 : 3.1120477612828124
Loss in iteration 112 : 2.6369303682931506
Loss in iteration 113 : 3.1049699284824963
Loss in iteration 114 : 2.6337133354739253
Loss in iteration 115 : 3.0981338351523493
Loss in iteration 116 : 2.630649249517226
Loss in iteration 117 : 3.091545876416097
Loss in iteration 118 : 2.627732351742124
Loss in iteration 119 : 3.0852079864729696
Loss in iteration 120 : 2.6249568936305794
Loss in iteration 121 : 3.0791172976634553
Loss in iteration 122 : 2.622316817431815
Loss in iteration 123 : 3.073266615408931
Loss in iteration 124 : 2.6198056698886383
Loss in iteration 125 : 3.0676453786077946
Loss in iteration 126 : 2.6174166773171064
Loss in iteration 127 : 3.0622407774817826
Loss in iteration 128 : 2.6151428985928877
Loss in iteration 129 : 3.057038791600091
Loss in iteration 130 : 2.6129773880148472
Loss in iteration 131 : 3.052025022867134
Loss in iteration 132 : 2.610913326988666
Loss in iteration 133 : 3.04718528942177
Loss in iteration 134 : 2.6089441091730587
Loss in iteration 135 : 3.0425060028691604
Loss in iteration 136 : 2.6070633817466606
Loss in iteration 137 : 3.037974376633216
Loss in iteration 138 : 2.6052650546824943
Loss in iteration 139 : 3.033578517365712
Loss in iteration 140 : 2.603543292146812
Loss in iteration 141 : 3.029307444038843
Loss in iteration 142 : 2.6018924980615155
Loss in iteration 143 : 3.025151067765975
Loss in iteration 144 : 2.60030730393504
Loss in iteration 145 : 3.0211001538886237
Loss in iteration 146 : 2.598782563007073
Loss in iteration 147 : 3.0171462785409675
Loss in iteration 148 : 2.597313351526007
Loss in iteration 149 : 3.013281785369649
Loss in iteration 150 : 2.595894975916527
Loss in iteration 151 : 3.009499744122221
Loss in iteration 152 : 2.594522983623936
Loss in iteration 153 : 3.0057939108287828
Loss in iteration 154 : 2.593193175269735
Loss in iteration 155 : 3.0021586886232545
Loss in iteration 156 : 2.5919016160969375
Loss in iteration 157 : 2.9985890883093136
Loss in iteration 158 : 2.5906446452480916
Loss in iteration 159 : 2.995080688151715
Loss in iteration 160 : 2.5894188820041317
Loss in iteration 161 : 2.991629592799737
Loss in iteration 162 : 2.5882212286067516
Loss in iteration 163 : 2.98823239158893
Loss in iteration 164 : 2.5870488696411686
Loss in iteration 165 : 2.984886116671275
Loss in iteration 166 : 2.585899268171792
Loss in iteration 167 : 2.981588201496345
Loss in iteration 168 : 2.5847701589231993
Loss in iteration 169 : 2.978336440140433
Loss in iteration 170 : 2.583659538820536
Loss in iteration 171 : 2.975128947894888
Loss in iteration 172 : 2.5825656551822096
Loss in iteration 173 : 2.9719641234175542
Loss in iteration 174 : 2.581486991825211
Loss in iteration 175 : 2.968840612649542
Loss in iteration 176 : 2.58042225332058
Loss in iteration 177 : 2.965757274622375
Loss in iteration 178 : 2.579370347636822
Loss in iteration 179 : 2.962713149235431
Loss in iteration 180 : 2.578330367434799
Loss in iteration 181 : 2.959707427070267
Loss in iteration 182 : 2.57730157032424
Loss in iteration 183 : 2.9567394213188365
Loss in iteration 184 : 2.5762833584500244
Loss in iteration 185 : 2.9538085419272475
Loss in iteration 186 : 2.575275257832761
Loss in iteration 187 : 2.9509142720814636
Loss in iteration 188 : 2.574276897929508
Loss in iteration 189 : 2.948056147177209
Loss in iteration 190 : 2.5732879918976392
Loss in iteration 191 : 2.9452337364128134
Loss in iteration 192 : 2.5723083180302915
Loss in iteration 193 : 2.942446627119378
Loss in iteration 194 : 2.5713377027848963
Loss in iteration 195 : 2.939694411895201
Loss in iteration 196 : 2.570376005750695
Loss in iteration 197 : 2.936976678547568
Loss in iteration 198 : 2.5694231068034123
Loss in iteration 199 : 2.9342930027696257
Loss in iteration 200 : 2.568478895586397
Loss in iteration 201 : 2.9316429434035984
Loss in iteration 202 : 2.567543263346787
Loss in iteration 203 : 2.9290260400695596
Loss in iteration 204 : 2.566616097053078
Loss in iteration 205 : 2.9264418128808747
Loss in iteration 206 : 2.5656972756331093
Loss in iteration 207 : 2.92388976392517
Loss in iteration 208 : 2.5647866681049565
Loss in iteration 209 : 2.9213693801666287
Loss in iteration 210 : 2.563884133327713
Loss in iteration 211 : 2.9188801374209294
Loss in iteration 212 : 2.562989521075817
Loss in iteration 213 : 2.9164215050658386
Loss in iteration 214 : 2.562102674135064
Loss in iteration 215 : 2.9139929511745923
Loss in iteration 216 : 2.561223431129073
Loss in iteration 217 : 2.911593947792772
Loss in iteration 218 : 2.560351629806398
Loss in iteration 219 : 2.9092239761181915
Loss in iteration 220 : 2.5594871105471855
Loss in iteration 221 : 2.90688253138299
Loss in iteration 222 : 2.5586297198817154
Loss in iteration 223 : 2.904569127277301
Loss in iteration 224 : 2.557779313847061
Loss in iteration 225 : 2.902283299790978
Loss in iteration 226 : 2.5569357610422823
Loss in iteration 227 : 2.900024610383081
Loss in iteration 228 : 2.556098945274758
Loss in iteration 229 : 2.8977926484194425
Loss in iteration 230 : 2.555268767719435
Loss in iteration 231 : 2.8955870328444817
Loss in iteration 232 : 2.5544451485402426
Loss in iteration 233 : 2.893407413077055
Loss in iteration 234 : 2.5536280279465613
Loss in iteration 235 : 2.891253469140466
Loss in iteration 236 : 2.5528173666795433
Loss in iteration 237 : 2.8891249110556427
Loss in iteration 238 : 2.552013145941847
Loss in iteration 239 : 2.887021477543618
Loss in iteration 240 : 2.551215366801297
Loss in iteration 241 : 2.8849429340992394
Loss in iteration 242 : 2.5504240491133876
Loss in iteration 243 : 2.8828890705125483
Loss in iteration 244 : 2.5496392300197157
Loss in iteration 245 : 2.8808596979276677
Loss in iteration 246 : 2.548860962089717
Loss in iteration 247 : 2.8788546455402693
Loss in iteration 248 : 2.5480893111800804
Loss in iteration 249 : 2.876873757043838
Loss in iteration 250 : 2.5473243540918333
Loss in iteration 251 : 2.874916886941548
Loss in iteration 252 : 2.5465661761068277
Loss in iteration 253 : 2.8729838968431207
Loss in iteration 254 : 2.545814868485355
Loss in iteration 255 : 2.871074651865537
Loss in iteration 256 : 2.5450705260032045
Loss in iteration 257 : 2.8691890172513532
Loss in iteration 258 : 2.5443332446010216
Loss in iteration 259 : 2.8673268553093068
Loss in iteration 260 : 2.543603119210634
Loss in iteration 261 : 2.8654880227686954
Loss in iteration 262 : 2.542880241812644
Loss in iteration 263 : 2.8636723686220273
Loss in iteration 264 : 2.5421646997683136
Loss in iteration 265 : 2.861879732510795
Loss in iteration 266 : 2.5414565744555886
Loss in iteration 267 : 2.8601099436870414
Loss in iteration 268 : 2.540755940225751
Loss in iteration 269 : 2.8583628205600986
Loss in iteration 270 : 2.5400628636839127
Loss in iteration 271 : 2.8566381708143087
Loss in iteration 272 : 2.5393774032836536
Loss in iteration 273 : 2.8549357920612994
Loss in iteration 274 : 2.5386996092145457
Loss in iteration 275 : 2.8532554729692325
Loss in iteration 276 : 2.538029523550957
Loss in iteration 277 : 2.8515969947941686
Loss in iteration 278 : 2.5373671806223954
Loss in iteration 279 : 2.8499601332238313
Loss in iteration 280 : 2.5367126075595414
Loss in iteration 281 : 2.848344660434652
Loss in iteration 282 : 2.536065824966165
Loss in iteration 283 : 2.8467503472568905
Loss in iteration 284 : 2.535426847666118
Loss in iteration 285 : 2.845176965342439
Loss in iteration 286 : 2.534795685474917
Loss in iteration 287 : 2.8436242892331807
Loss in iteration 288 : 2.534172343948993
Loss in iteration 289 : 2.842092098236698
Loss in iteration 290 : 2.533556825070375
Loss in iteration 291 : 2.8405801780277233
Loss in iteration 292 : 2.532949127831302
Loss in iteration 293 : 2.839088321909508
Loss in iteration 294 : 2.5323492486908714
Loss in iteration 295 : 2.837616331686713
Loss in iteration 296 : 2.531757181884855
Loss in iteration 297 : 2.8361640181211887
Loss in iteration 298 : 2.5311729195783457
Loss in iteration 299 : 2.834731200961239
Loss in iteration 300 : 2.5305964518602453
Loss in iteration 301 : 2.833317708554997
Loss in iteration 302 : 2.5300277665867585
Loss in iteration 303 : 2.8319233770758236
Loss in iteration 304 : 2.52946684908907
Loss in iteration 305 : 2.83054804940401
Loss in iteration 306 : 2.5289136817660345
Loss in iteration 307 : 2.829191573721415
Loss in iteration 308 : 2.5283682435884165
Loss in iteration 309 : 2.827853801885525
Loss in iteration 310 : 2.52783050954391
Loss in iteration 311 : 2.826534587654839
Loss in iteration 312 : 2.5273004500540557
Loss in iteration 313 : 2.825233784839844
Loss in iteration 314 : 2.5267780303939458
Loss in iteration 315 : 2.823951245452328
Loss in iteration 316 : 2.526263210144264
Loss in iteration 317 : 2.8226868179207787
Loss in iteration 318 : 2.525755942702029
Loss in iteration 319 : 2.8214403454327757
Loss in iteration 320 : 2.525256174872654
Loss in iteration 321 : 2.820211664455409
Loss in iteration 322 : 2.5247638465609796
Loss in iteration 323 : 2.8190006034739543
Loss in iteration 324 : 2.524278890574194
Loss in iteration 325 : 2.8178069819780562
Loss in iteration 326 : 2.523801232543664
Loss in iteration 327 : 2.816630609712238
Loss in iteration 328 : 2.523330790968129
Loss in iteration 329 : 2.815471286197211
Loss in iteration 330 : 2.5228674773755384
Loss in iteration 331 : 2.8143288005180667
Loss in iteration 332 : 2.5224111965968605
Loss in iteration 333 : 2.8132029313668436
Loss in iteration 334 : 2.521961847141768
Loss in iteration 335 : 2.8120934473196466
Loss in iteration 336 : 2.521519321663271
Loss in iteration 337 : 2.8110001073231947
Loss in iteration 338 : 2.5210835074966362
Loss in iteration 339 : 2.809922661361906
Loss in iteration 340 : 2.520654287256996
Loss in iteration 341 : 2.8088608512741793
Loss in iteration 342 : 2.520231539479427
Loss in iteration 343 : 2.8078144116860533
Loss in iteration 344 : 2.519815139285847
Loss in iteration 345 : 2.8067830710308344
Loss in iteration 346 : 2.5194049590638614
Loss in iteration 347 : 2.8057665526247013
Loss in iteration 348 : 2.5190008691437322
Loss in iteration 349 : 2.8047645757705815
Loss in iteration 350 : 2.5186027384614174
Loss in iteration 351 : 2.8037768568657517
Loss in iteration 352 : 2.518210435197294
Loss in iteration 353 : 2.80280311049125
Loss in iteration 354 : 2.5178238273817506
Loss in iteration 355 : 2.8018430504650937
Loss in iteration 356 : 2.517442783460979
Loss in iteration 357 : 2.8008963908442497
Loss in iteration 358 : 2.517067172817856
Loss in iteration 359 : 2.799962846864003
Loss in iteration 360 : 2.5166968662443496
Loss in iteration 361 : 2.7990421358057747
Loss in iteration 362 : 2.5163317363635658
Loss in iteration 363 : 2.7981339777879004
Loss in iteration 364 : 2.515971658000624
Loss in iteration 365 : 2.7972380964759194
Loss in iteration 366 : 2.5156165085029967
Loss in iteration 367 : 2.796354219711034
Loss in iteration 368 : 2.515266168011286
Loss in iteration 369 : 2.7954820800575644
Loss in iteration 370 : 2.5149205196828532
Loss in iteration 371 : 2.7946214152711484
Loss in iteration 372 : 2.5145794498708
Loss in iteration 373 : 2.7937719686910913
Loss in iteration 374 : 2.514242848261295
Loss in iteration 375 : 2.7929334895608324
Loss in iteration 376 : 2.5139106079726963
Loss in iteration 377 : 2.792105733281189
Loss in iteration 378 : 2.51358262561986
Loss in iteration 379 : 2.791288461601673
Loss in iteration 380 : 2.5132588013472295
Loss in iteration 381 : 2.7904814427549964
Loss in iteration 382 : 2.5129390388342308
Loss in iteration 383 : 2.789684451540441
Loss in iteration 384 : 2.512623245276227
Loss in iteration 385 : 2.788897269361235
Loss in iteration 386 : 2.5123113313446392
Loss in iteration 387 : 2.7881196842213782
Loss in iteration 388 : 2.5120032111288837
Loss in iteration 389 : 2.7873514906868895
Loss in iteration 390 : 2.511698802063326
Loss in iteration 391 : 2.786592489816123
Loss in iteration 392 : 2.5113980248418692
Loss in iteration 393 : 2.7858424890638553
Loss in iteration 394 : 2.511100803322408
Loss in iteration 395 : 2.785101302162924
Loss in iteration 396 : 2.5108070644235227
Loss in iteration 397 : 2.7843687489875246
Loss in iteration 398 : 2.510516738015276
Loss in iteration 399 : 2.783644655401401
Loss in iteration 400 : 2.5102297568058725
Testing accuracy  of updater 0 on alg 0 with rate 7.0 = 0.722625, training accuracy 0.722625, time elapsed: 6443 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 1.0091931589813274
Loss in iteration 3 : 6.845865820119187
Loss in iteration 4 : 4.81170941983833
Loss in iteration 5 : 4.225991860017634
Loss in iteration 6 : 6.451963321816749
Loss in iteration 7 : 1.9252729828140727
Loss in iteration 8 : 6.310173133290426
Loss in iteration 9 : 1.801571630038021
Loss in iteration 10 : 5.73736427308909
Loss in iteration 11 : 2.1332355913561107
Loss in iteration 12 : 5.458823567179207
Loss in iteration 13 : 2.182242081802692
Loss in iteration 14 : 4.955491518217083
Loss in iteration 15 : 2.4437092500807536
Loss in iteration 16 : 4.583116197931897
Loss in iteration 17 : 2.5639856098525033
Loss in iteration 18 : 4.169307997210953
Loss in iteration 19 : 2.6954659652103854
Loss in iteration 20 : 3.800803294962593
Loss in iteration 21 : 2.759819048030379
Loss in iteration 22 : 3.465457318041337
Loss in iteration 23 : 2.773805005161517
Loss in iteration 24 : 3.1751951074156706
Loss in iteration 25 : 2.742295076799051
Loss in iteration 26 : 2.928368552999853
Loss in iteration 27 : 2.682461192963723
Loss in iteration 28 : 2.720201971139368
Loss in iteration 29 : 2.608853372776411
Loss in iteration 30 : 2.546931130452117
Loss in iteration 31 : 2.530687682285972
Loss in iteration 32 : 2.4032473223883306
Loss in iteration 33 : 2.4540887926478043
Loss in iteration 34 : 2.2840905546984303
Loss in iteration 35 : 2.3826916244531486
Loss in iteration 36 : 2.1856709690437257
Loss in iteration 37 : 2.318397090100668
Loss in iteration 38 : 2.104723680619862
Loss in iteration 39 : 2.2618646807225598
Loss in iteration 40 : 2.0378600322925715
Loss in iteration 41 : 2.212754984713535
Loss in iteration 42 : 1.981986438812563
Loss in iteration 43 : 2.1701739618317286
Loss in iteration 44 : 1.9346259701543813
Loss in iteration 45 : 2.1330901959538315
Loss in iteration 46 : 1.8939130155792692
Loss in iteration 47 : 2.1005525989919125
Loss in iteration 48 : 1.8584788923160662
Loss in iteration 49 : 2.071765324567229
Loss in iteration 50 : 1.8273271614701325
Loss in iteration 51 : 2.046086235164914
Loss in iteration 52 : 1.7997241375851523
Loss in iteration 53 : 2.022997222178267
Loss in iteration 54 : 1.7751145734484204
Loss in iteration 55 : 2.0020761973075816
Loss in iteration 56 : 1.7530644465495753
Loss in iteration 57 : 1.9829764367402232
Loss in iteration 58 : 1.7332249591693025
Loss in iteration 59 : 1.9654020460975736
Loss in iteration 60 : 1.7153082048317236
Loss in iteration 61 : 1.94905402064132
Loss in iteration 62 : 1.6990637839288638
Loss in iteration 63 : 1.9335026960354353
Loss in iteration 64 : 1.6842429116295434
Loss in iteration 65 : 1.9180712126301924
Loss in iteration 66 : 1.6705636186805664
Loss in iteration 67 : 1.9025009664570525
Loss in iteration 68 : 1.6578685959983448
Loss in iteration 69 : 1.8878590149381254
Loss in iteration 70 : 1.6463574266679404
Loss in iteration 71 : 1.8749255105769866
Loss in iteration 72 : 1.6361431558328818
Loss in iteration 73 : 1.8634943903020835
Loss in iteration 74 : 1.6270636966327399
Loss in iteration 75 : 1.8531641246146882
Loss in iteration 76 : 1.6189132724666713
Loss in iteration 77 : 1.8436625482376057
Loss in iteration 78 : 1.611536771716265
Loss in iteration 79 : 1.8348348911075965
Loss in iteration 80 : 1.6048244416699338
Loss in iteration 81 : 1.8265903912990042
Loss in iteration 82 : 1.598694201298842
Loss in iteration 83 : 1.818869237597335
Loss in iteration 84 : 1.5930802843039356
Loss in iteration 85 : 1.8116257497888766
Loss in iteration 86 : 1.5879270998971333
Loss in iteration 87 : 1.8048200213076568
Loss in iteration 88 : 1.583185921670562
Loss in iteration 89 : 1.798414199799493
Loss in iteration 90 : 1.5788131527286091
Loss in iteration 91 : 1.7923715273461962
Loss in iteration 92 : 1.5747695225916998
Loss in iteration 93 : 1.7866568327362573
Loss in iteration 94 : 1.5710197792656233
Loss in iteration 95 : 1.781237491611618
Loss in iteration 96 : 1.5675325621360365
Loss in iteration 97 : 1.7760842492398492
Loss in iteration 98 : 1.5642802673074883
Loss in iteration 99 : 1.7711716514872746
Loss in iteration 100 : 1.561238827054829
Loss in iteration 101 : 1.766478063973652
Loss in iteration 102 : 1.5583873969606552
Loss in iteration 103 : 1.7619853725247352
Loss in iteration 104 : 1.5557079786086514
Loss in iteration 105 : 1.7576784888009327
Loss in iteration 106 : 1.5531850148087085
Loss in iteration 107 : 1.7535447737425076
Loss in iteration 108 : 1.550804990646515
Loss in iteration 109 : 1.7495734646369774
Loss in iteration 110 : 1.548556065213998
Loss in iteration 111 : 1.7457551625707255
Loss in iteration 112 : 1.5464277497391834
Loss in iteration 113 : 1.742081411366813
Loss in iteration 114 : 1.544410639678937
Loss in iteration 115 : 1.7385443787494737
Loss in iteration 116 : 1.542496201765138
Loss in iteration 117 : 1.7351366358074256
Loss in iteration 118 : 1.540676612172581
Loss in iteration 119 : 1.7318510215531278
Loss in iteration 120 : 1.5389446388843289
Loss in iteration 121 : 1.7286805747644105
Loss in iteration 122 : 1.537293559805778
Loss in iteration 123 : 1.725618514329108
Loss in iteration 124 : 1.5357171079413967
Loss in iteration 125 : 1.7226582508504558
Loss in iteration 126 : 1.5342094356419649
Loss in iteration 127 : 1.719793415225537
Loss in iteration 128 : 1.5327650911831179
Loss in iteration 129 : 1.7170178933906022
Loss in iteration 130 : 1.5313790024213996
Loss in iteration 131 : 1.7143258598063689
Loss in iteration 132 : 1.5300464637422315
Loss in iteration 133 : 1.7117118051627613
Loss in iteration 134 : 1.5287631238078452
Loss in iteration 135 : 1.7091705560570103
Loss in iteration 136 : 1.527524972654419
Loss in iteration 137 : 1.7066972860309049
Loss in iteration 138 : 1.526328327459762
Loss in iteration 139 : 1.704287518416877
Loss in iteration 140 : 1.5251698168302845
Loss in iteration 141 : 1.7019371220536719
Loss in iteration 142 : 1.524046363780007
Loss in iteration 143 : 1.699642301208712
Loss in iteration 144 : 1.5229551677452338
Loss in iteration 145 : 1.6973995810951665
Loss in iteration 146 : 1.5218936860416346
Loss in iteration 147 : 1.6952057902823712
Loss in iteration 148 : 1.5208596151643665
Loss in iteration 149 : 1.6930580411346206
Loss in iteration 150 : 1.5198508722865285
Loss in iteration 151 : 1.6909537092198086
Loss in iteration 152 : 1.5188655772473372
Loss in iteration 153 : 1.6888904124350885
Loss in iteration 154 : 1.5179020352532928
Loss in iteration 155 : 1.6868659904175383
Loss in iteration 156 : 1.5169587204511434
Loss in iteration 157 : 1.6848784846533127
Loss in iteration 158 : 1.5160342604756907
Loss in iteration 159 : 1.6829261195698009
Loss in iteration 160 : 1.5151274220288906
Loss in iteration 161 : 1.6810072847925657
Loss in iteration 162 : 1.5142370975108097
Loss in iteration 163 : 1.6791205186687213
Loss in iteration 164 : 1.5133622926952417
Loss in iteration 165 : 1.6772644930979077
Loss in iteration 166 : 1.5125021154230576
Loss in iteration 167 : 1.6754379996679594
Loss in iteration 168 : 1.5116557652726887
Loss in iteration 169 : 1.673639937060553
Loss in iteration 170 : 1.5108225241578985
Loss in iteration 171 : 1.6718692996708777
Loss in iteration 172 : 1.5100017477977972
Loss in iteration 173 : 1.6701251673713156
Loss in iteration 174 : 1.5091928580011535
Loss in iteration 175 : 1.6684066963410742
Loss in iteration 176 : 1.5083953357062247
Loss in iteration 177 : 1.6667131108795783
Loss in iteration 178 : 1.5076087147175306
Loss in iteration 179 : 1.6650436961204496
Loss in iteration 180 : 1.5068325760826955
Loss in iteration 181 : 1.6633977915643277
Loss in iteration 182 : 1.5060665430539568
Loss in iteration 183 : 1.6617747853510894
Loss in iteration 184 : 1.5053102765817872
Loss in iteration 185 : 1.6601741091961362
Loss in iteration 186 : 1.5045634712902178
Loss in iteration 187 : 1.6585952339195893
Loss in iteration 188 : 1.503825851886824
Loss in iteration 189 : 1.6570376655022547
Loss in iteration 190 : 1.5030971699628508
Loss in iteration 191 : 1.6555009416070983
Loss in iteration 192 : 1.5023772011423713
Loss in iteration 193 : 1.6539846285103221
Loss in iteration 194 : 1.5016657425421847
Loss in iteration 195 : 1.6524883183912003
Loss in iteration 196 : 1.5009626105075744
Loss in iteration 197 : 1.6510116269349597
Loss in iteration 198 : 1.5002676385919522
Loss in iteration 199 : 1.6495541912077447
Loss in iteration 200 : 1.4995806757513992
Loss in iteration 201 : 1.6481156677675433
Loss in iteration 202 : 1.4989015847284082
Loss in iteration 203 : 1.6466957309792352
Loss in iteration 204 : 1.4982302406015255
Loss in iteration 205 : 1.6452940715060895
Loss in iteration 206 : 1.4975665294806453
Loss in iteration 207 : 1.6439103949537803
Loss in iteration 208 : 1.4969103473300776
Loss in iteration 209 : 1.6425444206465085
Loss in iteration 210 : 1.4962615989038477
Loss in iteration 211 : 1.641195880517727
Loss in iteration 212 : 1.4956201967799083
Loss in iteration 213 : 1.6398645181009484
Loss in iteration 214 : 1.4949860604818
Loss in iteration 215 : 1.6385500876082333
Loss in iteration 216 : 1.494359115677983
Loss in iteration 217 : 1.6372523530861671
Loss in iteration 218 : 1.4937392934506801
Loss in iteration 219 : 1.6359710876407672
Loss in iteration 220 : 1.4931265296272105
Loss in iteration 221 : 1.634706072724135
Loss in iteration 222 : 1.4925207641679064
Loss in iteration 223 : 1.6334570974769123
Loss in iteration 224 : 1.491921940605756
Loss in iteration 225 : 1.6322239581213922
Loss in iteration 226 : 1.4913300055333607
Loss in iteration 227 : 1.6310064574007783
Loss in iteration 228 : 1.4907449081336572
Loss in iteration 229 : 1.6298044040607842
Loss in iteration 230 : 1.490166599751165
Loss in iteration 231 : 1.6286176123698752
Loss in iteration 232 : 1.4895950335007966
Loss in iteration 233 : 1.627445901674937
Loss in iteration 234 : 1.4890301639118182
Loss in iteration 235 : 1.6262890959892293
Loss in iteration 236 : 1.4884719466043488
Loss in iteration 237 : 1.625147023609641
Loss in iteration 238 : 1.4879203379962755
Loss in iteration 239 : 1.624019516760441
Loss in iteration 240 : 1.4873752950384183
Loss in iteration 241 : 1.6229064112608034
Loss in iteration 242 : 1.4868367749759013
Loss in iteration 243 : 1.6218075462136057
Loss in iteration 244 : 1.4863047351337932
Loss in iteration 245 : 1.62072276371309
Loss in iteration 246 : 1.4857791327252476
Loss in iteration 247 : 1.6196519085692411
Loss in iteration 248 : 1.48525992468032
Loss in iteration 249 : 1.6185948280470002
Loss in iteration 250 : 1.4847470674939545
Loss in iteration 251 : 1.6175513716186916
Loss in iteration 252 : 1.4842405170916113
Loss in iteration 253 : 1.6165213907282996
Loss in iteration 254 : 1.4837402287112182
Loss in iteration 255 : 1.6155047385667067
Loss in iteration 256 : 1.483246156800239
Loss in iteration 257 : 1.614501269857045
Loss in iteration 258 : 1.482758254926859
Loss in iteration 259 : 1.613510840650015
Loss in iteration 260 : 1.482276475704319
Loss in iteration 261 : 1.6125333081289221
Loss in iteration 262 : 1.4818007707277243
Loss in iteration 263 : 1.6115685304247467
Loss in iteration 264 : 1.4813310905225958
Loss in iteration 265 : 1.6106163664416722
Loss in iteration 266 : 1.4808673845047369
Loss in iteration 267 : 1.609676675693654
Loss in iteration 268 : 1.4804096009508971
Loss in iteration 269 : 1.6087493181527344
Loss in iteration 270 : 1.479957686979916
Loss in iteration 271 : 1.6078341541100822
Loss in iteration 272 : 1.4795115885440508
Loss in iteration 273 : 1.6069310440503892
Loss in iteration 274 : 1.4790712504301073
Loss in iteration 275 : 1.606039848540539
Loss in iteration 276 : 1.4786366162701852
Loss in iteration 277 : 1.6051604281332925
Loss in iteration 278 : 1.4782076285617067
Loss in iteration 279 : 1.604292643286486
Loss in iteration 280 : 1.477784228696305
Loss in iteration 281 : 1.6034363542982832
Loss in iteration 282 : 1.4773663569973676
Loss in iteration 283 : 1.602591421258659
Loss in iteration 284 : 1.476953952765584
Loss in iteration 285 : 1.6017577040172057
Loss in iteration 286 : 1.4765469543322256
Loss in iteration 287 : 1.6009350621670868
Loss in iteration 288 : 1.4761452991193695
Loss in iteration 289 : 1.6001233550447442
Loss in iteration 290 : 1.4757489237066368
Loss in iteration 291 : 1.5993224417448277
Loss in iteration 292 : 1.4753577639036768
Loss in iteration 293 : 1.5985321811494773
Loss in iteration 294 : 1.4749717548276275
Loss in iteration 295 : 1.5977524319711698
Loss in iteration 296 : 1.4745908309848952
Loss in iteration 297 : 1.5969830528077864
Loss in iteration 298 : 1.4742149263563615
Loss in iteration 299 : 1.5962239022089464
Loss in iteration 300 : 1.4738439744852687
Loss in iteration 301 : 1.5954748387521476
Loss in iteration 302 : 1.473477908566913
Loss in iteration 303 : 1.5947357211272708
Loss in iteration 304 : 1.4731166615393922
Loss in iteration 305 : 1.594006408228232
Loss in iteration 306 : 1.4727601661746048
Loss in iteration 307 : 1.593286759250136
Loss in iteration 308 : 1.472408355168669
Loss in iteration 309 : 1.592576633790693
Loss in iteration 310 : 1.47206116123127
Loss in iteration 311 : 1.591875891954439
Loss in iteration 312 : 1.4717185171729747
Loss in iteration 313 : 1.591184394458456
Loss in iteration 314 : 1.4713803559901917
Loss in iteration 315 : 1.5905020027384256
Loss in iteration 316 : 1.4710466109470326
Loss in iteration 317 : 1.589828579053752
Loss in iteration 318 : 1.4707172156537547
Loss in iteration 319 : 1.5891639865908982
Loss in iteration 320 : 1.470392104141277
Loss in iteration 321 : 1.5885080895638148
Loss in iteration 322 : 1.4700712109314726
Loss in iteration 323 : 1.5878607533107492
Loss in iteration 324 : 1.4697544711030068
Loss in iteration 325 : 1.587221844386754
Loss in iteration 326 : 1.4694418203524469
Loss in iteration 327 : 1.5865912306511711
Loss in iteration 328 : 1.4691331950505864
Loss in iteration 329 : 1.5859687813498022
Loss in iteration 330 : 1.46882853229383
Loss in iteration 331 : 1.5853543671911887
Loss in iteration 332 : 1.4685277699506696
Loss in iteration 333 : 1.5847478604168939
Loss in iteration 334 : 1.4682308467032696
Loss in iteration 335 : 1.5841491348654875
Loss in iteration 336 : 1.4679377020842155
Loss in iteration 337 : 1.5835580660301958
Loss in iteration 338 : 1.4676482765085352
Loss in iteration 339 : 1.5829745311101506
Loss in iteration 340 : 1.4673625113011448
Loss in iteration 341 : 1.582398409055348
Loss in iteration 342 : 1.4670803487199209
Loss in iteration 343 : 1.5818295806053237
Loss in iteration 344 : 1.4668017319745068
Loss in iteration 345 : 1.5812679283218098
Loss in iteration 346 : 1.4665266052411752
Loss in iteration 347 : 1.580713336615453
Loss in iteration 348 : 1.4662549136739016
Loss in iteration 349 : 1.5801656917669644
Loss in iteration 350 : 1.465986603411905
Loss in iteration 351 : 1.5796248819428154
Loss in iteration 352 : 1.4657216215838988
Loss in iteration 353 : 1.579090797205902
Loss in iteration 354 : 1.4654599163092983
Loss in iteration 355 : 1.5785633295213852
Loss in iteration 356 : 1.4652014366966728
Loss in iteration 357 : 1.578042372758035
Loss in iteration 358 : 1.4649461328395592
Loss in iteration 359 : 1.5775278226854361
Loss in iteration 360 : 1.464693955810052
Loss in iteration 361 : 1.5770195769673234
Loss in iteration 362 : 1.4644448576502374
Loss in iteration 363 : 1.5765175351513907
Loss in iteration 364 : 1.4641987913618173
Loss in iteration 365 : 1.5760215986559052
Loss in iteration 366 : 1.4639557108940613
Loss in iteration 367 : 1.5755316707533442
Loss in iteration 368 : 1.4637155711302992
Loss in iteration 369 : 1.575047656551477
Loss in iteration 370 : 1.4634783278731738
Loss in iteration 371 : 1.574569462972041
Loss in iteration 372 : 1.4632439378288342
Loss in iteration 373 : 1.5740969987274094
Loss in iteration 374 : 1.4630123585901855
Loss in iteration 375 : 1.5736301742953995
Loss in iteration 376 : 1.4627835486194307
Loss in iteration 377 : 1.5731689018925124
Loss in iteration 378 : 1.4625574672299517
Loss in iteration 379 : 1.5727130954458535
Loss in iteration 380 : 1.462334074567766
Loss in iteration 381 : 1.5722626705638763
Loss in iteration 382 : 1.4621133315926165
Loss in iteration 383 : 1.57181754450626
Loss in iteration 384 : 1.4618952000587777
Loss in iteration 385 : 1.5713776361529304
Loss in iteration 386 : 1.4616796424958047
Loss in iteration 387 : 1.5709428659725941
Loss in iteration 388 : 1.461466622189144
Loss in iteration 389 : 1.5705131559907923
Loss in iteration 390 : 1.4612561031608142
Loss in iteration 391 : 1.5700884297576514
Loss in iteration 392 : 1.4610480501501835
Loss in iteration 393 : 1.5696686123155115
Loss in iteration 394 : 1.4608424285949213
Loss in iteration 395 : 1.5692536301664741
Loss in iteration 396 : 1.4606392046121182
Loss in iteration 397 : 1.5688434112400351
Loss in iteration 398 : 1.4604383449797473
Loss in iteration 399 : 1.5684378848608211
Loss in iteration 400 : 1.4602398171183408
Testing accuracy  of updater 0 on alg 0 with rate 4.0 = 0.724, training accuracy 0.724, time elapsed: 8976 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6826496441898315
Loss in iteration 3 : 0.725167450273125
Loss in iteration 4 : 0.9043540796081131
Loss in iteration 5 : 0.9965633061226778
Loss in iteration 6 : 1.2081521260774444
Loss in iteration 7 : 0.8731585215684704
Loss in iteration 8 : 1.073014754531772
Loss in iteration 9 : 0.8494648798849441
Loss in iteration 10 : 1.0034789436340166
Loss in iteration 11 : 0.8103927207765956
Loss in iteration 12 : 0.926232059609523
Loss in iteration 13 : 0.7768120276425868
Loss in iteration 14 : 0.8607201840207828
Loss in iteration 15 : 0.744377769829368
Loss in iteration 16 : 0.8036105906410181
Loss in iteration 17 : 0.7136515407463072
Loss in iteration 18 : 0.7544269284821339
Loss in iteration 19 : 0.6849606934737429
Loss in iteration 20 : 0.7122413323717436
Loss in iteration 21 : 0.65856187768203
Loss in iteration 22 : 0.6761260965331404
Loss in iteration 23 : 0.6345720640813586
Loss in iteration 24 : 0.6452250038763838
Loss in iteration 25 : 0.612982572124985
Loss in iteration 26 : 0.6187857076040149
Loss in iteration 27 : 0.5936977934166677
Loss in iteration 28 : 0.5961615356064669
Loss in iteration 29 : 0.5765719973701614
Loss in iteration 30 : 0.5768031478058391
Loss in iteration 31 : 0.5614411483053
Loss in iteration 32 : 0.5602505572244704
Loss in iteration 33 : 0.5481516058199746
Loss in iteration 34 : 0.5461293625914495
Loss in iteration 35 : 0.5365757609816606
Loss in iteration 36 : 0.5341429191740229
Loss in iteration 37 : 0.5266014870814502
Loss in iteration 38 : 0.524048343243742
Loss in iteration 39 : 0.5181100333716702
Loss in iteration 40 : 0.5156273972185385
Loss in iteration 41 : 0.5109670373527165
Loss in iteration 42 : 0.5086719651262644
Loss in iteration 43 : 0.5050273743713423
Loss in iteration 44 : 0.5029833242899197
Loss in iteration 45 : 0.5001430085720845
Loss in iteration 46 : 0.4983750962674798
Loss in iteration 47 : 0.49616863274259093
Loss in iteration 48 : 0.4946751998761718
Loss in iteration 49 : 0.49296513038157924
Loss in iteration 50 : 0.4917267397637677
Loss in iteration 51 : 0.49040204575618407
Loss in iteration 52 : 0.4893886385832387
Loss in iteration 53 : 0.48835975704582474
Loss in iteration 54 : 0.48753634037645566
Loss in iteration 55 : 0.4867314363284446
Loss in iteration 56 : 0.4860624333064284
Loss in iteration 57 : 0.4854245479649703
Loss in iteration 58 : 0.4848768597073078
Loss in iteration 59 : 0.4843615883623385
Loss in iteration 60 : 0.4839064482816865
Loss in iteration 61 : 0.4834799069309339
Loss in iteration 62 : 0.48309369286384074
Loss in iteration 63 : 0.48273065202844134
Loss in iteration 64 : 0.4823948985864859
Loss in iteration 65 : 0.4820770578169879
Loss in iteration 66 : 0.48177794402331814
Loss in iteration 67 : 0.481492373927718
Loss in iteration 68 : 0.48121994232734266
Loss in iteration 69 : 0.4809577343122768
Loss in iteration 70 : 0.4807050421351248
Loss in iteration 71 : 0.48046019288457753
Loss in iteration 72 : 0.48022252631115814
Loss in iteration 73 : 0.47999106030261485
Loss in iteration 74 : 0.47976527878439323
Loss in iteration 75 : 0.47954459001548877
Loss in iteration 76 : 0.4793286196210813
Loss in iteration 77 : 0.4791169993370011
Loss in iteration 78 : 0.47890946459118233
Loss in iteration 79 : 0.47870577621154825
Loss in iteration 80 : 0.47850574595656564
Loss in iteration 81 : 0.4783092097067878
Loss in iteration 82 : 0.47811602922303964
Loss in iteration 83 : 0.4779260843467569
Loss in iteration 84 : 0.4777392686075833
Loss in iteration 85 : 0.4775554879529196
Loss in iteration 86 : 0.477374655955149
Loss in iteration 87 : 0.477196694478557
Loss in iteration 88 : 0.4770215299500116
Loss in iteration 89 : 0.4768490943783651
Loss in iteration 90 : 0.47667932277115815
Loss in iteration 91 : 0.4765121540244468
Loss in iteration 92 : 0.47634752922361784
Loss in iteration 93 : 0.47618539230456447
Loss in iteration 94 : 0.4760256889616746
Loss in iteration 95 : 0.47586836710102876
Loss in iteration 96 : 0.4757133761436962
Loss in iteration 97 : 0.47556066731927604
Loss in iteration 98 : 0.47541019321988315
Loss in iteration 99 : 0.47526190798027346
Loss in iteration 100 : 0.47511576698885183
Loss in iteration 101 : 0.4749717269909188
Loss in iteration 102 : 0.47482974589764143
Loss in iteration 103 : 0.47468978283899615
Loss in iteration 104 : 0.4745517980339206
Loss in iteration 105 : 0.4744157528113855
Loss in iteration 106 : 0.4742816095189929
Loss in iteration 107 : 0.47414933152450567
Loss in iteration 108 : 0.4740188831488743
Loss in iteration 109 : 0.4738902296562818
Loss in iteration 110 : 0.47376333720292757
Loss in iteration 111 : 0.4736381728207311
Loss in iteration 112 : 0.4735147043765021
Loss in iteration 113 : 0.47339290055250455
Loss in iteration 114 : 0.47327273081267185
Loss in iteration 115 : 0.47315416538197697
Loss in iteration 116 : 0.47303717521758654
Loss in iteration 117 : 0.47292173198820514
Loss in iteration 118 : 0.4728078080488255
Loss in iteration 119 : 0.472695376420671
Loss in iteration 120 : 0.4725844107687235
Loss in iteration 121 : 0.47247488538257704
Loss in iteration 122 : 0.4723667751561804
Loss in iteration 123 : 0.47226005556976847
Loss in iteration 124 : 0.47215470267141735
Loss in iteration 125 : 0.4720506930601139
Loss in iteration 126 : 0.4719480038688481
Loss in iteration 127 : 0.4718466127488097
Loss in iteration 128 : 0.47174649785380735
Loss in iteration 129 : 0.4716476378255505
Loss in iteration 130 : 0.4715500117792553
Loss in iteration 131 : 0.47145359928993846
Loss in iteration 132 : 0.4713583803790802
Loss in iteration 133 : 0.47126433550187163
Loss in iteration 134 : 0.47117144553483203
Loss in iteration 135 : 0.47107969176393844
Loss in iteration 136 : 0.47098905587311646
Loss in iteration 137 : 0.4708995199331739
Loss in iteration 138 : 0.470811066391099
Loss in iteration 139 : 0.47072367805974014
Loss in iteration 140 : 0.4706373381078321
Loss in iteration 141 : 0.47055203005036145
Loss in iteration 142 : 0.47046773773927475
Loss in iteration 143 : 0.4703844453544815
Loss in iteration 144 : 0.4703021373951681
Loss in iteration 145 : 0.470220798671401
Loss in iteration 146 : 0.470140414295997
Loss in iteration 147 : 0.4700609696766787
Loss in iteration 148 : 0.4699824505084695
Loss in iteration 149 : 0.4699048427663432
Loss in iteration 150 : 0.4698281326981096
Loss in iteration 151 : 0.46975230681752556
Loss in iteration 152 : 0.4696773518976273
Loss in iteration 153 : 0.46960325496427424
Loss in iteration 154 : 0.4695300032898887
Loss in iteration 155 : 0.4694575843874035
Loss in iteration 156 : 0.46938598600438436
Loss in iteration 157 : 0.46931519611734124
Loss in iteration 158 : 0.4692452029262125
Loss in iteration 159 : 0.46917599484900957
Loss in iteration 160 : 0.46910756051663943
Loss in iteration 161 : 0.469039888767864
Loss in iteration 162 : 0.46897296864442933
Loss in iteration 163 : 0.46890678938631186
Loss in iteration 164 : 0.46884134042713993
Loss in iteration 165 : 0.4687766113897211
Loss in iteration 166 : 0.46871259208171007
Loss in iteration 167 : 0.46864927249140603
Loss in iteration 168 : 0.4685866427836605
Loss in iteration 169 : 0.4685246932959151
Loss in iteration 170 : 0.4684634145343398
Loss in iteration 171 : 0.4684027971700843
Loss in iteration 172 : 0.4683428320356364
Loss in iteration 173 : 0.4682835101212834
Loss in iteration 174 : 0.4682248225716677
Loss in iteration 175 : 0.46816676068243546
Loss in iteration 176 : 0.4681093158969842
Loss in iteration 177 : 0.46805247980328907
Loss in iteration 178 : 0.4679962441308315
Loss in iteration 179 : 0.4679406007475826
Loss in iteration 180 : 0.4678855416571021
Loss in iteration 181 : 0.4678310589956819
Loss in iteration 182 : 0.46777714502958884
Loss in iteration 183 : 0.4677237921523634
Loss in iteration 184 : 0.46767099288220254
Loss in iteration 185 : 0.4676187398593964
Loss in iteration 186 : 0.46756702584384585
Loss in iteration 187 : 0.4675158437126321
Loss in iteration 188 : 0.467465186457649
Loss in iteration 189 : 0.4674150471833084
Loss in iteration 190 : 0.4673654191042805
Loss in iteration 191 : 0.4673162955433194
Loss in iteration 192 : 0.46726766992911256
Loss in iteration 193 : 0.46721953579420533
Loss in iteration 194 : 0.46717188677297394
Loss in iteration 195 : 0.4671247165996276
Loss in iteration 196 : 0.46707801910629604
Loss in iteration 197 : 0.4670317882211298
Loss in iteration 198 : 0.4669860179664579
Loss in iteration 199 : 0.46694070245699754
Loss in iteration 200 : 0.4668958358980934
Loss in iteration 201 : 0.4668514125840088
Loss in iteration 202 : 0.46680742689625687
Loss in iteration 203 : 0.46676387330195174
Loss in iteration 204 : 0.4667207463522286
Loss in iteration 205 : 0.4666780406806829
Loss in iteration 206 : 0.4666357510018372
Loss in iteration 207 : 0.4665938721096695
Loss in iteration 208 : 0.46655239887614075
Loss in iteration 209 : 0.4665113262497943
Loss in iteration 210 : 0.4664706492543494
Loss in iteration 211 : 0.46643036298735263
Loss in iteration 212 : 0.46639046261885003
Loss in iteration 213 : 0.466350943390085
Loss in iteration 214 : 0.46631180061223604
Loss in iteration 215 : 0.4662730296651661
Loss in iteration 216 : 0.4662346259962181
Loss in iteration 217 : 0.4661965851190194
Loss in iteration 218 : 0.4661589026123238
Loss in iteration 219 : 0.4661215741188721
Loss in iteration 220 : 0.4660845953442831
Loss in iteration 221 : 0.4660479620559593
Loss in iteration 222 : 0.46601167008202365
Loss in iteration 223 : 0.4659757153102789
Loss in iteration 224 : 0.46594009368718103
Loss in iteration 225 : 0.46590480121684197
Loss in iteration 226 : 0.46586983396004783
Loss in iteration 227 : 0.46583518803330864
Loss in iteration 228 : 0.4658008596079039
Loss in iteration 229 : 0.46576684490897546
Loss in iteration 230 : 0.4657331402146206
Loss in iteration 231 : 0.4656997418550106
Loss in iteration 232 : 0.4656666462115246
Loss in iteration 233 : 0.46563384971590516
Loss in iteration 234 : 0.4656013488494224
Loss in iteration 235 : 0.46556914014206496
Loss in iteration 236 : 0.4655372201717403
Loss in iteration 237 : 0.46550558556348925
Loss in iteration 238 : 0.46547423298872553
Loss in iteration 239 : 0.4654431591644814
Loss in iteration 240 : 0.4654123608526692
Loss in iteration 241 : 0.4653818348593593
Loss in iteration 242 : 0.46535157803407334
Loss in iteration 243 : 0.4653215872690894
Loss in iteration 244 : 0.4652918594987557
Loss in iteration 245 : 0.4652623916988276
Loss in iteration 246 : 0.4652331808858091
Loss in iteration 247 : 0.46520422411631057
Loss in iteration 248 : 0.4651755184864152
Loss in iteration 249 : 0.46514706113106274
Loss in iteration 250 : 0.4651188492234381
Loss in iteration 251 : 0.46509087997438053
Loss in iteration 252 : 0.4650631506317877
Loss in iteration 253 : 0.4650356584800578
Loss in iteration 254 : 0.4650084008395075
Loss in iteration 255 : 0.4649813750658315
Loss in iteration 256 : 0.46495457854955036
Loss in iteration 257 : 0.46492800871548545
Loss in iteration 258 : 0.46490166302222835
Loss in iteration 259 : 0.464875538961627
Loss in iteration 260 : 0.464849634058286
Loss in iteration 261 : 0.4648239458690616
Loss in iteration 262 : 0.4647984719825848
Loss in iteration 263 : 0.4647732100187765
Loss in iteration 264 : 0.46474815762837696
Loss in iteration 265 : 0.46472331249248644
Loss in iteration 266 : 0.4646986723221124
Loss in iteration 267 : 0.46467423485772336
Loss in iteration 268 : 0.4646499978688088
Loss in iteration 269 : 0.4646259591534499
Loss in iteration 270 : 0.46460211653790184
Loss in iteration 271 : 0.46457846787617113
Loss in iteration 272 : 0.46455501104961183
Loss in iteration 273 : 0.4645317439665222
Loss in iteration 274 : 0.4645086645617531
Loss in iteration 275 : 0.4644857707963206
Loss in iteration 276 : 0.4644630606570229
Loss in iteration 277 : 0.464440532156063
Loss in iteration 278 : 0.4644181833306881
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.790625, training accuracy 0.790625, time elapsed: 11252 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6764423585722834
Loss in iteration 3 : 0.6636030288156461
Loss in iteration 4 : 0.6546988798254658
Loss in iteration 5 : 0.6454238064820683
Loss in iteration 6 : 0.6398796334925164
Loss in iteration 7 : 0.6303147793398004
Loss in iteration 8 : 0.6251010450489457
Loss in iteration 9 : 0.6145171897899866
Loss in iteration 10 : 0.6084799191501263
Loss in iteration 11 : 0.5982429001284968
Loss in iteration 12 : 0.5917704869346342
Loss in iteration 13 : 0.583122905434388
Loss in iteration 14 : 0.5770352440320289
Loss in iteration 15 : 0.5703244024245505
Loss in iteration 16 : 0.5651575314765116
Loss in iteration 17 : 0.5601273276104559
Loss in iteration 18 : 0.5559811033825367
Loss in iteration 19 : 0.5521735053362268
Loss in iteration 20 : 0.5488680929932919
Loss in iteration 21 : 0.5458619534408681
Loss in iteration 22 : 0.5431489396427313
Loss in iteration 23 : 0.5406494855655338
Loss in iteration 24 : 0.5383339857980095
Loss in iteration 25 : 0.5361681862257786
Loss in iteration 26 : 0.5341312023841629
Loss in iteration 27 : 0.5322065516740065
Loss in iteration 28 : 0.5303819581274725
Loss in iteration 29 : 0.5286481246298306
Loss in iteration 30 : 0.5269973287491472
Loss in iteration 31 : 0.5254232869722223
Loss in iteration 32 : 0.5239204457808664
Loss in iteration 33 : 0.5224839733468482
Loss in iteration 34 : 0.5211094801988508
Loss in iteration 35 : 0.519793012393832
Loss in iteration 36 : 0.5185309419780219
Loss in iteration 37 : 0.5173199517108158
Loss in iteration 38 : 0.5161569850204712
Loss in iteration 39 : 0.5150392275014837
Loss in iteration 40 : 0.5139640783216727
Loss in iteration 41 : 0.5129291326104749
Loss in iteration 42 : 0.5119321619374445
Loss in iteration 43 : 0.5109710991608414
Loss in iteration 44 : 0.5100440237353935
Loss in iteration 45 : 0.5091491491853201
Loss in iteration 46 : 0.5082848115348203
Loss in iteration 47 : 0.5074494590747481
Loss in iteration 48 : 0.5066416430544824
Loss in iteration 49 : 0.5058600093249991
Loss in iteration 50 : 0.5051032907601695
Loss in iteration 51 : 0.5043703004042516
Loss in iteration 52 : 0.5036599252503394
Loss in iteration 53 : 0.502971120590755
Loss in iteration 54 : 0.5023029048753512
Loss in iteration 55 : 0.5016543550271195
Loss in iteration 56 : 0.5010246021675995
Loss in iteration 57 : 0.5004128277112893
Loss in iteration 58 : 0.49981825979236383
Loss in iteration 59 : 0.4992401699912507
Loss in iteration 60 : 0.49867787033217564
Loss in iteration 61 : 0.4981307105258712
Loss in iteration 62 : 0.49759807543448425
Loss in iteration 63 : 0.4970793827380465
Loss in iteration 64 : 0.4965740807841346
Loss in iteration 65 : 0.4960816466041239
Loss in iteration 66 : 0.4956015840812256
Loss in iteration 67 : 0.4951334222569318
Loss in iteration 68 : 0.4946767137638133
Loss in iteration 69 : 0.4942310333738475
Loss in iteration 70 : 0.49379597665243596
Loss in iteration 71 : 0.4933711587092746
Loss in iteration 72 : 0.49295621303802567
Loss in iteration 73 : 0.49255079043752903
Loss in iteration 74 : 0.49215455800794483
Loss in iteration 75 : 0.4917671982158233
Loss in iteration 76 : 0.4913884080226461
Loss in iteration 77 : 0.49101789807187357
Loss in iteration 78 : 0.49065539192996194
Loss in iteration 79 : 0.4903006253772199
Loss in iteration 80 : 0.48995334574472604
Loss in iteration 81 : 0.48961331129385544
Loss in iteration 82 : 0.48928029063526074
Loss in iteration 83 : 0.4889540621844021
Loss in iteration 84 : 0.4886344136509743
Loss in iteration 85 : 0.48832114155981304
Loss in iteration 86 : 0.4880140508010015
Loss in iteration 87 : 0.487712954207164
Loss in iteration 88 : 0.48741767215602816
Loss in iteration 89 : 0.48712803219649964
Loss in iteration 90 : 0.486843868696679
Loss in iteration 91 : 0.4865650225122987
Loss in iteration 92 : 0.48629134067423385
Loss in iteration 93 : 0.4860226760938038
Loss in iteration 94 : 0.4857588872847138
Loss in iteration 95 : 0.4854998381005367
Loss in iteration 96 : 0.4852453974867349
Loss in iteration 97 : 0.4849954392462981
Loss in iteration 98 : 0.48474984181812886
Loss in iteration 99 : 0.48450848806737634
Loss in iteration 100 : 0.48427126508697643
Loss in iteration 101 : 0.4840380640097045
Loss in iteration 102 : 0.4838087798300982
Loss in iteration 103 : 0.48358331123565507
Loss in iteration 104 : 0.48336156044674394
Loss in iteration 105 : 0.48314343306471474
Loss in iteration 106 : 0.4829288379277129
Loss in iteration 107 : 0.48271768697376455
Loss in iteration 108 : 0.48250989511069364
Loss in iteration 109 : 0.4823053800924854
Loss in iteration 110 : 0.4821040624017211
Loss in iteration 111 : 0.4819058651377529
Loss in iteration 112 : 0.4817107139102884
Loss in iteration 113 : 0.4815185367380726
Loss in iteration 114 : 0.48132926395240416
Loss in iteration 115 : 0.48114282810521003
Loss in iteration 116 : 0.4809591638814337
Loss in iteration 117 : 0.48077820801550003
Loss in iteration 118 : 0.48059989921163665
Loss in iteration 119 : 0.48042417806785975
Loss in iteration 120 : 0.48025098700341423
Loss in iteration 121 : 0.48008027018949645
Loss in iteration 122 : 0.4799119734830791
Loss in iteration 123 : 0.47974604436370283
Loss in iteration 124 : 0.47958243187304195
Loss in iteration 125 : 0.47942108655714266
Loss in iteration 126 : 0.47926196041117464
Loss in iteration 127 : 0.47910500682657925
Loss in iteration 128 : 0.47895018054048555
Loss in iteration 129 : 0.4787974375873023
Loss in iteration 130 : 0.4786467352523486
Loss in iteration 131 : 0.4784980320274569
Loss in iteration 132 : 0.47835128756842143
Loss in iteration 133 : 0.4782064626542309
Loss in iteration 134 : 0.4780635191479735
Loss in iteration 135 : 0.4779224199593632
Loss in iteration 136 : 0.4777831290087815
Loss in iteration 137 : 0.4776456111927898
Loss in iteration 138 : 0.4775098323510242
Loss in iteration 139 : 0.47737575923441966
Loss in iteration 140 : 0.4772433594746957
Loss in iteration 141 : 0.47711260155506147
Loss in iteration 142 : 0.47698345478204957
Loss in iteration 143 : 0.47685588925847516
Loss in iteration 144 : 0.4767298758574303
Loss in iteration 145 : 0.4766053861972898
Loss in iteration 146 : 0.4764823926176697
Loss in iteration 147 : 0.4763608681563114
Loss in iteration 148 : 0.47624078652683965
Loss in iteration 149 : 0.4761221220973482
Loss in iteration 150 : 0.47600484986981
Loss in iteration 151 : 0.47588894546024
Loss in iteration 152 : 0.47577438507959025
Loss in iteration 153 : 0.4756611455153665
Loss in iteration 154 : 0.475549204113897
Loss in iteration 155 : 0.475438538763266
Loss in iteration 156 : 0.47532912787684906
Loss in iteration 157 : 0.47522095037746076
Loss in iteration 158 : 0.475113985682052
Loss in iteration 159 : 0.4750082136869624
Loss in iteration 160 : 0.4749036147537009
Loss in iteration 161 : 0.47480016969520844
Loss in iteration 162 : 0.47469785976262335
Loss in iteration 163 : 0.47459666663250344
Loss in iteration 164 : 0.47449657239447485
Loss in iteration 165 : 0.4743975595393335
Loss in iteration 166 : 0.4742996109475357
Loss in iteration 167 : 0.4742027098780842
Loss in iteration 168 : 0.47410683995780484
Loss in iteration 169 : 0.47401198517096943
Loss in iteration 170 : 0.4739181298492739
Loss in iteration 171 : 0.47382525866215874
Loss in iteration 172 : 0.4737333566074379
Loss in iteration 173 : 0.4736424090022472
Loss in iteration 174 : 0.4735524014742968
Loss in iteration 175 : 0.4734633199533864
Loss in iteration 176 : 0.473375150663229
Loss in iteration 177 : 0.4732878801135174
Loss in iteration 178 : 0.4732014950922543
Loss in iteration 179 : 0.47311598265833005
Loss in iteration 180 : 0.47303133013433046
Loss in iteration 181 : 0.4729475250995895
Loss in iteration 182 : 0.47286455538344163
Loss in iteration 183 : 0.47278240905870417
Loss in iteration 184 : 0.4727010744353524
Loss in iteration 185 : 0.47262054005439663
Loss in iteration 186 : 0.4725407946819488
Loss in iteration 187 : 0.4724618273034765
Loss in iteration 188 : 0.4723836271182251
Loss in iteration 189 : 0.4723061835338204
Loss in iteration 190 : 0.47222948616102667
Loss in iteration 191 : 0.4721535248086673
Loss in iteration 192 : 0.47207828947870717
Loss in iteration 193 : 0.472003770361464
Loss in iteration 194 : 0.4719299578309833
Loss in iteration 195 : 0.47185684244053716
Loss in iteration 196 : 0.47178441491825845
Loss in iteration 197 : 0.4717126661629122
Loss in iteration 198 : 0.47164158723978106
Loss in iteration 199 : 0.4715711693766749
Loss in iteration 200 : 0.4715014039600597
Loss in iteration 201 : 0.47143228253129044
Loss in iteration 202 : 0.4713637967829613
Loss in iteration 203 : 0.47129593855535656
Loss in iteration 204 : 0.47122869983300153
Loss in iteration 205 : 0.4711620727413124
Loss in iteration 206 : 0.4710960495433454
Loss in iteration 207 : 0.47103062263662854
Loss in iteration 208 : 0.47096578455009064
Loss in iteration 209 : 0.47090152794107054
Loss in iteration 210 : 0.47083784559241887
Loss in iteration 211 : 0.47077473040966267
Loss in iteration 212 : 0.4707121754182628
Loss in iteration 213 : 0.4706501737609447
Loss in iteration 214 : 0.4705887186950972
Loss in iteration 215 : 0.47052780359024066
Loss in iteration 216 : 0.4704674219255768
Loss in iteration 217 : 0.4704075672875832
Loss in iteration 218 : 0.4703482333676953
Loss in iteration 219 : 0.47028941396002816
Loss in iteration 220 : 0.4702311029591797
Loss in iteration 221 : 0.4701732943580792
Loss in iteration 222 : 0.47011598224588774
Loss in iteration 223 : 0.47005916080597465
Loss in iteration 224 : 0.47000282431392404
Loss in iteration 225 : 0.4699469671356077
Loss in iteration 226 : 0.469891583725298
Loss in iteration 227 : 0.4698366686238389
Loss in iteration 228 : 0.46978221645686075
Loss in iteration 229 : 0.46972822193303376
Loss in iteration 230 : 0.46967467984237493
Loss in iteration 231 : 0.4696215850545949
Loss in iteration 232 : 0.46956893251748794
Loss in iteration 233 : 0.46951671725535415
Loss in iteration 234 : 0.46946493436747505
Loss in iteration 235 : 0.4694135790266162
Loss in iteration 236 : 0.4693626464775666
Loss in iteration 237 : 0.4693121320357279
Loss in iteration 238 : 0.4692620310857181
Loss in iteration 239 : 0.4692123390800274
Loss in iteration 240 : 0.46916305153769383
Loss in iteration 241 : 0.469114164043019
Loss in iteration 242 : 0.46906567224431195
Loss in iteration 243 : 0.46901757185266185
Loss in iteration 244 : 0.4689698586407441
Loss in iteration 245 : 0.4689225284416527
Loss in iteration 246 : 0.46887557714775424
Loss in iteration 247 : 0.468829000709584
Loss in iteration 248 : 0.468782795134754
Loss in iteration 249 : 0.4687369564868883
Loss in iteration 250 : 0.4686914808845964
Loss in iteration 251 : 0.4686463645004492
Loss in iteration 252 : 0.46860160355999864
Loss in iteration 253 : 0.46855719434080506
Loss in iteration 254 : 0.4685131331715005
Loss in iteration 255 : 0.4684694164308564
Loss in iteration 256 : 0.46842604054689196
Loss in iteration 257 : 0.46838300199598665
Loss in iteration 258 : 0.46834029730202154
Loss in iteration 259 : 0.46829792303553286
Loss in iteration 260 : 0.46825587581290096
Loss in iteration 261 : 0.46821415229552815
Loss in iteration 262 : 0.4681727491890668
Loss in iteration 263 : 0.46813166324264355
Loss in iteration 264 : 0.4680908912481043
Loss in iteration 265 : 0.46805043003928815
Loss in iteration 266 : 0.46801027649128846
Loss in iteration 267 : 0.46797042751977025
Loss in iteration 268 : 0.4679308800802678
Loss in iteration 269 : 0.4678916311675139
Loss in iteration 270 : 0.46785267781477785
Loss in iteration 271 : 0.46781401709322257
Loss in iteration 272 : 0.4677756461112722
Loss in iteration 273 : 0.46773756201398997
Loss in iteration 274 : 0.4676997619824745
Loss in iteration 275 : 0.46766224323327094
Loss in iteration 276 : 0.4676250030177856
Loss in iteration 277 : 0.4675880386217136
Loss in iteration 278 : 0.4675513473644936
Loss in iteration 279 : 0.46751492659875304
Loss in iteration 280 : 0.4674787737097765
Loss in iteration 281 : 0.46744288611498214
Loss in iteration 282 : 0.4674072612634165
Loss in iteration 283 : 0.4673718966352362
Loss in iteration 284 : 0.46733678974123277
Loss in iteration 285 : 0.46730193812234483
Loss in iteration 286 : 0.46726733934917797
Loss in iteration 287 : 0.4672329910215542
Loss in iteration 288 : 0.4671988907680499
Loss in iteration 289 : 0.46716503624555356
Loss in iteration 290 : 0.4671314251388329
Loss in iteration 291 : 0.46709805516009834
Loss in iteration 292 : 0.46706492404859307
Loss in iteration 293 : 0.4670320295701806
Loss in iteration 294 : 0.46699936951693644
Loss in iteration 295 : 0.46696694170675745
Loss in iteration 296 : 0.4669347439829712
Loss in iteration 297 : 0.46690277421396204
Loss in iteration 298 : 0.46687103029278887
Loss in iteration 299 : 0.46683951013682723
Loss in iteration 300 : 0.4668082116874019
Loss in iteration 301 : 0.4667771329094446
Loss in iteration 302 : 0.46674627179114064
Loss in iteration 303 : 0.4667156263435926
Loss in iteration 304 : 0.46668519460049024
Loss in iteration 305 : 0.4666549746177734
Loss in iteration 306 : 0.46662496447332474
Loss in iteration 307 : 0.46659516226664655
Loss in iteration 308 : 0.46656556611855604
Loss in iteration 309 : 0.46653617417087323
Loss in iteration 310 : 0.46650698458613266
Loss in iteration 311 : 0.46647799554728975
Loss in iteration 312 : 0.466449205257426
Loss in iteration 313 : 0.4664206119394776
Loss in iteration 314 : 0.4663922138359459
Loss in iteration 315 : 0.4663640092086353
Loss in iteration 316 : 0.4663359963383802
Testing accuracy  of updater 0 on alg 0 with rate 0.7 = 0.789, training accuracy 0.789, time elapsed: 8366 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6779278019447145
Loss in iteration 3 : 0.6676407914035285
Loss in iteration 4 : 0.6587709164435137
Loss in iteration 5 : 0.6507863015112562
Loss in iteration 6 : 0.643513029278568
Loss in iteration 7 : 0.636837665360076
Loss in iteration 8 : 0.6306748010676408
Loss in iteration 9 : 0.6249574519740635
Loss in iteration 10 : 0.6196318559878254
Loss in iteration 11 : 0.6146540004663777
Loss in iteration 12 : 0.6099872481511689
Loss in iteration 13 : 0.6056006633492057
Loss in iteration 14 : 0.6014678063015334
Loss in iteration 15 : 0.5975658449815987
Loss in iteration 16 : 0.5938748873368646
Loss in iteration 17 : 0.5903774697477799
Loss in iteration 18 : 0.5870581584312776
Loss in iteration 19 : 0.5839032340327633
Loss in iteration 20 : 0.5809004385600937
Loss in iteration 21 : 0.5780387697905357
Loss in iteration 22 : 0.5753083123650403
Loss in iteration 23 : 0.5727000976214117
Loss in iteration 24 : 0.5702059862213009
Loss in iteration 25 : 0.5678185690625449
Loss in iteration 26 : 0.5655310830132839
Loss in iteration 27 : 0.563337338774992
Loss in iteration 28 : 0.5612316587571651
Loss in iteration 29 : 0.5592088232817579
Loss in iteration 30 : 0.5572640237682861
Loss in iteration 31 : 0.5553928218080073
Loss in iteration 32 : 0.5535911132364474
Loss in iteration 33 : 0.5518550964720804
Loss in iteration 34 : 0.5501812445149707
Loss in iteration 35 : 0.5485662801002213
Loss in iteration 36 : 0.5470071535827474
Loss in iteration 37 : 0.5455010231964534
Loss in iteration 38 : 0.5440452373852681
Loss in iteration 39 : 0.5426373189485092
Loss in iteration 40 : 0.5412749507802952
Loss in iteration 41 : 0.5399559630138069
Loss in iteration 42 : 0.5386783214072612
Loss in iteration 43 : 0.5374401168304667
Loss in iteration 44 : 0.5362395557293009
Loss in iteration 45 : 0.5350749514613863
Loss in iteration 46 : 0.5339447164095743
Loss in iteration 47 : 0.5328473547914627
Loss in iteration 48 : 0.5317814560930583
Loss in iteration 49 : 0.5307456890632463
Loss in iteration 50 : 0.5297387962131165
Loss in iteration 51 : 0.5287595887706221
Loss in iteration 52 : 0.5278069420466057
Loss in iteration 53 : 0.526879791173118
Loss in iteration 54 : 0.5259771271791989
Loss in iteration 55 : 0.5250979933730177
Loss in iteration 56 : 0.5242414820025653
Loss in iteration 57 : 0.5234067311699714
Loss in iteration 58 : 0.5225929219770692
Loss in iteration 59 : 0.5217992758820774
Loss in iteration 60 : 0.5210250522493023
Loss in iteration 61 : 0.5202695460754901
Loss in iteration 62 : 0.5195320858780793
Loss in iteration 63 : 0.5188120317319836
Loss in iteration 64 : 0.5181087734428138
Loss in iteration 65 : 0.517421728845547
Loss in iteration 66 : 0.5167503422186803
Loss in iteration 67 : 0.5160940828047963
Loss in iteration 68 : 0.5154524434292755
Loss in iteration 69 : 0.514824939209627
Loss in iteration 70 : 0.5142111063485679
Loss in iteration 71 : 0.513610501004574
Loss in iteration 72 : 0.5130226982341369
Loss in iteration 73 : 0.5124472910004818
Loss in iteration 74 : 0.5118838892439266
Loss in iteration 75 : 0.5113321190094402
Loss in iteration 76 : 0.5107916216273436
Loss in iteration 77 : 0.5102620529434213
Loss in iteration 78 : 0.5097430825949905
Loss in iteration 79 : 0.5092343933297715
Loss in iteration 80 : 0.508735680364635
Loss in iteration 81 : 0.5082466507815262
Loss in iteration 82 : 0.5077670229580906
Loss in iteration 83 : 0.5072965260306903
Loss in iteration 84 : 0.506834899387684
Loss in iteration 85 : 0.5063818921910093
Loss in iteration 86 : 0.5059372629242417
Loss in iteration 87 : 0.5055007789654353
Loss in iteration 88 : 0.5050722161831785
Loss in iteration 89 : 0.5046513585544127
Loss in iteration 90 : 0.5042379978026503
Loss in iteration 91 : 0.5038319330553551
Loss in iteration 92 : 0.5034329705192904
Loss in iteration 93 : 0.5030409231727705
Loss in iteration 94 : 0.5026556104737671
Loss in iteration 95 : 0.5022768580829728
Loss in iteration 96 : 0.501904497600901
Loss in iteration 97 : 0.5015383663182026
Loss in iteration 98 : 0.5011783069784628
Loss in iteration 99 : 0.5008241675527256
Loss in iteration 100 : 0.5004758010250882
Loss in iteration 101 : 0.5001330651887376
Loss in iteration 102 : 0.49979582245183873
Loss in iteration 103 : 0.4994639396527456
Loss in iteration 104 : 0.4991372878839733
Loss in iteration 105 : 0.4988157423245
Loss in iteration 106 : 0.4984991820799097
Loss in iteration 107 : 0.4981874900299655
Loss in iteration 108 : 0.49788055268322123
Loss in iteration 109 : 0.4975782600382759
Loss in iteration 110 : 0.4972805054513359
Loss in iteration 111 : 0.4969871855097568
Loss in iteration 112 : 0.4966981999112265
Loss in iteration 113 : 0.4964134513483415
Loss in iteration 114 : 0.4961328453982476
Loss in iteration 115 : 0.4958562904171384
Loss in iteration 116 : 0.49558369743932323
Loss in iteration 117 : 0.49531498008065716
Loss in iteration 118 : 0.4950500544461095
Loss in iteration 119 : 0.4947888390412766
Loss in iteration 120 : 0.4945312546876106
Loss in iteration 121 : 0.49427722444123
Loss in iteration 122 : 0.4940266735151037
Loss in iteration 123 : 0.4937795292044523
Loss in iteration 124 : 0.49353572081522257
Loss in iteration 125 : 0.49329517959549013
Loss in iteration 126 : 0.49305783866961994
Loss in iteration 127 : 0.49282363297510456
Loss in iteration 128 : 0.49259249920192394
Loss in iteration 129 : 0.4923643757343028
Loss in iteration 130 : 0.49213920259478744
Loss in iteration 131 : 0.4919169213905042
Loss in iteration 132 : 0.4916974752615231
Loss in iteration 133 : 0.49148080883120576
Loss in iteration 134 : 0.49126686815848586
Loss in iteration 135 : 0.49105560069196147
Loss in iteration 136 : 0.49084695522572824
Loss in iteration 137 : 0.49064088185689186
Loss in iteration 138 : 0.49043733194467315
Loss in iteration 139 : 0.49023625807102855
Loss in iteration 140 : 0.49003761400274426
Loss in iteration 141 : 0.4898413546549156
Loss in iteration 142 : 0.4896474360557713
Loss in iteration 143 : 0.48945581531277177
Loss in iteration 144 : 0.48926645057994633
Loss in iteration 145 : 0.48907930102638164
Loss in iteration 146 : 0.48889432680586575
Loss in iteration 147 : 0.4887114890275917
Loss in iteration 148 : 0.48853074972789523
Loss in iteration 149 : 0.4883520718429992
Loss in iteration 150 : 0.48817541918270274
Loss in iteration 151 : 0.48800075640497326
Loss in iteration 152 : 0.4878280489914378
Loss in iteration 153 : 0.4876572632236955
Loss in iteration 154 : 0.48748836616045027
Loss in iteration 155 : 0.4873213256154121
Loss in iteration 156 : 0.4871561101359552
Loss in iteration 157 : 0.48699268898247927
Loss in iteration 158 : 0.4868310321084642
Loss in iteration 159 : 0.4866711101411999
Loss in iteration 160 : 0.4865128943631255
Loss in iteration 161 : 0.4863563566938121
Loss in iteration 162 : 0.48620146967250516
Loss in iteration 163 : 0.4860482064412554
Loss in iteration 164 : 0.4858965407285874
Loss in iteration 165 : 0.4857464468336866
Loss in iteration 166 : 0.48559789961110617
Loss in iteration 167 : 0.4854508744559418
Loss in iteration 168 : 0.485305347289495
Loss in iteration 169 : 0.4851612945453725
Loss in iteration 170 : 0.48501869315603063
Loss in iteration 171 : 0.4848775205397309
Loss in iteration 172 : 0.4847377545879046
Loss in iteration 173 : 0.48459937365291045
Loss in iteration 174 : 0.48446235653615916
Loss in iteration 175 : 0.48432668247660704
Loss in iteration 176 : 0.4841923311395957
Loss in iteration 177 : 0.48405928260603265
Loss in iteration 178 : 0.4839275173618934
Loss in iteration 179 : 0.48379701628803845
Loss in iteration 180 : 0.48366776065034023
Loss in iteration 181 : 0.48353973209008916
Loss in iteration 182 : 0.4834129126146969
Loss in iteration 183 : 0.48328728458866205
Loss in iteration 184 : 0.48316283072480826
Loss in iteration 185 : 0.48303953407576555
Loss in iteration 186 : 0.48291737802571294
Loss in iteration 187 : 0.48279634628235363
Loss in iteration 188 : 0.4826764228691017
Loss in iteration 189 : 0.4825575921175274
Loss in iteration 190 : 0.48243983865998846
Loss in iteration 191 : 0.48232314742247956
Loss in iteration 192 : 0.4822075036176808
Loss in iteration 193 : 0.48209289273820766
Loss in iteration 194 : 0.4819793005500421
Loss in iteration 195 : 0.4818667130861438
Loss in iteration 196 : 0.4817551166402535
Loss in iteration 197 : 0.4816444977608392
Loss in iteration 198 : 0.4815348432452438
Loss in iteration 199 : 0.48142614013395835
Loss in iteration 200 : 0.48131837570507036
Loss in iteration 201 : 0.48121153746886286
Loss in iteration 202 : 0.4811056131625492
Loss in iteration 203 : 0.4810005907451505
Loss in iteration 204 : 0.4808964583925208
Loss in iteration 205 : 0.48079320449249086
Loss in iteration 206 : 0.48069081764014643
Loss in iteration 207 : 0.48058928663323397
Loss in iteration 208 : 0.4804886004676805
Loss in iteration 209 : 0.48038874833323747
Loss in iteration 210 : 0.4802897196092264
Loss in iteration 211 : 0.4801915038604168
Loss in iteration 212 : 0.4800940908329787
Loss in iteration 213 : 0.47999747045057073
Loss in iteration 214 : 0.47990163281051035
Loss in iteration 215 : 0.47980656818004047
Loss in iteration 216 : 0.47971226699270797
Loss in iteration 217 : 0.47961871984480964
Loss in iteration 218 : 0.47952591749194745
Loss in iteration 219 : 0.47943385084566364
Loss in iteration 220 : 0.47934251097015224
Loss in iteration 221 : 0.4792518890790684
Loss in iteration 222 : 0.4791619765324007
Loss in iteration 223 : 0.479072764833433
Loss in iteration 224 : 0.4789842456257734
Loss in iteration 225 : 0.47889641069045824
Loss in iteration 226 : 0.478809251943127
Loss in iteration 227 : 0.4787227614312703
Loss in iteration 228 : 0.47863693133153173
Loss in iteration 229 : 0.4785517539470943
Loss in iteration 230 : 0.4784672217051074
Loss in iteration 231 : 0.4783833271541963
Loss in iteration 232 : 0.47830006296201666
Loss in iteration 233 : 0.47821742191287314
Loss in iteration 234 : 0.47813539690539586
Loss in iteration 235 : 0.478053980950261
Loss in iteration 236 : 0.47797316716798455
Loss in iteration 237 : 0.4778929487867496
Loss in iteration 238 : 0.47781331914029296
Loss in iteration 239 : 0.47773427166583665
Loss in iteration 240 : 0.47765579990207496
Loss in iteration 241 : 0.4775778974871963
Loss in iteration 242 : 0.47750055815696374
Loss in iteration 243 : 0.4774237757428311
Loss in iteration 244 : 0.4773475441700965
Loss in iteration 245 : 0.47727185745611816
Loss in iteration 246 : 0.4771967097085468
Loss in iteration 247 : 0.4771220951236146
Loss in iteration 248 : 0.4770480079844588
Loss in iteration 249 : 0.4769744426594717
Loss in iteration 250 : 0.47690139360071254
Loss in iteration 251 : 0.4768288553423242
Loss in iteration 252 : 0.4767568224990078
Loss in iteration 253 : 0.4766852897645249
Loss in iteration 254 : 0.4766142519102268
Loss in iteration 255 : 0.4765437037836224
Loss in iteration 256 : 0.4764736403069777
Loss in iteration 257 : 0.47640405647594536
Loss in iteration 258 : 0.47633494735821014
Loss in iteration 259 : 0.47626630809219705
Loss in iteration 260 : 0.4761981338857676
Loss in iteration 261 : 0.4761304200149764
Loss in iteration 262 : 0.47606316182283376
Loss in iteration 263 : 0.47599635471810153
Loss in iteration 264 : 0.4759299941741233
Loss in iteration 265 : 0.4758640757276636
Loss in iteration 266 : 0.4757985949777858
Loss in iteration 267 : 0.4757335475847384
Loss in iteration 268 : 0.47566892926888543
Loss in iteration 269 : 0.4756047358096437
Loss in iteration 270 : 0.4755409630444377
Loss in iteration 271 : 0.4754776068676986
Loss in iteration 272 : 0.47541466322986353
Loss in iteration 273 : 0.47535212813640126
Loss in iteration 274 : 0.4752899976468617
Loss in iteration 275 : 0.4752282678739456
Loss in iteration 276 : 0.4751669349825804
Loss in iteration 277 : 0.47510599518903945
Loss in iteration 278 : 0.47504544476004923
Loss in iteration 279 : 0.47498528001194207
Loss in iteration 280 : 0.4749254973098092
Loss in iteration 281 : 0.4748660930666751
Loss in iteration 282 : 0.4748070637426922
Loss in iteration 283 : 0.4747484058443425
Loss in iteration 284 : 0.4746901159236721
Loss in iteration 285 : 0.47463219057752165
Loss in iteration 286 : 0.47457462644678133
Loss in iteration 287 : 0.47451742021566656
Loss in iteration 288 : 0.474460568610994
Loss in iteration 289 : 0.4744040684014869
Loss in iteration 290 : 0.4743479163970751
Loss in iteration 291 : 0.4742921094482359
Loss in iteration 292 : 0.47423664444532015
Loss in iteration 293 : 0.4741815183179086
Loss in iteration 294 : 0.4741267280341743
Loss in iteration 295 : 0.4740722706002574
Loss in iteration 296 : 0.4740181430596601
Loss in iteration 297 : 0.47396434249263425
Loss in iteration 298 : 0.4739108660156081
Loss in iteration 299 : 0.47385771078059613
Loss in iteration 300 : 0.4738048739746397
Loss in iteration 301 : 0.47375235281924816
Loss in iteration 302 : 0.4737001445698584
Loss in iteration 303 : 0.4736482465152959
Loss in iteration 304 : 0.4735966559772509
Loss in iteration 305 : 0.47354537030976823
Loss in iteration 306 : 0.47349438689873524
Loss in iteration 307 : 0.4734437031613926
Loss in iteration 308 : 0.4733933165458443
Loss in iteration 309 : 0.47334322453058053
Loss in iteration 310 : 0.47329342462401314
Loss in iteration 311 : 0.47324391436400903
Loss in iteration 312 : 0.4731946913174474
Loss in iteration 313 : 0.4731457530797697
Loss in iteration 314 : 0.4730970972745487
Loss in iteration 315 : 0.4730487215530584
Loss in iteration 316 : 0.47300062359386036
Loss in iteration 317 : 0.4729528011023894
Loss in iteration 318 : 0.47290525181054355
Loss in iteration 319 : 0.4728579734762978
Loss in iteration 320 : 0.4728109638833085
Loss in iteration 321 : 0.4727642208405335
Loss in iteration 322 : 0.47271774218185353
Loss in iteration 323 : 0.4726715257657046
Loss in iteration 324 : 0.4726255694747179
Loss in iteration 325 : 0.47257987121535894
Loss in iteration 326 : 0.4725344289175826
Loss in iteration 327 : 0.47248924053449093
Loss in iteration 328 : 0.47244430404198634
Loss in iteration 329 : 0.47239961743845016
Loss in iteration 330 : 0.4723551787444183
Loss in iteration 331 : 0.4723109860022495
Loss in iteration 332 : 0.4722670372758203
Loss in iteration 333 : 0.4722233306502147
Loss in iteration 334 : 0.4721798642314237
Loss in iteration 335 : 0.47213663614603374
Loss in iteration 336 : 0.4720936445409539
Loss in iteration 337 : 0.4720508875831119
Loss in iteration 338 : 0.47200836345917757
Loss in iteration 339 : 0.4719660703752811
Loss in iteration 340 : 0.4719240065567465
Loss in iteration 341 : 0.47188217024781354
Loss in iteration 342 : 0.4718405597113789
Loss in iteration 343 : 0.4717991732287403
Loss in iteration 344 : 0.47175800909932947
Loss in iteration 345 : 0.47171706564047183
Loss in iteration 346 : 0.47167634118713603
Loss in iteration 347 : 0.4716358340916911
Loss in iteration 348 : 0.4715955427236631
Loss in iteration 349 : 0.47155546546951066
Loss in iteration 350 : 0.4715156007323844
Loss in iteration 351 : 0.47147594693190853
Loss in iteration 352 : 0.471436502503949
Loss in iteration 353 : 0.4713972659003996
Loss in iteration 354 : 0.4713582355889647
Loss in iteration 355 : 0.4713194100529464
Loss in iteration 356 : 0.47128078779103555
Loss in iteration 357 : 0.4712423673171106
Testing accuracy  of updater 0 on alg 0 with rate 0.4 = 0.784875, training accuracy 0.784875, time elapsed: 8211 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6878658918511898
Loss in iteration 3 : 0.6839999898353142
Loss in iteration 4 : 0.6808139087951758
Loss in iteration 5 : 0.6779759015464742
Loss in iteration 6 : 0.6753351733505979
Loss in iteration 7 : 0.6728218611194686
Loss in iteration 8 : 0.6704024793367385
Loss in iteration 9 : 0.6680600344706835
Loss in iteration 10 : 0.6657851055363639
Loss in iteration 11 : 0.6635718203046845
Loss in iteration 12 : 0.6614160283793032
Loss in iteration 13 : 0.6593144648356358
Loss in iteration 14 : 0.6572643624102471
Loss in iteration 15 : 0.6552632676432818
Loss in iteration 16 : 0.6533089502150735
Loss in iteration 17 : 0.6513993551912489
Loss in iteration 18 : 0.6495325752975566
Loss in iteration 19 : 0.6477068327923036
Loss in iteration 20 : 0.6459204661632976
Loss in iteration 21 : 0.6441719194500578
Loss in iteration 22 : 0.6424597331622375
Loss in iteration 23 : 0.6407825362970553
Loss in iteration 24 : 0.6391390392002929
Loss in iteration 25 : 0.6375280271260977
Loss in iteration 26 : 0.6359483544020409
Loss in iteration 27 : 0.6343989391308689
Loss in iteration 28 : 0.6328787583733315
Loss in iteration 29 : 0.63138684376433
Loss in iteration 30 : 0.6299222775200337
Loss in iteration 31 : 0.6284841887979914
Loss in iteration 32 : 0.6270717503759338
Loss in iteration 33 : 0.6256841756183785
Loss in iteration 34 : 0.6243207157031229
Loss in iteration 35 : 0.6229806570825679
Loss in iteration 36 : 0.6216633191572712
Loss in iteration 37 : 0.6203680521414728
Loss in iteration 38 : 0.6190942351023505
Loss in iteration 39 : 0.6178412741567088
Loss in iteration 40 : 0.6166086008103466
Loss in iteration 41 : 0.6153956704270135
Loss in iteration 42 : 0.6142019608150303
Loss in iteration 43 : 0.6130269709209849
Loss in iteration 44 : 0.6118702196209101
Loss in iteration 45 : 0.6107312446003315
Loss in iteration 46 : 0.6096096013154189
Loss in iteration 47 : 0.6085048620282412
Loss in iteration 48 : 0.6074166149098146
Loss in iteration 49 : 0.606344463205223
Loss in iteration 50 : 0.6052880244556702
Loss in iteration 51 : 0.6042469297727403
Loss in iteration 52 : 0.6032208231607084
Loss in iteration 53 : 0.6022093608829873
Loss in iteration 54 : 0.6012122108692547
Loss in iteration 55 : 0.600229052160073
Loss in iteration 56 : 0.5992595743861374
Loss in iteration 57 : 0.5983034772794824
Loss in iteration 58 : 0.5973604702142854
Loss in iteration 59 : 0.5964302717750442
Loss in iteration 60 : 0.5955126093501358
Loss in iteration 61 : 0.5946072187489426
Loss in iteration 62 : 0.5937138438408323
Loss in iteration 63 : 0.5928322362144686
Loss in iteration 64 : 0.5919621548560601
Loss in iteration 65 : 0.5911033658451945
Loss in iteration 66 : 0.5902556420671362
Loss in iteration 67 : 0.5894187629404285
Loss in iteration 68 : 0.5885925141588142
Loss in iteration 69 : 0.5877766874465385
Loss in iteration 70 : 0.5869710803261741
Loss in iteration 71 : 0.5861754958981621
Loss in iteration 72 : 0.5853897426313479
Loss in iteration 73 : 0.5846136341638047
Loss in iteration 74 : 0.5838469891133456
Loss in iteration 75 : 0.5830896308971142
Loss in iteration 76 : 0.5823413875597203
Loss in iteration 77 : 0.5816020916094078
Loss in iteration 78 : 0.580871579861795
Loss in iteration 79 : 0.5801496932907491
Loss in iteration 80 : 0.579436276885978
Loss in iteration 81 : 0.578731179516969
Loss in iteration 82 : 0.5780342538029286
Loss in iteration 83 : 0.5773453559883697
Loss in iteration 84 : 0.5766643458240654
Loss in iteration 85 : 0.5759910864530619
Loss in iteration 86 : 0.5753254443014858
Loss in iteration 87 : 0.5746672889739062
Loss in iteration 88 : 0.5740164931529833
Loss in iteration 89 : 0.5733729325032358
Loss in iteration 90 : 0.572736485578649
Loss in iteration 91 : 0.5721070337339967
Loss in iteration 92 : 0.5714844610396388
Loss in iteration 93 : 0.5708686541996695
Loss in iteration 94 : 0.5702595024732089
Loss in iteration 95 : 0.5696568975987187
Loss in iteration 96 : 0.5690607337211947
Loss in iteration 97 : 0.5684709073220674
Loss in iteration 98 : 0.5678873171517357
Loss in iteration 99 : 0.5673098641645661
Loss in iteration 100 : 0.5667384514562779
Loss in iteration 101 : 0.5661729842035761
Loss in iteration 102 : 0.5656133696059705
Loss in iteration 103 : 0.5650595168296256
Loss in iteration 104 : 0.5645113369532251
Loss in iteration 105 : 0.5639687429156889
Loss in iteration 106 : 0.563431649465731
Loss in iteration 107 : 0.5628999731131152
Loss in iteration 108 : 0.5623736320815962
Loss in iteration 109 : 0.561852546263424
Loss in iteration 110 : 0.561336637175388
Loss in iteration 111 : 0.5608258279163042
Loss in iteration 112 : 0.5603200431259152
Loss in iteration 113 : 0.5598192089451138
Loss in iteration 114 : 0.5593232529774745
Loss in iteration 115 : 0.5588321042520125
Loss in iteration 116 : 0.5583456931871228
Loss in iteration 117 : 0.5578639515556751
Loss in iteration 118 : 0.5573868124511995
Loss in iteration 119 : 0.5569142102551149
Loss in iteration 120 : 0.5564460806049967
Loss in iteration 121 : 0.5559823603637962
Loss in iteration 122 : 0.5555229875900161
Loss in iteration 123 : 0.5550679015087795
Loss in iteration 124 : 0.5546170424837766
Loss in iteration 125 : 0.5541703519900384
Loss in iteration 126 : 0.5537277725875431
Loss in iteration 127 : 0.5532892478955703
Loss in iteration 128 : 0.5528547225678234
Loss in iteration 129 : 0.552424142268282
Loss in iteration 130 : 0.5519974536477373
Loss in iteration 131 : 0.5515746043210106
Loss in iteration 132 : 0.5511555428448187
Loss in iteration 133 : 0.5507402186962724
Loss in iteration 134 : 0.5503285822519699
Loss in iteration 135 : 0.5499205847676966
Loss in iteration 136 : 0.5495161783586641
Loss in iteration 137 : 0.5491153159803134
Loss in iteration 138 : 0.5487179514096426
Loss in iteration 139 : 0.5483240392270542
Loss in iteration 140 : 0.5479335347986641
Loss in iteration 141 : 0.5475463942591394
Loss in iteration 142 : 0.5471625744949411
Loss in iteration 143 : 0.5467820331280697
Loss in iteration 144 : 0.5464047285001964
Loss in iteration 145 : 0.5460306196572451
Loss in iteration 146 : 0.5456596663343598
Loss in iteration 147 : 0.5452918289412833
Loss in iteration 148 : 0.5449270685480945
Loss in iteration 149 : 0.5445653468713367
Loss in iteration 150 : 0.5442066262604823
Loss in iteration 151 : 0.5438508696847607
Loss in iteration 152 : 0.5434980407203079
Loss in iteration 153 : 0.543148103537654
Loss in iteration 154 : 0.5428010228895156
Loss in iteration 155 : 0.5424567640989
Loss in iteration 156 : 0.5421152930474964
Loss in iteration 157 : 0.5417765761643755
Loss in iteration 158 : 0.541440580414948
Loss in iteration 159 : 0.5411072732901961
Loss in iteration 160 : 0.5407766227961857
Loss in iteration 161 : 0.5404485974438105
Loss in iteration 162 : 0.5401231662388015
Loss in iteration 163 : 0.5398002986719714
Loss in iteration 164 : 0.5394799647096882
Loss in iteration 165 : 0.5391621347845819
Loss in iteration 166 : 0.5388467797864687
Loss in iteration 167 : 0.5385338710534981
Loss in iteration 168 : 0.5382233803634981
Loss in iteration 169 : 0.5379152799255184
Loss in iteration 170 : 0.5376095423715903
Loss in iteration 171 : 0.5373061407486655
Loss in iteration 172 : 0.537005048510732
Loss in iteration 173 : 0.5367062395111318
Loss in iteration 174 : 0.5364096879950401
Loss in iteration 175 : 0.5361153685921249
Loss in iteration 176 : 0.5358232563093577
Loss in iteration 177 : 0.5355333265240092
Loss in iteration 178 : 0.5352455549767818
Loss in iteration 179 : 0.5349599177651111
Loss in iteration 180 : 0.5346763913366124
Loss in iteration 181 : 0.5343949524826606
Loss in iteration 182 : 0.534115578332136
Loss in iteration 183 : 0.5338382463452856
Loss in iteration 184 : 0.5335629343077333
Loss in iteration 185 : 0.5332896203246098
Loss in iteration 186 : 0.5330182828148268
Loss in iteration 187 : 0.5327489005054538
Loss in iteration 188 : 0.5324814524262329
Loss in iteration 189 : 0.5322159179042092
Loss in iteration 190 : 0.531952276558467
Loss in iteration 191 : 0.5316905082949829
Loss in iteration 192 : 0.5314305933016
Loss in iteration 193 : 0.5311725120430877
Loss in iteration 194 : 0.5309162452563184
Loss in iteration 195 : 0.530661773945549
Loss in iteration 196 : 0.5304090793777873
Loss in iteration 197 : 0.5301581430782684
Loss in iteration 198 : 0.5299089468260186
Loss in iteration 199 : 0.5296614726495069
Loss in iteration 200 : 0.5294157028224009
Loss in iteration 201 : 0.529171619859389
Loss in iteration 202 : 0.5289292065121094
Loss in iteration 203 : 0.5286884457651447
Loss in iteration 204 : 0.528449320832105
Loss in iteration 205 : 0.5282118151517964
Loss in iteration 206 : 0.5279759123844546
Loss in iteration 207 : 0.527741596408059
Loss in iteration 208 : 0.5275088513147268
Loss in iteration 209 : 0.5272776614071684
Loss in iteration 210 : 0.5270480111952226
Loss in iteration 211 : 0.5268198853924541
Loss in iteration 212 : 0.5265932689128179
Loss in iteration 213 : 0.5263681468673931
Loss in iteration 214 : 0.5261445045611867
Loss in iteration 215 : 0.5259223274899767
Loss in iteration 216 : 0.525701601337246
Loss in iteration 217 : 0.5254823119711537
Loss in iteration 218 : 0.525264445441576
Loss in iteration 219 : 0.5250479879772034
Loss in iteration 220 : 0.5248329259826839
Loss in iteration 221 : 0.5246192460358357
Loss in iteration 222 : 0.5244069348849016
Loss in iteration 223 : 0.5241959794458578
Loss in iteration 224 : 0.5239863667997806
Loss in iteration 225 : 0.5237780841902514
Loss in iteration 226 : 0.5235711190208238
Loss in iteration 227 : 0.5233654588525203
Loss in iteration 228 : 0.5231610914013951
Loss in iteration 229 : 0.5229580045361306
Loss in iteration 230 : 0.5227561862756805
Loss in iteration 231 : 0.5225556247869576
Loss in iteration 232 : 0.5223563083825657
Loss in iteration 233 : 0.522158225518568
Loss in iteration 234 : 0.521961364792302
Loss in iteration 235 : 0.5217657149402345
Loss in iteration 236 : 0.5215712648358486
Loss in iteration 237 : 0.5213780034875749
Loss in iteration 238 : 0.5211859200367606
Loss in iteration 239 : 0.5209950037556766
Loss in iteration 240 : 0.5208052440455458
Loss in iteration 241 : 0.5206166304346289
Loss in iteration 242 : 0.5204291525763342
Loss in iteration 243 : 0.5202428002473548
Loss in iteration 244 : 0.5200575633458454
Loss in iteration 245 : 0.5198734318896359
Loss in iteration 246 : 0.5196903960144641
Loss in iteration 247 : 0.519508445972253
Loss in iteration 248 : 0.5193275721294079
Loss in iteration 249 : 0.5191477649651509
Loss in iteration 250 : 0.5189690150698778
Loss in iteration 251 : 0.5187913131435484
Loss in iteration 252 : 0.5186146499941027
Loss in iteration 253 : 0.5184390165359016
Loss in iteration 254 : 0.5182644037882066
Loss in iteration 255 : 0.5180908028736669
Loss in iteration 256 : 0.5179182050168456
Loss in iteration 257 : 0.5177466015427711
Loss in iteration 258 : 0.5175759838755042
Loss in iteration 259 : 0.5174063435367389
Loss in iteration 260 : 0.5172376721444212
Loss in iteration 261 : 0.5170699614113944
Loss in iteration 262 : 0.5169032031440651
Loss in iteration 263 : 0.516737389241093
Loss in iteration 264 : 0.5165725116921001
Loss in iteration 265 : 0.5164085625764037
Loss in iteration 266 : 0.5162455340617771
Loss in iteration 267 : 0.5160834184032118
Loss in iteration 268 : 0.5159222079417234
Loss in iteration 269 : 0.5157618951031566
Loss in iteration 270 : 0.5156024723970286
Loss in iteration 271 : 0.5154439324153743
Loss in iteration 272 : 0.5152862678316273
Loss in iteration 273 : 0.5151294713994994
Loss in iteration 274 : 0.5149735359518952
Loss in iteration 275 : 0.5148184543998424
Loss in iteration 276 : 0.5146642197314285
Loss in iteration 277 : 0.5145108250107636
Loss in iteration 278 : 0.5143582633769569
Loss in iteration 279 : 0.5142065280431145
Loss in iteration 280 : 0.5140556122953451
Loss in iteration 281 : 0.5139055094917906
Loss in iteration 282 : 0.5137562130616563
Loss in iteration 283 : 0.5136077165042855
Loss in iteration 284 : 0.5134600133882141
Loss in iteration 285 : 0.5133130973502673
Loss in iteration 286 : 0.5131669620946526
Loss in iteration 287 : 0.5130216013920829
Loss in iteration 288 : 0.5128770090788973
Loss in iteration 289 : 0.5127331790562093
Loss in iteration 290 : 0.5125901052890552
Loss in iteration 291 : 0.5124477818055727
Loss in iteration 292 : 0.5123062026961755
Loss in iteration 293 : 0.5121653621127491
Loss in iteration 294 : 0.5120252542678584
Loss in iteration 295 : 0.5118858734339649
Loss in iteration 296 : 0.5117472139426605
Loss in iteration 297 : 0.5116092701839083
Loss in iteration 298 : 0.5114720366052993
Loss in iteration 299 : 0.5113355077113143
Loss in iteration 300 : 0.511199678062603
Loss in iteration 301 : 0.5110645422752719
Loss in iteration 302 : 0.5109300950201827
Loss in iteration 303 : 0.5107963310222609
Loss in iteration 304 : 0.5106632450598184
Loss in iteration 305 : 0.5105308319638783
Loss in iteration 306 : 0.5103990866175199
Loss in iteration 307 : 0.5102680039552241
Loss in iteration 308 : 0.5101375789622351
Loss in iteration 309 : 0.5100078066739298
Loss in iteration 310 : 0.5098786821751958
Loss in iteration 311 : 0.5097502005998161
Loss in iteration 312 : 0.5096223571298691
Loss in iteration 313 : 0.5094951469951343
Loss in iteration 314 : 0.5093685654725054
Loss in iteration 315 : 0.5092426078854125
Loss in iteration 316 : 0.5091172696032511
Loss in iteration 317 : 0.5089925460408267
Loss in iteration 318 : 0.5088684326578015
Loss in iteration 319 : 0.5087449249581436
Loss in iteration 320 : 0.5086220184895993
Loss in iteration 321 : 0.5084997088431618
Loss in iteration 322 : 0.5083779916525455
Loss in iteration 323 : 0.5082568625936771
Loss in iteration 324 : 0.5081363173841915
Loss in iteration 325 : 0.5080163517829221
Loss in iteration 326 : 0.5078969615894225
Loss in iteration 327 : 0.5077781426434751
Loss in iteration 328 : 0.5076598908246112
Loss in iteration 329 : 0.5075422020516441
Loss in iteration 330 : 0.5074250722822028
Loss in iteration 331 : 0.5073084975122709
Loss in iteration 332 : 0.5071924737757435
Loss in iteration 333 : 0.507076997143971
Loss in iteration 334 : 0.5069620637253294
Loss in iteration 335 : 0.506847669664787
Loss in iteration 336 : 0.5067338111434632
Loss in iteration 337 : 0.5066204843782344
Loss in iteration 338 : 0.5065076856212933
Loss in iteration 339 : 0.5063954111597535
Loss in iteration 340 : 0.50628365731524
Loss in iteration 341 : 0.5061724204434996
Loss in iteration 342 : 0.5060616969339983
Loss in iteration 343 : 0.5059514832095385
Loss in iteration 344 : 0.5058417757258781
Loss in iteration 345 : 0.5057325709713524
Loss in iteration 346 : 0.5056238654665051
Loss in iteration 347 : 0.505515655763716
Loss in iteration 348 : 0.5054079384468495
Loss in iteration 349 : 0.5053007101308836
Loss in iteration 350 : 0.5051939674615757
Loss in iteration 351 : 0.5050877071151011
Loss in iteration 352 : 0.5049819257977182
Loss in iteration 353 : 0.5048766202454271
Loss in iteration 354 : 0.5047717872236386
Loss in iteration 355 : 0.5046674235268466
Loss in iteration 356 : 0.5045635259783002
Loss in iteration 357 : 0.5044600914296863
Loss in iteration 358 : 0.5043571167608127
Loss in iteration 359 : 0.5042545988792987
Loss in iteration 360 : 0.5041525347202612
Loss in iteration 361 : 0.5040509212460225
Loss in iteration 362 : 0.503949755445797
Loss in iteration 363 : 0.5038490343354094
Loss in iteration 364 : 0.5037487549569932
Loss in iteration 365 : 0.5036489143787058
Loss in iteration 366 : 0.5035495096944476
Loss in iteration 367 : 0.5034505380235759
Loss in iteration 368 : 0.503351996510635
Loss in iteration 369 : 0.5032538823250764
Loss in iteration 370 : 0.5031561926609942
Loss in iteration 371 : 0.5030589247368574
Loss in iteration 372 : 0.5029620757952432
Loss in iteration 373 : 0.5028656431025849
Loss in iteration 374 : 0.5027696239489137
Loss in iteration 375 : 0.5026740156476
Loss in iteration 376 : 0.5025788155351144
Loss in iteration 377 : 0.502484020970772
Loss in iteration 378 : 0.5023896293364948
Loss in iteration 379 : 0.5022956380365675
Loss in iteration 380 : 0.5022020444974052
Loss in iteration 381 : 0.502108846167318
Loss in iteration 382 : 0.5020160405162775
Loss in iteration 383 : 0.5019236250356887
Loss in iteration 384 : 0.5018315972381701
Loss in iteration 385 : 0.5017399546573251
Loss in iteration 386 : 0.5016486948475241
Loss in iteration 387 : 0.5015578153836907
Loss in iteration 388 : 0.5014673138610828
Loss in iteration 389 : 0.5013771878950828
Loss in iteration 390 : 0.5012874351209897
Loss in iteration 391 : 0.5011980531938175
Loss in iteration 392 : 0.501109039788076
Loss in iteration 393 : 0.5010203925975857
Loss in iteration 394 : 0.5009321093352747
Loss in iteration 395 : 0.5008441877329762
Loss in iteration 396 : 0.5007566255412409
Loss in iteration 397 : 0.5006694205291455
Loss in iteration 398 : 0.5005825704840983
Loss in iteration 399 : 0.5004960732116569
Loss in iteration 400 : 0.5004099265353421
Testing accuracy  of updater 0 on alg 0 with rate 0.09999999999999998 = 0.776625, training accuracy 0.776625, time elapsed: 7761 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 2.141549710106488
Loss in iteration 3 : 19.912833318335288
Loss in iteration 4 : 7.26329565976292
Loss in iteration 5 : 24.584270937815585
Loss in iteration 6 : 31.901257938297874
Loss in iteration 7 : 15.571335992746048
Loss in iteration 8 : 13.779093573662813
Loss in iteration 9 : 23.582129583930527
Loss in iteration 10 : 8.618430102979152
Loss in iteration 11 : 10.918571670793302
Loss in iteration 12 : 17.659501645415308
Loss in iteration 13 : 7.160537080861416
Loss in iteration 14 : 8.200299065885332
Loss in iteration 15 : 14.427973212444378
Loss in iteration 16 : 9.454683713401053
Loss in iteration 17 : 5.318414726782044
Loss in iteration 18 : 11.046389048206672
Loss in iteration 19 : 9.955899554541949
Loss in iteration 20 : 5.466558546742433
Loss in iteration 21 : 8.305500239795158
Loss in iteration 22 : 9.74469786013283
Loss in iteration 23 : 6.645437470716246
Loss in iteration 24 : 6.001239377838218
Loss in iteration 25 : 8.502556853610052
Loss in iteration 26 : 6.876025234315871
Loss in iteration 27 : 5.391798725838787
Loss in iteration 28 : 7.042042128121348
Loss in iteration 29 : 6.893136347497736
Loss in iteration 30 : 5.133855167160736
Loss in iteration 31 : 5.791644534734023
Loss in iteration 32 : 6.294025807286129
Loss in iteration 33 : 4.758703691352934
Loss in iteration 34 : 5.168414150441388
Loss in iteration 35 : 5.525393191109944
Loss in iteration 36 : 4.392151481039302
Loss in iteration 37 : 4.543210763209445
Loss in iteration 38 : 4.784818901469662
Loss in iteration 39 : 3.8050358843141123
Loss in iteration 40 : 4.250634764019573
Loss in iteration 41 : 4.014342802231846
Loss in iteration 42 : 3.3997963330524255
Loss in iteration 43 : 3.8467454039220934
Loss in iteration 44 : 3.160172225413199
Loss in iteration 45 : 3.3101894563293044
Loss in iteration 46 : 3.163996049870915
Loss in iteration 47 : 2.8055997658705594
Loss in iteration 48 : 3.0097529552486173
Loss in iteration 49 : 2.521242703766333
Loss in iteration 50 : 2.7692677749648236
Loss in iteration 51 : 2.3237423376401694
Loss in iteration 52 : 2.5692828285173634
Loss in iteration 53 : 2.158678859170491
Loss in iteration 54 : 2.377332257454441
Loss in iteration 55 : 2.0380957616833637
Loss in iteration 56 : 2.1626779084226886
Loss in iteration 57 : 2.0730223767231433
Loss in iteration 58 : 1.8608108547270668
Loss in iteration 59 : 2.0778006146006875
Loss in iteration 60 : 1.9286789847298016
Loss in iteration 61 : 1.67475989263265
Loss in iteration 62 : 1.811206436634948
Loss in iteration 63 : 1.9646897769623806
Loss in iteration 64 : 1.7589848327163684
Loss in iteration 65 : 1.5074865543603386
Loss in iteration 66 : 1.3359119160781312
Loss in iteration 67 : 1.287207494471519
Loss in iteration 68 : 1.362842920965161
Loss in iteration 69 : 1.6733350209947684
Loss in iteration 70 : 2.593327549247612
Loss in iteration 71 : 2.943139864469047
Loss in iteration 72 : 2.7794999319680365
Loss in iteration 73 : 1.6589877320217208
Loss in iteration 74 : 1.2420285142861898
Loss in iteration 75 : 1.0752238167409511
Loss in iteration 76 : 1.0156554324751519
Loss in iteration 77 : 0.9769628586304493
Loss in iteration 78 : 0.935036781583341
Loss in iteration 79 : 0.8882470091216371
Loss in iteration 80 : 0.8450223117940671
Loss in iteration 81 : 0.8034707569475906
Loss in iteration 82 : 0.7585552061800782
Loss in iteration 83 : 0.9160433050819802
Loss in iteration 84 : 4.712245396643273
Loss in iteration 85 : 9.4295832536735
Loss in iteration 86 : 0.7199790456102292
Loss in iteration 87 : 5.741364710427388
Loss in iteration 88 : 7.184991107177819
Loss in iteration 89 : 1.11095303237753
Loss in iteration 90 : 3.67399586353951
Loss in iteration 91 : 5.5455282839411595
Loss in iteration 92 : 1.3896223786878754
Loss in iteration 93 : 5.680948323657673
Loss in iteration 94 : 2.102350120655962
Loss in iteration 95 : 3.1540044331402526
Loss in iteration 96 : 2.9165252805932655
Loss in iteration 97 : 1.881645181959208
Loss in iteration 98 : 3.1470744796259686
Loss in iteration 99 : 1.763401801349125
Loss in iteration 100 : 3.335792831976215
Loss in iteration 101 : 2.0127698586558536
Loss in iteration 102 : 2.4350252968594033
Loss in iteration 103 : 2.2273781072046064
Loss in iteration 104 : 1.6129409760977795
Loss in iteration 105 : 2.2208311297357057
Loss in iteration 106 : 1.4963520060607065
Loss in iteration 107 : 1.6828859016998954
Loss in iteration 108 : 2.1344421289335074
Loss in iteration 109 : 1.354671476210652
Loss in iteration 110 : 1.2836146789128648
Loss in iteration 111 : 1.949175367706022
Loss in iteration 112 : 2.304987060726714
Loss in iteration 113 : 2.4047599828981605
Loss in iteration 114 : 1.899597598587554
Loss in iteration 115 : 1.8571026314760248
Loss in iteration 116 : 2.1154346479781525
Loss in iteration 117 : 2.8516122918563327
Loss in iteration 118 : 2.9296340028370635
Loss in iteration 119 : 2.2610562129335157
Loss in iteration 120 : 1.647695699641764
Loss in iteration 121 : 1.3394954643322212
Loss in iteration 122 : 1.1689546310611414
Loss in iteration 123 : 1.0780431814834082
Loss in iteration 124 : 1.023861849493166
Loss in iteration 125 : 1.0033532937248164
Loss in iteration 126 : 1.0582194617358127
Loss in iteration 127 : 1.584854750396372
Loss in iteration 128 : 3.775851553236046
Loss in iteration 129 : 5.670835979643233
Loss in iteration 130 : 1.0474903083843643
Loss in iteration 131 : 2.1629034402580327
Loss in iteration 132 : 6.642006360243778
Loss in iteration 133 : 1.1108875495401862
Loss in iteration 134 : 4.330357352548269
Loss in iteration 135 : 4.52695489639609
Loss in iteration 136 : 1.640144909150356
Loss in iteration 137 : 6.06327907330742
Loss in iteration 138 : 1.5440453943299344
Loss in iteration 139 : 4.31171215497728
Loss in iteration 140 : 1.9015471508322461
Loss in iteration 141 : 2.953414301702348
Loss in iteration 142 : 2.933097538442061
Loss in iteration 143 : 2.067856536250197
Loss in iteration 144 : 3.5956931331611877
Loss in iteration 145 : 1.6175766206276976
Loss in iteration 146 : 2.644504479896867
Loss in iteration 147 : 1.874536089799145
Loss in iteration 148 : 1.6309594985461007
Loss in iteration 149 : 2.356495873808738
Loss in iteration 150 : 1.4143129339087246
Loss in iteration 151 : 1.703699369098662
Loss in iteration 152 : 2.513447015379411
Loss in iteration 153 : 1.4155006908684877
Loss in iteration 154 : 1.177183061004788
Loss in iteration 155 : 1.904073949091236
Loss in iteration 156 : 2.648517352406797
Loss in iteration 157 : 2.8647313187003247
Loss in iteration 158 : 1.8361689634448475
Loss in iteration 159 : 1.4058710764724613
Loss in iteration 160 : 1.3015756090956483
Loss in iteration 161 : 1.4674803454666443
Loss in iteration 162 : 2.201769026640768
Loss in iteration 163 : 3.70785975163971
Loss in iteration 164 : 3.0611857345228364
Loss in iteration 165 : 2.0653408468600585
Loss in iteration 166 : 1.540720617975829
Loss in iteration 167 : 1.4176556527931585
Loss in iteration 168 : 1.3783580064048087
Loss in iteration 169 : 1.496550015024416
Loss in iteration 170 : 1.6721093859796012
Loss in iteration 171 : 2.1121319071474827
Loss in iteration 172 : 2.127877039927662
Loss in iteration 173 : 2.2992328411767287
Loss in iteration 174 : 1.763114224432275
Loss in iteration 175 : 1.63929383988347
Loss in iteration 176 : 1.440104947450238
Loss in iteration 177 : 1.5249318531827576
Loss in iteration 178 : 1.6606492890810707
Loss in iteration 179 : 2.258926485621814
Loss in iteration 180 : 2.4418816180902208
Loss in iteration 181 : 2.6127182742781176
Loss in iteration 182 : 1.9190948923739466
Loss in iteration 183 : 1.6707861886876676
Loss in iteration 184 : 1.4592013903199035
Loss in iteration 185 : 1.519147907752767
Loss in iteration 186 : 1.6595187121153527
Loss in iteration 187 : 2.318720852661679
Loss in iteration 188 : 2.6232490255920937
Loss in iteration 189 : 2.879797992045602
Loss in iteration 190 : 1.8522882911967065
Loss in iteration 191 : 1.5249388259284802
Loss in iteration 192 : 1.337338145822986
Loss in iteration 193 : 1.4045264128117285
Loss in iteration 194 : 1.5489239199187714
Loss in iteration 195 : 2.1635155782107285
Loss in iteration 196 : 2.5315610185857014
Loss in iteration 197 : 2.8992116711685805
Loss in iteration 198 : 1.9042263181620533
Loss in iteration 199 : 1.540518131830815
Loss in iteration 200 : 1.3482201377399328
Loss in iteration 201 : 1.4190982565082062
Loss in iteration 202 : 1.6295238629761046
Loss in iteration 203 : 2.3777007782424984
Loss in iteration 204 : 2.742901360223701
Loss in iteration 205 : 2.8440709136320805
Loss in iteration 206 : 1.819790196101439
Loss in iteration 207 : 1.457143872450547
Loss in iteration 208 : 1.2736452207939821
Loss in iteration 209 : 1.2968929075481765
Loss in iteration 210 : 1.4377226376670746
Loss in iteration 211 : 2.0703038365619935
Loss in iteration 212 : 2.762386035825253
Loss in iteration 213 : 3.3208894746426263
Loss in iteration 214 : 1.8663757047305634
Loss in iteration 215 : 1.3635500272052443
Loss in iteration 216 : 1.1589746131173988
Loss in iteration 217 : 1.1308107540670007
Loss in iteration 218 : 1.1940383052485182
Loss in iteration 219 : 1.6214745256760426
Loss in iteration 220 : 2.7206867027226598
Loss in iteration 221 : 4.185709374486894
Loss in iteration 222 : 1.8910117454020487
Loss in iteration 223 : 1.208056782685708
Loss in iteration 224 : 1.031580398417431
Loss in iteration 225 : 0.9899413295192184
Loss in iteration 226 : 1.0036184578564904
Loss in iteration 227 : 1.2528438771016186
Loss in iteration 228 : 2.5172690781911777
Loss in iteration 229 : 5.400797045146242
Loss in iteration 230 : 1.9289765956796945
Loss in iteration 231 : 1.104717910370171
Loss in iteration 232 : 0.9686418291550571
Loss in iteration 233 : 0.9441917748854368
Loss in iteration 234 : 0.9707931926249944
Loss in iteration 235 : 1.2686297831540596
Loss in iteration 236 : 2.7844003229249275
Loss in iteration 237 : 5.64871538700613
Loss in iteration 238 : 1.630296706144663
Loss in iteration 239 : 0.9137977279982159
Loss in iteration 240 : 1.2277355999105874
Loss in iteration 241 : 2.6322715709003033
Loss in iteration 242 : 4.5994735504957225
Loss in iteration 243 : 1.4701796830893903
Loss in iteration 244 : 1.1104406120215617
Loss in iteration 245 : 2.5753278370400725
Loss in iteration 246 : 2.9841933140742345
Loss in iteration 247 : 2.082494961351215
Loss in iteration 248 : 1.1129428699168675
Loss in iteration 249 : 1.4736222549985
Loss in iteration 250 : 2.712472831554077
Loss in iteration 251 : 2.0053513101516907
Loss in iteration 252 : 1.3713780798440183
Loss in iteration 253 : 1.0424114251319683
Loss in iteration 254 : 1.0899917385494577
Loss in iteration 255 : 1.5955029462373185
Loss in iteration 256 : 2.6772275275954094
Loss in iteration 257 : 4.164031204302175
Loss in iteration 258 : 1.7798214623358342
Loss in iteration 259 : 1.138095058705581
Loss in iteration 260 : 0.9856991670779381
Loss in iteration 261 : 0.9466647929446151
Loss in iteration 262 : 0.9715333420148836
Loss in iteration 263 : 1.3300298622088513
Loss in iteration 264 : 3.3528954427882756
Loss in iteration 265 : 5.94642603629712
Loss in iteration 266 : 1.468978325834299
Loss in iteration 267 : 0.9113013377818836
Loss in iteration 268 : 1.8357341413043973
Loss in iteration 269 : 4.079796055148328
Loss in iteration 270 : 3.5307337382227297
Loss in iteration 271 : 1.1469760920617562
Loss in iteration 272 : 1.758790981665362
Loss in iteration 273 : 3.6978691187217128
Loss in iteration 274 : 1.6639337091757975
Loss in iteration 275 : 1.2656344267301243
Loss in iteration 276 : 2.4643334477798007
Loss in iteration 277 : 2.050260118374392
Loss in iteration 278 : 1.3444035736091662
Loss in iteration 279 : 1.1560052745899747
Loss in iteration 280 : 1.6591067231508043
Loss in iteration 281 : 2.6266847980774126
Loss in iteration 282 : 1.8243728364719414
Loss in iteration 283 : 1.4283295061949344
Loss in iteration 284 : 1.1808211376109525
Loss in iteration 285 : 1.1806835322417273
Loss in iteration 286 : 1.4208339172733868
Loss in iteration 287 : 2.740929561075275
Loss in iteration 288 : 4.006833392473497
Loss in iteration 289 : 3.121847972269257
Loss in iteration 290 : 1.721360051276997
Loss in iteration 291 : 1.3362444398915898
Loss in iteration 292 : 1.2383506809443507
Loss in iteration 293 : 1.2693343505594028
Loss in iteration 294 : 1.4697185251859703
Loss in iteration 295 : 2.241172169449551
Loss in iteration 296 : 3.1887678384637193
Loss in iteration 297 : 3.437416067477167
Loss in iteration 298 : 1.584162569374811
Loss in iteration 299 : 1.108297286496424
Loss in iteration 300 : 0.9940138070029082
Loss in iteration 301 : 0.9966105880517111
Loss in iteration 302 : 1.139758513888253
Loss in iteration 303 : 1.6881750182820618
Loss in iteration 304 : 3.532701508909712
Loss in iteration 305 : 3.0772988694295127
Loss in iteration 306 : 2.4289270021567453
Loss in iteration 307 : 1.6513063746501684
Loss in iteration 308 : 1.4841083070185583
Loss in iteration 309 : 1.3887386136741047
Loss in iteration 310 : 1.4810850891345617
Loss in iteration 311 : 1.6841365466404892
Loss in iteration 312 : 2.259662533937754
Loss in iteration 313 : 2.4913866910296503
Loss in iteration 314 : 2.7019342214734907
Loss in iteration 315 : 1.909102941707819
Loss in iteration 316 : 1.720479079361212
Loss in iteration 317 : 1.513664882963541
Loss in iteration 318 : 1.6476928658360244
Loss in iteration 319 : 1.7540391053544337
Loss in iteration 320 : 2.299275581504014
Loss in iteration 321 : 2.238629470391312
Loss in iteration 322 : 2.292519128376307
Loss in iteration 323 : 1.7660718549025223
Loss in iteration 324 : 1.6176079611207348
Loss in iteration 325 : 1.4659665119477858
Loss in iteration 326 : 1.5550315723330654
Loss in iteration 327 : 1.6753178692586244
Loss in iteration 328 : 2.2397015241958336
Loss in iteration 329 : 2.522994789871721
Loss in iteration 330 : 2.815888509688508
Loss in iteration 331 : 1.984949583901189
Loss in iteration 332 : 1.6794684213848075
Loss in iteration 333 : 1.4943347877844961
Loss in iteration 334 : 1.5894158593544196
Loss in iteration 335 : 1.7095608760769427
Loss in iteration 336 : 2.156053485917859
Loss in iteration 337 : 2.2482840014787433
Loss in iteration 338 : 2.426648985312453
Loss in iteration 339 : 1.9005160495670588
Loss in iteration 340 : 1.7375845366643867
Loss in iteration 341 : 1.5483695876995316
Loss in iteration 342 : 1.6398557753319232
Loss in iteration 343 : 1.7244462251451835
Loss in iteration 344 : 2.154218588433273
Loss in iteration 345 : 2.222345981628708
Loss in iteration 346 : 2.41781033833539
Loss in iteration 347 : 1.9248649772268387
Loss in iteration 348 : 1.766046526010543
Loss in iteration 349 : 1.5646109602785023
Loss in iteration 350 : 1.6302736285617507
Loss in iteration 351 : 1.7013559898135902
Loss in iteration 352 : 2.1088754830886955
Loss in iteration 353 : 2.255100064614531
Loss in iteration 354 : 2.5176209536276617
Loss in iteration 355 : 1.9916210869053386
Loss in iteration 356 : 1.8017642208793598
Loss in iteration 357 : 1.5692507760148915
Loss in iteration 358 : 1.6124613070769027
Loss in iteration 359 : 1.6567584123315882
Loss in iteration 360 : 2.025384164446991
Loss in iteration 361 : 2.2011123404098445
Loss in iteration 362 : 2.527746816306297
Loss in iteration 363 : 2.0240205949526526
Loss in iteration 364 : 1.8359121167583945
Loss in iteration 365 : 1.5828807787237313
Loss in iteration 366 : 1.6119135405821865
Loss in iteration 367 : 1.6330879519713453
Loss in iteration 368 : 1.9690195909668577
Loss in iteration 369 : 2.1665323330062924
Loss in iteration 370 : 2.5443817418758883
Loss in iteration 371 : 2.0773800378211345
Loss in iteration 372 : 1.8910902184366938
Loss in iteration 373 : 1.6148831876995033
Loss in iteration 374 : 1.62924340015018
Loss in iteration 375 : 1.6258443466618093
Loss in iteration 376 : 1.9206201807206875
Loss in iteration 377 : 2.1063603588995488
Loss in iteration 378 : 2.5092655783511932
Loss in iteration 379 : 2.115119665754486
Loss in iteration 380 : 1.963252479123043
Loss in iteration 381 : 1.6593352523032872
Loss in iteration 382 : 1.6556777059606056
Loss in iteration 383 : 1.6160642269023564
Loss in iteration 384 : 1.854387408221269
Loss in iteration 385 : 2.0101801884959554
Loss in iteration 386 : 2.4253941763950277
Loss in iteration 387 : 2.1524881675740573
Loss in iteration 388 : 2.0684070063597284
Loss in iteration 389 : 1.7308487073440537
Loss in iteration 390 : 1.699721744934994
Loss in iteration 391 : 1.613622273708375
Loss in iteration 392 : 1.7897086307223657
Loss in iteration 393 : 1.9131595723773294
Loss in iteration 394 : 2.3268598165385517
Loss in iteration 395 : 2.1871770985859373
Loss in iteration 396 : 2.189444244470667
Loss in iteration 397 : 1.811750826946252
Loss in iteration 398 : 1.7423595018043512
Loss in iteration 399 : 1.6050849508639702
Loss in iteration 400 : 1.720464914561617
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.67725, training accuracy 0.67725, time elapsed: 7647 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 1.54457676584438
Loss in iteration 3 : 12.918036793810147
Loss in iteration 4 : 3.342109370361077
Loss in iteration 5 : 18.157764066221482
Loss in iteration 6 : 22.65727556020892
Loss in iteration 7 : 10.667492709355397
Loss in iteration 8 : 10.611830969997335
Loss in iteration 9 : 17.31002489298252
Loss in iteration 10 : 5.811859219479867
Loss in iteration 11 : 8.565784461881691
Loss in iteration 12 : 13.402754083841034
Loss in iteration 13 : 5.46372492757178
Loss in iteration 14 : 5.883978141419462
Loss in iteration 15 : 10.84821126998702
Loss in iteration 16 : 7.278601879013783
Loss in iteration 17 : 3.6950564299466393
Loss in iteration 18 : 8.03319307242308
Loss in iteration 19 : 7.711851463642371
Loss in iteration 20 : 3.9526963773356347
Loss in iteration 21 : 5.781807734369751
Loss in iteration 22 : 7.309355715517355
Loss in iteration 23 : 5.1808613268991195
Loss in iteration 24 : 4.090706337712133
Loss in iteration 25 : 6.129351586965465
Loss in iteration 26 : 5.455137452655604
Loss in iteration 27 : 3.8229548246436953
Loss in iteration 28 : 4.91609417790245
Loss in iteration 29 : 5.29148673205385
Loss in iteration 30 : 3.9517722740773316
Loss in iteration 31 : 3.90390351931788
Loss in iteration 32 : 4.763306770237238
Loss in iteration 33 : 3.733925495071852
Loss in iteration 34 : 3.5279991292418638
Loss in iteration 35 : 4.160023948305463
Loss in iteration 36 : 3.54601960549589
Loss in iteration 37 : 3.0919224296591965
Loss in iteration 38 : 3.6725657006055688
Loss in iteration 39 : 3.0239295067128813
Loss in iteration 40 : 2.897511420119935
Loss in iteration 41 : 3.192148737962443
Loss in iteration 42 : 2.5866940075177847
Loss in iteration 43 : 2.7016583094947904
Loss in iteration 44 : 2.6802829632896534
Loss in iteration 45 : 2.24408616655059
Loss in iteration 46 : 2.5268340989071363
Loss in iteration 47 : 2.113784148966196
Loss in iteration 48 : 2.2046075337430584
Loss in iteration 49 : 2.0749154534903456
Loss in iteration 50 : 1.922593386792837
Loss in iteration 51 : 1.9928630515465346
Loss in iteration 52 : 1.7226897872366675
Loss in iteration 53 : 1.8626389937186063
Loss in iteration 54 : 1.6053481357362342
Loss in iteration 55 : 1.715671739520452
Loss in iteration 56 : 1.529306600966884
Loss in iteration 57 : 1.5501020249590478
Loss in iteration 58 : 1.5382175808294432
Loss in iteration 59 : 1.367802879742723
Loss in iteration 60 : 1.521672249776931
Loss in iteration 61 : 1.3733963940942349
Loss in iteration 62 : 1.2606635201775793
Loss in iteration 63 : 1.3927219532140769
Loss in iteration 64 : 1.3487219791987828
Loss in iteration 65 : 1.137142588521912
Loss in iteration 66 : 1.094737882253264
Loss in iteration 67 : 1.1982237161951539
Loss in iteration 68 : 1.2591299740964286
Loss in iteration 69 : 1.246805085215872
Loss in iteration 70 : 1.1487868905722276
Loss in iteration 71 : 1.0771945144887616
Loss in iteration 72 : 1.0436547443383823
Loss in iteration 73 : 1.1138643344445145
Loss in iteration 74 : 1.318396123383105
Loss in iteration 75 : 1.7971315976851505
Loss in iteration 76 : 1.771531724323354
Loss in iteration 77 : 1.6494742042067951
Loss in iteration 78 : 1.2050862418216268
Loss in iteration 79 : 1.0376317587848838
Loss in iteration 80 : 0.9349724047385216
Loss in iteration 81 : 0.9437372948620711
Loss in iteration 82 : 1.0157449212138299
Loss in iteration 83 : 1.3701361972952442
Loss in iteration 84 : 1.7943334968716231
Loss in iteration 85 : 2.4791325677848155
Loss in iteration 86 : 1.4757491234249533
Loss in iteration 87 : 1.1903460753051986
Loss in iteration 88 : 1.051710425642843
Loss in iteration 89 : 1.2142507271664167
Loss in iteration 90 : 1.4291961274433322
Loss in iteration 91 : 1.9804977451310781
Loss in iteration 92 : 1.6566421498129194
Loss in iteration 93 : 1.4574548896045445
Loss in iteration 94 : 1.1358126958091885
Loss in iteration 95 : 1.081607192218373
Loss in iteration 96 : 1.0639518878761405
Loss in iteration 97 : 1.2936452483843832
Loss in iteration 98 : 1.5909975800130738
Loss in iteration 99 : 2.2098742969309377
Loss in iteration 100 : 1.7028424967585651
Loss in iteration 101 : 1.4526998642936078
Loss in iteration 102 : 1.1758493062536601
Loss in iteration 103 : 1.1783957503448903
Loss in iteration 104 : 1.1655432807077704
Loss in iteration 105 : 1.3205766803691013
Loss in iteration 106 : 1.375738349116954
Loss in iteration 107 : 1.5271939435395674
Loss in iteration 108 : 1.3999712156859654
Loss in iteration 109 : 1.3745392960532918
Loss in iteration 110 : 1.2306660913312928
Loss in iteration 111 : 1.2472456212744332
Loss in iteration 112 : 1.2283530611668807
Loss in iteration 113 : 1.3777617841566903
Loss in iteration 114 : 1.4377825433718567
Loss in iteration 115 : 1.5973109377043981
Loss in iteration 116 : 1.443308554004774
Loss in iteration 117 : 1.3651928975190004
Loss in iteration 118 : 1.1928753111007928
Loss in iteration 119 : 1.1497460779822524
Loss in iteration 120 : 1.1055220097864775
Loss in iteration 121 : 1.1871662508831746
Loss in iteration 122 : 1.3005529572083385
Loss in iteration 123 : 1.5896199275364877
Loss in iteration 124 : 1.6078643441671472
Loss in iteration 125 : 1.623194591444836
Loss in iteration 126 : 1.3222648187052934
Loss in iteration 127 : 1.195019580947138
Loss in iteration 128 : 1.0712075614193906
Loss in iteration 129 : 1.0749025496110112
Loss in iteration 130 : 1.1198450527986348
Loss in iteration 131 : 1.3441847657371768
Loss in iteration 132 : 1.5704122111815029
Loss in iteration 133 : 1.85120855791618
Loss in iteration 134 : 1.5086125824979781
Loss in iteration 135 : 1.3113307572371937
Loss in iteration 136 : 1.1068450753402193
Loss in iteration 137 : 1.070181231830501
Loss in iteration 138 : 1.069373883745238
Loss in iteration 139 : 1.237495243700224
Loss in iteration 140 : 1.4770612286334277
Loss in iteration 141 : 1.8508335120820456
Loss in iteration 142 : 1.6071710096873704
Loss in iteration 143 : 1.4192340034952193
Loss in iteration 144 : 1.1624696853462206
Loss in iteration 145 : 1.097249279976341
Loss in iteration 146 : 1.0492405676822103
Loss in iteration 147 : 1.1486395861183905
Loss in iteration 148 : 1.3314530850051922
Loss in iteration 149 : 1.744338522246339
Loss in iteration 150 : 1.7226706855401872
Loss in iteration 151 : 1.6355895011176136
Loss in iteration 152 : 1.2735268889900944
Loss in iteration 153 : 1.1429985960855136
Loss in iteration 154 : 1.0331454764448147
Loss in iteration 155 : 1.0517223624151097
Loss in iteration 156 : 1.1287799814047172
Loss in iteration 157 : 1.4403039175198478
Loss in iteration 158 : 1.7263971403691964
Loss in iteration 159 : 1.9677740518839508
Loss in iteration 160 : 1.4544489282562838
Loss in iteration 161 : 1.2051197891918515
Loss in iteration 162 : 1.0335603164343883
Loss in iteration 163 : 1.0185285583779786
Loss in iteration 164 : 1.0534245110167608
Loss in iteration 165 : 1.2845032562020398
Loss in iteration 166 : 1.6058192666994413
Loss in iteration 167 : 2.0031809890755983
Loss in iteration 168 : 1.5691865292409826
Loss in iteration 169 : 1.2982958356630856
Loss in iteration 170 : 1.0850172435083678
Loss in iteration 171 : 1.0525424233577394
Loss in iteration 172 : 1.0534368898388624
Loss in iteration 173 : 1.2159256231606617
Loss in iteration 174 : 1.4588970343827812
Loss in iteration 175 : 1.865176390880333
Loss in iteration 176 : 1.6423924474669234
Loss in iteration 177 : 1.4513196455983324
Loss in iteration 178 : 1.1844086386770951
Loss in iteration 179 : 1.118292928772204
Loss in iteration 180 : 1.0666043905853795
Loss in iteration 181 : 1.1484336358446048
Loss in iteration 182 : 1.2814830102542056
Loss in iteration 183 : 1.6237147639576697
Loss in iteration 184 : 1.6799276506442975
Loss in iteration 185 : 1.6838181401459595
Loss in iteration 186 : 1.3309120034933193
Loss in iteration 187 : 1.1877780178792134
Loss in iteration 188 : 1.0625889868371192
Loss in iteration 189 : 1.0709324684412878
Loss in iteration 190 : 1.1229389053211407
Loss in iteration 191 : 1.3782470907259516
Loss in iteration 192 : 1.6355260965749947
Loss in iteration 193 : 1.911874627589611
Loss in iteration 194 : 1.5000724093745956
Loss in iteration 195 : 1.271273720613361
Loss in iteration 196 : 1.082466152827222
Loss in iteration 197 : 1.0556573416327182
Loss in iteration 198 : 1.064267362718332
Loss in iteration 199 : 1.2401862012533285
Loss in iteration 200 : 1.4930509652762867
Loss in iteration 201 : 1.8820383912238459
Loss in iteration 202 : 1.621899341227913
Loss in iteration 203 : 1.4179971445353312
Loss in iteration 204 : 1.1650095751242064
Loss in iteration 205 : 1.1038226045573474
Loss in iteration 206 : 1.061021397263267
Loss in iteration 207 : 1.1569154742522405
Loss in iteration 208 : 1.3196137782484438
Loss in iteration 209 : 1.6891609292838339
Loss in iteration 210 : 1.6912599209737356
Loss in iteration 211 : 1.6353047357817414
Loss in iteration 212 : 1.2925537744474904
Loss in iteration 213 : 1.1625153113728601
Loss in iteration 214 : 1.0504382613180925
Loss in iteration 215 : 1.0675571367757903
Loss in iteration 216 : 1.137781994413216
Loss in iteration 217 : 1.4270921511506909
Loss in iteration 218 : 1.6881726141772027
Loss in iteration 219 : 1.921684577949733
Loss in iteration 220 : 1.4646114934716783
Loss in iteration 221 : 1.2281217047808535
Loss in iteration 222 : 1.051883724065779
Loss in iteration 223 : 1.0290819338747819
Loss in iteration 224 : 1.0511344785673713
Loss in iteration 225 : 1.2535536047847342
Loss in iteration 226 : 1.5544527593135509
Loss in iteration 227 : 1.9706740604068929
Loss in iteration 228 : 1.6139345420783802
Loss in iteration 229 : 1.3609351400392433
Loss in iteration 230 : 1.1228750099020097
Loss in iteration 231 : 1.0732877695205834
Loss in iteration 232 : 1.0506726298810878
Loss in iteration 233 : 1.1725055413182672
Loss in iteration 234 : 1.3740161808850104
Loss in iteration 235 : 1.7703641168078947
Loss in iteration 236 : 1.6872071534036357
Loss in iteration 237 : 1.5635226297270919
Loss in iteration 238 : 1.2471907350354823
Loss in iteration 239 : 1.1415281313681889
Loss in iteration 240 : 1.0516930268163955
Loss in iteration 241 : 1.0895522067074965
Loss in iteration 242 : 1.1840593904477985
Loss in iteration 243 : 1.504420790042463
Loss in iteration 244 : 1.7111280550431693
Loss in iteration 245 : 1.8516344843977957
Loss in iteration 246 : 1.4129188754388489
Loss in iteration 247 : 1.2018299395175176
Loss in iteration 248 : 1.0420744441045908
Loss in iteration 249 : 1.0246222168470323
Loss in iteration 250 : 1.0553543813929778
Loss in iteration 251 : 1.276648478741584
Loss in iteration 252 : 1.5984030658600918
Loss in iteration 253 : 2.0107152165599267
Loss in iteration 254 : 1.5963314420386976
Loss in iteration 255 : 1.3237820925281416
Loss in iteration 256 : 1.0980758656324345
Loss in iteration 257 : 1.0541605078369758
Loss in iteration 258 : 1.0424947804151665
Loss in iteration 259 : 1.1791853188239443
Loss in iteration 260 : 1.4069790666384734
Loss in iteration 261 : 1.8204761286703943
Loss in iteration 262 : 1.6858492850990332
Loss in iteration 263 : 1.5246556282552588
Loss in iteration 264 : 1.221269215541288
Loss in iteration 265 : 1.1267260270465513
Loss in iteration 266 : 1.0491004663425183
Loss in iteration 267 : 1.099559314593363
Loss in iteration 268 : 1.2125892252713557
Loss in iteration 269 : 1.5548694269689454
Loss in iteration 270 : 1.7255609455508503
Loss in iteration 271 : 1.809464162629718
Loss in iteration 272 : 1.3814063475962173
Loss in iteration 273 : 1.1845886768613498
Loss in iteration 274 : 1.0344669147705061
Loss in iteration 275 : 1.0201494496271288
Loss in iteration 276 : 1.0570331669511746
Loss in iteration 277 : 1.2922905670600922
Loss in iteration 278 : 1.6302975850994612
Loss in iteration 279 : 2.0387879351006175
Loss in iteration 280 : 1.5837241630211507
Loss in iteration 281 : 1.2978370553407663
Loss in iteration 282 : 1.0796436787225325
Loss in iteration 283 : 1.038316817207468
Loss in iteration 284 : 1.0331867013001956
Loss in iteration 285 : 1.17950235617152
Loss in iteration 286 : 1.4295454307235091
Loss in iteration 287 : 1.8633788752317468
Loss in iteration 288 : 1.6920589606202425
Loss in iteration 289 : 1.5022096471750621
Loss in iteration 290 : 1.2032651054997383
Loss in iteration 291 : 1.1138306677530758
Loss in iteration 292 : 1.0436545392851377
Loss in iteration 293 : 1.1011592777598558
Loss in iteration 294 : 1.2266698263188158
Loss in iteration 295 : 1.5842618154896324
Loss in iteration 296 : 1.7378459517610436
Loss in iteration 297 : 1.7912429794202944
Loss in iteration 298 : 1.3647195292307814
Loss in iteration 299 : 1.1736147147587899
Loss in iteration 300 : 1.028587104875829
Loss in iteration 301 : 1.015913864544488
Loss in iteration 302 : 1.056604005354643
Loss in iteration 303 : 1.3005349632563215
Loss in iteration 304 : 1.6503076194677024
Loss in iteration 305 : 2.0571739984568205
Loss in iteration 306 : 1.577247832523877
Loss in iteration 307 : 1.2823985665534574
Loss in iteration 308 : 1.067732862967506
Loss in iteration 309 : 1.026828000215444
Loss in iteration 310 : 1.0245446272391494
Loss in iteration 311 : 1.1750615375936528
Loss in iteration 312 : 1.440671580169419
Loss in iteration 313 : 1.8939349396176877
Loss in iteration 314 : 1.7032796762096185
Loss in iteration 315 : 1.4943234887283119
Loss in iteration 316 : 1.1935856292112157
Loss in iteration 317 : 1.1044889314207027
Loss in iteration 318 : 1.0369399532580683
Loss in iteration 319 : 1.0956002801363929
Loss in iteration 320 : 1.2264096017587045
Loss in iteration 321 : 1.5928264429062213
Loss in iteration 322 : 1.7502212676653441
Loss in iteration 323 : 1.7964805265469148
Loss in iteration 324 : 1.3611624512305358
Loss in iteration 325 : 1.1672457896037853
Loss in iteration 326 : 1.0226948532194033
Loss in iteration 327 : 1.0092219962588254
Loss in iteration 328 : 1.0507237040631694
Loss in iteration 329 : 1.2972198058494946
Loss in iteration 330 : 1.659504542040395
Loss in iteration 331 : 2.076942979754121
Loss in iteration 332 : 1.58148688562857
Loss in iteration 333 : 1.2769475804609696
Loss in iteration 334 : 1.0610864581750725
Loss in iteration 335 : 1.0185308724461897
Loss in iteration 336 : 1.015785876423629
Loss in iteration 337 : 1.1646636252419487
Loss in iteration 338 : 1.4381174515592283
Loss in iteration 339 : 1.9094309857236744
Loss in iteration 340 : 1.7208872137283104
Loss in iteration 341 : 1.5027502032905105
Loss in iteration 342 : 1.193068619203048
Loss in iteration 343 : 1.0994484282413468
Loss in iteration 344 : 1.0294202819364902
Loss in iteration 345 : 1.0831173775839342
Loss in iteration 346 : 1.2113092857666334
Loss in iteration 347 : 1.5796207053199143
Loss in iteration 348 : 1.7635730219401748
Loss in iteration 349 : 1.8266701505285043
Loss in iteration 350 : 1.3702443433313924
Loss in iteration 351 : 1.1643657983003852
Loss in iteration 352 : 1.0158571181044704
Loss in iteration 353 : 0.9990693906882642
Loss in iteration 354 : 1.038263288277196
Loss in iteration 355 : 1.2810094639991616
Loss in iteration 356 : 1.6582315881604772
Loss in iteration 357 : 2.1026346285930066
Loss in iteration 358 : 1.598504855072008
Loss in iteration 359 : 1.2812182350365529
Loss in iteration 360 : 1.0591718789724691
Loss in iteration 361 : 1.0130251099578536
Loss in iteration 362 : 1.0067726091033717
Loss in iteration 363 : 1.148094783359018
Loss in iteration 364 : 1.4203929441009777
Loss in iteration 365 : 1.9068717106971302
Loss in iteration 366 : 1.7455814078548189
Loss in iteration 367 : 1.5297858766395347
Loss in iteration 368 : 1.2024900182357805
Loss in iteration 369 : 1.0989239628515015
Loss in iteration 370 : 1.0213594991345367
Loss in iteration 371 : 1.064429433699475
Loss in iteration 372 : 1.1821366052527713
Loss in iteration 373 : 1.5435565591591183
Loss in iteration 374 : 1.7749273802673224
Loss in iteration 375 : 1.8814675284146882
Loss in iteration 376 : 1.3920987241879237
Loss in iteration 377 : 1.1652753675901577
Loss in iteration 378 : 1.009001877143648
Loss in iteration 379 : 0.9873491448332574
Loss in iteration 380 : 1.0219121680984902
Loss in iteration 381 : 1.2554394837002048
Loss in iteration 382 : 1.6456792150742572
Loss in iteration 383 : 2.1276466696221124
Loss in iteration 384 : 1.6259797804740133
Loss in iteration 385 : 1.295096601980114
Loss in iteration 386 : 1.0623917755936654
Loss in iteration 387 : 1.0109659257416834
Loss in iteration 388 : 0.9986035127326978
Loss in iteration 389 : 1.1275562200758922
Loss in iteration 390 : 1.3891962324728484
Loss in iteration 391 : 1.8847187267511554
Loss in iteration 392 : 1.7752562381108996
Loss in iteration 393 : 1.5752464901744754
Loss in iteration 394 : 1.220946799381854
Loss in iteration 395 : 1.101699044704235
Loss in iteration 396 : 1.0126314080603829
Loss in iteration 397 : 1.041351302169417
Loss in iteration 398 : 1.1429723022226392
Loss in iteration 399 : 1.4882697271623522
Loss in iteration 400 : 1.7793775720269536
Testing accuracy  of updater 1 on alg 0 with rate 7.0 = 0.68675, training accuracy 0.68675, time elapsed: 7713 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 1.0091931589813274
Loss in iteration 3 : 5.670171579851012
Loss in iteration 4 : 0.6135639479662652
Loss in iteration 5 : 2.034580389072898
Loss in iteration 6 : 3.2577976202442414
Loss in iteration 7 : 0.7237551165544778
Loss in iteration 8 : 3.706033111468409
Loss in iteration 9 : 0.9310964820237514
Loss in iteration 10 : 3.489842151896243
Loss in iteration 11 : 1.8680274962542072
Loss in iteration 12 : 2.701597451512545
Loss in iteration 13 : 2.3787992686740926
Loss in iteration 14 : 1.760459388160784
Loss in iteration 15 : 2.727692502554093
Loss in iteration 16 : 1.233431614825207
Loss in iteration 17 : 2.4615769589092573
Loss in iteration 18 : 1.7063257304921622
Loss in iteration 19 : 1.7194272798932857
Loss in iteration 20 : 2.2131353427127394
Loss in iteration 21 : 1.2764952909508211
Loss in iteration 22 : 2.01766104300792
Loss in iteration 23 : 1.5128202961392654
Loss in iteration 24 : 1.520513148099069
Loss in iteration 25 : 1.7949286034920293
Loss in iteration 26 : 1.1581440551271946
Loss in iteration 27 : 1.727233439589651
Loss in iteration 28 : 1.1584955363434453
Loss in iteration 29 : 1.4498854137678323
Loss in iteration 30 : 1.2960552598071724
Loss in iteration 31 : 1.165875270878255
Loss in iteration 32 : 1.2446939716023853
Loss in iteration 33 : 1.0624772209242401
Loss in iteration 34 : 1.162943997481092
Loss in iteration 35 : 0.9485834561604811
Loss in iteration 36 : 1.0320163481372193
Loss in iteration 37 : 0.9536548687053135
Loss in iteration 38 : 0.888053949234617
Loss in iteration 39 : 0.9881538151881087
Loss in iteration 40 : 0.7426235144051723
Loss in iteration 41 : 0.951161236755645
Loss in iteration 42 : 0.7732469138299002
Loss in iteration 43 : 0.741191013370735
Loss in iteration 44 : 0.9010650653930443
Loss in iteration 45 : 0.7507472398394555
Loss in iteration 46 : 0.6590994893203952
Loss in iteration 47 : 0.7846616888202886
Loss in iteration 48 : 0.8858339828436325
Loss in iteration 49 : 0.8054824765959465
Loss in iteration 50 : 0.6778840817296753
Loss in iteration 51 : 0.6021569737449987
Loss in iteration 52 : 0.5814455618094225
Loss in iteration 53 : 0.6035061922300949
Loss in iteration 54 : 0.6829422661601147
Loss in iteration 55 : 0.8690306555930581
Loss in iteration 56 : 1.0909896562685235
Loss in iteration 57 : 1.081742735460377
Loss in iteration 58 : 0.8344281034408726
Loss in iteration 59 : 0.640278593870439
Loss in iteration 60 : 0.5502915710040468
Loss in iteration 61 : 0.5314550836809342
Loss in iteration 62 : 0.5619670100857125
Loss in iteration 63 : 0.6445228218494412
Loss in iteration 64 : 0.7912759935734951
Loss in iteration 65 : 1.008534434564169
Loss in iteration 66 : 1.0201166454896917
Loss in iteration 67 : 0.9639308416694701
Loss in iteration 68 : 0.7838524435172785
Loss in iteration 69 : 0.7123300235341062
Loss in iteration 70 : 0.6708589213476311
Loss in iteration 71 : 0.6895766341061653
Loss in iteration 72 : 0.7390022654046543
Loss in iteration 73 : 0.8442123696318911
Loss in iteration 74 : 0.9284227848493479
Loss in iteration 75 : 0.9644636508218234
Loss in iteration 76 : 0.8761021499987071
Loss in iteration 77 : 0.7938256277898215
Loss in iteration 78 : 0.7133346349325789
Loss in iteration 79 : 0.687145134094952
Loss in iteration 80 : 0.697911204949262
Loss in iteration 81 : 0.7808288296268854
Loss in iteration 82 : 0.9237464780129606
Loss in iteration 83 : 1.0653101571569688
Loss in iteration 84 : 0.997000244150527
Loss in iteration 85 : 0.8477766006409938
Loss in iteration 86 : 0.6991883258708212
Loss in iteration 87 : 0.6171849622797122
Loss in iteration 88 : 0.5697391363572554
Loss in iteration 89 : 0.547940151230601
Loss in iteration 90 : 0.5425937250250431
Loss in iteration 91 : 0.5565835008508966
Loss in iteration 92 : 0.6273530706256975
Loss in iteration 93 : 0.8843442510243071
Loss in iteration 94 : 1.390724751181432
Loss in iteration 95 : 1.40559351424222
Loss in iteration 96 : 0.8726816920862999
Loss in iteration 97 : 0.610751145829108
Loss in iteration 98 : 0.5290076757731541
Loss in iteration 99 : 0.5331550799298167
Loss in iteration 100 : 0.6006728570978778
Loss in iteration 101 : 0.7383227618212271
Loss in iteration 102 : 0.9126628973080362
Loss in iteration 103 : 0.9885897688262778
Loss in iteration 104 : 0.9044824112508955
Loss in iteration 105 : 0.7317380633108581
Loss in iteration 106 : 0.6267929778141538
Loss in iteration 107 : 0.5646789072471815
Loss in iteration 108 : 0.5393151387422007
Loss in iteration 109 : 0.5358354233868561
Loss in iteration 110 : 0.5484144839700296
Loss in iteration 111 : 0.604833565828417
Loss in iteration 112 : 0.8264698831531729
Loss in iteration 113 : 1.3378232402413373
Loss in iteration 114 : 1.4602584463132937
Loss in iteration 115 : 0.9166257910895685
Loss in iteration 116 : 0.6154940943691757
Loss in iteration 117 : 0.5266061912652809
Loss in iteration 118 : 0.5573771732855474
Loss in iteration 119 : 0.691552619358485
Loss in iteration 120 : 0.9165752888597819
Loss in iteration 121 : 1.0189902715059833
Loss in iteration 122 : 0.880787501480034
Loss in iteration 123 : 0.718007250629951
Loss in iteration 124 : 0.6056109478479241
Loss in iteration 125 : 0.5514113287837965
Loss in iteration 126 : 0.5225680397088796
Loss in iteration 127 : 0.5116505265649114
Loss in iteration 128 : 0.5096568010883287
Loss in iteration 129 : 0.5067027786104606
Loss in iteration 130 : 0.511137651918284
Loss in iteration 131 : 0.5862799491712217
Loss in iteration 132 : 1.0056788353550083
Loss in iteration 133 : 1.9940839010795444
Loss in iteration 134 : 1.2482203390219737
Loss in iteration 135 : 0.7391273545442504
Loss in iteration 136 : 0.5832953516968855
Loss in iteration 137 : 0.5434852081507817
Loss in iteration 138 : 0.5317449687690617
Loss in iteration 139 : 0.5251147851251483
Loss in iteration 140 : 0.5235403829669956
Loss in iteration 141 : 0.5522486870075234
Loss in iteration 142 : 0.6917572753501247
Loss in iteration 143 : 1.1350167937482742
Loss in iteration 144 : 1.4881574936141493
Loss in iteration 145 : 1.2176488435563182
Loss in iteration 146 : 0.7094056386333953
Loss in iteration 147 : 0.545528420399441
Loss in iteration 148 : 0.5233349864060586
Loss in iteration 149 : 0.5772287989538193
Loss in iteration 150 : 0.693484041801494
Loss in iteration 151 : 0.8258809435360197
Loss in iteration 152 : 0.863971892708737
Loss in iteration 153 : 0.7772307369258824
Loss in iteration 154 : 0.6738489641485608
Loss in iteration 155 : 0.597298310484618
Loss in iteration 156 : 0.5607244912658594
Loss in iteration 157 : 0.5602605300174385
Loss in iteration 158 : 0.6300266952438374
Loss in iteration 159 : 0.907160901470705
Loss in iteration 160 : 1.4868836803585679
Loss in iteration 161 : 1.3993739073282485
Loss in iteration 162 : 0.8637026841666454
Loss in iteration 163 : 0.6083610414871582
Loss in iteration 164 : 0.5336538298734919
Loss in iteration 165 : 0.535210619362005
Loss in iteration 166 : 0.5925155514954711
Loss in iteration 167 : 0.7031544525890281
Loss in iteration 168 : 0.8490443146849272
Loss in iteration 169 : 0.937991482621305
Loss in iteration 170 : 0.8820981505633937
Loss in iteration 171 : 0.7931515403300623
Loss in iteration 172 : 0.6978705946357094
Loss in iteration 173 : 0.6533133006045555
Loss in iteration 174 : 0.6374039579625825
Loss in iteration 175 : 0.6751926415390733
Loss in iteration 176 : 0.7797931270958149
Loss in iteration 177 : 0.9515917849590403
Loss in iteration 178 : 1.049396079758847
Loss in iteration 179 : 0.9466377647140188
Loss in iteration 180 : 0.7674567262642935
Loss in iteration 181 : 0.6398552462963277
Loss in iteration 182 : 0.5655072033366653
Loss in iteration 183 : 0.5284773428659659
Loss in iteration 184 : 0.5139208524748451
Loss in iteration 185 : 0.511894490196195
Loss in iteration 186 : 0.5234781806862482
Loss in iteration 187 : 0.5944067438018072
Loss in iteration 188 : 0.9251664883942112
Loss in iteration 189 : 1.6698308104609896
Loss in iteration 190 : 1.4650017706546945
Loss in iteration 191 : 0.7950148574111913
Loss in iteration 192 : 0.565103330431275
Loss in iteration 193 : 0.5204542228124303
Loss in iteration 194 : 0.5630323971688538
Loss in iteration 195 : 0.6809780203878998
Loss in iteration 196 : 0.8573219729955738
Loss in iteration 197 : 0.9419015733132684
Loss in iteration 198 : 0.8556068453495407
Loss in iteration 199 : 0.7255485109532853
Loss in iteration 200 : 0.618625089798331
Loss in iteration 201 : 0.5632569049137964
Loss in iteration 202 : 0.5354952770032592
Loss in iteration 203 : 0.5318781806291185
Loss in iteration 204 : 0.5555668901822719
Loss in iteration 205 : 0.6625270910638913
Loss in iteration 206 : 1.0475017671955522
Loss in iteration 207 : 1.6291904324738715
Loss in iteration 208 : 1.215774424513339
Loss in iteration 209 : 0.7362624119852849
Loss in iteration 210 : 0.5526201033371713
Loss in iteration 211 : 0.5397927869246575
Loss in iteration 212 : 0.649656967856406
Loss in iteration 213 : 0.857072733146232
Loss in iteration 214 : 0.9667512893922514
Loss in iteration 215 : 0.8636333895358953
Loss in iteration 216 : 0.6996551537750687
Loss in iteration 217 : 0.5947563802326465
Loss in iteration 218 : 0.5495983577703819
Loss in iteration 219 : 0.5250462224707108
Loss in iteration 220 : 0.51087467210445
Loss in iteration 221 : 0.5056923498373679
Loss in iteration 222 : 0.5030906417535183
Loss in iteration 223 : 0.49782181489922595
Loss in iteration 224 : 0.5058294898023583
Loss in iteration 225 : 0.6197127891365289
Loss in iteration 226 : 1.2912754183549013
Loss in iteration 227 : 2.2818676591599365
Loss in iteration 228 : 1.1685614551257817
Loss in iteration 229 : 0.5903794319264062
Loss in iteration 230 : 0.5438352130979018
Loss in iteration 231 : 0.7780055141253582
Loss in iteration 232 : 1.1201625432647122
Loss in iteration 233 : 1.05054809245749
Loss in iteration 234 : 0.6677296173806266
Loss in iteration 235 : 0.5478640409164605
Loss in iteration 236 : 0.7072374939978456
Loss in iteration 237 : 0.9302108699905288
Loss in iteration 238 : 0.7879298002899994
Loss in iteration 239 : 0.6113989995573524
Loss in iteration 240 : 0.5443621204221643
Loss in iteration 241 : 0.6095677418361644
Loss in iteration 242 : 0.7861643261728852
Loss in iteration 243 : 0.9238313308860585
Loss in iteration 244 : 0.9431286931012692
Loss in iteration 245 : 0.7906801053128415
Loss in iteration 246 : 0.6732756826989551
Loss in iteration 247 : 0.6053809777720541
Loss in iteration 248 : 0.5703338164942094
Loss in iteration 249 : 0.5570520651337335
Loss in iteration 250 : 0.5751408766938959
Loss in iteration 251 : 0.686982387310901
Loss in iteration 252 : 1.0518968994353188
Loss in iteration 253 : 1.6471423021478173
Loss in iteration 254 : 1.1779988967493236
Loss in iteration 255 : 0.7455570920998675
Loss in iteration 256 : 0.592401195070817
Loss in iteration 257 : 0.5417170447825067
Loss in iteration 258 : 0.5237885939560906
Loss in iteration 259 : 0.5205520413656
Loss in iteration 260 : 0.526249757472671
Loss in iteration 261 : 0.5465544705582128
Loss in iteration 262 : 0.6215447845714327
Loss in iteration 263 : 0.8743636603489782
Loss in iteration 264 : 1.3196568616822069
Loss in iteration 265 : 1.396486754618818
Loss in iteration 266 : 0.8569363966804142
Loss in iteration 267 : 0.6010356262582546
Loss in iteration 268 : 0.5241839779799478
Loss in iteration 269 : 0.5340281083707363
Loss in iteration 270 : 0.6089573296365661
Loss in iteration 271 : 0.7609236757708545
Loss in iteration 272 : 0.9501663483655884
Loss in iteration 273 : 1.016817077037119
Loss in iteration 274 : 0.9028758968613735
Loss in iteration 275 : 0.7221930058650219
Loss in iteration 276 : 0.6177457077103823
Loss in iteration 277 : 0.5597980024390988
Loss in iteration 278 : 0.5395375626214122
Loss in iteration 279 : 0.5446027248974883
Loss in iteration 280 : 0.582563219649057
Loss in iteration 281 : 0.7312768330214644
Loss in iteration 282 : 1.1636605529415571
Loss in iteration 283 : 1.5040877003188429
Loss in iteration 284 : 1.0794763881634046
Loss in iteration 285 : 0.6788529767246255
Loss in iteration 286 : 0.5391167557571124
Loss in iteration 287 : 0.5385591613890142
Loss in iteration 288 : 0.644298097183465
Loss in iteration 289 : 0.8388245888444757
Loss in iteration 290 : 1.0155246956468718
Loss in iteration 291 : 0.9387517026918694
Loss in iteration 292 : 0.7474690379586529
Loss in iteration 293 : 0.6302874989002227
Loss in iteration 294 : 0.5652876133540603
Loss in iteration 295 : 0.5318981345738232
Loss in iteration 296 : 0.5156666031285313
Loss in iteration 297 : 0.5123907817839866
Loss in iteration 298 : 0.5147792433059556
Loss in iteration 299 : 0.5350198752294661
Loss in iteration 300 : 0.6794422350557953
Loss in iteration 301 : 1.3055883134975712
Loss in iteration 302 : 1.9515033854883632
Loss in iteration 303 : 0.948732839339633
Loss in iteration 304 : 0.5597616450463316
Loss in iteration 305 : 0.5499681349982055
Loss in iteration 306 : 0.7828195034444768
Loss in iteration 307 : 1.1707441942586492
Loss in iteration 308 : 1.0145267202882617
Loss in iteration 309 : 0.6874261217804276
Loss in iteration 310 : 0.5384874006275298
Loss in iteration 311 : 0.5791072375532678
Loss in iteration 312 : 0.749232614786617
Loss in iteration 313 : 0.9432858746161469
Loss in iteration 314 : 0.8723267075678447
Loss in iteration 315 : 0.7509413310481664
Loss in iteration 316 : 0.6327401764611903
Loss in iteration 317 : 0.5829448138413537
Loss in iteration 318 : 0.5643372740487832
Loss in iteration 319 : 0.5720892549958259
Loss in iteration 320 : 0.6217385467273809
Loss in iteration 321 : 0.7724166842874459
Loss in iteration 322 : 1.0712749108495405
Loss in iteration 323 : 1.2558827774525594
Loss in iteration 324 : 1.0536707587249694
Loss in iteration 325 : 0.7489985706517204
Loss in iteration 326 : 0.5961667576779289
Loss in iteration 327 : 0.5349307242841362
Loss in iteration 328 : 0.5199605561674839
Loss in iteration 329 : 0.5344548967204027
Loss in iteration 330 : 0.6028898414634286
Loss in iteration 331 : 0.8391450151600509
Loss in iteration 332 : 1.3304487471073847
Loss in iteration 333 : 1.3150778975937842
Loss in iteration 334 : 0.942797162521494
Loss in iteration 335 : 0.6478706788055679
Loss in iteration 336 : 0.5422576098030439
Loss in iteration 337 : 0.5240939391916191
Loss in iteration 338 : 0.55894937926323
Loss in iteration 339 : 0.6374607361258547
Loss in iteration 340 : 0.7590597465265605
Loss in iteration 341 : 0.8870486588107942
Loss in iteration 342 : 0.9274119294877815
Loss in iteration 343 : 0.8683454359642088
Loss in iteration 344 : 0.7474935296311825
Loss in iteration 345 : 0.6699132259502883
Loss in iteration 346 : 0.6380205796039786
Loss in iteration 347 : 0.6698628809840639
Loss in iteration 348 : 0.7936864430929378
Loss in iteration 349 : 1.0057573146492575
Loss in iteration 350 : 1.1385572688467287
Loss in iteration 351 : 0.9801026675795067
Loss in iteration 352 : 0.7464013786232188
Loss in iteration 353 : 0.6082242762535428
Loss in iteration 354 : 0.5407717424265599
Loss in iteration 355 : 0.5221352916004448
Loss in iteration 356 : 0.5419379718030264
Loss in iteration 357 : 0.6113681032565912
Loss in iteration 358 : 0.7888751847094616
Loss in iteration 359 : 1.1164257647961635
Loss in iteration 360 : 1.294343495568297
Loss in iteration 361 : 0.9626660405823233
Loss in iteration 362 : 0.7124607126045303
Loss in iteration 363 : 0.5821822958738231
Loss in iteration 364 : 0.530583388148175
Loss in iteration 365 : 0.5189195991275337
Loss in iteration 366 : 0.5282950198199513
Loss in iteration 367 : 0.5568738074441218
Loss in iteration 368 : 0.6543759283519324
Loss in iteration 369 : 0.9538687156317499
Loss in iteration 370 : 1.400798104136073
Loss in iteration 371 : 1.2859964243585864
Loss in iteration 372 : 0.8044121010004592
Loss in iteration 373 : 0.5859664395955213
Loss in iteration 374 : 0.5200107757957262
Loss in iteration 375 : 0.5347362306652924
Loss in iteration 376 : 0.6138354164915435
Loss in iteration 377 : 0.7814176983836051
Loss in iteration 378 : 1.0059351222550792
Loss in iteration 379 : 1.0703241567495894
Loss in iteration 380 : 0.9132218315293223
Loss in iteration 381 : 0.6949846036032579
Loss in iteration 382 : 0.5774412984923243
Loss in iteration 383 : 0.52403737864488
Loss in iteration 384 : 0.5159362419808543
Loss in iteration 385 : 0.5395369753706553
Loss in iteration 386 : 0.6114180254869582
Loss in iteration 387 : 0.8416471271056203
Loss in iteration 388 : 1.3188210303015704
Loss in iteration 389 : 1.392728020371414
Loss in iteration 390 : 0.888381296543547
Loss in iteration 391 : 0.6174065962105654
Loss in iteration 392 : 0.5265653189367691
Loss in iteration 393 : 0.5470024991414691
Loss in iteration 394 : 0.6706040864500514
Loss in iteration 395 : 0.8984576373693582
Loss in iteration 396 : 1.0412244580998795
Loss in iteration 397 : 0.9391941171026699
Loss in iteration 398 : 0.7580744976353376
Loss in iteration 399 : 0.6267965330863221
Loss in iteration 400 : 0.5617440881693652
Testing accuracy  of updater 1 on alg 0 with rate 4.0 = 0.782125, training accuracy 0.782125, time elapsed: 7295 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6826496441898315
Loss in iteration 3 : 0.6339254231375371
Loss in iteration 4 : 0.5867704951414214
Loss in iteration 5 : 0.5629247813403965
Loss in iteration 6 : 0.5309264422630869
Loss in iteration 7 : 0.5148220298238643
Loss in iteration 8 : 0.5014300866706939
Loss in iteration 9 : 0.49361340845674695
Loss in iteration 10 : 0.4895398353767431
Loss in iteration 11 : 0.4873414015503836
Loss in iteration 12 : 0.48534746038929416
Loss in iteration 13 : 0.48596599650215777
Loss in iteration 14 : 0.4840173168519073
Loss in iteration 15 : 0.4857146609389189
Loss in iteration 16 : 0.48378769976390923
Loss in iteration 17 : 0.48470766070971777
Loss in iteration 18 : 0.483368334567589
Loss in iteration 19 : 0.482548489487799
Loss in iteration 20 : 0.4819658599700555
Loss in iteration 21 : 0.47999165564753715
Loss in iteration 22 : 0.479631916670203
Loss in iteration 23 : 0.4776483370884671
Loss in iteration 24 : 0.47671713059767423
Loss in iteration 25 : 0.47526808608302457
Loss in iteration 26 : 0.4735834702065309
Loss in iteration 27 : 0.47253592833282193
Loss in iteration 28 : 0.4706389269933046
Loss in iteration 29 : 0.46969701289099397
Loss in iteration 30 : 0.46819256438895185
Loss in iteration 31 : 0.4671926091231691
Loss in iteration 32 : 0.46627051259625035
Loss in iteration 33 : 0.4652834041919633
Loss in iteration 34 : 0.46482699453703136
Loss in iteration 35 : 0.4640628105703771
Loss in iteration 36 : 0.4638858293576205
Loss in iteration 37 : 0.46348106477789874
Loss in iteration 38 : 0.46340871264224054
Loss in iteration 39 : 0.46329872825637836
Loss in iteration 40 : 0.4632104743355541
Loss in iteration 41 : 0.4632567117253562
Loss in iteration 42 : 0.463144547139271
Loss in iteration 43 : 0.46324091804174156
Loss in iteration 44 : 0.46313981722026326
Loss in iteration 45 : 0.4632067765776688
Loss in iteration 46 : 0.46312256262487006
Loss in iteration 47 : 0.46310951998571054
Loss in iteration 48 : 0.463032772281511
Loss in iteration 49 : 0.4629412567149818
Loss in iteration 50 : 0.46286719860269726
Loss in iteration 51 : 0.4627243964351826
Loss in iteration 52 : 0.46263978907545183
Loss in iteration 53 : 0.462469779020908
Loss in iteration 54 : 0.4623628086743383
Loss in iteration 55 : 0.46219243478786026
Loss in iteration 56 : 0.4620706673277446
Loss in iteration 57 : 0.46192604452490743
Loss in iteration 58 : 0.46180401697337514
Loss in iteration 59 : 0.4616935354725827
Loss in iteration 60 : 0.4615795874842697
Loss in iteration 61 : 0.4614967689330251
Loss in iteration 62 : 0.46139748605998226
Loss in iteration 63 : 0.46133387465538556
Loss in iteration 64 : 0.46125477348586275
Loss in iteration 65 : 0.4612021818535512
Loss in iteration 66 : 0.46114207350457104
Loss in iteration 67 : 0.46109296267961086
Loss in iteration 68 : 0.46104641306191013
Loss in iteration 69 : 0.46099839924326036
Loss in iteration 70 : 0.4609606847620851
Loss in iteration 71 : 0.46091549269076415
Loss in iteration 72 : 0.46088221312122674
Loss in iteration 73 : 0.46084083016433774
Loss in iteration 74 : 0.4608076279118877
Loss in iteration 75 : 0.4607695808087607
Loss in iteration 76 : 0.4607345057109088
Loss in iteration 77 : 0.46069934420399694
Loss in iteration 78 : 0.4606630076610583
Loss in iteration 79 : 0.4606299671091063
Loss in iteration 80 : 0.46059361545984384
Loss in iteration 81 : 0.46056162310107573
Loss in iteration 82 : 0.4605265930050762
Loss in iteration 83 : 0.46049539722689165
Loss in iteration 84 : 0.4604630641970623
Loss in iteration 85 : 0.4604331470013138
Loss in iteration 86 : 0.46040406424013136
Loss in iteration 87 : 0.460375740001066
Loss in iteration 88 : 0.46034944497340835
Loss in iteration 89 : 0.46032284027084414
Loss in iteration 90 : 0.4602986516056323
Loss in iteration 91 : 0.46027395580339175
Loss in iteration 92 : 0.4602513475455744
Loss in iteration 93 : 0.460228547407536
Loss in iteration 94 : 0.460207051340294
Loss in iteration 95 : 0.46018584877011337
Loss in iteration 96 : 0.46016518049895916
Loss in iteration 97 : 0.46014524774362653
Loss in iteration 98 : 0.46012535856491954
Loss in iteration 99 : 0.4601063894534248
Loss in iteration 100 : 0.460087246968224
Loss in iteration 101 : 0.46006891118800636
Loss in iteration 102 : 0.4600504297269225
Loss in iteration 103 : 0.4600325139243531
Loss in iteration 104 : 0.4600146612598623
Loss in iteration 105 : 0.4599971450034431
Loss in iteration 106 : 0.45997992293334566
Loss in iteration 107 : 0.4599628661744211
Loss in iteration 108 : 0.45994624413583174
Loss in iteration 109 : 0.4599297156539
Loss in iteration 110 : 0.45991366568084263
Loss in iteration 111 : 0.4598977411090762
Loss in iteration 112 : 0.4598822594784118
Loss in iteration 113 : 0.45986697955883826
Loss in iteration 114 : 0.45985205252424216
Loss in iteration 115 : 0.45983739709280996
Loss in iteration 116 : 0.4598230038549574
Loss in iteration 117 : 0.45980893050836086
Loss in iteration 118 : 0.459795066532481
Loss in iteration 119 : 0.4597815360845379
Loss in iteration 120 : 0.4597681967184152
Loss in iteration 121 : 0.45975516566523345
Loss in iteration 122 : 0.4597423294163551
Loss in iteration 123 : 0.4597297573301422
Loss in iteration 124 : 0.45971739659249355
Loss in iteration 125 : 0.4597052581533237
Loss in iteration 126 : 0.4596933453434739
Loss in iteration 127 : 0.45968162299809
Loss in iteration 128 : 0.45967012722093314
Loss in iteration 129 : 0.45965880429873984
Loss in iteration 130 : 0.4596476983046837
Loss in iteration 131 : 0.4596367630065025
Loss in iteration 132 : 0.4596260302952764
Loss in iteration 133 : 0.4596154740197111
Loss in iteration 134 : 0.4596051040551422
Loss in iteration 135 : 0.45959491538139174
Loss in iteration 136 : 0.45958489868767877
Loss in iteration 137 : 0.45957506405102866
Loss in iteration 138 : 0.45956539237170285
Loss in iteration 139 : 0.45955589887160697
Loss in iteration 140 : 0.4595465637686337
Loss in iteration 141 : 0.45953739856109255
Loss in iteration 142 : 0.45952838918876865
Loss in iteration 143 : 0.4595195395413979
Loss in iteration 144 : 0.45951084381986923
Loss in iteration 145 : 0.45950229863170017
Loss in iteration 146 : 0.4594939046917487
Loss in iteration 147 : 0.45948565385833684
Loss in iteration 148 : 0.4594775496148292
Loss in iteration 149 : 0.4594695826326366
Loss in iteration 150 : 0.4594617559580024
Loss in iteration 151 : 0.4594540621120516
Loss in iteration 152 : 0.4594465018539716
Loss in iteration 153 : 0.4594390707841742
Loss in iteration 154 : 0.4594317668911938
Loss in iteration 155 : 0.45942458847416545
Loss in iteration 156 : 0.45941753156845516
Loss in iteration 157 : 0.45941059607613965
Loss in iteration 158 : 0.4594037774666975
Loss in iteration 159 : 0.45939707594713936
Loss in iteration 160 : 0.45939048750302697
Loss in iteration 161 : 0.4593840116448474
Loss in iteration 162 : 0.4593776454078457
Loss in iteration 163 : 0.45937138727704085
Loss in iteration 164 : 0.4593652353831448
Loss in iteration 165 : 0.45935918742784027
Loss in iteration 166 : 0.4593532422862555
Loss in iteration 167 : 0.45934739732215923
Loss in iteration 168 : 0.45934165163035495
Loss in iteration 169 : 0.4593360026686852
Loss in iteration 170 : 0.4593304493670012
Loss in iteration 171 : 0.45932498957594825
Loss in iteration 172 : 0.45931962190424214
Loss in iteration 173 : 0.45931434465709275
Loss in iteration 174 : 0.4593091561452583
Loss in iteration 175 : 0.4593040550063476
Loss in iteration 176 : 0.45929903940031774
Loss in iteration 177 : 0.459294108121367
Loss in iteration 178 : 0.4592892593672367
Loss in iteration 179 : 0.45928449194452764
Loss in iteration 180 : 0.4592798042174524
Loss in iteration 181 : 0.4592751949160037
Loss in iteration 182 : 0.45927066260387084
Loss in iteration 183 : 0.4592662059246129
Loss in iteration 184 : 0.4592618236132372
Loss in iteration 185 : 0.45925751427814365
Loss in iteration 186 : 0.4592532767622797
Loss in iteration 187 : 0.4592491097018753
Loss in iteration 188 : 0.4592450119798326
Loss in iteration 189 : 0.4592409823060669
Loss in iteration 190 : 0.45923701956048035
Loss in iteration 191 : 0.4592331225480771
Loss in iteration 192 : 0.4592292901388856
Loss in iteration 193 : 0.459225521228952
Loss in iteration 194 : 0.4592218146919423
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.7895, training accuracy 0.7895, time elapsed: 3549 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6764423585722834
Loss in iteration 3 : 0.6408918116599488
Loss in iteration 4 : 0.6157861866124221
Loss in iteration 5 : 0.5797556513211066
Loss in iteration 6 : 0.5586815183147682
Loss in iteration 7 : 0.5340349733563151
Loss in iteration 8 : 0.5206045494602441
Loss in iteration 9 : 0.5072011032502883
Loss in iteration 10 : 0.49860062366737506
Loss in iteration 11 : 0.49419225628043806
Loss in iteration 12 : 0.48851307247089854
Loss in iteration 13 : 0.4878108751271054
Loss in iteration 14 : 0.48510674270617926
Loss in iteration 15 : 0.48372086646829976
Loss in iteration 16 : 0.483794781401748
Loss in iteration 17 : 0.48206563716508877
Loss in iteration 18 : 0.48200022048592317
Loss in iteration 19 : 0.4817065753164987
Loss in iteration 20 : 0.48038839871665445
Loss in iteration 21 : 0.48024883263932816
Loss in iteration 22 : 0.47939816720526574
Loss in iteration 23 : 0.4781310890161289
Loss in iteration 24 : 0.47768211812970324
Loss in iteration 25 : 0.47657276473284316
Loss in iteration 26 : 0.4754493849758516
Loss in iteration 27 : 0.4748600828051857
Loss in iteration 28 : 0.4737096486316973
Loss in iteration 29 : 0.4727055164153814
Loss in iteration 30 : 0.47197746730618445
Loss in iteration 31 : 0.470817290489502
Loss in iteration 32 : 0.4699040661581843
Loss in iteration 33 : 0.46912679435853794
Loss in iteration 34 : 0.46810116511759886
Loss in iteration 35 : 0.46738038880482213
Loss in iteration 36 : 0.46670249638418215
Loss in iteration 37 : 0.4659207500902827
Loss in iteration 38 : 0.46541938995006943
Loss in iteration 39 : 0.4648934428494161
Loss in iteration 40 : 0.46437535077928294
Loss in iteration 41 : 0.4640797867975044
Loss in iteration 42 : 0.4637336031265342
Loss in iteration 43 : 0.46346416292896053
Loss in iteration 44 : 0.463329326633867
Loss in iteration 45 : 0.463131945508198
Loss in iteration 46 : 0.4630234765173418
Loss in iteration 47 : 0.4629580449893284
Loss in iteration 48 : 0.4628416020869988
Loss in iteration 49 : 0.462805052706787
Loss in iteration 50 : 0.46275836627749833
Loss in iteration 51 : 0.4626932269035106
Loss in iteration 52 : 0.46268471042021847
Loss in iteration 53 : 0.4626413624365826
Loss in iteration 54 : 0.46260307039301607
Loss in iteration 55 : 0.4625885512525015
Loss in iteration 56 : 0.4625372824058221
Loss in iteration 57 : 0.4625038445145292
Loss in iteration 58 : 0.46247043993418513
Loss in iteration 59 : 0.4624142170393453
Loss in iteration 60 : 0.4623772395436824
Loss in iteration 61 : 0.46232690350247097
Loss in iteration 62 : 0.46226809509039335
Loss in iteration 63 : 0.46222129964873704
Loss in iteration 64 : 0.4621587675826061
Loss in iteration 65 : 0.46209942073435323
Loss in iteration 66 : 0.46204574394568493
Loss in iteration 67 : 0.4619824936684749
Loss in iteration 68 : 0.4619286302180242
Loss in iteration 69 : 0.461875631657507
Loss in iteration 70 : 0.4618194751770007
Loss in iteration 71 : 0.4617723653985511
Loss in iteration 72 : 0.4617231926513263
Loss in iteration 73 : 0.4616754792535824
Loss in iteration 74 : 0.46163414104585787
Loss in iteration 75 : 0.46159053381140813
Loss in iteration 76 : 0.4615508106001617
Loss in iteration 77 : 0.4615142832334281
Loss in iteration 78 : 0.46147619931249184
Loss in iteration 79 : 0.4614420483328787
Loss in iteration 80 : 0.4614085323753868
Loss in iteration 81 : 0.46137459469449527
Loss in iteration 82 : 0.4613438046526736
Loss in iteration 83 : 0.4613125750239805
Loss in iteration 84 : 0.4612820524536529
Loss in iteration 85 : 0.46125356221561153
Loss in iteration 86 : 0.4612243303111515
Loss in iteration 87 : 0.46119623209668675
Loss in iteration 88 : 0.46116899354131724
Loss in iteration 89 : 0.4611411608348235
Loss in iteration 90 : 0.4611144853273143
Loss in iteration 91 : 0.4610879674857134
Loss in iteration 92 : 0.46106129696888654
Loss in iteration 93 : 0.461035599131583
Loss in iteration 94 : 0.46100975375258907
Loss in iteration 95 : 0.46098412905509095
Loss in iteration 96 : 0.4609591750799976
Loss in iteration 97 : 0.46093406798831327
Loss in iteration 98 : 0.46090945156988744
Loss in iteration 99 : 0.4608852944220679
Loss in iteration 100 : 0.4608611792123986
Loss in iteration 101 : 0.4608376805244459
Loss in iteration 102 : 0.46081449353945614
Loss in iteration 103 : 0.46079152411623536
Loss in iteration 104 : 0.4607691244606332
Loss in iteration 105 : 0.4607469407310883
Loss in iteration 106 : 0.46072509069941486
Loss in iteration 107 : 0.4607037095364518
Loss in iteration 108 : 0.46068253074492477
Loss in iteration 109 : 0.46066174610484767
Loss in iteration 110 : 0.4606413258333988
Loss in iteration 111 : 0.4606211251172561
Loss in iteration 112 : 0.4606013111088974
Loss in iteration 113 : 0.46058177471551753
Loss in iteration 114 : 0.4605624832433264
Loss in iteration 115 : 0.46054354049986573
Loss in iteration 116 : 0.4605248287640295
Loss in iteration 117 : 0.46050638195758814
Loss in iteration 118 : 0.4604882322334073
Loss in iteration 119 : 0.4604702889735555
Loss in iteration 120 : 0.4604526075106745
Loss in iteration 121 : 0.46043517070926876
Loss in iteration 122 : 0.4604179317751357
Loss in iteration 123 : 0.46040094136915843
Loss in iteration 124 : 0.46038416212721683
Loss in iteration 125 : 0.46036758457341376
Loss in iteration 126 : 0.46035123893188007
Loss in iteration 127 : 0.46033508646828286
Loss in iteration 128 : 0.46031913846052447
Loss in iteration 129 : 0.46030340340400133
Loss in iteration 130 : 0.4602878535155809
Loss in iteration 131 : 0.4602725072969597
Loss in iteration 132 : 0.4602573585885807
Loss in iteration 133 : 0.46024239329150735
Loss in iteration 134 : 0.4602276265158196
Loss in iteration 135 : 0.4602130447231319
Loss in iteration 136 : 0.46019864419033396
Loss in iteration 137 : 0.46018443252698865
Loss in iteration 138 : 0.46017039602617477
Loss in iteration 139 : 0.46015653732377904
Loss in iteration 140 : 0.4601428572704883
Loss in iteration 141 : 0.4601293457960659
Loss in iteration 142 : 0.46011600724224916
Loss in iteration 143 : 0.46010283762314014
Loss in iteration 144 : 0.4600898309982654
Loss in iteration 145 : 0.46007699040029243
Loss in iteration 146 : 0.46006430984556246
Loss in iteration 147 : 0.4600517868857922
Loss in iteration 148 : 0.460039422357494
Loss in iteration 149 : 0.4600272104880783
Loss in iteration 150 : 0.4600151507531964
Loss in iteration 151 : 0.46000324180634333
Loss in iteration 152 : 0.4599914790579678
Loss in iteration 153 : 0.4599798624981602
Loss in iteration 154 : 0.45996838945944546
Loss in iteration 155 : 0.4599570568973916
Loss in iteration 156 : 0.4599458645726687
Loss in iteration 157 : 0.4599348094246439
Loss in iteration 158 : 0.4599238896173344
Loss in iteration 159 : 0.4599131042701735
Loss in iteration 160 : 0.45990245047202644
Loss in iteration 161 : 0.4598919270220334
Loss in iteration 162 : 0.4598815324272563
Loss in iteration 163 : 0.45987126425253677
Loss in iteration 164 : 0.4598611215169286
Loss in iteration 165 : 0.4598511023800801
Loss in iteration 166 : 0.4598412049237379
Loss in iteration 167 : 0.45983142808571403
Loss in iteration 168 : 0.45982176991311385
Loss in iteration 169 : 0.45981222887166245
Loss in iteration 170 : 0.45980280371807514
Loss in iteration 171 : 0.45979349259728064
Loss in iteration 172 : 0.459784294216346
Loss in iteration 173 : 0.4597752071854801
Loss in iteration 174 : 0.45976622984076093
Loss in iteration 175 : 0.45975736098057907
Loss in iteration 176 : 0.4597485991327432
Loss in iteration 177 : 0.4597399428214311
Loss in iteration 178 : 0.4597313908453674
Loss in iteration 179 : 0.4597229417353235
Loss in iteration 180 : 0.4597145941730728
Loss in iteration 181 : 0.4597063469369526
Loss in iteration 182 : 0.45969819862601924
Loss in iteration 183 : 0.4596901480279055
Loss in iteration 184 : 0.459682193905454
Loss in iteration 185 : 0.45967433495026316
Loss in iteration 186 : 0.45966657001093375
Loss in iteration 187 : 0.45965889786178343
Loss in iteration 188 : 0.4596513172914954
Loss in iteration 189 : 0.4596438271850179
Loss in iteration 190 : 0.45963642635368634
Loss in iteration 191 : 0.4596291136680179
Loss in iteration 192 : 0.4596218880368045
Loss in iteration 193 : 0.4596147483228294
Loss in iteration 194 : 0.45960769345832886
Loss in iteration 195 : 0.4596007223763504
Loss in iteration 196 : 0.4595938339981486
Loss in iteration 197 : 0.4595870273034286
Loss in iteration 198 : 0.45958030125692134
Loss in iteration 199 : 0.4595736548371135
Loss in iteration 200 : 0.4595670870605763
Loss in iteration 201 : 0.4595605969290082
Loss in iteration 202 : 0.45955418347144644
Loss in iteration 203 : 0.45954784573687396
Loss in iteration 204 : 0.45954158276826185
Loss in iteration 205 : 0.459535393638567
Loss in iteration 206 : 0.4595292774285441
Loss in iteration 207 : 0.45952323322309485
Loss in iteration 208 : 0.45951726013288025
Loss in iteration 209 : 0.45951135727110964
Loss in iteration 210 : 0.4595055237627879
Loss in iteration 211 : 0.4594997587520554
Loss in iteration 212 : 0.4594940613856105
Loss in iteration 213 : 0.45948843082598423
Loss in iteration 214 : 0.4594828662489083
Loss in iteration 215 : 0.45947736683516566
Loss in iteration 216 : 0.4594719317817587
Loss in iteration 217 : 0.45946656029477284
Loss in iteration 218 : 0.4594612515882933
Loss in iteration 219 : 0.4594560048910521
Loss in iteration 220 : 0.4594508194390638
Loss in iteration 221 : 0.4594456944784779
Loss in iteration 222 : 0.459440629267713
Testing accuracy  of updater 1 on alg 0 with rate 0.7 = 0.789125, training accuracy 0.789125, time elapsed: 4490 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6779278019447145
Loss in iteration 3 : 0.6634198106997483
Loss in iteration 4 : 0.6368166143880439
Loss in iteration 5 : 0.6155023551698123
Loss in iteration 6 : 0.5952353833474606
Loss in iteration 7 : 0.5722964750836268
Loss in iteration 8 : 0.556759928298842
Loss in iteration 9 : 0.5416863982168202
Loss in iteration 10 : 0.5276334114792544
Loss in iteration 11 : 0.5188898784711605
Loss in iteration 12 : 0.5106589911015653
Loss in iteration 13 : 0.503023774109956
Loss in iteration 14 : 0.498740543983693
Loss in iteration 15 : 0.49518390539967094
Loss in iteration 16 : 0.4913011869626578
Loss in iteration 17 : 0.4890014769937459
Loss in iteration 18 : 0.4875809638342375
Loss in iteration 19 : 0.48557508867080595
Loss in iteration 20 : 0.4839133671189929
Loss in iteration 21 : 0.48314891278701294
Loss in iteration 22 : 0.48223982239105573
Loss in iteration 23 : 0.4810212193732078
Loss in iteration 24 : 0.4802597726326543
Loss in iteration 25 : 0.479773820561483
Loss in iteration 26 : 0.47895414105460654
Loss in iteration 27 : 0.4780662995320925
Loss in iteration 28 : 0.4774954588748632
Loss in iteration 29 : 0.4769243768992708
Loss in iteration 30 : 0.47612143663707557
Loss in iteration 31 : 0.4753977949565597
Loss in iteration 32 : 0.47487638483039246
Loss in iteration 33 : 0.47428659159559095
Loss in iteration 34 : 0.47359148157884734
Loss in iteration 35 : 0.4730162121402359
Loss in iteration 36 : 0.4725385458082686
Loss in iteration 37 : 0.4719760264714315
Loss in iteration 38 : 0.4713805206014813
Loss in iteration 39 : 0.4708862018016841
Loss in iteration 40 : 0.4704223750302267
Loss in iteration 41 : 0.46989729621645265
Loss in iteration 42 : 0.46939179154096855
Loss in iteration 43 : 0.46896741661472163
Loss in iteration 44 : 0.4685494117915348
Loss in iteration 45 : 0.46810729416169194
Loss in iteration 46 : 0.46771043800052914
Loss in iteration 47 : 0.4673690327701513
Loss in iteration 48 : 0.4670242573400019
Loss in iteration 49 : 0.4666805408095489
Loss in iteration 50 : 0.46638413502090986
Loss in iteration 51 : 0.46612036704400656
Loss in iteration 52 : 0.465854531581469
Loss in iteration 53 : 0.46560589803016555
Loss in iteration 54 : 0.4653970162754964
Loss in iteration 55 : 0.46520509998011783
Loss in iteration 56 : 0.4650147759637305
Loss in iteration 57 : 0.46484526138003945
Loss in iteration 58 : 0.4647010848713403
Loss in iteration 59 : 0.46456213235814686
Loss in iteration 60 : 0.46442639022703236
Loss in iteration 61 : 0.4643078001305871
Loss in iteration 62 : 0.4642021145423888
Loss in iteration 63 : 0.4640970540886464
Loss in iteration 64 : 0.4639974773673074
Loss in iteration 65 : 0.46391071896887287
Loss in iteration 66 : 0.4638296991246483
Loss in iteration 67 : 0.4637489921377572
Loss in iteration 68 : 0.4636744620695141
Loss in iteration 69 : 0.4636078103423752
Loss in iteration 70 : 0.4635427017153659
Loss in iteration 71 : 0.46347833009452744
Loss in iteration 72 : 0.4634191414819887
Loss in iteration 73 : 0.4633639102697481
Loss in iteration 74 : 0.4633087289660512
Loss in iteration 75 : 0.4632550628562302
Loss in iteration 76 : 0.4632052438747701
Loss in iteration 77 : 0.46315706564336173
Loss in iteration 78 : 0.46310879339133304
Loss in iteration 79 : 0.4630622663855814
Loss in iteration 80 : 0.46301806621901326
Loss in iteration 81 : 0.46297424867829834
Loss in iteration 82 : 0.46293058451090646
Loss in iteration 83 : 0.46288852020621607
Loss in iteration 84 : 0.46284775619350577
Loss in iteration 85 : 0.462807125545997
Loss in iteration 86 : 0.4627671230774462
Loss in iteration 87 : 0.4627285465957584
Loss in iteration 88 : 0.4626907778244017
Loss in iteration 89 : 0.4626532973436462
Loss in iteration 90 : 0.46261669627257324
Loss in iteration 91 : 0.46258120600034713
Loss in iteration 92 : 0.46254624528911176
Loss in iteration 93 : 0.4625117228018418
Loss in iteration 94 : 0.4624780893361234
Loss in iteration 95 : 0.4624452739381408
Loss in iteration 96 : 0.4624128968561008
Loss in iteration 97 : 0.4623810748934644
Loss in iteration 98 : 0.4623500541158212
Loss in iteration 99 : 0.46231964501058154
Loss in iteration 100 : 0.46228965285117657
Loss in iteration 101 : 0.4622602331719597
Loss in iteration 102 : 0.462231462092217
Loss in iteration 103 : 0.4622031521258102
Loss in iteration 104 : 0.46217524416243716
Loss in iteration 105 : 0.46214786585374507
Loss in iteration 106 : 0.4621210014043618
Loss in iteration 107 : 0.4620945202826447
Loss in iteration 108 : 0.46206843642748574
Loss in iteration 109 : 0.4620428241853553
Loss in iteration 110 : 0.4620176272257107
Loss in iteration 111 : 0.46199276985210574
Loss in iteration 112 : 0.4619682860996633
Loss in iteration 113 : 0.4619442007436671
Loss in iteration 114 : 0.4619204525463039
Loss in iteration 115 : 0.46189700843577336
Loss in iteration 116 : 0.4618739001064379
Loss in iteration 117 : 0.46185112352893676
Loss in iteration 118 : 0.46182863187991224
Loss in iteration 119 : 0.4618064181648668
Loss in iteration 120 : 0.4617845021401413
Loss in iteration 121 : 0.4617628662138301
Loss in iteration 122 : 0.4617414802688213
Loss in iteration 123 : 0.4617203480102905
Loss in iteration 124 : 0.4616994766600427
Loss in iteration 125 : 0.4616788463085652
Loss in iteration 126 : 0.46165844130909434
Loss in iteration 127 : 0.4616382681229084
Loss in iteration 128 : 0.4616183258982327
Loss in iteration 129 : 0.4615985983929908
Loss in iteration 130 : 0.4615790793939491
Loss in iteration 131 : 0.46155977348678245
Loss in iteration 132 : 0.4615406751806142
Loss in iteration 133 : 0.46152177278221823
Loss in iteration 134 : 0.4615030644037351
Loss in iteration 135 : 0.46148455132539395
Loss in iteration 136 : 0.461466226517844
Loss in iteration 137 : 0.46144808254867764
Loss in iteration 138 : 0.4614301191592509
Loss in iteration 139 : 0.4614123352049685
Loss in iteration 140 : 0.4613947242495255
Loss in iteration 141 : 0.46137728200681066
Loss in iteration 142 : 0.46136000824876616
Loss in iteration 143 : 0.46134290033432357
Loss in iteration 144 : 0.46132595304822266
Loss in iteration 145 : 0.4613091638664067
Loss in iteration 146 : 0.46129253194712566
Loss in iteration 147 : 0.46127605408998185
Loss in iteration 148 : 0.4612597264269256
Loss in iteration 149 : 0.4612435473241755
Loss in iteration 150 : 0.4612275153770637
Loss in iteration 151 : 0.4612116274782938
Loss in iteration 152 : 0.461195880840657
Loss in iteration 153 : 0.46118027410972795
Loss in iteration 154 : 0.46116480547449723
Loss in iteration 155 : 0.461149472164928
Loss in iteration 156 : 0.46113427208669316
Loss in iteration 157 : 0.4611192038781229
Loss in iteration 158 : 0.461104265577432
Loss in iteration 159 : 0.4610894548650964
Loss in iteration 160 : 0.4610747700920963
Loss in iteration 161 : 0.4610602098619993
Loss in iteration 162 : 0.4610457722837924
Loss in iteration 163 : 0.46103145546426494
Loss in iteration 164 : 0.46101725800177734
Loss in iteration 165 : 0.4610031784769617
Loss in iteration 166 : 0.4609892151577634
Loss in iteration 167 : 0.46097536647224907
Loss in iteration 168 : 0.46096163114504163
Loss in iteration 169 : 0.4609480077840595
Loss in iteration 170 : 0.46093449485630955
Loss in iteration 171 : 0.4609210910253796
Loss in iteration 172 : 0.46090779509645247
Loss in iteration 173 : 0.4608946057515145
Loss in iteration 174 : 0.4608815216467759
Loss in iteration 175 : 0.46086854160318297
Loss in iteration 176 : 0.4608556644842209
Loss in iteration 177 : 0.4608428890651071
Loss in iteration 178 : 0.46083021415748304
Loss in iteration 179 : 0.46081763868736414
Loss in iteration 180 : 0.4608051615764035
Loss in iteration 181 : 0.46079278170272964
Loss in iteration 182 : 0.4607804980040903
Loss in iteration 183 : 0.46076830948627967
Loss in iteration 184 : 0.460756215136111
Loss in iteration 185 : 0.4607442139311131
Loss in iteration 186 : 0.4607323049061126
Loss in iteration 187 : 0.4607204871296546
Loss in iteration 188 : 0.4607087596536343
Loss in iteration 189 : 0.46069712154179054
Loss in iteration 190 : 0.4606855719034544
Loss in iteration 191 : 0.460674109862364
Loss in iteration 192 : 0.4606627345349054
Loss in iteration 193 : 0.4606514450589062
Loss in iteration 194 : 0.46064024060414904
Loss in iteration 195 : 0.460629120345812
Loss in iteration 196 : 0.4606180834605549
Loss in iteration 197 : 0.4606071291479055
Loss in iteration 198 : 0.4605962566280787
Loss in iteration 199 : 0.46058546512427906
Loss in iteration 200 : 0.4605747538674406
Loss in iteration 201 : 0.46056412210884845
Loss in iteration 202 : 0.4605535691130297
Loss in iteration 203 : 0.46054309414842015
Loss in iteration 204 : 0.4605326964945036
Loss in iteration 205 : 0.4605223754472996
Loss in iteration 206 : 0.4605121303118362
Loss in iteration 207 : 0.46050196039868446
Loss in iteration 208 : 0.4604918650301721
Loss in iteration 209 : 0.4604818435413448
Loss in iteration 210 : 0.4604718952742003
Loss in iteration 211 : 0.4604620195776524
Loss in iteration 212 : 0.4604522158117575
Loss in iteration 213 : 0.4604424833464411
Loss in iteration 214 : 0.4604328215579143
Loss in iteration 215 : 0.4604232298300307
Loss in iteration 216 : 0.4604137075564923
Loss in iteration 217 : 0.4604042541388826
Loss in iteration 218 : 0.46039486898492826
Loss in iteration 219 : 0.460385551510072
Loss in iteration 220 : 0.4603763011382246
Loss in iteration 221 : 0.4603671172999884
Loss in iteration 222 : 0.46035799943208483
Loss in iteration 223 : 0.4603489469786113
Loss in iteration 224 : 0.4603399593909234
Loss in iteration 225 : 0.46033103612636445
Loss in iteration 226 : 0.4603221766483421
Loss in iteration 227 : 0.46031338042707876
Loss in iteration 228 : 0.46030464693912276
Loss in iteration 229 : 0.4602959756665883
Loss in iteration 230 : 0.4602873660974456
Loss in iteration 231 : 0.46027881772582724
Loss in iteration 232 : 0.4602703300515016
Loss in iteration 233 : 0.46026190257949245
Loss in iteration 234 : 0.4602535348203918
Loss in iteration 235 : 0.4602452262903743
Loss in iteration 236 : 0.460236976510754
Loss in iteration 237 : 0.4602287850078517
Loss in iteration 238 : 0.46022065131318945
Testing accuracy  of updater 1 on alg 0 with rate 0.4 = 0.789875, training accuracy 0.789875, time elapsed: 4478 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6878658918511898
Loss in iteration 3 : 0.6807027170966673
Loss in iteration 4 : 0.6739294871347788
Loss in iteration 5 : 0.6668886231145068
Loss in iteration 6 : 0.6579684441529692
Loss in iteration 7 : 0.6470866160584787
Loss in iteration 8 : 0.6358501545886254
Loss in iteration 9 : 0.6258176075743548
Loss in iteration 10 : 0.6170799796352974
Loss in iteration 11 : 0.6086149317782455
Loss in iteration 12 : 0.5996947076182616
Loss in iteration 13 : 0.5906248408492144
Loss in iteration 14 : 0.5822230682148332
Loss in iteration 15 : 0.574914685914503
Loss in iteration 16 : 0.5684454167556635
Loss in iteration 17 : 0.5623019911408141
Loss in iteration 18 : 0.5562424100141602
Loss in iteration 19 : 0.5504326885724559
Loss in iteration 20 : 0.5451825083412979
Loss in iteration 21 : 0.5406159468252317
Loss in iteration 22 : 0.5365812551388358
Loss in iteration 23 : 0.5328221106337717
Loss in iteration 24 : 0.5291993831674031
Loss in iteration 25 : 0.525760250628342
Loss in iteration 26 : 0.5226337961602792
Loss in iteration 27 : 0.5198843942263012
Loss in iteration 28 : 0.5174547948874161
Loss in iteration 29 : 0.5152224074099973
Loss in iteration 30 : 0.5130951623791218
Loss in iteration 31 : 0.5110616419569142
Loss in iteration 32 : 0.5091677590336268
Loss in iteration 33 : 0.5074555801347295
Loss in iteration 34 : 0.5059201648144045
Loss in iteration 35 : 0.5045134235986846
Loss in iteration 36 : 0.5031813971044401
Loss in iteration 37 : 0.5018989110339537
Loss in iteration 38 : 0.5006757386534609
Loss in iteration 39 : 0.4995358671741112
Loss in iteration 40 : 0.49849131296566696
Loss in iteration 41 : 0.4975310688101306
Loss in iteration 42 : 0.49662985596631243
Loss in iteration 43 : 0.49576578659868803
Loss in iteration 44 : 0.4949320733439207
Loss in iteration 45 : 0.49413559345821256
Loss in iteration 46 : 0.4933861683257528
Loss in iteration 47 : 0.49268622853532656
Loss in iteration 48 : 0.4920281782595213
Loss in iteration 49 : 0.491399718080306
Loss in iteration 50 : 0.4907916535732544
Loss in iteration 51 : 0.4902020984228692
Loss in iteration 52 : 0.489634821506781
Loss in iteration 53 : 0.4890941049426018
Loss in iteration 54 : 0.4885804409233668
Loss in iteration 55 : 0.4880898936479985
Loss in iteration 56 : 0.4876167941338949
Loss in iteration 57 : 0.487157184537277
Loss in iteration 58 : 0.48671045083526493
Loss in iteration 59 : 0.4862783720903899
Loss in iteration 60 : 0.4858627606230021
Loss in iteration 61 : 0.4854636000986494
Loss in iteration 62 : 0.48507885849478
Loss in iteration 63 : 0.48470576326723647
Loss in iteration 64 : 0.4843423654281744
Loss in iteration 65 : 0.4839882588628247
Loss in iteration 66 : 0.4836441295503578
Loss in iteration 67 : 0.4833106756981422
Loss in iteration 68 : 0.4829877600924281
Loss in iteration 69 : 0.48267432391649207
Loss in iteration 70 : 0.4823689670102038
Loss in iteration 71 : 0.4820706668370649
Loss in iteration 72 : 0.48177912080797397
Loss in iteration 73 : 0.4814945544928545
Loss in iteration 74 : 0.4812172299497165
Loss in iteration 75 : 0.48094704354828277
Loss in iteration 76 : 0.4806834648577373
Loss in iteration 77 : 0.48042578893623544
Loss in iteration 78 : 0.4801734694714035
Loss in iteration 79 : 0.47992629218375016
Loss in iteration 80 : 0.47968430268418705
Loss in iteration 81 : 0.47944758434255375
Loss in iteration 82 : 0.47921606307838194
Loss in iteration 83 : 0.4789894630679938
Loss in iteration 84 : 0.47876741239221454
Loss in iteration 85 : 0.4785495982493625
Loss in iteration 86 : 0.47833585842962967
Loss in iteration 87 : 0.4781261605289943
Loss in iteration 88 : 0.4779205044102549
Loss in iteration 89 : 0.4777188272581826
Loss in iteration 90 : 0.477520973256807
Loss in iteration 91 : 0.47732673503584433
Loss in iteration 92 : 0.4771359252626396
Loss in iteration 93 : 0.47694842504935026
Loss in iteration 94 : 0.4767641815804591
Loss in iteration 95 : 0.47658316619176516
Loss in iteration 96 : 0.47640532792255535
Loss in iteration 97 : 0.4762305736654909
Loss in iteration 98 : 0.4760587824946961
Loss in iteration 99 : 0.4758898378862771
Loss in iteration 100 : 0.4757236530472288
Loss in iteration 101 : 0.47556017395497596
Loss in iteration 102 : 0.47539936231089985
Loss in iteration 103 : 0.47524117331380283
Loss in iteration 104 : 0.4750855436433983
Loss in iteration 105 : 0.47493239539988513
Loss in iteration 106 : 0.4747816502468817
Loss in iteration 107 : 0.47463324249938044
Loss in iteration 108 : 0.474487122789071
Loss in iteration 109 : 0.47434325169896385
Loss in iteration 110 : 0.47420158939315354
Loss in iteration 111 : 0.47406208869283206
Loss in iteration 112 : 0.4739246953657453
Loss in iteration 113 : 0.47378935399279665
Loss in iteration 114 : 0.47365601448764516
Loss in iteration 115 : 0.4735246348838857
Loss in iteration 116 : 0.4733951792712602
Loss in iteration 117 : 0.4732676131132624
Loss in iteration 118 : 0.4731418994350269
Loss in iteration 119 : 0.47301799813244083
Loss in iteration 120 : 0.47289586820643864
Loss in iteration 121 : 0.47277547088256144
Loss in iteration 122 : 0.4726567714038058
Loss in iteration 123 : 0.4725397385685741
Loss in iteration 124 : 0.47242434271593803
Loss in iteration 125 : 0.4723105537225417
Loss in iteration 126 : 0.472198340266217
Loss in iteration 127 : 0.4720876705573992
Loss in iteration 128 : 0.4719785137620186
Loss in iteration 129 : 0.4718708410492527
Loss in iteration 130 : 0.4717646256463809
Loss in iteration 131 : 0.4716598420394223
Loss in iteration 132 : 0.47155646497874903
Loss in iteration 133 : 0.4714544689468322
Loss in iteration 134 : 0.4713538283237596
Loss in iteration 135 : 0.47125451799588747
Loss in iteration 136 : 0.4711565139200753
Loss in iteration 137 : 0.47105979327893216
Loss in iteration 138 : 0.4709643341937726
Loss in iteration 139 : 0.47087011525024797
Loss in iteration 140 : 0.4707771151618119
Loss in iteration 141 : 0.47068531274601366
Loss in iteration 142 : 0.4705946871565271
Loss in iteration 143 : 0.47050521816299434
Loss in iteration 144 : 0.47041688628125566
Loss in iteration 145 : 0.47032967269034587
Loss in iteration 146 : 0.470243559020711
Loss in iteration 147 : 0.47015852716405937
Loss in iteration 148 : 0.47007455921296504
Loss in iteration 149 : 0.4699916375357317
Loss in iteration 150 : 0.46990974490574383
Loss in iteration 151 : 0.4698288645859182
Loss in iteration 152 : 0.46974898031670725
Loss in iteration 153 : 0.46967007622720314
Loss in iteration 154 : 0.46959213673390376
Loss in iteration 155 : 0.46951514648720893
Loss in iteration 156 : 0.46943909038423554
Loss in iteration 157 : 0.4693639536213378
Loss in iteration 158 : 0.469289721740069
Loss in iteration 159 : 0.4692163806335866
Loss in iteration 160 : 0.46914391651204423
Loss in iteration 161 : 0.4690723158517996
Loss in iteration 162 : 0.46900156535885196
Loss in iteration 163 : 0.468931651962332
Loss in iteration 164 : 0.4688625628320087
Loss in iteration 165 : 0.46879428540002965
Loss in iteration 166 : 0.46872680736836786
Loss in iteration 167 : 0.46866011669610413
Loss in iteration 168 : 0.4685942015745544
Loss in iteration 169 : 0.4685290504044641
Loss in iteration 170 : 0.46846465178556795
Loss in iteration 171 : 0.4684009945191629
Loss in iteration 172 : 0.46833806761625035
Loss in iteration 173 : 0.46827586030182744
Loss in iteration 174 : 0.4682143620102679
Loss in iteration 175 : 0.46815356237342104
Loss in iteration 176 : 0.46809345120736545
Loss in iteration 177 : 0.4680340185036229
Loss in iteration 178 : 0.46797525442673793
Loss in iteration 179 : 0.46791714931592504
Loss in iteration 180 : 0.4678596936864535
Loss in iteration 181 : 0.4678028782274632
Loss in iteration 182 : 0.46774669379587247
Loss in iteration 183 : 0.46769113140857677
Loss in iteration 184 : 0.4676361822358418
Loss in iteration 185 : 0.4675818375975605
Loss in iteration 186 : 0.4675280889619976
Loss in iteration 187 : 0.46747492794521817
Loss in iteration 188 : 0.46742234630942503
Loss in iteration 189 : 0.46737033595944844
Loss in iteration 190 : 0.4673188889380541
Loss in iteration 191 : 0.46726799742135416
Loss in iteration 192 : 0.4672176537153769
Loss in iteration 193 : 0.4671678502539586
Loss in iteration 194 : 0.46711857959733244
Loss in iteration 195 : 0.4670698344305013
Loss in iteration 196 : 0.4670216075608561
Loss in iteration 197 : 0.4669738919150709
Loss in iteration 198 : 0.46692668053584374
Loss in iteration 199 : 0.4668799665790122
Loss in iteration 200 : 0.46683374331132077
Loss in iteration 201 : 0.4667880041086926
Loss in iteration 202 : 0.46674274245457614
Loss in iteration 203 : 0.4666979519380557
Loss in iteration 204 : 0.46665362625162915
Loss in iteration 205 : 0.4666097591888233
Loss in iteration 206 : 0.46656634464191143
Loss in iteration 207 : 0.4665233765999286
Loss in iteration 208 : 0.46648084914697574
Loss in iteration 209 : 0.46643875646065713
Loss in iteration 210 : 0.4663970928104739
Loss in iteration 211 : 0.4663558525560854
Loss in iteration 212 : 0.46631503014545195
Loss in iteration 213 : 0.4662746201130017
Loss in iteration 214 : 0.4662346170779198
Loss in iteration 215 : 0.46619501574258343
Loss in iteration 216 : 0.466155810891134
Loss in iteration 217 : 0.46611699738804657
Loss in iteration 218 : 0.4660785701767094
Loss in iteration 219 : 0.46604052427791487
Loss in iteration 220 : 0.4660028547883836
Loss in iteration 221 : 0.4659655568793221
Loss in iteration 222 : 0.4659286257950705
Loss in iteration 223 : 0.46589205685182516
Loss in iteration 224 : 0.4658558454364028
Loss in iteration 225 : 0.4658199870050128
Loss in iteration 226 : 0.46578447708201043
Loss in iteration 227 : 0.4657493112586594
Loss in iteration 228 : 0.46571448519191105
Loss in iteration 229 : 0.46567999460322795
Loss in iteration 230 : 0.46564583527746695
Loss in iteration 231 : 0.46561200306178036
Loss in iteration 232 : 0.4655784938645592
Loss in iteration 233 : 0.4655453036543733
Loss in iteration 234 : 0.4655124284589177
Loss in iteration 235 : 0.46547986436398164
Loss in iteration 236 : 0.46544760751243736
Loss in iteration 237 : 0.46541565410326996
Loss in iteration 238 : 0.46538400039062205
Loss in iteration 239 : 0.46535264268287724
Loss in iteration 240 : 0.4653215773417412
Loss in iteration 241 : 0.4652908007813378
Loss in iteration 242 : 0.4652603094673275
Loss in iteration 243 : 0.46523009991602615
Loss in iteration 244 : 0.46520016869356595
Loss in iteration 245 : 0.46517051241505597
Loss in iteration 246 : 0.46514112774377636
Loss in iteration 247 : 0.4651120113903859
Loss in iteration 248 : 0.4650831601121416
Loss in iteration 249 : 0.4650545707121262
Loss in iteration 250 : 0.46502624003849163
Loss in iteration 251 : 0.4649981649837262
Loss in iteration 252 : 0.4649703424839295
Loss in iteration 253 : 0.46494276951809743
Loss in iteration 254 : 0.4649154431074477
Loss in iteration 255 : 0.46488836031471464
Loss in iteration 256 : 0.46486151824350147
Loss in iteration 257 : 0.4648349140376038
Loss in iteration 258 : 0.46480854488037526
Loss in iteration 259 : 0.4647824079940935
Loss in iteration 260 : 0.46475650063932944
Loss in iteration 261 : 0.46473082011434935
Loss in iteration 262 : 0.46470536375451876
Loss in iteration 263 : 0.46468012893170785
Loss in iteration 264 : 0.46465511305372137
Loss in iteration 265 : 0.4646303135637295
Loss in iteration 266 : 0.46460572793971616
Loss in iteration 267 : 0.4645813536939307
Loss in iteration 268 : 0.46455718837235577
Loss in iteration 269 : 0.4645332295541859
Loss in iteration 270 : 0.46450947485130417
Loss in iteration 271 : 0.46448592190778126
Loss in iteration 272 : 0.46446256839938005
Loss in iteration 273 : 0.4644394120330591
Loss in iteration 274 : 0.4644164505465023
Loss in iteration 275 : 0.4643936817076367
Loss in iteration 276 : 0.46437110331418113
Loss in iteration 277 : 0.46434871319318416
Loss in iteration 278 : 0.46432650920057916
Loss in iteration 279 : 0.46430448922074996
Loss in iteration 280 : 0.4642826511660899
Loss in iteration 281 : 0.46426099297658857
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.79075, training accuracy 0.79075, time elapsed: 4888 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 4.016337962504751
Loss in iteration 3 : 42.50079140495103
Loss in iteration 4 : 1.3409879028154255
Loss in iteration 5 : 20.770324756889575
Loss in iteration 6 : 4.661303411900024
Loss in iteration 7 : 19.463252964054096
Loss in iteration 8 : 7.37061480770914
Loss in iteration 9 : 12.116460569775128
Loss in iteration 10 : 11.860532355582269
Loss in iteration 11 : 7.767552345407355
Loss in iteration 12 : 7.8526885767327395
Loss in iteration 13 : 6.041902174972181
Loss in iteration 14 : 5.54703072320268
Loss in iteration 15 : 4.615297343999984
Loss in iteration 16 : 4.258664681104029
Loss in iteration 17 : 3.91475001594677
Loss in iteration 18 : 3.7499132410104066
Loss in iteration 19 : 3.5714106103577907
Loss in iteration 20 : 3.466928672438601
Loss in iteration 21 : 3.3673763127174525
Loss in iteration 22 : 3.2722016231195616
Loss in iteration 23 : 3.1792720495428437
Loss in iteration 24 : 3.0921629230412173
Loss in iteration 25 : 3.010242823125025
Loss in iteration 26 : 2.9700342839289795
Loss in iteration 27 : 2.9863717026569025
Loss in iteration 28 : 3.3054091620550103
Loss in iteration 29 : 3.9039537556999013
Loss in iteration 30 : 6.572198296404988
Loss in iteration 31 : 6.7071852448610585
Loss in iteration 32 : 9.984871282708227
Loss in iteration 33 : 6.4335846766340055
Loss in iteration 34 : 8.869845279207537
Loss in iteration 35 : 7.052267480835764
Loss in iteration 36 : 8.294011505600043
Loss in iteration 37 : 6.55849908979351
Loss in iteration 38 : 7.215849315672143
Loss in iteration 39 : 6.035675146856262
Loss in iteration 40 : 6.297184467435166
Loss in iteration 41 : 5.467627252363796
Loss in iteration 42 : 5.526578377162166
Loss in iteration 43 : 4.9543890494676655
Loss in iteration 44 : 5.067758336459337
Loss in iteration 45 : 4.682253840805415
Loss in iteration 46 : 4.939047091029664
Loss in iteration 47 : 4.646012768924973
Loss in iteration 48 : 5.08526436080914
Loss in iteration 49 : 4.738782098156359
Loss in iteration 50 : 5.407379719812947
Loss in iteration 51 : 5.018141541691384
Loss in iteration 52 : 5.9286447707385
Loss in iteration 53 : 5.338793878847575
Loss in iteration 54 : 6.3497378081065365
Loss in iteration 55 : 5.496226967775113
Loss in iteration 56 : 6.447996614487119
Loss in iteration 57 : 5.485340426203826
Loss in iteration 58 : 6.306572941388068
Loss in iteration 59 : 5.39576126786729
Loss in iteration 60 : 6.0627863111658336
Loss in iteration 61 : 5.238397992947783
Loss in iteration 62 : 5.778797292674799
Loss in iteration 63 : 5.071282968765223
Loss in iteration 64 : 5.545409152424139
Loss in iteration 65 : 4.959385816909771
Loss in iteration 66 : 5.426255087280009
Loss in iteration 67 : 4.912105062255796
Loss in iteration 68 : 5.416511903188551
Loss in iteration 69 : 4.9522651038874885
Loss in iteration 70 : 5.520521279082606
Loss in iteration 71 : 5.062832642475028
Loss in iteration 72 : 5.692361505089935
Loss in iteration 73 : 5.18559026225153
Loss in iteration 74 : 5.852072140972345
Loss in iteration 75 : 5.288263573172415
Loss in iteration 76 : 5.961474864878832
Loss in iteration 77 : 5.339965595904985
Loss in iteration 78 : 5.990958430398514
Loss in iteration 79 : 5.338429377458419
Loss in iteration 80 : 5.9508183552696865
Loss in iteration 81 : 5.29951639655088
Loss in iteration 82 : 5.872041352041496
Loss in iteration 83 : 5.237653662372388
Loss in iteration 84 : 5.781050137386947
Loss in iteration 85 : 5.172940974660581
Loss in iteration 86 : 5.704260510611095
Loss in iteration 87 : 5.124091582887643
Loss in iteration 88 : 5.660332987327769
Loss in iteration 89 : 5.101689436925228
Loss in iteration 90 : 5.655170878424995
Loss in iteration 91 : 5.107646545500014
Loss in iteration 92 : 5.683206390354489
Loss in iteration 93 : 5.134746721547794
Loss in iteration 94 : 5.729230287276209
Loss in iteration 95 : 5.170376008777291
Loss in iteration 96 : 5.774902296910435
Loss in iteration 97 : 5.201816183208776
Loss in iteration 98 : 5.805640310368617
Loss in iteration 99 : 5.220306886645028
Loss in iteration 100 : 5.814596198310314
Loss in iteration 101 : 5.223352214987688
Loss in iteration 102 : 5.803627781259595
Loss in iteration 103 : 5.213918236929661
Loss in iteration 104 : 5.780552908465222
Loss in iteration 105 : 5.198001573171768
Loss in iteration 106 : 5.755067021657724
Loss in iteration 107 : 5.182292221210571
Loss in iteration 108 : 5.735534214301161
Loss in iteration 109 : 5.172111216796624
Loss in iteration 110 : 5.726840190258474
Loss in iteration 111 : 5.16992171254494
Loss in iteration 112 : 5.72943762960154
Loss in iteration 113 : 5.174952362456417
Loss in iteration 114 : 5.739891157404829
Loss in iteration 115 : 5.184046703963547
Loss in iteration 116 : 5.752720994293095
Loss in iteration 117 : 5.193230886740731
Loss in iteration 118 : 5.762679641167436
Loss in iteration 119 : 5.19925026843188
Loss in iteration 120 : 5.766498701615359
Loss in iteration 121 : 5.200536026720276
Loss in iteration 122 : 5.763594972986123
Loss in iteration 123 : 5.197394036559046
Loss in iteration 124 : 5.755720952436009
Loss in iteration 125 : 5.191537706508965
Loss in iteration 126 : 5.745924054704944
Loss in iteration 127 : 5.185275156239433
Loss in iteration 128 : 5.737318946042292
Loss in iteration 129 : 5.180663516218641
Loss in iteration 130 : 5.732079772616395
Loss in iteration 131 : 5.178879851929262
Loss in iteration 132 : 5.730900979807057
Loss in iteration 133 : 5.179975330138413
Loss in iteration 134 : 5.733022805757062
Loss in iteration 135 : 5.183049742345264
Loss in iteration 136 : 5.73674135319436
Loss in iteration 137 : 5.186724925468098
Loss in iteration 138 : 5.740154888714753
Loss in iteration 139 : 5.1896939959889075
Loss in iteration 140 : 5.741837736230929
Loss in iteration 141 : 5.191137001460572
Loss in iteration 142 : 5.741217917027764
Loss in iteration 143 : 5.190889838405355
Loss in iteration 144 : 5.738592009838072
Loss in iteration 145 : 5.189364229716542
Loss in iteration 146 : 5.734850848351207
Loss in iteration 147 : 5.187297503621582
Loss in iteration 148 : 5.731067435221867
Loss in iteration 149 : 5.185446323718646
Loss in iteration 150 : 5.728107930244559
Loss in iteration 151 : 5.184332927323398
Loss in iteration 152 : 5.726385988145973
Loss in iteration 153 : 5.184117429374562
Loss in iteration 154 : 5.725814129548109
Loss in iteration 155 : 5.184616928080299
Loss in iteration 156 : 5.725933225552488
Loss in iteration 157 : 5.185438091675505
Loss in iteration 158 : 5.726143568078327
Loss in iteration 159 : 5.186155374718502
Loss in iteration 160 : 5.725938451771842
Loss in iteration 161 : 5.186464023507924
Loss in iteration 162 : 5.725059110130936
Loss in iteration 163 : 5.186260724307986
Loss in iteration 164 : 5.723533953218652
Loss in iteration 165 : 5.185639554540407
Loss in iteration 166 : 5.721612522629404
Loss in iteration 167 : 5.184821738896699
Loss in iteration 168 : 5.719637716275568
Loss in iteration 169 : 5.184055766498386
Loss in iteration 170 : 5.717911336891485
Loss in iteration 171 : 5.1835271533620455
Loss in iteration 172 : 5.716599359584463
Loss in iteration 173 : 5.183306650815461
Loss in iteration 174 : 5.715701316469705
Loss in iteration 175 : 5.1833474468360015
Loss in iteration 176 : 5.7150821204869935
Loss in iteration 177 : 5.183523160071849
Loss in iteration 178 : 5.714543692176239
Loss in iteration 179 : 5.183685849536643
Loss in iteration 180 : 5.713904301285658
Loss in iteration 181 : 5.183720311640235
Loss in iteration 182 : 5.713056992119213
Loss in iteration 183 : 5.18357683235279
Loss in iteration 184 : 5.711991214130321
Loss in iteration 185 : 5.183275418612215
Loss in iteration 186 : 5.710777480828907
Loss in iteration 187 : 5.1828855778765135
Loss in iteration 188 : 5.709527485943951
Loss in iteration 189 : 5.182493200156754
Loss in iteration 190 : 5.7083479854524795
Loss in iteration 191 : 5.182168250815926
Loss in iteration 192 : 5.707305313307726
Loss in iteration 193 : 5.181944109194647
Loss in iteration 194 : 5.706410558024275
Loss in iteration 195 : 5.18181338028875
Loss in iteration 196 : 5.7056264960799234
Loss in iteration 197 : 5.181738470652643
Loss in iteration 198 : 5.704889748666159
Loss in iteration 199 : 5.18167048523412
Loss in iteration 200 : 5.7041376532704
Loss in iteration 201 : 5.1815683644145105
Loss in iteration 202 : 5.7033297128134866
Loss in iteration 203 : 5.1814116288760115
Loss in iteration 204 : 5.702457272861669
Loss in iteration 205 : 5.181203556641802
Loss in iteration 206 : 5.701540336793962
Loss in iteration 207 : 5.180965513245603
Loss in iteration 208 : 5.700615067087807
Loss in iteration 209 : 5.180726069896728
Loss in iteration 210 : 5.6997180582801885
Loss in iteration 211 : 5.180509660126037
Loss in iteration 212 : 5.698873431142876
Loss in iteration 213 : 5.180328796873999
Loss in iteration 214 : 5.698086687763754
Loss in iteration 215 : 5.180181892807645
Loss in iteration 216 : 5.697346195808146
Loss in iteration 217 : 5.180056407644503
Loss in iteration 218 : 5.6966303686816415
Loss in iteration 219 : 5.179935247331245
Loss in iteration 220 : 5.695916989465021
Loss in iteration 221 : 5.179803587230944
Loss in iteration 222 : 5.69519103351599
Loss in iteration 223 : 5.1796536574441285
Loss in iteration 224 : 5.694448534563982
Loss in iteration 225 : 5.179486185875387
Loss in iteration 226 : 5.69369586791284
Loss in iteration 227 : 5.179308599058269
Loss in iteration 228 : 5.692945539870006
Loss in iteration 229 : 5.179131197562594
Loss in iteration 230 : 5.69221059638459
Loss in iteration 231 : 5.178963013151561
Loss in iteration 232 : 5.691499863520834
Loss in iteration 233 : 5.178808859616581
Loss in iteration 234 : 5.690815540088501
Loss in iteration 235 : 5.178668399384302
Loss in iteration 236 : 5.690153563032678
Loss in iteration 237 : 5.178537192912281
Loss in iteration 238 : 5.689506116286238
Loss in iteration 239 : 5.1784090060289225
Loss in iteration 240 : 5.6888650120968265
Loss in iteration 241 : 5.178278335562301
Loss in iteration 242 : 5.688224593171086
Loss in iteration 243 : 5.178142220631045
Loss in iteration 244 : 5.687583214128414
Loss in iteration 245 : 5.1780008247552765
Loss in iteration 246 : 5.686943031183793
Loss in iteration 247 : 5.177856800838425
Loss in iteration 248 : 5.686308477166769
Loss in iteration 249 : 5.177713881405545
Loss in iteration 250 : 5.685684199193061
Loss in iteration 251 : 5.177575334282766
Loss in iteration 252 : 5.685073289954182
Loss in iteration 253 : 5.177442859333737
Loss in iteration 254 : 5.684476392272684
Loss in iteration 255 : 5.1773162435062385
Loss in iteration 256 : 5.6838918428944565
Loss in iteration 257 : 5.177193764861421
Loss in iteration 258 : 5.6833166209142965
Loss in iteration 259 : 5.177073069621743
Loss in iteration 260 : 5.682747619194357
Loss in iteration 261 : 5.176952124445183
Loss in iteration 262 : 5.682182725276336
Loss in iteration 263 : 5.176829887895955
Loss in iteration 264 : 5.6816213559822
Loss in iteration 265 : 5.176706507554549
Loss in iteration 266 : 5.681064348117329
Loss in iteration 267 : 5.176583053483477
Loss in iteration 268 : 5.680513356506015
Loss in iteration 269 : 5.176460964220133
Loss in iteration 270 : 5.67997006212815
Loss in iteration 271 : 5.176341455079991
Loss in iteration 272 : 5.6794355094489966
Loss in iteration 273 : 5.176225109383904
Loss in iteration 274 : 5.678909790369682
Loss in iteration 275 : 5.176111769274981
Loss in iteration 276 : 5.678392129415901
Loss in iteration 277 : 5.17600071432647
Loss in iteration 278 : 5.677881270071035
Loss in iteration 279 : 5.175891013795511
Loss in iteration 280 : 5.677375970130688
Loss in iteration 281 : 5.175781894805315
Loss in iteration 282 : 5.676875407293766
Loss in iteration 283 : 5.175672989827753
Loss in iteration 284 : 5.676379362858181
Loss in iteration 285 : 5.175564394031782
Loss in iteration 286 : 5.675888154738655
Loss in iteration 287 : 5.175456544157172
Loss in iteration 288 : 5.675402387276134
Loss in iteration 289 : 5.175349993685096
Loss in iteration 290 : 5.674922640611256
Loss in iteration 291 : 5.175245184221219
Loss in iteration 292 : 5.674449223508094
Loss in iteration 293 : 5.175142297396448
Loss in iteration 294 : 5.6739820691921485
Loss in iteration 295 : 5.175041227794993
Loss in iteration 296 : 5.673520787874914
Loss in iteration 297 : 5.174941666200361
Loss in iteration 298 : 5.673064829914901
Loss in iteration 299 : 5.1748432438092715
Loss in iteration 300 : 5.672613680827462
Loss in iteration 301 : 5.17474567404323
Loss in iteration 302 : 5.6721670110513465
Loss in iteration 303 : 5.174648840197885
Loss in iteration 304 : 5.671724733083296
Loss in iteration 305 : 5.174552805842508
Loss in iteration 306 : 5.671286960684426
Loss in iteration 307 : 5.174457757214851
Loss in iteration 308 : 5.67085390178895
Loss in iteration 309 : 5.174363910338978
Loss in iteration 310 : 5.670425735794239
Loss in iteration 311 : 5.174271423063677
Loss in iteration 312 : 5.670002523078354
Loss in iteration 313 : 5.174180343614229
Loss in iteration 314 : 5.669584174605838
Loss in iteration 315 : 5.174090608441994
Loss in iteration 316 : 5.669170482597025
Loss in iteration 317 : 5.17400208179879
Loss in iteration 318 : 5.668761190534291
Loss in iteration 319 : 5.173914615333486
Loss in iteration 320 : 5.668356069933068
Loss in iteration 321 : 5.173828102279693
Loss in iteration 322 : 5.667954974327691
Loss in iteration 323 : 5.1737425071056204
Loss in iteration 324 : 5.667557854357452
Loss in iteration 325 : 5.1736578638143795
Loss in iteration 326 : 5.667164734964174
Loss in iteration 327 : 5.173574248840892
Loss in iteration 328 : 5.666775669568306
Loss in iteration 329 : 5.173491742895212
Loss in iteration 330 : 5.66639069209505
Loss in iteration 331 : 5.173410397765376
Loss in iteration 332 : 5.666009784976653
Loss in iteration 333 : 5.173330219539257
Loss in iteration 334 : 5.665632872256465
Loss in iteration 335 : 5.1732511716702465
Loss in iteration 336 : 5.665259836058713
Loss in iteration 337 : 5.173173193365753
Loss in iteration 338 : 5.664890546312584
Loss in iteration 339 : 5.173096223855969
Loss in iteration 340 : 5.664524890417713
Loss in iteration 341 : 5.173020222521121
Loss in iteration 342 : 5.66416279182452
Loss in iteration 343 : 5.172945178097348
Loss in iteration 344 : 5.663804212499057
Loss in iteration 345 : 5.172871105395934
Loss in iteration 346 : 5.6634491410893135
Loss in iteration 347 : 5.172798032880542
Loss in iteration 348 : 5.663097573617976
Loss in iteration 349 : 5.172725987279248
Loss in iteration 350 : 5.662749495143882
Loss in iteration 351 : 5.172654981460347
Loss in iteration 352 : 5.6624048690345345
Loss in iteration 353 : 5.1725850095241865
Loss in iteration 354 : 5.662063636526633
Loss in iteration 355 : 5.172516049703134
Loss in iteration 356 : 5.661725724964123
Loss in iteration 357 : 5.172448072653027
Loss in iteration 358 : 5.661391060148433
Loss in iteration 359 : 5.172381051124497
Loss in iteration 360 : 5.661059577484503
Loss in iteration 361 : 5.172314967174632
Loss in iteration 362 : 5.660731227965851
Loss in iteration 363 : 5.172249814653585
Loss in iteration 364 : 5.660405977643408
Loss in iteration 365 : 5.172185596850162
Loss in iteration 366 : 5.660083801884247
Loss in iteration 367 : 5.172122321005629
Loss in iteration 368 : 5.6597646774452075
Loss in iteration 369 : 5.172059992279296
Loss in iteration 370 : 5.659448575681641
Loss in iteration 371 : 5.1719986095104105
Loss in iteration 372 : 5.659135459218203
Loss in iteration 373 : 5.171938164048046
Loss in iteration 374 : 5.658825282716436
Loss in iteration 375 : 5.171878641556441
Loss in iteration 376 : 5.658517996737202
Loss in iteration 377 : 5.171820025610159
Loss in iteration 378 : 5.658213552713498
Loss in iteration 379 : 5.171762301428716
Loss in iteration 380 : 5.6579119069788355
Loss in iteration 381 : 5.171705458334697
Loss in iteration 382 : 5.657613022502851
Loss in iteration 383 : 5.171649490239678
Loss in iteration 384 : 5.6573168680765
Loss in iteration 385 : 5.171594394319567
Loss in iteration 386 : 5.657023415685444
Loss in iteration 387 : 5.171540168689656
Loss in iteration 388 : 5.656732637360125
Loss in iteration 389 : 5.171486810122095
Loss in iteration 390 : 5.65644450276264
Loss in iteration 391 : 5.171434312653076
Loss in iteration 392 : 5.656158978277507
Loss in iteration 393 : 5.171382667446221
Loss in iteration 394 : 5.6558760276784135
Loss in iteration 395 : 5.171331863746929
Loss in iteration 396 : 5.655595613842302
Loss in iteration 397 : 5.171281890382846
Loss in iteration 398 : 5.655317700682802
Loss in iteration 399 : 5.1712327371563465
Loss in iteration 400 : 5.65504225453773
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.76675, training accuracy 0.76675, time elapsed: 7219 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 2.8221613825594996
Loss in iteration 3 : 29.3331097261064
Loss in iteration 4 : 0.8426875793204027
Loss in iteration 5 : 10.188635600706196
Loss in iteration 6 : 9.691811433651164
Loss in iteration 7 : 14.165975302571066
Loss in iteration 8 : 3.0775860644430115
Loss in iteration 9 : 6.130453386531466
Loss in iteration 10 : 9.740182897982558
Loss in iteration 11 : 4.305891032758421
Loss in iteration 12 : 4.712536355543842
Loss in iteration 13 : 4.05384343473462
Loss in iteration 14 : 3.844643584907309
Loss in iteration 15 : 3.0966391064401733
Loss in iteration 16 : 2.905530566038704
Loss in iteration 17 : 2.7156951942095975
Loss in iteration 18 : 2.6263767437583865
Loss in iteration 19 : 2.5276698815003185
Loss in iteration 20 : 2.4655692316048836
Loss in iteration 21 : 2.4041569335847406
Loss in iteration 22 : 2.339392973323779
Loss in iteration 23 : 2.2707891904144963
Loss in iteration 24 : 2.2002919564253443
Loss in iteration 25 : 2.1285636439941316
Loss in iteration 26 : 2.067473626767053
Loss in iteration 27 : 2.019885277582767
Loss in iteration 28 : 2.061275332340987
Loss in iteration 29 : 2.270257891946093
Loss in iteration 30 : 3.2791421050686385
Loss in iteration 31 : 4.255839581165059
Loss in iteration 32 : 6.995119727411121
Loss in iteration 33 : 4.657451882024045
Loss in iteration 34 : 6.484945874230292
Loss in iteration 35 : 5.157048190540185
Loss in iteration 36 : 6.103805640487828
Loss in iteration 37 : 4.839323253914479
Loss in iteration 38 : 5.330654147944034
Loss in iteration 39 : 4.509713622821011
Loss in iteration 40 : 4.64081492174776
Loss in iteration 41 : 3.9841841698799945
Loss in iteration 42 : 4.0233682083031095
Loss in iteration 43 : 3.6103142314951397
Loss in iteration 44 : 3.6505473743264036
Loss in iteration 45 : 3.3444484805017494
Loss in iteration 46 : 3.4691795705954656
Loss in iteration 47 : 3.2474245371959243
Loss in iteration 48 : 3.4869342109303942
Loss in iteration 49 : 3.2743149305598065
Loss in iteration 50 : 3.65799298977998
Loss in iteration 51 : 3.3955102625495983
Loss in iteration 52 : 3.94012895850578
Loss in iteration 53 : 3.602912813013765
Loss in iteration 54 : 4.265022254714842
Loss in iteration 55 : 3.7815315745381106
Loss in iteration 56 : 4.448837760801824
Loss in iteration 57 : 3.829067720845193
Loss in iteration 58 : 4.42440764150528
Loss in iteration 59 : 3.796849904880462
Loss in iteration 60 : 4.293655178712091
Loss in iteration 61 : 3.7139846157594687
Loss in iteration 62 : 4.1163460615675325
Loss in iteration 63 : 3.6075880432217535
Loss in iteration 64 : 3.9457136963967168
Loss in iteration 65 : 3.5167523782420846
Loss in iteration 66 : 3.8297818891833395
Loss in iteration 67 : 3.463351417148411
Loss in iteration 68 : 3.7859163165759875
Loss in iteration 69 : 3.4626189299095342
Loss in iteration 70 : 3.819514658630647
Loss in iteration 71 : 3.513227604209329
Loss in iteration 72 : 3.9148213108391094
Loss in iteration 73 : 3.595349579200411
Loss in iteration 74 : 4.0341291232265135
Loss in iteration 75 : 3.6750547742465614
Loss in iteration 76 : 4.130929898743037
Loss in iteration 77 : 3.7269610311898282
Loss in iteration 78 : 4.17831395164513
Loss in iteration 79 : 3.742422590865046
Loss in iteration 80 : 4.173852327078967
Loss in iteration 81 : 3.728079462493731
Loss in iteration 82 : 4.133379191084109
Loss in iteration 83 : 3.6939759643987946
Loss in iteration 84 : 4.0756403496262275
Loss in iteration 85 : 3.6513091379640708
Loss in iteration 86 : 4.018000499190803
Loss in iteration 87 : 3.6119770919712986
Loss in iteration 88 : 3.974781494994315
Loss in iteration 89 : 3.585840550343243
Loss in iteration 90 : 3.9547939245813177
Loss in iteration 91 : 3.578270663346768
Loss in iteration 92 : 3.9598166939829564
Loss in iteration 93 : 3.5884006884188957
Loss in iteration 94 : 3.983974673601022
Loss in iteration 95 : 3.60965568634979
Loss in iteration 96 : 4.015850342721203
Loss in iteration 97 : 3.633014331057125
Loss in iteration 98 : 4.0434432437675865
Loss in iteration 99 : 3.650912777718577
Loss in iteration 100 : 4.058864262248083
Loss in iteration 101 : 3.659516404367862
Loss in iteration 102 : 4.060153350519393
Loss in iteration 103 : 3.6588465331435565
Loss in iteration 104 : 4.050272412900608
Loss in iteration 105 : 3.6515799766623003
Loss in iteration 106 : 4.034743222684112
Loss in iteration 107 : 3.64160257017684
Loss in iteration 108 : 4.019381982187393
Loss in iteration 109 : 3.6327319304813734
Loss in iteration 110 : 4.008666511826728
Loss in iteration 111 : 3.6276597389020067
Loss in iteration 112 : 4.0047586559864765
Loss in iteration 113 : 3.6273019303368583
Loss in iteration 114 : 4.007252476297769
Loss in iteration 115 : 3.630793491697442
Loss in iteration 116 : 4.01372520011908
Loss in iteration 117 : 3.6361171751543817
Loss in iteration 118 : 4.020900630062322
Loss in iteration 119 : 3.641040596531038
Loss in iteration 120 : 4.025928926627362
Loss in iteration 121 : 3.6439286260547714
Loss in iteration 122 : 4.027260803943069
Loss in iteration 123 : 3.644153537097975
Loss in iteration 124 : 4.024874091844635
Loss in iteration 125 : 3.642074283362987
Loss in iteration 126 : 4.019943170888318
Loss in iteration 127 : 3.638718355656356
Loss in iteration 128 : 4.014215123725969
Loss in iteration 129 : 3.635336882146108
Loss in iteration 130 : 4.009356745901404
Loss in iteration 131 : 3.632978549532335
Loss in iteration 132 : 4.006458681727303
Loss in iteration 133 : 3.6321946555662943
Loss in iteration 134 : 4.00580123065906
Loss in iteration 135 : 3.632945753047552
Loss in iteration 136 : 4.006909214353138
Loss in iteration 137 : 3.634715133141961
Loss in iteration 138 : 4.008839347941777
Loss in iteration 139 : 3.6367614809447733
Loss in iteration 140 : 4.010571443897816
Loss in iteration 141 : 3.6383997275439497
Loss in iteration 142 : 4.011352392771332
Loss in iteration 143 : 3.639207594881861
Loss in iteration 144 : 4.010883458553719
Loss in iteration 145 : 3.639104065324268
Loss in iteration 146 : 4.009320367708489
Loss in iteration 147 : 3.63830305967722
Loss in iteration 148 : 4.007127950046714
Loss in iteration 149 : 3.63718502929003
Loss in iteration 150 : 4.004868625141067
Loss in iteration 151 : 3.636143681331626
Loss in iteration 152 : 4.0030051010978065
Loss in iteration 153 : 3.6354597244382374
Loss in iteration 154 : 4.00177496999299
Loss in iteration 155 : 3.6352354570448386
Loss in iteration 156 : 4.001161704928692
Loss in iteration 157 : 3.635399462260984
Loss in iteration 158 : 4.0009527916439245
Loss in iteration 159 : 3.6357667405780254
Loss in iteration 160 : 4.000849895221243
Loss in iteration 161 : 3.6361238976238677
Loss in iteration 162 : 4.000585091298272
Loss in iteration 163 : 3.636306204418902
Loss in iteration 164 : 4.0000035205196784
Loss in iteration 165 : 3.636242610770858
Loss in iteration 166 : 3.999091780992438
Loss in iteration 167 : 3.635960453639677
Loss in iteration 168 : 3.997953784100475
Loss in iteration 169 : 3.6355566542702746
Loss in iteration 170 : 3.9967528521968223
Loss in iteration 171 : 3.6351516734797706
Loss in iteration 172 : 3.9956458990759076
Loss in iteration 173 : 3.6348447087132914
Loss in iteration 174 : 3.9947327222894646
Loss in iteration 175 : 3.634684549848176
Loss in iteration 176 : 3.994033940870412
Loss in iteration 177 : 3.634662678332169
Loss in iteration 178 : 3.993499244427579
Loss in iteration 179 : 3.6347266908176907
Loss in iteration 180 : 3.99303745269124
Loss in iteration 181 : 3.63480577401205
Loss in iteration 182 : 3.992554316387208
Loss in iteration 183 : 3.6348374898845686
Loss in iteration 184 : 3.991984066988771
Loss in iteration 185 : 3.634786718602112
Loss in iteration 186 : 3.9913054608102803
Loss in iteration 187 : 3.6346519750047666
Loss in iteration 188 : 3.990540013707918
Loss in iteration 189 : 3.634459440360913
Loss in iteration 190 : 3.989736491774502
Loss in iteration 191 : 3.6342490522361275
Loss in iteration 192 : 3.9889494711870612
Loss in iteration 193 : 3.6340587692186856
Loss in iteration 194 : 3.9882201950381946
Loss in iteration 195 : 3.633912543279037
Loss in iteration 196 : 3.9875655855359957
Loss in iteration 197 : 3.6338152482702464
Loss in iteration 198 : 3.9869774476794273
Loss in iteration 199 : 3.633754898096273
Loss in iteration 200 : 3.9864301478868036
Loss in iteration 201 : 3.633710012596359
Loss in iteration 202 : 3.9858925765381263
Loss in iteration 203 : 3.6336586878872836
Loss in iteration 204 : 3.9853395833635497
Loss in iteration 205 : 3.633586018224713
Loss in iteration 206 : 3.9847591738466908
Loss in iteration 207 : 3.6334877110888146
Loss in iteration 208 : 3.984153886777557
Loss in iteration 209 : 3.633369427356408
Loss in iteration 210 : 3.983537026193464
Loss in iteration 211 : 3.6332429007756017
Loss in iteration 212 : 3.982926013309223
Loss in iteration 213 : 3.6331207662322824
Loss in iteration 214 : 3.982335648728587
Loss in iteration 215 : 3.6330120865110698
Loss in iteration 216 : 3.9817735647166517
Loss in iteration 217 : 3.632919949843176
Loss in iteration 218 : 3.981238973323167
Loss in iteration 219 : 3.632841551671965
Loss in iteration 220 : 3.980724497873537
Loss in iteration 221 : 3.6327702589332023
Loss in iteration 222 : 3.980219879871272
Loss in iteration 223 : 3.632698581834541
Loss in iteration 224 : 3.979715952397485
Loss in iteration 225 : 3.6326208751463747
Loss in iteration 226 : 3.979207487433036
Loss in iteration 227 : 3.632534905194475
Loss in iteration 228 : 3.9786941695066833
Loss in iteration 229 : 3.632441965866964
Loss in iteration 230 : 3.9781797233288407
Loss in iteration 231 : 3.632345778067556
Loss in iteration 232 : 3.9776698418295737
Loss in iteration 233 : 3.6322507747201644
Loss in iteration 234 : 3.9771698436308007
Loss in iteration 235 : 3.632160467796509
Loss in iteration 236 : 3.9766829037873466
Loss in iteration 237 : 3.632076434641516
Loss in iteration 238 : 3.9762093473790725
Loss in iteration 239 : 3.631998149853246
Loss in iteration 240 : 3.975747042964431
Loss in iteration 241 : 3.6319235599304376
Loss in iteration 242 : 3.97529255351535
Loss in iteration 243 : 3.6318500655667174
Loss in iteration 244 : 3.9748425107125014
Loss in iteration 245 : 3.631775501435579
Loss in iteration 246 : 3.9743947039986978
Loss in iteration 247 : 3.631698781601896
Loss in iteration 248 : 3.973948569077891
Loss in iteration 249 : 3.631620055023391
Loss in iteration 250 : 3.9735050244228223
Loss in iteration 251 : 3.631540411618165
Loss in iteration 252 : 3.9730658358038746
Loss in iteration 253 : 3.6314613246366534
Loss in iteration 254 : 3.972632815229196
Loss in iteration 255 : 3.631384069645333
Loss in iteration 256 : 3.972207158814421
Loss in iteration 257 : 3.631309322795779
Loss in iteration 258 : 3.9717891228601734
Loss in iteration 259 : 3.6312370414227555
Loss in iteration 260 : 3.971378084197444
Loss in iteration 261 : 3.6311666140478156
Loss in iteration 262 : 3.970972890944023
Loss in iteration 263 : 3.631097177205192
Loss in iteration 264 : 3.97057232840152
Loss in iteration 265 : 3.631027958824667
Loss in iteration 266 : 3.970175518671893
Loss in iteration 267 : 3.630958525317044
Loss in iteration 268 : 3.969782129610484
Loss in iteration 269 : 3.6308888657087546
Loss in iteration 270 : 3.969392357560246
Loss in iteration 271 : 3.6308193146342127
Loss in iteration 272 : 3.969006732545282
Loss in iteration 273 : 3.6307503709516906
Loss in iteration 274 : 3.968625846261511
Loss in iteration 275 : 3.6306824938926012
Loss in iteration 276 : 3.968250110725881
Loss in iteration 277 : 3.630615950961287
Loss in iteration 278 : 3.9678796245719177
Loss in iteration 279 : 3.630550760083477
Loss in iteration 280 : 3.967514172527396
Loss in iteration 281 : 3.630486728157598
Loss in iteration 282 : 3.967153333102672
Loss in iteration 283 : 3.6304235546614843
Loss in iteration 284 : 3.9667966371019965
Loss in iteration 285 : 3.630360952529036
Loss in iteration 286 : 3.966443712963869
Loss in iteration 287 : 3.630298741576053
Loss in iteration 288 : 3.966094371546812
Loss in iteration 289 : 3.6302368876091258
Loss in iteration 290 : 3.965748612728066
Loss in iteration 291 : 3.6301754840773635
Loss in iteration 292 : 3.9654065663752345
Loss in iteration 293 : 3.6301146934624215
Loss in iteration 294 : 3.965068400423508
Loss in iteration 295 : 3.6300546761900097
Loss in iteration 296 : 3.96473423389778
Loss in iteration 297 : 3.6299955338783336
Loss in iteration 298 : 3.96440408384522
Loss in iteration 299 : 3.6299372837257566
Loss in iteration 300 : 3.96407785798305
Loss in iteration 301 : 3.6298798669353247
Loss in iteration 302 : 3.963755386890207
Loss in iteration 303 : 3.629823181799506
Loss in iteration 304 : 3.9634364771224924
Loss in iteration 305 : 3.629767125346174
Loss in iteration 306 : 3.9631209629526873
Loss in iteration 307 : 3.6297116275452073
Loss in iteration 308 : 3.9628087391393256
Loss in iteration 309 : 3.6296566676729727
Loss in iteration 310 : 3.962499767005693
Loss in iteration 311 : 3.629602270559722
Loss in iteration 312 : 3.9621940567801075
Loss in iteration 313 : 3.6295484878211974
Loss in iteration 314 : 3.9618916367801025
Loss in iteration 315 : 3.6294953733957347
Loss in iteration 316 : 3.961592522558225
Loss in iteration 317 : 3.6294429629118095
Loss in iteration 318 : 3.961296696655996
Loss in iteration 319 : 3.6293912632871783
Loss in iteration 320 : 3.9610041039339814
Loss in iteration 321 : 3.629340254216369
Loss in iteration 322 : 3.9607146611288253
Loss in iteration 323 : 3.6292898987841147
Loss in iteration 324 : 3.9604282746325596
Loss in iteration 325 : 3.6292401578136806
Loss in iteration 326 : 3.9601448587901427
Loss in iteration 327 : 3.6291910022891933
Loss in iteration 328 : 3.9598643482934084
Loss in iteration 329 : 3.6291424199290403
Loss in iteration 330 : 3.9595867015075457
Loss in iteration 331 : 3.629094414750396
Loss in iteration 332 : 3.959311895297393
Loss in iteration 333 : 3.6290470011061178
Loss in iteration 334 : 3.9590399147494852
Loss in iteration 335 : 3.629000195303048
Loss in iteration 336 : 3.9587707423026206
Loss in iteration 337 : 3.628954008155587
Loss in iteration 338 : 3.958504350146451
Loss in iteration 339 : 3.628908440868031
Loss in iteration 340 : 3.958240697883877
Loss in iteration 341 : 3.6288634850330204
Loss in iteration 342 : 3.957979735254366
Loss in iteration 343 : 3.628819125961502
Loss in iteration 344 : 3.9577214080052956
Loss in iteration 345 : 3.6287753475570796
Loss in iteration 346 : 3.957465664274665
Loss in iteration 347 : 3.628732136754025
Loss in iteration 348 : 3.9572124591744533
Loss in iteration 349 : 3.6286894860669787
Loss in iteration 350 : 3.9569617563269337
Loss in iteration 351 : 3.6286473937299624
Loss in iteration 352 : 3.9567135263979374
Loss in iteration 353 : 3.628605861836145
Loss in iteration 354 : 3.9564677437014377
Loss in iteration 355 : 3.6285648935031003
Loss in iteration 356 : 3.9562243824124166
Loss in iteration 357 : 3.6285244902303364
Loss in iteration 358 : 3.9559834137672008
Loss in iteration 359 : 3.6284846503265595
Loss in iteration 360 : 3.955744805025836
Loss in iteration 361 : 3.628445368746836
Loss in iteration 362 : 3.9555085202137255
Loss in iteration 363 : 3.6284066381268176
Loss in iteration 364 : 3.9552745220411825
Loss in iteration 365 : 3.628368450427628
Loss in iteration 366 : 3.9550427741066523
Loss in iteration 367 : 3.6283307985053197
Loss in iteration 368 : 3.9548132425626092
Loss in iteration 369 : 3.6282936770762437
Loss in iteration 370 : 3.954585896765605
Loss in iteration 371 : 3.628257082859328
Loss in iteration 372 : 3.954360708875526
Loss in iteration 373 : 3.6282210140033047
Loss in iteration 374 : 3.9541376527391323
Loss in iteration 375 : 3.6281854691333892
Loss in iteration 376 : 3.9539167025768225
Loss in iteration 377 : 3.6281504464202006
Loss in iteration 378 : 3.95369783196057
Loss in iteration 379 : 3.6281159429881704
Loss in iteration 380 : 3.953481013376551
Loss in iteration 381 : 3.628081954803482
Loss in iteration 382 : 3.9532662184081184
Loss in iteration 383 : 3.6280484769880563
Loss in iteration 384 : 3.953053418352831
Loss in iteration 385 : 3.6280155043692055
Loss in iteration 386 : 3.9528425849734714
Loss in iteration 387 : 3.627983032029895
Loss in iteration 388 : 3.9526336910939137
Loss in iteration 389 : 3.627951055668751
Loss in iteration 390 : 3.952426710860104
Loss in iteration 391 : 3.627919571682066
Loss in iteration 392 : 3.9522216196371764
Loss in iteration 393 : 3.627888576992894
Loss in iteration 394 : 3.952018393644866
Loss in iteration 395 : 3.627858068735015
Loss in iteration 396 : 3.9518170095049228
Loss in iteration 397 : 3.6278280439292065
Loss in iteration 398 : 3.951617443871043
Loss in iteration 399 : 3.627798499265408
Loss in iteration 400 : 3.9514196732507108
Testing accuracy  of updater 2 on alg 0 with rate 7.0 = 0.7665, training accuracy 0.7665, time elapsed: 8322 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 1.6610396359867332
Loss in iteration 3 : 15.521226478266215
Loss in iteration 4 : 0.9449563731583854
Loss in iteration 5 : 5.11345259688148
Loss in iteration 6 : 11.049877114601035
Loss in iteration 7 : 1.1696453906194266
Loss in iteration 8 : 6.150366853160022
Loss in iteration 9 : 3.539657283441941
Loss in iteration 10 : 4.535556727803108
Loss in iteration 11 : 2.7210345298210026
Loss in iteration 12 : 2.863188066328786
Loss in iteration 13 : 2.128647519011896
Loss in iteration 14 : 2.036626794763684
Loss in iteration 15 : 1.7372865181314963
Loss in iteration 16 : 1.6487895119604523
Loss in iteration 17 : 1.5752916471386202
Loss in iteration 18 : 1.5399273804547353
Loss in iteration 19 : 1.5112620603713693
Loss in iteration 20 : 1.4844061364726644
Loss in iteration 21 : 1.4543417245188217
Loss in iteration 22 : 1.419179125578382
Loss in iteration 23 : 1.3795717522311879
Loss in iteration 24 : 1.3361150859036497
Loss in iteration 25 : 1.2897586291947982
Loss in iteration 26 : 1.24140518106927
Loss in iteration 27 : 1.1920452353980253
Loss in iteration 28 : 1.142881208305224
Loss in iteration 29 : 1.0962022989912834
Loss in iteration 30 : 1.0591608927896259
Loss in iteration 31 : 1.0536162587714253
Loss in iteration 32 : 1.189945855314744
Loss in iteration 33 : 1.7606501908065275
Loss in iteration 34 : 3.77968799545207
Loss in iteration 35 : 3.60364298375466
Loss in iteration 36 : 4.301682285810892
Loss in iteration 37 : 3.3477520417631004
Loss in iteration 38 : 3.90808575645424
Loss in iteration 39 : 3.1990574509035707
Loss in iteration 40 : 3.3233707662624337
Loss in iteration 41 : 2.860371483572174
Loss in iteration 42 : 2.7806699886188753
Loss in iteration 43 : 2.427339714069084
Loss in iteration 44 : 2.329309333043298
Loss in iteration 45 : 2.0868124383040594
Loss in iteration 46 : 2.023957459501423
Loss in iteration 47 : 1.8767160333047046
Loss in iteration 48 : 1.8696824641332277
Loss in iteration 49 : 1.7708015694073953
Loss in iteration 50 : 1.8329850790163777
Loss in iteration 51 : 1.7627492401553346
Loss in iteration 52 : 1.9114175251440202
Loss in iteration 53 : 1.8383548878831781
Loss in iteration 54 : 2.0938266547403184
Loss in iteration 55 : 1.9836910419074159
Loss in iteration 56 : 2.335016632065073
Loss in iteration 57 : 2.129502312392022
Loss in iteration 58 : 2.5099003487853246
Loss in iteration 59 : 2.1998812979987887
Loss in iteration 60 : 2.5488388120221916
Loss in iteration 61 : 2.206923707842633
Loss in iteration 62 : 2.496804241388888
Loss in iteration 63 : 2.175298433995632
Loss in iteration 64 : 2.4000437014559464
Loss in iteration 65 : 2.119550477072454
Loss in iteration 66 : 2.2923210024602487
Loss in iteration 67 : 2.0581458025850106
Loss in iteration 68 : 2.2026809076952314
Loss in iteration 69 : 2.0107447160068572
Loss in iteration 70 : 2.150577989623438
Loss in iteration 71 : 1.9909614120843175
Loss in iteration 72 : 2.144358945727202
Loss in iteration 73 : 2.0029746335951866
Loss in iteration 74 : 2.1814977896229943
Loss in iteration 75 : 2.0419800588346706
Loss in iteration 76 : 2.2482987403783916
Loss in iteration 77 : 2.0936570685714906
Loss in iteration 78 : 2.320326481537144
Loss in iteration 79 : 2.138241859840693
Loss in iteration 80 : 2.3717802044421434
Loss in iteration 81 : 2.1624656896755923
Loss in iteration 82 : 2.39025420936751
Loss in iteration 83 : 2.164362344550852
Loss in iteration 84 : 2.3785979621455553
Loss in iteration 85 : 2.1487165065296376
Loss in iteration 86 : 2.347293400002888
Loss in iteration 87 : 2.122890597140747
Loss in iteration 88 : 2.308538040531573
Loss in iteration 89 : 2.0948021806991792
Loss in iteration 90 : 2.2731387307860795
Loss in iteration 91 : 2.0716244194979954
Loss in iteration 92 : 2.248976697075741
Loss in iteration 93 : 2.0583475523277475
Loss in iteration 94 : 2.2400073458987673
Loss in iteration 95 : 2.056692606886282
Loss in iteration 96 : 2.2457738210249922
Loss in iteration 97 : 2.065017690028783
Loss in iteration 98 : 2.261902520366325
Loss in iteration 99 : 2.079124195857773
Loss in iteration 100 : 2.2816598078434174
Loss in iteration 101 : 2.093826658437984
Loss in iteration 102 : 2.2983490571449376
Loss in iteration 103 : 2.1048573291798838
Loss in iteration 104 : 2.307632928841704
Loss in iteration 105 : 2.1100972960018525
Loss in iteration 106 : 2.308505992109831
Loss in iteration 107 : 2.109644979753978
Loss in iteration 108 : 2.3027131249095993
Loss in iteration 109 : 2.105143812344776
Loss in iteration 110 : 2.2934399959979284
Loss in iteration 111 : 2.0989201196877434
Loss in iteration 112 : 2.284033651576662
Loss in iteration 113 : 2.0932042120210563
Loss in iteration 114 : 2.2770719776778923
Loss in iteration 115 : 2.089554165481907
Loss in iteration 116 : 2.2738561787880283
Loss in iteration 117 : 2.0885662324883505
Loss in iteration 118 : 2.2743250939818513
Loss in iteration 119 : 2.0899023348887886
Loss in iteration 120 : 2.2773424476033868
Loss in iteration 121 : 2.092582605711601
Loss in iteration 122 : 2.2812397348892075
Loss in iteration 123 : 2.095420274020869
Loss in iteration 124 : 2.2844298747394522
Loss in iteration 125 : 2.097438768079691
Loss in iteration 126 : 2.2858848755801837
Loss in iteration 127 : 2.0981335556611747
Loss in iteration 128 : 2.285337078432103
Loss in iteration 129 : 2.0975246302546795
Loss in iteration 130 : 2.2831939066440907
Loss in iteration 131 : 2.0960352210519115
Loss in iteration 132 : 2.2802646352257
Loss in iteration 133 : 2.0942783837502548
Loss in iteration 134 : 2.27743184112134
Loss in iteration 135 : 2.0928350437546084
Loss in iteration 136 : 2.2753760151601936
Loss in iteration 137 : 2.0920870077757976
Loss in iteration 138 : 2.274416811021596
Loss in iteration 139 : 2.0921403869541475
Loss in iteration 140 : 2.274489678954078
Loss in iteration 141 : 2.0928443892137025
Loss in iteration 142 : 2.2752384660382647
Loss in iteration 143 : 2.0938830925697896
Loss in iteration 144 : 2.27617662178949
Loss in iteration 145 : 2.0948999500593164
Loss in iteration 146 : 2.2768570541667112
Loss in iteration 147 : 2.095611093153987
Loss in iteration 148 : 2.2769972212933496
Loss in iteration 149 : 2.095874570967919
Loss in iteration 150 : 2.2765293600510987
Loss in iteration 151 : 2.0957030981139804
Loss in iteration 152 : 2.2755757173831825
Loss in iteration 153 : 2.0952284091254065
Loss in iteration 154 : 2.2743725696316632
Loss in iteration 155 : 2.094638365780773
Loss in iteration 156 : 2.273177152052591
Loss in iteration 157 : 2.0941111909346346
Loss in iteration 158 : 2.272188653499874
Loss in iteration 159 : 2.093766508892196
Loss in iteration 160 : 2.2715031183827166
Loss in iteration 161 : 2.0936437708629665
Loss in iteration 162 : 2.271108026496112
Loss in iteration 163 : 2.093708603786699
Loss in iteration 164 : 2.270909814498186
Loss in iteration 165 : 2.0938794551100064
Loss in iteration 166 : 2.270779478715905
Loss in iteration 167 : 2.0940624718221343
Loss in iteration 168 : 2.270599019164897
Loss in iteration 169 : 2.0941824888786997
Loss in iteration 170 : 2.270294621972206
Loss in iteration 171 : 2.09420161772156
Loss in iteration 172 : 2.2698493269622495
Loss in iteration 173 : 2.094122457148442
Loss in iteration 174 : 2.269295695783859
Loss in iteration 175 : 2.0939782705819807
Loss in iteration 176 : 2.268695005961863
Loss in iteration 177 : 2.0938159360728696
Loss in iteration 178 : 2.268112191422465
Loss in iteration 179 : 2.0936784070419514
Loss in iteration 180 : 2.2675950329656334
Loss in iteration 181 : 2.0935921148704058
Loss in iteration 182 : 2.2671629840808603
Loss in iteration 183 : 2.0935620970164903
Loss in iteration 184 : 2.2668069773401993
Loss in iteration 185 : 2.093574721120988
Loss in iteration 186 : 2.2664979790277284
Loss in iteration 187 : 2.093605606731985
Loss in iteration 188 : 2.266199888028904
Loss in iteration 189 : 2.0936292597289805
Loss in iteration 190 : 2.265881963070331
Loss in iteration 191 : 2.0936271212071973
Loss in iteration 192 : 2.2655270882935774
Loss in iteration 193 : 2.093591887257925
Loss in iteration 194 : 2.265134210100555
Loss in iteration 195 : 2.093527534134044
Loss in iteration 196 : 2.2647153999048095
Loss in iteration 197 : 2.0934459125325158
Loss in iteration 198 : 2.2642895424902303
Loss in iteration 199 : 2.0933616442408725
Loss in iteration 200 : 2.2638752610885002
Loss in iteration 201 : 2.0932872139166947
Loss in iteration 202 : 2.2634853781412314
Loss in iteration 203 : 2.0932296996269706
Loss in iteration 204 : 2.2631242671707885
Loss in iteration 205 : 2.093189792799096
Loss in iteration 206 : 2.2627882957421317
Loss in iteration 207 : 2.0931629327314085
Loss in iteration 208 : 2.2624685842567933
Loss in iteration 209 : 2.0931417779766135
Loss in iteration 210 : 2.262154765804527
Loss in iteration 211 : 2.0931189927325002
Loss in iteration 212 : 2.2618384015082
Loss in iteration 213 : 2.0930894417505415
Loss in iteration 214 : 2.2615150893620894
Loss in iteration 215 : 2.093051254254941
Loss in iteration 216 : 2.2611849015268
Loss in iteration 217 : 2.0930056729126494
Loss in iteration 218 : 2.2608513696005583
Loss in iteration 219 : 2.0929559906623907
Loss in iteration 220 : 2.260519633350077
Loss in iteration 221 : 2.0929060921313964
Loss in iteration 222 : 2.2601944938540948
Loss in iteration 223 : 2.092859128039113
Loss in iteration 224 : 2.2598789852532573
Loss in iteration 225 : 2.092816699149875
Loss in iteration 226 : 2.259573791925467
Loss in iteration 227 : 2.0927786915301914
Loss in iteration 228 : 2.2592775111694348
Loss in iteration 229 : 2.0927436759753864
Loss in iteration 230 : 2.2589875030890654
Loss in iteration 231 : 2.092709630005301
Loss in iteration 232 : 2.258700943772589
Loss in iteration 233 : 2.092674692442062
Loss in iteration 234 : 2.258415715766505
Loss in iteration 235 : 2.09263771108831
Loss in iteration 236 : 2.258130896050379
Loss in iteration 237 : 2.0925984575050443
Loss in iteration 238 : 2.2578467751794555
Loss in iteration 239 : 2.092557511520906
Loss in iteration 240 : 2.257564499027034
Loss in iteration 241 : 2.092515919252025
Loss in iteration 242 : 2.257285521483051
Loss in iteration 243 : 2.092474776688905
Loss in iteration 244 : 2.2570110754436192
Loss in iteration 245 : 2.092434882436727
Loss in iteration 246 : 2.2567418208337484
Loss in iteration 247 : 2.092396552343617
Loss in iteration 248 : 2.2564777411935864
Loss in iteration 249 : 2.0923596199091787
Loss in iteration 250 : 2.256218269285063
Loss in iteration 251 : 2.0923235843839256
Loss in iteration 252 : 2.2559625559601573
Loss in iteration 253 : 2.092287830952895
Loss in iteration 254 : 2.2557097703216575
Loss in iteration 255 : 2.092251840812321
Loss in iteration 256 : 2.2554593327615216
Loss in iteration 257 : 2.092215328944457
Loss in iteration 258 : 2.25521102339963
Loss in iteration 259 : 2.092178282357894
Loss in iteration 260 : 2.2549649585419593
Loss in iteration 261 : 2.092140907668983
Loss in iteration 262 : 2.2547214697593003
Loss in iteration 263 : 2.0921035228283644
Loss in iteration 264 : 2.2544809428488573
Loss in iteration 265 : 2.0920664376871496
Loss in iteration 266 : 2.2542436743286896
Loss in iteration 267 : 2.0920298622297997
Loss in iteration 268 : 2.254009785733842
Loss in iteration 269 : 2.0919938647409873
Loss in iteration 270 : 2.253779209859159
Loss in iteration 271 : 2.091958382222679
Loss in iteration 272 : 2.253551738065121
Loss in iteration 273 : 2.0919232688652505
Loss in iteration 274 : 2.253327101288721
Loss in iteration 275 : 2.0918883596614104
Loss in iteration 276 : 2.253105052764709
Loss in iteration 277 : 2.091853526393321
Loss in iteration 278 : 2.2528854266272047
Loss in iteration 279 : 2.0918187103382633
Loss in iteration 280 : 2.2526681593871127
Loss in iteration 281 : 2.0917839264892537
Loss in iteration 282 : 2.25245327543575
Loss in iteration 283 : 2.091749243962375
Loss in iteration 284 : 2.252240848575793
Loss in iteration 285 : 2.091714753640709
Loss in iteration 286 : 2.252030956473007
Loss in iteration 287 : 2.0916805357455015
Loss in iteration 288 : 2.25182364358755
Loss in iteration 289 : 2.091646637407504
Loss in iteration 290 : 2.2516189022880733
Loss in iteration 291 : 2.0916130651265465
Loss in iteration 292 : 2.251416674219791
Loss in iteration 293 : 2.09157979138808
Loss in iteration 294 : 2.251216867311286
Loss in iteration 295 : 2.091546770476268
Loss in iteration 296 : 2.2510193799303844
Loss in iteration 297 : 2.091513956701716
Loss in iteration 298 : 2.2508241232480004
Loss in iteration 299 : 2.091481318912357
Loss in iteration 300 : 2.250631035271167
Loss in iteration 301 : 2.0914488475663555
Loss in iteration 302 : 2.2504400839284164
Loss in iteration 303 : 2.091416553707991
Loss in iteration 304 : 2.250251260509249
Loss in iteration 305 : 2.0913844618212685
Loss in iteration 306 : 2.2500645674642
Loss in iteration 307 : 2.091352600017174
Loss in iteration 308 : 2.2498800054819936
Loss in iteration 309 : 2.0913209911195216
Loss in iteration 310 : 2.2496975639649057
Loss in iteration 311 : 2.091289647201671
Loss in iteration 312 : 2.2495172171223436
Loss in iteration 313 : 2.0912585685360665
Loss in iteration 314 : 2.2493389257113376
Loss in iteration 315 : 2.0912277463606923
Loss in iteration 316 : 2.249162642707111
Loss in iteration 317 : 2.0911971678155523
Loss in iteration 318 : 2.2489883203332868
Loss in iteration 319 : 2.0911668210763614
Loss in iteration 320 : 2.248815915997736
Loss in iteration 321 : 2.0911366990635853
Loss in iteration 322 : 2.2486453955296946
Loss in iteration 323 : 2.09110680088244
Loss in iteration 324 : 2.2484767332813225
Loss in iteration 325 : 2.091077131026255
Loss in iteration 326 : 2.248309909716403
Loss in iteration 327 : 2.091047697059664
Loss in iteration 328 : 2.248144907755397
Loss in iteration 329 : 2.0910185068184024
Loss in iteration 330 : 2.247981709265866
Loss in iteration 331 : 2.090989566095334
Loss in iteration 332 : 2.247820292752583
Loss in iteration 333 : 2.090960877429783
Loss in iteration 334 : 2.247660632710701
Loss in iteration 335 : 2.0909324401465184
Loss in iteration 336 : 2.2475027004939068
Loss in iteration 337 : 2.090904251372813
Loss in iteration 338 : 2.2473464661111247
Loss in iteration 339 : 2.0908763075153383
Loss in iteration 340 : 2.247191900199538
Loss in iteration 341 : 2.090848605643056
Loss in iteration 342 : 2.247038975523039
Loss in iteration 343 : 2.090821144366515
Loss in iteration 344 : 2.246887667627544
Loss in iteration 345 : 2.0907939240445246
Loss in iteration 346 : 2.2467379546220054
Loss in iteration 347 : 2.090766946392837
Loss in iteration 348 : 2.2465898163311677
Loss in iteration 349 : 2.0907402137392035
Loss in iteration 350 : 2.2464432332099133
Loss in iteration 351 : 2.090713728228657
Loss in iteration 352 : 2.246298185402206
Loss in iteration 353 : 2.090687491235007
Loss in iteration 354 : 2.2461546522033724
Loss in iteration 355 : 2.090661503118564
Loss in iteration 356 : 2.246012612006416
Loss in iteration 357 : 2.0906357633333226
Loss in iteration 358 : 2.2458720426464933
Loss in iteration 359 : 2.090610270778825
Loss in iteration 360 : 2.2457329219523037
Loss in iteration 361 : 2.0905850242372304
Loss in iteration 362 : 2.2455952282889937
Loss in iteration 363 : 2.090560022743428
Loss in iteration 364 : 2.245458940924518
Loss in iteration 365 : 2.0905352657877856
Loss in iteration 366 : 2.2453240401409396
Loss in iteration 367 : 2.0905107533242173
Loss in iteration 368 : 2.2451905071075715
Loss in iteration 369 : 2.0904864856217227
Loss in iteration 370 : 2.245058323602979
Loss in iteration 371 : 2.0904624630383304
Loss in iteration 372 : 2.2449274717016143
Loss in iteration 373 : 2.090438685803936
Loss in iteration 374 : 2.24479793352786
Loss in iteration 375 : 2.090415153877567
Loss in iteration 376 : 2.2446696911378132
Loss in iteration 377 : 2.090391866908111
Loss in iteration 378 : 2.244542726537094
Loss in iteration 379 : 2.090368824289347
Loss in iteration 380 : 2.244417021799311
Loss in iteration 381 : 2.0903460252726256
Loss in iteration 382 : 2.244292559225548
Loss in iteration 383 : 2.0903234690905745
Loss in iteration 384 : 2.2441693214854848
Loss in iteration 385 : 2.0903011550511432
Loss in iteration 386 : 2.2440472916983283
Loss in iteration 387 : 2.0902790825793183
Loss in iteration 388 : 2.2439264534396406
Loss in iteration 389 : 2.090257251204397
Loss in iteration 390 : 2.243806790685642
Loss in iteration 391 : 2.090235660508618
Loss in iteration 392 : 2.2436882877239768
Loss in iteration 393 : 2.090214310061017
Loss in iteration 394 : 2.243570929064055
Loss in iteration 395 : 2.0901931993606055
Loss in iteration 396 : 2.2434546993736078
Loss in iteration 397 : 2.0901723278046846
Loss in iteration 398 : 2.2433395834543637
Loss in iteration 399 : 2.090151694687337
Loss in iteration 400 : 2.243225566254839
Testing accuracy  of updater 2 on alg 0 with rate 4.0 = 0.766875, training accuracy 0.766875, time elapsed: 7796 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.7390243244034626
Loss in iteration 3 : 1.5461235979835175
Loss in iteration 4 : 2.4961351716319125
Loss in iteration 5 : 0.5491945676139464
Loss in iteration 6 : 0.5116117840937401
Loss in iteration 7 : 0.5594905327553612
Loss in iteration 8 : 0.6994699929099957
Loss in iteration 9 : 0.9559049119020779
Loss in iteration 10 : 1.0668534021602731
Loss in iteration 11 : 0.808824983537983
Loss in iteration 12 : 0.7371409817547577
Loss in iteration 13 : 0.6752825726820127
Loss in iteration 14 : 0.6109967122349372
Loss in iteration 15 : 0.5707663607584027
Loss in iteration 16 : 0.543075121035778
Loss in iteration 17 : 0.5282697556481837
Loss in iteration 18 : 0.5200434020403024
Loss in iteration 19 : 0.5156113921810067
Loss in iteration 20 : 0.5126013245511593
Loss in iteration 21 : 0.5099866661947244
Loss in iteration 22 : 0.5073499834642364
Loss in iteration 23 : 0.504566829433719
Loss in iteration 24 : 0.5016376127338616
Loss in iteration 25 : 0.4985886438503925
Loss in iteration 26 : 0.4954598727295745
Loss in iteration 27 : 0.4922913044277051
Loss in iteration 28 : 0.4891266471024051
Loss in iteration 29 : 0.48601204513431456
Loss in iteration 30 : 0.4829957859599715
Loss in iteration 31 : 0.4801269729000892
Loss in iteration 32 : 0.47745189603337895
Loss in iteration 33 : 0.4750119075352789
Loss in iteration 34 : 0.4728386820019561
Loss in iteration 35 : 0.4709530630044385
Loss in iteration 36 : 0.46936168196075406
Loss in iteration 37 : 0.46805877035351534
Loss in iteration 38 : 0.46702534225380454
Loss in iteration 39 : 0.46623403452433104
Loss in iteration 40 : 0.46564976296017596
Loss in iteration 41 : 0.46523579018915506
Loss in iteration 42 : 0.4649538811014268
Loss in iteration 43 : 0.4647698641185812
Loss in iteration 44 : 0.4646523023446003
Loss in iteration 45 : 0.4645770410084978
Loss in iteration 46 : 0.4645250676678275
Loss in iteration 47 : 0.46448695878670276
Loss in iteration 48 : 0.4644632103405315
Loss in iteration 49 : 0.4644725025335746
Loss in iteration 50 : 0.46456707268111813
Loss in iteration 51 : 0.4648597833013292
Loss in iteration 52 : 0.46561439367850477
Loss in iteration 53 : 0.46733371200598456
Loss in iteration 54 : 0.4712436551832381
Loss in iteration 55 : 0.47930623611989126
Loss in iteration 56 : 0.4967595508507202
Loss in iteration 57 : 0.5276572085585572
Loss in iteration 58 : 0.5867717004375643
Loss in iteration 59 : 0.650164560303307
Loss in iteration 60 : 0.7342256375699746
Loss in iteration 61 : 0.7343765515332784
Loss in iteration 62 : 0.7532407557680411
Loss in iteration 63 : 0.7028029576797065
Loss in iteration 64 : 0.6887390378255568
Loss in iteration 65 : 0.6448547639881952
Loss in iteration 66 : 0.6232080658500647
Loss in iteration 67 : 0.5904625848613482
Loss in iteration 68 : 0.5706431804272818
Loss in iteration 69 : 0.548749159105419
Loss in iteration 70 : 0.5342875686250125
Loss in iteration 71 : 0.5207888449437339
Loss in iteration 72 : 0.511423651325227
Loss in iteration 73 : 0.5034977485287155
Loss in iteration 74 : 0.4978302100906427
Loss in iteration 75 : 0.49330918129383294
Loss in iteration 76 : 0.49006088359656247
Loss in iteration 77 : 0.487610466079337
Loss in iteration 78 : 0.48595508427596934
Loss in iteration 79 : 0.48488430580482783
Loss in iteration 80 : 0.484450956273303
Loss in iteration 81 : 0.4845640855489104
Loss in iteration 82 : 0.48543983651906103
Loss in iteration 83 : 0.48701310841335316
Loss in iteration 84 : 0.48982281146528384
Loss in iteration 85 : 0.49364944439362013
Loss in iteration 86 : 0.4996906380670317
Loss in iteration 87 : 0.5070127680266758
Loss in iteration 88 : 0.5181619701814855
Loss in iteration 89 : 0.5298206251667814
Loss in iteration 90 : 0.5471955130557408
Loss in iteration 91 : 0.5610181468901625
Loss in iteration 92 : 0.5817627399628864
Loss in iteration 93 : 0.5907293836366093
Loss in iteration 94 : 0.6077158794506377
Loss in iteration 95 : 0.6058864261498703
Loss in iteration 96 : 0.614465844512486
Loss in iteration 97 : 0.6036152609291762
Loss in iteration 98 : 0.6047515874397354
Loss in iteration 99 : 0.5902758384529267
Loss in iteration 100 : 0.5870478779922684
Loss in iteration 101 : 0.5729236593284488
Loss in iteration 102 : 0.5680283855262608
Loss in iteration 103 : 0.5561369668952153
Loss in iteration 104 : 0.5513180791399116
Loss in iteration 105 : 0.5421774999590553
Loss in iteration 106 : 0.538320701767385
Loss in iteration 107 : 0.5317723690488467
Loss in iteration 108 : 0.5292074222002707
Loss in iteration 109 : 0.5248546649662553
Loss in iteration 110 : 0.523636099779327
Loss in iteration 111 : 0.5210702032117789
Loss in iteration 112 : 0.521161574063039
Loss in iteration 113 : 0.5200460037383705
Loss in iteration 114 : 0.5214111989372909
Loss in iteration 115 : 0.5214784234271799
Loss in iteration 116 : 0.5240944402432269
Loss in iteration 117 : 0.5250849053694812
Loss in iteration 118 : 0.5288892472649979
Loss in iteration 119 : 0.5304600845800344
Loss in iteration 120 : 0.5352607739545834
Loss in iteration 121 : 0.5369174255996118
Loss in iteration 122 : 0.5423255686450366
Loss in iteration 123 : 0.5434522095300653
Loss in iteration 124 : 0.5489127972814161
Loss in iteration 125 : 0.5489367505723408
Loss in iteration 126 : 0.5538798594432637
Loss in iteration 127 : 0.5524871311125694
Loss in iteration 128 : 0.5565153003175144
Loss in iteration 129 : 0.5537592103360665
Loss in iteration 130 : 0.5567472573779498
Loss in iteration 131 : 0.5529779570931721
Loss in iteration 132 : 0.5550433881505938
Loss in iteration 133 : 0.5507349223779546
Loss in iteration 134 : 0.5521341778881428
Loss in iteration 135 : 0.5477278078970361
Loss in iteration 136 : 0.5487517680788367
Loss in iteration 137 : 0.5445727051225678
Loss in iteration 138 : 0.5454787817701693
Loss in iteration 139 : 0.541719255161066
Loss in iteration 140 : 0.5427023104715708
Loss in iteration 141 : 0.5394410873520666
Loss in iteration 142 : 0.540630263624481
Loss in iteration 143 : 0.5378634419172501
Loss in iteration 144 : 0.5393310438452791
Loss in iteration 145 : 0.5370005175716651
Loss in iteration 146 : 0.5387736266581341
Loss in iteration 147 : 0.5367887739394743
Loss in iteration 148 : 0.5388593335596676
Loss in iteration 149 : 0.537112606184505
Loss in iteration 150 : 0.5394454230115113
Loss in iteration 151 : 0.5378244147988032
Loss in iteration 152 : 0.5403643833126631
Loss in iteration 153 : 0.5387626063848604
Loss in iteration 154 : 0.5414425665288581
Loss in iteration 155 : 0.5397695478942983
Loss in iteration 156 : 0.542519100504139
Loss in iteration 157 : 0.540708686928592
Loss in iteration 158 : 0.5434629638492122
Loss in iteration 159 : 0.5414779740800152
Loss in iteration 160 : 0.5441846916505754
Loss in iteration 161 : 0.5420166965622814
Loss in iteration 162 : 0.5446401626017885
Loss in iteration 163 : 0.5423046921191849
Loss in iteration 164 : 0.5448264471891073
Loss in iteration 165 : 0.5423553161723105
Loss in iteration 166 : 0.5447720983530294
Loss in iteration 167 : 0.5422050798275304
Loss in iteration 168 : 0.5445253164105159
Loss in iteration 169 : 0.5419030061932953
Loss in iteration 170 : 0.5441429802903919
Loss in iteration 171 : 0.5415018219356602
Loss in iteration 172 : 0.5436822650822853
Loss in iteration 173 : 0.5410518401674538
Loss in iteration 174 : 0.5431952499200675
Loss in iteration 175 : 0.540597379927439
Loss in iteration 176 : 0.5427260362930666
Loss in iteration 177 : 0.5401750387118355
Loss in iteration 178 : 0.5423095438602054
Loss in iteration 179 : 0.5398130420850851
Loss in iteration 180 : 0.5419711975844579
Loss in iteration 181 : 0.5395310705882492
Loss in iteration 182 : 0.5417269716139486
Loss in iteration 183 : 0.5393402395038388
Loss in iteration 184 : 0.5415835517545067
Loss in iteration 185 : 0.5392431646644733
Loss in iteration 186 : 0.5415386237991843
Loss in iteration 187 : 0.5392342281658834
Loss in iteration 188 : 0.5415814473897083
Loss in iteration 189 : 0.539300242238015
Loss in iteration 190 : 0.5416939241806097
Loss in iteration 191 : 0.5394216987771492
Loss in iteration 192 : 0.5418523199895559
Loss in iteration 193 : 0.5395746976090314
Loss in iteration 194 : 0.5420296694586616
Loss in iteration 195 : 0.5397334906551698
Loss in iteration 196 : 0.5421987095491347
Loss in iteration 197 : 0.5398733993300613
Loss in iteration 198 : 0.5423350039358457
Loss in iteration 199 : 0.5399737106286132
Loss in iteration 200 : 0.5424197939466843
Loss in iteration 201 : 0.5400200873014697
Loss in iteration 202 : 0.5424420944235923
Loss in iteration 203 : 0.540006073692801
Loss in iteration 204 : 0.542399663932865
Loss in iteration 205 : 0.5399334378061946
Loss in iteration 206 : 0.5422986921398925
Loss in iteration 207 : 0.5398113180426827
Loss in iteration 208 : 0.5421522998409971
Loss in iteration 209 : 0.5396543725615306
Loss in iteration 210 : 0.5419781654115322
Loss in iteration 211 : 0.539480297775968
Loss in iteration 212 : 0.5417957208558923
Loss in iteration 213 : 0.539307155399468
Loss in iteration 214 : 0.5416233833630251
Loss in iteration 215 : 0.5391509249661947
Loss in iteration 216 : 0.5414762210528387
Loss in iteration 217 : 0.5390236069617863
Loss in iteration 218 : 0.5413643301402781
Loss in iteration 219 : 0.5389320757596485
Loss in iteration 220 : 0.5412920614742043
Loss in iteration 221 : 0.5388777512924354
Loss in iteration 222 : 0.5412581029849112
Loss in iteration 223 : 0.5388570420529304
Loss in iteration 224 : 0.5412563138840654
Loss in iteration 225 : 0.5388624178782654
Loss in iteration 226 : 0.5412771213097873
Loss in iteration 227 : 0.5388839026984115
Loss in iteration 228 : 0.5413092334021965
Loss in iteration 229 : 0.5389107386633661
Loss in iteration 230 : 0.5413413988500408
Loss in iteration 231 : 0.5389329683056251
Loss in iteration 232 : 0.5413639563581994
Loss in iteration 233 : 0.5389427132740086
Loss in iteration 234 : 0.5413699689585424
Loss in iteration 235 : 0.5389349934752949
Loss in iteration 236 : 0.5413558205770005
Loss in iteration 237 : 0.5389080180370464
Loss in iteration 238 : 0.541321250409224
Loss in iteration 239 : 0.53886297196147
Loss in iteration 240 : 0.5412688943994415
Loss in iteration 241 : 0.5388034009545
Loss in iteration 242 : 0.5412034738973757
Loss in iteration 243 : 0.5387343473792909
Loss in iteration 244 : 0.5411308078949608
Loss in iteration 245 : 0.5386614065237382
Loss in iteration 246 : 0.541056825221381
Loss in iteration 247 : 0.5385898570113625
Loss in iteration 248 : 0.540986723247681
Loss in iteration 249 : 0.5385239811051952
Loss in iteration 250 : 0.5409243714261718
Loss in iteration 251 : 0.5384666412684987
Loss in iteration 252 : 0.5408720036010395
Loss in iteration 253 : 0.5384191293024567
Loss in iteration 254 : 0.540830192614171
Loss in iteration 255 : 0.5383812616293606
Loss in iteration 256 : 0.5407980610528496
Loss in iteration 257 : 0.5383516635598544
Loss in iteration 258 : 0.5407736564499405
Loss in iteration 259 : 0.5383281685494218
Loss in iteration 260 : 0.5407544085636257
Loss in iteration 261 : 0.5383082551893036
Loss in iteration 262 : 0.5407375892668049
Loss in iteration 263 : 0.5382894530549895
Loss in iteration 264 : 0.5407207093698896
Loss in iteration 265 : 0.5382696654573574
Loss in iteration 266 : 0.5407018076918221
Loss in iteration 267 : 0.538247378789408
Loss in iteration 268 : 0.5406796116692912
Loss in iteration 269 : 0.5382217505210922
Loss in iteration 270 : 0.5406535716190451
Loss in iteration 271 : 0.5381925873688567
Loss in iteration 272 : 0.5406237890411233
Loss in iteration 273 : 0.5381602391471151
Loss in iteration 274 : 0.5405908709345368
Loss in iteration 275 : 0.5381254410398683
Loss in iteration 276 : 0.5405557463629789
Loss in iteration 277 : 0.538089137654253
Loss in iteration 278 : 0.5405194792294171
Loss in iteration 279 : 0.538052317511774
Loss in iteration 280 : 0.5404831041675547
Loss in iteration 281 : 0.5380158785449896
Loss in iteration 282 : 0.5404475028650124
Loss in iteration 283 : 0.5379805358026652
Loss in iteration 284 : 0.5404133281598483
Loss in iteration 285 : 0.5379467737560979
Loss in iteration 286 : 0.5403809746083809
Loss in iteration 287 : 0.537914838633056
Loss in iteration 288 : 0.5403505879671232
Loss in iteration 289 : 0.5378847617354622
Loss in iteration 290 : 0.5403221025511125
Loss in iteration 291 : 0.5378564027860228
Loss in iteration 292 : 0.5402952945453954
Loss in iteration 293 : 0.5378295026130118
Loss in iteration 294 : 0.5402698404886656
Loss in iteration 295 : 0.5378037362724447
Loss in iteration 296 : 0.5402453725613987
Loss in iteration 297 : 0.5377787602896762
Loss in iteration 298 : 0.540221525229757
Loss in iteration 299 : 0.5377542504123712
Loss in iteration 300 : 0.5401979705838664
Loss in iteration 301 : 0.5377299286065804
Loss in iteration 302 : 0.5401744419373379
Loss in iteration 303 : 0.5377055797170867
Loss in iteration 304 : 0.5401507467298416
Loss in iteration 305 : 0.5376810591831502
Loss in iteration 306 : 0.5401267705101943
Loss in iteration 307 : 0.5376562935501188
Loss in iteration 308 : 0.5401024739337578
Loss in iteration 309 : 0.5376312754442658
Loss in iteration 310 : 0.5400778845166128
Loss in iteration 311 : 0.5376060544080954
Loss in iteration 312 : 0.5400530845786461
Loss in iteration 313 : 0.5375807247179613
Loss in iteration 314 : 0.5400281965508953
Loss in iteration 315 : 0.5375554111445853
Loss in iteration 316 : 0.5400033667058332
Loss in iteration 317 : 0.5375302536041912
Loss in iteration 318 : 0.5399787483938258
Loss in iteration 319 : 0.5375053917446685
Loss in iteration 320 : 0.5399544859692835
Loss in iteration 321 : 0.5374809506319912
Loss in iteration 322 : 0.5399307006693848
Loss in iteration 323 : 0.5374570287508758
Loss in iteration 324 : 0.5399074796732511
Loss in iteration 325 : 0.5374336894332997
Loss in iteration 326 : 0.5398848693597161
Loss in iteration 327 : 0.5374109565453709
Loss in iteration 328 : 0.5398628733869875
Loss in iteration 329 : 0.537388814812939
Loss in iteration 330 : 0.5398414556788291
Loss in iteration 331 : 0.5373672146117437
Loss in iteration 332 : 0.5398205478015187
Loss in iteration 333 : 0.5373460804805172
Loss in iteration 334 : 0.5398000596567518
Loss in iteration 335 : 0.5373253221357693
Loss in iteration 336 : 0.5397798919980024
Loss in iteration 337 : 0.5373048464608838
Loss in iteration 338 : 0.5397599490764108
Loss in iteration 339 : 0.5372845688640586
Loss in iteration 340 : 0.5397401497730935
Loss in iteration 341 : 0.5372644225611921
Loss in iteration 342 : 0.5397204358662573
Loss in iteration 343 : 0.5372443647097533
Loss in iteration 344 : 0.5397007765608653
Loss in iteration 345 : 0.5372243788309993
Loss in iteration 346 : 0.5396811689915215
Loss in iteration 347 : 0.5372044735221121
Loss in iteration 348 : 0.5396616349977746
Loss in iteration 349 : 0.5371846779845567
Loss in iteration 350 : 0.5396422149724768
Loss in iteration 351 : 0.5371650353002415
Loss in iteration 352 : 0.5396229599256204
Loss in iteration 353 : 0.537145594619064
Loss in iteration 354 : 0.5396039230482331
Loss in iteration 355 : 0.5371264034587026
Loss in iteration 356 : 0.5395851519990377
Loss in iteration 357 : 0.5371075011728692
Loss in iteration 358 : 0.5395666828997684
Loss in iteration 359 : 0.537088914358016
Loss in iteration 360 : 0.5395485366679177
Loss in iteration 361 : 0.5370706545999883
Loss in iteration 362 : 0.5395307179057622
Loss in iteration 363 : 0.5370527185769167
Loss in iteration 364 : 0.5395132161707747
Loss in iteration 365 : 0.5370350901938657
Loss in iteration 366 : 0.5394960091329545
Loss in iteration 367 : 0.5370177441748354
Loss in iteration 368 : 0.5394790669189266
Loss in iteration 369 : 0.537000650405347
Loss in iteration 370 : 0.5394623568673451
Loss in iteration 371 : 0.5369837783088695
Loss in iteration 372 : 0.5394458479696764
Loss in iteration 373 : 0.5369671006375907
Loss in iteration 374 : 0.5394295144200646
Loss in iteration 375 : 0.5369505962330254
Loss in iteration 376 : 0.5394133379110427
Loss in iteration 377 : 0.5369342515268114
Loss in iteration 378 : 0.5393973085470318
Loss in iteration 379 : 0.5369180607684987
Loss in iteration 380 : 0.5393814244656698
Loss in iteration 381 : 0.5369020251520149
Loss in iteration 382 : 0.5393656904281183
Loss in iteration 383 : 0.5368861511428604
Loss in iteration 384 : 0.5393501157446622
Loss in iteration 385 : 0.5368704483728528
Loss in iteration 386 : 0.5393347119362637
Loss in iteration 387 : 0.5368549274693367
Loss in iteration 388 : 0.5393194905024609
Loss in iteration 389 : 0.5368395981320184
Loss in iteration 390 : 0.5393044610867396
Loss in iteration 391 : 0.5368244676803227
Loss in iteration 392 : 0.5392896302224985
Loss in iteration 393 : 0.5368095401873209
Loss in iteration 394 : 0.539275000727385
Loss in iteration 395 : 0.5367948162118548
Loss in iteration 396 : 0.5392605717095682
Loss in iteration 397 : 0.5367802930538133
Loss in iteration 398 : 0.5392463390701759
Loss in iteration 399 : 0.5367659653981266
Loss in iteration 400 : 0.5392322963385713
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.785875, training accuracy 0.785875, time elapsed: 7324 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.697339624428007
Loss in iteration 3 : 0.8333624651029125
Loss in iteration 4 : 1.2523754556624467
Loss in iteration 5 : 0.7988378024504437
Loss in iteration 6 : 0.9002598800450431
Loss in iteration 7 : 0.7800503132563401
Loss in iteration 8 : 0.73098604430267
Loss in iteration 9 : 0.6202163710548
Loss in iteration 10 : 0.5604775993447793
Loss in iteration 11 : 0.5219917054729385
Loss in iteration 12 : 0.49948533560831077
Loss in iteration 13 : 0.49120716660669506
Loss in iteration 14 : 0.48876403545506514
Loss in iteration 15 : 0.48853331552739293
Loss in iteration 16 : 0.4885618717943911
Loss in iteration 17 : 0.4884306165884461
Loss in iteration 18 : 0.4880733297605272
Loss in iteration 19 : 0.48750873601255634
Loss in iteration 20 : 0.4867630367900231
Loss in iteration 21 : 0.4858593201023964
Loss in iteration 22 : 0.4848173201915059
Loss in iteration 23 : 0.4836560696799072
Loss in iteration 24 : 0.4823961363030247
Loss in iteration 25 : 0.4810602686401271
Loss in iteration 26 : 0.47967257721171025
Loss in iteration 27 : 0.47825703541005343
Loss in iteration 28 : 0.4768361216796118
Loss in iteration 29 : 0.4754300790676052
Loss in iteration 30 : 0.47405682982720215
Loss in iteration 31 : 0.47273227155562847
Loss in iteration 32 : 0.47147059103631517
Loss in iteration 33 : 0.47028433435729305
Loss in iteration 34 : 0.46918416600681473
Loss in iteration 35 : 0.4681784251815382
Loss in iteration 36 : 0.4672726740719268
Loss in iteration 37 : 0.46646941823902793
Loss in iteration 38 : 0.465768097378727
Loss in iteration 39 : 0.46516534744157767
Loss in iteration 40 : 0.4646554640093948
Loss in iteration 41 : 0.4642309693768684
Loss in iteration 42 : 0.46388319580459925
Loss in iteration 43 : 0.4636028262455415
Loss in iteration 44 : 0.46338036303931973
Loss in iteration 45 : 0.463206514343829
Loss in iteration 46 : 0.4630724965480537
Loss in iteration 47 : 0.4629702533799434
Loss in iteration 48 : 0.4628925943245237
Loss in iteration 49 : 0.4628332590037964
Loss in iteration 50 : 0.462786919715581
Loss in iteration 51 : 0.4627491389270476
Loss in iteration 52 : 0.46271629995919294
Loss in iteration 53 : 0.46268552675833097
Loss in iteration 54 : 0.4626546036151577
Loss in iteration 55 : 0.4626218999469023
Loss in iteration 56 : 0.4625863006390235
Loss in iteration 57 : 0.4625471399256641
Loss in iteration 58 : 0.46250413635606824
Loss in iteration 59 : 0.46245732735739364
Loss in iteration 60 : 0.46240700338117435
Loss in iteration 61 : 0.4623536429159224
Loss in iteration 62 : 0.462297850411648
Loss in iteration 63 : 0.4622402993493641
Loss in iteration 64 : 0.4621816824303845
Loss in iteration 65 : 0.4621226703315995
Loss in iteration 66 : 0.4620638798213623
Loss in iteration 67 : 0.4620058513580447
Loss in iteration 68 : 0.46194903567032464
Loss in iteration 69 : 0.4618937882995476
Loss in iteration 70 : 0.46184037071448253
Loss in iteration 71 : 0.4617889564141682
Loss in iteration 72 : 0.46173964041786664
Loss in iteration 73 : 0.46169245067717546
Loss in iteration 74 : 0.46164736018810104
Loss in iteration 75 : 0.4616042988760222
Loss in iteration 76 : 0.46156316462343544
Loss in iteration 77 : 0.46152383307239325
Loss in iteration 78 : 0.46148616604058723
Loss in iteration 79 : 0.4614500185370373
Loss in iteration 80 : 0.46141524445605886
Loss in iteration 81 : 0.46138170107737986
Loss in iteration 82 : 0.4613492525184904
Loss in iteration 83 : 0.461317772283404
Loss in iteration 84 : 0.4612871450391317
Loss in iteration 85 : 0.46125726773398806
Loss in iteration 86 : 0.46122805015490437
Loss in iteration 87 : 0.4611994150070816
Loss in iteration 88 : 0.4611712975898764
Loss in iteration 89 : 0.46114364513749173
Loss in iteration 90 : 0.46111641589114594
Loss in iteration 91 : 0.46108957796907507
Loss in iteration 92 : 0.46106310810049306
Loss in iteration 93 : 0.4610369902880472
Loss in iteration 94 : 0.4610112144593749
Loss in iteration 95 : 0.4609857751619887
Loss in iteration 96 : 0.46096067034685284
Loss in iteration 97 : 0.46093590027561776
Loss in iteration 98 : 0.4609114665751029
Loss in iteration 99 : 0.46088737145149183
Loss in iteration 100 : 0.46086361706644086
Loss in iteration 101 : 0.46084020506867535
Loss in iteration 102 : 0.4608171362680015
Loss in iteration 103 : 0.4607944104341725
Loss in iteration 104 : 0.46077202620061436
Loss in iteration 105 : 0.46074998105242115
Loss in iteration 106 : 0.4607282713788107
Loss in iteration 107 : 0.46070689257204206
Loss in iteration 108 : 0.4606858391572183
Loss in iteration 109 : 0.46066510493995055
Loss in iteration 110 : 0.4606446831614967
Loss in iteration 111 : 0.46062456665323487
Loss in iteration 112 : 0.46060474798439127
Loss in iteration 113 : 0.4605852195985894
Loss in iteration 114 : 0.4605659739361059
Loss in iteration 115 : 0.46054700353985495
Loss in iteration 116 : 0.46052830114396964
Loss in iteration 117 : 0.46050985974461944
Loss in iteration 118 : 0.46049167265333835
Loss in iteration 119 : 0.460473733533672
Loss in iteration 120 : 0.4604560364224381
Loss in iteration 121 : 0.46043857573725777
Loss in iteration 122 : 0.46042134627229664
Loss in iteration 123 : 0.46040434318430734
Loss in iteration 124 : 0.4603875619711654
Loss in iteration 125 : 0.4603709984449822
Loss in iteration 126 : 0.4603546487018183
Loss in iteration 127 : 0.46033850908972795
Loss in iteration 128 : 0.4603225761766822
Loss in iteration 129 : 0.46030684671956484
Loss in iteration 130 : 0.4602913176351727
Loss in iteration 131 : 0.46027598597388397
Loss in iteration 132 : 0.4602608488963539
Loss in iteration 133 : 0.4602459036534415
Loss in iteration 134 : 0.46023114756935135
Loss in iteration 135 : 0.460216578027859
Loss in iteration 136 : 0.46020219246140276
Loss in iteration 137 : 0.4601879883427368
Loss in iteration 138 : 0.460173963178842
Loss in iteration 139 : 0.46016011450674466
Loss in iteration 140 : 0.4601464398909345
Loss in iteration 141 : 0.46013293692206414
Loss in iteration 142 : 0.4601196032166486
Loss in iteration 143 : 0.460106436417487
Loss in iteration 144 : 0.460093434194629
Loss in iteration 145 : 0.4600805942466216
Loss in iteration 146 : 0.46006791430194177
Loss in iteration 147 : 0.4600553921204332
Loss in iteration 148 : 0.4600430254946564
Loss in iteration 149 : 0.4600308122510778
Loss in iteration 150 : 0.46001875025103983
Loss in iteration 151 : 0.46000683739146425
Loss in iteration 152 : 0.45999507160531583
Loss in iteration 153 : 0.4599834508617577
Loss in iteration 154 : 0.45997197316609706
Loss in iteration 155 : 0.4599606365594591
Loss in iteration 156 : 0.4599494391182933
Loss in iteration 157 : 0.4599383789536862
Loss in iteration 158 : 0.4599274542105507
Loss in iteration 159 : 0.459916663066724
Loss in iteration 160 : 0.4599060037319778
Loss in iteration 161 : 0.4598954744470061
Loss in iteration 162 : 0.4598850734823954
Loss in iteration 163 : 0.45987479913760865
Loss in iteration 164 : 0.4598646497399827
Loss in iteration 165 : 0.4598546236437851
Loss in iteration 166 : 0.4598447192293046
Loss in iteration 167 : 0.45983493490200805
Loss in iteration 168 : 0.45982526909175725
Loss in iteration 169 : 0.45981572025207834
Loss in iteration 170 : 0.4598062868595095
Loss in iteration 171 : 0.45979696741298415
Loss in iteration 172 : 0.4597877604332957
Loss in iteration 173 : 0.4597786644625903
Loss in iteration 174 : 0.45976967806391494
Loss in iteration 175 : 0.45976079982081064
Loss in iteration 176 : 0.4597520283369273
Loss in iteration 177 : 0.4597433622356796
Loss in iteration 178 : 0.459734800159926
Loss in iteration 179 : 0.4597263407716546
Loss in iteration 180 : 0.4597179827517019
Loss in iteration 181 : 0.4597097247994744
Loss in iteration 182 : 0.4597015656326746
Loss in iteration 183 : 0.45969350398704184
Loss in iteration 184 : 0.4596855386160971
Loss in iteration 185 : 0.45967766829087325
Loss in iteration 186 : 0.45966989179966955
Loss in iteration 187 : 0.4596622079477921
Loss in iteration 188 : 0.4596546155573061
Loss in iteration 189 : 0.45964711346676645
Loss in iteration 190 : 0.45963970053098
Loss in iteration 191 : 0.45963237562073966
Loss in iteration 192 : 0.4596251376225785
Loss in iteration 193 : 0.4596179854385174
Loss in iteration 194 : 0.4596109179858167
Loss in iteration 195 : 0.4596039341967289
Loss in iteration 196 : 0.4595970330182606
Loss in iteration 197 : 0.4595902134119235
Loss in iteration 198 : 0.45958347435351515
Loss in iteration 199 : 0.4595768148328727
Loss in iteration 200 : 0.4595702338536564
Loss in iteration 201 : 0.4595637304331227
Loss in iteration 202 : 0.459557303601909
Loss in iteration 203 : 0.45955095240382277
Loss in iteration 204 : 0.45954467589563125
Loss in iteration 205 : 0.4595384731468589
Loss in iteration 206 : 0.4595323432395945
Loss in iteration 207 : 0.45952628526828565
Loss in iteration 208 : 0.45952029833955943
Loss in iteration 209 : 0.45951438157203234
Loss in iteration 210 : 0.4595085340961302
Loss in iteration 211 : 0.4595027550539074
Loss in iteration 212 : 0.4594970435988763
Loss in iteration 213 : 0.45949139889582896
Loss in iteration 214 : 0.4594858201206814
Loss in iteration 215 : 0.45948030646029575
Loss in iteration 216 : 0.4594748571123275
Loss in iteration 217 : 0.459469471285061
Loss in iteration 218 : 0.4594641481972625
Loss in iteration 219 : 0.4594588870780099
Loss in iteration 220 : 0.4594536871665616
Loss in iteration 221 : 0.45944854771219173
Loss in iteration 222 : 0.45944346797405833
Testing accuracy  of updater 2 on alg 0 with rate 0.7 = 0.78925, training accuracy 0.78925, time elapsed: 4971 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6770954948228857
Loss in iteration 3 : 0.6477805041574042
Loss in iteration 4 : 0.6224186658453382
Loss in iteration 5 : 0.5990766596821572
Loss in iteration 6 : 0.5783279331563668
Loss in iteration 7 : 0.5607071198495914
Loss in iteration 8 : 0.5456383601545708
Loss in iteration 9 : 0.5330345293580848
Loss in iteration 10 : 0.5226131316325786
Loss in iteration 11 : 0.5141077467760287
Loss in iteration 12 : 0.507224571280289
Loss in iteration 13 : 0.5016784882744357
Loss in iteration 14 : 0.49721199602064253
Loss in iteration 15 : 0.4936044419501287
Loss in iteration 16 : 0.49067359599403115
Loss in iteration 17 : 0.4882725382180575
Loss in iteration 18 : 0.48628445156475536
Loss in iteration 19 : 0.48461708766997774
Loss in iteration 20 : 0.483197849729047
Loss in iteration 21 : 0.48196981509034936
Loss in iteration 22 : 0.48088864972645473
Loss in iteration 23 : 0.4799202090111465
Loss in iteration 24 : 0.4790386035295544
Loss in iteration 25 : 0.47822456480249675
Loss in iteration 26 : 0.4774640205695677
Loss in iteration 27 : 0.4767468507010743
Loss in iteration 28 : 0.4760658298018964
Loss in iteration 29 : 0.47541577144832076
Loss in iteration 30 : 0.47479287951189814
Loss in iteration 31 : 0.47419429424105897
Loss in iteration 32 : 0.4736178035029237
Loss in iteration 33 : 0.47306167850243847
Loss in iteration 34 : 0.47252459050887247
Loss in iteration 35 : 0.4720055698222028
Loss in iteration 36 : 0.47150397782262043
Loss in iteration 37 : 0.47101947433038505
Loss in iteration 38 : 0.47055197293932965
Loss in iteration 39 : 0.47010158472897623
Loss in iteration 40 : 0.46966855517979383
Loss in iteration 41 : 0.46925320052245334
Loss in iteration 42 : 0.4688558490266439
Loss in iteration 43 : 0.46847679093342665
Loss in iteration 44 : 0.46811623876650843
Loss in iteration 45 : 0.4677742982054416
Loss in iteration 46 : 0.4674509487898124
Loss in iteration 47 : 0.46714603338459004
Loss in iteration 48 : 0.46685925535389794
Loss in iteration 49 : 0.4665901825188455
Loss in iteration 50 : 0.46633825703719634
Loss in iteration 51 : 0.46610281026759176
Loss in iteration 52 : 0.4658830814961219
Loss in iteration 53 : 0.4656782391940768
Loss in iteration 54 : 0.4654874033399981
Loss in iteration 55 : 0.4653096673458307
Loss in iteration 56 : 0.46514411829737484
Loss in iteration 57 : 0.46498985452727526
Loss in iteration 58 : 0.46484599992510767
Loss in iteration 59 : 0.46471171478364814
Loss in iteration 60 : 0.46458620332104544
Loss in iteration 61 : 0.4644687182658142
Loss in iteration 62 : 0.4643585630307729
Loss in iteration 63 : 0.46425509204282756
Loss in iteration 64 : 0.4641577097611336
Loss in iteration 65 : 0.46406586883610246
Loss in iteration 66 : 0.4639790677632432
Loss in iteration 67 : 0.46389684828902056
Loss in iteration 68 : 0.4638187927426743
Loss in iteration 69 : 0.4637445214026091
Loss in iteration 70 : 0.4636736899579472
Loss in iteration 71 : 0.463605987092021
Loss in iteration 72 : 0.4635411321915075
Loss in iteration 73 : 0.46347887317004716
Loss in iteration 74 : 0.4634189843866293
Loss in iteration 75 : 0.46336126463613886
Loss in iteration 76 : 0.46330553519121054
Loss in iteration 77 : 0.4632516378802747
Loss in iteration 78 : 0.46319943319495976
Loss in iteration 79 : 0.4631487984291863
Loss in iteration 80 : 0.46309962586097836
Loss in iteration 81 : 0.46305182099447634
Loss in iteration 82 : 0.4630053008834627
Loss in iteration 83 : 0.46295999255811887
Loss in iteration 84 : 0.4629158315745309
Loss in iteration 85 : 0.4628727607018893
Loss in iteration 86 : 0.4628307287565855
Loss in iteration 87 : 0.4627896895862849
Loss in iteration 88 : 0.46274960120121217
Loss in iteration 89 : 0.4627104250451207
Loss in iteration 90 : 0.4626721253946026
Loss in iteration 91 : 0.4626346688731261
Loss in iteration 92 : 0.46259802406478023
Loss in iteration 93 : 0.4625621612125865
Loss in iteration 94 : 0.4625270519866728
Loss in iteration 95 : 0.4624926693086902
Loss in iteration 96 : 0.4624589872204025
Loss in iteration 97 : 0.4624259807859049
Loss in iteration 98 : 0.4623936260187853
Loss in iteration 99 : 0.4623618998271723
Loss in iteration 100 : 0.46233077997123306
Loss in iteration 101 : 0.4623002450290682
Loss in iteration 102 : 0.4622702743681784
Loss in iteration 103 : 0.4622408481205738
Loss in iteration 104 : 0.462211947160356
Loss in iteration 105 : 0.4621835530831051
Loss in iteration 106 : 0.4621556481866702
Loss in iteration 107 : 0.4621282154531969
Loss in iteration 108 : 0.4621012385322263
Loss in iteration 109 : 0.46207470172473314
Loss in iteration 110 : 0.46204858996792386
Loss in iteration 111 : 0.46202288882055814
Loss in iteration 112 : 0.46199758444853634
Loss in iteration 113 : 0.4619726636104755
Loss in iteration 114 : 0.46194811364301236
Loss in iteration 115 : 0.461923922445581
Loss in iteration 116 : 0.4619000784645092
Loss in iteration 117 : 0.46187657067627985
Loss in iteration 118 : 0.4618533885699253
Loss in iteration 119 : 0.46183052212856945
Loss in iteration 120 : 0.46180796181018113
Loss in iteration 121 : 0.4617856985276919
Loss in iteration 122 : 0.46176372362862445
Loss in iteration 123 : 0.46174202887445315
Loss in iteration 124 : 0.4617206064198962
Loss in iteration 125 : 0.4616994487923373
Loss in iteration 126 : 0.46167854887161297
Loss in iteration 127 : 0.4616578998702804
Loss in iteration 128 : 0.46163749531457327
Loss in iteration 129 : 0.4616173290261315
Loss in iteration 130 : 0.4615973951045974
Loss in iteration 131 : 0.46157768791112436
Loss in iteration 132 : 0.46155820205286613
Loss in iteration 133 : 0.4615389323683595
Loss in iteration 134 : 0.4615198739139187
Loss in iteration 135 : 0.46150102195086423
Loss in iteration 136 : 0.4614823719336624
Loss in iteration 137 : 0.4614639194988556
Loss in iteration 138 : 0.46144566045476976
Loss in iteration 139 : 0.46142759077193213
Loss in iteration 140 : 0.4614097065741423
Loss in iteration 141 : 0.4613920041301762
Loss in iteration 142 : 0.4613744798460433
Loss in iteration 143 : 0.46135713025779024
Loss in iteration 144 : 0.4613399520247768
Loss in iteration 145 : 0.4613229419234219
Loss in iteration 146 : 0.46130609684137347
Loss in iteration 147 : 0.4612894137720511
Loss in iteration 148 : 0.4612728898095892
Loss in iteration 149 : 0.46125652214407453
Loss in iteration 150 : 0.46124030805712607
Loss in iteration 151 : 0.4612242449177482
Loss in iteration 152 : 0.4612083301784467
Loss in iteration 153 : 0.461192561371582
Loss in iteration 154 : 0.46117693610595417
Loss in iteration 155 : 0.46116145206356735
Loss in iteration 156 : 0.46114610699659486
Loss in iteration 157 : 0.46113089872450036
Loss in iteration 158 : 0.46111582513130517
Loss in iteration 159 : 0.46110088416300327
Loss in iteration 160 : 0.46108607382509964
Loss in iteration 161 : 0.46107139218025334
Loss in iteration 162 : 0.4610568373460418
Loss in iteration 163 : 0.46104240749281455
Loss in iteration 164 : 0.4610281008416445
Loss in iteration 165 : 0.4610139156623647
Loss in iteration 166 : 0.4609998502716847
Loss in iteration 167 : 0.4609859030313919
Loss in iteration 168 : 0.4609720723466213
Loss in iteration 169 : 0.46095835666419777
Loss in iteration 170 : 0.46094475447104505
Loss in iteration 171 : 0.4609312642926651
Loss in iteration 172 : 0.46091788469168044
Loss in iteration 173 : 0.46090461426642504
Loss in iteration 174 : 0.4608914516496125
Loss in iteration 175 : 0.46087839550704474
Loss in iteration 176 : 0.46086544453637973
Loss in iteration 177 : 0.4608525974659507
Loss in iteration 178 : 0.46083985305364344
Loss in iteration 179 : 0.4608272100858023
Loss in iteration 180 : 0.4608146673762007
Loss in iteration 181 : 0.4608022237650462
Loss in iteration 182 : 0.46078987811802724
Loss in iteration 183 : 0.4607776293254094
Loss in iteration 184 : 0.4607654763011571
Loss in iteration 185 : 0.46075341798210456
Loss in iteration 186 : 0.4607414533271514
Loss in iteration 187 : 0.4607295813165065
Loss in iteration 188 : 0.4607178009509503
Loss in iteration 189 : 0.4607061112511336
Loss in iteration 190 : 0.4606945112569122
Loss in iteration 191 : 0.46068300002670104
Loss in iteration 192 : 0.4606715766368625
Loss in iteration 193 : 0.46066024018111174
Loss in iteration 194 : 0.46064898976996405
Loss in iteration 195 : 0.46063782453018176
Loss in iteration 196 : 0.4606267436042689
Loss in iteration 197 : 0.4606157461499698
Loss in iteration 198 : 0.46060483133979385
Loss in iteration 199 : 0.4605939983605627
Loss in iteration 200 : 0.4605832464129719
Loss in iteration 201 : 0.4605725747111732
Loss in iteration 202 : 0.4605619824823711
Loss in iteration 203 : 0.4605514689664433
Loss in iteration 204 : 0.46054103341555663
Loss in iteration 205 : 0.4605306750938345
Loss in iteration 206 : 0.4605203932769887
Loss in iteration 207 : 0.46051018725201637
Loss in iteration 208 : 0.4605000563168643
Loss in iteration 209 : 0.4604899997801435
Loss in iteration 210 : 0.4604800169608287
Loss in iteration 211 : 0.46047010718797354
Loss in iteration 212 : 0.4604602698004546
Loss in iteration 213 : 0.4604505041466981
Loss in iteration 214 : 0.46044080958444833
Loss in iteration 215 : 0.46043118548050516
Loss in iteration 216 : 0.46042163121050883
Loss in iteration 217 : 0.46041214615871323
Loss in iteration 218 : 0.46040272971776797
Loss in iteration 219 : 0.4603933812885184
Loss in iteration 220 : 0.4603841002797964
Loss in iteration 221 : 0.4603748861082403
Loss in iteration 222 : 0.46036573819809434
Loss in iteration 223 : 0.4603566559810478
Loss in iteration 224 : 0.460347638896046
Loss in iteration 225 : 0.4603386863891377
Loss in iteration 226 : 0.4603297979132981
Loss in iteration 227 : 0.4603209729282921
Loss in iteration 228 : 0.4603122109005118
Loss in iteration 229 : 0.4603035113028341
Loss in iteration 230 : 0.46029487361448607
Loss in iteration 231 : 0.46028629732089976
Loss in iteration 232 : 0.46027778191359375
Loss in iteration 233 : 0.46026932689003514
Loss in iteration 234 : 0.4602609317535243
Loss in iteration 235 : 0.4602525960130769
Loss in iteration 236 : 0.4602443191833035
Loss in iteration 237 : 0.4602361007843047
Loss in iteration 238 : 0.4602279403415592
Loss in iteration 239 : 0.460219837385826
Testing accuracy  of updater 2 on alg 0 with rate 0.4 = 0.789875, training accuracy 0.789875, time elapsed: 5059 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6839617669630191
Loss in iteration 3 : 0.6760909594261041
Loss in iteration 4 : 0.6680109399659566
Loss in iteration 5 : 0.6586683759720922
Loss in iteration 6 : 0.6484556293033409
Loss in iteration 7 : 0.6381864457992892
Loss in iteration 8 : 0.6282892130227825
Loss in iteration 9 : 0.6187991309855131
Loss in iteration 10 : 0.6096646851465802
Loss in iteration 11 : 0.600907387893033
Loss in iteration 12 : 0.5925951698544829
Loss in iteration 13 : 0.5847773046070243
Loss in iteration 14 : 0.5774643669672366
Loss in iteration 15 : 0.5706436570107116
Loss in iteration 16 : 0.5642971258998939
Loss in iteration 17 : 0.5584076534118918
Loss in iteration 18 : 0.5529573676419242
Loss in iteration 19 : 0.5479252529308157
Loss in iteration 20 : 0.5432870845320394
Loss in iteration 21 : 0.5390169075937247
Loss in iteration 22 : 0.5350884742152318
Loss in iteration 23 : 0.5314759734266667
Loss in iteration 24 : 0.5281542653509892
Loss in iteration 25 : 0.5250990260571523
Loss in iteration 26 : 0.5222869855558226
Loss in iteration 27 : 0.5196962120922339
Loss in iteration 28 : 0.5173063329482663
Loss in iteration 29 : 0.5150986366144183
Loss in iteration 30 : 0.513056069757436
Loss in iteration 31 : 0.5111631709570726
Loss in iteration 32 : 0.5094059755886404
Loss in iteration 33 : 0.5077719080048294
Loss in iteration 34 : 0.5062496658524179
Loss in iteration 35 : 0.5048290994581306
Loss in iteration 36 : 0.5035010913054334
Loss in iteration 37 : 0.5022574416396791
Loss in iteration 38 : 0.501090764794626
Loss in iteration 39 : 0.4999943981699422
Loss in iteration 40 : 0.4989623235302441
Loss in iteration 41 : 0.4979890991577861
Loss in iteration 42 : 0.4970698011760986
Loss in iteration 43 : 0.4961999725443536
Loss in iteration 44 : 0.4953755784264779
Loss in iteration 45 : 0.4945929667656164
Loss in iteration 46 : 0.4938488330045416
Loss in iteration 47 : 0.4931401880546265
Loss in iteration 48 : 0.49246432883621993
Loss in iteration 49 : 0.4918188109479458
Loss in iteration 50 : 0.49120142322182225
Loss in iteration 51 : 0.49061016405933794
Loss in iteration 52 : 0.4900432195222157
Loss in iteration 53 : 0.48949894318571996
Loss in iteration 54 : 0.4889758377682837
Loss in iteration 55 : 0.48847253853965905
Loss in iteration 56 : 0.4879877984866384
Loss in iteration 57 : 0.48752047518506453
Loss in iteration 58 : 0.48706951929384695
Loss in iteration 59 : 0.48663396455599334
Loss in iteration 60 : 0.4862129191672905
Loss in iteration 61 : 0.48580555835774236
Loss in iteration 62 : 0.4854111180245469
Loss in iteration 63 : 0.48502888925769755
Loss in iteration 64 : 0.4846582136084031
Loss in iteration 65 : 0.4842984789647081
Loss in iteration 66 : 0.4839491159162077
Loss in iteration 67 : 0.4836095945087892
Loss in iteration 68 : 0.4832794213096368
Loss in iteration 69 : 0.4829581367207963
Loss in iteration 70 : 0.48264531249588855
Loss in iteration 71 : 0.48234054942807564
Loss in iteration 72 : 0.48204347518826673
Loss in iteration 73 : 0.4817537423005454
Loss in iteration 74 : 0.4814710262473202
Loss in iteration 75 : 0.4811950236999932
Loss in iteration 76 : 0.4809254508725764
Loss in iteration 77 : 0.4806620419959182
Loss in iteration 78 : 0.4804045479097172
Loss in iteration 79 : 0.4801527347684195
Loss in iteration 80 : 0.4799063828558866
Loss in iteration 81 : 0.4796652855026344
Loss in iteration 82 : 0.47942924809847637
Loss in iteration 83 : 0.4791980871928539
Loss in iteration 84 : 0.4789716296748492
Loss in iteration 85 : 0.47874971202499317
Loss in iteration 86 : 0.4785321796312753
Loss in iteration 87 : 0.47831888616241086
Loss in iteration 88 : 0.4781096929920483
Loss in iteration 89 : 0.47790446866844405
Loss in iteration 90 : 0.47770308842489273
Loss in iteration 91 : 0.4775054337269562
Loss in iteration 92 : 0.4773113918531657
Loss in iteration 93 : 0.47712085550651256
Loss in iteration 94 : 0.4769337224544177
Loss in iteration 95 : 0.4767498951952373
Loss in iteration 96 : 0.4765692806496445
Loss in iteration 97 : 0.47639178987532965
Loss in iteration 98 : 0.47621733780359227
Loss in iteration 99 : 0.4760458429964415
Loss in iteration 100 : 0.4758772274228306
Loss in iteration 101 : 0.4757114162526699
Loss in iteration 102 : 0.4755483376672629
Loss in iteration 103 : 0.4753879226848402
Loss in iteration 104 : 0.47523010499986124
Loss in iteration 105 : 0.4750748208348432
Loss in iteration 106 : 0.4749220088035248
Loss in iteration 107 : 0.47477160978421756
Loss in iteration 108 : 0.47462356680235723
Loss in iteration 109 : 0.4744778249212801
Loss in iteration 110 : 0.4743343311404207
Loss in iteration 111 : 0.47419303430018805
Loss in iteration 112 : 0.4740538849928641
Loss in iteration 113 : 0.47391683547900965
Loss in iteration 114 : 0.47378183960883496
Loss in iteration 115 : 0.47364885274821256
Loss in iteration 116 : 0.4735178317089042
Loss in iteration 117 : 0.47338873468274845
Loss in iteration 118 : 0.4732615211795262
Loss in iteration 119 : 0.4731361519682656
Loss in iteration 120 : 0.4730125890217945
Loss in iteration 121 : 0.4728907954643346
Loss in iteration 122 : 0.47277073552195
Loss in iteration 123 : 0.47265237447571656
Loss in iteration 124 : 0.47253567861740453
Loss in iteration 125 : 0.4724206152075789
Loss in iteration 126 : 0.4723071524359192
Loss in iteration 127 : 0.4721952593836588
Loss in iteration 128 : 0.47208490598800845
Loss in iteration 129 : 0.4719760630084187
Loss in iteration 130 : 0.4718687019945929
Loss in iteration 131 : 0.4717627952561237
Loss in iteration 132 : 0.47165831583367124
Loss in iteration 133 : 0.4715552374715488
Loss in iteration 134 : 0.4714535345916915
Loss in iteration 135 : 0.471353182268861
Loss in iteration 136 : 0.47125415620706557
Loss in iteration 137 : 0.4711564327170918
Loss in iteration 138 : 0.4710599886950995
Loss in iteration 139 : 0.4709648016022109
Loss in iteration 140 : 0.4708708494450589
Loss in iteration 141 : 0.470778110757218
Loss in iteration 142 : 0.4706865645814778
Loss in iteration 143 : 0.47059619045292
Loss in iteration 144 : 0.47050696838275813
Loss in iteration 145 : 0.4704188788428767
Loss in iteration 146 : 0.4703319027510553
Loss in iteration 147 : 0.4702460214568375
Loss in iteration 148 : 0.47016121672799793
Loss in iteration 149 : 0.4700774707375754
Loss in iteration 150 : 0.46999476605147744
Loss in iteration 151 : 0.4699130856165592
Loss in iteration 152 : 0.46983241274920934
Loss in iteration 153 : 0.46975273112441207
Loss in iteration 154 : 0.4696740247652057
Loss in iteration 155 : 0.4695962780325922
Loss in iteration 156 : 0.46951947561582313
Loss in iteration 157 : 0.4694436025230535
Loss in iteration 158 : 0.4693686440723702
Loss in iteration 159 : 0.46929458588314255
Loss in iteration 160 : 0.46922141386769123
Loss in iteration 161 : 0.46914911422328137
Loss in iteration 162 : 0.4690776734243875
Loss in iteration 163 : 0.46900707821525384
Loss in iteration 164 : 0.4689373156027067
Loss in iteration 165 : 0.46886837284923105
Loss in iteration 166 : 0.4688002374662824
Loss in iteration 167 : 0.4687328972078367
Loss in iteration 168 : 0.46866634006415697
Loss in iteration 169 : 0.46860055425577557
Loss in iteration 170 : 0.4685355282276776
Loss in iteration 171 : 0.4684712506436754
Loss in iteration 172 : 0.46840771038098283
Loss in iteration 173 : 0.46834489652494643
Loss in iteration 174 : 0.4682827983639663
Loss in iteration 175 : 0.4682214053845703
Loss in iteration 176 : 0.4681607072666543
Loss in iteration 177 : 0.4681006938788599
Loss in iteration 178 : 0.46804135527410995
Loss in iteration 179 : 0.4679826816852784
Loss in iteration 180 : 0.46792466352098916
Loss in iteration 181 : 0.4678672913615517
Loss in iteration 182 : 0.46781055595500787
Loss in iteration 183 : 0.4677544482133169
Loss in iteration 184 : 0.46769895920863136
Loss in iteration 185 : 0.46764408016970016
Loss in iteration 186 : 0.467589802478364
Loss in iteration 187 : 0.4675361176661663
Loss in iteration 188 : 0.4674830174110523
Loss in iteration 189 : 0.46743049353416366
Loss in iteration 190 : 0.46737853799672346
Loss in iteration 191 : 0.46732714289702004
Loss in iteration 192 : 0.46727630046745205
Loss in iteration 193 : 0.4672260030716804
Loss in iteration 194 : 0.4671762432018367
Loss in iteration 195 : 0.4671270134758317
Loss in iteration 196 : 0.4670783066347093
Loss in iteration 197 : 0.4670301155400923
Loss in iteration 198 : 0.4669824331716888
Loss in iteration 199 : 0.4669352526248608
Loss in iteration 200 : 0.4668885671082703
Loss in iteration 201 : 0.4668423699415647
Loss in iteration 202 : 0.46679665455314207
Loss in iteration 203 : 0.4667514144779674
Loss in iteration 204 : 0.46670664335543804
Loss in iteration 205 : 0.4666623349273143
Loss in iteration 206 : 0.4666184830356918
Loss in iteration 207 : 0.46657508162102884
Loss in iteration 208 : 0.4665321247202309
Loss in iteration 209 : 0.46648960646476384
Loss in iteration 210 : 0.4664475210788288
Loss in iteration 211 : 0.46640586287757685
Loss in iteration 212 : 0.4663646262653657
Loss in iteration 213 : 0.4663238057340596
Loss in iteration 214 : 0.46628339586136497
Loss in iteration 215 : 0.4662433913092132
Loss in iteration 216 : 0.46620378682218144
Loss in iteration 217 : 0.46616457722593796
Loss in iteration 218 : 0.4661257574257365
Loss in iteration 219 : 0.46608732240494793
Loss in iteration 220 : 0.46604926722360357
Loss in iteration 221 : 0.466011587017004
Loss in iteration 222 : 0.46597427699433003
Loss in iteration 223 : 0.4659373324373064
Loss in iteration 224 : 0.46590074869888176
Loss in iteration 225 : 0.4658645212019428
Loss in iteration 226 : 0.4658286454380582
Loss in iteration 227 : 0.4657931169662514
Loss in iteration 228 : 0.4657579314117949
Loss in iteration 229 : 0.46572308446503335
Loss in iteration 230 : 0.465688571880237
Loss in iteration 231 : 0.4656543894744717
Loss in iteration 232 : 0.46562053312649926
Loss in iteration 233 : 0.4655869987756986
Loss in iteration 234 : 0.46555378242100814
Loss in iteration 235 : 0.46552088011989806
Loss in iteration 236 : 0.46548828798735337
Loss in iteration 237 : 0.4654560021948852
Loss in iteration 238 : 0.4654240189695636
Loss in iteration 239 : 0.46539233459306556
Loss in iteration 240 : 0.4653609454007485
Loss in iteration 241 : 0.4653298477807298
Loss in iteration 242 : 0.4652990381730083
Loss in iteration 243 : 0.46526851306857864
Loss in iteration 244 : 0.4652382690085797
Loss in iteration 245 : 0.46520830258345575
Loss in iteration 246 : 0.46517861043213227
Loss in iteration 247 : 0.46514918924120907
Loss in iteration 248 : 0.46512003574417543
Loss in iteration 249 : 0.4650911467206266
Loss in iteration 250 : 0.4650625189955132
Loss in iteration 251 : 0.4650341494383907
Loss in iteration 252 : 0.46500603496269405
Loss in iteration 253 : 0.4649781725250199
Loss in iteration 254 : 0.4649505591244225
Loss in iteration 255 : 0.4649231918017306
Loss in iteration 256 : 0.4648960676388684
Loss in iteration 257 : 0.46486918375819436
Loss in iteration 258 : 0.46484253732185254
Loss in iteration 259 : 0.46481612553112994
Loss in iteration 260 : 0.4647899456258448
Loss in iteration 261 : 0.4647639948837153
Loss in iteration 262 : 0.4647382706197677
Loss in iteration 263 : 0.4647127701857486
Loss in iteration 264 : 0.4646874909695365
Loss in iteration 265 : 0.464662430394577
Loss in iteration 266 : 0.4646375859193248
Loss in iteration 267 : 0.4646129550366966
Loss in iteration 268 : 0.46458853527352745
Loss in iteration 269 : 0.46456432419004795
Loss in iteration 270 : 0.46454031937935975
Loss in iteration 271 : 0.46451651846693687
Loss in iteration 272 : 0.4644929191101103
Loss in iteration 273 : 0.46446951899759087
Loss in iteration 274 : 0.4644463158489756
Loss in iteration 275 : 0.46442330741428217
Loss in iteration 276 : 0.46440049147348167
Loss in iteration 277 : 0.46437786583603746
Loss in iteration 278 : 0.46435542834046045
Loss in iteration 279 : 0.4643331768538636
Loss in iteration 280 : 0.46431110927153907
Loss in iteration 281 : 0.46428922351651947
Loss in iteration 282 : 0.46426751753916573
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.7905, training accuracy 0.7905, time elapsed: 5397 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 19.998616144859007
Loss in iteration 3 : 27.601506627795963
Loss in iteration 4 : 9.938174287340898
Loss in iteration 5 : 11.106207851814611
Loss in iteration 6 : 7.491720966129548
Loss in iteration 7 : 7.967658525853581
Loss in iteration 8 : 6.552448045373032
Loss in iteration 9 : 5.986946808556069
Loss in iteration 10 : 5.211545294583746
Loss in iteration 11 : 4.834898569203402
Loss in iteration 12 : 4.334936535605953
Loss in iteration 13 : 4.161527587209117
Loss in iteration 14 : 3.8032519955082487
Loss in iteration 15 : 3.74362151702139
Loss in iteration 16 : 3.390202323257717
Loss in iteration 17 : 3.404044311737088
Loss in iteration 18 : 3.090429751891933
Loss in iteration 19 : 3.1406097310943095
Loss in iteration 20 : 2.870714649649095
Loss in iteration 21 : 2.947224550217715
Loss in iteration 22 : 2.7094191522488256
Loss in iteration 23 : 2.802199189347175
Loss in iteration 24 : 2.5873107600849896
Loss in iteration 25 : 2.6920876234776006
Loss in iteration 26 : 2.478476038654413
Loss in iteration 27 : 2.5965133818514667
Loss in iteration 28 : 2.37360415897832
Loss in iteration 29 : 2.513481325158702
Loss in iteration 30 : 2.2641300940057327
Loss in iteration 31 : 2.4459832813554776
Loss in iteration 32 : 2.1297988066688585
Loss in iteration 33 : 2.3306158322750097
Loss in iteration 34 : 1.9622470157320695
Loss in iteration 35 : 2.136412850155245
Loss in iteration 36 : 1.8405640051290457
Loss in iteration 37 : 2.0116993299663526
Loss in iteration 38 : 1.7917621006382847
Loss in iteration 39 : 1.9711821949338282
Loss in iteration 40 : 1.8149027180072308
Loss in iteration 41 : 2.0140150835053845
Loss in iteration 42 : 1.9581126422823705
Loss in iteration 43 : 2.190528991852575
Loss in iteration 44 : 2.1453900501793215
Loss in iteration 45 : 2.363725504207516
Loss in iteration 46 : 2.189652444507524
Loss in iteration 47 : 2.351775287804881
Loss in iteration 48 : 2.1289134208101474
Loss in iteration 49 : 2.251814533174765
Loss in iteration 50 : 2.0398683134989883
Loss in iteration 51 : 2.142767661163953
Loss in iteration 52 : 1.9584896739004147
Loss in iteration 53 : 2.0523024367491822
Loss in iteration 54 : 1.8917626852043208
Loss in iteration 55 : 1.9824165133443503
Loss in iteration 56 : 1.8385842416364675
Loss in iteration 57 : 1.9295434593989507
Loss in iteration 58 : 1.7960843446987607
Loss in iteration 59 : 1.8894395727252058
Loss in iteration 60 : 1.7614441596642367
Loss in iteration 61 : 1.8584296319917923
Loss in iteration 62 : 1.7323844603838894
Loss in iteration 63 : 1.8336631238548804
Loss in iteration 64 : 1.7072415247454449
Loss in iteration 65 : 1.8130679081787522
Loss in iteration 66 : 1.6849206583655596
Loss in iteration 67 : 1.795229632400996
Loss in iteration 68 : 1.664796950720556
Loss in iteration 69 : 1.7792487976036417
Loss in iteration 70 : 1.6465920033211505
Loss in iteration 71 : 1.764598976051926
Loss in iteration 72 : 1.6302509184450542
Loss in iteration 73 : 1.7510049925760736
Loss in iteration 74 : 1.6158324679596203
Loss in iteration 75 : 1.738344968653431
Loss in iteration 76 : 1.6034102789956173
Loss in iteration 77 : 1.726564639391333
Loss in iteration 78 : 1.5929772935759716
Loss in iteration 79 : 1.7155915758345985
Loss in iteration 80 : 1.584355197831326
Loss in iteration 81 : 1.7052532235872249
Loss in iteration 82 : 1.5771281220666606
Loss in iteration 83 : 1.6952231391563886
Loss in iteration 84 : 1.5706345115985068
Loss in iteration 85 : 1.6850289461614136
Loss in iteration 86 : 1.5640494872496897
Loss in iteration 87 : 1.6741416308745765
Loss in iteration 88 : 1.5565575378268255
Loss in iteration 89 : 1.6621255916075675
Loss in iteration 90 : 1.5475568168462568
Loss in iteration 91 : 1.6487825825502895
Loss in iteration 92 : 1.5367975197862394
Loss in iteration 93 : 1.6342128951751058
Loss in iteration 94 : 1.5243877604506013
Loss in iteration 95 : 1.618764685542797
Loss in iteration 96 : 1.510682948745244
Loss in iteration 97 : 1.6029098942643119
Loss in iteration 98 : 1.4961334189717217
Loss in iteration 99 : 1.5871149481843905
Loss in iteration 100 : 1.4811598488588285
Loss in iteration 101 : 1.5717539298439396
Loss in iteration 102 : 1.4660850316346286
Loss in iteration 103 : 1.5570742177114214
Loss in iteration 104 : 1.4511163624913193
Loss in iteration 105 : 1.5432003349521024
Loss in iteration 106 : 1.4363608897039204
Loss in iteration 107 : 1.5301562880666661
Loss in iteration 108 : 1.4218572071947315
Loss in iteration 109 : 1.517891693604308
Loss in iteration 110 : 1.4076156231983155
Loss in iteration 111 : 1.5063047697962242
Loss in iteration 112 : 1.3936637180203815
Loss in iteration 113 : 1.495262719505597
Loss in iteration 114 : 1.3800960403127824
Loss in iteration 115 : 1.4846267031479967
Loss in iteration 116 : 1.3671239854731287
Loss in iteration 117 : 1.4742930299384378
Loss in iteration 118 : 1.3551160595737917
Loss in iteration 119 : 1.4642580639800062
Loss in iteration 120 : 1.3446113626979481
Loss in iteration 121 : 1.4546920938237933
Loss in iteration 122 : 1.3362817959456847
Loss in iteration 123 : 1.4459705418862996
Loss in iteration 124 : 1.3308155626802831
Loss in iteration 125 : 1.4385935012233895
Loss in iteration 126 : 1.3287044870035374
Loss in iteration 127 : 1.4329652647545776
Loss in iteration 128 : 1.329948515314119
Loss in iteration 129 : 1.4290911014788164
Loss in iteration 130 : 1.333754326959089
Loss in iteration 131 : 1.4263292015409297
Loss in iteration 132 : 1.3384163227432415
Loss in iteration 133 : 1.4233870878705412
Loss in iteration 134 : 1.341651666937494
Loss in iteration 135 : 1.418717821982692
Loss in iteration 136 : 1.3414493318178837
Loss in iteration 137 : 1.4112140317856947
Loss in iteration 138 : 1.336929681600964
Loss in iteration 139 : 1.4007288041119632
Loss in iteration 140 : 1.3285230110712278
Loss in iteration 141 : 1.388010947629982
Loss in iteration 142 : 1.3174375104991332
Loss in iteration 143 : 1.3741994531443147
Loss in iteration 144 : 1.3049856600013428
Loss in iteration 145 : 1.360333181431249
Loss in iteration 146 : 1.2921875790662256
Loss in iteration 147 : 1.3471194499423214
Loss in iteration 148 : 1.2796745591300918
Loss in iteration 149 : 1.3349226940796792
Loss in iteration 150 : 1.2677513318300688
Loss in iteration 151 : 1.3238509995181689
Loss in iteration 152 : 1.2565007371746488
Loss in iteration 153 : 1.313856054134877
Loss in iteration 154 : 1.245876818239231
Loss in iteration 155 : 1.304811790372219
Loss in iteration 156 : 1.235772213314284
Loss in iteration 157 : 1.2965651880209257
Loss in iteration 158 : 1.2260625731612114
Loss in iteration 159 : 1.2889635788516112
Loss in iteration 160 : 1.2166354851927186
Loss in iteration 161 : 1.281865418371671
Loss in iteration 162 : 1.207411307285612
Loss in iteration 163 : 1.2751415261438983
Loss in iteration 164 : 1.1983616232406964
Loss in iteration 165 : 1.2686739680539305
Loss in iteration 166 : 1.189528698174015
Loss in iteration 167 : 1.2623607531622985
Loss in iteration 168 : 1.1810461713980667
Loss in iteration 169 : 1.2561346119231842
Loss in iteration 170 : 1.173156630186405
Loss in iteration 171 : 1.2499988279767567
Loss in iteration 172 : 1.1662154766085764
Loss in iteration 173 : 1.2440684406220308
Loss in iteration 174 : 1.1606645730781364
Loss in iteration 175 : 1.2385859507905903
Loss in iteration 176 : 1.1569587721130061
Loss in iteration 177 : 1.2338748744676056
Loss in iteration 178 : 1.155439875206551
Loss in iteration 179 : 1.2302190489104943
Loss in iteration 180 : 1.1561770317067277
Loss in iteration 181 : 1.2277025304477533
Loss in iteration 182 : 1.1588228548657935
Loss in iteration 183 : 1.2260823292996128
Loss in iteration 184 : 1.1625589150893898
Loss in iteration 185 : 1.2247697363604888
Loss in iteration 186 : 1.1662062543701994
Loss in iteration 187 : 1.222963380513594
Loss in iteration 188 : 1.1685243523087565
Loss in iteration 189 : 1.2199089610469742
Loss in iteration 190 : 1.168597444611149
Loss in iteration 191 : 1.2151702577569437
Loss in iteration 192 : 1.1660980079762162
Loss in iteration 193 : 1.208758248024956
Loss in iteration 194 : 1.1612786037420137
Loss in iteration 195 : 1.2010550696718791
Loss in iteration 196 : 1.1547456066979875
Loss in iteration 197 : 1.1926143362513963
Loss in iteration 198 : 1.1471903738205356
Loss in iteration 199 : 1.1839702222019786
Loss in iteration 200 : 1.1392062604558348
Loss in iteration 201 : 1.1755288127324401
Loss in iteration 202 : 1.1312155575842004
Loss in iteration 203 : 1.1675408672122753
Loss in iteration 204 : 1.1234715402969446
Loss in iteration 205 : 1.1601228068577343
Loss in iteration 206 : 1.1160941628060932
Loss in iteration 207 : 1.153294898926832
Loss in iteration 208 : 1.10911242513184
Loss in iteration 209 : 1.1470185423720196
Loss in iteration 210 : 1.1025010712265326
Loss in iteration 211 : 1.1412253930388838
Loss in iteration 212 : 1.0962081863386826
Loss in iteration 213 : 1.1358371680878294
Loss in iteration 214 : 1.0901743417922138
Loss in iteration 215 : 1.1307775727751814
Loss in iteration 216 : 1.0843452642795013
Loss in iteration 217 : 1.1259785194655674
Loss in iteration 218 : 1.0786800684241542
Loss in iteration 219 : 1.1213827329866453
Loss in iteration 220 : 1.0731566647935442
Loss in iteration 221 : 1.1169445038325139
Loss in iteration 222 : 1.0677753730034034
Loss in iteration 223 : 1.112629962280665
Loss in iteration 224 : 1.062561145590023
Loss in iteration 225 : 1.1084178009478585
Loss in iteration 226 : 1.0575641866050258
Loss in iteration 227 : 1.1043007746320859
Loss in iteration 228 : 1.0528581908200465
Loss in iteration 229 : 1.100287485690168
Loss in iteration 230 : 1.0485350623094323
Loss in iteration 231 : 1.096403030250271
Loss in iteration 232 : 1.044694992995528
Loss in iteration 233 : 1.0926864289642422
Loss in iteration 234 : 1.0414314061296286
Loss in iteration 235 : 1.0891829824940968
Loss in iteration 236 : 1.0388116041617963
Loss in iteration 237 : 1.0859311852283982
Loss in iteration 238 : 1.0368558525495069
Loss in iteration 239 : 1.0829463628244358
Loss in iteration 240 : 1.035519567153788
Loss in iteration 241 : 1.0802057346851175
Loss in iteration 242 : 1.0346844197235132
Loss in iteration 243 : 1.077640740934064
Loss in iteration 244 : 1.034163598592639
Loss in iteration 245 : 1.07514122970779
Loss in iteration 246 : 1.033723514975713
Loss in iteration 247 : 1.0725725015954686
Loss in iteration 248 : 1.0331191505181825
Loss in iteration 249 : 1.06980142449888
Loss in iteration 250 : 1.032134660620645
Loss in iteration 251 : 1.0667238928999938
Loss in iteration 252 : 1.030617691185961
Loss in iteration 253 : 1.0632849209911657
Loss in iteration 254 : 1.0284975730345134
Loss in iteration 255 : 1.059485505208988
Loss in iteration 256 : 1.0257837857365006
Loss in iteration 257 : 1.0553757152868175
Loss in iteration 258 : 1.0225483959095205
Loss in iteration 259 : 1.0510383642438936
Loss in iteration 260 : 1.018900644877926
Loss in iteration 261 : 1.046569716575611
Loss in iteration 262 : 1.0149619830139882
Loss in iteration 263 : 1.0420627504188518
Loss in iteration 264 : 1.0108469493519754
Loss in iteration 265 : 1.0375959175556202
Loss in iteration 266 : 1.0066517339227132
Loss in iteration 267 : 1.0332278142414784
Loss in iteration 268 : 1.002449668516075
Loss in iteration 269 : 1.0289965965487704
Loss in iteration 270 : 0.9982916968750342
Loss in iteration 271 : 1.0249224239716628
Loss in iteration 272 : 0.9942097443994841
Loss in iteration 273 : 1.0210113493312778
Loss in iteration 274 : 0.9902213179253788
Loss in iteration 275 : 1.017259502799908
Loss in iteration 276 : 0.9863342192003905
Loss in iteration 277 : 1.0136568803195558
Loss in iteration 278 : 0.9825507410298062
Loss in iteration 279 : 1.0101904180386814
Loss in iteration 280 : 0.9788710622260428
Loss in iteration 281 : 1.0068462833956746
Loss in iteration 282 : 0.9752957697922062
Loss in iteration 283 : 1.003611453574714
Loss in iteration 284 : 0.9718275451245382
Loss in iteration 285 : 1.0004747106433902
Loss in iteration 286 : 0.968472090363327
Loss in iteration 287 : 0.9974271866389688
Loss in iteration 288 : 0.9652383716063315
Loss in iteration 289 : 0.9944625633529024
Loss in iteration 290 : 0.9621382415481012
Loss in iteration 291 : 0.9915769886435253
Loss in iteration 292 : 0.9591854937156743
Loss in iteration 293 : 0.9887687296918539
Loss in iteration 294 : 0.9563944072597828
Loss in iteration 295 : 0.9860375583605112
Loss in iteration 296 : 0.9537778729626023
Loss in iteration 297 : 0.9833838667966542
Loss in iteration 298 : 0.9513452481824465
Loss in iteration 299 : 0.980807548522003
Loss in iteration 300 : 0.9491001622491758
Loss in iteration 301 : 0.9783067467056855
Loss in iteration 302 : 0.9470385659774855
Loss in iteration 303 : 0.9758766490928724
Loss in iteration 304 : 0.9451473632657468
Loss in iteration 305 : 0.9735085690820037
Loss in iteration 306 : 0.9434039508336189
Loss in iteration 307 : 0.9711895628864947
Loss in iteration 308 : 0.9417769029959345
Loss in iteration 309 : 0.9689027717187646
Loss in iteration 310 : 0.9402278695195423
Loss in iteration 311 : 0.9666285457832159
Loss in iteration 312 : 0.9387145306913658
Loss in iteration 313 : 0.964346231504151
Loss in iteration 314 : 0.9371942263952776
Loss in iteration 315 : 0.9620363347481654
Loss in iteration 316 : 0.9356277112151767
Loss in iteration 317 : 0.959682665889778
Loss in iteration 318 : 0.9339824419637136
Loss in iteration 319 : 0.9572740657188702
Loss in iteration 320 : 0.9322348997268173
Loss in iteration 321 : 0.9548054089826655
Loss in iteration 322 : 0.9303716602177238
Loss in iteration 323 : 0.9522777527728845
Loss in iteration 324 : 0.928389189421847
Loss in iteration 325 : 0.9496976848710155
Loss in iteration 326 : 0.9262925788828177
Loss in iteration 327 : 0.9470760773433436
Loss in iteration 328 : 0.9240935870703554
Loss in iteration 329 : 0.9444265284763173
Loss in iteration 330 : 0.9218083974528458
Loss in iteration 331 : 0.9417637761226725
Loss in iteration 332 : 0.9194554544903789
Loss in iteration 333 : 0.939102306518381
Loss in iteration 334 : 0.9170536325724836
Loss in iteration 335 : 0.936455294800213
Loss in iteration 336 : 0.9146208706811026
Loss in iteration 337 : 0.9338339252789268
Loss in iteration 338 : 0.9121732983189977
Loss in iteration 339 : 0.9312470699117824
Loss in iteration 340 : 0.9097248021729684
Loss in iteration 341 : 0.9287012599924648
Loss in iteration 342 : 0.9072869409728705
Loss in iteration 343 : 0.926200867648603
Loss in iteration 344 : 0.9048691025401979
Loss in iteration 345 : 0.9237484142748501
Loss in iteration 346 : 0.9024788031532949
Loss in iteration 347 : 0.9213449351561634
Loss in iteration 348 : 0.9001220462743724
Loss in iteration 349 : 0.9189903467499232
Loss in iteration 350 : 0.8978036785015435
Loss in iteration 351 : 0.9166837807292796
Loss in iteration 352 : 0.8955277008526574
Loss in iteration 353 : 0.9144238642018574
Loss in iteration 354 : 0.8932975108163514
Loss in iteration 355 : 0.9122089373357025
Loss in iteration 356 : 0.8911160642369458
Loss in iteration 357 : 0.9100372078593675
Loss in iteration 358 : 0.888985956216283
Loss in iteration 359 : 0.9079068470913586
Loss in iteration 360 : 0.8869094274703998
Loss in iteration 361 : 0.905816035103225
Loss in iteration 362 : 0.8848883077423438
Loss in iteration 363 : 0.9037629641575307
Loss in iteration 364 : 0.8829239116005776
Loss in iteration 365 : 0.9017458103718642
Loss in iteration 366 : 0.8810169046624391
Loss in iteration 367 : 0.8997626840960151
Loss in iteration 368 : 0.8791671601097122
Loss in iteration 369 : 0.8978115699331786
Loss in iteration 370 : 0.8773736261887759
Loss in iteration 371 : 0.8958902676163722
Loss in iteration 372 : 0.8756342249195175
Loss in iteration 373 : 0.8939963448046748
Loss in iteration 374 : 0.873945800138092
Loss in iteration 375 : 0.8921271119420492
Loss in iteration 376 : 0.872304129034529
Loss in iteration 377 : 0.8902796273239728
Loss in iteration 378 : 0.8707040055299017
Loss in iteration 379 : 0.8884507373228864
Loss in iteration 380 : 0.8691393965289308
Loss in iteration 381 : 0.886637152490312
Loss in iteration 382 : 0.8676036640152929
Loss in iteration 383 : 0.8848355554429447
Loss in iteration 384 : 0.8660898381586933
Loss in iteration 385 : 0.8830427317601017
Loss in iteration 386 : 0.8645909202325406
Loss in iteration 387 : 0.881255711378821
Loss in iteration 388 : 0.8631001902453661
Loss in iteration 389 : 0.8794719058897796
Loss in iteration 390 : 0.8616114934608952
Loss in iteration 391 : 0.877689227153062
Loss in iteration 392 : 0.8601194825798777
Loss in iteration 393 : 0.8759061748088958
Loss in iteration 394 : 0.8586197978485008
Loss in iteration 395 : 0.8741218841743252
Loss in iteration 396 : 0.8571091748037627
Loss in iteration 397 : 0.8723361309801118
Loss in iteration 398 : 0.8555854775288773
Loss in iteration 399 : 0.870549294540425
Loss in iteration 400 : 0.8540476629063533
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.74525, training accuracy 0.74525, time elapsed: 7494 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.73270219119234
Loss in iteration 3 : 1.5371991797164293
Loss in iteration 4 : 2.8000129953811084
Loss in iteration 5 : 1.0762219539266697
Loss in iteration 6 : 2.272830360333671
Loss in iteration 7 : 1.3164207860778505
Loss in iteration 8 : 2.457406355649102
Loss in iteration 9 : 0.9722867217959397
Loss in iteration 10 : 1.9087949454085833
Loss in iteration 11 : 1.23999520886976
Loss in iteration 12 : 1.9694880148822482
Loss in iteration 13 : 1.05765448026899
Loss in iteration 14 : 1.6450787094282513
Loss in iteration 15 : 1.1406162791653114
Loss in iteration 16 : 1.55413224156481
Loss in iteration 17 : 1.0863614074944312
Loss in iteration 18 : 1.3912436076321524
Loss in iteration 19 : 1.0676903093137957
Loss in iteration 20 : 1.2772395487923682
Loss in iteration 21 : 1.0312741173022042
Loss in iteration 22 : 1.17482124581619
Loss in iteration 23 : 0.9931416076561314
Loss in iteration 24 : 1.0893570691156578
Loss in iteration 25 : 0.9542381412962664
Loss in iteration 26 : 1.0176541753485384
Loss in iteration 27 : 0.9167398075935715
Loss in iteration 28 : 0.9577146167896572
Loss in iteration 29 : 0.8819091045934735
Loss in iteration 30 : 0.9076043903699315
Loss in iteration 31 : 0.8503627237858129
Loss in iteration 32 : 0.8655958856046128
Loss in iteration 33 : 0.822247892080731
Loss in iteration 34 : 0.8302067206952327
Loss in iteration 35 : 0.7974220116234153
Loss in iteration 36 : 0.8002030077937109
Loss in iteration 37 : 0.7755935795042908
Loss in iteration 38 : 0.7745761104309208
Loss in iteration 39 : 0.7564137855820589
Loss in iteration 40 : 0.7525071227715616
Loss in iteration 41 : 0.7395265594128237
Loss in iteration 42 : 0.7333294358487277
Loss in iteration 43 : 0.7245922092871722
Loss in iteration 44 : 0.7164952247805465
Loss in iteration 45 : 0.7113050294575175
Loss in iteration 46 : 0.7015528304734742
Loss in iteration 47 : 0.6994243826450474
Loss in iteration 48 : 0.6881435570056207
Loss in iteration 49 : 0.688806595161593
Loss in iteration 50 : 0.6760079732339785
Loss in iteration 51 : 0.6793904348591077
Loss in iteration 52 : 0.6649677185930063
Loss in iteration 53 : 0.6711445403914733
Loss in iteration 54 : 0.6548840015610912
Loss in iteration 55 : 0.6640456277314288
Loss in iteration 56 : 0.6456389503349987
Loss in iteration 57 : 0.6581026076270419
Loss in iteration 58 : 0.6371555765608139
Loss in iteration 59 : 0.6533679160715
Loss in iteration 60 : 0.6294199557382337
Loss in iteration 61 : 0.6498069126506271
Loss in iteration 62 : 0.6224058868938894
Loss in iteration 63 : 0.6468204213238956
Loss in iteration 64 : 0.6157572019118545
Loss in iteration 65 : 0.6425764545575862
Loss in iteration 66 : 0.6084712385598998
Loss in iteration 67 : 0.6346156386803836
Loss in iteration 68 : 0.5996413591408939
Loss in iteration 69 : 0.6226630375163291
Loss in iteration 70 : 0.5900988530592753
Loss in iteration 71 : 0.6100432091453111
Loss in iteration 72 : 0.5825046355936065
Loss in iteration 73 : 0.6014389059726661
Loss in iteration 74 : 0.579784865059297
Loss in iteration 75 : 0.6003483188020429
Loss in iteration 76 : 0.5832152881219145
Loss in iteration 77 : 0.6060091210360906
Loss in iteration 78 : 0.5885711163941578
Loss in iteration 79 : 0.6082330992424361
Loss in iteration 80 : 0.585412908479922
Loss in iteration 81 : 0.5957882947997816
Loss in iteration 82 : 0.5707849546699209
Loss in iteration 83 : 0.5740319629804758
Loss in iteration 84 : 0.5527935539523771
Loss in iteration 85 : 0.5538954178114186
Loss in iteration 86 : 0.5378545512745265
Loss in iteration 87 : 0.5395373430021239
Loss in iteration 88 : 0.5276734366926669
Loss in iteration 89 : 0.5310751370226663
Loss in iteration 90 : 0.5222761538524815
Loss in iteration 91 : 0.5281723251622402
Loss in iteration 92 : 0.5218322756612829
Loss in iteration 93 : 0.5311004657340277
Loss in iteration 94 : 0.5270748146817904
Loss in iteration 95 : 0.540575605838452
Loss in iteration 96 : 0.5390965016028869
Loss in iteration 97 : 0.5570305181728499
Loss in iteration 98 : 0.5588114415291512
Loss in iteration 99 : 0.5797574905668155
Loss in iteration 100 : 0.5858889520840629
Loss in iteration 101 : 0.6061745181532496
Loss in iteration 102 : 0.6165418417493724
Loss in iteration 103 : 0.6310247574846398
Loss in iteration 104 : 0.6425766672935692
Loss in iteration 105 : 0.6478274463712903
Loss in iteration 106 : 0.6568956541133577
Loss in iteration 107 : 0.6538709736686594
Loss in iteration 108 : 0.6594016282928683
Loss in iteration 109 : 0.6516002484302085
Loss in iteration 110 : 0.6544607301668289
Loss in iteration 111 : 0.6449176213979082
Loss in iteration 112 : 0.6461934266007394
Loss in iteration 113 : 0.6366204287739974
Loss in iteration 114 : 0.6370506422219748
Loss in iteration 115 : 0.6281993471442185
Loss in iteration 116 : 0.6282106479738608
Loss in iteration 117 : 0.6203246205011846
Loss in iteration 118 : 0.6201509419442756
Loss in iteration 119 : 0.6132411913983041
Loss in iteration 120 : 0.6130077294815143
Loss in iteration 121 : 0.6069910100728297
Loss in iteration 122 : 0.6067625513424216
Loss in iteration 123 : 0.6015255146163113
Loss in iteration 124 : 0.601333273339261
Loss in iteration 125 : 0.5967606534698229
Loss in iteration 126 : 0.5966172842379046
Loss in iteration 127 : 0.5926033138563853
Loss in iteration 128 : 0.5925114267575797
Loss in iteration 129 : 0.5889636051335142
Loss in iteration 130 : 0.5889206382023485
Loss in iteration 131 : 0.5857601156676858
Loss in iteration 132 : 0.5857611218024003
Loss in iteration 133 : 0.5829216824222012
Loss in iteration 134 : 0.582960902604259
Loss in iteration 135 : 0.5803874581513803
Loss in iteration 136 : 0.5804591809110391
Loss in iteration 137 : 0.5781061869198798
Loss in iteration 138 : 0.5782051865581465
Loss in iteration 139 : 0.576035155170169
Loss in iteration 140 : 0.5761568827300592
Loss in iteration 141 : 0.5741390570263202
Loss in iteration 142 : 0.5742796888445224
Loss in iteration 143 : 0.5723888932542811
Loss in iteration 144 : 0.5725453007733419
Loss in iteration 145 : 0.5707609604149176
Loss in iteration 146 : 0.570930640003505
Loss in iteration 147 : 0.5692359533322547
Loss in iteration 148 : 0.5694169395058888
Loss in iteration 149 : 0.5677981861866199
Loss in iteration 150 : 0.5679889620384936
Loss in iteration 151 : 0.5664349281202298
Loss in iteration 152 : 0.5666343407426987
Loss in iteration 153 : 0.5651358444289313
Loss in iteration 154 : 0.5653430293111656
Loss in iteration 155 : 0.5638925321994114
Loss in iteration 156 : 0.5641068482219541
Loss in iteration 157 : 0.5626981385430013
Loss in iteration 158 : 0.562919113754027
Loss in iteration 159 : 0.5615470497586931
Loss in iteration 160 : 0.5617743372878943
Loss in iteration 161 : 0.560434640461531
Loss in iteration 162 : 0.5606679834945972
Loss in iteration 163 : 0.5593570727073234
Loss in iteration 164 : 0.5595962772654567
Loss in iteration 165 : 0.5583111362777553
Loss in iteration 166 : 0.5585560505270373
Loss in iteration 167 : 0.5572941224586364
Loss in iteration 168 : 0.557544621348331
Loss in iteration 169 : 0.5563037247791556
Loss in iteration 170 : 0.5565596989325593
Loss in iteration 171 : 0.555337961237365
Loss in iteration 172 : 0.5555993091648399
Loss in iteration 173 : 0.5543951134910917
Loss in iteration 174 : 0.5546617363439067
Loss in iteration 175 : 0.553473679331878
Loss in iteration 176 : 0.5537454775566045
Loss in iteration 177 : 0.552572335480616
Loss in iteration 178 : 0.552849206860745
Loss in iteration 179 : 0.5516899083516256
Loss in iteration 180 : 0.5519717470333012
Loss in iteration 181 : 0.5508253509361863
Loss in iteration 182 : 0.5511120471278741
Loss in iteration 183 : 0.5499777243681232
Loss in iteration 184 : 0.5502691644805031
Loss in iteration 185 : 0.5491461830651981
Loss in iteration 186 : 0.5494422501191748
Loss in iteration 187 : 0.548329962602879
Loss in iteration 188 : 0.5486305367822254
Loss in iteration 189 : 0.5475283696829837
Loss in iteration 190 : 0.5478333289457297
Loss in iteration 191 : 0.5467407737189336
Loss in iteration 192 : 0.5470499944102426
Loss in iteration 193 : 0.5459665996811632
Loss in iteration 194 : 0.5462799571115435
Loss in iteration 195 : 0.5452053219380875
Loss in iteration 196 : 0.5455226909062365
Loss in iteration 197 : 0.5444564588966766
Loss in iteration 198 : 0.5447777141470553
Loss in iteration 199 : 0.5437195682972129
Loss in iteration 200 : 0.5440445849098653
Loss in iteration 201 : 0.5429942430538015
Loss in iteration 202 : 0.5433228967686577
Loss in iteration 203 : 0.5422801075587234
Loss in iteration 204 : 0.5426122750395356
Loss in iteration 205 : 0.5415768143877943
Loss in iteration 206 : 0.541912373432418
Loss in iteration 207 : 0.5408840413574003
Loss in iteration 208 : 0.541222871061638
Loss in iteration 209 : 0.5402014888933981
Loss in iteration 210 : 0.5405434697756545
Loss in iteration 211 : 0.5395288776787683
Loss in iteration 212 : 0.539873891772324
Loss in iteration 213 : 0.5388659465518376
Loss in iteration 214 : 0.5392138774708806
Loss in iteration 215 : 0.5382124506302841
Loss in iteration 216 : 0.5385631836151132
Loss in iteration 217 : 0.5375681596388672
Loss in iteration 218 : 0.537921581584874
Loss in iteration 219 : 0.536932856420859
Loss in iteration 220 : 0.5372888558951758
Loss in iteration 221 : 0.5363063356149357
Loss in iteration 222 : 0.5366648028639445
Loss in iteration 223 : 0.5356884024807411
Loss in iteration 224 : 0.536049229430977
Loss in iteration 225 : 0.5350788718577337
Loss in iteration 226 : 0.5354419521122523
Loss in iteration 227 : 0.5344775672431852
Loss in iteration 228 : 0.534842796074951
Loss in iteration 229 : 0.5338843199763983
Loss in iteration 230 : 0.5342515943198998
Loss in iteration 231 : 0.5332989685173541
Loss in iteration 232 : 0.5336681869593244
Loss in iteration 233 : 0.5327213578090878
Loss in iteration 234 : 0.533092420578992
Loss in iteration 235 : 0.5321513387141232
Loss in iteration 236 : 0.5325241476748214
Loss in iteration 237 : 0.5315887675162653
Loss in iteration 238 : 0.5319632261551617
Loss in iteration 239 : 0.5310335054799595
Loss in iteration 240 : 0.5314095189007985
Loss in iteration 241 : 0.5304854184602714
Loss in iteration 242 : 0.5308628933756542
Loss in iteration 243 : 0.529944376557318
Loss in iteration 244 : 0.5303232212819553
Loss in iteration 245 : 0.5294102538096835
Loss in iteration 246 : 0.5297903782543738
Loss in iteration 247 : 0.5288829279220009
Loss in iteration 248 : 0.5292642435882441
Loss in iteration 249 : 0.5283622800224675
Loss in iteration 250 : 0.5287446999976494
Loss in iteration 251 : 0.5278481944465709
Loss in iteration 252 : 0.528231633399633
Loss in iteration 253 : 0.5273405585437732
Loss in iteration 254 : 0.5277249327212558
Loss in iteration 255 : 0.5268392625043044
Loss in iteration 256 : 0.5272244897266765
Loss in iteration 257 : 0.5263441992035927
Loss in iteration 258 : 0.5267301988617714
Loss in iteration 259 : 0.5258552640621581
Loss in iteration 260 : 0.5262419571141395
Loss in iteration 261 : 0.5253723549190943
Loss in iteration 262 : 0.5257596638865905
Loss in iteration 263 : 0.5248953719174937
Loss in iteration 264 : 0.5252832208825232
Loss in iteration 265 : 0.5244242174003855
Loss in iteration 266 : 0.5248125320017519
Loss in iteration 267 : 0.5239587958159573
Loss in iteration 268 : 0.5243475032455291
Loss in iteration 269 : 0.5234990136309686
Loss in iteration 270 : 0.523888042629737
Loss in iteration 271 : 0.5230447792514091
Loss in iteration 272 : 0.5234340601052679
Loss in iteration 273 : 0.5225960029495942
Loss in iteration 274 : 0.5229854674847896
Loss in iteration 275 : 0.5221525967969641
Loss in iteration 276 : 0.522542178375209
Loss in iteration 277 : 0.5217144746019724
Loss in iteration 278 : 0.5221041081151757
Loss in iteration 279 : 0.5212815518525041
Loss in iteration 280 : 0.5216711737170983
Loss in iteration 281 : 0.5208537456623559
Loss in iteration 282 : 0.521243293813206
Loss in iteration 283 : 0.5204309747213272
Loss in iteration 284 : 0.5208203886052172
Loss in iteration 285 : 0.5200131592485934
Loss in iteration 286 : 0.5204023798172666
Loss in iteration 287 : 0.5196002209489943
Loss in iteration 288 : 0.5199891906517446
Loss in iteration 289 : 0.5191920829719661
Loss in iteration 290 : 0.5195807457477788
Loss in iteration 291 : 0.5187886698728664
Loss in iteration 292 : 0.5191769711421211
Loss in iteration 293 : 0.5183899075764512
Loss in iteration 294 : 0.5187777942321552
Loss in iteration 295 : 0.5179957233423145
Loss in iteration 296 : 0.5183831437409079
Loss in iteration 297 : 0.5176060457321119
Loss in iteration 298 : 0.5179929496838217
Loss in iteration 299 : 0.5172208045784039
Loss in iteration 300 : 0.517607143337171
Loss in iteration 301 : 0.5168399309549764
Loss in iteration 302 : 0.517225657207948
Loss in iteration 303 : 0.5164633571485279
Loss in iteration 304 : 0.5168484250051408
Loss in iteration 305 : 0.5160910166315789
Loss in iteration 306 : 0.5164753816122379
Loss in iteration 307 : 0.5157228440365509
Loss in iteration 308 : 0.5161064630608933
Loss in iteration 309 : 0.5153587751308653
Loss in iteration 310 : 0.5157416065056586
Loss in iteration 311 : 0.5149987467930385
Loss in iteration 312 : 0.5153807501996903
Loss in iteration 313 : 0.5146426969896524
Loss in iteration 314 : 0.5150238334713635
Loss in iteration 315 : 0.5142905647531731
Loss in iteration 316 : 0.5146707967017276
Loss in iteration 317 : 0.5139422901605253
Loss in iteration 318 : 0.5143215813027558
Loss in iteration 319 : 0.5135978143124044
Loss in iteration 320 : 0.5139761296963083
Loss in iteration 321 : 0.5132570793132332
Loss in iteration 322 : 0.5136343852937869
Loss in iteration 323 : 0.5129200282517667
Loss in iteration 324 : 0.5132962924764245
Loss in iteration 325 : 0.512586605182277
Loss in iteration 326 : 0.5129617965761712
Loss in iteration 327 : 0.5122567551062825
Loss in iteration 328 : 0.5126308438571407
Loss in iteration 329 : 0.5119304239547978
Loss in iteration 330 : 0.5123033814975942
Loss in iteration 331 : 0.5116075585710844
Loss in iteration 332 : 0.5119793575724025
Loss in iteration 333 : 0.5112881066938406
Loss in iteration 334 : 0.5116587210359991
Loss in iteration 335 : 0.5109720169408468
Loss in iteration 336 : 0.511341421705774
Loss in iteration 337 : 0.5106592387930143
Loss in iteration 338 : 0.5110274102458777
Loss in iteration 339 : 0.5103497225788332
Loss in iteration 340 : 0.5107166381514473
Loss in iteration 341 : 0.5100434194591887
Loss in iteration 342 : 0.5104090577332003
Loss in iteration 343 : 0.5097402814125435
Loss in iteration 344 : 0.5101046221023984
Loss in iteration 345 : 0.5094402612204473
Loss in iteration 346 : 0.509803285156157
Loss in iteration 347 : 0.5091433124533948
Loss in iteration 348 : 0.5095050015630955
Loss in iteration 349 : 0.5088493894569625
Loss in iteration 350 : 0.5092097267492992
Loss in iteration 351 : 0.5085584473382851
Loss in iteration 352 : 0.508917416884589
Loss in iteration 353 : 0.5082704419527821
Loss in iteration 354 : 0.5086280288690952
Loss in iteration 355 : 0.5079853298911972
Loss in iteration 356 : 0.5083415203200954
Loss in iteration 357 : 0.5077030684668727
Loss in iteration 358 : 0.5080578495591535
Loss in iteration 359 : 0.5074236157033064
Loss in iteration 360 : 0.5077769755994928
Loss in iteration 361 : 0.5071469303219431
Loss in iteration 362 : 0.5074988581336485
Loss in iteration 363 : 0.5068729717302128
Loss in iteration 364 : 0.5072234575213563
Loss in iteration 365 : 0.5066017000097948
Loss in iteration 366 : 0.5069507347776766
Loss in iteration 367 : 0.5063330759051314
Loss in iteration 368 : 0.5066806515613606
Loss in iteration 369 : 0.5060670608121173
Loss in iteration 370 : 0.5064131701634355
Loss in iteration 371 : 0.5058036167670471
Loss in iteration 372 : 0.5061482534960051
Loss in iteration 373 : 0.5055427064357373
Loss in iteration 374 : 0.5058858650812734
Loss in iteration 375 : 0.5052842931028676
Loss in iteration 376 : 0.5056259690407693
Loss in iteration 377 : 0.5050283406615079
Loss in iteration 378 : 0.5053685300847675
Loss in iteration 379 : 0.5047748136028339
Loss in iteration 380 : 0.5051135135019207
Loss in iteration 381 : 0.5045236770060377
Loss in iteration 382 : 0.5048608851490696
Loss in iteration 383 : 0.5042748965284082
Loss in iteration 384 : 0.5046106114412536
Loss in iteration 385 : 0.5040284383955963
Loss in iteration 386 : 0.5043626593418946
Loss in iteration 387 : 0.503784269392046
Loss in iteration 388 : 0.5041169963531624
Loss in iteration 389 : 0.5035423568515885
Loss in iteration 390 : 0.5038735905065311
Loss in iteration 391 : 0.5033026686482185
Loss in iteration 392 : 0.5036324103534777
Loss in iteration 393 : 0.5030651731870046
Loss in iteration 394 : 0.5033934249563875
Loss in iteration 395 : 0.5028298393951748
Loss in iteration 396 : 0.5031566038795856
Loss in iteration 397 : 0.5025966367133569
Loss in iteration 398 : 0.5029219171805677
Loss in iteration 399 : 0.5023655350869453
Loss in iteration 400 : 0.5026893354013661
Testing accuracy  of updater 3 on alg 0 with rate 7.0 = 0.767625, training accuracy 0.767625, time elapsed: 7312 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6753776413573576
Loss in iteration 3 : 0.6691076907399485
Loss in iteration 4 : 0.6751928597439223
Loss in iteration 5 : 0.681022717701567
Loss in iteration 6 : 0.7082785333526286
Loss in iteration 7 : 0.6952201536159562
Loss in iteration 8 : 0.7222167371576282
Loss in iteration 9 : 0.6794435670852979
Loss in iteration 10 : 0.6953512579608164
Loss in iteration 11 : 0.650448276828776
Loss in iteration 12 : 0.6564572181376542
Loss in iteration 13 : 0.6213697041176532
Loss in iteration 14 : 0.6207310918325283
Loss in iteration 15 : 0.5956760745104728
Loss in iteration 16 : 0.5914498944924258
Loss in iteration 17 : 0.5742210320068646
Loss in iteration 18 : 0.5686829912623739
Loss in iteration 19 : 0.557085051228856
Loss in iteration 20 : 0.5516764751649758
Loss in iteration 21 : 0.543972135107175
Loss in iteration 22 : 0.5394130098915892
Loss in iteration 23 : 0.5343153843602457
Loss in iteration 24 : 0.5307856840004522
Loss in iteration 25 : 0.5273634894508924
Loss in iteration 26 : 0.5247231879868388
Loss in iteration 27 : 0.5223236210997559
Loss in iteration 28 : 0.5203195408173292
Loss in iteration 29 : 0.5185148210327138
Loss in iteration 30 : 0.5169166657604144
Loss in iteration 31 : 0.5154518298442246
Loss in iteration 32 : 0.514102648469197
Loss in iteration 33 : 0.512840079007965
Loss in iteration 34 : 0.5116504086732131
Loss in iteration 35 : 0.5105212588895262
Loss in iteration 36 : 0.5094448425432673
Loss in iteration 37 : 0.5084152705445806
Loss in iteration 38 : 0.5074281794710663
Loss in iteration 39 : 0.5064802320879294
Loss in iteration 40 : 0.5055686563775688
Loss in iteration 41 : 0.5046911585187821
Loss in iteration 42 : 0.5038456952345883
Loss in iteration 43 : 0.5030304626328542
Loss in iteration 44 : 0.5022438019675097
Loss in iteration 45 : 0.5014841987989969
Loss in iteration 46 : 0.5007502443323993
Loss in iteration 47 : 0.5000406333287256
Loss in iteration 48 : 0.4993541464838429
Loss in iteration 49 : 0.49868964664696036
Loss in iteration 50 : 0.49804606919739625
Loss in iteration 51 : 0.4974224176805915
Loss in iteration 52 : 0.4968177575110617
Loss in iteration 53 : 0.4962312117728976
Loss in iteration 54 : 0.4956619565523302
Loss in iteration 55 : 0.4951092172034361
Loss in iteration 56 : 0.4945722646449547
Loss in iteration 57 : 0.49405041214848056
Loss in iteration 58 : 0.49354301229942815
Loss in iteration 59 : 0.4930494542679267
Loss in iteration 60 : 0.49256916127199846
Loss in iteration 61 : 0.49210158826452405
Loss in iteration 62 : 0.49164621979527534
Loss in iteration 63 : 0.49120256804753065
Loss in iteration 64 : 0.4907701710248065
Loss in iteration 65 : 0.49034859087906146
Loss in iteration 66 : 0.4899374123652562
Loss in iteration 67 : 0.48953624141266217
Loss in iteration 68 : 0.48914470380208874
Loss in iteration 69 : 0.4887624439404266
Loss in iteration 70 : 0.4883891237240755
Loss in iteration 71 : 0.48802442148397973
Loss in iteration 72 : 0.4876680310054959
Loss in iteration 73 : 0.48731966061703824
Loss in iteration 74 : 0.4869790323419529
Loss in iteration 75 : 0.4866458811085778
Loss in iteration 76 : 0.4863199540139328
Loss in iteration 77 : 0.4860010096368487
Loss in iteration 78 : 0.4856888173967211
Loss in iteration 79 : 0.48538315695443707
Loss in iteration 80 : 0.48508381765224945
Loss in iteration 81 : 0.48479059798974977
Loss in iteration 82 : 0.48450330513320977
Loss in iteration 83 : 0.4842217544559064
Loss in iteration 84 : 0.4839457691071395
Loss in iteration 85 : 0.48367517960792406
Loss in iteration 86 : 0.4834098234714395
Loss in iteration 87 : 0.48314954484650513
Loss in iteration 88 : 0.4828941941824699
Loss in iteration 89 : 0.4826436279140526
Loss in iteration 90 : 0.48239770816475813
Loss in iteration 91 : 0.4821563024676217
Loss in iteration 92 : 0.48191928350210994
Loss in iteration 93 : 0.48168652884611124
Loss in iteration 94 : 0.4814579207420245
Loss in iteration 95 : 0.48123334587602096
Loss in iteration 96 : 0.4810126951696219
Loss in iteration 97 : 0.4807958635828295
Loss in iteration 98 : 0.48058274992802874
Loss in iteration 99 : 0.48037325669404574
Loss in iteration 100 : 0.4801672898796654
Loss in iteration 101 : 0.47996475883605355
Loss in iteration 102 : 0.4797655761175597
Loss in iteration 103 : 0.4795696573403337
Loss in iteration 104 : 0.47937692104833207
Loss in iteration 105 : 0.4791872885862532
Loss in iteration 106 : 0.47900068397899015
Loss in iteration 107 : 0.47881703381722
Loss in iteration 108 : 0.4786362671487697
Loss in iteration 109 : 0.4784583153754246
Loss in iteration 110 : 0.47828311215486097
Loss in iteration 111 : 0.47811059330742756
Loss in iteration 112 : 0.4779406967274721
Loss in iteration 113 : 0.4777733622989913
Loss in iteration 114 : 0.47760853181532886
Loss in iteration 115 : 0.47744614890273007
Loss in iteration 116 : 0.47728615894751797
Loss in iteration 117 : 0.4771285090267046
Loss in iteration 118 : 0.4769731478418435
Loss in iteration 119 : 0.47682002565596177
Loss in iteration 120 : 0.47666909423338905
Loss in iteration 121 : 0.47652030678235235
Loss in iteration 122 : 0.47637361790017096
Loss in iteration 123 : 0.47622898352093623
Loss in iteration 124 : 0.4760863608655131
Loss in iteration 125 : 0.4759457083937862
Loss in iteration 126 : 0.4758069857590175
Loss in iteration 127 : 0.47567015376418453
Loss in iteration 128 : 0.47553517432024933
Loss in iteration 129 : 0.47540201040620944
Loss in iteration 130 : 0.4752706260308846
Loss in iteration 131 : 0.4751409861963184
Loss in iteration 132 : 0.4750130568627497
Loss in iteration 133 : 0.4748868049150476
Loss in iteration 134 : 0.4747621981305515
Loss in iteration 135 : 0.47463920514825747
Loss in iteration 136 : 0.47451779543926725
Loss in iteration 137 : 0.47439793927845486
Loss in iteration 138 : 0.47427960771729216
Loss in iteration 139 : 0.47416277255776407
Loss in iteration 140 : 0.47404740632734294
Loss in iteration 141 : 0.47393348225496773
Loss in iteration 142 : 0.47382097424796427
Loss in iteration 143 : 0.4737098568698905
Loss in iteration 144 : 0.4736001053192391
Loss in iteration 145 : 0.47349169540898356
Loss in iteration 146 : 0.47338460354691025
Loss in iteration 147 : 0.4732788067167025
Loss in iteration 148 : 0.4731742824597646
Loss in iteration 149 : 0.47307100885771847
Loss in iteration 150 : 0.47296896451558257
Loss in iteration 151 : 0.4728681285455628
Loss in iteration 152 : 0.4727684805514659
Loss in iteration 153 : 0.4726700006136797
Loss in iteration 154 : 0.47257266927471986
Loss in iteration 155 : 0.47247646752529393
Loss in iteration 156 : 0.47238137679088255
Loss in iteration 157 : 0.4722873789188121
Loss in iteration 158 : 0.4721944561657708
Loss in iteration 159 : 0.4721025911858078
Loss in iteration 160 : 0.4720117670187218
Loss in iteration 161 : 0.47192196707889955
Loss in iteration 162 : 0.4718331751445055
Loss in iteration 163 : 0.47174537534709077
Loss in iteration 164 : 0.4716585521615382
Loss in iteration 165 : 0.47157269039635985
Loss in iteration 166 : 0.4714877751843376
Loss in iteration 167 : 0.47140379197347465
Loss in iteration 168 : 0.47132072651826057
Loss in iteration 169 : 0.47123856487122534
Loss in iteration 170 : 0.4711572933747889
Loss in iteration 171 : 0.4710768986533737
Loss in iteration 172 : 0.4709973676057773
Loss in iteration 173 : 0.470918687397816
Loss in iteration 174 : 0.47084084545518307
Loss in iteration 175 : 0.4707638294565654
Loss in iteration 176 : 0.47068762732696884
Loss in iteration 177 : 0.4706122272312675
Loss in iteration 178 : 0.47053761756795387
Loss in iteration 179 : 0.47046378696309565
Loss in iteration 180 : 0.4703907242644842
Loss in iteration 181 : 0.4703184185359638
Loss in iteration 182 : 0.4702468590519397
Loss in iteration 183 : 0.47017603529206803
Loss in iteration 184 : 0.4701059369360917
Loss in iteration 185 : 0.4700365538588618
Loss in iteration 186 : 0.4699678761254902
Loss in iteration 187 : 0.46989989398666
Loss in iteration 188 : 0.46983259787408077
Loss in iteration 189 : 0.46976597839607753
Loss in iteration 190 : 0.4697000263333147
Loss in iteration 191 : 0.46963473263464767
Loss in iteration 192 : 0.4695700884130969
Loss in iteration 193 : 0.469506084941949
Loss in iteration 194 : 0.4694427136509555
Loss in iteration 195 : 0.4693799661226675
Loss in iteration 196 : 0.4693178340888565
Loss in iteration 197 : 0.4692563094270465
Loss in iteration 198 : 0.46919538415715345
Loss in iteration 199 : 0.46913505043820974
Loss in iteration 200 : 0.4690753005651856
Loss in iteration 201 : 0.46901612696591094
Loss in iteration 202 : 0.46895752219806675
Loss in iteration 203 : 0.4688994789462803
Loss in iteration 204 : 0.4688419900192838
Loss in iteration 205 : 0.4687850483471642
Loss in iteration 206 : 0.46872864697868133
Loss in iteration 207 : 0.46867277907867244
Loss in iteration 208 : 0.46861743792551325
Loss in iteration 209 : 0.4685626169086548
Loss in iteration 210 : 0.46850830952622813
Loss in iteration 211 : 0.46845450938271577
Loss in iteration 212 : 0.46840121018667674
Loss in iteration 213 : 0.4683484057485402
Loss in iteration 214 : 0.4682960899784577
Loss in iteration 215 : 0.46824425688420807
Loss in iteration 216 : 0.46819290056915974
Loss in iteration 217 : 0.46814201523028864
Loss in iteration 218 : 0.46809159515624194
Loss in iteration 219 : 0.4680416347254593
Loss in iteration 220 : 0.4679921284043355
Loss in iteration 221 : 0.4679430707454301
Loss in iteration 222 : 0.46789445638574034
Loss in iteration 223 : 0.46784628004498496
Loss in iteration 224 : 0.46779853652396447
Loss in iteration 225 : 0.46775122070294
Loss in iteration 226 : 0.4677043275400632
Loss in iteration 227 : 0.46765785206984867
Loss in iteration 228 : 0.46761178940166687
Loss in iteration 229 : 0.4675661347182995
Loss in iteration 230 : 0.4675208832745036
Loss in iteration 231 : 0.4674760303956347
Loss in iteration 232 : 0.4674315714762837
Loss in iteration 233 : 0.46738750197896695
Loss in iteration 234 : 0.4673438174328168
Loss in iteration 235 : 0.4673005134323479
Loss in iteration 236 : 0.46725758563620523
Loss in iteration 237 : 0.46721502976598034
Loss in iteration 238 : 0.4671728416050292
Loss in iteration 239 : 0.4671310169973358
Loss in iteration 240 : 0.46708955184639184
Loss in iteration 241 : 0.46704844211410207
Loss in iteration 242 : 0.467007683819722
Loss in iteration 243 : 0.4669672730388152
Loss in iteration 244 : 0.46692720590223663
Loss in iteration 245 : 0.4668874785951342
Loss in iteration 246 : 0.4668480873559813
Loss in iteration 247 : 0.4668090284756216
Loss in iteration 248 : 0.4667702982963445
Loss in iteration 249 : 0.46673189321097425
Loss in iteration 250 : 0.46669380966198515
Loss in iteration 251 : 0.4666560441406303
Loss in iteration 252 : 0.4666185931860895
Loss in iteration 253 : 0.46658145338464646
Loss in iteration 254 : 0.4665446213688699
Loss in iteration 255 : 0.4665080938168236
Loss in iteration 256 : 0.46647186745128155
Loss in iteration 257 : 0.4664359390389762
Loss in iteration 258 : 0.4664003053898463
Loss in iteration 259 : 0.466364963356311
Loss in iteration 260 : 0.4663299098325567
Loss in iteration 261 : 0.4662951417538385
Loss in iteration 262 : 0.46626065609579825
Loss in iteration 263 : 0.4662264498737882
Loss in iteration 264 : 0.466192520142227
Loss in iteration 265 : 0.4661588639939466
Loss in iteration 266 : 0.46612547855957237
Loss in iteration 267 : 0.46609236100690077
Loss in iteration 268 : 0.4660595085403017
Loss in iteration 269 : 0.46602691840012334
Loss in iteration 270 : 0.4659945878621159
Loss in iteration 271 : 0.4659625142368609
Loss in iteration 272 : 0.4659306948692226
Loss in iteration 273 : 0.465899127137796
Loss in iteration 274 : 0.4658678084543734
Loss in iteration 275 : 0.4658367362634286
Loss in iteration 276 : 0.46580590804159233
Loss in iteration 277 : 0.4657753212971585
Loss in iteration 278 : 0.46574497356958966
Loss in iteration 279 : 0.4657148624290261
Loss in iteration 280 : 0.4656849854758242
Loss in iteration 281 : 0.4656553403400805
Loss in iteration 282 : 0.46562592468117864
Loss in iteration 283 : 0.4655967361873453
Loss in iteration 284 : 0.4655677725752074
Loss in iteration 285 : 0.46553903158936405
Loss in iteration 286 : 0.4655105110019632
Loss in iteration 287 : 0.4654822086122847
Loss in iteration 288 : 0.4654541222463382
Loss in iteration 289 : 0.4654262497564601
Loss in iteration 290 : 0.4653985890209251
Loss in iteration 291 : 0.46537113794355833
Loss in iteration 292 : 0.4653438944533573
Loss in iteration 293 : 0.46531685650412874
Loss in iteration 294 : 0.46529002207411485
Loss in iteration 295 : 0.4652633891656417
Loss in iteration 296 : 0.46523695580476976
Loss in iteration 297 : 0.46521072004094305
Loss in iteration 298 : 0.4651846799466572
Loss in iteration 299 : 0.46515883361712873
Testing accuracy  of updater 3 on alg 0 with rate 4.0 = 0.78975, training accuracy 0.78975, time elapsed: 5459 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6823725138027207
Loss in iteration 3 : 0.676177968741121
Loss in iteration 4 : 0.6710754506324406
Loss in iteration 5 : 0.6664513058080324
Loss in iteration 6 : 0.6621363642316599
Loss in iteration 7 : 0.6580569920481962
Loss in iteration 8 : 0.6541718767378354
Loss in iteration 9 : 0.6504548312481726
Loss in iteration 10 : 0.6468878838916605
Loss in iteration 11 : 0.6434578445091295
Loss in iteration 12 : 0.6401544396544564
Loss in iteration 13 : 0.6369692563759289
Loss in iteration 14 : 0.6338951269582951
Loss in iteration 15 : 0.6309257613749312
Loss in iteration 16 : 0.628055521744697
Loss in iteration 17 : 0.6252792795798046
Loss in iteration 18 : 0.6225923220802053
Loss in iteration 19 : 0.6199902879775891
Loss in iteration 20 : 0.6174691215300074
Loss in iteration 21 : 0.615025037928763
Loss in iteration 22 : 0.6126544960901408
Loss in iteration 23 : 0.6103541763951892
Loss in iteration 24 : 0.6081209618817295
Loss in iteration 25 : 0.6059519219536794
Loss in iteration 26 : 0.6038442980096718
Loss in iteration 27 : 0.6017954905971733
Loss in iteration 28 : 0.5998030478234991
Loss in iteration 29 : 0.5978646548328629
Loss in iteration 30 : 0.5959781242077711
Loss in iteration 31 : 0.5941413871850668
Loss in iteration 32 : 0.5923524855981622
Loss in iteration 33 : 0.5906095644718897
Loss in iteration 34 : 0.5889108652070781
Loss in iteration 35 : 0.5872547193000784
Loss in iteration 36 : 0.5856395425488947
Loss in iteration 37 : 0.5840638297027515
Loss in iteration 38 : 0.5825261495164502
Loss in iteration 39 : 0.58102514017458
Loss in iteration 40 : 0.5795595050540434
Loss in iteration 41 : 0.578128008796368
Loss in iteration 42 : 0.5767294736638152
Loss in iteration 43 : 0.5753627761558262
Loss in iteration 44 : 0.574026843864344
Loss in iteration 45 : 0.5727206525486254
Loss in iteration 46 : 0.5714432234118069
Loss in iteration 47 : 0.5701936205631062
Loss in iteration 48 : 0.5689709486510239
Loss in iteration 49 : 0.5677743506540869
Loss in iteration 50 : 0.566603005817002
Loss in iteration 51 : 0.5654561277210057
Loss in iteration 52 : 0.56433296247827
Loss in iteration 53 : 0.56323278704102
Loss in iteration 54 : 0.5621549076168678
Loss in iteration 55 : 0.5610986581825359
Loss in iteration 56 : 0.5600633990888263
Loss in iteration 57 : 0.5590485157502874
Loss in iteration 58 : 0.5580534174135221
Loss in iteration 59 : 0.5570775359986275
Loss in iteration 60 : 0.5561203250086763
Loss in iteration 61 : 0.555181258502526
Loss in iteration 62 : 0.5542598301266635
Loss in iteration 63 : 0.5533555522021172
Loss in iteration 64 : 0.5524679548627046
Loss in iteration 65 : 0.5515965852413043
Loss in iteration 66 : 0.5507410067009422
Loss in iteration 67 : 0.5499007981078419
Loss in iteration 68 : 0.5490755531437359
Loss in iteration 69 : 0.5482648796549258
Loss in iteration 70 : 0.5474683990358447
Loss in iteration 71 : 0.5466857456449096
Loss in iteration 72 : 0.5459165662507273
Loss in iteration 73 : 0.5451605195067766
Loss in iteration 74 : 0.5444172754528686
Loss in iteration 75 : 0.5436865150417691
Loss in iteration 76 : 0.5429679296895015
Loss in iteration 77 : 0.5422612208479372
Loss in iteration 78 : 0.5415660995983876
Loss in iteration 79 : 0.5408822862649689
Loss in iteration 80 : 0.5402095100466439
Loss in iteration 81 : 0.5395475086668503
Loss in iteration 82 : 0.5388960280397452
Loss in iteration 83 : 0.5382548219521587
Loss in iteration 84 : 0.5376236517603593
Loss in iteration 85 : 0.5370022861008453
Loss in iteration 86 : 0.536390500614403
Loss in iteration 87 : 0.5357880776827016
Loss in iteration 88 : 0.5351948061767863
Loss in iteration 89 : 0.5346104812168103
Loss in iteration 90 : 0.5340349039424431
Loss in iteration 91 : 0.5334678812933772
Loss in iteration 92 : 0.5329092257994528
Loss in iteration 93 : 0.532358755379859
Loss in iteration 94 : 0.5318162931509677
Loss in iteration 95 : 0.5312816672424125
Loss in iteration 96 : 0.5307547106209234
Loss in iteration 97 : 0.530235260921592
Loss in iteration 98 : 0.5297231602861763
Loss in iteration 99 : 0.5292182552081135
Loss in iteration 100 : 0.5287203963839029
Loss in iteration 101 : 0.5282294385705686
Loss in iteration 102 : 0.5277452404488971
Loss in iteration 103 : 0.5272676644921914
Loss in iteration 104 : 0.5267965768402647
Loss in iteration 105 : 0.5263318471784472
Loss in iteration 106 : 0.5258733486213659
Loss in iteration 107 : 0.5254209576012662
Loss in iteration 108 : 0.5249745537606931
Loss in iteration 109 : 0.5245340198493118
Loss in iteration 110 : 0.5240992416246946
Loss in iteration 111 : 0.5236701077568818
Loss in iteration 112 : 0.5232465097365775
Loss in iteration 113 : 0.5228283417867783
Loss in iteration 114 : 0.522415500777718
Loss in iteration 115 : 0.5220078861449885
Loss in iteration 116 : 0.5216053998106454
Loss in iteration 117 : 0.5212079461072446
Loss in iteration 118 : 0.5208154317046374
Loss in iteration 119 : 0.5204277655394043
Loss in iteration 120 : 0.5200448587468518
Loss in iteration 121 : 0.5196666245954246
Loss in iteration 122 : 0.5192929784234669
Loss in iteration 123 : 0.5189238375782017
Loss in iteration 124 : 0.5185591213568723
Loss in iteration 125 : 0.5181987509499252
Loss in iteration 126 : 0.51784264938618
Loss in iteration 127 : 0.51749074147988
Loss in iteration 128 : 0.5171429537795691
Loss in iteration 129 : 0.5167992145187051
Loss in iteration 130 : 0.51645945356796
Loss in iteration 131 : 0.5161236023891129
Loss in iteration 132 : 0.5157915939905142
Loss in iteration 133 : 0.5154633628840033
Loss in iteration 134 : 0.5151388450432836
Loss in iteration 135 : 0.5148179778636557
Loss in iteration 136 : 0.5145007001230772
Loss in iteration 137 : 0.5141869519444819
Loss in iteration 138 : 0.5138766747593438
Loss in iteration 139 : 0.5135698112723899
Loss in iteration 140 : 0.5132663054274649
Loss in iteration 141 : 0.5129661023744799
Loss in iteration 142 : 0.512669148437403
Loss in iteration 143 : 0.5123753910832783
Loss in iteration 144 : 0.5120847788921963
Loss in iteration 145 : 0.51179726152822
Loss in iteration 146 : 0.5115127897112083
Loss in iteration 147 : 0.5112313151895106
Loss in iteration 148 : 0.5109527907135022
Loss in iteration 149 : 0.5106771700099284
Loss in iteration 150 : 0.5104044077570375
Loss in iteration 151 : 0.5101344595604558
Loss in iteration 152 : 0.5098672819297984
Loss in iteration 153 : 0.5096028322559751
Loss in iteration 154 : 0.5093410687891888
Loss in iteration 155 : 0.5090819506175616
Loss in iteration 156 : 0.5088254376464191
Loss in iteration 157 : 0.508571490578168
Loss in iteration 158 : 0.5083200708927654
Loss in iteration 159 : 0.5080711408287675
Loss in iteration 160 : 0.5078246633649086
Loss in iteration 161 : 0.5075806022022288
Loss in iteration 162 : 0.5073389217467046
Loss in iteration 163 : 0.5070995870923838
Loss in iteration 164 : 0.5068625640049961
Loss in iteration 165 : 0.5066278189060234
Loss in iteration 166 : 0.5063953188572256
Loss in iteration 167 : 0.5061650315456006
Loss in iteration 168 : 0.5059369252687614
Loss in iteration 169 : 0.5057109689207118
Loss in iteration 170 : 0.5054871319780407
Loss in iteration 171 : 0.505265384486463
Loss in iteration 172 : 0.5050456970477594
Loss in iteration 173 : 0.5048280408070508
Loss in iteration 174 : 0.5046123874404322
Loss in iteration 175 : 0.504398709142939
Loss in iteration 176 : 0.5041869786168256
Loss in iteration 177 : 0.5039771690601799
Loss in iteration 178 : 0.5037692541558171
Loss in iteration 179 : 0.5035632080604903
Loss in iteration 180 : 0.5033590053943775
Loss in iteration 181 : 0.5031566212308379
Loss in iteration 182 : 0.5029560310864503
Loss in iteration 183 : 0.5027572109113055
Loss in iteration 184 : 0.5025601370795517
Loss in iteration 185 : 0.5023647863801863
Loss in iteration 186 : 0.5021711360080813
Loss in iteration 187 : 0.5019791635552417
Loss in iteration 188 : 0.5017888470022922
Loss in iteration 189 : 0.50160016471017
Loss in iteration 190 : 0.5014130954120399
Loss in iteration 191 : 0.5012276182054087
Loss in iteration 192 : 0.5010437125444295
Loss in iteration 193 : 0.500861358232417
Loss in iteration 194 : 0.500680535414535
Loss in iteration 195 : 0.5005012245706657
Loss in iteration 196 : 0.5003234065084734
Loss in iteration 197 : 0.5001470623566115
Loss in iteration 198 : 0.499972173558117
Loss in iteration 199 : 0.49979872186396174
Loss in iteration 200 : 0.49962668932675447
Loss in iteration 201 : 0.4994560582946065
Loss in iteration 202 : 0.49928681140513215
Loss in iteration 203 : 0.49911893157960846
Loss in iteration 204 : 0.4989524020172645
Loss in iteration 205 : 0.4987872061897046
Loss in iteration 206 : 0.49862332783548474
Loss in iteration 207 : 0.49846075095478404
Loss in iteration 208 : 0.4982994598042342
Loss in iteration 209 : 0.49813943889184953
Loss in iteration 210 : 0.4979806729720906
Loss in iteration 211 : 0.49782314704102626
Loss in iteration 212 : 0.49766684633162694
Loss in iteration 213 : 0.4975117563091527
Loss in iteration 214 : 0.4973578626666499
Loss in iteration 215 : 0.49720515132056387
Loss in iteration 216 : 0.4970536084064336
Loss in iteration 217 : 0.4969032202746964
Loss in iteration 218 : 0.4967539734865875
Loss in iteration 219 : 0.49660585481012653
Loss in iteration 220 : 0.4964588512162035
Loss in iteration 221 : 0.49631294987473845
Loss in iteration 222 : 0.4961681381509502
Loss in iteration 223 : 0.4960244036016793
Loss in iteration 224 : 0.49588173397182167
Loss in iteration 225 : 0.49574011719081673
Loss in iteration 226 : 0.4955995413692333
Loss in iteration 227 : 0.49545999479541375
Loss in iteration 228 : 0.4953214659322044
Loss in iteration 229 : 0.4951839434137524
Loss in iteration 230 : 0.495047416042372
Loss in iteration 231 : 0.49491187278547016
Loss in iteration 232 : 0.49477730277256693
Loss in iteration 233 : 0.4946436952923431
Loss in iteration 234 : 0.49451103978977945
Loss in iteration 235 : 0.49437932586333966
Loss in iteration 236 : 0.4942485432622284
Loss in iteration 237 : 0.49411868188369035
Loss in iteration 238 : 0.49398973177038535
Loss in iteration 239 : 0.49386168310780293
Loss in iteration 240 : 0.4937345262217424
Loss in iteration 241 : 0.49360825157583404
Loss in iteration 242 : 0.49348284976912793
Loss in iteration 243 : 0.49335831153371795
Loss in iteration 244 : 0.4932346277324267
Loss in iteration 245 : 0.4931117893565291
Loss in iteration 246 : 0.492989787523524
Loss in iteration 247 : 0.49286861347496236
Loss in iteration 248 : 0.49274825857430743
Loss in iteration 249 : 0.4926287143048429
Loss in iteration 250 : 0.4925099722676266
Loss in iteration 251 : 0.492392024179477
Loss in iteration 252 : 0.49227486187101555
Loss in iteration 253 : 0.49215847728473333
Loss in iteration 254 : 0.4920428624731018
Loss in iteration 255 : 0.4919280095967281
Loss in iteration 256 : 0.49181391092253324
Loss in iteration 257 : 0.49170055882198
Loss in iteration 258 : 0.4915879457693279
Loss in iteration 259 : 0.49147606433992985
Loss in iteration 260 : 0.4913649072085539
Loss in iteration 261 : 0.4912544671477381
Loss in iteration 262 : 0.4911447370261856
Loss in iteration 263 : 0.4910357098071942
Loss in iteration 264 : 0.49092737854709323
Loss in iteration 265 : 0.49081973639373677
Loss in iteration 266 : 0.490712776585018
Loss in iteration 267 : 0.49060649244739846
Loss in iteration 268 : 0.4905008773944922
Loss in iteration 269 : 0.49039592492564726
Loss in iteration 270 : 0.49029162862458076
Loss in iteration 271 : 0.49018798215802056
Loss in iteration 272 : 0.49008497927438316
Loss in iteration 273 : 0.4899826138024723
Loss in iteration 274 : 0.4898808796502089
Loss in iteration 275 : 0.4897797708033716
Loss in iteration 276 : 0.48967928132437216
Loss in iteration 277 : 0.4895794053510532
Loss in iteration 278 : 0.4894801370955049
Loss in iteration 279 : 0.48938147084289746
Loss in iteration 280 : 0.48928340095035366
Loss in iteration 281 : 0.4891859218458186
Loss in iteration 282 : 0.48908902802697524
Loss in iteration 283 : 0.48899271406015504
Loss in iteration 284 : 0.48889697457928777
Loss in iteration 285 : 0.48880180428486253
Loss in iteration 286 : 0.48870719794290496
Loss in iteration 287 : 0.4886131503839788
Loss in iteration 288 : 0.48851965650220575
Loss in iteration 289 : 0.4884267112542952
Loss in iteration 290 : 0.48833430965860136
Loss in iteration 291 : 0.4882424467941894
Loss in iteration 292 : 0.4881511177999231
Loss in iteration 293 : 0.48806031787356835
Loss in iteration 294 : 0.4879700422709065
Loss in iteration 295 : 0.4878802863048768
Loss in iteration 296 : 0.48779104534471707
Loss in iteration 297 : 0.4877023148151355
Loss in iteration 298 : 0.48761409019548296
Loss in iteration 299 : 0.48752636701894947
Loss in iteration 300 : 0.48743914087177553
Loss in iteration 301 : 0.4873524073924667
Loss in iteration 302 : 0.4872661622710315
Loss in iteration 303 : 0.48718040124823014
Loss in iteration 304 : 0.48709512011483846
Loss in iteration 305 : 0.4870103147109131
Loss in iteration 306 : 0.48692598092508843
Loss in iteration 307 : 0.4868421146938677
Loss in iteration 308 : 0.4867587120009388
Loss in iteration 309 : 0.48667576887649155
Loss in iteration 310 : 0.4865932813965561
Loss in iteration 311 : 0.48651124568234566
Loss in iteration 312 : 0.4864296578996147
Loss in iteration 313 : 0.48634851425802755
Loss in iteration 314 : 0.4862678110105278
Loss in iteration 315 : 0.4861875444527384
Loss in iteration 316 : 0.4861077109223521
Loss in iteration 317 : 0.4860283067985433
Loss in iteration 318 : 0.48594932850138717
Loss in iteration 319 : 0.48587077249128446
Loss in iteration 320 : 0.4857926352684004
Loss in iteration 321 : 0.48571491337211625
Loss in iteration 322 : 0.48563760338047973
Loss in iteration 323 : 0.4855607019096689
Loss in iteration 324 : 0.4854842056134722
Loss in iteration 325 : 0.485408111182767
Loss in iteration 326 : 0.4853324153450061
Loss in iteration 327 : 0.48525711486372836
Loss in iteration 328 : 0.48518220653805405
Loss in iteration 329 : 0.48510768720220815
Loss in iteration 330 : 0.48503355372503704
Loss in iteration 331 : 0.4849598030095425
Loss in iteration 332 : 0.48488643199242437
Loss in iteration 333 : 0.4848134376436144
Loss in iteration 334 : 0.4847408169658408
Loss in iteration 335 : 0.4846685669941858
Loss in iteration 336 : 0.48459668479564333
Loss in iteration 337 : 0.48452516746870855
Loss in iteration 338 : 0.48445401214294204
Loss in iteration 339 : 0.4843832159785711
Loss in iteration 340 : 0.4843127761660748
Loss in iteration 341 : 0.48424268992578784
Loss in iteration 342 : 0.48417295450750364
Loss in iteration 343 : 0.4841035671900944
Loss in iteration 344 : 0.4840345252811152
Loss in iteration 345 : 0.48396582611644806
Loss in iteration 346 : 0.48389746705991415
Loss in iteration 347 : 0.48382944550292134
Loss in iteration 348 : 0.4837617588641002
Loss in iteration 349 : 0.48369440458895563
Loss in iteration 350 : 0.483627380149512
Loss in iteration 351 : 0.4835606830439808
Loss in iteration 352 : 0.4834943107964162
Loss in iteration 353 : 0.4834282609563884
Loss in iteration 354 : 0.48336253109865107
Loss in iteration 355 : 0.4832971188228267
Loss in iteration 356 : 0.4832320217530835
Loss in iteration 357 : 0.4831672375378281
Loss in iteration 358 : 0.4831027638493945
Loss in iteration 359 : 0.48303859838374547
Loss in iteration 360 : 0.48297473886016934
Loss in iteration 361 : 0.4829111830209874
Loss in iteration 362 : 0.48284792863126985
Loss in iteration 363 : 0.4827849734785482
Loss in iteration 364 : 0.4827223153725248
Loss in iteration 365 : 0.482659952144812
Loss in iteration 366 : 0.4825978816486514
Loss in iteration 367 : 0.48253610175864187
Loss in iteration 368 : 0.48247461037048384
Loss in iteration 369 : 0.48241340540071065
Loss in iteration 370 : 0.4823524847864362
Loss in iteration 371 : 0.48229184648509804
Loss in iteration 372 : 0.48223148847421726
Loss in iteration 373 : 0.4821714087511357
Loss in iteration 374 : 0.48211160533279257
Loss in iteration 375 : 0.48205207625547364
Loss in iteration 376 : 0.4819928195745761
Loss in iteration 377 : 0.4819338333643859
Loss in iteration 378 : 0.4818751157178391
Loss in iteration 379 : 0.48181666474630164
Loss in iteration 380 : 0.4817584785793459
Loss in iteration 381 : 0.48170055536453343
Loss in iteration 382 : 0.4816428932671945
Loss in iteration 383 : 0.48158549047022475
Loss in iteration 384 : 0.48152834517386384
Loss in iteration 385 : 0.4814714555954992
Loss in iteration 386 : 0.48141481996945257
Loss in iteration 387 : 0.4813584365467891
Loss in iteration 388 : 0.48130230359511017
Loss in iteration 389 : 0.481246419398365
Loss in iteration 390 : 0.4811907822566541
Loss in iteration 391 : 0.48113539048604
Loss in iteration 392 : 0.4810802424183606
Loss in iteration 393 : 0.4810253364010485
Loss in iteration 394 : 0.48097067079693673
Loss in iteration 395 : 0.48091624398409416
Loss in iteration 396 : 0.4808620543556401
Loss in iteration 397 : 0.48080810031956944
Loss in iteration 398 : 0.48075438029858436
Loss in iteration 399 : 0.4807008927299217
Loss in iteration 400 : 0.4806476360651857
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.781625, training accuracy 0.781625, time elapsed: 7923 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6851676890713962
Loss in iteration 3 : 0.6799861190936313
Loss in iteration 4 : 0.6758780826698086
Loss in iteration 5 : 0.6722626811862711
Loss in iteration 6 : 0.6689264363322782
Loss in iteration 7 : 0.6657794277428204
Loss in iteration 8 : 0.6627771100680341
Loss in iteration 9 : 0.6598938194667527
Loss in iteration 10 : 0.6571129737268679
Loss in iteration 11 : 0.6544229853547884
Loss in iteration 12 : 0.6518153065124345
Loss in iteration 13 : 0.6492833678113034
Loss in iteration 14 : 0.6468219447692182
Loss in iteration 15 : 0.6444267552135439
Loss in iteration 16 : 0.642094193340672
Loss in iteration 17 : 0.6398211496606173
Loss in iteration 18 : 0.6376048869698788
Loss in iteration 19 : 0.6354429537339141
Loss in iteration 20 : 0.633333122834131
Loss in iteration 21 : 0.6312733477050843
Loss in iteration 22 : 0.629261730501755
Loss in iteration 23 : 0.6272964986549375
Loss in iteration 24 : 0.6253759873197773
Loss in iteration 25 : 0.6234986259966172
Loss in iteration 26 : 0.6216629281303674
Loss in iteration 27 : 0.619867482855685
Loss in iteration 28 : 0.6181109483042594
Loss in iteration 29 : 0.6163920460627887
Loss in iteration 30 : 0.6147095564903068
Loss in iteration 31 : 0.6130623146871625
Loss in iteration 32 : 0.6114492069667627
Loss in iteration 33 : 0.6098691677224759
Loss in iteration 34 : 0.6083211766112862
Loss in iteration 35 : 0.606804255996486
Loss in iteration 36 : 0.6053174686063736
Loss in iteration 37 : 0.6038599153765313
Loss in iteration 38 : 0.6024307334507576
Loss in iteration 39 : 0.6010290943212089
Loss in iteration 40 : 0.5996542020923926
Loss in iteration 41 : 0.5983052918563901
Loss in iteration 42 : 0.5969816281691233
Loss in iteration 43 : 0.5956825036188629
Loss in iteration 44 : 0.594407237479616
Loss in iteration 45 : 0.5931551744429048
Loss in iteration 46 : 0.591925683422222
Loss in iteration 47 : 0.5907181564250836
Loss in iteration 48 : 0.5895320074881064
Loss in iteration 49 : 0.5883666716709163
Loss in iteration 50 : 0.5872216041050959
Loss in iteration 51 : 0.5860962790946647
Loss in iteration 52 : 0.5849901892648569
Loss in iteration 53 : 0.5839028447562182
Loss in iteration 54 : 0.5828337724612173
Loss in iteration 55 : 0.5817825153008201
Loss in iteration 56 : 0.5807486315386096
Loss in iteration 57 : 0.5797316941301905
Loss in iteration 58 : 0.578731290105831
Loss in iteration 59 : 0.5777470199843487
Loss in iteration 60 : 0.5767784972164185
Loss in iteration 61 : 0.5758253476556192
Loss in iteration 62 : 0.5748872090555955
Loss in iteration 63 : 0.5739637305918543
Loss in iteration 64 : 0.5730545724067909
Loss in iteration 65 : 0.5721594051766347
Loss in iteration 66 : 0.5712779096990878
Loss in iteration 67 : 0.5704097765005042
Loss in iteration 68 : 0.5695547054615241
Loss in iteration 69 : 0.5687124054601679
Loss in iteration 70 : 0.5678825940314154
Loss in iteration 71 : 0.56706499704241
Loss in iteration 72 : 0.5662593483824062
Loss in iteration 73 : 0.5654653896667318
Loss in iteration 74 : 0.5646828699539694
Loss in iteration 75 : 0.5639115454756974
Loss in iteration 76 : 0.5631511793781342
Loss in iteration 77 : 0.5624015414750506
Loss in iteration 78 : 0.5616624080113939
Loss in iteration 79 : 0.5609335614370721
Loss in iteration 80 : 0.5602147901903819
Loss in iteration 81 : 0.5595058884905909
Loss in iteration 82 : 0.558806656139241
Loss in iteration 83 : 0.5581168983297184
Loss in iteration 84 : 0.5574364254647036
Loss in iteration 85 : 0.5567650529810917
Loss in iteration 86 : 0.5561026011820615
Loss in iteration 87 : 0.5554488950759189
Loss in iteration 88 : 0.5548037642214053
Loss in iteration 89 : 0.5541670425791587
Loss in iteration 90 : 0.5535385683690586
Loss in iteration 91 : 0.5529181839331455
Loss in iteration 92 : 0.5523057356038983
Loss in iteration 93 : 0.5517010735775976
Loss in iteration 94 : 0.5511040517925428
Loss in iteration 95 : 0.5505145278119256
Loss in iteration 96 : 0.5499323627111228
Loss in iteration 97 : 0.549357420969237
Loss in iteration 98 : 0.5487895703646771
Loss in iteration 99 : 0.548228681874618
Loss in iteration 100 : 0.5476746295781476
Loss in iteration 101 : 0.5471272905629752
Loss in iteration 102 : 0.5465865448355133
Loss in iteration 103 : 0.5460522752342157
Loss in iteration 104 : 0.5455243673460071
Loss in iteration 105 : 0.5450027094257043
Loss in iteration 106 : 0.5444871923182809
Loss in iteration 107 : 0.5439777093838625
Loss in iteration 108 : 0.5434741564253502
Loss in iteration 109 : 0.5429764316185407
Loss in iteration 110 : 0.5424844354446762
Loss in iteration 111 : 0.541998070625289
Loss in iteration 112 : 0.5415172420592754
Loss in iteration 113 : 0.5410418567620906
Loss in iteration 114 : 0.5405718238069992
Loss in iteration 115 : 0.5401070542682849
Loss in iteration 116 : 0.5396474611663471
Loss in iteration 117 : 0.5391929594146058
Loss in iteration 118 : 0.5387434657681683
Loss in iteration 119 : 0.538298898774151
Loss in iteration 120 : 0.537859178723626
Loss in iteration 121 : 0.5374242276051131
Loss in iteration 122 : 0.536993969059563
Loss in iteration 123 : 0.5365683283367839
Loss in iteration 124 : 0.5361472322532415
Loss in iteration 125 : 0.5357306091511865
Loss in iteration 126 : 0.5353183888590803
Loss in iteration 127 : 0.5349105026532429
Loss in iteration 128 : 0.5345068832206775
Loss in iteration 129 : 0.5341074646230758
Loss in iteration 130 : 0.5337121822618931
Loss in iteration 131 : 0.5333209728445039
Loss in iteration 132 : 0.5329337743513881
Loss in iteration 133 : 0.5325505260042976
Loss in iteration 134 : 0.5321711682353809
Loss in iteration 135 : 0.5317956426572376
Loss in iteration 136 : 0.5314238920338512
Loss in iteration 137 : 0.5310558602523817
Loss in iteration 138 : 0.5306914922958043
Loss in iteration 139 : 0.5303307342163182
Loss in iteration 140 : 0.5299735331095555
Loss in iteration 141 : 0.5296198370895194
Loss in iteration 142 : 0.5292695952642619
Loss in iteration 143 : 0.5289227577122325
Loss in iteration 144 : 0.528579275459323
Loss in iteration 145 : 0.5282391004565623
Loss in iteration 146 : 0.527902185558421
Loss in iteration 147 : 0.5275684845017475
Loss in iteration 148 : 0.5272379518852803
Loss in iteration 149 : 0.5269105431497376
Loss in iteration 150 : 0.5265862145584532
Loss in iteration 151 : 0.5262649231785369
Loss in iteration 152 : 0.5259466268625742
Loss in iteration 153 : 0.5256312842308057
Loss in iteration 154 : 0.5253188546537967
Loss in iteration 155 : 0.5250092982355736
Loss in iteration 156 : 0.5247025757972212
Loss in iteration 157 : 0.5243986488609041
Loss in iteration 158 : 0.5240974796343346
Loss in iteration 159 : 0.523799030995628
Loss in iteration 160 : 0.5235032664785813
Loss in iteration 161 : 0.5232101502583253
Loss in iteration 162 : 0.522919647137352
Loss in iteration 163 : 0.5226317225319149
Loss in iteration 164 : 0.5223463424587717
Loss in iteration 165 : 0.5220634735222793
Loss in iteration 166 : 0.5217830829018096
Loss in iteration 167 : 0.5215051383394925
Loss in iteration 168 : 0.521229608128279
Loss in iteration 169 : 0.5209564611002936
Loss in iteration 170 : 0.5206856666154895
Loss in iteration 171 : 0.5204171945505817
Loss in iteration 172 : 0.5201510152882747
Loss in iteration 173 : 0.5198870997067265
Loss in iteration 174 : 0.5196254191693152
Loss in iteration 175 : 0.5193659455146232
Loss in iteration 176 : 0.5191086510466888
Loss in iteration 177 : 0.5188535085254871
Loss in iteration 178 : 0.5186004911576478
Loss in iteration 179 : 0.518349572587395
Loss in iteration 180 : 0.5181007268877125
Loss in iteration 181 : 0.5178539285517204
Loss in iteration 182 : 0.5176091524842461
Loss in iteration 183 : 0.5173663739936198
Loss in iteration 184 : 0.5171255687836442
Loss in iteration 185 : 0.5168867129457742
Loss in iteration 186 : 0.5166497829514561
Loss in iteration 187 : 0.5164147556446738
Loss in iteration 188 : 0.5161816082346563
Loss in iteration 189 : 0.5159503182887547
Loss in iteration 190 : 0.5157208637254851
Loss in iteration 191 : 0.5154932228077407
Loss in iteration 192 : 0.5152673741361529
Loss in iteration 193 : 0.5150432966426022
Loss in iteration 194 : 0.514820969583888
Loss in iteration 195 : 0.5146003725355386
Loss in iteration 196 : 0.5143814853857481
Loss in iteration 197 : 0.5141642883294841
Loss in iteration 198 : 0.5139487618626873
Loss in iteration 199 : 0.5137348867766305
Loss in iteration 200 : 0.5135226441523949
Loss in iteration 201 : 0.5133120153554649
Loss in iteration 202 : 0.5131029820304529
Loss in iteration 203 : 0.5128955260959337
Loss in iteration 204 : 0.5126896297393898
Loss in iteration 205 : 0.5124852754122784
Loss in iteration 206 : 0.5122824458251978
Loss in iteration 207 : 0.5120811239431627
Loss in iteration 208 : 0.5118812929809793
Loss in iteration 209 : 0.5116829363987265
Loss in iteration 210 : 0.511486037897326
Loss in iteration 211 : 0.5112905814142173
Loss in iteration 212 : 0.5110965511191154
Loss in iteration 213 : 0.5109039314098662
Loss in iteration 214 : 0.5107127069083921
Loss in iteration 215 : 0.5105228624567094
Loss in iteration 216 : 0.5103343831130519
Loss in iteration 217 : 0.51014725414805
Loss in iteration 218 : 0.5099614610410171
Loss in iteration 219 : 0.5097769894762956
Loss in iteration 220 : 0.5095938253396776
Loss in iteration 221 : 0.509411954714915
Loss in iteration 222 : 0.5092313638802907
Loss in iteration 223 : 0.5090520393052552
Loss in iteration 224 : 0.5088739676471505
Loss in iteration 225 : 0.5086971357479841
Loss in iteration 226 : 0.5085215306312765
Loss in iteration 227 : 0.5083471394989675
Loss in iteration 228 : 0.5081739497284027
Loss in iteration 229 : 0.5080019488693434
Loss in iteration 230 : 0.5078311246410838
Loss in iteration 231 : 0.5076614649295906
Loss in iteration 232 : 0.5074929577847161
Loss in iteration 233 : 0.5073255914174569
Loss in iteration 234 : 0.5071593541972798
Loss in iteration 235 : 0.506994234649493
Loss in iteration 236 : 0.5068302214526651
Loss in iteration 237 : 0.5066673034361038
Loss in iteration 238 : 0.5065054695773772
Loss in iteration 239 : 0.5063447089998919
Loss in iteration 240 : 0.5061850109705033
Loss in iteration 241 : 0.5060263648971896
Loss in iteration 242 : 0.5058687603267623
Loss in iteration 243 : 0.5057121869426168
Loss in iteration 244 : 0.5055566345625367
Loss in iteration 245 : 0.5054020931365284
Loss in iteration 246 : 0.5052485527447078
Loss in iteration 247 : 0.5050960035952221
Loss in iteration 248 : 0.50494443602221
Loss in iteration 249 : 0.5047938404838082
Loss in iteration 250 : 0.504644207560172
Loss in iteration 251 : 0.5044955279515779
Loss in iteration 252 : 0.5043477924765118
Loss in iteration 253 : 0.5042009920698273
Loss in iteration 254 : 0.5040551177809165
Loss in iteration 255 : 0.5039101607719425
Loss in iteration 256 : 0.5037661123160696
Loss in iteration 257 : 0.503622963795756
Loss in iteration 258 : 0.5034807067010547
Loss in iteration 259 : 0.5033393326279685
Loss in iteration 260 : 0.5031988332768189
Loss in iteration 261 : 0.5030592004506416
Loss in iteration 262 : 0.5029204260536321
Loss in iteration 263 : 0.502782502089599
Loss in iteration 264 : 0.5026454206604501
Loss in iteration 265 : 0.5025091739647158
Loss in iteration 266 : 0.502373754296083
Loss in iteration 267 : 0.5022391540419752
Loss in iteration 268 : 0.5021053656821303
Loss in iteration 269 : 0.5019723817872384
Loss in iteration 270 : 0.501840195017572
Loss in iteration 271 : 0.5017087981216588
Loss in iteration 272 : 0.5015781839349767
Loss in iteration 273 : 0.5014483453786603
Loss in iteration 274 : 0.5013192754582456
Loss in iteration 275 : 0.5011909672624288
Loss in iteration 276 : 0.5010634139618367
Loss in iteration 277 : 0.5009366088078446
Loss in iteration 278 : 0.5008105451313908
Loss in iteration 279 : 0.5006852163418152
Loss in iteration 280 : 0.5005606159257372
Loss in iteration 281 : 0.5004367374459242
Loss in iteration 282 : 0.5003135745401988
Loss in iteration 283 : 0.5001911209203677
Loss in iteration 284 : 0.5000693703711449
Loss in iteration 285 : 0.4999483167491268
Loss in iteration 286 : 0.49982795398175356
Loss in iteration 287 : 0.4997082760663092
Loss in iteration 288 : 0.4995892770689302
Loss in iteration 289 : 0.4994709511236311
Loss in iteration 290 : 0.49935329243134313
Loss in iteration 291 : 0.49923629525898217
Loss in iteration 292 : 0.49911995393851416
Loss in iteration 293 : 0.49900426286605126
Loss in iteration 294 : 0.4988892165009534
Loss in iteration 295 : 0.4987748093649471
Loss in iteration 296 : 0.49866103604126566
Loss in iteration 297 : 0.49854789117379084
Loss in iteration 298 : 0.4984353694662217
Loss in iteration 299 : 0.4983234656812471
Loss in iteration 300 : 0.4982121746397355
Loss in iteration 301 : 0.49810149121994096
Loss in iteration 302 : 0.49799141035671746
Loss in iteration 303 : 0.49788192704074546
Loss in iteration 304 : 0.4977730363177806
Loss in iteration 305 : 0.4976647332878974
Loss in iteration 306 : 0.4975570131047631
Loss in iteration 307 : 0.4974498709749103
Loss in iteration 308 : 0.49734330215702743
Loss in iteration 309 : 0.49723730196125654
Loss in iteration 310 : 0.49713186574851337
Loss in iteration 311 : 0.49702698892979885
Loss in iteration 312 : 0.4969226669655387
Loss in iteration 313 : 0.4968188953649251
Loss in iteration 314 : 0.4967156696852686
Loss in iteration 315 : 0.49661298553137034
Loss in iteration 316 : 0.4965108385548866
Loss in iteration 317 : 0.4964092244537192
Loss in iteration 318 : 0.4963081389714039
Loss in iteration 319 : 0.4962075778965219
Loss in iteration 320 : 0.49610753706210137
Loss in iteration 321 : 0.4960080123450453
Loss in iteration 322 : 0.49590899966556706
Loss in iteration 323 : 0.49581049498661467
Loss in iteration 324 : 0.4957124943133324
Loss in iteration 325 : 0.49561499369251166
Loss in iteration 326 : 0.49551798921205537
Loss in iteration 327 : 0.4954214770004501
Loss in iteration 328 : 0.4953254532262519
Loss in iteration 329 : 0.4952299140975658
Loss in iteration 330 : 0.4951348558615542
Loss in iteration 331 : 0.4950402748039279
Loss in iteration 332 : 0.4949461672484713
Loss in iteration 333 : 0.4948525295565481
Loss in iteration 334 : 0.4947593581266338
Loss in iteration 335 : 0.49466664939385396
Loss in iteration 336 : 0.49457439982951346
Loss in iteration 337 : 0.4944826059406525
Loss in iteration 338 : 0.4943912642695962
Loss in iteration 339 : 0.4943003713935128
Loss in iteration 340 : 0.49420992392398905
Loss in iteration 341 : 0.4941199185065943
Loss in iteration 342 : 0.49403035182046473
Loss in iteration 343 : 0.49394122057788514
Loss in iteration 344 : 0.4938525215238908
Loss in iteration 345 : 0.4937642514358527
Loss in iteration 346 : 0.4936764071230862
Loss in iteration 347 : 0.4935889854264623
Loss in iteration 348 : 0.49350198321802324
Loss in iteration 349 : 0.493415397400599
Loss in iteration 350 : 0.49332922490743636
Loss in iteration 351 : 0.4932434627018295
Loss in iteration 352 : 0.4931581077767572
Loss in iteration 353 : 0.4930731571545256
Loss in iteration 354 : 0.4929886078864136
Loss in iteration 355 : 0.4929044570523255
Loss in iteration 356 : 0.49282070176045123
Loss in iteration 357 : 0.492737339146921
Loss in iteration 358 : 0.49265436637548476
Loss in iteration 359 : 0.49257178063716744
Loss in iteration 360 : 0.4924895791499591
Loss in iteration 361 : 0.49240775915848856
Loss in iteration 362 : 0.49232631793371034
Loss in iteration 363 : 0.49224525277259396
Loss in iteration 364 : 0.492164560997822
Loss in iteration 365 : 0.4920842399574781
Loss in iteration 366 : 0.4920042870247601
Loss in iteration 367 : 0.4919246995976808
Loss in iteration 368 : 0.49184547509878357
Loss in iteration 369 : 0.4917666109748519
Loss in iteration 370 : 0.49168810469662744
Loss in iteration 371 : 0.49160995375853883
Loss in iteration 372 : 0.4915321556784223
Loss in iteration 373 : 0.49145470799725327
Loss in iteration 374 : 0.49137760827888294
Loss in iteration 375 : 0.4913008541097685
Loss in iteration 376 : 0.49122444309872626
Loss in iteration 377 : 0.491148372876661
Loss in iteration 378 : 0.4910726410963249
Loss in iteration 379 : 0.49099724543206685
Loss in iteration 380 : 0.4909221835795862
Loss in iteration 381 : 0.49084745325568485
Loss in iteration 382 : 0.4907730521980433
Loss in iteration 383 : 0.49069897816496977
Loss in iteration 384 : 0.49062522893517474
Loss in iteration 385 : 0.49055180230754375
Loss in iteration 386 : 0.4904786961009081
Loss in iteration 387 : 0.4904059081538209
Loss in iteration 388 : 0.49033343632433596
Loss in iteration 389 : 0.4902612784898013
Loss in iteration 390 : 0.49018943254662545
Loss in iteration 391 : 0.49011789641008263
Loss in iteration 392 : 0.49004666801409785
Loss in iteration 393 : 0.48997574531104054
Loss in iteration 394 : 0.48990512627151805
Loss in iteration 395 : 0.4898348088841825
Loss in iteration 396 : 0.4897647911555263
Loss in iteration 397 : 0.48969507110969174
Loss in iteration 398 : 0.4896256467882707
Loss in iteration 399 : 0.4895565162501248
Loss in iteration 400 : 0.48948767757118417
Testing accuracy  of updater 3 on alg 0 with rate 0.7 = 0.77975, training accuracy 0.77975, time elapsed: 7719 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6883460179231655
Loss in iteration 3 : 0.6846582710786797
Loss in iteration 4 : 0.6816397656627601
Loss in iteration 5 : 0.6790352735747982
Loss in iteration 6 : 0.6766970485333521
Loss in iteration 7 : 0.6745382778983326
Loss in iteration 8 : 0.6725068524926681
Loss in iteration 9 : 0.6705706322759447
Loss in iteration 10 : 0.6687091414135934
Loss in iteration 11 : 0.6669088507119351
Loss in iteration 12 : 0.6651604675146272
Loss in iteration 13 : 0.6634573559742635
Loss in iteration 14 : 0.6617945994155643
Loss in iteration 15 : 0.6601684315195697
Loss in iteration 16 : 0.6585758823069892
Loss in iteration 17 : 0.6570145513481841
Loss in iteration 18 : 0.6554824578752914
Loss in iteration 19 : 0.6539779384995928
Loss in iteration 20 : 0.6524995752080291
Loss in iteration 21 : 0.6510461431959991
Loss in iteration 22 : 0.6496165721002999
Loss in iteration 23 : 0.6482099165626637
Loss in iteration 24 : 0.6468253334775376
Loss in iteration 25 : 0.645462064151593
Loss in iteration 26 : 0.6441194201521339
Loss in iteration 27 : 0.6427967719768719
Loss in iteration 28 : 0.6414935399135177
Loss in iteration 29 : 0.6402091866192432
Loss in iteration 30 : 0.6389432110636059
Loss in iteration 31 : 0.6376951435605117
Loss in iteration 32 : 0.6364645416752621
Loss in iteration 33 : 0.6352509868381958
Loss in iteration 34 : 0.634054081531268
Loss in iteration 35 : 0.6328734469407566
Loss in iteration 36 : 0.6317087209904497
Loss in iteration 37 : 0.6305595566861993
Loss in iteration 38 : 0.6294256207159981
Loss in iteration 39 : 0.6283065922603022
Loss in iteration 40 : 0.6272021619756973
Loss in iteration 41 : 0.6261120311219234
Loss in iteration 42 : 0.6250359108077385
Loss in iteration 43 : 0.6239735213355323
Loss in iteration 44 : 0.6229245916282782
Loss in iteration 45 : 0.621888858725293
Loss in iteration 46 : 0.6208660673357448
Loss in iteration 47 : 0.6198559694407251
Loss in iteration 48 : 0.6188583239363572
Loss in iteration 49 : 0.6178728963117396
Loss in iteration 50 : 0.616899458356487
Loss in iteration 51 : 0.6159377878936811
Loss in iteration 52 : 0.6149876685345949
Loss in iteration 53 : 0.6140488894522811
Loss in iteration 54 : 0.6131212451715173
Loss in iteration 55 : 0.6122045353730753
Loss in iteration 56 : 0.6112985647105386
Loss in iteration 57 : 0.610403142638243
Loss in iteration 58 : 0.6095180832490962
Loss in iteration 59 : 0.60864320512122
Loss in iteration 60 : 0.6077783311725654
Loss in iteration 61 : 0.606923288522698
Loss in iteration 62 : 0.60607790836115
Loss in iteration 63 : 0.6052420258217589
Loss in iteration 64 : 0.6044154798624987
Loss in iteration 65 : 0.603598113150432
Loss in iteration 66 : 0.6027897719513594
Loss in iteration 67 : 0.6019903060238946
Loss in iteration 68 : 0.6011995685176491
Loss in iteration 69 : 0.600417415875284
Loss in iteration 70 : 0.5996437077382117
Loss in iteration 71 : 0.5988783068557452
Loss in iteration 72 : 0.5981210789974976
Loss in iteration 73 : 0.5973718928688772
Loss in iteration 74 : 0.5966306200295287
Loss in iteration 75 : 0.5958971348145752
Loss in iteration 76 : 0.5951713142585426
Loss in iteration 77 : 0.5944530380218257
Loss in iteration 78 : 0.593742188319618
Loss in iteration 79 : 0.5930386498531781
Loss in iteration 80 : 0.592342309743332
Loss in iteration 81 : 0.5916530574661619
Loss in iteration 82 : 0.5909707847907272
Loss in iteration 83 : 0.5902953857188095
Loss in iteration 84 : 0.5896267564265486
Loss in iteration 85 : 0.5889647952079368
Loss in iteration 86 : 0.5883094024200677
Loss in iteration 87 : 0.5876604804301193
Loss in iteration 88 : 0.5870179335639399
Loss in iteration 89 : 0.5863816680562668
Loss in iteration 90 : 0.5857515920024297
Loss in iteration 91 : 0.5851276153115547
Loss in iteration 92 : 0.5845096496611852
Loss in iteration 93 : 0.5838976084532469
Loss in iteration 94 : 0.5832914067713807
Loss in iteration 95 : 0.5826909613395094
Loss in iteration 96 : 0.5820961904816615
Loss in iteration 97 : 0.5815070140829696
Loss in iteration 98 : 0.5809233535518369
Loss in iteration 99 : 0.5803451317831861
Loss in iteration 100 : 0.5797722731228069
Loss in iteration 101 : 0.5792047033327145
Loss in iteration 102 : 0.578642349557527
Loss in iteration 103 : 0.5780851402918142
Loss in iteration 104 : 0.5775330053483431
Loss in iteration 105 : 0.5769858758272823
Loss in iteration 106 : 0.5764436840862288
Loss in iteration 107 : 0.5759063637111217
Loss in iteration 108 : 0.5753738494879399
Loss in iteration 109 : 0.5748460773752074
Loss in iteration 110 : 0.5743229844772713
Loss in iteration 111 : 0.5738045090182825
Loss in iteration 112 : 0.5732905903169481
Loss in iteration 113 : 0.5727811687619225
Loss in iteration 114 : 0.5722761857879116
Loss in iteration 115 : 0.5717755838523904
Loss in iteration 116 : 0.5712793064129814
Loss in iteration 117 : 0.5707872979054205
Loss in iteration 118 : 0.5702995037221206
Loss in iteration 119 : 0.5698158701913046
Loss in iteration 120 : 0.569336344556692
Loss in iteration 121 : 0.5688608749577246
Loss in iteration 122 : 0.5683894104103003
Loss in iteration 123 : 0.5679219007880333
Loss in iteration 124 : 0.5674582968039737
Loss in iteration 125 : 0.5669985499928222
Loss in iteration 126 : 0.5665426126935934
Loss in iteration 127 : 0.5660904380327278
Loss in iteration 128 : 0.5656419799076337
Loss in iteration 129 : 0.5651971929706407
Loss in iteration 130 : 0.5647560326133797
Loss in iteration 131 : 0.5643184549515279
Loss in iteration 132 : 0.5638844168099625
Loss in iteration 133 : 0.5634538757082661
Loss in iteration 134 : 0.5630267898466002
Loss in iteration 135 : 0.562603118091937
Loss in iteration 136 : 0.5621828199646077
Loss in iteration 137 : 0.5617658556251989
Loss in iteration 138 : 0.5613521858617705
Loss in iteration 139 : 0.5609417720773633
Loss in iteration 140 : 0.5605345762778335
Loss in iteration 141 : 0.560130561059965
Loss in iteration 142 : 0.5597296895998692
Loss in iteration 143 : 0.5593319256416667
Loss in iteration 144 : 0.5589372334864331
Loss in iteration 145 : 0.5585455779814025
Loss in iteration 146 : 0.558156924509437
Loss in iteration 147 : 0.5577712389787362
Loss in iteration 148 : 0.5573884878127789
Loss in iteration 149 : 0.5570086379405188
Loss in iteration 150 : 0.5566316567867837
Loss in iteration 151 : 0.5562575122629141
Loss in iteration 152 : 0.5558861727576129
Loss in iteration 153 : 0.5555176071279896
Loss in iteration 154 : 0.5551517846908355
Loss in iteration 155 : 0.5547886752140622
Loss in iteration 156 : 0.5544282489083744
Loss in iteration 157 : 0.5540704764190871
Loss in iteration 158 : 0.5537153288181611
Loss in iteration 159 : 0.5533627775963927
Loss in iteration 160 : 0.5530127946557891
Loss in iteration 161 : 0.5526653523021077
Loss in iteration 162 : 0.5523204232375615
Loss in iteration 163 : 0.5519779805536783
Loss in iteration 164 : 0.5516379977243334
Loss in iteration 165 : 0.5513004485989047
Loss in iteration 166 : 0.5509653073956033
Loss in iteration 167 : 0.5506325486949395
Loss in iteration 168 : 0.5503021474333182
Loss in iteration 169 : 0.5499740788967858
Loss in iteration 170 : 0.5496483187149045
Loss in iteration 171 : 0.5493248428547577
Loss in iteration 172 : 0.5490036276150777
Loss in iteration 173 : 0.5486846496205042
Loss in iteration 174 : 0.5483678858159629
Loss in iteration 175 : 0.5480533134611482
Loss in iteration 176 : 0.547740910125148
Loss in iteration 177 : 0.5474306536811453
Loss in iteration 178 : 0.5471225223012609
Loss in iteration 179 : 0.5468164944514787
Loss in iteration 180 : 0.5465125488866991
Loss in iteration 181 : 0.5462106646458619
Loss in iteration 182 : 0.5459108210471998
Loss in iteration 183 : 0.5456129976835687
Loss in iteration 184 : 0.5453171744178807
Loss in iteration 185 : 0.545023331378621
Loss in iteration 186 : 0.5447314489554701
Loss in iteration 187 : 0.5444415077949909
Loss in iteration 188 : 0.5441534887964282
Loss in iteration 189 : 0.543867373107566
Loss in iteration 190 : 0.543583142120685
Loss in iteration 191 : 0.5433007774685937
Loss in iteration 192 : 0.5430202610207305
Loss in iteration 193 : 0.5427415748793553
Loss in iteration 194 : 0.5424647013758029
Loss in iteration 195 : 0.5421896230668226
Loss in iteration 196 : 0.5419163227309702
Loss in iteration 197 : 0.5416447833650879
Loss in iteration 198 : 0.5413749881808407
Loss in iteration 199 : 0.5411069206013239
Loss in iteration 200 : 0.5408405642577305
Loss in iteration 201 : 0.5405759029860908
Loss in iteration 202 : 0.5403129208240642
Loss in iteration 203 : 0.540051602007797
Loss in iteration 204 : 0.5397919309688357
Loss in iteration 205 : 0.5395338923311056
Loss in iteration 206 : 0.539277470907936
Loss in iteration 207 : 0.539022651699147
Loss in iteration 208 : 0.53876941988819
Loss in iteration 209 : 0.538517760839341
Loss in iteration 210 : 0.5382676600949399
Loss in iteration 211 : 0.5380191033726912
Loss in iteration 212 : 0.5377720765630043
Loss in iteration 213 : 0.537526565726388
Loss in iteration 214 : 0.5372825570908919
Loss in iteration 215 : 0.5370400370495932
Loss in iteration 216 : 0.5367989921581259
Loss in iteration 217 : 0.5365594091322584
Loss in iteration 218 : 0.536321274845519
Loss in iteration 219 : 0.5360845763268515
Loss in iteration 220 : 0.5358493007583223
Loss in iteration 221 : 0.5356154354728707
Loss in iteration 222 : 0.5353829679520878
Loss in iteration 223 : 0.5351518858240413
Loss in iteration 224 : 0.5349221768611444
Loss in iteration 225 : 0.5346938289780501
Loss in iteration 226 : 0.5344668302295938
Loss in iteration 227 : 0.5342411688087607
Loss in iteration 228 : 0.5340168330446984
Loss in iteration 229 : 0.5337938114007587
Loss in iteration 230 : 0.5335720924725779
Loss in iteration 231 : 0.5333516649861829
Loss in iteration 232 : 0.5331325177961296
Loss in iteration 233 : 0.5329146398836896
Loss in iteration 234 : 0.5326980203550467
Loss in iteration 235 : 0.5324826484395362
Loss in iteration 236 : 0.5322685134879062
Loss in iteration 237 : 0.5320556049706261
Loss in iteration 238 : 0.5318439124761921
Loss in iteration 239 : 0.5316334257094987
Loss in iteration 240 : 0.5314241344902042
Loss in iteration 241 : 0.5312160287511476
Loss in iteration 242 : 0.5310090985367778
Loss in iteration 243 : 0.5308033340016116
Loss in iteration 244 : 0.5305987254087285
Loss in iteration 245 : 0.53039526312827
Loss in iteration 246 : 0.5301929376359805
Loss in iteration 247 : 0.5299917395117686
Loss in iteration 248 : 0.5297916594382859
Loss in iteration 249 : 0.5295926881995299
Loss in iteration 250 : 0.5293948166794833
Loss in iteration 251 : 0.5291980358607563
Loss in iteration 252 : 0.52900233682326
Loss in iteration 253 : 0.5288077107429044
Loss in iteration 254 : 0.5286141488903131
Loss in iteration 255 : 0.5284216426295575
Loss in iteration 256 : 0.5282301834169106
Loss in iteration 257 : 0.5280397627996316
Loss in iteration 258 : 0.5278503724147526
Loss in iteration 259 : 0.5276620039878996
Loss in iteration 260 : 0.5274746493321211
Loss in iteration 261 : 0.5272883003467457
Loss in iteration 262 : 0.5271029490162464
Loss in iteration 263 : 0.526918587409134
Loss in iteration 264 : 0.5267352076768554
Loss in iteration 265 : 0.5265528020527254
Loss in iteration 266 : 0.5263713628508568
Loss in iteration 267 : 0.5261908824651228
Loss in iteration 268 : 0.526011353368126
Loss in iteration 269 : 0.5258327681101834
Loss in iteration 270 : 0.5256551193183384
Loss in iteration 271 : 0.5254783996953692
Loss in iteration 272 : 0.5253026020188301
Loss in iteration 273 : 0.5251277191400934
Loss in iteration 274 : 0.5249537439834244
Loss in iteration 275 : 0.5247806695450418
Loss in iteration 276 : 0.5246084888922258
Loss in iteration 277 : 0.5244371951624092
Loss in iteration 278 : 0.5242667815623081
Loss in iteration 279 : 0.5240972413670464
Loss in iteration 280 : 0.5239285679192992
Loss in iteration 281 : 0.523760754628457
Loss in iteration 282 : 0.5235937949697939
Loss in iteration 283 : 0.5234276824836468
Loss in iteration 284 : 0.5232624107746147
Loss in iteration 285 : 0.5230979735107588
Loss in iteration 286 : 0.5229343644228358
Loss in iteration 287 : 0.522771577303507
Loss in iteration 288 : 0.5226096060065988
Loss in iteration 289 : 0.5224484444463405
Loss in iteration 290 : 0.5222880865966419
Loss in iteration 291 : 0.5221285264903525
Loss in iteration 292 : 0.5219697582185596
Loss in iteration 293 : 0.5218117759298744
Loss in iteration 294 : 0.5216545738297436
Loss in iteration 295 : 0.5214981461797572
Loss in iteration 296 : 0.521342487296984
Loss in iteration 297 : 0.5211875915532985
Loss in iteration 298 : 0.521033453374728
Loss in iteration 299 : 0.5208800672408096
Loss in iteration 300 : 0.5207274276839486
Loss in iteration 301 : 0.5205755292887903
Loss in iteration 302 : 0.5204243666916077
Loss in iteration 303 : 0.5202739345796874
Loss in iteration 304 : 0.520124227690727
Loss in iteration 305 : 0.5199752408122413
Loss in iteration 306 : 0.5198269687809834
Loss in iteration 307 : 0.5196794064823641
Loss in iteration 308 : 0.519532548849883
Loss in iteration 309 : 0.5193863908645697
Loss in iteration 310 : 0.5192409275544325
Loss in iteration 311 : 0.519096153993911
Loss in iteration 312 : 0.5189520653033398
Loss in iteration 313 : 0.5188086566484199
Loss in iteration 314 : 0.5186659232396917
Loss in iteration 315 : 0.5185238603320279
Loss in iteration 316 : 0.5183824632241171
Loss in iteration 317 : 0.5182417272579674
Loss in iteration 318 : 0.5181016478184064
Loss in iteration 319 : 0.5179622203326053
Loss in iteration 320 : 0.5178234402695814
Loss in iteration 321 : 0.5176853031397377
Loss in iteration 322 : 0.517547804494389
Loss in iteration 323 : 0.5174109399252973
Loss in iteration 324 : 0.5172747050642227
Loss in iteration 325 : 0.5171390955824721
Loss in iteration 326 : 0.5170041071904483
Loss in iteration 327 : 0.5168697356372253
Loss in iteration 328 : 0.5167359767101095
Loss in iteration 329 : 0.5166028262342117
Loss in iteration 330 : 0.5164702800720338
Loss in iteration 331 : 0.516338334123047
Loss in iteration 332 : 0.5162069843232926
Loss in iteration 333 : 0.5160762266449624
Loss in iteration 334 : 0.5159460570960199
Loss in iteration 335 : 0.5158164717197914
Loss in iteration 336 : 0.5156874665945869
Loss in iteration 337 : 0.5155590378333123
Loss in iteration 338 : 0.515431181583099
Loss in iteration 339 : 0.5153038940249164
Loss in iteration 340 : 0.5151771713732239
Loss in iteration 341 : 0.5150510098755895
Loss in iteration 342 : 0.5149254058123458
Loss in iteration 343 : 0.5148003554962213
Loss in iteration 344 : 0.5146758552720042
Loss in iteration 345 : 0.5145519015161901
Loss in iteration 346 : 0.5144284906366474
Loss in iteration 347 : 0.5143056190722698
Loss in iteration 348 : 0.5141832832926617
Loss in iteration 349 : 0.5140614797977957
Loss in iteration 350 : 0.5139402051176982
Loss in iteration 351 : 0.5138194558121236
Loss in iteration 352 : 0.5136992284702447
Loss in iteration 353 : 0.5135795197103425
Loss in iteration 354 : 0.5134603261794869
Loss in iteration 355 : 0.5133416445532489
Loss in iteration 356 : 0.5132234715353962
Loss in iteration 357 : 0.5131058038575892
Loss in iteration 358 : 0.5129886382791006
Loss in iteration 359 : 0.5128719715865198
Loss in iteration 360 : 0.5127558005934668
Loss in iteration 361 : 0.5126401221403212
Loss in iteration 362 : 0.5125249330939282
Loss in iteration 363 : 0.5124102303473422
Loss in iteration 364 : 0.5122960108195425
Loss in iteration 365 : 0.5121822714551644
Loss in iteration 366 : 0.5120690092242522
Loss in iteration 367 : 0.5119562211219796
Loss in iteration 368 : 0.5118439041683983
Loss in iteration 369 : 0.5117320554081857
Loss in iteration 370 : 0.5116206719103893
Loss in iteration 371 : 0.5115097507681847
Loss in iteration 372 : 0.5113992890986182
Loss in iteration 373 : 0.511289284042379
Loss in iteration 374 : 0.5111797327635462
Loss in iteration 375 : 0.5110706324493612
Loss in iteration 376 : 0.5109619803099923
Loss in iteration 377 : 0.5108537735782971
Loss in iteration 378 : 0.5107460095096067
Loss in iteration 379 : 0.5106386853814846
Loss in iteration 380 : 0.5105317984935174
Loss in iteration 381 : 0.5104253461670883
Loss in iteration 382 : 0.5103193257451614
Loss in iteration 383 : 0.5102137345920633
Loss in iteration 384 : 0.5101085700932783
Loss in iteration 385 : 0.5100038296552266
Loss in iteration 386 : 0.5098995107050706
Loss in iteration 387 : 0.5097956106905026
Loss in iteration 388 : 0.5096921270795401
Loss in iteration 389 : 0.5095890573603318
Loss in iteration 390 : 0.5094863990409525
Loss in iteration 391 : 0.5093841496492193
Loss in iteration 392 : 0.5092823067324823
Loss in iteration 393 : 0.5091808678574504
Loss in iteration 394 : 0.5090798306099873
Loss in iteration 395 : 0.5089791925949386
Loss in iteration 396 : 0.508878951435938
Loss in iteration 397 : 0.5087791047752304
Loss in iteration 398 : 0.5086796502734863
Loss in iteration 399 : 0.5085805856096325
Loss in iteration 400 : 0.5084819084806655
Testing accuracy  of updater 3 on alg 0 with rate 0.4 = 0.776, training accuracy 0.776, time elapsed: 7390 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6918910538500311
Loss in iteration 3 : 0.690717971868775
Loss in iteration 4 : 0.6896187340860412
Loss in iteration 5 : 0.6885852065519499
Loss in iteration 6 : 0.687610200937914
Loss in iteration 7 : 0.6866873661596478
Loss in iteration 8 : 0.6858110915612513
Loss in iteration 9 : 0.684976420653083
Loss in iteration 10 : 0.6841789744290213
Loss in iteration 11 : 0.683414883335659
Loss in iteration 12 : 0.6826807270218918
Loss in iteration 13 : 0.6819734810586763
Loss in iteration 14 : 0.6812904698825316
Loss in iteration 15 : 0.6806293252803391
Loss in iteration 16 : 0.6799879497956782
Loss in iteration 17 : 0.6793644844967941
Loss in iteration 18 : 0.6787572806030552
Loss in iteration 19 : 0.6781648745194004
Loss in iteration 20 : 0.6775859658769285
Loss in iteration 21 : 0.6770193982222715
Loss in iteration 22 : 0.676464142038712
Loss in iteration 23 : 0.6759192798184104
Loss in iteration 24 : 0.6753839929378319
Loss in iteration 25 : 0.6748575501176346
Loss in iteration 26 : 0.6743392972743303
Loss in iteration 27 : 0.6738286485941262
Loss in iteration 28 : 0.6733250786798658
Loss in iteration 29 : 0.6728281156400338
Loss in iteration 30 : 0.6723373350048124
Loss in iteration 31 : 0.6718523543681514
Loss in iteration 32 : 0.671372828667346
Loss in iteration 33 : 0.6708984460223559
Loss in iteration 34 : 0.6704289240667023
Loss in iteration 35 : 0.669964006710227
Loss in iteration 36 : 0.6695034612812528
Loss in iteration 37 : 0.6690470760022234
Loss in iteration 38 : 0.6685946577585262
Loss in iteration 39 : 0.6681460301251186
Loss in iteration 40 : 0.6677010316200089
Loss in iteration 41 : 0.6672595141573617
Loss in iteration 42 : 0.6668213416763742
Loss in iteration 43 : 0.666386388924986
Loss in iteration 44 : 0.6659545403800371
Loss in iteration 45 : 0.6655256892877346
Loss in iteration 46 : 0.6650997368102278
Loss in iteration 47 : 0.6646765912658793
Loss in iteration 48 : 0.6642561674522507
Loss in iteration 49 : 0.663838386042164
Loss in iteration 50 : 0.6634231730444399
Loss in iteration 51 : 0.6630104593217553
Loss in iteration 52 : 0.6626001801592074
Loss in iteration 53 : 0.6621922748776701
Loss in iteration 54 : 0.6617866864869586
Loss in iteration 55 : 0.6613833613742688
Loss in iteration 56 : 0.6609822490239622
Loss in iteration 57 : 0.660583301765206
Loss in iteration 58 : 0.6601864745443925
Loss in iteration 59 : 0.6597917247196142
Loss in iteration 60 : 0.6593990118748014
Loss in iteration 61 : 0.6590082976514049
Loss in iteration 62 : 0.6586195455957402
Loss in iteration 63 : 0.6582327210203285
Loss in iteration 64 : 0.6578477908777605
Loss in iteration 65 : 0.6574647236458057
Loss in iteration 66 : 0.6570834892225752
Loss in iteration 67 : 0.6567040588307168
Loss in iteration 68 : 0.6563264049297725
Loss in iteration 69 : 0.655950501135806
Loss in iteration 70 : 0.6555763221476757
Loss in iteration 71 : 0.6552038436792285
Loss in iteration 72 : 0.6548330423968808
Loss in iteration 73 : 0.6544638958620833
Loss in iteration 74 : 0.6540963824781891
Loss in iteration 75 : 0.653730481441321
Loss in iteration 76 : 0.6533661726948954
Loss in iteration 77 : 0.6530034368874458
Loss in iteration 78 : 0.652642255333474
Loss in iteration 79 : 0.6522826099770634
Loss in iteration 80 : 0.6519244833579915
Loss in iteration 81 : 0.6515678585801761
Loss in iteration 82 : 0.6512127192822161
Loss in iteration 83 : 0.6508590496098899
Loss in iteration 84 : 0.6505068341904201
Loss in iteration 85 : 0.6501560581083998
Loss in iteration 86 : 0.6498067068832262
Loss in iteration 87 : 0.649458766447937
Loss in iteration 88 : 0.6491122231293314
Loss in iteration 89 : 0.6487670636292898
Loss in iteration 90 : 0.6484232750072295
Loss in iteration 91 : 0.6480808446635431
Loss in iteration 92 : 0.6477397603240304
Loss in iteration 93 : 0.6474000100252116
Loss in iteration 94 : 0.6470615821004645
Loss in iteration 95 : 0.6467244651669335
Loss in iteration 96 : 0.646388648113171
Loss in iteration 97 : 0.6460541200874443
Loss in iteration 98 : 0.6457208704866685
Loss in iteration 99 : 0.6453888889459467
Loss in iteration 100 : 0.6450581653286453
Loss in iteration 101 : 0.6447286897170014
Loss in iteration 102 : 0.644400452403207
Loss in iteration 103 : 0.6440734438809618
Loss in iteration 104 : 0.643747654837443
Loss in iteration 105 : 0.643423076145694
Loss in iteration 106 : 0.6430996988573899
Loss in iteration 107 : 0.6427775141959506
Loss in iteration 108 : 0.6424565135500109
Loss in iteration 109 : 0.6421366884672011
Loss in iteration 110 : 0.641818030648228
Loss in iteration 111 : 0.6415005319412403
Loss in iteration 112 : 0.641184184336476
Loss in iteration 113 : 0.6408689799611416
Loss in iteration 114 : 0.6405549110745603
Loss in iteration 115 : 0.6402419700635192
Loss in iteration 116 : 0.6399301494378549
Loss in iteration 117 : 0.6396194418262325
Loss in iteration 118 : 0.639309839972131
Loss in iteration 119 : 0.6390013367299835
Loss in iteration 120 : 0.6386939250615341
Loss in iteration 121 : 0.6383875980323194
Loss in iteration 122 : 0.638082348808336
Loss in iteration 123 : 0.6377781706528399
Loss in iteration 124 : 0.6374750569232993
Loss in iteration 125 : 0.6371730010684746
Loss in iteration 126 : 0.6368719966256264
Loss in iteration 127 : 0.636572037217844
Loss in iteration 128 : 0.6362731165514967
Loss in iteration 129 : 0.6359752284137928
Loss in iteration 130 : 0.6356783666704307
Loss in iteration 131 : 0.6353825252633671
Loss in iteration 132 : 0.6350876982086707
Loss in iteration 133 : 0.6347938795944622
Loss in iteration 134 : 0.6345010635789452
Loss in iteration 135 : 0.6342092443885279
Loss in iteration 136 : 0.6339184163160068
Loss in iteration 137 : 0.6336285737188223
Loss in iteration 138 : 0.6333397110174056
Loss in iteration 139 : 0.6330518226935701
Loss in iteration 140 : 0.6327649032889926
Loss in iteration 141 : 0.6324789474037239
Loss in iteration 142 : 0.6321939496947804
Loss in iteration 143 : 0.6319099048747809
Loss in iteration 144 : 0.6316268077106556
Loss in iteration 145 : 0.6313446530223707
Loss in iteration 146 : 0.631063435681728
Loss in iteration 147 : 0.6307831506112132
Loss in iteration 148 : 0.6305037927828647
Loss in iteration 149 : 0.6302253572172033
Loss in iteration 150 : 0.6299478389822015
Loss in iteration 151 : 0.6296712331922804
Loss in iteration 152 : 0.6293955350073518
Loss in iteration 153 : 0.6291207396318914
Loss in iteration 154 : 0.6288468423140511
Loss in iteration 155 : 0.628573838344791
Loss in iteration 156 : 0.628301723057056
Loss in iteration 157 : 0.6280304918249742
Loss in iteration 158 : 0.6277601400630738
Loss in iteration 159 : 0.6274906632255636
Loss in iteration 160 : 0.6272220568055775
Loss in iteration 161 : 0.6269543163345012
Loss in iteration 162 : 0.6266874373812953
Loss in iteration 163 : 0.6264214155518382
Loss in iteration 164 : 0.6261562464883068
Loss in iteration 165 : 0.6258919258685571
Loss in iteration 166 : 0.6256284494055394
Loss in iteration 167 : 0.6253658128467271
Loss in iteration 168 : 0.625104011973568
Loss in iteration 169 : 0.6248430426009451
Loss in iteration 170 : 0.624582900576654
Loss in iteration 171 : 0.6243235817809039
Loss in iteration 172 : 0.6240650821258314
Loss in iteration 173 : 0.623807397555026
Loss in iteration 174 : 0.623550524043063
Loss in iteration 175 : 0.6232944575950671
Loss in iteration 176 : 0.6230391942462777
Loss in iteration 177 : 0.6227847300616173
Loss in iteration 178 : 0.6225310611352957
Loss in iteration 179 : 0.6222781835903965
Loss in iteration 180 : 0.6220260935785126
Loss in iteration 181 : 0.621774787279343
Loss in iteration 182 : 0.6215242609003409
Loss in iteration 183 : 0.6212745106763611
Loss in iteration 184 : 0.6210255328692915
Loss in iteration 185 : 0.6207773237677393
Loss in iteration 186 : 0.6205298796866773
Loss in iteration 187 : 0.620283196967142
Loss in iteration 188 : 0.6200372719759
Loss in iteration 189 : 0.6197921011051571
Loss in iteration 190 : 0.6195476807722471
Loss in iteration 191 : 0.6193040074193502
Loss in iteration 192 : 0.6190610775131926
Loss in iteration 193 : 0.6188188875447815
Loss in iteration 194 : 0.6185774340291196
Loss in iteration 195 : 0.6183367135049566
Loss in iteration 196 : 0.6180967225345041
Loss in iteration 197 : 0.6178574577032003
Loss in iteration 198 : 0.6176189156194464
Loss in iteration 199 : 0.6173810929143722
Loss in iteration 200 : 0.6171439862415892
Loss in iteration 201 : 0.6169075922769568
Loss in iteration 202 : 0.616671907718357
Loss in iteration 203 : 0.6164369292854617
Loss in iteration 204 : 0.6162026537195202
Loss in iteration 205 : 0.6159690777831319
Loss in iteration 206 : 0.6157361982600419
Loss in iteration 207 : 0.6155040119549308
Loss in iteration 208 : 0.6152725156932008
Loss in iteration 209 : 0.6150417063207919
Loss in iteration 210 : 0.6148115807039651
Loss in iteration 211 : 0.614582135729124
Loss in iteration 212 : 0.6143533683026099
Loss in iteration 213 : 0.6141252753505277
Loss in iteration 214 : 0.6138978538185542
Loss in iteration 215 : 0.6136711006717521
Loss in iteration 216 : 0.613445012894406
Loss in iteration 217 : 0.6132195874898323
Loss in iteration 218 : 0.6129948214802178
Loss in iteration 219 : 0.612770711906436
Loss in iteration 220 : 0.6125472558278975
Loss in iteration 221 : 0.6123244503223707
Loss in iteration 222 : 0.6121022924858261
Loss in iteration 223 : 0.6118807794322804
Loss in iteration 224 : 0.6116599082936263
Loss in iteration 225 : 0.6114396762194919
Loss in iteration 226 : 0.6112200803770773
Loss in iteration 227 : 0.61100111795101
Loss in iteration 228 : 0.6107827861431925
Loss in iteration 229 : 0.610565082172659
Loss in iteration 230 : 0.6103480032754313
Loss in iteration 231 : 0.6101315467043672
Loss in iteration 232 : 0.6099157097290322
Loss in iteration 233 : 0.6097004896355575
Loss in iteration 234 : 0.6094858837264958
Loss in iteration 235 : 0.6092718893206949
Loss in iteration 236 : 0.6090585037531555
Loss in iteration 237 : 0.608845724374913
Loss in iteration 238 : 0.6086335485528891
Loss in iteration 239 : 0.608421973669781
Loss in iteration 240 : 0.6082109971239222
Loss in iteration 241 : 0.6080006163291605
Loss in iteration 242 : 0.6077908287147399
Loss in iteration 243 : 0.6075816317251663
Loss in iteration 244 : 0.6073730228200991
Loss in iteration 245 : 0.607164999474229
Loss in iteration 246 : 0.6069575591771419
Loss in iteration 247 : 0.6067506994332315
Loss in iteration 248 : 0.6065444177615681
Loss in iteration 249 : 0.6063387116957777
Loss in iteration 250 : 0.6061335787839438
Loss in iteration 251 : 0.6059290165884839
Loss in iteration 252 : 0.605725022686049
Loss in iteration 253 : 0.6055215946674065
Loss in iteration 254 : 0.6053187301373336
Loss in iteration 255 : 0.6051164267145164
Loss in iteration 256 : 0.6049146820314343
Loss in iteration 257 : 0.6047134937342642
Loss in iteration 258 : 0.6045128594827719
Loss in iteration 259 : 0.6043127769502089
Loss in iteration 260 : 0.6041132438232198
Loss in iteration 261 : 0.6039142578017289
Loss in iteration 262 : 0.603715816598846
Loss in iteration 263 : 0.6035179179407762
Loss in iteration 264 : 0.6033205595667102
Loss in iteration 265 : 0.6031237392287342
Loss in iteration 266 : 0.6029274546917357
Loss in iteration 267 : 0.6027317037333052
Loss in iteration 268 : 0.6025364841436412
Loss in iteration 269 : 0.6023417937254669
Loss in iteration 270 : 0.6021476302939275
Loss in iteration 271 : 0.6019539916765078
Loss in iteration 272 : 0.6017608757129341
Loss in iteration 273 : 0.6015682802550926
Loss in iteration 274 : 0.6013762031669366
Loss in iteration 275 : 0.6011846423244008
Loss in iteration 276 : 0.6009935956153177
Loss in iteration 277 : 0.6008030609393205
Loss in iteration 278 : 0.6006130362077737
Loss in iteration 279 : 0.600423519343678
Loss in iteration 280 : 0.6002345082815875
Loss in iteration 281 : 0.6000460009675372
Loss in iteration 282 : 0.5998579953589452
Loss in iteration 283 : 0.5996704894245404
Loss in iteration 284 : 0.5994834811442902
Loss in iteration 285 : 0.5992969685093048
Loss in iteration 286 : 0.599110949521767
Loss in iteration 287 : 0.5989254221948476
Loss in iteration 288 : 0.5987403845526429
Loss in iteration 289 : 0.59855583463008
Loss in iteration 290 : 0.5983717704728504
Loss in iteration 291 : 0.5981881901373325
Loss in iteration 292 : 0.5980050916905139
Loss in iteration 293 : 0.597822473209926
Loss in iteration 294 : 0.5976403327835558
Loss in iteration 295 : 0.5974586685097895
Loss in iteration 296 : 0.5972774784973306
Loss in iteration 297 : 0.5970967608651194
Loss in iteration 298 : 0.5969165137422895
Loss in iteration 299 : 0.5967367352680665
Loss in iteration 300 : 0.5965574235917209
Loss in iteration 301 : 0.596378576872485
Loss in iteration 302 : 0.5962001932794896
Loss in iteration 303 : 0.5960222709916966
Loss in iteration 304 : 0.5958448081978327
Loss in iteration 305 : 0.5956678030963187
Loss in iteration 306 : 0.5954912538952042
Loss in iteration 307 : 0.5953151588121027
Loss in iteration 308 : 0.5951395160741274
Loss in iteration 309 : 0.5949643239178247
Loss in iteration 310 : 0.5947895805891098
Loss in iteration 311 : 0.5946152843432071
Loss in iteration 312 : 0.5944414334445846
Loss in iteration 313 : 0.5942680261668828
Loss in iteration 314 : 0.5940950607928742
Loss in iteration 315 : 0.593922535614376
Loss in iteration 316 : 0.5937504489322132
Loss in iteration 317 : 0.5935787990561362
Loss in iteration 318 : 0.5934075843047767
Loss in iteration 319 : 0.5932368030055846
Loss in iteration 320 : 0.5930664534947606
Loss in iteration 321 : 0.5928965341172102
Loss in iteration 322 : 0.5927270432264808
Loss in iteration 323 : 0.5925579791846949
Loss in iteration 324 : 0.5923893403625096
Loss in iteration 325 : 0.5922211251390481
Loss in iteration 326 : 0.5920533319018473
Loss in iteration 327 : 0.5918859590468031
Loss in iteration 328 : 0.591719004978111
Loss in iteration 329 : 0.5915524681082178
Loss in iteration 330 : 0.5913863468577601
Loss in iteration 331 : 0.591220639655519
Loss in iteration 332 : 0.5910553449383533
Loss in iteration 333 : 0.5908904611511616
Loss in iteration 334 : 0.5907259867468233
Loss in iteration 335 : 0.5905619201861375
Loss in iteration 336 : 0.5903982599377905
Loss in iteration 337 : 0.590235004478283
Loss in iteration 338 : 0.5900721522918975
Loss in iteration 339 : 0.5899097018706355
Loss in iteration 340 : 0.5897476517141688
Loss in iteration 341 : 0.5895860003297991
Loss in iteration 342 : 0.5894247462323976
Loss in iteration 343 : 0.589263887944362
Loss in iteration 344 : 0.5891034239955638
Loss in iteration 345 : 0.5889433529233042
Loss in iteration 346 : 0.5887836732722641
Loss in iteration 347 : 0.5886243835944588
Loss in iteration 348 : 0.5884654824491846
Loss in iteration 349 : 0.5883069684029799
Loss in iteration 350 : 0.5881488400295741
Loss in iteration 351 : 0.5879910959098427
Loss in iteration 352 : 0.5878337346317644
Loss in iteration 353 : 0.5876767547903704
Loss in iteration 354 : 0.5875201549877024
Loss in iteration 355 : 0.5873639338327681
Loss in iteration 356 : 0.5872080899414984
Loss in iteration 357 : 0.5870526219366977
Loss in iteration 358 : 0.58689752844801
Loss in iteration 359 : 0.5867428081118662
Loss in iteration 360 : 0.5865884595714436
Loss in iteration 361 : 0.5864344814766282
Loss in iteration 362 : 0.5862808724839679
Loss in iteration 363 : 0.5861276312566304
Loss in iteration 364 : 0.5859747564643633
Loss in iteration 365 : 0.5858222467834546
Loss in iteration 366 : 0.585670100896684
Loss in iteration 367 : 0.5855183174932931
Loss in iteration 368 : 0.5853668952689379
Loss in iteration 369 : 0.5852158329256479
Loss in iteration 370 : 0.5850651291717913
Loss in iteration 371 : 0.5849147827220331
Loss in iteration 372 : 0.5847647922972933
Loss in iteration 373 : 0.5846151566247136
Loss in iteration 374 : 0.5844658744376177
Loss in iteration 375 : 0.5843169444754642
Loss in iteration 376 : 0.5841683654838228
Loss in iteration 377 : 0.5840201362143261
Loss in iteration 378 : 0.5838722554246323
Loss in iteration 379 : 0.583724721878401
Loss in iteration 380 : 0.5835775343452342
Loss in iteration 381 : 0.5834306916006624
Loss in iteration 382 : 0.5832841924260915
Loss in iteration 383 : 0.5831380356087784
Loss in iteration 384 : 0.5829922199417845
Loss in iteration 385 : 0.5828467442239512
Loss in iteration 386 : 0.5827016072598554
Loss in iteration 387 : 0.5825568078597798
Loss in iteration 388 : 0.5824123448396735
Loss in iteration 389 : 0.5822682170211286
Loss in iteration 390 : 0.5821244232313318
Loss in iteration 391 : 0.5819809623030358
Loss in iteration 392 : 0.5818378330745343
Loss in iteration 393 : 0.5816950343896119
Loss in iteration 394 : 0.5815525650975235
Loss in iteration 395 : 0.5814104240529598
Loss in iteration 396 : 0.5812686101160082
Loss in iteration 397 : 0.5811271221521258
Loss in iteration 398 : 0.5809859590321096
Loss in iteration 399 : 0.580845119632056
Loss in iteration 400 : 0.5807046028333365
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.76725, training accuracy 0.76725, time elapsed: 7470 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 100.0 = 0.726, training accuracy 0.726, time elapsed: 7441 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 70.0 = 0.726, training accuracy 0.726, time elapsed: 7652 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 40.0 = 0.726, training accuracy 0.726, time elapsed: 7435 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.726, training accuracy 0.726, time elapsed: 7421 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 7.0 = 0.726, training accuracy 0.726, time elapsed: 7398 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 4.0 = 0.726, training accuracy 0.726, time elapsed: 8336 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6929235752666227
Loss in iteration 3 : 0.6926960665968462
Loss in iteration 4 : 0.6924666730027986
Loss in iteration 5 : 0.6922363372674499
Loss in iteration 6 : 0.6920055789324592
Loss in iteration 7 : 0.691774712053391
Loss in iteration 8 : 0.6915439381601184
Loss in iteration 9 : 0.6913133922107217
Loss in iteration 10 : 0.6910831675971454
Loss in iteration 11 : 0.6908533306884829
Loss in iteration 12 : 0.6906239297064439
Loss in iteration 13 : 0.6903950003411546
Loss in iteration 14 : 0.6901665694061512
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231979
Loss in iteration 17 : 0.6894844480963943
Loss in iteration 18 : 0.689258172092525
Loss in iteration 19 : 0.6890324583520105
Loss in iteration 20 : 0.688807311878355
Loss in iteration 21 : 0.6885827361540362
Loss in iteration 22 : 0.6883587333817476
Loss in iteration 23 : 0.6881353046728773
Loss in iteration 24 : 0.6879124501982635
Loss in iteration 25 : 0.687690169311446
Loss in iteration 26 : 0.6874684606515445
Loss in iteration 27 : 0.6872473222307222
Loss in iteration 28 : 0.6870267515096296
Loss in iteration 29 : 0.6868067454633641
Loss in iteration 30 : 0.6865873006395918
Loss in iteration 31 : 0.686368413210221
Loss in iteration 32 : 0.6861500790174885
Loss in iteration 33 : 0.6859322936152177
Loss in iteration 34 : 0.6857150523058203
Loss in iteration 35 : 0.6854983501734859
Loss in iteration 36 : 0.6852821821139223
Loss in iteration 37 : 0.6850665428610205
Loss in iteration 38 : 0.6848514270106477
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359671
Loss in iteration 41 : 0.6842091641928554
Loss in iteration 42 : 0.6839960858461501
Loss in iteration 43 : 0.683783502475952
Loss in iteration 44 : 0.6835714082201878
Loss in iteration 45 : 0.6833597971844081
Loss in iteration 46 : 0.6831486634501975
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348099
Loss in iteration 49 : 0.6825180666557323
Loss in iteration 50 : 0.6823087826913214
Loss in iteration 51 : 0.6820999462889619
Loss in iteration 52 : 0.6818915514994243
Loss in iteration 53 : 0.6816835923783221
Loss in iteration 54 : 0.6814760629868173
Loss in iteration 55 : 0.6812689573916448
Loss in iteration 56 : 0.6810622696644758
Loss in iteration 57 : 0.6808559938806238
Loss in iteration 58 : 0.6806501241171412
Loss in iteration 59 : 0.6804446544502766
Loss in iteration 60 : 0.6802395789522896
Loss in iteration 61 : 0.6800348916876395
Loss in iteration 62 : 0.6798305867084532
Loss in iteration 63 : 0.6796266580492965
Loss in iteration 64 : 0.6794230997211287
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411933
Loss in iteration 67 : 0.6788145863262021
Loss in iteration 68 : 0.678612448696558
Loss in iteration 69 : 0.6784106508201553
Loss in iteration 70 : 0.6782091863823759
Loss in iteration 71 : 0.6780080489709492
Loss in iteration 72 : 0.67780723205871
Loss in iteration 73 : 0.6776067289840471
Loss in iteration 74 : 0.6774065329288731
Loss in iteration 75 : 0.6772066368941004
Loss in iteration 76 : 0.6770070336728068
Loss in iteration 77 : 0.6768077158217213
Loss in iteration 78 : 0.6766086756322008
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.676211395916268
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569093
Loss in iteration 83 : 0.675617348325563
Loss in iteration 84 : 0.6754197948878227
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.675025323682299
Loss in iteration 87 : 0.6748283863086912
Loss in iteration 88 : 0.6746316348550659
Loss in iteration 89 : 0.6744350599463953
Loss in iteration 90 : 0.6742386525578963
Loss in iteration 91 : 0.6740424040750547
Loss in iteration 92 : 0.67384630632539
Loss in iteration 93 : 0.6736503515866574
Loss in iteration 94 : 0.6734545325782229
Loss in iteration 95 : 0.673258842442173
Loss in iteration 96 : 0.6730632747194034
Loss in iteration 97 : 0.6728678233241567
Loss in iteration 98 : 0.6726724825192095
Loss in iteration 99 : 0.672477246892672
Loss in iteration 100 : 0.672282111336883
Loss in iteration 101 : 0.6720870710293809
Loss in iteration 102 : 0.6718921214158095
Loss in iteration 103 : 0.6716972581945452
Loss in iteration 104 : 0.6715024773027857
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759103
Loss in iteration 107 : 0.6709185913007195
Loss in iteration 108 : 0.6707241034543506
Loss in iteration 109 : 0.6705296807976894
Loss in iteration 110 : 0.6703353204679108
Loss in iteration 111 : 0.6701410197704022
Loss in iteration 112 : 0.6699467761711548
Loss in iteration 113 : 0.6697525872896085
Loss in iteration 114 : 0.6695584508918566
Loss in iteration 115 : 0.6693643648842078
Loss in iteration 116 : 0.6691703273070827
Loss in iteration 117 : 0.6689763363292015
Loss in iteration 118 : 0.6687823902420608
Loss in iteration 119 : 0.6685884874546636
Loss in iteration 120 : 0.6683946264884936
Loss in iteration 121 : 0.6682008059727261
Loss in iteration 122 : 0.6680070246396377
Loss in iteration 123 : 0.6678132813202299
Loss in iteration 124 : 0.6676195749400379
Loss in iteration 125 : 0.6674259045151237
Loss in iteration 126 : 0.667232269148228
Loss in iteration 127 : 0.667038668025086
Loss in iteration 128 : 0.6668451004109114
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474696
Loss in iteration 131 : 0.6662645923961665
Loss in iteration 132 : 0.6660711529436428
Loss in iteration 133 : 0.665877744404295
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254216
Loss in iteration 136 : 0.6652977013095177
Loss in iteration 137 : 0.6651044137490177
Loss in iteration 138 : 0.6649111560380927
Loss in iteration 139 : 0.6647179281196696
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627471
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155855
Loss in iteration 144 : 0.6637522365612187
Loss in iteration 145 : 0.6635591886637094
Loss in iteration 146 : 0.6633661713503325
Loss in iteration 147 : 0.6631731848817296
Loss in iteration 148 : 0.6629802295502835
Loss in iteration 149 : 0.6627873056785707
Loss in iteration 150 : 0.6625944136178621
Loss in iteration 151 : 0.6624015537466766
Loss in iteration 152 : 0.6622087264694014
Loss in iteration 153 : 0.6620159322149516
Loss in iteration 154 : 0.6618231714354788
Loss in iteration 155 : 0.6616304446051515
Loss in iteration 156 : 0.6614377522189504
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561483
Loss in iteration 159 : 0.660859886963547
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913331
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939863
Loss in iteration 164 : 0.6598975185214344
Loss in iteration 165 : 0.6597051613103267
Loss in iteration 166 : 0.6595128444079461
Loss in iteration 167 : 0.6593205684721011
Loss in iteration 168 : 0.6591283341703846
Loss in iteration 169 : 0.6589361421795201
Loss in iteration 170 : 0.6587439931847026
Loss in iteration 171 : 0.6585518878789912
Loss in iteration 172 : 0.6583598269627259
Loss in iteration 173 : 0.6581678111429787
Loss in iteration 174 : 0.6579758411330449
Loss in iteration 175 : 0.6577839176519344
Loss in iteration 176 : 0.6575920414239275
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481179
Loss in iteration 179 : 0.65701670357145
Loss in iteration 180 : 0.6568250236894342
Loss in iteration 181 : 0.6566333947467301
Loss in iteration 182 : 0.6564418174910612
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452749
Loss in iteration 185 : 0.6558674033633619
Loss in iteration 186 : 0.6556760403843819
Loss in iteration 187 : 0.6554847328672831
Loss in iteration 188 : 0.6552934815725109
Loss in iteration 189 : 0.6551022872617626
Loss in iteration 190 : 0.6549111506977277
Loss in iteration 191 : 0.6547200726438117
Loss in iteration 192 : 0.6545290538638485
Loss in iteration 193 : 0.6543380951217601
Loss in iteration 194 : 0.6541471971811902
Loss in iteration 195 : 0.6539563608050981
Loss in iteration 196 : 0.6537655867552814
Loss in iteration 197 : 0.6535748757918566
Loss in iteration 198 : 0.6533842286726632
Loss in iteration 199 : 0.6531936461525875
Loss in iteration 200 : 0.653003128982836
Loss in iteration 201 : 0.6528126779100925
Loss in iteration 202 : 0.6526222936756358
Loss in iteration 203 : 0.6524319770143664
Loss in iteration 204 : 0.6522417286537278
Loss in iteration 205 : 0.6520515493126077
Loss in iteration 206 : 0.6518614397001324
Loss in iteration 207 : 0.651671400514421
Loss in iteration 208 : 0.6514814324412715
Loss in iteration 209 : 0.6512915361528184
Loss in iteration 210 : 0.6511017123061177
Loss in iteration 211 : 0.6509119615417064
Loss in iteration 212 : 0.6507222844821396
Loss in iteration 213 : 0.6505326817304534
Loss in iteration 214 : 0.6503431538686223
Loss in iteration 215 : 0.6501537014559656
Loss in iteration 216 : 0.6499643250275178
Loss in iteration 217 : 0.6497750250923703
Loss in iteration 218 : 0.6495858021319837
Loss in iteration 219 : 0.6493966565984831
Loss in iteration 220 : 0.6492075889129876
Loss in iteration 221 : 0.6490185994639678
Loss in iteration 222 : 0.6488296886057647
Loss in iteration 223 : 0.6486408566572915
Loss in iteration 224 : 0.6484521039010835
Loss in iteration 225 : 0.6482634305828623
Loss in iteration 226 : 0.6480748369117985
Loss in iteration 227 : 0.647886323061708
Loss in iteration 228 : 0.64769788917346
Loss in iteration 229 : 0.6475095353587869
Loss in iteration 230 : 0.6473212617056988
Loss in iteration 231 : 0.6471330682854867
Loss in iteration 232 : 0.6469449551612092
Loss in iteration 233 : 0.646756922397225
Loss in iteration 234 : 0.6465689700691694
Loss in iteration 235 : 0.6463810982736484
Loss in iteration 236 : 0.6461933071368225
Loss in iteration 237 : 0.6460055968212823
Loss in iteration 238 : 0.6458179675308101
Loss in iteration 239 : 0.6456304195129219
Loss in iteration 240 : 0.6454429530593564
Loss in iteration 241 : 0.6452555685049133
Loss in iteration 242 : 0.645068266225024
Loss in iteration 243 : 0.6448810466325241
Loss in iteration 244 : 0.6446939101740161
Loss in iteration 245 : 0.6445068573260437
Loss in iteration 246 : 0.6443198885913674
Loss in iteration 247 : 0.6441330044953829
Loss in iteration 248 : 0.6439462055828149
Loss in iteration 249 : 0.6437594924146947
Loss in iteration 250 : 0.6435728655656119
Loss in iteration 251 : 0.6433863256212781
Loss in iteration 252 : 0.6431998731763218
Loss in iteration 253 : 0.6430135088323655
Loss in iteration 254 : 0.6428272331963023
Loss in iteration 255 : 0.6426410468787648
Loss in iteration 256 : 0.6424549504928054
Loss in iteration 257 : 0.6422689446526941
Loss in iteration 258 : 0.6420830299728993
Loss in iteration 259 : 0.6418972070671641
Loss in iteration 260 : 0.641711476547701
Loss in iteration 261 : 0.6415258390244867
Loss in iteration 262 : 0.6413402951046446
Loss in iteration 263 : 0.6411548453919075
Loss in iteration 264 : 0.6409694904861272
Loss in iteration 265 : 0.6407842309828747
Loss in iteration 266 : 0.6405990674730616
Loss in iteration 267 : 0.6404140005426404
Loss in iteration 268 : 0.640229030772317
Loss in iteration 269 : 0.6400441587373172
Loss in iteration 270 : 0.6398593850071848
Loss in iteration 271 : 0.639674710145599
Loss in iteration 272 : 0.6394901347102335
Loss in iteration 273 : 0.639305659252621
Loss in iteration 274 : 0.6391212843180487
Loss in iteration 275 : 0.638937010445479
Loss in iteration 276 : 0.638752838167464
Loss in iteration 277 : 0.6385687680101043
Loss in iteration 278 : 0.6383848004929946
Loss in iteration 279 : 0.6382009361292014
Loss in iteration 280 : 0.6380171754252336
Loss in iteration 281 : 0.6378335188810368
Loss in iteration 282 : 0.637649966989992
Loss in iteration 283 : 0.6374665202389228
Loss in iteration 284 : 0.6372831791080968
Loss in iteration 285 : 0.6370999440712594
Loss in iteration 286 : 0.6369168155956506
Loss in iteration 287 : 0.6367337941420306
Loss in iteration 288 : 0.6365508801647254
Loss in iteration 289 : 0.6363680741116532
Loss in iteration 290 : 0.6361853764243693
Loss in iteration 291 : 0.6360027875381179
Loss in iteration 292 : 0.6358203078818729
Loss in iteration 293 : 0.6356379378783885
Loss in iteration 294 : 0.6354556779442525
Loss in iteration 295 : 0.6352735284899457
Loss in iteration 296 : 0.6350914899198921
Loss in iteration 297 : 0.6349095626325213
Loss in iteration 298 : 0.6347277470203182
Loss in iteration 299 : 0.6345460434699073
Loss in iteration 300 : 0.6343644523620822
Loss in iteration 301 : 0.6341829740718926
Loss in iteration 302 : 0.6340016089686892
Loss in iteration 303 : 0.6338203574162106
Loss in iteration 304 : 0.633639219772621
Loss in iteration 305 : 0.6334581963905884
Loss in iteration 306 : 0.6332772876173535
Loss in iteration 307 : 0.6330964937947804
Loss in iteration 308 : 0.6329158152594332
Loss in iteration 309 : 0.6327352523426334
Loss in iteration 310 : 0.6325548053705294
Loss in iteration 311 : 0.6323744746641569
Loss in iteration 312 : 0.6321942605395111
Loss in iteration 313 : 0.6320141633075939
Loss in iteration 314 : 0.6318341832744998
Loss in iteration 315 : 0.6316543207414572
Loss in iteration 316 : 0.6314745760049126
Loss in iteration 317 : 0.6312949493565745
Loss in iteration 318 : 0.6311154410834854
Loss in iteration 319 : 0.6309360514680845
Loss in iteration 320 : 0.6307567807882652
Loss in iteration 321 : 0.6305776293174354
Loss in iteration 322 : 0.630398597324578
Loss in iteration 323 : 0.6302196850743191
Loss in iteration 324 : 0.6300408928269712
Loss in iteration 325 : 0.6298622208386057
Loss in iteration 326 : 0.6296836693611023
Loss in iteration 327 : 0.629505238642216
Loss in iteration 328 : 0.6293269289256227
Loss in iteration 329 : 0.6291487404509839
Loss in iteration 330 : 0.6289706734539993
Loss in iteration 331 : 0.628792728166462
Loss in iteration 332 : 0.6286149048163151
Loss in iteration 333 : 0.6284372036277082
Loss in iteration 334 : 0.6282596248210414
Loss in iteration 335 : 0.628082168613027
Loss in iteration 336 : 0.6279048352167442
Loss in iteration 337 : 0.6277276248416838
Loss in iteration 338 : 0.627550537693804
Loss in iteration 339 : 0.6273735739755779
Loss in iteration 340 : 0.6271967338860511
Loss in iteration 341 : 0.627020017620885
Loss in iteration 342 : 0.6268434253724154
Loss in iteration 343 : 0.6266669573296862
Loss in iteration 344 : 0.6264906136785104
Loss in iteration 345 : 0.6263143946015229
Loss in iteration 346 : 0.6261383002782166
Loss in iteration 347 : 0.6259623308849916
Loss in iteration 348 : 0.625786486595212
Loss in iteration 349 : 0.625610767579242
Loss in iteration 350 : 0.6254351740045059
Loss in iteration 351 : 0.6252597060355184
Loss in iteration 352 : 0.6250843638339423
Loss in iteration 353 : 0.6249091475586299
Loss in iteration 354 : 0.6247340573656706
Loss in iteration 355 : 0.6245590934084377
Loss in iteration 356 : 0.6243842558376298
Loss in iteration 357 : 0.6242095448013192
Loss in iteration 358 : 0.6240349604449996
Loss in iteration 359 : 0.6238605029116274
Loss in iteration 360 : 0.623686172341664
Loss in iteration 361 : 0.6235119688731322
Loss in iteration 362 : 0.6233378926416571
Loss in iteration 363 : 0.6231639437805041
Loss in iteration 364 : 0.6229901224206351
Loss in iteration 365 : 0.6228164286907494
Loss in iteration 366 : 0.6226428627173387
Loss in iteration 367 : 0.6224694246247156
Loss in iteration 368 : 0.6222961145350872
Loss in iteration 369 : 0.6221229325685811
Loss in iteration 370 : 0.6219498788433027
Loss in iteration 371 : 0.621776953475392
Loss in iteration 372 : 0.6216041565790597
Loss in iteration 373 : 0.6214314882666478
Loss in iteration 374 : 0.6212589486486783
Loss in iteration 375 : 0.6210865378339118
Loss in iteration 376 : 0.6209142559293943
Loss in iteration 377 : 0.6207421030405187
Loss in iteration 378 : 0.6205700792710779
Loss in iteration 379 : 0.6203981847233268
Loss in iteration 380 : 0.6202264194980369
Loss in iteration 381 : 0.6200547836945696
Loss in iteration 382 : 0.6198832774109301
Loss in iteration 383 : 0.6197119007438298
Loss in iteration 384 : 0.6195406537887674
Loss in iteration 385 : 0.6193695366400889
Loss in iteration 386 : 0.6191985493910619
Loss in iteration 387 : 0.6190276921339456
Loss in iteration 388 : 0.6188569649600788
Loss in iteration 389 : 0.6186863679599471
Loss in iteration 390 : 0.6185159012232659
Loss in iteration 391 : 0.6183455648390703
Loss in iteration 392 : 0.6181753588957913
Loss in iteration 393 : 0.6180052834813475
Loss in iteration 394 : 0.6178353386832349
Loss in iteration 395 : 0.6176655245886088
Loss in iteration 396 : 0.6174958412843844
Loss in iteration 397 : 0.617326288857317
Loss in iteration 398 : 0.6171568673940976
Loss in iteration 399 : 0.6169875769814386
Loss in iteration 400 : 0.616818417706152
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.726, training accuracy 0.726, time elapsed: 8081 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 65.93856858740773
Loss in iteration 3 : 84.85542655623208
Loss in iteration 4 : 34.27691872039004
Loss in iteration 5 : 40.092561245509295
Loss in iteration 6 : 28.27806602126496
Loss in iteration 7 : 29.248996869359594
Loss in iteration 8 : 25.40268577693369
Loss in iteration 9 : 23.852585622720596
Loss in iteration 10 : 22.677779140495183
Loss in iteration 11 : 21.498266423517432
Loss in iteration 12 : 20.728604530229074
Loss in iteration 13 : 19.857287706840438
Loss in iteration 14 : 18.69437548795499
Loss in iteration 15 : 18.961392619818948
Loss in iteration 16 : 17.37213854585892
Loss in iteration 17 : 18.251118560721938
Loss in iteration 18 : 17.098513658999757
Loss in iteration 19 : 18.121731624418608
Loss in iteration 20 : 16.493724833817552
Loss in iteration 21 : 18.046254887381615
Loss in iteration 22 : 15.310892049383227
Loss in iteration 23 : 17.811896200384627
Loss in iteration 24 : 14.025791290785092
Loss in iteration 25 : 17.580158458896438
Loss in iteration 26 : 15.50260488970056
Loss in iteration 27 : 18.437089891763755
Loss in iteration 28 : 16.452054720749302
Loss in iteration 29 : 18.076336835244916
Loss in iteration 30 : 15.988827838594904
Loss in iteration 31 : 17.465143213334102
Loss in iteration 32 : 15.214956164111719
Loss in iteration 33 : 17.09167190935195
Loss in iteration 34 : 14.95920823625357
Loss in iteration 35 : 17.041640736742476
Loss in iteration 36 : 14.7910832301524
Loss in iteration 37 : 17.08611909372059
Loss in iteration 38 : 14.462614656350524
Loss in iteration 39 : 17.31138175814769
Loss in iteration 40 : 13.976097676197982
Loss in iteration 41 : 17.83744740869134
Loss in iteration 42 : 12.691749160892893
Loss in iteration 43 : 16.89158098359636
Loss in iteration 44 : 14.391248455156513
Loss in iteration 45 : 18.195093748749176
Loss in iteration 46 : 15.614546507149115
Loss in iteration 47 : 18.155127113721434
Loss in iteration 48 : 15.40383211915219
Loss in iteration 49 : 17.57801854988747
Loss in iteration 50 : 14.627180422834208
Loss in iteration 51 : 17.00007908793063
Loss in iteration 52 : 14.333609098941487
Loss in iteration 53 : 16.902434318003245
Loss in iteration 54 : 13.913479753537999
Loss in iteration 55 : 17.107402064557967
Loss in iteration 56 : 13.589072546711751
Loss in iteration 57 : 17.187703310521872
Loss in iteration 58 : 12.560079185570343
Loss in iteration 59 : 16.664036384081083
Loss in iteration 60 : 14.002883919241691
Loss in iteration 61 : 18.04644648426826
Loss in iteration 62 : 15.307472776847874
Loss in iteration 63 : 18.194745728554174
Loss in iteration 64 : 15.128786932900658
Loss in iteration 65 : 17.562019187240303
Loss in iteration 66 : 14.313437951405726
Loss in iteration 67 : 16.886995545535843
Loss in iteration 68 : 14.05491650165142
Loss in iteration 69 : 16.837067442828303
Loss in iteration 70 : 13.772836813126814
Loss in iteration 71 : 17.27576308599983
Loss in iteration 72 : 13.008544165370251
Loss in iteration 73 : 16.63496481346382
Loss in iteration 74 : 12.55230946903658
Loss in iteration 75 : 16.74000579139852
Loss in iteration 76 : 14.33988571597496
Loss in iteration 77 : 18.253731569555455
Loss in iteration 78 : 15.234819154574936
Loss in iteration 79 : 18.061249040300797
Loss in iteration 80 : 14.761682842274057
Loss in iteration 81 : 17.23633077256928
Loss in iteration 82 : 14.259970649474715
Loss in iteration 83 : 16.822325856859628
Loss in iteration 84 : 13.77491771667208
Loss in iteration 85 : 16.868698029772524
Loss in iteration 86 : 13.48649367526422
Loss in iteration 87 : 17.07035139166983
Loss in iteration 88 : 12.661675973206947
Loss in iteration 89 : 16.361933201666922
Loss in iteration 90 : 13.485655727483634
Loss in iteration 91 : 17.511296279489812
Loss in iteration 92 : 14.797305048854817
Loss in iteration 93 : 17.985240873691954
Loss in iteration 94 : 14.829805195376762
Loss in iteration 95 : 17.426129443204232
Loss in iteration 96 : 14.299104661769674
Loss in iteration 97 : 16.799218614515105
Loss in iteration 98 : 14.031688792403484
Loss in iteration 99 : 16.792994097405398
Loss in iteration 100 : 13.722299569920105
Loss in iteration 101 : 17.05833707707053
Loss in iteration 102 : 12.9245060236455
Loss in iteration 103 : 16.43246198114017
Loss in iteration 104 : 13.091534207918212
Loss in iteration 105 : 17.039428061437278
Loss in iteration 106 : 14.418869251532625
Loss in iteration 107 : 17.70700008919939
Loss in iteration 108 : 14.809126951455344
Loss in iteration 109 : 17.44477774745625
Loss in iteration 110 : 14.412869921377645
Loss in iteration 111 : 16.88720455603074
Loss in iteration 112 : 14.037764312304297
Loss in iteration 113 : 16.657272261767382
Loss in iteration 114 : 13.794906258826954
Loss in iteration 115 : 17.00163782371371
Loss in iteration 116 : 13.21618866071105
Loss in iteration 117 : 16.561929512257365
Loss in iteration 118 : 12.687028867117023
Loss in iteration 119 : 16.425423916749583
Loss in iteration 120 : 13.975252691903545
Loss in iteration 121 : 17.566368093595784
Loss in iteration 122 : 14.923495597387625
Loss in iteration 123 : 17.651291672761587
Loss in iteration 124 : 14.691900350361633
Loss in iteration 125 : 17.086565816289664
Loss in iteration 126 : 14.073108774067867
Loss in iteration 127 : 16.490224058329094
Loss in iteration 128 : 13.926959700344689
Loss in iteration 129 : 16.724097120872795
Loss in iteration 130 : 13.486733880258923
Loss in iteration 131 : 16.74783194007909
Loss in iteration 132 : 12.648918570817733
Loss in iteration 133 : 16.180889616899602
Loss in iteration 134 : 13.198335797158744
Loss in iteration 135 : 17.08755643337657
Loss in iteration 136 : 14.737562004711414
Loss in iteration 137 : 17.758755860420614
Loss in iteration 138 : 14.979989668121762
Loss in iteration 139 : 17.333985772833692
Loss in iteration 140 : 14.259686231087315
Loss in iteration 141 : 16.516933911066985
Loss in iteration 142 : 14.026322316221963
Loss in iteration 143 : 16.503982698454905
Loss in iteration 144 : 13.662699427711148
Loss in iteration 145 : 16.799183699131174
Loss in iteration 146 : 12.971055705512692
Loss in iteration 147 : 16.33494924841564
Loss in iteration 148 : 12.648061583020448
Loss in iteration 149 : 16.386505132749317
Loss in iteration 150 : 14.278495527112018
Loss in iteration 151 : 17.654866267705184
Loss in iteration 152 : 14.918334963205647
Loss in iteration 153 : 17.362389105438584
Loss in iteration 154 : 14.623551072331262
Loss in iteration 155 : 16.833559105807346
Loss in iteration 156 : 14.09239140657933
Loss in iteration 157 : 16.38138461911296
Loss in iteration 158 : 13.810912612138464
Loss in iteration 159 : 16.555134509782416
Loss in iteration 160 : 13.504308934895947
Loss in iteration 161 : 16.7283832751954
Loss in iteration 162 : 12.552808732977267
Loss in iteration 163 : 16.013090655324408
Loss in iteration 164 : 13.136138528182464
Loss in iteration 165 : 16.90401817168505
Loss in iteration 166 : 14.723640594825545
Loss in iteration 167 : 17.669780991514372
Loss in iteration 168 : 15.022993292491632
Loss in iteration 169 : 17.190133069497723
Loss in iteration 170 : 14.350422297711452
Loss in iteration 171 : 16.45275064931419
Loss in iteration 172 : 13.990682560477381
Loss in iteration 173 : 16.320800385832364
Loss in iteration 174 : 13.636752111275767
Loss in iteration 175 : 16.625150896133768
Loss in iteration 176 : 13.071517402510333
Loss in iteration 177 : 16.329117907119933
Loss in iteration 178 : 12.571232888653267
Loss in iteration 179 : 16.207589707218588
Loss in iteration 180 : 14.252745894643509
Loss in iteration 181 : 17.532912818097024
Loss in iteration 182 : 14.896414901520561
Loss in iteration 183 : 17.201234619692965
Loss in iteration 184 : 14.583977929968166
Loss in iteration 185 : 16.715171043402705
Loss in iteration 186 : 14.051287201013881
Loss in iteration 187 : 16.18942005560917
Loss in iteration 188 : 13.897149950786181
Loss in iteration 189 : 16.351628961774626
Loss in iteration 190 : 13.672109088770766
Loss in iteration 191 : 16.809783897846124
Loss in iteration 192 : 12.601304515900955
Loss in iteration 193 : 15.896975577127417
Loss in iteration 194 : 12.860935001641613
Loss in iteration 195 : 16.61074788272237
Loss in iteration 196 : 14.726263458458286
Loss in iteration 197 : 17.629935144905577
Loss in iteration 198 : 15.018864896176416
Loss in iteration 199 : 17.078662359046895
Loss in iteration 200 : 14.3980902604307
Loss in iteration 201 : 16.334017353266937
Loss in iteration 202 : 14.113573316804787
Loss in iteration 203 : 16.195691988146425
Loss in iteration 204 : 13.699267314818513
Loss in iteration 205 : 16.395673940853293
Loss in iteration 206 : 13.33793555781735
Loss in iteration 207 : 16.54613980764669
Loss in iteration 208 : 12.615707893220716
Loss in iteration 209 : 16.014301773315267
Loss in iteration 210 : 13.694019606757804
Loss in iteration 211 : 16.967839018067362
Loss in iteration 212 : 14.841319627639955
Loss in iteration 213 : 17.225798566773936
Loss in iteration 214 : 14.635258205112699
Loss in iteration 215 : 16.730925298869323
Loss in iteration 216 : 14.13874108937148
Loss in iteration 217 : 16.169043751729863
Loss in iteration 218 : 14.02914527804985
Loss in iteration 219 : 16.240476801620694
Loss in iteration 220 : 13.681787026271108
Loss in iteration 221 : 16.665218080303827
Loss in iteration 222 : 13.015969510088187
Loss in iteration 223 : 16.14801683999602
Loss in iteration 224 : 12.5432218913043
Loss in iteration 225 : 16.128172266605112
Loss in iteration 226 : 14.46212853472963
Loss in iteration 227 : 17.45205257195877
Loss in iteration 228 : 14.969451358685028
Loss in iteration 229 : 16.990481428282408
Loss in iteration 230 : 14.538112380060301
Loss in iteration 231 : 16.505719675809516
Loss in iteration 232 : 13.912099439036421
Loss in iteration 233 : 15.929960780099556
Loss in iteration 234 : 13.881170273612101
Loss in iteration 235 : 16.193053602391714
Loss in iteration 236 : 13.751168650557876
Loss in iteration 237 : 16.79949891225003
Loss in iteration 238 : 12.798891882125666
Loss in iteration 239 : 15.957474306477675
Loss in iteration 240 : 12.866176316430279
Loss in iteration 241 : 16.406703091447284
Loss in iteration 242 : 14.64338826538599
Loss in iteration 243 : 17.37128282538919
Loss in iteration 244 : 14.963774477863677
Loss in iteration 245 : 16.871740652073814
Loss in iteration 246 : 14.47640421050452
Loss in iteration 247 : 16.264317905963996
Loss in iteration 248 : 14.024393053704644
Loss in iteration 249 : 15.97233095047869
Loss in iteration 250 : 13.688526288730472
Loss in iteration 251 : 16.220135393534765
Loss in iteration 252 : 13.523650955465678
Loss in iteration 253 : 16.654330715909865
Loss in iteration 254 : 12.582466026156846
Loss in iteration 255 : 15.8590855295894
Loss in iteration 256 : 13.590246759332095
Loss in iteration 257 : 16.80886751204892
Loss in iteration 258 : 14.88870425716997
Loss in iteration 259 : 17.193349389963796
Loss in iteration 260 : 14.73575424490432
Loss in iteration 261 : 16.6181539504029
Loss in iteration 262 : 14.194499177772318
Loss in iteration 263 : 16.001516637202087
Loss in iteration 264 : 14.058807631521491
Loss in iteration 265 : 16.036051589663177
Loss in iteration 266 : 13.637141862407105
Loss in iteration 267 : 16.356488450307307
Loss in iteration 268 : 13.260685268928595
Loss in iteration 269 : 16.383994064136186
Loss in iteration 270 : 12.505381361280394
Loss in iteration 271 : 15.903113614271085
Loss in iteration 272 : 14.152075975998029
Loss in iteration 273 : 17.147313627338914
Loss in iteration 274 : 15.034055203510965
Loss in iteration 275 : 17.03284651247833
Loss in iteration 276 : 14.650101262456708
Loss in iteration 277 : 16.455127469593553
Loss in iteration 278 : 14.033704687793403
Loss in iteration 279 : 15.822705022114022
Loss in iteration 280 : 13.890610430808989
Loss in iteration 281 : 16.011134408973138
Loss in iteration 282 : 13.728703825646518
Loss in iteration 283 : 16.578327366504485
Loss in iteration 284 : 13.109175716729354
Loss in iteration 285 : 16.127641579493677
Loss in iteration 286 : 12.679280203965158
Loss in iteration 287 : 16.08145939764113
Loss in iteration 288 : 14.437328484346839
Loss in iteration 289 : 17.205504490247524
Loss in iteration 290 : 14.92617899660606
Loss in iteration 291 : 16.84603541457946
Loss in iteration 292 : 14.618424228264345
Loss in iteration 293 : 16.342058425382714
Loss in iteration 294 : 13.926557214104422
Loss in iteration 295 : 15.78038368314703
Loss in iteration 296 : 13.717875697239219
Loss in iteration 297 : 16.090592839866197
Loss in iteration 298 : 13.745511818525925
Loss in iteration 299 : 16.7186152863029
Loss in iteration 300 : 12.703960303931963
Loss in iteration 301 : 15.774166557105433
Loss in iteration 302 : 13.419967222449625
Loss in iteration 303 : 16.567496124215875
Loss in iteration 304 : 14.71698160519744
Loss in iteration 305 : 17.0799164465796
Loss in iteration 306 : 14.829191813209924
Loss in iteration 307 : 16.62561492988363
Loss in iteration 308 : 14.351603737366899
Loss in iteration 309 : 15.991283264787715
Loss in iteration 310 : 13.955909630133146
Loss in iteration 311 : 15.89402345317392
Loss in iteration 312 : 13.688531303273855
Loss in iteration 313 : 16.331005480463226
Loss in iteration 314 : 13.294191043146416
Loss in iteration 315 : 16.300676940944992
Loss in iteration 316 : 12.59408524983673
Loss in iteration 317 : 15.887415648481209
Loss in iteration 318 : 14.217450378168245
Loss in iteration 319 : 17.058345375045743
Loss in iteration 320 : 14.901688847768371
Loss in iteration 321 : 16.864106053316174
Loss in iteration 322 : 14.72992267064606
Loss in iteration 323 : 16.40209505295782
Loss in iteration 324 : 13.97619192190079
Loss in iteration 325 : 15.731705236230496
Loss in iteration 326 : 13.816723387675408
Loss in iteration 327 : 16.018218775008215
Loss in iteration 328 : 13.749626930091237
Loss in iteration 329 : 16.52811301216386
Loss in iteration 330 : 13.062548335064038
Loss in iteration 331 : 15.963446594843452
Loss in iteration 332 : 13.077310495602184
Loss in iteration 333 : 16.288558253567526
Loss in iteration 334 : 14.477313892242345
Loss in iteration 335 : 17.046325736722917
Loss in iteration 336 : 14.764167348233215
Loss in iteration 337 : 16.58121764787485
Loss in iteration 338 : 14.576679811328901
Loss in iteration 339 : 16.245111603003213
Loss in iteration 340 : 13.786398163431924
Loss in iteration 341 : 15.755249524193198
Loss in iteration 342 : 13.792255621833974
Loss in iteration 343 : 16.297634466823613
Loss in iteration 344 : 13.52247276860743
Loss in iteration 345 : 16.351877748300822
Loss in iteration 346 : 12.805506878601394
Loss in iteration 347 : 15.919126525776445
Loss in iteration 348 : 13.98195507320064
Loss in iteration 349 : 16.747041300341024
Loss in iteration 350 : 14.611024613834427
Loss in iteration 351 : 16.75626263035884
Loss in iteration 352 : 14.712061986566248
Loss in iteration 353 : 16.412378653867407
Loss in iteration 354 : 14.074255536449368
Loss in iteration 355 : 15.86947833163676
Loss in iteration 356 : 13.877919465292534
Loss in iteration 357 : 16.078681847921782
Loss in iteration 358 : 13.788690459784226
Loss in iteration 359 : 16.548410494409886
Loss in iteration 360 : 12.867124956516419
Loss in iteration 361 : 15.754497185647862
Loss in iteration 362 : 13.27601306305579
Loss in iteration 363 : 16.412421165222593
Loss in iteration 364 : 14.600763158345812
Loss in iteration 365 : 17.00520067365095
Loss in iteration 366 : 14.845816540868768
Loss in iteration 367 : 16.51531905620506
Loss in iteration 368 : 14.45027318361125
Loss in iteration 369 : 16.052631329294265
Loss in iteration 370 : 13.808648192821554
Loss in iteration 371 : 15.827174271544715
Loss in iteration 372 : 13.770860134904614
Loss in iteration 373 : 16.331084751206326
Loss in iteration 374 : 13.327510857461425
Loss in iteration 375 : 16.12305748371617
Loss in iteration 376 : 12.85213831574214
Loss in iteration 377 : 15.993957743852048
Loss in iteration 378 : 14.23081027129572
Loss in iteration 379 : 16.877468283460225
Loss in iteration 380 : 14.691420068088913
Loss in iteration 381 : 16.66461068692061
Loss in iteration 382 : 14.65099800911331
Loss in iteration 383 : 16.295927350528384
Loss in iteration 384 : 13.963575595480549
Loss in iteration 385 : 15.809054319690455
Loss in iteration 386 : 13.837675242232992
Loss in iteration 387 : 16.126723031563316
Loss in iteration 388 : 13.748218246411863
Loss in iteration 389 : 16.502474799355152
Loss in iteration 390 : 12.767847686530573
Loss in iteration 391 : 15.730122422985797
Loss in iteration 392 : 13.609258482962007
Loss in iteration 393 : 16.531598834015945
Loss in iteration 394 : 14.711820803731229
Loss in iteration 395 : 16.891694052393053
Loss in iteration 396 : 14.808578200926652
Loss in iteration 397 : 16.390272197070438
Loss in iteration 398 : 14.310862219589685
Loss in iteration 399 : 15.91144980161417
Loss in iteration 400 : 13.862381099823484
Testing accuracy  of updater 5 on alg 0 with rate 10.0 = 0.720125, training accuracy 0.720125, time elapsed: 7811 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 6.565648976939898
Loss in iteration 3 : 60.4608779709339
Loss in iteration 4 : 10.361724772734934
Loss in iteration 5 : 41.184401759805404
Loss in iteration 6 : 14.001047108511033
Loss in iteration 7 : 27.72443449980709
Loss in iteration 8 : 16.707848034607633
Loss in iteration 9 : 21.146236740687375
Loss in iteration 10 : 18.386897034731366
Loss in iteration 11 : 15.444381481878295
Loss in iteration 12 : 20.161015650871143
Loss in iteration 13 : 10.924171713277968
Loss in iteration 14 : 18.829686366133338
Loss in iteration 15 : 11.116292922920554
Loss in iteration 16 : 15.868539218128639
Loss in iteration 17 : 10.445108639195677
Loss in iteration 18 : 13.126178355742283
Loss in iteration 19 : 13.487111111289941
Loss in iteration 20 : 15.07850529894119
Loss in iteration 21 : 12.923016621314284
Loss in iteration 22 : 14.491068861049568
Loss in iteration 23 : 12.097712454861055
Loss in iteration 24 : 13.097686698243551
Loss in iteration 25 : 12.12684101308442
Loss in iteration 26 : 12.131957245202422
Loss in iteration 27 : 12.078728127267546
Loss in iteration 28 : 11.443239199668408
Loss in iteration 29 : 12.060402572368105
Loss in iteration 30 : 10.841542147877087
Loss in iteration 31 : 12.431127216193998
Loss in iteration 32 : 9.680694725374709
Loss in iteration 33 : 12.386841876328058
Loss in iteration 34 : 8.411082128742452
Loss in iteration 35 : 11.911956418954551
Loss in iteration 36 : 9.797254358825317
Loss in iteration 37 : 13.304620463946964
Loss in iteration 38 : 11.525696333887069
Loss in iteration 39 : 12.829185770936762
Loss in iteration 40 : 10.83498445335876
Loss in iteration 41 : 12.317549870356865
Loss in iteration 42 : 10.272939512792135
Loss in iteration 43 : 12.06679268379575
Loss in iteration 44 : 10.228685014419222
Loss in iteration 45 : 12.02242265780273
Loss in iteration 46 : 9.89665518686303
Loss in iteration 47 : 11.926619379816723
Loss in iteration 48 : 9.809865596578476
Loss in iteration 49 : 12.057394846028988
Loss in iteration 50 : 9.676654273090808
Loss in iteration 51 : 12.077562218717151
Loss in iteration 52 : 9.660008969063728
Loss in iteration 53 : 12.194850441124647
Loss in iteration 54 : 8.80733857831591
Loss in iteration 55 : 11.926155635029001
Loss in iteration 56 : 9.51110511862171
Loss in iteration 57 : 12.618408817670952
Loss in iteration 58 : 10.532476118921664
Loss in iteration 59 : 12.643705171572238
Loss in iteration 60 : 10.127611058208805
Loss in iteration 61 : 12.26058446095172
Loss in iteration 62 : 9.791018306216346
Loss in iteration 63 : 12.033981618167967
Loss in iteration 64 : 9.640942820692493
Loss in iteration 65 : 12.037588258230072
Loss in iteration 66 : 9.498119388079035
Loss in iteration 67 : 12.035582319024869
Loss in iteration 68 : 9.415884070405356
Loss in iteration 69 : 12.08763225326003
Loss in iteration 70 : 9.424331137207231
Loss in iteration 71 : 12.321132027643069
Loss in iteration 72 : 8.795070327718859
Loss in iteration 73 : 12.309967333939415
Loss in iteration 74 : 8.778924447062108
Loss in iteration 75 : 11.97062432052434
Loss in iteration 76 : 9.529641189619086
Loss in iteration 77 : 12.736826877114366
Loss in iteration 78 : 10.4194393346071
Loss in iteration 79 : 12.7835401592975
Loss in iteration 80 : 10.114793177247797
Loss in iteration 81 : 12.306856273416775
Loss in iteration 82 : 9.738759165433533
Loss in iteration 83 : 11.994340546417279
Loss in iteration 84 : 9.688173703560087
Loss in iteration 85 : 11.936260840030933
Loss in iteration 86 : 9.514744626355364
Loss in iteration 87 : 11.888741795663524
Loss in iteration 88 : 9.422611027003128
Loss in iteration 89 : 11.833232902165655
Loss in iteration 90 : 9.41244236199786
Loss in iteration 91 : 11.905820647834421
Loss in iteration 92 : 9.638335796655987
Loss in iteration 93 : 12.276995501850648
Loss in iteration 94 : 8.7943157142203
Loss in iteration 95 : 11.612844187842985
Loss in iteration 96 : 8.806579773194754
Loss in iteration 97 : 11.915014915499956
Loss in iteration 98 : 10.25096694096733
Loss in iteration 99 : 12.848766555927659
Loss in iteration 100 : 10.429686812629177
Loss in iteration 101 : 12.525204753249305
Loss in iteration 102 : 9.803055859224202
Loss in iteration 103 : 11.820779889361436
Loss in iteration 104 : 9.522571780992322
Loss in iteration 105 : 11.698037518050505
Loss in iteration 106 : 9.339558000591783
Loss in iteration 107 : 11.824613582480604
Loss in iteration 108 : 9.28663781120666
Loss in iteration 109 : 12.211640228476899
Loss in iteration 110 : 8.866396201599622
Loss in iteration 111 : 11.711805398715223
Loss in iteration 112 : 8.736580185036306
Loss in iteration 113 : 11.97156262738643
Loss in iteration 114 : 9.925051744712116
Loss in iteration 115 : 12.593390330297071
Loss in iteration 116 : 10.356783170362448
Loss in iteration 117 : 12.452758353107486
Loss in iteration 118 : 9.88253173966264
Loss in iteration 119 : 11.857944735492445
Loss in iteration 120 : 9.569080612399485
Loss in iteration 121 : 11.704885720539608
Loss in iteration 122 : 9.399626627341151
Loss in iteration 123 : 11.817676252321474
Loss in iteration 124 : 9.36398694142611
Loss in iteration 125 : 12.254867016180237
Loss in iteration 126 : 8.776678009399669
Loss in iteration 127 : 11.525215962825422
Loss in iteration 128 : 8.730039084577198
Loss in iteration 129 : 11.899753907610961
Loss in iteration 130 : 10.081243750408602
Loss in iteration 131 : 12.680905137431772
Loss in iteration 132 : 10.34394942933663
Loss in iteration 133 : 12.32328249699585
Loss in iteration 134 : 9.83447505901354
Loss in iteration 135 : 11.792510896524197
Loss in iteration 136 : 9.613702504240921
Loss in iteration 137 : 11.659866698665034
Loss in iteration 138 : 9.479440681254127
Loss in iteration 139 : 11.73142874860267
Loss in iteration 140 : 9.440375260763682
Loss in iteration 141 : 12.210678224236547
Loss in iteration 142 : 8.847117111438209
Loss in iteration 143 : 11.526177120030615
Loss in iteration 144 : 8.654307012002555
Loss in iteration 145 : 11.639790411156012
Loss in iteration 146 : 9.98928133261195
Loss in iteration 147 : 12.601219733272288
Loss in iteration 148 : 10.328324509355337
Loss in iteration 149 : 12.28388881919054
Loss in iteration 150 : 9.945800773641116
Loss in iteration 151 : 11.825578100633747
Loss in iteration 152 : 9.66678071811709
Loss in iteration 153 : 11.588816967705469
Loss in iteration 154 : 9.56057892563205
Loss in iteration 155 : 11.631159991131437
Loss in iteration 156 : 9.434133736049306
Loss in iteration 157 : 12.031497289057471
Loss in iteration 158 : 9.04452955533407
Loss in iteration 159 : 11.726047943741648
Loss in iteration 160 : 8.541128600725525
Loss in iteration 161 : 11.353147710621482
Loss in iteration 162 : 9.737743697351663
Loss in iteration 163 : 12.430896282917406
Loss in iteration 164 : 10.409337365519503
Loss in iteration 165 : 12.338657618056814
Loss in iteration 166 : 10.079643930845311
Loss in iteration 167 : 11.860456920898086
Loss in iteration 168 : 9.7278332425613
Loss in iteration 169 : 11.508010981292609
Loss in iteration 170 : 9.620874728594842
Loss in iteration 171 : 11.513413555284462
Loss in iteration 172 : 9.464696723111741
Loss in iteration 173 : 11.725024939612442
Loss in iteration 174 : 9.406887505219968
Loss in iteration 175 : 12.117391297046565
Loss in iteration 176 : 8.529843545417119
Loss in iteration 177 : 11.146450005522938
Loss in iteration 178 : 8.998797885151177
Loss in iteration 179 : 11.920315221268838
Loss in iteration 180 : 10.344067985675421
Loss in iteration 181 : 12.521350994190277
Loss in iteration 182 : 10.377871005984067
Loss in iteration 183 : 12.095825717270701
Loss in iteration 184 : 9.892863685887864
Loss in iteration 185 : 11.510389643090944
Loss in iteration 186 : 9.706015855725486
Loss in iteration 187 : 11.457127493369928
Loss in iteration 188 : 9.499732463049902
Loss in iteration 189 : 11.485004377514107
Loss in iteration 190 : 9.36599907002506
Loss in iteration 191 : 11.929895128008974
Loss in iteration 192 : 8.998115391711313
Loss in iteration 193 : 11.486220522281817
Loss in iteration 194 : 8.694703845839841
Loss in iteration 195 : 11.423807908374304
Loss in iteration 196 : 9.946662646178352
Loss in iteration 197 : 12.390685713610845
Loss in iteration 198 : 10.43905270118243
Loss in iteration 199 : 12.138774009758563
Loss in iteration 200 : 10.093864604703022
Loss in iteration 201 : 11.727236612496286
Loss in iteration 202 : 9.685193460904928
Loss in iteration 203 : 11.352668753466082
Loss in iteration 204 : 9.681101818413502
Loss in iteration 205 : 11.437533886078263
Loss in iteration 206 : 9.504003594994254
Loss in iteration 207 : 11.572035431244146
Loss in iteration 208 : 9.414083785348954
Loss in iteration 209 : 11.990930194301244
Loss in iteration 210 : 8.799973143865403
Loss in iteration 211 : 11.185992930292446
Loss in iteration 212 : 8.80225065296153
Loss in iteration 213 : 11.54898063155054
Loss in iteration 214 : 10.294521802252866
Loss in iteration 215 : 12.490204849906483
Loss in iteration 216 : 10.450624809441031
Loss in iteration 217 : 12.048095407777046
Loss in iteration 218 : 9.979466131415764
Loss in iteration 219 : 11.476930916975427
Loss in iteration 220 : 9.749650237145278
Loss in iteration 221 : 11.34592455140831
Loss in iteration 222 : 9.58785893185505
Loss in iteration 223 : 11.343845099468073
Loss in iteration 224 : 9.41527304716699
Loss in iteration 225 : 11.771673081639463
Loss in iteration 226 : 9.12685300720363
Loss in iteration 227 : 11.579401300708923
Loss in iteration 228 : 8.622367789079197
Loss in iteration 229 : 11.215720718413174
Loss in iteration 230 : 9.841295364912261
Loss in iteration 231 : 12.225643065550605
Loss in iteration 232 : 10.491681669405148
Loss in iteration 233 : 12.090273463208646
Loss in iteration 234 : 10.126338575378178
Loss in iteration 235 : 11.691722745599094
Loss in iteration 236 : 9.703162562400845
Loss in iteration 237 : 11.22861886217074
Loss in iteration 238 : 9.741435366764739
Loss in iteration 239 : 11.355768484823368
Loss in iteration 240 : 9.541052829595767
Loss in iteration 241 : 11.403083097535658
Loss in iteration 242 : 9.400241351047928
Loss in iteration 243 : 11.868790568862918
Loss in iteration 244 : 9.040351498259595
Loss in iteration 245 : 11.387573015330789
Loss in iteration 246 : 8.706211568769188
Loss in iteration 247 : 11.286851005939555
Loss in iteration 248 : 10.050126018540611
Loss in iteration 249 : 12.29171886255238
Loss in iteration 250 : 10.492167153289298
Loss in iteration 251 : 12.017169433583877
Loss in iteration 252 : 10.085594375264227
Loss in iteration 253 : 11.561949173952284
Loss in iteration 254 : 9.73805788910768
Loss in iteration 255 : 11.235140923098028
Loss in iteration 256 : 9.710560414760954
Loss in iteration 257 : 11.308852500686319
Loss in iteration 258 : 9.465082461778469
Loss in iteration 259 : 11.49392069656621
Loss in iteration 260 : 9.395904929386147
Loss in iteration 261 : 11.904627177352374
Loss in iteration 262 : 8.673261747005627
Loss in iteration 263 : 11.078876204662949
Loss in iteration 264 : 9.23661318377749
Loss in iteration 265 : 11.737840692797109
Loss in iteration 266 : 10.450772979352806
Loss in iteration 267 : 12.229729184498224
Loss in iteration 268 : 10.311355871749885
Loss in iteration 269 : 11.789607680753681
Loss in iteration 270 : 9.904531901390342
Loss in iteration 271 : 11.27907170277732
Loss in iteration 272 : 9.849383273008025
Loss in iteration 273 : 11.292998307592311
Loss in iteration 274 : 9.504347020974077
Loss in iteration 275 : 11.160427975999555
Loss in iteration 276 : 9.39066981889061
Loss in iteration 277 : 11.560227507585001
Loss in iteration 278 : 9.395613085298478
Loss in iteration 279 : 11.700609927183999
Loss in iteration 280 : 8.564712294174695
Loss in iteration 281 : 11.087501374918771
Loss in iteration 282 : 9.759955179065575
Loss in iteration 283 : 12.07623135707233
Loss in iteration 284 : 10.574047677388984
Loss in iteration 285 : 12.060979973100604
Loss in iteration 286 : 10.185106437934927
Loss in iteration 287 : 11.598977817448198
Loss in iteration 288 : 9.7918323565584
Loss in iteration 289 : 11.17442036054883
Loss in iteration 290 : 9.787580499972545
Loss in iteration 291 : 11.273042320489832
Loss in iteration 292 : 9.435298079620583
Loss in iteration 293 : 11.333171670564877
Loss in iteration 294 : 9.477650758226563
Loss in iteration 295 : 11.862784405482248
Loss in iteration 296 : 8.79072988421334
Loss in iteration 297 : 11.095469008237545
Loss in iteration 298 : 9.173442052490321
Loss in iteration 299 : 11.585826784446562
Loss in iteration 300 : 10.265821235668048
Loss in iteration 301 : 12.135684185088575
Loss in iteration 302 : 10.391844891665428
Loss in iteration 303 : 11.785395881771548
Loss in iteration 304 : 10.007199107447502
Loss in iteration 305 : 11.319418384648584
Loss in iteration 306 : 9.836404398773098
Loss in iteration 307 : 11.206832197874862
Loss in iteration 308 : 9.497524658640108
Loss in iteration 309 : 11.151317614506645
Loss in iteration 310 : 9.385451909837215
Loss in iteration 311 : 11.579947784838671
Loss in iteration 312 : 9.337300843577236
Loss in iteration 313 : 11.56297376168814
Loss in iteration 314 : 8.631600950119232
Loss in iteration 315 : 11.0867787353594
Loss in iteration 316 : 9.987024027409847
Loss in iteration 317 : 12.12660473031013
Loss in iteration 318 : 10.534533218109482
Loss in iteration 319 : 11.91320733080867
Loss in iteration 320 : 10.204301253641761
Loss in iteration 321 : 11.523511562026222
Loss in iteration 322 : 9.717985896385159
Loss in iteration 323 : 11.06148880051144
Loss in iteration 324 : 9.713495099881165
Loss in iteration 325 : 11.253215831612048
Loss in iteration 326 : 9.418536462857446
Loss in iteration 327 : 11.474097959698108
Loss in iteration 328 : 9.358609747714453
Loss in iteration 329 : 11.61858958552704
Loss in iteration 330 : 8.647198738289942
Loss in iteration 331 : 11.01168535466145
Loss in iteration 332 : 9.851600223896726
Loss in iteration 333 : 12.00266112633222
Loss in iteration 334 : 10.470193407214374
Loss in iteration 335 : 11.910910815512425
Loss in iteration 336 : 10.268091120345472
Loss in iteration 337 : 11.57545140760932
Loss in iteration 338 : 9.718721731565154
Loss in iteration 339 : 11.022697692295296
Loss in iteration 340 : 9.767417932742472
Loss in iteration 341 : 11.258169770009934
Loss in iteration 342 : 9.416695966898958
Loss in iteration 343 : 11.412534897528177
Loss in iteration 344 : 9.412795751849448
Loss in iteration 345 : 11.631374383497025
Loss in iteration 346 : 8.638389245463545
Loss in iteration 347 : 10.966591326466174
Loss in iteration 348 : 9.786185681592913
Loss in iteration 349 : 11.940698488016015
Loss in iteration 350 : 10.472984895595568
Loss in iteration 351 : 11.937392705562146
Loss in iteration 352 : 10.291401680815602
Loss in iteration 353 : 11.570169646510912
Loss in iteration 354 : 9.752513006470675
Loss in iteration 355 : 11.025362460274057
Loss in iteration 356 : 9.789949872912924
Loss in iteration 357 : 11.245910342342517
Loss in iteration 358 : 9.417039520886183
Loss in iteration 359 : 11.36259596974236
Loss in iteration 360 : 9.442468248248995
Loss in iteration 361 : 11.630156286674081
Loss in iteration 362 : 8.659735346277062
Loss in iteration 363 : 10.95584133666529
Loss in iteration 364 : 9.732329393664962
Loss in iteration 365 : 11.880424832491236
Loss in iteration 366 : 10.453595307398096
Loss in iteration 367 : 11.94271657063855
Loss in iteration 368 : 10.301331843585142
Loss in iteration 369 : 11.559511772538611
Loss in iteration 370 : 9.786687739212958
Loss in iteration 371 : 11.038772292841593
Loss in iteration 372 : 9.814398709907891
Loss in iteration 373 : 11.239133264108375
Loss in iteration 374 : 9.42082962784312
Loss in iteration 375 : 11.319693359166758
Loss in iteration 376 : 9.467987164411436
Loss in iteration 377 : 11.631587639393238
Loss in iteration 378 : 8.683690673376685
Loss in iteration 379 : 10.949508357479132
Loss in iteration 380 : 9.681979033045016
Loss in iteration 381 : 11.821565429839906
Loss in iteration 382 : 10.43855051675293
Loss in iteration 383 : 11.943764067538059
Loss in iteration 384 : 10.314120357344535
Loss in iteration 385 : 11.544398111705425
Loss in iteration 386 : 9.835878107666813
Loss in iteration 387 : 11.062065564336997
Loss in iteration 388 : 9.828192389973532
Loss in iteration 389 : 11.219062091542964
Loss in iteration 390 : 9.421305302502212
Loss in iteration 391 : 11.267602938445616
Loss in iteration 392 : 9.50599885167028
Loss in iteration 393 : 11.650558259932488
Loss in iteration 394 : 8.720633230238297
Loss in iteration 395 : 10.94590379491328
Loss in iteration 396 : 9.595274926554557
Loss in iteration 397 : 11.74288129746758
Loss in iteration 398 : 10.409567939397482
Loss in iteration 399 : 11.951516652596641
Loss in iteration 400 : 10.336862780574265
Testing accuracy  of updater 5 on alg 0 with rate 7.0 = 0.715875, training accuracy 0.715875, time elapsed: 7806 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 3.7648041356685935
Loss in iteration 3 : 34.57798331291874
Loss in iteration 4 : 5.875203132267503
Loss in iteration 5 : 23.525246109057733
Loss in iteration 6 : 8.032524599139364
Loss in iteration 7 : 15.687280341094633
Loss in iteration 8 : 9.686131376445555
Loss in iteration 9 : 11.991654027267943
Loss in iteration 10 : 10.629150956532532
Loss in iteration 11 : 8.690235485373796
Loss in iteration 12 : 11.621370945618862
Loss in iteration 13 : 6.156737187106353
Loss in iteration 14 : 10.761346821499473
Loss in iteration 15 : 6.286611387199489
Loss in iteration 16 : 9.471877540019596
Loss in iteration 17 : 6.217026125218645
Loss in iteration 18 : 7.653852794610843
Loss in iteration 19 : 6.228303941460599
Loss in iteration 20 : 8.27367800347795
Loss in iteration 21 : 8.095776064473887
Loss in iteration 22 : 8.678946369492035
Loss in iteration 23 : 6.92372691105593
Loss in iteration 24 : 7.775106646702425
Loss in iteration 25 : 6.911501989960762
Loss in iteration 26 : 7.102450819849704
Loss in iteration 27 : 6.925809897021621
Loss in iteration 28 : 6.7009860933678
Loss in iteration 29 : 6.860709308967573
Loss in iteration 30 : 6.360948296742343
Loss in iteration 31 : 6.849297945907313
Loss in iteration 32 : 6.105283074534649
Loss in iteration 33 : 6.868916310837778
Loss in iteration 34 : 5.901833398628599
Loss in iteration 35 : 6.948326367415928
Loss in iteration 36 : 5.822229944086685
Loss in iteration 37 : 6.911091621279343
Loss in iteration 38 : 5.303976471299627
Loss in iteration 39 : 6.737864589942301
Loss in iteration 40 : 5.373310476209746
Loss in iteration 41 : 7.210306702884459
Loss in iteration 42 : 6.283243435952867
Loss in iteration 43 : 7.278383988764417
Loss in iteration 44 : 6.1579340017980995
Loss in iteration 45 : 7.042557154152742
Loss in iteration 46 : 5.828780049590202
Loss in iteration 47 : 6.8880205613132475
Loss in iteration 48 : 5.801779357926645
Loss in iteration 49 : 6.873028993616865
Loss in iteration 50 : 5.6468245765473455
Loss in iteration 51 : 6.846987225583943
Loss in iteration 52 : 5.585987515892632
Loss in iteration 53 : 6.87342128086417
Loss in iteration 54 : 5.542782339323805
Loss in iteration 55 : 6.873385002612037
Loss in iteration 56 : 5.583071904708384
Loss in iteration 57 : 6.9195654061943666
Loss in iteration 58 : 5.6351554533820245
Loss in iteration 59 : 7.1207029041373975
Loss in iteration 60 : 5.1122953375376
Loss in iteration 61 : 6.737100436808975
Loss in iteration 62 : 5.032153350464969
Loss in iteration 63 : 6.950521248709334
Loss in iteration 64 : 6.025453350703938
Loss in iteration 65 : 7.390202520243747
Loss in iteration 66 : 5.944153250347636
Loss in iteration 67 : 7.148965965343572
Loss in iteration 68 : 5.64749071477906
Loss in iteration 69 : 6.868268554162628
Loss in iteration 70 : 5.642244872842389
Loss in iteration 71 : 6.874403566402875
Loss in iteration 72 : 5.504862767218322
Loss in iteration 73 : 6.823134620827838
Loss in iteration 74 : 5.4677289826467455
Loss in iteration 75 : 6.8494180291222415
Loss in iteration 76 : 5.389826796717745
Loss in iteration 77 : 6.9468795255037
Loss in iteration 78 : 5.250761947765725
Loss in iteration 79 : 7.04327123725333
Loss in iteration 80 : 5.016178879185015
Loss in iteration 81 : 6.65766992357483
Loss in iteration 82 : 5.178501486495332
Loss in iteration 83 : 7.122588860295196
Loss in iteration 84 : 5.938144988674733
Loss in iteration 85 : 7.394715678120432
Loss in iteration 86 : 5.929088704785194
Loss in iteration 87 : 7.12491655196256
Loss in iteration 88 : 5.65526742796022
Loss in iteration 89 : 6.842842190893829
Loss in iteration 90 : 5.567144970781241
Loss in iteration 91 : 6.7640455367328
Loss in iteration 92 : 5.471154654354574
Loss in iteration 93 : 6.738993368611276
Loss in iteration 94 : 5.4149974462749215
Loss in iteration 95 : 6.747385482699126
Loss in iteration 96 : 5.392853182491135
Loss in iteration 97 : 6.8649399387048415
Loss in iteration 98 : 5.4008751468090805
Loss in iteration 99 : 6.963355309048239
Loss in iteration 100 : 4.966800752442696
Loss in iteration 101 : 6.759463194591992
Loss in iteration 102 : 5.246701918373316
Loss in iteration 103 : 6.909517174771291
Loss in iteration 104 : 5.593277315657074
Loss in iteration 105 : 7.158717795522256
Loss in iteration 106 : 5.937592713646493
Loss in iteration 107 : 7.158584560315305
Loss in iteration 108 : 5.706607029038212
Loss in iteration 109 : 6.847760697959675
Loss in iteration 110 : 5.5414103798432155
Loss in iteration 111 : 6.7211406859023315
Loss in iteration 112 : 5.4117925155492586
Loss in iteration 113 : 6.6971555132939615
Loss in iteration 114 : 5.395740209759774
Loss in iteration 115 : 6.920952530841028
Loss in iteration 116 : 5.243227484323037
Loss in iteration 117 : 6.911302540900305
Loss in iteration 118 : 4.863101230980834
Loss in iteration 119 : 6.531372965612256
Loss in iteration 120 : 5.399915166370967
Loss in iteration 121 : 7.127551427966287
Loss in iteration 122 : 6.028747824133333
Loss in iteration 123 : 7.256989495901577
Loss in iteration 124 : 5.825884397097846
Loss in iteration 125 : 6.913438977212189
Loss in iteration 126 : 5.545276891205224
Loss in iteration 127 : 6.648538337121323
Loss in iteration 128 : 5.496801332856191
Loss in iteration 129 : 6.661511443290537
Loss in iteration 130 : 5.425386852435768
Loss in iteration 131 : 6.7090587149856455
Loss in iteration 132 : 5.4280629520172425
Loss in iteration 133 : 6.986951178742178
Loss in iteration 134 : 5.1128627204595105
Loss in iteration 135 : 6.628134942648454
Loss in iteration 136 : 4.914725757219486
Loss in iteration 137 : 6.625467727510462
Loss in iteration 138 : 5.756135874959982
Loss in iteration 139 : 7.249501212166733
Loss in iteration 140 : 5.979068979985416
Loss in iteration 141 : 7.069232550262843
Loss in iteration 142 : 5.716125846178091
Loss in iteration 143 : 6.7571962128378695
Loss in iteration 144 : 5.5480036672903825
Loss in iteration 145 : 6.613576087570362
Loss in iteration 146 : 5.497988101756801
Loss in iteration 147 : 6.634237361337847
Loss in iteration 148 : 5.419568775265867
Loss in iteration 149 : 6.773310574762812
Loss in iteration 150 : 5.364424643395127
Loss in iteration 151 : 6.914961371129373
Loss in iteration 152 : 4.887670837195493
Loss in iteration 153 : 6.412299372435856
Loss in iteration 154 : 5.227715093405068
Loss in iteration 155 : 6.914210581585635
Loss in iteration 156 : 5.98401448689641
Loss in iteration 157 : 7.211735852386944
Loss in iteration 158 : 5.9018583759529495
Loss in iteration 159 : 6.914825718272507
Loss in iteration 160 : 5.629982983301961
Loss in iteration 161 : 6.61198829072947
Loss in iteration 162 : 5.564097314783867
Loss in iteration 163 : 6.592287302355464
Loss in iteration 164 : 5.457133904723062
Loss in iteration 165 : 6.598228976240481
Loss in iteration 166 : 5.418769099011208
Loss in iteration 167 : 6.855699528730349
Loss in iteration 168 : 5.187950376156781
Loss in iteration 169 : 6.661569010417074
Loss in iteration 170 : 4.913507219903024
Loss in iteration 171 : 6.490011930498645
Loss in iteration 172 : 5.654196178206353
Loss in iteration 173 : 7.110487910602305
Loss in iteration 174 : 5.997233473071004
Loss in iteration 175 : 7.023781254016299
Loss in iteration 176 : 5.7965779756317515
Loss in iteration 177 : 6.734535710469571
Loss in iteration 178 : 5.583143825763409
Loss in iteration 179 : 6.53643721437468
Loss in iteration 180 : 5.534855488957356
Loss in iteration 181 : 6.5519897989825475
Loss in iteration 182 : 5.446702568634436
Loss in iteration 183 : 6.617954736417407
Loss in iteration 184 : 5.425518205013616
Loss in iteration 185 : 6.883966993761752
Loss in iteration 186 : 5.081343704248755
Loss in iteration 187 : 6.451763963673244
Loss in iteration 188 : 5.015119764481764
Loss in iteration 189 : 6.603767089772837
Loss in iteration 190 : 5.871503277823705
Loss in iteration 191 : 7.163552919132518
Loss in iteration 192 : 5.9982386226278805
Loss in iteration 193 : 6.9262052540166605
Loss in iteration 194 : 5.742956421884869
Loss in iteration 195 : 6.621017005674399
Loss in iteration 196 : 5.576933051858357
Loss in iteration 197 : 6.503751064847029
Loss in iteration 198 : 5.505624401217264
Loss in iteration 199 : 6.522975501519569
Loss in iteration 200 : 5.4261185042604065
Loss in iteration 201 : 6.694982332628446
Loss in iteration 202 : 5.3451270652595175
Loss in iteration 203 : 6.766970854565781
Loss in iteration 204 : 4.909912672701248
Loss in iteration 205 : 6.365464528825218
Loss in iteration 206 : 5.4217093090254185
Loss in iteration 207 : 6.902822620822483
Loss in iteration 208 : 6.007549056354322
Loss in iteration 209 : 7.02852368738341
Loss in iteration 210 : 5.8943025101217845
Loss in iteration 211 : 6.7713380900692455
Loss in iteration 212 : 5.6089289269082
Loss in iteration 213 : 6.482238984742931
Loss in iteration 214 : 5.580925368806956
Loss in iteration 215 : 6.506424623553054
Loss in iteration 216 : 5.476876850158984
Loss in iteration 217 : 6.514036568178617
Loss in iteration 218 : 5.4130545535460355
Loss in iteration 219 : 6.759333526843955
Loss in iteration 220 : 5.262479687298124
Loss in iteration 221 : 6.637009480362905
Loss in iteration 222 : 4.932037730114749
Loss in iteration 223 : 6.404743349114849
Loss in iteration 224 : 5.667573169969542
Loss in iteration 225 : 7.0112976169082835
Loss in iteration 226 : 6.028556698261566
Loss in iteration 227 : 6.932867604232137
Loss in iteration 228 : 5.8230049580231835
Loss in iteration 229 : 6.663895821697455
Loss in iteration 230 : 5.5839402822696
Loss in iteration 231 : 6.440144699313592
Loss in iteration 232 : 5.562943059865253
Loss in iteration 233 : 6.477528937362856
Loss in iteration 234 : 5.469212791928114
Loss in iteration 235 : 6.524725518161944
Loss in iteration 236 : 5.415829345854302
Loss in iteration 237 : 6.791356439437708
Loss in iteration 238 : 5.171630302565653
Loss in iteration 239 : 6.458864609967497
Loss in iteration 240 : 5.022983341707094
Loss in iteration 241 : 6.500709399043859
Loss in iteration 242 : 5.821212530330348
Loss in iteration 243 : 7.053767293041909
Loss in iteration 244 : 6.021644485707125
Loss in iteration 245 : 6.868745084389866
Loss in iteration 246 : 5.776771199146662
Loss in iteration 247 : 6.576583226886412
Loss in iteration 248 : 5.587524379661241
Loss in iteration 249 : 6.426129975557753
Loss in iteration 250 : 5.5244080241997215
Loss in iteration 251 : 6.455108507369543
Loss in iteration 252 : 5.434940812494193
Loss in iteration 253 : 6.626188232911878
Loss in iteration 254 : 5.363775862507146
Loss in iteration 255 : 6.716793681886978
Loss in iteration 256 : 4.956650294168462
Loss in iteration 257 : 6.344474890031084
Loss in iteration 258 : 5.474727412211816
Loss in iteration 259 : 6.8307876170290935
Loss in iteration 260 : 6.003333999708226
Loss in iteration 261 : 6.937312867896648
Loss in iteration 262 : 5.876336188882462
Loss in iteration 263 : 6.681839184746565
Loss in iteration 264 : 5.637939727356981
Loss in iteration 265 : 6.431046263646885
Loss in iteration 266 : 5.592691352547839
Loss in iteration 267 : 6.446048955553218
Loss in iteration 268 : 5.457572356519149
Loss in iteration 269 : 6.470370273292347
Loss in iteration 270 : 5.423593061789681
Loss in iteration 271 : 6.748932690470157
Loss in iteration 272 : 5.198060562214184
Loss in iteration 273 : 6.446712393099662
Loss in iteration 274 : 5.053697451057306
Loss in iteration 275 : 6.47408498413135
Loss in iteration 276 : 5.8079849140098485
Loss in iteration 277 : 6.990573239613461
Loss in iteration 278 : 6.0228922149249495
Loss in iteration 279 : 6.829266415183534
Loss in iteration 280 : 5.794190940547564
Loss in iteration 281 : 6.550330798557889
Loss in iteration 282 : 5.591723910649071
Loss in iteration 283 : 6.393039370994318
Loss in iteration 284 : 5.514443251023576
Loss in iteration 285 : 6.434158065069028
Loss in iteration 286 : 5.4269277963491875
Loss in iteration 287 : 6.63478722033353
Loss in iteration 288 : 5.322421119901601
Loss in iteration 289 : 6.60989601873189
Loss in iteration 290 : 4.984243551316917
Loss in iteration 291 : 6.3558171000359245
Loss in iteration 292 : 5.654846238083639
Loss in iteration 293 : 6.889870237852339
Loss in iteration 294 : 6.007675244528991
Loss in iteration 295 : 6.840701827883806
Loss in iteration 296 : 5.848883762237007
Loss in iteration 297 : 6.602083574856008
Loss in iteration 298 : 5.601809360936681
Loss in iteration 299 : 6.376087740192437
Loss in iteration 300 : 5.561249099550319
Loss in iteration 301 : 6.426942055911649
Loss in iteration 302 : 5.426088458882746
Loss in iteration 303 : 6.5389026741608145
Loss in iteration 304 : 5.421731532973987
Loss in iteration 305 : 6.726282718139186
Loss in iteration 306 : 4.967609866685156
Loss in iteration 307 : 6.289203666167141
Loss in iteration 308 : 5.461642183519931
Loss in iteration 309 : 6.76622963042763
Loss in iteration 310 : 5.980064398461092
Loss in iteration 311 : 6.893655315237425
Loss in iteration 312 : 5.9169886932381
Loss in iteration 313 : 6.661167838494135
Loss in iteration 314 : 5.6668786451819235
Loss in iteration 315 : 6.397578003878704
Loss in iteration 316 : 5.603104772383292
Loss in iteration 317 : 6.411671416803513
Loss in iteration 318 : 5.421978618622315
Loss in iteration 319 : 6.453446310621614
Loss in iteration 320 : 5.449860302763938
Loss in iteration 321 : 6.735144651014682
Loss in iteration 322 : 5.081360240600472
Loss in iteration 323 : 6.334517141006579
Loss in iteration 324 : 5.2731321142229515
Loss in iteration 325 : 6.604622780629127
Loss in iteration 326 : 5.8804322613530005
Loss in iteration 327 : 6.903778865114135
Loss in iteration 328 : 5.9721858670484025
Loss in iteration 329 : 6.716574620011271
Loss in iteration 330 : 5.756076499480851
Loss in iteration 331 : 6.457561679566211
Loss in iteration 332 : 5.6012275929553565
Loss in iteration 333 : 6.378792269862583
Loss in iteration 334 : 5.439634602342902
Loss in iteration 335 : 6.430179687927184
Loss in iteration 336 : 5.447449138748692
Loss in iteration 337 : 6.694937680800149
Loss in iteration 338 : 5.154210609486496
Loss in iteration 339 : 6.3749413865322655
Loss in iteration 340 : 5.218431806429329
Loss in iteration 341 : 6.5367120990852685
Loss in iteration 342 : 5.837234062860241
Loss in iteration 343 : 6.885950415739253
Loss in iteration 344 : 5.981155535063613
Loss in iteration 345 : 6.7198649604824645
Loss in iteration 346 : 5.79173591615992
Loss in iteration 347 : 6.483068135088573
Loss in iteration 348 : 5.579506431459751
Loss in iteration 349 : 6.350917127729315
Loss in iteration 350 : 5.459599974670285
Loss in iteration 351 : 6.436256941338985
Loss in iteration 352 : 5.456278137176568
Loss in iteration 353 : 6.6774054898651976
Loss in iteration 354 : 5.168446188304397
Loss in iteration 355 : 6.374704361898456
Loss in iteration 356 : 5.230074793167622
Loss in iteration 357 : 6.528778585275208
Loss in iteration 358 : 5.8283308494089905
Loss in iteration 359 : 6.8647346140961405
Loss in iteration 360 : 5.976013515149857
Loss in iteration 361 : 6.704065240928714
Loss in iteration 362 : 5.800521461390273
Loss in iteration 363 : 6.481890297401263
Loss in iteration 364 : 5.5712998188971445
Loss in iteration 365 : 6.340078509523032
Loss in iteration 366 : 5.4635493378261195
Loss in iteration 367 : 6.440941009133005
Loss in iteration 368 : 5.463182455801412
Loss in iteration 369 : 6.671031062774302
Loss in iteration 370 : 5.147553166017565
Loss in iteration 371 : 6.350402807956372
Loss in iteration 372 : 5.277765491633787
Loss in iteration 373 : 6.553824765166315
Loss in iteration 374 : 5.842298446545248
Loss in iteration 375 : 6.8464485740455405
Loss in iteration 376 : 5.967369181046534
Loss in iteration 377 : 6.679402861747382
Loss in iteration 378 : 5.792850978143302
Loss in iteration 379 : 6.463810378964555
Loss in iteration 380 : 5.574210421442758
Loss in iteration 381 : 6.341209654330562
Loss in iteration 382 : 5.458019512674898
Loss in iteration 383 : 6.443160748546072
Loss in iteration 384 : 5.4642801910973375
Loss in iteration 385 : 6.664168289034909
Loss in iteration 386 : 5.110640666285616
Loss in iteration 387 : 6.3177745077483705
Loss in iteration 388 : 5.345636950853945
Loss in iteration 389 : 6.595291514322444
Loss in iteration 390 : 5.873399518002232
Loss in iteration 391 : 6.831136707663591
Loss in iteration 392 : 5.956793263735923
Loss in iteration 393 : 6.6492435551747855
Loss in iteration 394 : 5.771975065165754
Loss in iteration 395 : 6.4349165539436965
Loss in iteration 396 : 5.582091425089839
Loss in iteration 397 : 6.349082421980544
Loss in iteration 398 : 5.450250007531197
Loss in iteration 399 : 6.445605925123975
Loss in iteration 400 : 5.460087627315939
Testing accuracy  of updater 5 on alg 0 with rate 4.0 = 0.7205, training accuracy 0.7205, time elapsed: 7644 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 1.1027141248839525
Loss in iteration 3 : 6.986285540540104
Loss in iteration 4 : 3.271337430850989
Loss in iteration 5 : 4.056637717659426
Loss in iteration 6 : 3.54986305064743
Loss in iteration 7 : 2.632903129075942
Loss in iteration 8 : 3.497223031444619
Loss in iteration 9 : 1.952993208307951
Loss in iteration 10 : 3.5481147622270695
Loss in iteration 11 : 1.425158146942024
Loss in iteration 12 : 3.0239056585629522
Loss in iteration 13 : 1.6431305015215645
Loss in iteration 14 : 2.8052129681638043
Loss in iteration 15 : 1.6894314677131044
Loss in iteration 16 : 2.1480093324282814
Loss in iteration 17 : 1.5727774680940683
Loss in iteration 18 : 2.153631105738516
Loss in iteration 19 : 2.014442797586883
Loss in iteration 20 : 2.441141287590144
Loss in iteration 21 : 1.6897004543384464
Loss in iteration 22 : 2.1791815653632383
Loss in iteration 23 : 1.7063493883915681
Loss in iteration 24 : 2.0235020899545804
Loss in iteration 25 : 1.6963955113215898
Loss in iteration 26 : 1.9058937364708621
Loss in iteration 27 : 1.6892750275965769
Loss in iteration 28 : 1.8148527342339957
Loss in iteration 29 : 1.6894972335717333
Loss in iteration 30 : 1.7418704753111296
Loss in iteration 31 : 1.6953181317840613
Loss in iteration 32 : 1.6860491880586812
Loss in iteration 33 : 1.702081688486565
Loss in iteration 34 : 1.6589608751300167
Loss in iteration 35 : 1.715796782266068
Loss in iteration 36 : 1.6510340242379733
Loss in iteration 37 : 1.7805181132550176
Loss in iteration 38 : 1.5267865833422725
Loss in iteration 39 : 1.6595684777689992
Loss in iteration 40 : 1.4414835554267995
Loss in iteration 41 : 1.7805630941025916
Loss in iteration 42 : 1.7007294136169107
Loss in iteration 43 : 1.8329478813964477
Loss in iteration 44 : 1.7613575955362202
Loss in iteration 45 : 1.7642000228245949
Loss in iteration 46 : 1.678988678404594
Loss in iteration 47 : 1.7218395334580765
Loss in iteration 48 : 1.6226335563768575
Loss in iteration 49 : 1.7057215360634517
Loss in iteration 50 : 1.5881502817710542
Loss in iteration 51 : 1.7033161018825762
Loss in iteration 52 : 1.566119780824223
Loss in iteration 53 : 1.7055106112701506
Loss in iteration 54 : 1.5579783261341211
Loss in iteration 55 : 1.71327964539014
Loss in iteration 56 : 1.5572875258887882
Loss in iteration 57 : 1.758447459506641
Loss in iteration 58 : 1.5307158209682807
Loss in iteration 59 : 1.7460909501411934
Loss in iteration 60 : 1.3382784574873314
Loss in iteration 61 : 1.6931724740618066
Loss in iteration 62 : 1.5545233736987756
Loss in iteration 63 : 1.834038861751701
Loss in iteration 64 : 1.6795607624009161
Loss in iteration 65 : 1.8072307940995613
Loss in iteration 66 : 1.6326712210239203
Loss in iteration 67 : 1.7452261576037857
Loss in iteration 68 : 1.5725594139500427
Loss in iteration 69 : 1.7110737328027905
Loss in iteration 70 : 1.540929466376519
Loss in iteration 71 : 1.7027537037996703
Loss in iteration 72 : 1.5226294801602458
Loss in iteration 73 : 1.7030569747778856
Loss in iteration 74 : 1.516523942869472
Loss in iteration 75 : 1.7100942683545823
Loss in iteration 76 : 1.518906708751963
Loss in iteration 77 : 1.7560724579148523
Loss in iteration 78 : 1.5045948788824721
Loss in iteration 79 : 1.7432972712438404
Loss in iteration 80 : 1.3202407400690899
Loss in iteration 81 : 1.6658111137818843
Loss in iteration 82 : 1.5301388563725855
Loss in iteration 83 : 1.8305168666541878
Loss in iteration 84 : 1.6539956273563274
Loss in iteration 85 : 1.8155556743397074
Loss in iteration 86 : 1.606248584622216
Loss in iteration 87 : 1.7445591039411605
Loss in iteration 88 : 1.5451706989249487
Loss in iteration 89 : 1.7023246845001485
Loss in iteration 90 : 1.5145748111600903
Loss in iteration 91 : 1.6920259626780412
Loss in iteration 92 : 1.4986139447857219
Loss in iteration 93 : 1.694448823117892
Loss in iteration 94 : 1.4958866381166718
Loss in iteration 95 : 1.724408112300891
Loss in iteration 96 : 1.4993644527691
Loss in iteration 97 : 1.7640577351174724
Loss in iteration 98 : 1.3390328847756192
Loss in iteration 99 : 1.6557472442856074
Loss in iteration 100 : 1.4819829364464416
Loss in iteration 101 : 1.789341009277862
Loss in iteration 102 : 1.6189165796979326
Loss in iteration 103 : 1.8106718810094422
Loss in iteration 104 : 1.6016462758047392
Loss in iteration 105 : 1.7474982553951204
Loss in iteration 106 : 1.5415507843091067
Loss in iteration 107 : 1.698664685529665
Loss in iteration 108 : 1.507853173566045
Loss in iteration 109 : 1.6855477533373355
Loss in iteration 110 : 1.4904352796311637
Loss in iteration 111 : 1.6917779705091525
Loss in iteration 112 : 1.4887151939868994
Loss in iteration 113 : 1.7459939389442811
Loss in iteration 114 : 1.4380466621668446
Loss in iteration 115 : 1.7021919279065532
Loss in iteration 116 : 1.3753412993615888
Loss in iteration 117 : 1.6895957492340707
Loss in iteration 118 : 1.5465910707501744
Loss in iteration 119 : 1.8030855602036504
Loss in iteration 120 : 1.6156432312606
Loss in iteration 121 : 1.7773906580845615
Loss in iteration 122 : 1.5670640341408704
Loss in iteration 123 : 1.7140240643699916
Loss in iteration 124 : 1.5186339410030132
Loss in iteration 125 : 1.6819315877923868
Loss in iteration 126 : 1.493006587027704
Loss in iteration 127 : 1.6788049567211756
Loss in iteration 128 : 1.4826071037727906
Loss in iteration 129 : 1.7075086697260327
Loss in iteration 130 : 1.478547947937962
Loss in iteration 131 : 1.7389745909128846
Loss in iteration 132 : 1.3576602228508763
Loss in iteration 133 : 1.6520318717673812
Loss in iteration 134 : 1.4839939820254002
Loss in iteration 135 : 1.7669116960893285
Loss in iteration 136 : 1.6004937040701483
Loss in iteration 137 : 1.7859941614140125
Loss in iteration 138 : 1.5839218588151773
Loss in iteration 139 : 1.7276556202661455
Loss in iteration 140 : 1.5299875114376837
Loss in iteration 141 : 1.6822593608622922
Loss in iteration 142 : 1.497627173888777
Loss in iteration 143 : 1.6717859828488348
Loss in iteration 144 : 1.4814100068449751
Loss in iteration 145 : 1.692115825186735
Loss in iteration 146 : 1.478297162859103
Loss in iteration 147 : 1.7369916754059331
Loss in iteration 148 : 1.3667717122989103
Loss in iteration 149 : 1.6476010696962429
Loss in iteration 150 : 1.4609004571094992
Loss in iteration 151 : 1.7436254522586336
Loss in iteration 152 : 1.588702339457935
Loss in iteration 153 : 1.7818064055844647
Loss in iteration 154 : 1.5880740809465792
Loss in iteration 155 : 1.728795140273207
Loss in iteration 156 : 1.534435904632757
Loss in iteration 157 : 1.6793418638684985
Loss in iteration 158 : 1.4988095988983283
Loss in iteration 159 : 1.6652870107225277
Loss in iteration 160 : 1.4804053702669249
Loss in iteration 161 : 1.6830023006782862
Loss in iteration 162 : 1.4766556048237354
Loss in iteration 163 : 1.730312109489262
Loss in iteration 164 : 1.37251101740496
Loss in iteration 165 : 1.6447796934436474
Loss in iteration 166 : 1.4542014242454233
Loss in iteration 167 : 1.7311833943071029
Loss in iteration 168 : 1.5825990957981746
Loss in iteration 169 : 1.7748966624011584
Loss in iteration 170 : 1.588044367364594
Loss in iteration 171 : 1.724918012969859
Loss in iteration 172 : 1.535269818131922
Loss in iteration 173 : 1.6747023378437234
Loss in iteration 174 : 1.4982314863071706
Loss in iteration 175 : 1.659694491046999
Loss in iteration 176 : 1.4791177087740328
Loss in iteration 177 : 1.6786147690874875
Loss in iteration 178 : 1.4737429391653905
Loss in iteration 179 : 1.721654500049573
Loss in iteration 180 : 1.3727481354842412
Loss in iteration 181 : 1.640407858934272
Loss in iteration 182 : 1.4614725953533318
Loss in iteration 183 : 1.7292068620406837
Loss in iteration 184 : 1.5825099500531246
Loss in iteration 185 : 1.766282374319559
Loss in iteration 186 : 1.5841379102376385
Loss in iteration 187 : 1.7162893136503174
Loss in iteration 188 : 1.5320550821306702
Loss in iteration 189 : 1.6682503060271756
Loss in iteration 190 : 1.4956754967960353
Loss in iteration 191 : 1.6554156880334827
Loss in iteration 192 : 1.4774100125802743
Loss in iteration 193 : 1.6804582069316494
Loss in iteration 194 : 1.4664890728830742
Loss in iteration 195 : 1.7077254044193793
Loss in iteration 196 : 1.3719328160904034
Loss in iteration 197 : 1.6378543067533473
Loss in iteration 198 : 1.483379522998424
Loss in iteration 199 : 1.736412082959805
Loss in iteration 200 : 1.5876186141227069
Loss in iteration 201 : 1.7551219071702229
Loss in iteration 202 : 1.5755457482746147
Loss in iteration 203 : 1.70244837842146
Loss in iteration 204 : 1.5242252640861214
Loss in iteration 205 : 1.659962587423974
Loss in iteration 206 : 1.4906885086450412
Loss in iteration 207 : 1.6531304670131532
Loss in iteration 208 : 1.47538492348419
Loss in iteration 209 : 1.689970620961451
Loss in iteration 210 : 1.4463416284522603
Loss in iteration 211 : 1.6829577001587708
Loss in iteration 212 : 1.3844939895304442
Loss in iteration 213 : 1.6487183095187679
Loss in iteration 214 : 1.5156135240456436
Loss in iteration 215 : 1.7457461043336975
Loss in iteration 216 : 1.5919298472650716
Loss in iteration 217 : 1.739636274972576
Loss in iteration 218 : 1.56219978768774
Loss in iteration 219 : 1.6851608279991093
Loss in iteration 220 : 1.5134808243218432
Loss in iteration 221 : 1.6516086717944605
Loss in iteration 222 : 1.4843099395505266
Loss in iteration 223 : 1.654307293042044
Loss in iteration 224 : 1.4734671285669287
Loss in iteration 225 : 1.700610429722108
Loss in iteration 226 : 1.412541535480845
Loss in iteration 227 : 1.6531363612136956
Loss in iteration 228 : 1.4187005024901296
Loss in iteration 229 : 1.6767737567456684
Loss in iteration 230 : 1.5469221350067606
Loss in iteration 231 : 1.747925732957263
Loss in iteration 232 : 1.587850625201514
Loss in iteration 233 : 1.7200166589864916
Loss in iteration 234 : 1.546007319609319
Loss in iteration 235 : 1.6687647645926391
Loss in iteration 236 : 1.5028099400217538
Loss in iteration 237 : 1.6464633691561021
Loss in iteration 238 : 1.4787817040989648
Loss in iteration 239 : 1.6632802831368532
Loss in iteration 240 : 1.466827902516667
Loss in iteration 241 : 1.6957724013652213
Loss in iteration 242 : 1.386668977841617
Loss in iteration 243 : 1.6342009252184453
Loss in iteration 244 : 1.469735826655764
Loss in iteration 245 : 1.7107105057294065
Loss in iteration 246 : 1.5729053779546425
Loss in iteration 247 : 1.739726960125324
Loss in iteration 248 : 1.5740742070296954
Loss in iteration 249 : 1.6958586096592472
Loss in iteration 250 : 1.526754079813908
Loss in iteration 251 : 1.654094247621842
Loss in iteration 252 : 1.4912566492949566
Loss in iteration 253 : 1.6472963170721218
Loss in iteration 254 : 1.4741336246755208
Loss in iteration 255 : 1.684133466341282
Loss in iteration 256 : 1.4310177606847196
Loss in iteration 257 : 1.6598649466727697
Loss in iteration 258 : 1.4090257727072788
Loss in iteration 259 : 1.6569114497400366
Loss in iteration 260 : 1.5279545336657188
Loss in iteration 261 : 1.7328803576669989
Loss in iteration 262 : 1.583426400622358
Loss in iteration 263 : 1.718214484090514
Loss in iteration 264 : 1.5510593252672225
Loss in iteration 265 : 1.6694081207146347
Loss in iteration 266 : 1.5067488687602257
Loss in iteration 267 : 1.6439889472413916
Loss in iteration 268 : 1.480305352664538
Loss in iteration 269 : 1.65879161405292
Loss in iteration 270 : 1.4644952808667848
Loss in iteration 271 : 1.6854442395197016
Loss in iteration 272 : 1.394404284812336
Loss in iteration 273 : 1.6333517410649612
Loss in iteration 274 : 1.4731390103640578
Loss in iteration 275 : 1.70326608904351
Loss in iteration 276 : 1.5685272653999718
Loss in iteration 277 : 1.7294086348906952
Loss in iteration 278 : 1.5696810674407091
Loss in iteration 279 : 1.6886618612182323
Loss in iteration 280 : 1.5248772757799152
Loss in iteration 281 : 1.650038781461591
Loss in iteration 282 : 1.4901895643690146
Loss in iteration 283 : 1.6465583996450883
Loss in iteration 284 : 1.4726714380993147
Loss in iteration 285 : 1.6815133639560975
Loss in iteration 286 : 1.4199783313509065
Loss in iteration 287 : 1.6459174498384306
Loss in iteration 288 : 1.4302660919459236
Loss in iteration 289 : 1.6671060866707308
Loss in iteration 290 : 1.5390256408813912
Loss in iteration 291 : 1.7259104272384447
Loss in iteration 292 : 1.5771807290105613
Loss in iteration 293 : 1.7040737375402146
Loss in iteration 294 : 1.5420329671260429
Loss in iteration 295 : 1.6598025118398434
Loss in iteration 296 : 1.5013592903598287
Loss in iteration 297 : 1.6419970939604513
Loss in iteration 298 : 1.4778112168522637
Loss in iteration 299 : 1.6659741979777332
Loss in iteration 300 : 1.448987789624499
Loss in iteration 301 : 1.6652565243341932
Loss in iteration 302 : 1.4064659429066777
Loss in iteration 303 : 1.6405766172248715
Loss in iteration 304 : 1.5048738872134344
Loss in iteration 305 : 1.7118321632188012
Loss in iteration 306 : 1.5739004058181387
Loss in iteration 307 : 1.714198365200304
Loss in iteration 308 : 1.5562101774606494
Loss in iteration 309 : 1.67123856210445
Loss in iteration 310 : 1.5129551482057146
Loss in iteration 311 : 1.6428338514022949
Loss in iteration 312 : 1.4836339007239385
Loss in iteration 313 : 1.6534128172422424
Loss in iteration 314 : 1.463658162142785
Loss in iteration 315 : 1.674184199674834
Loss in iteration 316 : 1.4058577937655727
Loss in iteration 317 : 1.633265839487315
Loss in iteration 318 : 1.4748077857084354
Loss in iteration 319 : 1.6925185868499388
Loss in iteration 320 : 1.561610433111675
Loss in iteration 321 : 1.7172435188874888
Loss in iteration 322 : 1.5648965259626693
Loss in iteration 323 : 1.681553027208315
Loss in iteration 324 : 1.5239074820699217
Loss in iteration 325 : 1.6467407039854152
Loss in iteration 326 : 1.4902068964032336
Loss in iteration 327 : 1.6463667052438882
Loss in iteration 328 : 1.4704414405186101
Loss in iteration 329 : 1.673505437206334
Loss in iteration 330 : 1.41718117909897
Loss in iteration 331 : 1.6371354086462042
Loss in iteration 332 : 1.4520748816172506
Loss in iteration 333 : 1.673586768571698
Loss in iteration 334 : 1.5458986596144408
Loss in iteration 335 : 1.7149573336583912
Loss in iteration 336 : 1.5688190256884693
Loss in iteration 337 : 1.6893615947268785
Loss in iteration 338 : 1.533348133526506
Loss in iteration 339 : 1.6514728327894719
Loss in iteration 340 : 1.4966936215360884
Loss in iteration 341 : 1.6425418733722066
Loss in iteration 342 : 1.474873648708917
Loss in iteration 343 : 1.6685137027005665
Loss in iteration 344 : 1.43100248906564
Loss in iteration 345 : 1.6442676634923186
Loss in iteration 346 : 1.43613445051143
Loss in iteration 347 : 1.6578404453441202
Loss in iteration 348 : 1.5300297860358156
Loss in iteration 349 : 1.7097296415147953
Loss in iteration 350 : 1.5695080131458647
Loss in iteration 351 : 1.6949345466623122
Loss in iteration 352 : 1.5411653608390883
Loss in iteration 353 : 1.6562192840872134
Loss in iteration 354 : 1.5027341682538595
Loss in iteration 355 : 1.6407397295034043
Loss in iteration 356 : 1.4784185226996724
Loss in iteration 357 : 1.6624292478657385
Loss in iteration 358 : 1.4427983021635786
Loss in iteration 359 : 1.6507269013411432
Loss in iteration 360 : 1.4267964532260669
Loss in iteration 361 : 1.6467600051701552
Loss in iteration 362 : 1.5157592176990282
Loss in iteration 363 : 1.7030227556018827
Loss in iteration 364 : 1.567717018698581
Loss in iteration 365 : 1.6983149168123721
Loss in iteration 366 : 1.5470741422806327
Loss in iteration 367 : 1.6604939370829401
Loss in iteration 368 : 1.5080517433330747
Loss in iteration 369 : 1.6403355369693637
Loss in iteration 370 : 1.4814945467611909
Loss in iteration 371 : 1.6571658893659689
Loss in iteration 372 : 1.4511069745917617
Loss in iteration 373 : 1.6548689265147223
Loss in iteration 374 : 1.4231010064546754
Loss in iteration 375 : 1.640365403427526
Loss in iteration 376 : 1.504323024773704
Loss in iteration 377 : 1.6961033398769656
Loss in iteration 378 : 1.564346439919495
Loss in iteration 379 : 1.699626285616454
Loss in iteration 380 : 1.5509304206786318
Loss in iteration 381 : 1.6638343638310165
Loss in iteration 382 : 1.5123723135724658
Loss in iteration 383 : 1.6407159843076506
Loss in iteration 384 : 1.4841480112170486
Loss in iteration 385 : 1.6533697240282015
Loss in iteration 386 : 1.4564327145679594
Loss in iteration 387 : 1.65673538811459
Loss in iteration 388 : 1.4230652197501308
Loss in iteration 389 : 1.6373440520403408
Loss in iteration 390 : 1.4962067192282602
Loss in iteration 391 : 1.689986804790007
Loss in iteration 392 : 1.560514047190025
Loss in iteration 393 : 1.6993531781788216
Loss in iteration 394 : 1.5529922121525241
Loss in iteration 395 : 1.6660060033114732
Loss in iteration 396 : 1.5155481899737309
Loss in iteration 397 : 1.64133579435001
Loss in iteration 398 : 1.4862868404102374
Loss in iteration 399 : 1.6509052159298234
Loss in iteration 400 : 1.459706260912223
Testing accuracy  of updater 5 on alg 0 with rate 1.0 = 0.7255, training accuracy 0.7255, time elapsed: 7706 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.9096425991567848
Loss in iteration 3 : 4.378923790931676
Loss in iteration 4 : 2.9388981074139746
Loss in iteration 5 : 2.336283301888413
Loss in iteration 6 : 2.8177608766493387
Loss in iteration 7 : 1.6202907887358844
Loss in iteration 8 : 2.680171899622685
Loss in iteration 9 : 1.2512127162644429
Loss in iteration 10 : 2.4344081847172996
Loss in iteration 11 : 1.1690144492558172
Loss in iteration 12 : 2.103690966096827
Loss in iteration 13 : 1.2579646330300545
Loss in iteration 14 : 1.874340336983993
Loss in iteration 15 : 1.196826210466154
Loss in iteration 16 : 1.6068694812279631
Loss in iteration 17 : 1.2141729005303956
Loss in iteration 18 : 1.5589056292869368
Loss in iteration 19 : 1.388426382332781
Loss in iteration 20 : 1.6933635494107235
Loss in iteration 21 : 1.244018175437485
Loss in iteration 22 : 1.5426038305341363
Loss in iteration 23 : 1.233877156824366
Loss in iteration 24 : 1.4491474883965556
Loss in iteration 25 : 1.2231036315943689
Loss in iteration 26 : 1.3762339217689552
Loss in iteration 27 : 1.2213151681054821
Loss in iteration 28 : 1.3170654203644572
Loss in iteration 29 : 1.225435797825592
Loss in iteration 30 : 1.2772863816625664
Loss in iteration 31 : 1.2410904237745413
Loss in iteration 32 : 1.244783812260319
Loss in iteration 33 : 1.256608696629066
Loss in iteration 34 : 1.1696654449360682
Loss in iteration 35 : 1.2305359977689427
Loss in iteration 36 : 1.2183857735735166
Loss in iteration 37 : 1.275480133240624
Loss in iteration 38 : 1.3018638258213007
Loss in iteration 39 : 1.2650153419507342
Loss in iteration 40 : 1.2897706137951328
Loss in iteration 41 : 1.2391492286609305
Loss in iteration 42 : 1.251111474156281
Loss in iteration 43 : 1.2258004355003045
Loss in iteration 44 : 1.2202040787917596
Loss in iteration 45 : 1.223058127861854
Loss in iteration 46 : 1.1987046015486764
Loss in iteration 47 : 1.2309568995305515
Loss in iteration 48 : 1.1857152886925264
Loss in iteration 49 : 1.256228542819409
Loss in iteration 50 : 1.1323212105746912
Loss in iteration 51 : 1.2292100628787552
Loss in iteration 52 : 1.1271070921605248
Loss in iteration 53 : 1.2547517313885224
Loss in iteration 54 : 1.2186162820790474
Loss in iteration 55 : 1.279246212506532
Loss in iteration 56 : 1.2408177085578627
Loss in iteration 57 : 1.257124664747045
Loss in iteration 58 : 1.2127363273130427
Loss in iteration 59 : 1.234621890138536
Loss in iteration 60 : 1.1839082918429726
Loss in iteration 61 : 1.225384947868234
Loss in iteration 62 : 1.1642200594701133
Loss in iteration 63 : 1.2297983327726565
Loss in iteration 64 : 1.1528591396487737
Loss in iteration 65 : 1.2543414968657
Loss in iteration 66 : 1.110018769648674
Loss in iteration 67 : 1.2286489044111433
Loss in iteration 68 : 1.099136220468749
Loss in iteration 69 : 1.245049670258422
Loss in iteration 70 : 1.1857233088226995
Loss in iteration 71 : 1.2798595177068528
Loss in iteration 72 : 1.2136154773659091
Loss in iteration 73 : 1.2614683551674553
Loss in iteration 74 : 1.1889382268112965
Loss in iteration 75 : 1.2355665604753594
Loss in iteration 76 : 1.1611900003423885
Loss in iteration 77 : 1.2234613168660913
Loss in iteration 78 : 1.1429196441588636
Loss in iteration 79 : 1.2275769362379747
Loss in iteration 80 : 1.1329806670983527
Loss in iteration 81 : 1.2499217520267174
Loss in iteration 82 : 1.087835560192216
Loss in iteration 83 : 1.2192728229619396
Loss in iteration 84 : 1.096089441588906
Loss in iteration 85 : 1.2436794155658384
Loss in iteration 86 : 1.1767565066891643
Loss in iteration 87 : 1.2770365785927653
Loss in iteration 88 : 1.1977654233180788
Loss in iteration 89 : 1.2574031630384161
Loss in iteration 90 : 1.1720286974458192
Loss in iteration 91 : 1.2300860942246745
Loss in iteration 92 : 1.1455647106917988
Loss in iteration 93 : 1.2184211119146475
Loss in iteration 94 : 1.1292825938420552
Loss in iteration 95 : 1.227242489641801
Loss in iteration 96 : 1.1183051638596324
Loss in iteration 97 : 1.2424779949597
Loss in iteration 98 : 1.0698863080336805
Loss in iteration 99 : 1.2127072094718674
Loss in iteration 100 : 1.1142232251151263
Loss in iteration 101 : 1.2530683660503372
Loss in iteration 102 : 1.1791586697870948
Loss in iteration 103 : 1.269219478637111
Loss in iteration 104 : 1.1821194604647707
Loss in iteration 105 : 1.2447236830315505
Loss in iteration 106 : 1.1549979390884428
Loss in iteration 107 : 1.2217161304033792
Loss in iteration 108 : 1.1323008866113866
Loss in iteration 109 : 1.2168523828939095
Loss in iteration 110 : 1.1193521746841424
Loss in iteration 111 : 1.2341112050767162
Loss in iteration 112 : 1.093946111355048
Loss in iteration 113 : 1.222761701720871
Loss in iteration 114 : 1.0759955369009573
Loss in iteration 115 : 1.2215261376818003
Loss in iteration 116 : 1.1438302957823472
Loss in iteration 117 : 1.2615400549002582
Loss in iteration 118 : 1.1792409032653102
Loss in iteration 119 : 1.2548564477623696
Loss in iteration 120 : 1.1633385666492322
Loss in iteration 121 : 1.2284400414449888
Loss in iteration 122 : 1.1375676316248524
Loss in iteration 123 : 1.2140351406692684
Loss in iteration 124 : 1.120075028466375
Loss in iteration 125 : 1.2213406310580535
Loss in iteration 126 : 1.1055867029809634
Loss in iteration 127 : 1.2300141995744853
Loss in iteration 128 : 1.0694910545919256
Loss in iteration 129 : 1.2088098805897682
Loss in iteration 130 : 1.1130345398614572
Loss in iteration 131 : 1.2444463527224938
Loss in iteration 132 : 1.1664619002858168
Loss in iteration 133 : 1.25678899866453
Loss in iteration 134 : 1.1672681827948592
Loss in iteration 135 : 1.2345792791330805
Loss in iteration 136 : 1.142877172673573
Loss in iteration 137 : 1.2145170661712013
Loss in iteration 138 : 1.1221724960264041
Loss in iteration 139 : 1.213821147217324
Loss in iteration 140 : 1.1084676785535876
Loss in iteration 141 : 1.227483769157739
Loss in iteration 142 : 1.076379454750223
Loss in iteration 143 : 1.2081602177843616
Loss in iteration 144 : 1.093315761315879
Loss in iteration 145 : 1.2281590812387604
Loss in iteration 146 : 1.1511440026080522
Loss in iteration 147 : 1.252617195088552
Loss in iteration 148 : 1.166716880664831
Loss in iteration 149 : 1.2379176691500355
Loss in iteration 150 : 1.1467770865953435
Loss in iteration 151 : 1.2156832100425181
Loss in iteration 152 : 1.1244648856291886
Loss in iteration 153 : 1.2094743677158388
Loss in iteration 154 : 1.1095574854918364
Loss in iteration 155 : 1.2219072737270185
Loss in iteration 156 : 1.0836064019560292
Loss in iteration 157 : 1.2097009094899216
Loss in iteration 158 : 1.0827235889439093
Loss in iteration 159 : 1.2165237737101928
Loss in iteration 160 : 1.1378355312484956
Loss in iteration 161 : 1.2460976972752777
Loss in iteration 162 : 1.1636456531397517
Loss in iteration 163 : 1.2387359954491584
Loss in iteration 164 : 1.1490463087483818
Loss in iteration 165 : 1.216551158144483
Loss in iteration 166 : 1.126346550433938
Loss in iteration 167 : 1.2068318605031467
Loss in iteration 168 : 1.1101795365415064
Loss in iteration 169 : 1.216605537281132
Loss in iteration 170 : 1.0881868090493736
Loss in iteration 171 : 1.210144476707851
Loss in iteration 172 : 1.0783591034489568
Loss in iteration 173 : 1.209407451645765
Loss in iteration 174 : 1.1280419132542359
Loss in iteration 175 : 1.2393980984820874
Loss in iteration 176 : 1.159480191063156
Loss in iteration 177 : 1.237583130172403
Loss in iteration 178 : 1.1496650004754303
Loss in iteration 179 : 1.2166665405528752
Loss in iteration 180 : 1.1274601986011015
Loss in iteration 181 : 1.2050917271092945
Loss in iteration 182 : 1.1104649543464116
Loss in iteration 183 : 1.2124815971083358
Loss in iteration 184 : 1.0904698882009007
Loss in iteration 185 : 1.2091536234834122
Loss in iteration 186 : 1.0776438041643566
Loss in iteration 187 : 1.2054412676738502
Loss in iteration 188 : 1.1219139529787499
Loss in iteration 189 : 1.233627389033392
Loss in iteration 190 : 1.155270192672595
Loss in iteration 191 : 1.2350971398524075
Loss in iteration 192 : 1.148869133426783
Loss in iteration 193 : 1.2158769867913393
Loss in iteration 194 : 1.127647361291343
Loss in iteration 195 : 1.203764627001422
Loss in iteration 196 : 1.110336067861415
Loss in iteration 197 : 1.209494996836055
Loss in iteration 198 : 1.091207973417858
Loss in iteration 199 : 1.2071998140247306
Loss in iteration 200 : 1.0788604222079552
Loss in iteration 201 : 1.203389607594774
Loss in iteration 202 : 1.1189992048796882
Loss in iteration 203 : 1.2291663442920615
Loss in iteration 204 : 1.151633005557267
Loss in iteration 205 : 1.2318514000718463
Loss in iteration 206 : 1.1470191000000514
Loss in iteration 207 : 1.214257798624981
Loss in iteration 208 : 1.126916582980091
Loss in iteration 209 : 1.2025979997364082
Loss in iteration 210 : 1.109717437732265
Loss in iteration 211 : 1.2073483662014979
Loss in iteration 212 : 1.0909374517820702
Loss in iteration 213 : 1.2047572745021193
Loss in iteration 214 : 1.0812167796552186
Loss in iteration 215 : 1.2025515290706266
Loss in iteration 216 : 1.1186247756131136
Loss in iteration 217 : 1.2259191246913674
Loss in iteration 218 : 1.1487267089892752
Loss in iteration 219 : 1.2282125558714174
Loss in iteration 220 : 1.144423306264998
Loss in iteration 221 : 1.2120030910636452
Loss in iteration 222 : 1.12538077350616
Loss in iteration 223 : 1.2015213626239691
Loss in iteration 224 : 1.1085852949447594
Loss in iteration 225 : 1.2057156155579962
Loss in iteration 226 : 1.0900655295991402
Loss in iteration 227 : 1.202176433727065
Loss in iteration 228 : 1.0845132563536348
Loss in iteration 229 : 1.2026226708656782
Loss in iteration 230 : 1.1200750873952723
Loss in iteration 231 : 1.223547674651621
Loss in iteration 232 : 1.1463804760589638
Loss in iteration 233 : 1.224357075216456
Loss in iteration 234 : 1.141286298941005
Loss in iteration 235 : 1.209354482298261
Loss in iteration 236 : 1.1232043173345705
Loss in iteration 237 : 1.2005732508951188
Loss in iteration 238 : 1.1069624392857336
Loss in iteration 239 : 1.2042885901190825
Loss in iteration 240 : 1.0890259379297131
Loss in iteration 241 : 1.1997678798919504
Loss in iteration 242 : 1.0887747794818732
Loss in iteration 243 : 1.2034432397501391
Loss in iteration 244 : 1.1226496840913236
Loss in iteration 245 : 1.2216341094818624
Loss in iteration 246 : 1.144266972457816
Loss in iteration 247 : 1.22035892433632
Loss in iteration 248 : 1.1377358022066495
Loss in iteration 249 : 1.2065659411876941
Loss in iteration 250 : 1.1205641557148553
Loss in iteration 251 : 1.1998207916984576
Loss in iteration 252 : 1.1049070823374814
Loss in iteration 253 : 1.2028000356534645
Loss in iteration 254 : 1.088356578877027
Loss in iteration 255 : 1.197871556839575
Loss in iteration 256 : 1.0939880677186382
Loss in iteration 257 : 1.2048125663857072
Loss in iteration 258 : 1.1256782231244191
Loss in iteration 259 : 1.2197817725607947
Loss in iteration 260 : 1.142048670256826
Loss in iteration 261 : 1.2162756265167014
Loss in iteration 262 : 1.1338710215039265
Loss in iteration 263 : 1.2038823592353487
Loss in iteration 264 : 1.1176236806192026
Loss in iteration 265 : 1.1992815307055502
Loss in iteration 266 : 1.102533544121963
Loss in iteration 267 : 1.2010877148618175
Loss in iteration 268 : 1.088655432241987
Loss in iteration 269 : 1.196827036126246
Loss in iteration 270 : 1.0999592986439575
Loss in iteration 271 : 1.2064150769584172
Loss in iteration 272 : 1.1285394989091302
Loss in iteration 273 : 1.2176830496759672
Loss in iteration 274 : 1.1394765233716264
Loss in iteration 275 : 1.212204452004088
Loss in iteration 276 : 1.129802026275547
Loss in iteration 277 : 1.201516782349972
Loss in iteration 278 : 1.1145224029527594
Loss in iteration 279 : 1.1988685542152036
Loss in iteration 280 : 1.1000828893946295
Loss in iteration 281 : 1.1991894589830527
Loss in iteration 282 : 1.0904107898887414
Loss in iteration 283 : 1.196843254133808
Loss in iteration 284 : 1.1062810313136493
Loss in iteration 285 : 1.2078517026660915
Loss in iteration 286 : 1.1307057168796286
Loss in iteration 287 : 1.2151686118841507
Loss in iteration 288 : 1.1364374655948706
Loss in iteration 289 : 1.208300279343825
Loss in iteration 290 : 1.1256643834225515
Loss in iteration 291 : 1.1996176315518046
Loss in iteration 292 : 1.1113846655646311
Loss in iteration 293 : 1.1984002688338058
Loss in iteration 294 : 1.0979954120860653
Loss in iteration 295 : 1.1973865392630478
Loss in iteration 296 : 1.0937880459388758
Loss in iteration 297 : 1.1978515101184206
Loss in iteration 298 : 1.1123925272407336
Loss in iteration 299 : 1.2087381040491105
Loss in iteration 300 : 1.1318104189258924
Loss in iteration 301 : 1.2122333182940428
Loss in iteration 302 : 1.1329574662259447
Loss in iteration 303 : 1.2047540499889278
Loss in iteration 304 : 1.1216050272102585
Loss in iteration 305 : 1.198227132558739
Loss in iteration 306 : 1.1083508487941938
Loss in iteration 307 : 1.197703615719161
Loss in iteration 308 : 1.0968633587122998
Loss in iteration 309 : 1.196101111893972
Loss in iteration 310 : 1.0985239582475523
Loss in iteration 311 : 1.1994748952626084
Loss in iteration 312 : 1.1177062759466818
Loss in iteration 313 : 1.2088118397764123
Loss in iteration 314 : 1.1317055767049278
Loss in iteration 315 : 1.2090222769783072
Loss in iteration 316 : 1.129172172998318
Loss in iteration 317 : 1.2017408623583392
Loss in iteration 318 : 1.117757780049671
Loss in iteration 319 : 1.1972554577106607
Loss in iteration 320 : 1.1056329682815085
Loss in iteration 321 : 1.1967620675932027
Loss in iteration 322 : 1.0972026206368077
Loss in iteration 323 : 1.1956625858528451
Loss in iteration 324 : 1.1040090534392224
Loss in iteration 325 : 1.2011667936555646
Loss in iteration 326 : 1.1217506173192846
Loss in iteration 327 : 1.207998842294761
Loss in iteration 328 : 1.1304636926225644
Loss in iteration 329 : 1.2057749270225773
Loss in iteration 330 : 1.1252793134239565
Loss in iteration 331 : 1.1993612534444538
Loss in iteration 332 : 1.1142401620166624
Loss in iteration 333 : 1.1965151081703744
Loss in iteration 334 : 1.1035596693511456
Loss in iteration 335 : 1.1957823235591585
Loss in iteration 336 : 1.0991762102295828
Loss in iteration 337 : 1.1960968708002713
Loss in iteration 338 : 1.1094896276630108
Loss in iteration 339 : 1.202422414904681
Loss in iteration 340 : 1.1242740762648387
Loss in iteration 341 : 1.2064185179197133
Loss in iteration 342 : 1.1283216277399508
Loss in iteration 343 : 1.2027485527242374
Loss in iteration 344 : 1.1214887294820648
Loss in iteration 345 : 1.1976033363808993
Loss in iteration 346 : 1.1111814094438823
Loss in iteration 347 : 1.1958305792847181
Loss in iteration 348 : 1.102520978211459
Loss in iteration 349 : 1.1950898449432148
Loss in iteration 350 : 1.1024864147187543
Loss in iteration 351 : 1.1970988896439339
Loss in iteration 352 : 1.1142768985791545
Loss in iteration 353 : 1.2029332912144408
Loss in iteration 354 : 1.125279673160738
Loss in iteration 355 : 1.2043314014412567
Loss in iteration 356 : 1.1256001592945313
Loss in iteration 357 : 1.2001455858888779
Loss in iteration 358 : 1.1179876617436584
Loss in iteration 359 : 1.196348199420463
Loss in iteration 360 : 1.1087559537342873
Loss in iteration 361 : 1.1951557573099165
Loss in iteration 362 : 1.1027834501141114
Loss in iteration 363 : 1.1949151705022847
Loss in iteration 364 : 1.106505345952347
Loss in iteration 365 : 1.1981995261929865
Loss in iteration 366 : 1.117898526399793
Loss in iteration 367 : 1.202634520792214
Loss in iteration 368 : 1.1249850066678746
Loss in iteration 369 : 1.2020516483156276
Loss in iteration 370 : 1.1226302220857862
Loss in iteration 371 : 1.1980644269139684
Loss in iteration 372 : 1.1149342908118252
Loss in iteration 373 : 1.1954289427935418
Loss in iteration 374 : 1.1071752456940132
Loss in iteration 375 : 1.1945922919587229
Loss in iteration 376 : 1.1043097518724945
Loss in iteration 377 : 1.1952446759548114
Loss in iteration 378 : 1.1105228943320926
Loss in iteration 379 : 1.1989856466100288
Loss in iteration 380 : 1.120163755911427
Loss in iteration 381 : 1.201663099554244
Loss in iteration 382 : 1.123734649906354
Loss in iteration 383 : 1.1998597479562492
Loss in iteration 384 : 1.1197025940003535
Loss in iteration 385 : 1.1964908857227232
Loss in iteration 386 : 1.1124712339876273
Loss in iteration 387 : 1.1947170391609083
Loss in iteration 388 : 1.106606211902939
Loss in iteration 389 : 1.1942845839835268
Loss in iteration 390 : 1.1067421953594971
Loss in iteration 391 : 1.195845457509055
Loss in iteration 392 : 1.113960063484699
Loss in iteration 393 : 1.199233677443764
Loss in iteration 394 : 1.1211403061367329
Loss in iteration 395 : 1.200267952060676
Loss in iteration 396 : 1.1219075467142998
Loss in iteration 397 : 1.1979445843465997
Loss in iteration 398 : 1.1170473635507705
Loss in iteration 399 : 1.1953339927117428
Loss in iteration 400 : 1.110728869290158
Testing accuracy  of updater 5 on alg 0 with rate 0.7 = 0.7245, training accuracy 0.7245, time elapsed: 7777 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.7460058356895666
Loss in iteration 3 : 1.7725457582208424
Loss in iteration 4 : 2.3119784451594834
Loss in iteration 5 : 0.9828719667259008
Loss in iteration 6 : 1.6017357759920394
Loss in iteration 7 : 1.1337415949199814
Loss in iteration 8 : 1.5636227021873503
Loss in iteration 9 : 0.8907851303198127
Loss in iteration 10 : 1.2499490729477112
Loss in iteration 11 : 0.9270764943569091
Loss in iteration 12 : 1.1652937117908704
Loss in iteration 13 : 0.8864322372861676
Loss in iteration 14 : 1.0246520425689274
Loss in iteration 15 : 0.8858131534094844
Loss in iteration 16 : 0.9129911554597276
Loss in iteration 17 : 0.8334104440850055
Loss in iteration 18 : 0.9406971929521168
Loss in iteration 19 : 0.8701296464703776
Loss in iteration 20 : 0.9968020776260634
Loss in iteration 21 : 0.8378970893960562
Loss in iteration 22 : 0.942491256628688
Loss in iteration 23 : 0.818140426635669
Loss in iteration 24 : 0.9046778691059869
Loss in iteration 25 : 0.8088779628322271
Loss in iteration 26 : 0.880606362063808
Loss in iteration 27 : 0.8079536167180225
Loss in iteration 28 : 0.8618020893244178
Loss in iteration 29 : 0.8115855845247766
Loss in iteration 30 : 0.8486679084188826
Loss in iteration 31 : 0.8125956197726473
Loss in iteration 32 : 0.8289850059961621
Loss in iteration 33 : 0.8054408967925447
Loss in iteration 34 : 0.8480926385437383
Loss in iteration 35 : 0.8189425926957149
Loss in iteration 36 : 0.8680616891252262
Loss in iteration 37 : 0.8170985563470465
Loss in iteration 38 : 0.858454290564149
Loss in iteration 39 : 0.807502729351327
Loss in iteration 40 : 0.8416370861251926
Loss in iteration 41 : 0.801219467015051
Loss in iteration 42 : 0.828984976500482
Loss in iteration 43 : 0.8005511481063448
Loss in iteration 44 : 0.8215497039113016
Loss in iteration 45 : 0.8046333623808191
Loss in iteration 46 : 0.8128496712261272
Loss in iteration 47 : 0.8015775940049163
Loss in iteration 48 : 0.8056844607167607
Loss in iteration 49 : 0.8023290498632114
Loss in iteration 50 : 0.8204346533888918
Loss in iteration 51 : 0.8093528351868561
Loss in iteration 52 : 0.8277422754998182
Loss in iteration 53 : 0.8071371641395881
Loss in iteration 54 : 0.8211969617106872
Loss in iteration 55 : 0.8008744137569398
Loss in iteration 56 : 0.8111624746112412
Loss in iteration 57 : 0.7974167545902529
Loss in iteration 58 : 0.8034648114361347
Loss in iteration 59 : 0.7982887920811909
Loss in iteration 60 : 0.7967605189880196
Loss in iteration 61 : 0.7976411251129459
Loss in iteration 62 : 0.7913767574391661
Loss in iteration 63 : 0.7969096074561346
Loss in iteration 64 : 0.7984992260514842
Loss in iteration 65 : 0.8016970245338537
Loss in iteration 66 : 0.8062420057487526
Loss in iteration 67 : 0.802473547402472
Loss in iteration 68 : 0.8047545105952123
Loss in iteration 69 : 0.7984633924236969
Loss in iteration 70 : 0.7980339306962726
Loss in iteration 71 : 0.7949152260410222
Loss in iteration 72 : 0.791461218089435
Loss in iteration 73 : 0.7941846580684883
Loss in iteration 74 : 0.7859968468358501
Loss in iteration 75 : 0.7935724794888998
Loss in iteration 76 : 0.7831543993849511
Loss in iteration 77 : 0.7936194311281362
Loss in iteration 78 : 0.7877539623180587
Loss in iteration 79 : 0.7966817752485804
Loss in iteration 80 : 0.7932411954227814
Loss in iteration 81 : 0.7975576082750633
Loss in iteration 82 : 0.7929216168744287
Loss in iteration 83 : 0.7950036552127504
Loss in iteration 84 : 0.7883710431643272
Loss in iteration 85 : 0.7923162043391598
Loss in iteration 86 : 0.7833088028778084
Loss in iteration 87 : 0.7913109949579736
Loss in iteration 88 : 0.7793042097922916
Loss in iteration 89 : 0.7908343599982125
Loss in iteration 90 : 0.7781586710334414
Loss in iteration 91 : 0.7913197288735039
Loss in iteration 92 : 0.7813879153866786
Loss in iteration 93 : 0.7931559504051624
Loss in iteration 94 : 0.7846917397519169
Loss in iteration 95 : 0.7934813971289302
Loss in iteration 96 : 0.7842848692904586
Loss in iteration 97 : 0.791741018090672
Loss in iteration 98 : 0.7810537327765013
Loss in iteration 99 : 0.7898857905840215
Loss in iteration 100 : 0.7773702584029436
Loss in iteration 101 : 0.7889765454259738
Loss in iteration 102 : 0.7747253685711353
Loss in iteration 103 : 0.7886520297897033
Loss in iteration 104 : 0.7745028719092364
Loss in iteration 105 : 0.7891823255602943
Loss in iteration 106 : 0.7766982122918965
Loss in iteration 107 : 0.7901838126094974
Loss in iteration 108 : 0.7784773942310731
Loss in iteration 109 : 0.7900991925647343
Loss in iteration 110 : 0.7778635711334557
Loss in iteration 111 : 0.7888407214291268
Loss in iteration 112 : 0.7755015208670488
Loss in iteration 113 : 0.7875521638951041
Loss in iteration 114 : 0.7729310262105458
Loss in iteration 115 : 0.7868354394942004
Loss in iteration 116 : 0.7713473812874918
Loss in iteration 117 : 0.7866586515529148
Loss in iteration 118 : 0.771536580415682
Loss in iteration 119 : 0.7870687084667838
Loss in iteration 120 : 0.7729411037170275
Loss in iteration 121 : 0.7875275344844673
Loss in iteration 122 : 0.7737660832828028
Loss in iteration 123 : 0.7872317483770794
Loss in iteration 124 : 0.7730509438435477
Loss in iteration 125 : 0.7862955521035203
Loss in iteration 126 : 0.771316357915819
Loss in iteration 127 : 0.7853912892857354
Loss in iteration 128 : 0.7695956714089025
Loss in iteration 129 : 0.7848740052920304
Loss in iteration 130 : 0.7687254947104992
Loss in iteration 131 : 0.7847862517865697
Loss in iteration 132 : 0.7690189207246441
Loss in iteration 133 : 0.7850285872536142
Loss in iteration 134 : 0.769838501274784
Loss in iteration 135 : 0.7851539355606291
Loss in iteration 136 : 0.7701034764748211
Loss in iteration 137 : 0.7847828862977023
Loss in iteration 138 : 0.7693965591895519
Loss in iteration 139 : 0.7840788679920627
Loss in iteration 140 : 0.768140766802261
Loss in iteration 141 : 0.7834404074756419
Loss in iteration 142 : 0.76703242475654
Loss in iteration 143 : 0.7830813327983137
Loss in iteration 144 : 0.766584972992512
Loss in iteration 145 : 0.7830226032281127
Loss in iteration 146 : 0.7668339790690416
Loss in iteration 147 : 0.7831140761782667
Loss in iteration 148 : 0.7672480334703577
Loss in iteration 149 : 0.7830523333570972
Loss in iteration 150 : 0.7672106531528332
Loss in iteration 151 : 0.782681273985668
Loss in iteration 152 : 0.7665808657300078
Loss in iteration 153 : 0.7821494308105684
Loss in iteration 154 : 0.7656913633411049
Loss in iteration 155 : 0.7816953266878663
Loss in iteration 156 : 0.7649974296073225
Loss in iteration 157 : 0.7814424376438847
Loss in iteration 158 : 0.7647720275741529
Loss in iteration 159 : 0.7813769055579771
Loss in iteration 160 : 0.7649272008337034
Loss in iteration 161 : 0.7813580410550138
Loss in iteration 162 : 0.7650800702130036
Loss in iteration 163 : 0.7812043467100581
Loss in iteration 164 : 0.7649004742918494
Loss in iteration 165 : 0.7808680149196661
Loss in iteration 166 : 0.7643775284151133
Loss in iteration 167 : 0.7804644615085518
Loss in iteration 168 : 0.7637601945752462
Loss in iteration 169 : 0.7801360240247052
Loss in iteration 170 : 0.7633299872377672
Loss in iteration 171 : 0.779946258987522
Loss in iteration 172 : 0.7632060879143919
Loss in iteration 173 : 0.7798611142146288
Loss in iteration 174 : 0.7632687448812387
Loss in iteration 175 : 0.779773142912751
Loss in iteration 176 : 0.763267147163448
Loss in iteration 177 : 0.7795846125533425
Loss in iteration 178 : 0.7630389559238739
Loss in iteration 179 : 0.7792946734229735
Loss in iteration 180 : 0.762625405741069
Loss in iteration 181 : 0.7789858319986136
Loss in iteration 182 : 0.7622020451121809
Loss in iteration 183 : 0.7787407503192945
Loss in iteration 184 : 0.7619303449943163
Loss in iteration 185 : 0.7785855078616619
Loss in iteration 186 : 0.7618442761657498
Loss in iteration 187 : 0.7784818109262175
Loss in iteration 188 : 0.7618358211612206
Loss in iteration 189 : 0.7783574894433499
Loss in iteration 190 : 0.7617529927184866
Loss in iteration 191 : 0.7781656523762213
Loss in iteration 192 : 0.761526304468642
Loss in iteration 193 : 0.7779219772860753
Loss in iteration 194 : 0.7612095183324155
Loss in iteration 195 : 0.7776819305626896
Loss in iteration 196 : 0.7609185605585447
Loss in iteration 197 : 0.777490881783453
Loss in iteration 198 : 0.7607375113415181
Loss in iteration 199 : 0.7773541872359783
Loss in iteration 200 : 0.7606606165617982
Loss in iteration 201 : 0.77723876039469
Loss in iteration 202 : 0.7606061516154862
Loss in iteration 203 : 0.7771007119454897
Loss in iteration 204 : 0.760488792637455
Loss in iteration 205 : 0.77692111213474
Loss in iteration 206 : 0.7602865918714035
Loss in iteration 207 : 0.7767183036504514
Loss in iteration 208 : 0.7600478518948199
Loss in iteration 209 : 0.7765274534635574
Loss in iteration 210 : 0.7598439113604495
Loss in iteration 211 : 0.7763712819360551
Loss in iteration 212 : 0.7597129611462752
Loss in iteration 213 : 0.7762457188079687
Loss in iteration 214 : 0.759635612979242
Loss in iteration 215 : 0.7761262083214876
Loss in iteration 216 : 0.7595566035800546
Loss in iteration 217 : 0.7759883979953538
Loss in iteration 218 : 0.7594320626129551
Loss in iteration 219 : 0.7758272124193443
Loss in iteration 220 : 0.7592615981024455
Loss in iteration 221 : 0.7756582036806214
Loss in iteration 222 : 0.759081920304518
Loss in iteration 223 : 0.7755022888565091
Loss in iteration 224 : 0.7589336659882265
Loss in iteration 225 : 0.7753692566274769
Loss in iteration 226 : 0.7588303810932978
Loss in iteration 227 : 0.7752522131025739
Loss in iteration 228 : 0.7587518293638171
Loss in iteration 229 : 0.775134818203443
Loss in iteration 230 : 0.7586638303570862
Loss in iteration 231 : 0.7750048579388826
Loss in iteration 232 : 0.7585463563195929
Loss in iteration 233 : 0.7748631729223512
Loss in iteration 234 : 0.7584067135771732
Loss in iteration 235 : 0.7747211011695334
Loss in iteration 236 : 0.75826985364819
Loss in iteration 237 : 0.7745901927995233
Loss in iteration 238 : 0.758156935815716
Loss in iteration 239 : 0.7744735138675986
Loss in iteration 240 : 0.7580698805356312
Loss in iteration 241 : 0.7743645609079947
Loss in iteration 242 : 0.7579925218919706
Loss in iteration 243 : 0.7742534053302121
Loss in iteration 244 : 0.7579054010225217
Loss in iteration 245 : 0.7741347026373147
Loss in iteration 246 : 0.7578010528788379
Loss in iteration 247 : 0.7740111455045899
Loss in iteration 248 : 0.7576878659187094
Loss in iteration 249 : 0.7738901867918312
Loss in iteration 250 : 0.7575813197464525
Loss in iteration 251 : 0.7737776405163618
Loss in iteration 252 : 0.7574912136324979
Loss in iteration 253 : 0.7736735380020621
Loss in iteration 254 : 0.7574150383997821
Loss in iteration 255 : 0.7735729124991196
Loss in iteration 256 : 0.7573415999076444
Loss in iteration 257 : 0.773470228341639
Loss in iteration 258 : 0.757260755282424
Loss in iteration 259 : 0.7733636596675383
Loss in iteration 260 : 0.7571709414411859
Loss in iteration 261 : 0.7732559258809848
Loss in iteration 262 : 0.7570790409380032
Loss in iteration 263 : 0.7731515149458478
Loss in iteration 264 : 0.7569938695131991
Loss in iteration 265 : 0.7730529964596898
Loss in iteration 266 : 0.7569191878393183
Loss in iteration 267 : 0.7729593497383409
Loss in iteration 268 : 0.7568515643520454
Loss in iteration 269 : 0.7728672356269296
Loss in iteration 270 : 0.7567840045073474
Loss in iteration 271 : 0.7727738406547557
Loss in iteration 272 : 0.7567117637957986
Loss in iteration 273 : 0.7726789208223583
Loss in iteration 274 : 0.7566355831984816
Loss in iteration 275 : 0.7725845831081709
Loss in iteration 276 : 0.7565602669016362
Loss in iteration 277 : 0.7724933078365704
Loss in iteration 278 : 0.7564904005832299
Loss in iteration 279 : 0.7724059947938231
Loss in iteration 280 : 0.7564268847360213
Loss in iteration 281 : 0.7723215009933212
Loss in iteration 282 : 0.7563667778802576
Loss in iteration 283 : 0.7722377888733714
Loss in iteration 284 : 0.7563060793760189
Loss in iteration 285 : 0.7721535848728338
Loss in iteration 286 : 0.7562429005990357
Loss in iteration 287 : 0.7720692132287947
Loss in iteration 288 : 0.7561785500080803
Loss in iteration 289 : 0.7719860875559224
Loss in iteration 290 : 0.7561160269232456
Loss in iteration 291 : 0.7719054532204528
Loss in iteration 292 : 0.7560574671283956
Loss in iteration 293 : 0.771827452192234
Loss in iteration 294 : 0.7560026339797862
Loss in iteration 295 : 0.771751165408278
Loss in iteration 296 : 0.7559494375237585
Loss in iteration 297 : 0.7716754515468189
Loss in iteration 298 : 0.7558957811793335
Loss in iteration 299 : 0.7715998286139263
Loss in iteration 300 : 0.7558411310239153
Loss in iteration 301 : 0.7715247213428409
Loss in iteration 302 : 0.755786657438719
Loss in iteration 303 : 0.771450981955082
Loss in iteration 304 : 0.7557340634716897
Loss in iteration 305 : 0.7713791615287205
Loss in iteration 306 : 0.7556841924593464
Loss in iteration 307 : 0.7713091230888147
Loss in iteration 308 : 0.7556365032101067
Loss in iteration 309 : 0.7712402353544804
Loss in iteration 310 : 0.7555896761370104
Loss in iteration 311 : 0.771171911237504
Loss in iteration 312 : 0.7555427160478794
Loss in iteration 313 : 0.7711040303272529
Loss in iteration 314 : 0.7554956374321389
Loss in iteration 315 : 0.7710369421179182
Loss in iteration 316 : 0.7554492773997028
Loss in iteration 317 : 0.7709711125771634
Loss in iteration 318 : 0.7554045152792368
Loss in iteration 319 : 0.7709067382852061
Loss in iteration 320 : 0.7553615834350583
Loss in iteration 321 : 0.7708436253940889
Loss in iteration 322 : 0.7553199768298549
Loss in iteration 323 : 0.7707813833095609
Loss in iteration 324 : 0.7552789358854436
Loss in iteration 325 : 0.7707197373580589
Loss in iteration 326 : 0.7552380465031101
Loss in iteration 327 : 0.7706587048800905
Loss in iteration 328 : 0.7551974837162407
Loss in iteration 329 : 0.7705985224879226
Loss in iteration 330 : 0.7551577707395375
Loss in iteration 331 : 0.7705394189531654
Loss in iteration 332 : 0.7551193130552721
Loss in iteration 333 : 0.7704814303634909
Loss in iteration 334 : 0.7550820961798331
Loss in iteration 335 : 0.7704243907305461
Loss in iteration 336 : 0.7550457499207506
Loss in iteration 337 : 0.7703680772865273
Loss in iteration 338 : 0.7550098721085106
Loss in iteration 339 : 0.7703123759499294
Loss in iteration 340 : 0.7549743251116929
Loss in iteration 341 : 0.7702573383706799
Loss in iteration 342 : 0.7549392847899542
Loss in iteration 343 : 0.7702031045642153
Loss in iteration 344 : 0.7549050445337416
Loss in iteration 345 : 0.770149771271487
Loss in iteration 346 : 0.7548717621825072
Loss in iteration 347 : 0.7700973144446752
Loss in iteration 348 : 0.7548393506203844
Loss in iteration 349 : 0.7700456158173431
Loss in iteration 350 : 0.7548075717268634
Loss in iteration 351 : 0.7699945567262865
Loss in iteration 352 : 0.7547762297368902
Loss in iteration 353 : 0.7699440973486594
Loss in iteration 354 : 0.7547453015013617
Loss in iteration 355 : 0.7698942836828533
Loss in iteration 356 : 0.7547149152194885
Loss in iteration 357 : 0.7698451886449639
Loss in iteration 358 : 0.7546852183467222
Loss in iteration 359 : 0.7697968425515241
Loss in iteration 360 : 0.7546562526869488
Loss in iteration 361 : 0.769749207323801
Loss in iteration 362 : 0.754627930031601
Loss in iteration 363 : 0.7697022068254816
Loss in iteration 364 : 0.7546001103894812
Loss in iteration 365 : 0.7696557812511526
Loss in iteration 366 : 0.7545727068418009
Loss in iteration 367 : 0.7696099204853161
Loss in iteration 368 : 0.7545457332932469
Loss in iteration 369 : 0.7695646546029478
Loss in iteration 370 : 0.7545192683290989
Loss in iteration 371 : 0.7695200157008019
Loss in iteration 372 : 0.7544933762968069
Loss in iteration 373 : 0.7694760046359366
Loss in iteration 374 : 0.7544680523529762
Loss in iteration 375 : 0.7694325870277471
Loss in iteration 376 : 0.7544432291039208
Loss in iteration 377 : 0.7693897167398694
Loss in iteration 378 : 0.7544188296606404
Loss in iteration 379 : 0.7693473645588815
Loss in iteration 380 : 0.754394819301607
Loss in iteration 381 : 0.7693055295138402
Loss in iteration 382 : 0.754371216881429
Loss in iteration 383 : 0.7692642271303305
Loss in iteration 384 : 0.7543480640155358
Loss in iteration 385 : 0.7692234673116186
Loss in iteration 386 : 0.7543253823487325
Loss in iteration 387 : 0.7691832402909291
Loss in iteration 388 : 0.7543031532025187
Loss in iteration 389 : 0.7691435199386956
Loss in iteration 390 : 0.7542813312694259
Loss in iteration 391 : 0.7691042790421668
Loss in iteration 392 : 0.7542598763248309
Loss in iteration 393 : 0.7690655030112523
Loss in iteration 394 : 0.7542387759650186
Loss in iteration 395 : 0.7690271919039778
Loss in iteration 396 : 0.7542180436700398
Loss in iteration 397 : 0.7689893511822299
Loss in iteration 398 : 0.7541976977784193
Loss in iteration 399 : 0.7689519800653247
Loss in iteration 400 : 0.7541777404639618
Testing accuracy  of updater 5 on alg 0 with rate 0.4 = 0.733875, training accuracy 0.733875, time elapsed: 7599 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6739863670112279
Loss in iteration 3 : 0.6600890111247913
Loss in iteration 4 : 0.6481102293270543
Loss in iteration 5 : 0.6372019027319259
Loss in iteration 6 : 0.6273151249918157
Loss in iteration 7 : 0.6182707272807234
Loss in iteration 8 : 0.6102695468238102
Loss in iteration 9 : 0.6033247090094723
Loss in iteration 10 : 0.5983914406813933
Loss in iteration 11 : 0.595914599150466
Loss in iteration 12 : 0.6000589403452563
Loss in iteration 13 : 0.608623870057475
Loss in iteration 14 : 0.6299977527526076
Loss in iteration 15 : 0.6268150948064842
Loss in iteration 16 : 0.6318138455108717
Loss in iteration 17 : 0.6018027684787323
Loss in iteration 18 : 0.5952994613860825
Loss in iteration 19 : 0.5775776068025634
Loss in iteration 20 : 0.5719374767033581
Loss in iteration 21 : 0.5626340768194337
Loss in iteration 22 : 0.5600288920121252
Loss in iteration 23 : 0.5554362145016642
Loss in iteration 24 : 0.55576144573994
Loss in iteration 25 : 0.5538394295019788
Loss in iteration 26 : 0.5569807003835137
Loss in iteration 27 : 0.5560331025374325
Loss in iteration 28 : 0.5605951691525911
Loss in iteration 29 : 0.5574394452057276
Loss in iteration 30 : 0.5601555845031219
Loss in iteration 31 : 0.5535465010982459
Loss in iteration 32 : 0.5536264669968438
Loss in iteration 33 : 0.54651647136317
Loss in iteration 34 : 0.5458645564827936
Loss in iteration 35 : 0.5402973992689053
Loss in iteration 36 : 0.540101961455919
Loss in iteration 37 : 0.5361576609456679
Loss in iteration 38 : 0.5369479411180522
Loss in iteration 39 : 0.534283676976645
Loss in iteration 40 : 0.5360966310055665
Loss in iteration 41 : 0.5338882024884677
Loss in iteration 42 : 0.5360256352699041
Loss in iteration 43 : 0.5332934948745951
Loss in iteration 44 : 0.5348975291274853
Loss in iteration 45 : 0.5313594924707632
Loss in iteration 46 : 0.5322372110465462
Loss in iteration 47 : 0.5284925529652058
Loss in iteration 48 : 0.5290845151066583
Loss in iteration 49 : 0.5257568216264432
Loss in iteration 50 : 0.5264365815348068
Loss in iteration 51 : 0.5236474608476308
Loss in iteration 52 : 0.5245898335200252
Loss in iteration 53 : 0.522267945551358
Loss in iteration 54 : 0.5235587509432166
Loss in iteration 55 : 0.5214601551068858
Loss in iteration 56 : 0.5228675851424932
Loss in iteration 57 : 0.5206396891898166
Loss in iteration 58 : 0.5218496505752894
Loss in iteration 59 : 0.5193862508555633
Loss in iteration 60 : 0.5203308461257627
Loss in iteration 61 : 0.5178345843969857
Loss in iteration 62 : 0.5186704823980651
Loss in iteration 63 : 0.5163727458705943
Loss in iteration 64 : 0.5172274934442386
Loss in iteration 65 : 0.5151605747865694
Loss in iteration 66 : 0.5160762814713111
Loss in iteration 67 : 0.5141951955435019
Loss in iteration 68 : 0.5152330490731399
Loss in iteration 69 : 0.5134627283294976
Loss in iteration 70 : 0.5145570172536603
Loss in iteration 71 : 0.5127552832318975
Loss in iteration 72 : 0.5137645379463902
Loss in iteration 73 : 0.511879435121629
Loss in iteration 74 : 0.5127654829531182
Loss in iteration 75 : 0.5108903403286613
Loss in iteration 76 : 0.5117232874148225
Loss in iteration 77 : 0.5099706885072522
Loss in iteration 78 : 0.5108021395368745
Loss in iteration 79 : 0.5091765861134141
Loss in iteration 80 : 0.5100119952960254
Loss in iteration 81 : 0.5084766317752482
Loss in iteration 82 : 0.5093611303642094
Loss in iteration 83 : 0.50788802956262
Loss in iteration 84 : 0.5088111488295448
Loss in iteration 85 : 0.507330759507788
Loss in iteration 86 : 0.5082136216429602
Loss in iteration 87 : 0.5066925721426864
Loss in iteration 88 : 0.5075040006660472
Loss in iteration 89 : 0.5059961368197262
Loss in iteration 90 : 0.5067725288349475
Loss in iteration 91 : 0.505349334930794
Loss in iteration 92 : 0.5061170188707557
Loss in iteration 93 : 0.5047786271506051
Loss in iteration 94 : 0.5055345106916432
Loss in iteration 95 : 0.504250246503018
Loss in iteration 96 : 0.5050311523962537
Loss in iteration 97 : 0.503787034442703
Loss in iteration 98 : 0.5046016727874713
Loss in iteration 99 : 0.5033550329942525
Loss in iteration 100 : 0.5041498437813674
Loss in iteration 101 : 0.502873953673248
Loss in iteration 102 : 0.5036191450287222
Loss in iteration 103 : 0.5023508086435183
Loss in iteration 104 : 0.5030677464002842
Loss in iteration 105 : 0.5018625303740285
Loss in iteration 106 : 0.5025684913226657
Loss in iteration 107 : 0.501428082511111
Loss in iteration 108 : 0.5021184545904062
Loss in iteration 109 : 0.5010163499592541
Loss in iteration 110 : 0.5017233706234538
Loss in iteration 111 : 0.5006499834604549
Loss in iteration 112 : 0.5013899159658065
Loss in iteration 113 : 0.5003136044981388
Loss in iteration 114 : 0.5010443267472761
Loss in iteration 115 : 0.4999407338720437
Loss in iteration 116 : 0.5006323326495608
Loss in iteration 117 : 0.4995299472775108
Loss in iteration 118 : 0.500196433312497
Loss in iteration 119 : 0.49914429708994335
Loss in iteration 120 : 0.4997995044571329
Loss in iteration 121 : 0.49880174425032314
Loss in iteration 122 : 0.49944077247003466
Loss in iteration 123 : 0.4984731623390701
Loss in iteration 124 : 0.49912561370499064
Loss in iteration 125 : 0.4981800662432199
Loss in iteration 126 : 0.49886616741438305
Loss in iteration 127 : 0.4979160931554292
Loss in iteration 128 : 0.49859932110004374
Loss in iteration 129 : 0.4976209030808951
Loss in iteration 130 : 0.498270539649748
Loss in iteration 131 : 0.4972878413086044
Loss in iteration 132 : 0.49791403497961506
Loss in iteration 133 : 0.4969737530670445
Loss in iteration 134 : 0.4975890890169633
Loss in iteration 135 : 0.49669760934051205
Loss in iteration 136 : 0.4972966790525895
Loss in iteration 137 : 0.49643054406864384
Loss in iteration 138 : 0.4970415198970868
Loss in iteration 139 : 0.49619326547528286
Loss in iteration 140 : 0.4968395398188359
Loss in iteration 141 : 0.4959851099430074
Loss in iteration 142 : 0.4966328801546741
Loss in iteration 143 : 0.4957479122654764
Loss in iteration 144 : 0.49636502526486087
Loss in iteration 145 : 0.49547107247865246
Loss in iteration 146 : 0.496065568254647
Loss in iteration 147 : 0.4952093069139522
Loss in iteration 148 : 0.49579360741436956
Loss in iteration 149 : 0.4949834848305575
Loss in iteration 150 : 0.4955511245906933
Loss in iteration 151 : 0.494763232539309
Loss in iteration 152 : 0.49534176832201043
Loss in iteration 153 : 0.49456904795376283
Loss in iteration 154 : 0.4951853240259524
Loss in iteration 155 : 0.494405058369156
Loss in iteration 156 : 0.4950262977574173
Loss in iteration 157 : 0.49421269431685666
Loss in iteration 158 : 0.4948045793850514
Loss in iteration 159 : 0.4939776880227088
Loss in iteration 160 : 0.494547136539345
Loss in iteration 161 : 0.4937552487743543
Loss in iteration 162 : 0.4943152782493748
Loss in iteration 163 : 0.49356891448193074
Loss in iteration 164 : 0.4941113563823244
Loss in iteration 165 : 0.4933850584055973
Loss in iteration 166 : 0.4939373941400836
Loss in iteration 167 : 0.4932245294106565
Loss in iteration 168 : 0.49381788446144725
Loss in iteration 169 : 0.49309647288496417
Loss in iteration 170 : 0.4936979203312872
Loss in iteration 171 : 0.4929398650043882
Loss in iteration 172 : 0.4935120584274515
Loss in iteration 173 : 0.49273643439316267
Loss in iteration 174 : 0.4932857962263023
Loss in iteration 175 : 0.49254390626714245
Loss in iteration 176 : 0.4930847780751025
Loss in iteration 177 : 0.49238947681967055
Loss in iteration 178 : 0.49291125103650746
Loss in iteration 179 : 0.4922343987934193
Loss in iteration 180 : 0.4927647681929778
Loss in iteration 181 : 0.4921002875966876
Loss in iteration 182 : 0.4926759045149529
Loss in iteration 183 : 0.4920022928045069
Loss in iteration 184 : 0.4925891550654828
Loss in iteration 185 : 0.49187505454870883
Loss in iteration 186 : 0.4924317538408285
Loss in iteration 187 : 0.49169548458618806
Loss in iteration 188 : 0.49222838925711576
Loss in iteration 189 : 0.4915256193379826
Loss in iteration 190 : 0.4920512426483418
Loss in iteration 191 : 0.49139770741460165
Loss in iteration 192 : 0.4919021420305602
Loss in iteration 193 : 0.4912657242401847
Loss in iteration 194 : 0.4917768582463574
Loss in iteration 195 : 0.4911522256931911
Loss in iteration 196 : 0.4917139434222008
Loss in iteration 197 : 0.49108009348217335
Loss in iteration 198 : 0.4916565320724701
Loss in iteration 199 : 0.49097773939130473
Loss in iteration 200 : 0.49152216941652827
Loss in iteration 201 : 0.49081595745822615
Loss in iteration 202 : 0.49133501300217736
Loss in iteration 203 : 0.49066282600519867
Loss in iteration 204 : 0.491176275878296
Loss in iteration 205 : 0.49055764619595343
Loss in iteration 206 : 0.4910472753294063
Loss in iteration 207 : 0.4904445189660067
Loss in iteration 208 : 0.49093795515060035
Loss in iteration 209 : 0.49034669762913924
Loss in iteration 210 : 0.49089736356646024
Loss in iteration 211 : 0.4902974490522134
Loss in iteration 212 : 0.4908669488453943
Loss in iteration 213 : 0.49021695852597413
Loss in iteration 214 : 0.4907516694587476
Loss in iteration 215 : 0.49006799947261154
Loss in iteration 216 : 0.4905750174375623
Loss in iteration 217 : 0.4899264034747308
Loss in iteration 218 : 0.49043019948272193
Loss in iteration 219 : 0.48984141806107684
Loss in iteration 220 : 0.490318340205382
Loss in iteration 221 : 0.4897441239006789
Loss in iteration 222 : 0.4902203703234492
Loss in iteration 223 : 0.4896574575236042
Loss in iteration 224 : 0.4901991286092681
Loss in iteration 225 : 0.48962901484992155
Loss in iteration 226 : 0.4901946702896784
Loss in iteration 227 : 0.4895686237317649
Loss in iteration 228 : 0.490095737241119
Loss in iteration 229 : 0.489428314066513
Loss in iteration 230 : 0.4899244511747146
Loss in iteration 231 : 0.489293297442733
Loss in iteration 232 : 0.4897895907394282
Loss in iteration 233 : 0.48922697494905854
Loss in iteration 234 : 0.48969322279707184
Loss in iteration 235 : 0.48914372158494507
Loss in iteration 236 : 0.4896023028396879
Loss in iteration 237 : 0.4890636723381592
Loss in iteration 238 : 0.48959767501904344
Loss in iteration 239 : 0.48905458744238994
Loss in iteration 240 : 0.489619334257609
Loss in iteration 241 : 0.4890137519517473
Loss in iteration 242 : 0.4895351857979101
Loss in iteration 243 : 0.48887855948565484
Loss in iteration 244 : 0.4893643872978571
Loss in iteration 245 : 0.4887449203040121
Loss in iteration 246 : 0.48923557197876105
Loss in iteration 247 : 0.4886964916101133
Loss in iteration 248 : 0.48915449914468706
Loss in iteration 249 : 0.4886270418779645
Loss in iteration 250 : 0.48906644939782085
Loss in iteration 251 : 0.4885485919561115
Loss in iteration 252 : 0.48907541337196847
Loss in iteration 253 : 0.4885577282548783
Loss in iteration 254 : 0.4891245400967399
Loss in iteration 255 : 0.48853724917230135
Loss in iteration 256 : 0.4890549626529018
Loss in iteration 257 : 0.48840431078525365
Loss in iteration 258 : 0.48887983161948684
Loss in iteration 259 : 0.48826604353425557
Loss in iteration 260 : 0.4887525328370118
Loss in iteration 261 : 0.48823507407088956
Loss in iteration 262 : 0.4886883651124472
Loss in iteration 263 : 0.48818156496269977
Loss in iteration 264 : 0.4885991557851163
Loss in iteration 265 : 0.48809867809557433
Loss in iteration 266 : 0.4886176008002173
Loss in iteration 267 : 0.4881246971378319
Loss in iteration 268 : 0.4886967512049229
Loss in iteration 269 : 0.48812701951499254
Loss in iteration 270 : 0.4886433423996404
Loss in iteration 271 : 0.4879944344637605
Loss in iteration 272 : 0.48845908234372915
Loss in iteration 273 : 0.48784407357350357
Loss in iteration 274 : 0.4883270924554598
Loss in iteration 275 : 0.4878294773511334
Loss in iteration 276 : 0.4882837372468668
Loss in iteration 277 : 0.4877980939945486
Loss in iteration 278 : 0.4881900876427708
Loss in iteration 279 : 0.487703160620329
Loss in iteration 280 : 0.4882114359718836
Loss in iteration 281 : 0.48774351370162355
Loss in iteration 282 : 0.4883242904013027
Loss in iteration 283 : 0.4877732223878909
Loss in iteration 284 : 0.4882913674710801
Loss in iteration 285 : 0.4876408159881447
Loss in iteration 286 : 0.48809354133161376
Loss in iteration 287 : 0.48746877277446127
Loss in iteration 288 : 0.4879472404161364
Loss in iteration 289 : 0.4874664714968416
Loss in iteration 290 : 0.4879310823922416
Loss in iteration 291 : 0.48747051883063763
Loss in iteration 292 : 0.4878326250601399
Loss in iteration 293 : 0.48735424134365507
Loss in iteration 294 : 0.48784534507623856
Loss in iteration 295 : 0.487402990705364
Loss in iteration 296 : 0.48799615665723517
Loss in iteration 297 : 0.48746745345218423
Loss in iteration 298 : 0.48799237809320756
Loss in iteration 299 : 0.48733842448393627
Loss in iteration 300 : 0.48777811084045336
Loss in iteration 301 : 0.4871327641680038
Loss in iteration 302 : 0.4876019242631514
Loss in iteration 303 : 0.4871304787173807
Loss in iteration 304 : 0.48761985551917797
Loss in iteration 305 : 0.4871950231015408
Loss in iteration 306 : 0.48752557057979445
Loss in iteration 307 : 0.4870487889927374
Loss in iteration 308 : 0.48750874256855026
Loss in iteration 309 : 0.48709164067084326
Loss in iteration 310 : 0.48770019188729336
Loss in iteration 311 : 0.4872013955367521
Loss in iteration 312 : 0.487741271888764
Loss in iteration 313 : 0.4870856334413132
Loss in iteration 314 : 0.4875124270440372
Loss in iteration 315 : 0.48683362821547427
Loss in iteration 316 : 0.4872823447881769
Loss in iteration 317 : 0.4868009562139705
Loss in iteration 318 : 0.487331676060268
Loss in iteration 319 : 0.48696519251455594
Loss in iteration 320 : 0.48727717032968726
Loss in iteration 321 : 0.486794218791674
Loss in iteration 322 : 0.4871938122713885
Loss in iteration 323 : 0.48679680555337257
Loss in iteration 324 : 0.4874196479361909
Loss in iteration 325 : 0.48696377039897204
Loss in iteration 326 : 0.48753236755544993
Loss in iteration 327 : 0.48688424318148893
Loss in iteration 328 : 0.4873030571942846
Loss in iteration 329 : 0.4865788850047201
Loss in iteration 330 : 0.48698854715504586
Loss in iteration 331 : 0.4864551197142479
Loss in iteration 332 : 0.48702521014693284
Loss in iteration 333 : 0.48674809397953184
Loss in iteration 334 : 0.48710895409075977
Loss in iteration 335 : 0.4866262090949579
Loss in iteration 336 : 0.4869065185038338
Loss in iteration 337 : 0.4865073913028219
Loss in iteration 338 : 0.48712647494078726
Loss in iteration 339 : 0.48673231994298966
Loss in iteration 340 : 0.48735195622265814
Loss in iteration 341 : 0.4867365493757759
Loss in iteration 342 : 0.4871654465667316
Loss in iteration 343 : 0.4863944613412137
Loss in iteration 344 : 0.48674711327062153
Loss in iteration 345 : 0.48609685343234527
Loss in iteration 346 : 0.4866328572241498
Loss in iteration 347 : 0.4864033607776204
Loss in iteration 348 : 0.48700507137847865
Loss in iteration 349 : 0.4866402286796399
Loss in iteration 350 : 0.4867231740537255
Loss in iteration 351 : 0.48624757480150904
Loss in iteration 352 : 0.48677474567866025
Loss in iteration 353 : 0.48645365630199455
Loss in iteration 354 : 0.4871513475526483
Loss in iteration 355 : 0.48662752644273427
Loss in iteration 356 : 0.48711726594489996
Loss in iteration 357 : 0.48633240004363093
Loss in iteration 358 : 0.48663891696652023
Loss in iteration 359 : 0.4858332889925961
Loss in iteration 360 : 0.4862044615344629
Loss in iteration 361 : 0.48571929996184765
Loss in iteration 362 : 0.48648459698469687
Loss in iteration 363 : 0.486604521938169
Loss in iteration 364 : 0.48703449191647974
Loss in iteration 365 : 0.4864273821894612
Loss in iteration 366 : 0.4864289483602287
Loss in iteration 367 : 0.4860410851890562
Loss in iteration 368 : 0.4867404313290546
Loss in iteration 369 : 0.48641454056460154
Loss in iteration 370 : 0.4870876068775592
Loss in iteration 371 : 0.4864317862765137
Loss in iteration 372 : 0.48680333885150945
Loss in iteration 373 : 0.4859133694076443
Loss in iteration 374 : 0.4861465637687295
Loss in iteration 375 : 0.48532931658633355
Loss in iteration 376 : 0.48567582315938124
Loss in iteration 377 : 0.48515195909008607
Loss in iteration 378 : 0.4857439596226727
Loss in iteration 379 : 0.48566148545456056
Loss in iteration 380 : 0.4865111767449008
Loss in iteration 381 : 0.4866949583114
Loss in iteration 382 : 0.487044096448367
Loss in iteration 383 : 0.48767516369757336
Loss in iteration 384 : 0.48800486906228735
Loss in iteration 385 : 0.48754170267840424
Loss in iteration 386 : 0.48717176523849676
Loss in iteration 387 : 0.4863350062189295
Loss in iteration 388 : 0.4861999716820876
Loss in iteration 389 : 0.4855138643847621
Loss in iteration 390 : 0.485793928792043
Loss in iteration 391 : 0.48538274803551235
Loss in iteration 392 : 0.4859228560720211
Loss in iteration 393 : 0.48567625675697795
Loss in iteration 394 : 0.48629381160121554
Loss in iteration 395 : 0.4860842020170297
Loss in iteration 396 : 0.486625413145374
Loss in iteration 397 : 0.4863595408330048
Loss in iteration 398 : 0.48670811064287317
Loss in iteration 399 : 0.48630822936420154
Loss in iteration 400 : 0.4864686695385975
Testing accuracy  of updater 5 on alg 0 with rate 0.09999999999999998 = 0.773, training accuracy 0.773, time elapsed: 8065 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 3.01661754226205
Loss in iteration 3 : 2.468606915006185
Loss in iteration 4 : 1.839756115281385
Loss in iteration 5 : 1.2652937429736604
Loss in iteration 6 : 1.4080173861407126
Loss in iteration 7 : 0.7223093056718833
Loss in iteration 8 : 1.3929399098024342
Loss in iteration 9 : 1.120983698449653
Loss in iteration 10 : 0.7978223405699186
Loss in iteration 11 : 1.171313618063247
Loss in iteration 12 : 0.9406699714211916
Loss in iteration 13 : 0.6524642584034589
Loss in iteration 14 : 0.9278276752219883
Loss in iteration 15 : 0.827841962283879
Loss in iteration 16 : 0.6659761107210641
Loss in iteration 17 : 0.8685010221846614
Loss in iteration 18 : 0.8062859743013452
Loss in iteration 19 : 0.6641875453761626
Loss in iteration 20 : 0.7921104715473573
Loss in iteration 21 : 0.7407351539948345
Loss in iteration 22 : 0.5953801747093573
Loss in iteration 23 : 0.7035187050469127
Loss in iteration 24 : 0.6831287096403862
Loss in iteration 25 : 0.5912355107702933
Loss in iteration 26 : 0.6793436453779825
Loss in iteration 27 : 0.6404537141497422
Loss in iteration 28 : 0.571838536277377
Loss in iteration 29 : 0.6250141272450315
Loss in iteration 30 : 0.5630339634797176
Loss in iteration 31 : 0.5515256697411476
Loss in iteration 32 : 0.5870752339356232
Loss in iteration 33 : 0.5273216252009586
Loss in iteration 34 : 0.5626451201095944
Loss in iteration 35 : 0.5415109423159097
Loss in iteration 36 : 0.5104608887367251
Loss in iteration 37 : 0.5390041482402657
Loss in iteration 38 : 0.49291919814323654
Loss in iteration 39 : 0.5210282347977018
Loss in iteration 40 : 0.5008740127146766
Loss in iteration 41 : 0.5083286232754236
Loss in iteration 42 : 0.4991041376309612
Loss in iteration 43 : 0.49206719747269473
Loss in iteration 44 : 0.4969867125083717
Loss in iteration 45 : 0.48489349573082463
Loss in iteration 46 : 0.4976511648924832
Loss in iteration 47 : 0.48539072671670225
Loss in iteration 48 : 0.4910590904879071
Loss in iteration 49 : 0.4812820602703366
Loss in iteration 50 : 0.48439956813546275
Loss in iteration 51 : 0.48116003925794787
Loss in iteration 52 : 0.48130028141416237
Loss in iteration 53 : 0.4805344083768848
Loss in iteration 54 : 0.4750310099644994
Loss in iteration 55 : 0.4772587205666613
Loss in iteration 56 : 0.47050241474504334
Loss in iteration 57 : 0.47655302783800313
Loss in iteration 58 : 0.46777804215092167
Loss in iteration 59 : 0.4727788908814945
Loss in iteration 60 : 0.465337313554962
Loss in iteration 61 : 0.46828192480023495
Loss in iteration 62 : 0.466331166041497
Loss in iteration 63 : 0.4649982405803996
Loss in iteration 64 : 0.4665815559619489
Loss in iteration 65 : 0.46219945144201585
Loss in iteration 66 : 0.46535809981445175
Loss in iteration 67 : 0.4624500215370399
Loss in iteration 68 : 0.4638256876111543
Loss in iteration 69 : 0.463340884533388
Loss in iteration 70 : 0.4616627397741662
Loss in iteration 71 : 0.46342621407127466
Loss in iteration 72 : 0.46151023479347575
Loss in iteration 73 : 0.4625998356837416
Loss in iteration 74 : 0.46212953181929917
Loss in iteration 75 : 0.4610291153481562
Loss in iteration 76 : 0.46200893427873607
Loss in iteration 77 : 0.46102242334947535
Loss in iteration 78 : 0.46105627216832057
Loss in iteration 79 : 0.461399070740229
Loss in iteration 80 : 0.46036216268981167
Loss in iteration 81 : 0.460797222180377
Loss in iteration 82 : 0.4609835017467767
Loss in iteration 83 : 0.46025712937402147
Loss in iteration 84 : 0.4607578243730212
Loss in iteration 85 : 0.46079382481769243
Loss in iteration 86 : 0.46066463038783334
Loss in iteration 87 : 0.4616452948697636
Loss in iteration 88 : 0.462730519405309
Loss in iteration 89 : 0.4640477921728783
Loss in iteration 90 : 0.46549687031678705
Loss in iteration 91 : 0.4639621941231182
Loss in iteration 92 : 0.4609024394694305
Loss in iteration 93 : 0.45990291120497634
Loss in iteration 94 : 0.4616906682829
Loss in iteration 95 : 0.4629030789087284
Loss in iteration 96 : 0.4613612109135107
Loss in iteration 97 : 0.45976763516185964
Loss in iteration 98 : 0.4603099479556595
Loss in iteration 99 : 0.46142099078139565
Loss in iteration 100 : 0.4609419044883683
Loss in iteration 101 : 0.45965926712230815
Loss in iteration 102 : 0.4597995159495588
Loss in iteration 103 : 0.4607035510396865
Loss in iteration 104 : 0.4604447774640006
Loss in iteration 105 : 0.45953715531551775
Loss in iteration 106 : 0.45959628946965825
Loss in iteration 107 : 0.4602100333331116
Loss in iteration 108 : 0.46007436325620055
Loss in iteration 109 : 0.45946778564801327
Loss in iteration 110 : 0.45945955339135286
Loss in iteration 111 : 0.4598315877886604
Loss in iteration 112 : 0.45980725063621786
Loss in iteration 113 : 0.4594573630512853
Loss in iteration 114 : 0.459350818757684
Loss in iteration 115 : 0.4595278987463103
Loss in iteration 116 : 0.45960392960398044
Loss in iteration 117 : 0.45945923364266655
Loss in iteration 118 : 0.4593258281763949
Loss in iteration 119 : 0.4593230547676652
Loss in iteration 120 : 0.45938238720521746
Loss in iteration 121 : 0.4593960504234318
Loss in iteration 122 : 0.4593513912950223
Loss in iteration 123 : 0.4592871679104849
Loss in iteration 124 : 0.4592410597742404
Loss in iteration 125 : 0.45923731187216354
Loss in iteration 126 : 0.4592681218034687
Loss in iteration 127 : 0.4592893717843186
Loss in iteration 128 : 0.45926313915093414
Loss in iteration 129 : 0.4591999749119787
Loss in iteration 130 : 0.45914874194119176
Loss in iteration 131 : 0.4591472943534886
Loss in iteration 132 : 0.4591832627812391
Loss in iteration 133 : 0.4592135723954274
Loss in iteration 134 : 0.4592051656263949
Loss in iteration 135 : 0.4591635622046851
Loss in iteration 136 : 0.45911824057805095
Loss in iteration 137 : 0.4590955244716306
Loss in iteration 138 : 0.4590983337433054
Loss in iteration 139 : 0.45911352619637225
Loss in iteration 140 : 0.4591263833970859
Loss in iteration 141 : 0.4591305381697988
Loss in iteration 142 : 0.45912815006327873
Loss in iteration 143 : 0.45912282893890766
Loss in iteration 144 : 0.45911899642262827
Loss in iteration 145 : 0.459115675518534
Loss in iteration 146 : 0.45911315634191013
Loss in iteration 147 : 0.45910897497347164
Loss in iteration 148 : 0.4591044888249814
Loss in iteration 149 : 0.45910079873648424
Loss in iteration 150 : 0.45910130372915436
Loss in iteration 151 : 0.4591082818741625
Loss in iteration 152 : 0.45912780780259743
Loss in iteration 153 : 0.4591662329465386
Loss in iteration 154 : 0.4592421493127061
Loss in iteration 155 : 0.45938076743485157
Loss in iteration 156 : 0.45965590171566656
Loss in iteration 157 : 0.46016991959970893
Loss in iteration 158 : 0.46124407579386867
Loss in iteration 159 : 0.4632178352174895
Loss in iteration 160 : 0.46741538150359735
Loss in iteration 161 : 0.47398823062898654
Loss in iteration 162 : 0.4864744658574106
Loss in iteration 163 : 0.5008789196254039
Loss in iteration 164 : 0.5206212092361646
Loss in iteration 165 : 0.5453751273178354
Loss in iteration 166 : 0.5440897927691389
Loss in iteration 167 : 0.5251961761899522
Loss in iteration 168 : 0.4873046674600959
Loss in iteration 169 : 0.4652563544989713
Loss in iteration 170 : 0.46498487963791124
Loss in iteration 171 : 0.4786338860658517
Loss in iteration 172 : 0.49475284110433954
Loss in iteration 173 : 0.49947571054992024
Loss in iteration 174 : 0.48647849827571576
Loss in iteration 175 : 0.469840575608805
Loss in iteration 176 : 0.46124429696087854
Loss in iteration 177 : 0.4629945409062517
Loss in iteration 178 : 0.47063074409158895
Loss in iteration 179 : 0.4786916201275777
Loss in iteration 180 : 0.4842127120067985
Loss in iteration 181 : 0.48620213332175044
Loss in iteration 182 : 0.4805466644752031
Loss in iteration 183 : 0.47377378571527184
Loss in iteration 184 : 0.46752909746006294
Loss in iteration 185 : 0.46314339469121035
Loss in iteration 186 : 0.4604315994457442
Loss in iteration 187 : 0.45930864720297804
Loss in iteration 188 : 0.4599912115375613
Loss in iteration 189 : 0.4620484445276405
Loss in iteration 190 : 0.46522218817314637
Loss in iteration 191 : 0.47112433403989107
Loss in iteration 192 : 0.4833080236496881
Loss in iteration 193 : 0.5040118515375279
Loss in iteration 194 : 0.5410596500639713
Loss in iteration 195 : 0.566649806337231
Loss in iteration 196 : 0.5783450280685501
Loss in iteration 197 : 0.5299913711457174
Loss in iteration 198 : 0.48177827612332647
Loss in iteration 199 : 0.4616669274722567
Loss in iteration 200 : 0.47671009491637356
Loss in iteration 201 : 0.5060080646741494
Loss in iteration 202 : 0.5143019118659347
Loss in iteration 203 : 0.4973873888811253
Loss in iteration 204 : 0.47181873600897967
Loss in iteration 205 : 0.4612919237230922
Loss in iteration 206 : 0.4686113397798907
Loss in iteration 207 : 0.48413751378335124
Loss in iteration 208 : 0.4955595538398613
Loss in iteration 209 : 0.4917064367638378
Loss in iteration 210 : 0.4793197271531846
Loss in iteration 211 : 0.46604973203756456
Loss in iteration 212 : 0.46029635839713745
Loss in iteration 213 : 0.46267594178236754
Loss in iteration 214 : 0.4701424121720697
Loss in iteration 215 : 0.4798059815044827
Loss in iteration 216 : 0.48799041709748453
Loss in iteration 217 : 0.4962936497047119
Loss in iteration 218 : 0.5000669557829908
Loss in iteration 219 : 0.5044465789051743
Loss in iteration 220 : 0.5016005159935911
Loss in iteration 221 : 0.49838126442783665
Loss in iteration 222 : 0.48833670053659484
Loss in iteration 223 : 0.4786149844224286
Loss in iteration 224 : 0.46891962965353046
Loss in iteration 225 : 0.4624938433349242
Loss in iteration 226 : 0.45975010177843006
Loss in iteration 227 : 0.46028950199845
Loss in iteration 228 : 0.463335605759327
Loss in iteration 229 : 0.4681173515710071
Loss in iteration 230 : 0.47449339071490815
Loss in iteration 231 : 0.4818165455249783
Loss in iteration 232 : 0.4910339923199702
Loss in iteration 233 : 0.4986364084801723
Loss in iteration 234 : 0.5067353837727656
Loss in iteration 235 : 0.5049998344041412
Loss in iteration 236 : 0.49974504253638136
Loss in iteration 237 : 0.48584770539966415
Loss in iteration 238 : 0.47312096253890273
Loss in iteration 239 : 0.46381103104650273
Loss in iteration 240 : 0.4599334461730781
Loss in iteration 241 : 0.4609251064480146
Loss in iteration 242 : 0.4656495505133083
Loss in iteration 243 : 0.47368885306855146
Loss in iteration 244 : 0.48448509940922635
Loss in iteration 245 : 0.4991063849796584
Loss in iteration 246 : 0.5132245735615079
Loss in iteration 247 : 0.5276429098402052
Loss in iteration 248 : 0.5241482098833933
Loss in iteration 249 : 0.5127657010403851
Loss in iteration 250 : 0.4876383583841483
Loss in iteration 251 : 0.468632496227624
Loss in iteration 252 : 0.4610135130691806
Loss in iteration 253 : 0.46486031677889067
Loss in iteration 254 : 0.47584788973747716
Loss in iteration 255 : 0.48813207683634013
Loss in iteration 256 : 0.49936814288638165
Loss in iteration 257 : 0.502989999150234
Loss in iteration 258 : 0.5000335076379241
Loss in iteration 259 : 0.48995754497078897
Loss in iteration 260 : 0.47976344382468844
Loss in iteration 261 : 0.469447835103983
Loss in iteration 262 : 0.462688550480306
Loss in iteration 263 : 0.45998684757597824
Loss in iteration 264 : 0.4610718001033059
Loss in iteration 265 : 0.46465697233680237
Loss in iteration 266 : 0.4694887663958432
Loss in iteration 267 : 0.4760691810694932
Loss in iteration 268 : 0.4837798834510619
Loss in iteration 269 : 0.4950769197395959
Loss in iteration 270 : 0.5068204597208945
Loss in iteration 271 : 0.5209986387675003
Loss in iteration 272 : 0.5247329147089054
Loss in iteration 273 : 0.5240700692133801
Loss in iteration 274 : 0.5040925974762228
Loss in iteration 275 : 0.4834836024626867
Loss in iteration 276 : 0.4666096411571669
Loss in iteration 277 : 0.4607602052557303
Loss in iteration 278 : 0.4646991144787829
Loss in iteration 279 : 0.4741743818572591
Loss in iteration 280 : 0.48583030766052937
Loss in iteration 281 : 0.49420550997703877
Loss in iteration 282 : 0.4976417937388633
Loss in iteration 283 : 0.4936998252961712
Loss in iteration 284 : 0.486274552063048
Loss in iteration 285 : 0.476414444856023
Loss in iteration 286 : 0.4685156525377836
Loss in iteration 287 : 0.4626638403268547
Loss in iteration 288 : 0.46000609847592033
Loss in iteration 289 : 0.46028311451991455
Loss in iteration 290 : 0.4626973264183687
Loss in iteration 291 : 0.46675770178353465
Loss in iteration 292 : 0.47244191827733995
Loss in iteration 293 : 0.48203236147013717
Loss in iteration 294 : 0.4956682149719783
Loss in iteration 295 : 0.5175817988853355
Loss in iteration 296 : 0.5378938570233572
Loss in iteration 297 : 0.5570001818844538
Loss in iteration 298 : 0.5414492214786851
Loss in iteration 299 : 0.5151377085653289
Loss in iteration 300 : 0.4798297930637815
Loss in iteration 301 : 0.46237391903536423
Loss in iteration 302 : 0.46579798525684407
Loss in iteration 303 : 0.4815835252395999
Loss in iteration 304 : 0.49817654366463004
Loss in iteration 305 : 0.5020689331768565
Loss in iteration 306 : 0.4940828474955682
Loss in iteration 307 : 0.47929931371763396
Loss in iteration 308 : 0.4663873734705141
Loss in iteration 309 : 0.46051868081015934
Loss in iteration 310 : 0.46174336432585056
Loss in iteration 311 : 0.46757240882713075
Loss in iteration 312 : 0.47503411192799105
Loss in iteration 313 : 0.4813110154950035
Loss in iteration 314 : 0.48407015616586
Loss in iteration 315 : 0.4851808489860294
Loss in iteration 316 : 0.48219590466398693
Loss in iteration 317 : 0.4796626402011684
Loss in iteration 318 : 0.4762505936721371
Loss in iteration 319 : 0.4745993787771997
Loss in iteration 320 : 0.47378069092504316
Loss in iteration 321 : 0.4744722571066361
Loss in iteration 322 : 0.4764409855149879
Loss in iteration 323 : 0.4810579635340848
Loss in iteration 324 : 0.4871134421202929
Loss in iteration 325 : 0.497416741722942
Loss in iteration 326 : 0.505478964012447
Loss in iteration 327 : 0.5146338673633625
Loss in iteration 328 : 0.5115658456258263
Loss in iteration 329 : 0.5034940030920049
Loss in iteration 330 : 0.4867078394661729
Loss in iteration 331 : 0.4720128895616093
Loss in iteration 332 : 0.4625107087084182
Loss in iteration 333 : 0.459958637387885
Loss in iteration 334 : 0.46317195095703834
Loss in iteration 335 : 0.4701644551699995
Loss in iteration 336 : 0.4795833482062815
Loss in iteration 337 : 0.4890293163710454
Loss in iteration 338 : 0.49850519203800864
Loss in iteration 339 : 0.5025711564106926
Loss in iteration 340 : 0.5039091058501831
Loss in iteration 341 : 0.49584432282030755
Loss in iteration 342 : 0.48613686473008055
Loss in iteration 343 : 0.4741205094912491
Loss in iteration 344 : 0.4654307343210311
Loss in iteration 345 : 0.4607374399514146
Loss in iteration 346 : 0.45982794122398646
Loss in iteration 347 : 0.46191633624166134
Loss in iteration 348 : 0.46663735893568303
Loss in iteration 349 : 0.4749127220552954
Loss in iteration 350 : 0.4876591039306478
Loss in iteration 351 : 0.5079804636665319
Loss in iteration 352 : 0.5306672089980925
Loss in iteration 353 : 0.5546359221751472
Loss in iteration 354 : 0.5469682664115264
Loss in iteration 355 : 0.5255664207790135
Loss in iteration 356 : 0.48731500968278213
Loss in iteration 357 : 0.4647881363367346
Loss in iteration 358 : 0.464044369701107
Loss in iteration 359 : 0.47870237443013447
Loss in iteration 360 : 0.49736832744416026
Loss in iteration 361 : 0.5055548213081512
Loss in iteration 362 : 0.5015724733364729
Loss in iteration 363 : 0.487529595415068
Loss in iteration 364 : 0.47229752670373865
Loss in iteration 365 : 0.46274384485700876
Loss in iteration 366 : 0.4604157280055243
Loss in iteration 367 : 0.46377908704556986
Loss in iteration 368 : 0.4703010666022824
Loss in iteration 369 : 0.47709180271059054
Loss in iteration 370 : 0.481663841385169
Loss in iteration 371 : 0.48427626505022864
Loss in iteration 372 : 0.4829779692704329
Loss in iteration 373 : 0.481591144404177
Loss in iteration 374 : 0.47903751549510015
Loss in iteration 375 : 0.47826842783228346
Loss in iteration 376 : 0.47814866714758975
Loss in iteration 377 : 0.47992859933870136
Loss in iteration 378 : 0.4828433365811336
Loss in iteration 379 : 0.48857842994322825
Loss in iteration 380 : 0.4943711281344034
Loss in iteration 381 : 0.5029644430546725
Loss in iteration 382 : 0.5058377749782559
Loss in iteration 383 : 0.5070690235258273
Loss in iteration 384 : 0.49825484714961327
Loss in iteration 385 : 0.487078238003292
Loss in iteration 386 : 0.4743339085230591
Loss in iteration 387 : 0.46506036380682203
Loss in iteration 388 : 0.46042501175984535
Loss in iteration 389 : 0.4601307323445252
Loss in iteration 390 : 0.4631606684836258
Loss in iteration 391 : 0.4685821125508462
Loss in iteration 392 : 0.4761591475302305
Loss in iteration 393 : 0.48528872200843237
Loss in iteration 394 : 0.4966369453839187
Loss in iteration 395 : 0.5058366703169597
Loss in iteration 396 : 0.5139395544225279
Loss in iteration 397 : 0.5102873111400795
Loss in iteration 398 : 0.50226293660369
Loss in iteration 399 : 0.48650940737394516
Loss in iteration 400 : 0.4731307835477286
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.789125, training accuracy 0.789125, time elapsed: 7644 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.674141656384205
Loss in iteration 3 : 0.7996599903843428
Loss in iteration 4 : 1.1567475430574288
Loss in iteration 5 : 0.5607142885132747
Loss in iteration 6 : 0.712202043424841
Loss in iteration 7 : 0.7887882109781703
Loss in iteration 8 : 0.5424748305964036
Loss in iteration 9 : 0.7706704666268083
Loss in iteration 10 : 0.5351388621334702
Loss in iteration 11 : 0.7020567984700338
Loss in iteration 12 : 0.5460537244878617
Loss in iteration 13 : 0.666441915259117
Loss in iteration 14 : 0.5527132650292012
Loss in iteration 15 : 0.6328794008326581
Loss in iteration 16 : 0.5624763749326269
Loss in iteration 17 : 0.6068371691373484
Loss in iteration 18 : 0.5622972304898222
Loss in iteration 19 : 0.5822573249890534
Loss in iteration 20 : 0.557963234774369
Loss in iteration 21 : 0.5556129909777143
Loss in iteration 22 : 0.551728916974238
Loss in iteration 23 : 0.5350438260164357
Loss in iteration 24 : 0.5376596136130913
Loss in iteration 25 : 0.5168686272207423
Loss in iteration 26 : 0.5251833343394796
Loss in iteration 27 : 0.5045524955936699
Loss in iteration 28 : 0.510911474340866
Loss in iteration 29 : 0.49604881232384046
Loss in iteration 30 : 0.4992494587010728
Loss in iteration 31 : 0.4921539147057085
Loss in iteration 32 : 0.4882214430900941
Loss in iteration 33 : 0.48992102877648835
Loss in iteration 34 : 0.4802752006016275
Loss in iteration 35 : 0.4892828058758669
Loss in iteration 36 : 0.47737144068791953
Loss in iteration 37 : 0.48493184753792257
Loss in iteration 38 : 0.4792328372942469
Loss in iteration 39 : 0.4768570933265475
Loss in iteration 40 : 0.4823725304797805
Loss in iteration 41 : 0.4711479433051735
Loss in iteration 42 : 0.4809858741253305
Loss in iteration 43 : 0.4713547238031913
Loss in iteration 44 : 0.47338830756271044
Loss in iteration 45 : 0.47297571852316617
Loss in iteration 46 : 0.46727739460918294
Loss in iteration 47 : 0.4709874667195727
Loss in iteration 48 : 0.46611076583052513
Loss in iteration 49 : 0.4664640643096486
Loss in iteration 50 : 0.4659859082658029
Loss in iteration 51 : 0.4635584114310102
Loss in iteration 52 : 0.4639066640587221
Loss in iteration 53 : 0.46344648908790864
Loss in iteration 54 : 0.46158472478664836
Loss in iteration 55 : 0.4633222481292631
Loss in iteration 56 : 0.46127607348182775
Loss in iteration 57 : 0.461908736454248
Loss in iteration 58 : 0.4621545667485184
Loss in iteration 59 : 0.4606789748902877
Loss in iteration 60 : 0.46213323718953
Loss in iteration 61 : 0.4608461097016958
Loss in iteration 62 : 0.46114312922280315
Loss in iteration 63 : 0.46130654776769486
Loss in iteration 64 : 0.4605326951249251
Loss in iteration 65 : 0.4608575682995869
Loss in iteration 66 : 0.4606482799184984
Loss in iteration 67 : 0.46019293751030077
Loss in iteration 68 : 0.4604670350659711
Loss in iteration 69 : 0.4601796631435823
Loss in iteration 70 : 0.4599126945094581
Loss in iteration 71 : 0.4602539334830998
Loss in iteration 72 : 0.4598367217508985
Loss in iteration 73 : 0.45986074737891863
Loss in iteration 74 : 0.4600378719466181
Loss in iteration 75 : 0.45969968820759116
Loss in iteration 76 : 0.45984274757475635
Loss in iteration 77 : 0.459867864068254
Loss in iteration 78 : 0.4596587508643564
Loss in iteration 79 : 0.4597719427271605
Loss in iteration 80 : 0.45973738378428947
Loss in iteration 81 : 0.4596066275394899
Loss in iteration 82 : 0.4596495161164059
Loss in iteration 83 : 0.4596244210478764
Loss in iteration 84 : 0.4595030708629212
Loss in iteration 85 : 0.4595380591335445
Loss in iteration 86 : 0.4595091945036112
Loss in iteration 87 : 0.4594025595276497
Loss in iteration 88 : 0.45944981793378115
Loss in iteration 89 : 0.4594043669827875
Loss in iteration 90 : 0.4593382572321913
Loss in iteration 91 : 0.459379059643841
Loss in iteration 92 : 0.4593291334399921
Loss in iteration 93 : 0.4593050895607326
Loss in iteration 94 : 0.4593203328073693
Loss in iteration 95 : 0.4592922365282187
Loss in iteration 96 : 0.4592736281410391
Loss in iteration 97 : 0.4592812406161918
Loss in iteration 98 : 0.4592661254502666
Loss in iteration 99 : 0.4592415936871066
Loss in iteration 100 : 0.45925648473256875
Loss in iteration 101 : 0.4592330981685442
Loss in iteration 102 : 0.45921944573828793
Loss in iteration 103 : 0.4592281404591597
Loss in iteration 104 : 0.4592038778260932
Loss in iteration 105 : 0.4591990114454471
Loss in iteration 106 : 0.4591974796690787
Loss in iteration 107 : 0.459182527157953
Loss in iteration 108 : 0.45917640741486504
Loss in iteration 109 : 0.4591734008490119
Loss in iteration 110 : 0.4591646207762798
Loss in iteration 111 : 0.45915579976981546
Loss in iteration 112 : 0.459156474690301
Loss in iteration 113 : 0.45914715514744403
Loss in iteration 114 : 0.459140453263837
Loss in iteration 115 : 0.45914116496148105
Loss in iteration 116 : 0.4591319098632663
Loss in iteration 117 : 0.45912784849029536
Loss in iteration 118 : 0.4591257578325514
Loss in iteration 119 : 0.4591191830973138
Loss in iteration 120 : 0.45911478010346835
Loss in iteration 121 : 0.4591117760806678
Loss in iteration 122 : 0.459107126701763
Loss in iteration 123 : 0.45910196119411284
Loss in iteration 124 : 0.4590995620456597
Loss in iteration 125 : 0.4590956097226679
Loss in iteration 126 : 0.4590907477696191
Loss in iteration 127 : 0.45908881111606503
Loss in iteration 128 : 0.45908522244255034
Loss in iteration 129 : 0.45908130000766934
Loss in iteration 130 : 0.4590792729670045
Loss in iteration 131 : 0.45907626636336585
Loss in iteration 132 : 0.45907308579468137
Loss in iteration 133 : 0.45907083403013654
Loss in iteration 134 : 0.4590684843238969
Loss in iteration 135 : 0.4590656221737645
Loss in iteration 136 : 0.45906335820997446
Loss in iteration 137 : 0.4590613898824339
Loss in iteration 138 : 0.45905880805067195
Loss in iteration 139 : 0.4590566331256197
Loss in iteration 140 : 0.4590548635251869
Loss in iteration 141 : 0.45905259518576935
Loss in iteration 142 : 0.4590505578468128
Loss in iteration 143 : 0.459048907611706
Loss in iteration 144 : 0.4590469424166533
Testing accuracy  of updater 6 on alg 0 with rate 1.4000000000000001 = 0.7895, training accuracy 0.7895, time elapsed: 2678 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.684688754788703
Loss in iteration 3 : 0.6448968418496943
Loss in iteration 4 : 0.5833790186845277
Loss in iteration 5 : 0.5505522993499465
Loss in iteration 6 : 0.5312314122042644
Loss in iteration 7 : 0.5061270375305268
Loss in iteration 8 : 0.5020538089352483
Loss in iteration 9 : 0.4889365581270739
Loss in iteration 10 : 0.49024131440282964
Loss in iteration 11 : 0.4844240572728024
Loss in iteration 12 : 0.48773016038603023
Loss in iteration 13 : 0.4844986216953993
Loss in iteration 14 : 0.48677380768419426
Loss in iteration 15 : 0.4846305081253237
Loss in iteration 16 : 0.48542277163088177
Loss in iteration 17 : 0.48501145332943835
Loss in iteration 18 : 0.48414538080267594
Loss in iteration 19 : 0.4841712485256134
Loss in iteration 20 : 0.48165495323928204
Loss in iteration 21 : 0.48130828305921874
Loss in iteration 22 : 0.4786120917579591
Loss in iteration 23 : 0.47770240107959877
Loss in iteration 24 : 0.47559096265482714
Loss in iteration 25 : 0.4739154751753433
Loss in iteration 26 : 0.4724068803638521
Loss in iteration 27 : 0.4703172579466117
Loss in iteration 28 : 0.46943119398251376
Loss in iteration 29 : 0.46766848435319913
Loss in iteration 30 : 0.4671994924051784
Loss in iteration 31 : 0.46591931808389037
Loss in iteration 32 : 0.465463435212293
Loss in iteration 33 : 0.4646800266131373
Loss in iteration 34 : 0.46429857460300206
Loss in iteration 35 : 0.46403479039875867
Loss in iteration 36 : 0.46377191894996417
Loss in iteration 37 : 0.46379332272474494
Loss in iteration 38 : 0.46355444534556794
Loss in iteration 39 : 0.4636788446399643
Loss in iteration 40 : 0.46349483226880467
Loss in iteration 41 : 0.46367509124951006
Loss in iteration 42 : 0.46350685341282544
Loss in iteration 43 : 0.4635918803036096
Loss in iteration 44 : 0.4633584135863141
Loss in iteration 45 : 0.4633378522142784
Loss in iteration 46 : 0.46309324729455115
Loss in iteration 47 : 0.46300893633537327
Loss in iteration 48 : 0.4627537929754943
Loss in iteration 49 : 0.4626026753582496
Loss in iteration 50 : 0.4623368155497874
Loss in iteration 51 : 0.46215839148063825
Loss in iteration 52 : 0.46192573857438457
Loss in iteration 53 : 0.46175527426061763
Loss in iteration 54 : 0.46155524018106847
Loss in iteration 55 : 0.46138970300199944
Loss in iteration 56 : 0.4612320694931554
Loss in iteration 57 : 0.4610998570785594
Loss in iteration 58 : 0.4609974773774029
Loss in iteration 59 : 0.46089827459413935
Loss in iteration 60 : 0.4608321708998185
Loss in iteration 61 : 0.46075426029250277
Loss in iteration 62 : 0.46071292032882605
Loss in iteration 63 : 0.46065551360975426
Loss in iteration 64 : 0.46063171696393923
Loss in iteration 65 : 0.4605844104477127
Loss in iteration 66 : 0.4605621934912094
Loss in iteration 67 : 0.4605175021703963
Loss in iteration 68 : 0.4604954781333655
Loss in iteration 69 : 0.46045435808879
Loss in iteration 70 : 0.4604294918515784
Loss in iteration 71 : 0.4603870946909179
Loss in iteration 72 : 0.46035657757131104
Loss in iteration 73 : 0.4603131997210102
Loss in iteration 74 : 0.4602811260748717
Loss in iteration 75 : 0.46024049701995173
Loss in iteration 76 : 0.460209090566609
Loss in iteration 77 : 0.4601710675454757
Loss in iteration 78 : 0.46014132786367723
Loss in iteration 79 : 0.4601082497979747
Loss in iteration 80 : 0.46008251277769785
Loss in iteration 81 : 0.4600545874067591
Loss in iteration 82 : 0.4600317044579271
Loss in iteration 83 : 0.460007314822585
Loss in iteration 84 : 0.45998663874161416
Loss in iteration 85 : 0.4599654587254013
Loss in iteration 86 : 0.4599469429528746
Loss in iteration 87 : 0.4599278170526486
Loss in iteration 88 : 0.459910020889875
Loss in iteration 89 : 0.45989165331056814
Loss in iteration 90 : 0.459874299957371
Loss in iteration 91 : 0.45985656622123644
Loss in iteration 92 : 0.45983937437929817
Loss in iteration 93 : 0.45982171066552635
Loss in iteration 94 : 0.45980440519508126
Loss in iteration 95 : 0.45978689447318943
Loss in iteration 96 : 0.4597699132890697
Loss in iteration 97 : 0.45975295341755007
Loss in iteration 98 : 0.45973646815511343
Loss in iteration 99 : 0.4597200091304925
Loss in iteration 100 : 0.4597040652424
Loss in iteration 101 : 0.45968834943770276
Loss in iteration 102 : 0.45967320252226024
Loss in iteration 103 : 0.45965831038265437
Loss in iteration 104 : 0.4596439052180244
Loss in iteration 105 : 0.4596297838164587
Loss in iteration 106 : 0.4596161439930453
Loss in iteration 107 : 0.45960283700939497
Loss in iteration 108 : 0.4595899668433874
Loss in iteration 109 : 0.4595773640500056
Loss in iteration 110 : 0.4595651053091651
Loss in iteration 111 : 0.4595530839461421
Loss in iteration 112 : 0.4595413969541019
Loss in iteration 113 : 0.4595299279571377
Loss in iteration 114 : 0.4595187434185634
Loss in iteration 115 : 0.4595077362109838
Loss in iteration 116 : 0.4594969879883481
Loss in iteration 117 : 0.4594864152051904
Loss in iteration 118 : 0.4594760970619363
Loss in iteration 119 : 0.45946594597000895
Loss in iteration 120 : 0.4594560259457924
Loss in iteration 121 : 0.4594462596419471
Loss in iteration 122 : 0.4594367256757968
Loss in iteration 123 : 0.4594273575116483
Loss in iteration 124 : 0.4594182213955618
Loss in iteration 125 : 0.45940924523888715
Loss in iteration 126 : 0.4594004903210563
Loss in iteration 127 : 0.45939189434303074
Loss in iteration 128 : 0.4593835143477474
Loss in iteration 129 : 0.4593752926147701
Loss in iteration 130 : 0.4593672737900592
Loss in iteration 131 : 0.45935940286991284
Loss in iteration 132 : 0.4593517199583799
Loss in iteration 133 : 0.45934418118396475
Loss in iteration 134 : 0.4593368215024842
Loss in iteration 135 : 0.4593295989346917
Loss in iteration 136 : 0.45932254119165433
Loss in iteration 137 : 0.4593156127339071
Loss in iteration 138 : 0.45930883908458076
Loss in iteration 139 : 0.4593021909851351
Loss in iteration 140 : 0.459295689638644
Loss in iteration 141 : 0.4592893094295194
Loss in iteration 142 : 0.45928306737296143
Loss in iteration 143 : 0.4592769435047677
Loss in iteration 144 : 0.4592709528889827
Loss in iteration 145 : 0.45926507883745105
Loss in iteration 146 : 0.45925933201089447
Loss in iteration 147 : 0.45925369814969813
Loss in iteration 148 : 0.45924818575644805
Loss in iteration 149 : 0.459242783634622
Loss in iteration 150 : 0.45923749801109726
Loss in iteration 151 : 0.4592323194895563
Loss in iteration 152 : 0.45922725168712575
Loss in iteration 153 : 0.45922228710987373
Loss in iteration 154 : 0.4592174279826062
Loss in iteration 155 : 0.459212668835669
Loss in iteration 156 : 0.4592080100861272
Loss in iteration 157 : 0.45920344737817276
Loss in iteration 158 : 0.4591989799651354
Loss in iteration 159 : 0.45919460498019204
Loss in iteration 160 : 0.4591903210230732
Loss in iteration 161 : 0.45918612614188703
Loss in iteration 162 : 0.45918201822396704
Loss in iteration 163 : 0.4591779958588877
Loss in iteration 164 : 0.45917405664762884
Loss in iteration 165 : 0.459170199778885
Loss in iteration 166 : 0.45916642270173735
Loss in iteration 167 : 0.4591627247339601
Loss in iteration 168 : 0.4591591032003933
Loss in iteration 169 : 0.45915555758963605
Loss in iteration 170 : 0.4591520853791436
Loss in iteration 171 : 0.4591486860868898
Loss in iteration 172 : 0.4591453573005663
Loss in iteration 173 : 0.45914209840497777
Loss in iteration 174 : 0.4591389071657392
Loss in iteration 175 : 0.45913578289088436
Loss in iteration 176 : 0.45913272361826113
Loss in iteration 177 : 0.4591297285080973
Loss in iteration 178 : 0.4591267957828147
Loss in iteration 179 : 0.4591239244776331
Loss in iteration 180 : 0.45912111305880887
Testing accuracy  of updater 6 on alg 0 with rate 0.8 = 0.7895, training accuracy 0.7895, time elapsed: 3377 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6780148520419932
Loss in iteration 3 : 0.6658406032479727
Loss in iteration 4 : 0.6464715837406732
Loss in iteration 5 : 0.6251008933449957
Loss in iteration 6 : 0.608312336629614
Loss in iteration 7 : 0.5877358243323029
Loss in iteration 8 : 0.567990590467706
Loss in iteration 9 : 0.553814947723361
Loss in iteration 10 : 0.5404398325389563
Loss in iteration 11 : 0.5285045871868406
Loss in iteration 12 : 0.5205687245649522
Loss in iteration 13 : 0.513751761750685
Loss in iteration 14 : 0.5066004927198369
Loss in iteration 15 : 0.5012584512414799
Loss in iteration 16 : 0.49768586995883024
Loss in iteration 17 : 0.49411185887200176
Loss in iteration 18 : 0.4908052733642771
Loss in iteration 19 : 0.4887876703355991
Loss in iteration 20 : 0.4873982622211399
Loss in iteration 21 : 0.4856952810604169
Loss in iteration 22 : 0.4840850722965977
Loss in iteration 23 : 0.4830911799383906
Loss in iteration 24 : 0.48225739714811267
Loss in iteration 25 : 0.4811416235951168
Loss in iteration 26 : 0.48008250391856233
Loss in iteration 27 : 0.47940562886312316
Loss in iteration 28 : 0.47883299413930835
Loss in iteration 29 : 0.4780921736169499
Loss in iteration 30 : 0.47737287959892444
Loss in iteration 31 : 0.4768589618195705
Loss in iteration 32 : 0.47637100262545484
Loss in iteration 33 : 0.47573219915679427
Loss in iteration 34 : 0.4750666851753057
Loss in iteration 35 : 0.47452124647810046
Loss in iteration 36 : 0.47401112767835835
Loss in iteration 37 : 0.4734255693477663
Loss in iteration 38 : 0.47283613602238644
Loss in iteration 39 : 0.4723390078234875
Loss in iteration 40 : 0.4718813082485204
Loss in iteration 41 : 0.47138140623784647
Loss in iteration 42 : 0.47087549935202616
Loss in iteration 43 : 0.4704288829044284
Loss in iteration 44 : 0.4700146951287293
Loss in iteration 45 : 0.4695807974005562
Loss in iteration 46 : 0.4691500892163364
Loss in iteration 47 : 0.4687690507900768
Loss in iteration 48 : 0.4684238230091182
Loss in iteration 49 : 0.4680783571923963
Loss in iteration 50 : 0.4677428790791556
Loss in iteration 51 : 0.467444791205338
Loss in iteration 52 : 0.46717293471290544
Loss in iteration 53 : 0.46690122502677783
Loss in iteration 54 : 0.4666357123089806
Loss in iteration 55 : 0.46639633524609997
Loss in iteration 56 : 0.4661778439858009
Loss in iteration 57 : 0.46596384713735256
Loss in iteration 58 : 0.46575850017315623
Loss in iteration 59 : 0.46557476973448525
Loss in iteration 60 : 0.46540805485229125
Loss in iteration 61 : 0.4652458580177583
Loss in iteration 62 : 0.46508995494925026
Loss in iteration 63 : 0.46494874470145237
Loss in iteration 64 : 0.46481905857130446
Loss in iteration 65 : 0.46469253340029787
Loss in iteration 66 : 0.46457063674962096
Loss in iteration 67 : 0.4644593608661958
Loss in iteration 68 : 0.4643567498844981
Loss in iteration 69 : 0.4642571424366921
Loss in iteration 70 : 0.4641612850721458
Loss in iteration 71 : 0.4640727851944614
Loss in iteration 72 : 0.46398992251006044
Loss in iteration 73 : 0.4639087112391785
Loss in iteration 74 : 0.46382965217706085
Loss in iteration 75 : 0.46375524903600446
Loss in iteration 76 : 0.4636845692706395
Loss in iteration 77 : 0.4636152733838442
Loss in iteration 78 : 0.46354788896878263
Loss in iteration 79 : 0.46348401560715763
Loss in iteration 80 : 0.4634228797596472
Loss in iteration 81 : 0.4633628663185283
Loss in iteration 82 : 0.4633042834739648
Loss in iteration 83 : 0.46324808556697006
Loss in iteration 84 : 0.4631937064039919
Loss in iteration 85 : 0.46314016712121087
Loss in iteration 86 : 0.4630877857937298
Loss in iteration 87 : 0.46303721441885193
Loss in iteration 88 : 0.46298807654323415
Loss in iteration 89 : 0.4629397763341111
Loss in iteration 90 : 0.46289255420636916
Loss in iteration 91 : 0.46284678380285416
Loss in iteration 92 : 0.4628021453185251
Loss in iteration 93 : 0.46275823710397174
Loss in iteration 94 : 0.46271523509969364
Loss in iteration 95 : 0.4626733807353097
Loss in iteration 96 : 0.4626324686859768
Loss in iteration 97 : 0.46259227668049563
Loss in iteration 98 : 0.46255295267482244
Loss in iteration 99 : 0.4625146398644218
Loss in iteration 100 : 0.4624771723859708
Loss in iteration 101 : 0.46244039248308105
Loss in iteration 102 : 0.46240438759379426
Loss in iteration 103 : 0.46236922724960106
Loss in iteration 104 : 0.46233478609527434
Loss in iteration 105 : 0.46230096757055844
Loss in iteration 106 : 0.4622678375889692
Loss in iteration 107 : 0.4622354352706733
Loss in iteration 108 : 0.4622036713331648
Loss in iteration 109 : 0.4621724852089343
Loss in iteration 110 : 0.4621419186966883
Loss in iteration 111 : 0.46211198333046394
Loss in iteration 112 : 0.4620826067553578
Loss in iteration 113 : 0.4620537446720959
Loss in iteration 114 : 0.46202542091780274
Loss in iteration 115 : 0.4619976356211387
Loss in iteration 116 : 0.46197033751294314
Loss in iteration 117 : 0.46194349986081734
Loss in iteration 118 : 0.46191713834980985
Loss in iteration 119 : 0.4618912470647748
Loss in iteration 120 : 0.4618657862185262
Loss in iteration 121 : 0.46184073484792953
Loss in iteration 122 : 0.4618160987513918
Loss in iteration 123 : 0.46179186791398436
Loss in iteration 124 : 0.46176801270749107
Loss in iteration 125 : 0.46174451891780816
Loss in iteration 126 : 0.4617213890310936
Loss in iteration 127 : 0.4616986136026032
Loss in iteration 128 : 0.4616761715003634
Loss in iteration 129 : 0.46165405279022
Loss in iteration 130 : 0.46163225699299504
Loss in iteration 131 : 0.4616107745156688
Loss in iteration 132 : 0.4615895891411901
Loss in iteration 133 : 0.46156869297505104
Loss in iteration 134 : 0.4615480836317087
Loss in iteration 135 : 0.4615277524652913
Loss in iteration 136 : 0.4615076876923588
Loss in iteration 137 : 0.46148788364689747
Loss in iteration 138 : 0.4614683374348846
Loss in iteration 139 : 0.461449041469518
Loss in iteration 140 : 0.4614299865934128
Loss in iteration 141 : 0.4614111678807603
Loss in iteration 142 : 0.46139258170400044
Loss in iteration 143 : 0.46137422128711747
Loss in iteration 144 : 0.461356079472038
Loss in iteration 145 : 0.4613381521487417
Loss in iteration 146 : 0.46132043572559994
Loss in iteration 147 : 0.4613029244844589
Loss in iteration 148 : 0.4612856127903507
Loss in iteration 149 : 0.461268497028518
Loss in iteration 150 : 0.4612515736213793
Loss in iteration 151 : 0.46123483758277223
Loss in iteration 152 : 0.46121828422874456
Loss in iteration 153 : 0.46120191021619034
Loss in iteration 154 : 0.4611857120867555
Loss in iteration 155 : 0.46116968556196536
Loss in iteration 156 : 0.4611538267851821
Loss in iteration 157 : 0.46113813277183774
Loss in iteration 158 : 0.4611226003412161
Loss in iteration 159 : 0.4611072258231262
Loss in iteration 160 : 0.46109200592945637
Loss in iteration 161 : 0.4610769378998283
Loss in iteration 162 : 0.4610620187825014
Loss in iteration 163 : 0.4610472453928882
Loss in iteration 164 : 0.46103261490511366
Loss in iteration 165 : 0.4610181248238211
Loss in iteration 166 : 0.4610037724992227
Loss in iteration 167 : 0.46098955519136947
Loss in iteration 168 : 0.4609754704489834
Loss in iteration 169 : 0.4609615160076621
Loss in iteration 170 : 0.4609476894833741
Loss in iteration 171 : 0.4609339884826511
Loss in iteration 172 : 0.460920410837915
Loss in iteration 173 : 0.4609069544901082
Loss in iteration 174 : 0.4608936173074942
Loss in iteration 175 : 0.46088039719976076
Loss in iteration 176 : 0.46086729225125134
Loss in iteration 177 : 0.4608543006086266
Loss in iteration 178 : 0.4608414203782318
Loss in iteration 179 : 0.4608286497230507
Loss in iteration 180 : 0.46081598693096
Loss in iteration 181 : 0.46080343032466603
Loss in iteration 182 : 0.4607909782138604
Loss in iteration 183 : 0.4607786289721547
Loss in iteration 184 : 0.46076638106528445
Loss in iteration 185 : 0.4607542329826526
Loss in iteration 186 : 0.4607421832192841
Loss in iteration 187 : 0.4607302303302734
Loss in iteration 188 : 0.46071837293575996
Loss in iteration 189 : 0.4607066096730251
Loss in iteration 190 : 0.4606949391954847
Loss in iteration 191 : 0.460683360209243
Loss in iteration 192 : 0.4606718714672154
Loss in iteration 193 : 0.4606604717383147
Loss in iteration 194 : 0.46064915981439136
Loss in iteration 195 : 0.4606379345326402
Loss in iteration 196 : 0.46062679476520757
Loss in iteration 197 : 0.460615739400256
Loss in iteration 198 : 0.46060476735064637
Loss in iteration 199 : 0.46059387756600095
Loss in iteration 200 : 0.4605830690221751
Loss in iteration 201 : 0.4605723407112982
Loss in iteration 202 : 0.4605616916502349
Loss in iteration 203 : 0.46055112088610306
Loss in iteration 204 : 0.46054062748732166
Loss in iteration 205 : 0.46053021053899124
Loss in iteration 206 : 0.46051986914928034
Loss in iteration 207 : 0.46050960245078626
Loss in iteration 208 : 0.46049940959377045
Loss in iteration 209 : 0.46048928974482783
Loss in iteration 210 : 0.46047924209132735
Loss in iteration 211 : 0.4604692658406467
Loss in iteration 212 : 0.4604593602156034
Loss in iteration 213 : 0.4604495244547499
Loss in iteration 214 : 0.46043975781500296
Loss in iteration 215 : 0.4604300595699621
Loss in iteration 216 : 0.46042042900704555
Loss in iteration 217 : 0.4604108654283646
Loss in iteration 218 : 0.46040136815194854
Loss in iteration 219 : 0.4603919365098946
Loss in iteration 220 : 0.4603825698467828
Loss in iteration 221 : 0.4603732675206252
Loss in iteration 222 : 0.46036402890322226
Loss in iteration 223 : 0.46035485337854165
Loss in iteration 224 : 0.4603457403419644
Loss in iteration 225 : 0.4603366892009973
Loss in iteration 226 : 0.4603276993750753
Loss in iteration 227 : 0.4603187702942933
Loss in iteration 228 : 0.46030990139911204
Loss in iteration 229 : 0.46030109214078646
Loss in iteration 230 : 0.4602923419809131
Loss in iteration 231 : 0.4602836503905629
Loss in iteration 232 : 0.460275016850219
Loss in iteration 233 : 0.4602664408499487
Loss in iteration 234 : 0.46025792188888326
Loss in iteration 235 : 0.46024945947464374
Loss in iteration 236 : 0.4602410531233407
Loss in iteration 237 : 0.4602327023595385
Loss in iteration 238 : 0.4602244067157794
Loss in iteration 239 : 0.4602161657322289
Loss in iteration 240 : 0.46020797895668647
Loss in iteration 241 : 0.4601998459444353
Loss in iteration 242 : 0.4601917662578557
Loss in iteration 243 : 0.46018373946619884
Loss in iteration 244 : 0.4601757651455587
Loss in iteration 245 : 0.4601678428786611
Loss in iteration 246 : 0.4601599722545671
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.78875, training accuracy 0.78875, time elapsed: 5219 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6794191582029157
Loss in iteration 3 : 0.6679759210534721
Loss in iteration 4 : 0.6522101751244955
Loss in iteration 5 : 0.6318359527769749
Loss in iteration 6 : 0.6154683887752544
Loss in iteration 7 : 0.5988251171347547
Loss in iteration 8 : 0.5794781281140806
Loss in iteration 9 : 0.5637053628914673
Loss in iteration 10 : 0.5514135409524672
Loss in iteration 11 : 0.5393220342768243
Loss in iteration 12 : 0.5290118039577294
Loss in iteration 13 : 0.5218334150381992
Loss in iteration 14 : 0.5155059349774537
Loss in iteration 15 : 0.5089725845640538
Loss in iteration 16 : 0.5037316810236386
Loss in iteration 17 : 0.5001027247323685
Loss in iteration 18 : 0.49679328646204146
Loss in iteration 19 : 0.4934899284767373
Loss in iteration 20 : 0.4910024167118533
Loss in iteration 21 : 0.4893982132785774
Loss in iteration 22 : 0.4878781221858403
Loss in iteration 23 : 0.4861818503327735
Loss in iteration 24 : 0.4847758612889706
Loss in iteration 25 : 0.48381656940781487
Loss in iteration 26 : 0.4829005555725884
Loss in iteration 27 : 0.4818115368942683
Loss in iteration 28 : 0.4808095398588455
Loss in iteration 29 : 0.48009653176005374
Loss in iteration 30 : 0.47949403515552913
Loss in iteration 31 : 0.47879628457013834
Loss in iteration 32 : 0.47808557019074394
Loss in iteration 33 : 0.4775179704628084
Loss in iteration 34 : 0.4770376278208498
Loss in iteration 35 : 0.47649033584498396
Loss in iteration 36 : 0.47587561921325655
Loss in iteration 37 : 0.47531279232743734
Loss in iteration 38 : 0.47483104543502824
Loss in iteration 39 : 0.4743458027826637
Loss in iteration 40 : 0.4738187739197243
Loss in iteration 41 : 0.4733118603781361
Loss in iteration 42 : 0.4728707675994153
Loss in iteration 43 : 0.4724554571754459
Loss in iteration 44 : 0.4720180895449119
Loss in iteration 45 : 0.4715776600609921
Loss in iteration 46 : 0.4711754232854885
Loss in iteration 47 : 0.4708048413843821
Loss in iteration 48 : 0.47043141350989764
Loss in iteration 49 : 0.4700536157443877
Loss in iteration 50 : 0.46970039357950005
Loss in iteration 51 : 0.46938094045617446
Loss in iteration 52 : 0.46907497896110895
Loss in iteration 53 : 0.46877074734358826
Loss in iteration 54 : 0.4684811841304708
Loss in iteration 55 : 0.4682169790023547
Loss in iteration 56 : 0.4679679510800205
Loss in iteration 57 : 0.46772138632989263
Loss in iteration 58 : 0.46748150456839394
Loss in iteration 59 : 0.46725874229069975
Loss in iteration 60 : 0.4670514568263211
Loss in iteration 61 : 0.4668505994266031
Loss in iteration 62 : 0.46665542587687375
Loss in iteration 63 : 0.4664727934904068
Loss in iteration 64 : 0.4663040602071818
Loss in iteration 65 : 0.4661429621595861
Loss in iteration 66 : 0.4659860581724628
Loss in iteration 67 : 0.4658367599701198
Loss in iteration 68 : 0.4656976372831973
Loss in iteration 69 : 0.4655655478643671
Loss in iteration 70 : 0.4654369876322661
Loss in iteration 71 : 0.4653132046617027
Loss in iteration 72 : 0.4651968329998009
Loss in iteration 73 : 0.4650869448780209
Loss in iteration 74 : 0.46498073429555664
Loss in iteration 75 : 0.46487788193274787
Loss in iteration 76 : 0.46478006571803154
Loss in iteration 77 : 0.46468731226098087
Loss in iteration 78 : 0.46459769039510773
Loss in iteration 79 : 0.4645102647933315
Loss in iteration 80 : 0.46442597443958245
Loss in iteration 81 : 0.46434541847403715
Loss in iteration 82 : 0.4642676847874378
Loss in iteration 83 : 0.4641918821937109
Loss in iteration 84 : 0.46411836235688986
Loss in iteration 85 : 0.4640477017187505
Loss in iteration 86 : 0.46397951002571153
Loss in iteration 87 : 0.4639130250260083
Loss in iteration 88 : 0.46384819558034224
Loss in iteration 89 : 0.4637854171153458
Loss in iteration 90 : 0.4637246154199418
Loss in iteration 91 : 0.4636652920369958
Loss in iteration 92 : 0.4636072721910407
Loss in iteration 93 : 0.4635508044453461
Loss in iteration 94 : 0.4634959711384306
Loss in iteration 95 : 0.46344249382212716
Loss in iteration 96 : 0.46339016754617673
Loss in iteration 97 : 0.46333909247558946
Loss in iteration 98 : 0.46328936310265045
Loss in iteration 99 : 0.46324082591319166
Loss in iteration 100 : 0.46319329057805847
Loss in iteration 101 : 0.4631467681920234
Loss in iteration 102 : 0.4631013457574411
Loss in iteration 103 : 0.46305697079361763
Loss in iteration 104 : 0.4630135121900429
Loss in iteration 105 : 0.46297094240062286
Loss in iteration 106 : 0.4629293177800568
Loss in iteration 107 : 0.46288862550490145
Loss in iteration 108 : 0.46284877160082877
Loss in iteration 109 : 0.46280970334689703
Loss in iteration 110 : 0.46277144169046697
Loss in iteration 111 : 0.4627339887617315
Loss in iteration 112 : 0.46269728597480514
Loss in iteration 113 : 0.4626612819510854
Loss in iteration 114 : 0.4626259783835698
Loss in iteration 115 : 0.4625913826724636
Loss in iteration 116 : 0.46255746198755937
Loss in iteration 117 : 0.46252417318468986
Loss in iteration 118 : 0.46249150475039047
Loss in iteration 119 : 0.4624594593123051
Loss in iteration 120 : 0.4624280163541335
Loss in iteration 121 : 0.46239714019333916
Loss in iteration 122 : 0.4623668121182511
Loss in iteration 123 : 0.4623370298452203
Loss in iteration 124 : 0.46230778146785195
Loss in iteration 125 : 0.46227904139957454
Loss in iteration 126 : 0.46225079072799896
Loss in iteration 127 : 0.46222302375588054
Loss in iteration 128 : 0.46219573224430804
Loss in iteration 129 : 0.46216889716260867
Loss in iteration 130 : 0.4621425000736243
Loss in iteration 131 : 0.4621165319208867
Loss in iteration 132 : 0.46209098539817944
Loss in iteration 133 : 0.46206584656349425
Loss in iteration 134 : 0.46204109964851514
Loss in iteration 135 : 0.4620167347304478
Loss in iteration 136 : 0.4619927451363113
Loss in iteration 137 : 0.4619691207243543
Loss in iteration 138 : 0.46194584877205863
Loss in iteration 139 : 0.4619229194969237
Loss in iteration 140 : 0.46190032620109606
Loss in iteration 141 : 0.4618780607752177
Loss in iteration 142 : 0.4618561128102123
Loss in iteration 143 : 0.46183447309738956
Loss in iteration 144 : 0.461813134949394
Loss in iteration 145 : 0.4617920916095926
Loss in iteration 146 : 0.46177133477116306
Loss in iteration 147 : 0.4617508564173448
Loss in iteration 148 : 0.4617306502743749
Loss in iteration 149 : 0.4617107105037242
Loss in iteration 150 : 0.4616910302781195
Loss in iteration 151 : 0.46167160259389345
Loss in iteration 152 : 0.46165242154242525
Loss in iteration 153 : 0.46163348184530634
Loss in iteration 154 : 0.46161477774883874
Loss in iteration 155 : 0.461596303229295
Loss in iteration 156 : 0.461578052918444
Loss in iteration 157 : 0.4615600220695421
Loss in iteration 158 : 0.46154220578607175
Loss in iteration 159 : 0.46152459891653846
Loss in iteration 160 : 0.4615071966512493
Loss in iteration 161 : 0.4614899946845983
Loss in iteration 162 : 0.461472988740433
Loss in iteration 163 : 0.4614561743559556
Loss in iteration 164 : 0.46143954722669583
Loss in iteration 165 : 0.46142310343010784
Loss in iteration 166 : 0.46140683917704245
Loss in iteration 167 : 0.46139075059031454
Loss in iteration 168 : 0.4613748338682128
Loss in iteration 169 : 0.46135908548076865
Loss in iteration 170 : 0.461343502057437
Loss in iteration 171 : 0.4613280802029632
Loss in iteration 172 : 0.46131281655360273
Loss in iteration 173 : 0.46129770792751024
Loss in iteration 174 : 0.46128275129571056
Loss in iteration 175 : 0.46126794365284046
Loss in iteration 176 : 0.4612532820194495
Loss in iteration 177 : 0.4612387635431643
Loss in iteration 178 : 0.46122438550633216
Loss in iteration 179 : 0.46121014524073733
Loss in iteration 180 : 0.46119604010560733
Loss in iteration 181 : 0.46118206754923713
Loss in iteration 182 : 0.461168225130178
Loss in iteration 183 : 0.4611545104654465
Loss in iteration 184 : 0.46114092120258315
Loss in iteration 185 : 0.4611274550546491
Loss in iteration 186 : 0.461114109824295
Loss in iteration 187 : 0.46110088337544486
Loss in iteration 188 : 0.4610877736073792
Loss in iteration 189 : 0.46107477847179745
Loss in iteration 190 : 0.4610618959931879
Loss in iteration 191 : 0.4610491242543501
Loss in iteration 192 : 0.46103646137480303
Loss in iteration 193 : 0.46102390551690475
Loss in iteration 194 : 0.46101145490147144
Loss in iteration 195 : 0.46099910780219694
Loss in iteration 196 : 0.46098686252987386
Loss in iteration 197 : 0.4609747174327307
Loss in iteration 198 : 0.4609626709071656
Loss in iteration 199 : 0.46095072139661697
Loss in iteration 200 : 0.4609388673805438
Loss in iteration 201 : 0.4609271073719188
Loss in iteration 202 : 0.4609154399238702
Loss in iteration 203 : 0.4609038636304898
Loss in iteration 204 : 0.46089237711959197
Loss in iteration 205 : 0.46088097904928665
Loss in iteration 206 : 0.4608696681116802
Loss in iteration 207 : 0.4608584430343673
Loss in iteration 208 : 0.4608473025759678
Loss in iteration 209 : 0.4608362455228546
Loss in iteration 210 : 0.4608252706909126
Loss in iteration 211 : 0.4608143769269337
Loss in iteration 212 : 0.46080356310585696
Loss in iteration 213 : 0.46079282812787986
Loss in iteration 214 : 0.4607821709189536
Loss in iteration 215 : 0.46077159043182236
Loss in iteration 216 : 0.46076108564443125
Loss in iteration 217 : 0.4607506555576648
Loss in iteration 218 : 0.46074029919520554
Loss in iteration 219 : 0.46073001560420435
Loss in iteration 220 : 0.4607198038543378
Loss in iteration 221 : 0.46070966303608746
Loss in iteration 222 : 0.4606995922602165
Loss in iteration 223 : 0.46068959065809567
Loss in iteration 224 : 0.4606796573811192
Loss in iteration 225 : 0.46066979159942145
Loss in iteration 226 : 0.46065999250126993
Loss in iteration 227 : 0.4606502592931239
Loss in iteration 228 : 0.46064059119925493
Loss in iteration 229 : 0.4606309874607908
Loss in iteration 230 : 0.4606214473351214
Loss in iteration 231 : 0.4606119700957926
Loss in iteration 232 : 0.4606025550321899
Loss in iteration 233 : 0.46059320144883875
Loss in iteration 234 : 0.4605839086648518
Loss in iteration 235 : 0.4605746760137234
Loss in iteration 236 : 0.46056550284306413
Loss in iteration 237 : 0.46055638851405967
Loss in iteration 238 : 0.46054733240097423
Loss in iteration 239 : 0.46053833389092913
Loss in iteration 240 : 0.46052939238363955
Loss in iteration 241 : 0.46052050729099053
Testing accuracy  of updater 6 on alg 0 with rate 0.14 = 0.78825, training accuracy 0.78825, time elapsed: 5060 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6829122888730501
Loss in iteration 3 : 0.6726486925928166
Loss in iteration 4 : 0.6627551273923818
Loss in iteration 5 : 0.648705699064385
Loss in iteration 6 : 0.6328093752398254
Loss in iteration 7 : 0.6193451380443712
Loss in iteration 8 : 0.6068315278286184
Loss in iteration 9 : 0.5925310852517189
Loss in iteration 10 : 0.5782003966362492
Loss in iteration 11 : 0.5664555704928443
Loss in iteration 12 : 0.5566933661948343
Loss in iteration 13 : 0.547303400823195
Loss in iteration 14 : 0.5385163860205425
Loss in iteration 15 : 0.5314018968694315
Loss in iteration 16 : 0.5257893765328229
Loss in iteration 17 : 0.5205787965216738
Loss in iteration 18 : 0.5153875996807078
Loss in iteration 19 : 0.5107857563259818
Loss in iteration 20 : 0.507161621543274
Loss in iteration 21 : 0.5041645758617546
Loss in iteration 22 : 0.5013024269549005
Loss in iteration 23 : 0.49856881689955207
Loss in iteration 24 : 0.4962726421045097
Loss in iteration 25 : 0.494497309988904
Loss in iteration 26 : 0.4929784051802898
Loss in iteration 27 : 0.49146035438096625
Loss in iteration 28 : 0.4899685178709149
Loss in iteration 29 : 0.4886749509733833
Loss in iteration 30 : 0.48762097716720415
Loss in iteration 31 : 0.4866718592414856
Loss in iteration 32 : 0.4857033803982873
Loss in iteration 33 : 0.4847388600592742
Loss in iteration 34 : 0.4838810193403438
Loss in iteration 35 : 0.48316433797914327
Loss in iteration 36 : 0.4825207950231977
Loss in iteration 37 : 0.48187501151749756
Loss in iteration 38 : 0.48122748496001805
Loss in iteration 39 : 0.4806296528327325
Loss in iteration 40 : 0.48010399885262156
Loss in iteration 41 : 0.4796160433691856
Loss in iteration 42 : 0.47912083743449124
Loss in iteration 43 : 0.47861399129994736
Loss in iteration 44 : 0.4781263626602161
Loss in iteration 45 : 0.47767969552015604
Loss in iteration 46 : 0.4772624546223856
Loss in iteration 47 : 0.476849640460255
Loss in iteration 48 : 0.4764343416748591
Loss in iteration 49 : 0.4760316429049562
Loss in iteration 50 : 0.4756558162741613
Loss in iteration 51 : 0.47530281094704047
Loss in iteration 52 : 0.4749574891000105
Loss in iteration 53 : 0.4746123547087194
Loss in iteration 54 : 0.47427380062602653
Loss in iteration 55 : 0.4739511448577782
Loss in iteration 56 : 0.4736446207037264
Loss in iteration 57 : 0.47334649289412983
Loss in iteration 58 : 0.47305139972746324
Loss in iteration 59 : 0.4727620180255932
Loss in iteration 60 : 0.4724843037869116
Loss in iteration 61 : 0.4722198341969954
Loss in iteration 62 : 0.471964686378204
Loss in iteration 63 : 0.4717148402865829
Loss in iteration 64 : 0.4714705262506166
Loss in iteration 65 : 0.47123467976467737
Loss in iteration 66 : 0.47100849647454107
Loss in iteration 67 : 0.4707898912653036
Loss in iteration 68 : 0.47057614871385467
Loss in iteration 69 : 0.47036688249718533
Loss in iteration 70 : 0.4701637757858044
Loss in iteration 71 : 0.46996800219138174
Loss in iteration 72 : 0.46977877032078813
Loss in iteration 73 : 0.46959446516484865
Loss in iteration 74 : 0.4694145378938102
Loss in iteration 75 : 0.46923977146310714
Loss in iteration 76 : 0.46907090861611106
Loss in iteration 77 : 0.46890756263481354
Loss in iteration 78 : 0.46874864860087256
Loss in iteration 79 : 0.46859355810063597
Loss in iteration 80 : 0.4684425659227688
Loss in iteration 81 : 0.4682961344016732
Loss in iteration 82 : 0.4681541373634445
Loss in iteration 83 : 0.46801593275248515
Loss in iteration 84 : 0.46788104433973216
Loss in iteration 85 : 0.4677495390202851
Loss in iteration 86 : 0.4676217086676375
Loss in iteration 87 : 0.4674975522665178
Loss in iteration 88 : 0.46737669807192883
Loss in iteration 89 : 0.4672587738306886
Loss in iteration 90 : 0.46714370818916445
Loss in iteration 91 : 0.46703162370583834
Loss in iteration 92 : 0.46692252410769913
Loss in iteration 93 : 0.46681618252532897
Loss in iteration 94 : 0.4667123266477414
Loss in iteration 95 : 0.4666108506938717
Loss in iteration 96 : 0.46651180197058
Loss in iteration 97 : 0.4664151993926372
Loss in iteration 98 : 0.46632092615468845
Loss in iteration 99 : 0.46622880613155737
Loss in iteration 100 : 0.46613873832870734
Loss in iteration 101 : 0.4660507201963669
Loss in iteration 102 : 0.46596475329860165
Loss in iteration 103 : 0.46588076407797835
Loss in iteration 104 : 0.4657986296759159
Loss in iteration 105 : 0.4657182587183243
Loss in iteration 106 : 0.46563962225441435
Loss in iteration 107 : 0.465562708482639
Loss in iteration 108 : 0.4654874691639106
Loss in iteration 109 : 0.4654138222045361
Loss in iteration 110 : 0.4653416965014137
Loss in iteration 111 : 0.46527105810932384
Loss in iteration 112 : 0.46520189033144743
Loss in iteration 113 : 0.4651341596520255
Loss in iteration 114 : 0.4650678099196025
Loss in iteration 115 : 0.4650027858783804
Loss in iteration 116 : 0.46493905250876727
Loss in iteration 117 : 0.4648765881141612
Loss in iteration 118 : 0.4648153643341152
Loss in iteration 119 : 0.46475533922287005
Loss in iteration 120 : 0.46469646917554747
Loss in iteration 121 : 0.46463872213603385
Loss in iteration 122 : 0.4645820762828472
Loss in iteration 123 : 0.4645265084397653
Loss in iteration 124 : 0.4644719877078143
Loss in iteration 125 : 0.4644184806834829
Loss in iteration 126 : 0.4643659597556025
Loss in iteration 127 : 0.4643144039851718
Loss in iteration 128 : 0.464263792856284
Loss in iteration 129 : 0.4642141015813449
Loss in iteration 130 : 0.46416530318536375
Loss in iteration 131 : 0.46411737361033306
Loss in iteration 132 : 0.4640702932310536
Loss in iteration 133 : 0.46402404366618544
Loss in iteration 134 : 0.46397860453510853
Loss in iteration 135 : 0.46393395398601794
Loss in iteration 136 : 0.46389007162888685
Loss in iteration 137 : 0.4638469399164407
Loss in iteration 138 : 0.46380454259931414
Loss in iteration 139 : 0.4637628626025521
Loss in iteration 140 : 0.4637218819094744
Loss in iteration 141 : 0.4636815831718975
Loss in iteration 142 : 0.4636419507768878
Loss in iteration 143 : 0.4636029701811467
Loss in iteration 144 : 0.46356462661256326
Loss in iteration 145 : 0.4635269047895374
Loss in iteration 146 : 0.4634897897930824
Loss in iteration 147 : 0.46345326783152346
Loss in iteration 148 : 0.46341732599843566
Loss in iteration 149 : 0.4633819514858794
Loss in iteration 150 : 0.4633471312733631
Loss in iteration 151 : 0.4633128525551956
Loss in iteration 152 : 0.4632791032412673
Loss in iteration 153 : 0.46324587190357364
Loss in iteration 154 : 0.4632131473188929
Loss in iteration 155 : 0.4631809182131792
Loss in iteration 156 : 0.4631491734632647
Loss in iteration 157 : 0.46311790242152573
Loss in iteration 158 : 0.46308709494566985
Loss in iteration 159 : 0.4630567411444014
Loss in iteration 160 : 0.4630268311858972
Loss in iteration 161 : 0.4629973553770911
Loss in iteration 162 : 0.46296830436109265
Loss in iteration 163 : 0.4629396691678452
Loss in iteration 164 : 0.4629114410766293
Loss in iteration 165 : 0.4628836114801061
Loss in iteration 166 : 0.4628561719021357
Loss in iteration 167 : 0.4628291141104858
Loss in iteration 168 : 0.4628024301642868
Loss in iteration 169 : 0.46277611234463145
Loss in iteration 170 : 0.46275015306622097
Loss in iteration 171 : 0.46272454487270726
Loss in iteration 172 : 0.4626992804998227
Loss in iteration 173 : 0.46267435291290837
Loss in iteration 174 : 0.462649755273647
Loss in iteration 175 : 0.462625480883586
Loss in iteration 176 : 0.4626015231705121
Loss in iteration 177 : 0.462577875719982
Loss in iteration 178 : 0.46255453230003685
Loss in iteration 179 : 0.462531486845436
Loss in iteration 180 : 0.46250873342279497
Loss in iteration 181 : 0.4624862662171722
Loss in iteration 182 : 0.46246407954750807
Loss in iteration 183 : 0.4624421678825588
Loss in iteration 184 : 0.4624205258338629
Loss in iteration 185 : 0.4623991481343588
Loss in iteration 186 : 0.4623780296269952
Loss in iteration 187 : 0.4623571652711722
Loss in iteration 188 : 0.46233655015207864
Loss in iteration 189 : 0.4623161794773917
Loss in iteration 190 : 0.46229604856400636
Loss in iteration 191 : 0.46227615282905604
Loss in iteration 192 : 0.4622564877917535
Loss in iteration 193 : 0.4622370490784265
Loss in iteration 194 : 0.4622178324207862
Loss in iteration 195 : 0.46219883364756925
Loss in iteration 196 : 0.4621800486777377
Loss in iteration 197 : 0.4621614735201813
Loss in iteration 198 : 0.46214310427616306
Loss in iteration 199 : 0.46212493713832936
Loss in iteration 200 : 0.46210696838536863
Loss in iteration 201 : 0.46208919437699375
Loss in iteration 202 : 0.4620716115526017
Loss in iteration 203 : 0.46205421643207567
Loss in iteration 204 : 0.4620370056148729
Loss in iteration 205 : 0.46201997577644305
Loss in iteration 206 : 0.46200312366446855
Loss in iteration 207 : 0.4619864460972045
Loss in iteration 208 : 0.4619699399633622
Loss in iteration 209 : 0.46195360222120574
Loss in iteration 210 : 0.461937429896096
Loss in iteration 211 : 0.46192142007768805
Loss in iteration 212 : 0.4619055699183223
Loss in iteration 213 : 0.4618898766324171
Loss in iteration 214 : 0.4618743374955571
Loss in iteration 215 : 0.46185894984263964
Loss in iteration 216 : 0.46184371106574423
Loss in iteration 217 : 0.46182861861262986
Loss in iteration 218 : 0.46181366998590256
Loss in iteration 219 : 0.4617988627420693
Loss in iteration 220 : 0.4617841944900928
Loss in iteration 221 : 0.4617696628897148
Loss in iteration 222 : 0.4617552656501698
Loss in iteration 223 : 0.4617410005292752
Loss in iteration 224 : 0.4617268653325534
Loss in iteration 225 : 0.4617128579120283
Loss in iteration 226 : 0.4616989761648906
Loss in iteration 227 : 0.46168521803234863
Loss in iteration 228 : 0.4616715814987591
Loss in iteration 229 : 0.4616580645907604
Loss in iteration 230 : 0.46164466537626914
Loss in iteration 231 : 0.46163138196337217
Loss in iteration 232 : 0.46161821249934165
Loss in iteration 233 : 0.46160515516980427
Loss in iteration 234 : 0.46159220819797436
Loss in iteration 235 : 0.46157936984376147
Loss in iteration 236 : 0.4615666384028574
Loss in iteration 237 : 0.461554012205867
Loss in iteration 238 : 0.4615414896175563
Loss in iteration 239 : 0.4615290690361331
Loss in iteration 240 : 0.4615167488924938
Loss in iteration 241 : 0.4615045276494234
Loss in iteration 242 : 0.46149240380084905
Loss in iteration 243 : 0.461480375871165
Loss in iteration 244 : 0.46146844241458024
Loss in iteration 245 : 0.4614566020144529
Loss in iteration 246 : 0.46144485328260576
Testing accuracy  of updater 6 on alg 0 with rate 0.08000000000000002 = 0.78875, training accuracy 0.78875, time elapsed: 4526 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6897408945933193
Loss in iteration 3 : 0.6844837726488197
Loss in iteration 4 : 0.6789128160288329
Loss in iteration 5 : 0.6737773452770327
Loss in iteration 6 : 0.6689058206884408
Loss in iteration 7 : 0.6636899609011983
Loss in iteration 8 : 0.6576915133872765
Loss in iteration 9 : 0.6509517239660891
Loss in iteration 10 : 0.6439077461674565
Loss in iteration 11 : 0.6370754420826398
Loss in iteration 12 : 0.6307436839305585
Loss in iteration 13 : 0.6248637315329562
Loss in iteration 14 : 0.6191657023152723
Loss in iteration 15 : 0.6133864271284154
Loss in iteration 16 : 0.607444199663689
Loss in iteration 17 : 0.6014641929330115
Loss in iteration 18 : 0.5956734359640243
Loss in iteration 19 : 0.5902592322719812
Loss in iteration 20 : 0.5852826603159944
Loss in iteration 21 : 0.580682143229225
Loss in iteration 22 : 0.576341042400187
Loss in iteration 23 : 0.5721650618453974
Loss in iteration 24 : 0.5681254580155799
Loss in iteration 25 : 0.5642545160059098
Loss in iteration 26 : 0.5606081893208269
Loss in iteration 27 : 0.557223849313631
Loss in iteration 28 : 0.5540968315789633
Loss in iteration 29 : 0.5511842783103942
Loss in iteration 30 : 0.5484286140924384
Loss in iteration 31 : 0.5457843087546482
Loss in iteration 32 : 0.543233353742751
Loss in iteration 33 : 0.5407839189877519
Loss in iteration 34 : 0.5384565450708986
Loss in iteration 35 : 0.536267570070946
Loss in iteration 36 : 0.5342186229836083
Loss in iteration 37 : 0.5322959206523944
Loss in iteration 38 : 0.5304774072671964
Loss in iteration 39 : 0.5287424392956737
Loss in iteration 40 : 0.5270787548540823
Loss in iteration 41 : 0.5254839958849244
Loss in iteration 42 : 0.523962260846452
Loss in iteration 43 : 0.5225184022188676
Loss in iteration 44 : 0.5211532488238579
Loss in iteration 45 : 0.51986181679902
Loss in iteration 46 : 0.5186347361074248
Loss in iteration 47 : 0.5174615659477999
Loss in iteration 48 : 0.5163340652506073
Loss in iteration 49 : 0.5152479076796835
Loss in iteration 50 : 0.5142023679359529
Loss in iteration 51 : 0.513198542154702
Loss in iteration 52 : 0.5122372186967519
Loss in iteration 53 : 0.5113174364663243
Loss in iteration 54 : 0.5104362320540772
Loss in iteration 55 : 0.5095894303735196
Loss in iteration 56 : 0.5087728937906904
Loss in iteration 57 : 0.50798356396834
Loss in iteration 58 : 0.5072198653496887
Loss in iteration 59 : 0.5064814155382157
Loss in iteration 60 : 0.5057683082861609
Loss in iteration 61 : 0.5050803700008429
Loss in iteration 62 : 0.5044167204540142
Loss in iteration 63 : 0.5037757667744296
Loss in iteration 64 : 0.5031555428331479
Loss in iteration 65 : 0.5025541724331816
Loss in iteration 66 : 0.5019702259520609
Loss in iteration 67 : 0.5014028343447555
Loss in iteration 68 : 0.5008515589574577
Loss in iteration 69 : 0.500316122996757
Loss in iteration 70 : 0.4997961493183687
Loss in iteration 71 : 0.4992910167697877
Loss in iteration 72 : 0.4987998726712791
Loss in iteration 73 : 0.4983217634400189
Loss in iteration 74 : 0.4978558018228977
Loss in iteration 75 : 0.4974012898607467
Loss in iteration 76 : 0.4969577521147939
Loss in iteration 77 : 0.49652488145403767
Loss in iteration 78 : 0.49610243680924787
Loss in iteration 79 : 0.4956901450430431
Loss in iteration 80 : 0.49528764697417726
Loss in iteration 81 : 0.49489450082957004
Loss in iteration 82 : 0.4945102293517635
Loss in iteration 83 : 0.4941343810437305
Loss in iteration 84 : 0.49376657601630913
Loss in iteration 85 : 0.4934065194519606
Loss in iteration 86 : 0.4930539829388349
Loss in iteration 87 : 0.4927087675850692
Loss in iteration 88 : 0.49237066781930416
Loss in iteration 89 : 0.4920394507513089
Loss in iteration 90 : 0.49171485643984647
Loss in iteration 91 : 0.4913966145347957
Loss in iteration 92 : 0.4910844667716102
Loss in iteration 93 : 0.49077818443980264
Loss in iteration 94 : 0.49047757423904703
Loss in iteration 95 : 0.49018247213776656
Loss in iteration 96 : 0.4898927299725317
Loss in iteration 97 : 0.4896082016392737
Loss in iteration 98 : 0.48932873454627246
Loss in iteration 99 : 0.4890541686770768
Loss in iteration 100 : 0.4887843419668113
Loss in iteration 101 : 0.48851909831757756
Loss in iteration 102 : 0.48825829421014627
Loss in iteration 103 : 0.48800180126431614
Loss in iteration 104 : 0.4877495043253972
Loss in iteration 105 : 0.4875012966147399
Loss in iteration 106 : 0.487257074412359
Loss in iteration 107 : 0.4870167334565106
Loss in iteration 108 : 0.48678016810741065
Loss in iteration 109 : 0.4865472729742022
Loss in iteration 110 : 0.48631794574817766
Loss in iteration 111 : 0.4860920897410895
Loss in iteration 112 : 0.4858696150550256
Loss in iteration 113 : 0.4856504380986689
Loss in iteration 114 : 0.4854344799148896
Loss in iteration 115 : 0.48522166419232937
Loss in iteration 116 : 0.4850119157974935
Loss in iteration 117 : 0.48480516028616194
Loss in iteration 118 : 0.48460132435798803
Loss in iteration 119 : 0.48440033683439526
Loss in iteration 120 : 0.4842021296051067
Loss in iteration 121 : 0.4840066381101829
Loss in iteration 122 : 0.4838138012021013
Loss in iteration 123 : 0.48362356052084726
Loss in iteration 124 : 0.4834358596925239
Loss in iteration 125 : 0.4832506436768582
Loss in iteration 126 : 0.4830678584665164
Loss in iteration 127 : 0.4828874511584739
Loss in iteration 128 : 0.48270937026222066
Loss in iteration 129 : 0.48253356603987446
Loss in iteration 130 : 0.48235999070176977
Loss in iteration 131 : 0.4821885983768103
Loss in iteration 132 : 0.48201934488740616
Loss in iteration 133 : 0.4818521874355672
Loss in iteration 134 : 0.4816870843246736
Loss in iteration 135 : 0.4815239948042821
Loss in iteration 136 : 0.48136287905908876
Loss in iteration 137 : 0.4812036983006644
Loss in iteration 138 : 0.48104641488680194
Loss in iteration 139 : 0.48089099239690736
Loss in iteration 140 : 0.4807373956240273
Loss in iteration 141 : 0.48058559048642935
Loss in iteration 142 : 0.48043554389471615
Loss in iteration 143 : 0.48028722362250315
Loss in iteration 144 : 0.48014059821878263
Loss in iteration 145 : 0.4799956369763738
Loss in iteration 146 : 0.4798523099458241
Loss in iteration 147 : 0.47971058796805544
Loss in iteration 148 : 0.4795704426969993
Loss in iteration 149 : 0.4794318465935292
Loss in iteration 150 : 0.4792947728877409
Loss in iteration 151 : 0.4791591955206742
Loss in iteration 152 : 0.4790250890832558
Loss in iteration 153 : 0.47889242876860155
Loss in iteration 154 : 0.4787611903456243
Loss in iteration 155 : 0.47863135015220554
Loss in iteration 156 : 0.47850288509884875
Loss in iteration 157 : 0.4783757726715593
Loss in iteration 158 : 0.4782499909254033
Loss in iteration 159 : 0.4781255184659059
Loss in iteration 160 : 0.47800233442123297
Loss in iteration 161 : 0.47788041841146184
Loss in iteration 162 : 0.4777597505215851
Loss in iteration 163 : 0.47764031128228623
Loss in iteration 164 : 0.477522081658856
Loss in iteration 165 : 0.4774050430455015
Loss in iteration 166 : 0.47728917726079423
Loss in iteration 167 : 0.4771744665406185
Loss in iteration 168 : 0.4770608935268475
Loss in iteration 169 : 0.476948441252259
Loss in iteration 170 : 0.47683709312379724
Loss in iteration 171 : 0.4767268329067541
Loss in iteration 172 : 0.4766176447117382
Loss in iteration 173 : 0.47650951298495536
Loss in iteration 174 : 0.4764024225010661
Loss in iteration 175 : 0.47629635835711
Loss in iteration 176 : 0.47619130596595155
Loss in iteration 177 : 0.47608725104838334
Loss in iteration 178 : 0.4759841796237786
Loss in iteration 179 : 0.47588207799997484
Loss in iteration 180 : 0.47578093276332756
Loss in iteration 181 : 0.47568073076975825
Loss in iteration 182 : 0.4755814591371388
Loss in iteration 183 : 0.47548310523888665
Loss in iteration 184 : 0.4753856566982554
Loss in iteration 185 : 0.47528910138271574
Loss in iteration 186 : 0.47519342739803805
Loss in iteration 187 : 0.47509862308190803
Loss in iteration 188 : 0.47500467699729143
Loss in iteration 189 : 0.47491157792583855
Loss in iteration 190 : 0.47481931486169554
Loss in iteration 191 : 0.4747278770058628
Loss in iteration 192 : 0.474637253761129
Loss in iteration 193 : 0.4745474347273729
Loss in iteration 194 : 0.4744584096970484
Loss in iteration 195 : 0.47437016865063547
Loss in iteration 196 : 0.4742827017519886
Loss in iteration 197 : 0.4741959993436086
Loss in iteration 198 : 0.47411005194197064
Loss in iteration 199 : 0.4740248502330037
Loss in iteration 200 : 0.47394038506784764
Loss in iteration 201 : 0.47385664745888106
Loss in iteration 202 : 0.47377362857597444
Loss in iteration 203 : 0.4736913197428854
Loss in iteration 204 : 0.4736097124337338
Loss in iteration 205 : 0.4735287982694843
Loss in iteration 206 : 0.47344856901445576
Loss in iteration 207 : 0.47336901657289376
Loss in iteration 208 : 0.47329013298562916
Loss in iteration 209 : 0.47321191042688315
Loss in iteration 210 : 0.47313434120122333
Loss in iteration 211 : 0.47305741774064464
Loss in iteration 212 : 0.4729811326017485
Loss in iteration 213 : 0.4729054784630145
Loss in iteration 214 : 0.4728304481220773
Loss in iteration 215 : 0.4727560344930817
Loss in iteration 216 : 0.4726822306040631
Loss in iteration 217 : 0.4726090295944061
Loss in iteration 218 : 0.4725364247123792
Loss in iteration 219 : 0.47246440931275496
Loss in iteration 220 : 0.4723929768545185
Loss in iteration 221 : 0.47232212089865494
Loss in iteration 222 : 0.4722518351059786
Loss in iteration 223 : 0.4721821132350351
Loss in iteration 224 : 0.47211294914002655
Loss in iteration 225 : 0.4720443367688019
Loss in iteration 226 : 0.47197627016087856
Loss in iteration 227 : 0.47190874344553047
Loss in iteration 228 : 0.47184175083993884
Loss in iteration 229 : 0.47177528664738316
Loss in iteration 230 : 0.47170934525549396
Loss in iteration 231 : 0.47164392113455667
Loss in iteration 232 : 0.47157900883584297
Loss in iteration 233 : 0.4715146029899873
Loss in iteration 234 : 0.47145069830540504
Loss in iteration 235 : 0.47138728956672604
Loss in iteration 236 : 0.4713243716332977
Loss in iteration 237 : 0.47126193943771083
Loss in iteration 238 : 0.47119998798436824
Loss in iteration 239 : 0.4711385123480836
Loss in iteration 240 : 0.4710775076727374
Loss in iteration 241 : 0.4710169691699414
Loss in iteration 242 : 0.4709568921177463
Loss in iteration 243 : 0.4708972718593859
Loss in iteration 244 : 0.4708381038020324
Loss in iteration 245 : 0.4707793834155981
Loss in iteration 246 : 0.4707211062315658
Loss in iteration 247 : 0.4706632678418312
Loss in iteration 248 : 0.4706058638976001
Loss in iteration 249 : 0.47054889010829015
Loss in iteration 250 : 0.47049234224047054
Loss in iteration 251 : 0.47043621611682274
Loss in iteration 252 : 0.47038050761511796
Loss in iteration 253 : 0.47032521266723143
Loss in iteration 254 : 0.47027032725816714
Loss in iteration 255 : 0.47021584742511263
Loss in iteration 256 : 0.47016176925651154
Loss in iteration 257 : 0.4701080888911576
Loss in iteration 258 : 0.4700548025173156
Loss in iteration 259 : 0.4700019063718553
Loss in iteration 260 : 0.4699493967394093
Loss in iteration 261 : 0.46989726995154024
Loss in iteration 262 : 0.46984552238595184
Loss in iteration 263 : 0.4697941504656747
Loss in iteration 264 : 0.46974315065831806
Loss in iteration 265 : 0.46969251947529916
Loss in iteration 266 : 0.4696422534711161
Loss in iteration 267 : 0.4695923492426216
Loss in iteration 268 : 0.4695428034283182
Loss in iteration 269 : 0.46949361270766576
Loss in iteration 270 : 0.4694447738004091
Loss in iteration 271 : 0.4693962834659159
Loss in iteration 272 : 0.46934813850252466
Loss in iteration 273 : 0.46930033574691904
Loss in iteration 274 : 0.469252872073498
Loss in iteration 275 : 0.4692057443937771
Loss in iteration 276 : 0.46915894965579136
Loss in iteration 277 : 0.46911248484350804
Loss in iteration 278 : 0.46906634697626587
Loss in iteration 279 : 0.46902053310820985
Loss in iteration 280 : 0.46897504032774806
Loss in iteration 281 : 0.4689298657570141
Loss in iteration 282 : 0.46888500655134524
Loss in iteration 283 : 0.4688404598987672
Loss in iteration 284 : 0.46879622301948876
Loss in iteration 285 : 0.4687522931654131
Loss in iteration 286 : 0.4687086676196467
Loss in iteration 287 : 0.4686653436960374
Loss in iteration 288 : 0.46862231873869564
Loss in iteration 289 : 0.4685795901215546
Loss in iteration 290 : 0.4685371552479082
Loss in iteration 291 : 0.46849501154998635
Loss in iteration 292 : 0.468453156488519
Loss in iteration 293 : 0.4684115875523154
Loss in iteration 294 : 0.46837030225785875
Loss in iteration 295 : 0.468329298148887
Loss in iteration 296 : 0.46828857279601566
Loss in iteration 297 : 0.46824812379632785
Loss in iteration 298 : 0.46820794877300864
Loss in iteration 299 : 0.4681680453749611
Loss in iteration 300 : 0.46812841127644345
Loss in iteration 301 : 0.46808904417670344
Loss in iteration 302 : 0.4680499417996292
Loss in iteration 303 : 0.46801110189340056
Loss in iteration 304 : 0.46797252223014435
Loss in iteration 305 : 0.46793420060560376
Loss in iteration 306 : 0.467896134838807
Loss in iteration 307 : 0.46785832277174905
Loss in iteration 308 : 0.4678207622690677
Loss in iteration 309 : 0.4677834512177401
Loss in iteration 310 : 0.46774638752677083
Loss in iteration 311 : 0.46770956912689327
Loss in iteration 312 : 0.4676729939702786
Loss in iteration 313 : 0.4676366600302449
Loss in iteration 314 : 0.4676005653009709
Loss in iteration 315 : 0.4675647077972145
Loss in iteration 316 : 0.4675290855540452
Loss in iteration 317 : 0.4674936966265682
Loss in iteration 318 : 0.4674585390896638
Loss in iteration 319 : 0.46742361103772284
Loss in iteration 320 : 0.46738891058439547
Loss in iteration 321 : 0.46735443586233394
Loss in iteration 322 : 0.4673201850229534
Loss in iteration 323 : 0.46728615623617925
Loss in iteration 324 : 0.4672523476902169
Loss in iteration 325 : 0.46721875759131004
Loss in iteration 326 : 0.4671853841635152
Loss in iteration 327 : 0.46715222564847236
Loss in iteration 328 : 0.4671192803051798
Loss in iteration 329 : 0.46708654640977515
Loss in iteration 330 : 0.467054022255325
Loss in iteration 331 : 0.4670217061516073
Loss in iteration 332 : 0.466989596424897
Loss in iteration 333 : 0.46695769141777466
Loss in iteration 334 : 0.46692598948891045
Loss in iteration 335 : 0.4668944890128756
Loss in iteration 336 : 0.46686318837993723
Loss in iteration 337 : 0.466832085995874
Loss in iteration 338 : 0.46680118028178197
Loss in iteration 339 : 0.4667704696738901
Loss in iteration 340 : 0.4667399526233735
Loss in iteration 341 : 0.46670962759617735
Loss in iteration 342 : 0.4666794930728379
Loss in iteration 343 : 0.4666495475482999
Loss in iteration 344 : 0.4666197895317606
Loss in iteration 345 : 0.46659021754648095
Loss in iteration 346 : 0.4665608301296319
Loss in iteration 347 : 0.4665316258321248
Loss in iteration 348 : 0.46650260321845344
Loss in iteration 349 : 0.46647376086652503
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.7895, training accuracy 0.7895, time elapsed: 6846 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 20.90243994357049
Loss in iteration 3 : 12.517217681051934
Loss in iteration 4 : 12.194135457128455
Loss in iteration 5 : 6.689634168295119
Loss in iteration 6 : 7.672148955668924
Loss in iteration 7 : 2.987619417986647
Loss in iteration 8 : 6.708088140274809
Loss in iteration 9 : 5.70103792037505
Loss in iteration 10 : 5.415723511937633
Loss in iteration 11 : 5.344570548341493
Loss in iteration 12 : 3.591161506583002
Loss in iteration 13 : 3.7273300847818045
Loss in iteration 14 : 4.35727417140801
Loss in iteration 15 : 3.757872133469872
Loss in iteration 16 : 3.4722586201249435
Loss in iteration 17 : 3.4423614117700443
Loss in iteration 18 : 3.2277178660685313
Loss in iteration 19 : 3.4742633290523526
Loss in iteration 20 : 3.602575932295087
Loss in iteration 21 : 3.0316464418226485
Loss in iteration 22 : 2.4875756558645046
Loss in iteration 23 : 2.5832205040610834
Loss in iteration 24 : 2.5967153893571377
Loss in iteration 25 : 2.5996784612260293
Loss in iteration 26 : 2.652912795718911
Loss in iteration 27 : 2.2691396241660224
Loss in iteration 28 : 2.1724366603116367
Loss in iteration 29 : 2.1653741279886525
Loss in iteration 30 : 1.9281047838589485
Loss in iteration 31 : 1.9303509436778028
Loss in iteration 32 : 1.7322635755721634
Loss in iteration 33 : 1.7607365968527404
Loss in iteration 34 : 1.732396943023086
Loss in iteration 35 : 1.6080456779959196
Loss in iteration 36 : 1.5851269284250156
Loss in iteration 37 : 1.4452720524539586
Loss in iteration 38 : 1.4200118073928651
Loss in iteration 39 : 1.316743122856346
Loss in iteration 40 : 1.3867726880473403
Loss in iteration 41 : 1.3099323016930726
Loss in iteration 42 : 1.2495815452936414
Loss in iteration 43 : 1.1697296380618032
Loss in iteration 44 : 1.1675372948839136
Loss in iteration 45 : 1.1683680037844697
Loss in iteration 46 : 1.047534529102782
Loss in iteration 47 : 1.0894958662398515
Loss in iteration 48 : 1.0388372150022684
Loss in iteration 49 : 0.9638454012953593
Loss in iteration 50 : 0.9479003879314478
Loss in iteration 51 : 0.9554563399452397
Loss in iteration 52 : 0.9242441820329391
Loss in iteration 53 : 0.8573120597244169
Loss in iteration 54 : 0.824740950680018
Loss in iteration 55 : 0.8005714799016787
Loss in iteration 56 : 0.7376934159102673
Loss in iteration 57 : 0.7221141220825767
Loss in iteration 58 : 0.691544882123159
Loss in iteration 59 : 0.6253458107613833
Loss in iteration 60 : 0.6421145455262065
Loss in iteration 61 : 0.6291511466708095
Loss in iteration 62 : 0.7496253075573628
Loss in iteration 63 : 1.6704611635906828
Loss in iteration 64 : 3.358499936856498
Loss in iteration 65 : 0.9142584208748763
Loss in iteration 66 : 1.276803236540182
Loss in iteration 67 : 2.446395668525969
Loss in iteration 68 : 0.7937994938656753
Loss in iteration 69 : 1.462499285866225
Loss in iteration 70 : 1.6372053860313127
Loss in iteration 71 : 0.886356607350463
Loss in iteration 72 : 1.6925698511940268
Loss in iteration 73 : 1.0252796077673108
Loss in iteration 74 : 1.151973197073933
Loss in iteration 75 : 1.302334693231502
Loss in iteration 76 : 0.8285261031511089
Loss in iteration 77 : 1.167515707226244
Loss in iteration 78 : 0.8983001881212371
Loss in iteration 79 : 0.8008382757569359
Loss in iteration 80 : 1.0540645360437293
Loss in iteration 81 : 0.819370888211828
Loss in iteration 82 : 0.6880921888213568
Loss in iteration 83 : 0.858245388472199
Loss in iteration 84 : 0.8599372072467595
Loss in iteration 85 : 0.7527240631084251
Loss in iteration 86 : 0.6583496299666521
Loss in iteration 87 : 0.5819799404488205
Loss in iteration 88 : 0.5919833472640796
Loss in iteration 89 : 0.5737233406817152
Loss in iteration 90 : 0.6973170482413045
Loss in iteration 91 : 1.2641989614089788
Loss in iteration 92 : 2.5793622659077196
Loss in iteration 93 : 1.8346680942321463
Loss in iteration 94 : 0.5761139012964273
Loss in iteration 95 : 1.1031263508944171
Loss in iteration 96 : 1.958989041736537
Loss in iteration 97 : 0.995328856379911
Loss in iteration 98 : 0.7916960000102652
Loss in iteration 99 : 0.9267630072410523
Loss in iteration 100 : 1.1073695743602283
Loss in iteration 101 : 0.9107166556817072
Loss in iteration 102 : 0.6621463279183476
Loss in iteration 103 : 0.8507593237954002
Loss in iteration 104 : 0.906696930441055
Loss in iteration 105 : 0.7243616719523978
Loss in iteration 106 : 0.8461176810583378
Loss in iteration 107 : 0.623440632418968
Loss in iteration 108 : 0.7494277717659045
Loss in iteration 109 : 0.6718157682520652
Loss in iteration 110 : 0.6603502844572438
Loss in iteration 111 : 0.7187100586774814
Loss in iteration 112 : 0.6751762691878376
Loss in iteration 113 : 0.8405246277630141
Loss in iteration 114 : 0.942411207185429
Loss in iteration 115 : 1.1495912946806381
Loss in iteration 116 : 1.6147991671781696
Loss in iteration 117 : 0.8745425604290916
Loss in iteration 118 : 1.0994300728711683
Loss in iteration 119 : 0.8050017673170308
Loss in iteration 120 : 0.9686046297017763
Loss in iteration 121 : 1.0362656464261621
Loss in iteration 122 : 1.2271784651417839
Loss in iteration 123 : 1.2839028185043657
Loss in iteration 124 : 1.2088924252064095
Loss in iteration 125 : 0.7070888538924108
Loss in iteration 126 : 0.7536528604709505
Loss in iteration 127 : 0.6172881346306924
Loss in iteration 128 : 0.6398888150195312
Loss in iteration 129 : 0.6596425911554759
Loss in iteration 130 : 0.5896858604958891
Loss in iteration 131 : 0.743075401277088
Loss in iteration 132 : 0.8672114957884637
Loss in iteration 133 : 1.5820561057012976
Loss in iteration 134 : 1.6016789871202741
Loss in iteration 135 : 1.0119765444265283
Loss in iteration 136 : 0.7145223138650374
Loss in iteration 137 : 0.6269770724449482
Loss in iteration 138 : 0.5895569439363227
Loss in iteration 139 : 0.6411606129593618
Loss in iteration 140 : 0.6750734218425136
Loss in iteration 141 : 0.9491248645298156
Loss in iteration 142 : 1.5857057524802116
Loss in iteration 143 : 1.7623550382412558
Loss in iteration 144 : 1.1988012474293737
Loss in iteration 145 : 0.7384402907328453
Loss in iteration 146 : 0.6672153079499076
Loss in iteration 147 : 0.6172623096777693
Loss in iteration 148 : 0.6267630959734043
Loss in iteration 149 : 0.6342501436308805
Loss in iteration 150 : 0.6688436483943676
Loss in iteration 151 : 0.8459464730796519
Loss in iteration 152 : 1.306581602856685
Loss in iteration 153 : 1.818628871594437
Loss in iteration 154 : 1.2844174096971213
Loss in iteration 155 : 0.7944803850762377
Loss in iteration 156 : 0.6674895647807
Loss in iteration 157 : 0.6030752639319262
Loss in iteration 158 : 0.5974906980960091
Loss in iteration 159 : 0.5743226323520759
Loss in iteration 160 : 0.5609179160809219
Loss in iteration 161 : 0.5598790377411291
Loss in iteration 162 : 0.6038655651095786
Loss in iteration 163 : 0.971977443766513
Loss in iteration 164 : 2.342373219397302
Loss in iteration 165 : 1.8161623669973377
Loss in iteration 166 : 0.8274206343531748
Loss in iteration 167 : 0.5848070134489669
Loss in iteration 168 : 0.569915760276797
Loss in iteration 169 : 0.6325615058721247
Loss in iteration 170 : 0.7854159409833896
Loss in iteration 171 : 1.0280746640597536
Loss in iteration 172 : 1.2325927782067947
Loss in iteration 173 : 1.1453213428033846
Loss in iteration 174 : 0.9337051060959668
Loss in iteration 175 : 0.7920650146846522
Loss in iteration 176 : 0.7291586304120132
Loss in iteration 177 : 0.7459906461849466
Loss in iteration 178 : 0.8199389863911211
Loss in iteration 179 : 0.950799656120367
Loss in iteration 180 : 1.0775791050356573
Loss in iteration 181 : 1.0587464161101656
Loss in iteration 182 : 0.9296029071025971
Loss in iteration 183 : 0.800016884641788
Loss in iteration 184 : 0.6995507648123109
Loss in iteration 185 : 0.6612382189475168
Loss in iteration 186 : 0.6461910466055404
Loss in iteration 187 : 0.7007434646219127
Loss in iteration 188 : 0.8973313945795344
Loss in iteration 189 : 1.4371586378130365
Loss in iteration 190 : 1.7612738360425393
Loss in iteration 191 : 1.2128849433745972
Loss in iteration 192 : 0.7863663231279362
Loss in iteration 193 : 0.6529771951616501
Loss in iteration 194 : 0.6026826239869758
Loss in iteration 195 : 0.5903578098296858
Loss in iteration 196 : 0.571585321721527
Loss in iteration 197 : 0.5663863155328985
Loss in iteration 198 : 0.5790598284846065
Loss in iteration 199 : 0.7170870232170955
Loss in iteration 200 : 1.3777479657409395
Loss in iteration 201 : 2.399553763315075
Loss in iteration 202 : 1.1581788086414926
Loss in iteration 203 : 0.6568473587959366
Loss in iteration 204 : 0.5645571584862805
Loss in iteration 205 : 0.5840585389392104
Loss in iteration 206 : 0.6746329134731754
Loss in iteration 207 : 0.9037255400129373
Loss in iteration 208 : 1.2813937028712556
Loss in iteration 209 : 1.4328722922366952
Loss in iteration 210 : 1.113820662857777
Loss in iteration 211 : 0.8288020370144015
Loss in iteration 212 : 0.7097079636146141
Loss in iteration 213 : 0.6612854977328334
Loss in iteration 214 : 0.6795662361639806
Loss in iteration 215 : 0.7412022883442624
Loss in iteration 216 : 0.9189247597710297
Loss in iteration 217 : 1.2121310836033554
Loss in iteration 218 : 1.3441329209684318
Loss in iteration 219 : 1.1315813299275883
Loss in iteration 220 : 0.871836590691033
Loss in iteration 221 : 0.7010800408962401
Loss in iteration 222 : 0.6381931995395941
Loss in iteration 223 : 0.5978680441791183
Loss in iteration 224 : 0.5850169556443334
Loss in iteration 225 : 0.5746564661220711
Loss in iteration 226 : 0.6347863208508918
Loss in iteration 227 : 0.9673607709076041
Loss in iteration 228 : 2.063650796539943
Loss in iteration 229 : 1.9087681310431905
Loss in iteration 230 : 1.0485559812707315
Loss in iteration 231 : 0.7315760409395939
Loss in iteration 232 : 0.6831925674749448
Loss in iteration 233 : 0.6794477505221775
Loss in iteration 234 : 0.7175183740602167
Loss in iteration 235 : 0.7459514691180007
Loss in iteration 236 : 0.8699228402196092
Loss in iteration 237 : 1.0621651002686154
Loss in iteration 238 : 1.3198099693803564
Loss in iteration 239 : 1.2383764960562944
Loss in iteration 240 : 1.067417977140629
Loss in iteration 241 : 0.921306547092415
Loss in iteration 242 : 0.8809451170866123
Loss in iteration 243 : 0.832537101324803
Loss in iteration 244 : 0.8308863033454433
Loss in iteration 245 : 0.7981045950258969
Loss in iteration 246 : 0.8108220577341462
Loss in iteration 247 : 0.7916804708597388
Loss in iteration 248 : 0.8708226509239764
Loss in iteration 249 : 0.9862298710713474
Loss in iteration 250 : 1.2469768206068808
Loss in iteration 251 : 1.3173364269668082
Loss in iteration 252 : 1.2233039933285415
Loss in iteration 253 : 0.9907422798423636
Loss in iteration 254 : 0.8787437750762787
Loss in iteration 255 : 0.7752545060256414
Loss in iteration 256 : 0.7639851321230542
Loss in iteration 257 : 0.7491814018873313
Loss in iteration 258 : 0.8308671018968214
Loss in iteration 259 : 0.9663438878287058
Loss in iteration 260 : 1.2563709644176357
Loss in iteration 261 : 1.4200700382757714
Loss in iteration 262 : 1.2467149670155868
Loss in iteration 263 : 0.9312486834852887
Loss in iteration 264 : 0.7887114057966572
Loss in iteration 265 : 0.7014701910518361
Loss in iteration 266 : 0.6934771260201964
Loss in iteration 267 : 0.6815622357025295
Loss in iteration 268 : 0.7515161706307456
Loss in iteration 269 : 0.9314235770348662
Loss in iteration 270 : 1.3799944791422372
Loss in iteration 271 : 1.6614959736397554
Loss in iteration 272 : 1.312240624285386
Loss in iteration 273 : 0.9304679831802576
Loss in iteration 274 : 0.7651551520780935
Loss in iteration 275 : 0.6938515882416941
Loss in iteration 276 : 0.6743856328158929
Loss in iteration 277 : 0.6697007737627438
Loss in iteration 278 : 0.7189594035946015
Loss in iteration 279 : 0.8984270773921043
Loss in iteration 280 : 1.38359095729214
Loss in iteration 281 : 1.867490317354308
Loss in iteration 282 : 1.3548701751776395
Loss in iteration 283 : 0.8876043223596687
Loss in iteration 284 : 0.7268399104447921
Loss in iteration 285 : 0.6709432272125423
Loss in iteration 286 : 0.6661636611254039
Loss in iteration 287 : 0.6669247364421245
Loss in iteration 288 : 0.7271927209380842
Loss in iteration 289 : 0.927370286831374
Loss in iteration 290 : 1.4388089991327275
Loss in iteration 291 : 1.8464907506739852
Loss in iteration 292 : 1.331954079626084
Loss in iteration 293 : 0.9174322088079334
Loss in iteration 294 : 0.7629788444003409
Loss in iteration 295 : 0.7183168443966818
Loss in iteration 296 : 0.7197278021161969
Loss in iteration 297 : 0.7538955110354428
Loss in iteration 298 : 0.8592025048274602
Loss in iteration 299 : 1.1125830447692115
Loss in iteration 300 : 1.4369228823927274
Loss in iteration 301 : 1.5180449833028655
Loss in iteration 302 : 1.160873389305374
Loss in iteration 303 : 0.900141363919845
Loss in iteration 304 : 0.7897758038754653
Loss in iteration 305 : 0.754266335783096
Loss in iteration 306 : 0.7702752943853657
Loss in iteration 307 : 0.8260936516875079
Loss in iteration 308 : 0.9835133581583941
Loss in iteration 309 : 1.2633430392118952
Loss in iteration 310 : 1.442796202922982
Loss in iteration 311 : 1.2845686493047346
Loss in iteration 312 : 1.0390510315610524
Loss in iteration 313 : 0.8839247053834847
Loss in iteration 314 : 0.8349427301079921
Loss in iteration 315 : 0.8167560249415496
Loss in iteration 316 : 0.8694657222834914
Loss in iteration 317 : 0.9729761995632975
Loss in iteration 318 : 1.1674652539890047
Loss in iteration 319 : 1.3072559215323527
Loss in iteration 320 : 1.282502900803757
Loss in iteration 321 : 1.108181022282841
Loss in iteration 322 : 0.977002212857735
Loss in iteration 323 : 0.8648565205624962
Loss in iteration 324 : 0.8554366459501895
Loss in iteration 325 : 0.8536581215647946
Loss in iteration 326 : 0.9541565779939211
Loss in iteration 327 : 1.0561084935180425
Loss in iteration 328 : 1.25749402768157
Loss in iteration 329 : 1.293417477286026
Loss in iteration 330 : 1.2359930247210413
Loss in iteration 331 : 0.9937553921602957
Loss in iteration 332 : 0.9269690002554483
Loss in iteration 333 : 0.8407506776036336
Loss in iteration 334 : 0.9007713394059975
Loss in iteration 335 : 0.9116257715066384
Loss in iteration 336 : 1.1017336727067881
Loss in iteration 337 : 1.2310474640660463
Loss in iteration 338 : 1.3741803516681483
Loss in iteration 339 : 1.1435286977879986
Loss in iteration 340 : 1.0394949069250137
Loss in iteration 341 : 0.9025681851574499
Loss in iteration 342 : 0.9302763189313269
Loss in iteration 343 : 0.903717042605927
Loss in iteration 344 : 1.0288792415935681
Loss in iteration 345 : 1.1140014164149825
Loss in iteration 346 : 1.2752284426449116
Loss in iteration 347 : 1.1838589216749844
Loss in iteration 348 : 1.1262841120344114
Loss in iteration 349 : 0.9898292983338561
Loss in iteration 350 : 0.9841990088704913
Loss in iteration 351 : 0.9245872942224548
Loss in iteration 352 : 1.0031332336398209
Loss in iteration 353 : 1.0489282661385593
Loss in iteration 354 : 1.204181344632244
Loss in iteration 355 : 1.1760247867124485
Loss in iteration 356 : 1.1862977230904468
Loss in iteration 357 : 1.0607543213443968
Loss in iteration 358 : 1.0465424920735584
Loss in iteration 359 : 0.9441918196302119
Loss in iteration 360 : 0.9931531657994762
Loss in iteration 361 : 0.9948340556226064
Loss in iteration 362 : 1.1454181148261642
Loss in iteration 363 : 1.148805754698289
Loss in iteration 364 : 1.2418478936120172
Loss in iteration 365 : 1.1429657941079163
Loss in iteration 366 : 1.1346586002445456
Loss in iteration 367 : 0.9788046313646799
Loss in iteration 368 : 0.9932843661509999
Loss in iteration 369 : 0.9456663875513452
Loss in iteration 370 : 1.0659148083962335
Loss in iteration 371 : 1.0775159963374084
Loss in iteration 372 : 1.242435125801576
Loss in iteration 373 : 1.222245711179653
Loss in iteration 374 : 1.2513035371536958
Loss in iteration 375 : 1.0449433388546714
Loss in iteration 376 : 1.0152764871727251
Loss in iteration 377 : 0.9224973828083501
Loss in iteration 378 : 1.0030681504079253
Loss in iteration 379 : 0.9991560072165716
Loss in iteration 380 : 1.1833484584920353
Loss in iteration 381 : 1.2497253293350021
Loss in iteration 382 : 1.353743299405638
Loss in iteration 383 : 1.1330498409235037
Loss in iteration 384 : 1.0624825555202597
Loss in iteration 385 : 0.9353592153922925
Loss in iteration 386 : 0.981986147323824
Loss in iteration 387 : 0.9589252454146894
Loss in iteration 388 : 1.1195797908281622
Loss in iteration 389 : 1.2163354240993103
Loss in iteration 390 : 1.3808533207239875
Loss in iteration 391 : 1.2083516567953838
Loss in iteration 392 : 1.1295183661186707
Loss in iteration 393 : 0.9800977779416986
Loss in iteration 394 : 0.997473871615703
Loss in iteration 395 : 0.94842384441944
Loss in iteration 396 : 1.0701867867154724
Loss in iteration 397 : 1.1486843698634608
Loss in iteration 398 : 1.3430683622463389
Loss in iteration 399 : 1.2525390420554319
Loss in iteration 400 : 1.2149971353762639
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.736375, training accuracy 0.736375, time elapsed: 7940 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 1.2072834954275573
Loss in iteration 3 : 4.802876348716899
Loss in iteration 4 : 2.6030459733453233
Loss in iteration 5 : 5.7742913478875595
Loss in iteration 6 : 5.113638184594809
Loss in iteration 7 : 1.0972645454164736
Loss in iteration 8 : 3.3893312369533453
Loss in iteration 9 : 1.1304766179309322
Loss in iteration 10 : 2.5206652682219675
Loss in iteration 11 : 2.377803170815035
Loss in iteration 12 : 1.0372668301674175
Loss in iteration 13 : 2.3562363931305663
Loss in iteration 14 : 1.4794801035165697
Loss in iteration 15 : 1.4641246714818101
Loss in iteration 16 : 2.113096527363673
Loss in iteration 17 : 1.3791206986132674
Loss in iteration 18 : 1.4216469928358288
Loss in iteration 19 : 1.844171441227255
Loss in iteration 20 : 1.1933116467883431
Loss in iteration 21 : 1.4580294931559592
Loss in iteration 22 : 1.6192891213945704
Loss in iteration 23 : 1.1618668194413275
Loss in iteration 24 : 1.392739586326998
Loss in iteration 25 : 1.3794525639050361
Loss in iteration 26 : 1.06408369292611
Loss in iteration 27 : 1.3223770438903484
Loss in iteration 28 : 1.14619468683197
Loss in iteration 29 : 1.041702385912025
Loss in iteration 30 : 1.1971269115365735
Loss in iteration 31 : 0.9273429471144521
Loss in iteration 32 : 1.053818551395631
Loss in iteration 33 : 0.9528176125236
Loss in iteration 34 : 0.8784590057789137
Loss in iteration 35 : 0.9482516814554584
Loss in iteration 36 : 0.7819697949581031
Loss in iteration 37 : 0.886662104146525
Loss in iteration 38 : 0.7315188564203563
Loss in iteration 39 : 0.8264462088402592
Loss in iteration 40 : 0.6941919307808657
Loss in iteration 41 : 0.771258707740614
Loss in iteration 42 : 0.6621181764061148
Loss in iteration 43 : 0.7428902060909572
Loss in iteration 44 : 0.6340989668276049
Loss in iteration 45 : 0.7064139709540426
Loss in iteration 46 : 0.6255411522229083
Loss in iteration 47 : 0.6559643501335684
Loss in iteration 48 : 0.6570352387560849
Loss in iteration 49 : 0.5963104986140321
Loss in iteration 50 : 0.6524549215644256
Loss in iteration 51 : 0.6141407318347327
Loss in iteration 52 : 0.5780026455062004
Loss in iteration 53 : 0.6184035672459843
Loss in iteration 54 : 0.5980486144010005
Loss in iteration 55 : 0.5489173759081543
Loss in iteration 56 : 0.5635606659647997
Loss in iteration 57 : 0.5784750746540717
Loss in iteration 58 : 0.5417294352375405
Loss in iteration 59 : 0.5139820748708209
Loss in iteration 60 : 0.5273212493791108
Loss in iteration 61 : 0.5365447686013308
Loss in iteration 62 : 0.5134059611318184
Loss in iteration 63 : 0.49008442619555725
Loss in iteration 64 : 0.4906289173998161
Loss in iteration 65 : 0.5042174087246929
Loss in iteration 66 : 0.5084992436813333
Loss in iteration 67 : 0.4970804442904174
Loss in iteration 68 : 0.4817815406955085
Loss in iteration 69 : 0.4723900919081915
Loss in iteration 70 : 0.4712730512847352
Loss in iteration 71 : 0.47628753596516027
Loss in iteration 72 : 0.4872970877540618
Loss in iteration 73 : 0.5070227614248854
Loss in iteration 74 : 0.542096091611291
Loss in iteration 75 : 0.6157021608109496
Loss in iteration 76 : 0.701658603982442
Loss in iteration 77 : 0.7910344435331799
Loss in iteration 78 : 0.7098489921715928
Loss in iteration 79 : 0.619532157826186
Loss in iteration 80 : 0.5328256042206997
Loss in iteration 81 : 0.48734435745905
Loss in iteration 82 : 0.4705886291312304
Loss in iteration 83 : 0.47464746889565607
Loss in iteration 84 : 0.5008607149633263
Loss in iteration 85 : 0.5594156679684096
Loss in iteration 86 : 0.6761467072892887
Loss in iteration 87 : 0.7867051641620085
Loss in iteration 88 : 0.8064972984656785
Loss in iteration 89 : 0.6405769535614024
Loss in iteration 90 : 0.5182812270609831
Loss in iteration 91 : 0.47891273463705597
Loss in iteration 92 : 0.5117842379706566
Loss in iteration 93 : 0.5751029263520985
Loss in iteration 94 : 0.5984338016954796
Loss in iteration 95 : 0.5725501403010317
Loss in iteration 96 : 0.5236596133356102
Loss in iteration 97 : 0.4923533395253666
Loss in iteration 98 : 0.47660898226085113
Loss in iteration 99 : 0.47014979667525786
Loss in iteration 100 : 0.46752203123147124
Loss in iteration 101 : 0.4651872901652271
Loss in iteration 102 : 0.4648980446138552
Loss in iteration 103 : 0.4654528833871713
Loss in iteration 104 : 0.4712347873752339
Loss in iteration 105 : 0.4880464676561301
Loss in iteration 106 : 0.5399997967100486
Loss in iteration 107 : 0.6696261801921801
Loss in iteration 108 : 0.9279302513660255
Loss in iteration 109 : 0.891096303621506
Loss in iteration 110 : 0.7191014410635684
Loss in iteration 111 : 0.5368376980104577
Loss in iteration 112 : 0.4791052375335607
Loss in iteration 113 : 0.5092019306363144
Loss in iteration 114 : 0.5869940436308486
Loss in iteration 115 : 0.6505617307043781
Loss in iteration 116 : 0.6246256331471828
Loss in iteration 117 : 0.5541102777734013
Loss in iteration 118 : 0.49631197270365374
Loss in iteration 119 : 0.47729454276874184
Loss in iteration 120 : 0.489316428144385
Loss in iteration 121 : 0.5204978352897478
Loss in iteration 122 : 0.5556680215346319
Loss in iteration 123 : 0.5716599635867271
Loss in iteration 124 : 0.5640333035458429
Loss in iteration 125 : 0.536040386466641
Loss in iteration 126 : 0.5108647172314772
Loss in iteration 127 : 0.49359521996230593
Loss in iteration 128 : 0.48239965128158463
Loss in iteration 129 : 0.4800975124757797
Loss in iteration 130 : 0.48702698490019813
Loss in iteration 131 : 0.5163283637062702
Loss in iteration 132 : 0.6240833822008879
Loss in iteration 133 : 0.862920657627084
Loss in iteration 134 : 1.1125998845309428
Loss in iteration 135 : 0.7464279987934638
Loss in iteration 136 : 0.5310202205086039
Loss in iteration 137 : 0.4885183594189987
Loss in iteration 138 : 0.5658543782010145
Loss in iteration 139 : 0.6812589233538344
Loss in iteration 140 : 0.6697341860966352
Loss in iteration 141 : 0.5668028351122291
Loss in iteration 142 : 0.4922035280689096
Loss in iteration 143 : 0.4853805937347229
Loss in iteration 144 : 0.5397597749752819
Loss in iteration 145 : 0.6169499107737357
Loss in iteration 146 : 0.6799531188677779
Loss in iteration 147 : 0.6631467446911286
Loss in iteration 148 : 0.5835632119779002
Loss in iteration 149 : 0.5161685542071485
Loss in iteration 150 : 0.47957268900529787
Loss in iteration 151 : 0.5074587243032601
Loss in iteration 152 : 0.5439423414318023
Loss in iteration 153 : 0.6084995932315728
Loss in iteration 154 : 0.6559285552076565
Loss in iteration 155 : 0.6671611249500153
Loss in iteration 156 : 0.6623422817278937
Loss in iteration 157 : 0.6262838975783919
Loss in iteration 158 : 0.6426329446314144
Loss in iteration 159 : 0.5586285179792477
Loss in iteration 160 : 0.576242188273336
Loss in iteration 161 : 0.49615081865694005
Loss in iteration 162 : 0.5244474368213681
Loss in iteration 163 : 0.4867971955088142
Loss in iteration 164 : 0.5333419436438454
Loss in iteration 165 : 0.5733940611814647
Loss in iteration 166 : 0.7157186782366741
Loss in iteration 167 : 0.8010365098044697
Loss in iteration 168 : 0.8365032885534672
Loss in iteration 169 : 0.6683111034307094
Loss in iteration 170 : 0.5604579272195499
Loss in iteration 171 : 0.5334237213306663
Loss in iteration 172 : 0.5293890470578014
Loss in iteration 173 : 0.638546627616046
Loss in iteration 174 : 0.6585216222615095
Loss in iteration 175 : 0.6882449420813213
Loss in iteration 176 : 0.5927599384960425
Loss in iteration 177 : 0.5574173960704882
Loss in iteration 178 : 0.5304491890530956
Loss in iteration 179 : 0.49758288152739205
Loss in iteration 180 : 0.5401798980606449
Loss in iteration 181 : 0.5397819653128515
Loss in iteration 182 : 0.6488226272112907
Loss in iteration 183 : 0.727006541121649
Loss in iteration 184 : 0.78055895548474
Loss in iteration 185 : 0.662611582112256
Loss in iteration 186 : 0.6023592579387139
Loss in iteration 187 : 0.5610710697926307
Loss in iteration 188 : 0.5156622815797409
Loss in iteration 189 : 0.6007552894802329
Loss in iteration 190 : 0.6029361610765034
Loss in iteration 191 : 0.7989436864913607
Loss in iteration 192 : 0.7621430035323863
Loss in iteration 193 : 0.7805679992826712
Loss in iteration 194 : 0.616254598291725
Loss in iteration 195 : 0.5893748631282183
Loss in iteration 196 : 0.5231273692447089
Loss in iteration 197 : 0.5154472107777672
Loss in iteration 198 : 0.5324892531475205
Loss in iteration 199 : 0.5282171079865204
Loss in iteration 200 : 0.6065576659489939
Loss in iteration 201 : 0.6394833288992053
Loss in iteration 202 : 0.753010921940775
Loss in iteration 203 : 0.722525793956511
Loss in iteration 204 : 0.7306642959525854
Loss in iteration 205 : 0.6253483689177312
Loss in iteration 206 : 0.587672187356108
Loss in iteration 207 : 0.5530753330336865
Loss in iteration 208 : 0.5172090919324821
Loss in iteration 209 : 0.5781315366505053
Loss in iteration 210 : 0.5805771923414577
Loss in iteration 211 : 0.7514654824066236
Loss in iteration 212 : 0.7798577230528407
Loss in iteration 213 : 0.8710452856262555
Loss in iteration 214 : 0.6643836009411535
Loss in iteration 215 : 0.6225867741531582
Loss in iteration 216 : 0.5426530661658672
Loss in iteration 217 : 0.5423766721523706
Loss in iteration 218 : 0.5850102841970415
Loss in iteration 219 : 0.5993887542207292
Loss in iteration 220 : 0.6897510616683208
Loss in iteration 221 : 0.6755488282844992
Loss in iteration 222 : 0.7204971591545221
Loss in iteration 223 : 0.6495450889187009
Loss in iteration 224 : 0.6524474343928222
Loss in iteration 225 : 0.5944740573491231
Loss in iteration 226 : 0.5678553625001973
Loss in iteration 227 : 0.5761923282986503
Loss in iteration 228 : 0.5217973750679508
Loss in iteration 229 : 0.5997564144430405
Loss in iteration 230 : 0.6029636461887526
Loss in iteration 231 : 0.812050441330575
Loss in iteration 232 : 0.8427921512036972
Loss in iteration 233 : 0.876981460154372
Loss in iteration 234 : 0.6464714602494233
Loss in iteration 235 : 0.5995976186827492
Loss in iteration 236 : 0.5060579606146822
Loss in iteration 237 : 0.5332845944804073
Loss in iteration 238 : 0.48634385699192806
Loss in iteration 239 : 0.5209585250767049
Loss in iteration 240 : 0.4865001534874582
Loss in iteration 241 : 0.5221093488476424
Loss in iteration 242 : 0.5103271766125733
Loss in iteration 243 : 0.5986563206209566
Loss in iteration 244 : 0.8075798098688278
Loss in iteration 245 : 1.1760668115074304
Loss in iteration 246 : 1.116182665178511
Loss in iteration 247 : 0.603056587842738
Loss in iteration 248 : 0.5628323262648357
Loss in iteration 249 : 0.6477960274128264
Loss in iteration 250 : 0.8159930869620814
Loss in iteration 251 : 0.7080877947821997
Loss in iteration 252 : 0.5666229386906858
Loss in iteration 253 : 0.5345467763639478
Loss in iteration 254 : 0.5640931860342249
Loss in iteration 255 : 0.6774822477240755
Loss in iteration 256 : 0.7434160793246916
Loss in iteration 257 : 0.7867678302832334
Loss in iteration 258 : 0.7219245422267897
Loss in iteration 259 : 0.6253394822943431
Loss in iteration 260 : 0.5638579903436316
Loss in iteration 261 : 0.5196278817978082
Loss in iteration 262 : 0.5488222052107138
Loss in iteration 263 : 0.5536707470225416
Loss in iteration 264 : 0.599787183424962
Loss in iteration 265 : 0.6030362560442447
Loss in iteration 266 : 0.6639996770589056
Loss in iteration 267 : 0.7690279183387397
Loss in iteration 268 : 0.9080140877296551
Loss in iteration 269 : 1.0082479523706687
Loss in iteration 270 : 0.6769655298639261
Loss in iteration 271 : 0.6570786472827856
Loss in iteration 272 : 0.5298788706749267
Loss in iteration 273 : 0.5499645840457434
Loss in iteration 274 : 0.5183730446090529
Loss in iteration 275 : 0.5053467694342919
Loss in iteration 276 : 0.5134392886008774
Loss in iteration 277 : 0.4898272249132715
Loss in iteration 278 : 0.5387185003838669
Loss in iteration 279 : 0.5802561760861656
Loss in iteration 280 : 0.9034098253204839
Loss in iteration 281 : 1.2827778039290436
Loss in iteration 282 : 1.1317825558653873
Loss in iteration 283 : 0.6446147242007003
Loss in iteration 284 : 0.5209967174774872
Loss in iteration 285 : 0.6902397802712208
Loss in iteration 286 : 0.8746934353726236
Loss in iteration 287 : 0.770068521823152
Loss in iteration 288 : 0.5657111480870163
Loss in iteration 289 : 0.5042940225006337
Loss in iteration 290 : 0.6110883275419319
Loss in iteration 291 : 0.8133409186889369
Loss in iteration 292 : 0.8918698546383821
Loss in iteration 293 : 0.7615480114410694
Loss in iteration 294 : 0.6030102124997786
Loss in iteration 295 : 0.5201965939735235
Loss in iteration 296 : 0.5065363476782114
Loss in iteration 297 : 0.5365407923136282
Loss in iteration 298 : 0.5924236483215334
Loss in iteration 299 : 0.6687009166282285
Loss in iteration 300 : 0.7204539537095639
Loss in iteration 301 : 0.7717807597083904
Loss in iteration 302 : 0.7678292480455302
Loss in iteration 303 : 0.7887395441050942
Loss in iteration 304 : 0.7632168286222273
Loss in iteration 305 : 0.7661535786520123
Loss in iteration 306 : 0.7333756569646322
Loss in iteration 307 : 0.7177739887439258
Loss in iteration 308 : 0.663465276681415
Loss in iteration 309 : 0.6315592330340005
Loss in iteration 310 : 0.5963039314997188
Loss in iteration 311 : 0.5708755802158828
Loss in iteration 312 : 0.5789085281432959
Loss in iteration 313 : 0.5414076647762471
Loss in iteration 314 : 0.5859342413555398
Loss in iteration 315 : 0.5209587274610412
Loss in iteration 316 : 0.5866107361853959
Loss in iteration 317 : 0.5023228763611078
Loss in iteration 318 : 0.5716735292119697
Loss in iteration 319 : 0.5216183659635708
Loss in iteration 320 : 0.6847286945338392
Loss in iteration 321 : 0.9786433089762134
Loss in iteration 322 : 1.4848384437405717
Loss in iteration 323 : 0.7688518872214382
Loss in iteration 324 : 0.6085320426770003
Loss in iteration 325 : 0.5367176357440315
Loss in iteration 326 : 0.6474389298831742
Loss in iteration 327 : 0.8250149034641234
Loss in iteration 328 : 0.9667649681131737
Loss in iteration 329 : 0.9263969556717022
Loss in iteration 330 : 0.673263870406742
Loss in iteration 331 : 0.6327233098142082
Loss in iteration 332 : 0.5223761260259401
Loss in iteration 333 : 0.6107119732872918
Loss in iteration 334 : 0.5182646804615525
Loss in iteration 335 : 0.5913400995614899
Loss in iteration 336 : 0.507475586481267
Loss in iteration 337 : 0.5622724378303455
Loss in iteration 338 : 0.5392346584037204
Loss in iteration 339 : 0.6631328500143936
Loss in iteration 340 : 0.871987808301795
Loss in iteration 341 : 1.2605783821001797
Loss in iteration 342 : 1.0543495971653702
Loss in iteration 343 : 0.7495428941321749
Loss in iteration 344 : 0.5705157897955688
Loss in iteration 345 : 0.5261779262688293
Loss in iteration 346 : 0.5504428918121433
Loss in iteration 347 : 0.6484025775511636
Loss in iteration 348 : 0.8377048282860378
Loss in iteration 349 : 1.0251920151546017
Loss in iteration 350 : 1.0028911416412496
Loss in iteration 351 : 0.7688934707042713
Loss in iteration 352 : 0.6208345341526612
Loss in iteration 353 : 0.5292663299857882
Loss in iteration 354 : 0.5283397945211802
Loss in iteration 355 : 0.5240266145802507
Loss in iteration 356 : 0.5634150924427288
Loss in iteration 357 : 0.5864534257380146
Loss in iteration 358 : 0.6577534800124137
Loss in iteration 359 : 0.7790182117892989
Loss in iteration 360 : 0.9051927493560654
Loss in iteration 361 : 1.029491086820336
Loss in iteration 362 : 0.8103608645269625
Loss in iteration 363 : 0.7872071415930664
Loss in iteration 364 : 0.6084216634329487
Loss in iteration 365 : 0.6530066739236743
Loss in iteration 366 : 0.5554328780645569
Loss in iteration 367 : 0.6212021074943148
Loss in iteration 368 : 0.5980032432529953
Loss in iteration 369 : 0.7563322246762325
Loss in iteration 370 : 0.9260638908444071
Loss in iteration 371 : 1.139758472774549
Loss in iteration 372 : 0.9166615535036811
Loss in iteration 373 : 0.7366738519314158
Loss in iteration 374 : 0.6086562205250503
Loss in iteration 375 : 0.5454960641196415
Loss in iteration 376 : 0.5932479469603101
Loss in iteration 377 : 0.6173247366712805
Loss in iteration 378 : 0.7811765877289593
Loss in iteration 379 : 0.8297576157666683
Loss in iteration 380 : 0.8860840624960356
Loss in iteration 381 : 0.7799525382461342
Loss in iteration 382 : 0.7376505486509359
Loss in iteration 383 : 0.6537377121747309
Loss in iteration 384 : 0.6544180229378046
Loss in iteration 385 : 0.6397185572494168
Loss in iteration 386 : 0.655918046451244
Loss in iteration 387 : 0.6769679786053254
Loss in iteration 388 : 0.701789868818207
Loss in iteration 389 : 0.7535950262459888
Loss in iteration 390 : 0.7957331652150677
Loss in iteration 391 : 0.7416259203099932
Loss in iteration 392 : 0.9324872357257146
Loss in iteration 393 : 0.731190828447541
Loss in iteration 394 : 0.8883369675072659
Loss in iteration 395 : 0.766988264667986
Loss in iteration 396 : 0.67222102372672
Loss in iteration 397 : 0.6483768326938325
Loss in iteration 398 : 0.5987028691561774
Loss in iteration 399 : 0.5809295202844924
Loss in iteration 400 : 0.6275448153232018
Testing accuracy  of updater 7 on alg 0 with rate 14.0 = 0.745, training accuracy 0.745, time elapsed: 9122 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.7099904538824187
Loss in iteration 3 : 1.1184647522117956
Loss in iteration 4 : 1.5678273852787725
Loss in iteration 5 : 0.6696062114393392
Loss in iteration 6 : 0.7092340199671444
Loss in iteration 7 : 0.9266332108572545
Loss in iteration 8 : 0.5462044645780172
Loss in iteration 9 : 0.8351813951772159
Loss in iteration 10 : 0.5444094621114197
Loss in iteration 11 : 0.7876589552103328
Loss in iteration 12 : 0.5646592001386532
Loss in iteration 13 : 0.7320995336074303
Loss in iteration 14 : 0.5847615493257574
Loss in iteration 15 : 0.6939295344240326
Loss in iteration 16 : 0.6002711577139025
Loss in iteration 17 : 0.6462285971321773
Loss in iteration 18 : 0.6121113635069062
Loss in iteration 19 : 0.6054145649416468
Loss in iteration 20 : 0.6072586755786787
Loss in iteration 21 : 0.5742142490892878
Loss in iteration 22 : 0.5919048751033242
Loss in iteration 23 : 0.5465647762882905
Loss in iteration 24 : 0.5724364050961743
Loss in iteration 25 : 0.5265288923878315
Loss in iteration 26 : 0.5466326016722106
Loss in iteration 27 : 0.515235786789664
Loss in iteration 28 : 0.5278171985289379
Loss in iteration 29 : 0.505652482098343
Loss in iteration 30 : 0.5068384416373511
Loss in iteration 31 : 0.5068868091679731
Loss in iteration 32 : 0.48851224651937497
Loss in iteration 33 : 0.5079905796053906
Loss in iteration 34 : 0.4791693217166967
Loss in iteration 35 : 0.5036307749233813
Loss in iteration 36 : 0.4833331983817039
Loss in iteration 37 : 0.4889275498222872
Loss in iteration 38 : 0.49435282389294793
Loss in iteration 39 : 0.47744655616492193
Loss in iteration 40 : 0.49172139282626814
Loss in iteration 41 : 0.48201376692121883
Loss in iteration 42 : 0.4778688297199069
Loss in iteration 43 : 0.48576006594368426
Loss in iteration 44 : 0.4736872498064485
Loss in iteration 45 : 0.47674300278302745
Loss in iteration 46 : 0.4774166784834479
Loss in iteration 47 : 0.46854931018477025
Loss in iteration 48 : 0.47344854086732996
Loss in iteration 49 : 0.46979388454209575
Loss in iteration 50 : 0.46557953005150066
Loss in iteration 51 : 0.4696437481373392
Loss in iteration 52 : 0.4649039008844544
Loss in iteration 53 : 0.4643236751637091
Loss in iteration 54 : 0.46657736927843874
Loss in iteration 55 : 0.4625726971033378
Loss in iteration 56 : 0.463793749036785
Loss in iteration 57 : 0.46434611748400106
Loss in iteration 58 : 0.4616792953123605
Loss in iteration 59 : 0.46337580819403434
Loss in iteration 60 : 0.4628052448605985
Loss in iteration 61 : 0.4612903684670216
Loss in iteration 62 : 0.46269953613014125
Loss in iteration 63 : 0.46165757640130994
Loss in iteration 64 : 0.4608147531548568
Loss in iteration 65 : 0.4618099471778026
Loss in iteration 66 : 0.4607802495667235
Loss in iteration 67 : 0.46029652417055467
Loss in iteration 68 : 0.46105996971396734
Loss in iteration 69 : 0.4602644366766535
Loss in iteration 70 : 0.45993596561911454
Loss in iteration 71 : 0.4605656047087256
Loss in iteration 72 : 0.4600747943668069
Loss in iteration 73 : 0.45978758054891594
Loss in iteration 74 : 0.46027527026967563
Loss in iteration 75 : 0.46008802736383486
Loss in iteration 76 : 0.45978052102062633
Loss in iteration 77 : 0.4600752312492057
Loss in iteration 78 : 0.46009514190245354
Loss in iteration 79 : 0.45976967606462643
Loss in iteration 80 : 0.4598730396006987
Loss in iteration 81 : 0.4599900851118961
Loss in iteration 82 : 0.4597223960252374
Loss in iteration 83 : 0.4596868410644888
Loss in iteration 84 : 0.4598085299943692
Loss in iteration 85 : 0.45965192911133196
Loss in iteration 86 : 0.4595433917978054
Loss in iteration 87 : 0.45962697664829316
Loss in iteration 88 : 0.45958010363379176
Loss in iteration 89 : 0.45945927797657354
Loss in iteration 90 : 0.4594999603883374
Loss in iteration 91 : 0.45951721455374755
Loss in iteration 92 : 0.4594165708694372
Loss in iteration 93 : 0.45941918084495137
Loss in iteration 94 : 0.4594549196778604
Loss in iteration 95 : 0.4593892288188227
Loss in iteration 96 : 0.4593643980565724
Loss in iteration 97 : 0.4593937977526092
Loss in iteration 98 : 0.45936148175694674
Loss in iteration 99 : 0.4593211808336074
Loss in iteration 100 : 0.45933546020153504
Loss in iteration 101 : 0.45932567499131305
Loss in iteration 102 : 0.45928466784331956
Loss in iteration 103 : 0.4592853190882946
Loss in iteration 104 : 0.4592871186815497
Loss in iteration 105 : 0.4592562837280065
Loss in iteration 106 : 0.45924658109498623
Loss in iteration 107 : 0.4592514166301022
Loss in iteration 108 : 0.4592342552612016
Loss in iteration 109 : 0.45921879089590706
Loss in iteration 110 : 0.4592218856397201
Loss in iteration 111 : 0.4592159649040987
Loss in iteration 112 : 0.4591998787227797
Loss in iteration 113 : 0.45919832549765316
Loss in iteration 114 : 0.4591976635299377
Loss in iteration 115 : 0.45918500952024766
Loss in iteration 116 : 0.45917858011387763
Loss in iteration 117 : 0.45917826669555073
Loss in iteration 118 : 0.45917067289052954
Loss in iteration 119 : 0.4591615916931047
Loss in iteration 120 : 0.4591592071167272
Loss in iteration 121 : 0.45915538490254987
Loss in iteration 122 : 0.4591466886297257
Loss in iteration 123 : 0.45914213287340716
Loss in iteration 124 : 0.45913987865691613
Loss in iteration 125 : 0.4591336669051693
Loss in iteration 126 : 0.4591280287178198
Loss in iteration 127 : 0.45912556013475375
Loss in iteration 128 : 0.4591218490185668
Loss in iteration 129 : 0.4591163725682693
Loss in iteration 130 : 0.459113005750287
Loss in iteration 131 : 0.45911056174940357
Loss in iteration 132 : 0.45910615895889767
Loss in iteration 133 : 0.459102176251557
Loss in iteration 134 : 0.4590997242712781
Loss in iteration 135 : 0.4590965357035445
Loss in iteration 136 : 0.45909260537566143
Loss in iteration 137 : 0.45908966087367115
Loss in iteration 138 : 0.4590872262886308
Loss in iteration 139 : 0.45908393457164287
Loss in iteration 140 : 0.45908077102479367
Loss in iteration 141 : 0.4590784674122596
Loss in iteration 142 : 0.4590759239762215
Loss in iteration 143 : 0.4590730131245223
Loss in iteration 144 : 0.45907055529917823
Loss in iteration 145 : 0.4590684627399585
Loss in iteration 146 : 0.45906606284393925
Loss in iteration 147 : 0.4590636021293337
Loss in iteration 148 : 0.4590615923652159
Loss in iteration 149 : 0.45905961239025195
Loss in iteration 150 : 0.45905741234348535
Testing accuracy  of updater 7 on alg 0 with rate 8.0 = 0.789625, training accuracy 0.789625, time elapsed: 3216 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6754111303928089
Loss in iteration 3 : 0.6575614308756933
Loss in iteration 4 : 0.6314556403415575
Loss in iteration 5 : 0.6087060712249006
Loss in iteration 6 : 0.5805144309980731
Loss in iteration 7 : 0.5602624860564213
Loss in iteration 8 : 0.5438801876718301
Loss in iteration 9 : 0.5297931876532943
Loss in iteration 10 : 0.5200971373468956
Loss in iteration 11 : 0.5102113954185192
Loss in iteration 12 : 0.5031020471413553
Loss in iteration 13 : 0.49885725633019845
Loss in iteration 14 : 0.49470582518465406
Loss in iteration 15 : 0.4920769111405868
Loss in iteration 16 : 0.49043382196553603
Loss in iteration 17 : 0.48821949454732366
Loss in iteration 18 : 0.48662811338289075
Loss in iteration 19 : 0.48573273295001446
Loss in iteration 20 : 0.484523103707661
Loss in iteration 21 : 0.4835866741677384
Loss in iteration 22 : 0.4831140611122445
Loss in iteration 23 : 0.4822678912627096
Loss in iteration 24 : 0.4812340144228269
Loss in iteration 25 : 0.48046738485884216
Loss in iteration 26 : 0.47956939254786346
Loss in iteration 27 : 0.47850828279010016
Loss in iteration 28 : 0.4776897197080686
Loss in iteration 29 : 0.47690797250019035
Loss in iteration 30 : 0.47593265621054853
Loss in iteration 31 : 0.4750341307199219
Loss in iteration 32 : 0.47422104644490864
Loss in iteration 33 : 0.4732804664302425
Loss in iteration 34 : 0.47236712005181025
Loss in iteration 35 : 0.47160647686078905
Loss in iteration 36 : 0.4708365856886584
Loss in iteration 37 : 0.4700720423268865
Loss in iteration 38 : 0.4694290930618239
Loss in iteration 39 : 0.468803060494923
Loss in iteration 40 : 0.4681484495158178
Loss in iteration 41 : 0.46758287771106755
Loss in iteration 42 : 0.46708789750615437
Loss in iteration 43 : 0.4665978610128076
Loss in iteration 44 : 0.466177648451319
Loss in iteration 45 : 0.4658328630105474
Loss in iteration 46 : 0.4654933325815657
Loss in iteration 47 : 0.4651864654018116
Loss in iteration 48 : 0.46493907552842034
Loss in iteration 49 : 0.4647030692802988
Loss in iteration 50 : 0.4644870535504258
Loss in iteration 51 : 0.4643239233718091
Loss in iteration 52 : 0.46417992827067256
Loss in iteration 53 : 0.4640414621389682
Loss in iteration 54 : 0.4639309414510574
Loss in iteration 55 : 0.46383029225088307
Loss in iteration 56 : 0.4637263640723848
Loss in iteration 57 : 0.4636407170993653
Loss in iteration 58 : 0.4635677794027437
Loss in iteration 59 : 0.4634922544379094
Loss in iteration 60 : 0.4634261839142463
Loss in iteration 61 : 0.46336850367729676
Loss in iteration 62 : 0.4633054205791326
Loss in iteration 63 : 0.46324421205894417
Loss in iteration 64 : 0.46318900480923664
Loss in iteration 65 : 0.4631310944914736
Loss in iteration 66 : 0.4630746207494936
Loss in iteration 67 : 0.4630240199186087
Loss in iteration 68 : 0.4629717309740888
Loss in iteration 69 : 0.4629180171384695
Loss in iteration 70 : 0.46286745259571466
Loss in iteration 71 : 0.4628161827803355
Loss in iteration 72 : 0.46276408305647443
Loss in iteration 73 : 0.4627154128690472
Loss in iteration 74 : 0.4626678400877307
Loss in iteration 75 : 0.4626198103594859
Loss in iteration 76 : 0.4625741035877872
Loss in iteration 77 : 0.46252951351109445
Loss in iteration 78 : 0.46248444668353705
Loss in iteration 79 : 0.4624411684202669
Loss in iteration 80 : 0.46239969622318733
Loss in iteration 81 : 0.46235865888389366
Loss in iteration 82 : 0.46231930928753334
Loss in iteration 83 : 0.46228170927390544
Loss in iteration 84 : 0.46224444825987987
Loss in iteration 85 : 0.462208209266591
Loss in iteration 86 : 0.46217351116677824
Loss in iteration 87 : 0.4621394654691135
Loss in iteration 88 : 0.46210638067234583
Loss in iteration 89 : 0.46207470667132133
Loss in iteration 90 : 0.4620436871901164
Loss in iteration 91 : 0.4620132641246529
Loss in iteration 92 : 0.46198380872371814
Loss in iteration 93 : 0.46195484342335114
Loss in iteration 94 : 0.4619262677274713
Loss in iteration 95 : 0.46189847579433757
Loss in iteration 96 : 0.46187121322438507
Loss in iteration 97 : 0.46184426931572625
Loss in iteration 98 : 0.46181786386693363
Loss in iteration 99 : 0.4617918363620591
Loss in iteration 100 : 0.4617660020963062
Loss in iteration 101 : 0.46174056471157776
Loss in iteration 102 : 0.46171549666447165
Loss in iteration 103 : 0.4616906432012251
Loss in iteration 104 : 0.46166612588392275
Loss in iteration 105 : 0.46164194359481053
Loss in iteration 106 : 0.46161795172117626
Loss in iteration 107 : 0.4615942140568477
Loss in iteration 108 : 0.4615707664602669
Loss in iteration 109 : 0.4615475215398676
Loss in iteration 110 : 0.46152452761279145
Loss in iteration 111 : 0.4615018291518172
Loss in iteration 112 : 0.46147935009221636
Loss in iteration 113 : 0.4614570971532934
Loss in iteration 114 : 0.461435107012132
Loss in iteration 115 : 0.46141333190437356
Loss in iteration 116 : 0.4613917744941494
Loss in iteration 117 : 0.4613704720425703
Loss in iteration 118 : 0.4613493934362938
Loss in iteration 119 : 0.4613285286181504
Loss in iteration 120 : 0.4613079008212125
Loss in iteration 121 : 0.46128748569345773
Loss in iteration 122 : 0.4612672685318962
Loss in iteration 123 : 0.4612472683329343
Loss in iteration 124 : 0.4612274735648102
Loss in iteration 125 : 0.4612078718221431
Loss in iteration 126 : 0.46118847505177785
Loss in iteration 127 : 0.46116927314099565
Loss in iteration 128 : 0.4611502511722427
Loss in iteration 129 : 0.46113141588475404
Loss in iteration 130 : 0.46111276243757243
Loss in iteration 131 : 0.4610942798726794
Loss in iteration 132 : 0.4610759727970553
Loss in iteration 133 : 0.46105783845020115
Loss in iteration 134 : 0.4610398671156687
Loss in iteration 135 : 0.4610220602982841
Loss in iteration 136 : 0.46100441586251484
Loss in iteration 137 : 0.4609869260501887
Loss in iteration 138 : 0.4609695919364735
Loss in iteration 139 : 0.46095241334051
Loss in iteration 140 : 0.4609353846564734
Loss in iteration 141 : 0.46091850562098274
Loss in iteration 142 : 0.46090177554350886
Loss in iteration 143 : 0.46088518964829256
Loss in iteration 144 : 0.4608687472505053
Loss in iteration 145 : 0.46085244812776366
Loss in iteration 146 : 0.4608362887840331
Loss in iteration 147 : 0.46082026834134787
Loss in iteration 148 : 0.46080438650704014
Loss in iteration 149 : 0.4607886403204066
Loss in iteration 150 : 0.46077302844946294
Loss in iteration 151 : 0.4607575503313199
Loss in iteration 152 : 0.46074220360786367
Loss in iteration 153 : 0.46072698710486715
Loss in iteration 154 : 0.46071190033155085
Loss in iteration 155 : 0.4606969413050463
Loss in iteration 156 : 0.4606821086844888
Loss in iteration 157 : 0.46066740173864573
Loss in iteration 158 : 0.460652818715744
Loss in iteration 159 : 0.4606383583256718
Loss in iteration 160 : 0.46062401982702444
Loss in iteration 161 : 0.4606098017414962
Loss in iteration 162 : 0.4605957028549574
Loss in iteration 163 : 0.4605817223586012
Loss in iteration 164 : 0.4605678588789517
Loss in iteration 165 : 0.4605541112013299
Loss in iteration 166 : 0.4605404784716248
Loss in iteration 167 : 0.46052695947552436
Loss in iteration 168 : 0.46051355311048076
Loss in iteration 169 : 0.46050025853588755
Loss in iteration 170 : 0.4604870746234852
Loss in iteration 171 : 0.4604740002993758
Loss in iteration 172 : 0.46046103470057553
Loss in iteration 173 : 0.46044817677697314
Loss in iteration 174 : 0.46043542552492206
Loss in iteration 175 : 0.46042278010850984
Loss in iteration 176 : 0.46041023955926286
Loss in iteration 177 : 0.4603978029326819
Loss in iteration 178 : 0.4603854694002671
Loss in iteration 179 : 0.46037323803262303
Loss in iteration 180 : 0.4603611079217484
Loss in iteration 181 : 0.46034907826019356
Loss in iteration 182 : 0.4603371481770949
Loss in iteration 183 : 0.4603253168150695
Loss in iteration 184 : 0.4603135833844798
Loss in iteration 185 : 0.46030194704604843
Loss in iteration 186 : 0.4602904069720755
Loss in iteration 187 : 0.4602789623889963
Loss in iteration 188 : 0.4602676124911447
Loss in iteration 189 : 0.4602563564855891
Loss in iteration 190 : 0.4602451936219907
Loss in iteration 191 : 0.46023412312672307
Loss in iteration 192 : 0.4602231442341233
Loss in iteration 193 : 0.46021225620848893
Loss in iteration 194 : 0.4602014582990436
Loss in iteration 195 : 0.46019074976582547
Loss in iteration 196 : 0.4601801298941291
Loss in iteration 197 : 0.4601695979593779
Loss in iteration 198 : 0.4601591532456856
Loss in iteration 199 : 0.460148795055717
Loss in iteration 200 : 0.46013852268574224
Loss in iteration 201 : 0.4601283354405715
Loss in iteration 202 : 0.46011823264034873
Loss in iteration 203 : 0.4601082136026962
Loss in iteration 204 : 0.46009827765407363
Loss in iteration 205 : 0.46008842413311385
Loss in iteration 206 : 0.4600786523770395
Loss in iteration 207 : 0.46006896173060263
Loss in iteration 208 : 0.46005935154851363
Loss in iteration 209 : 0.4600498211863168
Loss in iteration 210 : 0.4600403700073932
Loss in iteration 211 : 0.46003099738364184
Loss in iteration 212 : 0.4600217026885393
Loss in iteration 213 : 0.4600124853024678
Loss in iteration 214 : 0.460003344612865
Loss in iteration 215 : 0.45999428000954995
Loss in iteration 216 : 0.4599852908890408
Loss in iteration 217 : 0.4599763766542509
Loss in iteration 218 : 0.4599675367111895
Loss in iteration 219 : 0.4599587704720495
Loss in iteration 220 : 0.4599500773544614
Loss in iteration 221 : 0.4599414567792967
Loss in iteration 222 : 0.45993290817321647
Loss in iteration 223 : 0.4599244309680205
Loss in iteration 224 : 0.459916024599168
Loss in iteration 225 : 0.45990768850756114
Loss in iteration 226 : 0.4598994221387396
Loss in iteration 227 : 0.45989122494198786
Loss in iteration 228 : 0.45988309637168456
Loss in iteration 229 : 0.459875035886579
Loss in iteration 230 : 0.4598670429492787
Loss in iteration 231 : 0.45985911702723836
Loss in iteration 232 : 0.45985125759210704
Loss in iteration 233 : 0.45984346411941474
Loss in iteration 234 : 0.4598357360892353
Loss in iteration 235 : 0.4598280729856185
Loss in iteration 236 : 0.459820474296487
Loss in iteration 237 : 0.45981293951410557
Loss in iteration 238 : 0.45980546813460527
Loss in iteration 239 : 0.45979805965796167
Loss in iteration 240 : 0.4597907135882894
Loss in iteration 241 : 0.4597834294334393
Loss in iteration 242 : 0.4597762067050377
Loss in iteration 243 : 0.4597690449186523
Loss in iteration 244 : 0.4597619435934978
Loss in iteration 245 : 0.4597549022525144
Loss in iteration 246 : 0.45974792042244506
Loss in iteration 247 : 0.45974099763358617
Loss in iteration 248 : 0.45973413341986114
Loss in iteration 249 : 0.4597273273188353
Loss in iteration 250 : 0.4597205788715507
Loss in iteration 251 : 0.45971388762260174
Loss in iteration 252 : 0.45970725312010713
Loss in iteration 253 : 0.4597006749155917
Loss in iteration 254 : 0.4596941525640323
Loss in iteration 255 : 0.4596876856238136
Loss in iteration 256 : 0.4596812736566479
Loss in iteration 257 : 0.45967491622761225
Loss in iteration 258 : 0.45966861290509786
Loss in iteration 259 : 0.45966236326075544
Loss in iteration 260 : 0.45965616686950694
Loss in iteration 261 : 0.4596500233094995
Loss in iteration 262 : 0.4596439321620584
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.79, training accuracy 0.79, time elapsed: 5275 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6761221352195868
Loss in iteration 3 : 0.6600290705575551
Loss in iteration 4 : 0.6352742557740024
Loss in iteration 5 : 0.6137483987966615
Loss in iteration 6 : 0.5879124929871773
Loss in iteration 7 : 0.5657942576575127
Loss in iteration 8 : 0.5501585809066623
Loss in iteration 9 : 0.5353461136687867
Loss in iteration 10 : 0.5247209988483438
Loss in iteration 11 : 0.5156633539308438
Loss in iteration 12 : 0.5070549853004397
Loss in iteration 13 : 0.5017413783868161
Loss in iteration 14 : 0.49779778156019183
Loss in iteration 15 : 0.4940884012767688
Loss in iteration 16 : 0.49189426849245654
Loss in iteration 17 : 0.4901541598103362
Loss in iteration 18 : 0.4880154040016268
Loss in iteration 19 : 0.48653797511799407
Loss in iteration 20 : 0.4855958566181206
Loss in iteration 21 : 0.48441523929657593
Loss in iteration 22 : 0.48345358429081386
Loss in iteration 23 : 0.48290369857007187
Loss in iteration 24 : 0.48210878736617796
Loss in iteration 25 : 0.481091006275093
Loss in iteration 26 : 0.48028748075272176
Loss in iteration 27 : 0.4794876588264114
Loss in iteration 28 : 0.4785053623070486
Loss in iteration 29 : 0.47764878395655386
Loss in iteration 30 : 0.4769388565549576
Loss in iteration 31 : 0.47609915811586667
Loss in iteration 32 : 0.4752120274256059
Loss in iteration 33 : 0.4744452856891112
Loss in iteration 34 : 0.47365667638158415
Loss in iteration 35 : 0.4727948351495859
Loss in iteration 36 : 0.4720290510886522
Loss in iteration 37 : 0.4713538844038571
Loss in iteration 38 : 0.47065445110245685
Loss in iteration 39 : 0.4699921028518526
Loss in iteration 40 : 0.4694201072879248
Loss in iteration 41 : 0.46884764900161374
Loss in iteration 42 : 0.46826770071421187
Loss in iteration 43 : 0.4677642502614775
Loss in iteration 44 : 0.46731595864471953
Loss in iteration 45 : 0.4668760742710135
Loss in iteration 46 : 0.46648809378623274
Loss in iteration 47 : 0.46616239351015004
Loss in iteration 48 : 0.46584800870836635
Loss in iteration 49 : 0.46555254853697037
Loss in iteration 50 : 0.4653054607080113
Loss in iteration 51 : 0.465079738431399
Loss in iteration 52 : 0.46486355074478
Loss in iteration 53 : 0.46468447777781297
Loss in iteration 54 : 0.4645346167821771
Loss in iteration 55 : 0.4643893076323102
Loss in iteration 56 : 0.46425888303512763
Loss in iteration 57 : 0.46414802928890747
Loss in iteration 58 : 0.46403903758896026
Loss in iteration 59 : 0.463935021995732
Loss in iteration 60 : 0.4638476463806745
Loss in iteration 61 : 0.46376667551717593
Loss in iteration 62 : 0.4636862572707481
Loss in iteration 63 : 0.4636149020210221
Loss in iteration 64 : 0.46354901131956044
Loss in iteration 65 : 0.46348063653055616
Loss in iteration 66 : 0.4634151184922148
Loss in iteration 67 : 0.4633548781376654
Loss in iteration 68 : 0.4632942650157562
Loss in iteration 69 : 0.4632349813020164
Loss in iteration 70 : 0.4631805118654502
Loss in iteration 71 : 0.463126571770317
Loss in iteration 72 : 0.4630715843050435
Loss in iteration 73 : 0.4630189705663482
Loss in iteration 74 : 0.4629677061088062
Loss in iteration 75 : 0.4629159144337284
Loss in iteration 76 : 0.4628660385025788
Loss in iteration 77 : 0.46281852368032717
Loss in iteration 78 : 0.4627711714986282
Loss in iteration 79 : 0.46272470021003753
Loss in iteration 80 : 0.46268016057269695
Loss in iteration 81 : 0.46263611360202483
Loss in iteration 82 : 0.46259249648863743
Loss in iteration 83 : 0.46255064657597156
Loss in iteration 84 : 0.4625100608765811
Loss in iteration 85 : 0.4624701335467099
Loss in iteration 86 : 0.46243163741699844
Loss in iteration 87 : 0.46239443766986754
Loss in iteration 88 : 0.4623577522285641
Loss in iteration 89 : 0.46232198291717985
Loss in iteration 90 : 0.4622874749236339
Loss in iteration 91 : 0.46225371963448936
Loss in iteration 92 : 0.4622207471653406
Loss in iteration 93 : 0.4621888929609302
Loss in iteration 94 : 0.46215780483101315
Loss in iteration 95 : 0.46212725053945736
Loss in iteration 96 : 0.4620974770801547
Loss in iteration 97 : 0.4620683719844992
Loss in iteration 98 : 0.4620396930168607
Loss in iteration 99 : 0.46201161210360125
Loss in iteration 100 : 0.46198417771406775
Loss in iteration 101 : 0.4619571597976378
Loss in iteration 102 : 0.46193056325146103
Loss in iteration 103 : 0.4619044606912876
Loss in iteration 104 : 0.4618787059802595
Loss in iteration 105 : 0.4618532615286926
Loss in iteration 106 : 0.4618282304679559
Loss in iteration 107 : 0.46180355573086057
Loss in iteration 108 : 0.4617791625645358
Loss in iteration 109 : 0.4617551106588762
Loss in iteration 110 : 0.46173138598231334
Loss in iteration 111 : 0.46170790790959654
Loss in iteration 112 : 0.4616847004472927
Loss in iteration 113 : 0.46166178799530294
Loss in iteration 114 : 0.46163912218152986
Loss in iteration 115 : 0.4616167068697286
Loss in iteration 116 : 0.46159457224815037
Loss in iteration 117 : 0.4615726836671372
Loss in iteration 118 : 0.4615510198162355
Loss in iteration 119 : 0.4615296031259029
Loss in iteration 120 : 0.46150842176342277
Loss in iteration 121 : 0.4614874547776064
Loss in iteration 122 : 0.46146671793852967
Loss in iteration 123 : 0.46144621174235445
Loss in iteration 124 : 0.4614259148226657
Loss in iteration 125 : 0.4614058297363499
Loss in iteration 126 : 0.4613859599268993
Loss in iteration 127 : 0.4613662884129917
Loss in iteration 128 : 0.4613468115586119
Loss in iteration 129 : 0.461327535773
Loss in iteration 130 : 0.46130845183653413
Loss in iteration 131 : 0.4612895527164511
Loss in iteration 132 : 0.46127084169509586
Loss in iteration 133 : 0.4612523122841933
Loss in iteration 134 : 0.46123395531242
Loss in iteration 135 : 0.46121577180398915
Loss in iteration 136 : 0.46119775949447783
Loss in iteration 137 : 0.4611799109865373
Loss in iteration 138 : 0.4611622254125308
Loss in iteration 139 : 0.46114470191345036
Loss in iteration 140 : 0.46112733427143543
Loss in iteration 141 : 0.4611101197550557
Loss in iteration 142 : 0.4610930578027834
Loss in iteration 143 : 0.46107614395812685
Loss in iteration 144 : 0.4610593752226787
Loss in iteration 145 : 0.46104275149992996
Loss in iteration 146 : 0.46102627003960006
Loss in iteration 147 : 0.46100992760819676
Loss in iteration 148 : 0.460993723376248
Loss in iteration 149 : 0.46097765530951246
Loss in iteration 150 : 0.46096172044670597
Loss in iteration 151 : 0.4609459177636195
Loss in iteration 152 : 0.46093024599142307
Loss in iteration 153 : 0.46091470267691353
Loss in iteration 154 : 0.46089928651814044
Loss in iteration 155 : 0.4608839964829917
Loss in iteration 156 : 0.46086883046518284
Loss in iteration 157 : 0.46085378691445744
Loss in iteration 158 : 0.460838864865634
Loss in iteration 159 : 0.4608240626611732
Loss in iteration 160 : 0.4608093788270721
Loss in iteration 161 : 0.46079481246135784
Loss in iteration 162 : 0.460780362181837
Loss in iteration 163 : 0.460766026499204
Loss in iteration 164 : 0.4607518044217934
Loss in iteration 165 : 0.4607376947617718
Loss in iteration 166 : 0.46072369615022923
Loss in iteration 167 : 0.4607098075958538
Loss in iteration 168 : 0.4606960280733549
Loss in iteration 169 : 0.4606823563444583
Loss in iteration 170 : 0.46066879140224154
Loss in iteration 171 : 0.4606553322892976
Loss in iteration 172 : 0.4606419778615838
Loss in iteration 173 : 0.4606287271127603
Loss in iteration 174 : 0.46061557914930557
Loss in iteration 175 : 0.460602532951095
Loss in iteration 176 : 0.460589587556604
Loss in iteration 177 : 0.46057674211134714
Loss in iteration 178 : 0.4605639956717354
Loss in iteration 179 : 0.46055134730494807
Loss in iteration 180 : 0.4605387961810346
Loss in iteration 181 : 0.46052634143002863
Loss in iteration 182 : 0.4605139821722901
Loss in iteration 183 : 0.46050171761140496
Loss in iteration 184 : 0.46048954693884686
Loss in iteration 185 : 0.460477469323572
Loss in iteration 186 : 0.4604654839923308
Loss in iteration 187 : 0.4604535901763915
Loss in iteration 188 : 0.4604417870862685
Loss in iteration 189 : 0.46043007397671293
Loss in iteration 190 : 0.460418450120701
Loss in iteration 191 : 0.4604069147736236
Loss in iteration 192 : 0.4603954672168305
Loss in iteration 193 : 0.46038410675122016
Loss in iteration 194 : 0.46037283266574364
Loss in iteration 195 : 0.46036164426652426
Loss in iteration 196 : 0.4603505408809781
Loss in iteration 197 : 0.46033952183090027
Loss in iteration 198 : 0.46032858644889985
Loss in iteration 199 : 0.4603177340869273
Loss in iteration 200 : 0.4603069640948452
Loss in iteration 201 : 0.4602962758280393
Loss in iteration 202 : 0.46028566865820403
Loss in iteration 203 : 0.4602751419590977
Loss in iteration 204 : 0.46026469510903145
Loss in iteration 205 : 0.4602543275003726
Loss in iteration 206 : 0.46024403852936596
Loss in iteration 207 : 0.4602338275951894
Loss in iteration 208 : 0.4602236941082124
Loss in iteration 209 : 0.46021363748396815
Loss in iteration 210 : 0.4602036571408358
Loss in iteration 211 : 0.46019375250654937
Loss in iteration 212 : 0.4601839230148655
Loss in iteration 213 : 0.46017416810264244
Loss in iteration 214 : 0.4601644872144109
Loss in iteration 215 : 0.4601548798006137
Loss in iteration 216 : 0.460145345314712
Loss in iteration 217 : 0.46013588321656124
Loss in iteration 218 : 0.46012649297196834
Loss in iteration 219 : 0.46011717405022634
Loss in iteration 220 : 0.46010792592623984
Loss in iteration 221 : 0.4600987480805201
Loss in iteration 222 : 0.4600896399970866
Loss in iteration 223 : 0.4600806011647913
Loss in iteration 224 : 0.4600716310777198
Loss in iteration 225 : 0.46006272923360597
Loss in iteration 226 : 0.46005389513462885
Loss in iteration 227 : 0.46004512828791744
Loss in iteration 228 : 0.46003642820430274
Loss in iteration 229 : 0.4600277943986662
Loss in iteration 230 : 0.4600192263903999
Loss in iteration 231 : 0.4600107237025112
Loss in iteration 232 : 0.4600022858618256
Loss in iteration 233 : 0.4599939123994346
Loss in iteration 234 : 0.45998560285005025
Loss in iteration 235 : 0.4599773567519943
Loss in iteration 236 : 0.45996917364754575
Loss in iteration 237 : 0.45996105308248014
Loss in iteration 238 : 0.45995299460602196
Loss in iteration 239 : 0.45994499777113446
Loss in iteration 240 : 0.459937062134194
Loss in iteration 241 : 0.45992918725490595
Loss in iteration 242 : 0.45992137269651495
Loss in iteration 243 : 0.45991361802557135
Loss in iteration 244 : 0.4599059228118323
Loss in iteration 245 : 0.45989828662839854
Loss in iteration 246 : 0.4598907090515804
Loss in iteration 247 : 0.45988318966078645
Loss in iteration 248 : 0.45987572803863774
Loss in iteration 249 : 0.4598683237708523
Loss in iteration 250 : 0.4598609764461508
Loss in iteration 251 : 0.4598536856563335
Loss in iteration 252 : 0.45984645099619936
Loss in iteration 253 : 0.4598392720634595
Loss in iteration 254 : 0.45983214845878045
Loss in iteration 255 : 0.45982507978573617
Loss in iteration 256 : 0.4598180656507302
Loss in iteration 257 : 0.45981110566302796
Loss in iteration 258 : 0.45980419943470735
Loss in iteration 259 : 0.45979734658059895
Loss in iteration 260 : 0.45979054671828873
Loss in iteration 261 : 0.45978379946809766
Loss in iteration 262 : 0.4597771044530132
Loss in iteration 263 : 0.4597704612987102
Loss in iteration 264 : 0.4597638696335148
Testing accuracy  of updater 7 on alg 0 with rate 1.4 = 0.78975, training accuracy 0.78975, time elapsed: 6038 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.678819345106268
Loss in iteration 3 : 0.6659574089404837
Loss in iteration 4 : 0.6474674042005434
Loss in iteration 5 : 0.6271174771234571
Loss in iteration 6 : 0.6085699887142355
Loss in iteration 7 : 0.5871400101779775
Loss in iteration 8 : 0.5685055405617131
Loss in iteration 9 : 0.5549542765389094
Loss in iteration 10 : 0.5424056889236936
Loss in iteration 11 : 0.53159346608794
Loss in iteration 12 : 0.5234566830300614
Loss in iteration 13 : 0.5158743951013662
Loss in iteration 14 : 0.5089693762451708
Loss in iteration 15 : 0.504170428950296
Loss in iteration 16 : 0.5006194565416905
Loss in iteration 17 : 0.49722992694129603
Loss in iteration 18 : 0.49450512170672856
Loss in iteration 19 : 0.49263428595700753
Loss in iteration 20 : 0.49083389114967363
Loss in iteration 21 : 0.48893858706258503
Loss in iteration 22 : 0.4874769358048503
Loss in iteration 23 : 0.4864282030206559
Loss in iteration 24 : 0.48535663795942424
Loss in iteration 25 : 0.48429080809447445
Loss in iteration 26 : 0.4834889297527813
Loss in iteration 27 : 0.48280845717547866
Loss in iteration 28 : 0.4819792454398641
Loss in iteration 29 : 0.48108772491344404
Loss in iteration 30 : 0.48032713923929143
Loss in iteration 31 : 0.47962723724074363
Loss in iteration 32 : 0.47885941707919144
Loss in iteration 33 : 0.47810195154609186
Loss in iteration 34 : 0.4774547303188358
Loss in iteration 35 : 0.4768390332978274
Loss in iteration 36 : 0.47616409641559254
Loss in iteration 37 : 0.4754870295578881
Loss in iteration 38 : 0.4748741661941598
Loss in iteration 39 : 0.47427880799492356
Loss in iteration 40 : 0.47365686413640534
Loss in iteration 41 : 0.4730575392597091
Loss in iteration 42 : 0.47252276176690805
Loss in iteration 43 : 0.472015731975233
Loss in iteration 44 : 0.4715047205142781
Loss in iteration 45 : 0.4710176615265128
Loss in iteration 46 : 0.4705733308505014
Loss in iteration 47 : 0.47014344599175845
Loss in iteration 48 : 0.46971307036673854
Loss in iteration 49 : 0.4693083399063405
Loss in iteration 50 : 0.4689436497986801
Loss in iteration 51 : 0.46859971026255903
Loss in iteration 52 : 0.46826664587760797
Loss in iteration 53 : 0.4679589977765786
Loss in iteration 54 : 0.4676804047374438
Loss in iteration 55 : 0.46741519728447134
Loss in iteration 56 : 0.46715869596755333
Loss in iteration 57 : 0.4669218516519886
Loss in iteration 58 : 0.46670599722153966
Loss in iteration 59 : 0.466501053380557
Loss in iteration 60 : 0.4663060324748393
Loss in iteration 61 : 0.46612814960492505
Loss in iteration 62 : 0.4659657499636487
Loss in iteration 63 : 0.46581062363271164
Loss in iteration 64 : 0.4656620623076227
Loss in iteration 65 : 0.46552415509299644
Loss in iteration 66 : 0.46539508744399477
Loss in iteration 67 : 0.4652705566371705
Loss in iteration 68 : 0.4651518252672152
Loss in iteration 69 : 0.46504162423797885
Loss in iteration 70 : 0.4649377783939386
Loss in iteration 71 : 0.4648372389481183
Loss in iteration 72 : 0.4647409372068745
Loss in iteration 73 : 0.4646500959832
Loss in iteration 74 : 0.4645628252181486
Loss in iteration 75 : 0.4644775831393606
Loss in iteration 76 : 0.46439554033972474
Loss in iteration 77 : 0.4643174470274958
Loss in iteration 78 : 0.46424201405152454
Loss in iteration 79 : 0.46416854589068796
Loss in iteration 80 : 0.4640978432637995
Loss in iteration 81 : 0.4640299403954409
Loss in iteration 82 : 0.4639637191959229
Loss in iteration 83 : 0.46389888858242223
Loss in iteration 84 : 0.46383610994951974
Loss in iteration 85 : 0.46377537682944003
Loss in iteration 86 : 0.46371607245944063
Loss in iteration 87 : 0.46365821340452407
Loss in iteration 88 : 0.4636021974152926
Loss in iteration 89 : 0.46354781646953414
Loss in iteration 90 : 0.4634946066808791
Loss in iteration 91 : 0.4634426292923139
Loss in iteration 92 : 0.4633921053062657
Loss in iteration 93 : 0.46334284185672786
Loss in iteration 94 : 0.46329460590772326
Loss in iteration 95 : 0.4632475312616053
Loss in iteration 96 : 0.4632017459147309
Loss in iteration 97 : 0.46315707549526
Loss in iteration 98 : 0.46311337419008036
Loss in iteration 99 : 0.46307071942250766
Loss in iteration 100 : 0.4630291265922655
Loss in iteration 101 : 0.4629884446916084
Loss in iteration 102 : 0.46294861259657727
Loss in iteration 103 : 0.46290970923856994
Loss in iteration 104 : 0.46287173068950477
Loss in iteration 105 : 0.46283456983054705
Loss in iteration 106 : 0.46279819343138934
Loss in iteration 107 : 0.4627626334229145
Loss in iteration 108 : 0.46272784825520064
Loss in iteration 109 : 0.46269375424137693
Loss in iteration 110 : 0.46266033693482256
Loss in iteration 111 : 0.4626276114496218
Loss in iteration 112 : 0.46259553815534266
Loss in iteration 113 : 0.46256406626789803
Loss in iteration 114 : 0.4625331938996442
Loss in iteration 115 : 0.4625029211503158
Loss in iteration 116 : 0.4624732062702378
Loss in iteration 117 : 0.4624440103713706
Loss in iteration 118 : 0.46241532856700585
Loss in iteration 119 : 0.46238715166957967
Loss in iteration 120 : 0.4623594480689516
Loss in iteration 121 : 0.46233219715363405
Loss in iteration 122 : 0.4623053973524865
Loss in iteration 123 : 0.46227903617288274
Loss in iteration 124 : 0.462253087505851
Loss in iteration 125 : 0.4622275364474833
Loss in iteration 126 : 0.46220237807951936
Loss in iteration 127 : 0.4621775979734524
Loss in iteration 128 : 0.4621531766516792
Loss in iteration 129 : 0.4621291049976567
Loss in iteration 130 : 0.4621053782112179
Loss in iteration 131 : 0.46208198396490524
Loss in iteration 132 : 0.4620589086788072
Loss in iteration 133 : 0.4620361455714768
Loss in iteration 134 : 0.46201368797833814
Loss in iteration 135 : 0.46199152404068367
Loss in iteration 136 : 0.46196964338116764
Loss in iteration 137 : 0.4619480406175443
Loss in iteration 138 : 0.4619267094204519
Loss in iteration 139 : 0.46190564050448746
Loss in iteration 140 : 0.46188482653639074
Loss in iteration 141 : 0.4618642626975039
Loss in iteration 142 : 0.46184394248302413
Loss in iteration 143 : 0.4618238579719381
Loss in iteration 144 : 0.46180400324942067
Loss in iteration 145 : 0.4617843736505845
Loss in iteration 146 : 0.46176496316686344
Loss in iteration 147 : 0.4617457655466172
Loss in iteration 148 : 0.46172677617767033
Loss in iteration 149 : 0.4617079907762089
Loss in iteration 150 : 0.4616894040017311
Loss in iteration 151 : 0.4616710107027303
Loss in iteration 152 : 0.4616528067851596
Loss in iteration 153 : 0.46163478806332797
Loss in iteration 154 : 0.4616169498217372
Loss in iteration 155 : 0.4615992878695166
Loss in iteration 156 : 0.46158179869265015
Loss in iteration 157 : 0.4615644785301149
Loss in iteration 158 : 0.46154732338274096
Loss in iteration 159 : 0.46153032973046026
Loss in iteration 160 : 0.46151349437228834
Loss in iteration 161 : 0.4614968138757191
Loss in iteration 162 : 0.46148028482064984
Loss in iteration 163 : 0.4614639042009801
Loss in iteration 164 : 0.46144766914613583
Loss in iteration 165 : 0.46143157664917034
Loss in iteration 166 : 0.46141562384038876
Loss in iteration 167 : 0.4613998081352571
Loss in iteration 168 : 0.46138412696031456
Loss in iteration 169 : 0.46136857767409467
Loss in iteration 170 : 0.46135315780125236
Loss in iteration 171 : 0.4613378650504203
Loss in iteration 172 : 0.4613226971238499
Loss in iteration 173 : 0.4613076517419371
Loss in iteration 174 : 0.4612927267845232
Loss in iteration 175 : 0.46127792022925335
Loss in iteration 176 : 0.4612632300404598
Loss in iteration 177 : 0.461248654236862
Loss in iteration 178 : 0.4612341909619129
Loss in iteration 179 : 0.46121983840878533
Loss in iteration 180 : 0.4612055947765157
Loss in iteration 181 : 0.4611914583384509
Loss in iteration 182 : 0.46117742746013163
Loss in iteration 183 : 0.4611635005360995
Loss in iteration 184 : 0.46114967598458356
Loss in iteration 185 : 0.46113595229382087
Loss in iteration 186 : 0.46112232800991493
Loss in iteration 187 : 0.4611088016981223
Loss in iteration 188 : 0.46109537195968586
Loss in iteration 189 : 0.46108203745765464
Loss in iteration 190 : 0.46106879689486546
Loss in iteration 191 : 0.46105564899519785
Loss in iteration 192 : 0.461042592522557
Loss in iteration 193 : 0.4610296262880598
Loss in iteration 194 : 0.46101674912969853
Loss in iteration 195 : 0.46100395990868187
Loss in iteration 196 : 0.46099125752438497
Loss in iteration 197 : 0.46097864091138646
Loss in iteration 198 : 0.4609661090260725
Loss in iteration 199 : 0.46095366085056577
Loss in iteration 200 : 0.4609412954006826
Loss in iteration 201 : 0.46092901171858125
Loss in iteration 202 : 0.46091680886590375
Loss in iteration 203 : 0.46090468592933254
Loss in iteration 204 : 0.46089264202277147
Loss in iteration 205 : 0.4608806762805852
Loss in iteration 206 : 0.4608687878561352
Loss in iteration 207 : 0.46085697592621844
Loss in iteration 208 : 0.46084523968960356
Loss in iteration 209 : 0.46083357836226346
Loss in iteration 210 : 0.46082199117832945
Loss in iteration 211 : 0.4608104773922585
Loss in iteration 212 : 0.4607990362760699
Loss in iteration 213 : 0.46078766711698826
Loss in iteration 214 : 0.4607763692190817
Loss in iteration 215 : 0.4607651419035165
Loss in iteration 216 : 0.4607539845060877
Loss in iteration 217 : 0.4607428963766362
Loss in iteration 218 : 0.46073187688017625
Loss in iteration 219 : 0.4607209253959658
Loss in iteration 220 : 0.46071004131577264
Loss in iteration 221 : 0.4606992240440995
Loss in iteration 222 : 0.46068847299858545
Loss in iteration 223 : 0.4606777876088535
Loss in iteration 224 : 0.46066716731567026
Loss in iteration 225 : 0.46065661157133814
Loss in iteration 226 : 0.4606461198394446
Loss in iteration 227 : 0.4606356915938339
Loss in iteration 228 : 0.46062532631834474
Loss in iteration 229 : 0.4606150235069797
Loss in iteration 230 : 0.46060478266334576
Loss in iteration 231 : 0.4605946033000064
Loss in iteration 232 : 0.46058448493849447
Loss in iteration 233 : 0.4605744271092267
Loss in iteration 234 : 0.4605644293509353
Loss in iteration 235 : 0.4605544912103199
Loss in iteration 236 : 0.4605446122420435
Loss in iteration 237 : 0.4605347920084483
Loss in iteration 238 : 0.46052503007912526
Loss in iteration 239 : 0.46051532603076656
Loss in iteration 240 : 0.4605056794470976
Loss in iteration 241 : 0.4604960899185387
Loss in iteration 242 : 0.4604865570419284
Loss in iteration 243 : 0.4604770804204361
Loss in iteration 244 : 0.46046765966339365
Loss in iteration 245 : 0.46045829438600744
Loss in iteration 246 : 0.4604489842091948
Loss in iteration 247 : 0.46043972875948397
Loss in iteration 248 : 0.4604305276688217
Loss in iteration 249 : 0.46042138057435705
Loss in iteration 250 : 0.4604122871183315
Loss in iteration 251 : 0.4604032469479627
Loss in iteration 252 : 0.46039425971525194
Loss in iteration 253 : 0.46038532507683144
Loss in iteration 254 : 0.46037644269388317
Loss in iteration 255 : 0.46036761223199346
Loss in iteration 256 : 0.4603588333609989
Loss in iteration 257 : 0.4603501057548907
Loss in iteration 258 : 0.4603414290917026
Loss in iteration 259 : 0.46033280305339597
Loss in iteration 260 : 0.46032422732573275
Loss in iteration 261 : 0.46031570159819535
Loss in iteration 262 : 0.46030722556388515
Loss in iteration 263 : 0.4602987989194084
Loss in iteration 264 : 0.4602904213647923
Loss in iteration 265 : 0.46028209260339553
Loss in iteration 266 : 0.4602738123418211
Testing accuracy  of updater 7 on alg 0 with rate 0.8 = 0.789, training accuracy 0.789, time elapsed: 5494 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6873782410935
Loss in iteration 3 : 0.6795131261804124
Loss in iteration 4 : 0.6720593712310007
Loss in iteration 5 : 0.664553650468758
Loss in iteration 6 : 0.6556327083177605
Loss in iteration 7 : 0.6453114290272554
Loss in iteration 8 : 0.6348083868200572
Loss in iteration 9 : 0.6249427684215509
Loss in iteration 10 : 0.6153990253363513
Loss in iteration 11 : 0.6055473408427161
Loss in iteration 12 : 0.595479981460315
Loss in iteration 13 : 0.5859684283999352
Loss in iteration 14 : 0.5776294947783496
Loss in iteration 15 : 0.5704104367208128
Loss in iteration 16 : 0.5638411775228097
Loss in iteration 17 : 0.5575784077282454
Loss in iteration 18 : 0.5516411487926579
Loss in iteration 19 : 0.5462263886593353
Loss in iteration 20 : 0.5414134047312513
Loss in iteration 21 : 0.5370730994132489
Loss in iteration 22 : 0.5330148828697825
Loss in iteration 23 : 0.5291661679221564
Loss in iteration 24 : 0.5256002495232289
Loss in iteration 25 : 0.5224197906064101
Loss in iteration 26 : 0.5196372526556797
Loss in iteration 27 : 0.5171612342886094
Loss in iteration 28 : 0.5148755785695596
Loss in iteration 29 : 0.5127206322559547
Loss in iteration 30 : 0.5107075407948404
Loss in iteration 31 : 0.5088707381252636
Loss in iteration 32 : 0.5072145457295824
Loss in iteration 33 : 0.5057010044556155
Loss in iteration 34 : 0.5042796510485991
Loss in iteration 35 : 0.502923861069805
Loss in iteration 36 : 0.5016407719038624
Loss in iteration 37 : 0.500451664376693
Loss in iteration 38 : 0.49936532212486007
Loss in iteration 39 : 0.49836777257722403
Loss in iteration 40 : 0.4974330869740902
Loss in iteration 41 : 0.49654169581293905
Loss in iteration 42 : 0.4956896993403164
Loss in iteration 43 : 0.49488369209037025
Loss in iteration 44 : 0.49412854216918994
Loss in iteration 45 : 0.4934196287504435
Loss in iteration 46 : 0.4927449709772971
Loss in iteration 47 : 0.4920935186411407
Loss in iteration 48 : 0.49146162215292377
Loss in iteration 49 : 0.490852751283973
Loss in iteration 50 : 0.4902719252471325
Loss in iteration 51 : 0.48972023849794294
Loss in iteration 52 : 0.4891937031977648
Loss in iteration 53 : 0.4886863685086688
Loss in iteration 54 : 0.4881943379991576
Loss in iteration 55 : 0.4877173239256273
Loss in iteration 56 : 0.48725703094951245
Loss in iteration 57 : 0.486814319340422
Loss in iteration 58 : 0.486387671720942
Loss in iteration 59 : 0.48597394945169015
Loss in iteration 60 : 0.4855704093638026
Loss in iteration 61 : 0.4851761439197471
Loss in iteration 62 : 0.48479190896144997
Loss in iteration 63 : 0.48441876657744287
Loss in iteration 64 : 0.4840568147021802
Loss in iteration 65 : 0.4837049590659791
Loss in iteration 66 : 0.4833616839960549
Loss in iteration 67 : 0.4830260123741479
Loss in iteration 68 : 0.4826978622922231
Loss in iteration 69 : 0.4823776386622759
Loss in iteration 70 : 0.4820655293837713
Loss in iteration 71 : 0.4817611162173664
Loss in iteration 72 : 0.48146354713840916
Loss in iteration 73 : 0.4811720309402541
Loss in iteration 74 : 0.4808862088728687
Loss in iteration 75 : 0.48060613874931074
Loss in iteration 76 : 0.4803319788961678
Loss in iteration 77 : 0.48006367525540194
Loss in iteration 78 : 0.4798008929541151
Loss in iteration 79 : 0.4795431958228446
Loss in iteration 80 : 0.47929028290114156
Loss in iteration 81 : 0.47904208297676176
Loss in iteration 82 : 0.47879865708008984
Loss in iteration 83 : 0.4785600187186523
Loss in iteration 84 : 0.47832602594204254
Loss in iteration 85 : 0.47809641540401193
Loss in iteration 86 : 0.47787092581674767
Loss in iteration 87 : 0.4776493992440569
Loss in iteration 88 : 0.477431787223524
Loss in iteration 89 : 0.4772180762942842
Loss in iteration 90 : 0.47700820681581074
Loss in iteration 91 : 0.47680204942193266
Loss in iteration 92 : 0.47659944577778834
Loss in iteration 93 : 0.47640026865623963
Loss in iteration 94 : 0.4762044497958966
Loss in iteration 95 : 0.47601195902167126
Loss in iteration 96 : 0.47582275946509306
Loss in iteration 97 : 0.47563677792905024
Loss in iteration 98 : 0.47545391050359587
Loss in iteration 99 : 0.4752740524291335
Loss in iteration 100 : 0.4750971244504678
Loss in iteration 101 : 0.47492307587923177
Loss in iteration 102 : 0.474751866439808
Loss in iteration 103 : 0.47458344508832895
Loss in iteration 104 : 0.47441774290158856
Loss in iteration 105 : 0.47425468292968426
Loss in iteration 106 : 0.47409419612849796
Loss in iteration 107 : 0.4739362297851474
Loss in iteration 108 : 0.47378074323477914
Loss in iteration 109 : 0.4736276965158329
Loss in iteration 110 : 0.4734770420099064
Loss in iteration 111 : 0.47332872485857974
Loss in iteration 112 : 0.4731826900021637
Loss in iteration 113 : 0.47303888894606555
Loss in iteration 114 : 0.4728972809208263
Loss in iteration 115 : 0.47275782854483644
Loss in iteration 116 : 0.47262049242503457
Loss in iteration 117 : 0.4724852291568092
Loss in iteration 118 : 0.472351993692975
Loss in iteration 119 : 0.47222074343848
Loss in iteration 120 : 0.47209144054979474
Loss in iteration 121 : 0.47196405095872185
Loss in iteration 122 : 0.47183854146148135
Loss in iteration 123 : 0.4717148774644027
Loss in iteration 124 : 0.4715930229730388
Loss in iteration 125 : 0.47147294235993703
Loss in iteration 126 : 0.47135460214861696
Loss in iteration 127 : 0.47123797138005136
Loss in iteration 128 : 0.47112302051795013
Loss in iteration 129 : 0.47100972000946867
Loss in iteration 130 : 0.47089803968123
Loss in iteration 131 : 0.4707879492737292
Loss in iteration 132 : 0.47067941946650915
Loss in iteration 133 : 0.47057242248348896
Loss in iteration 134 : 0.4704669318704956
Loss in iteration 135 : 0.4703629217661575
Loss in iteration 136 : 0.47026036632613805
Loss in iteration 137 : 0.4701592397154417
Loss in iteration 138 : 0.4700595165564596
Loss in iteration 139 : 0.4699611723801161
Loss in iteration 140 : 0.4698641837078844
Loss in iteration 141 : 0.4697685277520249
Loss in iteration 142 : 0.4696741820249841
Loss in iteration 143 : 0.46958112416716435
Loss in iteration 144 : 0.46948933207405524
Loss in iteration 145 : 0.4693987841545943
Loss in iteration 146 : 0.46930945948301317
Loss in iteration 147 : 0.46922133773697056
Loss in iteration 148 : 0.4691343990051076
Loss in iteration 149 : 0.4690486236350625
Loss in iteration 150 : 0.46896399222856344
Loss in iteration 151 : 0.4688804857529759
Loss in iteration 152 : 0.46879808565139613
Loss in iteration 153 : 0.4687167738559532
Loss in iteration 154 : 0.46863653270337957
Loss in iteration 155 : 0.46855734482963984
Loss in iteration 156 : 0.4684791931230107
Loss in iteration 157 : 0.46840206075423907
Loss in iteration 158 : 0.4683259312382496
Loss in iteration 159 : 0.4682507884659068
Loss in iteration 160 : 0.468176616680125
Loss in iteration 161 : 0.4681034004201026
Loss in iteration 162 : 0.4680311244786819
Loss in iteration 163 : 0.46795977389913834
Loss in iteration 164 : 0.4678893340015456
Loss in iteration 165 : 0.4678197904072066
Loss in iteration 166 : 0.4677511290371282
Loss in iteration 167 : 0.46768333608576573
Loss in iteration 168 : 0.4676163979907867
Loss in iteration 169 : 0.467550301418878
Loss in iteration 170 : 0.46748503327110563
Loss in iteration 171 : 0.46742058069512665
Loss in iteration 172 : 0.4673569310884984
Loss in iteration 173 : 0.4672940720876595
Loss in iteration 174 : 0.467231991549799
Loss in iteration 175 : 0.4671706775394174
Loss in iteration 176 : 0.4671101183256656
Loss in iteration 177 : 0.4670503023869126
Loss in iteration 178 : 0.4669912184140128
Loss in iteration 179 : 0.4669328553064556
Loss in iteration 180 : 0.46687520216241807
Loss in iteration 181 : 0.4668182482684818
Loss in iteration 182 : 0.4667619830939905
Loss in iteration 183 : 0.4667063962903796
Loss in iteration 184 : 0.46665147769184046
Loss in iteration 185 : 0.4665972173133551
Loss in iteration 186 : 0.4665436053450956
Loss in iteration 187 : 0.4664906321454479
Loss in iteration 188 : 0.4664382882356536
Loss in iteration 189 : 0.46638656429736325
Loss in iteration 190 : 0.4663354511718014
Loss in iteration 191 : 0.46628493985830727
Loss in iteration 192 : 0.46623502151097557
Loss in iteration 193 : 0.46618568743392164
Loss in iteration 194 : 0.466136929076807
Loss in iteration 195 : 0.4660887380317784
Loss in iteration 196 : 0.4660411060316611
Loss in iteration 197 : 0.4659940249483389
Loss in iteration 198 : 0.4659474867903409
Loss in iteration 199 : 0.46590148369954226
Loss in iteration 200 : 0.46585600794766485
Loss in iteration 201 : 0.46581105193337885
Loss in iteration 202 : 0.4657666081801597
Loss in iteration 203 : 0.4657226693345261
Loss in iteration 204 : 0.4656792281640449
Loss in iteration 205 : 0.4656362775548495
Loss in iteration 206 : 0.4655938105089302
Loss in iteration 207 : 0.4655518201415936
Loss in iteration 208 : 0.46551029967936464
Loss in iteration 209 : 0.46546924245818766
Loss in iteration 210 : 0.4654286419216545
Loss in iteration 211 : 0.4653884916189867
Loss in iteration 212 : 0.4653487852028725
Loss in iteration 213 : 0.46530951642732726
Loss in iteration 214 : 0.4652706791457632
Loss in iteration 215 : 0.4652322673092856
Loss in iteration 216 : 0.46519427496506727
Loss in iteration 217 : 0.4651566962546557
Loss in iteration 218 : 0.4651195254121637
Loss in iteration 219 : 0.46508275676246474
Loss in iteration 220 : 0.4650463847194706
Loss in iteration 221 : 0.46501040378455405
Loss in iteration 222 : 0.46497480854505613
Loss in iteration 223 : 0.46493959367280163
Loss in iteration 224 : 0.4649047539225681
Loss in iteration 225 : 0.46487028413054476
Loss in iteration 226 : 0.46483617921284226
Loss in iteration 227 : 0.4648024341640755
Loss in iteration 228 : 0.4647690440560269
Loss in iteration 229 : 0.4647360040363211
Loss in iteration 230 : 0.46470330932709203
Loss in iteration 231 : 0.46467095522365937
Loss in iteration 232 : 0.4646389370931971
Loss in iteration 233 : 0.46460725037349015
Loss in iteration 234 : 0.46457589057170834
Loss in iteration 235 : 0.46454485326323347
Loss in iteration 236 : 0.4645141340904915
Loss in iteration 237 : 0.464483728761786
Loss in iteration 238 : 0.4644536330501593
Loss in iteration 239 : 0.46442384279226656
Loss in iteration 240 : 0.4643943538873074
Loss in iteration 241 : 0.464365162295974
Loss in iteration 242 : 0.46433626403941247
Loss in iteration 243 : 0.46430765519820005
Loss in iteration 244 : 0.464279331911335
Loss in iteration 245 : 0.46425129037524526
Loss in iteration 246 : 0.46422352684282103
Loss in iteration 247 : 0.46419603762248096
Loss in iteration 248 : 0.46416881907724916
Loss in iteration 249 : 0.46414186762384635
Loss in iteration 250 : 0.46411517973179583
Loss in iteration 251 : 0.46408875192255006
Loss in iteration 252 : 0.4640625807686248
Loss in iteration 253 : 0.4640366628927767
Loss in iteration 254 : 0.4640109949671649
Loss in iteration 255 : 0.46398557371256494
Loss in iteration 256 : 0.4639603958975506
Loss in iteration 257 : 0.46393545833772865
Loss in iteration 258 : 0.4639107578949653
Loss in iteration 259 : 0.46388629147664406
Loss in iteration 260 : 0.46386205603492137
Loss in iteration 261 : 0.46383804856601735
Loss in iteration 262 : 0.4638142661094979
Loss in iteration 263 : 0.4637907057475794
Loss in iteration 264 : 0.4637673646044527
Loss in iteration 265 : 0.46374423984560176
Loss in iteration 266 : 0.463721328677162
Loss in iteration 267 : 0.4636986283452612
Loss in iteration 268 : 0.4636761361353929
Loss in iteration 269 : 0.4636538493717934
Loss in iteration 270 : 0.4636317654168295
Loss in iteration 271 : 0.46360988167039396
Loss in iteration 272 : 0.46358819556932224
Loss in iteration 273 : 0.46356670458681265
Loss in iteration 274 : 0.46354540623185564
Loss in iteration 275 : 0.4635242980486743
Loss in iteration 276 : 0.4635033776161833
Loss in iteration 277 : 0.46348264254744415
Loss in iteration 278 : 0.463462090489136
Loss in iteration 279 : 0.46344171912104365
Loss in iteration 280 : 0.4634215261555356
Loss in iteration 281 : 0.46340150933707525
Loss in iteration 282 : 0.4633816664417177
Loss in iteration 283 : 0.46336199527662875
Loss in iteration 284 : 0.4633424936796112
Loss in iteration 285 : 0.4633231595186371
Loss in iteration 286 : 0.4633039906913851
Loss in iteration 287 : 0.46328498512479127
Loss in iteration 288 : 0.4632661407746043
Loss in iteration 289 : 0.463247455624951
Loss in iteration 290 : 0.4632289276879088
Loss in iteration 291 : 0.46321055500307934
Loss in iteration 292 : 0.46319233563718
Loss in iteration 293 : 0.4631742676836342
Loss in iteration 294 : 0.46315634926217125
Loss in iteration 295 : 0.46313857851843315
Loss in iteration 296 : 0.4631209536235882
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.7905, training accuracy 0.7905, time elapsed: 6308 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 4.8558452841449675
Loss in iteration 3 : 4.932588061608756
Loss in iteration 4 : 1.480491367804904
Loss in iteration 5 : 1.1069842418577869
Loss in iteration 6 : 0.7976234227949963
Loss in iteration 7 : 0.8172371760950683
Loss in iteration 8 : 0.857296416147334
Loss in iteration 9 : 0.8436630400673937
Loss in iteration 10 : 0.7858796440778205
Loss in iteration 11 : 0.7165514312599892
Loss in iteration 12 : 0.6618541843399676
Loss in iteration 13 : 0.6310898224293143
Loss in iteration 14 : 0.6285544225302222
Loss in iteration 15 : 0.6348439335914198
Loss in iteration 16 : 0.6474860388341624
Loss in iteration 17 : 0.6511573874959251
Loss in iteration 18 : 0.6712594616115521
Loss in iteration 19 : 0.6857601463553247
Loss in iteration 20 : 0.7383678023389129
Loss in iteration 21 : 0.7535748885309259
Loss in iteration 22 : 0.7830334340617182
Loss in iteration 23 : 0.7548960761957554
Loss in iteration 24 : 0.7320765210040606
Loss in iteration 25 : 0.7010767868717379
Loss in iteration 26 : 0.6792776755744433
Loss in iteration 27 : 0.660331509731629
Loss in iteration 28 : 0.6520374746877986
Loss in iteration 29 : 0.643914150193179
Loss in iteration 30 : 0.6479278066149691
Loss in iteration 31 : 0.6478249747712957
Loss in iteration 32 : 0.6634967057905011
Loss in iteration 33 : 0.6670704311162214
Loss in iteration 34 : 0.6884518265210604
Loss in iteration 35 : 0.6898856540710244
Loss in iteration 36 : 0.7055531271635905
Loss in iteration 37 : 0.7100767846666572
Loss in iteration 38 : 0.7126944963821154
Loss in iteration 39 : 0.7130377966463004
Loss in iteration 40 : 0.7015705036840033
Loss in iteration 41 : 0.6928539963884183
Loss in iteration 42 : 0.6771844703092685
Loss in iteration 43 : 0.6643259479473856
Loss in iteration 44 : 0.6520061811353598
Loss in iteration 45 : 0.6394317341600723
Loss in iteration 46 : 0.632891454104299
Loss in iteration 47 : 0.6226387153692876
Loss in iteration 48 : 0.6220785841060007
Loss in iteration 49 : 0.6145926396442055
Loss in iteration 50 : 0.6196880569845046
Loss in iteration 51 : 0.6142298851889626
Loss in iteration 52 : 0.624119460889303
Loss in iteration 53 : 0.6183227563074875
Loss in iteration 54 : 0.6300137818740991
Loss in iteration 55 : 0.6233230049133981
Loss in iteration 56 : 0.634213774640444
Loss in iteration 57 : 0.6324133022363346
Loss in iteration 58 : 0.6423619264715992
Loss in iteration 59 : 0.6459004613603208
Loss in iteration 60 : 0.6523580805195236
Loss in iteration 61 : 0.6560860986057684
Loss in iteration 62 : 0.6567290025295741
Loss in iteration 63 : 0.6574902137175498
Loss in iteration 64 : 0.6536021909510346
Loss in iteration 65 : 0.651657139986778
Loss in iteration 66 : 0.646390135139025
Loss in iteration 67 : 0.643417385347639
Loss in iteration 68 : 0.6392772161779535
Loss in iteration 69 : 0.6367298122455968
Loss in iteration 70 : 0.6349207685196033
Loss in iteration 71 : 0.6335252363846827
Loss in iteration 72 : 0.634227375692676
Loss in iteration 73 : 0.634011750136511
Loss in iteration 74 : 0.636716701861987
Loss in iteration 75 : 0.6371365502577735
Loss in iteration 76 : 0.6409366241580791
Loss in iteration 77 : 0.6410725429181954
Loss in iteration 78 : 0.6450336586095912
Loss in iteration 79 : 0.643937479388926
Loss in iteration 80 : 0.6474999910564988
Loss in iteration 81 : 0.6446116285453499
Loss in iteration 82 : 0.6477484469574933
Loss in iteration 83 : 0.6431825061302645
Loss in iteration 84 : 0.6462077210658479
Loss in iteration 85 : 0.6407516927852789
Loss in iteration 86 : 0.6440141434694305
Loss in iteration 87 : 0.6387907595593013
Loss in iteration 88 : 0.6424749030139537
Loss in iteration 89 : 0.6384251030017997
Loss in iteration 90 : 0.642449013088294
Loss in iteration 91 : 0.6399662739026339
Loss in iteration 92 : 0.6439495942021961
Loss in iteration 93 : 0.6428375242820923
Loss in iteration 94 : 0.64620140639125
Loss in iteration 95 : 0.6458742066076844
Loss in iteration 96 : 0.6480904081119172
Loss in iteration 97 : 0.64787719348557
Loss in iteration 98 : 0.6487410329557595
Loss in iteration 99 : 0.6481584977938769
Loss in iteration 100 : 0.6478966312561812
Loss in iteration 101 : 0.6467731402247036
Loss in iteration 102 : 0.6459232727507515
Loss in iteration 103 : 0.644350340044846
Loss in iteration 104 : 0.6435287898042925
Loss in iteration 105 : 0.6417174216546639
Loss in iteration 106 : 0.6414248258945049
Loss in iteration 107 : 0.6395637675481446
Loss in iteration 108 : 0.640091398192252
Loss in iteration 109 : 0.6382606257349246
Loss in iteration 110 : 0.6396822514952172
Loss in iteration 111 : 0.6378327670930328
Loss in iteration 112 : 0.6400473019194801
Loss in iteration 113 : 0.638040309256073
Loss in iteration 114 : 0.6408392513831507
Loss in iteration 115 : 0.6385289154586635
Loss in iteration 116 : 0.6416695168016238
Loss in iteration 117 : 0.6389968302884342
Loss in iteration 118 : 0.6422621972685183
Loss in iteration 119 : 0.639310163945446
Loss in iteration 120 : 0.642540367583616
Loss in iteration 121 : 0.6395123013945615
Loss in iteration 122 : 0.6426023233007142
Loss in iteration 123 : 0.6397351967453384
Loss in iteration 124 : 0.6426118620477554
Loss in iteration 125 : 0.6400820084722466
Loss in iteration 126 : 0.6426813674268183
Loss in iteration 127 : 0.6405542903202923
Loss in iteration 128 : 0.6428157649196983
Loss in iteration 129 : 0.641050952355305
Loss in iteration 130 : 0.6429301643785248
Loss in iteration 131 : 0.6414207458854028
Loss in iteration 132 : 0.6429104985939138
Loss in iteration 133 : 0.6415314106502358
Loss in iteration 134 : 0.6426764595385225
Loss in iteration 135 : 0.6413222013425361
Loss in iteration 136 : 0.642218287347637
Loss in iteration 137 : 0.6408215817050413
Loss in iteration 138 : 0.6415983544365123
Loss in iteration 139 : 0.6401300656201433
Loss in iteration 140 : 0.6409250748275207
Loss in iteration 141 : 0.6393817997633794
Loss in iteration 142 : 0.6403150326412149
Loss in iteration 143 : 0.6387028831760136
Loss in iteration 144 : 0.6398589075701211
Loss in iteration 145 : 0.6381806569190883
Loss in iteration 146 : 0.6396012108236514
Loss in iteration 147 : 0.6378507962783033
Loss in iteration 148 : 0.6395370619837103
Loss in iteration 149 : 0.6377019377808524
Loss in iteration 150 : 0.6396235811357283
Loss in iteration 151 : 0.6376925037390135
Loss in iteration 152 : 0.6397997067168242
Loss in iteration 153 : 0.6377717586892714
Loss in iteration 154 : 0.6400066733442503
Loss in iteration 155 : 0.6378971588069299
Loss in iteration 156 : 0.6402023007667794
Loss in iteration 157 : 0.6380427280197145
Loss in iteration 158 : 0.6403654875165663
Loss in iteration 159 : 0.6381975831429447
Loss in iteration 160 : 0.6404916558654664
Loss in iteration 161 : 0.638357918413761
Loss in iteration 162 : 0.6405834039668591
Loss in iteration 163 : 0.6385178690136618
Loss in iteration 164 : 0.6406417427768804
Loss in iteration 165 : 0.6386640437322303
Loss in iteration 166 : 0.6406618526146294
Loss in iteration 167 : 0.6387759346604396
Loss in iteration 168 : 0.6406344127944531
Loss in iteration 169 : 0.6388313812357514
Loss in iteration 170 : 0.6405508086096329
Loss in iteration 171 : 0.6388141320176817
Loss in iteration 172 : 0.6404090598081607
Loss in iteration 173 : 0.6387200053335941
Loss in iteration 174 : 0.6402174908969676
Loss in iteration 175 : 0.6385591388240887
Loss in iteration 176 : 0.6399945672912127
Loss in iteration 177 : 0.6383536738467108
Loss in iteration 178 : 0.6397651209178449
Loss in iteration 179 : 0.6381320347418296
Loss in iteration 180 : 0.639554570174897
Loss in iteration 181 : 0.6379220177462347
Loss in iteration 182 : 0.6393832494599843
Loss in iteration 183 : 0.6377449534641932
Loss in iteration 184 : 0.6392626080139368
Loss in iteration 185 : 0.6376124614985409
Loss in iteration 186 : 0.6391941434820124
Loss in iteration 187 : 0.6375262235744436
Loss in iteration 188 : 0.6391709291759368
Loss in iteration 189 : 0.637480205047674
Loss in iteration 190 : 0.6391808289430112
Loss in iteration 191 : 0.637464148849616
Loss in iteration 192 : 0.6392101651990604
Loss in iteration 193 : 0.6374670548775653
Loss in iteration 194 : 0.6392467334716718
Loss in iteration 195 : 0.6374796725573677
Loss in iteration 196 : 0.639281510645179
Loss in iteration 197 : 0.6374955854518071
Loss in iteration 198 : 0.639308968398222
Loss in iteration 199 : 0.6375110238340242
Loss in iteration 200 : 0.6393263643162044
Loss in iteration 201 : 0.6375239172885916
Loss in iteration 202 : 0.6393326064060059
Loss in iteration 203 : 0.6375328094664305
Loss in iteration 204 : 0.6393272512474351
Loss in iteration 205 : 0.637536126514162
Loss in iteration 206 : 0.63930997655733
Loss in iteration 207 : 0.6375320209018221
Loss in iteration 208 : 0.639280586080293
Loss in iteration 209 : 0.6375187279110274
Loss in iteration 210 : 0.6392393716681816
Loss in iteration 211 : 0.6374951739271097
Loss in iteration 212 : 0.6391875440088605
Loss in iteration 213 : 0.637461514406926
Loss in iteration 214 : 0.6391274635992269
Loss in iteration 215 : 0.6374193495755532
Loss in iteration 216 : 0.6390625229385652
Loss in iteration 217 : 0.6373715187145707
Loss in iteration 218 : 0.6389966881033476
Loss in iteration 219 : 0.6373215409018668
Loss in iteration 220 : 0.6389338398909343
Loss in iteration 221 : 0.6372728897516342
Loss in iteration 222 : 0.6388771186954778
Loss in iteration 223 : 0.6372283276592662
Loss in iteration 224 : 0.6388284614660138
Loss in iteration 225 : 0.6371894812794507
Loss in iteration 226 : 0.638788441371835
Loss in iteration 227 : 0.6371567427100556
Loss in iteration 228 : 0.6387564172186517
Loss in iteration 229 : 0.6371294715999385
Loss in iteration 230 : 0.638730908877982
Loss in iteration 231 : 0.6371063901369525
Loss in iteration 232 : 0.6387100646716543
Loss in iteration 233 : 0.6370860278997428
Loss in iteration 234 : 0.6386920860711912
Loss in iteration 235 : 0.6370670890083094
Loss in iteration 236 : 0.6386755153195729
Loss in iteration 237 : 0.6370486650490005
Loss in iteration 238 : 0.6386593519316862
Loss in iteration 239 : 0.6370302803468517
Loss in iteration 240 : 0.6386430211770653
Loss in iteration 241 : 0.6370118086698323
Loss in iteration 242 : 0.6386262540920943
Loss in iteration 243 : 0.6369933279973474
Loss in iteration 244 : 0.6386089473281102
Loss in iteration 245 : 0.6369749795172566
Loss in iteration 246 : 0.638591055890826
Loss in iteration 247 : 0.6369568753792298
Loss in iteration 248 : 0.638572543359531
Loss in iteration 249 : 0.6369390692088427
Loss in iteration 250 : 0.6385533852175513
Loss in iteration 251 : 0.6369215763770777
Loss in iteration 252 : 0.6385336014355669
Loss in iteration 253 : 0.6369044158682868
Loss in iteration 254 : 0.6385132888797291
Loss in iteration 255 : 0.6368876446938146
Loss in iteration 256 : 0.638492631133716
Loss in iteration 257 : 0.6368713661232726
Loss in iteration 258 : 0.6384718775357014
Loss in iteration 259 : 0.6368557083046797
Loss in iteration 260 : 0.6384512979086598
Loss in iteration 261 : 0.6368407834831303
Loss in iteration 262 : 0.6384311291329311
Loss in iteration 263 : 0.6368266453333692
Loss in iteration 264 : 0.6384115317738055
Loss in iteration 265 : 0.6368132613882735
Loss in iteration 266 : 0.6383925700149782
Loss in iteration 267 : 0.6368005107126189
Loss in iteration 268 : 0.638374219090176
Loss in iteration 269 : 0.6367882073039007
Loss in iteration 270 : 0.6383563950708303
Loss in iteration 271 : 0.6367761410333683
Loss in iteration 272 : 0.6383389954976391
Loss in iteration 273 : 0.6367641230977042
Loss in iteration 274 : 0.6383219376096485
Loss in iteration 275 : 0.6367520229793748
Loss in iteration 276 : 0.638305183650756
Loss in iteration 277 : 0.6367397880431032
Loss in iteration 278 : 0.6382887482952919
Loss in iteration 279 : 0.6367274431311459
Loss in iteration 280 : 0.6382726893630059
Loss in iteration 281 : 0.6367150735251714
Loss in iteration 282 : 0.6382570876473533
Loss in iteration 283 : 0.6367027985772182
Loss in iteration 284 : 0.6382420236748318
Loss in iteration 285 : 0.6366907443451885
Loss in iteration 286 : 0.6382275584452558
Loss in iteration 287 : 0.6366790219563571
Loss in iteration 288 : 0.638213722476965
Loss in iteration 289 : 0.6366677152138197
Loss in iteration 290 : 0.6382005140639349
Loss in iteration 291 : 0.6366568774948677
Loss in iteration 292 : 0.6381879047674061
Loss in iteration 293 : 0.6366465354015238
Loss in iteration 294 : 0.6381758486016217
Loss in iteration 295 : 0.636636695514173
Loss in iteration 296 : 0.6381642913193166
Loss in iteration 297 : 0.636627350956209
Loss in iteration 298 : 0.6381531773098982
Loss in iteration 299 : 0.6366184858100193
Loss in iteration 300 : 0.6381424532397642
Loss in iteration 301 : 0.6366100770314805
Loss in iteration 302 : 0.6381320690167825
Loss in iteration 303 : 0.6366020947619458
Loss in iteration 304 : 0.6381219774869585
Loss in iteration 305 : 0.6365945024721322
Loss in iteration 306 : 0.6381121343290017
Loss in iteration 307 : 0.6365872581636043
Loss in iteration 308 : 0.6381024990607695
Loss in iteration 309 : 0.6365803171439013
Loss in iteration 310 : 0.6380930372513366
Loss in iteration 311 : 0.6365736360539982
Loss in iteration 312 : 0.6380837233094158
Loss in iteration 313 : 0.6365671772073768
Loss in iteration 314 : 0.6380745428526534
Loss in iteration 315 : 0.6365609121015289
Loss in iteration 316 : 0.6380654937349413
Loss in iteration 317 : 0.6365548232013811
Loss in iteration 318 : 0.6380565852311928
Loss in iteration 319 : 0.6365489036285106
Loss in iteration 320 : 0.6380478354535414
Loss in iteration 321 : 0.6365431549986231
Loss in iteration 322 : 0.6380392675805602
Loss in iteration 323 : 0.6365375841205787
Loss in iteration 324 : 0.6380309057614807
Loss in iteration 325 : 0.6365321994714379
Loss in iteration 326 : 0.6380227715511214
Loss in iteration 327 : 0.6365270082716092
Loss in iteration 328 : 0.638014881482881
Loss in iteration 329 : 0.6365220146774295
Loss in iteration 330 : 0.6380072460112683
Loss in iteration 331 : 0.6365172192162386
Loss in iteration 332 : 0.637999869686605
Loss in iteration 333 : 0.6365126192441243
Loss in iteration 334 : 0.6379927521698252
Loss in iteration 335 : 0.6365082100016123
Loss in iteration 336 : 0.6379858896057898
Loss in iteration 337 : 0.636503985807123
Loss in iteration 338 : 0.6379792759376628
Loss in iteration 339 : 0.6364999410334485
Loss in iteration 340 : 0.6379729039073307
Loss in iteration 341 : 0.6364960706915553
Loss in iteration 342 : 0.63796676567256
Loss in iteration 343 : 0.6364923706244251
Loss in iteration 344 : 0.637960853117295
Loss in iteration 345 : 0.6364888374349411
Loss in iteration 346 : 0.6379551580011603
Loss in iteration 347 : 0.6364854683116393
Loss in iteration 348 : 0.6379496720862965
Loss in iteration 349 : 0.6364822608838511
Loss in iteration 350 : 0.6379443873191346
Loss in iteration 351 : 0.636479213164672
Loss in iteration 352 : 0.63793929606856
Loss in iteration 353 : 0.6364763235647064
Loss in iteration 354 : 0.6379343913640696
Loss in iteration 355 : 0.6364735909115623
Loss in iteration 356 : 0.6379296670573517
Loss in iteration 357 : 0.6364710144030987
Loss in iteration 358 : 0.6379251178493791
Loss in iteration 359 : 0.6364685934515025
Loss in iteration 360 : 0.6379207391692504
Loss in iteration 361 : 0.6364663274231044
Loss in iteration 362 : 0.637916526939687
Loss in iteration 363 : 0.6364642153238426
Loss in iteration 364 : 0.6379124772985099
Loss in iteration 365 : 0.6364622555055901
Loss in iteration 366 : 0.637908586354456
Loss in iteration 367 : 0.6364604454666796
Loss in iteration 368 : 0.6379048500390829
Loss in iteration 369 : 0.6364587817943554
Loss in iteration 370 : 0.637901264082651
Loss in iteration 371 : 0.6364572602581238
Loss in iteration 372 : 0.6378978241037733
Loss in iteration 373 : 0.6364558760253425
Loss in iteration 374 : 0.6378945257728756
Loss in iteration 375 : 0.6364546239447202
Loss in iteration 376 : 0.6378913649955393
Loss in iteration 377 : 0.6364534988358648
Loss in iteration 378 : 0.637888338065449
Loss in iteration 379 : 0.6364524957326164
Loss in iteration 380 : 0.6378854417535411
Loss in iteration 381 : 0.6364516100488911
Loss in iteration 382 : 0.6378826733224092
Loss in iteration 383 : 0.636450837659876
Loss in iteration 384 : 0.6378800304757668
Loss in iteration 385 : 0.6364501749113376
Loss in iteration 386 : 0.6378775112657301
Loss in iteration 387 : 0.6364496185809091
Loss in iteration 388 : 0.6378751139843604
Loss in iteration 389 : 0.6364491658166938
Loss in iteration 390 : 0.6378728370612562
Loss in iteration 391 : 0.6364488140726926
Loss in iteration 392 : 0.6378706789800124
Loss in iteration 393 : 0.6364485610514621
Loss in iteration 394 : 0.637868638216666
Loss in iteration 395 : 0.6364484046558742
Loss in iteration 396 : 0.6378667131965658
Loss in iteration 397 : 0.6364483429467683
Loss in iteration 398 : 0.6378649022634224
Loss in iteration 399 : 0.6364483741022419
Loss in iteration 400 : 0.6378632036554027
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.717875, training accuracy 0.717875, time elapsed: 8798 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.8581589981712553
Loss in iteration 3 : 1.3205816299411726
Loss in iteration 4 : 1.754623473167917
Loss in iteration 5 : 0.6541375084832478
Loss in iteration 6 : 0.721411425100458
Loss in iteration 7 : 0.9354458904889897
Loss in iteration 8 : 0.9547092682711384
Loss in iteration 9 : 0.6605710900846772
Loss in iteration 10 : 0.6013910614929219
Loss in iteration 11 : 0.5600483221000176
Loss in iteration 12 : 0.5225160445608378
Loss in iteration 13 : 0.5056243290669731
Loss in iteration 14 : 0.49792552785146144
Loss in iteration 15 : 0.4961240620604934
Loss in iteration 16 : 0.4958181706403824
Loss in iteration 17 : 0.4957409811229124
Loss in iteration 18 : 0.49546178343619535
Loss in iteration 19 : 0.49491182961382174
Loss in iteration 20 : 0.49407470309036466
Loss in iteration 21 : 0.49293603722243245
Loss in iteration 22 : 0.4914943622183922
Loss in iteration 23 : 0.48977359691276445
Loss in iteration 24 : 0.48782317120203383
Loss in iteration 25 : 0.4857085514818467
Loss in iteration 26 : 0.4834998972707334
Loss in iteration 27 : 0.48126378359801675
Loss in iteration 28 : 0.4790591118972098
Loss in iteration 29 : 0.4769357178686327
Loss in iteration 30 : 0.47493383572158854
Loss in iteration 31 : 0.4730835746182394
Loss in iteration 32 : 0.4714046428012911
Loss in iteration 33 : 0.4699068939386879
Loss in iteration 34 : 0.46859189000319806
Loss in iteration 35 : 0.4674550871025219
Loss in iteration 36 : 0.46648796209543414
Loss in iteration 37 : 0.46567954909083586
Loss in iteration 38 : 0.4650172523690729
Loss in iteration 39 : 0.4644871450538375
Loss in iteration 40 : 0.464074084477154
Loss in iteration 41 : 0.46376189779828153
Loss in iteration 42 : 0.4635337424667938
Loss in iteration 43 : 0.4633726303359476
Loss in iteration 44 : 0.4632620514464752
Loss in iteration 45 : 0.4631866193017675
Loss in iteration 46 : 0.4631326568660332
Loss in iteration 47 : 0.46308864620971
Loss in iteration 48 : 0.46304548341906093
Loss in iteration 49 : 0.4629965168812463
Loss in iteration 50 : 0.46293739055012273
Loss in iteration 51 : 0.4628657475280702
Loss in iteration 52 : 0.462780862226872
Loss in iteration 53 : 0.46268326187888215
Loss in iteration 54 : 0.46257437862041556
Loss in iteration 55 : 0.462456251805967
Loss in iteration 56 : 0.46233128347417357
Loss in iteration 57 : 0.46220204036284485
Loss in iteration 58 : 0.46207109312795885
Loss in iteration 59 : 0.46194088490694923
Loss in iteration 60 : 0.4618136251970966
Loss in iteration 61 : 0.4616912081806593
Loss in iteration 62 : 0.4615751570340141
Loss in iteration 63 : 0.4614665951620948
Loss in iteration 64 : 0.46136624454218067
Loss in iteration 65 : 0.4612744479092412
Loss in iteration 66 : 0.4611912107977746
Loss in iteration 67 : 0.4611162561687957
Loss in iteration 68 : 0.4610490865098667
Loss in iteration 69 : 0.46098904590087536
Loss in iteration 70 : 0.4609353797570417
Loss in iteration 71 : 0.4608872869216159
Loss in iteration 72 : 0.4608439661232699
Loss in iteration 73 : 0.46080465356530775
Loss in iteration 74 : 0.46076865767571384
Loss in iteration 75 : 0.4607353896425856
Loss in iteration 76 : 0.46070440055731754
Loss in iteration 77 : 0.46067542977152914
Loss in iteration 78 : 0.46064848604765907
Loss in iteration 79 : 0.46062399195362014
Loss in iteration 80 : 0.4606030441225247
Loss in iteration 81 : 0.4605879264935947
Loss in iteration 82 : 0.46058301617975117
Loss in iteration 83 : 0.46059667023879314
Loss in iteration 84 : 0.46064440485325053
Loss in iteration 85 : 0.4607559676305058
Loss in iteration 86 : 0.46098619118954526
Loss in iteration 87 : 0.46144157916160833
Loss in iteration 88 : 0.4623123286896208
Loss in iteration 89 : 0.463964525921517
Loss in iteration 90 : 0.46697076182940034
Loss in iteration 91 : 0.4723149201164518
Loss in iteration 92 : 0.480747557942118
Loss in iteration 93 : 0.49275064296871596
Loss in iteration 94 : 0.5046111387778491
Loss in iteration 95 : 0.513300732730735
Loss in iteration 96 : 0.5121420150365028
Loss in iteration 97 : 0.5074319493696704
Loss in iteration 98 : 0.4978432133602671
Loss in iteration 99 : 0.4907687437435121
Loss in iteration 100 : 0.4828994548795657
Loss in iteration 101 : 0.47817187443040254
Loss in iteration 102 : 0.47352515590326666
Loss in iteration 103 : 0.47122612927528396
Loss in iteration 104 : 0.46890564445477834
Loss in iteration 105 : 0.46816418112697217
Loss in iteration 106 : 0.4671344563367851
Loss in iteration 107 : 0.46730438781890776
Loss in iteration 108 : 0.466929642627877
Loss in iteration 109 : 0.4676824423125991
Loss in iteration 110 : 0.46760378866519564
Loss in iteration 111 : 0.46877734463680665
Loss in iteration 112 : 0.4687546924303401
Loss in iteration 113 : 0.47023079243472754
Loss in iteration 114 : 0.470062387028402
Loss in iteration 115 : 0.47170716446019473
Loss in iteration 116 : 0.47124500263339864
Loss in iteration 117 : 0.47294090734017075
Loss in iteration 118 : 0.472191592152446
Loss in iteration 119 : 0.47394623742614383
Loss in iteration 120 : 0.47319305004573187
Loss in iteration 121 : 0.47525475924561056
Loss in iteration 122 : 0.47517686516770025
Loss in iteration 123 : 0.4781343273417943
Loss in iteration 124 : 0.4800299422887882
Loss in iteration 125 : 0.48493910704015414
Loss in iteration 126 : 0.4910910265048083
Loss in iteration 127 : 0.49937428579398374
Loss in iteration 128 : 0.5126784135636904
Loss in iteration 129 : 0.5243548673532178
Loss in iteration 130 : 0.5443676066411108
Loss in iteration 131 : 0.5536509303894397
Loss in iteration 132 : 0.5714096747709382
Loss in iteration 133 : 0.5697302952288892
Loss in iteration 134 : 0.5769958999278221
Loss in iteration 135 : 0.5657621006200694
Loss in iteration 136 : 0.5642620276371137
Loss in iteration 137 : 0.5506660557065469
Loss in iteration 138 : 0.5451656579048549
Loss in iteration 139 : 0.5334425776891155
Loss in iteration 140 : 0.5273372738935236
Loss in iteration 141 : 0.5186334348306291
Loss in iteration 142 : 0.5135332006321953
Loss in iteration 143 : 0.5076218028094722
Loss in iteration 144 : 0.5039609313956701
Loss in iteration 145 : 0.5002275651348185
Loss in iteration 146 : 0.49790569256448874
Loss in iteration 147 : 0.49575886398200225
Loss in iteration 148 : 0.4945356087813421
Loss in iteration 149 : 0.4935415996882767
Loss in iteration 150 : 0.4932217262487145
Loss in iteration 151 : 0.4931179141475417
Loss in iteration 152 : 0.4936079382975755
Loss in iteration 153 : 0.49426183243092126
Loss in iteration 154 : 0.49556060720577105
Loss in iteration 155 : 0.496907971802319
Loss in iteration 156 : 0.4990701124121389
Loss in iteration 157 : 0.5010473318682432
Loss in iteration 158 : 0.5041336532457951
Loss in iteration 159 : 0.5066050471266474
Loss in iteration 160 : 0.5106149421669846
Loss in iteration 161 : 0.5132967491502202
Loss in iteration 162 : 0.5180803063136552
Loss in iteration 163 : 0.5204943352540197
Loss in iteration 164 : 0.5256782821155355
Loss in iteration 165 : 0.527208743107848
Loss in iteration 166 : 0.5322134898788029
Loss in iteration 167 : 0.5323122636351061
Loss in iteration 168 : 0.5365132908579208
Loss in iteration 169 : 0.5349589563903727
Loss in iteration 170 : 0.5379237772723697
Loss in iteration 171 : 0.5349326059408754
Loss in iteration 172 : 0.5365703061779222
Loss in iteration 173 : 0.5326702916888939
Loss in iteration 174 : 0.533202582054319
Loss in iteration 175 : 0.528993258127578
Loss in iteration 176 : 0.5288063037305353
Loss in iteration 177 : 0.5247741738280063
Loss in iteration 178 : 0.5242664928026202
Loss in iteration 179 : 0.5207175890427723
Loss in iteration 180 : 0.5202105053804271
Loss in iteration 181 : 0.517287625999354
Loss in iteration 182 : 0.5170011278320455
Loss in iteration 183 : 0.5147335539047797
Loss in iteration 184 : 0.5148009002861099
Loss in iteration 185 : 0.5131524260515599
Loss in iteration 186 : 0.5136466839781565
Loss in iteration 187 : 0.5125482714383288
Loss in iteration 188 : 0.5135046417127819
Loss in iteration 189 : 0.512870303404352
Loss in iteration 190 : 0.5142981374033593
Loss in iteration 191 : 0.5140280872002089
Loss in iteration 192 : 0.5159126958291544
Loss in iteration 193 : 0.5158897251248834
Loss in iteration 194 : 0.5181872889059628
Loss in iteration 195 : 0.5182729725653902
Loss in iteration 196 : 0.520903714415235
Loss in iteration 197 : 0.520941018074067
Loss in iteration 198 : 0.5237869489082361
Loss in iteration 199 : 0.5236140980265755
Loss in iteration 200 : 0.5265271532309155
Loss in iteration 201 : 0.5260028091964557
Loss in iteration 202 : 0.5288255183964504
Loss in iteration 203 : 0.5278580983490083
Loss in iteration 204 : 0.5304523918365747
Loss in iteration 205 : 0.5290211918330555
Loss in iteration 206 : 0.5312952429137059
Loss in iteration 207 : 0.5294530107410212
Loss in iteration 208 : 0.5313757674325665
Loss in iteration 209 : 0.5292313962351978
Loss in iteration 210 : 0.5308304010345322
Loss in iteration 211 : 0.5285199314374389
Loss in iteration 212 : 0.5298658056811962
Loss in iteration 213 : 0.5275233500749881
Loss in iteration 214 : 0.5287087587927886
Loss in iteration 215 : 0.5264456647702244
Loss in iteration 216 : 0.527566099767317
Loss in iteration 217 : 0.525460805983303
Loss in iteration 218 : 0.526601145210692
Loss in iteration 219 : 0.5246977780396203
Loss in iteration 220 : 0.5259251009382581
Loss in iteration 221 : 0.5242372309702629
Loss in iteration 222 : 0.5255983391896234
Loss in iteration 223 : 0.5241147427873868
Loss in iteration 224 : 0.5256363638399167
Loss in iteration 225 : 0.5243269001413305
Loss in iteration 226 : 0.5260170438531775
Loss in iteration 227 : 0.5248379960267378
Loss in iteration 228 : 0.5266877430792578
Loss in iteration 229 : 0.5255867866606728
Loss in iteration 230 : 0.5275724838903892
Loss in iteration 231 : 0.5264937408637195
Loss in iteration 232 : 0.528579948748103
Loss in iteration 233 : 0.5274694231129679
Loss in iteration 234 : 0.5296129478825902
Loss in iteration 235 : 0.5284241708461481
Loss in iteration 236 : 0.5305791717262673
Loss in iteration 237 : 0.5292783520202856
Loss in iteration 238 : 0.5314020086237167
Loss in iteration 239 : 0.5299716693320707
Loss in iteration 240 : 0.5320294725034103
Loss in iteration 241 : 0.5304696707395395
Loss in iteration 242 : 0.5324392932233522
Loss in iteration 243 : 0.5307660654600862
Loss in iteration 244 : 0.5326390614554568
Loss in iteration 245 : 0.5308804795446624
Loss in iteration 246 : 0.5326616328622529
Loss in iteration 247 : 0.5308524535966219
Loss in iteration 248 : 0.5325571870003066
Loss in iteration 249 : 0.5307332934227916
Loss in iteration 250 : 0.5323839318318833
Loss in iteration 251 : 0.5305775807185866
Loss in iteration 252 : 0.5321993341039954
Loss in iteration 253 : 0.5304358057370453
Loss in iteration 254 : 0.5320531626213566
Loss in iteration 255 : 0.5303489684528728
Loss in iteration 256 : 0.5319829058458588
Loss in iteration 257 : 0.5303453927404337
Loss in iteration 258 : 0.5320115416477205
Loss in iteration 259 : 0.5304395803797493
Loss in iteration 260 : 0.5321473070192422
Loss in iteration 261 : 0.5306327327019063
Loss in iteration 262 : 0.5323850114928484
Loss in iteration 263 : 0.5309145299455308
Loss in iteration 264 : 0.5327084646353708
Loss in iteration 265 : 0.5312657945097168
Loss in iteration 266 : 0.533093648622026
Loss in iteration 267 : 0.5316617035849938
Loss in iteration 268 : 0.5335123015040272
Loss in iteration 269 : 0.5320752269650889
Loss in iteration 270 : 0.5339355722058362
Loss in iteration 271 : 0.5324804535592076
Loss in iteration 272 : 0.5343373885138639
Loss in iteration 273 : 0.5328554655998687
Loss in iteration 274 : 0.5346971845190089
Loss in iteration 275 : 0.5331844559950308
Loss in iteration 276 : 0.5350016966559993
Loss in iteration 277 : 0.5334588784851633
Loss in iteration 278 : 0.5352456646822633
Loss in iteration 279 : 0.5336775628442515
Loss in iteration 280 : 0.535431442732401
Loss in iteration 281 : 0.5338458864035454
Loss in iteration 282 : 0.5355676934308042
Loss in iteration 283 : 0.5339742286143555
Loss in iteration 284 : 0.535667461913727
Loss in iteration 285 : 0.5340760159336394
Loss in iteration 286 : 0.5357459804528861
Loss in iteration 287 : 0.5341656784737916
Loss in iteration 288 : 0.5358185365753203
Loss in iteration 289 : 0.5342567965543388
Loss in iteration 290 : 0.5358986667609125
Loss in iteration 291 : 0.5343606364200464
Loss in iteration 292 : 0.5359968416190336
Loss in iteration 293 : 0.5344851843734345
Loss in iteration 294 : 0.5361197122038273
Loss in iteration 295 : 0.5346347062854875
Loss in iteration 296 : 0.5362699070329086
Loss in iteration 297 : 0.5348097949141684
Loss in iteration 298 : 0.5364463115009437
Loss in iteration 299 : 0.5350078228612615
Loss in iteration 300 : 0.5366447245902086
Loss in iteration 301 : 0.5352236919861246
Loss in iteration 302 : 0.5368587679242828
Loss in iteration 303 : 0.5354507571846021
Loss in iteration 304 : 0.537080915449573
Loss in iteration 305 : 0.5356818011123126
Loss in iteration 306 : 0.5373035162912424
Loss in iteration 307 : 0.5359099455574151
Loss in iteration 308 : 0.5375196979924286
Loss in iteration 309 : 0.5361294041984054
Loss in iteration 310 : 0.5377240619028505
Loss in iteration 311 : 0.5363360091986767
Loss in iteration 312 : 0.5379131151858134
Loss in iteration 313 : 0.5365274777404525
Loss in iteration 314 : 0.5380854211382639
Loss in iteration 315 : 0.5367034198857839
Loss in iteration 316 : 0.5382414861730069
Loss in iteration 317 : 0.5368651210455501
Loss in iteration 318 : 0.5383834325285616
Loss in iteration 319 : 0.5370151563409792
Loss in iteration 320 : 0.5385145263328638
Loss in iteration 321 : 0.5371569074142901
Loss in iteration 322 : 0.5386386390179836
Loss in iteration 323 : 0.5372940541623111
Loss in iteration 324 : 0.538759716697732
Loss in iteration 325 : 0.5374301058904527
Loss in iteration 326 : 0.5388813194205497
Loss in iteration 327 : 0.5375680213867929
Loss in iteration 328 : 0.5390062737378327
Loss in iteration 329 : 0.5377099488020909
Loss in iteration 330 : 0.5391364614567128
Loss in iteration 331 : 0.5378570971557339
Loss in iteration 332 : 0.5392727478637465
Loss in iteration 333 : 0.5380097342164769
Loss in iteration 334 : 0.5394150362653751
Loss in iteration 335 : 0.5381672920026942
Loss in iteration 336 : 0.5395624235953072
Loss in iteration 337 : 0.5383285519830984
Loss in iteration 338 : 0.5397134245134642
Loss in iteration 339 : 0.538491877391243
Loss in iteration 340 : 0.5398662287412642
Loss in iteration 341 : 0.5386554596134264
Loss in iteration 342 : 0.5400189578500337
Loss in iteration 343 : 0.5388175487800627
Loss in iteration 344 : 0.5401698925956343
Loss in iteration 345 : 0.5389766446700264
Loss in iteration 346 : 0.5403176492360021
Loss in iteration 347 : 0.5391316318472215
Loss in iteration 348 : 0.5404612920122189
Loss in iteration 349 : 0.5392818515228478
Loss in iteration 350 : 0.5406003780029237
Loss in iteration 351 : 0.5394271109229903
Loss in iteration 352 : 0.5407349388242269
Loss in iteration 353 : 0.5395676380186484
Loss in iteration 354 : 0.5408654102775537
Loss in iteration 355 : 0.5397039946634048
Loss in iteration 356 : 0.5409925254690973
Loss in iteration 357 : 0.5398369641155538
Loss in iteration 358 : 0.5411171889058773
Loss in iteration 359 : 0.5399674295606066
Loss in iteration 360 : 0.5412403487231534
Loss in iteration 361 : 0.5400962588556549
Loss in iteration 362 : 0.5413628819045071
Loss in iteration 363 : 0.5402242077631884
Loss in iteration 364 : 0.5414855036896954
Loss in iteration 365 : 0.5403518500188033
Loss in iteration 366 : 0.5416087079710178
Loss in iteration 367 : 0.5404795382930775
Loss in iteration 368 : 0.5417327409779739
Loss in iteration 369 : 0.5406073960098399
Loss in iteration 370 : 0.54185760646845
Loss in iteration 371 : 0.5407353365038405
Loss in iteration 372 : 0.5419830973715755
Loss in iteration 373 : 0.5408631034317614
Loss in iteration 374 : 0.5421088465976011
Loss in iteration 375 : 0.5409903248368737
Loss in iteration 376 : 0.5422343886301765
Loss in iteration 377 : 0.5411165728189904
Loss in iteration 378 : 0.5423592235004652
Loss in iteration 379 : 0.5412414212733276
Loss in iteration 380 : 0.5424828756586976
Loss in iteration 381 : 0.5413644954425597
Loss in iteration 382 : 0.5426049418782322
Loss in iteration 383 : 0.5414855088281604
Loss in iteration 384 : 0.5427251243765338
Loss in iteration 385 : 0.5416042850586539
Loss in iteration 386 : 0.5428432475319899
Loss in iteration 387 : 0.5417207643499442
Loss in iteration 388 : 0.5429592586502997
Loss in iteration 389 : 0.5418349959887286
Loss in iteration 390 : 0.5430732149709545
Loss in iteration 391 : 0.5419471196531174
Loss in iteration 392 : 0.5431852603515739
Loss in iteration 393 : 0.5420573392539095
Loss in iteration 394 : 0.5432955957494373
Loss in iteration 395 : 0.5421658933075826
Loss in iteration 396 : 0.5434044477360155
Loss in iteration 397 : 0.5422730256760349
Loss in iteration 398 : 0.5435120388967873
Loss in iteration 399 : 0.542378959920268
Loss in iteration 400 : 0.5436185632007821
Testing accuracy  of updater 8 on alg 0 with rate 1.4000000000000001 = 0.757125, training accuracy 0.757125, time elapsed: 7806 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6607925250883446
Loss in iteration 3 : 0.6433099748229305
Loss in iteration 4 : 0.6375900814063571
Loss in iteration 5 : 0.6467524628564868
Loss in iteration 6 : 0.6748905011654944
Loss in iteration 7 : 0.6165367783352906
Loss in iteration 8 : 0.5884064787832416
Loss in iteration 9 : 0.5401639835997876
Loss in iteration 10 : 0.5120870918581433
Loss in iteration 11 : 0.49684144236432154
Loss in iteration 12 : 0.488947331375377
Loss in iteration 13 : 0.48545995453435087
Loss in iteration 14 : 0.4833805373282154
Loss in iteration 15 : 0.48196242034707887
Loss in iteration 16 : 0.4808979645129321
Loss in iteration 17 : 0.4800569606258414
Loss in iteration 18 : 0.47934996496919907
Loss in iteration 19 : 0.47871291331025767
Loss in iteration 20 : 0.4781001317214051
Loss in iteration 21 : 0.47748050691082927
Loss in iteration 22 : 0.4768348002435359
Loss in iteration 23 : 0.47615333614714134
Loss in iteration 24 : 0.4754338278359518
Loss in iteration 25 : 0.4746793821061043
Loss in iteration 26 : 0.47389678333772534
Loss in iteration 27 : 0.4730951015368247
Loss in iteration 28 : 0.4722845981020538
Loss in iteration 29 : 0.4714758677614757
Loss in iteration 30 : 0.47067916155476924
Loss in iteration 31 : 0.469903863062477
Loss in iteration 32 : 0.4691581137036581
Loss in iteration 33 : 0.4684485893121723
Loss in iteration 34 : 0.46778042022829097
Loss in iteration 35 : 0.4671572306610037
Loss in iteration 36 : 0.4665812607717876
Loss in iteration 37 : 0.46605353254311865
Loss in iteration 38 : 0.46557402749845056
Loss in iteration 39 : 0.46514185617661075
Loss in iteration 40 : 0.4647554107755949
Loss in iteration 41 : 0.464412500320203
Loss in iteration 42 : 0.46411047141459566
Loss in iteration 43 : 0.46384631824483225
Loss in iteration 44 : 0.46361678461811345
Loss in iteration 45 : 0.46341845963237666
Loss in iteration 46 : 0.4632478675225613
Loss in iteration 47 : 0.46310155133702996
Loss in iteration 48 : 0.46297614929724523
Loss in iteration 49 : 0.4628684620629066
Loss in iteration 50 : 0.46277550885333507
Loss in iteration 51 : 0.4626945706157352
Loss in iteration 52 : 0.4626232191698183
Loss in iteration 53 : 0.4625593323083871
Loss in iteration 54 : 0.462501095916073
Loss in iteration 55 : 0.4624469950321485
Loss in iteration 56 : 0.46239579628462285
Loss in iteration 57 : 0.46234652424386097
Loss in iteration 58 : 0.46229843405999305
Loss in iteration 59 : 0.4622509823754677
Loss in iteration 60 : 0.46220379805494916
Loss in iteration 61 : 0.4621566538354524
Loss in iteration 62 : 0.462109439623076
Loss in iteration 63 : 0.4620621378731373
Loss in iteration 64 : 0.46201480129019695
Loss in iteration 65 : 0.46196753296064047
Loss in iteration 66 : 0.46192046896163463
Loss in iteration 67 : 0.4618737634523513
Loss in iteration 68 : 0.4618275762252992
Loss in iteration 69 : 0.46178206266333904
Loss in iteration 70 : 0.4617373660058553
Loss in iteration 71 : 0.4616936117774927
Loss in iteration 72 : 0.46165090418140553
Loss in iteration 73 : 0.46160932421429346
Loss in iteration 74 : 0.46156892922927
Loss in iteration 75 : 0.46152975365876897
Loss in iteration 76 : 0.461491810613841
Loss in iteration 77 : 0.4614550940959379
Loss in iteration 78 : 0.4614195815885034
Loss in iteration 79 : 0.46138523683339794
Loss in iteration 80 : 0.46135201263677805
Loss in iteration 81 : 0.461319853586603
Loss in iteration 82 : 0.46128869859698707
Loss in iteration 83 : 0.4612584832219577
Loss in iteration 84 : 0.46122914170261664
Loss in iteration 85 : 0.4612006087281269
Loss in iteration 86 : 0.4611728209031919
Loss in iteration 87 : 0.4611457179242154
Loss in iteration 88 : 0.4611192434738666
Loss in iteration 89 : 0.4610933458500754
Loss in iteration 90 : 0.46106797835094115
Loss in iteration 91 : 0.461043099441488
Loss in iteration 92 : 0.46101867273176195
Loss in iteration 93 : 0.46099466679802803
Loss in iteration 94 : 0.4609710548798425
Loss in iteration 95 : 0.4609478144853439
Loss in iteration 96 : 0.4609249269354009
Loss in iteration 97 : 0.4609023768745132
Loss in iteration 98 : 0.46088015177269714
Loss in iteration 99 : 0.4608582414385295
Loss in iteration 100 : 0.4608366375592387
Loss in iteration 101 : 0.46081533327956176
Loss in iteration 102 : 0.46079432282718913
Loss in iteration 103 : 0.46077360118923955
Loss in iteration 104 : 0.46075316384127596
Loss in iteration 105 : 0.4607330065280455
Loss in iteration 106 : 0.46071312509327833
Loss in iteration 107 : 0.46069351535457276
Loss in iteration 108 : 0.46067417301839636
Loss in iteration 109 : 0.46065509362976936
Loss in iteration 110 : 0.4606362725508703
Loss in iteration 111 : 0.460617704962855
Loss in iteration 112 : 0.46059938588540916
Loss in iteration 113 : 0.4605813102089488
Loss in iteration 114 : 0.4605634727348931
Loss in iteration 115 : 0.4605458682200912
Loss in iteration 116 : 0.4605284914221131
Loss in iteration 117 : 0.46051133714280207
Loss in iteration 118 : 0.4604944002681633
Loss in iteration 119 : 0.46047767580323046
Loss in iteration 120 : 0.4604611589011163
Loss in iteration 121 : 0.46044484488592097
Loss in iteration 122 : 0.4604287292694906
Loss in iteration 123 : 0.46041280776236965
Loss in iteration 124 : 0.4603970762794224
Loss in iteration 125 : 0.46038153094078904
Loss in iteration 126 : 0.46036616806885927
Loss in iteration 127 : 0.46035098418201503
Loss in iteration 128 : 0.46033597598583736
Loss in iteration 129 : 0.46032114036244215
Loss in iteration 130 : 0.4603064743585826
Loss in iteration 131 : 0.4602919751730132
Loss in iteration 132 : 0.46027764014361455
Loss in iteration 133 : 0.4602634667346218
Loss in iteration 134 : 0.46024945252430194
Loss in iteration 135 : 0.4602355951932815
Loss in iteration 136 : 0.46022189251370305
Loss in iteration 137 : 0.4602083423393308
Loss in iteration 138 : 0.46019494259664895
Loss in iteration 139 : 0.4601816912769826
Loss in iteration 140 : 0.4601685864296004
Loss in iteration 141 : 0.46015562615580163
Loss in iteration 142 : 0.4601428086038597
Loss in iteration 143 : 0.460130131964804
Loss in iteration 144 : 0.4601175944689072
Loss in iteration 145 : 0.4601051943828204
Loss in iteration 146 : 0.46009293000724494
Loss in iteration 147 : 0.46008079967507765
Loss in iteration 148 : 0.46006880174990544
Loss in iteration 149 : 0.4600569346248352
Loss in iteration 150 : 0.4600451967215354
Loss in iteration 151 : 0.4600335864894576
Loss in iteration 152 : 0.4600221024051849
Loss in iteration 153 : 0.4600107429718632
Loss in iteration 154 : 0.4599995067186789
Loss in iteration 155 : 0.459988392200354
Loss in iteration 156 : 0.4599773979966449
Loss in iteration 157 : 0.45996652271182914
Loss in iteration 158 : 0.4599557649741617
Loss in iteration 159 : 0.4599451234353164
Loss in iteration 160 : 0.4599345967697858
Loss in iteration 161 : 0.4599241836742651
Loss in iteration 162 : 0.4599138828670172
Loss in iteration 163 : 0.45990369308720913
Loss in iteration 164 : 0.4598936130942502
Loss in iteration 165 : 0.4598836416671252
Loss in iteration 166 : 0.45987377760372816
Loss in iteration 167 : 0.45986401972020957
Loss in iteration 168 : 0.45985436685034325
Loss in iteration 169 : 0.4598448178449058
Loss in iteration 170 : 0.45983537157108645
Loss in iteration 171 : 0.4598260269119292
Loss in iteration 172 : 0.4598167827657985
Loss in iteration 173 : 0.4598076380458788
Loss in iteration 174 : 0.45979859167970855
Loss in iteration 175 : 0.4597896426087518
Loss in iteration 176 : 0.45978078978798714
Loss in iteration 177 : 0.45977203218553553
Loss in iteration 178 : 0.45976336878232255
Loss in iteration 179 : 0.4597547985717523
Loss in iteration 180 : 0.4597463205594249
Loss in iteration 181 : 0.4597379337628514
Loss in iteration 182 : 0.4597296372112097
Loss in iteration 183 : 0.4597214299451075
Loss in iteration 184 : 0.45971331101635793
Loss in iteration 185 : 0.45970527948778067
Loss in iteration 186 : 0.45969733443299576
Loss in iteration 187 : 0.4596894749362484
Loss in iteration 188 : 0.459681700092219
Loss in iteration 189 : 0.45967400900585764
Loss in iteration 190 : 0.45966640079221865
Loss in iteration 191 : 0.4596588745763003
Loss in iteration 192 : 0.4596514294928863
Loss in iteration 193 : 0.4596440646863956
Loss in iteration 194 : 0.4596367793107394
Loss in iteration 195 : 0.4596295725291641
Loss in iteration 196 : 0.459622443514129
Loss in iteration 197 : 0.4596153914471496
Loss in iteration 198 : 0.459608415518675
Loss in iteration 199 : 0.45960151492795226
Loss in iteration 200 : 0.45959468888290084
Loss in iteration 201 : 0.45958793659998637
Loss in iteration 202 : 0.45958125730409544
Loss in iteration 203 : 0.45957465022842575
Loss in iteration 204 : 0.4595681146143603
Loss in iteration 205 : 0.45956164971136176
Loss in iteration 206 : 0.4595552547768643
Loss in iteration 207 : 0.459548929076164
Loss in iteration 208 : 0.45954267188231235
Loss in iteration 209 : 0.4595364824760239
Loss in iteration 210 : 0.45953036014557247
Loss in iteration 211 : 0.45952430418670054
Loss in iteration 212 : 0.45951831390251663
Loss in iteration 213 : 0.459512388603417
Loss in iteration 214 : 0.4595065276069919
Loss in iteration 215 : 0.4595007302379306
Loss in iteration 216 : 0.45949499582795783
Loss in iteration 217 : 0.459489323715728
Loss in iteration 218 : 0.4594837132467604
Loss in iteration 219 : 0.45947816377335515
Loss in iteration 220 : 0.45947267465451
Loss in iteration 221 : 0.45946724525585747
Loss in iteration 222 : 0.45946187494957536
Loss in iteration 223 : 0.4594565631143246
Loss in iteration 224 : 0.45945130913517074
Loss in iteration 225 : 0.4594461124035191
Loss in iteration 226 : 0.4594409723170357
Loss in iteration 227 : 0.45943588827959014
Loss in iteration 228 : 0.4594308597011775
Testing accuracy  of updater 8 on alg 0 with rate 0.8 = 0.789125, training accuracy 0.789125, time elapsed: 4406 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6774903202996975
Loss in iteration 3 : 0.6660365229340838
Loss in iteration 4 : 0.652644722704028
Loss in iteration 5 : 0.6386642063388089
Loss in iteration 6 : 0.624832923846787
Loss in iteration 7 : 0.6111416520442192
Loss in iteration 8 : 0.5978684626017973
Loss in iteration 9 : 0.5853833688772495
Loss in iteration 10 : 0.573906219356837
Loss in iteration 11 : 0.5635135176592599
Loss in iteration 12 : 0.5541923040982447
Loss in iteration 13 : 0.5458774949674522
Loss in iteration 14 : 0.5384803843994461
Loss in iteration 15 : 0.5319088611423703
Loss in iteration 16 : 0.5260772738613497
Loss in iteration 17 : 0.5209086238759455
Loss in iteration 18 : 0.516333348779992
Loss in iteration 19 : 0.5122875102624393
Loss in iteration 20 : 0.508711549654656
Loss in iteration 21 : 0.5055498376624905
Loss in iteration 22 : 0.5027508220141118
Loss in iteration 23 : 0.5002674383638277
Loss in iteration 24 : 0.49805748681943485
Loss in iteration 25 : 0.49608379844891737
Loss in iteration 26 : 0.4943141405794929
Loss in iteration 27 : 0.49272089302116684
Loss in iteration 28 : 0.4912805658435691
Loss in iteration 29 : 0.4899732372097445
Loss in iteration 30 : 0.4887819811067053
Loss in iteration 31 : 0.48769233822881053
Loss in iteration 32 : 0.48669186358677735
Loss in iteration 33 : 0.4857697649778079
Loss in iteration 34 : 0.48491662997294155
Loss in iteration 35 : 0.48412422748248607
Loss in iteration 36 : 0.4833853640688443
Loss in iteration 37 : 0.4826937746201969
Loss in iteration 38 : 0.48204403044107
Loss in iteration 39 : 0.4814314534013613
Loss in iteration 40 : 0.4808520306686348
Loss in iteration 41 : 0.4803023293349222
Loss in iteration 42 : 0.4797794132194539
Loss in iteration 43 : 0.47928076522329627
Loss in iteration 44 : 0.47880421824121483
Loss in iteration 45 : 0.4783478964232832
Loss in iteration 46 : 0.4779101671308354
Loss in iteration 47 : 0.477489602695474
Loss in iteration 48 : 0.47708495030157244
Loss in iteration 49 : 0.4766951080175166
Loss in iteration 50 : 0.4763191051142965
Loss in iteration 51 : 0.47595608518486127
Loss in iteration 52 : 0.4756052910608053
Loss in iteration 53 : 0.47526605098960295
Loss in iteration 54 : 0.47493776590467496
Loss in iteration 55 : 0.4746198978554697
Loss in iteration 56 : 0.47431195976489005
Loss in iteration 57 : 0.47401350667142583
Loss in iteration 58 : 0.4737241285308952
Loss in iteration 59 : 0.4734434445384969
Loss in iteration 60 : 0.47317109882125735
Loss in iteration 61 : 0.4729067572692049
Loss in iteration 62 : 0.4726501052334216
Loss in iteration 63 : 0.4724008458214323
Loss in iteration 64 : 0.4721586985568211
Loss in iteration 65 : 0.4719233982274818
Loss in iteration 66 : 0.4716946938106425
Loss in iteration 67 : 0.4714723474208663
Loss in iteration 68 : 0.4712561332713076
Loss in iteration 69 : 0.47104583666548405
Loss in iteration 70 : 0.47084125304753704
Loss in iteration 71 : 0.4706421871370653
Loss in iteration 72 : 0.4704484521652928
Loss in iteration 73 : 0.47025986921719537
Loss in iteration 74 : 0.47007626667325164
Loss in iteration 75 : 0.4698974797368754
Loss in iteration 76 : 0.46972335003014987
Loss in iteration 77 : 0.46955372524074973
Loss in iteration 78 : 0.46938845880596264
Loss in iteration 79 : 0.4692274096238808
Loss in iteration 80 : 0.46907044178616536
Loss in iteration 81 : 0.46891742433011724
Loss in iteration 82 : 0.46876823100995296
Loss in iteration 83 : 0.4686227400878909
Loss in iteration 84 : 0.4684808341453352
Loss in iteration 85 : 0.46834239991346827
Loss in iteration 86 : 0.4682073281213595
Loss in iteration 87 : 0.46807551335872494
Loss in iteration 88 : 0.4679468539498486
Loss in iteration 89 : 0.4678212518351398
Loss in iteration 90 : 0.46769861245716016
Loss in iteration 91 : 0.46757884464869465
Loss in iteration 92 : 0.46746186052129496
Loss in iteration 93 : 0.46734757535361565
Loss in iteration 94 : 0.4672359074795339
Loss in iteration 95 : 0.4671267781765996
Loss in iteration 96 : 0.46702011155560497
Loss in iteration 97 : 0.46691583445209195
Loss in iteration 98 : 0.4668138763205652
Loss in iteration 99 : 0.4667141691319544
Loss in iteration 100 : 0.4666166472746623
Loss in iteration 101 : 0.4665212474593282
Loss in iteration 102 : 0.46642790862731115
Loss in iteration 103 : 0.466336571862742
Loss in iteration 104 : 0.4662471803080408
Loss in iteration 105 : 0.46615967908269323
Loss in iteration 106 : 0.4660740152051839
Loss in iteration 107 : 0.4659901375179502
Loss in iteration 108 : 0.46590799661528304
Loss in iteration 109 : 0.4658275447741012
Loss in iteration 110 : 0.46574873588750615
Loss in iteration 111 : 0.46567152540107587
Loss in iteration 112 : 0.46559587025177857
Loss in iteration 113 : 0.4655217288094107
Loss in iteration 114 : 0.46544906082047394
Loss in iteration 115 : 0.46537782735435745
Loss in iteration 116 : 0.46530799075173057
Loss in iteration 117 : 0.4652395145750459
Loss in iteration 118 : 0.4651723635610741
Loss in iteration 119 : 0.46510650357536726
Loss in iteration 120 : 0.46504190156863145
Loss in iteration 121 : 0.46497852553489166
Loss in iteration 122 : 0.4649163444714454
Loss in iteration 123 : 0.46485532834052457
Loss in iteration 124 : 0.4647954480326108
Loss in iteration 125 : 0.4647366753313641
Loss in iteration 126 : 0.46467898288008574
Loss in iteration 127 : 0.46462234414966774
Loss in iteration 128 : 0.4645667334079485
Loss in iteration 129 : 0.46451212569043093
Loss in iteration 130 : 0.46445849677227036
Loss in iteration 131 : 0.4644058231414921
Loss in iteration 132 : 0.4643540819733651
Loss in iteration 133 : 0.46430325110588144
Loss in iteration 134 : 0.4642533090162686
Loss in iteration 135 : 0.46420423479851514
Loss in iteration 136 : 0.46415600814181973
Loss in iteration 137 : 0.46410860930995945
Loss in iteration 138 : 0.46406201912150175
Loss in iteration 139 : 0.4640162189308321
Loss in iteration 140 : 0.46397119060996067
Loss in iteration 141 : 0.46392691653107854
Loss in iteration 142 : 0.463883379549801
Loss in iteration 143 : 0.46384056298910387
Loss in iteration 144 : 0.46379845062388814
Loss in iteration 145 : 0.46375702666617324
Loss in iteration 146 : 0.4637162757508711
Loss in iteration 147 : 0.46367618292212115
Loss in iteration 148 : 0.463636733620156
Loss in iteration 149 : 0.4635979136687041
Loss in iteration 150 : 0.46355970926284984
Loss in iteration 151 : 0.4635221069573922
Loss in iteration 152 : 0.4634850936556264
Loss in iteration 153 : 0.46344865659857803
Loss in iteration 154 : 0.4634127833546255
Loss in iteration 155 : 0.4633774618095202
Loss in iteration 156 : 0.4633426801567855
Loss in iteration 157 : 0.46330842688846335
Loss in iteration 158 : 0.46327469078620387
Loss in iteration 159 : 0.4632414609126782
Loss in iteration 160 : 0.46320872660330425
Loss in iteration 161 : 0.4631764774582662
Loss in iteration 162 : 0.4631447033348201
Loss in iteration 163 : 0.46311339433986126
Loss in iteration 164 : 0.4630825408227654
Loss in iteration 165 : 0.46305213336846734
Loss in iteration 166 : 0.4630221627907794
Loss in iteration 167 : 0.4629926201259333
Loss in iteration 168 : 0.4629634966263502
Loss in iteration 169 : 0.4629347837546072
Loss in iteration 170 : 0.46290647317761047
Loss in iteration 171 : 0.4628785567609639
Loss in iteration 172 : 0.4628510265635082
Loss in iteration 173 : 0.4628238748320578
Loss in iteration 174 : 0.4627970939962838
Loss in iteration 175 : 0.4627706766637785
Loss in iteration 176 : 0.4627446156152711
Loss in iteration 177 : 0.46271890379999026
Loss in iteration 178 : 0.4626935343311767
Loss in iteration 179 : 0.46266850048173114
Loss in iteration 180 : 0.46264379568000474
Loss in iteration 181 : 0.46261941350570274
Loss in iteration 182 : 0.4625953476859312
Loss in iteration 183 : 0.46257159209135595
Loss in iteration 184 : 0.46254814073247
Loss in iteration 185 : 0.46252498775598644
Loss in iteration 186 : 0.4625021274413243
Loss in iteration 187 : 0.4624795541972137
Loss in iteration 188 : 0.46245726255838693
Loss in iteration 189 : 0.46243524718237106
Loss in iteration 190 : 0.4624135028463826
Loss in iteration 191 : 0.4623920244443006
Loss in iteration 192 : 0.46237080698372607
Loss in iteration 193 : 0.46234984558313896
Loss in iteration 194 : 0.4623291354691214
Loss in iteration 195 : 0.4623086719736738
Loss in iteration 196 : 0.46228845053158907
Loss in iteration 197 : 0.46226846667792193
Loss in iteration 198 : 0.462248716045511
Loss in iteration 199 : 0.4622291943625787
Loss in iteration 200 : 0.4622098974503984
Loss in iteration 201 : 0.46219082122102395
Loss in iteration 202 : 0.46217196167507957
Loss in iteration 203 : 0.46215331489961414
Loss in iteration 204 : 0.4621348770660109
Loss in iteration 205 : 0.46211664442795986
Loss in iteration 206 : 0.46209861331947677
Loss in iteration 207 : 0.46208078015298
Loss in iteration 208 : 0.4620631414174229
Loss in iteration 209 : 0.46204569367646614
Loss in iteration 210 : 0.4620284335667104
Loss in iteration 211 : 0.46201135779596475
Loss in iteration 212 : 0.46199446314157305
Loss in iteration 213 : 0.46197774644877154
Loss in iteration 214 : 0.46196120462910406
Loss in iteration 215 : 0.46194483465885866
Loss in iteration 216 : 0.46192863357757247
Loss in iteration 217 : 0.46191259848654304
Loss in iteration 218 : 0.4618967265474132
Loss in iteration 219 : 0.461881014980761
Loss in iteration 220 : 0.46186546106474596
Loss in iteration 221 : 0.461850062133781
Loss in iteration 222 : 0.46183481557724226
Loss in iteration 223 : 0.4618197188382158
Loss in iteration 224 : 0.46180476941225723
Loss in iteration 225 : 0.4617899648462156
Loss in iteration 226 : 0.4617753027370485
Loss in iteration 227 : 0.4617607807307013
Loss in iteration 228 : 0.4617463965209937
Loss in iteration 229 : 0.4617321478485358
Loss in iteration 230 : 0.4617180324996866
Loss in iteration 231 : 0.4617040483055161
Loss in iteration 232 : 0.4616901931408098
Loss in iteration 233 : 0.46167646492309516
Loss in iteration 234 : 0.46166286161168113
Loss in iteration 235 : 0.4616493812067379
Loss in iteration 236 : 0.46163602174838764
Loss in iteration 237 : 0.46162278131581563
Loss in iteration 238 : 0.46160965802641524
Loss in iteration 239 : 0.46159665003494504
Loss in iteration 240 : 0.46158375553270264
Loss in iteration 241 : 0.46157097274673153
Loss in iteration 242 : 0.46155829993903147
Loss in iteration 243 : 0.46154573540580257
Loss in iteration 244 : 0.4615332774766933
Loss in iteration 245 : 0.46152092451407817
Loss in iteration 246 : 0.46150867491234904
Loss in iteration 247 : 0.4614965270972183
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.78925, training accuracy 0.78925, time elapsed: 4902 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6790734891625118
Loss in iteration 3 : 0.6688109525799066
Loss in iteration 4 : 0.6574411783585911
Loss in iteration 5 : 0.6449734988602586
Loss in iteration 6 : 0.6324741665241149
Loss in iteration 7 : 0.6201629296429956
Loss in iteration 8 : 0.6080514463957195
Loss in iteration 9 : 0.5963589353877942
Loss in iteration 10 : 0.5853461531091732
Loss in iteration 11 : 0.5751662592702039
Loss in iteration 12 : 0.5658683020393589
Loss in iteration 13 : 0.5574396518037767
Loss in iteration 14 : 0.5498333352000259
Loss in iteration 15 : 0.5429845791931038
Loss in iteration 16 : 0.5368235928301429
Loss in iteration 17 : 0.5312839465643353
Loss in iteration 18 : 0.5263057753948996
Loss in iteration 19 : 0.521835508313742
Loss in iteration 20 : 0.5178244045090221
Loss in iteration 21 : 0.5142272470935818
Loss in iteration 22 : 0.5110015980317096
Loss in iteration 23 : 0.5081075666776766
Loss in iteration 24 : 0.5055079184450069
Loss in iteration 25 : 0.5031683369922826
Loss in iteration 26 : 0.5010576796956511
Loss in iteration 27 : 0.49914811996792585
Loss in iteration 28 : 0.49741513401656756
Loss in iteration 29 : 0.4958373412928935
Loss in iteration 30 : 0.4943962360816822
Loss in iteration 31 : 0.4930758555147604
Loss in iteration 32 : 0.49186242553613213
Loss in iteration 33 : 0.4907440179466137
Loss in iteration 34 : 0.48971024192691015
Loss in iteration 35 : 0.48875198362036115
Loss in iteration 36 : 0.48786119825760527
Loss in iteration 37 : 0.487030751805413
Loss in iteration 38 : 0.48625430394665564
Loss in iteration 39 : 0.48552622163668785
Loss in iteration 40 : 0.4848415123248521
Loss in iteration 41 : 0.4841957675942551
Loss in iteration 42 : 0.4835851106840437
Loss in iteration 43 : 0.4830061443338457
Loss in iteration 44 : 0.4824558979905919
Loss in iteration 45 : 0.48193177524499614
Loss in iteration 46 : 0.48143150328889683
Loss in iteration 47 : 0.48095308630441114
Loss in iteration 48 : 0.48049476425262355
Loss in iteration 49 : 0.4800549778097607
Loss in iteration 50 : 0.47963233945487915
Loss in iteration 51 : 0.47922561011819803
Loss in iteration 52 : 0.47883368043626806
Loss in iteration 53 : 0.47845555553459124
Loss in iteration 54 : 0.4780903423247257
Loss in iteration 55 : 0.4777372384924238
Loss in iteration 56 : 0.47739552259569656
Loss in iteration 57 : 0.47706454493026607
Loss in iteration 58 : 0.47674371901674406
Loss in iteration 59 : 0.4764325137008911
Loss in iteration 60 : 0.4761304459325793
Loss in iteration 61 : 0.4758370743091994
Loss in iteration 62 : 0.47555199344917914
Loss in iteration 63 : 0.4752748292177562
Loss in iteration 64 : 0.4750052347754425
Loss in iteration 65 : 0.4747428873723286
Loss in iteration 66 : 0.47448748577704375
Loss in iteration 67 : 0.4742387482118234
Loss in iteration 68 : 0.47399641066481946
Loss in iteration 69 : 0.47376022546478863
Loss in iteration 70 : 0.4735299600265089
Loss in iteration 71 : 0.4733053957025081
Loss in iteration 72 : 0.47308632670304657
Loss in iteration 73 : 0.47287255906812775
Loss in iteration 74 : 0.4726639096906606
Loss in iteration 75 : 0.4724602053984821
Loss in iteration 76 : 0.4722612821053909
Loss in iteration 77 : 0.47206698403954084
Loss in iteration 78 : 0.47187716305316324
Loss in iteration 79 : 0.47169167801247047
Loss in iteration 80 : 0.4715103942621424
Loss in iteration 81 : 0.471333183155571
Loss in iteration 82 : 0.47115992164065257
Loss in iteration 83 : 0.47099049189093734
Loss in iteration 84 : 0.47082478097317765
Loss in iteration 85 : 0.4706626805442154
Loss in iteration 86 : 0.4705040865721778
Loss in iteration 87 : 0.4703488990788518
Loss in iteration 88 : 0.47019702190146245
Loss in iteration 89 : 0.47004836247306053
Loss in iteration 90 : 0.469902831620961
Loss in iteration 91 : 0.4697603433827723
Loss in iteration 92 : 0.4696208148391577
Loss in iteration 93 : 0.4694841659621489
Loss in iteration 94 : 0.46935031947741707
Loss in iteration 95 : 0.4692192007387182
Loss in iteration 96 : 0.4690907376125973
Loss in iteration 97 : 0.4689648603715574
Loss in iteration 98 : 0.46884150159410937
Loss in iteration 99 : 0.4687205960704058
Loss in iteration 100 : 0.4686020807125404
Loss in iteration 101 : 0.46848589446883376
Loss in iteration 102 : 0.46837197824181553
Loss in iteration 103 : 0.46826027480965604
Loss in iteration 104 : 0.4681507287510509
Loss in iteration 105 : 0.46804328637353093
Loss in iteration 106 : 0.4679378956452015
Loss in iteration 107 : 0.4678345061298462
Loss in iteration 108 : 0.4677330689253363
Loss in iteration 109 : 0.4676335366051701
Loss in iteration 110 : 0.46753586316298174
Loss in iteration 111 : 0.46744000395981244
Loss in iteration 112 : 0.4673459156739346
Loss in iteration 113 : 0.4672535562530414
Loss in iteration 114 : 0.4671628848686184
Loss in iteration 115 : 0.4670738618723379
Loss in iteration 116 : 0.4669864487543633
Loss in iteration 117 : 0.46690060810344347
Loss in iteration 118 : 0.4668163035686982
Loss in iteration 119 : 0.46673349982302836
Loss in iteration 120 : 0.46665216252805725
Loss in iteration 121 : 0.46657225830054677
Loss in iteration 122 : 0.4664937546801996
Loss in iteration 123 : 0.4664166200987961
Loss in iteration 124 : 0.46634082385055886
Loss in iteration 125 : 0.4662663360637357
Loss in iteration 126 : 0.4661931276732674
Loss in iteration 127 : 0.4661211703945445
Loss in iteration 128 : 0.46605043669815177
Loss in iteration 129 : 0.46598089978557977
Loss in iteration 130 : 0.46591253356584983
Loss in iteration 131 : 0.4658453126330079
Loss in iteration 132 : 0.46577921224447927
Loss in iteration 133 : 0.465714208300202
Loss in iteration 134 : 0.4656502773225629
Loss in iteration 135 : 0.465587396437071
Loss in iteration 136 : 0.46552554335374946
Loss in iteration 137 : 0.4654646963492157
Loss in iteration 138 : 0.46540483424943013
Loss in iteration 139 : 0.46534593641306876
Loss in iteration 140 : 0.4652879827155011
Loss in iteration 141 : 0.4652309535333613
Loss in iteration 142 : 0.46517482972965063
Loss in iteration 143 : 0.46511959263939046
Loss in iteration 144 : 0.46506522405576095
Loss in iteration 145 : 0.46501170621674326
Loss in iteration 146 : 0.4649590217922098
Loss in iteration 147 : 0.46490715387146286
Loss in iteration 148 : 0.46485608595120587
Loss in iteration 149 : 0.46480580192391524
Loss in iteration 150 : 0.4647562860665962
Loss in iteration 151 : 0.46470752302993906
Loss in iteration 152 : 0.46465949782781235
Loss in iteration 153 : 0.46461219582711377
Loss in iteration 154 : 0.46456560273794745
Loss in iteration 155 : 0.46451970460414205
Loss in iteration 156 : 0.4644744877940421
Loss in iteration 157 : 0.464429938991636
Loss in iteration 158 : 0.46438604518793547
Loss in iteration 159 : 0.464342793672659
Loss in iteration 160 : 0.46430017202615037
Loss in iteration 161 : 0.46425816811158577
Loss in iteration 162 : 0.4642167700673997
Loss in iteration 163 : 0.46417596629996355
Loss in iteration 164 : 0.464135745476486
Loss in iteration 165 : 0.46409609651814354
Loss in iteration 166 : 0.46405700859340904
Loss in iteration 167 : 0.46401847111160227
Loss in iteration 168 : 0.463980473716627
Loss in iteration 169 : 0.4639430062809007
Loss in iteration 170 : 0.46390605889947734
Loss in iteration 171 : 0.46386962188433195
Loss in iteration 172 : 0.4638336857588389
Loss in iteration 173 : 0.46379824125239066
Loss in iteration 174 : 0.4637632792951965
Loss in iteration 175 : 0.46372879101322767
Loss in iteration 176 : 0.46369476772330986
Loss in iteration 177 : 0.46366120092835966
Loss in iteration 178 : 0.46362808231276775
Loss in iteration 179 : 0.46359540373790686
Loss in iteration 180 : 0.46356315723777713
Loss in iteration 181 : 0.46353133501476285
Loss in iteration 182 : 0.46349992943552987
Loss in iteration 183 : 0.4634689330270338
Loss in iteration 184 : 0.46343833847262905
Loss in iteration 185 : 0.4634081386082961
Loss in iteration 186 : 0.46337832641898796
Loss in iteration 187 : 0.46334889503505383
Loss in iteration 188 : 0.463319837728778
Loss in iteration 189 : 0.4632911479110125
Loss in iteration 190 : 0.4632628191279047
Loss in iteration 191 : 0.4632348450577036
Loss in iteration 192 : 0.46320721950766497
Loss in iteration 193 : 0.46317993641103705
Loss in iteration 194 : 0.46315298982412784
Loss in iteration 195 : 0.463126373923448
Loss in iteration 196 : 0.46310008300293587
Loss in iteration 197 : 0.46307411147125094
Loss in iteration 198 : 0.4630484538491489
Loss in iteration 199 : 0.4630231047669103
Loss in iteration 200 : 0.46299805896185353
Loss in iteration 201 : 0.4629733112759026
Loss in iteration 202 : 0.4629488566532238
Loss in iteration 203 : 0.4629246901379145
Loss in iteration 204 : 0.4629008068717715
Loss in iteration 205 : 0.4628772020920902
Loss in iteration 206 : 0.46285387112954535
Loss in iteration 207 : 0.46283080940610904
Loss in iteration 208 : 0.462808012433032
Loss in iteration 209 : 0.46278547580886825
Loss in iteration 210 : 0.46276319521755294
Loss in iteration 211 : 0.4627411664265345
Loss in iteration 212 : 0.46271938528494033
Loss in iteration 213 : 0.4626978477218033
Loss in iteration 214 : 0.4626765497443198
Loss in iteration 215 : 0.4626554874361569
Loss in iteration 216 : 0.4626346569558041
Loss in iteration 217 : 0.46261405453496157
Loss in iteration 218 : 0.46259367647696376
Loss in iteration 219 : 0.4625735191552498
Loss in iteration 220 : 0.46255357901187255
Loss in iteration 221 : 0.4625338525560241
Loss in iteration 222 : 0.46251433636262923
Loss in iteration 223 : 0.46249502707094226
Loss in iteration 224 : 0.46247592138319876
Loss in iteration 225 : 0.46245701606328304
Loss in iteration 226 : 0.46243830793544083
Loss in iteration 227 : 0.462419793883016
Loss in iteration 228 : 0.4624014708472179
Loss in iteration 229 : 0.46238333582591856
Loss in iteration 230 : 0.4623653858724754
Loss in iteration 231 : 0.4623476180945866
Loss in iteration 232 : 0.46233002965316855
Loss in iteration 233 : 0.46231261776126087
Loss in iteration 234 : 0.46229537968296097
Loss in iteration 235 : 0.4622783127323703
Loss in iteration 236 : 0.4622614142725838
Loss in iteration 237 : 0.4622446817146902
Loss in iteration 238 : 0.4622281125167914
Loss in iteration 239 : 0.46221170418305557
Loss in iteration 240 : 0.4621954542627897
Loss in iteration 241 : 0.4621793603495228
Loss in iteration 242 : 0.46216342008012024
Loss in iteration 243 : 0.4621476311339196
Loss in iteration 244 : 0.462131991231871
Loss in iteration 245 : 0.46211649813571715
Loss in iteration 246 : 0.4621011496471781
Loss in iteration 247 : 0.46208594360714983
Loss in iteration 248 : 0.46207087789494367
Loss in iteration 249 : 0.46205595042751363
Loss in iteration 250 : 0.4620411591587174
Loss in iteration 251 : 0.46202650207859747
Loss in iteration 252 : 0.46201197721266174
Loss in iteration 253 : 0.4619975826211969
Loss in iteration 254 : 0.4619833163985826
Loss in iteration 255 : 0.4619691766726298
Loss in iteration 256 : 0.4619551616039355
Testing accuracy  of updater 8 on alg 0 with rate 0.14 = 0.790375, training accuracy 0.790375, time elapsed: 5439 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6826356567310199
Loss in iteration 3 : 0.6740554570330047
Loss in iteration 4 : 0.6659602707909438
Loss in iteration 5 : 0.6570400113745946
Loss in iteration 6 : 0.6473823715305631
Loss in iteration 7 : 0.6375760800762453
Loss in iteration 8 : 0.6279396897013199
Loss in iteration 9 : 0.6184943161430893
Loss in iteration 10 : 0.6092312221313939
Loss in iteration 11 : 0.600237301569951
Loss in iteration 12 : 0.5916455847988243
Loss in iteration 13 : 0.583557565248535
Loss in iteration 14 : 0.5760177318735853
Loss in iteration 15 : 0.5690284097328048
Loss in iteration 16 : 0.5625708364616469
Loss in iteration 17 : 0.5566170321879068
Loss in iteration 18 : 0.5511341835089338
Loss in iteration 19 : 0.5460869920330489
Loss in iteration 20 : 0.5414402438466408
Loss in iteration 21 : 0.5371610536033935
Loss in iteration 22 : 0.5332198963889594
Loss in iteration 23 : 0.5295904154330062
Loss in iteration 24 : 0.5262486319721524
Loss in iteration 25 : 0.5231721770562654
Loss in iteration 26 : 0.5203398272685891
Loss in iteration 27 : 0.5177313342211667
Loss in iteration 28 : 0.5153274279712596
Loss in iteration 29 : 0.5131098921352946
Loss in iteration 30 : 0.5110616572024613
Loss in iteration 31 : 0.509166887665027
Loss in iteration 32 : 0.5074110463066127
Loss in iteration 33 : 0.5057809212027726
Loss in iteration 34 : 0.5042646075180884
Loss in iteration 35 : 0.5028514462174453
Loss in iteration 36 : 0.5015319301493995
Loss in iteration 37 : 0.5002975913315656
Loss in iteration 38 : 0.4991408820643025
Loss in iteration 39 : 0.4980550591033525
Loss in iteration 40 : 0.497034076697886
Loss in iteration 41 : 0.496072491775601
Loss in iteration 42 : 0.49516538285016715
Loss in iteration 43 : 0.4943082829366472
Loss in iteration 44 : 0.4934971256498217
Loss in iteration 45 : 0.4927282027468838
Loss in iteration 46 : 0.49199813078803595
Loss in iteration 47 : 0.49130382440445464
Loss in iteration 48 : 0.49064247384858795
Loss in iteration 49 : 0.4900115249405815
Loss in iteration 50 : 0.489408660079486
Loss in iteration 51 : 0.48883177954528345
Loss in iteration 52 : 0.4882789828019535
Loss in iteration 53 : 0.48774854988046196
Loss in iteration 54 : 0.4872389231559644
Loss in iteration 55 : 0.48674868993728454
Loss in iteration 56 : 0.48627656627719373
Loss in iteration 57 : 0.48582138231976113
Loss in iteration 58 : 0.48538206936385425
Loss in iteration 59 : 0.48495764867530033
Loss in iteration 60 : 0.4845472219527533
Loss in iteration 61 : 0.48414996326072146
Loss in iteration 62 : 0.4837651121930961
Loss in iteration 63 : 0.4833919680188554
Loss in iteration 64 : 0.4830298845796149
Loss in iteration 65 : 0.48267826574587125
Loss in iteration 66 : 0.48233656128465263
Loss in iteration 67 : 0.4820042630375359
Loss in iteration 68 : 0.4816809013485663
Loss in iteration 69 : 0.4813660417132188
Loss in iteration 70 : 0.48105928164118644
Loss in iteration 71 : 0.48076024773787157
Loss in iteration 72 : 0.48046859301378536
Loss in iteration 73 : 0.48018399442963017
Loss in iteration 74 : 0.4799061506798555
Loss in iteration 75 : 0.4796347802108003
Loss in iteration 76 : 0.4793696194626484
Loss in iteration 77 : 0.4791104213185305
Loss in iteration 78 : 0.47885695373964915
Loss in iteration 79 : 0.4786089985628632
Loss in iteration 80 : 0.4783663504365095
Loss in iteration 81 : 0.4781288158713428
Loss in iteration 82 : 0.47789621238581254
Loss in iteration 83 : 0.4776683677281609
Loss in iteration 84 : 0.47744511916141336
Loss in iteration 85 : 0.47722631280098116
Loss in iteration 86 : 0.477011802997761
Loss in iteration 87 : 0.4768014517622251
Loss in iteration 88 : 0.47659512822697114
Loss in iteration 89 : 0.47639270814629203
Loss in iteration 90 : 0.4761940734319435
Loss in iteration 91 : 0.47599911172432013
Loss in iteration 92 : 0.47580771599800087
Loss in iteration 93 : 0.475619784200165
Loss in iteration 94 : 0.4754352189199473
Loss in iteration 95 : 0.47525392708640424
Loss in iteration 96 : 0.47507581969250623
Loss in iteration 97 : 0.47490081154249675
Loss in iteration 98 : 0.4747288210200191
Loss in iteration 99 : 0.4745597698745828
Loss in iteration 100 : 0.47439358302425116
Loss in iteration 101 : 0.47423018837269215
Loss in iteration 102 : 0.47406951663910124
Loss in iteration 103 : 0.4739115011997651
Loss in iteration 104 : 0.47375607794028457
Loss in iteration 105 : 0.4736031851176929
Loss in iteration 106 : 0.47345276323178914
Loss in iteration 107 : 0.47330475490516205
Loss in iteration 108 : 0.47315910477135903
Loss in iteration 109 : 0.4730157593707395
Loss in iteration 110 : 0.47287466705348646
Loss in iteration 111 : 0.4727357778893122
Loss in iteration 112 : 0.4725990435833501
Loss in iteration 113 : 0.4724644173977425
Loss in iteration 114 : 0.47233185407843553
Loss in iteration 115 : 0.47220130978673497
Loss in iteration 116 : 0.4720727420351966
Loss in iteration 117 : 0.47194610962745626
Loss in iteration 118 : 0.471821372601662
Loss in iteration 119 : 0.4716984921771915
Loss in iteration 120 : 0.47157743070441555
Loss in iteration 121 : 0.4714581516172416
Loss in iteration 122 : 0.47134061938827504
Loss in iteration 123 : 0.4712247994864209
Loss in iteration 124 : 0.4711106583367524
Loss in iteration 125 : 0.4709981632825367
Loss in iteration 126 : 0.4708872825492763
Loss in iteration 127 : 0.47077798521064607
Loss in iteration 128 : 0.4706702411562293
Loss in iteration 129 : 0.4705640210609308
Loss in iteration 130 : 0.47045929635595224
Loss in iteration 131 : 0.4703560392012633
Loss in iteration 132 : 0.4702542224594369
Loss in iteration 133 : 0.4701538196707811
Loss in iteration 134 : 0.47005480502968006
Loss in iteration 135 : 0.46995715336206595
Loss in iteration 136 : 0.4698608401039326
Loss in iteration 137 : 0.4697658412808687
Loss in iteration 138 : 0.4696721334884893
Loss in iteration 139 : 0.46957969387377624
Loss in iteration 140 : 0.46948850011722043
Loss in iteration 141 : 0.46939853041575874
Loss in iteration 142 : 0.4693097634664393
Loss in iteration 143 : 0.46922217845079006
Loss in iteration 144 : 0.46913575501984456
Loss in iteration 145 : 0.4690504732797907
Loss in iteration 146 : 0.46896631377822057
Loss in iteration 147 : 0.4688832574909276
Loss in iteration 148 : 0.46880128580923996
Loss in iteration 149 : 0.4687203805278748
Loss in iteration 150 : 0.4686405238332375
Loss in iteration 151 : 0.4685616982922013
Loss in iteration 152 : 0.46848388684130504
Loss in iteration 153 : 0.46840707277636845
Loss in iteration 154 : 0.46833123974248
Loss in iteration 155 : 0.46825637172437273
Loss in iteration 156 : 0.46818245303713896
Loss in iteration 157 : 0.46810946831728767
Loss in iteration 158 : 0.4680374025141198
Loss in iteration 159 : 0.4679662408814085
Loss in iteration 160 : 0.46789596896937763
Loss in iteration 161 : 0.46782657261693933
Loss in iteration 162 : 0.4677580379442316
Loss in iteration 163 : 0.46769035134537557
Loss in iteration 164 : 0.4676234994815058
Loss in iteration 165 : 0.46755746927401154
Loss in iteration 166 : 0.46749224789801996
Loss in iteration 167 : 0.4674278227760823
Loss in iteration 168 : 0.46736418157206766
Loss in iteration 169 : 0.46730131218525234
Loss in iteration 170 : 0.4672392027446033
Loss in iteration 171 : 0.4671778416032378
Loss in iteration 172 : 0.4671172173330551
Loss in iteration 173 : 0.46705731871953926
Loss in iteration 174 : 0.4669981347567168
Loss in iteration 175 : 0.4669396546422744
Loss in iteration 176 : 0.4668818677728138
Loss in iteration 177 : 0.46682476373926307
Loss in iteration 178 : 0.46676833232240916
Loss in iteration 179 : 0.46671256348857454
Loss in iteration 180 : 0.46665744738541215
Loss in iteration 181 : 0.4666029743378234
Loss in iteration 182 : 0.46654913484399535
Loss in iteration 183 : 0.46649591957155173
Loss in iteration 184 : 0.4664433193538084
Loss in iteration 185 : 0.46639132518613724
Loss in iteration 186 : 0.4663399282224293
Loss in iteration 187 : 0.4662891197716608
Loss in iteration 188 : 0.4662388912945397
Loss in iteration 189 : 0.4661892344002597
Loss in iteration 190 : 0.46614014084333355
Loss in iteration 191 : 0.4660916025205125
Loss in iteration 192 : 0.4660436114677813
Loss in iteration 193 : 0.46599615985744863
Loss in iteration 194 : 0.46594923999529364
Loss in iteration 195 : 0.4659028443178049
Loss in iteration 196 : 0.4658569653894751
Loss in iteration 197 : 0.46581159590018056
Loss in iteration 198 : 0.46576672866261276
Loss in iteration 199 : 0.46572235660978706
Loss in iteration 200 : 0.46567847279260477
Loss in iteration 201 : 0.465635070377483
Loss in iteration 202 : 0.4655921426440425
Loss in iteration 203 : 0.46554968298284255
Loss in iteration 204 : 0.46550768489319233
Loss in iteration 205 : 0.4654661419809927
Loss in iteration 206 : 0.4654250479566442
Loss in iteration 207 : 0.4653843966330037
Loss in iteration 208 : 0.4653441819233873
Loss in iteration 209 : 0.46530439783961924
Loss in iteration 210 : 0.465265038490131
Loss in iteration 211 : 0.46522609807810467
Loss in iteration 212 : 0.4651875708996619
Loss in iteration 213 : 0.4651494513420836
Loss in iteration 214 : 0.46511173388208377
Loss in iteration 215 : 0.46507441308411945
Loss in iteration 216 : 0.4650374835987384
Loss in iteration 217 : 0.4650009401609608
Loss in iteration 218 : 0.4649647775887058
Loss in iteration 219 : 0.46492899078124467
Loss in iteration 220 : 0.46489357471769754
Loss in iteration 221 : 0.464858524455554
Loss in iteration 222 : 0.46482383512923686
Loss in iteration 223 : 0.464789501948685
Loss in iteration 224 : 0.46475552019798655
Loss in iteration 225 : 0.4647218852340178
Loss in iteration 226 : 0.46468859248513034
Loss in iteration 227 : 0.46465563744986055
Loss in iteration 228 : 0.46462301569566533
Loss in iteration 229 : 0.46459072285768854
Loss in iteration 230 : 0.46455875463755103
Loss in iteration 231 : 0.464527106802162
Loss in iteration 232 : 0.4644957751825698
Loss in iteration 233 : 0.4644647556728231
Loss in iteration 234 : 0.46443404422885876
Loss in iteration 235 : 0.46440363686741704
Loss in iteration 236 : 0.4643735296649801
Loss in iteration 237 : 0.4643437187567257
Loss in iteration 238 : 0.4643142003355065
Loss in iteration 239 : 0.46428497065085456
Loss in iteration 240 : 0.4642560260079989
Loss in iteration 241 : 0.4642273627669024
Loss in iteration 242 : 0.4641989773413304
Loss in iteration 243 : 0.46417086619792464
Loss in iteration 244 : 0.4641430258553017
Loss in iteration 245 : 0.46411545288316675
Loss in iteration 246 : 0.46408814390145586
Loss in iteration 247 : 0.4640610955794739
Loss in iteration 248 : 0.46403430463507234
Loss in iteration 249 : 0.4640077678338286
Loss in iteration 250 : 0.4639814819882449
Loss in iteration 251 : 0.46395544395697275
Loss in iteration 252 : 0.46392965064402925
Loss in iteration 253 : 0.4639040989980634
Loss in iteration 254 : 0.46387878601159344
Loss in iteration 255 : 0.46385370872030024
Loss in iteration 256 : 0.46382886420230895
Loss in iteration 257 : 0.46380424957749566
Loss in iteration 258 : 0.46377986200679433
Loss in iteration 259 : 0.46375569869153654
Loss in iteration 260 : 0.46373175687278756
Loss in iteration 261 : 0.46370803383070247
Loss in iteration 262 : 0.46368452688389095
Loss in iteration 263 : 0.4636612333887978
Loss in iteration 264 : 0.46363815073909365
Loss in iteration 265 : 0.4636152763650696
Loss in iteration 266 : 0.4635926077330622
Loss in iteration 267 : 0.4635701423448645
Loss in iteration 268 : 0.46354787773716943
Loss in iteration 269 : 0.463525811481006
Loss in iteration 270 : 0.4635039411812027
Loss in iteration 271 : 0.4634822644758486
Loss in iteration 272 : 0.4634607790357653
Loss in iteration 273 : 0.4634394825639942
Loss in iteration 274 : 0.4634183727952925
Loss in iteration 275 : 0.4633974474956282
Loss in iteration 276 : 0.46337670446170387
Loss in iteration 277 : 0.46335614152045895
Loss in iteration 278 : 0.463335756528622
Loss in iteration 279 : 0.4633155473722297
Loss in iteration 280 : 0.463295511966179
Loss in iteration 281 : 0.46327564825378176
Loss in iteration 282 : 0.4632559542063294
Loss in iteration 283 : 0.4632364278226534
Loss in iteration 284 : 0.46321706712871297
Loss in iteration 285 : 0.46319787017717323
Loss in iteration 286 : 0.46317883504700114
Testing accuracy  of updater 8 on alg 0 with rate 0.08000000000000002 = 0.790625, training accuracy 0.790625, time elapsed: 6626 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.68957453864036
Loss in iteration 3 : 0.6853812280993324
Loss in iteration 4 : 0.6811181216951311
Loss in iteration 5 : 0.6770488672020005
Loss in iteration 6 : 0.6731923397012249
Loss in iteration 7 : 0.6694312652562314
Loss in iteration 8 : 0.6656224747520133
Loss in iteration 9 : 0.6616693206403166
Loss in iteration 10 : 0.6575472152657972
Loss in iteration 11 : 0.6532931060170234
Loss in iteration 12 : 0.6489768730692516
Loss in iteration 13 : 0.6446711223477885
Loss in iteration 14 : 0.6404300591232701
Loss in iteration 15 : 0.6362813405812431
Loss in iteration 16 : 0.6322292216724553
Loss in iteration 17 : 0.6282641532758559
Loss in iteration 18 : 0.6243734720883447
Loss in iteration 19 : 0.6205492624106699
Loss in iteration 20 : 0.6167917424442348
Loss in iteration 21 : 0.6131085758979645
Loss in iteration 22 : 0.6095117111832088
Loss in iteration 23 : 0.6060135966852281
Loss in iteration 24 : 0.6026241738286034
Loss in iteration 25 : 0.5993493128912865
Loss in iteration 26 : 0.5961906783662558
Loss in iteration 27 : 0.5931465862323448
Loss in iteration 28 : 0.5902132815952346
Loss in iteration 29 : 0.5873861520470032
Loss in iteration 30 : 0.5846605890952474
Loss in iteration 31 : 0.5820324166355028
Loss in iteration 32 : 0.5794979576337469
Loss in iteration 33 : 0.5770538845684762
Loss in iteration 34 : 0.57469700367064
Loss in iteration 35 : 0.5724240824103853
Loss in iteration 36 : 0.5702317723578815
Loss in iteration 37 : 0.5681166281632893
Loss in iteration 38 : 0.566075190713011
Loss in iteration 39 : 0.5641040913625471
Loss in iteration 40 : 0.5622001401655611
Loss in iteration 41 : 0.5603603764373766
Loss in iteration 42 : 0.5585820770286222
Loss in iteration 43 : 0.5568627307859435
Loss in iteration 44 : 0.5551999943320033
Loss in iteration 45 : 0.5535916447043585
Loss in iteration 46 : 0.5520355404657178
Loss in iteration 47 : 0.5505295971294722
Loss in iteration 48 : 0.5490717773046558
Loss in iteration 49 : 0.5476600922052463
Loss in iteration 50 : 0.5462926095294445
Loss in iteration 51 : 0.5449674629140671
Loss in iteration 52 : 0.5436828595295281
Loss in iteration 53 : 0.5424370841454852
Loss in iteration 54 : 0.5412284995616733
Loss in iteration 55 : 0.5400555443188714
Loss in iteration 56 : 0.5389167290008652
Loss in iteration 57 : 0.5378106323238451
Loss in iteration 58 : 0.5367358977960521
Loss in iteration 59 : 0.5356912312375615
Loss in iteration 60 : 0.5346753990435067
Loss in iteration 61 : 0.5336872268401586
Loss in iteration 62 : 0.532725598132002
Loss in iteration 63 : 0.5317894526254817
Loss in iteration 64 : 0.5308777840740602
Loss in iteration 65 : 0.5299896376535869
Loss in iteration 66 : 0.5291241070002133
Loss in iteration 67 : 0.5282803311037663
Loss in iteration 68 : 0.527457491249376
Loss in iteration 69 : 0.5266548081563198
Loss in iteration 70 : 0.5258715393985584
Loss in iteration 71 : 0.5251069771278715
Loss in iteration 72 : 0.5243604460722393
Loss in iteration 73 : 0.5236313017557637
Loss in iteration 74 : 0.5229189288807998
Loss in iteration 75 : 0.5222227398224486
Loss in iteration 76 : 0.5215421732025071
Loss in iteration 77 : 0.5208766925275765
Loss in iteration 78 : 0.520225784889516
Loss in iteration 79 : 0.5195889597337193
Loss in iteration 80 : 0.5189657477019621
Loss in iteration 81 : 0.5183556995537187
Loss in iteration 82 : 0.5177583851648457
Loss in iteration 83 : 0.5171733925978014
Loss in iteration 84 : 0.5166003272342323
Loss in iteration 85 : 0.516038810959617
Loss in iteration 86 : 0.5154884813903171
Loss in iteration 87 : 0.5149489911355382
Loss in iteration 88 : 0.5144200070892951
Loss in iteration 89 : 0.5139012097499782
Loss in iteration 90 : 0.5133922925669805
Loss in iteration 91 : 0.5128929613149328
Loss in iteration 92 : 0.512402933496372
Loss in iteration 93 : 0.511921937773413
Loss in iteration 94 : 0.5114497134284105
Loss in iteration 95 : 0.510986009853053
Loss in iteration 96 : 0.5105305860647069
Loss in iteration 97 : 0.5100832102486995
Loss in iteration 98 : 0.5096436593249996
Loss in iteration 99 : 0.5092117185379538
Loss in iteration 100 : 0.5087871810678081
Loss in iteration 101 : 0.5083698476629728
Loss in iteration 102 : 0.5079595262920852
Loss in iteration 103 : 0.5075560318150146
Loss in iteration 104 : 0.5071591856719512
Loss in iteration 105 : 0.5067688155896679
Loss in iteration 106 : 0.5063847553040397
Loss in iteration 107 : 0.5060068442977805
Loss in iteration 108 : 0.5056349275524217
Loss in iteration 109 : 0.5052688553135309
Loss in iteration 110 : 0.5049084828681871
Loss in iteration 111 : 0.5045536703338674
Loss in iteration 112 : 0.5042042824579497
Loss in iteration 113 : 0.5038601884270881
Loss in iteration 114 : 0.5035212616858835
Loss in iteration 115 : 0.5031873797642649
Loss in iteration 116 : 0.5028584241130766
Loss in iteration 117 : 0.5025342799474489
Loss in iteration 118 : 0.5022148360975103
Loss in iteration 119 : 0.5018999848660514
Loss in iteration 120 : 0.5015896218927818
Loss in iteration 121 : 0.5012836460248394
Loss in iteration 122 : 0.5009819591932189
Loss in iteration 123 : 0.5006844662948288
Loss in iteration 124 : 0.5003910750798845
Loss in iteration 125 : 0.5001016960443784
Loss in iteration 126 : 0.4998162423273765
Loss in iteration 127 : 0.4995346296128935
Loss in iteration 128 : 0.49925677603614826
Loss in iteration 129 : 0.49898260209395406
Loss in iteration 130 : 0.49871203055906427
Loss in iteration 131 : 0.498444986398266
Loss in iteration 132 : 0.49818139669402817
Loss in iteration 133 : 0.49792119056954787
Loss in iteration 134 : 0.4976642991169988
Loss in iteration 135 : 0.4974106553288376
Loss in iteration 136 : 0.4971601940319973
Loss in iteration 137 : 0.49691285182485456
Loss in iteration 138 : 0.49666856701680046
Loss in iteration 139 : 0.4964272795702984
Loss in iteration 140 : 0.49618893104532874
Loss in iteration 141 : 0.4959534645460627
Loss in iteration 142 : 0.4957208246697027
Loss in iteration 143 : 0.49549095745735555
Loss in iteration 144 : 0.49526381034685524
Loss in iteration 145 : 0.49503933212744367
Loss in iteration 146 : 0.4948174728962249
Loss in iteration 147 : 0.4945981840163016
Loss in iteration 148 : 0.49438141807652586
Loss in iteration 149 : 0.4941671288527901
Loss in iteration 150 : 0.4939552712707937
Loss in iteration 151 : 0.4937458013701987
Loss in iteration 152 : 0.49353867627014697
Loss in iteration 153 : 0.49333385413604497
Loss in iteration 154 : 0.493131294147587
Loss in iteration 155 : 0.4929309564679572
Loss in iteration 156 : 0.4927328022141376
Loss in iteration 157 : 0.4925367934283212
Loss in iteration 158 : 0.4923428930503291
Loss in iteration 159 : 0.4921510648910361
Loss in iteration 160 : 0.4919612736067367
Loss in iteration 161 : 0.49177348467441606
Loss in iteration 162 : 0.4915876643678922
Loss in iteration 163 : 0.4914037797348009
Loss in iteration 164 : 0.4912217985743731
Loss in iteration 165 : 0.49104168941597665
Loss in iteration 166 : 0.4908634214984082
Loss in iteration 167 : 0.4906869647498845
Loss in iteration 168 : 0.4905122897687065
Loss in iteration 169 : 0.4903393678045959
Loss in iteration 170 : 0.4901681707406336
Loss in iteration 171 : 0.48999867107581574
Loss in iteration 172 : 0.48983084190818293
Loss in iteration 173 : 0.48966465691850897
Loss in iteration 174 : 0.48950009035451547
Loss in iteration 175 : 0.4893371170156092
Loss in iteration 176 : 0.4891757122381072
Loss in iteration 177 : 0.4890158518809435
Loss in iteration 178 : 0.48885751231182073
Loss in iteration 179 : 0.4887006703938194
Loss in iteration 180 : 0.4885453034724185
Loss in iteration 181 : 0.4883913893629241
Loss in iteration 182 : 0.48823890633829714
Loss in iteration 183 : 0.48808783311736
Loss in iteration 184 : 0.48793814885335474
Loss in iteration 185 : 0.48778983312287466
Loss in iteration 186 : 0.48764286591511485
Loss in iteration 187 : 0.4874972276214581
Loss in iteration 188 : 0.4873528990253764
Loss in iteration 189 : 0.4872098612926411
Loss in iteration 190 : 0.4870680959618076
Loss in iteration 191 : 0.48692758493500854
Loss in iteration 192 : 0.4867883104689983
Loss in iteration 193 : 0.48665025516647875
Loss in iteration 194 : 0.4865134019676638
Loss in iteration 195 : 0.48637773414209656
Loss in iteration 196 : 0.4862432352807173
Loss in iteration 197 : 0.48610988928812904
Loss in iteration 198 : 0.4859776803751138
Loss in iteration 199 : 0.4858465930513604
Loss in iteration 200 : 0.485716612118377
Loss in iteration 201 : 0.48558772266262645
Loss in iteration 202 : 0.4854599100488466
Loss in iteration 203 : 0.4853331599135611
Loss in iteration 204 : 0.48520745815876115
Loss in iteration 205 : 0.4850827909457786
Loss in iteration 206 : 0.4849591446893141
Loss in iteration 207 : 0.4848365060516384
Loss in iteration 208 : 0.48471486193694474
Loss in iteration 209 : 0.48459419948586574
Loss in iteration 210 : 0.4844745060701241
Loss in iteration 211 : 0.4843557692873446
Loss in iteration 212 : 0.4842379769559886
Loss in iteration 213 : 0.4841211171104387
Loss in iteration 214 : 0.4840051779962032
Loss in iteration 215 : 0.4838901480652536
Loss in iteration 216 : 0.48377601597148373
Loss in iteration 217 : 0.48366277056628454
Loss in iteration 218 : 0.4835504008942419
Loss in iteration 219 : 0.4834388961889391
Loss in iteration 220 : 0.48332824586887563
Loss in iteration 221 : 0.483218439533477
Loss in iteration 222 : 0.483109466959228
Loss in iteration 223 : 0.4830013180958871
Loss in iteration 224 : 0.48289398306279985
Loss in iteration 225 : 0.4827874521453242
Loss in iteration 226 : 0.48268171579131064
Loss in iteration 227 : 0.4825767646077094
Loss in iteration 228 : 0.482472589357232
Loss in iteration 229 : 0.4823691809551212
Loss in iteration 230 : 0.4822665304659725
Loss in iteration 231 : 0.48216462910066504
Loss in iteration 232 : 0.4820634682133452
Loss in iteration 233 : 0.48196303929849127
Loss in iteration 234 : 0.4818633339880589
Loss in iteration 235 : 0.4817643440486755
Loss in iteration 236 : 0.48166606137892404
Loss in iteration 237 : 0.4815684780066826
Loss in iteration 238 : 0.4814715860865215
Loss in iteration 239 : 0.48137537789717405
Loss in iteration 240 : 0.4812798458390655
Loss in iteration 241 : 0.4811849824318915
Loss in iteration 242 : 0.481090780312265
Loss in iteration 243 : 0.4809972322314144
Loss in iteration 244 : 0.4809043310529288
Loss in iteration 245 : 0.48081206975057283
Loss in iteration 246 : 0.4807204414061323
Loss in iteration 247 : 0.4806294392073232
Loss in iteration 248 : 0.48053905644574707
Loss in iteration 249 : 0.48044928651488383
Loss in iteration 250 : 0.48036012290814983
Loss in iteration 251 : 0.4802715592169797
Loss in iteration 252 : 0.48018358912896203
Loss in iteration 253 : 0.4800962064260187
Loss in iteration 254 : 0.4800094049826201
Loss in iteration 255 : 0.4799231787640479
Loss in iteration 256 : 0.47983752182468187
Loss in iteration 257 : 0.4797524283063453
Loss in iteration 258 : 0.47966789243667335
Loss in iteration 259 : 0.47958390852751687
Loss in iteration 260 : 0.47950047097339454
Loss in iteration 261 : 0.4794175742499594
Loss in iteration 262 : 0.47933521291251896
Loss in iteration 263 : 0.4792533815945746
Loss in iteration 264 : 0.4791720750063961
Loss in iteration 265 : 0.479091287933629
Loss in iteration 266 : 0.4790110152359307
Loss in iteration 267 : 0.4789312518456356
Loss in iteration 268 : 0.4788519927664479
Loss in iteration 269 : 0.4787732330721687
Loss in iteration 270 : 0.4786949679054378
Loss in iteration 271 : 0.4786171924765134
Loss in iteration 272 : 0.4785399020620821
Loss in iteration 273 : 0.4784630920040644
Loss in iteration 274 : 0.47838675770849154
Loss in iteration 275 : 0.47831089464436544
Loss in iteration 276 : 0.478235498342562
Loss in iteration 277 : 0.478160564394754
Loss in iteration 278 : 0.47808608845235845
Loss in iteration 279 : 0.478012066225496
Loss in iteration 280 : 0.47793849348198375
Loss in iteration 281 : 0.4778653660463434
Loss in iteration 282 : 0.4777926797988257
Loss in iteration 283 : 0.47772043067447134
Loss in iteration 284 : 0.4776486146621586
Loss in iteration 285 : 0.4775772278037098
Loss in iteration 286 : 0.47750626619298603
Loss in iteration 287 : 0.4774357259750114
Loss in iteration 288 : 0.4773656033451183
Loss in iteration 289 : 0.47729589454809995
Loss in iteration 290 : 0.47722659587739086
Loss in iteration 291 : 0.4771577036742512
Loss in iteration 292 : 0.47708921432698187
Loss in iteration 293 : 0.47702112427014154
Loss in iteration 294 : 0.47695342998378865
Loss in iteration 295 : 0.4768861279927338
Loss in iteration 296 : 0.47681921486580336
Loss in iteration 297 : 0.4767526872151276
Loss in iteration 298 : 0.47668654169543434
Loss in iteration 299 : 0.4766207750033616
Loss in iteration 300 : 0.4765553838767712
Loss in iteration 301 : 0.4764903650940988
Loss in iteration 302 : 0.4764257154736906
Loss in iteration 303 : 0.47636143187317015
Loss in iteration 304 : 0.47629751118881597
Loss in iteration 305 : 0.4762339503549373
Loss in iteration 306 : 0.47617074634327766
Loss in iteration 307 : 0.47610789616242377
Loss in iteration 308 : 0.4760453968572228
Loss in iteration 309 : 0.4759832455082155
Loss in iteration 310 : 0.4759214392310748
Loss in iteration 311 : 0.4758599751760579
Loss in iteration 312 : 0.47579885052747023
Loss in iteration 313 : 0.4757380625031347
Loss in iteration 314 : 0.4756776083538794
Loss in iteration 315 : 0.47561748536302123
Loss in iteration 316 : 0.47555769084586885
Loss in iteration 317 : 0.4754982221492385
Loss in iteration 318 : 0.4754390766509619
Loss in iteration 319 : 0.4753802517594281
Loss in iteration 320 : 0.4753217449131068
Loss in iteration 321 : 0.47526355358009775
Loss in iteration 322 : 0.47520567525768626
Loss in iteration 323 : 0.47514810747190256
Loss in iteration 324 : 0.4750908477770882
Loss in iteration 325 : 0.47503389375547594
Loss in iteration 326 : 0.4749772430167745
Loss in iteration 327 : 0.4749208931977561
Loss in iteration 328 : 0.474864841961858
Loss in iteration 329 : 0.4748090869987916
Loss in iteration 330 : 0.4747536260241478
Loss in iteration 331 : 0.4746984567790267
Loss in iteration 332 : 0.47464357702965654
Loss in iteration 333 : 0.4745889845670289
Loss in iteration 334 : 0.4745346772065378
Loss in iteration 335 : 0.4744806527876313
Loss in iteration 336 : 0.4744269091734513
Loss in iteration 337 : 0.47437344425050737
Loss in iteration 338 : 0.4743202559283271
Loss in iteration 339 : 0.4742673421391303
Loss in iteration 340 : 0.4742147008375113
Loss in iteration 341 : 0.4741623300001101
Loss in iteration 342 : 0.47411022762530286
Loss in iteration 343 : 0.474058391732894
Loss in iteration 344 : 0.47400682036381736
Loss in iteration 345 : 0.47395551157982835
Loss in iteration 346 : 0.4739044634632234
Loss in iteration 347 : 0.4738536741165434
Loss in iteration 348 : 0.4738031416622936
Loss in iteration 349 : 0.47375286424266666
Loss in iteration 350 : 0.4737028400192669
Loss in iteration 351 : 0.47365306717284783
Loss in iteration 352 : 0.4736035439030356
Loss in iteration 353 : 0.47355426842808385
Loss in iteration 354 : 0.4735052389846045
Loss in iteration 355 : 0.4734564538273262
Loss in iteration 356 : 0.47340791122884124
Loss in iteration 357 : 0.4733596094793683
Loss in iteration 358 : 0.4733115468865052
Loss in iteration 359 : 0.4732637217750007
Loss in iteration 360 : 0.4732161324865177
Loss in iteration 361 : 0.47316877737941404
Loss in iteration 362 : 0.473121654828507
Loss in iteration 363 : 0.47307476322486175
Loss in iteration 364 : 0.47302810097557163
Loss in iteration 365 : 0.4729816665035464
Loss in iteration 366 : 0.47293545824729594
Loss in iteration 367 : 0.47288947466073766
Loss in iteration 368 : 0.47284371421297716
Loss in iteration 369 : 0.4727981753881217
Loss in iteration 370 : 0.47275285668507466
Loss in iteration 371 : 0.47270775661734593
Loss in iteration 372 : 0.472662873712863
Loss in iteration 373 : 0.47261820651377695
Loss in iteration 374 : 0.4725737535762881
Loss in iteration 375 : 0.4725295134704543
Loss in iteration 376 : 0.47248548478001884
Loss in iteration 377 : 0.4724416661022306
Loss in iteration 378 : 0.47239805604767576
Loss in iteration 379 : 0.4723546532401015
Loss in iteration 380 : 0.4723114563162499
Loss in iteration 381 : 0.47226846392569816
Loss in iteration 382 : 0.47222567473068794
Loss in iteration 383 : 0.4721830874059715
Loss in iteration 384 : 0.4721407006386446
Loss in iteration 385 : 0.4720985131280082
Loss in iteration 386 : 0.4720565235853994
Loss in iteration 387 : 0.4720147307340462
Loss in iteration 388 : 0.47197313330892404
Loss in iteration 389 : 0.4719317300566022
Loss in iteration 390 : 0.47189051973510354
Loss in iteration 391 : 0.4718495011137675
Loss in iteration 392 : 0.4718086729731027
Loss in iteration 393 : 0.4717680341046497
Loss in iteration 394 : 0.47172758331085873
Loss in iteration 395 : 0.4716873194049399
Loss in iteration 396 : 0.47164724121074053
Loss in iteration 397 : 0.47160734756261186
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.7835, training accuracy 0.7835, time elapsed: 7303 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 12.569829085734174
Loss in iteration 3 : 13.880316658153774
Loss in iteration 4 : 8.01656966282436
Loss in iteration 5 : 9.181551262563625
Loss in iteration 6 : 6.868710291161589
Loss in iteration 7 : 4.3840141478995776
Loss in iteration 8 : 7.925858092046513
Loss in iteration 9 : 3.089631588936259
Loss in iteration 10 : 5.819007554891209
Loss in iteration 11 : 5.6138049390914855
Loss in iteration 12 : 2.4261894468436984
Loss in iteration 13 : 4.8616263305679945
Loss in iteration 14 : 4.518143093545742
Loss in iteration 15 : 2.530655831765028
Loss in iteration 16 : 4.344273712842106
Loss in iteration 17 : 3.8584510908143375
Loss in iteration 18 : 2.616707257922222
Loss in iteration 19 : 3.851840714637694
Loss in iteration 20 : 3.3488412445782996
Loss in iteration 21 : 2.429400420533823
Loss in iteration 22 : 3.3385344421293794
Loss in iteration 23 : 2.816023945490412
Loss in iteration 24 : 2.4040462982645256
Loss in iteration 25 : 3.0186591741522104
Loss in iteration 26 : 2.4779136073416486
Loss in iteration 27 : 2.1415486861230493
Loss in iteration 28 : 2.6069465804773437
Loss in iteration 29 : 1.9681903844424584
Loss in iteration 30 : 2.1172058877882454
Loss in iteration 31 : 2.2136074627713747
Loss in iteration 32 : 1.7436470477457082
Loss in iteration 33 : 2.0219463487106997
Loss in iteration 34 : 1.7109765525889933
Loss in iteration 35 : 1.6676263810083547
Loss in iteration 36 : 1.7224346619592712
Loss in iteration 37 : 1.4093322169279616
Loss in iteration 38 : 1.6439530718874293
Loss in iteration 39 : 1.2994242826258364
Loss in iteration 40 : 1.4896755472158256
Loss in iteration 41 : 1.2055869881545636
Loss in iteration 42 : 1.3928948830010752
Loss in iteration 43 : 1.1267215890734519
Loss in iteration 44 : 1.3301920292898457
Loss in iteration 45 : 1.070667684952798
Loss in iteration 46 : 1.2027638800284743
Loss in iteration 47 : 1.1661931785755055
Loss in iteration 48 : 1.0510193765837408
Loss in iteration 49 : 1.1349543349958373
Loss in iteration 50 : 1.056114689630855
Loss in iteration 51 : 1.0784635428287428
Loss in iteration 52 : 1.0027546075804286
Loss in iteration 53 : 1.0766585179779786
Loss in iteration 54 : 0.9775281764454544
Loss in iteration 55 : 1.2013451625655194
Loss in iteration 56 : 1.073893054787787
Loss in iteration 57 : 0.9049619013899659
Loss in iteration 58 : 1.1069739350641497
Loss in iteration 59 : 1.0093260713228756
Loss in iteration 60 : 0.8321899692331449
Loss in iteration 61 : 0.688698730799028
Loss in iteration 62 : 0.7178221783769257
Loss in iteration 63 : 0.8599585151092745
Loss in iteration 64 : 0.9624942176140819
Loss in iteration 65 : 0.9658661590926615
Loss in iteration 66 : 0.772020865921136
Loss in iteration 67 : 0.7922655685713961
Loss in iteration 68 : 0.6127731112218542
Loss in iteration 69 : 0.7737537636287668
Loss in iteration 70 : 0.5727806140121247
Loss in iteration 71 : 0.829448065407425
Loss in iteration 72 : 0.9900884412848165
Loss in iteration 73 : 1.7564896249791664
Loss in iteration 74 : 1.1535386638635763
Loss in iteration 75 : 0.7409687962864488
Loss in iteration 76 : 0.7528990111518833
Loss in iteration 77 : 0.7420647328913063
Loss in iteration 78 : 1.51159778335686
Loss in iteration 79 : 1.142518826644304
Loss in iteration 80 : 0.9147490706417388
Loss in iteration 81 : 0.5847423571280872
Loss in iteration 82 : 0.7775405556707414
Loss in iteration 83 : 0.8099058347404716
Loss in iteration 84 : 0.9406797010383554
Loss in iteration 85 : 0.7422198198198859
Loss in iteration 86 : 0.6574848663257894
Loss in iteration 87 : 0.6727888643486526
Loss in iteration 88 : 0.7056092819340104
Loss in iteration 89 : 0.9207348067127736
Loss in iteration 90 : 1.0119561403815127
Loss in iteration 91 : 0.9089110331661819
Loss in iteration 92 : 0.7852233493854162
Loss in iteration 93 : 0.5884784322281261
Loss in iteration 94 : 0.6476251310147614
Loss in iteration 95 : 0.5323446707865979
Loss in iteration 96 : 0.6474023276674918
Loss in iteration 97 : 0.5394867755678429
Loss in iteration 98 : 0.6437833480382317
Loss in iteration 99 : 0.6288882715047361
Loss in iteration 100 : 0.8093950511842449
Loss in iteration 101 : 1.2428605732047444
Loss in iteration 102 : 1.1025556771544878
Loss in iteration 103 : 1.1549567605160815
Loss in iteration 104 : 0.5921826544209273
Loss in iteration 105 : 0.7059504634354008
Loss in iteration 106 : 0.9463952329885608
Loss in iteration 107 : 0.7660340628905278
Loss in iteration 108 : 0.7610583397753089
Loss in iteration 109 : 0.5542308872337567
Loss in iteration 110 : 0.674445261711573
Loss in iteration 111 : 0.7015001176624374
Loss in iteration 112 : 0.6470619260814698
Loss in iteration 113 : 0.7231736413913167
Loss in iteration 114 : 0.5975057046382674
Loss in iteration 115 : 0.6329194034791336
Loss in iteration 116 : 0.585131431575199
Loss in iteration 117 : 0.5900328466722475
Loss in iteration 118 : 0.5724837278095878
Loss in iteration 119 : 0.6012667846557178
Loss in iteration 120 : 0.5632178390764466
Loss in iteration 121 : 0.6828822829636513
Loss in iteration 122 : 0.701979378526651
Loss in iteration 123 : 1.1003313539790884
Loss in iteration 124 : 1.0016651121286622
Loss in iteration 125 : 0.8993082709167912
Loss in iteration 126 : 0.692675142858317
Loss in iteration 127 : 0.561911233887068
Loss in iteration 128 : 0.5114388490008671
Loss in iteration 129 : 0.5165340427529416
Loss in iteration 130 : 0.5374810065026329
Loss in iteration 131 : 0.620177187828781
Loss in iteration 132 : 0.7656726000775703
Loss in iteration 133 : 0.871144685605936
Loss in iteration 134 : 0.8939114139799108
Loss in iteration 135 : 0.6314428472067912
Loss in iteration 136 : 0.5323374638425682
Loss in iteration 137 : 0.5117328159189289
Loss in iteration 138 : 0.5715011373507056
Loss in iteration 139 : 0.6637445664445608
Loss in iteration 140 : 0.6693549805995304
Loss in iteration 141 : 0.6518336090051716
Loss in iteration 142 : 0.5752942755914318
Loss in iteration 143 : 0.551031111886389
Loss in iteration 144 : 0.5232499632006823
Loss in iteration 145 : 0.5340755706168909
Loss in iteration 146 : 0.5560131828637728
Loss in iteration 147 : 0.6697185370420022
Loss in iteration 148 : 0.8509661760247906
Loss in iteration 149 : 1.1108728635752991
Loss in iteration 150 : 0.7151517199787788
Loss in iteration 151 : 0.5731052208273081
Loss in iteration 152 : 0.4992995434389748
Loss in iteration 153 : 0.5947832386503765
Loss in iteration 154 : 0.6490852481202379
Loss in iteration 155 : 0.6605821074681265
Loss in iteration 156 : 0.6452147135048472
Loss in iteration 157 : 0.5585487116175748
Loss in iteration 158 : 0.5768607640754309
Loss in iteration 159 : 0.5027442050562286
Loss in iteration 160 : 0.5563079264205887
Loss in iteration 161 : 0.49317872012639175
Loss in iteration 162 : 0.5740317193068432
Loss in iteration 163 : 0.5590852433364212
Loss in iteration 164 : 0.7259818620557358
Loss in iteration 165 : 0.7360649108265183
Loss in iteration 166 : 0.8081568676938341
Loss in iteration 167 : 0.6266229860526801
Loss in iteration 168 : 0.5446812900850833
Loss in iteration 169 : 0.4890762272365138
Loss in iteration 170 : 0.4930115817565545
Loss in iteration 171 : 0.5230001738714409
Loss in iteration 172 : 0.5985232271906036
Loss in iteration 173 : 0.7664417783195354
Loss in iteration 174 : 0.8267558347304648
Loss in iteration 175 : 0.8417938348751464
Loss in iteration 176 : 0.597510274342154
Loss in iteration 177 : 0.5270093275133398
Loss in iteration 178 : 0.4898361150223028
Loss in iteration 179 : 0.550980319109751
Loss in iteration 180 : 0.5719583907977233
Loss in iteration 181 : 0.5658245802660897
Loss in iteration 182 : 0.5464318415701872
Loss in iteration 183 : 0.4944816675426123
Loss in iteration 184 : 0.5170675211361391
Loss in iteration 185 : 0.47499343075109174
Loss in iteration 186 : 0.5045753573150864
Loss in iteration 187 : 0.47202950922539816
Loss in iteration 188 : 0.49874508812951274
Loss in iteration 189 : 0.47753473709990385
Loss in iteration 190 : 0.521441958729242
Loss in iteration 191 : 0.5953011623370246
Loss in iteration 192 : 0.8813778684691251
Loss in iteration 193 : 0.9928676504613648
Loss in iteration 194 : 0.8967495872193137
Loss in iteration 195 : 0.5696293363169093
Loss in iteration 196 : 0.4914231674898941
Loss in iteration 197 : 0.6191431630233505
Loss in iteration 198 : 0.7384794399127381
Loss in iteration 199 : 0.6676817062564933
Loss in iteration 200 : 0.5139397642786844
Loss in iteration 201 : 0.4883265752194193
Loss in iteration 202 : 0.5799924135808548
Loss in iteration 203 : 0.656218646412425
Loss in iteration 204 : 0.6057487792387815
Loss in iteration 205 : 0.5073846790057944
Loss in iteration 206 : 0.4839345892200384
Loss in iteration 207 : 0.534196640562741
Loss in iteration 208 : 0.577623025818009
Loss in iteration 209 : 0.5589588883334511
Loss in iteration 210 : 0.5015145675754183
Loss in iteration 211 : 0.47402684843103865
Loss in iteration 212 : 0.48543793141690844
Loss in iteration 213 : 0.5265390861296473
Loss in iteration 214 : 0.5988128223607491
Loss in iteration 215 : 0.6735955473750564
Loss in iteration 216 : 0.7622965038508512
Loss in iteration 217 : 0.6963719528780846
Loss in iteration 218 : 0.6193330607610728
Loss in iteration 219 : 0.5226581410324966
Loss in iteration 220 : 0.4785171195719666
Loss in iteration 221 : 0.4783964352140432
Loss in iteration 222 : 0.5100848758289719
Loss in iteration 223 : 0.5555132505757724
Loss in iteration 224 : 0.5832555460886526
Loss in iteration 225 : 0.589833260269879
Loss in iteration 226 : 0.5594140636814771
Loss in iteration 227 : 0.5304151673859412
Loss in iteration 228 : 0.4988254870965509
Loss in iteration 229 : 0.48150803966642436
Loss in iteration 230 : 0.47121270652335534
Loss in iteration 231 : 0.46789066183226635
Loss in iteration 232 : 0.46675164798503327
Loss in iteration 233 : 0.47052806684191617
Loss in iteration 234 : 0.47820677212492446
Loss in iteration 235 : 0.49617453681142115
Loss in iteration 236 : 0.5375379835739559
Loss in iteration 237 : 0.6201770552605677
Loss in iteration 238 : 0.783942377575567
Loss in iteration 239 : 0.7863514488513642
Loss in iteration 240 : 0.707033615668335
Loss in iteration 241 : 0.5330034397811251
Loss in iteration 242 : 0.476349239364362
Loss in iteration 243 : 0.5198392712549678
Loss in iteration 244 : 0.5953022622249898
Loss in iteration 245 : 0.6097981152196547
Loss in iteration 246 : 0.5366692018950325
Loss in iteration 247 : 0.4813586699238698
Loss in iteration 248 : 0.47570657448718917
Loss in iteration 249 : 0.5137539985380375
Loss in iteration 250 : 0.555580705545703
Loss in iteration 251 : 0.5662342925892582
Loss in iteration 252 : 0.5440196170577501
Loss in iteration 253 : 0.5012161686933243
Loss in iteration 254 : 0.4739286502558868
Loss in iteration 255 : 0.46933350632009324
Loss in iteration 256 : 0.4859431941498584
Loss in iteration 257 : 0.5097458533403789
Loss in iteration 258 : 0.5343344541601607
Loss in iteration 259 : 0.5621426750042585
Loss in iteration 260 : 0.579752485025599
Loss in iteration 261 : 0.6094083463624381
Loss in iteration 262 : 0.6031220201580167
Loss in iteration 263 : 0.5990519863475425
Loss in iteration 264 : 0.5482789746041781
Loss in iteration 265 : 0.5129155580611973
Loss in iteration 266 : 0.4805596383740647
Loss in iteration 267 : 0.4695403510881538
Loss in iteration 268 : 0.46740631614524636
Loss in iteration 269 : 0.4812419293012855
Loss in iteration 270 : 0.49885836242506876
Loss in iteration 271 : 0.5234202109667537
Loss in iteration 272 : 0.5458249906095471
Loss in iteration 273 : 0.5581845292112353
Loss in iteration 274 : 0.5716024181825062
Loss in iteration 275 : 0.5523669955381438
Loss in iteration 276 : 0.5303425761613358
Loss in iteration 277 : 0.493654481256556
Loss in iteration 278 : 0.4763138002140285
Loss in iteration 279 : 0.4654208305856599
Loss in iteration 280 : 0.47296320855195734
Loss in iteration 281 : 0.48185248231727323
Loss in iteration 282 : 0.5049967318911696
Loss in iteration 283 : 0.535314307897369
Loss in iteration 284 : 0.5758859142604612
Loss in iteration 285 : 0.62385786113422
Loss in iteration 286 : 0.6168029172599345
Loss in iteration 287 : 0.598115496554885
Loss in iteration 288 : 0.5276677533488292
Loss in iteration 289 : 0.4868166552712007
Loss in iteration 290 : 0.46894037128825355
Loss in iteration 291 : 0.49014132132020516
Loss in iteration 292 : 0.5127876646393595
Loss in iteration 293 : 0.5331248876590332
Loss in iteration 294 : 0.5258616665278462
Loss in iteration 295 : 0.5082042804446407
Loss in iteration 296 : 0.49272467351221394
Loss in iteration 297 : 0.47987909911331433
Loss in iteration 298 : 0.4733433913224739
Loss in iteration 299 : 0.4660358223166603
Loss in iteration 300 : 0.46640790040359503
Loss in iteration 301 : 0.4627345299458494
Loss in iteration 302 : 0.4658151625360206
Loss in iteration 303 : 0.46237339486416734
Loss in iteration 304 : 0.46595597303957925
Loss in iteration 305 : 0.46485561071409703
Loss in iteration 306 : 0.47308987186534784
Loss in iteration 307 : 0.48427534393018923
Loss in iteration 308 : 0.521870931574181
Loss in iteration 309 : 0.6138709363135613
Loss in iteration 310 : 0.7289399843674603
Loss in iteration 311 : 0.8211591269837768
Loss in iteration 312 : 0.6127414027577518
Loss in iteration 313 : 0.49026555052965987
Loss in iteration 314 : 0.4839756256399467
Loss in iteration 315 : 0.559760644218606
Loss in iteration 316 : 0.5956176552637212
Loss in iteration 317 : 0.535373323318834
Loss in iteration 318 : 0.47601189066277655
Loss in iteration 319 : 0.48562795586876495
Loss in iteration 320 : 0.5356751108831931
Loss in iteration 321 : 0.5509878875347708
Loss in iteration 322 : 0.5141731054961942
Loss in iteration 323 : 0.47527757850345254
Loss in iteration 324 : 0.47083422927947005
Loss in iteration 325 : 0.492044967648283
Loss in iteration 326 : 0.5128777900551769
Loss in iteration 327 : 0.5076714047697147
Loss in iteration 328 : 0.48852622556450614
Loss in iteration 329 : 0.4688665049325146
Loss in iteration 330 : 0.46245005212506063
Loss in iteration 331 : 0.4689377950238888
Loss in iteration 332 : 0.4847889567949463
Loss in iteration 333 : 0.5149922692249898
Loss in iteration 334 : 0.5571899936169901
Loss in iteration 335 : 0.6299172930186189
Loss in iteration 336 : 0.6455485682276465
Loss in iteration 337 : 0.6339001054104478
Loss in iteration 338 : 0.5400485125204575
Loss in iteration 339 : 0.4801451272512463
Loss in iteration 340 : 0.46832489245779707
Loss in iteration 341 : 0.4981811003787851
Loss in iteration 342 : 0.5327240974484732
Loss in iteration 343 : 0.5304536562145572
Loss in iteration 344 : 0.5048058587031388
Loss in iteration 345 : 0.47606659404902507
Loss in iteration 346 : 0.46451558800922577
Loss in iteration 347 : 0.47256431660651854
Loss in iteration 348 : 0.49149103503472236
Loss in iteration 349 : 0.5090830347834271
Loss in iteration 350 : 0.5107618962700643
Loss in iteration 351 : 0.5005141797187647
Loss in iteration 352 : 0.48154289152589186
Loss in iteration 353 : 0.467377008794082
Loss in iteration 354 : 0.46207809259275223
Loss in iteration 355 : 0.46550369471192155
Loss in iteration 356 : 0.475778881476157
Loss in iteration 357 : 0.4908234787606443
Loss in iteration 358 : 0.5174234079404555
Loss in iteration 359 : 0.5486219407639749
Loss in iteration 360 : 0.5981753438108287
Loss in iteration 361 : 0.6056996581842784
Loss in iteration 362 : 0.5984862759403726
Loss in iteration 363 : 0.533756580938282
Loss in iteration 364 : 0.4846859242577869
Loss in iteration 365 : 0.46448758753479236
Loss in iteration 366 : 0.47692842447603506
Loss in iteration 367 : 0.5046867660662676
Loss in iteration 368 : 0.5194305852511331
Loss in iteration 369 : 0.5166668121620726
Loss in iteration 370 : 0.4948378253875226
Loss in iteration 371 : 0.47434509773561545
Loss in iteration 372 : 0.46333747523425983
Loss in iteration 373 : 0.46326820921140843
Loss in iteration 374 : 0.4710841236867392
Loss in iteration 375 : 0.4822449101627299
Loss in iteration 376 : 0.4931805962785911
Loss in iteration 377 : 0.4971888214165644
Loss in iteration 378 : 0.49804702019954655
Loss in iteration 379 : 0.49063436699562424
Loss in iteration 380 : 0.48408013198152033
Loss in iteration 381 : 0.4768668587478606
Loss in iteration 382 : 0.4728052399396745
Loss in iteration 383 : 0.4705366087197883
Loss in iteration 384 : 0.47085537456023896
Loss in iteration 385 : 0.47356339300248446
Loss in iteration 386 : 0.48150006077562096
Loss in iteration 387 : 0.49563872432171935
Loss in iteration 388 : 0.5252954534076119
Loss in iteration 389 : 0.5575521107146655
Loss in iteration 390 : 0.5986891520122022
Loss in iteration 391 : 0.5791289880595505
Loss in iteration 392 : 0.5409042706193027
Loss in iteration 393 : 0.4878272487432206
Loss in iteration 394 : 0.4638003834773408
Loss in iteration 395 : 0.471742217642049
Loss in iteration 396 : 0.4977327133353814
Loss in iteration 397 : 0.5224616442343126
Loss in iteration 398 : 0.521114077159293
Loss in iteration 399 : 0.5047498738604924
Loss in iteration 400 : 0.4805974982550871
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.785125, training accuracy 0.785125, time elapsed: 8071 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.8325272336287851
Loss in iteration 3 : 1.36117373502792
Loss in iteration 4 : 1.0723632320484249
Loss in iteration 5 : 0.529267242192507
Loss in iteration 6 : 1.026593247122837
Loss in iteration 7 : 0.7009818144187342
Loss in iteration 8 : 0.6293624399702251
Loss in iteration 9 : 0.7723914343794108
Loss in iteration 10 : 0.5726823361597213
Loss in iteration 11 : 0.7326116549400431
Loss in iteration 12 : 0.5841066128581872
Loss in iteration 13 : 0.6919687799384056
Loss in iteration 14 : 0.6249118856655311
Loss in iteration 15 : 0.6381444646522227
Loss in iteration 16 : 0.6372530157686264
Loss in iteration 17 : 0.5934545760780224
Loss in iteration 18 : 0.6430333098962074
Loss in iteration 19 : 0.5660407598771878
Loss in iteration 20 : 0.6307003583065586
Loss in iteration 21 : 0.5489766732760568
Loss in iteration 22 : 0.6036691448665215
Loss in iteration 23 : 0.5347998054326749
Loss in iteration 24 : 0.5777062515645348
Loss in iteration 25 : 0.5255532861301133
Loss in iteration 26 : 0.5541995615244537
Loss in iteration 27 : 0.5116483568964039
Loss in iteration 28 : 0.5352440075856221
Loss in iteration 29 : 0.5000250110399341
Loss in iteration 30 : 0.5230324678162519
Loss in iteration 31 : 0.4901104657675735
Loss in iteration 32 : 0.5134492917550482
Loss in iteration 33 : 0.48325226982795855
Loss in iteration 34 : 0.5042404215505202
Loss in iteration 35 : 0.48344703283090656
Loss in iteration 36 : 0.49235644464921025
Loss in iteration 37 : 0.4894924985636443
Loss in iteration 38 : 0.47961426420293674
Loss in iteration 39 : 0.49380419913872164
Loss in iteration 40 : 0.4760499722493256
Loss in iteration 41 : 0.48699567723508436
Loss in iteration 42 : 0.4808497382167955
Loss in iteration 43 : 0.4744296867397487
Loss in iteration 44 : 0.4829452108246691
Loss in iteration 45 : 0.47126584929575865
Loss in iteration 46 : 0.4746215752569688
Loss in iteration 47 : 0.4735485478552639
Loss in iteration 48 : 0.466670520789442
Loss in iteration 49 : 0.4710995770763785
Loss in iteration 50 : 0.4660836836054467
Loss in iteration 51 : 0.4646892190177751
Loss in iteration 52 : 0.46655410618684984
Loss in iteration 53 : 0.46219691928185885
Loss in iteration 54 : 0.4640017697091741
Loss in iteration 55 : 0.46355510026367996
Loss in iteration 56 : 0.46108874856286614
Loss in iteration 57 : 0.463559365991557
Loss in iteration 58 : 0.46159352966629685
Loss in iteration 59 : 0.4614972145894299
Loss in iteration 60 : 0.4627313672041624
Loss in iteration 61 : 0.4606130403428386
Loss in iteration 62 : 0.46187052042310023
Loss in iteration 63 : 0.46156133136668415
Loss in iteration 64 : 0.4604651623074564
Loss in iteration 65 : 0.46152606967427573
Loss in iteration 66 : 0.46053173237972655
Loss in iteration 67 : 0.4603750161289724
Loss in iteration 68 : 0.4608700347201481
Loss in iteration 69 : 0.459982109748081
Loss in iteration 70 : 0.46015830042810435
Loss in iteration 71 : 0.46036798670722123
Loss in iteration 72 : 0.4597680018254182
Loss in iteration 73 : 0.4600055616032142
Loss in iteration 74 : 0.4600989762012835
Loss in iteration 75 : 0.45966467433929326
Loss in iteration 76 : 0.4599366779117206
Loss in iteration 77 : 0.45995523542015737
Loss in iteration 78 : 0.45961331509240855
Loss in iteration 79 : 0.4598603117883346
Loss in iteration 80 : 0.4598055590815343
Loss in iteration 81 : 0.4595602251696362
Loss in iteration 82 : 0.4597343670907642
Loss in iteration 83 : 0.4596316651997948
Loss in iteration 84 : 0.4594743954789209
Loss in iteration 85 : 0.4595875798753068
Loss in iteration 86 : 0.45948696363987634
Loss in iteration 87 : 0.4593816666910159
Loss in iteration 88 : 0.45945514045677727
Loss in iteration 89 : 0.4593817294118211
Loss in iteration 90 : 0.45931025027575206
Loss in iteration 91 : 0.4593645952607112
Loss in iteration 92 : 0.45930739465281656
Loss in iteration 93 : 0.45926635524824205
Loss in iteration 94 : 0.45931326846737636
Loss in iteration 95 : 0.45925889153141997
Loss in iteration 96 : 0.4592410559731029
Loss in iteration 97 : 0.45927503990536195
Loss in iteration 98 : 0.45922649892253703
Loss in iteration 99 : 0.4592244317838962
Loss in iteration 100 : 0.4592395508533075
Loss in iteration 101 : 0.45920138904143276
Loss in iteration 102 : 0.45920606834326966
Loss in iteration 103 : 0.4592077676076261
Loss in iteration 104 : 0.4591791448886287
Loss in iteration 105 : 0.459182890071358
Loss in iteration 106 : 0.45917998041961905
Loss in iteration 107 : 0.45915908773041575
Loss in iteration 108 : 0.45916138975012405
Loss in iteration 109 : 0.459157160306562
Loss in iteration 110 : 0.4591415440857915
Loss in iteration 111 : 0.45914455106834945
Loss in iteration 112 : 0.45913922739849455
Loss in iteration 113 : 0.4591273145051287
Loss in iteration 114 : 0.4591305624913865
Loss in iteration 115 : 0.45912454617882054
Loss in iteration 116 : 0.4591160644408852
Loss in iteration 117 : 0.45911782287744324
Loss in iteration 118 : 0.4591114572313494
Loss in iteration 119 : 0.4591054724393349
Loss in iteration 120 : 0.459105473189543
Loss in iteration 121 : 0.45909945353228
Loss in iteration 122 : 0.4590946623019594
Loss in iteration 123 : 0.4590936801181031
Loss in iteration 124 : 0.45908862802591377
Loss in iteration 125 : 0.45908450821870833
Loss in iteration 126 : 0.45908318415933
Loss in iteration 127 : 0.4590789640218956
Loss in iteration 128 : 0.45907558977933105
Loss in iteration 129 : 0.459074313071609
Loss in iteration 130 : 0.45907062827461514
Loss in iteration 131 : 0.4590679620396786
Loss in iteration 132 : 0.45906673415121024
Loss in iteration 133 : 0.4590635279306588
Loss in iteration 134 : 0.4590613477030245
Loss in iteration 135 : 0.4590599765988016
Loss in iteration 136 : 0.45905724160470607
Loss in iteration 137 : 0.4590553654581116
Loss in iteration 138 : 0.45905387924021
Testing accuracy  of updater 9 on alg 0 with rate 1.4000000000000001 = 0.789875, training accuracy 0.789875, time elapsed: 2741 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6820497160227583
Loss in iteration 3 : 0.6390193176920766
Loss in iteration 4 : 0.584166067290115
Loss in iteration 5 : 0.556520849094546
Loss in iteration 6 : 0.5292205551068919
Loss in iteration 7 : 0.5107508198302163
Loss in iteration 8 : 0.5013147478440109
Loss in iteration 9 : 0.4903416130507364
Loss in iteration 10 : 0.48880020417010484
Loss in iteration 11 : 0.4852088546201702
Loss in iteration 12 : 0.48567624890882505
Loss in iteration 13 : 0.48501492176501027
Loss in iteration 14 : 0.48403608901825046
Loss in iteration 15 : 0.484986304930635
Loss in iteration 16 : 0.4833924705908906
Loss in iteration 17 : 0.48505435929690666
Loss in iteration 18 : 0.4832955135204286
Loss in iteration 19 : 0.48331471634485307
Loss in iteration 20 : 0.4817699061519575
Loss in iteration 21 : 0.4801538093041293
Loss in iteration 22 : 0.47936432191606004
Loss in iteration 23 : 0.4771045748817724
Loss in iteration 24 : 0.47629908469273746
Loss in iteration 25 : 0.47402517962572954
Loss in iteration 26 : 0.4726189913728755
Loss in iteration 27 : 0.4710216193564448
Loss in iteration 28 : 0.46942771151848833
Loss in iteration 29 : 0.468605228950074
Loss in iteration 30 : 0.46714135432333725
Loss in iteration 31 : 0.46656399691779504
Loss in iteration 32 : 0.4654240570603275
Loss in iteration 33 : 0.4649190758109662
Loss in iteration 34 : 0.4643504476535321
Loss in iteration 35 : 0.4639540198823247
Loss in iteration 36 : 0.46381318168838753
Loss in iteration 37 : 0.46345957835093415
Loss in iteration 38 : 0.4634982250036868
Loss in iteration 39 : 0.46324056090600946
Loss in iteration 40 : 0.4633803174733857
Loss in iteration 41 : 0.46326284943047213
Loss in iteration 42 : 0.4633568038504027
Loss in iteration 43 : 0.4632581194584725
Loss in iteration 44 : 0.46322125553574595
Loss in iteration 45 : 0.46313871260565
Loss in iteration 46 : 0.46302899706328915
Loss in iteration 47 : 0.4629571433926253
Loss in iteration 48 : 0.4627704995569817
Loss in iteration 49 : 0.4626578612436008
Loss in iteration 50 : 0.4624258632135611
Loss in iteration 51 : 0.4622998990939033
Loss in iteration 52 : 0.4620803365903882
Loss in iteration 53 : 0.4619455301685989
Loss in iteration 54 : 0.46173977221345036
Loss in iteration 55 : 0.4615900056129239
Loss in iteration 56 : 0.46142245738146126
Loss in iteration 57 : 0.4612903173537079
Loss in iteration 58 : 0.4611738726327602
Loss in iteration 59 : 0.46106090992205123
Loss in iteration 60 : 0.46097852643355497
Loss in iteration 61 : 0.46088334384776475
Loss in iteration 62 : 0.4608305994715303
Loss in iteration 63 : 0.46076092365506977
Loss in iteration 64 : 0.46072714021369937
Loss in iteration 65 : 0.4606741911440732
Loss in iteration 66 : 0.46064432560876856
Loss in iteration 67 : 0.4606030018159222
Loss in iteration 68 : 0.46057543566633946
Loss in iteration 69 : 0.4605447870410852
Loss in iteration 70 : 0.4605152372872826
Loss in iteration 71 : 0.4604870211754657
Loss in iteration 72 : 0.4604523244897358
Loss in iteration 73 : 0.46042384732852853
Loss in iteration 74 : 0.4603877127274242
Loss in iteration 75 : 0.46035960697034056
Loss in iteration 76 : 0.46032332051544655
Loss in iteration 77 : 0.46029373601054624
Loss in iteration 78 : 0.4602586829792361
Loss in iteration 79 : 0.46022962940997686
Loss in iteration 80 : 0.4601988619045173
Loss in iteration 81 : 0.4601716912029617
Loss in iteration 82 : 0.460144533307108
Loss in iteration 83 : 0.4601186726608698
Loss in iteration 84 : 0.4600948942153333
Loss in iteration 85 : 0.4600714302864644
Loss in iteration 86 : 0.46005104797450874
Loss in iteration 87 : 0.46002979865667515
Loss in iteration 88 : 0.46001125770413626
Loss in iteration 89 : 0.4599914563277679
Loss in iteration 90 : 0.4599741866095233
Loss in iteration 91 : 0.4599558920990013
Loss in iteration 92 : 0.4599393618926136
Loss in iteration 93 : 0.4599218769585804
Loss in iteration 94 : 0.4599053355548833
Loss in iteration 95 : 0.4598883455870671
Loss in iteration 96 : 0.4598719179635445
Loss in iteration 97 : 0.4598555439043347
Loss in iteration 98 : 0.4598392980784317
Loss in iteration 99 : 0.4598233435890046
Loss in iteration 100 : 0.4598072741773907
Loss in iteration 101 : 0.4597917800058832
Loss in iteration 102 : 0.45977618284310995
Loss in iteration 103 : 0.4597612670806676
Loss in iteration 104 : 0.4597462133687403
Loss in iteration 105 : 0.459731800797859
Loss in iteration 106 : 0.4597173477850396
Loss in iteration 107 : 0.459703525046847
Loss in iteration 108 : 0.4596897970618973
Loss in iteration 109 : 0.4596765890828662
Loss in iteration 110 : 0.45966352378459785
Loss in iteration 111 : 0.45965084600480055
Loss in iteration 112 : 0.45963839692541664
Loss in iteration 113 : 0.45962626100372883
Loss in iteration 114 : 0.45961440300418893
Loss in iteration 115 : 0.4596027601424105
Loss in iteration 116 : 0.4595913945849964
Loss in iteration 117 : 0.4595801929587654
Loss in iteration 118 : 0.4595692868092389
Loss in iteration 119 : 0.45955852450892387
Loss in iteration 120 : 0.4595480388997624
Loss in iteration 121 : 0.4595376743471473
Loss in iteration 122 : 0.45952755725902744
Loss in iteration 123 : 0.45951756922072273
Loss in iteration 124 : 0.45950781523515993
Loss in iteration 125 : 0.45949820295344795
Loss in iteration 126 : 0.4594887972675026
Loss in iteration 127 : 0.4594795412304352
Loss in iteration 128 : 0.4594704719261406
Loss in iteration 129 : 0.4594615671703239
Loss in iteration 130 : 0.4594528363757052
Loss in iteration 131 : 0.4594442749789775
Loss in iteration 132 : 0.4594358708175239
Loss in iteration 133 : 0.4594276354132077
Loss in iteration 134 : 0.45941954768364784
Loss in iteration 135 : 0.45941162859949747
Loss in iteration 136 : 0.4594038510710972
Loss in iteration 137 : 0.45939623457721845
Loss in iteration 138 : 0.4593887515038306
Loss in iteration 139 : 0.45938142086115363
Loss in iteration 140 : 0.4593742197320177
Loss in iteration 141 : 0.4593671631089529
Loss in iteration 142 : 0.45936023195997006
Loss in iteration 143 : 0.45935343517969013
Loss in iteration 144 : 0.45934675961008115
Loss in iteration 145 : 0.4593402107715575
Loss in iteration 146 : 0.45933378110942263
Loss in iteration 147 : 0.459327471851289
Loss in iteration 148 : 0.45932127856594274
Loss in iteration 149 : 0.45931519910713575
Loss in iteration 150 : 0.45930923249104016
Loss in iteration 151 : 0.4593033752267477
Loss in iteration 152 : 0.4592976280197228
Loss in iteration 153 : 0.45929198587833103
Loss in iteration 154 : 0.45928645003388224
Loss in iteration 155 : 0.459281015208896
Loss in iteration 156 : 0.4592756831108169
Loss in iteration 157 : 0.45927044881574514
Loss in iteration 158 : 0.45926531352728434
Loss in iteration 159 : 0.45926027253707835
Loss in iteration 160 : 0.4592553264124468
Loss in iteration 161 : 0.4592504712572369
Loss in iteration 162 : 0.4592457071919842
Loss in iteration 163 : 0.4592410310488649
Loss in iteration 164 : 0.45923644225646965
Loss in iteration 165 : 0.45923193821495295
Loss in iteration 166 : 0.45922751790298627
Loss in iteration 167 : 0.45922317936438933
Loss in iteration 168 : 0.45921892128808
Loss in iteration 169 : 0.45921474212234725
Loss in iteration 170 : 0.45921064024748304
Loss in iteration 171 : 0.45920661437017374
Loss in iteration 172 : 0.4592026628040538
Loss in iteration 173 : 0.4591987845173669
Loss in iteration 174 : 0.4591949778016581
Loss in iteration 175 : 0.45919124169607867
Loss in iteration 176 : 0.4591875744875813
Loss in iteration 177 : 0.45918397526014637
Loss in iteration 178 : 0.459180442411194
Loss in iteration 179 : 0.459176975051613
Loss in iteration 180 : 0.4591735716599806
Loss in iteration 181 : 0.4591702313052399
Loss in iteration 182 : 0.45916695256305684
Loss in iteration 183 : 0.45916373449264847
Testing accuracy  of updater 9 on alg 0 with rate 0.8 = 0.789875, training accuracy 0.789875, time elapsed: 4163 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6791962034667745
Loss in iteration 3 : 0.6676488887390074
Loss in iteration 4 : 0.651821453626469
Loss in iteration 5 : 0.6314906978216437
Loss in iteration 6 : 0.614971084288563
Loss in iteration 7 : 0.5977364275831109
Loss in iteration 8 : 0.5780464170775237
Loss in iteration 9 : 0.5623747416272574
Loss in iteration 10 : 0.550216107507402
Loss in iteration 11 : 0.5382769130261518
Loss in iteration 12 : 0.5281475793220013
Loss in iteration 13 : 0.5209679031955589
Loss in iteration 14 : 0.5145018186888562
Loss in iteration 15 : 0.507923934852429
Loss in iteration 16 : 0.5027122160666736
Loss in iteration 17 : 0.49906356808629493
Loss in iteration 18 : 0.49575890815278056
Loss in iteration 19 : 0.4925848538678843
Loss in iteration 20 : 0.4902610758956819
Loss in iteration 21 : 0.48872394009125253
Loss in iteration 22 : 0.487201751840721
Loss in iteration 23 : 0.4855262987229404
Loss in iteration 24 : 0.4841561212731031
Loss in iteration 25 : 0.48319139435036185
Loss in iteration 26 : 0.4822516037781812
Loss in iteration 27 : 0.48118913499609867
Loss in iteration 28 : 0.4802659250496162
Loss in iteration 29 : 0.4796254917874477
Loss in iteration 30 : 0.4790635998784564
Loss in iteration 31 : 0.4783994541405513
Loss in iteration 32 : 0.47772767188972975
Loss in iteration 33 : 0.4771807244383167
Loss in iteration 34 : 0.47668889566612443
Loss in iteration 35 : 0.47612128589447683
Loss in iteration 36 : 0.47550499620658593
Loss in iteration 37 : 0.47495743125418005
Loss in iteration 38 : 0.4744908936916381
Loss in iteration 39 : 0.4740181722071151
Loss in iteration 40 : 0.47350984093348736
Loss in iteration 41 : 0.47302724741120267
Loss in iteration 42 : 0.4726047904484819
Loss in iteration 43 : 0.4721968315737522
Loss in iteration 44 : 0.4717613571619032
Loss in iteration 45 : 0.471323886254463
Loss in iteration 46 : 0.47092551462717575
Loss in iteration 47 : 0.47055803137659336
Loss in iteration 48 : 0.4701892624412129
Loss in iteration 49 : 0.46982076261634836
Loss in iteration 50 : 0.4694804189644069
Loss in iteration 51 : 0.46917359402310516
Loss in iteration 52 : 0.4688776120314706
Loss in iteration 53 : 0.4685806462846949
Loss in iteration 54 : 0.468296176246009
Loss in iteration 55 : 0.4680353302926857
Loss in iteration 56 : 0.4677887092809423
Loss in iteration 57 : 0.46754455198568406
Loss in iteration 58 : 0.46730767771912773
Loss in iteration 59 : 0.46708889438576684
Loss in iteration 60 : 0.4668868000559107
Loss in iteration 61 : 0.46669193066667847
Loss in iteration 62 : 0.466502458709993
Loss in iteration 63 : 0.46632454588298683
Loss in iteration 64 : 0.4661598447739981
Loss in iteration 65 : 0.4660025471245314
Loss in iteration 66 : 0.4658490023036499
Loss in iteration 67 : 0.46570231456672523
Loss in iteration 68 : 0.4655655208481571
Loss in iteration 69 : 0.46543627660762354
Loss in iteration 70 : 0.4653111463328671
Loss in iteration 71 : 0.4651907961260231
Loss in iteration 72 : 0.4650776082220449
Loss in iteration 73 : 0.46497093569405085
Loss in iteration 74 : 0.4648680276542743
Loss in iteration 75 : 0.46476812148460456
Loss in iteration 76 : 0.4646726251998838
Loss in iteration 77 : 0.46458188703587366
Loss in iteration 78 : 0.46449438372140467
Loss in iteration 79 : 0.4644091585445937
Loss in iteration 80 : 0.46432696491463626
Loss in iteration 81 : 0.4642484922151497
Loss in iteration 82 : 0.46417303030279594
Loss in iteration 83 : 0.4640996080393132
Loss in iteration 84 : 0.4640283134390819
Loss in iteration 85 : 0.46395965970744146
Loss in iteration 86 : 0.46389340219804504
Loss in iteration 87 : 0.4638288224434792
Loss in iteration 88 : 0.4637657557614721
Loss in iteration 89 : 0.4637045767325998
Loss in iteration 90 : 0.4636453574166235
Loss in iteration 91 : 0.4635876926337114
Loss in iteration 92 : 0.46353133287952447
Loss in iteration 93 : 0.4634764468364898
Loss in iteration 94 : 0.4634231582096346
Loss in iteration 95 : 0.46337123863105273
Loss in iteration 96 : 0.46332042510994076
Loss in iteration 97 : 0.4632707412689928
Loss in iteration 98 : 0.4632223111764679
Loss in iteration 99 : 0.46317506176639656
Loss in iteration 100 : 0.46312881095858777
Loss in iteration 101 : 0.4630835242964955
Loss in iteration 102 : 0.46303929244203557
Loss in iteration 103 : 0.4629961137931992
Loss in iteration 104 : 0.46295386426122515
Loss in iteration 105 : 0.462912466310315
Loss in iteration 106 : 0.4628719499367397
Loss in iteration 107 : 0.46283233080431946
Loss in iteration 108 : 0.46279353789645006
Loss in iteration 109 : 0.4627554987107819
Loss in iteration 110 : 0.46271821561339593
Loss in iteration 111 : 0.4626817114341978
Loss in iteration 112 : 0.4626459546919856
Loss in iteration 113 : 0.46261088698824854
Loss in iteration 114 : 0.4625764887316179
Loss in iteration 115 : 0.462542770334523
Loss in iteration 116 : 0.4625097170099401
Loss in iteration 117 : 0.46247728464424137
Loss in iteration 118 : 0.4624454442761416
Loss in iteration 119 : 0.4624141953543503
Loss in iteration 120 : 0.4623835327845653
Loss in iteration 121 : 0.462353429232257
Loss in iteration 122 : 0.46232385777446544
Loss in iteration 123 : 0.46229481052412075
Loss in iteration 124 : 0.4622662837955778
Loss in iteration 125 : 0.4622382597146665
Loss in iteration 126 : 0.4622107143587083
Loss in iteration 127 : 0.4621836344117465
Loss in iteration 128 : 0.4621570141544588
Loss in iteration 129 : 0.4621308414999868
Loss in iteration 130 : 0.4621050976934582
Loss in iteration 131 : 0.4620797686728259
Loss in iteration 132 : 0.46205484767064503
Loss in iteration 133 : 0.4620303264380844
Loss in iteration 134 : 0.4620061911542874
Loss in iteration 135 : 0.461982428551178
Loss in iteration 136 : 0.4619590304291178
Loss in iteration 137 : 0.461935989630476
Loss in iteration 138 : 0.4619132955802083
Loss in iteration 139 : 0.46189093665037867
Loss in iteration 140 : 0.46186890427119054
Loss in iteration 141 : 0.46184719187786516
Loss in iteration 142 : 0.46182579138543745
Loss in iteration 143 : 0.4618046933197047
Loss in iteration 144 : 0.4617838896606089
Loss in iteration 145 : 0.4617633742502022
Loss in iteration 146 : 0.4617431405087707
Loss in iteration 147 : 0.4617231806001661
Loss in iteration 148 : 0.4617034870992744
Loss in iteration 149 : 0.4616840540112832
Loss in iteration 150 : 0.46166487564049785
Loss in iteration 151 : 0.4616459455473261
Loss in iteration 152 : 0.46162725725498743
Loss in iteration 153 : 0.4616088052252735
Loss in iteration 154 : 0.46159058446334356
Loss in iteration 155 : 0.461572589626413
Loss in iteration 156 : 0.46155481514142727
Loss in iteration 157 : 0.46153725592577677
Loss in iteration 158 : 0.46151990741662774
Loss in iteration 159 : 0.4615027649856105
Loss in iteration 160 : 0.4614858238008064
Loss in iteration 161 : 0.4614690792760202
Loss in iteration 162 : 0.4614525272683645
Loss in iteration 163 : 0.4614361637456495
Loss in iteration 164 : 0.46141998455186617
Loss in iteration 165 : 0.46140398562108476
Loss in iteration 166 : 0.4613881631985358
Loss in iteration 167 : 0.4613725136993906
Loss in iteration 168 : 0.4613570334928307
Loss in iteration 169 : 0.4613417189680058
Loss in iteration 170 : 0.46132656671752537
Loss in iteration 171 : 0.4613115735165534
Loss in iteration 172 : 0.4612967361678302
Loss in iteration 173 : 0.46128205148610174
Loss in iteration 174 : 0.46126751641628183
Loss in iteration 175 : 0.46125312806239677
Loss in iteration 176 : 0.46123888359048393
Loss in iteration 177 : 0.4612247801808086
Loss in iteration 178 : 0.46121081509278294
Loss in iteration 179 : 0.4611969857118619
Loss in iteration 180 : 0.46118328950186877
Loss in iteration 181 : 0.4611697239545409
Loss in iteration 182 : 0.4611562866158241
Loss in iteration 183 : 0.4611429751289453
Loss in iteration 184 : 0.46112978721811
Loss in iteration 185 : 0.46111672064717757
Loss in iteration 186 : 0.4611037732217358
Loss in iteration 187 : 0.4610909428195998
Loss in iteration 188 : 0.4610782273914606
Loss in iteration 189 : 0.4610656249331242
Loss in iteration 190 : 0.4610531334770367
Loss in iteration 191 : 0.4610407511106528
Loss in iteration 192 : 0.46102847598427477
Loss in iteration 193 : 0.46101630629527546
Loss in iteration 194 : 0.46100424027662085
Loss in iteration 195 : 0.46099227620541966
Loss in iteration 196 : 0.4609804124115531
Loss in iteration 197 : 0.4609686472700082
Loss in iteration 198 : 0.46095697919017925
Loss in iteration 199 : 0.46094540661792655
Loss in iteration 200 : 0.4609339280425589
Loss in iteration 201 : 0.46092254199449045
Loss in iteration 202 : 0.4609112470373596
Loss in iteration 203 : 0.4609000417668199
Loss in iteration 204 : 0.46088892481505717
Loss in iteration 205 : 0.460877894850951
Loss in iteration 206 : 0.4608669505748125
Loss in iteration 207 : 0.46085609071574024
Loss in iteration 208 : 0.46084531403382983
Loss in iteration 209 : 0.4608346193211649
Loss in iteration 210 : 0.46082400539870616
Loss in iteration 211 : 0.4608134711135316
Loss in iteration 212 : 0.46080301533953366
Loss in iteration 213 : 0.4607926369784875
Loss in iteration 214 : 0.46078233495845417
Loss in iteration 215 : 0.4607721082314113
Loss in iteration 216 : 0.46076195577299733
Loss in iteration 217 : 0.4607518765831924
Loss in iteration 218 : 0.460741869685522
Loss in iteration 219 : 0.46073193412524693
Loss in iteration 220 : 0.4607220689686523
Loss in iteration 221 : 0.4607122733033479
Loss in iteration 222 : 0.4607025462379316
Loss in iteration 223 : 0.46069288690070814
Loss in iteration 224 : 0.460683294438854
Loss in iteration 225 : 0.4606737680184054
Loss in iteration 226 : 0.46066430682406995
Loss in iteration 227 : 0.46065491005835124
Loss in iteration 228 : 0.46064557694073643
Loss in iteration 229 : 0.4606363067074653
Loss in iteration 230 : 0.46062709861137535
Loss in iteration 231 : 0.4606179519213042
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.788125, training accuracy 0.788125, time elapsed: 5200 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.681404118803596
Loss in iteration 3 : 0.6705442881651473
Loss in iteration 4 : 0.6589408926905835
Loss in iteration 5 : 0.6422333462905815
Loss in iteration 6 : 0.6256042282290986
Loss in iteration 7 : 0.6117086019712944
Loss in iteration 8 : 0.5964414485223406
Loss in iteration 9 : 0.5798442712153825
Loss in iteration 10 : 0.566034686499223
Loss in iteration 11 : 0.5552067263399474
Loss in iteration 12 : 0.5449796532850216
Loss in iteration 13 : 0.5353641210148017
Loss in iteration 14 : 0.527758572790386
Loss in iteration 15 : 0.5218077218412664
Loss in iteration 16 : 0.5160973969578155
Loss in iteration 17 : 0.510498551489414
Loss in iteration 18 : 0.5058835134347309
Loss in iteration 19 : 0.5024026590071624
Loss in iteration 20 : 0.49937120329210827
Loss in iteration 21 : 0.4964304877625906
Loss in iteration 22 : 0.49389585040237777
Loss in iteration 23 : 0.4920158510517456
Loss in iteration 24 : 0.49050753147497944
Loss in iteration 25 : 0.48898026860365734
Loss in iteration 26 : 0.4874356920327709
Loss in iteration 27 : 0.4861199662935576
Loss in iteration 28 : 0.48508848124311704
Loss in iteration 29 : 0.4841493831404573
Loss in iteration 30 : 0.48316693057139115
Loss in iteration 31 : 0.48222328096998524
Loss in iteration 32 : 0.48144985063005186
Loss in iteration 33 : 0.48082662892770667
Loss in iteration 34 : 0.48022485268448484
Loss in iteration 35 : 0.47958480275772525
Loss in iteration 36 : 0.47896466416547195
Loss in iteration 37 : 0.4784246983921553
Loss in iteration 38 : 0.47793646490856295
Loss in iteration 39 : 0.4774323848250965
Loss in iteration 40 : 0.476900766644351
Loss in iteration 41 : 0.47639086050682544
Loss in iteration 42 : 0.47593779433171185
Loss in iteration 43 : 0.4755202120324179
Loss in iteration 44 : 0.4750989604129779
Loss in iteration 45 : 0.4746704042754934
Loss in iteration 46 : 0.4742622098468778
Loss in iteration 47 : 0.47388877092391957
Loss in iteration 48 : 0.47353270538291553
Loss in iteration 49 : 0.4731718126335726
Loss in iteration 50 : 0.47280781950682504
Loss in iteration 51 : 0.47245924775104436
Loss in iteration 52 : 0.4721344337827983
Loss in iteration 53 : 0.47182327833139787
Loss in iteration 54 : 0.4715145449016749
Loss in iteration 55 : 0.4712110766621057
Loss in iteration 56 : 0.47092339910869063
Loss in iteration 57 : 0.47065413927684396
Loss in iteration 58 : 0.4703953639309129
Loss in iteration 59 : 0.4701399908981947
Loss in iteration 60 : 0.4698899346636686
Loss in iteration 61 : 0.4696511489198051
Loss in iteration 62 : 0.46942454780846277
Loss in iteration 63 : 0.46920538686197233
Loss in iteration 64 : 0.4689902760010522
Loss in iteration 65 : 0.46878119974630933
Loss in iteration 66 : 0.46858180415754946
Loss in iteration 67 : 0.46839217685102286
Loss in iteration 68 : 0.4682091565463883
Loss in iteration 69 : 0.46803071369975635
Loss in iteration 70 : 0.4678579511164751
Loss in iteration 71 : 0.46769261139129814
Loss in iteration 72 : 0.46753423308892184
Loss in iteration 73 : 0.4673807622867299
Loss in iteration 74 : 0.46723118225819105
Loss in iteration 75 : 0.4670863445402034
Loss in iteration 76 : 0.4669472448710946
Loss in iteration 77 : 0.4668134648525521
Loss in iteration 78 : 0.4666837682250688
Loss in iteration 79 : 0.46655765062142984
Loss in iteration 80 : 0.4664356279925339
Loss in iteration 81 : 0.4663181232438338
Loss in iteration 82 : 0.4662046755551836
Loss in iteration 83 : 0.4660944521766182
Loss in iteration 84 : 0.46598715486682324
Loss in iteration 85 : 0.46588306919300715
Loss in iteration 86 : 0.46578236289662006
Loss in iteration 87 : 0.465684698429179
Loss in iteration 88 : 0.46558960555773365
Loss in iteration 89 : 0.46549697559387215
Loss in iteration 90 : 0.4654069992805226
Loss in iteration 91 : 0.4653197288661295
Loss in iteration 92 : 0.4652349053899459
Loss in iteration 93 : 0.46515222561580244
Loss in iteration 94 : 0.4650716158860764
Loss in iteration 95 : 0.4649931548918107
Loss in iteration 96 : 0.4649168164860323
Loss in iteration 97 : 0.46484240679426825
Loss in iteration 98 : 0.4647697427597202
Loss in iteration 99 : 0.46469879229539457
Loss in iteration 100 : 0.4646295986179901
Loss in iteration 101 : 0.46456212737534586
Loss in iteration 102 : 0.46449624979705617
Loss in iteration 103 : 0.4644318562241217
Loss in iteration 104 : 0.46436892483690745
Loss in iteration 105 : 0.46430746323097927
Loss in iteration 106 : 0.4642474238338933
Loss in iteration 107 : 0.4641887098894871
Loss in iteration 108 : 0.46413124785963883
Loss in iteration 109 : 0.4640750192818075
Loss in iteration 110 : 0.46402001823039973
Loss in iteration 111 : 0.4639662042336397
Loss in iteration 112 : 0.4639135129701454
Loss in iteration 113 : 0.46386189923254667
Loss in iteration 114 : 0.4638113489899534
Loss in iteration 115 : 0.46376184989381175
Loss in iteration 116 : 0.4637133662928955
Loss in iteration 117 : 0.463665850455104
Loss in iteration 118 : 0.46361926844739004
Loss in iteration 119 : 0.46357360415034043
Loss in iteration 120 : 0.46352884054856486
Loss in iteration 121 : 0.46348494736487844
Loss in iteration 122 : 0.46344189014489207
Loss in iteration 123 : 0.46339964473059203
Loss in iteration 124 : 0.4633581969119406
Loss in iteration 125 : 0.4633175302898063
Loss in iteration 126 : 0.4632776203201082
Loss in iteration 127 : 0.4632384410584671
Loss in iteration 128 : 0.46319997326973555
Loss in iteration 129 : 0.46316220294731086
Loss in iteration 130 : 0.46312511397097467
Loss in iteration 131 : 0.46308868565201056
Loss in iteration 132 : 0.46305289749511724
Loss in iteration 133 : 0.4630177335991219
Loss in iteration 134 : 0.4629831810030987
Loss in iteration 135 : 0.46294922527113996
Loss in iteration 136 : 0.46291584957808224
Loss in iteration 137 : 0.46288303776348166
Loss in iteration 138 : 0.4628507765276365
Loss in iteration 139 : 0.462819053998072
Loss in iteration 140 : 0.46278785720109755
Loss in iteration 141 : 0.46275717194414384
Loss in iteration 142 : 0.46272698479460017
Loss in iteration 143 : 0.4626972841536646
Loss in iteration 144 : 0.4626680591849568
Loss in iteration 145 : 0.4626392984118335
Loss in iteration 146 : 0.46261098988664523
Loss in iteration 147 : 0.46258312240331784
Loss in iteration 148 : 0.46255568594582347
Loss in iteration 149 : 0.46252867090485483
Loss in iteration 150 : 0.4625020673000888
Loss in iteration 151 : 0.4624758650064234
Loss in iteration 152 : 0.4624500544796705
Loss in iteration 153 : 0.4624246269220936
Loss in iteration 154 : 0.4623995737688325
Loss in iteration 155 : 0.46237488629231405
Loss in iteration 156 : 0.4623505558148089
Loss in iteration 157 : 0.46232657412635114
Loss in iteration 158 : 0.4623029335123972
Loss in iteration 159 : 0.4622796264225941
Loss in iteration 160 : 0.46225664527740873
Loss in iteration 161 : 0.46223398263128485
Loss in iteration 162 : 0.46221163139810895
Loss in iteration 163 : 0.46218958481806455
Loss in iteration 164 : 0.4621678362478933
Loss in iteration 165 : 0.46214637907585526
Loss in iteration 166 : 0.4621252068430576
Loss in iteration 167 : 0.4621043133666012
Loss in iteration 168 : 0.4620836926968178
Loss in iteration 169 : 0.4620633389915712
Loss in iteration 170 : 0.46204324648383577
Loss in iteration 171 : 0.46202340956185867
Loss in iteration 172 : 0.4620038228285191
Loss in iteration 173 : 0.46198448105907997
Loss in iteration 174 : 0.4619653791259226
Loss in iteration 175 : 0.4619465119895169
Loss in iteration 176 : 0.4619278747488422
Loss in iteration 177 : 0.4619094626677007
Loss in iteration 178 : 0.46189127114191947
Loss in iteration 179 : 0.46187329565777874
Loss in iteration 180 : 0.46185553179449595
Loss in iteration 181 : 0.46183797525513703
Loss in iteration 182 : 0.46182062187553075
Loss in iteration 183 : 0.4618034675992884
Loss in iteration 184 : 0.4617865084541998
Loss in iteration 185 : 0.4617697405568743
Loss in iteration 186 : 0.4617531601300942
Loss in iteration 187 : 0.4617367635035064
Loss in iteration 188 : 0.4617205470961095
Loss in iteration 189 : 0.4617045074036446
Loss in iteration 190 : 0.461688641003379
Loss in iteration 191 : 0.4616729445635344
Loss in iteration 192 : 0.46165741484098405
Loss in iteration 193 : 0.46164204866958947
Loss in iteration 194 : 0.46162684295352496
Loss in iteration 195 : 0.46161179467066144
Loss in iteration 196 : 0.4615969008767064
Loss in iteration 197 : 0.46158215870165387
Loss in iteration 198 : 0.4615675653420049
Loss in iteration 199 : 0.4615531180573052
Loss in iteration 200 : 0.46153881417234105
Loss in iteration 201 : 0.46152465107859125
Loss in iteration 202 : 0.46151062623084854
Loss in iteration 203 : 0.46149673714215556
Loss in iteration 204 : 0.4614829813819578
Loss in iteration 205 : 0.46146935657722554
Loss in iteration 206 : 0.46145586041244885
Loss in iteration 207 : 0.4614424906267018
Loss in iteration 208 : 0.4614292450103221
Loss in iteration 209 : 0.46141612140376775
Loss in iteration 210 : 0.46140311769794956
Loss in iteration 211 : 0.4613902318335065
Loss in iteration 212 : 0.46137746179846034
Loss in iteration 213 : 0.46136480562598886
Loss in iteration 214 : 0.4613522613936422
Loss in iteration 215 : 0.4613398272231791
Loss in iteration 216 : 0.4613275012796006
Loss in iteration 217 : 0.4613152817693251
Loss in iteration 218 : 0.4613031669386589
Loss in iteration 219 : 0.46129115507314855
Loss in iteration 220 : 0.4612792444971347
Loss in iteration 221 : 0.4612674335727361
Loss in iteration 222 : 0.4612557206984085
Loss in iteration 223 : 0.4612441043078293
Loss in iteration 224 : 0.46123258286929847
Loss in iteration 225 : 0.46122115488515497
Loss in iteration 226 : 0.4612098188908409
Loss in iteration 227 : 0.4611985734537903
Loss in iteration 228 : 0.46118741717258127
Loss in iteration 229 : 0.4611763486763619
Loss in iteration 230 : 0.4611653666242378
Loss in iteration 231 : 0.46115446970443663
Loss in iteration 232 : 0.4611436566334203
Loss in iteration 233 : 0.4611329261551958
Testing accuracy  of updater 9 on alg 0 with rate 0.14 = 0.789, training accuracy 0.789, time elapsed: 5158 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6849713203595502
Loss in iteration 3 : 0.6755119000397595
Loss in iteration 4 : 0.6673210127066921
Loss in iteration 5 : 0.6576042997863909
Loss in iteration 6 : 0.645028932814572
Loss in iteration 7 : 0.6319691764121558
Loss in iteration 8 : 0.6205588394359625
Loss in iteration 9 : 0.6098964489122648
Loss in iteration 10 : 0.5982766334647751
Loss in iteration 11 : 0.5861014558799023
Loss in iteration 12 : 0.5751252869466842
Loss in iteration 13 : 0.5660430056030131
Loss in iteration 14 : 0.5580740311991992
Loss in iteration 15 : 0.550405933953944
Loss in iteration 16 : 0.5431427951152982
Loss in iteration 17 : 0.5368363313911627
Loss in iteration 18 : 0.5316008168438974
Loss in iteration 19 : 0.5269742442306563
Loss in iteration 20 : 0.5225034725211356
Loss in iteration 21 : 0.5181937469055913
Loss in iteration 22 : 0.5143438319211789
Loss in iteration 23 : 0.5111102763047869
Loss in iteration 24 : 0.5083446561707343
Loss in iteration 25 : 0.5057981199166949
Loss in iteration 26 : 0.5033762982327257
Loss in iteration 27 : 0.5011674626952987
Loss in iteration 28 : 0.49927389518867477
Loss in iteration 29 : 0.497668181465114
Loss in iteration 30 : 0.49621693222136143
Loss in iteration 31 : 0.49481117183008416
Loss in iteration 32 : 0.4934483502795893
Loss in iteration 33 : 0.4921947234426193
Loss in iteration 34 : 0.49109202146231007
Loss in iteration 35 : 0.4901120175648948
Loss in iteration 36 : 0.4891914195101515
Loss in iteration 37 : 0.488294284572745
Loss in iteration 38 : 0.4874352249081777
Loss in iteration 39 : 0.4866494645636369
Loss in iteration 40 : 0.4859494885524779
Loss in iteration 41 : 0.48531221958096615
Loss in iteration 42 : 0.48470256194412764
Loss in iteration 43 : 0.484103659454666
Loss in iteration 44 : 0.48352414484003736
Loss in iteration 45 : 0.48298053951466036
Loss in iteration 46 : 0.4824766137307073
Loss in iteration 47 : 0.48199968735174187
Loss in iteration 48 : 0.48153376824266036
Loss in iteration 49 : 0.48107372362109335
Loss in iteration 50 : 0.4806269481493472
Loss in iteration 51 : 0.480203303717269
Loss in iteration 52 : 0.47980479443703833
Loss in iteration 53 : 0.479424453420619
Loss in iteration 54 : 0.47905362567716847
Loss in iteration 55 : 0.4786893186574707
Loss in iteration 56 : 0.4783348971207184
Loss in iteration 57 : 0.477994839548769
Loss in iteration 58 : 0.477669567444444
Loss in iteration 59 : 0.47735509206179777
Loss in iteration 60 : 0.4770468726549856
Loss in iteration 61 : 0.4767435483854358
Loss in iteration 62 : 0.4764471642420112
Loss in iteration 63 : 0.47616035144860236
Loss in iteration 64 : 0.47588359067947816
Loss in iteration 65 : 0.47561499765500515
Loss in iteration 66 : 0.47535230883748825
Loss in iteration 67 : 0.47509481793532643
Loss in iteration 68 : 0.4748435047936758
Loss in iteration 69 : 0.47459958828103027
Loss in iteration 70 : 0.4743631252600007
Loss in iteration 71 : 0.4741329251115725
Loss in iteration 72 : 0.47390761384228325
Loss in iteration 73 : 0.4736866737563893
Loss in iteration 74 : 0.4734705313497496
Loss in iteration 75 : 0.4732598008687007
Loss in iteration 76 : 0.4730545286142217
Loss in iteration 77 : 0.4728541170129126
Loss in iteration 78 : 0.4726578620861009
Loss in iteration 79 : 0.47246549667704596
Loss in iteration 80 : 0.47227724331193754
Loss in iteration 81 : 0.47209341865022475
Loss in iteration 82 : 0.47191402873103666
Loss in iteration 83 : 0.4717387195411284
Loss in iteration 84 : 0.4715670588863559
Loss in iteration 85 : 0.4713988332771178
Loss in iteration 86 : 0.47123409171585023
Loss in iteration 87 : 0.4710729477724309
Loss in iteration 88 : 0.47091536628101704
Loss in iteration 89 : 0.4707611306837991
Loss in iteration 90 : 0.47060998608505233
Loss in iteration 91 : 0.4704617952187107
Loss in iteration 92 : 0.4703165636962253
Loss in iteration 93 : 0.47017433591445773
Loss in iteration 94 : 0.47003507914589465
Loss in iteration 95 : 0.4698986615490989
Loss in iteration 96 : 0.46976492524242336
Loss in iteration 97 : 0.4696337700580145
Loss in iteration 98 : 0.4695051703621564
Loss in iteration 99 : 0.4693791225701866
Loss in iteration 100 : 0.4692555839107998
Loss in iteration 101 : 0.4691344593855562
Loss in iteration 102 : 0.4690156396994987
Loss in iteration 103 : 0.46889904658287107
Loss in iteration 104 : 0.46878464357242217
Loss in iteration 105 : 0.46867240931569804
Loss in iteration 106 : 0.4685623046056385
Loss in iteration 107 : 0.46845426389480016
Loss in iteration 108 : 0.468348214041693
Loss in iteration 109 : 0.4682440979690641
Loss in iteration 110 : 0.4681418807526204
Loss in iteration 111 : 0.46804153578364627
Loss in iteration 112 : 0.4679430270785731
Loss in iteration 113 : 0.4678463042812598
Loss in iteration 114 : 0.46775131240358836
Loss in iteration 115 : 0.46765800475679575
Loss in iteration 116 : 0.46756634688463194
Loss in iteration 117 : 0.4674763097299535
Loss in iteration 118 : 0.46738786027523055
Loss in iteration 119 : 0.4673009585792527
Loss in iteration 120 : 0.4672155626570433
Loss in iteration 121 : 0.46713163529758917
Loss in iteration 122 : 0.4670491462746853
Loss in iteration 123 : 0.4669680687788121
Loss in iteration 124 : 0.46688837431002933
Loss in iteration 125 : 0.4668100308504974
Loss in iteration 126 : 0.46673300527795114
Loss in iteration 127 : 0.4666572669968797
Loss in iteration 128 : 0.46658278925683067
Loss in iteration 129 : 0.4665095473906581
Loss in iteration 130 : 0.46643751613206863
Loss in iteration 131 : 0.46636666859720743
Loss in iteration 132 : 0.4662969775307706
Loss in iteration 133 : 0.46622841726483943
Loss in iteration 134 : 0.4661609644868669
Loss in iteration 135 : 0.46609459733887515
Loss in iteration 136 : 0.4660292939561283
Loss in iteration 137 : 0.465965031844381
Loss in iteration 138 : 0.4659017884742211
Loss in iteration 139 : 0.4658395423041456
Loss in iteration 140 : 0.46577827320765464
Loss in iteration 141 : 0.465717962010434
Loss in iteration 142 : 0.46565858970166984
Loss in iteration 143 : 0.46560013707336123
Loss in iteration 144 : 0.46554258501793633
Loss in iteration 145 : 0.4654859150825649
Loss in iteration 146 : 0.46543010972599425
Loss in iteration 147 : 0.4653751520968393
Loss in iteration 148 : 0.465321025616313
Loss in iteration 149 : 0.46526771376950865
Loss in iteration 150 : 0.4652152002439432
Loss in iteration 151 : 0.46516346921438456
Loss in iteration 152 : 0.4651125054786531
Loss in iteration 153 : 0.4650622943388137
Loss in iteration 154 : 0.4650128213714967
Loss in iteration 155 : 0.4649640723043918
Loss in iteration 156 : 0.4649160330803479
Loss in iteration 157 : 0.4648686900071684
Loss in iteration 158 : 0.46482202983407533
Loss in iteration 159 : 0.46477603969232756
Loss in iteration 160 : 0.4647307069719859
Loss in iteration 161 : 0.4646860192511112
Loss in iteration 162 : 0.46464196432492555
Loss in iteration 163 : 0.46459853028378645
Loss in iteration 164 : 0.46455570555462644
Loss in iteration 165 : 0.46451347886936906
Loss in iteration 166 : 0.4644718391962447
Loss in iteration 167 : 0.4644307756964154
Loss in iteration 168 : 0.4643902777337617
Loss in iteration 169 : 0.4643503349127758
Loss in iteration 170 : 0.4643109370992853
Loss in iteration 171 : 0.4642720744030455
Loss in iteration 172 : 0.46423373713988225
Loss in iteration 173 : 0.4641959158063771
Loss in iteration 174 : 0.46415860108275603
Loss in iteration 175 : 0.4641217838514777
Loss in iteration 176 : 0.464085455207386
Loss in iteration 177 : 0.464049606447581
Loss in iteration 178 : 0.4640142290497957
Loss in iteration 179 : 0.46397931465682224
Loss in iteration 180 : 0.46394485507585287
Loss in iteration 181 : 0.46391084228653534
Loss in iteration 182 : 0.46387726844494853
Loss in iteration 183 : 0.4638441258769026
Loss in iteration 184 : 0.46381140706493007
Loss in iteration 185 : 0.4637791046383504
Loss in iteration 186 : 0.46374721137133823
Loss in iteration 187 : 0.4637157201859316
Loss in iteration 188 : 0.4636846241531214
Loss in iteration 189 : 0.46365391648831933
Loss in iteration 190 : 0.4636235905433233
Loss in iteration 191 : 0.463593639799755
Loss in iteration 192 : 0.4635640578667301
Loss in iteration 193 : 0.46353483848125515
Loss in iteration 194 : 0.4635059755077621
Loss in iteration 195 : 0.46347746293471004
Loss in iteration 196 : 0.46344929486933334
Loss in iteration 197 : 0.4634214655331235
Loss in iteration 198 : 0.46339396925954585
Loss in iteration 199 : 0.46336680049327283
Loss in iteration 200 : 0.46333995378895815
Loss in iteration 201 : 0.4633134238085425
Loss in iteration 202 : 0.46328720531753187
Loss in iteration 203 : 0.46326129318170894
Loss in iteration 204 : 0.46323568236505125
Loss in iteration 205 : 0.4632103679284934
Loss in iteration 206 : 0.46318534502851433
Loss in iteration 207 : 0.46316060891493166
Loss in iteration 208 : 0.4631361549281927
Loss in iteration 209 : 0.46311197849686414
Loss in iteration 210 : 0.46308807513580624
Loss in iteration 211 : 0.46306444044479633
Loss in iteration 212 : 0.46304107010707957
Loss in iteration 213 : 0.46301795988752414
Loss in iteration 214 : 0.4629951056304916
Loss in iteration 215 : 0.46297250325785055
Loss in iteration 216 : 0.46295014876734136
Loss in iteration 217 : 0.46292803823123085
Loss in iteration 218 : 0.46290616779492827
Loss in iteration 219 : 0.4628845336754265
Loss in iteration 220 : 0.4628631321596
Loss in iteration 221 : 0.462841959602601
Loss in iteration 222 : 0.46282101242645196
Loss in iteration 223 : 0.46280028711878934
Loss in iteration 224 : 0.46277978023162053
Loss in iteration 225 : 0.4627594883799665
Loss in iteration 226 : 0.4627394082404597
Loss in iteration 227 : 0.4627195365499858
Loss in iteration 228 : 0.4626998701044628
Loss in iteration 229 : 0.4626804057576887
Loss in iteration 230 : 0.46266114042023115
Loss in iteration 231 : 0.4626420710582569
Loss in iteration 232 : 0.4626231946923428
Loss in iteration 233 : 0.4626045083963459
Loss in iteration 234 : 0.4625860092963178
Loss in iteration 235 : 0.4625676945695087
Loss in iteration 236 : 0.4625495614433698
Loss in iteration 237 : 0.46253160719453557
Loss in iteration 238 : 0.4625138291478239
Loss in iteration 239 : 0.46249622467524126
Loss in iteration 240 : 0.4624787911950575
Loss in iteration 241 : 0.4624615261709005
Loss in iteration 242 : 0.46244442711088507
Loss in iteration 243 : 0.46242749156672575
Loss in iteration 244 : 0.4624107171328669
Loss in iteration 245 : 0.46239410144563553
Loss in iteration 246 : 0.4623776421824302
Loss in iteration 247 : 0.4623613370609172
Loss in iteration 248 : 0.462345183838268
Loss in iteration 249 : 0.4623291803103955
Loss in iteration 250 : 0.4623133243111778
Loss in iteration 251 : 0.4622976137117394
Loss in iteration 252 : 0.46228204641971693
Loss in iteration 253 : 0.4622666203785743
Loss in iteration 254 : 0.4622513335669214
Loss in iteration 255 : 0.46223618399783084
Loss in iteration 256 : 0.4622211697181867
Testing accuracy  of updater 9 on alg 0 with rate 0.08000000000000002 = 0.791125, training accuracy 0.791125, time elapsed: 5223 millisecond.
Loss in iteration 1 : 0.6931471805599603
Loss in iteration 2 : 0.6905791265145829
Loss in iteration 3 : 0.6864060076885692
Loss in iteration 4 : 0.6816434633743511
Loss in iteration 5 : 0.6770096328979414
Loss in iteration 6 : 0.6727261297443407
Loss in iteration 7 : 0.6686016063752505
Loss in iteration 8 : 0.6642847661380674
Loss in iteration 9 : 0.6595148462825858
Loss in iteration 10 : 0.6542533106809826
Loss in iteration 11 : 0.6486728276131168
Loss in iteration 12 : 0.6430479233042236
Loss in iteration 13 : 0.6376200173695513
Loss in iteration 14 : 0.6325030548643651
Loss in iteration 15 : 0.6276648883998591
Loss in iteration 16 : 0.6229787967666767
Loss in iteration 17 : 0.6183079117130592
Loss in iteration 18 : 0.6135769138567719
Loss in iteration 19 : 0.6088005536470404
Loss in iteration 20 : 0.6040651491639046
Loss in iteration 21 : 0.5994816943828926
Loss in iteration 22 : 0.5951377554411518
Loss in iteration 23 : 0.5910693247749764
Loss in iteration 24 : 0.5872597869327956
Loss in iteration 25 : 0.5836597084189543
Loss in iteration 26 : 0.580214136390344
Loss in iteration 27 : 0.5768846062014366
Loss in iteration 28 : 0.5736585936375793
Loss in iteration 29 : 0.570545965265518
Loss in iteration 30 : 0.5675671075807741
Loss in iteration 31 : 0.564739451659316
Loss in iteration 32 : 0.5620681317445064
Loss in iteration 33 : 0.5595436028307181
Loss in iteration 34 : 0.5571456989027942
Loss in iteration 35 : 0.5548511937135591
Loss in iteration 36 : 0.5526411717438773
Loss in iteration 37 : 0.5505053750140256
Loss in iteration 38 : 0.548442477415309
Loss in iteration 39 : 0.5464570416058825
Loss in iteration 40 : 0.5445550199810828
Loss in iteration 41 : 0.5427397992940864
Loss in iteration 42 : 0.5410101326039378
Loss in iteration 43 : 0.5393602926923129
Loss in iteration 44 : 0.5377818842337085
Loss in iteration 45 : 0.5362662741848253
Loss in iteration 46 : 0.5348066142790663
Loss in iteration 47 : 0.5333988114120634
Loss in iteration 48 : 0.5320413261440766
Loss in iteration 49 : 0.5307341270271995
Loss in iteration 50 : 0.5294773592938956
Loss in iteration 51 : 0.5282702685367434
Loss in iteration 52 : 0.5271107144018161
Loss in iteration 53 : 0.5259953286278836
Loss in iteration 54 : 0.5249201306891782
Loss in iteration 55 : 0.5238812896028961
Loss in iteration 56 : 0.5228757319507139
Loss in iteration 57 : 0.5219014113690764
Loss in iteration 58 : 0.5209572112567228
Loss in iteration 59 : 0.5200425863282057
Loss in iteration 60 : 0.5191571175993682
Loss in iteration 61 : 0.518300149081699
Loss in iteration 62 : 0.5174706111914873
Loss in iteration 63 : 0.516667049835761
Loss in iteration 64 : 0.5158878059099962
Loss in iteration 65 : 0.5151312504368635
Loss in iteration 66 : 0.514395981760563
Loss in iteration 67 : 0.5136809239020881
Loss in iteration 68 : 0.5129853115532037
Loss in iteration 69 : 0.5123085886989018
Loss in iteration 70 : 0.5116502716012752
Loss in iteration 71 : 0.51100982863768
Loss in iteration 72 : 0.5103866132645672
Loss in iteration 73 : 0.5097798613211842
Loss in iteration 74 : 0.5091887401769448
Loss in iteration 75 : 0.5086124224920496
Loss in iteration 76 : 0.5080501546765553
Loss in iteration 77 : 0.5075012979327905
Loss in iteration 78 : 0.5069653333185994
Loss in iteration 79 : 0.5064418359026976
Loss in iteration 80 : 0.5059304322661508
Loss in iteration 81 : 0.505430758211247
Loss in iteration 82 : 0.5049424300604481
Loss in iteration 83 : 0.504465035758945
Loss in iteration 84 : 0.5039981442493412
Loss in iteration 85 : 0.503541325923524
Loss in iteration 86 : 0.5030941748007566
Loss in iteration 87 : 0.5026563244175448
Loss in iteration 88 : 0.5022274530948738
Loss in iteration 89 : 0.5018072785980088
Loss in iteration 90 : 0.5013955456756178
Loss in iteration 91 : 0.5009920116115071
Loss in iteration 92 : 0.5005964345787335
Loss in iteration 93 : 0.5002085677627622
Loss in iteration 94 : 0.499828159801538
Loss in iteration 95 : 0.4994549599728733
Loss in iteration 96 : 0.4990887253639693
Loss in iteration 97 : 0.4987292271834484
Loss in iteration 98 : 0.4983762542234119
Loss in iteration 99 : 0.4980296127927961
Loss in iteration 100 : 0.4976891237107351
Loss in iteration 101 : 0.49735461777481305
Loss in iteration 102 : 0.49702593132913414
Loss in iteration 103 : 0.49670290320769933
Loss in iteration 104 : 0.49638537364314644
Loss in iteration 105 : 0.49607318499461633
Loss in iteration 106 : 0.495766183605214
Loss in iteration 107 : 0.4954642218829159
Loss in iteration 108 : 0.49516715981204057
Loss in iteration 109 : 0.49487486544372805
Loss in iteration 110 : 0.4945872143301501
Loss in iteration 111 : 0.4943040882117192
Loss in iteration 112 : 0.49402537344547176
Loss in iteration 113 : 0.49375095965217325
Loss in iteration 114 : 0.4934807389009002
Loss in iteration 115 : 0.4932146055219509
Loss in iteration 116 : 0.49295245642716534
Loss in iteration 117 : 0.49269419168463513
Loss in iteration 118 : 0.49243971506760387
Loss in iteration 119 : 0.4921889343632477
Loss in iteration 120 : 0.4919417613468204
Loss in iteration 121 : 0.49169811145216386
Loss in iteration 122 : 0.49145790326070676
Loss in iteration 123 : 0.491221057965918
Loss in iteration 124 : 0.4909874989491299
Loss in iteration 125 : 0.4907571515432521
Loss in iteration 126 : 0.49052994298920355
Loss in iteration 127 : 0.4903058025306748
Loss in iteration 128 : 0.49008466156200575
Loss in iteration 129 : 0.48986645374557597
Loss in iteration 130 : 0.4896511150422342
Loss in iteration 131 : 0.4894385836375568
Loss in iteration 132 : 0.4892287997835069
Loss in iteration 133 : 0.4890217055985696
Loss in iteration 134 : 0.48881724487489725
Loss in iteration 135 : 0.48861536293032776
Loss in iteration 136 : 0.48841600652279576
Loss in iteration 137 : 0.4882191238227845
Loss in iteration 138 : 0.48802466442319625
Loss in iteration 139 : 0.4878325793591795
Loss in iteration 140 : 0.4876428211134504
Loss in iteration 141 : 0.4874553435924913
Loss in iteration 142 : 0.4872701020714469
Loss in iteration 143 : 0.4870870531162344
Loss in iteration 144 : 0.48690615449729807
Loss in iteration 145 : 0.4867273651098191
Loss in iteration 146 : 0.4865506449108927
Loss in iteration 147 : 0.48637595487749147
Loss in iteration 148 : 0.4862032569824941
Loss in iteration 149 : 0.48603251418152127
Loss in iteration 150 : 0.48586369040196237
Loss in iteration 151 : 0.48569675052705513
Loss in iteration 152 : 0.485531660371193
Loss in iteration 153 : 0.48536838664654214
Loss in iteration 154 : 0.48520689692405317
Loss in iteration 155 : 0.4850471595934555
Loss in iteration 156 : 0.48488914382657267
Loss in iteration 157 : 0.48473281954678
Loss in iteration 158 : 0.4845781574052828
Loss in iteration 159 : 0.4844251287629542
Loss in iteration 160 : 0.48427370567527916
Loss in iteration 161 : 0.4841238608777199
Loss in iteration 162 : 0.48397556776943484
Loss in iteration 163 : 0.4838288003944161
Loss in iteration 164 : 0.4836835334202639
Loss in iteration 165 : 0.4835397421157492
Loss in iteration 166 : 0.4833974023286204
Loss in iteration 167 : 0.48325649046497693
Loss in iteration 168 : 0.483116983470994
Loss in iteration 169 : 0.4829788588170847
Loss in iteration 170 : 0.48284209448405774
Loss in iteration 171 : 0.4827066689504061
Loss in iteration 172 : 0.48257256117995984
Loss in iteration 173 : 0.4824397506092226
Loss in iteration 174 : 0.4823082171342072
Loss in iteration 175 : 0.48217794109682954
Loss in iteration 176 : 0.48204890327125316
Loss in iteration 177 : 0.4819210848506153
Loss in iteration 178 : 0.48179446743451
Loss in iteration 179 : 0.48166903301741637
Loss in iteration 180 : 0.48154476397806933
Loss in iteration 181 : 0.4814216430695707
Loss in iteration 182 : 0.4812996534099728
Loss in iteration 183 : 0.48117877847308244
Loss in iteration 184 : 0.4810590020792754
Loss in iteration 185 : 0.48094030838630036
Loss in iteration 186 : 0.4808226818800614
Loss in iteration 187 : 0.48070610736555464
Loss in iteration 188 : 0.4805905699580636
Loss in iteration 189 : 0.48047605507475305
Loss in iteration 190 : 0.4803625484266684
Loss in iteration 191 : 0.48025003601118676
Loss in iteration 192 : 0.48013850410481107
Loss in iteration 193 : 0.48002793925623555
Loss in iteration 194 : 0.4799183282795918
Loss in iteration 195 : 0.47980965824782845
Loss in iteration 196 : 0.47970191648619664
Loss in iteration 197 : 0.47959509056585625
Loss in iteration 198 : 0.4794891682976307
Loss in iteration 199 : 0.47938413772597177
Loss in iteration 200 : 0.47927998712312336
Loss in iteration 201 : 0.4791767049835285
Loss in iteration 202 : 0.47907428001845154
Loss in iteration 203 : 0.47897270115078594
Loss in iteration 204 : 0.47887195751002615
Loss in iteration 205 : 0.4787720384273646
Loss in iteration 206 : 0.4786729334309064
Loss in iteration 207 : 0.47857463224096924
Loss in iteration 208 : 0.4784771247655235
Loss in iteration 209 : 0.47838040109570956
Loss in iteration 210 : 0.4782844515015105
Loss in iteration 211 : 0.4781892664275462
Loss in iteration 212 : 0.47809483648901274
Loss in iteration 213 : 0.47800115246774044
Loss in iteration 214 : 0.4779082053083785
Loss in iteration 215 : 0.4778159861146957
Loss in iteration 216 : 0.47772448614596164
Loss in iteration 217 : 0.4776336968134437
Loss in iteration 218 : 0.4775436096769632
Loss in iteration 219 : 0.47745421644156455
Loss in iteration 220 : 0.4773655089542564
Loss in iteration 221 : 0.4772774792008428
Loss in iteration 222 : 0.47719011930285676
Loss in iteration 223 : 0.47710342151456164
Loss in iteration 224 : 0.4770173782200593
Loss in iteration 225 : 0.4769319819304551
Loss in iteration 226 : 0.4768472252811108
Loss in iteration 227 : 0.47676310102897085
Loss in iteration 228 : 0.476679602049936
Loss in iteration 229 : 0.47659672133632275
Loss in iteration 230 : 0.4765144519943716
Loss in iteration 231 : 0.47643278724181914
Loss in iteration 232 : 0.476351720405548
Loss in iteration 233 : 0.47627124491926964
Loss in iteration 234 : 0.4761913543212961
Loss in iteration 235 : 0.47611204225235154
Loss in iteration 236 : 0.47603330245345077
Loss in iteration 237 : 0.47595512876382173
Loss in iteration 238 : 0.47587751511888715
Loss in iteration 239 : 0.4758004555482944
Loss in iteration 240 : 0.47572394417398844
Loss in iteration 241 : 0.47564797520834307
Loss in iteration 242 : 0.47557254295231915
Loss in iteration 243 : 0.47549764179369075
Loss in iteration 244 : 0.4754232662052999
Loss in iteration 245 : 0.47534941074335607
Loss in iteration 246 : 0.47527607004578437
Loss in iteration 247 : 0.47520323883060506
Loss in iteration 248 : 0.47513091189435613
Loss in iteration 249 : 0.4750590841105542
Loss in iteration 250 : 0.47498775042819014
Loss in iteration 251 : 0.4749169058702568
Loss in iteration 252 : 0.4748465455323195
Loss in iteration 253 : 0.4747766645811088
Loss in iteration 254 : 0.4747072582531536
Loss in iteration 255 : 0.4746383218534482
Loss in iteration 256 : 0.47456985075414443
Loss in iteration 257 : 0.4745018403932736
Loss in iteration 258 : 0.4744342862735083
Loss in iteration 259 : 0.4743671839609409
Loss in iteration 260 : 0.4743005290838954
Loss in iteration 261 : 0.47423431733176863
Loss in iteration 262 : 0.4741685444538866
Loss in iteration 263 : 0.47410320625840996
Loss in iteration 264 : 0.474038298611232
Loss in iteration 265 : 0.4739738174349345
Loss in iteration 266 : 0.4739097587077362
Loss in iteration 267 : 0.4738461184624966
Loss in iteration 268 : 0.47378289278570834
Loss in iteration 269 : 0.4737200778165426
Loss in iteration 270 : 0.47365766974589113
Loss in iteration 271 : 0.4735956648154465
Loss in iteration 272 : 0.4735340593167934
Loss in iteration 273 : 0.47347284959052116
Loss in iteration 274 : 0.47341203202535725
Loss in iteration 275 : 0.4733516030573199
Loss in iteration 276 : 0.473291559168886
Loss in iteration 277 : 0.4732318968881772
Loss in iteration 278 : 0.4731726127881695
Loss in iteration 279 : 0.4731137034859117
Loss in iteration 280 : 0.4730551656417608
Loss in iteration 281 : 0.4729969959586449
Loss in iteration 282 : 0.4729391911813291
Loss in iteration 283 : 0.47288174809569794
Loss in iteration 284 : 0.47282466352806
Loss in iteration 285 : 0.4727679343444687
Loss in iteration 286 : 0.47271155745003535
Loss in iteration 287 : 0.4726555297882943
Loss in iteration 288 : 0.47259984834054375
Loss in iteration 289 : 0.47254451012521853
Loss in iteration 290 : 0.472489512197287
Loss in iteration 291 : 0.47243485164762355
Loss in iteration 292 : 0.47238052560244137
Loss in iteration 293 : 0.4723265312226939
Loss in iteration 294 : 0.47227286570351723
Loss in iteration 295 : 0.4722195262736725
Loss in iteration 296 : 0.4721665101949948
Loss in iteration 297 : 0.4721138147618635
Loss in iteration 298 : 0.4720614373006816
Loss in iteration 299 : 0.4720093751693545
Loss in iteration 300 : 0.47195762575679423
Loss in iteration 301 : 0.471906186482419
Loss in iteration 302 : 0.47185505479567996
Loss in iteration 303 : 0.4718042281755755
Loss in iteration 304 : 0.47175370413019446
Loss in iteration 305 : 0.47170348019626257
Loss in iteration 306 : 0.47165355393868597
Loss in iteration 307 : 0.47160392295012304
Loss in iteration 308 : 0.4715545848505474
Loss in iteration 309 : 0.47150553728683564
Loss in iteration 310 : 0.4714567779323422
Loss in iteration 311 : 0.4714083044865049
Loss in iteration 312 : 0.471360114674444
Loss in iteration 313 : 0.47131220624656817
Loss in iteration 314 : 0.4712645769781956
Loss in iteration 315 : 0.47121722466918003
Loss in iteration 316 : 0.47117014714354155
Loss in iteration 317 : 0.4711233422490982
Loss in iteration 318 : 0.47107680785712636
Loss in iteration 319 : 0.47103054186199556
Loss in iteration 320 : 0.4709845421808418
Loss in iteration 321 : 0.4709388067532197
Loss in iteration 322 : 0.47089333354078494
Loss in iteration 323 : 0.4708481205269644
Loss in iteration 324 : 0.4708031657166372
Loss in iteration 325 : 0.47075846713582986
Loss in iteration 326 : 0.4707140228314078
Loss in iteration 327 : 0.47066983087077485
Loss in iteration 328 : 0.4706258893415759
Loss in iteration 329 : 0.470582196351418
Loss in iteration 330 : 0.4705387500275723
Loss in iteration 331 : 0.4704955485167053
Loss in iteration 332 : 0.47045258998459966
Loss in iteration 333 : 0.47040987261588735
Loss in iteration 334 : 0.47036739461378624
Loss in iteration 335 : 0.4703251541998369
Loss in iteration 336 : 0.4702831496136512
Loss in iteration 337 : 0.47024137911266084
Loss in iteration 338 : 0.47019984097186923
Loss in iteration 339 : 0.47015853348361025
Loss in iteration 340 : 0.47011745495731483
Loss in iteration 341 : 0.4700766037192699
Loss in iteration 342 : 0.47003597811239595
Loss in iteration 343 : 0.469995576496018
Loss in iteration 344 : 0.46995539724564406
Loss in iteration 345 : 0.469915438752747
Loss in iteration 346 : 0.46987569942455254
Loss in iteration 347 : 0.4698361776838284
Loss in iteration 348 : 0.4697968719686712
Loss in iteration 349 : 0.46975778073231733
Loss in iteration 350 : 0.469718902442926
Loss in iteration 351 : 0.46968023558339556
Loss in iteration 352 : 0.4696417786511615
Loss in iteration 353 : 0.46960353015801587
Loss in iteration 354 : 0.4695654886299068
Loss in iteration 355 : 0.4695276526067734
Loss in iteration 356 : 0.46949002064234524
Loss in iteration 357 : 0.4694525913039788
Loss in iteration 358 : 0.46941536317247573
Loss in iteration 359 : 0.4693783348419132
Loss in iteration 360 : 0.4693415049194745
Loss in iteration 361 : 0.4693048720252843
Loss in iteration 362 : 0.46926843479224306
Loss in iteration 363 : 0.4692321918658628
Loss in iteration 364 : 0.4691961419041162
Loss in iteration 365 : 0.46916028357727446
Loss in iteration 366 : 0.4691246155677601
Loss in iteration 367 : 0.4690891365699874
Loss in iteration 368 : 0.4690538452902216
Loss in iteration 369 : 0.46901874044642966
Loss in iteration 370 : 0.4689838207681357
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.78575, training accuracy 0.78575, time elapsed: 7477 millisecond.
