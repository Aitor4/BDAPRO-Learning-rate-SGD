objc[2889]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1077094c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10778d4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/03/02 13:14:09 INFO SparkContext: Running Spark version 2.0.0
18/03/02 13:14:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/02 13:14:11 INFO SecurityManager: Changing view acls to: Aitor
18/03/02 13:14:11 INFO SecurityManager: Changing modify acls to: Aitor
18/03/02 13:14:11 INFO SecurityManager: Changing view acls groups to: 
18/03/02 13:14:11 INFO SecurityManager: Changing modify acls groups to: 
18/03/02 13:14:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/03/02 13:14:12 INFO Utils: Successfully started service 'sparkDriver' on port 52628.
18/03/02 13:14:12 INFO SparkEnv: Registering MapOutputTracker
18/03/02 13:14:12 INFO SparkEnv: Registering BlockManagerMaster
18/03/02 13:14:13 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-4714f16a-a2c7-4a58-94f8-81c7c28a789f
18/03/02 13:14:13 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/03/02 13:14:13 INFO SparkEnv: Registering OutputCommitCoordinator
18/03/02 13:14:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/03/02 13:14:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/03/02 13:14:14 INFO Executor: Starting executor ID driver on host localhost
18/03/02 13:14:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52631.
18/03/02 13:14:14 INFO NettyBlockTransferService: Server created on 192.168.2.140:52631
18/03/02 13:14:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 52631)
18/03/02 13:14:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:52631 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 52631)
18/03/02 13:14:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 52631)
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 2.141549710106487
Loss in iteration 3 : 22.85361191192403
Loss in iteration 4 : 7.213353416805543
Loss in iteration 5 : 16.31191673601092
Loss in iteration 6 : 11.44216580852446
Loss in iteration 7 : 9.853829878915429
Loss in iteration 8 : 14.822841816914094
Loss in iteration 9 : 5.208611633251857
Loss in iteration 10 : 14.418368881409107
Loss in iteration 11 : 5.027456977489385
Loss in iteration 12 : 13.161213417949355
Loss in iteration 13 : 5.7313757847536895
Loss in iteration 14 : 12.185442371765932
Loss in iteration 15 : 6.159044453681338
Loss in iteration 16 : 11.112710431471646
Loss in iteration 17 : 6.63699721121109
Loss in iteration 18 : 10.102649849043807
Loss in iteration 19 : 6.967889281158184
Loss in iteration 20 : 9.149213153490674
Loss in iteration 21 : 7.157181689747249
Loss in iteration 22 : 8.298333441163846
Loss in iteration 23 : 7.163533922187184
Loss in iteration 24 : 7.592512334522438
Loss in iteration 25 : 7.024069640371653
Loss in iteration 26 : 7.001492905922758
Loss in iteration 27 : 6.835819726364151
Loss in iteration 28 : 6.460638912855642
Loss in iteration 29 : 6.638765314460323
Loss in iteration 30 : 6.033601126197935
Loss in iteration 31 : 6.4398244301918615
Loss in iteration 32 : 5.710873207592276
Loss in iteration 33 : 6.2453509395436155
Loss in iteration 34 : 5.443091679953752
Loss in iteration 35 : 6.059407581220304
Loss in iteration 36 : 5.200301852161033
Loss in iteration 37 : 5.885788329523038
Loss in iteration 38 : 5.004616699027926
Loss in iteration 39 : 5.7368066821235075
Loss in iteration 40 : 4.851267538800054
Loss in iteration 41 : 5.613576091966804
Loss in iteration 42 : 4.729931568486463
Loss in iteration 43 : 5.512083071690397
Loss in iteration 44 : 4.630276631813023
Loss in iteration 45 : 5.427013934654451
Loss in iteration 46 : 4.545162113838046
Loss in iteration 47 : 5.353566656837555
Loss in iteration 48 : 4.469785388997432
Loss in iteration 49 : 5.2881638570347596
Loss in iteration 50 : 4.401410122187842
Loss in iteration 51 : 5.229022865600883
Loss in iteration 52 : 4.338588314309601
Loss in iteration 53 : 5.174854409074386
Loss in iteration 54 : 4.280386792480607
Loss in iteration 55 : 5.124470984959158
Loss in iteration 56 : 4.2262262790317475
Loss in iteration 57 : 5.077069142260419
Loss in iteration 58 : 4.175914895832695
Loss in iteration 59 : 5.032397573236796
Loss in iteration 60 : 4.129519093742628
Loss in iteration 61 : 4.9905743926272494
Loss in iteration 62 : 4.087138671940436
Loss in iteration 63 : 4.9520648480287175
Loss in iteration 64 : 4.048840182490217
Loss in iteration 65 : 4.916261251653176
Loss in iteration 66 : 4.014409499607037
Loss in iteration 67 : 4.874381138066344
Loss in iteration 68 : 3.9817692881923556
Loss in iteration 69 : 4.83448277459639
Loss in iteration 70 : 3.9528729896449293
Loss in iteration 71 : 4.802269142623692
Loss in iteration 72 : 3.928628877456005
Loss in iteration 73 : 4.773430204168936
Loss in iteration 74 : 3.9075326140819673
Loss in iteration 75 : 4.747192746517277
Loss in iteration 76 : 3.8890180694357106
Loss in iteration 77 : 4.723103048137378
Loss in iteration 78 : 3.8726655838955986
Loss in iteration 79 : 4.700407225364934
Loss in iteration 80 : 3.8580813833123844
Loss in iteration 81 : 4.678727245758647
Loss in iteration 82 : 3.8449907952584836
Loss in iteration 83 : 4.658333625544989
Loss in iteration 84 : 3.8332396266691995
Loss in iteration 85 : 4.639642696867713
Loss in iteration 86 : 3.8227048524843505
Loss in iteration 87 : 4.622707303622449
Loss in iteration 88 : 3.8132326192603223
Loss in iteration 89 : 4.607271357472222
Loss in iteration 90 : 3.804653931589657
Loss in iteration 91 : 4.593003643782058
Loss in iteration 92 : 3.796815037685731
Loss in iteration 93 : 4.579613750471652
Loss in iteration 94 : 3.7895880692322117
Loss in iteration 95 : 4.566871186531451
Loss in iteration 96 : 3.7828689975756142
Loss in iteration 97 : 4.554595405577436
Loss in iteration 98 : 3.776572791462354
Loss in iteration 99 : 4.542647400503683
Loss in iteration 100 : 3.7706296498440635
Loss in iteration 101 : 4.530928520818161
Loss in iteration 102 : 3.7649828747257943
Loss in iteration 103 : 4.519380758973126
Loss in iteration 104 : 3.7595876083399964
Loss in iteration 105 : 4.507981275665571
Loss in iteration 106 : 3.7544094130874366
Loss in iteration 107 : 4.496731860714981
Loss in iteration 108 : 3.7494225376554597
Loss in iteration 109 : 4.485651338605161
Loss in iteration 110 : 3.74460872698373
Loss in iteration 111 : 4.47477442362361
Loss in iteration 112 : 3.739957026183863
Loss in iteration 113 : 4.464150638117983
Loss in iteration 114 : 3.735463585827825
Loss in iteration 115 : 4.453836230867919
Loss in iteration 116 : 3.7311300825786473
Loss in iteration 117 : 4.443880688490426
Loss in iteration 118 : 3.7269606722615047
Loss in iteration 119 : 4.434315458348576
Loss in iteration 120 : 3.7229587921521374
Loss in iteration 121 : 4.425150136896229
Loss in iteration 122 : 3.7191251801056024
Loss in iteration 123 : 4.416375643569705
Loss in iteration 124 : 3.7154575145051867
Loss in iteration 125 : 4.407970755457085
Loss in iteration 126 : 3.711951210870759
Loss in iteration 127 : 4.399908642092709
Loss in iteration 128 : 3.7086006449495286
Loss in iteration 129 : 4.392161674376894
Loss in iteration 130 : 3.7054002371300285
Loss in iteration 131 : 4.384704178381812
Loss in iteration 132 : 3.7023451152271383
Loss in iteration 133 : 4.377513519395153
Loss in iteration 134 : 3.6994313007040436
Loss in iteration 135 : 4.370570095459722
Loss in iteration 136 : 3.696655501401559
Loss in iteration 137 : 4.363856757756021
Loss in iteration 138 : 3.694014660123165
Loss in iteration 139 : 4.357358037644813
Loss in iteration 140 : 3.6915054248646393
Loss in iteration 141 : 4.351059427891871
Loss in iteration 142 : 3.6891236862678984
Loss in iteration 143 : 4.344946858132718
Loss in iteration 144 : 3.686864281360451
Loss in iteration 145 : 4.339006419114514
Loss in iteration 146 : 3.6847209036573085
Loss in iteration 147 : 4.3332243237615256
Loss in iteration 148 : 3.6826862048130886
Loss in iteration 149 : 4.327587047880171
Loss in iteration 150 : 3.6807520355953165
Loss in iteration 151 : 4.32208157221889
Loss in iteration 152 : 3.6789097592329996
Loss in iteration 153 : 4.316695649062869
Loss in iteration 154 : 3.6771505749916784
Loss in iteration 155 : 4.311418033798199
Loss in iteration 156 : 3.6754658062230328
Loss in iteration 157 : 4.306238645869299
Loss in iteration 158 : 3.6738471269411
Loss in iteration 159 : 4.3011486465161
Loss in iteration 160 : 3.6722867184940697
Loss in iteration 161 : 4.296140438085904
Loss in iteration 162 : 3.670777360526268
Loss in iteration 163 : 4.291207600256289
Loss in iteration 164 : 3.669312467870629
Loss in iteration 165 : 4.286344782975208
Loss in iteration 166 : 3.6678860881866697
Loss in iteration 167 : 4.2815475759943045
Loss in iteration 168 : 3.666492875302524
Loss in iteration 169 : 4.2768123722457485
Loss in iteration 170 : 3.665128051446496
Loss in iteration 171 : 4.272136238343607
Loss in iteration 172 : 3.663787368705065
Loss in iteration 173 : 4.267516801100019
Loss in iteration 174 : 3.6624670767063234
Loss in iteration 175 : 4.262952154707882
Loss in iteration 176 : 3.6611639001111964
Loss in iteration 177 : 4.258440789492559
Loss in iteration 178 : 3.659875026306523
Loss in iteration 179 : 4.253981540076023
Loss in iteration 180 : 3.65859810099108
Loss in iteration 181 : 4.249573548557094
Loss in iteration 182 : 3.6573312273540783
Loss in iteration 183 : 4.245216236976742
Loss in iteration 184 : 3.656072963451222
Loss in iteration 185 : 4.240909282953666
Loss in iteration 186 : 3.6548223122986743
Loss in iteration 187 : 4.236652592923431
Loss in iteration 188 : 3.6535787001259123
Loss in iteration 189 : 4.23244626878411
Loss in iteration 190 : 3.6523419399978865
Loss in iteration 191 : 4.2282905657204
Loss in iteration 192 : 3.6511121803217628
Loss in iteration 193 : 4.2241858412179525
Loss in iteration 194 : 3.649889840166032
Loss in iteration 195 : 4.220132497401896
Loss in iteration 196 : 3.648675535383163
Loss in iteration 197 : 4.216130920463316
Loss in iteration 198 : 3.647470000862175
Loss in iteration 199 : 4.2121814218026215
Loss in iteration 200 : 3.646274014640818
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.7265, training accuracy 0.7165, time elapsed: 11850 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 1.5445767658443816
Loss in iteration 3 : 14.976574100308971
Loss in iteration 4 : 5.9131209274774
Loss in iteration 5 : 10.391293331019439
Loss in iteration 6 : 8.877626532625069
Loss in iteration 7 : 5.9169367223938485
Loss in iteration 8 : 10.872142003493886
Loss in iteration 9 : 3.1978435354988077
Loss in iteration 10 : 10.012385159536606
Loss in iteration 11 : 3.6505761133236447
Loss in iteration 12 : 9.390779742550452
Loss in iteration 13 : 3.886079942179213
Loss in iteration 14 : 8.599851142524573
Loss in iteration 15 : 4.286850914166155
Loss in iteration 16 : 7.890998143262185
Loss in iteration 17 : 4.577237825248424
Loss in iteration 18 : 7.1740600643498205
Loss in iteration 19 : 4.819044478524902
Loss in iteration 20 : 6.507773728571612
Loss in iteration 21 : 4.953327112631843
Loss in iteration 22 : 5.9081019322968675
Loss in iteration 23 : 4.973404411267025
Loss in iteration 24 : 5.398866955426437
Loss in iteration 25 : 4.895580400853663
Loss in iteration 26 : 4.972469618724161
Loss in iteration 27 : 4.7726516297399035
Loss in iteration 28 : 4.601967088588735
Loss in iteration 29 : 4.636315122418402
Loss in iteration 30 : 4.299755494125264
Loss in iteration 31 : 4.4955886215515015
Loss in iteration 32 : 4.059916448554173
Loss in iteration 33 : 4.358240682614666
Loss in iteration 34 : 3.8609313082420256
Loss in iteration 35 : 4.229221189274461
Loss in iteration 36 : 3.691053477174865
Loss in iteration 37 : 4.111507789870327
Loss in iteration 38 : 3.5529460518408946
Loss in iteration 39 : 4.0090756905472045
Loss in iteration 40 : 3.4421653604228624
Loss in iteration 41 : 3.922249259773955
Loss in iteration 42 : 3.3521483256497246
Loss in iteration 43 : 3.8489485671358588
Loss in iteration 44 : 3.2772719610089216
Loss in iteration 45 : 3.786524401878397
Loss in iteration 46 : 3.2133331338582565
Loss in iteration 47 : 3.732439579287051
Loss in iteration 48 : 3.157366383364548
Loss in iteration 49 : 3.6846836549746067
Loss in iteration 50 : 3.1074080885509665
Loss in iteration 51 : 3.641895346824004
Loss in iteration 52 : 3.062229934086237
Loss in iteration 53 : 3.603106562863255
Loss in iteration 54 : 3.021080133288857
Loss in iteration 55 : 3.567573564734731
Loss in iteration 56 : 2.9834881267517113
Loss in iteration 57 : 3.5347484851754962
Loss in iteration 58 : 2.9491386295071096
Loss in iteration 59 : 3.504269981406665
Loss in iteration 60 : 2.9177945945535315
Loss in iteration 61 : 3.4759403047605275
Loss in iteration 62 : 2.8892580539532373
Loss in iteration 63 : 3.4495604017963695
Loss in iteration 64 : 2.8633329445356788
Loss in iteration 65 : 3.4240125355585094
Loss in iteration 66 : 2.8396645043461133
Loss in iteration 67 : 3.3955514318517355
Loss in iteration 68 : 2.817364804562565
Loss in iteration 69 : 3.367785760083262
Loss in iteration 70 : 2.7971720157548594
Loss in iteration 71 : 3.3447510512835534
Loss in iteration 72 : 2.7797842928249406
Loss in iteration 73 : 3.3245131629900486
Loss in iteration 74 : 2.7645050905331003
Loss in iteration 75 : 3.3059869184731943
Loss in iteration 76 : 2.750888433995084
Loss in iteration 77 : 3.2887843011656126
Loss in iteration 78 : 2.7386812268418437
Loss in iteration 79 : 3.2726829323269127
Loss in iteration 80 : 2.7276919056105635
Loss in iteration 81 : 3.2575598204532707
Loss in iteration 82 : 2.717767928569782
Loss in iteration 83 : 3.2434000121463695
Loss in iteration 84 : 2.7087914022441617
Loss in iteration 85 : 3.230230068013832
Loss in iteration 86 : 2.700663519347953
Loss in iteration 87 : 3.218034175800672
Loss in iteration 88 : 2.69328875554166
Loss in iteration 89 : 3.2067284652373234
Loss in iteration 90 : 2.68657075450712
Loss in iteration 91 : 3.196186182236743
Loss in iteration 92 : 2.6804169351247977
Loss in iteration 93 : 3.186271660779755
Loss in iteration 94 : 2.6747438674159207
Loss in iteration 95 : 3.1768618889829674
Loss in iteration 96 : 2.6694799101656526
Loss in iteration 97 : 3.167855573853445
Loss in iteration 98 : 2.664565459204894
Loss in iteration 99 : 3.1591751704162827
Loss in iteration 100 : 2.6599520111903727
Loss in iteration 101 : 3.1507656998472133
Loss in iteration 102 : 2.6556008223179677
Loss in iteration 103 : 3.1425920443221487
Loss in iteration 104 : 2.6514814811301313
Loss in iteration 105 : 3.1346354555595077
Loss in iteration 106 : 2.6475705093246815
Loss in iteration 107 : 3.1268897902822834
Loss in iteration 108 : 2.6438500532156515
Loss in iteration 109 : 3.119357825159238
Loss in iteration 110 : 2.6403066954087206
Loss in iteration 111 : 3.1120477612828124
Loss in iteration 112 : 2.636930368293158
Loss in iteration 113 : 3.1049699284825003
Loss in iteration 114 : 2.633713335473925
Loss in iteration 115 : 3.0981338351523515
Loss in iteration 116 : 2.6306492495172242
Loss in iteration 117 : 3.0915458764160957
Loss in iteration 118 : 2.627732351742129
Loss in iteration 119 : 3.0852079864729673
Loss in iteration 120 : 2.624956893630578
Loss in iteration 121 : 3.0791172976634473
Loss in iteration 122 : 2.622316817431811
Loss in iteration 123 : 3.073266615408932
Loss in iteration 124 : 2.6198056698886427
Loss in iteration 125 : 3.0676453786077964
Loss in iteration 126 : 2.6174166773171135
Loss in iteration 127 : 3.0622407774817857
Loss in iteration 128 : 2.6151428985928944
Loss in iteration 129 : 3.0570387916000907
Loss in iteration 130 : 2.612977388014854
Loss in iteration 131 : 3.0520250228671344
Loss in iteration 132 : 2.6109133269886726
Loss in iteration 133 : 3.047185289421767
Loss in iteration 134 : 2.608944109173064
Loss in iteration 135 : 3.0425060028691577
Loss in iteration 136 : 2.607063381746663
Loss in iteration 137 : 3.0379743766332146
Loss in iteration 138 : 2.6052650546824934
Loss in iteration 139 : 3.033578517365704
Loss in iteration 140 : 2.603543292146809
Loss in iteration 141 : 3.029307444038837
Loss in iteration 142 : 2.601892498061512
Loss in iteration 143 : 3.025151067765975
Loss in iteration 144 : 2.600307303935043
Loss in iteration 145 : 3.0211001538886193
Loss in iteration 146 : 2.598782563007073
Loss in iteration 147 : 3.0171462785409666
Loss in iteration 148 : 2.5973133515260134
Loss in iteration 149 : 3.0132817853696534
Loss in iteration 150 : 2.5958949759165235
Loss in iteration 151 : 3.009499744122221
Loss in iteration 152 : 2.5945229836239383
Loss in iteration 153 : 3.005793910828788
Loss in iteration 154 : 2.593193175269732
Loss in iteration 155 : 3.002158688623255
Loss in iteration 156 : 2.5919016160969375
Loss in iteration 157 : 2.9985890883093047
Loss in iteration 158 : 2.590644645248087
Loss in iteration 159 : 2.9950806881517185
Loss in iteration 160 : 2.589418882004135
Loss in iteration 161 : 2.991629592799737
Loss in iteration 162 : 2.5882212286067543
Loss in iteration 163 : 2.9882323915889333
Loss in iteration 164 : 2.587048869641174
Loss in iteration 165 : 2.984886116671277
Loss in iteration 166 : 2.585899268171791
Loss in iteration 167 : 2.9815882014963524
Loss in iteration 168 : 2.5847701589231993
Loss in iteration 169 : 2.9783364401404375
Loss in iteration 170 : 2.583659538820532
Loss in iteration 171 : 2.9751289478948886
Loss in iteration 172 : 2.582565655182213
Loss in iteration 173 : 2.971964123417554
Loss in iteration 174 : 2.581486991825218
Loss in iteration 175 : 2.968840612649543
Loss in iteration 176 : 2.5804222533205796
Loss in iteration 177 : 2.965757274622372
Loss in iteration 178 : 2.579370347636822
Loss in iteration 179 : 2.962713149235431
Loss in iteration 180 : 2.5783303674347993
Loss in iteration 181 : 2.959707427070259
Loss in iteration 182 : 2.5773015703242375
Loss in iteration 183 : 2.9567394213188365
Loss in iteration 184 : 2.5762833584500227
Loss in iteration 185 : 2.9538085419272435
Loss in iteration 186 : 2.575275257832757
Loss in iteration 187 : 2.9509142720814707
Loss in iteration 188 : 2.574276897929507
Loss in iteration 189 : 2.948056147177212
Loss in iteration 190 : 2.573287991897645
Loss in iteration 191 : 2.945233736412817
Loss in iteration 192 : 2.572308318030295
Loss in iteration 193 : 2.9424466271193834
Loss in iteration 194 : 2.5713377027849
Loss in iteration 195 : 2.939694411895208
Loss in iteration 196 : 2.5703760057506964
Loss in iteration 197 : 2.9369766785475653
Loss in iteration 198 : 2.569423106803416
Loss in iteration 199 : 2.934293002769628
Loss in iteration 200 : 2.5684788955864
Testing accuracy  of updater 0 on alg 0 with rate 7.0 = 0.726, training accuracy 0.7165, time elapsed: 6524 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 1.0091931589813257
Loss in iteration 3 : 6.845865820119179
Loss in iteration 4 : 4.811709419838326
Loss in iteration 5 : 4.22599186001762
Loss in iteration 6 : 6.451963321816758
Loss in iteration 7 : 1.9252729828140656
Loss in iteration 8 : 6.310173133290415
Loss in iteration 9 : 1.8015716300380227
Loss in iteration 10 : 5.737364273089088
Loss in iteration 11 : 2.1332355913561
Loss in iteration 12 : 5.458823567179196
Loss in iteration 13 : 2.1822420818026913
Loss in iteration 14 : 4.955491518217074
Loss in iteration 15 : 2.4437092500807562
Loss in iteration 16 : 4.583116197931904
Loss in iteration 17 : 2.563985609852499
Loss in iteration 18 : 4.169307997210945
Loss in iteration 19 : 2.695465965210386
Loss in iteration 20 : 3.8008032949625936
Loss in iteration 21 : 2.7598190480303715
Loss in iteration 22 : 3.4654573180413344
Loss in iteration 23 : 2.7738050051615155
Loss in iteration 24 : 3.1751951074156666
Loss in iteration 25 : 2.742295076799049
Loss in iteration 26 : 2.9283685529998587
Loss in iteration 27 : 2.682461192963724
Loss in iteration 28 : 2.7202019711393706
Loss in iteration 29 : 2.608853372776411
Loss in iteration 30 : 2.5469311304521143
Loss in iteration 31 : 2.5306876822859623
Loss in iteration 32 : 2.4032473223883266
Loss in iteration 33 : 2.454088792647802
Loss in iteration 34 : 2.2840905546984303
Loss in iteration 35 : 2.382691624453143
Loss in iteration 36 : 2.1856709690437297
Loss in iteration 37 : 2.3183970901006643
Loss in iteration 38 : 2.10472368061987
Loss in iteration 39 : 2.261864680722551
Loss in iteration 40 : 2.037860032292578
Loss in iteration 41 : 2.2127549847135315
Loss in iteration 42 : 1.9819864388125639
Loss in iteration 43 : 2.1701739618317344
Loss in iteration 44 : 1.9346259701543829
Loss in iteration 45 : 2.1330901959538258
Loss in iteration 46 : 1.893913015579278
Loss in iteration 47 : 2.100552598991912
Loss in iteration 48 : 1.858478892316067
Loss in iteration 49 : 2.071765324567226
Loss in iteration 50 : 1.8273271614701307
Loss in iteration 51 : 2.0460862351649123
Loss in iteration 52 : 1.7997241375851571
Loss in iteration 53 : 2.022997222178264
Loss in iteration 54 : 1.7751145734484255
Loss in iteration 55 : 2.002076197307584
Loss in iteration 56 : 1.7530644465495775
Loss in iteration 57 : 1.9829764367402258
Loss in iteration 58 : 1.7332249591693096
Loss in iteration 59 : 1.9654020460975734
Loss in iteration 60 : 1.715308204831726
Loss in iteration 61 : 1.9490540206413143
Loss in iteration 62 : 1.6990637839288694
Loss in iteration 63 : 1.9335026960354376
Loss in iteration 64 : 1.6842429116295417
Loss in iteration 65 : 1.9180712126301906
Loss in iteration 66 : 1.670563618680571
Loss in iteration 67 : 1.902500966457058
Loss in iteration 68 : 1.6578685959983512
Loss in iteration 69 : 1.8878590149381276
Loss in iteration 70 : 1.64635742666794
Loss in iteration 71 : 1.8749255105769862
Loss in iteration 72 : 1.6361431558328852
Loss in iteration 73 : 1.8634943903020844
Loss in iteration 74 : 1.6270636966327372
Loss in iteration 75 : 1.8531641246146902
Loss in iteration 76 : 1.6189132724666737
Loss in iteration 77 : 1.843662548237604
Loss in iteration 78 : 1.6115367717162639
Loss in iteration 79 : 1.834834891107597
Loss in iteration 80 : 1.6048244416699393
Loss in iteration 81 : 1.8265903912990056
Loss in iteration 82 : 1.5986942012988405
Loss in iteration 83 : 1.8188692375973319
Loss in iteration 84 : 1.5930802843039444
Loss in iteration 85 : 1.8116257497888775
Loss in iteration 86 : 1.5879270998971373
Loss in iteration 87 : 1.8048200213076508
Loss in iteration 88 : 1.583185921670562
Loss in iteration 89 : 1.7984141997994931
Loss in iteration 90 : 1.5788131527286087
Loss in iteration 91 : 1.7923715273461944
Loss in iteration 92 : 1.5747695225917018
Loss in iteration 93 : 1.786656832736258
Loss in iteration 94 : 1.571019779265624
Loss in iteration 95 : 1.7812374916116211
Loss in iteration 96 : 1.5675325621360352
Loss in iteration 97 : 1.776084249239852
Loss in iteration 98 : 1.5642802673074867
Loss in iteration 99 : 1.7711716514872733
Loss in iteration 100 : 1.5612388270548323
Loss in iteration 101 : 1.7664780639736473
Loss in iteration 102 : 1.5583873969606572
Loss in iteration 103 : 1.7619853725247365
Loss in iteration 104 : 1.5557079786086538
Loss in iteration 105 : 1.7576784888009307
Loss in iteration 106 : 1.5531850148087136
Loss in iteration 107 : 1.753544773742507
Loss in iteration 108 : 1.550804990646516
Loss in iteration 109 : 1.7495734646369774
Loss in iteration 110 : 1.5485560652140011
Loss in iteration 111 : 1.7457551625707286
Loss in iteration 112 : 1.5464277497391876
Loss in iteration 113 : 1.7420814113668168
Loss in iteration 114 : 1.5444106396789394
Loss in iteration 115 : 1.7385443787494756
Loss in iteration 116 : 1.5424962017651394
Loss in iteration 117 : 1.7351366358074252
Loss in iteration 118 : 1.5406766121725832
Loss in iteration 119 : 1.731851021553131
Loss in iteration 120 : 1.5389446388843315
Loss in iteration 121 : 1.7286805747644134
Loss in iteration 122 : 1.537293559805781
Loss in iteration 123 : 1.7256185143291083
Loss in iteration 124 : 1.535717107941397
Loss in iteration 125 : 1.7226582508504547
Loss in iteration 126 : 1.5342094356419609
Loss in iteration 127 : 1.7197934152255376
Loss in iteration 128 : 1.5327650911831179
Loss in iteration 129 : 1.7170178933906028
Loss in iteration 130 : 1.5313790024214058
Loss in iteration 131 : 1.714325859806372
Loss in iteration 132 : 1.5300464637422293
Loss in iteration 133 : 1.7117118051627613
Loss in iteration 134 : 1.5287631238078478
Loss in iteration 135 : 1.7091705560570116
Loss in iteration 136 : 1.5275249726544198
Loss in iteration 137 : 1.7066972860309042
Loss in iteration 138 : 1.5263283274597625
Loss in iteration 139 : 1.7042875184168802
Loss in iteration 140 : 1.5251698168302834
Loss in iteration 141 : 1.7019371220536714
Loss in iteration 142 : 1.5240463637800048
Loss in iteration 143 : 1.6996423012087096
Loss in iteration 144 : 1.522955167745235
Loss in iteration 145 : 1.6973995810951665
Loss in iteration 146 : 1.521893686041635
Loss in iteration 147 : 1.6952057902823776
Loss in iteration 148 : 1.5208596151643716
Loss in iteration 149 : 1.6930580411346197
Loss in iteration 150 : 1.5198508722865285
Loss in iteration 151 : 1.6909537092198104
Loss in iteration 152 : 1.5188655772473407
Loss in iteration 153 : 1.6888904124350845
Loss in iteration 154 : 1.5179020352532888
Loss in iteration 155 : 1.6868659904175365
Loss in iteration 156 : 1.5169587204511412
Loss in iteration 157 : 1.6848784846533094
Loss in iteration 158 : 1.516034260475687
Loss in iteration 159 : 1.6829261195698006
Loss in iteration 160 : 1.5151274220288877
Loss in iteration 161 : 1.6810072847925723
Loss in iteration 162 : 1.5142370975108121
Loss in iteration 163 : 1.6791205186687237
Loss in iteration 164 : 1.5133622926952415
Loss in iteration 165 : 1.6772644930979095
Loss in iteration 166 : 1.5125021154230593
Loss in iteration 167 : 1.6754379996679603
Loss in iteration 168 : 1.5116557652726905
Loss in iteration 169 : 1.6736399370605572
Loss in iteration 170 : 1.5108225241579003
Loss in iteration 171 : 1.6718692996708797
Loss in iteration 172 : 1.5100017477977956
Loss in iteration 173 : 1.6701251673713158
Loss in iteration 174 : 1.5091928580011575
Loss in iteration 175 : 1.6684066963410786
Loss in iteration 176 : 1.5083953357062236
Loss in iteration 177 : 1.666713110879579
Loss in iteration 178 : 1.5076087147175352
Loss in iteration 179 : 1.6650436961204518
Loss in iteration 180 : 1.506832576082695
Loss in iteration 181 : 1.6633977915643308
Loss in iteration 182 : 1.50606654305396
Loss in iteration 183 : 1.6617747853510905
Loss in iteration 184 : 1.505310276581787
Loss in iteration 185 : 1.6601741091961293
Loss in iteration 186 : 1.504563471290217
Loss in iteration 187 : 1.6585952339195877
Loss in iteration 188 : 1.5038258518868222
Loss in iteration 189 : 1.6570376655022572
Loss in iteration 190 : 1.5030971699628528
Loss in iteration 191 : 1.655500941607102
Loss in iteration 192 : 1.5023772011423713
Loss in iteration 193 : 1.653984628510322
Loss in iteration 194 : 1.5016657425421804
Loss in iteration 195 : 1.6524883183912005
Loss in iteration 196 : 1.5009626105075766
Loss in iteration 197 : 1.651011626934959
Loss in iteration 198 : 1.5002676385919482
Loss in iteration 199 : 1.6495541912077416
Loss in iteration 200 : 1.4995806757513963
Testing accuracy  of updater 0 on alg 0 with rate 4.0 = 0.7275, training accuracy 0.718375, time elapsed: 5886 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6826496441898317
Loss in iteration 3 : 0.7251674502731246
Loss in iteration 4 : 0.9043540796081143
Loss in iteration 5 : 0.9965633061226776
Loss in iteration 6 : 1.2081521260774446
Loss in iteration 7 : 0.8731585215684701
Loss in iteration 8 : 1.073014754531772
Loss in iteration 9 : 0.8494648798849442
Loss in iteration 10 : 1.0034789436340183
Loss in iteration 11 : 0.8103927207765962
Loss in iteration 12 : 0.9262320596095244
Loss in iteration 13 : 0.7768120276425861
Loss in iteration 14 : 0.8607201840207791
Loss in iteration 15 : 0.74437776982937
Loss in iteration 16 : 0.8036105906410157
Loss in iteration 17 : 0.7136515407463082
Loss in iteration 18 : 0.7544269284821324
Loss in iteration 19 : 0.684960693473743
Loss in iteration 20 : 0.7122413323717429
Loss in iteration 21 : 0.6585618776820297
Loss in iteration 22 : 0.6761260965331398
Loss in iteration 23 : 0.6345720640813589
Loss in iteration 24 : 0.645225003876382
Loss in iteration 25 : 0.6129825721249873
Loss in iteration 26 : 0.6187857076040143
Loss in iteration 27 : 0.5936977934166691
Loss in iteration 28 : 0.5961615356064666
Loss in iteration 29 : 0.5765719973701612
Loss in iteration 30 : 0.576803147805839
Loss in iteration 31 : 0.5614411483053005
Loss in iteration 32 : 0.5602505572244721
Loss in iteration 33 : 0.5481516058199751
Loss in iteration 34 : 0.54612936259145
Loss in iteration 35 : 0.5365757609816606
Loss in iteration 36 : 0.5341429191740227
Loss in iteration 37 : 0.5266014870814502
Loss in iteration 38 : 0.5240483432437416
Loss in iteration 39 : 0.5181100333716709
Loss in iteration 40 : 0.5156273972185369
Loss in iteration 41 : 0.5109670373527164
Loss in iteration 42 : 0.5086719651262649
Loss in iteration 43 : 0.5050273743713418
Loss in iteration 44 : 0.5029833242899197
Loss in iteration 45 : 0.5001430085720839
Loss in iteration 46 : 0.49837509626747994
Loss in iteration 47 : 0.49616863274259104
Loss in iteration 48 : 0.4946751998761723
Loss in iteration 49 : 0.4929651303815787
Loss in iteration 50 : 0.4917267397637676
Loss in iteration 51 : 0.49040204575618435
Loss in iteration 52 : 0.4893886385832377
Loss in iteration 53 : 0.4883597570458246
Loss in iteration 54 : 0.4875363403764554
Loss in iteration 55 : 0.48673143632844507
Loss in iteration 56 : 0.4860624333064276
Loss in iteration 57 : 0.4854245479649704
Loss in iteration 58 : 0.48487685970730665
Loss in iteration 59 : 0.4843615883623378
Loss in iteration 60 : 0.4839064482816853
Loss in iteration 61 : 0.48347990693093446
Loss in iteration 62 : 0.4830936928638413
Loss in iteration 63 : 0.4827306520284418
Loss in iteration 64 : 0.4823948985864853
Loss in iteration 65 : 0.48207705781698873
Loss in iteration 66 : 0.481777944023318
Loss in iteration 67 : 0.48149237392771793
Loss in iteration 68 : 0.4812199423273428
Loss in iteration 69 : 0.48095773431227723
Loss in iteration 70 : 0.4807050421351246
Loss in iteration 71 : 0.4804601928845779
Loss in iteration 72 : 0.48022252631115747
Loss in iteration 73 : 0.4799910603026138
Loss in iteration 74 : 0.47976527878439235
Loss in iteration 75 : 0.4795445900154898
Loss in iteration 76 : 0.4793286196210815
Loss in iteration 77 : 0.4791169993370014
Loss in iteration 78 : 0.4789094645911824
Loss in iteration 79 : 0.47870577621154803
Loss in iteration 80 : 0.47850574595656575
Loss in iteration 81 : 0.4783092097067881
Loss in iteration 82 : 0.47811602922303953
Loss in iteration 83 : 0.4779260843467565
Loss in iteration 84 : 0.47773926860758414
Loss in iteration 85 : 0.47755548795291963
Loss in iteration 86 : 0.4773746559551485
Loss in iteration 87 : 0.47719669447855595
Loss in iteration 88 : 0.47702152995001107
Loss in iteration 89 : 0.4768490943783641
Loss in iteration 90 : 0.4766793227711588
Loss in iteration 91 : 0.4765121540244471
Loss in iteration 92 : 0.476347529223618
Loss in iteration 93 : 0.47618539230456486
Loss in iteration 94 : 0.4760256889616755
Loss in iteration 95 : 0.4758683671010295
Loss in iteration 96 : 0.4757133761436958
Loss in iteration 97 : 0.4755606673192752
Loss in iteration 98 : 0.4754101932198838
Loss in iteration 99 : 0.47526190798027457
Loss in iteration 100 : 0.47511576698885205
Loss in iteration 101 : 0.47497172699091894
Loss in iteration 102 : 0.4748297458976418
Loss in iteration 103 : 0.47468978283899477
Loss in iteration 104 : 0.4745517980339202
Loss in iteration 105 : 0.4744157528113852
Loss in iteration 106 : 0.4742816095189929
Loss in iteration 107 : 0.47414933152450617
Loss in iteration 108 : 0.474018883148874
Loss in iteration 109 : 0.4738902296562816
Loss in iteration 110 : 0.47376333720292696
Loss in iteration 111 : 0.4736381728207309
Loss in iteration 112 : 0.47351470437650217
Loss in iteration 113 : 0.47339290055250527
Loss in iteration 114 : 0.4732727308126721
Loss in iteration 115 : 0.4731541653819755
Loss in iteration 116 : 0.47303717521758754
Loss in iteration 117 : 0.4729217319882045
Loss in iteration 118 : 0.47280780804882555
Loss in iteration 119 : 0.4726953764206714
Loss in iteration 120 : 0.4725844107687227
Loss in iteration 121 : 0.4724748853825775
Loss in iteration 122 : 0.4723667751561802
Loss in iteration 123 : 0.47226005556976863
Loss in iteration 124 : 0.4721547026714162
Loss in iteration 125 : 0.4720506930601141
Loss in iteration 126 : 0.47194800386884933
Loss in iteration 127 : 0.4718466127488105
Loss in iteration 128 : 0.4717464978538073
Loss in iteration 129 : 0.4716476378255506
Loss in iteration 130 : 0.4715500117792559
Loss in iteration 131 : 0.4714535992899392
Loss in iteration 132 : 0.47135838037908123
Loss in iteration 133 : 0.4712643355018713
Loss in iteration 134 : 0.47117144553483276
Loss in iteration 135 : 0.4710796917639378
Loss in iteration 136 : 0.47098905587311557
Loss in iteration 137 : 0.4708995199331737
Loss in iteration 138 : 0.4708110663910997
Loss in iteration 139 : 0.47072367805974186
Loss in iteration 140 : 0.4706373381078316
Loss in iteration 141 : 0.4705520300503612
Loss in iteration 142 : 0.4704677377392743
Loss in iteration 143 : 0.47038444535448115
Loss in iteration 144 : 0.47030213739516935
Loss in iteration 145 : 0.4702207986714011
Loss in iteration 146 : 0.4701404142959973
Loss in iteration 147 : 0.470060969676679
Loss in iteration 148 : 0.4699824505084694
Loss in iteration 149 : 0.469904842766343
Loss in iteration 150 : 0.4698281326981087
Loss in iteration 151 : 0.4697523068175253
Loss in iteration 152 : 0.469677351897627
Loss in iteration 153 : 0.46960325496427324
Loss in iteration 154 : 0.46953000328988853
Loss in iteration 155 : 0.4694575843874029
Loss in iteration 156 : 0.4693859860043849
Loss in iteration 157 : 0.4693151961173413
Loss in iteration 158 : 0.46924520292621147
Loss in iteration 159 : 0.46917599484900974
Loss in iteration 160 : 0.46910756051663915
Loss in iteration 161 : 0.46903988876786623
Loss in iteration 162 : 0.46897296864442967
Loss in iteration 163 : 0.4689067893863123
Loss in iteration 164 : 0.4688413404271405
Loss in iteration 165 : 0.4687766113897206
Loss in iteration 166 : 0.46871259208170957
Loss in iteration 167 : 0.468649272491405
Loss in iteration 168 : 0.46858664278366113
Loss in iteration 169 : 0.4685246932959168
Loss in iteration 170 : 0.4684634145343401
Loss in iteration 171 : 0.468402797170084
Loss in iteration 172 : 0.4683428320356361
Loss in iteration 173 : 0.4682835101212835
Loss in iteration 174 : 0.4682248225716671
Loss in iteration 175 : 0.4681667606824352
Loss in iteration 176 : 0.46810931589698335
Loss in iteration 177 : 0.46805247980328996
Loss in iteration 178 : 0.46799624413083096
Loss in iteration 179 : 0.46794060074758276
Loss in iteration 180 : 0.4678855416571017
Loss in iteration 181 : 0.4678310589956816
Loss in iteration 182 : 0.4677771450295882
Loss in iteration 183 : 0.467723792152363
Loss in iteration 184 : 0.4676709928822023
Loss in iteration 185 : 0.4676187398593956
Loss in iteration 186 : 0.46756702584384574
Loss in iteration 187 : 0.46751584371263194
Loss in iteration 188 : 0.46746518645764923
Loss in iteration 189 : 0.4674150471833075
Loss in iteration 190 : 0.4673654191042813
Loss in iteration 191 : 0.4673162955433197
Loss in iteration 192 : 0.46726766992911245
Loss in iteration 193 : 0.46721953579420633
Loss in iteration 194 : 0.4671718867729726
Loss in iteration 195 : 0.46712471659962784
Loss in iteration 196 : 0.4670780191062972
Loss in iteration 197 : 0.46703178822112995
Loss in iteration 198 : 0.4669860179664578
Loss in iteration 199 : 0.46694070245699726
Loss in iteration 200 : 0.4668958358980927
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.786, training accuracy 0.788875, time elapsed: 5459 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6764423585722852
Loss in iteration 3 : 0.6636030288156455
Loss in iteration 4 : 0.6546988798254639
Loss in iteration 5 : 0.645423806482069
Loss in iteration 6 : 0.639879633492517
Loss in iteration 7 : 0.6303147793398012
Loss in iteration 8 : 0.6251010450489449
Loss in iteration 9 : 0.6145171897899864
Loss in iteration 10 : 0.6084799191501256
Loss in iteration 11 : 0.598242900128498
Loss in iteration 12 : 0.5917704869346342
Loss in iteration 13 : 0.5831229054343882
Loss in iteration 14 : 0.5770352440320282
Loss in iteration 15 : 0.57032440242455
Loss in iteration 16 : 0.5651575314765107
Loss in iteration 17 : 0.5601273276104553
Loss in iteration 18 : 0.5559811033825358
Loss in iteration 19 : 0.5521735053362266
Loss in iteration 20 : 0.5488680929932923
Loss in iteration 21 : 0.5458619534408676
Loss in iteration 22 : 0.5431489396427318
Loss in iteration 23 : 0.5406494855655335
Loss in iteration 24 : 0.5383339857980088
Loss in iteration 25 : 0.5361681862257782
Loss in iteration 26 : 0.5341312023841617
Loss in iteration 27 : 0.5322065516740061
Loss in iteration 28 : 0.5303819581274725
Loss in iteration 29 : 0.5286481246298305
Loss in iteration 30 : 0.5269973287491475
Loss in iteration 31 : 0.5254232869722226
Loss in iteration 32 : 0.5239204457808659
Loss in iteration 33 : 0.522483973346848
Loss in iteration 34 : 0.5211094801988508
Loss in iteration 35 : 0.5197930123938314
Loss in iteration 36 : 0.5185309419780224
Loss in iteration 37 : 0.5173199517108151
Loss in iteration 38 : 0.5161569850204712
Loss in iteration 39 : 0.5150392275014845
Loss in iteration 40 : 0.5139640783216728
Loss in iteration 41 : 0.5129291326104738
Loss in iteration 42 : 0.5119321619374455
Loss in iteration 43 : 0.5109710991608404
Loss in iteration 44 : 0.5100440237353933
Loss in iteration 45 : 0.5091491491853201
Loss in iteration 46 : 0.5082848115348207
Loss in iteration 47 : 0.5074494590747479
Loss in iteration 48 : 0.5066416430544816
Loss in iteration 49 : 0.5058600093249999
Loss in iteration 50 : 0.5051032907601699
Loss in iteration 51 : 0.504370300404252
Loss in iteration 52 : 0.5036599252503403
Loss in iteration 53 : 0.5029711205907558
Loss in iteration 54 : 0.5023029048753505
Loss in iteration 55 : 0.5016543550271204
Loss in iteration 56 : 0.501024602167599
Loss in iteration 57 : 0.500412827711289
Loss in iteration 58 : 0.49981825979236294
Loss in iteration 59 : 0.4992401699912511
Loss in iteration 60 : 0.4986778703321748
Loss in iteration 61 : 0.4981307105258716
Loss in iteration 62 : 0.4975980754344834
Loss in iteration 63 : 0.4970793827380465
Loss in iteration 64 : 0.49657408078413484
Loss in iteration 65 : 0.49608164660412307
Loss in iteration 66 : 0.4956015840812256
Loss in iteration 67 : 0.49513342225693113
Loss in iteration 68 : 0.4946767137638136
Loss in iteration 69 : 0.494231033373847
Loss in iteration 70 : 0.4937959766524357
Loss in iteration 71 : 0.49337115870927467
Loss in iteration 72 : 0.49295621303802517
Loss in iteration 73 : 0.49255079043752953
Loss in iteration 74 : 0.4921545580079455
Loss in iteration 75 : 0.4917671982158243
Loss in iteration 76 : 0.49138840802264666
Loss in iteration 77 : 0.4910178980718738
Loss in iteration 78 : 0.49065539192996227
Loss in iteration 79 : 0.4903006253772204
Loss in iteration 80 : 0.48995334574472577
Loss in iteration 81 : 0.48961331129385566
Loss in iteration 82 : 0.4892802906352608
Loss in iteration 83 : 0.4889540621844018
Loss in iteration 84 : 0.48863441365097476
Loss in iteration 85 : 0.4883211415598127
Loss in iteration 86 : 0.4880140508010006
Loss in iteration 87 : 0.48771295420716426
Loss in iteration 88 : 0.4874176721560274
Loss in iteration 89 : 0.4871280321964987
Loss in iteration 90 : 0.4868438686966782
Loss in iteration 91 : 0.48656502251229905
Loss in iteration 92 : 0.4862913406742335
Loss in iteration 93 : 0.48602267609380334
Loss in iteration 94 : 0.48575888728471356
Loss in iteration 95 : 0.4854998381005367
Loss in iteration 96 : 0.4852453974867343
Loss in iteration 97 : 0.48499543924629746
Loss in iteration 98 : 0.48474984181812864
Loss in iteration 99 : 0.4845084880673768
Loss in iteration 100 : 0.4842712650869767
Loss in iteration 101 : 0.48403806400970467
Loss in iteration 102 : 0.48380877983009757
Loss in iteration 103 : 0.48358331123565446
Loss in iteration 104 : 0.48336156044674383
Loss in iteration 105 : 0.48314343306471436
Loss in iteration 106 : 0.48292883792771213
Loss in iteration 107 : 0.4827176869737645
Loss in iteration 108 : 0.4825098951106938
Loss in iteration 109 : 0.4823053800924848
Loss in iteration 110 : 0.48210406240171966
Loss in iteration 111 : 0.4819058651377526
Loss in iteration 112 : 0.4817107139102887
Loss in iteration 113 : 0.48151853673807293
Loss in iteration 114 : 0.4813292639524037
Loss in iteration 115 : 0.4811428281052103
Loss in iteration 116 : 0.4809591638814344
Loss in iteration 117 : 0.4807782080154994
Loss in iteration 118 : 0.4805998992116364
Loss in iteration 119 : 0.4804241780678596
Loss in iteration 120 : 0.48025098700341445
Loss in iteration 121 : 0.4800802701894961
Loss in iteration 122 : 0.47991197348307957
Loss in iteration 123 : 0.47974604436370283
Loss in iteration 124 : 0.47958243187304206
Loss in iteration 125 : 0.47942108655714294
Loss in iteration 126 : 0.479261960411175
Loss in iteration 127 : 0.4791050068265789
Loss in iteration 128 : 0.47895018054048555
Loss in iteration 129 : 0.47879743758730137
Loss in iteration 130 : 0.47864673525234835
Loss in iteration 131 : 0.4784980320274566
Loss in iteration 132 : 0.4783512875684217
Loss in iteration 133 : 0.47820646265423034
Loss in iteration 134 : 0.4780635191479735
Loss in iteration 135 : 0.47792241995936297
Loss in iteration 136 : 0.47778312900878095
Loss in iteration 137 : 0.47764561119278925
Loss in iteration 138 : 0.47750983235102396
Loss in iteration 139 : 0.4773757592344188
Loss in iteration 140 : 0.4772433594746975
Loss in iteration 141 : 0.4771126015550618
Loss in iteration 142 : 0.47698345478204957
Loss in iteration 143 : 0.47685588925847433
Loss in iteration 144 : 0.47672987585743026
Loss in iteration 145 : 0.4766053861972893
Loss in iteration 146 : 0.47648239261766956
Loss in iteration 147 : 0.476360868156312
Loss in iteration 148 : 0.4762407865268394
Loss in iteration 149 : 0.47612212209734756
Loss in iteration 150 : 0.4760048498698102
Loss in iteration 151 : 0.47588894546023935
Loss in iteration 152 : 0.4757743850795901
Loss in iteration 153 : 0.4756611455153669
Loss in iteration 154 : 0.4755492041138973
Loss in iteration 155 : 0.47543853876326553
Loss in iteration 156 : 0.47532912787684933
Loss in iteration 157 : 0.4752209503774612
Loss in iteration 158 : 0.47511398568205176
Loss in iteration 159 : 0.47500821368696333
Loss in iteration 160 : 0.47490361475370085
Loss in iteration 161 : 0.4748001696952077
Loss in iteration 162 : 0.47469785976262374
Loss in iteration 163 : 0.4745966666325024
Loss in iteration 164 : 0.4744965723944747
Loss in iteration 165 : 0.4743975595393342
Loss in iteration 166 : 0.4742996109475352
Loss in iteration 167 : 0.4742027098780843
Loss in iteration 168 : 0.47410683995780517
Loss in iteration 169 : 0.47401198517096943
Loss in iteration 170 : 0.4739181298492745
Loss in iteration 171 : 0.4738252586621589
Loss in iteration 172 : 0.4737333566074375
Loss in iteration 173 : 0.4736424090022481
Loss in iteration 174 : 0.47355240147429567
Loss in iteration 175 : 0.47346331995338603
Loss in iteration 176 : 0.4733751506632295
Loss in iteration 177 : 0.47328788011351847
Loss in iteration 178 : 0.4732014950922552
Loss in iteration 179 : 0.4731159826583292
Loss in iteration 180 : 0.47303133013433
Loss in iteration 181 : 0.47294752509958926
Loss in iteration 182 : 0.47286455538344146
Loss in iteration 183 : 0.4727824090587046
Loss in iteration 184 : 0.4727010744353527
Loss in iteration 185 : 0.4726205400543958
Loss in iteration 186 : 0.4725407946819486
Loss in iteration 187 : 0.4724618273034761
Loss in iteration 188 : 0.4723836271182255
Loss in iteration 189 : 0.47230618353382103
Loss in iteration 190 : 0.47222948616102634
Loss in iteration 191 : 0.47215352480866785
Loss in iteration 192 : 0.4720782894787064
Loss in iteration 193 : 0.4720037703614646
Loss in iteration 194 : 0.4719299578309841
Loss in iteration 195 : 0.47185684244053666
Loss in iteration 196 : 0.47178441491825823
Loss in iteration 197 : 0.47171266616291146
Loss in iteration 198 : 0.4716415872397808
Loss in iteration 199 : 0.47157116937667504
Loss in iteration 200 : 0.47150140396005913
Testing accuracy  of updater 0 on alg 0 with rate 0.7 = 0.783, training accuracy 0.784375, time elapsed: 5168 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.677927801944716
Loss in iteration 3 : 0.6676407914035284
Loss in iteration 4 : 0.6587709164435127
Loss in iteration 5 : 0.6507863015112567
Loss in iteration 6 : 0.6435130292785676
Loss in iteration 7 : 0.6368376653600751
Loss in iteration 8 : 0.6306748010676408
Loss in iteration 9 : 0.6249574519740627
Loss in iteration 10 : 0.6196318559878252
Loss in iteration 11 : 0.6146540004663773
Loss in iteration 12 : 0.6099872481511698
Loss in iteration 13 : 0.6056006633492054
Loss in iteration 14 : 0.6014678063015331
Loss in iteration 15 : 0.5975658449815996
Loss in iteration 16 : 0.5938748873368637
Loss in iteration 17 : 0.5903774697477796
Loss in iteration 18 : 0.5870581584312771
Loss in iteration 19 : 0.5839032340327622
Loss in iteration 20 : 0.5809004385600935
Loss in iteration 21 : 0.5780387697905345
Loss in iteration 22 : 0.5753083123650395
Loss in iteration 23 : 0.5727000976214105
Loss in iteration 24 : 0.5702059862213016
Loss in iteration 25 : 0.567818569062545
Loss in iteration 26 : 0.5655310830132843
Loss in iteration 27 : 0.5633373387749914
Loss in iteration 28 : 0.5612316587571659
Loss in iteration 29 : 0.559208823281757
Loss in iteration 30 : 0.557264023768286
Loss in iteration 31 : 0.5553928218080082
Loss in iteration 32 : 0.5535911132364463
Loss in iteration 33 : 0.5518550964720796
Loss in iteration 34 : 0.5501812445149714
Loss in iteration 35 : 0.5485662801002215
Loss in iteration 36 : 0.547007153582747
Loss in iteration 37 : 0.5455010231964527
Loss in iteration 38 : 0.544045237385267
Loss in iteration 39 : 0.5426373189485086
Loss in iteration 40 : 0.5412749507802951
Loss in iteration 41 : 0.5399559630138061
Loss in iteration 42 : 0.5386783214072626
Loss in iteration 43 : 0.5374401168304673
Loss in iteration 44 : 0.5362395557293
Loss in iteration 45 : 0.5350749514613861
Loss in iteration 46 : 0.5339447164095739
Loss in iteration 47 : 0.5328473547914625
Loss in iteration 48 : 0.5317814560930577
Loss in iteration 49 : 0.5307456890632459
Loss in iteration 50 : 0.5297387962131167
Loss in iteration 51 : 0.5287595887706221
Loss in iteration 52 : 0.5278069420466057
Loss in iteration 53 : 0.5268797911731178
Loss in iteration 54 : 0.5259771271791984
Loss in iteration 55 : 0.5250979933730173
Loss in iteration 56 : 0.5242414820025647
Loss in iteration 57 : 0.5234067311699719
Loss in iteration 58 : 0.5225929219770689
Loss in iteration 59 : 0.5217992758820779
Loss in iteration 60 : 0.521025052249302
Loss in iteration 61 : 0.5202695460754904
Loss in iteration 62 : 0.5195320858780792
Loss in iteration 63 : 0.5188120317319838
Loss in iteration 64 : 0.5181087734428138
Loss in iteration 65 : 0.5174217288455468
Loss in iteration 66 : 0.5167503422186808
Loss in iteration 67 : 0.5160940828047971
Loss in iteration 68 : 0.5154524434292759
Loss in iteration 69 : 0.5148249392096257
Loss in iteration 70 : 0.5142111063485675
Loss in iteration 71 : 0.5136105010045742
Loss in iteration 72 : 0.5130226982341364
Loss in iteration 73 : 0.512447291000482
Loss in iteration 74 : 0.511883889243927
Loss in iteration 75 : 0.5113321190094403
Loss in iteration 76 : 0.510791621627344
Loss in iteration 77 : 0.510262052943422
Loss in iteration 78 : 0.5097430825949907
Loss in iteration 79 : 0.5092343933297717
Loss in iteration 80 : 0.5087356803646343
Loss in iteration 81 : 0.5082466507815255
Loss in iteration 82 : 0.5077670229580914
Loss in iteration 83 : 0.5072965260306909
Loss in iteration 84 : 0.5068348993876837
Loss in iteration 85 : 0.5063818921910092
Loss in iteration 86 : 0.5059372629242415
Loss in iteration 87 : 0.5055007789654349
Loss in iteration 88 : 0.5050722161831789
Loss in iteration 89 : 0.504651358554412
Loss in iteration 90 : 0.5042379978026489
Loss in iteration 91 : 0.5038319330553542
Loss in iteration 92 : 0.5034329705192911
Loss in iteration 93 : 0.5030409231727703
Loss in iteration 94 : 0.5026556104737672
Loss in iteration 95 : 0.5022768580829736
Loss in iteration 96 : 0.5019044976009002
Loss in iteration 97 : 0.5015383663182013
Loss in iteration 98 : 0.5011783069784622
Loss in iteration 99 : 0.5008241675527264
Loss in iteration 100 : 0.500475801025089
Loss in iteration 101 : 0.5001330651887372
Loss in iteration 102 : 0.4997958224518395
Loss in iteration 103 : 0.49946393965274616
Loss in iteration 104 : 0.499137287883974
Loss in iteration 105 : 0.4988157423245015
Loss in iteration 106 : 0.4984991820799095
Loss in iteration 107 : 0.49818749002996543
Loss in iteration 108 : 0.4978805526832211
Loss in iteration 109 : 0.4975782600382754
Loss in iteration 110 : 0.4972805054513368
Loss in iteration 111 : 0.49698718550975607
Loss in iteration 112 : 0.49669819991122655
Loss in iteration 113 : 0.496413451348341
Loss in iteration 114 : 0.49613284539824715
Loss in iteration 115 : 0.49585629041713886
Loss in iteration 116 : 0.49558369743932357
Loss in iteration 117 : 0.4953149800806566
Loss in iteration 118 : 0.49505005444611017
Loss in iteration 119 : 0.49478883904127735
Loss in iteration 120 : 0.49453125468761033
Loss in iteration 121 : 0.4942772244412301
Loss in iteration 122 : 0.49402667351510304
Loss in iteration 123 : 0.49377952920445156
Loss in iteration 124 : 0.4935357208152234
Loss in iteration 125 : 0.49329517959549035
Loss in iteration 126 : 0.4930578386696199
Loss in iteration 127 : 0.4928236329751051
Loss in iteration 128 : 0.4925924992019241
Loss in iteration 129 : 0.49236437573430253
Loss in iteration 130 : 0.49213920259478716
Loss in iteration 131 : 0.4919169213905048
Loss in iteration 132 : 0.4916974752615226
Loss in iteration 133 : 0.4914808088312055
Loss in iteration 134 : 0.49126686815848697
Loss in iteration 135 : 0.49105560069196197
Loss in iteration 136 : 0.4908469552257276
Loss in iteration 137 : 0.4906408818568916
Loss in iteration 138 : 0.49043733194467337
Loss in iteration 139 : 0.49023625807102883
Loss in iteration 140 : 0.49003761400274426
Loss in iteration 141 : 0.48984135465491585
Loss in iteration 142 : 0.4896474360557712
Loss in iteration 143 : 0.48945581531277293
Loss in iteration 144 : 0.48926645057994556
Loss in iteration 145 : 0.4890793010263819
Loss in iteration 146 : 0.4888943268058669
Loss in iteration 147 : 0.4887114890275915
Loss in iteration 148 : 0.48853074972789423
Loss in iteration 149 : 0.48835207184299984
Loss in iteration 150 : 0.48817541918270213
Loss in iteration 151 : 0.48800075640497353
Loss in iteration 152 : 0.48782804899143806
Loss in iteration 153 : 0.48765726322369557
Loss in iteration 154 : 0.4874883661604499
Loss in iteration 155 : 0.48732132561541264
Loss in iteration 156 : 0.48715611013595567
Loss in iteration 157 : 0.48699268898247816
Loss in iteration 158 : 0.486831032108465
Loss in iteration 159 : 0.48667111014119957
Loss in iteration 160 : 0.4865128943631265
Loss in iteration 161 : 0.48635635669381205
Loss in iteration 162 : 0.48620146967250494
Loss in iteration 163 : 0.4860482064412556
Loss in iteration 164 : 0.48589654072858707
Loss in iteration 165 : 0.4857464468336877
Loss in iteration 166 : 0.4855978996111055
Loss in iteration 167 : 0.48545087445594104
Loss in iteration 168 : 0.48530534728949476
Loss in iteration 169 : 0.4851612945453727
Loss in iteration 170 : 0.48501869315603074
Loss in iteration 171 : 0.48487752053973027
Loss in iteration 172 : 0.4847377545879044
Loss in iteration 173 : 0.4845993736529109
Loss in iteration 174 : 0.48446235653615993
Loss in iteration 175 : 0.48432668247660765
Loss in iteration 176 : 0.4841923311395962
Loss in iteration 177 : 0.4840592826060323
Loss in iteration 178 : 0.4839275173618934
Loss in iteration 179 : 0.48379701628803906
Loss in iteration 180 : 0.4836677606503405
Loss in iteration 181 : 0.4835397320900896
Loss in iteration 182 : 0.483412912614697
Loss in iteration 183 : 0.48328728458866266
Loss in iteration 184 : 0.4831628307248074
Loss in iteration 185 : 0.4830395340757646
Loss in iteration 186 : 0.4829173780257141
Loss in iteration 187 : 0.48279634628235285
Loss in iteration 188 : 0.4826764228691018
Loss in iteration 189 : 0.48255759211752836
Loss in iteration 190 : 0.48243983865998935
Loss in iteration 191 : 0.48232314742247956
Loss in iteration 192 : 0.4822075036176808
Loss in iteration 193 : 0.4820928927382078
Loss in iteration 194 : 0.48197930055004135
Loss in iteration 195 : 0.48186671308614426
Loss in iteration 196 : 0.48175511664025295
Loss in iteration 197 : 0.48164449776083984
Loss in iteration 198 : 0.48153484324524365
Loss in iteration 199 : 0.4814261401339588
Loss in iteration 200 : 0.4813183757050703
Testing accuracy  of updater 0 on alg 0 with rate 0.4 = 0.7805, training accuracy 0.78175, time elapsed: 5749 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6878658918511905
Loss in iteration 3 : 0.6839999898353137
Loss in iteration 4 : 0.6808139087951742
Loss in iteration 5 : 0.677975901546472
Loss in iteration 6 : 0.6753351733505975
Loss in iteration 7 : 0.6728218611194671
Loss in iteration 8 : 0.6704024793367365
Loss in iteration 9 : 0.6680600344706835
Loss in iteration 10 : 0.665785105536363
Loss in iteration 11 : 0.6635718203046861
Loss in iteration 12 : 0.6614160283793025
Loss in iteration 13 : 0.6593144648356347
Loss in iteration 14 : 0.6572643624102471
Loss in iteration 15 : 0.6552632676432811
Loss in iteration 16 : 0.6533089502150727
Loss in iteration 17 : 0.6513993551912485
Loss in iteration 18 : 0.6495325752975569
Loss in iteration 19 : 0.6477068327923042
Loss in iteration 20 : 0.6459204661632978
Loss in iteration 21 : 0.644171919450056
Loss in iteration 22 : 0.6424597331622379
Loss in iteration 23 : 0.6407825362970553
Loss in iteration 24 : 0.6391390392002939
Loss in iteration 25 : 0.6375280271260978
Loss in iteration 26 : 0.6359483544020398
Loss in iteration 27 : 0.6343989391308696
Loss in iteration 28 : 0.6328787583733338
Loss in iteration 29 : 0.6313868437643312
Loss in iteration 30 : 0.6299222775200342
Loss in iteration 31 : 0.628484188797992
Loss in iteration 32 : 0.6270717503759341
Loss in iteration 33 : 0.6256841756183785
Loss in iteration 34 : 0.6243207157031215
Loss in iteration 35 : 0.6229806570825672
Loss in iteration 36 : 0.6216633191572709
Loss in iteration 37 : 0.6203680521414721
Loss in iteration 38 : 0.6190942351023517
Loss in iteration 39 : 0.617841274156708
Loss in iteration 40 : 0.6166086008103467
Loss in iteration 41 : 0.6153956704270143
Loss in iteration 42 : 0.6142019608150308
Loss in iteration 43 : 0.6130269709209855
Loss in iteration 44 : 0.6118702196209093
Loss in iteration 45 : 0.6107312446003313
Loss in iteration 46 : 0.6096096013154192
Loss in iteration 47 : 0.6085048620282401
Loss in iteration 48 : 0.6074166149098144
Loss in iteration 49 : 0.6063444632052245
Loss in iteration 50 : 0.6052880244556705
Loss in iteration 51 : 0.6042469297727397
Loss in iteration 52 : 0.6032208231607084
Loss in iteration 53 : 0.6022093608829874
Loss in iteration 54 : 0.6012122108692548
Loss in iteration 55 : 0.6002290521600734
Loss in iteration 56 : 0.5992595743861366
Loss in iteration 57 : 0.5983034772794824
Loss in iteration 58 : 0.5973604702142865
Loss in iteration 59 : 0.5964302717750439
Loss in iteration 60 : 0.5955126093501357
Loss in iteration 61 : 0.5946072187489437
Loss in iteration 62 : 0.5937138438408326
Loss in iteration 63 : 0.5928322362144707
Loss in iteration 64 : 0.5919621548560601
Loss in iteration 65 : 0.5911033658451936
Loss in iteration 66 : 0.5902556420671355
Loss in iteration 67 : 0.5894187629404288
Loss in iteration 68 : 0.5885925141588136
Loss in iteration 69 : 0.5877766874465371
Loss in iteration 70 : 0.5869710803261735
Loss in iteration 71 : 0.5861754958981634
Loss in iteration 72 : 0.5853897426313487
Loss in iteration 73 : 0.5846136341638044
Loss in iteration 74 : 0.5838469891133449
Loss in iteration 75 : 0.5830896308971145
Loss in iteration 76 : 0.5823413875597198
Loss in iteration 77 : 0.5816020916094072
Loss in iteration 78 : 0.5808715798617952
Loss in iteration 79 : 0.5801496932907493
Loss in iteration 80 : 0.5794362768859773
Loss in iteration 81 : 0.5787311795169685
Loss in iteration 82 : 0.5780342538029273
Loss in iteration 83 : 0.5773453559883688
Loss in iteration 84 : 0.5766643458240656
Loss in iteration 85 : 0.5759910864530615
Loss in iteration 86 : 0.5753254443014857
Loss in iteration 87 : 0.5746672889739048
Loss in iteration 88 : 0.5740164931529829
Loss in iteration 89 : 0.5733729325032351
Loss in iteration 90 : 0.5727364855786494
Loss in iteration 91 : 0.5721070337339966
Loss in iteration 92 : 0.5714844610396395
Loss in iteration 93 : 0.5708686541996693
Loss in iteration 94 : 0.5702595024732074
Loss in iteration 95 : 0.5696568975987188
Loss in iteration 96 : 0.5690607337211949
Loss in iteration 97 : 0.5684709073220681
Loss in iteration 98 : 0.5678873171517355
Loss in iteration 99 : 0.5673098641645654
Loss in iteration 100 : 0.5667384514562771
Loss in iteration 101 : 0.5661729842035769
Loss in iteration 102 : 0.5656133696059708
Loss in iteration 103 : 0.5650595168296259
Loss in iteration 104 : 0.5645113369532248
Loss in iteration 105 : 0.5639687429156892
Loss in iteration 106 : 0.5634316494657308
Loss in iteration 107 : 0.5628999731131155
Loss in iteration 108 : 0.562373632081596
Loss in iteration 109 : 0.5618525462634238
Loss in iteration 110 : 0.5613366371753887
Loss in iteration 111 : 0.5608258279163049
Loss in iteration 112 : 0.5603200431259148
Loss in iteration 113 : 0.5598192089451131
Loss in iteration 114 : 0.5593232529774744
Loss in iteration 115 : 0.558832104252012
Loss in iteration 116 : 0.5583456931871229
Loss in iteration 117 : 0.5578639515556761
Loss in iteration 118 : 0.5573868124511993
Loss in iteration 119 : 0.5569142102551146
Loss in iteration 120 : 0.556446080604996
Loss in iteration 121 : 0.5559823603637956
Loss in iteration 122 : 0.555522987590016
Loss in iteration 123 : 0.5550679015087794
Loss in iteration 124 : 0.5546170424837757
Loss in iteration 125 : 0.5541703519900388
Loss in iteration 126 : 0.553727772587544
Loss in iteration 127 : 0.55328924789557
Loss in iteration 128 : 0.5528547225678229
Loss in iteration 129 : 0.5524241422682826
Loss in iteration 130 : 0.5519974536477374
Loss in iteration 131 : 0.551574604321011
Loss in iteration 132 : 0.5511555428448192
Loss in iteration 133 : 0.5507402186962719
Loss in iteration 134 : 0.5503285822519707
Loss in iteration 135 : 0.549920584767697
Loss in iteration 136 : 0.5495161783586634
Loss in iteration 137 : 0.5491153159803114
Loss in iteration 138 : 0.5487179514096435
Loss in iteration 139 : 0.5483240392270539
Loss in iteration 140 : 0.5479335347986659
Loss in iteration 141 : 0.5475463942591391
Loss in iteration 142 : 0.5471625744949421
Loss in iteration 143 : 0.5467820331280704
Loss in iteration 144 : 0.5464047285001973
Loss in iteration 145 : 0.5460306196572443
Loss in iteration 146 : 0.54565966633436
Loss in iteration 147 : 0.5452918289412828
Loss in iteration 148 : 0.5449270685480949
Loss in iteration 149 : 0.5445653468713367
Loss in iteration 150 : 0.5442066262604824
Loss in iteration 151 : 0.5438508696847598
Loss in iteration 152 : 0.5434980407203076
Loss in iteration 153 : 0.5431481035376544
Loss in iteration 154 : 0.5428010228895167
Loss in iteration 155 : 0.5424567640988994
Loss in iteration 156 : 0.5421152930474964
Loss in iteration 157 : 0.5417765761643764
Loss in iteration 158 : 0.541440580414948
Loss in iteration 159 : 0.541107273290196
Loss in iteration 160 : 0.5407766227961844
Loss in iteration 161 : 0.5404485974438098
Loss in iteration 162 : 0.5401231662388019
Loss in iteration 163 : 0.5398002986719722
Loss in iteration 164 : 0.5394799647096877
Loss in iteration 165 : 0.5391621347845804
Loss in iteration 166 : 0.5388467797864682
Loss in iteration 167 : 0.5385338710534989
Loss in iteration 168 : 0.5382233803634978
Loss in iteration 169 : 0.5379152799255178
Loss in iteration 170 : 0.5376095423715904
Loss in iteration 171 : 0.537306140748665
Loss in iteration 172 : 0.5370050485107308
Loss in iteration 173 : 0.5367062395111304
Loss in iteration 174 : 0.5364096879950407
Loss in iteration 175 : 0.5361153685921244
Loss in iteration 176 : 0.5358232563093571
Loss in iteration 177 : 0.5355333265240081
Loss in iteration 178 : 0.5352455549767813
Loss in iteration 179 : 0.5349599177651115
Loss in iteration 180 : 0.5346763913366122
Loss in iteration 181 : 0.5343949524826616
Loss in iteration 182 : 0.534115578332137
Loss in iteration 183 : 0.5338382463452865
Loss in iteration 184 : 0.5335629343077335
Loss in iteration 185 : 0.5332896203246097
Loss in iteration 186 : 0.5330182828148259
Loss in iteration 187 : 0.5327489005054529
Loss in iteration 188 : 0.5324814524262332
Loss in iteration 189 : 0.5322159179042101
Loss in iteration 190 : 0.5319522765584663
Loss in iteration 191 : 0.5316905082949829
Loss in iteration 192 : 0.5314305933016004
Loss in iteration 193 : 0.5311725120430871
Loss in iteration 194 : 0.5309162452563185
Loss in iteration 195 : 0.5306617739455485
Loss in iteration 196 : 0.5304090793777873
Loss in iteration 197 : 0.530158143078269
Loss in iteration 198 : 0.5299089468260175
Loss in iteration 199 : 0.5296614726495077
Loss in iteration 200 : 0.529415702822401
Testing accuracy  of updater 0 on alg 0 with rate 0.09999999999999998 = 0.7705, training accuracy 0.77325, time elapsed: 5386 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 2.1415497101064873
Loss in iteration 3 : 19.912833318335267
Loss in iteration 4 : 7.263295659762948
Loss in iteration 5 : 24.58427093781558
Loss in iteration 6 : 31.90125793829789
Loss in iteration 7 : 15.571335992746047
Loss in iteration 8 : 13.779093573662788
Loss in iteration 9 : 23.582129583930502
Loss in iteration 10 : 8.618430102979145
Loss in iteration 11 : 10.91857167079331
Loss in iteration 12 : 17.65950164541531
Loss in iteration 13 : 7.16053708086142
Loss in iteration 14 : 8.200299065885341
Loss in iteration 15 : 14.427973212444344
Loss in iteration 16 : 9.454683713401028
Loss in iteration 17 : 5.318414726782042
Loss in iteration 18 : 11.046389048206684
Loss in iteration 19 : 9.955899554541906
Loss in iteration 20 : 5.4665585467424265
Loss in iteration 21 : 8.305500239795174
Loss in iteration 22 : 9.744697860132808
Loss in iteration 23 : 6.6454374707162
Loss in iteration 24 : 6.001239377838232
Loss in iteration 25 : 8.502556853610043
Loss in iteration 26 : 6.876025234315828
Loss in iteration 27 : 5.391798725838794
Loss in iteration 28 : 7.042042128121366
Loss in iteration 29 : 6.893136347497706
Loss in iteration 30 : 5.1338551671607116
Loss in iteration 31 : 5.7916445347340515
Loss in iteration 32 : 6.294025807286105
Loss in iteration 33 : 4.7587036913529115
Loss in iteration 34 : 5.168414150441411
Loss in iteration 35 : 5.525393191109931
Loss in iteration 36 : 4.392151481039281
Loss in iteration 37 : 4.543210763209461
Loss in iteration 38 : 4.784818901469641
Loss in iteration 39 : 3.8050358843140994
Loss in iteration 40 : 4.250634764019585
Loss in iteration 41 : 4.014342802231827
Loss in iteration 42 : 3.39979633305243
Loss in iteration 43 : 3.8467454039220867
Loss in iteration 44 : 3.1601722254131754
Loss in iteration 45 : 3.310189456329311
Loss in iteration 46 : 3.1639960498708923
Loss in iteration 47 : 2.8055997658705696
Loss in iteration 48 : 3.0097529552486004
Loss in iteration 49 : 2.5212427037663288
Loss in iteration 50 : 2.7692677749648253
Loss in iteration 51 : 2.3237423376401605
Loss in iteration 52 : 2.5692828285173572
Loss in iteration 53 : 2.1586788591704895
Loss in iteration 54 : 2.37733225745443
Loss in iteration 55 : 2.038095761683368
Loss in iteration 56 : 2.16267790842266
Loss in iteration 57 : 2.0730223767231584
Loss in iteration 58 : 1.8608108547270505
Loss in iteration 59 : 2.077800614600671
Loss in iteration 60 : 1.9286789847298256
Loss in iteration 61 : 1.6747598926326435
Loss in iteration 62 : 1.8112064366349205
Loss in iteration 63 : 1.964689776962385
Loss in iteration 64 : 1.7589848327164126
Loss in iteration 65 : 1.5074865543603833
Loss in iteration 66 : 1.3359119160781405
Loss in iteration 67 : 1.2872074944714798
Loss in iteration 68 : 1.3628429209650372
Loss in iteration 69 : 1.6733350209944715
Loss in iteration 70 : 2.5933275492471366
Loss in iteration 71 : 2.9431398644694116
Loss in iteration 72 : 2.7794999319688056
Loss in iteration 73 : 1.6589877320216746
Loss in iteration 74 : 1.2420285142859464
Loss in iteration 75 : 1.075223816740748
Loss in iteration 76 : 1.0156554324749831
Loss in iteration 77 : 0.9769628586303154
Loss in iteration 78 : 0.9350367815832358
Loss in iteration 79 : 0.8882470091215651
Loss in iteration 80 : 0.8450223117942378
Loss in iteration 81 : 0.8034707569502133
Loss in iteration 82 : 0.758555206220308
Loss in iteration 83 : 0.9160433059420793
Loss in iteration 84 : 4.7122454053746825
Loss in iteration 85 : 9.429583235005099
Loss in iteration 86 : 0.7199790479004767
Loss in iteration 87 : 5.741364603283766
Loss in iteration 88 : 7.1849913090145625
Loss in iteration 89 : 1.1109530035385764
Loss in iteration 90 : 3.673996353028433
Loss in iteration 91 : 5.5455280667473055
Loss in iteration 92 : 1.3896224370539545
Loss in iteration 93 : 5.680948558438089
Loss in iteration 94 : 2.1023498502288462
Loss in iteration 95 : 3.1540047957169906
Loss in iteration 96 : 2.916524799043378
Loss in iteration 97 : 1.8816454160862393
Loss in iteration 98 : 3.1470744736247807
Loss in iteration 99 : 1.7634018978924397
Loss in iteration 100 : 3.3357930734584733
Loss in iteration 101 : 2.0127696517595637
Loss in iteration 102 : 2.4350254652919063
Loss in iteration 103 : 2.227377772671787
Loss in iteration 104 : 1.6129411038358226
Loss in iteration 105 : 2.220831228424035
Loss in iteration 106 : 1.496351889445497
Loss in iteration 107 : 1.68288632515773
Loss in iteration 108 : 2.134442513005027
Loss in iteration 109 : 1.3546712873236333
Loss in iteration 110 : 1.2836151757901038
Loss in iteration 111 : 1.9491769551753972
Loss in iteration 112 : 2.3049869978179514
Loss in iteration 113 : 2.4047575447612157
Loss in iteration 114 : 1.8995959949178416
Loss in iteration 115 : 1.8571017560827003
Loss in iteration 116 : 2.1154349425053023
Loss in iteration 117 : 2.851614264861121
Loss in iteration 118 : 2.9296350388520414
Loss in iteration 119 : 2.2610556663972723
Loss in iteration 120 : 1.6476951120098362
Loss in iteration 121 : 1.3394952388256602
Loss in iteration 122 : 1.1689546461531506
Loss in iteration 123 : 1.0780433048647144
Loss in iteration 124 : 1.0238620594071657
Loss in iteration 125 : 1.0033538393645618
Loss in iteration 126 : 1.0582215731041094
Loss in iteration 127 : 1.5848663222104753
Loss in iteration 128 : 3.7758715427252683
Loss in iteration 129 : 5.670801168743868
Loss in iteration 130 : 1.047493260900793
Loss in iteration 131 : 2.162865633571639
Loss in iteration 132 : 6.641937855747267
Loss in iteration 133 : 1.1109050576979325
Loss in iteration 134 : 4.330074340928238
Loss in iteration 135 : 4.527190709380954
Loss in iteration 136 : 1.640096378624141
Loss in iteration 137 : 6.063446558043864
Loss in iteration 138 : 1.5440391128499973
Loss in iteration 139 : 4.3117814629597815
Loss in iteration 140 : 1.9015412999436585
Loss in iteration 141 : 2.9534567750059897
Loss in iteration 142 : 2.933119173481696
Loss in iteration 143 : 2.0678867929540785
Loss in iteration 144 : 3.5957200618958978
Loss in iteration 145 : 1.617571295541985
Loss in iteration 146 : 2.6445613771974714
Loss in iteration 147 : 1.874491109340414
Loss in iteration 148 : 1.6310374725305843
Loss in iteration 149 : 2.3565785681136346
Loss in iteration 150 : 1.4142773259214523
Loss in iteration 151 : 1.7039484876298212
Loss in iteration 152 : 2.5137585401091855
Loss in iteration 153 : 1.4153252137697103
Loss in iteration 154 : 1.1774698544320927
Loss in iteration 155 : 1.9054954013920917
Loss in iteration 156 : 2.648787338993805
Loss in iteration 157 : 2.8626005059766273
Loss in iteration 158 : 1.8351081422581192
Loss in iteration 159 : 1.4055504779590238
Loss in iteration 160 : 1.301517659975678
Loss in iteration 161 : 1.4677377548047716
Loss in iteration 162 : 2.2030495787798925
Loss in iteration 163 : 3.709674290195582
Loss in iteration 164 : 3.059574428017947
Loss in iteration 165 : 2.064038861936705
Loss in iteration 166 : 1.5405006178075245
Loss in iteration 167 : 1.4180469530916684
Loss in iteration 168 : 1.3792388188136675
Loss in iteration 169 : 1.4982083546767078
Loss in iteration 170 : 1.674117964921084
Loss in iteration 171 : 2.1135603877898506
Loss in iteration 172 : 2.125942348122269
Loss in iteration 173 : 2.294315496986174
Loss in iteration 174 : 1.761728338559009
Loss in iteration 175 : 1.6398103007444957
Loss in iteration 176 : 1.441376420873386
Loss in iteration 177 : 1.5271972913506628
Loss in iteration 178 : 1.6631394808104407
Loss in iteration 179 : 2.2613428902796127
Loss in iteration 180 : 2.439949596438362
Loss in iteration 181 : 2.6077077523365295
Loss in iteration 182 : 1.9178540155644381
Loss in iteration 183 : 1.6717058329005858
Loss in iteration 184 : 1.4607502476629781
Loss in iteration 185 : 1.5213920536011716
Loss in iteration 186 : 1.661524128079658
Loss in iteration 187 : 2.3195060072343954
Loss in iteration 188 : 2.6201675017656374
Loss in iteration 189 : 2.8752974903605146
Loss in iteration 190 : 1.8528061285765003
Loss in iteration 191 : 1.5272294349281146
Loss in iteration 192 : 1.3394468986128705
Loss in iteration 193 : 1.4070998712006326
Loss in iteration 194 : 1.5510668806249883
Loss in iteration 195 : 2.1637776412515954
Loss in iteration 196 : 2.526783239797124
Loss in iteration 197 : 2.8920497190304566
Loss in iteration 198 : 1.9053269976039677
Loss in iteration 199 : 1.5444974217359249
Loss in iteration 200 : 1.3519050353024897
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.769, training accuracy 0.760625, time elapsed: 7111 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 1.5445767658443816
Loss in iteration 3 : 12.918036793810153
Loss in iteration 4 : 3.3421093703611127
Loss in iteration 5 : 18.157764066221418
Loss in iteration 6 : 22.657275560208923
Loss in iteration 7 : 10.66749270935545
Loss in iteration 8 : 10.611830969997266
Loss in iteration 9 : 17.31002489298248
Loss in iteration 10 : 5.811859219479891
Loss in iteration 11 : 8.565784461881632
Loss in iteration 12 : 13.40275408384096
Loss in iteration 13 : 5.463724927571757
Loss in iteration 14 : 5.883978141419458
Loss in iteration 15 : 10.848211269986974
Loss in iteration 16 : 7.278601879013755
Loss in iteration 17 : 3.695056429946645
Loss in iteration 18 : 8.03319307242306
Loss in iteration 19 : 7.7118514636423106
Loss in iteration 20 : 3.9526963773356205
Loss in iteration 21 : 5.781807734369762
Loss in iteration 22 : 7.3093557155173245
Loss in iteration 23 : 5.180861326899081
Loss in iteration 24 : 4.090706337712137
Loss in iteration 25 : 6.129351586965455
Loss in iteration 26 : 5.4551374526555705
Loss in iteration 27 : 3.822954824643686
Loss in iteration 28 : 4.916094177902462
Loss in iteration 29 : 5.291486732053814
Loss in iteration 30 : 3.951772274077311
Loss in iteration 31 : 3.9039035193178897
Loss in iteration 32 : 4.763306770237216
Loss in iteration 33 : 3.733925495071823
Loss in iteration 34 : 3.5279991292418735
Loss in iteration 35 : 4.160023948305455
Loss in iteration 36 : 3.5460196054958626
Loss in iteration 37 : 3.091922429659201
Loss in iteration 38 : 3.672565700605559
Loss in iteration 39 : 3.023929506712854
Loss in iteration 40 : 2.8975114201199452
Loss in iteration 41 : 3.1921487379624343
Loss in iteration 42 : 2.5866940075177647
Loss in iteration 43 : 2.7016583094948015
Loss in iteration 44 : 2.6802829632896246
Loss in iteration 45 : 2.244086166550589
Loss in iteration 46 : 2.5268340989071314
Loss in iteration 47 : 2.113784148966172
Loss in iteration 48 : 2.204607533743066
Loss in iteration 49 : 2.074915453490321
Loss in iteration 50 : 1.9225933867928418
Loss in iteration 51 : 1.9928630515465138
Loss in iteration 52 : 1.7226897872366684
Loss in iteration 53 : 1.8626389937185879
Loss in iteration 54 : 1.605348135736238
Loss in iteration 55 : 1.7156717395204357
Loss in iteration 56 : 1.5293066009668943
Loss in iteration 57 : 1.5501020249590314
Loss in iteration 58 : 1.5382175808294578
Loss in iteration 59 : 1.3678028797427095
Loss in iteration 60 : 1.521672249776921
Loss in iteration 61 : 1.3733963940942562
Loss in iteration 62 : 1.260663520177564
Loss in iteration 63 : 1.3927219532140676
Loss in iteration 64 : 1.3487219791988159
Loss in iteration 65 : 1.1371425885219242
Loss in iteration 66 : 1.0947378822532379
Loss in iteration 67 : 1.1982237161951133
Loss in iteration 68 : 1.2591299740964232
Loss in iteration 69 : 1.2468050852159112
Loss in iteration 70 : 1.148786890572291
Loss in iteration 71 : 1.0771945144888377
Loss in iteration 72 : 1.043654744338462
Loss in iteration 73 : 1.1138643344445975
Loss in iteration 74 : 1.3183961233831303
Loss in iteration 75 : 1.7971315976849978
Loss in iteration 76 : 1.7715317243231894
Loss in iteration 77 : 1.6494742042067072
Loss in iteration 78 : 1.2050862418216524
Loss in iteration 79 : 1.037631758784943
Loss in iteration 80 : 0.9349724047385767
Loss in iteration 81 : 0.9437372948621496
Loss in iteration 82 : 1.0157449212139358
Loss in iteration 83 : 1.3701361972954242
Loss in iteration 84 : 1.7943334968716282
Loss in iteration 85 : 2.4791325677844993
Loss in iteration 86 : 1.4757491234249178
Loss in iteration 87 : 1.1903460753052482
Loss in iteration 88 : 1.0517104256428875
Loss in iteration 89 : 1.2142507271664265
Loss in iteration 90 : 1.4291961274432436
Loss in iteration 91 : 1.9804977451308543
Loss in iteration 92 : 1.6566421498128978
Loss in iteration 93 : 1.457454889604701
Loss in iteration 94 : 1.1358126958093282
Loss in iteration 95 : 1.0816071922185342
Loss in iteration 96 : 1.063951887876274
Loss in iteration 97 : 1.2936452483845213
Loss in iteration 98 : 1.590997580013025
Loss in iteration 99 : 2.2098742969305336
Loss in iteration 100 : 1.7028424967584483
Loss in iteration 101 : 1.4526998642936397
Loss in iteration 102 : 1.1758493062536899
Loss in iteration 103 : 1.1783957503448703
Loss in iteration 104 : 1.165543280707703
Loss in iteration 105 : 1.3205766803690007
Loss in iteration 106 : 1.3757383491169413
Loss in iteration 107 : 1.5271939435397317
Loss in iteration 108 : 1.3999712156861843
Loss in iteration 109 : 1.3745392960535088
Loss in iteration 110 : 1.230666091331368
Loss in iteration 111 : 1.2472456212743728
Loss in iteration 112 : 1.2283530611667022
Loss in iteration 113 : 1.3777617841563496
Loss in iteration 114 : 1.4377825433716203
Loss in iteration 115 : 1.5973109377043777
Loss in iteration 116 : 1.443308554004992
Loss in iteration 117 : 1.3651928975193128
Loss in iteration 118 : 1.1928753111009696
Loss in iteration 119 : 1.1497460779823219
Loss in iteration 120 : 1.1055220097864162
Loss in iteration 121 : 1.1871662508829528
Loss in iteration 122 : 1.3005529572080088
Loss in iteration 123 : 1.589619927536133
Loss in iteration 124 : 1.6078643441672589
Loss in iteration 125 : 1.6231945914453032
Loss in iteration 126 : 1.3222648187055652
Loss in iteration 127 : 1.1950195809472253
Loss in iteration 128 : 1.0712075614193264
Loss in iteration 129 : 1.0749025496108198
Loss in iteration 130 : 1.1198450527983261
Loss in iteration 131 : 1.3441847657367296
Loss in iteration 132 : 1.5704122111813716
Loss in iteration 133 : 1.8512085579166375
Loss in iteration 134 : 1.5086125824983956
Loss in iteration 135 : 1.3113307572374493
Loss in iteration 136 : 1.106845075340315
Loss in iteration 137 : 1.0701812318305206
Loss in iteration 138 : 1.0693738837451428
Loss in iteration 139 : 1.2374952436999112
Loss in iteration 140 : 1.4770612286329678
Loss in iteration 141 : 1.8508335120816932
Loss in iteration 142 : 1.6071710096876484
Loss in iteration 143 : 1.4192340034957367
Loss in iteration 144 : 1.1624696853465322
Loss in iteration 145 : 1.0972492799765237
Loss in iteration 146 : 1.0492405676822085
Loss in iteration 147 : 1.1486395861181395
Loss in iteration 148 : 1.3314530850046185
Loss in iteration 149 : 1.744338522245598
Loss in iteration 150 : 1.7226706855403995
Loss in iteration 151 : 1.635589501118448
Loss in iteration 152 : 1.2735268889905131
Loss in iteration 153 : 1.142998596085625
Loss in iteration 154 : 1.0331454764447217
Loss in iteration 155 : 1.0517223624148169
Loss in iteration 156 : 1.1287799814042183
Loss in iteration 157 : 1.4403039175190788
Loss in iteration 158 : 1.7263971403690923
Loss in iteration 159 : 1.967774051884871
Loss in iteration 160 : 1.4544489282568045
Loss in iteration 161 : 1.2051197891920122
Loss in iteration 162 : 1.0335603164344003
Loss in iteration 163 : 1.0185285583779398
Loss in iteration 164 : 1.0534245110166598
Loss in iteration 165 : 1.2845032562017742
Loss in iteration 166 : 1.6058192666990956
Loss in iteration 167 : 2.0031809890754055
Loss in iteration 168 : 1.5691865292411866
Loss in iteration 169 : 1.2982958356633636
Loss in iteration 170 : 1.0850172435085388
Loss in iteration 171 : 1.0525424233578677
Loss in iteration 172 : 1.0534368898389075
Loss in iteration 173 : 1.2159256231605657
Loss in iteration 174 : 1.458897034382481
Loss in iteration 175 : 1.8651763908799712
Loss in iteration 176 : 1.6423924474670368
Loss in iteration 177 : 1.451319645598691
Loss in iteration 178 : 1.1844086386773216
Loss in iteration 179 : 1.118292928772327
Loss in iteration 180 : 1.0666043905853817
Loss in iteration 181 : 1.1484336358444425
Loss in iteration 182 : 1.2814830102538333
Loss in iteration 183 : 1.623714763957138
Loss in iteration 184 : 1.6799276506443244
Loss in iteration 185 : 1.683818140146514
Loss in iteration 186 : 1.3309120034936732
Loss in iteration 187 : 1.1877780178793613
Loss in iteration 188 : 1.0625889868371206
Loss in iteration 189 : 1.0709324684411727
Loss in iteration 190 : 1.122938905320896
Loss in iteration 191 : 1.3782470907254973
Loss in iteration 192 : 1.635526096574719
Loss in iteration 193 : 1.9118746275898382
Loss in iteration 194 : 1.500072409374961
Loss in iteration 195 : 1.2712737206136502
Loss in iteration 196 : 1.0824661528273718
Loss in iteration 197 : 1.0556573416328066
Loss in iteration 198 : 1.0642673627183217
Loss in iteration 199 : 1.2401862012531384
Loss in iteration 200 : 1.4930509652758945
Testing accuracy  of updater 1 on alg 0 with rate 7.0 = 0.705, training accuracy 0.692, time elapsed: 6866 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 1.0091931589813257
Loss in iteration 3 : 5.670171579851011
Loss in iteration 4 : 0.6135639479662657
Loss in iteration 5 : 2.0345803890729037
Loss in iteration 6 : 3.2577976202442334
Loss in iteration 7 : 0.7237551165544747
Loss in iteration 8 : 3.7060331114683844
Loss in iteration 9 : 0.9310964820237396
Loss in iteration 10 : 3.489842151896202
Loss in iteration 11 : 1.8680274962541565
Loss in iteration 12 : 2.7015974515125674
Loss in iteration 13 : 2.378799268674049
Loss in iteration 14 : 1.7604593881608184
Loss in iteration 15 : 2.7276925025540804
Loss in iteration 16 : 1.233431614825191
Loss in iteration 17 : 2.4615769589092817
Loss in iteration 18 : 1.706325730492128
Loss in iteration 19 : 1.719427279893313
Loss in iteration 20 : 2.2131353427127283
Loss in iteration 21 : 1.2764952909508078
Loss in iteration 22 : 2.0176610430079394
Loss in iteration 23 : 1.5128202961392385
Loss in iteration 24 : 1.5205131480990934
Loss in iteration 25 : 1.79492860349202
Loss in iteration 26 : 1.1581440551271887
Loss in iteration 27 : 1.727233439589661
Loss in iteration 28 : 1.1584955363434244
Loss in iteration 29 : 1.4498854137678485
Loss in iteration 30 : 1.2960552598071504
Loss in iteration 31 : 1.1658752708782771
Loss in iteration 32 : 1.2446939716023615
Loss in iteration 33 : 1.0624772209242592
Loss in iteration 34 : 1.162943997481077
Loss in iteration 35 : 0.948583456160502
Loss in iteration 36 : 1.0320163481371953
Loss in iteration 37 : 0.9536548687053373
Loss in iteration 38 : 0.8880539492345977
Loss in iteration 39 : 0.9881538151881367
Loss in iteration 40 : 0.7426235144051643
Loss in iteration 41 : 0.9511612367556467
Loss in iteration 42 : 0.7732469138299216
Loss in iteration 43 : 0.7411910133707169
Loss in iteration 44 : 0.9010650653930455
Loss in iteration 45 : 0.7507472398394757
Loss in iteration 46 : 0.6590994893203865
Loss in iteration 47 : 0.7846616888202698
Loss in iteration 48 : 0.8858339828436342
Loss in iteration 49 : 0.8054824765959684
Loss in iteration 50 : 0.6778840817296896
Loss in iteration 51 : 0.6021569737450038
Loss in iteration 52 : 0.5814455618094196
Loss in iteration 53 : 0.6035061922300828
Loss in iteration 54 : 0.6829422661600868
Loss in iteration 55 : 0.8690306555930056
Loss in iteration 56 : 1.0909896562684922
Loss in iteration 57 : 1.0817427354604248
Loss in iteration 58 : 0.8344281034409249
Loss in iteration 59 : 0.6402785938704634
Loss in iteration 60 : 0.5502915710040537
Loss in iteration 61 : 0.5314550836809336
Loss in iteration 62 : 0.5619670100857087
Loss in iteration 63 : 0.6445228218494319
Loss in iteration 64 : 0.7912759935734758
Loss in iteration 65 : 1.0085344345641316
Loss in iteration 66 : 1.0201166454896748
Loss in iteration 67 : 0.9639308416694761
Loss in iteration 68 : 0.7838524435172884
Loss in iteration 69 : 0.7123300235341206
Loss in iteration 70 : 0.6708589213476402
Loss in iteration 71 : 0.6895766341061755
Loss in iteration 72 : 0.7390022654046658
Loss in iteration 73 : 0.844212369631904
Loss in iteration 74 : 0.9284227848493486
Loss in iteration 75 : 0.9644636508218053
Loss in iteration 76 : 0.8761021499986879
Loss in iteration 77 : 0.7938256277898089
Loss in iteration 78 : 0.7133346349325732
Loss in iteration 79 : 0.6871451340949499
Loss in iteration 80 : 0.6979112049492642
Loss in iteration 81 : 0.78082882962689
Loss in iteration 82 : 0.9237464780129734
Loss in iteration 83 : 1.0653101571569858
Loss in iteration 84 : 0.9970002441505318
Loss in iteration 85 : 0.8477766006409877
Loss in iteration 86 : 0.6991883258708154
Loss in iteration 87 : 0.6171849622797079
Loss in iteration 88 : 0.5697391363572523
Loss in iteration 89 : 0.5479401512305987
Loss in iteration 90 : 0.5425937250250394
Loss in iteration 91 : 0.5565835008508877
Loss in iteration 92 : 0.6273530706256765
Loss in iteration 93 : 0.8843442510242537
Loss in iteration 94 : 1.3907247511814027
Loss in iteration 95 : 1.4055935142423042
Loss in iteration 96 : 0.8726816920863435
Loss in iteration 97 : 0.6107511458291182
Loss in iteration 98 : 0.5290076757731551
Loss in iteration 99 : 0.533155079929817
Loss in iteration 100 : 0.6006728570978843
Loss in iteration 101 : 0.7383227618212455
Loss in iteration 102 : 0.9126628973080463
Loss in iteration 103 : 0.9885897688262529
Loss in iteration 104 : 0.9044824112508493
Loss in iteration 105 : 0.7317380633108326
Loss in iteration 106 : 0.6267929778141399
Loss in iteration 107 : 0.564678907247178
Loss in iteration 108 : 0.5393151387422
Loss in iteration 109 : 0.5358354233868595
Loss in iteration 110 : 0.5484144839700427
Loss in iteration 111 : 0.6048335658284629
Loss in iteration 112 : 0.8264698831533112
Loss in iteration 113 : 1.3378232402415131
Loss in iteration 114 : 1.4602584463131048
Loss in iteration 115 : 0.916625791089411
Loss in iteration 116 : 0.6154940943691224
Loss in iteration 117 : 0.526606191265277
Loss in iteration 118 : 0.5573771732855688
Loss in iteration 119 : 0.6915526193585265
Loss in iteration 120 : 0.9165752888598235
Loss in iteration 121 : 1.0189902715059853
Loss in iteration 122 : 0.8807875014800096
Loss in iteration 123 : 0.7180072506299419
Loss in iteration 124 : 0.6056109478479279
Loss in iteration 125 : 0.5514113287838045
Loss in iteration 126 : 0.5225680397088871
Loss in iteration 127 : 0.5116505265649164
Loss in iteration 128 : 0.5096568010883229
Loss in iteration 129 : 0.5067027786104187
Loss in iteration 130 : 0.5111376519180759
Loss in iteration 131 : 0.5862799491701386
Loss in iteration 132 : 1.0056788353505262
Loss in iteration 133 : 1.9940839010769325
Loss in iteration 134 : 1.2482203390295337
Loss in iteration 135 : 0.7391273545490874
Loss in iteration 136 : 0.5832953516994189
Loss in iteration 137 : 0.5434852081523588
Loss in iteration 138 : 0.5317449687698117
Loss in iteration 139 : 0.5251147851246663
Loss in iteration 140 : 0.5235403829633491
Loss in iteration 141 : 0.5522486869937172
Loss in iteration 142 : 0.6917572753031367
Loss in iteration 143 : 1.1350167936434936
Loss in iteration 144 : 1.4881574936510193
Loss in iteration 145 : 1.2176488436725768
Loss in iteration 146 : 0.7094056386613851
Loss in iteration 147 : 0.5455284204002427
Loss in iteration 148 : 0.5233349864052654
Loss in iteration 149 : 0.5772287989651226
Loss in iteration 150 : 0.693484041837369
Loss in iteration 151 : 0.8258809435815581
Loss in iteration 152 : 0.8639718927103714
Loss in iteration 153 : 0.7772307368756518
Loss in iteration 154 : 0.6738489640801816
Loss in iteration 155 : 0.5972983104231891
Loss in iteration 156 : 0.5607244912027657
Loss in iteration 157 : 0.5602605299249124
Loss in iteration 158 : 0.6300266950372774
Loss in iteration 159 : 0.9071609010194487
Loss in iteration 160 : 1.4868836802381553
Loss in iteration 161 : 1.399373908132584
Loss in iteration 162 : 0.8637026844616114
Loss in iteration 163 : 0.6083610415060504
Loss in iteration 164 : 0.5336538298541685
Loss in iteration 165 : 0.535210619388041
Loss in iteration 166 : 0.5925155516296448
Loss in iteration 167 : 0.7031544528601708
Loss in iteration 168 : 0.8490443149554917
Loss in iteration 169 : 0.9379914824955952
Loss in iteration 170 : 0.8820981500651105
Loss in iteration 171 : 0.7931515397706956
Loss in iteration 172 : 0.6978705942699283
Loss in iteration 173 : 0.6533133003969894
Loss in iteration 174 : 0.6374039579370722
Loss in iteration 175 : 0.675192641785486
Loss in iteration 176 : 0.7797931278029592
Loss in iteration 177 : 0.9515917861065435
Loss in iteration 178 : 1.0493960802605962
Loss in iteration 179 : 0.946637764090979
Loss in iteration 180 : 0.7674567254094137
Loss in iteration 181 : 0.6398552456583441
Loss in iteration 182 : 0.5655072029593851
Loss in iteration 183 : 0.5284773426756665
Loss in iteration 184 : 0.5139208524322211
Loss in iteration 185 : 0.5118944903652317
Loss in iteration 186 : 0.523478181517458
Loss in iteration 187 : 0.5944067473748841
Loss in iteration 188 : 0.9251665010177065
Loss in iteration 189 : 1.6698308197347824
Loss in iteration 190 : 1.465001748575069
Loss in iteration 191 : 0.7950148493666653
Loss in iteration 192 : 0.5651033292855268
Loss in iteration 193 : 0.5204542228451535
Loss in iteration 194 : 0.5630323956135408
Loss in iteration 195 : 0.6809780148794676
Loss in iteration 196 : 0.857321965218393
Loss in iteration 197 : 0.9419015747069641
Loss in iteration 198 : 0.8556068578148533
Loss in iteration 199 : 0.7255485252258274
Loss in iteration 200 : 0.6186250996111243
Testing accuracy  of updater 1 on alg 0 with rate 4.0 = 0.786, training accuracy 0.781125, time elapsed: 5299 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6826496441898317
Loss in iteration 3 : 0.6339254231375362
Loss in iteration 4 : 0.5867704951414214
Loss in iteration 5 : 0.5629247813403957
Loss in iteration 6 : 0.5309264422630863
Loss in iteration 7 : 0.5148220298238638
Loss in iteration 8 : 0.5014300866706937
Loss in iteration 9 : 0.49361340845674767
Loss in iteration 10 : 0.48953983537674245
Loss in iteration 11 : 0.4873414015503837
Loss in iteration 12 : 0.485347460389294
Loss in iteration 13 : 0.4859659965021575
Loss in iteration 14 : 0.48401731685190746
Loss in iteration 15 : 0.4857146609389199
Loss in iteration 16 : 0.4837876997639102
Loss in iteration 17 : 0.4847076607097184
Loss in iteration 18 : 0.4833683345675886
Loss in iteration 19 : 0.48254848948779866
Loss in iteration 20 : 0.48196585997005537
Loss in iteration 21 : 0.47999165564753615
Loss in iteration 22 : 0.47963191667020355
Loss in iteration 23 : 0.4776483370884679
Loss in iteration 24 : 0.47671713059767395
Loss in iteration 25 : 0.4752680860830252
Loss in iteration 26 : 0.4735834702065299
Loss in iteration 27 : 0.4725359283328226
Loss in iteration 28 : 0.4706389269933054
Loss in iteration 29 : 0.4696970128909924
Loss in iteration 30 : 0.4681925643889509
Loss in iteration 31 : 0.4671926091231702
Loss in iteration 32 : 0.46627051259625146
Loss in iteration 33 : 0.4652834041919633
Loss in iteration 34 : 0.46482699453703147
Loss in iteration 35 : 0.4640628105703775
Loss in iteration 36 : 0.46388582935762
Loss in iteration 37 : 0.4634810647778982
Loss in iteration 38 : 0.4634087126422414
Loss in iteration 39 : 0.46329872825637874
Loss in iteration 40 : 0.46321047433555473
Loss in iteration 41 : 0.4632567117253558
Loss in iteration 42 : 0.4631445471392709
Loss in iteration 43 : 0.4632409180417412
Loss in iteration 44 : 0.46313981722026343
Loss in iteration 45 : 0.463206776577669
Loss in iteration 46 : 0.4631225626248698
Loss in iteration 47 : 0.463109519985711
Loss in iteration 48 : 0.463032772281511
Loss in iteration 49 : 0.462941256714982
Loss in iteration 50 : 0.46286719860269726
Loss in iteration 51 : 0.46272439643518243
Loss in iteration 52 : 0.46263978907545183
Loss in iteration 53 : 0.46246977902090736
Loss in iteration 54 : 0.46236280867433716
Loss in iteration 55 : 0.4621924347878605
Loss in iteration 56 : 0.462070667327745
Loss in iteration 57 : 0.46192604452490754
Loss in iteration 58 : 0.46180401697337475
Loss in iteration 59 : 0.46169353547258263
Loss in iteration 60 : 0.46157958748427025
Loss in iteration 61 : 0.46149676893302477
Loss in iteration 62 : 0.4613974860599837
Loss in iteration 63 : 0.4613338746553849
Loss in iteration 64 : 0.46125477348586363
Loss in iteration 65 : 0.46120218185355194
Loss in iteration 66 : 0.46114207350457126
Loss in iteration 67 : 0.46109296267961036
Loss in iteration 68 : 0.4610464130619111
Loss in iteration 69 : 0.46099839924326047
Loss in iteration 70 : 0.46096068476208524
Loss in iteration 71 : 0.4609154926907647
Loss in iteration 72 : 0.46088221312122696
Loss in iteration 73 : 0.4608408301643374
Loss in iteration 74 : 0.46080762791188684
Loss in iteration 75 : 0.46076958080876157
Loss in iteration 76 : 0.46073450571090796
Loss in iteration 77 : 0.4606993442039976
Loss in iteration 78 : 0.46066300766105794
Loss in iteration 79 : 0.4606299671091067
Loss in iteration 80 : 0.460593615459844
Loss in iteration 81 : 0.4605616231010759
Loss in iteration 82 : 0.46052659300507587
Loss in iteration 83 : 0.4604953972268911
Loss in iteration 84 : 0.46046306419706273
Loss in iteration 85 : 0.4604331470013139
Loss in iteration 86 : 0.46040406424013053
Loss in iteration 87 : 0.4603757400010657
Loss in iteration 88 : 0.46034944497340907
Loss in iteration 89 : 0.46032284027084414
Loss in iteration 90 : 0.4602986516056312
Loss in iteration 91 : 0.46027395580339264
Loss in iteration 92 : 0.4602513475455746
Loss in iteration 93 : 0.4602285474075361
Loss in iteration 94 : 0.4602070513402943
Loss in iteration 95 : 0.4601858487701123
Loss in iteration 96 : 0.4601651804989591
Loss in iteration 97 : 0.4601452477436256
Loss in iteration 98 : 0.4601253585649193
Loss in iteration 99 : 0.460106389453425
Loss in iteration 100 : 0.46008724696822306
Loss in iteration 101 : 0.4600689111880061
Loss in iteration 102 : 0.46005042972692245
Loss in iteration 103 : 0.4600325139243527
Loss in iteration 104 : 0.4600146612598618
Loss in iteration 105 : 0.4599971450034425
Loss in iteration 106 : 0.4599799229333456
Loss in iteration 107 : 0.45996286617442106
Loss in iteration 108 : 0.459946244135832
Loss in iteration 109 : 0.4599297156539008
Loss in iteration 110 : 0.45991366568084274
Loss in iteration 111 : 0.45989774110907583
Loss in iteration 112 : 0.45988225947841077
Loss in iteration 113 : 0.45986697955883843
Loss in iteration 114 : 0.45985205252424216
Loss in iteration 115 : 0.4598373970928094
Loss in iteration 116 : 0.459823003854957
Loss in iteration 117 : 0.45980893050836097
Loss in iteration 118 : 0.45979506653248187
Loss in iteration 119 : 0.45978153608453737
Loss in iteration 120 : 0.4597681967184145
Loss in iteration 121 : 0.4597551656652332
Loss in iteration 122 : 0.4597423294163562
Loss in iteration 123 : 0.4597297573301423
Loss in iteration 124 : 0.45971739659249383
Loss in iteration 125 : 0.4597052581533231
Loss in iteration 126 : 0.4596933453434739
Loss in iteration 127 : 0.4596816229980902
Loss in iteration 128 : 0.4596701272209326
Loss in iteration 129 : 0.4596588042987402
Loss in iteration 130 : 0.4596476983046849
Loss in iteration 131 : 0.45963676300650197
Loss in iteration 132 : 0.459626030295277
Loss in iteration 133 : 0.45961547401971176
Loss in iteration 134 : 0.4596051040551409
Loss in iteration 135 : 0.4595949153813907
Loss in iteration 136 : 0.45958489868767843
Loss in iteration 137 : 0.4595750640510282
Loss in iteration 138 : 0.4595653923717035
Loss in iteration 139 : 0.4595558988716067
Loss in iteration 140 : 0.4595465637686346
Loss in iteration 141 : 0.4595373985610921
Loss in iteration 142 : 0.4595283891887687
Loss in iteration 143 : 0.459519539541399
Loss in iteration 144 : 0.45951084381986895
Loss in iteration 145 : 0.45950229863169895
Loss in iteration 146 : 0.45949390469174844
Loss in iteration 147 : 0.4594856538583364
Loss in iteration 148 : 0.45947754961482984
Loss in iteration 149 : 0.45946958263263626
Loss in iteration 150 : 0.45946175595800254
Loss in iteration 151 : 0.45945406211205103
Loss in iteration 152 : 0.45944650185397157
Loss in iteration 153 : 0.4594390707841749
Loss in iteration 154 : 0.4594317668911937
Loss in iteration 155 : 0.4594245884741664
Loss in iteration 156 : 0.459417531568454
Loss in iteration 157 : 0.45941059607613993
Loss in iteration 158 : 0.4594037774666971
Loss in iteration 159 : 0.4593970759471388
Loss in iteration 160 : 0.4593904875030272
Loss in iteration 161 : 0.459384011644847
Loss in iteration 162 : 0.45937764540784537
Loss in iteration 163 : 0.45937138727704047
Loss in iteration 164 : 0.45936523538314533
Loss in iteration 165 : 0.4593591874278399
Loss in iteration 166 : 0.45935324228625574
Loss in iteration 167 : 0.4593473973221592
Loss in iteration 168 : 0.45934165163035506
Loss in iteration 169 : 0.4593360026686851
Loss in iteration 170 : 0.45933044936700074
Loss in iteration 171 : 0.45932498957594775
Loss in iteration 172 : 0.4593196219042427
Loss in iteration 173 : 0.4593143446570927
Loss in iteration 174 : 0.4593091561452591
Loss in iteration 175 : 0.45930405500634736
Loss in iteration 176 : 0.45929903940031797
Loss in iteration 177 : 0.45929410812136673
Loss in iteration 178 : 0.4592892593672371
Loss in iteration 179 : 0.45928449194452703
Loss in iteration 180 : 0.4592798042174509
Loss in iteration 181 : 0.45927519491600444
Loss in iteration 182 : 0.45927066260387034
Loss in iteration 183 : 0.45926620592461337
Loss in iteration 184 : 0.4592618236132378
Loss in iteration 185 : 0.4592575142781436
Loss in iteration 186 : 0.45925327676228006
Loss in iteration 187 : 0.4592491097018753
Loss in iteration 188 : 0.45924501197983103
Loss in iteration 189 : 0.45924098230606714
Loss in iteration 190 : 0.4592370195604802
Loss in iteration 191 : 0.45923312254807697
Loss in iteration 192 : 0.4592292901388858
Loss in iteration 193 : 0.45922552122895155
Loss in iteration 194 : 0.4592218146919417
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.7865, training accuracy 0.7895, time elapsed: 5769 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6764423585722852
Loss in iteration 3 : 0.6408918116599482
Loss in iteration 4 : 0.6157861866124222
Loss in iteration 5 : 0.5797556513211073
Loss in iteration 6 : 0.5586815183147685
Loss in iteration 7 : 0.5340349733563164
Loss in iteration 8 : 0.520604549460244
Loss in iteration 9 : 0.507201103250287
Loss in iteration 10 : 0.4986006236673751
Loss in iteration 11 : 0.4941922562804369
Loss in iteration 12 : 0.48851307247089865
Loss in iteration 13 : 0.4878108751271057
Loss in iteration 14 : 0.48510674270617915
Loss in iteration 15 : 0.48372086646829937
Loss in iteration 16 : 0.4837947814017478
Loss in iteration 17 : 0.48206563716508943
Loss in iteration 18 : 0.48200022048592384
Loss in iteration 19 : 0.48170657531649896
Loss in iteration 20 : 0.4803883987166546
Loss in iteration 21 : 0.480248832639329
Loss in iteration 22 : 0.4793981672052653
Loss in iteration 23 : 0.47813108901612883
Loss in iteration 24 : 0.47768211812970346
Loss in iteration 25 : 0.4765727647328432
Loss in iteration 26 : 0.4754493849758509
Loss in iteration 27 : 0.47486008280518566
Loss in iteration 28 : 0.47370964863169734
Loss in iteration 29 : 0.4727055164153832
Loss in iteration 30 : 0.4719774673061843
Loss in iteration 31 : 0.4708172904895018
Loss in iteration 32 : 0.46990406615818453
Loss in iteration 33 : 0.4691267943585367
Loss in iteration 34 : 0.46810116511759825
Loss in iteration 35 : 0.46738038880482113
Loss in iteration 36 : 0.46670249638418226
Loss in iteration 37 : 0.4659207500902825
Loss in iteration 38 : 0.4654193899500706
Loss in iteration 39 : 0.4648934428494162
Loss in iteration 40 : 0.4643753507792824
Loss in iteration 41 : 0.46407978679750456
Loss in iteration 42 : 0.46373360312653394
Loss in iteration 43 : 0.46346416292895926
Loss in iteration 44 : 0.46332932663386633
Loss in iteration 45 : 0.4631319455081978
Loss in iteration 46 : 0.4630234765173426
Loss in iteration 47 : 0.46295804498932835
Loss in iteration 48 : 0.4628416020869995
Loss in iteration 49 : 0.462805052706786
Loss in iteration 50 : 0.4627583662774982
Loss in iteration 51 : 0.4626932269035098
Loss in iteration 52 : 0.46268471042021925
Loss in iteration 53 : 0.4626413624365822
Loss in iteration 54 : 0.4626030703930173
Loss in iteration 55 : 0.46258855125250187
Loss in iteration 56 : 0.4625372824058214
Loss in iteration 57 : 0.4625038445145284
Loss in iteration 58 : 0.4624704399341848
Loss in iteration 59 : 0.4624142170393451
Loss in iteration 60 : 0.4623772395436829
Loss in iteration 61 : 0.4623269035024697
Loss in iteration 62 : 0.462268095090394
Loss in iteration 63 : 0.46222129964873543
Loss in iteration 64 : 0.46215876758260727
Loss in iteration 65 : 0.4620994207343539
Loss in iteration 66 : 0.4620457439456857
Loss in iteration 67 : 0.46198249366847516
Loss in iteration 68 : 0.4619286302180237
Loss in iteration 69 : 0.4618756316575065
Loss in iteration 70 : 0.4618194751770015
Loss in iteration 71 : 0.4617723653985516
Loss in iteration 72 : 0.461723192651326
Loss in iteration 73 : 0.4616754792535825
Loss in iteration 74 : 0.4616341410458576
Loss in iteration 75 : 0.4615905338114085
Loss in iteration 76 : 0.46155081060016256
Loss in iteration 77 : 0.4615142832334284
Loss in iteration 78 : 0.4614761993124913
Loss in iteration 79 : 0.46144204833287816
Loss in iteration 80 : 0.4614085323753865
Loss in iteration 81 : 0.4613745946944948
Loss in iteration 82 : 0.4613438046526737
Loss in iteration 83 : 0.46131257502398015
Loss in iteration 84 : 0.4612820524536522
Loss in iteration 85 : 0.46125356221561126
Loss in iteration 86 : 0.4612243303111518
Loss in iteration 87 : 0.46119623209668725
Loss in iteration 88 : 0.4611689935413167
Loss in iteration 89 : 0.4611411608348228
Loss in iteration 90 : 0.4611144853273163
Loss in iteration 91 : 0.46108796748571457
Loss in iteration 92 : 0.461061296968888
Loss in iteration 93 : 0.4610355991315834
Loss in iteration 94 : 0.4610097537525894
Loss in iteration 95 : 0.4609841290550913
Loss in iteration 96 : 0.4609591750799972
Loss in iteration 97 : 0.46093406798831427
Loss in iteration 98 : 0.46090945156988694
Loss in iteration 99 : 0.46088529442206805
Loss in iteration 100 : 0.46086117921239816
Loss in iteration 101 : 0.460837680524447
Loss in iteration 102 : 0.460814493539456
Loss in iteration 103 : 0.46079152411623553
Loss in iteration 104 : 0.46076912446063323
Loss in iteration 105 : 0.46074694073108935
Loss in iteration 106 : 0.46072509069941503
Loss in iteration 107 : 0.4607037095364523
Loss in iteration 108 : 0.46068253074492455
Loss in iteration 109 : 0.4606617461048473
Loss in iteration 110 : 0.460641325833398
Loss in iteration 111 : 0.46062112511725606
Loss in iteration 112 : 0.4606013111088978
Loss in iteration 113 : 0.4605817747155183
Loss in iteration 114 : 0.4605624832433283
Loss in iteration 115 : 0.4605435404998653
Loss in iteration 116 : 0.46052482876402917
Loss in iteration 117 : 0.4605063819575885
Loss in iteration 118 : 0.46048823223340724
Loss in iteration 119 : 0.4604702889735546
Loss in iteration 120 : 0.46045260751067496
Loss in iteration 121 : 0.46043517070926804
Loss in iteration 122 : 0.4604179317751355
Loss in iteration 123 : 0.4604009413691592
Loss in iteration 124 : 0.4603841621272169
Loss in iteration 125 : 0.4603675845734141
Loss in iteration 126 : 0.46035123893187985
Loss in iteration 127 : 0.46033508646828225
Loss in iteration 128 : 0.4603191384605238
Loss in iteration 129 : 0.46030340340400155
Loss in iteration 130 : 0.4602878535155807
Loss in iteration 131 : 0.46027250729696023
Loss in iteration 132 : 0.46025735858858113
Loss in iteration 133 : 0.46024239329150685
Loss in iteration 134 : 0.4602276265158196
Loss in iteration 135 : 0.4602130447231302
Loss in iteration 136 : 0.4601986441903347
Loss in iteration 137 : 0.460184432526989
Loss in iteration 138 : 0.4601703960261749
Loss in iteration 139 : 0.4601565373237789
Loss in iteration 140 : 0.46014285727048776
Loss in iteration 141 : 0.46012934579606635
Loss in iteration 142 : 0.4601160072422496
Loss in iteration 143 : 0.46010283762314
Loss in iteration 144 : 0.46008983099826495
Loss in iteration 145 : 0.46007699040029154
Loss in iteration 146 : 0.4600643098455626
Loss in iteration 147 : 0.4600517868857913
Loss in iteration 148 : 0.46003942235749246
Loss in iteration 149 : 0.46002721048807804
Loss in iteration 150 : 0.4600151507531965
Loss in iteration 151 : 0.46000324180634294
Loss in iteration 152 : 0.45999147905796745
Loss in iteration 153 : 0.4599798624981594
Loss in iteration 154 : 0.4599683894594454
Loss in iteration 155 : 0.45995705689739186
Loss in iteration 156 : 0.4599458645726681
Loss in iteration 157 : 0.4599348094246449
Loss in iteration 158 : 0.4599238896173339
Loss in iteration 159 : 0.4599131042701746
Loss in iteration 160 : 0.45990245047202644
Loss in iteration 161 : 0.4598919270220332
Loss in iteration 162 : 0.45988153242725754
Loss in iteration 163 : 0.4598712642525365
Loss in iteration 164 : 0.45986112151692893
Loss in iteration 165 : 0.4598511023800808
Loss in iteration 166 : 0.4598412049237386
Loss in iteration 167 : 0.45983142808571487
Loss in iteration 168 : 0.4598217699131143
Loss in iteration 169 : 0.4598122288716639
Loss in iteration 170 : 0.45980280371807497
Loss in iteration 171 : 0.4597934925972808
Loss in iteration 172 : 0.4597842942163471
Loss in iteration 173 : 0.45977520718548076
Loss in iteration 174 : 0.4597662298407606
Loss in iteration 175 : 0.4597573609805789
Loss in iteration 176 : 0.4597485991327426
Loss in iteration 177 : 0.45973994282143044
Loss in iteration 178 : 0.459731390845368
Loss in iteration 179 : 0.45972294173532424
Loss in iteration 180 : 0.4597145941730723
Loss in iteration 181 : 0.4597063469369528
Loss in iteration 182 : 0.4596981986260194
Loss in iteration 183 : 0.4596901480279065
Loss in iteration 184 : 0.4596821939054544
Loss in iteration 185 : 0.45967433495026383
Loss in iteration 186 : 0.4596665700109337
Loss in iteration 187 : 0.4596588978617833
Loss in iteration 188 : 0.45965131729149583
Loss in iteration 189 : 0.4596438271850177
Loss in iteration 190 : 0.4596364263536856
Loss in iteration 191 : 0.45962911366801806
Loss in iteration 192 : 0.45962188803680426
Loss in iteration 193 : 0.4596147483228291
Loss in iteration 194 : 0.45960769345832897
Loss in iteration 195 : 0.4596007223763505
Loss in iteration 196 : 0.4595938339981496
Loss in iteration 197 : 0.4595870273034288
Loss in iteration 198 : 0.45958030125692234
Loss in iteration 199 : 0.4595736548371135
Loss in iteration 200 : 0.45956708706057603
Testing accuracy  of updater 1 on alg 0 with rate 0.7 = 0.788, training accuracy 0.789375, time elapsed: 7288 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.677927801944716
Loss in iteration 3 : 0.6634198106997475
Loss in iteration 4 : 0.6368166143880444
Loss in iteration 5 : 0.6155023551698124
Loss in iteration 6 : 0.5952353833474601
Loss in iteration 7 : 0.5722964750836272
Loss in iteration 8 : 0.5567599282988421
Loss in iteration 9 : 0.5416863982168213
Loss in iteration 10 : 0.5276334114792539
Loss in iteration 11 : 0.5188898784711614
Loss in iteration 12 : 0.5106589911015653
Loss in iteration 13 : 0.5030237741099556
Loss in iteration 14 : 0.4987405439836931
Loss in iteration 15 : 0.4951839053996713
Loss in iteration 16 : 0.49130118696265823
Loss in iteration 17 : 0.4890014769937467
Loss in iteration 18 : 0.4875809638342375
Loss in iteration 19 : 0.4855750886708063
Loss in iteration 20 : 0.4839133671189926
Loss in iteration 21 : 0.4831489127870124
Loss in iteration 22 : 0.48223982239105556
Loss in iteration 23 : 0.4810212193732077
Loss in iteration 24 : 0.4802597726326553
Loss in iteration 25 : 0.47977382056148293
Loss in iteration 26 : 0.4789541410546062
Loss in iteration 27 : 0.47806629953209256
Loss in iteration 28 : 0.4774954588748637
Loss in iteration 29 : 0.4769243768992699
Loss in iteration 30 : 0.47612143663707523
Loss in iteration 31 : 0.47539779495655954
Loss in iteration 32 : 0.47487638483039324
Loss in iteration 33 : 0.47428659159559067
Loss in iteration 34 : 0.4735914815788468
Loss in iteration 35 : 0.4730162121402363
Loss in iteration 36 : 0.47253854580826815
Loss in iteration 37 : 0.47197602647143205
Loss in iteration 38 : 0.4713805206014818
Loss in iteration 39 : 0.47088620180168533
Loss in iteration 40 : 0.47042237503022727
Loss in iteration 41 : 0.4698972962164539
Loss in iteration 42 : 0.4693917915409688
Loss in iteration 43 : 0.46896741661472197
Loss in iteration 44 : 0.4685494117915342
Loss in iteration 45 : 0.468107294161691
Loss in iteration 46 : 0.46771043800052786
Loss in iteration 47 : 0.4673690327701525
Loss in iteration 48 : 0.467024257340003
Loss in iteration 49 : 0.4666805408095482
Loss in iteration 50 : 0.46638413502091
Loss in iteration 51 : 0.46612036704400694
Loss in iteration 52 : 0.46585453158146894
Loss in iteration 53 : 0.465605898030165
Loss in iteration 54 : 0.46539701627549684
Loss in iteration 55 : 0.46520509998011705
Loss in iteration 56 : 0.46501477596373
Loss in iteration 57 : 0.46484526138003956
Loss in iteration 58 : 0.4647010848713385
Loss in iteration 59 : 0.4645621323581477
Loss in iteration 60 : 0.4644263902270321
Loss in iteration 61 : 0.4643078001305867
Loss in iteration 62 : 0.46420211454238985
Loss in iteration 63 : 0.4640970540886467
Loss in iteration 64 : 0.4639974773673077
Loss in iteration 65 : 0.4639107189688725
Loss in iteration 66 : 0.4638296991246483
Loss in iteration 67 : 0.46374899213775767
Loss in iteration 68 : 0.4636744620695142
Loss in iteration 69 : 0.46360781034237514
Loss in iteration 70 : 0.4635427017153674
Loss in iteration 71 : 0.4634783300945266
Loss in iteration 72 : 0.46341914148198854
Loss in iteration 73 : 0.46336391026974816
Loss in iteration 74 : 0.46330872896605096
Loss in iteration 75 : 0.4632550628562291
Loss in iteration 76 : 0.46320524387476963
Loss in iteration 77 : 0.4631570656433626
Loss in iteration 78 : 0.463108793391333
Loss in iteration 79 : 0.46306226638558184
Loss in iteration 80 : 0.46301806621901326
Loss in iteration 81 : 0.4629742486782982
Loss in iteration 82 : 0.4629305845109057
Loss in iteration 83 : 0.46288852020621624
Loss in iteration 84 : 0.4628477561935051
Loss in iteration 85 : 0.4628071255459963
Loss in iteration 86 : 0.4627671230774457
Loss in iteration 87 : 0.4627285465957585
Loss in iteration 88 : 0.4626907778244014
Loss in iteration 89 : 0.4626532973436464
Loss in iteration 90 : 0.4626166962725737
Loss in iteration 91 : 0.462581206000347
Loss in iteration 92 : 0.46254624528911237
Loss in iteration 93 : 0.4625117228018419
Loss in iteration 94 : 0.46247808933612444
Loss in iteration 95 : 0.46244527393814044
Loss in iteration 96 : 0.4624128968561015
Loss in iteration 97 : 0.46238107489346447
Loss in iteration 98 : 0.4623500541158211
Loss in iteration 99 : 0.46231964501058254
Loss in iteration 100 : 0.46228965285117585
Loss in iteration 101 : 0.46226023317195913
Loss in iteration 102 : 0.4622314620922174
Loss in iteration 103 : 0.4622031521258108
Loss in iteration 104 : 0.46217524416243644
Loss in iteration 105 : 0.4621478658537461
Loss in iteration 106 : 0.4621210014043624
Loss in iteration 107 : 0.46209452028264514
Loss in iteration 108 : 0.4620684364274858
Loss in iteration 109 : 0.46204282418535564
Loss in iteration 110 : 0.4620176272257111
Loss in iteration 111 : 0.4619927698521061
Loss in iteration 112 : 0.46196828609966295
Loss in iteration 113 : 0.4619442007436674
Loss in iteration 114 : 0.46192045254630343
Loss in iteration 115 : 0.461897008435773
Loss in iteration 116 : 0.4618739001064384
Loss in iteration 117 : 0.4618511235289366
Loss in iteration 118 : 0.4618286318799121
Loss in iteration 119 : 0.46180641816486667
Loss in iteration 120 : 0.46178450214014133
Loss in iteration 121 : 0.46176286621382945
Loss in iteration 122 : 0.4617414802688216
Loss in iteration 123 : 0.4617203480102909
Loss in iteration 124 : 0.4616994766600432
Loss in iteration 125 : 0.4616788463085652
Loss in iteration 126 : 0.4616584413090948
Loss in iteration 127 : 0.4616382681229085
Loss in iteration 128 : 0.4616183258982322
Loss in iteration 129 : 0.4615985983929906
Loss in iteration 130 : 0.4615790793939484
Loss in iteration 131 : 0.4615597734867832
Loss in iteration 132 : 0.46154067518061426
Loss in iteration 133 : 0.46152177278221773
Loss in iteration 134 : 0.4615030644037353
Loss in iteration 135 : 0.4614845513253938
Loss in iteration 136 : 0.4614662265178425
Loss in iteration 137 : 0.46144808254867703
Loss in iteration 138 : 0.4614301191592498
Loss in iteration 139 : 0.4614123352049696
Loss in iteration 140 : 0.4613947242495253
Loss in iteration 141 : 0.4613772820068113
Loss in iteration 142 : 0.4613600082487672
Loss in iteration 143 : 0.4613429003343236
Loss in iteration 144 : 0.4613259530482222
Loss in iteration 145 : 0.4613091638664058
Loss in iteration 146 : 0.46129253194712644
Loss in iteration 147 : 0.4612760540899817
Loss in iteration 148 : 0.461259726426926
Loss in iteration 149 : 0.46124354732417633
Loss in iteration 150 : 0.4612275153770638
Loss in iteration 151 : 0.46121162747829414
Loss in iteration 152 : 0.461195880840657
Loss in iteration 153 : 0.4611802741097275
Loss in iteration 154 : 0.46116480547449823
Loss in iteration 155 : 0.46114947216492835
Loss in iteration 156 : 0.4611342720866933
Loss in iteration 157 : 0.46111920387812366
Loss in iteration 158 : 0.4611042655774319
Loss in iteration 159 : 0.4610894548650974
Loss in iteration 160 : 0.46107477009209535
Loss in iteration 161 : 0.4610602098619989
Loss in iteration 162 : 0.46104577228379157
Loss in iteration 163 : 0.4610314554642644
Loss in iteration 164 : 0.4610172580017785
Loss in iteration 165 : 0.46100317847696115
Loss in iteration 166 : 0.46098921515776353
Loss in iteration 167 : 0.4609753664722494
Loss in iteration 168 : 0.4609616311450422
Loss in iteration 169 : 0.46094800778405987
Loss in iteration 170 : 0.4609344948563089
Loss in iteration 171 : 0.4609210910253788
Loss in iteration 172 : 0.46090779509645186
Loss in iteration 173 : 0.4608946057515136
Loss in iteration 174 : 0.4608815216467759
Loss in iteration 175 : 0.46086854160318314
Loss in iteration 176 : 0.46085566448422083
Loss in iteration 177 : 0.46084288906510745
Loss in iteration 178 : 0.4608302141574834
Loss in iteration 179 : 0.4608176386873634
Loss in iteration 180 : 0.4608051615764037
Loss in iteration 181 : 0.46079278170273047
Loss in iteration 182 : 0.46078049800409016
Loss in iteration 183 : 0.4607683094862791
Loss in iteration 184 : 0.4607562151361114
Loss in iteration 185 : 0.4607442139311132
Loss in iteration 186 : 0.4607323049061121
Loss in iteration 187 : 0.46072048712965513
Loss in iteration 188 : 0.4607087596536345
Loss in iteration 189 : 0.4606971215417911
Loss in iteration 190 : 0.46068557190345466
Loss in iteration 191 : 0.4606741098623644
Loss in iteration 192 : 0.4606627345349054
Loss in iteration 193 : 0.46065144505890576
Loss in iteration 194 : 0.4606402406041475
Loss in iteration 195 : 0.46062912034581194
Loss in iteration 196 : 0.4606180834605546
Loss in iteration 197 : 0.4606071291479057
Loss in iteration 198 : 0.4605962566280777
Loss in iteration 199 : 0.4605854651242798
Loss in iteration 200 : 0.46057475386744084
Testing accuracy  of updater 1 on alg 0 with rate 0.4 = 0.7875, training accuracy 0.789375, time elapsed: 7442 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6878658918511905
Loss in iteration 3 : 0.6807027170966676
Loss in iteration 4 : 0.6739294871347783
Loss in iteration 5 : 0.6668886231145078
Loss in iteration 6 : 0.6579684441529687
Loss in iteration 7 : 0.6470866160584807
Loss in iteration 8 : 0.6358501545886249
Loss in iteration 9 : 0.6258176075743542
Loss in iteration 10 : 0.6170799796352966
Loss in iteration 11 : 0.6086149317782449
Loss in iteration 12 : 0.599694707618262
Loss in iteration 13 : 0.5906248408492141
Loss in iteration 14 : 0.5822230682148323
Loss in iteration 15 : 0.5749146859145032
Loss in iteration 16 : 0.5684454167556627
Loss in iteration 17 : 0.5623019911408129
Loss in iteration 18 : 0.5562424100141591
Loss in iteration 19 : 0.5504326885724572
Loss in iteration 20 : 0.5451825083412973
Loss in iteration 21 : 0.5406159468252321
Loss in iteration 22 : 0.5365812551388363
Loss in iteration 23 : 0.532822110633772
Loss in iteration 24 : 0.5291993831674038
Loss in iteration 25 : 0.5257602506283421
Loss in iteration 26 : 0.5226337961602794
Loss in iteration 27 : 0.5198843942263013
Loss in iteration 28 : 0.5174547948874173
Loss in iteration 29 : 0.5152224074099973
Loss in iteration 30 : 0.5130951623791216
Loss in iteration 31 : 0.5110616419569136
Loss in iteration 32 : 0.5091677590336269
Loss in iteration 33 : 0.5074555801347291
Loss in iteration 34 : 0.5059201648144043
Loss in iteration 35 : 0.5045134235986847
Loss in iteration 36 : 0.5031813971044398
Loss in iteration 37 : 0.5018989110339537
Loss in iteration 38 : 0.5006757386534603
Loss in iteration 39 : 0.4995358671741111
Loss in iteration 40 : 0.49849131296566734
Loss in iteration 41 : 0.4975310688101293
Loss in iteration 42 : 0.496629855966314
Loss in iteration 43 : 0.4957657865986875
Loss in iteration 44 : 0.4949320733439208
Loss in iteration 45 : 0.4941355934582124
Loss in iteration 46 : 0.49338616832575266
Loss in iteration 47 : 0.49268622853532734
Loss in iteration 48 : 0.4920281782595225
Loss in iteration 49 : 0.4913997180803061
Loss in iteration 50 : 0.490791653573255
Loss in iteration 51 : 0.4902020984228689
Loss in iteration 52 : 0.48963482150678056
Loss in iteration 53 : 0.4890941049426021
Loss in iteration 54 : 0.4885804409233666
Loss in iteration 55 : 0.48808989364799865
Loss in iteration 56 : 0.48761679413389564
Loss in iteration 57 : 0.48715718453727674
Loss in iteration 58 : 0.4867104508352633
Loss in iteration 59 : 0.48627837209039115
Loss in iteration 60 : 0.4858627606230023
Loss in iteration 61 : 0.48546360009864853
Loss in iteration 62 : 0.4850788584947807
Loss in iteration 63 : 0.4847057632672365
Loss in iteration 64 : 0.48434236542817505
Loss in iteration 65 : 0.4839882588628249
Loss in iteration 66 : 0.48364412955035746
Loss in iteration 67 : 0.48331067569814223
Loss in iteration 68 : 0.48298776009242944
Loss in iteration 69 : 0.4826743239164927
Loss in iteration 70 : 0.4823689670102037
Loss in iteration 71 : 0.4820706668370648
Loss in iteration 72 : 0.48177912080797397
Loss in iteration 73 : 0.48149455449285355
Loss in iteration 74 : 0.4812172299497166
Loss in iteration 75 : 0.48094704354828127
Loss in iteration 76 : 0.4806834648577372
Loss in iteration 77 : 0.48042578893623633
Loss in iteration 78 : 0.4801734694714032
Loss in iteration 79 : 0.47992629218374977
Loss in iteration 80 : 0.47968430268418694
Loss in iteration 81 : 0.4794475843425531
Loss in iteration 82 : 0.4792160630783826
Loss in iteration 83 : 0.47898946306799467
Loss in iteration 84 : 0.4787674123922142
Loss in iteration 85 : 0.4785495982493619
Loss in iteration 86 : 0.4783358584296298
Loss in iteration 87 : 0.47812616052899404
Loss in iteration 88 : 0.47792050441025413
Loss in iteration 89 : 0.47771882725818215
Loss in iteration 90 : 0.47752097325680753
Loss in iteration 91 : 0.4773267350358431
Loss in iteration 92 : 0.4771359252626399
Loss in iteration 93 : 0.4769484250493499
Loss in iteration 94 : 0.4767641815804593
Loss in iteration 95 : 0.4765831661917653
Loss in iteration 96 : 0.47640532792255585
Loss in iteration 97 : 0.4762305736654916
Loss in iteration 98 : 0.47605878249469663
Loss in iteration 99 : 0.4758898378862781
Loss in iteration 100 : 0.47572365304722875
Loss in iteration 101 : 0.4755601739549757
Loss in iteration 102 : 0.4753993623108987
Loss in iteration 103 : 0.4752411733138024
Loss in iteration 104 : 0.4750855436433983
Loss in iteration 105 : 0.4749323953998847
Loss in iteration 106 : 0.47478165024688085
Loss in iteration 107 : 0.4746332424993802
Loss in iteration 108 : 0.47448712278907057
Loss in iteration 109 : 0.474343251698964
Loss in iteration 110 : 0.4742015893931522
Loss in iteration 111 : 0.47406208869283134
Loss in iteration 112 : 0.4739246953657457
Loss in iteration 113 : 0.4737893539927975
Loss in iteration 114 : 0.47365601448764505
Loss in iteration 115 : 0.47352463488388474
Loss in iteration 116 : 0.4733951792712605
Loss in iteration 117 : 0.47326761311326193
Loss in iteration 118 : 0.47314189943502516
Loss in iteration 119 : 0.47301799813244066
Loss in iteration 120 : 0.4728958682064387
Loss in iteration 121 : 0.47277547088256083
Loss in iteration 122 : 0.47265677140380546
Loss in iteration 123 : 0.4725397385685758
Loss in iteration 124 : 0.47242434271593786
Loss in iteration 125 : 0.4723105537225426
Loss in iteration 126 : 0.4721983402662162
Loss in iteration 127 : 0.47208767055739864
Loss in iteration 128 : 0.47197851376201805
Loss in iteration 129 : 0.4718708410492526
Loss in iteration 130 : 0.47176462564638094
Loss in iteration 131 : 0.4716598420394222
Loss in iteration 132 : 0.4715564649787484
Loss in iteration 133 : 0.47145446894683124
Loss in iteration 134 : 0.4713538283237593
Loss in iteration 135 : 0.47125451799588697
Loss in iteration 136 : 0.47115651392007557
Loss in iteration 137 : 0.4710597932789331
Loss in iteration 138 : 0.4709643341937727
Loss in iteration 139 : 0.4708701152502489
Loss in iteration 140 : 0.47077711516181103
Loss in iteration 141 : 0.4706853127460128
Loss in iteration 142 : 0.4705946871565275
Loss in iteration 143 : 0.4705052181629933
Loss in iteration 144 : 0.470416886281255
Loss in iteration 145 : 0.47032967269034576
Loss in iteration 146 : 0.47024355902071036
Loss in iteration 147 : 0.4701585271640597
Loss in iteration 148 : 0.4700745592129648
Loss in iteration 149 : 0.46999163753573125
Loss in iteration 150 : 0.46990974490574333
Loss in iteration 151 : 0.469828864585919
Loss in iteration 152 : 0.46974898031670737
Loss in iteration 153 : 0.4696700762272031
Loss in iteration 154 : 0.46959213673390354
Loss in iteration 155 : 0.4695151464872088
Loss in iteration 156 : 0.46943909038423576
Loss in iteration 157 : 0.4693639536213384
Loss in iteration 158 : 0.46928972174006856
Loss in iteration 159 : 0.4692163806335855
Loss in iteration 160 : 0.46914391651204473
Loss in iteration 161 : 0.469072315851799
Loss in iteration 162 : 0.46900156535885196
Loss in iteration 163 : 0.46893165196233205
Loss in iteration 164 : 0.4688625628320091
Loss in iteration 165 : 0.4687942854000297
Loss in iteration 166 : 0.46872680736836875
Loss in iteration 167 : 0.4686601166961049
Loss in iteration 168 : 0.46859420157455395
Loss in iteration 169 : 0.4685290504044649
Loss in iteration 170 : 0.46846465178556757
Loss in iteration 171 : 0.4684009945191623
Loss in iteration 172 : 0.4683380676162504
Loss in iteration 173 : 0.4682758603018276
Loss in iteration 174 : 0.46821436201026945
Loss in iteration 175 : 0.4681535623734201
Loss in iteration 176 : 0.46809345120736484
Loss in iteration 177 : 0.46803401850362436
Loss in iteration 178 : 0.46797525442673815
Loss in iteration 179 : 0.4679171493159254
Loss in iteration 180 : 0.46785969368645375
Loss in iteration 181 : 0.4678028782274632
Loss in iteration 182 : 0.4677466937958727
Loss in iteration 183 : 0.46769113140857704
Loss in iteration 184 : 0.4676361822358424
Loss in iteration 185 : 0.46758183759756117
Loss in iteration 186 : 0.46752808896199655
Loss in iteration 187 : 0.4674749279452183
Loss in iteration 188 : 0.4674223463094256
Loss in iteration 189 : 0.4673703359594491
Loss in iteration 190 : 0.4673188889380543
Loss in iteration 191 : 0.46726799742135394
Loss in iteration 192 : 0.4672176537153766
Loss in iteration 193 : 0.4671678502539584
Loss in iteration 194 : 0.46711857959733166
Loss in iteration 195 : 0.4670698344305021
Loss in iteration 196 : 0.4670216075608554
Loss in iteration 197 : 0.4669738919150707
Loss in iteration 198 : 0.4669266805358437
Loss in iteration 199 : 0.4668799665790116
Loss in iteration 200 : 0.46683374331132077
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.786, training accuracy 0.78875, time elapsed: 5351 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 4.016337962504759
Loss in iteration 3 : 42.50079140495107
Loss in iteration 4 : 1.34098790281544
Loss in iteration 5 : 20.7703247568897
Loss in iteration 6 : 4.6613034118998256
Loss in iteration 7 : 19.46325296405382
Loss in iteration 8 : 7.3706148077095675
Loss in iteration 9 : 12.1164605697753
Loss in iteration 10 : 11.860532355582002
Loss in iteration 11 : 7.767552345407422
Loss in iteration 12 : 7.852688576732801
Loss in iteration 13 : 6.041902174972181
Loss in iteration 14 : 5.547030723202693
Loss in iteration 15 : 4.615297343999991
Loss in iteration 16 : 4.258664681104033
Loss in iteration 17 : 3.9147500159467787
Loss in iteration 18 : 3.74991324101041
Loss in iteration 19 : 3.5714106103577943
Loss in iteration 20 : 3.4669286724385993
Loss in iteration 21 : 3.3673763127174534
Loss in iteration 22 : 3.2722016231195665
Loss in iteration 23 : 3.1792720495428406
Loss in iteration 24 : 3.0921629230412124
Loss in iteration 25 : 3.0102428231250173
Loss in iteration 26 : 2.9700342839289755
Loss in iteration 27 : 2.9863717026568812
Loss in iteration 28 : 3.3054091620549633
Loss in iteration 29 : 3.903953755699833
Loss in iteration 30 : 6.572198296404854
Loss in iteration 31 : 6.707185244861064
Loss in iteration 32 : 9.984871282708282
Loss in iteration 33 : 6.433584676633967
Loss in iteration 34 : 8.869845279207537
Loss in iteration 35 : 7.052267480835784
Loss in iteration 36 : 8.2940115056001
Loss in iteration 37 : 6.5584990897935205
Loss in iteration 38 : 7.215849315672162
Loss in iteration 39 : 6.035675146856264
Loss in iteration 40 : 6.297184467435189
Loss in iteration 41 : 5.4676272523638225
Loss in iteration 42 : 5.526578377162184
Loss in iteration 43 : 4.954389049467675
Loss in iteration 44 : 5.067758336459345
Loss in iteration 45 : 4.682253840805408
Loss in iteration 46 : 4.9390470910296695
Loss in iteration 47 : 4.646012768924976
Loss in iteration 48 : 5.08526436080914
Loss in iteration 49 : 4.73878209815634
Loss in iteration 50 : 5.407379719812931
Loss in iteration 51 : 5.018141541691363
Loss in iteration 52 : 5.928644770738484
Loss in iteration 53 : 5.338793878847565
Loss in iteration 54 : 6.349737808106546
Loss in iteration 55 : 5.4962269677751125
Loss in iteration 56 : 6.447996614487127
Loss in iteration 57 : 5.48534042620383
Loss in iteration 58 : 6.3065729413880804
Loss in iteration 59 : 5.395761267867281
Loss in iteration 60 : 6.062786311165834
Loss in iteration 61 : 5.238397992947776
Loss in iteration 62 : 5.778797292674787
Loss in iteration 63 : 5.071282968765207
Loss in iteration 64 : 5.545409152424112
Loss in iteration 65 : 4.959385816909762
Loss in iteration 66 : 5.426255087280014
Loss in iteration 67 : 4.912105062255777
Loss in iteration 68 : 5.416511903188536
Loss in iteration 69 : 4.9522651038874725
Loss in iteration 70 : 5.520521279082584
Loss in iteration 71 : 5.062832642475017
Loss in iteration 72 : 5.692361505089935
Loss in iteration 73 : 5.185590262251545
Loss in iteration 74 : 5.852072140972369
Loss in iteration 75 : 5.28826357317242
Loss in iteration 76 : 5.961474864878847
Loss in iteration 77 : 5.339965595904988
Loss in iteration 78 : 5.990958430398531
Loss in iteration 79 : 5.338429377458414
Loss in iteration 80 : 5.950818355269697
Loss in iteration 81 : 5.2995163965508905
Loss in iteration 82 : 5.872041352041509
Loss in iteration 83 : 5.237653662372395
Loss in iteration 84 : 5.781050137386954
Loss in iteration 85 : 5.172940974660583
Loss in iteration 86 : 5.704260510611103
Loss in iteration 87 : 5.124091582887636
Loss in iteration 88 : 5.660332987327753
Loss in iteration 89 : 5.101689436925227
Loss in iteration 90 : 5.655170878424996
Loss in iteration 91 : 5.107646545500013
Loss in iteration 92 : 5.6832063903544885
Loss in iteration 93 : 5.134746721547796
Loss in iteration 94 : 5.729230287276206
Loss in iteration 95 : 5.170376008777291
Loss in iteration 96 : 5.774902296910447
Loss in iteration 97 : 5.201816183208792
Loss in iteration 98 : 5.805640310368645
Loss in iteration 99 : 5.220306886645044
Loss in iteration 100 : 5.814596198310328
Loss in iteration 101 : 5.223352214987693
Loss in iteration 102 : 5.803627781259622
Loss in iteration 103 : 5.2139182369296675
Loss in iteration 104 : 5.780552908465224
Loss in iteration 105 : 5.19800157317177
Loss in iteration 106 : 5.755067021657689
Loss in iteration 107 : 5.182292221210542
Loss in iteration 108 : 5.735534214301129
Loss in iteration 109 : 5.1721112167966075
Loss in iteration 110 : 5.72684019025848
Loss in iteration 111 : 5.1699217125449355
Loss in iteration 112 : 5.729437629601538
Loss in iteration 113 : 5.174952362456408
Loss in iteration 114 : 5.739891157404828
Loss in iteration 115 : 5.184046703963548
Loss in iteration 116 : 5.752720994293109
Loss in iteration 117 : 5.193230886740734
Loss in iteration 118 : 5.762679641167452
Loss in iteration 119 : 5.1992502684319115
Loss in iteration 120 : 5.766498701615391
Loss in iteration 121 : 5.200536026720306
Loss in iteration 122 : 5.763594972986147
Loss in iteration 123 : 5.197394036559047
Loss in iteration 124 : 5.755720952436021
Loss in iteration 125 : 5.191537706508955
Loss in iteration 126 : 5.745924054704947
Loss in iteration 127 : 5.185275156239423
Loss in iteration 128 : 5.737318946042292
Loss in iteration 129 : 5.180663516218634
Loss in iteration 130 : 5.732079772616399
Loss in iteration 131 : 5.17887985192924
Loss in iteration 132 : 5.730900979807044
Loss in iteration 133 : 5.179975330138411
Loss in iteration 134 : 5.733022805757072
Loss in iteration 135 : 5.183049742345261
Loss in iteration 136 : 5.736741353194359
Loss in iteration 137 : 5.1867249254680905
Loss in iteration 138 : 5.740154888714753
Loss in iteration 139 : 5.189693995988913
Loss in iteration 140 : 5.74183773623094
Loss in iteration 141 : 5.191137001460574
Loss in iteration 142 : 5.741217917027782
Loss in iteration 143 : 5.190889838405372
Loss in iteration 144 : 5.738592009838093
Loss in iteration 145 : 5.189364229716552
Loss in iteration 146 : 5.734850848351229
Loss in iteration 147 : 5.187297503621611
Loss in iteration 148 : 5.731067435221912
Loss in iteration 149 : 5.185446323718664
Loss in iteration 150 : 5.728107930244574
Loss in iteration 151 : 5.184332927323394
Loss in iteration 152 : 5.726385988145967
Loss in iteration 153 : 5.184117429374551
Loss in iteration 154 : 5.725814129548099
Loss in iteration 155 : 5.184616928080296
Loss in iteration 156 : 5.725933225552532
Loss in iteration 157 : 5.185438091675514
Loss in iteration 158 : 5.726143568078365
Loss in iteration 159 : 5.186155374718528
Loss in iteration 160 : 5.7259384517718885
Loss in iteration 161 : 5.186464023507944
Loss in iteration 162 : 5.725059110130957
Loss in iteration 163 : 5.186260724308005
Loss in iteration 164 : 5.723533953218672
Loss in iteration 165 : 5.185639554540414
Loss in iteration 166 : 5.721612522629409
Loss in iteration 167 : 5.184821738896666
Loss in iteration 168 : 5.719637716275543
Loss in iteration 169 : 5.18405576649836
Loss in iteration 170 : 5.717911336891466
Loss in iteration 171 : 5.183527153362018
Loss in iteration 172 : 5.716599359584454
Loss in iteration 173 : 5.183306650815454
Loss in iteration 174 : 5.715701316469725
Loss in iteration 175 : 5.1833474468360095
Loss in iteration 176 : 5.7150821204870095
Loss in iteration 177 : 5.183523160071853
Loss in iteration 178 : 5.7145436921762425
Loss in iteration 179 : 5.1836858495366345
Loss in iteration 180 : 5.713904301285661
Loss in iteration 181 : 5.183720311640221
Loss in iteration 182 : 5.7130569921192285
Loss in iteration 183 : 5.183576832352778
Loss in iteration 184 : 5.71199121413033
Loss in iteration 185 : 5.183275418612199
Loss in iteration 186 : 5.710777480828899
Loss in iteration 187 : 5.182885577876495
Loss in iteration 188 : 5.7095274859439575
Loss in iteration 189 : 5.182493200156759
Loss in iteration 190 : 5.708347985452492
Loss in iteration 191 : 5.182168250815919
Loss in iteration 192 : 5.707305313307721
Loss in iteration 193 : 5.181944109194635
Loss in iteration 194 : 5.706410558024259
Loss in iteration 195 : 5.181813380288734
Loss in iteration 196 : 5.705626496079916
Loss in iteration 197 : 5.181738470652628
Loss in iteration 198 : 5.704889748666152
Loss in iteration 199 : 5.181670485234112
Loss in iteration 200 : 5.704137653270398
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.76, training accuracy 0.76575, time elapsed: 4494 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 2.8221613825595044
Loss in iteration 3 : 29.333109726106414
Loss in iteration 4 : 0.8426875793204006
Loss in iteration 5 : 10.188635600706231
Loss in iteration 6 : 9.691811433651134
Loss in iteration 7 : 14.165975302571056
Loss in iteration 8 : 3.0775860644430275
Loss in iteration 9 : 6.130453386531507
Loss in iteration 10 : 9.740182897982534
Loss in iteration 11 : 4.305891032758406
Loss in iteration 12 : 4.7125363555438495
Loss in iteration 13 : 4.053843434734618
Loss in iteration 14 : 3.8446435849073133
Loss in iteration 15 : 3.096639106440169
Loss in iteration 16 : 2.905530566038703
Loss in iteration 17 : 2.7156951942095997
Loss in iteration 18 : 2.6263767437584025
Loss in iteration 19 : 2.5276698815003176
Loss in iteration 20 : 2.46556923160488
Loss in iteration 21 : 2.404156933584739
Loss in iteration 22 : 2.3393929733237777
Loss in iteration 23 : 2.270789190414491
Loss in iteration 24 : 2.200291956425337
Loss in iteration 25 : 2.1285636439941333
Loss in iteration 26 : 2.0674736267670553
Loss in iteration 27 : 2.0198852775827696
Loss in iteration 28 : 2.0612753323410113
Loss in iteration 29 : 2.270257891946144
Loss in iteration 30 : 3.2791421050688037
Loss in iteration 31 : 4.255839581165179
Loss in iteration 32 : 6.995119727411167
Loss in iteration 33 : 4.6574518820239605
Loss in iteration 34 : 6.484945874230201
Loss in iteration 35 : 5.1570481905401575
Loss in iteration 36 : 6.103805640487813
Loss in iteration 37 : 4.839323253914419
Loss in iteration 38 : 5.3306541479439975
Loss in iteration 39 : 4.50971362282096
Loss in iteration 40 : 4.640814921747735
Loss in iteration 41 : 3.9841841698799794
Loss in iteration 42 : 4.0233682083031
Loss in iteration 43 : 3.6103142314951358
Loss in iteration 44 : 3.650547374326405
Loss in iteration 45 : 3.3444484805017476
Loss in iteration 46 : 3.469179570595479
Loss in iteration 47 : 3.247424537195916
Loss in iteration 48 : 3.486934210930403
Loss in iteration 49 : 3.2743149305598083
Loss in iteration 50 : 3.657992989779989
Loss in iteration 51 : 3.3955102625496085
Loss in iteration 52 : 3.9401289585057993
Loss in iteration 53 : 3.6029128130137713
Loss in iteration 54 : 4.265022254714862
Loss in iteration 55 : 3.781531574538108
Loss in iteration 56 : 4.448837760801814
Loss in iteration 57 : 3.829067720845191
Loss in iteration 58 : 4.424407641505281
Loss in iteration 59 : 3.796849904880457
Loss in iteration 60 : 4.293655178712091
Loss in iteration 61 : 3.7139846157594634
Loss in iteration 62 : 4.11634606156753
Loss in iteration 63 : 3.607588043221747
Loss in iteration 64 : 3.945713696396702
Loss in iteration 65 : 3.5167523782420855
Loss in iteration 66 : 3.8297818891833364
Loss in iteration 67 : 3.4633514171484077
Loss in iteration 68 : 3.7859163165759866
Loss in iteration 69 : 3.462618929909532
Loss in iteration 70 : 3.819514658630642
Loss in iteration 71 : 3.513227604209331
Loss in iteration 72 : 3.9148213108391148
Loss in iteration 73 : 3.5953495792004047
Loss in iteration 74 : 4.034129123226511
Loss in iteration 75 : 3.675054774246565
Loss in iteration 76 : 4.130929898743044
Loss in iteration 77 : 3.726961031189823
Loss in iteration 78 : 4.1783139516451255
Loss in iteration 79 : 3.7424225908650444
Loss in iteration 80 : 4.173852327078976
Loss in iteration 81 : 3.7280794624937355
Loss in iteration 82 : 4.13337919108411
Loss in iteration 83 : 3.693975964398786
Loss in iteration 84 : 4.075640349626227
Loss in iteration 85 : 3.6513091379640787
Loss in iteration 86 : 4.01800049919081
Loss in iteration 87 : 3.6119770919712946
Loss in iteration 88 : 3.974781494994305
Loss in iteration 89 : 3.5858405503432387
Loss in iteration 90 : 3.9547939245813133
Loss in iteration 91 : 3.578270663346772
Loss in iteration 92 : 3.9598166939829604
Loss in iteration 93 : 3.5884006884188873
Loss in iteration 94 : 3.983974673601011
Loss in iteration 95 : 3.6096556863497935
Loss in iteration 96 : 4.015850342721197
Loss in iteration 97 : 3.6330143310571192
Loss in iteration 98 : 4.043443243767593
Loss in iteration 99 : 3.6509127777185877
Loss in iteration 100 : 4.058864262248095
Loss in iteration 101 : 3.659516404367866
Loss in iteration 102 : 4.060153350519379
Loss in iteration 103 : 3.6588465331435382
Loss in iteration 104 : 4.050272412900593
Loss in iteration 105 : 3.6515799766622927
Loss in iteration 106 : 4.0347432226841144
Loss in iteration 107 : 3.641602570176831
Loss in iteration 108 : 4.0193819821873875
Loss in iteration 109 : 3.6327319304813677
Loss in iteration 110 : 4.008666511826731
Loss in iteration 111 : 3.6276597389019996
Loss in iteration 112 : 4.004758655986472
Loss in iteration 113 : 3.627301930336851
Loss in iteration 114 : 4.00725247629776
Loss in iteration 115 : 3.6307934916974265
Loss in iteration 116 : 4.013725200119072
Loss in iteration 117 : 3.6361171751543617
Loss in iteration 118 : 4.020900630062324
Loss in iteration 119 : 3.6410405965310275
Loss in iteration 120 : 4.025928926627357
Loss in iteration 121 : 3.6439286260547865
Loss in iteration 122 : 4.0272608039430935
Loss in iteration 123 : 3.644153537097981
Loss in iteration 124 : 4.024874091844649
Loss in iteration 125 : 3.6420742833629856
Loss in iteration 126 : 4.019943170888323
Loss in iteration 127 : 3.638718355656355
Loss in iteration 128 : 4.014215123725982
Loss in iteration 129 : 3.635336882146113
Loss in iteration 130 : 4.009356745901408
Loss in iteration 131 : 3.6329785495323383
Loss in iteration 132 : 4.006458681727307
Loss in iteration 133 : 3.632194655566294
Loss in iteration 134 : 4.005801230659037
Loss in iteration 135 : 3.6329457530475358
Loss in iteration 136 : 4.006909214353125
Loss in iteration 137 : 3.6347151331419414
Loss in iteration 138 : 4.008839347941768
Loss in iteration 139 : 3.636761480944763
Loss in iteration 140 : 4.010571443897818
Loss in iteration 141 : 3.638399727543946
Loss in iteration 142 : 4.01135239277134
Loss in iteration 143 : 3.639207594881873
Loss in iteration 144 : 4.010883458553739
Loss in iteration 145 : 3.6391040653242817
Loss in iteration 146 : 4.009320367708501
Loss in iteration 147 : 3.638303059677233
Loss in iteration 148 : 4.007127950046727
Loss in iteration 149 : 3.637185029290023
Loss in iteration 150 : 4.004868625141078
Loss in iteration 151 : 3.6361436813316295
Loss in iteration 152 : 4.003005101097811
Loss in iteration 153 : 3.635459724438243
Loss in iteration 154 : 4.001774969992989
Loss in iteration 155 : 3.6352354570448404
Loss in iteration 156 : 4.0011617049287045
Loss in iteration 157 : 3.635399462260989
Loss in iteration 158 : 4.000952791643939
Loss in iteration 159 : 3.6357667405780365
Loss in iteration 160 : 4.000849895221256
Loss in iteration 161 : 3.636123897623868
Loss in iteration 162 : 4.00058509129827
Loss in iteration 163 : 3.636306204418908
Loss in iteration 164 : 4.000003520519674
Loss in iteration 165 : 3.636242610770863
Loss in iteration 166 : 3.99909178099244
Loss in iteration 167 : 3.6359604536396692
Loss in iteration 168 : 3.99795378410045
Loss in iteration 169 : 3.6355566542702613
Loss in iteration 170 : 3.9967528521968103
Loss in iteration 171 : 3.635151673479772
Loss in iteration 172 : 3.995645899075898
Loss in iteration 173 : 3.634844708713293
Loss in iteration 174 : 3.9947327222894637
Loss in iteration 175 : 3.634684549848185
Loss in iteration 176 : 3.994033940870421
Loss in iteration 177 : 3.634662678332181
Loss in iteration 178 : 3.9934992444275967
Loss in iteration 179 : 3.6347266908176996
Loss in iteration 180 : 3.993037452691248
Loss in iteration 181 : 3.6348057740120594
Loss in iteration 182 : 3.992554316387226
Loss in iteration 183 : 3.6348374898845868
Loss in iteration 184 : 3.9919840669887963
Loss in iteration 185 : 3.634786718602122
Loss in iteration 186 : 3.991305460810278
Loss in iteration 187 : 3.6346519750047683
Loss in iteration 188 : 3.9905400137079194
Loss in iteration 189 : 3.6344594403609114
Loss in iteration 190 : 3.989736491774501
Loss in iteration 191 : 3.6342490522361204
Loss in iteration 192 : 3.988949471187056
Loss in iteration 193 : 3.6340587692186714
Loss in iteration 194 : 3.9882201950381795
Loss in iteration 195 : 3.6339125432790302
Loss in iteration 196 : 3.9875655855360024
Loss in iteration 197 : 3.633815248270241
Loss in iteration 198 : 3.9869774476794344
Loss in iteration 199 : 3.6337548980962717
Loss in iteration 200 : 3.9864301478868205
Testing accuracy  of updater 2 on alg 0 with rate 7.0 = 0.7595, training accuracy 0.76575, time elapsed: 4752 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 1.6610396359867279
Loss in iteration 3 : 15.521226478266275
Loss in iteration 4 : 0.9449563731583576
Loss in iteration 5 : 5.1134525968812
Loss in iteration 6 : 11.04987711460128
Loss in iteration 7 : 1.169645390619575
Loss in iteration 8 : 6.150366853161244
Loss in iteration 9 : 3.539657283441055
Loss in iteration 10 : 4.535556727803099
Loss in iteration 11 : 2.7210345298211682
Loss in iteration 12 : 2.8631880663287896
Loss in iteration 13 : 2.128647519011871
Loss in iteration 14 : 2.036626794763658
Loss in iteration 15 : 1.737286518131491
Loss in iteration 16 : 1.6487895119604488
Loss in iteration 17 : 1.5752916471386225
Loss in iteration 18 : 1.539927380454737
Loss in iteration 19 : 1.511262060371376
Loss in iteration 20 : 1.4844061364726748
Loss in iteration 21 : 1.454341724518833
Loss in iteration 22 : 1.4191791255783948
Loss in iteration 23 : 1.3795717522311997
Loss in iteration 24 : 1.33611508590366
Loss in iteration 25 : 1.2897586291948104
Loss in iteration 26 : 1.2414051810692797
Loss in iteration 27 : 1.1920452353980366
Loss in iteration 28 : 1.1428812083052318
Loss in iteration 29 : 1.0962022989912885
Loss in iteration 30 : 1.0591608927896206
Loss in iteration 31 : 1.05361625877139
Loss in iteration 32 : 1.1899458553145679
Loss in iteration 33 : 1.7606501908060013
Loss in iteration 34 : 3.7796879954511104
Loss in iteration 35 : 3.6036429837551927
Loss in iteration 36 : 4.301682285811041
Loss in iteration 37 : 3.3477520417630373
Loss in iteration 38 : 3.908085756454461
Loss in iteration 39 : 3.199057450903747
Loss in iteration 40 : 3.3233707662625664
Loss in iteration 41 : 2.8603714835722935
Loss in iteration 42 : 2.780669988618995
Loss in iteration 43 : 2.427339714069195
Loss in iteration 44 : 2.3293093330433816
Loss in iteration 45 : 2.086812438304113
Loss in iteration 46 : 2.0239574595014513
Loss in iteration 47 : 1.876716033304721
Loss in iteration 48 : 1.869682464133225
Loss in iteration 49 : 1.7708015694073986
Loss in iteration 50 : 1.8329850790163575
Loss in iteration 51 : 1.7627492401553109
Loss in iteration 52 : 1.911417525143975
Loss in iteration 53 : 1.838354887883138
Loss in iteration 54 : 2.09382665474025
Loss in iteration 55 : 1.9836910419073674
Loss in iteration 56 : 2.3350166320650194
Loss in iteration 57 : 2.129502312391998
Loss in iteration 58 : 2.509900348785301
Loss in iteration 59 : 2.1998812979987834
Loss in iteration 60 : 2.548838812022195
Loss in iteration 61 : 2.206923707842633
Loss in iteration 62 : 2.4968042413888987
Loss in iteration 63 : 2.17529843399564
Loss in iteration 64 : 2.4000437014559703
Loss in iteration 65 : 2.119550477072461
Loss in iteration 66 : 2.292321002460268
Loss in iteration 67 : 2.05814580258502
Loss in iteration 68 : 2.2026809076952496
Loss in iteration 69 : 2.0107447160068674
Loss in iteration 70 : 2.1505779896234465
Loss in iteration 71 : 1.990961412084321
Loss in iteration 72 : 2.1443589457272045
Loss in iteration 73 : 2.0029746335951835
Loss in iteration 74 : 2.1814977896229792
Loss in iteration 75 : 2.041980058834662
Loss in iteration 76 : 2.2482987403783707
Loss in iteration 77 : 2.0936570685714844
Loss in iteration 78 : 2.320326481537146
Loss in iteration 79 : 2.1382418598406954
Loss in iteration 80 : 2.371780204442145
Loss in iteration 81 : 2.162465689675596
Loss in iteration 82 : 2.3902542093675154
Loss in iteration 83 : 2.1643623445508595
Loss in iteration 84 : 2.378597962145555
Loss in iteration 85 : 2.148716506529639
Loss in iteration 86 : 2.3472934000028927
Loss in iteration 87 : 2.1228905971407572
Loss in iteration 88 : 2.308538040531579
Loss in iteration 89 : 2.0948021806991868
Loss in iteration 90 : 2.273138730786082
Loss in iteration 91 : 2.0716244194979994
Loss in iteration 92 : 2.248976697075738
Loss in iteration 93 : 2.0583475523277426
Loss in iteration 94 : 2.2400073458987615
Loss in iteration 95 : 2.0566926068862785
Loss in iteration 96 : 2.245773821024989
Loss in iteration 97 : 2.065017690028787
Loss in iteration 98 : 2.261902520366327
Loss in iteration 99 : 2.079124195857769
Loss in iteration 100 : 2.281659807843415
Loss in iteration 101 : 2.093826658437986
Loss in iteration 102 : 2.298349057144932
Loss in iteration 103 : 2.1048573291798807
Loss in iteration 104 : 2.3076329288417026
Loss in iteration 105 : 2.1100972960018534
Loss in iteration 106 : 2.308505992109835
Loss in iteration 107 : 2.1096449797539822
Loss in iteration 108 : 2.3027131249095953
Loss in iteration 109 : 2.105143812344774
Loss in iteration 110 : 2.2934399959979297
Loss in iteration 111 : 2.098920119687744
Loss in iteration 112 : 2.284033651576661
Loss in iteration 113 : 2.0932042120210603
Loss in iteration 114 : 2.277071977677888
Loss in iteration 115 : 2.0895541654819065
Loss in iteration 116 : 2.2738561787880265
Loss in iteration 117 : 2.088566232488346
Loss in iteration 118 : 2.2743250939818407
Loss in iteration 119 : 2.089902334888777
Loss in iteration 120 : 2.277342447603375
Loss in iteration 121 : 2.0925826057115957
Loss in iteration 122 : 2.2812397348891964
Loss in iteration 123 : 2.0954202740208667
Loss in iteration 124 : 2.2844298747394483
Loss in iteration 125 : 2.0974387680796878
Loss in iteration 126 : 2.285884875580188
Loss in iteration 127 : 2.098133555661173
Loss in iteration 128 : 2.2853370784321134
Loss in iteration 129 : 2.0975246302546777
Loss in iteration 130 : 2.283193906644088
Loss in iteration 131 : 2.096035221051905
Loss in iteration 132 : 2.2802646352256946
Loss in iteration 133 : 2.094278383750249
Loss in iteration 134 : 2.277431841121341
Loss in iteration 135 : 2.0928350437545986
Loss in iteration 136 : 2.275376015160177
Loss in iteration 137 : 2.092087007775789
Loss in iteration 138 : 2.2744168110215823
Loss in iteration 139 : 2.092140386954132
Loss in iteration 140 : 2.2744896789540654
Loss in iteration 141 : 2.0928443892136923
Loss in iteration 142 : 2.2752384660382545
Loss in iteration 143 : 2.0938830925697873
Loss in iteration 144 : 2.2761766217894923
Loss in iteration 145 : 2.094899950059311
Loss in iteration 146 : 2.2768570541667055
Loss in iteration 147 : 2.095611093153982
Loss in iteration 148 : 2.276997221293347
Loss in iteration 149 : 2.095874570967923
Loss in iteration 150 : 2.276529360051104
Loss in iteration 151 : 2.095703098113977
Loss in iteration 152 : 2.2755757173831754
Loss in iteration 153 : 2.0952284091254025
Loss in iteration 154 : 2.274372569631664
Loss in iteration 155 : 2.094638365780773
Loss in iteration 156 : 2.2731771520525883
Loss in iteration 157 : 2.094111190934634
Loss in iteration 158 : 2.2721886534998657
Loss in iteration 159 : 2.093766508892189
Loss in iteration 160 : 2.2715031183827095
Loss in iteration 161 : 2.093643770862965
Loss in iteration 162 : 2.27110802649611
Loss in iteration 163 : 2.093708603786695
Loss in iteration 164 : 2.2709098144981894
Loss in iteration 165 : 2.0938794551100095
Loss in iteration 166 : 2.270779478715903
Loss in iteration 167 : 2.0940624718221392
Loss in iteration 168 : 2.270599019164887
Loss in iteration 169 : 2.094182488878697
Loss in iteration 170 : 2.2702946219722007
Loss in iteration 171 : 2.094201617721554
Loss in iteration 172 : 2.2698493269622455
Loss in iteration 173 : 2.0941224571484405
Loss in iteration 174 : 2.269295695783854
Loss in iteration 175 : 2.0939782705819776
Loss in iteration 176 : 2.2686950059618676
Loss in iteration 177 : 2.093815936072869
Loss in iteration 178 : 2.2681121914224764
Loss in iteration 179 : 2.093678407041956
Loss in iteration 180 : 2.2675950329656414
Loss in iteration 181 : 2.0935921148704137
Loss in iteration 182 : 2.2671629840808625
Loss in iteration 183 : 2.0935620970164943
Loss in iteration 184 : 2.2668069773402095
Loss in iteration 185 : 2.0935747211209863
Loss in iteration 186 : 2.2664979790277204
Loss in iteration 187 : 2.0936056067319786
Loss in iteration 188 : 2.266199888028903
Loss in iteration 189 : 2.0936292597289663
Loss in iteration 190 : 2.2658819630703206
Loss in iteration 191 : 2.093627121207186
Loss in iteration 192 : 2.265527088293562
Loss in iteration 193 : 2.0935918872579142
Loss in iteration 194 : 2.265134210100547
Loss in iteration 195 : 2.093527534134039
Loss in iteration 196 : 2.264715399904807
Loss in iteration 197 : 2.0934459125325153
Loss in iteration 198 : 2.2642895424902263
Loss in iteration 199 : 2.0933616442408747
Loss in iteration 200 : 2.2638752610885113
Testing accuracy  of updater 2 on alg 0 with rate 4.0 = 0.76, training accuracy 0.7665, time elapsed: 5236 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.7390243244034621
Loss in iteration 3 : 1.5461235979835177
Loss in iteration 4 : 2.4961351716319102
Loss in iteration 5 : 0.5491945676139456
Loss in iteration 6 : 0.5116117840937398
Loss in iteration 7 : 0.559490532755358
Loss in iteration 8 : 0.6994699929099846
Loss in iteration 9 : 0.9559049119020632
Loss in iteration 10 : 1.0668534021602691
Loss in iteration 11 : 0.8088249835379872
Loss in iteration 12 : 0.7371409817547588
Loss in iteration 13 : 0.6752825726820135
Loss in iteration 14 : 0.6109967122349387
Loss in iteration 15 : 0.5707663607584035
Loss in iteration 16 : 0.5430751210357777
Loss in iteration 17 : 0.5282697556481832
Loss in iteration 18 : 0.5200434020403025
Loss in iteration 19 : 0.5156113921810063
Loss in iteration 20 : 0.512601324551158
Loss in iteration 21 : 0.5099866661947243
Loss in iteration 22 : 0.5073499834642358
Loss in iteration 23 : 0.5045668294337196
Loss in iteration 24 : 0.5016376127338616
Loss in iteration 25 : 0.4985886438503922
Loss in iteration 26 : 0.49545987272957354
Loss in iteration 27 : 0.4922913044277038
Loss in iteration 28 : 0.48912664710240494
Loss in iteration 29 : 0.4860120451343148
Loss in iteration 30 : 0.48299578595997134
Loss in iteration 31 : 0.48012697290008927
Loss in iteration 32 : 0.47745189603337956
Loss in iteration 33 : 0.47501190753527894
Loss in iteration 34 : 0.47283868200195706
Loss in iteration 35 : 0.4709530630044377
Loss in iteration 36 : 0.46936168196075273
Loss in iteration 37 : 0.4680587703535148
Loss in iteration 38 : 0.4670253422538045
Loss in iteration 39 : 0.46623403452433115
Loss in iteration 40 : 0.4656497629601747
Loss in iteration 41 : 0.46523579018915506
Loss in iteration 42 : 0.46495388110142666
Loss in iteration 43 : 0.46476986411858207
Loss in iteration 44 : 0.46465230234460014
Loss in iteration 45 : 0.4645770410084976
Loss in iteration 46 : 0.4645250676678271
Loss in iteration 47 : 0.4644869587867027
Loss in iteration 48 : 0.46446321034053045
Loss in iteration 49 : 0.46447250253357286
Loss in iteration 50 : 0.46456707268111536
Loss in iteration 51 : 0.46485978330132444
Loss in iteration 52 : 0.46561439367849294
Loss in iteration 53 : 0.46733371200596285
Loss in iteration 54 : 0.47124365518319183
Loss in iteration 55 : 0.47930623611979706
Loss in iteration 56 : 0.49675955085052814
Loss in iteration 57 : 0.5276572085582216
Loss in iteration 58 : 0.5867717004370068
Loss in iteration 59 : 0.6501645603027357
Loss in iteration 60 : 0.7342256375695281
Loss in iteration 61 : 0.7343765515332278
Loss in iteration 62 : 0.7532407557682191
Loss in iteration 63 : 0.7028029576799077
Loss in iteration 64 : 0.6887390378258161
Loss in iteration 65 : 0.6448547639884203
Loss in iteration 66 : 0.6232080658502964
Loss in iteration 67 : 0.5904625848615361
Loss in iteration 68 : 0.5706431804274547
Loss in iteration 69 : 0.5487491591055544
Loss in iteration 70 : 0.5342875686251268
Loss in iteration 71 : 0.5207888449438213
Loss in iteration 72 : 0.5114236513252981
Loss in iteration 73 : 0.503497748528769
Loss in iteration 74 : 0.49783021009068545
Loss in iteration 75 : 0.49330918129386514
Loss in iteration 76 : 0.4900608835965868
Loss in iteration 77 : 0.4876104660793556
Loss in iteration 78 : 0.4859550842759836
Loss in iteration 79 : 0.4848843058048375
Loss in iteration 80 : 0.48445095627330786
Loss in iteration 81 : 0.48456408554891084
Loss in iteration 82 : 0.4854398365190559
Loss in iteration 83 : 0.4870131084133432
Loss in iteration 84 : 0.48982281146526574
Loss in iteration 85 : 0.49364944439359204
Loss in iteration 86 : 0.49969063806698927
Loss in iteration 87 : 0.5070127680266193
Loss in iteration 88 : 0.5181619701814104
Loss in iteration 89 : 0.5298206251666925
Loss in iteration 90 : 0.5471955130556305
Loss in iteration 91 : 0.5610181468900513
Loss in iteration 92 : 0.5817627399627736
Loss in iteration 93 : 0.590729383636524
Loss in iteration 94 : 0.6077158794505746
Loss in iteration 95 : 0.6058864261498459
Loss in iteration 96 : 0.614465844512489
Loss in iteration 97 : 0.6036152609292035
Loss in iteration 98 : 0.6047515874397824
Loss in iteration 99 : 0.5902758384529793
Loss in iteration 100 : 0.5870478779923315
Loss in iteration 101 : 0.572923659328506
Loss in iteration 102 : 0.5680283855263231
Loss in iteration 103 : 0.5561369668952707
Loss in iteration 104 : 0.5513180791399642
Loss in iteration 105 : 0.5421774999590987
Loss in iteration 106 : 0.5383207017674253
Loss in iteration 107 : 0.5317723690488777
Loss in iteration 108 : 0.5292074222002981
Loss in iteration 109 : 0.5248546649662755
Loss in iteration 110 : 0.5236360997793422
Loss in iteration 111 : 0.5210702032117883
Loss in iteration 112 : 0.5211615740630445
Loss in iteration 113 : 0.5200460037383725
Loss in iteration 114 : 0.5214111989372863
Loss in iteration 115 : 0.5214784234271731
Loss in iteration 116 : 0.5240944402432145
Loss in iteration 117 : 0.5250849053694674
Loss in iteration 118 : 0.5288892472649804
Loss in iteration 119 : 0.5304600845800145
Loss in iteration 120 : 0.5352607739545597
Loss in iteration 121 : 0.5369174255995897
Loss in iteration 122 : 0.5423255686450124
Loss in iteration 123 : 0.5434522095300436
Loss in iteration 124 : 0.5489127972813939
Loss in iteration 125 : 0.5489367505723225
Loss in iteration 126 : 0.5538798594432462
Loss in iteration 127 : 0.5524871311125571
Loss in iteration 128 : 0.5565153003175045
Loss in iteration 129 : 0.553759210336062
Loss in iteration 130 : 0.5567472573779487
Loss in iteration 131 : 0.5529779570931744
Loss in iteration 132 : 0.5550433881505994
Loss in iteration 133 : 0.5507349223779608
Loss in iteration 134 : 0.5521341778881533
Loss in iteration 135 : 0.5477278078970467
Loss in iteration 136 : 0.548751768078849
Loss in iteration 137 : 0.5445727051225793
Loss in iteration 138 : 0.545478781770181
Loss in iteration 139 : 0.5417192551610741
Loss in iteration 140 : 0.54270231047158
Loss in iteration 141 : 0.5394410873520742
Loss in iteration 142 : 0.5406302636244888
Loss in iteration 143 : 0.5378634419172552
Loss in iteration 144 : 0.5393310438452839
Loss in iteration 145 : 0.5370005175716677
Loss in iteration 146 : 0.5387736266581356
Loss in iteration 147 : 0.5367887739394742
Loss in iteration 148 : 0.5388593335596681
Loss in iteration 149 : 0.5371126061845037
Loss in iteration 150 : 0.5394454230115078
Loss in iteration 151 : 0.5378244147988009
Loss in iteration 152 : 0.5403643833126589
Loss in iteration 153 : 0.5387626063848558
Loss in iteration 154 : 0.5414425665288525
Loss in iteration 155 : 0.539769547894294
Loss in iteration 156 : 0.5425191005041342
Loss in iteration 157 : 0.5407086869285889
Loss in iteration 158 : 0.5434629638492074
Loss in iteration 159 : 0.5414779740800115
Loss in iteration 160 : 0.5441846916505724
Loss in iteration 161 : 0.542016696562279
Loss in iteration 162 : 0.5446401626017865
Loss in iteration 163 : 0.5423046921191835
Loss in iteration 164 : 0.5448264471891077
Loss in iteration 165 : 0.5423553161723107
Loss in iteration 166 : 0.5447720983530314
Loss in iteration 167 : 0.5422050798275324
Loss in iteration 168 : 0.5445253164105186
Loss in iteration 169 : 0.5419030061932972
Loss in iteration 170 : 0.5441429802903938
Loss in iteration 171 : 0.5415018219356622
Loss in iteration 172 : 0.5436822650822876
Loss in iteration 173 : 0.5410518401674557
Loss in iteration 174 : 0.5431952499200712
Loss in iteration 175 : 0.5405973799274427
Loss in iteration 176 : 0.5427260362930698
Loss in iteration 177 : 0.5401750387118373
Loss in iteration 178 : 0.542309543860207
Loss in iteration 179 : 0.539813042085086
Loss in iteration 180 : 0.541971197584459
Loss in iteration 181 : 0.5395310705882502
Loss in iteration 182 : 0.5417269716139496
Loss in iteration 183 : 0.5393402395038385
Loss in iteration 184 : 0.5415835517545061
Loss in iteration 185 : 0.5392431646644725
Loss in iteration 186 : 0.5415386237991848
Loss in iteration 187 : 0.539234228165883
Loss in iteration 188 : 0.5415814473897083
Loss in iteration 189 : 0.5393002422380139
Loss in iteration 190 : 0.5416939241806096
Loss in iteration 191 : 0.5394216987771485
Loss in iteration 192 : 0.5418523199895544
Loss in iteration 193 : 0.5395746976090292
Loss in iteration 194 : 0.5420296694586595
Loss in iteration 195 : 0.5397334906551678
Loss in iteration 196 : 0.5421987095491333
Loss in iteration 197 : 0.5398733993300596
Loss in iteration 198 : 0.5423350039358434
Loss in iteration 199 : 0.5399737106286098
Loss in iteration 200 : 0.5424197939466824
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.7885, training accuracy 0.787, time elapsed: 5110 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6973396244280051
Loss in iteration 3 : 0.8333624651029116
Loss in iteration 4 : 1.2523754556624471
Loss in iteration 5 : 0.7988378024504444
Loss in iteration 6 : 0.900259880045044
Loss in iteration 7 : 0.7800503132563403
Loss in iteration 8 : 0.7309860443026682
Loss in iteration 9 : 0.6202163710547992
Loss in iteration 10 : 0.5604775993447795
Loss in iteration 11 : 0.5219917054729369
Loss in iteration 12 : 0.4994853356083117
Loss in iteration 13 : 0.4912071666066958
Loss in iteration 14 : 0.48876403545506436
Loss in iteration 15 : 0.48853331552739354
Loss in iteration 16 : 0.48856187179439087
Loss in iteration 17 : 0.4884306165884463
Loss in iteration 18 : 0.48807332976052664
Loss in iteration 19 : 0.4875087360125554
Loss in iteration 20 : 0.48676303679002253
Loss in iteration 21 : 0.48585932010239696
Loss in iteration 22 : 0.4848173201915058
Loss in iteration 23 : 0.48365606967990793
Loss in iteration 24 : 0.4823961363030251
Loss in iteration 25 : 0.4810602686401277
Loss in iteration 26 : 0.47967257721170997
Loss in iteration 27 : 0.47825703541005393
Loss in iteration 28 : 0.4768361216796129
Loss in iteration 29 : 0.4754300790676052
Loss in iteration 30 : 0.47405682982720293
Loss in iteration 31 : 0.4727322715556288
Loss in iteration 32 : 0.4714705910363151
Loss in iteration 33 : 0.4702843343572922
Loss in iteration 34 : 0.46918416600681506
Loss in iteration 35 : 0.4681784251815376
Loss in iteration 36 : 0.46727267407192713
Loss in iteration 37 : 0.4664694182390281
Loss in iteration 38 : 0.4657680973787276
Loss in iteration 39 : 0.4651653474415773
Loss in iteration 40 : 0.4646554640093946
Loss in iteration 41 : 0.46423096937686803
Loss in iteration 42 : 0.46388319580459897
Loss in iteration 43 : 0.46360282624554017
Loss in iteration 44 : 0.4633803630393201
Loss in iteration 45 : 0.46320651434382976
Loss in iteration 46 : 0.463072496548053
Loss in iteration 47 : 0.46297025337994224
Loss in iteration 48 : 0.46289259432452384
Loss in iteration 49 : 0.46283325900379724
Loss in iteration 50 : 0.46278691971558045
Loss in iteration 51 : 0.46274913892704794
Loss in iteration 52 : 0.46271629995919306
Loss in iteration 53 : 0.46268552675833086
Loss in iteration 54 : 0.4626546036151565
Loss in iteration 55 : 0.46262189994690206
Loss in iteration 56 : 0.4625863006390235
Loss in iteration 57 : 0.4625471399256645
Loss in iteration 58 : 0.46250413635606824
Loss in iteration 59 : 0.46245732735739364
Loss in iteration 60 : 0.46240700338117313
Loss in iteration 61 : 0.4623536429159229
Loss in iteration 62 : 0.4622978504116482
Loss in iteration 63 : 0.4622402993493639
Loss in iteration 64 : 0.462181682430385
Loss in iteration 65 : 0.4621226703315999
Loss in iteration 66 : 0.4620638798213613
Loss in iteration 67 : 0.46200585135804534
Loss in iteration 68 : 0.4619490356703242
Loss in iteration 69 : 0.4618937882995471
Loss in iteration 70 : 0.4618403707144835
Loss in iteration 71 : 0.46178895641416784
Loss in iteration 72 : 0.46173964041786725
Loss in iteration 73 : 0.4616924506771753
Loss in iteration 74 : 0.4616473601881005
Loss in iteration 75 : 0.4616042988760217
Loss in iteration 76 : 0.4615631646234363
Loss in iteration 77 : 0.4615238330723939
Loss in iteration 78 : 0.4614861660405869
Loss in iteration 79 : 0.46145001853703776
Loss in iteration 80 : 0.46141524445605975
Loss in iteration 81 : 0.46138170107737947
Loss in iteration 82 : 0.46134925251848996
Loss in iteration 83 : 0.46131777228340337
Loss in iteration 84 : 0.461287145039132
Loss in iteration 85 : 0.4612572677339901
Loss in iteration 86 : 0.4612280501549045
Loss in iteration 87 : 0.4611994150070805
Loss in iteration 88 : 0.461171297589875
Loss in iteration 89 : 0.4611436451374914
Loss in iteration 90 : 0.4611164158911459
Loss in iteration 91 : 0.4610895779690745
Loss in iteration 92 : 0.4610631081004933
Loss in iteration 93 : 0.46103699028804696
Loss in iteration 94 : 0.461011214459376
Loss in iteration 95 : 0.4609857751619883
Loss in iteration 96 : 0.46096067034685234
Loss in iteration 97 : 0.4609359002756174
Loss in iteration 98 : 0.4609114665751032
Loss in iteration 99 : 0.46088737145149183
Loss in iteration 100 : 0.46086361706643997
Loss in iteration 101 : 0.46084020506867424
Loss in iteration 102 : 0.4608171362680022
Loss in iteration 103 : 0.4607944104341727
Loss in iteration 104 : 0.4607720262006139
Loss in iteration 105 : 0.46074998105242143
Loss in iteration 106 : 0.4607282713788106
Loss in iteration 107 : 0.46070689257204267
Loss in iteration 108 : 0.46068583915721784
Loss in iteration 109 : 0.46066510493995005
Loss in iteration 110 : 0.46064468316149665
Loss in iteration 111 : 0.46062456665323387
Loss in iteration 112 : 0.4606047479843911
Loss in iteration 113 : 0.4605852195985886
Loss in iteration 114 : 0.4605659739361058
Loss in iteration 115 : 0.4605470035398553
Loss in iteration 116 : 0.4605283011439696
Loss in iteration 117 : 0.4605098597446195
Loss in iteration 118 : 0.4604916726533393
Loss in iteration 119 : 0.4604737335336721
Loss in iteration 120 : 0.4604560364224379
Loss in iteration 121 : 0.4604385757372576
Loss in iteration 122 : 0.4604213462722965
Loss in iteration 123 : 0.460404343184308
Loss in iteration 124 : 0.4603875619711655
Loss in iteration 125 : 0.4603709984449822
Loss in iteration 126 : 0.4603546487018179
Loss in iteration 127 : 0.46033850908972773
Loss in iteration 128 : 0.4603225761766827
Loss in iteration 129 : 0.46030684671956446
Loss in iteration 130 : 0.4602913176351724
Loss in iteration 131 : 0.4602759859738842
Loss in iteration 132 : 0.4602608488963538
Loss in iteration 133 : 0.46024590365344176
Loss in iteration 134 : 0.46023114756935113
Loss in iteration 135 : 0.460216578027859
Loss in iteration 136 : 0.46020219246140226
Loss in iteration 137 : 0.46018798834273794
Loss in iteration 138 : 0.4601739631788432
Loss in iteration 139 : 0.46016011450674477
Loss in iteration 140 : 0.46014643989093407
Loss in iteration 141 : 0.46013293692206536
Loss in iteration 142 : 0.46011960321664863
Loss in iteration 143 : 0.46010643641748855
Loss in iteration 144 : 0.460093434194629
Loss in iteration 145 : 0.4600805942466216
Loss in iteration 146 : 0.46006791430194144
Loss in iteration 147 : 0.4600553921204332
Loss in iteration 148 : 0.4600430254946566
Loss in iteration 149 : 0.46003081225107884
Loss in iteration 150 : 0.4600187502510394
Loss in iteration 151 : 0.46000683739146553
Loss in iteration 152 : 0.45999507160531594
Loss in iteration 153 : 0.45998345086175957
Loss in iteration 154 : 0.459971973166097
Loss in iteration 155 : 0.4599606365594598
Loss in iteration 156 : 0.4599494391182934
Loss in iteration 157 : 0.45993837895368467
Loss in iteration 158 : 0.4599274542105506
Loss in iteration 159 : 0.45991666306672385
Loss in iteration 160 : 0.459906003731978
Loss in iteration 161 : 0.4598954744470063
Loss in iteration 162 : 0.45988507348239593
Loss in iteration 163 : 0.45987479913760865
Loss in iteration 164 : 0.4598646497399824
Loss in iteration 165 : 0.45985462364378477
Loss in iteration 166 : 0.4598447192293035
Loss in iteration 167 : 0.4598349349020084
Loss in iteration 168 : 0.4598252690917566
Loss in iteration 169 : 0.4598157202520781
Loss in iteration 170 : 0.45980628685950853
Loss in iteration 171 : 0.45979696741298415
Loss in iteration 172 : 0.4597877604332957
Loss in iteration 173 : 0.45977866446258975
Loss in iteration 174 : 0.45976967806391467
Loss in iteration 175 : 0.4597607998208094
Loss in iteration 176 : 0.45975202833692663
Loss in iteration 177 : 0.4597433622356801
Loss in iteration 178 : 0.4597348001599258
Loss in iteration 179 : 0.45972634077165403
Loss in iteration 180 : 0.4597179827517024
Loss in iteration 181 : 0.4597097247994742
Loss in iteration 182 : 0.4597015656326746
Loss in iteration 183 : 0.4596935039870433
Loss in iteration 184 : 0.45968553861609757
Loss in iteration 185 : 0.45967766829087303
Loss in iteration 186 : 0.45966989179966866
Loss in iteration 187 : 0.4596622079477921
Loss in iteration 188 : 0.45965461555730563
Loss in iteration 189 : 0.459647113466767
Loss in iteration 190 : 0.4596397005309799
Loss in iteration 191 : 0.4596323756207396
Loss in iteration 192 : 0.45962513762257895
Loss in iteration 193 : 0.45961798543851773
Loss in iteration 194 : 0.4596109179858173
Loss in iteration 195 : 0.4596039341967292
Loss in iteration 196 : 0.4595970330182598
Loss in iteration 197 : 0.45959021341192396
Loss in iteration 198 : 0.4595834743535147
Loss in iteration 199 : 0.4595768148328732
Loss in iteration 200 : 0.45957023385365664
Testing accuracy  of updater 2 on alg 0 with rate 0.7 = 0.788, training accuracy 0.789375, time elapsed: 5117 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6770954948228846
Loss in iteration 3 : 0.6477805041574043
Loss in iteration 4 : 0.6224186658453403
Loss in iteration 5 : 0.5990766596821578
Loss in iteration 6 : 0.5783279331563669
Loss in iteration 7 : 0.5607071198495908
Loss in iteration 8 : 0.5456383601545703
Loss in iteration 9 : 0.5330345293580854
Loss in iteration 10 : 0.5226131316325792
Loss in iteration 11 : 0.5141077467760283
Loss in iteration 12 : 0.5072245712802891
Loss in iteration 13 : 0.5016784882744352
Loss in iteration 14 : 0.49721199602064237
Loss in iteration 15 : 0.4936044419501285
Loss in iteration 16 : 0.49067359599403154
Loss in iteration 17 : 0.4882725382180562
Loss in iteration 18 : 0.48628445156475575
Loss in iteration 19 : 0.48461708766997696
Loss in iteration 20 : 0.48319784972904783
Loss in iteration 21 : 0.48196981509034864
Loss in iteration 22 : 0.4808886497264555
Loss in iteration 23 : 0.4799202090111464
Loss in iteration 24 : 0.4790386035295536
Loss in iteration 25 : 0.4782245648024965
Loss in iteration 26 : 0.47746402056956794
Loss in iteration 27 : 0.47674685070107475
Loss in iteration 28 : 0.4760658298018964
Loss in iteration 29 : 0.4754157714483195
Loss in iteration 30 : 0.47479287951189786
Loss in iteration 31 : 0.4741942942410593
Loss in iteration 32 : 0.47361780350292476
Loss in iteration 33 : 0.47306167850243924
Loss in iteration 34 : 0.472524590508872
Loss in iteration 35 : 0.4720055698222023
Loss in iteration 36 : 0.4715039778226208
Loss in iteration 37 : 0.47101947433038616
Loss in iteration 38 : 0.47055197293933004
Loss in iteration 39 : 0.4701015847289766
Loss in iteration 40 : 0.46966855517979383
Loss in iteration 41 : 0.469253200522453
Loss in iteration 42 : 0.46885584902664384
Loss in iteration 43 : 0.4684767909334274
Loss in iteration 44 : 0.4681162387665089
Loss in iteration 45 : 0.4677742982054418
Loss in iteration 46 : 0.4674509487898124
Loss in iteration 47 : 0.46714603338459043
Loss in iteration 48 : 0.4668592553538981
Loss in iteration 49 : 0.4665901825188466
Loss in iteration 50 : 0.46633825703719656
Loss in iteration 51 : 0.4661028102675921
Loss in iteration 52 : 0.46588308149612295
Loss in iteration 53 : 0.46567823919407636
Loss in iteration 54 : 0.4654874033399988
Loss in iteration 55 : 0.4653096673458299
Loss in iteration 56 : 0.4651441182973742
Loss in iteration 57 : 0.46498985452727426
Loss in iteration 58 : 0.4648459999251095
Loss in iteration 59 : 0.4647117147836476
Loss in iteration 60 : 0.4645862033210451
Loss in iteration 61 : 0.46446871826581454
Loss in iteration 62 : 0.46435856303077205
Loss in iteration 63 : 0.46425509204282717
Loss in iteration 64 : 0.46415770976113335
Loss in iteration 65 : 0.46406586883610257
Loss in iteration 66 : 0.4639790677632435
Loss in iteration 67 : 0.4638968482890214
Loss in iteration 68 : 0.46381879274267523
Loss in iteration 69 : 0.46374452140260924
Loss in iteration 70 : 0.46367368995794833
Loss in iteration 71 : 0.4636059870920215
Loss in iteration 72 : 0.4635411321915086
Loss in iteration 73 : 0.46347887317004643
Loss in iteration 74 : 0.46341898438662876
Loss in iteration 75 : 0.4633612646361394
Loss in iteration 76 : 0.4633055351912105
Loss in iteration 77 : 0.4632516378802753
Loss in iteration 78 : 0.46319943319495943
Loss in iteration 79 : 0.46314879842918766
Loss in iteration 80 : 0.46309962586097786
Loss in iteration 81 : 0.46305182099447656
Loss in iteration 82 : 0.46300530088346187
Loss in iteration 83 : 0.4629599925581183
Loss in iteration 84 : 0.4629158315745316
Loss in iteration 85 : 0.46287276070188915
Loss in iteration 86 : 0.4628307287565861
Loss in iteration 87 : 0.4627896895862835
Loss in iteration 88 : 0.4627496012012125
Loss in iteration 89 : 0.46271042504512
Loss in iteration 90 : 0.462672125394603
Loss in iteration 91 : 0.46263466887312604
Loss in iteration 92 : 0.4625980240647796
Loss in iteration 93 : 0.4625621612125879
Loss in iteration 94 : 0.46252705198667227
Loss in iteration 95 : 0.46249266930869054
Loss in iteration 96 : 0.4624589872204022
Loss in iteration 97 : 0.4624259807859051
Loss in iteration 98 : 0.46239362601878575
Loss in iteration 99 : 0.4623618998271726
Loss in iteration 100 : 0.46233077997123295
Loss in iteration 101 : 0.4623002450290683
Loss in iteration 102 : 0.4622702743681785
Loss in iteration 103 : 0.4622408481205726
Loss in iteration 104 : 0.4622119471603557
Loss in iteration 105 : 0.4621835530831047
Loss in iteration 106 : 0.4621556481866702
Loss in iteration 107 : 0.46212821545319765
Loss in iteration 108 : 0.46210123853222673
Loss in iteration 109 : 0.4620747017247331
Loss in iteration 110 : 0.46204858996792414
Loss in iteration 111 : 0.4620228888205586
Loss in iteration 112 : 0.46199758444853617
Loss in iteration 113 : 0.4619726636104762
Loss in iteration 114 : 0.4619481136430117
Loss in iteration 115 : 0.46192392244558106
Loss in iteration 116 : 0.4619000784645092
Loss in iteration 117 : 0.46187657067627874
Loss in iteration 118 : 0.4618533885699254
Loss in iteration 119 : 0.4618305221285689
Loss in iteration 120 : 0.4618079618101822
Loss in iteration 121 : 0.4617856985276924
Loss in iteration 122 : 0.4617637236286253
Loss in iteration 123 : 0.4617420288744536
Loss in iteration 124 : 0.4617206064198951
Loss in iteration 125 : 0.46169944879233843
Loss in iteration 126 : 0.4616785488716133
Loss in iteration 127 : 0.4616578998702805
Loss in iteration 128 : 0.46163749531457293
Loss in iteration 129 : 0.46161732902613223
Loss in iteration 130 : 0.4615973951045964
Loss in iteration 131 : 0.4615776879111268
Loss in iteration 132 : 0.46155820205286513
Loss in iteration 133 : 0.4615389323683608
Loss in iteration 134 : 0.4615198739139192
Loss in iteration 135 : 0.4615010219508647
Loss in iteration 136 : 0.46148237193366226
Loss in iteration 137 : 0.4614639194988556
Loss in iteration 138 : 0.4614456604547709
Loss in iteration 139 : 0.46142759077193224
Loss in iteration 140 : 0.46140970657414243
Loss in iteration 141 : 0.46139200413017545
Loss in iteration 142 : 0.4613744798460435
Loss in iteration 143 : 0.46135713025778985
Loss in iteration 144 : 0.46133995202477646
Loss in iteration 145 : 0.4613229419234225
Loss in iteration 146 : 0.4613060968413729
Loss in iteration 147 : 0.46128941377205196
Loss in iteration 148 : 0.461272889809589
Loss in iteration 149 : 0.46125652214407403
Loss in iteration 150 : 0.4612403080571259
Loss in iteration 151 : 0.46122424491774866
Loss in iteration 152 : 0.46120833017844665
Loss in iteration 153 : 0.46119256137158293
Loss in iteration 154 : 0.46117693610595467
Loss in iteration 155 : 0.4611614520635678
Loss in iteration 156 : 0.46114610699659536
Loss in iteration 157 : 0.4611308987244993
Loss in iteration 158 : 0.46111582513130417
Loss in iteration 159 : 0.4611008841630032
Loss in iteration 160 : 0.4610860738250993
Loss in iteration 161 : 0.4610713921802534
Loss in iteration 162 : 0.46105683734604236
Loss in iteration 163 : 0.46104240749281394
Loss in iteration 164 : 0.46102810084164514
Loss in iteration 165 : 0.4610139156623646
Loss in iteration 166 : 0.4609998502716849
Loss in iteration 167 : 0.46098590303139253
Loss in iteration 168 : 0.4609720723466221
Loss in iteration 169 : 0.46095835666419777
Loss in iteration 170 : 0.4609447544710448
Loss in iteration 171 : 0.46093126429266496
Loss in iteration 172 : 0.46091788469167977
Loss in iteration 173 : 0.46090461426642476
Loss in iteration 174 : 0.4608914516496129
Loss in iteration 175 : 0.4608783955070448
Loss in iteration 176 : 0.4608654445363792
Loss in iteration 177 : 0.4608525974659515
Loss in iteration 178 : 0.4608398530536441
Loss in iteration 179 : 0.460827210085803
Loss in iteration 180 : 0.4608146673762013
Loss in iteration 181 : 0.4608022237650456
Loss in iteration 182 : 0.46078987811802696
Loss in iteration 183 : 0.46077762932540905
Loss in iteration 184 : 0.46076547630115744
Loss in iteration 185 : 0.4607534179821041
Loss in iteration 186 : 0.46074145332715133
Loss in iteration 187 : 0.46072958131650676
Loss in iteration 188 : 0.4607178009509497
Loss in iteration 189 : 0.46070611125113375
Loss in iteration 190 : 0.4606945112569121
Loss in iteration 191 : 0.46068300002670187
Loss in iteration 192 : 0.46067157663686176
Loss in iteration 193 : 0.4606602401811121
Loss in iteration 194 : 0.46064898976996377
Loss in iteration 195 : 0.4606378245301813
Loss in iteration 196 : 0.460626743604269
Loss in iteration 197 : 0.4606157461499697
Loss in iteration 198 : 0.4606048313397938
Loss in iteration 199 : 0.460593998360563
Loss in iteration 200 : 0.4605832464129717
Testing accuracy  of updater 2 on alg 0 with rate 0.4 = 0.7875, training accuracy 0.78925, time elapsed: 4863 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.683961766963019
Loss in iteration 3 : 0.6760909594261063
Loss in iteration 4 : 0.6680109399659574
Loss in iteration 5 : 0.6586683759720917
Loss in iteration 6 : 0.648455629303341
Loss in iteration 7 : 0.6381864457992905
Loss in iteration 8 : 0.6282892130227827
Loss in iteration 9 : 0.6187991309855126
Loss in iteration 10 : 0.6096646851465796
Loss in iteration 11 : 0.6009073878930333
Loss in iteration 12 : 0.592595169854483
Loss in iteration 13 : 0.5847773046070234
Loss in iteration 14 : 0.577464366967237
Loss in iteration 15 : 0.5706436570107121
Loss in iteration 16 : 0.5642971258998936
Loss in iteration 17 : 0.558407653411892
Loss in iteration 18 : 0.5529573676419243
Loss in iteration 19 : 0.5479252529308147
Loss in iteration 20 : 0.5432870845320393
Loss in iteration 21 : 0.5390169075937251
Loss in iteration 22 : 0.535088474215232
Loss in iteration 23 : 0.531475973426667
Loss in iteration 24 : 0.5281542653509885
Loss in iteration 25 : 0.525099026057153
Loss in iteration 26 : 0.5222869855558229
Loss in iteration 27 : 0.5196962120922348
Loss in iteration 28 : 0.5173063329482654
Loss in iteration 29 : 0.5150986366144199
Loss in iteration 30 : 0.5130560697574374
Loss in iteration 31 : 0.5111631709570728
Loss in iteration 32 : 0.5094059755886396
Loss in iteration 33 : 0.5077719080048295
Loss in iteration 34 : 0.5062496658524186
Loss in iteration 35 : 0.5048290994581305
Loss in iteration 36 : 0.5035010913054336
Loss in iteration 37 : 0.50225744163968
Loss in iteration 38 : 0.5010907647946262
Loss in iteration 39 : 0.4999943981699422
Loss in iteration 40 : 0.4989623235302444
Loss in iteration 41 : 0.4979890991577862
Loss in iteration 42 : 0.4970698011760974
Loss in iteration 43 : 0.49619997254435394
Loss in iteration 44 : 0.49537557842647906
Loss in iteration 45 : 0.49459296676561576
Loss in iteration 46 : 0.4938488330045416
Loss in iteration 47 : 0.4931401880546277
Loss in iteration 48 : 0.4924643288362189
Loss in iteration 49 : 0.49181881094794494
Loss in iteration 50 : 0.4912014232218221
Loss in iteration 51 : 0.4906101640593382
Loss in iteration 52 : 0.4900432195222154
Loss in iteration 53 : 0.48949894318571985
Loss in iteration 54 : 0.4889758377682842
Loss in iteration 55 : 0.48847253853965916
Loss in iteration 56 : 0.48798779848663765
Loss in iteration 57 : 0.4875204751850635
Loss in iteration 58 : 0.4870695192938468
Loss in iteration 59 : 0.486633964555992
Loss in iteration 60 : 0.4862129191672906
Loss in iteration 61 : 0.4858055583577431
Loss in iteration 62 : 0.4854111180245458
Loss in iteration 63 : 0.4850288892576975
Loss in iteration 64 : 0.48465821360840283
Loss in iteration 65 : 0.484298478964709
Loss in iteration 66 : 0.4839491159162072
Loss in iteration 67 : 0.4836095945087897
Loss in iteration 68 : 0.4832794213096364
Loss in iteration 69 : 0.4829581367207961
Loss in iteration 70 : 0.482645312495888
Loss in iteration 71 : 0.4823405494280752
Loss in iteration 72 : 0.48204347518826524
Loss in iteration 73 : 0.4817537423005453
Loss in iteration 74 : 0.48147102624731974
Loss in iteration 75 : 0.4811950236999937
Loss in iteration 76 : 0.48092545087257665
Loss in iteration 77 : 0.48066204199591794
Loss in iteration 78 : 0.4804045479097173
Loss in iteration 79 : 0.48015273476841874
Loss in iteration 80 : 0.4799063828558867
Loss in iteration 81 : 0.47966528550263443
Loss in iteration 82 : 0.4794292480984765
Loss in iteration 83 : 0.47919808719285345
Loss in iteration 84 : 0.4789716296748501
Loss in iteration 85 : 0.47874971202499317
Loss in iteration 86 : 0.47853217963127553
Loss in iteration 87 : 0.47831888616241147
Loss in iteration 88 : 0.47810969299204753
Loss in iteration 89 : 0.4779044686684433
Loss in iteration 90 : 0.47770308842489423
Loss in iteration 91 : 0.47750543372695586
Loss in iteration 92 : 0.4773113918531648
Loss in iteration 93 : 0.47712085550651284
Loss in iteration 94 : 0.4769337224544179
Loss in iteration 95 : 0.47674989519523686
Loss in iteration 96 : 0.4765692806496447
Loss in iteration 97 : 0.4763917898753294
Loss in iteration 98 : 0.47621733780359204
Loss in iteration 99 : 0.4760458429964423
Loss in iteration 100 : 0.47587722742283095
Loss in iteration 101 : 0.4757114162526696
Loss in iteration 102 : 0.475548337667264
Loss in iteration 103 : 0.47538792268484165
Loss in iteration 104 : 0.4752301049998617
Loss in iteration 105 : 0.4750748208348438
Loss in iteration 106 : 0.47492200880352387
Loss in iteration 107 : 0.47477160978421756
Loss in iteration 108 : 0.474623566802357
Loss in iteration 109 : 0.4744778249212793
Loss in iteration 110 : 0.4743343311404203
Loss in iteration 111 : 0.47419303430018644
Loss in iteration 112 : 0.47405388499286477
Loss in iteration 113 : 0.4739168354790089
Loss in iteration 114 : 0.4737818396088338
Loss in iteration 115 : 0.47364885274821156
Loss in iteration 116 : 0.4735178317089039
Loss in iteration 117 : 0.47338873468274856
Loss in iteration 118 : 0.47326152117952547
Loss in iteration 119 : 0.47313615196826536
Loss in iteration 120 : 0.47301258902179516
Loss in iteration 121 : 0.4728907954643344
Loss in iteration 122 : 0.4727707355219503
Loss in iteration 123 : 0.4726523744757161
Loss in iteration 124 : 0.47253567861740553
Loss in iteration 125 : 0.47242061520757944
Loss in iteration 126 : 0.47230715243591864
Loss in iteration 127 : 0.4721952593836591
Loss in iteration 128 : 0.47208490598800856
Loss in iteration 129 : 0.47197606300841866
Loss in iteration 130 : 0.4718687019945918
Loss in iteration 131 : 0.47176279525612486
Loss in iteration 132 : 0.4716583158336714
Loss in iteration 133 : 0.47155523747154887
Loss in iteration 134 : 0.4714535345916907
Loss in iteration 135 : 0.47135318226885975
Loss in iteration 136 : 0.4712541562070646
Loss in iteration 137 : 0.4711564327170921
Loss in iteration 138 : 0.4710599886950987
Loss in iteration 139 : 0.47096480160220994
Loss in iteration 140 : 0.47087084944505886
Loss in iteration 141 : 0.47077811075721787
Loss in iteration 142 : 0.47068656458147656
Loss in iteration 143 : 0.47059619045291995
Loss in iteration 144 : 0.47050696838275874
Loss in iteration 145 : 0.4704188788428768
Loss in iteration 146 : 0.47033190275105596
Loss in iteration 147 : 0.47024602145683825
Loss in iteration 148 : 0.470161216727997
Loss in iteration 149 : 0.47007747073757694
Loss in iteration 150 : 0.46999476605147833
Loss in iteration 151 : 0.4699130856165576
Loss in iteration 152 : 0.46983241274921056
Loss in iteration 153 : 0.46975273112441196
Loss in iteration 154 : 0.4696740247652065
Loss in iteration 155 : 0.46959627803259296
Loss in iteration 156 : 0.4695194756158226
Loss in iteration 157 : 0.46944360252305317
Loss in iteration 158 : 0.46936864407237033
Loss in iteration 159 : 0.46929458588314177
Loss in iteration 160 : 0.4692214138676911
Loss in iteration 161 : 0.46914911422328065
Loss in iteration 162 : 0.46907767342438744
Loss in iteration 163 : 0.4690070782152537
Loss in iteration 164 : 0.46893731560270663
Loss in iteration 165 : 0.46886837284923044
Loss in iteration 166 : 0.4688002374662822
Loss in iteration 167 : 0.4687328972078365
Loss in iteration 168 : 0.46866634006415714
Loss in iteration 169 : 0.4686005542557762
Loss in iteration 170 : 0.46853552822767686
Loss in iteration 171 : 0.46847125064367606
Loss in iteration 172 : 0.4684077103809832
Loss in iteration 173 : 0.46834489652494676
Loss in iteration 174 : 0.46828279836396575
Loss in iteration 175 : 0.4682214053845705
Loss in iteration 176 : 0.4681607072666543
Loss in iteration 177 : 0.46810069387886005
Loss in iteration 178 : 0.4680413552741105
Loss in iteration 179 : 0.4679826816852785
Loss in iteration 180 : 0.46792466352098966
Loss in iteration 181 : 0.46786729136155064
Loss in iteration 182 : 0.46781055595500787
Loss in iteration 183 : 0.46775444821331724
Loss in iteration 184 : 0.4676989592086325
Loss in iteration 185 : 0.4676440801697005
Loss in iteration 186 : 0.46758980247836357
Loss in iteration 187 : 0.46753611766616665
Loss in iteration 188 : 0.46748301741105225
Loss in iteration 189 : 0.46743049353416344
Loss in iteration 190 : 0.46737853799672424
Loss in iteration 191 : 0.4673271428970196
Loss in iteration 192 : 0.467276300467452
Loss in iteration 193 : 0.46722600307167944
Loss in iteration 194 : 0.4671762432018374
Loss in iteration 195 : 0.4671270134758314
Loss in iteration 196 : 0.4670783066347094
Loss in iteration 197 : 0.4670301155400922
Loss in iteration 198 : 0.46698243317168786
Loss in iteration 199 : 0.46693525262486085
Loss in iteration 200 : 0.4668885671082705
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.7865, training accuracy 0.788875, time elapsed: 5228 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 19.998616144858993
Loss in iteration 3 : 27.601506627795978
Loss in iteration 4 : 9.938174287340932
Loss in iteration 5 : 11.106207851814618
Loss in iteration 6 : 7.49172096612954
Loss in iteration 7 : 7.967658525853579
Loss in iteration 8 : 6.552448045373024
Loss in iteration 9 : 5.986946808556069
Loss in iteration 10 : 5.211545294583748
Loss in iteration 11 : 4.834898569203404
Loss in iteration 12 : 4.334936535605956
Loss in iteration 13 : 4.1615275872091235
Loss in iteration 14 : 3.803251995508256
Loss in iteration 15 : 3.7436215170213853
Loss in iteration 16 : 3.390202323257722
Loss in iteration 17 : 3.4040443117370907
Loss in iteration 18 : 3.0904297518919357
Loss in iteration 19 : 3.1406097310943073
Loss in iteration 20 : 2.8707146496490896
Loss in iteration 21 : 2.947224550217715
Loss in iteration 22 : 2.709419152248822
Loss in iteration 23 : 2.8021991893471783
Loss in iteration 24 : 2.587310760084994
Loss in iteration 25 : 2.692087623477606
Loss in iteration 26 : 2.4784760386544185
Loss in iteration 27 : 2.5965133818514645
Loss in iteration 28 : 2.3736041589783334
Loss in iteration 29 : 2.5134813251587014
Loss in iteration 30 : 2.26413009400574
Loss in iteration 31 : 2.445983281355484
Loss in iteration 32 : 2.1297988066688656
Loss in iteration 33 : 2.330615832275014
Loss in iteration 34 : 1.9622470157320793
Loss in iteration 35 : 2.1364128501552484
Loss in iteration 36 : 1.8405640051290486
Loss in iteration 37 : 2.011699329966354
Loss in iteration 38 : 1.7917621006382851
Loss in iteration 39 : 1.9711821949338217
Loss in iteration 40 : 1.814902718007218
Loss in iteration 41 : 2.014015083505373
Loss in iteration 42 : 1.958112642282354
Loss in iteration 43 : 2.1905289918525512
Loss in iteration 44 : 2.1453900501793104
Loss in iteration 45 : 2.363725504207513
Loss in iteration 46 : 2.1896524445075314
Loss in iteration 47 : 2.351775287804893
Loss in iteration 48 : 2.1289134208101532
Loss in iteration 49 : 2.251814533174777
Loss in iteration 50 : 2.0398683134989946
Loss in iteration 51 : 2.1427676611639663
Loss in iteration 52 : 1.958489673900417
Loss in iteration 53 : 2.0523024367491924
Loss in iteration 54 : 1.8917626852043268
Loss in iteration 55 : 1.9824165133443543
Loss in iteration 56 : 1.8385842416364744
Loss in iteration 57 : 1.9295434593989576
Loss in iteration 58 : 1.7960843446987627
Loss in iteration 59 : 1.8894395727252091
Loss in iteration 60 : 1.7614441596642392
Loss in iteration 61 : 1.8584296319917923
Loss in iteration 62 : 1.7323844603838936
Loss in iteration 63 : 1.8336631238548844
Loss in iteration 64 : 1.7072415247454504
Loss in iteration 65 : 1.8130679081787566
Loss in iteration 66 : 1.68492065836556
Loss in iteration 67 : 1.7952296324009982
Loss in iteration 68 : 1.6647969507205573
Loss in iteration 69 : 1.779248797603643
Loss in iteration 70 : 1.64659200332115
Loss in iteration 71 : 1.764598976051925
Loss in iteration 72 : 1.6302509184450542
Loss in iteration 73 : 1.7510049925760764
Loss in iteration 74 : 1.6158324679596203
Loss in iteration 75 : 1.7383449686534291
Loss in iteration 76 : 1.6034102789956148
Loss in iteration 77 : 1.7265646393913294
Loss in iteration 78 : 1.5929772935759765
Loss in iteration 79 : 1.7155915758345999
Loss in iteration 80 : 1.584355197831327
Loss in iteration 81 : 1.705253223587222
Loss in iteration 82 : 1.5771281220666586
Loss in iteration 83 : 1.6952231391563861
Loss in iteration 84 : 1.5706345115985059
Loss in iteration 85 : 1.6850289461614132
Loss in iteration 86 : 1.5640494872496842
Loss in iteration 87 : 1.6741416308745765
Loss in iteration 88 : 1.5565575378268242
Loss in iteration 89 : 1.6621255916075686
Loss in iteration 90 : 1.5475568168462566
Loss in iteration 91 : 1.6487825825502886
Loss in iteration 92 : 1.5367975197862325
Loss in iteration 93 : 1.6342128951750996
Loss in iteration 94 : 1.5243877604506009
Loss in iteration 95 : 1.6187646855427902
Loss in iteration 96 : 1.5106829487452456
Loss in iteration 97 : 1.6029098942643123
Loss in iteration 98 : 1.496133418971725
Loss in iteration 99 : 1.587114948184393
Loss in iteration 100 : 1.481159848858829
Loss in iteration 101 : 1.5717539298439434
Loss in iteration 102 : 1.4660850316346323
Loss in iteration 103 : 1.5570742177114194
Loss in iteration 104 : 1.4511163624913201
Loss in iteration 105 : 1.543200334952103
Loss in iteration 106 : 1.436360889703922
Loss in iteration 107 : 1.5301562880666622
Loss in iteration 108 : 1.421857207194734
Loss in iteration 109 : 1.51789169360431
Loss in iteration 110 : 1.407615623198321
Loss in iteration 111 : 1.5063047697962255
Loss in iteration 112 : 1.3936637180203828
Loss in iteration 113 : 1.495262719505601
Loss in iteration 114 : 1.3800960403127838
Loss in iteration 115 : 1.4846267031479958
Loss in iteration 116 : 1.3671239854731296
Loss in iteration 117 : 1.4742930299384363
Loss in iteration 118 : 1.3551160595737906
Loss in iteration 119 : 1.4642580639800076
Loss in iteration 120 : 1.3446113626979477
Loss in iteration 121 : 1.4546920938237897
Loss in iteration 122 : 1.3362817959456834
Loss in iteration 123 : 1.445970541886297
Loss in iteration 124 : 1.3308155626802793
Loss in iteration 125 : 1.4385935012233904
Loss in iteration 126 : 1.3287044870035325
Loss in iteration 127 : 1.4329652647545774
Loss in iteration 128 : 1.3299485153141117
Loss in iteration 129 : 1.4290911014788146
Loss in iteration 130 : 1.3337543269590832
Loss in iteration 131 : 1.4263292015409272
Loss in iteration 132 : 1.3384163227432377
Loss in iteration 133 : 1.4233870878705401
Loss in iteration 134 : 1.341651666937491
Loss in iteration 135 : 1.4187178219826941
Loss in iteration 136 : 1.341449331817878
Loss in iteration 137 : 1.4112140317856967
Loss in iteration 138 : 1.3369296816009624
Loss in iteration 139 : 1.4007288041119668
Loss in iteration 140 : 1.328523011071233
Loss in iteration 141 : 1.3880109476299836
Loss in iteration 142 : 1.3174375104991363
Loss in iteration 143 : 1.3741994531443198
Loss in iteration 144 : 1.3049856600013445
Loss in iteration 145 : 1.3603331814312518
Loss in iteration 146 : 1.2921875790662263
Loss in iteration 147 : 1.3471194499423214
Loss in iteration 148 : 1.2796745591300918
Loss in iteration 149 : 1.3349226940796817
Loss in iteration 150 : 1.2677513318300735
Loss in iteration 151 : 1.3238509995181689
Loss in iteration 152 : 1.256500737174656
Loss in iteration 153 : 1.3138560541348765
Loss in iteration 154 : 1.2458768182392346
Loss in iteration 155 : 1.3048117903722236
Loss in iteration 156 : 1.2357722133142879
Loss in iteration 157 : 1.296565188020926
Loss in iteration 158 : 1.2260625731612178
Loss in iteration 159 : 1.2889635788516125
Loss in iteration 160 : 1.216635485192724
Loss in iteration 161 : 1.2818654183716744
Loss in iteration 162 : 1.2074113072856183
Loss in iteration 163 : 1.2751415261438992
Loss in iteration 164 : 1.1983616232406993
Loss in iteration 165 : 1.268673968053931
Loss in iteration 166 : 1.1895286981740212
Loss in iteration 167 : 1.2623607531622951
Loss in iteration 168 : 1.1810461713980716
Loss in iteration 169 : 1.2561346119231853
Loss in iteration 170 : 1.1731566301864054
Loss in iteration 171 : 1.2499988279767542
Loss in iteration 172 : 1.1662154766085766
Loss in iteration 173 : 1.244068440622031
Loss in iteration 174 : 1.1606645730781389
Loss in iteration 175 : 1.2385859507905896
Loss in iteration 176 : 1.1569587721130075
Loss in iteration 177 : 1.2338748744676031
Loss in iteration 178 : 1.155439875206547
Loss in iteration 179 : 1.2302190489104954
Loss in iteration 180 : 1.156177031706719
Loss in iteration 181 : 1.2277025304477462
Loss in iteration 182 : 1.1588228548657853
Loss in iteration 183 : 1.226082329299606
Loss in iteration 184 : 1.1625589150893847
Loss in iteration 185 : 1.2247697363604844
Loss in iteration 186 : 1.1662062543701923
Loss in iteration 187 : 1.2229633805135904
Loss in iteration 188 : 1.168524352308751
Loss in iteration 189 : 1.2199089610469738
Loss in iteration 190 : 1.1685974446111462
Loss in iteration 191 : 1.2151702577569459
Loss in iteration 192 : 1.1660980079762158
Loss in iteration 193 : 1.2087582480249615
Loss in iteration 194 : 1.161278603742015
Loss in iteration 195 : 1.20105506967188
Loss in iteration 196 : 1.1547456066979924
Loss in iteration 197 : 1.1926143362514035
Loss in iteration 198 : 1.1471903738205387
Loss in iteration 199 : 1.1839702222019854
Loss in iteration 200 : 1.13920626045584
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.743, training accuracy 0.740875, time elapsed: 5357 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.7987920457267951
Loss in iteration 3 : 2.7262015533145196
Loss in iteration 4 : 3.7077077599143125
Loss in iteration 5 : 1.4197840910810138
Loss in iteration 6 : 3.317468239291409
Loss in iteration 7 : 1.4336486474101982
Loss in iteration 8 : 3.4581175577835688
Loss in iteration 9 : 1.0286892946550283
Loss in iteration 10 : 2.6036255135161985
Loss in iteration 11 : 1.560247345731747
Loss in iteration 12 : 2.781029111134265
Loss in iteration 13 : 1.186244380814545
Loss in iteration 14 : 2.1823281817686295
Loss in iteration 15 : 1.45341234780686
Loss in iteration 16 : 2.1166248282186864
Loss in iteration 17 : 1.3274238107947067
Loss in iteration 18 : 1.8295861726710487
Loss in iteration 19 : 1.3448439682947808
Loss in iteration 20 : 1.6613658860055567
Loss in iteration 21 : 1.2991245435227032
Loss in iteration 22 : 1.5005354283121215
Loss in iteration 23 : 1.2515832823979423
Loss in iteration 24 : 1.3697211040765322
Loss in iteration 25 : 1.1982746210544466
Loss in iteration 26 : 1.261626644457462
Loss in iteration 27 : 1.1448997887142864
Loss in iteration 28 : 1.1729460675319079
Loss in iteration 29 : 1.0946604933319453
Loss in iteration 30 : 1.1001869130387254
Loss in iteration 31 : 1.0492891912074302
Loss in iteration 32 : 1.0401373314085773
Loss in iteration 33 : 1.0093559450200669
Loss in iteration 34 : 0.990021902426715
Loss in iteration 35 : 0.9747091296405408
Loss in iteration 36 : 0.9475491294227613
Loss in iteration 37 : 0.9448518618881819
Loss in iteration 38 : 0.9108607903724176
Loss in iteration 39 : 0.9191929541543734
Loss in iteration 40 : 0.8784380441224049
Loss in iteration 41 : 0.8972183754353795
Loss in iteration 42 : 0.8490264469510211
Loss in iteration 43 : 0.8787183341915215
Loss in iteration 44 : 0.821682419734997
Loss in iteration 45 : 0.8643466425268479
Loss in iteration 46 : 0.796291353694976
Loss in iteration 47 : 0.856602013186493
Loss in iteration 48 : 0.7748598209914451
Loss in iteration 49 : 0.8561672226779067
Loss in iteration 50 : 0.7574867252584699
Loss in iteration 51 : 0.8406413809737233
Loss in iteration 52 : 0.7302793794230502
Loss in iteration 53 : 0.7890290182670371
Loss in iteration 54 : 0.6947007502153876
Loss in iteration 55 : 0.7386564209922801
Loss in iteration 56 : 0.677754734013327
Loss in iteration 57 : 0.7325467125619796
Loss in iteration 58 : 0.7105681867648884
Loss in iteration 59 : 0.7716497795966129
Loss in iteration 60 : 0.7198702626618303
Loss in iteration 61 : 0.7106763553848883
Loss in iteration 62 : 0.6471357426982822
Loss in iteration 63 : 0.6285165165000999
Loss in iteration 64 : 0.594256350448122
Loss in iteration 65 : 0.5851729792781328
Loss in iteration 66 : 0.566428322810763
Loss in iteration 67 : 0.5652339582797793
Loss in iteration 68 : 0.5539927451291251
Loss in iteration 69 : 0.5608525060955539
Loss in iteration 70 : 0.5540588428878334
Loss in iteration 71 : 0.5725447757361641
Loss in iteration 72 : 0.5681848362543714
Loss in iteration 73 : 0.6040227815640037
Loss in iteration 74 : 0.6004653696439898
Loss in iteration 75 : 0.6578618730125979
Loss in iteration 76 : 0.6637506761961639
Loss in iteration 77 : 0.7398653418314541
Loss in iteration 78 : 0.7731147480449272
Loss in iteration 79 : 0.8386608649804929
Loss in iteration 80 : 0.8722858100175876
Loss in iteration 81 : 0.8873397315378098
Loss in iteration 82 : 0.8917100933112183
Loss in iteration 83 : 0.8783533190388261
Loss in iteration 84 : 0.8693399036309367
Loss in iteration 85 : 0.8521763922520558
Loss in iteration 86 : 0.8404169751334847
Loss in iteration 87 : 0.8257652122061514
Loss in iteration 88 : 0.8141221361787436
Loss in iteration 89 : 0.8026060136279414
Loss in iteration 90 : 0.7917383620636049
Loss in iteration 91 : 0.7829034925144052
Loss in iteration 92 : 0.7729319070883407
Loss in iteration 93 : 0.7662373548008715
Loss in iteration 94 : 0.7571378114690365
Loss in iteration 95 : 0.7521281530330112
Loss in iteration 96 : 0.743831165345306
Loss in iteration 97 : 0.7401453304829124
Loss in iteration 98 : 0.7325658986488831
Loss in iteration 99 : 0.7299197854711313
Loss in iteration 100 : 0.7229702206507455
Loss in iteration 101 : 0.7211394417928024
Loss in iteration 102 : 0.7147363791784082
Loss in iteration 103 : 0.7135428778568889
Loss in iteration 104 : 0.7076110779713753
Loss in iteration 105 : 0.7069131526455451
Loss in iteration 106 : 0.7013871317367675
Loss in iteration 107 : 0.7010719321990463
Loss in iteration 108 : 0.6958961682213651
Loss in iteration 109 : 0.6958739187074406
Loss in iteration 110 : 0.6910022122472865
Loss in iteration 111 : 0.6912016583064616
Loss in iteration 112 : 0.6865960640793964
Loss in iteration 113 : 0.686960819371324
Loss in iteration 114 : 0.6825904225055692
Loss in iteration 115 : 0.6830760071737724
Loss in iteration 116 : 0.6789157112414798
Loss in iteration 117 : 0.6794871426445013
Loss in iteration 118 : 0.6755165614404315
Loss in iteration 119 : 0.6761463977831025
Loss in iteration 120 : 0.6723488933472714
Loss in iteration 121 : 0.6730156534927114
Loss in iteration 122 : 0.6693775318595528
Loss in iteration 123 : 0.6700644282413744
Loss in iteration 124 : 0.6665742862396022
Loss in iteration 125 : 0.6672682169762504
Loss in iteration 126 : 0.6639164238967694
Loss in iteration 127 : 0.6646071774034408
Loss in iteration 128 : 0.6613854714559929
Loss in iteration 129 : 0.6620651031982115
Loss in iteration 130 : 0.6589662822314273
Loss in iteration 131 : 0.6596286291800083
Loss in iteration 132 : 0.6566463166455422
Loss in iteration 133 : 0.6572866205329783
Loss in iteration 134 : 0.6544150901271715
Loss in iteration 135 : 0.6550297056953676
Loss in iteration 136 : 0.6522637508694484
Loss in iteration 137 : 0.6528499198465213
Loss in iteration 138 : 0.6501847570570767
Loss in iteration 139 : 0.6507404325473839
Loss in iteration 140 : 0.6481716295204654
Loss in iteration 141 : 0.6486953388232377
Loss in iteration 142 : 0.6462187611433228
Loss in iteration 143 : 0.6467094977568606
Loss in iteration 144 : 0.6443212687532999
Loss in iteration 145 : 0.6447784065270914
Loss in iteration 146 : 0.6424748767432839
Loss in iteration 147 : 0.6428981008783408
Loss in iteration 148 : 0.640675824419284
Loss in iteration 149 : 0.6410650753611501
Loss in iteration 150 : 0.6389207911757346
Loss in iteration 151 : 0.6392762184668804
Loss in iteration 152 : 0.6372068351829167
Loss in iteration 153 : 0.6375287591067685
Loss in iteration 154 : 0.635531342444195
Loss in iteration 155 : 0.6358202218585245
Loss in iteration 156 : 0.6338919839372177
Loss in iteration 157 : 0.6341483891072546
Loss in iteration 158 : 0.6322866791708605
Loss in iteration 159 : 0.6325112687105972
Loss in iteration 160 : 0.6307135649303305
Loss in iteration 161 : 0.6309070661742419
Loss in iteration 162 : 0.6291709682949251
Loss in iteration 163 : 0.6293341605749085
Loss in iteration 164 : 0.6276573832327403
Loss in iteration 165 : 0.6277910836441936
Loss in iteration 166 : 0.6261714502317759
Loss in iteration 167 : 0.6262765015510648
Loss in iteration 168 : 0.6247119385366973
Loss in iteration 169 : 0.624789199009456
Loss in iteration 170 : 0.6232777306396854
Loss in iteration 171 : 0.6233280654018966
Loss in iteration 172 : 0.6218678087317892
Loss in iteration 173 : 0.6218920826581947
Loss in iteration 174 : 0.6204812428651144
Loss in iteration 175 : 0.6204803146653097
Loss in iteration 176 : 0.6191171806105473
Loss in iteration 177 : 0.6190918980141984
Loss in iteration 178 : 0.6177748380234398
Loss in iteration 179 : 0.6177260339139735
Loss in iteration 180 : 0.6164534917529568
Loss in iteration 181 : 0.6163819811245371
Loss in iteration 182 : 0.6151524721506747
Loss in iteration 183 : 0.6150590497769771
Loss in iteration 184 : 0.6138711572514113
Loss in iteration 185 : 0.6137565959669703
Loss in iteration 186 : 0.612608967514626
Loss in iteration 187 : 0.6124740170205847
Loss in iteration 188 : 0.6113653612283995
Loss in iteration 189 : 0.6112107473444581
Loss in iteration 190 : 0.6101398304901325
Loss in iteration 191 : 0.6099662547835609
Loss in iteration 192 : 0.6089318976889291
Loss in iteration 193 : 0.6087400374196233
Loss in iteration 194 : 0.6077411124242084
Loss in iteration 195 : 0.607531620752172
Loss in iteration 196 : 0.6065670488035821
Loss in iteration 197 : 0.6063405552117843
Loss in iteration 198 : 0.605409303070481
Loss in iteration 199 : 0.6051664139619682
Loss in iteration 200 : 0.6042674915185523
Testing accuracy  of updater 3 on alg 0 with rate 7.0 = 0.7525, training accuracy 0.746875, time elapsed: 4327 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6788881480339669
Loss in iteration 3 : 0.6985427933239086
Loss in iteration 4 : 0.7892915690672677
Loss in iteration 5 : 0.8619022022362592
Loss in iteration 6 : 1.0120811127331537
Loss in iteration 7 : 0.8315206425263685
Loss in iteration 8 : 0.9482482441598505
Loss in iteration 9 : 0.7802929105895481
Loss in iteration 10 : 0.8719671500334277
Loss in iteration 11 : 0.7384155826455622
Loss in iteration 12 : 0.8022077625299298
Loss in iteration 13 : 0.7012778892695651
Loss in iteration 14 : 0.7419827408061117
Loss in iteration 15 : 0.6675658377434295
Loss in iteration 16 : 0.691461509280054
Loss in iteration 17 : 0.6372122777420438
Loss in iteration 18 : 0.649606382891796
Loss in iteration 19 : 0.610330165876949
Loss in iteration 20 : 0.6152556765782329
Loss in iteration 21 : 0.5869443680003604
Loss in iteration 22 : 0.587305212295709
Loss in iteration 23 : 0.5669545389298198
Loss in iteration 24 : 0.5647696063633625
Loss in iteration 25 : 0.5501661529482478
Loss in iteration 26 : 0.5467975258433894
Loss in iteration 27 : 0.5363358498792723
Loss in iteration 28 : 0.5326643868589672
Loss in iteration 29 : 0.5251969629241778
Loss in iteration 30 : 0.5217476946789311
Loss in iteration 31 : 0.5164512589852502
Loss in iteration 32 : 0.5134877074484456
Loss in iteration 33 : 0.5097564723633838
Loss in iteration 34 : 0.5073596825893306
Loss in iteration 35 : 0.5047390005084574
Loss in iteration 36 : 0.5028757778589464
Loss in iteration 37 : 0.501024648999436
Loss in iteration 38 : 0.4996034851681069
Loss in iteration 39 : 0.49827054457403525
Loss in iteration 40 : 0.4971830266561187
Loss in iteration 41 : 0.49618854036211985
Loss in iteration 42 : 0.4953353927121843
Loss in iteration 43 : 0.4945560930470678
Loss in iteration 44 : 0.49385933967092294
Loss in iteration 45 : 0.4932147764917515
Loss in iteration 46 : 0.4926197133451212
Loss in iteration 47 : 0.49206006372091204
Loss in iteration 48 : 0.49153157177133416
Loss in iteration 49 : 0.4910274575828546
Loss in iteration 50 : 0.49054446708444943
Loss in iteration 51 : 0.4900792112160536
Loss in iteration 52 : 0.4896296171290455
Loss in iteration 53 : 0.4891939077452605
Loss in iteration 54 : 0.4887708262301671
Loss in iteration 55 : 0.4883593691630078
Loss in iteration 56 : 0.4879587509802625
Loss in iteration 57 : 0.48756834086248757
Loss in iteration 58 : 0.487187604743902
Loss in iteration 59 : 0.48681609420701466
Loss in iteration 60 : 0.48645340754693905
Loss in iteration 61 : 0.48609919051212763
Loss in iteration 62 : 0.4857531146576714
Loss in iteration 63 : 0.4854148797581173
Loss in iteration 64 : 0.4850842025333745
Loss in iteration 65 : 0.4847608184660517
Loss in iteration 66 : 0.48444447599701873
Loss in iteration 67 : 0.48413493751299547
Loss in iteration 68 : 0.4838319762820551
Loss in iteration 69 : 0.4835353768284556
Loss in iteration 70 : 0.48324493322052586
Loss in iteration 71 : 0.4829604490771508
Loss in iteration 72 : 0.48268173652955254
Loss in iteration 73 : 0.482408616036454
Loss in iteration 74 : 0.48214091569220563
Loss in iteration 75 : 0.48187847095901193
Loss in iteration 76 : 0.4816211241627043
Loss in iteration 77 : 0.4813687242015951
Loss in iteration 78 : 0.4811211261517037
Loss in iteration 79 : 0.4808781909819448
Loss in iteration 80 : 0.4806397852291109
Loss in iteration 81 : 0.4804057807324552
Loss in iteration 82 : 0.48017605435733285
Loss in iteration 83 : 0.4799504877536439
Loss in iteration 84 : 0.47972896711614654
Loss in iteration 85 : 0.47951138296711554
Loss in iteration 86 : 0.4792976299459411
Loss in iteration 87 : 0.4790876066145426
Loss in iteration 88 : 0.4788812152712957
Loss in iteration 89 : 0.4786783617770883
Loss in iteration 90 : 0.4784789553899548
Loss in iteration 91 : 0.4782829086095624
Loss in iteration 92 : 0.4780901370297195
Loss in iteration 93 : 0.4779005591992007
Loss in iteration 94 : 0.4777140964898361
Loss in iteration 95 : 0.4775306729717674
Loss in iteration 96 : 0.4773502152952039
Loss in iteration 97 : 0.477172652578423
Loss in iteration 98 : 0.4769979163015526
Loss in iteration 99 : 0.4768259402058494
Loss in iteration 100 : 0.47665666019809827
Loss in iteration 101 : 0.4764900142598678
Loss in iteration 102 : 0.47632594236131365
Loss in iteration 103 : 0.47616438637928776
Loss in iteration 104 : 0.4760052900194801
Loss in iteration 105 : 0.47584859874239155
Loss in iteration 106 : 0.4756942596928952
Loss in iteration 107 : 0.475542221633206
Loss in iteration 108 : 0.4753924348790522
Loss in iteration 109 : 0.4752448512388834
Loss in iteration 110 : 0.47509942395593935
Loss in iteration 111 : 0.47495610765303464
Loss in iteration 112 : 0.47481485827989944
Loss in iteration 113 : 0.47467563306294885
Loss in iteration 114 : 0.4745383904573499
Loss in iteration 115 : 0.4744030901012615
Loss in iteration 116 : 0.4742696927721342
Loss in iteration 117 : 0.47413816034496553
Loss in iteration 118 : 0.474008455752406
Loss in iteration 119 : 0.4738805429466241
Loss in iteration 120 : 0.47375438686283455
Loss in iteration 121 : 0.47362995338441743
Loss in iteration 122 : 0.4735072093095286
Loss in iteration 123 : 0.47338612231915295
Loss in iteration 124 : 0.4732666609465009
Loss in iteration 125 : 0.4731487945477098
Loss in iteration 126 : 0.4730324932737568
Loss in iteration 127 : 0.472917728043551
Loss in iteration 128 : 0.4728044705181376
Loss in iteration 129 : 0.472692693075947
Loss in iteration 130 : 0.472582368789069
Loss in iteration 131 : 0.4724734714004719
Loss in iteration 132 : 0.47236597530214636
Loss in iteration 133 : 0.4722598555141146
Loss in iteration 134 : 0.4721550876642761
Loss in iteration 135 : 0.4720516479690417
Loss in iteration 136 : 0.4719495132147284
Loss in iteration 137 : 0.47184866073967424
Loss in iteration 138 : 0.4717490684170447
Loss in iteration 139 : 0.4716507146382905
Loss in iteration 140 : 0.4715535782972494
Loss in iteration 141 : 0.47145763877483
Loss in iteration 142 : 0.4713628759242895
Loss in iteration 143 : 0.4712692700570402
Loss in iteration 144 : 0.47117680192900524
Loss in iteration 145 : 0.4710854527274537
Loss in iteration 146 : 0.4709952040583331
Loss in iteration 147 : 0.4709060379340527
Loss in iteration 148 : 0.4708179367617146
Loss in iteration 149 : 0.4707308833317606
Loss in iteration 150 : 0.4706448608070287
Loss in iteration 151 : 0.4705598527121919
Loss in iteration 152 : 0.470475842923574
Loss in iteration 153 : 0.4703928156593162
Loss in iteration 154 : 0.4703107554698889
Loss in iteration 155 : 0.4702296472289278
Loss in iteration 156 : 0.4701494761243866
Loss in iteration 157 : 0.4700702276499883
Loss in iteration 158 : 0.46999188759697164
Loss in iteration 159 : 0.469914442046104
Loss in iteration 160 : 0.469837877359977
Loss in iteration 161 : 0.4697621801755422
Loss in iteration 162 : 0.46968733739690416
Loss in iteration 163 : 0.4696133361883423
Loss in iteration 164 : 0.46954016396757164
Loss in iteration 165 : 0.4694678083992054
Loss in iteration 166 : 0.4693962573884412
Loss in iteration 167 : 0.4693254990749434
Loss in iteration 168 : 0.46925552182692604
Loss in iteration 169 : 0.46918631423541024
Loss in iteration 170 : 0.4691178651086775
Loss in iteration 171 : 0.46905016346688444
Loss in iteration 172 : 0.46898319853684994
Loss in iteration 173 : 0.4689169597470035
Loss in iteration 174 : 0.46885143672248447
Loss in iteration 175 : 0.4687866192803937
Loss in iteration 176 : 0.4687224974251898
Loss in iteration 177 : 0.46865906134422186
Loss in iteration 178 : 0.4685963014033946
Loss in iteration 179 : 0.46853420814296726
Loss in iteration 180 : 0.4684727722734725
Loss in iteration 181 : 0.4684119846717567
Loss in iteration 182 : 0.46835183637713385
Loss in iteration 183 : 0.4682923185876578
Loss in iteration 184 : 0.4682334226564909
Loss in iteration 185 : 0.4681751400883909
Loss in iteration 186 : 0.4681174625362821
Loss in iteration 187 : 0.4680603817979383
Loss in iteration 188 : 0.46800388981275354
Loss in iteration 189 : 0.467947978658597
Loss in iteration 190 : 0.4678926405487664
Loss in iteration 191 : 0.4678378678290203
Loss in iteration 192 : 0.467783652974693
Loss in iteration 193 : 0.46772998858788384
Loss in iteration 194 : 0.4676768673947346
Loss in iteration 195 : 0.46762428224276814
Loss in iteration 196 : 0.4675722260983051
Loss in iteration 197 : 0.4675206920439495
Loss in iteration 198 : 0.46746967327614
Loss in iteration 199 : 0.46741916310276665
Loss in iteration 200 : 0.4673691549408467
Testing accuracy  of updater 3 on alg 0 with rate 4.0 = 0.7845, training accuracy 0.787125, time elapsed: 4333 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6810928712640398
Loss in iteration 3 : 0.6744522438796804
Loss in iteration 4 : 0.6688530906646639
Loss in iteration 5 : 0.6637493995527475
Loss in iteration 6 : 0.658983358749135
Loss in iteration 7 : 0.6544808633321992
Loss in iteration 8 : 0.6502001405289558
Loss in iteration 9 : 0.646114799183914
Loss in iteration 10 : 0.6422063983774576
Loss in iteration 11 : 0.6384608776950924
Loss in iteration 12 : 0.6348667635115499
Loss in iteration 13 : 0.6314142344460865
Loss in iteration 14 : 0.628094615603346
Loss in iteration 15 : 0.6249000921228014
Loss in iteration 16 : 0.6218235374583815
Loss in iteration 17 : 0.6188584031483667
Loss in iteration 18 : 0.6159986425313834
Loss in iteration 19 : 0.6132386539480869
Loss in iteration 20 : 0.6105732357198821
Loss in iteration 21 : 0.6079975487181158
Loss in iteration 22 : 0.6055070841919187
Loss in iteration 23 : 0.6030976355084211
Loss in iteration 24 : 0.6007652729886308
Loss in iteration 25 : 0.5985063213107453
Loss in iteration 26 : 0.5963173391136014
Loss in iteration 27 : 0.594195100525642
Loss in iteration 28 : 0.5921365784010278
Loss in iteration 29 : 0.5901389290809442
Loss in iteration 30 : 0.5881994785235971
Loss in iteration 31 : 0.5863157096657017
Loss in iteration 32 : 0.5844852508937134
Loss in iteration 33 : 0.582705865516169
Loss in iteration 34 : 0.5809754421398181
Loss in iteration 35 : 0.5792919858623464
Loss in iteration 36 : 0.5776536102033621
Loss in iteration 37 : 0.5760585297034115
Loss in iteration 38 : 0.5745050531279732
Loss in iteration 39 : 0.5729915772197797
Loss in iteration 40 : 0.5715165809486854
Loss in iteration 41 : 0.570078620213377
Loss in iteration 42 : 0.5686763229539242
Loss in iteration 43 : 0.5673083846382565
Loss in iteration 44 : 0.5659735640893667
Loss in iteration 45 : 0.5646706796233368
Loss in iteration 46 : 0.5633986054712321
Loss in iteration 47 : 0.5621562684605238
Loss in iteration 48 : 0.5609426449340986
Loss in iteration 49 : 0.5597567578869551
Loss in iteration 50 : 0.5585976743026342
Loss in iteration 51 : 0.5574645026730785
Loss in iteration 52 : 0.5563563906871305
Loss in iteration 53 : 0.5552725230742623
Loss in iteration 54 : 0.554212119591317
Loss in iteration 55 : 0.5531744331411547
Loss in iteration 56 : 0.5521587480130852
Loss in iteration 57 : 0.5511643782358412
Loss in iteration 58 : 0.550190666034667
Loss in iteration 59 : 0.5492369803848028
Loss in iteration 60 : 0.5483027156543138
Loss in iteration 61 : 0.5473872903298019
Loss in iteration 62 : 0.5464901458190529
Loss in iteration 63 : 0.5456107453251996
Loss in iteration 64 : 0.5447485727873839
Loss in iteration 65 : 0.5439031318833158
Loss in iteration 66 : 0.543073945089506
Loss in iteration 67 : 0.5422605527952543
Loss in iteration 68 : 0.5414625124668069
Loss in iteration 69 : 0.5406793978583377
Loss in iteration 70 : 0.5399107982667147
Loss in iteration 71 : 0.5391563178271715
Loss in iteration 72 : 0.5384155748472911
Loss in iteration 73 : 0.5376882011768407
Loss in iteration 74 : 0.5369738416112125
Loss in iteration 75 : 0.5362721533263722
Loss in iteration 76 : 0.535582805343368
Loss in iteration 77 : 0.5349054780205976
Loss in iteration 78 : 0.5342398625721435
Loss in iteration 79 : 0.5335856606106241
Loss in iteration 80 : 0.5329425837130904
Loss in iteration 81 : 0.532310353008624
Loss in iteration 82 : 0.5316886987863609
Loss in iteration 83 : 0.5310773601227623
Loss in iteration 84 : 0.5304760845270329
Loss in iteration 85 : 0.5298846276036495
Loss in iteration 86 : 0.5293027527310376
Loss in iteration 87 : 0.5287302307555078
Loss in iteration 88 : 0.528166839699581
Loss in iteration 89 : 0.527612364483937
Loss in iteration 90 : 0.5270665966622349
Loss in iteration 91 : 0.526529334168112
Loss in iteration 92 : 0.5260003810737035
Loss in iteration 93 : 0.5254795473590859
Loss in iteration 94 : 0.5249666486920493
Loss in iteration 95 : 0.5244615062176836
Loss in iteration 96 : 0.5239639463572384
Loss in iteration 97 : 0.5234738006158112
Loss in iteration 98 : 0.5229909053983954
Loss in iteration 99 : 0.5225151018338632
Loss in iteration 100 : 0.5220462356064977
Loss in iteration 101 : 0.5215841567946884
Loss in iteration 102 : 0.5211287197164352
Loss in iteration 103 : 0.520679782781328
Loss in iteration 104 : 0.520237208348693
Loss in iteration 105 : 0.5198008625915934
Loss in iteration 106 : 0.519370615366417
Loss in iteration 107 : 0.5189463400877716
Loss in iteration 108 : 0.518527913608449
Loss in iteration 109 : 0.5181152161042082
Loss in iteration 110 : 0.517708130963162
Loss in iteration 111 : 0.5173065446795446
Loss in iteration 112 : 0.5169103467516646
Loss in iteration 113 : 0.5165194295838528
Loss in iteration 114 : 0.5161336883922124
Loss in iteration 115 : 0.5157530211140194
Loss in iteration 116 : 0.5153773283205887
Loss in iteration 117 : 0.5150065131334639
Loss in iteration 118 : 0.5146404811437817
Loss in iteration 119 : 0.5142791403346691
Loss in iteration 120 : 0.513922401006543
Loss in iteration 121 : 0.513570175705184
Loss in iteration 122 : 0.513222379152467
Loss in iteration 123 : 0.5128789281796344
Loss in iteration 124 : 0.512539741663
Loss in iteration 125 : 0.512204740461988
Loss in iteration 126 : 0.5118738473594018
Loss in iteration 127 : 0.5115469870038313
Loss in iteration 128 : 0.5112240858541136
Loss in iteration 129 : 0.5109050721257579
Loss in iteration 130 : 0.510589875739256
Loss in iteration 131 : 0.5102784282701968
Loss in iteration 132 : 0.5099706629011235
Loss in iteration 133 : 0.5096665143750486
Loss in iteration 134 : 0.5093659189505686
Loss in iteration 135 : 0.5090688143585187
Loss in iteration 136 : 0.508775139760089
Loss in iteration 137 : 0.5084848357063697
Loss in iteration 138 : 0.5081978440992478
Loss in iteration 139 : 0.5079141081536098
Loss in iteration 140 : 0.5076335723608102
Loss in iteration 141 : 0.5073561824533342
Loss in iteration 142 : 0.507081885370636
Loss in iteration 143 : 0.5068106292260848
Loss in iteration 144 : 0.5065423632749877
Loss in iteration 145 : 0.5062770378836522
Loss in iteration 146 : 0.5060146044994378
Loss in iteration 147 : 0.5057550156217712
Loss in iteration 148 : 0.505498224774078
Loss in iteration 149 : 0.5052441864766158
Loss in iteration 150 : 0.5049928562201472
Loss in iteration 151 : 0.5047441904404516
Loss in iteration 152 : 0.5044981464936272
Loss in iteration 153 : 0.5042546826321554
Loss in iteration 154 : 0.5040137579817129
Loss in iteration 155 : 0.5037753325186868
Loss in iteration 156 : 0.5035393670483836
Loss in iteration 157 : 0.5033058231839008
Loss in iteration 158 : 0.5030746633256304
Loss in iteration 159 : 0.5028458506413901
Loss in iteration 160 : 0.5026193490471415
Loss in iteration 161 : 0.5023951231882884
Loss in iteration 162 : 0.5021731384215277
Loss in iteration 163 : 0.5019533607972362
Loss in iteration 164 : 0.5017357570423758
Loss in iteration 165 : 0.5015202945438979
Loss in iteration 166 : 0.5013069413326341
Loss in iteration 167 : 0.5010956660676472
Loss in iteration 168 : 0.5008864380210413
Loss in iteration 169 : 0.5006792270631983
Loss in iteration 170 : 0.5004740036484471
Loss in iteration 171 : 0.5002707388011329
Loss in iteration 172 : 0.5000694041020812
Loss in iteration 173 : 0.49986997167544744
Loss in iteration 174 : 0.49967241417592984
Loss in iteration 175 : 0.4994767047763448
Loss in iteration 176 : 0.4992828171555426
Loss in iteration 177 : 0.49909072548666217
Loss in iteration 178 : 0.49890040442570305
Loss in iteration 179 : 0.49871182910041983
Loss in iteration 180 : 0.4985249750995099
Loss in iteration 181 : 0.49833981846210235
Loss in iteration 182 : 0.4981563356675277
Loss in iteration 183 : 0.4979745036253636
Loss in iteration 184 : 0.49779429966574923
Loss in iteration 185 : 0.4976157015299563
Loss in iteration 186 : 0.4974386873612121
Loss in iteration 187 : 0.4972632356957643
Loss in iteration 188 : 0.4970893254541839
Loss in iteration 189 : 0.4969169359328941
Loss in iteration 190 : 0.4967460467959204
Loss in iteration 191 : 0.4965766380668555
Loss in iteration 192 : 0.49640869012103633
Loss in iteration 193 : 0.4962421836779177
Loss in iteration 194 : 0.4960770997936456
Loss in iteration 195 : 0.49591341985381626
Loss in iteration 196 : 0.49575112556642814
Loss in iteration 197 : 0.49559019895500195
Loss in iteration 198 : 0.4954306223518842
Loss in iteration 199 : 0.4952723783917136
Loss in iteration 200 : 0.4951154500050543
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.7785, training accuracy 0.777875, time elapsed: 4296 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6841222468437518
Loss in iteration 3 : 0.6785553692096401
Loss in iteration 4 : 0.6741124600296955
Loss in iteration 5 : 0.6701575983548502
Loss in iteration 6 : 0.6664860305481224
Loss in iteration 7 : 0.6630145129856482
Loss in iteration 8 : 0.65970074004002
Loss in iteration 9 : 0.656519504621284
Loss in iteration 10 : 0.6534542058679835
Loss in iteration 11 : 0.650493144641174
Loss in iteration 12 : 0.6476276234652496
Loss in iteration 13 : 0.6448508678746616
Loss in iteration 14 : 0.6421573781593609
Loss in iteration 15 : 0.6395425266772323
Loss in iteration 16 : 0.6370023020991801
Loss in iteration 17 : 0.6345331440192271
Loss in iteration 18 : 0.6321318341344715
Loss in iteration 19 : 0.6297954233067173
Loss in iteration 20 : 0.6275211816364077
Loss in iteration 21 : 0.6253065634490128
Loss in iteration 22 : 0.6231491820482175
Loss in iteration 23 : 0.6210467909411683
Loss in iteration 24 : 0.6189972694111611
Loss in iteration 25 : 0.6169986110584452
Loss in iteration 26 : 0.6150489144074835
Loss in iteration 27 : 0.613146374986819
Loss in iteration 28 : 0.6112892784870557
Loss in iteration 29 : 0.6094759947321247
Loss in iteration 30 : 0.607704972283731
Loss in iteration 31 : 0.605974733554534
Loss in iteration 32 : 0.6042838703422793
Loss in iteration 33 : 0.6026310397215385
Loss in iteration 34 : 0.6010149602459915
Loss in iteration 35 : 0.5994344084252392
Loss in iteration 36 : 0.5978882154477156
Loss in iteration 37 : 0.5963752641265223
Loss in iteration 38 : 0.5948944860487628
Loss in iteration 39 : 0.5934448589117368
Loss in iteration 40 : 0.5920254040313551
Loss in iteration 41 : 0.5906351840098227
Loss in iteration 42 : 0.5892733005508585
Loss in iteration 43 : 0.5879388924118435
Loss in iteration 44 : 0.5866311334831508
Loss in iteration 45 : 0.5853492309857516
Loss in iteration 46 : 0.5840924237788578
Loss in iteration 47 : 0.5828599807700008
Loss in iteration 48 : 0.5816511994205352
Loss in iteration 49 : 0.5804654043400546
Loss in iteration 50 : 0.5793019459637075
Loss in iteration 51 : 0.5781601993068307
Loss in iteration 52 : 0.5770395627917244
Loss in iteration 53 : 0.5759394571417853
Loss in iteration 54 : 0.5748593243385436
Loss in iteration 55 : 0.5737986266374882
Loss in iteration 56 : 0.5727568456388379
Loss in iteration 57 : 0.5717334814097302
Loss in iteration 58 : 0.5707280516545074
Loss in iteration 59 : 0.5697400909300572
Loss in iteration 60 : 0.5687691499033514
Loss in iteration 61 : 0.5678147946485452
Loss in iteration 62 : 0.5668766059811753
Loss in iteration 63 : 0.5659541788271786
Loss in iteration 64 : 0.5650471216245935
Loss in iteration 65 : 0.56415505575597
Loss in iteration 66 : 0.5632776150096463
Loss in iteration 67 : 0.5624144450681648
Loss in iteration 68 : 0.5615652030222303
Loss in iteration 69 : 0.5607295569087187
Loss in iteration 70 : 0.5599071852713285
Loss in iteration 71 : 0.5590977767425848
Loss in iteration 72 : 0.55830102964597
Loss in iteration 73 : 0.557516651617047
Loss in iteration 74 : 0.5567443592425092
Loss in iteration 75 : 0.5559838777161538
Loss in iteration 76 : 0.5552349405108624
Loss in iteration 77 : 0.5544972890656948
Loss in iteration 78 : 0.5537706724872883
Loss in iteration 79 : 0.5530548472647951
Loss in iteration 80 : 0.5523495769976264
Loss in iteration 81 : 0.5516546321353346
Loss in iteration 82 : 0.5509697897290016
Loss in iteration 83 : 0.5502948331935215
Loss in iteration 84 : 0.5496295520802249
Loss in iteration 85 : 0.5489737418593181
Loss in iteration 86 : 0.5483272037116395
Loss in iteration 87 : 0.547689744329246
Loss in iteration 88 : 0.5470611757244231
Loss in iteration 89 : 0.5464413150466666
Loss in iteration 90 : 0.5458299844072688
Loss in iteration 91 : 0.5452270107111222
Loss in iteration 92 : 0.5446322254954072
Loss in iteration 93 : 0.5440454647748144
Loss in iteration 94 : 0.5434665688930059
Loss in iteration 95 : 0.5428953823800179
Loss in iteration 96 : 0.5423317538153128
Loss in iteration 97 : 0.541775535696237
Loss in iteration 98 : 0.5412265843116195
Loss in iteration 99 : 0.5406847596202844
Loss in iteration 100 : 0.5401499251342392
Loss in iteration 101 : 0.5396219478063563
Loss in iteration 102 : 0.5391006979223051
Loss in iteration 103 : 0.5385860489965846
Loss in iteration 104 : 0.5380778776724462
Loss in iteration 105 : 0.5375760636255504
Loss in iteration 106 : 0.5370804894711907
Loss in iteration 107 : 0.5365910406749286
Loss in iteration 108 : 0.5361076054664918
Loss in iteration 109 : 0.5356300747568012
Loss in iteration 110 : 0.5351583420579871
Loss in iteration 111 : 0.5346923034062692
Loss in iteration 112 : 0.5342318572875906
Loss in iteration 113 : 0.5337769045658709
Loss in iteration 114 : 0.5333273484137928
Loss in iteration 115 : 0.5328830942459957
Loss in iteration 116 : 0.5324440496545997
Loss in iteration 117 : 0.532010124346946
Loss in iteration 118 : 0.5315812300854772
Loss in iteration 119 : 0.5311572806296635
Loss in iteration 120 : 0.5307381916799035
Loss in iteration 121 : 0.5303238808233032
Loss in iteration 122 : 0.5299142674812826
Loss in iteration 123 : 0.5295092728589188
Loss in iteration 124 : 0.5291088198959646
Loss in iteration 125 : 0.5287128332194888
Loss in iteration 126 : 0.5283212390980536
Loss in iteration 127 : 0.527933965397384
Loss in iteration 128 : 0.5275509415374823
Loss in iteration 129 : 0.527172098451108
Loss in iteration 130 : 0.5267973685435913
Loss in iteration 131 : 0.5264266856539261
Loss in iteration 132 : 0.5260599850170915
Loss in iteration 133 : 0.5256972032275579
Loss in iteration 134 : 0.5253382782039349
Loss in iteration 135 : 0.5249831491547188
Loss in iteration 136 : 0.5246317565451044
Loss in iteration 137 : 0.5242840420648092
Loss in iteration 138 : 0.5239399485968921
Loss in iteration 139 : 0.5235994201875144
Loss in iteration 140 : 0.5232624020166251
Loss in iteration 141 : 0.5229288403695165
Loss in iteration 142 : 0.5225986826092488
Loss in iteration 143 : 0.522271877149884
Loss in iteration 144 : 0.5219483734305206
Loss in iteration 145 : 0.5216281218900939
Loss in iteration 146 : 0.521311073942914
Loss in iteration 147 : 0.5209971819549226
Loss in iteration 148 : 0.5206863992206385
Loss in iteration 149 : 0.5203786799407674
Loss in iteration 150 : 0.5200739792004615
Loss in iteration 151 : 0.5197722529481987
Loss in iteration 152 : 0.5194734579752638
Loss in iteration 153 : 0.5191775518958158
Loss in iteration 154 : 0.5188844931275101
Loss in iteration 155 : 0.5185942408726766
Loss in iteration 156 : 0.5183067551000117
Loss in iteration 157 : 0.5180219965267906
Loss in iteration 158 : 0.5177399266015656
Loss in iteration 159 : 0.517460507487346
Loss in iteration 160 : 0.5171837020452316
Loss in iteration 161 : 0.5169094738185077
Loss in iteration 162 : 0.5166377870171582
Loss in iteration 163 : 0.5163686065028037
Loss in iteration 164 : 0.5161018977740476
Loss in iteration 165 : 0.5158376269522139
Loss in iteration 166 : 0.5155757607674609
Loss in iteration 167 : 0.5153162665452722
Loss in iteration 168 : 0.5150591121932999
Loss in iteration 169 : 0.5148042661885545
Loss in iteration 170 : 0.5145516975649315
Loss in iteration 171 : 0.5143013759010672
Loss in iteration 172 : 0.5140532713085038
Loss in iteration 173 : 0.5138073544201675
Loss in iteration 174 : 0.5135635963791362
Loss in iteration 175 : 0.5133219688277036
Loss in iteration 176 : 0.5130824438967165
Loss in iteration 177 : 0.5128449941951838
Loss in iteration 178 : 0.5126095928001557
Loss in iteration 179 : 0.5123762132468478
Loss in iteration 180 : 0.5121448295190225
Loss in iteration 181 : 0.5119154160396044
Loss in iteration 182 : 0.5116879476615325
Loss in iteration 183 : 0.5114623996588401
Loss in iteration 184 : 0.5112387477179556
Loss in iteration 185 : 0.511016967929212
Loss in iteration 186 : 0.5107970367785686
Loss in iteration 187 : 0.5105789311395403
Loss in iteration 188 : 0.5103626282653096
Loss in iteration 189 : 0.5101481057810419
Loss in iteration 190 : 0.5099353416763781
Loss in iteration 191 : 0.5097243142981152
Loss in iteration 192 : 0.5095150023430525
Loss in iteration 193 : 0.5093073848510169
Loss in iteration 194 : 0.509101441198049
Loss in iteration 195 : 0.5088971510897495
Loss in iteration 196 : 0.5086944945547871
Loss in iteration 197 : 0.5084934519385556
Loss in iteration 198 : 0.5082940038969759
Loss in iteration 199 : 0.5080961313904506
Loss in iteration 200 : 0.5078998156779478
Testing accuracy  of updater 3 on alg 0 with rate 0.7 = 0.775, training accuracy 0.776125, time elapsed: 4511 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.68766359382622
Loss in iteration 3 : 0.6836083819087685
Loss in iteration 4 : 0.6803413313312534
Loss in iteration 5 : 0.6775289826086944
Loss in iteration 6 : 0.6749950553226493
Loss in iteration 7 : 0.672643845848472
Loss in iteration 8 : 0.670421464114138
Loss in iteration 9 : 0.6682961735909979
Loss in iteration 10 : 0.6662483114043865
Loss in iteration 11 : 0.6642650309561525
Loss in iteration 12 : 0.6623374961452269
Loss in iteration 13 : 0.6604593399396145
Loss in iteration 14 : 0.6586257874020292
Loss in iteration 15 : 0.6568331363257648
Loss in iteration 16 : 0.655078436072102
Loss in iteration 17 : 0.6533592801582165
Loss in iteration 18 : 0.6516736667517855
Loss in iteration 19 : 0.650019901436587
Loss in iteration 20 : 0.6483965274058178
Loss in iteration 21 : 0.6468022741488795
Loss in iteration 22 : 0.6452360190312811
Loss in iteration 23 : 0.6436967581146766
Loss in iteration 24 : 0.642183583745818
Loss in iteration 25 : 0.6406956671891075
Loss in iteration 26 : 0.6392322450664774
Loss in iteration 27 : 0.6377926087002878
Loss in iteration 28 : 0.6363760956870321
Loss in iteration 29 : 0.6349820831959739
Loss in iteration 30 : 0.6336099826083884
Loss in iteration 31 : 0.6322592352032248
Loss in iteration 32 : 0.6309293086627518
Loss in iteration 33 : 0.6296196942230009
Loss in iteration 34 : 0.6283299043330149
Loss in iteration 35 : 0.6270594707169354
Loss in iteration 36 : 0.625807942756131
Loss in iteration 37 : 0.6245748861265239
Loss in iteration 38 : 0.6233598816401891
Loss in iteration 39 : 0.6221625242511576
Loss in iteration 40 : 0.6209824221938074
Loss in iteration 41 : 0.61981919622887
Loss in iteration 42 : 0.618672478977272
Loss in iteration 43 : 0.617541914326079
Loss in iteration 44 : 0.6164271568940708
Loss in iteration 45 : 0.6153278715469487
Loss in iteration 46 : 0.6142437329542094
Loss in iteration 47 : 0.6131744251812777
Loss in iteration 48 : 0.6121196413117331
Loss in iteration 49 : 0.611079083095474
Loss in iteration 50 : 0.6100524606194234
Loss in iteration 51 : 0.6090394919980148
Loss in iteration 52 : 0.6080399030812003
Loss in iteration 53 : 0.6070534271781078
Loss in iteration 54 : 0.606079804794784
Loss in iteration 55 : 0.6051187833847496
Loss in iteration 56 : 0.6041701171112361
Loss in iteration 57 : 0.6032335666202152
Loss in iteration 58 : 0.6023088988233745
Loss in iteration 59 : 0.601395886690396
Loss in iteration 60 : 0.6004943090498943
Loss in iteration 61 : 0.5996039503985172
Loss in iteration 62 : 0.598724600717704
Loss in iteration 63 : 0.5978560552977054
Loss in iteration 64 : 0.5969981145684644
Loss in iteration 65 : 0.5961505839370266
Loss in iteration 66 : 0.5953132736311354
Loss in iteration 67 : 0.5944859985487544
Loss in iteration 68 : 0.5936685781132012
Loss in iteration 69 : 0.5928608361336792
Loss in iteration 70 : 0.5920626006709374
Loss in iteration 71 : 0.591273703907859
Loss in iteration 72 : 0.5904939820247447
Loss in iteration 73 : 0.5897232750791129
Loss in iteration 74 : 0.5889614268898181
Loss in iteration 75 : 0.588208284925306
Loss in iteration 76 : 0.5874637001958406
Loss in iteration 77 : 0.5867275271495432
Loss in iteration 78 : 0.5859996235720805
Loss in iteration 79 : 0.5852798504898629
Loss in iteration 80 : 0.5845680720766113
Loss in iteration 81 : 0.5838641555631557
Loss in iteration 82 : 0.5831679711503363
Loss in iteration 83 : 0.582479391924893
Loss in iteration 84 : 0.5817982937782127
Loss in iteration 85 : 0.581124555327842
Loss in iteration 86 : 0.5804580578416303
Loss in iteration 87 : 0.5797986851644337
Loss in iteration 88 : 0.5791463236472552
Loss in iteration 89 : 0.5785008620787317
Loss in iteration 90 : 0.5778621916189022
Loss in iteration 91 : 0.5772302057351236
Loss in iteration 92 : 0.5766048001401081
Loss in iteration 93 : 0.5759858727319533
Loss in iteration 94 : 0.5753733235361189
Loss in iteration 95 : 0.5747670546492712
Loss in iteration 96 : 0.5741669701849166
Loss in iteration 97 : 0.5735729762207704
Loss in iteration 98 : 0.5729849807477833
Loss in iteration 99 : 0.5724028936207828
Loss in iteration 100 : 0.5718266265106476
Loss in iteration 101 : 0.5712560928579794
Loss in iteration 102 : 0.5706912078282027
Loss in iteration 103 : 0.5701318882680529
Loss in iteration 104 : 0.5695780526633928
Loss in iteration 105 : 0.5690296210983151
Loss in iteration 106 : 0.5684865152154884
Loss in iteration 107 : 0.5679486581776898
Loss in iteration 108 : 0.5674159746305013
Loss in iteration 109 : 0.566888390666109
Loss in iteration 110 : 0.5663658337881806
Loss in iteration 111 : 0.5658482328777814
Loss in iteration 112 : 0.5653355181602817
Loss in iteration 113 : 0.5648276211732441
Loss in iteration 114 : 0.5643244747352281
Loss in iteration 115 : 0.5638260129155069
Loss in iteration 116 : 0.5633321710046507
Loss in iteration 117 : 0.5628428854859483
Loss in iteration 118 : 0.5623580940076458
Loss in iteration 119 : 0.5618777353559672
Loss in iteration 120 : 0.5614017494288961
Loss in iteration 121 : 0.5609300772106905
Loss in iteration 122 : 0.5604626607471076
Loss in iteration 123 : 0.5599994431213161
Loss in iteration 124 : 0.5595403684304724
Loss in iteration 125 : 0.5590853817629369
Loss in iteration 126 : 0.5586344291761156
Loss in iteration 127 : 0.5581874576749026
Loss in iteration 128 : 0.5577444151907005
Loss in iteration 129 : 0.5573052505610153
Loss in iteration 130 : 0.5568699135095837
Loss in iteration 131 : 0.5564383546270435
Loss in iteration 132 : 0.5560105253521009
Loss in iteration 133 : 0.55558637795321
Loss in iteration 134 : 0.5551658655107207
Loss in iteration 135 : 0.5547489418995004
Loss in iteration 136 : 0.5543355617719982
Loss in iteration 137 : 0.5539256805417603
Loss in iteration 138 : 0.5535192543673559
Loss in iteration 139 : 0.5531162401367259
Loss in iteration 140 : 0.5527165954519212
Loss in iteration 141 : 0.5523202786142372
Loss in iteration 142 : 0.5519272486097141
Loss in iteration 143 : 0.5515374650950107
Loss in iteration 144 : 0.551150888383626
Loss in iteration 145 : 0.5507674794324632
Loss in iteration 146 : 0.5503871998287363
Loss in iteration 147 : 0.5500100117771815
Loss in iteration 148 : 0.5496358780876057
Loss in iteration 149 : 0.549264762162716
Loss in iteration 150 : 0.5488966279862668
Loss in iteration 151 : 0.5485314401114796
Loss in iteration 152 : 0.5481691636497449
Loss in iteration 153 : 0.5478097642596025
Loss in iteration 154 : 0.5474532081359763
Loss in iteration 155 : 0.5470994619996711
Loss in iteration 156 : 0.5467484930871173
Loss in iteration 157 : 0.5464002691403548
Loss in iteration 158 : 0.546054758397257
Loss in iteration 159 : 0.5457119295819791
Loss in iteration 160 : 0.5453717518956324
Loss in iteration 161 : 0.5450341950071707
Loss in iteration 162 : 0.5446992290444955
Loss in iteration 163 : 0.5443668245857513
Loss in iteration 164 : 0.5440369526508395
Loss in iteration 165 : 0.5437095846931085
Loss in iteration 166 : 0.5433846925912426
Loss in iteration 167 : 0.543062248641331
Loss in iteration 168 : 0.5427422255491147
Loss in iteration 169 : 0.542424596422411
Loss in iteration 170 : 0.5421093347637049
Loss in iteration 171 : 0.541796414462902
Loss in iteration 172 : 0.5414858097902508
Loss in iteration 173 : 0.5411774953894101
Loss in iteration 174 : 0.5408714462706795
Loss in iteration 175 : 0.5405676378043718
Loss in iteration 176 : 0.5402660457143291
Loss in iteration 177 : 0.539966646071586
Loss in iteration 178 : 0.5396694152881657
Loss in iteration 179 : 0.5393743301110057
Loss in iteration 180 : 0.5390813676160248
Loss in iteration 181 : 0.5387905052023068
Loss in iteration 182 : 0.5385017205864127
Loss in iteration 183 : 0.5382149917968142
Loss in iteration 184 : 0.5379302971684435
Loss in iteration 185 : 0.5376476153373568
Loss in iteration 186 : 0.537366925235513
Loss in iteration 187 : 0.5370882060856589
Loss in iteration 188 : 0.536811437396321
Loss in iteration 189 : 0.5365365989569035
Loss in iteration 190 : 0.5362636708328851
Loss in iteration 191 : 0.5359926333611157
Loss in iteration 192 : 0.5357234671452115
Loss in iteration 193 : 0.5354561530510411
Loss in iteration 194 : 0.5351906722023025
Loss in iteration 195 : 0.5349270059761991
Loss in iteration 196 : 0.534665135999187
Loss in iteration 197 : 0.5344050441428262
Loss in iteration 198 : 0.5341467125197016
Loss in iteration 199 : 0.5338901234794295
Loss in iteration 200 : 0.5336352596047496
Testing accuracy  of updater 3 on alg 0 with rate 0.4 = 0.772, training accuracy 0.77275, time elapsed: 4707 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6916994458088274
Loss in iteration 3 : 0.6903616700987405
Loss in iteration 4 : 0.6891198942606511
Loss in iteration 5 : 0.6879620161924759
Loss in iteration 6 : 0.6868775484260013
Loss in iteration 7 : 0.6858574049233641
Loss in iteration 8 : 0.684893714282155
Loss in iteration 9 : 0.6839796566097882
Loss in iteration 10 : 0.6831093214659871
Loss in iteration 11 : 0.6822775844468288
Loss in iteration 12 : 0.6814800001790834
Loss in iteration 13 : 0.6807127096974459
Loss in iteration 14 : 0.6799723603807242
Loss in iteration 15 : 0.6792560368197317
Loss in iteration 16 : 0.6785612011752333
Loss in iteration 17 : 0.6778856417561755
Loss in iteration 18 : 0.6772274287053951
Loss in iteration 19 : 0.6765848758214863
Loss in iteration 20 : 0.6759565076719297
Loss in iteration 21 : 0.6753410312646504
Loss in iteration 22 : 0.67473731164385
Loss in iteration 23 : 0.6741443508623519
Loss in iteration 24 : 0.6735612698581165
Loss in iteration 25 : 0.6729872928281033
Loss in iteration 26 : 0.6724217337493706
Loss in iteration 27 : 0.6718639847464489
Loss in iteration 28 : 0.6713135060463031
Loss in iteration 29 : 0.6707698172987131
Loss in iteration 30 : 0.6702324900712643
Loss in iteration 31 : 0.669701141355122
Loss in iteration 32 : 0.6691754279409264
Loss in iteration 33 : 0.6686550415440439
Loss in iteration 34 : 0.6681397045754519
Loss in iteration 35 : 0.6676291664692067
Loss in iteration 36 : 0.6671232004899612
Loss in iteration 37 : 0.6666216009547956
Loss in iteration 38 : 0.6661241808128465
Loss in iteration 39 : 0.6656307695341307
Loss in iteration 40 : 0.6651412112657676
Loss in iteration 41 : 0.6646553632196387
Loss in iteration 42 : 0.6641730942604795
Loss in iteration 43 : 0.6636942836677616
Loss in iteration 44 : 0.6632188200483504
Loss in iteration 45 : 0.6627466003801035
Loss in iteration 46 : 0.6622775291693269
Loss in iteration 47 : 0.6618115177072921
Loss in iteration 48 : 0.6613484834130697
Loss in iteration 49 : 0.6608883492516393
Loss in iteration 50 : 0.6604310432177473
Loss in iteration 51 : 0.6599764978772172
Loss in iteration 52 : 0.6595246499585974
Loss in iteration 53 : 0.6590754399889025
Loss in iteration 54 : 0.6586288119680805
Loss in iteration 55 : 0.6581847130775147
Loss in iteration 56 : 0.657743093418499
Loss in iteration 57 : 0.657303905777142
Loss in iteration 58 : 0.6568671054126088
Loss in iteration 59 : 0.6564326498660185
Loss in iteration 60 : 0.6560004987876364
Loss in iteration 61 : 0.6555706137803181
Loss in iteration 62 : 0.6551429582573952
Loss in iteration 63 : 0.6547174973134385
Loss in iteration 64 : 0.6542941976064984
Loss in iteration 65 : 0.6538730272506321
Loss in iteration 66 : 0.6534539557176205
Loss in iteration 67 : 0.6530369537469496
Loss in iteration 68 : 0.6526219932632246
Loss in iteration 69 : 0.6522090473002633
Loss in iteration 70 : 0.6517980899312428
Loss in iteration 71 : 0.651389096204295
Loss in iteration 72 : 0.6509820420830615
Loss in iteration 73 : 0.6505769043917369
Loss in iteration 74 : 0.6501736607641863
Loss in iteration 75 : 0.6497722895967966
Loss in iteration 76 : 0.6493727700047179
Loss in iteration 77 : 0.6489750817811957
Loss in iteration 78 : 0.6485792053597621
Loss in iteration 79 : 0.6481851217790104
Loss in iteration 80 : 0.6477928126497803
Loss in iteration 81 : 0.6474022601245256
Loss in iteration 82 : 0.6470134468687195
Loss in iteration 83 : 0.646626356034127
Loss in iteration 84 : 0.6462409712337893
Loss in iteration 85 : 0.6458572765186225
Loss in iteration 86 : 0.6454752563554816
Loss in iteration 87 : 0.6450948956065966
Loss in iteration 88 : 0.6447161795102749
Loss in iteration 89 : 0.6443390936627968
Loss in iteration 90 : 0.643963624001386
Loss in iteration 91 : 0.643589756788226
Loss in iteration 92 : 0.6432174785954102
Loss in iteration 93 : 0.642846776290782
Loss in iteration 94 : 0.6424776370246102
Loss in iteration 95 : 0.642110048217028
Loss in iteration 96 : 0.6417439975461999
Loss in iteration 97 : 0.6413794729371577
Loss in iteration 98 : 0.6410164625512836
Loss in iteration 99 : 0.6406549547763543
Loss in iteration 100 : 0.640294938217179
Loss in iteration 101 : 0.6399364016867225
Loss in iteration 102 : 0.6395793341977394
Loss in iteration 103 : 0.6392237249548486
Loss in iteration 104 : 0.6388695633470484
Loss in iteration 105 : 0.6385168389406304
Loss in iteration 106 : 0.6381655414724743
Loss in iteration 107 : 0.6378156608436882
Loss in iteration 108 : 0.6374671871136088
Loss in iteration 109 : 0.6371201104940847
Loss in iteration 110 : 0.6367744213440802
Loss in iteration 111 : 0.6364301101645496
Loss in iteration 112 : 0.6360871675935725
Loss in iteration 113 : 0.6357455844017396
Loss in iteration 114 : 0.6354053514877721
Loss in iteration 115 : 0.6350664598743575
Loss in iteration 116 : 0.6347289007041972
Loss in iteration 117 : 0.634392665236247
Loss in iteration 118 : 0.6340577448421431
Loss in iteration 119 : 0.6337241310027977
Loss in iteration 120 : 0.6333918153051696
Loss in iteration 121 : 0.6330607894391773
Loss in iteration 122 : 0.6327310451947642
Loss in iteration 123 : 0.632402574459104
Loss in iteration 124 : 0.6320753692139391
Loss in iteration 125 : 0.6317494215330299
Loss in iteration 126 : 0.6314247235797297
Loss in iteration 127 : 0.6311012676046778
Loss in iteration 128 : 0.6307790459435801
Loss in iteration 129 : 0.6304580510150996
Loss in iteration 130 : 0.6301382753188385
Loss in iteration 131 : 0.629819711433405
Loss in iteration 132 : 0.6295023520145681
Loss in iteration 133 : 0.6291861897934875
Loss in iteration 134 : 0.6288712175750212
Loss in iteration 135 : 0.6285574282361082
Loss in iteration 136 : 0.6282448147242068
Loss in iteration 137 : 0.6279333700558043
Loss in iteration 138 : 0.62762308731499
Loss in iteration 139 : 0.6273139596520809
Loss in iteration 140 : 0.6270059802823009
Loss in iteration 141 : 0.6266991424845119
Loss in iteration 142 : 0.626393439599998
Loss in iteration 143 : 0.6260888650312911
Loss in iteration 144 : 0.6257854122410461
Loss in iteration 145 : 0.6254830747509444
Loss in iteration 146 : 0.6251818461406583
Loss in iteration 147 : 0.6248817200468343
Loss in iteration 148 : 0.6245826901621222
Loss in iteration 149 : 0.6242847502342368
Loss in iteration 150 : 0.6239878940650442
Loss in iteration 151 : 0.6236921155096958
Loss in iteration 152 : 0.6233974084757724
Loss in iteration 153 : 0.6231037669224654
Loss in iteration 154 : 0.6228111848597895
Loss in iteration 155 : 0.6225196563478116
Loss in iteration 156 : 0.6222291754959066
Loss in iteration 157 : 0.6219397364620379
Loss in iteration 158 : 0.6216513334520567
Loss in iteration 159 : 0.6213639607190289
Loss in iteration 160 : 0.6210776125625694
Loss in iteration 161 : 0.6207922833282122
Loss in iteration 162 : 0.6205079674067805
Loss in iteration 163 : 0.6202246592337864
Loss in iteration 164 : 0.6199423532888482
Loss in iteration 165 : 0.619661044095106
Loss in iteration 166 : 0.6193807262186815
Loss in iteration 167 : 0.6191013942681232
Loss in iteration 168 : 0.6188230428938842
Loss in iteration 169 : 0.6185456667878052
Loss in iteration 170 : 0.6182692606826112
Loss in iteration 171 : 0.6179938193514221
Loss in iteration 172 : 0.617719337607275
Loss in iteration 173 : 0.6174458103026533
Loss in iteration 174 : 0.6171732323290311
Loss in iteration 175 : 0.6169015986164286
Loss in iteration 176 : 0.6166309041329728
Loss in iteration 177 : 0.6163611438844666
Loss in iteration 178 : 0.6160923129139788
Loss in iteration 179 : 0.6158244063014229
Loss in iteration 180 : 0.6155574191631683
Loss in iteration 181 : 0.6152913466516353
Loss in iteration 182 : 0.6150261839549167
Loss in iteration 183 : 0.6147619262963974
Loss in iteration 184 : 0.6144985689343833
Loss in iteration 185 : 0.6142361071617364
Loss in iteration 186 : 0.6139745363055169
Loss in iteration 187 : 0.6137138517266398
Loss in iteration 188 : 0.6134540488195178
Loss in iteration 189 : 0.6131951230117316
Loss in iteration 190 : 0.612937069763695
Loss in iteration 191 : 0.612679884568327
Loss in iteration 192 : 0.6124235629507303
Loss in iteration 193 : 0.6121681004678755
Loss in iteration 194 : 0.6119134927082885
Loss in iteration 195 : 0.6116597352917487
Loss in iteration 196 : 0.6114068238689783
Loss in iteration 197 : 0.611154754121354
Loss in iteration 198 : 0.6109035217606125
Loss in iteration 199 : 0.6106531225285614
Loss in iteration 200 : 0.6104035521967928
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.745, training accuracy 0.74825, time elapsed: 4328 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666216
Loss in iteration 3 : 0.6926960665968459
Loss in iteration 4 : 0.692466673002798
Loss in iteration 5 : 0.692236337267449
Loss in iteration 6 : 0.6920055789324585
Loss in iteration 7 : 0.69177471205339
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107216
Loss in iteration 10 : 0.6910831675971449
Loss in iteration 11 : 0.6908533306884844
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.6903950003411579
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783552
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982641
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633648
Loss in iteration 30 : 0.6865873006395931
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139226
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106488
Loss in iteration 39 : 0.6846368290419
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928563
Loss in iteration 42 : 0.6839960858461508
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844098
Loss in iteration 46 : 0.6831486634501976
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913213
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994245
Loss in iteration 53 : 0.6816835923783227
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644757
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502752
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084534
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262016
Loss in iteration 68 : 0.6786124486965575
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.6778072320587119
Loss in iteration 73 : 0.6776067289840476
Loss in iteration 74 : 0.6774065329288738
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.677007033672807
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322007
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261117
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255632
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127074
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086908
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.6744350599463959
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782224
Loss in iteration 95 : 0.6732588424421752
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926724
Loss in iteration 100 : 0.6722821113368843
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027845
Loss in iteration 105 : 0.6713077749039102
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007186
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679126
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711536
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.6695584508918571
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363291998
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546626
Loss in iteration 120 : 0.6683946264884942
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396387
Loss in iteration 123 : 0.66781328132023
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151227
Loss in iteration 126 : 0.6672322691482263
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.6668451004109129
Loss in iteration 129 : 0.6666515656469992
Loss in iteration 130 : 0.6664580631474694
Loss in iteration 131 : 0.6662645923961656
Loss in iteration 132 : 0.666071152943642
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254213
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832375
Loss in iteration 141 : 0.6643315616627489
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155865
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637078
Loss in iteration 146 : 0.6633661713503329
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502826
Loss in iteration 149 : 0.6627873056785705
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.661823171435479
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915361
Loss in iteration 158 : 0.6610524728561491
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913315
Loss in iteration 162 : 0.6602823512919482
Loss in iteration 163 : 0.6600899153939861
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703856
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627238
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519347
Loss in iteration 176 : 0.6575920414239279
Loss in iteration 177 : 0.6574002131781415
Loss in iteration 178 : 0.6572084336481163
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894333
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910627
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452745
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843811
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725102
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438127
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217606
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.6539563608050971
Loss in iteration 196 : 0.6537655867552801
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726638
Loss in iteration 199 : 0.6531936461525889
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 100.0 = 0.666, training accuracy 0.6705, time elapsed: 4272 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666216
Loss in iteration 3 : 0.6926960665968458
Loss in iteration 4 : 0.692466673002798
Loss in iteration 5 : 0.692236337267449
Loss in iteration 6 : 0.6920055789324584
Loss in iteration 7 : 0.6917747120533901
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107216
Loss in iteration 10 : 0.6910831675971449
Loss in iteration 11 : 0.6908533306884844
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.6903950003411579
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783552
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982641
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.686587300639593
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139227
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106488
Loss in iteration 39 : 0.6846368290419002
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928564
Loss in iteration 42 : 0.6839960858461507
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501976
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913213
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994245
Loss in iteration 53 : 0.6816835923783227
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644756
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.680650124117142
Loss in iteration 59 : 0.6804446544502751
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084534
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262016
Loss in iteration 68 : 0.6786124486965573
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.677807232058712
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288739
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.6770070336728071
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322006
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261117
Loss in iteration 82 : 0.6758151266569102
Loss in iteration 83 : 0.6756173483255633
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127075
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086907
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.674435059946396
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782224
Loss in iteration 95 : 0.6732588424421753
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926724
Loss in iteration 100 : 0.6722821113368843
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027843
Loss in iteration 105 : 0.6713077749039102
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007186
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679124
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711537
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.669558450891857
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363291998
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546625
Loss in iteration 120 : 0.6683946264884943
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396387
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151227
Loss in iteration 126 : 0.6672322691482262
Loss in iteration 127 : 0.6670386680250864
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.666651565646999
Loss in iteration 130 : 0.6664580631474695
Loss in iteration 131 : 0.6662645923961655
Loss in iteration 132 : 0.666071152943642
Loss in iteration 133 : 0.6658777444042953
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254212
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832375
Loss in iteration 141 : 0.6643315616627489
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155866
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637078
Loss in iteration 146 : 0.6633661713503328
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502825
Loss in iteration 149 : 0.6627873056785704
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.661823171435479
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915361
Loss in iteration 158 : 0.6610524728561491
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913316
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.660089915393986
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703855
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627239
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519347
Loss in iteration 176 : 0.6575920414239279
Loss in iteration 177 : 0.6574002131781415
Loss in iteration 178 : 0.6572084336481163
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894335
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910627
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452746
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843811
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725102
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438127
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217606
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.653956360805097
Loss in iteration 196 : 0.65376558675528
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726638
Loss in iteration 199 : 0.6531936461525889
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 70.0 = 0.666, training accuracy 0.6705, time elapsed: 4227 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666217
Loss in iteration 3 : 0.6926960665968458
Loss in iteration 4 : 0.692466673002798
Loss in iteration 5 : 0.692236337267449
Loss in iteration 6 : 0.6920055789324585
Loss in iteration 7 : 0.6917747120533901
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107216
Loss in iteration 10 : 0.691083167597145
Loss in iteration 11 : 0.6908533306884844
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.6903950003411579
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783552
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982642
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.686587300639593
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139227
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106488
Loss in iteration 39 : 0.6846368290419
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928563
Loss in iteration 42 : 0.6839960858461507
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501976
Loss in iteration 47 : 0.6829380010823383
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913213
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994245
Loss in iteration 53 : 0.6816835923783227
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644756
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502751
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084533
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262015
Loss in iteration 68 : 0.6786124486965575
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.677807232058712
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288738
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.6770070336728071
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322007
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261117
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255633
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127074
Loss in iteration 86 : 0.6750253236822997
Loss in iteration 87 : 0.6748283863086907
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.674435059946396
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782223
Loss in iteration 95 : 0.6732588424421753
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926725
Loss in iteration 100 : 0.6722821113368844
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027843
Loss in iteration 105 : 0.6713077749039102
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007186
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679126
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711535
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.6695584508918571
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363291998
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546625
Loss in iteration 120 : 0.6683946264884942
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396386
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151226
Loss in iteration 126 : 0.6672322691482262
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.6666515656469991
Loss in iteration 130 : 0.6664580631474694
Loss in iteration 131 : 0.6662645923961655
Loss in iteration 132 : 0.666071152943642
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254212
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832375
Loss in iteration 141 : 0.6643315616627489
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155866
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637079
Loss in iteration 146 : 0.6633661713503329
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502826
Loss in iteration 149 : 0.6627873056785704
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.661823171435479
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915361
Loss in iteration 158 : 0.661052472856149
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913315
Loss in iteration 162 : 0.6602823512919482
Loss in iteration 163 : 0.6600899153939861
Loss in iteration 164 : 0.6598975185214339
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703855
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627238
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519346
Loss in iteration 176 : 0.657592041423928
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481164
Loss in iteration 179 : 0.657016703571449
Loss in iteration 180 : 0.6568250236894335
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910626
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452745
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843811
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725103
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438126
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217604
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.6539563608050971
Loss in iteration 196 : 0.65376558675528
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726638
Loss in iteration 199 : 0.6531936461525888
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 40.0 = 0.666, training accuracy 0.6705, time elapsed: 4519 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666216
Loss in iteration 3 : 0.6926960665968458
Loss in iteration 4 : 0.692466673002798
Loss in iteration 5 : 0.6922363372674492
Loss in iteration 6 : 0.6920055789324585
Loss in iteration 7 : 0.6917747120533901
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107215
Loss in iteration 10 : 0.6910831675971449
Loss in iteration 11 : 0.6908533306884842
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.690395000341158
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783552
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982642
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.686587300639593
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139227
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106488
Loss in iteration 39 : 0.6846368290419
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928563
Loss in iteration 42 : 0.6839960858461507
Loss in iteration 43 : 0.6837835024759528
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501978
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913213
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994244
Loss in iteration 53 : 0.6816835923783224
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644757
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502752
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084534
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.6790170699411942
Loss in iteration 67 : 0.6788145863262015
Loss in iteration 68 : 0.6786124486965575
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.677807232058712
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288739
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.6770070336728071
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322007
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255632
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127075
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086907
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.674435059946396
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782223
Loss in iteration 95 : 0.6732588424421752
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926724
Loss in iteration 100 : 0.6722821113368844
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027842
Loss in iteration 105 : 0.6713077749039104
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007187
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679124
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711536
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.6695584508918571
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363291998
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546625
Loss in iteration 120 : 0.6683946264884943
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396386
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151227
Loss in iteration 126 : 0.6672322691482262
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.666651565646999
Loss in iteration 130 : 0.6664580631474694
Loss in iteration 131 : 0.6662645923961655
Loss in iteration 132 : 0.666071152943642
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254213
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832375
Loss in iteration 141 : 0.6643315616627488
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155866
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637078
Loss in iteration 146 : 0.6633661713503329
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502826
Loss in iteration 149 : 0.6627873056785705
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.661823171435479
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915361
Loss in iteration 158 : 0.661052472856149
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913315
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939861
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703856
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789904
Loss in iteration 172 : 0.6583598269627238
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519347
Loss in iteration 176 : 0.6575920414239279
Loss in iteration 177 : 0.6574002131781415
Loss in iteration 178 : 0.6572084336481163
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894333
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910626
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452746
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843811
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725102
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438126
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217604
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.653956360805097
Loss in iteration 196 : 0.65376558675528
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726638
Loss in iteration 199 : 0.6531936461525889
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.666, training accuracy 0.6705, time elapsed: 4767 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666217
Loss in iteration 3 : 0.6926960665968458
Loss in iteration 4 : 0.6924666730027979
Loss in iteration 5 : 0.6922363372674492
Loss in iteration 6 : 0.6920055789324585
Loss in iteration 7 : 0.69177471205339
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107215
Loss in iteration 10 : 0.6910831675971449
Loss in iteration 11 : 0.6908533306884842
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.690395000341158
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783552
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982642
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.686587300639593
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139226
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106488
Loss in iteration 39 : 0.6846368290419
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928564
Loss in iteration 42 : 0.6839960858461507
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501978
Loss in iteration 47 : 0.6829380010823384
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557313
Loss in iteration 50 : 0.6823087826913213
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994244
Loss in iteration 53 : 0.6816835923783224
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644756
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502751
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084533
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262015
Loss in iteration 68 : 0.6786124486965575
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.6778072320587119
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288739
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.677007033672807
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322006
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255633
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127076
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086908
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.674435059946396
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782223
Loss in iteration 95 : 0.6732588424421753
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926725
Loss in iteration 100 : 0.6722821113368843
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027842
Loss in iteration 105 : 0.6713077749039102
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007186
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679126
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711536
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.6695584508918571
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363292
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546625
Loss in iteration 120 : 0.6683946264884942
Loss in iteration 121 : 0.6682008059727268
Loss in iteration 122 : 0.6680070246396386
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151227
Loss in iteration 126 : 0.6672322691482262
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.6666515656469991
Loss in iteration 130 : 0.6664580631474695
Loss in iteration 131 : 0.6662645923961655
Loss in iteration 132 : 0.6660711529436419
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254213
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832375
Loss in iteration 141 : 0.6643315616627489
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155865
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637078
Loss in iteration 146 : 0.6633661713503329
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502825
Loss in iteration 149 : 0.6627873056785705
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.6618231714354789
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915361
Loss in iteration 158 : 0.6610524728561491
Loss in iteration 159 : 0.6608598869635476
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913315
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.660089915393986
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079474
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703855
Loss in iteration 169 : 0.658936142179521
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627238
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519346
Loss in iteration 176 : 0.6575920414239279
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481162
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894335
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910626
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452745
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843811
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725103
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438127
Loss in iteration 192 : 0.6545290538638502
Loss in iteration 193 : 0.6543380951217604
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.6539563608050971
Loss in iteration 196 : 0.65376558675528
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726637
Loss in iteration 199 : 0.6531936461525888
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 7.0 = 0.666, training accuracy 0.6705, time elapsed: 4618 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666217
Loss in iteration 3 : 0.6926960665968459
Loss in iteration 4 : 0.692466673002798
Loss in iteration 5 : 0.692236337267449
Loss in iteration 6 : 0.6920055789324584
Loss in iteration 7 : 0.69177471205339
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107215
Loss in iteration 10 : 0.6910831675971449
Loss in iteration 11 : 0.6908533306884844
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.690395000341158
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783553
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982642
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307213
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.686587300639593
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152162
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139227
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106488
Loss in iteration 39 : 0.6846368290419003
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928564
Loss in iteration 42 : 0.6839960858461507
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501976
Loss in iteration 47 : 0.6829380010823383
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913212
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994244
Loss in iteration 53 : 0.6816835923783224
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644756
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502752
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084533
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262016
Loss in iteration 68 : 0.6786124486965575
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.677807232058712
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288738
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.677007033672807
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322007
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261116
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255633
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127075
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086907
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.674435059946396
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782223
Loss in iteration 95 : 0.6732588424421753
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926725
Loss in iteration 100 : 0.6722821113368844
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027845
Loss in iteration 105 : 0.6713077749039102
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007186
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679126
Loss in iteration 111 : 0.670141019770401
Loss in iteration 112 : 0.6699467761711535
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.669558450891857
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363291998
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546626
Loss in iteration 120 : 0.6683946264884942
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396386
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400383
Loss in iteration 125 : 0.6674259045151227
Loss in iteration 126 : 0.6672322691482263
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.666651565646999
Loss in iteration 130 : 0.6664580631474694
Loss in iteration 131 : 0.6662645923961654
Loss in iteration 132 : 0.666071152943642
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254212
Loss in iteration 136 : 0.6652977013095168
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832376
Loss in iteration 141 : 0.6643315616627489
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155866
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637078
Loss in iteration 146 : 0.6633661713503329
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502825
Loss in iteration 149 : 0.6627873056785705
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.661823171435479
Loss in iteration 155 : 0.6616304446051507
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561491
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913316
Loss in iteration 162 : 0.6602823512919482
Loss in iteration 163 : 0.6600899153939861
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703855
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627239
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519347
Loss in iteration 176 : 0.6575920414239279
Loss in iteration 177 : 0.6574002131781413
Loss in iteration 178 : 0.6572084336481163
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894333
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910627
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452746
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843811
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725103
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438126
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217604
Loss in iteration 194 : 0.6541471971811891
Loss in iteration 195 : 0.6539563608050971
Loss in iteration 196 : 0.65376558675528
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726638
Loss in iteration 199 : 0.6531936461525888
Loss in iteration 200 : 0.653003128982834
Testing accuracy  of updater 4 on alg 0 with rate 4.0 = 0.666, training accuracy 0.6705, time elapsed: 4628 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6929235752666216
Loss in iteration 3 : 0.6926960665968458
Loss in iteration 4 : 0.6924666730027979
Loss in iteration 5 : 0.692236337267449
Loss in iteration 6 : 0.6920055789324584
Loss in iteration 7 : 0.69177471205339
Loss in iteration 8 : 0.6915439381601188
Loss in iteration 9 : 0.6913133922107215
Loss in iteration 10 : 0.6910831675971449
Loss in iteration 11 : 0.6908533306884843
Loss in iteration 12 : 0.6906239297064454
Loss in iteration 13 : 0.6903950003411579
Loss in iteration 14 : 0.6901665694061496
Loss in iteration 15 : 0.6899386572726083
Loss in iteration 16 : 0.6897112795231967
Loss in iteration 17 : 0.6894844480963954
Loss in iteration 18 : 0.6892581720925248
Loss in iteration 19 : 0.6890324583520112
Loss in iteration 20 : 0.6888073118783553
Loss in iteration 21 : 0.6885827361540363
Loss in iteration 22 : 0.6883587333817472
Loss in iteration 23 : 0.6881353046728775
Loss in iteration 24 : 0.6879124501982641
Loss in iteration 25 : 0.6876901693114452
Loss in iteration 26 : 0.6874684606515441
Loss in iteration 27 : 0.6872473222307214
Loss in iteration 28 : 0.6870267515096307
Loss in iteration 29 : 0.6868067454633647
Loss in iteration 30 : 0.6865873006395931
Loss in iteration 31 : 0.6863684132102226
Loss in iteration 32 : 0.6861500790174879
Loss in iteration 33 : 0.6859322936152161
Loss in iteration 34 : 0.6857150523058196
Loss in iteration 35 : 0.6854983501734838
Loss in iteration 36 : 0.6852821821139226
Loss in iteration 37 : 0.6850665428610211
Loss in iteration 38 : 0.6848514270106488
Loss in iteration 39 : 0.6846368290419
Loss in iteration 40 : 0.6844227433359665
Loss in iteration 41 : 0.6842091641928564
Loss in iteration 42 : 0.6839960858461507
Loss in iteration 43 : 0.6837835024759527
Loss in iteration 44 : 0.6835714082201896
Loss in iteration 45 : 0.6833597971844099
Loss in iteration 46 : 0.6831486634501976
Loss in iteration 47 : 0.6829380010823383
Loss in iteration 48 : 0.6827278041348106
Loss in iteration 49 : 0.6825180666557314
Loss in iteration 50 : 0.6823087826913213
Loss in iteration 51 : 0.6820999462889611
Loss in iteration 52 : 0.6818915514994244
Loss in iteration 53 : 0.6816835923783224
Loss in iteration 54 : 0.681476062986818
Loss in iteration 55 : 0.681268957391646
Loss in iteration 56 : 0.6810622696644756
Loss in iteration 57 : 0.6808559938806242
Loss in iteration 58 : 0.6806501241171421
Loss in iteration 59 : 0.6804446544502752
Loss in iteration 60 : 0.6802395789522903
Loss in iteration 61 : 0.6800348916876392
Loss in iteration 62 : 0.6798305867084533
Loss in iteration 63 : 0.6796266580492966
Loss in iteration 64 : 0.6794230997211296
Loss in iteration 65 : 0.6792199057044054
Loss in iteration 66 : 0.679017069941194
Loss in iteration 67 : 0.6788145863262015
Loss in iteration 68 : 0.6786124486965575
Loss in iteration 69 : 0.6784106508201545
Loss in iteration 70 : 0.6782091863823767
Loss in iteration 71 : 0.6780080489709516
Loss in iteration 72 : 0.677807232058712
Loss in iteration 73 : 0.6776067289840477
Loss in iteration 74 : 0.6774065329288739
Loss in iteration 75 : 0.6772066368941017
Loss in iteration 76 : 0.677007033672807
Loss in iteration 77 : 0.6768077158217204
Loss in iteration 78 : 0.6766086756322006
Loss in iteration 79 : 0.6764099051027634
Loss in iteration 80 : 0.6762113959162673
Loss in iteration 81 : 0.6760131394261117
Loss in iteration 82 : 0.6758151266569101
Loss in iteration 83 : 0.6756173483255633
Loss in iteration 84 : 0.6754197948878221
Loss in iteration 85 : 0.6752224566127075
Loss in iteration 86 : 0.6750253236822996
Loss in iteration 87 : 0.6748283863086908
Loss in iteration 88 : 0.6746316348550656
Loss in iteration 89 : 0.6744350599463959
Loss in iteration 90 : 0.6742386525578968
Loss in iteration 91 : 0.6740424040750551
Loss in iteration 92 : 0.6738463063253899
Loss in iteration 93 : 0.6736503515866578
Loss in iteration 94 : 0.6734545325782223
Loss in iteration 95 : 0.6732588424421753
Loss in iteration 96 : 0.6730632747194031
Loss in iteration 97 : 0.6728678233241574
Loss in iteration 98 : 0.6726724825192106
Loss in iteration 99 : 0.6724772468926725
Loss in iteration 100 : 0.6722821113368843
Loss in iteration 101 : 0.6720870710293817
Loss in iteration 102 : 0.6718921214158105
Loss in iteration 103 : 0.671697258194545
Loss in iteration 104 : 0.6715024773027842
Loss in iteration 105 : 0.6713077749039102
Loss in iteration 106 : 0.6711131473759093
Loss in iteration 107 : 0.6709185913007186
Loss in iteration 108 : 0.6707241034543497
Loss in iteration 109 : 0.6705296807976902
Loss in iteration 110 : 0.6703353204679126
Loss in iteration 111 : 0.6701410197704011
Loss in iteration 112 : 0.6699467761711536
Loss in iteration 113 : 0.6697525872896092
Loss in iteration 114 : 0.669558450891857
Loss in iteration 115 : 0.6693643648842087
Loss in iteration 116 : 0.6691703273070829
Loss in iteration 117 : 0.6689763363292
Loss in iteration 118 : 0.6687823902420595
Loss in iteration 119 : 0.6685884874546625
Loss in iteration 120 : 0.6683946264884943
Loss in iteration 121 : 0.6682008059727267
Loss in iteration 122 : 0.6680070246396386
Loss in iteration 123 : 0.6678132813202301
Loss in iteration 124 : 0.6676195749400384
Loss in iteration 125 : 0.6674259045151227
Loss in iteration 126 : 0.6672322691482263
Loss in iteration 127 : 0.6670386680250863
Loss in iteration 128 : 0.666845100410913
Loss in iteration 129 : 0.6666515656469991
Loss in iteration 130 : 0.6664580631474694
Loss in iteration 131 : 0.6662645923961655
Loss in iteration 132 : 0.666071152943642
Loss in iteration 133 : 0.6658777444042951
Loss in iteration 134 : 0.6656843664535942
Loss in iteration 135 : 0.6654910188254213
Loss in iteration 136 : 0.6652977013095167
Loss in iteration 137 : 0.6651044137490169
Loss in iteration 138 : 0.6649111560380934
Loss in iteration 139 : 0.6647179281196691
Loss in iteration 140 : 0.6645247299832375
Loss in iteration 141 : 0.6643315616627489
Loss in iteration 142 : 0.6641384232345808
Loss in iteration 143 : 0.6639453148155865
Loss in iteration 144 : 0.6637522365612181
Loss in iteration 145 : 0.6635591886637078
Loss in iteration 146 : 0.6633661713503329
Loss in iteration 147 : 0.6631731848817287
Loss in iteration 148 : 0.6629802295502827
Loss in iteration 149 : 0.6627873056785705
Loss in iteration 150 : 0.6625944136178619
Loss in iteration 151 : 0.6624015537466772
Loss in iteration 152 : 0.6622087264694023
Loss in iteration 153 : 0.6620159322149509
Loss in iteration 154 : 0.661823171435479
Loss in iteration 155 : 0.6616304446051506
Loss in iteration 156 : 0.66143775221895
Loss in iteration 157 : 0.6612450947915363
Loss in iteration 158 : 0.6610524728561491
Loss in iteration 159 : 0.6608598869635478
Loss in iteration 160 : 0.6606673376810045
Loss in iteration 161 : 0.6604748255913316
Loss in iteration 162 : 0.6602823512919483
Loss in iteration 163 : 0.6600899153939861
Loss in iteration 164 : 0.659897518521434
Loss in iteration 165 : 0.6597051613103248
Loss in iteration 166 : 0.6595128444079473
Loss in iteration 167 : 0.6593205684721009
Loss in iteration 168 : 0.6591283341703855
Loss in iteration 169 : 0.6589361421795211
Loss in iteration 170 : 0.658743993184703
Loss in iteration 171 : 0.6585518878789903
Loss in iteration 172 : 0.6583598269627238
Loss in iteration 173 : 0.6581678111429801
Loss in iteration 174 : 0.6579758411330453
Loss in iteration 175 : 0.6577839176519346
Loss in iteration 176 : 0.6575920414239279
Loss in iteration 177 : 0.6574002131781415
Loss in iteration 178 : 0.6572084336481164
Loss in iteration 179 : 0.6570167035714489
Loss in iteration 180 : 0.6568250236894333
Loss in iteration 181 : 0.656633394746732
Loss in iteration 182 : 0.6564418174910626
Loss in iteration 183 : 0.656250292672915
Loss in iteration 184 : 0.6560588210452745
Loss in iteration 185 : 0.6558674033633612
Loss in iteration 186 : 0.6556760403843812
Loss in iteration 187 : 0.655484732867282
Loss in iteration 188 : 0.6552934815725102
Loss in iteration 189 : 0.6551022872617625
Loss in iteration 190 : 0.6549111506977265
Loss in iteration 191 : 0.6547200726438127
Loss in iteration 192 : 0.6545290538638501
Loss in iteration 193 : 0.6543380951217604
Loss in iteration 194 : 0.6541471971811892
Loss in iteration 195 : 0.6539563608050971
Loss in iteration 196 : 0.65376558675528
Loss in iteration 197 : 0.6535748757918564
Loss in iteration 198 : 0.6533842286726638
Loss in iteration 199 : 0.6531936461525888
Loss in iteration 200 : 0.6530031289828341
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.666, training accuracy 0.6705, time elapsed: 4152 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 1.4540402070126797
Loss in iteration 3 : 1.7731849893106941
Loss in iteration 4 : 0.954571706293835
Loss in iteration 5 : 0.8295842906546173
Loss in iteration 6 : 0.7653201722295488
Loss in iteration 7 : 0.6991952388582623
Loss in iteration 8 : 0.7050524403128593
Loss in iteration 9 : 0.6706815959689497
Loss in iteration 10 : 0.688231693631619
Loss in iteration 11 : 0.6447790173484123
Loss in iteration 12 : 0.6627891888672344
Loss in iteration 13 : 0.6320750924975554
Loss in iteration 14 : 0.6522444838234432
Loss in iteration 15 : 0.6244587462202158
Loss in iteration 16 : 0.6424204786793308
Loss in iteration 17 : 0.6173380726490585
Loss in iteration 18 : 0.6327710467177557
Loss in iteration 19 : 0.6112198898588641
Loss in iteration 20 : 0.625381598836477
Loss in iteration 21 : 0.6070918189304302
Loss in iteration 22 : 0.6218826367792885
Loss in iteration 23 : 0.6058280646130604
Loss in iteration 24 : 0.6201272564082061
Loss in iteration 25 : 0.6033988691784979
Loss in iteration 26 : 0.6153355908118895
Loss in iteration 27 : 0.6001521663826812
Loss in iteration 28 : 0.6135378876974957
Loss in iteration 29 : 0.5996497933213699
Loss in iteration 30 : 0.6129663894612164
Loss in iteration 31 : 0.5983542086704494
Loss in iteration 32 : 0.6100809804105514
Loss in iteration 33 : 0.5953575913850441
Loss in iteration 34 : 0.6058378686896114
Loss in iteration 35 : 0.5923301890659116
Loss in iteration 36 : 0.6024707106452664
Loss in iteration 37 : 0.5907679910604094
Loss in iteration 38 : 0.6006230094259444
Loss in iteration 39 : 0.5897998481546974
Loss in iteration 40 : 0.5982596471151443
Loss in iteration 41 : 0.5879111269422664
Loss in iteration 42 : 0.5968550879094595
Loss in iteration 43 : 0.5874730309374329
Loss in iteration 44 : 0.5971835479268671
Loss in iteration 45 : 0.5873758360287136
Loss in iteration 46 : 0.5964217519881984
Loss in iteration 47 : 0.5858844788903594
Loss in iteration 48 : 0.5939251145860109
Loss in iteration 49 : 0.5836339807794406
Loss in iteration 50 : 0.5912183936466223
Loss in iteration 51 : 0.5820984664433875
Loss in iteration 52 : 0.589535025158677
Loss in iteration 53 : 0.5814466758524524
Loss in iteration 54 : 0.5880420075702314
Loss in iteration 55 : 0.5802771568438436
Loss in iteration 56 : 0.5868936785506512
Loss in iteration 57 : 0.5797758627963433
Loss in iteration 58 : 0.5872822978063093
Loss in iteration 59 : 0.579998202963666
Loss in iteration 60 : 0.5873277871775933
Loss in iteration 61 : 0.5792804024866721
Loss in iteration 62 : 0.5858219412375897
Loss in iteration 63 : 0.5776098626836763
Loss in iteration 64 : 0.5836735491685743
Loss in iteration 65 : 0.5762228095980383
Loss in iteration 66 : 0.5821756087613482
Loss in iteration 67 : 0.5757094038708173
Loss in iteration 68 : 0.5810725773554496
Loss in iteration 69 : 0.574933402357866
Loss in iteration 70 : 0.5800612850366376
Loss in iteration 71 : 0.5743941164866151
Loss in iteration 72 : 0.5804028907356885
Loss in iteration 73 : 0.5747231824729258
Loss in iteration 74 : 0.5808509019309058
Loss in iteration 75 : 0.574445339605335
Loss in iteration 76 : 0.5799350782616881
Loss in iteration 77 : 0.5731421481785287
Loss in iteration 78 : 0.5781441627703015
Loss in iteration 79 : 0.5718487514097373
Loss in iteration 80 : 0.5767607247017708
Loss in iteration 81 : 0.5714057374055307
Loss in iteration 82 : 0.575884806933924
Loss in iteration 83 : 0.5708938316292047
Loss in iteration 84 : 0.5749454592525051
Loss in iteration 85 : 0.5703083672356145
Loss in iteration 86 : 0.5752305344991566
Loss in iteration 87 : 0.5706895040526657
Loss in iteration 88 : 0.5759741698492191
Loss in iteration 89 : 0.570732481552356
Loss in iteration 90 : 0.5754860002372867
Loss in iteration 91 : 0.569680965346513
Loss in iteration 92 : 0.5739220734876423
Loss in iteration 93 : 0.5684120022285808
Loss in iteration 94 : 0.5725760859327301
Loss in iteration 95 : 0.5679913285931758
Loss in iteration 96 : 0.5718626664786531
Loss in iteration 97 : 0.5677198167321617
Loss in iteration 98 : 0.5709571289791056
Loss in iteration 99 : 0.5670620851630085
Loss in iteration 100 : 0.5711475105205011
Loss in iteration 101 : 0.567460238798091
Loss in iteration 102 : 0.5721667741554778
Loss in iteration 103 : 0.5678011621920498
Loss in iteration 104 : 0.5720610273107525
Loss in iteration 105 : 0.5669547249622666
Loss in iteration 106 : 0.57064525763297
Loss in iteration 107 : 0.5656414028164227
Loss in iteration 108 : 0.5692481073629353
Loss in iteration 109 : 0.565172089933149
Loss in iteration 110 : 0.5686582056819408
Loss in iteration 111 : 0.5651819947596903
Loss in iteration 112 : 0.5677904042661389
Loss in iteration 113 : 0.5644276942052221
Loss in iteration 114 : 0.567782360675679
Loss in iteration 115 : 0.5647674170404815
Loss in iteration 116 : 0.5690798170885963
Loss in iteration 117 : 0.5654379318731403
Loss in iteration 118 : 0.5694005594263947
Loss in iteration 119 : 0.5648145066360024
Loss in iteration 120 : 0.5681072361076909
Loss in iteration 121 : 0.5633947278579785
Loss in iteration 122 : 0.5665528240381913
Loss in iteration 123 : 0.5627462187107559
Loss in iteration 124 : 0.5660193629927744
Loss in iteration 125 : 0.5631158234171785
Loss in iteration 126 : 0.5653071460797308
Loss in iteration 127 : 0.5623210242311882
Loss in iteration 128 : 0.5648891129086201
Loss in iteration 129 : 0.5624081666823869
Loss in iteration 130 : 0.566427212565695
Loss in iteration 131 : 0.5634697925474452
Loss in iteration 132 : 0.5673212082595319
Loss in iteration 133 : 0.5631827856255458
Loss in iteration 134 : 0.566211978392085
Loss in iteration 135 : 0.5616277332346332
Loss in iteration 136 : 0.5643820146004369
Loss in iteration 137 : 0.5605732365170675
Loss in iteration 138 : 0.5636925678566866
Loss in iteration 139 : 0.5612678139503824
Loss in iteration 140 : 0.56350530547187
Loss in iteration 141 : 0.5608534973465592
Loss in iteration 142 : 0.5623580689607954
Loss in iteration 143 : 0.5602156182266317
Loss in iteration 144 : 0.5638733441216731
Loss in iteration 145 : 0.5616691617866345
Loss in iteration 146 : 0.5656234506143846
Loss in iteration 147 : 0.5620149825134216
Loss in iteration 148 : 0.5649613896359095
Loss in iteration 149 : 0.5604216810781746
Loss in iteration 150 : 0.5627923983865084
Loss in iteration 151 : 0.5586781044204562
Loss in iteration 152 : 0.5614342431248787
Loss in iteration 153 : 0.5589996996475919
Loss in iteration 154 : 0.5621336860846017
Loss in iteration 155 : 0.560352682202363
Loss in iteration 156 : 0.5606423127260797
Loss in iteration 157 : 0.5583401985076607
Loss in iteration 158 : 0.5609108163783982
Loss in iteration 159 : 0.5595344909386516
Loss in iteration 160 : 0.5637985164657386
Loss in iteration 161 : 0.5611338168430369
Loss in iteration 162 : 0.5643899185697556
Loss in iteration 163 : 0.5600474712045838
Loss in iteration 164 : 0.5621654027502825
Loss in iteration 165 : 0.5575633546775598
Loss in iteration 166 : 0.5596168062211423
Loss in iteration 167 : 0.5560942517528559
Loss in iteration 168 : 0.5590768746375413
Loss in iteration 169 : 0.557925240465635
Loss in iteration 170 : 0.5616732520740083
Loss in iteration 171 : 0.5602148113955238
Loss in iteration 172 : 0.5583108766980575
Loss in iteration 173 : 0.5563303153856201
Loss in iteration 174 : 0.5595697069096458
Loss in iteration 175 : 0.5588389804639944
Loss in iteration 176 : 0.5634236283512811
Loss in iteration 177 : 0.5606113371680964
Loss in iteration 178 : 0.5634174473761523
Loss in iteration 179 : 0.5586876685491802
Loss in iteration 180 : 0.5602252078543637
Loss in iteration 181 : 0.5555917500571399
Loss in iteration 182 : 0.5572787488811936
Loss in iteration 183 : 0.5542887772764394
Loss in iteration 184 : 0.5571634681491102
Loss in iteration 185 : 0.5580505956597929
Loss in iteration 186 : 0.5601883890801141
Loss in iteration 187 : 0.5609625770881246
Loss in iteration 188 : 0.5583771096132077
Loss in iteration 189 : 0.5593446915010871
Loss in iteration 190 : 0.5597598795518427
Loss in iteration 191 : 0.558872316496784
Loss in iteration 192 : 0.5600539028316602
Loss in iteration 193 : 0.5586989750959498
Loss in iteration 194 : 0.5608310113497668
Loss in iteration 195 : 0.5584505571502235
Loss in iteration 196 : 0.5599035323357556
Loss in iteration 197 : 0.5571079747043509
Loss in iteration 198 : 0.558206781251102
Loss in iteration 199 : 0.5560714362978676
Loss in iteration 200 : 0.5573162345556283
Testing accuracy  of updater 5 on alg 0 with rate 0.19999999999999998 = 0.751, training accuracy 0.748, time elapsed: 3697 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6839063469572051
Loss in iteration 3 : 0.753796687860404
Loss in iteration 4 : 0.9283123220163777
Loss in iteration 5 : 0.8009022042306535
Loss in iteration 6 : 0.8008594498457292
Loss in iteration 7 : 0.6836293130922829
Loss in iteration 8 : 0.6823826361131887
Loss in iteration 9 : 0.6443359367072222
Loss in iteration 10 : 0.6434871638310818
Loss in iteration 11 : 0.621208359872454
Loss in iteration 12 : 0.6201442087007474
Loss in iteration 13 : 0.6053538903782278
Loss in iteration 14 : 0.6093545049695388
Loss in iteration 15 : 0.6012143234378774
Loss in iteration 16 : 0.6102401667918985
Loss in iteration 17 : 0.6020022817986729
Loss in iteration 18 : 0.612233271131413
Loss in iteration 19 : 0.599854736576365
Loss in iteration 20 : 0.6077464583352596
Loss in iteration 21 : 0.5928182132353991
Loss in iteration 22 : 0.5982269609442987
Loss in iteration 23 : 0.584498182849595
Loss in iteration 24 : 0.5890684855462135
Loss in iteration 25 : 0.5780517939992206
Loss in iteration 26 : 0.5830628191354864
Loss in iteration 27 : 0.5737283110097229
Loss in iteration 28 : 0.578388894410156
Loss in iteration 29 : 0.5696638329629402
Loss in iteration 30 : 0.5751038485058761
Loss in iteration 31 : 0.5675669645809368
Loss in iteration 32 : 0.5737759666961902
Loss in iteration 33 : 0.5659553953490137
Loss in iteration 34 : 0.5715737520987523
Loss in iteration 35 : 0.5630631727348916
Loss in iteration 36 : 0.5676922463111895
Loss in iteration 37 : 0.5593256725153797
Loss in iteration 38 : 0.5634810587995436
Loss in iteration 39 : 0.5560857874364482
Loss in iteration 40 : 0.5603371562080997
Loss in iteration 41 : 0.5539159055191203
Loss in iteration 42 : 0.5580127360051189
Loss in iteration 43 : 0.5518724159085199
Loss in iteration 44 : 0.5558396590027562
Loss in iteration 45 : 0.5501541270825462
Loss in iteration 46 : 0.5545170222535625
Loss in iteration 47 : 0.5490166300693842
Loss in iteration 48 : 0.5533157644972156
Loss in iteration 49 : 0.5475586868993578
Loss in iteration 50 : 0.5513927967139232
Loss in iteration 51 : 0.5455548213082215
Loss in iteration 52 : 0.5490422427494482
Loss in iteration 53 : 0.5435958976940252
Loss in iteration 54 : 0.5470325455956893
Loss in iteration 55 : 0.5421718618235991
Loss in iteration 56 : 0.5455124426377269
Loss in iteration 57 : 0.5409366237936499
Loss in iteration 58 : 0.5441101765611127
Loss in iteration 59 : 0.5397651225364771
Loss in iteration 60 : 0.5431351170690367
Loss in iteration 61 : 0.5389672131735167
Loss in iteration 62 : 0.5424063307703456
Loss in iteration 63 : 0.5381205319365452
Loss in iteration 64 : 0.5413110590610981
Loss in iteration 65 : 0.5369060400322887
Loss in iteration 66 : 0.539828846356168
Loss in iteration 67 : 0.5355967589343295
Loss in iteration 68 : 0.5384250799832199
Loss in iteration 69 : 0.5345772947326385
Loss in iteration 70 : 0.5373261559174235
Loss in iteration 71 : 0.5337321758866309
Loss in iteration 72 : 0.5363444366744352
Loss in iteration 73 : 0.5329078945823743
Loss in iteration 74 : 0.5356334592476666
Loss in iteration 75 : 0.5323313433161907
Loss in iteration 76 : 0.5351605516860816
Loss in iteration 77 : 0.5317878148478208
Loss in iteration 78 : 0.5344740709283836
Loss in iteration 79 : 0.5309773281504035
Loss in iteration 80 : 0.5334524840336323
Loss in iteration 81 : 0.5300309839357905
Loss in iteration 82 : 0.5324065461712938
Loss in iteration 83 : 0.5292590290614008
Loss in iteration 84 : 0.531567492476161
Loss in iteration 85 : 0.5286394023007402
Loss in iteration 86 : 0.5308368792394446
Loss in iteration 87 : 0.528029811252181
Loss in iteration 88 : 0.5303057153838534
Loss in iteration 89 : 0.5275996360647134
Loss in iteration 90 : 0.5299929076001372
Loss in iteration 91 : 0.5272315053162784
Loss in iteration 92 : 0.5295416400859861
Loss in iteration 93 : 0.5266538874710466
Loss in iteration 94 : 0.528792087603348
Loss in iteration 95 : 0.525927226613779
Loss in iteration 96 : 0.5279711139363555
Loss in iteration 97 : 0.5253160821335694
Loss in iteration 98 : 0.5273032556535696
Loss in iteration 99 : 0.5248419967183231
Loss in iteration 100 : 0.5267340001881606
Loss in iteration 101 : 0.5243717116871482
Loss in iteration 102 : 0.5263257243504442
Loss in iteration 103 : 0.5240423443341485
Loss in iteration 104 : 0.5261237950421946
Loss in iteration 105 : 0.523789703355769
Loss in iteration 106 : 0.5258254990702885
Loss in iteration 107 : 0.5233607299633952
Loss in iteration 108 : 0.5252499479513816
Loss in iteration 109 : 0.5227751842813265
Loss in iteration 110 : 0.52457594598233
Loss in iteration 111 : 0.5222725355837824
Loss in iteration 112 : 0.524024940140331
Loss in iteration 113 : 0.5218998198594175
Loss in iteration 114 : 0.523565011653857
Loss in iteration 115 : 0.5215247055862012
Loss in iteration 116 : 0.5232424618121196
Loss in iteration 117 : 0.5212673891591068
Loss in iteration 118 : 0.5231236631202535
Loss in iteration 119 : 0.5210991110302031
Loss in iteration 120 : 0.5229357111793326
Loss in iteration 121 : 0.5207735347064086
Loss in iteration 122 : 0.5224787086241667
Loss in iteration 123 : 0.5202822603538885
Loss in iteration 124 : 0.5219026931864688
Loss in iteration 125 : 0.5198543018118775
Loss in iteration 126 : 0.5214333728265865
Loss in iteration 127 : 0.5195568065647007
Loss in iteration 128 : 0.5210507467384179
Loss in iteration 129 : 0.5192496700367639
Loss in iteration 130 : 0.5207887717428641
Loss in iteration 131 : 0.5190449956685778
Loss in iteration 132 : 0.5207365431073788
Loss in iteration 133 : 0.5189438418851887
Loss in iteration 134 : 0.5206359735977865
Loss in iteration 135 : 0.5186958970892879
Loss in iteration 136 : 0.5202641506387116
Loss in iteration 137 : 0.5182687113724161
Loss in iteration 138 : 0.5197531451682926
Loss in iteration 139 : 0.5178918556134345
Loss in iteration 140 : 0.5193414968768949
Loss in iteration 141 : 0.5176535674096766
Loss in iteration 142 : 0.5190156060876177
Loss in iteration 143 : 0.5173968592803421
Loss in iteration 144 : 0.5187958346292876
Loss in iteration 145 : 0.5172303991315131
Loss in iteration 146 : 0.5187997880370904
Loss in iteration 147 : 0.5171866613762888
Loss in iteration 148 : 0.5187747246609048
Loss in iteration 149 : 0.5170014963079277
Loss in iteration 150 : 0.5184672306487432
Loss in iteration 151 : 0.516617452470318
Loss in iteration 152 : 0.5179972338637081
Loss in iteration 153 : 0.5162735647941529
Loss in iteration 154 : 0.5176257466616332
Loss in iteration 155 : 0.5160847286287085
Loss in iteration 156 : 0.5173432648446206
Loss in iteration 157 : 0.5158669754332429
Loss in iteration 158 : 0.5171508027719596
Loss in iteration 159 : 0.5157265123519059
Loss in iteration 160 : 0.5172039059708646
Loss in iteration 161 : 0.5157349931601245
Loss in iteration 162 : 0.5172496958098296
Loss in iteration 163 : 0.5156048040002492
Loss in iteration 164 : 0.5169936169287584
Loss in iteration 165 : 0.5152483741854791
Loss in iteration 166 : 0.516545595106326
Loss in iteration 167 : 0.5149216968435542
Loss in iteration 168 : 0.5162002906851947
Loss in iteration 169 : 0.514776748482752
Loss in iteration 170 : 0.5159532823441382
Loss in iteration 171 : 0.5145911895932709
Loss in iteration 172 : 0.5157745381822905
Loss in iteration 173 : 0.5144646020122645
Loss in iteration 174 : 0.5158711012737794
Loss in iteration 175 : 0.5145229375703797
Loss in iteration 176 : 0.5159886719008975
Loss in iteration 177 : 0.5144454383416037
Loss in iteration 178 : 0.5157770746293512
Loss in iteration 179 : 0.514104631907458
Loss in iteration 180 : 0.5153345704810386
Loss in iteration 181 : 0.5137793929466044
Loss in iteration 182 : 0.5150026359716326
Loss in iteration 183 : 0.5136755370167417
Loss in iteration 184 : 0.5147882657970428
Loss in iteration 185 : 0.5135205595012322
Loss in iteration 186 : 0.5146094693986731
Loss in iteration 187 : 0.5133937377393974
Loss in iteration 188 : 0.5147430134881829
Loss in iteration 189 : 0.5135010496658433
Loss in iteration 190 : 0.5149383986229245
Loss in iteration 191 : 0.513479385963818
Loss in iteration 192 : 0.5147701393609871
Loss in iteration 193 : 0.5131452099186343
Loss in iteration 194 : 0.5143178938991937
Loss in iteration 195 : 0.5128033184864962
Loss in iteration 196 : 0.5139849896479646
Loss in iteration 197 : 0.5127386497442948
Loss in iteration 198 : 0.5138065044342451
Loss in iteration 199 : 0.5126205444416628
Loss in iteration 200 : 0.5136130713548053
Testing accuracy  of updater 5 on alg 0 with rate 0.13999999999999999 = 0.7615, training accuracy 0.7585, time elapsed: 3741 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6759094302330232
Loss in iteration 3 : 0.6712463161618221
Loss in iteration 4 : 0.679478525951299
Loss in iteration 5 : 0.6794360597823794
Loss in iteration 6 : 0.6793337629915539
Loss in iteration 7 : 0.647346235894194
Loss in iteration 8 : 0.6355242882346706
Loss in iteration 9 : 0.6165352017906016
Loss in iteration 10 : 0.6085990367124303
Loss in iteration 11 : 0.5987416223320391
Loss in iteration 12 : 0.5930464352592464
Loss in iteration 13 : 0.5860430855945966
Loss in iteration 14 : 0.5818903637420053
Loss in iteration 15 : 0.5772713226214293
Loss in iteration 16 : 0.5755532275201184
Loss in iteration 17 : 0.5729916989162434
Loss in iteration 18 : 0.5739109928414068
Loss in iteration 19 : 0.5720441302337483
Loss in iteration 20 : 0.5739357571994341
Loss in iteration 21 : 0.569519868270707
Loss in iteration 22 : 0.5691955725351243
Loss in iteration 23 : 0.562133748984736
Loss in iteration 24 : 0.5599342009412441
Loss in iteration 25 : 0.5534788145589122
Loss in iteration 26 : 0.5512980114615457
Loss in iteration 27 : 0.5465433825230248
Loss in iteration 28 : 0.5451013573400756
Loss in iteration 29 : 0.5417733187059652
Loss in iteration 30 : 0.5413327359270264
Loss in iteration 31 : 0.5390596964792519
Loss in iteration 32 : 0.5395627102620497
Loss in iteration 33 : 0.5376718687027008
Loss in iteration 34 : 0.5385006111421342
Loss in iteration 35 : 0.5361135041470068
Loss in iteration 36 : 0.5364440934775822
Loss in iteration 37 : 0.5332776495904411
Loss in iteration 38 : 0.5329350008116519
Loss in iteration 39 : 0.5295883955199122
Loss in iteration 40 : 0.5290402424870803
Loss in iteration 41 : 0.5261228291334364
Loss in iteration 42 : 0.5257730992335709
Loss in iteration 43 : 0.52343011662365
Loss in iteration 44 : 0.5234491461371593
Loss in iteration 45 : 0.5215905258705044
Loss in iteration 46 : 0.5219825163558299
Loss in iteration 47 : 0.520352068604227
Loss in iteration 48 : 0.5208823645087701
Loss in iteration 49 : 0.5191459223131525
Loss in iteration 50 : 0.5195165574761844
Loss in iteration 51 : 0.5175411759172152
Loss in iteration 52 : 0.5176672545898252
Loss in iteration 53 : 0.5156146554908297
Loss in iteration 54 : 0.5156393532466574
Loss in iteration 55 : 0.5137377410926627
Loss in iteration 56 : 0.5138189467924793
Loss in iteration 57 : 0.5121533692777714
Loss in iteration 58 : 0.5123646548688262
Loss in iteration 59 : 0.5109102676457986
Loss in iteration 60 : 0.511266539523683
Loss in iteration 61 : 0.5099314597803717
Loss in iteration 62 : 0.5103496747437831
Loss in iteration 63 : 0.5090006103856108
Loss in iteration 64 : 0.5093590956564176
Loss in iteration 65 : 0.5079355202952169
Loss in iteration 66 : 0.50819466535924
Loss in iteration 67 : 0.506755585756845
Loss in iteration 68 : 0.5069700706401553
Loss in iteration 69 : 0.5056096560970127
Loss in iteration 70 : 0.5058423155915286
Loss in iteration 71 : 0.5045948377627998
Loss in iteration 72 : 0.504872693819228
Loss in iteration 73 : 0.503723625900299
Loss in iteration 74 : 0.5040593045705271
Loss in iteration 75 : 0.5029730673939489
Loss in iteration 76 : 0.5033397235094612
Loss in iteration 77 : 0.502259578427569
Loss in iteration 78 : 0.5026021436144534
Loss in iteration 79 : 0.5014987827697066
Loss in iteration 80 : 0.5017955391444421
Loss in iteration 81 : 0.5006964934497594
Loss in iteration 82 : 0.5009713876888527
Loss in iteration 83 : 0.4999226338119613
Loss in iteration 84 : 0.500202771327233
Loss in iteration 85 : 0.49921755905868365
Loss in iteration 86 : 0.4995121935693404
Loss in iteration 87 : 0.49857881737006865
Loss in iteration 88 : 0.49889846409779626
Loss in iteration 89 : 0.4980010613107454
Loss in iteration 90 : 0.49833966995298423
Loss in iteration 91 : 0.49745090151217203
Loss in iteration 92 : 0.4977797903714464
Loss in iteration 93 : 0.4968831935610765
Loss in iteration 94 : 0.497187447817726
Loss in iteration 95 : 0.4962987561394354
Loss in iteration 96 : 0.4965898129306902
Loss in iteration 97 : 0.49573641852760353
Loss in iteration 98 : 0.49602751885296925
Loss in iteration 99 : 0.4952153082590727
Loss in iteration 100 : 0.49550952071405957
Loss in iteration 101 : 0.49472868032514256
Loss in iteration 102 : 0.495035402631123
Loss in iteration 103 : 0.49427741163659433
Loss in iteration 104 : 0.4945985062177054
Loss in iteration 105 : 0.49384769047352384
Loss in iteration 106 : 0.4941654585046008
Loss in iteration 107 : 0.4934105924152706
Loss in iteration 108 : 0.49371269980510246
Loss in iteration 109 : 0.4929643691769128
Loss in iteration 110 : 0.493256629761124
Loss in iteration 111 : 0.4925345919271603
Loss in iteration 112 : 0.49282465819000015
Loss in iteration 113 : 0.49213256187243415
Loss in iteration 114 : 0.4924214424040283
Loss in iteration 115 : 0.49175085609352903
Loss in iteration 116 : 0.4920473331382227
Loss in iteration 117 : 0.4913926279731276
Loss in iteration 118 : 0.4917017526498974
Loss in iteration 119 : 0.49105212274973226
Loss in iteration 120 : 0.4913610545500999
Loss in iteration 121 : 0.49070743093668595
Loss in iteration 122 : 0.4910047945362296
Loss in iteration 123 : 0.4903552825660327
Loss in iteration 124 : 0.4906441814298279
Loss in iteration 125 : 0.49001529714053516
Loss in iteration 126 : 0.49030104327495383
Loss in iteration 127 : 0.4896959725595576
Loss in iteration 128 : 0.4899788532793605
Loss in iteration 129 : 0.4893900233234777
Loss in iteration 130 : 0.4896784559192241
Loss in iteration 131 : 0.4891014263005472
Loss in iteration 132 : 0.48940195156578137
Loss in iteration 133 : 0.48882807504341147
Loss in iteration 134 : 0.48913013675334255
Loss in iteration 135 : 0.4885511991699748
Loss in iteration 136 : 0.48884368256458505
Loss in iteration 137 : 0.488266522866779
Loss in iteration 138 : 0.4885511470451839
Loss in iteration 139 : 0.4879909857165199
Loss in iteration 140 : 0.48827206279189966
Loss in iteration 141 : 0.4877322065345679
Loss in iteration 142 : 0.4880096680513468
Loss in iteration 143 : 0.4874829478938354
Loss in iteration 144 : 0.4877650174485839
Loss in iteration 145 : 0.4872474982775138
Loss in iteration 146 : 0.4875416683085766
Loss in iteration 147 : 0.4870257166046134
Loss in iteration 148 : 0.48732253491594896
Loss in iteration 149 : 0.48680016472089505
Loss in iteration 150 : 0.48708838877994454
Loss in iteration 151 : 0.48656577578578764
Loss in iteration 152 : 0.48684638287619103
Loss in iteration 153 : 0.48633843919421504
Loss in iteration 154 : 0.48661534755053154
Loss in iteration 155 : 0.4861256941642523
Loss in iteration 156 : 0.48639851728264905
Loss in iteration 157 : 0.48592002012163477
Loss in iteration 158 : 0.4861969330666491
Loss in iteration 159 : 0.48572588142825135
Loss in iteration 160 : 0.4860152559796524
Loss in iteration 161 : 0.4855445077265825
Loss in iteration 162 : 0.4858373809167992
Loss in iteration 163 : 0.4853588026330617
Loss in iteration 164 : 0.4856435107267411
Loss in iteration 165 : 0.48516293493092844
Loss in iteration 166 : 0.48544004971429766
Loss in iteration 167 : 0.48497268164729285
Loss in iteration 168 : 0.4852460515928997
Loss in iteration 169 : 0.4847959111073869
Loss in iteration 170 : 0.485064754953413
Loss in iteration 171 : 0.4846244318396515
Loss in iteration 172 : 0.48489700294703936
Loss in iteration 173 : 0.4844628981175897
Loss in iteration 174 : 0.48474858063473797
Loss in iteration 175 : 0.48431378479273723
Loss in iteration 176 : 0.48460372884856734
Loss in iteration 177 : 0.4841596732959314
Loss in iteration 178 : 0.4844415195343181
Loss in iteration 179 : 0.48399387179894415
Loss in iteration 180 : 0.48426798333046644
Loss in iteration 181 : 0.4838326912036879
Loss in iteration 182 : 0.48410306972870853
Loss in iteration 183 : 0.4836846230962165
Loss in iteration 184 : 0.4839499634116006
Loss in iteration 185 : 0.48354038634256175
Loss in iteration 186 : 0.4838091250385584
Loss in iteration 187 : 0.48340489429200445
Loss in iteration 188 : 0.4836876742942839
Loss in iteration 189 : 0.4832819802068669
Loss in iteration 190 : 0.4835697930281772
Loss in iteration 191 : 0.4831533646184282
Loss in iteration 192 : 0.4834328686455086
Loss in iteration 193 : 0.4830113078372298
Loss in iteration 194 : 0.483282791253993
Loss in iteration 195 : 0.4828731883807192
Loss in iteration 196 : 0.483141005832572
Loss in iteration 197 : 0.4827484123246539
Loss in iteration 198 : 0.48301056416423194
Loss in iteration 199 : 0.48262613718402747
Loss in iteration 200 : 0.4828913198267634
Testing accuracy  of updater 5 on alg 0 with rate 0.08 = 0.7795, training accuracy 0.77325, time elapsed: 3739 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6793281100412396
Loss in iteration 3 : 0.6714682897026338
Loss in iteration 4 : 0.6645200440787903
Loss in iteration 5 : 0.6581502009736101
Loss in iteration 6 : 0.6521966570346109
Loss in iteration 7 : 0.6465686093162326
Loss in iteration 8 : 0.6412107410513788
Loss in iteration 9 : 0.6360868218372443
Loss in iteration 10 : 0.6311714175244509
Loss in iteration 11 : 0.6264454873685485
Loss in iteration 12 : 0.6218939780947416
Loss in iteration 13 : 0.6175044774840271
Loss in iteration 14 : 0.6132664376277831
Loss in iteration 15 : 0.6091707066736488
Loss in iteration 16 : 0.6052092295179663
Loss in iteration 17 : 0.6013748432356087
Loss in iteration 18 : 0.5976611279015039
Loss in iteration 19 : 0.5940622918204824
Loss in iteration 20 : 0.5905730797307687
Loss in iteration 21 : 0.5871886974757722
Loss in iteration 22 : 0.5839047491937979
Loss in iteration 23 : 0.5807171844299226
Loss in iteration 24 : 0.5776222533280666
Loss in iteration 25 : 0.5746164685135313
Loss in iteration 26 : 0.5716965725729108
Loss in iteration 27 : 0.5688595102500901
Loss in iteration 28 : 0.5661024046379309
Loss in iteration 29 : 0.5634225367726331
Loss in iteration 30 : 0.5608173281406681
Loss in iteration 31 : 0.5582843256924254
Loss in iteration 32 : 0.5558211890258004
Loss in iteration 33 : 0.553425679459712
Loss in iteration 34 : 0.5510956507639229
Loss in iteration 35 : 0.5488290413490743
Loss in iteration 36 : 0.5466238677508038
Loss in iteration 37 : 0.544478219264959
Loss in iteration 38 : 0.5423902536082373
Loss in iteration 39 : 0.5403581934903803
Loss in iteration 40 : 0.5383803239912015
Loss in iteration 41 : 0.5364549906400358
Loss in iteration 42 : 0.5345805981262278
Loss in iteration 43 : 0.5327556105656047
Loss in iteration 44 : 0.5309785879900826
Loss in iteration 45 : 0.529249570037659
Loss in iteration 46 : 0.5276235013423293
Loss in iteration 47 : 0.5265587198439579
Loss in iteration 48 : 0.5245083619418668
Loss in iteration 49 : 0.5232170691299627
Loss in iteration 50 : 0.5215585612410141
Loss in iteration 51 : 0.5216368225600486
Loss in iteration 52 : 0.527585713891705
Loss in iteration 53 : 0.524026004595502
Loss in iteration 54 : 0.5192680264428038
Loss in iteration 55 : 0.516773282915015
Loss in iteration 56 : 0.5154454048893502
Loss in iteration 57 : 0.5144096010862991
Loss in iteration 58 : 0.5134738846509876
Loss in iteration 59 : 0.5125700695990062
Loss in iteration 60 : 0.5116839590782046
Loss in iteration 61 : 0.5108092882969967
Loss in iteration 62 : 0.5099510735002692
Loss in iteration 63 : 0.509116611522039
Loss in iteration 64 : 0.5083240532677311
Loss in iteration 65 : 0.5075873272944781
Loss in iteration 66 : 0.50690998471834
Loss in iteration 67 : 0.5063735761843722
Loss in iteration 68 : 0.5062297975763692
Loss in iteration 69 : 0.5064474254976362
Loss in iteration 70 : 0.5064708912419993
Loss in iteration 71 : 0.505379374490649
Loss in iteration 72 : 0.5040247687589597
Loss in iteration 73 : 0.5027174130105582
Loss in iteration 74 : 0.5017475093847764
Loss in iteration 75 : 0.500931794470461
Loss in iteration 76 : 0.5002483128951032
Loss in iteration 77 : 0.49962168792924977
Loss in iteration 78 : 0.4990600401508381
Loss in iteration 79 : 0.49854098953550036
Loss in iteration 80 : 0.4980912064054057
Loss in iteration 81 : 0.49769551865773504
Loss in iteration 82 : 0.4974047107702267
Loss in iteration 83 : 0.49717074393387856
Loss in iteration 84 : 0.4969971037154094
Loss in iteration 85 : 0.49664127036666833
Loss in iteration 86 : 0.49615798018806495
Loss in iteration 87 : 0.49544658445995826
Loss in iteration 88 : 0.49476928199890313
Loss in iteration 89 : 0.49408612418780545
Loss in iteration 90 : 0.49351483916129973
Loss in iteration 91 : 0.4929826846376181
Loss in iteration 92 : 0.4925349480817946
Loss in iteration 93 : 0.4921172235382676
Loss in iteration 94 : 0.49176721466660855
Loss in iteration 95 : 0.4914381897813569
Loss in iteration 96 : 0.491173046159861
Loss in iteration 97 : 0.49090608732559815
Loss in iteration 98 : 0.490677949482415
Loss in iteration 99 : 0.4903754447306121
Loss in iteration 100 : 0.4900607898935286
Loss in iteration 101 : 0.4896353494869386
Loss in iteration 102 : 0.48921934113039534
Loss in iteration 103 : 0.488752844909221
Loss in iteration 104 : 0.4883418745773984
Loss in iteration 105 : 0.4879280612358573
Loss in iteration 106 : 0.48757838409888143
Loss in iteration 107 : 0.48723607307926065
Loss in iteration 108 : 0.4869509575239069
Loss in iteration 109 : 0.48666752344818354
Loss in iteration 110 : 0.4864325073429269
Loss in iteration 111 : 0.4861820279639292
Loss in iteration 112 : 0.4859643124942561
Loss in iteration 113 : 0.48570168881271236
Loss in iteration 114 : 0.48545368137177103
Loss in iteration 115 : 0.48514571395270517
Loss in iteration 116 : 0.484855844594221
Loss in iteration 117 : 0.48452371724778415
Loss in iteration 118 : 0.4842275020416532
Loss in iteration 119 : 0.48391289362749307
Loss in iteration 120 : 0.4836435332030906
Loss in iteration 121 : 0.4833663227372816
Loss in iteration 122 : 0.4831334618666313
Loss in iteration 123 : 0.48289136442591635
Loss in iteration 124 : 0.48268773303831086
Loss in iteration 125 : 0.48246539199387306
Loss in iteration 126 : 0.48227242090741684
Loss in iteration 127 : 0.48204756189384146
Loss in iteration 128 : 0.4818440626126601
Loss in iteration 129 : 0.48160233210590125
Loss in iteration 130 : 0.4813823735485067
Loss in iteration 131 : 0.4811304724002422
Loss in iteration 132 : 0.48090726264294603
Loss in iteration 133 : 0.48066306013513393
Loss in iteration 134 : 0.4804528094372825
Loss in iteration 135 : 0.48022814183925683
Loss in iteration 136 : 0.48003784724685544
Loss in iteration 137 : 0.479833294135815
Loss in iteration 138 : 0.4796598762740786
Loss in iteration 139 : 0.4794675877563073
Loss in iteration 140 : 0.4793014226481848
Loss in iteration 141 : 0.4791103699864385
Loss in iteration 142 : 0.4789414691589484
Loss in iteration 143 : 0.4787449668887775
Loss in iteration 144 : 0.4785703902913444
Loss in iteration 145 : 0.4783708606249534
Loss in iteration 146 : 0.47819600899545867
Loss in iteration 147 : 0.47800131772662197
Loss in iteration 148 : 0.4778338169846104
Loss in iteration 149 : 0.4776500590956014
Loss in iteration 150 : 0.47749382120453115
Loss in iteration 151 : 0.47732175802236376
Loss in iteration 152 : 0.4771754599130507
Loss in iteration 153 : 0.47701124105313714
Loss in iteration 154 : 0.47687007398654563
Loss in iteration 155 : 0.4767082809867801
Loss in iteration 156 : 0.4765674495660094
Loss in iteration 157 : 0.47640487956846134
Loss in iteration 158 : 0.4762629411218927
Loss in iteration 159 : 0.47610055301629983
Loss in iteration 160 : 0.4759598623221462
Loss in iteration 161 : 0.4758012090003012
Loss in iteration 162 : 0.4756653296279325
Loss in iteration 163 : 0.47551337123567
Loss in iteration 164 : 0.47538426292013697
Loss in iteration 165 : 0.47523944477557334
Loss in iteration 166 : 0.47511647458155704
Loss in iteration 167 : 0.4749769002492578
Loss in iteration 168 : 0.4748576859436931
Loss in iteration 169 : 0.47472070202387906
Loss in iteration 170 : 0.47460292819676914
Loss in iteration 171 : 0.47446698902026874
Loss in iteration 172 : 0.474349956442703
Loss in iteration 173 : 0.4742154672709221
Loss in iteration 174 : 0.474100242989756
Loss in iteration 175 : 0.4739688314380657
Loss in iteration 176 : 0.4738570727037411
Loss in iteration 177 : 0.4737301252541
Loss in iteration 178 : 0.47362274862722076
Loss in iteration 179 : 0.47350045314193495
Loss in iteration 180 : 0.47339711373301036
Loss in iteration 181 : 0.47327851323873066
Loss in iteration 182 : 0.47317802282035526
Loss in iteration 183 : 0.4730618128743956
Loss in iteration 184 : 0.4729630479403242
Loss in iteration 185 : 0.4728484796570212
Loss in iteration 186 : 0.47275110313259555
Loss in iteration 187 : 0.472638353080698
Loss in iteration 188 : 0.472542855909369
Loss in iteration 189 : 0.4724326693875691
Loss in iteration 190 : 0.47233981275053943
Loss in iteration 191 : 0.47223281562189995
Loss in iteration 192 : 0.47214300064145226
Loss in iteration 193 : 0.4720392408875416
Loss in iteration 194 : 0.4719522596144762
Loss in iteration 195 : 0.4718512380688399
Loss in iteration 196 : 0.47176648799027115
Loss in iteration 197 : 0.4716675557258944
Loss in iteration 198 : 0.47158448608710307
Loss in iteration 199 : 0.4714872758865123
Loss in iteration 200 : 0.4714057214924184
Testing accuracy  of updater 5 on alg 0 with rate 0.02 = 0.783, training accuracy 0.7865, time elapsed: 3790 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6778698287180743
Loss in iteration 3 : 0.6676413701938029
Loss in iteration 4 : 0.6595985387300962
Loss in iteration 5 : 0.6527818677861076
Loss in iteration 6 : 0.6467157798441976
Loss in iteration 7 : 0.6411919885005716
Loss in iteration 8 : 0.636087590665749
Loss in iteration 9 : 0.6313266589360986
Loss in iteration 10 : 0.6268554877830074
Loss in iteration 11 : 0.6226350142114585
Loss in iteration 12 : 0.6186342937599654
Loss in iteration 13 : 0.6148260858326134
Loss in iteration 14 : 0.611185822187736
Loss in iteration 15 : 0.6076979980202145
Loss in iteration 16 : 0.604351927719123
Loss in iteration 17 : 0.6011288635844008
Loss in iteration 18 : 0.5980132382871728
Loss in iteration 19 : 0.5949987961644485
Loss in iteration 20 : 0.5920861705743731
Loss in iteration 21 : 0.5892712777872668
Loss in iteration 22 : 0.5865484491668453
Loss in iteration 23 : 0.583911067340337
Loss in iteration 24 : 0.581354071366808
Loss in iteration 25 : 0.5788729983028291
Loss in iteration 26 : 0.5764652433184071
Loss in iteration 27 : 0.5741303091952256
Loss in iteration 28 : 0.5718777987816527
Loss in iteration 29 : 0.5697372960948832
Loss in iteration 30 : 0.5678100281868039
Loss in iteration 31 : 0.5661442441018282
Loss in iteration 32 : 0.564660075030143
Loss in iteration 33 : 0.5627180887507205
Loss in iteration 34 : 0.5606586222326878
Loss in iteration 35 : 0.5585953671962295
Loss in iteration 36 : 0.556716215573432
Loss in iteration 37 : 0.5549355483895221
Loss in iteration 38 : 0.5532406601329496
Loss in iteration 39 : 0.5515951199597814
Loss in iteration 40 : 0.5499952651013698
Loss in iteration 41 : 0.5484300995255555
Loss in iteration 42 : 0.5469037885886696
Loss in iteration 43 : 0.545412927703846
Loss in iteration 44 : 0.5439691544823803
Loss in iteration 45 : 0.5425724165423608
Loss in iteration 46 : 0.5412467560649891
Loss in iteration 47 : 0.5399771100343193
Loss in iteration 48 : 0.5387797009031928
Loss in iteration 49 : 0.5375630492199949
Loss in iteration 50 : 0.5363409990071349
Loss in iteration 51 : 0.535045226270774
Loss in iteration 52 : 0.5337716353297406
Loss in iteration 53 : 0.5325025198708654
Loss in iteration 54 : 0.5312922184264967
Loss in iteration 55 : 0.5301132705250512
Loss in iteration 56 : 0.5289827870733489
Loss in iteration 57 : 0.5278798498652223
Loss in iteration 58 : 0.5268158093743298
Loss in iteration 59 : 0.5257769519139781
Loss in iteration 60 : 0.5247771019668355
Loss in iteration 61 : 0.5238031335014414
Loss in iteration 62 : 0.5228707840567268
Loss in iteration 63 : 0.5219559245385845
Loss in iteration 64 : 0.5210724141165075
Loss in iteration 65 : 0.5201804752813111
Loss in iteration 66 : 0.5193028995556025
Loss in iteration 67 : 0.518405706219034
Loss in iteration 68 : 0.5175290147150557
Loss in iteration 69 : 0.5166498986305453
Loss in iteration 70 : 0.5158039417975282
Loss in iteration 71 : 0.514969129975653
Loss in iteration 72 : 0.5141692782548044
Loss in iteration 73 : 0.5133835896562046
Loss in iteration 74 : 0.512630934333737
Loss in iteration 75 : 0.5118919833135391
Loss in iteration 76 : 0.5111850751319371
Loss in iteration 77 : 0.5104885996527194
Loss in iteration 78 : 0.5098213822677147
Loss in iteration 79 : 0.5091556930434317
Loss in iteration 80 : 0.5085130052362439
Loss in iteration 81 : 0.5078614921089565
Loss in iteration 82 : 0.5072295661359483
Loss in iteration 83 : 0.5065871328663374
Loss in iteration 84 : 0.505967860925872
Loss in iteration 85 : 0.5053441144996963
Loss in iteration 86 : 0.5047480066566653
Loss in iteration 87 : 0.5041526468794916
Loss in iteration 88 : 0.5035864177451456
Loss in iteration 89 : 0.503022663792448
Loss in iteration 90 : 0.5024878832210112
Loss in iteration 91 : 0.5019543661692094
Loss in iteration 92 : 0.5014486059028433
Loss in iteration 93 : 0.5009398672773054
Loss in iteration 94 : 0.5004563111873912
Loss in iteration 95 : 0.49996386495993395
Loss in iteration 96 : 0.4994941742002719
Loss in iteration 97 : 0.49901187682647213
Loss in iteration 98 : 0.49855235296007777
Loss in iteration 99 : 0.4980808838955766
Loss in iteration 100 : 0.49763413171209814
Loss in iteration 101 : 0.49717823222817414
Loss in iteration 102 : 0.49674869449724834
Loss in iteration 103 : 0.4963120452829451
Loss in iteration 104 : 0.49590233611784207
Loss in iteration 105 : 0.49548568841397445
Loss in iteration 106 : 0.49509558231087625
Loss in iteration 107 : 0.49469667156920694
Loss in iteration 108 : 0.49432301178997023
Loss in iteration 109 : 0.49393741035642846
Loss in iteration 110 : 0.4935755442060666
Loss in iteration 111 : 0.49319916351446186
Loss in iteration 112 : 0.4928458448838071
Loss in iteration 113 : 0.4924774778556272
Loss in iteration 114 : 0.49213259532635983
Loss in iteration 115 : 0.4917738208350199
Loss in iteration 116 : 0.4914392932744913
Loss in iteration 117 : 0.4910923047263316
Loss in iteration 118 : 0.4907700087576982
Loss in iteration 119 : 0.49043589971900575
Loss in iteration 120 : 0.4901263609930292
Loss in iteration 121 : 0.48980445257206495
Loss in iteration 122 : 0.48950638064040947
Loss in iteration 123 : 0.4891944896462297
Loss in iteration 124 : 0.48890543023030125
Loss in iteration 125 : 0.48860116450152324
Loss in iteration 126 : 0.4883190226743948
Loss in iteration 127 : 0.4880212098871424
Loss in iteration 128 : 0.48774536551276276
Loss in iteration 129 : 0.4874543593127418
Loss in iteration 130 : 0.4871854871735626
Loss in iteration 131 : 0.48690233863334703
Loss in iteration 132 : 0.4866414797894164
Loss in iteration 133 : 0.4863669742856994
Loss in iteration 134 : 0.48611467097966365
Loss in iteration 135 : 0.48584871675253244
Loss in iteration 136 : 0.4856045113772968
Loss in iteration 137 : 0.4853460814251988
Loss in iteration 138 : 0.48510872253478593
Loss in iteration 139 : 0.48485648761345773
Loss in iteration 140 : 0.48462473604935646
Loss in iteration 141 : 0.4843778730732633
Loss in iteration 142 : 0.48415117574063754
Loss in iteration 143 : 0.4839096494080265
Loss in iteration 144 : 0.48368818002292446
Loss in iteration 145 : 0.4834524328652352
Loss in iteration 146 : 0.4832367024236017
Loss in iteration 147 : 0.48300718456743147
Loss in iteration 148 : 0.4827975681733865
Loss in iteration 149 : 0.48257433668266964
Loss in iteration 150 : 0.48237069299648716
Loss in iteration 151 : 0.4821532639005478
Loss in iteration 152 : 0.4819549479063637
Loss in iteration 153 : 0.4817425854793424
Loss in iteration 154 : 0.48154887234719457
Loss in iteration 155 : 0.4813410459928255
Loss in iteration 156 : 0.48115153287471957
Loss in iteration 157 : 0.48094811388358266
Loss in iteration 158 : 0.4807627927857816
Loss in iteration 159 : 0.4805639380064397
Loss in iteration 160 : 0.48038303739863225
Loss in iteration 161 : 0.4801889736243197
Loss in iteration 162 : 0.48001271801564216
Loss in iteration 163 : 0.4798235087898891
Loss in iteration 164 : 0.47965186109837493
Loss in iteration 165 : 0.4794672644625807
Loss in iteration 166 : 0.47929987389692397
Loss in iteration 167 : 0.479119468019637
Loss in iteration 168 : 0.47895589707719816
Loss in iteration 169 : 0.4787793430347207
Loss in iteration 170 : 0.4786193108538172
Loss in iteration 171 : 0.4784464783449648
Loss in iteration 172 : 0.47828992050135766
Loss in iteration 173 : 0.47812084149013945
Loss in iteration 174 : 0.4779678475315325
Loss in iteration 175 : 0.47780262243599725
Loss in iteration 176 : 0.4776533184309523
Loss in iteration 177 : 0.4774919868977839
Loss in iteration 178 : 0.47734636498207234
Loss in iteration 179 : 0.47718879182408047
Loss in iteration 180 : 0.47704664410583564
Loss in iteration 181 : 0.4768925711181648
Loss in iteration 182 : 0.4767536176153408
Loss in iteration 183 : 0.4766028218684702
Loss in iteration 184 : 0.47646686449664183
Loss in iteration 185 : 0.4763192364900002
Loss in iteration 186 : 0.4761862003473607
Loss in iteration 187 : 0.4760417213001771
Loss in iteration 188 : 0.4759116310497554
Loss in iteration 189 : 0.47577033546870495
Loss in iteration 190 : 0.47564326037579674
Loss in iteration 191 : 0.475505166164147
Loss in iteration 192 : 0.47538110468493366
Loss in iteration 193 : 0.47524612534855
Loss in iteration 194 : 0.4751249414155523
Loss in iteration 195 : 0.47499290542952566
Loss in iteration 196 : 0.47487440653855945
Loss in iteration 197 : 0.47474515992676136
Loss in iteration 198 : 0.47462920140919834
Loss in iteration 199 : 0.47450265676077924
Loss in iteration 200 : 0.4743891684547656
Testing accuracy  of updater 5 on alg 0 with rate 0.014 = 0.783, training accuracy 0.785, time elapsed: 3778 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6814407645670365
Loss in iteration 3 : 0.6747211110694254
Loss in iteration 4 : 0.6692812233221181
Loss in iteration 5 : 0.6645707876882198
Loss in iteration 6 : 0.6603466759556511
Loss in iteration 7 : 0.6564782661190489
Loss in iteration 8 : 0.6528846005359533
Loss in iteration 9 : 0.6495114672995184
Loss in iteration 10 : 0.6463204877077017
Loss in iteration 11 : 0.6432833540265629
Loss in iteration 12 : 0.6403785207557922
Loss in iteration 13 : 0.6375891678017942
Loss in iteration 14 : 0.6349018900946353
Loss in iteration 15 : 0.6323058132743663
Loss in iteration 16 : 0.629791986274138
Loss in iteration 17 : 0.6273529465421197
Loss in iteration 18 : 0.6249824279600507
Loss in iteration 19 : 0.6226752116576434
Loss in iteration 20 : 0.6204274380572163
Loss in iteration 21 : 0.6182384164467692
Loss in iteration 22 : 0.6161108368246554
Loss in iteration 23 : 0.6140211653394182
Loss in iteration 24 : 0.611964677791139
Loss in iteration 25 : 0.6099665907006643
Loss in iteration 26 : 0.6080012310655687
Loss in iteration 27 : 0.6060572045030337
Loss in iteration 28 : 0.604155206826397
Loss in iteration 29 : 0.602294630402263
Loss in iteration 30 : 0.6004705281657817
Loss in iteration 31 : 0.598680003508028
Loss in iteration 32 : 0.5969214054318911
Loss in iteration 33 : 0.5951935349240255
Loss in iteration 34 : 0.5934955641417742
Loss in iteration 35 : 0.5918266024547109
Loss in iteration 36 : 0.590185576828074
Loss in iteration 37 : 0.5885704558100707
Loss in iteration 38 : 0.5869799021437584
Loss in iteration 39 : 0.5854136857879796
Loss in iteration 40 : 0.5838718097407084
Loss in iteration 41 : 0.5823514621985001
Loss in iteration 42 : 0.5808514581330741
Loss in iteration 43 : 0.5793742852575251
Loss in iteration 44 : 0.5779243935906011
Loss in iteration 45 : 0.5765094759724068
Loss in iteration 46 : 0.5751574653117161
Loss in iteration 47 : 0.5738579645836857
Loss in iteration 48 : 0.5725892323851818
Loss in iteration 49 : 0.5712340628179521
Loss in iteration 50 : 0.5698915682508539
Loss in iteration 51 : 0.5685682374436891
Loss in iteration 52 : 0.5672854839707717
Loss in iteration 53 : 0.5660268831868394
Loss in iteration 54 : 0.5647908055631375
Loss in iteration 55 : 0.5635714368992207
Loss in iteration 56 : 0.5623692260566457
Loss in iteration 57 : 0.5611822937807089
Loss in iteration 58 : 0.5600118970420828
Loss in iteration 59 : 0.5588572157460467
Loss in iteration 60 : 0.557720744204726
Loss in iteration 61 : 0.5566021712416209
Loss in iteration 62 : 0.5555065279605894
Loss in iteration 63 : 0.5544306260985506
Loss in iteration 64 : 0.5533799255943715
Loss in iteration 65 : 0.5523371131096257
Loss in iteration 66 : 0.5513076227216249
Loss in iteration 67 : 0.5502724982961569
Loss in iteration 68 : 0.5492504780090874
Loss in iteration 69 : 0.548233945083245
Loss in iteration 70 : 0.5472366233119769
Loss in iteration 71 : 0.5462516942571389
Loss in iteration 72 : 0.5452845860203803
Loss in iteration 73 : 0.5443300122408995
Loss in iteration 74 : 0.5433911451319616
Loss in iteration 75 : 0.5424643719992787
Loss in iteration 76 : 0.541552879615734
Loss in iteration 77 : 0.5406535617501221
Loss in iteration 78 : 0.5397701570871953
Loss in iteration 79 : 0.5388983230088088
Loss in iteration 80 : 0.5380421242531901
Loss in iteration 81 : 0.5371941705801392
Loss in iteration 82 : 0.5363592050229213
Loss in iteration 83 : 0.5355283403387591
Loss in iteration 84 : 0.5347088166894514
Loss in iteration 85 : 0.5338937630500263
Loss in iteration 86 : 0.5330912867524412
Loss in iteration 87 : 0.5322961409919746
Loss in iteration 88 : 0.5315144026124379
Loss in iteration 89 : 0.5307415042069759
Loss in iteration 90 : 0.5299816832705473
Loss in iteration 91 : 0.529230969660463
Loss in iteration 92 : 0.5284928595891646
Loss in iteration 93 : 0.5277636612519223
Loss in iteration 94 : 0.5270467079063567
Loss in iteration 95 : 0.526337972976258
Loss in iteration 96 : 0.525640794417783
Loss in iteration 97 : 0.5249504200648029
Loss in iteration 98 : 0.5242704468352083
Loss in iteration 99 : 0.5235958752730945
Loss in iteration 100 : 0.5229309400942138
Loss in iteration 101 : 0.5222712291123079
Loss in iteration 102 : 0.5216212964727587
Loss in iteration 103 : 0.5209774104273696
Loss in iteration 104 : 0.5203436700769614
Loss in iteration 105 : 0.5197167290030609
Loss in iteration 106 : 0.5190999876105011
Loss in iteration 107 : 0.5184903283226902
Loss in iteration 108 : 0.5178906713948953
Loss in iteration 109 : 0.5172979938863836
Loss in iteration 110 : 0.5167149692736888
Loss in iteration 111 : 0.5161384878979111
Loss in iteration 112 : 0.5155711269924077
Loss in iteration 113 : 0.5150096266892646
Loss in iteration 114 : 0.514456636392194
Loss in iteration 115 : 0.5139089150036771
Loss in iteration 116 : 0.5133693187594958
Loss in iteration 117 : 0.5128348327222202
Loss in iteration 118 : 0.5123084396575934
Loss in iteration 119 : 0.5117873682125574
Loss in iteration 120 : 0.5112745085527808
Loss in iteration 121 : 0.5107672319128833
Loss in iteration 122 : 0.5102682166492193
Loss in iteration 123 : 0.5097748859861975
Loss in iteration 124 : 0.5092897293813211
Loss in iteration 125 : 0.5088101441310356
Loss in iteration 126 : 0.5083385099072891
Loss in iteration 127 : 0.5078721334355697
Loss in iteration 128 : 0.5074133835273135
Loss in iteration 129 : 0.5069594693417676
Loss in iteration 130 : 0.5065128599292001
Loss in iteration 131 : 0.5060707210317376
Loss in iteration 132 : 0.5056356954085302
Loss in iteration 133 : 0.5052049523418306
Loss in iteration 134 : 0.504781295209248
Loss in iteration 135 : 0.5043618843515891
Loss in iteration 136 : 0.5039496178851703
Loss in iteration 137 : 0.5035415927559134
Loss in iteration 138 : 0.5031407571386548
Loss in iteration 139 : 0.5027440798702572
Loss in iteration 140 : 0.5023545671110731
Loss in iteration 141 : 0.5019689982571687
Loss in iteration 142 : 0.5015904907437607
Loss in iteration 143 : 0.5012155904160902
Loss in iteration 144 : 0.5008476073773117
Loss in iteration 145 : 0.5004828391978018
Loss in iteration 146 : 0.5001248708981817
Loss in iteration 147 : 0.4997697581135163
Loss in iteration 148 : 0.4994214100161433
Loss in iteration 149 : 0.4990756423033859
Loss in iteration 150 : 0.4987366908702812
Loss in iteration 151 : 0.4984001116908298
Loss in iteration 152 : 0.4980704451288599
Loss in iteration 153 : 0.49774294708917705
Loss in iteration 154 : 0.4974224503591343
Loss in iteration 155 : 0.4971038609567314
Loss in iteration 156 : 0.4967923223505074
Loss in iteration 157 : 0.49648234543521275
Loss in iteration 158 : 0.49617943063724596
Loss in iteration 159 : 0.49587766614702794
Loss in iteration 160 : 0.49558296482436764
Loss in iteration 161 : 0.4952889875431154
Loss in iteration 162 : 0.49500209953891994
Loss in iteration 163 : 0.4947155456272109
Loss in iteration 164 : 0.4944361514114084
Loss in iteration 165 : 0.4941567580573356
Loss in iteration 166 : 0.49388463069057337
Loss in iteration 167 : 0.49361220909826314
Loss in iteration 168 : 0.493347167756471
Loss in iteration 169 : 0.493081536486129
Loss in iteration 170 : 0.4928233797789765
Loss in iteration 171 : 0.49256430688501185
Loss in iteration 172 : 0.4923127707382233
Loss in iteration 173 : 0.49205995934558927
Loss in iteration 174 : 0.49181472179999863
Loss in iteration 175 : 0.4915678418377355
Loss in iteration 176 : 0.49132856596257746
Loss in iteration 177 : 0.4910873069575411
Loss in iteration 178 : 0.49085369158331615
Loss in iteration 179 : 0.49061780160447527
Loss in iteration 180 : 0.4903896078173867
Loss in iteration 181 : 0.4901588958993186
Loss in iteration 182 : 0.4899359357490336
Loss in iteration 183 : 0.4897102412561132
Loss in iteration 184 : 0.48949234148850496
Loss in iteration 185 : 0.48927149533765735
Loss in iteration 186 : 0.48905846386302015
Loss in iteration 187 : 0.4888422681462588
Loss in iteration 188 : 0.4886338847014925
Loss in iteration 189 : 0.4884221207372778
Loss in iteration 190 : 0.488218153941472
Loss in iteration 191 : 0.4880106090166625
Loss in iteration 192 : 0.48781084509136874
Loss in iteration 193 : 0.4876073380808684
Loss in iteration 194 : 0.4874116012891979
Loss in iteration 195 : 0.4872119907646531
Loss in iteration 196 : 0.48702014326740595
Loss in iteration 197 : 0.4868243149609098
Loss in iteration 198 : 0.48663623874448764
Loss in iteration 199 : 0.4864440833330627
Loss in iteration 200 : 0.48625965862786386
Testing accuracy  of updater 5 on alg 0 with rate 0.008 = 0.782, training accuracy 0.77825, time elapsed: 3774 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6892634686487361
Loss in iteration 3 : 0.6867938057348211
Loss in iteration 4 : 0.684850860794412
Loss in iteration 5 : 0.6831971116976113
Loss in iteration 6 : 0.6817287070838774
Loss in iteration 7 : 0.6803890118344678
Loss in iteration 8 : 0.679143542482904
Loss in iteration 9 : 0.6779699837580716
Loss in iteration 10 : 0.6768532835726634
Loss in iteration 11 : 0.6757829024124935
Loss in iteration 12 : 0.6747511729992869
Loss in iteration 13 : 0.6737523041833172
Loss in iteration 14 : 0.6727817714411877
Loss in iteration 15 : 0.6718359382869941
Loss in iteration 16 : 0.6709118150409893
Loss in iteration 17 : 0.6700069001442882
Loss in iteration 18 : 0.6691190722543187
Loss in iteration 19 : 0.6682465145388954
Loss in iteration 20 : 0.6673876600196234
Loss in iteration 21 : 0.666541151037044
Loss in iteration 22 : 0.6657058083666242
Loss in iteration 23 : 0.66488060699943
Loss in iteration 24 : 0.6640646565447873
Loss in iteration 25 : 0.6632571848467687
Loss in iteration 26 : 0.6624575238566742
Loss in iteration 27 : 0.6616650971340535
Loss in iteration 28 : 0.6608794085913213
Loss in iteration 29 : 0.6601000322684456
Loss in iteration 30 : 0.6593266030354027
Loss in iteration 31 : 0.6585588081820245
Loss in iteration 32 : 0.6577963798796226
Loss in iteration 33 : 0.6570390884988715
Loss in iteration 34 : 0.6562867367554536
Loss in iteration 35 : 0.6555391546378091
Loss in iteration 36 : 0.654796195056041
Loss in iteration 37 : 0.6540577301405258
Loss in iteration 38 : 0.6533236481139848
Loss in iteration 39 : 0.6525938506610962
Loss in iteration 40 : 0.6518682507238461
Loss in iteration 41 : 0.6511467706574434
Loss in iteration 42 : 0.650429340689367
Loss in iteration 43 : 0.6497158976320178
Loss in iteration 44 : 0.6490063838070721
Loss in iteration 45 : 0.6483007461462578
Loss in iteration 46 : 0.6475989354392295
Loss in iteration 47 : 0.6469009057040865
Loss in iteration 48 : 0.6462066136602704
Loss in iteration 49 : 0.6455160182870139
Loss in iteration 50 : 0.6448290804533678
Loss in iteration 51 : 0.6441457626082122
Loss in iteration 52 : 0.6434660285206387
Loss in iteration 53 : 0.6427898430627134
Loss in iteration 54 : 0.6421171720279917
Loss in iteration 55 : 0.6414479819802816
Loss in iteration 56 : 0.6407822401280757
Loss in iteration 57 : 0.6401199142208681
Loss in iteration 58 : 0.6394609724641998
Loss in iteration 59 : 0.6388053834508101
Loss in iteration 60 : 0.6381531161057986
Loss in iteration 61 : 0.6375041396438169
Loss in iteration 62 : 0.6368584235376084
Loss in iteration 63 : 0.6362159374945233
Loss in iteration 64 : 0.6355766514589681
Loss in iteration 65 : 0.6349405357417491
Loss in iteration 66 : 0.6343075639499807
Loss in iteration 67 : 0.6336777633798165
Loss in iteration 68 : 0.6330520363697723
Loss in iteration 69 : 0.6324328042315145
Loss in iteration 70 : 0.6318104765060749
Loss in iteration 71 : 0.6311905231715133
Loss in iteration 72 : 0.6305747256490818
Loss in iteration 73 : 0.6299625229357312
Loss in iteration 74 : 0.6293534862836404
Loss in iteration 75 : 0.6287474254097224
Loss in iteration 76 : 0.6281442618741289
Loss in iteration 77 : 0.6275439492015338
Loss in iteration 78 : 0.6269464582694211
Loss in iteration 79 : 0.6263517636297137
Loss in iteration 80 : 0.6257598498857008
Loss in iteration 81 : 0.625170707983065
Loss in iteration 82 : 0.6245843529619163
Loss in iteration 83 : 0.6240008191657539
Loss in iteration 84 : 0.6234201584469613
Loss in iteration 85 : 0.6228422614977086
Loss in iteration 86 : 0.6222667605633502
Loss in iteration 87 : 0.6216933003831826
Loss in iteration 88 : 0.6211221896228992
Loss in iteration 89 : 0.6205537451310202
Loss in iteration 90 : 0.6199880491108807
Loss in iteration 91 : 0.6194250407704126
Loss in iteration 92 : 0.6188647207045133
Loss in iteration 93 : 0.6183071188109229
Loss in iteration 94 : 0.61775221014971
Loss in iteration 95 : 0.6171996713166218
Loss in iteration 96 : 0.6166493282046004
Loss in iteration 97 : 0.6161014137559933
Loss in iteration 98 : 0.6155560855985338
Loss in iteration 99 : 0.6150132566847959
Loss in iteration 100 : 0.6144728623966662
Loss in iteration 101 : 0.613934751851778
Loss in iteration 102 : 0.6133989503660329
Loss in iteration 103 : 0.6128654551211158
Loss in iteration 104 : 0.6123343576706829
Loss in iteration 105 : 0.6118056366411526
Loss in iteration 106 : 0.6112793306624694
Loss in iteration 107 : 0.6107553870090363
Loss in iteration 108 : 0.6102338416562878
Loss in iteration 109 : 0.6097146280212677
Loss in iteration 110 : 0.6091977810301408
Loss in iteration 111 : 0.6086831917589204
Loss in iteration 112 : 0.608170892572026
Loss in iteration 113 : 0.6076607705279847
Loss in iteration 114 : 0.6071528900692947
Loss in iteration 115 : 0.6066471550403844
Loss in iteration 116 : 0.6061436409089196
Loss in iteration 117 : 0.6056422726956325
Loss in iteration 118 : 0.6051431300752054
Loss in iteration 119 : 0.6046461452562997
Loss in iteration 120 : 0.6041513826284187
Loss in iteration 121 : 0.603658767437476
Loss in iteration 122 : 0.603168355638761
Loss in iteration 123 : 0.6026800663541858
Loss in iteration 124 : 0.60219395686631
Loss in iteration 125 : 0.6017099346322679
Loss in iteration 126 : 0.6012280593022011
Loss in iteration 127 : 0.6007482216379335
Loss in iteration 128 : 0.6002704870178719
Loss in iteration 129 : 0.5997947413312325
Loss in iteration 130 : 0.5993210655220182
Loss in iteration 131 : 0.5988493557669549
Loss in iteration 132 : 0.5983797042264587
Loss in iteration 133 : 0.5979120128263367
Loss in iteration 134 : 0.5974463707901488
Loss in iteration 135 : 0.5969826775924941
Loss in iteration 136 : 0.5965210160532245
Loss in iteration 137 : 0.5960612816729992
Loss in iteration 138 : 0.595603554089553
Loss in iteration 139 : 0.5951477243628449
Loss in iteration 140 : 0.5946938713617098
Loss in iteration 141 : 0.5942418818063634
Loss in iteration 142 : 0.5937918355801949
Loss in iteration 143 : 0.5933436188764233
Loss in iteration 144 : 0.5928973145901555
Loss in iteration 145 : 0.5924528140427116
Loss in iteration 146 : 0.5920102019477879
Loss in iteration 147 : 0.5915693766161786
Loss in iteration 148 : 0.5911304194889238
Loss in iteration 149 : 0.5906932345635446
Loss in iteration 150 : 0.5902578957527325
Loss in iteration 151 : 0.5898243120535719
Loss in iteration 152 : 0.5893925480917495
Loss in iteration 153 : 0.5889625184911499
Loss in iteration 154 : 0.588534277496251
Loss in iteration 155 : 0.5881077472469072
Loss in iteration 156 : 0.5876829700611379
Loss in iteration 157 : 0.5872598793645374
Loss in iteration 158 : 0.5868385035468718
Loss in iteration 159 : 0.5864187924679711
Loss in iteration 160 : 0.5860007580705836
Loss in iteration 161 : 0.5855843709137569
Loss in iteration 162 : 0.5851696238134608
Loss in iteration 163 : 0.58475650956621
Loss in iteration 164 : 0.5843450002682837
Loss in iteration 165 : 0.5839351098177578
Loss in iteration 166 : 0.5835267899254812
Loss in iteration 167 : 0.5831200730022362
Loss in iteration 168 : 0.5827148924034691
Loss in iteration 169 : 0.5823112962086812
Loss in iteration 170 : 0.5819092025973125
Loss in iteration 171 : 0.5815086726540725
Loss in iteration 172 : 0.5811096134540783
Loss in iteration 173 : 0.5807120961792511
Loss in iteration 174 : 0.5803160216365646
Loss in iteration 175 : 0.5799214672299674
Loss in iteration 176 : 0.5795283327487077
Loss in iteration 177 : 0.5791366968158841
Loss in iteration 178 : 0.5787464632475091
Loss in iteration 179 : 0.5783577064997287
Loss in iteration 180 : 0.5779703386275582
Loss in iteration 181 : 0.5775844252854874
Loss in iteration 182 : 0.5771998898504179
Loss in iteration 183 : 0.5768167860228761
Loss in iteration 184 : 0.5764350505657236
Loss in iteration 185 : 0.5760547235364879
Loss in iteration 186 : 0.5756757564362733
Loss in iteration 187 : 0.5752981749273725
Loss in iteration 188 : 0.5749219461529936
Loss in iteration 189 : 0.5745470809862118
Loss in iteration 190 : 0.5741735627961987
Loss in iteration 191 : 0.5738013873034709
Loss in iteration 192 : 0.5734305544009021
Loss in iteration 193 : 0.573061044288157
Loss in iteration 194 : 0.5726928734102282
Loss in iteration 195 : 0.5723260061921704
Loss in iteration 196 : 0.5719604754423612
Loss in iteration 197 : 0.5715962297834006
Loss in iteration 198 : 0.571233318069829
Loss in iteration 199 : 0.5708716733341488
Loss in iteration 200 : 0.5705113601127705
Testing accuracy  of updater 5 on alg 0 with rate 0.0019999999999999983 = 0.7565, training accuracy 0.75975, time elapsed: 4647 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 3.0166175422620536
Loss in iteration 3 : 2.4686069150061822
Loss in iteration 4 : 1.839756115281386
Loss in iteration 5 : 1.2652937429736602
Loss in iteration 6 : 1.4080173861407155
Loss in iteration 7 : 0.7223093056718862
Loss in iteration 8 : 1.3929399098024358
Loss in iteration 9 : 1.120983698449655
Loss in iteration 10 : 0.797822340569918
Loss in iteration 11 : 1.17131361806325
Loss in iteration 12 : 0.9406699714211949
Loss in iteration 13 : 0.6524642584034595
Loss in iteration 14 : 0.9278276752219854
Loss in iteration 15 : 0.8278419622838823
Loss in iteration 16 : 0.6659761107210661
Loss in iteration 17 : 0.8685010221846599
Loss in iteration 18 : 0.8062859743013502
Loss in iteration 19 : 0.6641875453761639
Loss in iteration 20 : 0.7921104715473566
Loss in iteration 21 : 0.7407351539948374
Loss in iteration 22 : 0.5953801747093554
Loss in iteration 23 : 0.7035187050469114
Loss in iteration 24 : 0.6831287096403904
Loss in iteration 25 : 0.5912355107702943
Loss in iteration 26 : 0.6793436453779834
Loss in iteration 27 : 0.6404537141497466
Loss in iteration 28 : 0.5718385362773754
Loss in iteration 29 : 0.6250141272450316
Loss in iteration 30 : 0.56303396347972
Loss in iteration 31 : 0.5515256697411455
Loss in iteration 32 : 0.587075233935624
Loss in iteration 33 : 0.5273216252009593
Loss in iteration 34 : 0.5626451201095928
Loss in iteration 35 : 0.5415109423159122
Loss in iteration 36 : 0.5104608887367237
Loss in iteration 37 : 0.5390041482402655
Loss in iteration 38 : 0.49291919814323715
Loss in iteration 39 : 0.5210282347977016
Loss in iteration 40 : 0.5008740127146781
Loss in iteration 41 : 0.5083286232754226
Loss in iteration 42 : 0.49910413763096245
Loss in iteration 43 : 0.4920671974726938
Loss in iteration 44 : 0.49698671250837195
Loss in iteration 45 : 0.4848934957308238
Loss in iteration 46 : 0.4976511648924844
Loss in iteration 47 : 0.485390726716702
Loss in iteration 48 : 0.4910590904879077
Loss in iteration 49 : 0.4812820602703367
Loss in iteration 50 : 0.4843995681354625
Loss in iteration 51 : 0.4811600392579486
Loss in iteration 52 : 0.4813002814141636
Loss in iteration 53 : 0.48053440837688477
Loss in iteration 54 : 0.47503100996449815
Loss in iteration 55 : 0.4772587205666604
Loss in iteration 56 : 0.47050241474504306
Loss in iteration 57 : 0.4765530278380029
Loss in iteration 58 : 0.46777804215092117
Loss in iteration 59 : 0.47277889088149466
Loss in iteration 60 : 0.4653373135549625
Loss in iteration 61 : 0.46828192480023473
Loss in iteration 62 : 0.466331166041497
Loss in iteration 63 : 0.46499824058039924
Loss in iteration 64 : 0.466581555961949
Loss in iteration 65 : 0.4621994514420156
Loss in iteration 66 : 0.4653580998144513
Loss in iteration 67 : 0.46245002153704023
Loss in iteration 68 : 0.46382568761115517
Loss in iteration 69 : 0.4633408845333878
Loss in iteration 70 : 0.4616627397741657
Loss in iteration 71 : 0.4634262140712753
Loss in iteration 72 : 0.4615102347934745
Loss in iteration 73 : 0.46259983568374036
Loss in iteration 74 : 0.4621295318192978
Loss in iteration 75 : 0.4610291153481556
Loss in iteration 76 : 0.4620089342787354
Loss in iteration 77 : 0.46102242334947513
Loss in iteration 78 : 0.4610562721683209
Loss in iteration 79 : 0.461399070740228
Loss in iteration 80 : 0.4603621626898115
Loss in iteration 81 : 0.4607972221803774
Loss in iteration 82 : 0.460983501746775
Loss in iteration 83 : 0.4602571293740215
Loss in iteration 84 : 0.4607578243730205
Loss in iteration 85 : 0.46079382481769227
Loss in iteration 86 : 0.46066463038783323
Loss in iteration 87 : 0.46164529486976297
Loss in iteration 88 : 0.46273051940530946
Loss in iteration 89 : 0.46404779217287845
Loss in iteration 90 : 0.46549687031678766
Loss in iteration 91 : 0.4639621941231178
Loss in iteration 92 : 0.4609024394694314
Loss in iteration 93 : 0.45990291120497634
Loss in iteration 94 : 0.4616906682829007
Loss in iteration 95 : 0.4629030789087291
Loss in iteration 96 : 0.46136121091350984
Loss in iteration 97 : 0.45976763516186003
Loss in iteration 98 : 0.46030994795566005
Loss in iteration 99 : 0.4614209907813968
Loss in iteration 100 : 0.4609419044883681
Loss in iteration 101 : 0.4596592671223085
Loss in iteration 102 : 0.45979951594955937
Loss in iteration 103 : 0.4607035510396869
Loss in iteration 104 : 0.46044477746400037
Loss in iteration 105 : 0.4595371553155166
Loss in iteration 106 : 0.4595962894696588
Loss in iteration 107 : 0.4602100333331119
Loss in iteration 108 : 0.4600743632561993
Loss in iteration 109 : 0.45946778564801244
Loss in iteration 110 : 0.4594595533913532
Loss in iteration 111 : 0.45983158778866173
Loss in iteration 112 : 0.45980725063621897
Loss in iteration 113 : 0.45945736305128515
Loss in iteration 114 : 0.4593508187576842
Loss in iteration 115 : 0.4595278987463119
Loss in iteration 116 : 0.4596039296039806
Loss in iteration 117 : 0.45945923364266583
Loss in iteration 118 : 0.45932582817639456
Loss in iteration 119 : 0.4593230547676661
Loss in iteration 120 : 0.45938238720521707
Loss in iteration 121 : 0.45939605042343107
Loss in iteration 122 : 0.45935139129502334
Loss in iteration 123 : 0.4592871679104846
Loss in iteration 124 : 0.45924105977424096
Loss in iteration 125 : 0.459237311872164
Loss in iteration 126 : 0.45926812180346843
Loss in iteration 127 : 0.4592893717843182
Loss in iteration 128 : 0.45926313915093414
Loss in iteration 129 : 0.459199974911979
Loss in iteration 130 : 0.4591487419411928
Loss in iteration 131 : 0.45914729435348967
Loss in iteration 132 : 0.45918326278123955
Loss in iteration 133 : 0.4592135723954277
Loss in iteration 134 : 0.45920516562639385
Loss in iteration 135 : 0.4591635622046849
Loss in iteration 136 : 0.4591182405780504
Loss in iteration 137 : 0.4590955244716304
Loss in iteration 138 : 0.45909833374330594
Loss in iteration 139 : 0.45911352619637247
Loss in iteration 140 : 0.45912638339708517
Loss in iteration 141 : 0.4591305381697981
Loss in iteration 142 : 0.4591281500632789
Loss in iteration 143 : 0.4591228289389089
Loss in iteration 144 : 0.45911899642262755
Loss in iteration 145 : 0.45911567551853405
Loss in iteration 146 : 0.4591131563419109
Loss in iteration 147 : 0.45910897497347103
Loss in iteration 148 : 0.45910448882498084
Loss in iteration 149 : 0.45910079873648524
Loss in iteration 150 : 0.459101303729154
Loss in iteration 151 : 0.4591082818741613
Loss in iteration 152 : 0.45912780780259616
Loss in iteration 153 : 0.45916623294653647
Loss in iteration 154 : 0.45924214931270335
Loss in iteration 155 : 0.45938076743484657
Loss in iteration 156 : 0.45965590171565884
Loss in iteration 157 : 0.4601699195996946
Loss in iteration 158 : 0.46124407579384136
Loss in iteration 159 : 0.46321783521744103
Loss in iteration 160 : 0.46741538150350675
Loss in iteration 161 : 0.4739882306288528
Loss in iteration 162 : 0.4864744658572211
Loss in iteration 163 : 0.5008789196251674
Loss in iteration 164 : 0.5206212092359582
Loss in iteration 165 : 0.5453751273176903
Loss in iteration 166 : 0.5440897927693767
Loss in iteration 167 : 0.5251961761905477
Loss in iteration 168 : 0.48730466746053747
Loss in iteration 169 : 0.4652563544991062
Loss in iteration 170 : 0.46498487963781276
Loss in iteration 171 : 0.478633886065699
Loss in iteration 172 : 0.4947528411042543
Loss in iteration 173 : 0.4994757105499833
Loss in iteration 174 : 0.4864784982759019
Loss in iteration 175 : 0.46984057560894915
Loss in iteration 176 : 0.4612442969609032
Loss in iteration 177 : 0.46299454090621406
Loss in iteration 178 : 0.4706307440915588
Loss in iteration 179 : 0.4786916201275589
Loss in iteration 180 : 0.4842127120067954
Loss in iteration 181 : 0.4862021333217667
Loss in iteration 182 : 0.4805466644752176
Loss in iteration 183 : 0.4737737857152564
Loss in iteration 184 : 0.46752909746002086
Loss in iteration 185 : 0.46314339469117527
Loss in iteration 186 : 0.46043159944572903
Loss in iteration 187 : 0.45930864720298065
Loss in iteration 188 : 0.4599912115375937
Loss in iteration 189 : 0.46204844452773186
Loss in iteration 190 : 0.4652221881733389
Loss in iteration 191 : 0.47112433404025517
Loss in iteration 192 : 0.4833080236503625
Loss in iteration 193 : 0.5040118515385742
Loss in iteration 194 : 0.5410596500653883
Loss in iteration 195 : 0.5666498063378288
Loss in iteration 196 : 0.5783450280673919
Loss in iteration 197 : 0.5299913711436862
Loss in iteration 198 : 0.4817782761218239
Loss in iteration 199 : 0.46166692747222765
Loss in iteration 200 : 0.4767100949173322
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.752, training accuracy 0.759375, time elapsed: 4430 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.9175282647014122
Loss in iteration 3 : 1.2936045149065807
Loss in iteration 4 : 0.8032809541244761
Loss in iteration 5 : 0.5511936679746665
Loss in iteration 6 : 0.8712153059651921
Loss in iteration 7 : 0.5588387781605177
Loss in iteration 8 : 0.6180993705070149
Loss in iteration 9 : 0.5940728818847876
Loss in iteration 10 : 0.5743842174430969
Loss in iteration 11 : 0.5808385345594084
Loss in iteration 12 : 0.5737746970836025
Loss in iteration 13 : 0.5591448807101479
Loss in iteration 14 : 0.5862293125623469
Loss in iteration 15 : 0.5391961756067193
Loss in iteration 16 : 0.5837213477108008
Loss in iteration 17 : 0.5259603250841001
Loss in iteration 18 : 0.569023625564426
Loss in iteration 19 : 0.5226655809465488
Loss in iteration 20 : 0.5515314476236715
Loss in iteration 21 : 0.5219541325544383
Loss in iteration 22 : 0.5312556634879405
Loss in iteration 23 : 0.5158651377447951
Loss in iteration 24 : 0.511443227606837
Loss in iteration 25 : 0.5079703865203226
Loss in iteration 26 : 0.4982419548028605
Loss in iteration 27 : 0.49911539565490487
Loss in iteration 28 : 0.48754858224836606
Loss in iteration 29 : 0.49071309381165273
Loss in iteration 30 : 0.48136710587172704
Loss in iteration 31 : 0.48438061753837286
Loss in iteration 32 : 0.47840334185622857
Loss in iteration 33 : 0.48017067627533266
Loss in iteration 34 : 0.47726764522239534
Loss in iteration 35 : 0.47552523398619007
Loss in iteration 36 : 0.47666596564208746
Loss in iteration 37 : 0.473058099743163
Loss in iteration 38 : 0.4770524230179495
Loss in iteration 39 : 0.4716056537287126
Loss in iteration 40 : 0.4754983711727598
Loss in iteration 41 : 0.4710282895294412
Loss in iteration 42 : 0.4725980675156056
Loss in iteration 43 : 0.4705807832170738
Loss in iteration 44 : 0.4695253535757718
Loss in iteration 45 : 0.4691114937649103
Loss in iteration 46 : 0.46724126521301007
Loss in iteration 47 : 0.46718739297085604
Loss in iteration 48 : 0.46536427284741116
Loss in iteration 49 : 0.4650192275610576
Loss in iteration 50 : 0.46368754658667716
Loss in iteration 51 : 0.4636348640297845
Loss in iteration 52 : 0.462547892749747
Loss in iteration 53 : 0.46267222964765126
Loss in iteration 54 : 0.46173525471975796
Loss in iteration 55 : 0.46173397464921845
Loss in iteration 56 : 0.4615974173774489
Loss in iteration 57 : 0.46110295718788025
Loss in iteration 58 : 0.4616631457432837
Loss in iteration 59 : 0.4607379186330982
Loss in iteration 60 : 0.46149535034166084
Loss in iteration 61 : 0.4607672715991483
Loss in iteration 62 : 0.46103011633935
Loss in iteration 63 : 0.4609032337050142
Loss in iteration 64 : 0.4604893810814359
Loss in iteration 65 : 0.4608684553988652
Loss in iteration 66 : 0.46025236656331897
Loss in iteration 67 : 0.46057071579532916
Loss in iteration 68 : 0.46021293958438414
Loss in iteration 69 : 0.4601488117748242
Loss in iteration 70 : 0.4601741417530848
Loss in iteration 71 : 0.45992517882109035
Loss in iteration 72 : 0.4600340950144088
Loss in iteration 73 : 0.4598610359841267
Loss in iteration 74 : 0.4598923753270259
Loss in iteration 75 : 0.4598466270583885
Loss in iteration 76 : 0.4598323624053065
Loss in iteration 77 : 0.45981822249726706
Loss in iteration 78 : 0.45976593093003887
Loss in iteration 79 : 0.4597995617172126
Loss in iteration 80 : 0.4597083836795805
Loss in iteration 81 : 0.45974510042497174
Loss in iteration 82 : 0.4596890463539365
Loss in iteration 83 : 0.45963468559847037
Loss in iteration 84 : 0.4596670230222618
Loss in iteration 85 : 0.4595660802952738
Loss in iteration 86 : 0.45957788693020357
Loss in iteration 87 : 0.45954346164868626
Loss in iteration 88 : 0.4594815020684756
Loss in iteration 89 : 0.4595039763048796
Loss in iteration 90 : 0.4594448952012034
Loss in iteration 91 : 0.4594343792746903
Loss in iteration 92 : 0.45942528908557345
Loss in iteration 93 : 0.45938848464591897
Loss in iteration 94 : 0.4593913620254617
Loss in iteration 95 : 0.4593673053892993
Loss in iteration 96 : 0.45935657832257
Loss in iteration 97 : 0.45934616387158866
Loss in iteration 98 : 0.4593328684118295
Loss in iteration 99 : 0.45932679200609694
Loss in iteration 100 : 0.45930947668450417
Loss in iteration 101 : 0.45930631242573366
Loss in iteration 102 : 0.4592911648179626
Loss in iteration 103 : 0.45928090258359117
Loss in iteration 104 : 0.4592769906230601
Loss in iteration 105 : 0.4592585076303248
Loss in iteration 106 : 0.45925546720228555
Loss in iteration 107 : 0.4592457659204129
Loss in iteration 108 : 0.4592327009327431
Loss in iteration 109 : 0.45923043480633874
Loss in iteration 110 : 0.45921804465576266
Loss in iteration 111 : 0.4592110541309306
Loss in iteration 112 : 0.4592065023916575
Loss in iteration 113 : 0.4591965732513177
Loss in iteration 114 : 0.45919202210671295
Loss in iteration 115 : 0.45918562597886853
Loss in iteration 116 : 0.45917902899644997
Loss in iteration 117 : 0.4591741586161599
Loss in iteration 118 : 0.45916774283139855
Loss in iteration 119 : 0.45916278090295637
Loss in iteration 120 : 0.45915696090161556
Loss in iteration 121 : 0.45915169424358726
Loss in iteration 122 : 0.4591471351611079
Loss in iteration 123 : 0.4591410338736076
Loss in iteration 124 : 0.45913676865083614
Loss in iteration 125 : 0.4591321327301271
Loss in iteration 126 : 0.45912664578923323
Loss in iteration 127 : 0.4591230216480578
Loss in iteration 128 : 0.459118434860823
Loss in iteration 129 : 0.45911390226311327
Loss in iteration 130 : 0.4591106307110663
Loss in iteration 131 : 0.459106417864592
Loss in iteration 132 : 0.4591026657587791
Loss in iteration 133 : 0.45909946936405777
Loss in iteration 134 : 0.4590957773837413
Loss in iteration 135 : 0.4590925624278009
Loss in iteration 136 : 0.4590894114910373
Loss in iteration 137 : 0.45908621521568016
Loss in iteration 138 : 0.45908331351626797
Loss in iteration 139 : 0.4590803006985487
Loss in iteration 140 : 0.45907751737398567
Loss in iteration 141 : 0.4590748068770862
Loss in iteration 142 : 0.45907200469289355
Loss in iteration 143 : 0.4590695510219112
Loss in iteration 144 : 0.45906704534642095
Loss in iteration 145 : 0.45906450429475437
Loss in iteration 146 : 0.4590622897497749
Loss in iteration 147 : 0.4590600019189509
Loss in iteration 148 : 0.45905773058950256
Loss in iteration 149 : 0.4590556964409579
Loss in iteration 150 : 0.4590536168245619
Loss in iteration 151 : 0.4590515902041418
Loss in iteration 152 : 0.4590497138690563
Loss in iteration 153 : 0.4590478304460651
Testing accuracy  of updater 6 on alg 0 with rate 1.4000000000000001 = 0.789, training accuracy 0.7895, time elapsed: 2852 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6851251935256075
Loss in iteration 3 : 0.647258912282221
Loss in iteration 4 : 0.5848011960139812
Loss in iteration 5 : 0.5489605992610048
Loss in iteration 6 : 0.5327958443560716
Loss in iteration 7 : 0.505857602863537
Loss in iteration 8 : 0.5025859463288526
Loss in iteration 9 : 0.4900335530691053
Loss in iteration 10 : 0.49090038597450464
Loss in iteration 11 : 0.48532348412551696
Loss in iteration 12 : 0.4885941756983732
Loss in iteration 13 : 0.48520216294689533
Loss in iteration 14 : 0.4880366316934735
Loss in iteration 15 : 0.48493444480714437
Loss in iteration 16 : 0.4866622234507915
Loss in iteration 17 : 0.4849892691128341
Loss in iteration 18 : 0.485061865785475
Loss in iteration 19 : 0.4841435899920464
Loss in iteration 20 : 0.48223480630743404
Loss in iteration 21 : 0.4814608064414987
Loss in iteration 22 : 0.47864590404187346
Loss in iteration 23 : 0.4778671007623719
Loss in iteration 24 : 0.4752250365793217
Loss in iteration 25 : 0.4741269249768581
Loss in iteration 26 : 0.4719041219478675
Loss in iteration 27 : 0.4704496289243507
Loss in iteration 28 : 0.4689845835096161
Loss in iteration 29 : 0.46764224705522267
Loss in iteration 30 : 0.46691754782717804
Loss in iteration 31 : 0.4657803291344804
Loss in iteration 32 : 0.4654499195379169
Loss in iteration 33 : 0.46452125283051415
Loss in iteration 34 : 0.4644463860056911
Loss in iteration 35 : 0.4638858341503635
Loss in iteration 36 : 0.4640319766156635
Loss in iteration 37 : 0.4637173643150566
Loss in iteration 38 : 0.46387409598884743
Loss in iteration 39 : 0.4636712699725031
Loss in iteration 40 : 0.463819517929717
Loss in iteration 41 : 0.463697384730083
Loss in iteration 42 : 0.46378351973324045
Loss in iteration 43 : 0.4636312528229159
Loss in iteration 44 : 0.4635783639522181
Loss in iteration 45 : 0.4633655731167163
Loss in iteration 46 : 0.46322843694553917
Loss in iteration 47 : 0.46301281704898234
Loss in iteration 48 : 0.462823997068978
Loss in iteration 49 : 0.4625880769474801
Loss in iteration 50 : 0.4623584195750596
Loss in iteration 51 : 0.46213348457939374
Loss in iteration 52 : 0.46191562356596844
Loss in iteration 53 : 0.4617270334389086
Loss in iteration 54 : 0.46153246379294166
Loss in iteration 55 : 0.4613750016271388
Loss in iteration 56 : 0.4612103630921139
Loss in iteration 57 : 0.4610997031387403
Loss in iteration 58 : 0.46098400098816156
Loss in iteration 59 : 0.46091616318254625
Loss in iteration 60 : 0.4608315358019569
Loss in iteration 61 : 0.46078576221720163
Loss in iteration 62 : 0.4607229967553509
Loss in iteration 63 : 0.46069308401417347
Loss in iteration 64 : 0.4606461153802333
Loss in iteration 65 : 0.46062115591172176
Loss in iteration 66 : 0.4605767219683878
Loss in iteration 67 : 0.46054800382784356
Loss in iteration 68 : 0.4605046637544245
Loss in iteration 69 : 0.46047489841496847
Loss in iteration 70 : 0.4604313611205157
Loss in iteration 71 : 0.4603971011815001
Loss in iteration 72 : 0.46035106964641104
Loss in iteration 73 : 0.4603141490938942
Loss in iteration 74 : 0.4602698637910245
Loss in iteration 75 : 0.4602350207930105
Loss in iteration 76 : 0.4601949383230758
Loss in iteration 77 : 0.46016256399317007
Loss in iteration 78 : 0.46012705201721193
Loss in iteration 79 : 0.4600991967975052
Loss in iteration 80 : 0.4600697985321469
Loss in iteration 81 : 0.46004627511721913
Loss in iteration 82 : 0.46002104589216464
Loss in iteration 83 : 0.45999996077687455
Loss in iteration 84 : 0.45997756911169235
Loss in iteration 85 : 0.459958537910693
Loss in iteration 86 : 0.45993854096620174
Loss in iteration 87 : 0.45992042102533537
Loss in iteration 88 : 0.45990115702345413
Loss in iteration 89 : 0.4598829561795633
Loss in iteration 90 : 0.4598641828110101
Loss in iteration 91 : 0.459845973279572
Loss in iteration 92 : 0.4598274699802782
Loss in iteration 93 : 0.4598090304217148
Loss in iteration 94 : 0.4597906367254518
Loss in iteration 95 : 0.45977231591011436
Loss in iteration 96 : 0.45975458881852044
Loss in iteration 97 : 0.45973691619278234
Loss in iteration 98 : 0.45971992797602307
Loss in iteration 99 : 0.4597029041511423
Loss in iteration 100 : 0.45968673524084713
Loss in iteration 101 : 0.4596706086751889
Loss in iteration 102 : 0.4596554056113343
Loss in iteration 103 : 0.4596402108078114
Loss in iteration 104 : 0.459625854385677
Loss in iteration 105 : 0.4596114956848016
Loss in iteration 106 : 0.459597960269562
Loss in iteration 107 : 0.45958446622926147
Loss in iteration 108 : 0.45957166273570926
Loss in iteration 109 : 0.4595588563958265
Loss in iteration 110 : 0.45954660712055967
Loss in iteration 111 : 0.45953439512934996
Loss in iteration 112 : 0.45952265686634763
Loss in iteration 113 : 0.45951100174116843
Loss in iteration 114 : 0.45949971286799207
Loss in iteration 115 : 0.4594885335075031
Loss in iteration 116 : 0.4594776551118462
Loss in iteration 117 : 0.4594669515847326
Loss in iteration 118 : 0.45945650084451584
Loss in iteration 119 : 0.45944625026032454
Loss in iteration 120 : 0.459436203517784
Loss in iteration 121 : 0.45942638541486625
Loss in iteration 122 : 0.4594167557323437
Loss in iteration 123 : 0.4594073813502976
Loss in iteration 124 : 0.45939817909108815
Loss in iteration 125 : 0.45938922796620746
Loss in iteration 126 : 0.45938043324454736
Loss in iteration 127 : 0.4593718870687513
Loss in iteration 128 : 0.4593634933384948
Loss in iteration 129 : 0.4593553309290211
Loss in iteration 130 : 0.4593473111609812
Loss in iteration 131 : 0.45933949984767675
Loss in iteration 132 : 0.45933182959972474
Loss in iteration 133 : 0.4593243502489135
Loss in iteration 134 : 0.45931701353975063
Loss in iteration 135 : 0.45930984539915337
Loss in iteration 136 : 0.4593028177895924
Loss in iteration 137 : 0.4592959421700879
Loss in iteration 138 : 0.45928921003288464
Loss in iteration 139 : 0.4592826178929385
Loss in iteration 140 : 0.4592761689638965
Loss in iteration 141 : 0.45926985006064935
Loss in iteration 142 : 0.4592636727002379
Loss in iteration 143 : 0.4592576205705896
Loss in iteration 144 : 0.45925170765831064
Loss in iteration 145 : 0.4592459156169914
Loss in iteration 146 : 0.4592402559794347
Loss in iteration 147 : 0.45923471370990526
Loss in iteration 148 : 0.45922929722590855
Loss in iteration 149 : 0.45922399587923507
Loss in iteration 150 : 0.45921881286372535
Loss in iteration 151 : 0.4592137417865132
Loss in iteration 152 : 0.4592087813412682
Loss in iteration 153 : 0.45920392976766816
Loss in iteration 154 : 0.4591991827495973
Loss in iteration 155 : 0.45919454093624695
Loss in iteration 156 : 0.4591899978504321
Loss in iteration 157 : 0.4591855555865559
Loss in iteration 158 : 0.45918120766441495
Loss in iteration 159 : 0.45917695635538064
Loss in iteration 160 : 0.4591727959395084
Loss in iteration 161 : 0.45916872755001753
Loss in iteration 162 : 0.4591647467470551
Loss in iteration 163 : 0.4591608534934819
Loss in iteration 164 : 0.45915704502760063
Loss in iteration 165 : 0.4591533199565398
Loss in iteration 166 : 0.459149676622595
Loss in iteration 167 : 0.45914611271929084
Loss in iteration 168 : 0.45914262742853984
Loss in iteration 169 : 0.4591392181322692
Loss in iteration 170 : 0.45913588427910873
Loss in iteration 171 : 0.4591326232446627
Loss in iteration 172 : 0.4591294342533289
Loss in iteration 173 : 0.4591263151067869
Loss in iteration 174 : 0.4591232647416435
Loss in iteration 175 : 0.4591202814616919
Loss in iteration 176 : 0.45911736379488166
Loss in iteration 177 : 0.4591145104676118
Loss in iteration 178 : 0.4591117197848815
Loss in iteration 179 : 0.45910899077793815
Loss in iteration 180 : 0.45910632172034466
Loss in iteration 181 : 0.459103711698552
Testing accuracy  of updater 6 on alg 0 with rate 0.8 = 0.7865, training accuracy 0.7895, time elapsed: 3301 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6776177557761789
Loss in iteration 3 : 0.6649129567543532
Loss in iteration 4 : 0.6436615312842168
Loss in iteration 5 : 0.6223954687125675
Loss in iteration 6 : 0.6048946291391054
Loss in iteration 7 : 0.5827159981274096
Loss in iteration 8 : 0.5638903959031029
Loss in iteration 9 : 0.5498403921424678
Loss in iteration 10 : 0.5357808205764721
Loss in iteration 11 : 0.5249166386054961
Loss in iteration 12 : 0.5177024917045876
Loss in iteration 13 : 0.5103011241946886
Loss in iteration 14 : 0.5036388264345345
Loss in iteration 15 : 0.49944551034976337
Loss in iteration 16 : 0.49588718242376045
Loss in iteration 17 : 0.4921369415100162
Loss in iteration 18 : 0.489572893816229
Loss in iteration 19 : 0.4881140096847439
Loss in iteration 20 : 0.48644971761367867
Loss in iteration 21 : 0.4847052051867058
Loss in iteration 22 : 0.483641439832451
Loss in iteration 23 : 0.48285372489130785
Loss in iteration 24 : 0.4817162252390811
Loss in iteration 25 : 0.48058187528162716
Loss in iteration 26 : 0.47987815706185766
Loss in iteration 27 : 0.4792753156571597
Loss in iteration 28 : 0.4784637540806533
Loss in iteration 29 : 0.47771156255631975
Loss in iteration 30 : 0.4772041086001443
Loss in iteration 31 : 0.4766747043124861
Loss in iteration 32 : 0.4759654232011967
Loss in iteration 33 : 0.4752843932295313
Loss in iteration 34 : 0.47474158459670035
Loss in iteration 35 : 0.47416773862203654
Loss in iteration 36 : 0.47349451843468043
Loss in iteration 37 : 0.47287390136369206
Loss in iteration 38 : 0.47236151626234696
Loss in iteration 39 : 0.4718371797516231
Loss in iteration 40 : 0.4712644998152999
Loss in iteration 41 : 0.47074076964876815
Loss in iteration 42 : 0.47028620854209047
Loss in iteration 43 : 0.46981880146941457
Loss in iteration 44 : 0.4693284640768321
Loss in iteration 45 : 0.4688860370721826
Loss in iteration 46 : 0.468500148132696
Loss in iteration 47 : 0.4681178653598045
Loss in iteration 48 : 0.4677384488819968
Loss in iteration 49 : 0.4674036878129374
Loss in iteration 50 : 0.46710758934462226
Loss in iteration 51 : 0.46681307940327416
Loss in iteration 52 : 0.46652463975535124
Loss in iteration 53 : 0.46626853523632805
Loss in iteration 54 : 0.4660363357191683
Loss in iteration 55 : 0.46580670312410277
Loss in iteration 56 : 0.46558827828341
Loss in iteration 57 : 0.4653967803778593
Loss in iteration 58 : 0.4652225186740347
Loss in iteration 59 : 0.4650526309448639
Loss in iteration 60 : 0.4648945671865314
Loss in iteration 61 : 0.46475505146197366
Loss in iteration 62 : 0.46462427417934454
Loss in iteration 63 : 0.4644957452038984
Loss in iteration 64 : 0.4643766005675027
Loss in iteration 65 : 0.46426981729879957
Loss in iteration 66 : 0.4641681299893781
Loss in iteration 67 : 0.4640692474447615
Loss in iteration 68 : 0.46397836974119233
Loss in iteration 69 : 0.46389527279623566
Loss in iteration 70 : 0.4638143375204583
Loss in iteration 71 : 0.4637352867557329
Loss in iteration 72 : 0.4636615918156143
Loss in iteration 73 : 0.4635919741751275
Loss in iteration 74 : 0.4635230686557346
Loss in iteration 75 : 0.46345595796261585
Loss in iteration 76 : 0.46339289521823024
Loss in iteration 77 : 0.4633323901604326
Loss in iteration 78 : 0.4632725937003197
Loss in iteration 79 : 0.46321470632012046
Loss in iteration 80 : 0.4631596557649989
Loss in iteration 81 : 0.46310590808156543
Loss in iteration 82 : 0.463052606017247
Loss in iteration 83 : 0.4630008807340809
Loss in iteration 84 : 0.46295107847329336
Loss in iteration 85 : 0.4629021302968341
Loss in iteration 86 : 0.46285388035825215
Loss in iteration 87 : 0.46280716885175194
Loss in iteration 88 : 0.4627619014127377
Loss in iteration 89 : 0.4627173134999816
Loss in iteration 90 : 0.4626735118933572
Loss in iteration 91 : 0.46263100397821055
Loss in iteration 92 : 0.46258952809960135
Loss in iteration 93 : 0.46254864818316355
Loss in iteration 94 : 0.46250861048543457
Loss in iteration 95 : 0.4624697053849644
Loss in iteration 96 : 0.46243166694286025
Loss in iteration 97 : 0.46239428493523543
Loss in iteration 98 : 0.4623577791874146
Loss in iteration 99 : 0.4623222324175196
Loss in iteration 100 : 0.4622873885518862
Loss in iteration 101 : 0.46225315961954505
Loss in iteration 102 : 0.4622197099595081
Loss in iteration 103 : 0.4621870328554011
Loss in iteration 104 : 0.46215495522027256
Loss in iteration 105 : 0.462123476302502
Loss in iteration 106 : 0.46209269953135257
Loss in iteration 107 : 0.46206256787729305
Loss in iteration 108 : 0.46203296632465013
Loss in iteration 109 : 0.4620039178861206
Loss in iteration 110 : 0.4619754656680427
Loss in iteration 111 : 0.46194753786242826
Loss in iteration 112 : 0.461920070458887
Loss in iteration 113 : 0.4618930947255404
Loss in iteration 114 : 0.4618666213756322
Loss in iteration 115 : 0.4618405910143207
Loss in iteration 116 : 0.4618149759960933
Loss in iteration 117 : 0.46178979910543044
Loss in iteration 118 : 0.461765046424838
Loss in iteration 119 : 0.4617406699096901
Loss in iteration 120 : 0.4617166590747674
Loss in iteration 121 : 0.4616930258135175
Loss in iteration 122 : 0.46166974957338575
Loss in iteration 123 : 0.46164680060479557
Loss in iteration 124 : 0.4616241795260695
Loss in iteration 125 : 0.46160188986801043
Loss in iteration 126 : 0.46157991088358646
Loss in iteration 127 : 0.4615582250666507
Loss in iteration 128 : 0.46153683454766536
Loss in iteration 129 : 0.46151573536180385
Loss in iteration 130 : 0.46149490947214517
Loss in iteration 131 : 0.4614743474995601
Loss in iteration 132 : 0.4614540507388297
Loss in iteration 133 : 0.4614340124082782
Loss in iteration 134 : 0.46141421976043395
Loss in iteration 135 : 0.46139466846199745
Loss in iteration 136 : 0.4613753575443472
Loss in iteration 137 : 0.4613562787478407
Loss in iteration 138 : 0.46133742291137697
Loss in iteration 139 : 0.4613187872620589
Loss in iteration 140 : 0.4613003687792254
Loss in iteration 141 : 0.4612821598816672
Loss in iteration 142 : 0.4612641547360227
Loss in iteration 143 : 0.46124635122223034
Loss in iteration 144 : 0.46122874532633873
Loss in iteration 145 : 0.46121133080507487
Loss in iteration 146 : 0.4611941037254201
Loss in iteration 147 : 0.46117706159724375
Loss in iteration 148 : 0.46116019989137974
Loss in iteration 149 : 0.4611435136093703
Loss in iteration 150 : 0.4611269997678579
Loss in iteration 151 : 0.46111065555539255
Loss in iteration 152 : 0.4610944767843273
Loss in iteration 153 : 0.4610784597511705
Loss in iteration 154 : 0.4610626019857704
Loss in iteration 155 : 0.4610469005074509
Loss in iteration 156 : 0.46103135163363995
Loss in iteration 157 : 0.461015952448386
Loss in iteration 158 : 0.4610007005957174
Loss in iteration 159 : 0.4609855931477093
Loss in iteration 160 : 0.46097062706797726
Loss in iteration 161 : 0.46095580001012065
Loss in iteration 162 : 0.4609411097246455
Loss in iteration 163 : 0.46092655355407436
Loss in iteration 164 : 0.4609121290365839
Loss in iteration 165 : 0.46089783414186103
Loss in iteration 166 : 0.46088366670231323
Loss in iteration 167 : 0.4608696243671731
Loss in iteration 168 : 0.46085570508000484
Loss in iteration 169 : 0.46084190698951455
Loss in iteration 170 : 0.4608282280854706
Loss in iteration 171 : 0.4608146663703988
Loss in iteration 172 : 0.4608012201039742
Loss in iteration 173 : 0.4607878875840852
Loss in iteration 174 : 0.4607746669975515
Loss in iteration 175 : 0.4607615566360095
Loss in iteration 176 : 0.46074855495527767
Loss in iteration 177 : 0.4607356603784237
Loss in iteration 178 : 0.46072287129962086
Loss in iteration 179 : 0.4607101862433913
Loss in iteration 180 : 0.46069760381465935
Loss in iteration 181 : 0.4606851225783704
Loss in iteration 182 : 0.46067274113011475
Loss in iteration 183 : 0.4606604581707776
Loss in iteration 184 : 0.46064827242312856
Loss in iteration 185 : 0.46063618259008277
Loss in iteration 186 : 0.46062418743206013
Loss in iteration 187 : 0.4606122857779227
Loss in iteration 188 : 0.46060047645772384
Loss in iteration 189 : 0.4605887583113198
Loss in iteration 190 : 0.4605771302382749
Loss in iteration 191 : 0.46056559117378015
Loss in iteration 192 : 0.46055414005241063
Loss in iteration 193 : 0.4605427758357423
Loss in iteration 194 : 0.46053149753147643
Loss in iteration 195 : 0.4605203041636038
Loss in iteration 196 : 0.46050919476416124
Loss in iteration 197 : 0.46049816839825897
Loss in iteration 198 : 0.4604872241616805
Loss in iteration 199 : 0.46047636115997737
Loss in iteration 200 : 0.4604655785154229
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.7865, training accuracy 0.78825, time elapsed: 3704 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6790356274608992
Loss in iteration 3 : 0.6673288507074603
Loss in iteration 4 : 0.6500418463014052
Loss in iteration 5 : 0.6291566146921654
Loss in iteration 6 : 0.6129164724535147
Loss in iteration 7 : 0.594864678416074
Loss in iteration 8 : 0.5751589918052039
Loss in iteration 9 : 0.5602405884295121
Loss in iteration 10 : 0.5476374128288055
Loss in iteration 11 : 0.5352394903809858
Loss in iteration 12 : 0.5258536152056646
Loss in iteration 13 : 0.5191630901111188
Loss in iteration 14 : 0.5124250036988048
Loss in iteration 15 : 0.5061658668427471
Loss in iteration 16 : 0.5018415983142523
Loss in iteration 17 : 0.4984648292604144
Loss in iteration 18 : 0.49493798150087553
Loss in iteration 19 : 0.49195537301315884
Loss in iteration 20 : 0.4900892692942656
Loss in iteration 21 : 0.48857971805464423
Loss in iteration 22 : 0.48684126589758503
Loss in iteration 23 : 0.4853111273952391
Loss in iteration 24 : 0.4843105373963955
Loss in iteration 25 : 0.4834132463509931
Loss in iteration 26 : 0.48229039675310537
Loss in iteration 27 : 0.4812228274795278
Loss in iteration 28 : 0.4804767850886228
Loss in iteration 29 : 0.47984751901268136
Loss in iteration 30 : 0.47909637736767297
Loss in iteration 31 : 0.47834431007022055
Loss in iteration 32 : 0.47776977011638616
Loss in iteration 33 : 0.4772718014506436
Loss in iteration 34 : 0.47667473755099354
Loss in iteration 35 : 0.4760230360961032
Loss in iteration 36 : 0.4754571510089612
Loss in iteration 37 : 0.4749596153926201
Loss in iteration 38 : 0.4744176346280329
Loss in iteration 39 : 0.47383435076214936
Loss in iteration 40 : 0.4733062924740893
Loss in iteration 41 : 0.4728485149380071
Loss in iteration 42 : 0.4723870805552362
Loss in iteration 43 : 0.47189881151037716
Loss in iteration 44 : 0.47143814100239134
Loss in iteration 45 : 0.4710292071946117
Loss in iteration 46 : 0.470629679809395
Loss in iteration 47 : 0.4702137560719526
Loss in iteration 48 : 0.4698133375754378
Loss in iteration 49 : 0.4694560235201392
Loss in iteration 50 : 0.4691221807747676
Loss in iteration 51 : 0.46878817224340324
Loss in iteration 52 : 0.4684665082872362
Loss in iteration 53 : 0.46817659348199697
Loss in iteration 54 : 0.46790892368111636
Loss in iteration 55 : 0.4676445356750113
Loss in iteration 56 : 0.46738672245488233
Loss in iteration 57 : 0.4671494472024291
Loss in iteration 58 : 0.46693048665339404
Loss in iteration 59 : 0.46671760314000504
Loss in iteration 60 : 0.4665107315473158
Loss in iteration 61 : 0.4663192918182847
Loss in iteration 62 : 0.46614382325691867
Loss in iteration 63 : 0.4659760722369406
Loss in iteration 64 : 0.46581377586602507
Loss in iteration 65 : 0.46566207485925304
Loss in iteration 66 : 0.46552182809050896
Loss in iteration 67 : 0.4653875980602838
Loss in iteration 68 : 0.46525708600010646
Loss in iteration 69 : 0.4651336276964223
Loss in iteration 70 : 0.46501868393545887
Loss in iteration 71 : 0.4649090686771415
Loss in iteration 72 : 0.46480274647414666
Loss in iteration 73 : 0.4647015197641526
Loss in iteration 74 : 0.46460650558222083
Loss in iteration 75 : 0.4645156066042619
Loss in iteration 76 : 0.46442704555680525
Loss in iteration 77 : 0.464341746053574
Loss in iteration 78 : 0.464260659011847
Loss in iteration 79 : 0.46418262066372756
Loss in iteration 80 : 0.46410637261323534
Loss in iteration 81 : 0.4640324621334417
Loss in iteration 82 : 0.46396171633579236
Loss in iteration 83 : 0.46389353983258264
Loss in iteration 84 : 0.46382699857288434
Loss in iteration 85 : 0.4637622731670776
Loss in iteration 86 : 0.4636998875484345
Loss in iteration 87 : 0.4636394837497059
Loss in iteration 88 : 0.46358037934661284
Loss in iteration 89 : 0.46352262108983355
Loss in iteration 90 : 0.4634666113871738
Loss in iteration 91 : 0.46341221685820766
Loss in iteration 92 : 0.46335899760441707
Loss in iteration 93 : 0.463306938377399
Loss in iteration 94 : 0.4632563056757592
Loss in iteration 95 : 0.46320703898516363
Loss in iteration 96 : 0.46315882522069435
Loss in iteration 97 : 0.4631116021928317
Loss in iteration 98 : 0.46306553477822354
Loss in iteration 99 : 0.46302060466624034
Loss in iteration 100 : 0.46297660426667675
Loss in iteration 101 : 0.46293347312185684
Loss in iteration 102 : 0.46289132535549504
Loss in iteration 103 : 0.46285017354342983
Loss in iteration 104 : 0.4628098829648567
Loss in iteration 105 : 0.4627703905355776
Loss in iteration 106 : 0.4627317556413451
Loss in iteration 107 : 0.4626939850387571
Loss in iteration 108 : 0.46265698091347107
Loss in iteration 109 : 0.46262068251252236
Loss in iteration 110 : 0.4625851201127584
Loss in iteration 111 : 0.462550303866584
Loss in iteration 112 : 0.46251617165542713
Loss in iteration 113 : 0.46248267501601176
Loss in iteration 114 : 0.46244982653925437
Loss in iteration 115 : 0.4624176326283028
Loss in iteration 116 : 0.46238605011421585
Loss in iteration 117 : 0.46235503773836917
Loss in iteration 118 : 0.4623245946602073
Loss in iteration 119 : 0.4622947214385954
Loss in iteration 120 : 0.4622653868707633
Loss in iteration 121 : 0.46223655807008823
Loss in iteration 122 : 0.4622082294351075
Loss in iteration 123 : 0.46218039997343296
Loss in iteration 124 : 0.462153048227735
Loss in iteration 125 : 0.46212614880023617
Loss in iteration 126 : 0.4620996931889194
Loss in iteration 127 : 0.46207367730118815
Loss in iteration 128 : 0.4620480839329717
Loss in iteration 129 : 0.46202289223213183
Loss in iteration 130 : 0.46199809267775005
Loss in iteration 131 : 0.46197368028521624
Loss in iteration 132 : 0.46194964209123335
Loss in iteration 133 : 0.46192596201738145
Loss in iteration 134 : 0.46190263123756053
Loss in iteration 135 : 0.46187964462932346
Loss in iteration 136 : 0.461856992032496
Loss in iteration 137 : 0.4618346606122225
Loss in iteration 138 : 0.4618126420365426
Loss in iteration 139 : 0.4617909308702371
Loss in iteration 140 : 0.4617695186468565
Loss in iteration 141 : 0.4617483949201508
Loss in iteration 142 : 0.4617275521873371
Loss in iteration 143 : 0.46170698529430076
Loss in iteration 144 : 0.4616866873810774
Loss in iteration 145 : 0.46166665010995084
Loss in iteration 146 : 0.46164686689146994
Loss in iteration 147 : 0.46162733278879864
Loss in iteration 148 : 0.46160804187073173
Loss in iteration 149 : 0.46158898716645236
Loss in iteration 150 : 0.46157016280613317
Loss in iteration 151 : 0.4615515641484741
Loss in iteration 152 : 0.46153318605832294
Loss in iteration 153 : 0.46151502273366485
Loss in iteration 154 : 0.46149706907601923
Loss in iteration 155 : 0.461479320859988
Loss in iteration 156 : 0.46146177361566326
Loss in iteration 157 : 0.46144442243719774
Loss in iteration 158 : 0.461427262855788
Loss in iteration 159 : 0.46141029099828185
Loss in iteration 160 : 0.4613935028761782
Loss in iteration 161 : 0.46137689423864886
Loss in iteration 162 : 0.46136046114518714
Loss in iteration 163 : 0.46134420009685945
Loss in iteration 164 : 0.4613281075721489
Loss in iteration 165 : 0.4613121799033681
Loss in iteration 166 : 0.46129641363793356
Loss in iteration 167 : 0.4612808056321453
Loss in iteration 168 : 0.4612653527504401
Loss in iteration 169 : 0.46125005177837364
Loss in iteration 170 : 0.461234899660214
Loss in iteration 171 : 0.46121989356845855
Loss in iteration 172 : 0.46120503070844693
Loss in iteration 173 : 0.4611903082560859
Loss in iteration 174 : 0.46117572351126157
Loss in iteration 175 : 0.46116127394511225
Loss in iteration 176 : 0.4611469570714363
Loss in iteration 177 : 0.46113277040230605
Loss in iteration 178 : 0.46111871154585565
Loss in iteration 179 : 0.46110477823649226
Loss in iteration 180 : 0.46109096825089857
Loss in iteration 181 : 0.4610772793800319
Loss in iteration 182 : 0.4610637094943011
Loss in iteration 183 : 0.46105025656325666
Loss in iteration 184 : 0.4610369185998453
Loss in iteration 185 : 0.46102369364175694
Loss in iteration 186 : 0.4610105797932881
Loss in iteration 187 : 0.46099757523621326
Loss in iteration 188 : 0.46098467819190847
Loss in iteration 189 : 0.4609718869097123
Loss in iteration 190 : 0.46095919969495425
Loss in iteration 191 : 0.46094661491510125
Loss in iteration 192 : 0.46093413097398367
Loss in iteration 193 : 0.46092174630471006
Loss in iteration 194 : 0.46090945938864525
Loss in iteration 195 : 0.46089726875864007
Loss in iteration 196 : 0.46088517298102283
Loss in iteration 197 : 0.4608731706508084
Loss in iteration 198 : 0.4608612604041117
Loss in iteration 199 : 0.4608494409194537
Loss in iteration 200 : 0.46083771090524467
Testing accuracy  of updater 6 on alg 0 with rate 0.14 = 0.786, training accuracy 0.78825, time elapsed: 3934 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.682613830926152
Loss in iteration 3 : 0.6722786829994615
Loss in iteration 4 : 0.6618040263006462
Loss in iteration 5 : 0.6467552698354565
Loss in iteration 6 : 0.6305635413591303
Loss in iteration 7 : 0.6172102552675128
Loss in iteration 8 : 0.6040942799597111
Loss in iteration 9 : 0.5890372771356424
Loss in iteration 10 : 0.5748286833557261
Loss in iteration 11 : 0.5635258797956966
Loss in iteration 12 : 0.5536523256470557
Loss in iteration 13 : 0.5439712098599001
Loss in iteration 14 : 0.5354246320477676
Loss in iteration 15 : 0.5288444510788938
Loss in iteration 16 : 0.5233854470005541
Loss in iteration 17 : 0.5179994050218335
Loss in iteration 18 : 0.5128889577243282
Loss in iteration 19 : 0.5087423890469385
Loss in iteration 20 : 0.5054968008331384
Loss in iteration 21 : 0.5025355858517053
Loss in iteration 22 : 0.4996203552365148
Loss in iteration 23 : 0.4970604555922991
Loss in iteration 24 : 0.4950906648884136
Loss in iteration 25 : 0.4934990834009494
Loss in iteration 26 : 0.4919481609923276
Loss in iteration 27 : 0.4903957032136232
Loss in iteration 28 : 0.4890339241020251
Loss in iteration 29 : 0.48794284751426836
Loss in iteration 30 : 0.4869784647795737
Loss in iteration 31 : 0.4859845764929921
Loss in iteration 32 : 0.4849805231138304
Loss in iteration 33 : 0.48408932522908726
Loss in iteration 34 : 0.4833523582156182
Loss in iteration 35 : 0.4826872365801554
Loss in iteration 36 : 0.4820101827147392
Loss in iteration 37 : 0.4813334370877344
Loss in iteration 38 : 0.4807209156898432
Loss in iteration 39 : 0.48018837669043724
Loss in iteration 40 : 0.4796853098092454
Loss in iteration 41 : 0.47916466580543693
Loss in iteration 42 : 0.47863594931107206
Loss in iteration 43 : 0.4781391557101806
Loss in iteration 44 : 0.47768731988046936
Loss in iteration 45 : 0.4772545648682506
Loss in iteration 46 : 0.4768151974737043
Loss in iteration 47 : 0.47637527354666453
Loss in iteration 48 : 0.475958849898971
Loss in iteration 49 : 0.475574530167299
Loss in iteration 50 : 0.47520726732127305
Loss in iteration 51 : 0.4748405687737132
Loss in iteration 52 : 0.47447597167466643
Loss in iteration 53 : 0.4741265522706257
Loss in iteration 54 : 0.47379761889678246
Loss in iteration 55 : 0.473480885920136
Loss in iteration 56 : 0.4731667585422418
Loss in iteration 57 : 0.47285615029831013
Loss in iteration 58 : 0.4725573242028348
Loss in iteration 59 : 0.47227439369004515
Loss in iteration 60 : 0.47200307815351095
Loss in iteration 61 : 0.47173742782116274
Loss in iteration 62 : 0.47147712521916074
Loss in iteration 63 : 0.4712263380967794
Loss in iteration 64 : 0.47098722949587135
Loss in iteration 65 : 0.4707571748188177
Loss in iteration 66 : 0.4705324173840243
Loss in iteration 67 : 0.47031244151087387
Loss in iteration 68 : 0.47009956140086
Loss in iteration 69 : 0.4698951743114726
Loss in iteration 70 : 0.4696979408963488
Loss in iteration 71 : 0.4695057419100406
Loss in iteration 72 : 0.4693182391683888
Loss in iteration 73 : 0.4691367449310175
Loss in iteration 74 : 0.4689620723676169
Loss in iteration 75 : 0.4687934065708597
Loss in iteration 76 : 0.46862938771242085
Loss in iteration 77 : 0.46846963403315267
Loss in iteration 78 : 0.4683147386129824
Loss in iteration 79 : 0.46816505623046895
Loss in iteration 80 : 0.46802003979796614
Loss in iteration 81 : 0.46787885550831493
Loss in iteration 82 : 0.4677412642080839
Loss in iteration 83 : 0.467607613631978
Loss in iteration 84 : 0.4674781144338473
Loss in iteration 85 : 0.4673524342706968
Loss in iteration 86 : 0.4672300517462196
Loss in iteration 87 : 0.4671107802908573
Loss in iteration 88 : 0.4669947731265095
Loss in iteration 89 : 0.4668821021110587
Loss in iteration 90 : 0.4667725207037898
Loss in iteration 91 : 0.4666656757740682
Loss in iteration 92 : 0.4665614201461074
Loss in iteration 93 : 0.4664598164148766
Loss in iteration 94 : 0.466360886028357
Loss in iteration 95 : 0.4662644664502116
Loss in iteration 96 : 0.4661703352276787
Loss in iteration 97 : 0.4660783946225198
Loss in iteration 98 : 0.4659886722998405
Loss in iteration 99 : 0.4659011695865895
Loss in iteration 100 : 0.46581577427227366
Loss in iteration 101 : 0.4657323339459082
Loss in iteration 102 : 0.4656507673338934
Loss in iteration 103 : 0.4655710672284439
Loss in iteration 104 : 0.4654932125803418
Loss in iteration 105 : 0.4654171180534817
Loss in iteration 106 : 0.46534267792749867
Loss in iteration 107 : 0.46526983167047875
Loss in iteration 108 : 0.46519856455137043
Loss in iteration 109 : 0.465128854296622
Loss in iteration 110 : 0.4650606404445505
Loss in iteration 111 : 0.4649938501375166
Loss in iteration 112 : 0.464928436871925
Loss in iteration 113 : 0.4648643807211645
Loss in iteration 114 : 0.4648016568222758
Loss in iteration 115 : 0.46474021769116475
Loss in iteration 116 : 0.4646800090343981
Loss in iteration 117 : 0.4646209928694241
Loss in iteration 118 : 0.4645631474631977
Loss in iteration 119 : 0.4645064485369018
Loss in iteration 120 : 0.4644508589920173
Loss in iteration 121 : 0.4643963385521206
Loss in iteration 122 : 0.46434285737578307
Loss in iteration 123 : 0.4642903955471136
Loss in iteration 124 : 0.4642389314841541
Loss in iteration 125 : 0.4641884357531143
Loss in iteration 126 : 0.4641388769622255
Loss in iteration 127 : 0.46409022998536303
Loss in iteration 128 : 0.46404247566779
Loss in iteration 129 : 0.4639955939532178
Loss in iteration 130 : 0.46394956033975115
Loss in iteration 131 : 0.46390434954477505
Loss in iteration 132 : 0.46385994042480905
Loss in iteration 133 : 0.4638163156842122
Loss in iteration 134 : 0.463773457657847
Loss in iteration 135 : 0.46373134616364065
Loss in iteration 136 : 0.4636899607030206
Loss in iteration 137 : 0.4636492833841208
Loss in iteration 138 : 0.46360929870559753
Loss in iteration 139 : 0.4635699910182925
Loss in iteration 140 : 0.46353134326227813
Loss in iteration 141 : 0.46349333830287764
Loss in iteration 142 : 0.4634559606714409
Loss in iteration 143 : 0.4634191964261186
Loss in iteration 144 : 0.4633830316561448
Loss in iteration 145 : 0.46334745176892916
Loss in iteration 146 : 0.46331244230437596
Loss in iteration 147 : 0.46327798994447916
Loss in iteration 148 : 0.463244082384566
Loss in iteration 149 : 0.4632107074203625
Loss in iteration 150 : 0.4631778525299593
Loss in iteration 151 : 0.4631455053635749
Loss in iteration 152 : 0.46311365432967383
Loss in iteration 153 : 0.4630822885016098
Loss in iteration 154 : 0.4630513970815473
Loss in iteration 155 : 0.4630209691784155
Loss in iteration 156 : 0.46299099411571865
Loss in iteration 157 : 0.46296146176981673
Loss in iteration 158 : 0.4629323624929062
Loss in iteration 159 : 0.4629036867864528
Loss in iteration 160 : 0.4628754251777655
Loss in iteration 161 : 0.46284756840922375
Loss in iteration 162 : 0.4628201076303772
Loss in iteration 163 : 0.46279303433662583
Loss in iteration 164 : 0.46276634016917145
Loss in iteration 165 : 0.4627400168475129
Loss in iteration 166 : 0.4627140562874623
Loss in iteration 167 : 0.46268845071193837
Loss in iteration 168 : 0.46266319260722183
Loss in iteration 169 : 0.462638274602446
Loss in iteration 170 : 0.46261368943420395
Loss in iteration 171 : 0.46258943001971625
Loss in iteration 172 : 0.4625654895188091
Loss in iteration 173 : 0.46254186130093317
Loss in iteration 174 : 0.4625185388700187
Loss in iteration 175 : 0.462495515844169
Loss in iteration 176 : 0.46247278599961
Loss in iteration 177 : 0.4624503433049021
Loss in iteration 178 : 0.46242818189791945
Loss in iteration 179 : 0.4624062960402943
Loss in iteration 180 : 0.4623846801065115
Loss in iteration 181 : 0.46236332861011364
Loss in iteration 182 : 0.4623422362217764
Loss in iteration 183 : 0.4623213977530704
Loss in iteration 184 : 0.46230080812865515
Loss in iteration 185 : 0.46228046238028664
Loss in iteration 186 : 0.46226035566190443
Loss in iteration 187 : 0.46224048325828637
Loss in iteration 188 : 0.4622208405732392
Loss in iteration 189 : 0.4622014231122147
Loss in iteration 190 : 0.462182226478653
Loss in iteration 191 : 0.4621632463822461
Loss in iteration 192 : 0.46214447864253344
Loss in iteration 193 : 0.46212591918060647
Loss in iteration 194 : 0.4621075640084996
Loss in iteration 195 : 0.46208940922713587
Loss in iteration 196 : 0.46207145103069225
Loss in iteration 197 : 0.4620536857074455
Loss in iteration 198 : 0.46203610963361763
Loss in iteration 199 : 0.46201871926648513
Loss in iteration 200 : 0.4620015111428382
Testing accuracy  of updater 6 on alg 0 with rate 0.08000000000000002 = 0.786, training accuracy 0.789875, time elapsed: 4218 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6896497249660131
Loss in iteration 3 : 0.6843096891947236
Loss in iteration 4 : 0.6787003215030202
Loss in iteration 5 : 0.6735308573518309
Loss in iteration 6 : 0.6685536843226944
Loss in iteration 7 : 0.6631237003868614
Loss in iteration 8 : 0.6568424208935078
Loss in iteration 9 : 0.6498406746380108
Loss in iteration 10 : 0.6426319093043287
Loss in iteration 11 : 0.6357437049656235
Loss in iteration 12 : 0.6294079848619459
Loss in iteration 13 : 0.6234948743735451
Loss in iteration 14 : 0.6176880113719769
Loss in iteration 15 : 0.6117416746027518
Loss in iteration 16 : 0.6056369702495681
Loss in iteration 17 : 0.5995599942627615
Loss in iteration 18 : 0.5937575043763211
Loss in iteration 19 : 0.588388748621996
Loss in iteration 20 : 0.5834633912749999
Loss in iteration 21 : 0.5788792596495471
Loss in iteration 22 : 0.574511215451651
Loss in iteration 23 : 0.5702871562540991
Loss in iteration 24 : 0.5662126265083636
Loss in iteration 25 : 0.5623442879142999
Loss in iteration 26 : 0.5587399234513266
Loss in iteration 27 : 0.555418270981314
Loss in iteration 28 : 0.5523488407437454
Loss in iteration 29 : 0.5494710716657978
Loss in iteration 30 : 0.5467262683309185
Loss in iteration 31 : 0.5440823137380696
Loss in iteration 32 : 0.5415394323164971
Loss in iteration 33 : 0.5391181062013287
Loss in iteration 34 : 0.536839508302091
Loss in iteration 35 : 0.5347103359488283
Loss in iteration 36 : 0.5327188202280391
Loss in iteration 37 : 0.5308413177437977
Loss in iteration 38 : 0.5290536536763445
Loss in iteration 39 : 0.5273403872986638
Loss in iteration 40 : 0.5256978180420422
Loss in iteration 41 : 0.5241306158069069
Loss in iteration 42 : 0.5226451239077107
Loss in iteration 43 : 0.5212433438922385
Loss in iteration 44 : 0.5199204147829638
Loss in iteration 45 : 0.5186660957634937
Loss in iteration 46 : 0.5174686997527173
Loss in iteration 47 : 0.5163190551564235
Loss in iteration 48 : 0.5152125674128117
Loss in iteration 49 : 0.5141487797663872
Loss in iteration 50 : 0.5131291675924233
Loss in iteration 51 : 0.5121545925748994
Loss in iteration 52 : 0.5112236990728186
Loss in iteration 53 : 0.5103328094457058
Loss in iteration 54 : 0.5094770435659014
Loss in iteration 55 : 0.5086518678691282
Loss in iteration 56 : 0.5078542463734128
Loss in iteration 57 : 0.5070829276517448
Loss in iteration 58 : 0.5063379030281882
Loss in iteration 59 : 0.5056194499664455
Loss in iteration 60 : 0.5049272823343236
Loss in iteration 61 : 0.5042601736338617
Loss in iteration 62 : 0.5036161269195994
Loss in iteration 63 : 0.5029928987759742
Loss in iteration 64 : 0.5023885616221958
Loss in iteration 65 : 0.5018018360145221
Loss in iteration 66 : 0.5012320856405226
Loss in iteration 67 : 0.5006790448365775
Loss in iteration 68 : 0.5001424569848539
Loss in iteration 69 : 0.49962180652685495
Loss in iteration 70 : 0.4991162472239177
Loss in iteration 71 : 0.4986247188452229
Loss in iteration 72 : 0.498146160244038
Loss in iteration 73 : 0.49767970156191316
Loss in iteration 74 : 0.4972247512932531
Loss in iteration 75 : 0.49678095829631785
Loss in iteration 76 : 0.4963480891874681
Loss in iteration 77 : 0.4959258916248755
Loss in iteration 78 : 0.4955140064186236
Loss in iteration 79 : 0.49511195714897066
Loss in iteration 80 : 0.4947192056937083
Loss in iteration 81 : 0.49433523496350773
Loss in iteration 82 : 0.4939596157020711
Loss in iteration 83 : 0.4935920298995416
Loss in iteration 84 : 0.49323224818785216
Loss in iteration 85 : 0.49288007965860026
Loss in iteration 86 : 0.4925353212367608
Loss in iteration 87 : 0.4921977286619175
Loss in iteration 88 : 0.49186701727429566
Loss in iteration 89 : 0.49154288606114743
Loss in iteration 90 : 0.491225049483216
Loss in iteration 91 : 0.490913261256276
Loss in iteration 92 : 0.4906073209492717
Loss in iteration 93 : 0.4903070636379446
Loss in iteration 94 : 0.49001234026687357
Loss in iteration 95 : 0.4897229989595214
Loss in iteration 96 : 0.4894388750970767
Loss in iteration 97 : 0.4891597926126034
Loss in iteration 98 : 0.4888855735548014
Loss in iteration 99 : 0.48861604993045715
Loss in iteration 100 : 0.4883510720075359
Loss in iteration 101 : 0.48809050994429476
Loss in iteration 102 : 0.4878342491386435
Loss in iteration 103 : 0.4875821823603335
Loss in iteration 104 : 0.48733420254234777
Loss in iteration 105 : 0.48709019907737083
Loss in iteration 106 : 0.4868500584013401
Loss in iteration 107 : 0.48661366764857333
Loss in iteration 108 : 0.4863809190850946
Loss in iteration 109 : 0.4861517131537754
Loss in iteration 110 : 0.4859259590033331
Loss in iteration 111 : 0.4857035726931011
Loss in iteration 112 : 0.48548447424001506
Loss in iteration 113 : 0.48526858495008
Loss in iteration 114 : 0.48505582607289693
Loss in iteration 115 : 0.4848461190434924
Loss in iteration 116 : 0.4846393868391928
Loss in iteration 117 : 0.48443555559162516
Loss in iteration 118 : 0.48423455565430545
Loss in iteration 119 : 0.4840363217201773
Loss in iteration 120 : 0.4838407920752084
Loss in iteration 121 : 0.4836479074319533
Loss in iteration 122 : 0.4834576098827871
Loss in iteration 123 : 0.4832698423566084
Loss in iteration 124 : 0.483084548672065
Loss in iteration 125 : 0.48290167400683387
Loss in iteration 126 : 0.48272116546077676
Loss in iteration 127 : 0.48254297241596267
Loss in iteration 128 : 0.4823670465449091
Loss in iteration 129 : 0.48219334150157456
Loss in iteration 130 : 0.48202181246182707
Loss in iteration 131 : 0.4818524157142409
Loss in iteration 132 : 0.48168510844271645
Loss in iteration 133 : 0.48151984873346265
Loss in iteration 134 : 0.4813565957371024
Loss in iteration 135 : 0.481195309864484
Loss in iteration 136 : 0.48103595290511203
Loss in iteration 137 : 0.48087848801314675
Loss in iteration 138 : 0.48072287957472076
Loss in iteration 139 : 0.4805690930195914
Loss in iteration 140 : 0.4804170946525892
Loss in iteration 141 : 0.4802668515576758
Loss in iteration 142 : 0.4801183315864025
Loss in iteration 143 : 0.4799715034043754
Loss in iteration 144 : 0.4798263365500212
Loss in iteration 145 : 0.47968280146406383
Loss in iteration 146 : 0.47954086946945496
Loss in iteration 147 : 0.4794005127074191
Loss in iteration 148 : 0.47926170405377183
Loss in iteration 149 : 0.47912441704409475
Loss in iteration 150 : 0.4789886258276096
Loss in iteration 151 : 0.47885430515390914
Loss in iteration 152 : 0.47872143038216985
Loss in iteration 153 : 0.47858997749521504
Loss in iteration 154 : 0.47845992310244756
Loss in iteration 155 : 0.4783312444238967
Loss in iteration 156 : 0.4782039192575766
Loss in iteration 157 : 0.4780779259394466
Loss in iteration 158 : 0.4779532433069762
Loss in iteration 159 : 0.4778298506739664
Loss in iteration 160 : 0.4777077278182444
Loss in iteration 161 : 0.47758685497828635
Loss in iteration 162 : 0.47746721285200794
Loss in iteration 163 : 0.4773487825915673
Loss in iteration 164 : 0.477231545791192
Loss in iteration 165 : 0.47711548446883645
Loss in iteration 166 : 0.4770005810452359
Loss in iteration 167 : 0.4768868183245522
Loss in iteration 168 : 0.4767741794795449
Loss in iteration 169 : 0.4766626480418833
Loss in iteration 170 : 0.47655220789604963
Loss in iteration 171 : 0.47644284327420894
Loss in iteration 172 : 0.4763345387496557
Loss in iteration 173 : 0.47622727922765773
Loss in iteration 174 : 0.4761210499339956
Loss in iteration 175 : 0.4760158364025964
Loss in iteration 176 : 0.4759116244638741
Loss in iteration 177 : 0.47580840023495646
Loss in iteration 178 : 0.4757061501120479
Loss in iteration 179 : 0.47560486076434233
Loss in iteration 180 : 0.4755045191284972
Loss in iteration 181 : 0.4754051124027062
Loss in iteration 182 : 0.47530662803992485
Loss in iteration 183 : 0.47520905374033223
Loss in iteration 184 : 0.4751123774435384
Loss in iteration 185 : 0.47501658732117075
Loss in iteration 186 : 0.4749216717702632
Loss in iteration 187 : 0.47482761940754653
Loss in iteration 188 : 0.47473441906440816
Loss in iteration 189 : 0.47464205978213886
Loss in iteration 190 : 0.47455053080708676
Loss in iteration 191 : 0.47445982158554706
Loss in iteration 192 : 0.4743699217584198
Loss in iteration 193 : 0.474280821155827
Loss in iteration 194 : 0.4741925097919386
Loss in iteration 195 : 0.474104977860178
Loss in iteration 196 : 0.4740182157288336
Loss in iteration 197 : 0.4739322139370041
Loss in iteration 198 : 0.4738469631907221
Loss in iteration 199 : 0.4737624543591044
Loss in iteration 200 : 0.47367867847047124
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.7835, training accuracy 0.783375, time elapsed: 4599 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 20.902439943570485
Loss in iteration 3 : 12.517217681051939
Loss in iteration 4 : 12.194135457128466
Loss in iteration 5 : 6.689634168295111
Loss in iteration 6 : 7.672148955668923
Loss in iteration 7 : 2.987619417986652
Loss in iteration 8 : 6.7080881402748185
Loss in iteration 9 : 5.701037920375063
Loss in iteration 10 : 5.415723511937644
Loss in iteration 11 : 5.344570548341498
Loss in iteration 12 : 3.591161506583009
Loss in iteration 13 : 3.727330084781799
Loss in iteration 14 : 4.357274171408016
Loss in iteration 15 : 3.7578721334698737
Loss in iteration 16 : 3.472258620124941
Loss in iteration 17 : 3.442361411770052
Loss in iteration 18 : 3.227717866068533
Loss in iteration 19 : 3.474263329052351
Loss in iteration 20 : 3.6025759322950943
Loss in iteration 21 : 3.0316464418226556
Loss in iteration 22 : 2.4875756558645024
Loss in iteration 23 : 2.5832205040610865
Loss in iteration 24 : 2.5967153893571377
Loss in iteration 25 : 2.5996784612260297
Loss in iteration 26 : 2.6529127957189105
Loss in iteration 27 : 2.269139624166023
Loss in iteration 28 : 2.172436660311636
Loss in iteration 29 : 2.165374127988659
Loss in iteration 30 : 1.9281047838589478
Loss in iteration 31 : 1.930350943677803
Loss in iteration 32 : 1.732263575572163
Loss in iteration 33 : 1.7607365968527406
Loss in iteration 34 : 1.7323969430230857
Loss in iteration 35 : 1.6080456779959191
Loss in iteration 36 : 1.5851269284250136
Loss in iteration 37 : 1.4452720524539562
Loss in iteration 38 : 1.4200118073928656
Loss in iteration 39 : 1.3167431228563469
Loss in iteration 40 : 1.3867726880473425
Loss in iteration 41 : 1.3099323016930722
Loss in iteration 42 : 1.2495815452936392
Loss in iteration 43 : 1.1697296380618032
Loss in iteration 44 : 1.1675372948839118
Loss in iteration 45 : 1.1683680037844713
Loss in iteration 46 : 1.0475345291027813
Loss in iteration 47 : 1.0894958662398544
Loss in iteration 48 : 1.0388372150022684
Loss in iteration 49 : 0.9638454012953582
Loss in iteration 50 : 0.947900387931447
Loss in iteration 51 : 0.9554563399452394
Loss in iteration 52 : 0.9242441820329409
Loss in iteration 53 : 0.8573120597244177
Loss in iteration 54 : 0.8247409506800191
Loss in iteration 55 : 0.8005714799016794
Loss in iteration 56 : 0.7376934159102676
Loss in iteration 57 : 0.722114122082578
Loss in iteration 58 : 0.6915448821231609
Loss in iteration 59 : 0.6253458107613871
Loss in iteration 60 : 0.6421145455262217
Loss in iteration 61 : 0.6291511466708725
Loss in iteration 62 : 0.7496253075578035
Loss in iteration 63 : 1.6704611635930233
Loss in iteration 64 : 3.3584999368546313
Loss in iteration 65 : 0.9142584208748155
Loss in iteration 66 : 1.276803236539185
Loss in iteration 67 : 2.446395668525269
Loss in iteration 68 : 0.793799493866138
Loss in iteration 69 : 1.4624992858647878
Loss in iteration 70 : 1.6372053860317697
Loss in iteration 71 : 0.8863566073494371
Loss in iteration 72 : 1.69256985119362
Loss in iteration 73 : 1.0252796077682542
Loss in iteration 74 : 1.151973197072101
Loss in iteration 75 : 1.3023346932334754
Loss in iteration 76 : 0.8285261031509821
Loss in iteration 77 : 1.1675157072254305
Loss in iteration 78 : 0.8983001881226993
Loss in iteration 79 : 0.8008382757558007
Loss in iteration 80 : 1.054064536042359
Loss in iteration 81 : 0.8193708882131243
Loss in iteration 82 : 0.6880921888203896
Loss in iteration 83 : 0.858245388468646
Loss in iteration 84 : 0.85993720724648
Loss in iteration 85 : 0.752724063111644
Loss in iteration 86 : 0.6583496299700838
Loss in iteration 87 : 0.5819799404506112
Loss in iteration 88 : 0.5919833472640611
Loss in iteration 89 : 0.573723340674499
Loss in iteration 90 : 0.6973170481957806
Loss in iteration 91 : 1.2641989611953206
Loss in iteration 92 : 2.579362265915069
Loss in iteration 93 : 1.8346680944766052
Loss in iteration 94 : 0.5761139012922363
Loss in iteration 95 : 1.1031263511266767
Loss in iteration 96 : 1.9589890421804337
Loss in iteration 97 : 0.9953288559824921
Loss in iteration 98 : 0.7916959999824269
Loss in iteration 99 : 0.9267630077408698
Loss in iteration 100 : 1.1073695744174639
Loss in iteration 101 : 0.910716655385597
Loss in iteration 102 : 0.6621463279688823
Loss in iteration 103 : 0.8507593240370624
Loss in iteration 104 : 0.9066969306630248
Loss in iteration 105 : 0.7243616717021432
Loss in iteration 106 : 0.8461176808967656
Loss in iteration 107 : 0.623440632392695
Loss in iteration 108 : 0.749427771634589
Loss in iteration 109 : 0.6718157682267671
Loss in iteration 110 : 0.660350284407072
Loss in iteration 111 : 0.718710058648718
Loss in iteration 112 : 0.6751762692993785
Loss in iteration 113 : 0.8405246280641208
Loss in iteration 114 : 0.9424112080642321
Loss in iteration 115 : 1.1495912953688263
Loss in iteration 116 : 1.6147991678793014
Loss in iteration 117 : 0.8745425593738263
Loss in iteration 118 : 1.0994300705487405
Loss in iteration 119 : 0.8050017659131556
Loss in iteration 120 : 0.9686046284961947
Loss in iteration 121 : 1.036265646689049
Loss in iteration 122 : 1.2271784681529392
Loss in iteration 123 : 1.2839028219797775
Loss in iteration 124 : 1.208892425784325
Loss in iteration 125 : 0.7070888528993471
Loss in iteration 126 : 0.7536528594574662
Loss in iteration 127 : 0.6172881343330708
Loss in iteration 128 : 0.6398888152370611
Loss in iteration 129 : 0.6596425929099262
Loss in iteration 130 : 0.5896858632953134
Loss in iteration 131 : 0.7430754130107604
Loss in iteration 132 : 0.8672115199129509
Loss in iteration 133 : 1.5820561298775104
Loss in iteration 134 : 1.6016789422633781
Loss in iteration 135 : 1.0119765129939982
Loss in iteration 136 : 0.7145223062820301
Loss in iteration 137 : 0.6269770710387553
Loss in iteration 138 : 0.589556943874609
Loss in iteration 139 : 0.64116061509138
Loss in iteration 140 : 0.6750734242665083
Loss in iteration 141 : 0.949124878382076
Loss in iteration 142 : 1.5857057841854407
Loss in iteration 143 : 1.7623550241962505
Loss in iteration 144 : 1.1988012355683526
Loss in iteration 145 : 0.7384402939465754
Loss in iteration 146 : 0.6672153119663424
Loss in iteration 147 : 0.6172623174835896
Loss in iteration 148 : 0.6267631052037843
Loss in iteration 149 : 0.6342501598640571
Loss in iteration 150 : 0.6688436786573086
Loss in iteration 151 : 0.8459465360140075
Loss in iteration 152 : 1.3065816667353078
Loss in iteration 153 : 1.818628765150184
Loss in iteration 154 : 1.2844173227551459
Loss in iteration 155 : 0.7944803726420696
Loss in iteration 156 : 0.6674895651808931
Loss in iteration 157 : 0.6030752660550918
Loss in iteration 158 : 0.5974906991164011
Loss in iteration 159 : 0.5743226322482476
Loss in iteration 160 : 0.560917919832977
Loss in iteration 161 : 0.5598790505607053
Loss in iteration 162 : 0.6038656403610573
Loss in iteration 163 : 0.9719778665132663
Loss in iteration 164 : 2.342373895818312
Loss in iteration 165 : 1.816160981642649
Loss in iteration 166 : 0.8274203735968358
Loss in iteration 167 : 0.5848070735384482
Loss in iteration 168 : 0.5699157104885367
Loss in iteration 169 : 0.6325610056197961
Loss in iteration 170 : 0.7854145594012641
Loss in iteration 171 : 1.028072880577864
Loss in iteration 172 : 1.2325931359836042
Loss in iteration 173 : 1.1453242108166892
Loss in iteration 174 : 0.9337076230101823
Loss in iteration 175 : 0.7920661322352565
Loss in iteration 176 : 0.7291588244503568
Loss in iteration 177 : 0.7459899572386665
Loss in iteration 178 : 0.8199369892718699
Loss in iteration 179 : 0.9507963244117664
Loss in iteration 180 : 1.077576085963276
Loss in iteration 181 : 1.0587467981264687
Loss in iteration 182 : 0.9296059110177793
Loss in iteration 183 : 0.8000200619062062
Loss in iteration 184 : 0.6995532222484221
Loss in iteration 185 : 0.661240340454839
Loss in iteration 186 : 0.646193256315754
Loss in iteration 187 : 0.7007463846782545
Loss in iteration 188 : 0.8973343528864802
Loss in iteration 189 : 1.4371550057806308
Loss in iteration 190 : 1.7612617212970236
Loss in iteration 191 : 1.2128842332037675
Loss in iteration 192 : 0.7863688435582481
Loss in iteration 193 : 0.6529789843117451
Loss in iteration 194 : 0.6026837307358843
Loss in iteration 195 : 0.5903585779348136
Loss in iteration 196 : 0.5715861732585049
Loss in iteration 197 : 0.5663878598413876
Loss in iteration 198 : 0.5790643799273824
Loss in iteration 199 : 0.7171049898675133
Loss in iteration 200 : 1.3778036623620542
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.471, training accuracy 0.481625, time elapsed: 4555 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 3.7557949052966304
Loss in iteration 3 : 0.5751463388375738
Loss in iteration 4 : 4.175676484102264
Loss in iteration 5 : 0.5742962152922161
Loss in iteration 6 : 2.568035212700447
Loss in iteration 7 : 0.7222468388282
Loss in iteration 8 : 2.1368416054007486
Loss in iteration 9 : 0.6098783904677243
Loss in iteration 10 : 1.5791690662644924
Loss in iteration 11 : 0.6923223742467699
Loss in iteration 12 : 1.299422130256735
Loss in iteration 13 : 0.7294352155923719
Loss in iteration 14 : 1.209234392480619
Loss in iteration 15 : 0.7461883391296453
Loss in iteration 16 : 1.0734104956493729
Loss in iteration 17 : 0.8444626046046676
Loss in iteration 18 : 0.9661744850815164
Loss in iteration 19 : 0.8270218310899913
Loss in iteration 20 : 0.8724366844450223
Loss in iteration 21 : 0.8607410714807127
Loss in iteration 22 : 0.7920204276000843
Loss in iteration 23 : 0.7952896847229831
Loss in iteration 24 : 0.7440045449817959
Loss in iteration 25 : 0.7663973556971908
Loss in iteration 26 : 0.7077483282885525
Loss in iteration 27 : 0.6675805345185996
Loss in iteration 28 : 0.6949203252771745
Loss in iteration 29 : 0.6351508484927911
Loss in iteration 30 : 0.6688812265539893
Loss in iteration 31 : 0.5695321400409302
Loss in iteration 32 : 0.6563905177001687
Loss in iteration 33 : 0.5760059558844568
Loss in iteration 34 : 0.5681405683708163
Loss in iteration 35 : 0.6228333792742321
Loss in iteration 36 : 0.5419443619447012
Loss in iteration 37 : 0.5595352093512084
Loss in iteration 38 : 0.6084233639186369
Loss in iteration 39 : 0.5480739489731495
Loss in iteration 40 : 0.522957877441111
Loss in iteration 41 : 0.5667635196396871
Loss in iteration 42 : 0.5716048291597562
Loss in iteration 43 : 0.54581155420467
Loss in iteration 44 : 0.5165201526229931
Loss in iteration 45 : 0.49853149598760205
Loss in iteration 46 : 0.5195287815056856
Loss in iteration 47 : 0.5210518737838871
Loss in iteration 48 : 0.5152892686003417
Loss in iteration 49 : 0.5054062065617209
Loss in iteration 50 : 0.4813853180719409
Loss in iteration 51 : 0.48169313216905857
Loss in iteration 52 : 0.4750099126063983
Loss in iteration 53 : 0.47794630897120943
Loss in iteration 54 : 0.48625903764649614
Loss in iteration 55 : 0.4854344620158235
Loss in iteration 56 : 0.4979139648935894
Loss in iteration 57 : 0.49904571190341207
Loss in iteration 58 : 0.5111890821069123
Loss in iteration 59 : 0.5240270537479055
Loss in iteration 60 : 0.5463727000677047
Loss in iteration 61 : 0.5735828758465938
Loss in iteration 62 : 0.6046900156497714
Loss in iteration 63 : 0.6129132047398849
Loss in iteration 64 : 0.6221689048014414
Loss in iteration 65 : 0.5907792312288758
Loss in iteration 66 : 0.5682397478278244
Loss in iteration 67 : 0.5332047920544767
Loss in iteration 68 : 0.5140216695646455
Loss in iteration 69 : 0.5011433137515489
Loss in iteration 70 : 0.49845907608468837
Loss in iteration 71 : 0.5054800017097901
Loss in iteration 72 : 0.5281680653473994
Loss in iteration 73 : 0.5750301082880838
Loss in iteration 74 : 0.6693354757783534
Loss in iteration 75 : 0.7340356867499134
Loss in iteration 76 : 0.7415589097994362
Loss in iteration 77 : 0.60840162397398
Loss in iteration 78 : 0.514905946664448
Loss in iteration 79 : 0.4751174430161038
Loss in iteration 80 : 0.49407895014585135
Loss in iteration 81 : 0.5431335659250598
Loss in iteration 82 : 0.5905752421146797
Loss in iteration 83 : 0.6077213412427886
Loss in iteration 84 : 0.5842198009771953
Loss in iteration 85 : 0.5553171335290288
Loss in iteration 86 : 0.5244677616024831
Loss in iteration 87 : 0.5079141607868429
Loss in iteration 88 : 0.49237352082600944
Loss in iteration 89 : 0.48941318882138507
Loss in iteration 90 : 0.48208021611641605
Loss in iteration 91 : 0.48584367606829193
Loss in iteration 92 : 0.48285708618871026
Loss in iteration 93 : 0.494663220072159
Loss in iteration 94 : 0.5062966010542203
Loss in iteration 95 : 0.5538365688565634
Loss in iteration 96 : 0.6247635319365004
Loss in iteration 97 : 0.768925456897978
Loss in iteration 98 : 0.7693141203067586
Loss in iteration 99 : 0.7304120928437849
Loss in iteration 100 : 0.5657334312680075
Loss in iteration 101 : 0.5177156701269517
Loss in iteration 102 : 0.47509653456644774
Loss in iteration 103 : 0.5127047217793258
Loss in iteration 104 : 0.5157015152043742
Loss in iteration 105 : 0.5488629002429755
Loss in iteration 106 : 0.5414347085265068
Loss in iteration 107 : 0.5438822493768423
Loss in iteration 108 : 0.53406420683988
Loss in iteration 109 : 0.5240672063550542
Loss in iteration 110 : 0.5302569583399467
Loss in iteration 111 : 0.5188000152562848
Loss in iteration 112 : 0.5441690207952523
Loss in iteration 113 : 0.5294294320930049
Loss in iteration 114 : 0.5779235511752222
Loss in iteration 115 : 0.5535689111086224
Loss in iteration 116 : 0.6265940803589211
Loss in iteration 117 : 0.5756431650347862
Loss in iteration 118 : 0.6663066607238282
Loss in iteration 119 : 0.5835517384366254
Loss in iteration 120 : 0.6829731396496266
Loss in iteration 121 : 0.5775070904115808
Loss in iteration 122 : 0.6744748135786985
Loss in iteration 123 : 0.5625294019970117
Loss in iteration 124 : 0.6484154780598617
Loss in iteration 125 : 0.5455692409673037
Loss in iteration 126 : 0.6164781974855548
Loss in iteration 127 : 0.53152549281795
Loss in iteration 128 : 0.5905648010951065
Loss in iteration 129 : 0.5232809123846559
Loss in iteration 130 : 0.572983309519998
Loss in iteration 131 : 0.5252837571339334
Loss in iteration 132 : 0.5750604828234415
Loss in iteration 133 : 0.5457163428717536
Loss in iteration 134 : 0.6008515143446249
Loss in iteration 135 : 0.5842514148573981
Loss in iteration 136 : 0.6428893139985367
Loss in iteration 137 : 0.6115217359483248
Loss in iteration 138 : 0.6429152233739691
Loss in iteration 139 : 0.5795360695233731
Loss in iteration 140 : 0.5904592982126848
Loss in iteration 141 : 0.5332876449815134
Loss in iteration 142 : 0.5504820693003206
Loss in iteration 143 : 0.5152500291157917
Loss in iteration 144 : 0.5450976237759813
Loss in iteration 145 : 0.5355945685544251
Loss in iteration 146 : 0.5931617014587459
Loss in iteration 147 : 0.6245145264354814
Loss in iteration 148 : 0.7383407703365228
Loss in iteration 149 : 0.7073859535860031
Loss in iteration 150 : 0.7384753285160314
Loss in iteration 151 : 0.5696452212265067
Loss in iteration 152 : 0.5801064749786917
Loss in iteration 153 : 0.4955727662573494
Loss in iteration 154 : 0.5428950436452562
Loss in iteration 155 : 0.49678082696281073
Loss in iteration 156 : 0.527519149383415
Loss in iteration 157 : 0.495196349624478
Loss in iteration 158 : 0.5087868845924624
Loss in iteration 159 : 0.4925514357333405
Loss in iteration 160 : 0.4973381911499276
Loss in iteration 161 : 0.4867607840013017
Loss in iteration 162 : 0.4899564125777309
Loss in iteration 163 : 0.4783469777833321
Loss in iteration 164 : 0.4871971276531839
Loss in iteration 165 : 0.4713924312620332
Loss in iteration 166 : 0.48893522275692386
Loss in iteration 167 : 0.4747172444050675
Loss in iteration 168 : 0.5328030644818588
Loss in iteration 169 : 0.6669209007839481
Loss in iteration 170 : 1.179097497116859
Loss in iteration 171 : 1.116877528617897
Loss in iteration 172 : 0.8235442232984781
Loss in iteration 173 : 0.5580537217820987
Loss in iteration 174 : 0.49030414071507117
Loss in iteration 175 : 0.5339647375348292
Loss in iteration 176 : 0.637048546061893
Loss in iteration 177 : 0.6783887182646581
Loss in iteration 178 : 0.6100516208223883
Loss in iteration 179 : 0.53131099266537
Loss in iteration 180 : 0.49176277525517154
Loss in iteration 181 : 0.5091451742360938
Loss in iteration 182 : 0.5473218594963102
Loss in iteration 183 : 0.6081218978698962
Loss in iteration 184 : 0.6432377060088273
Loss in iteration 185 : 0.6416616131266331
Loss in iteration 186 : 0.6037036223760572
Loss in iteration 187 : 0.549189346381064
Loss in iteration 188 : 0.520496529780808
Loss in iteration 189 : 0.49342213572343674
Loss in iteration 190 : 0.48957643413598145
Loss in iteration 191 : 0.47535194787392365
Loss in iteration 192 : 0.47784053750115685
Loss in iteration 193 : 0.4723749276479169
Loss in iteration 194 : 0.47383479777882725
Loss in iteration 195 : 0.4762973268138223
Loss in iteration 196 : 0.47233922926593985
Loss in iteration 197 : 0.4939719479805437
Loss in iteration 198 : 0.5835668709771561
Loss in iteration 199 : 1.046991012519452
Loss in iteration 200 : 1.40682709814473
Testing accuracy  of updater 7 on alg 0 with rate 14.0 = 0.5735, training accuracy 0.5875, time elapsed: 4532 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 1.8996383862980475
Loss in iteration 3 : 0.5884580689693956
Loss in iteration 4 : 1.5079160770620241
Loss in iteration 5 : 1.0813794209204912
Loss in iteration 6 : 0.7697459573387926
Loss in iteration 7 : 1.2705313186575067
Loss in iteration 8 : 0.6721591101793549
Loss in iteration 9 : 1.1563541524730918
Loss in iteration 10 : 0.8015092413867716
Loss in iteration 11 : 0.9297441001788297
Loss in iteration 12 : 0.9791797316892623
Loss in iteration 13 : 0.6617828991666431
Loss in iteration 14 : 1.001510381218845
Loss in iteration 15 : 0.6306998677588177
Loss in iteration 16 : 0.8709733234450284
Loss in iteration 17 : 0.7812531847917624
Loss in iteration 18 : 0.6783758585695583
Loss in iteration 19 : 0.8445081561497138
Loss in iteration 20 : 0.6233949410134392
Loss in iteration 21 : 0.7724649370367382
Loss in iteration 22 : 0.6794142974832083
Loss in iteration 23 : 0.6532600082217673
Loss in iteration 24 : 0.7120157580014309
Loss in iteration 25 : 0.5882965877262011
Loss in iteration 26 : 0.686417928389911
Loss in iteration 27 : 0.5731541058622631
Loss in iteration 28 : 0.6305112014809002
Loss in iteration 29 : 0.5734966278294916
Loss in iteration 30 : 0.5795098801443699
Loss in iteration 31 : 0.5717726027479918
Loss in iteration 32 : 0.5418181071721033
Loss in iteration 33 : 0.5589796496515425
Loss in iteration 34 : 0.5198945634444229
Loss in iteration 35 : 0.5430338855185496
Loss in iteration 36 : 0.5092658824810017
Loss in iteration 37 : 0.5273228417937097
Loss in iteration 38 : 0.5057662453709489
Loss in iteration 39 : 0.5111988256958012
Loss in iteration 40 : 0.5084691822002153
Loss in iteration 41 : 0.4963485840541372
Loss in iteration 42 : 0.5134819856574999
Loss in iteration 43 : 0.4858529694416792
Loss in iteration 44 : 0.5105566733135075
Loss in iteration 45 : 0.4853556930443509
Loss in iteration 46 : 0.49770577561251217
Loss in iteration 47 : 0.4927878262537681
Loss in iteration 48 : 0.4823020876816749
Loss in iteration 49 : 0.4946199428361499
Loss in iteration 50 : 0.4764613794526798
Loss in iteration 51 : 0.4843093907123584
Loss in iteration 52 : 0.47940060433004483
Loss in iteration 53 : 0.4721834801908523
Loss in iteration 54 : 0.4790265056802556
Loss in iteration 55 : 0.4681384550154615
Loss in iteration 56 : 0.47207909524321895
Loss in iteration 57 : 0.47014698987756515
Loss in iteration 58 : 0.46505403163432585
Loss in iteration 59 : 0.46989810931081455
Loss in iteration 60 : 0.46399249881975957
Loss in iteration 61 : 0.4654965272541819
Loss in iteration 62 : 0.46604730214874485
Loss in iteration 63 : 0.4621596148256345
Loss in iteration 64 : 0.4653069620517566
Loss in iteration 65 : 0.46282199343372205
Loss in iteration 66 : 0.4623028452941651
Loss in iteration 67 : 0.4638987219858441
Loss in iteration 68 : 0.4611218778266228
Loss in iteration 69 : 0.4622893440016436
Loss in iteration 70 : 0.46213038684316565
Loss in iteration 71 : 0.4604001940360162
Loss in iteration 72 : 0.4617346733762278
Loss in iteration 73 : 0.4607788630158877
Loss in iteration 74 : 0.4600215551872773
Loss in iteration 75 : 0.46106291353121415
Loss in iteration 76 : 0.4600810609796588
Loss in iteration 77 : 0.45985357764980833
Loss in iteration 78 : 0.46061338580061806
Loss in iteration 79 : 0.45985140226267845
Loss in iteration 80 : 0.4598261874225139
Loss in iteration 81 : 0.4603870797824642
Loss in iteration 82 : 0.45982593646261904
Loss in iteration 83 : 0.4598074886930106
Loss in iteration 84 : 0.4602247967798428
Loss in iteration 85 : 0.45981333119612344
Loss in iteration 86 : 0.4597308131538384
Loss in iteration 87 : 0.46002883784317955
Loss in iteration 88 : 0.4597289232341949
Loss in iteration 89 : 0.4596048858438836
Loss in iteration 90 : 0.4598158985228586
Loss in iteration 91 : 0.4596082985081486
Loss in iteration 92 : 0.459477915935245
Loss in iteration 93 : 0.4596352707648744
Loss in iteration 94 : 0.45950586948424305
Loss in iteration 95 : 0.4593871511515084
Loss in iteration 96 : 0.459504594647229
Loss in iteration 97 : 0.45943135388774614
Loss in iteration 98 : 0.45933313670478415
Loss in iteration 99 : 0.45941761729944175
Loss in iteration 100 : 0.45937585562409816
Loss in iteration 101 : 0.4592967996770662
Loss in iteration 102 : 0.45935495933099824
Loss in iteration 103 : 0.4593297421813296
Loss in iteration 104 : 0.4592640757701143
Loss in iteration 105 : 0.4593008826622183
Loss in iteration 106 : 0.4592864148300415
Loss in iteration 107 : 0.45923212390049284
Loss in iteration 108 : 0.4592534987975073
Loss in iteration 109 : 0.4592459596887823
Loss in iteration 110 : 0.4592027524299502
Loss in iteration 111 : 0.45921434616418155
Loss in iteration 112 : 0.45921210648139205
Loss in iteration 113 : 0.45917876581435507
Loss in iteration 114 : 0.4591839798019781
Loss in iteration 115 : 0.45918623111260215
Loss in iteration 116 : 0.4591614280623067
Loss in iteration 117 : 0.4591620338713985
Loss in iteration 118 : 0.45916635389313526
Loss in iteration 119 : 0.4591484676944261
Loss in iteration 120 : 0.459145600646485
Loss in iteration 121 : 0.45914980050050314
Loss in iteration 122 : 0.459136914431783
Loss in iteration 123 : 0.4591314562847143
Loss in iteration 124 : 0.4591345139117065
Loss in iteration 125 : 0.4591253092224904
Loss in iteration 126 : 0.4591182348106723
Loss in iteration 127 : 0.45911970030933436
Loss in iteration 128 : 0.4591134525828032
Loss in iteration 129 : 0.45910608015452226
Loss in iteration 130 : 0.45910603393361304
Loss in iteration 131 : 0.4591020971505737
Loss in iteration 132 : 0.45909542705981304
Loss in iteration 133 : 0.45909425716497027
Loss in iteration 134 : 0.4590918989853823
Loss in iteration 135 : 0.45908628550639463
Loss in iteration 136 : 0.459084273755578
Loss in iteration 137 : 0.45908276968790745
Loss in iteration 138 : 0.45907828137831225
Loss in iteration 139 : 0.4590756108521255
Loss in iteration 140 : 0.45907436453688283
Loss in iteration 141 : 0.459070900277301
Loss in iteration 142 : 0.4590678813295648
Loss in iteration 143 : 0.45906652580607
Loss in iteration 144 : 0.459063898987901
Loss in iteration 145 : 0.45906090009807193
Loss in iteration 146 : 0.4590593283180354
Loss in iteration 147 : 0.45905734807849663
Loss in iteration 148 : 0.45905463385334117
Testing accuracy  of updater 7 on alg 0 with rate 8.0 = 0.7885, training accuracy 0.78975, time elapsed: 3299 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6764936279264102
Loss in iteration 3 : 0.6596172390655715
Loss in iteration 4 : 0.6334633115343355
Loss in iteration 5 : 0.6136972706641135
Loss in iteration 6 : 0.5873815056476079
Loss in iteration 7 : 0.5660418587785362
Loss in iteration 8 : 0.5489403569732215
Loss in iteration 9 : 0.5329059755186281
Loss in iteration 10 : 0.5234283099475104
Loss in iteration 11 : 0.5142421452239303
Loss in iteration 12 : 0.5062172418229288
Loss in iteration 13 : 0.5015533663648517
Loss in iteration 14 : 0.4966817971737592
Loss in iteration 15 : 0.4928290731944446
Loss in iteration 16 : 0.49109190886412696
Loss in iteration 17 : 0.48904166377993213
Loss in iteration 18 : 0.48710511218761965
Loss in iteration 19 : 0.4862749433462597
Loss in iteration 20 : 0.4851355570427128
Loss in iteration 21 : 0.48363077143740535
Loss in iteration 22 : 0.48281974446143167
Loss in iteration 23 : 0.48211239901568537
Loss in iteration 24 : 0.48104194917930987
Loss in iteration 25 : 0.4802949184620592
Loss in iteration 26 : 0.4797491040086735
Loss in iteration 27 : 0.4788261759041284
Loss in iteration 28 : 0.4778752385257889
Loss in iteration 29 : 0.47715358147948417
Loss in iteration 30 : 0.47626471012363614
Loss in iteration 31 : 0.4752584535434902
Loss in iteration 32 : 0.47447600482620117
Loss in iteration 33 : 0.4737279250421252
Loss in iteration 34 : 0.47284908720150165
Loss in iteration 35 : 0.4720740337356328
Loss in iteration 36 : 0.4713997689032853
Loss in iteration 37 : 0.4706335914337627
Loss in iteration 38 : 0.4698932256061716
Loss in iteration 39 : 0.4692943131792496
Loss in iteration 40 : 0.468702278282479
Loss in iteration 41 : 0.46811258638271147
Loss in iteration 42 : 0.46763823545621463
Loss in iteration 43 : 0.46720864202258106
Loss in iteration 44 : 0.46675753771638107
Loss in iteration 45 : 0.466369661468934
Loss in iteration 46 : 0.4660438380763441
Loss in iteration 47 : 0.4657091670143742
Loss in iteration 48 : 0.4654089489058991
Loss in iteration 49 : 0.46517678518456224
Loss in iteration 50 : 0.46495544356443713
Loss in iteration 51 : 0.4647451968875745
Loss in iteration 52 : 0.4645818103873065
Loss in iteration 53 : 0.46443192036054576
Loss in iteration 54 : 0.46427859410881106
Loss in iteration 55 : 0.46415439778900947
Loss in iteration 56 : 0.46404944564321565
Loss in iteration 57 : 0.46394081301411877
Loss in iteration 58 : 0.4638480090065292
Loss in iteration 59 : 0.4637728264510702
Loss in iteration 60 : 0.46369283297010416
Loss in iteration 61 : 0.4636157082709531
Loss in iteration 62 : 0.4635505655649915
Loss in iteration 63 : 0.46348270370399924
Loss in iteration 64 : 0.46341316581826836
Loss in iteration 65 : 0.4633529446393238
Loss in iteration 66 : 0.4632940441042227
Loss in iteration 67 : 0.4632325691735008
Loss in iteration 68 : 0.46317665475220643
Loss in iteration 69 : 0.46312270844607656
Loss in iteration 70 : 0.46306531634368014
Loss in iteration 71 : 0.4630104845081877
Loss in iteration 72 : 0.4629587679337138
Loss in iteration 73 : 0.46290531107407096
Loss in iteration 74 : 0.46285322289399605
Loss in iteration 75 : 0.46280459704560584
Loss in iteration 76 : 0.46275557097063447
Loss in iteration 77 : 0.4627068501514699
Loss in iteration 78 : 0.46266081707644613
Loss in iteration 79 : 0.4626151287392412
Loss in iteration 80 : 0.4625694637243272
Loss in iteration 81 : 0.46252618803408646
Loss in iteration 82 : 0.4624842749572963
Loss in iteration 83 : 0.462442660674811
Loss in iteration 84 : 0.4624028800915405
Loss in iteration 85 : 0.4623646117919671
Loss in iteration 86 : 0.4623265919754297
Loss in iteration 87 : 0.4622897121557507
Loss in iteration 88 : 0.46225426778751477
Loss in iteration 89 : 0.4622192807726576
Loss in iteration 90 : 0.4621851381214179
Loss in iteration 91 : 0.46215232613971824
Loss in iteration 92 : 0.46212011120023705
Loss in iteration 93 : 0.46208844467831073
Loss in iteration 94 : 0.4620577687583048
Loss in iteration 95 : 0.46202764068481045
Loss in iteration 96 : 0.4619978593203152
Loss in iteration 97 : 0.46196883524199434
Loss in iteration 98 : 0.4619404025663751
Loss in iteration 99 : 0.4619122864096958
Loss in iteration 100 : 0.46188473131621455
Loss in iteration 101 : 0.46185769798259263
Loss in iteration 102 : 0.46183091287197126
Loss in iteration 103 : 0.4618045024408351
Loss in iteration 104 : 0.46177853178214223
Loss in iteration 105 : 0.46175280808074304
Loss in iteration 106 : 0.46172738434369254
Loss in iteration 107 : 0.4617023629620898
Loss in iteration 108 : 0.4616776117875306
Loss in iteration 109 : 0.4616531146082343
Loss in iteration 110 : 0.461628960794699
Loss in iteration 111 : 0.46160507265295003
Loss in iteration 112 : 0.46158140955785987
Loss in iteration 113 : 0.46155805141491574
Loss in iteration 114 : 0.46153497162043505
Loss in iteration 115 : 0.461512120764359
Loss in iteration 116 : 0.4614895480943651
Loss in iteration 117 : 0.46146724813729223
Loss in iteration 118 : 0.4614451714112794
Loss in iteration 119 : 0.4614233429732746
Loss in iteration 120 : 0.4614017713306482
Loss in iteration 121 : 0.4613804181877492
Loss in iteration 122 : 0.4613592949239651
Loss in iteration 123 : 0.4613384173985729
Loss in iteration 124 : 0.4613177571017268
Loss in iteration 125 : 0.46129731121332995
Loss in iteration 126 : 0.4612770925708593
Loss in iteration 127 : 0.4612570811595075
Loss in iteration 128 : 0.4612372682447791
Loss in iteration 129 : 0.4612176647324296
Loss in iteration 130 : 0.4611982594602053
Loss in iteration 131 : 0.46117904217089156
Loss in iteration 132 : 0.4611600199774223
Loss in iteration 133 : 0.4611411861870253
Loss in iteration 134 : 0.46112252971275297
Loss in iteration 135 : 0.4611040539648237
Loss in iteration 136 : 0.4610857557306147
Loss in iteration 137 : 0.46106762559821324
Loss in iteration 138 : 0.4610496650599774
Loss in iteration 139 : 0.4610318735231053
Loss in iteration 140 : 0.4610142435643636
Loss in iteration 141 : 0.46099677451287074
Loss in iteration 142 : 0.4609794660946047
Loss in iteration 143 : 0.4609623123446783
Loss in iteration 144 : 0.4609453115790645
Loss in iteration 145 : 0.46092846398203935
Loss in iteration 146 : 0.46091176527662275
Loss in iteration 147 : 0.46089521339426154
Loss in iteration 148 : 0.46087880851870944
Loss in iteration 149 : 0.4608625474755418
Loss in iteration 150 : 0.46084642780945745
Loss in iteration 151 : 0.46083044925946276
Loss in iteration 152 : 0.46081460942476793
Loss in iteration 153 : 0.4607989059613715
Loss in iteration 154 : 0.46078333846294234
Loss in iteration 155 : 0.46076790519542277
Loss in iteration 156 : 0.4607526039822146
Loss in iteration 157 : 0.46073743409734275
Loss in iteration 158 : 0.4607223940544312
Loss in iteration 159 : 0.4607074817949828
Loss in iteration 160 : 0.460692696390368
Loss in iteration 161 : 0.46067803660591583
Loss in iteration 162 : 0.46066350064031475
Loss in iteration 163 : 0.46064908748865946
Loss in iteration 164 : 0.4606347960706611
Loss in iteration 165 : 0.46062062477051574
Loss in iteration 166 : 0.46060657248222076
Loss in iteration 167 : 0.46059263816756485
Loss in iteration 168 : 0.4605788203780831
Loss in iteration 169 : 0.46056511801255895
Loss in iteration 170 : 0.46055153011103606
Loss in iteration 171 : 0.46053805539027165
Loss in iteration 172 : 0.46052469276414726
Loss in iteration 173 : 0.460511441295767
Loss in iteration 174 : 0.4604982998070054
Loss in iteration 175 : 0.4604852672316358
Loss in iteration 176 : 0.4604723426574699
Loss in iteration 177 : 0.4604595250107849
Loss in iteration 178 : 0.4604468132766877
Loss in iteration 179 : 0.4604342065762974
Loss in iteration 180 : 0.460421703920122
Loss in iteration 181 : 0.4604093043370331
Loss in iteration 182 : 0.46039700696428476
Loss in iteration 183 : 0.4603848108692442
Loss in iteration 184 : 0.46037271512539274
Loss in iteration 185 : 0.4603607188983056
Loss in iteration 186 : 0.46034882131215776
Loss in iteration 187 : 0.46033702148543976
Loss in iteration 188 : 0.4603253186073854
Loss in iteration 189 : 0.460313711844076
Loss in iteration 190 : 0.46030220035158836
Loss in iteration 191 : 0.4602907833402417
Loss in iteration 192 : 0.46027946001120496
Loss in iteration 193 : 0.46026822955786306
Loss in iteration 194 : 0.46025709121611286
Loss in iteration 195 : 0.46024604421983667
Loss in iteration 196 : 0.4602350877952673
Loss in iteration 197 : 0.4602242212004015
Loss in iteration 198 : 0.4602134436954798
Loss in iteration 199 : 0.4602027545351202
Loss in iteration 200 : 0.4601921529989406
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.7865, training accuracy 0.788875, time elapsed: 4559 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6775977460897922
Loss in iteration 3 : 0.6633522496737174
Loss in iteration 4 : 0.6398779631935061
Loss in iteration 5 : 0.6202395580002683
Loss in iteration 6 : 0.5992533741044642
Loss in iteration 7 : 0.575993503594453
Loss in iteration 8 : 0.5595218860310021
Loss in iteration 9 : 0.5439107861480119
Loss in iteration 10 : 0.5309303718440292
Loss in iteration 11 : 0.5226795628464332
Loss in iteration 12 : 0.5143355375014858
Loss in iteration 13 : 0.5070865057679101
Loss in iteration 14 : 0.5025575286773103
Loss in iteration 15 : 0.49821025780243317
Loss in iteration 16 : 0.4942455131363357
Loss in iteration 17 : 0.49208156302930217
Loss in iteration 18 : 0.49033412742432564
Loss in iteration 19 : 0.488241789012227
Loss in iteration 20 : 0.48685338343517653
Loss in iteration 21 : 0.48595120949943793
Loss in iteration 22 : 0.48462540853172226
Loss in iteration 23 : 0.48332311174291726
Loss in iteration 24 : 0.4825568843874155
Loss in iteration 25 : 0.48179112854686174
Loss in iteration 26 : 0.4808060844223889
Loss in iteration 27 : 0.48005302990100907
Loss in iteration 28 : 0.4794813688699524
Loss in iteration 29 : 0.47868400820150303
Loss in iteration 30 : 0.4777912140614687
Loss in iteration 31 : 0.47707603696026135
Loss in iteration 32 : 0.4763598463811639
Loss in iteration 33 : 0.4755023217940919
Loss in iteration 34 : 0.47471801396120106
Loss in iteration 35 : 0.4740747409796438
Loss in iteration 36 : 0.47338644888212084
Loss in iteration 37 : 0.47265220044497813
Loss in iteration 38 : 0.4720151782633374
Loss in iteration 39 : 0.4714259832209403
Loss in iteration 40 : 0.47078405477966373
Loss in iteration 41 : 0.47016932120111593
Loss in iteration 42 : 0.4696516072984544
Loss in iteration 43 : 0.4691561134053383
Loss in iteration 44 : 0.4686564437631524
Loss in iteration 45 : 0.46822089105936093
Loss in iteration 46 : 0.4678418338889384
Loss in iteration 47 : 0.46745768761335865
Loss in iteration 48 : 0.4670886628051376
Loss in iteration 49 : 0.4667741496379735
Loss in iteration 50 : 0.4664822876012673
Loss in iteration 51 : 0.4661915774437281
Loss in iteration 52 : 0.4659358012992375
Loss in iteration 53 : 0.46572004608978124
Loss in iteration 54 : 0.4655122651421837
Loss in iteration 55 : 0.46531493998165385
Loss in iteration 56 : 0.46514707719180365
Loss in iteration 57 : 0.464993835441112
Loss in iteration 58 : 0.4648410339889759
Loss in iteration 59 : 0.4647033803089141
Loss in iteration 60 : 0.4645849358725714
Loss in iteration 61 : 0.4644704431533291
Loss in iteration 62 : 0.46436052708391007
Loss in iteration 63 : 0.4642648856771598
Loss in iteration 64 : 0.4641762789993765
Loss in iteration 65 : 0.46408668462301106
Loss in iteration 66 : 0.464002612886407
Loss in iteration 67 : 0.4639261717465454
Loss in iteration 68 : 0.4638501032930506
Loss in iteration 69 : 0.4637749734913839
Loss in iteration 70 : 0.46370607258311075
Loss in iteration 71 : 0.4636404795499393
Loss in iteration 72 : 0.46357461583415593
Loss in iteration 73 : 0.4635115909807854
Loss in iteration 74 : 0.46345218923090864
Loss in iteration 75 : 0.46339282278816224
Loss in iteration 76 : 0.4633338800621533
Loss in iteration 77 : 0.4632779665450836
Loss in iteration 78 : 0.4632236996055425
Loss in iteration 79 : 0.4631695408305692
Loss in iteration 80 : 0.46311720325421246
Loss in iteration 81 : 0.4630671398104553
Loss in iteration 82 : 0.46301763127902085
Loss in iteration 83 : 0.46296882778690185
Loss in iteration 84 : 0.4629218665133961
Loss in iteration 85 : 0.46287602568916036
Loss in iteration 86 : 0.46283065129460615
Loss in iteration 87 : 0.46278664091557165
Loss in iteration 88 : 0.4627442040279902
Loss in iteration 89 : 0.4627025347690384
Loss in iteration 90 : 0.4626617459537249
Loss in iteration 91 : 0.46262233972384187
Loss in iteration 92 : 0.4625838849418298
Loss in iteration 93 : 0.4625460289815166
Loss in iteration 94 : 0.46250917091747634
Loss in iteration 95 : 0.4624733638171673
Loss in iteration 96 : 0.4624382156873686
Loss in iteration 97 : 0.46240380351444615
Loss in iteration 98 : 0.46237035644305946
Loss in iteration 99 : 0.46233764237302394
Loss in iteration 100 : 0.462305484901234
Loss in iteration 101 : 0.4622740497544836
Loss in iteration 102 : 0.4622433159384556
Loss in iteration 103 : 0.46221307908092923
Loss in iteration 104 : 0.46218338340603454
Loss in iteration 105 : 0.46215432883517366
Loss in iteration 106 : 0.462125791295099
Loss in iteration 107 : 0.46209769113022403
Loss in iteration 108 : 0.4620701046414033
Loss in iteration 109 : 0.4620430024030019
Loss in iteration 110 : 0.46201627641621384
Loss in iteration 111 : 0.46198994499687124
Loss in iteration 112 : 0.4619640441883008
Loss in iteration 113 : 0.4619385080652484
Loss in iteration 114 : 0.46191330657476204
Loss in iteration 115 : 0.4618884795339542
Loss in iteration 116 : 0.4618640073139256
Loss in iteration 117 : 0.46183983891952746
Loss in iteration 118 : 0.46181598518691974
Loss in iteration 119 : 0.46179245623777804
Loss in iteration 120 : 0.46176921503058316
Loss in iteration 121 : 0.46174624987492724
Loss in iteration 122 : 0.4617235796128253
Loss in iteration 123 : 0.46170119068809
Loss in iteration 124 : 0.4616790592992506
Loss in iteration 125 : 0.46165719189869503
Loss in iteration 126 : 0.46163558928820503
Loss in iteration 127 : 0.4616142304957584
Loss in iteration 128 : 0.4615931096151459
Loss in iteration 129 : 0.4615722323879557
Loss in iteration 130 : 0.4615515880606066
Loss in iteration 131 : 0.4615311651501434
Loss in iteration 132 : 0.4615109667965058
Loss in iteration 133 : 0.4614909902466852
Loss in iteration 134 : 0.4614712233449686
Loss in iteration 135 : 0.461451662809862
Loss in iteration 136 : 0.46143230890116455
Loss in iteration 137 : 0.46141315335341243
Loss in iteration 138 : 0.4613941895254218
Loss in iteration 139 : 0.4613754175091522
Loss in iteration 140 : 0.4613568334259165
Loss in iteration 141 : 0.461338430173211
Loss in iteration 142 : 0.46132020559181147
Loss in iteration 143 : 0.4613021578779623
Loss in iteration 144 : 0.4612842811590947
Loss in iteration 145 : 0.4612665714455461
Loss in iteration 146 : 0.4612490274644307
Loss in iteration 147 : 0.4612316454649434
Loss in iteration 148 : 0.4612144210726311
Loss in iteration 149 : 0.4611973526118896
Loss in iteration 150 : 0.4611804379159599
Loss in iteration 151 : 0.46116367306728945
Loss in iteration 152 : 0.4611470555324788
Loss in iteration 153 : 0.461130583660002
Loss in iteration 154 : 0.46111425443956655
Loss in iteration 155 : 0.4610980650628737
Loss in iteration 156 : 0.46108201397628445
Loss in iteration 157 : 0.4610660990670227
Loss in iteration 158 : 0.46105031768861604
Loss in iteration 159 : 0.4610346681016108
Loss in iteration 160 : 0.46101914870078986
Loss in iteration 161 : 0.4610037572028959
Loss in iteration 162 : 0.4609884916753788
Loss in iteration 163 : 0.46097335068894413
Loss in iteration 164 : 0.46095833242083084
Loss in iteration 165 : 0.46094343499346324
Loss in iteration 166 : 0.46092865702212765
Loss in iteration 167 : 0.46091399704021596
Loss in iteration 168 : 0.4608994533277901
Loss in iteration 169 : 0.46088502445095636
Loss in iteration 170 : 0.4608707091158957
Loss in iteration 171 : 0.4608565058087925
Loss in iteration 172 : 0.4608424131083316
Loss in iteration 173 : 0.46082842981870586
Loss in iteration 174 : 0.4608145546435595
Loss in iteration 175 : 0.4608007862433537
Loss in iteration 176 : 0.46078712346333334
Loss in iteration 177 : 0.46077356515860185
Loss in iteration 178 : 0.4607601100943603
Loss in iteration 179 : 0.4607467571378578
Loss in iteration 180 : 0.46073350523821244
Loss in iteration 181 : 0.4607203532835406
Loss in iteration 182 : 0.46070730019431416
Loss in iteration 183 : 0.460694344986377
Loss in iteration 184 : 0.4606814866562473
Loss in iteration 185 : 0.46066872418743193
Loss in iteration 186 : 0.460656056636988
Loss in iteration 187 : 0.46064348308119485
Loss in iteration 188 : 0.46063100257382456
Loss in iteration 189 : 0.46061861421279693
Loss in iteration 190 : 0.4606063171369484
Loss in iteration 191 : 0.46059411047238835
Loss in iteration 192 : 0.46058199336305233
Loss in iteration 193 : 0.4605699649954337
Loss in iteration 194 : 0.4605580245578051
Loss in iteration 195 : 0.46054617124064484
Loss in iteration 196 : 0.4605344042678399
Loss in iteration 197 : 0.460522722877946
Loss in iteration 198 : 0.4605111263089811
Loss in iteration 199 : 0.4604996138216536
Loss in iteration 200 : 0.4604881846973578
Testing accuracy  of updater 7 on alg 0 with rate 1.4 = 0.7855, training accuracy 0.788375, time elapsed: 4450 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6807196963988122
Loss in iteration 3 : 0.6693552087878889
Loss in iteration 4 : 0.6539728013651712
Loss in iteration 5 : 0.634782702139247
Loss in iteration 6 : 0.6189731565485345
Loss in iteration 7 : 0.6024115857961044
Loss in iteration 8 : 0.5835951766955042
Loss in iteration 9 : 0.5682833676009
Loss in iteration 10 : 0.5559225030779938
Loss in iteration 11 : 0.5438018886423008
Loss in iteration 12 : 0.5337416878290299
Loss in iteration 13 : 0.5265682958075385
Loss in iteration 14 : 0.5199804963298995
Loss in iteration 15 : 0.5134524611153116
Loss in iteration 16 : 0.5083787528273175
Loss in iteration 17 : 0.5046003574588598
Loss in iteration 18 : 0.5009661441937897
Loss in iteration 19 : 0.4975943569164739
Loss in iteration 20 : 0.49518501829074535
Loss in iteration 21 : 0.4934332805733197
Loss in iteration 22 : 0.4916319885162958
Loss in iteration 23 : 0.4898562512039669
Loss in iteration 24 : 0.4885143721556806
Loss in iteration 25 : 0.4874602537164543
Loss in iteration 26 : 0.4863013933521908
Loss in iteration 27 : 0.4850846106594235
Loss in iteration 28 : 0.4841054008621028
Loss in iteration 29 : 0.4833497472921051
Loss in iteration 30 : 0.48257295247981746
Loss in iteration 31 : 0.4817386989053006
Loss in iteration 32 : 0.4810100404153725
Loss in iteration 33 : 0.48040912837237854
Loss in iteration 34 : 0.4797852479504236
Loss in iteration 35 : 0.4790824940257301
Loss in iteration 36 : 0.47840544807036456
Loss in iteration 37 : 0.47781465277452034
Loss in iteration 38 : 0.4772391056588197
Loss in iteration 39 : 0.4766239637724282
Loss in iteration 40 : 0.47602256732885745
Loss in iteration 41 : 0.4754907440305406
Loss in iteration 42 : 0.4749947110331524
Loss in iteration 43 : 0.4744814804102837
Loss in iteration 44 : 0.4739660684257268
Loss in iteration 45 : 0.47349066397314
Loss in iteration 46 : 0.4730482231499202
Loss in iteration 47 : 0.4726032225391077
Loss in iteration 48 : 0.47215700784498577
Loss in iteration 49 : 0.4717410589957648
Loss in iteration 50 : 0.47136178467231954
Loss in iteration 51 : 0.4709960208245269
Loss in iteration 52 : 0.4706346630600035
Loss in iteration 53 : 0.4702935484294246
Loss in iteration 54 : 0.4699803089591675
Loss in iteration 55 : 0.4696809184879227
Loss in iteration 56 : 0.46938510531485067
Loss in iteration 57 : 0.46910144793118613
Loss in iteration 58 : 0.4688388547734184
Loss in iteration 59 : 0.46859123085239035
Loss in iteration 60 : 0.4683500862452892
Loss in iteration 61 : 0.4681187775960871
Loss in iteration 62 : 0.46790418545727275
Loss in iteration 63 : 0.4677037330675404
Loss in iteration 64 : 0.467510261991618
Loss in iteration 65 : 0.46732343716297964
Loss in iteration 66 : 0.4671475850737649
Loss in iteration 67 : 0.4669822994464412
Loss in iteration 68 : 0.46682301555037536
Loss in iteration 69 : 0.46666870624987644
Loss in iteration 70 : 0.46652233716885116
Loss in iteration 71 : 0.4663845538973047
Loss in iteration 72 : 0.4662524214991844
Loss in iteration 73 : 0.46612437863952755
Loss in iteration 74 : 0.46600189585174534
Loss in iteration 75 : 0.4658856754259566
Loss in iteration 76 : 0.4657738744668179
Loss in iteration 77 : 0.46566502889885886
Loss in iteration 78 : 0.46555989378556545
Loss in iteration 79 : 0.46545929538538944
Loss in iteration 80 : 0.465362353278134
Loss in iteration 81 : 0.4652679819789892
Loss in iteration 82 : 0.46517648756299557
Loss in iteration 83 : 0.465088517320281
Loss in iteration 84 : 0.4650036280623674
Loss in iteration 85 : 0.4649209653207263
Loss in iteration 86 : 0.46484050471364396
Loss in iteration 87 : 0.4647626553750276
Loss in iteration 88 : 0.4646872267959631
Loss in iteration 89 : 0.46461364981578973
Loss in iteration 90 : 0.46454184261151227
Loss in iteration 91 : 0.46447211625930934
Loss in iteration 92 : 0.4644044444616314
Loss in iteration 93 : 0.4643384476347096
Loss in iteration 94 : 0.46427398034990186
Loss in iteration 95 : 0.46421121027737694
Loss in iteration 96 : 0.46415015215233973
Loss in iteration 97 : 0.4640905541283545
Loss in iteration 98 : 0.4640322650666934
Loss in iteration 99 : 0.46397536893963387
Loss in iteration 100 : 0.46391990426078883
Loss in iteration 101 : 0.4638657264663736
Loss in iteration 102 : 0.46381271895602577
Loss in iteration 103 : 0.4637609224075756
Loss in iteration 104 : 0.46371037241350493
Loss in iteration 105 : 0.463660974955774
Loss in iteration 106 : 0.4636126250366465
Loss in iteration 107 : 0.4635653212454267
Loss in iteration 108 : 0.46351908298621197
Loss in iteration 109 : 0.4634738507741171
Loss in iteration 110 : 0.4634295445428612
Loss in iteration 111 : 0.46338615079147266
Loss in iteration 112 : 0.463343683374972
Loss in iteration 113 : 0.46330210764187896
Loss in iteration 114 : 0.4632613634907737
Loss in iteration 115 : 0.4632214283410871
Loss in iteration 116 : 0.4631823042172149
Loss in iteration 117 : 0.46314396546668074
Loss in iteration 118 : 0.4631063645357945
Loss in iteration 119 : 0.4630694753836109
Loss in iteration 120 : 0.4630332933745567
Loss in iteration 121 : 0.46299780043216676
Loss in iteration 122 : 0.4629629620401953
Loss in iteration 123 : 0.4629287550366325
Loss in iteration 124 : 0.46289517205661324
Loss in iteration 125 : 0.46286219878015156
Loss in iteration 126 : 0.46282980792578055
Loss in iteration 127 : 0.4627979771812265
Loss in iteration 128 : 0.4627666958899961
Loss in iteration 129 : 0.4627359513451039
Loss in iteration 130 : 0.4627057223657941
Loss in iteration 131 : 0.4626759899731666
Loss in iteration 132 : 0.4626467435142349
Loss in iteration 133 : 0.4626179724170793
Loss in iteration 134 : 0.4625896603793051
Loss in iteration 135 : 0.4625617914232262
Loss in iteration 136 : 0.4625343549811846
Loss in iteration 137 : 0.46250734131521914
Loss in iteration 138 : 0.46248073700751385
Loss in iteration 139 : 0.4624545283548617
Loss in iteration 140 : 0.4624287053407071
Loss in iteration 141 : 0.46240325921767295
Loss in iteration 142 : 0.4623781791079849
Loss in iteration 143 : 0.46235345369150505
Loss in iteration 144 : 0.46232907403429224
Loss in iteration 145 : 0.4623050323377124
Loss in iteration 146 : 0.462281319467233
Loss in iteration 147 : 0.46225792574583824
Loss in iteration 148 : 0.46223484301172524
Loss in iteration 149 : 0.4622120641293046
Loss in iteration 150 : 0.46218958129153237
Loss in iteration 151 : 0.4621673862961186
Loss in iteration 152 : 0.46214547192666267
Loss in iteration 153 : 0.4621238317999253
Loss in iteration 154 : 0.4621024592163759
Loss in iteration 155 : 0.46208134719188676
Loss in iteration 156 : 0.4620604893675261
Loss in iteration 157 : 0.46203988000937235
Loss in iteration 158 : 0.4620195132446537
Loss in iteration 159 : 0.4619993830046045
Loss in iteration 160 : 0.46197948363938723
Loss in iteration 161 : 0.461959809998501
Loss in iteration 162 : 0.4619403569304578
Loss in iteration 163 : 0.4619211191807188
Loss in iteration 164 : 0.46190209178109365
Loss in iteration 165 : 0.4618832701401942
Loss in iteration 166 : 0.4618646497181498
Loss in iteration 167 : 0.46184622592370406
Loss in iteration 168 : 0.4618279943620481
Loss in iteration 169 : 0.46180995092144966
Loss in iteration 170 : 0.46179209156984685
Loss in iteration 171 : 0.46177441227084376
Loss in iteration 172 : 0.4617569091407231
Loss in iteration 173 : 0.4617395785175038
Loss in iteration 174 : 0.4617224168291856
Loss in iteration 175 : 0.46170542052507635
Loss in iteration 176 : 0.46168858617298375
Loss in iteration 177 : 0.4616719105111969
Loss in iteration 178 : 0.4616553903637042
Loss in iteration 179 : 0.4616390225880318
Loss in iteration 180 : 0.46162280413709594
Loss in iteration 181 : 0.4616067320997481
Loss in iteration 182 : 0.4615908036482127
Loss in iteration 183 : 0.46157501599937906
Loss in iteration 184 : 0.46155936645147844
Loss in iteration 185 : 0.4615438524113106
Loss in iteration 186 : 0.46152847136004127
Loss in iteration 187 : 0.46151322082551416
Loss in iteration 188 : 0.4614980984050468
Loss in iteration 189 : 0.461483101784248
Loss in iteration 190 : 0.46146822871479
Loss in iteration 191 : 0.4614534769948141
Loss in iteration 192 : 0.4614388444831676
Loss in iteration 193 : 0.46142432911236314
Loss in iteration 194 : 0.46140992887398813
Loss in iteration 195 : 0.4613956418046065
Loss in iteration 196 : 0.4613814659940236
Loss in iteration 197 : 0.4613673995935028
Loss in iteration 198 : 0.46135344080586405
Loss in iteration 199 : 0.46133958787560264
Loss in iteration 200 : 0.461325839093933
Testing accuracy  of updater 7 on alg 0 with rate 0.8 = 0.7855, training accuracy 0.788125, time elapsed: 4636 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.688541461766785
Loss in iteration 3 : 0.6819333067477088
Loss in iteration 4 : 0.6754333365802974
Loss in iteration 5 : 0.669208342433061
Loss in iteration 6 : 0.6622230052823704
Loss in iteration 7 : 0.6538859952379342
Loss in iteration 8 : 0.6447439815758957
Loss in iteration 9 : 0.6358532225380421
Loss in iteration 10 : 0.627755900617945
Loss in iteration 11 : 0.6201290558520295
Loss in iteration 12 : 0.6123248820071432
Loss in iteration 13 : 0.6041108941216214
Loss in iteration 14 : 0.5958644727220134
Loss in iteration 15 : 0.5881633768233334
Loss in iteration 16 : 0.5812897075053799
Loss in iteration 17 : 0.5751054755016244
Loss in iteration 18 : 0.5693008204890028
Loss in iteration 19 : 0.5637009293734163
Loss in iteration 20 : 0.5583673773747209
Loss in iteration 21 : 0.5534725193327018
Loss in iteration 22 : 0.5491090921204582
Loss in iteration 23 : 0.5452055901611756
Loss in iteration 24 : 0.5415926564400936
Loss in iteration 25 : 0.5381344547310668
Loss in iteration 26 : 0.5348069425923856
Loss in iteration 27 : 0.531674600418584
Loss in iteration 28 : 0.5288076997996725
Loss in iteration 29 : 0.5262169320626852
Loss in iteration 30 : 0.5238494013032077
Loss in iteration 31 : 0.5216331650403884
Loss in iteration 32 : 0.519525482077395
Loss in iteration 33 : 0.5175287877723994
Loss in iteration 34 : 0.5156704572015065
Loss in iteration 35 : 0.5139691417634525
Loss in iteration 36 : 0.5124147966399781
Loss in iteration 37 : 0.510973790742154
Loss in iteration 38 : 0.5096102115231291
Loss in iteration 39 : 0.5083049172100302
Loss in iteration 40 : 0.5070594559138896
Loss in iteration 41 : 0.5058853763083764
Loss in iteration 42 : 0.504789504174595
Loss in iteration 43 : 0.503766198093429
Loss in iteration 44 : 0.5028003372205435
Loss in iteration 45 : 0.5018766114322178
Loss in iteration 46 : 0.5009873685025669
Loss in iteration 47 : 0.5001339365563212
Loss in iteration 48 : 0.49932183085922477
Loss in iteration 49 : 0.4985542810141655
Loss in iteration 50 : 0.4978287239406063
Loss in iteration 51 : 0.4971379758728204
Loss in iteration 52 : 0.4964743302895624
Loss in iteration 53 : 0.49583324683197066
Loss in iteration 54 : 0.4952142630282227
Loss in iteration 55 : 0.4946191020163483
Loss in iteration 56 : 0.4940488189435285
Loss in iteration 57 : 0.49350208286655084
Loss in iteration 58 : 0.49297550219374814
Loss in iteration 59 : 0.4924653607400459
Loss in iteration 60 : 0.4919693276705723
Loss in iteration 61 : 0.49148701078894946
Loss in iteration 62 : 0.4910192074325693
Loss in iteration 63 : 0.4905665884919948
Loss in iteration 64 : 0.49012877600415355
Loss in iteration 65 : 0.4897043291322596
Loss in iteration 66 : 0.48929146087834513
Loss in iteration 67 : 0.48888886823725775
Loss in iteration 68 : 0.4884961078515998
Loss in iteration 69 : 0.4881133585598268
Loss in iteration 70 : 0.4877408436406331
Loss in iteration 71 : 0.48737835398903173
Loss in iteration 72 : 0.487025161788294
Loss in iteration 73 : 0.4866803046055181
Loss in iteration 74 : 0.4863429841956291
Loss in iteration 75 : 0.48601280023118926
Loss in iteration 76 : 0.4856897002332172
Loss in iteration 77 : 0.4853737328121826
Loss in iteration 78 : 0.4850648006923979
Loss in iteration 79 : 0.4847625715256514
Loss in iteration 80 : 0.48446657188998166
Loss in iteration 81 : 0.4841763667956088
Loss in iteration 82 : 0.48389169082081684
Loss in iteration 83 : 0.4836124540900722
Loss in iteration 84 : 0.48333864182829117
Loss in iteration 85 : 0.48307019125950074
Loss in iteration 86 : 0.4828069282601849
Loss in iteration 87 : 0.48254859218126467
Loss in iteration 88 : 0.4822949154848183
Loss in iteration 89 : 0.48204569640093875
Loss in iteration 90 : 0.4818008194247533
Loss in iteration 91 : 0.48156022074232213
Loss in iteration 92 : 0.4813238315571483
Loss in iteration 93 : 0.48109154005338706
Loss in iteration 94 : 0.4808631928464
Loss in iteration 95 : 0.480638627317361
Loss in iteration 96 : 0.48041770794081173
Loss in iteration 97 : 0.4802003421948475
Loss in iteration 98 : 0.4799864693420811
Loss in iteration 99 : 0.4797760340354616
Loss in iteration 100 : 0.47956896418279116
Loss in iteration 101 : 0.4793651661291952
Loss in iteration 102 : 0.47916453670168563
Loss in iteration 103 : 0.47896698100815177
Loss in iteration 104 : 0.47877242335839254
Loss in iteration 105 : 0.47858080551108645
Loss in iteration 106 : 0.4783920756847017
Loss in iteration 107 : 0.47820617704411045
Loss in iteration 108 : 0.47802304307140026
Loss in iteration 109 : 0.4778426014329845
Loss in iteration 110 : 0.47766478222497233
Loss in iteration 111 : 0.4774895244658943
Loss in iteration 112 : 0.47731677699119546
Loss in iteration 113 : 0.4771464942093681
Loss in iteration 114 : 0.476978630368299
Loss in iteration 115 : 0.47681313622643473
Loss in iteration 116 : 0.47664995973151264
Loss in iteration 117 : 0.4764890494315851
Loss in iteration 118 : 0.476330357834432
Loss in iteration 119 : 0.4761738424981065
Loss in iteration 120 : 0.4760194645194736
Loss in iteration 121 : 0.4758671858302759
Loss in iteration 122 : 0.47571696721942086
Loss in iteration 123 : 0.47556876818106236
Loss in iteration 124 : 0.47542254831590935
Loss in iteration 125 : 0.47527826907227877
Loss in iteration 126 : 0.4751358946264901
Loss in iteration 127 : 0.47499539148164915
Loss in iteration 128 : 0.4748567272659058
Loss in iteration 129 : 0.4747198696340881
Loss in iteration 130 : 0.47458478593940456
Loss in iteration 131 : 0.47445144372116027
Loss in iteration 132 : 0.4743198115211151
Loss in iteration 133 : 0.47418985942127545
Loss in iteration 134 : 0.47406155898902486
Loss in iteration 135 : 0.4739348827556292
Loss in iteration 136 : 0.47380980363102765
Loss in iteration 137 : 0.4736862946240783
Loss in iteration 138 : 0.4735643289730324
Loss in iteration 139 : 0.4734438805090753
Loss in iteration 140 : 0.4733249239607656
Loss in iteration 141 : 0.47320743500154855
Loss in iteration 142 : 0.4730913900461408
Loss in iteration 143 : 0.47297676596283167
Loss in iteration 144 : 0.4728635398918658
Loss in iteration 145 : 0.4727516892562244
Loss in iteration 146 : 0.47264119191120646
Loss in iteration 147 : 0.4725320263002607
Loss in iteration 148 : 0.4724241715059822
Loss in iteration 149 : 0.47231760717433974
Loss in iteration 150 : 0.47221231337664293
Loss in iteration 151 : 0.4721082705021866
Loss in iteration 152 : 0.4720054592374054
Loss in iteration 153 : 0.4719038606209767
Loss in iteration 154 : 0.47180345611720875
Loss in iteration 155 : 0.471704227649073
Loss in iteration 156 : 0.47160615756950663
Loss in iteration 157 : 0.47150922859384603
Loss in iteration 158 : 0.4714134237374859
Loss in iteration 159 : 0.47131872629160215
Loss in iteration 160 : 0.471225119839381
Loss in iteration 161 : 0.4711325882888312
Loss in iteration 162 : 0.4710411158923297
Loss in iteration 163 : 0.47095068723750344
Loss in iteration 164 : 0.4708612872157523
Loss in iteration 165 : 0.4707729009882153
Loss in iteration 166 : 0.47068551396707575
Loss in iteration 167 : 0.4705991118169077
Loss in iteration 168 : 0.47051368046696884
Loss in iteration 169 : 0.4704292061200568
Loss in iteration 170 : 0.47034567524858206
Loss in iteration 171 : 0.4702630745786965
Loss in iteration 172 : 0.470181391071033
Loss in iteration 173 : 0.47010061190730973
Loss in iteration 174 : 0.47002072448661025
Loss in iteration 175 : 0.46994171642822935
Loss in iteration 176 : 0.46986357557438674
Loss in iteration 177 : 0.46978628998763067
Loss in iteration 178 : 0.46970984794227755
Loss in iteration 179 : 0.469634237913444
Loss in iteration 180 : 0.46955944856823717
Loss in iteration 181 : 0.4694854687615258
Loss in iteration 182 : 0.46941228753535963
Loss in iteration 183 : 0.4693398941189658
Loss in iteration 184 : 0.4692682779265234
Loss in iteration 185 : 0.4691974285519547
Loss in iteration 186 : 0.4691273357621306
Loss in iteration 187 : 0.46905798949072103
Loss in iteration 188 : 0.46898937983415756
Loss in iteration 189 : 0.46892149704955455
Loss in iteration 190 : 0.468854331553268
Loss in iteration 191 : 0.4687878739186565
Loss in iteration 192 : 0.46872211487246684
Loss in iteration 193 : 0.4686570452903656
Loss in iteration 194 : 0.46859265619265317
Loss in iteration 195 : 0.46852893874095874
Loss in iteration 196 : 0.4684658842359572
Loss in iteration 197 : 0.4684034841155519
Loss in iteration 198 : 0.4683417299527719
Loss in iteration 199 : 0.4682806134530473
Loss in iteration 200 : 0.46822012645099953
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.7855, training accuracy 0.786, time elapsed: 4490 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 4.855845284144978
Loss in iteration 3 : 4.932588061608754
Loss in iteration 4 : 1.4804913678049032
Loss in iteration 5 : 1.1069842418577838
Loss in iteration 6 : 0.7976234227949941
Loss in iteration 7 : 0.8172371760950751
Loss in iteration 8 : 0.8572964161473376
Loss in iteration 9 : 0.8436630400673937
Loss in iteration 10 : 0.7858796440778238
Loss in iteration 11 : 0.7165514312599902
Loss in iteration 12 : 0.6618541843399646
Loss in iteration 13 : 0.631089822429313
Loss in iteration 14 : 0.6285544225302216
Loss in iteration 15 : 0.6348439335914196
Loss in iteration 16 : 0.6474860388341638
Loss in iteration 17 : 0.6511573874959242
Loss in iteration 18 : 0.6712594616115503
Loss in iteration 19 : 0.6857601463553221
Loss in iteration 20 : 0.7383678023389098
Loss in iteration 21 : 0.7535748885309254
Loss in iteration 22 : 0.7830334340617197
Loss in iteration 23 : 0.7548960761957582
Loss in iteration 24 : 0.732076521004062
Loss in iteration 25 : 0.7010767868717392
Loss in iteration 26 : 0.6792776755744443
Loss in iteration 27 : 0.6603315097316281
Loss in iteration 28 : 0.6520374746877994
Loss in iteration 29 : 0.6439141501931783
Loss in iteration 30 : 0.6479278066149665
Loss in iteration 31 : 0.6478249747712952
Loss in iteration 32 : 0.6634967057905005
Loss in iteration 33 : 0.6670704311162206
Loss in iteration 34 : 0.6884518265210593
Loss in iteration 35 : 0.6898856540710238
Loss in iteration 36 : 0.705553127163588
Loss in iteration 37 : 0.7100767846666575
Loss in iteration 38 : 0.7126944963821162
Loss in iteration 39 : 0.7130377966462997
Loss in iteration 40 : 0.7015705036840031
Loss in iteration 41 : 0.6928539963884184
Loss in iteration 42 : 0.6771844703092713
Loss in iteration 43 : 0.664325947947387
Loss in iteration 44 : 0.6520061811353611
Loss in iteration 45 : 0.6394317341600736
Loss in iteration 46 : 0.6328914541042981
Loss in iteration 47 : 0.6226387153692857
Loss in iteration 48 : 0.6220785841059998
Loss in iteration 49 : 0.6145926396442056
Loss in iteration 50 : 0.6196880569845031
Loss in iteration 51 : 0.6142298851889626
Loss in iteration 52 : 0.6241194608893013
Loss in iteration 53 : 0.6183227563074857
Loss in iteration 54 : 0.630013781874095
Loss in iteration 55 : 0.623323004913398
Loss in iteration 56 : 0.6342137746404433
Loss in iteration 57 : 0.6324133022363335
Loss in iteration 58 : 0.6423619264715982
Loss in iteration 59 : 0.6459004613603208
Loss in iteration 60 : 0.652358080519524
Loss in iteration 61 : 0.6560860986057697
Loss in iteration 62 : 0.6567290025295764
Loss in iteration 63 : 0.6574902137175546
Loss in iteration 64 : 0.653602190951037
Loss in iteration 65 : 0.6516571399867798
Loss in iteration 66 : 0.6463901351390269
Loss in iteration 67 : 0.6434173853476396
Loss in iteration 68 : 0.6392772161779537
Loss in iteration 69 : 0.6367298122455965
Loss in iteration 70 : 0.6349207685196032
Loss in iteration 71 : 0.6335252363846817
Loss in iteration 72 : 0.6342273756926751
Loss in iteration 73 : 0.6340117501365117
Loss in iteration 74 : 0.636716701861987
Loss in iteration 75 : 0.6371365502577725
Loss in iteration 76 : 0.6409366241580794
Loss in iteration 77 : 0.6410725429181927
Loss in iteration 78 : 0.6450336586095917
Loss in iteration 79 : 0.6439374793889264
Loss in iteration 80 : 0.6474999910564985
Loss in iteration 81 : 0.6446116285453504
Loss in iteration 82 : 0.6477484469574925
Loss in iteration 83 : 0.6431825061302647
Loss in iteration 84 : 0.6462077210658481
Loss in iteration 85 : 0.6407516927852773
Loss in iteration 86 : 0.6440141434694318
Loss in iteration 87 : 0.6387907595593006
Loss in iteration 88 : 0.6424749030139542
Loss in iteration 89 : 0.6384251030017996
Loss in iteration 90 : 0.6424490130882934
Loss in iteration 91 : 0.6399662739026325
Loss in iteration 92 : 0.6439495942021947
Loss in iteration 93 : 0.6428375242820923
Loss in iteration 94 : 0.646201406391249
Loss in iteration 95 : 0.6458742066076838
Loss in iteration 96 : 0.6480904081119162
Loss in iteration 97 : 0.6478771934855702
Loss in iteration 98 : 0.6487410329557587
Loss in iteration 99 : 0.6481584977938767
Loss in iteration 100 : 0.647896631256182
Loss in iteration 101 : 0.6467731402247052
Loss in iteration 102 : 0.6459232727507518
Loss in iteration 103 : 0.6443503400448471
Loss in iteration 104 : 0.6435287898042918
Loss in iteration 105 : 0.6417174216546635
Loss in iteration 106 : 0.6414248258945056
Loss in iteration 107 : 0.639563767548145
Loss in iteration 108 : 0.6400913981922528
Loss in iteration 109 : 0.638260625734927
Loss in iteration 110 : 0.6396822514952196
Loss in iteration 111 : 0.6378327670930309
Loss in iteration 112 : 0.6400473019194812
Loss in iteration 113 : 0.6380403092560724
Loss in iteration 114 : 0.64083925138315
Loss in iteration 115 : 0.6385289154586622
Loss in iteration 116 : 0.6416695168016222
Loss in iteration 117 : 0.6389968302884351
Loss in iteration 118 : 0.6422621972685192
Loss in iteration 119 : 0.639310163945446
Loss in iteration 120 : 0.6425403675836167
Loss in iteration 121 : 0.6395123013945614
Loss in iteration 122 : 0.6426023233007127
Loss in iteration 123 : 0.6397351967453367
Loss in iteration 124 : 0.6426118620477529
Loss in iteration 125 : 0.640082008472245
Loss in iteration 126 : 0.6426813674268174
Loss in iteration 127 : 0.6405542903202913
Loss in iteration 128 : 0.6428157649196976
Loss in iteration 129 : 0.6410509523553043
Loss in iteration 130 : 0.6429301643785247
Loss in iteration 131 : 0.6414207458854037
Loss in iteration 132 : 0.6429104985939136
Loss in iteration 133 : 0.641531410650237
Loss in iteration 134 : 0.6426764595385223
Loss in iteration 135 : 0.6413222013425379
Loss in iteration 136 : 0.6422182873476359
Loss in iteration 137 : 0.6408215817050416
Loss in iteration 138 : 0.6415983544365113
Loss in iteration 139 : 0.640130065620144
Loss in iteration 140 : 0.6409250748275196
Loss in iteration 141 : 0.6393817997633789
Loss in iteration 142 : 0.6403150326412138
Loss in iteration 143 : 0.6387028831760135
Loss in iteration 144 : 0.6398589075701192
Loss in iteration 145 : 0.6381806569190883
Loss in iteration 146 : 0.6396012108236503
Loss in iteration 147 : 0.6378507962783021
Loss in iteration 148 : 0.6395370619837086
Loss in iteration 149 : 0.6377019377808525
Loss in iteration 150 : 0.6396235811357278
Loss in iteration 151 : 0.6376925037390123
Loss in iteration 152 : 0.6397997067168232
Loss in iteration 153 : 0.6377717586892706
Loss in iteration 154 : 0.6400066733442499
Loss in iteration 155 : 0.6378971588069283
Loss in iteration 156 : 0.6402023007667786
Loss in iteration 157 : 0.6380427280197145
Loss in iteration 158 : 0.6403654875165649
Loss in iteration 159 : 0.6381975831429421
Loss in iteration 160 : 0.6404916558654649
Loss in iteration 161 : 0.6383579184137608
Loss in iteration 162 : 0.6405834039668585
Loss in iteration 163 : 0.6385178690136616
Loss in iteration 164 : 0.6406417427768807
Loss in iteration 165 : 0.6386640437322305
Loss in iteration 166 : 0.6406618526146306
Loss in iteration 167 : 0.6387759346604396
Loss in iteration 168 : 0.6406344127944523
Loss in iteration 169 : 0.6388313812357522
Loss in iteration 170 : 0.6405508086096348
Loss in iteration 171 : 0.6388141320176817
Loss in iteration 172 : 0.6404090598081627
Loss in iteration 173 : 0.6387200053335962
Loss in iteration 174 : 0.6402174908969694
Loss in iteration 175 : 0.6385591388240915
Loss in iteration 176 : 0.6399945672912141
Loss in iteration 177 : 0.6383536738467119
Loss in iteration 178 : 0.6397651209178448
Loss in iteration 179 : 0.6381320347418298
Loss in iteration 180 : 0.6395545701748979
Loss in iteration 181 : 0.6379220177462341
Loss in iteration 182 : 0.6393832494599837
Loss in iteration 183 : 0.6377449534641911
Loss in iteration 184 : 0.6392626080139341
Loss in iteration 185 : 0.6376124614985386
Loss in iteration 186 : 0.639194143482011
Loss in iteration 187 : 0.6375262235744418
Loss in iteration 188 : 0.6391709291759348
Loss in iteration 189 : 0.6374802050476739
Loss in iteration 190 : 0.6391808289430106
Loss in iteration 191 : 0.6374641488496156
Loss in iteration 192 : 0.6392101651990625
Loss in iteration 193 : 0.6374670548775669
Loss in iteration 194 : 0.6392467334716739
Loss in iteration 195 : 0.6374796725573695
Loss in iteration 196 : 0.6392815106451788
Loss in iteration 197 : 0.6374955854518077
Loss in iteration 198 : 0.6393089683982203
Loss in iteration 199 : 0.6375110238340246
Loss in iteration 200 : 0.639326364316204
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.7075, training accuracy 0.7165, time elapsed: 4497 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.8588856873396297
Loss in iteration 3 : 1.3184387414907046
Loss in iteration 4 : 1.7326195005308143
Loss in iteration 5 : 0.6498326498240256
Loss in iteration 6 : 0.70308934438447
Loss in iteration 7 : 0.9034706909965514
Loss in iteration 8 : 0.9194348334909033
Loss in iteration 9 : 0.6540778414403706
Loss in iteration 10 : 0.5949308095103679
Loss in iteration 11 : 0.5531722343018486
Loss in iteration 12 : 0.5181357241354382
Loss in iteration 13 : 0.5028621288102508
Loss in iteration 14 : 0.4959629186739479
Loss in iteration 15 : 0.49433728086396855
Loss in iteration 16 : 0.4939950226353559
Loss in iteration 17 : 0.49384823168232184
Loss in iteration 18 : 0.4935245642965108
Loss in iteration 19 : 0.49296262665687957
Loss in iteration 20 : 0.4921450125796346
Loss in iteration 21 : 0.4910540732387214
Loss in iteration 22 : 0.48968374479382576
Loss in iteration 23 : 0.48805255335213643
Loss in iteration 24 : 0.48620469691544643
Loss in iteration 25 : 0.48420111243944
Loss in iteration 26 : 0.4821082101422439
Loss in iteration 27 : 0.47998941128539924
Loss in iteration 28 : 0.4779009151827298
Loss in iteration 29 : 0.4758902956420465
Loss in iteration 30 : 0.47399603217310704
Loss in iteration 31 : 0.4722470199007083
Loss in iteration 32 : 0.4706622411596609
Loss in iteration 33 : 0.46925119186900427
Loss in iteration 34 : 0.46801531660637347
Loss in iteration 35 : 0.4669501006142036
Loss in iteration 36 : 0.4660471473075065
Loss in iteration 37 : 0.4652956926792224
Loss in iteration 38 : 0.46468340641012024
Loss in iteration 39 : 0.4641966794115381
Loss in iteration 40 : 0.4638207311651193
Loss in iteration 41 : 0.46353979180034816
Loss in iteration 42 : 0.46333746370366374
Loss in iteration 43 : 0.4631972467847203
Loss in iteration 44 : 0.4631031603790548
Loss in iteration 45 : 0.4630403810453559
Loss in iteration 46 : 0.4629958159064365
Loss in iteration 47 : 0.46295853517779006
Loss in iteration 48 : 0.4629200075927625
Loss in iteration 49 : 0.46287411930764233
Loss in iteration 50 : 0.4628170007750075
Loss in iteration 51 : 0.4627467195097469
Loss in iteration 52 : 0.4626629083519425
Loss in iteration 53 : 0.46256639048989723
Loss in iteration 54 : 0.46245884316810093
Loss in iteration 55 : 0.46234252146757426
Loss in iteration 56 : 0.4622200544090651
Loss in iteration 57 : 0.4620943232087898
Loss in iteration 58 : 0.4619684613180334
Loss in iteration 59 : 0.46184605725513034
Loss in iteration 60 : 0.4617317811411197
Loss in iteration 61 : 0.461632888543817
Loss in iteration 62 : 0.46156264814461984
Loss in iteration 63 : 0.4615480215424457
Loss in iteration 64 : 0.46164636034492657
Loss in iteration 65 : 0.4619826212210308
Loss in iteration 66 : 0.46282417927939945
Loss in iteration 67 : 0.46473504386642184
Loss in iteration 68 : 0.4687678045523014
Loss in iteration 69 : 0.47657711479146403
Loss in iteration 70 : 0.4890196448494524
Loss in iteration 71 : 0.5035698897274771
Loss in iteration 72 : 0.5112423705920635
Loss in iteration 73 : 0.5098720878010784
Loss in iteration 74 : 0.5008857006807051
Loss in iteration 75 : 0.4927325079042025
Loss in iteration 76 : 0.48400199226341883
Loss in iteration 77 : 0.4785080699876223
Loss in iteration 78 : 0.47417089533593315
Loss in iteration 79 : 0.4726765791566598
Loss in iteration 80 : 0.4717899881652921
Loss in iteration 81 : 0.47267223662603314
Loss in iteration 82 : 0.4732841532245367
Loss in iteration 83 : 0.4747478589823048
Loss in iteration 84 : 0.4753536732598991
Loss in iteration 85 : 0.4757235383709012
Loss in iteration 86 : 0.4755407167341515
Loss in iteration 87 : 0.474313641181208
Loss in iteration 88 : 0.47383313778614333
Loss in iteration 89 : 0.4720667283125599
Loss in iteration 90 : 0.47235645129793946
Loss in iteration 91 : 0.47149105787227874
Loss in iteration 92 : 0.47359205906983437
Loss in iteration 93 : 0.47516031419290666
Loss in iteration 94 : 0.4806031256376465
Loss in iteration 95 : 0.4872013620537906
Loss in iteration 96 : 0.498533809933817
Loss in iteration 97 : 0.5143749200254994
Loss in iteration 98 : 0.5326644476153402
Loss in iteration 99 : 0.5590660391427423
Loss in iteration 100 : 0.5749044414490254
Loss in iteration 101 : 0.6001985484819476
Loss in iteration 102 : 0.5973632921742115
Loss in iteration 103 : 0.6076914772920129
Loss in iteration 104 : 0.589713502251076
Loss in iteration 105 : 0.5872286343833083
Loss in iteration 106 : 0.5673525344810939
Loss in iteration 107 : 0.5597808358370112
Loss in iteration 108 : 0.5438393262919985
Loss in iteration 109 : 0.5360961955346132
Loss in iteration 110 : 0.5248799040144928
Loss in iteration 111 : 0.5188991112488529
Loss in iteration 112 : 0.5115962660774175
Loss in iteration 113 : 0.5076872678515679
Loss in iteration 114 : 0.5032380907235215
Loss in iteration 115 : 0.5011080983135254
Loss in iteration 116 : 0.4986412640173741
Loss in iteration 117 : 0.4979214601678003
Loss in iteration 118 : 0.49685746797919195
Loss in iteration 119 : 0.49731463448628627
Loss in iteration 120 : 0.4973383594101545
Loss in iteration 121 : 0.49891475251799255
Loss in iteration 122 : 0.49988567531896
Loss in iteration 123 : 0.5026515327375825
Loss in iteration 124 : 0.5044845792937503
Loss in iteration 125 : 0.5085477461411901
Loss in iteration 126 : 0.5110818562738385
Loss in iteration 127 : 0.5164670421760952
Loss in iteration 128 : 0.5193356397477515
Loss in iteration 129 : 0.5258390548855861
Loss in iteration 130 : 0.5283927332544759
Loss in iteration 131 : 0.5354679736600515
Loss in iteration 132 : 0.5368618389003641
Loss in iteration 133 : 0.543664683830761
Loss in iteration 134 : 0.543178235538951
Loss in iteration 135 : 0.5488433959682526
Loss in iteration 136 : 0.5462586142369007
Loss in iteration 137 : 0.5502548958734278
Loss in iteration 138 : 0.5459619598369674
Loss in iteration 139 : 0.5482555323785849
Loss in iteration 140 : 0.5430053976621583
Loss in iteration 141 : 0.5439521981958182
Loss in iteration 142 : 0.5385078264482496
Loss in iteration 143 : 0.5386243459205425
Loss in iteration 144 : 0.5335476822269721
Loss in iteration 145 : 0.5333232521646605
Loss in iteration 146 : 0.528931272850201
Loss in iteration 147 : 0.5287390161012818
Loss in iteration 148 : 0.5251532364448448
Loss in iteration 149 : 0.525239343001653
Loss in iteration 150 : 0.5224575420533728
Loss in iteration 151 : 0.522968143469914
Loss in iteration 152 : 0.5209224684764853
Loss in iteration 153 : 0.5219367800511348
Loss in iteration 154 : 0.5205282444058534
Loss in iteration 155 : 0.5220820384762768
Loss in iteration 156 : 0.5211939278458766
Loss in iteration 157 : 0.5232890038581945
Loss in iteration 158 : 0.522786457469178
Loss in iteration 159 : 0.5253884720661193
Loss in iteration 160 : 0.5251131412786026
Loss in iteration 161 : 0.528143888314803
Loss in iteration 162 : 0.527913080174283
Loss in iteration 163 : 0.5312455796408961
Loss in iteration 164 : 0.5308640351153006
Loss in iteration 165 : 0.5343290276226458
Loss in iteration 166 : 0.5336163005028864
Loss in iteration 167 : 0.5370248517823293
Loss in iteration 168 : 0.5358517906810492
Loss in iteration 169 : 0.5390302926478303
Loss in iteration 170 : 0.5373493507324333
Loss in iteration 171 : 0.5401746436809756
Loss in iteration 172 : 0.5380286381014303
Loss in iteration 173 : 0.5404491243526668
Loss in iteration 174 : 0.5379538238355375
Loss in iteration 175 : 0.5399894934143682
Loss in iteration 176 : 0.5372990169974298
Loss in iteration 177 : 0.5390237869587292
Loss in iteration 178 : 0.536293987413271
Loss in iteration 179 : 0.5378101888215205
Loss in iteration 180 : 0.5351716721471519
Loss in iteration 181 : 0.5365862746876022
Loss in iteration 182 : 0.5341309340357707
Loss in iteration 183 : 0.5355388046753108
Loss in iteration 184 : 0.5333176621822767
Loss in iteration 185 : 0.5347926502342054
Loss in iteration 186 : 0.5328204827033343
Loss in iteration 187 : 0.5344124990906958
Loss in iteration 188 : 0.5326752109448306
Loss in iteration 189 : 0.5344108448505752
Loss in iteration 190 : 0.532873144235411
Loss in iteration 191 : 0.5347579317280972
Loss in iteration 192 : 0.5333703831304171
Loss in iteration 193 : 0.535391767574877
Loss in iteration 194 : 0.5340972365089465
Loss in iteration 195 : 0.5362280097050073
Loss in iteration 196 : 0.5349678193197953
Loss in iteration 197 : 0.5371701900121717
Loss in iteration 198 : 0.5358901260152734
Loss in iteration 199 : 0.5381205251819846
Loss in iteration 200 : 0.5367763988565979
Testing accuracy  of updater 8 on alg 0 with rate 1.4000000000000001 = 0.734, training accuracy 0.744625, time elapsed: 4464 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.7343441186945143
Loss in iteration 3 : 0.74526230137517
Loss in iteration 4 : 0.8394251497427182
Loss in iteration 5 : 0.7148817715593004
Loss in iteration 6 : 0.7102861933028337
Loss in iteration 7 : 0.6109221307233655
Loss in iteration 8 : 0.5673033291333354
Loss in iteration 9 : 0.5285698240420855
Loss in iteration 10 : 0.5049442887293198
Loss in iteration 11 : 0.49409904471982374
Loss in iteration 12 : 0.48850497945821975
Loss in iteration 13 : 0.4856440522664661
Loss in iteration 14 : 0.4837364181581886
Loss in iteration 15 : 0.4823709409809991
Loss in iteration 16 : 0.48134415837496974
Loss in iteration 17 : 0.4805326877236496
Loss in iteration 18 : 0.47984876706551016
Loss in iteration 19 : 0.47922923555060654
Loss in iteration 20 : 0.478628897360576
Loss in iteration 21 : 0.4780166956255185
Loss in iteration 22 : 0.4773732701384466
Loss in iteration 23 : 0.4766889422386097
Loss in iteration 24 : 0.4759617121509338
Loss in iteration 25 : 0.4751952548214545
Loss in iteration 26 : 0.4743970735038025
Loss in iteration 27 : 0.47357695551384116
Loss in iteration 28 : 0.4727457770825736
Loss in iteration 29 : 0.4719146177940356
Loss in iteration 30 : 0.4710941109956876
Loss in iteration 31 : 0.47029396819884556
Loss in iteration 32 : 0.4695226448192945
Loss in iteration 33 : 0.468787136677981
Loss in iteration 34 : 0.46809290118130087
Loss in iteration 35 : 0.4674438877357153
Loss in iteration 36 : 0.46684264919859764
Loss in iteration 37 : 0.4662904993830971
Loss in iteration 38 : 0.46578768446692265
Loss in iteration 39 : 0.4653335463641091
Loss in iteration 40 : 0.46492666845973446
Loss in iteration 41 : 0.4645650038067149
Loss in iteration 42 : 0.4642459907352387
Loss in iteration 43 : 0.46396666130305725
Loss in iteration 44 : 0.4637237460477241
Loss in iteration 45 : 0.463513776032549
Loss in iteration 46 : 0.4633331813204331
Loss in iteration 47 : 0.4631783839907535
Loss in iteration 48 : 0.4630458833902644
Loss in iteration 49 : 0.46293233121366345
Loss in iteration 50 : 0.4628345941638009
Loss in iteration 51 : 0.4627498024036649
Loss in iteration 52 : 0.4626753828195752
Loss in iteration 53 : 0.4626090771749416
Loss in iteration 54 : 0.462548946332393
Loss in iteration 55 : 0.46249336261727914
Loss in iteration 56 : 0.4624409929210058
Loss in iteration 57 : 0.46239077525861394
Loss in iteration 58 : 0.4623418912747868
Loss in iteration 59 : 0.4622937367690366
Loss in iteration 60 : 0.4622458918187167
Loss in iteration 61 : 0.46219809161740977
Loss in iteration 62 : 0.4621501987700178
Loss in iteration 63 : 0.4621021775103562
Loss in iteration 64 : 0.4620540701237211
Loss in iteration 65 : 0.4620059757448459
Loss in iteration 66 : 0.46195803163622035
Loss in iteration 67 : 0.46191039700930453
Loss in iteration 68 : 0.46186323941293517
Loss in iteration 69 : 0.4618167236677425
Loss in iteration 70 : 0.46177100326908066
Loss in iteration 71 : 0.46172621411742604
Loss in iteration 72 : 0.4616824703723382
Loss in iteration 73 : 0.46163986217287956
Loss in iteration 74 : 0.46159845493118395
Loss in iteration 75 : 0.461558289890291
Loss in iteration 76 : 0.4615193856423931
Loss in iteration 77 : 0.46148174032594236
Loss in iteration 78 : 0.4614453342544484
Loss in iteration 79 : 0.4614101327705446
Loss in iteration 80 : 0.46137608916077727
Loss in iteration 81 : 0.4613431475056657
Loss in iteration 82 : 0.4613112453733728
Loss in iteration 83 : 0.46128031629296445
Loss in iteration 84 : 0.4612502919647915
Loss in iteration 85 : 0.4612211041820417
Loss in iteration 86 : 0.4611926864502582
Loss in iteration 87 : 0.4611649753018182
Loss in iteration 88 : 0.46113791131097426
Loss in iteration 89 : 0.46111143982270303
Loss in iteration 90 : 0.46108551141536974
Loss in iteration 91 : 0.46106008212311017
Loss in iteration 92 : 0.46103511344847853
Loss in iteration 93 : 0.46101057219918107
Loss in iteration 94 : 0.4609864301842813
Loss in iteration 95 : 0.46096266380521883
Loss in iteration 96 : 0.4609392535753608
Loss in iteration 97 : 0.4609161835988796
Loss in iteration 98 : 0.4608934410359293
Loss in iteration 99 : 0.46087101557663046
Loss in iteration 100 : 0.4608488989417815
Loss in iteration 101 : 0.4608270844236172
Loss in iteration 102 : 0.4608055664757909
Loss in iteration 103 : 0.4607843403579597
Loss in iteration 104 : 0.4607634018371788
Loss in iteration 105 : 0.46074274694571293
Loss in iteration 106 : 0.46072237179275166
Loss in iteration 107 : 0.4607022724259859
Loss in iteration 108 : 0.4606824447378517
Loss in iteration 109 : 0.46066288441054526
Loss in iteration 110 : 0.4606435868935505
Loss in iteration 111 : 0.4606245474073256
Loss in iteration 112 : 0.46060576096701816
Loss in iteration 113 : 0.46058722242046607
Loss in iteration 114 : 0.46056892649530795
Loss in iteration 115 : 0.4605508678507203
Loss in iteration 116 : 0.46053304113003374
Loss in iteration 117 : 0.4605154410112563
Loss in iteration 118 : 0.4604980622532976
Loss in iteration 119 : 0.4604808997363405
Loss in iteration 120 : 0.4604639484954875
Loss in iteration 121 : 0.46044720374726056
Loss in iteration 122 : 0.4604306609090156
Loss in iteration 123 : 0.46041431561162055
Loss in iteration 124 : 0.4603981637059905
Loss in iteration 125 : 0.4603822012642221
Loss in iteration 126 : 0.4603664245761358
Loss in iteration 127 : 0.4603508301420945
Loss in iteration 128 : 0.46033541466290506
Loss in iteration 129 : 0.4603201750276024
Loss in iteration 130 : 0.4603051082998187
Loss in iteration 131 : 0.46029021170336926
Loss in iteration 132 : 0.46027548260759604
Loss in iteration 133 : 0.46026091851291845
Loss in iteration 134 : 0.4602465170369369
Loss in iteration 135 : 0.46023227590136856
Loss in iteration 136 : 0.4602181929200035
Loss in iteration 137 : 0.4602042659878049
Loss in iteration 138 : 0.46019049307119997
Loss in iteration 139 : 0.46017687219958886
Loss in iteration 140 : 0.46016340145802703
Loss in iteration 141 : 0.4601500789810197
Loss in iteration 142 : 0.46013690294734494
Loss in iteration 143 : 0.4601238715758035
Loss in iteration 144 : 0.46011098312178716
Loss in iteration 145 : 0.46009823587454374
Loss in iteration 146 : 0.4600856281550502
Loss in iteration 147 : 0.4600731583143616
Loss in iteration 148 : 0.4600608247323614
Loss in iteration 149 : 0.46004862581680406
Loss in iteration 150 : 0.46003656000259224
Loss in iteration 151 : 0.46002462575120107
Loss in iteration 152 : 0.4600128215502095
Loss in iteration 153 : 0.4600011459128749
Loss in iteration 154 : 0.4599895973777371
Loss in iteration 155 : 0.4599781745082014
Loss in iteration 156 : 0.4599668758920973
Loss in iteration 157 : 0.4599557001411951
Loss in iteration 158 : 0.45994464589068257
Loss in iteration 159 : 0.45993371179857895
Loss in iteration 160 : 0.45992289654512714
Loss in iteration 161 : 0.4599121988321264
Loss in iteration 162 : 0.4599016173822501
Loss in iteration 163 : 0.45989115093833965
Loss in iteration 164 : 0.4598807982626816
Loss in iteration 165 : 0.45987055813629185
Loss in iteration 166 : 0.45986042935820226
Loss in iteration 167 : 0.45985041074476535
Loss in iteration 168 : 0.45984050112897995
Loss in iteration 169 : 0.4598306993598465
Loss in iteration 170 : 0.45982100430176104
Loss in iteration 171 : 0.45981141483393323
Loss in iteration 172 : 0.45980192984985785
Loss in iteration 173 : 0.45979254825681387
Loss in iteration 174 : 0.4597832689754102
Loss in iteration 175 : 0.45977409093916866
Loss in iteration 176 : 0.4597650130941381
Loss in iteration 177 : 0.4597560343985504
Loss in iteration 178 : 0.4597471538225054
Loss in iteration 179 : 0.45973837034768045
Loss in iteration 180 : 0.459729682967072
Loss in iteration 181 : 0.45972109068475675
Loss in iteration 182 : 0.45971259251566854
Loss in iteration 183 : 0.4597041874853966
Loss in iteration 184 : 0.4596958746300002
Loss in iteration 185 : 0.45968765299582903
Loss in iteration 186 : 0.4596795216393523
Loss in iteration 187 : 0.4596714796270029
Loss in iteration 188 : 0.4596635260350247
Loss in iteration 189 : 0.45965565994932084
Loss in iteration 190 : 0.4596478804653134
Loss in iteration 191 : 0.45964018668780254
Loss in iteration 192 : 0.4596325777308314
Loss in iteration 193 : 0.45962505271755005
Loss in iteration 194 : 0.4596176107800892
Loss in iteration 195 : 0.45961025105942993
Loss in iteration 196 : 0.45960297270527883
Loss in iteration 197 : 0.45959577487594977
Loss in iteration 198 : 0.45958865673823873
Loss in iteration 199 : 0.45958161746731424
Loss in iteration 200 : 0.4595746562466027
Testing accuracy  of updater 8 on alg 0 with rate 0.8 = 0.788, training accuracy 0.789625, time elapsed: 4487 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6772652651010135
Loss in iteration 3 : 0.6655397962844368
Loss in iteration 4 : 0.6517844575731242
Loss in iteration 5 : 0.6375617861590457
Loss in iteration 6 : 0.6234804988473738
Loss in iteration 7 : 0.6095415434251968
Loss in iteration 8 : 0.5960843498755607
Loss in iteration 9 : 0.5834855625324767
Loss in iteration 10 : 0.5719507334019825
Loss in iteration 11 : 0.561543189400785
Loss in iteration 12 : 0.5522379594193362
Loss in iteration 13 : 0.5439600228815041
Loss in iteration 14 : 0.5366142935673913
Loss in iteration 15 : 0.5301051333623513
Loss in iteration 16 : 0.5243446388616424
Loss in iteration 17 : 0.5192537735665792
Loss in iteration 18 : 0.5147607680376366
Loss in iteration 19 : 0.5107993272998982
Loss in iteration 20 : 0.5073075898255435
Loss in iteration 21 : 0.5042279163062048
Loss in iteration 22 : 0.5015072082080139
Loss in iteration 23 : 0.49909737841374857
Loss in iteration 24 : 0.49695569008647056
Loss in iteration 25 : 0.4950448275710056
Loss in iteration 26 : 0.4933326854581124
Loss in iteration 27 : 0.49179193298837554
Loss in iteration 28 : 0.4903994386771361
Loss in iteration 29 : 0.4891356407546438
Loss in iteration 30 : 0.4879839351867033
Loss in iteration 31 : 0.4869301321271289
Loss in iteration 32 : 0.4859620085363693
Loss in iteration 33 : 0.4850689632623708
Loss in iteration 34 : 0.4842417641803079
Loss in iteration 35 : 0.48347236685046335
Loss in iteration 36 : 0.48275378085317366
Loss in iteration 37 : 0.48207996231731975
Loss in iteration 38 : 0.48144571704015476
Loss in iteration 39 : 0.4808466056211645
Loss in iteration 40 : 0.48027884823041284
Loss in iteration 41 : 0.4797392308342881
Loss in iteration 42 : 0.47922501660750927
Loss in iteration 43 : 0.47873386623784914
Loss in iteration 44 : 0.478263769597218
Loss in iteration 45 : 0.4778129895710792
Loss in iteration 46 : 0.4773800173032481
Loss in iteration 47 : 0.47696353706711614
Loss in iteration 48 : 0.47656239851620014
Loss in iteration 49 : 0.4761755941208788
Loss in iteration 50 : 0.47580223999747123
Loss in iteration 51 : 0.4754415588942531
Loss in iteration 52 : 0.4750928646587495
Loss in iteration 53 : 0.4747555479642096
Loss in iteration 54 : 0.47442906336759644
Loss in iteration 55 : 0.47411291790047083
Loss in iteration 56 : 0.4738066613851462
Loss in iteration 57 : 0.4735098785675466
Loss in iteration 58 : 0.473222183016909
Loss in iteration 59 : 0.4729432126062493
Loss in iteration 60 : 0.4726726262887095
Loss in iteration 61 : 0.4724101018393526
Loss in iteration 62 : 0.47215533423984873
Loss in iteration 63 : 0.47190803443327517
Loss in iteration 64 : 0.4716679282499931
Loss in iteration 65 : 0.47143475538473867
Loss in iteration 66 : 0.4712082683743539
Loss in iteration 67 : 0.4709882315758739
Loss in iteration 68 : 0.47077442017289356
Loss in iteration 69 : 0.4705666192465936
Loss in iteration 70 : 0.4703646229419596
Loss in iteration 71 : 0.4701682337461478
Loss in iteration 72 : 0.4699772618807893
Loss in iteration 73 : 0.46979152479741887
Loss in iteration 74 : 0.46961084675760806
Loss in iteration 75 : 0.4694350584770339
Loss in iteration 76 : 0.4692639968146915
Loss in iteration 77 : 0.46909750449308135
Loss in iteration 78 : 0.46893542984064346
Loss in iteration 79 : 0.4687776265525663
Loss in iteration 80 : 0.46862395346945185
Loss in iteration 81 : 0.46847427437483247
Loss in iteration 82 : 0.468328457812525
Loss in iteration 83 : 0.46818637692368426
Loss in iteration 84 : 0.4680479093018211
Loss in iteration 85 : 0.46791293686264473
Loss in iteration 86 : 0.46778134572460683
Loss in iteration 87 : 0.46765302609578474
Loss in iteration 88 : 0.46752787216315644
Loss in iteration 89 : 0.4674057819812004
Loss in iteration 90 : 0.4672866573579318
Loss in iteration 91 : 0.4671704037376302
Loss in iteration 92 : 0.4670569300805295
Loss in iteration 93 : 0.46694614874040463
Loss in iteration 94 : 0.46683797534136984
Loss in iteration 95 : 0.46673232865524333
Loss in iteration 96 : 0.4666291304807162
Loss in iteration 97 : 0.4665283055252412
Loss in iteration 98 : 0.46642978129027796
Loss in iteration 99 : 0.46633348796021545
Loss in iteration 100 : 0.466239358295104
Loss in iteration 101 : 0.4661473275271571
Loss in iteration 102 : 0.4660573332609483
Loss in iteration 103 : 0.4659693153771947
Loss in iteration 104 : 0.46588321594004717
Loss in iteration 105 : 0.46579897910781837
Loss in iteration 106 : 0.46571655104710735
Loss in iteration 107 : 0.465635879850276
Loss in iteration 108 : 0.46555691545622335
Loss in iteration 109 : 0.46547960957438955
Loss in iteration 110 : 0.4654039156118939
Loss in iteration 111 : 0.4653297886036989
Loss in iteration 112 : 0.46525718514568165
Loss in iteration 113 : 0.4651860633304802
Loss in iteration 114 : 0.4651163826860208
Loss in iteration 115 : 0.46504810411659403
Loss in iteration 116 : 0.4649811898464104
Loss in iteration 117 : 0.464915603365556
Loss in iteration 118 : 0.4648513093782662
Loss in iteration 119 : 0.46478827375347653
Loss in iteration 120 : 0.46472646347757507
Loss in iteration 121 : 0.464665846609292
Loss in iteration 122 : 0.46460639223666994
Loss in iteration 123 : 0.46454807043602653
Loss in iteration 124 : 0.46449085223283776
Loss in iteration 125 : 0.46443470956445104
Loss in iteration 126 : 0.4643796152445431
Loss in iteration 127 : 0.46432554292924133
Loss in iteration 128 : 0.46427246708480685
Loss in iteration 129 : 0.46422036295681984
Loss in iteration 130 : 0.4641692065407642
Loss in iteration 131 : 0.4641189745539539
Loss in iteration 132 : 0.4640696444087262
Loss in iteration 133 : 0.46402119418683035
Loss in iteration 134 : 0.4639736026149592
Loss in iteration 135 : 0.4639268490413598
Loss in iteration 136 : 0.46388091341347193
Loss in iteration 137 : 0.46383577625654
Loss in iteration 138 : 0.4637914186531546
Loss in iteration 139 : 0.46374782222368
Loss in iteration 140 : 0.4637049691075118
Loss in iteration 141 : 0.4636628419451474
Loss in iteration 142 : 0.4636214238610079
Loss in iteration 143 : 0.4635806984469991
Loss in iteration 144 : 0.4635406497467499
Loss in iteration 145 : 0.46350126224052424
Loss in iteration 146 : 0.46346252083075584
Loss in iteration 147 : 0.4634244108281824
Loss in iteration 148 : 0.4633869179385525
Loss in iteration 149 : 0.4633500282498744
Loss in iteration 150 : 0.46331372822017997
Loss in iteration 151 : 0.4632780046657839
Loss in iteration 152 : 0.4632428447500061
Loss in iteration 153 : 0.4632082359723412
Loss in iteration 154 : 0.4631741661580486
Loss in iteration 155 : 0.4631406234481466
Loss in iteration 156 : 0.4631075962897824
Loss in iteration 157 : 0.46307507342697524
Loss in iteration 158 : 0.46304304389169537
Loss in iteration 159 : 0.46301149699528044
Loss in iteration 160 : 0.4629804223201588
Loss in iteration 161 : 0.46294980971187694
Loss in iteration 162 : 0.46291964927140894
Loss in iteration 163 : 0.4628899313477441
Loss in iteration 164 : 0.46286064653072184
Loss in iteration 165 : 0.4628317856441326
Loss in iteration 166 : 0.4628033397390386
Loss in iteration 167 : 0.4627753000873354
Loss in iteration 168 : 0.4627476581755235
Loss in iteration 169 : 0.4627204056986887
Loss in iteration 170 : 0.4626935345546867
Loss in iteration 171 : 0.4626670368385129
Loss in iteration 172 : 0.4626409048368572
Loss in iteration 173 : 0.4626151310228405
Loss in iteration 174 : 0.46258970805091354
Loss in iteration 175 : 0.46256462875191584
Loss in iteration 176 : 0.46253988612830327
Loss in iteration 177 : 0.46251547334951015
Loss in iteration 178 : 0.46249138374746623
Loss in iteration 179 : 0.4624676108122506
Loss in iteration 180 : 0.46244414818787355
Loss in iteration 181 : 0.46242098966819506
Loss in iteration 182 : 0.46239812919296186
Loss in iteration 183 : 0.4623755608439663
Loss in iteration 184 : 0.462353278841321
Loss in iteration 185 : 0.46233127753984204
Loss in iteration 186 : 0.46230955142554314
Loss in iteration 187 : 0.4622880951122311
Loss in iteration 188 : 0.4622669033382
Loss in iteration 189 : 0.46224597096302666
Loss in iteration 190 : 0.4622252929644529
Loss in iteration 191 : 0.4622048644353653
Loss in iteration 192 : 0.462184680580857
Loss in iteration 193 : 0.46216473671537267
Loss in iteration 194 : 0.46214502825994225
Loss in iteration 195 : 0.46212555073948547
Loss in iteration 196 : 0.4621062997801963
Loss in iteration 197 : 0.4620872711070027
Loss in iteration 198 : 0.46206846054109346
Loss in iteration 199 : 0.46204986399751696
Loss in iteration 200 : 0.4620314774828506
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.7855, training accuracy 0.790625, time elapsed: 5470 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6786223556903926
Loss in iteration 3 : 0.6680527006305247
Loss in iteration 4 : 0.6561310092190936
Loss in iteration 5 : 0.64323249229681
Loss in iteration 6 : 0.6303920394925085
Loss in iteration 7 : 0.6177254370280746
Loss in iteration 8 : 0.6052922432236105
Loss in iteration 9 : 0.5933700435485045
Loss in iteration 10 : 0.582219412092445
Loss in iteration 11 : 0.5719719855094957
Loss in iteration 12 : 0.5626592679585665
Loss in iteration 13 : 0.5542549847665404
Loss in iteration 14 : 0.5467004324232595
Loss in iteration 15 : 0.5399222491222795
Loss in iteration 16 : 0.5338456267721204
Loss in iteration 17 : 0.5284013497315028
Loss in iteration 18 : 0.5235274356617278
Loss in iteration 19 : 0.5191680643504542
Loss in iteration 20 : 0.5152719868127963
Loss in iteration 21 : 0.5117913851976543
Loss in iteration 22 : 0.508681360071842
Loss in iteration 23 : 0.5058999089390339
Loss in iteration 24 : 0.5034081766181325
Loss in iteration 25 : 0.501170766088597
Loss in iteration 26 : 0.4991559558716431
Loss in iteration 27 : 0.4973357482264189
Loss in iteration 28 : 0.49568574081211625
Loss in iteration 29 : 0.49418485627673303
Loss in iteration 30 : 0.49281498048671174
Loss in iteration 31 : 0.49156056012600136
Loss in iteration 32 : 0.4904082025667534
Loss in iteration 33 : 0.4893463098621988
Loss in iteration 34 : 0.48836476658856903
Loss in iteration 35 : 0.4874546895159363
Loss in iteration 36 : 0.48660823709153295
Loss in iteration 37 : 0.48581846957949953
Loss in iteration 38 : 0.4850792469387023
Loss in iteration 39 : 0.48438515097130946
Loss in iteration 40 : 0.4837314202152941
Loss in iteration 41 : 0.4831138894465091
Loss in iteration 42 : 0.4825289294263783
Loss in iteration 43 : 0.48197338580616594
Loss in iteration 44 : 0.48144451835208896
Loss in iteration 45 : 0.48093994273772883
Loss in iteration 46 : 0.4804575772178194
Loss in iteration 47 : 0.4799955958835498
Loss in iteration 48 : 0.4795523892797321
Loss in iteration 49 : 0.47912653225510865
Loss in iteration 50 : 0.4787167582237471
Loss in iteration 51 : 0.47832193862403943
Loss in iteration 52 : 0.4779410662632604
Loss in iteration 53 : 0.4775732413617735
Loss in iteration 54 : 0.4772176593709431
Loss in iteration 55 : 0.476873599945789
Loss in iteration 56 : 0.47654041674020003
Loss in iteration 57 : 0.4762175279174505
Loss in iteration 58 : 0.4759044074138833
Loss in iteration 59 : 0.47560057706004943
Loss in iteration 60 : 0.4753055996651389
Loss in iteration 61 : 0.47501907312765085
Loss in iteration 62 : 0.47474062557004904
Loss in iteration 63 : 0.4744699114275775
Loss in iteration 64 : 0.4742066083664093
Loss in iteration 65 : 0.4739504148728282
Loss in iteration 66 : 0.47370104834574794
Loss in iteration 67 : 0.4734582435371669
Loss in iteration 68 : 0.47322175121281185
Loss in iteration 69 : 0.47299133694093765
Loss in iteration 70 : 0.47276677995340965
Loss in iteration 71 : 0.47254787205427723
Loss in iteration 72 : 0.4723344165735832
Loss in iteration 73 : 0.47212622737712107
Loss in iteration 74 : 0.4719231279472336
Loss in iteration 75 : 0.47172495054767466
Loss in iteration 76 : 0.4715315354797844
Loss in iteration 77 : 0.47134273043014935
Loss in iteration 78 : 0.4711583899036543
Loss in iteration 79 : 0.47097837473141535
Loss in iteration 80 : 0.4708025516409919
Loss in iteration 81 : 0.47063079287635584
Loss in iteration 82 : 0.47046297585674984
Loss in iteration 83 : 0.4702989828660845
Loss in iteration 84 : 0.47013870076726744
Loss in iteration 85 : 0.4699820207382437
Loss in iteration 86 : 0.46982883802824277
Loss in iteration 87 : 0.4696790517337298
Loss in iteration 88 : 0.46953256459380915
Loss in iteration 89 : 0.4693892828045922
Loss in iteration 90 : 0.4692491158515247
Loss in iteration 91 : 0.46911197635806656
Loss in iteration 92 : 0.46897777994863976
Loss in iteration 93 : 0.46884644512349744
Loss in iteration 94 : 0.46871789314315143
Loss in iteration 95 : 0.4685920479202239
Loss in iteration 96 : 0.46846883591696403
Loss in iteration 97 : 0.4683481860471335
Loss in iteration 98 : 0.46823002958144994
Loss in iteration 99 : 0.4681143000561677
Loss in iteration 100 : 0.4680009331846686
Loss in iteration 101 : 0.4678898667721654
Loss in iteration 102 : 0.46778104063365766
Loss in iteration 103 : 0.4676743965152889
Loss in iteration 104 : 0.46756987801921485
Loss in iteration 105 : 0.4674674305319574
Loss in iteration 106 : 0.467367001156196
Loss in iteration 107 : 0.46726853864580825
Loss in iteration 108 : 0.4671719933439978
Loss in iteration 109 : 0.4670773171242643
Loss in iteration 110 : 0.46698446333401894
Loss in iteration 111 : 0.4668933867406676
Loss in iteration 112 : 0.4668040434799724
Loss in iteration 113 : 0.46671639100658885
Loss in iteration 114 : 0.46663038804664553
Loss in iteration 115 : 0.4665459945522939
Loss in iteration 116 : 0.4664631716581306
Loss in iteration 117 : 0.46638188163943023
Loss in iteration 118 : 0.46630208787209165
Loss in iteration 119 : 0.46622375479424033
Loss in iteration 120 : 0.46614684786938415
Loss in iteration 121 : 0.4660713335510634
Loss in iteration 122 : 0.4659971792488975
Loss in iteration 123 : 0.4659243532959666
Loss in iteration 124 : 0.46585282491746394
Loss in iteration 125 : 0.465782564200549
Loss in iteration 126 : 0.46571354206536025
Loss in iteration 127 : 0.46564573023713524
Loss in iteration 128 : 0.4655791012193928
Loss in iteration 129 : 0.4655136282681538
Loss in iteration 130 : 0.4654492853671443
Loss in iteration 131 : 0.46538604720396437
Loss in iteration 132 : 0.4653238891471749
Loss in iteration 133 : 0.46526278722426523
Loss in iteration 134 : 0.46520271810048586
Loss in iteration 135 : 0.465143659058478
Loss in iteration 136 : 0.4650855879786886
Loss in iteration 137 : 0.46502848332053415
Loss in iteration 138 : 0.4649723241042644
Loss in iteration 139 : 0.46491708989350916
Loss in iteration 140 : 0.46486276077846767
Loss in iteration 141 : 0.46480931735971864
Loss in iteration 142 : 0.4647567407326224
Loss in iteration 143 : 0.46470501247227813
Loss in iteration 144 : 0.46465411461903383
Loss in iteration 145 : 0.464604029664503
Loss in iteration 146 : 0.46455474053808365
Loss in iteration 147 : 0.4645062305939503
Loss in iteration 148 : 0.46445848359850017
Loss in iteration 149 : 0.4644114837182393
Loss in iteration 150 : 0.4643652155080851
Loss in iteration 151 : 0.4643196639000749
Loss in iteration 152 : 0.46427481419246125
Loss in iteration 153 : 0.4642306520391752
Loss in iteration 154 : 0.46418716343965144
Loss in iteration 155 : 0.4641443347289969
Loss in iteration 156 : 0.46410215256848697
Loss in iteration 157 : 0.4640606039363835
Loss in iteration 158 : 0.46401967611906003
Loss in iteration 159 : 0.4639793567024132
Loss in iteration 160 : 0.46393963356357326
Loss in iteration 161 : 0.4639004948628704
Loss in iteration 162 : 0.46386192903607587
Loss in iteration 163 : 0.46382392478689427
Loss in iteration 164 : 0.4637864710796874
Loss in iteration 165 : 0.4637495571324538
Loss in iteration 166 : 0.4637131724100107
Loss in iteration 167 : 0.46367730661740675
Loss in iteration 168 : 0.4636419496935421
Loss in iteration 169 : 0.46360709180498305
Loss in iteration 170 : 0.46357272333997684
Loss in iteration 171 : 0.4635388349026464
Loss in iteration 172 : 0.46350541730737105
Loss in iteration 173 : 0.4634724615733366
Loss in iteration 174 : 0.46343995891924966
Loss in iteration 175 : 0.4634079007582195
Loss in iteration 176 : 0.4633762786927862
Loss in iteration 177 : 0.46334508451010403
Loss in iteration 178 : 0.46331431017726915
Loss in iteration 179 : 0.4632839478367791
Loss in iteration 180 : 0.46325398980213695
Loss in iteration 181 : 0.46322442855357493
Loss in iteration 182 : 0.46319525673390877
Loss in iteration 183 : 0.46316646714451143
Loss in iteration 184 : 0.4631380527414019
Loss in iteration 185 : 0.4631100066314461
Loss in iteration 186 : 0.46308232206866945
Loss in iteration 187 : 0.46305499245067283
Loss in iteration 188 : 0.46302801131514754
Loss in iteration 189 : 0.4630013723364915
Loss in iteration 190 : 0.46297506932252075
Loss in iteration 191 : 0.46294909621126906
Loss in iteration 192 : 0.46292344706787625
Loss in iteration 193 : 0.4628981160815712
Loss in iteration 194 : 0.4628730975627236
Loss in iteration 195 : 0.4628483859399878
Loss in iteration 196 : 0.4628239757575201
Loss in iteration 197 : 0.46279986167226705
Loss in iteration 198 : 0.4627760384513382
Loss in iteration 199 : 0.46275250096943443
Loss in iteration 200 : 0.4627292442063597
Testing accuracy  of updater 8 on alg 0 with rate 0.14 = 0.7865, training accuracy 0.791125, time elapsed: 5037 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6822437256366517
Loss in iteration 3 : 0.6734908357350042
Loss in iteration 4 : 0.6651018554247321
Loss in iteration 5 : 0.6557890348844828
Loss in iteration 6 : 0.6457896619470263
Loss in iteration 7 : 0.635721923681519
Loss in iteration 8 : 0.6258460434207106
Loss in iteration 9 : 0.6161544666011816
Loss in iteration 10 : 0.6066637608716781
Loss in iteration 11 : 0.59749317788996
Loss in iteration 12 : 0.5887836551957318
Loss in iteration 13 : 0.5806257615353163
Loss in iteration 14 : 0.5730504938620066
Loss in iteration 15 : 0.5660514199745531
Loss in iteration 16 : 0.5596045382232294
Loss in iteration 17 : 0.5536774987801548
Loss in iteration 18 : 0.548233349563806
Loss in iteration 19 : 0.5432335574199577
Loss in iteration 20 : 0.5386409920642945
Loss in iteration 21 : 0.5344218294737111
Loss in iteration 22 : 0.5305459202001669
Loss in iteration 23 : 0.5269861297623925
Loss in iteration 24 : 0.5237174349653292
Loss in iteration 25 : 0.5207162646032855
Loss in iteration 26 : 0.5179601875063643
Loss in iteration 27 : 0.515427847802776
Loss in iteration 28 : 0.5130990230648419
Loss in iteration 29 : 0.5109547267105339
Loss in iteration 30 : 0.5089773144930525
Loss in iteration 31 : 0.5071505699675546
Loss in iteration 32 : 0.5054597493773073
Loss in iteration 33 : 0.5038915746940201
Loss in iteration 34 : 0.5024341753672709
Loss in iteration 35 : 0.5010769894924145
Loss in iteration 36 : 0.4998106398034872
Loss in iteration 37 : 0.4986267993031541
Loss in iteration 38 : 0.4975180579742097
Loss in iteration 39 : 0.49647779828461824
Loss in iteration 40 : 0.49550008420882335
Loss in iteration 41 : 0.49457956626075134
Loss in iteration 42 : 0.4937114032339691
Loss in iteration 43 : 0.4928911997948756
Loss in iteration 44 : 0.4921149578404511
Loss in iteration 45 : 0.491379038760857
Loss in iteration 46 : 0.49068013350856543
Loss in iteration 47 : 0.4900152376087217
Loss in iteration 48 : 0.48938162880069136
Loss in iteration 49 : 0.48877684570693986
Loss in iteration 50 : 0.4881986666372289
Loss in iteration 51 : 0.48764508825058306
Loss in iteration 52 : 0.48711430425370233
Loss in iteration 53 : 0.4866046845878964
Loss in iteration 54 : 0.4861147556541017
Loss in iteration 55 : 0.4856431820790871
Loss in iteration 56 : 0.4851887503834068
Loss in iteration 57 : 0.4847503547253948
Loss in iteration 58 : 0.4843269847113653
Loss in iteration 59 : 0.48391771511273673
Loss in iteration 60 : 0.48352169723243027
Loss in iteration 61 : 0.48313815161822904
Loss in iteration 62 : 0.48276636182198274
Loss in iteration 63 : 0.48240566893792164
Loss in iteration 64 : 0.4820554667065719
Loss in iteration 65 : 0.4817151970301652
Loss in iteration 66 : 0.4813843458013358
Loss in iteration 67 : 0.48106243899315804
Loss in iteration 68 : 0.4807490389923399
Loss in iteration 69 : 0.4804437411783578
Loss in iteration 70 : 0.48014617076106647
Loss in iteration 71 : 0.47985597989031725
Loss in iteration 72 : 0.47957284504608444
Loss in iteration 73 : 0.47929646470930787
Loss in iteration 74 : 0.4790265573042757
Loss in iteration 75 : 0.4787628593946894
Loss in iteration 76 : 0.4785051241086702
Loss in iteration 77 : 0.4782531197636815
Loss in iteration 78 : 0.4780066286606329
Loss in iteration 79 : 0.47776544601732374
Loss in iteration 80 : 0.4775293790141755
Loss in iteration 81 : 0.47729824592947223
Loss in iteration 82 : 0.47707187534616374
Loss in iteration 83 : 0.4768501054172258
Loss in iteration 84 : 0.47663278318095875
Loss in iteration 85 : 0.47641976392116986
Loss in iteration 86 : 0.4762109105697295
Loss in iteration 87 : 0.4760060931504695
Loss in iteration 88 : 0.4758051882640658
Loss in iteration 89 : 0.4756080786134394
Loss in iteration 90 : 0.47541465256875337
Loss in iteration 91 : 0.4752248037703804
Loss in iteration 92 : 0.47503843076754015
Loss in iteration 93 : 0.47485543668973756
Loss in iteration 94 : 0.4746757289478227
Loss in iteration 95 : 0.47449921896136976
Loss in iteration 96 : 0.47432582190920664
Loss in iteration 97 : 0.4741554565001865
Loss in iteration 98 : 0.4739880447616952
Loss in iteration 99 : 0.4738235118437899
Loss in iteration 100 : 0.4736617858373004
Loss in iteration 101 : 0.47350279760457786
Loss in iteration 102 : 0.47334648062186774
Loss in iteration 103 : 0.4731927708325171
Loss in iteration 104 : 0.4730416065103509
Loss in iteration 105 : 0.47289292813263456
Loss in iteration 106 : 0.4727466782620688
Loss in iteration 107 : 0.4726028014372588
Loss in iteration 108 : 0.4724612440710929
Loss in iteration 109 : 0.4723219543564276
Loss in iteration 110 : 0.4721848821784929
Loss in iteration 111 : 0.47204997903341644
Loss in iteration 112 : 0.4719171979522996
Loss in iteration 113 : 0.47178649343031515
Loss in iteration 114 : 0.4716578213603255
Loss in iteration 115 : 0.47153113897061144
Loss in iteration 116 : 0.4714064047663131
Loss in iteration 117 : 0.4712835784742914
Loss in iteration 118 : 0.47116262099111145
Loss in iteration 119 : 0.47104349433394654
Loss in iteration 120 : 0.4709261615941906
Loss in iteration 121 : 0.47081058689362176
Loss in iteration 122 : 0.47069673534296197
Loss in iteration 123 : 0.47058457300268935
Loss in iteration 124 : 0.4704740668459792
Loss in iteration 125 : 0.4703651847236439
Loss in iteration 126 : 0.4702578953309368
Loss in iteration 127 : 0.4701521681761207
Loss in iteration 128 : 0.4700479735506683
Loss in iteration 129 : 0.46994528250098666
Loss in iteration 130 : 0.46984406680157476
Loss in iteration 131 : 0.4697442989294939
Loss in iteration 132 : 0.4696459520400866
Loss in iteration 133 : 0.46954899994384386
Loss in iteration 134 : 0.4694534170843534
Loss in iteration 135 : 0.46935917851726516
Loss in iteration 136 : 0.46926625989020787
Loss in iteration 137 : 0.4691746374236002
Loss in iteration 138 : 0.46908428789231543
Loss in iteration 139 : 0.46899518860813416
Loss in iteration 140 : 0.4689073174029657
Loss in iteration 141 : 0.46882065261277533
Loss in iteration 142 : 0.468735173062187
Loss in iteration 143 : 0.46865085804973095
Loss in iteration 144 : 0.46856768733369414
Loss in iteration 145 : 0.46848564111853624
Loss in iteration 146 : 0.46840470004185897
Loss in iteration 147 : 0.4683248451618762
Loss in iteration 148 : 0.468246057945382
Loss in iteration 149 : 0.46816832025616945
Loss in iteration 150 : 0.46809161434389035
Loss in iteration 151 : 0.4680159228333336
Loss in iteration 152 : 0.4679412287140958
Loss in iteration 153 : 0.46786751533062804
Loss in iteration 154 : 0.46779476637264505
Loss in iteration 155 : 0.4677229658658757
Loss in iteration 156 : 0.46765209816313613
Loss in iteration 157 : 0.46758214793572095
Loss in iteration 158 : 0.46751310016509307
Loss in iteration 159 : 0.46744494013485266
Loss in iteration 160 : 0.4673776534229863
Loss in iteration 161 : 0.46731122589436996
Loss in iteration 162 : 0.46724564369352695
Loss in iteration 163 : 0.46718089323762435
Loss in iteration 164 : 0.4671169612096948
Loss in iteration 165 : 0.4670538345520804
Loss in iteration 166 : 0.46699150046008375
Loss in iteration 167 : 0.4669299463758247
Loss in iteration 168 : 0.4668691599822841
Loss in iteration 169 : 0.46680912919753803
Loss in iteration 170 : 0.4667498421691644
Loss in iteration 171 : 0.4666912872688245
Loss in iteration 172 : 0.4666334530870069
Loss in iteration 173 : 0.4665763284279284
Loss in iteration 174 : 0.46651990230458684
Loss in iteration 175 : 0.4664641639339579
Loss in iteration 176 : 0.46640910273233627
Loss in iteration 177 : 0.46635470831080716
Loss in iteration 178 : 0.4663009704708509
Loss in iteration 179 : 0.4662478792000681
Loss in iteration 180 : 0.4661954246680353
Loss in iteration 181 : 0.46614359722226467
Loss in iteration 182 : 0.4660923873842817
Loss in iteration 183 : 0.46604178584581535
Loss in iteration 184 : 0.4659917834650826
Loss in iteration 185 : 0.46594237126318294
Loss in iteration 186 : 0.46589354042058584
Loss in iteration 187 : 0.4658452822737113
Loss in iteration 188 : 0.4657975883116043
Loss in iteration 189 : 0.46575045017269323
Loss in iteration 190 : 0.46570385964164007
Loss in iteration 191 : 0.4656578086462621
Loss in iteration 192 : 0.46561228925454445
Loss in iteration 193 : 0.46556729367172034
Loss in iteration 194 : 0.4655228142374315
Loss in iteration 195 : 0.4654788434229573
Loss in iteration 196 : 0.46543537382851646
Loss in iteration 197 : 0.4653923981806336
Loss in iteration 198 : 0.46534990932957443
Loss in iteration 199 : 0.4653079002468391
Loss in iteration 200 : 0.4652663640227254
Testing accuracy  of updater 8 on alg 0 with rate 0.08000000000000002 = 0.784, training accuracy 0.789625, time elapsed: 5011 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6894903758970572
Loss in iteration 3 : 0.6852286827881773
Loss in iteration 4 : 0.6809168626902012
Loss in iteration 5 : 0.6768110173673152
Loss in iteration 6 : 0.6729182050748967
Loss in iteration 7 : 0.6691121453910538
Loss in iteration 8 : 0.6652470411406357
Loss in iteration 9 : 0.6612296325188058
Loss in iteration 10 : 0.6570419264462276
Loss in iteration 11 : 0.6527273289770072
Loss in iteration 12 : 0.6483596163831155
Loss in iteration 13 : 0.6440118819812563
Loss in iteration 14 : 0.6397360510591728
Loss in iteration 15 : 0.6355562504169319
Loss in iteration 16 : 0.6314735637013741
Loss in iteration 17 : 0.6274767095462425
Loss in iteration 18 : 0.6235530178895208
Loss in iteration 19 : 0.6196958873140795
Loss in iteration 20 : 0.6159074064837983
Loss in iteration 21 : 0.6121969089391979
Loss in iteration 22 : 0.6085773216729314
Loss in iteration 23 : 0.6050612434633478
Loss in iteration 24 : 0.6016580942273472
Loss in iteration 25 : 0.5983728609513118
Loss in iteration 26 : 0.5952062821526208
Loss in iteration 27 : 0.592155932813198
Loss in iteration 28 : 0.5892175997251152
Loss in iteration 29 : 0.5863864770821023
Loss in iteration 30 : 0.5836579395475194
Loss in iteration 31 : 0.5810278629239103
Loss in iteration 32 : 0.5784926033644742
Loss in iteration 33 : 0.5760488008967584
Loss in iteration 34 : 0.573693158377245
Loss in iteration 35 : 0.5714222932224888
Loss in iteration 36 : 0.5692326966807304
Loss in iteration 37 : 0.5671207857039238
Loss in iteration 38 : 0.565083005704437
Loss in iteration 39 : 0.5631159383706709
Loss in iteration 40 : 0.5612163804518708
Loss in iteration 41 : 0.5593813779280131
Loss in iteration 42 : 0.5576082173643001
Loss in iteration 43 : 0.555894387695824
Loss in iteration 44 : 0.5542375297888411
Loss in iteration 45 : 0.5526353891017471
Loss in iteration 46 : 0.5510857812000965
Loss in iteration 47 : 0.5495865734860919
Loss in iteration 48 : 0.5481356813167667
Loss in iteration 49 : 0.5467310737137032
Loss in iteration 50 : 0.5453707831628404
Loss in iteration 51 : 0.5440529149793168
Loss in iteration 52 : 0.5427756535266604
Loss in iteration 53 : 0.5415372644529944
Loss in iteration 54 : 0.5403360935147417
Loss in iteration 55 : 0.5391705632845515
Loss in iteration 56 : 0.5380391691298958
Loss in iteration 57 : 0.5369404755048746
Loss in iteration 58 : 0.5358731130718757
Loss in iteration 59 : 0.5348357766763603
Loss in iteration 60 : 0.5338272238689974
Loss in iteration 61 : 0.5328462735453374
Loss in iteration 62 : 0.5318918043236345
Loss in iteration 63 : 0.5309627524366599
Loss in iteration 64 : 0.5300581090966595
Loss in iteration 65 : 0.5291769174439614
Loss in iteration 66 : 0.5283182692763511
Loss in iteration 67 : 0.5274813017734278
Loss in iteration 68 : 0.5266651943927988
Loss in iteration 69 : 0.5258691660481964
Loss in iteration 70 : 0.5250924726083204
Loss in iteration 71 : 0.5243344046979921
Loss in iteration 72 : 0.5235942857490331
Loss in iteration 73 : 0.522871470237802
Loss in iteration 74 : 0.5221653420538117
Loss in iteration 75 : 0.5214753129611848
Loss in iteration 76 : 0.5208008211337788
Loss in iteration 77 : 0.5201413297601232
Loss in iteration 78 : 0.5194963257229553
Loss in iteration 79 : 0.5188653183601629
Loss in iteration 80 : 0.5182478383110822
Loss in iteration 81 : 0.517643436446901
Loss in iteration 82 : 0.5170516828786641
Loss in iteration 83 : 0.5164721660328027
Loss in iteration 84 : 0.5159044917827216
Loss in iteration 85 : 0.5153482826258994
Loss in iteration 86 : 0.5148031768981675
Loss in iteration 87 : 0.5142688280198286
Loss in iteration 88 : 0.5137449037709837
Loss in iteration 89 : 0.5132310855955262
Loss in iteration 90 : 0.5127270679343849
Loss in iteration 91 : 0.5122325575889456
Loss in iteration 92 : 0.5117472731152679
Loss in iteration 93 : 0.5112709442491183
Loss in iteration 94 : 0.5108033113612028
Loss in iteration 95 : 0.5103441249414454
Loss in iteration 96 : 0.509893145110927
Loss in iteration 97 : 0.5094501411599789
Loss in iteration 98 : 0.5090148911110487
Loss in iteration 99 : 0.5085871813050943
Loss in iteration 100 : 0.5081668060104491
Loss in iteration 101 : 0.5077535670531615
Loss in iteration 102 : 0.5073472734678991
Loss in iteration 103 : 0.5069477411684524
Loss in iteration 104 : 0.5065547926368072
Loss in iteration 105 : 0.506168256629726
Loss in iteration 106 : 0.5057879679016852
Loss in iteration 107 : 0.5054137669430495
Loss in iteration 108 : 0.5050454997323502
Loss in iteration 109 : 0.5046830175016516
Loss in iteration 110 : 0.5043261765140247
Loss in iteration 111 : 0.5039748378522922
Loss in iteration 112 : 0.5036288672182835
Loss in iteration 113 : 0.5032881347419272
Loss in iteration 114 : 0.5029525147996053
Loss in iteration 115 : 0.5026218858412229
Loss in iteration 116 : 0.5022961302255151
Loss in iteration 117 : 0.5019751340631438
Loss in iteration 118 : 0.5016587870671742
Loss in iteration 119 : 0.5013469824105296
Loss in iteration 120 : 0.501039616590067
Loss in iteration 121 : 0.5007365892969402
Loss in iteration 122 : 0.5004378032929132
Loss in iteration 123 : 0.5001431642923463
Loss in iteration 124 : 0.4998525808495533
Loss in iteration 125 : 0.4995659642512885
Loss in iteration 126 : 0.499283228414083
Loss in iteration 127 : 0.4990042897862186
Loss in iteration 128 : 0.49872906725409344
Loss in iteration 129 : 0.49845748205275786
Loss in iteration 130 : 0.4981894576804218
Loss in iteration 131 : 0.4979249198167219
Loss in iteration 132 : 0.49766379624455814
Loss in iteration 133 : 0.4974060167753288
Loss in iteration 134 : 0.4971515131773643
Loss in iteration 135 : 0.49690021910743054
Loss in iteration 136 : 0.49665207004512457
Loss in iteration 137 : 0.49640700323001996
Loss in iteration 138 : 0.49616495760143814
Loss in iteration 139 : 0.4959258737407034
Loss in iteration 140 : 0.4956896938157619
Loss in iteration 141 : 0.4954563615280577
Loss in iteration 142 : 0.49522582206155336
Loss in iteration 143 : 0.4949980220337848
Loss in iteration 144 : 0.49477290944886837
Loss in iteration 145 : 0.4945504336523559
Loss in iteration 146 : 0.4943305452878661
Loss in iteration 147 : 0.4941131962553884
Loss in iteration 148 : 0.493898339671213
Loss in iteration 149 : 0.49368592982938075
Loss in iteration 150 : 0.49347592216461567
Loss in iteration 151 : 0.49326827321664735
Loss in iteration 152 : 0.4930629405958847
Loss in iteration 153 : 0.49285988295036365
Loss in iteration 154 : 0.4926590599339296
Loss in iteration 155 : 0.49246043217558394
Loss in iteration 156 : 0.4922639612499638
Loss in iteration 157 : 0.4920696096488882
Loss in iteration 158 : 0.49187734075394096
Loss in iteration 159 : 0.49168711881004123
Loss in iteration 160 : 0.4914989088999491
Loss in iteration 161 : 0.4913126769196906
Loss in iteration 162 : 0.49112838955483873
Loss in iteration 163 : 0.4909460142576404
Loss in iteration 164 : 0.4907655192249268
Loss in iteration 165 : 0.49058687337680373
Loss in iteration 166 : 0.4904100463360676
Loss in iteration 167 : 0.49023500840832673
Loss in iteration 168 : 0.4900617305628067
Loss in iteration 169 : 0.48989018441379745
Loss in iteration 170 : 0.4897203422027263
Loss in iteration 171 : 0.48955217678083435
Loss in iteration 172 : 0.4893856615924226
Loss in iteration 173 : 0.4892207706586544
Loss in iteration 174 : 0.48905747856189
Loss in iteration 175 : 0.4888957604305287
Loss in iteration 176 : 0.4887355919243477
Loss in iteration 177 : 0.48857694922031336
Loss in iteration 178 : 0.48841980899883747
Loss in iteration 179 : 0.4882641484304877
Loss in iteration 180 : 0.4881099451631078
Loss in iteration 181 : 0.48795717730934424
Loss in iteration 182 : 0.48780582343457424
Loss in iteration 183 : 0.4876558625451909
Loss in iteration 184 : 0.4875072740772739
Loss in iteration 185 : 0.4873600378855923
Loss in iteration 186 : 0.48721413423295196
Loss in iteration 187 : 0.48706954377986483
Loss in iteration 188 : 0.486926247574538
Loss in iteration 189 : 0.48678422704315183
Loss in iteration 190 : 0.4866434639804483
Loss in iteration 191 : 0.4865039405405805
Loss in iteration 192 : 0.486365639228247
Loss in iteration 193 : 0.48622854289008527
Loss in iteration 194 : 0.4860926347063148
Loss in iteration 195 : 0.48595789818262997
Loss in iteration 196 : 0.4858243171423217
Loss in iteration 197 : 0.48569187571863315
Loss in iteration 198 : 0.4855605583473331
Loss in iteration 199 : 0.48543034975950256
Loss in iteration 200 : 0.4853012349745247
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.7795, training accuracy 0.78, time elapsed: 4985 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 12.569829085734186
Loss in iteration 3 : 13.880316658153783
Loss in iteration 4 : 8.01656966282435
Loss in iteration 5 : 9.181551262563623
Loss in iteration 6 : 6.868710291161584
Loss in iteration 7 : 4.384014147899584
Loss in iteration 8 : 7.9258580920465045
Loss in iteration 9 : 3.089631588936264
Loss in iteration 10 : 5.819007554891202
Loss in iteration 11 : 5.613804939091497
Loss in iteration 12 : 2.4261894468436997
Loss in iteration 13 : 4.861626330568006
Loss in iteration 14 : 4.518143093545736
Loss in iteration 15 : 2.5306558317650296
Loss in iteration 16 : 4.344273712842111
Loss in iteration 17 : 3.8584510908143366
Loss in iteration 18 : 2.616707257922228
Loss in iteration 19 : 3.8518407146376963
Loss in iteration 20 : 3.348841244578296
Loss in iteration 21 : 2.429400420533823
Loss in iteration 22 : 3.338534442129396
Loss in iteration 23 : 2.816023945490411
Loss in iteration 24 : 2.4040462982645248
Loss in iteration 25 : 3.0186591741522095
Loss in iteration 26 : 2.477913607341643
Loss in iteration 27 : 2.1415486861230497
Loss in iteration 28 : 2.606946580477345
Loss in iteration 29 : 1.9681903844424495
Loss in iteration 30 : 2.117205887788249
Loss in iteration 31 : 2.2136074627713755
Loss in iteration 32 : 1.7436470477457073
Loss in iteration 33 : 2.0219463487106997
Loss in iteration 34 : 1.7109765525889866
Loss in iteration 35 : 1.6676263810083574
Loss in iteration 36 : 1.7224346619592652
Loss in iteration 37 : 1.4093322169279618
Loss in iteration 38 : 1.6439530718874265
Loss in iteration 39 : 1.299424282625834
Loss in iteration 40 : 1.489675547215823
Loss in iteration 41 : 1.2055869881545587
Loss in iteration 42 : 1.3928948830010766
Loss in iteration 43 : 1.126721589073453
Loss in iteration 44 : 1.3301920292898457
Loss in iteration 45 : 1.070667684952798
Loss in iteration 46 : 1.202763880028471
Loss in iteration 47 : 1.1661931785755246
Loss in iteration 48 : 1.0510193765837348
Loss in iteration 49 : 1.1349543349958457
Loss in iteration 50 : 1.0561146896308988
Loss in iteration 51 : 1.0784635428287455
Loss in iteration 52 : 1.0027546075804934
Loss in iteration 53 : 1.0766585179781423
Loss in iteration 54 : 0.977528176445457
Loss in iteration 55 : 1.201345162566104
Loss in iteration 56 : 1.073893054787534
Loss in iteration 57 : 0.9049619013902778
Loss in iteration 58 : 1.106973935063326
Loss in iteration 59 : 1.009326071322634
Loss in iteration 60 : 0.8321899692327105
Loss in iteration 61 : 0.68869873079915
Loss in iteration 62 : 0.717822178376784
Loss in iteration 63 : 0.8599585151096956
Loss in iteration 64 : 0.9624942176145109
Loss in iteration 65 : 0.9658661590935712
Loss in iteration 66 : 0.7720208659211119
Loss in iteration 67 : 0.792265568571595
Loss in iteration 68 : 0.6127731112217463
Loss in iteration 69 : 0.7737537636288857
Loss in iteration 70 : 0.5727806140122061
Loss in iteration 71 : 0.8294480654077443
Loss in iteration 72 : 0.9900884412851295
Loss in iteration 73 : 1.7564896249778796
Loss in iteration 74 : 1.1535386638633713
Loss in iteration 75 : 0.7409687962862945
Loss in iteration 76 : 0.7528990111521903
Loss in iteration 77 : 0.7420647328918494
Loss in iteration 78 : 1.5115977833580798
Loss in iteration 79 : 1.142518826642728
Loss in iteration 80 : 0.9147490706400302
Loss in iteration 81 : 0.5847423571279947
Loss in iteration 82 : 0.7775405556706766
Loss in iteration 83 : 0.8099058347403388
Loss in iteration 84 : 0.9406797010382921
Loss in iteration 85 : 0.7422198198201143
Loss in iteration 86 : 0.6574848663259542
Loss in iteration 87 : 0.6727888643482455
Loss in iteration 88 : 0.7056092819323121
Loss in iteration 89 : 0.9207348067104821
Loss in iteration 90 : 1.0119561403813835
Loss in iteration 91 : 0.9089110331701322
Loss in iteration 92 : 0.7852233493894919
Loss in iteration 93 : 0.5884784322293304
Loss in iteration 94 : 0.6476251310155696
Loss in iteration 95 : 0.5323446707863848
Loss in iteration 96 : 0.6474023276677862
Loss in iteration 97 : 0.539486775567099
Loss in iteration 98 : 0.6437833480362161
Loss in iteration 99 : 0.6288882714980821
Loss in iteration 100 : 0.8093950511683787
Loss in iteration 101 : 1.2428605731720082
Loss in iteration 102 : 1.1025556771853273
Loss in iteration 103 : 1.1549567605671702
Loss in iteration 104 : 0.5921826544195803
Loss in iteration 105 : 0.7059504634657559
Loss in iteration 106 : 0.9463952330787337
Loss in iteration 107 : 0.7660340628758151
Loss in iteration 108 : 0.7610583397435293
Loss in iteration 109 : 0.5542308872474603
Loss in iteration 110 : 0.6744452617391379
Loss in iteration 111 : 0.7015001177096506
Loss in iteration 112 : 0.6470619260552559
Loss in iteration 113 : 0.7231736413645103
Loss in iteration 114 : 0.5975057046122129
Loss in iteration 115 : 0.6329194034416334
Loss in iteration 116 : 0.5851314315589755
Loss in iteration 117 : 0.5900328466293575
Loss in iteration 118 : 0.5724837277793086
Loss in iteration 119 : 0.6012667845889612
Loss in iteration 120 : 0.5632178390204996
Loss in iteration 121 : 0.6828822828439031
Loss in iteration 122 : 0.7019793784481283
Loss in iteration 123 : 1.1003313540853055
Loss in iteration 124 : 1.0016651124135045
Loss in iteration 125 : 0.8993082711750212
Loss in iteration 126 : 0.6926751428663099
Loss in iteration 127 : 0.5619112338430753
Loss in iteration 128 : 0.5114388489639147
Loss in iteration 129 : 0.5165340428069525
Loss in iteration 130 : 0.5374810067159315
Loss in iteration 131 : 0.6201771882162375
Loss in iteration 132 : 0.7656726005831883
Loss in iteration 133 : 0.8711446854328491
Loss in iteration 134 : 0.8939114130671928
Loss in iteration 135 : 0.6314428467360506
Loss in iteration 136 : 0.5323374637429905
Loss in iteration 137 : 0.5117328159326433
Loss in iteration 138 : 0.5715011373050107
Loss in iteration 139 : 0.663744566265327
Loss in iteration 140 : 0.6693549804479079
Loss in iteration 141 : 0.6518336091529848
Loss in iteration 142 : 0.5752942758599257
Loss in iteration 143 : 0.5510311122919329
Loss in iteration 144 : 0.5232499636401668
Loss in iteration 145 : 0.5340755712722691
Loss in iteration 146 : 0.5560131839494348
Loss in iteration 147 : 0.6697185393864732
Loss in iteration 148 : 0.8509661775977366
Loss in iteration 149 : 1.1108728611798124
Loss in iteration 150 : 0.7151517169803902
Loss in iteration 151 : 0.573105219651301
Loss in iteration 152 : 0.499299543820027
Loss in iteration 153 : 0.5947832400945516
Loss in iteration 154 : 0.6490852490760919
Loss in iteration 155 : 0.6605821064725713
Loss in iteration 156 : 0.6452147122622136
Loss in iteration 157 : 0.5585487103064647
Loss in iteration 158 : 0.5768607637398365
Loss in iteration 159 : 0.5027442047084736
Loss in iteration 160 : 0.5563079266543762
Loss in iteration 161 : 0.4931787204598035
Loss in iteration 162 : 0.5740317207399731
Loss in iteration 163 : 0.5590852463923037
Loss in iteration 164 : 0.7259818678921034
Loss in iteration 165 : 0.7360649130868712
Loss in iteration 166 : 0.8081568628265183
Loss in iteration 167 : 0.626622981305866
Loss in iteration 168 : 0.5446812864043306
Loss in iteration 169 : 0.48907622643644666
Loss in iteration 170 : 0.4930115828713471
Loss in iteration 171 : 0.5230001782963221
Loss in iteration 172 : 0.5985232355469418
Loss in iteration 173 : 0.7664417911349538
Loss in iteration 174 : 0.826755832464255
Loss in iteration 175 : 0.8417938180827775
Loss in iteration 176 : 0.5975102656180913
Loss in iteration 177 : 0.5270093253231439
Loss in iteration 178 : 0.48983611531307675
Loss in iteration 179 : 0.5509803179172625
Loss in iteration 180 : 0.5719583880850193
Loss in iteration 181 : 0.5658245776521719
Loss in iteration 182 : 0.5464318416237955
Loss in iteration 183 : 0.4944816687592488
Loss in iteration 184 : 0.5170675218216702
Loss in iteration 185 : 0.47499343108905884
Loss in iteration 186 : 0.5045753575611542
Loss in iteration 187 : 0.47202950922445963
Loss in iteration 188 : 0.4987450931989389
Loss in iteration 189 : 0.47753475997820816
Loss in iteration 190 : 0.5214420503308984
Loss in iteration 191 : 0.5953014261617057
Loss in iteration 192 : 0.8813784367341401
Loss in iteration 193 : 0.9928674624850217
Loss in iteration 194 : 0.8967487487529417
Loss in iteration 195 : 0.569629136669332
Loss in iteration 196 : 0.4914231624442655
Loss in iteration 197 : 0.6191429642345615
Loss in iteration 198 : 0.7384791501541008
Loss in iteration 199 : 0.6676817457686134
Loss in iteration 200 : 0.5139399209246311
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.789, training accuracy 0.784, time elapsed: 5182 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.8177790968637116
Loss in iteration 3 : 1.665338952456962
Loss in iteration 4 : 1.1879641697505599
Loss in iteration 5 : 0.5266315306675803
Loss in iteration 6 : 1.131741291994036
Loss in iteration 7 : 0.7517232996699266
Loss in iteration 8 : 0.6482835209761226
Loss in iteration 9 : 0.8505889464890346
Loss in iteration 10 : 0.6028301206037899
Loss in iteration 11 : 0.8160725877038801
Loss in iteration 12 : 0.6183008777270318
Loss in iteration 13 : 0.76636909439402
Loss in iteration 14 : 0.6666087949063683
Loss in iteration 15 : 0.7010560056489387
Loss in iteration 16 : 0.6734918003034256
Loss in iteration 17 : 0.6478473367200015
Loss in iteration 18 : 0.682565151582481
Loss in iteration 19 : 0.6055809601654302
Loss in iteration 20 : 0.6695857837753197
Loss in iteration 21 : 0.5784472404782046
Loss in iteration 22 : 0.6403431851557084
Loss in iteration 23 : 0.551693943840642
Loss in iteration 24 : 0.6132527727417263
Loss in iteration 25 : 0.5359060313562497
Loss in iteration 26 : 0.5825739322436457
Loss in iteration 27 : 0.5179271076151626
Loss in iteration 28 : 0.5550653297460011
Loss in iteration 29 : 0.5108050155454855
Loss in iteration 30 : 0.5323947017163152
Loss in iteration 31 : 0.5102783955303919
Loss in iteration 32 : 0.5068476261090766
Loss in iteration 33 : 0.5157359411316186
Loss in iteration 34 : 0.4880871692931457
Loss in iteration 35 : 0.5159449869957748
Loss in iteration 36 : 0.4888301397764894
Loss in iteration 37 : 0.49536889663875916
Loss in iteration 38 : 0.5038539211849744
Loss in iteration 39 : 0.48240292996075534
Loss in iteration 40 : 0.4958806796628099
Loss in iteration 41 : 0.4951681313000086
Loss in iteration 42 : 0.4773790978779319
Loss in iteration 43 : 0.4886152828202925
Loss in iteration 44 : 0.48676453304605916
Loss in iteration 45 : 0.4715745865652953
Loss in iteration 46 : 0.479783283468194
Loss in iteration 47 : 0.4777267357038296
Loss in iteration 48 : 0.466490280564063
Loss in iteration 49 : 0.4726572840104338
Loss in iteration 50 : 0.47103160387984216
Loss in iteration 51 : 0.46292261196037976
Loss in iteration 52 : 0.46769232037711744
Loss in iteration 53 : 0.46781002610514777
Loss in iteration 54 : 0.46163061122957694
Loss in iteration 55 : 0.4650299316550811
Loss in iteration 56 : 0.4662404862374289
Loss in iteration 57 : 0.4613977050186673
Loss in iteration 58 : 0.4636674382883168
Loss in iteration 59 : 0.46503418291343135
Loss in iteration 60 : 0.4612822944607468
Loss in iteration 61 : 0.46244177053321617
Loss in iteration 62 : 0.4639011358423501
Loss in iteration 63 : 0.4611620582298796
Loss in iteration 64 : 0.46100886088972276
Loss in iteration 65 : 0.4625751032109864
Loss in iteration 66 : 0.46111913470290167
Loss in iteration 67 : 0.45994892760048334
Loss in iteration 68 : 0.4611944787305224
Loss in iteration 69 : 0.4612202025790607
Loss in iteration 70 : 0.4598670020109204
Loss in iteration 71 : 0.46008362109301687
Loss in iteration 72 : 0.4609221384846214
Loss in iteration 73 : 0.4603801320199901
Loss in iteration 74 : 0.45973114274638444
Loss in iteration 75 : 0.46020172347685356
Loss in iteration 76 : 0.46053215040761836
Loss in iteration 77 : 0.45996098246485245
Loss in iteration 78 : 0.4596606839689507
Loss in iteration 79 : 0.46004402023556973
Loss in iteration 80 : 0.46003685531750893
Loss in iteration 81 : 0.45955637486792356
Loss in iteration 82 : 0.45950978054065134
Loss in iteration 83 : 0.459744858804027
Loss in iteration 84 : 0.4595953764868403
Loss in iteration 85 : 0.4593197682928362
Loss in iteration 86 : 0.45938974126894105
Loss in iteration 87 : 0.45950581967953585
Loss in iteration 88 : 0.4593334354799216
Loss in iteration 89 : 0.45922123557846234
Loss in iteration 90 : 0.4593480191185738
Loss in iteration 91 : 0.4593490588045806
Loss in iteration 92 : 0.4592112567727287
Loss in iteration 93 : 0.4592283208574136
Loss in iteration 94 : 0.4593021043599084
Loss in iteration 95 : 0.4592387857324767
Loss in iteration 96 : 0.45917461037348606
Loss in iteration 97 : 0.45922289269543076
Loss in iteration 98 : 0.4592361073244669
Loss in iteration 99 : 0.4591659397912375
Loss in iteration 100 : 0.4591557733429631
Loss in iteration 101 : 0.45919275068203247
Loss in iteration 102 : 0.459163610095154
Loss in iteration 103 : 0.4591231538401981
Loss in iteration 104 : 0.45913992657370123
Loss in iteration 105 : 0.45914893749914876
Loss in iteration 106 : 0.45911828131124216
Loss in iteration 107 : 0.4591060048786359
Loss in iteration 108 : 0.4591226009466067
Loss in iteration 109 : 0.45911695199780866
Loss in iteration 110 : 0.45909421723033067
Loss in iteration 111 : 0.45909662446248745
Loss in iteration 112 : 0.4591051037083587
Loss in iteration 113 : 0.45909153824428467
Loss in iteration 114 : 0.4590799605969303
Loss in iteration 115 : 0.4590845770586843
Loss in iteration 116 : 0.459083732472062
Loss in iteration 117 : 0.4590714020272932
Loss in iteration 118 : 0.45906621179771934
Loss in iteration 119 : 0.4590694919020142
Loss in iteration 120 : 0.45906507265868357
Loss in iteration 121 : 0.4590561025181106
Loss in iteration 122 : 0.4590554657245708
Loss in iteration 123 : 0.4590567054121226
Loss in iteration 124 : 0.45905133795170505
Loss in iteration 125 : 0.4590466947109353
Loss in iteration 126 : 0.45904740833372615
Loss in iteration 127 : 0.4590468575250054
Loss in iteration 128 : 0.4590424653644426
Loss in iteration 129 : 0.45904001945757783
Loss in iteration 130 : 0.45904059599031
Loss in iteration 131 : 0.4590389008621414
Loss in iteration 132 : 0.45903536474664136
Testing accuracy  of updater 9 on alg 0 with rate 1.4000000000000001 = 0.788, training accuracy 0.789625, time elapsed: 3301 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6873080888418495
Loss in iteration 3 : 0.6591607368773201
Loss in iteration 4 : 0.5920660115694849
Loss in iteration 5 : 0.541066337884812
Loss in iteration 6 : 0.536013820252378
Loss in iteration 7 : 0.5076276591972638
Loss in iteration 8 : 0.49917743625094385
Loss in iteration 9 : 0.4931827708372554
Loss in iteration 10 : 0.4882378895849912
Loss in iteration 11 : 0.48913063581712446
Loss in iteration 12 : 0.48767033944342847
Loss in iteration 13 : 0.48745121726950014
Loss in iteration 14 : 0.4882362548897666
Loss in iteration 15 : 0.4862698051140311
Loss in iteration 16 : 0.48902653534860235
Loss in iteration 17 : 0.48554826370127796
Loss in iteration 18 : 0.4879471635226135
Loss in iteration 19 : 0.4837126950529072
Loss in iteration 20 : 0.4843911435057227
Loss in iteration 21 : 0.48114542636435403
Loss in iteration 22 : 0.48017216725057205
Loss in iteration 23 : 0.47826858743018646
Loss in iteration 24 : 0.47573067016351844
Loss in iteration 25 : 0.4746733723981741
Loss in iteration 26 : 0.47165006812807525
Loss in iteration 27 : 0.4712702269142311
Loss in iteration 28 : 0.46874021384853676
Loss in iteration 29 : 0.46855498050803124
Loss in iteration 30 : 0.4666549206716227
Loss in iteration 31 : 0.466335804099999
Loss in iteration 32 : 0.4651970477089045
Loss in iteration 33 : 0.46493001174711396
Loss in iteration 34 : 0.46450393214931673
Loss in iteration 35 : 0.4642566497364969
Loss in iteration 36 : 0.4641796578452759
Loss in iteration 37 : 0.4639097015229388
Loss in iteration 38 : 0.4640739688079159
Loss in iteration 39 : 0.46387509420249967
Loss in iteration 40 : 0.46410823737093104
Loss in iteration 41 : 0.46383894364774303
Loss in iteration 42 : 0.46397007725624195
Loss in iteration 43 : 0.4636380940091308
Loss in iteration 44 : 0.4637040784769558
Loss in iteration 45 : 0.46335642169651625
Loss in iteration 46 : 0.4633308641379333
Loss in iteration 47 : 0.4629337233758679
Loss in iteration 48 : 0.46283211539747865
Loss in iteration 49 : 0.4624494074860431
Loss in iteration 50 : 0.4623355104189845
Loss in iteration 51 : 0.4619860353179122
Loss in iteration 52 : 0.4618562465769159
Loss in iteration 53 : 0.46154242684722796
Loss in iteration 54 : 0.46143503767832955
Loss in iteration 55 : 0.46119625900281397
Loss in iteration 56 : 0.4611269207138258
Loss in iteration 57 : 0.4609458901074831
Loss in iteration 58 : 0.46089742774286907
Loss in iteration 59 : 0.46076600101120757
Loss in iteration 60 : 0.46074276585147034
Loss in iteration 61 : 0.46065540976928293
Loss in iteration 62 : 0.4606421466460995
Loss in iteration 63 : 0.46057464595737985
Loss in iteration 64 : 0.4605599887409032
Loss in iteration 65 : 0.4605085985935653
Loss in iteration 66 : 0.4604942406970102
Loss in iteration 67 : 0.46045027234747343
Loss in iteration 68 : 0.4604266461188729
Loss in iteration 69 : 0.4603812771294053
Loss in iteration 70 : 0.460350631177053
Loss in iteration 71 : 0.46030768511559933
Loss in iteration 72 : 0.46027402226515657
Loss in iteration 73 : 0.460232052562548
Loss in iteration 74 : 0.46019592840984286
Loss in iteration 75 : 0.46015707451356574
Loss in iteration 76 : 0.4601239849975497
Loss in iteration 77 : 0.4600913279193476
Loss in iteration 78 : 0.46006188600503345
Loss in iteration 79 : 0.4600333214647044
Loss in iteration 80 : 0.46000714488423816
Loss in iteration 81 : 0.4599834521875542
Loss in iteration 82 : 0.4599615209721448
Loss in iteration 83 : 0.45994151704426245
Loss in iteration 84 : 0.45992178194556677
Loss in iteration 85 : 0.4599035220368865
Loss in iteration 86 : 0.45988534850508
Loss in iteration 87 : 0.4598685241284023
Loss in iteration 88 : 0.45985121286383834
Loss in iteration 89 : 0.45983445732253325
Loss in iteration 90 : 0.45981696467759525
Loss in iteration 91 : 0.4598000724384177
Loss in iteration 92 : 0.4597828402505894
Loss in iteration 93 : 0.4597661907912675
Loss in iteration 94 : 0.45974916262917837
Loss in iteration 95 : 0.4597325747405446
Loss in iteration 96 : 0.459715893772694
Loss in iteration 97 : 0.45969985809713476
Loss in iteration 98 : 0.45968392176587725
Loss in iteration 99 : 0.4596685602526556
Loss in iteration 100 : 0.4596533253420399
Loss in iteration 101 : 0.45963868467663727
Loss in iteration 102 : 0.4596243402722129
Loss in iteration 103 : 0.4596106276124968
Loss in iteration 104 : 0.4595971957579555
Loss in iteration 105 : 0.4595842608623184
Loss in iteration 106 : 0.4595715775029799
Loss in iteration 107 : 0.4595593570132124
Loss in iteration 108 : 0.4595474074484792
Loss in iteration 109 : 0.45953584159127303
Loss in iteration 110 : 0.4595244824038062
Loss in iteration 111 : 0.45951342475309415
Loss in iteration 112 : 0.4595025777037942
Loss in iteration 113 : 0.4594920105742236
Loss in iteration 114 : 0.4594816425912511
Loss in iteration 115 : 0.4594715003076794
Loss in iteration 116 : 0.4594615318088364
Loss in iteration 117 : 0.45945176817076194
Loss in iteration 118 : 0.4594421956531131
Loss in iteration 119 : 0.4594328277364196
Loss in iteration 120 : 0.45942364606816216
Loss in iteration 121 : 0.45941465171765017
Loss in iteration 122 : 0.4594058459425718
Loss in iteration 123 : 0.4593972311380504
Loss in iteration 124 : 0.45938881208897187
Loss in iteration 125 : 0.4593805746892953
Loss in iteration 126 : 0.4593725215673039
Loss in iteration 127 : 0.45936463816818923
Loss in iteration 128 : 0.4593569376891535
Loss in iteration 129 : 0.45934940185333156
Loss in iteration 130 : 0.4593420416767787
Loss in iteration 131 : 0.45933483161184496
Loss in iteration 132 : 0.45932778438931454
Loss in iteration 133 : 0.459320876997616
Loss in iteration 134 : 0.4593141263907689
Loss in iteration 135 : 0.459307506907106
Loss in iteration 136 : 0.459301033070901
Loss in iteration 137 : 0.4592946804854421
Loss in iteration 138 : 0.4592884665597677
Loss in iteration 139 : 0.4592823702320403
Loss in iteration 140 : 0.4592764079587959
Loss in iteration 141 : 0.45927055840250347
Loss in iteration 142 : 0.459264836084036
Loss in iteration 143 : 0.4592592227111918
Loss in iteration 144 : 0.45925373273237835
Loss in iteration 145 : 0.4592483494750227
Loss in iteration 146 : 0.4592430848100669
Loss in iteration 147 : 0.45923792288178883
Loss in iteration 148 : 0.45923287427871995
Loss in iteration 149 : 0.45922792557517833
Loss in iteration 150 : 0.45922308601206985
Loss in iteration 151 : 0.45921834269994033
Loss in iteration 152 : 0.4592137029133874
Loss in iteration 153 : 0.45920915523951344
Loss in iteration 154 : 0.45920470623689363
Loss in iteration 155 : 0.4592003460251137
Loss in iteration 156 : 0.45919607989829414
Loss in iteration 157 : 0.45919189878461053
Loss in iteration 158 : 0.45918780702244383
Loss in iteration 159 : 0.45918379699972794
Loss in iteration 160 : 0.45917987240022806
Loss in iteration 161 : 0.4591760265257522
Loss in iteration 162 : 0.4591722620588138
Loss in iteration 163 : 0.45916857315168197
Loss in iteration 164 : 0.4591649619704092
Loss in iteration 165 : 0.45916142367492696
Loss in iteration 166 : 0.45915795978397134
Loss in iteration 167 : 0.45915456601821025
Loss in iteration 168 : 0.45915124331206775
Loss in iteration 169 : 0.45914798805579676
Loss in iteration 170 : 0.4591448008158228
Loss in iteration 171 : 0.45914167851111143
Loss in iteration 172 : 0.45913862125494187
Loss in iteration 173 : 0.4591356263386181
Loss in iteration 174 : 0.4591326936092636
Testing accuracy  of updater 9 on alg 0 with rate 0.8 = 0.787, training accuracy 0.7895, time elapsed: 4331 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6782986564638585
Loss in iteration 3 : 0.6663132404916332
Loss in iteration 4 : 0.6481131010384756
Loss in iteration 5 : 0.6270591589606115
Loss in iteration 6 : 0.6103414033303216
Loss in iteration 7 : 0.5905201698643427
Loss in iteration 8 : 0.5705181042278914
Loss in iteration 9 : 0.5561399482904043
Loss in iteration 10 : 0.5433818086466811
Loss in iteration 11 : 0.5313123141871701
Loss in iteration 12 : 0.5226959672050575
Loss in iteration 13 : 0.5158955506758143
Loss in iteration 14 : 0.5087494093319774
Loss in iteration 15 : 0.5027775046109748
Loss in iteration 16 : 0.49882810424104845
Loss in iteration 17 : 0.49540822184417466
Loss in iteration 18 : 0.4920163878058762
Loss in iteration 19 : 0.48959117600694635
Loss in iteration 20 : 0.4880928176894986
Loss in iteration 21 : 0.4865323170042156
Loss in iteration 22 : 0.48479599751212676
Loss in iteration 23 : 0.48349476861807134
Loss in iteration 24 : 0.48260873537300847
Loss in iteration 25 : 0.4816249126565792
Loss in iteration 26 : 0.48053265060569683
Loss in iteration 27 : 0.4797146969105697
Loss in iteration 28 : 0.47917048138806756
Loss in iteration 29 : 0.4785622820610146
Loss in iteration 30 : 0.4778285918665189
Loss in iteration 31 : 0.47719627124450803
Loss in iteration 32 : 0.47670548209658264
Loss in iteration 33 : 0.47616209719343866
Loss in iteration 34 : 0.4755079826928594
Loss in iteration 35 : 0.47489410750305405
Loss in iteration 36 : 0.47439033595418234
Loss in iteration 37 : 0.4738911225458537
Loss in iteration 38 : 0.47333441147845134
Loss in iteration 39 : 0.4727999220737739
Loss in iteration 40 : 0.4723442469192293
Loss in iteration 41 : 0.4719047699868263
Loss in iteration 42 : 0.4714239564769679
Loss in iteration 43 : 0.47094359938426145
Loss in iteration 44 : 0.47051642075557454
Loss in iteration 45 : 0.470118527276999
Loss in iteration 46 : 0.469709030459137
Loss in iteration 47 : 0.4693063989091654
Loss in iteration 48 : 0.46894802121476975
Loss in iteration 49 : 0.46862346184152126
Loss in iteration 50 : 0.4682998943151043
Loss in iteration 51 : 0.46797969407653556
Loss in iteration 52 : 0.4676870358674536
Loss in iteration 53 : 0.46742025009358495
Loss in iteration 54 : 0.4671579472029437
Loss in iteration 55 : 0.46689862798055076
Loss in iteration 56 : 0.4666596239097705
Loss in iteration 57 : 0.4664442088959802
Loss in iteration 58 : 0.4662385974252428
Loss in iteration 59 : 0.46603812380854154
Loss in iteration 60 : 0.46585226565451454
Loss in iteration 61 : 0.46568399699543683
Loss in iteration 62 : 0.4655240273301252
Loss in iteration 63 : 0.4653673997393998
Loss in iteration 64 : 0.46521986516432445
Loss in iteration 65 : 0.4650849375634183
Loss in iteration 66 : 0.4649574929146337
Loss in iteration 67 : 0.46483363636734376
Loss in iteration 68 : 0.46471661602477937
Loss in iteration 69 : 0.4646091554029149
Loss in iteration 70 : 0.4645079255351469
Loss in iteration 71 : 0.46440937413092787
Loss in iteration 72 : 0.464314825294469
Loss in iteration 73 : 0.46422628036882657
Loss in iteration 74 : 0.4641420252964207
Loss in iteration 75 : 0.4640597156130274
Loss in iteration 76 : 0.463980144080766
Loss in iteration 77 : 0.4639049513924885
Loss in iteration 78 : 0.46383331243633663
Loss in iteration 79 : 0.4637634970821366
Loss in iteration 80 : 0.4636956737559403
Loss in iteration 81 : 0.4636308699093956
Loss in iteration 82 : 0.4635686417877394
Loss in iteration 83 : 0.46350776722906906
Loss in iteration 84 : 0.4634482102323618
Loss in iteration 85 : 0.463390721632946
Loss in iteration 86 : 0.46333521487715335
Loss in iteration 87 : 0.4632809491909915
Loss in iteration 88 : 0.4632278326812558
Loss in iteration 89 : 0.46317634825598875
Loss in iteration 90 : 0.46312648211219776
Loss in iteration 91 : 0.46307770606090076
Loss in iteration 92 : 0.463029853708263
Loss in iteration 93 : 0.4629832128390384
Loss in iteration 94 : 0.46293783341066747
Loss in iteration 95 : 0.4628934051198136
Loss in iteration 96 : 0.46284980512588675
Loss in iteration 97 : 0.4628072299820645
Loss in iteration 98 : 0.46276575370076517
Loss in iteration 99 : 0.4627251813541549
Loss in iteration 100 : 0.46268539107080836
Loss in iteration 101 : 0.4626464764142433
Loss in iteration 102 : 0.4626084831115177
Loss in iteration 103 : 0.4625712800949573
Loss in iteration 104 : 0.462534768075238
Loss in iteration 105 : 0.4624990011794707
Loss in iteration 106 : 0.4624640261032724
Loss in iteration 107 : 0.46242976941510255
Loss in iteration 108 : 0.4623961565999454
Loss in iteration 109 : 0.4623632094767874
Loss in iteration 110 : 0.4623309556636123
Loss in iteration 111 : 0.4622993433399606
Loss in iteration 112 : 0.462268309546949
Loss in iteration 113 : 0.4622378554910648
Loss in iteration 114 : 0.4622079977175224
Loss in iteration 115 : 0.4621787043118903
Loss in iteration 116 : 0.4621499301485279
Loss in iteration 117 : 0.46212167075702004
Loss in iteration 118 : 0.4620939366579761
Loss in iteration 119 : 0.4620667067443288
Loss in iteration 120 : 0.4620399450429953
Loss in iteration 121 : 0.4620136396498622
Loss in iteration 122 : 0.46198779233853404
Loss in iteration 123 : 0.4619623868622017
Loss in iteration 124 : 0.46193739571069176
Loss in iteration 125 : 0.4619128066814286
Loss in iteration 126 : 0.4618886193440681
Loss in iteration 127 : 0.4618648229955232
Loss in iteration 128 : 0.4618413975976544
Loss in iteration 129 : 0.46181833140621487
Loss in iteration 130 : 0.46179562125437074
Loss in iteration 131 : 0.4617732581973036
Loss in iteration 132 : 0.46175122632257415
Loss in iteration 133 : 0.4617295144632353
Loss in iteration 134 : 0.46170811816486
Loss in iteration 135 : 0.4616870304414158
Loss in iteration 136 : 0.46166623953961095
Loss in iteration 137 : 0.4616457361816391
Loss in iteration 138 : 0.46162551573355365
Loss in iteration 139 : 0.4616055723482574
Loss in iteration 140 : 0.4615858967158273
Loss in iteration 141 : 0.4615664806173427
Loss in iteration 142 : 0.46154731906139385
Loss in iteration 143 : 0.4615284067740511
Loss in iteration 144 : 0.46150973627772
Loss in iteration 145 : 0.46149130063546995
Loss in iteration 146 : 0.4614730951873187
Loss in iteration 147 : 0.4614551154136313
Loss in iteration 148 : 0.46143735534449754
Loss in iteration 149 : 0.46141980913134
Loss in iteration 150 : 0.4614024723812383
Loss in iteration 151 : 0.46138534093464273
Loss in iteration 152 : 0.4613684097031027
Loss in iteration 153 : 0.4613516736011255
Loss in iteration 154 : 0.4613351285602352
Loss in iteration 155 : 0.4613187708420042
Loss in iteration 156 : 0.461302596170554
Loss in iteration 157 : 0.46128660022310675
Loss in iteration 158 : 0.461270779340197
Loss in iteration 159 : 0.46125513014638375
Loss in iteration 160 : 0.46123964893487257
Loss in iteration 161 : 0.4612243319302198
Loss in iteration 162 : 0.4612091757985634
Loss in iteration 163 : 0.4611941774623297
Loss in iteration 164 : 0.4611793336764222
Loss in iteration 165 : 0.4611646411554323
Loss in iteration 166 : 0.4611500969221515
Loss in iteration 167 : 0.4611356982166769
Loss in iteration 168 : 0.461121442199014
Loss in iteration 169 : 0.4611073259970151
Loss in iteration 170 : 0.46109334694187315
Loss in iteration 171 : 0.46107950253506125
Loss in iteration 172 : 0.46106579025089284
Loss in iteration 173 : 0.4610522075505411
Loss in iteration 174 : 0.46103875204153005
Loss in iteration 175 : 0.46102542147269965
Loss in iteration 176 : 0.4610122136016451
Loss in iteration 177 : 0.4609991261883443
Loss in iteration 178 : 0.4609861570979085
Loss in iteration 179 : 0.46097330430587247
Loss in iteration 180 : 0.460960565810118
Loss in iteration 181 : 0.46094793961806557
Loss in iteration 182 : 0.4609354238142801
Loss in iteration 183 : 0.46092301657170137
Loss in iteration 184 : 0.4609107160953949
Loss in iteration 185 : 0.46089852060886516
Loss in iteration 186 : 0.4608864283967018
Loss in iteration 187 : 0.4608744378148093
Loss in iteration 188 : 0.4608625472531901
Loss in iteration 189 : 0.46085075512291007
Loss in iteration 190 : 0.46083905988277984
Loss in iteration 191 : 0.4608274600484383
Loss in iteration 192 : 0.46081595416884746
Loss in iteration 193 : 0.4608045408159634
Loss in iteration 194 : 0.4607932186015315
Loss in iteration 195 : 0.46078198618432487
Loss in iteration 196 : 0.4607708422550014
Loss in iteration 197 : 0.4607597855277845
Loss in iteration 198 : 0.46074881475046153
Loss in iteration 199 : 0.46073792870949054
Loss in iteration 200 : 0.4607271262201293
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.786, training accuracy 0.788125, time elapsed: 5199 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6806778890716203
Loss in iteration 3 : 0.6696052239105007
Loss in iteration 4 : 0.6567344135062316
Loss in iteration 5 : 0.6385808516907729
Loss in iteration 6 : 0.621937587500877
Loss in iteration 7 : 0.607441152484872
Loss in iteration 8 : 0.5904614654603279
Loss in iteration 9 : 0.5735861345437663
Loss in iteration 10 : 0.5606330825496714
Loss in iteration 11 : 0.5497391257663167
Loss in iteration 12 : 0.5390545486426462
Loss in iteration 13 : 0.5299886583318892
Loss in iteration 14 : 0.5232598363746286
Loss in iteration 15 : 0.5172938955352643
Loss in iteration 16 : 0.511240740047927
Loss in iteration 17 : 0.5060381920074865
Loss in iteration 18 : 0.5022334196461944
Loss in iteration 19 : 0.49909084345151655
Loss in iteration 20 : 0.4960042025653235
Loss in iteration 21 : 0.49329581794315297
Loss in iteration 22 : 0.4913422170519266
Loss in iteration 23 : 0.48983131058952967
Loss in iteration 24 : 0.4882714550693877
Loss in iteration 25 : 0.48667916770201014
Loss in iteration 26 : 0.48536834982602467
Loss in iteration 27 : 0.4843719304134413
Loss in iteration 28 : 0.4834306872015714
Loss in iteration 29 : 0.48242049910815216
Loss in iteration 30 : 0.48148971690430037
Loss in iteration 31 : 0.48077196324751753
Loss in iteration 32 : 0.4801787296329926
Loss in iteration 33 : 0.4795514649974598
Loss in iteration 34 : 0.47888183589103506
Loss in iteration 35 : 0.47827536312204916
Loss in iteration 36 : 0.4777655613791486
Loss in iteration 37 : 0.47727215663547307
Loss in iteration 38 : 0.4767315567152509
Loss in iteration 39 : 0.4761807580994529
Loss in iteration 40 : 0.47568611025231466
Loss in iteration 41 : 0.47524626627884936
Loss in iteration 42 : 0.47480861385856493
Loss in iteration 43 : 0.4743530116178126
Loss in iteration 44 : 0.4739124617680204
Loss in iteration 45 : 0.4735145501094314
Loss in iteration 46 : 0.47314140238538427
Loss in iteration 47 : 0.47276095609532515
Loss in iteration 48 : 0.4723727201991682
Loss in iteration 49 : 0.4720019013041594
Loss in iteration 50 : 0.4716598936129784
Loss in iteration 51 : 0.471332062230072
Loss in iteration 52 : 0.47100408466922705
Loss in iteration 53 : 0.4706824760294747
Loss in iteration 54 : 0.4703816977538718
Loss in iteration 55 : 0.4701021848630332
Loss in iteration 56 : 0.46983168420020555
Loss in iteration 57 : 0.4695637250743733
Loss in iteration 58 : 0.46930443898345914
Loss in iteration 59 : 0.4690606226678321
Loss in iteration 60 : 0.4688293388523707
Loss in iteration 61 : 0.4686033173715042
Loss in iteration 62 : 0.46838162906013586
Loss in iteration 63 : 0.4681696491652081
Loss in iteration 64 : 0.4679702551922946
Loss in iteration 65 : 0.46778009972017814
Loss in iteration 66 : 0.4675951464667985
Loss in iteration 67 : 0.46741597721099315
Loss in iteration 68 : 0.4672455267827976
Loss in iteration 69 : 0.4670838984436973
Loss in iteration 70 : 0.4669281342527379
Loss in iteration 71 : 0.46677638462287896
Loss in iteration 72 : 0.4666298261361178
Loss in iteration 73 : 0.4664900544367217
Loss in iteration 74 : 0.46635644310751057
Loss in iteration 75 : 0.46622715849986796
Loss in iteration 76 : 0.46610169134958246
Loss in iteration 77 : 0.46598103900502713
Loss in iteration 78 : 0.4658657051876281
Loss in iteration 79 : 0.46575473467343054
Loss in iteration 80 : 0.4656469604169424
Loss in iteration 81 : 0.46554231910257843
Loss in iteration 82 : 0.46544139556641834
Loss in iteration 83 : 0.4653441843858236
Loss in iteration 84 : 0.46524996083022246
Loss in iteration 85 : 0.4651582268751368
Loss in iteration 86 : 0.46506918676350073
Loss in iteration 87 : 0.4649831585577648
Loss in iteration 88 : 0.46489993957834347
Loss in iteration 89 : 0.4648190220799843
Loss in iteration 90 : 0.464740195352888
Loss in iteration 91 : 0.4646636170185134
Loss in iteration 92 : 0.464589357318046
Loss in iteration 93 : 0.4645171609500868
Loss in iteration 94 : 0.464446723523656
Loss in iteration 95 : 0.46437799860540424
Loss in iteration 96 : 0.4643110984802031
Loss in iteration 97 : 0.46424600385754794
Loss in iteration 98 : 0.4641825237468764
Loss in iteration 99 : 0.4641205103944208
Loss in iteration 100 : 0.46405997513928166
Loss in iteration 101 : 0.46400095930493274
Loss in iteration 102 : 0.4639433882523985
Loss in iteration 103 : 0.4638871181373737
Loss in iteration 104 : 0.4638320744437374
Loss in iteration 105 : 0.46377827143000117
Loss in iteration 106 : 0.4637257087670067
Loss in iteration 107 : 0.46367431430426526
Loss in iteration 108 : 0.4636240040679356
Loss in iteration 109 : 0.4635747516404217
Loss in iteration 110 : 0.4635265663121353
Loss in iteration 111 : 0.4634794270976811
Loss in iteration 112 : 0.4634332731111901
Loss in iteration 113 : 0.4633880526563875
Loss in iteration 114 : 0.463343750710159
Loss in iteration 115 : 0.4633003605927467
Loss in iteration 116 : 0.46325785091030525
Loss in iteration 117 : 0.4632161757037431
Loss in iteration 118 : 0.4631753054829446
Loss in iteration 119 : 0.46313523134032253
Loss in iteration 120 : 0.46309594114496255
Loss in iteration 121 : 0.4630574062718376
Loss in iteration 122 : 0.4630195951982466
Loss in iteration 123 : 0.4629824894319087
Loss in iteration 124 : 0.46294607873669724
Loss in iteration 125 : 0.4629103462964268
Loss in iteration 126 : 0.4628752667061634
Loss in iteration 127 : 0.4628408172862423
Loss in iteration 128 : 0.46280698432258527
Loss in iteration 129 : 0.4627737564436014
Loss in iteration 130 : 0.4627411169213129
Loss in iteration 131 : 0.4627090459255123
Loss in iteration 132 : 0.46267752758445063
Loss in iteration 133 : 0.46264655086003675
Loss in iteration 134 : 0.4626161040430848
Loss in iteration 135 : 0.4625861717442975
Loss in iteration 136 : 0.4625567381128846
Loss in iteration 137 : 0.4625277905710342
Loss in iteration 138 : 0.46249931872911404
Loss in iteration 139 : 0.46247131095868893
Loss in iteration 140 : 0.4624437539280249
Loss in iteration 141 : 0.4624166352037086
Loss in iteration 142 : 0.4623899446528141
Loss in iteration 143 : 0.4623636728592854
Loss in iteration 144 : 0.4623378093108569
Loss in iteration 145 : 0.4623123429183767
Loss in iteration 146 : 0.46228726366160333
Loss in iteration 147 : 0.4622625628027936
Loss in iteration 148 : 0.4622382316239479
Loss in iteration 149 : 0.46221426074470306
Loss in iteration 150 : 0.4621906408752099
Loss in iteration 151 : 0.4621673636720041
Loss in iteration 152 : 0.46214442146787854
Loss in iteration 153 : 0.46212180646319334
Loss in iteration 154 : 0.4620995106092931
Loss in iteration 155 : 0.4620775261999658
Loss in iteration 156 : 0.46205584618255463
Loss in iteration 157 : 0.4620344637854135
Loss in iteration 158 : 0.46201337210420046
Loss in iteration 159 : 0.46199256422924395
Loss in iteration 160 : 0.4619720336236019
Loss in iteration 161 : 0.46195177416399574
Loss in iteration 162 : 0.46193177984619926
Loss in iteration 163 : 0.46191204463092417
Loss in iteration 164 : 0.4618925626161091
Loss in iteration 165 : 0.4618733282228016
Loss in iteration 166 : 0.46185433612057814
Loss in iteration 167 : 0.46183558103844574
Loss in iteration 168 : 0.4618170577414014
Loss in iteration 169 : 0.46179876116525725
Loss in iteration 170 : 0.46178068648052345
Loss in iteration 171 : 0.4617628290017783
Loss in iteration 172 : 0.46174518409545756
Loss in iteration 173 : 0.46172774721300325
Loss in iteration 174 : 0.46171051397436524
Loss in iteration 175 : 0.4616934801691382
Loss in iteration 176 : 0.46167664168429734
Loss in iteration 177 : 0.46165999446973816
Loss in iteration 178 : 0.4616435345779135
Loss in iteration 179 : 0.4616272582016751
Loss in iteration 180 : 0.46161116165160637
Loss in iteration 181 : 0.4615952413117887
Loss in iteration 182 : 0.46157949363675127
Loss in iteration 183 : 0.46156391518226575
Loss in iteration 184 : 0.4615485026160371
Loss in iteration 185 : 0.4615332526933228
Loss in iteration 186 : 0.46151816223538217
Loss in iteration 187 : 0.4615032281375291
Loss in iteration 188 : 0.461488447386266
Loss in iteration 189 : 0.46147381705585133
Loss in iteration 190 : 0.461459334289602
Loss in iteration 191 : 0.4614449962921303
Loss in iteration 192 : 0.4614308003383625
Loss in iteration 193 : 0.46141674378028447
Loss in iteration 194 : 0.4614028240393435
Loss in iteration 195 : 0.4613890385953318
Loss in iteration 196 : 0.4613753849856953
Loss in iteration 197 : 0.46136186081178465
Loss in iteration 198 : 0.4613484637393645
Loss in iteration 199 : 0.4613351914913232
Loss in iteration 200 : 0.46132204184211406
Testing accuracy  of updater 9 on alg 0 with rate 0.14 = 0.786, training accuracy 0.7885, time elapsed: 5187 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6845068787411874
Loss in iteration 3 : 0.6748176014883949
Loss in iteration 4 : 0.666332803669467
Loss in iteration 5 : 0.6557653063093918
Loss in iteration 6 : 0.6423068460283242
Loss in iteration 7 : 0.6289893940133059
Loss in iteration 8 : 0.6175305141097852
Loss in iteration 9 : 0.6062670310172041
Loss in iteration 10 : 0.5937647626894964
Loss in iteration 11 : 0.581272394734874
Loss in iteration 12 : 0.5706099927931049
Loss in iteration 13 : 0.5617875916928778
Loss in iteration 14 : 0.5536715265331825
Loss in iteration 15 : 0.5457995438375298
Loss in iteration 16 : 0.5386946926247732
Loss in iteration 17 : 0.5328111145800737
Loss in iteration 18 : 0.5278405740652813
Loss in iteration 19 : 0.523158307718078
Loss in iteration 20 : 0.5185732282383533
Loss in iteration 21 : 0.5143907128607126
Loss in iteration 22 : 0.5108894002590231
Loss in iteration 23 : 0.507971940833178
Loss in iteration 24 : 0.5053325441021295
Loss in iteration 25 : 0.5028100637293023
Loss in iteration 26 : 0.5004902820718172
Loss in iteration 27 : 0.4985128636101431
Loss in iteration 28 : 0.49686567790324987
Loss in iteration 29 : 0.4953926049372308
Loss in iteration 30 : 0.49395893351433506
Loss in iteration 31 : 0.492563070232731
Loss in iteration 32 : 0.49128888982945174
Loss in iteration 33 : 0.4901845391340079
Loss in iteration 34 : 0.4892091808765693
Loss in iteration 35 : 0.488285525021566
Loss in iteration 36 : 0.48737930359839615
Loss in iteration 37 : 0.48651833430940233
Loss in iteration 38 : 0.4857448389240088
Loss in iteration 39 : 0.4850631455381618
Loss in iteration 40 : 0.48443623223788124
Loss in iteration 41 : 0.4838245873297647
Loss in iteration 42 : 0.4832201060744979
Loss in iteration 43 : 0.4826425742092727
Loss in iteration 44 : 0.48211010451651465
Loss in iteration 45 : 0.4816181188397719
Loss in iteration 46 : 0.48114569895335385
Loss in iteration 47 : 0.4806773149851167
Loss in iteration 48 : 0.4802154149421432
Loss in iteration 49 : 0.47977332410736634
Loss in iteration 50 : 0.4793591947275721
Loss in iteration 51 : 0.4789682758353729
Loss in iteration 52 : 0.4785892607123462
Loss in iteration 53 : 0.47821569174311107
Loss in iteration 54 : 0.4778503342257774
Loss in iteration 55 : 0.4774997002123856
Loss in iteration 56 : 0.4771660346210184
Loss in iteration 57 : 0.47684507710009216
Loss in iteration 58 : 0.4765306660316069
Loss in iteration 59 : 0.4762204222162731
Loss in iteration 60 : 0.47591683019376135
Loss in iteration 61 : 0.475623559819949
Loss in iteration 62 : 0.4753414582913798
Loss in iteration 63 : 0.4750680860822111
Loss in iteration 64 : 0.4748005147488663
Loss in iteration 65 : 0.4745380380158175
Loss in iteration 66 : 0.4742821972179461
Loss in iteration 67 : 0.474034618776912
Loss in iteration 68 : 0.4737951705214647
Loss in iteration 69 : 0.47356215164529564
Loss in iteration 70 : 0.4733339491316558
Loss in iteration 71 : 0.47311029979889074
Loss in iteration 72 : 0.47289203473404545
Loss in iteration 73 : 0.47267984975881755
Loss in iteration 74 : 0.4724734692484228
Loss in iteration 75 : 0.47227192790178807
Loss in iteration 76 : 0.4720744829885848
Loss in iteration 77 : 0.4718811571423271
Loss in iteration 78 : 0.4716924557368726
Loss in iteration 79 : 0.47150868041374716
Loss in iteration 80 : 0.47132956746437343
Loss in iteration 81 : 0.4711545318062441
Loss in iteration 82 : 0.4709831670664308
Loss in iteration 83 : 0.4708154764073229
Loss in iteration 84 : 0.47065166753320475
Loss in iteration 85 : 0.47049179374157324
Loss in iteration 86 : 0.4703356158898153
Loss in iteration 87 : 0.47018277692915683
Loss in iteration 88 : 0.4700330635519126
Loss in iteration 89 : 0.4698864881715433
Loss in iteration 90 : 0.4697431461861367
Loss in iteration 91 : 0.4696030264002053
Loss in iteration 92 : 0.46946596330102736
Loss in iteration 93 : 0.4693317485847424
Loss in iteration 94 : 0.4692002641292469
Loss in iteration 95 : 0.46907150456535124
Loss in iteration 96 : 0.46894548833587923
Loss in iteration 97 : 0.4688221631811233
Loss in iteration 98 : 0.46870139766123176
Loss in iteration 99 : 0.4685830511965491
Loss in iteration 100 : 0.4684670417204159
Loss in iteration 101 : 0.46835334751659335
Loss in iteration 102 : 0.4682419543289026
Loss in iteration 103 : 0.4681328085912853
Loss in iteration 104 : 0.46802581996813813
Loss in iteration 105 : 0.4679209010180819
Loss in iteration 106 : 0.46781799866680435
Loss in iteration 107 : 0.4677170886174161
Loss in iteration 108 : 0.4676181447187202
Loss in iteration 109 : 0.4675211170169996
Loss in iteration 110 : 0.4674259376695679
Loss in iteration 111 : 0.4673325438123412
Loss in iteration 112 : 0.46724089244672845
Loss in iteration 113 : 0.46715095476106777
Loss in iteration 114 : 0.46706269926882454
Loss in iteration 115 : 0.46697608199255725
Loss in iteration 116 : 0.466891051726499
Loss in iteration 117 : 0.4668075625049458
Loss in iteration 118 : 0.46672558001646214
Loss in iteration 119 : 0.4666450769603488
Loss in iteration 120 : 0.46656602378935486
Loss in iteration 121 : 0.4664883844257613
Loss in iteration 122 : 0.4664121199732789
Loss in iteration 123 : 0.4663371952734506
Loss in iteration 124 : 0.4662635814548208
Loss in iteration 125 : 0.4661912527732292
Loss in iteration 126 : 0.46612018182569465
Loss in iteration 127 : 0.46605033800598106
Loss in iteration 128 : 0.4659816900614786
Loss in iteration 129 : 0.46591420954095014
Loss in iteration 130 : 0.46584787168041
Loss in iteration 131 : 0.46578265336304286
Loss in iteration 132 : 0.4657185306346501
Loss in iteration 133 : 0.46565547819737296
Loss in iteration 134 : 0.46559347095237785
Loss in iteration 135 : 0.4655324856946728
Loss in iteration 136 : 0.46547250128088014
Loss in iteration 137 : 0.4654134973689215
Loss in iteration 138 : 0.4653554531747227
Loss in iteration 139 : 0.4652983474041862
Loss in iteration 140 : 0.4652421591828882
Loss in iteration 141 : 0.4651868688938058
Loss in iteration 142 : 0.4651324581328478
Loss in iteration 143 : 0.46507890898615206
Loss in iteration 144 : 0.46502620344136697
Loss in iteration 145 : 0.4649743234610936
Loss in iteration 146 : 0.46492325151171426
Loss in iteration 147 : 0.46487297094277186
Loss in iteration 148 : 0.4648234658675986
Loss in iteration 149 : 0.4647747207374588
Loss in iteration 150 : 0.4647267200579017
Loss in iteration 151 : 0.4646794484763241
Loss in iteration 152 : 0.46463289107348393
Loss in iteration 153 : 0.4645870335303148
Loss in iteration 154 : 0.4645418620242369
Loss in iteration 155 : 0.46449736299632294
Loss in iteration 156 : 0.46445352302812465
Loss in iteration 157 : 0.464410328917515
Loss in iteration 158 : 0.4643677678360564
Loss in iteration 159 : 0.464325827394645
Loss in iteration 160 : 0.4642844955644297
Loss in iteration 161 : 0.46424376054807986
Loss in iteration 162 : 0.4642036107258283
Loss in iteration 163 : 0.4641640347056396
Loss in iteration 164 : 0.4641250214017885
Loss in iteration 165 : 0.46408656005343135
Loss in iteration 166 : 0.4640486401691365
Loss in iteration 167 : 0.4640112514572896
Loss in iteration 168 : 0.4639743838050916
Loss in iteration 169 : 0.4639380273111365
Loss in iteration 170 : 0.46390217232481096
Loss in iteration 171 : 0.4638668094482974
Loss in iteration 172 : 0.46383192950117574
Loss in iteration 173 : 0.46379752348343123
Loss in iteration 174 : 0.4637635825674629
Loss in iteration 175 : 0.46373009811623556
Loss in iteration 176 : 0.46369706170028274
Loss in iteration 177 : 0.4636644650926336
Loss in iteration 178 : 0.46363230024591295
Loss in iteration 179 : 0.4636005592722678
Loss in iteration 180 : 0.46356923444038656
Loss in iteration 181 : 0.46353831818496993
Loss in iteration 182 : 0.4635078031131849
Loss in iteration 183 : 0.46347768199855854
Loss in iteration 184 : 0.46344794776673726
Loss in iteration 185 : 0.4634185934846012
Loss in iteration 186 : 0.4633896123589767
Loss in iteration 187 : 0.4633609977408867
Loss in iteration 188 : 0.4633327431268917
Loss in iteration 189 : 0.4633048421534868
Loss in iteration 190 : 0.46327728858804623
Loss in iteration 191 : 0.4632500763225124
Loss in iteration 192 : 0.4632231993723001
Loss in iteration 193 : 0.46319665187751696
Loss in iteration 194 : 0.46317042810199766
Loss in iteration 195 : 0.4631445224286815
Loss in iteration 196 : 0.46311892935373977
Loss in iteration 197 : 0.46309364348266063
Loss in iteration 198 : 0.4630686595291049
Loss in iteration 199 : 0.4630439723146195
Loss in iteration 200 : 0.4630195767668843
Testing accuracy  of updater 9 on alg 0 with rate 0.08000000000000002 = 0.786, training accuracy 0.790875, time elapsed: 5273 millisecond.
Loss in iteration 1 : 0.6931471805599112
Loss in iteration 2 : 0.6904660280924654
Loss in iteration 3 : 0.6861557742494893
Loss in iteration 4 : 0.6812828515116262
Loss in iteration 5 : 0.6765763225006475
Loss in iteration 6 : 0.6722266748446424
Loss in iteration 7 : 0.6679980112553642
Loss in iteration 8 : 0.6635149742541211
Loss in iteration 9 : 0.6585291443838502
Loss in iteration 10 : 0.6530426077130564
Loss in iteration 11 : 0.6472734455859624
Loss in iteration 12 : 0.6415221019522998
Loss in iteration 13 : 0.6360241738994042
Loss in iteration 14 : 0.6308615086652818
Loss in iteration 15 : 0.6259627198391576
Loss in iteration 16 : 0.6211761349002481
Loss in iteration 17 : 0.6163661173104507
Loss in iteration 18 : 0.6114817770358923
Loss in iteration 19 : 0.6065710434180176
Loss in iteration 20 : 0.6017452575979757
Loss in iteration 21 : 0.5971217539280833
Loss in iteration 22 : 0.5927756771833599
Loss in iteration 23 : 0.588720349795918
Loss in iteration 24 : 0.5849177382314431
Loss in iteration 25 : 0.5813069459383641
Loss in iteration 26 : 0.5778341922234701
Loss in iteration 27 : 0.5744715998584039
Loss in iteration 28 : 0.5712200936773276
Loss in iteration 29 : 0.5680993325318294
Loss in iteration 30 : 0.5651320243046164
Loss in iteration 31 : 0.5623304243139918
Loss in iteration 32 : 0.559690124716907
Loss in iteration 33 : 0.5571921505050593
Loss in iteration 34 : 0.5548107743734252
Loss in iteration 35 : 0.5525226774093493
Loss in iteration 36 : 0.5503134590936962
Loss in iteration 37 : 0.5481794256786121
Loss in iteration 38 : 0.5461249325793003
Loss in iteration 39 : 0.5441572695496243
Loss in iteration 40 : 0.542281606974273
Loss in iteration 41 : 0.5404979323387767
Loss in iteration 42 : 0.5388006985953736
Loss in iteration 43 : 0.5371807030677653
Loss in iteration 44 : 0.535627980433514
Loss in iteration 45 : 0.5341343915752035
Loss in iteration 46 : 0.5326950053346736
Loss in iteration 47 : 0.5313080236532572
Loss in iteration 48 : 0.5299735961452293
Loss in iteration 49 : 0.5286922047415863
Loss in iteration 50 : 0.5274633107023037
Loss in iteration 51 : 0.5262847122301557
Loss in iteration 52 : 0.5251527048749217
Loss in iteration 53 : 0.5240628220977103
Loss in iteration 54 : 0.5230107658699538
Loss in iteration 55 : 0.5219931473747621
Loss in iteration 56 : 0.521007804829242
Loss in iteration 57 : 0.5200536667004119
Loss in iteration 58 : 0.5191302992534889
Loss in iteration 59 : 0.5182373619060489
Loss in iteration 60 : 0.5173741801035668
Loss in iteration 61 : 0.5165395595099911
Loss in iteration 62 : 0.5157318536299642
Loss in iteration 63 : 0.5149492049372625
Loss in iteration 64 : 0.5141898355234884
Loss in iteration 65 : 0.5134522722996433
Loss in iteration 66 : 0.5127354392053572
Loss in iteration 67 : 0.5120386098532222
Loss in iteration 68 : 0.5113612644321581
Loss in iteration 69 : 0.510702919270978
Loss in iteration 70 : 0.5100629931505813
Loss in iteration 71 : 0.5094407487785935
Loss in iteration 72 : 0.50883531410238
Loss in iteration 73 : 0.5082457596408125
Loss in iteration 74 : 0.5076711935132139
Loss in iteration 75 : 0.5071108374690891
Loss in iteration 76 : 0.5065640611208047
Loss in iteration 77 : 0.5060303704715525
Loss in iteration 78 : 0.5055093632156067
Loss in iteration 79 : 0.5050006720764765
Loss in iteration 80 : 0.504503917203428
Loss in iteration 81 : 0.5040186813128694
Loss in iteration 82 : 0.5035445107924641
Loss in iteration 83 : 0.5030809365775831
Loss in iteration 84 : 0.5026275032233596
Loss in iteration 85 : 0.5021837942176165
Loss in iteration 86 : 0.5017494453048356
Loss in iteration 87 : 0.5013241433233769
Loss in iteration 88 : 0.5009076134627095
Loss in iteration 89 : 0.500499601180947
Loss in iteration 90 : 0.5000998555921914
Loss in iteration 91 : 0.49970811930879594
Loss in iteration 92 : 0.49932412659895786
Loss in iteration 93 : 0.4989476085990328
Loss in iteration 94 : 0.49857830224128824
Loss in iteration 95 : 0.49821595900344146
Loss in iteration 96 : 0.4978603504342005
Loss in iteration 97 : 0.4975112690961036
Loss in iteration 98 : 0.4971685253654305
Loss in iteration 99 : 0.4968319418208704
Loss in iteration 100 : 0.49650134741266466
Loss in iteration 101 : 0.4961765732473182
Loss in iteration 102 : 0.49585745093178524
Loss in iteration 103 : 0.4955438133963647
Loss in iteration 104 : 0.49523549732161537
Loss in iteration 105 : 0.49493234594620644
Loss in iteration 106 : 0.49463421115489303
Loss in iteration 107 : 0.4943409542045278
Loss in iteration 108 : 0.4940524450238625
Loss in iteration 109 : 0.4937685605069631
Loss in iteration 110 : 0.49348918247107576
Loss in iteration 111 : 0.4932141959320073
Loss in iteration 112 : 0.4929434881227929
Loss in iteration 113 : 0.49267694836016684
Loss in iteration 114 : 0.49241446857126603
Loss in iteration 115 : 0.4921559441192549
Loss in iteration 116 : 0.49190127454417926
Loss in iteration 117 : 0.49165036394161266
Loss in iteration 118 : 0.4914031208766974
Loss in iteration 119 : 0.49115945790461857
Loss in iteration 120 : 0.4909192908846473
Loss in iteration 121 : 0.4906825383073281
Loss in iteration 122 : 0.4904491208093199
Loss in iteration 123 : 0.49021896095763984
Loss in iteration 124 : 0.4899919832840308
Loss in iteration 125 : 0.48976811447508983
Loss in iteration 126 : 0.48954728359377186
Loss in iteration 127 : 0.48932942222372433
Loss in iteration 128 : 0.4891144644754887
Loss in iteration 129 : 0.4889023468514335
Loss in iteration 130 : 0.4886930080137029
Loss in iteration 131 : 0.4884863885233334
Loss in iteration 132 : 0.4882824306159129
Loss in iteration 133 : 0.48808107805591644
Loss in iteration 134 : 0.48788227607945645
Loss in iteration 135 : 0.4876859714059751
Loss in iteration 136 : 0.4874921122819764
Loss in iteration 137 : 0.4873006485175981
Loss in iteration 138 : 0.4871115314874674
Loss in iteration 139 : 0.4869247140849325
Loss in iteration 140 : 0.48674015063644754
Loss in iteration 141 : 0.48655779679495814
Loss in iteration 142 : 0.48637760943488423
Loss in iteration 143 : 0.48619954656706793
Loss in iteration 144 : 0.48602356728282003
Loss in iteration 145 : 0.48584963172583834
Loss in iteration 146 : 0.4856777010827598
Loss in iteration 147 : 0.4855077375795779
Loss in iteration 148 : 0.48533970447234803
Loss in iteration 149 : 0.48517356602516215
Loss in iteration 150 : 0.4850092874743061
Loss in iteration 151 : 0.4848468349825441
Loss in iteration 152 : 0.4846861755903108
Loss in iteration 153 : 0.48452727717064636
Loss in iteration 154 : 0.48437010839261996
Loss in iteration 155 : 0.48421463869469505
Loss in iteration 156 : 0.4840608382664354
Loss in iteration 157 : 0.48390867803488263
Loss in iteration 158 : 0.4837581296514819
Loss in iteration 159 : 0.4836091654762922
Loss in iteration 160 : 0.4834617585579747
Loss in iteration 161 : 0.4833158826098973
Loss in iteration 162 : 0.48317151198412217
Loss in iteration 163 : 0.48302862164556476
Loss in iteration 164 : 0.48288718714837087
Loss in iteration 165 : 0.48274718461569366
Loss in iteration 166 : 0.4826085907229429
Loss in iteration 167 : 0.48247138268371753
Loss in iteration 168 : 0.48233553823712666
Loss in iteration 169 : 0.48220103563520844
Loss in iteration 170 : 0.48206785362958476
Loss in iteration 171 : 0.4819359714570476
Loss in iteration 172 : 0.48180536882436303
Loss in iteration 173 : 0.48167602589292186
Loss in iteration 174 : 0.4815479232639533
Loss in iteration 175 : 0.48142104196484536
Loss in iteration 176 : 0.481295363436814
Loss in iteration 177 : 0.48117086952382376
Loss in iteration 178 : 0.48104754246239745
Loss in iteration 179 : 0.4809253648718927
Loss in iteration 180 : 0.48080431974483695
Loss in iteration 181 : 0.4806843904371
Loss in iteration 182 : 0.480565560657881
Loss in iteration 183 : 0.4804478144596251
Loss in iteration 184 : 0.4803311362281126
Loss in iteration 185 : 0.48021551067292434
Loss in iteration 186 : 0.48010092281845596
Loss in iteration 187 : 0.47998735799548764
Loss in iteration 188 : 0.4798748018332879
Loss in iteration 189 : 0.47976324025209444
Loss in iteration 190 : 0.47965265945584606
Loss in iteration 191 : 0.47954304592504204
Loss in iteration 192 : 0.4794343864096735
Loss in iteration 193 : 0.4793266679222362
Loss in iteration 194 : 0.4792198777308614
Loss in iteration 195 : 0.4791140033526401
Loss in iteration 196 : 0.47900903254720023
Loss in iteration 197 : 0.47890495331056165
Loss in iteration 198 : 0.4788017538692687
Loss in iteration 199 : 0.47869942267476784
Loss in iteration 200 : 0.47859794839797554
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.782, training accuracy 0.782125, time elapsed: 5203 millisecond.
