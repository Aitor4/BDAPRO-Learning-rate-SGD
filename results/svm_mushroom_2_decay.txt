objc[3478]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10da124c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10da964e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 10:13:10 INFO SparkContext: Running Spark version 2.0.0
18/02/27 10:13:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 10:13:11 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 10:13:11 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 10:13:11 INFO SecurityManager: Changing view acls groups to: 
18/02/27 10:13:11 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 10:13:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 10:13:12 INFO Utils: Successfully started service 'sparkDriver' on port 50792.
18/02/27 10:13:12 INFO SparkEnv: Registering MapOutputTracker
18/02/27 10:13:12 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 10:13:12 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-b917bbb3-5ac0-4773-b8ad-bc027f431e86
18/02/27 10:13:12 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 10:13:12 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 10:13:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 10:13:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 10:13:13 INFO Executor: Starting executor ID driver on host localhost
18/02/27 10:13:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50793.
18/02/27 10:13:13 INFO NettyBlockTransferService: Server created on 192.168.2.140:50793
18/02/27 10:13:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50793)
18/02/27 10:13:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50793 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50793)
18/02/27 10:13:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50793)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 12.298271355201228
Loss in iteration 3 : 2.1568168216018733
Loss in iteration 4 : 1.4043921307439018
Loss in iteration 5 : 1.176760422029621
Loss in iteration 6 : 1.0706215741064038
Loss in iteration 7 : 0.9950283319826249
Loss in iteration 8 : 0.9311431084891094
Loss in iteration 9 : 0.8747761040233676
Loss in iteration 10 : 0.8231165327785128
Loss in iteration 11 : 0.7764296231408032
Loss in iteration 12 : 0.7333604978956993
Loss in iteration 13 : 0.6931348823561193
Loss in iteration 14 : 0.6554921523556444
Loss in iteration 15 : 0.620340297873922
Loss in iteration 16 : 0.587093029542629
Loss in iteration 17 : 0.5555303397992462
Loss in iteration 18 : 0.5258686220150699
Loss in iteration 19 : 0.49805303766022396
Loss in iteration 20 : 0.47238394919789234
Loss in iteration 21 : 0.44788720299807144
Loss in iteration 22 : 0.42419889810604033
Loss in iteration 23 : 0.4010961676629019
Loss in iteration 24 : 0.3788223628937217
Loss in iteration 25 : 0.3581275456284777
Loss in iteration 26 : 0.3396494166914164
Loss in iteration 27 : 0.3230769984887129
Loss in iteration 28 : 0.3089873881019949
Loss in iteration 29 : 0.29687785476611783
Loss in iteration 30 : 0.2863161895629462
Loss in iteration 31 : 0.2770236233808337
Loss in iteration 32 : 0.2683564760245306
Loss in iteration 33 : 0.26026643908286784
Loss in iteration 34 : 0.2528967831082103
Loss in iteration 35 : 0.24643280356599184
Loss in iteration 36 : 0.24042126988061607
Loss in iteration 37 : 0.23479310384550275
Loss in iteration 38 : 0.22944668154524395
Loss in iteration 39 : 0.2241877620294078
Loss in iteration 40 : 0.2190458226144257
Loss in iteration 41 : 0.21413563581165926
Loss in iteration 42 : 0.2096929978366744
Loss in iteration 43 : 0.2055025393962934
Loss in iteration 44 : 0.20171489887822383
Loss in iteration 45 : 0.1980633978238488
Loss in iteration 46 : 0.19470950921215785
Loss in iteration 47 : 0.19139877491641005
Loss in iteration 48 : 0.18813483958529775
Loss in iteration 49 : 0.1850991482018692
Loss in iteration 50 : 0.18226304111673342
Loss in iteration 51 : 0.1794621780799742
Loss in iteration 52 : 0.17678706640829442
Loss in iteration 53 : 0.1744665579012668
Loss in iteration 54 : 0.17219826941953595
Loss in iteration 55 : 0.1702330173603598
Loss in iteration 56 : 0.16800398996273988
Loss in iteration 57 : 0.16606648588410527
Loss in iteration 58 : 0.1641102566275483
Loss in iteration 59 : 0.16226309217500323
Loss in iteration 60 : 0.1604688175208679
Loss in iteration 61 : 0.158783867972669
Loss in iteration 62 : 0.1569136088439966
Loss in iteration 63 : 0.1552838791680783
Loss in iteration 64 : 0.15361759429856417
Loss in iteration 65 : 0.15203474878360054
Loss in iteration 66 : 0.15058942820187945
Loss in iteration 67 : 0.1491175552614123
Loss in iteration 68 : 0.14758814062654024
Loss in iteration 69 : 0.14620646175928015
Loss in iteration 70 : 0.14473817755994806
Loss in iteration 71 : 0.14335669659391814
Loss in iteration 72 : 0.14198426966693614
Loss in iteration 73 : 0.14062188898398084
Loss in iteration 74 : 0.13926795255331975
Loss in iteration 75 : 0.13792342483322542
Loss in iteration 76 : 0.13659120663322527
Loss in iteration 77 : 0.13526628499787163
Loss in iteration 78 : 0.13394684446910313
Loss in iteration 79 : 0.1326386318905012
Loss in iteration 80 : 0.131340976792465
Loss in iteration 81 : 0.1300521989158709
Loss in iteration 82 : 0.1287850022608562
Loss in iteration 83 : 0.1275278262552837
Loss in iteration 84 : 0.12627392469809418
Loss in iteration 85 : 0.12503387131577232
Loss in iteration 86 : 0.12379391804175546
Loss in iteration 87 : 0.122565830845494
Loss in iteration 88 : 0.12135062129544488
Loss in iteration 89 : 0.1201495924842375
Loss in iteration 90 : 0.11896646673978278
Loss in iteration 91 : 0.11778997602433436
Loss in iteration 92 : 0.11662092275518127
Loss in iteration 93 : 0.11546507705324084
Loss in iteration 94 : 0.1143158519247954
Loss in iteration 95 : 0.11316169955144095
Loss in iteration 96 : 0.11203062695552114
Loss in iteration 97 : 0.11091299191486066
Loss in iteration 98 : 0.10980505913092152
Loss in iteration 99 : 0.10871051402142934
Loss in iteration 100 : 0.10770749426071377
Loss in iteration 101 : 0.10670617285758928
Loss in iteration 102 : 0.10570695816773552
Loss in iteration 103 : 0.10471878844763129
Loss in iteration 104 : 0.10373369380921917
Loss in iteration 105 : 0.10278809257206084
Loss in iteration 106 : 0.10192634237179607
Loss in iteration 107 : 0.1011048377159324
Loss in iteration 108 : 0.10028666531958971
Loss in iteration 109 : 0.09947321972584681
Loss in iteration 110 : 0.09866243544943536
Loss in iteration 111 : 0.09785630745306692
Loss in iteration 112 : 0.09705476386491149
Loss in iteration 113 : 0.0962717256736702
Testing accuracy  of updater 0 on alg 1 with rate 70.0 = 0.9128888888888889, training accuracy 0.989997142040583, time elapsed: 6393 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 8.651901461904647
Loss in iteration 3 : 1.53179739025043
Loss in iteration 4 : 0.9960884142513413
Loss in iteration 5 : 0.8334373508722734
Loss in iteration 6 : 0.7606357891314309
Loss in iteration 7 : 0.7086994793618516
Loss in iteration 8 : 0.6639046740174575
Loss in iteration 9 : 0.6238744095388813
Loss in iteration 10 : 0.5874072660434823
Loss in iteration 11 : 0.5544079935020885
Loss in iteration 12 : 0.523959056744547
Loss in iteration 13 : 0.49584881432261846
Loss in iteration 14 : 0.46941391659290155
Loss in iteration 15 : 0.44452177120720754
Loss in iteration 16 : 0.42111825021183646
Loss in iteration 17 : 0.3988924652940312
Loss in iteration 18 : 0.3779363006711586
Loss in iteration 19 : 0.3582034369589905
Loss in iteration 20 : 0.3398588926164403
Loss in iteration 21 : 0.3226457824266544
Loss in iteration 22 : 0.3060680552092166
Loss in iteration 23 : 0.28990497950677585
Loss in iteration 24 : 0.27417607392798493
Loss in iteration 25 : 0.25927794424196826
Loss in iteration 26 : 0.24571305222682974
Loss in iteration 27 : 0.2335863631675145
Loss in iteration 28 : 0.22285208053083755
Loss in iteration 29 : 0.21384365638998995
Loss in iteration 30 : 0.20594128845136875
Loss in iteration 31 : 0.19911265460738453
Loss in iteration 32 : 0.1930006535358149
Loss in iteration 33 : 0.18741468264941677
Loss in iteration 34 : 0.1823912177179714
Loss in iteration 35 : 0.17794596892340347
Loss in iteration 36 : 0.17388444352290264
Loss in iteration 37 : 0.16998428035817356
Loss in iteration 38 : 0.16625797561517883
Loss in iteration 39 : 0.16263707136003203
Loss in iteration 40 : 0.15906867459854637
Loss in iteration 41 : 0.1556007190119526
Loss in iteration 42 : 0.15245476212497844
Loss in iteration 43 : 0.14934840391548745
Loss in iteration 44 : 0.1463808961299956
Loss in iteration 45 : 0.1436018895147997
Loss in iteration 46 : 0.1410185551801604
Loss in iteration 47 : 0.13852366699387061
Loss in iteration 48 : 0.13614442863439727
Loss in iteration 49 : 0.13385734401192897
Loss in iteration 50 : 0.13163325096095843
Loss in iteration 51 : 0.1295265806519345
Loss in iteration 52 : 0.12757833046594957
Loss in iteration 53 : 0.12569859655995574
Loss in iteration 54 : 0.12397434516106366
Loss in iteration 55 : 0.12242726166083283
Loss in iteration 56 : 0.12093406619742654
Loss in iteration 57 : 0.11942805081523572
Loss in iteration 58 : 0.11804592907921506
Loss in iteration 59 : 0.11657168360292455
Loss in iteration 60 : 0.11536125937091982
Loss in iteration 61 : 0.1139911638753059
Loss in iteration 62 : 0.11280591043400875
Loss in iteration 63 : 0.11151993957684492
Loss in iteration 64 : 0.11036571018408785
Loss in iteration 65 : 0.10921251751558575
Loss in iteration 66 : 0.10807186914406536
Loss in iteration 67 : 0.10701718514741401
Loss in iteration 68 : 0.10597391584152856
Loss in iteration 69 : 0.10493517251874167
Loss in iteration 70 : 0.10393982904133983
Loss in iteration 71 : 0.10294059334222767
Loss in iteration 72 : 0.101965278441814
Loss in iteration 73 : 0.10100035161999414
Loss in iteration 74 : 0.1000428742442108
Loss in iteration 75 : 0.09909643070073361
Loss in iteration 76 : 0.0981610091299328
Loss in iteration 77 : 0.09722158850396866
Loss in iteration 78 : 0.09630035192545927
Loss in iteration 79 : 0.09538358658622956
Loss in iteration 80 : 0.09447174440824245
Loss in iteration 81 : 0.09356561916547143
Loss in iteration 82 : 0.0926666136904392
Loss in iteration 83 : 0.09177385475704196
Loss in iteration 84 : 0.09088425598158839
Loss in iteration 85 : 0.0900053322516335
Loss in iteration 86 : 0.08912806048015141
Loss in iteration 87 : 0.08825714693927343
Loss in iteration 88 : 0.08739821568488602
Loss in iteration 89 : 0.08655235768417156
Loss in iteration 90 : 0.0857095894937743
Loss in iteration 91 : 0.08486894193974692
Loss in iteration 92 : 0.08403612795366823
Loss in iteration 93 : 0.08320785249721513
Loss in iteration 94 : 0.08238612587063222
Loss in iteration 95 : 0.08156689595331344
Loss in iteration 96 : 0.08075635111813846
Loss in iteration 97 : 0.07995748284227963
Loss in iteration 98 : 0.07916688875305997
Loss in iteration 99 : 0.07838215313445222
Loss in iteration 100 : 0.07760199467643862
Loss in iteration 101 : 0.07682534423618227
Loss in iteration 102 : 0.07607251560932618
Loss in iteration 103 : 0.07537040274456279
Loss in iteration 104 : 0.07467864240507009
Loss in iteration 105 : 0.07399383111159645
Loss in iteration 106 : 0.07330854972174387
Loss in iteration 107 : 0.07262860161366409
Loss in iteration 108 : 0.07199096024812514
Loss in iteration 109 : 0.07141780528822024
Loss in iteration 110 : 0.07084709366102993
Loss in iteration 111 : 0.07028166608064541
Loss in iteration 112 : 0.06971821161848078
Loss in iteration 113 : 0.06915831532115509
Loss in iteration 114 : 0.06860397445078024
Testing accuracy  of updater 0 on alg 1 with rate 49.0 = 0.9137777777777778, training accuracy 0.989997142040583, time elapsed: 3567 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 5.005994690354107
Loss in iteration 3 : 0.8991777020685264
Loss in iteration 4 : 0.5974554553955413
Loss in iteration 5 : 0.49677507613399885
Loss in iteration 6 : 0.45309198949431384
Loss in iteration 7 : 0.4236835733517142
Loss in iteration 8 : 0.39784341289323805
Loss in iteration 9 : 0.374245314017127
Loss in iteration 10 : 0.3530261686290189
Loss in iteration 11 : 0.3335166846358246
Loss in iteration 12 : 0.3157027446714835
Loss in iteration 13 : 0.2992386549133351
Loss in iteration 14 : 0.2838729398215653
Loss in iteration 15 : 0.2692799964103322
Loss in iteration 16 : 0.2554006823037695
Loss in iteration 17 : 0.24230186998202127
Loss in iteration 18 : 0.22992000524923056
Loss in iteration 19 : 0.21828880238396287
Loss in iteration 20 : 0.20733880992192352
Loss in iteration 21 : 0.19722807999429925
Loss in iteration 22 : 0.187617263255909
Loss in iteration 23 : 0.1783303419067859
Loss in iteration 24 : 0.16926261551675653
Loss in iteration 25 : 0.16047215221497274
Loss in iteration 26 : 0.15218363607977786
Loss in iteration 27 : 0.144431875600933
Loss in iteration 28 : 0.13749929442455003
Loss in iteration 29 : 0.13136600125775505
Loss in iteration 30 : 0.12653213181880874
Loss in iteration 31 : 0.12233007738102597
Loss in iteration 32 : 0.11880755643589888
Loss in iteration 33 : 0.1156965545384845
Loss in iteration 34 : 0.1128429425113444
Loss in iteration 35 : 0.11026173781196738
Loss in iteration 36 : 0.10789870938564566
Loss in iteration 37 : 0.10568959258474442
Loss in iteration 38 : 0.10357722267393521
Loss in iteration 39 : 0.1015143694468971
Loss in iteration 40 : 0.09947646648267844
Loss in iteration 41 : 0.09746712873872342
Loss in iteration 42 : 0.09561156228390043
Loss in iteration 43 : 0.09380946403552362
Loss in iteration 44 : 0.09205520207241746
Loss in iteration 45 : 0.09042499767861839
Loss in iteration 46 : 0.0888201598471183
Loss in iteration 47 : 0.08724431226076049
Loss in iteration 48 : 0.08570213703861505
Loss in iteration 49 : 0.08420278397722687
Loss in iteration 50 : 0.08273272982708563
Loss in iteration 51 : 0.08133723411580028
Loss in iteration 52 : 0.0800116517568773
Loss in iteration 53 : 0.07870277006051626
Loss in iteration 54 : 0.0774486906704087
Loss in iteration 55 : 0.07627808377300045
Loss in iteration 56 : 0.07515140792209196
Loss in iteration 57 : 0.07409323738858933
Loss in iteration 58 : 0.07312805239580984
Loss in iteration 59 : 0.07225903753146067
Loss in iteration 60 : 0.07140878780485217
Loss in iteration 61 : 0.07056253182492604
Loss in iteration 62 : 0.06974067421942065
Loss in iteration 63 : 0.06891128926466603
Loss in iteration 64 : 0.06814553990437719
Loss in iteration 65 : 0.06742008089241071
Loss in iteration 66 : 0.06671507408758973
Loss in iteration 67 : 0.06603399910408873
Loss in iteration 68 : 0.0653442523742131
Loss in iteration 69 : 0.06468822104663934
Loss in iteration 70 : 0.06405646072352396
Loss in iteration 71 : 0.0634300822634146
Loss in iteration 72 : 0.06283057685418408
Loss in iteration 73 : 0.062215818197339574
Loss in iteration 74 : 0.061637126204501985
Loss in iteration 75 : 0.06106226871715333
Loss in iteration 76 : 0.06049437404279615
Loss in iteration 77 : 0.05993673005267908
Loss in iteration 78 : 0.059395265340428514
Loss in iteration 79 : 0.058860193049878586
Loss in iteration 80 : 0.05832820470335627
Loss in iteration 81 : 0.057799551739848964
Loss in iteration 82 : 0.05727497340587653
Loss in iteration 83 : 0.056753268818443926
Loss in iteration 84 : 0.056234257751565535
Loss in iteration 85 : 0.05571834528591429
Loss in iteration 86 : 0.05520671147170091
Loss in iteration 87 : 0.05470280849237918
Loss in iteration 88 : 0.054200157366255504
Loss in iteration 89 : 0.05370428822147538
Loss in iteration 90 : 0.05321154132302388
Loss in iteration 91 : 0.052721332240532874
Loss in iteration 92 : 0.052233910912548684
Loss in iteration 93 : 0.051749178939542506
Loss in iteration 94 : 0.05126763874118486
Loss in iteration 95 : 0.050788718210735447
Loss in iteration 96 : 0.05031082650451592
Loss in iteration 97 : 0.04983634586148705
Loss in iteration 98 : 0.04936497939843702
Loss in iteration 99 : 0.04889670198121027
Loss in iteration 100 : 0.04843132243820461
Loss in iteration 101 : 0.04797467887907006
Loss in iteration 102 : 0.04752024764073535
Loss in iteration 103 : 0.04706487716016307
Loss in iteration 104 : 0.04661483464173183
Loss in iteration 105 : 0.04616737816754855
Loss in iteration 106 : 0.045722057535985954
Loss in iteration 107 : 0.045279033764222855
Loss in iteration 108 : 0.0448440155949744
Loss in iteration 109 : 0.04444175437003145
Loss in iteration 110 : 0.044041522115333735
Loss in iteration 111 : 0.04364198710066087
Loss in iteration 112 : 0.04324528359109932
Loss in iteration 113 : 0.04285036532729524
Loss in iteration 114 : 0.042461466148055196
Loss in iteration 115 : 0.042103198680559506
Loss in iteration 116 : 0.04177633631874282
Loss in iteration 117 : 0.04144435913377639
Loss in iteration 118 : 0.041115829061992105
Testing accuracy  of updater 0 on alg 1 with rate 28.0 = 0.9164444444444444, training accuracy 0.9905687339239783, time elapsed: 3182 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3637007789185018
Loss in iteration 3 : 0.27400981839496896
Loss in iteration 4 : 0.19768300877784298
Loss in iteration 5 : 0.16090231532963883
Loss in iteration 6 : 0.1494485856866745
Loss in iteration 7 : 0.1403478420277372
Loss in iteration 8 : 0.13292165218399882
Loss in iteration 9 : 0.12642088461534554
Loss in iteration 10 : 0.12064072123824475
Loss in iteration 11 : 0.11539745840157842
Loss in iteration 12 : 0.11050307197024327
Loss in iteration 13 : 0.105877089008262
Loss in iteration 14 : 0.10149623645662235
Loss in iteration 15 : 0.0973644909527374
Loss in iteration 16 : 0.09343632129617209
Loss in iteration 17 : 0.08970696234493379
Loss in iteration 18 : 0.08622163921506702
Loss in iteration 19 : 0.0829846827341569
Loss in iteration 20 : 0.07995793410932805
Loss in iteration 21 : 0.07706816219838382
Loss in iteration 22 : 0.07433189004779897
Loss in iteration 23 : 0.07179340562513681
Loss in iteration 24 : 0.06948104832167679
Loss in iteration 25 : 0.06735536839423746
Loss in iteration 26 : 0.06537416185364106
Loss in iteration 27 : 0.06354107641631879
Loss in iteration 28 : 0.061776411652971645
Loss in iteration 29 : 0.0600642971817078
Loss in iteration 30 : 0.05841085599661163
Loss in iteration 31 : 0.05680833349572578
Loss in iteration 32 : 0.05528424235656891
Loss in iteration 33 : 0.053797565617484396
Loss in iteration 34 : 0.05238351711728768
Loss in iteration 35 : 0.05104659735153904
Loss in iteration 36 : 0.04980655417851021
Loss in iteration 37 : 0.04866538655731328
Loss in iteration 38 : 0.04756362581590124
Loss in iteration 39 : 0.04650126877981227
Loss in iteration 40 : 0.045514235228493086
Loss in iteration 41 : 0.04456506950165106
Loss in iteration 42 : 0.043633627846507574
Loss in iteration 43 : 0.042729760442728554
Loss in iteration 44 : 0.0418868896160592
Loss in iteration 45 : 0.0410808984854053
Loss in iteration 46 : 0.040289250291293884
Loss in iteration 47 : 0.03951902420176682
Loss in iteration 48 : 0.03875961308807633
Loss in iteration 49 : 0.03801429023932484
Loss in iteration 50 : 0.037294896849763406
Loss in iteration 51 : 0.03659652283372344
Loss in iteration 52 : 0.035914852841604505
Loss in iteration 53 : 0.03527158863726057
Loss in iteration 54 : 0.03466698573161972
Loss in iteration 55 : 0.0340928340308485
Loss in iteration 56 : 0.033554076125184774
Loss in iteration 57 : 0.03304360119054078
Loss in iteration 58 : 0.03256633175651684
Loss in iteration 59 : 0.03211061667164631
Loss in iteration 60 : 0.03167424127124286
Loss in iteration 61 : 0.031249422580356567
Loss in iteration 62 : 0.030837206995169515
Loss in iteration 63 : 0.030444590512391886
Loss in iteration 64 : 0.030063003030023242
Loss in iteration 65 : 0.029686049276047736
Loss in iteration 66 : 0.029318250440650742
Loss in iteration 67 : 0.028960854887287724
Loss in iteration 68 : 0.028607156472321225
Loss in iteration 69 : 0.02825778394934157
Loss in iteration 70 : 0.027914539718200407
Loss in iteration 71 : 0.027581570404006873
Loss in iteration 72 : 0.027254732895537606
Loss in iteration 73 : 0.0269322478207395
Loss in iteration 74 : 0.026618001204415345
Loss in iteration 75 : 0.026305864023820926
Loss in iteration 76 : 0.025995814740777126
Loss in iteration 77 : 0.02568781201001987
Loss in iteration 78 : 0.025386029486901697
Loss in iteration 79 : 0.02509337894860678
Loss in iteration 80 : 0.02480302145490973
Loss in iteration 81 : 0.02451624302790514
Loss in iteration 82 : 0.024232322929457838
Loss in iteration 83 : 0.023955314650552628
Loss in iteration 84 : 0.02368052624041175
Loss in iteration 85 : 0.02341291831699556
Loss in iteration 86 : 0.023152451852423622
Loss in iteration 87 : 0.02289896916633167
Loss in iteration 88 : 0.022649249232785933
Loss in iteration 89 : 0.02240263307266811
Loss in iteration 90 : 0.022159829791079805
Loss in iteration 91 : 0.02192511477729585
Loss in iteration 92 : 0.021699262904102054
Loss in iteration 93 : 0.021478716494989055
Loss in iteration 94 : 0.021263418563163366
Loss in iteration 95 : 0.021053590703596494
Loss in iteration 96 : 0.02085167610976599
Loss in iteration 97 : 0.020659990634533502
Loss in iteration 98 : 0.020474774034883406
Loss in iteration 99 : 0.02029356383264719
Loss in iteration 100 : 0.020117286633348837
Loss in iteration 101 : 0.01994812703076041
Loss in iteration 102 : 0.019781476804839836
Loss in iteration 103 : 0.019616889667826327
Loss in iteration 104 : 0.019454579749734278
Loss in iteration 105 : 0.0192947623835791
Loss in iteration 106 : 0.019141793268000994
Loss in iteration 107 : 0.01898977459540291
Loss in iteration 108 : 0.018839996780238635
Loss in iteration 109 : 0.018692329908458503
Loss in iteration 110 : 0.018548916107455304
Loss in iteration 111 : 0.018406373561088357
Loss in iteration 112 : 0.018266836153245335
Loss in iteration 113 : 0.018131419354949378
Loss in iteration 114 : 0.018000046842765235
Loss in iteration 115 : 0.017869953341102196
Loss in iteration 116 : 0.017740716799607835
Loss in iteration 117 : 0.01761405662035303
Loss in iteration 118 : 0.017489743029498087
Loss in iteration 119 : 0.017366405135258958
Loss in iteration 120 : 0.01724643510709178
Loss in iteration 121 : 0.017129325102515615
Loss in iteration 122 : 0.017013966567488718
Loss in iteration 123 : 0.01690052633275282
Loss in iteration 124 : 0.016788217855187967
Loss in iteration 125 : 0.016677981730222204
Loss in iteration 126 : 0.016570857590410167
Loss in iteration 127 : 0.01646490467449642
Loss in iteration 128 : 0.016362001261505093
Loss in iteration 129 : 0.016259500603143306
Loss in iteration 130 : 0.016157398007094474
Loss in iteration 131 : 0.016055688871453498
Loss in iteration 132 : 0.015954754038310898
Loss in iteration 133 : 0.01585480122429458
Loss in iteration 134 : 0.015755224881725264
Loss in iteration 135 : 0.015656020788468547
Loss in iteration 136 : 0.015557184800723383
Loss in iteration 137 : 0.015458712851002433
Loss in iteration 138 : 0.015360600946178861
Loss in iteration 139 : 0.01526284516559694
Loss in iteration 140 : 0.015165768869628012
Loss in iteration 141 : 0.015069933971652592
Testing accuracy  of updater 0 on alg 1 with rate 7.0 = 0.9395555555555556, training accuracy 0.996141754787082, time elapsed: 3386 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0022527809970772
Loss in iteration 3 : 0.211992070101269
Loss in iteration 4 : 0.16985090246836868
Loss in iteration 5 : 0.15316808665030401
Loss in iteration 6 : 0.1265693624073734
Loss in iteration 7 : 0.11634641544158615
Loss in iteration 8 : 0.10982598842009907
Loss in iteration 9 : 0.1043133800346772
Loss in iteration 10 : 0.09964398059941106
Loss in iteration 11 : 0.09551598543873024
Loss in iteration 12 : 0.09189119590748504
Loss in iteration 13 : 0.08860598330714219
Loss in iteration 14 : 0.08554963314487701
Loss in iteration 15 : 0.08271019294987063
Loss in iteration 16 : 0.08000253542227267
Loss in iteration 17 : 0.07742987256428398
Loss in iteration 18 : 0.07497447467313591
Loss in iteration 19 : 0.07265523356803322
Loss in iteration 20 : 0.07045862857919269
Loss in iteration 21 : 0.06836011377595266
Loss in iteration 22 : 0.06632125287098113
Loss in iteration 23 : 0.06435668361944684
Loss in iteration 24 : 0.0625068379079649
Loss in iteration 25 : 0.0607886953050471
Loss in iteration 26 : 0.05916835754750175
Loss in iteration 27 : 0.05763616115922799
Loss in iteration 28 : 0.05619886224372272
Loss in iteration 29 : 0.05486086147455519
Loss in iteration 30 : 0.05357824257760854
Loss in iteration 31 : 0.05234998878428196
Loss in iteration 32 : 0.05116212198871285
Loss in iteration 33 : 0.05001130117314206
Loss in iteration 34 : 0.04891637050058656
Loss in iteration 35 : 0.04787628987230105
Loss in iteration 36 : 0.046880335789416486
Loss in iteration 37 : 0.04594205894328952
Loss in iteration 38 : 0.04504336629109232
Loss in iteration 39 : 0.044178281597367225
Loss in iteration 40 : 0.043350372366379786
Loss in iteration 41 : 0.04258540706230983
Loss in iteration 42 : 0.041835925608340194
Loss in iteration 43 : 0.041105809342137235
Loss in iteration 44 : 0.04040276106119937
Loss in iteration 45 : 0.03971584547991592
Loss in iteration 46 : 0.03905937038225687
Loss in iteration 47 : 0.038435881091462246
Loss in iteration 48 : 0.0378260915659773
Loss in iteration 49 : 0.037234661018939415
Loss in iteration 50 : 0.03668130780510979
Loss in iteration 51 : 0.0361494097357065
Loss in iteration 52 : 0.03562766428920971
Loss in iteration 53 : 0.035118108084994576
Loss in iteration 54 : 0.034623039823075824
Loss in iteration 55 : 0.03413465217756067
Loss in iteration 56 : 0.03365102121349231
Loss in iteration 57 : 0.03317608472357546
Loss in iteration 58 : 0.03270586808642886
Loss in iteration 59 : 0.03224588293001925
Loss in iteration 60 : 0.031796276276279546
Loss in iteration 61 : 0.03135229321561886
Loss in iteration 62 : 0.03092027913055765
Loss in iteration 63 : 0.030497331349545576
Loss in iteration 64 : 0.030091569819287473
Loss in iteration 65 : 0.029693796288323526
Loss in iteration 66 : 0.029304682560233088
Loss in iteration 67 : 0.028927315644635757
Loss in iteration 68 : 0.02855660985320007
Loss in iteration 69 : 0.028193728091466145
Loss in iteration 70 : 0.027837165872580918
Loss in iteration 71 : 0.027486657722051926
Loss in iteration 72 : 0.02714189614250529
Loss in iteration 73 : 0.02680141343634493
Loss in iteration 74 : 0.026463232315826667
Loss in iteration 75 : 0.026127776975969505
Loss in iteration 76 : 0.025794986539581913
Loss in iteration 77 : 0.025467012031616866
Loss in iteration 78 : 0.025154986933000077
Loss in iteration 79 : 0.024853729462550607
Loss in iteration 80 : 0.0245557318289034
Loss in iteration 81 : 0.024269749607095336
Loss in iteration 82 : 0.02399996830162905
Loss in iteration 83 : 0.023737197684515435
Loss in iteration 84 : 0.02348198432181589
Loss in iteration 85 : 0.023232025667962015
Loss in iteration 86 : 0.022986340423173695
Loss in iteration 87 : 0.02275300645588466
Loss in iteration 88 : 0.022521303896297806
Loss in iteration 89 : 0.022293700377787704
Loss in iteration 90 : 0.02207274381901116
Loss in iteration 91 : 0.02186367977828973
Loss in iteration 92 : 0.021658054338093133
Loss in iteration 93 : 0.021457876934206976
Loss in iteration 94 : 0.021259998068932695
Loss in iteration 95 : 0.021066277734734257
Loss in iteration 96 : 0.020875011368264397
Loss in iteration 97 : 0.02068397983308519
Loss in iteration 98 : 0.020499179170246403
Loss in iteration 99 : 0.020321217918906502
Loss in iteration 100 : 0.020147803689145025
Loss in iteration 101 : 0.019976978440491174
Loss in iteration 102 : 0.019808224300941018
Loss in iteration 103 : 0.019645241390207176
Loss in iteration 104 : 0.0194853655100029
Loss in iteration 105 : 0.019326361759394266
Loss in iteration 106 : 0.01916925463895384
Loss in iteration 107 : 0.019018931261119573
Loss in iteration 108 : 0.01887297465130841
Loss in iteration 109 : 0.018731749865755317
Loss in iteration 110 : 0.018594032158388545
Loss in iteration 111 : 0.01845889697354772
Loss in iteration 112 : 0.018325825143641516
Loss in iteration 113 : 0.018196298306874175
Loss in iteration 114 : 0.018067345871326326
Loss in iteration 115 : 0.017938960262415
Loss in iteration 116 : 0.01781113407058268
Loss in iteration 117 : 0.01768402774573373
Loss in iteration 118 : 0.01755975012115527
Loss in iteration 119 : 0.017436000217111777
Loss in iteration 120 : 0.017313392937244577
Loss in iteration 121 : 0.017192532802828283
Loss in iteration 122 : 0.017072718812976645
Loss in iteration 123 : 0.01695587378562351
Loss in iteration 124 : 0.016841133885195647
Loss in iteration 125 : 0.01672696496409002
Loss in iteration 126 : 0.016614176439420286
Loss in iteration 127 : 0.016503074559067743
Loss in iteration 128 : 0.016394072270152986
Loss in iteration 129 : 0.01628799305696456
Loss in iteration 130 : 0.016183477575027142
Loss in iteration 131 : 0.01607981784036365
Loss in iteration 132 : 0.015979419143700047
Loss in iteration 133 : 0.015885215571399627
Loss in iteration 134 : 0.015791418472713994
Loss in iteration 135 : 0.015698624236975628
Loss in iteration 136 : 0.015608651965341775
Loss in iteration 137 : 0.01552022460949665
Loss in iteration 138 : 0.015433050519143747
Loss in iteration 139 : 0.015348181291209528
Loss in iteration 140 : 0.015265105187214715
Loss in iteration 141 : 0.015183009925359784
Loss in iteration 142 : 0.015101485148064656
Loss in iteration 143 : 0.015021023418605412
Loss in iteration 144 : 0.01494170450107077
Loss in iteration 145 : 0.014862900674648398
Loss in iteration 146 : 0.014785399875744606
Loss in iteration 147 : 0.014708195159945838
Loss in iteration 148 : 0.014632376478273488
Testing accuracy  of updater 0 on alg 1 with rate 4.9 = 0.9386666666666666, training accuracy 0.9969991426121749, time elapsed: 3618 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6466881771879295
Loss in iteration 3 : 0.15245465702727953
Loss in iteration 4 : 0.19135065948921862
Loss in iteration 5 : 0.3048500966400812
Loss in iteration 6 : 0.13292186612714485
Loss in iteration 7 : 0.10139065456624909
Loss in iteration 8 : 0.09508375858357004
Loss in iteration 9 : 0.09065268158889296
Loss in iteration 10 : 0.08687151835785561
Loss in iteration 11 : 0.08354694300403873
Loss in iteration 12 : 0.08052809522405617
Loss in iteration 13 : 0.07785341391220521
Loss in iteration 14 : 0.07545650769912254
Loss in iteration 15 : 0.07328764769284166
Loss in iteration 16 : 0.07129192618800283
Loss in iteration 17 : 0.0694574790355807
Loss in iteration 18 : 0.0677715451304429
Loss in iteration 19 : 0.06620400769768738
Loss in iteration 20 : 0.06471453750331112
Loss in iteration 21 : 0.06331727233170148
Loss in iteration 22 : 0.06201555427353137
Loss in iteration 23 : 0.06075789137581562
Loss in iteration 24 : 0.059548656716183083
Loss in iteration 25 : 0.05838183413717846
Loss in iteration 26 : 0.057260082456832645
Loss in iteration 27 : 0.056170770943596966
Loss in iteration 28 : 0.055115893478954306
Loss in iteration 29 : 0.05411567595076419
Loss in iteration 30 : 0.053170515528632674
Loss in iteration 31 : 0.0522567771524204
Loss in iteration 32 : 0.051389701357277474
Loss in iteration 33 : 0.05059866862789211
Loss in iteration 34 : 0.04985563527874716
Loss in iteration 35 : 0.04915487132329496
Loss in iteration 36 : 0.04850051830416
Loss in iteration 37 : 0.04787626852020754
Loss in iteration 38 : 0.04727510955391285
Loss in iteration 39 : 0.04669697869880209
Loss in iteration 40 : 0.04614326854484034
Loss in iteration 41 : 0.04561915366973053
Loss in iteration 42 : 0.04511428595372034
Loss in iteration 43 : 0.044624932136384944
Loss in iteration 44 : 0.04415469442055581
Loss in iteration 45 : 0.04370736073748152
Loss in iteration 46 : 0.04326788563830783
Loss in iteration 47 : 0.04284001053484819
Loss in iteration 48 : 0.04242356199076985
Loss in iteration 49 : 0.042013123885833226
Loss in iteration 50 : 0.04160829723885007
Loss in iteration 51 : 0.04121142490838547
Loss in iteration 52 : 0.0408249170726834
Loss in iteration 53 : 0.04044713888189831
Loss in iteration 54 : 0.040080500863293136
Loss in iteration 55 : 0.03972328255659545
Loss in iteration 56 : 0.03937292192778466
Loss in iteration 57 : 0.039026760137568395
Loss in iteration 58 : 0.038686563091491225
Loss in iteration 59 : 0.0383511573324439
Loss in iteration 60 : 0.03802199122969978
Loss in iteration 61 : 0.03769584563148761
Loss in iteration 62 : 0.037375289988780196
Loss in iteration 63 : 0.03705877712425161
Loss in iteration 64 : 0.036748318190142404
Loss in iteration 65 : 0.0364418573804112
Loss in iteration 66 : 0.03613873371863437
Loss in iteration 67 : 0.03584193651227701
Loss in iteration 68 : 0.03554745858120265
Loss in iteration 69 : 0.035256223162687964
Loss in iteration 70 : 0.03496877640906221
Loss in iteration 71 : 0.03468658557597615
Loss in iteration 72 : 0.03441117094892731
Loss in iteration 73 : 0.034143692478986035
Loss in iteration 74 : 0.03388259381998243
Loss in iteration 75 : 0.033624754535945135
Loss in iteration 76 : 0.03336974410414545
Loss in iteration 77 : 0.033116466077806224
Loss in iteration 78 : 0.032866124573328485
Loss in iteration 79 : 0.032618220230920376
Loss in iteration 80 : 0.03237255118999601
Loss in iteration 81 : 0.03212988655708422
Loss in iteration 82 : 0.031894090132610006
Loss in iteration 83 : 0.03166716816834993
Loss in iteration 84 : 0.03144251077966553
Loss in iteration 85 : 0.03122403681063245
Loss in iteration 86 : 0.031008728894066948
Loss in iteration 87 : 0.030796639963817738
Loss in iteration 88 : 0.030586717998241638
Loss in iteration 89 : 0.030379715842948796
Loss in iteration 90 : 0.030175448575096665
Loss in iteration 91 : 0.029973811478576442
Loss in iteration 92 : 0.029774286991878986
Loss in iteration 93 : 0.029576798591723224
Loss in iteration 94 : 0.029383080573242612
Loss in iteration 95 : 0.02919098016009338
Loss in iteration 96 : 0.02900157528047263
Loss in iteration 97 : 0.028814598607399003
Loss in iteration 98 : 0.028629125239873356
Loss in iteration 99 : 0.0284447793180001
Loss in iteration 100 : 0.02826152226626497
Loss in iteration 101 : 0.028079690360366023
Loss in iteration 102 : 0.02790181932263876
Loss in iteration 103 : 0.02772583582459334
Loss in iteration 104 : 0.02755145312250073
Loss in iteration 105 : 0.027381685192456432
Loss in iteration 106 : 0.027212746804261712
Loss in iteration 107 : 0.027044607183606507
Loss in iteration 108 : 0.026877704765776497
Loss in iteration 109 : 0.02671395416397749
Loss in iteration 110 : 0.02655372715228123
Loss in iteration 111 : 0.02639432940989943
Loss in iteration 112 : 0.02623704937948259
Loss in iteration 113 : 0.026080752762686794
Loss in iteration 114 : 0.02592559951251917
Loss in iteration 115 : 0.0257719701238916
Loss in iteration 116 : 0.025620023811381253
Loss in iteration 117 : 0.025469407507904412
Loss in iteration 118 : 0.025319668853912777
Loss in iteration 119 : 0.02517106733710438
Loss in iteration 120 : 0.025023729067894122
Loss in iteration 121 : 0.024877977267896364
Loss in iteration 122 : 0.02473302691849042
Loss in iteration 123 : 0.024588998428694688
Loss in iteration 124 : 0.02444714035201883
Loss in iteration 125 : 0.024308427931645313
Loss in iteration 126 : 0.024170459096796812
Loss in iteration 127 : 0.0240339047956427
Loss in iteration 128 : 0.023900284510598203
Loss in iteration 129 : 0.02376880819734169
Loss in iteration 130 : 0.023638294986691715
Loss in iteration 131 : 0.023508603291472006
Loss in iteration 132 : 0.023380187601770207
Loss in iteration 133 : 0.02325326984434134
Loss in iteration 134 : 0.023127388693963347
Loss in iteration 135 : 0.023002938539511037
Loss in iteration 136 : 0.022879612909004816
Loss in iteration 137 : 0.02275726448477511
Loss in iteration 138 : 0.022635928380692776
Loss in iteration 139 : 0.022515144829255127
Loss in iteration 140 : 0.022395192603552126
Loss in iteration 141 : 0.02227552940196885
Loss in iteration 142 : 0.02215686010610809
Loss in iteration 143 : 0.02203948929506443
Loss in iteration 144 : 0.021923292620183894
Loss in iteration 145 : 0.021808643903241793
Loss in iteration 146 : 0.02169599987891285
Loss in iteration 147 : 0.021583730922568066
Loss in iteration 148 : 0.021472740848029585
Loss in iteration 149 : 0.02136214472847528
Loss in iteration 150 : 0.021252042973419486
Loss in iteration 151 : 0.021142168405328704
Loss in iteration 152 : 0.02103276870774427
Loss in iteration 153 : 0.020924181313447858
Loss in iteration 154 : 0.020818102609343872
Loss in iteration 155 : 0.02071236887754892
Loss in iteration 156 : 0.020606976774213408
Loss in iteration 157 : 0.020501923009161396
Loss in iteration 158 : 0.020397204344692183
Loss in iteration 159 : 0.02029281759441597
Loss in iteration 160 : 0.020188759622122642
Loss in iteration 161 : 0.020085440668332066
Loss in iteration 162 : 0.019982906263734397
Loss in iteration 163 : 0.019881315815551203
Loss in iteration 164 : 0.019780210923649757
Loss in iteration 165 : 0.01967984497601823
Loss in iteration 166 : 0.019579681329926824
Loss in iteration 167 : 0.019480194466030068
Loss in iteration 168 : 0.019381413516510886
Loss in iteration 169 : 0.01928298669071087
Loss in iteration 170 : 0.01918485624660348
Loss in iteration 171 : 0.01908748652318457
Loss in iteration 172 : 0.018990701585923028
Loss in iteration 173 : 0.01889570760879649
Loss in iteration 174 : 0.01880129112899913
Loss in iteration 175 : 0.018709992809542372
Loss in iteration 176 : 0.01861865999797248
Loss in iteration 177 : 0.01852819808650949
Loss in iteration 178 : 0.01843821456145873
Loss in iteration 179 : 0.018348701166471654
Loss in iteration 180 : 0.01826089678186588
Loss in iteration 181 : 0.018177336793592718
Loss in iteration 182 : 0.018093704016537433
Loss in iteration 183 : 0.018010696540201137
Loss in iteration 184 : 0.017928203003391464
Loss in iteration 185 : 0.017845933939206877
Loss in iteration 186 : 0.01776407744303772
Loss in iteration 187 : 0.017682360881216098
Loss in iteration 188 : 0.01760208483694709
Loss in iteration 189 : 0.017524110456622166
Loss in iteration 190 : 0.01744651356194969
Loss in iteration 191 : 0.01736915299072795
Loss in iteration 192 : 0.017292416963737695
Loss in iteration 193 : 0.017216026993322607
Loss in iteration 194 : 0.01713993400855386
Loss in iteration 195 : 0.017065128437840298
Loss in iteration 196 : 0.01699280475017073
Loss in iteration 197 : 0.016920521962252886
Loss in iteration 198 : 0.016848537722809618
Loss in iteration 199 : 0.01677665372297098
Loss in iteration 200 : 0.01670500399090084
Testing accuracy  of updater 0 on alg 1 with rate 2.8 = 0.9511111111111111, training accuracy 0.9975707344955702, time elapsed: 4883 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38217011907456366
Loss in iteration 3 : 0.7217402374740635
Loss in iteration 4 : 0.30307721274930144
Loss in iteration 5 : 0.19456854642051863
Loss in iteration 6 : 0.18162534407287356
Loss in iteration 7 : 0.17250137393682777
Loss in iteration 8 : 0.1651624897727307
Loss in iteration 9 : 0.1591252299931209
Loss in iteration 10 : 0.1535611572898927
Loss in iteration 11 : 0.14862550894935864
Loss in iteration 12 : 0.1440222779510203
Loss in iteration 13 : 0.13978499763523103
Loss in iteration 14 : 0.13576855597054038
Loss in iteration 15 : 0.13196579859036284
Loss in iteration 16 : 0.12840520041596573
Loss in iteration 17 : 0.12506674820934693
Loss in iteration 18 : 0.12201681773823837
Loss in iteration 19 : 0.11925113538061759
Loss in iteration 20 : 0.11671484455864621
Loss in iteration 21 : 0.11437367147566024
Loss in iteration 22 : 0.11222271348869448
Loss in iteration 23 : 0.11020210562133086
Loss in iteration 24 : 0.10831055479989624
Loss in iteration 25 : 0.10653150278265057
Loss in iteration 26 : 0.10485698865858878
Loss in iteration 27 : 0.1033018059612149
Loss in iteration 28 : 0.10183845591595989
Loss in iteration 29 : 0.10046258312277474
Loss in iteration 30 : 0.09916962713134297
Loss in iteration 31 : 0.09795431772894893
Loss in iteration 32 : 0.09679117934320909
Loss in iteration 33 : 0.09568847900653464
Loss in iteration 34 : 0.09465584157676174
Loss in iteration 35 : 0.0936717583923679
Loss in iteration 36 : 0.09272564781406849
Loss in iteration 37 : 0.09181689655566698
Loss in iteration 38 : 0.09094531705757852
Loss in iteration 39 : 0.09011256548988347
Loss in iteration 40 : 0.08933317152849343
Loss in iteration 41 : 0.08859051715200951
Loss in iteration 42 : 0.08787942531620867
Loss in iteration 43 : 0.08719486940533346
Loss in iteration 44 : 0.08653900982361154
Loss in iteration 45 : 0.08591296007729998
Loss in iteration 46 : 0.08531209924696194
Loss in iteration 47 : 0.08473341886154212
Loss in iteration 48 : 0.08417413872881632
Loss in iteration 49 : 0.08362876756509072
Loss in iteration 50 : 0.08309675686489455
Loss in iteration 51 : 0.08258281745896764
Loss in iteration 52 : 0.08208277195811375
Loss in iteration 53 : 0.08160162449497577
Loss in iteration 54 : 0.08113460086107924
Loss in iteration 55 : 0.08067928215197778
Loss in iteration 56 : 0.08023782343057473
Loss in iteration 57 : 0.07980584766904719
Loss in iteration 58 : 0.07938492049635415
Loss in iteration 59 : 0.0789750067108151
Loss in iteration 60 : 0.07857161752829947
Loss in iteration 61 : 0.0781761948163121
Loss in iteration 62 : 0.0777873234686505
Loss in iteration 63 : 0.07740634480991196
Loss in iteration 64 : 0.07703474211663966
Loss in iteration 65 : 0.07667290302080769
Loss in iteration 66 : 0.07632113893914962
Loss in iteration 67 : 0.07597418842711574
Loss in iteration 68 : 0.07563544885936452
Loss in iteration 69 : 0.07530195354670552
Loss in iteration 70 : 0.07497514492876789
Loss in iteration 71 : 0.07465539673653189
Loss in iteration 72 : 0.07434080511958542
Loss in iteration 73 : 0.07402990803754413
Loss in iteration 74 : 0.07372301466461278
Loss in iteration 75 : 0.07341929862294631
Loss in iteration 76 : 0.07312095662934827
Loss in iteration 77 : 0.07282564797096695
Loss in iteration 78 : 0.07253363921932413
Loss in iteration 79 : 0.07224508486022588
Loss in iteration 80 : 0.07196257749546245
Loss in iteration 81 : 0.07168540447998885
Loss in iteration 82 : 0.0714110190943453
Loss in iteration 83 : 0.07113979877793253
Loss in iteration 84 : 0.07087205923818267
Loss in iteration 85 : 0.0706086413201856
Loss in iteration 86 : 0.0703472236955169
Loss in iteration 87 : 0.07008837727915329
Loss in iteration 88 : 0.06983146826674463
Loss in iteration 89 : 0.06957728663847502
Loss in iteration 90 : 0.06932734334449181
Loss in iteration 91 : 0.06907993613009335
Loss in iteration 92 : 0.06883620792191142
Loss in iteration 93 : 0.06859600599400793
Loss in iteration 94 : 0.06835837481003058
Loss in iteration 95 : 0.06812319211949146
Loss in iteration 96 : 0.06789323433238394
Loss in iteration 97 : 0.0676658151838558
Loss in iteration 98 : 0.06744257111623116
Loss in iteration 99 : 0.06722078809724447
Loss in iteration 100 : 0.06700080274078293
Loss in iteration 101 : 0.06678284106619026
Loss in iteration 102 : 0.0665661644421829
Loss in iteration 103 : 0.06635134277439594
Loss in iteration 104 : 0.0661381035117824
Loss in iteration 105 : 0.06592618761531087
Loss in iteration 106 : 0.06571610674795317
Loss in iteration 107 : 0.06550834795409678
Loss in iteration 108 : 0.06530284535934791
Loss in iteration 109 : 0.06509870774559727
Loss in iteration 110 : 0.06489688731215185
Loss in iteration 111 : 0.06469724073013988
Loss in iteration 112 : 0.06449918133659927
Loss in iteration 113 : 0.06430368334254441
Loss in iteration 114 : 0.0641099551942354
Loss in iteration 115 : 0.06391760761161866
Loss in iteration 116 : 0.06372712406191294
Loss in iteration 117 : 0.06353866652161966
Loss in iteration 118 : 0.06335120954796282
Loss in iteration 119 : 0.06316481745036226
Loss in iteration 120 : 0.0629800383334825
Loss in iteration 121 : 0.06279735205291692
Loss in iteration 122 : 0.06261569359188707
Loss in iteration 123 : 0.06243535432752684
Loss in iteration 124 : 0.06225640537120796
Loss in iteration 125 : 0.06207848677158413
Loss in iteration 126 : 0.061901527952782594
Loss in iteration 127 : 0.06172679890833738
Loss in iteration 128 : 0.06155390000361774
Loss in iteration 129 : 0.061381920318219006
Loss in iteration 130 : 0.061210930839154284
Loss in iteration 131 : 0.06104086275807448
Loss in iteration 132 : 0.06087151454793396
Loss in iteration 133 : 0.06070291193942827
Loss in iteration 134 : 0.060535635135430636
Loss in iteration 135 : 0.06037038087847782
Loss in iteration 136 : 0.06020601184232239
Loss in iteration 137 : 0.06004260751860308
Loss in iteration 138 : 0.059881576707482606
Loss in iteration 139 : 0.05972319321115556
Loss in iteration 140 : 0.05956633737868602
Loss in iteration 141 : 0.05941035110257743
Loss in iteration 142 : 0.05925565927333123
Loss in iteration 143 : 0.059102383938302736
Loss in iteration 144 : 0.05894964547124615
Loss in iteration 145 : 0.05879743827003243
Loss in iteration 146 : 0.058645756829288596
Loss in iteration 147 : 0.05849520838460942
Loss in iteration 148 : 0.05834666035917714
Loss in iteration 149 : 0.058198697757838154
Loss in iteration 150 : 0.05805139479134402
Loss in iteration 151 : 0.05790518021625745
Loss in iteration 152 : 0.05776020731313973
Loss in iteration 153 : 0.05761595999627637
Loss in iteration 154 : 0.057472355759922954
Loss in iteration 155 : 0.057329356833413676
Loss in iteration 156 : 0.05718729173735048
Loss in iteration 157 : 0.057046871861285486
Loss in iteration 158 : 0.056907445090074435
Loss in iteration 159 : 0.056768499936178156
Loss in iteration 160 : 0.05663126807821287
Loss in iteration 161 : 0.05649658426088155
Loss in iteration 162 : 0.0563625737592785
Loss in iteration 163 : 0.056229312988273446
Loss in iteration 164 : 0.056096683894677
Loss in iteration 165 : 0.055964839185065986
Loss in iteration 166 : 0.05583350538746543
Loss in iteration 167 : 0.05570273801326218
Loss in iteration 168 : 0.05557246913975042
Loss in iteration 169 : 0.055443735250504916
Loss in iteration 170 : 0.05531551647838693
Loss in iteration 171 : 0.05518781885298402
Loss in iteration 172 : 0.05506156711682274
Loss in iteration 173 : 0.05493750231376824
Loss in iteration 174 : 0.054815895711726736
Loss in iteration 175 : 0.0546949103767468
Loss in iteration 176 : 0.05457468637530914
Loss in iteration 177 : 0.05445517628522808
Loss in iteration 178 : 0.054336177492181156
Loss in iteration 179 : 0.05421752363660665
Loss in iteration 180 : 0.05409921549630358
Loss in iteration 181 : 0.053981358259012964
Loss in iteration 182 : 0.053864169121229134
Loss in iteration 183 : 0.05374810310893269
Loss in iteration 184 : 0.05363281807349908
Loss in iteration 185 : 0.05351865442981682
Loss in iteration 186 : 0.05340511384184483
Loss in iteration 187 : 0.053293855392145574
Loss in iteration 188 : 0.05318347078585646
Loss in iteration 189 : 0.05307345385553617
Loss in iteration 190 : 0.05296372836130487
Loss in iteration 191 : 0.05285429199931324
Loss in iteration 192 : 0.05274516043697573
Loss in iteration 193 : 0.05263673438337869
Loss in iteration 194 : 0.052529097840670176
Loss in iteration 195 : 0.05242187982471508
Loss in iteration 196 : 0.05231565126296097
Loss in iteration 197 : 0.052210656322594654
Loss in iteration 198 : 0.05210693945941563
Loss in iteration 199 : 0.05200414792498367
Loss in iteration 200 : 0.0519019427643097
Testing accuracy  of updater 0 on alg 1 with rate 0.7000000000000002 = 0.9937777777777778, training accuracy 0.9847099171191769, time elapsed: 4775 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.882639477036716
Loss in iteration 3 : 1.781828790380347
Loss in iteration 4 : 0.9718612533124027
Loss in iteration 5 : 0.6686801876452072
Loss in iteration 6 : 0.9077449995579171
Loss in iteration 7 : 1.1583736372093494
Loss in iteration 8 : 1.3920900775806941
Loss in iteration 9 : 1.468843273986117
Loss in iteration 10 : 1.402320953294068
Loss in iteration 11 : 1.3083704276790518
Loss in iteration 12 : 1.220618947294148
Loss in iteration 13 : 1.1417586894945146
Loss in iteration 14 : 1.0772496482038354
Loss in iteration 15 : 1.029739173603137
Loss in iteration 16 : 0.9871599880086336
Loss in iteration 17 : 0.9517639505770288
Loss in iteration 18 : 0.9270914273852344
Loss in iteration 19 : 0.9173861960640445
Loss in iteration 20 : 0.9192514069047426
Loss in iteration 21 : 0.9228212846292944
Loss in iteration 22 : 0.9226300709797802
Loss in iteration 23 : 0.9124992219772758
Loss in iteration 24 : 0.8923781831247355
Loss in iteration 25 : 0.8631168144009077
Loss in iteration 26 : 0.8271789223744099
Loss in iteration 27 : 0.7869472612960063
Loss in iteration 28 : 0.7454053251303976
Loss in iteration 29 : 0.705420439779821
Loss in iteration 30 : 0.6670169215709936
Loss in iteration 31 : 0.6317833470643147
Loss in iteration 32 : 0.5988898475979297
Loss in iteration 33 : 0.5684026592948821
Loss in iteration 34 : 0.5392478093112179
Loss in iteration 35 : 0.5115271185770164
Loss in iteration 36 : 0.4870804544270018
Loss in iteration 37 : 0.46572466938329243
Loss in iteration 38 : 0.446831865483468
Loss in iteration 39 : 0.4295174683357063
Loss in iteration 40 : 0.413253516207122
Loss in iteration 41 : 0.3980206790740744
Loss in iteration 42 : 0.38424065999994156
Loss in iteration 43 : 0.3720060773074697
Loss in iteration 44 : 0.3611533821175877
Loss in iteration 45 : 0.35116613314525674
Loss in iteration 46 : 0.34172814793792344
Loss in iteration 47 : 0.3329155413707407
Loss in iteration 48 : 0.32452472083472783
Loss in iteration 49 : 0.3161909175104101
Loss in iteration 50 : 0.3078695366388492
Loss in iteration 51 : 0.29971425320559236
Loss in iteration 52 : 0.2917936754691397
Loss in iteration 53 : 0.284868373326602
Loss in iteration 54 : 0.2789111048089907
Loss in iteration 55 : 0.27377264632785897
Loss in iteration 56 : 0.2692665209414167
Loss in iteration 57 : 0.2651182813392491
Loss in iteration 58 : 0.26125884123894944
Loss in iteration 59 : 0.257703769254562
Loss in iteration 60 : 0.25434919737658046
Loss in iteration 61 : 0.25115945001654416
Loss in iteration 62 : 0.2480580081003777
Loss in iteration 63 : 0.2449656544166444
Loss in iteration 64 : 0.24196825689825302
Loss in iteration 65 : 0.23904487033280344
Loss in iteration 66 : 0.2361503132200144
Loss in iteration 67 : 0.23329896861442306
Loss in iteration 68 : 0.23046820309312382
Loss in iteration 69 : 0.22769590861511002
Loss in iteration 70 : 0.22500861648646817
Loss in iteration 71 : 0.2224483916862021
Loss in iteration 72 : 0.2199408320863183
Loss in iteration 73 : 0.21748164988140292
Loss in iteration 74 : 0.21511237657354368
Loss in iteration 75 : 0.21278434215230657
Loss in iteration 76 : 0.2104915562407193
Loss in iteration 77 : 0.20824007436791708
Loss in iteration 78 : 0.20601298559524012
Loss in iteration 79 : 0.2038199914323962
Loss in iteration 80 : 0.20166559884617966
Loss in iteration 81 : 0.19954707412300962
Loss in iteration 82 : 0.1974715588234691
Loss in iteration 83 : 0.19544260787289044
Loss in iteration 84 : 0.19343565124345458
Loss in iteration 85 : 0.1914682655166173
Loss in iteration 86 : 0.18952428039516742
Loss in iteration 87 : 0.1875992442500654
Loss in iteration 88 : 0.1856984695665577
Loss in iteration 89 : 0.1838188052121258
Loss in iteration 90 : 0.18195721587190442
Loss in iteration 91 : 0.18011284052215012
Loss in iteration 92 : 0.1782848885970353
Loss in iteration 93 : 0.17647811375220115
Loss in iteration 94 : 0.1747174348173448
Loss in iteration 95 : 0.17298698349836256
Loss in iteration 96 : 0.17127244484917145
Loss in iteration 97 : 0.16956542553136536
Loss in iteration 98 : 0.1678660208243689
Loss in iteration 99 : 0.1661743034788171
Loss in iteration 100 : 0.16449040390003722
Loss in iteration 101 : 0.16282330961766545
Loss in iteration 102 : 0.16116572060376932
Loss in iteration 103 : 0.15951745883818996
Loss in iteration 104 : 0.15788265545735453
Loss in iteration 105 : 0.15627878195923176
Loss in iteration 106 : 0.15468713604909462
Loss in iteration 107 : 0.15312376868599548
Loss in iteration 108 : 0.15159153943279108
Loss in iteration 109 : 0.15006951480828576
Loss in iteration 110 : 0.148608739956988
Loss in iteration 111 : 0.14716395609592647
Loss in iteration 112 : 0.14573479524643437
Loss in iteration 113 : 0.14432103060319448
Loss in iteration 114 : 0.14292286916550856
Loss in iteration 115 : 0.1415819354562145
Loss in iteration 116 : 0.14027233997311717
Loss in iteration 117 : 0.13898268311057008
Loss in iteration 118 : 0.13774382927986098
Loss in iteration 119 : 0.13653010743675603
Loss in iteration 120 : 0.13533094551285604
Loss in iteration 121 : 0.1341684945722696
Loss in iteration 122 : 0.1330249113310211
Testing accuracy  of updater 1 on alg 1 with rate 10.0 = 0.9208888888888889, training accuracy 0.9908545298656759, time elapsed: 3132 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3637007789185018
Loss in iteration 3 : 1.259355274637683
Loss in iteration 4 : 0.6815528021207102
Loss in iteration 5 : 0.48898995589779076
Loss in iteration 6 : 0.6650806959224712
Loss in iteration 7 : 0.8482461617444447
Loss in iteration 8 : 1.0081942512963356
Loss in iteration 9 : 1.0400568783045772
Loss in iteration 10 : 0.9832958438310091
Loss in iteration 11 : 0.9187815937947864
Loss in iteration 12 : 0.8591153766402679
Loss in iteration 13 : 0.8056419053134064
Loss in iteration 14 : 0.7639935347169113
Loss in iteration 15 : 0.7325858933514922
Loss in iteration 16 : 0.7050578243763139
Loss in iteration 17 : 0.6830499611891853
Loss in iteration 18 : 0.670671711054538
Loss in iteration 19 : 0.6686561269428838
Loss in iteration 20 : 0.6725466950110494
Loss in iteration 21 : 0.675744128114009
Loss in iteration 22 : 0.6732347423181274
Loss in iteration 23 : 0.6627777376314744
Loss in iteration 24 : 0.6450518748727344
Loss in iteration 25 : 0.6212838779893927
Loss in iteration 26 : 0.593223826917562
Loss in iteration 27 : 0.5629587874657763
Loss in iteration 28 : 0.5329772913025164
Loss in iteration 29 : 0.504606753780635
Loss in iteration 30 : 0.4782254252973016
Loss in iteration 31 : 0.4541402514447373
Loss in iteration 32 : 0.43157800440219996
Loss in iteration 33 : 0.4105951405364281
Loss in iteration 34 : 0.39039878387639726
Loss in iteration 35 : 0.37137939359667466
Loss in iteration 36 : 0.35406138831226164
Loss in iteration 37 : 0.33847440983287147
Loss in iteration 38 : 0.3244563041349652
Loss in iteration 39 : 0.3116254456526038
Loss in iteration 40 : 0.2995570003871575
Loss in iteration 41 : 0.2883423089641735
Loss in iteration 42 : 0.277809744500726
Loss in iteration 43 : 0.26846748541506027
Loss in iteration 44 : 0.26011125843826566
Loss in iteration 45 : 0.2523005474110565
Loss in iteration 46 : 0.24515975701552487
Loss in iteration 47 : 0.23836484222838047
Loss in iteration 48 : 0.23208510487073813
Loss in iteration 49 : 0.2261917918292736
Loss in iteration 50 : 0.22041463109537945
Loss in iteration 51 : 0.2147883089464877
Loss in iteration 52 : 0.20940029501633206
Loss in iteration 53 : 0.20468658382453703
Loss in iteration 54 : 0.20058371534499653
Loss in iteration 55 : 0.19689907225843395
Loss in iteration 56 : 0.19364835288505527
Loss in iteration 57 : 0.19066678349810348
Loss in iteration 58 : 0.18783677475423105
Loss in iteration 59 : 0.18522701106873704
Loss in iteration 60 : 0.18275540006077132
Loss in iteration 61 : 0.18039250620634004
Loss in iteration 62 : 0.17810599983762315
Loss in iteration 63 : 0.17588177398690108
Loss in iteration 64 : 0.17372644935943246
Loss in iteration 65 : 0.17162655992240403
Loss in iteration 66 : 0.16956738684558909
Loss in iteration 67 : 0.16752989852974773
Loss in iteration 68 : 0.1655133065372904
Loss in iteration 69 : 0.16356014569730873
Loss in iteration 70 : 0.1616688968638573
Loss in iteration 71 : 0.15985013581522076
Loss in iteration 72 : 0.1580621341206603
Loss in iteration 73 : 0.15631469754272392
Loss in iteration 74 : 0.15463302287130265
Loss in iteration 75 : 0.1529846954784736
Loss in iteration 76 : 0.15135658808755767
Loss in iteration 77 : 0.1497559636045152
Loss in iteration 78 : 0.14817929055836324
Loss in iteration 79 : 0.14661879875301845
Loss in iteration 80 : 0.1450737683976974
Loss in iteration 81 : 0.14354353478386428
Loss in iteration 82 : 0.14204176892629425
Loss in iteration 83 : 0.14056781898610932
Loss in iteration 84 : 0.13911319851659182
Loss in iteration 85 : 0.13768768388801242
Loss in iteration 86 : 0.1362775268021522
Loss in iteration 87 : 0.13489454605993437
Loss in iteration 88 : 0.13352775684315385
Loss in iteration 89 : 0.13218831105724616
Loss in iteration 90 : 0.1308680386768409
Loss in iteration 91 : 0.1295621237911006
Loss in iteration 92 : 0.12828416613631177
Loss in iteration 93 : 0.12704093280461537
Loss in iteration 94 : 0.12581235252018733
Loss in iteration 95 : 0.12459153156477626
Loss in iteration 96 : 0.12337830139071959
Loss in iteration 97 : 0.12217250078983902
Loss in iteration 98 : 0.12097397540536801
Loss in iteration 99 : 0.11978257728388426
Loss in iteration 100 : 0.11859816446364481
Loss in iteration 101 : 0.117420600596065
Loss in iteration 102 : 0.11624975459738345
Loss in iteration 103 : 0.11508550032783496
Loss in iteration 104 : 0.11393275291590647
Loss in iteration 105 : 0.11278893004622663
Loss in iteration 106 : 0.11165995493708017
Loss in iteration 107 : 0.1105499203391178
Loss in iteration 108 : 0.10945364451764109
Loss in iteration 109 : 0.1083701384731571
Loss in iteration 110 : 0.10729332383274426
Loss in iteration 111 : 0.10623051774263943
Loss in iteration 112 : 0.10521935619816901
Loss in iteration 113 : 0.1042165350165132
Loss in iteration 114 : 0.10322163158599706
Loss in iteration 115 : 0.10223807912455438
Loss in iteration 116 : 0.10128270406739188
Loss in iteration 117 : 0.10037425840760093
Loss in iteration 118 : 0.09947487154968508
Loss in iteration 119 : 0.09858398490064815
Loss in iteration 120 : 0.09770333583340889
Loss in iteration 121 : 0.09686513600536058
Loss in iteration 122 : 0.09603399801887379
Loss in iteration 123 : 0.09522157313402191
Testing accuracy  of updater 1 on alg 1 with rate 7.0 = 0.9208888888888889, training accuracy 0.9908545298656759, time elapsed: 3344 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8483775750733009
Loss in iteration 3 : 0.7420488605807645
Loss in iteration 4 : 0.39898500290170474
Loss in iteration 5 : 0.30812111870949005
Loss in iteration 6 : 0.418031380838099
Loss in iteration 7 : 0.5372514649237599
Loss in iteration 8 : 0.6129515243130553
Loss in iteration 9 : 0.6001748883401886
Loss in iteration 10 : 0.5605644524099584
Loss in iteration 11 : 0.5264598447403277
Loss in iteration 12 : 0.4938778539721625
Loss in iteration 13 : 0.46691245409138427
Loss in iteration 14 : 0.4479506322693344
Loss in iteration 15 : 0.43308921404044914
Loss in iteration 16 : 0.4212452611610639
Loss in iteration 17 : 0.4144137610462657
Loss in iteration 18 : 0.4137794134914547
Loss in iteration 19 : 0.416960110472244
Loss in iteration 20 : 0.41948197684809263
Loss in iteration 21 : 0.4178474781837361
Loss in iteration 22 : 0.4113444395900025
Loss in iteration 23 : 0.4001931372267563
Loss in iteration 24 : 0.3853880079995107
Loss in iteration 25 : 0.3681974246609166
Loss in iteration 26 : 0.3498224161791085
Loss in iteration 27 : 0.33196703206839556
Loss in iteration 28 : 0.3155891015809072
Loss in iteration 29 : 0.30056310290508004
Loss in iteration 30 : 0.2868058712518761
Loss in iteration 31 : 0.27430152847697464
Loss in iteration 32 : 0.2627861779728331
Loss in iteration 33 : 0.2515306329686938
Loss in iteration 34 : 0.24061134075644416
Loss in iteration 35 : 0.23008042537885504
Loss in iteration 36 : 0.22004440497208916
Loss in iteration 37 : 0.21063198363878657
Loss in iteration 38 : 0.20182118878067815
Loss in iteration 39 : 0.1936078250007902
Loss in iteration 40 : 0.1859329791493462
Loss in iteration 41 : 0.17860135492511994
Loss in iteration 42 : 0.17178977177038218
Loss in iteration 43 : 0.16555509982350494
Loss in iteration 44 : 0.15981346521065876
Loss in iteration 45 : 0.1545364645915794
Loss in iteration 46 : 0.14954548486900787
Loss in iteration 47 : 0.14507783230452956
Loss in iteration 48 : 0.14113754278325338
Loss in iteration 49 : 0.13743985298696373
Loss in iteration 50 : 0.13395564196510992
Loss in iteration 51 : 0.1306888595725687
Loss in iteration 52 : 0.1276986283566648
Loss in iteration 53 : 0.12499964869700438
Loss in iteration 54 : 0.12265929982542849
Loss in iteration 55 : 0.12051067947981282
Loss in iteration 56 : 0.1185325232663912
Loss in iteration 57 : 0.11667022613975422
Loss in iteration 58 : 0.11490800200814787
Loss in iteration 59 : 0.11322747533641157
Loss in iteration 60 : 0.11165683450826457
Loss in iteration 61 : 0.11014412286723384
Loss in iteration 62 : 0.10869021828791924
Loss in iteration 63 : 0.10727161273567386
Loss in iteration 64 : 0.10588539589365527
Loss in iteration 65 : 0.10457797920645782
Loss in iteration 66 : 0.10330119553741737
Loss in iteration 67 : 0.10206289484664935
Loss in iteration 68 : 0.1008545364422046
Loss in iteration 69 : 0.09969954139230204
Loss in iteration 70 : 0.0985768239892024
Loss in iteration 71 : 0.09748230010006396
Loss in iteration 72 : 0.0964196427679336
Loss in iteration 73 : 0.09537402321509204
Loss in iteration 74 : 0.09435391012219589
Loss in iteration 75 : 0.09335673795794855
Loss in iteration 76 : 0.09239351868177027
Loss in iteration 77 : 0.09144203564848337
Loss in iteration 78 : 0.09050430707670866
Loss in iteration 79 : 0.08957848364305263
Loss in iteration 80 : 0.08866862779330509
Loss in iteration 81 : 0.08776923441817437
Loss in iteration 82 : 0.08687864558906783
Loss in iteration 83 : 0.08599647709329498
Loss in iteration 84 : 0.08512450918112136
Loss in iteration 85 : 0.08426300303948825
Loss in iteration 86 : 0.08340937501188438
Loss in iteration 87 : 0.08256518895460005
Loss in iteration 88 : 0.08173067300677099
Loss in iteration 89 : 0.08090349166809441
Loss in iteration 90 : 0.08008738339832998
Loss in iteration 91 : 0.07928562658695712
Loss in iteration 92 : 0.07849135420707587
Loss in iteration 93 : 0.07771480180930315
Loss in iteration 94 : 0.07695531314853908
Loss in iteration 95 : 0.07620324783125228
Loss in iteration 96 : 0.07546542698606722
Loss in iteration 97 : 0.07474626652618817
Loss in iteration 98 : 0.07404682017201232
Loss in iteration 99 : 0.07335681358690302
Loss in iteration 100 : 0.07267070965145041
Loss in iteration 101 : 0.07198844440855243
Loss in iteration 102 : 0.07130995543858536
Loss in iteration 103 : 0.07063565195576983
Loss in iteration 104 : 0.06997420723829725
Loss in iteration 105 : 0.06931945294502735
Loss in iteration 106 : 0.06866949020745446
Loss in iteration 107 : 0.06802483744724365
Loss in iteration 108 : 0.06738708365431029
Loss in iteration 109 : 0.0667536631141595
Loss in iteration 110 : 0.06612441580009402
Loss in iteration 111 : 0.06549919395276574
Loss in iteration 112 : 0.06487786093776039
Loss in iteration 113 : 0.064261317422027
Loss in iteration 114 : 0.06367152676879549
Loss in iteration 115 : 0.06308710576752323
Loss in iteration 116 : 0.06250825637615595
Loss in iteration 117 : 0.061933697672201624
Loss in iteration 118 : 0.061363266393024855
Loss in iteration 119 : 0.0608056738664733
Loss in iteration 120 : 0.060275925785960346
Loss in iteration 121 : 0.0597550585384018
Loss in iteration 122 : 0.05924247942050805
Loss in iteration 123 : 0.05874296767364028
Loss in iteration 124 : 0.058251037583464095
Loss in iteration 125 : 0.05777617314192785
Testing accuracy  of updater 1 on alg 1 with rate 4.0 = 0.9217777777777778, training accuracy 0.9908545298656759, time elapsed: 3251 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38032923872178076
Loss in iteration 3 : 0.22940722792410484
Loss in iteration 4 : 0.1389204072998803
Loss in iteration 5 : 0.20083503796829874
Loss in iteration 6 : 0.18972637144748516
Loss in iteration 7 : 0.1617062063067907
Loss in iteration 8 : 0.15693788344741427
Loss in iteration 9 : 0.15760772908827403
Loss in iteration 10 : 0.16143311543775
Loss in iteration 11 : 0.16669330543554214
Loss in iteration 12 : 0.17021721967173367
Loss in iteration 13 : 0.17104126067387398
Loss in iteration 14 : 0.16941004337459115
Loss in iteration 15 : 0.16587681800696502
Loss in iteration 16 : 0.16156748299714968
Loss in iteration 17 : 0.15722227470748115
Loss in iteration 18 : 0.1533195444455313
Loss in iteration 19 : 0.15005081832410858
Loss in iteration 20 : 0.14711370500543836
Loss in iteration 21 : 0.14447621072062342
Loss in iteration 22 : 0.14185538843040957
Loss in iteration 23 : 0.13908539049222954
Loss in iteration 24 : 0.1361729035029079
Loss in iteration 25 : 0.1330514757483918
Loss in iteration 26 : 0.12974928990584556
Loss in iteration 27 : 0.12630569310714237
Loss in iteration 28 : 0.12276018291501108
Loss in iteration 29 : 0.11912996276576654
Loss in iteration 30 : 0.11546734502261506
Loss in iteration 31 : 0.11194166512751814
Loss in iteration 32 : 0.10857953316452092
Loss in iteration 33 : 0.10534574224276766
Loss in iteration 34 : 0.10235003800916663
Loss in iteration 35 : 0.09948863851537164
Loss in iteration 36 : 0.09671181468059688
Loss in iteration 37 : 0.09404960287425722
Loss in iteration 38 : 0.09149032300829668
Loss in iteration 39 : 0.08902200551803215
Loss in iteration 40 : 0.08662437555571345
Loss in iteration 41 : 0.08424024078159131
Loss in iteration 42 : 0.0819073391265934
Loss in iteration 43 : 0.07966627967140305
Loss in iteration 44 : 0.07746350292015941
Loss in iteration 45 : 0.07526581407115385
Loss in iteration 46 : 0.07307871592251772
Loss in iteration 47 : 0.07092581921614721
Loss in iteration 48 : 0.06892163766777255
Loss in iteration 49 : 0.06702713941783674
Loss in iteration 50 : 0.06531244800175197
Loss in iteration 51 : 0.06380005602472132
Loss in iteration 52 : 0.062386056890573194
Loss in iteration 53 : 0.06104349419581401
Loss in iteration 54 : 0.05980671814647078
Loss in iteration 55 : 0.05864004498238973
Loss in iteration 56 : 0.05749163198801603
Loss in iteration 57 : 0.056365130404143565
Loss in iteration 58 : 0.0552722034594122
Loss in iteration 59 : 0.05424343242786517
Loss in iteration 60 : 0.05325908018588308
Loss in iteration 61 : 0.05232615196218933
Loss in iteration 62 : 0.05143346853593304
Loss in iteration 63 : 0.05059550827697758
Loss in iteration 64 : 0.049795252238300315
Loss in iteration 65 : 0.04904684803632916
Loss in iteration 66 : 0.048319373356182554
Loss in iteration 67 : 0.047610549745241766
Loss in iteration 68 : 0.04693338500426375
Loss in iteration 69 : 0.046275235339819115
Loss in iteration 70 : 0.045635147568943844
Loss in iteration 71 : 0.04502941156896296
Loss in iteration 72 : 0.04444180358915085
Loss in iteration 73 : 0.04386918616056011
Loss in iteration 74 : 0.04332664109680855
Loss in iteration 75 : 0.04279685034771613
Loss in iteration 76 : 0.04228141786278888
Loss in iteration 77 : 0.04178029365041281
Loss in iteration 78 : 0.041292947211254656
Loss in iteration 79 : 0.04081457218080273
Loss in iteration 80 : 0.040347138569680946
Loss in iteration 81 : 0.03988192745496874
Loss in iteration 82 : 0.03942198034424624
Loss in iteration 83 : 0.03896534300007517
Loss in iteration 84 : 0.03851375284644438
Loss in iteration 85 : 0.03806566384642375
Loss in iteration 86 : 0.037626094891182196
Loss in iteration 87 : 0.037190693170543784
Loss in iteration 88 : 0.036760000225282995
Loss in iteration 89 : 0.03633661892421361
Loss in iteration 90 : 0.03592133463014407
Loss in iteration 91 : 0.03551054507956113
Loss in iteration 92 : 0.035104444088809555
Loss in iteration 93 : 0.03470280804821641
Loss in iteration 94 : 0.034306865579046546
Loss in iteration 95 : 0.033913780615726995
Loss in iteration 96 : 0.033523424918640084
Loss in iteration 97 : 0.03313571657672452
Loss in iteration 98 : 0.0327544699603723
Loss in iteration 99 : 0.03238524823771256
Loss in iteration 100 : 0.032026897778430846
Loss in iteration 101 : 0.03168518263297565
Loss in iteration 102 : 0.03136078475438252
Loss in iteration 103 : 0.031053022998597333
Loss in iteration 104 : 0.03076019438760646
Loss in iteration 105 : 0.03048059282075332
Loss in iteration 106 : 0.03021296851974005
Loss in iteration 107 : 0.02995795927739802
Loss in iteration 108 : 0.029707923685537707
Loss in iteration 109 : 0.029461378629837404
Loss in iteration 110 : 0.029221439518437562
Loss in iteration 111 : 0.028992041142301445
Loss in iteration 112 : 0.028767206444789084
Loss in iteration 113 : 0.02854668007444757
Loss in iteration 114 : 0.028331034040200414
Loss in iteration 115 : 0.028118809016203904
Loss in iteration 116 : 0.02790971626909358
Loss in iteration 117 : 0.027704620646438596
Loss in iteration 118 : 0.027501560438655776
Loss in iteration 119 : 0.027300410551098008
Loss in iteration 120 : 0.027101168193865915
Loss in iteration 121 : 0.02690398464253206
Loss in iteration 122 : 0.026708288250773024
Loss in iteration 123 : 0.02651496789195841
Loss in iteration 124 : 0.026324292488387652
Loss in iteration 125 : 0.02613544757111795
Loss in iteration 126 : 0.02594872562671182
Loss in iteration 127 : 0.02576434022541437
Loss in iteration 128 : 0.025581301080350494
Loss in iteration 129 : 0.025400342953326934
Loss in iteration 130 : 0.02522215920100036
Loss in iteration 131 : 0.025046542709162237
Loss in iteration 132 : 0.024872168805915095
Loss in iteration 133 : 0.02469902867938867
Loss in iteration 134 : 0.02452805502273947
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.9413333333333334, training accuracy 0.9914261217490712, time elapsed: 3145 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38217011907456366
Loss in iteration 3 : 0.19975602447282512
Loss in iteration 4 : 0.18044212027086265
Loss in iteration 5 : 0.14217439960143757
Loss in iteration 6 : 0.14341477938698402
Loss in iteration 7 : 0.14842542773833695
Loss in iteration 8 : 0.15234251071115146
Loss in iteration 9 : 0.15476813031228542
Loss in iteration 10 : 0.15582451823101756
Loss in iteration 11 : 0.1558901091852048
Loss in iteration 12 : 0.15528232076476242
Loss in iteration 13 : 0.15451931165605265
Loss in iteration 14 : 0.15372984786474664
Loss in iteration 15 : 0.15287226907314413
Loss in iteration 16 : 0.15185094952299644
Loss in iteration 17 : 0.15054619503545758
Loss in iteration 18 : 0.1489523019082215
Loss in iteration 19 : 0.1470980860157758
Loss in iteration 20 : 0.1450238075304225
Loss in iteration 21 : 0.14274073770254464
Loss in iteration 22 : 0.14039482706461104
Loss in iteration 23 : 0.13795516235025526
Loss in iteration 24 : 0.13549880927261024
Loss in iteration 25 : 0.13300491520856678
Loss in iteration 26 : 0.1304528194180185
Loss in iteration 27 : 0.12790417658106673
Loss in iteration 28 : 0.1253391640939255
Loss in iteration 29 : 0.1228193126470564
Loss in iteration 30 : 0.120332384277962
Loss in iteration 31 : 0.11788690512834958
Loss in iteration 32 : 0.11555524155145766
Loss in iteration 33 : 0.11327388664326142
Loss in iteration 34 : 0.11101151079007682
Loss in iteration 35 : 0.1087657041363927
Loss in iteration 36 : 0.10657682480912214
Loss in iteration 37 : 0.10444353656126097
Loss in iteration 38 : 0.1023388221315609
Loss in iteration 39 : 0.10025316974109402
Loss in iteration 40 : 0.09818506730659313
Loss in iteration 41 : 0.09614641112636362
Loss in iteration 42 : 0.09413730861817439
Loss in iteration 43 : 0.09216690967553019
Loss in iteration 44 : 0.09023772168641796
Loss in iteration 45 : 0.08834925127555403
Loss in iteration 46 : 0.0865093389598999
Loss in iteration 47 : 0.08473505992088667
Loss in iteration 48 : 0.08300880062120619
Loss in iteration 49 : 0.0813405309271248
Loss in iteration 50 : 0.079807582697545
Loss in iteration 51 : 0.0784262852583369
Loss in iteration 52 : 0.07710997424611597
Loss in iteration 53 : 0.07583483821845544
Loss in iteration 54 : 0.0746025560247112
Loss in iteration 55 : 0.07341496526235024
Loss in iteration 56 : 0.07229846969007393
Loss in iteration 57 : 0.07124247952059065
Loss in iteration 58 : 0.07021810412832184
Loss in iteration 59 : 0.06919592327608276
Loss in iteration 60 : 0.06818212366075085
Loss in iteration 61 : 0.06718147812292023
Loss in iteration 62 : 0.06619039252288189
Loss in iteration 63 : 0.06521199373770659
Loss in iteration 64 : 0.0642481791178799
Loss in iteration 65 : 0.06329797698363249
Loss in iteration 66 : 0.0623677470738636
Loss in iteration 67 : 0.06145759972564013
Loss in iteration 68 : 0.060573480250436296
Loss in iteration 69 : 0.0597135771961078
Loss in iteration 70 : 0.058866790341253514
Loss in iteration 71 : 0.058044658829968836
Loss in iteration 72 : 0.05723502559117369
Loss in iteration 73 : 0.0564537919308817
Loss in iteration 74 : 0.0556892857524973
Loss in iteration 75 : 0.05494951206195207
Loss in iteration 76 : 0.05421957261750023
Loss in iteration 77 : 0.05349930273834342
Loss in iteration 78 : 0.05278545227044112
Loss in iteration 79 : 0.052077290184946404
Loss in iteration 80 : 0.051375660431440644
Loss in iteration 81 : 0.05069038832637581
Loss in iteration 82 : 0.05001318234152274
Loss in iteration 83 : 0.04934145215795621
Loss in iteration 84 : 0.04868225361602472
Loss in iteration 85 : 0.04803936254936923
Loss in iteration 86 : 0.047420831290685035
Loss in iteration 87 : 0.04681986553804098
Loss in iteration 88 : 0.046236536447024155
Loss in iteration 89 : 0.04567297081758799
Loss in iteration 90 : 0.04514043807490389
Loss in iteration 91 : 0.04462873254697365
Loss in iteration 92 : 0.044131066891246094
Loss in iteration 93 : 0.04364834049591176
Loss in iteration 94 : 0.043175549718073566
Loss in iteration 95 : 0.04271480121528036
Loss in iteration 96 : 0.04226814427719656
Loss in iteration 97 : 0.041833428055498265
Loss in iteration 98 : 0.04141607219813406
Loss in iteration 99 : 0.04100681776222189
Loss in iteration 100 : 0.040610893277504216
Loss in iteration 101 : 0.04022652399976317
Loss in iteration 102 : 0.0398470361719126
Loss in iteration 103 : 0.039484351697739346
Loss in iteration 104 : 0.039128348841541746
Loss in iteration 105 : 0.03878209341168224
Loss in iteration 106 : 0.03844547826541338
Loss in iteration 107 : 0.03811513321941712
Loss in iteration 108 : 0.03779820225711784
Loss in iteration 109 : 0.03748726229974751
Loss in iteration 110 : 0.03718313666421676
Loss in iteration 111 : 0.03688589263207457
Loss in iteration 112 : 0.03659475199334191
Loss in iteration 113 : 0.03630758151206549
Loss in iteration 114 : 0.036024212605680235
Loss in iteration 115 : 0.035744578725944894
Loss in iteration 116 : 0.03546802930751857
Loss in iteration 117 : 0.03519739076715962
Loss in iteration 118 : 0.03492999353319608
Loss in iteration 119 : 0.03466545760143357
Loss in iteration 120 : 0.034403967195723556
Loss in iteration 121 : 0.0341453867375787
Loss in iteration 122 : 0.03389115732919907
Loss in iteration 123 : 0.033641523167080974
Loss in iteration 124 : 0.033395867706024276
Loss in iteration 125 : 0.033152637884269774
Loss in iteration 126 : 0.03291329281898746
Loss in iteration 127 : 0.03267845210037892
Loss in iteration 128 : 0.0324482180374223
Loss in iteration 129 : 0.032221676298259984
Loss in iteration 130 : 0.03199969448004125
Loss in iteration 131 : 0.03178315632203092
Loss in iteration 132 : 0.03157084170076307
Loss in iteration 133 : 0.031360993217073786
Loss in iteration 134 : 0.031153793136583886
Loss in iteration 135 : 0.030950241826303966
Loss in iteration 136 : 0.03075135795649793
Loss in iteration 137 : 0.03055540346106296
Loss in iteration 138 : 0.030361925584771427
Loss in iteration 139 : 0.03017043422807452
Loss in iteration 140 : 0.029983702602218606
Loss in iteration 141 : 0.02979960586796683
Testing accuracy  of updater 1 on alg 1 with rate 0.7 = 0.9324444444444444, training accuracy 0.9895684481280366, time elapsed: 3159 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4795865082425461
Loss in iteration 3 : 0.2487017947662193
Loss in iteration 4 : 0.21135113139952488
Loss in iteration 5 : 0.1743556501187872
Loss in iteration 6 : 0.14052007859925109
Loss in iteration 7 : 0.1410968325990069
Loss in iteration 8 : 0.1533113053529826
Loss in iteration 9 : 0.14930372097843475
Loss in iteration 10 : 0.1398088731882916
Loss in iteration 11 : 0.13327018878516544
Loss in iteration 12 : 0.13001612425545128
Loss in iteration 13 : 0.12886521015841676
Loss in iteration 14 : 0.1285443015970191
Loss in iteration 15 : 0.128230787178494
Loss in iteration 16 : 0.127695801590277
Loss in iteration 17 : 0.12699836631903258
Loss in iteration 18 : 0.12612472140391473
Loss in iteration 19 : 0.12492808000515099
Loss in iteration 20 : 0.1233989511496532
Loss in iteration 21 : 0.12158515552984162
Loss in iteration 22 : 0.11951692888506482
Loss in iteration 23 : 0.11732643450107579
Loss in iteration 24 : 0.11509126193319688
Loss in iteration 25 : 0.11287593620525076
Loss in iteration 26 : 0.11078074556824744
Loss in iteration 27 : 0.10885261402447087
Loss in iteration 28 : 0.1070311688484759
Loss in iteration 29 : 0.10537035225377872
Loss in iteration 30 : 0.10380211749103406
Loss in iteration 31 : 0.10226822136027063
Loss in iteration 32 : 0.10079957857455925
Loss in iteration 33 : 0.09934529674028848
Loss in iteration 34 : 0.09786862752979471
Loss in iteration 35 : 0.09635977876997104
Loss in iteration 36 : 0.09483873766254068
Loss in iteration 37 : 0.09331310629875814
Loss in iteration 38 : 0.09179107500916722
Loss in iteration 39 : 0.09027359487567824
Loss in iteration 40 : 0.08875961741386824
Loss in iteration 41 : 0.08725346172099506
Loss in iteration 42 : 0.08577174268152496
Loss in iteration 43 : 0.08431550979105745
Loss in iteration 44 : 0.08290238894947609
Loss in iteration 45 : 0.08153113260029805
Loss in iteration 46 : 0.08018601881298255
Loss in iteration 47 : 0.0788704158953324
Loss in iteration 48 : 0.0775913287328074
Loss in iteration 49 : 0.0763721030761701
Loss in iteration 50 : 0.07520939232776427
Loss in iteration 51 : 0.07408770411202772
Loss in iteration 52 : 0.07300452926488363
Loss in iteration 53 : 0.07194364279895159
Loss in iteration 54 : 0.07090922613243245
Loss in iteration 55 : 0.06990115971887723
Loss in iteration 56 : 0.06892102660628797
Loss in iteration 57 : 0.06798053659426188
Loss in iteration 58 : 0.06706423102655595
Loss in iteration 59 : 0.0661731312824487
Loss in iteration 60 : 0.06530991753759288
Loss in iteration 61 : 0.06447061328971929
Loss in iteration 62 : 0.06365535096446791
Loss in iteration 63 : 0.06287148181808298
Loss in iteration 64 : 0.06211855768243349
Loss in iteration 65 : 0.06139350751974417
Loss in iteration 66 : 0.0606996035503765
Loss in iteration 67 : 0.06003198673261058
Loss in iteration 68 : 0.05938544015588869
Loss in iteration 69 : 0.058763306216548884
Loss in iteration 70 : 0.05816111358719231
Loss in iteration 71 : 0.0575720806680027
Loss in iteration 72 : 0.05699796742078395
Loss in iteration 73 : 0.056434644758609806
Loss in iteration 74 : 0.055886629036549555
Loss in iteration 75 : 0.055360095009380716
Loss in iteration 76 : 0.05484278649395393
Loss in iteration 77 : 0.05433389699798656
Loss in iteration 78 : 0.05383237596546011
Loss in iteration 79 : 0.05334343756076202
Loss in iteration 80 : 0.05286737169225781
Loss in iteration 81 : 0.05240022728372189
Loss in iteration 82 : 0.051940802511412
Loss in iteration 83 : 0.05148657845648918
Loss in iteration 84 : 0.05103749042261111
Loss in iteration 85 : 0.05059602672872586
Loss in iteration 86 : 0.05015904629465971
Loss in iteration 87 : 0.04972625634827777
Loss in iteration 88 : 0.04929773594919739
Loss in iteration 89 : 0.04887234946732983
Loss in iteration 90 : 0.04845042816282997
Loss in iteration 91 : 0.048031678533405744
Loss in iteration 92 : 0.047615549057891164
Loss in iteration 93 : 0.047204968393076495
Loss in iteration 94 : 0.04681093875354032
Loss in iteration 95 : 0.04642589251613755
Loss in iteration 96 : 0.046044568370413146
Loss in iteration 97 : 0.045667909488594886
Loss in iteration 98 : 0.045295006818887346
Loss in iteration 99 : 0.04492531755032123
Loss in iteration 100 : 0.04455873931818122
Loss in iteration 101 : 0.044196351388046874
Loss in iteration 102 : 0.04383844494364002
Loss in iteration 103 : 0.04348479011054286
Loss in iteration 104 : 0.04313420725490149
Loss in iteration 105 : 0.04278612241123704
Loss in iteration 106 : 0.04244044119820619
Loss in iteration 107 : 0.04209707646804241
Loss in iteration 108 : 0.041761220411990596
Loss in iteration 109 : 0.04143127237750354
Loss in iteration 110 : 0.04111102794678427
Loss in iteration 111 : 0.04079792076028704
Loss in iteration 112 : 0.04049504775288401
Loss in iteration 113 : 0.04019631333929538
Loss in iteration 114 : 0.03990080760169412
Loss in iteration 115 : 0.03960885481141266
Loss in iteration 116 : 0.03931945811613231
Loss in iteration 117 : 0.039033706376089314
Loss in iteration 118 : 0.03875049770088516
Loss in iteration 119 : 0.0384695904849879
Loss in iteration 120 : 0.03819110747166389
Loss in iteration 121 : 0.03791687349709757
Loss in iteration 122 : 0.03764658920047175
Loss in iteration 123 : 0.03737850022490286
Loss in iteration 124 : 0.037112621877991574
Loss in iteration 125 : 0.036848802071274146
Loss in iteration 126 : 0.03658808978079946
Loss in iteration 127 : 0.03632926733473653
Loss in iteration 128 : 0.03607220214202028
Loss in iteration 129 : 0.035816600387046614
Loss in iteration 130 : 0.035562409874286376
Loss in iteration 131 : 0.03530958253933754
Loss in iteration 132 : 0.03505807405662385
Loss in iteration 133 : 0.03480784348577081
Loss in iteration 134 : 0.03455885295280979
Loss in iteration 135 : 0.03431106736274578
Loss in iteration 136 : 0.03406567958617781
Loss in iteration 137 : 0.03382249838700009
Loss in iteration 138 : 0.03358281278753311
Loss in iteration 139 : 0.03334686314821158
Loss in iteration 140 : 0.033114986678957514
Loss in iteration 141 : 0.032885497778841845
Loss in iteration 142 : 0.032658357103079544
Loss in iteration 143 : 0.03243300599710418
Loss in iteration 144 : 0.03221135005962545
Loss in iteration 145 : 0.0319909640006572
Loss in iteration 146 : 0.03177157062834244
Loss in iteration 147 : 0.031553142872922414
Loss in iteration 148 : 0.03133614546292228
Loss in iteration 149 : 0.031123715217455
Loss in iteration 150 : 0.03091500465062515
Loss in iteration 151 : 0.030708975832615667
Loss in iteration 152 : 0.030507107734358444
Loss in iteration 153 : 0.030307690600491325
Loss in iteration 154 : 0.03011352559888506
Loss in iteration 155 : 0.029923454716507973
Loss in iteration 156 : 0.029734874309491705
Loss in iteration 157 : 0.02954782826283061
Loss in iteration 158 : 0.02936465824636532
Loss in iteration 159 : 0.029183896840902827
Loss in iteration 160 : 0.02900442878430473
Loss in iteration 161 : 0.028826603949162106
Loss in iteration 162 : 0.028650988176896958
Loss in iteration 163 : 0.028479596894716838
Loss in iteration 164 : 0.02830956266276378
Loss in iteration 165 : 0.02814066126918343
Loss in iteration 166 : 0.027973283973094436
Loss in iteration 167 : 0.027807696644391047
Loss in iteration 168 : 0.027643417801555936
Loss in iteration 169 : 0.02748095361217374
Loss in iteration 170 : 0.02732249652807116
Loss in iteration 171 : 0.027169409040284708
Loss in iteration 172 : 0.0270176702346021
Loss in iteration 173 : 0.02686851481567865
Loss in iteration 174 : 0.026721718336868507
Testing accuracy  of updater 1 on alg 1 with rate 0.4 = 0.9448888888888889, training accuracy 0.9898542440697342, time elapsed: 3893 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8642561778766499
Loss in iteration 3 : 0.6461013608380332
Loss in iteration 4 : 0.4390153278960388
Loss in iteration 5 : 0.3210308046950645
Loss in iteration 6 : 0.2585817363816546
Loss in iteration 7 : 0.22584488145815176
Loss in iteration 8 : 0.20644871588164676
Loss in iteration 9 : 0.1919418299258489
Loss in iteration 10 : 0.17841037408979488
Loss in iteration 11 : 0.16657287577819155
Loss in iteration 12 : 0.15822328151200765
Loss in iteration 13 : 0.1535185983563835
Loss in iteration 14 : 0.1508436792224778
Loss in iteration 15 : 0.14755161852086468
Loss in iteration 16 : 0.1426201052992952
Loss in iteration 17 : 0.13644108317526688
Loss in iteration 18 : 0.1301609968636037
Loss in iteration 19 : 0.1250074177390911
Loss in iteration 20 : 0.1212571192661801
Loss in iteration 21 : 0.1187186252915669
Loss in iteration 22 : 0.11684283163755663
Loss in iteration 23 : 0.11532745231906795
Loss in iteration 24 : 0.11403007620000842
Loss in iteration 25 : 0.1128322202541161
Loss in iteration 26 : 0.11162369219493326
Loss in iteration 27 : 0.11041391114569288
Loss in iteration 28 : 0.10919655105890873
Loss in iteration 29 : 0.1079677185981923
Loss in iteration 30 : 0.1067278112338277
Loss in iteration 31 : 0.10550190586061262
Loss in iteration 32 : 0.10430266494719097
Loss in iteration 33 : 0.10315660792203087
Loss in iteration 34 : 0.1020809278554391
Loss in iteration 35 : 0.10106566356459672
Loss in iteration 36 : 0.10010783039451768
Loss in iteration 37 : 0.09918912532865659
Loss in iteration 38 : 0.09829243956231613
Loss in iteration 39 : 0.09741483124052067
Loss in iteration 40 : 0.09655247583535603
Loss in iteration 41 : 0.0957210158183377
Loss in iteration 42 : 0.09491433108484953
Loss in iteration 43 : 0.09413473869946742
Loss in iteration 44 : 0.09337293934376122
Loss in iteration 45 : 0.09263216481373246
Loss in iteration 46 : 0.0919133816059206
Loss in iteration 47 : 0.09121010103492144
Loss in iteration 48 : 0.09052012234418606
Loss in iteration 49 : 0.08984204677759032
Loss in iteration 50 : 0.08918167630993351
Loss in iteration 51 : 0.08853649515436066
Loss in iteration 52 : 0.08790189841932027
Loss in iteration 53 : 0.0872755641742236
Loss in iteration 54 : 0.08665860911554774
Loss in iteration 55 : 0.0860512269180666
Loss in iteration 56 : 0.08545058368674065
Loss in iteration 57 : 0.084858781756427
Loss in iteration 58 : 0.0842801066259738
Loss in iteration 59 : 0.08371878599422868
Loss in iteration 60 : 0.08316957104125484
Loss in iteration 61 : 0.08263466973020378
Loss in iteration 62 : 0.08211001113893822
Loss in iteration 63 : 0.08159733552222316
Loss in iteration 64 : 0.08109255892837494
Loss in iteration 65 : 0.08059501319914236
Loss in iteration 66 : 0.08010642377964636
Loss in iteration 67 : 0.07962939173585404
Loss in iteration 68 : 0.07916599410223496
Loss in iteration 69 : 0.07871091932876623
Loss in iteration 70 : 0.07826308782417774
Loss in iteration 71 : 0.07782275608186451
Loss in iteration 72 : 0.07738987395097738
Loss in iteration 73 : 0.07696544623810801
Loss in iteration 74 : 0.07654639254180107
Loss in iteration 75 : 0.07613267201047094
Loss in iteration 76 : 0.07572824339949168
Loss in iteration 77 : 0.07533036170008815
Loss in iteration 78 : 0.07493642744201309
Loss in iteration 79 : 0.07454870521492052
Loss in iteration 80 : 0.07416974404745938
Loss in iteration 81 : 0.0737976488879659
Loss in iteration 82 : 0.07343204371849896
Loss in iteration 83 : 0.07307332236051721
Loss in iteration 84 : 0.07272342077926798
Loss in iteration 85 : 0.07237969924515887
Loss in iteration 86 : 0.07204122716883976
Loss in iteration 87 : 0.07170951861019276
Loss in iteration 88 : 0.07138519478605654
Loss in iteration 89 : 0.07106694002577395
Loss in iteration 90 : 0.07075596288142179
Loss in iteration 91 : 0.07045029555080742
Loss in iteration 92 : 0.07014979366954673
Loss in iteration 93 : 0.06985418527068257
Loss in iteration 94 : 0.06956188960420004
Loss in iteration 95 : 0.06927313651244464
Loss in iteration 96 : 0.06898929537226697
Loss in iteration 97 : 0.06870940946562919
Loss in iteration 98 : 0.06843276740931624
Loss in iteration 99 : 0.06815964963580033
Loss in iteration 100 : 0.06789157546851446
Loss in iteration 101 : 0.06762831543169234
Loss in iteration 102 : 0.06737014756558299
Loss in iteration 103 : 0.06711546174666813
Loss in iteration 104 : 0.0668631947438834
Loss in iteration 105 : 0.06661493339495475
Loss in iteration 106 : 0.06636967652778811
Loss in iteration 107 : 0.06612762210525668
Loss in iteration 108 : 0.06588887961730525
Loss in iteration 109 : 0.06565229477471424
Loss in iteration 110 : 0.06541752682833914
Loss in iteration 111 : 0.06518584245108114
Loss in iteration 112 : 0.06495737363687257
Loss in iteration 113 : 0.06473190804345165
Loss in iteration 114 : 0.06450889216395587
Loss in iteration 115 : 0.06428813912631748
Loss in iteration 116 : 0.06406913984817596
Loss in iteration 117 : 0.06385231646472382
Loss in iteration 118 : 0.06363819611117497
Loss in iteration 119 : 0.06342717745385715
Loss in iteration 120 : 0.06321875960965556
Loss in iteration 121 : 0.06301244467299255
Loss in iteration 122 : 0.06280808079052748
Loss in iteration 123 : 0.06260563736448145
Loss in iteration 124 : 0.06240489001105953
Loss in iteration 125 : 0.062206347814356425
Loss in iteration 126 : 0.06200966081572629
Loss in iteration 127 : 0.06181427954128681
Loss in iteration 128 : 0.061620145856675135
Loss in iteration 129 : 0.061427206590414556
Loss in iteration 130 : 0.06123541305414155
Loss in iteration 131 : 0.06104496114217519
Loss in iteration 132 : 0.06085567774957011
Loss in iteration 133 : 0.06066774421095763
Loss in iteration 134 : 0.06048084380441651
Loss in iteration 135 : 0.06029505380089691
Loss in iteration 136 : 0.06011022514846806
Loss in iteration 137 : 0.059926474540509055
Loss in iteration 138 : 0.059743662173493105
Loss in iteration 139 : 0.05956176914113133
Loss in iteration 140 : 0.059381153117946936
Loss in iteration 141 : 0.05920193017851071
Loss in iteration 142 : 0.059023821449056386
Loss in iteration 143 : 0.05884674929763908
Loss in iteration 144 : 0.05867054533560962
Loss in iteration 145 : 0.05849548664173441
Loss in iteration 146 : 0.0583220350406504
Loss in iteration 147 : 0.05815011672432477
Loss in iteration 148 : 0.05797929593042333
Loss in iteration 149 : 0.05780945559326375
Loss in iteration 150 : 0.057641943120911664
Loss in iteration 151 : 0.05747657037279675
Loss in iteration 152 : 0.05731256850215791
Loss in iteration 153 : 0.0571495878614093
Loss in iteration 154 : 0.056988392285250405
Loss in iteration 155 : 0.056829525827469606
Loss in iteration 156 : 0.05667196281159658
Loss in iteration 157 : 0.05651544493547734
Loss in iteration 158 : 0.05636049707452814
Loss in iteration 159 : 0.05620668102055237
Loss in iteration 160 : 0.05605389094204765
Loss in iteration 161 : 0.05590192936399368
Loss in iteration 162 : 0.05575081241243929
Loss in iteration 163 : 0.05560083400027324
Loss in iteration 164 : 0.05545242697238781
Loss in iteration 165 : 0.05530478134717973
Loss in iteration 166 : 0.055157863713766936
Loss in iteration 167 : 0.05501164361566287
Loss in iteration 168 : 0.05486623564563399
Loss in iteration 169 : 0.05472249116991954
Loss in iteration 170 : 0.05457960860393994
Loss in iteration 171 : 0.05443749374677936
Loss in iteration 172 : 0.054296054876711665
Loss in iteration 173 : 0.0541553208159939
Testing accuracy  of updater 1 on alg 1 with rate 0.09999999999999998 = 0.9742222222222222, training accuracy 0.9799942840811661, time elapsed: 4172 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.4547303741133404
Loss in iteration 3 : 1.3138535839505783
Loss in iteration 4 : 0.5407740661403062
Loss in iteration 5 : 0.556036398288999
Loss in iteration 6 : 0.6466882308024265
Loss in iteration 7 : 0.7101556899164616
Loss in iteration 8 : 0.7468243339667264
Loss in iteration 9 : 0.7617743032065593
Loss in iteration 10 : 0.7599164844357534
Loss in iteration 11 : 0.7479548197938629
Loss in iteration 12 : 0.7300418268252321
Loss in iteration 13 : 0.7101346064126696
Loss in iteration 14 : 0.6889283509943231
Loss in iteration 15 : 0.6657231962761572
Loss in iteration 16 : 0.6413536158218638
Loss in iteration 17 : 0.6170164311521333
Loss in iteration 18 : 0.5941526043949547
Loss in iteration 19 : 0.5729181586720317
Loss in iteration 20 : 0.5543137850488302
Loss in iteration 21 : 0.5370111775025208
Loss in iteration 22 : 0.5213708864329305
Loss in iteration 23 : 0.5063341256842773
Loss in iteration 24 : 0.49063010274554
Loss in iteration 25 : 0.4741938765767348
Loss in iteration 26 : 0.4570300785385682
Loss in iteration 27 : 0.43913024034688214
Loss in iteration 28 : 0.4209574232822388
Loss in iteration 29 : 0.40253735924260187
Loss in iteration 30 : 0.3843428747316325
Loss in iteration 31 : 0.36644511320548384
Loss in iteration 32 : 0.34931520560765356
Loss in iteration 33 : 0.3330814333487283
Loss in iteration 34 : 0.31831473791263937
Loss in iteration 35 : 0.3053619299860209
Loss in iteration 36 : 0.29483679296774445
Loss in iteration 37 : 0.28622613771916033
Loss in iteration 38 : 0.27883500531207883
Loss in iteration 39 : 0.2722185580750236
Loss in iteration 40 : 0.2659477476072171
Loss in iteration 41 : 0.2600852848648225
Loss in iteration 42 : 0.25455633130301436
Loss in iteration 43 : 0.24954106485490665
Loss in iteration 44 : 0.24478834828515764
Loss in iteration 45 : 0.24027495228149004
Loss in iteration 46 : 0.23581812743111263
Loss in iteration 47 : 0.2315657080975823
Loss in iteration 48 : 0.2273567395829292
Loss in iteration 49 : 0.22346494801379682
Loss in iteration 50 : 0.21974893268101287
Loss in iteration 51 : 0.21623217782386536
Loss in iteration 52 : 0.21277481099405832
Loss in iteration 53 : 0.20935287369071076
Loss in iteration 54 : 0.20598360250953768
Loss in iteration 55 : 0.20264044507623716
Loss in iteration 56 : 0.19933440926617033
Loss in iteration 57 : 0.19623044125925831
Loss in iteration 58 : 0.1931920945709995
Loss in iteration 59 : 0.19028800508311475
Loss in iteration 60 : 0.18744558263106376
Loss in iteration 61 : 0.18470321397728892
Loss in iteration 62 : 0.18201330069263014
Loss in iteration 63 : 0.17936342521360005
Loss in iteration 64 : 0.17683228889944183
Loss in iteration 65 : 0.1744349174278216
Loss in iteration 66 : 0.17217488539360254
Loss in iteration 67 : 0.17001057386556861
Loss in iteration 68 : 0.16792179237017152
Loss in iteration 69 : 0.1658421209275467
Loss in iteration 70 : 0.16377794330681394
Loss in iteration 71 : 0.161732830848691
Loss in iteration 72 : 0.15970074738712328
Loss in iteration 73 : 0.1577205439155254
Loss in iteration 74 : 0.15578980420194505
Loss in iteration 75 : 0.153946209071583
Loss in iteration 76 : 0.15223708319875204
Loss in iteration 77 : 0.15057796038718366
Loss in iteration 78 : 0.14896325765900675
Loss in iteration 79 : 0.1473876736573856
Loss in iteration 80 : 0.14583767826229282
Loss in iteration 81 : 0.1443136021454195
Loss in iteration 82 : 0.1428115851875751
Loss in iteration 83 : 0.14132492817880066
Loss in iteration 84 : 0.13985290080992369
Loss in iteration 85 : 0.13839483153483115
Loss in iteration 86 : 0.13697044962075577
Loss in iteration 87 : 0.13556191440649595
Loss in iteration 88 : 0.13418847701141062
Loss in iteration 89 : 0.13282814232431653
Loss in iteration 90 : 0.13149390075420192
Loss in iteration 91 : 0.13017707373579324
Loss in iteration 92 : 0.1288809329987148
Loss in iteration 93 : 0.1276045653921122
Loss in iteration 94 : 0.12633910493842057
Loss in iteration 95 : 0.1250840773810544
Loss in iteration 96 : 0.12383904622636119
Loss in iteration 97 : 0.12260584021985146
Loss in iteration 98 : 0.12138979166735887
Loss in iteration 99 : 0.12018332653128559
Loss in iteration 100 : 0.11898691050191852
Loss in iteration 101 : 0.11780208920315857
Loss in iteration 102 : 0.11662580462893578
Loss in iteration 103 : 0.11545773851004375
Loss in iteration 104 : 0.11429759665914271
Loss in iteration 105 : 0.11314510674748923
Loss in iteration 106 : 0.11200459741542511
Loss in iteration 107 : 0.1108729194601664
Loss in iteration 108 : 0.10974829302103314
Loss in iteration 109 : 0.10863050117911197
Loss in iteration 110 : 0.10752069968900155
Loss in iteration 111 : 0.10645171234641929
Loss in iteration 112 : 0.10540644782543167
Loss in iteration 113 : 0.1043665833321924
Loss in iteration 114 : 0.10334176148203927
Loss in iteration 115 : 0.10234124043675041
Loss in iteration 116 : 0.10134881862519234
Loss in iteration 117 : 0.1003870642049562
Loss in iteration 118 : 0.09948404276063899
Loss in iteration 119 : 0.09860987665112547
Loss in iteration 120 : 0.09777920447317609
Loss in iteration 121 : 0.09695814087131689
Loss in iteration 122 : 0.09614602686141763
Loss in iteration 123 : 0.09534463879332696
Loss in iteration 124 : 0.09455326180383695
Loss in iteration 125 : 0.09377205766647043
Loss in iteration 126 : 0.09302507546645253
Testing accuracy  of updater 2 on alg 1 with rate 7.0 = 0.9208888888888889, training accuracy 0.9907116318948271, time elapsed: 2680 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.7631435764809624
Loss in iteration 3 : 0.9384411089669142
Loss in iteration 4 : 0.3938903076783822
Loss in iteration 5 : 0.40861140692754955
Loss in iteration 6 : 0.4736548660577557
Loss in iteration 7 : 0.5174699828558338
Loss in iteration 8 : 0.5426568744881942
Loss in iteration 9 : 0.5510095815834293
Loss in iteration 10 : 0.5472531119301569
Loss in iteration 11 : 0.5371261410666974
Loss in iteration 12 : 0.523874849124284
Loss in iteration 13 : 0.5099608937847042
Loss in iteration 14 : 0.4951778078670285
Loss in iteration 15 : 0.4792456423306663
Loss in iteration 16 : 0.4625006267685399
Loss in iteration 17 : 0.4459741891161099
Loss in iteration 18 : 0.4304091923754356
Loss in iteration 19 : 0.4162914615820612
Loss in iteration 20 : 0.40381467852561026
Loss in iteration 21 : 0.3922158599102681
Loss in iteration 22 : 0.38146061419998106
Loss in iteration 23 : 0.37063094791478257
Loss in iteration 24 : 0.35935623058600796
Loss in iteration 25 : 0.34752889026440054
Loss in iteration 26 : 0.33504896267552137
Loss in iteration 27 : 0.3219956941822145
Loss in iteration 28 : 0.30846455913537146
Loss in iteration 29 : 0.29500581291713524
Loss in iteration 30 : 0.2815629677398913
Loss in iteration 31 : 0.26838861534862724
Loss in iteration 32 : 0.25566040907905685
Loss in iteration 33 : 0.2435698232159627
Loss in iteration 34 : 0.2323739082589893
Loss in iteration 35 : 0.22286337990743252
Loss in iteration 36 : 0.21516738653973863
Loss in iteration 37 : 0.20905731008136566
Loss in iteration 38 : 0.20394495398117826
Loss in iteration 39 : 0.1991774578987633
Loss in iteration 40 : 0.19453715323458745
Loss in iteration 41 : 0.19008466103974894
Loss in iteration 42 : 0.1859123144388564
Loss in iteration 43 : 0.18198301439478975
Loss in iteration 44 : 0.1783321832191964
Loss in iteration 45 : 0.17491819420011867
Loss in iteration 46 : 0.17162628128989538
Loss in iteration 47 : 0.1684164990376695
Loss in iteration 48 : 0.16527714527206186
Loss in iteration 49 : 0.16223415954670592
Loss in iteration 50 : 0.15941053515316705
Loss in iteration 51 : 0.1568027003055785
Loss in iteration 52 : 0.15429841089101093
Loss in iteration 53 : 0.1518419795738764
Loss in iteration 54 : 0.1494576588855404
Loss in iteration 55 : 0.14713436760561294
Loss in iteration 56 : 0.14482868790523945
Loss in iteration 57 : 0.14259753492508112
Loss in iteration 58 : 0.14044886153289896
Loss in iteration 59 : 0.13834867899745726
Loss in iteration 60 : 0.13633077911315977
Loss in iteration 61 : 0.13435988201447774
Loss in iteration 62 : 0.13241789527569256
Loss in iteration 63 : 0.13051089432202026
Loss in iteration 64 : 0.1286438930910667
Loss in iteration 65 : 0.1268021862648913
Loss in iteration 66 : 0.12500590020452854
Loss in iteration 67 : 0.12328192538821788
Loss in iteration 68 : 0.12164210752448551
Loss in iteration 69 : 0.12006325931102697
Loss in iteration 70 : 0.11855333898934213
Loss in iteration 71 : 0.11709295555528516
Loss in iteration 72 : 0.11566671771982823
Loss in iteration 73 : 0.11425022808640546
Loss in iteration 74 : 0.11287613556218506
Loss in iteration 75 : 0.1115500314060556
Loss in iteration 76 : 0.11024264003780256
Loss in iteration 77 : 0.10898592482198538
Loss in iteration 78 : 0.10778590768315979
Loss in iteration 79 : 0.10663607148590484
Loss in iteration 80 : 0.10550828182282568
Loss in iteration 81 : 0.10442793994977283
Loss in iteration 82 : 0.10336638806721068
Loss in iteration 83 : 0.10231600878372887
Loss in iteration 84 : 0.10127625236482614
Loss in iteration 85 : 0.1002466139991653
Loss in iteration 86 : 0.09922662959935398
Loss in iteration 87 : 0.09821834154580211
Loss in iteration 88 : 0.09722602314840838
Loss in iteration 89 : 0.09625258684976369
Loss in iteration 90 : 0.09529053741496345
Loss in iteration 91 : 0.09434968737974284
Loss in iteration 92 : 0.09341738445142343
Loss in iteration 93 : 0.0924932353974075
Loss in iteration 94 : 0.09158220880693066
Loss in iteration 95 : 0.09068612142028491
Loss in iteration 96 : 0.08979753167908926
Loss in iteration 97 : 0.08892520446239675
Loss in iteration 98 : 0.08806192688245534
Loss in iteration 99 : 0.08720535801885605
Loss in iteration 100 : 0.08635522638871954
Loss in iteration 101 : 0.08551586896239524
Loss in iteration 102 : 0.08468483296641766
Loss in iteration 103 : 0.08386009347228326
Loss in iteration 104 : 0.0830442398419951
Loss in iteration 105 : 0.08223411800264599
Loss in iteration 106 : 0.08142951367684116
Loss in iteration 107 : 0.08063061645836835
Loss in iteration 108 : 0.0798420424881957
Loss in iteration 109 : 0.07905847257082126
Loss in iteration 110 : 0.07828883331821408
Loss in iteration 111 : 0.07752812856216042
Loss in iteration 112 : 0.07677247233837403
Loss in iteration 113 : 0.07602857855511701
Loss in iteration 114 : 0.07532292680742958
Loss in iteration 115 : 0.07462274033235428
Loss in iteration 116 : 0.07392734052326529
Loss in iteration 117 : 0.07323652826598183
Loss in iteration 118 : 0.07255101825992774
Loss in iteration 119 : 0.07188871611443591
Loss in iteration 120 : 0.0712446634980713
Loss in iteration 121 : 0.07061345660133789
Loss in iteration 122 : 0.07000438075239021
Loss in iteration 123 : 0.06940830750040378
Loss in iteration 124 : 0.06881985135889956
Loss in iteration 125 : 0.06824885054549226
Loss in iteration 126 : 0.06768687130808285
Loss in iteration 127 : 0.0671389102944804
Loss in iteration 128 : 0.06659844641913988
Testing accuracy  of updater 2 on alg 1 with rate 4.8999999999999995 = 0.9217777777777778, training accuracy 0.9907116318948271, time elapsed: 2466 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0743084510080494
Loss in iteration 3 : 0.5596798359218478
Loss in iteration 4 : 0.24652863724616292
Loss in iteration 5 : 0.2637341956991757
Loss in iteration 6 : 0.3028620377851249
Loss in iteration 7 : 0.327932349018147
Loss in iteration 8 : 0.34017896174963663
Loss in iteration 9 : 0.3420770036263546
Loss in iteration 10 : 0.336365495080958
Loss in iteration 11 : 0.32743278894372135
Loss in iteration 12 : 0.31890104921073437
Loss in iteration 13 : 0.3105511514189464
Loss in iteration 14 : 0.30219466291225905
Loss in iteration 15 : 0.293274641559302
Loss in iteration 16 : 0.284156318738004
Loss in iteration 17 : 0.27553660616561115
Loss in iteration 18 : 0.2679687209119917
Loss in iteration 19 : 0.2612082342413783
Loss in iteration 20 : 0.25517433592815936
Loss in iteration 21 : 0.2491195353611642
Loss in iteration 22 : 0.24286609508580553
Loss in iteration 23 : 0.23619249945665852
Loss in iteration 24 : 0.22900786387113992
Loss in iteration 25 : 0.22132681135374213
Loss in iteration 26 : 0.21326628733203132
Loss in iteration 27 : 0.20491838004436363
Loss in iteration 28 : 0.19636665514831028
Loss in iteration 29 : 0.18788412265522877
Loss in iteration 30 : 0.1796406316288307
Loss in iteration 31 : 0.17155859922282746
Loss in iteration 32 : 0.1635833161816011
Loss in iteration 33 : 0.15576759747398153
Loss in iteration 34 : 0.1488069504151071
Loss in iteration 35 : 0.14285909823003395
Loss in iteration 36 : 0.13791701564009032
Loss in iteration 37 : 0.13385102859373926
Loss in iteration 38 : 0.13024340207990276
Loss in iteration 39 : 0.12688542236924535
Loss in iteration 40 : 0.1237779622377993
Loss in iteration 41 : 0.12077722306548289
Loss in iteration 42 : 0.11787766619105454
Loss in iteration 43 : 0.11511631432058454
Loss in iteration 44 : 0.11252775735462917
Loss in iteration 45 : 0.11014868380335478
Loss in iteration 46 : 0.10787132278367274
Loss in iteration 47 : 0.10568537981000828
Loss in iteration 48 : 0.10358922084021976
Loss in iteration 49 : 0.10154274238036691
Loss in iteration 50 : 0.09961311232030981
Loss in iteration 51 : 0.0978373801354958
Loss in iteration 52 : 0.09622008323551268
Loss in iteration 53 : 0.09467553211451595
Loss in iteration 54 : 0.09319905354133706
Loss in iteration 55 : 0.0917822867243446
Loss in iteration 56 : 0.09041558145381087
Loss in iteration 57 : 0.08908268123177017
Loss in iteration 58 : 0.08780481214329584
Loss in iteration 59 : 0.08655846641286699
Loss in iteration 60 : 0.08534960748786237
Loss in iteration 61 : 0.08417066528140608
Loss in iteration 62 : 0.08302671023305483
Loss in iteration 63 : 0.08190073650436924
Loss in iteration 64 : 0.08078978692355135
Loss in iteration 65 : 0.07969900721773919
Loss in iteration 66 : 0.07862377197874171
Loss in iteration 67 : 0.07756238581864236
Loss in iteration 68 : 0.0765201347660648
Loss in iteration 69 : 0.0754898209470506
Loss in iteration 70 : 0.07447258721292195
Loss in iteration 71 : 0.07347147372100503
Loss in iteration 72 : 0.07248090384198333
Loss in iteration 73 : 0.07150255804329837
Loss in iteration 74 : 0.07056375964151906
Loss in iteration 75 : 0.06966428887550717
Loss in iteration 76 : 0.06880658909291369
Loss in iteration 77 : 0.06797252913014344
Loss in iteration 78 : 0.06717050435680855
Loss in iteration 79 : 0.06641586024039828
Loss in iteration 80 : 0.0656844092103858
Loss in iteration 81 : 0.06500505877426144
Loss in iteration 82 : 0.06433940986449385
Loss in iteration 83 : 0.06367778062386253
Loss in iteration 84 : 0.06302151455075378
Loss in iteration 85 : 0.06237115038846017
Loss in iteration 86 : 0.06172945195485915
Loss in iteration 87 : 0.06111910770826188
Loss in iteration 88 : 0.06052069697560928
Loss in iteration 89 : 0.05993620791106865
Loss in iteration 90 : 0.05936281857173846
Loss in iteration 91 : 0.058803045512961935
Loss in iteration 92 : 0.058250749903148054
Loss in iteration 93 : 0.05770312932960453
Loss in iteration 94 : 0.05715978533181259
Loss in iteration 95 : 0.05662055687019771
Loss in iteration 96 : 0.0560852948300203
Loss in iteration 97 : 0.055554594161288176
Loss in iteration 98 : 0.05502957851606507
Loss in iteration 99 : 0.054514758975189224
Loss in iteration 100 : 0.05400387072054044
Loss in iteration 101 : 0.05350117059651192
Loss in iteration 102 : 0.053004641463222785
Loss in iteration 103 : 0.052513238971395364
Loss in iteration 104 : 0.05202831418292575
Loss in iteration 105 : 0.05154605136889637
Loss in iteration 106 : 0.05106759247083941
Loss in iteration 107 : 0.050592382964044745
Loss in iteration 108 : 0.050120304999559995
Loss in iteration 109 : 0.0496559108685118
Loss in iteration 110 : 0.049198722714657216
Loss in iteration 111 : 0.04874949871650267
Loss in iteration 112 : 0.048306953025837936
Loss in iteration 113 : 0.04786787930581926
Loss in iteration 114 : 0.04743321998404377
Loss in iteration 115 : 0.047002857597628296
Loss in iteration 116 : 0.04657610159285426
Loss in iteration 117 : 0.04615361442810605
Loss in iteration 118 : 0.04573482082835158
Loss in iteration 119 : 0.04532800355707182
Loss in iteration 120 : 0.044931826111674664
Loss in iteration 121 : 0.04454400874273669
Loss in iteration 122 : 0.04416407262347631
Loss in iteration 123 : 0.04378720544146988
Loss in iteration 124 : 0.043415825143498885
Loss in iteration 125 : 0.043062390678059205
Loss in iteration 126 : 0.04271243294635849
Loss in iteration 127 : 0.042365730060994644
Loss in iteration 128 : 0.04202208084560593
Loss in iteration 129 : 0.041694332353308924
Loss in iteration 130 : 0.04137163018340529
Testing accuracy  of updater 2 on alg 1 with rate 2.8 = 0.9262222222222222, training accuracy 0.9911403258073735, time elapsed: 2329 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4189035227147329
Loss in iteration 3 : 0.19306931631162516
Loss in iteration 4 : 0.13858926043143785
Loss in iteration 5 : 0.12353323460440147
Loss in iteration 6 : 0.12180138782696362
Loss in iteration 7 : 0.12221993201180129
Loss in iteration 8 : 0.12219815595356234
Loss in iteration 9 : 0.12147501611748536
Loss in iteration 10 : 0.12010813105210891
Loss in iteration 11 : 0.11846761075686649
Loss in iteration 12 : 0.11685314959653897
Loss in iteration 13 : 0.11533982590855096
Loss in iteration 14 : 0.11396727128744352
Loss in iteration 15 : 0.11264243049637378
Loss in iteration 16 : 0.11111456484546774
Loss in iteration 17 : 0.10937322922944359
Loss in iteration 18 : 0.1074491378925184
Loss in iteration 19 : 0.10537885568018912
Loss in iteration 20 : 0.10321471756251412
Loss in iteration 21 : 0.1009862350301755
Loss in iteration 22 : 0.09871991922775859
Loss in iteration 23 : 0.09642022484158104
Loss in iteration 24 : 0.09405890885289823
Loss in iteration 25 : 0.09167935334286856
Loss in iteration 26 : 0.08931778404827788
Loss in iteration 27 : 0.08697154922871525
Loss in iteration 28 : 0.0846508559661799
Loss in iteration 29 : 0.08238847383501054
Loss in iteration 30 : 0.08017915705343136
Loss in iteration 31 : 0.07796969699486352
Loss in iteration 32 : 0.07580352668921833
Loss in iteration 33 : 0.07373464191999543
Loss in iteration 34 : 0.0717644240961855
Loss in iteration 35 : 0.06987843938147197
Loss in iteration 36 : 0.06805921712370472
Loss in iteration 37 : 0.0663372226403682
Loss in iteration 38 : 0.06469763931640085
Loss in iteration 39 : 0.06312051421468298
Loss in iteration 40 : 0.06163295711125233
Loss in iteration 41 : 0.0602231096099912
Loss in iteration 42 : 0.05887245135426523
Loss in iteration 43 : 0.05757880826415954
Loss in iteration 44 : 0.056354780503518706
Loss in iteration 45 : 0.0551941623673224
Loss in iteration 46 : 0.05407743909280126
Loss in iteration 47 : 0.053021587874429635
Loss in iteration 48 : 0.052019879246026135
Loss in iteration 49 : 0.05104241650970632
Loss in iteration 50 : 0.05013999361078265
Loss in iteration 51 : 0.049261040217903784
Loss in iteration 52 : 0.04839972027423132
Loss in iteration 53 : 0.04756557298818165
Loss in iteration 54 : 0.04674501086206543
Loss in iteration 55 : 0.04593692980498448
Loss in iteration 56 : 0.04514255409993984
Loss in iteration 57 : 0.044373452383809554
Loss in iteration 58 : 0.043614200323794226
Loss in iteration 59 : 0.0428780423462385
Loss in iteration 60 : 0.042177618486805596
Loss in iteration 61 : 0.04149065043643248
Loss in iteration 62 : 0.04081024122891359
Loss in iteration 63 : 0.040138388588765395
Loss in iteration 64 : 0.03947972277357737
Loss in iteration 65 : 0.03883741516010425
Loss in iteration 66 : 0.03820691593870478
Loss in iteration 67 : 0.03758343482421831
Loss in iteration 68 : 0.036972817537925695
Loss in iteration 69 : 0.03638423163774356
Loss in iteration 70 : 0.035821468876662155
Loss in iteration 71 : 0.03527845893198197
Loss in iteration 72 : 0.03476185703449746
Loss in iteration 73 : 0.03426711168505925
Loss in iteration 74 : 0.03379081377022486
Loss in iteration 75 : 0.03333169312624741
Loss in iteration 76 : 0.0328858931384182
Loss in iteration 77 : 0.0324517544317949
Loss in iteration 78 : 0.03203632644748839
Loss in iteration 79 : 0.031633400046837304
Loss in iteration 80 : 0.03124106704851701
Loss in iteration 81 : 0.03086047388952145
Loss in iteration 82 : 0.030490883380838642
Loss in iteration 83 : 0.030129757848182885
Loss in iteration 84 : 0.029780548943383132
Loss in iteration 85 : 0.029438881182985186
Loss in iteration 86 : 0.029104854210695717
Loss in iteration 87 : 0.02877653325833925
Loss in iteration 88 : 0.028455135091386405
Loss in iteration 89 : 0.028140786264126832
Loss in iteration 90 : 0.027830964402075126
Loss in iteration 91 : 0.0275259522658241
Loss in iteration 92 : 0.02722565003476462
Loss in iteration 93 : 0.0269294882528873
Loss in iteration 94 : 0.026637457762477907
Loss in iteration 95 : 0.026348580422685498
Loss in iteration 96 : 0.02606305805426113
Loss in iteration 97 : 0.02578257716715476
Loss in iteration 98 : 0.02550784378635484
Loss in iteration 99 : 0.025240417674965394
Loss in iteration 100 : 0.024975792959710672
Loss in iteration 101 : 0.024714031177446322
Loss in iteration 102 : 0.024455160958410112
Loss in iteration 103 : 0.024203337915059138
Loss in iteration 104 : 0.023961897808891817
Loss in iteration 105 : 0.023725779925416208
Loss in iteration 106 : 0.02349523462513278
Loss in iteration 107 : 0.023272289357052955
Loss in iteration 108 : 0.023057454271813816
Loss in iteration 109 : 0.022851185615714073
Loss in iteration 110 : 0.022652630315493232
Loss in iteration 111 : 0.022458705186233503
Loss in iteration 112 : 0.022268034735951715
Loss in iteration 113 : 0.02208101692020149
Loss in iteration 114 : 0.021900627670567403
Loss in iteration 115 : 0.021725755969474292
Loss in iteration 116 : 0.021556236955786446
Loss in iteration 117 : 0.02138981528097797
Loss in iteration 118 : 0.021226390985259292
Loss in iteration 119 : 0.0210671926756392
Loss in iteration 120 : 0.020910300383972708
Loss in iteration 121 : 0.020756967449485735
Loss in iteration 122 : 0.02060610866045264
Loss in iteration 123 : 0.020457876775346915
Loss in iteration 124 : 0.020312208414035034
Loss in iteration 125 : 0.02016785039832992
Loss in iteration 126 : 0.020024767968814056
Loss in iteration 127 : 0.0198832826449339
Loss in iteration 128 : 0.019744169546753794
Loss in iteration 129 : 0.01960628182474143
Loss in iteration 130 : 0.019469333466483326
Loss in iteration 131 : 0.019333578669525765
Loss in iteration 132 : 0.01919893004798651
Loss in iteration 133 : 0.01906603089235762
Loss in iteration 134 : 0.01893432634106685
Loss in iteration 135 : 0.018803715581107973
Loss in iteration 136 : 0.018675573020301928
Loss in iteration 137 : 0.01854832139902213
Loss in iteration 138 : 0.01842233171985599
Loss in iteration 139 : 0.018297980725611226
Loss in iteration 140 : 0.018175392177699548
Loss in iteration 141 : 0.01805360741054513
Loss in iteration 142 : 0.01793275335966759
Loss in iteration 143 : 0.01781272519073417
Loss in iteration 144 : 0.017693423169818548
Loss in iteration 145 : 0.01757481365648323
Loss in iteration 146 : 0.01745686597303285
Loss in iteration 147 : 0.017339567452572518
Loss in iteration 148 : 0.017223444910989677
Testing accuracy  of updater 2 on alg 1 with rate 0.7 = 0.9448888888888889, training accuracy 0.9937124892826522, time elapsed: 2732 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.37535242102814503
Loss in iteration 3 : 0.19422358173040216
Loss in iteration 4 : 0.15635480972606716
Loss in iteration 5 : 0.12541784940240963
Loss in iteration 6 : 0.11907101796619247
Loss in iteration 7 : 0.11622026187487186
Loss in iteration 8 : 0.11471855132128633
Loss in iteration 9 : 0.11331416042633864
Loss in iteration 10 : 0.11198019656106908
Loss in iteration 11 : 0.11050283942291697
Loss in iteration 12 : 0.10907030395666552
Loss in iteration 13 : 0.10730281704964126
Loss in iteration 14 : 0.1054204002353899
Loss in iteration 15 : 0.10360246689306317
Loss in iteration 16 : 0.10187623594790801
Loss in iteration 17 : 0.10024206023069915
Loss in iteration 18 : 0.09865180591703515
Loss in iteration 19 : 0.09708237790926709
Loss in iteration 20 : 0.0954684741079451
Loss in iteration 21 : 0.09384066927361456
Loss in iteration 22 : 0.09214549666700154
Loss in iteration 23 : 0.0903911129809557
Loss in iteration 24 : 0.08859534010396411
Loss in iteration 25 : 0.08679411750097686
Loss in iteration 26 : 0.08498925357019767
Loss in iteration 27 : 0.08318218634939178
Loss in iteration 28 : 0.08138771854129069
Loss in iteration 29 : 0.07959737785971856
Loss in iteration 30 : 0.07780480450561596
Loss in iteration 31 : 0.076044215067979
Loss in iteration 32 : 0.07433300986922767
Loss in iteration 33 : 0.07264229519519544
Loss in iteration 34 : 0.0709998042152664
Loss in iteration 35 : 0.06944960953776502
Loss in iteration 36 : 0.06798024495866475
Loss in iteration 37 : 0.06656999652502789
Loss in iteration 38 : 0.06518886586499034
Loss in iteration 39 : 0.0638470710034609
Loss in iteration 40 : 0.06255550839198007
Loss in iteration 41 : 0.061317646595851476
Loss in iteration 42 : 0.060147006044080596
Loss in iteration 43 : 0.059048619176057957
Loss in iteration 44 : 0.058006156579308625
Loss in iteration 45 : 0.05700975286512867
Loss in iteration 46 : 0.05604574000350777
Loss in iteration 47 : 0.05513122920485873
Loss in iteration 48 : 0.05425481562375028
Loss in iteration 49 : 0.05341573010199928
Loss in iteration 50 : 0.052603085589355725
Loss in iteration 51 : 0.05181715900549314
Loss in iteration 52 : 0.05104889498875197
Loss in iteration 53 : 0.050308230893697986
Loss in iteration 54 : 0.0495843129117233
Loss in iteration 55 : 0.04888359705802696
Loss in iteration 56 : 0.04821212238517288
Loss in iteration 57 : 0.04755692774405883
Loss in iteration 58 : 0.04691523853381197
Loss in iteration 59 : 0.04628325290029716
Loss in iteration 60 : 0.045661555894712
Loss in iteration 61 : 0.04504869722783458
Loss in iteration 62 : 0.04445009840635652
Loss in iteration 63 : 0.043872070023980796
Loss in iteration 64 : 0.043320146024578884
Loss in iteration 65 : 0.04278914176225184
Loss in iteration 66 : 0.04227062337929796
Loss in iteration 67 : 0.041763096825938296
Loss in iteration 68 : 0.041264676671047704
Loss in iteration 69 : 0.04077556993062188
Loss in iteration 70 : 0.040294766711404854
Loss in iteration 71 : 0.03982023985705268
Loss in iteration 72 : 0.03935144160416794
Loss in iteration 73 : 0.03889017219929266
Loss in iteration 74 : 0.038434085464028066
Loss in iteration 75 : 0.03798324987087959
Loss in iteration 76 : 0.03753754665323785
Loss in iteration 77 : 0.03709640217988135
Loss in iteration 78 : 0.03666078943744664
Loss in iteration 79 : 0.036235697065737774
Loss in iteration 80 : 0.0358256103788683
Loss in iteration 81 : 0.03542389000237506
Loss in iteration 82 : 0.03502737919665372
Loss in iteration 83 : 0.034636258391476174
Loss in iteration 84 : 0.03425173786243587
Loss in iteration 85 : 0.03387167770753069
Loss in iteration 86 : 0.03349656458284323
Loss in iteration 87 : 0.03312723210318012
Loss in iteration 88 : 0.032762632049228295
Loss in iteration 89 : 0.03240251189093514
Loss in iteration 90 : 0.03204619364931297
Loss in iteration 91 : 0.031693302566637875
Loss in iteration 92 : 0.03134340366996541
Loss in iteration 93 : 0.030996371983021423
Loss in iteration 94 : 0.03065209223771866
Loss in iteration 95 : 0.030310994377140136
Loss in iteration 96 : 0.0299728480551626
Loss in iteration 97 : 0.02963715800656572
Loss in iteration 98 : 0.029303839125234845
Loss in iteration 99 : 0.02897685399002181
Loss in iteration 100 : 0.028656227455291134
Loss in iteration 101 : 0.02833931976739834
Loss in iteration 102 : 0.028028657029311812
Loss in iteration 103 : 0.027735839720667295
Loss in iteration 104 : 0.027449566184496073
Loss in iteration 105 : 0.027165936157088337
Loss in iteration 106 : 0.02688701348767351
Loss in iteration 107 : 0.026621060690790326
Loss in iteration 108 : 0.02637123114530661
Loss in iteration 109 : 0.026125576987424366
Loss in iteration 110 : 0.025885754620799388
Loss in iteration 111 : 0.025651650138587712
Loss in iteration 112 : 0.025424904272643375
Loss in iteration 113 : 0.02520087144153795
Loss in iteration 114 : 0.024983097474183785
Loss in iteration 115 : 0.02476893128907247
Loss in iteration 116 : 0.024557213425808258
Loss in iteration 117 : 0.02434833120900927
Loss in iteration 118 : 0.02414514747268032
Loss in iteration 119 : 0.023944648232519673
Loss in iteration 120 : 0.023746232790362334
Loss in iteration 121 : 0.023549768443859604
Loss in iteration 122 : 0.02335513360307309
Loss in iteration 123 : 0.023163590806983006
Loss in iteration 124 : 0.022976201174300732
Loss in iteration 125 : 0.022790835382979328
Loss in iteration 126 : 0.022607086215048432
Loss in iteration 127 : 0.022425172476677302
Loss in iteration 128 : 0.02224524715748234
Loss in iteration 129 : 0.022066718458294828
Loss in iteration 130 : 0.021890525944523682
Loss in iteration 131 : 0.02171816201252288
Loss in iteration 132 : 0.021548269051999677
Loss in iteration 133 : 0.021380644436350512
Loss in iteration 134 : 0.021218943472556054
Loss in iteration 135 : 0.02106163860581698
Loss in iteration 136 : 0.02090721744499397
Loss in iteration 137 : 0.02075492301685846
Loss in iteration 138 : 0.02060554899492879
Loss in iteration 139 : 0.02046062309773407
Loss in iteration 140 : 0.020319518760510365
Loss in iteration 141 : 0.02018237153513826
Loss in iteration 142 : 0.020048409401687638
Loss in iteration 143 : 0.019916969753470595
Loss in iteration 144 : 0.01978798924045941
Loss in iteration 145 : 0.019660224397362095
Loss in iteration 146 : 0.019534308609760447
Loss in iteration 147 : 0.01941055061536289
Loss in iteration 148 : 0.01928788904327654
Loss in iteration 149 : 0.01916847141614042
Loss in iteration 150 : 0.019051919465518038
Loss in iteration 151 : 0.018936936492982556
Loss in iteration 152 : 0.018822873868552508
Loss in iteration 153 : 0.018709674201321618
Loss in iteration 154 : 0.018597868469639074
Loss in iteration 155 : 0.0184878633763659
Loss in iteration 156 : 0.018380096716947864
Loss in iteration 157 : 0.01827406526234093
Testing accuracy  of updater 2 on alg 1 with rate 0.49 = 0.944, training accuracy 0.9939982852243499, time elapsed: 3123 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.413205377096678
Loss in iteration 3 : 0.3485505440256175
Loss in iteration 4 : 0.2280072952869128
Loss in iteration 5 : 0.21846249114973015
Loss in iteration 6 : 0.19871412818406878
Loss in iteration 7 : 0.1690870397287613
Loss in iteration 8 : 0.13735829379190898
Loss in iteration 9 : 0.11565852517533626
Loss in iteration 10 : 0.10999999916852883
Loss in iteration 11 : 0.11253367528427405
Loss in iteration 12 : 0.11357648768181697
Loss in iteration 13 : 0.11156501237885477
Loss in iteration 14 : 0.10763253114065766
Loss in iteration 15 : 0.10383325088686635
Loss in iteration 16 : 0.10089690884913603
Loss in iteration 17 : 0.09870891990453368
Loss in iteration 18 : 0.0969932675732836
Loss in iteration 19 : 0.09544783137588084
Loss in iteration 20 : 0.09412313824299168
Loss in iteration 21 : 0.09298259582824288
Loss in iteration 22 : 0.09187927381834426
Loss in iteration 23 : 0.09076431513045884
Loss in iteration 24 : 0.0896328044002083
Loss in iteration 25 : 0.08846321224260652
Loss in iteration 26 : 0.0872693008193222
Loss in iteration 27 : 0.08605197674034977
Loss in iteration 28 : 0.08481365478524322
Loss in iteration 29 : 0.08355613156613576
Loss in iteration 30 : 0.08229595560295799
Loss in iteration 31 : 0.08104754559419293
Loss in iteration 32 : 0.07983779083907473
Loss in iteration 33 : 0.07869417214768304
Loss in iteration 34 : 0.07760559015875378
Loss in iteration 35 : 0.07654162248339103
Loss in iteration 36 : 0.07548629584130956
Loss in iteration 37 : 0.07444739242350927
Loss in iteration 38 : 0.07343700137288657
Loss in iteration 39 : 0.07245513722477188
Loss in iteration 40 : 0.0714989664574114
Loss in iteration 41 : 0.07055665768485983
Loss in iteration 42 : 0.06962033311173482
Loss in iteration 43 : 0.06870449015629093
Loss in iteration 44 : 0.06780350295453369
Loss in iteration 45 : 0.06693218302159397
Loss in iteration 46 : 0.06608027260078163
Loss in iteration 47 : 0.06524838582101058
Loss in iteration 48 : 0.06443365607862786
Loss in iteration 49 : 0.06364464348079368
Loss in iteration 50 : 0.06288101547030389
Loss in iteration 51 : 0.06213917725056785
Loss in iteration 52 : 0.06142953783060002
Loss in iteration 53 : 0.060756567679293126
Loss in iteration 54 : 0.06010079562535085
Loss in iteration 55 : 0.0594559756190564
Loss in iteration 56 : 0.05882558805719265
Loss in iteration 57 : 0.05821539218829846
Loss in iteration 58 : 0.05763068002833344
Loss in iteration 59 : 0.05706511193735504
Loss in iteration 60 : 0.056508451862298
Loss in iteration 61 : 0.05596503221249309
Loss in iteration 62 : 0.055437477261429405
Loss in iteration 63 : 0.054922751762855784
Loss in iteration 64 : 0.05441681282299008
Loss in iteration 65 : 0.05391738054960615
Loss in iteration 66 : 0.053429500518954294
Loss in iteration 67 : 0.05295401882559327
Loss in iteration 68 : 0.05249654985784859
Loss in iteration 69 : 0.05204986490813195
Loss in iteration 70 : 0.051614994397704984
Loss in iteration 71 : 0.051192441199877696
Loss in iteration 72 : 0.05078479808674105
Loss in iteration 73 : 0.05039819012082367
Loss in iteration 74 : 0.0500185152803927
Loss in iteration 75 : 0.04964588532601707
Loss in iteration 76 : 0.04927882345990362
Loss in iteration 77 : 0.0489175745629914
Loss in iteration 78 : 0.0485616037179705
Loss in iteration 79 : 0.0482091001864376
Loss in iteration 80 : 0.047859724744823195
Loss in iteration 81 : 0.04751757022786913
Loss in iteration 82 : 0.047182626577210715
Loss in iteration 83 : 0.0468511847901074
Loss in iteration 84 : 0.04652479821083923
Loss in iteration 85 : 0.04620158139840982
Loss in iteration 86 : 0.045881087911022043
Loss in iteration 87 : 0.04556347609448411
Loss in iteration 88 : 0.04524834651178318
Loss in iteration 89 : 0.044936107753626095
Loss in iteration 90 : 0.04462708458023026
Loss in iteration 91 : 0.04432091808898514
Loss in iteration 92 : 0.04401709094318132
Loss in iteration 93 : 0.043716286668286766
Loss in iteration 94 : 0.04341937529364382
Loss in iteration 95 : 0.0431294134502701
Loss in iteration 96 : 0.04284928054597601
Loss in iteration 97 : 0.042572098051646565
Loss in iteration 98 : 0.042297270137444914
Loss in iteration 99 : 0.04202478590133418
Loss in iteration 100 : 0.041755408043123034
Loss in iteration 101 : 0.041488103520961564
Loss in iteration 102 : 0.04122368848489468
Loss in iteration 103 : 0.04096181401565191
Loss in iteration 104 : 0.04070215059063124
Loss in iteration 105 : 0.040444520509345
Loss in iteration 106 : 0.04018889589585463
Loss in iteration 107 : 0.03993504802288446
Loss in iteration 108 : 0.03968290966875482
Loss in iteration 109 : 0.03943241880963952
Loss in iteration 110 : 0.03918607016104277
Loss in iteration 111 : 0.03894704651100098
Loss in iteration 112 : 0.038710730178126346
Loss in iteration 113 : 0.03847701633501022
Loss in iteration 114 : 0.038245580639715865
Loss in iteration 115 : 0.038015918701961936
Loss in iteration 116 : 0.03778800838475569
Loss in iteration 117 : 0.03756165955536692
Loss in iteration 118 : 0.037337811503323194
Loss in iteration 119 : 0.03711950791845888
Loss in iteration 120 : 0.03690666870327635
Loss in iteration 121 : 0.03669699780342602
Loss in iteration 122 : 0.03649001164891552
Loss in iteration 123 : 0.036285215256090506
Loss in iteration 124 : 0.03608231568570473
Loss in iteration 125 : 0.035881418437151376
Loss in iteration 126 : 0.03568223668448254
Loss in iteration 127 : 0.03548471078673721
Loss in iteration 128 : 0.03528903813960612
Loss in iteration 129 : 0.03509496658202429
Loss in iteration 130 : 0.03490236116974275
Loss in iteration 131 : 0.0347111338950386
Loss in iteration 132 : 0.03452161562899028
Loss in iteration 133 : 0.03433423221382874
Loss in iteration 134 : 0.03414890951070673
Loss in iteration 135 : 0.033965204635799315
Loss in iteration 136 : 0.03378273352197445
Loss in iteration 137 : 0.03360202604165267
Loss in iteration 138 : 0.03342248210805802
Loss in iteration 139 : 0.033243905794753115
Loss in iteration 140 : 0.033066261150779694
Loss in iteration 141 : 0.032890101890781835
Loss in iteration 142 : 0.03271658186228105
Loss in iteration 143 : 0.0325443977283913
Loss in iteration 144 : 0.032373582572240184
Loss in iteration 145 : 0.03220547602747394
Loss in iteration 146 : 0.03203835954540391
Loss in iteration 147 : 0.031872447136088486
Loss in iteration 148 : 0.03170757915549475
Loss in iteration 149 : 0.031543618212593036
Loss in iteration 150 : 0.031381196634481
Loss in iteration 151 : 0.03122087164617923
Loss in iteration 152 : 0.03106177230658576
Loss in iteration 153 : 0.030905882994391655
Loss in iteration 154 : 0.030751691331352204
Loss in iteration 155 : 0.030599759536051804
Loss in iteration 156 : 0.030448654148271657
Loss in iteration 157 : 0.030298445592738706
Loss in iteration 158 : 0.03015101154999628
Loss in iteration 159 : 0.030005727841608522
Loss in iteration 160 : 0.02986179123691235
Loss in iteration 161 : 0.029718644661686267
Loss in iteration 162 : 0.029576414617660918
Loss in iteration 163 : 0.029435026829672008
Loss in iteration 164 : 0.029294496228451067
Loss in iteration 165 : 0.02915477890614455
Loss in iteration 166 : 0.029015798064099717
Loss in iteration 167 : 0.02887773791145209
Loss in iteration 168 : 0.028740528636961443
Loss in iteration 169 : 0.028603946157538977
Loss in iteration 170 : 0.0284679663818524
Loss in iteration 171 : 0.028332802276031684
Loss in iteration 172 : 0.02819833705086665
Loss in iteration 173 : 0.02806456818530539
Loss in iteration 174 : 0.02793168837337849
Loss in iteration 175 : 0.027799289042116863
Loss in iteration 176 : 0.02766735859787639
Loss in iteration 177 : 0.02753588629687225
Loss in iteration 178 : 0.027404930309833864
Loss in iteration 179 : 0.02727462415568004
Loss in iteration 180 : 0.027145025673530977
Loss in iteration 181 : 0.027016417275362394
Loss in iteration 182 : 0.02688946796167199
Loss in iteration 183 : 0.02676408326407702
Loss in iteration 184 : 0.026639311015220265
Loss in iteration 185 : 0.02651507258394445
Loss in iteration 186 : 0.026391346752568712
Loss in iteration 187 : 0.02626811731988707
Testing accuracy  of updater 2 on alg 1 with rate 0.27999999999999997 = 0.9502222222222222, training accuracy 0.9898542440697342, time elapsed: 3714 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8194607165759449
Loss in iteration 3 : 0.6148334178522948
Loss in iteration 4 : 0.4450331534029816
Loss in iteration 5 : 0.3193532808874474
Loss in iteration 6 : 0.24761072746768317
Loss in iteration 7 : 0.21954650032111042
Loss in iteration 8 : 0.20443182280011413
Loss in iteration 9 : 0.1946443059384409
Loss in iteration 10 : 0.18696107342603713
Loss in iteration 11 : 0.1796669164801836
Loss in iteration 12 : 0.17254502079325174
Loss in iteration 13 : 0.16573567196403993
Loss in iteration 14 : 0.1592597174959289
Loss in iteration 15 : 0.15330085085897913
Loss in iteration 16 : 0.14798070712370337
Loss in iteration 17 : 0.14333357693467783
Loss in iteration 18 : 0.13913564526504701
Loss in iteration 19 : 0.13533048055930888
Loss in iteration 20 : 0.13180267934538709
Loss in iteration 21 : 0.12859651070057673
Loss in iteration 22 : 0.12563596260156826
Loss in iteration 23 : 0.122862626106415
Loss in iteration 24 : 0.12033357307318528
Loss in iteration 25 : 0.11803828902032625
Loss in iteration 26 : 0.11588925862313729
Loss in iteration 27 : 0.11389200880870369
Loss in iteration 28 : 0.11207719416888319
Loss in iteration 29 : 0.11041597976831002
Loss in iteration 30 : 0.10888439842585955
Loss in iteration 31 : 0.10748068695102517
Loss in iteration 32 : 0.10617503254408825
Loss in iteration 33 : 0.1049411884457147
Loss in iteration 34 : 0.10377416697998053
Loss in iteration 35 : 0.10267656881207844
Loss in iteration 36 : 0.10164372846547157
Loss in iteration 37 : 0.10066877720551633
Loss in iteration 38 : 0.09974099673966584
Loss in iteration 39 : 0.0988615168298272
Loss in iteration 40 : 0.09802787281903985
Loss in iteration 41 : 0.09722903742041776
Loss in iteration 42 : 0.0964617577327125
Loss in iteration 43 : 0.09572305452801724
Loss in iteration 44 : 0.09501007146655316
Loss in iteration 45 : 0.09432802963557935
Loss in iteration 46 : 0.09367322522649271
Loss in iteration 47 : 0.09304239375064483
Loss in iteration 48 : 0.09243597595522825
Loss in iteration 49 : 0.09184891089307351
Loss in iteration 50 : 0.09127575446509266
Loss in iteration 51 : 0.09071638555413482
Loss in iteration 52 : 0.09016760488222107
Loss in iteration 53 : 0.08963187296668246
Loss in iteration 54 : 0.0891089603141223
Loss in iteration 55 : 0.08859746684438034
Loss in iteration 56 : 0.0881000678758971
Loss in iteration 57 : 0.08761297385559667
Loss in iteration 58 : 0.08713623391455523
Loss in iteration 59 : 0.08666781037024242
Loss in iteration 60 : 0.08620737658718597
Loss in iteration 61 : 0.0857546867478925
Loss in iteration 62 : 0.08531017627175212
Loss in iteration 63 : 0.08487316559629207
Loss in iteration 64 : 0.0844444941621481
Loss in iteration 65 : 0.08402216319083725
Loss in iteration 66 : 0.0836059944423858
Loss in iteration 67 : 0.08319598465203487
Loss in iteration 68 : 0.0827911812501839
Loss in iteration 69 : 0.08239213533766372
Loss in iteration 70 : 0.08199938041054866
Loss in iteration 71 : 0.08161254105311247
Loss in iteration 72 : 0.08123089809729811
Loss in iteration 73 : 0.080854473451545
Loss in iteration 74 : 0.08048402965879772
Loss in iteration 75 : 0.08012002843070093
Loss in iteration 76 : 0.07976129783312227
Loss in iteration 77 : 0.07940841999690189
Loss in iteration 78 : 0.07906210214236345
Loss in iteration 79 : 0.07871975088170118
Loss in iteration 80 : 0.07838209606784184
Loss in iteration 81 : 0.07804940106403108
Loss in iteration 82 : 0.077722363563219
Loss in iteration 83 : 0.07739890666749591
Loss in iteration 84 : 0.07707875325392165
Loss in iteration 85 : 0.0767622804904826
Loss in iteration 86 : 0.07644928929525785
Loss in iteration 87 : 0.07614007598349339
Loss in iteration 88 : 0.07583470681576307
Loss in iteration 89 : 0.07553282954563138
Loss in iteration 90 : 0.07523518767517742
Loss in iteration 91 : 0.0749409348628742
Loss in iteration 92 : 0.07464944989342849
Loss in iteration 93 : 0.07436165525708122
Loss in iteration 94 : 0.07407721128928006
Loss in iteration 95 : 0.07379532319968174
Loss in iteration 96 : 0.07351663694973096
Loss in iteration 97 : 0.07324152388944848
Loss in iteration 98 : 0.07296922175579863
Loss in iteration 99 : 0.0726994357855275
Loss in iteration 100 : 0.07243210281728589
Loss in iteration 101 : 0.07216707625869007
Loss in iteration 102 : 0.07190421328090069
Loss in iteration 103 : 0.07164353321852804
Loss in iteration 104 : 0.0713847583663345
Loss in iteration 105 : 0.07112807870916049
Loss in iteration 106 : 0.07087351656210739
Loss in iteration 107 : 0.07062272954700888
Loss in iteration 108 : 0.07037502655317068
Loss in iteration 109 : 0.0701306028053048
Loss in iteration 110 : 0.06988932871290017
Loss in iteration 111 : 0.0696513099348276
Loss in iteration 112 : 0.06941656242954049
Loss in iteration 113 : 0.06918430983399071
Loss in iteration 114 : 0.06895446612110967
Loss in iteration 115 : 0.06872628563804285
Loss in iteration 116 : 0.0685007088541795
Loss in iteration 117 : 0.06827750041797917
Loss in iteration 118 : 0.06805668057111411
Loss in iteration 119 : 0.06783839493508853
Loss in iteration 120 : 0.06762223584763943
Loss in iteration 121 : 0.06740837496728906
Loss in iteration 122 : 0.06719635972785289
Loss in iteration 123 : 0.06698594401760068
Loss in iteration 124 : 0.06677779397884419
Loss in iteration 125 : 0.06657163796822722
Loss in iteration 126 : 0.06636709943210296
Loss in iteration 127 : 0.06616457819710889
Loss in iteration 128 : 0.06596353722404219
Loss in iteration 129 : 0.06576391721633922
Loss in iteration 130 : 0.06556567059969759
Loss in iteration 131 : 0.06536886038329516
Loss in iteration 132 : 0.06517366310856958
Loss in iteration 133 : 0.06497957157599442
Loss in iteration 134 : 0.0647866917992267
Loss in iteration 135 : 0.06459547117156195
Loss in iteration 136 : 0.0644059562443406
Loss in iteration 137 : 0.06421825490131507
Loss in iteration 138 : 0.06403223011065995
Loss in iteration 139 : 0.06384801036071104
Loss in iteration 140 : 0.06366545484140919
Loss in iteration 141 : 0.06348415332783117
Loss in iteration 142 : 0.06330476081917807
Loss in iteration 143 : 0.06312737210746168
Loss in iteration 144 : 0.06295234134051166
Loss in iteration 145 : 0.06277984517164513
Loss in iteration 146 : 0.06261140806366183
Loss in iteration 147 : 0.062445620449105614
Loss in iteration 148 : 0.06228207682192336
Loss in iteration 149 : 0.062120471528320016
Loss in iteration 150 : 0.06196096711980999
Loss in iteration 151 : 0.06180315791526933
Loss in iteration 152 : 0.061646762767564006
Loss in iteration 153 : 0.06149184434859515
Loss in iteration 154 : 0.06133823133762496
Loss in iteration 155 : 0.061186327355361156
Loss in iteration 156 : 0.06103601058056591
Loss in iteration 157 : 0.06088687787661144
Loss in iteration 158 : 0.06073870442559015
Loss in iteration 159 : 0.06059188752494411
Loss in iteration 160 : 0.060446877937160276
Loss in iteration 161 : 0.06030315866659393
Loss in iteration 162 : 0.060160404322354574
Loss in iteration 163 : 0.060018728329121314
Loss in iteration 164 : 0.059878752727215455
Loss in iteration 165 : 0.05973987026991537
Loss in iteration 166 : 0.05960221290984727
Loss in iteration 167 : 0.0594657897008166
Loss in iteration 168 : 0.059330297972046454
Loss in iteration 169 : 0.059195540943147135
Loss in iteration 170 : 0.059061818991967656
Loss in iteration 171 : 0.05892924961817136
Loss in iteration 172 : 0.0587981167784358
Loss in iteration 173 : 0.058667857255885934
Loss in iteration 174 : 0.05853893042642296
Loss in iteration 175 : 0.058410817639693795
Loss in iteration 176 : 0.05828515341329064
Loss in iteration 177 : 0.05816044880696283
Loss in iteration 178 : 0.05803727322673335
Loss in iteration 179 : 0.05791524395894492
Loss in iteration 180 : 0.0577946881974061
Testing accuracy  of updater 2 on alg 1 with rate 0.06999999999999995 = 0.9831111111111112, training accuracy 0.9799942840811661, time elapsed: 4725 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 48.24184739076987
Loss in iteration 3 : 5.333779831709181
Loss in iteration 4 : 2.953807603364911
Loss in iteration 5 : 2.321032669065514
Loss in iteration 6 : 1.9633668205565171
Loss in iteration 7 : 1.6862793440219972
Loss in iteration 8 : 1.4487548941098383
Loss in iteration 9 : 1.2418606643840302
Loss in iteration 10 : 1.0928187877711484
Loss in iteration 11 : 0.9893992663980942
Loss in iteration 12 : 0.9044096832670593
Loss in iteration 13 : 0.8416197068698602
Loss in iteration 14 : 0.7880974001807131
Loss in iteration 15 : 0.7422000620079177
Loss in iteration 16 : 0.7021922454470615
Loss in iteration 17 : 0.668408371788643
Loss in iteration 18 : 0.6369458109192527
Loss in iteration 19 : 0.6067831403480367
Loss in iteration 20 : 0.5789077452772644
Loss in iteration 21 : 0.5534876924138197
Loss in iteration 22 : 0.5293590981025896
Loss in iteration 23 : 0.5074848113092203
Loss in iteration 24 : 0.4882600314459763
Loss in iteration 25 : 0.471887112358493
Loss in iteration 26 : 0.4564194015296036
Loss in iteration 27 : 0.44220963957321235
Loss in iteration 28 : 0.42965281531400096
Loss in iteration 29 : 0.4182838675890669
Loss in iteration 30 : 0.4073306588640839
Loss in iteration 31 : 0.3968447680806686
Loss in iteration 32 : 0.3868091500235996
Loss in iteration 33 : 0.3775717208655764
Loss in iteration 34 : 0.36879948996392703
Loss in iteration 35 : 0.36081847889442087
Loss in iteration 36 : 0.35301252063841904
Loss in iteration 37 : 0.3457819304222553
Loss in iteration 38 : 0.338871164581182
Loss in iteration 39 : 0.33240756132534
Loss in iteration 40 : 0.32625758242666286
Loss in iteration 41 : 0.3201829582392821
Loss in iteration 42 : 0.3141888261730415
Loss in iteration 43 : 0.3084296087583443
Loss in iteration 44 : 0.3029916697617652
Loss in iteration 45 : 0.29768029895080295
Loss in iteration 46 : 0.292449987989927
Loss in iteration 47 : 0.28744478460453354
Loss in iteration 48 : 0.2828567216824326
Loss in iteration 49 : 0.27846760728220105
Loss in iteration 50 : 0.2741305057275586
Loss in iteration 51 : 0.27005278390236254
Loss in iteration 52 : 0.2660174644789873
Loss in iteration 53 : 0.2621381249814509
Loss in iteration 54 : 0.2585126313890992
Loss in iteration 55 : 0.25492275016214533
Loss in iteration 56 : 0.25138018398020473
Loss in iteration 57 : 0.24797308334505672
Loss in iteration 58 : 0.2446017673123113
Loss in iteration 59 : 0.24132846569584435
Loss in iteration 60 : 0.23822060271242398
Loss in iteration 61 : 0.23516020966156215
Loss in iteration 62 : 0.23212469233266314
Loss in iteration 63 : 0.22911531632728496
Loss in iteration 64 : 0.22618579936739236
Loss in iteration 65 : 0.22347733953730223
Loss in iteration 66 : 0.22085161624822852
Loss in iteration 67 : 0.21825460279981843
Testing accuracy  of updater 3 on alg 1 with rate 40.0 = 0.9644444444444444, training accuracy 0.9919977136324665, time elapsed: 1881 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 16.116009267827828
Loss in iteration 3 : 1.4854519553346563
Loss in iteration 4 : 1.1008644591199868
Loss in iteration 5 : 0.8762612550708002
Loss in iteration 6 : 0.7239299708686534
Loss in iteration 7 : 0.6133690962064428
Loss in iteration 8 : 0.5234312953432476
Loss in iteration 9 : 0.4552920884524727
Loss in iteration 10 : 0.4030320522682428
Loss in iteration 11 : 0.3658450607446784
Loss in iteration 12 : 0.33464742233549966
Loss in iteration 13 : 0.3064021384148679
Loss in iteration 14 : 0.2822002555768338
Loss in iteration 15 : 0.2633938847751359
Loss in iteration 16 : 0.2482401959055625
Loss in iteration 17 : 0.23653714765940898
Loss in iteration 18 : 0.2263110286710435
Loss in iteration 19 : 0.21705114632751812
Loss in iteration 20 : 0.20856937309701878
Loss in iteration 21 : 0.20123243333177734
Loss in iteration 22 : 0.19438261319393715
Loss in iteration 23 : 0.18818134143953794
Loss in iteration 24 : 0.1824574074457579
Loss in iteration 25 : 0.177287480039955
Loss in iteration 26 : 0.17249116000545744
Loss in iteration 27 : 0.1678960188402191
Loss in iteration 28 : 0.16348239304570628
Loss in iteration 29 : 0.1592760308978479
Loss in iteration 30 : 0.15524746538029033
Loss in iteration 31 : 0.15148784254431816
Loss in iteration 32 : 0.14799255299910605
Loss in iteration 33 : 0.14459384106515769
Loss in iteration 34 : 0.14130064174765455
Loss in iteration 35 : 0.1382670493761182
Loss in iteration 36 : 0.13535313781210473
Loss in iteration 37 : 0.13269022772790476
Loss in iteration 38 : 0.13015048711163185
Loss in iteration 39 : 0.12764535251258957
Loss in iteration 40 : 0.12517349313149492
Loss in iteration 41 : 0.12275060179274991
Loss in iteration 42 : 0.12038135523680471
Loss in iteration 43 : 0.11804135375056059
Loss in iteration 44 : 0.11572957811589317
Loss in iteration 45 : 0.11344803960957751
Loss in iteration 46 : 0.1111987837976634
Loss in iteration 47 : 0.10897741835139135
Loss in iteration 48 : 0.10677769632053208
Loss in iteration 49 : 0.10463906794392018
Loss in iteration 50 : 0.10258193989752139
Loss in iteration 51 : 0.10062387294854182
Loss in iteration 52 : 0.0987226224727991
Loss in iteration 53 : 0.09688986204651087
Loss in iteration 54 : 0.09512388411352596
Loss in iteration 55 : 0.09342408893885203
Loss in iteration 56 : 0.09179517875447309
Loss in iteration 57 : 0.09030593723941073
Loss in iteration 58 : 0.08900344195221274
Loss in iteration 59 : 0.08772185064500132
Loss in iteration 60 : 0.08647933384012325
Loss in iteration 61 : 0.0852748172420658
Loss in iteration 62 : 0.08408831761566594
Loss in iteration 63 : 0.08295741677404002
Loss in iteration 64 : 0.08192561154253018
Loss in iteration 65 : 0.08095178285521414
Loss in iteration 66 : 0.08008940438289132
Loss in iteration 67 : 0.07925767142916092
Loss in iteration 68 : 0.07850400224672817
Loss in iteration 69 : 0.07777031964036092
Loss in iteration 70 : 0.07706685125686491
Loss in iteration 71 : 0.07637057883967922
Loss in iteration 72 : 0.07567963416033237
Loss in iteration 73 : 0.07499189129818414
Testing accuracy  of updater 3 on alg 1 with rate 28.0 = 0.96, training accuracy 0.9949985710202915, time elapsed: 1857 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.580836962657011
Loss in iteration 3 : 0.7126329747691676
Loss in iteration 4 : 0.5465877846457393
Loss in iteration 5 : 0.44155091378535705
Loss in iteration 6 : 0.36330108165694946
Loss in iteration 7 : 0.3094444807261949
Loss in iteration 8 : 0.2655990771598424
Loss in iteration 9 : 0.2309992631529583
Loss in iteration 10 : 0.20375306681810862
Loss in iteration 11 : 0.18364798680248592
Loss in iteration 12 : 0.1681786030049529
Loss in iteration 13 : 0.15481748957895058
Loss in iteration 14 : 0.14226656121947592
Loss in iteration 15 : 0.13191017910686392
Loss in iteration 16 : 0.12425716015041408
Loss in iteration 17 : 0.11800310739160087
Loss in iteration 18 : 0.11276284124774442
Loss in iteration 19 : 0.10823319632435453
Loss in iteration 20 : 0.10406729269725856
Loss in iteration 21 : 0.10019066524222627
Loss in iteration 22 : 0.09676761113032929
Loss in iteration 23 : 0.09381940767205388
Loss in iteration 24 : 0.0910380016918653
Loss in iteration 25 : 0.08838439076770913
Loss in iteration 26 : 0.08593595683689514
Loss in iteration 27 : 0.08363343174417187
Loss in iteration 28 : 0.08151538369394325
Loss in iteration 29 : 0.0794679729881989
Loss in iteration 30 : 0.07747894478088725
Loss in iteration 31 : 0.07553177104985108
Loss in iteration 32 : 0.0736520767672525
Loss in iteration 33 : 0.07183174382184426
Loss in iteration 34 : 0.07008255136312919
Loss in iteration 35 : 0.0684048045516709
Loss in iteration 36 : 0.06678891595726509
Loss in iteration 37 : 0.06532071080892089
Loss in iteration 38 : 0.06399535050444755
Loss in iteration 39 : 0.06270527506960291
Loss in iteration 40 : 0.061460136387082
Loss in iteration 41 : 0.06025882291316389
Loss in iteration 42 : 0.05908146017003267
Loss in iteration 43 : 0.05795078215018231
Loss in iteration 44 : 0.05683358686495821
Loss in iteration 45 : 0.05573277763731169
Loss in iteration 46 : 0.05465771371720391
Loss in iteration 47 : 0.053594646926162226
Loss in iteration 48 : 0.052550069744501146
Loss in iteration 49 : 0.051544478867659164
Loss in iteration 50 : 0.050601825896884375
Loss in iteration 51 : 0.049697545661428845
Loss in iteration 52 : 0.04880237302176858
Loss in iteration 53 : 0.04791604549903436
Loss in iteration 54 : 0.04705402027374264
Loss in iteration 55 : 0.04621990982413126
Loss in iteration 56 : 0.045422933657276496
Loss in iteration 57 : 0.04468602889080112
Loss in iteration 58 : 0.04397014155898627
Loss in iteration 59 : 0.043277391821699235
Loss in iteration 60 : 0.0425943545756859
Loss in iteration 61 : 0.041917180047480976
Loss in iteration 62 : 0.041273596724187685
Loss in iteration 63 : 0.04070482765727052
Loss in iteration 64 : 0.04014738058123566
Loss in iteration 65 : 0.039606561134916
Loss in iteration 66 : 0.03910956080873191
Loss in iteration 67 : 0.03865088074978099
Loss in iteration 68 : 0.03820936736865282
Loss in iteration 69 : 0.03778479222124554
Loss in iteration 70 : 0.037384475664454075
Loss in iteration 71 : 0.03700332899245172
Loss in iteration 72 : 0.0366273430946223
Loss in iteration 73 : 0.03627299598454256
Loss in iteration 74 : 0.03594107364475744
Loss in iteration 75 : 0.035610258287025284
Loss in iteration 76 : 0.0352832661068118
Loss in iteration 77 : 0.03497779424055538
Testing accuracy  of updater 3 on alg 1 with rate 16.0 = 0.96, training accuracy 0.9949985710202915, time elapsed: 1817 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.7422716924164834
Loss in iteration 3 : 0.19664087492984836
Loss in iteration 4 : 0.15297200821977913
Loss in iteration 5 : 0.1263861272239749
Loss in iteration 6 : 0.10640526086204634
Loss in iteration 7 : 0.09082382031602769
Loss in iteration 8 : 0.07912191972428867
Loss in iteration 9 : 0.06932676350724332
Loss in iteration 10 : 0.061789621549219464
Loss in iteration 11 : 0.05571152820519427
Loss in iteration 12 : 0.05081824851720588
Loss in iteration 13 : 0.04710859400995508
Loss in iteration 14 : 0.04408662627251623
Loss in iteration 15 : 0.041302468789516864
Loss in iteration 16 : 0.038689472109335335
Loss in iteration 17 : 0.036376801618043644
Loss in iteration 18 : 0.034607094506628466
Loss in iteration 19 : 0.03308781541695614
Loss in iteration 20 : 0.031718812817495314
Loss in iteration 21 : 0.030460369582470937
Loss in iteration 22 : 0.029276663664534394
Loss in iteration 23 : 0.02827403105139112
Loss in iteration 24 : 0.02740668939801169
Loss in iteration 25 : 0.02657266060850525
Loss in iteration 26 : 0.025821654930223294
Loss in iteration 27 : 0.025138354945658015
Loss in iteration 28 : 0.024478554518723793
Loss in iteration 29 : 0.0238573760777965
Loss in iteration 30 : 0.02325434396542099
Loss in iteration 31 : 0.022666867833947456
Loss in iteration 32 : 0.02209516346660052
Loss in iteration 33 : 0.021563210515515806
Loss in iteration 34 : 0.021085489327375688
Loss in iteration 35 : 0.02061906480246145
Loss in iteration 36 : 0.020159529574393046
Loss in iteration 37 : 0.01971077791015779
Loss in iteration 38 : 0.019281205101858743
Loss in iteration 39 : 0.01888052881635742
Loss in iteration 40 : 0.018485178673349298
Loss in iteration 41 : 0.018095651490656166
Loss in iteration 42 : 0.01772033190975533
Loss in iteration 43 : 0.017363556510366144
Loss in iteration 44 : 0.017020328673978777
Loss in iteration 45 : 0.01668978876983137
Loss in iteration 46 : 0.01636383726138731
Loss in iteration 47 : 0.016058439537783922
Loss in iteration 48 : 0.015763098835681912
Loss in iteration 49 : 0.015473523278805294
Loss in iteration 50 : 0.015187734705753865
Loss in iteration 51 : 0.014912695879636168
Loss in iteration 52 : 0.014643149099221542
Loss in iteration 53 : 0.014382200428450528
Loss in iteration 54 : 0.014133416384126308
Loss in iteration 55 : 0.013892262798116237
Loss in iteration 56 : 0.013654625916904735
Loss in iteration 57 : 0.013421220012156992
Loss in iteration 58 : 0.013211156037366735
Loss in iteration 59 : 0.013017317315287557
Loss in iteration 60 : 0.012833961679865901
Loss in iteration 61 : 0.012659899370192133
Loss in iteration 62 : 0.0124881363084575
Loss in iteration 63 : 0.01231799646854545
Loss in iteration 64 : 0.012149236222691552
Loss in iteration 65 : 0.01198182326603024
Loss in iteration 66 : 0.011816191074365733
Loss in iteration 67 : 0.011660957500758573
Loss in iteration 68 : 0.011512429221272543
Loss in iteration 69 : 0.01136813918551794
Loss in iteration 70 : 0.011225788853139757
Loss in iteration 71 : 0.011090794863463694
Loss in iteration 72 : 0.010959612361336963
Loss in iteration 73 : 0.010831065098885295
Loss in iteration 74 : 0.01070341787615062
Loss in iteration 75 : 0.010577143737566635
Loss in iteration 76 : 0.010456718860276272
Loss in iteration 77 : 0.010337104337667245
Loss in iteration 78 : 0.010218284411083753
Loss in iteration 79 : 0.010100243827751463
Loss in iteration 80 : 0.009986271417317052
Loss in iteration 81 : 0.009877793773327904
Loss in iteration 82 : 0.009771285884784481
Loss in iteration 83 : 0.009668140354275444
Loss in iteration 84 : 0.009566124693456258
Loss in iteration 85 : 0.009464444921455572
Loss in iteration 86 : 0.00936345688622095
Loss in iteration 87 : 0.009266621220112554
Loss in iteration 88 : 0.009172259097033376
Loss in iteration 89 : 0.009078964504670075
Loss in iteration 90 : 0.008989398084422636
Testing accuracy  of updater 3 on alg 1 with rate 4.0 = 0.9617777777777777, training accuracy 0.9971420405830237, time elapsed: 1979 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.1388370832011163
Loss in iteration 3 : 0.1461884302251895
Loss in iteration 4 : 0.1163683720331888
Loss in iteration 5 : 0.0962311131146337
Loss in iteration 6 : 0.08204853473490117
Loss in iteration 7 : 0.07093046133211489
Loss in iteration 8 : 0.06184808328625673
Loss in iteration 9 : 0.05494561958438286
Loss in iteration 10 : 0.04919520798083201
Loss in iteration 11 : 0.04440607018533463
Loss in iteration 12 : 0.040321641582037375
Loss in iteration 13 : 0.03716228914520549
Loss in iteration 14 : 0.03474035262184113
Loss in iteration 15 : 0.03275830830688172
Loss in iteration 16 : 0.03107829478064302
Loss in iteration 17 : 0.029533602579577486
Loss in iteration 18 : 0.028092097393062235
Loss in iteration 19 : 0.026776623327010713
Loss in iteration 20 : 0.025638026439167398
Loss in iteration 21 : 0.024622562839192813
Loss in iteration 22 : 0.023736020055069557
Loss in iteration 23 : 0.022928339152737322
Loss in iteration 24 : 0.02217415331842666
Loss in iteration 25 : 0.02148645130374568
Loss in iteration 26 : 0.020866348924571957
Loss in iteration 27 : 0.020297787768280603
Loss in iteration 28 : 0.019749113217718498
Loss in iteration 29 : 0.019221334531033907
Loss in iteration 30 : 0.01873611336299751
Loss in iteration 31 : 0.018276219679755543
Loss in iteration 32 : 0.017841407835933163
Loss in iteration 33 : 0.017429385955465802
Loss in iteration 34 : 0.017046497081701428
Loss in iteration 35 : 0.016673085497273085
Loss in iteration 36 : 0.01631351399771855
Loss in iteration 37 : 0.015967679581420965
Loss in iteration 38 : 0.015637273796615985
Loss in iteration 39 : 0.015318757770238327
Loss in iteration 40 : 0.015010680950356346
Loss in iteration 41 : 0.014706944688748884
Loss in iteration 42 : 0.01441261324588582
Loss in iteration 43 : 0.014132401724406975
Loss in iteration 44 : 0.01386936878893849
Loss in iteration 45 : 0.01361004159967131
Loss in iteration 46 : 0.013355151563768184
Loss in iteration 47 : 0.013106540510961407
Loss in iteration 48 : 0.012870516159606755
Loss in iteration 49 : 0.01263984229167251
Loss in iteration 50 : 0.012414107849386798
Loss in iteration 51 : 0.012192556132540909
Loss in iteration 52 : 0.01198123657117524
Loss in iteration 53 : 0.011777226103087617
Loss in iteration 54 : 0.011575205152858632
Loss in iteration 55 : 0.01137511858345492
Loss in iteration 56 : 0.011177520478436068
Loss in iteration 57 : 0.010986776156757024
Loss in iteration 58 : 0.010802559480548667
Loss in iteration 59 : 0.010636639110949872
Loss in iteration 60 : 0.010476698375437259
Loss in iteration 61 : 0.01031752883356712
Loss in iteration 62 : 0.01016075172107287
Loss in iteration 63 : 0.010005060205736615
Loss in iteration 64 : 0.009852515065456459
Loss in iteration 65 : 0.009704125761676556
Loss in iteration 66 : 0.009556093175103796
Loss in iteration 67 : 0.009413181580945562
Loss in iteration 68 : 0.009274906423515993
Loss in iteration 69 : 0.009138131377056043
Loss in iteration 70 : 0.009004771425373353
Loss in iteration 71 : 0.008878318344877244
Loss in iteration 72 : 0.008752634023153258
Loss in iteration 73 : 0.008629873911735902
Loss in iteration 74 : 0.008507098019780683
Loss in iteration 75 : 0.008386592927235415
Loss in iteration 76 : 0.008269832686296698
Loss in iteration 77 : 0.008166288372633692
Loss in iteration 78 : 0.008068557948612685
Loss in iteration 79 : 0.007970593833634045
Loss in iteration 80 : 0.007877945140302633
Loss in iteration 81 : 0.0077900114119881314
Loss in iteration 82 : 0.0077023063734514155
Loss in iteration 83 : 0.007616184575926035
Loss in iteration 84 : 0.007531435634411216
Loss in iteration 85 : 0.007450227782913138
Loss in iteration 86 : 0.007368677452755397
Loss in iteration 87 : 0.00728840136013851
Loss in iteration 88 : 0.0072080274475300864
Loss in iteration 89 : 0.007129996317001756
Loss in iteration 90 : 0.007053371641741658
Loss in iteration 91 : 0.0069777176082813225
Loss in iteration 92 : 0.006902133669842418
Loss in iteration 93 : 0.006827070197168089
Loss in iteration 94 : 0.006752567866640862
Loss in iteration 95 : 0.006678936417754162
Loss in iteration 96 : 0.00660877106570214
Loss in iteration 97 : 0.006540914967571828
Loss in iteration 98 : 0.006473864395050285
Loss in iteration 99 : 0.0064084977014756345
Loss in iteration 100 : 0.006344838012137216
Loss in iteration 101 : 0.006281994442454666
Loss in iteration 102 : 0.006220809529603228
Loss in iteration 103 : 0.006161969189592016
Loss in iteration 104 : 0.006103568500045501
Loss in iteration 105 : 0.006045474294040668
Loss in iteration 106 : 0.005988317033433254
Loss in iteration 107 : 0.005935417227184365
Loss in iteration 108 : 0.00588430718896972
Testing accuracy  of updater 3 on alg 1 with rate 2.8 = 0.9617777777777777, training accuracy 0.9985710202915119, time elapsed: 2483 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.661299337492592
Loss in iteration 3 : 0.11282436569682383
Loss in iteration 4 : 0.08973696327330424
Loss in iteration 5 : 0.07588337981825559
Loss in iteration 6 : 0.06616998523043147
Loss in iteration 7 : 0.056621137032696923
Loss in iteration 8 : 0.05039651493344746
Loss in iteration 9 : 0.04468231735424101
Loss in iteration 10 : 0.0402458320858763
Loss in iteration 11 : 0.036396594292646106
Loss in iteration 12 : 0.033451989205674236
Loss in iteration 13 : 0.031040218023294132
Loss in iteration 14 : 0.028954213017436732
Loss in iteration 15 : 0.027283658203503317
Loss in iteration 16 : 0.025852632267820263
Loss in iteration 17 : 0.024751125635101907
Loss in iteration 18 : 0.023789078538448923
Loss in iteration 19 : 0.022905529479389973
Loss in iteration 20 : 0.022069652473597778
Loss in iteration 21 : 0.021278993863090883
Loss in iteration 22 : 0.020520654627344027
Loss in iteration 23 : 0.01982749352625737
Loss in iteration 24 : 0.01923386358640288
Loss in iteration 25 : 0.018694471561331593
Loss in iteration 26 : 0.018184806562383825
Loss in iteration 27 : 0.01769735598020212
Loss in iteration 28 : 0.017228643325838424
Loss in iteration 29 : 0.016785921656756488
Loss in iteration 30 : 0.01635849921171336
Loss in iteration 31 : 0.01596301740542057
Loss in iteration 32 : 0.01561027004030188
Loss in iteration 33 : 0.015282189962583261
Loss in iteration 34 : 0.014965059628637761
Loss in iteration 35 : 0.014665435381099027
Loss in iteration 36 : 0.014386954908197302
Loss in iteration 37 : 0.01411895150872337
Loss in iteration 38 : 0.013860201185065674
Loss in iteration 39 : 0.01361458579574943
Loss in iteration 40 : 0.013381401405308075
Loss in iteration 41 : 0.013164393398269445
Loss in iteration 42 : 0.012960490658939028
Loss in iteration 43 : 0.012767496255042265
Loss in iteration 44 : 0.012580609753773701
Loss in iteration 45 : 0.012399973713821354
Loss in iteration 46 : 0.012221405967738035
Loss in iteration 47 : 0.012045851273493894
Loss in iteration 48 : 0.011873410800582466
Loss in iteration 49 : 0.0117042298568876
Loss in iteration 50 : 0.011536510776563287
Loss in iteration 51 : 0.01137140040854827
Loss in iteration 52 : 0.011209817015491279
Loss in iteration 53 : 0.011051313133977789
Loss in iteration 54 : 0.010895316335291493
Loss in iteration 55 : 0.010742051786707285
Loss in iteration 56 : 0.010590220221902794
Loss in iteration 57 : 0.010439785062846436
Loss in iteration 58 : 0.01029136193135968
Loss in iteration 59 : 0.010148842946464832
Loss in iteration 60 : 0.010013102056234905
Loss in iteration 61 : 0.009885724028348017
Loss in iteration 62 : 0.009757789630310449
Loss in iteration 63 : 0.009636471862300365
Loss in iteration 64 : 0.009517539102215741
Loss in iteration 65 : 0.009399835165881314
Loss in iteration 66 : 0.009284611076511356
Loss in iteration 67 : 0.009172583764519973
Loss in iteration 68 : 0.009062067165600578
Loss in iteration 69 : 0.00895263361687336
Loss in iteration 70 : 0.0088476770683656
Loss in iteration 71 : 0.008743284680054952
Loss in iteration 72 : 0.008639950514007596
Loss in iteration 73 : 0.00853772319042715
Loss in iteration 74 : 0.00843611181347249
Loss in iteration 75 : 0.008335277388749972
Loss in iteration 76 : 0.008235111915678767
Loss in iteration 77 : 0.008136086027815534
Loss in iteration 78 : 0.00804108801305493
Loss in iteration 79 : 0.007946689295354348
Loss in iteration 80 : 0.00785356659329497
Loss in iteration 81 : 0.007765100170374567
Loss in iteration 82 : 0.0076830468748411715
Loss in iteration 83 : 0.00760207981424289
Loss in iteration 84 : 0.007519616852358643
Loss in iteration 85 : 0.007440680951569872
Loss in iteration 86 : 0.007363117053422403
Loss in iteration 87 : 0.007284221758005475
Loss in iteration 88 : 0.00721125990236971
Loss in iteration 89 : 0.007139125347987338
Loss in iteration 90 : 0.007070896782706396
Loss in iteration 91 : 0.0069990870953560045
Loss in iteration 92 : 0.006929996870718707
Loss in iteration 93 : 0.006870186967912977
Loss in iteration 94 : 0.006807172358131667
Loss in iteration 95 : 0.00674666969948484
Loss in iteration 96 : 0.006690214276322717
Loss in iteration 97 : 0.006635054386296217
Loss in iteration 98 : 0.006582200137552611
Loss in iteration 99 : 0.00653039617063545
Loss in iteration 100 : 0.006478612361881617
Loss in iteration 101 : 0.006427748412877935
Loss in iteration 102 : 0.006376941833093649
Loss in iteration 103 : 0.006327015029561232
Loss in iteration 104 : 0.006280269922604056
Loss in iteration 105 : 0.006234091677567359
Loss in iteration 106 : 0.0061888382113664386
Loss in iteration 107 : 0.006143127934904737
Loss in iteration 108 : 0.006098297656626923
Loss in iteration 109 : 0.00605314954437274
Loss in iteration 110 : 0.006008195839130443
Loss in iteration 111 : 0.005964392199018054
Loss in iteration 112 : 0.005920079001132582
Loss in iteration 113 : 0.005875925923765929
Loss in iteration 114 : 0.005832776165597105
Loss in iteration 115 : 0.005789024321853036
Loss in iteration 116 : 0.005746697381877912
Loss in iteration 117 : 0.0057054012317465826
Loss in iteration 118 : 0.005663897467665929
Loss in iteration 119 : 0.005623647346000037
Loss in iteration 120 : 0.0055832344872019015
Loss in iteration 121 : 0.0055434066853891064
Loss in iteration 122 : 0.005503750183062185
Loss in iteration 123 : 0.005464262884575481
Loss in iteration 124 : 0.005424942736814407
Loss in iteration 125 : 0.005385787727994674
Loss in iteration 126 : 0.005346795886504752
Loss in iteration 127 : 0.0053079652797897395
Loss in iteration 128 : 0.005269294013274804
Loss in iteration 129 : 0.0052308752387622715
Loss in iteration 130 : 0.005193157010855535
Loss in iteration 131 : 0.005155107084161978
Loss in iteration 132 : 0.005117494109242964
Loss in iteration 133 : 0.005080060705446966
Loss in iteration 134 : 0.005042774233936243
Loss in iteration 135 : 0.00500563305876321
Loss in iteration 136 : 0.004968635574216223
Loss in iteration 137 : 0.004931780204041511
Loss in iteration 138 : 0.004895070922829648
Loss in iteration 139 : 0.004858723217945671
Loss in iteration 140 : 0.00482240578892363
Loss in iteration 141 : 0.004786140885041982
Loss in iteration 142 : 0.00475078819574309
Loss in iteration 143 : 0.004714529570251058
Loss in iteration 144 : 0.004678947700343923
Loss in iteration 145 : 0.004643949723236801
Loss in iteration 146 : 0.004608712907024091
Loss in iteration 147 : 0.004573543959550166
Loss in iteration 148 : 0.004538597440777052
Loss in iteration 149 : 0.004504331847533677
Testing accuracy  of updater 3 on alg 1 with rate 1.6 = 0.976, training accuracy 0.9989997142040583, time elapsed: 2777 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3537007251979503
Loss in iteration 3 : 0.831474503856482
Loss in iteration 4 : 0.2391472147177154
Loss in iteration 5 : 0.17909030409360843
Loss in iteration 6 : 0.1530519637750358
Loss in iteration 7 : 0.1380632495994642
Loss in iteration 8 : 0.12565732283943362
Loss in iteration 9 : 0.11523085831960256
Loss in iteration 10 : 0.10643934081366312
Loss in iteration 11 : 0.09904590848996571
Loss in iteration 12 : 0.09280649592565773
Loss in iteration 13 : 0.0874880481625582
Loss in iteration 14 : 0.08301943600816948
Loss in iteration 15 : 0.0791251284888548
Loss in iteration 16 : 0.07558288447385793
Loss in iteration 17 : 0.07248933206231166
Loss in iteration 18 : 0.06969988345754577
Loss in iteration 19 : 0.0672507254476966
Loss in iteration 20 : 0.06504935461211393
Loss in iteration 21 : 0.06299880596469869
Loss in iteration 22 : 0.06108065802345554
Loss in iteration 23 : 0.05933919533695968
Loss in iteration 24 : 0.05778715506519556
Loss in iteration 25 : 0.056355163380472875
Loss in iteration 26 : 0.05504585330671653
Loss in iteration 27 : 0.05381562954509005
Loss in iteration 28 : 0.052660071158573796
Loss in iteration 29 : 0.05158199005417885
Loss in iteration 30 : 0.050572211644371715
Loss in iteration 31 : 0.04961494002657831
Loss in iteration 32 : 0.048705451468202494
Loss in iteration 33 : 0.04784572258562001
Loss in iteration 34 : 0.047025030642019315
Loss in iteration 35 : 0.04624596941730671
Loss in iteration 36 : 0.04549527777329462
Loss in iteration 37 : 0.04476632835221133
Loss in iteration 38 : 0.04406424191208689
Loss in iteration 39 : 0.043391897151883345
Loss in iteration 40 : 0.04275146705179652
Loss in iteration 41 : 0.04213593491211493
Loss in iteration 42 : 0.041554827095444495
Loss in iteration 43 : 0.04100343003176416
Loss in iteration 44 : 0.040463330148857043
Loss in iteration 45 : 0.039934628757116966
Loss in iteration 46 : 0.03942013018648325
Loss in iteration 47 : 0.038917519068690266
Loss in iteration 48 : 0.03842465835409772
Loss in iteration 49 : 0.0379489671321745
Loss in iteration 50 : 0.03748733658025511
Loss in iteration 51 : 0.03704441282850808
Loss in iteration 52 : 0.03660995828084025
Loss in iteration 53 : 0.03618257383141944
Loss in iteration 54 : 0.03576861734018844
Loss in iteration 55 : 0.03536377782887075
Loss in iteration 56 : 0.03496775200933049
Loss in iteration 57 : 0.03457750336503794
Loss in iteration 58 : 0.03419449773610015
Loss in iteration 59 : 0.03382118718595247
Loss in iteration 60 : 0.03345994826289756
Loss in iteration 61 : 0.03310534017403681
Loss in iteration 62 : 0.03275633828801221
Loss in iteration 63 : 0.032417422693849646
Loss in iteration 64 : 0.03208745734390509
Loss in iteration 65 : 0.03176449357057905
Loss in iteration 66 : 0.03144646053858884
Loss in iteration 67 : 0.03113315649781233
Loss in iteration 68 : 0.03082346409864889
Loss in iteration 69 : 0.030517498632680797
Loss in iteration 70 : 0.030216828051716153
Loss in iteration 71 : 0.02992565511197883
Loss in iteration 72 : 0.029642285054054498
Loss in iteration 73 : 0.029362978402657728
Loss in iteration 74 : 0.029087637040864533
Loss in iteration 75 : 0.028818953241755776
Loss in iteration 76 : 0.0285572195734024
Loss in iteration 77 : 0.02830079209411309
Loss in iteration 78 : 0.028052139204972473
Loss in iteration 79 : 0.027809521243934498
Loss in iteration 80 : 0.0275731395203004
Loss in iteration 81 : 0.027342607990114124
Loss in iteration 82 : 0.02711777408906005
Loss in iteration 83 : 0.026898225697700843
Loss in iteration 84 : 0.02668210420789241
Loss in iteration 85 : 0.02646943937842179
Loss in iteration 86 : 0.0262616367620621
Loss in iteration 87 : 0.026060160446573534
Loss in iteration 88 : 0.025868913561259158
Loss in iteration 89 : 0.025683431141440324
Loss in iteration 90 : 0.025501523188883966
Loss in iteration 91 : 0.025323227181527762
Loss in iteration 92 : 0.02514896387234565
Loss in iteration 93 : 0.024978415109673448
Loss in iteration 94 : 0.024812623071351587
Loss in iteration 95 : 0.024652281709575844
Loss in iteration 96 : 0.024494872659639635
Loss in iteration 97 : 0.024340061545416892
Loss in iteration 98 : 0.024187494404759424
Loss in iteration 99 : 0.024037788205788
Loss in iteration 100 : 0.0238908681712397
Loss in iteration 101 : 0.023746738866951948
Loss in iteration 102 : 0.023604928588941802
Loss in iteration 103 : 0.023465503900705138
Loss in iteration 104 : 0.023328567865318433
Loss in iteration 105 : 0.023195397729099274
Loss in iteration 106 : 0.023064690469521608
Loss in iteration 107 : 0.022935450782582443
Loss in iteration 108 : 0.02280834552400778
Loss in iteration 109 : 0.022682462028894533
Loss in iteration 110 : 0.022559614691567056
Loss in iteration 111 : 0.022439171098896267
Loss in iteration 112 : 0.02231948602926326
Loss in iteration 113 : 0.022200696885159292
Loss in iteration 114 : 0.022084571891063005
Loss in iteration 115 : 0.021970864795836813
Loss in iteration 116 : 0.021858535517292558
Loss in iteration 117 : 0.021748478108943384
Loss in iteration 118 : 0.02164041233332039
Loss in iteration 119 : 0.021533337363124627
Loss in iteration 120 : 0.021427243607523495
Loss in iteration 121 : 0.02132240937802922
Loss in iteration 122 : 0.021218765047049296
Loss in iteration 123 : 0.021116544425394526
Loss in iteration 124 : 0.021015223870346832
Loss in iteration 125 : 0.020915356835551922
Loss in iteration 126 : 0.020816599172810667
Loss in iteration 127 : 0.02071820763845914
Loss in iteration 128 : 0.020620834047649823
Loss in iteration 129 : 0.02052581827473881
Loss in iteration 130 : 0.02043165152064075
Loss in iteration 131 : 0.020338071318246904
Loss in iteration 132 : 0.020246828569467683
Loss in iteration 133 : 0.02015680496670895
Loss in iteration 134 : 0.020067607447845928
Loss in iteration 135 : 0.019978957326109995
Loss in iteration 136 : 0.019891781872105798
Loss in iteration 137 : 0.019805909670397352
Loss in iteration 138 : 0.019721317168733286
Loss in iteration 139 : 0.019638761311645398
Loss in iteration 140 : 0.01955790523990055
Loss in iteration 141 : 0.019478688882172763
Loss in iteration 142 : 0.019400792967065237
Loss in iteration 143 : 0.019323762919330818
Loss in iteration 144 : 0.019247708281880462
Loss in iteration 145 : 0.019172364993406275
Loss in iteration 146 : 0.019098019961039766
Loss in iteration 147 : 0.019024565017973248
Loss in iteration 148 : 0.01895189081666887
Loss in iteration 149 : 0.01887955532089388
Loss in iteration 150 : 0.018808076843635366
Loss in iteration 151 : 0.01873703239947022
Loss in iteration 152 : 0.01866635637414476
Loss in iteration 153 : 0.018596395586904582
Loss in iteration 154 : 0.018527234322447392
Loss in iteration 155 : 0.01845873012645099
Loss in iteration 156 : 0.018390665433769226
Loss in iteration 157 : 0.01832325472405201
Loss in iteration 158 : 0.018256567620998067
Loss in iteration 159 : 0.018190304724221926
Loss in iteration 160 : 0.018124913887069653
Testing accuracy  of updater 3 on alg 1 with rate 0.3999999999999999 = 0.9857777777777778, training accuracy 0.9985710202915119, time elapsed: 2972 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610751
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540818
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778983
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543957
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209122
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160542
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987295
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743325
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712587
Loss in iteration 67 : 0.8027762269894211
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940244
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454701
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134936
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256582
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955337
Loss in iteration 110 : 0.6516008788407875
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154833
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748412
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036678
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234755
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.5018535958418039
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.48627209050369236
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575792
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.3995523073557874
Loss in iteration 187 : 0.3962532440560076
Loss in iteration 188 : 0.39295876891020853
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.3736917204558962
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 10000.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 4163 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614217
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311626
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433591
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.9341560838258172
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310218
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512602
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418897
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651215
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.861181509448161
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142405
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249812
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427347
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088157
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955006
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505614
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001563
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407875
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934181
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294274
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892694
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234755
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.5018535958418039
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786574
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550484
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.43866690280212967
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614924984
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.41596146823413105
Loss in iteration 182 : 0.41269199476764656
Loss in iteration 183 : 0.4094159654317907
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.3995523073557874
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 7000.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 4345 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845752
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610751
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807446
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826744
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651215
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209122
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142405
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630267
Loss in iteration 60 : 0.8258473575743325
Loss in iteration 61 : 0.8225790333857054
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249812
Loss in iteration 66 : 0.8060995208712588
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955006
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702151
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134936
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001563
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256584
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955337
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146481
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748412
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137508
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234753
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.5018535958418039
Loss in iteration 155 : 0.49874963817009316
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472434
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.4610645411786028
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.454696771363955
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575792
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.4354425326891861
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.4224809016575409
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.41596146823413105
Loss in iteration 182 : 0.41269199476764656
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.3995523073557874
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 4000.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 4020 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610751
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614215
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468235
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037803
Loss in iteration 19 : 0.9514417630540818
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.9312352176477111
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694353
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793263
Loss in iteration 43 : 0.8799678200651215
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209122
Loss in iteration 51 : 0.8548422260830087
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160542
Loss in iteration 54 : 0.8452619046243425
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454701
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626255
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134936
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256584
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955337
Loss in iteration 110 : 0.6516008788407875
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934181
Loss in iteration 118 : 0.6217649050146481
Loss in iteration 119 : 0.6179991561294274
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423256
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036678
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234752
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.5018535958418039
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550484
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.4450968060893727
Loss in iteration 173 : 0.4418848318575792
Loss in iteration 174 : 0.43866690280212967
Loss in iteration 175 : 0.43544253268918603
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.4094159654317907
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.39295876891020853
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 3974 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468235
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778983
Loss in iteration 22 : 0.9428512337433591
Loss in iteration 23 : 0.9399642391807446
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.9341560838258172
Loss in iteration 26 : 0.9312352176477111
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694353
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725948
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793263
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209122
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160542
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142405
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987295
Loss in iteration 59 : 0.8291064249630267
Loss in iteration 60 : 0.8258473575743325
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712588
Loss in iteration 67 : 0.8027762269894211
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652766
Loss in iteration 71 : 0.7893926071299427
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454701
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626255
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001563
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753548
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372487
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955337
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146481
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782388
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954033
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979853
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986777
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234755
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.5018535958418039
Loss in iteration 155 : 0.49874963817009316
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.48627209050369236
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472434
Loss in iteration 162 : 0.4768664044786574
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.454696771363955
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.3962532440560076
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.3832241331850308
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 3775 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302796
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610751
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468235
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311625
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433591
Loss in iteration 23 : 0.9399642391807446
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.9341560838258172
Loss in iteration 26 : 0.9312352176477111
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310218
Loss in iteration 30 : 0.9194426884487195
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874446
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793263
Loss in iteration 43 : 0.8799678200651215
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160542
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142405
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743325
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088157
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955006
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702151
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626255
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166519
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934181
Loss in iteration 118 : 0.6217649050146481
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423256
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986777
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265787
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296424
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234753
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.5018535958418039
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.48627209050369236
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472434
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550484
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.4610645411786028
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.454696771363955
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575792
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918603
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.4257311461492499
Loss in iteration 179 : 0.4224809016575409
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589605
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 4330 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433591
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487191
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826744
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784296
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651215
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987295
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223987
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249812
Loss in iteration 66 : 0.8060995208712587
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088157
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955006
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702151
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626255
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134936
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001563
Loss in iteration 97 : 0.6989667568166519
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825358
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955337
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.6404730705088589
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954033
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423256
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.5142439889004201
Loss in iteration 151 : 0.5111414489234755
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472434
Loss in iteration 162 : 0.4768664044786574
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550484
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.4450968060893727
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.4354425326891861
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.4224809016575409
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 4096 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 15.377994222861098
Loss in iteration 3 : 1.647046438192474
Loss in iteration 4 : 0.9440938565157319
Loss in iteration 5 : 0.7157729795740931
Loss in iteration 6 : 0.5883428088665248
Loss in iteration 7 : 0.4844482166294344
Loss in iteration 8 : 0.39370371866868015
Loss in iteration 9 : 0.3300612213946376
Loss in iteration 10 : 0.28652817382173157
Loss in iteration 11 : 0.25455555088919174
Loss in iteration 12 : 0.22839347066536725
Loss in iteration 13 : 0.20678893209503743
Loss in iteration 14 : 0.18724867843185689
Loss in iteration 15 : 0.16965709341411656
Loss in iteration 16 : 0.1545986247627726
Loss in iteration 17 : 0.1422940310254581
Loss in iteration 18 : 0.1315742518644409
Loss in iteration 19 : 0.12200499514285479
Loss in iteration 20 : 0.11330316697859914
Loss in iteration 21 : 0.10514990318396648
Loss in iteration 22 : 0.09769245977543925
Loss in iteration 23 : 0.09081749749931738
Loss in iteration 24 : 0.08446915595842769
Loss in iteration 25 : 0.07846699396977048
Loss in iteration 26 : 0.07304797186654362
Loss in iteration 27 : 0.06812813303323707
Loss in iteration 28 : 0.06328173910612699
Loss in iteration 29 : 0.059074349114840816
Loss in iteration 30 : 0.05513388158026569
Loss in iteration 31 : 0.051910377700721586
Loss in iteration 32 : 0.04901206775099086
Loss in iteration 33 : 0.046338573687700335
Loss in iteration 34 : 0.044220728051561735
Loss in iteration 35 : 0.04229649201983224
Loss in iteration 36 : 0.040693915669901166
Loss in iteration 37 : 0.03912028630615197
Loss in iteration 38 : 0.03745798218845186
Loss in iteration 39 : 0.035803923262875584
Loss in iteration 40 : 0.034370537851687595
Loss in iteration 41 : 0.03309633170781551
Loss in iteration 42 : 0.031771604665855276
Loss in iteration 43 : 0.030552588830446155
Loss in iteration 44 : 0.029269794040221602
Loss in iteration 45 : 0.027942501557872552
Loss in iteration 46 : 0.026593867091801757
Loss in iteration 47 : 0.02517586506298171
Loss in iteration 48 : 0.02383919540827855
Loss in iteration 49 : 0.022716524905861797
Loss in iteration 50 : 0.021657655262017912
Loss in iteration 51 : 0.02053961631648885
Loss in iteration 52 : 0.01944244359023809
Loss in iteration 53 : 0.018410564213426007
Loss in iteration 54 : 0.017887216333609315
Loss in iteration 55 : 0.016818472808580752
Loss in iteration 56 : 0.016445982948950298
Loss in iteration 57 : 0.01565168716054931
Loss in iteration 58 : 0.014989634582606165
Loss in iteration 59 : 0.014349037666660952
Loss in iteration 60 : 0.013686462822742846
Loss in iteration 61 : 0.01306038110582997
Loss in iteration 62 : 0.01249605783268384
Loss in iteration 63 : 0.011964853396971625
Loss in iteration 64 : 0.011396607792979362
Loss in iteration 65 : 0.010803127421674929
Loss in iteration 66 : 0.010508405322719086
Loss in iteration 67 : 0.009351108216621001
Loss in iteration 68 : 0.008395901745596776
Loss in iteration 69 : 0.0076635856432841765
Loss in iteration 70 : 0.007045475752118503
Loss in iteration 71 : 0.006350876503826335
Loss in iteration 72 : 0.005802419137159335
Loss in iteration 73 : 0.005686539229885053
Loss in iteration 74 : 0.00579573284327981
Loss in iteration 75 : 0.0037049930141245524
Loss in iteration 76 : 0.0030062688592969737
Loss in iteration 77 : 0.002577309682284528
Loss in iteration 78 : 0.0011773194302855479
Loss in iteration 79 : 6.576878206149525E-4
Loss in iteration 80 : 5.960506220918005E-4
Loss in iteration 81 : 8.368357638625852E-4
Loss in iteration 82 : 0.0035037500454195574
Loss in iteration 83 : 3.489208697739343E-4
Loss in iteration 84 : 6.938472043498217E-4
Loss in iteration 85 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 4.0 = 0.992, training accuracy 1.0, time elapsed: 1823 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 10.801594996056462
Loss in iteration 3 : 1.170355851941571
Loss in iteration 4 : 0.6722667833411635
Loss in iteration 5 : 0.5090114250466525
Loss in iteration 6 : 0.41817709542354164
Loss in iteration 7 : 0.3456299100287148
Loss in iteration 8 : 0.28140327878546356
Loss in iteration 9 : 0.23591808877769382
Loss in iteration 10 : 0.20428035370362546
Loss in iteration 11 : 0.18173688810783017
Loss in iteration 12 : 0.1632051392305314
Loss in iteration 13 : 0.14758358627712154
Loss in iteration 14 : 0.13397110635836287
Loss in iteration 15 : 0.12157542148520471
Loss in iteration 16 : 0.1107742333443074
Loss in iteration 17 : 0.10206717651735601
Loss in iteration 18 : 0.09421018154317792
Loss in iteration 19 : 0.08724967100601419
Loss in iteration 20 : 0.08082085686622707
Loss in iteration 21 : 0.07509023994531659
Loss in iteration 22 : 0.06967426808598896
Loss in iteration 23 : 0.06475862794110326
Loss in iteration 24 : 0.060231012870082465
Loss in iteration 25 : 0.055946893541828954
Loss in iteration 26 : 0.05207954892382951
Loss in iteration 27 : 0.048567401265586244
Loss in iteration 28 : 0.04519760702982026
Loss in iteration 29 : 0.04219012945922742
Loss in iteration 30 : 0.039358365690125435
Loss in iteration 31 : 0.036960304171084805
Loss in iteration 32 : 0.03496737756307957
Loss in iteration 33 : 0.033008739333275285
Loss in iteration 34 : 0.03142889838343619
Loss in iteration 35 : 0.030007077820668786
Loss in iteration 36 : 0.028777227584607244
Loss in iteration 37 : 0.02760948589964539
Loss in iteration 38 : 0.026481669850660427
Loss in iteration 39 : 0.025292766529197586
Loss in iteration 40 : 0.024376344863198056
Loss in iteration 41 : 0.023360056392301487
Loss in iteration 42 : 0.022421661797764787
Loss in iteration 43 : 0.021528956201688543
Loss in iteration 44 : 0.020631250666880395
Loss in iteration 45 : 0.019704068767740588
Loss in iteration 46 : 0.018751525712418577
Loss in iteration 47 : 0.017770275940235173
Loss in iteration 48 : 0.016872809051477828
Loss in iteration 49 : 0.016118496725912562
Loss in iteration 50 : 0.015371771148125551
Loss in iteration 51 : 0.014606661366016745
Loss in iteration 52 : 0.01392255115951038
Loss in iteration 53 : 0.013182215719428004
Loss in iteration 54 : 0.012634155005005636
Loss in iteration 55 : 0.012042233437302142
Loss in iteration 56 : 0.01161877959372149
Loss in iteration 57 : 0.011201319855227461
Loss in iteration 58 : 0.010693382170924859
Loss in iteration 59 : 0.010245783248766014
Loss in iteration 60 : 0.009794941640994755
Loss in iteration 61 : 0.009373548398608617
Loss in iteration 62 : 0.009046948358141578
Loss in iteration 63 : 0.008557001861018142
Loss in iteration 64 : 0.008019417682893626
Loss in iteration 65 : 0.0074545526790608845
Loss in iteration 66 : 0.006937918475908676
Loss in iteration 67 : 0.006546601433530831
Loss in iteration 68 : 0.0061143903497599275
Loss in iteration 69 : 0.005882522637449921
Loss in iteration 70 : 0.008595850513158384
Loss in iteration 71 : 0.010538806293798941
Loss in iteration 72 : 0.004860190970405926
Loss in iteration 73 : 0.0040222756882136775
Loss in iteration 74 : 0.0032174345331639396
Loss in iteration 75 : 0.002695661697300534
Loss in iteration 76 : 0.0023271988270984257
Loss in iteration 77 : 0.001902318839326493
Loss in iteration 78 : 0.0011667568754499264
Loss in iteration 79 : 5.570376743931541E-4
Loss in iteration 80 : 8.529835302980835E-4
Loss in iteration 81 : 7.274787702372516E-4
Loss in iteration 82 : 9.414806233227213E-4
Loss in iteration 83 : 1.98666857167192E-4
Loss in iteration 84 : 2.164180369615786E-4
Loss in iteration 85 : 2.7525148207090004E-5
Loss in iteration 86 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 2.8000000000000003 = 0.992, training accuracy 1.0, time elapsed: 1936 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 6.2473636395813035
Loss in iteration 3 : 0.7002511076254833
Loss in iteration 4 : 0.40365631585521156
Loss in iteration 5 : 0.3062631971218696
Loss in iteration 6 : 0.25145662172808414
Loss in iteration 7 : 0.20885799909251357
Loss in iteration 8 : 0.17136866761469405
Loss in iteration 9 : 0.1426562728569724
Loss in iteration 10 : 0.12296486953152734
Loss in iteration 11 : 0.10881231148441067
Loss in iteration 12 : 0.09776063880124335
Loss in iteration 13 : 0.08858789705775826
Loss in iteration 14 : 0.08064013224912318
Loss in iteration 15 : 0.0736250455836593
Loss in iteration 16 : 0.06738333858049635
Loss in iteration 17 : 0.0620943050173178
Loss in iteration 18 : 0.05722340723300136
Loss in iteration 19 : 0.052809241441198265
Loss in iteration 20 : 0.048820073211252445
Loss in iteration 21 : 0.04540341057397439
Loss in iteration 22 : 0.04215988582064826
Loss in iteration 23 : 0.03921420612107083
Loss in iteration 24 : 0.036471238626216464
Loss in iteration 25 : 0.03395902742511311
Loss in iteration 26 : 0.03150104025255331
Loss in iteration 27 : 0.02930247695087787
Loss in iteration 28 : 0.027272244128931006
Loss in iteration 29 : 0.025439599866079167
Loss in iteration 30 : 0.02371072564021327
Loss in iteration 31 : 0.022165155889530105
Loss in iteration 32 : 0.020804304251410832
Loss in iteration 33 : 0.019688946881349905
Loss in iteration 34 : 0.01869347508231673
Loss in iteration 35 : 0.01777067216435748
Loss in iteration 36 : 0.017000840027352537
Loss in iteration 37 : 0.016343974960763345
Loss in iteration 38 : 0.015668183843607256
Loss in iteration 39 : 0.015000419914951537
Loss in iteration 40 : 0.01433698243504187
Loss in iteration 41 : 0.013840293266724283
Loss in iteration 42 : 0.013253461265663926
Loss in iteration 43 : 0.012703024431682454
Loss in iteration 44 : 0.012183204887308573
Loss in iteration 45 : 0.011651906821586266
Loss in iteration 46 : 0.01111206135842739
Loss in iteration 47 : 0.010565876260550045
Loss in iteration 48 : 0.010047974855856442
Loss in iteration 49 : 0.009632608945510218
Loss in iteration 50 : 0.009206754525148676
Loss in iteration 51 : 0.008767435403632586
Loss in iteration 52 : 0.008366273072529306
Loss in iteration 53 : 0.007946181437720929
Loss in iteration 54 : 0.007530281355092831
Loss in iteration 55 : 0.007116395905147266
Loss in iteration 56 : 0.0069903173002883985
Loss in iteration 57 : 0.006781863488279044
Loss in iteration 58 : 0.0064619908152166615
Loss in iteration 59 : 0.0061721026739466394
Loss in iteration 60 : 0.005878830788659885
Loss in iteration 61 : 0.005626370090918688
Loss in iteration 62 : 0.005344606279354698
Loss in iteration 63 : 0.005089623381472231
Loss in iteration 64 : 0.004819205972344783
Loss in iteration 65 : 0.004536148710701566
Loss in iteration 66 : 0.004222143465225402
Loss in iteration 67 : 0.004050484222044942
Loss in iteration 68 : 0.004381627326548083
Loss in iteration 69 : 0.0049424234551170634
Loss in iteration 70 : 0.006755182219739958
Loss in iteration 71 : 0.0033116805182296025
Loss in iteration 72 : 0.002783526068064592
Loss in iteration 73 : 0.002490134754685308
Loss in iteration 74 : 0.002232486661671939
Loss in iteration 75 : 0.0018697176351226959
Loss in iteration 76 : 0.0015382486280721586
Loss in iteration 77 : 0.0011980160072888033
Loss in iteration 78 : 8.822341909137065E-4
Loss in iteration 79 : 5.725250594697887E-4
Loss in iteration 80 : 5.368441474687221E-4
Loss in iteration 81 : 8.746260423619678E-4
Loss in iteration 82 : 4.4769964884820994E-4
Loss in iteration 83 : 6.106632428719214E-4
Loss in iteration 84 : 3.1892804980863053E-5
Loss in iteration 85 : 4.490016578077538E-5
Loss in iteration 86 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 1.6 = 0.992, training accuracy 1.0, time elapsed: 1783 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.6928559083116743
Loss in iteration 3 : 0.23929377104327118
Loss in iteration 4 : 0.13990811000298797
Loss in iteration 5 : 0.1034973439220518
Loss in iteration 6 : 0.0866783855298673
Loss in iteration 7 : 0.07391282653024357
Loss in iteration 8 : 0.06375800754024653
Loss in iteration 9 : 0.05468709341549938
Loss in iteration 10 : 0.04700658583697989
Loss in iteration 11 : 0.04158928405496916
Loss in iteration 12 : 0.03708810902221391
Loss in iteration 13 : 0.033745269320422384
Loss in iteration 14 : 0.030966314791621465
Loss in iteration 15 : 0.028504306903401385
Loss in iteration 16 : 0.026343621629850537
Loss in iteration 17 : 0.024413182414600783
Loss in iteration 18 : 0.02276390103194118
Loss in iteration 19 : 0.021165352821792497
Loss in iteration 20 : 0.019632402937751394
Loss in iteration 21 : 0.018212464973407453
Loss in iteration 22 : 0.01692186897557219
Loss in iteration 23 : 0.015683566928728486
Loss in iteration 24 : 0.014522061411671084
Loss in iteration 25 : 0.013371097575578004
Loss in iteration 26 : 0.012282599075527163
Loss in iteration 27 : 0.01124351217263139
Loss in iteration 28 : 0.010308201606902427
Loss in iteration 29 : 0.009484377424046625
Loss in iteration 30 : 0.008837591839769286
Loss in iteration 31 : 0.008205011719654357
Loss in iteration 32 : 0.007609757043124761
Loss in iteration 33 : 0.007037872558796994
Loss in iteration 34 : 0.006542098938119646
Loss in iteration 35 : 0.006093486973899861
Loss in iteration 36 : 0.0057267635381704676
Loss in iteration 37 : 0.00541338767177479
Loss in iteration 38 : 0.005154007019763584
Loss in iteration 39 : 0.004871823097395666
Loss in iteration 40 : 0.004619948519626603
Loss in iteration 41 : 0.004378752223662767
Loss in iteration 42 : 0.004152427599517469
Loss in iteration 43 : 0.003960160402023984
Loss in iteration 44 : 0.003766821160981895
Loss in iteration 45 : 0.0035918233065530283
Loss in iteration 46 : 0.0034165212537974952
Loss in iteration 47 : 0.003303725194939901
Loss in iteration 48 : 0.0031733886607779906
Loss in iteration 49 : 0.0030757393697751836
Loss in iteration 50 : 0.0029989029860928848
Loss in iteration 51 : 0.002897419795101888
Loss in iteration 52 : 0.0028029522919067297
Loss in iteration 53 : 0.0026842622651466223
Loss in iteration 54 : 0.0025931686480184703
Loss in iteration 55 : 0.0024954898213362196
Loss in iteration 56 : 0.002426967911479137
Loss in iteration 57 : 0.002344957075011635
Loss in iteration 58 : 0.002251546775481581
Loss in iteration 59 : 0.002172520622245889
Loss in iteration 60 : 0.002126412801851083
Loss in iteration 61 : 0.0020516296626338503
Loss in iteration 62 : 0.002204306523824293
Loss in iteration 63 : 0.002782826585731144
Loss in iteration 64 : 0.00955850226957649
Loss in iteration 65 : 0.008618126021217273
Loss in iteration 66 : 0.0028004856623138345
Loss in iteration 67 : 0.0019186285661330763
Loss in iteration 68 : 0.001733669224280581
Loss in iteration 69 : 0.0016733926078304209
Loss in iteration 70 : 0.0016107715081939355
Loss in iteration 71 : 0.0015523438182848562
Loss in iteration 72 : 0.0015039770315833107
Loss in iteration 73 : 0.001477309601663916
Loss in iteration 74 : 0.0014179604680323022
Loss in iteration 75 : 0.001332761027885335
Loss in iteration 76 : 0.001282229628097291
Loss in iteration 77 : 0.001249999251276911
Loss in iteration 78 : 0.0011873508756767173
Loss in iteration 79 : 0.0010926835173267393
Loss in iteration 80 : 0.0010307980369817788
Loss in iteration 81 : 9.644499461572417E-4
Loss in iteration 82 : 9.375375524647294E-4
Loss in iteration 83 : 8.565626898526617E-4
Loss in iteration 84 : 7.986289504098687E-4
Loss in iteration 85 : 7.420879931009166E-4
Loss in iteration 86 : 6.862725613021014E-4
Loss in iteration 87 : 6.592661894067952E-4
Loss in iteration 88 : 6.504736927077126E-4
Loss in iteration 89 : 5.597683253204739E-4
Loss in iteration 90 : 4.954133933825966E-4
Loss in iteration 91 : 3.5704146993501527E-4
Loss in iteration 92 : 2.871572700260583E-4
Loss in iteration 93 : 3.165140972663751E-4
Loss in iteration 94 : 3.4895820990783554E-4
Loss in iteration 95 : 8.048710233497956E-4
Loss in iteration 96 : 1.079657700878902E-4
Loss in iteration 97 : 9.181808865901879E-5
Loss in iteration 98 : 2.027515645814263E-5
Loss in iteration 99 : 4.978262583665262E-6
Loss in iteration 100 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 0.4 = 0.9964444444444445, training accuracy 1.0, time elapsed: 2082 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.2445667276101742
Loss in iteration 3 : 0.18182288059670892
Loss in iteration 4 : 0.12121112298728141
Loss in iteration 5 : 0.0995660954996688
Loss in iteration 6 : 0.08415380037098612
Loss in iteration 7 : 0.07277956330975308
Loss in iteration 8 : 0.06460349260512482
Loss in iteration 9 : 0.057808334105101895
Loss in iteration 10 : 0.051753181584987366
Loss in iteration 11 : 0.046357384686658516
Loss in iteration 12 : 0.04127932842224848
Loss in iteration 13 : 0.03685782569527217
Loss in iteration 14 : 0.033484175370673636
Loss in iteration 15 : 0.03064213332335528
Loss in iteration 16 : 0.028203034160644104
Loss in iteration 17 : 0.026107772025031395
Loss in iteration 18 : 0.024227342870453517
Loss in iteration 19 : 0.02253576618632285
Loss in iteration 20 : 0.020957126557508136
Loss in iteration 21 : 0.01945312202990666
Loss in iteration 22 : 0.01803103149258174
Loss in iteration 23 : 0.01673204619358377
Loss in iteration 24 : 0.015534565437839012
Loss in iteration 25 : 0.014388250315579517
Loss in iteration 26 : 0.013298399644650355
Loss in iteration 27 : 0.012276268184685139
Loss in iteration 28 : 0.011302271846266537
Loss in iteration 29 : 0.010405875435340524
Loss in iteration 30 : 0.009584225119829405
Loss in iteration 31 : 0.008824889338717504
Loss in iteration 32 : 0.008130379988016493
Loss in iteration 33 : 0.0075661359119157555
Loss in iteration 34 : 0.007056477181417458
Loss in iteration 35 : 0.006549627759866968
Loss in iteration 36 : 0.006071392729241082
Loss in iteration 37 : 0.00563246436775383
Loss in iteration 38 : 0.005221077163063847
Loss in iteration 39 : 0.0048480089837065685
Loss in iteration 40 : 0.004549826000608978
Loss in iteration 41 : 0.004278312862872755
Loss in iteration 42 : 0.004145501968970831
Loss in iteration 43 : 0.0039000859466980704
Loss in iteration 44 : 0.0036685542330495524
Loss in iteration 45 : 0.0034595072582241948
Loss in iteration 46 : 0.0032902883857420222
Loss in iteration 47 : 0.0030964511129668157
Loss in iteration 48 : 0.0029484204818545066
Loss in iteration 49 : 0.002820631550369725
Loss in iteration 50 : 0.002793981762600395
Loss in iteration 51 : 0.0031392127582468413
Loss in iteration 52 : 0.00395693650764725
Loss in iteration 53 : 0.007287555759723695
Loss in iteration 54 : 0.005790475894474607
Loss in iteration 55 : 0.002982756153573957
Loss in iteration 56 : 0.0025128999882547483
Loss in iteration 57 : 0.0023156782185799717
Loss in iteration 58 : 0.002238909398267851
Loss in iteration 59 : 0.0021397594984099142
Loss in iteration 60 : 0.0020687097330742703
Loss in iteration 61 : 0.0020091361118183284
Loss in iteration 62 : 0.001960767588649056
Loss in iteration 63 : 0.0019161688930295407
Loss in iteration 64 : 0.0018545908786725864
Loss in iteration 65 : 0.0018105194562018662
Loss in iteration 66 : 0.0017569185626361609
Loss in iteration 67 : 0.0017193092751231244
Loss in iteration 68 : 0.0017114861510703731
Loss in iteration 69 : 0.0016246710268908008
Loss in iteration 70 : 0.0015829054129840525
Loss in iteration 71 : 0.0015416162095031683
Loss in iteration 72 : 0.0014962554757942412
Loss in iteration 73 : 0.0014527181781492382
Loss in iteration 74 : 0.0014090196833459593
Loss in iteration 75 : 0.0013647390749439471
Loss in iteration 76 : 0.0013379836931864337
Loss in iteration 77 : 0.0014473086159871387
Loss in iteration 78 : 0.002517882519632267
Loss in iteration 79 : 0.005299354499195868
Loss in iteration 80 : 0.0056402270935471625
Loss in iteration 81 : 0.0019245922904109084
Loss in iteration 82 : 0.0012333123464877411
Loss in iteration 83 : 0.0011872323212031278
Loss in iteration 84 : 0.0011463773092555583
Loss in iteration 85 : 0.0011105583046211176
Loss in iteration 86 : 0.001072686741808875
Loss in iteration 87 : 0.001041452676797745
Loss in iteration 88 : 0.001024079044801786
Loss in iteration 89 : 9.695657672961481E-4
Loss in iteration 90 : 8.981897636481005E-4
Loss in iteration 91 : 8.519759097619853E-4
Loss in iteration 92 : 8.143267795605041E-4
Loss in iteration 93 : 7.92845535890947E-4
Loss in iteration 94 : 7.11506074365749E-4
Loss in iteration 95 : 6.696766357826775E-4
Loss in iteration 96 : 6.507750738788228E-4
Loss in iteration 97 : 6.22116783562383E-4
Loss in iteration 98 : 7.000015626381061E-4
Loss in iteration 99 : 5.209572263181495E-4
Loss in iteration 100 : 4.8130965468106817E-4
Loss in iteration 101 : 4.1791606140792866E-4
Loss in iteration 102 : 3.683887552484526E-4
Loss in iteration 103 : 3.2818072824081283E-4
Loss in iteration 104 : 3.9708504499772536E-4
Loss in iteration 105 : 4.454619124624625E-4
Loss in iteration 106 : 5.859252670509097E-4
Loss in iteration 107 : 2.097584984784071E-4
Loss in iteration 108 : 1.629420822946433E-4
Loss in iteration 109 : 1.6843199876801014E-4
Loss in iteration 110 : 3.405496132316047E-4
Loss in iteration 111 : 1.5625307893177114E-4
Loss in iteration 112 : 1.5401242416690825E-4
Loss in iteration 113 : 4.27882766685958E-5
Loss in iteration 114 : 6.400626445028186E-5
Loss in iteration 115 : 3.06918911989346E-6
Loss in iteration 116 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 0.28 = 0.9973333333333333, training accuracy 1.0, time elapsed: 2221 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8271897613029072
Loss in iteration 3 : 0.15237451352170667
Loss in iteration 4 : 0.1054672526742365
Loss in iteration 5 : 0.08843398211439697
Loss in iteration 6 : 0.07319133688210763
Loss in iteration 7 : 0.06387898942305607
Loss in iteration 8 : 0.05653987231542539
Loss in iteration 9 : 0.05032214722264514
Loss in iteration 10 : 0.04533883489617545
Loss in iteration 11 : 0.04092321679073898
Loss in iteration 12 : 0.03700148860838377
Loss in iteration 13 : 0.033610423187393365
Loss in iteration 14 : 0.03054338150444639
Loss in iteration 15 : 0.0278822597183252
Loss in iteration 16 : 0.025757701634251156
Loss in iteration 17 : 0.023868105206630495
Loss in iteration 18 : 0.022210187464817732
Loss in iteration 19 : 0.020758394983988415
Loss in iteration 20 : 0.019508404709477983
Loss in iteration 21 : 0.01840523438601031
Loss in iteration 22 : 0.01738318011465181
Loss in iteration 23 : 0.016363181562923067
Loss in iteration 24 : 0.015396724818230699
Loss in iteration 25 : 0.014504733551462087
Loss in iteration 26 : 0.013630785199457492
Loss in iteration 27 : 0.012858440951975376
Loss in iteration 28 : 0.012170722789146925
Loss in iteration 29 : 0.011539857849912833
Loss in iteration 30 : 0.010930128745409322
Loss in iteration 31 : 0.01035023959927647
Loss in iteration 32 : 0.009755928347497009
Loss in iteration 33 : 0.009167641119802172
Loss in iteration 34 : 0.008615243574412798
Loss in iteration 35 : 0.008034851265673578
Loss in iteration 36 : 0.007489506393828832
Loss in iteration 37 : 0.0069631206875050465
Loss in iteration 38 : 0.0064996653989023425
Loss in iteration 39 : 0.006107814737010464
Loss in iteration 40 : 0.005842096563322639
Loss in iteration 41 : 0.005677331624838393
Loss in iteration 42 : 0.005826652621593429
Loss in iteration 43 : 0.006356969435882069
Loss in iteration 44 : 0.007106206444432503
Loss in iteration 45 : 0.006958624544936142
Loss in iteration 46 : 0.005066758717851901
Loss in iteration 47 : 0.004195566458491257
Loss in iteration 48 : 0.0037917572952259034
Loss in iteration 49 : 0.003464727401678057
Loss in iteration 50 : 0.003186322295122258
Loss in iteration 51 : 0.0029619764059109865
Loss in iteration 52 : 0.0028417294568342396
Loss in iteration 53 : 0.0027579698808108993
Loss in iteration 54 : 0.0026752426406122585
Loss in iteration 55 : 0.0026646156143088607
Loss in iteration 56 : 0.0028906353132814935
Loss in iteration 57 : 0.003241175527947848
Loss in iteration 58 : 0.003993142785514929
Loss in iteration 59 : 0.003239260662478291
Loss in iteration 60 : 0.0023727321618794823
Loss in iteration 61 : 0.0020976196144100036
Loss in iteration 62 : 0.0019442881212640764
Loss in iteration 63 : 0.0018095240472363135
Loss in iteration 64 : 0.0017008637175797104
Loss in iteration 65 : 0.001629943816948742
Loss in iteration 66 : 0.0015707958490585212
Loss in iteration 67 : 0.001519971702077251
Loss in iteration 68 : 0.0014783937492140178
Loss in iteration 69 : 0.0014693021082088423
Loss in iteration 70 : 0.0016740200021451734
Loss in iteration 71 : 0.0019411155107456773
Loss in iteration 72 : 0.002339527118517136
Loss in iteration 73 : 0.0020055596952317525
Loss in iteration 74 : 0.001633537817798491
Loss in iteration 75 : 0.0013432105429144658
Loss in iteration 76 : 0.0012999505211211623
Loss in iteration 77 : 0.0012614875791014553
Loss in iteration 78 : 0.0012416062006757628
Loss in iteration 79 : 0.0012179070680328778
Loss in iteration 80 : 0.001168075087395642
Loss in iteration 81 : 0.0011470102239829333
Loss in iteration 82 : 0.001121785765370841
Loss in iteration 83 : 0.0010969440574084693
Loss in iteration 84 : 0.0010714088538047708
Loss in iteration 85 : 0.001035078746618786
Loss in iteration 86 : 0.001002603403818271
Loss in iteration 87 : 9.794175441785165E-4
Loss in iteration 88 : 9.680891900318408E-4
Loss in iteration 89 : 9.708689528996958E-4
Loss in iteration 90 : 9.718948012903914E-4
Loss in iteration 91 : 0.0013165119308517984
Loss in iteration 92 : 0.0024839452099879498
Loss in iteration 93 : 0.005181638676081639
Loss in iteration 94 : 0.0016157722277348719
Loss in iteration 95 : 8.860527492884046E-4
Loss in iteration 96 : 8.447201091222185E-4
Loss in iteration 97 : 8.249658896815374E-4
Loss in iteration 98 : 8.11843453587769E-4
Loss in iteration 99 : 7.802678176431786E-4
Loss in iteration 100 : 7.782941478778803E-4
Loss in iteration 101 : 7.491253644099897E-4
Loss in iteration 102 : 7.489794091629254E-4
Loss in iteration 103 : 7.099822183044921E-4
Loss in iteration 104 : 6.849664002518858E-4
Loss in iteration 105 : 6.494898479143185E-4
Loss in iteration 106 : 6.401203821297424E-4
Loss in iteration 107 : 6.439026229051909E-4
Loss in iteration 108 : 6.634466328085538E-4
Loss in iteration 109 : 5.73842892629284E-4
Loss in iteration 110 : 5.499652122445792E-4
Loss in iteration 111 : 5.140242920574896E-4
Loss in iteration 112 : 5.09060828147696E-4
Loss in iteration 113 : 5.772924160690762E-4
Loss in iteration 114 : 8.536416863721962E-4
Loss in iteration 115 : 5.105064706474088E-4
Loss in iteration 116 : 4.6070661881979566E-4
Loss in iteration 117 : 4.4781849014547006E-4
Loss in iteration 118 : 4.166980601401456E-4
Loss in iteration 119 : 3.913119591957013E-4
Loss in iteration 120 : 3.732501833709788E-4
Loss in iteration 121 : 3.794253259349362E-4
Loss in iteration 122 : 3.947906995087498E-4
Loss in iteration 123 : 7.051746486284274E-4
Loss in iteration 124 : 3.239103327279782E-4
Loss in iteration 125 : 2.9239576003123156E-4
Loss in iteration 126 : 2.754052346782455E-4
Loss in iteration 127 : 3.077810455165103E-4
Loss in iteration 128 : 2.596548126327497E-4
Loss in iteration 129 : 2.8036380191989794E-4
Loss in iteration 130 : 2.0923321828197991E-4
Loss in iteration 131 : 1.8201830802992635E-4
Loss in iteration 132 : 1.671491202967388E-4
Loss in iteration 133 : 1.7976108715559672E-4
Loss in iteration 134 : 2.888777547070826E-4
Loss in iteration 135 : 9.976773482953845E-4
Loss in iteration 136 : 2.9083798710605445E-4
Loss in iteration 137 : 3.0693435537990245E-4
Loss in iteration 138 : 1.4904668598626306E-4
Loss in iteration 139 : 1.0490058986886266E-4
Loss in iteration 140 : 6.654990308993232E-5
Loss in iteration 141 : 3.99692127008784E-5
Loss in iteration 142 : 3.210777156996956E-5
Loss in iteration 143 : 3.818830855357913E-5
Loss in iteration 144 : 1.206819207232045E-4
Loss in iteration 145 : 7.721615381985459E-6
Loss in iteration 146 : 2.0997992800610187E-6
Loss in iteration 147 : 3.1871084322788416E-6
Loss in iteration 148 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 0.16000000000000003 = 0.9982222222222222, training accuracy 1.0, time elapsed: 2525 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.509162679868967
Loss in iteration 3 : 0.2952354973931779
Loss in iteration 4 : 0.21381057423551342
Loss in iteration 5 : 0.1711714563170684
Loss in iteration 6 : 0.15564432136254375
Loss in iteration 7 : 0.14452305434205814
Loss in iteration 8 : 0.13541920769233554
Loss in iteration 9 : 0.1276541038044341
Loss in iteration 10 : 0.1204692577615479
Loss in iteration 11 : 0.11385544422322645
Loss in iteration 12 : 0.10790553558483595
Loss in iteration 13 : 0.10263313475854106
Loss in iteration 14 : 0.09772229937733291
Loss in iteration 15 : 0.093251084801089
Loss in iteration 16 : 0.0890709776786517
Loss in iteration 17 : 0.0853322141530697
Loss in iteration 18 : 0.08193385997314533
Loss in iteration 19 : 0.07869340133188076
Loss in iteration 20 : 0.0756731427147649
Loss in iteration 21 : 0.0729097034248078
Loss in iteration 22 : 0.0704000173971194
Loss in iteration 23 : 0.06799251876608678
Loss in iteration 24 : 0.06568912859552217
Loss in iteration 25 : 0.06353798498131934
Loss in iteration 26 : 0.061503323990162465
Loss in iteration 27 : 0.05955278134927817
Loss in iteration 28 : 0.057695156159193454
Loss in iteration 29 : 0.05597058159220946
Loss in iteration 30 : 0.0542990245840017
Loss in iteration 31 : 0.0526758974056886
Loss in iteration 32 : 0.05111352445970742
Loss in iteration 33 : 0.04960132905886176
Loss in iteration 34 : 0.04816584376033902
Loss in iteration 35 : 0.046786788946519005
Loss in iteration 36 : 0.04545864177938047
Loss in iteration 37 : 0.04426584698677296
Loss in iteration 38 : 0.043158296183388394
Loss in iteration 39 : 0.04207613875871948
Loss in iteration 40 : 0.04099330676213265
Loss in iteration 41 : 0.03996145347434954
Loss in iteration 42 : 0.03895579239305913
Loss in iteration 43 : 0.037925901066247554
Loss in iteration 44 : 0.03699243690163018
Loss in iteration 45 : 0.036113094148392866
Loss in iteration 46 : 0.03528355088557897
Loss in iteration 47 : 0.034448307910512434
Loss in iteration 48 : 0.03382177437819474
Loss in iteration 49 : 0.0332798090714226
Loss in iteration 50 : 0.032484153319496674
Loss in iteration 51 : 0.031884478464396786
Loss in iteration 52 : 0.03084947251159465
Loss in iteration 53 : 0.03017727813624492
Loss in iteration 54 : 0.029257663229082922
Loss in iteration 55 : 0.028555685616918017
Loss in iteration 56 : 0.02788059799453596
Loss in iteration 57 : 0.027238702821926652
Loss in iteration 58 : 0.0266147552976729
Loss in iteration 59 : 0.02611335122848507
Loss in iteration 60 : 0.025575920487989894
Loss in iteration 61 : 0.025194349678258062
Loss in iteration 62 : 0.02485850114462588
Loss in iteration 63 : 0.024514660604452034
Loss in iteration 64 : 0.02355198975673011
Loss in iteration 65 : 0.022820405606977224
Loss in iteration 66 : 0.022161219261786245
Loss in iteration 67 : 0.02170532836558855
Loss in iteration 68 : 0.021180857016597925
Loss in iteration 69 : 0.020783643259371588
Loss in iteration 70 : 0.020319188818592283
Loss in iteration 71 : 0.01984344441706869
Loss in iteration 72 : 0.019425225495469586
Loss in iteration 73 : 0.018861599129102423
Loss in iteration 74 : 0.018527471425330785
Loss in iteration 75 : 0.01802832747934392
Loss in iteration 76 : 0.017749135100363158
Loss in iteration 77 : 0.017380789912546508
Loss in iteration 78 : 0.0170023115637109
Loss in iteration 79 : 0.016615463125975418
Loss in iteration 80 : 0.016209759769098442
Loss in iteration 81 : 0.015886199546398295
Loss in iteration 82 : 0.015548105503627322
Loss in iteration 83 : 0.015295242298031206
Loss in iteration 84 : 0.015038088549862669
Loss in iteration 85 : 0.014897058802546766
Loss in iteration 86 : 0.014762008921999678
Loss in iteration 87 : 0.01449921318435522
Loss in iteration 88 : 0.014150014640198756
Loss in iteration 89 : 0.013795572144142083
Loss in iteration 90 : 0.01347981717320651
Loss in iteration 91 : 0.013196521651925457
Loss in iteration 92 : 0.012978783401748906
Loss in iteration 93 : 0.012766102608952477
Loss in iteration 94 : 0.01256304109288655
Loss in iteration 95 : 0.012382258509034943
Loss in iteration 96 : 0.012235312841198545
Loss in iteration 97 : 0.012105884353929465
Loss in iteration 98 : 0.011978428499490369
Loss in iteration 99 : 0.01199280836455982
Loss in iteration 100 : 0.011942168533919544
Loss in iteration 101 : 0.011821718112729538
Loss in iteration 102 : 0.011452027006621294
Loss in iteration 103 : 0.011075339603925592
Loss in iteration 104 : 0.010901237234608653
Loss in iteration 105 : 0.010719658420615199
Loss in iteration 106 : 0.010559035199076455
Loss in iteration 107 : 0.010401391632144976
Loss in iteration 108 : 0.010246565738407196
Loss in iteration 109 : 0.010093971535520406
Loss in iteration 110 : 0.00996045620421673
Loss in iteration 111 : 0.009938613931408593
Loss in iteration 112 : 0.010200426059103516
Loss in iteration 113 : 0.010124351304172111
Loss in iteration 114 : 0.009982953584129052
Loss in iteration 115 : 0.009359808988945197
Loss in iteration 116 : 0.009197599124192617
Loss in iteration 117 : 0.009041095127558537
Loss in iteration 118 : 0.008923359301931734
Loss in iteration 119 : 0.0087822894271029
Loss in iteration 120 : 0.00872720809011105
Loss in iteration 121 : 0.008647781666318688
Loss in iteration 122 : 0.008709483610928371
Loss in iteration 123 : 0.008408554706372662
Loss in iteration 124 : 0.008291384324077458
Loss in iteration 125 : 0.008109003326536565
Loss in iteration 126 : 0.00799835348451017
Loss in iteration 127 : 0.007891674661730325
Loss in iteration 128 : 0.007876327072160717
Loss in iteration 129 : 0.007683861189444681
Loss in iteration 130 : 0.007588351837776369
Loss in iteration 131 : 0.007466725343157202
Loss in iteration 132 : 0.007387128534093395
Loss in iteration 133 : 0.007252382912667566
Loss in iteration 134 : 0.007109378564293185
Loss in iteration 135 : 0.00700077761329641
Loss in iteration 136 : 0.006892055170762782
Loss in iteration 137 : 0.006792223825209216
Loss in iteration 138 : 0.006707909537363827
Loss in iteration 139 : 0.006661034989509079
Loss in iteration 140 : 0.006613329004213566
Loss in iteration 141 : 0.006409757413147004
Loss in iteration 142 : 0.006252566957138827
Loss in iteration 143 : 0.00611608873019768
Loss in iteration 144 : 0.006003270320611255
Loss in iteration 145 : 0.005893772832207371
Loss in iteration 146 : 0.005804869542182958
Loss in iteration 147 : 0.005780216267957964
Loss in iteration 148 : 0.005865815648096169
Loss in iteration 149 : 0.005767664614026699
Loss in iteration 150 : 0.005566030505280261
Loss in iteration 151 : 0.005372100173154294
Loss in iteration 152 : 0.005241542713674089
Loss in iteration 153 : 0.0051288705779765536
Loss in iteration 154 : 0.005037215910607088
Loss in iteration 155 : 0.004951344475989884
Loss in iteration 156 : 0.0048740906182825035
Loss in iteration 157 : 0.00479917344010819
Loss in iteration 158 : 0.004789372360462836
Loss in iteration 159 : 0.004744648078072838
Loss in iteration 160 : 0.0047368832938801736
Loss in iteration 161 : 0.004650999843100037
Loss in iteration 162 : 0.004540212880237553
Loss in iteration 163 : 0.004371486980533713
Loss in iteration 164 : 0.004271680269418554
Loss in iteration 165 : 0.004193583706760477
Loss in iteration 166 : 0.004117878199369906
Loss in iteration 167 : 0.0040456585683990515
Loss in iteration 168 : 0.003974309314323264
Loss in iteration 169 : 0.0039059466077623717
Loss in iteration 170 : 0.0038352811499231857
Loss in iteration 171 : 0.0037646659111846355
Loss in iteration 172 : 0.0037016506177813945
Loss in iteration 173 : 0.0036706507379627453
Loss in iteration 174 : 0.003784440502796751
Loss in iteration 175 : 0.003836495142431039
Loss in iteration 176 : 0.003666960891643922
Loss in iteration 177 : 0.003472972795197802
Loss in iteration 178 : 0.0033461474648753715
Loss in iteration 179 : 0.0032729909290264932
Loss in iteration 180 : 0.003211597083594463
Loss in iteration 181 : 0.003154827961673204
Loss in iteration 182 : 0.0030970675655964536
Loss in iteration 183 : 0.003038992465042604
Loss in iteration 184 : 0.002980402114259411
Loss in iteration 185 : 0.0029210862762504824
Loss in iteration 186 : 0.0028673305209011794
Loss in iteration 187 : 0.0028417740774683508
Loss in iteration 188 : 0.002889906544859448
Loss in iteration 189 : 0.002897043425900582
Loss in iteration 190 : 0.002930304755662585
Loss in iteration 191 : 0.0026674184834980376
Loss in iteration 192 : 0.0025711583461489012
Loss in iteration 193 : 0.0025007328022164346
Loss in iteration 194 : 0.002451927510561064
Loss in iteration 195 : 0.0024083877421529143
Loss in iteration 196 : 0.002375600565345147
Loss in iteration 197 : 0.0024010913071204993
Loss in iteration 198 : 0.002478693643922231
Loss in iteration 199 : 0.0023290365736988654
Loss in iteration 200 : 0.002276036562263398
Testing accuracy  of updater 5 on alg 1 with rate 0.03999999999999998 = 1.0, training accuracy 0.9995713060874536, time elapsed: 3393 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.9628308837434816
Loss in iteration 3 : 0.9856460555902717
Loss in iteration 4 : 0.2968296166408892
Loss in iteration 5 : 0.333963360493962
Loss in iteration 6 : 0.5032308234260107
Loss in iteration 7 : 0.551684178826161
Loss in iteration 8 : 0.4696371744469674
Loss in iteration 9 : 0.3626353478143771
Loss in iteration 10 : 0.2762935368610386
Loss in iteration 11 : 0.21152771725342595
Loss in iteration 12 : 0.16381022202189294
Loss in iteration 13 : 0.13424758307141677
Loss in iteration 14 : 0.12111813349884344
Loss in iteration 15 : 0.11851055278067403
Loss in iteration 16 : 0.12382367559588942
Loss in iteration 17 : 0.1284577118986906
Loss in iteration 18 : 0.12805603320783834
Loss in iteration 19 : 0.12101418192270774
Loss in iteration 20 : 0.10875727457131851
Loss in iteration 21 : 0.09502538410827138
Loss in iteration 22 : 0.08204401072742919
Loss in iteration 23 : 0.07071609428866936
Loss in iteration 24 : 0.061024132910715734
Loss in iteration 25 : 0.0522920104487236
Loss in iteration 26 : 0.04507365730228719
Loss in iteration 27 : 0.040685740004546375
Loss in iteration 28 : 0.03781961403549242
Loss in iteration 29 : 0.0365338421595323
Loss in iteration 30 : 0.035589816192191724
Loss in iteration 31 : 0.03465243218686994
Loss in iteration 32 : 0.03373841237366445
Loss in iteration 33 : 0.032769282903993883
Loss in iteration 34 : 0.03169683307415008
Loss in iteration 35 : 0.030421004669709803
Loss in iteration 36 : 0.029091290200807043
Loss in iteration 37 : 0.027656177418786166
Loss in iteration 38 : 0.026144284098132242
Loss in iteration 39 : 0.02472587788165625
Loss in iteration 40 : 0.02338755900622975
Loss in iteration 41 : 0.022212847426903427
Loss in iteration 42 : 0.021050275071094204
Loss in iteration 43 : 0.02002540463599703
Loss in iteration 44 : 0.019110229679857475
Loss in iteration 45 : 0.018361654069106326
Loss in iteration 46 : 0.01766016235431166
Loss in iteration 47 : 0.017143317168833338
Loss in iteration 48 : 0.016815594591320867
Loss in iteration 49 : 0.01652780146053736
Loss in iteration 50 : 0.016258332578905837
Loss in iteration 51 : 0.016018777752876003
Loss in iteration 52 : 0.01576145134730178
Loss in iteration 53 : 0.015476922060710229
Loss in iteration 54 : 0.015167930034456338
Loss in iteration 55 : 0.0148692105810676
Loss in iteration 56 : 0.014614916896428149
Loss in iteration 57 : 0.0143690719802747
Loss in iteration 58 : 0.014144642796180855
Loss in iteration 59 : 0.013932767981668044
Loss in iteration 60 : 0.01374039375316126
Loss in iteration 61 : 0.013558516838538419
Loss in iteration 62 : 0.013378282861456911
Loss in iteration 63 : 0.013199524950428736
Loss in iteration 64 : 0.013022092981100127
Loss in iteration 65 : 0.012845851883891445
Loss in iteration 66 : 0.012670680124407868
Loss in iteration 67 : 0.012496468338669157
Loss in iteration 68 : 0.012334943949807664
Loss in iteration 69 : 0.012177642630135294
Loss in iteration 70 : 0.012025068903976291
Loss in iteration 71 : 0.011884386254262037
Loss in iteration 72 : 0.01174591634352611
Loss in iteration 73 : 0.011609436155948033
Loss in iteration 74 : 0.011474744986168644
Loss in iteration 75 : 0.011348951380087591
Loss in iteration 76 : 0.01122582963020405
Loss in iteration 77 : 0.011103535173117827
Loss in iteration 78 : 0.010981983554822675
Loss in iteration 79 : 0.010861098754228039
Loss in iteration 80 : 0.010740812338868028
Loss in iteration 81 : 0.010621062705364382
Loss in iteration 82 : 0.01050179439609911
Loss in iteration 83 : 0.010382957484420533
Loss in iteration 84 : 0.010278125194880411
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.976, training accuracy 0.9987139182623607, time elapsed: 1490 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7946676272575838
Loss in iteration 3 : 0.4590544857080164
Loss in iteration 4 : 0.16338753916744153
Loss in iteration 5 : 0.2543451109568538
Loss in iteration 6 : 0.31144148538032373
Loss in iteration 7 : 0.2617208050377156
Loss in iteration 8 : 0.19020428234722997
Loss in iteration 9 : 0.13521225654928928
Loss in iteration 10 : 0.09473376650365553
Loss in iteration 11 : 0.07504364140716334
Loss in iteration 12 : 0.07162501787232582
Loss in iteration 13 : 0.07967041127915452
Loss in iteration 14 : 0.08499345762262736
Loss in iteration 15 : 0.08034550500647318
Loss in iteration 16 : 0.06831465637669598
Loss in iteration 17 : 0.05504376192249749
Loss in iteration 18 : 0.045216258868432294
Loss in iteration 19 : 0.036739035856910233
Loss in iteration 20 : 0.03022666882909972
Loss in iteration 21 : 0.026652376314921823
Loss in iteration 22 : 0.024768638133416836
Loss in iteration 23 : 0.02393275938572779
Loss in iteration 24 : 0.023462816978348094
Loss in iteration 25 : 0.023361188513786213
Loss in iteration 26 : 0.023303723410398644
Loss in iteration 27 : 0.022926369033025633
Loss in iteration 28 : 0.02207691461525123
Loss in iteration 29 : 0.020869151364209946
Loss in iteration 30 : 0.019506174502679812
Loss in iteration 31 : 0.018195560455859097
Loss in iteration 32 : 0.016974605044361005
Loss in iteration 33 : 0.01577052282826082
Loss in iteration 34 : 0.014683046060196868
Loss in iteration 35 : 0.01377338225874797
Loss in iteration 36 : 0.013092386089727752
Loss in iteration 37 : 0.012516497137953065
Loss in iteration 38 : 0.012179141047846195
Loss in iteration 39 : 0.011852300930451528
Loss in iteration 40 : 0.011606613086946703
Loss in iteration 41 : 0.011364167637965883
Loss in iteration 42 : 0.011124633987663812
Loss in iteration 43 : 0.010857444894565811
Loss in iteration 44 : 0.010612010818063863
Loss in iteration 45 : 0.01036937119154955
Loss in iteration 46 : 0.010118046359912999
Loss in iteration 47 : 0.009865063954270496
Loss in iteration 48 : 0.009616254893106548
Loss in iteration 49 : 0.009371690725280804
Loss in iteration 50 : 0.00915780431321369
Loss in iteration 51 : 0.008979932374452484
Loss in iteration 52 : 0.008806975539372094
Loss in iteration 53 : 0.00865566783192548
Loss in iteration 54 : 0.008517856518888131
Loss in iteration 55 : 0.008384284588076912
Loss in iteration 56 : 0.008256474472151694
Loss in iteration 57 : 0.00813645249157226
Loss in iteration 58 : 0.00801871816524157
Loss in iteration 59 : 0.007903069560590802
Loss in iteration 60 : 0.007790292837115019
Loss in iteration 61 : 0.007690514048127606
Loss in iteration 62 : 0.007592517207513956
Loss in iteration 63 : 0.007496566563629417
Loss in iteration 64 : 0.007404113780696386
Loss in iteration 65 : 0.007311093707940639
Loss in iteration 66 : 0.007218905448646107
Loss in iteration 67 : 0.0071256209849489905
Loss in iteration 68 : 0.007031663340685315
Loss in iteration 69 : 0.006939221021110912
Loss in iteration 70 : 0.0068465341638684505
Loss in iteration 71 : 0.00675397456277563
Loss in iteration 72 : 0.006676489155430157
Loss in iteration 73 : 0.006601951269699002
Loss in iteration 74 : 0.006528301405642746
Loss in iteration 75 : 0.0064578164759702935
Loss in iteration 76 : 0.006388145593684342
Testing accuracy  of updater 6 on alg 1 with rate 1.4000000000000001 = 0.9724444444444444, training accuracy 0.9988568162332095, time elapsed: 1432 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5568453304327997
Loss in iteration 3 : 0.3309186960000064
Loss in iteration 4 : 0.12565387248930399
Loss in iteration 5 : 0.20135184122883668
Loss in iteration 6 : 0.22788647490691535
Loss in iteration 7 : 0.170898226513617
Loss in iteration 8 : 0.11888034607593957
Loss in iteration 9 : 0.0846555994866382
Loss in iteration 10 : 0.06576461176991208
Loss in iteration 11 : 0.060561890242197304
Loss in iteration 12 : 0.06567220124432609
Loss in iteration 13 : 0.06921634138570901
Loss in iteration 14 : 0.06462989527930751
Loss in iteration 15 : 0.05353802311282281
Loss in iteration 16 : 0.0418344020408839
Loss in iteration 17 : 0.032971543891007815
Loss in iteration 18 : 0.02689448435087746
Loss in iteration 19 : 0.023512635185652775
Loss in iteration 20 : 0.022034475137153745
Loss in iteration 21 : 0.021399682555969492
Loss in iteration 22 : 0.021105293137536525
Loss in iteration 23 : 0.020791605576524298
Loss in iteration 24 : 0.02058183808050721
Loss in iteration 25 : 0.020187481931291083
Loss in iteration 26 : 0.019675280374199588
Loss in iteration 27 : 0.018863382683305688
Loss in iteration 28 : 0.017806050457033436
Loss in iteration 29 : 0.016507167100474094
Loss in iteration 30 : 0.01507555099241071
Loss in iteration 31 : 0.013664798283761086
Loss in iteration 32 : 0.01248847884647322
Loss in iteration 33 : 0.01154491452748125
Loss in iteration 34 : 0.010768081427830376
Loss in iteration 35 : 0.010254993137588419
Loss in iteration 36 : 0.009924623577470953
Loss in iteration 37 : 0.009735925459021449
Loss in iteration 38 : 0.009533945825074617
Loss in iteration 39 : 0.009334076338480652
Loss in iteration 40 : 0.009096666334611295
Loss in iteration 41 : 0.008808796331160353
Loss in iteration 42 : 0.008498011952010135
Loss in iteration 43 : 0.008183747649815982
Loss in iteration 44 : 0.007851679304924726
Loss in iteration 45 : 0.007521747690548689
Loss in iteration 46 : 0.007226010349890991
Loss in iteration 47 : 0.006971703342959098
Loss in iteration 48 : 0.0067694408277677726
Loss in iteration 49 : 0.006600044693062782
Loss in iteration 50 : 0.0064457588344080975
Loss in iteration 51 : 0.006318537388136314
Loss in iteration 52 : 0.006201060295451839
Loss in iteration 53 : 0.0060981536622224984
Loss in iteration 54 : 0.0059980604769046385
Loss in iteration 55 : 0.005903675991005593
Loss in iteration 56 : 0.005815741243074781
Loss in iteration 57 : 0.005733428882446064
Loss in iteration 58 : 0.005657289343915449
Loss in iteration 59 : 0.005582919647020002
Loss in iteration 60 : 0.005509933992243806
Loss in iteration 61 : 0.005441211778523712
Loss in iteration 62 : 0.00537470294372853
Loss in iteration 63 : 0.005308574550409708
Loss in iteration 64 : 0.005242815981743217
Loss in iteration 65 : 0.005177417091277254
Loss in iteration 66 : 0.005114927175927162
Loss in iteration 67 : 0.005053922627008667
Loss in iteration 68 : 0.00499389484238762
Loss in iteration 69 : 0.004934763102838343
Loss in iteration 70 : 0.004878689900990586
Loss in iteration 71 : 0.004829561660675951
Loss in iteration 72 : 0.004783279882090103
Testing accuracy  of updater 6 on alg 1 with rate 0.8 = 0.9724444444444444, training accuracy 0.9988568162332095, time elapsed: 1270 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.326506132164513
Loss in iteration 3 : 0.22471448381739503
Loss in iteration 4 : 0.1414624993412093
Loss in iteration 5 : 0.10976644830169238
Loss in iteration 6 : 0.09208071993643636
Loss in iteration 7 : 0.07903047746825678
Loss in iteration 8 : 0.07055120636872808
Loss in iteration 9 : 0.0646540305920006
Loss in iteration 10 : 0.05910534539094933
Loss in iteration 11 : 0.05346698819126996
Loss in iteration 12 : 0.04755391457868921
Loss in iteration 13 : 0.04178780355217193
Loss in iteration 14 : 0.03654032321386103
Loss in iteration 15 : 0.03217469168058337
Loss in iteration 16 : 0.02860898877903308
Loss in iteration 17 : 0.02569570338020646
Loss in iteration 18 : 0.02332926963014332
Loss in iteration 19 : 0.021498203217904792
Loss in iteration 20 : 0.020014882652587045
Loss in iteration 21 : 0.018678058590809326
Loss in iteration 22 : 0.017441095699030334
Loss in iteration 23 : 0.016357377884416317
Loss in iteration 24 : 0.01534375719663751
Loss in iteration 25 : 0.014419228251568175
Loss in iteration 26 : 0.013620325282594927
Loss in iteration 27 : 0.012915762919228257
Loss in iteration 28 : 0.012336954367271483
Loss in iteration 29 : 0.011844559403797115
Loss in iteration 30 : 0.011432429517668034
Loss in iteration 31 : 0.011052228664691507
Loss in iteration 32 : 0.01068450463525931
Loss in iteration 33 : 0.010320017564813122
Loss in iteration 34 : 0.009963583707694849
Loss in iteration 35 : 0.00961300229229937
Loss in iteration 36 : 0.009270370486002065
Loss in iteration 37 : 0.008933877650905867
Loss in iteration 38 : 0.008602264377879563
Loss in iteration 39 : 0.008276394905051724
Loss in iteration 40 : 0.00796218512856981
Loss in iteration 41 : 0.007673568175984672
Loss in iteration 42 : 0.007416352273659765
Loss in iteration 43 : 0.0071678101609696995
Loss in iteration 44 : 0.006923862247504811
Loss in iteration 45 : 0.006690352907587839
Loss in iteration 46 : 0.006461857490333436
Loss in iteration 47 : 0.006253089035665535
Loss in iteration 48 : 0.006056503820095429
Loss in iteration 49 : 0.005869582826210519
Loss in iteration 50 : 0.0056980276955487345
Loss in iteration 51 : 0.005553574559340451
Loss in iteration 52 : 0.005423997525990837
Loss in iteration 53 : 0.005302310069853756
Loss in iteration 54 : 0.00519016991173878
Loss in iteration 55 : 0.00508289666526226
Loss in iteration 56 : 0.004978513702591405
Loss in iteration 57 : 0.004879133690854369
Loss in iteration 58 : 0.0047822144676173
Loss in iteration 59 : 0.00468987525375199
Loss in iteration 60 : 0.004599921291021897
Loss in iteration 61 : 0.004511060730535073
Loss in iteration 62 : 0.004425970684231923
Loss in iteration 63 : 0.004346751786382569
Loss in iteration 64 : 0.004270569215929679
Loss in iteration 65 : 0.004206453578676751
Loss in iteration 66 : 0.004146039644316577
Loss in iteration 67 : 0.004087088990867186
Loss in iteration 68 : 0.004029898232300618
Loss in iteration 69 : 0.003974615769537158
Loss in iteration 70 : 0.003921649016665855
Loss in iteration 71 : 0.003872149654625935
Loss in iteration 72 : 0.003823067787005667
Loss in iteration 73 : 0.003775651843808608
Loss in iteration 74 : 0.0037280924167861127
Loss in iteration 75 : 0.003680358856599694
Loss in iteration 76 : 0.0036327177998151617
Loss in iteration 77 : 0.0035858349889993308
Loss in iteration 78 : 0.0035403602457162926
Loss in iteration 79 : 0.0034961365188499764
Loss in iteration 80 : 0.0034534094103483284
Loss in iteration 81 : 0.0034111497475322957
Loss in iteration 82 : 0.003369323639440745
Loss in iteration 83 : 0.003327900490346179
Loss in iteration 84 : 0.003287017862713404
Loss in iteration 85 : 0.0032467625979435096
Loss in iteration 86 : 0.003207228432544144
Loss in iteration 87 : 0.003168989679657374
Loss in iteration 88 : 0.0031340226549440658
Loss in iteration 89 : 0.0031005922728028824
Loss in iteration 90 : 0.003069727871504363
Loss in iteration 91 : 0.003039240587363404
Loss in iteration 92 : 0.0030112768505432238
Loss in iteration 93 : 0.0029853087758056774
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.9804444444444445, training accuracy 0.9991426121749071, time elapsed: 1610 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3671968459838455
Loss in iteration 3 : 0.253804758540035
Loss in iteration 4 : 0.16413590679496073
Loss in iteration 5 : 0.14880771797206974
Loss in iteration 6 : 0.11931368374886991
Loss in iteration 7 : 0.08969125246830187
Loss in iteration 8 : 0.07710979692013092
Loss in iteration 9 : 0.07570696124006217
Loss in iteration 10 : 0.07406397487006718
Loss in iteration 11 : 0.06847803379826975
Loss in iteration 12 : 0.060835577396617074
Loss in iteration 13 : 0.05253213320827789
Loss in iteration 14 : 0.04573086192762963
Loss in iteration 15 : 0.04014147848177509
Loss in iteration 16 : 0.03559589015674345
Loss in iteration 17 : 0.03225466671605901
Loss in iteration 18 : 0.030072215336823808
Loss in iteration 19 : 0.02837046350473291
Loss in iteration 20 : 0.026840679897661878
Loss in iteration 21 : 0.025125776997275705
Loss in iteration 22 : 0.02339931561400697
Loss in iteration 23 : 0.021678241202511172
Loss in iteration 24 : 0.020073933230801904
Loss in iteration 25 : 0.01856786432849183
Loss in iteration 26 : 0.017257100180617976
Loss in iteration 27 : 0.016209598000055885
Loss in iteration 28 : 0.015357646192927362
Loss in iteration 29 : 0.0146829136102004
Loss in iteration 30 : 0.014184567903113145
Loss in iteration 31 : 0.013750359954817834
Loss in iteration 32 : 0.01337024380662175
Loss in iteration 33 : 0.01302952401973509
Loss in iteration 34 : 0.01271817681391105
Loss in iteration 35 : 0.012399892264582288
Loss in iteration 36 : 0.012067042442394626
Loss in iteration 37 : 0.011719289710401752
Loss in iteration 38 : 0.011357166874688995
Loss in iteration 39 : 0.010997640873113222
Loss in iteration 40 : 0.01064775713145489
Loss in iteration 41 : 0.010311137178997797
Loss in iteration 42 : 0.009984714789569474
Loss in iteration 43 : 0.00967818126470748
Loss in iteration 44 : 0.009384448237108503
Loss in iteration 45 : 0.009103998949910295
Loss in iteration 46 : 0.008835832428572696
Loss in iteration 47 : 0.008598455584870282
Loss in iteration 48 : 0.00837133853185196
Loss in iteration 49 : 0.008152263453138034
Loss in iteration 50 : 0.00794733436713418
Loss in iteration 51 : 0.007766047123324221
Loss in iteration 52 : 0.007594837095499328
Loss in iteration 53 : 0.007422751292062342
Loss in iteration 54 : 0.007250608341159229
Loss in iteration 55 : 0.007075528249514825
Loss in iteration 56 : 0.0069013817137827775
Loss in iteration 57 : 0.006737445407701636
Loss in iteration 58 : 0.006580618922453579
Loss in iteration 59 : 0.006428777259428445
Loss in iteration 60 : 0.006280323376781794
Loss in iteration 61 : 0.006134904906796228
Loss in iteration 62 : 0.005996903924066318
Loss in iteration 63 : 0.005867196512075488
Loss in iteration 64 : 0.0057461132638917285
Loss in iteration 65 : 0.005627338623638437
Loss in iteration 66 : 0.005515421785065302
Loss in iteration 67 : 0.005408595405271157
Loss in iteration 68 : 0.005304708529275877
Loss in iteration 69 : 0.005208369639388726
Loss in iteration 70 : 0.00511847942017525
Loss in iteration 71 : 0.005032418769476102
Loss in iteration 72 : 0.004951922176493446
Loss in iteration 73 : 0.004874122690691435
Loss in iteration 74 : 0.004802378152914171
Loss in iteration 75 : 0.004732153180831781
Loss in iteration 76 : 0.004662315942156788
Loss in iteration 77 : 0.004593339109916423
Loss in iteration 78 : 0.004526206120686737
Loss in iteration 79 : 0.004461184124555108
Loss in iteration 80 : 0.004398577223968328
Loss in iteration 81 : 0.004338285553629238
Loss in iteration 82 : 0.0042789231379229496
Loss in iteration 83 : 0.004220414430199102
Loss in iteration 84 : 0.004164298893797471
Loss in iteration 85 : 0.004110686953180089
Loss in iteration 86 : 0.004058532405947905
Loss in iteration 87 : 0.004008063182488286
Loss in iteration 88 : 0.003957695345307925
Loss in iteration 89 : 0.003907262028169052
Loss in iteration 90 : 0.003857084041135802
Loss in iteration 91 : 0.003808674210104891
Loss in iteration 92 : 0.0037621094629243287
Loss in iteration 93 : 0.0037169931812626403
Loss in iteration 94 : 0.003672682565058546
Loss in iteration 95 : 0.003629907495273839
Loss in iteration 96 : 0.0035881358330111697
Loss in iteration 97 : 0.003546700918468817
Loss in iteration 98 : 0.0035060177048015094
Loss in iteration 99 : 0.0034663117904576417
Loss in iteration 100 : 0.0034292091303674286
Loss in iteration 101 : 0.0033937397253987873
Loss in iteration 102 : 0.003360115975912619
Loss in iteration 103 : 0.0033283620246192876
Loss in iteration 104 : 0.0032971702183995674
Loss in iteration 105 : 0.0032671593105738114
Loss in iteration 106 : 0.003238562991228603
Loss in iteration 107 : 0.0032109049124070584
Loss in iteration 108 : 0.003183903684642988
Loss in iteration 109 : 0.0031571097993223483
Loss in iteration 110 : 0.0031305092576535596
Loss in iteration 111 : 0.0031040894067335587
Testing accuracy  of updater 6 on alg 1 with rate 0.14 = 0.9813333333333333, training accuracy 0.9991426121749071, time elapsed: 1976 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.586169679362707
Loss in iteration 3 : 0.3288153787585623
Loss in iteration 4 : 0.20942892930422788
Loss in iteration 5 : 0.17815565159861152
Loss in iteration 6 : 0.15693627492085807
Loss in iteration 7 : 0.1426946582750095
Loss in iteration 8 : 0.1279485943967833
Loss in iteration 9 : 0.11052391838626296
Loss in iteration 10 : 0.09510695662720557
Loss in iteration 11 : 0.0844187855183992
Loss in iteration 12 : 0.07717786335764967
Loss in iteration 13 : 0.07173751324483726
Loss in iteration 14 : 0.06684866398661264
Loss in iteration 15 : 0.06232686247103835
Loss in iteration 16 : 0.05818250183995573
Loss in iteration 17 : 0.054222686235718735
Loss in iteration 18 : 0.050503767090645024
Loss in iteration 19 : 0.047124739152758756
Loss in iteration 20 : 0.04411117658188543
Loss in iteration 21 : 0.041364124917035366
Loss in iteration 22 : 0.03882825399767859
Loss in iteration 23 : 0.03644234590194025
Loss in iteration 24 : 0.03420191763952299
Loss in iteration 25 : 0.03210641278550038
Loss in iteration 26 : 0.030143998727094456
Loss in iteration 27 : 0.0282985267070823
Loss in iteration 28 : 0.026690080146522195
Loss in iteration 29 : 0.025246102173002587
Loss in iteration 30 : 0.023954531495329265
Loss in iteration 31 : 0.02289151157859199
Loss in iteration 32 : 0.021980536419249543
Loss in iteration 33 : 0.0211487161805809
Loss in iteration 34 : 0.02041309856521774
Loss in iteration 35 : 0.019721601352757474
Loss in iteration 36 : 0.019071309685229892
Loss in iteration 37 : 0.01847696899400576
Loss in iteration 38 : 0.017923215579202657
Loss in iteration 39 : 0.01743755483069567
Loss in iteration 40 : 0.016990791138839258
Loss in iteration 41 : 0.016569336596959024
Loss in iteration 42 : 0.016170675111778728
Loss in iteration 43 : 0.015800058817721573
Loss in iteration 44 : 0.015442013250895469
Loss in iteration 45 : 0.015098047693791809
Loss in iteration 46 : 0.014774244364320456
Loss in iteration 47 : 0.014482421771377107
Loss in iteration 48 : 0.014211424280895519
Loss in iteration 49 : 0.013962103250806088
Loss in iteration 50 : 0.01372310604733443
Loss in iteration 51 : 0.013489034728747774
Loss in iteration 52 : 0.013259328713355414
Loss in iteration 53 : 0.013036691696965906
Loss in iteration 54 : 0.012823185664937608
Loss in iteration 55 : 0.01261675161271864
Loss in iteration 56 : 0.012415918237253283
Loss in iteration 57 : 0.01222285510410342
Loss in iteration 58 : 0.0120342716674184
Loss in iteration 59 : 0.01184945933357048
Loss in iteration 60 : 0.011669920920210925
Loss in iteration 61 : 0.011497807994143308
Loss in iteration 62 : 0.011332413306970768
Loss in iteration 63 : 0.011173902699904665
Loss in iteration 64 : 0.011020984232288705
Loss in iteration 65 : 0.01087105284925173
Loss in iteration 66 : 0.010723354093841004
Loss in iteration 67 : 0.010580307695746116
Loss in iteration 68 : 0.010440524109272193
Loss in iteration 69 : 0.0103035647457745
Loss in iteration 70 : 0.010168206400319096
Loss in iteration 71 : 0.010034275027692484
Loss in iteration 72 : 0.009901965216859435
Loss in iteration 73 : 0.00977101596720952
Loss in iteration 74 : 0.009642545907859094
Loss in iteration 75 : 0.00951724024382328
Loss in iteration 76 : 0.009395279462674815
Loss in iteration 77 : 0.009276290508890561
Loss in iteration 78 : 0.009161063412968665
Loss in iteration 79 : 0.009050739599657322
Loss in iteration 80 : 0.008944685858120645
Loss in iteration 81 : 0.008841719340585821
Loss in iteration 82 : 0.008740944011304567
Loss in iteration 83 : 0.00864137600031296
Loss in iteration 84 : 0.008543031057920305
Loss in iteration 85 : 0.008446019164846814
Loss in iteration 86 : 0.00835042183613501
Loss in iteration 87 : 0.008256088937663492
Loss in iteration 88 : 0.008162512471084089
Loss in iteration 89 : 0.00806964686981634
Loss in iteration 90 : 0.007978236427545141
Loss in iteration 91 : 0.007888652755130881
Loss in iteration 92 : 0.007800000072055396
Loss in iteration 93 : 0.007712685956431186
Loss in iteration 94 : 0.007628485699408938
Loss in iteration 95 : 0.007545717975794365
Loss in iteration 96 : 0.0074635905282961984
Loss in iteration 97 : 0.007382200880469249
Loss in iteration 98 : 0.007301741504772505
Loss in iteration 99 : 0.007221797312145984
Loss in iteration 100 : 0.007142379022285135
Loss in iteration 101 : 0.007065752895757041
Loss in iteration 102 : 0.006991389452964552
Loss in iteration 103 : 0.006917484485504924
Loss in iteration 104 : 0.006844243120084709
Loss in iteration 105 : 0.006771818196701135
Loss in iteration 106 : 0.006701833690307837
Loss in iteration 107 : 0.0066327590989357366
Loss in iteration 108 : 0.006564232995379021
Loss in iteration 109 : 0.006496160905869989
Loss in iteration 110 : 0.006429230588362317
Loss in iteration 111 : 0.006362801116187964
Loss in iteration 112 : 0.006296742029296754
Loss in iteration 113 : 0.00623173643795103
Loss in iteration 114 : 0.006167865326427094
Loss in iteration 115 : 0.006105529462695067
Loss in iteration 116 : 0.006045589891827852
Loss in iteration 117 : 0.005986082411599192
Loss in iteration 118 : 0.005927082725217301
Loss in iteration 119 : 0.005872132085812522
Loss in iteration 120 : 0.005818813434383033
Loss in iteration 121 : 0.005766193429525793
Loss in iteration 122 : 0.005715762801026165
Loss in iteration 123 : 0.005666253241448578
Loss in iteration 124 : 0.005617324936506603
Loss in iteration 125 : 0.005569310297470168
Loss in iteration 126 : 0.005521938845945834
Loss in iteration 127 : 0.00547497529392832
Loss in iteration 128 : 0.005428389807616126
Loss in iteration 129 : 0.005382624870232874
Loss in iteration 130 : 0.005338440861468897
Loss in iteration 131 : 0.00529474748184262
Loss in iteration 132 : 0.005251376336285696
Loss in iteration 133 : 0.005208461056861744
Loss in iteration 134 : 0.0051667321239809005
Loss in iteration 135 : 0.005126018066785714
Loss in iteration 136 : 0.005085726082647421
Loss in iteration 137 : 0.005046144528042314
Loss in iteration 138 : 0.0050076142266015115
Loss in iteration 139 : 0.004970167736093858
Loss in iteration 140 : 0.004933591938874183
Loss in iteration 141 : 0.004897333128291921
Loss in iteration 142 : 0.004861366735380205
Loss in iteration 143 : 0.0048260057228998446
Loss in iteration 144 : 0.00479147397550394
Loss in iteration 145 : 0.004757560442061008
Loss in iteration 146 : 0.004723819182545012
Loss in iteration 147 : 0.004690239860463996
Loss in iteration 148 : 0.004657143541536862
Loss in iteration 149 : 0.004625077238368501
Loss in iteration 150 : 0.004593170370954784
Loss in iteration 151 : 0.004561451914992995
Loss in iteration 152 : 0.004530224175842792
Loss in iteration 153 : 0.00449956435893453
Loss in iteration 154 : 0.004469100953311465
Loss in iteration 155 : 0.0044388091713704406
Loss in iteration 156 : 0.004408726796224345
Loss in iteration 157 : 0.0043790108200129785
Loss in iteration 158 : 0.004349599488881412
Loss in iteration 159 : 0.004320257777901779
Loss in iteration 160 : 0.004290994847826714
Loss in iteration 161 : 0.004261878098551604
Loss in iteration 162 : 0.004232902572387984
Loss in iteration 163 : 0.004204179194746759
Loss in iteration 164 : 0.004175621995852719
Loss in iteration 165 : 0.0041471832358994576
Loss in iteration 166 : 0.004119535000955347
Loss in iteration 167 : 0.004092483827704225
Loss in iteration 168 : 0.004065372043755554
Loss in iteration 169 : 0.004038210947608388
Loss in iteration 170 : 0.004011576385116442
Testing accuracy  of updater 6 on alg 1 with rate 0.08000000000000002 = 0.9822222222222222, training accuracy 0.9989997142040583, time elapsed: 2933 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8940819384500261
Loss in iteration 3 : 0.7582847235328353
Loss in iteration 4 : 0.6072804641432233
Loss in iteration 5 : 0.45554682427366366
Loss in iteration 6 : 0.36924882008830656
Loss in iteration 7 : 0.2924439387569267
Loss in iteration 8 : 0.23722484603287886
Loss in iteration 9 : 0.21107505977716767
Loss in iteration 10 : 0.19870106304906185
Loss in iteration 11 : 0.1917906158040118
Loss in iteration 12 : 0.18663560880621838
Loss in iteration 13 : 0.18132825458404034
Loss in iteration 14 : 0.17536915089598495
Loss in iteration 15 : 0.16892363799626148
Loss in iteration 16 : 0.1623871333094356
Loss in iteration 17 : 0.15599644429267587
Loss in iteration 18 : 0.14995323294458734
Loss in iteration 19 : 0.14406830926310688
Loss in iteration 20 : 0.13840937144498627
Loss in iteration 21 : 0.1330289271571221
Loss in iteration 22 : 0.12781825222296023
Loss in iteration 23 : 0.12282689416073424
Loss in iteration 24 : 0.11809510143323704
Loss in iteration 25 : 0.11361593823298397
Loss in iteration 26 : 0.10939654442043154
Loss in iteration 27 : 0.10546808171854082
Loss in iteration 28 : 0.10182546245823003
Loss in iteration 29 : 0.09848657451157417
Loss in iteration 30 : 0.09536042310624676
Loss in iteration 31 : 0.09242886167666739
Loss in iteration 32 : 0.08977371352293279
Loss in iteration 33 : 0.08729772266559044
Loss in iteration 34 : 0.08493455504364916
Loss in iteration 35 : 0.08271304978816223
Loss in iteration 36 : 0.08063391693402519
Loss in iteration 37 : 0.07868344326052455
Loss in iteration 38 : 0.07683905568334247
Loss in iteration 39 : 0.07511621630874175
Loss in iteration 40 : 0.07349355273957049
Loss in iteration 41 : 0.07197297664955259
Loss in iteration 42 : 0.07054333238678527
Loss in iteration 43 : 0.06919356066625812
Loss in iteration 44 : 0.0679204784004435
Loss in iteration 45 : 0.06670920048579995
Loss in iteration 46 : 0.0655514909456995
Loss in iteration 47 : 0.0644422826064921
Loss in iteration 48 : 0.06338387917750055
Loss in iteration 49 : 0.06237222268495513
Loss in iteration 50 : 0.06139321978989589
Loss in iteration 51 : 0.06044348148194322
Loss in iteration 52 : 0.059524368897727435
Loss in iteration 53 : 0.05863694994864755
Loss in iteration 54 : 0.057779353001286164
Loss in iteration 55 : 0.05695871846010637
Loss in iteration 56 : 0.0561655633543956
Loss in iteration 57 : 0.055391423870572415
Loss in iteration 58 : 0.054643766122467796
Loss in iteration 59 : 0.05392100641411126
Loss in iteration 60 : 0.053219966829969725
Loss in iteration 61 : 0.05254115564260202
Loss in iteration 62 : 0.051880666905124884
Loss in iteration 63 : 0.05123717654142531
Loss in iteration 64 : 0.050606207347589235
Loss in iteration 65 : 0.04998975326862298
Loss in iteration 66 : 0.04938518587912162
Loss in iteration 67 : 0.04879114991931253
Loss in iteration 68 : 0.04820726798857332
Loss in iteration 69 : 0.047635366245655085
Loss in iteration 70 : 0.04708363859951772
Loss in iteration 71 : 0.04655355586870249
Loss in iteration 72 : 0.04603900557134994
Loss in iteration 73 : 0.04554571240705446
Loss in iteration 74 : 0.04506716383984555
Loss in iteration 75 : 0.044598507290226436
Loss in iteration 76 : 0.044141778425369343
Loss in iteration 77 : 0.04369379218635278
Loss in iteration 78 : 0.04325511821329843
Loss in iteration 79 : 0.042824079283965324
Loss in iteration 80 : 0.04240072538580481
Loss in iteration 81 : 0.04198558461830789
Loss in iteration 82 : 0.04158340609117929
Loss in iteration 83 : 0.04119173400190418
Loss in iteration 84 : 0.04080807104781682
Loss in iteration 85 : 0.04043369298904598
Loss in iteration 86 : 0.0400659605190371
Loss in iteration 87 : 0.039703892458581955
Loss in iteration 88 : 0.039347623627322165
Loss in iteration 89 : 0.03899624130393655
Loss in iteration 90 : 0.03864954590024606
Loss in iteration 91 : 0.03830847743758412
Loss in iteration 92 : 0.0379741157220632
Loss in iteration 93 : 0.03764494769370668
Loss in iteration 94 : 0.03732059417687288
Loss in iteration 95 : 0.03700235487847118
Loss in iteration 96 : 0.03668985811864144
Loss in iteration 97 : 0.03638299488618529
Loss in iteration 98 : 0.03607962789965476
Loss in iteration 99 : 0.03577985362363516
Loss in iteration 100 : 0.03548604918935868
Loss in iteration 101 : 0.03519750732244591
Loss in iteration 102 : 0.034915457907712284
Loss in iteration 103 : 0.03463687351895852
Loss in iteration 104 : 0.03436244454272205
Loss in iteration 105 : 0.034092951227349705
Loss in iteration 106 : 0.03382767340820551
Loss in iteration 107 : 0.03356596842434883
Loss in iteration 108 : 0.03330754243548611
Loss in iteration 109 : 0.03305354294428651
Loss in iteration 110 : 0.03280252996169453
Loss in iteration 111 : 0.032554458986846856
Loss in iteration 112 : 0.0323099757744305
Loss in iteration 113 : 0.032069936998463874
Loss in iteration 114 : 0.03183313803670799
Loss in iteration 115 : 0.03159988223074739
Loss in iteration 116 : 0.03136968806708412
Loss in iteration 117 : 0.031142889364474256
Loss in iteration 118 : 0.030919447031561905
Loss in iteration 119 : 0.030698104438378573
Loss in iteration 120 : 0.030479312595573176
Loss in iteration 121 : 0.030264048145508263
Loss in iteration 122 : 0.030051714975232683
Loss in iteration 123 : 0.029843151606282342
Loss in iteration 124 : 0.02963665973557338
Loss in iteration 125 : 0.029432733842154803
Loss in iteration 126 : 0.029230883016474724
Loss in iteration 127 : 0.02903158362250516
Loss in iteration 128 : 0.028834333207076217
Loss in iteration 129 : 0.028640379964115555
Loss in iteration 130 : 0.028448631540632863
Loss in iteration 131 : 0.028259273763449888
Loss in iteration 132 : 0.028072172329675675
Loss in iteration 133 : 0.027888042497852514
Loss in iteration 134 : 0.0277070333721843
Loss in iteration 135 : 0.02752775573001031
Loss in iteration 136 : 0.027351091973168578
Loss in iteration 137 : 0.02717647804175858
Loss in iteration 138 : 0.027003824682035733
Loss in iteration 139 : 0.02683329910777294
Loss in iteration 140 : 0.02666552516529707
Loss in iteration 141 : 0.026500151664695173
Loss in iteration 142 : 0.02633618305354719
Loss in iteration 143 : 0.026174799890303274
Loss in iteration 144 : 0.026014734318474
Loss in iteration 145 : 0.02585591159860334
Loss in iteration 146 : 0.025698600323898188
Loss in iteration 147 : 0.02554272858476003
Loss in iteration 148 : 0.025388195740669636
Loss in iteration 149 : 0.025235548142402957
Loss in iteration 150 : 0.02508412119377775
Loss in iteration 151 : 0.024933620573154295
Loss in iteration 152 : 0.024784866627233535
Loss in iteration 153 : 0.0246381137947786
Loss in iteration 154 : 0.024492543383111968
Loss in iteration 155 : 0.024349093453244926
Loss in iteration 156 : 0.0242074338386226
Loss in iteration 157 : 0.02406720020389318
Loss in iteration 158 : 0.023929392629496143
Loss in iteration 159 : 0.023794606951223186
Loss in iteration 160 : 0.023662777300855375
Loss in iteration 161 : 0.023533576343515723
Loss in iteration 162 : 0.023406945714582253
Loss in iteration 163 : 0.023282237578492827
Loss in iteration 164 : 0.023159062828322804
Loss in iteration 165 : 0.023037458628695124
Loss in iteration 166 : 0.02291780924864009
Loss in iteration 167 : 0.022800002633041924
Loss in iteration 168 : 0.022683781641955635
Loss in iteration 169 : 0.022569689289340507
Loss in iteration 170 : 0.02245853483909688
Loss in iteration 171 : 0.022349508953176042
Loss in iteration 172 : 0.022242148469230175
Loss in iteration 173 : 0.022137651391081103
Loss in iteration 174 : 0.022035141734688735
Loss in iteration 175 : 0.021934353453874834
Loss in iteration 176 : 0.02183485797684939
Loss in iteration 177 : 0.02173670742965647
Loss in iteration 178 : 0.02164023643623989
Loss in iteration 179 : 0.021544601016678516
Loss in iteration 180 : 0.021449837009734057
Loss in iteration 181 : 0.0213567619802591
Loss in iteration 182 : 0.02126471033512569
Loss in iteration 183 : 0.021173477161038852
Loss in iteration 184 : 0.021083001971980438
Loss in iteration 185 : 0.02099337368159071
Loss in iteration 186 : 0.020904421093927657
Loss in iteration 187 : 0.02081654054864671
Loss in iteration 188 : 0.020729288323133158
Loss in iteration 189 : 0.02064275275916411
Loss in iteration 190 : 0.020557381166862707
Loss in iteration 191 : 0.02047268758664041
Loss in iteration 192 : 0.020388578709151516
Loss in iteration 193 : 0.0203049265785507
Loss in iteration 194 : 0.02022171249298439
Loss in iteration 195 : 0.020139121038127514
Loss in iteration 196 : 0.02005740877832885
Loss in iteration 197 : 0.019976216125827787
Loss in iteration 198 : 0.019895471114256612
Loss in iteration 199 : 0.019815164871295525
Loss in iteration 200 : 0.019736034231717596
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.9822222222222222, training accuracy 0.9967133466704773, time elapsed: 3551 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 9.028587115115597
Loss in iteration 3 : 4.556104648520197
Loss in iteration 4 : 1.2268795221449436
Loss in iteration 5 : 1.2013419235168126
Loss in iteration 6 : 1.686777917440568
Loss in iteration 7 : 1.9611956368933752
Loss in iteration 8 : 1.925449016893489
Loss in iteration 9 : 1.7061551980134542
Loss in iteration 10 : 1.4080859549999887
Loss in iteration 11 : 1.1265807586126002
Loss in iteration 12 : 0.8924198171365479
Loss in iteration 13 : 0.7110105244726772
Loss in iteration 14 : 0.5698425441528886
Loss in iteration 15 : 0.4665441593346158
Loss in iteration 16 : 0.39849339893636715
Loss in iteration 17 : 0.36012672623960473
Loss in iteration 18 : 0.3418414735693378
Loss in iteration 19 : 0.3370268329274236
Loss in iteration 20 : 0.33371780045237204
Loss in iteration 21 : 0.32950473904441474
Loss in iteration 22 : 0.32652493458349163
Loss in iteration 23 : 0.3223941320240149
Loss in iteration 24 : 0.3170013989833553
Loss in iteration 25 : 0.3088627812121342
Loss in iteration 26 : 0.29786664265986457
Loss in iteration 27 : 0.28519750501702396
Loss in iteration 28 : 0.27130524518526516
Loss in iteration 29 : 0.2568439731392717
Loss in iteration 30 : 0.2413702174540207
Loss in iteration 31 : 0.2255877970113306
Loss in iteration 32 : 0.21006396186726425
Loss in iteration 33 : 0.19570839759648279
Loss in iteration 34 : 0.18194760745721061
Loss in iteration 35 : 0.1689299996192421
Loss in iteration 36 : 0.15736699625213885
Loss in iteration 37 : 0.14878319209367097
Loss in iteration 38 : 0.14317619676905993
Loss in iteration 39 : 0.13843554050477047
Loss in iteration 40 : 0.13454738599326385
Loss in iteration 41 : 0.13099521414524296
Loss in iteration 42 : 0.12811081755579443
Loss in iteration 43 : 0.12562911494291845
Loss in iteration 44 : 0.12328772673261704
Loss in iteration 45 : 0.12125060309443499
Loss in iteration 46 : 0.11949260390454067
Loss in iteration 47 : 0.11810220479427262
Loss in iteration 48 : 0.11676104107075926
Loss in iteration 49 : 0.11542278905375568
Loss in iteration 50 : 0.1140925709955595
Loss in iteration 51 : 0.11271895373390045
Loss in iteration 52 : 0.11135446149799494
Loss in iteration 53 : 0.10996186743129203
Loss in iteration 54 : 0.10854548654439461
Loss in iteration 55 : 0.1071618812583184
Loss in iteration 56 : 0.10579579602527282
Loss in iteration 57 : 0.10443235554414318
Loss in iteration 58 : 0.103072210146336
Loss in iteration 59 : 0.10171591674662099
Loss in iteration 60 : 0.10036395016030485
Loss in iteration 61 : 0.09901671306566816
Loss in iteration 62 : 0.09767454477822185
Testing accuracy  of updater 7 on alg 1 with rate 14.0 = 0.9697777777777777, training accuracy 0.9944269791368963, time elapsed: 1148 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0636009603738619
Loss in iteration 3 : 0.42663253958847186
Loss in iteration 4 : 0.1875245313823248
Loss in iteration 5 : 0.42313241063452045
Loss in iteration 6 : 0.3289714582135135
Loss in iteration 7 : 0.1827564488447514
Loss in iteration 8 : 0.10208471104730556
Loss in iteration 9 : 0.06686491354378386
Loss in iteration 10 : 0.06532275842028132
Loss in iteration 11 : 0.07907309783008316
Loss in iteration 12 : 0.09042200079728296
Loss in iteration 13 : 0.08816659388558054
Loss in iteration 14 : 0.07427341425365684
Loss in iteration 15 : 0.05624302127312539
Loss in iteration 16 : 0.039116435566452244
Loss in iteration 17 : 0.02766919504381612
Loss in iteration 18 : 0.020777731015638876
Loss in iteration 19 : 0.018423963089568503
Loss in iteration 20 : 0.018140768888673876
Loss in iteration 21 : 0.018727349141342668
Loss in iteration 22 : 0.019137785356764064
Loss in iteration 23 : 0.019208804558265298
Loss in iteration 24 : 0.019090600776060677
Loss in iteration 25 : 0.01871701450899076
Loss in iteration 26 : 0.01806592532780572
Loss in iteration 27 : 0.017148513379632146
Loss in iteration 28 : 0.016197787527681314
Loss in iteration 29 : 0.015580514162192851
Loss in iteration 30 : 0.015013339271617653
Loss in iteration 31 : 0.014531527629618011
Loss in iteration 32 : 0.014063246750098178
Loss in iteration 33 : 0.013598372929800021
Loss in iteration 34 : 0.013174669994750777
Loss in iteration 35 : 0.012809511910106307
Loss in iteration 36 : 0.012450344487829971
Loss in iteration 37 : 0.012099094914673973
Loss in iteration 38 : 0.011903444315124604
Loss in iteration 39 : 0.011792210103549304
Loss in iteration 40 : 0.011664149450037778
Loss in iteration 41 : 0.011521425843809477
Loss in iteration 42 : 0.011365943865053336
Loss in iteration 43 : 0.01120421907531745
Loss in iteration 44 : 0.011071825362520181
Loss in iteration 45 : 0.010949644859632237
Loss in iteration 46 : 0.01082639804533597
Loss in iteration 47 : 0.01068848541783128
Loss in iteration 48 : 0.010537003674650325
Loss in iteration 49 : 0.010386653726091306
Loss in iteration 50 : 0.010240411434190902
Loss in iteration 51 : 0.010109192697484214
Loss in iteration 52 : 0.009987147613438503
Loss in iteration 53 : 0.00986520928795304
Loss in iteration 54 : 0.009743456851821469
Loss in iteration 55 : 0.0096299527393148
Testing accuracy  of updater 7 on alg 1 with rate 9.799999999999999 = 0.9768888888888889, training accuracy 0.9988568162332095, time elapsed: 938 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5944742609385294
Loss in iteration 3 : 0.2717691148466347
Loss in iteration 4 : 0.15177050900016958
Loss in iteration 5 : 0.2640765161468636
Loss in iteration 6 : 0.18341010548759842
Loss in iteration 7 : 0.10967296191048727
Loss in iteration 8 : 0.07020062738701759
Loss in iteration 9 : 0.05860831849225154
Loss in iteration 10 : 0.06286296149133583
Loss in iteration 11 : 0.06734135065070752
Loss in iteration 12 : 0.06401377786329597
Loss in iteration 13 : 0.05235889102041672
Loss in iteration 14 : 0.04025651085440287
Loss in iteration 15 : 0.029754810244029966
Loss in iteration 16 : 0.022384725578604363
Loss in iteration 17 : 0.018520195670518276
Loss in iteration 18 : 0.017105941257750258
Loss in iteration 19 : 0.01631421591990663
Loss in iteration 20 : 0.016007891554706125
Loss in iteration 21 : 0.01596780206041569
Loss in iteration 22 : 0.01561267318796347
Loss in iteration 23 : 0.015000961966137753
Loss in iteration 24 : 0.014209394864947553
Loss in iteration 25 : 0.013425623295339528
Loss in iteration 26 : 0.012571824114528908
Loss in iteration 27 : 0.011737653351916113
Loss in iteration 28 : 0.011128657910570258
Loss in iteration 29 : 0.010707877677359107
Loss in iteration 30 : 0.010331353024176195
Loss in iteration 31 : 0.009985564107596915
Loss in iteration 32 : 0.009662406909885509
Loss in iteration 33 : 0.009349901088566146
Loss in iteration 34 : 0.009032695210157584
Loss in iteration 35 : 0.008750492881103095
Loss in iteration 36 : 0.00853573827467821
Loss in iteration 37 : 0.008388214297021171
Loss in iteration 38 : 0.008236591295911231
Loss in iteration 39 : 0.008072932788225797
Loss in iteration 40 : 0.007894966982406005
Loss in iteration 41 : 0.007704601856911049
Loss in iteration 42 : 0.007551504522842236
Loss in iteration 43 : 0.007401509110875619
Loss in iteration 44 : 0.007245952311665941
Loss in iteration 45 : 0.007096609985279921
Loss in iteration 46 : 0.00695122720145224
Loss in iteration 47 : 0.006805216753356311
Loss in iteration 48 : 0.006693984625222498
Loss in iteration 49 : 0.006585457092934623
Loss in iteration 50 : 0.006476112696418294
Loss in iteration 51 : 0.006377995907560983
Loss in iteration 52 : 0.006284093690345719
Loss in iteration 53 : 0.006194326181870755
Loss in iteration 54 : 0.006110331833515709
Loss in iteration 55 : 0.006033489116953503
Loss in iteration 56 : 0.005958263865717532
Loss in iteration 57 : 0.005884524264880589
Loss in iteration 58 : 0.005826468888091553
Testing accuracy  of updater 7 on alg 1 with rate 5.6 = 0.9777777777777777, training accuracy 0.9989997142040583, time elapsed: 1055 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.34807753567702654
Loss in iteration 3 : 0.17069671612196158
Loss in iteration 4 : 0.15680521557861662
Loss in iteration 5 : 0.09405898741542212
Loss in iteration 6 : 0.07603709591232274
Loss in iteration 7 : 0.06608107158251923
Loss in iteration 8 : 0.05568984309290221
Loss in iteration 9 : 0.046540399884257365
Loss in iteration 10 : 0.03910669654618598
Loss in iteration 11 : 0.03330599311712894
Loss in iteration 12 : 0.028612424837868283
Loss in iteration 13 : 0.025102441359099316
Loss in iteration 14 : 0.02231089710932601
Loss in iteration 15 : 0.020095534728266307
Loss in iteration 16 : 0.018064945822234477
Loss in iteration 17 : 0.0161105389214788
Loss in iteration 18 : 0.014466907477905792
Loss in iteration 19 : 0.013115652065516754
Loss in iteration 20 : 0.011917866539215446
Loss in iteration 21 : 0.010922279188839836
Loss in iteration 22 : 0.010151405439452213
Loss in iteration 23 : 0.009560691891639706
Loss in iteration 24 : 0.009018146009589481
Loss in iteration 25 : 0.008532362566662092
Loss in iteration 26 : 0.008122203417580158
Loss in iteration 27 : 0.007809023609357579
Loss in iteration 28 : 0.007525825386254189
Loss in iteration 29 : 0.007267412185172555
Loss in iteration 30 : 0.007023599425651978
Loss in iteration 31 : 0.006777207795643549
Loss in iteration 32 : 0.006529385576353043
Loss in iteration 33 : 0.006282299638555652
Loss in iteration 34 : 0.006038501347615071
Loss in iteration 35 : 0.005813586214146552
Loss in iteration 36 : 0.00560778296171418
Loss in iteration 37 : 0.005428575667041928
Loss in iteration 38 : 0.0052652851560597335
Loss in iteration 39 : 0.005124764651557941
Loss in iteration 40 : 0.004998800492731437
Loss in iteration 41 : 0.0048825170905874285
Loss in iteration 42 : 0.004780276145279435
Loss in iteration 43 : 0.004679442255710759
Loss in iteration 44 : 0.00457994058051454
Loss in iteration 45 : 0.004481702594476899
Loss in iteration 46 : 0.004386362884329071
Loss in iteration 47 : 0.004293966667228823
Loss in iteration 48 : 0.004203095996430819
Loss in iteration 49 : 0.004113647442702366
Loss in iteration 50 : 0.0040255275568975785
Loss in iteration 51 : 0.003938651781588572
Loss in iteration 52 : 0.003853069123421405
Loss in iteration 53 : 0.0037735695399513177
Loss in iteration 54 : 0.003704384718328353
Loss in iteration 55 : 0.003643672568822115
Loss in iteration 56 : 0.0035859571001277007
Loss in iteration 57 : 0.0035301194971669673
Loss in iteration 58 : 0.0034758562389265
Loss in iteration 59 : 0.0034270598135718484
Loss in iteration 60 : 0.003385276031671198
Loss in iteration 61 : 0.003348078091867073
Loss in iteration 62 : 0.0033124002229372273
Loss in iteration 63 : 0.0032773984063862987
Loss in iteration 64 : 0.0032430184068542468
Loss in iteration 65 : 0.0032092114898448175
Testing accuracy  of updater 7 on alg 1 with rate 1.4 = 0.9884444444444445, training accuracy 0.9989997142040583, time elapsed: 1517 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38481241614690587
Loss in iteration 3 : 0.20174903240172604
Loss in iteration 4 : 0.14313403523788287
Loss in iteration 5 : 0.10580761983200798
Loss in iteration 6 : 0.0757880243740086
Loss in iteration 7 : 0.0626710433455108
Loss in iteration 8 : 0.0565032489191521
Loss in iteration 9 : 0.049421310623462424
Loss in iteration 10 : 0.04227497682034846
Loss in iteration 11 : 0.03621605196481008
Loss in iteration 12 : 0.03107385681080424
Loss in iteration 13 : 0.026955284121739625
Loss in iteration 14 : 0.023845051671557163
Loss in iteration 15 : 0.021331456161764552
Loss in iteration 16 : 0.019264260683131078
Loss in iteration 17 : 0.01757026953726968
Loss in iteration 18 : 0.016073615929235847
Loss in iteration 19 : 0.014734032525912233
Loss in iteration 20 : 0.013519597556969734
Loss in iteration 21 : 0.012370244243585583
Loss in iteration 22 : 0.011340237897622278
Loss in iteration 23 : 0.010514710142980626
Loss in iteration 24 : 0.009774848220989221
Loss in iteration 25 : 0.009091374736016754
Loss in iteration 26 : 0.008469829557869117
Loss in iteration 27 : 0.007937369580819361
Loss in iteration 28 : 0.007465503754684641
Loss in iteration 29 : 0.007070400689399656
Loss in iteration 30 : 0.006738434643178868
Loss in iteration 31 : 0.006429367310135002
Loss in iteration 32 : 0.006154956500676898
Loss in iteration 33 : 0.005924818912810797
Loss in iteration 34 : 0.005729623880883327
Loss in iteration 35 : 0.005540952239714578
Loss in iteration 36 : 0.00535783081598176
Loss in iteration 37 : 0.005195371627902438
Loss in iteration 38 : 0.005044589576600728
Loss in iteration 39 : 0.004904361501629085
Loss in iteration 40 : 0.004778570921395162
Loss in iteration 41 : 0.0046655088645285656
Loss in iteration 42 : 0.004561025301584306
Loss in iteration 43 : 0.004460306936800966
Loss in iteration 44 : 0.004361902617868887
Loss in iteration 45 : 0.0042658012596024425
Loss in iteration 46 : 0.004172588198875984
Loss in iteration 47 : 0.004081700727319772
Loss in iteration 48 : 0.0039938131612148224
Loss in iteration 49 : 0.003907680894542538
Loss in iteration 50 : 0.003823529595865228
Loss in iteration 51 : 0.0037446060522841034
Loss in iteration 52 : 0.003668396810053367
Loss in iteration 53 : 0.003595624493042859
Loss in iteration 54 : 0.00352783835456731
Loss in iteration 55 : 0.0034637983040279414
Loss in iteration 56 : 0.0034033651292921198
Loss in iteration 57 : 0.0033444821433613204
Loss in iteration 58 : 0.003287921740992423
Loss in iteration 59 : 0.0032337875883263886
Loss in iteration 60 : 0.0031819926387018087
Loss in iteration 61 : 0.0031307618176254688
Loss in iteration 62 : 0.0030817800614691376
Loss in iteration 63 : 0.0030346984741711105
Loss in iteration 64 : 0.0029905603622420386
Loss in iteration 65 : 0.0029488441948900533
Loss in iteration 66 : 0.0029084353661572478
Loss in iteration 67 : 0.002870595163093924
Loss in iteration 68 : 0.002835639657083583
Loss in iteration 69 : 0.0028021864697339053
Testing accuracy  of updater 7 on alg 1 with rate 0.98 = 0.9884444444444445, training accuracy 0.9991426121749071, time elapsed: 1333 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6069807151579093
Loss in iteration 3 : 0.3565086030527311
Loss in iteration 4 : 0.216818826975779
Loss in iteration 5 : 0.172347238323345
Loss in iteration 6 : 0.13511625954359188
Loss in iteration 7 : 0.1144029972466283
Loss in iteration 8 : 0.11041404201611857
Loss in iteration 9 : 0.09822588171908223
Loss in iteration 10 : 0.07819714868325472
Loss in iteration 11 : 0.0628831892428471
Loss in iteration 12 : 0.05421282557301062
Loss in iteration 13 : 0.049245139389046996
Loss in iteration 14 : 0.04566215307591127
Loss in iteration 15 : 0.04236458716387637
Loss in iteration 16 : 0.03894301262456578
Loss in iteration 17 : 0.03542609062620795
Loss in iteration 18 : 0.03190121768220581
Loss in iteration 19 : 0.028673835703299407
Loss in iteration 20 : 0.026101299255425623
Loss in iteration 21 : 0.02391350351291657
Loss in iteration 22 : 0.022048043081058795
Loss in iteration 23 : 0.020460348941928642
Loss in iteration 24 : 0.019136650075696874
Loss in iteration 25 : 0.01797888615809563
Loss in iteration 26 : 0.01700236396936604
Loss in iteration 27 : 0.016170168546184464
Loss in iteration 28 : 0.015416083481302294
Loss in iteration 29 : 0.014706666542846728
Loss in iteration 30 : 0.014033046237655005
Loss in iteration 31 : 0.013382470260138498
Loss in iteration 32 : 0.012743652902719439
Loss in iteration 33 : 0.012146187634156105
Loss in iteration 34 : 0.01159326478645469
Loss in iteration 35 : 0.011064386444337842
Loss in iteration 36 : 0.01056362027395756
Loss in iteration 37 : 0.01010502174127182
Loss in iteration 38 : 0.009724091522319964
Loss in iteration 39 : 0.009408901229891695
Loss in iteration 40 : 0.009124346042292427
Loss in iteration 41 : 0.008862073372098175
Loss in iteration 42 : 0.008624442820442337
Loss in iteration 43 : 0.008397427821060628
Loss in iteration 44 : 0.008189308006250378
Loss in iteration 45 : 0.007987920321133723
Loss in iteration 46 : 0.0077953397592167885
Loss in iteration 47 : 0.007605897760943389
Loss in iteration 48 : 0.007430730827418684
Loss in iteration 49 : 0.007257036125795424
Loss in iteration 50 : 0.007080531797812649
Loss in iteration 51 : 0.006912037300930807
Loss in iteration 52 : 0.006749668079242965
Loss in iteration 53 : 0.006588216535452156
Loss in iteration 54 : 0.006433444422134743
Loss in iteration 55 : 0.006287241353730454
Loss in iteration 56 : 0.006151210286017766
Loss in iteration 57 : 0.006021119281546879
Loss in iteration 58 : 0.005903352084061303
Loss in iteration 59 : 0.0057955541093549145
Loss in iteration 60 : 0.005694565216030693
Loss in iteration 61 : 0.005593192379294873
Loss in iteration 62 : 0.005490430803208797
Loss in iteration 63 : 0.005391286790857672
Loss in iteration 64 : 0.005295884843241891
Loss in iteration 65 : 0.005205885341566277
Loss in iteration 66 : 0.005118530325316772
Loss in iteration 67 : 0.0050373240063616015
Loss in iteration 68 : 0.004962650068143479
Loss in iteration 69 : 0.004892376159916196
Loss in iteration 70 : 0.0048263722056620835
Loss in iteration 71 : 0.004764556960405331
Loss in iteration 72 : 0.004707553319998005
Loss in iteration 73 : 0.0046519886704965065
Loss in iteration 74 : 0.004599254698333442
Loss in iteration 75 : 0.004549837236899683
Loss in iteration 76 : 0.004501640481575963
Loss in iteration 77 : 0.004454552878569259
Loss in iteration 78 : 0.004408370799586573
Loss in iteration 79 : 0.0043630195240907665
Loss in iteration 80 : 0.004318786990217735
Loss in iteration 81 : 0.0042755799381404445
Loss in iteration 82 : 0.004233144819849342
Loss in iteration 83 : 0.004192439060751124
Loss in iteration 84 : 0.004153042677995046
Testing accuracy  of updater 7 on alg 1 with rate 0.5599999999999999 = 0.9875555555555555, training accuracy 0.9989997142040583, time elapsed: 1641 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9023099099188154
Loss in iteration 3 : 0.7704224607545979
Loss in iteration 4 : 0.6165604895404966
Loss in iteration 5 : 0.4548428945875481
Loss in iteration 6 : 0.37005610502346925
Loss in iteration 7 : 0.2941447449170517
Loss in iteration 8 : 0.23529453808630418
Loss in iteration 9 : 0.20391599421871973
Loss in iteration 10 : 0.18611864958917038
Loss in iteration 11 : 0.17483201419439495
Loss in iteration 12 : 0.16661913493945893
Loss in iteration 13 : 0.15944933813624687
Loss in iteration 14 : 0.1522585727468915
Loss in iteration 15 : 0.1442822545972533
Loss in iteration 16 : 0.1356607439850678
Loss in iteration 17 : 0.12680879322229494
Loss in iteration 18 : 0.11814911454189844
Loss in iteration 19 : 0.11007245723118576
Loss in iteration 20 : 0.1028266402720279
Loss in iteration 21 : 0.09653441180513758
Loss in iteration 22 : 0.0908341339551544
Loss in iteration 23 : 0.085774073944979
Loss in iteration 24 : 0.08111307844903817
Loss in iteration 25 : 0.07687855484262178
Loss in iteration 26 : 0.07312301319268025
Loss in iteration 27 : 0.06971055359930735
Loss in iteration 28 : 0.06663554602356037
Loss in iteration 29 : 0.06381429995827227
Loss in iteration 30 : 0.06125983838418224
Loss in iteration 31 : 0.05892328718809696
Loss in iteration 32 : 0.05674587675142343
Loss in iteration 33 : 0.05471750846095491
Loss in iteration 34 : 0.05285660412532704
Loss in iteration 35 : 0.05119076289903528
Loss in iteration 36 : 0.04965042731762497
Loss in iteration 37 : 0.04818928983861933
Loss in iteration 38 : 0.046810854630067504
Loss in iteration 39 : 0.045515582091612244
Loss in iteration 40 : 0.044292891235878776
Loss in iteration 41 : 0.043132357358519446
Loss in iteration 42 : 0.04203736119268328
Loss in iteration 43 : 0.04099595519306678
Loss in iteration 44 : 0.03998395894731756
Loss in iteration 45 : 0.03900815277063725
Loss in iteration 46 : 0.03807883583177402
Loss in iteration 47 : 0.03719724376000693
Loss in iteration 48 : 0.036349085357020375
Loss in iteration 49 : 0.03554555817629053
Loss in iteration 50 : 0.03478856483434893
Loss in iteration 51 : 0.03406738293361572
Loss in iteration 52 : 0.033380259142785314
Loss in iteration 53 : 0.032717440594341216
Loss in iteration 54 : 0.03208104821560194
Loss in iteration 55 : 0.031470758563956376
Loss in iteration 56 : 0.03089358140093701
Loss in iteration 57 : 0.030348204900069987
Loss in iteration 58 : 0.02982962365974017
Loss in iteration 59 : 0.02934969530069904
Loss in iteration 60 : 0.02889078346067116
Loss in iteration 61 : 0.028446637043160465
Loss in iteration 62 : 0.028025792621009425
Loss in iteration 63 : 0.027620162470872362
Loss in iteration 64 : 0.02722826086570219
Loss in iteration 65 : 0.026854510758213573
Loss in iteration 66 : 0.0264922981118847
Loss in iteration 67 : 0.026140032426783394
Loss in iteration 68 : 0.025796533855920876
Loss in iteration 69 : 0.025465957198124972
Loss in iteration 70 : 0.025145087798981582
Loss in iteration 71 : 0.02483168907632155
Loss in iteration 72 : 0.024526806302255517
Loss in iteration 73 : 0.024230848916901193
Loss in iteration 74 : 0.02394454900900267
Loss in iteration 75 : 0.023665263410702148
Loss in iteration 76 : 0.02339325468083035
Loss in iteration 77 : 0.023127389468574525
Loss in iteration 78 : 0.022866021187047645
Loss in iteration 79 : 0.022610829137472827
Loss in iteration 80 : 0.02236101591493249
Loss in iteration 81 : 0.02211666831156288
Loss in iteration 82 : 0.021877026651244764
Loss in iteration 83 : 0.021642475350608127
Loss in iteration 84 : 0.02141324521957388
Loss in iteration 85 : 0.021187881535204234
Loss in iteration 86 : 0.02096581792200559
Loss in iteration 87 : 0.02074813310570223
Loss in iteration 88 : 0.020534734145559286
Loss in iteration 89 : 0.02032480004362628
Loss in iteration 90 : 0.020118347931005337
Loss in iteration 91 : 0.019915573742405972
Loss in iteration 92 : 0.019718392505202173
Loss in iteration 93 : 0.019525188685421333
Loss in iteration 94 : 0.019334892294716324
Loss in iteration 95 : 0.019147503221769144
Loss in iteration 96 : 0.018962972994528603
Loss in iteration 97 : 0.018780190377477572
Loss in iteration 98 : 0.01859996383428543
Loss in iteration 99 : 0.018422589300226194
Loss in iteration 100 : 0.01824920756585198
Loss in iteration 101 : 0.018077861023354465
Loss in iteration 102 : 0.017908632921661984
Loss in iteration 103 : 0.01774121671807929
Loss in iteration 104 : 0.017576340246999743
Loss in iteration 105 : 0.01741457338565637
Loss in iteration 106 : 0.017254291095507263
Loss in iteration 107 : 0.017095805385154025
Loss in iteration 108 : 0.016940279428721495
Loss in iteration 109 : 0.016786214830722058
Loss in iteration 110 : 0.016634469618608868
Loss in iteration 111 : 0.016485642217955017
Loss in iteration 112 : 0.01634050981680494
Loss in iteration 113 : 0.01619879859182823
Loss in iteration 114 : 0.016059670501652905
Loss in iteration 115 : 0.01592415694888249
Loss in iteration 116 : 0.01579156563590575
Loss in iteration 117 : 0.015661838846022343
Loss in iteration 118 : 0.015533535686819406
Loss in iteration 119 : 0.01540671418349209
Loss in iteration 120 : 0.015281441219409815
Loss in iteration 121 : 0.015157297932506999
Loss in iteration 122 : 0.015034626339212416
Loss in iteration 123 : 0.014913862471815312
Loss in iteration 124 : 0.014794650911872957
Loss in iteration 125 : 0.01467678777414388
Loss in iteration 126 : 0.014560459431158274
Loss in iteration 127 : 0.014445163669425557
Loss in iteration 128 : 0.014331968829806188
Loss in iteration 129 : 0.014220649010825355
Loss in iteration 130 : 0.014110690430700477
Loss in iteration 131 : 0.014001693787863468
Loss in iteration 132 : 0.013893589343004104
Loss in iteration 133 : 0.01378633975072821
Loss in iteration 134 : 0.013680887683871417
Loss in iteration 135 : 0.013577102828670164
Loss in iteration 136 : 0.013474624547832378
Loss in iteration 137 : 0.013373282325160414
Loss in iteration 138 : 0.013272868885834462
Loss in iteration 139 : 0.01317378112842602
Loss in iteration 140 : 0.01307627411312502
Loss in iteration 141 : 0.01298169808736096
Loss in iteration 142 : 0.012888892266489621
Loss in iteration 143 : 0.012800242738643108
Loss in iteration 144 : 0.012713029581962106
Loss in iteration 145 : 0.012628453743424563
Loss in iteration 146 : 0.012545439685580122
Loss in iteration 147 : 0.01246520400325864
Loss in iteration 148 : 0.012386225217248246
Loss in iteration 149 : 0.012310101023023596
Loss in iteration 150 : 0.012235222541241564
Loss in iteration 151 : 0.012161332944591242
Loss in iteration 152 : 0.012088735682895755
Loss in iteration 153 : 0.012017144872848797
Loss in iteration 154 : 0.011946194099712554
Loss in iteration 155 : 0.011875995812257237
Loss in iteration 156 : 0.011807081963186597
Loss in iteration 157 : 0.0117388747508382
Loss in iteration 158 : 0.011671487631103202
Loss in iteration 159 : 0.011604897094340687
Loss in iteration 160 : 0.011538866900684995
Loss in iteration 161 : 0.011474210060604294
Loss in iteration 162 : 0.011410102378009362
Loss in iteration 163 : 0.011346748390609623
Loss in iteration 164 : 0.011284218720749117
Loss in iteration 165 : 0.011222124716160109
Loss in iteration 166 : 0.011160376884696697
Loss in iteration 167 : 0.01109894516226727
Loss in iteration 168 : 0.011037808307212939
Loss in iteration 169 : 0.010976947131605332
Loss in iteration 170 : 0.010916705783943716
Loss in iteration 171 : 0.010856808219690983
Loss in iteration 172 : 0.010797150518835634
Loss in iteration 173 : 0.01073817510411305
Testing accuracy  of updater 7 on alg 1 with rate 0.1399999999999999 = 0.9902222222222222, training accuracy 0.999285510145756, time elapsed: 3464 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.216235808154345
Loss in iteration 3 : 0.6692897592317286
Loss in iteration 4 : 0.22390290221816955
Loss in iteration 5 : 0.24760709421323907
Loss in iteration 6 : 0.2633750757047832
Loss in iteration 7 : 0.25267335123917367
Loss in iteration 8 : 0.2240357608972742
Loss in iteration 9 : 0.1946989070250396
Loss in iteration 10 : 0.16382618986999162
Loss in iteration 11 : 0.13776229182074118
Loss in iteration 12 : 0.11708008767220368
Loss in iteration 13 : 0.1008084385298262
Loss in iteration 14 : 0.08881414002484277
Loss in iteration 15 : 0.0796737318416223
Loss in iteration 16 : 0.07319888315427812
Loss in iteration 17 : 0.06863899630187105
Loss in iteration 18 : 0.06513047987173574
Loss in iteration 19 : 0.06178176778715456
Loss in iteration 20 : 0.058406451152132006
Loss in iteration 21 : 0.05489724284117646
Loss in iteration 22 : 0.0511067787948699
Loss in iteration 23 : 0.04724831452086313
Loss in iteration 24 : 0.04334657607489855
Loss in iteration 25 : 0.03975382975864842
Loss in iteration 26 : 0.03648154503831994
Loss in iteration 27 : 0.03376932749648836
Loss in iteration 28 : 0.03143422188500542
Loss in iteration 29 : 0.029508117747337845
Loss in iteration 30 : 0.028023309723175186
Loss in iteration 31 : 0.026957996238336557
Loss in iteration 32 : 0.02599789176076372
Loss in iteration 33 : 0.025032157132473726
Loss in iteration 34 : 0.02404752830271289
Loss in iteration 35 : 0.023065330624158203
Loss in iteration 36 : 0.022082706810589672
Loss in iteration 37 : 0.021105815828877113
Loss in iteration 38 : 0.020174154759532815
Loss in iteration 39 : 0.019234066547922857
Loss in iteration 40 : 0.01834563541175486
Loss in iteration 41 : 0.01757665978002576
Loss in iteration 42 : 0.016850420343949183
Loss in iteration 43 : 0.016173206993150436
Loss in iteration 44 : 0.0155512913209374
Loss in iteration 45 : 0.014958896047162495
Loss in iteration 46 : 0.014429531866050204
Loss in iteration 47 : 0.013961793583660608
Loss in iteration 48 : 0.013570050146849373
Loss in iteration 49 : 0.013220217982744205
Loss in iteration 50 : 0.012895401786274656
Loss in iteration 51 : 0.01262694057471222
Loss in iteration 52 : 0.012361803584419068
Loss in iteration 53 : 0.012099092431302446
Loss in iteration 54 : 0.011884670217568706
Loss in iteration 55 : 0.011677798774057863
Loss in iteration 56 : 0.01147302795699107
Loss in iteration 57 : 0.0112701449899445
Loss in iteration 58 : 0.01107640871214421
Loss in iteration 59 : 0.010891140185732345
Loss in iteration 60 : 0.010717350140911781
Loss in iteration 61 : 0.010555112018696751
Loss in iteration 62 : 0.01039625439875769
Loss in iteration 63 : 0.010243869138109925
Loss in iteration 64 : 0.010093924226319082
Loss in iteration 65 : 0.009946469672801182
Loss in iteration 66 : 0.009823303335817971
Loss in iteration 67 : 0.009704306518361084
Loss in iteration 68 : 0.009586940876988604
Loss in iteration 69 : 0.009480694996256015
Loss in iteration 70 : 0.009379480793126664
Loss in iteration 71 : 0.009279305455582101
Loss in iteration 72 : 0.009180063859564932
Loss in iteration 73 : 0.009081661392949386
Loss in iteration 74 : 0.008984012901136188
Loss in iteration 75 : 0.008887041738835352
Loss in iteration 76 : 0.008791991860651509
Loss in iteration 77 : 0.008699972339110339
Loss in iteration 78 : 0.008608443814666513
Loss in iteration 79 : 0.008517355944442842
Loss in iteration 80 : 0.008426663409299338
Loss in iteration 81 : 0.008336325411118735
Loss in iteration 82 : 0.00824630522051672
Loss in iteration 83 : 0.00816008253044825
Loss in iteration 84 : 0.008085751631437526
Loss in iteration 85 : 0.008011342680191375
Loss in iteration 86 : 0.007936862296434687
Loss in iteration 87 : 0.007862316431150911
Testing accuracy  of updater 8 on alg 1 with rate 1.4000000000000004 = 0.976, training accuracy 0.9987139182623607, time elapsed: 1679 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0988209300179255
Loss in iteration 3 : 0.3272458102459776
Loss in iteration 4 : 0.17828121514613146
Loss in iteration 5 : 0.20178106209966334
Loss in iteration 6 : 0.18529911556880133
Loss in iteration 7 : 0.15562217029684913
Loss in iteration 8 : 0.12352440662510969
Loss in iteration 9 : 0.09816561783232465
Loss in iteration 10 : 0.07963792193118213
Loss in iteration 11 : 0.0668389849493895
Loss in iteration 12 : 0.058618605758506054
Loss in iteration 13 : 0.05263941085740634
Loss in iteration 14 : 0.0487911388248045
Loss in iteration 15 : 0.04555378942565133
Loss in iteration 16 : 0.042282065218853276
Loss in iteration 17 : 0.03843852374861154
Loss in iteration 18 : 0.034553135525433175
Loss in iteration 19 : 0.0310989679448838
Loss in iteration 20 : 0.027999499996301577
Loss in iteration 21 : 0.025395410648713996
Loss in iteration 22 : 0.023200812112643594
Loss in iteration 23 : 0.02172854373082196
Loss in iteration 24 : 0.020753153212847035
Loss in iteration 25 : 0.019961966419938904
Loss in iteration 26 : 0.019150194117673342
Loss in iteration 27 : 0.01832863757716534
Loss in iteration 28 : 0.01748205070383906
Loss in iteration 29 : 0.016608927188455507
Loss in iteration 30 : 0.01575201618980234
Loss in iteration 31 : 0.014948724588048369
Loss in iteration 32 : 0.01419088655898948
Loss in iteration 33 : 0.013486421652095253
Loss in iteration 34 : 0.012824001636006728
Loss in iteration 35 : 0.012196579731561817
Loss in iteration 36 : 0.011650963040633415
Loss in iteration 37 : 0.011217102178268006
Loss in iteration 38 : 0.010847943713259534
Loss in iteration 39 : 0.010512387677527676
Loss in iteration 40 : 0.010204701935386728
Loss in iteration 41 : 0.009935041670639723
Loss in iteration 42 : 0.009720181921394545
Loss in iteration 43 : 0.009504029424023468
Loss in iteration 44 : 0.009288969215570627
Loss in iteration 45 : 0.00908527537580321
Loss in iteration 46 : 0.008897774859788146
Loss in iteration 47 : 0.008729727199785249
Loss in iteration 48 : 0.008574930661443042
Loss in iteration 49 : 0.008440487654396278
Loss in iteration 50 : 0.008308603612872443
Loss in iteration 51 : 0.008179059192311293
Loss in iteration 52 : 0.008051657467139916
Loss in iteration 53 : 0.007926221487916128
Loss in iteration 54 : 0.007805424958154204
Loss in iteration 55 : 0.007691001437664
Loss in iteration 56 : 0.007578848212376688
Loss in iteration 57 : 0.00747437282354875
Loss in iteration 58 : 0.007374514984918995
Loss in iteration 59 : 0.007275567777003805
Loss in iteration 60 : 0.007177471723078184
Loss in iteration 61 : 0.007084123534639482
Loss in iteration 62 : 0.006994814594297959
Loss in iteration 63 : 0.006906741369027474
Loss in iteration 64 : 0.006819802622717665
Loss in iteration 65 : 0.006733907180504535
Loss in iteration 66 : 0.006649967098754197
Loss in iteration 67 : 0.00657980387712957
Loss in iteration 68 : 0.006511113915795585
Loss in iteration 69 : 0.006442469692838678
Loss in iteration 70 : 0.006373888506153315
Loss in iteration 71 : 0.006305385345444902
Loss in iteration 72 : 0.006236973152587375
Loss in iteration 73 : 0.006168663053351128
Loss in iteration 74 : 0.006100464563664159
Loss in iteration 75 : 0.006032385773214076
Loss in iteration 76 : 0.005973723439999373
Loss in iteration 77 : 0.005916272092313053
Testing accuracy  of updater 8 on alg 1 with rate 0.9800000000000001 = 0.9697777777777777, training accuracy 0.9987139182623607, time elapsed: 1472 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7366844051172666
Loss in iteration 3 : 0.22781015894546916
Loss in iteration 4 : 0.14215146453086314
Loss in iteration 5 : 0.14392789153445168
Loss in iteration 6 : 0.12062523985904702
Loss in iteration 7 : 0.09912037943776189
Loss in iteration 8 : 0.08228837811235488
Loss in iteration 9 : 0.06873927013555008
Loss in iteration 10 : 0.05794436178648
Loss in iteration 11 : 0.049552034171738236
Loss in iteration 12 : 0.04372086296616673
Loss in iteration 13 : 0.038944604565515975
Loss in iteration 14 : 0.034774994098502224
Loss in iteration 15 : 0.031356794942248065
Loss in iteration 16 : 0.028553919218239604
Loss in iteration 17 : 0.025797983879276697
Loss in iteration 18 : 0.02345613743046808
Loss in iteration 19 : 0.021460622996914844
Loss in iteration 20 : 0.019743945788033724
Loss in iteration 21 : 0.018308573639859608
Loss in iteration 22 : 0.017157558789475006
Loss in iteration 23 : 0.0162515658651105
Loss in iteration 24 : 0.01546932331129261
Loss in iteration 25 : 0.01481018506020375
Loss in iteration 26 : 0.014161400878052574
Loss in iteration 27 : 0.013556506542701402
Loss in iteration 28 : 0.012959132616792537
Loss in iteration 29 : 0.012375061807308853
Loss in iteration 30 : 0.011793170870613696
Loss in iteration 31 : 0.011209941994900585
Loss in iteration 32 : 0.010657124694578314
Loss in iteration 33 : 0.01013861929234346
Loss in iteration 34 : 0.00963522299346235
Loss in iteration 35 : 0.00917574521615363
Loss in iteration 36 : 0.008772055969003253
Loss in iteration 37 : 0.008446739896858776
Loss in iteration 38 : 0.008127625808796828
Loss in iteration 39 : 0.007812756305712683
Loss in iteration 40 : 0.007538460895627037
Loss in iteration 41 : 0.007293137598409064
Loss in iteration 42 : 0.007103046016872533
Loss in iteration 43 : 0.006920963719064975
Loss in iteration 44 : 0.00674549681209032
Loss in iteration 45 : 0.0065787660419983
Loss in iteration 46 : 0.006431384628069132
Loss in iteration 47 : 0.0062894275989598715
Loss in iteration 48 : 0.00615051150186132
Loss in iteration 49 : 0.006014390294140545
Loss in iteration 50 : 0.005887026937902944
Loss in iteration 51 : 0.005768599439762101
Loss in iteration 52 : 0.005667890377365911
Loss in iteration 53 : 0.005572762053740899
Loss in iteration 54 : 0.005480091949477356
Loss in iteration 55 : 0.005389657079210791
Loss in iteration 56 : 0.005305485615163788
Loss in iteration 57 : 0.005233663257631856
Loss in iteration 58 : 0.005163709112139852
Loss in iteration 59 : 0.005095452228908678
Loss in iteration 60 : 0.0050296718140725345
Loss in iteration 61 : 0.004967062279096036
Loss in iteration 62 : 0.004905791726367444
Loss in iteration 63 : 0.004847969315103892
Loss in iteration 64 : 0.004795055624021285
Loss in iteration 65 : 0.004742657685847234
Loss in iteration 66 : 0.004690742987268547
Loss in iteration 67 : 0.0046392820049182965
Loss in iteration 68 : 0.004590775378788671
Loss in iteration 69 : 0.00454341953794479
Loss in iteration 70 : 0.00449670145139906
Loss in iteration 71 : 0.004450681625474812
Loss in iteration 72 : 0.0044050233122055514
Loss in iteration 73 : 0.0043599553282335805
Loss in iteration 74 : 0.004315351600941257
Testing accuracy  of updater 8 on alg 1 with rate 0.56 = 0.9733333333333334, training accuracy 0.9987139182623607, time elapsed: 1431 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.33259143126082075
Loss in iteration 3 : 0.23815922938535913
Loss in iteration 4 : 0.17565057647560586
Loss in iteration 5 : 0.12385499450791045
Loss in iteration 6 : 0.0824399385400009
Loss in iteration 7 : 0.0701292791657765
Loss in iteration 8 : 0.06571030411727807
Loss in iteration 9 : 0.05951981001758253
Loss in iteration 10 : 0.05229356590511606
Loss in iteration 11 : 0.04604520575935345
Loss in iteration 12 : 0.04069138680764454
Loss in iteration 13 : 0.03603335051735916
Loss in iteration 14 : 0.03195435145954545
Loss in iteration 15 : 0.028836652170859484
Loss in iteration 16 : 0.026062638274406184
Loss in iteration 17 : 0.023957910420712283
Loss in iteration 18 : 0.022141620292023847
Loss in iteration 19 : 0.020557734738330315
Loss in iteration 20 : 0.019186275324842947
Loss in iteration 21 : 0.017998597196805185
Loss in iteration 22 : 0.01698002200773314
Loss in iteration 23 : 0.016086618659687475
Loss in iteration 24 : 0.015318306937166614
Loss in iteration 25 : 0.014655101899920557
Loss in iteration 26 : 0.014085364302886724
Loss in iteration 27 : 0.013574491541660001
Loss in iteration 28 : 0.013097043703262616
Loss in iteration 29 : 0.012635118694330077
Loss in iteration 30 : 0.012211453955043432
Loss in iteration 31 : 0.011809491212921686
Loss in iteration 32 : 0.011432893645822003
Loss in iteration 33 : 0.01107215116336447
Loss in iteration 34 : 0.010719473805868702
Loss in iteration 35 : 0.010371105999500281
Loss in iteration 36 : 0.010028290764989145
Loss in iteration 37 : 0.009695027162666153
Loss in iteration 38 : 0.009374504931537759
Loss in iteration 39 : 0.009073407218386056
Loss in iteration 40 : 0.008782755860036335
Loss in iteration 41 : 0.00850123541495475
Loss in iteration 42 : 0.008225725246473014
Loss in iteration 43 : 0.007956511677051435
Loss in iteration 44 : 0.00769590689922938
Loss in iteration 45 : 0.007447857268635763
Loss in iteration 46 : 0.007213478787025928
Loss in iteration 47 : 0.007000022347780193
Loss in iteration 48 : 0.006790051725847718
Loss in iteration 49 : 0.006587286163109265
Loss in iteration 50 : 0.00639569970814893
Loss in iteration 51 : 0.006216872918820132
Loss in iteration 52 : 0.0060466780333942845
Loss in iteration 53 : 0.005883407250893129
Loss in iteration 54 : 0.005726673854750818
Loss in iteration 55 : 0.005577924531664578
Loss in iteration 56 : 0.0054435187348099495
Loss in iteration 57 : 0.005320743884797253
Loss in iteration 58 : 0.0052011342551859675
Loss in iteration 59 : 0.005087251644612401
Loss in iteration 60 : 0.004979259000241728
Loss in iteration 61 : 0.00487379711921062
Loss in iteration 62 : 0.004774568431087132
Loss in iteration 63 : 0.004677768050213612
Loss in iteration 64 : 0.004584629908900001
Loss in iteration 65 : 0.004495436494126996
Loss in iteration 66 : 0.00440898175545522
Loss in iteration 67 : 0.0043262083198933494
Loss in iteration 68 : 0.00424632444642313
Loss in iteration 69 : 0.00416769263850299
Loss in iteration 70 : 0.004090844709794575
Loss in iteration 71 : 0.004018407013853485
Loss in iteration 72 : 0.0039503256806892265
Loss in iteration 73 : 0.003883659092384731
Loss in iteration 74 : 0.0038198159362408993
Loss in iteration 75 : 0.0037577459113036053
Loss in iteration 76 : 0.003698020237808342
Loss in iteration 77 : 0.0036403090032079343
Loss in iteration 78 : 0.003583676953561221
Loss in iteration 79 : 0.003527704590913568
Loss in iteration 80 : 0.0034732564859154902
Loss in iteration 81 : 0.003419919510013537
Loss in iteration 82 : 0.003368930311233195
Loss in iteration 83 : 0.003323191609583493
Loss in iteration 84 : 0.0032799862641666915
Loss in iteration 85 : 0.003237280608569247
Loss in iteration 86 : 0.0031949273198132776
Loss in iteration 87 : 0.003153299847815487
Loss in iteration 88 : 0.0031131233195277165
Loss in iteration 89 : 0.0030740741607942004
Loss in iteration 90 : 0.003035908824218964
Loss in iteration 91 : 0.00299959564252071
Loss in iteration 92 : 0.0029636052282716783
Loss in iteration 93 : 0.002927960272492317
Loss in iteration 94 : 0.0028933263304431384
Loss in iteration 95 : 0.00286069445458132
Loss in iteration 96 : 0.002828611202472769
Loss in iteration 97 : 0.002797334950719961
Loss in iteration 98 : 0.0027663450267770276
Loss in iteration 99 : 0.0027356026467981135
Loss in iteration 100 : 0.0027054606482392256
Loss in iteration 101 : 0.0026749189507228126
Loss in iteration 102 : 0.00264465716978481
Loss in iteration 103 : 0.0026147515953116747
Loss in iteration 104 : 0.0025864733387511315
Loss in iteration 105 : 0.0025591984341857966
Loss in iteration 106 : 0.002533300596692558
Loss in iteration 107 : 0.0025095529404741235
Loss in iteration 108 : 0.00248644715842511
Loss in iteration 109 : 0.0024646982184320823
Testing accuracy  of updater 8 on alg 1 with rate 0.14 = 0.9831111111111112, training accuracy 0.999285510145756, time elapsed: 2136 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.31050015104219403
Loss in iteration 3 : 0.3756083518294534
Loss in iteration 4 : 0.19234475401709852
Loss in iteration 5 : 0.16918225669993847
Loss in iteration 6 : 0.13170461162844183
Loss in iteration 7 : 0.09320689079181307
Loss in iteration 8 : 0.07278840938094017
Loss in iteration 9 : 0.0672466903399115
Loss in iteration 10 : 0.06357033114361695
Loss in iteration 11 : 0.058621707670687834
Loss in iteration 12 : 0.05264695874904357
Loss in iteration 13 : 0.04682476507466574
Loss in iteration 14 : 0.04150086028713564
Loss in iteration 15 : 0.03735749497690189
Loss in iteration 16 : 0.03403081956021704
Loss in iteration 17 : 0.03121288966505815
Loss in iteration 18 : 0.02888315469215073
Loss in iteration 19 : 0.0268255108911337
Loss in iteration 20 : 0.02497237861019587
Loss in iteration 21 : 0.023363430697289578
Loss in iteration 22 : 0.021992942947056622
Loss in iteration 23 : 0.020781463592484284
Loss in iteration 24 : 0.019699322672634512
Loss in iteration 25 : 0.01871548249428157
Loss in iteration 26 : 0.017834859049799125
Loss in iteration 27 : 0.017087535384941625
Loss in iteration 28 : 0.016410307507979446
Loss in iteration 29 : 0.015795157851687135
Loss in iteration 30 : 0.01525633760253174
Loss in iteration 31 : 0.014794599864929226
Loss in iteration 32 : 0.01438689418749633
Loss in iteration 33 : 0.014002314837024056
Loss in iteration 34 : 0.0136372810706926
Loss in iteration 35 : 0.013290556507277815
Loss in iteration 36 : 0.012956652386449621
Loss in iteration 37 : 0.012627854480291389
Loss in iteration 38 : 0.012307867759741006
Loss in iteration 39 : 0.011995943206300642
Loss in iteration 40 : 0.011691926206954213
Loss in iteration 41 : 0.011401249951895552
Loss in iteration 42 : 0.011128312079572066
Loss in iteration 43 : 0.010869481988674256
Loss in iteration 44 : 0.010627204099740079
Loss in iteration 45 : 0.01039760650306419
Loss in iteration 46 : 0.010180175331696364
Loss in iteration 47 : 0.009970841270478809
Loss in iteration 48 : 0.009766973339752676
Loss in iteration 49 : 0.009575416999030634
Loss in iteration 50 : 0.009389531779515084
Loss in iteration 51 : 0.009206460734433364
Loss in iteration 52 : 0.009026583689436476
Loss in iteration 53 : 0.008848695091718279
Loss in iteration 54 : 0.008673073163239918
Loss in iteration 55 : 0.008500201984051237
Loss in iteration 56 : 0.008329371205011503
Loss in iteration 57 : 0.008160910787446884
Loss in iteration 58 : 0.007995785993959035
Loss in iteration 59 : 0.007833931009807274
Loss in iteration 60 : 0.007677013241827967
Loss in iteration 61 : 0.007529248024527924
Loss in iteration 62 : 0.007387280200510337
Loss in iteration 63 : 0.007250111696625708
Loss in iteration 64 : 0.007130060992451304
Loss in iteration 65 : 0.007016740936731477
Loss in iteration 66 : 0.006905911690784209
Loss in iteration 67 : 0.006797968124352552
Loss in iteration 68 : 0.0066919886440223
Loss in iteration 69 : 0.0065883497127440845
Loss in iteration 70 : 0.006485912285340208
Loss in iteration 71 : 0.006384345265214449
Loss in iteration 72 : 0.006283609536009193
Loss in iteration 73 : 0.006184744468690525
Loss in iteration 74 : 0.00608652921127782
Loss in iteration 75 : 0.0059894964082656465
Loss in iteration 76 : 0.005893731722657981
Loss in iteration 77 : 0.005799407130854703
Loss in iteration 78 : 0.005705830229586356
Loss in iteration 79 : 0.00561296380851062
Loss in iteration 80 : 0.005523597759203014
Loss in iteration 81 : 0.005437058211488813
Loss in iteration 82 : 0.005352024832638664
Loss in iteration 83 : 0.005271524034572791
Loss in iteration 84 : 0.005192979375936495
Loss in iteration 85 : 0.005116506121326873
Loss in iteration 86 : 0.0050426917338173135
Loss in iteration 87 : 0.004970469879771079
Loss in iteration 88 : 0.004900441254289777
Loss in iteration 89 : 0.004831368135575754
Loss in iteration 90 : 0.004764414749656911
Loss in iteration 91 : 0.004698618200689187
Loss in iteration 92 : 0.004634452740872317
Loss in iteration 93 : 0.00457292170586322
Loss in iteration 94 : 0.004514154823572872
Loss in iteration 95 : 0.004457917415741301
Loss in iteration 96 : 0.004404020141004666
Loss in iteration 97 : 0.004351247251719261
Loss in iteration 98 : 0.004299032979552056
Loss in iteration 99 : 0.004247335953420991
Loss in iteration 100 : 0.004196175572278495
Loss in iteration 101 : 0.004145625631102685
Loss in iteration 102 : 0.004095759607411102
Loss in iteration 103 : 0.004046492017290705
Loss in iteration 104 : 0.003998257330416601
Loss in iteration 105 : 0.003951690968793128
Loss in iteration 106 : 0.003906938786796032
Loss in iteration 107 : 0.003862725521019879
Loss in iteration 108 : 0.00381887910841987
Loss in iteration 109 : 0.0037764476801536296
Loss in iteration 110 : 0.0037351519670432653
Loss in iteration 111 : 0.003694238048758689
Loss in iteration 112 : 0.003654622964736738
Loss in iteration 113 : 0.0036174910468524915
Loss in iteration 114 : 0.0035814531228324075
Loss in iteration 115 : 0.0035459717434480585
Loss in iteration 116 : 0.0035108247680702294
Loss in iteration 117 : 0.0034759868389783537
Loss in iteration 118 : 0.0034416761970784944
Loss in iteration 119 : 0.0034079296090701082
Loss in iteration 120 : 0.0033753560459273903
Loss in iteration 121 : 0.0033431462346704137
Loss in iteration 122 : 0.0033116073537816986
Loss in iteration 123 : 0.0032803506658774426
Loss in iteration 124 : 0.0032496783025753356
Testing accuracy  of updater 8 on alg 1 with rate 0.098 = 0.984, training accuracy 0.999285510145756, time elapsed: 2486 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4279173895052459
Loss in iteration 3 : 0.24569581966866041
Loss in iteration 4 : 0.19028243406642087
Loss in iteration 5 : 0.16973421153361762
Loss in iteration 6 : 0.1498494885733662
Loss in iteration 7 : 0.13072987592744528
Loss in iteration 8 : 0.11354703547734867
Loss in iteration 9 : 0.0992434648450692
Loss in iteration 10 : 0.08851484128375108
Loss in iteration 11 : 0.07989920458315664
Loss in iteration 12 : 0.07239026840779184
Loss in iteration 13 : 0.06618804767282842
Loss in iteration 14 : 0.06120702929581116
Loss in iteration 15 : 0.0569058698800312
Loss in iteration 16 : 0.05307728040596268
Loss in iteration 17 : 0.04962039806079377
Loss in iteration 18 : 0.04653102905103817
Loss in iteration 19 : 0.04383971437219932
Loss in iteration 20 : 0.04138166268330425
Loss in iteration 21 : 0.03915088952297818
Loss in iteration 22 : 0.037062934291961135
Loss in iteration 23 : 0.035099377046346136
Loss in iteration 24 : 0.03325110534761671
Loss in iteration 25 : 0.031538508865264624
Loss in iteration 26 : 0.029929323354230885
Loss in iteration 27 : 0.02843551286114992
Loss in iteration 28 : 0.027029661892188687
Loss in iteration 29 : 0.025741780997120885
Loss in iteration 30 : 0.024651262895929775
Loss in iteration 31 : 0.02365760969851212
Loss in iteration 32 : 0.02273681589918796
Loss in iteration 33 : 0.021885317884850833
Loss in iteration 34 : 0.021143685910570312
Loss in iteration 35 : 0.020480251378304747
Loss in iteration 36 : 0.019873649701575687
Loss in iteration 37 : 0.019322275079316123
Loss in iteration 38 : 0.018806939442625844
Loss in iteration 39 : 0.01833414079167419
Loss in iteration 40 : 0.017892490661581287
Loss in iteration 41 : 0.01748865696681044
Loss in iteration 42 : 0.017107715349568862
Loss in iteration 43 : 0.016745244422246047
Loss in iteration 44 : 0.01641185739301904
Loss in iteration 45 : 0.01609755746522309
Loss in iteration 46 : 0.015800950813114913
Loss in iteration 47 : 0.015528894758619906
Loss in iteration 48 : 0.015269114849520982
Loss in iteration 49 : 0.015020512494517777
Loss in iteration 50 : 0.014782911526169142
Loss in iteration 51 : 0.014560069124055428
Loss in iteration 52 : 0.014352205500490656
Loss in iteration 53 : 0.014150196099205424
Loss in iteration 54 : 0.013954182433041282
Loss in iteration 55 : 0.013764986502014305
Loss in iteration 56 : 0.013580998509672475
Loss in iteration 57 : 0.013401691807114008
Loss in iteration 58 : 0.013227117175018618
Loss in iteration 59 : 0.013055785127396146
Loss in iteration 60 : 0.01288808654575098
Loss in iteration 61 : 0.012724407857615435
Loss in iteration 62 : 0.012565313961132179
Loss in iteration 63 : 0.01241350044270606
Loss in iteration 64 : 0.012267342757706232
Loss in iteration 65 : 0.012126901088666852
Loss in iteration 66 : 0.011992218264083155
Loss in iteration 67 : 0.011860957383123143
Loss in iteration 68 : 0.011731858121940882
Loss in iteration 69 : 0.011604971914673048
Loss in iteration 70 : 0.011480679435870857
Loss in iteration 71 : 0.011360021889435119
Loss in iteration 72 : 0.011242341884328816
Loss in iteration 73 : 0.011126738927715775
Loss in iteration 74 : 0.011012679980805783
Loss in iteration 75 : 0.01090148356830069
Loss in iteration 76 : 0.010791509543132561
Loss in iteration 77 : 0.010682760219232127
Loss in iteration 78 : 0.010575778672742982
Loss in iteration 79 : 0.010470152326519419
Loss in iteration 80 : 0.0103666615769263
Loss in iteration 81 : 0.010265083530095176
Loss in iteration 82 : 0.010164584463585901
Loss in iteration 83 : 0.010065636328847277
Loss in iteration 84 : 0.009968308718974346
Loss in iteration 85 : 0.00987252267114824
Loss in iteration 86 : 0.009778391732714338
Loss in iteration 87 : 0.00968656929945782
Loss in iteration 88 : 0.009596738833908347
Loss in iteration 89 : 0.00950858816710586
Loss in iteration 90 : 0.0094214362244762
Loss in iteration 91 : 0.009334936505211238
Loss in iteration 92 : 0.009249024407970345
Loss in iteration 93 : 0.009164194537718868
Loss in iteration 94 : 0.009080261782200161
Loss in iteration 95 : 0.008996865858058582
Loss in iteration 96 : 0.008914436780544226
Loss in iteration 97 : 0.00883274458155874
Loss in iteration 98 : 0.008751535139969544
Loss in iteration 99 : 0.008670776376435915
Loss in iteration 100 : 0.008590715647476944
Loss in iteration 101 : 0.008511505173302658
Loss in iteration 102 : 0.008432845019439908
Loss in iteration 103 : 0.008354601826269584
Loss in iteration 104 : 0.008276879034656416
Loss in iteration 105 : 0.008199864399041842
Loss in iteration 106 : 0.008123445722478782
Loss in iteration 107 : 0.00804788498416892
Loss in iteration 108 : 0.007973342013162526
Loss in iteration 109 : 0.007900172403533127
Loss in iteration 110 : 0.007827856008881405
Loss in iteration 111 : 0.007755984030938793
Loss in iteration 112 : 0.007684551199577192
Loss in iteration 113 : 0.0076138861365298675
Loss in iteration 114 : 0.0075442211063023006
Loss in iteration 115 : 0.0074754372196444
Loss in iteration 116 : 0.007407038230085696
Loss in iteration 117 : 0.007339475604453414
Loss in iteration 118 : 0.00727227081955833
Loss in iteration 119 : 0.00720560893703259
Loss in iteration 120 : 0.007139481132067023
Loss in iteration 121 : 0.007074241204247886
Loss in iteration 122 : 0.007009438216791756
Loss in iteration 123 : 0.006945100115566857
Loss in iteration 124 : 0.006881750001179147
Loss in iteration 125 : 0.006820327965317325
Loss in iteration 126 : 0.006760906330752013
Loss in iteration 127 : 0.006702479695979957
Loss in iteration 128 : 0.006645222624735241
Loss in iteration 129 : 0.006589110639249504
Loss in iteration 130 : 0.006533668212912168
Loss in iteration 131 : 0.006479030994257532
Loss in iteration 132 : 0.006424744172001458
Loss in iteration 133 : 0.0063710990857347135
Loss in iteration 134 : 0.006318145564635092
Loss in iteration 135 : 0.006265907593132664
Loss in iteration 136 : 0.006214262779747038
Loss in iteration 137 : 0.0061632133137883875
Loss in iteration 138 : 0.00611298587892615
Loss in iteration 139 : 0.006063366058029124
Loss in iteration 140 : 0.006014250658453008
Loss in iteration 141 : 0.005965481073366919
Loss in iteration 142 : 0.005917468095966435
Loss in iteration 143 : 0.005870187586090863
Loss in iteration 144 : 0.005823773066324625
Loss in iteration 145 : 0.005777766293957865
Loss in iteration 146 : 0.005732355405554489
Loss in iteration 147 : 0.005688050947937319
Loss in iteration 148 : 0.005644242484338105
Loss in iteration 149 : 0.005601594771621026
Loss in iteration 150 : 0.005559507854108475
Loss in iteration 151 : 0.005518538756028917
Loss in iteration 152 : 0.0054787561261401396
Loss in iteration 153 : 0.005439278501983105
Loss in iteration 154 : 0.005400114632585565
Loss in iteration 155 : 0.005361846586755461
Loss in iteration 156 : 0.005324150918227531
Loss in iteration 157 : 0.0052868185835440356
Loss in iteration 158 : 0.0052501912696013755
Loss in iteration 159 : 0.005214047372793741
Loss in iteration 160 : 0.005178668938754089
Loss in iteration 161 : 0.005144221653413463
Loss in iteration 162 : 0.005110595038365719
Loss in iteration 163 : 0.005078106409405001
Loss in iteration 164 : 0.005045878055771646
Loss in iteration 165 : 0.005014150521973035
Loss in iteration 166 : 0.004983239599712736
Loss in iteration 167 : 0.004953416454550237
Loss in iteration 168 : 0.004923914976391743
Loss in iteration 169 : 0.004894712066063733
Loss in iteration 170 : 0.004866639123740652
Loss in iteration 171 : 0.004839252120768234
Loss in iteration 172 : 0.004812272042656696
Loss in iteration 173 : 0.004785654582255306
Loss in iteration 174 : 0.004759336277317543
Loss in iteration 175 : 0.00473320210642005
Loss in iteration 176 : 0.004707237984783413
Loss in iteration 177 : 0.004681480767093405
Loss in iteration 178 : 0.004655928138243387
Loss in iteration 179 : 0.004630504928764893
Loss in iteration 180 : 0.00460524531971951
Loss in iteration 181 : 0.00458005802463154
Loss in iteration 182 : 0.004554995642753772
Loss in iteration 183 : 0.0045300969851792584
Loss in iteration 184 : 0.0045054333676136545
Loss in iteration 185 : 0.004480851946980217
Loss in iteration 186 : 0.004456356639444367
Loss in iteration 187 : 0.004432347776944051
Loss in iteration 188 : 0.004409080230850057
Loss in iteration 189 : 0.004386174166402905
Loss in iteration 190 : 0.004363876599862562
Loss in iteration 191 : 0.004341786580196774
Loss in iteration 192 : 0.004319912891211844
Loss in iteration 193 : 0.004299200718752
Loss in iteration 194 : 0.004278935740336634
Loss in iteration 195 : 0.004258633721247103
Loss in iteration 196 : 0.004238335051671417
Loss in iteration 197 : 0.004218081765663017
Loss in iteration 198 : 0.0041978204859842395
Loss in iteration 199 : 0.0041776109012412405
Loss in iteration 200 : 0.004157462672079387
Testing accuracy  of updater 8 on alg 1 with rate 0.05600000000000001 = 0.9866666666666667, training accuracy 0.9994284081166047, time elapsed: 3964 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8432188215696261
Loss in iteration 3 : 0.6943603792242213
Loss in iteration 4 : 0.5481464190418025
Loss in iteration 5 : 0.41976436434760456
Loss in iteration 6 : 0.33505970863739687
Loss in iteration 7 : 0.26551917501192995
Loss in iteration 8 : 0.23170617094330065
Loss in iteration 9 : 0.21511024342572593
Loss in iteration 10 : 0.2049153775081121
Loss in iteration 11 : 0.19749175110379621
Loss in iteration 12 : 0.19134547531011747
Loss in iteration 13 : 0.1860125438366254
Loss in iteration 14 : 0.18100389997430208
Loss in iteration 15 : 0.1761105385311247
Loss in iteration 16 : 0.17131512035219298
Loss in iteration 17 : 0.1666378946006032
Loss in iteration 18 : 0.16205593657342326
Loss in iteration 19 : 0.15756177325923404
Loss in iteration 20 : 0.15316505576205305
Loss in iteration 21 : 0.1489141537344626
Loss in iteration 22 : 0.14479929551557105
Loss in iteration 23 : 0.14079543117082752
Loss in iteration 24 : 0.13691203774520008
Loss in iteration 25 : 0.13315611877770767
Loss in iteration 26 : 0.12953243522947122
Loss in iteration 27 : 0.1260341965810971
Loss in iteration 28 : 0.12263554862042302
Loss in iteration 29 : 0.11937070325885635
Loss in iteration 30 : 0.11621380352107027
Loss in iteration 31 : 0.11319200208452558
Loss in iteration 32 : 0.11036807263141431
Loss in iteration 33 : 0.10767498740832136
Loss in iteration 34 : 0.10507646665257168
Loss in iteration 35 : 0.10259126973304865
Loss in iteration 36 : 0.1002487333536811
Loss in iteration 37 : 0.0980437945967661
Loss in iteration 38 : 0.09593044428939641
Loss in iteration 39 : 0.09390553009145675
Loss in iteration 40 : 0.09198794458774141
Loss in iteration 41 : 0.09017056859147478
Loss in iteration 42 : 0.08844980951996118
Loss in iteration 43 : 0.08682629080559505
Loss in iteration 44 : 0.0852831834379333
Loss in iteration 45 : 0.08381714887176657
Loss in iteration 46 : 0.0824145821962889
Loss in iteration 47 : 0.08106683738187931
Loss in iteration 48 : 0.07978114508606673
Loss in iteration 49 : 0.07854826640554338
Loss in iteration 50 : 0.0773702702834381
Loss in iteration 51 : 0.07625446122624048
Loss in iteration 52 : 0.07518865182063215
Loss in iteration 53 : 0.07417265924330167
Loss in iteration 54 : 0.07319481189216016
Loss in iteration 55 : 0.07226059698182788
Loss in iteration 56 : 0.07136517001801755
Loss in iteration 57 : 0.07050245472053639
Loss in iteration 58 : 0.06966651036968381
Loss in iteration 59 : 0.0688515967734797
Loss in iteration 60 : 0.06805823479847564
Loss in iteration 61 : 0.06728697646899968
Loss in iteration 62 : 0.06654007916060635
Loss in iteration 63 : 0.06581503674951185
Loss in iteration 64 : 0.0651067358858672
Loss in iteration 65 : 0.0644156313724172
Loss in iteration 66 : 0.06373926485738439
Loss in iteration 67 : 0.06307776776863959
Loss in iteration 68 : 0.062432054627967096
Loss in iteration 69 : 0.061803843603711196
Loss in iteration 70 : 0.061196173190936884
Loss in iteration 71 : 0.06060568095300946
Loss in iteration 72 : 0.06003040312060479
Loss in iteration 73 : 0.05946961951051619
Loss in iteration 74 : 0.058921288079060455
Loss in iteration 75 : 0.058388341914577806
Loss in iteration 76 : 0.05786853543980947
Loss in iteration 77 : 0.057361238102878465
Loss in iteration 78 : 0.05686440574203727
Loss in iteration 79 : 0.05637740708331567
Loss in iteration 80 : 0.05590023949974776
Loss in iteration 81 : 0.055434354875982424
Loss in iteration 82 : 0.05497884316572364
Loss in iteration 83 : 0.05453415623861968
Loss in iteration 84 : 0.054099524594316044
Loss in iteration 85 : 0.05367740714012595
Loss in iteration 86 : 0.05326231094235926
Loss in iteration 87 : 0.05285264378991305
Loss in iteration 88 : 0.05244840731310097
Loss in iteration 89 : 0.05204920474820943
Loss in iteration 90 : 0.05165479080676872
Loss in iteration 91 : 0.05126605966530833
Loss in iteration 92 : 0.050882112179961504
Loss in iteration 93 : 0.05050212665617534
Loss in iteration 94 : 0.050127825526543955
Loss in iteration 95 : 0.049760439792463536
Loss in iteration 96 : 0.049399022941659564
Loss in iteration 97 : 0.04904188083565834
Loss in iteration 98 : 0.04868967791839917
Loss in iteration 99 : 0.04834340558220994
Loss in iteration 100 : 0.04800251793880641
Loss in iteration 101 : 0.04766653033278266
Loss in iteration 102 : 0.04733666660890518
Loss in iteration 103 : 0.04701154795952183
Loss in iteration 104 : 0.04669590465034118
Loss in iteration 105 : 0.04638553120916619
Loss in iteration 106 : 0.046080139003943046
Loss in iteration 107 : 0.04578191413980304
Loss in iteration 108 : 0.04548922001388323
Loss in iteration 109 : 0.0452027179163257
Loss in iteration 110 : 0.04492045419341053
Loss in iteration 111 : 0.04464274886275735
Loss in iteration 112 : 0.044368401470493445
Loss in iteration 113 : 0.04409828222469867
Loss in iteration 114 : 0.04383325936926414
Loss in iteration 115 : 0.04357351916594217
Loss in iteration 116 : 0.043318477513050575
Loss in iteration 117 : 0.04306689018978567
Loss in iteration 118 : 0.04281883643898074
Loss in iteration 119 : 0.04257572990665453
Loss in iteration 120 : 0.042336879455494
Loss in iteration 121 : 0.042101426763124714
Loss in iteration 122 : 0.041870640340406245
Loss in iteration 123 : 0.04164318631249885
Loss in iteration 124 : 0.041418893162561074
Loss in iteration 125 : 0.041199239113469344
Loss in iteration 126 : 0.0409832188930863
Loss in iteration 127 : 0.04077155994000282
Loss in iteration 128 : 0.0405621348118638
Loss in iteration 129 : 0.04035526296294529
Loss in iteration 130 : 0.04015060281226766
Loss in iteration 131 : 0.039948009414571765
Loss in iteration 132 : 0.039747675264980034
Loss in iteration 133 : 0.039549030672816524
Loss in iteration 134 : 0.039351723857282084
Loss in iteration 135 : 0.03915651477027557
Loss in iteration 136 : 0.03896326875840752
Loss in iteration 137 : 0.038773028284447626
Loss in iteration 138 : 0.038585902412723506
Loss in iteration 139 : 0.03840091862741814
Loss in iteration 140 : 0.03821790655163513
Loss in iteration 141 : 0.038036264957206134
Loss in iteration 142 : 0.037855928504542995
Loss in iteration 143 : 0.037677007375235584
Loss in iteration 144 : 0.03750032591958025
Loss in iteration 145 : 0.037325168887925954
Loss in iteration 146 : 0.03715265122251599
Loss in iteration 147 : 0.036982897395647585
Loss in iteration 148 : 0.03681448576615332
Loss in iteration 149 : 0.03664820012912334
Loss in iteration 150 : 0.036483812028015816
Loss in iteration 151 : 0.03632067248604664
Loss in iteration 152 : 0.03615878071662331
Loss in iteration 153 : 0.03599819317287197
Loss in iteration 154 : 0.03583937069057376
Loss in iteration 155 : 0.035682974562070674
Loss in iteration 156 : 0.03552818304295202
Loss in iteration 157 : 0.03537492185805016
Loss in iteration 158 : 0.0352228415496364
Loss in iteration 159 : 0.03507195790633109
Loss in iteration 160 : 0.03492227475489971
Loss in iteration 161 : 0.03477396016390985
Loss in iteration 162 : 0.03462742094115521
Loss in iteration 163 : 0.03448205621455404
Loss in iteration 164 : 0.03433752892720455
Loss in iteration 165 : 0.03419388123013215
Loss in iteration 166 : 0.03405137343793915
Loss in iteration 167 : 0.03391044843644262
Loss in iteration 168 : 0.03377077746585911
Loss in iteration 169 : 0.03363241110500502
Loss in iteration 170 : 0.03349487852882711
Loss in iteration 171 : 0.03335804197012515
Loss in iteration 172 : 0.03322245345498389
Loss in iteration 173 : 0.03308775631847526
Loss in iteration 174 : 0.0329543583401194
Loss in iteration 175 : 0.03282212458030801
Loss in iteration 176 : 0.03269098379080754
Loss in iteration 177 : 0.03256130774572091
Loss in iteration 178 : 0.03243218346241025
Loss in iteration 179 : 0.032304179034583526
Loss in iteration 180 : 0.032176929179623566
Loss in iteration 181 : 0.03205095204696964
Loss in iteration 182 : 0.031925749869274156
Loss in iteration 183 : 0.031801173700580936
Loss in iteration 184 : 0.03167784886069493
Loss in iteration 185 : 0.031555639597807085
Loss in iteration 186 : 0.031434041866497876
Loss in iteration 187 : 0.03131308615379631
Loss in iteration 188 : 0.031192767634029374
Loss in iteration 189 : 0.031073221465755866
Loss in iteration 190 : 0.030954340169982792
Loss in iteration 191 : 0.03083636758652318
Loss in iteration 192 : 0.030719173835669848
Loss in iteration 193 : 0.030602562422789394
Loss in iteration 194 : 0.030486732416634497
Loss in iteration 195 : 0.03037168049018968
Loss in iteration 196 : 0.030257937496736286
Loss in iteration 197 : 0.03014532761493237
Loss in iteration 198 : 0.03003386298918286
Loss in iteration 199 : 0.029923647244770677
Loss in iteration 200 : 0.029814038763572276
Testing accuracy  of updater 8 on alg 1 with rate 0.013999999999999985 = 0.9857777777777778, training accuracy 0.993855387253501, time elapsed: 3653 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.214372160696211
Loss in iteration 3 : 1.390783396378807
Loss in iteration 4 : 0.434519936061364
Loss in iteration 5 : 0.7924769335029783
Loss in iteration 6 : 1.0710491360245045
Loss in iteration 7 : 0.9275067464948795
Loss in iteration 8 : 0.6769842322028131
Loss in iteration 9 : 0.487192770316982
Loss in iteration 10 : 0.3498145444486623
Loss in iteration 11 : 0.2626648937214073
Loss in iteration 12 : 0.2326513795249877
Loss in iteration 13 : 0.23654799070329793
Loss in iteration 14 : 0.25959861081037044
Loss in iteration 15 : 0.27241258519410133
Loss in iteration 16 : 0.2641209505002374
Loss in iteration 17 : 0.23540519233724605
Loss in iteration 18 : 0.1976557252739803
Loss in iteration 19 : 0.16444815549451824
Loss in iteration 20 : 0.13654776086270912
Loss in iteration 21 : 0.11327440009588005
Loss in iteration 22 : 0.09400158473190363
Loss in iteration 23 : 0.08199513944539648
Loss in iteration 24 : 0.07477852742784483
Loss in iteration 25 : 0.07047412851931797
Loss in iteration 26 : 0.06810925539237359
Loss in iteration 27 : 0.0667810690955378
Loss in iteration 28 : 0.06555424726696193
Loss in iteration 29 : 0.06410611087806391
Loss in iteration 30 : 0.06229154369090491
Loss in iteration 31 : 0.06006550097497758
Loss in iteration 32 : 0.057495447494338096
Loss in iteration 33 : 0.05492950245400379
Loss in iteration 34 : 0.05234053633044857
Loss in iteration 35 : 0.05007301006985863
Loss in iteration 36 : 0.047945823037465155
Loss in iteration 37 : 0.04589241012689835
Loss in iteration 38 : 0.04385211596158993
Loss in iteration 39 : 0.04185822462744602
Loss in iteration 40 : 0.04009346264476494
Loss in iteration 41 : 0.03843252286886608
Loss in iteration 42 : 0.03711333688537291
Loss in iteration 43 : 0.035895486239649714
Loss in iteration 44 : 0.034801491242348485
Loss in iteration 45 : 0.03395891026070233
Loss in iteration 46 : 0.033296097682712904
Loss in iteration 47 : 0.03272223143031396
Loss in iteration 48 : 0.03220385685287657
Loss in iteration 49 : 0.031790757967331246
Loss in iteration 50 : 0.03138534451673716
Loss in iteration 51 : 0.031027912176059724
Loss in iteration 52 : 0.030700916716027853
Loss in iteration 53 : 0.030385281105272593
Loss in iteration 54 : 0.030073617169951893
Loss in iteration 55 : 0.029765712868079997
Loss in iteration 56 : 0.02946137464938077
Loss in iteration 57 : 0.02916042553809655
Loss in iteration 58 : 0.028900474638206154
Loss in iteration 59 : 0.028660706276853305
Loss in iteration 60 : 0.028418578575246413
Loss in iteration 61 : 0.028174535322881097
Loss in iteration 62 : 0.027928967687363443
Loss in iteration 63 : 0.027682220201552504
Testing accuracy  of updater 9 on alg 1 with rate 0.7999999999999999 = 0.9724444444444444, training accuracy 0.997713632466419, time elapsed: 1080 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0576637836555676
Loss in iteration 3 : 0.6194008737736586
Loss in iteration 4 : 0.2024049678759792
Loss in iteration 5 : 0.3150679234685519
Loss in iteration 6 : 0.4101684040382636
Loss in iteration 7 : 0.3670602291352373
Loss in iteration 8 : 0.27219642857614545
Loss in iteration 9 : 0.19339711667257264
Loss in iteration 10 : 0.13260157882875248
Loss in iteration 11 : 0.09499215338064976
Loss in iteration 12 : 0.08375042456384348
Loss in iteration 13 : 0.08806948797945743
Loss in iteration 14 : 0.09942873112099597
Loss in iteration 15 : 0.10394705171483017
Loss in iteration 16 : 0.09678744496396152
Loss in iteration 17 : 0.08239114285896083
Loss in iteration 18 : 0.06730196939742593
Loss in iteration 19 : 0.055287953581470164
Loss in iteration 20 : 0.04450559797981106
Loss in iteration 21 : 0.036976995809012585
Loss in iteration 22 : 0.03254948168675294
Loss in iteration 23 : 0.030333992390212196
Loss in iteration 24 : 0.02888203939242375
Loss in iteration 25 : 0.028191513040824814
Loss in iteration 26 : 0.02830104416205435
Loss in iteration 27 : 0.02849269155437188
Loss in iteration 28 : 0.028442102408088255
Loss in iteration 29 : 0.027968856647970906
Loss in iteration 30 : 0.027012342146162816
Loss in iteration 31 : 0.025799156488186994
Loss in iteration 32 : 0.024396053725407853
Loss in iteration 33 : 0.022964009959935444
Loss in iteration 34 : 0.02165379766132923
Loss in iteration 35 : 0.020446175923138358
Loss in iteration 36 : 0.019253573882984737
Loss in iteration 37 : 0.018170440572824107
Loss in iteration 38 : 0.01719867036934536
Loss in iteration 39 : 0.0164277950802786
Loss in iteration 40 : 0.015759592903963497
Loss in iteration 41 : 0.015141941455123644
Loss in iteration 42 : 0.014766255131402277
Loss in iteration 43 : 0.014461689854551582
Loss in iteration 44 : 0.014200909117255129
Loss in iteration 45 : 0.014006263152085611
Loss in iteration 46 : 0.01380267929959894
Loss in iteration 47 : 0.013586587786026045
Loss in iteration 48 : 0.013356031704149375
Loss in iteration 49 : 0.013137677197564894
Loss in iteration 50 : 0.012953155638511979
Loss in iteration 51 : 0.012777454940632255
Loss in iteration 52 : 0.012600767833926958
Loss in iteration 53 : 0.012439622917343225
Loss in iteration 54 : 0.012280798647843079
Loss in iteration 55 : 0.012123909730434979
Loss in iteration 56 : 0.011968854854458803
Loss in iteration 57 : 0.011815541453515765
Loss in iteration 58 : 0.011663884804760232
Loss in iteration 59 : 0.011513807228376787
Loss in iteration 60 : 0.01136523737552133
Loss in iteration 61 : 0.01121829339522153
Loss in iteration 62 : 0.01109103998619918
Loss in iteration 63 : 0.010969917036223723
Loss in iteration 64 : 0.010850910456948816
Loss in iteration 65 : 0.010733860625224748
Loss in iteration 66 : 0.010618623916819949
Loss in iteration 67 : 0.010505070984941458
Testing accuracy  of updater 9 on alg 1 with rate 0.5599999999999999 = 0.9715555555555555, training accuracy 0.9981423263789654, time elapsed: 1162 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5591602353548171
Loss in iteration 3 : 0.3285230081062222
Loss in iteration 4 : 0.12631681063898195
Loss in iteration 5 : 0.20763625888181575
Loss in iteration 6 : 0.22718021219038387
Loss in iteration 7 : 0.16810606881053827
Loss in iteration 8 : 0.11746754178613729
Loss in iteration 9 : 0.0835920941700878
Loss in iteration 10 : 0.06565712772769162
Loss in iteration 11 : 0.061026612136754727
Loss in iteration 12 : 0.06585246598183991
Loss in iteration 13 : 0.06882195228238398
Loss in iteration 14 : 0.06365555586467644
Loss in iteration 15 : 0.052402896093098436
Loss in iteration 16 : 0.04089837232552161
Loss in iteration 17 : 0.03242437486807864
Loss in iteration 18 : 0.026393142766872036
Loss in iteration 19 : 0.023026597720788404
Loss in iteration 20 : 0.02163226017689907
Loss in iteration 21 : 0.02108323000002649
Loss in iteration 22 : 0.020730074394494133
Loss in iteration 23 : 0.020388781171713555
Loss in iteration 24 : 0.020114200180126447
Loss in iteration 25 : 0.01966366332497671
Loss in iteration 26 : 0.019070547523115558
Loss in iteration 27 : 0.018238151745037113
Loss in iteration 28 : 0.017163904540455882
Loss in iteration 29 : 0.015887450455964937
Loss in iteration 30 : 0.014558966197153972
Loss in iteration 31 : 0.013275960567753856
Loss in iteration 32 : 0.012264129446007743
Loss in iteration 33 : 0.011425622428528619
Loss in iteration 34 : 0.010735761876358954
Loss in iteration 35 : 0.010344348746493195
Loss in iteration 36 : 0.010081701257690286
Loss in iteration 37 : 0.009867120857608757
Loss in iteration 38 : 0.009674963610177879
Loss in iteration 39 : 0.009445127582088446
Loss in iteration 40 : 0.009199120459144025
Loss in iteration 41 : 0.008924352817840528
Loss in iteration 42 : 0.008637851848681538
Loss in iteration 43 : 0.00834431250002252
Loss in iteration 44 : 0.008040737220231497
Loss in iteration 45 : 0.00774730742912361
Loss in iteration 46 : 0.007486710734777211
Loss in iteration 47 : 0.007247867001824675
Loss in iteration 48 : 0.007035891083803468
Loss in iteration 49 : 0.00686576523478047
Loss in iteration 50 : 0.006734551587125998
Loss in iteration 51 : 0.00661913030402077
Loss in iteration 52 : 0.006510295124154412
Loss in iteration 53 : 0.006418511866778537
Loss in iteration 54 : 0.006329242779822697
Loss in iteration 55 : 0.006242267300859608
Loss in iteration 56 : 0.006157388119823885
Loss in iteration 57 : 0.006075085143230786
Loss in iteration 58 : 0.005998511245732757
Loss in iteration 59 : 0.00592419438525951
Loss in iteration 60 : 0.005851479424264886
Loss in iteration 61 : 0.005780235497974845
Loss in iteration 62 : 0.005713049256174329
Loss in iteration 63 : 0.005647955076302512
Loss in iteration 64 : 0.005584558873409489
Loss in iteration 65 : 0.005527551947284736
Loss in iteration 66 : 0.00547336650181177
Loss in iteration 67 : 0.005420113310208521
Loss in iteration 68 : 0.005367721581728307
Testing accuracy  of updater 9 on alg 1 with rate 0.32 = 0.9697777777777777, training accuracy 0.9987139182623607, time elapsed: 1199 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3409826656947078
Loss in iteration 3 : 0.2815647997519935
Loss in iteration 4 : 0.15006521665153424
Loss in iteration 5 : 0.13094817254894028
Loss in iteration 6 : 0.10856235015448462
Loss in iteration 7 : 0.08536631202391183
Loss in iteration 8 : 0.07252929966948676
Loss in iteration 9 : 0.06766953906409018
Loss in iteration 10 : 0.06489711617675889
Loss in iteration 11 : 0.06065288772846755
Loss in iteration 12 : 0.05498903786875653
Loss in iteration 13 : 0.04877641791921763
Loss in iteration 14 : 0.04290476616282443
Loss in iteration 15 : 0.037541110802063085
Loss in iteration 16 : 0.0330854693051529
Loss in iteration 17 : 0.029447079929250223
Loss in iteration 18 : 0.026632326455019246
Loss in iteration 19 : 0.02456482203173141
Loss in iteration 20 : 0.023005921769435257
Loss in iteration 21 : 0.021879078792215973
Loss in iteration 22 : 0.020871949648730886
Loss in iteration 23 : 0.019740775387011647
Loss in iteration 24 : 0.01855547911899293
Loss in iteration 25 : 0.01741027338317389
Loss in iteration 26 : 0.016376369471291084
Loss in iteration 27 : 0.01545450807735849
Loss in iteration 28 : 0.01465368123131101
Loss in iteration 29 : 0.013930380067762359
Loss in iteration 30 : 0.013302612643005966
Loss in iteration 31 : 0.012818105051085164
Loss in iteration 32 : 0.012403209392735754
Loss in iteration 33 : 0.012026683244734783
Loss in iteration 34 : 0.011695986903474755
Loss in iteration 35 : 0.011386451736048117
Loss in iteration 36 : 0.011079688684620954
Loss in iteration 37 : 0.010774316453292446
Loss in iteration 38 : 0.01046920547917629
Loss in iteration 39 : 0.010161110603765648
Loss in iteration 40 : 0.009853855312554916
Loss in iteration 41 : 0.00954408218078585
Loss in iteration 42 : 0.009232428538567839
Loss in iteration 43 : 0.008919448150012187
Loss in iteration 44 : 0.00861016262335768
Loss in iteration 45 : 0.008310489607634019
Loss in iteration 46 : 0.008034499478278709
Loss in iteration 47 : 0.007777293154534839
Loss in iteration 48 : 0.0075764637869017125
Loss in iteration 49 : 0.007383318398814105
Loss in iteration 50 : 0.007197652168604081
Loss in iteration 51 : 0.007031662278374864
Loss in iteration 52 : 0.006874844342367727
Loss in iteration 53 : 0.006724409587029035
Loss in iteration 54 : 0.006575577211670926
Loss in iteration 55 : 0.00643128991732484
Loss in iteration 56 : 0.006287736137562743
Loss in iteration 57 : 0.00614706195264482
Loss in iteration 58 : 0.0060141108647492735
Loss in iteration 59 : 0.005890964249762723
Loss in iteration 60 : 0.005770888568061925
Loss in iteration 61 : 0.005658782010439737
Loss in iteration 62 : 0.00555150395395791
Loss in iteration 63 : 0.005449597899792683
Loss in iteration 64 : 0.005352823890901907
Loss in iteration 65 : 0.005265048184704892
Loss in iteration 66 : 0.005180572625924543
Loss in iteration 67 : 0.005100482960656653
Loss in iteration 68 : 0.005030282485912054
Loss in iteration 69 : 0.0049631590723085255
Loss in iteration 70 : 0.004899705522024507
Loss in iteration 71 : 0.004837126436233132
Loss in iteration 72 : 0.004774862694125429
Loss in iteration 73 : 0.004712675859733087
Loss in iteration 74 : 0.004650761269780145
Loss in iteration 75 : 0.004589662057411415
Loss in iteration 76 : 0.004528869651801717
Loss in iteration 77 : 0.0044683847294411565
Loss in iteration 78 : 0.004410494123238207
Loss in iteration 79 : 0.004355811488698458
Loss in iteration 80 : 0.004302140627267765
Loss in iteration 81 : 0.0042498605912890015
Loss in iteration 82 : 0.004199054204175776
Loss in iteration 83 : 0.004148892323568006
Loss in iteration 84 : 0.004099762970309518
Loss in iteration 85 : 0.0040519225799707185
Loss in iteration 86 : 0.004004725745076936
Loss in iteration 87 : 0.003961442527857228
Loss in iteration 88 : 0.003921347472500704
Loss in iteration 89 : 0.003882695650961206
Loss in iteration 90 : 0.003844611622076533
Loss in iteration 91 : 0.003806415554044
Loss in iteration 92 : 0.0037681378325752453
Loss in iteration 93 : 0.0037300050297897945
Loss in iteration 94 : 0.003692528890053424
Loss in iteration 95 : 0.00365528380412109
Loss in iteration 96 : 0.0036182616021257206
Loss in iteration 97 : 0.0035814797886338776
Loss in iteration 98 : 0.0035470665813672666
Loss in iteration 99 : 0.003514307828541846
Loss in iteration 100 : 0.0034829376207816585
Loss in iteration 101 : 0.00345192293923999
Loss in iteration 102 : 0.0034212385074273597
Loss in iteration 103 : 0.003390861536719984
Loss in iteration 104 : 0.00336077146757227
Testing accuracy  of updater 9 on alg 1 with rate 0.08 = 0.9768888888888889, training accuracy 0.9991426121749071, time elapsed: 1950 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.47364677817951467
Loss in iteration 3 : 0.2796896542832874
Loss in iteration 4 : 0.21769889659371297
Loss in iteration 5 : 0.17787666549710363
Loss in iteration 6 : 0.13639055200408057
Loss in iteration 7 : 0.1197160443819434
Loss in iteration 8 : 0.12192532658291848
Loss in iteration 9 : 0.11056315323095918
Loss in iteration 10 : 0.08981906181610533
Loss in iteration 11 : 0.07527486972709628
Loss in iteration 12 : 0.0676730031387654
Loss in iteration 13 : 0.06347479374404083
Loss in iteration 14 : 0.06010485430024894
Loss in iteration 15 : 0.05654182411808567
Loss in iteration 16 : 0.052781551606395895
Loss in iteration 17 : 0.048864329528303795
Loss in iteration 18 : 0.04484708663802462
Loss in iteration 19 : 0.04090865845165191
Loss in iteration 20 : 0.03727824995425844
Loss in iteration 21 : 0.03406321177912475
Loss in iteration 22 : 0.031286335050209405
Loss in iteration 23 : 0.0290829349176163
Loss in iteration 24 : 0.027241641599897254
Loss in iteration 25 : 0.02559315137387418
Loss in iteration 26 : 0.024074428610925864
Loss in iteration 27 : 0.022752227649400606
Loss in iteration 28 : 0.021635331179128747
Loss in iteration 29 : 0.02062623057332551
Loss in iteration 30 : 0.019696519877981836
Loss in iteration 31 : 0.01884894325391652
Loss in iteration 32 : 0.018066693608637795
Loss in iteration 33 : 0.017367222258631708
Loss in iteration 34 : 0.016738084948402846
Loss in iteration 35 : 0.016169718005065916
Loss in iteration 36 : 0.015656125155582568
Loss in iteration 37 : 0.015204300769895302
Loss in iteration 38 : 0.014807985956191283
Loss in iteration 39 : 0.014451709803173337
Loss in iteration 40 : 0.014116611445031693
Loss in iteration 41 : 0.01379462282149365
Loss in iteration 42 : 0.013496405977262661
Loss in iteration 43 : 0.013213346993559473
Loss in iteration 44 : 0.012940974078669008
Loss in iteration 45 : 0.012678339389945894
Loss in iteration 46 : 0.012428394335340336
Loss in iteration 47 : 0.012189772940987888
Loss in iteration 48 : 0.011961407771833082
Loss in iteration 49 : 0.011739609324747853
Loss in iteration 50 : 0.011525925395965727
Loss in iteration 51 : 0.01131694439217428
Loss in iteration 52 : 0.011112650392244178
Loss in iteration 53 : 0.010914602094455546
Loss in iteration 54 : 0.010723329290356841
Loss in iteration 55 : 0.01053629036748594
Loss in iteration 56 : 0.010353685689055078
Loss in iteration 57 : 0.010173791634381627
Loss in iteration 58 : 0.009996437058102194
Loss in iteration 59 : 0.00982274425402696
Loss in iteration 60 : 0.009653565083955
Loss in iteration 61 : 0.009488478629329387
Loss in iteration 62 : 0.009329968080199879
Loss in iteration 63 : 0.009176012485869257
Loss in iteration 64 : 0.009024679903390807
Loss in iteration 65 : 0.008876121482466961
Loss in iteration 66 : 0.008732203643986441
Loss in iteration 67 : 0.008595652218632753
Loss in iteration 68 : 0.008461139073441933
Loss in iteration 69 : 0.008329896730282458
Loss in iteration 70 : 0.008201726240328387
Loss in iteration 71 : 0.008076759466576879
Loss in iteration 72 : 0.00795410892726192
Loss in iteration 73 : 0.00783399149706697
Loss in iteration 74 : 0.007715640774017971
Loss in iteration 75 : 0.007598786495556197
Loss in iteration 76 : 0.0074842789874542135
Loss in iteration 77 : 0.007371339317474724
Loss in iteration 78 : 0.007260885023981745
Loss in iteration 79 : 0.007152761915852457
Loss in iteration 80 : 0.007045535327680758
Loss in iteration 81 : 0.00693993677674908
Loss in iteration 82 : 0.006835827212219753
Loss in iteration 83 : 0.00673925760811836
Loss in iteration 84 : 0.006645402809883231
Loss in iteration 85 : 0.006553777962963024
Loss in iteration 86 : 0.006464974837741336
Loss in iteration 87 : 0.006381318101199782
Loss in iteration 88 : 0.0062996443656164235
Loss in iteration 89 : 0.0062199745291568265
Loss in iteration 90 : 0.006142647273653266
Loss in iteration 91 : 0.006068189504520787
Loss in iteration 92 : 0.00599446591987469
Loss in iteration 93 : 0.00592122248011118
Loss in iteration 94 : 0.005851169565615733
Loss in iteration 95 : 0.0057836531375985
Loss in iteration 96 : 0.005717294283828898
Loss in iteration 97 : 0.0056517302370848
Loss in iteration 98 : 0.005586930293409936
Loss in iteration 99 : 0.005522894440141165
Loss in iteration 100 : 0.005462017703620027
Loss in iteration 101 : 0.005403614983188066
Loss in iteration 102 : 0.00534775300112571
Loss in iteration 103 : 0.00529418215087223
Loss in iteration 104 : 0.005241815528257065
Loss in iteration 105 : 0.005190796484586047
Loss in iteration 106 : 0.005142767838436849
Loss in iteration 107 : 0.005095657326865601
Loss in iteration 108 : 0.005049549597011551
Loss in iteration 109 : 0.005004260763828181
Loss in iteration 110 : 0.004959524684106852
Loss in iteration 111 : 0.004915299619860899
Loss in iteration 112 : 0.00487154799048341
Loss in iteration 113 : 0.004828330369337248
Loss in iteration 114 : 0.004786132589838474
Loss in iteration 115 : 0.004744764707398925
Loss in iteration 116 : 0.0047037410372107475
Loss in iteration 117 : 0.004663040033831028
Loss in iteration 118 : 0.0046226422222911
Loss in iteration 119 : 0.004582529984966728
Loss in iteration 120 : 0.0045427830482684195
Loss in iteration 121 : 0.004503968012761365
Loss in iteration 122 : 0.004465895876530812
Loss in iteration 123 : 0.00442920824841991
Loss in iteration 124 : 0.004392924359816292
Loss in iteration 125 : 0.004356881880847156
Loss in iteration 126 : 0.004321067416134045
Loss in iteration 127 : 0.004285745008522179
Loss in iteration 128 : 0.004251529258013337
Loss in iteration 129 : 0.004217627184829436
Testing accuracy  of updater 9 on alg 1 with rate 0.056 = 0.976, training accuracy 0.9989997142040583, time elapsed: 2209 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7319295634791829
Loss in iteration 3 : 0.41539554812739526
Loss in iteration 4 : 0.25402987396597493
Loss in iteration 5 : 0.2012134297984659
Loss in iteration 6 : 0.18333150607072918
Loss in iteration 7 : 0.16920578514447882
Loss in iteration 8 : 0.15500831608368296
Loss in iteration 9 : 0.14031275653866254
Loss in iteration 10 : 0.12612494473192393
Loss in iteration 11 : 0.11301927448690781
Loss in iteration 12 : 0.1016087656814439
Loss in iteration 13 : 0.09232179609108392
Loss in iteration 14 : 0.08482230627821129
Loss in iteration 15 : 0.07858418141002027
Loss in iteration 16 : 0.07343710572606038
Loss in iteration 17 : 0.06904065798925756
Loss in iteration 18 : 0.06517332903729242
Loss in iteration 19 : 0.061821555823074734
Loss in iteration 20 : 0.05873154345204625
Loss in iteration 21 : 0.05587824378696135
Loss in iteration 22 : 0.05323511306979884
Loss in iteration 23 : 0.05074857497519029
Loss in iteration 24 : 0.048411631237622745
Loss in iteration 25 : 0.04626243717914082
Loss in iteration 26 : 0.04426368202049809
Loss in iteration 27 : 0.042375227021329384
Loss in iteration 28 : 0.040589726429258115
Loss in iteration 29 : 0.03890075187115904
Loss in iteration 30 : 0.03730741490616704
Loss in iteration 31 : 0.03577489565325811
Loss in iteration 32 : 0.034321309012565866
Loss in iteration 33 : 0.03298802853629491
Loss in iteration 34 : 0.031734001070646646
Loss in iteration 35 : 0.030544329232828273
Loss in iteration 36 : 0.029417469054085404
Loss in iteration 37 : 0.028387800093160224
Loss in iteration 38 : 0.0274340064505129
Loss in iteration 39 : 0.02654144551214454
Loss in iteration 40 : 0.025700758233003804
Loss in iteration 41 : 0.024919575319680275
Loss in iteration 42 : 0.02418286646035875
Loss in iteration 43 : 0.02351166508897081
Loss in iteration 44 : 0.022904943192576396
Loss in iteration 45 : 0.022336686041672067
Loss in iteration 46 : 0.021793122618014502
Loss in iteration 47 : 0.02129568192852828
Loss in iteration 48 : 0.020842848805778854
Loss in iteration 49 : 0.02041016819915209
Loss in iteration 50 : 0.02000295566249296
Loss in iteration 51 : 0.019622819185798213
Loss in iteration 52 : 0.019259399172397017
Loss in iteration 53 : 0.018922574805088473
Loss in iteration 54 : 0.018612292771073163
Loss in iteration 55 : 0.018314554503769204
Loss in iteration 56 : 0.018031902782524565
Loss in iteration 57 : 0.017762705149776875
Loss in iteration 58 : 0.017501284820496083
Loss in iteration 59 : 0.01724712552048645
Loss in iteration 60 : 0.017004670242764534
Loss in iteration 61 : 0.016769184365194492
Loss in iteration 62 : 0.01654817151360584
Loss in iteration 63 : 0.016338140109425883
Loss in iteration 64 : 0.016133347517426903
Loss in iteration 65 : 0.015934988565906753
Loss in iteration 66 : 0.015744755843144276
Loss in iteration 67 : 0.015560020582548706
Loss in iteration 68 : 0.015381418182094994
Loss in iteration 69 : 0.01521129208260772
Loss in iteration 70 : 0.015048572154958606
Loss in iteration 71 : 0.014892292423665625
Loss in iteration 72 : 0.014741328368418023
Loss in iteration 73 : 0.014597663026511562
Loss in iteration 74 : 0.014460066862084675
Loss in iteration 75 : 0.014326657639778804
Loss in iteration 76 : 0.014196654083726707
Loss in iteration 77 : 0.014070831541945682
Loss in iteration 78 : 0.013948504314675345
Loss in iteration 79 : 0.013828402554341075
Loss in iteration 80 : 0.013710545363566713
Loss in iteration 81 : 0.013594535785763799
Loss in iteration 82 : 0.01348052119673656
Loss in iteration 83 : 0.013369470491137438
Loss in iteration 84 : 0.013259852383742027
Loss in iteration 85 : 0.013151688372284876
Loss in iteration 86 : 0.013044917060394989
Loss in iteration 87 : 0.012939479346730228
Loss in iteration 88 : 0.012835954872400785
Loss in iteration 89 : 0.012733782677132755
Loss in iteration 90 : 0.012633006106620341
Loss in iteration 91 : 0.01253465281997422
Loss in iteration 92 : 0.012438789017180098
Loss in iteration 93 : 0.012344575163527233
Loss in iteration 94 : 0.012251783874736884
Loss in iteration 95 : 0.01216036525163221
Loss in iteration 96 : 0.012070450683595597
Loss in iteration 97 : 0.011981762289683884
Loss in iteration 98 : 0.011893801102609649
Loss in iteration 99 : 0.011806697059631612
Loss in iteration 100 : 0.011720295499321636
Loss in iteration 101 : 0.011636126980797865
Loss in iteration 102 : 0.011554875576749063
Loss in iteration 103 : 0.011475028259497097
Loss in iteration 104 : 0.011397166501901536
Loss in iteration 105 : 0.01132163003826137
Loss in iteration 106 : 0.011247451274308469
Loss in iteration 107 : 0.011173662872070545
Loss in iteration 108 : 0.011100253381207605
Loss in iteration 109 : 0.011027212170239675
Loss in iteration 110 : 0.01095477788782495
Loss in iteration 111 : 0.010883402038725508
Loss in iteration 112 : 0.01081244429390745
Loss in iteration 113 : 0.01074206765650425
Loss in iteration 114 : 0.01067252861235835
Loss in iteration 115 : 0.010603845501991083
Loss in iteration 116 : 0.010535751636221571
Loss in iteration 117 : 0.010468170501813921
Loss in iteration 118 : 0.010401489018655145
Loss in iteration 119 : 0.010335329672157782
Loss in iteration 120 : 0.01026960879874549
Loss in iteration 121 : 0.010204397847424187
Loss in iteration 122 : 0.010139637385447191
Loss in iteration 123 : 0.010074961150526723
Loss in iteration 124 : 0.010010288176562746
Loss in iteration 125 : 0.009945948070928079
Loss in iteration 126 : 0.009882639684574883
Loss in iteration 127 : 0.00981963614633354
Loss in iteration 128 : 0.009757314885308037
Loss in iteration 129 : 0.009695649645153844
Loss in iteration 130 : 0.009634455082225
Loss in iteration 131 : 0.009573507307686828
Loss in iteration 132 : 0.009512800605454357
Loss in iteration 133 : 0.009452329635442678
Loss in iteration 134 : 0.009392089398247143
Loss in iteration 135 : 0.009332075203417889
Loss in iteration 136 : 0.009272282640955075
Loss in iteration 137 : 0.009212707555690317
Loss in iteration 138 : 0.009153359661635543
Loss in iteration 139 : 0.009094298684784214
Loss in iteration 140 : 0.009035459986418389
Loss in iteration 141 : 0.00897683851457824
Loss in iteration 142 : 0.008918467769465204
Loss in iteration 143 : 0.008860666177951207
Loss in iteration 144 : 0.008803197817376379
Loss in iteration 145 : 0.008746692005718015
Loss in iteration 146 : 0.008690400781183298
Loss in iteration 147 : 0.008634326259970829
Loss in iteration 148 : 0.00857862261321739
Loss in iteration 149 : 0.008523240846163557
Loss in iteration 150 : 0.00846826509083096
Loss in iteration 151 : 0.008414099037500854
Loss in iteration 152 : 0.008360524678926042
Loss in iteration 153 : 0.008307198617108127
Loss in iteration 154 : 0.00825418788092167
Loss in iteration 155 : 0.008201644214817984
Loss in iteration 156 : 0.008149290627480541
Loss in iteration 157 : 0.008097773637013994
Loss in iteration 158 : 0.008047095156773496
Loss in iteration 159 : 0.00799697359364253
Loss in iteration 160 : 0.007947295954552975
Loss in iteration 161 : 0.007897974793298582
Loss in iteration 162 : 0.007848781804071179
Loss in iteration 163 : 0.0077998259054580035
Loss in iteration 164 : 0.007751217546580817
Loss in iteration 165 : 0.0077027504737904665
Loss in iteration 166 : 0.007654830962249941
Loss in iteration 167 : 0.007607670665192781
Loss in iteration 168 : 0.007561119374688363
Loss in iteration 169 : 0.0075150068557713005
Loss in iteration 170 : 0.0074691044030826765
Loss in iteration 171 : 0.007423388306081527
Loss in iteration 172 : 0.007377795611121587
Loss in iteration 173 : 0.007332264350271082
Loss in iteration 174 : 0.007286818723084207
Loss in iteration 175 : 0.007242362463025509
Loss in iteration 176 : 0.007198803048031375
Loss in iteration 177 : 0.007155801155101823
Loss in iteration 178 : 0.0071139861739604165
Loss in iteration 179 : 0.007072811473667499
Loss in iteration 180 : 0.007031874905568772
Loss in iteration 181 : 0.0069916371464927045
Loss in iteration 182 : 0.006951875265127323
Loss in iteration 183 : 0.0069123718450293985
Loss in iteration 184 : 0.006873317981419167
Loss in iteration 185 : 0.0068344717329519985
Loss in iteration 186 : 0.006795773738954722
Loss in iteration 187 : 0.0067577437937628194
Testing accuracy  of updater 9 on alg 1 with rate 0.032 = 0.9831111111111112, training accuracy 0.9991426121749071, time elapsed: 3167 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.938742260777213
Loss in iteration 3 : 0.8600170657940961
Loss in iteration 4 : 0.7723300991723977
Loss in iteration 5 : 0.6798834518632992
Loss in iteration 6 : 0.5851346938825924
Loss in iteration 7 : 0.4925296436024767
Loss in iteration 8 : 0.4251730069299277
Loss in iteration 9 : 0.3778080140477018
Loss in iteration 10 : 0.32825190609018506
Loss in iteration 11 : 0.28039555882839085
Loss in iteration 12 : 0.2461629623447106
Loss in iteration 13 : 0.22631056384857065
Loss in iteration 14 : 0.21518600910729951
Loss in iteration 15 : 0.20927242806014212
Loss in iteration 16 : 0.20509564711433076
Loss in iteration 17 : 0.2012638912206811
Loss in iteration 18 : 0.19742548527366197
Loss in iteration 19 : 0.1933148795562297
Loss in iteration 20 : 0.18912313996993999
Loss in iteration 21 : 0.18500469511196346
Loss in iteration 22 : 0.18108587123129227
Loss in iteration 23 : 0.17758723771025403
Loss in iteration 24 : 0.17437439423877038
Loss in iteration 25 : 0.17136193151762397
Loss in iteration 26 : 0.16846525110968874
Loss in iteration 27 : 0.1656759875665848
Loss in iteration 28 : 0.16301099305578234
Loss in iteration 29 : 0.16036524902881044
Loss in iteration 30 : 0.15772080562665391
Loss in iteration 31 : 0.15508023080996444
Loss in iteration 32 : 0.15246299979337358
Loss in iteration 33 : 0.1498687725247524
Loss in iteration 34 : 0.14731160950785424
Loss in iteration 35 : 0.1447945058400314
Loss in iteration 36 : 0.14234157631022507
Loss in iteration 37 : 0.13995249420940598
Loss in iteration 38 : 0.13761858946763153
Loss in iteration 39 : 0.1353616285395172
Loss in iteration 40 : 0.1331979672367362
Loss in iteration 41 : 0.13111420916294117
Loss in iteration 42 : 0.1290822921334006
Loss in iteration 43 : 0.1271142779607805
Loss in iteration 44 : 0.12520342508716803
Loss in iteration 45 : 0.12334149400488831
Loss in iteration 46 : 0.12153341501337919
Loss in iteration 47 : 0.11977266416593481
Loss in iteration 48 : 0.1180493399800218
Loss in iteration 49 : 0.11636377095837226
Loss in iteration 50 : 0.1147117948312118
Loss in iteration 51 : 0.11309367805690708
Loss in iteration 52 : 0.11152257587150416
Loss in iteration 53 : 0.11000799174559667
Loss in iteration 54 : 0.10855221853281888
Loss in iteration 55 : 0.10713567719283823
Loss in iteration 56 : 0.10576089118641313
Loss in iteration 57 : 0.10442240832420703
Loss in iteration 58 : 0.10311695894550361
Loss in iteration 59 : 0.10184281173510007
Loss in iteration 60 : 0.10060515178337279
Loss in iteration 61 : 0.09941432721616232
Loss in iteration 62 : 0.09826676889596123
Loss in iteration 63 : 0.09716178653788955
Loss in iteration 64 : 0.09608668793650892
Loss in iteration 65 : 0.0950415733877761
Loss in iteration 66 : 0.09402595452542416
Loss in iteration 67 : 0.09303289405378302
Loss in iteration 68 : 0.09206548119913663
Loss in iteration 69 : 0.09112280737136018
Loss in iteration 70 : 0.09020949885390728
Loss in iteration 71 : 0.0893259411633069
Loss in iteration 72 : 0.0884751381150733
Loss in iteration 73 : 0.08765238659232
Loss in iteration 74 : 0.08685950084470358
Loss in iteration 75 : 0.08608693566303395
Loss in iteration 76 : 0.08533125184017706
Loss in iteration 77 : 0.08459415713739932
Loss in iteration 78 : 0.08387358258890437
Loss in iteration 79 : 0.08316998609967532
Loss in iteration 80 : 0.08248047423400244
Loss in iteration 81 : 0.08180552244920908
Loss in iteration 82 : 0.08114569629529837
Loss in iteration 83 : 0.08049976586139934
Loss in iteration 84 : 0.07986785544414868
Loss in iteration 85 : 0.07924852821524854
Loss in iteration 86 : 0.0786413745416576
Loss in iteration 87 : 0.07804779343010765
Loss in iteration 88 : 0.07746904371944542
Loss in iteration 89 : 0.0769038249563958
Loss in iteration 90 : 0.07635080757940588
Loss in iteration 91 : 0.07580974127369475
Loss in iteration 92 : 0.07528287699021673
Loss in iteration 93 : 0.0747677881524308
Loss in iteration 94 : 0.07426405913229144
Loss in iteration 95 : 0.073770615989228
Loss in iteration 96 : 0.07328997422671252
Loss in iteration 97 : 0.07281993369453163
Loss in iteration 98 : 0.07235960081675702
Loss in iteration 99 : 0.07190959267968879
Loss in iteration 100 : 0.0714699389639116
Loss in iteration 101 : 0.07104096040191357
Loss in iteration 102 : 0.07061942215149997
Loss in iteration 103 : 0.07020496758632709
Loss in iteration 104 : 0.06979654082611654
Loss in iteration 105 : 0.06939392334106616
Loss in iteration 106 : 0.06899688752418015
Loss in iteration 107 : 0.06860624433353782
Loss in iteration 108 : 0.0682206260109595
Loss in iteration 109 : 0.06784071968884704
Loss in iteration 110 : 0.0674660148797856
Loss in iteration 111 : 0.06709639618409749
Loss in iteration 112 : 0.06673215988245451
Loss in iteration 113 : 0.06637273369761269
Loss in iteration 114 : 0.06601721427222357
Loss in iteration 115 : 0.06566673118505775
Loss in iteration 116 : 0.06532042344161088
Loss in iteration 117 : 0.06497901305173395
Loss in iteration 118 : 0.06464219567847783
Loss in iteration 119 : 0.06430938713323876
Loss in iteration 120 : 0.0639811042647518
Loss in iteration 121 : 0.06365708689926812
Loss in iteration 122 : 0.063336574934565
Loss in iteration 123 : 0.06302016023246677
Loss in iteration 124 : 0.06270718971912174
Loss in iteration 125 : 0.06239723492203455
Loss in iteration 126 : 0.0620913748834635
Loss in iteration 127 : 0.06178905627560978
Loss in iteration 128 : 0.061490032340667335
Loss in iteration 129 : 0.06119490363373841
Loss in iteration 130 : 0.06090367131114832
Loss in iteration 131 : 0.06061579089744183
Loss in iteration 132 : 0.060330936180557584
Loss in iteration 133 : 0.060050157689123966
Loss in iteration 134 : 0.0597726739243756
Loss in iteration 135 : 0.05949820189161724
Loss in iteration 136 : 0.05922713655536194
Loss in iteration 137 : 0.05895858905469307
Loss in iteration 138 : 0.0586925678721718
Loss in iteration 139 : 0.05842985181549439
Loss in iteration 140 : 0.05816974718714111
Loss in iteration 141 : 0.05791250467221609
Loss in iteration 142 : 0.0576592299200568
Loss in iteration 143 : 0.057409323449448206
Loss in iteration 144 : 0.057163620827448344
Loss in iteration 145 : 0.056920200687635064
Loss in iteration 146 : 0.056679660766868804
Loss in iteration 147 : 0.056442064003142355
Loss in iteration 148 : 0.05620792392602097
Loss in iteration 149 : 0.055976045184487074
Loss in iteration 150 : 0.055746769461723154
Loss in iteration 151 : 0.05551969354553093
Loss in iteration 152 : 0.055295257612489206
Loss in iteration 153 : 0.055073053298397366
Loss in iteration 154 : 0.0548549580663421
Loss in iteration 155 : 0.05463898438701588
Loss in iteration 156 : 0.05442488185826293
Loss in iteration 157 : 0.054212556547975854
Loss in iteration 158 : 0.05400243869192466
Loss in iteration 159 : 0.05379498674217208
Loss in iteration 160 : 0.05358974280251741
Loss in iteration 161 : 0.05338750319127508
Loss in iteration 162 : 0.05318792851503187
Loss in iteration 163 : 0.05299029153552665
Loss in iteration 164 : 0.05279466407361563
Loss in iteration 165 : 0.052601614906986775
Loss in iteration 166 : 0.05241030171961512
Loss in iteration 167 : 0.05222052438210849
Loss in iteration 168 : 0.052032544075807215
Loss in iteration 169 : 0.05184642813979877
Loss in iteration 170 : 0.05166170416026954
Loss in iteration 171 : 0.05147815952542856
Loss in iteration 172 : 0.051295976784906626
Loss in iteration 173 : 0.05111532259738531
Loss in iteration 174 : 0.05093600904772656
Loss in iteration 175 : 0.050758325904130173
Loss in iteration 176 : 0.05058216175193497
Loss in iteration 177 : 0.0504081803993338
Loss in iteration 178 : 0.050236008660485935
Loss in iteration 179 : 0.05006502874939762
Loss in iteration 180 : 0.04989543241845735
Loss in iteration 181 : 0.04972698496659418
Loss in iteration 182 : 0.04955949114855857
Loss in iteration 183 : 0.04939316101816505
Loss in iteration 184 : 0.04922764698934839
Loss in iteration 185 : 0.049063125695393894
Loss in iteration 186 : 0.04889987009719467
Loss in iteration 187 : 0.04873754501473518
Loss in iteration 188 : 0.048576250263621994
Loss in iteration 189 : 0.04841686004488635
Loss in iteration 190 : 0.048258874226478225
Loss in iteration 191 : 0.04810184072588841
Loss in iteration 192 : 0.04794606896317349
Loss in iteration 193 : 0.04779134421751456
Loss in iteration 194 : 0.04763914772226085
Loss in iteration 195 : 0.04748815500174931
Loss in iteration 196 : 0.047338062961138544
Loss in iteration 197 : 0.04718885637186892
Loss in iteration 198 : 0.04704114193304103
Loss in iteration 199 : 0.046895450763786116
Loss in iteration 200 : 0.04675078249638048
Testing accuracy  of updater 9 on alg 1 with rate 0.007999999999999993 = 0.9875555555555555, training accuracy 0.9904258359531295, time elapsed: 4240 millisecond.
