objc[1323]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x109ff94c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10a07d4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 19:03:38 INFO SparkContext: Running Spark version 2.0.0
18/02/26 19:03:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 19:03:39 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 19:03:39 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 19:03:39 INFO SecurityManager: Changing view acls groups to: 
18/02/26 19:03:39 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 19:03:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 19:03:40 INFO Utils: Successfully started service 'sparkDriver' on port 50209.
18/02/26 19:03:40 INFO SparkEnv: Registering MapOutputTracker
18/02/26 19:03:40 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 19:03:40 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-5185ef0e-a382-42fb-b2a2-38e6cb87aa86
18/02/26 19:03:40 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 19:03:40 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 19:03:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 19:03:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 19:03:41 INFO Executor: Starting executor ID driver on host localhost
18/02/26 19:03:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50210.
18/02/26 19:03:41 INFO NettyBlockTransferService: Server created on 192.168.2.140:50210
18/02/26 19:03:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50210)
18/02/26 19:03:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50210 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50210)
18/02/26 19:03:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50210)
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 2.17168732111473
Loss in iteration 3 : 23.95846846104125
Loss in iteration 4 : 7.719186227123502
Loss in iteration 5 : 16.185207401804025
Loss in iteration 6 : 11.521919665971758
Loss in iteration 7 : 10.372583561386586
Loss in iteration 8 : 14.456340665766042
Loss in iteration 9 : 4.861796053088498
Loss in iteration 10 : 14.432006747306723
Loss in iteration 11 : 5.3790627107781654
Loss in iteration 12 : 13.143162479185946
Loss in iteration 13 : 5.149184519338702
Loss in iteration 14 : 11.99208627547989
Loss in iteration 15 : 6.270132587733775
Loss in iteration 16 : 11.691807787272113
Loss in iteration 17 : 6.260310456691086
Loss in iteration 18 : 10.33505136795148
Loss in iteration 19 : 6.648518229352857
Loss in iteration 20 : 9.16064386663737
Loss in iteration 21 : 7.690224184820254
Loss in iteration 22 : 8.312297994695573
Loss in iteration 23 : 7.153001716180582
Loss in iteration 24 : 7.401927216955664
Loss in iteration 25 : 6.926276321941025
Loss in iteration 26 : 6.572218784457697
Loss in iteration 27 : 6.491358781081434
Loss in iteration 28 : 6.329632109726039
Loss in iteration 29 : 6.985037176573773
Loss in iteration 30 : 6.576229950238359
Loss in iteration 31 : 7.094518430245062
Loss in iteration 32 : 6.3456573605132505
Loss in iteration 33 : 6.387158726908203
Loss in iteration 34 : 5.455919148661861
Loss in iteration 35 : 6.5286673698703925
Loss in iteration 36 : 5.30615586201587
Loss in iteration 37 : 6.008214355465538
Loss in iteration 38 : 5.369787603046968
Loss in iteration 39 : 6.158511409978258
Loss in iteration 40 : 4.9803712189665665
Loss in iteration 41 : 5.680700339089055
Loss in iteration 42 : 4.441809236330237
Loss in iteration 43 : 5.309336104651495
Loss in iteration 44 : 4.752994199690909
Loss in iteration 45 : 5.331337779017501
Loss in iteration 46 : 4.801305329454631
Loss in iteration 47 : 5.778321838919195
Loss in iteration 48 : 4.567474586491557
Loss in iteration 49 : 4.9937628121785265
Loss in iteration 50 : 4.256560028425357
Loss in iteration 51 : 5.282927137960543
Loss in iteration 52 : 4.536549175093371
Loss in iteration 53 : 5.476217818199879
Loss in iteration 54 : 4.363945125298359
Loss in iteration 55 : 5.146605878630236
Loss in iteration 56 : 4.594707149865927
Loss in iteration 57 : 5.2641869264149275
Loss in iteration 58 : 4.169120909694338
Loss in iteration 59 : 4.878452521522002
Loss in iteration 60 : 3.9837445600336903
Loss in iteration 61 : 4.625652988662526
Loss in iteration 62 : 4.075492126701441
Loss in iteration 63 : 4.7469388871975235
Loss in iteration 64 : 4.085728803314708
Loss in iteration 65 : 5.063891752973585
Loss in iteration 66 : 3.8398427800844424
Loss in iteration 67 : 4.795690636574511
Loss in iteration 68 : 3.6601745301312727
Loss in iteration 69 : 4.457241019316138
Loss in iteration 70 : 3.69799833821981
Loss in iteration 71 : 4.825106451457431
Loss in iteration 72 : 4.101368507542827
Loss in iteration 73 : 4.673307241670726
Loss in iteration 74 : 3.91572829681122
Loss in iteration 75 : 4.6333936350997185
Loss in iteration 76 : 4.053657501337465
Loss in iteration 77 : 4.634586996836427
Loss in iteration 78 : 3.6680974421326855
Loss in iteration 79 : 4.107714292922973
Loss in iteration 80 : 3.429321484894342
Loss in iteration 81 : 4.088273836757181
Loss in iteration 82 : 3.8656706078786107
Loss in iteration 83 : 4.6093600492528495
Loss in iteration 84 : 4.243052926436781
Loss in iteration 85 : 4.66464345988158
Loss in iteration 86 : 3.8751768179278225
Loss in iteration 87 : 4.69076595038061
Loss in iteration 88 : 4.2860468034938535
Loss in iteration 89 : 4.958675779831173
Loss in iteration 90 : 3.767626303734819
Loss in iteration 91 : 4.7302712759962695
Loss in iteration 92 : 3.565690211881102
Loss in iteration 93 : 4.098933402331273
Loss in iteration 94 : 3.6740490498471456
Loss in iteration 95 : 4.445910331819373
Loss in iteration 96 : 3.7674421500939417
Loss in iteration 97 : 4.615406292059572
Loss in iteration 98 : 4.194469571429936
Loss in iteration 99 : 4.836631656461396
Loss in iteration 100 : 3.8978138837101683
Loss in iteration 101 : 4.622654631892583
Loss in iteration 102 : 3.6932203595002138
Loss in iteration 103 : 4.51881118062438
Loss in iteration 104 : 3.944834277665034
Loss in iteration 105 : 4.545573030794959
Loss in iteration 106 : 3.904963288776975
Loss in iteration 107 : 4.692474403389023
Loss in iteration 108 : 4.339391404084757
Loss in iteration 109 : 5.023513054243742
Loss in iteration 110 : 4.021445943120948
Loss in iteration 111 : 4.8824561008051575
Loss in iteration 112 : 3.135960554864051
Loss in iteration 113 : 3.5265813529102243
Loss in iteration 114 : 3.4420000997064157
Loss in iteration 115 : 4.209419454263883
Loss in iteration 116 : 3.6404509896110895
Loss in iteration 117 : 4.561151431949148
Loss in iteration 118 : 3.8831461502944884
Loss in iteration 119 : 4.36408668921162
Loss in iteration 120 : 4.117564441907507
Loss in iteration 121 : 4.816458953394591
Loss in iteration 122 : 3.838561746308428
Loss in iteration 123 : 4.161519482290122
Loss in iteration 124 : 3.6320938375888328
Loss in iteration 125 : 4.37027661543433
Loss in iteration 126 : 3.610311313829391
Loss in iteration 127 : 4.459490749175937
Loss in iteration 128 : 3.752240937408279
Loss in iteration 129 : 4.309513670044026
Loss in iteration 130 : 3.6922285728033586
Loss in iteration 131 : 4.206883626551162
Loss in iteration 132 : 3.3760316998647597
Loss in iteration 133 : 4.150294766081572
Loss in iteration 134 : 3.7345614966072502
Loss in iteration 135 : 4.403199231548229
Loss in iteration 136 : 3.4957287872879306
Loss in iteration 137 : 4.136253563464503
Loss in iteration 138 : 3.5002232756687137
Loss in iteration 139 : 4.295949240165046
Loss in iteration 140 : 3.445801574779165
Loss in iteration 141 : 4.062772066243173
Loss in iteration 142 : 3.73970290466939
Loss in iteration 143 : 4.625275043121065
Loss in iteration 144 : 3.761974745672724
Loss in iteration 145 : 4.2877540853574585
Loss in iteration 146 : 3.549771454711476
Loss in iteration 147 : 3.977944365196571
Loss in iteration 148 : 3.714948770862733
Loss in iteration 149 : 4.516736017486215
Loss in iteration 150 : 3.5032669598878936
Loss in iteration 151 : 4.1782466429920895
Loss in iteration 152 : 3.7994931170245483
Loss in iteration 153 : 4.364433947127113
Loss in iteration 154 : 3.8137211416696437
Loss in iteration 155 : 4.314044558982918
Loss in iteration 156 : 3.619310511574784
Loss in iteration 157 : 4.738519396667553
Loss in iteration 158 : 3.9828086069438022
Loss in iteration 159 : 4.406889224760045
Loss in iteration 160 : 3.5919587298122404
Loss in iteration 161 : 4.271959485631038
Loss in iteration 162 : 3.6051761751975153
Loss in iteration 163 : 4.148349076947649
Loss in iteration 164 : 3.8573398718030445
Loss in iteration 165 : 4.575786891171659
Loss in iteration 166 : 3.843733154660782
Loss in iteration 167 : 4.164006981565331
Loss in iteration 168 : 3.636009222038377
Loss in iteration 169 : 4.150808103154387
Loss in iteration 170 : 3.868916207150857
Loss in iteration 171 : 4.241361672185208
Loss in iteration 172 : 3.6414315821208065
Loss in iteration 173 : 4.233347931232262
Loss in iteration 174 : 3.597784628534709
Loss in iteration 175 : 4.341465912078989
Loss in iteration 176 : 3.5884074999850313
Loss in iteration 177 : 4.2735409080813005
Loss in iteration 178 : 3.822081970939469
Loss in iteration 179 : 4.640573778043001
Loss in iteration 180 : 3.7341549367467928
Loss in iteration 181 : 4.494479475142207
Loss in iteration 182 : 3.8295108783003693
Loss in iteration 183 : 4.360763470334889
Loss in iteration 184 : 3.6601009570574443
Loss in iteration 185 : 4.137580406198623
Loss in iteration 186 : 3.4174925926865907
Loss in iteration 187 : 3.803879185687373
Loss in iteration 188 : 3.406936730014119
Loss in iteration 189 : 3.938210847096435
Loss in iteration 190 : 3.3875739173828228
Loss in iteration 191 : 3.77652629932783
Loss in iteration 192 : 3.6093322995953816
Loss in iteration 193 : 4.457330861408796
Loss in iteration 194 : 3.779438727706733
Loss in iteration 195 : 4.463854395072095
Loss in iteration 196 : 3.984102315449849
Loss in iteration 197 : 4.300636599583239
Loss in iteration 198 : 3.7326341748145957
Loss in iteration 199 : 4.000697263589709
Loss in iteration 200 : 3.3475109500174476
Loss in iteration 201 : 4.219925642023806
Loss in iteration 202 : 4.014573291191549
Loss in iteration 203 : 4.39655814507404
Loss in iteration 204 : 3.813542186193243
Loss in iteration 205 : 4.5598323597051875
Loss in iteration 206 : 3.954903387019399
Loss in iteration 207 : 4.33431944371889
Loss in iteration 208 : 3.80888833332384
Loss in iteration 209 : 4.20481184893755
Loss in iteration 210 : 3.545433305139488
Loss in iteration 211 : 4.074336771891549
Loss in iteration 212 : 3.5628903258519657
Loss in iteration 213 : 3.9275998592826356
Loss in iteration 214 : 3.6747594735548184
Loss in iteration 215 : 4.289878597556631
Loss in iteration 216 : 3.8075502193564597
Loss in iteration 217 : 4.425965415079897
Loss in iteration 218 : 3.2241460239834945
Loss in iteration 219 : 3.924820834065501
Loss in iteration 220 : 3.6833739998139787
Loss in iteration 221 : 4.0932059311606945
Loss in iteration 222 : 3.3553334959105205
Loss in iteration 223 : 3.885587330765692
Loss in iteration 224 : 3.9268759598112144
Loss in iteration 225 : 4.470909048114838
Loss in iteration 226 : 3.706391700726669
Loss in iteration 227 : 4.05112281749294
Loss in iteration 228 : 3.460662473677354
Loss in iteration 229 : 3.9287447803175497
Loss in iteration 230 : 3.447234638241272
Loss in iteration 231 : 3.9804190301587465
Loss in iteration 232 : 3.5554812719869733
Loss in iteration 233 : 4.116161239716783
Loss in iteration 234 : 3.837516965823874
Loss in iteration 235 : 4.056056307921339
Loss in iteration 236 : 3.4069788209377556
Loss in iteration 237 : 3.8448787602290584
Loss in iteration 238 : 3.2625970303885925
Loss in iteration 239 : 3.7675489408460474
Loss in iteration 240 : 3.2031191864887107
Loss in iteration 241 : 3.8184385097603326
Loss in iteration 242 : 3.9845793694672174
Loss in iteration 243 : 4.625443020384521
Loss in iteration 244 : 3.9629681892106112
Loss in iteration 245 : 4.756892737534664
Loss in iteration 246 : 3.6692943977095305
Loss in iteration 247 : 3.9532111014242735
Loss in iteration 248 : 3.7526505996340522
Loss in iteration 249 : 4.219645344949484
Loss in iteration 250 : 3.6015556166166878
Loss in iteration 251 : 4.202894231924791
Loss in iteration 252 : 3.6464619859795255
Loss in iteration 253 : 4.354545747435089
Loss in iteration 254 : 3.6178793450016085
Loss in iteration 255 : 4.1626663307395555
Loss in iteration 256 : 3.7789543015470572
Loss in iteration 257 : 3.9530182805301837
Loss in iteration 258 : 3.5518983645225854
Loss in iteration 259 : 4.046506809818456
Loss in iteration 260 : 3.640910112452743
Loss in iteration 261 : 4.246525367728704
Loss in iteration 262 : 3.705586704700807
Loss in iteration 263 : 4.2763026495210665
Loss in iteration 264 : 3.7856835034510943
Loss in iteration 265 : 4.014996042837804
Loss in iteration 266 : 3.525611956698211
Loss in iteration 267 : 3.5032487372601064
Loss in iteration 268 : 3.337470468036098
Loss in iteration 269 : 3.808131609219126
Loss in iteration 270 : 3.693224679718287
Loss in iteration 271 : 4.301405137002464
Loss in iteration 272 : 3.9492282833475576
Loss in iteration 273 : 4.494399113405161
Loss in iteration 274 : 3.5818250904288824
Loss in iteration 275 : 4.081829924197638
Loss in iteration 276 : 3.582758070445782
Loss in iteration 277 : 4.069412633499251
Loss in iteration 278 : 3.6880938625349833
Loss in iteration 279 : 4.2401496863377295
Loss in iteration 280 : 3.6557803893028833
Loss in iteration 281 : 3.9371348860541975
Loss in iteration 282 : 3.4568314907598694
Loss in iteration 283 : 3.987492497721466
Loss in iteration 284 : 3.6337914758472647
Loss in iteration 285 : 4.332941659741191
Loss in iteration 286 : 3.757040185992184
Loss in iteration 287 : 4.227340727515435
Loss in iteration 288 : 3.5163735960874023
Loss in iteration 289 : 3.680140661159057
Loss in iteration 290 : 3.1015665265673285
Loss in iteration 291 : 3.6291557345093746
Loss in iteration 292 : 3.398344334231433
Loss in iteration 293 : 3.9555787351491043
Loss in iteration 294 : 3.697719817443576
Loss in iteration 295 : 4.058920699985502
Loss in iteration 296 : 3.855034428581722
Loss in iteration 297 : 4.427850641256733
Loss in iteration 298 : 3.8151862550910605
Loss in iteration 299 : 4.288788442882396
Loss in iteration 300 : 3.771530747267531
Loss in iteration 301 : 4.400620399092715
Loss in iteration 302 : 3.8292229686979704
Loss in iteration 303 : 4.043240384386312
Loss in iteration 304 : 3.591900822925102
Loss in iteration 305 : 3.815040919109426
Loss in iteration 306 : 3.4245747402002804
Loss in iteration 307 : 3.8046655244832626
Loss in iteration 308 : 3.5494974907558996
Loss in iteration 309 : 3.9157788383383125
Loss in iteration 310 : 3.609943237788006
Loss in iteration 311 : 4.209439812216964
Loss in iteration 312 : 3.7644008756956797
Loss in iteration 313 : 3.9636424174189187
Loss in iteration 314 : 3.3965022096029163
Loss in iteration 315 : 3.938013305011995
Loss in iteration 316 : 3.6607306821137238
Loss in iteration 317 : 3.967635595323376
Loss in iteration 318 : 3.7336915911625206
Loss in iteration 319 : 4.0177696711043795
Loss in iteration 320 : 3.6941439352769536
Loss in iteration 321 : 4.379546560690656
Loss in iteration 322 : 3.753706985256102
Loss in iteration 323 : 4.06943571323365
Loss in iteration 324 : 3.520363860039322
Loss in iteration 325 : 3.855887369571525
Loss in iteration 326 : 3.8144058868434434
Loss in iteration 327 : 3.815026100107397
Loss in iteration 328 : 3.3406130459015593
Loss in iteration 329 : 3.84226894936078
Loss in iteration 330 : 3.6416399773514248
Loss in iteration 331 : 4.034420689204439
Loss in iteration 332 : 3.304010938064961
Loss in iteration 333 : 3.770605223288787
Loss in iteration 334 : 3.5207046958025683
Loss in iteration 335 : 3.8802273911396954
Loss in iteration 336 : 3.4978160142119945
Loss in iteration 337 : 4.132055119928467
Loss in iteration 338 : 3.706504037626438
Loss in iteration 339 : 4.102854891143459
Loss in iteration 340 : 3.393454137434803
Loss in iteration 341 : 3.6387935017923962
Loss in iteration 342 : 3.3758326119734536
Loss in iteration 343 : 3.921940898037295
Loss in iteration 344 : 3.569372216428229
Loss in iteration 345 : 4.042739170707871
Loss in iteration 346 : 3.6154477425793243
Loss in iteration 347 : 3.9122025316694384
Loss in iteration 348 : 3.8098465674094353
Loss in iteration 349 : 4.4392937286380105
Loss in iteration 350 : 4.039931821697129
Loss in iteration 351 : 4.274433102868721
Loss in iteration 352 : 3.540568812324505
Loss in iteration 353 : 3.6958247530872677
Loss in iteration 354 : 3.4417071445175242
Loss in iteration 355 : 3.908427470416519
Loss in iteration 356 : 3.416122377982727
Loss in iteration 357 : 3.904827935073167
Loss in iteration 358 : 3.5504924468302437
Loss in iteration 359 : 3.713672124443336
Loss in iteration 360 : 3.530932561799644
Loss in iteration 361 : 3.839041859801067
Loss in iteration 362 : 3.6176201030724764
Loss in iteration 363 : 4.018510551169126
Loss in iteration 364 : 3.6456098562633006
Loss in iteration 365 : 4.243805857356886
Loss in iteration 366 : 3.7014379294706234
Loss in iteration 367 : 4.162964872046987
Loss in iteration 368 : 3.4011492636229756
Loss in iteration 369 : 3.936293266755176
Loss in iteration 370 : 3.7232956820771532
Loss in iteration 371 : 4.23849828407068
Loss in iteration 372 : 3.629952850217421
Loss in iteration 373 : 4.2585447098103995
Loss in iteration 374 : 4.14582104653923
Loss in iteration 375 : 4.341255981062167
Loss in iteration 376 : 3.850851314485859
Loss in iteration 377 : 4.079805341474379
Loss in iteration 378 : 3.5539477597549873
Loss in iteration 379 : 3.800515462061359
Loss in iteration 380 : 3.45250096026708
Loss in iteration 381 : 3.756461510910997
Loss in iteration 382 : 3.506753147267571
Loss in iteration 383 : 3.9901654199796677
Loss in iteration 384 : 3.4999910675463277
Loss in iteration 385 : 3.6337991946949972
Loss in iteration 386 : 3.3177045842618367
Loss in iteration 387 : 3.828234550046631
Loss in iteration 388 : 3.5065719280568826
Loss in iteration 389 : 3.6044702695449455
Loss in iteration 390 : 3.168828940729121
Loss in iteration 391 : 3.5746250857131185
Loss in iteration 392 : 3.383973606039148
Loss in iteration 393 : 3.7628862089580006
Loss in iteration 394 : 3.4013694894701407
Loss in iteration 395 : 3.9310888453421415
Loss in iteration 396 : 3.5179353067208017
Loss in iteration 397 : 4.07473326995339
Loss in iteration 398 : 3.534517413674742
Loss in iteration 399 : 3.8751733022546597
Loss in iteration 400 : 3.1238380832509614
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.727625, training accuracy 0.727625, time elapsed: 9800 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.566322372852807
Loss in iteration 3 : 15.705449647037703
Loss in iteration 4 : 6.2941238772599934
Loss in iteration 5 : 10.290742554315024
Loss in iteration 6 : 8.957112794049875
Loss in iteration 7 : 6.255167625987286
Loss in iteration 8 : 10.671914812097924
Loss in iteration 9 : 2.9021300696819696
Loss in iteration 10 : 9.845938516672938
Loss in iteration 11 : 4.079523443438701
Loss in iteration 12 : 9.43832328976061
Loss in iteration 13 : 3.4195571424542233
Loss in iteration 14 : 8.3751974764175
Loss in iteration 15 : 4.447046463643887
Loss in iteration 16 : 8.293461244197935
Loss in iteration 17 : 4.314481744900581
Loss in iteration 18 : 7.299294643497096
Loss in iteration 19 : 4.624904720670881
Loss in iteration 20 : 6.5013771076536875
Loss in iteration 21 : 5.334976132307958
Loss in iteration 22 : 5.89848252826911
Loss in iteration 23 : 4.977384009401881
Loss in iteration 24 : 5.2560850097988245
Loss in iteration 25 : 4.823233018192207
Loss in iteration 26 : 4.6610530619846235
Loss in iteration 27 : 4.535613114682588
Loss in iteration 28 : 4.4986527917110255
Loss in iteration 29 : 4.876231438772557
Loss in iteration 30 : 4.667658790405525
Loss in iteration 31 : 4.949667718993702
Loss in iteration 32 : 4.513098175263473
Loss in iteration 33 : 4.456681580409833
Loss in iteration 34 : 3.8779287200150825
Loss in iteration 35 : 4.554574836643237
Loss in iteration 36 : 3.770246273496055
Loss in iteration 37 : 4.200287749827573
Loss in iteration 38 : 3.7982221915066954
Loss in iteration 39 : 4.30262471930837
Loss in iteration 40 : 3.536694277909177
Loss in iteration 41 : 3.9591548351782926
Loss in iteration 42 : 3.14333346963091
Loss in iteration 43 : 3.6922637685785564
Loss in iteration 44 : 3.3446792215349928
Loss in iteration 45 : 3.7138239283184764
Loss in iteration 46 : 3.383309466283606
Loss in iteration 47 : 4.024654565946351
Loss in iteration 48 : 3.2272287822822276
Loss in iteration 49 : 3.489331109429336
Loss in iteration 50 : 3.0056906367010567
Loss in iteration 51 : 3.6878854191705916
Loss in iteration 52 : 3.2030516680566787
Loss in iteration 53 : 3.811910542116233
Loss in iteration 54 : 3.0880696191540267
Loss in iteration 55 : 3.5874776008765292
Loss in iteration 56 : 3.241079964634843
Loss in iteration 57 : 3.6629386478474015
Loss in iteration 58 : 2.939159040478609
Loss in iteration 59 : 3.3918403220799394
Loss in iteration 60 : 2.8198396986008385
Loss in iteration 61 : 3.2371207191506817
Loss in iteration 62 : 2.8820391641526486
Loss in iteration 63 : 3.3080026627870533
Loss in iteration 64 : 2.882869591486656
Loss in iteration 65 : 3.5278456749362723
Loss in iteration 66 : 2.720327797872199
Loss in iteration 67 : 3.3452181812813215
Loss in iteration 68 : 2.5897650833975874
Loss in iteration 69 : 3.100382039596461
Loss in iteration 70 : 2.6052960105309237
Loss in iteration 71 : 3.3440952811831983
Loss in iteration 72 : 2.8855100570088714
Loss in iteration 73 : 3.2472784476807837
Loss in iteration 74 : 2.7589561886435603
Loss in iteration 75 : 3.229306122588018
Loss in iteration 76 : 2.858752597707214
Loss in iteration 77 : 3.2124908383147743
Loss in iteration 78 : 2.586547093074749
Loss in iteration 79 : 2.8631168105692932
Loss in iteration 80 : 2.4201041982688505
Loss in iteration 81 : 2.856427275090203
Loss in iteration 82 : 2.720346413079521
Loss in iteration 83 : 3.206470376188618
Loss in iteration 84 : 2.984268178601932
Loss in iteration 85 : 3.2410465628855167
Loss in iteration 86 : 2.741621358511474
Loss in iteration 87 : 3.2754962370161
Loss in iteration 88 : 3.020081706420979
Loss in iteration 89 : 3.451799313606877
Loss in iteration 90 : 2.673627409479896
Loss in iteration 91 : 3.3010411361111243
Loss in iteration 92 : 2.524508519932482
Loss in iteration 93 : 2.8676117806843053
Loss in iteration 94 : 2.594630138849088
Loss in iteration 95 : 3.09406648691257
Loss in iteration 96 : 2.652732778963258
Loss in iteration 97 : 3.2069147204386885
Loss in iteration 98 : 2.9558697281762236
Loss in iteration 99 : 3.3623101071504418
Loss in iteration 100 : 2.7570857921082763
Loss in iteration 101 : 3.2248347163727353
Loss in iteration 102 : 2.6064406553100157
Loss in iteration 103 : 3.149934561647337
Loss in iteration 104 : 2.7801208400262185
Loss in iteration 105 : 3.1605251258686127
Loss in iteration 106 : 2.758641268754259
Loss in iteration 107 : 3.2683475246634894
Loss in iteration 108 : 3.0509718988739647
Loss in iteration 109 : 3.486055168884775
Loss in iteration 110 : 2.8562946567962237
Loss in iteration 111 : 3.3961865844154686
Loss in iteration 112 : 2.2392518521893687
Loss in iteration 113 : 2.4829451141810144
Loss in iteration 114 : 2.439103727394942
Loss in iteration 115 : 2.947091888993015
Loss in iteration 116 : 2.5663113870737733
Loss in iteration 117 : 3.1728488845000284
Loss in iteration 118 : 2.717886711774923
Loss in iteration 119 : 3.016806541398398
Loss in iteration 120 : 2.8746698618275435
Loss in iteration 121 : 3.3333693449741064
Loss in iteration 122 : 2.7171061396457064
Loss in iteration 123 : 2.9032051577086704
Loss in iteration 124 : 2.5556321069801364
Loss in iteration 125 : 3.040729485836893
Loss in iteration 126 : 2.551220041171672
Loss in iteration 127 : 3.1079784250588345
Loss in iteration 128 : 2.6459185302827573
Loss in iteration 129 : 2.9922918699065884
Loss in iteration 130 : 2.596249163773665
Loss in iteration 131 : 2.9224401994686793
Loss in iteration 132 : 2.379829958807591
Loss in iteration 133 : 2.8938392816441736
Loss in iteration 134 : 2.6281205529420917
Loss in iteration 135 : 3.047141173044537
Loss in iteration 136 : 2.464845283327644
Loss in iteration 137 : 2.889364499616378
Loss in iteration 138 : 2.471464598158385
Loss in iteration 139 : 2.996042651455908
Loss in iteration 140 : 2.43780713494831
Loss in iteration 141 : 2.8345681583283424
Loss in iteration 142 : 2.6309942604509464
Loss in iteration 143 : 3.2014620372955056
Loss in iteration 144 : 2.660958033402575
Loss in iteration 145 : 2.99708307763056
Loss in iteration 146 : 2.507981394236099
Loss in iteration 147 : 2.767000147958511
Loss in iteration 148 : 2.5960068059177654
Loss in iteration 149 : 3.115648921404897
Loss in iteration 150 : 2.473594766733672
Loss in iteration 151 : 2.919349389272866
Loss in iteration 152 : 2.688128363512898
Loss in iteration 153 : 3.0464758003276553
Loss in iteration 154 : 2.692944996438991
Loss in iteration 155 : 3.0088676107428225
Loss in iteration 156 : 2.5585171723144806
Loss in iteration 157 : 3.2984996146057197
Loss in iteration 158 : 2.815855937652887
Loss in iteration 159 : 3.0653366994407705
Loss in iteration 160 : 2.5390075923738675
Loss in iteration 161 : 2.9697421250012335
Loss in iteration 162 : 2.548427229024322
Loss in iteration 163 : 2.897209083437402
Loss in iteration 164 : 2.7113794809811926
Loss in iteration 165 : 3.167101729512467
Loss in iteration 166 : 2.7057933704983084
Loss in iteration 167 : 2.8928436895418765
Loss in iteration 168 : 2.5624431527630076
Loss in iteration 169 : 2.892398445363333
Loss in iteration 170 : 2.7203773362381174
Loss in iteration 171 : 2.9483700466507856
Loss in iteration 172 : 2.568813908054
Loss in iteration 173 : 2.945605876981674
Loss in iteration 174 : 2.530972826157841
Loss in iteration 175 : 3.0168399864999698
Loss in iteration 176 : 2.526802571315138
Loss in iteration 177 : 2.9671068287043054
Loss in iteration 178 : 2.6887800509399247
Loss in iteration 179 : 3.218514096736322
Loss in iteration 180 : 2.637464401643963
Loss in iteration 181 : 3.1186287993554034
Loss in iteration 182 : 2.70116184245929
Loss in iteration 183 : 3.039637569937153
Loss in iteration 184 : 2.5833193493530695
Loss in iteration 185 : 2.885635138056421
Loss in iteration 186 : 2.4116476724160427
Loss in iteration 187 : 2.6560726949619835
Loss in iteration 188 : 2.404110020029859
Loss in iteration 189 : 2.749451409507737
Loss in iteration 190 : 2.3856701162821623
Loss in iteration 191 : 2.63129032977589
Loss in iteration 192 : 2.547357198106959
Loss in iteration 193 : 3.101994469739602
Loss in iteration 194 : 2.6682526727629767
Loss in iteration 195 : 3.1100605914741526
Loss in iteration 196 : 2.815238091647557
Loss in iteration 197 : 2.9947604929453364
Loss in iteration 198 : 2.635809964935593
Loss in iteration 199 : 2.795549684200699
Loss in iteration 200 : 2.3599349437719352
Loss in iteration 201 : 2.9348202274006385
Loss in iteration 202 : 2.8232287735808548
Loss in iteration 203 : 3.053437047047723
Loss in iteration 204 : 2.685141948485347
Loss in iteration 205 : 3.168036397534455
Loss in iteration 206 : 2.7899011474207196
Loss in iteration 207 : 3.0074467036014707
Loss in iteration 208 : 2.682082328946696
Loss in iteration 209 : 2.9350610587013777
Loss in iteration 210 : 2.4974487804010868
Loss in iteration 211 : 2.840048073045158
Loss in iteration 212 : 2.512407561290471
Loss in iteration 213 : 2.7428051814450036
Loss in iteration 214 : 2.578715541565223
Loss in iteration 215 : 2.975440543699995
Loss in iteration 216 : 2.6732287091413705
Loss in iteration 217 : 3.064834996548389
Loss in iteration 218 : 2.286834563239327
Loss in iteration 219 : 2.756786222671981
Loss in iteration 220 : 2.5983995728468905
Loss in iteration 221 : 2.8547318542434277
Loss in iteration 222 : 2.370001713668679
Loss in iteration 223 : 2.7175712816659146
Loss in iteration 224 : 2.7575452361586987
Loss in iteration 225 : 3.101852668535761
Loss in iteration 226 : 2.615789317574393
Loss in iteration 227 : 2.8211005882635276
Loss in iteration 228 : 2.4453701407370723
Loss in iteration 229 : 2.742250900980863
Loss in iteration 230 : 2.424490910782581
Loss in iteration 231 : 2.7740492025596475
Loss in iteration 232 : 2.4989001551740317
Loss in iteration 233 : 2.877765155727217
Loss in iteration 234 : 2.7059484158993534
Loss in iteration 235 : 2.833562074512894
Loss in iteration 236 : 2.400203624589006
Loss in iteration 237 : 2.67450666804417
Loss in iteration 238 : 2.291886311886613
Loss in iteration 239 : 2.6148707932510775
Loss in iteration 240 : 2.2555722268850036
Loss in iteration 241 : 2.6716467518988263
Loss in iteration 242 : 2.797148345332118
Loss in iteration 243 : 3.2080164549096377
Loss in iteration 244 : 2.783733347463359
Loss in iteration 245 : 3.300954961094103
Loss in iteration 246 : 2.5897065652139717
Loss in iteration 247 : 2.7702375518071634
Loss in iteration 248 : 2.638588746588695
Loss in iteration 249 : 2.9309997732987356
Loss in iteration 250 : 2.536217656799167
Loss in iteration 251 : 2.9330216303103414
Loss in iteration 252 : 2.5652117563408448
Loss in iteration 253 : 3.022439820244064
Loss in iteration 254 : 2.5443288600121186
Loss in iteration 255 : 2.896123697039758
Loss in iteration 256 : 2.657562830974894
Loss in iteration 257 : 2.757383271101972
Loss in iteration 258 : 2.5065310670137624
Loss in iteration 259 : 2.8236366236163986
Loss in iteration 260 : 2.56499182050918
Loss in iteration 261 : 2.95980219642388
Loss in iteration 262 : 2.6200040761625796
Loss in iteration 263 : 2.9880156250648016
Loss in iteration 264 : 2.6656798652886966
Loss in iteration 265 : 2.79868984859846
Loss in iteration 266 : 2.4809447033375314
Loss in iteration 267 : 2.4541816120594486
Loss in iteration 268 : 2.3708605813289463
Loss in iteration 269 : 2.6823914197320513
Loss in iteration 270 : 2.603872272879153
Loss in iteration 271 : 3.001031529659549
Loss in iteration 272 : 2.7755763813046674
Loss in iteration 273 : 3.1084591534090764
Loss in iteration 274 : 2.53303200700861
Loss in iteration 275 : 2.855373829464047
Loss in iteration 276 : 2.5281408861694437
Loss in iteration 277 : 2.8454998651572536
Loss in iteration 278 : 2.5974167934175085
Loss in iteration 279 : 2.956592398080528
Loss in iteration 280 : 2.567113851221693
Loss in iteration 281 : 2.7467452209962806
Loss in iteration 282 : 2.428001729090634
Loss in iteration 283 : 2.781013569848396
Loss in iteration 284 : 2.5518853951892027
Loss in iteration 285 : 3.0115562414048136
Loss in iteration 286 : 2.6466736907574915
Loss in iteration 287 : 2.9419189427911365
Loss in iteration 288 : 2.4696619232241064
Loss in iteration 289 : 2.5613163767101006
Loss in iteration 290 : 2.1802532864792794
Loss in iteration 291 : 2.526378318445553
Loss in iteration 292 : 2.388524696983903
Loss in iteration 293 : 2.756233578988206
Loss in iteration 294 : 2.6016958196312876
Loss in iteration 295 : 2.828678778978432
Loss in iteration 296 : 2.708803300435971
Loss in iteration 297 : 3.0951612421376544
Loss in iteration 298 : 2.6922619409250155
Loss in iteration 299 : 2.9949548816954086
Loss in iteration 300 : 2.649343178631916
Loss in iteration 301 : 3.061395703486682
Loss in iteration 302 : 2.685776601138267
Loss in iteration 303 : 2.805623077557683
Loss in iteration 304 : 2.5306440671133927
Loss in iteration 305 : 2.6669988571221155
Loss in iteration 306 : 2.40666329157059
Loss in iteration 307 : 2.6599120602191415
Loss in iteration 308 : 2.4955228200675252
Loss in iteration 309 : 2.72305107660228
Loss in iteration 310 : 2.537071191907552
Loss in iteration 311 : 2.936475451188485
Loss in iteration 312 : 2.6483937137950515
Loss in iteration 313 : 2.7603318184005317
Loss in iteration 314 : 2.3882663476625607
Loss in iteration 315 : 2.7401042202694845
Loss in iteration 316 : 2.565883220917227
Loss in iteration 317 : 2.755771218753742
Loss in iteration 318 : 2.6214952448770545
Loss in iteration 319 : 2.795406481164999
Loss in iteration 320 : 2.5944303079537288
Loss in iteration 321 : 3.0477942394329736
Loss in iteration 322 : 2.637860710357126
Loss in iteration 323 : 2.842828115406696
Loss in iteration 324 : 2.484346212899307
Loss in iteration 325 : 2.702304010243545
Loss in iteration 326 : 2.6836234776462544
Loss in iteration 327 : 2.669426317741271
Loss in iteration 328 : 2.3582248524749128
Loss in iteration 329 : 2.694000426501496
Loss in iteration 330 : 2.5682967764756133
Loss in iteration 331 : 2.823595876342848
Loss in iteration 332 : 2.3288455109554707
Loss in iteration 333 : 2.6327414244626297
Loss in iteration 334 : 2.472588865661962
Loss in iteration 335 : 2.7135499913759356
Loss in iteration 336 : 2.4663934575494793
Loss in iteration 337 : 2.887868484059175
Loss in iteration 338 : 2.6092669825696175
Loss in iteration 339 : 2.8570935699597975
Loss in iteration 340 : 2.387636107964573
Loss in iteration 341 : 2.527710692635832
Loss in iteration 342 : 2.360769531752898
Loss in iteration 343 : 2.7249703483402534
Loss in iteration 344 : 2.508340703401643
Loss in iteration 345 : 2.807732025912982
Loss in iteration 346 : 2.543068344760622
Loss in iteration 347 : 2.734418560462553
Loss in iteration 348 : 2.6750134561659795
Loss in iteration 349 : 3.0844497769439014
Loss in iteration 350 : 2.8351767526630627
Loss in iteration 351 : 2.9743596415270948
Loss in iteration 352 : 2.491446188328661
Loss in iteration 353 : 2.573359318860042
Loss in iteration 354 : 2.4170331874721063
Loss in iteration 355 : 2.726415264586417
Loss in iteration 356 : 2.4062624411766804
Loss in iteration 357 : 2.73119653995536
Loss in iteration 358 : 2.497797258730612
Loss in iteration 359 : 2.590039805802167
Loss in iteration 360 : 2.486575133545995
Loss in iteration 361 : 2.6855314269862616
Loss in iteration 362 : 2.5476884617969464
Loss in iteration 363 : 2.8056749493209994
Loss in iteration 364 : 2.5645426334084402
Loss in iteration 365 : 2.9554115174098414
Loss in iteration 366 : 2.598557726626797
Loss in iteration 367 : 2.898596038044199
Loss in iteration 368 : 2.3907550266404267
Loss in iteration 369 : 2.7396997571804618
Loss in iteration 370 : 2.61350590048317
Loss in iteration 371 : 2.9479331828511017
Loss in iteration 372 : 2.55533330064188
Loss in iteration 373 : 2.973695064241266
Loss in iteration 374 : 2.907236921061815
Loss in iteration 375 : 3.0150463278171418
Loss in iteration 376 : 2.7061860371910176
Loss in iteration 377 : 2.8459717568382215
Loss in iteration 378 : 2.501125604216739
Loss in iteration 379 : 2.6593971717466336
Loss in iteration 380 : 2.4386986810040283
Loss in iteration 381 : 2.6324647430449724
Loss in iteration 382 : 2.4681057895402256
Loss in iteration 383 : 2.7881684073107666
Loss in iteration 384 : 2.471572642604614
Loss in iteration 385 : 2.550770837179272
Loss in iteration 386 : 2.3448553009996385
Loss in iteration 387 : 2.680729157885554
Loss in iteration 388 : 2.4703407180737202
Loss in iteration 389 : 2.511871550958649
Loss in iteration 390 : 2.2328964241190454
Loss in iteration 391 : 2.5044719670825355
Loss in iteration 392 : 2.390179448556999
Loss in iteration 393 : 2.630833638350908
Loss in iteration 394 : 2.3921446333410312
Loss in iteration 395 : 2.7349982598804763
Loss in iteration 396 : 2.4690818861331674
Loss in iteration 397 : 2.823440927742095
Loss in iteration 398 : 2.4747463184237963
Loss in iteration 399 : 2.683763144159983
Loss in iteration 400 : 2.190185262528848
Testing accuracy  of updater 0 on alg 0 with rate 7.0 = 0.728125, training accuracy 0.728125, time elapsed: 11041 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.022807699595522
Loss in iteration 3 : 7.202929981677378
Loss in iteration 4 : 5.062365335221583
Loss in iteration 5 : 4.1616536620099644
Loss in iteration 6 : 6.5151905066375
Loss in iteration 7 : 2.0953351413133023
Loss in iteration 8 : 6.313221796036391
Loss in iteration 9 : 1.5236177796498906
Loss in iteration 10 : 5.4109338798951985
Loss in iteration 11 : 2.622347658293397
Loss in iteration 12 : 5.607947396584217
Loss in iteration 13 : 1.8064243458471538
Loss in iteration 14 : 4.657769818376071
Loss in iteration 15 : 2.6896482352773257
Loss in iteration 16 : 4.80606380082553
Loss in iteration 17 : 2.427856499859112
Loss in iteration 18 : 4.202718545257082
Loss in iteration 19 : 2.621482247131361
Loss in iteration 20 : 3.780829231453565
Loss in iteration 21 : 2.9846176123658097
Loss in iteration 22 : 3.450340101340168
Loss in iteration 23 : 2.785076348249881
Loss in iteration 24 : 3.0851838695582168
Loss in iteration 25 : 2.6971189743065946
Loss in iteration 26 : 2.7405776140574867
Loss in iteration 27 : 2.5509627319669628
Loss in iteration 28 : 2.656519899764192
Loss in iteration 29 : 2.7382407687032573
Loss in iteration 30 : 2.742005558058034
Loss in iteration 31 : 2.78391745361853
Loss in iteration 32 : 2.6645273469342015
Loss in iteration 33 : 2.511092815643734
Loss in iteration 34 : 2.302401304786802
Loss in iteration 35 : 2.5613869203593413
Loss in iteration 36 : 2.2424354388138488
Loss in iteration 37 : 2.370261553455039
Loss in iteration 38 : 2.233039090286428
Loss in iteration 39 : 2.4233092105873
Loss in iteration 40 : 2.1028580144479134
Loss in iteration 41 : 2.2260521087401712
Loss in iteration 42 : 1.856181235022249
Loss in iteration 43 : 2.06969794120489
Loss in iteration 44 : 1.9539533970442067
Loss in iteration 45 : 2.0877677416963922
Loss in iteration 46 : 1.9776714672259115
Loss in iteration 47 : 2.255586209894454
Loss in iteration 48 : 1.90727541208734
Loss in iteration 49 : 1.9788237125929322
Loss in iteration 50 : 1.7679733284891375
Loss in iteration 51 : 2.0789281887327595
Loss in iteration 52 : 1.8798411264328827
Loss in iteration 53 : 2.1352787029307745
Loss in iteration 54 : 1.820049096151044
Loss in iteration 55 : 2.019084228139112
Loss in iteration 56 : 1.89229486043663
Loss in iteration 57 : 2.048933103062246
Loss in iteration 58 : 1.7276609257690052
Loss in iteration 59 : 1.9046842545875713
Loss in iteration 60 : 1.661877994315196
Loss in iteration 61 : 1.829082412415733
Loss in iteration 62 : 1.6900534940586076
Loss in iteration 63 : 1.8569799848718105
Loss in iteration 64 : 1.6828066084574504
Loss in iteration 65 : 1.9757230190450328
Loss in iteration 66 : 1.6094388216832005
Loss in iteration 67 : 1.879860315663742
Loss in iteration 68 : 1.529719806247127
Loss in iteration 69 : 1.7426066375737266
Loss in iteration 70 : 1.5228436360191757
Loss in iteration 71 : 1.8579742445139613
Loss in iteration 72 : 1.6772117643233828
Loss in iteration 73 : 1.810308629680023
Loss in iteration 74 : 1.608109551702657
Loss in iteration 75 : 1.814724816442257
Loss in iteration 76 : 1.6756472990836833
Loss in iteration 77 : 1.7903007074025317
Loss in iteration 78 : 1.5139389006869826
Loss in iteration 79 : 1.607963164471837
Loss in iteration 80 : 1.4251635390864252
Loss in iteration 81 : 1.6191094456339306
Loss in iteration 82 : 1.5864080112245385
Loss in iteration 83 : 1.7950799795313557
Loss in iteration 84 : 1.732933782428628
Loss in iteration 85 : 1.8124440313329642
Loss in iteration 86 : 1.6134155975508173
Loss in iteration 87 : 1.8422517815906119
Loss in iteration 88 : 1.7646149437329008
Loss in iteration 89 : 1.9307896525265889
Loss in iteration 90 : 1.5867586014167325
Loss in iteration 91 : 1.8582307631027946
Loss in iteration 92 : 1.4962188971712571
Loss in iteration 93 : 1.6327203711953988
Loss in iteration 94 : 1.523190968667105
Loss in iteration 95 : 1.7332326132408695
Loss in iteration 96 : 1.5543094224106413
Loss in iteration 97 : 1.8009596345536718
Loss in iteration 98 : 1.7303570294957726
Loss in iteration 99 : 1.8833066702699857
Loss in iteration 100 : 1.6224005959356416
Loss in iteration 101 : 1.8156413178343123
Loss in iteration 102 : 1.5381761573169426
Loss in iteration 103 : 1.7745896189416615
Loss in iteration 104 : 1.629095467808966
Loss in iteration 105 : 1.769141185772626
Loss in iteration 106 : 1.6164730899842814
Loss in iteration 107 : 1.838946394666107
Loss in iteration 108 : 1.7740775713806503
Loss in iteration 109 : 1.9426004695234864
Loss in iteration 110 : 1.6902972150020739
Loss in iteration 111 : 1.900296342594293
Loss in iteration 112 : 1.347303193830086
Loss in iteration 113 : 1.4256029031766437
Loss in iteration 114 : 1.4390510504119591
Loss in iteration 115 : 1.6648703342245084
Loss in iteration 116 : 1.5064465546409984
Loss in iteration 117 : 1.7843301174943347
Loss in iteration 118 : 1.573058557920941
Loss in iteration 119 : 1.6700837850099048
Loss in iteration 120 : 1.6475215775555347
Loss in iteration 121 : 1.8515075047708924
Loss in iteration 122 : 1.6011715632596624
Loss in iteration 123 : 1.6418629933959183
Loss in iteration 124 : 1.4951217382480957
Loss in iteration 125 : 1.7147804280719925
Loss in iteration 126 : 1.504180044664716
Loss in iteration 127 : 1.7501815231192286
Loss in iteration 128 : 1.5467806738417942
Loss in iteration 129 : 1.6752071739273762
Loss in iteration 130 : 1.5165043507228448
Loss in iteration 131 : 1.6375623492680116
Loss in iteration 132 : 1.400098867675261
Loss in iteration 133 : 1.6320028626703171
Loss in iteration 134 : 1.5327529568640217
Loss in iteration 135 : 1.6979021960265126
Loss in iteration 136 : 1.4487478208189317
Loss in iteration 137 : 1.6379886542930424
Loss in iteration 138 : 1.4561678945026062
Loss in iteration 139 : 1.6898036200239708
Loss in iteration 140 : 1.441642983303969
Loss in iteration 141 : 1.6051384052262014
Loss in iteration 142 : 1.5320920598245584
Loss in iteration 143 : 1.7796579916662554
Loss in iteration 144 : 1.5599620086873331
Loss in iteration 145 : 1.6858824432305126
Loss in iteration 146 : 1.4716063378700108
Loss in iteration 147 : 1.5581570632393829
Loss in iteration 148 : 1.49477943869015
Loss in iteration 149 : 1.7248238008917376
Loss in iteration 150 : 1.4556905956158797
Loss in iteration 151 : 1.6485618834586766
Loss in iteration 152 : 1.578811617558361
Loss in iteration 153 : 1.714850543444395
Loss in iteration 154 : 1.5806392226928927
Loss in iteration 155 : 1.6992165972034223
Loss in iteration 156 : 1.5017712944777581
Loss in iteration 157 : 1.849944118123057
Loss in iteration 158 : 1.6503277192806747
Loss in iteration 159 : 1.721316352819269
Loss in iteration 160 : 1.4955140493907488
Loss in iteration 161 : 1.665802162992135
Loss in iteration 162 : 1.4981688768399537
Loss in iteration 163 : 1.6395965436325433
Loss in iteration 164 : 1.5753721352054224
Loss in iteration 165 : 1.7628350435983235
Loss in iteration 166 : 1.573742961053424
Loss in iteration 167 : 1.6208515529371912
Loss in iteration 168 : 1.497334461583615
Loss in iteration 169 : 1.6221208081281913
Loss in iteration 170 : 1.5752587505385116
Loss in iteration 171 : 1.6473246112614883
Loss in iteration 172 : 1.4993829570836077
Loss in iteration 173 : 1.6487861118399294
Loss in iteration 174 : 1.4712638583935196
Loss in iteration 175 : 1.686098398020912
Loss in iteration 176 : 1.4720569734749784
Loss in iteration 177 : 1.6591797652186075
Loss in iteration 178 : 1.5654577741458375
Loss in iteration 179 : 1.7947806059460791
Loss in iteration 180 : 1.543541224520204
Loss in iteration 181 : 1.7342767791454716
Loss in iteration 182 : 1.572946977543909
Loss in iteration 183 : 1.7096908985400314
Loss in iteration 184 : 1.514272616517765
Loss in iteration 185 : 1.6254055217572938
Loss in iteration 186 : 1.416985978102326
Loss in iteration 187 : 1.50422102207333
Loss in iteration 188 : 1.4085964355076346
Loss in iteration 189 : 1.5518436476163047
Loss in iteration 190 : 1.3929587225034263
Loss in iteration 191 : 1.4850176737346943
Loss in iteration 192 : 1.4904555125876737
Loss in iteration 193 : 1.735526639720251
Loss in iteration 194 : 1.5588612236430264
Loss in iteration 195 : 1.7406238795216333
Loss in iteration 196 : 1.6493410489347116
Loss in iteration 197 : 1.6834298344871954
Loss in iteration 198 : 1.5466931411993892
Loss in iteration 199 : 1.5868041984487287
Loss in iteration 200 : 1.3853812378253885
Loss in iteration 201 : 1.64660060201543
Loss in iteration 202 : 1.6369957143646157
Loss in iteration 203 : 1.7046906026853654
Loss in iteration 204 : 1.5656684429997123
Loss in iteration 205 : 1.7767588003603974
Loss in iteration 206 : 1.6320184441195738
Loss in iteration 207 : 1.686399607597951
Loss in iteration 208 : 1.5634478311680382
Loss in iteration 209 : 1.6587609455236056
Loss in iteration 210 : 1.4661795230182442
Loss in iteration 211 : 1.6065641549139953
Loss in iteration 212 : 1.4739578843125303
Loss in iteration 213 : 1.5521113827667772
Loss in iteration 214 : 1.4953470980889556
Loss in iteration 215 : 1.662909356799629
Loss in iteration 216 : 1.5487580402117025
Loss in iteration 217 : 1.6984166580638327
Loss in iteration 218 : 1.3510927301901512
Loss in iteration 219 : 1.562825701754718
Loss in iteration 220 : 1.510865666261231
Loss in iteration 221 : 1.6025679092277636
Loss in iteration 222 : 1.3895732812261925
Loss in iteration 223 : 1.5407281156192307
Loss in iteration 224 : 1.5941708459535353
Loss in iteration 225 : 1.7245462666109657
Loss in iteration 226 : 1.5227363856006002
Loss in iteration 227 : 1.5775630517812274
Loss in iteration 228 : 1.4380376515660698
Loss in iteration 229 : 1.5530614245987653
Loss in iteration 230 : 1.4184248372731494
Loss in iteration 231 : 1.57250912663498
Loss in iteration 232 : 1.458392858820787
Loss in iteration 233 : 1.6317293974015448
Loss in iteration 234 : 1.5788918276066097
Loss in iteration 235 : 1.6025043188279728
Loss in iteration 236 : 1.4056569483225763
Loss in iteration 237 : 1.5029062839255012
Loss in iteration 238 : 1.33281114150157
Loss in iteration 239 : 1.4614636423724676
Loss in iteration 240 : 1.3189634672721529
Loss in iteration 241 : 1.5157808090557021
Loss in iteration 242 : 1.6135410491066815
Loss in iteration 243 : 1.780438076248011
Loss in iteration 244 : 1.61393552804786
Loss in iteration 245 : 1.8414655995260845
Loss in iteration 246 : 1.5209208705438542
Loss in iteration 247 : 1.5756043259614407
Loss in iteration 248 : 1.535265286116856
Loss in iteration 249 : 1.6446113290305504
Loss in iteration 250 : 1.48163859118078
Loss in iteration 251 : 1.6585183410378586
Loss in iteration 252 : 1.4962789088586115
Loss in iteration 253 : 1.6896112249236743
Loss in iteration 254 : 1.4788603449712074
Loss in iteration 255 : 1.6270083894247347
Loss in iteration 256 : 1.5506810860459006
Loss in iteration 257 : 1.5601239937852822
Loss in iteration 258 : 1.467431215451074
Loss in iteration 259 : 1.58984653820931
Loss in iteration 260 : 1.4943389296930223
Loss in iteration 261 : 1.660792357946399
Loss in iteration 262 : 1.5385612011987184
Loss in iteration 263 : 1.6876970083501177
Loss in iteration 264 : 1.5570148682221434
Loss in iteration 265 : 1.5768334481463242
Loss in iteration 266 : 1.4441886738322467
Loss in iteration 267 : 1.4033578070492112
Loss in iteration 268 : 1.3996488059032257
Loss in iteration 269 : 1.527837194853552
Loss in iteration 270 : 1.515866012489837
Loss in iteration 271 : 1.68942991455703
Loss in iteration 272 : 1.6128530840496371
Loss in iteration 273 : 1.7242343091393248
Loss in iteration 274 : 1.4900510356055396
Loss in iteration 275 : 1.6210932153053843
Loss in iteration 276 : 1.485128843751703
Loss in iteration 277 : 1.611483964617284
Loss in iteration 278 : 1.51715686216905
Loss in iteration 279 : 1.6661806154256433
Loss in iteration 280 : 1.4937005964527426
Loss in iteration 281 : 1.5548729988031094
Loss in iteration 282 : 1.4131875684382038
Loss in iteration 283 : 1.5699859310582156
Loss in iteration 284 : 1.480011794194869
Loss in iteration 285 : 1.6851744553747483
Loss in iteration 286 : 1.5433704687434302
Loss in iteration 287 : 1.65005735549949
Loss in iteration 288 : 1.438145255856211
Loss in iteration 289 : 1.4418819198911645
Loss in iteration 290 : 1.272413424965788
Loss in iteration 291 : 1.4223929449546104
Loss in iteration 292 : 1.3894413463183675
Loss in iteration 293 : 1.5511949806384784
Loss in iteration 294 : 1.5112714060052481
Loss in iteration 295 : 1.5897789544806302
Loss in iteration 296 : 1.5672222633689363
Loss in iteration 297 : 1.7455595974152478
Loss in iteration 298 : 1.5735277694144358
Loss in iteration 299 : 1.6911112980535654
Loss in iteration 300 : 1.543158700737352
Loss in iteration 301 : 1.7198224551455197
Loss in iteration 302 : 1.5598453192717103
Loss in iteration 303 : 1.5744414911921722
Loss in iteration 304 : 1.4756706941770492
Loss in iteration 305 : 1.511624268926426
Loss in iteration 306 : 1.3988323535622498
Loss in iteration 307 : 1.5065358540724128
Loss in iteration 308 : 1.4464832802589003
Loss in iteration 309 : 1.525618481356689
Loss in iteration 310 : 1.4683420500143427
Loss in iteration 311 : 1.6510365914072287
Loss in iteration 312 : 1.5404073965164973
Loss in iteration 313 : 1.5492006858302325
Loss in iteration 314 : 1.3907409549614786
Loss in iteration 315 : 1.5341662257340563
Loss in iteration 316 : 1.4821218596973162
Loss in iteration 317 : 1.5441408929498543
Loss in iteration 318 : 1.5198561286751038
Loss in iteration 319 : 1.5693214916311484
Loss in iteration 320 : 1.504992618502111
Loss in iteration 321 : 1.7105771941817476
Loss in iteration 322 : 1.5352501092539985
Loss in iteration 323 : 1.6083473291316455
Loss in iteration 324 : 1.4559313855349367
Loss in iteration 325 : 1.5373222496711714
Loss in iteration 326 : 1.5546711244850049
Loss in iteration 327 : 1.5113035642447152
Loss in iteration 328 : 1.3858169414192065
Loss in iteration 329 : 1.5323579491878507
Loss in iteration 330 : 1.4991421347341491
Loss in iteration 331 : 1.5980491472994092
Loss in iteration 332 : 1.3635457771266537
Loss in iteration 333 : 1.4886605319973243
Loss in iteration 334 : 1.4341572981118276
Loss in iteration 335 : 1.536054124450389
Loss in iteration 336 : 1.4424998064900008
Loss in iteration 337 : 1.6335943975487193
Loss in iteration 338 : 1.5217789320974957
Loss in iteration 339 : 1.6079034486256065
Loss in iteration 340 : 1.3896759292701122
Loss in iteration 341 : 1.4172393467569058
Loss in iteration 342 : 1.3616287491351244
Loss in iteration 343 : 1.5206971698500271
Loss in iteration 344 : 1.4559212230249705
Loss in iteration 345 : 1.5759325953891763
Loss in iteration 346 : 1.4776531366201147
Loss in iteration 347 : 1.548684578510504
Loss in iteration 348 : 1.5475868511159108
Loss in iteration 349 : 1.7236613370898086
Loss in iteration 350 : 1.638167695548367
Loss in iteration 351 : 1.672305798623715
Loss in iteration 352 : 1.458069593729138
Loss in iteration 353 : 1.4546194696152137
Loss in iteration 354 : 1.406817440681179
Loss in iteration 355 : 1.540557859254794
Loss in iteration 356 : 1.4043717638563724
Loss in iteration 357 : 1.546934677160847
Loss in iteration 358 : 1.4530950991634881
Loss in iteration 359 : 1.4656397122986293
Loss in iteration 360 : 1.448170142536251
Loss in iteration 361 : 1.5165361975358933
Loss in iteration 362 : 1.4808363415663857
Loss in iteration 363 : 1.5799149715830882
Loss in iteration 364 : 1.487873980046052
Loss in iteration 365 : 1.6592334406657698
Loss in iteration 366 : 1.5106869475603772
Loss in iteration 367 : 1.635795859745283
Loss in iteration 368 : 1.3974962955315793
Loss in iteration 369 : 1.5408501684808695
Loss in iteration 370 : 1.5059139857062065
Loss in iteration 371 : 1.6496280791666318
Loss in iteration 372 : 1.484377950867526
Loss in iteration 373 : 1.6715259162222729
Loss in iteration 374 : 1.6725993730247366
Loss in iteration 375 : 1.6856029756047803
Loss in iteration 376 : 1.5676307602697044
Loss in iteration 377 : 1.6067786417768588
Loss in iteration 378 : 1.4555912089703913
Loss in iteration 379 : 1.5084522644916394
Loss in iteration 380 : 1.4256089026835221
Loss in iteration 381 : 1.4906740230620907
Loss in iteration 382 : 1.435910361202507
Loss in iteration 383 : 1.5749776364270311
Loss in iteration 384 : 1.4479024750876537
Loss in iteration 385 : 1.4559173617167493
Loss in iteration 386 : 1.3715469778797091
Loss in iteration 387 : 1.5238622710487335
Loss in iteration 388 : 1.4405148441554458
Loss in iteration 389 : 1.4162504293150213
Loss in iteration 390 : 1.3041803160259953
Loss in iteration 391 : 1.4271705301771307
Loss in iteration 392 : 1.397567815581196
Loss in iteration 393 : 1.4915628753674706
Loss in iteration 394 : 1.39213435649491
Loss in iteration 395 : 1.5394218375567341
Loss in iteration 396 : 1.4323936754991655
Loss in iteration 397 : 1.5777495080324329
Loss in iteration 398 : 1.430187634302192
Loss in iteration 399 : 1.4942359852114262
Loss in iteration 400 : 1.272985195104924
Testing accuracy  of updater 0 on alg 0 with rate 4.0 = 0.730125, training accuracy 0.730125, time elapsed: 10146 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6869956625181269
Loss in iteration 3 : 0.7516521361520582
Loss in iteration 4 : 1.0082773245760144
Loss in iteration 5 : 1.0280921423270604
Loss in iteration 6 : 1.2279781270127295
Loss in iteration 7 : 0.899013571356844
Loss in iteration 8 : 1.0782273058389167
Loss in iteration 9 : 0.8114494530230616
Loss in iteration 10 : 0.970833488278332
Loss in iteration 11 : 0.8319620705211335
Loss in iteration 12 : 0.9415186834600445
Loss in iteration 13 : 0.7501494665634578
Loss in iteration 14 : 0.8347967551309354
Loss in iteration 15 : 0.7462164388799257
Loss in iteration 16 : 0.815979456010145
Loss in iteration 17 : 0.7237982998958687
Loss in iteration 18 : 0.7666281426857569
Loss in iteration 19 : 0.6855955134635016
Loss in iteration 20 : 0.7106089529907789
Loss in iteration 21 : 0.6817804257808343
Loss in iteration 22 : 0.6900952882683422
Loss in iteration 23 : 0.6396177545437666
Loss in iteration 24 : 0.635742460598328
Loss in iteration 25 : 0.6056583905108262
Loss in iteration 26 : 0.5920460285932839
Loss in iteration 27 : 0.5702313766963366
Loss in iteration 28 : 0.5789645630371654
Loss in iteration 29 : 0.5777544016376783
Loss in iteration 30 : 0.5847500363472165
Loss in iteration 31 : 0.5772401634325528
Loss in iteration 32 : 0.5839319321049942
Loss in iteration 33 : 0.5606277540906105
Loss in iteration 34 : 0.5604023152923139
Loss in iteration 35 : 0.5615833938274628
Loss in iteration 36 : 0.5570994434539104
Loss in iteration 37 : 0.5427273782645811
Loss in iteration 38 : 0.5464360410727482
Loss in iteration 39 : 0.5389804378614257
Loss in iteration 40 : 0.540681991370703
Loss in iteration 41 : 0.5309951576397433
Loss in iteration 42 : 0.512673167253841
Loss in iteration 43 : 0.5111600960667059
Loss in iteration 44 : 0.5077767777016946
Loss in iteration 45 : 0.5043910739734759
Loss in iteration 46 : 0.504732403008494
Loss in iteration 47 : 0.5031451919046178
Loss in iteration 48 : 0.5039718848927687
Loss in iteration 49 : 0.49295080860429624
Loss in iteration 50 : 0.48727339861028907
Loss in iteration 51 : 0.5005798942095512
Loss in iteration 52 : 0.4912464752568186
Loss in iteration 53 : 0.4924688386124589
Loss in iteration 54 : 0.4900592593537312
Loss in iteration 55 : 0.492336940173016
Loss in iteration 56 : 0.49637615401717233
Loss in iteration 57 : 0.48129514324938183
Loss in iteration 58 : 0.486126872270964
Loss in iteration 59 : 0.48546899980501484
Loss in iteration 60 : 0.4896195870739263
Loss in iteration 61 : 0.48673202480555516
Loss in iteration 62 : 0.48665159877500413
Loss in iteration 63 : 0.4830693026371697
Loss in iteration 64 : 0.4825589914390574
Loss in iteration 65 : 0.4919712999249368
Loss in iteration 66 : 0.47631018345391263
Loss in iteration 67 : 0.48361649175830335
Loss in iteration 68 : 0.474747485865275
Loss in iteration 69 : 0.48361665609825016
Loss in iteration 70 : 0.47546524981155464
Loss in iteration 71 : 0.4891455080226855
Loss in iteration 72 : 0.4756552381123454
Loss in iteration 73 : 0.48338960764086936
Loss in iteration 74 : 0.47782874483980436
Loss in iteration 75 : 0.4783990242372387
Loss in iteration 76 : 0.4874415989203108
Loss in iteration 77 : 0.4739865246273461
Loss in iteration 78 : 0.4827586492974848
Loss in iteration 79 : 0.4684417812232307
Loss in iteration 80 : 0.48146155920555966
Loss in iteration 81 : 0.4835985003112674
Loss in iteration 82 : 0.4788089142480756
Loss in iteration 83 : 0.48093053588500084
Loss in iteration 84 : 0.4878424699220271
Loss in iteration 85 : 0.4804128813592014
Loss in iteration 86 : 0.4866159158049261
Loss in iteration 87 : 0.46873989156492446
Loss in iteration 88 : 0.48647524255397956
Loss in iteration 89 : 0.4708188051224694
Loss in iteration 90 : 0.4735995738567778
Loss in iteration 91 : 0.4817061333517623
Loss in iteration 92 : 0.47073331770308924
Loss in iteration 93 : 0.4754218434010913
Loss in iteration 94 : 0.4760240168371214
Loss in iteration 95 : 0.47240808452366934
Loss in iteration 96 : 0.48390756721825623
Loss in iteration 97 : 0.47369295588604005
Loss in iteration 98 : 0.47847267407796723
Loss in iteration 99 : 0.4740044419798805
Loss in iteration 100 : 0.47855594639388005
Loss in iteration 101 : 0.4713098660002828
Loss in iteration 102 : 0.4705341236499081
Loss in iteration 103 : 0.4800213429107434
Loss in iteration 104 : 0.4753368125607011
Loss in iteration 105 : 0.4627778482156953
Loss in iteration 106 : 0.47316248323487825
Loss in iteration 107 : 0.483498759596246
Loss in iteration 108 : 0.4885644031339283
Loss in iteration 109 : 0.47462903128353495
Loss in iteration 110 : 0.46954965406554183
Loss in iteration 111 : 0.4777233484545269
Loss in iteration 112 : 0.4691958625735192
Loss in iteration 113 : 0.46665124831562105
Loss in iteration 114 : 0.46673028746504264
Loss in iteration 115 : 0.46496251906295955
Loss in iteration 116 : 0.47331245247029713
Loss in iteration 117 : 0.4818067643778619
Loss in iteration 118 : 0.47928772083648086
Loss in iteration 119 : 0.4648091434994533
Loss in iteration 120 : 0.48025037470975324
Loss in iteration 121 : 0.47691217902584115
Loss in iteration 122 : 0.481367735965563
Loss in iteration 123 : 0.4741901387035127
Loss in iteration 124 : 0.46807205870921853
Loss in iteration 125 : 0.4673000030957858
Loss in iteration 126 : 0.46684487270149233
Loss in iteration 127 : 0.46821598273130743
Loss in iteration 128 : 0.47379989538505707
Loss in iteration 129 : 0.46812690610954744
Loss in iteration 130 : 0.47312662683300244
Loss in iteration 131 : 0.46862498429419136
Loss in iteration 132 : 0.4680008080366724
Loss in iteration 133 : 0.4732805389930967
Loss in iteration 134 : 0.4724272751600096
Loss in iteration 135 : 0.4705490296816132
Loss in iteration 136 : 0.4705464901855749
Loss in iteration 137 : 0.4738582971424255
Loss in iteration 138 : 0.47392779653526423
Loss in iteration 139 : 0.479218855203018
Loss in iteration 140 : 0.4737296650602382
Loss in iteration 141 : 0.475052427226755
Loss in iteration 142 : 0.4665010519644842
Loss in iteration 143 : 0.4710611516469878
Loss in iteration 144 : 0.4666921999481787
Loss in iteration 145 : 0.460966684227835
Loss in iteration 146 : 0.47450456337660424
Loss in iteration 147 : 0.480167413006767
Loss in iteration 148 : 0.47011488932594714
Loss in iteration 149 : 0.48204846987998257
Loss in iteration 150 : 0.4750340806792731
Loss in iteration 151 : 0.4681208391894361
Loss in iteration 152 : 0.4783231963933518
Loss in iteration 153 : 0.4573478462354281
Loss in iteration 154 : 0.47691569144522405
Loss in iteration 155 : 0.47320391634539094
Loss in iteration 156 : 0.45737091599737567
Loss in iteration 157 : 0.47204856539138274
Loss in iteration 158 : 0.4649039678589073
Loss in iteration 159 : 0.4671661230709277
Loss in iteration 160 : 0.46626979116734946
Loss in iteration 161 : 0.4657220931138453
Loss in iteration 162 : 0.4679316888241097
Loss in iteration 163 : 0.47198332101676826
Loss in iteration 164 : 0.47122250529996135
Loss in iteration 165 : 0.46468935501292513
Loss in iteration 166 : 0.4672034092337732
Loss in iteration 167 : 0.4765334623923913
Loss in iteration 168 : 0.4769829688573311
Loss in iteration 169 : 0.4677646778712252
Loss in iteration 170 : 0.47365523207751825
Loss in iteration 171 : 0.46337532441438684
Loss in iteration 172 : 0.4711407056776263
Loss in iteration 173 : 0.4666788361567614
Loss in iteration 174 : 0.4622709216607529
Loss in iteration 175 : 0.47628597063741945
Loss in iteration 176 : 0.46307927897837814
Loss in iteration 177 : 0.47512850629400094
Loss in iteration 178 : 0.4642192722393277
Loss in iteration 179 : 0.4735496325362517
Loss in iteration 180 : 0.4623993167459163
Loss in iteration 181 : 0.4702631816997761
Loss in iteration 182 : 0.4665158354974034
Loss in iteration 183 : 0.46572855044352324
Loss in iteration 184 : 0.4805415151028998
Loss in iteration 185 : 0.4664673318703751
Loss in iteration 186 : 0.4584714765322232
Loss in iteration 187 : 0.4620805563905575
Loss in iteration 188 : 0.47413579263124184
Loss in iteration 189 : 0.47276670671927234
Loss in iteration 190 : 0.4583798655906045
Loss in iteration 191 : 0.46108042216909834
Loss in iteration 192 : 0.46495550290094567
Loss in iteration 193 : 0.4684954260430752
Loss in iteration 194 : 0.46806283718141356
Loss in iteration 195 : 0.4754870305624291
Loss in iteration 196 : 0.4812740752587036
Loss in iteration 197 : 0.4581029310896498
Loss in iteration 198 : 0.47549835537675145
Loss in iteration 199 : 0.4660256120279632
Loss in iteration 200 : 0.45894873908341766
Loss in iteration 201 : 0.4658596823516564
Loss in iteration 202 : 0.4744210251871624
Loss in iteration 203 : 0.45691494822915457
Loss in iteration 204 : 0.46265770997358396
Loss in iteration 205 : 0.4686180669561816
Loss in iteration 206 : 0.4680349970562637
Loss in iteration 207 : 0.46451422785588975
Loss in iteration 208 : 0.46017545287745065
Loss in iteration 209 : 0.4715843599271532
Loss in iteration 210 : 0.4754682784469696
Loss in iteration 211 : 0.46899468817183476
Loss in iteration 212 : 0.4727456663205065
Loss in iteration 213 : 0.4738979043054293
Loss in iteration 214 : 0.48639145159153024
Loss in iteration 215 : 0.4751295682276623
Loss in iteration 216 : 0.46646184037873106
Loss in iteration 217 : 0.4644107174882584
Loss in iteration 218 : 0.4611959859926356
Loss in iteration 219 : 0.4729222833672252
Loss in iteration 220 : 0.4718714428696975
Loss in iteration 221 : 0.4701062376087638
Loss in iteration 222 : 0.46089124779791674
Loss in iteration 223 : 0.4723504160829768
Loss in iteration 224 : 0.47244684950062493
Loss in iteration 225 : 0.4649028159424281
Loss in iteration 226 : 0.46815692564074174
Loss in iteration 227 : 0.4589457668201281
Loss in iteration 228 : 0.4693150299667987
Loss in iteration 229 : 0.4692269387827324
Loss in iteration 230 : 0.4602658086598027
Loss in iteration 231 : 0.4684904578035464
Loss in iteration 232 : 0.4699702845601234
Loss in iteration 233 : 0.46428351375027943
Loss in iteration 234 : 0.47087720246396775
Loss in iteration 235 : 0.46107219370084224
Loss in iteration 236 : 0.4667883187621682
Loss in iteration 237 : 0.46757717670434884
Loss in iteration 238 : 0.46680314669522427
Loss in iteration 239 : 0.4683061397849195
Loss in iteration 240 : 0.460350613767448
Loss in iteration 241 : 0.46092484804305656
Loss in iteration 242 : 0.48681272296052747
Loss in iteration 243 : 0.4744696532553325
Loss in iteration 244 : 0.46475306718238957
Loss in iteration 245 : 0.47747537005810536
Loss in iteration 246 : 0.4633851790289888
Loss in iteration 247 : 0.45839412526347084
Loss in iteration 248 : 0.4668256016757706
Loss in iteration 249 : 0.4699399629048333
Loss in iteration 250 : 0.45948117731910276
Loss in iteration 251 : 0.47069641188483463
Loss in iteration 252 : 0.46437495627381464
Loss in iteration 253 : 0.47456349980006796
Loss in iteration 254 : 0.4519728756458439
Loss in iteration 255 : 0.4575919322517064
Loss in iteration 256 : 0.4792412080457777
Loss in iteration 257 : 0.4562296375144713
Loss in iteration 258 : 0.46160239087008376
Loss in iteration 259 : 0.46064362099038403
Loss in iteration 260 : 0.46384413414545345
Loss in iteration 261 : 0.45304854525543525
Loss in iteration 262 : 0.4644527005391848
Loss in iteration 263 : 0.4649459967811655
Loss in iteration 264 : 0.47794827941823465
Loss in iteration 265 : 0.4556172325597633
Loss in iteration 266 : 0.47393989763458105
Loss in iteration 267 : 0.4714340591732072
Loss in iteration 268 : 0.4758432359540935
Loss in iteration 269 : 0.4594611956886364
Loss in iteration 270 : 0.4643091396834866
Loss in iteration 271 : 0.45298626344825527
Loss in iteration 272 : 0.4651683832862215
Loss in iteration 273 : 0.4507025746562056
Loss in iteration 274 : 0.4595512505812346
Loss in iteration 275 : 0.4683698976278449
Loss in iteration 276 : 0.47140212594201125
Loss in iteration 277 : 0.45901499881608604
Loss in iteration 278 : 0.45763718421684024
Loss in iteration 279 : 0.45539455521736044
Loss in iteration 280 : 0.46067385440079067
Loss in iteration 281 : 0.4673250938835876
Loss in iteration 282 : 0.4608558708943268
Loss in iteration 283 : 0.467298597993898
Loss in iteration 284 : 0.4526485129336337
Loss in iteration 285 : 0.4550584949243438
Loss in iteration 286 : 0.4644645752491358
Loss in iteration 287 : 0.4683081624461306
Loss in iteration 288 : 0.4672088770812776
Loss in iteration 289 : 0.4593784750537639
Loss in iteration 290 : 0.4568790481722699
Loss in iteration 291 : 0.46990538879522764
Loss in iteration 292 : 0.4656003341737326
Loss in iteration 293 : 0.45957708113863116
Loss in iteration 294 : 0.46962111135714657
Loss in iteration 295 : 0.4564057898995418
Loss in iteration 296 : 0.4591345799165326
Loss in iteration 297 : 0.4685425254443226
Loss in iteration 298 : 0.46454519286819757
Loss in iteration 299 : 0.4660764563299667
Loss in iteration 300 : 0.4692716571917473
Loss in iteration 301 : 0.46807757160044344
Loss in iteration 302 : 0.462738453256138
Loss in iteration 303 : 0.456801094546023
Loss in iteration 304 : 0.4591288679104698
Loss in iteration 305 : 0.46065028180410955
Loss in iteration 306 : 0.4619396736926202
Loss in iteration 307 : 0.463167613314652
Loss in iteration 308 : 0.46454347101086013
Loss in iteration 309 : 0.4554360578061302
Loss in iteration 310 : 0.4616661727991843
Loss in iteration 311 : 0.4709083199063869
Loss in iteration 312 : 0.480606694033454
Loss in iteration 313 : 0.45184268954275997
Loss in iteration 314 : 0.4584942478525126
Loss in iteration 315 : 0.4613619348554767
Loss in iteration 316 : 0.47815503661879927
Loss in iteration 317 : 0.46222356189756497
Loss in iteration 318 : 0.47412887054431657
Loss in iteration 319 : 0.45845744077120687
Loss in iteration 320 : 0.45747703313940796
Loss in iteration 321 : 0.45902512680086294
Loss in iteration 322 : 0.46265639100055217
Loss in iteration 323 : 0.4602851363509665
Loss in iteration 324 : 0.4615883187942339
Loss in iteration 325 : 0.4603128861316554
Loss in iteration 326 : 0.4757675383208392
Loss in iteration 327 : 0.45483653051418876
Loss in iteration 328 : 0.4571398831879184
Loss in iteration 329 : 0.45864158993163995
Loss in iteration 330 : 0.45472603711810466
Loss in iteration 331 : 0.4631332156640649
Loss in iteration 332 : 0.4501314972225184
Loss in iteration 333 : 0.451217857344458
Loss in iteration 334 : 0.46159231475690105
Loss in iteration 335 : 0.4551404966590323
Loss in iteration 336 : 0.461799070856887
Loss in iteration 337 : 0.4660554257477538
Loss in iteration 338 : 0.4714285384324006
Loss in iteration 339 : 0.47366626532744976
Loss in iteration 340 : 0.46772079504185426
Loss in iteration 341 : 0.46065672431141635
Loss in iteration 342 : 0.4619267344157467
Loss in iteration 343 : 0.4612866946743966
Loss in iteration 344 : 0.4676326312821032
Loss in iteration 345 : 0.46140970126689046
Loss in iteration 346 : 0.45611556626659416
Loss in iteration 347 : 0.4630735629801077
Loss in iteration 348 : 0.4644479894837598
Loss in iteration 349 : 0.46992846116625053
Loss in iteration 350 : 0.4697568177497384
Loss in iteration 351 : 0.4644978740917533
Loss in iteration 352 : 0.4603352019652543
Loss in iteration 353 : 0.45547274749928807
Loss in iteration 354 : 0.4621743789747924
Loss in iteration 355 : 0.46242838775620676
Loss in iteration 356 : 0.45592563702330446
Loss in iteration 357 : 0.4601642869371914
Loss in iteration 358 : 0.46810060947796195
Loss in iteration 359 : 0.4609688347329243
Loss in iteration 360 : 0.46735494962323726
Loss in iteration 361 : 0.45329494812185184
Loss in iteration 362 : 0.47218779560068125
Loss in iteration 363 : 0.46331739757345086
Loss in iteration 364 : 0.46197122803508084
Loss in iteration 365 : 0.4650052564831561
Loss in iteration 366 : 0.4598687155932307
Loss in iteration 367 : 0.4684560407754988
Loss in iteration 368 : 0.46965383390478743
Loss in iteration 369 : 0.4749265126157521
Loss in iteration 370 : 0.45903406179763684
Loss in iteration 371 : 0.47644393404283075
Loss in iteration 372 : 0.4561573789427539
Loss in iteration 373 : 0.46289819520188896
Loss in iteration 374 : 0.46163889030021377
Loss in iteration 375 : 0.45984342806085576
Loss in iteration 376 : 0.4687668427630965
Loss in iteration 377 : 0.46290526366913093
Loss in iteration 378 : 0.4658634678636648
Loss in iteration 379 : 0.46519127501432644
Loss in iteration 380 : 0.4599890548935812
Loss in iteration 381 : 0.4513232927272387
Loss in iteration 382 : 0.45749442619617414
Loss in iteration 383 : 0.46459442630833825
Loss in iteration 384 : 0.46999124720280544
Loss in iteration 385 : 0.4632727543067686
Loss in iteration 386 : 0.4512891113363447
Loss in iteration 387 : 0.4676268428745875
Loss in iteration 388 : 0.47141820974299237
Loss in iteration 389 : 0.4583744523008945
Loss in iteration 390 : 0.46124621031830004
Loss in iteration 391 : 0.46397451446965415
Loss in iteration 392 : 0.45290251370697165
Loss in iteration 393 : 0.4555606257718539
Loss in iteration 394 : 0.46002896204494625
Loss in iteration 395 : 0.4676664538105881
Loss in iteration 396 : 0.46149395847660524
Loss in iteration 397 : 0.4736796333370232
Loss in iteration 398 : 0.4707906732020172
Loss in iteration 399 : 0.45984271056325476
Loss in iteration 400 : 0.4548668038656144
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.788875, training accuracy 0.788875, time elapsed: 7903 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6796051606376952
Loss in iteration 3 : 0.6702160493372893
Loss in iteration 4 : 0.6763205627377816
Loss in iteration 5 : 0.6667451928819387
Loss in iteration 6 : 0.671965250594574
Loss in iteration 7 : 0.6584518081106561
Loss in iteration 8 : 0.6513085397566759
Loss in iteration 9 : 0.6244186613511286
Loss in iteration 10 : 0.6200736777264346
Loss in iteration 11 : 0.6052104697404895
Loss in iteration 12 : 0.5972180558510144
Loss in iteration 13 : 0.5818510339608459
Loss in iteration 14 : 0.5738681303168423
Loss in iteration 15 : 0.5715973109211241
Loss in iteration 16 : 0.5635962759235824
Loss in iteration 17 : 0.5584804632862461
Loss in iteration 18 : 0.5545080990710936
Loss in iteration 19 : 0.5532099484888992
Loss in iteration 20 : 0.5470606853850255
Loss in iteration 21 : 0.5513600504436342
Loss in iteration 22 : 0.5473455464748284
Loss in iteration 23 : 0.5447747326315036
Loss in iteration 24 : 0.535550965903112
Loss in iteration 25 : 0.5381529208358293
Loss in iteration 26 : 0.5295656434292563
Loss in iteration 27 : 0.5294899210109245
Loss in iteration 28 : 0.5299577585009777
Loss in iteration 29 : 0.5338830133357996
Loss in iteration 30 : 0.5327061014383931
Loss in iteration 31 : 0.5280174961641887
Loss in iteration 32 : 0.5268582105698588
Loss in iteration 33 : 0.5212553433580264
Loss in iteration 34 : 0.5200247434707599
Loss in iteration 35 : 0.5244896413707623
Loss in iteration 36 : 0.5219024253362581
Loss in iteration 37 : 0.5177348836023623
Loss in iteration 38 : 0.515963136952933
Loss in iteration 39 : 0.5135082954025125
Loss in iteration 40 : 0.5148426279508537
Loss in iteration 41 : 0.5161103969784681
Loss in iteration 42 : 0.5079547728022623
Loss in iteration 43 : 0.5139466867130436
Loss in iteration 44 : 0.512631443092584
Loss in iteration 45 : 0.5117935373321221
Loss in iteration 46 : 0.5107940279801425
Loss in iteration 47 : 0.5082832442950428
Loss in iteration 48 : 0.511045980368091
Loss in iteration 49 : 0.5048566702648589
Loss in iteration 50 : 0.5006401206842827
Loss in iteration 51 : 0.5098673992053804
Loss in iteration 52 : 0.5016972945496759
Loss in iteration 53 : 0.5018508935201278
Loss in iteration 54 : 0.49845462639021876
Loss in iteration 55 : 0.5006997979882002
Loss in iteration 56 : 0.5038477258229422
Loss in iteration 57 : 0.49147820953617466
Loss in iteration 58 : 0.49456399491826786
Loss in iteration 59 : 0.4948270174147219
Loss in iteration 60 : 0.4980710496204105
Loss in iteration 61 : 0.4980684312309906
Loss in iteration 62 : 0.49748813364106764
Loss in iteration 63 : 0.4948772186472045
Loss in iteration 64 : 0.49459624540837654
Loss in iteration 65 : 0.5019321258478819
Loss in iteration 66 : 0.4895003660370469
Loss in iteration 67 : 0.4948746171261793
Loss in iteration 68 : 0.4886573262739291
Loss in iteration 69 : 0.4967750336511338
Loss in iteration 70 : 0.4898354909343028
Loss in iteration 71 : 0.5015886550384409
Loss in iteration 72 : 0.4894158936423456
Loss in iteration 73 : 0.49486973308890064
Loss in iteration 74 : 0.4908394453533452
Loss in iteration 75 : 0.4907959317359838
Loss in iteration 76 : 0.497948288706517
Loss in iteration 77 : 0.4869968817754557
Loss in iteration 78 : 0.49316348091216555
Loss in iteration 79 : 0.47881980082817804
Loss in iteration 80 : 0.4893362168466858
Loss in iteration 81 : 0.4920621439215798
Loss in iteration 82 : 0.487718816683837
Loss in iteration 83 : 0.49116903653784266
Loss in iteration 84 : 0.495730101737573
Loss in iteration 85 : 0.4895335496871134
Loss in iteration 86 : 0.49482352330976437
Loss in iteration 87 : 0.4799056521202977
Loss in iteration 88 : 0.4961388384688389
Loss in iteration 89 : 0.48198099360552404
Loss in iteration 90 : 0.48487988766641194
Loss in iteration 91 : 0.4914579796132924
Loss in iteration 92 : 0.48133375814840873
Loss in iteration 93 : 0.4846910237164775
Loss in iteration 94 : 0.4858786283184966
Loss in iteration 95 : 0.4814387446356546
Loss in iteration 96 : 0.49273443105390324
Loss in iteration 97 : 0.4836087459434595
Loss in iteration 98 : 0.48721689935752593
Loss in iteration 99 : 0.4834972917259763
Loss in iteration 100 : 0.487926949318612
Loss in iteration 101 : 0.48065139267493684
Loss in iteration 102 : 0.4802668964513078
Loss in iteration 103 : 0.4884658876205318
Loss in iteration 104 : 0.4839557453670393
Loss in iteration 105 : 0.4732506518089217
Loss in iteration 106 : 0.48179052597020766
Loss in iteration 107 : 0.4903446260792573
Loss in iteration 108 : 0.4953334443940083
Loss in iteration 109 : 0.4819953328545324
Loss in iteration 110 : 0.47700626769583343
Loss in iteration 111 : 0.4848456082805371
Loss in iteration 112 : 0.4792997362861026
Loss in iteration 113 : 0.47592144624022537
Loss in iteration 114 : 0.4752179875126594
Loss in iteration 115 : 0.4739626037440292
Loss in iteration 116 : 0.480659892218256
Loss in iteration 117 : 0.4884628848734309
Loss in iteration 118 : 0.48656623748143835
Loss in iteration 119 : 0.4733640928279821
Loss in iteration 120 : 0.48713249204765474
Loss in iteration 121 : 0.48468846487788647
Loss in iteration 122 : 0.48764876185945555
Loss in iteration 123 : 0.4812716336131308
Loss in iteration 124 : 0.4755194494495301
Loss in iteration 125 : 0.47538642528413616
Loss in iteration 126 : 0.4742718483946554
Loss in iteration 127 : 0.4760352018286609
Loss in iteration 128 : 0.4805164923995325
Loss in iteration 129 : 0.47541253045704573
Loss in iteration 130 : 0.47988244587580897
Loss in iteration 131 : 0.4756651812109106
Loss in iteration 132 : 0.47545939899318884
Loss in iteration 133 : 0.47958397855417473
Loss in iteration 134 : 0.47964017447151897
Loss in iteration 135 : 0.4772715484939268
Loss in iteration 136 : 0.4773587424695387
Loss in iteration 137 : 0.4795990653770423
Loss in iteration 138 : 0.4804946371379425
Loss in iteration 139 : 0.48485175221455706
Loss in iteration 140 : 0.4799714152110104
Loss in iteration 141 : 0.48135526640497744
Loss in iteration 142 : 0.4737579331040673
Loss in iteration 143 : 0.4770829602074605
Loss in iteration 144 : 0.4737053245690725
Loss in iteration 145 : 0.4683977725521739
Loss in iteration 146 : 0.480344074580646
Loss in iteration 147 : 0.4849275163320045
Loss in iteration 148 : 0.47559323266438014
Loss in iteration 149 : 0.48644540396822505
Loss in iteration 150 : 0.4804272652458138
Loss in iteration 151 : 0.4741106125872544
Loss in iteration 152 : 0.4833959598877153
Loss in iteration 153 : 0.4642264384223113
Loss in iteration 154 : 0.48234046086269244
Loss in iteration 155 : 0.4790061228684098
Loss in iteration 156 : 0.4644903505581038
Loss in iteration 157 : 0.47779257107363343
Loss in iteration 158 : 0.46982958588546303
Loss in iteration 159 : 0.47239501257654
Loss in iteration 160 : 0.47118176306865794
Loss in iteration 161 : 0.4719607680688389
Loss in iteration 162 : 0.47381418269853853
Loss in iteration 163 : 0.477548500631643
Loss in iteration 164 : 0.4766928645071352
Loss in iteration 165 : 0.4703761180968311
Loss in iteration 166 : 0.4724363904014292
Loss in iteration 167 : 0.48228246198857644
Loss in iteration 168 : 0.48143404694157665
Loss in iteration 169 : 0.47352746804940177
Loss in iteration 170 : 0.4787050868565697
Loss in iteration 171 : 0.46860822026249815
Loss in iteration 172 : 0.4754530540666946
Loss in iteration 173 : 0.4722423882654212
Loss in iteration 174 : 0.46767593490879583
Loss in iteration 175 : 0.481287033692281
Loss in iteration 176 : 0.4688002084024224
Loss in iteration 177 : 0.4798596856160533
Loss in iteration 178 : 0.46961600565114336
Loss in iteration 179 : 0.4783857752326975
Loss in iteration 180 : 0.46789024885445435
Loss in iteration 181 : 0.47474428470262003
Loss in iteration 182 : 0.47159554638505247
Loss in iteration 183 : 0.4709397752108537
Loss in iteration 184 : 0.4846333436908874
Loss in iteration 185 : 0.4714831416249653
Loss in iteration 186 : 0.46436581067338345
Loss in iteration 187 : 0.4668756851179937
Loss in iteration 188 : 0.4774974469636435
Loss in iteration 189 : 0.4769995598335629
Loss in iteration 190 : 0.46407619403888173
Loss in iteration 191 : 0.4665026807562379
Loss in iteration 192 : 0.46979528982277446
Loss in iteration 193 : 0.47276108651443777
Loss in iteration 194 : 0.47264114799433127
Loss in iteration 195 : 0.47950972975691913
Loss in iteration 196 : 0.48499756282020834
Loss in iteration 197 : 0.4629016772902738
Loss in iteration 198 : 0.48015605142999795
Loss in iteration 199 : 0.4707850965801246
Loss in iteration 200 : 0.4642084438546066
Loss in iteration 201 : 0.47120584170428964
Loss in iteration 202 : 0.4781378602137674
Loss in iteration 203 : 0.462038025930385
Loss in iteration 204 : 0.4666873309028033
Loss in iteration 205 : 0.4721035724107303
Loss in iteration 206 : 0.47036817150096755
Loss in iteration 207 : 0.4675091929088584
Loss in iteration 208 : 0.4628531066284826
Loss in iteration 209 : 0.47471219454662794
Loss in iteration 210 : 0.4792116414610546
Loss in iteration 211 : 0.47330683178196703
Loss in iteration 212 : 0.47706002388551444
Loss in iteration 213 : 0.47774029912755217
Loss in iteration 214 : 0.489329633970059
Loss in iteration 215 : 0.4788207594100559
Loss in iteration 216 : 0.4715337979935267
Loss in iteration 217 : 0.46891580826110085
Loss in iteration 218 : 0.46519599690189983
Loss in iteration 219 : 0.47580529966911655
Loss in iteration 220 : 0.4748515996783603
Loss in iteration 221 : 0.4735844387421555
Loss in iteration 222 : 0.463714359751435
Loss in iteration 223 : 0.47638445047567823
Loss in iteration 224 : 0.475984599872533
Loss in iteration 225 : 0.46906499674246294
Loss in iteration 226 : 0.4720039887102671
Loss in iteration 227 : 0.4626993425860897
Loss in iteration 228 : 0.47191748200497025
Loss in iteration 229 : 0.4726059186163653
Loss in iteration 230 : 0.463932762491858
Loss in iteration 231 : 0.4721310711155676
Loss in iteration 232 : 0.47312471147401075
Loss in iteration 233 : 0.468486499455465
Loss in iteration 234 : 0.4745153641286611
Loss in iteration 235 : 0.46466768955092824
Loss in iteration 236 : 0.4701326357911204
Loss in iteration 237 : 0.47103846814670597
Loss in iteration 238 : 0.4695496205393629
Loss in iteration 239 : 0.46932390931917595
Loss in iteration 240 : 0.46354613015155205
Loss in iteration 241 : 0.464897765094426
Loss in iteration 242 : 0.48980337462253226
Loss in iteration 243 : 0.4778637655434522
Loss in iteration 244 : 0.46853328506980324
Loss in iteration 245 : 0.48061146444319713
Loss in iteration 246 : 0.46738119147883483
Loss in iteration 247 : 0.46237996504105355
Loss in iteration 248 : 0.4701700966688322
Loss in iteration 249 : 0.4733867729657287
Loss in iteration 250 : 0.463326838710657
Loss in iteration 251 : 0.47403368580139865
Loss in iteration 252 : 0.4674926840989809
Loss in iteration 253 : 0.4774451062040176
Loss in iteration 254 : 0.45602269842928117
Loss in iteration 255 : 0.4617228101146389
Loss in iteration 256 : 0.48181351708019643
Loss in iteration 257 : 0.4604545369988518
Loss in iteration 258 : 0.46489549169105476
Loss in iteration 259 : 0.4637755309386596
Loss in iteration 260 : 0.4675512376741739
Loss in iteration 261 : 0.45708213483419285
Loss in iteration 262 : 0.4670374545163742
Loss in iteration 263 : 0.46828152073956486
Loss in iteration 264 : 0.4809755623807325
Loss in iteration 265 : 0.45968787709954656
Loss in iteration 266 : 0.4768538537986429
Loss in iteration 267 : 0.47404763695881447
Loss in iteration 268 : 0.4779952803965815
Loss in iteration 269 : 0.4630904348332394
Loss in iteration 270 : 0.4680544901119404
Loss in iteration 271 : 0.4570954833067339
Loss in iteration 272 : 0.46853842719908917
Loss in iteration 273 : 0.45416005364623274
Loss in iteration 274 : 0.46259530992447584
Loss in iteration 275 : 0.47082766123802117
Loss in iteration 276 : 0.47357094298282554
Loss in iteration 277 : 0.4617628459705863
Loss in iteration 278 : 0.4602883390952165
Loss in iteration 279 : 0.4578131727524125
Loss in iteration 280 : 0.4629942829530214
Loss in iteration 281 : 0.4708758765270692
Loss in iteration 282 : 0.4640444062469477
Loss in iteration 283 : 0.4704119696583779
Loss in iteration 284 : 0.4556197393997796
Loss in iteration 285 : 0.45824109935955726
Loss in iteration 286 : 0.4668847856745075
Loss in iteration 287 : 0.4704261727792033
Loss in iteration 288 : 0.46984458000798374
Loss in iteration 289 : 0.46270473407321694
Loss in iteration 290 : 0.45951516133274817
Loss in iteration 291 : 0.4710679523862133
Loss in iteration 292 : 0.46845851877677075
Loss in iteration 293 : 0.46273836810543095
Loss in iteration 294 : 0.4723563871771363
Loss in iteration 295 : 0.459500793228489
Loss in iteration 296 : 0.46206443914308
Loss in iteration 297 : 0.4703840227072249
Loss in iteration 298 : 0.46740093768376406
Loss in iteration 299 : 0.46840822299700335
Loss in iteration 300 : 0.4717002408402816
Loss in iteration 301 : 0.47018620484525203
Loss in iteration 302 : 0.4652691934880709
Loss in iteration 303 : 0.45932161470870614
Loss in iteration 304 : 0.4620320748096491
Loss in iteration 305 : 0.4630378364286239
Loss in iteration 306 : 0.46442094319890165
Loss in iteration 307 : 0.4660261864357805
Loss in iteration 308 : 0.4677741756789545
Loss in iteration 309 : 0.4587949329465966
Loss in iteration 310 : 0.4643792612802218
Loss in iteration 311 : 0.47326634246003085
Loss in iteration 312 : 0.48210999538534327
Loss in iteration 313 : 0.4546836602490541
Loss in iteration 314 : 0.46115138059679195
Loss in iteration 315 : 0.46354162972823004
Loss in iteration 316 : 0.47990551645501855
Loss in iteration 317 : 0.46409469542732534
Loss in iteration 318 : 0.4763908693116418
Loss in iteration 319 : 0.461172093260787
Loss in iteration 320 : 0.46002274760581113
Loss in iteration 321 : 0.4615497943330376
Loss in iteration 322 : 0.46396586789382555
Loss in iteration 323 : 0.46245512547612794
Loss in iteration 324 : 0.46403499205916177
Loss in iteration 325 : 0.4630515590500024
Loss in iteration 326 : 0.4777210436691294
Loss in iteration 327 : 0.4575065906605216
Loss in iteration 328 : 0.4601815652727728
Loss in iteration 329 : 0.4613019551649942
Loss in iteration 330 : 0.45707911654882505
Loss in iteration 331 : 0.46521850564465705
Loss in iteration 332 : 0.4527728458881214
Loss in iteration 333 : 0.45424707512054796
Loss in iteration 334 : 0.4645521232702912
Loss in iteration 335 : 0.4575072167811684
Loss in iteration 336 : 0.4643107335769187
Loss in iteration 337 : 0.4678195598946274
Loss in iteration 338 : 0.47309858902340124
Loss in iteration 339 : 0.4755281596448588
Loss in iteration 340 : 0.4701959255354822
Loss in iteration 341 : 0.4628998967856671
Loss in iteration 342 : 0.46423755385114024
Loss in iteration 343 : 0.46372738741389286
Loss in iteration 344 : 0.4698340625529476
Loss in iteration 345 : 0.4641883671189261
Loss in iteration 346 : 0.45901555244022635
Loss in iteration 347 : 0.46576643819312036
Loss in iteration 348 : 0.4666783773185503
Loss in iteration 349 : 0.4716648883089068
Loss in iteration 350 : 0.4704814979161746
Loss in iteration 351 : 0.46573493850876274
Loss in iteration 352 : 0.4625731923566483
Loss in iteration 353 : 0.45821898479910633
Loss in iteration 354 : 0.4645270153282619
Loss in iteration 355 : 0.46462618092416635
Loss in iteration 356 : 0.4580483328385599
Loss in iteration 357 : 0.4625654125578589
Loss in iteration 358 : 0.4697115344944996
Loss in iteration 359 : 0.4631489529367587
Loss in iteration 360 : 0.46942005794039726
Loss in iteration 361 : 0.45604608513553135
Loss in iteration 362 : 0.47404696029777765
Loss in iteration 363 : 0.4653238893627804
Loss in iteration 364 : 0.4645962465757828
Loss in iteration 365 : 0.46716331754711377
Loss in iteration 366 : 0.4622057893190776
Loss in iteration 367 : 0.4700036038521142
Loss in iteration 368 : 0.4716557234551169
Loss in iteration 369 : 0.47656978246249077
Loss in iteration 370 : 0.4613956314409612
Loss in iteration 371 : 0.478563469115585
Loss in iteration 372 : 0.45804175666980707
Loss in iteration 373 : 0.46500067025832126
Loss in iteration 374 : 0.4634683662677504
Loss in iteration 375 : 0.46162860254629984
Loss in iteration 376 : 0.4708535113712225
Loss in iteration 377 : 0.4649382282554499
Loss in iteration 378 : 0.46802626832588756
Loss in iteration 379 : 0.4669537287352252
Loss in iteration 380 : 0.46199283110906575
Loss in iteration 381 : 0.4540003573158001
Loss in iteration 382 : 0.4597853524278071
Loss in iteration 383 : 0.4663184547195834
Loss in iteration 384 : 0.47142869247542657
Loss in iteration 385 : 0.4654041004045989
Loss in iteration 386 : 0.4538249774699937
Loss in iteration 387 : 0.4690365939802096
Loss in iteration 388 : 0.4732280638528577
Loss in iteration 389 : 0.45990421651834323
Loss in iteration 390 : 0.4627473795109937
Loss in iteration 391 : 0.4654959511501992
Loss in iteration 392 : 0.4550575572840379
Loss in iteration 393 : 0.45734628208139705
Loss in iteration 394 : 0.4621498654008322
Loss in iteration 395 : 0.4694834625797441
Loss in iteration 396 : 0.4635129320693785
Loss in iteration 397 : 0.4753070016909925
Loss in iteration 398 : 0.47254338648241356
Loss in iteration 399 : 0.46091535003832
Loss in iteration 400 : 0.45638052671677637
Testing accuracy  of updater 0 on alg 0 with rate 0.7 = 0.789375, training accuracy 0.789375, time elapsed: 6759 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.679809075633425
Loss in iteration 3 : 0.6666093530904296
Loss in iteration 4 : 0.6606421511251391
Loss in iteration 5 : 0.6499252123264239
Loss in iteration 6 : 0.6457207444124605
Loss in iteration 7 : 0.6380796832690094
Loss in iteration 8 : 0.6314264743275076
Loss in iteration 9 : 0.6264121705005457
Loss in iteration 10 : 0.6220005495154457
Loss in iteration 11 : 0.6135192644875499
Loss in iteration 12 : 0.6100546522435355
Loss in iteration 13 : 0.6055846477325131
Loss in iteration 14 : 0.6001049657876443
Loss in iteration 15 : 0.5992265090338617
Loss in iteration 16 : 0.5929519266061024
Loss in iteration 17 : 0.5884093469067645
Loss in iteration 18 : 0.5854485591338064
Loss in iteration 19 : 0.5850083446690244
Loss in iteration 20 : 0.5799977282731705
Loss in iteration 21 : 0.5809752511451796
Loss in iteration 22 : 0.5780171472689858
Loss in iteration 23 : 0.5761483493196806
Loss in iteration 24 : 0.5680847692707675
Loss in iteration 25 : 0.5698089481020544
Loss in iteration 26 : 0.5622369611056214
Loss in iteration 27 : 0.561325770221663
Loss in iteration 28 : 0.5612534657714825
Loss in iteration 29 : 0.5633451608011854
Loss in iteration 30 : 0.5623609230020175
Loss in iteration 31 : 0.5585115296452398
Loss in iteration 32 : 0.5569233762981158
Loss in iteration 33 : 0.5520324546252939
Loss in iteration 34 : 0.5500575857694503
Loss in iteration 35 : 0.5527148726187057
Loss in iteration 36 : 0.5493652424652213
Loss in iteration 37 : 0.5459483113300054
Loss in iteration 38 : 0.5438427285153542
Loss in iteration 39 : 0.5417567348571392
Loss in iteration 40 : 0.5421582266686981
Loss in iteration 41 : 0.5423300111962283
Loss in iteration 42 : 0.5358603347986279
Loss in iteration 43 : 0.5400684193953699
Loss in iteration 44 : 0.5382395084903191
Loss in iteration 45 : 0.537352868073642
Loss in iteration 46 : 0.5362747151994912
Loss in iteration 47 : 0.5337904403088831
Loss in iteration 48 : 0.5359391541009915
Loss in iteration 49 : 0.5302220978909223
Loss in iteration 50 : 0.5270180350061154
Loss in iteration 51 : 0.5331917923869971
Loss in iteration 52 : 0.526310447976357
Loss in iteration 53 : 0.5261801556220407
Loss in iteration 54 : 0.522199693882095
Loss in iteration 55 : 0.5237215852758947
Loss in iteration 56 : 0.5266576521072951
Loss in iteration 57 : 0.5162167567404751
Loss in iteration 58 : 0.5182913912637905
Loss in iteration 59 : 0.5184749170324653
Loss in iteration 60 : 0.5204837609246254
Loss in iteration 61 : 0.5209763668824481
Loss in iteration 62 : 0.519373734064502
Loss in iteration 63 : 0.5169992866352611
Loss in iteration 64 : 0.5165809630901517
Loss in iteration 65 : 0.5218513453775078
Loss in iteration 66 : 0.5118470711264896
Loss in iteration 67 : 0.5148863506588693
Loss in iteration 68 : 0.5102160943200944
Loss in iteration 69 : 0.5170445342141997
Loss in iteration 70 : 0.5111055790447906
Loss in iteration 71 : 0.5211641818174539
Loss in iteration 72 : 0.5105345431577419
Loss in iteration 73 : 0.5138189238129097
Loss in iteration 74 : 0.511170282355539
Loss in iteration 75 : 0.5105545815095978
Loss in iteration 76 : 0.515948002866992
Loss in iteration 77 : 0.5074369890468986
Loss in iteration 78 : 0.51180140414924
Loss in iteration 79 : 0.4985237122187447
Loss in iteration 80 : 0.5073600939504739
Loss in iteration 81 : 0.5105211721757744
Loss in iteration 82 : 0.5060439297361203
Loss in iteration 83 : 0.5098594195699299
Loss in iteration 84 : 0.5120260197948852
Loss in iteration 85 : 0.5075377299697653
Loss in iteration 86 : 0.5117054177771538
Loss in iteration 87 : 0.4991028114326829
Loss in iteration 88 : 0.5127172249429455
Loss in iteration 89 : 0.5001833891321621
Loss in iteration 90 : 0.5032925603508472
Loss in iteration 91 : 0.5082573746405715
Loss in iteration 92 : 0.49914092926357984
Loss in iteration 93 : 0.5012587331141277
Loss in iteration 94 : 0.5032695535474149
Loss in iteration 95 : 0.4984602995303576
Loss in iteration 96 : 0.5090116600860701
Loss in iteration 97 : 0.5003084416546467
Loss in iteration 98 : 0.5025066314299663
Loss in iteration 99 : 0.5000816316081295
Loss in iteration 100 : 0.5044133792656031
Loss in iteration 101 : 0.49705189237692726
Loss in iteration 102 : 0.4970934449423637
Loss in iteration 103 : 0.5037838351741386
Loss in iteration 104 : 0.49964306692598276
Loss in iteration 105 : 0.49135069817929283
Loss in iteration 106 : 0.497474587762971
Loss in iteration 107 : 0.5042601332537879
Loss in iteration 108 : 0.5092152311818636
Loss in iteration 109 : 0.4973904519811563
Loss in iteration 110 : 0.49303617883624606
Loss in iteration 111 : 0.4995336825002647
Loss in iteration 112 : 0.4947253551222656
Loss in iteration 113 : 0.49151467021206985
Loss in iteration 114 : 0.4910459025521092
Loss in iteration 115 : 0.48965744331742234
Loss in iteration 116 : 0.4947534034389547
Loss in iteration 117 : 0.5015931284625165
Loss in iteration 118 : 0.5001432700461501
Loss in iteration 119 : 0.4887684959955677
Loss in iteration 120 : 0.5004534211917064
Loss in iteration 121 : 0.49900634570626573
Loss in iteration 122 : 0.5003024901227209
Loss in iteration 123 : 0.49443895164209317
Loss in iteration 124 : 0.4898474708385208
Loss in iteration 125 : 0.49038904538854206
Loss in iteration 126 : 0.48848853131524217
Loss in iteration 127 : 0.490329456681751
Loss in iteration 128 : 0.49358562059997557
Loss in iteration 129 : 0.489344830817552
Loss in iteration 130 : 0.4931028596720741
Loss in iteration 131 : 0.48906049015827624
Loss in iteration 132 : 0.48951766810581493
Loss in iteration 133 : 0.4921225008793396
Loss in iteration 134 : 0.49294803333456194
Loss in iteration 135 : 0.49032227301109393
Loss in iteration 136 : 0.4904637570818262
Loss in iteration 137 : 0.49132138931333563
Loss in iteration 138 : 0.4932335755153096
Loss in iteration 139 : 0.4962297821594388
Loss in iteration 140 : 0.4923692797102132
Loss in iteration 141 : 0.4937125839814126
Loss in iteration 142 : 0.48740773807322313
Loss in iteration 143 : 0.4891826181570528
Loss in iteration 144 : 0.4869727606118253
Loss in iteration 145 : 0.48219789502992977
Loss in iteration 146 : 0.49159495448267077
Loss in iteration 147 : 0.49568747453374756
Loss in iteration 148 : 0.487903956931069
Loss in iteration 149 : 0.496967435342881
Loss in iteration 150 : 0.4922411249344624
Loss in iteration 151 : 0.48666202880419857
Loss in iteration 152 : 0.4940714666025223
Loss in iteration 153 : 0.4773209278313936
Loss in iteration 154 : 0.4931188006440283
Loss in iteration 155 : 0.49051285936663996
Loss in iteration 156 : 0.47814688642376774
Loss in iteration 157 : 0.48925129710685805
Loss in iteration 158 : 0.4812191257192194
Loss in iteration 159 : 0.4844056225131728
Loss in iteration 160 : 0.48260052171703055
Loss in iteration 161 : 0.48458558548642383
Loss in iteration 162 : 0.4853497525267909
Loss in iteration 163 : 0.48872745881960744
Loss in iteration 164 : 0.4879093362079446
Loss in iteration 165 : 0.4820279848399508
Loss in iteration 166 : 0.48354308449070366
Loss in iteration 167 : 0.49316224165329203
Loss in iteration 168 : 0.49099722245156546
Loss in iteration 169 : 0.48542094692939197
Loss in iteration 170 : 0.4895941151898787
Loss in iteration 171 : 0.48021615958067293
Loss in iteration 172 : 0.4858447047354035
Loss in iteration 173 : 0.48385671996763174
Loss in iteration 174 : 0.47908400152186914
Loss in iteration 175 : 0.4916136470657484
Loss in iteration 176 : 0.48037841546951404
Loss in iteration 177 : 0.48974967330097047
Loss in iteration 178 : 0.48055424296918425
Loss in iteration 179 : 0.48849432528659076
Loss in iteration 180 : 0.47917672058918825
Loss in iteration 181 : 0.48483473336878713
Loss in iteration 182 : 0.4825351265246926
Loss in iteration 183 : 0.4817524743174575
Loss in iteration 184 : 0.49319604955504776
Loss in iteration 185 : 0.48184877274751337
Loss in iteration 186 : 0.476155166601679
Loss in iteration 187 : 0.4773777482424987
Loss in iteration 188 : 0.48619442783255906
Loss in iteration 189 : 0.4870871779618516
Loss in iteration 190 : 0.47596412790222303
Loss in iteration 191 : 0.4777449174175254
Loss in iteration 192 : 0.47981750144753954
Loss in iteration 193 : 0.4818710359659856
Loss in iteration 194 : 0.4827415563314128
Loss in iteration 195 : 0.48894227001964713
Loss in iteration 196 : 0.49366946182749416
Loss in iteration 197 : 0.4737938724769782
Loss in iteration 198 : 0.4897784180865149
Loss in iteration 199 : 0.4806195763304495
Loss in iteration 200 : 0.47528469133914564
Loss in iteration 201 : 0.4820384488729666
Loss in iteration 202 : 0.4861969092174526
Loss in iteration 203 : 0.4725906214893175
Loss in iteration 204 : 0.4761764612498645
Loss in iteration 205 : 0.48150305935105714
Loss in iteration 206 : 0.4793340632422768
Loss in iteration 207 : 0.47785024936423875
Loss in iteration 208 : 0.47287127402485446
Loss in iteration 209 : 0.4839754270202106
Loss in iteration 210 : 0.4878427454644943
Loss in iteration 211 : 0.48276153720696446
Loss in iteration 212 : 0.4861506161708029
Loss in iteration 213 : 0.48637896263060426
Loss in iteration 214 : 0.49672678314966695
Loss in iteration 215 : 0.4874572185276805
Loss in iteration 216 : 0.48163521798342496
Loss in iteration 217 : 0.4784607368728452
Loss in iteration 218 : 0.47449664465025865
Loss in iteration 219 : 0.48384290454877815
Loss in iteration 220 : 0.4828096175478785
Loss in iteration 221 : 0.48251307109486585
Loss in iteration 222 : 0.4724234407666682
Loss in iteration 223 : 0.48575763434998465
Loss in iteration 224 : 0.4835174403613738
Loss in iteration 225 : 0.4782937257606275
Loss in iteration 226 : 0.48051448147573295
Loss in iteration 227 : 0.4719469015720178
Loss in iteration 228 : 0.47981781268100127
Loss in iteration 229 : 0.4812651531469338
Loss in iteration 230 : 0.47262437707505534
Loss in iteration 231 : 0.48045531545603126
Loss in iteration 232 : 0.48069684442438865
Loss in iteration 233 : 0.4774710567679001
Loss in iteration 234 : 0.48301602837381397
Loss in iteration 235 : 0.4730306324893927
Loss in iteration 236 : 0.47785078874495746
Loss in iteration 237 : 0.47946858356147126
Loss in iteration 238 : 0.4777451411229854
Loss in iteration 239 : 0.4763447951813776
Loss in iteration 240 : 0.473090341966602
Loss in iteration 241 : 0.47340297496489236
Loss in iteration 242 : 0.496879069971539
Loss in iteration 243 : 0.48558373269580957
Loss in iteration 244 : 0.47697058729464864
Loss in iteration 245 : 0.48778059091250003
Loss in iteration 246 : 0.4762489980690591
Loss in iteration 247 : 0.47134295110454716
Loss in iteration 248 : 0.4780173683436362
Loss in iteration 249 : 0.4812167609553102
Loss in iteration 250 : 0.4720224406985172
Loss in iteration 251 : 0.4819579139935309
Loss in iteration 252 : 0.4752551329891039
Loss in iteration 253 : 0.48442102038778573
Loss in iteration 254 : 0.4652401624412334
Loss in iteration 255 : 0.47068626750850856
Loss in iteration 256 : 0.48829411060680694
Loss in iteration 257 : 0.4697153390035976
Loss in iteration 258 : 0.4727899382190395
Loss in iteration 259 : 0.47138758636058997
Loss in iteration 260 : 0.476009294187787
Loss in iteration 261 : 0.46619830922401506
Loss in iteration 262 : 0.4738020273080416
Loss in iteration 263 : 0.47616726785672875
Loss in iteration 264 : 0.48819122505493956
Loss in iteration 265 : 0.4687499736843252
Loss in iteration 266 : 0.48367885084573226
Loss in iteration 267 : 0.48083723821427227
Loss in iteration 268 : 0.48443631856012914
Loss in iteration 269 : 0.4715598982316977
Loss in iteration 270 : 0.47622835029691535
Loss in iteration 271 : 0.46597151520544666
Loss in iteration 272 : 0.47645226043638356
Loss in iteration 273 : 0.46274788982759546
Loss in iteration 274 : 0.47018017775971577
Loss in iteration 275 : 0.47730278879851606
Loss in iteration 276 : 0.47982676076623265
Loss in iteration 277 : 0.4694795840593425
Loss in iteration 278 : 0.46843610730572094
Loss in iteration 279 : 0.46635143361916337
Loss in iteration 280 : 0.47072958779105223
Loss in iteration 281 : 0.47895154917030597
Loss in iteration 282 : 0.4718152114133138
Loss in iteration 283 : 0.4778493684123776
Loss in iteration 284 : 0.463649923493004
Loss in iteration 285 : 0.46704430980643147
Loss in iteration 286 : 0.4740862676145701
Loss in iteration 287 : 0.47665783118650473
Loss in iteration 288 : 0.47658037983164764
Loss in iteration 289 : 0.4702087369511201
Loss in iteration 290 : 0.4667155858798084
Loss in iteration 291 : 0.47701972919302554
Loss in iteration 292 : 0.47577994840199156
Loss in iteration 293 : 0.4697439244154462
Loss in iteration 294 : 0.47915721814214407
Loss in iteration 295 : 0.46701409613094047
Loss in iteration 296 : 0.46938914550931216
Loss in iteration 297 : 0.4761639913261706
Loss in iteration 298 : 0.4749940115880993
Loss in iteration 299 : 0.4750077795296277
Loss in iteration 300 : 0.47856161021034677
Loss in iteration 301 : 0.4769707913382103
Loss in iteration 302 : 0.47280362269220955
Loss in iteration 303 : 0.46689967035995616
Loss in iteration 304 : 0.46957374128648993
Loss in iteration 305 : 0.4690710478929189
Loss in iteration 306 : 0.4708360462543736
Loss in iteration 307 : 0.47316613565642157
Loss in iteration 308 : 0.4749517738480773
Loss in iteration 309 : 0.4666983957588397
Loss in iteration 310 : 0.4714179227517748
Loss in iteration 311 : 0.4797964123010479
Loss in iteration 312 : 0.4871367214846194
Loss in iteration 313 : 0.46201589260492787
Loss in iteration 314 : 0.4681437749999279
Loss in iteration 315 : 0.4694699338997593
Loss in iteration 316 : 0.48507833177225557
Loss in iteration 317 : 0.46967317960364807
Loss in iteration 318 : 0.48226434025653714
Loss in iteration 319 : 0.4681849875135695
Loss in iteration 320 : 0.46686862000703333
Loss in iteration 321 : 0.4687608544277752
Loss in iteration 322 : 0.46983192406674057
Loss in iteration 323 : 0.46914840444644323
Loss in iteration 324 : 0.4705664741767234
Loss in iteration 325 : 0.4696619755261166
Loss in iteration 326 : 0.48288677789919154
Loss in iteration 327 : 0.46430746923720695
Loss in iteration 328 : 0.46747655070985
Loss in iteration 329 : 0.4682167396959644
Loss in iteration 330 : 0.4633186440296293
Loss in iteration 331 : 0.4711965794726878
Loss in iteration 332 : 0.4595978844978415
Loss in iteration 333 : 0.4616030608798759
Loss in iteration 334 : 0.47170060095080885
Loss in iteration 335 : 0.46384935617735384
Loss in iteration 336 : 0.47064594658644765
Loss in iteration 337 : 0.4729475213913332
Loss in iteration 338 : 0.4788512619787352
Loss in iteration 339 : 0.4810660677517618
Loss in iteration 340 : 0.4758827383201055
Loss in iteration 341 : 0.4688166167786107
Loss in iteration 342 : 0.4706652215339622
Loss in iteration 343 : 0.47017241900359674
Loss in iteration 344 : 0.47568036870016517
Loss in iteration 345 : 0.4709661196488861
Loss in iteration 346 : 0.4659948486825256
Loss in iteration 347 : 0.4726140060638546
Loss in iteration 348 : 0.47271843492643933
Loss in iteration 349 : 0.4773165423833081
Loss in iteration 350 : 0.47500762511543937
Loss in iteration 351 : 0.47122310879575036
Loss in iteration 352 : 0.4687535819706574
Loss in iteration 353 : 0.4648601303959944
Loss in iteration 354 : 0.470666848140304
Loss in iteration 355 : 0.4704635023650753
Loss in iteration 356 : 0.46388144751911614
Loss in iteration 357 : 0.46874346884911455
Loss in iteration 358 : 0.47459442449009676
Loss in iteration 359 : 0.4690567099646764
Loss in iteration 360 : 0.47529779925584537
Loss in iteration 361 : 0.462965630009121
Loss in iteration 362 : 0.4790180408144942
Loss in iteration 363 : 0.4707394207322207
Loss in iteration 364 : 0.47138393150772734
Loss in iteration 365 : 0.47280867348543454
Loss in iteration 366 : 0.4681958880303617
Loss in iteration 367 : 0.4746990926975935
Loss in iteration 368 : 0.47671593467554924
Loss in iteration 369 : 0.48206711159215726
Loss in iteration 370 : 0.46749464276472974
Loss in iteration 371 : 0.4838811713945258
Loss in iteration 372 : 0.46340894620828676
Loss in iteration 373 : 0.47085491044811895
Loss in iteration 374 : 0.4688750254528595
Loss in iteration 375 : 0.46772514729012554
Loss in iteration 376 : 0.4769471989531422
Loss in iteration 377 : 0.4706160003499947
Loss in iteration 378 : 0.4735486607500486
Loss in iteration 379 : 0.4719651012940132
Loss in iteration 380 : 0.46757765200591395
Loss in iteration 381 : 0.46081774694567523
Loss in iteration 382 : 0.46581900344596444
Loss in iteration 383 : 0.47112269000025886
Loss in iteration 384 : 0.47614178207261526
Loss in iteration 385 : 0.4708684886858441
Loss in iteration 386 : 0.46020330087425637
Loss in iteration 387 : 0.47341337045334697
Testing accuracy  of updater 0 on alg 0 with rate 0.4 = 0.7855, training accuracy 0.7855, time elapsed: 5542 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6883546246267902
Loss in iteration 3 : 0.6832310756991506
Loss in iteration 4 : 0.6816290474789006
Loss in iteration 5 : 0.677852467987328
Loss in iteration 6 : 0.6768920552352543
Loss in iteration 7 : 0.6729495762786983
Loss in iteration 8 : 0.6698165307544036
Loss in iteration 9 : 0.6684832837180198
Loss in iteration 10 : 0.6677023146325438
Loss in iteration 11 : 0.6625898633178691
Loss in iteration 12 : 0.6610301473933403
Loss in iteration 13 : 0.6599410551313373
Loss in iteration 14 : 0.6565401753796272
Loss in iteration 15 : 0.6558145110361217
Loss in iteration 16 : 0.6529257630574122
Loss in iteration 17 : 0.6497147655177463
Loss in iteration 18 : 0.6485432757149568
Loss in iteration 19 : 0.6484820308243762
Loss in iteration 20 : 0.6468464953184554
Loss in iteration 21 : 0.6447784637126384
Loss in iteration 22 : 0.6430891819466338
Loss in iteration 23 : 0.6424545152775172
Loss in iteration 24 : 0.6373868297431943
Loss in iteration 25 : 0.6384819077698554
Loss in iteration 26 : 0.6330648332344647
Loss in iteration 27 : 0.6330686844948101
Loss in iteration 28 : 0.633334404675508
Loss in iteration 29 : 0.6327231153292457
Loss in iteration 30 : 0.6328766758009867
Loss in iteration 31 : 0.6304803989509834
Loss in iteration 32 : 0.6297997689664807
Loss in iteration 33 : 0.6269221819066045
Loss in iteration 34 : 0.6250850847852235
Loss in iteration 35 : 0.6251870597023265
Loss in iteration 36 : 0.6219810523712377
Loss in iteration 37 : 0.6200710442158895
Loss in iteration 38 : 0.6192316237024102
Loss in iteration 39 : 0.6179860031364044
Loss in iteration 40 : 0.6178082268736527
Loss in iteration 41 : 0.6161390391225794
Loss in iteration 42 : 0.6128339784460649
Loss in iteration 43 : 0.6143162887516095
Loss in iteration 44 : 0.6124294948502473
Loss in iteration 45 : 0.612099148314133
Loss in iteration 46 : 0.6107030653064474
Loss in iteration 47 : 0.6088219048135444
Loss in iteration 48 : 0.6097062559892037
Loss in iteration 49 : 0.6061135208241418
Loss in iteration 50 : 0.6048793272579669
Loss in iteration 51 : 0.6061906073424881
Loss in iteration 52 : 0.6025789138982435
Loss in iteration 53 : 0.6020183815437905
Loss in iteration 54 : 0.5982768660859765
Loss in iteration 55 : 0.5980812233192778
Loss in iteration 56 : 0.6009451058419336
Loss in iteration 57 : 0.5953478050241975
Loss in iteration 58 : 0.5957876662669369
Loss in iteration 59 : 0.5951066700849817
Loss in iteration 60 : 0.5955794121224022
Loss in iteration 61 : 0.5958659072554139
Loss in iteration 62 : 0.5934367418756006
Loss in iteration 63 : 0.5919957303677709
Loss in iteration 64 : 0.5912022345327285
Loss in iteration 65 : 0.5924815292686858
Loss in iteration 66 : 0.5877203675355199
Loss in iteration 67 : 0.5874848937326423
Loss in iteration 68 : 0.5854836133268153
Loss in iteration 69 : 0.5890426771682462
Loss in iteration 70 : 0.5856951501839375
Loss in iteration 71 : 0.590387842125206
Loss in iteration 72 : 0.5848314625270319
Loss in iteration 73 : 0.5838843508779217
Loss in iteration 74 : 0.5836988108187252
Loss in iteration 75 : 0.5825776066301782
Loss in iteration 76 : 0.5844689340139559
Loss in iteration 77 : 0.5811455800666971
Loss in iteration 78 : 0.582776826014211
Loss in iteration 79 : 0.5725682319622477
Loss in iteration 80 : 0.5778838630790434
Loss in iteration 81 : 0.580756309565876
Loss in iteration 82 : 0.5763002841455231
Loss in iteration 83 : 0.5798533329881995
Loss in iteration 84 : 0.5778511910788415
Loss in iteration 85 : 0.577002655019519
Loss in iteration 86 : 0.5788157210938059
Loss in iteration 87 : 0.570293400860294
Loss in iteration 88 : 0.578679158469291
Loss in iteration 89 : 0.570559108839878
Loss in iteration 90 : 0.5733069510584321
Loss in iteration 91 : 0.5747451156924471
Loss in iteration 92 : 0.5682024390767471
Loss in iteration 93 : 0.568372798697171
Loss in iteration 94 : 0.5709525584174369
Loss in iteration 95 : 0.5677985547085814
Loss in iteration 96 : 0.5742525821711509
Loss in iteration 97 : 0.5669571651000758
Loss in iteration 98 : 0.5676779768820884
Loss in iteration 99 : 0.5669463154205114
Loss in iteration 100 : 0.5702434089354942
Loss in iteration 101 : 0.5638028619629313
Loss in iteration 102 : 0.5642358617375414
Loss in iteration 103 : 0.5668728583730444
Loss in iteration 104 : 0.564524361808014
Loss in iteration 105 : 0.5619173405690049
Loss in iteration 106 : 0.5626645094257875
Loss in iteration 107 : 0.5657663687942281
Loss in iteration 108 : 0.5693098732284987
Loss in iteration 109 : 0.5628390913434427
Loss in iteration 110 : 0.5596168151546483
Loss in iteration 111 : 0.5622356695563108
Loss in iteration 112 : 0.5587465140599812
Loss in iteration 113 : 0.5578365622130738
Loss in iteration 114 : 0.5564213948288351
Loss in iteration 115 : 0.5559656807260133
Loss in iteration 116 : 0.5576052746919578
Loss in iteration 117 : 0.5605421191613486
Loss in iteration 118 : 0.5593772732796626
Loss in iteration 119 : 0.5538039220918888
Loss in iteration 120 : 0.5598591589084386
Loss in iteration 121 : 0.5598821649800518
Loss in iteration 122 : 0.5585513236390387
Loss in iteration 123 : 0.5545182787981245
Loss in iteration 124 : 0.5523274032585722
Loss in iteration 125 : 0.5540162263309929
Loss in iteration 126 : 0.5514152896880169
Loss in iteration 127 : 0.5521619585926122
Loss in iteration 128 : 0.552699627367338
Loss in iteration 129 : 0.550883745233663
Loss in iteration 130 : 0.5527448511175116
Loss in iteration 131 : 0.5494115849000009
Loss in iteration 132 : 0.5506748661669466
Loss in iteration 133 : 0.5501316648972463
Loss in iteration 134 : 0.5516676026286049
Loss in iteration 135 : 0.5499046689302981
Loss in iteration 136 : 0.5500062933349912
Loss in iteration 137 : 0.5484874819759911
Loss in iteration 138 : 0.5510224714090534
Loss in iteration 139 : 0.550355196484146
Loss in iteration 140 : 0.549157780180966
Loss in iteration 141 : 0.5502845054455044
Loss in iteration 142 : 0.5465184209730506
Loss in iteration 143 : 0.5460430317529739
Loss in iteration 144 : 0.5450798279060419
Loss in iteration 145 : 0.5421254868987505
Loss in iteration 146 : 0.5459685602842947
Loss in iteration 147 : 0.5490437911433953
Loss in iteration 148 : 0.5446643622142722
Loss in iteration 149 : 0.5494317462454117
Loss in iteration 150 : 0.5463365218558544
Loss in iteration 151 : 0.5437582694146501
Loss in iteration 152 : 0.5466201324832413
Loss in iteration 153 : 0.5359491966994978
Loss in iteration 154 : 0.545691484221302
Loss in iteration 155 : 0.5445607066752908
Loss in iteration 156 : 0.5379157748180333
Loss in iteration 157 : 0.5428943668144156
Loss in iteration 158 : 0.5373918732325675
Loss in iteration 159 : 0.5401833365479802
Loss in iteration 160 : 0.5373368106474277
Loss in iteration 161 : 0.5407321784115283
Loss in iteration 162 : 0.5391994337263403
Loss in iteration 163 : 0.5420467603918804
Loss in iteration 164 : 0.5410845961460475
Loss in iteration 165 : 0.5372362014798339
Loss in iteration 166 : 0.5371650910104468
Loss in iteration 167 : 0.5439113879744878
Loss in iteration 168 : 0.5410373456468525
Loss in iteration 169 : 0.5392746457167121
Loss in iteration 170 : 0.5401079343892032
Loss in iteration 171 : 0.5351661670295366
Loss in iteration 172 : 0.5369917375976972
Loss in iteration 173 : 0.5363130381571365
Loss in iteration 174 : 0.5333194455496726
Loss in iteration 175 : 0.5408804175636653
Loss in iteration 176 : 0.5333150088315823
Loss in iteration 177 : 0.5381153987352327
Loss in iteration 178 : 0.5331635215871018
Loss in iteration 179 : 0.537791375145189
Loss in iteration 180 : 0.5320935940850198
Loss in iteration 181 : 0.5359794156984069
Loss in iteration 182 : 0.5348487535121271
Loss in iteration 183 : 0.5322368951107992
Loss in iteration 184 : 0.5383847010999214
Loss in iteration 185 : 0.532127692872364
Loss in iteration 186 : 0.5294711720109393
Loss in iteration 187 : 0.5283461817647088
Loss in iteration 188 : 0.5335698421273141
Loss in iteration 189 : 0.5355298814007068
Loss in iteration 190 : 0.5284407748225225
Loss in iteration 191 : 0.5304337193298688
Loss in iteration 192 : 0.5297569149619162
Loss in iteration 193 : 0.5292830768047992
Loss in iteration 194 : 0.532029865540075
Loss in iteration 195 : 0.5358803520133816
Loss in iteration 196 : 0.537495995072749
Loss in iteration 197 : 0.526189223521818
Loss in iteration 198 : 0.535887982304999
Loss in iteration 199 : 0.5300224078769146
Loss in iteration 200 : 0.5268390861301344
Loss in iteration 201 : 0.5310793880373281
Loss in iteration 202 : 0.5300936090223238
Loss in iteration 203 : 0.5229581487969378
Loss in iteration 204 : 0.5248401836206577
Loss in iteration 205 : 0.5289608917293369
Loss in iteration 206 : 0.5263345756369316
Loss in iteration 207 : 0.5274106817483732
Loss in iteration 208 : 0.5218543985780972
Loss in iteration 209 : 0.5299459877919364
Loss in iteration 210 : 0.5321324737607593
Loss in iteration 211 : 0.528769844641505
Loss in iteration 212 : 0.5310281868141062
Loss in iteration 213 : 0.5313422721312834
Loss in iteration 214 : 0.5370713305177929
Loss in iteration 215 : 0.5313978630742764
Loss in iteration 216 : 0.5286083559159639
Loss in iteration 217 : 0.5240693543487962
Loss in iteration 218 : 0.522083596059421
Loss in iteration 219 : 0.5270784091543551
Loss in iteration 220 : 0.5268996036741632
Loss in iteration 221 : 0.5266615821383336
Loss in iteration 222 : 0.5198559371285449
Loss in iteration 223 : 0.529960438379583
Loss in iteration 224 : 0.5255065452820197
Loss in iteration 225 : 0.5244887974785462
Loss in iteration 226 : 0.5241455123019717
Loss in iteration 227 : 0.5196421284573499
Loss in iteration 228 : 0.5232200637012548
Loss in iteration 229 : 0.5252225978757254
Loss in iteration 230 : 0.5181223521065375
Loss in iteration 231 : 0.5228521657723192
Loss in iteration 232 : 0.5228539700442821
Loss in iteration 233 : 0.5225666154898307
Loss in iteration 234 : 0.5251943462862899
Loss in iteration 235 : 0.5179366259821401
Loss in iteration 236 : 0.5204989622923422
Loss in iteration 237 : 0.5224077349039901
Loss in iteration 238 : 0.5209371450763643
Loss in iteration 239 : 0.5179346376259646
Loss in iteration 240 : 0.5177569205241424
Loss in iteration 241 : 0.5167853076315779
Loss in iteration 242 : 0.5342532301909096
Loss in iteration 243 : 0.5257368864004551
Loss in iteration 244 : 0.5196532228444473
Loss in iteration 245 : 0.5260611959314822
Loss in iteration 246 : 0.5194749352051408
Loss in iteration 247 : 0.5163713646186813
Loss in iteration 248 : 0.5188507545752464
Loss in iteration 249 : 0.5217288844077203
Loss in iteration 250 : 0.5159067190694068
Loss in iteration 251 : 0.5236138594170964
Loss in iteration 252 : 0.5179605693060682
Loss in iteration 253 : 0.5225677123250209
Loss in iteration 254 : 0.5114282201087902
Loss in iteration 255 : 0.5138415120031696
Loss in iteration 256 : 0.5251361151986509
Loss in iteration 257 : 0.5140364317497428
Loss in iteration 258 : 0.5148973082321576
Loss in iteration 259 : 0.5130898674656328
Loss in iteration 260 : 0.517800897095427
Loss in iteration 261 : 0.5112498650290892
Loss in iteration 262 : 0.5135212450656329
Loss in iteration 263 : 0.5167548566385306
Loss in iteration 264 : 0.5249637114616614
Loss in iteration 265 : 0.5120068925216865
Loss in iteration 266 : 0.521797670823103
Loss in iteration 267 : 0.5190424260926286
Loss in iteration 268 : 0.5204848036195319
Loss in iteration 269 : 0.5130981322827287
Loss in iteration 270 : 0.5172456838138548
Loss in iteration 271 : 0.5100389516421511
Loss in iteration 272 : 0.517228681972299
Loss in iteration 273 : 0.5072932169352944
Loss in iteration 274 : 0.5110916231881375
Loss in iteration 275 : 0.5144406944265687
Loss in iteration 276 : 0.516790677574672
Loss in iteration 277 : 0.5108398781726723
Loss in iteration 278 : 0.509188641822297
Loss in iteration 279 : 0.5096706682529812
Loss in iteration 280 : 0.5113889638046225
Loss in iteration 281 : 0.5179017977496475
Loss in iteration 282 : 0.5118774738624245
Loss in iteration 283 : 0.5150463473331023
Loss in iteration 284 : 0.506244643959672
Loss in iteration 285 : 0.5099842586320664
Loss in iteration 286 : 0.5122244565069557
Loss in iteration 287 : 0.5139962696014136
Loss in iteration 288 : 0.5141635767357542
Loss in iteration 289 : 0.5096677945476709
Loss in iteration 290 : 0.5070544926094548
Loss in iteration 291 : 0.512959577316086
Loss in iteration 292 : 0.5136836917870493
Loss in iteration 293 : 0.5088604256336378
Loss in iteration 294 : 0.5151727389950449
Loss in iteration 295 : 0.5070556001968407
Loss in iteration 296 : 0.5081593959566599
Loss in iteration 297 : 0.511478669297006
Loss in iteration 298 : 0.5130974699442394
Loss in iteration 299 : 0.5114636744771108
Loss in iteration 300 : 0.5145037778068098
Loss in iteration 301 : 0.5136125982535871
Loss in iteration 302 : 0.5105921779419754
Loss in iteration 303 : 0.5068262310396977
Loss in iteration 304 : 0.5083360141324427
Loss in iteration 305 : 0.5056009092138254
Loss in iteration 306 : 0.5087532181075071
Loss in iteration 307 : 0.5097887792966175
Loss in iteration 308 : 0.5106107248389458
Loss in iteration 309 : 0.5069104226932987
Loss in iteration 310 : 0.5086120597544028
Loss in iteration 311 : 0.514543449651154
Loss in iteration 312 : 0.5188721692014425
Loss in iteration 313 : 0.5019531641911028
Loss in iteration 314 : 0.5067827605056173
Loss in iteration 315 : 0.5053158952520853
Loss in iteration 316 : 0.517445367596862
Loss in iteration 317 : 0.5045117681619046
Loss in iteration 318 : 0.515470451933038
Loss in iteration 319 : 0.5061456988497597
Loss in iteration 320 : 0.5039493006240192
Loss in iteration 321 : 0.5059068548124751
Loss in iteration 322 : 0.5049376187424864
Loss in iteration 323 : 0.5058292870011682
Loss in iteration 324 : 0.5072145451923071
Loss in iteration 325 : 0.5057437236618039
Loss in iteration 326 : 0.5129198962724848
Loss in iteration 327 : 0.5016238073507341
Loss in iteration 328 : 0.5047418003658584
Loss in iteration 329 : 0.505996320987092
Loss in iteration 330 : 0.4999047624422927
Loss in iteration 331 : 0.5058225493548214
Loss in iteration 332 : 0.4982545290749708
Loss in iteration 333 : 0.500089325156315
Loss in iteration 334 : 0.5085483110182294
Loss in iteration 335 : 0.5012263694542413
Loss in iteration 336 : 0.5051273587757972
Loss in iteration 337 : 0.5055342542583091
Loss in iteration 338 : 0.5120258248146311
Loss in iteration 339 : 0.5130246830313168
Loss in iteration 340 : 0.5088647098141494
Loss in iteration 341 : 0.5027304012041666
Loss in iteration 342 : 0.5056809553422555
Loss in iteration 343 : 0.5047790305178335
Loss in iteration 344 : 0.5093617021677617
Loss in iteration 345 : 0.5065495776274938
Loss in iteration 346 : 0.5019571842336606
Loss in iteration 347 : 0.5083900948029073
Loss in iteration 348 : 0.5058257429564869
Loss in iteration 349 : 0.5097592214189034
Loss in iteration 350 : 0.5054088769265912
Loss in iteration 351 : 0.503797466704871
Loss in iteration 352 : 0.503000947325083
Loss in iteration 353 : 0.5011161557656311
Loss in iteration 354 : 0.504631380355738
Loss in iteration 355 : 0.5034315700741003
Loss in iteration 356 : 0.4986172843926378
Loss in iteration 357 : 0.5027164849002216
Loss in iteration 358 : 0.5062916965846406
Loss in iteration 359 : 0.5024317612247697
Loss in iteration 360 : 0.5073692213915624
Loss in iteration 361 : 0.49867855842267855
Loss in iteration 362 : 0.5082208613104435
Loss in iteration 363 : 0.50318251556039
Loss in iteration 364 : 0.5063836250180783
Loss in iteration 365 : 0.5043112061357748
Loss in iteration 366 : 0.5012506872939606
Loss in iteration 367 : 0.504199787527517
Loss in iteration 368 : 0.5076750497400676
Loss in iteration 369 : 0.5121674881758285
Loss in iteration 370 : 0.500489695821039
Loss in iteration 371 : 0.5132692424436193
Loss in iteration 372 : 0.4965062059594207
Loss in iteration 373 : 0.5038772022166249
Loss in iteration 374 : 0.5002378540549771
Loss in iteration 375 : 0.500873124919685
Loss in iteration 376 : 0.507583549322754
Loss in iteration 377 : 0.5029505705813705
Loss in iteration 378 : 0.5041974966245918
Loss in iteration 379 : 0.5022870133338747
Loss in iteration 380 : 0.5008866151884988
Loss in iteration 381 : 0.4974131773742339
Loss in iteration 382 : 0.49898560834785416
Loss in iteration 383 : 0.5010812833667584
Loss in iteration 384 : 0.5044800628210404
Loss in iteration 385 : 0.5019881948497396
Loss in iteration 386 : 0.4950377646594405
Loss in iteration 387 : 0.5032943580133478
Loss in iteration 388 : 0.5077683895545871
Loss in iteration 389 : 0.4967848712478979
Loss in iteration 390 : 0.5009141268745976
Loss in iteration 391 : 0.5019825089293483
Loss in iteration 392 : 0.4942810345845887
Loss in iteration 393 : 0.49429001555549773
Loss in iteration 394 : 0.4990965218929905
Loss in iteration 395 : 0.5035331293759708
Loss in iteration 396 : 0.500081205833396
Loss in iteration 397 : 0.5074287867978542
Loss in iteration 398 : 0.5056502353324758
Loss in iteration 399 : 0.49501237864134684
Loss in iteration 400 : 0.4962739742983043
Testing accuracy  of updater 0 on alg 0 with rate 0.09999999999999998 = 0.776875, training accuracy 0.776875, time elapsed: 5777 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 2.17168732111473
Loss in iteration 3 : 20.94885281271472
Loss in iteration 4 : 6.9808830278475655
Loss in iteration 5 : 24.250980530785267
Loss in iteration 6 : 31.21332185319588
Loss in iteration 7 : 13.611942301789176
Loss in iteration 8 : 16.59140108317314
Loss in iteration 9 : 24.79672936833688
Loss in iteration 10 : 8.741783187136964
Loss in iteration 11 : 11.067010786735668
Loss in iteration 12 : 18.272624725284537
Loss in iteration 13 : 8.206922276107642
Loss in iteration 14 : 7.720367866738263
Loss in iteration 15 : 15.182234953072276
Loss in iteration 16 : 10.557838318135136
Loss in iteration 17 : 5.147073461226726
Loss in iteration 18 : 10.665079602230767
Loss in iteration 19 : 11.237083023336885
Loss in iteration 20 : 5.828243277075688
Loss in iteration 21 : 8.187032203095143
Loss in iteration 22 : 10.705862334058168
Loss in iteration 23 : 8.054407895336517
Loss in iteration 24 : 5.993706238699358
Loss in iteration 25 : 8.533832993746739
Loss in iteration 26 : 8.106619561924212
Loss in iteration 27 : 5.587881185950856
Loss in iteration 28 : 6.812690083163629
Loss in iteration 29 : 8.108758530700568
Loss in iteration 30 : 6.208378283455511
Loss in iteration 31 : 5.596118492886868
Loss in iteration 32 : 7.152259555928857
Loss in iteration 33 : 5.784273099923491
Loss in iteration 34 : 4.834424882013336
Loss in iteration 35 : 6.3843656504611985
Loss in iteration 36 : 5.644827853064473
Loss in iteration 37 : 4.331200609937147
Loss in iteration 38 : 5.635716498279814
Loss in iteration 39 : 4.466127780256225
Loss in iteration 40 : 4.171226202077921
Loss in iteration 41 : 4.946795735987987
Loss in iteration 42 : 3.937102753772638
Loss in iteration 43 : 3.9074949422752017
Loss in iteration 44 : 4.097569532540046
Loss in iteration 45 : 3.2983470346731036
Loss in iteration 46 : 3.6664680491654087
Loss in iteration 47 : 3.1819831996330463
Loss in iteration 48 : 3.0799266562477348
Loss in iteration 49 : 3.2247265086236103
Loss in iteration 50 : 2.5007852345918096
Loss in iteration 51 : 3.046473950044803
Loss in iteration 52 : 2.4499196421265412
Loss in iteration 53 : 2.6845659653383116
Loss in iteration 54 : 2.2260807327751104
Loss in iteration 55 : 2.5201386880543546
Loss in iteration 56 : 2.230894326039433
Loss in iteration 57 : 2.3275281341068785
Loss in iteration 58 : 1.9218504261268126
Loss in iteration 59 : 2.0657017515711424
Loss in iteration 60 : 2.092438763224537
Loss in iteration 61 : 1.8634743041013992
Loss in iteration 62 : 2.151066852947631
Loss in iteration 63 : 1.7967962420066703
Loss in iteration 64 : 1.6411466218433801
Loss in iteration 65 : 1.99667311938584
Loss in iteration 66 : 1.8761364283218602
Loss in iteration 67 : 1.7661277314066282
Loss in iteration 68 : 1.385977649160708
Loss in iteration 69 : 1.3607315207250794
Loss in iteration 70 : 1.7320748932963108
Loss in iteration 71 : 2.2574451258270956
Loss in iteration 72 : 2.477770866600259
Loss in iteration 73 : 1.7765507657701936
Loss in iteration 74 : 1.3451192696195822
Loss in iteration 75 : 1.2958232309703286
Loss in iteration 76 : 1.5400686652398192
Loss in iteration 77 : 1.9034152771393757
Loss in iteration 78 : 3.240892001643942
Loss in iteration 79 : 2.5212298764223706
Loss in iteration 80 : 2.041329932527069
Loss in iteration 81 : 1.466982814882403
Loss in iteration 82 : 1.349892024994666
Loss in iteration 83 : 1.2800099054903282
Loss in iteration 84 : 1.6304617497946399
Loss in iteration 85 : 2.1289886781618406
Loss in iteration 86 : 3.5462813421026342
Loss in iteration 87 : 2.4762813540460686
Loss in iteration 88 : 1.9843279606069213
Loss in iteration 89 : 1.4988073528063088
Loss in iteration 90 : 1.632138080611793
Loss in iteration 91 : 1.851976207809649
Loss in iteration 92 : 2.662931336400858
Loss in iteration 93 : 2.4701754928251165
Loss in iteration 94 : 2.3892280895719202
Loss in iteration 95 : 1.7659310808819824
Loss in iteration 96 : 1.6559728323498097
Loss in iteration 97 : 1.4198719819462298
Loss in iteration 98 : 1.526285188314611
Loss in iteration 99 : 1.726895256820848
Loss in iteration 100 : 2.645278758756312
Loss in iteration 101 : 3.017792011148696
Loss in iteration 102 : 3.039147526242322
Loss in iteration 103 : 1.5763458398648933
Loss in iteration 104 : 1.0749627848265026
Loss in iteration 105 : 0.9431334753670084
Loss in iteration 106 : 1.3522223599969243
Loss in iteration 107 : 2.426140757911199
Loss in iteration 108 : 3.6795263948179224
Loss in iteration 109 : 3.261840392657866
Loss in iteration 110 : 1.6027625456901298
Loss in iteration 111 : 1.1708498674599153
Loss in iteration 112 : 0.9835355231893788
Loss in iteration 113 : 1.0005026576132576
Loss in iteration 114 : 1.4407993432412427
Loss in iteration 115 : 2.1092386400318284
Loss in iteration 116 : 3.2672397886751923
Loss in iteration 117 : 2.282811028307817
Loss in iteration 118 : 1.8658182894384043
Loss in iteration 119 : 1.8730631500724777
Loss in iteration 120 : 2.2287834958610677
Loss in iteration 121 : 2.0780308487984307
Loss in iteration 122 : 2.116000722841049
Loss in iteration 123 : 1.888987673058876
Loss in iteration 124 : 1.6975775920010685
Loss in iteration 125 : 1.4168551404552119
Loss in iteration 126 : 1.3470538053776375
Loss in iteration 127 : 1.2261322592619848
Loss in iteration 128 : 1.3244945398402994
Loss in iteration 129 : 1.6249429563472864
Loss in iteration 130 : 2.9992882368201483
Loss in iteration 131 : 3.034603671978776
Loss in iteration 132 : 2.979686060148672
Loss in iteration 133 : 1.738027363226881
Loss in iteration 134 : 1.2074498704372094
Loss in iteration 135 : 1.0285096854516533
Loss in iteration 136 : 1.0621300258770232
Loss in iteration 137 : 1.301996974976221
Loss in iteration 138 : 1.8475595668020932
Loss in iteration 139 : 3.5375919325526977
Loss in iteration 140 : 3.1883759000670926
Loss in iteration 141 : 2.5572789436488907
Loss in iteration 142 : 1.5936466364534674
Loss in iteration 143 : 1.3776561074907319
Loss in iteration 144 : 1.2617218265054455
Loss in iteration 145 : 1.5772958714297978
Loss in iteration 146 : 2.1527722951095924
Loss in iteration 147 : 2.423370172322537
Loss in iteration 148 : 2.0209990312593753
Loss in iteration 149 : 1.9748873590383735
Loss in iteration 150 : 1.5339571393823193
Loss in iteration 151 : 1.4295776463799448
Loss in iteration 152 : 1.4542074989450349
Loss in iteration 153 : 1.6900056004486146
Loss in iteration 154 : 2.3860551414589137
Loss in iteration 155 : 3.3573927454149883
Loss in iteration 156 : 2.5310937395436173
Loss in iteration 157 : 2.2160372487759155
Loss in iteration 158 : 1.8829870844430632
Loss in iteration 159 : 1.76540291375229
Loss in iteration 160 : 1.5216037355144694
Loss in iteration 161 : 1.3861410534552039
Loss in iteration 162 : 1.2716843449789106
Loss in iteration 163 : 1.2134310062988425
Loss in iteration 164 : 1.274900497938635
Loss in iteration 165 : 1.7942009628259847
Loss in iteration 166 : 3.1279088156633508
Loss in iteration 167 : 3.859184023952288
Loss in iteration 168 : 1.5482048319546684
Loss in iteration 169 : 0.9702017293639597
Loss in iteration 170 : 0.9603222335780758
Loss in iteration 171 : 1.0806498137775775
Loss in iteration 172 : 1.9239433801022454
Loss in iteration 173 : 3.986298449657238
Loss in iteration 174 : 3.802368527052534
Loss in iteration 175 : 1.4248905985724274
Loss in iteration 176 : 0.9499125559100018
Loss in iteration 177 : 1.4499355435474748
Loss in iteration 178 : 2.844324977644172
Loss in iteration 179 : 4.1371667571170265
Loss in iteration 180 : 1.5872977660730312
Loss in iteration 181 : 1.020840071795646
Loss in iteration 182 : 1.6453682472562445
Loss in iteration 183 : 2.7473678329483
Loss in iteration 184 : 3.2702680906520216
Loss in iteration 185 : 1.4670872368163526
Loss in iteration 186 : 1.0015215508295916
Loss in iteration 187 : 1.6240563919167126
Loss in iteration 188 : 2.3050850709413875
Loss in iteration 189 : 2.526105471479484
Loss in iteration 190 : 1.4440019765704297
Loss in iteration 191 : 1.0019057912367355
Loss in iteration 192 : 0.9893637434518405
Loss in iteration 193 : 1.2576413008253229
Loss in iteration 194 : 2.0666746123022124
Loss in iteration 195 : 3.4148653224427172
Loss in iteration 196 : 3.8936184539408853
Loss in iteration 197 : 1.5491395402173072
Loss in iteration 198 : 1.0682593350461622
Loss in iteration 199 : 0.942971560428301
Loss in iteration 200 : 0.8506208178657025
Loss in iteration 201 : 0.8521001141243554
Loss in iteration 202 : 0.8952447426591216
Loss in iteration 203 : 0.7932011749785781
Loss in iteration 204 : 1.0168356005898063
Loss in iteration 205 : 3.986011131989831
Loss in iteration 206 : 7.639401163098273
Loss in iteration 207 : 0.8415195801343001
Loss in iteration 208 : 4.163221937071685
Loss in iteration 209 : 7.754673389938713
Loss in iteration 210 : 1.2227683054335394
Loss in iteration 211 : 9.310109332056898
Loss in iteration 212 : 1.3993669738216614
Loss in iteration 213 : 6.881545355234664
Loss in iteration 214 : 2.3957766230313786
Loss in iteration 215 : 3.6075146159610125
Loss in iteration 216 : 3.308833296232138
Loss in iteration 217 : 1.9547799116275149
Loss in iteration 218 : 3.594528606357374
Loss in iteration 219 : 1.8636771221007495
Loss in iteration 220 : 3.7362717586993948
Loss in iteration 221 : 1.880999439182579
Loss in iteration 222 : 2.9870102257228135
Loss in iteration 223 : 1.8282753183833447
Loss in iteration 224 : 2.746505589190629
Loss in iteration 225 : 2.678839863430112
Loss in iteration 226 : 2.011769963999044
Loss in iteration 227 : 3.5554346046024614
Loss in iteration 228 : 1.7924628654768178
Loss in iteration 229 : 2.421811409845051
Loss in iteration 230 : 2.466708032466052
Loss in iteration 231 : 1.3960692632912561
Loss in iteration 232 : 2.005669185125168
Loss in iteration 233 : 1.7440580955142726
Loss in iteration 234 : 1.2646541100908069
Loss in iteration 235 : 1.4286578630184779
Loss in iteration 236 : 1.7184875415286966
Loss in iteration 237 : 1.7152856907352794
Loss in iteration 238 : 1.4171909476298301
Loss in iteration 239 : 1.4873200180169748
Loss in iteration 240 : 1.8595611541326595
Loss in iteration 241 : 3.9188877964424838
Loss in iteration 242 : 4.4079935270586645
Loss in iteration 243 : 2.1196217673866453
Loss in iteration 244 : 1.3879070409205974
Loss in iteration 245 : 1.3251597096951264
Loss in iteration 246 : 1.3087058406863437
Loss in iteration 247 : 1.3866371772886126
Loss in iteration 248 : 1.7002649645207992
Loss in iteration 249 : 1.9880873097204654
Loss in iteration 250 : 2.182576302845689
Loss in iteration 251 : 2.5181231074069093
Loss in iteration 252 : 2.0107158760877692
Loss in iteration 253 : 1.9724273288382943
Loss in iteration 254 : 1.6179303120032895
Loss in iteration 255 : 1.7687071479376855
Loss in iteration 256 : 1.7762373602238344
Loss in iteration 257 : 1.7798391799784163
Loss in iteration 258 : 1.678376200938331
Loss in iteration 259 : 1.8689448910113164
Loss in iteration 260 : 1.9696976846466967
Loss in iteration 261 : 2.490234661013814
Loss in iteration 262 : 2.1950667302067854
Loss in iteration 263 : 2.0824633715025374
Loss in iteration 264 : 1.689532656376634
Loss in iteration 265 : 1.4892855752543712
Loss in iteration 266 : 1.3780144891298067
Loss in iteration 267 : 1.2820708084176684
Loss in iteration 268 : 1.3785325666448898
Loss in iteration 269 : 2.0877510520269427
Loss in iteration 270 : 3.711517824916164
Loss in iteration 271 : 4.558575630942195
Loss in iteration 272 : 1.799992747830633
Loss in iteration 273 : 1.1055078947774968
Loss in iteration 274 : 1.0204006534588583
Loss in iteration 275 : 1.1116406994329449
Loss in iteration 276 : 1.358145703971459
Loss in iteration 277 : 1.5949885096057395
Loss in iteration 278 : 2.486257680266092
Loss in iteration 279 : 2.519176553431981
Loss in iteration 280 : 2.627149539698256
Loss in iteration 281 : 1.9654365878833795
Loss in iteration 282 : 2.0705541127661653
Loss in iteration 283 : 1.84790613167944
Loss in iteration 284 : 1.6186633701296456
Loss in iteration 285 : 1.315244436155918
Loss in iteration 286 : 1.2797890029438364
Loss in iteration 287 : 1.2531898267829389
Loss in iteration 288 : 1.3661256483447035
Loss in iteration 289 : 1.7810775374452312
Loss in iteration 290 : 3.3950172021761706
Loss in iteration 291 : 3.4290515067464584
Loss in iteration 292 : 2.6598203083140266
Loss in iteration 293 : 1.3921941101571307
Loss in iteration 294 : 1.0789860107595817
Loss in iteration 295 : 0.9670489917052343
Loss in iteration 296 : 0.9802728814658698
Loss in iteration 297 : 1.1254691740686298
Loss in iteration 298 : 1.6006367077375632
Loss in iteration 299 : 3.5655124704152596
Loss in iteration 300 : 3.8336469615700492
Loss in iteration 301 : 2.800221714179953
Loss in iteration 302 : 1.6060773848131775
Loss in iteration 303 : 1.2039516884747192
Loss in iteration 304 : 1.0937437112913964
Loss in iteration 305 : 1.1091177664440734
Loss in iteration 306 : 1.157571527321509
Loss in iteration 307 : 1.4383470357538952
Loss in iteration 308 : 2.3289628750224742
Loss in iteration 309 : 4.043914564307236
Loss in iteration 310 : 2.451870088968733
Loss in iteration 311 : 1.8753416010225135
Loss in iteration 312 : 1.4603593269380446
Loss in iteration 313 : 1.2498664938288
Loss in iteration 314 : 1.3000093341579506
Loss in iteration 315 : 1.71399779874291
Loss in iteration 316 : 2.440703612917095
Loss in iteration 317 : 3.013578961106302
Loss in iteration 318 : 2.350453996418705
Loss in iteration 319 : 1.8245273639976414
Loss in iteration 320 : 1.4514332896945588
Loss in iteration 321 : 1.5290013752113367
Loss in iteration 322 : 1.5817248721494195
Loss in iteration 323 : 1.8330925352708767
Loss in iteration 324 : 1.9571375020857775
Loss in iteration 325 : 2.3221177101842154
Loss in iteration 326 : 2.3491860618100495
Loss in iteration 327 : 2.2485820426863774
Loss in iteration 328 : 1.742379763791452
Loss in iteration 329 : 1.7429832051681198
Loss in iteration 330 : 1.8320297314424157
Loss in iteration 331 : 2.212036949054874
Loss in iteration 332 : 1.6888425654741483
Loss in iteration 333 : 1.5065356332054916
Loss in iteration 334 : 1.377469180514467
Loss in iteration 335 : 1.4151857586336825
Loss in iteration 336 : 1.6107541838608816
Loss in iteration 337 : 2.365095793098597
Loss in iteration 338 : 2.8681918026506885
Loss in iteration 339 : 2.858170950843033
Loss in iteration 340 : 1.6245112423466868
Loss in iteration 341 : 1.1184095326378303
Loss in iteration 342 : 0.9548077616720608
Loss in iteration 343 : 0.9715387152472296
Loss in iteration 344 : 1.093526725000855
Loss in iteration 345 : 1.9621397138417702
Loss in iteration 346 : 4.735922361143322
Loss in iteration 347 : 2.984676624049559
Loss in iteration 348 : 1.8049696640740778
Loss in iteration 349 : 1.3288227665060968
Loss in iteration 350 : 1.3908142191705408
Loss in iteration 351 : 1.4215000844716406
Loss in iteration 352 : 1.858653548411208
Loss in iteration 353 : 2.2225388949043765
Loss in iteration 354 : 2.767248664268234
Loss in iteration 355 : 1.9901283516831942
Loss in iteration 356 : 1.5025965577739901
Loss in iteration 357 : 1.280247286351188
Loss in iteration 358 : 1.316812820122857
Loss in iteration 359 : 1.479142170803705
Loss in iteration 360 : 2.283158551349256
Loss in iteration 361 : 2.6316067911633745
Loss in iteration 362 : 2.9653583431981025
Loss in iteration 363 : 1.626398284349092
Loss in iteration 364 : 1.263241303753036
Loss in iteration 365 : 1.1906404567004771
Loss in iteration 366 : 1.4155660067429494
Loss in iteration 367 : 2.1564938598791565
Loss in iteration 368 : 3.7918399226640354
Loss in iteration 369 : 2.8549777670509155
Loss in iteration 370 : 1.7101273835526911
Loss in iteration 371 : 1.138212123643786
Loss in iteration 372 : 0.9627823973264092
Loss in iteration 373 : 1.149659309473979
Loss in iteration 374 : 1.9165337928942594
Loss in iteration 375 : 3.919262424803501
Loss in iteration 376 : 3.1699161543596888
Loss in iteration 377 : 2.0245561101184086
Loss in iteration 378 : 1.4200528768280651
Loss in iteration 379 : 1.159914764384531
Loss in iteration 380 : 1.0883768516712793
Loss in iteration 381 : 1.0908537530215072
Loss in iteration 382 : 1.384725621577231
Loss in iteration 383 : 2.463655463050354
Loss in iteration 384 : 3.690522364166396
Loss in iteration 385 : 3.0512577087036465
Loss in iteration 386 : 1.4527504079262104
Loss in iteration 387 : 1.1306628278068773
Loss in iteration 388 : 1.0396464245354258
Loss in iteration 389 : 0.9531469311542662
Loss in iteration 390 : 1.023408030950413
Loss in iteration 391 : 1.4416047439103756
Loss in iteration 392 : 3.467987290321273
Loss in iteration 393 : 3.926996587731474
Loss in iteration 394 : 2.535730881903049
Loss in iteration 395 : 1.4086425150313664
Loss in iteration 396 : 1.0904112861262047
Loss in iteration 397 : 1.076021819145919
Loss in iteration 398 : 1.0148751450067808
Loss in iteration 399 : 0.9454542176910578
Loss in iteration 400 : 0.9285640717006504
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.694875, training accuracy 0.694875, time elapsed: 6996 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.566322372852807
Loss in iteration 3 : 13.598726254720269
Loss in iteration 4 : 3.1737544098546397
Loss in iteration 5 : 18.058694559674706
Loss in iteration 6 : 22.39934370704009
Loss in iteration 7 : 9.576796336987883
Loss in iteration 8 : 12.288387305177405
Loss in iteration 9 : 17.882732830852557
Loss in iteration 10 : 5.840708943341963
Loss in iteration 11 : 8.527541012741626
Loss in iteration 12 : 13.537897296723235
Loss in iteration 13 : 6.001827426258663
Loss in iteration 14 : 5.549594450956036
Loss in iteration 15 : 11.20016171215235
Loss in iteration 16 : 7.783363240267445
Loss in iteration 17 : 3.6208156357990586
Loss in iteration 18 : 7.657969396983556
Loss in iteration 19 : 8.250820679792515
Loss in iteration 20 : 4.219942757230291
Loss in iteration 21 : 5.712666051482727
Loss in iteration 22 : 7.732067327238261
Loss in iteration 23 : 5.919877107422706
Loss in iteration 24 : 4.209094904777212
Loss in iteration 25 : 6.014391325508804
Loss in iteration 26 : 5.998429630651464
Loss in iteration 27 : 4.053797729749724
Loss in iteration 28 : 4.7335138942347275
Loss in iteration 29 : 5.85052359965253
Loss in iteration 30 : 4.622734848076008
Loss in iteration 31 : 3.880920112363545
Loss in iteration 32 : 5.009585931728463
Loss in iteration 33 : 4.345343281731837
Loss in iteration 34 : 3.3898561044290885
Loss in iteration 35 : 4.467414575936579
Loss in iteration 36 : 4.214807493815049
Loss in iteration 37 : 3.123329869897517
Loss in iteration 38 : 3.9149551771805777
Loss in iteration 39 : 3.418728338271679
Loss in iteration 40 : 2.9246330385845436
Loss in iteration 41 : 3.4950667125296686
Loss in iteration 42 : 3.0293949111726737
Loss in iteration 43 : 2.6339133182853978
Loss in iteration 44 : 3.0183280003925588
Loss in iteration 45 : 2.46075870401196
Loss in iteration 46 : 2.5267376613408534
Loss in iteration 47 : 2.508211634364929
Loss in iteration 48 : 2.0984100938414025
Loss in iteration 49 : 2.4340861619379788
Loss in iteration 50 : 1.8138866247475454
Loss in iteration 51 : 2.2052689059797106
Loss in iteration 52 : 1.866572854990874
Loss in iteration 53 : 1.8731340285351465
Loss in iteration 54 : 1.7835292885251741
Loss in iteration 55 : 1.7459837719113747
Loss in iteration 56 : 1.7634951133367063
Loss in iteration 57 : 1.6288784113067971
Loss in iteration 58 : 1.4892690225460303
Loss in iteration 59 : 1.5919375764649908
Loss in iteration 60 : 1.4250357477147735
Loss in iteration 61 : 1.6701778386231114
Loss in iteration 62 : 1.4848405065130958
Loss in iteration 63 : 1.3345241696970258
Loss in iteration 64 : 1.7281783366171055
Loss in iteration 65 : 1.5563916478411604
Loss in iteration 66 : 1.108655805574559
Loss in iteration 67 : 1.5078905801297617
Loss in iteration 68 : 1.6869204688508745
Loss in iteration 69 : 1.2316088855762821
Loss in iteration 70 : 0.9526735064633286
Loss in iteration 71 : 1.0939865207383195
Loss in iteration 72 : 1.243124991651629
Loss in iteration 73 : 1.177804537853798
Loss in iteration 74 : 0.8873940062173004
Loss in iteration 75 : 0.7937766056828766
Loss in iteration 76 : 0.8258563646739098
Loss in iteration 77 : 0.8671410106300586
Loss in iteration 78 : 1.6019947523361544
Loss in iteration 79 : 2.6665179401743266
Loss in iteration 80 : 2.4987148276636044
Loss in iteration 81 : 1.0762025450676278
Loss in iteration 82 : 0.7641078196099397
Loss in iteration 83 : 0.7222611391030765
Loss in iteration 84 : 0.8416695485596742
Loss in iteration 85 : 1.1097224844673974
Loss in iteration 86 : 1.8992251605475292
Loss in iteration 87 : 3.0573629526530754
Loss in iteration 88 : 1.7531775870273592
Loss in iteration 89 : 1.2012771291740394
Loss in iteration 90 : 0.9447063143971604
Loss in iteration 91 : 0.9666972740880823
Loss in iteration 92 : 0.9871050324812303
Loss in iteration 93 : 1.2986464553012764
Loss in iteration 94 : 1.7440637320985026
Loss in iteration 95 : 2.2726664741236613
Loss in iteration 96 : 1.3814642695617438
Loss in iteration 97 : 1.0984387952202663
Loss in iteration 98 : 1.1817576228652094
Loss in iteration 99 : 1.5885613629503552
Loss in iteration 100 : 1.6980728050120926
Loss in iteration 101 : 1.8314219615929623
Loss in iteration 102 : 1.2979930124771875
Loss in iteration 103 : 1.0631761471141212
Loss in iteration 104 : 0.9822973934483685
Loss in iteration 105 : 1.0447638173311407
Loss in iteration 106 : 1.3216192224783094
Loss in iteration 107 : 1.7716824871289485
Loss in iteration 108 : 2.121549104557394
Loss in iteration 109 : 2.1717718004937954
Loss in iteration 110 : 1.3784309970266149
Loss in iteration 111 : 1.07220379720088
Loss in iteration 112 : 0.7999862986253516
Loss in iteration 113 : 0.7101213108536749
Loss in iteration 114 : 0.8569784116275194
Loss in iteration 115 : 1.078707189369491
Loss in iteration 116 : 1.56083925060833
Loss in iteration 117 : 1.8199328801283647
Loss in iteration 118 : 2.0782254386012697
Loss in iteration 119 : 1.643837269101348
Loss in iteration 120 : 1.3735948732295455
Loss in iteration 121 : 1.0696288239098661
Loss in iteration 122 : 1.0547402459676403
Loss in iteration 123 : 1.1451327695260978
Loss in iteration 124 : 1.403982116952744
Loss in iteration 125 : 1.60737534351591
Loss in iteration 126 : 1.6235607941519328
Loss in iteration 127 : 1.1176121510269903
Loss in iteration 128 : 0.881947730895632
Loss in iteration 129 : 0.7519900002899916
Loss in iteration 130 : 0.7329385249079278
Loss in iteration 131 : 0.6993818997415214
Loss in iteration 132 : 0.6644574365864382
Loss in iteration 133 : 0.6767623519110246
Loss in iteration 134 : 0.7322792183126496
Loss in iteration 135 : 1.5544549179571077
Loss in iteration 136 : 4.448774819358849
Loss in iteration 137 : 1.8996423921723224
Loss in iteration 138 : 1.124778782045332
Loss in iteration 139 : 1.4780034118980718
Loss in iteration 140 : 2.575821034597951
Loss in iteration 141 : 2.1214982374931792
Loss in iteration 142 : 1.1554319905235497
Loss in iteration 143 : 0.8515517018384589
Loss in iteration 144 : 0.9965228744326452
Loss in iteration 145 : 1.3964660237475497
Loss in iteration 146 : 1.7187057342881755
Loss in iteration 147 : 1.3741506200261135
Loss in iteration 148 : 1.034648774159914
Loss in iteration 149 : 0.876362433109258
Loss in iteration 150 : 0.8581489096726631
Loss in iteration 151 : 1.072282987692515
Loss in iteration 152 : 1.6960707436448696
Loss in iteration 153 : 2.5830807874949593
Loss in iteration 154 : 1.445648398841567
Loss in iteration 155 : 0.9126975914453005
Loss in iteration 156 : 0.7159249224513511
Loss in iteration 157 : 0.7389182699076469
Loss in iteration 158 : 0.7263490664523788
Loss in iteration 159 : 0.6859714160955974
Loss in iteration 160 : 0.721772732832084
Loss in iteration 161 : 1.095503906007016
Loss in iteration 162 : 3.2218867731603344
Loss in iteration 163 : 3.6136499428799946
Loss in iteration 164 : 1.3396474511971186
Loss in iteration 165 : 0.9945255419370236
Loss in iteration 166 : 1.2889979007169656
Loss in iteration 167 : 1.918884239574371
Loss in iteration 168 : 1.7572239480841934
Loss in iteration 169 : 1.2448518628615102
Loss in iteration 170 : 0.9142406011892795
Loss in iteration 171 : 0.802951028561256
Loss in iteration 172 : 0.9980359281383905
Loss in iteration 173 : 1.309060254037409
Loss in iteration 174 : 1.6086965511538742
Loss in iteration 175 : 1.2457106023422708
Loss in iteration 176 : 0.9654676505652723
Loss in iteration 177 : 0.87683340374251
Loss in iteration 178 : 0.8434157871914728
Loss in iteration 179 : 1.0057104776817465
Loss in iteration 180 : 1.7741300347759548
Loss in iteration 181 : 2.794692705766351
Loss in iteration 182 : 2.1584276943611522
Loss in iteration 183 : 1.215534028587622
Loss in iteration 184 : 1.0448481100867422
Loss in iteration 185 : 0.9629966668701888
Loss in iteration 186 : 0.907304578897066
Loss in iteration 187 : 0.9524133778190585
Loss in iteration 188 : 1.2477314236368657
Loss in iteration 189 : 1.800852540224049
Loss in iteration 190 : 2.394365887918011
Loss in iteration 191 : 1.4229706762740475
Loss in iteration 192 : 1.0165582085157252
Loss in iteration 193 : 0.8409395991018354
Loss in iteration 194 : 0.7723551912452075
Loss in iteration 195 : 0.7446070055447523
Loss in iteration 196 : 0.7720927230429292
Loss in iteration 197 : 0.7052188414296435
Loss in iteration 198 : 0.9816855958365764
Loss in iteration 199 : 2.3889730843160675
Loss in iteration 200 : 4.027630207471161
Loss in iteration 201 : 0.954618885403811
Loss in iteration 202 : 0.8266390745918414
Loss in iteration 203 : 2.373128947131879
Loss in iteration 204 : 3.0766388201726547
Loss in iteration 205 : 1.44207498844869
Loss in iteration 206 : 0.8217928031445046
Loss in iteration 207 : 1.0722741808399323
Loss in iteration 208 : 1.5717707339635243
Loss in iteration 209 : 1.2694920469078874
Loss in iteration 210 : 0.9442150452947892
Loss in iteration 211 : 0.8266788609465724
Loss in iteration 212 : 0.948532850739023
Loss in iteration 213 : 1.1220756399645724
Loss in iteration 214 : 1.247355163767787
Loss in iteration 215 : 1.5807162886469772
Loss in iteration 216 : 1.7800297888319612
Loss in iteration 217 : 2.2419331764686135
Loss in iteration 218 : 1.1765615309503874
Loss in iteration 219 : 0.9030026981976998
Loss in iteration 220 : 0.8088230898853863
Loss in iteration 221 : 0.764005562391865
Loss in iteration 222 : 0.7526077599040828
Loss in iteration 223 : 0.8515845372547938
Loss in iteration 224 : 1.5568360094271925
Loss in iteration 225 : 3.7431117403105385
Loss in iteration 226 : 2.1610248313844505
Loss in iteration 227 : 1.162447206487142
Loss in iteration 228 : 0.9158839050479646
Loss in iteration 229 : 0.8881419038156536
Loss in iteration 230 : 1.0330796597219707
Loss in iteration 231 : 1.574305876518748
Loss in iteration 232 : 1.8021894250999122
Loss in iteration 233 : 1.7961991239618535
Loss in iteration 234 : 1.3070742884327695
Loss in iteration 235 : 0.9675265714417836
Loss in iteration 236 : 0.789978449409026
Loss in iteration 237 : 0.8829971210951967
Loss in iteration 238 : 1.2800717489141646
Loss in iteration 239 : 2.088473640217094
Loss in iteration 240 : 2.364064331264301
Loss in iteration 241 : 0.9279336968518724
Loss in iteration 242 : 0.8250713052145441
Loss in iteration 243 : 1.5615330023435345
Loss in iteration 244 : 2.736972661432646
Loss in iteration 245 : 2.232227979430727
Loss in iteration 246 : 0.888088436859775
Loss in iteration 247 : 0.8358746394651372
Loss in iteration 248 : 1.8650687655239186
Loss in iteration 249 : 2.1567508485041538
Loss in iteration 250 : 1.4737395189693168
Loss in iteration 251 : 0.8488982148835099
Loss in iteration 252 : 0.8683568725970198
Loss in iteration 253 : 1.4288327739194397
Loss in iteration 254 : 1.7399154520570468
Loss in iteration 255 : 1.6759227702619683
Loss in iteration 256 : 1.1130926045851348
Loss in iteration 257 : 0.759078040613011
Loss in iteration 258 : 0.7297879571289099
Loss in iteration 259 : 0.7636195702837272
Loss in iteration 260 : 1.0926861332596975
Loss in iteration 261 : 2.1701174629817035
Loss in iteration 262 : 3.5920353411677053
Loss in iteration 263 : 1.023899960422945
Loss in iteration 264 : 0.8009806552794401
Loss in iteration 265 : 1.7326047807772988
Loss in iteration 266 : 2.667335825591503
Loss in iteration 267 : 1.2622650925413001
Loss in iteration 268 : 0.8694533457822325
Loss in iteration 269 : 1.6780340757491539
Loss in iteration 270 : 2.3485978472388993
Loss in iteration 271 : 1.010911464001777
Loss in iteration 272 : 0.9134504508261269
Loss in iteration 273 : 1.7670070089813688
Loss in iteration 274 : 1.745463028152902
Loss in iteration 275 : 1.1808439691139645
Loss in iteration 276 : 0.8619498038101966
Loss in iteration 277 : 1.0856554251122112
Loss in iteration 278 : 1.761178480806488
Loss in iteration 279 : 1.3377447212462488
Loss in iteration 280 : 1.0128934898757267
Loss in iteration 281 : 0.8265722280601732
Loss in iteration 282 : 0.750705891101688
Loss in iteration 283 : 0.7479740663049359
Loss in iteration 284 : 0.6977988073205835
Loss in iteration 285 : 0.6915181847455605
Loss in iteration 286 : 0.9184991925825007
Loss in iteration 287 : 2.303650738376408
Loss in iteration 288 : 4.224960614141697
Loss in iteration 289 : 1.651295544585834
Loss in iteration 290 : 0.8855450438470963
Loss in iteration 291 : 0.9154074896207901
Loss in iteration 292 : 1.2149461159958002
Loss in iteration 293 : 1.8123437550728116
Loss in iteration 294 : 2.2167885552665423
Loss in iteration 295 : 1.3572805284767326
Loss in iteration 296 : 0.8849940722805941
Loss in iteration 297 : 0.8071347543088635
Loss in iteration 298 : 1.0577168823834486
Loss in iteration 299 : 1.6772278446066864
Loss in iteration 300 : 1.6471454596375947
Loss in iteration 301 : 1.5048157056488267
Loss in iteration 302 : 1.0824894711869792
Loss in iteration 303 : 0.882507801923788
Loss in iteration 304 : 0.8137451015548927
Loss in iteration 305 : 0.809833341067944
Loss in iteration 306 : 0.8420809169803328
Loss in iteration 307 : 1.1583886361442364
Loss in iteration 308 : 2.10959262463396
Loss in iteration 309 : 3.1360371616497074
Loss in iteration 310 : 1.731125008901784
Loss in iteration 311 : 1.1358585617896282
Loss in iteration 312 : 0.9814231010909426
Loss in iteration 313 : 0.8472668178681501
Loss in iteration 314 : 0.8502035908414609
Loss in iteration 315 : 0.9108799145909412
Loss in iteration 316 : 1.1994136917736602
Loss in iteration 317 : 1.8770315503769743
Loss in iteration 318 : 2.4933567743722254
Loss in iteration 319 : 2.0585905207909767
Loss in iteration 320 : 1.1049084679905032
Loss in iteration 321 : 0.9094430708781992
Loss in iteration 322 : 0.922768054412853
Loss in iteration 323 : 1.028379667183443
Loss in iteration 324 : 1.1945139004585186
Loss in iteration 325 : 1.4901165974433215
Loss in iteration 326 : 1.6684321998045206
Loss in iteration 327 : 1.5468945844291206
Loss in iteration 328 : 1.1952463167927214
Loss in iteration 329 : 1.0960257352372118
Loss in iteration 330 : 1.1635181436780775
Loss in iteration 331 : 1.4979248604038689
Loss in iteration 332 : 1.4870095644761354
Loss in iteration 333 : 1.6077464683858886
Loss in iteration 334 : 1.4596556704727253
Loss in iteration 335 : 1.357242944961566
Loss in iteration 336 : 1.1412615000858652
Loss in iteration 337 : 1.0945536872881114
Loss in iteration 338 : 1.1092326969460278
Loss in iteration 339 : 1.2635894518889836
Loss in iteration 340 : 1.2414454715754772
Loss in iteration 341 : 1.2927831550175104
Loss in iteration 342 : 1.3336815306900283
Loss in iteration 343 : 1.7355674013226101
Loss in iteration 344 : 1.7956047300761226
Loss in iteration 345 : 1.7473011796939732
Loss in iteration 346 : 1.3014495757360978
Loss in iteration 347 : 1.057238341454507
Loss in iteration 348 : 0.967033751273391
Loss in iteration 349 : 0.9995535198889991
Loss in iteration 350 : 1.1490191935612037
Loss in iteration 351 : 1.4151578600954018
Loss in iteration 352 : 1.678249026538496
Loss in iteration 353 : 1.6859049255731822
Loss in iteration 354 : 1.3270168023356148
Loss in iteration 355 : 1.234631244848208
Loss in iteration 356 : 1.150656925714041
Loss in iteration 357 : 1.3627365968537115
Loss in iteration 358 : 1.618197286808793
Loss in iteration 359 : 1.5740536797269546
Loss in iteration 360 : 1.3483018456406182
Loss in iteration 361 : 1.2287923787041954
Loss in iteration 362 : 1.2232899399467532
Loss in iteration 363 : 1.2288020342864534
Loss in iteration 364 : 1.2455406679560166
Loss in iteration 365 : 1.4512532375238212
Loss in iteration 366 : 1.4687967629102858
Loss in iteration 367 : 1.5947982811143757
Loss in iteration 368 : 1.3360045615861684
Loss in iteration 369 : 1.1983247481816086
Loss in iteration 370 : 1.122086679135983
Loss in iteration 371 : 1.254187479301671
Loss in iteration 372 : 1.3826723282160254
Loss in iteration 373 : 1.835784091971805
Loss in iteration 374 : 1.9266036140764151
Loss in iteration 375 : 1.6184174691677988
Loss in iteration 376 : 1.1677700528120525
Loss in iteration 377 : 0.9194057848811006
Loss in iteration 378 : 0.8057029800675701
Loss in iteration 379 : 0.7527199811406009
Loss in iteration 380 : 0.7399145901043036
Loss in iteration 381 : 0.8499315905316247
Loss in iteration 382 : 1.6604103870734535
Loss in iteration 383 : 2.785929725036375
Loss in iteration 384 : 2.2996734008568733
Loss in iteration 385 : 1.179200474245959
Loss in iteration 386 : 0.9689606062317413
Loss in iteration 387 : 1.0762855871633705
Loss in iteration 388 : 1.4625161703423681
Loss in iteration 389 : 1.8305949691061316
Loss in iteration 390 : 1.7835928895737805
Loss in iteration 391 : 1.2594194893224355
Loss in iteration 392 : 0.9151796201087334
Loss in iteration 393 : 0.7716719508632871
Loss in iteration 394 : 0.8143968886908739
Loss in iteration 395 : 1.0772086411849455
Loss in iteration 396 : 1.513096101941556
Loss in iteration 397 : 2.3122564537059507
Loss in iteration 398 : 1.7168623017105142
Loss in iteration 399 : 1.3260559958265086
Loss in iteration 400 : 0.9404740954339871
Testing accuracy  of updater 1 on alg 0 with rate 7.0 = 0.767875, training accuracy 0.767875, time elapsed: 5235 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.022807699595522
Loss in iteration 3 : 5.99961331380481
Loss in iteration 4 : 0.6323248083306727
Loss in iteration 5 : 1.7885173928852778
Loss in iteration 6 : 3.626010396552772
Loss in iteration 7 : 0.8611267785667455
Loss in iteration 8 : 4.407778528441887
Loss in iteration 9 : 1.9691446562357928
Loss in iteration 10 : 4.294371015720812
Loss in iteration 11 : 4.1522448632226885
Loss in iteration 12 : 1.3381224016245847
Loss in iteration 13 : 3.7259344796478913
Loss in iteration 14 : 1.377383697451389
Loss in iteration 15 : 2.7768041491063298
Loss in iteration 16 : 2.852172710522923
Loss in iteration 17 : 1.3431334785628133
Loss in iteration 18 : 2.7744302175177147
Loss in iteration 19 : 1.9840949655699924
Loss in iteration 20 : 1.6873595498392904
Loss in iteration 21 : 2.6491206247395382
Loss in iteration 22 : 1.7261204234738685
Loss in iteration 23 : 1.8897545897905732
Loss in iteration 24 : 2.1933629346220624
Loss in iteration 25 : 1.4267072152047673
Loss in iteration 26 : 1.889186838660987
Loss in iteration 27 : 1.6449255693237563
Loss in iteration 28 : 1.442533472872031
Loss in iteration 29 : 1.816234859316838
Loss in iteration 30 : 1.2593996226365514
Loss in iteration 31 : 1.5864121597407275
Loss in iteration 32 : 1.333963253310878
Loss in iteration 33 : 1.28041241975101
Loss in iteration 34 : 1.2772070969867686
Loss in iteration 35 : 1.1644185429824805
Loss in iteration 36 : 1.2768444258902039
Loss in iteration 37 : 0.9714138640101787
Loss in iteration 38 : 1.2482977051327446
Loss in iteration 39 : 0.9230559541832202
Loss in iteration 40 : 1.0675116542893155
Loss in iteration 41 : 0.9199922710278073
Loss in iteration 42 : 0.9125843368894154
Loss in iteration 43 : 0.8957898980411321
Loss in iteration 44 : 0.7905168301071388
Loss in iteration 45 : 0.9414535626270789
Loss in iteration 46 : 0.7636748880126687
Loss in iteration 47 : 0.8149834641517733
Loss in iteration 48 : 0.9514433229424789
Loss in iteration 49 : 0.7393226539230673
Loss in iteration 50 : 0.68831365150227
Loss in iteration 51 : 0.9201602385808283
Loss in iteration 52 : 0.8953352030375804
Loss in iteration 53 : 0.6959567350174102
Loss in iteration 54 : 0.6246908804989734
Loss in iteration 55 : 0.8084787963985337
Loss in iteration 56 : 1.097344028549476
Loss in iteration 57 : 0.9971885832995532
Loss in iteration 58 : 0.7253027236353461
Loss in iteration 59 : 0.5453481988795054
Loss in iteration 60 : 0.6179213416015605
Loss in iteration 61 : 0.7785004757865667
Loss in iteration 62 : 0.8699900609986259
Loss in iteration 63 : 0.7209939640803482
Loss in iteration 64 : 0.5732031062951729
Loss in iteration 65 : 0.5511952365584382
Loss in iteration 66 : 0.5952615140390924
Loss in iteration 67 : 0.8147515753706278
Loss in iteration 68 : 0.920829305352881
Loss in iteration 69 : 0.9361497170431672
Loss in iteration 70 : 0.77414048352996
Loss in iteration 71 : 0.7405182941160716
Loss in iteration 72 : 0.7349740829342295
Loss in iteration 73 : 0.7897888821431088
Loss in iteration 74 : 0.8088222702023063
Loss in iteration 75 : 0.8831219068110409
Loss in iteration 76 : 0.9453295351070562
Loss in iteration 77 : 0.8851287766739477
Loss in iteration 78 : 0.7564323964980295
Loss in iteration 79 : 0.594171740833892
Loss in iteration 80 : 0.5420240032762565
Loss in iteration 81 : 0.5198348099176141
Loss in iteration 82 : 0.5087973291951793
Loss in iteration 83 : 0.5373038941872668
Loss in iteration 84 : 0.7434687610460113
Loss in iteration 85 : 1.4304158313761455
Loss in iteration 86 : 1.925262323533332
Loss in iteration 87 : 1.0061669948279426
Loss in iteration 88 : 0.5933690676887408
Loss in iteration 89 : 0.5051400325793113
Loss in iteration 90 : 0.6266901639075065
Loss in iteration 91 : 0.916714574249282
Loss in iteration 92 : 0.9874781588570672
Loss in iteration 93 : 0.7327578113590215
Loss in iteration 94 : 0.5488868802463697
Loss in iteration 95 : 0.5498900266735801
Loss in iteration 96 : 0.6826408687619964
Loss in iteration 97 : 0.8196453346492574
Loss in iteration 98 : 0.912383047903728
Loss in iteration 99 : 0.8382898480760835
Loss in iteration 100 : 0.7792518921923741
Loss in iteration 101 : 0.7491206997115107
Loss in iteration 102 : 0.7665842000704075
Loss in iteration 103 : 0.7488662840764366
Loss in iteration 104 : 0.7219246131565854
Loss in iteration 105 : 0.6835756219703245
Loss in iteration 106 : 0.7196781899251236
Loss in iteration 107 : 0.7320610002938834
Loss in iteration 108 : 0.6973452113650144
Loss in iteration 109 : 0.6649251785939392
Loss in iteration 110 : 0.7080783253550653
Loss in iteration 111 : 0.8544833465824605
Loss in iteration 112 : 1.13670930811178
Loss in iteration 113 : 1.2624914478467264
Loss in iteration 114 : 0.9044475608024938
Loss in iteration 115 : 0.5620133259192821
Loss in iteration 116 : 0.5688683812473627
Loss in iteration 117 : 0.8724037536721313
Loss in iteration 118 : 1.1358261669153777
Loss in iteration 119 : 0.8158091396666586
Loss in iteration 120 : 0.5940751915889196
Loss in iteration 121 : 0.5445885434753687
Loss in iteration 122 : 0.7028701941495975
Loss in iteration 123 : 0.963578120580484
Loss in iteration 124 : 1.1115108953598045
Loss in iteration 125 : 0.7838280375368014
Loss in iteration 126 : 0.5775920749487665
Loss in iteration 127 : 0.5143867894380104
Loss in iteration 128 : 0.619127748164368
Loss in iteration 129 : 0.8638235002365436
Loss in iteration 130 : 1.1230637199830436
Loss in iteration 131 : 0.9973150465000425
Loss in iteration 132 : 0.6631879141239471
Loss in iteration 133 : 0.5455102354627448
Loss in iteration 134 : 0.5430987946783569
Loss in iteration 135 : 0.6167258081820332
Loss in iteration 136 : 0.8000286658041491
Loss in iteration 137 : 1.1298805716400766
Loss in iteration 138 : 1.1236989217285855
Loss in iteration 139 : 0.8040583752205708
Loss in iteration 140 : 0.6235616126761327
Loss in iteration 141 : 0.5849894971849086
Loss in iteration 142 : 0.5722224371392869
Loss in iteration 143 : 0.5998823898981975
Loss in iteration 144 : 0.6392550718372247
Loss in iteration 145 : 0.8822209664999164
Loss in iteration 146 : 1.409276050968419
Loss in iteration 147 : 1.4595425050155744
Loss in iteration 148 : 0.8274916945800141
Loss in iteration 149 : 0.5794879111545016
Loss in iteration 150 : 0.6086431399077004
Loss in iteration 151 : 0.867673756182548
Loss in iteration 152 : 1.092887519054355
Loss in iteration 153 : 0.843467694151064
Loss in iteration 154 : 0.623277184806712
Loss in iteration 155 : 0.5438500024752355
Loss in iteration 156 : 0.6338923840341192
Loss in iteration 157 : 0.8807959042257569
Loss in iteration 158 : 0.962700983239515
Loss in iteration 159 : 0.7898466001034412
Loss in iteration 160 : 0.6516823994071497
Loss in iteration 161 : 0.5506201742406439
Loss in iteration 162 : 0.5362188222394754
Loss in iteration 163 : 0.5316552660666631
Loss in iteration 164 : 0.5424451474754793
Loss in iteration 165 : 0.5670435192210451
Loss in iteration 166 : 0.7071291560347002
Loss in iteration 167 : 1.2579157441246855
Loss in iteration 168 : 1.8921306493303238
Loss in iteration 169 : 1.242919790547616
Loss in iteration 170 : 0.653060853636919
Loss in iteration 171 : 0.5382817306032089
Loss in iteration 172 : 0.7155190436210939
Loss in iteration 173 : 0.9408870346054156
Loss in iteration 174 : 0.8995240586833225
Loss in iteration 175 : 0.6699106642896135
Loss in iteration 176 : 0.5275527775926167
Loss in iteration 177 : 0.6368279386630429
Loss in iteration 178 : 0.8040036925160112
Loss in iteration 179 : 0.841950796620403
Loss in iteration 180 : 0.7783005638106575
Loss in iteration 181 : 0.6813025968877189
Loss in iteration 182 : 0.6079864658174413
Loss in iteration 183 : 0.5666999933268223
Loss in iteration 184 : 0.6369158998833815
Loss in iteration 185 : 0.7615335413205239
Loss in iteration 186 : 1.0251224340419303
Loss in iteration 187 : 1.291211512414101
Loss in iteration 188 : 1.1394094463504234
Loss in iteration 189 : 0.8384513068376829
Loss in iteration 190 : 0.6066716983649463
Loss in iteration 191 : 0.5290857792584425
Loss in iteration 192 : 0.607577994977314
Loss in iteration 193 : 0.8108207406716387
Loss in iteration 194 : 1.0029300563915968
Loss in iteration 195 : 0.9659281606084271
Loss in iteration 196 : 0.8303262032304722
Loss in iteration 197 : 0.6669766017796263
Loss in iteration 198 : 0.6282160264288486
Loss in iteration 199 : 0.5619698894465248
Loss in iteration 200 : 0.49964044961749376
Loss in iteration 201 : 0.5042606404508914
Loss in iteration 202 : 0.530761659662639
Loss in iteration 203 : 0.50932347386452
Loss in iteration 204 : 0.569074169082425
Loss in iteration 205 : 0.7635273495934831
Loss in iteration 206 : 1.3250101885090129
Loss in iteration 207 : 1.537112246151618
Loss in iteration 208 : 0.8821480592702722
Loss in iteration 209 : 0.6117981650028768
Loss in iteration 210 : 0.5460201013933741
Loss in iteration 211 : 0.5283018393298774
Loss in iteration 212 : 0.5507181451902404
Loss in iteration 213 : 0.5562185272284077
Loss in iteration 214 : 0.6110056107209012
Loss in iteration 215 : 0.7670668485415435
Loss in iteration 216 : 1.2284000144665543
Loss in iteration 217 : 1.5756865538544795
Loss in iteration 218 : 0.76736864698089
Loss in iteration 219 : 0.5476285521983703
Loss in iteration 220 : 0.6916072835260237
Loss in iteration 221 : 1.1956737030145352
Loss in iteration 222 : 1.3768459111152285
Loss in iteration 223 : 0.7974098492708647
Loss in iteration 224 : 0.5955922563448384
Loss in iteration 225 : 1.0058726190012195
Loss in iteration 226 : 1.1019290778140483
Loss in iteration 227 : 0.6370478001708276
Loss in iteration 228 : 0.6777917338577335
Loss in iteration 229 : 1.0001702592350696
Loss in iteration 230 : 0.8956446036143002
Loss in iteration 231 : 0.5882618429417803
Loss in iteration 232 : 0.6564623513241619
Loss in iteration 233 : 0.9604956705793898
Loss in iteration 234 : 1.0595566046021319
Loss in iteration 235 : 0.7488780670789029
Loss in iteration 236 : 0.5538000953499215
Loss in iteration 237 : 0.6982941909736886
Loss in iteration 238 : 1.1099553768692023
Loss in iteration 239 : 1.2066738523795344
Loss in iteration 240 : 0.7709249801635779
Loss in iteration 241 : 0.5312072978771686
Loss in iteration 242 : 0.7024213714044509
Loss in iteration 243 : 1.070137516166253
Loss in iteration 244 : 1.2046135380125345
Loss in iteration 245 : 0.8356348494978695
Loss in iteration 246 : 0.5574434215646815
Loss in iteration 247 : 0.5296048891761479
Loss in iteration 248 : 0.7501052984986737
Loss in iteration 249 : 1.036244059572225
Loss in iteration 250 : 1.1211089577452116
Loss in iteration 251 : 0.8356243221692401
Loss in iteration 252 : 0.6275337498352106
Loss in iteration 253 : 0.5671293223964303
Loss in iteration 254 : 0.5005753199131645
Loss in iteration 255 : 0.5094927793141982
Loss in iteration 256 : 0.6175741137729505
Loss in iteration 257 : 0.7686576991665932
Loss in iteration 258 : 1.1117911209177729
Loss in iteration 259 : 1.2775534346130255
Loss in iteration 260 : 0.8994933415374843
Loss in iteration 261 : 0.6786110421396487
Loss in iteration 262 : 0.6415514080517475
Loss in iteration 263 : 0.6240316708363759
Loss in iteration 264 : 0.6745156609990536
Loss in iteration 265 : 0.6717089206526479
Loss in iteration 266 : 0.7213568720950799
Loss in iteration 267 : 0.6633409321868005
Loss in iteration 268 : 0.6552924201625668
Loss in iteration 269 : 0.6955089389681064
Loss in iteration 270 : 0.9581044221831493
Loss in iteration 271 : 1.4199682682052384
Loss in iteration 272 : 1.3772858450955912
Loss in iteration 273 : 0.8453046640812892
Loss in iteration 274 : 0.5631213238227633
Loss in iteration 275 : 0.5890136270994455
Loss in iteration 276 : 0.7741918852754939
Loss in iteration 277 : 0.7549568912521724
Loss in iteration 278 : 0.6305471026598943
Loss in iteration 279 : 0.5198746188185034
Loss in iteration 280 : 0.5412570221946252
Loss in iteration 281 : 0.6415522847652159
Loss in iteration 282 : 0.7971999260777674
Loss in iteration 283 : 1.0298755408734181
Loss in iteration 284 : 1.0711304483807307
Loss in iteration 285 : 1.0745773925711797
Loss in iteration 286 : 0.8449896842455222
Loss in iteration 287 : 0.6751574963434379
Loss in iteration 288 : 0.5750303598399786
Loss in iteration 289 : 0.5232984815527896
Loss in iteration 290 : 0.5688508134622087
Loss in iteration 291 : 0.8090145886523822
Loss in iteration 292 : 1.088661564120739
Loss in iteration 293 : 1.052489394785438
Loss in iteration 294 : 0.7934597502944697
Loss in iteration 295 : 0.6113886396458307
Loss in iteration 296 : 0.5320593158617193
Loss in iteration 297 : 0.5207502557748654
Loss in iteration 298 : 0.5455885012003201
Loss in iteration 299 : 0.697912168460733
Loss in iteration 300 : 1.0932751862061318
Loss in iteration 301 : 1.490849036017773
Loss in iteration 302 : 1.1373557105269194
Loss in iteration 303 : 0.6664399110266211
Loss in iteration 304 : 0.5243365779856616
Loss in iteration 305 : 0.5792325757889338
Loss in iteration 306 : 0.8335831381598554
Loss in iteration 307 : 1.093852505820808
Loss in iteration 308 : 0.8280837218222354
Loss in iteration 309 : 0.5536024547241245
Loss in iteration 310 : 0.5504845281918139
Loss in iteration 311 : 0.7742898114807131
Loss in iteration 312 : 1.0335448151066413
Loss in iteration 313 : 0.9827994325376941
Loss in iteration 314 : 0.6920305323967854
Loss in iteration 315 : 0.5449098544050528
Loss in iteration 316 : 0.5541616057041048
Loss in iteration 317 : 0.6228154727400955
Loss in iteration 318 : 0.8236174127795837
Loss in iteration 319 : 1.0715168447832126
Loss in iteration 320 : 1.086980197092588
Loss in iteration 321 : 0.76744375382443
Loss in iteration 322 : 0.5725163391033568
Loss in iteration 323 : 0.5327588803738241
Loss in iteration 324 : 0.61384314199802
Loss in iteration 325 : 0.8051723334341498
Loss in iteration 326 : 1.0986215522825822
Loss in iteration 327 : 1.0046586015291066
Loss in iteration 328 : 0.7661946670963782
Loss in iteration 329 : 0.6238899570161516
Loss in iteration 330 : 0.5817538468657375
Loss in iteration 331 : 0.588484004691066
Loss in iteration 332 : 0.5483326146200223
Loss in iteration 333 : 0.5669823818224149
Loss in iteration 334 : 0.6530760007089405
Loss in iteration 335 : 0.8563932262221805
Loss in iteration 336 : 1.212212871826599
Loss in iteration 337 : 1.314943131179625
Loss in iteration 338 : 0.9809814542070493
Loss in iteration 339 : 0.6763985504221435
Loss in iteration 340 : 0.5453367502064382
Loss in iteration 341 : 0.6270445306754738
Loss in iteration 342 : 0.8963209545465589
Loss in iteration 343 : 1.0152036093861594
Loss in iteration 344 : 0.7783919478018662
Loss in iteration 345 : 0.59410157273652
Loss in iteration 346 : 0.5133313251903254
Loss in iteration 347 : 0.5266909471301925
Loss in iteration 348 : 0.6201161328873077
Loss in iteration 349 : 0.9029272686602003
Loss in iteration 350 : 1.3536631248985358
Loss in iteration 351 : 1.2650037714296103
Loss in iteration 352 : 0.7830754266414045
Loss in iteration 353 : 0.5303646587288975
Loss in iteration 354 : 0.5393314064052381
Loss in iteration 355 : 0.6966055795712409
Loss in iteration 356 : 0.9083284536448667
Loss in iteration 357 : 0.9861452766821796
Loss in iteration 358 : 0.8373840559059724
Loss in iteration 359 : 0.699832783586245
Loss in iteration 360 : 0.6271643528355287
Loss in iteration 361 : 0.5368483444481271
Loss in iteration 362 : 0.5507796142970534
Loss in iteration 363 : 0.5134432027281361
Loss in iteration 364 : 0.5135270051228855
Loss in iteration 365 : 0.5862175639848284
Loss in iteration 366 : 0.8818158545159809
Loss in iteration 367 : 1.7034349340839872
Loss in iteration 368 : 1.472377462411717
Loss in iteration 369 : 0.74741877892024
Loss in iteration 370 : 0.5369059859310427
Loss in iteration 371 : 0.5619831484704433
Loss in iteration 372 : 0.6263967627849538
Loss in iteration 373 : 0.815132792899433
Loss in iteration 374 : 0.9323368301017574
Loss in iteration 375 : 0.7932569779302184
Loss in iteration 376 : 0.6525144841751453
Loss in iteration 377 : 0.5764214828926016
Loss in iteration 378 : 0.5601761528236343
Loss in iteration 379 : 0.5430126986194499
Loss in iteration 380 : 0.5477249344902432
Loss in iteration 381 : 0.6225105556537572
Loss in iteration 382 : 1.0075562653631487
Loss in iteration 383 : 1.4735756347601892
Loss in iteration 384 : 1.0964328910668169
Loss in iteration 385 : 0.7536730820199812
Loss in iteration 386 : 0.6284951194068364
Loss in iteration 387 : 0.6124497713297588
Loss in iteration 388 : 0.598840852073725
Loss in iteration 389 : 0.5634080756697263
Loss in iteration 390 : 0.576376739441348
Loss in iteration 391 : 0.631978277557032
Loss in iteration 392 : 0.7572427267030027
Loss in iteration 393 : 0.993178646895631
Loss in iteration 394 : 1.1466809073751287
Loss in iteration 395 : 0.9598790618920284
Loss in iteration 396 : 0.7510563185275503
Loss in iteration 397 : 0.6665711191664364
Loss in iteration 398 : 0.6268692701866099
Loss in iteration 399 : 0.6408997246988793
Loss in iteration 400 : 0.7101886018564962
Testing accuracy  of updater 1 on alg 0 with rate 4.0 = 0.67775, training accuracy 0.67775, time elapsed: 4946 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6869956625181269
Loss in iteration 3 : 0.6434624231379923
Loss in iteration 4 : 0.590492955096279
Loss in iteration 5 : 0.5499797112171739
Loss in iteration 6 : 0.5391701995936675
Loss in iteration 7 : 0.5128473579900138
Loss in iteration 8 : 0.5078104377647918
Loss in iteration 9 : 0.4973196948506767
Loss in iteration 10 : 0.49198911134886625
Loss in iteration 11 : 0.48577017175270987
Loss in iteration 12 : 0.48042010582083544
Loss in iteration 13 : 0.47767598961933727
Loss in iteration 14 : 0.4772441810781685
Loss in iteration 15 : 0.4945805366396503
Loss in iteration 16 : 0.4881805735191745
Loss in iteration 17 : 0.4789613425499327
Loss in iteration 18 : 0.4768543919330682
Loss in iteration 19 : 0.4855251355662274
Loss in iteration 20 : 0.47398806767334595
Loss in iteration 21 : 0.4941387552462347
Loss in iteration 22 : 0.488851860131881
Loss in iteration 23 : 0.48782405587143607
Loss in iteration 24 : 0.4798972511896624
Loss in iteration 25 : 0.48132993768844046
Loss in iteration 26 : 0.46474041971925095
Loss in iteration 27 : 0.4726949829841395
Loss in iteration 28 : 0.48057770477372086
Loss in iteration 29 : 0.48016312353970414
Loss in iteration 30 : 0.4820917193394404
Loss in iteration 31 : 0.4742245818449185
Loss in iteration 32 : 0.4710933893916907
Loss in iteration 33 : 0.4623550187548166
Loss in iteration 34 : 0.46146743789801836
Loss in iteration 35 : 0.47338758020036054
Loss in iteration 36 : 0.4752382970810637
Loss in iteration 37 : 0.46651456332081054
Loss in iteration 38 : 0.46687511923129066
Loss in iteration 39 : 0.46209769506536974
Loss in iteration 40 : 0.4679628751291482
Loss in iteration 41 : 0.4746607598633029
Loss in iteration 42 : 0.457256743865287
Loss in iteration 43 : 0.46494134971144874
Loss in iteration 44 : 0.4662683476115632
Loss in iteration 45 : 0.46914110005307347
Loss in iteration 46 : 0.47306878422835175
Loss in iteration 47 : 0.4609157418990839
Loss in iteration 48 : 0.4687376729171161
Loss in iteration 49 : 0.46388136230397836
Loss in iteration 50 : 0.4544168808683546
Loss in iteration 51 : 0.4697549815428439
Loss in iteration 52 : 0.46009091915922756
Loss in iteration 53 : 0.4619111693776749
Loss in iteration 54 : 0.4596226186091473
Loss in iteration 55 : 0.4624414999712268
Loss in iteration 56 : 0.46932824469404155
Loss in iteration 57 : 0.4510337928571189
Loss in iteration 58 : 0.45552166664888516
Loss in iteration 59 : 0.4526073325141489
Loss in iteration 60 : 0.4628782906210512
Loss in iteration 61 : 0.46067103366065415
Loss in iteration 62 : 0.46218711169499316
Loss in iteration 63 : 0.46144625060979166
Loss in iteration 64 : 0.46004383332304616
Loss in iteration 65 : 0.4714762460937914
Loss in iteration 66 : 0.45365452229971015
Loss in iteration 67 : 0.46671307380304466
Loss in iteration 68 : 0.45718834136524483
Loss in iteration 69 : 0.4647995272432615
Loss in iteration 70 : 0.4541354184527615
Loss in iteration 71 : 0.4687418482472608
Loss in iteration 72 : 0.454790237803832
Loss in iteration 73 : 0.4669991803500106
Loss in iteration 74 : 0.45769354043906846
Loss in iteration 75 : 0.45966386492838096
Loss in iteration 76 : 0.47260442282575554
Loss in iteration 77 : 0.4554971292478074
Loss in iteration 78 : 0.46546136793506737
Loss in iteration 79 : 0.4417874264021983
Loss in iteration 80 : 0.46080011085002814
Loss in iteration 81 : 0.46411453080883625
Loss in iteration 82 : 0.4598871386183232
Loss in iteration 83 : 0.46114772948410715
Loss in iteration 84 : 0.473778072732171
Loss in iteration 85 : 0.4623663266996702
Loss in iteration 86 : 0.4706381738884474
Loss in iteration 87 : 0.4500178430565906
Loss in iteration 88 : 0.46995750159248595
Loss in iteration 89 : 0.4537793183347631
Loss in iteration 90 : 0.45734400750632126
Loss in iteration 91 : 0.4657066038402452
Loss in iteration 92 : 0.45450591596218676
Loss in iteration 93 : 0.4592451585081848
Loss in iteration 94 : 0.4580120453642144
Loss in iteration 95 : 0.45783504752288895
Loss in iteration 96 : 0.4666582156662813
Loss in iteration 97 : 0.45961066504179543
Loss in iteration 98 : 0.4663318219963713
Loss in iteration 99 : 0.4572092317117853
Loss in iteration 100 : 0.4620615484138374
Loss in iteration 101 : 0.45793496034327197
Loss in iteration 102 : 0.45255806699843015
Loss in iteration 103 : 0.464618093269272
Loss in iteration 104 : 0.4601052481188845
Loss in iteration 105 : 0.4448239401180134
Loss in iteration 106 : 0.4608261461619447
Loss in iteration 107 : 0.47468145057143213
Loss in iteration 108 : 0.4775668853205912
Loss in iteration 109 : 0.45929470815480516
Loss in iteration 110 : 0.4536222977230069
Loss in iteration 111 : 0.4625776351997148
Loss in iteration 112 : 0.4547906274644636
Loss in iteration 113 : 0.45273847897196984
Loss in iteration 114 : 0.4541928262917691
Loss in iteration 115 : 0.455667423186158
Loss in iteration 116 : 0.4635103831472503
Loss in iteration 117 : 0.47072235505621624
Loss in iteration 118 : 0.46816805694681934
Loss in iteration 119 : 0.45350597386394065
Loss in iteration 120 : 0.47015043562719766
Loss in iteration 121 : 0.4641821150807919
Loss in iteration 122 : 0.4726273065780104
Loss in iteration 123 : 0.46616291576629304
Loss in iteration 124 : 0.45596175296106195
Loss in iteration 125 : 0.4593049964733475
Loss in iteration 126 : 0.45508196967056946
Loss in iteration 127 : 0.4578725501706722
Loss in iteration 128 : 0.4630214284380208
Loss in iteration 129 : 0.45862898670411806
Loss in iteration 130 : 0.4652498900022008
Loss in iteration 131 : 0.45719322342589763
Loss in iteration 132 : 0.45493777116198675
Loss in iteration 133 : 0.4630835311746095
Loss in iteration 134 : 0.4606419394929385
Loss in iteration 135 : 0.4607550899122657
Loss in iteration 136 : 0.45882510872786836
Loss in iteration 137 : 0.46690709517342605
Loss in iteration 138 : 0.46449911685655804
Loss in iteration 139 : 0.4710612616358773
Loss in iteration 140 : 0.4652327396834283
Loss in iteration 141 : 0.4681375239155531
Loss in iteration 142 : 0.45485366062951504
Loss in iteration 143 : 0.4635813901492174
Loss in iteration 144 : 0.4528657468467506
Loss in iteration 145 : 0.4490856246171953
Loss in iteration 146 : 0.46516027857359094
Loss in iteration 147 : 0.4694421744696398
Loss in iteration 148 : 0.4595219721147966
Loss in iteration 149 : 0.47301902866857454
Loss in iteration 150 : 0.463826041315176
Loss in iteration 151 : 0.45925539451881436
Loss in iteration 152 : 0.47120772928040344
Loss in iteration 153 : 0.4459624092606294
Loss in iteration 154 : 0.4675193102370193
Loss in iteration 155 : 0.46399013474188616
Loss in iteration 156 : 0.4475845083604844
Loss in iteration 157 : 0.46323122754389623
Loss in iteration 158 : 0.4575859937343515
Loss in iteration 159 : 0.4584768335041607
Loss in iteration 160 : 0.4564761645823514
Loss in iteration 161 : 0.4541894260299912
Loss in iteration 162 : 0.4597741867123037
Loss in iteration 163 : 0.4636832399298444
Loss in iteration 164 : 0.46307347997728665
Loss in iteration 165 : 0.4563092882311468
Loss in iteration 166 : 0.4583258943682203
Loss in iteration 167 : 0.46873668348755265
Loss in iteration 168 : 0.46877202820807645
Loss in iteration 169 : 0.45665952212558475
Loss in iteration 170 : 0.4632883812599332
Loss in iteration 171 : 0.4545395838991521
Loss in iteration 172 : 0.4643494374987483
Loss in iteration 173 : 0.45523998542631355
Loss in iteration 174 : 0.45353931254739926
Loss in iteration 175 : 0.4675244915290334
Loss in iteration 176 : 0.45350175105513474
Loss in iteration 177 : 0.46732698747631934
Loss in iteration 178 : 0.4571336754803355
Loss in iteration 179 : 0.46536714908956456
Loss in iteration 180 : 0.45328744621934003
Loss in iteration 181 : 0.4643654193483742
Loss in iteration 182 : 0.46102533829721265
Loss in iteration 183 : 0.4575713623936967
Loss in iteration 184 : 0.47866856391928575
Loss in iteration 185 : 0.45770580557851703
Loss in iteration 186 : 0.45216310351079664
Loss in iteration 187 : 0.45411417928965364
Loss in iteration 188 : 0.4737127350443726
Loss in iteration 189 : 0.4652169885579386
Loss in iteration 190 : 0.45181693890577884
Loss in iteration 191 : 0.4545114190680498
Loss in iteration 192 : 0.45928413486247843
Loss in iteration 193 : 0.4662108699071247
Loss in iteration 194 : 0.4615658732501071
Loss in iteration 195 : 0.4706098902355625
Loss in iteration 196 : 0.47498991889693876
Loss in iteration 197 : 0.4540756067536832
Loss in iteration 198 : 0.46914887322217164
Loss in iteration 199 : 0.4654772043763416
Loss in iteration 200 : 0.44957960130730557
Loss in iteration 201 : 0.4605639896724969
Loss in iteration 202 : 0.46939002418239933
Loss in iteration 203 : 0.4487785870340096
Loss in iteration 204 : 0.45792607385760314
Loss in iteration 205 : 0.4601123704989693
Loss in iteration 206 : 0.4590174004074163
Loss in iteration 207 : 0.4541650963806837
Loss in iteration 208 : 0.45063893934009336
Loss in iteration 209 : 0.4641961871169504
Loss in iteration 210 : 0.4729189213236236
Loss in iteration 211 : 0.4605161929060615
Loss in iteration 212 : 0.46867092278630634
Loss in iteration 213 : 0.47005285023141957
Loss in iteration 214 : 0.48258850506632284
Loss in iteration 215 : 0.47202619178504507
Loss in iteration 216 : 0.4581070119204988
Loss in iteration 217 : 0.46044315544227854
Loss in iteration 218 : 0.45601920209301294
Loss in iteration 219 : 0.47000410301176304
Loss in iteration 220 : 0.4666499702312384
Loss in iteration 221 : 0.4652547194054505
Loss in iteration 222 : 0.455502692523253
Loss in iteration 223 : 0.46897645684853506
Loss in iteration 224 : 0.47066489049412713
Loss in iteration 225 : 0.46186523238236393
Loss in iteration 226 : 0.46422497358205705
Loss in iteration 227 : 0.45188087691276435
Loss in iteration 228 : 0.46418059497616154
Loss in iteration 229 : 0.463751651958923
Loss in iteration 230 : 0.4546588940673732
Loss in iteration 231 : 0.4606181892872583
Loss in iteration 232 : 0.46602758398791055
Loss in iteration 233 : 0.4559182898465166
Loss in iteration 234 : 0.464823863457664
Loss in iteration 235 : 0.4562612169734772
Loss in iteration 236 : 0.46502604293456173
Loss in iteration 237 : 0.46142039310959554
Loss in iteration 238 : 0.45693659666679615
Loss in iteration 239 : 0.4651526881042156
Loss in iteration 240 : 0.45047253640473833
Loss in iteration 241 : 0.4609552279545449
Loss in iteration 242 : 0.4820795600469209
Loss in iteration 243 : 0.47335012054233766
Loss in iteration 244 : 0.45983337377030015
Loss in iteration 245 : 0.4782735093814593
Loss in iteration 246 : 0.4583565986350507
Loss in iteration 247 : 0.4549875184659475
Loss in iteration 248 : 0.4663781090197231
Loss in iteration 249 : 0.46486186476031494
Loss in iteration 250 : 0.45941221442966007
Loss in iteration 251 : 0.4655747052121528
Loss in iteration 252 : 0.4648462582828747
Loss in iteration 253 : 0.47124690241204775
Loss in iteration 254 : 0.4490487876154247
Loss in iteration 255 : 0.45203020243381753
Loss in iteration 256 : 0.47424414173421103
Loss in iteration 257 : 0.4508515945830632
Loss in iteration 258 : 0.456339252656014
Loss in iteration 259 : 0.45688131615545075
Loss in iteration 260 : 0.459427462066687
Loss in iteration 261 : 0.44846348558348226
Loss in iteration 262 : 0.46065299073063476
Loss in iteration 263 : 0.4601425734692846
Loss in iteration 264 : 0.47340566639595016
Loss in iteration 265 : 0.4503506290819856
Loss in iteration 266 : 0.4697807080027009
Loss in iteration 267 : 0.46584045306715094
Loss in iteration 268 : 0.4704298059021211
Loss in iteration 269 : 0.45335305317437957
Loss in iteration 270 : 0.45740026109881204
Loss in iteration 271 : 0.44582204957796945
Loss in iteration 272 : 0.4580808484852961
Loss in iteration 273 : 0.44604256856944086
Loss in iteration 274 : 0.45473021384545115
Loss in iteration 275 : 0.4639618344394953
Loss in iteration 276 : 0.46786092913972444
Loss in iteration 277 : 0.4540627714223254
Loss in iteration 278 : 0.4485069422415715
Loss in iteration 279 : 0.44806604595304833
Loss in iteration 280 : 0.45562706567318756
Loss in iteration 281 : 0.4613596324312637
Loss in iteration 282 : 0.45709736768415643
Loss in iteration 283 : 0.46147596220662634
Loss in iteration 284 : 0.44811977329041913
Loss in iteration 285 : 0.44758472428661955
Loss in iteration 286 : 0.4588215340228162
Loss in iteration 287 : 0.4647983278500173
Loss in iteration 288 : 0.4637089962645581
Loss in iteration 289 : 0.45632240824550163
Loss in iteration 290 : 0.45187058707939115
Loss in iteration 291 : 0.4639618138520377
Loss in iteration 292 : 0.46189741723567
Loss in iteration 293 : 0.45569813190408315
Loss in iteration 294 : 0.4650506417234652
Loss in iteration 295 : 0.45166198837296107
Loss in iteration 296 : 0.45502750262976804
Loss in iteration 297 : 0.4637474343012828
Loss in iteration 298 : 0.46001173887098207
Loss in iteration 299 : 0.4598582768502495
Loss in iteration 300 : 0.46733761539668284
Loss in iteration 301 : 0.4652484307536836
Loss in iteration 302 : 0.4564254107056131
Loss in iteration 303 : 0.45117691231067286
Loss in iteration 304 : 0.4536441566916316
Loss in iteration 305 : 0.4578053650670606
Loss in iteration 306 : 0.4597563030121975
Loss in iteration 307 : 0.4592543633485316
Loss in iteration 308 : 0.4585074170241353
Loss in iteration 309 : 0.450205352312303
Loss in iteration 310 : 0.4568721896374126
Loss in iteration 311 : 0.4670470523476015
Loss in iteration 312 : 0.47778035149849635
Loss in iteration 313 : 0.44726561396828524
Loss in iteration 314 : 0.4544409503167238
Loss in iteration 315 : 0.4572070421540417
Loss in iteration 316 : 0.4771388918547095
Loss in iteration 317 : 0.46137687143783496
Loss in iteration 318 : 0.4706672199375758
Loss in iteration 319 : 0.4539865096634691
Loss in iteration 320 : 0.45284790409421816
Loss in iteration 321 : 0.45401035717421506
Loss in iteration 322 : 0.4606618230732497
Loss in iteration 323 : 0.45752377544615713
Loss in iteration 324 : 0.456526928283779
Loss in iteration 325 : 0.4565831619868158
Loss in iteration 326 : 0.4711089408554077
Loss in iteration 327 : 0.44979046328022304
Loss in iteration 328 : 0.45246132550745616
Loss in iteration 329 : 0.4560305236906354
Loss in iteration 330 : 0.45028107293306396
Loss in iteration 331 : 0.4601542406033829
Loss in iteration 332 : 0.4460973220118064
Loss in iteration 333 : 0.44686858992306594
Loss in iteration 334 : 0.45662633801290664
Loss in iteration 335 : 0.4512094644052405
Loss in iteration 336 : 0.45747383318332885
Loss in iteration 337 : 0.46437922318474206
Loss in iteration 338 : 0.46768916467445765
Loss in iteration 339 : 0.4737301069128279
Loss in iteration 340 : 0.46360547989977957
Loss in iteration 341 : 0.4553248191056204
Loss in iteration 342 : 0.45745787556272566
Loss in iteration 343 : 0.45769721511546974
Loss in iteration 344 : 0.4630213157317659
Loss in iteration 345 : 0.45681591937226806
Loss in iteration 346 : 0.4501147664015417
Loss in iteration 347 : 0.45954446920887987
Loss in iteration 348 : 0.4614127176217963
Loss in iteration 349 : 0.46593037285233924
Loss in iteration 350 : 0.465527940073341
Loss in iteration 351 : 0.45970264580319237
Loss in iteration 352 : 0.4562302256266621
Loss in iteration 353 : 0.4511560360724773
Loss in iteration 354 : 0.4584339407289767
Loss in iteration 355 : 0.4597304887160073
Loss in iteration 356 : 0.45225467626567917
Loss in iteration 357 : 0.45637192434078855
Loss in iteration 358 : 0.4654351002311123
Loss in iteration 359 : 0.45529392025255744
Loss in iteration 360 : 0.4647846690269612
Loss in iteration 361 : 0.4501001478009148
Loss in iteration 362 : 0.46962859698255116
Loss in iteration 363 : 0.460599103617988
Loss in iteration 364 : 0.45802383798566976
Loss in iteration 365 : 0.4615618526761035
Loss in iteration 366 : 0.45558867179289214
Loss in iteration 367 : 0.4657030828774043
Loss in iteration 368 : 0.46670298503800095
Loss in iteration 369 : 0.472656250981739
Loss in iteration 370 : 0.4570989014269817
Loss in iteration 371 : 0.47551000700990004
Loss in iteration 372 : 0.45590330688652925
Loss in iteration 373 : 0.4592926788865644
Loss in iteration 374 : 0.4602323740427628
Loss in iteration 375 : 0.45522801863686263
Loss in iteration 376 : 0.4656671132794565
Loss in iteration 377 : 0.4596916750984395
Loss in iteration 378 : 0.46320243263001537
Loss in iteration 379 : 0.4625404434299537
Loss in iteration 380 : 0.45782861128870644
Loss in iteration 381 : 0.4472321474494103
Loss in iteration 382 : 0.4547291212775366
Loss in iteration 383 : 0.4611192342051987
Loss in iteration 384 : 0.46668142401466406
Loss in iteration 385 : 0.4597296048861737
Loss in iteration 386 : 0.447052362213437
Loss in iteration 387 : 0.4648955298749413
Loss in iteration 388 : 0.46926727818324115
Loss in iteration 389 : 0.4581469589743203
Loss in iteration 390 : 0.4571982596111437
Loss in iteration 391 : 0.46106946489765455
Loss in iteration 392 : 0.44972748499652093
Loss in iteration 393 : 0.45427282640008576
Loss in iteration 394 : 0.456680306404671
Loss in iteration 395 : 0.46503083256721084
Loss in iteration 396 : 0.456612824553378
Loss in iteration 397 : 0.47262649743609003
Loss in iteration 398 : 0.4676029013130587
Loss in iteration 399 : 0.45439077840700365
Loss in iteration 400 : 0.45024729101182637
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.788, training accuracy 0.788, time elapsed: 5993 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6796051606376952
Loss in iteration 3 : 0.6378339370777507
Loss in iteration 4 : 0.6089916340268651
Loss in iteration 5 : 0.5766736709320749
Loss in iteration 6 : 0.5592243931144082
Loss in iteration 7 : 0.5368104067400701
Loss in iteration 8 : 0.5232020138469708
Loss in iteration 9 : 0.5103147277426894
Loss in iteration 10 : 0.4980654786783305
Loss in iteration 11 : 0.491522455498356
Loss in iteration 12 : 0.4820781470400177
Loss in iteration 13 : 0.4787961900984728
Loss in iteration 14 : 0.4766777215032128
Loss in iteration 15 : 0.4899264937017568
Loss in iteration 16 : 0.4844171001708029
Loss in iteration 17 : 0.4764818414583482
Loss in iteration 18 : 0.4756652592293838
Loss in iteration 19 : 0.4820244053027667
Loss in iteration 20 : 0.4735158045528646
Loss in iteration 21 : 0.4936608801004988
Loss in iteration 22 : 0.4885031774377646
Loss in iteration 23 : 0.48684815086510447
Loss in iteration 24 : 0.47956575756495173
Loss in iteration 25 : 0.4826704284981419
Loss in iteration 26 : 0.46642279614117244
Loss in iteration 27 : 0.4731566337639341
Loss in iteration 28 : 0.48045336835858504
Loss in iteration 29 : 0.4816739508787312
Loss in iteration 30 : 0.48303197029235706
Loss in iteration 31 : 0.4763330681341305
Loss in iteration 32 : 0.4740927895683798
Loss in iteration 33 : 0.46597383605071363
Loss in iteration 34 : 0.4643530059987209
Loss in iteration 35 : 0.47722847126737106
Loss in iteration 36 : 0.4763911871033503
Loss in iteration 37 : 0.467927973636924
Loss in iteration 38 : 0.46964356079935504
Loss in iteration 39 : 0.4599077941316945
Loss in iteration 40 : 0.4690921161111038
Loss in iteration 41 : 0.4727856586728337
Loss in iteration 42 : 0.4582215344072499
Loss in iteration 43 : 0.4670560106519655
Loss in iteration 44 : 0.46642916194296347
Loss in iteration 45 : 0.4695791497655092
Loss in iteration 46 : 0.4696884085671064
Loss in iteration 47 : 0.4604496567209687
Loss in iteration 48 : 0.4667033782358146
Loss in iteration 49 : 0.462705642880413
Loss in iteration 50 : 0.45252061762314605
Loss in iteration 51 : 0.4700986457977499
Loss in iteration 52 : 0.4599033746188494
Loss in iteration 53 : 0.46138650532040043
Loss in iteration 54 : 0.45846254800005937
Loss in iteration 55 : 0.4624487649014209
Loss in iteration 56 : 0.4690463635291213
Loss in iteration 57 : 0.45105886180564164
Loss in iteration 58 : 0.4546566700753443
Loss in iteration 59 : 0.45324334413995687
Loss in iteration 60 : 0.4638495609725839
Loss in iteration 61 : 0.46037013705447266
Loss in iteration 62 : 0.462993077676167
Loss in iteration 63 : 0.46327107769799647
Loss in iteration 64 : 0.4603124206171927
Loss in iteration 65 : 0.47171900013508056
Loss in iteration 66 : 0.45510376403890934
Loss in iteration 67 : 0.46713842338948725
Loss in iteration 68 : 0.4590095344433227
Loss in iteration 69 : 0.4644855128483829
Loss in iteration 70 : 0.4548112369109308
Loss in iteration 71 : 0.4690668189750022
Loss in iteration 72 : 0.45552421434946266
Loss in iteration 73 : 0.46771891206836197
Loss in iteration 74 : 0.4580456248561245
Loss in iteration 75 : 0.45984513569938
Loss in iteration 76 : 0.4728171052236381
Loss in iteration 77 : 0.455768135870579
Loss in iteration 78 : 0.46460304104327627
Loss in iteration 79 : 0.44233476193600807
Loss in iteration 80 : 0.46210986006332516
Loss in iteration 81 : 0.4634640364668439
Loss in iteration 82 : 0.46128309855246774
Loss in iteration 83 : 0.46141112506127185
Loss in iteration 84 : 0.47433332732424455
Loss in iteration 85 : 0.46151982773676287
Loss in iteration 86 : 0.47192842190539575
Loss in iteration 87 : 0.4499751436662879
Loss in iteration 88 : 0.4713385042888567
Loss in iteration 89 : 0.45480637946881836
Loss in iteration 90 : 0.457704663603201
Loss in iteration 91 : 0.46622655006587616
Loss in iteration 92 : 0.4551425684152841
Loss in iteration 93 : 0.46034449375209585
Loss in iteration 94 : 0.4588426138422518
Loss in iteration 95 : 0.458710805027399
Loss in iteration 96 : 0.4672458350260643
Loss in iteration 97 : 0.4607172821642588
Loss in iteration 98 : 0.46654298944918476
Loss in iteration 99 : 0.4581708615685641
Loss in iteration 100 : 0.4628452781014857
Loss in iteration 101 : 0.4581865706988709
Loss in iteration 102 : 0.4525861318472151
Loss in iteration 103 : 0.46571931694618346
Loss in iteration 104 : 0.46062979785653246
Loss in iteration 105 : 0.44582182650293445
Loss in iteration 106 : 0.46059552388833785
Loss in iteration 107 : 0.4763026650714688
Loss in iteration 108 : 0.47602495887044405
Loss in iteration 109 : 0.45977982652542404
Loss in iteration 110 : 0.4531196689470056
Loss in iteration 111 : 0.46350253816418807
Loss in iteration 112 : 0.4583851865785052
Loss in iteration 113 : 0.4512334316146019
Loss in iteration 114 : 0.4583561412013406
Loss in iteration 115 : 0.4511596316925082
Loss in iteration 116 : 0.46458389437030306
Loss in iteration 117 : 0.47178769581831304
Loss in iteration 118 : 0.46776494921437745
Loss in iteration 119 : 0.45606284703316563
Loss in iteration 120 : 0.47361646140479546
Loss in iteration 121 : 0.46466908516816197
Loss in iteration 122 : 0.4773149775987259
Loss in iteration 123 : 0.46316476927971245
Loss in iteration 124 : 0.45732247767721085
Loss in iteration 125 : 0.4557015756194138
Loss in iteration 126 : 0.4558014177732031
Loss in iteration 127 : 0.458067755180586
Loss in iteration 128 : 0.4634633137297214
Loss in iteration 129 : 0.45913581394117914
Loss in iteration 130 : 0.4657880476036154
Loss in iteration 131 : 0.45671672030797933
Loss in iteration 132 : 0.45707826809754076
Loss in iteration 133 : 0.46534380899453
Loss in iteration 134 : 0.4616717821587104
Loss in iteration 135 : 0.46301895574834395
Loss in iteration 136 : 0.45933594774193515
Loss in iteration 137 : 0.4670278533536065
Loss in iteration 138 : 0.4643950601955896
Loss in iteration 139 : 0.4705888466489447
Loss in iteration 140 : 0.46466959952086573
Loss in iteration 141 : 0.46787231526433276
Loss in iteration 142 : 0.45459246441359735
Loss in iteration 143 : 0.46305813092800957
Loss in iteration 144 : 0.4537461651329484
Loss in iteration 145 : 0.4484364957526046
Loss in iteration 146 : 0.46581112171032707
Loss in iteration 147 : 0.4699922038105735
Loss in iteration 148 : 0.4593801596671611
Loss in iteration 149 : 0.4741106373107025
Loss in iteration 150 : 0.46542665601115135
Loss in iteration 151 : 0.4598426826489239
Loss in iteration 152 : 0.47248952881174044
Loss in iteration 153 : 0.4475459702067429
Loss in iteration 154 : 0.46790045525172724
Loss in iteration 155 : 0.46489226736987815
Loss in iteration 156 : 0.44752392867675983
Loss in iteration 157 : 0.46353768315044935
Loss in iteration 158 : 0.4587304995357202
Loss in iteration 159 : 0.46005732527807064
Loss in iteration 160 : 0.45710009239165467
Loss in iteration 161 : 0.4550113104375598
Loss in iteration 162 : 0.4610236840152197
Loss in iteration 163 : 0.4636228524183231
Loss in iteration 164 : 0.46536834845836084
Loss in iteration 165 : 0.45575294483901563
Loss in iteration 166 : 0.4591451041616032
Loss in iteration 167 : 0.4674413130091743
Loss in iteration 168 : 0.469870814142252
Loss in iteration 169 : 0.457028746250003
Loss in iteration 170 : 0.462887916325052
Loss in iteration 171 : 0.4539536192991065
Loss in iteration 172 : 0.4648448271801125
Loss in iteration 173 : 0.45622490822086675
Loss in iteration 174 : 0.45383798633390743
Loss in iteration 175 : 0.4680081007535898
Loss in iteration 176 : 0.45359353802868
Loss in iteration 177 : 0.4672452646377372
Loss in iteration 178 : 0.4573036932981837
Loss in iteration 179 : 0.46491430380753085
Loss in iteration 180 : 0.4542065793093377
Loss in iteration 181 : 0.4644182901259341
Loss in iteration 182 : 0.4596269005655311
Loss in iteration 183 : 0.4574162769403228
Loss in iteration 184 : 0.47548916854680445
Loss in iteration 185 : 0.45818949277975773
Loss in iteration 186 : 0.44872672212480313
Loss in iteration 187 : 0.4556942627856868
Loss in iteration 188 : 0.46688326320919415
Loss in iteration 189 : 0.4649261225841772
Loss in iteration 190 : 0.4483187017882248
Loss in iteration 191 : 0.4526610243444903
Loss in iteration 192 : 0.4599433079220735
Loss in iteration 193 : 0.4617053558085946
Loss in iteration 194 : 0.4605596590397643
Loss in iteration 195 : 0.4680838926127295
Loss in iteration 196 : 0.47452174694214794
Loss in iteration 197 : 0.45022954721868313
Loss in iteration 198 : 0.46872024404843354
Loss in iteration 199 : 0.4605568763153615
Loss in iteration 200 : 0.44927771620228185
Loss in iteration 201 : 0.45804645180613274
Loss in iteration 202 : 0.4688319583988615
Loss in iteration 203 : 0.44832968969443976
Loss in iteration 204 : 0.45628889012138557
Loss in iteration 205 : 0.45937457859016556
Loss in iteration 206 : 0.45892040405667567
Loss in iteration 207 : 0.45439161191933974
Loss in iteration 208 : 0.4501759479368654
Loss in iteration 209 : 0.4647602895471536
Loss in iteration 210 : 0.47057877446372776
Loss in iteration 211 : 0.4612003279522515
Loss in iteration 212 : 0.46671878924581306
Loss in iteration 213 : 0.4702302983784295
Loss in iteration 214 : 0.4811205363183978
Loss in iteration 215 : 0.4706350595315178
Loss in iteration 216 : 0.45730903995357514
Loss in iteration 217 : 0.45687428236913025
Loss in iteration 218 : 0.4543806307497609
Loss in iteration 219 : 0.4675809308377379
Loss in iteration 220 : 0.46647014979616463
Loss in iteration 221 : 0.4615309763958951
Loss in iteration 222 : 0.45525944070309776
Loss in iteration 223 : 0.4659290909155511
Loss in iteration 224 : 0.469685520112995
Loss in iteration 225 : 0.4603932909361375
Loss in iteration 226 : 0.4630957672720203
Loss in iteration 227 : 0.4520989325799826
Loss in iteration 228 : 0.46429212588280877
Loss in iteration 229 : 0.46291191654914327
Loss in iteration 230 : 0.4559642231422302
Loss in iteration 231 : 0.46076125685906405
Loss in iteration 232 : 0.4658157140741924
Loss in iteration 233 : 0.4589074854470443
Loss in iteration 234 : 0.46352646322560914
Loss in iteration 235 : 0.4588080064441629
Loss in iteration 236 : 0.46223726735721754
Loss in iteration 237 : 0.46146257823277265
Loss in iteration 238 : 0.46058524918580734
Loss in iteration 239 : 0.46073309817866226
Loss in iteration 240 : 0.4519153773732277
Loss in iteration 241 : 0.4550509322316784
Loss in iteration 242 : 0.48254090064320654
Loss in iteration 243 : 0.4686920021902125
Loss in iteration 244 : 0.4587429030369804
Loss in iteration 245 : 0.4723248419225646
Loss in iteration 246 : 0.4561470801225887
Loss in iteration 247 : 0.452126249608889
Loss in iteration 248 : 0.4626785885870141
Loss in iteration 249 : 0.464219576766654
Loss in iteration 250 : 0.4542747729518371
Loss in iteration 251 : 0.4655459053429504
Loss in iteration 252 : 0.460891663480944
Loss in iteration 253 : 0.4705905043987462
Loss in iteration 254 : 0.4469029325355096
Loss in iteration 255 : 0.45063678611161506
Loss in iteration 256 : 0.47391069652852824
Loss in iteration 257 : 0.44892803099517153
Loss in iteration 258 : 0.45678617937677024
Loss in iteration 259 : 0.45542853162395336
Loss in iteration 260 : 0.45908891180926514
Loss in iteration 261 : 0.44820014518206
Loss in iteration 262 : 0.46091004061520513
Loss in iteration 263 : 0.4593663978166343
Loss in iteration 264 : 0.47301165201155343
Loss in iteration 265 : 0.4496053684712195
Loss in iteration 266 : 0.4703123805560229
Loss in iteration 267 : 0.46516193206833534
Loss in iteration 268 : 0.47040192448409307
Loss in iteration 269 : 0.4530000101520257
Loss in iteration 270 : 0.4574019173302382
Loss in iteration 271 : 0.44648362561038235
Loss in iteration 272 : 0.4585540665692657
Loss in iteration 273 : 0.44578636114425074
Loss in iteration 274 : 0.4549815464736665
Loss in iteration 275 : 0.4640461790684725
Loss in iteration 276 : 0.46729489396514323
Loss in iteration 277 : 0.4535954462045456
Loss in iteration 278 : 0.44892510636668614
Loss in iteration 279 : 0.4481822174281015
Loss in iteration 280 : 0.4559615705958686
Loss in iteration 281 : 0.46205541046337417
Loss in iteration 282 : 0.4569889395162213
Loss in iteration 283 : 0.4629657768842008
Loss in iteration 284 : 0.4469823706682302
Loss in iteration 285 : 0.44772606685760824
Loss in iteration 286 : 0.4588325709557848
Loss in iteration 287 : 0.46473772824413356
Loss in iteration 288 : 0.46384159489035126
Loss in iteration 289 : 0.4552183015118072
Loss in iteration 290 : 0.4507681591022022
Loss in iteration 291 : 0.4646462350698655
Loss in iteration 292 : 0.46157520261005547
Loss in iteration 293 : 0.4552843917508807
Loss in iteration 294 : 0.46492484789533817
Loss in iteration 295 : 0.45168424822768877
Loss in iteration 296 : 0.45502069900403436
Loss in iteration 297 : 0.46341127162495727
Loss in iteration 298 : 0.45998048726730417
Loss in iteration 299 : 0.4602979875328251
Loss in iteration 300 : 0.46510270629543066
Loss in iteration 301 : 0.46418188291766793
Loss in iteration 302 : 0.45598633753341944
Loss in iteration 303 : 0.45143972024580065
Loss in iteration 304 : 0.4537899799783592
Loss in iteration 305 : 0.4575728900678745
Loss in iteration 306 : 0.45776540519324543
Loss in iteration 307 : 0.4584872650279364
Loss in iteration 308 : 0.45773309940308843
Loss in iteration 309 : 0.45046396899701496
Loss in iteration 310 : 0.457050666740498
Loss in iteration 311 : 0.46668785304318366
Loss in iteration 312 : 0.4780754052772866
Loss in iteration 313 : 0.44721775312172046
Loss in iteration 314 : 0.45420655011241856
Loss in iteration 315 : 0.45713208569436503
Loss in iteration 316 : 0.47705135653742814
Loss in iteration 317 : 0.46073539897778554
Loss in iteration 318 : 0.4706652668776039
Loss in iteration 319 : 0.4548045760089171
Loss in iteration 320 : 0.4532230124889097
Loss in iteration 321 : 0.4556654901301892
Loss in iteration 322 : 0.45945176408417654
Loss in iteration 323 : 0.456267401378541
Loss in iteration 324 : 0.45705653444939265
Loss in iteration 325 : 0.4560431560079956
Loss in iteration 326 : 0.47164320167178336
Loss in iteration 327 : 0.45032583026281897
Loss in iteration 328 : 0.4530708141810322
Loss in iteration 329 : 0.45505349240972837
Loss in iteration 330 : 0.4517419367366794
Loss in iteration 331 : 0.45999633017659497
Loss in iteration 332 : 0.4467116837735346
Loss in iteration 333 : 0.4476964913176909
Loss in iteration 334 : 0.45697899143883475
Loss in iteration 335 : 0.4516508611842568
Loss in iteration 336 : 0.4589897948373412
Loss in iteration 337 : 0.46274321737733104
Loss in iteration 338 : 0.4684601865467192
Loss in iteration 339 : 0.471182400265558
Loss in iteration 340 : 0.46340971300777184
Loss in iteration 341 : 0.45647575151640907
Loss in iteration 342 : 0.45755657438899083
Loss in iteration 343 : 0.4584670976419056
Loss in iteration 344 : 0.464641117932476
Loss in iteration 345 : 0.4573597941277742
Loss in iteration 346 : 0.4510082934998866
Loss in iteration 347 : 0.4597054105720732
Loss in iteration 348 : 0.460690987653785
Loss in iteration 349 : 0.4674546340645087
Loss in iteration 350 : 0.46602252468481165
Loss in iteration 351 : 0.4603606972872338
Loss in iteration 352 : 0.45663393358524906
Loss in iteration 353 : 0.45129548089352656
Loss in iteration 354 : 0.45858610623228113
Loss in iteration 355 : 0.4596285133112321
Loss in iteration 356 : 0.45254796289075194
Loss in iteration 357 : 0.45655412648606936
Loss in iteration 358 : 0.4658115500510413
Loss in iteration 359 : 0.4562693495087709
Loss in iteration 360 : 0.4645856657337996
Loss in iteration 361 : 0.4492660380840717
Loss in iteration 362 : 0.469833359592047
Loss in iteration 363 : 0.4607758527179498
Loss in iteration 364 : 0.4581871991460018
Loss in iteration 365 : 0.46269407648619654
Loss in iteration 366 : 0.45546700053505473
Loss in iteration 367 : 0.466339597174718
Loss in iteration 368 : 0.47029357675921935
Loss in iteration 369 : 0.47041559680795003
Loss in iteration 370 : 0.4600156178447251
Loss in iteration 371 : 0.4734850947285536
Loss in iteration 372 : 0.4552221650040461
Loss in iteration 373 : 0.46219463666800337
Loss in iteration 374 : 0.45619924904812215
Loss in iteration 375 : 0.45508663789838955
Loss in iteration 376 : 0.4634763480040853
Loss in iteration 377 : 0.459395185391886
Loss in iteration 378 : 0.46293365125582403
Loss in iteration 379 : 0.46289357555165256
Loss in iteration 380 : 0.4575446456328853
Loss in iteration 381 : 0.4486516006503065
Loss in iteration 382 : 0.45428895598212815
Loss in iteration 383 : 0.4639160351863926
Loss in iteration 384 : 0.4670497209283699
Loss in iteration 385 : 0.4606674500794376
Loss in iteration 386 : 0.4478699570789544
Loss in iteration 387 : 0.4645853655369783
Loss in iteration 388 : 0.47070926225226617
Loss in iteration 389 : 0.4558390414621075
Loss in iteration 390 : 0.4566470066304623
Loss in iteration 391 : 0.46009622650487714
Loss in iteration 392 : 0.4497898156992388
Loss in iteration 393 : 0.4539396685938957
Loss in iteration 394 : 0.4570377340845364
Loss in iteration 395 : 0.4640494662755364
Loss in iteration 396 : 0.456984117253911
Loss in iteration 397 : 0.4722389896608231
Loss in iteration 398 : 0.46761413905287175
Loss in iteration 399 : 0.4546815324147173
Loss in iteration 400 : 0.450210422110937
Testing accuracy  of updater 1 on alg 0 with rate 0.7 = 0.78825, training accuracy 0.78825, time elapsed: 5130 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.679809075633425
Loss in iteration 3 : 0.6583210122381107
Loss in iteration 4 : 0.6384642404758656
Loss in iteration 5 : 0.6124547949519722
Loss in iteration 6 : 0.599194790486855
Loss in iteration 7 : 0.575879676128495
Loss in iteration 8 : 0.5596383465220712
Loss in iteration 9 : 0.5487018332757865
Loss in iteration 10 : 0.5297219536610429
Loss in iteration 11 : 0.5188942279540824
Loss in iteration 12 : 0.5107341076935867
Loss in iteration 13 : 0.49757244601471873
Loss in iteration 14 : 0.4936939521767556
Loss in iteration 15 : 0.5015286464174573
Loss in iteration 16 : 0.49161910981010876
Loss in iteration 17 : 0.48589446104724054
Loss in iteration 18 : 0.48395154664906864
Loss in iteration 19 : 0.4874654857679587
Loss in iteration 20 : 0.4778930312332375
Loss in iteration 21 : 0.49519021351537285
Loss in iteration 22 : 0.4908916840938225
Loss in iteration 23 : 0.48880832822446346
Loss in iteration 24 : 0.4806808105646893
Loss in iteration 25 : 0.4855820055123865
Loss in iteration 26 : 0.47122732460923095
Loss in iteration 27 : 0.4754015524853147
Loss in iteration 28 : 0.48073068199772817
Loss in iteration 29 : 0.48631312508999147
Loss in iteration 30 : 0.4849296845633814
Loss in iteration 31 : 0.47960784148825714
Loss in iteration 32 : 0.47823336729245897
Loss in iteration 33 : 0.47118340849713586
Loss in iteration 34 : 0.4710791331927188
Loss in iteration 35 : 0.48232262971390666
Loss in iteration 36 : 0.4816762544323358
Loss in iteration 37 : 0.47360933823088347
Loss in iteration 38 : 0.47388079532890465
Loss in iteration 39 : 0.46504325442188427
Loss in iteration 40 : 0.47356926200479144
Loss in iteration 41 : 0.4778036087734703
Loss in iteration 42 : 0.46454994157876994
Loss in iteration 43 : 0.4731766871639193
Loss in iteration 44 : 0.472783229944133
Loss in iteration 45 : 0.47373123359732855
Loss in iteration 46 : 0.47311447604834533
Loss in iteration 47 : 0.4664942992924289
Loss in iteration 48 : 0.47130364215326975
Loss in iteration 49 : 0.46563386390244
Loss in iteration 50 : 0.45652427244227417
Loss in iteration 51 : 0.47435094156839946
Loss in iteration 52 : 0.46298627076339594
Loss in iteration 53 : 0.4637541235319079
Loss in iteration 54 : 0.46064853934539
Loss in iteration 55 : 0.46469159937693877
Loss in iteration 56 : 0.4705029751688681
Loss in iteration 57 : 0.45212424726123224
Loss in iteration 58 : 0.4571141546469525
Loss in iteration 59 : 0.45569231128933857
Loss in iteration 60 : 0.4650819948262982
Loss in iteration 61 : 0.4621081118132581
Loss in iteration 62 : 0.4650639740105829
Loss in iteration 63 : 0.46337605294311757
Loss in iteration 64 : 0.46164046966689637
Loss in iteration 65 : 0.47328695727182046
Loss in iteration 66 : 0.456508148813239
Loss in iteration 67 : 0.4683995988777155
Loss in iteration 68 : 0.45896713924630916
Loss in iteration 69 : 0.46625029305153587
Loss in iteration 70 : 0.45725461324292527
Loss in iteration 71 : 0.47144568015371274
Loss in iteration 72 : 0.4574637305404568
Loss in iteration 73 : 0.4697219747203832
Loss in iteration 74 : 0.45986488795620223
Loss in iteration 75 : 0.46147518470355703
Loss in iteration 76 : 0.4741583695200505
Loss in iteration 77 : 0.45584306815266745
Loss in iteration 78 : 0.4661641502668167
Loss in iteration 79 : 0.44552606408956186
Loss in iteration 80 : 0.4633844001025324
Loss in iteration 81 : 0.46497854294983165
Loss in iteration 82 : 0.4627169721747209
Loss in iteration 83 : 0.4633090600207844
Loss in iteration 84 : 0.47503967991787693
Loss in iteration 85 : 0.46303649508040073
Loss in iteration 86 : 0.472822463995941
Loss in iteration 87 : 0.4520910017314547
Loss in iteration 88 : 0.4722006755559181
Loss in iteration 89 : 0.4564299077982304
Loss in iteration 90 : 0.4594148194436268
Loss in iteration 91 : 0.467667861132788
Loss in iteration 92 : 0.45648395478909476
Loss in iteration 93 : 0.4619281224831563
Loss in iteration 94 : 0.46052566056499766
Loss in iteration 95 : 0.45964377059730377
Loss in iteration 96 : 0.46874780086532675
Loss in iteration 97 : 0.46102180585142716
Loss in iteration 98 : 0.4673883845281632
Loss in iteration 99 : 0.459582694335352
Loss in iteration 100 : 0.464584198959378
Loss in iteration 101 : 0.45912822627773264
Loss in iteration 102 : 0.45427802816746427
Loss in iteration 103 : 0.46695018198349975
Loss in iteration 104 : 0.46159100711488305
Loss in iteration 105 : 0.44736196660026145
Loss in iteration 106 : 0.46155758513391026
Loss in iteration 107 : 0.4768670525231456
Loss in iteration 108 : 0.4769818602506124
Loss in iteration 109 : 0.4608740058128293
Loss in iteration 110 : 0.4547064553210358
Loss in iteration 111 : 0.4649589882141962
Loss in iteration 112 : 0.4583848384541623
Loss in iteration 113 : 0.45282392974823754
Loss in iteration 114 : 0.455825640835056
Loss in iteration 115 : 0.452482982724316
Loss in iteration 116 : 0.46414775661202406
Loss in iteration 117 : 0.47101197424361224
Loss in iteration 118 : 0.470049050643904
Loss in iteration 119 : 0.4547451062158039
Loss in iteration 120 : 0.4715594228276766
Loss in iteration 121 : 0.46593722065683174
Loss in iteration 122 : 0.4738208619688802
Loss in iteration 123 : 0.464868620308715
Loss in iteration 124 : 0.4571012932225326
Loss in iteration 125 : 0.45654243427055696
Loss in iteration 126 : 0.4575921598925718
Loss in iteration 127 : 0.4584958989569845
Loss in iteration 128 : 0.46438945479017696
Loss in iteration 129 : 0.4603268085128242
Loss in iteration 130 : 0.4666275849092271
Loss in iteration 131 : 0.4589535974424016
Loss in iteration 132 : 0.4566339531404902
Loss in iteration 133 : 0.4664820244101126
Loss in iteration 134 : 0.46284699956173714
Loss in iteration 135 : 0.4622183732827481
Loss in iteration 136 : 0.4607022565092907
Loss in iteration 137 : 0.46751856219893145
Loss in iteration 138 : 0.46623154533615235
Loss in iteration 139 : 0.47088168976800626
Loss in iteration 140 : 0.46668955055801564
Loss in iteration 141 : 0.46782113497373323
Loss in iteration 142 : 0.4556346450029006
Loss in iteration 143 : 0.4630987363296019
Loss in iteration 144 : 0.45549773965115736
Loss in iteration 145 : 0.4508985169409618
Loss in iteration 146 : 0.4667309906014935
Loss in iteration 147 : 0.4704037418507959
Loss in iteration 148 : 0.4603111849508732
Loss in iteration 149 : 0.47493385172955394
Loss in iteration 150 : 0.4653440168866143
Loss in iteration 151 : 0.4608523850913357
Loss in iteration 152 : 0.4720284343339852
Loss in iteration 153 : 0.44775867234535377
Loss in iteration 154 : 0.4699148878394372
Loss in iteration 155 : 0.46536676566633917
Loss in iteration 156 : 0.4479354750339257
Loss in iteration 157 : 0.4662394132048889
Loss in iteration 158 : 0.45788158235090165
Loss in iteration 159 : 0.45927542137120075
Loss in iteration 160 : 0.459093084714687
Loss in iteration 161 : 0.4571650116077696
Loss in iteration 162 : 0.4599924406697936
Loss in iteration 163 : 0.4649955770460394
Loss in iteration 164 : 0.464016689459948
Loss in iteration 165 : 0.4578548854975814
Loss in iteration 166 : 0.459666088380842
Loss in iteration 167 : 0.4695065030520048
Loss in iteration 168 : 0.471087081034261
Loss in iteration 169 : 0.4584466953774927
Loss in iteration 170 : 0.4644478447114543
Loss in iteration 171 : 0.455189504828695
Loss in iteration 172 : 0.4651700770987292
Loss in iteration 173 : 0.4569614058763343
Loss in iteration 174 : 0.4547096358715887
Loss in iteration 175 : 0.46958908840395924
Loss in iteration 176 : 0.4546795235555842
Loss in iteration 177 : 0.4680911472223369
Loss in iteration 178 : 0.4577477342434965
Loss in iteration 179 : 0.4662634314828137
Loss in iteration 180 : 0.4547186059853365
Loss in iteration 181 : 0.46539270521249443
Loss in iteration 182 : 0.45970121143928544
Loss in iteration 183 : 0.4582329341429009
Loss in iteration 184 : 0.47566724851308
Loss in iteration 185 : 0.4591083729744428
Loss in iteration 186 : 0.4498354905992167
Loss in iteration 187 : 0.4559292481576216
Loss in iteration 188 : 0.46799749233419763
Loss in iteration 189 : 0.4653449176063927
Loss in iteration 190 : 0.44877037693024624
Loss in iteration 191 : 0.45401941754330155
Loss in iteration 192 : 0.459486027447284
Loss in iteration 193 : 0.4626084868517508
Loss in iteration 194 : 0.4617657679268392
Loss in iteration 195 : 0.46924216844718697
Loss in iteration 196 : 0.4752416507039145
Loss in iteration 197 : 0.45143688192611015
Loss in iteration 198 : 0.47043043087944286
Loss in iteration 199 : 0.4606590980825171
Loss in iteration 200 : 0.45067883044897833
Loss in iteration 201 : 0.45833722101564883
Loss in iteration 202 : 0.4700561996478418
Loss in iteration 203 : 0.4494605269875831
Loss in iteration 204 : 0.4566011513537796
Loss in iteration 205 : 0.4613338497394699
Loss in iteration 206 : 0.45951231863542585
Loss in iteration 207 : 0.4547912473390073
Loss in iteration 208 : 0.4511872715768231
Loss in iteration 209 : 0.4659756742279845
Loss in iteration 210 : 0.47088909651791794
Loss in iteration 211 : 0.46274392626354793
Loss in iteration 212 : 0.4684716975046708
Loss in iteration 213 : 0.46843933419010775
Loss in iteration 214 : 0.48149822071832526
Loss in iteration 215 : 0.46998440878597103
Loss in iteration 216 : 0.45859444649044306
Loss in iteration 217 : 0.4580552252021212
Loss in iteration 218 : 0.4557271781966338
Loss in iteration 219 : 0.4684309990016166
Loss in iteration 220 : 0.4679008773320588
Loss in iteration 221 : 0.4637353905475991
Loss in iteration 222 : 0.45471491868692965
Loss in iteration 223 : 0.46630839884628833
Loss in iteration 224 : 0.46890624208701936
Loss in iteration 225 : 0.4593557269343925
Loss in iteration 226 : 0.46324369909058005
Loss in iteration 227 : 0.4521096221968613
Loss in iteration 228 : 0.4645362798752711
Loss in iteration 229 : 0.46384811948893334
Loss in iteration 230 : 0.45565350184294773
Loss in iteration 231 : 0.46240654743619747
Loss in iteration 232 : 0.46634241622594924
Loss in iteration 233 : 0.45867577499099127
Loss in iteration 234 : 0.4650065062609409
Loss in iteration 235 : 0.45662926552594646
Loss in iteration 236 : 0.46265052739431756
Loss in iteration 237 : 0.4626607989734243
Loss in iteration 238 : 0.4588217113401759
Loss in iteration 239 : 0.46145075553533355
Loss in iteration 240 : 0.45152972609688985
Loss in iteration 241 : 0.456519491357668
Loss in iteration 242 : 0.4831912079971258
Loss in iteration 243 : 0.4695216992055401
Loss in iteration 244 : 0.4605437800663582
Loss in iteration 245 : 0.47318972932343234
Loss in iteration 246 : 0.45718329083174497
Loss in iteration 247 : 0.45278655442080545
Loss in iteration 248 : 0.4631163719809669
Loss in iteration 249 : 0.4650062827935805
Loss in iteration 250 : 0.454462948321413
Loss in iteration 251 : 0.46635950619480426
Loss in iteration 252 : 0.4610647654782827
Loss in iteration 253 : 0.47087239562444483
Loss in iteration 254 : 0.4471871572855838
Loss in iteration 255 : 0.4516740920048576
Loss in iteration 256 : 0.4748682713865382
Loss in iteration 257 : 0.4500538223536184
Loss in iteration 258 : 0.4574052241886348
Loss in iteration 259 : 0.45636252081925877
Loss in iteration 260 : 0.45927245775442965
Loss in iteration 261 : 0.4478923589171261
Loss in iteration 262 : 0.4617807294306658
Loss in iteration 263 : 0.46040631536193966
Loss in iteration 264 : 0.4727049658921148
Loss in iteration 265 : 0.45054883965342274
Loss in iteration 266 : 0.47003260191474194
Loss in iteration 267 : 0.46668742069054975
Loss in iteration 268 : 0.47154024556960844
Loss in iteration 269 : 0.4537518247350791
Loss in iteration 270 : 0.45864491232052057
Loss in iteration 271 : 0.44751752726541916
Loss in iteration 272 : 0.4597461345871646
Loss in iteration 273 : 0.44650443704526005
Loss in iteration 274 : 0.4558835230077805
Loss in iteration 275 : 0.4653142988743904
Loss in iteration 276 : 0.46840225168010424
Loss in iteration 277 : 0.45380792102630185
Loss in iteration 278 : 0.45034310586828746
Loss in iteration 279 : 0.44896432322934665
Loss in iteration 280 : 0.4558443420985261
Loss in iteration 281 : 0.46334644360781513
Loss in iteration 282 : 0.4579820950736091
Loss in iteration 283 : 0.4621979677149473
Loss in iteration 284 : 0.44736787965007596
Loss in iteration 285 : 0.4494759249384376
Loss in iteration 286 : 0.459718439981962
Loss in iteration 287 : 0.4651126109840963
Loss in iteration 288 : 0.46406798632821283
Loss in iteration 289 : 0.45719275264399156
Loss in iteration 290 : 0.4505355878952769
Loss in iteration 291 : 0.46548342028812384
Loss in iteration 292 : 0.4609532461669045
Loss in iteration 293 : 0.45647827315314377
Loss in iteration 294 : 0.46576149913220194
Loss in iteration 295 : 0.45189206372230123
Loss in iteration 296 : 0.45614722271925934
Loss in iteration 297 : 0.4641011858480123
Loss in iteration 298 : 0.4604136799902741
Loss in iteration 299 : 0.4621511874136945
Loss in iteration 300 : 0.4649880404795093
Loss in iteration 301 : 0.4640061181721259
Loss in iteration 302 : 0.4571399055312705
Loss in iteration 303 : 0.4514110081758117
Loss in iteration 304 : 0.4543178271674043
Loss in iteration 305 : 0.45771904495696136
Loss in iteration 306 : 0.45894443892808806
Loss in iteration 307 : 0.4591082133673099
Loss in iteration 308 : 0.45898681358361293
Loss in iteration 309 : 0.4506344010305568
Loss in iteration 310 : 0.45758017199619333
Loss in iteration 311 : 0.4667586809137945
Loss in iteration 312 : 0.4777338391561893
Loss in iteration 313 : 0.4475254213766749
Loss in iteration 314 : 0.45472615738416183
Loss in iteration 315 : 0.4577681561962807
Loss in iteration 316 : 0.4770135615123044
Loss in iteration 317 : 0.46055064731738793
Loss in iteration 318 : 0.4705723771772335
Loss in iteration 319 : 0.4548360785418525
Loss in iteration 320 : 0.45372528489140035
Loss in iteration 321 : 0.4537630298125568
Loss in iteration 322 : 0.4597839173716542
Loss in iteration 323 : 0.4562312744816406
Loss in iteration 324 : 0.45711317221523995
Loss in iteration 325 : 0.4568623014202472
Loss in iteration 326 : 0.4730284177628678
Loss in iteration 327 : 0.45075092066222083
Loss in iteration 328 : 0.45315007641605365
Loss in iteration 329 : 0.4571422276553472
Loss in iteration 330 : 0.4507044179349656
Loss in iteration 331 : 0.4599870295187249
Loss in iteration 332 : 0.4465309448002622
Loss in iteration 333 : 0.4474861397523391
Loss in iteration 334 : 0.4569941694380002
Loss in iteration 335 : 0.4513564147663801
Loss in iteration 336 : 0.4578946285697666
Loss in iteration 337 : 0.4632331271602597
Loss in iteration 338 : 0.46765819407564435
Loss in iteration 339 : 0.47167618669946143
Loss in iteration 340 : 0.46374723014612285
Loss in iteration 341 : 0.4565081372864211
Loss in iteration 342 : 0.45815708818131334
Loss in iteration 343 : 0.45795682633502816
Loss in iteration 344 : 0.46439791356306875
Loss in iteration 345 : 0.4582596540052331
Loss in iteration 346 : 0.45098625165784845
Loss in iteration 347 : 0.4594632351640172
Loss in iteration 348 : 0.4618293002935053
Loss in iteration 349 : 0.4654030668547646
Loss in iteration 350 : 0.46591752489825133
Loss in iteration 351 : 0.46115674029567244
Loss in iteration 352 : 0.45747146911418834
Loss in iteration 353 : 0.45128700738903327
Loss in iteration 354 : 0.45966202226094466
Loss in iteration 355 : 0.45970945775568794
Loss in iteration 356 : 0.4523164338558024
Loss in iteration 357 : 0.4567002703506419
Loss in iteration 358 : 0.4671605243606796
Loss in iteration 359 : 0.45623546682318006
Loss in iteration 360 : 0.46467475379361306
Loss in iteration 361 : 0.4492049918460596
Loss in iteration 362 : 0.4697980391612536
Loss in iteration 363 : 0.4606305604493525
Loss in iteration 364 : 0.4590428397950412
Loss in iteration 365 : 0.4619358984069061
Loss in iteration 366 : 0.45607520667853196
Loss in iteration 367 : 0.46667618542816824
Loss in iteration 368 : 0.4667937248490724
Loss in iteration 369 : 0.4700944236598753
Loss in iteration 370 : 0.45567829455597875
Loss in iteration 371 : 0.4733527699106338
Loss in iteration 372 : 0.45392846349479515
Loss in iteration 373 : 0.45972916033884964
Loss in iteration 374 : 0.45656526958059307
Loss in iteration 375 : 0.45453866592060865
Loss in iteration 376 : 0.46385471291644764
Loss in iteration 377 : 0.4597036988328706
Loss in iteration 378 : 0.4625179007681795
Loss in iteration 379 : 0.4625494804052614
Loss in iteration 380 : 0.4571906229279146
Loss in iteration 381 : 0.4477265042146738
Loss in iteration 382 : 0.4544460079095865
Loss in iteration 383 : 0.4621174101398435
Loss in iteration 384 : 0.46722758216069704
Loss in iteration 385 : 0.4601898208193081
Loss in iteration 386 : 0.4476221990322021
Loss in iteration 387 : 0.465104542886134
Loss in iteration 388 : 0.469244126428443
Loss in iteration 389 : 0.4559462239190028
Loss in iteration 390 : 0.4572225448058091
Loss in iteration 391 : 0.4600335901392792
Loss in iteration 392 : 0.44992019168466885
Loss in iteration 393 : 0.4536738151859474
Loss in iteration 394 : 0.4569813809201571
Loss in iteration 395 : 0.4642380929646288
Loss in iteration 396 : 0.45764682543663643
Loss in iteration 397 : 0.47162014620959236
Loss in iteration 398 : 0.46760379004785513
Loss in iteration 399 : 0.4557170769426866
Loss in iteration 400 : 0.4500196804505158
Testing accuracy  of updater 1 on alg 0 with rate 0.4 = 0.789375, training accuracy 0.789375, time elapsed: 6125 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6883546246267902
Loss in iteration 3 : 0.6792497366984082
Loss in iteration 4 : 0.6756193677264165
Loss in iteration 5 : 0.6665783147711362
Loss in iteration 6 : 0.6606919352242426
Loss in iteration 7 : 0.6470967487250376
Loss in iteration 8 : 0.6360186059300698
Loss in iteration 9 : 0.6267159824708426
Loss in iteration 10 : 0.6185963939720195
Loss in iteration 11 : 0.606620481311534
Loss in iteration 12 : 0.6000946382538722
Loss in iteration 13 : 0.5900766978494454
Loss in iteration 14 : 0.581072691016831
Loss in iteration 15 : 0.5771374550950394
Loss in iteration 16 : 0.5677503711504134
Loss in iteration 17 : 0.5593157248881461
Loss in iteration 18 : 0.5541421282871284
Loss in iteration 19 : 0.5517155987127088
Loss in iteration 20 : 0.5432685467534971
Loss in iteration 21 : 0.5455497354393639
Loss in iteration 22 : 0.5406620116995943
Loss in iteration 23 : 0.5375175651422631
Loss in iteration 24 : 0.5275151286243049
Loss in iteration 25 : 0.5286028810387652
Loss in iteration 26 : 0.5181644567562943
Loss in iteration 27 : 0.5176254293761752
Loss in iteration 28 : 0.5179208864102767
Loss in iteration 29 : 0.522004229863567
Loss in iteration 30 : 0.5198572041970803
Loss in iteration 31 : 0.5131533084308209
Loss in iteration 32 : 0.5121850535300854
Loss in iteration 33 : 0.5059613965847704
Loss in iteration 34 : 0.5049871657190611
Loss in iteration 35 : 0.5103903345020689
Loss in iteration 36 : 0.5081006724855494
Loss in iteration 37 : 0.5027902914653141
Loss in iteration 38 : 0.50018547349864
Loss in iteration 39 : 0.49644112331985574
Loss in iteration 40 : 0.4993211253592771
Loss in iteration 41 : 0.5018370443225318
Loss in iteration 42 : 0.49192232426287646
Loss in iteration 43 : 0.4995446990336686
Loss in iteration 44 : 0.49820342463863165
Loss in iteration 45 : 0.4971416517870941
Loss in iteration 46 : 0.4963842493572141
Loss in iteration 47 : 0.49380805787095744
Loss in iteration 48 : 0.4970648285382763
Loss in iteration 49 : 0.4899495106282089
Loss in iteration 50 : 0.48513488302521673
Loss in iteration 51 : 0.49729030553579867
Loss in iteration 52 : 0.4872976011361815
Loss in iteration 53 : 0.48704995567329956
Loss in iteration 54 : 0.4842717205035989
Loss in iteration 55 : 0.487301263687467
Loss in iteration 56 : 0.490750836072258
Loss in iteration 57 : 0.4762064455996386
Loss in iteration 58 : 0.4807582282617057
Loss in iteration 59 : 0.48058626494034973
Loss in iteration 60 : 0.4851194101783912
Loss in iteration 61 : 0.4848937401687563
Loss in iteration 62 : 0.48563492297458716
Loss in iteration 63 : 0.482323574300029
Loss in iteration 64 : 0.4821543876360152
Loss in iteration 65 : 0.4913758914899241
Loss in iteration 66 : 0.4765134228924597
Loss in iteration 67 : 0.48445481316423994
Loss in iteration 68 : 0.4762356160048409
Loss in iteration 69 : 0.48567520453291607
Loss in iteration 70 : 0.47748037443418256
Loss in iteration 71 : 0.4910554506606533
Loss in iteration 72 : 0.47748088082398193
Loss in iteration 73 : 0.4849093044069733
Loss in iteration 74 : 0.47954680656921855
Loss in iteration 75 : 0.47994989899121315
Loss in iteration 76 : 0.48866027775274645
Loss in iteration 77 : 0.47493342907377606
Loss in iteration 78 : 0.4821400420370872
Loss in iteration 79 : 0.4662692276143297
Loss in iteration 80 : 0.4791708436311172
Loss in iteration 81 : 0.48210683870692267
Loss in iteration 82 : 0.4780006234775899
Loss in iteration 83 : 0.4809652657084569
Loss in iteration 84 : 0.4877706212498191
Loss in iteration 85 : 0.4799576807483888
Loss in iteration 86 : 0.4858008548473248
Loss in iteration 87 : 0.4693457023076179
Loss in iteration 88 : 0.48753301788005127
Loss in iteration 89 : 0.47207527133914673
Loss in iteration 90 : 0.4747756655186185
Loss in iteration 91 : 0.4826232140575785
Loss in iteration 92 : 0.4714694581541409
Loss in iteration 93 : 0.4762322775471356
Loss in iteration 94 : 0.47628610566592483
Loss in iteration 95 : 0.4721096303118717
Loss in iteration 96 : 0.4845901803149783
Loss in iteration 97 : 0.4745482284000934
Loss in iteration 98 : 0.47884407552595526
Loss in iteration 99 : 0.47469226264580106
Loss in iteration 100 : 0.4793708425286707
Loss in iteration 101 : 0.4720418361101077
Loss in iteration 102 : 0.4712893780626225
Loss in iteration 103 : 0.48040037767720717
Loss in iteration 104 : 0.4756535243288536
Loss in iteration 105 : 0.4631643090496505
Loss in iteration 106 : 0.4737885465423481
Loss in iteration 107 : 0.4838516926229051
Loss in iteration 108 : 0.4878997592744182
Loss in iteration 109 : 0.47336781523988153
Loss in iteration 110 : 0.4683103842976434
Loss in iteration 111 : 0.4777192532348698
Loss in iteration 112 : 0.4705086383804029
Loss in iteration 113 : 0.46622575101208
Loss in iteration 114 : 0.46727799442749196
Loss in iteration 115 : 0.4654367588035509
Loss in iteration 116 : 0.47352994235108486
Loss in iteration 117 : 0.48183131963260367
Loss in iteration 118 : 0.4799052710675745
Loss in iteration 119 : 0.465146971794988
Loss in iteration 120 : 0.48071182098346577
Loss in iteration 121 : 0.4771998955902134
Loss in iteration 122 : 0.48163458399014397
Loss in iteration 123 : 0.47440706056090143
Loss in iteration 124 : 0.46829183192164026
Loss in iteration 125 : 0.46735144117362587
Loss in iteration 126 : 0.46719404699879635
Loss in iteration 127 : 0.46870703862958635
Loss in iteration 128 : 0.4743803782533266
Loss in iteration 129 : 0.4686708137446756
Loss in iteration 130 : 0.4733171158101505
Loss in iteration 131 : 0.4689180123848109
Loss in iteration 132 : 0.4680497230212064
Loss in iteration 133 : 0.47341424767424556
Loss in iteration 134 : 0.4728138309418859
Loss in iteration 135 : 0.4708003077298489
Loss in iteration 136 : 0.4708963294177988
Loss in iteration 137 : 0.47401727039108804
Loss in iteration 138 : 0.4742304651850295
Loss in iteration 139 : 0.478964788244235
Loss in iteration 140 : 0.4741263538366586
Loss in iteration 141 : 0.475149886494146
Loss in iteration 142 : 0.4666670948044377
Loss in iteration 143 : 0.4712339550490256
Loss in iteration 144 : 0.46655658625708984
Loss in iteration 145 : 0.4612768789379595
Loss in iteration 146 : 0.4740072745914463
Loss in iteration 147 : 0.4787356966138604
Loss in iteration 148 : 0.4698573143504381
Loss in iteration 149 : 0.48199135967725937
Loss in iteration 150 : 0.474239780929014
Loss in iteration 151 : 0.46820675426424135
Loss in iteration 152 : 0.47848918547134933
Loss in iteration 153 : 0.4577114773014714
Loss in iteration 154 : 0.47711316236951823
Loss in iteration 155 : 0.4732816323272918
Loss in iteration 156 : 0.4574948651821874
Loss in iteration 157 : 0.4716545042316934
Loss in iteration 158 : 0.46373124053067233
Loss in iteration 159 : 0.4663127350077368
Loss in iteration 160 : 0.46566536973564254
Loss in iteration 161 : 0.4656720464659508
Loss in iteration 162 : 0.46788480217499406
Loss in iteration 163 : 0.47208063534005423
Loss in iteration 164 : 0.471482435720865
Loss in iteration 165 : 0.4642488676608333
Loss in iteration 166 : 0.4672636854972738
Loss in iteration 167 : 0.47654124729663405
Loss in iteration 168 : 0.4765944309194794
Loss in iteration 169 : 0.46727975104881203
Loss in iteration 170 : 0.473110988692931
Loss in iteration 171 : 0.46287183241293384
Loss in iteration 172 : 0.4702531293077568
Loss in iteration 173 : 0.4663125894439445
Loss in iteration 174 : 0.46215631409087127
Loss in iteration 175 : 0.47621198287398675
Loss in iteration 176 : 0.4630414929617959
Loss in iteration 177 : 0.47510386212583744
Loss in iteration 178 : 0.46441462542828144
Loss in iteration 179 : 0.47346483361782205
Loss in iteration 180 : 0.46229673264853466
Loss in iteration 181 : 0.4700519117540621
Loss in iteration 182 : 0.4662638360542482
Loss in iteration 183 : 0.4655140225742681
Loss in iteration 184 : 0.4808264480261243
Loss in iteration 185 : 0.46669067975806333
Loss in iteration 186 : 0.4586190976852429
Loss in iteration 187 : 0.4616209417297017
Loss in iteration 188 : 0.4728665274742902
Loss in iteration 189 : 0.4720222859916896
Loss in iteration 190 : 0.45774291823749647
Loss in iteration 191 : 0.46110962274040423
Loss in iteration 192 : 0.4651292137396912
Loss in iteration 193 : 0.4683880312416216
Loss in iteration 194 : 0.46753740025928703
Loss in iteration 195 : 0.47528111571914605
Loss in iteration 196 : 0.4810368599265286
Loss in iteration 197 : 0.4576263178186477
Loss in iteration 198 : 0.4755067461877751
Loss in iteration 199 : 0.46580914579138977
Loss in iteration 200 : 0.458582265148658
Loss in iteration 201 : 0.4659921694694583
Loss in iteration 202 : 0.47428661026417085
Loss in iteration 203 : 0.4568060878329237
Loss in iteration 204 : 0.46201018062181975
Loss in iteration 205 : 0.4665100136053503
Loss in iteration 206 : 0.46524849887926856
Loss in iteration 207 : 0.4620642548143367
Loss in iteration 208 : 0.4574650390551566
Loss in iteration 209 : 0.4705643688757532
Loss in iteration 210 : 0.47520160866766015
Loss in iteration 211 : 0.4688566356171279
Loss in iteration 212 : 0.47290364836629495
Loss in iteration 213 : 0.4733021649004766
Loss in iteration 214 : 0.48572120605688895
Loss in iteration 215 : 0.4751057716717993
Loss in iteration 216 : 0.46648558901874176
Loss in iteration 217 : 0.4643423134582996
Loss in iteration 218 : 0.4604068291752224
Loss in iteration 219 : 0.4718898347313767
Loss in iteration 220 : 0.4716112001065196
Loss in iteration 221 : 0.4695342629484471
Loss in iteration 222 : 0.4593363661613492
Loss in iteration 223 : 0.47211172167191456
Loss in iteration 224 : 0.47244777335029375
Loss in iteration 225 : 0.4649942667740995
Loss in iteration 226 : 0.4677533990528225
Loss in iteration 227 : 0.4575437838893565
Loss in iteration 228 : 0.4683780681195597
Loss in iteration 229 : 0.4686476288239481
Loss in iteration 230 : 0.460164926000735
Loss in iteration 231 : 0.46827755609555044
Loss in iteration 232 : 0.47012506977124213
Loss in iteration 233 : 0.463859133126768
Loss in iteration 234 : 0.4705672009290707
Loss in iteration 235 : 0.4608871680922162
Loss in iteration 236 : 0.46660475631056864
Loss in iteration 237 : 0.46680914299682325
Loss in iteration 238 : 0.4651948304195555
Loss in iteration 239 : 0.4649965722190472
Loss in iteration 240 : 0.45903072183684773
Loss in iteration 241 : 0.46068722030736703
Loss in iteration 242 : 0.4867820186547562
Loss in iteration 243 : 0.47446236997169144
Loss in iteration 244 : 0.46482781412388485
Loss in iteration 245 : 0.4771367148846079
Loss in iteration 246 : 0.46324657522451623
Loss in iteration 247 : 0.45828575554009526
Loss in iteration 248 : 0.4667502289846418
Loss in iteration 249 : 0.4700295071151994
Loss in iteration 250 : 0.4593903382203303
Loss in iteration 251 : 0.4706525921744343
Loss in iteration 252 : 0.464482678680037
Loss in iteration 253 : 0.4743972816159806
Loss in iteration 254 : 0.4520264036218256
Loss in iteration 255 : 0.4575652579189485
Loss in iteration 256 : 0.479087297827739
Loss in iteration 257 : 0.4562401690906754
Loss in iteration 258 : 0.461555513973943
Loss in iteration 259 : 0.4608580526472194
Loss in iteration 260 : 0.46388150889070673
Loss in iteration 261 : 0.4528460834102896
Loss in iteration 262 : 0.4644188976933456
Loss in iteration 263 : 0.46499553656200565
Loss in iteration 264 : 0.4779526038116427
Loss in iteration 265 : 0.45570057333106795
Loss in iteration 266 : 0.47467849069562484
Loss in iteration 267 : 0.4703350243592427
Loss in iteration 268 : 0.47541428433795213
Loss in iteration 269 : 0.45949279116478237
Loss in iteration 270 : 0.46503932410878335
Loss in iteration 271 : 0.452844759659098
Loss in iteration 272 : 0.4650971636491248
Loss in iteration 273 : 0.450423227018734
Loss in iteration 274 : 0.459516491964784
Loss in iteration 275 : 0.46828998267354505
Loss in iteration 276 : 0.4709758120888166
Loss in iteration 277 : 0.4581052745245775
Loss in iteration 278 : 0.4561777936685415
Loss in iteration 279 : 0.45422515122462065
Loss in iteration 280 : 0.4600674386295592
Loss in iteration 281 : 0.4671516042496721
Loss in iteration 282 : 0.4607167403023118
Loss in iteration 283 : 0.4669808841371249
Loss in iteration 284 : 0.45235329817048486
Loss in iteration 285 : 0.45389054863271555
Loss in iteration 286 : 0.4639154313946555
Loss in iteration 287 : 0.46830271854981165
Loss in iteration 288 : 0.46701089481020713
Loss in iteration 289 : 0.45929560874216274
Loss in iteration 290 : 0.4553918319899003
Loss in iteration 291 : 0.46840705983402864
Loss in iteration 292 : 0.4652808562813888
Loss in iteration 293 : 0.45980102037902
Loss in iteration 294 : 0.46991515389567223
Loss in iteration 295 : 0.45609638850374806
Loss in iteration 296 : 0.45891713908444914
Loss in iteration 297 : 0.46761583187182115
Loss in iteration 298 : 0.4642025816500562
Loss in iteration 299 : 0.46581999080349906
Loss in iteration 300 : 0.46845190817311483
Loss in iteration 301 : 0.4669476124827861
Loss in iteration 302 : 0.461682266207222
Loss in iteration 303 : 0.4559059939718692
Loss in iteration 304 : 0.45871864758202063
Loss in iteration 305 : 0.46079378420043804
Loss in iteration 306 : 0.46161248499979857
Loss in iteration 307 : 0.4627728529972714
Loss in iteration 308 : 0.4646312682391571
Loss in iteration 309 : 0.4552129775778689
Loss in iteration 310 : 0.4612259009462961
Loss in iteration 311 : 0.4704254579501577
Loss in iteration 312 : 0.480375440315344
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.7905, training accuracy 0.7905, time elapsed: 4308 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 4.072659931801297
Loss in iteration 3 : 44.62594104286722
Loss in iteration 4 : 1.1621854474937485
Loss in iteration 5 : 17.4573175609415
Loss in iteration 6 : 9.625788153036254
Loss in iteration 7 : 20.87228237314168
Loss in iteration 8 : 4.25374341268313
Loss in iteration 9 : 9.280235346385881
Loss in iteration 10 : 14.28898206729473
Loss in iteration 11 : 6.021825563629056
Loss in iteration 12 : 6.707836024217204
Loss in iteration 13 : 6.415079441643856
Loss in iteration 14 : 5.776922994047107
Loss in iteration 15 : 4.730021142814573
Loss in iteration 16 : 4.418805569085578
Loss in iteration 17 : 3.8156861366213373
Loss in iteration 18 : 3.601718006487755
Loss in iteration 19 : 3.5453060637079727
Loss in iteration 20 : 3.353449835099347
Loss in iteration 21 : 3.5367446300418925
Loss in iteration 22 : 3.351744884842539
Loss in iteration 23 : 3.296125087583807
Loss in iteration 24 : 3.2046383201870743
Loss in iteration 25 : 3.0229686549473693
Loss in iteration 26 : 2.769198575577636
Loss in iteration 27 : 2.8762794559381164
Loss in iteration 28 : 3.1702249832864235
Loss in iteration 29 : 3.59759393868444
Loss in iteration 30 : 6.1729436865483684
Loss in iteration 31 : 7.0930733561990404
Loss in iteration 32 : 10.396003156424916
Loss in iteration 33 : 7.068448706582562
Loss in iteration 34 : 9.861086140880055
Loss in iteration 35 : 6.643837523453059
Loss in iteration 36 : 7.816858736381544
Loss in iteration 37 : 7.484641801868379
Loss in iteration 38 : 7.344040744784229
Loss in iteration 39 : 5.3866461519410995
Loss in iteration 40 : 5.956010485049625
Loss in iteration 41 : 5.921653907502343
Loss in iteration 42 : 5.828648557515569
Loss in iteration 43 : 5.264809589019462
Loss in iteration 44 : 5.70701815021432
Loss in iteration 45 : 5.040834717413202
Loss in iteration 46 : 4.816175809360959
Loss in iteration 47 : 4.283818148251159
Loss in iteration 48 : 4.8589638847476815
Loss in iteration 49 : 4.858714585761182
Loss in iteration 50 : 5.441479113674607
Loss in iteration 51 : 4.800898780986481
Loss in iteration 52 : 5.264530613528082
Loss in iteration 53 : 4.715101136065134
Loss in iteration 54 : 5.413133421952073
Loss in iteration 55 : 4.9599974667877875
Loss in iteration 56 : 6.321832792042341
Loss in iteration 57 : 5.24263752836191
Loss in iteration 58 : 5.7429214749144855
Loss in iteration 59 : 4.996508444449826
Loss in iteration 60 : 6.614426711327822
Loss in iteration 61 : 5.215871905381816
Loss in iteration 62 : 5.860236870696557
Loss in iteration 63 : 5.09158967380802
Loss in iteration 64 : 5.716124481803201
Loss in iteration 65 : 5.264599179764665
Loss in iteration 66 : 5.480093098722813
Loss in iteration 67 : 5.179223097789606
Loss in iteration 68 : 5.55066535074288
Loss in iteration 69 : 5.315363619570196
Loss in iteration 70 : 5.763735170970392
Loss in iteration 71 : 4.94116173253416
Loss in iteration 72 : 5.189419398685256
Loss in iteration 73 : 4.94436273470356
Loss in iteration 74 : 5.336941041935369
Loss in iteration 75 : 5.575804673254517
Loss in iteration 76 : 6.504737630397075
Loss in iteration 77 : 5.480357237125282
Loss in iteration 78 : 6.57279275699764
Loss in iteration 79 : 5.505705610694903
Loss in iteration 80 : 5.9353734083955185
Loss in iteration 81 : 5.261050338439826
Loss in iteration 82 : 5.877675807839174
Loss in iteration 83 : 5.126152259209269
Loss in iteration 84 : 6.08799437309249
Loss in iteration 85 : 5.395779682933975
Loss in iteration 86 : 6.069082130311938
Loss in iteration 87 : 4.857996409953347
Loss in iteration 88 : 5.3978638097157585
Loss in iteration 89 : 4.866999345593665
Loss in iteration 90 : 5.497299937237605
Loss in iteration 91 : 5.15447240925275
Loss in iteration 92 : 5.787718848032659
Loss in iteration 93 : 5.370133681297305
Loss in iteration 94 : 5.832562482334076
Loss in iteration 95 : 5.198667294010492
Loss in iteration 96 : 5.7782431179090885
Loss in iteration 97 : 5.074441883766732
Loss in iteration 98 : 5.514145986700991
Loss in iteration 99 : 4.847357412390742
Loss in iteration 100 : 5.530873756427342
Loss in iteration 101 : 5.307338852963834
Loss in iteration 102 : 5.980029261495328
Loss in iteration 103 : 5.015743268430233
Loss in iteration 104 : 5.460758103499821
Loss in iteration 105 : 4.940709006643124
Loss in iteration 106 : 5.635246974028009
Loss in iteration 107 : 5.030492099876879
Loss in iteration 108 : 5.373361888222441
Loss in iteration 109 : 5.043706734599961
Loss in iteration 110 : 5.911384404899954
Loss in iteration 111 : 5.314264132756616
Loss in iteration 112 : 6.429124892031684
Loss in iteration 113 : 5.9617181530284284
Loss in iteration 114 : 6.166819929271586
Loss in iteration 115 : 4.614470535684242
Loss in iteration 116 : 4.967070271118681
Loss in iteration 117 : 4.806484108769082
Loss in iteration 118 : 5.48064465406632
Loss in iteration 119 : 5.307396238314157
Loss in iteration 120 : 5.815496094761051
Loss in iteration 121 : 5.295123935340962
Loss in iteration 122 : 6.362327835855262
Loss in iteration 123 : 6.077308385682767
Loss in iteration 124 : 6.3502588891495835
Loss in iteration 125 : 5.209599914043452
Loss in iteration 126 : 5.820904807623448
Loss in iteration 127 : 4.608870960936632
Loss in iteration 128 : 4.840934606269589
Loss in iteration 129 : 4.939237861438045
Loss in iteration 130 : 5.7270393960426205
Loss in iteration 131 : 5.152065923734538
Loss in iteration 132 : 5.829211382190742
Loss in iteration 133 : 5.1567795248124915
Loss in iteration 134 : 5.249592613202652
Loss in iteration 135 : 4.595222025942548
Loss in iteration 136 : 5.10243104425375
Loss in iteration 137 : 4.897829304258841
Loss in iteration 138 : 5.376939248280951
Loss in iteration 139 : 4.896878895660623
Loss in iteration 140 : 5.606411052517919
Loss in iteration 141 : 5.819125593142233
Loss in iteration 142 : 6.501761386375511
Loss in iteration 143 : 5.218087112254561
Loss in iteration 144 : 5.690024647544687
Loss in iteration 145 : 6.102489409939778
Loss in iteration 146 : 7.353175556012337
Loss in iteration 147 : 6.286116837407182
Loss in iteration 148 : 6.244826212522069
Loss in iteration 149 : 5.301862667009089
Loss in iteration 150 : 5.752219591720733
Loss in iteration 151 : 5.135286213294387
Loss in iteration 152 : 5.629286260411897
Loss in iteration 153 : 4.840282533339376
Loss in iteration 154 : 5.410821018998481
Loss in iteration 155 : 4.80483321370446
Loss in iteration 156 : 4.73535743631138
Loss in iteration 157 : 4.44348529991239
Loss in iteration 158 : 4.6394597812580205
Loss in iteration 159 : 4.617993885924127
Loss in iteration 160 : 5.342040648400562
Loss in iteration 161 : 4.6547615199067645
Loss in iteration 162 : 6.159646381669039
Loss in iteration 163 : 6.035258772366549
Loss in iteration 164 : 6.924030666746956
Loss in iteration 165 : 5.105236624823529
Loss in iteration 166 : 5.850646669742746
Loss in iteration 167 : 6.048281732398946
Loss in iteration 168 : 7.1314653030946324
Loss in iteration 169 : 5.810050714523076
Loss in iteration 170 : 6.408595554665571
Loss in iteration 171 : 5.658023350175291
Loss in iteration 172 : 6.193429542700896
Loss in iteration 173 : 5.25322058176056
Loss in iteration 174 : 5.105042730180599
Loss in iteration 175 : 4.604722148986928
Loss in iteration 176 : 4.544157799899467
Loss in iteration 177 : 4.437452365019974
Loss in iteration 178 : 4.4608667726100455
Loss in iteration 179 : 4.3129779828122725
Loss in iteration 180 : 4.966281443623225
Loss in iteration 181 : 4.904646744941171
Loss in iteration 182 : 5.540257752285639
Loss in iteration 183 : 5.441597816020357
Loss in iteration 184 : 7.295121072563971
Loss in iteration 185 : 5.973474481823248
Loss in iteration 186 : 6.124217248855402
Loss in iteration 187 : 5.930109491822315
Loss in iteration 188 : 7.103883109745033
Loss in iteration 189 : 5.797411805956127
Loss in iteration 190 : 6.026576365705156
Loss in iteration 191 : 4.83832789535884
Loss in iteration 192 : 5.149331235728145
Loss in iteration 193 : 4.551620867102579
Loss in iteration 194 : 4.3952634599884375
Loss in iteration 195 : 4.540094590896027
Loss in iteration 196 : 5.509413911876856
Loss in iteration 197 : 4.485605222060193
Loss in iteration 198 : 5.189707036894013
Loss in iteration 199 : 5.742158757643111
Loss in iteration 200 : 6.292148535635071
Loss in iteration 201 : 4.952877887404437
Loss in iteration 202 : 5.76976066564451
Loss in iteration 203 : 4.983980299741085
Loss in iteration 204 : 5.572695501637058
Loss in iteration 205 : 4.649734837666225
Loss in iteration 206 : 5.412653745578428
Loss in iteration 207 : 4.915805774058243
Loss in iteration 208 : 5.542860114605898
Loss in iteration 209 : 5.4644030506658225
Loss in iteration 210 : 6.825192667439937
Loss in iteration 211 : 5.738306980348682
Loss in iteration 212 : 6.154554263210074
Loss in iteration 213 : 5.836317812495348
Loss in iteration 214 : 6.6910692524797835
Loss in iteration 215 : 5.56801901480207
Loss in iteration 216 : 5.3742923699437855
Loss in iteration 217 : 5.1884398139154895
Loss in iteration 218 : 5.687077803053997
Loss in iteration 219 : 5.411567965997716
Loss in iteration 220 : 5.527210699501868
Loss in iteration 221 : 5.033854529952466
Loss in iteration 222 : 5.529009043853873
Loss in iteration 223 : 4.642353625671406
Loss in iteration 224 : 4.757091750971609
Loss in iteration 225 : 4.5386296707630995
Loss in iteration 226 : 5.831441641206034
Loss in iteration 227 : 5.7343825224423215
Loss in iteration 228 : 6.553783791172593
Loss in iteration 229 : 5.367156365014092
Loss in iteration 230 : 5.653509762783572
Loss in iteration 231 : 5.150205495821353
Loss in iteration 232 : 5.851322177103678
Loss in iteration 233 : 4.4150440400946875
Loss in iteration 234 : 4.953735836770552
Loss in iteration 235 : 5.394396650788943
Loss in iteration 236 : 6.200071123677531
Loss in iteration 237 : 5.455978244750603
Loss in iteration 238 : 6.245672386100046
Loss in iteration 239 : 6.32329639314741
Loss in iteration 240 : 6.51213707162292
Loss in iteration 241 : 4.423006359068852
Loss in iteration 242 : 4.879546789475949
Loss in iteration 243 : 5.2142026097438405
Loss in iteration 244 : 5.812684816468749
Loss in iteration 245 : 5.218730999444682
Loss in iteration 246 : 5.55374266612078
Loss in iteration 247 : 5.267563718187331
Loss in iteration 248 : 6.068088192113089
Loss in iteration 249 : 5.462995509840627
Loss in iteration 250 : 6.063877437580081
Loss in iteration 251 : 5.359361552570971
Loss in iteration 252 : 5.7722328527155335
Loss in iteration 253 : 5.596113901377506
Loss in iteration 254 : 5.354716672185664
Loss in iteration 255 : 4.934648652427135
Loss in iteration 256 : 5.69834659950432
Loss in iteration 257 : 4.9121889981814935
Loss in iteration 258 : 5.441114569394798
Loss in iteration 259 : 4.773156912530952
Loss in iteration 260 : 5.271328684635521
Loss in iteration 261 : 4.940010392602486
Loss in iteration 262 : 6.101243664245212
Loss in iteration 263 : 5.043298084193409
Loss in iteration 264 : 5.808775482038324
Loss in iteration 265 : 5.083745781669504
Loss in iteration 266 : 5.939927182741245
Loss in iteration 267 : 5.721237328035352
Loss in iteration 268 : 6.392937483780415
Loss in iteration 269 : 5.1621604093852085
Loss in iteration 270 : 5.795835524927213
Loss in iteration 271 : 4.80273642696288
Loss in iteration 272 : 5.279742394132585
Loss in iteration 273 : 4.667354940748451
Loss in iteration 274 : 5.3902771634419775
Loss in iteration 275 : 4.773364327600703
Loss in iteration 276 : 5.136870744770437
Loss in iteration 277 : 4.1372505157102175
Loss in iteration 278 : 4.611942551669101
Loss in iteration 279 : 4.4691013113495845
Loss in iteration 280 : 5.256845852140828
Loss in iteration 281 : 5.425830402910007
Loss in iteration 282 : 6.532924238467508
Loss in iteration 283 : 5.348047314196946
Loss in iteration 284 : 5.471223197281426
Loss in iteration 285 : 5.134032772141516
Loss in iteration 286 : 6.008310326735893
Loss in iteration 287 : 5.494990777276399
Loss in iteration 288 : 5.963677821961692
Loss in iteration 289 : 5.628912345244474
Loss in iteration 290 : 6.295004858446939
Loss in iteration 291 : 5.6892020745274765
Loss in iteration 292 : 5.8113577441987445
Loss in iteration 293 : 4.729838279651257
Loss in iteration 294 : 5.151076265780284
Loss in iteration 295 : 4.896364793374234
Loss in iteration 296 : 5.041316935773651
Loss in iteration 297 : 4.571733348835132
Loss in iteration 298 : 4.74338452689085
Loss in iteration 299 : 4.931741566242609
Loss in iteration 300 : 5.5240070963025465
Loss in iteration 301 : 5.0001121402601365
Loss in iteration 302 : 5.543938827792143
Loss in iteration 303 : 4.940275454048402
Loss in iteration 304 : 5.561764374814833
Loss in iteration 305 : 5.731036455412628
Loss in iteration 306 : 6.794876081490741
Loss in iteration 307 : 5.851247127796819
Loss in iteration 308 : 5.736089992895344
Loss in iteration 309 : 4.964765641117915
Loss in iteration 310 : 5.572994253111784
Loss in iteration 311 : 4.91960816987717
Loss in iteration 312 : 5.41648065430507
Loss in iteration 313 : 4.941250306135932
Loss in iteration 314 : 5.285650843031869
Loss in iteration 315 : 4.891197006725501
Loss in iteration 316 : 6.044744091930955
Loss in iteration 317 : 5.44027291769282
Loss in iteration 318 : 5.770545586632866
Loss in iteration 319 : 5.3285743712958995
Loss in iteration 320 : 5.63886350380426
Loss in iteration 321 : 4.383915445056805
Loss in iteration 322 : 4.724388867865518
Loss in iteration 323 : 4.775901274249972
Loss in iteration 324 : 5.367360288133951
Loss in iteration 325 : 5.263510990581686
Loss in iteration 326 : 6.1576934642042955
Loss in iteration 327 : 5.222212281035334
Loss in iteration 328 : 6.02973218235297
Loss in iteration 329 : 4.960642916089363
Loss in iteration 330 : 5.397840265487601
Loss in iteration 331 : 4.859096281379192
Loss in iteration 332 : 5.220635417685942
Loss in iteration 333 : 5.144104761574835
Loss in iteration 334 : 5.6778293580434624
Loss in iteration 335 : 5.517678543611937
Loss in iteration 336 : 5.972021988695634
Loss in iteration 337 : 4.325118724290085
Loss in iteration 338 : 4.5235334402846705
Loss in iteration 339 : 4.683430383948388
Loss in iteration 340 : 5.781764839782254
Loss in iteration 341 : 5.713353714226825
Loss in iteration 342 : 6.200318230478767
Loss in iteration 343 : 5.3253096287211585
Loss in iteration 344 : 5.556999444540531
Loss in iteration 345 : 5.475679895791987
Loss in iteration 346 : 5.8758936883349095
Loss in iteration 347 : 5.232716199761621
Loss in iteration 348 : 5.568216024210877
Loss in iteration 349 : 4.655178489917965
Loss in iteration 350 : 4.9572402059186444
Loss in iteration 351 : 4.66825375398634
Loss in iteration 352 : 5.317390133725357
Loss in iteration 353 : 4.83514619928991
Loss in iteration 354 : 5.7094656028319575
Loss in iteration 355 : 5.493124002883893
Loss in iteration 356 : 6.116918670320551
Loss in iteration 357 : 5.37289917925819
Loss in iteration 358 : 6.549552175978799
Loss in iteration 359 : 6.423477867026663
Loss in iteration 360 : 6.413474604662776
Loss in iteration 361 : 4.8997808591884535
Loss in iteration 362 : 5.466267081287903
Loss in iteration 363 : 4.5485043872347894
Loss in iteration 364 : 5.002496482085086
Loss in iteration 365 : 5.3541497754014635
Loss in iteration 366 : 5.865753547596433
Loss in iteration 367 : 5.169705045131595
Loss in iteration 368 : 6.019288189001321
Loss in iteration 369 : 6.075340502909659
Loss in iteration 370 : 5.628058156680028
Loss in iteration 371 : 4.580251062469269
Loss in iteration 372 : 4.9995017229174
Loss in iteration 373 : 5.118011396998468
Loss in iteration 374 : 5.471152951686045
Loss in iteration 375 : 4.4659598839778925
Loss in iteration 376 : 5.101542785566831
Loss in iteration 377 : 5.342990673778016
Loss in iteration 378 : 6.166893319917885
Loss in iteration 379 : 5.543306979934056
Loss in iteration 380 : 5.806151464235902
Loss in iteration 381 : 5.5879290000093595
Loss in iteration 382 : 6.419890988660438
Loss in iteration 383 : 5.135439020177923
Loss in iteration 384 : 5.136894954267163
Loss in iteration 385 : 5.334813919356642
Loss in iteration 386 : 5.994131633750516
Loss in iteration 387 : 4.96156529708253
Loss in iteration 388 : 5.7036223529028955
Loss in iteration 389 : 5.384576401076397
Loss in iteration 390 : 5.863893425225374
Loss in iteration 391 : 5.6864552330787275
Loss in iteration 392 : 6.084301727126312
Loss in iteration 393 : 4.881989013067197
Loss in iteration 394 : 4.85495132784611
Loss in iteration 395 : 4.712371577712003
Loss in iteration 396 : 4.9733147266511235
Loss in iteration 397 : 5.164487721062556
Loss in iteration 398 : 5.872983727174735
Loss in iteration 399 : 5.67572739339086
Loss in iteration 400 : 5.978318012117063
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.764125, training accuracy 0.764125, time elapsed: 5428 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 2.8617955214917234
Loss in iteration 3 : 30.798170256545305
Loss in iteration 4 : 0.7712055063098554
Loss in iteration 5 : 7.341175089901772
Loss in iteration 6 : 14.44343721591586
Loss in iteration 7 : 9.562345100638982
Loss in iteration 8 : 6.979434116490861
Loss in iteration 9 : 9.985595176728111
Loss in iteration 10 : 5.487052206435429
Loss in iteration 11 : 4.372688147056242
Loss in iteration 12 : 4.9328794366754805
Loss in iteration 13 : 3.6579829944846725
Loss in iteration 14 : 3.3136620798512135
Loss in iteration 15 : 3.1078676749822254
Loss in iteration 16 : 2.949342300017031
Loss in iteration 17 : 2.6364797954992607
Loss in iteration 18 : 2.5617638631616844
Loss in iteration 19 : 2.584139529225788
Loss in iteration 20 : 2.4515291310410237
Loss in iteration 21 : 2.609805601350736
Loss in iteration 22 : 2.4752885101903566
Loss in iteration 23 : 2.4353779478601036
Loss in iteration 24 : 2.3635786788279756
Loss in iteration 25 : 2.223436134645359
Loss in iteration 26 : 2.0138121774895885
Loss in iteration 27 : 2.0286065311316333
Loss in iteration 28 : 2.0198642184223052
Loss in iteration 29 : 1.9212640096878302
Loss in iteration 30 : 2.0006016197798377
Loss in iteration 31 : 2.425362352188793
Loss in iteration 32 : 4.3232075231238465
Loss in iteration 33 : 5.819640819987667
Loss in iteration 34 : 8.116461613170562
Loss in iteration 35 : 4.45287217257455
Loss in iteration 36 : 6.222428288596634
Loss in iteration 37 : 6.020430779300766
Loss in iteration 38 : 5.705509336241884
Loss in iteration 39 : 4.349309350509685
Loss in iteration 40 : 5.105683325561386
Loss in iteration 41 : 4.972120671290303
Loss in iteration 42 : 4.576460972305669
Loss in iteration 43 : 4.026467372323992
Loss in iteration 44 : 4.238531185787597
Loss in iteration 45 : 3.7220425429734867
Loss in iteration 46 : 3.4312663309515865
Loss in iteration 47 : 3.0238884655849794
Loss in iteration 48 : 3.2635023892993784
Loss in iteration 49 : 3.30708741949613
Loss in iteration 50 : 3.505697045161581
Loss in iteration 51 : 3.2250631584161455
Loss in iteration 52 : 3.4291540380513963
Loss in iteration 53 : 3.0821497053738987
Loss in iteration 54 : 3.4279924363849408
Loss in iteration 55 : 3.274088527785611
Loss in iteration 56 : 4.217221837378876
Loss in iteration 57 : 3.60619445038155
Loss in iteration 58 : 4.021043756901121
Loss in iteration 59 : 3.5356731754940562
Loss in iteration 60 : 4.732878272336909
Loss in iteration 61 : 3.6700229655400753
Loss in iteration 62 : 4.2145423232601855
Loss in iteration 63 : 3.6541298475648514
Loss in iteration 64 : 4.134886025858661
Loss in iteration 65 : 3.7364005300988583
Loss in iteration 66 : 3.904437026275598
Loss in iteration 67 : 3.657475499101977
Loss in iteration 68 : 3.8858234884048404
Loss in iteration 69 : 3.7110757143707707
Loss in iteration 70 : 3.9824426602965755
Loss in iteration 71 : 3.434173299192185
Loss in iteration 72 : 3.5602773070837044
Loss in iteration 73 : 3.4339869612208402
Loss in iteration 74 : 3.647658863429761
Loss in iteration 75 : 3.8416666264127772
Loss in iteration 76 : 4.454081456736254
Loss in iteration 77 : 3.8306026052946307
Loss in iteration 78 : 4.567692616841843
Loss in iteration 79 : 3.85703473486181
Loss in iteration 80 : 4.153894247915838
Loss in iteration 81 : 3.6924859289967134
Loss in iteration 82 : 4.1241267365046435
Loss in iteration 83 : 3.6092037199904943
Loss in iteration 84 : 4.297020324174471
Loss in iteration 85 : 3.8221246261546318
Loss in iteration 86 : 4.287778864231924
Loss in iteration 87 : 3.448615201908238
Loss in iteration 88 : 3.82971661270061
Loss in iteration 89 : 3.4585353654343756
Loss in iteration 90 : 3.880726283287086
Loss in iteration 91 : 3.634046893706184
Loss in iteration 92 : 4.047665223448992
Loss in iteration 93 : 3.7525031560423368
Loss in iteration 94 : 4.035911310849377
Loss in iteration 95 : 3.6049074514104227
Loss in iteration 96 : 3.96234077886786
Loss in iteration 97 : 3.5070372616756185
Loss in iteration 98 : 3.8190329608191145
Loss in iteration 99 : 3.3876542498177447
Loss in iteration 100 : 3.862448814932684
Loss in iteration 101 : 3.7264373901640515
Loss in iteration 102 : 4.185436440326867
Loss in iteration 103 : 3.517291630025209
Loss in iteration 104 : 3.8352578003799507
Loss in iteration 105 : 3.4879844161883327
Loss in iteration 106 : 3.9736734393769226
Loss in iteration 107 : 3.544617984831957
Loss in iteration 108 : 3.772501844636522
Loss in iteration 109 : 3.5535038316517187
Loss in iteration 110 : 4.1285449631377595
Loss in iteration 111 : 3.7151956076839996
Loss in iteration 112 : 4.47285005973793
Loss in iteration 113 : 4.170269111085176
Loss in iteration 114 : 4.3119297694229495
Loss in iteration 115 : 3.241495022213594
Loss in iteration 116 : 3.467669264459399
Loss in iteration 117 : 3.3697578570267153
Loss in iteration 118 : 3.834209142418549
Loss in iteration 119 : 3.725301061495321
Loss in iteration 120 : 4.053337383511956
Loss in iteration 121 : 3.698438599312713
Loss in iteration 122 : 4.44944492678124
Loss in iteration 123 : 4.263762563423604
Loss in iteration 124 : 4.451295830793603
Loss in iteration 125 : 3.663507991562432
Loss in iteration 126 : 4.0614680577141264
Loss in iteration 127 : 3.2269163893654516
Loss in iteration 128 : 3.3905426432297596
Loss in iteration 129 : 3.4502459045234595
Loss in iteration 130 : 3.989033567411103
Loss in iteration 131 : 3.6075630338179168
Loss in iteration 132 : 4.065909475693634
Loss in iteration 133 : 3.6003533564642916
Loss in iteration 134 : 3.668283432981828
Loss in iteration 135 : 3.2247867687508047
Loss in iteration 136 : 3.5779063355956318
Loss in iteration 137 : 3.458975972988699
Loss in iteration 138 : 3.774647441834751
Loss in iteration 139 : 3.438786883383323
Loss in iteration 140 : 3.931590592928313
Loss in iteration 141 : 4.067791832005976
Loss in iteration 142 : 4.545100384045206
Loss in iteration 143 : 3.673451603967087
Loss in iteration 144 : 3.9594500011798233
Loss in iteration 145 : 4.244443887664185
Loss in iteration 146 : 5.11528408549458
Loss in iteration 147 : 4.415835660037853
Loss in iteration 148 : 4.362107489287262
Loss in iteration 149 : 3.711839876277309
Loss in iteration 150 : 4.016227916934602
Loss in iteration 151 : 3.5989235648042763
Loss in iteration 152 : 3.9399725941544883
Loss in iteration 153 : 3.3940519032867953
Loss in iteration 154 : 3.783938315770447
Loss in iteration 155 : 3.377839855365269
Loss in iteration 156 : 3.328858653836988
Loss in iteration 157 : 3.133746879091154
Loss in iteration 158 : 3.274087380028445
Loss in iteration 159 : 3.2508164910188806
Loss in iteration 160 : 3.730981177726577
Loss in iteration 161 : 3.2648073013078154
Loss in iteration 162 : 4.288162095353269
Loss in iteration 163 : 4.198651763467749
Loss in iteration 164 : 4.786281866124513
Loss in iteration 165 : 3.5980933941379942
Loss in iteration 166 : 4.104292966348963
Loss in iteration 167 : 4.226843247646972
Loss in iteration 168 : 4.958059731218775
Loss in iteration 169 : 4.096607451047167
Loss in iteration 170 : 4.500539955526889
Loss in iteration 171 : 3.9576048992234734
Loss in iteration 172 : 4.319201806794485
Loss in iteration 173 : 3.6797333035004964
Loss in iteration 174 : 3.58710812173808
Loss in iteration 175 : 3.236605787436504
Loss in iteration 176 : 3.1959981012191165
Loss in iteration 177 : 3.124336763114495
Loss in iteration 178 : 3.1449627110221403
Loss in iteration 179 : 3.032361030230928
Loss in iteration 180 : 3.4703096620772587
Loss in iteration 181 : 3.431881207697553
Loss in iteration 182 : 3.852948275259792
Loss in iteration 183 : 3.7979932002277965
Loss in iteration 184 : 5.0609862670313115
Loss in iteration 185 : 4.205800258203956
Loss in iteration 186 : 4.280798256957484
Loss in iteration 187 : 4.152738374973399
Loss in iteration 188 : 4.941156403170373
Loss in iteration 189 : 4.067549803423317
Loss in iteration 190 : 4.198339353372285
Loss in iteration 191 : 3.3773804010266977
Loss in iteration 192 : 3.586481041413833
Loss in iteration 193 : 3.1827190502512384
Loss in iteration 194 : 3.0711993204143977
Loss in iteration 195 : 3.1617476590642757
Loss in iteration 196 : 3.8292114722909494
Loss in iteration 197 : 3.171959094454008
Loss in iteration 198 : 3.6794607520887816
Loss in iteration 199 : 4.052026278147914
Loss in iteration 200 : 4.405853815853668
Loss in iteration 201 : 3.4904397445579036
Loss in iteration 202 : 4.038208584768466
Loss in iteration 203 : 3.496588286776815
Loss in iteration 204 : 3.8817968677612926
Loss in iteration 205 : 3.2506350645871662
Loss in iteration 206 : 3.758288271131826
Loss in iteration 207 : 3.4304875145181937
Loss in iteration 208 : 3.8353546507839873
Loss in iteration 209 : 3.7994412906153268
Loss in iteration 210 : 4.716495670176579
Loss in iteration 211 : 4.045230819024008
Loss in iteration 212 : 4.3418250399298675
Loss in iteration 213 : 4.105666991088446
Loss in iteration 214 : 4.699859252678292
Loss in iteration 215 : 3.9288138077482326
Loss in iteration 216 : 3.786717411721773
Loss in iteration 217 : 3.65553216794187
Loss in iteration 218 : 3.991014724069824
Loss in iteration 219 : 3.8089242326436428
Loss in iteration 220 : 3.8749309730677197
Loss in iteration 221 : 3.5455238988510946
Loss in iteration 222 : 3.8928340433879387
Loss in iteration 223 : 3.265450107813223
Loss in iteration 224 : 3.320760099088176
Loss in iteration 225 : 3.1653064208386095
Loss in iteration 226 : 4.019713062678718
Loss in iteration 227 : 3.9801699097047094
Loss in iteration 228 : 4.5360116198798766
Loss in iteration 229 : 3.7517171757376713
Loss in iteration 230 : 3.9366128134239378
Loss in iteration 231 : 3.6001225315290313
Loss in iteration 232 : 4.072069145743708
Loss in iteration 233 : 3.0981902136024813
Loss in iteration 234 : 3.4802267628717485
Loss in iteration 235 : 3.7757716477537495
Loss in iteration 236 : 4.331397803338125
Loss in iteration 237 : 3.8408916175818
Loss in iteration 238 : 4.373601285017297
Loss in iteration 239 : 4.4354299412494065
Loss in iteration 240 : 4.557835400406236
Loss in iteration 241 : 3.1335175965737614
Loss in iteration 242 : 3.4585486884183876
Loss in iteration 243 : 3.6857937239893315
Loss in iteration 244 : 4.064202179989805
Loss in iteration 245 : 3.6513854851880057
Loss in iteration 246 : 3.839091413882335
Loss in iteration 247 : 3.678005993551858
Loss in iteration 248 : 4.22872513922249
Loss in iteration 249 : 3.821964772787689
Loss in iteration 250 : 4.219603157087697
Loss in iteration 251 : 3.7503279424279548
Loss in iteration 252 : 4.021880159568721
Loss in iteration 253 : 3.9142694233824304
Loss in iteration 254 : 3.7410684713121083
Loss in iteration 255 : 3.4566389943023292
Loss in iteration 256 : 3.997210398514662
Loss in iteration 257 : 3.4579270692145667
Loss in iteration 258 : 3.8185625420292557
Loss in iteration 259 : 3.3571501745524586
Loss in iteration 260 : 3.6973737045016906
Loss in iteration 261 : 3.481700480136642
Loss in iteration 262 : 4.274105468578692
Loss in iteration 263 : 3.541964053406231
Loss in iteration 264 : 4.057794745273722
Loss in iteration 265 : 3.5630733454156496
Loss in iteration 266 : 4.143360368820733
Loss in iteration 267 : 4.00103048673595
Loss in iteration 268 : 4.43515537656934
Loss in iteration 269 : 3.6195782381217274
Loss in iteration 270 : 4.04165229649042
Loss in iteration 271 : 3.357760221497381
Loss in iteration 272 : 3.69366742975828
Loss in iteration 273 : 3.291584896156492
Loss in iteration 274 : 3.7742831998991324
Loss in iteration 275 : 3.3562835915450218
Loss in iteration 276 : 3.607560269874229
Loss in iteration 277 : 2.9108485451832866
Loss in iteration 278 : 3.227538706080882
Loss in iteration 279 : 3.125125754060512
Loss in iteration 280 : 3.65208049862316
Loss in iteration 281 : 3.7986841240743936
Loss in iteration 282 : 4.560059544352984
Loss in iteration 283 : 3.768302161471819
Loss in iteration 284 : 3.8409929957102933
Loss in iteration 285 : 3.6037331375392787
Loss in iteration 286 : 4.1746958042046005
Loss in iteration 287 : 3.8353851840002626
Loss in iteration 288 : 4.145671473323727
Loss in iteration 289 : 3.9293041526452974
Loss in iteration 290 : 4.394210334009716
Loss in iteration 291 : 3.9997724039704514
Loss in iteration 292 : 4.082865342932641
Loss in iteration 293 : 3.3304630236517614
Loss in iteration 294 : 3.6193978761874988
Loss in iteration 295 : 3.44181471496789
Loss in iteration 296 : 3.5448490148815055
Loss in iteration 297 : 3.215561234368615
Loss in iteration 298 : 3.3458886041606886
Loss in iteration 299 : 3.450473523882838
Loss in iteration 300 : 3.8168720040020747
Loss in iteration 301 : 3.4987454597373553
Loss in iteration 302 : 3.840846441038399
Loss in iteration 303 : 3.444394189567071
Loss in iteration 304 : 3.8715075879731238
Loss in iteration 305 : 3.9985373543405145
Loss in iteration 306 : 4.734104654415077
Loss in iteration 307 : 4.1120640564513256
Loss in iteration 308 : 4.01672180852954
Loss in iteration 309 : 3.4844006010939514
Loss in iteration 310 : 3.90934008716183
Loss in iteration 311 : 3.4637781426330214
Loss in iteration 312 : 3.814160723358139
Loss in iteration 313 : 3.478855438193546
Loss in iteration 314 : 3.708534999433422
Loss in iteration 315 : 3.451513812498801
Loss in iteration 316 : 4.239021499141086
Loss in iteration 317 : 3.8203273977342422
Loss in iteration 318 : 4.032849811900792
Loss in iteration 319 : 3.720440227288469
Loss in iteration 320 : 3.916352251979633
Loss in iteration 321 : 3.0687915537620722
Loss in iteration 322 : 3.3041214160589636
Loss in iteration 323 : 3.3278663037492815
Loss in iteration 324 : 3.7336439967452946
Loss in iteration 325 : 3.7010046136633497
Loss in iteration 326 : 4.3009595190370025
Loss in iteration 327 : 3.6628672000458393
Loss in iteration 328 : 4.205705681742687
Loss in iteration 329 : 3.5053512413132535
Loss in iteration 330 : 3.788449115526271
Loss in iteration 331 : 3.409465036772349
Loss in iteration 332 : 3.6421027401638897
Loss in iteration 333 : 3.593339520573978
Loss in iteration 334 : 3.9631149214030303
Loss in iteration 335 : 3.8578531843953057
Loss in iteration 336 : 4.157661012881021
Loss in iteration 337 : 3.0472594567633537
Loss in iteration 338 : 3.178192103572608
Loss in iteration 339 : 3.291933615430724
Loss in iteration 340 : 4.0347227103710095
Loss in iteration 341 : 3.9917558798495456
Loss in iteration 342 : 4.312332550214742
Loss in iteration 343 : 3.733660064362884
Loss in iteration 344 : 3.8818852356821894
Loss in iteration 345 : 3.8316805230184556
Loss in iteration 346 : 4.096657843169643
Loss in iteration 347 : 3.671381820815257
Loss in iteration 348 : 3.8893797424141496
Loss in iteration 349 : 3.2720394632693575
Loss in iteration 350 : 3.4995484623552975
Loss in iteration 351 : 3.307382923903879
Loss in iteration 352 : 3.7531064359627493
Loss in iteration 353 : 3.406734795615573
Loss in iteration 354 : 3.987566501083009
Loss in iteration 355 : 3.8292884693472833
Loss in iteration 356 : 4.228986536566144
Loss in iteration 357 : 3.762661284098625
Loss in iteration 358 : 4.556122821245005
Loss in iteration 359 : 4.5074630243017415
Loss in iteration 360 : 4.498007745161565
Loss in iteration 361 : 3.449844630021985
Loss in iteration 362 : 3.832885095141709
Loss in iteration 363 : 3.187865986752225
Loss in iteration 364 : 3.502224138110937
Loss in iteration 365 : 3.7315950945193843
Loss in iteration 366 : 4.07129580562366
Loss in iteration 367 : 3.61188363952724
Loss in iteration 368 : 4.18527321052254
Loss in iteration 369 : 4.2583949032907915
Loss in iteration 370 : 3.9458827835076504
Loss in iteration 371 : 3.23026790642541
Loss in iteration 372 : 3.4981548719010642
Loss in iteration 373 : 3.572440806199385
Loss in iteration 374 : 3.817581026701175
Loss in iteration 375 : 3.1434539015534084
Loss in iteration 376 : 3.5797037385788038
Loss in iteration 377 : 3.7465682589824962
Loss in iteration 378 : 4.297428532713787
Loss in iteration 379 : 3.8900518781539595
Loss in iteration 380 : 4.054683765750588
Loss in iteration 381 : 3.9024096751427173
Loss in iteration 382 : 4.477650191383249
Loss in iteration 383 : 3.6124050817153113
Loss in iteration 384 : 3.6079779903778944
Loss in iteration 385 : 3.740155879417712
Loss in iteration 386 : 4.189213552387656
Loss in iteration 387 : 3.5047137365126626
Loss in iteration 388 : 4.011968040810228
Loss in iteration 389 : 3.803894785556721
Loss in iteration 390 : 4.104083969411835
Loss in iteration 391 : 3.9676015730671312
Loss in iteration 392 : 4.229807211836848
Loss in iteration 393 : 3.4331923106570637
Loss in iteration 394 : 3.4104517949571456
Loss in iteration 395 : 3.2970629441137316
Loss in iteration 396 : 3.456244766806231
Loss in iteration 397 : 3.610265657738557
Loss in iteration 398 : 4.088433298753444
Loss in iteration 399 : 3.969498660441548
Loss in iteration 400 : 4.162120337065646
Testing accuracy  of updater 2 on alg 0 with rate 7.0 = 0.764625, training accuracy 0.764625, time elapsed: 6217 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.6844386194236305
Loss in iteration 3 : 16.30101867010334
Loss in iteration 4 : 1.0764439782786344
Loss in iteration 5 : 6.3243687041697525
Loss in iteration 6 : 9.974969196112012
Loss in iteration 7 : 0.8381379825122803
Loss in iteration 8 : 1.0269976761040236
Loss in iteration 9 : 1.9739878914977955
Loss in iteration 10 : 5.128632560843271
Loss in iteration 11 : 2.901600737967475
Loss in iteration 12 : 3.4889771504655225
Loss in iteration 13 : 2.869307124664487
Loss in iteration 14 : 2.7352923348473164
Loss in iteration 15 : 2.0893380773159764
Loss in iteration 16 : 2.0244760680994034
Loss in iteration 17 : 1.6130576079077714
Loss in iteration 18 : 1.5032055842409597
Loss in iteration 19 : 1.4827001189412128
Loss in iteration 20 : 1.4006816430904836
Loss in iteration 21 : 1.4637623154234383
Loss in iteration 22 : 1.397705891122508
Loss in iteration 23 : 1.3692559485796303
Loss in iteration 24 : 1.3313188969859855
Loss in iteration 25 : 1.265042919284266
Loss in iteration 26 : 1.1563848663197953
Loss in iteration 27 : 1.1844639390147291
Loss in iteration 28 : 1.2506085285026656
Loss in iteration 29 : 1.3237935644582024
Loss in iteration 30 : 1.850759621726983
Loss in iteration 31 : 2.409565880711233
Loss in iteration 32 : 3.7807060755908295
Loss in iteration 33 : 3.031722389770725
Loss in iteration 34 : 3.953794515078805
Loss in iteration 35 : 2.6837027041668904
Loss in iteration 36 : 3.209987800494708
Loss in iteration 37 : 3.0559802275337584
Loss in iteration 38 : 2.99931479208586
Loss in iteration 39 : 2.2551326154845817
Loss in iteration 40 : 2.53552982712857
Loss in iteration 41 : 2.5243220413608527
Loss in iteration 42 : 2.430507998719059
Loss in iteration 43 : 2.194444489064329
Loss in iteration 44 : 2.3332900608962523
Loss in iteration 45 : 2.0791321637138775
Loss in iteration 46 : 1.9740337592223818
Loss in iteration 47 : 1.7584685867731114
Loss in iteration 48 : 1.9219253985894686
Loss in iteration 49 : 1.9362569940805823
Loss in iteration 50 : 2.071233647364461
Loss in iteration 51 : 1.9054518882372717
Loss in iteration 52 : 2.0382989459777874
Loss in iteration 53 : 1.866557643385411
Loss in iteration 54 : 2.0724425318879933
Loss in iteration 55 : 1.973005527418566
Loss in iteration 56 : 2.445779829601216
Loss in iteration 57 : 2.1032088441239485
Loss in iteration 58 : 2.259517811872843
Loss in iteration 59 : 2.0136323596415355
Loss in iteration 60 : 2.5808453678183647
Loss in iteration 61 : 2.136888683837687
Loss in iteration 62 : 2.393588882342349
Loss in iteration 63 : 2.0910584346175067
Loss in iteration 64 : 2.290627889538382
Loss in iteration 65 : 2.1423373592051043
Loss in iteration 66 : 2.193053344310703
Loss in iteration 67 : 2.126654402547107
Loss in iteration 68 : 2.2177236356540644
Loss in iteration 69 : 2.1406209342973597
Loss in iteration 70 : 2.2841375567654936
Loss in iteration 71 : 2.0096877899464682
Loss in iteration 72 : 2.06883208382101
Loss in iteration 73 : 1.9890109883851594
Loss in iteration 74 : 2.09736111883785
Loss in iteration 75 : 2.23062826130137
Loss in iteration 76 : 2.5485914305106516
Loss in iteration 77 : 2.214414251873537
Loss in iteration 78 : 2.5999143464260444
Loss in iteration 79 : 2.2319944104339475
Loss in iteration 80 : 2.362732096907844
Loss in iteration 81 : 2.121637250045919
Loss in iteration 82 : 2.30962763206844
Loss in iteration 83 : 2.0519418182969194
Loss in iteration 84 : 2.408950231753793
Loss in iteration 85 : 2.1798121138768374
Loss in iteration 86 : 2.4069810496308817
Loss in iteration 87 : 1.981330877069617
Loss in iteration 88 : 2.170392157238381
Loss in iteration 89 : 1.998524115693795
Loss in iteration 90 : 2.1924203953637473
Loss in iteration 91 : 2.1062119507424004
Loss in iteration 92 : 2.312287341739111
Loss in iteration 93 : 2.174536512525547
Loss in iteration 94 : 2.301928155689599
Loss in iteration 95 : 2.0929111056777865
Loss in iteration 96 : 2.2403950580134255
Loss in iteration 97 : 2.0092872559727337
Loss in iteration 98 : 2.168109087195224
Loss in iteration 99 : 1.972178731043868
Loss in iteration 100 : 2.2139563482309583
Loss in iteration 101 : 2.1580737313803375
Loss in iteration 102 : 2.365504252922395
Loss in iteration 103 : 2.0209384970668154
Loss in iteration 104 : 2.1783142816243086
Loss in iteration 105 : 2.0127979926298387
Loss in iteration 106 : 2.2593713797876784
Loss in iteration 107 : 2.0462477173630673
Loss in iteration 108 : 2.1335421730385113
Loss in iteration 109 : 2.0351311743239235
Loss in iteration 110 : 2.3096707729901937
Loss in iteration 111 : 2.131483068572527
Loss in iteration 112 : 2.5346593628386964
Loss in iteration 113 : 2.396573768886965
Loss in iteration 114 : 2.4515932143075396
Loss in iteration 115 : 1.8894017135680026
Loss in iteration 116 : 1.9887459536924341
Loss in iteration 117 : 1.9491024644407646
Loss in iteration 118 : 2.185831013340064
Loss in iteration 119 : 2.1500849151181267
Loss in iteration 120 : 2.299635208494365
Loss in iteration 121 : 2.125589040887628
Loss in iteration 122 : 2.534566357495547
Loss in iteration 123 : 2.465246489678923
Loss in iteration 124 : 2.5409814446619063
Loss in iteration 125 : 2.1154712608493553
Loss in iteration 126 : 2.2888848109216626
Loss in iteration 127 : 1.8619861766866856
Loss in iteration 128 : 1.944615531883243
Loss in iteration 129 : 1.9704155574797688
Loss in iteration 130 : 2.2393744306991246
Loss in iteration 131 : 2.073636387489642
Loss in iteration 132 : 2.2951074586876894
Loss in iteration 133 : 2.0552758218607616
Loss in iteration 134 : 2.0762076634075597
Loss in iteration 135 : 1.8615937153205449
Loss in iteration 136 : 2.051636758867232
Loss in iteration 137 : 2.027994307367104
Loss in iteration 138 : 2.167505201172805
Loss in iteration 139 : 1.9911159514996342
Loss in iteration 140 : 2.2454983921030127
Loss in iteration 141 : 2.323949227132485
Loss in iteration 142 : 2.5708527257598774
Loss in iteration 143 : 2.1380084453432326
Loss in iteration 144 : 2.230671311052519
Loss in iteration 145 : 2.394394810603699
Loss in iteration 146 : 2.85220213890135
Loss in iteration 147 : 2.558988591934378
Loss in iteration 148 : 2.48189489700001
Loss in iteration 149 : 2.1407778782357907
Loss in iteration 150 : 2.2851532871472426
Loss in iteration 151 : 2.0746259902732804
Loss in iteration 152 : 2.235749909704176
Loss in iteration 153 : 1.9588933969982933
Loss in iteration 154 : 2.1539911372690534
Loss in iteration 155 : 1.9612891429490689
Loss in iteration 156 : 1.9246722778875816
Loss in iteration 157 : 1.8310588189903652
Loss in iteration 158 : 1.8922609447377223
Loss in iteration 159 : 1.878842856316073
Loss in iteration 160 : 2.1123897141393986
Loss in iteration 161 : 1.8878984128281249
Loss in iteration 162 : 2.4160463787375632
Loss in iteration 163 : 2.387464835176726
Loss in iteration 164 : 2.654577368105034
Loss in iteration 165 : 2.0945986793214786
Loss in iteration 166 : 2.338839935067285
Loss in iteration 167 : 2.417979670477589
Loss in iteration 168 : 2.7921806123413417
Loss in iteration 169 : 2.387207512378503
Loss in iteration 170 : 2.56996850166556
Loss in iteration 171 : 2.2750712108468174
Loss in iteration 172 : 2.446328307432281
Loss in iteration 173 : 2.110280162829265
Loss in iteration 174 : 2.053642488499396
Loss in iteration 175 : 1.8815685442119652
Loss in iteration 176 : 1.8484418534267002
Loss in iteration 177 : 1.8229713480444285
Loss in iteration 178 : 1.8252017298340308
Loss in iteration 179 : 1.7614826484656136
Loss in iteration 180 : 1.9718599855117835
Loss in iteration 181 : 1.968278978470487
Loss in iteration 182 : 2.1685836608775504
Loss in iteration 183 : 2.160449360039538
Loss in iteration 184 : 2.809225082130262
Loss in iteration 185 : 2.4330597033818004
Loss in iteration 186 : 2.42318608089948
Loss in iteration 187 : 2.368763823929947
Loss in iteration 188 : 2.761650929555847
Loss in iteration 189 : 2.348106224633734
Loss in iteration 190 : 2.376935284660027
Loss in iteration 191 : 1.9385655846148042
Loss in iteration 192 : 2.037485447692868
Loss in iteration 193 : 1.8339006599108005
Loss in iteration 194 : 1.7601787865207683
Loss in iteration 195 : 1.81033900491727
Loss in iteration 196 : 2.167666045453781
Loss in iteration 197 : 1.864046646259386
Loss in iteration 198 : 2.1510340545398123
Loss in iteration 199 : 2.352817858020061
Loss in iteration 200 : 2.4979293748946345
Loss in iteration 201 : 2.0397271212219397
Loss in iteration 202 : 2.3025176595447614
Loss in iteration 203 : 2.0112154433282208
Loss in iteration 204 : 2.1831662020813702
Loss in iteration 205 : 1.8618471762514937
Loss in iteration 206 : 2.1056209077945405
Loss in iteration 207 : 1.9460007616507318
Loss in iteration 208 : 2.11526206250925
Loss in iteration 209 : 2.1449343404181644
Loss in iteration 210 : 2.6061846480203354
Loss in iteration 211 : 2.3415973463532582
Loss in iteration 212 : 2.5031057813987596
Loss in iteration 213 : 2.3853300426124573
Loss in iteration 214 : 2.7044243641417927
Loss in iteration 215 : 2.3002194955783057
Loss in iteration 216 : 2.1957286037584502
Loss in iteration 217 : 2.1291783809933427
Loss in iteration 218 : 2.2973417113328627
Loss in iteration 219 : 2.2160622225029094
Loss in iteration 220 : 2.2195190871951045
Loss in iteration 221 : 2.0606619096771635
Loss in iteration 222 : 2.2473426741232996
Loss in iteration 223 : 1.9005990889356361
Loss in iteration 224 : 1.8908558731978582
Loss in iteration 225 : 1.8063277491867413
Loss in iteration 226 : 2.2276666471382076
Loss in iteration 227 : 2.2459712063106307
Loss in iteration 228 : 2.5121399062917504
Loss in iteration 229 : 2.134329801848129
Loss in iteration 230 : 2.2061474790644584
Loss in iteration 231 : 2.0537874011735107
Loss in iteration 232 : 2.290986921265359
Loss in iteration 233 : 1.8003391468628143
Loss in iteration 234 : 2.0065302350394503
Loss in iteration 235 : 2.162053477927976
Loss in iteration 236 : 2.4563140718262217
Loss in iteration 237 : 2.2300121272392444
Loss in iteration 238 : 2.4994896585028146
Loss in iteration 239 : 2.559904833942418
Loss in iteration 240 : 2.5878650219876236
Loss in iteration 241 : 1.8475180153846298
Loss in iteration 242 : 2.022579242119536
Loss in iteration 243 : 2.1456655737627455
Loss in iteration 244 : 2.291600112286996
Loss in iteration 245 : 2.091357561944539
Loss in iteration 246 : 2.1366643613676404
Loss in iteration 247 : 2.090838684797973
Loss in iteration 248 : 2.378626707645276
Loss in iteration 249 : 2.188924795864482
Loss in iteration 250 : 2.3786141197950728
Loss in iteration 251 : 2.162194272909543
Loss in iteration 252 : 2.2827166912874355
Loss in iteration 253 : 2.2460811549471402
Loss in iteration 254 : 2.131408453546745
Loss in iteration 255 : 1.9984570786213847
Loss in iteration 256 : 2.2940321794470866
Loss in iteration 257 : 2.0015515101541594
Loss in iteration 258 : 2.18018011324041
Loss in iteration 259 : 1.9481606654312273
Loss in iteration 260 : 2.1144314476980752
Loss in iteration 261 : 2.0128322323560774
Loss in iteration 262 : 2.422019230118598
Loss in iteration 263 : 2.047215871967464
Loss in iteration 264 : 2.30174056748027
Loss in iteration 265 : 2.052948559022462
Loss in iteration 266 : 2.345475924076398
Loss in iteration 267 : 2.2939971732607094
Loss in iteration 268 : 2.490111783660442
Loss in iteration 269 : 2.0909139887072445
Loss in iteration 270 : 2.283138039474224
Loss in iteration 271 : 1.9269872596797502
Loss in iteration 272 : 2.1005246925574363
Loss in iteration 273 : 1.9133209736163526
Loss in iteration 274 : 2.1535821926063687
Loss in iteration 275 : 1.9485491899658272
Loss in iteration 276 : 2.067259242385785
Loss in iteration 277 : 1.6855671015868727
Loss in iteration 278 : 1.8368201685381647
Loss in iteration 279 : 1.790077925172236
Loss in iteration 280 : 2.0527889899809306
Loss in iteration 281 : 2.1668508136233906
Loss in iteration 282 : 2.561830323139858
Loss in iteration 283 : 2.190734002677445
Loss in iteration 284 : 2.2100077523211823
Loss in iteration 285 : 2.092682386177593
Loss in iteration 286 : 2.3590479256144845
Loss in iteration 287 : 2.190032833812759
Loss in iteration 288 : 2.3289767787575104
Loss in iteration 289 : 2.2440697266728185
Loss in iteration 290 : 2.4918870210347595
Loss in iteration 291 : 2.3121517867136268
Loss in iteration 292 : 2.3305926318547816
Loss in iteration 293 : 1.93525073859897
Loss in iteration 294 : 2.07362926737081
Loss in iteration 295 : 1.9858634817527654
Loss in iteration 296 : 2.027929451363033
Loss in iteration 297 : 1.8572519851686407
Loss in iteration 298 : 1.9347780724533288
Loss in iteration 299 : 1.9801909213501243
Loss in iteration 300 : 2.12977731833466
Loss in iteration 301 : 2.0055963445894673
Loss in iteration 302 : 2.1553789952798423
Loss in iteration 303 : 1.9736749731450756
Loss in iteration 304 : 2.201152539090843
Loss in iteration 305 : 2.2875984139092997
Loss in iteration 306 : 2.6704404405958697
Loss in iteration 307 : 2.368255063708934
Loss in iteration 308 : 2.2787301901169474
Loss in iteration 309 : 2.002818600227303
Loss in iteration 310 : 2.2218353894944536
Loss in iteration 311 : 2.004539430601277
Loss in iteration 312 : 2.185732248312674
Loss in iteration 313 : 2.006154980189931
Loss in iteration 314 : 2.1126533839675745
Loss in iteration 315 : 2.0063714734950775
Loss in iteration 316 : 2.4130459032005214
Loss in iteration 317 : 2.202066874568926
Loss in iteration 318 : 2.2912468478150485
Loss in iteration 319 : 2.133124727555353
Loss in iteration 320 : 2.215577171600817
Loss in iteration 321 : 1.7806556078873377
Loss in iteration 322 : 1.8942633919425684
Loss in iteration 323 : 1.9024811457842878
Loss in iteration 324 : 2.1120047137261135
Loss in iteration 325 : 2.1429327737638357
Loss in iteration 326 : 2.4382371248414287
Loss in iteration 327 : 2.115242425091502
Loss in iteration 328 : 2.385233826522654
Loss in iteration 329 : 2.042366592398578
Loss in iteration 330 : 2.16024458222356
Loss in iteration 331 : 1.9635600435194425
Loss in iteration 332 : 2.0550054529703092
Loss in iteration 333 : 2.053257232085242
Loss in iteration 334 : 2.2466128099486564
Loss in iteration 335 : 2.201781074039702
Loss in iteration 336 : 2.338288939074625
Loss in iteration 337 : 1.7738904991435434
Loss in iteration 338 : 1.8274213386912896
Loss in iteration 339 : 1.9019870822626208
Loss in iteration 340 : 2.2698895146140203
Loss in iteration 341 : 2.2766728543592083
Loss in iteration 342 : 2.425772669227502
Loss in iteration 343 : 2.146015063570902
Loss in iteration 344 : 2.2138234945744704
Loss in iteration 345 : 2.195552227568618
Loss in iteration 346 : 2.3159647608171734
Loss in iteration 347 : 2.1210578587653255
Loss in iteration 348 : 2.2159140069350642
Loss in iteration 349 : 1.8999188857683444
Loss in iteration 350 : 2.028906201147135
Loss in iteration 351 : 1.9364819982683001
Loss in iteration 352 : 2.1607694104225184
Loss in iteration 353 : 1.9757518525659494
Loss in iteration 354 : 2.25254012358284
Loss in iteration 355 : 2.1775363360899576
Loss in iteration 356 : 2.3472320427015916
Loss in iteration 357 : 2.151931782277332
Loss in iteration 358 : 2.55062206728055
Loss in iteration 359 : 2.5790923390947373
Loss in iteration 360 : 2.5696598817810874
Loss in iteration 361 : 2.014886358482639
Loss in iteration 362 : 2.20750377459738
Loss in iteration 363 : 1.8456380726904118
Loss in iteration 364 : 2.005225337010144
Loss in iteration 365 : 2.1274907287820732
Loss in iteration 366 : 2.284186621240196
Loss in iteration 367 : 2.0863853935309034
Loss in iteration 368 : 2.372839159339296
Loss in iteration 369 : 2.4449694479532114
Loss in iteration 370 : 2.250363147494679
Loss in iteration 371 : 1.8816923847154412
Loss in iteration 372 : 1.9938954177334023
Loss in iteration 373 : 2.0389598962877713
Loss in iteration 374 : 2.159480349509672
Loss in iteration 375 : 1.8195322537034084
Loss in iteration 376 : 2.0426963111385787
Loss in iteration 377 : 2.1488785715852265
Loss in iteration 378 : 2.4171898324289427
Loss in iteration 379 : 2.235285237363091
Loss in iteration 380 : 2.292442815774845
Loss in iteration 381 : 2.2310311110875376
Loss in iteration 382 : 2.5240736970609716
Loss in iteration 383 : 2.083953143240671
Loss in iteration 384 : 2.0687862249539672
Loss in iteration 385 : 2.150686853571514
Loss in iteration 386 : 2.383518824432496
Loss in iteration 387 : 2.058338976348784
Loss in iteration 388 : 2.3148739019012505
Loss in iteration 389 : 2.217809473308551
Loss in iteration 390 : 2.338413460958273
Loss in iteration 391 : 2.263337523112917
Loss in iteration 392 : 2.373128122159048
Loss in iteration 393 : 1.9904243453730992
Loss in iteration 394 : 1.9626047973892067
Loss in iteration 395 : 1.889051676456116
Loss in iteration 396 : 1.942930352750411
Loss in iteration 397 : 2.0529083699655666
Loss in iteration 398 : 2.2882772685256727
Loss in iteration 399 : 2.258435716205453
Loss in iteration 400 : 2.353155664338348
Testing accuracy  of updater 2 on alg 0 with rate 4.0 = 0.766375, training accuracy 0.766375, time elapsed: 5911 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.7465131716295621
Loss in iteration 3 : 1.6607190025884324
Loss in iteration 4 : 2.6464596586780593
Loss in iteration 5 : 0.5270155613026124
Loss in iteration 6 : 0.534511736407127
Loss in iteration 7 : 0.5450869606577717
Loss in iteration 8 : 0.7333292743864175
Loss in iteration 9 : 1.0851854502103393
Loss in iteration 10 : 0.9307843665811826
Loss in iteration 11 : 0.8328558713667765
Loss in iteration 12 : 0.7209905353685263
Loss in iteration 13 : 0.6720556044917005
Loss in iteration 14 : 0.6055029381718433
Loss in iteration 15 : 0.5811538511906762
Loss in iteration 16 : 0.5499734189365231
Loss in iteration 17 : 0.5165599998468973
Loss in iteration 18 : 0.5085461112576275
Loss in iteration 19 : 0.5156103022056003
Loss in iteration 20 : 0.5028012012639825
Loss in iteration 21 : 0.5295466145874484
Loss in iteration 22 : 0.5194967045735855
Loss in iteration 23 : 0.5171426069108759
Loss in iteration 24 : 0.5077598950338643
Loss in iteration 25 : 0.5057863345592152
Loss in iteration 26 : 0.48405615408229463
Loss in iteration 27 : 0.49205846749479215
Loss in iteration 28 : 0.5007356583575623
Loss in iteration 29 : 0.49604675494181194
Loss in iteration 30 : 0.49689008503421406
Loss in iteration 31 : 0.488274851592385
Loss in iteration 32 : 0.48356095220931195
Loss in iteration 33 : 0.47254892483138555
Loss in iteration 34 : 0.4681543604828998
Loss in iteration 35 : 0.48196832506353865
Loss in iteration 36 : 0.4806098667316287
Loss in iteration 37 : 0.47063654845189323
Loss in iteration 38 : 0.4720635815108643
Loss in iteration 39 : 0.46414334053788436
Loss in iteration 40 : 0.4771512191889162
Loss in iteration 41 : 0.48374122590665053
Loss in iteration 42 : 0.47073479612784674
Loss in iteration 43 : 0.479391607255744
Loss in iteration 44 : 0.4856779455781625
Loss in iteration 45 : 0.5005625415817925
Loss in iteration 46 : 0.5348368097485197
Loss in iteration 47 : 0.5819711532224313
Loss in iteration 48 : 0.6647837810971565
Loss in iteration 49 : 0.6712134765034589
Loss in iteration 50 : 0.6982120230809123
Loss in iteration 51 : 0.7147853970152171
Loss in iteration 52 : 0.7598596183746417
Loss in iteration 53 : 0.727758536904269
Loss in iteration 54 : 0.7068992554848291
Loss in iteration 55 : 0.650220555841263
Loss in iteration 56 : 0.6507839629954228
Loss in iteration 57 : 0.6031633591840507
Loss in iteration 58 : 0.5877313023355001
Loss in iteration 59 : 0.5455911882903425
Loss in iteration 60 : 0.5406563553257575
Loss in iteration 61 : 0.5135938899056208
Loss in iteration 62 : 0.5068667698226641
Loss in iteration 63 : 0.5019973710593169
Loss in iteration 64 : 0.5009571877797913
Loss in iteration 65 : 0.5134595287828382
Loss in iteration 66 : 0.4879774883145031
Loss in iteration 67 : 0.5000935565717503
Loss in iteration 68 : 0.4794057545795916
Loss in iteration 69 : 0.48219040602518437
Loss in iteration 70 : 0.4649110719770798
Loss in iteration 71 : 0.4810898310693528
Loss in iteration 72 : 0.4656286022328049
Loss in iteration 73 : 0.4789854014500567
Loss in iteration 74 : 0.465015597221957
Loss in iteration 75 : 0.46752212669429344
Loss in iteration 76 : 0.48242132554248274
Loss in iteration 77 : 0.4560804357552068
Loss in iteration 78 : 0.4677266766228181
Loss in iteration 79 : 0.4437090238383806
Loss in iteration 80 : 0.4671983435220127
Loss in iteration 81 : 0.471060115601627
Loss in iteration 82 : 0.47235215030295846
Loss in iteration 83 : 0.4750993390239772
Loss in iteration 84 : 0.4995908334588757
Loss in iteration 85 : 0.5066737849793793
Loss in iteration 86 : 0.5475678033164563
Loss in iteration 87 : 0.5587720862212309
Loss in iteration 88 : 0.5960705383277125
Loss in iteration 89 : 0.6198769222502742
Loss in iteration 90 : 0.6353279362324001
Loss in iteration 91 : 0.6851830550038831
Loss in iteration 92 : 0.6731910403615169
Loss in iteration 93 : 0.6907845296474109
Loss in iteration 94 : 0.6555647636034945
Loss in iteration 95 : 0.6518015077895468
Loss in iteration 96 : 0.6092562718794278
Loss in iteration 97 : 0.5719820611671526
Loss in iteration 98 : 0.5556792912526809
Loss in iteration 99 : 0.5346941375640296
Loss in iteration 100 : 0.5315181049616486
Loss in iteration 101 : 0.524427202774488
Loss in iteration 102 : 0.5086201587302603
Loss in iteration 103 : 0.5088173568058397
Loss in iteration 104 : 0.4951589011148382
Loss in iteration 105 : 0.472390254376407
Loss in iteration 106 : 0.4942570559621506
Loss in iteration 107 : 0.5048395713916703
Loss in iteration 108 : 0.49619597622093414
Loss in iteration 109 : 0.4741714540170491
Loss in iteration 110 : 0.46606090395763344
Loss in iteration 111 : 0.47898709732638745
Loss in iteration 112 : 0.4825004088618118
Loss in iteration 113 : 0.49781384255013794
Loss in iteration 114 : 0.500645941297244
Loss in iteration 115 : 0.49127442338807326
Loss in iteration 116 : 0.502557276442789
Loss in iteration 117 : 0.51402661726172
Loss in iteration 118 : 0.5258556239987064
Loss in iteration 119 : 0.5342388023494761
Loss in iteration 120 : 0.5585961243068681
Loss in iteration 121 : 0.5688970140553162
Loss in iteration 122 : 0.6097950083682766
Loss in iteration 123 : 0.6545618649339038
Loss in iteration 124 : 0.6522067256377895
Loss in iteration 125 : 0.6443790588994958
Loss in iteration 126 : 0.6151229089856645
Loss in iteration 127 : 0.5838401773403349
Loss in iteration 128 : 0.5656447211684483
Loss in iteration 129 : 0.5600367561775411
Loss in iteration 130 : 0.5548039154044169
Loss in iteration 131 : 0.5447418240788474
Loss in iteration 132 : 0.540654124910739
Loss in iteration 133 : 0.5408343796998024
Loss in iteration 134 : 0.5172476182097304
Loss in iteration 135 : 0.5066864830463885
Loss in iteration 136 : 0.5053753388219726
Loss in iteration 137 : 0.5166171616655706
Loss in iteration 138 : 0.5017490844088762
Loss in iteration 139 : 0.5026280070278062
Loss in iteration 140 : 0.49711211262413335
Loss in iteration 141 : 0.5104819170329575
Loss in iteration 142 : 0.501456586979103
Loss in iteration 143 : 0.5058278602599757
Loss in iteration 144 : 0.4877402205520919
Loss in iteration 145 : 0.496162041698992
Loss in iteration 146 : 0.545259042704418
Loss in iteration 147 : 0.5866058594248346
Loss in iteration 148 : 0.5814542705151663
Loss in iteration 149 : 0.603631023770647
Loss in iteration 150 : 0.6067812536848373
Loss in iteration 151 : 0.6116592360725318
Loss in iteration 152 : 0.6018199003688655
Loss in iteration 153 : 0.5765120648282731
Loss in iteration 154 : 0.5810875331485732
Loss in iteration 155 : 0.5741143963653981
Loss in iteration 156 : 0.5409829575227123
Loss in iteration 157 : 0.5413033257613327
Loss in iteration 158 : 0.5146134316511506
Loss in iteration 159 : 0.5099845225978642
Loss in iteration 160 : 0.5009190060544473
Loss in iteration 161 : 0.49411014301894063
Loss in iteration 162 : 0.5115196648008488
Loss in iteration 163 : 0.5258696374126294
Loss in iteration 164 : 0.5222161082201732
Loss in iteration 165 : 0.5056532129380041
Loss in iteration 166 : 0.5089322349694653
Loss in iteration 167 : 0.5374361903456027
Loss in iteration 168 : 0.5666478288501469
Loss in iteration 169 : 0.5761878731658356
Loss in iteration 170 : 0.5959725450144364
Loss in iteration 171 : 0.5998674802058789
Loss in iteration 172 : 0.6129833147269177
Loss in iteration 173 : 0.6015803516522596
Loss in iteration 174 : 0.573323703299586
Loss in iteration 175 : 0.5732871413735878
Loss in iteration 176 : 0.546048728184411
Loss in iteration 177 : 0.5571793025119194
Loss in iteration 178 : 0.5219728997300183
Loss in iteration 179 : 0.5191263434889837
Loss in iteration 180 : 0.5055060118825989
Loss in iteration 181 : 0.5151915341831185
Loss in iteration 182 : 0.4996354892099615
Loss in iteration 183 : 0.497525200478209
Loss in iteration 184 : 0.53222373634975
Loss in iteration 185 : 0.5216498840438409
Loss in iteration 186 : 0.5024593348088966
Loss in iteration 187 : 0.5210340395488571
Loss in iteration 188 : 0.5556131217378062
Loss in iteration 189 : 0.5606431807689036
Loss in iteration 190 : 0.5400084403232879
Loss in iteration 191 : 0.5336823390212371
Loss in iteration 192 : 0.5326897708339617
Loss in iteration 193 : 0.5306604642603281
Loss in iteration 194 : 0.5115568890653406
Loss in iteration 195 : 0.518915532948006
Loss in iteration 196 : 0.534649733124558
Loss in iteration 197 : 0.5061822652909193
Loss in iteration 198 : 0.5367693615579586
Loss in iteration 199 : 0.5551039281111462
Loss in iteration 200 : 0.5498371058680358
Loss in iteration 201 : 0.5590783116166573
Loss in iteration 202 : 0.5615462741939825
Loss in iteration 203 : 0.530135738481512
Loss in iteration 204 : 0.5245152625942262
Loss in iteration 205 : 0.5072454153671334
Loss in iteration 206 : 0.5005439695286049
Loss in iteration 207 : 0.4888519680460023
Loss in iteration 208 : 0.4767331049330753
Loss in iteration 209 : 0.49595466405216154
Loss in iteration 210 : 0.5140777355887841
Loss in iteration 211 : 0.5166202118117905
Loss in iteration 212 : 0.537427338495682
Loss in iteration 213 : 0.5669651684998193
Loss in iteration 214 : 0.6058728285902146
Loss in iteration 215 : 0.606198678638115
Loss in iteration 216 : 0.5748290384312194
Loss in iteration 217 : 0.5944074265239437
Loss in iteration 218 : 0.6041535094897261
Loss in iteration 219 : 0.6349908187779579
Loss in iteration 220 : 0.6012933205169464
Loss in iteration 221 : 0.5995937904152702
Loss in iteration 222 : 0.6025183965970724
Loss in iteration 223 : 0.5868899495298234
Loss in iteration 224 : 0.5449701015571352
Loss in iteration 225 : 0.5241449650717191
Loss in iteration 226 : 0.5408529172694572
Loss in iteration 227 : 0.5447701463526653
Loss in iteration 228 : 0.5523193163474281
Loss in iteration 229 : 0.5369043614615011
Loss in iteration 230 : 0.5111478312340719
Loss in iteration 231 : 0.5138217327871542
Loss in iteration 232 : 0.515541361843628
Loss in iteration 233 : 0.4851626611065094
Loss in iteration 234 : 0.4890564940243618
Loss in iteration 235 : 0.4866165380907382
Loss in iteration 236 : 0.5051029357831812
Loss in iteration 237 : 0.5169500459221017
Loss in iteration 238 : 0.5275148470689934
Loss in iteration 239 : 0.5661487241361146
Loss in iteration 240 : 0.55418334522431
Loss in iteration 241 : 0.5364546381237362
Loss in iteration 242 : 0.5622257624135957
Loss in iteration 243 : 0.568143918459088
Loss in iteration 244 : 0.5519073019390848
Loss in iteration 245 : 0.5637373444683065
Loss in iteration 246 : 0.5384031370169791
Loss in iteration 247 : 0.5422305621091358
Loss in iteration 248 : 0.5584457095239639
Loss in iteration 249 : 0.5658667481397242
Loss in iteration 250 : 0.558834436054399
Loss in iteration 251 : 0.5760984735152529
Loss in iteration 252 : 0.5664799957224562
Loss in iteration 253 : 0.5816530789223642
Loss in iteration 254 : 0.5360484471775159
Loss in iteration 255 : 0.5393664436799042
Loss in iteration 256 : 0.5632407311070861
Loss in iteration 257 : 0.5296745219732247
Loss in iteration 258 : 0.5283575591380946
Loss in iteration 259 : 0.528032815185127
Loss in iteration 260 : 0.5321352786319028
Loss in iteration 261 : 0.5196870145927911
Loss in iteration 262 : 0.5417353586286642
Loss in iteration 263 : 0.5317947754116581
Loss in iteration 264 : 0.5422485214243156
Loss in iteration 265 : 0.5167509824249227
Loss in iteration 266 : 0.546820969509325
Loss in iteration 267 : 0.562184927240433
Loss in iteration 268 : 0.5672080667711681
Loss in iteration 269 : 0.5456519670579766
Loss in iteration 270 : 0.5413969000192906
Loss in iteration 271 : 0.5172954727273545
Loss in iteration 272 : 0.5250924651680893
Loss in iteration 273 : 0.5102610434506826
Loss in iteration 274 : 0.5180339369344263
Loss in iteration 275 : 0.5266721082357112
Loss in iteration 276 : 0.5210575162168506
Loss in iteration 277 : 0.4854541121396714
Loss in iteration 278 : 0.47517218401508254
Loss in iteration 279 : 0.4690069543832112
Loss in iteration 280 : 0.4762414894306747
Loss in iteration 281 : 0.4882860394980046
Loss in iteration 282 : 0.4880051663858266
Loss in iteration 283 : 0.49091861299508294
Loss in iteration 284 : 0.4705716514173578
Loss in iteration 285 : 0.4712882870149461
Loss in iteration 286 : 0.48616667275935194
Loss in iteration 287 : 0.5008767849267279
Loss in iteration 288 : 0.5069580905383341
Loss in iteration 289 : 0.5206023455940598
Loss in iteration 290 : 0.5556728765361897
Loss in iteration 291 : 0.6096790192244971
Loss in iteration 292 : 0.5982133426777665
Loss in iteration 293 : 0.5901263961776073
Loss in iteration 294 : 0.5889838384317658
Loss in iteration 295 : 0.588042675091834
Loss in iteration 296 : 0.5717501960278127
Loss in iteration 297 : 0.5630852795619794
Loss in iteration 298 : 0.5515081396720419
Loss in iteration 299 : 0.5554589952083326
Loss in iteration 300 : 0.5401639580552687
Loss in iteration 301 : 0.5287467524235839
Loss in iteration 302 : 0.509413594445627
Loss in iteration 303 : 0.4969886380915362
Loss in iteration 304 : 0.5013403949872044
Loss in iteration 305 : 0.5162385213115535
Loss in iteration 306 : 0.5305171261321768
Loss in iteration 307 : 0.5370301376967568
Loss in iteration 308 : 0.5182709614607766
Loss in iteration 309 : 0.49758093617503474
Loss in iteration 310 : 0.5027496348129803
Loss in iteration 311 : 0.513342073659166
Loss in iteration 312 : 0.523616740250081
Loss in iteration 313 : 0.49167077326092556
Loss in iteration 314 : 0.5023924129619667
Loss in iteration 315 : 0.5134165087586564
Loss in iteration 316 : 0.5479255424472335
Loss in iteration 317 : 0.5383439563750472
Loss in iteration 318 : 0.5386543137986967
Loss in iteration 319 : 0.5299785125865143
Loss in iteration 320 : 0.5238249760579026
Loss in iteration 321 : 0.5121854325487569
Loss in iteration 322 : 0.5118826870826567
Loss in iteration 323 : 0.5116407938707505
Loss in iteration 324 : 0.5189463349144194
Loss in iteration 325 : 0.5376798036797168
Loss in iteration 326 : 0.5580436840728484
Loss in iteration 327 : 0.5406637113373828
Loss in iteration 328 : 0.5542403639414765
Loss in iteration 329 : 0.5537435191757175
Loss in iteration 330 : 0.5321938521629949
Loss in iteration 331 : 0.5394959367893561
Loss in iteration 332 : 0.5215733715539962
Loss in iteration 333 : 0.5331881967764462
Loss in iteration 334 : 0.5458945725509359
Loss in iteration 335 : 0.549998975647932
Loss in iteration 336 : 0.5442861994830934
Loss in iteration 337 : 0.5203958361582888
Loss in iteration 338 : 0.5135315858570243
Loss in iteration 339 : 0.5219822107334346
Loss in iteration 340 : 0.523708904600244
Loss in iteration 341 : 0.5294416822943224
Loss in iteration 342 : 0.530248375472443
Loss in iteration 343 : 0.5312113290473116
Loss in iteration 344 : 0.532696473791561
Loss in iteration 345 : 0.5295701162721259
Loss in iteration 346 : 0.519897148536211
Loss in iteration 347 : 0.5313890876497163
Loss in iteration 348 : 0.5277788849137842
Loss in iteration 349 : 0.5207454572658368
Loss in iteration 350 : 0.5179587323898296
Loss in iteration 351 : 0.5093823865663322
Loss in iteration 352 : 0.512398103800059
Loss in iteration 353 : 0.5127498135203873
Loss in iteration 354 : 0.5218629545336897
Loss in iteration 355 : 0.531032176533623
Loss in iteration 356 : 0.5255442288537039
Loss in iteration 357 : 0.5371997934970678
Loss in iteration 358 : 0.5636740621739913
Loss in iteration 359 : 0.5891577585108011
Loss in iteration 360 : 0.5972988856283922
Loss in iteration 361 : 0.5754453179972542
Loss in iteration 362 : 0.5831895253080316
Loss in iteration 363 : 0.5474993667664456
Loss in iteration 364 : 0.5408301482044913
Loss in iteration 365 : 0.558703887678502
Loss in iteration 366 : 0.5447330971288064
Loss in iteration 367 : 0.5561644117001483
Loss in iteration 368 : 0.5715288932239662
Loss in iteration 369 : 0.6024476329817132
Loss in iteration 370 : 0.5539852860619703
Loss in iteration 371 : 0.5516327826061402
Loss in iteration 372 : 0.5211086327652528
Loss in iteration 373 : 0.5234759781982181
Loss in iteration 374 : 0.5122945937499246
Loss in iteration 375 : 0.49513444750654734
Loss in iteration 376 : 0.5043319054513525
Loss in iteration 377 : 0.5060796306953576
Loss in iteration 378 : 0.5136125855838483
Loss in iteration 379 : 0.5172891929996729
Loss in iteration 380 : 0.5071277451410892
Loss in iteration 381 : 0.5054196807165606
Loss in iteration 382 : 0.5216757187117681
Loss in iteration 383 : 0.5211859836415731
Loss in iteration 384 : 0.5166743795802161
Loss in iteration 385 : 0.5215552664514389
Loss in iteration 386 : 0.5229716185034013
Loss in iteration 387 : 0.549442715741259
Loss in iteration 388 : 0.5656643351358479
Loss in iteration 389 : 0.5762137031721491
Loss in iteration 390 : 0.5841869010026797
Loss in iteration 391 : 0.6044990748011124
Loss in iteration 392 : 0.579765135646491
Loss in iteration 393 : 0.5723336553661087
Loss in iteration 394 : 0.5529130382842833
Loss in iteration 395 : 0.5513044863395902
Loss in iteration 396 : 0.5268582709798533
Loss in iteration 397 : 0.5482294110151118
Loss in iteration 398 : 0.5498413435563988
Loss in iteration 399 : 0.5482071620415696
Loss in iteration 400 : 0.5396908514982842
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.78225, training accuracy 0.78225, time elapsed: 5916 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.7028977094486158
Loss in iteration 3 : 0.8837218915690743
Loss in iteration 4 : 1.4034809864089148
Loss in iteration 5 : 0.7326032882765425
Loss in iteration 6 : 0.8077591961324742
Loss in iteration 7 : 0.8329421866095498
Loss in iteration 8 : 0.7853857302203958
Loss in iteration 9 : 0.599915143549137
Loss in iteration 10 : 0.5390298866315162
Loss in iteration 11 : 0.5182705230000423
Loss in iteration 12 : 0.4922318671809611
Loss in iteration 13 : 0.48201613215129296
Loss in iteration 14 : 0.47860317971654925
Loss in iteration 15 : 0.4957352655542733
Loss in iteration 16 : 0.49146568658936624
Loss in iteration 17 : 0.4825629961401685
Loss in iteration 18 : 0.48160073493522393
Loss in iteration 19 : 0.4889064828123446
Loss in iteration 20 : 0.47884953716302525
Loss in iteration 21 : 0.5011904141308366
Loss in iteration 22 : 0.49395412708687314
Loss in iteration 23 : 0.4930473111520091
Loss in iteration 24 : 0.48534856100333473
Loss in iteration 25 : 0.48718114914278116
Loss in iteration 26 : 0.46960034648896104
Loss in iteration 27 : 0.47676429266142767
Loss in iteration 28 : 0.48404008927600023
Loss in iteration 29 : 0.48472264733374637
Loss in iteration 30 : 0.48531576679495586
Loss in iteration 31 : 0.4781676605653686
Loss in iteration 32 : 0.47562955200029156
Loss in iteration 33 : 0.46727176090346156
Loss in iteration 34 : 0.4651888435993986
Loss in iteration 35 : 0.47790609953613566
Loss in iteration 36 : 0.4773836339394673
Loss in iteration 37 : 0.46851487763501154
Loss in iteration 38 : 0.46976119598286387
Loss in iteration 39 : 0.46090965476871115
Loss in iteration 40 : 0.4694051562668172
Loss in iteration 41 : 0.47313033943051674
Loss in iteration 42 : 0.45841981040364094
Loss in iteration 43 : 0.46633193519926575
Loss in iteration 44 : 0.46679012022717997
Loss in iteration 45 : 0.46902206405188135
Loss in iteration 46 : 0.46982520812237677
Loss in iteration 47 : 0.4613899970972648
Loss in iteration 48 : 0.46670468702735574
Loss in iteration 49 : 0.4626190692037121
Loss in iteration 50 : 0.4521788171057389
Loss in iteration 51 : 0.4695188718248962
Loss in iteration 52 : 0.46028536510211915
Loss in iteration 53 : 0.46275694778274634
Loss in iteration 54 : 0.459420472133849
Loss in iteration 55 : 0.4632505086614785
Loss in iteration 56 : 0.46948959102582266
Loss in iteration 57 : 0.4509522026177497
Loss in iteration 58 : 0.4555931371268299
Loss in iteration 59 : 0.4538543778285375
Loss in iteration 60 : 0.46400926475935755
Loss in iteration 61 : 0.4604735125028953
Loss in iteration 62 : 0.46297732175784356
Loss in iteration 63 : 0.4625729385357699
Loss in iteration 64 : 0.46085914132712585
Loss in iteration 65 : 0.47218664905144575
Loss in iteration 66 : 0.455127596593631
Loss in iteration 67 : 0.467145107703603
Loss in iteration 68 : 0.4585100337485129
Loss in iteration 69 : 0.4650426392859059
Loss in iteration 70 : 0.45566161462216503
Loss in iteration 71 : 0.4692037880225475
Loss in iteration 72 : 0.45513451267055205
Loss in iteration 73 : 0.46761950370764566
Loss in iteration 74 : 0.45781876170032426
Loss in iteration 75 : 0.4598238048275177
Loss in iteration 76 : 0.47313498107204394
Loss in iteration 77 : 0.45485212711447565
Loss in iteration 78 : 0.4669798830967629
Loss in iteration 79 : 0.4456808450544183
Loss in iteration 80 : 0.46379901981172905
Loss in iteration 81 : 0.46467939482671505
Loss in iteration 82 : 0.46135448618409325
Loss in iteration 83 : 0.4612152212732487
Loss in iteration 84 : 0.4742097241083497
Loss in iteration 85 : 0.46187026833216493
Loss in iteration 86 : 0.47255439560779533
Loss in iteration 87 : 0.44968967050704145
Loss in iteration 88 : 0.47107713012234687
Loss in iteration 89 : 0.4548141570648706
Loss in iteration 90 : 0.45783687109088095
Loss in iteration 91 : 0.46633988516230157
Loss in iteration 92 : 0.4547045097940155
Loss in iteration 93 : 0.4606110748035858
Loss in iteration 94 : 0.4593671680186863
Loss in iteration 95 : 0.45887782358620877
Loss in iteration 96 : 0.4671323790721009
Loss in iteration 97 : 0.4601200403160395
Loss in iteration 98 : 0.4673157297422709
Loss in iteration 99 : 0.4583641308411614
Loss in iteration 100 : 0.4627164256035968
Loss in iteration 101 : 0.45800370948882024
Loss in iteration 102 : 0.45254515925786853
Loss in iteration 103 : 0.4655053302073139
Loss in iteration 104 : 0.46086006256943435
Loss in iteration 105 : 0.4454923197520837
Loss in iteration 106 : 0.46074448075181135
Loss in iteration 107 : 0.47556942708451094
Loss in iteration 108 : 0.4772718167465341
Loss in iteration 109 : 0.46137981138036566
Loss in iteration 110 : 0.45416682232827704
Loss in iteration 111 : 0.4635822087430659
Loss in iteration 112 : 0.4562205692418363
Loss in iteration 113 : 0.4546365958261163
Loss in iteration 114 : 0.4537324423257324
Loss in iteration 115 : 0.4518526838757158
Loss in iteration 116 : 0.4641352774892269
Loss in iteration 117 : 0.47043861965941075
Loss in iteration 118 : 0.4675539331208981
Loss in iteration 119 : 0.4537976611797639
Loss in iteration 120 : 0.4702840334422045
Loss in iteration 121 : 0.4645833656784623
Loss in iteration 122 : 0.47287592144143725
Loss in iteration 123 : 0.4644500088137872
Loss in iteration 124 : 0.45724487568081534
Loss in iteration 125 : 0.4551218719071099
Loss in iteration 126 : 0.45564169166218943
Loss in iteration 127 : 0.45650570866085743
Loss in iteration 128 : 0.46336921617850135
Loss in iteration 129 : 0.45811765780178115
Loss in iteration 130 : 0.4645299297423765
Loss in iteration 131 : 0.45662015271671547
Loss in iteration 132 : 0.4556217755834436
Loss in iteration 133 : 0.46333321213802725
Loss in iteration 134 : 0.4610365443324977
Loss in iteration 135 : 0.4611976057104581
Loss in iteration 136 : 0.45923679860304967
Loss in iteration 137 : 0.4665305667111538
Loss in iteration 138 : 0.4638139144749874
Loss in iteration 139 : 0.4706934959150992
Loss in iteration 140 : 0.46460118704435205
Loss in iteration 141 : 0.46695180117957996
Loss in iteration 142 : 0.45462959513201107
Loss in iteration 143 : 0.46198689581937585
Loss in iteration 144 : 0.45388036579994584
Loss in iteration 145 : 0.447959283533426
Loss in iteration 146 : 0.4667297882379754
Loss in iteration 147 : 0.4724529629248022
Loss in iteration 148 : 0.460309288359369
Loss in iteration 149 : 0.47377856069719015
Loss in iteration 150 : 0.4644139734521665
Loss in iteration 151 : 0.45953995162283706
Loss in iteration 152 : 0.47094454996786433
Loss in iteration 153 : 0.446429215066246
Loss in iteration 154 : 0.467912015590144
Loss in iteration 155 : 0.46404472830680593
Loss in iteration 156 : 0.44697834143077386
Loss in iteration 157 : 0.4636624919125422
Loss in iteration 158 : 0.4590101035217508
Loss in iteration 159 : 0.45954286665815564
Loss in iteration 160 : 0.4575126632452346
Loss in iteration 161 : 0.45448113085833874
Loss in iteration 162 : 0.45941325959729773
Loss in iteration 163 : 0.4639236483386106
Loss in iteration 164 : 0.4627615290618838
Loss in iteration 165 : 0.4561550881407672
Loss in iteration 166 : 0.45855362314519343
Loss in iteration 167 : 0.4674177475116533
Loss in iteration 168 : 0.47109135935994617
Loss in iteration 169 : 0.4578779569003849
Loss in iteration 170 : 0.4638736171700922
Loss in iteration 171 : 0.45451749125833146
Loss in iteration 172 : 0.4648641322810015
Loss in iteration 173 : 0.4560689491823748
Loss in iteration 174 : 0.4537269053156938
Loss in iteration 175 : 0.46797453330892497
Loss in iteration 176 : 0.45358063017283506
Loss in iteration 177 : 0.4672841040216678
Loss in iteration 178 : 0.4570766421495031
Loss in iteration 179 : 0.4656385257739453
Loss in iteration 180 : 0.453488943916492
Loss in iteration 181 : 0.46426300216129807
Loss in iteration 182 : 0.45943686774768283
Loss in iteration 183 : 0.4574109314612453
Loss in iteration 184 : 0.4749274952912824
Loss in iteration 185 : 0.4583750817803991
Loss in iteration 186 : 0.4486036352739955
Loss in iteration 187 : 0.4543109142245977
Loss in iteration 188 : 0.4683860260172737
Loss in iteration 189 : 0.46486704318751065
Loss in iteration 190 : 0.44816520272146987
Loss in iteration 191 : 0.4524808492234825
Loss in iteration 192 : 0.45872888736763695
Loss in iteration 193 : 0.46213593632404604
Loss in iteration 194 : 0.4617211427485755
Loss in iteration 195 : 0.4683188392614049
Loss in iteration 196 : 0.47478626037722715
Loss in iteration 197 : 0.4502607173363412
Loss in iteration 198 : 0.46852529443685365
Loss in iteration 199 : 0.46049701120631914
Loss in iteration 200 : 0.45008894446196834
Loss in iteration 201 : 0.45740235157655695
Loss in iteration 202 : 0.4688927070499242
Loss in iteration 203 : 0.44875933108661553
Loss in iteration 204 : 0.4564593640139803
Loss in iteration 205 : 0.46091800684893675
Loss in iteration 206 : 0.46218713750351254
Loss in iteration 207 : 0.4561713228033082
Loss in iteration 208 : 0.45091831112260305
Loss in iteration 209 : 0.46511894632758016
Loss in iteration 210 : 0.4698172260505502
Loss in iteration 211 : 0.46089826531515266
Loss in iteration 212 : 0.4669061013696902
Loss in iteration 213 : 0.46922054207817077
Loss in iteration 214 : 0.4819362009600109
Loss in iteration 215 : 0.4692905111840531
Loss in iteration 216 : 0.4569997260364369
Loss in iteration 217 : 0.45651060494863205
Loss in iteration 218 : 0.45534587767653645
Loss in iteration 219 : 0.4691724269920744
Loss in iteration 220 : 0.46677585484570094
Loss in iteration 221 : 0.4621052227643881
Loss in iteration 222 : 0.4561568737870853
Loss in iteration 223 : 0.4652271729933526
Loss in iteration 224 : 0.46890684503583224
Loss in iteration 225 : 0.459080581055884
Loss in iteration 226 : 0.46247193756233634
Loss in iteration 227 : 0.4535675332229354
Loss in iteration 228 : 0.4653685119149919
Loss in iteration 229 : 0.4632479863865434
Loss in iteration 230 : 0.4549695102301716
Loss in iteration 231 : 0.4606872000430942
Loss in iteration 232 : 0.46566633688458364
Loss in iteration 233 : 0.4570303981294168
Loss in iteration 234 : 0.4648096051627468
Loss in iteration 235 : 0.455271198118965
Loss in iteration 236 : 0.46289997125031584
Loss in iteration 237 : 0.46293386230561245
Loss in iteration 238 : 0.45997573302910844
Loss in iteration 239 : 0.46495901566537245
Loss in iteration 240 : 0.4521449838912927
Loss in iteration 241 : 0.4549852487167047
Loss in iteration 242 : 0.4822106306448955
Loss in iteration 243 : 0.4683386537664443
Loss in iteration 244 : 0.4587224424525925
Loss in iteration 245 : 0.4730487336837093
Loss in iteration 246 : 0.4562910570281016
Loss in iteration 247 : 0.4517899138593933
Loss in iteration 248 : 0.46242633114933984
Loss in iteration 249 : 0.4642250384239467
Loss in iteration 250 : 0.45417750106371074
Loss in iteration 251 : 0.46556736454572084
Loss in iteration 252 : 0.460884689492523
Loss in iteration 253 : 0.4703689661107732
Loss in iteration 254 : 0.4467242571324711
Loss in iteration 255 : 0.45058950913178863
Loss in iteration 256 : 0.47359796697382867
Loss in iteration 257 : 0.44906525781602946
Loss in iteration 258 : 0.456605957281854
Loss in iteration 259 : 0.45518468968903714
Loss in iteration 260 : 0.4588641159550362
Loss in iteration 261 : 0.44735605638252823
Loss in iteration 262 : 0.4603949651067214
Loss in iteration 263 : 0.4593200122787271
Loss in iteration 264 : 0.4729981041441341
Loss in iteration 265 : 0.44936615600063234
Loss in iteration 266 : 0.4698633231326246
Loss in iteration 267 : 0.46733683898973866
Loss in iteration 268 : 0.4715970651279549
Loss in iteration 269 : 0.45300391335024
Loss in iteration 270 : 0.45744254755064295
Loss in iteration 271 : 0.44664557999740717
Loss in iteration 272 : 0.4590264763042643
Loss in iteration 273 : 0.44608552968001974
Loss in iteration 274 : 0.45492851801308004
Loss in iteration 275 : 0.4640643891535122
Loss in iteration 276 : 0.4677280915180838
Loss in iteration 277 : 0.4541503959345883
Loss in iteration 278 : 0.45087178966550456
Loss in iteration 279 : 0.44962511540088346
Loss in iteration 280 : 0.45630430841081004
Loss in iteration 281 : 0.46146616942780183
Loss in iteration 282 : 0.45684694705925244
Loss in iteration 283 : 0.4613780464499393
Loss in iteration 284 : 0.4478400254851656
Loss in iteration 285 : 0.4494057128591897
Loss in iteration 286 : 0.45932816505832225
Loss in iteration 287 : 0.4646398096989155
Loss in iteration 288 : 0.4633471448389996
Loss in iteration 289 : 0.45559073505830305
Loss in iteration 290 : 0.4528739652802443
Loss in iteration 291 : 0.46677976360563056
Loss in iteration 292 : 0.4603297862200525
Loss in iteration 293 : 0.45583517990411615
Loss in iteration 294 : 0.46491033586341973
Loss in iteration 295 : 0.4513757710892184
Loss in iteration 296 : 0.45396634799839525
Loss in iteration 297 : 0.46480844466737714
Loss in iteration 298 : 0.4599990899233039
Loss in iteration 299 : 0.4602679132817038
Loss in iteration 300 : 0.46537279572822415
Loss in iteration 301 : 0.4651138413125512
Loss in iteration 302 : 0.4568736172723322
Loss in iteration 303 : 0.4514887270200537
Loss in iteration 304 : 0.45360255438766484
Loss in iteration 305 : 0.4572011881876577
Loss in iteration 306 : 0.45870114301660003
Loss in iteration 307 : 0.45878792208794117
Loss in iteration 308 : 0.45769144101450904
Loss in iteration 309 : 0.45095204072744505
Loss in iteration 310 : 0.45761778995572766
Loss in iteration 311 : 0.4669149394639015
Loss in iteration 312 : 0.4777628290751253
Loss in iteration 313 : 0.4468253342485899
Loss in iteration 314 : 0.4542672925194858
Loss in iteration 315 : 0.45709505883978324
Loss in iteration 316 : 0.4770244788351813
Loss in iteration 317 : 0.46075616643770606
Loss in iteration 318 : 0.46997827102348966
Loss in iteration 319 : 0.4542810969396918
Loss in iteration 320 : 0.4529682428730083
Loss in iteration 321 : 0.454464420786508
Loss in iteration 322 : 0.461021688142739
Loss in iteration 323 : 0.45624504265148064
Loss in iteration 324 : 0.45623775864155464
Loss in iteration 325 : 0.45599093316801703
Loss in iteration 326 : 0.4711435289386869
Loss in iteration 327 : 0.44951068321792337
Loss in iteration 328 : 0.4525706974227288
Loss in iteration 329 : 0.4549728963302489
Loss in iteration 330 : 0.4503937558203226
Loss in iteration 331 : 0.4599123923371847
Loss in iteration 332 : 0.44561275815664597
Loss in iteration 333 : 0.4464655978460247
Loss in iteration 334 : 0.45672104720359163
Loss in iteration 335 : 0.45069751609015696
Loss in iteration 336 : 0.4573543528376123
Loss in iteration 337 : 0.46277657967186175
Loss in iteration 338 : 0.46865020931419343
Loss in iteration 339 : 0.47110227897039725
Loss in iteration 340 : 0.46402491779126986
Loss in iteration 341 : 0.4566574162209649
Loss in iteration 342 : 0.45784446900228076
Loss in iteration 343 : 0.4576165818762702
Loss in iteration 344 : 0.4633214465627023
Loss in iteration 345 : 0.45692949807886796
Loss in iteration 346 : 0.4503457228988701
Loss in iteration 347 : 0.4593462595556174
Loss in iteration 348 : 0.4609818781964907
Loss in iteration 349 : 0.46641648984421663
Loss in iteration 350 : 0.4671233104747415
Loss in iteration 351 : 0.46109168093978237
Loss in iteration 352 : 0.45632374023370476
Loss in iteration 353 : 0.45074390611362286
Loss in iteration 354 : 0.45846139905116956
Loss in iteration 355 : 0.45901489973194354
Loss in iteration 356 : 0.45177095920599203
Loss in iteration 357 : 0.456313360094685
Loss in iteration 358 : 0.46551106490054445
Loss in iteration 359 : 0.4559341029108408
Loss in iteration 360 : 0.46466039530190467
Loss in iteration 361 : 0.448662583554674
Loss in iteration 362 : 0.46928543425283437
Loss in iteration 363 : 0.4607570461088231
Loss in iteration 364 : 0.45814989908020104
Loss in iteration 365 : 0.46137729918902076
Loss in iteration 366 : 0.4553591026402038
Loss in iteration 367 : 0.4656667498911844
Loss in iteration 368 : 0.46671030529268537
Loss in iteration 369 : 0.4725824204891042
Loss in iteration 370 : 0.45495458893959584
Loss in iteration 371 : 0.47334134877941114
Loss in iteration 372 : 0.45439380774407767
Loss in iteration 373 : 0.45937509468836996
Loss in iteration 374 : 0.4570784793666038
Loss in iteration 375 : 0.45603730364835715
Loss in iteration 376 : 0.46353270210714076
Loss in iteration 377 : 0.4593923213164551
Loss in iteration 378 : 0.46193017822963933
Loss in iteration 379 : 0.4625222883962601
Loss in iteration 380 : 0.4570858880425068
Loss in iteration 381 : 0.4471344786939602
Loss in iteration 382 : 0.4542938963335711
Loss in iteration 383 : 0.46163070609221896
Loss in iteration 384 : 0.46750345250869396
Loss in iteration 385 : 0.45944455515482585
Loss in iteration 386 : 0.4469283283975891
Loss in iteration 387 : 0.46454122782655266
Loss in iteration 388 : 0.4692105555900818
Loss in iteration 389 : 0.45658789540960953
Loss in iteration 390 : 0.45818065815719644
Loss in iteration 391 : 0.46059590927400584
Loss in iteration 392 : 0.4496246896943245
Loss in iteration 393 : 0.453520086974265
Loss in iteration 394 : 0.4566280291665127
Loss in iteration 395 : 0.4641080387225523
Loss in iteration 396 : 0.45674790541747157
Loss in iteration 397 : 0.4714626877197656
Loss in iteration 398 : 0.4678252745947411
Loss in iteration 399 : 0.4562503261587932
Loss in iteration 400 : 0.45139228150716687
Testing accuracy  of updater 2 on alg 0 with rate 0.7 = 0.7875, training accuracy 0.7875, time elapsed: 6334 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6805021655082217
Loss in iteration 3 : 0.6520156330931588
Loss in iteration 4 : 0.6333338882072952
Loss in iteration 5 : 0.6028273855608077
Loss in iteration 6 : 0.5834529076423052
Loss in iteration 7 : 0.5640171326611892
Loss in iteration 8 : 0.5490791290959927
Loss in iteration 9 : 0.5362548989520877
Loss in iteration 10 : 0.5237944086274465
Loss in iteration 11 : 0.5127420413747911
Loss in iteration 12 : 0.5033630641544947
Loss in iteration 13 : 0.4960876897189089
Loss in iteration 14 : 0.49133836185352325
Loss in iteration 15 : 0.49824005968439206
Loss in iteration 16 : 0.49049912639709964
Loss in iteration 17 : 0.48420218990972996
Loss in iteration 18 : 0.4814896381378573
Loss in iteration 19 : 0.48576991335788255
Loss in iteration 20 : 0.4772518931570326
Loss in iteration 21 : 0.49364927949689136
Loss in iteration 22 : 0.4889775157246095
Loss in iteration 23 : 0.4874517589255733
Loss in iteration 24 : 0.47897991578875615
Loss in iteration 25 : 0.48367783556186517
Loss in iteration 26 : 0.46976040095607546
Loss in iteration 27 : 0.4741547226059088
Loss in iteration 28 : 0.4792840835663673
Loss in iteration 29 : 0.4846390356073399
Loss in iteration 30 : 0.4838240661962488
Loss in iteration 31 : 0.4778511652070104
Loss in iteration 32 : 0.4767068661844869
Loss in iteration 33 : 0.4701355929034373
Loss in iteration 34 : 0.469844539077477
Loss in iteration 35 : 0.48109850901823686
Loss in iteration 36 : 0.48036284922236594
Loss in iteration 37 : 0.47255895208235477
Loss in iteration 38 : 0.4730268252721044
Loss in iteration 39 : 0.46484384070956486
Loss in iteration 40 : 0.4726448283593939
Loss in iteration 41 : 0.47691505763126363
Loss in iteration 42 : 0.4636593909305492
Loss in iteration 43 : 0.47210541460280603
Loss in iteration 44 : 0.472033835617563
Loss in iteration 45 : 0.4731799159379895
Loss in iteration 46 : 0.47291494067154277
Loss in iteration 47 : 0.4663177374947314
Loss in iteration 48 : 0.47098179335054713
Loss in iteration 49 : 0.4657301387406103
Loss in iteration 50 : 0.4563989036040639
Loss in iteration 51 : 0.47417684205430266
Loss in iteration 52 : 0.46290806540431895
Loss in iteration 53 : 0.4636383104374718
Loss in iteration 54 : 0.4607704094252341
Loss in iteration 55 : 0.4647823748083
Loss in iteration 56 : 0.47049591202642643
Loss in iteration 57 : 0.45254207449744643
Loss in iteration 58 : 0.45711870271664123
Loss in iteration 59 : 0.4560073139886895
Loss in iteration 60 : 0.4651026305545896
Loss in iteration 61 : 0.4623038565769451
Loss in iteration 62 : 0.4651275954489449
Loss in iteration 63 : 0.46366088723686344
Loss in iteration 64 : 0.46190526393866105
Loss in iteration 65 : 0.4733716793540767
Loss in iteration 66 : 0.45661603843841597
Loss in iteration 67 : 0.4682758143467696
Loss in iteration 68 : 0.4591437453883325
Loss in iteration 69 : 0.4665634266788901
Loss in iteration 70 : 0.4572663908216805
Loss in iteration 71 : 0.4714003579578489
Loss in iteration 72 : 0.457619493638195
Loss in iteration 73 : 0.4692152322658839
Loss in iteration 74 : 0.4600137467509527
Loss in iteration 75 : 0.46161211019740483
Loss in iteration 76 : 0.4739570765323061
Loss in iteration 77 : 0.45646840159635976
Loss in iteration 78 : 0.46641719220228595
Loss in iteration 79 : 0.44585574819973245
Loss in iteration 80 : 0.463487707771694
Loss in iteration 81 : 0.4651147571375779
Loss in iteration 82 : 0.46271524632584343
Loss in iteration 83 : 0.46336881491375526
Loss in iteration 84 : 0.47495310098865506
Loss in iteration 85 : 0.4634051081051514
Loss in iteration 86 : 0.47293268822293844
Loss in iteration 87 : 0.45220370076403354
Loss in iteration 88 : 0.47246307631475915
Loss in iteration 89 : 0.4563461303563911
Loss in iteration 90 : 0.4594508373089372
Loss in iteration 91 : 0.4677808988399642
Loss in iteration 92 : 0.4564571509460953
Loss in iteration 93 : 0.4619710596282964
Loss in iteration 94 : 0.4605230639631331
Loss in iteration 95 : 0.45959685594601235
Loss in iteration 96 : 0.46873918546248705
Loss in iteration 97 : 0.4613342824728967
Loss in iteration 98 : 0.46748454887290836
Loss in iteration 99 : 0.45954834019421914
Loss in iteration 100 : 0.464581548397544
Loss in iteration 101 : 0.45918059373240566
Loss in iteration 102 : 0.45437273901756464
Loss in iteration 103 : 0.467000618631134
Loss in iteration 104 : 0.4617427196828578
Loss in iteration 105 : 0.4472763104608339
Loss in iteration 106 : 0.4616651177691742
Loss in iteration 107 : 0.47650091474614154
Loss in iteration 108 : 0.47757099747685916
Loss in iteration 109 : 0.46089208060884845
Loss in iteration 110 : 0.45465170105194924
Loss in iteration 111 : 0.4648836968374443
Loss in iteration 112 : 0.45805715113768114
Loss in iteration 113 : 0.4532049037886689
Loss in iteration 114 : 0.45606793489777503
Loss in iteration 115 : 0.45272459580729546
Loss in iteration 116 : 0.4639854861230104
Loss in iteration 117 : 0.4710008683291924
Loss in iteration 118 : 0.46890916387086795
Loss in iteration 119 : 0.4546013754749079
Loss in iteration 120 : 0.47159013987566417
Loss in iteration 121 : 0.4656225035455376
Loss in iteration 122 : 0.47404019320143603
Loss in iteration 123 : 0.4651162883180084
Loss in iteration 124 : 0.4571853837924391
Loss in iteration 125 : 0.4561816924669775
Loss in iteration 126 : 0.4568855612842216
Loss in iteration 127 : 0.457841069593476
Loss in iteration 128 : 0.46437781702486497
Loss in iteration 129 : 0.4594294919709434
Loss in iteration 130 : 0.4652346600342896
Loss in iteration 131 : 0.458095052773146
Loss in iteration 132 : 0.4566753753696263
Loss in iteration 133 : 0.4646444880178341
Loss in iteration 134 : 0.462375421827753
Loss in iteration 135 : 0.4620774234943717
Loss in iteration 136 : 0.4607179021716476
Loss in iteration 137 : 0.4668901212278786
Loss in iteration 138 : 0.46507720878877273
Loss in iteration 139 : 0.4710817720421089
Loss in iteration 140 : 0.4658377546242245
Loss in iteration 141 : 0.467703844333461
Loss in iteration 142 : 0.4556831773953046
Loss in iteration 143 : 0.46343532412036964
Loss in iteration 144 : 0.4551985007617049
Loss in iteration 145 : 0.4498982186943917
Loss in iteration 146 : 0.4674125486947863
Loss in iteration 147 : 0.47085429512820887
Loss in iteration 148 : 0.46041451770827707
Loss in iteration 149 : 0.4748059620964829
Loss in iteration 150 : 0.46522509509060583
Loss in iteration 151 : 0.46058788324852756
Loss in iteration 152 : 0.4720757065756061
Loss in iteration 153 : 0.44739490163796314
Loss in iteration 154 : 0.46916085701465404
Loss in iteration 155 : 0.4651087734762197
Loss in iteration 156 : 0.447915449135347
Loss in iteration 157 : 0.46475691534819036
Loss in iteration 158 : 0.4576835270530105
Loss in iteration 159 : 0.4592532831856031
Loss in iteration 160 : 0.45817094748128906
Loss in iteration 161 : 0.45566038741673703
Loss in iteration 162 : 0.46031431207052376
Loss in iteration 163 : 0.46482258099807094
Loss in iteration 164 : 0.4640710850486055
Loss in iteration 165 : 0.457039468932291
Loss in iteration 166 : 0.4593619323588987
Loss in iteration 167 : 0.46920716362411113
Loss in iteration 168 : 0.4708381457964137
Loss in iteration 169 : 0.45799376087227023
Loss in iteration 170 : 0.46437062127148077
Loss in iteration 171 : 0.45499603016480356
Loss in iteration 172 : 0.4646408978647475
Loss in iteration 173 : 0.45697554129917406
Loss in iteration 174 : 0.4545942342850975
Loss in iteration 175 : 0.4691531285265413
Loss in iteration 176 : 0.4546382476966556
Loss in iteration 177 : 0.46814824371112324
Loss in iteration 178 : 0.4582088627142458
Loss in iteration 179 : 0.4661747843894072
Loss in iteration 180 : 0.4545430947714673
Loss in iteration 181 : 0.46507040295054186
Loss in iteration 182 : 0.45985862224875174
Loss in iteration 183 : 0.4579155435075713
Loss in iteration 184 : 0.47626003573562276
Loss in iteration 185 : 0.4591151246707353
Loss in iteration 186 : 0.4500169456705405
Loss in iteration 187 : 0.45580569366970414
Loss in iteration 188 : 0.46837243655315425
Loss in iteration 189 : 0.4651359356964427
Loss in iteration 190 : 0.44874949708754464
Loss in iteration 191 : 0.4538471768792436
Loss in iteration 192 : 0.4593593880094013
Loss in iteration 193 : 0.46265145653194095
Loss in iteration 194 : 0.46145174503039493
Loss in iteration 195 : 0.46905502456530507
Loss in iteration 196 : 0.47515410866646995
Loss in iteration 197 : 0.4510511349686902
Loss in iteration 198 : 0.4694621327258549
Loss in iteration 199 : 0.460941551543856
Loss in iteration 200 : 0.4506323796212147
Loss in iteration 201 : 0.45825264729165827
Loss in iteration 202 : 0.46985306465777443
Loss in iteration 203 : 0.44941904612996725
Loss in iteration 204 : 0.4567588690042527
Loss in iteration 205 : 0.46073707184088203
Loss in iteration 206 : 0.45974608508857895
Loss in iteration 207 : 0.4548689260994606
Loss in iteration 208 : 0.45062079142563627
Loss in iteration 209 : 0.4649927947792338
Loss in iteration 210 : 0.47096637183178336
Loss in iteration 211 : 0.4623113822025512
Loss in iteration 212 : 0.4673766519297174
Loss in iteration 213 : 0.469150238686988
Loss in iteration 214 : 0.48148154288930023
Loss in iteration 215 : 0.4702879634669707
Loss in iteration 216 : 0.45844258612795996
Loss in iteration 217 : 0.4578090676356953
Loss in iteration 218 : 0.45584712827274754
Loss in iteration 219 : 0.4680549454280947
Loss in iteration 220 : 0.4677347247894209
Loss in iteration 221 : 0.4634013374004878
Loss in iteration 222 : 0.45528350830765163
Loss in iteration 223 : 0.4663370247754861
Loss in iteration 224 : 0.4695093174296505
Loss in iteration 225 : 0.4593437217166457
Loss in iteration 226 : 0.4635839460562898
Loss in iteration 227 : 0.4526271361840157
Loss in iteration 228 : 0.4645592671069213
Loss in iteration 229 : 0.4637529305069789
Loss in iteration 230 : 0.4556398118266842
Loss in iteration 231 : 0.46224972143025334
Loss in iteration 232 : 0.4662572668658201
Loss in iteration 233 : 0.4584118927619016
Loss in iteration 234 : 0.46462424856955065
Loss in iteration 235 : 0.4569259495016895
Loss in iteration 236 : 0.46287872861978135
Loss in iteration 237 : 0.46208745460593853
Loss in iteration 238 : 0.4592929674044191
Loss in iteration 239 : 0.4620325511275262
Loss in iteration 240 : 0.4515366621879026
Loss in iteration 241 : 0.45628136195573116
Loss in iteration 242 : 0.48277069157907904
Loss in iteration 243 : 0.46948960018294206
Loss in iteration 244 : 0.45973828093161234
Loss in iteration 245 : 0.47312297200932024
Loss in iteration 246 : 0.4571738148613407
Loss in iteration 247 : 0.4527480393218122
Loss in iteration 248 : 0.4628950424839363
Loss in iteration 249 : 0.46496478259442836
Loss in iteration 250 : 0.4545308033947153
Loss in iteration 251 : 0.4662512011534481
Loss in iteration 252 : 0.46103086037976854
Loss in iteration 253 : 0.47080336383440935
Loss in iteration 254 : 0.44709477960787375
Loss in iteration 255 : 0.4515587465112406
Loss in iteration 256 : 0.4748549665360769
Loss in iteration 257 : 0.44993881480700404
Loss in iteration 258 : 0.457296930899521
Loss in iteration 259 : 0.45619211338791094
Loss in iteration 260 : 0.45922139133370943
Loss in iteration 261 : 0.44786768436044766
Loss in iteration 262 : 0.4613171650853721
Loss in iteration 263 : 0.4600218474545859
Loss in iteration 264 : 0.47288982800103946
Loss in iteration 265 : 0.45004514985095073
Loss in iteration 266 : 0.470293222869107
Loss in iteration 267 : 0.466576814471651
Loss in iteration 268 : 0.4714040163887282
Loss in iteration 269 : 0.45362569310392015
Loss in iteration 270 : 0.4585636761750551
Loss in iteration 271 : 0.44704733569719635
Loss in iteration 272 : 0.45953374632056687
Loss in iteration 273 : 0.446212991052025
Loss in iteration 274 : 0.45535046407681895
Loss in iteration 275 : 0.46490986460970296
Loss in iteration 276 : 0.4681967928948411
Loss in iteration 277 : 0.4535746030265918
Loss in iteration 278 : 0.45005989210156083
Loss in iteration 279 : 0.44855948098346426
Loss in iteration 280 : 0.4558144191795456
Loss in iteration 281 : 0.4623367820263946
Loss in iteration 282 : 0.4569314055856642
Loss in iteration 283 : 0.46239982256252926
Loss in iteration 284 : 0.4475806420972236
Loss in iteration 285 : 0.448433524257934
Loss in iteration 286 : 0.45916257348950446
Loss in iteration 287 : 0.4651047843096699
Loss in iteration 288 : 0.46379429274954964
Loss in iteration 289 : 0.45563879036607674
Loss in iteration 290 : 0.45160815374917423
Loss in iteration 291 : 0.4651954589282165
Loss in iteration 292 : 0.46127405079419037
Loss in iteration 293 : 0.4560948448389565
Loss in iteration 294 : 0.46541703280688146
Loss in iteration 295 : 0.45185614909827565
Loss in iteration 296 : 0.45481701463655927
Loss in iteration 297 : 0.4645620791379858
Loss in iteration 298 : 0.46005797912023566
Loss in iteration 299 : 0.46103189996121907
Loss in iteration 300 : 0.4652071099943765
Loss in iteration 301 : 0.4641069905457395
Loss in iteration 302 : 0.4566188855863891
Loss in iteration 303 : 0.45122120531829607
Loss in iteration 304 : 0.4543144453632674
Loss in iteration 305 : 0.45790436792861194
Loss in iteration 306 : 0.4584885522566873
Loss in iteration 307 : 0.4586531502618978
Loss in iteration 308 : 0.459190653651901
Loss in iteration 309 : 0.4508025343199345
Loss in iteration 310 : 0.4573134624400078
Loss in iteration 311 : 0.4668356166604241
Loss in iteration 312 : 0.477848299929348
Loss in iteration 313 : 0.4474913873337192
Loss in iteration 314 : 0.45467847022002084
Loss in iteration 315 : 0.4577302511181581
Loss in iteration 316 : 0.4768859051054639
Loss in iteration 317 : 0.46061406948572275
Loss in iteration 318 : 0.4705519165810385
Loss in iteration 319 : 0.45448998605485047
Loss in iteration 320 : 0.4532205714826644
Loss in iteration 321 : 0.4540724271508366
Loss in iteration 322 : 0.45920054430931134
Loss in iteration 323 : 0.45591699643368744
Loss in iteration 324 : 0.45709507479041983
Loss in iteration 325 : 0.45644406663141845
Loss in iteration 326 : 0.47195593894664767
Loss in iteration 327 : 0.45040063946310754
Loss in iteration 328 : 0.45305145084159354
Loss in iteration 329 : 0.45544499269839817
Loss in iteration 330 : 0.45081612600379883
Loss in iteration 331 : 0.4599947282847699
Loss in iteration 332 : 0.4462870240783428
Loss in iteration 333 : 0.4469619692050761
Loss in iteration 334 : 0.4570678143327867
Loss in iteration 335 : 0.4512716544896376
Loss in iteration 336 : 0.45805289950126904
Loss in iteration 337 : 0.4628157563613714
Loss in iteration 338 : 0.4676134298705452
Loss in iteration 339 : 0.47142047218745936
Loss in iteration 340 : 0.4641697100926997
Loss in iteration 341 : 0.4561679198860226
Loss in iteration 342 : 0.45798537958079794
Loss in iteration 343 : 0.45789874055638646
Loss in iteration 344 : 0.46401156765813384
Loss in iteration 345 : 0.4575374831502923
Loss in iteration 346 : 0.45098803829884965
Loss in iteration 347 : 0.45931236493639255
Loss in iteration 348 : 0.46088696886344616
Loss in iteration 349 : 0.4658901369537957
Loss in iteration 350 : 0.4659885435010599
Loss in iteration 351 : 0.4605900676245605
Loss in iteration 352 : 0.45664997443983035
Loss in iteration 353 : 0.45141044528977947
Loss in iteration 354 : 0.4590667094890219
Loss in iteration 355 : 0.45906085397491286
Loss in iteration 356 : 0.45221716104387566
Loss in iteration 357 : 0.45665745960715637
Loss in iteration 358 : 0.46595888076866415
Loss in iteration 359 : 0.45649703501097516
Loss in iteration 360 : 0.46456094455232866
Loss in iteration 361 : 0.44882357643648696
Loss in iteration 362 : 0.4693974853196412
Loss in iteration 363 : 0.4607967070883972
Loss in iteration 364 : 0.45827460149649135
Loss in iteration 365 : 0.461791264946182
Loss in iteration 366 : 0.4560526953272647
Loss in iteration 367 : 0.4660882386378885
Loss in iteration 368 : 0.4676482975476656
Loss in iteration 369 : 0.4705730004979521
Loss in iteration 370 : 0.45587427993068474
Loss in iteration 371 : 0.4734908847105327
Loss in iteration 372 : 0.4538787636060111
Loss in iteration 373 : 0.45949930930758065
Loss in iteration 374 : 0.456935822634527
Loss in iteration 375 : 0.45490564821054774
Loss in iteration 376 : 0.4637997584883054
Loss in iteration 377 : 0.45975383539696285
Loss in iteration 378 : 0.4624474735640484
Loss in iteration 379 : 0.46255672796725367
Loss in iteration 380 : 0.4572613604968883
Testing accuracy  of updater 2 on alg 0 with rate 0.4 = 0.788375, training accuracy 0.788375, time elapsed: 5355 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6848799840363802
Loss in iteration 3 : 0.674060685911538
Loss in iteration 4 : 0.6699030476600605
Loss in iteration 5 : 0.6579061188399133
Loss in iteration 6 : 0.6508627734578277
Loss in iteration 7 : 0.639220312919456
Loss in iteration 8 : 0.6293108582372261
Loss in iteration 9 : 0.6198954147619048
Loss in iteration 10 : 0.611828279917922
Loss in iteration 11 : 0.5992140426360859
Loss in iteration 12 : 0.592808823232222
Loss in iteration 13 : 0.5843465305659433
Loss in iteration 14 : 0.5760447659946152
Loss in iteration 15 : 0.5728786122317883
Loss in iteration 16 : 0.5633336553208105
Loss in iteration 17 : 0.5559857810196885
Loss in iteration 18 : 0.5509537464398464
Loss in iteration 19 : 0.5492398726590296
Loss in iteration 20 : 0.5411460482107918
Loss in iteration 21 : 0.5440352938444186
Loss in iteration 22 : 0.5392252977933613
Loss in iteration 23 : 0.5361556695566034
Loss in iteration 24 : 0.5261965854182813
Loss in iteration 25 : 0.5280650257059728
Loss in iteration 26 : 0.5177824462018894
Loss in iteration 27 : 0.5173262404077593
Loss in iteration 28 : 0.5175082211312225
Loss in iteration 29 : 0.5216588768544287
Loss in iteration 30 : 0.5195756198204518
Loss in iteration 31 : 0.5135871740447437
Loss in iteration 32 : 0.5124518866529169
Loss in iteration 33 : 0.5063674158492877
Loss in iteration 34 : 0.5053461897389886
Loss in iteration 35 : 0.510537986560981
Loss in iteration 36 : 0.5082672333252848
Loss in iteration 37 : 0.5029758765926263
Loss in iteration 38 : 0.5008901189952255
Loss in iteration 39 : 0.4970061483919909
Loss in iteration 40 : 0.49985359030484544
Loss in iteration 41 : 0.5022034525629456
Loss in iteration 42 : 0.4925542233495264
Loss in iteration 43 : 0.4998437798602732
Loss in iteration 44 : 0.4986250677920833
Loss in iteration 45 : 0.4976315160685388
Loss in iteration 46 : 0.4968803942086829
Loss in iteration 47 : 0.49416299995061325
Loss in iteration 48 : 0.49737653301732687
Loss in iteration 49 : 0.4905022056256207
Loss in iteration 50 : 0.48571251756110007
Loss in iteration 51 : 0.49755509406782716
Loss in iteration 52 : 0.4877327987130707
Loss in iteration 53 : 0.4875507260952245
Loss in iteration 54 : 0.4846698975194921
Loss in iteration 55 : 0.48770975140317663
Loss in iteration 56 : 0.49110618747186624
Loss in iteration 57 : 0.47674998848160666
Loss in iteration 58 : 0.48111481969362785
Loss in iteration 59 : 0.4810084735106323
Loss in iteration 60 : 0.4854983269317605
Loss in iteration 61 : 0.4852592067714373
Loss in iteration 62 : 0.48590889673132776
Loss in iteration 63 : 0.48268772010923283
Loss in iteration 64 : 0.48248427060632004
Loss in iteration 65 : 0.4915721283625747
Loss in iteration 66 : 0.4768971423744567
Loss in iteration 67 : 0.48468363670844605
Loss in iteration 68 : 0.47660055108295873
Loss in iteration 69 : 0.4858809343884802
Loss in iteration 70 : 0.4777997721157513
Loss in iteration 71 : 0.49133151876218095
Loss in iteration 72 : 0.47778682683304735
Loss in iteration 73 : 0.4851314085503087
Loss in iteration 74 : 0.4798029200405073
Loss in iteration 75 : 0.48018819542098967
Loss in iteration 76 : 0.48881554818459827
Loss in iteration 77 : 0.4753080138375667
Loss in iteration 78 : 0.48237218991939135
Loss in iteration 79 : 0.4665808856662176
Loss in iteration 80 : 0.4793425981512813
Loss in iteration 81 : 0.48227053896390676
Loss in iteration 82 : 0.47818366620695807
Loss in iteration 83 : 0.48117604997177177
Loss in iteration 84 : 0.4878607332439315
Loss in iteration 85 : 0.4801354315307191
Loss in iteration 86 : 0.4859753915843542
Loss in iteration 87 : 0.46962552755951004
Loss in iteration 88 : 0.4876219411292705
Loss in iteration 89 : 0.47224293664398664
Loss in iteration 90 : 0.4749671268746331
Loss in iteration 91 : 0.48275064504371795
Loss in iteration 92 : 0.4717121842509623
Loss in iteration 93 : 0.4763068393925361
Loss in iteration 94 : 0.47643653422060694
Loss in iteration 95 : 0.47230887884054984
Loss in iteration 96 : 0.4846943710568893
Loss in iteration 97 : 0.47469786038010825
Loss in iteration 98 : 0.47901019962881936
Loss in iteration 99 : 0.4748325719440966
Loss in iteration 100 : 0.47949485747230935
Loss in iteration 101 : 0.4722014638463938
Loss in iteration 102 : 0.471424565376149
Loss in iteration 103 : 0.4805676620982311
Loss in iteration 104 : 0.47576134164765477
Loss in iteration 105 : 0.4633770941696348
Loss in iteration 106 : 0.4738949613575512
Loss in iteration 107 : 0.483975833789571
Loss in iteration 108 : 0.48799086436305944
Loss in iteration 109 : 0.47350680290759084
Loss in iteration 110 : 0.4684401268327849
Loss in iteration 111 : 0.4777636176167709
Loss in iteration 112 : 0.47077503782278163
Loss in iteration 113 : 0.4663141354623628
Loss in iteration 114 : 0.4675479687689246
Loss in iteration 115 : 0.46556777289746104
Loss in iteration 116 : 0.47363093292838016
Loss in iteration 117 : 0.48190834543656574
Loss in iteration 118 : 0.48004337350355064
Loss in iteration 119 : 0.46531345378754135
Loss in iteration 120 : 0.48080020124605943
Loss in iteration 121 : 0.4772888998061355
Loss in iteration 122 : 0.48173498385599595
Loss in iteration 123 : 0.47450635169514727
Loss in iteration 124 : 0.4683134108003309
Loss in iteration 125 : 0.4674770212032427
Loss in iteration 126 : 0.4672582211288322
Loss in iteration 127 : 0.46887256754220763
Loss in iteration 128 : 0.4743686462297159
Loss in iteration 129 : 0.46863154039701127
Loss in iteration 130 : 0.47337242959382486
Loss in iteration 131 : 0.46914379936780787
Loss in iteration 132 : 0.4681443430186195
Loss in iteration 133 : 0.47351398981302806
Loss in iteration 134 : 0.47284493776391234
Loss in iteration 135 : 0.4708298712048097
Loss in iteration 136 : 0.4710058019372778
Loss in iteration 137 : 0.47410314522367586
Loss in iteration 138 : 0.47427646138518387
Loss in iteration 139 : 0.47904623054018836
Loss in iteration 140 : 0.4741379245440242
Loss in iteration 141 : 0.4751983219793799
Loss in iteration 142 : 0.46677563947016487
Loss in iteration 143 : 0.4712048635884313
Loss in iteration 144 : 0.46666170810743934
Loss in iteration 145 : 0.4614316600010837
Loss in iteration 146 : 0.4741805333049609
Loss in iteration 147 : 0.47870516933174934
Loss in iteration 148 : 0.46973520214548964
Loss in iteration 149 : 0.4817652048661226
Loss in iteration 150 : 0.4743289719869896
Loss in iteration 151 : 0.46837440435684113
Loss in iteration 152 : 0.4784859804816269
Loss in iteration 153 : 0.457631805932569
Loss in iteration 154 : 0.47710088245239757
Loss in iteration 155 : 0.4734301156407695
Loss in iteration 156 : 0.457704657095104
Loss in iteration 157 : 0.4717407297351461
Loss in iteration 158 : 0.463819731201791
Loss in iteration 159 : 0.4664140697473973
Loss in iteration 160 : 0.4656786494808634
Loss in iteration 161 : 0.4656440895629564
Loss in iteration 162 : 0.467999800588295
Loss in iteration 163 : 0.4721046114117667
Loss in iteration 164 : 0.471470220830386
Loss in iteration 165 : 0.46437594366988866
Loss in iteration 166 : 0.46726548267498835
Loss in iteration 167 : 0.4767358415424839
Loss in iteration 168 : 0.47659793697097064
Loss in iteration 169 : 0.4673683942246066
Loss in iteration 170 : 0.4731464331187026
Loss in iteration 171 : 0.4629426415120935
Loss in iteration 172 : 0.47032952587641974
Loss in iteration 173 : 0.4663820612424262
Loss in iteration 174 : 0.4622232731842965
Loss in iteration 175 : 0.47627719166334953
Loss in iteration 176 : 0.4631395621982977
Loss in iteration 177 : 0.47516477072819896
Loss in iteration 178 : 0.46445783913539157
Loss in iteration 179 : 0.47345120494286114
Loss in iteration 180 : 0.4623254583820649
Loss in iteration 181 : 0.470124759524769
Loss in iteration 182 : 0.4663156187854886
Loss in iteration 183 : 0.46558686451629233
Loss in iteration 184 : 0.48083522448029703
Loss in iteration 185 : 0.4665880002357345
Loss in iteration 186 : 0.45855217038806023
Loss in iteration 187 : 0.4619278699281872
Loss in iteration 188 : 0.47289309886119263
Loss in iteration 189 : 0.47207322920148376
Loss in iteration 190 : 0.45784804524289413
Loss in iteration 191 : 0.4612294598640166
Loss in iteration 192 : 0.46512846212187503
Loss in iteration 193 : 0.46846628642266347
Loss in iteration 194 : 0.4675569645555238
Loss in iteration 195 : 0.47526730101614106
Loss in iteration 196 : 0.4810088101285461
Loss in iteration 197 : 0.45767773132158074
Loss in iteration 198 : 0.475589691709143
Loss in iteration 199 : 0.46590199175036967
Loss in iteration 200 : 0.45863317851500607
Loss in iteration 201 : 0.4659671733553779
Loss in iteration 202 : 0.47434277388952456
Loss in iteration 203 : 0.4568136923363142
Loss in iteration 204 : 0.46214331535568154
Loss in iteration 205 : 0.4666111672104097
Loss in iteration 206 : 0.46529616114766326
Loss in iteration 207 : 0.46214912341837355
Loss in iteration 208 : 0.4575082433189984
Loss in iteration 209 : 0.4705365730946259
Loss in iteration 210 : 0.47526069177735175
Loss in iteration 211 : 0.4689188664324088
Loss in iteration 212 : 0.47296922651645934
Loss in iteration 213 : 0.47338977454587
Loss in iteration 214 : 0.48574201795591415
Loss in iteration 215 : 0.4751054287356502
Loss in iteration 216 : 0.46652836184361696
Loss in iteration 217 : 0.4644647413219921
Loss in iteration 218 : 0.4604565983045401
Loss in iteration 219 : 0.4719465546220371
Loss in iteration 220 : 0.47162717013762717
Loss in iteration 221 : 0.4695569808636753
Loss in iteration 222 : 0.4593625290185971
Loss in iteration 223 : 0.47223193476495545
Loss in iteration 224 : 0.47245159263554815
Loss in iteration 225 : 0.46499893103327117
Loss in iteration 226 : 0.4678671599957331
Loss in iteration 227 : 0.45764580110238007
Loss in iteration 228 : 0.4683961826128535
Loss in iteration 229 : 0.4686792389808923
Loss in iteration 230 : 0.46021918212211055
Loss in iteration 231 : 0.4683399672657899
Loss in iteration 232 : 0.47012040696445645
Loss in iteration 233 : 0.4640139179775628
Loss in iteration 234 : 0.47059074503719883
Loss in iteration 235 : 0.4609691800068962
Loss in iteration 236 : 0.4666132447299853
Loss in iteration 237 : 0.46687245309929315
Loss in iteration 238 : 0.46521072117971335
Loss in iteration 239 : 0.4650929447396328
Loss in iteration 240 : 0.4590281498158391
Loss in iteration 241 : 0.4607928210385314
Loss in iteration 242 : 0.4868310951091269
Loss in iteration 243 : 0.4744763183855671
Loss in iteration 244 : 0.4648407643335411
Loss in iteration 245 : 0.4771709813960622
Loss in iteration 246 : 0.4632820440889787
Loss in iteration 247 : 0.45833206865244014
Loss in iteration 248 : 0.46678921110565513
Loss in iteration 249 : 0.4700500823710073
Loss in iteration 250 : 0.45942756539900687
Loss in iteration 251 : 0.4706741533012469
Loss in iteration 252 : 0.4644655900836807
Loss in iteration 253 : 0.47440955501752174
Loss in iteration 254 : 0.45203760719565816
Loss in iteration 255 : 0.4575892872619167
Loss in iteration 256 : 0.4791144833418508
Loss in iteration 257 : 0.45625776000374635
Loss in iteration 258 : 0.4615489663720786
Loss in iteration 259 : 0.4607553077307286
Loss in iteration 260 : 0.4637959549267291
Loss in iteration 261 : 0.4528941477034755
Loss in iteration 262 : 0.464466437875876
Loss in iteration 263 : 0.4650269173534735
Loss in iteration 264 : 0.477781987703166
Loss in iteration 265 : 0.45556582352035585
Loss in iteration 266 : 0.47425601312289417
Loss in iteration 267 : 0.4704887808255406
Loss in iteration 268 : 0.4755022909959118
Loss in iteration 269 : 0.45944637377727027
Loss in iteration 270 : 0.4647041218253371
Loss in iteration 271 : 0.45268999737115867
Loss in iteration 272 : 0.4649029255384861
Loss in iteration 273 : 0.4504234058270008
Loss in iteration 274 : 0.45952351096702077
Loss in iteration 275 : 0.46831083909855464
Loss in iteration 276 : 0.47107861515069294
Loss in iteration 277 : 0.4578864769604129
Loss in iteration 278 : 0.45607733611652285
Loss in iteration 279 : 0.45381613359478445
Loss in iteration 280 : 0.4597854211125881
Loss in iteration 281 : 0.4672033766585229
Loss in iteration 282 : 0.46077878811122075
Loss in iteration 283 : 0.46701684041606767
Loss in iteration 284 : 0.4521026880139831
Loss in iteration 285 : 0.4538442533687595
Loss in iteration 286 : 0.46382600970193066
Loss in iteration 287 : 0.4681911281208061
Loss in iteration 288 : 0.46704673668476077
Loss in iteration 289 : 0.45928345879733345
Loss in iteration 290 : 0.45543685246357174
Loss in iteration 291 : 0.4683550977720696
Loss in iteration 292 : 0.46542428524456814
Loss in iteration 293 : 0.4596143315460606
Loss in iteration 294 : 0.46972019967806133
Loss in iteration 295 : 0.45611190418503056
Loss in iteration 296 : 0.45910694364586496
Loss in iteration 297 : 0.4676129924549913
Loss in iteration 298 : 0.46423558764032585
Loss in iteration 299 : 0.465904365393785
Loss in iteration 300 : 0.4684511134126822
Loss in iteration 301 : 0.46707211139553356
Loss in iteration 302 : 0.4616917946416953
Loss in iteration 303 : 0.4559343004098533
Loss in iteration 304 : 0.4587608723155715
Loss in iteration 305 : 0.46083230446992174
Loss in iteration 306 : 0.46162844251368107
Loss in iteration 307 : 0.46281233751800643
Loss in iteration 308 : 0.4646690910972821
Loss in iteration 309 : 0.4552317027709242
Loss in iteration 310 : 0.461267184584049
Loss in iteration 311 : 0.470456715032692
Loss in iteration 312 : 0.48036407990936214
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.791, training accuracy 0.791, time elapsed: 4473 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 30.039009074333595
Loss in iteration 3 : 18.835593386002266
Loss in iteration 4 : 17.455220468338563
Loss in iteration 5 : 7.826767085045424
Loss in iteration 6 : 9.844451539776987
Loss in iteration 7 : 6.446339464637413
Loss in iteration 8 : 7.274701985935266
Loss in iteration 9 : 5.404236960909661
Loss in iteration 10 : 6.2047700479749315
Loss in iteration 11 : 4.8900329867942345
Loss in iteration 12 : 5.303697814659441
Loss in iteration 13 : 3.7728994335251644
Loss in iteration 14 : 3.944264226342384
Loss in iteration 15 : 3.7306840274116286
Loss in iteration 16 : 3.9816987725109843
Loss in iteration 17 : 3.341849122024515
Loss in iteration 18 : 3.374210882698365
Loss in iteration 19 : 2.9978594302146053
Loss in iteration 20 : 2.98268442682467
Loss in iteration 21 : 3.108889511436034
Loss in iteration 22 : 2.9029411207183444
Loss in iteration 23 : 2.9502174815725875
Loss in iteration 24 : 2.6811572397326824
Loss in iteration 25 : 2.7493488483901474
Loss in iteration 26 : 2.312292254005438
Loss in iteration 27 : 2.3098452357924435
Loss in iteration 28 : 2.3665362048771668
Loss in iteration 29 : 2.5750500100320126
Loss in iteration 30 : 2.6040993205779195
Loss in iteration 31 : 2.750416582468486
Loss in iteration 32 : 2.8052469818033163
Loss in iteration 33 : 2.550864910672391
Loss in iteration 34 : 2.4097865822759506
Loss in iteration 35 : 2.4364575060236473
Loss in iteration 36 : 2.4000316568728213
Loss in iteration 37 : 2.295910223403837
Loss in iteration 38 : 2.146898087425597
Loss in iteration 39 : 2.1548925628366558
Loss in iteration 40 : 2.129048212011396
Loss in iteration 41 : 2.0746746243846292
Loss in iteration 42 : 1.826972629877081
Loss in iteration 43 : 1.8080633822484806
Loss in iteration 44 : 1.8020006165810034
Loss in iteration 45 : 1.8029926466746302
Loss in iteration 46 : 1.7685002239230114
Loss in iteration 47 : 1.8803145894808027
Loss in iteration 48 : 1.8277663981255863
Loss in iteration 49 : 1.7648787749890322
Loss in iteration 50 : 1.7311108223319236
Loss in iteration 51 : 1.8484476950075377
Loss in iteration 52 : 1.8421415371725007
Loss in iteration 53 : 1.921563752499326
Loss in iteration 54 : 1.8503700311221505
Loss in iteration 55 : 1.8341139001606932
Loss in iteration 56 : 1.832081454479019
Loss in iteration 57 : 1.7749794257262705
Loss in iteration 58 : 1.6693796265352052
Loss in iteration 59 : 1.6367029455738642
Loss in iteration 60 : 1.566626914170177
Loss in iteration 61 : 1.5242462803931376
Loss in iteration 62 : 1.527241980865648
Loss in iteration 63 : 1.56117924587126
Loss in iteration 64 : 1.5346347991324825
Loss in iteration 65 : 1.6638449915612932
Loss in iteration 66 : 1.4861752737191585
Loss in iteration 67 : 1.6689869836844853
Loss in iteration 68 : 1.401432427050263
Loss in iteration 69 : 1.5481431876751628
Loss in iteration 70 : 1.3484247785199162
Loss in iteration 71 : 1.5459691251883023
Loss in iteration 72 : 1.3877808743421582
Loss in iteration 73 : 1.5243531010753393
Loss in iteration 74 : 1.2953685187141448
Loss in iteration 75 : 1.4867223535965552
Loss in iteration 76 : 1.4095855823494845
Loss in iteration 77 : 1.450361181494228
Loss in iteration 78 : 1.2393607832250142
Loss in iteration 79 : 1.2426577310997056
Loss in iteration 80 : 1.2085064271091237
Loss in iteration 81 : 1.3185598468603832
Loss in iteration 82 : 1.3284097942056698
Loss in iteration 83 : 1.5014902403056287
Loss in iteration 84 : 1.6549167556638318
Loss in iteration 85 : 1.7141928017143908
Loss in iteration 86 : 1.69204025188806
Loss in iteration 87 : 1.7722297718262716
Loss in iteration 88 : 1.887191809453581
Loss in iteration 89 : 1.8549283765500857
Loss in iteration 90 : 1.6531847403573203
Loss in iteration 91 : 1.736373308686534
Loss in iteration 92 : 1.564794602305832
Loss in iteration 93 : 1.5705373856149962
Loss in iteration 94 : 1.443830788773489
Loss in iteration 95 : 1.4911139788856131
Loss in iteration 96 : 1.4612700485072598
Loss in iteration 97 : 1.5408878976971143
Loss in iteration 98 : 1.5916836281185651
Loss in iteration 99 : 1.5825296791627543
Loss in iteration 100 : 1.4989116464603547
Loss in iteration 101 : 1.5056555710600525
Loss in iteration 102 : 1.3766986069444198
Loss in iteration 103 : 1.4620545760019075
Loss in iteration 104 : 1.4731212926821122
Loss in iteration 105 : 1.4762471882740758
Loss in iteration 106 : 1.4356706829294248
Loss in iteration 107 : 1.4924159551858391
Loss in iteration 108 : 1.4762750398587223
Loss in iteration 109 : 1.4893126225258293
Loss in iteration 110 : 1.4888163283070408
Loss in iteration 111 : 1.5538793344059234
Loss in iteration 112 : 1.2433998245426006
Loss in iteration 113 : 1.1953576583523016
Loss in iteration 114 : 1.241292083188756
Loss in iteration 115 : 1.3049123696897382
Loss in iteration 116 : 1.2177130321907919
Loss in iteration 117 : 1.3315467197849027
Loss in iteration 118 : 1.2155481109954598
Loss in iteration 119 : 1.213438931224386
Loss in iteration 120 : 1.2311063705229166
Loss in iteration 121 : 1.2933977034862845
Loss in iteration 122 : 1.3321073301004385
Loss in iteration 123 : 1.2971050915515812
Loss in iteration 124 : 1.2483440557441967
Loss in iteration 125 : 1.3252683084534516
Loss in iteration 126 : 1.2929743157056959
Loss in iteration 127 : 1.3776661238489634
Loss in iteration 128 : 1.300791809384916
Loss in iteration 129 : 1.3168232444818828
Loss in iteration 130 : 1.3113870593005035
Loss in iteration 131 : 1.3251040264916745
Loss in iteration 132 : 1.1690929476792402
Loss in iteration 133 : 1.236652452739437
Loss in iteration 134 : 1.2333721937768307
Loss in iteration 135 : 1.2780602265241947
Loss in iteration 136 : 1.153582942723819
Loss in iteration 137 : 1.2240604874736314
Loss in iteration 138 : 1.2092326629054204
Loss in iteration 139 : 1.2973338393810625
Loss in iteration 140 : 1.1835658925813377
Loss in iteration 141 : 1.2170328280309515
Loss in iteration 142 : 1.200554606670659
Loss in iteration 143 : 1.3246265980261094
Loss in iteration 144 : 1.2405100523990018
Loss in iteration 145 : 1.2360101180337717
Loss in iteration 146 : 1.2200468647478784
Loss in iteration 147 : 1.205237118048925
Loss in iteration 148 : 1.1722313665729505
Loss in iteration 149 : 1.2686181756074757
Loss in iteration 150 : 1.1929984296203269
Loss in iteration 151 : 1.275542850452387
Loss in iteration 152 : 1.258418030763865
Loss in iteration 153 : 1.2574599164350375
Loss in iteration 154 : 1.3032203784447876
Loss in iteration 155 : 1.30933826447754
Loss in iteration 156 : 1.2564694285564983
Loss in iteration 157 : 1.4056082691008256
Loss in iteration 158 : 1.3042345606668595
Loss in iteration 159 : 1.288868059457908
Loss in iteration 160 : 1.1532614101863234
Loss in iteration 161 : 1.1521135517441943
Loss in iteration 162 : 1.1554146432810157
Loss in iteration 163 : 1.1837362296162397
Loss in iteration 164 : 1.2160170216042905
Loss in iteration 165 : 1.260139002425471
Loss in iteration 166 : 1.1373622816846625
Loss in iteration 167 : 1.1207443838805868
Loss in iteration 168 : 1.113520051326663
Loss in iteration 169 : 1.109567450366509
Loss in iteration 170 : 1.1715676025299324
Loss in iteration 171 : 1.176753552551393
Loss in iteration 172 : 1.0902171636500864
Loss in iteration 173 : 1.1141470293417355
Loss in iteration 174 : 1.0686779023876671
Loss in iteration 175 : 1.1981250735746318
Loss in iteration 176 : 1.0566590207044941
Loss in iteration 177 : 1.160666016724159
Loss in iteration 178 : 1.115947836149051
Loss in iteration 179 : 1.204248880921942
Loss in iteration 180 : 1.1654288001110082
Loss in iteration 181 : 1.2803839634530139
Loss in iteration 182 : 1.2241503846418273
Loss in iteration 183 : 1.2842044408069357
Loss in iteration 184 : 1.1626832681072259
Loss in iteration 185 : 1.1645215856780509
Loss in iteration 186 : 1.0690569979897628
Loss in iteration 187 : 1.0700851267810783
Loss in iteration 188 : 1.0379239088439824
Loss in iteration 189 : 1.0471575952821983
Loss in iteration 190 : 0.9996749851431587
Loss in iteration 191 : 0.9993479637260283
Loss in iteration 192 : 1.0606968971719963
Loss in iteration 193 : 1.1729969869824621
Loss in iteration 194 : 1.1325168608066272
Loss in iteration 195 : 1.1961726601231175
Loss in iteration 196 : 1.2343675946955561
Loss in iteration 197 : 1.1891499348475663
Loss in iteration 198 : 1.1523480308020666
Loss in iteration 199 : 1.1191754555388616
Loss in iteration 200 : 1.0305675705195014
Loss in iteration 201 : 1.1036558842410122
Loss in iteration 202 : 1.1672982885757262
Loss in iteration 203 : 1.1332407420327049
Loss in iteration 204 : 1.1071312510427642
Loss in iteration 205 : 1.1988870372586677
Loss in iteration 206 : 1.177037387772891
Loss in iteration 207 : 1.1284554993793254
Loss in iteration 208 : 1.1064691396809403
Loss in iteration 209 : 1.125132456831279
Loss in iteration 210 : 1.1024182627404329
Loss in iteration 211 : 1.0881277703351377
Loss in iteration 212 : 1.056243242638667
Loss in iteration 213 : 1.0351721223402905
Loss in iteration 214 : 1.0544016437417008
Loss in iteration 215 : 1.0750340834777723
Loss in iteration 216 : 1.0542696314543907
Loss in iteration 217 : 1.0720985794053826
Loss in iteration 218 : 0.9923600304330698
Loss in iteration 219 : 1.0546155336853058
Loss in iteration 220 : 1.0309977817901335
Loss in iteration 221 : 1.020423626868901
Loss in iteration 222 : 0.9468425445139496
Loss in iteration 223 : 1.0166440538731099
Loss in iteration 224 : 1.0628593124638734
Loss in iteration 225 : 1.0830638786951798
Loss in iteration 226 : 0.9961591355248235
Loss in iteration 227 : 1.0086798592210655
Loss in iteration 228 : 1.0017026340878894
Loss in iteration 229 : 1.0274671441511498
Loss in iteration 230 : 0.9945796201211071
Loss in iteration 231 : 1.0809017684779159
Loss in iteration 232 : 1.021265111809619
Loss in iteration 233 : 1.069727240729149
Loss in iteration 234 : 1.0584307905954686
Loss in iteration 235 : 1.0420456938196256
Loss in iteration 236 : 0.9697352487031069
Loss in iteration 237 : 0.990584859794348
Loss in iteration 238 : 0.9092874351969314
Loss in iteration 239 : 0.9691076464982541
Loss in iteration 240 : 0.8678038760536806
Loss in iteration 241 : 0.9651298533968419
Loss in iteration 242 : 0.9925083738914918
Loss in iteration 243 : 1.0531547370072236
Loss in iteration 244 : 0.9941628764343846
Loss in iteration 245 : 1.121597667167594
Loss in iteration 246 : 1.0295231486585157
Loss in iteration 247 : 1.0086806981029188
Loss in iteration 248 : 1.0496112669034414
Loss in iteration 249 : 1.0546954928804453
Loss in iteration 250 : 1.020073462608879
Loss in iteration 251 : 1.0859874921251538
Loss in iteration 252 : 1.060818314733215
Loss in iteration 253 : 1.1145302758643052
Loss in iteration 254 : 1.0270663283072332
Loss in iteration 255 : 1.0702613177905242
Loss in iteration 256 : 1.1098765272878983
Loss in iteration 257 : 1.041945077257301
Loss in iteration 258 : 1.0198288590403877
Loss in iteration 259 : 1.0214428733310892
Loss in iteration 260 : 1.0179473528832286
Loss in iteration 261 : 1.0359796924261675
Loss in iteration 262 : 1.0864086383447205
Loss in iteration 263 : 1.0854234741511675
Loss in iteration 264 : 1.0780964205435133
Loss in iteration 265 : 1.0218827795038246
Loss in iteration 266 : 0.9482970142452004
Loss in iteration 267 : 0.8876644499330388
Loss in iteration 268 : 0.9176325317555809
Loss in iteration 269 : 0.9080689260883228
Loss in iteration 270 : 0.9490326364607249
Loss in iteration 271 : 0.9750355435740902
Loss in iteration 272 : 1.0121690777549726
Loss in iteration 273 : 1.0234898014560514
Loss in iteration 274 : 0.987953304433765
Loss in iteration 275 : 1.0231203552746642
Loss in iteration 276 : 1.0012323195897121
Loss in iteration 277 : 1.000298090814487
Loss in iteration 278 : 0.9732568174420472
Loss in iteration 279 : 0.9943769294279321
Loss in iteration 280 : 0.995007537945562
Loss in iteration 281 : 0.9672470602910274
Loss in iteration 282 : 0.9275213627173902
Loss in iteration 283 : 0.9627107480412667
Loss in iteration 284 : 0.936488525996036
Loss in iteration 285 : 1.0095453260296658
Loss in iteration 286 : 0.9811846054770766
Loss in iteration 287 : 0.9824610698914581
Loss in iteration 288 : 0.933236441463201
Loss in iteration 289 : 0.8550906989815473
Loss in iteration 290 : 0.8020630286599453
Loss in iteration 291 : 0.8484679116824746
Loss in iteration 292 : 0.8763899759192826
Loss in iteration 293 : 0.9255435585600441
Loss in iteration 294 : 0.9361451861446582
Loss in iteration 295 : 0.9363451876316526
Loss in iteration 296 : 0.9640959785364107
Loss in iteration 297 : 1.0411702348848384
Loss in iteration 298 : 1.0042299141597602
Loss in iteration 299 : 1.025304028037262
Loss in iteration 300 : 1.031545514441651
Loss in iteration 301 : 1.0516887399879393
Loss in iteration 302 : 0.9803891716712941
Loss in iteration 303 : 0.9426845450845398
Loss in iteration 304 : 0.9355351776513523
Loss in iteration 305 : 0.9270461903064307
Loss in iteration 306 : 0.8615310566895619
Loss in iteration 307 : 0.8842715167736264
Loss in iteration 308 : 0.9068622833699423
Loss in iteration 309 : 0.9132424633623386
Loss in iteration 310 : 0.9121836887621737
Loss in iteration 311 : 0.9614300619810058
Loss in iteration 312 : 0.9847497273569629
Loss in iteration 313 : 0.9122739927922192
Loss in iteration 314 : 0.8639357251275039
Loss in iteration 315 : 0.8822256924045445
Loss in iteration 316 : 0.9001494753539137
Loss in iteration 317 : 0.8806814430528128
Loss in iteration 318 : 0.9291184701450661
Loss in iteration 319 : 0.9118040329459738
Loss in iteration 320 : 0.9329085372816959
Loss in iteration 321 : 0.9895832056862156
Loss in iteration 322 : 0.9718637295562848
Loss in iteration 323 : 0.9408241490892991
Loss in iteration 324 : 0.9305900205534334
Loss in iteration 325 : 0.9017242584506943
Loss in iteration 326 : 0.9650901435672773
Loss in iteration 327 : 0.8922240429017622
Loss in iteration 328 : 0.8626713337202192
Loss in iteration 329 : 0.8947232929587005
Loss in iteration 330 : 0.9042246928346432
Loss in iteration 331 : 0.9079790826681026
Loss in iteration 332 : 0.8484935984370113
Loss in iteration 333 : 0.8575376954984968
Loss in iteration 334 : 0.8615798385118605
Loss in iteration 335 : 0.8913944010702609
Loss in iteration 336 : 0.8581650516234869
Loss in iteration 337 : 0.9174119585344388
Loss in iteration 338 : 0.9227370288356723
Loss in iteration 339 : 0.9471048309236108
Loss in iteration 340 : 0.8529643949964825
Loss in iteration 341 : 0.8481286859612647
Loss in iteration 342 : 0.815503126938612
Loss in iteration 343 : 0.8631319272271161
Loss in iteration 344 : 0.8413774770291746
Loss in iteration 345 : 0.8693432199578699
Loss in iteration 346 : 0.8393855215497114
Loss in iteration 347 : 0.8547995179809245
Loss in iteration 348 : 0.8579711289737272
Loss in iteration 349 : 0.9274942215679629
Loss in iteration 350 : 0.9487122922554285
Loss in iteration 351 : 0.9359649605226816
Loss in iteration 352 : 0.8744557062778158
Loss in iteration 353 : 0.8221827329311101
Loss in iteration 354 : 0.8499509609165681
Loss in iteration 355 : 0.8529480256028753
Loss in iteration 356 : 0.8394402131455894
Loss in iteration 357 : 0.8632980650381432
Loss in iteration 358 : 0.8739894014933546
Loss in iteration 359 : 0.8369845116694703
Loss in iteration 360 : 0.8958440333804933
Loss in iteration 361 : 0.8918858310303016
Loss in iteration 362 : 0.8560905480851411
Loss in iteration 363 : 0.8766422764457421
Loss in iteration 364 : 0.8485247862477323
Loss in iteration 365 : 0.9216321042354898
Loss in iteration 366 : 0.8757272177810407
Loss in iteration 367 : 0.9075619228556121
Loss in iteration 368 : 0.8365125726058806
Loss in iteration 369 : 0.8599772001908648
Loss in iteration 370 : 0.8723305272220678
Loss in iteration 371 : 0.9408162700472512
Loss in iteration 372 : 0.8581487861989028
Loss in iteration 373 : 0.9092337350242632
Loss in iteration 374 : 0.9415400951928787
Loss in iteration 375 : 0.9307617022242505
Loss in iteration 376 : 0.9298733026329298
Loss in iteration 377 : 0.9121559555331823
Loss in iteration 378 : 0.8752175393739127
Loss in iteration 379 : 0.8580820567906666
Loss in iteration 380 : 0.8618594027235511
Loss in iteration 381 : 0.8576126828903227
Loss in iteration 382 : 0.8571502636850789
Loss in iteration 383 : 0.9019942984106869
Loss in iteration 384 : 0.8535968069547621
Loss in iteration 385 : 0.8312668709999536
Loss in iteration 386 : 0.7803885779654892
Loss in iteration 387 : 0.8493383304539505
Loss in iteration 388 : 0.8219215579540741
Loss in iteration 389 : 0.7858884080582705
Loss in iteration 390 : 0.7426452715195551
Loss in iteration 391 : 0.7783679905507491
Loss in iteration 392 : 0.77816897685751
Loss in iteration 393 : 0.8134555336772332
Loss in iteration 394 : 0.7871671844527061
Loss in iteration 395 : 0.8295816753330404
Loss in iteration 396 : 0.8166727864769596
Loss in iteration 397 : 0.8509585449786434
Loss in iteration 398 : 0.8383832435060216
Loss in iteration 399 : 0.8308533988142479
Loss in iteration 400 : 0.7954246804791748
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.747375, training accuracy 0.747375, time elapsed: 6078 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.7389171742361543
Loss in iteration 3 : 1.6162869843433278
Loss in iteration 4 : 2.9489502547345854
Loss in iteration 5 : 1.0178316628796968
Loss in iteration 6 : 2.236945006839578
Loss in iteration 7 : 1.4181060162377728
Loss in iteration 8 : 2.3858994168268386
Loss in iteration 9 : 0.9245633640013444
Loss in iteration 10 : 1.8060670618173413
Loss in iteration 11 : 1.3494864150774568
Loss in iteration 12 : 1.9985043123459243
Loss in iteration 13 : 0.9563918175269919
Loss in iteration 14 : 1.5011714313813513
Loss in iteration 15 : 1.1924600695422691
Loss in iteration 16 : 1.625486283248424
Loss in iteration 17 : 1.0718518191120276
Loss in iteration 18 : 1.3919128340197207
Loss in iteration 19 : 1.0518645379783362
Loss in iteration 20 : 1.2563969016305785
Loss in iteration 21 : 1.097524632340693
Loss in iteration 22 : 1.1885709373658753
Loss in iteration 23 : 0.9984812031250271
Loss in iteration 24 : 1.0536815515379179
Loss in iteration 25 : 0.9416775460948731
Loss in iteration 26 : 0.9523885650665521
Loss in iteration 27 : 0.8623334302935394
Loss in iteration 28 : 0.9346493025689848
Loss in iteration 29 : 0.9123143346261345
Loss in iteration 30 : 0.9284953288942973
Loss in iteration 31 : 0.9168289671585397
Loss in iteration 32 : 0.952178065746898
Loss in iteration 33 : 0.8521462750943283
Loss in iteration 34 : 0.8622063022417639
Loss in iteration 35 : 0.8469722724780375
Loss in iteration 36 : 0.8433477506700138
Loss in iteration 37 : 0.800590738275598
Loss in iteration 38 : 0.81477250492933
Loss in iteration 39 : 0.7930244567437136
Loss in iteration 40 : 0.8001897510924764
Loss in iteration 41 : 0.7557668877704842
Loss in iteration 42 : 0.7083823965395374
Loss in iteration 43 : 0.6965565905615917
Loss in iteration 44 : 0.7074090066445479
Loss in iteration 45 : 0.7026534404402432
Loss in iteration 46 : 0.7128030973754407
Loss in iteration 47 : 0.7292788036783225
Loss in iteration 48 : 0.6989276449598444
Loss in iteration 49 : 0.6701226525343533
Loss in iteration 50 : 0.6581926948869434
Loss in iteration 51 : 0.6891926198353451
Loss in iteration 52 : 0.6878401502609618
Loss in iteration 53 : 0.6957172398704263
Loss in iteration 54 : 0.6823724250024333
Loss in iteration 55 : 0.673918326823739
Loss in iteration 56 : 0.6862329857321368
Loss in iteration 57 : 0.6587119221535739
Loss in iteration 58 : 0.6604224720583152
Loss in iteration 59 : 0.6467528980127311
Loss in iteration 60 : 0.6452954597561397
Loss in iteration 61 : 0.6229757770525297
Loss in iteration 62 : 0.6269840302744137
Loss in iteration 63 : 0.6174854474512627
Loss in iteration 64 : 0.6175936498519662
Loss in iteration 65 : 0.6370458017387407
Loss in iteration 66 : 0.6088385285439496
Loss in iteration 67 : 0.625902203993701
Loss in iteration 68 : 0.5880700407138474
Loss in iteration 69 : 0.5922873830151054
Loss in iteration 70 : 0.569312188798928
Loss in iteration 71 : 0.599006793159054
Loss in iteration 72 : 0.5881508140720493
Loss in iteration 73 : 0.5966164330725883
Loss in iteration 74 : 0.5771743639128243
Loss in iteration 75 : 0.5997019165292715
Loss in iteration 76 : 0.6084276714389083
Loss in iteration 77 : 0.6064581936276185
Loss in iteration 78 : 0.5806617331853579
Loss in iteration 79 : 0.5959530313532175
Loss in iteration 80 : 0.5988587198458313
Loss in iteration 81 : 0.6068638235820094
Loss in iteration 82 : 0.5823576127545993
Loss in iteration 83 : 0.6155568902006199
Loss in iteration 84 : 0.6217850340960834
Loss in iteration 85 : 0.5980477269949882
Loss in iteration 86 : 0.5809606326906868
Loss in iteration 87 : 0.5629669013829295
Loss in iteration 88 : 0.5811457748674967
Loss in iteration 89 : 0.5711504321291626
Loss in iteration 90 : 0.55637170698455
Loss in iteration 91 : 0.5374312233702441
Loss in iteration 92 : 0.5139008918656067
Loss in iteration 93 : 0.5225983153210527
Loss in iteration 94 : 0.5113176620690261
Loss in iteration 95 : 0.4971028100091945
Loss in iteration 96 : 0.506565785816626
Loss in iteration 97 : 0.49074749422327063
Loss in iteration 98 : 0.49967827500881945
Loss in iteration 99 : 0.4985122078533002
Loss in iteration 100 : 0.5125374807503482
Loss in iteration 101 : 0.5147206111783535
Loss in iteration 102 : 0.5101749723026058
Loss in iteration 103 : 0.5463980971224289
Loss in iteration 104 : 0.5526686031027036
Loss in iteration 105 : 0.5642938324964994
Loss in iteration 106 : 0.558969195314763
Loss in iteration 107 : 0.5926007258883396
Loss in iteration 108 : 0.5891429427663658
Loss in iteration 109 : 0.6048521388769775
Loss in iteration 110 : 0.6064499622457985
Loss in iteration 111 : 0.6312888021491502
Loss in iteration 112 : 0.5721300833247749
Loss in iteration 113 : 0.5618653898355632
Loss in iteration 114 : 0.5929961696530878
Loss in iteration 115 : 0.6126894129982069
Loss in iteration 116 : 0.6232919947508953
Loss in iteration 117 : 0.6549192308996039
Loss in iteration 118 : 0.6471805987798704
Loss in iteration 119 : 0.6130980446704095
Loss in iteration 120 : 0.6548407456886725
Loss in iteration 121 : 0.6537674093305743
Loss in iteration 122 : 0.6617190226333409
Loss in iteration 123 : 0.614249413611676
Loss in iteration 124 : 0.6162002132300907
Loss in iteration 125 : 0.6232311772642746
Loss in iteration 126 : 0.623789416031342
Loss in iteration 127 : 0.6273380302627767
Loss in iteration 128 : 0.6203181351059724
Loss in iteration 129 : 0.596320048209349
Loss in iteration 130 : 0.6076175447698428
Loss in iteration 131 : 0.5853960943791349
Loss in iteration 132 : 0.5673143567490795
Loss in iteration 133 : 0.5752909382846281
Loss in iteration 134 : 0.5869002088770607
Loss in iteration 135 : 0.5829396919012914
Loss in iteration 136 : 0.569950643961251
Loss in iteration 137 : 0.5714159061766781
Loss in iteration 138 : 0.5738990138561152
Loss in iteration 139 : 0.5869416021246135
Loss in iteration 140 : 0.571280898215277
Loss in iteration 141 : 0.5629427131423663
Loss in iteration 142 : 0.5633820878494894
Loss in iteration 143 : 0.5783577278905472
Loss in iteration 144 : 0.571908793782989
Loss in iteration 145 : 0.5570051151491795
Loss in iteration 146 : 0.5616923055584402
Loss in iteration 147 : 0.5505293474342624
Loss in iteration 148 : 0.5452489816470462
Loss in iteration 149 : 0.5601070894144451
Loss in iteration 150 : 0.5484678201288067
Loss in iteration 151 : 0.5509066329062745
Loss in iteration 152 : 0.573234815869355
Loss in iteration 153 : 0.5539885398993722
Loss in iteration 154 : 0.5848564267320804
Loss in iteration 155 : 0.5741374270234314
Loss in iteration 156 : 0.5622364891001961
Loss in iteration 157 : 0.6040642327622964
Loss in iteration 158 : 0.6076511174424231
Loss in iteration 159 : 0.59979593095514
Loss in iteration 160 : 0.5809685637076581
Loss in iteration 161 : 0.5665768600599107
Loss in iteration 162 : 0.564721927499338
Loss in iteration 163 : 0.5630156196572559
Loss in iteration 164 : 0.5757529583192453
Loss in iteration 165 : 0.5776474027592055
Loss in iteration 166 : 0.5738203918403081
Loss in iteration 167 : 0.5586723940042068
Loss in iteration 168 : 0.5562681519992456
Loss in iteration 169 : 0.5388169055816756
Loss in iteration 170 : 0.5559519905822755
Loss in iteration 171 : 0.5378129557287038
Loss in iteration 172 : 0.5465490504453732
Loss in iteration 173 : 0.5391656973792895
Loss in iteration 174 : 0.5365532929964479
Loss in iteration 175 : 0.5587687806285485
Loss in iteration 176 : 0.5383242538591232
Loss in iteration 177 : 0.5500951638835486
Loss in iteration 178 : 0.547765857602854
Loss in iteration 179 : 0.562452408390213
Loss in iteration 180 : 0.55355407729748
Loss in iteration 181 : 0.568664902720671
Loss in iteration 182 : 0.569553305212003
Loss in iteration 183 : 0.5701685167353726
Loss in iteration 184 : 0.5706022387906521
Loss in iteration 185 : 0.5473577451071816
Loss in iteration 186 : 0.5350595445471841
Loss in iteration 187 : 0.5239126020277477
Loss in iteration 188 : 0.530114854513398
Loss in iteration 189 : 0.527158917816101
Loss in iteration 190 : 0.5104050801647467
Loss in iteration 191 : 0.5120869949463892
Loss in iteration 192 : 0.5276870401579263
Loss in iteration 193 : 0.544192246752944
Loss in iteration 194 : 0.5467354461012752
Loss in iteration 195 : 0.5548680662104664
Loss in iteration 196 : 0.5745543884989983
Loss in iteration 197 : 0.5445435402471974
Loss in iteration 198 : 0.5658772196995245
Loss in iteration 199 : 0.5416383346356796
Loss in iteration 200 : 0.5250953309194667
Loss in iteration 201 : 0.5399755292344165
Loss in iteration 202 : 0.565013355201272
Loss in iteration 203 : 0.544855831064874
Loss in iteration 204 : 0.5608434195583247
Loss in iteration 205 : 0.5780985492835062
Loss in iteration 206 : 0.5876513602720124
Loss in iteration 207 : 0.568886995918477
Loss in iteration 208 : 0.5731350895837986
Loss in iteration 209 : 0.5703745147848843
Loss in iteration 210 : 0.5691439714483472
Loss in iteration 211 : 0.5556082284130285
Loss in iteration 212 : 0.5510594710554566
Loss in iteration 213 : 0.5359165194496487
Loss in iteration 214 : 0.5481485242780835
Loss in iteration 215 : 0.5394107705728441
Loss in iteration 216 : 0.5340904335459039
Loss in iteration 217 : 0.5246602795786474
Loss in iteration 218 : 0.5047329704834985
Loss in iteration 219 : 0.5209395270537702
Loss in iteration 220 : 0.5229360436178582
Loss in iteration 221 : 0.5104798834073037
Loss in iteration 222 : 0.49246481786634294
Loss in iteration 223 : 0.5101291092949731
Loss in iteration 224 : 0.5248375854347559
Loss in iteration 225 : 0.5178492304745766
Loss in iteration 226 : 0.5161066734043596
Loss in iteration 227 : 0.5003991348223932
Loss in iteration 228 : 0.5131937957869264
Loss in iteration 229 : 0.5118988286251426
Loss in iteration 230 : 0.5063829882158974
Loss in iteration 231 : 0.5186739664982982
Loss in iteration 232 : 0.5224002593333155
Loss in iteration 233 : 0.5250648109003032
Loss in iteration 234 : 0.5398062485846443
Loss in iteration 235 : 0.5198626302820151
Loss in iteration 236 : 0.5191903358820713
Loss in iteration 237 : 0.5113500975711739
Loss in iteration 238 : 0.5017166712297442
Loss in iteration 239 : 0.49829465973545894
Loss in iteration 240 : 0.4871940604567058
Loss in iteration 241 : 0.4996503066707883
Loss in iteration 242 : 0.5342325649888051
Loss in iteration 243 : 0.5244665716097895
Loss in iteration 244 : 0.5194946748238315
Loss in iteration 245 : 0.5459877452910663
Loss in iteration 246 : 0.5281716404390108
Loss in iteration 247 : 0.5176906810852713
Loss in iteration 248 : 0.5337911415921202
Loss in iteration 249 : 0.5333310207935837
Loss in iteration 250 : 0.5261678016594294
Loss in iteration 251 : 0.5412696833148785
Loss in iteration 252 : 0.5370954621922732
Loss in iteration 253 : 0.5490297081610171
Loss in iteration 254 : 0.5241933013687601
Loss in iteration 255 : 0.5345036034513043
Loss in iteration 256 : 0.5675883378440951
Loss in iteration 257 : 0.5335289087771881
Loss in iteration 258 : 0.5447194299947847
Loss in iteration 259 : 0.5362762354181275
Loss in iteration 260 : 0.5405143560553219
Loss in iteration 261 : 0.5341549335121604
Loss in iteration 262 : 0.555097750687068
Loss in iteration 263 : 0.5535953528911342
Loss in iteration 264 : 0.5703076878331114
Loss in iteration 265 : 0.5358655069593448
Loss in iteration 266 : 0.5381846483777399
Loss in iteration 267 : 0.5170564934430394
Loss in iteration 268 : 0.5244479631264342
Loss in iteration 269 : 0.5054588486324484
Loss in iteration 270 : 0.521456756069065
Loss in iteration 271 : 0.5161054542227964
Loss in iteration 272 : 0.5394908555477422
Loss in iteration 273 : 0.5238078271674295
Loss in iteration 274 : 0.5340543368281692
Loss in iteration 275 : 0.5401087663611621
Loss in iteration 276 : 0.5482626186619527
Loss in iteration 277 : 0.5402313008629466
Loss in iteration 278 : 0.5401144107031606
Loss in iteration 279 : 0.5378777790840114
Loss in iteration 280 : 0.5445288131294814
Loss in iteration 281 : 0.5363586787502045
Loss in iteration 282 : 0.5267043522817025
Loss in iteration 283 : 0.533402542564977
Loss in iteration 284 : 0.5238635056662534
Loss in iteration 285 : 0.5358441770501335
Loss in iteration 286 : 0.5437841278647902
Loss in iteration 287 : 0.5371252827669595
Loss in iteration 288 : 0.526723150354358
Loss in iteration 289 : 0.4995709553329389
Loss in iteration 290 : 0.4831472865692028
Loss in iteration 291 : 0.4945915372271094
Loss in iteration 292 : 0.4958172440207463
Loss in iteration 293 : 0.49866884529130273
Loss in iteration 294 : 0.5101426229235525
Loss in iteration 295 : 0.49604844878688675
Loss in iteration 296 : 0.5089823820050151
Loss in iteration 297 : 0.5289279464493202
Loss in iteration 298 : 0.5222467357009638
Loss in iteration 299 : 0.5258412484044677
Loss in iteration 300 : 0.5377115700744873
Loss in iteration 301 : 0.5402222846384092
Loss in iteration 302 : 0.5349479367656501
Loss in iteration 303 : 0.5229871951103771
Loss in iteration 304 : 0.5283539056032255
Loss in iteration 305 : 0.5201482934129401
Loss in iteration 306 : 0.5073961607086305
Loss in iteration 307 : 0.5084296100446459
Loss in iteration 308 : 0.5158686805342523
Loss in iteration 309 : 0.5088903592598054
Loss in iteration 310 : 0.517119326800413
Loss in iteration 311 : 0.527266900862987
Loss in iteration 312 : 0.5425893666289762
Loss in iteration 313 : 0.5045509927883586
Loss in iteration 314 : 0.5057914872593352
Loss in iteration 315 : 0.5032746466132716
Loss in iteration 316 : 0.5193786676916589
Loss in iteration 317 : 0.5039288139792043
Loss in iteration 318 : 0.5224814611896061
Loss in iteration 319 : 0.5036646464531339
Loss in iteration 320 : 0.5117635362007701
Loss in iteration 321 : 0.521166876277391
Loss in iteration 322 : 0.5290916434967728
Loss in iteration 323 : 0.5185542709378425
Loss in iteration 324 : 0.5180161892214302
Loss in iteration 325 : 0.508421982363286
Loss in iteration 326 : 0.532652720469359
Loss in iteration 327 : 0.5019016080364681
Loss in iteration 328 : 0.4998377485798169
Loss in iteration 329 : 0.5049543672257102
Loss in iteration 330 : 0.5080777219279685
Loss in iteration 331 : 0.5109956659349442
Loss in iteration 332 : 0.4914936012078299
Loss in iteration 333 : 0.4914369131455588
Loss in iteration 334 : 0.503415706322638
Loss in iteration 335 : 0.5000857724100171
Loss in iteration 336 : 0.507519879316069
Loss in iteration 337 : 0.5192379906605102
Loss in iteration 338 : 0.5278716850487082
Loss in iteration 339 : 0.5249041997754309
Loss in iteration 340 : 0.5068591845584687
Loss in iteration 341 : 0.4920270090078571
Loss in iteration 342 : 0.4924847509477801
Loss in iteration 343 : 0.49277091043941534
Loss in iteration 344 : 0.49823935538750785
Loss in iteration 345 : 0.49293503643732073
Loss in iteration 346 : 0.49055003381795087
Loss in iteration 347 : 0.49437912917160504
Loss in iteration 348 : 0.5047231965622432
Loss in iteration 349 : 0.5155440048664776
Loss in iteration 350 : 0.5263825663384326
Loss in iteration 351 : 0.5195162345286027
Loss in iteration 352 : 0.5098186120767612
Loss in iteration 353 : 0.4912325539683721
Loss in iteration 354 : 0.5007574514582195
Loss in iteration 355 : 0.5000682489491157
Loss in iteration 356 : 0.4939456934469208
Loss in iteration 357 : 0.4969810491552948
Loss in iteration 358 : 0.5019944538557817
Loss in iteration 359 : 0.4849564998226919
Loss in iteration 360 : 0.5010307303539538
Loss in iteration 361 : 0.48726728266892744
Loss in iteration 362 : 0.5075141981182468
Loss in iteration 363 : 0.49905367590279576
Loss in iteration 364 : 0.49620039586775627
Loss in iteration 365 : 0.5046248775730611
Loss in iteration 366 : 0.5014202879595757
Loss in iteration 367 : 0.5070416946069951
Loss in iteration 368 : 0.4953542019014487
Loss in iteration 369 : 0.49900129638421614
Loss in iteration 370 : 0.4909761624879384
Loss in iteration 371 : 0.5139650527451626
Loss in iteration 372 : 0.4924393305210238
Loss in iteration 373 : 0.503936448149015
Loss in iteration 374 : 0.5160927279620716
Loss in iteration 375 : 0.5151751734454555
Loss in iteration 376 : 0.5278501761501715
Loss in iteration 377 : 0.516815012436188
Loss in iteration 378 : 0.5158632819211714
Loss in iteration 379 : 0.5109081604673148
Loss in iteration 380 : 0.5067585329097845
Loss in iteration 381 : 0.4956544919152665
Loss in iteration 382 : 0.5060509479022862
Loss in iteration 383 : 0.521013599623584
Loss in iteration 384 : 0.5246264908000174
Loss in iteration 385 : 0.5081346121088817
Loss in iteration 386 : 0.4903202009666203
Loss in iteration 387 : 0.5103287620054957
Loss in iteration 388 : 0.5111912701261135
Loss in iteration 389 : 0.4893992863967551
Loss in iteration 390 : 0.48216564073734997
Loss in iteration 391 : 0.4868668792983352
Loss in iteration 392 : 0.4770005646658945
Loss in iteration 393 : 0.48326415220947544
Loss in iteration 394 : 0.48396660611804443
Loss in iteration 395 : 0.4928340312543743
Loss in iteration 396 : 0.4853182700665762
Loss in iteration 397 : 0.49638172175411277
Loss in iteration 398 : 0.4882908292726579
Loss in iteration 399 : 0.47304989232779093
Loss in iteration 400 : 0.4637208497518064
Testing accuracy  of updater 3 on alg 0 with rate 7.0 = 0.781625, training accuracy 0.781625, time elapsed: 5215 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6789901685911698
Loss in iteration 3 : 0.6793236229775977
Loss in iteration 4 : 0.7122925152467762
Loss in iteration 5 : 0.721320427250585
Loss in iteration 6 : 0.7657712845814815
Loss in iteration 7 : 0.7290812886987844
Loss in iteration 8 : 0.7457578621822429
Loss in iteration 9 : 0.6734854299159757
Loss in iteration 10 : 0.6877803638065165
Loss in iteration 11 : 0.6513302231814979
Loss in iteration 12 : 0.6536113567886155
Loss in iteration 13 : 0.6099464613157766
Loss in iteration 14 : 0.605796043638536
Loss in iteration 15 : 0.5921290608776812
Loss in iteration 16 : 0.5865806301672568
Loss in iteration 17 : 0.5735311931116942
Loss in iteration 18 : 0.5690057332635361
Loss in iteration 19 : 0.5577852306105197
Loss in iteration 20 : 0.5511594180630487
Loss in iteration 21 : 0.5537340096145141
Loss in iteration 22 : 0.5469715545667644
Loss in iteration 23 : 0.5414231409800653
Loss in iteration 24 : 0.529532588620237
Loss in iteration 25 : 0.5309030473158246
Loss in iteration 26 : 0.5204466018465773
Loss in iteration 27 : 0.5192001425681703
Loss in iteration 28 : 0.520891676373037
Loss in iteration 29 : 0.5248706378667753
Loss in iteration 30 : 0.5236934739404386
Loss in iteration 31 : 0.5190925175992765
Loss in iteration 32 : 0.5177501902217583
Loss in iteration 33 : 0.5118845689508135
Loss in iteration 34 : 0.5110668171434918
Loss in iteration 35 : 0.515969156876035
Loss in iteration 36 : 0.5136813181584775
Loss in iteration 37 : 0.5094764540201544
Loss in iteration 38 : 0.5079689716136772
Loss in iteration 39 : 0.5052792712731904
Loss in iteration 40 : 0.507378684544374
Loss in iteration 41 : 0.5091190572468536
Loss in iteration 42 : 0.49989106912366094
Loss in iteration 43 : 0.5064398889055881
Loss in iteration 44 : 0.5054519706725991
Loss in iteration 45 : 0.5048217262912161
Loss in iteration 46 : 0.5041147029082123
Loss in iteration 47 : 0.5012895419885312
Loss in iteration 48 : 0.5051617403068713
Loss in iteration 49 : 0.49953098872366175
Loss in iteration 50 : 0.4946615616312751
Loss in iteration 51 : 0.504999935331917
Loss in iteration 52 : 0.4955161602960675
Loss in iteration 53 : 0.4953440525861037
Loss in iteration 54 : 0.491993998601962
Loss in iteration 55 : 0.494666604300976
Loss in iteration 56 : 0.4979325924341074
Loss in iteration 57 : 0.4859615358053265
Loss in iteration 58 : 0.48897482902940903
Loss in iteration 59 : 0.4888337327783869
Loss in iteration 60 : 0.49234388671823925
Loss in iteration 61 : 0.49241938465264995
Loss in iteration 62 : 0.4916871308363078
Loss in iteration 63 : 0.4899180998103962
Loss in iteration 64 : 0.4886091408210544
Loss in iteration 65 : 0.49683028507557875
Loss in iteration 66 : 0.4841808327833456
Loss in iteration 67 : 0.489789501952812
Loss in iteration 68 : 0.483176338193795
Loss in iteration 69 : 0.4920048845200423
Loss in iteration 70 : 0.4849760403983301
Loss in iteration 71 : 0.49797869767331965
Loss in iteration 72 : 0.4841246614024096
Loss in iteration 73 : 0.48982613093798055
Loss in iteration 74 : 0.4857564288806779
Loss in iteration 75 : 0.48579181702655816
Loss in iteration 76 : 0.4933237272029338
Loss in iteration 77 : 0.4819583229657293
Loss in iteration 78 : 0.48846711373117385
Loss in iteration 79 : 0.4738709553217499
Loss in iteration 80 : 0.48546925643212846
Loss in iteration 81 : 0.4877324434239774
Loss in iteration 82 : 0.4835597205080911
Loss in iteration 83 : 0.48667743604986174
Loss in iteration 84 : 0.4921554908958818
Loss in iteration 85 : 0.48533300890121583
Loss in iteration 86 : 0.49079915943712316
Loss in iteration 87 : 0.4750906032170505
Loss in iteration 88 : 0.49211033719366576
Loss in iteration 89 : 0.4778095081542775
Loss in iteration 90 : 0.4815403050612722
Loss in iteration 91 : 0.4875333839372724
Loss in iteration 92 : 0.4768854570987266
Loss in iteration 93 : 0.48092516777635713
Loss in iteration 94 : 0.48161949424862466
Loss in iteration 95 : 0.4772656095912696
Loss in iteration 96 : 0.4885690527390371
Loss in iteration 97 : 0.47969406307029583
Loss in iteration 98 : 0.4833679453271468
Loss in iteration 99 : 0.480224691990054
Loss in iteration 100 : 0.48461522137710533
Loss in iteration 101 : 0.47672114994525305
Loss in iteration 102 : 0.47662922557398313
Loss in iteration 103 : 0.48424100888512983
Loss in iteration 104 : 0.48067881776490257
Loss in iteration 105 : 0.46972171194014906
Loss in iteration 106 : 0.4796050568244937
Loss in iteration 107 : 0.4878717845777768
Loss in iteration 108 : 0.4920494231552804
Loss in iteration 109 : 0.4785619561469348
Loss in iteration 110 : 0.47374486532513027
Loss in iteration 111 : 0.4814944523330044
Loss in iteration 112 : 0.4757062490142229
Loss in iteration 113 : 0.47254570690597203
Loss in iteration 114 : 0.47120185363418926
Loss in iteration 115 : 0.47049865621249515
Loss in iteration 116 : 0.4776595220447434
Loss in iteration 117 : 0.4858717409400678
Loss in iteration 118 : 0.48337936746296883
Loss in iteration 119 : 0.4700033724048146
Loss in iteration 120 : 0.48376236714300497
Loss in iteration 121 : 0.481693872407003
Loss in iteration 122 : 0.48475164331665416
Loss in iteration 123 : 0.47848698883995866
Loss in iteration 124 : 0.47203269099455814
Loss in iteration 125 : 0.47177358210316483
Loss in iteration 126 : 0.47252255280500055
Loss in iteration 127 : 0.4732838704817401
Loss in iteration 128 : 0.4774933137626333
Loss in iteration 129 : 0.4718603513061643
Loss in iteration 130 : 0.4770966263593055
Loss in iteration 131 : 0.4727611183171069
Loss in iteration 132 : 0.4723256764766517
Loss in iteration 133 : 0.47673200466607185
Loss in iteration 134 : 0.47638150331540613
Loss in iteration 135 : 0.4744554067147908
Loss in iteration 136 : 0.47433066177937316
Loss in iteration 137 : 0.4767514955585098
Loss in iteration 138 : 0.4770636799791484
Loss in iteration 139 : 0.48250684472849276
Loss in iteration 140 : 0.47718802858668086
Loss in iteration 141 : 0.47843038991362596
Loss in iteration 142 : 0.47047846481156563
Loss in iteration 143 : 0.47433470907267056
Loss in iteration 144 : 0.4705096295499899
Loss in iteration 145 : 0.4647020951390028
Loss in iteration 146 : 0.4775080875393827
Loss in iteration 147 : 0.48260240521713627
Loss in iteration 148 : 0.47273339274980175
Loss in iteration 149 : 0.48421579761297584
Loss in iteration 150 : 0.47725538137323137
Loss in iteration 151 : 0.47128144638971836
Loss in iteration 152 : 0.4808971443102633
Loss in iteration 153 : 0.4618413577337647
Loss in iteration 154 : 0.48033920222719695
Loss in iteration 155 : 0.47633886205659914
Loss in iteration 156 : 0.46118880842591947
Loss in iteration 157 : 0.47498513947888044
Loss in iteration 158 : 0.46815293684459836
Loss in iteration 159 : 0.47062820625819257
Loss in iteration 160 : 0.46843121003934785
Loss in iteration 161 : 0.46887957324026097
Loss in iteration 162 : 0.47102562957656197
Loss in iteration 163 : 0.4755735431097101
Loss in iteration 164 : 0.4744171646102541
Loss in iteration 165 : 0.46801666238807
Loss in iteration 166 : 0.4698960314561881
Loss in iteration 167 : 0.4796412177458795
Loss in iteration 168 : 0.47929785849570444
Loss in iteration 169 : 0.47046707245047364
Loss in iteration 170 : 0.4764336371860736
Loss in iteration 171 : 0.4657711760905378
Loss in iteration 172 : 0.4742370377028435
Loss in iteration 173 : 0.46962725298862645
Loss in iteration 174 : 0.46530233461868603
Loss in iteration 175 : 0.4790771856088008
Loss in iteration 176 : 0.46597082716140237
Loss in iteration 177 : 0.47770212896782444
Loss in iteration 178 : 0.4669677422284568
Loss in iteration 179 : 0.4759056218792289
Loss in iteration 180 : 0.4655152315188166
Loss in iteration 181 : 0.4732868773115297
Loss in iteration 182 : 0.46986649169322553
Loss in iteration 183 : 0.46837308998859933
Loss in iteration 184 : 0.48259899762762276
Loss in iteration 185 : 0.46899803457247397
Loss in iteration 186 : 0.4617889511663137
Loss in iteration 187 : 0.46472607793508525
Loss in iteration 188 : 0.47575779814306485
Loss in iteration 189 : 0.47448429053638
Loss in iteration 190 : 0.4615840741443974
Loss in iteration 191 : 0.46426634090090335
Loss in iteration 192 : 0.467036955102136
Loss in iteration 193 : 0.4716204974248282
Loss in iteration 194 : 0.4704365461066884
Loss in iteration 195 : 0.47711171954444703
Loss in iteration 196 : 0.48286121357454037
Loss in iteration 197 : 0.4605324179903676
Loss in iteration 198 : 0.47834955582327193
Loss in iteration 199 : 0.46853235703862034
Loss in iteration 200 : 0.46177332140486266
Loss in iteration 201 : 0.4687044781362379
Loss in iteration 202 : 0.4764982137784842
Loss in iteration 203 : 0.4597907239829476
Loss in iteration 204 : 0.4653938913735254
Loss in iteration 205 : 0.47002395071694664
Loss in iteration 206 : 0.46880947733373146
Loss in iteration 207 : 0.46522772638086407
Loss in iteration 208 : 0.46049473827480525
Loss in iteration 209 : 0.47241969107373283
Loss in iteration 210 : 0.47730444443988757
Loss in iteration 211 : 0.4709445264209284
Loss in iteration 212 : 0.4752447793580543
Loss in iteration 213 : 0.475951544986437
Loss in iteration 214 : 0.48781093530850644
Loss in iteration 215 : 0.4774174785794769
Loss in iteration 216 : 0.46917344028953956
Loss in iteration 217 : 0.46662233873808845
Loss in iteration 218 : 0.46347469004064984
Loss in iteration 219 : 0.47535378156558633
Loss in iteration 220 : 0.4740122111604462
Loss in iteration 221 : 0.4714083373733951
Loss in iteration 222 : 0.46194445771338066
Loss in iteration 223 : 0.47485256243099644
Loss in iteration 224 : 0.4743445646442853
Loss in iteration 225 : 0.4670193384993995
Loss in iteration 226 : 0.4701296253891627
Loss in iteration 227 : 0.460718054360134
Loss in iteration 228 : 0.47034106908739143
Loss in iteration 229 : 0.4706391118145483
Loss in iteration 230 : 0.46253685147448614
Loss in iteration 231 : 0.47115718749092406
Loss in iteration 232 : 0.4713039687028393
Loss in iteration 233 : 0.4665952771599256
Loss in iteration 234 : 0.47262316245901825
Loss in iteration 235 : 0.46316126376131306
Loss in iteration 236 : 0.4687859341671908
Loss in iteration 237 : 0.4690885100254985
Loss in iteration 238 : 0.4682022010405422
Loss in iteration 239 : 0.4682568542175854
Loss in iteration 240 : 0.4614128654907327
Loss in iteration 241 : 0.4633103178824586
Loss in iteration 242 : 0.48830345403729336
Loss in iteration 243 : 0.4759329749900926
Loss in iteration 244 : 0.4665905814812964
Loss in iteration 245 : 0.4793555184193645
Loss in iteration 246 : 0.4655267128699963
Loss in iteration 247 : 0.4606459142300793
Loss in iteration 248 : 0.4683618920848017
Loss in iteration 249 : 0.4712447794773009
Loss in iteration 250 : 0.46149171331612976
Loss in iteration 251 : 0.472573338880409
Loss in iteration 252 : 0.46604330281768885
Loss in iteration 253 : 0.47616046280049196
Loss in iteration 254 : 0.45391279621360375
Loss in iteration 255 : 0.45964633179229064
Loss in iteration 256 : 0.4803267636436831
Loss in iteration 257 : 0.4579749263450493
Loss in iteration 258 : 0.46289714179039515
Loss in iteration 259 : 0.4619052770431226
Loss in iteration 260 : 0.4656421560001217
Loss in iteration 261 : 0.4551329540751193
Loss in iteration 262 : 0.46523265367492345
Loss in iteration 263 : 0.4665194017746034
Loss in iteration 264 : 0.4798978143551127
Loss in iteration 265 : 0.4586489445418914
Loss in iteration 266 : 0.4750896237039282
Loss in iteration 267 : 0.4724897118608563
Loss in iteration 268 : 0.4767237810529499
Loss in iteration 269 : 0.46090387241436803
Loss in iteration 270 : 0.4665638260994445
Loss in iteration 271 : 0.45525820189478217
Loss in iteration 272 : 0.46664073971696274
Loss in iteration 273 : 0.45238391820399015
Loss in iteration 274 : 0.46089052301336486
Loss in iteration 275 : 0.469274729945827
Loss in iteration 276 : 0.4720906418610485
Loss in iteration 277 : 0.4600601493253181
Loss in iteration 278 : 0.458234590716806
Loss in iteration 279 : 0.45613177109327646
Loss in iteration 280 : 0.4615255591893869
Loss in iteration 281 : 0.46926200124376455
Loss in iteration 282 : 0.46220549752426976
Loss in iteration 283 : 0.4686531916084976
Loss in iteration 284 : 0.45383800558149057
Loss in iteration 285 : 0.45650985662599075
Loss in iteration 286 : 0.46502811641056374
Loss in iteration 287 : 0.4687200356042114
Loss in iteration 288 : 0.4683767878199397
Loss in iteration 289 : 0.4612414216330681
Loss in iteration 290 : 0.4580102909410253
Loss in iteration 291 : 0.4699160376250196
Loss in iteration 292 : 0.46684904285324247
Loss in iteration 293 : 0.46184242735829806
Loss in iteration 294 : 0.4705733462712895
Loss in iteration 295 : 0.45791294919849934
Loss in iteration 296 : 0.46067132741591965
Loss in iteration 297 : 0.4694481548101459
Loss in iteration 298 : 0.4656269726165437
Loss in iteration 299 : 0.46680602324331805
Loss in iteration 300 : 0.47055399663418623
Loss in iteration 301 : 0.4686799797506083
Loss in iteration 302 : 0.4637543547749745
Loss in iteration 303 : 0.45755387915501095
Loss in iteration 304 : 0.461703874387787
Loss in iteration 305 : 0.4623608839979402
Loss in iteration 306 : 0.46280266879361487
Loss in iteration 307 : 0.4645452200041329
Loss in iteration 308 : 0.4668237076153512
Loss in iteration 309 : 0.45719399060733007
Loss in iteration 310 : 0.46306170203698266
Loss in iteration 311 : 0.4721280944702014
Loss in iteration 312 : 0.48130460886445003
Loss in iteration 313 : 0.45270495351161893
Loss in iteration 314 : 0.4605705432663606
Loss in iteration 315 : 0.4624074086206281
Loss in iteration 316 : 0.4787644622181641
Loss in iteration 317 : 0.4639125346733018
Loss in iteration 318 : 0.4750925992141539
Loss in iteration 319 : 0.45958100764891263
Loss in iteration 320 : 0.4584130875644423
Loss in iteration 321 : 0.4602528509236354
Loss in iteration 322 : 0.46326326899591674
Loss in iteration 323 : 0.4608136804128959
Loss in iteration 324 : 0.46305654399729257
Loss in iteration 325 : 0.4616150779093099
Loss in iteration 326 : 0.4765643320760757
Loss in iteration 327 : 0.45602943403780727
Loss in iteration 328 : 0.4588201378834091
Loss in iteration 329 : 0.4597735612245696
Loss in iteration 330 : 0.4557407704906187
Loss in iteration 331 : 0.46393276863144267
Loss in iteration 332 : 0.4511965836275783
Loss in iteration 333 : 0.452627175112587
Loss in iteration 334 : 0.463158291346345
Loss in iteration 335 : 0.4569518391625837
Loss in iteration 336 : 0.46352716754743273
Loss in iteration 337 : 0.46639026193253025
Loss in iteration 338 : 0.4721402728499938
Loss in iteration 339 : 0.4745182576822081
Loss in iteration 340 : 0.4692016462269961
Loss in iteration 341 : 0.4615534502927752
Loss in iteration 342 : 0.46286942897733657
Loss in iteration 343 : 0.4623990521652871
Loss in iteration 344 : 0.46867025594794004
Loss in iteration 345 : 0.462567631854359
Loss in iteration 346 : 0.45815009489372427
Loss in iteration 347 : 0.4643336429165807
Loss in iteration 348 : 0.46621389918655776
Loss in iteration 349 : 0.47055277872914647
Loss in iteration 350 : 0.4692731934783881
Loss in iteration 351 : 0.46448132677792525
Loss in iteration 352 : 0.4615994815394078
Loss in iteration 353 : 0.4568313045931655
Loss in iteration 354 : 0.46349395139415756
Loss in iteration 355 : 0.46328600085743954
Loss in iteration 356 : 0.4570526603592431
Loss in iteration 357 : 0.4611457790201969
Loss in iteration 358 : 0.46906198572048835
Loss in iteration 359 : 0.4612701707159909
Loss in iteration 360 : 0.46920231786559313
Loss in iteration 361 : 0.4553537949361145
Loss in iteration 362 : 0.47289985736259676
Loss in iteration 363 : 0.4643813452633398
Loss in iteration 364 : 0.4631079981151314
Loss in iteration 365 : 0.4664444442598028
Loss in iteration 366 : 0.4610169335939959
Loss in iteration 367 : 0.46917586496895397
Loss in iteration 368 : 0.4706041367679865
Loss in iteration 369 : 0.47598264414589436
Loss in iteration 370 : 0.46045214901800596
Loss in iteration 371 : 0.47778585819649333
Loss in iteration 372 : 0.45658488087650956
Loss in iteration 373 : 0.46417298135946006
Loss in iteration 374 : 0.4621168012130318
Loss in iteration 375 : 0.4605638612840752
Loss in iteration 376 : 0.4700536711141877
Loss in iteration 377 : 0.4638167151156371
Loss in iteration 378 : 0.46755850313935216
Loss in iteration 379 : 0.4660327531765817
Loss in iteration 380 : 0.4608628367191343
Loss in iteration 381 : 0.4531901321998049
Loss in iteration 382 : 0.45874195997659933
Loss in iteration 383 : 0.465683160345352
Loss in iteration 384 : 0.47106948675463256
Loss in iteration 385 : 0.4642794972934273
Loss in iteration 386 : 0.4526072215672565
Loss in iteration 387 : 0.4682215120609873
Loss in iteration 388 : 0.47248554117510866
Loss in iteration 389 : 0.45960375991098673
Loss in iteration 390 : 0.46181961529835625
Loss in iteration 391 : 0.464758243180303
Loss in iteration 392 : 0.4542943009846855
Loss in iteration 393 : 0.45650765695935785
Loss in iteration 394 : 0.4610090540503326
Loss in iteration 395 : 0.46859070567227856
Loss in iteration 396 : 0.46244269969451884
Loss in iteration 397 : 0.474521239247173
Loss in iteration 398 : 0.4716725580705406
Loss in iteration 399 : 0.4602747506411529
Loss in iteration 400 : 0.45514807007961033
Testing accuracy  of updater 3 on alg 0 with rate 4.0 = 0.79, training accuracy 0.79, time elapsed: 4920 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6834418926287374
Loss in iteration 3 : 0.6747517695046055
Loss in iteration 4 : 0.672337854580794
Loss in iteration 5 : 0.665641385452001
Loss in iteration 6 : 0.664201099655861
Loss in iteration 7 : 0.6588245386075967
Loss in iteration 8 : 0.6538315354822889
Loss in iteration 9 : 0.6517593804892703
Loss in iteration 10 : 0.6492797812178895
Loss in iteration 11 : 0.6422847623842955
Loss in iteration 12 : 0.6403972407508125
Loss in iteration 13 : 0.6378588105483036
Loss in iteration 14 : 0.6332580062684653
Loss in iteration 15 : 0.632039923147197
Loss in iteration 16 : 0.6272517014489407
Loss in iteration 17 : 0.6236980398419899
Loss in iteration 18 : 0.6214536465758504
Loss in iteration 19 : 0.6206868430685255
Loss in iteration 20 : 0.6177873948912013
Loss in iteration 21 : 0.6165130815208961
Loss in iteration 22 : 0.6139326066743911
Loss in iteration 23 : 0.6129687520617898
Loss in iteration 24 : 0.6062617656771843
Loss in iteration 25 : 0.6074191693593499
Loss in iteration 26 : 0.6011572256284244
Loss in iteration 27 : 0.6001962061283455
Loss in iteration 28 : 0.6005525627784822
Loss in iteration 29 : 0.6009074470462095
Loss in iteration 30 : 0.6000595559492106
Loss in iteration 31 : 0.5969663452464791
Loss in iteration 32 : 0.5954612317134869
Loss in iteration 33 : 0.5916742303710225
Loss in iteration 34 : 0.5897654333102086
Loss in iteration 35 : 0.5905557138097962
Loss in iteration 36 : 0.5867399390476377
Loss in iteration 37 : 0.584327288043792
Loss in iteration 38 : 0.5825768047586084
Loss in iteration 39 : 0.581071939919448
Loss in iteration 40 : 0.5808541111399177
Loss in iteration 41 : 0.5798196104607404
Loss in iteration 42 : 0.5750830746219806
Loss in iteration 43 : 0.5777121229546125
Loss in iteration 44 : 0.5754576935015616
Loss in iteration 45 : 0.5747176318886464
Loss in iteration 46 : 0.5734715690740028
Loss in iteration 47 : 0.5710305137023554
Loss in iteration 48 : 0.5724342608520268
Loss in iteration 49 : 0.5678411890350462
Loss in iteration 50 : 0.5656024017779054
Loss in iteration 51 : 0.568753177720703
Loss in iteration 52 : 0.5636882046052318
Loss in iteration 53 : 0.5631080600589564
Loss in iteration 54 : 0.5587464256764356
Loss in iteration 55 : 0.5592910401510305
Loss in iteration 56 : 0.5622582553201068
Loss in iteration 57 : 0.5546479742268243
Loss in iteration 58 : 0.5550580140825454
Loss in iteration 59 : 0.555025401380601
Loss in iteration 60 : 0.5562147868646306
Loss in iteration 61 : 0.5567153486886135
Loss in iteration 62 : 0.554184157576446
Loss in iteration 63 : 0.5527738192288387
Loss in iteration 64 : 0.5512076746489156
Loss in iteration 65 : 0.5543716043013203
Loss in iteration 66 : 0.5475445575761708
Loss in iteration 67 : 0.5478315799528563
Loss in iteration 68 : 0.5449630968894695
Loss in iteration 69 : 0.5504811344108504
Loss in iteration 70 : 0.5454624736217092
Loss in iteration 71 : 0.5531456528071976
Loss in iteration 72 : 0.5448048804924542
Loss in iteration 73 : 0.5453855050291695
Loss in iteration 74 : 0.5443518262232738
Loss in iteration 75 : 0.5432472668618075
Loss in iteration 76 : 0.5464476849152624
Loss in iteration 77 : 0.5412800241660413
Loss in iteration 78 : 0.5433014441071851
Loss in iteration 79 : 0.5319623032483849
Loss in iteration 80 : 0.5387158597676076
Loss in iteration 81 : 0.5418892728830886
Loss in iteration 82 : 0.5369437893833208
Loss in iteration 83 : 0.5411837121226141
Loss in iteration 84 : 0.5408891763455723
Loss in iteration 85 : 0.5383979532310696
Loss in iteration 86 : 0.5410783696362437
Loss in iteration 87 : 0.5305590467135006
Loss in iteration 88 : 0.5413974641590276
Loss in iteration 89 : 0.5312613410682746
Loss in iteration 90 : 0.5345237234855671
Loss in iteration 91 : 0.537396409726284
Loss in iteration 92 : 0.5295008583238557
Loss in iteration 93 : 0.5302525990490305
Loss in iteration 94 : 0.5326367653682871
Loss in iteration 95 : 0.5283508478045073
Loss in iteration 96 : 0.5367034623430963
Loss in iteration 97 : 0.5292551405135155
Loss in iteration 98 : 0.5302183579241583
Loss in iteration 99 : 0.5293329324897199
Loss in iteration 100 : 0.532744201908466
Loss in iteration 101 : 0.5261521472079886
Loss in iteration 102 : 0.5259354506561816
Loss in iteration 103 : 0.5303655889006088
Loss in iteration 104 : 0.527272368034704
Loss in iteration 105 : 0.5219143466360073
Loss in iteration 106 : 0.5254854725596041
Loss in iteration 107 : 0.5296245923800934
Loss in iteration 108 : 0.534079627994061
Loss in iteration 109 : 0.5252112260140385
Loss in iteration 110 : 0.5215660614740948
Loss in iteration 111 : 0.5258574935188614
Loss in iteration 112 : 0.5217076728474314
Loss in iteration 113 : 0.5196884693440265
Loss in iteration 114 : 0.5182271655186139
Loss in iteration 115 : 0.5178257033856744
Loss in iteration 116 : 0.5208682171867289
Loss in iteration 117 : 0.5260266730660639
Loss in iteration 118 : 0.5246935170144964
Loss in iteration 119 : 0.5165295751754015
Loss in iteration 120 : 0.5246039602535391
Loss in iteration 121 : 0.524446652389331
Loss in iteration 122 : 0.5239266443383876
Loss in iteration 123 : 0.5191005265627918
Loss in iteration 124 : 0.5158281123166413
Loss in iteration 125 : 0.5168719160889548
Loss in iteration 126 : 0.5147842645512134
Loss in iteration 127 : 0.5160325440735296
Loss in iteration 128 : 0.5177196877637484
Loss in iteration 129 : 0.5143276942309851
Loss in iteration 130 : 0.517522230392438
Loss in iteration 131 : 0.5139227839972073
Loss in iteration 132 : 0.514663522304419
Loss in iteration 133 : 0.5156657588902541
Loss in iteration 134 : 0.5167131688416408
Loss in iteration 135 : 0.5148409144189157
Loss in iteration 136 : 0.51464676401692
Loss in iteration 137 : 0.5141679068071852
Loss in iteration 138 : 0.5164751898615293
Loss in iteration 139 : 0.5179035592735293
Loss in iteration 140 : 0.5154546014690362
Loss in iteration 141 : 0.5162356469291298
Loss in iteration 142 : 0.5114473313738065
Loss in iteration 143 : 0.5118246225610863
Loss in iteration 144 : 0.5104547058893415
Loss in iteration 145 : 0.5059501315567263
Loss in iteration 146 : 0.5132187421741345
Loss in iteration 147 : 0.5168213979717913
Loss in iteration 148 : 0.5105544146255805
Loss in iteration 149 : 0.5175802880446686
Loss in iteration 150 : 0.5137861451058984
Loss in iteration 151 : 0.5097118506091429
Loss in iteration 152 : 0.514406169935224
Loss in iteration 153 : 0.5008738121822073
Loss in iteration 154 : 0.5141222644604411
Loss in iteration 155 : 0.5122804946410008
Loss in iteration 156 : 0.5024992442877999
Loss in iteration 157 : 0.5106076224550071
Loss in iteration 158 : 0.503700903094784
Loss in iteration 159 : 0.5065567458958227
Loss in iteration 160 : 0.5041179772640955
Loss in iteration 161 : 0.5069731152469716
Loss in iteration 162 : 0.5062084523128787
Loss in iteration 163 : 0.5095629310712689
Loss in iteration 164 : 0.5091011252537821
Loss in iteration 165 : 0.5036307327800174
Loss in iteration 166 : 0.504725089351073
Loss in iteration 167 : 0.5126440973734565
Loss in iteration 168 : 0.5103270309798522
Loss in iteration 169 : 0.506762274676915
Loss in iteration 170 : 0.5093829579706626
Loss in iteration 171 : 0.5020197927950254
Loss in iteration 172 : 0.5062314697377337
Loss in iteration 173 : 0.5047669415507634
Loss in iteration 174 : 0.5007422691774767
Loss in iteration 175 : 0.5106602145792418
Loss in iteration 176 : 0.5009680135723099
Loss in iteration 177 : 0.5085390389836029
Loss in iteration 178 : 0.5012163530062471
Loss in iteration 179 : 0.5074698105271744
Loss in iteration 180 : 0.4995802930942025
Loss in iteration 181 : 0.5045930079544577
Loss in iteration 182 : 0.5028942185676148
Loss in iteration 183 : 0.5010696670425342
Loss in iteration 184 : 0.5099943038379366
Loss in iteration 185 : 0.501322913374766
Loss in iteration 186 : 0.4975501868840066
Loss in iteration 187 : 0.49730778065843734
Loss in iteration 188 : 0.5039360956039505
Loss in iteration 189 : 0.5058167056017528
Loss in iteration 190 : 0.4970616886939439
Loss in iteration 191 : 0.4983722076339708
Loss in iteration 192 : 0.499012793078461
Loss in iteration 193 : 0.4999124623684666
Loss in iteration 194 : 0.5019023152498985
Loss in iteration 195 : 0.5067176529751077
Loss in iteration 196 : 0.5101796639743051
Loss in iteration 197 : 0.4942450766082193
Loss in iteration 198 : 0.5075869472734577
Loss in iteration 199 : 0.4995567773928214
Loss in iteration 200 : 0.4952720339338386
Loss in iteration 201 : 0.5013984550037445
Loss in iteration 202 : 0.5024856445875402
Loss in iteration 203 : 0.49245894833768944
Loss in iteration 204 : 0.49505525847436027
Loss in iteration 205 : 0.4996475120622624
Loss in iteration 206 : 0.49744983496914513
Loss in iteration 207 : 0.49712034251984183
Loss in iteration 208 : 0.4916329944763427
Loss in iteration 209 : 0.5016418037966776
Loss in iteration 210 : 0.5043603607961485
Loss in iteration 211 : 0.5003072534700285
Loss in iteration 212 : 0.503300589721351
Loss in iteration 213 : 0.5032809286295551
Loss in iteration 214 : 0.5117481826664296
Loss in iteration 215 : 0.504037933009278
Loss in iteration 216 : 0.4994842078481124
Loss in iteration 217 : 0.49580441876038805
Loss in iteration 218 : 0.4928216796841919
Loss in iteration 219 : 0.5001233215201711
Loss in iteration 220 : 0.49907638454817743
Loss in iteration 221 : 0.4986497604830261
Loss in iteration 222 : 0.490120189911601
Loss in iteration 223 : 0.5028868879993683
Loss in iteration 224 : 0.4989661912256697
Loss in iteration 225 : 0.49554865586938396
Loss in iteration 226 : 0.496668758829099
Loss in iteration 227 : 0.49016769936941584
Loss in iteration 228 : 0.4960528992419268
Loss in iteration 229 : 0.4974759532690433
Loss in iteration 230 : 0.4893151686702607
Loss in iteration 231 : 0.49645945342434056
Loss in iteration 232 : 0.4962156695630936
Loss in iteration 233 : 0.49457676530902406
Loss in iteration 234 : 0.4989516676980202
Loss in iteration 235 : 0.4893568255060883
Loss in iteration 236 : 0.4933543212728474
Loss in iteration 237 : 0.49567949154393887
Loss in iteration 238 : 0.4937519639997454
Loss in iteration 239 : 0.4918901600619391
Loss in iteration 240 : 0.4902205494034323
Loss in iteration 241 : 0.489595603155211
Loss in iteration 242 : 0.5102923670118191
Loss in iteration 243 : 0.5005043684464987
Loss in iteration 244 : 0.49265677769687466
Loss in iteration 245 : 0.50194531235419
Loss in iteration 246 : 0.4925979457421844
Loss in iteration 247 : 0.48851968609437485
Loss in iteration 248 : 0.49312423247533843
Loss in iteration 249 : 0.49578363098475525
Loss in iteration 250 : 0.48856163446235995
Loss in iteration 251 : 0.49743107948131154
Loss in iteration 252 : 0.49104862230461654
Loss in iteration 253 : 0.49822803625072254
Loss in iteration 254 : 0.48265389222665117
Loss in iteration 255 : 0.4869836145721553
Loss in iteration 256 : 0.5013541318941332
Loss in iteration 257 : 0.48637385824000606
Loss in iteration 258 : 0.48818792487335294
Loss in iteration 259 : 0.4865748673577629
Loss in iteration 260 : 0.4919693973129691
Loss in iteration 261 : 0.4835886396212451
Loss in iteration 262 : 0.48784733695562493
Loss in iteration 263 : 0.491529395784032
Loss in iteration 264 : 0.5018693860309431
Loss in iteration 265 : 0.485357904061234
Loss in iteration 266 : 0.4969943168349573
Loss in iteration 267 : 0.4943948520710828
Loss in iteration 268 : 0.49760167171251163
Loss in iteration 269 : 0.486813573540475
Loss in iteration 270 : 0.4919484803485628
Loss in iteration 271 : 0.4825517401827013
Loss in iteration 272 : 0.4916305247011646
Loss in iteration 273 : 0.47942164438855817
Loss in iteration 274 : 0.48534615596704767
Loss in iteration 275 : 0.49034870844950756
Loss in iteration 276 : 0.493061615944809
Loss in iteration 277 : 0.4846480925071292
Loss in iteration 278 : 0.48356718499455226
Loss in iteration 279 : 0.48306444175426316
Loss in iteration 280 : 0.4857539439569481
Loss in iteration 281 : 0.4937366883627852
Loss in iteration 282 : 0.48675656466970996
Loss in iteration 283 : 0.4915893293176906
Loss in iteration 284 : 0.47958973453442355
Loss in iteration 285 : 0.48347978984339396
Loss in iteration 286 : 0.48772296976310336
Loss in iteration 287 : 0.4896226508766006
Loss in iteration 288 : 0.49059623017927984
Loss in iteration 289 : 0.4850899333227021
Loss in iteration 290 : 0.4812827980609508
Loss in iteration 291 : 0.48986108826188757
Loss in iteration 292 : 0.48986473773751243
Loss in iteration 293 : 0.4841551249733842
Loss in iteration 294 : 0.4920969099855795
Loss in iteration 295 : 0.4815971847331452
Loss in iteration 296 : 0.4835586221208348
Loss in iteration 297 : 0.4888257179359991
Loss in iteration 298 : 0.489213033312174
Loss in iteration 299 : 0.48802855231866693
Loss in iteration 300 : 0.49207337144462104
Loss in iteration 301 : 0.4905376390093545
Loss in iteration 302 : 0.48665387216991157
Loss in iteration 303 : 0.4821179799201033
Loss in iteration 304 : 0.4845727558853191
Loss in iteration 305 : 0.4825201512877961
Loss in iteration 306 : 0.48411443024290596
Loss in iteration 307 : 0.48678935142635815
Loss in iteration 308 : 0.4882810065822401
Loss in iteration 309 : 0.48197571064317885
Loss in iteration 310 : 0.4850322527985901
Loss in iteration 311 : 0.4923528815286575
Loss in iteration 312 : 0.4981214650374972
Loss in iteration 313 : 0.47645237915722566
Loss in iteration 314 : 0.48272249574772097
Loss in iteration 315 : 0.48247473061536206
Loss in iteration 316 : 0.4965213127173538
Loss in iteration 317 : 0.4828109867862627
Loss in iteration 318 : 0.494221300235314
Loss in iteration 319 : 0.4824211660961355
Loss in iteration 320 : 0.4802201052499858
Loss in iteration 321 : 0.4828878296140953
Loss in iteration 322 : 0.4824709440022442
Loss in iteration 323 : 0.48277747639733687
Loss in iteration 324 : 0.48390399169199655
Loss in iteration 325 : 0.48251306344205525
Loss in iteration 326 : 0.49319053245669275
Loss in iteration 327 : 0.47760622568599803
Loss in iteration 328 : 0.4815856187563389
Loss in iteration 329 : 0.4822106466504348
Loss in iteration 330 : 0.4763994206854775
Loss in iteration 331 : 0.4834127140885244
Loss in iteration 332 : 0.4736448920308456
Loss in iteration 333 : 0.4759197285665943
Loss in iteration 334 : 0.485244277340179
Loss in iteration 335 : 0.47713635872931315
Loss in iteration 336 : 0.48323691902472615
Loss in iteration 337 : 0.4844375210702254
Loss in iteration 338 : 0.49089182408070314
Loss in iteration 339 : 0.4928602777787272
Loss in iteration 340 : 0.48759615958974967
Loss in iteration 341 : 0.4809813019892501
Loss in iteration 342 : 0.4836042170315797
Loss in iteration 343 : 0.48284984029608635
Loss in iteration 344 : 0.4878730017216733
Loss in iteration 345 : 0.48372790614742106
Loss in iteration 346 : 0.4793700836335965
Loss in iteration 347 : 0.48576673581790963
Loss in iteration 348 : 0.4849011183060442
Loss in iteration 349 : 0.4891416517242432
Loss in iteration 350 : 0.4853548720706468
Loss in iteration 351 : 0.48281603842345183
Loss in iteration 352 : 0.48121948839452405
Loss in iteration 353 : 0.4782322933710958
Loss in iteration 354 : 0.4833929379487332
Loss in iteration 355 : 0.4823614475790741
Loss in iteration 356 : 0.47606452018238005
Loss in iteration 357 : 0.48146573237428464
Loss in iteration 358 : 0.48585558542210106
Loss in iteration 359 : 0.4809592257744406
Loss in iteration 360 : 0.4870906180283252
Loss in iteration 361 : 0.4763084565566325
Loss in iteration 362 : 0.4891153305898701
Loss in iteration 363 : 0.4826460634590689
Loss in iteration 364 : 0.48429015937577113
Loss in iteration 365 : 0.48423578331939987
Loss in iteration 366 : 0.48026534014997885
Loss in iteration 367 : 0.4849128297684805
Loss in iteration 368 : 0.4876847030227839
Loss in iteration 369 : 0.4934676590122571
Loss in iteration 370 : 0.4795174178493785
Loss in iteration 371 : 0.4948670325767295
Loss in iteration 372 : 0.47494259813256645
Loss in iteration 373 : 0.4830253924796078
Loss in iteration 374 : 0.47959378296048233
Loss in iteration 375 : 0.4803010083087662
Loss in iteration 376 : 0.488262911378297
Loss in iteration 377 : 0.48232706025335814
Loss in iteration 378 : 0.48499394300040605
Loss in iteration 379 : 0.4825959890539136
Loss in iteration 380 : 0.47969790596146894
Loss in iteration 381 : 0.4750214543331596
Loss in iteration 382 : 0.477987927215272
Loss in iteration 383 : 0.48139905006734673
Loss in iteration 384 : 0.48662660098285515
Loss in iteration 385 : 0.48196329438625
Loss in iteration 386 : 0.4731895446421348
Loss in iteration 387 : 0.4838094798393318
Loss in iteration 388 : 0.48918983967824387
Loss in iteration 389 : 0.4762187025679717
Loss in iteration 390 : 0.48015502562646106
Loss in iteration 391 : 0.4823918470858742
Loss in iteration 392 : 0.47306207838072517
Loss in iteration 393 : 0.47376734684877947
Loss in iteration 394 : 0.4791930045691993
Loss in iteration 395 : 0.48474940919630277
Loss in iteration 396 : 0.4800513682023617
Loss in iteration 397 : 0.48974551912361736
Loss in iteration 398 : 0.4873519820402527
Loss in iteration 399 : 0.4754816421044306
Loss in iteration 400 : 0.47468168775761516
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.781375, training accuracy 0.781375, time elapsed: 4967 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6859323983497716
Loss in iteration 3 : 0.6789186085924028
Loss in iteration 4 : 0.6769153775814908
Loss in iteration 5 : 0.6716903860518382
Loss in iteration 6 : 0.6707927727130573
Loss in iteration 7 : 0.6662620626973235
Loss in iteration 8 : 0.6621401114829417
Loss in iteration 9 : 0.6608399481151108
Loss in iteration 10 : 0.6593526165078453
Loss in iteration 11 : 0.6532420613414119
Loss in iteration 12 : 0.6519069981803809
Loss in iteration 13 : 0.6502642730921269
Loss in iteration 14 : 0.6464106108854158
Loss in iteration 15 : 0.6451945178663131
Loss in iteration 16 : 0.6413208305873895
Loss in iteration 17 : 0.6382651310241894
Loss in iteration 18 : 0.6365625036493524
Loss in iteration 19 : 0.6358876199123892
Loss in iteration 20 : 0.6341153544769829
Loss in iteration 21 : 0.6322602435348984
Loss in iteration 22 : 0.6299174079568346
Loss in iteration 23 : 0.629402595631303
Loss in iteration 24 : 0.6235759106338354
Loss in iteration 25 : 0.6247224935669382
Loss in iteration 26 : 0.619030323355109
Loss in iteration 27 : 0.6184694609763111
Loss in iteration 28 : 0.6189055818829643
Loss in iteration 29 : 0.618682287967044
Loss in iteration 30 : 0.6181965544952647
Loss in iteration 31 : 0.6155852970060823
Loss in iteration 32 : 0.6143985579985659
Loss in iteration 33 : 0.6111394811241523
Loss in iteration 34 : 0.609416678090699
Loss in iteration 35 : 0.6096931527703868
Loss in iteration 36 : 0.605959960045073
Loss in iteration 37 : 0.6037995692395428
Loss in iteration 38 : 0.6024787619199709
Loss in iteration 39 : 0.6013175774905463
Loss in iteration 40 : 0.6010099122493776
Loss in iteration 41 : 0.5995768813041038
Loss in iteration 42 : 0.5956576993105517
Loss in iteration 43 : 0.5977623201210509
Loss in iteration 44 : 0.5955385734359281
Loss in iteration 45 : 0.5949273692815124
Loss in iteration 46 : 0.5936128533058154
Loss in iteration 47 : 0.5913943272330807
Loss in iteration 48 : 0.5924080911824666
Loss in iteration 49 : 0.5884345766790757
Loss in iteration 50 : 0.5867490211729817
Loss in iteration 51 : 0.5886222008666839
Loss in iteration 52 : 0.5845179638335697
Loss in iteration 53 : 0.5838706491268025
Loss in iteration 54 : 0.579691624435605
Loss in iteration 55 : 0.5798182996572783
Loss in iteration 56 : 0.5827457292585106
Loss in iteration 57 : 0.57639622452914
Loss in iteration 58 : 0.5764946774986365
Loss in iteration 59 : 0.576175628248775
Loss in iteration 60 : 0.5770821781504345
Loss in iteration 61 : 0.5775271028262646
Loss in iteration 62 : 0.5748309853098249
Loss in iteration 63 : 0.5735142909348341
Loss in iteration 64 : 0.5719462733963626
Loss in iteration 65 : 0.5742328185937329
Loss in iteration 66 : 0.568622315876338
Loss in iteration 67 : 0.5682750325854644
Loss in iteration 68 : 0.5660306896093563
Loss in iteration 69 : 0.5706962162689757
Loss in iteration 70 : 0.5664106403655942
Loss in iteration 71 : 0.5725427813944578
Loss in iteration 72 : 0.5656711265883796
Loss in iteration 73 : 0.5651957157551768
Loss in iteration 74 : 0.5647304078111025
Loss in iteration 75 : 0.5635031945652296
Loss in iteration 76 : 0.5658853383955131
Loss in iteration 77 : 0.5620117956276544
Loss in iteration 78 : 0.5634100746552129
Loss in iteration 79 : 0.5528976775676326
Loss in iteration 80 : 0.5587590766334272
Loss in iteration 81 : 0.5618052895271718
Loss in iteration 82 : 0.5568773816191049
Loss in iteration 83 : 0.5609828180087065
Loss in iteration 84 : 0.5596971045066219
Loss in iteration 85 : 0.5581329428659129
Loss in iteration 86 : 0.5602561991716997
Loss in iteration 87 : 0.5507103127252556
Loss in iteration 88 : 0.5602381776572948
Loss in iteration 89 : 0.5512330775465888
Loss in iteration 90 : 0.5543986027431763
Loss in iteration 91 : 0.5563599887182752
Loss in iteration 92 : 0.5491842021401259
Loss in iteration 93 : 0.549486843311826
Loss in iteration 94 : 0.5518801723222749
Loss in iteration 95 : 0.5481274724793133
Loss in iteration 96 : 0.5553902539695484
Loss in iteration 97 : 0.5483324797213672
Loss in iteration 98 : 0.5489791695770347
Loss in iteration 99 : 0.5484326417685617
Loss in iteration 100 : 0.551587519029433
Loss in iteration 101 : 0.5452373067252034
Loss in iteration 102 : 0.545091017192286
Loss in iteration 103 : 0.5485098878856866
Loss in iteration 104 : 0.5458384167964384
Loss in iteration 105 : 0.5420155037616592
Loss in iteration 106 : 0.5441099791932927
Loss in iteration 107 : 0.5474094796870344
Loss in iteration 108 : 0.5514381835532562
Loss in iteration 109 : 0.5439517357007403
Loss in iteration 110 : 0.5406208810782832
Loss in iteration 111 : 0.5439601952857409
Loss in iteration 112 : 0.5401002706844418
Loss in iteration 113 : 0.5386811320131952
Loss in iteration 114 : 0.5370489037227947
Loss in iteration 115 : 0.5368651345304775
Loss in iteration 116 : 0.539006279139029
Loss in iteration 117 : 0.5430749551803357
Loss in iteration 118 : 0.5417939832364687
Loss in iteration 119 : 0.5351107096118127
Loss in iteration 120 : 0.5418126694360103
Loss in iteration 121 : 0.5419575122489366
Loss in iteration 122 : 0.5407674967751832
Loss in iteration 123 : 0.5365168066505817
Loss in iteration 124 : 0.5338408944314901
Loss in iteration 125 : 0.5351503259629142
Loss in iteration 126 : 0.5328646879495728
Loss in iteration 127 : 0.5338618525131485
Loss in iteration 128 : 0.5348516141105263
Loss in iteration 129 : 0.5321054022280827
Loss in iteration 130 : 0.5346925875216629
Loss in iteration 131 : 0.5313587482167511
Loss in iteration 132 : 0.5322691358480254
Loss in iteration 133 : 0.5325001835869827
Loss in iteration 134 : 0.5336580001162914
Loss in iteration 135 : 0.5320057237370387
Loss in iteration 136 : 0.5318599857139102
Loss in iteration 137 : 0.5308113252923278
Loss in iteration 138 : 0.5332566185239794
Loss in iteration 139 : 0.5335968979571657
Loss in iteration 140 : 0.5318851747356946
Loss in iteration 141 : 0.5326505658253878
Loss in iteration 142 : 0.5284028284355493
Loss in iteration 143 : 0.5283037434613714
Loss in iteration 144 : 0.5272127536040493
Loss in iteration 145 : 0.5232745641262531
Loss in iteration 146 : 0.5290225245527673
Loss in iteration 147 : 0.5323318778649829
Loss in iteration 148 : 0.5269706291752618
Loss in iteration 149 : 0.532864860720584
Loss in iteration 150 : 0.5294914400985026
Loss in iteration 151 : 0.5261668426735179
Loss in iteration 152 : 0.5296899209060314
Loss in iteration 153 : 0.5177402332925941
Loss in iteration 154 : 0.5293811144453204
Loss in iteration 155 : 0.527914834617933
Loss in iteration 156 : 0.519652366518064
Loss in iteration 157 : 0.5261793049521748
Loss in iteration 158 : 0.519883526898907
Loss in iteration 159 : 0.5226578712598784
Loss in iteration 160 : 0.5200216299947089
Loss in iteration 161 : 0.5231269824235668
Loss in iteration 162 : 0.5217541167661311
Loss in iteration 163 : 0.5250016842775104
Loss in iteration 164 : 0.5245203187460734
Loss in iteration 165 : 0.5195933071980298
Loss in iteration 166 : 0.5202826473859357
Loss in iteration 167 : 0.5273536199731386
Loss in iteration 168 : 0.5249353576774386
Loss in iteration 169 : 0.5223185671268753
Loss in iteration 170 : 0.5240217020929876
Loss in iteration 171 : 0.5179450786103726
Loss in iteration 172 : 0.5209937493225029
Loss in iteration 173 : 0.5198996397619645
Loss in iteration 174 : 0.5164144633539592
Loss in iteration 175 : 0.5248701115191028
Loss in iteration 176 : 0.5162619443536273
Loss in iteration 177 : 0.5225694164546689
Loss in iteration 178 : 0.5164658167839185
Loss in iteration 179 : 0.5217709087299672
Loss in iteration 180 : 0.5148281281662248
Loss in iteration 181 : 0.519457835366586
Loss in iteration 182 : 0.5179773328997943
Loss in iteration 183 : 0.5156644977545519
Loss in iteration 184 : 0.523237107621882
Loss in iteration 185 : 0.5159149464242626
Loss in iteration 186 : 0.5128605925843001
Loss in iteration 187 : 0.5120727797192569
Loss in iteration 188 : 0.5177899969686772
Loss in iteration 189 : 0.5198384552253857
Loss in iteration 190 : 0.5120422317832021
Loss in iteration 191 : 0.5135540080559118
Loss in iteration 192 : 0.5135782073932208
Loss in iteration 193 : 0.5137215724737034
Loss in iteration 194 : 0.516116486501614
Loss in iteration 195 : 0.520332313528419
Loss in iteration 196 : 0.5229251344028449
Loss in iteration 197 : 0.509340296042404
Loss in iteration 198 : 0.5208914996722143
Loss in iteration 199 : 0.5138482923284995
Loss in iteration 200 : 0.5100357007505453
Loss in iteration 201 : 0.5154970781483202
Loss in iteration 202 : 0.5153639163142739
Loss in iteration 203 : 0.507018056207944
Loss in iteration 204 : 0.509145213188242
Loss in iteration 205 : 0.5133841408026814
Loss in iteration 206 : 0.5110620979086222
Loss in iteration 207 : 0.5113387050382456
Loss in iteration 208 : 0.5058140273735798
Loss in iteration 209 : 0.5150113855286617
Loss in iteration 210 : 0.5171606584818547
Loss in iteration 211 : 0.5136557034025497
Loss in iteration 212 : 0.5162755540081636
Loss in iteration 213 : 0.5162721452182955
Loss in iteration 214 : 0.5235103477293362
Loss in iteration 215 : 0.5167640357216887
Loss in iteration 216 : 0.5130402081355868
Loss in iteration 217 : 0.5090180402092124
Loss in iteration 218 : 0.5065914954996207
Loss in iteration 219 : 0.5126019515338126
Loss in iteration 220 : 0.5119537501348816
Loss in iteration 221 : 0.5114209822395892
Loss in iteration 222 : 0.5039187289836866
Loss in iteration 223 : 0.515594502355471
Loss in iteration 224 : 0.5112241185406469
Loss in iteration 225 : 0.5089639120118268
Loss in iteration 226 : 0.5092805302259771
Loss in iteration 227 : 0.5038901612697947
Loss in iteration 228 : 0.508669111727629
Loss in iteration 229 : 0.5100876517866311
Loss in iteration 230 : 0.5024203506138867
Loss in iteration 231 : 0.5086880859960656
Loss in iteration 232 : 0.5086108038474058
Loss in iteration 233 : 0.507582532698783
Loss in iteration 234 : 0.5111173983135319
Loss in iteration 235 : 0.5023934747302721
Loss in iteration 236 : 0.5057477256829641
Loss in iteration 237 : 0.5080738922302143
Loss in iteration 238 : 0.506270279218686
Loss in iteration 239 : 0.5040079125647756
Loss in iteration 240 : 0.503102485009285
Loss in iteration 241 : 0.5021661073282689
Loss in iteration 242 : 0.5211067773820458
Loss in iteration 243 : 0.5121964088862931
Loss in iteration 244 : 0.5049725493984819
Loss in iteration 245 : 0.512998010680209
Loss in iteration 246 : 0.5050472093655689
Loss in iteration 247 : 0.5015564712944101
Loss in iteration 248 : 0.504963124737162
Loss in iteration 249 : 0.5075626412535881
Loss in iteration 250 : 0.5011606130335021
Loss in iteration 251 : 0.509433186225238
Loss in iteration 252 : 0.5035082448121868
Loss in iteration 253 : 0.5092530421773573
Loss in iteration 254 : 0.49597934728774506
Loss in iteration 255 : 0.49937523271420575
Loss in iteration 256 : 0.5121082723542157
Loss in iteration 257 : 0.49917374583432333
Loss in iteration 258 : 0.5004302709656077
Loss in iteration 259 : 0.49865934054105643
Loss in iteration 260 : 0.5039065621958616
Loss in iteration 261 : 0.49658636780302456
Loss in iteration 262 : 0.49952782197379386
Loss in iteration 263 : 0.5032917522691686
Loss in iteration 264 : 0.5123807720920678
Loss in iteration 265 : 0.4977541335871927
Loss in iteration 266 : 0.5081170374726752
Loss in iteration 267 : 0.5054355046374044
Loss in iteration 268 : 0.5080741712657548
Loss in iteration 269 : 0.4986684293626286
Loss in iteration 270 : 0.5037941513268394
Loss in iteration 271 : 0.4952683545997374
Loss in iteration 272 : 0.5034763668804454
Loss in iteration 273 : 0.49229106865561406
Loss in iteration 274 : 0.49725986144287304
Loss in iteration 275 : 0.5011762267619027
Loss in iteration 276 : 0.5038951698309153
Loss in iteration 277 : 0.4966028135096166
Loss in iteration 278 : 0.4952691652059033
Loss in iteration 279 : 0.49544291558627396
Loss in iteration 280 : 0.4974460682695558
Loss in iteration 281 : 0.504894485997135
Loss in iteration 282 : 0.4983115097904544
Loss in iteration 283 : 0.5023672832018754
Loss in iteration 284 : 0.4919244828131432
Loss in iteration 285 : 0.49572493832031045
Loss in iteration 286 : 0.49871728847450647
Loss in iteration 287 : 0.5005582067274007
Loss in iteration 288 : 0.5015394362408325
Loss in iteration 289 : 0.4964432819996787
Loss in iteration 290 : 0.4929603008794659
Loss in iteration 291 : 0.5002894864221834
Loss in iteration 292 : 0.5008510151630415
Loss in iteration 293 : 0.4954612866450839
Loss in iteration 294 : 0.5024551539576703
Loss in iteration 295 : 0.4931568460664701
Loss in iteration 296 : 0.494762236511236
Loss in iteration 297 : 0.4991660360148108
Loss in iteration 298 : 0.5001210052214009
Loss in iteration 299 : 0.4986370048647827
Loss in iteration 300 : 0.5023893754369566
Loss in iteration 301 : 0.5010933433094101
Loss in iteration 302 : 0.4974226240197557
Loss in iteration 303 : 0.49364092223157907
Loss in iteration 304 : 0.49564877958656084
Loss in iteration 305 : 0.4932477278707531
Loss in iteration 306 : 0.4950788311756713
Loss in iteration 307 : 0.49732030519031517
Loss in iteration 308 : 0.49853664848863655
Loss in iteration 309 : 0.4935350440202949
Loss in iteration 310 : 0.49567088568669804
Loss in iteration 311 : 0.5022830422902002
Loss in iteration 312 : 0.5073998676379918
Loss in iteration 313 : 0.48804673045090863
Loss in iteration 314 : 0.49380425025626173
Loss in iteration 315 : 0.4929494883481038
Loss in iteration 316 : 0.5059551091676728
Loss in iteration 317 : 0.49297079551663464
Loss in iteration 318 : 0.5038256372147472
Loss in iteration 319 : 0.49342161808922064
Loss in iteration 320 : 0.4909366195531065
Loss in iteration 321 : 0.49348070589471094
Loss in iteration 322 : 0.4927135465088334
Loss in iteration 323 : 0.4934259595191865
Loss in iteration 324 : 0.4944674514581218
Loss in iteration 325 : 0.4928478942715173
Loss in iteration 326 : 0.5019808062082671
Loss in iteration 327 : 0.488332019941646
Loss in iteration 328 : 0.4922967258893488
Loss in iteration 329 : 0.49308943120583115
Loss in iteration 330 : 0.48702214071412764
Loss in iteration 331 : 0.493425136826621
Loss in iteration 332 : 0.4848725866912658
Loss in iteration 333 : 0.4869152361075052
Loss in iteration 334 : 0.4958236827298178
Loss in iteration 335 : 0.48797066653579907
Loss in iteration 336 : 0.493144252597228
Loss in iteration 337 : 0.49396341177044445
Loss in iteration 338 : 0.5005434898314504
Loss in iteration 339 : 0.5021769360123962
Loss in iteration 340 : 0.4971135646233597
Loss in iteration 341 : 0.4908654126539281
Loss in iteration 342 : 0.4937326996939538
Loss in iteration 343 : 0.49279175459633495
Loss in iteration 344 : 0.497608791318237
Loss in iteration 345 : 0.49383734705388527
Loss in iteration 346 : 0.4896707059661067
Loss in iteration 347 : 0.49597083178100715
Loss in iteration 348 : 0.49442499239262944
Loss in iteration 349 : 0.49852047772445657
Loss in iteration 350 : 0.49429103468556057
Loss in iteration 351 : 0.49228790661302996
Loss in iteration 352 : 0.49098370613395526
Loss in iteration 353 : 0.4886525468331193
Loss in iteration 354 : 0.4932305731512701
Loss in iteration 355 : 0.49191968831458466
Loss in iteration 356 : 0.48613643565591924
Loss in iteration 357 : 0.49128244080861205
Loss in iteration 358 : 0.49510230322793075
Loss in iteration 359 : 0.49062828204000336
Loss in iteration 360 : 0.49623095661289096
Loss in iteration 361 : 0.4865546617144276
Loss in iteration 362 : 0.49762873832141713
Loss in iteration 363 : 0.49205509807863207
Loss in iteration 364 : 0.4941895133853557
Loss in iteration 365 : 0.49325992253516904
Loss in iteration 366 : 0.4898785857920729
Loss in iteration 367 : 0.4933755699327087
Loss in iteration 368 : 0.49674998190268543
Loss in iteration 369 : 0.501984890047442
Loss in iteration 370 : 0.4890649637515207
Loss in iteration 371 : 0.5032862858847907
Loss in iteration 372 : 0.4845977564918159
Loss in iteration 373 : 0.49254647944974556
Loss in iteration 374 : 0.48863972581071463
Loss in iteration 375 : 0.4897982214448635
Loss in iteration 376 : 0.4968916464613813
Loss in iteration 377 : 0.4916591562223331
Loss in iteration 378 : 0.49377955381392363
Loss in iteration 379 : 0.49137007223638635
Loss in iteration 380 : 0.48937106466043895
Loss in iteration 381 : 0.485553032305972
Loss in iteration 382 : 0.4875133307841382
Loss in iteration 383 : 0.4901230941737359
Loss in iteration 384 : 0.4948408175534202
Loss in iteration 385 : 0.4909411932220295
Loss in iteration 386 : 0.4832276069800174
Loss in iteration 387 : 0.49255268440573635
Loss in iteration 388 : 0.4976763183974493
Loss in iteration 389 : 0.48561116957448486
Loss in iteration 390 : 0.4895809809639487
Loss in iteration 391 : 0.4913664624469447
Loss in iteration 392 : 0.4825984532863844
Loss in iteration 393 : 0.4830406744587992
Loss in iteration 394 : 0.488276397997592
Loss in iteration 395 : 0.4931961098669348
Loss in iteration 396 : 0.48911870418865344
Loss in iteration 397 : 0.4977278105068944
Loss in iteration 398 : 0.49541782275631957
Loss in iteration 399 : 0.4841882776713256
Loss in iteration 400 : 0.48445044331660375
Testing accuracy  of updater 3 on alg 0 with rate 0.7 = 0.77975, training accuracy 0.77975, time elapsed: 5580 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6887922787626772
Loss in iteration 3 : 0.6840379956378154
Loss in iteration 4 : 0.6823412284502908
Loss in iteration 5 : 0.6787445579405322
Loss in iteration 6 : 0.6782086095071554
Loss in iteration 7 : 0.674797778282469
Loss in iteration 8 : 0.6717172281763119
Loss in iteration 9 : 0.6711348190536381
Loss in iteration 10 : 0.6706184827181857
Loss in iteration 11 : 0.6658566183708494
Loss in iteration 12 : 0.6649985240053051
Loss in iteration 13 : 0.6643580790518836
Loss in iteration 14 : 0.6614723997584443
Loss in iteration 15 : 0.6605741561629217
Loss in iteration 16 : 0.6579019135545634
Loss in iteration 17 : 0.6554996463162428
Loss in iteration 18 : 0.6545636430321161
Loss in iteration 19 : 0.6541710566816982
Loss in iteration 20 : 0.6537319026919228
Loss in iteration 21 : 0.6515112920078867
Loss in iteration 22 : 0.6495935307995289
Loss in iteration 23 : 0.6497165821009045
Loss in iteration 24 : 0.6450558454920502
Loss in iteration 25 : 0.6464503136058134
Loss in iteration 26 : 0.6414746759946495
Loss in iteration 27 : 0.6415705843143438
Loss in iteration 28 : 0.6422679334905287
Loss in iteration 29 : 0.6415984933073028
Loss in iteration 30 : 0.641694369342331
Loss in iteration 31 : 0.6396649148856354
Loss in iteration 32 : 0.6390695927825246
Loss in iteration 33 : 0.636661374216355
Loss in iteration 34 : 0.6354056314151602
Loss in iteration 35 : 0.6351051291996407
Loss in iteration 36 : 0.63176307501817
Loss in iteration 37 : 0.6300296366169387
Loss in iteration 38 : 0.629479786411443
Loss in iteration 39 : 0.6287655438723487
Loss in iteration 40 : 0.6285307553409617
Loss in iteration 41 : 0.6268877916068663
Loss in iteration 42 : 0.6239527697276066
Loss in iteration 43 : 0.6256649015212575
Loss in iteration 44 : 0.6236618715930801
Loss in iteration 45 : 0.6233327724429361
Loss in iteration 46 : 0.6220055831629874
Loss in iteration 47 : 0.6202530191632138
Loss in iteration 48 : 0.6208484297554292
Loss in iteration 49 : 0.6178800844549525
Loss in iteration 50 : 0.6169522359113563
Loss in iteration 51 : 0.6174441999506171
Loss in iteration 52 : 0.6148053540637997
Loss in iteration 53 : 0.6139873529933118
Loss in iteration 54 : 0.6103500483417404
Loss in iteration 55 : 0.6100317349249352
Loss in iteration 56 : 0.6128676658987993
Loss in iteration 57 : 0.6084769455268717
Loss in iteration 58 : 0.6083266439036287
Loss in iteration 59 : 0.607599321773802
Loss in iteration 60 : 0.6083054012686043
Loss in iteration 61 : 0.6087438715814114
Loss in iteration 62 : 0.6059084910961927
Loss in iteration 63 : 0.6047861023727871
Loss in iteration 64 : 0.6034474557696096
Loss in iteration 65 : 0.6048263152414469
Loss in iteration 66 : 0.6007443270153227
Loss in iteration 67 : 0.5999547861573679
Loss in iteration 68 : 0.5983667549176958
Loss in iteration 69 : 0.6020181565887714
Loss in iteration 70 : 0.5988560253285463
Loss in iteration 71 : 0.6028663615790216
Loss in iteration 72 : 0.5980477278598392
Loss in iteration 73 : 0.5964298543961137
Loss in iteration 74 : 0.596643652570558
Loss in iteration 75 : 0.5954121671959812
Loss in iteration 76 : 0.5969305060927771
Loss in iteration 77 : 0.5947368559914277
Loss in iteration 78 : 0.5954413671892071
Loss in iteration 79 : 0.5863562662580626
Loss in iteration 80 : 0.5909747318725969
Loss in iteration 81 : 0.5937708531176955
Loss in iteration 82 : 0.5890911822404591
Loss in iteration 83 : 0.5929351798273343
Loss in iteration 84 : 0.5906252339497641
Loss in iteration 85 : 0.5902657052364313
Loss in iteration 86 : 0.5916846280346795
Loss in iteration 87 : 0.5833445704419856
Loss in iteration 88 : 0.5913087293879368
Loss in iteration 89 : 0.5839573550088698
Loss in iteration 90 : 0.5868434577546319
Loss in iteration 91 : 0.5878956789478056
Loss in iteration 92 : 0.5815864575580427
Loss in iteration 93 : 0.5815797989172398
Loss in iteration 94 : 0.5837075328603305
Loss in iteration 95 : 0.5811259591258734
Loss in iteration 96 : 0.5867964906476985
Loss in iteration 97 : 0.5802866661147347
Loss in iteration 98 : 0.5808006630350706
Loss in iteration 99 : 0.580598494263069
Loss in iteration 100 : 0.5833455817427122
Loss in iteration 101 : 0.5774518340263454
Loss in iteration 102 : 0.5775660416854163
Loss in iteration 103 : 0.5794284283897443
Loss in iteration 104 : 0.5775945352598645
Loss in iteration 105 : 0.5758172810012865
Loss in iteration 106 : 0.576154611318168
Loss in iteration 107 : 0.5782717955609193
Loss in iteration 108 : 0.5815887392823055
Loss in iteration 109 : 0.5760865771904895
Loss in iteration 110 : 0.5732513765187057
Loss in iteration 111 : 0.5755549499479731
Loss in iteration 112 : 0.5720353367143453
Loss in iteration 113 : 0.5713187867731672
Loss in iteration 114 : 0.5698166621594141
Loss in iteration 115 : 0.5698446259160691
Loss in iteration 116 : 0.5708574061160425
Loss in iteration 117 : 0.5733542702871322
Loss in iteration 118 : 0.5720953264329308
Loss in iteration 119 : 0.5674893367925644
Loss in iteration 120 : 0.5723578229039703
Loss in iteration 121 : 0.5728171277854474
Loss in iteration 122 : 0.5709678244258538
Loss in iteration 123 : 0.5678154318645252
Loss in iteration 124 : 0.5657326934311178
Loss in iteration 125 : 0.5673993397734618
Loss in iteration 126 : 0.5651134249145029
Loss in iteration 127 : 0.5656250897257723
Loss in iteration 128 : 0.5655793391282704
Loss in iteration 129 : 0.5639661835224998
Loss in iteration 130 : 0.5655571133021753
Loss in iteration 131 : 0.5628513051692319
Loss in iteration 132 : 0.5639405135939759
Loss in iteration 133 : 0.5631216625487594
Loss in iteration 134 : 0.5643545701508795
Loss in iteration 135 : 0.5631177531645463
Loss in iteration 136 : 0.5631247066723117
Loss in iteration 137 : 0.5614938443436978
Loss in iteration 138 : 0.5638620286591107
Loss in iteration 139 : 0.5627275050943729
Loss in iteration 140 : 0.5620630089439209
Loss in iteration 141 : 0.5626361564328666
Loss in iteration 142 : 0.5593101019016605
Loss in iteration 143 : 0.5586710802103793
Loss in iteration 144 : 0.5578786893130694
Loss in iteration 145 : 0.5549797125453685
Loss in iteration 146 : 0.5583183427833308
Loss in iteration 147 : 0.5612650684311943
Loss in iteration 148 : 0.5574867830063398
Loss in iteration 149 : 0.5616678700528775
Loss in iteration 150 : 0.5588348480678074
Loss in iteration 151 : 0.5564907137154373
Loss in iteration 152 : 0.5584894416672446
Loss in iteration 153 : 0.5489136916369878
Loss in iteration 154 : 0.5583689636190318
Loss in iteration 155 : 0.5572342753518196
Loss in iteration 156 : 0.551166873492607
Loss in iteration 157 : 0.5553438004402412
Loss in iteration 158 : 0.5501891005412686
Loss in iteration 159 : 0.5526609371261456
Loss in iteration 160 : 0.5500872633438051
Loss in iteration 161 : 0.5532478807419611
Loss in iteration 162 : 0.5511729232083077
Loss in iteration 163 : 0.5541320210881882
Loss in iteration 164 : 0.5536638304941577
Loss in iteration 165 : 0.549601027701782
Loss in iteration 166 : 0.549846738779552
Loss in iteration 167 : 0.5555135062812656
Loss in iteration 168 : 0.5530753548902518
Loss in iteration 169 : 0.5515281343595582
Loss in iteration 170 : 0.5519774114179185
Loss in iteration 171 : 0.5481165097522279
Loss in iteration 172 : 0.549360655187251
Loss in iteration 173 : 0.5487344562535644
Loss in iteration 174 : 0.5463604626457867
Loss in iteration 175 : 0.5523522214031776
Loss in iteration 176 : 0.5454558161268764
Loss in iteration 177 : 0.549855189575449
Loss in iteration 178 : 0.5456149667121365
Loss in iteration 179 : 0.5494822823716241
Loss in iteration 180 : 0.5440895661556977
Loss in iteration 181 : 0.5481517750871456
Loss in iteration 182 : 0.5469911834389934
Loss in iteration 183 : 0.5438421108809044
Loss in iteration 184 : 0.5495235938668698
Loss in iteration 185 : 0.544230840489917
Loss in iteration 186 : 0.5420124751770663
Loss in iteration 187 : 0.5406984667838023
Loss in iteration 188 : 0.5451701289318579
Loss in iteration 189 : 0.5471969417129235
Loss in iteration 190 : 0.5407883534674379
Loss in iteration 191 : 0.5425347621203319
Loss in iteration 192 : 0.5419654569295742
Loss in iteration 193 : 0.5411564804258617
Loss in iteration 194 : 0.5438779311134926
Loss in iteration 195 : 0.5470397202492854
Loss in iteration 196 : 0.5484379807907372
Loss in iteration 197 : 0.5384988359632189
Loss in iteration 198 : 0.5470584701133564
Loss in iteration 199 : 0.5417783878502839
Loss in iteration 200 : 0.5385415028661688
Loss in iteration 201 : 0.5427845077061106
Loss in iteration 202 : 0.5414772458885878
Loss in iteration 203 : 0.5354010465921004
Loss in iteration 204 : 0.5369860320076592
Loss in iteration 205 : 0.5403091719273521
Loss in iteration 206 : 0.5380337013931576
Loss in iteration 207 : 0.5389094722062635
Loss in iteration 208 : 0.5336315368830877
Loss in iteration 209 : 0.5415681767541084
Loss in iteration 210 : 0.5428905833723411
Loss in iteration 211 : 0.5400197702749078
Loss in iteration 212 : 0.5421177509500811
Loss in iteration 213 : 0.5422980750114907
Loss in iteration 214 : 0.5475649097946722
Loss in iteration 215 : 0.542222049982839
Loss in iteration 216 : 0.5397437178249528
Loss in iteration 217 : 0.5354557477408822
Loss in iteration 218 : 0.5337551042759934
Loss in iteration 219 : 0.5379385890743921
Loss in iteration 220 : 0.5379878827253524
Loss in iteration 221 : 0.5370788036939715
Loss in iteration 222 : 0.5315008060936529
Loss in iteration 223 : 0.5407635596566914
Loss in iteration 224 : 0.5363451478337397
Loss in iteration 225 : 0.5356613343313893
Loss in iteration 226 : 0.5348416996518843
Loss in iteration 227 : 0.5310908435521854
Loss in iteration 228 : 0.5343075068913217
Loss in iteration 229 : 0.5355253713244424
Loss in iteration 230 : 0.5289214362417685
Loss in iteration 231 : 0.5336618880072618
Loss in iteration 232 : 0.5340563815633752
Loss in iteration 233 : 0.5336045408815144
Loss in iteration 234 : 0.53571238354804
Loss in iteration 235 : 0.5288702666827723
Loss in iteration 236 : 0.5310437538930779
Loss in iteration 237 : 0.5330655228769516
Loss in iteration 238 : 0.531565318004621
Loss in iteration 239 : 0.5289665998474251
Loss in iteration 240 : 0.5290148529054634
Loss in iteration 241 : 0.5276535690325552
Loss in iteration 242 : 0.5435424447842102
Loss in iteration 243 : 0.5363486719335647
Loss in iteration 244 : 0.5301596104709319
Loss in iteration 245 : 0.5361364327906402
Loss in iteration 246 : 0.5302875356730934
Loss in iteration 247 : 0.5276523843832842
Loss in iteration 248 : 0.5293092200590034
Loss in iteration 249 : 0.531807527728846
Loss in iteration 250 : 0.5267081574434641
Loss in iteration 251 : 0.5340061465031738
Loss in iteration 252 : 0.5288894200678061
Loss in iteration 253 : 0.53225851756513
Loss in iteration 254 : 0.5226414785177408
Loss in iteration 255 : 0.524453351767322
Loss in iteration 256 : 0.5347168406489063
Loss in iteration 257 : 0.5248650276617846
Loss in iteration 258 : 0.525581463587485
Loss in iteration 259 : 0.5236203492463203
Loss in iteration 260 : 0.5281300313924456
Loss in iteration 261 : 0.522699649580844
Loss in iteration 262 : 0.5238743709718664
Loss in iteration 263 : 0.5272889617373508
Loss in iteration 264 : 0.5344567352589722
Loss in iteration 265 : 0.5227677984364979
Loss in iteration 266 : 0.53133303487131
Loss in iteration 267 : 0.5286158369092043
Loss in iteration 268 : 0.5302239103254824
Loss in iteration 269 : 0.5230333316137649
Loss in iteration 270 : 0.5278739753345303
Loss in iteration 271 : 0.5210381123151842
Loss in iteration 272 : 0.5278045103060685
Loss in iteration 273 : 0.5182804227096355
Loss in iteration 274 : 0.5218401440705271
Loss in iteration 275 : 0.5242051525222226
Loss in iteration 276 : 0.5268341690292291
Loss in iteration 277 : 0.5210817124360099
Loss in iteration 278 : 0.5192679518218183
Loss in iteration 279 : 0.5204329115231789
Loss in iteration 280 : 0.521560787403689
Loss in iteration 281 : 0.5278371192761683
Loss in iteration 282 : 0.5221308301777422
Loss in iteration 283 : 0.5247947954221022
Loss in iteration 284 : 0.5171901739483723
Loss in iteration 285 : 0.5203999355033567
Loss in iteration 286 : 0.5216637448826149
Loss in iteration 287 : 0.5236369642685939
Loss in iteration 288 : 0.5243667387661852
Loss in iteration 289 : 0.5200564042461677
Loss in iteration 290 : 0.5170799015909694
Loss in iteration 291 : 0.5223447433235757
Loss in iteration 292 : 0.5237308835986189
Loss in iteration 293 : 0.5189367519040214
Loss in iteration 294 : 0.524215247879869
Loss in iteration 295 : 0.5171404383276089
Loss in iteration 296 : 0.5182085442993223
Loss in iteration 297 : 0.5212867933646606
Loss in iteration 298 : 0.5226796425703863
Loss in iteration 299 : 0.5210347292066394
Loss in iteration 300 : 0.5241173784274008
Loss in iteration 301 : 0.5230902514071635
Loss in iteration 302 : 0.5198665707455499
Loss in iteration 303 : 0.5172761020769663
Loss in iteration 304 : 0.5185851854491244
Loss in iteration 305 : 0.5160665752976068
Loss in iteration 306 : 0.5181381533267165
Loss in iteration 307 : 0.5192984429299734
Loss in iteration 308 : 0.5201206537648392
Loss in iteration 309 : 0.5171155909590817
Loss in iteration 310 : 0.5179655438656239
Loss in iteration 311 : 0.5231778180369703
Loss in iteration 312 : 0.527595864429689
Loss in iteration 313 : 0.5120315824612183
Loss in iteration 314 : 0.51682907934593
Loss in iteration 315 : 0.515283622497128
Loss in iteration 316 : 0.5261748768227545
Loss in iteration 317 : 0.514843141442356
Loss in iteration 318 : 0.5242994842347665
Loss in iteration 319 : 0.5161957783175555
Loss in iteration 320 : 0.5135259552926239
Loss in iteration 321 : 0.51541185584856
Loss in iteration 322 : 0.5144858559697197
Loss in iteration 323 : 0.5156461849642435
Loss in iteration 324 : 0.5166246158748007
Loss in iteration 325 : 0.5146485547832454
Loss in iteration 326 : 0.5213798434651638
Loss in iteration 327 : 0.5108486081808138
Loss in iteration 328 : 0.514554483778852
Loss in iteration 329 : 0.5155602731084997
Loss in iteration 330 : 0.5096197230243695
Loss in iteration 331 : 0.5148101393134465
Loss in iteration 332 : 0.50830812070648
Loss in iteration 333 : 0.5098091680129617
Loss in iteration 334 : 0.5177562482805385
Loss in iteration 335 : 0.5108938085159298
Loss in iteration 336 : 0.5142170287549356
Loss in iteration 337 : 0.5144688857309233
Loss in iteration 338 : 0.5210117976408287
Loss in iteration 339 : 0.5220859605872784
Loss in iteration 340 : 0.51744522854389
Loss in iteration 341 : 0.5119196929176156
Loss in iteration 342 : 0.5150482378601312
Loss in iteration 343 : 0.5137674089329621
Loss in iteration 344 : 0.5183354115121543
Loss in iteration 345 : 0.5150783169260085
Loss in iteration 346 : 0.5112744518276355
Loss in iteration 347 : 0.5171581592481516
Loss in iteration 348 : 0.5148149578633302
Loss in iteration 349 : 0.5184546295845761
Loss in iteration 350 : 0.51397402392224
Loss in iteration 351 : 0.5126489274307668
Loss in iteration 352 : 0.5118405086127578
Loss in iteration 353 : 0.5104721906894157
Loss in iteration 354 : 0.5139796272693543
Loss in iteration 355 : 0.5123692880578793
Loss in iteration 356 : 0.5076584604934274
Loss in iteration 357 : 0.5120163710583597
Loss in iteration 358 : 0.515084154219114
Loss in iteration 359 : 0.5111033241793199
Loss in iteration 360 : 0.5158077198718423
Loss in iteration 361 : 0.5080747672498509
Loss in iteration 362 : 0.5163929788574375
Loss in iteration 363 : 0.5122756065528606
Loss in iteration 364 : 0.5148271466314512
Loss in iteration 365 : 0.5127581518122155
Loss in iteration 366 : 0.5104191052891189
Loss in iteration 367 : 0.5121960365335329
Loss in iteration 368 : 0.5162475078133928
Loss in iteration 369 : 0.520247323453556
Loss in iteration 370 : 0.5094047475572994
Loss in iteration 371 : 0.5213422893384176
Loss in iteration 372 : 0.5054490028286727
Loss in iteration 373 : 0.5125977883572507
Loss in iteration 374 : 0.5083726214562513
Loss in iteration 375 : 0.5098255179238773
Loss in iteration 376 : 0.5153370038963601
Loss in iteration 377 : 0.511433297675404
Loss in iteration 378 : 0.5125609145352832
Loss in iteration 379 : 0.510453491336232
Loss in iteration 380 : 0.5098978061958934
Loss in iteration 381 : 0.5071945481304109
Loss in iteration 382 : 0.5077010962118305
Loss in iteration 383 : 0.5091988413586505
Loss in iteration 384 : 0.5127060192920538
Loss in iteration 385 : 0.5102583078376657
Loss in iteration 386 : 0.5043184178691045
Loss in iteration 387 : 0.511784260214476
Loss in iteration 388 : 0.5159867695520677
Loss in iteration 389 : 0.5059549059630946
Loss in iteration 390 : 0.5095619315155261
Loss in iteration 391 : 0.5104872615789096
Loss in iteration 392 : 0.5029346158546296
Loss in iteration 393 : 0.5030566431897371
Loss in iteration 394 : 0.507670540375821
Loss in iteration 395 : 0.5115660188438013
Loss in iteration 396 : 0.5085388687879738
Loss in iteration 397 : 0.515353470173892
Loss in iteration 398 : 0.5129689144593577
Loss in iteration 399 : 0.5034766389161085
Loss in iteration 400 : 0.5048985477263624
Testing accuracy  of updater 3 on alg 0 with rate 0.4 = 0.77575, training accuracy 0.77575, time elapsed: 5624 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6920048055612684
Loss in iteration 3 : 0.6905683861230131
Loss in iteration 4 : 0.6898316290336214
Loss in iteration 5 : 0.6885428606786973
Loss in iteration 6 : 0.6882105755767141
Loss in iteration 7 : 0.6868128187869281
Loss in iteration 8 : 0.6854687283745488
Loss in iteration 9 : 0.6851575754575087
Loss in iteration 10 : 0.6850812335282193
Loss in iteration 11 : 0.6830092398740292
Loss in iteration 12 : 0.682490132858674
Loss in iteration 13 : 0.6824027786085742
Loss in iteration 14 : 0.6810409374026458
Loss in iteration 15 : 0.6807092804135686
Loss in iteration 16 : 0.6796813264863066
Loss in iteration 17 : 0.6783945088612522
Loss in iteration 18 : 0.6781783447891234
Loss in iteration 19 : 0.6782681781299489
Loss in iteration 20 : 0.6786306104136772
Loss in iteration 21 : 0.6770315204339901
Loss in iteration 22 : 0.6761360128507167
Loss in iteration 23 : 0.6766943387824041
Loss in iteration 24 : 0.6740371174908193
Loss in iteration 25 : 0.6754924667273493
Loss in iteration 26 : 0.6721172310098026
Loss in iteration 27 : 0.6728968883287032
Loss in iteration 28 : 0.6738306586875731
Loss in iteration 29 : 0.673298709171117
Loss in iteration 30 : 0.6739861359319148
Loss in iteration 31 : 0.6726315047875259
Loss in iteration 32 : 0.6730791818223171
Loss in iteration 33 : 0.6722347924183895
Loss in iteration 34 : 0.6718513130591518
Loss in iteration 35 : 0.6708300115690304
Loss in iteration 36 : 0.6688624776805607
Loss in iteration 37 : 0.6683037258945097
Loss in iteration 38 : 0.6689806641804152
Loss in iteration 39 : 0.6684942886605165
Loss in iteration 40 : 0.6684222828148673
Loss in iteration 41 : 0.6675729965041765
Loss in iteration 42 : 0.6659377253793659
Loss in iteration 43 : 0.6672952538495602
Loss in iteration 44 : 0.6658516839710973
Loss in iteration 45 : 0.6662603763430198
Loss in iteration 46 : 0.6654631368803986
Loss in iteration 47 : 0.6642487078505436
Loss in iteration 48 : 0.6647829507228085
Loss in iteration 49 : 0.663727271799471
Loss in iteration 50 : 0.6638704556237257
Loss in iteration 51 : 0.6630512695486479
Loss in iteration 52 : 0.6630709914260033
Loss in iteration 53 : 0.6618185748102392
Loss in iteration 54 : 0.6598607641910077
Loss in iteration 55 : 0.65912049717378
Loss in iteration 56 : 0.6616663122156721
Loss in iteration 57 : 0.6603493558996897
Loss in iteration 58 : 0.6603034530608142
Loss in iteration 59 : 0.6593008065262379
Loss in iteration 60 : 0.66013645431377
Loss in iteration 61 : 0.660373230047809
Loss in iteration 62 : 0.6579417508200909
Loss in iteration 63 : 0.6574327727927692
Loss in iteration 64 : 0.6571570142258835
Loss in iteration 65 : 0.6576836760700308
Loss in iteration 66 : 0.6557738425011113
Loss in iteration 67 : 0.6551738373894926
Loss in iteration 68 : 0.6542995796672303
Loss in iteration 69 : 0.6568031106958466
Loss in iteration 70 : 0.6553633123540694
Loss in iteration 71 : 0.6566054155263672
Loss in iteration 72 : 0.6548904445152957
Loss in iteration 73 : 0.6526766803861258
Loss in iteration 74 : 0.6537005209455454
Loss in iteration 75 : 0.6530025385574135
Loss in iteration 76 : 0.6536863197476748
Loss in iteration 77 : 0.6538705447893245
Loss in iteration 78 : 0.6538486578430925
Loss in iteration 79 : 0.648469934248275
Loss in iteration 80 : 0.6507491748803844
Loss in iteration 81 : 0.6527290410228995
Loss in iteration 82 : 0.6496274325801739
Loss in iteration 83 : 0.6525158208301938
Loss in iteration 84 : 0.6498029299593234
Loss in iteration 85 : 0.6510878694070265
Loss in iteration 86 : 0.6516305750182722
Loss in iteration 87 : 0.6454299442894808
Loss in iteration 88 : 0.6513548321892878
Loss in iteration 89 : 0.6470621345490434
Loss in iteration 90 : 0.6488799203720163
Loss in iteration 91 : 0.6497659520211617
Loss in iteration 92 : 0.6449278786660004
Loss in iteration 93 : 0.6449841041853094
Loss in iteration 94 : 0.646103475026428
Loss in iteration 95 : 0.6465368740679706
Loss in iteration 96 : 0.6495247745783723
Loss in iteration 97 : 0.6443301650905364
Loss in iteration 98 : 0.6453967099154434
Loss in iteration 99 : 0.6454413583357644
Loss in iteration 100 : 0.6473025755216182
Loss in iteration 101 : 0.6430031159042767
Loss in iteration 102 : 0.6439166964928317
Loss in iteration 103 : 0.6430910300650783
Loss in iteration 104 : 0.6433200603885341
Loss in iteration 105 : 0.6441618217012273
Loss in iteration 106 : 0.6431095168274893
Loss in iteration 107 : 0.6431186048937747
Loss in iteration 108 : 0.6456441394302742
Loss in iteration 109 : 0.6429870997244237
Loss in iteration 110 : 0.6413599653115939
Loss in iteration 111 : 0.6431930984696144
Loss in iteration 112 : 0.6398350377766284
Loss in iteration 113 : 0.6400844533972168
Loss in iteration 114 : 0.6403290509291809
Loss in iteration 115 : 0.64012010015907
Loss in iteration 116 : 0.6401044234724769
Loss in iteration 117 : 0.640080594659836
Loss in iteration 118 : 0.6387540057763964
Loss in iteration 119 : 0.6375666669240722
Loss in iteration 120 : 0.6398391210519446
Loss in iteration 121 : 0.6405753695855212
Loss in iteration 122 : 0.6384369011045586
Loss in iteration 123 : 0.6382905759684502
Loss in iteration 124 : 0.636768668527515
Loss in iteration 125 : 0.6386421582122191
Loss in iteration 126 : 0.6373102903327992
Loss in iteration 127 : 0.6369999099908775
Loss in iteration 128 : 0.6349728399947739
Loss in iteration 129 : 0.6362237907211874
Loss in iteration 130 : 0.635757080746712
Loss in iteration 131 : 0.6347262255211246
Loss in iteration 132 : 0.6362914284634388
Loss in iteration 133 : 0.633996175217482
Loss in iteration 134 : 0.6353797594802931
Loss in iteration 135 : 0.6353929356941791
Loss in iteration 136 : 0.6354477929433242
Loss in iteration 137 : 0.6338101260111811
Loss in iteration 138 : 0.6357936432357723
Loss in iteration 139 : 0.6325519225027175
Loss in iteration 140 : 0.6337024341213279
Loss in iteration 141 : 0.633548114249539
Loss in iteration 142 : 0.6325784847293832
Loss in iteration 143 : 0.6311259503776457
Loss in iteration 144 : 0.6308282717705245
Loss in iteration 145 : 0.6305034123131938
Loss in iteration 146 : 0.6291802847357085
Loss in iteration 147 : 0.6318796832165916
Loss in iteration 148 : 0.6315974015480028
Loss in iteration 149 : 0.6329814914197668
Loss in iteration 150 : 0.6307788226450247
Loss in iteration 151 : 0.6299468177871851
Loss in iteration 152 : 0.6296758954154099
Loss in iteration 153 : 0.6247817467839167
Loss in iteration 154 : 0.6310421344418649
Loss in iteration 155 : 0.629946037466716
Loss in iteration 156 : 0.6281339240448546
Loss in iteration 157 : 0.6280235087575555
Loss in iteration 158 : 0.6253615723930788
Loss in iteration 159 : 0.6271820332535565
Loss in iteration 160 : 0.6257831616230431
Loss in iteration 161 : 0.6284638737061764
Loss in iteration 162 : 0.6256174712532315
Loss in iteration 163 : 0.6275720200816046
Loss in iteration 164 : 0.6277059432392834
Loss in iteration 165 : 0.6249431735744203
Loss in iteration 166 : 0.6252184404746047
Loss in iteration 167 : 0.6287163305632189
Loss in iteration 168 : 0.6260726563558585
Loss in iteration 169 : 0.6258378104351524
Loss in iteration 170 : 0.6244885937496932
Loss in iteration 171 : 0.6253235324569512
Loss in iteration 172 : 0.6235113057369425
Loss in iteration 173 : 0.6237763074308063
Loss in iteration 174 : 0.6243490668772214
Loss in iteration 175 : 0.6252340083105903
Loss in iteration 176 : 0.6215904713978242
Loss in iteration 177 : 0.62312127947454
Loss in iteration 178 : 0.622273245049584
Loss in iteration 179 : 0.623515664408068
Loss in iteration 180 : 0.6212402012635025
Loss in iteration 181 : 0.6236954546549521
Loss in iteration 182 : 0.623944252758529
Loss in iteration 183 : 0.6194655075726215
Loss in iteration 184 : 0.6216810094211451
Loss in iteration 185 : 0.6202367141526547
Loss in iteration 186 : 0.6192090380370961
Loss in iteration 187 : 0.6183385246702218
Loss in iteration 188 : 0.6203655647308784
Loss in iteration 189 : 0.6219460276574916
Loss in iteration 190 : 0.6180661515405534
Loss in iteration 191 : 0.6195559391670452
Loss in iteration 192 : 0.6189417425025834
Loss in iteration 193 : 0.6173093457150617
Loss in iteration 194 : 0.6199754150230637
Loss in iteration 195 : 0.6208473378687291
Loss in iteration 196 : 0.6209961058077713
Loss in iteration 197 : 0.6176427580206024
Loss in iteration 198 : 0.6196924809278443
Loss in iteration 199 : 0.6185058489371976
Loss in iteration 200 : 0.6167396781902938
Loss in iteration 201 : 0.6179749210695908
Loss in iteration 202 : 0.6172435578711423
Loss in iteration 203 : 0.6142606597668235
Loss in iteration 204 : 0.6154945907719293
Loss in iteration 205 : 0.6159033416900649
Loss in iteration 206 : 0.6147260873407059
Loss in iteration 207 : 0.6152093353322341
Loss in iteration 208 : 0.6120136929158668
Loss in iteration 209 : 0.6175652348556827
Loss in iteration 210 : 0.6174643304905798
Loss in iteration 211 : 0.6154507631935429
Loss in iteration 212 : 0.616367098252415
Loss in iteration 213 : 0.617945550608578
Loss in iteration 214 : 0.6189591476727885
Loss in iteration 215 : 0.6158526994589694
Loss in iteration 216 : 0.6160672698378704
Loss in iteration 217 : 0.6122987617339956
Loss in iteration 218 : 0.6112476417659609
Loss in iteration 219 : 0.6126541236624771
Loss in iteration 220 : 0.614044999147969
Loss in iteration 221 : 0.6125495381400904
Loss in iteration 222 : 0.6110617602532652
Loss in iteration 223 : 0.6141820859919911
Loss in iteration 224 : 0.6120923732881309
Loss in iteration 225 : 0.6131931761065635
Loss in iteration 226 : 0.6108326873311455
Loss in iteration 227 : 0.6101332350544615
Loss in iteration 228 : 0.6106662932027503
Loss in iteration 229 : 0.6113953454486533
Loss in iteration 230 : 0.607826087182533
Loss in iteration 231 : 0.6100058855327748
Loss in iteration 232 : 0.6110814464232086
Loss in iteration 233 : 0.6105851697286541
Loss in iteration 234 : 0.6102586994133262
Loss in iteration 235 : 0.6078333613281642
Loss in iteration 236 : 0.6078303280653945
Loss in iteration 237 : 0.6087857432744663
Loss in iteration 238 : 0.607409141986699
Loss in iteration 239 : 0.6059035558863787
Loss in iteration 240 : 0.6076560961277709
Loss in iteration 241 : 0.6047092793679852
Loss in iteration 242 : 0.6135218978768774
Loss in iteration 243 : 0.6112888315632099
Loss in iteration 244 : 0.6073339339255878
Loss in iteration 245 : 0.6096248196786908
Loss in iteration 246 : 0.6078740048445431
Loss in iteration 247 : 0.6062745446658295
Loss in iteration 248 : 0.6054544440711633
Loss in iteration 249 : 0.6073525517581041
Loss in iteration 250 : 0.6051245713277312
Loss in iteration 251 : 0.6100380461197712
Loss in iteration 252 : 0.606534657058288
Loss in iteration 253 : 0.605972106636885
Loss in iteration 254 : 0.6030224070682382
Loss in iteration 255 : 0.601980217111802
Loss in iteration 256 : 0.6079326855761332
Loss in iteration 257 : 0.6037713627154065
Loss in iteration 258 : 0.6041922248721371
Loss in iteration 259 : 0.6025957264379396
Loss in iteration 260 : 0.6044484728402244
Loss in iteration 261 : 0.6025527089443206
Loss in iteration 262 : 0.6015168502984563
Loss in iteration 263 : 0.602739470455632
Loss in iteration 264 : 0.6064801119852523
Loss in iteration 265 : 0.6005690052168146
Loss in iteration 266 : 0.6052457305220603
Loss in iteration 267 : 0.6039363994091969
Loss in iteration 268 : 0.6034089030199732
Loss in iteration 269 : 0.6012067526838861
Loss in iteration 270 : 0.6043487023084252
Loss in iteration 271 : 0.601375920329145
Loss in iteration 272 : 0.6052152747346908
Loss in iteration 273 : 0.5990512549936455
Loss in iteration 274 : 0.6002248394059269
Loss in iteration 275 : 0.6005606759702226
Loss in iteration 276 : 0.6031446712845782
Loss in iteration 277 : 0.5990906989937811
Loss in iteration 278 : 0.5973453143428201
Loss in iteration 279 : 0.5991732280406238
Loss in iteration 280 : 0.5995518942656294
Loss in iteration 281 : 0.6029073657093265
Loss in iteration 282 : 0.5994124710626086
Loss in iteration 283 : 0.5989755195165138
Loss in iteration 284 : 0.5983198110714523
Loss in iteration 285 : 0.5985076655879823
Loss in iteration 286 : 0.5976051231372695
Loss in iteration 287 : 0.6000247577897236
Loss in iteration 288 : 0.5997647605502759
Loss in iteration 289 : 0.5980263126709359
Loss in iteration 290 : 0.5952659148367495
Loss in iteration 291 : 0.5968224883084935
Loss in iteration 292 : 0.5996875313217133
Loss in iteration 293 : 0.5959222991594353
Loss in iteration 294 : 0.5976842436269916
Loss in iteration 295 : 0.5959957264128571
Loss in iteration 296 : 0.5966414727636974
Loss in iteration 297 : 0.5967517686358244
Loss in iteration 298 : 0.5977277746314896
Loss in iteration 299 : 0.5966844641101632
Loss in iteration 300 : 0.5982913545892494
Loss in iteration 301 : 0.5972513092321958
Loss in iteration 302 : 0.5957600442321596
Loss in iteration 303 : 0.5949552263232636
Loss in iteration 304 : 0.5951631083788769
Loss in iteration 305 : 0.5939072223174545
Loss in iteration 306 : 0.5954030066373452
Loss in iteration 307 : 0.5944268289982987
Loss in iteration 308 : 0.5945413618871537
Loss in iteration 309 : 0.5947532142583706
Loss in iteration 310 : 0.594053456047628
Loss in iteration 311 : 0.5958281956209129
Loss in iteration 312 : 0.5992238393076746
Loss in iteration 313 : 0.5918597341303852
Loss in iteration 314 : 0.5941918838933047
Loss in iteration 315 : 0.5926104041384567
Loss in iteration 316 : 0.5976281501110575
Loss in iteration 317 : 0.5913224230153054
Loss in iteration 318 : 0.5964990538763694
Loss in iteration 319 : 0.5930238693892458
Loss in iteration 320 : 0.5913741488920973
Loss in iteration 321 : 0.5906152741355377
Loss in iteration 322 : 0.5905652450384229
Loss in iteration 323 : 0.591807400656212
Loss in iteration 324 : 0.5924132271412773
Loss in iteration 325 : 0.5903946420652317
Loss in iteration 326 : 0.5928218817373679
Loss in iteration 327 : 0.5887883468792475
Loss in iteration 328 : 0.5914859933605283
Loss in iteration 329 : 0.5918612771315386
Loss in iteration 330 : 0.588486352255111
Loss in iteration 331 : 0.5905125838993088
Loss in iteration 332 : 0.5880172806655792
Loss in iteration 333 : 0.5886358714355349
Loss in iteration 334 : 0.5929156601314769
Loss in iteration 335 : 0.5896625358904531
Loss in iteration 336 : 0.5896094613566482
Loss in iteration 337 : 0.5881234973269722
Loss in iteration 338 : 0.5937088540821732
Loss in iteration 339 : 0.5936959168524019
Loss in iteration 340 : 0.5900828392333518
Loss in iteration 341 : 0.5871564075114751
Loss in iteration 342 : 0.5899337418121217
Loss in iteration 343 : 0.5885781291255727
Loss in iteration 344 : 0.5922353124746124
Loss in iteration 345 : 0.5900850988936404
Loss in iteration 346 : 0.5874773023998568
Loss in iteration 347 : 0.5908521683955388
Loss in iteration 348 : 0.589246486027037
Loss in iteration 349 : 0.5902893032906807
Loss in iteration 350 : 0.5874477945682353
Loss in iteration 351 : 0.5866552419477185
Loss in iteration 352 : 0.5871458263887532
Loss in iteration 353 : 0.587023684882444
Loss in iteration 354 : 0.5884012769089811
Loss in iteration 355 : 0.5867005102790785
Loss in iteration 356 : 0.5851302207004487
Loss in iteration 357 : 0.5866242411327133
Loss in iteration 358 : 0.5877357471750171
Loss in iteration 359 : 0.585538947843385
Loss in iteration 360 : 0.5881450999631989
Loss in iteration 361 : 0.5849768291866203
Loss in iteration 362 : 0.5883763739918533
Loss in iteration 363 : 0.586497357358272
Loss in iteration 364 : 0.5882199852812482
Loss in iteration 365 : 0.5856900245753368
Loss in iteration 366 : 0.585671104131294
Loss in iteration 367 : 0.5848574562568466
Loss in iteration 368 : 0.5876348608699539
Loss in iteration 369 : 0.589298346133993
Loss in iteration 370 : 0.5841506062131959
Loss in iteration 371 : 0.5901321523343226
Loss in iteration 372 : 0.5825009842519155
Loss in iteration 373 : 0.5852285318103853
Loss in iteration 374 : 0.5829044861955344
Loss in iteration 375 : 0.5833504847132857
Loss in iteration 376 : 0.5851896014322373
Loss in iteration 377 : 0.584435920972315
Loss in iteration 378 : 0.58306104148698
Loss in iteration 379 : 0.5828917656945094
Loss in iteration 380 : 0.5844431576809659
Loss in iteration 381 : 0.5838334450398023
Loss in iteration 382 : 0.5825743327281722
Loss in iteration 383 : 0.5816331178428616
Loss in iteration 384 : 0.5819958306127808
Loss in iteration 385 : 0.5830726219620311
Loss in iteration 386 : 0.5812230797514277
Loss in iteration 387 : 0.5849827297669203
Loss in iteration 388 : 0.5857914048588441
Loss in iteration 389 : 0.581921646915548
Loss in iteration 390 : 0.5832136750003902
Loss in iteration 391 : 0.5819462422925215
Loss in iteration 392 : 0.5786670735641571
Loss in iteration 393 : 0.5781280592213851
Loss in iteration 394 : 0.5807676077508662
Loss in iteration 395 : 0.5818679026404956
Loss in iteration 396 : 0.5814115934911552
Loss in iteration 397 : 0.5851361924870008
Loss in iteration 398 : 0.5816231532769368
Loss in iteration 399 : 0.5786452740624719
Loss in iteration 400 : 0.5798291032530749
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.767375, training accuracy 0.767375, time elapsed: 4971 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6929433221520231
Loss in iteration 3 : 0.6926577089007472
Loss in iteration 4 : 0.6925307644908585
Loss in iteration 5 : 0.6922247642690335
Loss in iteration 6 : 0.6921086413643279
Loss in iteration 7 : 0.691813406180849
Loss in iteration 8 : 0.6915261270578248
Loss in iteration 9 : 0.6914029089972621
Loss in iteration 10 : 0.6913474302534987
Loss in iteration 11 : 0.6907593470540198
Loss in iteration 12 : 0.6905671099062677
Loss in iteration 13 : 0.6905859951629546
Loss in iteration 14 : 0.6901316898167106
Loss in iteration 15 : 0.6901106745324649
Loss in iteration 16 : 0.6898462853700551
Loss in iteration 17 : 0.6893247295105349
Loss in iteration 18 : 0.6892114280618151
Loss in iteration 19 : 0.6892742436146446
Loss in iteration 20 : 0.6893537562943199
Loss in iteration 21 : 0.6888182896807824
Loss in iteration 22 : 0.6885217076278021
Loss in iteration 23 : 0.6885231079211237
Loss in iteration 24 : 0.6876411136020478
Loss in iteration 25 : 0.6880626830356736
Loss in iteration 26 : 0.687022640040458
Loss in iteration 27 : 0.6871253308769142
Loss in iteration 28 : 0.6873454824060324
Loss in iteration 29 : 0.6872050954594167
Loss in iteration 30 : 0.6873923339861742
Loss in iteration 31 : 0.6869662940252204
Loss in iteration 32 : 0.6869454687562611
Loss in iteration 33 : 0.6867806488229141
Loss in iteration 34 : 0.6863577215748786
Loss in iteration 35 : 0.6861362593011341
Loss in iteration 36 : 0.6855635561320247
Loss in iteration 37 : 0.6851396405231831
Loss in iteration 38 : 0.6854922078800081
Loss in iteration 39 : 0.6852723499388548
Loss in iteration 40 : 0.6852282970877842
Loss in iteration 41 : 0.6847335236056763
Loss in iteration 42 : 0.6840958438443017
Loss in iteration 43 : 0.6842303855715365
Loss in iteration 44 : 0.6838116455349307
Loss in iteration 45 : 0.6840057431682407
Loss in iteration 46 : 0.6837467681813535
Loss in iteration 47 : 0.6834375326431041
Loss in iteration 48 : 0.6835390930806223
Loss in iteration 49 : 0.6830724649543395
Loss in iteration 50 : 0.6830126373190685
Loss in iteration 51 : 0.6828543049958775
Loss in iteration 52 : 0.6827290754474833
Loss in iteration 53 : 0.682082803738299
Loss in iteration 54 : 0.6813519770399485
Loss in iteration 55 : 0.6809879344622907
Loss in iteration 56 : 0.6820424004362867
Loss in iteration 57 : 0.6815036252949241
Loss in iteration 58 : 0.6816001711207395
Loss in iteration 59 : 0.6809410877517313
Loss in iteration 60 : 0.681068533223391
Loss in iteration 61 : 0.6814783238700536
Loss in iteration 62 : 0.6801995883829011
Loss in iteration 63 : 0.6799708443880383
Loss in iteration 64 : 0.6799663290368586
Loss in iteration 65 : 0.6798607425843376
Loss in iteration 66 : 0.6793478024582195
Loss in iteration 67 : 0.6789815634592722
Loss in iteration 68 : 0.6783513600062199
Loss in iteration 69 : 0.6795438508713432
Loss in iteration 70 : 0.6789783151884174
Loss in iteration 71 : 0.6792366066628196
Loss in iteration 72 : 0.6785049171503074
Loss in iteration 73 : 0.677562391052838
Loss in iteration 74 : 0.6777896752366974
Loss in iteration 75 : 0.6775939027348078
Loss in iteration 76 : 0.6780272223741363
Loss in iteration 77 : 0.6783750352773646
Loss in iteration 78 : 0.6780710539727572
Loss in iteration 79 : 0.6756293581777744
Loss in iteration 80 : 0.6762015129239616
Loss in iteration 81 : 0.6773320763693166
Loss in iteration 82 : 0.6760053602904795
Loss in iteration 83 : 0.677513753285619
Loss in iteration 84 : 0.6760369338089989
Loss in iteration 85 : 0.6765092249803725
Loss in iteration 86 : 0.6765662392116031
Loss in iteration 87 : 0.6735415339563505
Loss in iteration 88 : 0.6768998817388785
Loss in iteration 89 : 0.6743583380892703
Loss in iteration 90 : 0.6750852741602825
Loss in iteration 91 : 0.6755924983849753
Loss in iteration 92 : 0.6730677376640439
Loss in iteration 93 : 0.6730104810498797
Loss in iteration 94 : 0.6739839540030903
Loss in iteration 95 : 0.6745211020339681
Loss in iteration 96 : 0.6760429409537425
Loss in iteration 97 : 0.6722523256422059
Loss in iteration 98 : 0.6733933051586757
Loss in iteration 99 : 0.673225595823576
Loss in iteration 100 : 0.6742076189024675
Loss in iteration 101 : 0.6722564747002983
Loss in iteration 102 : 0.6727569600145822
Loss in iteration 103 : 0.6718131613582434
Loss in iteration 104 : 0.6726263527104913
Loss in iteration 105 : 0.6727943616966092
Loss in iteration 106 : 0.6723167067302424
Loss in iteration 107 : 0.6720768402327085
Loss in iteration 108 : 0.6734174746912827
Loss in iteration 109 : 0.6716610381418576
Loss in iteration 110 : 0.6712458830357971
Loss in iteration 111 : 0.671897563241362
Loss in iteration 112 : 0.6703300255766466
Loss in iteration 113 : 0.6700445936282078
Loss in iteration 114 : 0.6713232602584122
Loss in iteration 115 : 0.6700497513051888
Loss in iteration 116 : 0.6702678371938197
Loss in iteration 117 : 0.6699578067026273
Loss in iteration 118 : 0.6690076212281669
Loss in iteration 119 : 0.6687645736766841
Loss in iteration 120 : 0.6700703098992299
Loss in iteration 121 : 0.6694365160473029
Loss in iteration 122 : 0.6690863800188832
Loss in iteration 123 : 0.669354895074614
Loss in iteration 124 : 0.6682774102323586
Loss in iteration 125 : 0.669467957476507
Loss in iteration 126 : 0.6688730978488785
Loss in iteration 127 : 0.6685551707792488
Loss in iteration 128 : 0.6662562682000904
Loss in iteration 129 : 0.6682902867625657
Loss in iteration 130 : 0.6675058497732278
Loss in iteration 131 : 0.6668789179951264
Loss in iteration 132 : 0.6685114865758891
Loss in iteration 133 : 0.6665081622323971
Loss in iteration 134 : 0.667695764308984
Loss in iteration 135 : 0.6682576592254387
Loss in iteration 136 : 0.6676431667055135
Loss in iteration 137 : 0.6668844728330712
Loss in iteration 138 : 0.6681445388985722
Loss in iteration 139 : 0.6655832103332097
Loss in iteration 140 : 0.6666465764807495
Loss in iteration 141 : 0.6658433400293766
Loss in iteration 142 : 0.6662171439329355
Loss in iteration 143 : 0.6649643377188338
Loss in iteration 144 : 0.6643385186531742
Loss in iteration 145 : 0.664129742060654
Loss in iteration 146 : 0.6632944254362396
Loss in iteration 147 : 0.6643127337463012
Loss in iteration 148 : 0.6652726079207312
Loss in iteration 149 : 0.6661374186539365
Loss in iteration 150 : 0.6639037316948336
Loss in iteration 151 : 0.6636943570133018
Loss in iteration 152 : 0.6634573843284888
Loss in iteration 153 : 0.6608976097904355
Loss in iteration 154 : 0.6648522541789051
Loss in iteration 155 : 0.6639521017065582
Loss in iteration 156 : 0.6632454765946582
Loss in iteration 157 : 0.6627909484799354
Loss in iteration 158 : 0.6611762841646185
Loss in iteration 159 : 0.6620282461136832
Loss in iteration 160 : 0.6608669233513313
Loss in iteration 161 : 0.6634842992946373
Loss in iteration 162 : 0.6617268344396199
Loss in iteration 163 : 0.6620892172551659
Loss in iteration 164 : 0.6625058877892489
Loss in iteration 165 : 0.6606386166011573
Loss in iteration 166 : 0.6609969721722142
Loss in iteration 167 : 0.6632372821633853
Loss in iteration 168 : 0.6614022593323723
Loss in iteration 169 : 0.6611894241969868
Loss in iteration 170 : 0.6600502335814282
Loss in iteration 171 : 0.6613371611186251
Loss in iteration 172 : 0.6589747565530624
Loss in iteration 173 : 0.659781970648134
Loss in iteration 174 : 0.6605123678065954
Loss in iteration 175 : 0.6603995958314555
Loss in iteration 176 : 0.6581343782361313
Loss in iteration 177 : 0.6590668158562452
Loss in iteration 178 : 0.6592336050724208
Loss in iteration 179 : 0.6596189762490143
Loss in iteration 180 : 0.6585753545913033
Loss in iteration 181 : 0.6591590669587784
Loss in iteration 182 : 0.6597557326335163
Loss in iteration 183 : 0.6565076254004195
Loss in iteration 184 : 0.6577476264072032
Loss in iteration 185 : 0.6572120665182758
Loss in iteration 186 : 0.6561122779292724
Loss in iteration 187 : 0.6560712942488524
Loss in iteration 188 : 0.6572113274233109
Loss in iteration 189 : 0.6582450002130056
Loss in iteration 190 : 0.6560141886909862
Loss in iteration 191 : 0.6567689531155467
Loss in iteration 192 : 0.6561647389673052
Loss in iteration 193 : 0.6552776840468958
Loss in iteration 194 : 0.6572633798005675
Loss in iteration 195 : 0.6568712454381266
Loss in iteration 196 : 0.6575557560193703
Loss in iteration 197 : 0.6553894630870184
Loss in iteration 198 : 0.6563283861298604
Loss in iteration 199 : 0.6557316426414824
Loss in iteration 200 : 0.6550413684749814
Loss in iteration 201 : 0.6554165128616938
Loss in iteration 202 : 0.6546625301485798
Loss in iteration 203 : 0.6534452508514077
Loss in iteration 204 : 0.6539875905623168
Loss in iteration 205 : 0.6539336235777663
Loss in iteration 206 : 0.6530388528352692
Loss in iteration 207 : 0.6540835999128924
Loss in iteration 208 : 0.6512485674341908
Loss in iteration 209 : 0.6544885714596866
Loss in iteration 210 : 0.6551264357278003
Loss in iteration 211 : 0.6533518933910407
Loss in iteration 212 : 0.6533989733576776
Loss in iteration 213 : 0.6556962538046615
Loss in iteration 214 : 0.6552141915195354
Loss in iteration 215 : 0.6531842949655964
Loss in iteration 216 : 0.653379552637535
Loss in iteration 217 : 0.6507896841003447
Loss in iteration 218 : 0.6499548157101037
Loss in iteration 219 : 0.6516406791689386
Loss in iteration 220 : 0.6518571206022512
Loss in iteration 221 : 0.6512334326501686
Loss in iteration 222 : 0.651218898949865
Loss in iteration 223 : 0.6514613189192796
Loss in iteration 224 : 0.6507611641567314
Loss in iteration 225 : 0.6517904058336801
Loss in iteration 226 : 0.6496286260400626
Loss in iteration 227 : 0.6498695127720486
Loss in iteration 228 : 0.6504781061090537
Loss in iteration 229 : 0.6513455411915106
Loss in iteration 230 : 0.6482831567612269
Loss in iteration 231 : 0.6493005979719175
Loss in iteration 232 : 0.6501727874650934
Loss in iteration 233 : 0.6496042842217442
Loss in iteration 234 : 0.6488519623717989
Loss in iteration 235 : 0.6475598932739594
Loss in iteration 236 : 0.6481211353010928
Loss in iteration 237 : 0.6486819129734571
Loss in iteration 238 : 0.6469636941328321
Loss in iteration 239 : 0.6461187677447972
Loss in iteration 240 : 0.6478610939768317
Loss in iteration 241 : 0.6452272658873958
Loss in iteration 242 : 0.6505108347101543
Loss in iteration 243 : 0.6497530899139009
Loss in iteration 244 : 0.6470408649384805
Loss in iteration 245 : 0.6486489595240484
Loss in iteration 246 : 0.6472947703774288
Loss in iteration 247 : 0.6456075355680378
Loss in iteration 248 : 0.6453945950776925
Loss in iteration 249 : 0.647143764979315
Loss in iteration 250 : 0.6454984039574262
Loss in iteration 251 : 0.6493478504497929
Loss in iteration 252 : 0.6459565894639153
Loss in iteration 253 : 0.6458373082430039
Loss in iteration 254 : 0.644229252123732
Loss in iteration 255 : 0.6421775143524329
Loss in iteration 256 : 0.6468855299713897
Loss in iteration 257 : 0.6441477451662451
Loss in iteration 258 : 0.6451470820435075
Loss in iteration 259 : 0.6432606033488292
Loss in iteration 260 : 0.6455365446539202
Loss in iteration 261 : 0.6431033483944061
Loss in iteration 262 : 0.642134074200382
Loss in iteration 263 : 0.6421524739984219
Loss in iteration 264 : 0.6450148118544212
Loss in iteration 265 : 0.6415780989261975
Loss in iteration 266 : 0.6443959876243375
Loss in iteration 267 : 0.6440514819038923
Loss in iteration 268 : 0.6429893045839653
Loss in iteration 269 : 0.6428674326700758
Loss in iteration 270 : 0.6440707231714571
Loss in iteration 271 : 0.6424832144785699
Loss in iteration 272 : 0.6454175813272245
Loss in iteration 273 : 0.6396524860446755
Loss in iteration 274 : 0.6411265805351066
Loss in iteration 275 : 0.6408301133269465
Loss in iteration 276 : 0.6428203424225758
Loss in iteration 277 : 0.6404107627340863
Loss in iteration 278 : 0.6385792844671369
Loss in iteration 279 : 0.6405580897429947
Loss in iteration 280 : 0.640489661753769
Loss in iteration 281 : 0.6425951071903335
Loss in iteration 282 : 0.6406736941929486
Loss in iteration 283 : 0.6393805463573501
Loss in iteration 284 : 0.640037346805457
Loss in iteration 285 : 0.639919989252374
Loss in iteration 286 : 0.6386269851169958
Loss in iteration 287 : 0.6406266756891562
Loss in iteration 288 : 0.63994604485219
Loss in iteration 289 : 0.6386910032541007
Loss in iteration 290 : 0.6367565746662726
Loss in iteration 291 : 0.6371292871005363
Loss in iteration 292 : 0.6399441847346014
Loss in iteration 293 : 0.63684675966288
Loss in iteration 294 : 0.6369357074221275
Loss in iteration 295 : 0.6381551950132568
Loss in iteration 296 : 0.6388667998093928
Loss in iteration 297 : 0.6373922645732767
Loss in iteration 298 : 0.6382335926299811
Loss in iteration 299 : 0.6370470458176438
Loss in iteration 300 : 0.6382498246829245
Loss in iteration 301 : 0.6373585192249115
Loss in iteration 302 : 0.637178683793874
Loss in iteration 303 : 0.635898185042012
Loss in iteration 304 : 0.6365435357628308
Loss in iteration 305 : 0.6347531115003879
Loss in iteration 306 : 0.6369179804463456
Loss in iteration 307 : 0.6347707765064806
Loss in iteration 308 : 0.6343809574108676
Loss in iteration 309 : 0.6351873846937451
Loss in iteration 310 : 0.6351974282570572
Loss in iteration 311 : 0.6366560403643605
Loss in iteration 312 : 0.6379454854667675
Loss in iteration 313 : 0.6343208606782992
Loss in iteration 314 : 0.6347442463105187
Loss in iteration 315 : 0.6335878490864162
Loss in iteration 316 : 0.6377173335256775
Loss in iteration 317 : 0.6318655597579227
Loss in iteration 318 : 0.6365352456864867
Loss in iteration 319 : 0.6340778672433968
Loss in iteration 320 : 0.6325285289962771
Loss in iteration 321 : 0.6322804297489601
Loss in iteration 322 : 0.6318700178029218
Loss in iteration 323 : 0.6329110553397269
Loss in iteration 324 : 0.6336841340131268
Loss in iteration 325 : 0.6315592212885368
Loss in iteration 326 : 0.6329169069176608
Loss in iteration 327 : 0.6305452103430188
Loss in iteration 328 : 0.633280798863426
Loss in iteration 329 : 0.6334378407449293
Loss in iteration 330 : 0.6306818938715371
Loss in iteration 331 : 0.631955016066221
Loss in iteration 332 : 0.6295589727771408
Loss in iteration 333 : 0.6314030440168555
Loss in iteration 334 : 0.6325398552570944
Loss in iteration 335 : 0.6309858695171409
Loss in iteration 336 : 0.6305145396519977
Loss in iteration 337 : 0.628672910005467
Loss in iteration 338 : 0.6332652690181911
Loss in iteration 339 : 0.6327951439629883
Loss in iteration 340 : 0.6304533040010606
Loss in iteration 341 : 0.6276976021783393
Loss in iteration 342 : 0.6303164319168105
Loss in iteration 343 : 0.6296542060235817
Loss in iteration 344 : 0.6329131710625592
Loss in iteration 345 : 0.6312377381983751
Loss in iteration 346 : 0.629571719174301
Loss in iteration 347 : 0.6307945295427185
Loss in iteration 348 : 0.6294723625547664
Loss in iteration 349 : 0.6300780092969966
Loss in iteration 350 : 0.6279867567789896
Loss in iteration 351 : 0.6271289332455705
Loss in iteration 352 : 0.6286664729898653
Loss in iteration 353 : 0.6282012576233124
Loss in iteration 354 : 0.6288743442481328
Loss in iteration 355 : 0.6279432694590759
Loss in iteration 356 : 0.6270271124109599
Loss in iteration 357 : 0.6270982046261907
Loss in iteration 358 : 0.6284389784701474
Loss in iteration 359 : 0.6265790625517994
Loss in iteration 360 : 0.6284729933382233
Loss in iteration 361 : 0.6259612990566418
Loss in iteration 362 : 0.6285155034129613
Loss in iteration 363 : 0.6269504512200387
Loss in iteration 364 : 0.6289708376962242
Loss in iteration 365 : 0.6259318900288082
Loss in iteration 366 : 0.6267717089238598
Loss in iteration 367 : 0.6250324386168333
Loss in iteration 368 : 0.6266723381920176
Loss in iteration 369 : 0.6287641636038276
Loss in iteration 370 : 0.6253268176670927
Loss in iteration 371 : 0.628829315959514
Loss in iteration 372 : 0.6243651230932982
Loss in iteration 373 : 0.6256123169344253
Loss in iteration 374 : 0.6239528547246056
Loss in iteration 375 : 0.6243679813113406
Loss in iteration 376 : 0.6249549090649017
Loss in iteration 377 : 0.6253896898312825
Loss in iteration 378 : 0.6231976158017092
Loss in iteration 379 : 0.6234926756840972
Loss in iteration 380 : 0.6249200157422707
Loss in iteration 381 : 0.6252489754026513
Loss in iteration 382 : 0.6237788574020056
Loss in iteration 383 : 0.6223160145446371
Loss in iteration 384 : 0.621448823405562
Loss in iteration 385 : 0.623788326235551
Loss in iteration 386 : 0.6231686330019692
Loss in iteration 387 : 0.6252542622876557
Loss in iteration 388 : 0.6245303070833295
Loss in iteration 389 : 0.6232646393346001
Loss in iteration 390 : 0.623741794763994
Loss in iteration 391 : 0.6213538176246713
Loss in iteration 392 : 0.619956707068664
Loss in iteration 393 : 0.619130630381952
Loss in iteration 394 : 0.6208435219997638
Loss in iteration 395 : 0.6214746312075895
Loss in iteration 396 : 0.621318989794238
Loss in iteration 397 : 0.6231111257938553
Loss in iteration 398 : 0.621116461811157
Loss in iteration 399 : 0.6195522994685221
Loss in iteration 400 : 0.6208430001493088
Testing accuracy  of updater 4 on alg 0 with rate 100.0 = 0.734375, training accuracy 0.734375, time elapsed: 5169 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6929433221520231
Loss in iteration 3 : 0.6926577089007472
Loss in iteration 4 : 0.6925307644908585
Loss in iteration 5 : 0.6922247642690335
Loss in iteration 6 : 0.6921086413643279
Loss in iteration 7 : 0.691813406180849
Loss in iteration 8 : 0.6915261270578248
Loss in iteration 9 : 0.6914029089972621
Loss in iteration 10 : 0.6913474302534987
Loss in iteration 11 : 0.6907593470540198
Loss in iteration 12 : 0.6905671099062677
Loss in iteration 13 : 0.6905859951629546
Loss in iteration 14 : 0.6901316898167106
Loss in iteration 15 : 0.6901106745324649
Loss in iteration 16 : 0.6898462853700551
Loss in iteration 17 : 0.6893247295105349
Loss in iteration 18 : 0.6892114280618151
Loss in iteration 19 : 0.6892742436146446
Loss in iteration 20 : 0.6893537562943199
Loss in iteration 21 : 0.6888182896807824
Loss in iteration 22 : 0.6885217076278021
Loss in iteration 23 : 0.6885231079211237
Loss in iteration 24 : 0.6876411136020478
Loss in iteration 25 : 0.6880626830356736
Loss in iteration 26 : 0.687022640040458
Loss in iteration 27 : 0.6871253308769142
Loss in iteration 28 : 0.6873454824060324
Loss in iteration 29 : 0.6872050954594167
Loss in iteration 30 : 0.6873923339861742
Loss in iteration 31 : 0.6869662940252204
Loss in iteration 32 : 0.6869454687562611
Loss in iteration 33 : 0.6867806488229141
Loss in iteration 34 : 0.6863577215748786
Loss in iteration 35 : 0.6861362593011341
Loss in iteration 36 : 0.6855635561320247
Loss in iteration 37 : 0.6851396405231831
Loss in iteration 38 : 0.6854922078800081
Loss in iteration 39 : 0.6852723499388548
Loss in iteration 40 : 0.6852282970877842
Loss in iteration 41 : 0.6847335236056763
Loss in iteration 42 : 0.6840958438443017
Loss in iteration 43 : 0.6842303855715365
Loss in iteration 44 : 0.6838116455349307
Loss in iteration 45 : 0.6840057431682407
Loss in iteration 46 : 0.6837467681813535
Loss in iteration 47 : 0.6834375326431041
Loss in iteration 48 : 0.6835390930806223
Loss in iteration 49 : 0.6830724649543395
Loss in iteration 50 : 0.6830126373190685
Loss in iteration 51 : 0.6828543049958775
Loss in iteration 52 : 0.6827290754474833
Loss in iteration 53 : 0.682082803738299
Loss in iteration 54 : 0.6813519770399485
Loss in iteration 55 : 0.6809879344622907
Loss in iteration 56 : 0.6820424004362867
Loss in iteration 57 : 0.6815036252949241
Loss in iteration 58 : 0.6816001711207395
Loss in iteration 59 : 0.6809410877517313
Loss in iteration 60 : 0.681068533223391
Loss in iteration 61 : 0.6814783238700536
Loss in iteration 62 : 0.6801995883829011
Loss in iteration 63 : 0.6799708443880383
Loss in iteration 64 : 0.6799663290368586
Loss in iteration 65 : 0.6798607425843376
Loss in iteration 66 : 0.6793478024582195
Loss in iteration 67 : 0.6789815634592722
Loss in iteration 68 : 0.6783513600062199
Loss in iteration 69 : 0.6795438508713432
Loss in iteration 70 : 0.6789783151884174
Loss in iteration 71 : 0.6792366066628196
Loss in iteration 72 : 0.6785049171503074
Loss in iteration 73 : 0.677562391052838
Loss in iteration 74 : 0.6777896752366974
Loss in iteration 75 : 0.6775939027348078
Loss in iteration 76 : 0.6780272223741363
Loss in iteration 77 : 0.6783750352773646
Loss in iteration 78 : 0.6780710539727572
Loss in iteration 79 : 0.6756293581777744
Loss in iteration 80 : 0.6762015129239616
Loss in iteration 81 : 0.6773320763693166
Loss in iteration 82 : 0.6760053602904795
Loss in iteration 83 : 0.677513753285619
Loss in iteration 84 : 0.6760369338089989
Loss in iteration 85 : 0.6765092249803725
Loss in iteration 86 : 0.6765662392116031
Loss in iteration 87 : 0.6735415339563505
Loss in iteration 88 : 0.6768998817388785
Loss in iteration 89 : 0.6743583380892703
Loss in iteration 90 : 0.6750852741602825
Loss in iteration 91 : 0.6755924983849753
Loss in iteration 92 : 0.6730677376640439
Loss in iteration 93 : 0.6730104810498797
Loss in iteration 94 : 0.6739839540030903
Loss in iteration 95 : 0.6745211020339681
Loss in iteration 96 : 0.6760429409537425
Loss in iteration 97 : 0.6722523256422059
Loss in iteration 98 : 0.6733933051586757
Loss in iteration 99 : 0.673225595823576
Loss in iteration 100 : 0.6742076189024675
Loss in iteration 101 : 0.6722564747002983
Loss in iteration 102 : 0.6727569600145822
Loss in iteration 103 : 0.6718131613582434
Loss in iteration 104 : 0.6726263527104913
Loss in iteration 105 : 0.6727943616966092
Loss in iteration 106 : 0.6723167067302424
Loss in iteration 107 : 0.6720768402327085
Loss in iteration 108 : 0.6734174746912827
Loss in iteration 109 : 0.6716610381418576
Loss in iteration 110 : 0.6712458830357971
Loss in iteration 111 : 0.671897563241362
Loss in iteration 112 : 0.6703300255766466
Loss in iteration 113 : 0.6700445936282078
Loss in iteration 114 : 0.6713232602584122
Loss in iteration 115 : 0.6700497513051888
Loss in iteration 116 : 0.6702678371938197
Loss in iteration 117 : 0.6699578067026273
Loss in iteration 118 : 0.6690076212281669
Loss in iteration 119 : 0.6687645736766841
Loss in iteration 120 : 0.6700703098992299
Loss in iteration 121 : 0.6694365160473029
Loss in iteration 122 : 0.6690863800188832
Loss in iteration 123 : 0.669354895074614
Loss in iteration 124 : 0.6682774102323586
Loss in iteration 125 : 0.669467957476507
Loss in iteration 126 : 0.6688730978488785
Loss in iteration 127 : 0.6685551707792488
Loss in iteration 128 : 0.6662562682000904
Loss in iteration 129 : 0.6682902867625657
Loss in iteration 130 : 0.6675058497732278
Loss in iteration 131 : 0.6668789179951264
Loss in iteration 132 : 0.6685114865758891
Loss in iteration 133 : 0.6665081622323971
Loss in iteration 134 : 0.667695764308984
Loss in iteration 135 : 0.6682576592254387
Loss in iteration 136 : 0.6676431667055135
Loss in iteration 137 : 0.6668844728330712
Loss in iteration 138 : 0.6681445388985722
Loss in iteration 139 : 0.6655832103332097
Loss in iteration 140 : 0.6666465764807495
Loss in iteration 141 : 0.6658433400293766
Loss in iteration 142 : 0.6662171439329355
Loss in iteration 143 : 0.6649643377188338
Loss in iteration 144 : 0.6643385186531742
Loss in iteration 145 : 0.664129742060654
Loss in iteration 146 : 0.6632944254362396
Loss in iteration 147 : 0.6643127337463012
Loss in iteration 148 : 0.6652726079207312
Loss in iteration 149 : 0.6661374186539365
Loss in iteration 150 : 0.6639037316948336
Loss in iteration 151 : 0.6636943570133018
Loss in iteration 152 : 0.6634573843284888
Loss in iteration 153 : 0.6608976097904355
Loss in iteration 154 : 0.6648522541789051
Loss in iteration 155 : 0.6639521017065582
Loss in iteration 156 : 0.6632454765946582
Loss in iteration 157 : 0.6627909484799354
Loss in iteration 158 : 0.6611762841646185
Loss in iteration 159 : 0.6620282461136832
Loss in iteration 160 : 0.6608669233513313
Loss in iteration 161 : 0.6634842992946373
Loss in iteration 162 : 0.6617268344396199
Loss in iteration 163 : 0.6620892172551659
Loss in iteration 164 : 0.6625058877892489
Loss in iteration 165 : 0.6606386166011573
Loss in iteration 166 : 0.6609969721722142
Loss in iteration 167 : 0.6632372821633853
Loss in iteration 168 : 0.6614022593323723
Loss in iteration 169 : 0.6611894241969868
Loss in iteration 170 : 0.6600502335814282
Loss in iteration 171 : 0.6613371611186251
Loss in iteration 172 : 0.6589747565530624
Loss in iteration 173 : 0.659781970648134
Loss in iteration 174 : 0.6605123678065954
Loss in iteration 175 : 0.6603995958314555
Loss in iteration 176 : 0.6581343782361313
Loss in iteration 177 : 0.6590668158562452
Loss in iteration 178 : 0.6592336050724208
Loss in iteration 179 : 0.6596189762490143
Loss in iteration 180 : 0.6585753545913033
Loss in iteration 181 : 0.6591590669587784
Loss in iteration 182 : 0.6597557326335163
Loss in iteration 183 : 0.6565076254004195
Loss in iteration 184 : 0.6577476264072032
Loss in iteration 185 : 0.6572120665182758
Loss in iteration 186 : 0.6561122779292724
Loss in iteration 187 : 0.6560712942488524
Loss in iteration 188 : 0.6572113274233109
Loss in iteration 189 : 0.6582450002130056
Loss in iteration 190 : 0.6560141886909862
Loss in iteration 191 : 0.6567689531155467
Loss in iteration 192 : 0.6561647389673052
Loss in iteration 193 : 0.6552776840468958
Loss in iteration 194 : 0.6572633798005675
Loss in iteration 195 : 0.6568712454381266
Loss in iteration 196 : 0.6575557560193703
Loss in iteration 197 : 0.6553894630870184
Loss in iteration 198 : 0.6563283861298604
Loss in iteration 199 : 0.6557316426414824
Loss in iteration 200 : 0.6550413684749814
Loss in iteration 201 : 0.6554165128616938
Loss in iteration 202 : 0.6546625301485798
Loss in iteration 203 : 0.6534452508514077
Loss in iteration 204 : 0.6539875905623168
Loss in iteration 205 : 0.6539336235777663
Loss in iteration 206 : 0.6530388528352692
Loss in iteration 207 : 0.6540835999128924
Loss in iteration 208 : 0.6512485674341908
Loss in iteration 209 : 0.6544885714596866
Loss in iteration 210 : 0.6551264357278003
Loss in iteration 211 : 0.6533518933910407
Loss in iteration 212 : 0.6533989733576776
Loss in iteration 213 : 0.6556962538046615
Loss in iteration 214 : 0.6552141915195354
Loss in iteration 215 : 0.6531842949655964
Loss in iteration 216 : 0.653379552637535
Loss in iteration 217 : 0.6507896841003447
Loss in iteration 218 : 0.6499548157101037
Loss in iteration 219 : 0.6516406791689386
Loss in iteration 220 : 0.6518571206022512
Loss in iteration 221 : 0.6512334326501686
Loss in iteration 222 : 0.651218898949865
Loss in iteration 223 : 0.6514613189192796
Loss in iteration 224 : 0.6507611641567314
Loss in iteration 225 : 0.6517904058336801
Loss in iteration 226 : 0.6496286260400626
Loss in iteration 227 : 0.6498695127720486
Loss in iteration 228 : 0.6504781061090537
Loss in iteration 229 : 0.6513455411915106
Loss in iteration 230 : 0.6482831567612269
Loss in iteration 231 : 0.6493005979719175
Loss in iteration 232 : 0.6501727874650934
Loss in iteration 233 : 0.6496042842217442
Loss in iteration 234 : 0.6488519623717989
Loss in iteration 235 : 0.6475598932739594
Loss in iteration 236 : 0.6481211353010928
Loss in iteration 237 : 0.6486819129734571
Loss in iteration 238 : 0.6469636941328321
Loss in iteration 239 : 0.6461187677447972
Loss in iteration 240 : 0.6478610939768317
Loss in iteration 241 : 0.6452272658873958
Loss in iteration 242 : 0.6505108347101543
Loss in iteration 243 : 0.6497530899139009
Loss in iteration 244 : 0.6470408649384805
Loss in iteration 245 : 0.6486489595240484
Loss in iteration 246 : 0.6472947703774288
Loss in iteration 247 : 0.6456075355680378
Loss in iteration 248 : 0.6453945950776925
Loss in iteration 249 : 0.647143764979315
Loss in iteration 250 : 0.6454984039574262
Loss in iteration 251 : 0.6493478504497929
Loss in iteration 252 : 0.6459565894639153
Loss in iteration 253 : 0.6458373082430039
Loss in iteration 254 : 0.644229252123732
Loss in iteration 255 : 0.6421775143524329
Loss in iteration 256 : 0.6468855299713897
Loss in iteration 257 : 0.6441477451662451
Loss in iteration 258 : 0.6451470820435075
Loss in iteration 259 : 0.6432606033488292
Loss in iteration 260 : 0.6455365446539202
Loss in iteration 261 : 0.6431033483944061
Loss in iteration 262 : 0.642134074200382
Loss in iteration 263 : 0.6421524739984219
Loss in iteration 264 : 0.6450148118544212
Loss in iteration 265 : 0.6415780989261975
Loss in iteration 266 : 0.6443959876243375
Loss in iteration 267 : 0.6440514819038923
Loss in iteration 268 : 0.6429893045839653
Loss in iteration 269 : 0.6428674326700758
Loss in iteration 270 : 0.6440707231714571
Loss in iteration 271 : 0.6424832144785699
Loss in iteration 272 : 0.6454175813272245
Loss in iteration 273 : 0.6396524860446755
Loss in iteration 274 : 0.6411265805351066
Loss in iteration 275 : 0.6408301133269465
Loss in iteration 276 : 0.6428203424225758
Loss in iteration 277 : 0.6404107627340863
Loss in iteration 278 : 0.6385792844671369
Loss in iteration 279 : 0.6405580897429947
Loss in iteration 280 : 0.640489661753769
Loss in iteration 281 : 0.6425951071903335
Loss in iteration 282 : 0.6406736941929486
Loss in iteration 283 : 0.6393805463573501
Loss in iteration 284 : 0.640037346805457
Loss in iteration 285 : 0.639919989252374
Loss in iteration 286 : 0.6386269851169958
Loss in iteration 287 : 0.6406266756891562
Loss in iteration 288 : 0.63994604485219
Loss in iteration 289 : 0.6386910032541007
Loss in iteration 290 : 0.6367565746662726
Loss in iteration 291 : 0.6371292871005363
Loss in iteration 292 : 0.6399441847346014
Loss in iteration 293 : 0.63684675966288
Loss in iteration 294 : 0.6369357074221275
Loss in iteration 295 : 0.6381551950132568
Loss in iteration 296 : 0.6388667998093928
Loss in iteration 297 : 0.6373922645732767
Loss in iteration 298 : 0.6382335926299811
Loss in iteration 299 : 0.6370470458176438
Loss in iteration 300 : 0.6382498246829245
Loss in iteration 301 : 0.6373585192249115
Loss in iteration 302 : 0.637178683793874
Loss in iteration 303 : 0.635898185042012
Loss in iteration 304 : 0.6365435357628308
Loss in iteration 305 : 0.6347531115003879
Loss in iteration 306 : 0.6369179804463456
Loss in iteration 307 : 0.6347707765064806
Loss in iteration 308 : 0.6343809574108676
Loss in iteration 309 : 0.6351873846937451
Loss in iteration 310 : 0.6351974282570572
Loss in iteration 311 : 0.6366560403643605
Loss in iteration 312 : 0.6379454854667675
Loss in iteration 313 : 0.6343208606782992
Loss in iteration 314 : 0.6347442463105187
Loss in iteration 315 : 0.6335878490864162
Loss in iteration 316 : 0.6377173335256775
Loss in iteration 317 : 0.6318655597579227
Loss in iteration 318 : 0.6365352456864867
Loss in iteration 319 : 0.6340778672433968
Loss in iteration 320 : 0.6325285289962771
Loss in iteration 321 : 0.6322804297489601
Loss in iteration 322 : 0.6318700178029218
Loss in iteration 323 : 0.6329110553397269
Loss in iteration 324 : 0.6336841340131268
Loss in iteration 325 : 0.6315592212885368
Loss in iteration 326 : 0.6329169069176608
Loss in iteration 327 : 0.6305452103430188
Loss in iteration 328 : 0.633280798863426
Loss in iteration 329 : 0.6334378407449293
Loss in iteration 330 : 0.6306818938715371
Loss in iteration 331 : 0.631955016066221
Loss in iteration 332 : 0.6295589727771408
Loss in iteration 333 : 0.6314030440168555
Loss in iteration 334 : 0.6325398552570944
Loss in iteration 335 : 0.6309858695171409
Loss in iteration 336 : 0.6305145396519977
Loss in iteration 337 : 0.628672910005467
Loss in iteration 338 : 0.6332652690181911
Loss in iteration 339 : 0.6327951439629883
Loss in iteration 340 : 0.6304533040010606
Loss in iteration 341 : 0.6276976021783393
Loss in iteration 342 : 0.6303164319168105
Loss in iteration 343 : 0.6296542060235817
Loss in iteration 344 : 0.6329131710625592
Loss in iteration 345 : 0.6312377381983751
Loss in iteration 346 : 0.629571719174301
Loss in iteration 347 : 0.6307945295427185
Loss in iteration 348 : 0.6294723625547664
Loss in iteration 349 : 0.6300780092969966
Loss in iteration 350 : 0.6279867567789896
Loss in iteration 351 : 0.6271289332455705
Loss in iteration 352 : 0.6286664729898653
Loss in iteration 353 : 0.6282012576233124
Loss in iteration 354 : 0.6288743442481328
Loss in iteration 355 : 0.6279432694590759
Loss in iteration 356 : 0.6270271124109599
Loss in iteration 357 : 0.6270982046261907
Loss in iteration 358 : 0.6284389784701474
Loss in iteration 359 : 0.6265790625517994
Loss in iteration 360 : 0.6284729933382233
Loss in iteration 361 : 0.6259612990566418
Loss in iteration 362 : 0.6285155034129613
Loss in iteration 363 : 0.6269504512200387
Loss in iteration 364 : 0.6289708376962242
Loss in iteration 365 : 0.6259318900288082
Loss in iteration 366 : 0.6267717089238598
Loss in iteration 367 : 0.6250324386168333
Loss in iteration 368 : 0.6266723381920176
Loss in iteration 369 : 0.6287641636038276
Loss in iteration 370 : 0.6253268176670927
Loss in iteration 371 : 0.628829315959514
Loss in iteration 372 : 0.6243651230932982
Loss in iteration 373 : 0.6256123169344253
Loss in iteration 374 : 0.6239528547246056
Loss in iteration 375 : 0.6243679813113406
Loss in iteration 376 : 0.6249549090649017
Loss in iteration 377 : 0.6253896898312825
Loss in iteration 378 : 0.6231976158017092
Loss in iteration 379 : 0.6234926756840972
Loss in iteration 380 : 0.6249200157422707
Loss in iteration 381 : 0.6252489754026513
Loss in iteration 382 : 0.6237788574020056
Loss in iteration 383 : 0.6223160145446371
Loss in iteration 384 : 0.621448823405562
Loss in iteration 385 : 0.623788326235551
Loss in iteration 386 : 0.6231686330019692
Loss in iteration 387 : 0.6252542622876557
Loss in iteration 388 : 0.6245303070833295
Loss in iteration 389 : 0.6232646393346001
Loss in iteration 390 : 0.623741794763994
Loss in iteration 391 : 0.6213538176246713
Loss in iteration 392 : 0.619956707068664
Loss in iteration 393 : 0.619130630381952
Loss in iteration 394 : 0.6208435219997638
Loss in iteration 395 : 0.6214746312075895
Loss in iteration 396 : 0.621318989794238
Loss in iteration 397 : 0.6231111257938553
Loss in iteration 398 : 0.621116461811157
Loss in iteration 399 : 0.6195522994685221
Loss in iteration 400 : 0.6208430001493088
Testing accuracy  of updater 4 on alg 0 with rate 70.0 = 0.734375, training accuracy 0.734375, time elapsed: 5289 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6929433221520231
Loss in iteration 3 : 0.6926577089007472
Loss in iteration 4 : 0.6925307644908585
Loss in iteration 5 : 0.6922247642690335
Loss in iteration 6 : 0.6921086413643279
Loss in iteration 7 : 0.691813406180849
Loss in iteration 8 : 0.6915261270578248
Loss in iteration 9 : 0.6914029089972621
Loss in iteration 10 : 0.6913474302534987
Loss in iteration 11 : 0.6907593470540198
Loss in iteration 12 : 0.6905671099062677
Loss in iteration 13 : 0.6905859951629546
Loss in iteration 14 : 0.6901316898167106
Loss in iteration 15 : 0.6901106745324649
Loss in iteration 16 : 0.6898462853700551
Loss in iteration 17 : 0.6893247295105349
Loss in iteration 18 : 0.6892114280618151
Loss in iteration 19 : 0.6892742436146446
Loss in iteration 20 : 0.6893537562943199
Loss in iteration 21 : 0.6888182896807824
Loss in iteration 22 : 0.6885217076278021
Loss in iteration 23 : 0.6885231079211237
Loss in iteration 24 : 0.6876411136020478
Loss in iteration 25 : 0.6880626830356736
Loss in iteration 26 : 0.687022640040458
Loss in iteration 27 : 0.6871253308769142
Loss in iteration 28 : 0.6873454824060324
Loss in iteration 29 : 0.6872050954594167
Loss in iteration 30 : 0.6873923339861742
Loss in iteration 31 : 0.6869662940252204
Loss in iteration 32 : 0.6869454687562611
Loss in iteration 33 : 0.6867806488229141
Loss in iteration 34 : 0.6863577215748786
Loss in iteration 35 : 0.6861362593011341
Loss in iteration 36 : 0.6855635561320247
Loss in iteration 37 : 0.6851396405231831
Loss in iteration 38 : 0.6854922078800081
Loss in iteration 39 : 0.6852723499388548
Loss in iteration 40 : 0.6852282970877842
Loss in iteration 41 : 0.6847335236056763
Loss in iteration 42 : 0.6840958438443017
Loss in iteration 43 : 0.6842303855715365
Loss in iteration 44 : 0.6838116455349307
Loss in iteration 45 : 0.6840057431682407
Loss in iteration 46 : 0.6837467681813535
Loss in iteration 47 : 0.6834375326431041
Loss in iteration 48 : 0.6835390930806223
Loss in iteration 49 : 0.6830724649543395
Loss in iteration 50 : 0.6830126373190685
Loss in iteration 51 : 0.6828543049958775
Loss in iteration 52 : 0.6827290754474833
Loss in iteration 53 : 0.682082803738299
Loss in iteration 54 : 0.6813519770399485
Loss in iteration 55 : 0.6809879344622907
Loss in iteration 56 : 0.6820424004362867
Loss in iteration 57 : 0.6815036252949241
Loss in iteration 58 : 0.6816001711207395
Loss in iteration 59 : 0.6809410877517313
Loss in iteration 60 : 0.681068533223391
Loss in iteration 61 : 0.6814783238700536
Loss in iteration 62 : 0.6801995883829011
Loss in iteration 63 : 0.6799708443880383
Loss in iteration 64 : 0.6799663290368586
Loss in iteration 65 : 0.6798607425843376
Loss in iteration 66 : 0.6793478024582195
Loss in iteration 67 : 0.6789815634592722
Loss in iteration 68 : 0.6783513600062199
Loss in iteration 69 : 0.6795438508713432
Loss in iteration 70 : 0.6789783151884174
Loss in iteration 71 : 0.6792366066628196
Loss in iteration 72 : 0.6785049171503074
Loss in iteration 73 : 0.677562391052838
Loss in iteration 74 : 0.6777896752366974
Loss in iteration 75 : 0.6775939027348078
Loss in iteration 76 : 0.6780272223741363
Loss in iteration 77 : 0.6783750352773646
Loss in iteration 78 : 0.6780710539727572
Loss in iteration 79 : 0.6756293581777744
Loss in iteration 80 : 0.6762015129239616
Loss in iteration 81 : 0.6773320763693166
Loss in iteration 82 : 0.6760053602904795
Loss in iteration 83 : 0.677513753285619
Loss in iteration 84 : 0.6760369338089989
Loss in iteration 85 : 0.6765092249803725
Loss in iteration 86 : 0.6765662392116031
Loss in iteration 87 : 0.6735415339563505
Loss in iteration 88 : 0.6768998817388785
Loss in iteration 89 : 0.6743583380892703
Loss in iteration 90 : 0.6750852741602825
Loss in iteration 91 : 0.6755924983849753
Loss in iteration 92 : 0.6730677376640439
Loss in iteration 93 : 0.6730104810498797
Loss in iteration 94 : 0.6739839540030903
Loss in iteration 95 : 0.6745211020339681
Loss in iteration 96 : 0.6760429409537425
Loss in iteration 97 : 0.6722523256422059
Loss in iteration 98 : 0.6733933051586757
Loss in iteration 99 : 0.673225595823576
Loss in iteration 100 : 0.6742076189024675
Loss in iteration 101 : 0.6722564747002983
Loss in iteration 102 : 0.6727569600145822
Loss in iteration 103 : 0.6718131613582434
Loss in iteration 104 : 0.6726263527104913
Loss in iteration 105 : 0.6727943616966092
Loss in iteration 106 : 0.6723167067302424
Loss in iteration 107 : 0.6720768402327085
Loss in iteration 108 : 0.6734174746912827
Loss in iteration 109 : 0.6716610381418576
Loss in iteration 110 : 0.6712458830357971
Loss in iteration 111 : 0.671897563241362
Loss in iteration 112 : 0.6703300255766466
Loss in iteration 113 : 0.6700445936282078
Loss in iteration 114 : 0.6713232602584122
Loss in iteration 115 : 0.6700497513051888
Loss in iteration 116 : 0.6702678371938197
Loss in iteration 117 : 0.6699578067026273
Loss in iteration 118 : 0.6690076212281669
Loss in iteration 119 : 0.6687645736766841
Loss in iteration 120 : 0.6700703098992299
Loss in iteration 121 : 0.6694365160473029
Loss in iteration 122 : 0.6690863800188832
Loss in iteration 123 : 0.669354895074614
Loss in iteration 124 : 0.6682774102323586
Loss in iteration 125 : 0.669467957476507
Loss in iteration 126 : 0.6688730978488785
Loss in iteration 127 : 0.6685551707792488
Loss in iteration 128 : 0.6662562682000904
Loss in iteration 129 : 0.6682902867625657
Loss in iteration 130 : 0.6675058497732278
Loss in iteration 131 : 0.6668789179951264
Loss in iteration 132 : 0.6685114865758891
Loss in iteration 133 : 0.6665081622323971
Loss in iteration 134 : 0.667695764308984
Loss in iteration 135 : 0.6682576592254387
Loss in iteration 136 : 0.6676431667055135
Loss in iteration 137 : 0.6668844728330712
Loss in iteration 138 : 0.6681445388985722
Loss in iteration 139 : 0.6655832103332097
Loss in iteration 140 : 0.6666465764807495
Loss in iteration 141 : 0.6658433400293766
Loss in iteration 142 : 0.6662171439329355
Loss in iteration 143 : 0.6649643377188338
Loss in iteration 144 : 0.6643385186531742
Loss in iteration 145 : 0.664129742060654
Loss in iteration 146 : 0.6632944254362396
Loss in iteration 147 : 0.6643127337463012
Loss in iteration 148 : 0.6652726079207312
Loss in iteration 149 : 0.6661374186539365
Loss in iteration 150 : 0.6639037316948336
Loss in iteration 151 : 0.6636943570133018
Loss in iteration 152 : 0.6634573843284888
Loss in iteration 153 : 0.6608976097904355
Loss in iteration 154 : 0.6648522541789051
Loss in iteration 155 : 0.6639521017065582
Loss in iteration 156 : 0.6632454765946582
Loss in iteration 157 : 0.6627909484799354
Loss in iteration 158 : 0.6611762841646185
Loss in iteration 159 : 0.6620282461136832
Loss in iteration 160 : 0.6608669233513313
Loss in iteration 161 : 0.6634842992946373
Loss in iteration 162 : 0.6617268344396199
Loss in iteration 163 : 0.6620892172551659
Loss in iteration 164 : 0.6625058877892489
Loss in iteration 165 : 0.6606386166011573
Loss in iteration 166 : 0.6609969721722142
Loss in iteration 167 : 0.6632372821633853
Loss in iteration 168 : 0.6614022593323723
Loss in iteration 169 : 0.6611894241969868
Loss in iteration 170 : 0.6600502335814282
Loss in iteration 171 : 0.6613371611186251
Loss in iteration 172 : 0.6589747565530624
Loss in iteration 173 : 0.659781970648134
Loss in iteration 174 : 0.6605123678065954
Loss in iteration 175 : 0.6603995958314555
Loss in iteration 176 : 0.6581343782361313
Loss in iteration 177 : 0.6590668158562452
Loss in iteration 178 : 0.6592336050724208
Loss in iteration 179 : 0.6596189762490143
Loss in iteration 180 : 0.6585753545913033
Loss in iteration 181 : 0.6591590669587784
Loss in iteration 182 : 0.6597557326335163
Loss in iteration 183 : 0.6565076254004195
Loss in iteration 184 : 0.6577476264072032
Loss in iteration 185 : 0.6572120665182758
Loss in iteration 186 : 0.6561122779292724
Loss in iteration 187 : 0.6560712942488524
Loss in iteration 188 : 0.6572113274233109
Loss in iteration 189 : 0.6582450002130056
Loss in iteration 190 : 0.6560141886909862
Loss in iteration 191 : 0.6567689531155467
Loss in iteration 192 : 0.6561647389673052
Loss in iteration 193 : 0.6552776840468958
Loss in iteration 194 : 0.6572633798005675
Loss in iteration 195 : 0.6568712454381266
Loss in iteration 196 : 0.6575557560193703
Loss in iteration 197 : 0.6553894630870184
Loss in iteration 198 : 0.6563283861298604
Loss in iteration 199 : 0.6557316426414824
Loss in iteration 200 : 0.6550413684749814
Loss in iteration 201 : 0.6554165128616938
Loss in iteration 202 : 0.6546625301485798
Loss in iteration 203 : 0.6534452508514077
Loss in iteration 204 : 0.6539875905623168
Loss in iteration 205 : 0.6539336235777663
Loss in iteration 206 : 0.6530388528352692
Loss in iteration 207 : 0.6540835999128924
Loss in iteration 208 : 0.6512485674341908
Loss in iteration 209 : 0.6544885714596866
Loss in iteration 210 : 0.6551264357278003
Loss in iteration 211 : 0.6533518933910407
Loss in iteration 212 : 0.6533989733576776
Loss in iteration 213 : 0.6556962538046615
Loss in iteration 214 : 0.6552141915195354
Loss in iteration 215 : 0.6531842949655964
Loss in iteration 216 : 0.653379552637535
Loss in iteration 217 : 0.6507896841003447
Loss in iteration 218 : 0.6499548157101037
Loss in iteration 219 : 0.6516406791689386
Loss in iteration 220 : 0.6518571206022512
Loss in iteration 221 : 0.6512334326501686
Loss in iteration 222 : 0.651218898949865
Loss in iteration 223 : 0.6514613189192796
Loss in iteration 224 : 0.6507611641567314
Loss in iteration 225 : 0.6517904058336801
Loss in iteration 226 : 0.6496286260400626
Loss in iteration 227 : 0.6498695127720486
Loss in iteration 228 : 0.6504781061090537
Loss in iteration 229 : 0.6513455411915106
Loss in iteration 230 : 0.6482831567612269
Loss in iteration 231 : 0.6493005979719175
Loss in iteration 232 : 0.6501727874650934
Loss in iteration 233 : 0.6496042842217442
Loss in iteration 234 : 0.6488519623717989
Loss in iteration 235 : 0.6475598932739594
Loss in iteration 236 : 0.6481211353010928
Loss in iteration 237 : 0.6486819129734571
Loss in iteration 238 : 0.6469636941328321
Loss in iteration 239 : 0.6461187677447972
Loss in iteration 240 : 0.6478610939768317
Loss in iteration 241 : 0.6452272658873958
Loss in iteration 242 : 0.6505108347101543
Loss in iteration 243 : 0.6497530899139009
Loss in iteration 244 : 0.6470408649384805
Loss in iteration 245 : 0.6486489595240484
Loss in iteration 246 : 0.6472947703774288
Loss in iteration 247 : 0.6456075355680378
Loss in iteration 248 : 0.6453945950776925
Loss in iteration 249 : 0.647143764979315
Loss in iteration 250 : 0.6454984039574262
Loss in iteration 251 : 0.6493478504497929
Loss in iteration 252 : 0.6459565894639153
Loss in iteration 253 : 0.6458373082430039
Loss in iteration 254 : 0.644229252123732
Loss in iteration 255 : 0.6421775143524329
Loss in iteration 256 : 0.6468855299713897
Loss in iteration 257 : 0.6441477451662451
Loss in iteration 258 : 0.6451470820435075
Loss in iteration 259 : 0.6432606033488292
Loss in iteration 260 : 0.6455365446539202
Loss in iteration 261 : 0.6431033483944061
Loss in iteration 262 : 0.642134074200382
Loss in iteration 263 : 0.6421524739984219
Loss in iteration 264 : 0.6450148118544212
Loss in iteration 265 : 0.6415780989261975
Loss in iteration 266 : 0.6443959876243375
Loss in iteration 267 : 0.6440514819038923
Loss in iteration 268 : 0.6429893045839653
Loss in iteration 269 : 0.6428674326700758
Loss in iteration 270 : 0.6440707231714571
Loss in iteration 271 : 0.6424832144785699
Loss in iteration 272 : 0.6454175813272245
Loss in iteration 273 : 0.6396524860446755
Loss in iteration 274 : 0.6411265805351066
Loss in iteration 275 : 0.6408301133269465
Loss in iteration 276 : 0.6428203424225758
Loss in iteration 277 : 0.6404107627340863
Loss in iteration 278 : 0.6385792844671369
Loss in iteration 279 : 0.6405580897429947
Loss in iteration 280 : 0.640489661753769
Loss in iteration 281 : 0.6425951071903335
Loss in iteration 282 : 0.6406736941929486
Loss in iteration 283 : 0.6393805463573501
Loss in iteration 284 : 0.640037346805457
Loss in iteration 285 : 0.639919989252374
Loss in iteration 286 : 0.6386269851169958
Loss in iteration 287 : 0.6406266756891562
Loss in iteration 288 : 0.63994604485219
Loss in iteration 289 : 0.6386910032541007
Loss in iteration 290 : 0.6367565746662726
Loss in iteration 291 : 0.6371292871005363
Loss in iteration 292 : 0.6399441847346014
Loss in iteration 293 : 0.63684675966288
Loss in iteration 294 : 0.6369357074221275
Loss in iteration 295 : 0.6381551950132568
Loss in iteration 296 : 0.6388667998093928
Loss in iteration 297 : 0.6373922645732767
Loss in iteration 298 : 0.6382335926299811
Loss in iteration 299 : 0.6370470458176438
Loss in iteration 300 : 0.6382498246829245
Loss in iteration 301 : 0.6373585192249115
Loss in iteration 302 : 0.637178683793874
Loss in iteration 303 : 0.635898185042012
Loss in iteration 304 : 0.6365435357628308
Loss in iteration 305 : 0.6347531115003879
Loss in iteration 306 : 0.6369179804463456
Loss in iteration 307 : 0.6347707765064806
Loss in iteration 308 : 0.6343809574108676
Loss in iteration 309 : 0.6351873846937451
Loss in iteration 310 : 0.6351974282570572
Loss in iteration 311 : 0.6366560403643605
Loss in iteration 312 : 0.6379454854667675
Loss in iteration 313 : 0.6343208606782992
Loss in iteration 314 : 0.6347442463105187
Loss in iteration 315 : 0.6335878490864162
Loss in iteration 316 : 0.6377173335256775
Loss in iteration 317 : 0.6318655597579227
Loss in iteration 318 : 0.6365352456864867
Loss in iteration 319 : 0.6340778672433968
Loss in iteration 320 : 0.6325285289962771
Loss in iteration 321 : 0.6322804297489601
Loss in iteration 322 : 0.6318700178029218
Loss in iteration 323 : 0.6329110553397269
Loss in iteration 324 : 0.6336841340131268
Loss in iteration 325 : 0.6315592212885368
Loss in iteration 326 : 0.6329169069176608
Loss in iteration 327 : 0.6305452103430188
Loss in iteration 328 : 0.633280798863426
Loss in iteration 329 : 0.6334378407449293
Loss in iteration 330 : 0.6306818938715371
Loss in iteration 331 : 0.631955016066221
Loss in iteration 332 : 0.6295589727771408
Loss in iteration 333 : 0.6314030440168555
Loss in iteration 334 : 0.6325398552570944
Loss in iteration 335 : 0.6309858695171409
Loss in iteration 336 : 0.6305145396519977
Loss in iteration 337 : 0.628672910005467
Loss in iteration 338 : 0.6332652690181911
Loss in iteration 339 : 0.6327951439629883
Loss in iteration 340 : 0.6304533040010606
Loss in iteration 341 : 0.6276976021783393
Loss in iteration 342 : 0.6303164319168105
Loss in iteration 343 : 0.6296542060235817
Loss in iteration 344 : 0.6329131710625592
Loss in iteration 345 : 0.6312377381983751
Loss in iteration 346 : 0.629571719174301
Loss in iteration 347 : 0.6307945295427185
Loss in iteration 348 : 0.6294723625547664
Loss in iteration 349 : 0.6300780092969966
Loss in iteration 350 : 0.6279867567789896
Loss in iteration 351 : 0.6271289332455705
Loss in iteration 352 : 0.6286664729898653
Loss in iteration 353 : 0.6282012576233124
Loss in iteration 354 : 0.6288743442481328
Loss in iteration 355 : 0.6279432694590759
Loss in iteration 356 : 0.6270271124109599
Loss in iteration 357 : 0.6270982046261907
Loss in iteration 358 : 0.6284389784701474
Loss in iteration 359 : 0.6265790625517994
Loss in iteration 360 : 0.6284729933382233
Loss in iteration 361 : 0.6259612990566418
Loss in iteration 362 : 0.6285155034129613
Loss in iteration 363 : 0.6269504512200387
Loss in iteration 364 : 0.6289708376962242
Loss in iteration 365 : 0.6259318900288082
Loss in iteration 366 : 0.6267717089238598
Loss in iteration 367 : 0.6250324386168333
Loss in iteration 368 : 0.6266723381920176
Loss in iteration 369 : 0.6287641636038276
Loss in iteration 370 : 0.6253268176670927
Loss in iteration 371 : 0.628829315959514
Loss in iteration 372 : 0.6243651230932982
Loss in iteration 373 : 0.6256123169344253
Loss in iteration 374 : 0.6239528547246056
Loss in iteration 375 : 0.6243679813113406
Loss in iteration 376 : 0.6249549090649017
Loss in iteration 377 : 0.6253896898312825
Loss in iteration 378 : 0.6231976158017092
Loss in iteration 379 : 0.6234926756840972
Loss in iteration 380 : 0.6249200157422707
Loss in iteration 381 : 0.6252489754026513
Loss in iteration 382 : 0.6237788574020056
Loss in iteration 383 : 0.6223160145446371
Loss in iteration 384 : 0.621448823405562
Loss in iteration 385 : 0.623788326235551
Loss in iteration 386 : 0.6231686330019692
Loss in iteration 387 : 0.6252542622876557
Loss in iteration 388 : 0.6245303070833295
Loss in iteration 389 : 0.6232646393346001
Loss in iteration 390 : 0.623741794763994
Loss in iteration 391 : 0.6213538176246713
Loss in iteration 392 : 0.619956707068664
Loss in iteration 393 : 0.619130630381952
Loss in iteration 394 : 0.6208435219997638
Loss in iteration 395 : 0.6214746312075895
Loss in iteration 396 : 0.621318989794238
Loss in iteration 397 : 0.6231111257938553
Loss in iteration 398 : 0.621116461811157
Loss in iteration 399 : 0.6195522994685221
Loss in iteration 400 : 0.6208430001493088
Testing accuracy  of updater 4 on alg 0 with rate 40.0 = 0.734375, training accuracy 0.734375, time elapsed: 5410 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6929433221520231
Loss in iteration 3 : 0.6926577089007472
Loss in iteration 4 : 0.6925307644908585
Loss in iteration 5 : 0.6922247642690335
Loss in iteration 6 : 0.6921086413643279
Loss in iteration 7 : 0.691813406180849
Loss in iteration 8 : 0.6915261270578248
Loss in iteration 9 : 0.6914029089972621
Loss in iteration 10 : 0.6913474302534987
Loss in iteration 11 : 0.6907593470540198
Loss in iteration 12 : 0.6905671099062677
Loss in iteration 13 : 0.6905859951629546
Loss in iteration 14 : 0.6901316898167106
Loss in iteration 15 : 0.6901106745324649
Loss in iteration 16 : 0.6898462853700551
Loss in iteration 17 : 0.6893247295105349
Loss in iteration 18 : 0.6892114280618151
Loss in iteration 19 : 0.6892742436146446
Loss in iteration 20 : 0.6893537562943199
Loss in iteration 21 : 0.6888182896807824
Loss in iteration 22 : 0.6885217076278021
Loss in iteration 23 : 0.6885231079211237
Loss in iteration 24 : 0.6876411136020478
Loss in iteration 25 : 0.6880626830356736
Loss in iteration 26 : 0.687022640040458
Loss in iteration 27 : 0.6871253308769142
Loss in iteration 28 : 0.6873454824060324
Loss in iteration 29 : 0.6872050954594167
Loss in iteration 30 : 0.6873923339861742
Loss in iteration 31 : 0.6869662940252204
Loss in iteration 32 : 0.6869454687562611
Loss in iteration 33 : 0.6867806488229141
Loss in iteration 34 : 0.6863577215748786
Loss in iteration 35 : 0.6861362593011341
Loss in iteration 36 : 0.6855635561320247
Loss in iteration 37 : 0.6851396405231831
Loss in iteration 38 : 0.6854922078800081
Loss in iteration 39 : 0.6852723499388548
Loss in iteration 40 : 0.6852282970877842
Loss in iteration 41 : 0.6847335236056763
Loss in iteration 42 : 0.6840958438443017
Loss in iteration 43 : 0.6842303855715365
Loss in iteration 44 : 0.6838116455349307
Loss in iteration 45 : 0.6840057431682407
Loss in iteration 46 : 0.6837467681813535
Loss in iteration 47 : 0.6834375326431041
Loss in iteration 48 : 0.6835390930806223
Loss in iteration 49 : 0.6830724649543395
Loss in iteration 50 : 0.6830126373190685
Loss in iteration 51 : 0.6828543049958775
Loss in iteration 52 : 0.6827290754474833
Loss in iteration 53 : 0.682082803738299
Loss in iteration 54 : 0.6813519770399485
Loss in iteration 55 : 0.6809879344622907
Loss in iteration 56 : 0.6820424004362867
Loss in iteration 57 : 0.6815036252949241
Loss in iteration 58 : 0.6816001711207395
Loss in iteration 59 : 0.6809410877517313
Loss in iteration 60 : 0.681068533223391
Loss in iteration 61 : 0.6814783238700536
Loss in iteration 62 : 0.6801995883829011
Loss in iteration 63 : 0.6799708443880383
Loss in iteration 64 : 0.6799663290368586
Loss in iteration 65 : 0.6798607425843376
Loss in iteration 66 : 0.6793478024582195
Loss in iteration 67 : 0.6789815634592722
Loss in iteration 68 : 0.6783513600062199
Loss in iteration 69 : 0.6795438508713432
Loss in iteration 70 : 0.6789783151884174
Loss in iteration 71 : 0.6792366066628196
Loss in iteration 72 : 0.6785049171503074
Loss in iteration 73 : 0.677562391052838
Loss in iteration 74 : 0.6777896752366974
Loss in iteration 75 : 0.6775939027348078
Loss in iteration 76 : 0.6780272223741363
Loss in iteration 77 : 0.6783750352773646
Loss in iteration 78 : 0.6780710539727572
Loss in iteration 79 : 0.6756293581777744
Loss in iteration 80 : 0.6762015129239616
Loss in iteration 81 : 0.6773320763693166
Loss in iteration 82 : 0.6760053602904795
Loss in iteration 83 : 0.677513753285619
Loss in iteration 84 : 0.6760369338089989
Loss in iteration 85 : 0.6765092249803725
Loss in iteration 86 : 0.6765662392116031
Loss in iteration 87 : 0.6735415339563505
Loss in iteration 88 : 0.6768998817388785
Loss in iteration 89 : 0.6743583380892703
Loss in iteration 90 : 0.6750852741602825
Loss in iteration 91 : 0.6755924983849753
Loss in iteration 92 : 0.6730677376640439
Loss in iteration 93 : 0.6730104810498797
Loss in iteration 94 : 0.6739839540030903
Loss in iteration 95 : 0.6745211020339681
Loss in iteration 96 : 0.6760429409537425
Loss in iteration 97 : 0.6722523256422059
Loss in iteration 98 : 0.6733933051586757
Loss in iteration 99 : 0.673225595823576
Loss in iteration 100 : 0.6742076189024675
Loss in iteration 101 : 0.6722564747002983
Loss in iteration 102 : 0.6727569600145822
Loss in iteration 103 : 0.6718131613582434
Loss in iteration 104 : 0.6726263527104913
Loss in iteration 105 : 0.6727943616966092
Loss in iteration 106 : 0.6723167067302424
Loss in iteration 107 : 0.6720768402327085
Loss in iteration 108 : 0.6734174746912827
Loss in iteration 109 : 0.6716610381418576
Loss in iteration 110 : 0.6712458830357971
Loss in iteration 111 : 0.671897563241362
Loss in iteration 112 : 0.6703300255766466
Loss in iteration 113 : 0.6700445936282078
Loss in iteration 114 : 0.6713232602584122
Loss in iteration 115 : 0.6700497513051888
Loss in iteration 116 : 0.6702678371938197
Loss in iteration 117 : 0.6699578067026273
Loss in iteration 118 : 0.6690076212281669
Loss in iteration 119 : 0.6687645736766841
Loss in iteration 120 : 0.6700703098992299
Loss in iteration 121 : 0.6694365160473029
Loss in iteration 122 : 0.6690863800188832
Loss in iteration 123 : 0.669354895074614
Loss in iteration 124 : 0.6682774102323586
Loss in iteration 125 : 0.669467957476507
Loss in iteration 126 : 0.6688730978488785
Loss in iteration 127 : 0.6685551707792488
Loss in iteration 128 : 0.6662562682000904
Loss in iteration 129 : 0.6682902867625657
Loss in iteration 130 : 0.6675058497732278
Loss in iteration 131 : 0.6668789179951264
Loss in iteration 132 : 0.6685114865758891
Loss in iteration 133 : 0.6665081622323971
Loss in iteration 134 : 0.667695764308984
Loss in iteration 135 : 0.6682576592254387
Loss in iteration 136 : 0.6676431667055135
Loss in iteration 137 : 0.6668844728330712
Loss in iteration 138 : 0.6681445388985722
Loss in iteration 139 : 0.6655832103332097
Loss in iteration 140 : 0.6666465764807495
Loss in iteration 141 : 0.6658433400293766
Loss in iteration 142 : 0.6662171439329355
Loss in iteration 143 : 0.6649643377188338
Loss in iteration 144 : 0.6643385186531742
Loss in iteration 145 : 0.664129742060654
Loss in iteration 146 : 0.6632944254362396
Loss in iteration 147 : 0.6643127337463012
Loss in iteration 148 : 0.6652726079207312
Loss in iteration 149 : 0.6661374186539365
Loss in iteration 150 : 0.6639037316948336
Loss in iteration 151 : 0.6636943570133018
Loss in iteration 152 : 0.6634573843284888
Loss in iteration 153 : 0.6608976097904355
Loss in iteration 154 : 0.6648522541789051
Loss in iteration 155 : 0.6639521017065582
Loss in iteration 156 : 0.6632454765946582
Loss in iteration 157 : 0.6627909484799354
Loss in iteration 158 : 0.6611762841646185
Loss in iteration 159 : 0.6620282461136832
Loss in iteration 160 : 0.6608669233513313
Loss in iteration 161 : 0.6634842992946373
Loss in iteration 162 : 0.6617268344396199
Loss in iteration 163 : 0.6620892172551659
Loss in iteration 164 : 0.6625058877892489
Loss in iteration 165 : 0.6606386166011573
Loss in iteration 166 : 0.6609969721722142
Loss in iteration 167 : 0.6632372821633853
Loss in iteration 168 : 0.6614022593323723
Loss in iteration 169 : 0.6611894241969868
Loss in iteration 170 : 0.6600502335814282
Loss in iteration 171 : 0.6613371611186251
Loss in iteration 172 : 0.6589747565530624
Loss in iteration 173 : 0.659781970648134
Loss in iteration 174 : 0.6605123678065954
Loss in iteration 175 : 0.6603995958314555
Loss in iteration 176 : 0.6581343782361313
Loss in iteration 177 : 0.6590668158562452
Loss in iteration 178 : 0.6592336050724208
Loss in iteration 179 : 0.6596189762490143
Loss in iteration 180 : 0.6585753545913033
Loss in iteration 181 : 0.6591590669587784
Loss in iteration 182 : 0.6597557326335163
Loss in iteration 183 : 0.6565076254004195
Loss in iteration 184 : 0.6577476264072032
Loss in iteration 185 : 0.6572120665182758
Loss in iteration 186 : 0.6561122779292724
Loss in iteration 187 : 0.6560712942488524
Loss in iteration 188 : 0.6572113274233109
Loss in iteration 189 : 0.6582450002130056
Loss in iteration 190 : 0.6560141886909862
Loss in iteration 191 : 0.6567689531155467
Loss in iteration 192 : 0.6561647389673052
Loss in iteration 193 : 0.6552776840468958
Loss in iteration 194 : 0.6572633798005675
Loss in iteration 195 : 0.6568712454381266
Loss in iteration 196 : 0.6575557560193703
Loss in iteration 197 : 0.6553894630870184
Loss in iteration 198 : 0.6563283861298604
Loss in iteration 199 : 0.6557316426414824
Loss in iteration 200 : 0.6550413684749814
Loss in iteration 201 : 0.6554165128616938
Loss in iteration 202 : 0.6546625301485798
Loss in iteration 203 : 0.6534452508514077
Loss in iteration 204 : 0.6539875905623168
Loss in iteration 205 : 0.6539336235777663
Loss in iteration 206 : 0.6530388528352692
Loss in iteration 207 : 0.6540835999128924
Loss in iteration 208 : 0.6512485674341908
Loss in iteration 209 : 0.6544885714596866
Loss in iteration 210 : 0.6551264357278003
Loss in iteration 211 : 0.6533518933910407
Loss in iteration 212 : 0.6533989733576776
Loss in iteration 213 : 0.6556962538046615
Loss in iteration 214 : 0.6552141915195354
Loss in iteration 215 : 0.6531842949655964
Loss in iteration 216 : 0.653379552637535
Loss in iteration 217 : 0.6507896841003447
Loss in iteration 218 : 0.6499548157101037
Loss in iteration 219 : 0.6516406791689386
Loss in iteration 220 : 0.6518571206022512
Loss in iteration 221 : 0.6512334326501686
Loss in iteration 222 : 0.651218898949865
Loss in iteration 223 : 0.6514613189192796
Loss in iteration 224 : 0.6507611641567314
Loss in iteration 225 : 0.6517904058336801
Loss in iteration 226 : 0.6496286260400626
Loss in iteration 227 : 0.6498695127720486
Loss in iteration 228 : 0.6504781061090537
Loss in iteration 229 : 0.6513455411915106
Loss in iteration 230 : 0.6482831567612269
Loss in iteration 231 : 0.6493005979719175
Loss in iteration 232 : 0.6501727874650934
Loss in iteration 233 : 0.6496042842217442
Loss in iteration 234 : 0.6488519623717989
Loss in iteration 235 : 0.6475598932739594
Loss in iteration 236 : 0.6481211353010928
Loss in iteration 237 : 0.6486819129734571
Loss in iteration 238 : 0.6469636941328321
Loss in iteration 239 : 0.6461187677447972
Loss in iteration 240 : 0.6478610939768317
Loss in iteration 241 : 0.6452272658873958
Loss in iteration 242 : 0.6505108347101543
Loss in iteration 243 : 0.6497530899139009
Loss in iteration 244 : 0.6470408649384805
Loss in iteration 245 : 0.6486489595240484
Loss in iteration 246 : 0.6472947703774288
Loss in iteration 247 : 0.6456075355680378
Loss in iteration 248 : 0.6453945950776925
Loss in iteration 249 : 0.647143764979315
Loss in iteration 250 : 0.6454984039574262
Loss in iteration 251 : 0.6493478504497929
Loss in iteration 252 : 0.6459565894639153
Loss in iteration 253 : 0.6458373082430039
Loss in iteration 254 : 0.644229252123732
Loss in iteration 255 : 0.6421775143524329
Loss in iteration 256 : 0.6468855299713897
Loss in iteration 257 : 0.6441477451662451
Loss in iteration 258 : 0.6451470820435075
Loss in iteration 259 : 0.6432606033488292
Loss in iteration 260 : 0.6455365446539202
Loss in iteration 261 : 0.6431033483944061
Loss in iteration 262 : 0.642134074200382
Loss in iteration 263 : 0.6421524739984219
Loss in iteration 264 : 0.6450148118544212
Loss in iteration 265 : 0.6415780989261975
Loss in iteration 266 : 0.6443959876243375
Loss in iteration 267 : 0.6440514819038923
Loss in iteration 268 : 0.6429893045839653
Loss in iteration 269 : 0.6428674326700758
Loss in iteration 270 : 0.6440707231714571
Loss in iteration 271 : 0.6424832144785699
Loss in iteration 272 : 0.6454175813272245
Loss in iteration 273 : 0.6396524860446755
Loss in iteration 274 : 0.6411265805351066
Loss in iteration 275 : 0.6408301133269465
Loss in iteration 276 : 0.6428203424225758
Loss in iteration 277 : 0.6404107627340863
Loss in iteration 278 : 0.6385792844671369
Loss in iteration 279 : 0.6405580897429947
Loss in iteration 280 : 0.640489661753769
Loss in iteration 281 : 0.6425951071903335
Loss in iteration 282 : 0.6406736941929486
Loss in iteration 283 : 0.6393805463573501
Loss in iteration 284 : 0.640037346805457
Loss in iteration 285 : 0.639919989252374
Loss in iteration 286 : 0.6386269851169958
Loss in iteration 287 : 0.6406266756891562
Loss in iteration 288 : 0.63994604485219
Loss in iteration 289 : 0.6386910032541007
Loss in iteration 290 : 0.6367565746662726
Loss in iteration 291 : 0.6371292871005363
Loss in iteration 292 : 0.6399441847346014
Loss in iteration 293 : 0.63684675966288
Loss in iteration 294 : 0.6369357074221275
Loss in iteration 295 : 0.6381551950132568
Loss in iteration 296 : 0.6388667998093928
Loss in iteration 297 : 0.6373922645732767
Loss in iteration 298 : 0.6382335926299811
Loss in iteration 299 : 0.6370470458176438
Loss in iteration 300 : 0.6382498246829245
Loss in iteration 301 : 0.6373585192249115
Loss in iteration 302 : 0.637178683793874
Loss in iteration 303 : 0.635898185042012
Loss in iteration 304 : 0.6365435357628308
Loss in iteration 305 : 0.6347531115003879
Loss in iteration 306 : 0.6369179804463456
Loss in iteration 307 : 0.6347707765064806
Loss in iteration 308 : 0.6343809574108676
Loss in iteration 309 : 0.6351873846937451
Loss in iteration 310 : 0.6351974282570572
Loss in iteration 311 : 0.6366560403643605
Loss in iteration 312 : 0.6379454854667675
Loss in iteration 313 : 0.6343208606782992
Loss in iteration 314 : 0.6347442463105187
Loss in iteration 315 : 0.6335878490864162
Loss in iteration 316 : 0.6377173335256775
Loss in iteration 317 : 0.6318655597579227
Loss in iteration 318 : 0.6365352456864867
Loss in iteration 319 : 0.6340778672433968
Loss in iteration 320 : 0.6325285289962771
Loss in iteration 321 : 0.6322804297489601
Loss in iteration 322 : 0.6318700178029218
Loss in iteration 323 : 0.6329110553397269
Loss in iteration 324 : 0.6336841340131268
Loss in iteration 325 : 0.6315592212885368
Loss in iteration 326 : 0.6329169069176608
Loss in iteration 327 : 0.6305452103430188
Loss in iteration 328 : 0.633280798863426
Loss in iteration 329 : 0.6334378407449293
Loss in iteration 330 : 0.6306818938715371
Loss in iteration 331 : 0.631955016066221
Loss in iteration 332 : 0.6295589727771408
Loss in iteration 333 : 0.6314030440168555
Loss in iteration 334 : 0.6325398552570944
Loss in iteration 335 : 0.6309858695171409
Loss in iteration 336 : 0.6305145396519977
Loss in iteration 337 : 0.628672910005467
Loss in iteration 338 : 0.6332652690181911
Loss in iteration 339 : 0.6327951439629883
Loss in iteration 340 : 0.6304533040010606
Loss in iteration 341 : 0.6276976021783393
Loss in iteration 342 : 0.6303164319168105
Loss in iteration 343 : 0.6296542060235817
Loss in iteration 344 : 0.6329131710625592
Loss in iteration 345 : 0.6312377381983751
Loss in iteration 346 : 0.629571719174301
Loss in iteration 347 : 0.6307945295427185
Loss in iteration 348 : 0.6294723625547664
Loss in iteration 349 : 0.6300780092969966
Loss in iteration 350 : 0.6279867567789896
Loss in iteration 351 : 0.6271289332455705
Loss in iteration 352 : 0.6286664729898653
Loss in iteration 353 : 0.6282012576233124
Loss in iteration 354 : 0.6288743442481328
Loss in iteration 355 : 0.6279432694590759
Loss in iteration 356 : 0.6270271124109599
Loss in iteration 357 : 0.6270982046261907
Loss in iteration 358 : 0.6284389784701474
Loss in iteration 359 : 0.6265790625517994
Loss in iteration 360 : 0.6284729933382233
Loss in iteration 361 : 0.6259612990566418
Loss in iteration 362 : 0.6285155034129613
Loss in iteration 363 : 0.6269504512200387
Loss in iteration 364 : 0.6289708376962242
Loss in iteration 365 : 0.6259318900288082
Loss in iteration 366 : 0.6267717089238598
Loss in iteration 367 : 0.6250324386168333
Loss in iteration 368 : 0.6266723381920176
Loss in iteration 369 : 0.6287641636038276
Loss in iteration 370 : 0.6253268176670927
Loss in iteration 371 : 0.628829315959514
Loss in iteration 372 : 0.6243651230932982
Loss in iteration 373 : 0.6256123169344253
Loss in iteration 374 : 0.6239528547246056
Loss in iteration 375 : 0.6243679813113406
Loss in iteration 376 : 0.6249549090649017
Loss in iteration 377 : 0.6253896898312825
Loss in iteration 378 : 0.6231976158017092
Loss in iteration 379 : 0.6234926756840972
Loss in iteration 380 : 0.6249200157422707
Loss in iteration 381 : 0.6252489754026513
Loss in iteration 382 : 0.6237788574020056
Loss in iteration 383 : 0.6223160145446371
Loss in iteration 384 : 0.621448823405562
Loss in iteration 385 : 0.623788326235551
Loss in iteration 386 : 0.6231686330019692
Loss in iteration 387 : 0.6252542622876557
Loss in iteration 388 : 0.6245303070833295
Loss in iteration 389 : 0.6232646393346001
Loss in iteration 390 : 0.623741794763994
Loss in iteration 391 : 0.6213538176246713
Loss in iteration 392 : 0.619956707068664
Loss in iteration 393 : 0.619130630381952
Loss in iteration 394 : 0.6208435219997638
Loss in iteration 395 : 0.6214746312075895
Loss in iteration 396 : 0.621318989794238
Loss in iteration 397 : 0.6231111257938553
Loss in iteration 398 : 0.621116461811157
Loss in iteration 399 : 0.6195522994685221
Loss in iteration 400 : 0.6208430001493088
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.734375, training accuracy 0.734375, time elapsed: 5467 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6929433221520231
Loss in iteration 3 : 0.6926577089007472
Loss in iteration 4 : 0.6925307644908585
Loss in iteration 5 : 0.6922247642690335
Loss in iteration 6 : 0.6921086413643279
Loss in iteration 7 : 0.691813406180849
Loss in iteration 8 : 0.6915261270578248
Loss in iteration 9 : 0.6914029089972621
Loss in iteration 10 : 0.6913474302534987
Loss in iteration 11 : 0.6907593470540198
Loss in iteration 12 : 0.6905671099062677
Loss in iteration 13 : 0.6905859951629546
Loss in iteration 14 : 0.6901316898167106
Loss in iteration 15 : 0.6901106745324649
Loss in iteration 16 : 0.6898462853700551
Loss in iteration 17 : 0.6893247295105349
Loss in iteration 18 : 0.6892114280618151
Loss in iteration 19 : 0.6892742436146446
Loss in iteration 20 : 0.6893537562943199
Loss in iteration 21 : 0.6888182896807824
Loss in iteration 22 : 0.6885217076278021
Loss in iteration 23 : 0.6885231079211237
Loss in iteration 24 : 0.6876411136020478
Loss in iteration 25 : 0.6880626830356736
Loss in iteration 26 : 0.687022640040458
Loss in iteration 27 : 0.6871253308769142
Loss in iteration 28 : 0.6873454824060324
Loss in iteration 29 : 0.6872050954594167
Loss in iteration 30 : 0.6873923339861742
Loss in iteration 31 : 0.6869662940252204
Loss in iteration 32 : 0.6869454687562611
Loss in iteration 33 : 0.6867806488229141
Loss in iteration 34 : 0.6863577215748786
Loss in iteration 35 : 0.6861362593011341
Loss in iteration 36 : 0.6855635561320247
Loss in iteration 37 : 0.6851396405231831
Loss in iteration 38 : 0.6854922078800081
Loss in iteration 39 : 0.6852723499388548
Loss in iteration 40 : 0.6852282970877842
Loss in iteration 41 : 0.6847335236056763
Loss in iteration 42 : 0.6840958438443017
Loss in iteration 43 : 0.6842303855715365
Loss in iteration 44 : 0.6838116455349307
Loss in iteration 45 : 0.6840057431682407
Loss in iteration 46 : 0.6837467681813535
Loss in iteration 47 : 0.6834375326431041
Loss in iteration 48 : 0.6835390930806223
Loss in iteration 49 : 0.6830724649543395
Loss in iteration 50 : 0.6830126373190685
Loss in iteration 51 : 0.6828543049958775
Loss in iteration 52 : 0.6827290754474833
Loss in iteration 53 : 0.682082803738299
Loss in iteration 54 : 0.6813519770399485
Loss in iteration 55 : 0.6809879344622907
Loss in iteration 56 : 0.6820424004362867
Loss in iteration 57 : 0.6815036252949241
Loss in iteration 58 : 0.6816001711207395
Loss in iteration 59 : 0.6809410877517313
Loss in iteration 60 : 0.681068533223391
Loss in iteration 61 : 0.6814783238700536
Loss in iteration 62 : 0.6801995883829011
Loss in iteration 63 : 0.6799708443880383
Loss in iteration 64 : 0.6799663290368586
Loss in iteration 65 : 0.6798607425843376
Loss in iteration 66 : 0.6793478024582195
Loss in iteration 67 : 0.6789815634592722
Loss in iteration 68 : 0.6783513600062199
Loss in iteration 69 : 0.6795438508713432
Loss in iteration 70 : 0.6789783151884174
Loss in iteration 71 : 0.6792366066628196
Loss in iteration 72 : 0.6785049171503074
Loss in iteration 73 : 0.677562391052838
Loss in iteration 74 : 0.6777896752366974
Loss in iteration 75 : 0.6775939027348078
Loss in iteration 76 : 0.6780272223741363
Loss in iteration 77 : 0.6783750352773646
Loss in iteration 78 : 0.6780710539727572
Loss in iteration 79 : 0.6756293581777744
Loss in iteration 80 : 0.6762015129239616
Loss in iteration 81 : 0.6773320763693166
Loss in iteration 82 : 0.6760053602904795
Loss in iteration 83 : 0.677513753285619
Loss in iteration 84 : 0.6760369338089989
Loss in iteration 85 : 0.6765092249803725
Loss in iteration 86 : 0.6765662392116031
Loss in iteration 87 : 0.6735415339563505
Loss in iteration 88 : 0.6768998817388785
Loss in iteration 89 : 0.6743583380892703
Loss in iteration 90 : 0.6750852741602825
Loss in iteration 91 : 0.6755924983849753
Loss in iteration 92 : 0.6730677376640439
Loss in iteration 93 : 0.6730104810498797
Loss in iteration 94 : 0.6739839540030903
Loss in iteration 95 : 0.6745211020339681
Loss in iteration 96 : 0.6760429409537425
Loss in iteration 97 : 0.6722523256422059
Loss in iteration 98 : 0.6733933051586757
Loss in iteration 99 : 0.673225595823576
Loss in iteration 100 : 0.6742076189024675
Loss in iteration 101 : 0.6722564747002983
Loss in iteration 102 : 0.6727569600145822
Loss in iteration 103 : 0.6718131613582434
Loss in iteration 104 : 0.6726263527104913
Loss in iteration 105 : 0.6727943616966092
Loss in iteration 106 : 0.6723167067302424
Loss in iteration 107 : 0.6720768402327085
Loss in iteration 108 : 0.6734174746912827
Loss in iteration 109 : 0.6716610381418576
Loss in iteration 110 : 0.6712458830357971
Loss in iteration 111 : 0.671897563241362
Loss in iteration 112 : 0.6703300255766466
Loss in iteration 113 : 0.6700445936282078
Loss in iteration 114 : 0.6713232602584122
Loss in iteration 115 : 0.6700497513051888
Loss in iteration 116 : 0.6702678371938197
Loss in iteration 117 : 0.6699578067026273
Loss in iteration 118 : 0.6690076212281669
Loss in iteration 119 : 0.6687645736766841
Loss in iteration 120 : 0.6700703098992299
Loss in iteration 121 : 0.6694365160473029
Loss in iteration 122 : 0.6690863800188832
Loss in iteration 123 : 0.669354895074614
Loss in iteration 124 : 0.6682774102323586
Loss in iteration 125 : 0.669467957476507
Loss in iteration 126 : 0.6688730978488785
Loss in iteration 127 : 0.6685551707792488
Loss in iteration 128 : 0.6662562682000904
Loss in iteration 129 : 0.6682902867625657
Loss in iteration 130 : 0.6675058497732278
Loss in iteration 131 : 0.6668789179951264
Loss in iteration 132 : 0.6685114865758891
Loss in iteration 133 : 0.6665081622323971
Loss in iteration 134 : 0.667695764308984
Loss in iteration 135 : 0.6682576592254387
Loss in iteration 136 : 0.6676431667055135
Loss in iteration 137 : 0.6668844728330712
Loss in iteration 138 : 0.6681445388985722
Loss in iteration 139 : 0.6655832103332097
Loss in iteration 140 : 0.6666465764807495
Loss in iteration 141 : 0.6658433400293766
Loss in iteration 142 : 0.6662171439329355
Loss in iteration 143 : 0.6649643377188338
Loss in iteration 144 : 0.6643385186531742
Loss in iteration 145 : 0.664129742060654
Loss in iteration 146 : 0.6632944254362396
Loss in iteration 147 : 0.6643127337463012
Loss in iteration 148 : 0.6652726079207312
Loss in iteration 149 : 0.6661374186539365
Loss in iteration 150 : 0.6639037316948336
Loss in iteration 151 : 0.6636943570133018
Loss in iteration 152 : 0.6634573843284888
Loss in iteration 153 : 0.6608976097904355
Loss in iteration 154 : 0.6648522541789051
Loss in iteration 155 : 0.6639521017065582
Loss in iteration 156 : 0.6632454765946582
Loss in iteration 157 : 0.6627909484799354
Loss in iteration 158 : 0.6611762841646185
Loss in iteration 159 : 0.6620282461136832
Loss in iteration 160 : 0.6608669233513313
Loss in iteration 161 : 0.6634842992946373
Loss in iteration 162 : 0.6617268344396199
Loss in iteration 163 : 0.6620892172551659
Loss in iteration 164 : 0.6625058877892489
Loss in iteration 165 : 0.6606386166011573
Loss in iteration 166 : 0.6609969721722142
Loss in iteration 167 : 0.6632372821633853
Loss in iteration 168 : 0.6614022593323723
Loss in iteration 169 : 0.6611894241969868
Loss in iteration 170 : 0.6600502335814282
Loss in iteration 171 : 0.6613371611186251
Loss in iteration 172 : 0.6589747565530624
Loss in iteration 173 : 0.659781970648134
Loss in iteration 174 : 0.6605123678065954
Loss in iteration 175 : 0.6603995958314555
Loss in iteration 176 : 0.6581343782361313
Loss in iteration 177 : 0.6590668158562452
Loss in iteration 178 : 0.6592336050724208
Loss in iteration 179 : 0.6596189762490143
Loss in iteration 180 : 0.6585753545913033
Loss in iteration 181 : 0.6591590669587784
Loss in iteration 182 : 0.6597557326335163
Loss in iteration 183 : 0.6565076254004195
Loss in iteration 184 : 0.6577476264072032
Loss in iteration 185 : 0.6572120665182758
Loss in iteration 186 : 0.6561122779292724
Loss in iteration 187 : 0.6560712942488524
Loss in iteration 188 : 0.6572113274233109
Loss in iteration 189 : 0.6582450002130056
Loss in iteration 190 : 0.6560141886909862
Loss in iteration 191 : 0.6567689531155467
Loss in iteration 192 : 0.6561647389673052
Loss in iteration 193 : 0.6552776840468958
Loss in iteration 194 : 0.6572633798005675
Loss in iteration 195 : 0.6568712454381266
Loss in iteration 196 : 0.6575557560193703
Loss in iteration 197 : 0.6553894630870184
Loss in iteration 198 : 0.6563283861298604
Loss in iteration 199 : 0.6557316426414824
Loss in iteration 200 : 0.6550413684749814
Loss in iteration 201 : 0.6554165128616938
Loss in iteration 202 : 0.6546625301485798
Loss in iteration 203 : 0.6534452508514077
Loss in iteration 204 : 0.6539875905623168
Loss in iteration 205 : 0.6539336235777663
Loss in iteration 206 : 0.6530388528352692
Loss in iteration 207 : 0.6540835999128924
Loss in iteration 208 : 0.6512485674341908
Loss in iteration 209 : 0.6544885714596866
Loss in iteration 210 : 0.6551264357278003
Loss in iteration 211 : 0.6533518933910407
Loss in iteration 212 : 0.6533989733576776
Loss in iteration 213 : 0.6556962538046615
Loss in iteration 214 : 0.6552141915195354
Loss in iteration 215 : 0.6531842949655964
Loss in iteration 216 : 0.653379552637535
Loss in iteration 217 : 0.6507896841003447
Loss in iteration 218 : 0.6499548157101037
Loss in iteration 219 : 0.6516406791689386
Loss in iteration 220 : 0.6518571206022512
Loss in iteration 221 : 0.6512334326501686
Loss in iteration 222 : 0.651218898949865
Loss in iteration 223 : 0.6514613189192796
Loss in iteration 224 : 0.6507611641567314
Loss in iteration 225 : 0.6517904058336801
Loss in iteration 226 : 0.6496286260400626
Loss in iteration 227 : 0.6498695127720486
Loss in iteration 228 : 0.6504781061090537
Loss in iteration 229 : 0.6513455411915106
Loss in iteration 230 : 0.6482831567612269
Loss in iteration 231 : 0.6493005979719175
Loss in iteration 232 : 0.6501727874650934
Loss in iteration 233 : 0.6496042842217442
Loss in iteration 234 : 0.6488519623717989
Loss in iteration 235 : 0.6475598932739594
Loss in iteration 236 : 0.6481211353010928
Loss in iteration 237 : 0.6486819129734571
Loss in iteration 238 : 0.6469636941328321
Loss in iteration 239 : 0.6461187677447972
Loss in iteration 240 : 0.6478610939768317
Loss in iteration 241 : 0.6452272658873958
Loss in iteration 242 : 0.6505108347101543
Loss in iteration 243 : 0.6497530899139009
Loss in iteration 244 : 0.6470408649384805
Loss in iteration 245 : 0.6486489595240484
Loss in iteration 246 : 0.6472947703774288
Loss in iteration 247 : 0.6456075355680378
Loss in iteration 248 : 0.6453945950776925
Loss in iteration 249 : 0.647143764979315
Loss in iteration 250 : 0.6454984039574262
Loss in iteration 251 : 0.6493478504497929
Loss in iteration 252 : 0.6459565894639153
Loss in iteration 253 : 0.6458373082430039
Loss in iteration 254 : 0.644229252123732
Loss in iteration 255 : 0.6421775143524329
Loss in iteration 256 : 0.6468855299713897
Loss in iteration 257 : 0.6441477451662451
Loss in iteration 258 : 0.6451470820435075
Loss in iteration 259 : 0.6432606033488292
Loss in iteration 260 : 0.6455365446539202
Loss in iteration 261 : 0.6431033483944061
Loss in iteration 262 : 0.642134074200382
Loss in iteration 263 : 0.6421524739984219
Loss in iteration 264 : 0.6450148118544212
Loss in iteration 265 : 0.6415780989261975
Loss in iteration 266 : 0.6443959876243375
Loss in iteration 267 : 0.6440514819038923
Loss in iteration 268 : 0.6429893045839653
Loss in iteration 269 : 0.6428674326700758
Loss in iteration 270 : 0.6440707231714571
Loss in iteration 271 : 0.6424832144785699
Loss in iteration 272 : 0.6454175813272245
Loss in iteration 273 : 0.6396524860446755
Loss in iteration 274 : 0.6411265805351066
Loss in iteration 275 : 0.6408301133269465
Loss in iteration 276 : 0.6428203424225758
Loss in iteration 277 : 0.6404107627340863
Loss in iteration 278 : 0.6385792844671369
Loss in iteration 279 : 0.6405580897429947
Loss in iteration 280 : 0.640489661753769
Loss in iteration 281 : 0.6425951071903335
Loss in iteration 282 : 0.6406736941929486
Loss in iteration 283 : 0.6393805463573501
Loss in iteration 284 : 0.640037346805457
Loss in iteration 285 : 0.639919989252374
Loss in iteration 286 : 0.6386269851169958
Loss in iteration 287 : 0.6406266756891562
Loss in iteration 288 : 0.63994604485219
Loss in iteration 289 : 0.6386910032541007
Loss in iteration 290 : 0.6367565746662726
Loss in iteration 291 : 0.6371292871005363
Loss in iteration 292 : 0.6399441847346014
Loss in iteration 293 : 0.63684675966288
Loss in iteration 294 : 0.6369357074221275
Loss in iteration 295 : 0.6381551950132568
Loss in iteration 296 : 0.6388667998093928
Loss in iteration 297 : 0.6373922645732767
Loss in iteration 298 : 0.6382335926299811
Loss in iteration 299 : 0.6370470458176438
Loss in iteration 300 : 0.6382498246829245
Loss in iteration 301 : 0.6373585192249115
Loss in iteration 302 : 0.637178683793874
Loss in iteration 303 : 0.635898185042012
Loss in iteration 304 : 0.6365435357628308
Loss in iteration 305 : 0.6347531115003879
Loss in iteration 306 : 0.6369179804463456
Loss in iteration 307 : 0.6347707765064806
Loss in iteration 308 : 0.6343809574108676
Loss in iteration 309 : 0.6351873846937451
Loss in iteration 310 : 0.6351974282570572
Loss in iteration 311 : 0.6366560403643605
Loss in iteration 312 : 0.6379454854667675
Loss in iteration 313 : 0.6343208606782992
Loss in iteration 314 : 0.6347442463105187
Loss in iteration 315 : 0.6335878490864162
Loss in iteration 316 : 0.6377173335256775
Loss in iteration 317 : 0.6318655597579227
Loss in iteration 318 : 0.6365352456864867
Loss in iteration 319 : 0.6340778672433968
Loss in iteration 320 : 0.6325285289962771
Loss in iteration 321 : 0.6322804297489601
Loss in iteration 322 : 0.6318700178029218
Loss in iteration 323 : 0.6329110553397269
Loss in iteration 324 : 0.6336841340131268
Loss in iteration 325 : 0.6315592212885368
Loss in iteration 326 : 0.6329169069176608
Loss in iteration 327 : 0.6305452103430188
Loss in iteration 328 : 0.633280798863426
Loss in iteration 329 : 0.6334378407449293
Loss in iteration 330 : 0.6306818938715371
Loss in iteration 331 : 0.631955016066221
Loss in iteration 332 : 0.6295589727771408
Loss in iteration 333 : 0.6314030440168555
Loss in iteration 334 : 0.6325398552570944
Loss in iteration 335 : 0.6309858695171409
Loss in iteration 336 : 0.6305145396519977
Loss in iteration 337 : 0.628672910005467
Loss in iteration 338 : 0.6332652690181911
Loss in iteration 339 : 0.6327951439629883
Loss in iteration 340 : 0.6304533040010606
Loss in iteration 341 : 0.6276976021783393
Loss in iteration 342 : 0.6303164319168105
Loss in iteration 343 : 0.6296542060235817
Loss in iteration 344 : 0.6329131710625592
Loss in iteration 345 : 0.6312377381983751
Loss in iteration 346 : 0.629571719174301
Loss in iteration 347 : 0.6307945295427185
Loss in iteration 348 : 0.6294723625547664
Loss in iteration 349 : 0.6300780092969966
Loss in iteration 350 : 0.6279867567789896
Loss in iteration 351 : 0.6271289332455705
Loss in iteration 352 : 0.6286664729898653
Loss in iteration 353 : 0.6282012576233124
Loss in iteration 354 : 0.6288743442481328
Loss in iteration 355 : 0.6279432694590759
Loss in iteration 356 : 0.6270271124109599
Loss in iteration 357 : 0.6270982046261907
Loss in iteration 358 : 0.6284389784701474
Loss in iteration 359 : 0.6265790625517994
Loss in iteration 360 : 0.6284729933382233
Loss in iteration 361 : 0.6259612990566418
Loss in iteration 362 : 0.6285155034129613
Loss in iteration 363 : 0.6269504512200387
Loss in iteration 364 : 0.6289708376962242
Loss in iteration 365 : 0.6259318900288082
Loss in iteration 366 : 0.6267717089238598
Loss in iteration 367 : 0.6250324386168333
Loss in iteration 368 : 0.6266723381920176
Loss in iteration 369 : 0.6287641636038276
Loss in iteration 370 : 0.6253268176670927
Loss in iteration 371 : 0.628829315959514
Loss in iteration 372 : 0.6243651230932982
Loss in iteration 373 : 0.6256123169344253
Loss in iteration 374 : 0.6239528547246056
Loss in iteration 375 : 0.6243679813113406
Loss in iteration 376 : 0.6249549090649017
Loss in iteration 377 : 0.6253896898312825
Loss in iteration 378 : 0.6231976158017092
Loss in iteration 379 : 0.6234926756840972
Loss in iteration 380 : 0.6249200157422707
Loss in iteration 381 : 0.6252489754026513
Loss in iteration 382 : 0.6237788574020056
Loss in iteration 383 : 0.6223160145446371
Loss in iteration 384 : 0.621448823405562
Loss in iteration 385 : 0.623788326235551
Loss in iteration 386 : 0.6231686330019692
Loss in iteration 387 : 0.6252542622876557
Loss in iteration 388 : 0.6245303070833295
Loss in iteration 389 : 0.6232646393346001
Loss in iteration 390 : 0.623741794763994
Loss in iteration 391 : 0.6213538176246713
Loss in iteration 392 : 0.619956707068664
Loss in iteration 393 : 0.619130630381952
Loss in iteration 394 : 0.6208435219997638
Loss in iteration 395 : 0.6214746312075895
Loss in iteration 396 : 0.621318989794238
Loss in iteration 397 : 0.6231111257938553
Loss in iteration 398 : 0.621116461811157
Loss in iteration 399 : 0.6195522994685221
Loss in iteration 400 : 0.6208430001493088
Testing accuracy  of updater 4 on alg 0 with rate 7.0 = 0.734375, training accuracy 0.734375, time elapsed: 5459 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6929433221520231
Loss in iteration 3 : 0.6926577089007472
Loss in iteration 4 : 0.6925307644908585
Loss in iteration 5 : 0.6922247642690335
Loss in iteration 6 : 0.6921086413643279
Loss in iteration 7 : 0.691813406180849
Loss in iteration 8 : 0.6915261270578248
Loss in iteration 9 : 0.6914029089972621
Loss in iteration 10 : 0.6913474302534987
Loss in iteration 11 : 0.6907593470540198
Loss in iteration 12 : 0.6905671099062677
Loss in iteration 13 : 0.6905859951629546
Loss in iteration 14 : 0.6901316898167106
Loss in iteration 15 : 0.6901106745324649
Loss in iteration 16 : 0.6898462853700551
Loss in iteration 17 : 0.6893247295105349
Loss in iteration 18 : 0.6892114280618151
Loss in iteration 19 : 0.6892742436146446
Loss in iteration 20 : 0.6893537562943199
Loss in iteration 21 : 0.6888182896807824
Loss in iteration 22 : 0.6885217076278021
Loss in iteration 23 : 0.6885231079211237
Loss in iteration 24 : 0.6876411136020478
Loss in iteration 25 : 0.6880626830356736
Loss in iteration 26 : 0.687022640040458
Loss in iteration 27 : 0.6871253308769142
Loss in iteration 28 : 0.6873454824060324
Loss in iteration 29 : 0.6872050954594167
Loss in iteration 30 : 0.6873923339861742
Loss in iteration 31 : 0.6869662940252204
Loss in iteration 32 : 0.6869454687562611
Loss in iteration 33 : 0.6867806488229141
Loss in iteration 34 : 0.6863577215748786
Loss in iteration 35 : 0.6861362593011341
Loss in iteration 36 : 0.6855635561320247
Loss in iteration 37 : 0.6851396405231831
Loss in iteration 38 : 0.6854922078800081
Loss in iteration 39 : 0.6852723499388548
Loss in iteration 40 : 0.6852282970877842
Loss in iteration 41 : 0.6847335236056763
Loss in iteration 42 : 0.6840958438443017
Loss in iteration 43 : 0.6842303855715365
Loss in iteration 44 : 0.6838116455349307
Loss in iteration 45 : 0.6840057431682407
Loss in iteration 46 : 0.6837467681813535
Loss in iteration 47 : 0.6834375326431041
Loss in iteration 48 : 0.6835390930806223
Loss in iteration 49 : 0.6830724649543395
Loss in iteration 50 : 0.6830126373190685
Loss in iteration 51 : 0.6828543049958775
Loss in iteration 52 : 0.6827290754474833
Loss in iteration 53 : 0.682082803738299
Loss in iteration 54 : 0.6813519770399485
Loss in iteration 55 : 0.6809879344622907
Loss in iteration 56 : 0.6820424004362867
Loss in iteration 57 : 0.6815036252949241
Loss in iteration 58 : 0.6816001711207395
Loss in iteration 59 : 0.6809410877517313
Loss in iteration 60 : 0.681068533223391
Loss in iteration 61 : 0.6814783238700536
Loss in iteration 62 : 0.6801995883829011
Loss in iteration 63 : 0.6799708443880383
Loss in iteration 64 : 0.6799663290368586
Loss in iteration 65 : 0.6798607425843376
Loss in iteration 66 : 0.6793478024582195
Loss in iteration 67 : 0.6789815634592722
Loss in iteration 68 : 0.6783513600062199
Loss in iteration 69 : 0.6795438508713432
Loss in iteration 70 : 0.6789783151884174
Loss in iteration 71 : 0.6792366066628196
Loss in iteration 72 : 0.6785049171503074
Loss in iteration 73 : 0.677562391052838
Loss in iteration 74 : 0.6777896752366974
Loss in iteration 75 : 0.6775939027348078
Loss in iteration 76 : 0.6780272223741363
Loss in iteration 77 : 0.6783750352773646
Loss in iteration 78 : 0.6780710539727572
Loss in iteration 79 : 0.6756293581777744
Loss in iteration 80 : 0.6762015129239616
Loss in iteration 81 : 0.6773320763693166
Loss in iteration 82 : 0.6760053602904795
Loss in iteration 83 : 0.677513753285619
Loss in iteration 84 : 0.6760369338089989
Loss in iteration 85 : 0.6765092249803725
Loss in iteration 86 : 0.6765662392116031
Loss in iteration 87 : 0.6735415339563505
Loss in iteration 88 : 0.6768998817388785
Loss in iteration 89 : 0.6743583380892703
Loss in iteration 90 : 0.6750852741602825
Loss in iteration 91 : 0.6755924983849753
Loss in iteration 92 : 0.6730677376640439
Loss in iteration 93 : 0.6730104810498797
Loss in iteration 94 : 0.6739839540030903
Loss in iteration 95 : 0.6745211020339681
Loss in iteration 96 : 0.6760429409537425
Loss in iteration 97 : 0.6722523256422059
Loss in iteration 98 : 0.6733933051586757
Loss in iteration 99 : 0.673225595823576
Loss in iteration 100 : 0.6742076189024675
Loss in iteration 101 : 0.6722564747002983
Loss in iteration 102 : 0.6727569600145822
Loss in iteration 103 : 0.6718131613582434
Loss in iteration 104 : 0.6726263527104913
Loss in iteration 105 : 0.6727943616966092
Loss in iteration 106 : 0.6723167067302424
Loss in iteration 107 : 0.6720768402327085
Loss in iteration 108 : 0.6734174746912827
Loss in iteration 109 : 0.6716610381418576
Loss in iteration 110 : 0.6712458830357971
Loss in iteration 111 : 0.671897563241362
Loss in iteration 112 : 0.6703300255766466
Loss in iteration 113 : 0.6700445936282078
Loss in iteration 114 : 0.6713232602584122
Loss in iteration 115 : 0.6700497513051888
Loss in iteration 116 : 0.6702678371938197
Loss in iteration 117 : 0.6699578067026273
Loss in iteration 118 : 0.6690076212281669
Loss in iteration 119 : 0.6687645736766841
Loss in iteration 120 : 0.6700703098992299
Loss in iteration 121 : 0.6694365160473029
Loss in iteration 122 : 0.6690863800188832
Loss in iteration 123 : 0.669354895074614
Loss in iteration 124 : 0.6682774102323586
Loss in iteration 125 : 0.669467957476507
Loss in iteration 126 : 0.6688730978488785
Loss in iteration 127 : 0.6685551707792488
Loss in iteration 128 : 0.6662562682000904
Loss in iteration 129 : 0.6682902867625657
Loss in iteration 130 : 0.6675058497732278
Loss in iteration 131 : 0.6668789179951264
Loss in iteration 132 : 0.6685114865758891
Loss in iteration 133 : 0.6665081622323971
Loss in iteration 134 : 0.667695764308984
Loss in iteration 135 : 0.6682576592254387
Loss in iteration 136 : 0.6676431667055135
Loss in iteration 137 : 0.6668844728330712
Loss in iteration 138 : 0.6681445388985722
Loss in iteration 139 : 0.6655832103332097
Loss in iteration 140 : 0.6666465764807495
Loss in iteration 141 : 0.6658433400293766
Loss in iteration 142 : 0.6662171439329355
Loss in iteration 143 : 0.6649643377188338
Loss in iteration 144 : 0.6643385186531742
Loss in iteration 145 : 0.664129742060654
Loss in iteration 146 : 0.6632944254362396
Loss in iteration 147 : 0.6643127337463012
Loss in iteration 148 : 0.6652726079207312
Loss in iteration 149 : 0.6661374186539365
Loss in iteration 150 : 0.6639037316948336
Loss in iteration 151 : 0.6636943570133018
Loss in iteration 152 : 0.6634573843284888
Loss in iteration 153 : 0.6608976097904355
Loss in iteration 154 : 0.6648522541789051
Loss in iteration 155 : 0.6639521017065582
Loss in iteration 156 : 0.6632454765946582
Loss in iteration 157 : 0.6627909484799354
Loss in iteration 158 : 0.6611762841646185
Loss in iteration 159 : 0.6620282461136832
Loss in iteration 160 : 0.6608669233513313
Loss in iteration 161 : 0.6634842992946373
Loss in iteration 162 : 0.6617268344396199
Loss in iteration 163 : 0.6620892172551659
Loss in iteration 164 : 0.6625058877892489
Loss in iteration 165 : 0.6606386166011573
Loss in iteration 166 : 0.6609969721722142
Loss in iteration 167 : 0.6632372821633853
Loss in iteration 168 : 0.6614022593323723
Loss in iteration 169 : 0.6611894241969868
Loss in iteration 170 : 0.6600502335814282
Loss in iteration 171 : 0.6613371611186251
Loss in iteration 172 : 0.6589747565530624
Loss in iteration 173 : 0.659781970648134
Loss in iteration 174 : 0.6605123678065954
Loss in iteration 175 : 0.6603995958314555
Loss in iteration 176 : 0.6581343782361313
Loss in iteration 177 : 0.6590668158562452
Loss in iteration 178 : 0.6592336050724208
Loss in iteration 179 : 0.6596189762490143
Loss in iteration 180 : 0.6585753545913033
Loss in iteration 181 : 0.6591590669587784
Loss in iteration 182 : 0.6597557326335163
Loss in iteration 183 : 0.6565076254004195
Loss in iteration 184 : 0.6577476264072032
Loss in iteration 185 : 0.6572120665182758
Loss in iteration 186 : 0.6561122779292724
Loss in iteration 187 : 0.6560712942488524
Loss in iteration 188 : 0.6572113274233109
Loss in iteration 189 : 0.6582450002130056
Loss in iteration 190 : 0.6560141886909862
Loss in iteration 191 : 0.6567689531155467
Loss in iteration 192 : 0.6561647389673052
Loss in iteration 193 : 0.6552776840468958
Loss in iteration 194 : 0.6572633798005675
Loss in iteration 195 : 0.6568712454381266
Loss in iteration 196 : 0.6575557560193703
Loss in iteration 197 : 0.6553894630870184
Loss in iteration 198 : 0.6563283861298604
Loss in iteration 199 : 0.6557316426414824
Loss in iteration 200 : 0.6550413684749814
Loss in iteration 201 : 0.6554165128616938
Loss in iteration 202 : 0.6546625301485798
Loss in iteration 203 : 0.6534452508514077
Loss in iteration 204 : 0.6539875905623168
Loss in iteration 205 : 0.6539336235777663
Loss in iteration 206 : 0.6530388528352692
Loss in iteration 207 : 0.6540835999128924
Loss in iteration 208 : 0.6512485674341908
Loss in iteration 209 : 0.6544885714596866
Loss in iteration 210 : 0.6551264357278003
Loss in iteration 211 : 0.6533518933910407
Loss in iteration 212 : 0.6533989733576776
Loss in iteration 213 : 0.6556962538046615
Loss in iteration 214 : 0.6552141915195354
Loss in iteration 215 : 0.6531842949655964
Loss in iteration 216 : 0.653379552637535
Loss in iteration 217 : 0.6507896841003447
Loss in iteration 218 : 0.6499548157101037
Loss in iteration 219 : 0.6516406791689386
Loss in iteration 220 : 0.6518571206022512
Loss in iteration 221 : 0.6512334326501686
Loss in iteration 222 : 0.651218898949865
Loss in iteration 223 : 0.6514613189192796
Loss in iteration 224 : 0.6507611641567314
Loss in iteration 225 : 0.6517904058336801
Loss in iteration 226 : 0.6496286260400626
Loss in iteration 227 : 0.6498695127720486
Loss in iteration 228 : 0.6504781061090537
Loss in iteration 229 : 0.6513455411915106
Loss in iteration 230 : 0.6482831567612269
Loss in iteration 231 : 0.6493005979719175
Loss in iteration 232 : 0.6501727874650934
Loss in iteration 233 : 0.6496042842217442
Loss in iteration 234 : 0.6488519623717989
Loss in iteration 235 : 0.6475598932739594
Loss in iteration 236 : 0.6481211353010928
Loss in iteration 237 : 0.6486819129734571
Loss in iteration 238 : 0.6469636941328321
Loss in iteration 239 : 0.6461187677447972
Loss in iteration 240 : 0.6478610939768317
Loss in iteration 241 : 0.6452272658873958
Loss in iteration 242 : 0.6505108347101543
Loss in iteration 243 : 0.6497530899139009
Loss in iteration 244 : 0.6470408649384805
Loss in iteration 245 : 0.6486489595240484
Loss in iteration 246 : 0.6472947703774288
Loss in iteration 247 : 0.6456075355680378
Loss in iteration 248 : 0.6453945950776925
Loss in iteration 249 : 0.647143764979315
Loss in iteration 250 : 0.6454984039574262
Loss in iteration 251 : 0.6493478504497929
Loss in iteration 252 : 0.6459565894639153
Loss in iteration 253 : 0.6458373082430039
Loss in iteration 254 : 0.644229252123732
Loss in iteration 255 : 0.6421775143524329
Loss in iteration 256 : 0.6468855299713897
Loss in iteration 257 : 0.6441477451662451
Loss in iteration 258 : 0.6451470820435075
Loss in iteration 259 : 0.6432606033488292
Loss in iteration 260 : 0.6455365446539202
Loss in iteration 261 : 0.6431033483944061
Loss in iteration 262 : 0.642134074200382
Loss in iteration 263 : 0.6421524739984219
Loss in iteration 264 : 0.6450148118544212
Loss in iteration 265 : 0.6415780989261975
Loss in iteration 266 : 0.6443959876243375
Loss in iteration 267 : 0.6440514819038923
Loss in iteration 268 : 0.6429893045839653
Loss in iteration 269 : 0.6428674326700758
Loss in iteration 270 : 0.6440707231714571
Loss in iteration 271 : 0.6424832144785699
Loss in iteration 272 : 0.6454175813272245
Loss in iteration 273 : 0.6396524860446755
Loss in iteration 274 : 0.6411265805351066
Loss in iteration 275 : 0.6408301133269465
Loss in iteration 276 : 0.6428203424225758
Loss in iteration 277 : 0.6404107627340863
Loss in iteration 278 : 0.6385792844671369
Loss in iteration 279 : 0.6405580897429947
Loss in iteration 280 : 0.640489661753769
Loss in iteration 281 : 0.6425951071903335
Loss in iteration 282 : 0.6406736941929486
Loss in iteration 283 : 0.6393805463573501
Loss in iteration 284 : 0.640037346805457
Loss in iteration 285 : 0.639919989252374
Loss in iteration 286 : 0.6386269851169958
Loss in iteration 287 : 0.6406266756891562
Loss in iteration 288 : 0.63994604485219
Loss in iteration 289 : 0.6386910032541007
Loss in iteration 290 : 0.6367565746662726
Loss in iteration 291 : 0.6371292871005363
Loss in iteration 292 : 0.6399441847346014
Loss in iteration 293 : 0.63684675966288
Loss in iteration 294 : 0.6369357074221275
Loss in iteration 295 : 0.6381551950132568
Loss in iteration 296 : 0.6388667998093928
Loss in iteration 297 : 0.6373922645732767
Loss in iteration 298 : 0.6382335926299811
Loss in iteration 299 : 0.6370470458176438
Loss in iteration 300 : 0.6382498246829245
Loss in iteration 301 : 0.6373585192249115
Loss in iteration 302 : 0.637178683793874
Loss in iteration 303 : 0.635898185042012
Loss in iteration 304 : 0.6365435357628308
Loss in iteration 305 : 0.6347531115003879
Loss in iteration 306 : 0.6369179804463456
Loss in iteration 307 : 0.6347707765064806
Loss in iteration 308 : 0.6343809574108676
Loss in iteration 309 : 0.6351873846937451
Loss in iteration 310 : 0.6351974282570572
Loss in iteration 311 : 0.6366560403643605
Loss in iteration 312 : 0.6379454854667675
Loss in iteration 313 : 0.6343208606782992
Loss in iteration 314 : 0.6347442463105187
Loss in iteration 315 : 0.6335878490864162
Loss in iteration 316 : 0.6377173335256775
Loss in iteration 317 : 0.6318655597579227
Loss in iteration 318 : 0.6365352456864867
Loss in iteration 319 : 0.6340778672433968
Loss in iteration 320 : 0.6325285289962771
Loss in iteration 321 : 0.6322804297489601
Loss in iteration 322 : 0.6318700178029218
Loss in iteration 323 : 0.6329110553397269
Loss in iteration 324 : 0.6336841340131268
Loss in iteration 325 : 0.6315592212885368
Loss in iteration 326 : 0.6329169069176608
Loss in iteration 327 : 0.6305452103430188
Loss in iteration 328 : 0.633280798863426
Loss in iteration 329 : 0.6334378407449293
Loss in iteration 330 : 0.6306818938715371
Loss in iteration 331 : 0.631955016066221
Loss in iteration 332 : 0.6295589727771408
Loss in iteration 333 : 0.6314030440168555
Loss in iteration 334 : 0.6325398552570944
Loss in iteration 335 : 0.6309858695171409
Loss in iteration 336 : 0.6305145396519977
Loss in iteration 337 : 0.628672910005467
Loss in iteration 338 : 0.6332652690181911
Loss in iteration 339 : 0.6327951439629883
Loss in iteration 340 : 0.6304533040010606
Loss in iteration 341 : 0.6276976021783393
Loss in iteration 342 : 0.6303164319168105
Loss in iteration 343 : 0.6296542060235817
Loss in iteration 344 : 0.6329131710625592
Loss in iteration 345 : 0.6312377381983751
Loss in iteration 346 : 0.629571719174301
Loss in iteration 347 : 0.6307945295427185
Loss in iteration 348 : 0.6294723625547664
Loss in iteration 349 : 0.6300780092969966
Loss in iteration 350 : 0.6279867567789896
Loss in iteration 351 : 0.6271289332455705
Loss in iteration 352 : 0.6286664729898653
Loss in iteration 353 : 0.6282012576233124
Loss in iteration 354 : 0.6288743442481328
Loss in iteration 355 : 0.6279432694590759
Loss in iteration 356 : 0.6270271124109599
Loss in iteration 357 : 0.6270982046261907
Loss in iteration 358 : 0.6284389784701474
Loss in iteration 359 : 0.6265790625517994
Loss in iteration 360 : 0.6284729933382233
Loss in iteration 361 : 0.6259612990566418
Loss in iteration 362 : 0.6285155034129613
Loss in iteration 363 : 0.6269504512200387
Loss in iteration 364 : 0.6289708376962242
Loss in iteration 365 : 0.6259318900288082
Loss in iteration 366 : 0.6267717089238598
Loss in iteration 367 : 0.6250324386168333
Loss in iteration 368 : 0.6266723381920176
Loss in iteration 369 : 0.6287641636038276
Loss in iteration 370 : 0.6253268176670927
Loss in iteration 371 : 0.628829315959514
Loss in iteration 372 : 0.6243651230932982
Loss in iteration 373 : 0.6256123169344253
Loss in iteration 374 : 0.6239528547246056
Loss in iteration 375 : 0.6243679813113406
Loss in iteration 376 : 0.6249549090649017
Loss in iteration 377 : 0.6253896898312825
Loss in iteration 378 : 0.6231976158017092
Loss in iteration 379 : 0.6234926756840972
Loss in iteration 380 : 0.6249200157422707
Loss in iteration 381 : 0.6252489754026513
Loss in iteration 382 : 0.6237788574020056
Loss in iteration 383 : 0.6223160145446371
Loss in iteration 384 : 0.621448823405562
Loss in iteration 385 : 0.623788326235551
Loss in iteration 386 : 0.6231686330019692
Loss in iteration 387 : 0.6252542622876557
Loss in iteration 388 : 0.6245303070833295
Loss in iteration 389 : 0.6232646393346001
Loss in iteration 390 : 0.623741794763994
Loss in iteration 391 : 0.6213538176246713
Loss in iteration 392 : 0.619956707068664
Loss in iteration 393 : 0.619130630381952
Loss in iteration 394 : 0.6208435219997638
Loss in iteration 395 : 0.6214746312075895
Loss in iteration 396 : 0.621318989794238
Loss in iteration 397 : 0.6231111257938553
Loss in iteration 398 : 0.621116461811157
Loss in iteration 399 : 0.6195522994685221
Loss in iteration 400 : 0.6208430001493088
Testing accuracy  of updater 4 on alg 0 with rate 4.0 = 0.734375, training accuracy 0.734375, time elapsed: 5354 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6929433221520231
Loss in iteration 3 : 0.6926577089007472
Loss in iteration 4 : 0.6925307644908585
Loss in iteration 5 : 0.6922247642690335
Loss in iteration 6 : 0.6921086413643279
Loss in iteration 7 : 0.691813406180849
Loss in iteration 8 : 0.6915261270578248
Loss in iteration 9 : 0.6914029089972621
Loss in iteration 10 : 0.6913474302534987
Loss in iteration 11 : 0.6907593470540198
Loss in iteration 12 : 0.6905671099062677
Loss in iteration 13 : 0.6905859951629546
Loss in iteration 14 : 0.6901316898167106
Loss in iteration 15 : 0.6901106745324649
Loss in iteration 16 : 0.6898462853700551
Loss in iteration 17 : 0.6893247295105349
Loss in iteration 18 : 0.6892114280618151
Loss in iteration 19 : 0.6892742436146446
Loss in iteration 20 : 0.6893537562943199
Loss in iteration 21 : 0.6888182896807824
Loss in iteration 22 : 0.6885217076278021
Loss in iteration 23 : 0.6885231079211237
Loss in iteration 24 : 0.6876411136020478
Loss in iteration 25 : 0.6880626830356736
Loss in iteration 26 : 0.687022640040458
Loss in iteration 27 : 0.6871253308769142
Loss in iteration 28 : 0.6873454824060324
Loss in iteration 29 : 0.6872050954594167
Loss in iteration 30 : 0.6873923339861742
Loss in iteration 31 : 0.6869662940252204
Loss in iteration 32 : 0.6869454687562611
Loss in iteration 33 : 0.6867806488229141
Loss in iteration 34 : 0.6863577215748786
Loss in iteration 35 : 0.6861362593011341
Loss in iteration 36 : 0.6855635561320247
Loss in iteration 37 : 0.6851396405231831
Loss in iteration 38 : 0.6854922078800081
Loss in iteration 39 : 0.6852723499388548
Loss in iteration 40 : 0.6852282970877842
Loss in iteration 41 : 0.6847335236056763
Loss in iteration 42 : 0.6840958438443017
Loss in iteration 43 : 0.6842303855715365
Loss in iteration 44 : 0.6838116455349307
Loss in iteration 45 : 0.6840057431682407
Loss in iteration 46 : 0.6837467681813535
Loss in iteration 47 : 0.6834375326431041
Loss in iteration 48 : 0.6835390930806223
Loss in iteration 49 : 0.6830724649543395
Loss in iteration 50 : 0.6830126373190685
Loss in iteration 51 : 0.6828543049958775
Loss in iteration 52 : 0.6827290754474833
Loss in iteration 53 : 0.682082803738299
Loss in iteration 54 : 0.6813519770399485
Loss in iteration 55 : 0.6809879344622907
Loss in iteration 56 : 0.6820424004362867
Loss in iteration 57 : 0.6815036252949241
Loss in iteration 58 : 0.6816001711207395
Loss in iteration 59 : 0.6809410877517313
Loss in iteration 60 : 0.681068533223391
Loss in iteration 61 : 0.6814783238700536
Loss in iteration 62 : 0.6801995883829011
Loss in iteration 63 : 0.6799708443880383
Loss in iteration 64 : 0.6799663290368586
Loss in iteration 65 : 0.6798607425843376
Loss in iteration 66 : 0.6793478024582195
Loss in iteration 67 : 0.6789815634592722
Loss in iteration 68 : 0.6783513600062199
Loss in iteration 69 : 0.6795438508713432
Loss in iteration 70 : 0.6789783151884174
Loss in iteration 71 : 0.6792366066628196
Loss in iteration 72 : 0.6785049171503074
Loss in iteration 73 : 0.677562391052838
Loss in iteration 74 : 0.6777896752366974
Loss in iteration 75 : 0.6775939027348078
Loss in iteration 76 : 0.6780272223741363
Loss in iteration 77 : 0.6783750352773646
Loss in iteration 78 : 0.6780710539727572
Loss in iteration 79 : 0.6756293581777744
Loss in iteration 80 : 0.6762015129239616
Loss in iteration 81 : 0.6773320763693166
Loss in iteration 82 : 0.6760053602904795
Loss in iteration 83 : 0.677513753285619
Loss in iteration 84 : 0.6760369338089989
Loss in iteration 85 : 0.6765092249803725
Loss in iteration 86 : 0.6765662392116031
Loss in iteration 87 : 0.6735415339563505
Loss in iteration 88 : 0.6768998817388785
Loss in iteration 89 : 0.6743583380892703
Loss in iteration 90 : 0.6750852741602825
Loss in iteration 91 : 0.6755924983849753
Loss in iteration 92 : 0.6730677376640439
Loss in iteration 93 : 0.6730104810498797
Loss in iteration 94 : 0.6739839540030903
Loss in iteration 95 : 0.6745211020339681
Loss in iteration 96 : 0.6760429409537425
Loss in iteration 97 : 0.6722523256422059
Loss in iteration 98 : 0.6733933051586757
Loss in iteration 99 : 0.673225595823576
Loss in iteration 100 : 0.6742076189024675
Loss in iteration 101 : 0.6722564747002983
Loss in iteration 102 : 0.6727569600145822
Loss in iteration 103 : 0.6718131613582434
Loss in iteration 104 : 0.6726263527104913
Loss in iteration 105 : 0.6727943616966092
Loss in iteration 106 : 0.6723167067302424
Loss in iteration 107 : 0.6720768402327085
Loss in iteration 108 : 0.6734174746912827
Loss in iteration 109 : 0.6716610381418576
Loss in iteration 110 : 0.6712458830357971
Loss in iteration 111 : 0.671897563241362
Loss in iteration 112 : 0.6703300255766466
Loss in iteration 113 : 0.6700445936282078
Loss in iteration 114 : 0.6713232602584122
Loss in iteration 115 : 0.6700497513051888
Loss in iteration 116 : 0.6702678371938197
Loss in iteration 117 : 0.6699578067026273
Loss in iteration 118 : 0.6690076212281669
Loss in iteration 119 : 0.6687645736766841
Loss in iteration 120 : 0.6700703098992299
Loss in iteration 121 : 0.6694365160473029
Loss in iteration 122 : 0.6690863800188832
Loss in iteration 123 : 0.669354895074614
Loss in iteration 124 : 0.6682774102323586
Loss in iteration 125 : 0.669467957476507
Loss in iteration 126 : 0.6688730978488785
Loss in iteration 127 : 0.6685551707792488
Loss in iteration 128 : 0.6662562682000904
Loss in iteration 129 : 0.6682902867625657
Loss in iteration 130 : 0.6675058497732278
Loss in iteration 131 : 0.6668789179951264
Loss in iteration 132 : 0.6685114865758891
Loss in iteration 133 : 0.6665081622323971
Loss in iteration 134 : 0.667695764308984
Loss in iteration 135 : 0.6682576592254387
Loss in iteration 136 : 0.6676431667055135
Loss in iteration 137 : 0.6668844728330712
Loss in iteration 138 : 0.6681445388985722
Loss in iteration 139 : 0.6655832103332097
Loss in iteration 140 : 0.6666465764807495
Loss in iteration 141 : 0.6658433400293766
Loss in iteration 142 : 0.6662171439329355
Loss in iteration 143 : 0.6649643377188338
Loss in iteration 144 : 0.6643385186531742
Loss in iteration 145 : 0.664129742060654
Loss in iteration 146 : 0.6632944254362396
Loss in iteration 147 : 0.6643127337463012
Loss in iteration 148 : 0.6652726079207312
Loss in iteration 149 : 0.6661374186539365
Loss in iteration 150 : 0.6639037316948336
Loss in iteration 151 : 0.6636943570133018
Loss in iteration 152 : 0.6634573843284888
Loss in iteration 153 : 0.6608976097904355
Loss in iteration 154 : 0.6648522541789051
Loss in iteration 155 : 0.6639521017065582
Loss in iteration 156 : 0.6632454765946582
Loss in iteration 157 : 0.6627909484799354
Loss in iteration 158 : 0.6611762841646185
Loss in iteration 159 : 0.6620282461136832
Loss in iteration 160 : 0.6608669233513313
Loss in iteration 161 : 0.6634842992946373
Loss in iteration 162 : 0.6617268344396199
Loss in iteration 163 : 0.6620892172551659
Loss in iteration 164 : 0.6625058877892489
Loss in iteration 165 : 0.6606386166011573
Loss in iteration 166 : 0.6609969721722142
Loss in iteration 167 : 0.6632372821633853
Loss in iteration 168 : 0.6614022593323723
Loss in iteration 169 : 0.6611894241969868
Loss in iteration 170 : 0.6600502335814282
Loss in iteration 171 : 0.6613371611186251
Loss in iteration 172 : 0.6589747565530624
Loss in iteration 173 : 0.659781970648134
Loss in iteration 174 : 0.6605123678065954
Loss in iteration 175 : 0.6603995958314555
Loss in iteration 176 : 0.6581343782361313
Loss in iteration 177 : 0.6590668158562452
Loss in iteration 178 : 0.6592336050724208
Loss in iteration 179 : 0.6596189762490143
Loss in iteration 180 : 0.6585753545913033
Loss in iteration 181 : 0.6591590669587784
Loss in iteration 182 : 0.6597557326335163
Loss in iteration 183 : 0.6565076254004195
Loss in iteration 184 : 0.6577476264072032
Loss in iteration 185 : 0.6572120665182758
Loss in iteration 186 : 0.6561122779292724
Loss in iteration 187 : 0.6560712942488524
Loss in iteration 188 : 0.6572113274233109
Loss in iteration 189 : 0.6582450002130056
Loss in iteration 190 : 0.6560141886909862
Loss in iteration 191 : 0.6567689531155467
Loss in iteration 192 : 0.6561647389673052
Loss in iteration 193 : 0.6552776840468958
Loss in iteration 194 : 0.6572633798005675
Loss in iteration 195 : 0.6568712454381266
Loss in iteration 196 : 0.6575557560193703
Loss in iteration 197 : 0.6553894630870184
Loss in iteration 198 : 0.6563283861298604
Loss in iteration 199 : 0.6557316426414824
Loss in iteration 200 : 0.6550413684749814
Loss in iteration 201 : 0.6554165128616938
Loss in iteration 202 : 0.6546625301485798
Loss in iteration 203 : 0.6534452508514077
Loss in iteration 204 : 0.6539875905623168
Loss in iteration 205 : 0.6539336235777663
Loss in iteration 206 : 0.6530388528352692
Loss in iteration 207 : 0.6540835999128924
Loss in iteration 208 : 0.6512485674341908
Loss in iteration 209 : 0.6544885714596866
Loss in iteration 210 : 0.6551264357278003
Loss in iteration 211 : 0.6533518933910407
Loss in iteration 212 : 0.6533989733576776
Loss in iteration 213 : 0.6556962538046615
Loss in iteration 214 : 0.6552141915195354
Loss in iteration 215 : 0.6531842949655964
Loss in iteration 216 : 0.653379552637535
Loss in iteration 217 : 0.6507896841003447
Loss in iteration 218 : 0.6499548157101037
Loss in iteration 219 : 0.6516406791689386
Loss in iteration 220 : 0.6518571206022512
Loss in iteration 221 : 0.6512334326501686
Loss in iteration 222 : 0.651218898949865
Loss in iteration 223 : 0.6514613189192796
Loss in iteration 224 : 0.6507611641567314
Loss in iteration 225 : 0.6517904058336801
Loss in iteration 226 : 0.6496286260400626
Loss in iteration 227 : 0.6498695127720486
Loss in iteration 228 : 0.6504781061090537
Loss in iteration 229 : 0.6513455411915106
Loss in iteration 230 : 0.6482831567612269
Loss in iteration 231 : 0.6493005979719175
Loss in iteration 232 : 0.6501727874650934
Loss in iteration 233 : 0.6496042842217442
Loss in iteration 234 : 0.6488519623717989
Loss in iteration 235 : 0.6475598932739594
Loss in iteration 236 : 0.6481211353010928
Loss in iteration 237 : 0.6486819129734571
Loss in iteration 238 : 0.6469636941328321
Loss in iteration 239 : 0.6461187677447972
Loss in iteration 240 : 0.6478610939768317
Loss in iteration 241 : 0.6452272658873958
Loss in iteration 242 : 0.6505108347101543
Loss in iteration 243 : 0.6497530899139009
Loss in iteration 244 : 0.6470408649384805
Loss in iteration 245 : 0.6486489595240484
Loss in iteration 246 : 0.6472947703774288
Loss in iteration 247 : 0.6456075355680378
Loss in iteration 248 : 0.6453945950776925
Loss in iteration 249 : 0.647143764979315
Loss in iteration 250 : 0.6454984039574262
Loss in iteration 251 : 0.6493478504497929
Loss in iteration 252 : 0.6459565894639153
Loss in iteration 253 : 0.6458373082430039
Loss in iteration 254 : 0.644229252123732
Loss in iteration 255 : 0.6421775143524329
Loss in iteration 256 : 0.6468855299713897
Loss in iteration 257 : 0.6441477451662451
Loss in iteration 258 : 0.6451470820435075
Loss in iteration 259 : 0.6432606033488292
Loss in iteration 260 : 0.6455365446539202
Loss in iteration 261 : 0.6431033483944061
Loss in iteration 262 : 0.642134074200382
Loss in iteration 263 : 0.6421524739984219
Loss in iteration 264 : 0.6450148118544212
Loss in iteration 265 : 0.6415780989261975
Loss in iteration 266 : 0.6443959876243375
Loss in iteration 267 : 0.6440514819038923
Loss in iteration 268 : 0.6429893045839653
Loss in iteration 269 : 0.6428674326700758
Loss in iteration 270 : 0.6440707231714571
Loss in iteration 271 : 0.6424832144785699
Loss in iteration 272 : 0.6454175813272245
Loss in iteration 273 : 0.6396524860446755
Loss in iteration 274 : 0.6411265805351066
Loss in iteration 275 : 0.6408301133269465
Loss in iteration 276 : 0.6428203424225758
Loss in iteration 277 : 0.6404107627340863
Loss in iteration 278 : 0.6385792844671369
Loss in iteration 279 : 0.6405580897429947
Loss in iteration 280 : 0.640489661753769
Loss in iteration 281 : 0.6425951071903335
Loss in iteration 282 : 0.6406736941929486
Loss in iteration 283 : 0.6393805463573501
Loss in iteration 284 : 0.640037346805457
Loss in iteration 285 : 0.639919989252374
Loss in iteration 286 : 0.6386269851169958
Loss in iteration 287 : 0.6406266756891562
Loss in iteration 288 : 0.63994604485219
Loss in iteration 289 : 0.6386910032541007
Loss in iteration 290 : 0.6367565746662726
Loss in iteration 291 : 0.6371292871005363
Loss in iteration 292 : 0.6399441847346014
Loss in iteration 293 : 0.63684675966288
Loss in iteration 294 : 0.6369357074221275
Loss in iteration 295 : 0.6381551950132568
Loss in iteration 296 : 0.6388667998093928
Loss in iteration 297 : 0.6373922645732767
Loss in iteration 298 : 0.6382335926299811
Loss in iteration 299 : 0.6370470458176438
Loss in iteration 300 : 0.6382498246829245
Loss in iteration 301 : 0.6373585192249115
Loss in iteration 302 : 0.637178683793874
Loss in iteration 303 : 0.635898185042012
Loss in iteration 304 : 0.6365435357628308
Loss in iteration 305 : 0.6347531115003879
Loss in iteration 306 : 0.6369179804463456
Loss in iteration 307 : 0.6347707765064806
Loss in iteration 308 : 0.6343809574108676
Loss in iteration 309 : 0.6351873846937451
Loss in iteration 310 : 0.6351974282570572
Loss in iteration 311 : 0.6366560403643605
Loss in iteration 312 : 0.6379454854667675
Loss in iteration 313 : 0.6343208606782992
Loss in iteration 314 : 0.6347442463105187
Loss in iteration 315 : 0.6335878490864162
Loss in iteration 316 : 0.6377173335256775
Loss in iteration 317 : 0.6318655597579227
Loss in iteration 318 : 0.6365352456864867
Loss in iteration 319 : 0.6340778672433968
Loss in iteration 320 : 0.6325285289962771
Loss in iteration 321 : 0.6322804297489601
Loss in iteration 322 : 0.6318700178029218
Loss in iteration 323 : 0.6329110553397269
Loss in iteration 324 : 0.6336841340131268
Loss in iteration 325 : 0.6315592212885368
Loss in iteration 326 : 0.6329169069176608
Loss in iteration 327 : 0.6305452103430188
Loss in iteration 328 : 0.633280798863426
Loss in iteration 329 : 0.6334378407449293
Loss in iteration 330 : 0.6306818938715371
Loss in iteration 331 : 0.631955016066221
Loss in iteration 332 : 0.6295589727771408
Loss in iteration 333 : 0.6314030440168555
Loss in iteration 334 : 0.6325398552570944
Loss in iteration 335 : 0.6309858695171409
Loss in iteration 336 : 0.6305145396519977
Loss in iteration 337 : 0.628672910005467
Loss in iteration 338 : 0.6332652690181911
Loss in iteration 339 : 0.6327951439629883
Loss in iteration 340 : 0.6304533040010606
Loss in iteration 341 : 0.6276976021783393
Loss in iteration 342 : 0.6303164319168105
Loss in iteration 343 : 0.6296542060235817
Loss in iteration 344 : 0.6329131710625592
Loss in iteration 345 : 0.6312377381983751
Loss in iteration 346 : 0.629571719174301
Loss in iteration 347 : 0.6307945295427185
Loss in iteration 348 : 0.6294723625547664
Loss in iteration 349 : 0.6300780092969966
Loss in iteration 350 : 0.6279867567789896
Loss in iteration 351 : 0.6271289332455705
Loss in iteration 352 : 0.6286664729898653
Loss in iteration 353 : 0.6282012576233124
Loss in iteration 354 : 0.6288743442481328
Loss in iteration 355 : 0.6279432694590759
Loss in iteration 356 : 0.6270271124109599
Loss in iteration 357 : 0.6270982046261907
Loss in iteration 358 : 0.6284389784701474
Loss in iteration 359 : 0.6265790625517994
Loss in iteration 360 : 0.6284729933382233
Loss in iteration 361 : 0.6259612990566418
Loss in iteration 362 : 0.6285155034129613
Loss in iteration 363 : 0.6269504512200387
Loss in iteration 364 : 0.6289708376962242
Loss in iteration 365 : 0.6259318900288082
Loss in iteration 366 : 0.6267717089238598
Loss in iteration 367 : 0.6250324386168333
Loss in iteration 368 : 0.6266723381920176
Loss in iteration 369 : 0.6287641636038276
Loss in iteration 370 : 0.6253268176670927
Loss in iteration 371 : 0.628829315959514
Loss in iteration 372 : 0.6243651230932982
Loss in iteration 373 : 0.6256123169344253
Loss in iteration 374 : 0.6239528547246056
Loss in iteration 375 : 0.6243679813113406
Loss in iteration 376 : 0.6249549090649017
Loss in iteration 377 : 0.6253896898312825
Loss in iteration 378 : 0.6231976158017092
Loss in iteration 379 : 0.6234926756840972
Loss in iteration 380 : 0.6249200157422707
Loss in iteration 381 : 0.6252489754026513
Loss in iteration 382 : 0.6237788574020056
Loss in iteration 383 : 0.6223160145446371
Loss in iteration 384 : 0.621448823405562
Loss in iteration 385 : 0.623788326235551
Loss in iteration 386 : 0.6231686330019692
Loss in iteration 387 : 0.6252542622876557
Loss in iteration 388 : 0.6245303070833295
Loss in iteration 389 : 0.6232646393346001
Loss in iteration 390 : 0.623741794763994
Loss in iteration 391 : 0.6213538176246713
Loss in iteration 392 : 0.619956707068664
Loss in iteration 393 : 0.619130630381952
Loss in iteration 394 : 0.6208435219997638
Loss in iteration 395 : 0.6214746312075895
Loss in iteration 396 : 0.621318989794238
Loss in iteration 397 : 0.6231111257938553
Loss in iteration 398 : 0.621116461811157
Loss in iteration 399 : 0.6195522994685221
Loss in iteration 400 : 0.6208430001493088
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.734375, training accuracy 0.734375, time elapsed: 4422 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 94.51712084304023
Loss in iteration 3 : 60.30509604560055
Loss in iteration 4 : 57.51579079292844
Loss in iteration 5 : 26.586425880783978
Loss in iteration 6 : 35.76419025202236
Loss in iteration 7 : 24.176415771380565
Loss in iteration 8 : 32.26676025344671
Loss in iteration 9 : 19.234613259969414
Loss in iteration 10 : 27.774078959552625
Loss in iteration 11 : 19.904145676300562
Loss in iteration 12 : 25.124045003261077
Loss in iteration 13 : 15.838211980718635
Loss in iteration 14 : 21.5763417111176
Loss in iteration 15 : 17.758168528975634
Loss in iteration 16 : 22.78231122382136
Loss in iteration 17 : 16.275491499132432
Loss in iteration 18 : 19.88907271308396
Loss in iteration 19 : 16.060565018631074
Loss in iteration 20 : 17.153724813696787
Loss in iteration 21 : 18.016120413946776
Loss in iteration 22 : 18.150951853072783
Loss in iteration 23 : 16.59611450811459
Loss in iteration 24 : 19.56535584380145
Loss in iteration 25 : 15.617464060076276
Loss in iteration 26 : 16.76830884937127
Loss in iteration 27 : 15.470915237923586
Loss in iteration 28 : 17.54005042872197
Loss in iteration 29 : 17.378747093488236
Loss in iteration 30 : 16.140058025009793
Loss in iteration 31 : 18.22238422196942
Loss in iteration 32 : 17.32779295608877
Loss in iteration 33 : 16.42619110042413
Loss in iteration 34 : 16.444299264632036
Loss in iteration 35 : 17.130154290260652
Loss in iteration 36 : 16.26954945057688
Loss in iteration 37 : 16.09064423417991
Loss in iteration 38 : 16.054849704934472
Loss in iteration 39 : 16.806750985026866
Loss in iteration 40 : 16.121456553854586
Loss in iteration 41 : 16.126111088628658
Loss in iteration 42 : 13.67292050085965
Loss in iteration 43 : 15.326835535011812
Loss in iteration 44 : 15.153508888437617
Loss in iteration 45 : 17.663045275201497
Loss in iteration 46 : 13.552054265447843
Loss in iteration 47 : 17.65227523323436
Loss in iteration 48 : 15.254447935880778
Loss in iteration 49 : 15.999187459041684
Loss in iteration 50 : 14.347245006669953
Loss in iteration 51 : 17.623245764095124
Loss in iteration 52 : 16.45420282964677
Loss in iteration 53 : 17.76172661839931
Loss in iteration 54 : 15.434069529060425
Loss in iteration 55 : 16.255491333195977
Loss in iteration 56 : 16.056057709957955
Loss in iteration 57 : 16.222212962977785
Loss in iteration 58 : 14.09397593186469
Loss in iteration 59 : 15.572175017285815
Loss in iteration 60 : 13.81717968721714
Loss in iteration 61 : 15.166879219028315
Loss in iteration 62 : 14.953510844498899
Loss in iteration 63 : 17.390311265239166
Loss in iteration 64 : 12.440238082547507
Loss in iteration 65 : 16.08345755961302
Loss in iteration 66 : 13.255916032914994
Loss in iteration 67 : 17.119910724266305
Loss in iteration 68 : 14.044435703010098
Loss in iteration 69 : 16.530171444260745
Loss in iteration 70 : 14.54511911445383
Loss in iteration 71 : 17.886385661800045
Loss in iteration 72 : 15.474346451668008
Loss in iteration 73 : 16.08789717201388
Loss in iteration 74 : 14.179067310705
Loss in iteration 75 : 16.469894745534138
Loss in iteration 76 : 13.737701411858058
Loss in iteration 77 : 16.37484406155648
Loss in iteration 78 : 12.507141299385214
Loss in iteration 79 : 14.119866828755296
Loss in iteration 80 : 12.248334863147996
Loss in iteration 81 : 15.34548648175
Loss in iteration 82 : 15.087623197783078
Loss in iteration 83 : 17.972547093963332
Loss in iteration 84 : 17.398477280577403
Loss in iteration 85 : 17.210146022512763
Loss in iteration 86 : 14.30442134350372
Loss in iteration 87 : 16.756527229631065
Loss in iteration 88 : 16.03898561050226
Loss in iteration 89 : 17.03577805709236
Loss in iteration 90 : 13.76319275722332
Loss in iteration 91 : 15.937182873878708
Loss in iteration 92 : 13.117525799777082
Loss in iteration 93 : 15.617297513007957
Loss in iteration 94 : 13.000706382756515
Loss in iteration 95 : 16.041037569860073
Loss in iteration 96 : 13.73144059479939
Loss in iteration 97 : 17.90935033910012
Loss in iteration 98 : 16.598032070402063
Loss in iteration 99 : 17.96784535229123
Loss in iteration 100 : 14.888213579911147
Loss in iteration 101 : 16.475459182736184
Loss in iteration 102 : 12.657713668266023
Loss in iteration 103 : 15.962568431486547
Loss in iteration 104 : 14.824126130165924
Loss in iteration 105 : 17.330012806810636
Loss in iteration 106 : 15.393066225997114
Loss in iteration 107 : 17.067335703645096
Loss in iteration 108 : 15.490809887602596
Loss in iteration 109 : 17.13067289827766
Loss in iteration 110 : 15.453555207391918
Loss in iteration 111 : 17.03948162110734
Loss in iteration 112 : 10.910752893355646
Loss in iteration 113 : 12.985802443631298
Loss in iteration 114 : 13.560643707738134
Loss in iteration 115 : 16.401581142796882
Loss in iteration 116 : 12.074925021221315
Loss in iteration 117 : 16.726989987778044
Loss in iteration 118 : 16.042889391853432
Loss in iteration 119 : 17.771978079340464
Loss in iteration 120 : 16.47069114438903
Loss in iteration 121 : 17.478069368617845
Loss in iteration 122 : 14.452445269036303
Loss in iteration 123 : 15.48256942587989
Loss in iteration 124 : 12.826738815291646
Loss in iteration 125 : 15.844385842205199
Loss in iteration 126 : 14.213748756545046
Loss in iteration 127 : 17.262825088755374
Loss in iteration 128 : 14.053707027690313
Loss in iteration 129 : 16.238477613120228
Loss in iteration 130 : 13.703037317223982
Loss in iteration 131 : 16.54144482027022
Loss in iteration 132 : 11.848743875401516
Loss in iteration 133 : 15.510207401600606
Loss in iteration 134 : 15.59556093132833
Loss in iteration 135 : 17.92603840019075
Loss in iteration 136 : 13.73189763803847
Loss in iteration 137 : 16.44628532698599
Loss in iteration 138 : 13.925402021924471
Loss in iteration 139 : 17.3165037086444
Loss in iteration 140 : 13.517996308304603
Loss in iteration 141 : 15.815274889578944
Loss in iteration 142 : 14.928059794032652
Loss in iteration 143 : 17.50523705387227
Loss in iteration 144 : 13.724965709658214
Loss in iteration 145 : 15.463070551422078
Loss in iteration 146 : 12.6253979274267
Loss in iteration 147 : 14.88896670739407
Loss in iteration 148 : 13.09520470977844
Loss in iteration 149 : 17.117582597763345
Loss in iteration 150 : 13.869643056644108
Loss in iteration 151 : 17.16525869905758
Loss in iteration 152 : 15.825090768371034
Loss in iteration 153 : 17.562743203899185
Loss in iteration 154 : 15.0469267173058
Loss in iteration 155 : 16.48265975071215
Loss in iteration 156 : 13.903810570764914
Loss in iteration 157 : 18.086488459912147
Loss in iteration 158 : 14.667311960328833
Loss in iteration 159 : 16.049991626625868
Loss in iteration 160 : 12.249141093997862
Loss in iteration 161 : 14.887676122921558
Loss in iteration 162 : 14.276599373210388
Loss in iteration 163 : 16.32520404294688
Loss in iteration 164 : 15.458968006200514
Loss in iteration 165 : 18.351258464939907
Loss in iteration 166 : 13.065639312864738
Loss in iteration 167 : 14.394154659927928
Loss in iteration 168 : 13.861259162503316
Loss in iteration 169 : 16.362600055032075
Loss in iteration 170 : 15.230251128133348
Loss in iteration 171 : 16.714388696648857
Loss in iteration 172 : 12.603721971687841
Loss in iteration 173 : 15.203685119612057
Loss in iteration 174 : 13.962849721768409
Loss in iteration 175 : 17.63160212414047
Loss in iteration 176 : 13.378979089822582
Loss in iteration 177 : 16.75764974939075
Loss in iteration 178 : 15.365064266253281
Loss in iteration 179 : 17.868779745520992
Loss in iteration 180 : 14.797383614624623
Loss in iteration 181 : 18.0273993993664
Loss in iteration 182 : 13.23035745434209
Loss in iteration 183 : 16.061666847687157
Loss in iteration 184 : 12.674093439969694
Loss in iteration 185 : 15.46257645691436
Loss in iteration 186 : 14.105580928356366
Loss in iteration 187 : 15.560567411670776
Loss in iteration 188 : 14.219239714783665
Loss in iteration 189 : 16.256304274752726
Loss in iteration 190 : 13.932614583215436
Loss in iteration 191 : 15.49083908087845
Loss in iteration 192 : 15.41864997951735
Loss in iteration 193 : 18.187119239155997
Loss in iteration 194 : 14.274064966838472
Loss in iteration 195 : 16.537209832173563
Loss in iteration 196 : 14.311654638347287
Loss in iteration 197 : 16.14875590413276
Loss in iteration 198 : 13.036904886728617
Loss in iteration 199 : 14.682091905112957
Loss in iteration 200 : 13.04469358666792
Loss in iteration 201 : 16.617447262115995
Loss in iteration 202 : 16.294777228283788
Loss in iteration 203 : 17.358033670730382
Loss in iteration 204 : 12.862533000313906
Loss in iteration 205 : 16.598179440462005
Loss in iteration 206 : 13.37870931164562
Loss in iteration 207 : 15.732681180700155
Loss in iteration 208 : 13.098981193786674
Loss in iteration 209 : 15.88058530053992
Loss in iteration 210 : 13.787354088481385
Loss in iteration 211 : 16.693956599269058
Loss in iteration 212 : 15.07172730976229
Loss in iteration 213 : 16.324125106919656
Loss in iteration 214 : 14.75299974686965
Loss in iteration 215 : 17.26181911036075
Loss in iteration 216 : 14.62201604753167
Loss in iteration 217 : 16.598616789900724
Loss in iteration 218 : 12.898749909353361
Loss in iteration 219 : 16.11365735964003
Loss in iteration 220 : 13.351725274053639
Loss in iteration 221 : 15.80948863100838
Loss in iteration 222 : 11.905022321726124
Loss in iteration 223 : 15.506854707254178
Loss in iteration 224 : 16.201036816471817
Loss in iteration 225 : 18.686786018220022
Loss in iteration 226 : 14.038026541450055
Loss in iteration 227 : 15.494114945567697
Loss in iteration 228 : 13.911299025613769
Loss in iteration 229 : 15.555989608718104
Loss in iteration 230 : 13.538648994459066
Loss in iteration 231 : 16.19466758838801
Loss in iteration 232 : 14.312414402015628
Loss in iteration 233 : 17.47932467628852
Loss in iteration 234 : 15.126899064698025
Loss in iteration 235 : 15.83279773756059
Loss in iteration 236 : 13.028893558959176
Loss in iteration 237 : 15.141155267651152
Loss in iteration 238 : 13.215619774237974
Loss in iteration 239 : 15.517801275597035
Loss in iteration 240 : 12.46555758615942
Loss in iteration 241 : 16.185550013375973
Loss in iteration 242 : 14.98698567890789
Loss in iteration 243 : 18.279799766137838
Loss in iteration 244 : 15.876951126322536
Loss in iteration 245 : 18.10879023145115
Loss in iteration 246 : 14.10824362524154
Loss in iteration 247 : 14.841456135956854
Loss in iteration 248 : 13.362893502092035
Loss in iteration 249 : 15.482298142012453
Loss in iteration 250 : 13.87634472928097
Loss in iteration 251 : 16.81020379675219
Loss in iteration 252 : 14.074791550965092
Loss in iteration 253 : 17.641725536966018
Loss in iteration 254 : 12.578372916634022
Loss in iteration 255 : 15.83735516577332
Loss in iteration 256 : 14.895296873247675
Loss in iteration 257 : 16.833453665513844
Loss in iteration 258 : 14.003665393114916
Loss in iteration 259 : 15.875317758739024
Loss in iteration 260 : 14.429937327500383
Loss in iteration 261 : 16.792125353108258
Loss in iteration 262 : 14.634114609998246
Loss in iteration 263 : 17.248356832947966
Loss in iteration 264 : 13.996228570881026
Loss in iteration 265 : 15.516509754180142
Loss in iteration 266 : 12.405381385200702
Loss in iteration 267 : 13.419451723031319
Loss in iteration 268 : 13.891350864442305
Loss in iteration 269 : 16.974986201568093
Loss in iteration 270 : 14.772469186148232
Loss in iteration 271 : 17.795555155374885
Loss in iteration 272 : 15.894798625677645
Loss in iteration 273 : 17.07707056203822
Loss in iteration 274 : 13.535836213144576
Loss in iteration 275 : 15.361822414384934
Loss in iteration 276 : 13.32356338969736
Loss in iteration 277 : 15.99600242919226
Loss in iteration 278 : 13.558280893038132
Loss in iteration 279 : 16.036985181584846
Loss in iteration 280 : 14.891859757388358
Loss in iteration 281 : 15.875119719218516
Loss in iteration 282 : 12.967855420463895
Loss in iteration 283 : 15.931702768981967
Loss in iteration 284 : 13.758412561260158
Loss in iteration 285 : 17.320240755656307
Loss in iteration 286 : 14.440069750161626
Loss in iteration 287 : 16.45753804911945
Loss in iteration 288 : 13.792402728881896
Loss in iteration 289 : 14.175936914703128
Loss in iteration 290 : 12.189215332291552
Loss in iteration 291 : 14.732922310317425
Loss in iteration 292 : 14.223641677815946
Loss in iteration 293 : 17.57709461188221
Loss in iteration 294 : 14.222376868666682
Loss in iteration 295 : 16.25094736692873
Loss in iteration 296 : 15.750206811483597
Loss in iteration 297 : 17.648341312503444
Loss in iteration 298 : 14.748005435106533
Loss in iteration 299 : 16.756664892459096
Loss in iteration 300 : 14.797193894008622
Loss in iteration 301 : 16.189774074669234
Loss in iteration 302 : 12.754813561192865
Loss in iteration 303 : 14.6294102587006
Loss in iteration 304 : 14.690326438650168
Loss in iteration 305 : 16.16302346503057
Loss in iteration 306 : 13.914608478572553
Loss in iteration 307 : 15.612504404212498
Loss in iteration 308 : 15.091394193203508
Loss in iteration 309 : 16.49611162392997
Loss in iteration 310 : 14.582080242651392
Loss in iteration 311 : 16.585768007965008
Loss in iteration 312 : 14.343889279655526
Loss in iteration 313 : 15.305745905747504
Loss in iteration 314 : 12.141917974773948
Loss in iteration 315 : 14.492154367121318
Loss in iteration 316 : 14.731411942877926
Loss in iteration 317 : 16.72117874928859
Loss in iteration 318 : 15.397202632989933
Loss in iteration 319 : 16.54781985347799
Loss in iteration 320 : 14.912712730565111
Loss in iteration 321 : 17.288536345676988
Loss in iteration 322 : 13.877230329289894
Loss in iteration 323 : 15.042493688246475
Loss in iteration 324 : 13.864126595797336
Loss in iteration 325 : 15.566300170391845
Loss in iteration 326 : 14.503451860940686
Loss in iteration 327 : 15.105361631907794
Loss in iteration 328 : 12.969245260140195
Loss in iteration 329 : 16.09296769570891
Loss in iteration 330 : 14.16542089855594
Loss in iteration 331 : 16.41309174101386
Loss in iteration 332 : 12.391504580835134
Loss in iteration 333 : 15.326500472263074
Loss in iteration 334 : 14.282558020199803
Loss in iteration 335 : 16.88733680655793
Loss in iteration 336 : 13.280037302813657
Loss in iteration 337 : 17.169627182835324
Loss in iteration 338 : 15.502230200660119
Loss in iteration 339 : 17.222811273327746
Loss in iteration 340 : 13.565133436327796
Loss in iteration 341 : 14.928435891106671
Loss in iteration 342 : 13.463920876428517
Loss in iteration 343 : 16.16540301376017
Loss in iteration 344 : 13.903308419457268
Loss in iteration 345 : 16.24240442329124
Loss in iteration 346 : 14.455972757988372
Loss in iteration 347 : 15.318493999616045
Loss in iteration 348 : 15.766876098260921
Loss in iteration 349 : 17.891508461710444
Loss in iteration 350 : 14.047267342447268
Loss in iteration 351 : 15.193340111325464
Loss in iteration 352 : 13.486966873490735
Loss in iteration 353 : 14.246639623730784
Loss in iteration 354 : 14.396109017611003
Loss in iteration 355 : 15.775981743146392
Loss in iteration 356 : 14.476260285641803
Loss in iteration 357 : 16.57814032290247
Loss in iteration 358 : 13.794024952007288
Loss in iteration 359 : 14.196095224688165
Loss in iteration 360 : 14.518421616835605
Loss in iteration 361 : 15.878190630757958
Loss in iteration 362 : 13.688739490847816
Loss in iteration 363 : 15.634998234046256
Loss in iteration 364 : 14.329575144515319
Loss in iteration 365 : 17.521333687988623
Loss in iteration 366 : 14.85846881117797
Loss in iteration 367 : 16.854033777295644
Loss in iteration 368 : 13.95588183022584
Loss in iteration 369 : 16.399346469627897
Loss in iteration 370 : 14.274110624053964
Loss in iteration 371 : 16.990045971722708
Loss in iteration 372 : 13.455425343424869
Loss in iteration 373 : 16.07489209170452
Loss in iteration 374 : 15.512496182067023
Loss in iteration 375 : 16.754925157367463
Loss in iteration 376 : 14.54409858411876
Loss in iteration 377 : 15.476208580035808
Loss in iteration 378 : 13.271099680463195
Loss in iteration 379 : 14.752499776720105
Loss in iteration 380 : 14.62705758811859
Loss in iteration 381 : 16.82773168946775
Loss in iteration 382 : 12.30717962190705
Loss in iteration 383 : 15.315850680340066
Loss in iteration 384 : 14.138786132880778
Loss in iteration 385 : 15.593292172126613
Loss in iteration 386 : 14.710435954732732
Loss in iteration 387 : 16.823098342985244
Loss in iteration 388 : 14.71915554842493
Loss in iteration 389 : 14.748485925687262
Loss in iteration 390 : 13.437818790812111
Loss in iteration 391 : 15.15790146638617
Loss in iteration 392 : 13.974292473904827
Loss in iteration 393 : 15.603064327650824
Loss in iteration 394 : 13.169816395729578
Loss in iteration 395 : 16.88075139845534
Loss in iteration 396 : 13.434381632580621
Loss in iteration 397 : 16.277658982244173
Loss in iteration 398 : 14.500282654053144
Loss in iteration 399 : 16.350083205955894
Loss in iteration 400 : 14.26968972784319
Testing accuracy  of updater 5 on alg 0 with rate 10.0 = 0.716375, training accuracy 0.716375, time elapsed: 4346 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 6.771097523411542
Loss in iteration 3 : 65.13518529217801
Loss in iteration 4 : 9.52834117754738
Loss in iteration 5 : 37.25978641521098
Loss in iteration 6 : 17.792445170725397
Loss in iteration 7 : 26.81940269385205
Loss in iteration 8 : 18.373927960960664
Loss in iteration 9 : 18.20482350214577
Loss in iteration 10 : 21.417732763435026
Loss in iteration 11 : 13.090885267772222
Loss in iteration 12 : 21.820378434347937
Loss in iteration 13 : 8.89374957051043
Loss in iteration 14 : 17.19712711384168
Loss in iteration 15 : 11.633796451944127
Loss in iteration 16 : 18.608004635929042
Loss in iteration 17 : 11.033131621802328
Loss in iteration 18 : 16.686374851696144
Loss in iteration 19 : 11.137737017079784
Loss in iteration 20 : 13.85637023998795
Loss in iteration 21 : 12.9066245902144
Loss in iteration 22 : 12.721572815588518
Loss in iteration 23 : 13.28219885631194
Loss in iteration 24 : 12.112818185194541
Loss in iteration 25 : 12.787839673269243
Loss in iteration 26 : 11.22235657030609
Loss in iteration 27 : 11.83347640318723
Loss in iteration 28 : 11.09830659851333
Loss in iteration 29 : 12.766295171038331
Loss in iteration 30 : 10.39630120137831
Loss in iteration 31 : 14.305707597111061
Loss in iteration 32 : 12.156374675607255
Loss in iteration 33 : 12.443252002969194
Loss in iteration 34 : 11.154925663705226
Loss in iteration 35 : 12.855085370904353
Loss in iteration 36 : 10.678459048086337
Loss in iteration 37 : 12.26843635274653
Loss in iteration 38 : 10.84535729973506
Loss in iteration 39 : 12.366376911647002
Loss in iteration 40 : 10.241262358878997
Loss in iteration 41 : 11.948731064705775
Loss in iteration 42 : 8.755534233839162
Loss in iteration 43 : 11.542479113362656
Loss in iteration 44 : 9.726900663895206
Loss in iteration 45 : 12.450581444904534
Loss in iteration 46 : 9.749185386073796
Loss in iteration 47 : 12.789501846173525
Loss in iteration 48 : 8.965845812425476
Loss in iteration 49 : 11.647805110111163
Loss in iteration 50 : 10.683600340513994
Loss in iteration 51 : 13.607176111678509
Loss in iteration 52 : 10.85108303062113
Loss in iteration 53 : 12.824134084897446
Loss in iteration 54 : 10.166344434600624
Loss in iteration 55 : 11.85582562589683
Loss in iteration 56 : 10.580431974662357
Loss in iteration 57 : 11.8531026530844
Loss in iteration 58 : 9.661189936925966
Loss in iteration 59 : 11.38171314705264
Loss in iteration 60 : 9.485076843746654
Loss in iteration 61 : 11.392869848212767
Loss in iteration 62 : 9.587171437762423
Loss in iteration 63 : 11.933812092331365
Loss in iteration 64 : 8.678673173690377
Loss in iteration 65 : 12.148473833533203
Loss in iteration 66 : 8.877895928031311
Loss in iteration 67 : 11.74487388178423
Loss in iteration 68 : 8.547700017741647
Loss in iteration 69 : 12.02714581496732
Loss in iteration 70 : 10.559658817597015
Loss in iteration 71 : 13.329193813612747
Loss in iteration 72 : 10.757899250986632
Loss in iteration 73 : 11.841294367743027
Loss in iteration 74 : 9.868557664430908
Loss in iteration 75 : 11.940319151124358
Loss in iteration 76 : 9.643532609728211
Loss in iteration 77 : 11.729017123392827
Loss in iteration 78 : 8.321078466427867
Loss in iteration 79 : 10.278214064145022
Loss in iteration 80 : 8.963838068968158
Loss in iteration 81 : 11.549941558916839
Loss in iteration 82 : 10.695765257288608
Loss in iteration 83 : 12.629176374726816
Loss in iteration 84 : 11.41952442914572
Loss in iteration 85 : 12.028649767148705
Loss in iteration 86 : 9.547553714475734
Loss in iteration 87 : 12.019232226912862
Loss in iteration 88 : 10.506970362186976
Loss in iteration 89 : 12.215015044096582
Loss in iteration 90 : 9.48584924980082
Loss in iteration 91 : 11.797588307584382
Loss in iteration 92 : 9.433922272025594
Loss in iteration 93 : 11.439747670163712
Loss in iteration 94 : 8.662099122761273
Loss in iteration 95 : 11.162691130052059
Loss in iteration 96 : 9.9100396066237
Loss in iteration 97 : 13.35060320683923
Loss in iteration 98 : 11.479647640773612
Loss in iteration 99 : 12.881237785506764
Loss in iteration 100 : 10.130931039767766
Loss in iteration 101 : 11.72052064791134
Loss in iteration 102 : 8.884769922836917
Loss in iteration 103 : 11.390068787335014
Loss in iteration 104 : 9.996726729451593
Loss in iteration 105 : 12.206745808978551
Loss in iteration 106 : 10.342898998363268
Loss in iteration 107 : 12.147915283951082
Loss in iteration 108 : 10.675394022784207
Loss in iteration 109 : 12.369574346144184
Loss in iteration 110 : 10.908970920987414
Loss in iteration 111 : 12.280964722101738
Loss in iteration 112 : 7.520521258889331
Loss in iteration 113 : 9.139913053125982
Loss in iteration 114 : 9.826870844396982
Loss in iteration 115 : 12.139152447770842
Loss in iteration 116 : 8.622833506056608
Loss in iteration 117 : 11.9063413506782
Loss in iteration 118 : 10.836632679678646
Loss in iteration 119 : 12.291368212162322
Loss in iteration 120 : 10.646932005158224
Loss in iteration 121 : 11.967623905783025
Loss in iteration 122 : 9.943752744529101
Loss in iteration 123 : 11.161509078570381
Loss in iteration 124 : 9.272364602855873
Loss in iteration 125 : 11.593010267311897
Loss in iteration 126 : 9.718271721707225
Loss in iteration 127 : 12.248679592758897
Loss in iteration 128 : 9.524151147649286
Loss in iteration 129 : 11.490599982433492
Loss in iteration 130 : 9.448920160078604
Loss in iteration 131 : 11.96290078451138
Loss in iteration 132 : 7.819512097951572
Loss in iteration 133 : 10.647198150864307
Loss in iteration 134 : 10.681767312149645
Loss in iteration 135 : 13.03444727235206
Loss in iteration 136 : 9.727802347285007
Loss in iteration 137 : 11.899284618382175
Loss in iteration 138 : 9.685865214088668
Loss in iteration 139 : 12.458945319798179
Loss in iteration 140 : 9.212278039683225
Loss in iteration 141 : 11.26862298337358
Loss in iteration 142 : 10.434639329855349
Loss in iteration 143 : 12.504844506335319
Loss in iteration 144 : 9.617449674076532
Loss in iteration 145 : 11.034566379606352
Loss in iteration 146 : 8.881073207054348
Loss in iteration 147 : 10.88241817431754
Loss in iteration 148 : 8.863808686551057
Loss in iteration 149 : 11.830883629347332
Loss in iteration 150 : 9.569067062601597
Loss in iteration 151 : 12.12517516225577
Loss in iteration 152 : 10.894469003592373
Loss in iteration 153 : 12.440472822533156
Loss in iteration 154 : 10.521648819355034
Loss in iteration 155 : 11.845837708636733
Loss in iteration 156 : 9.652223733281609
Loss in iteration 157 : 12.764068937721884
Loss in iteration 158 : 10.291735891773927
Loss in iteration 159 : 11.546059898470578
Loss in iteration 160 : 8.367699724036013
Loss in iteration 161 : 10.448196333346111
Loss in iteration 162 : 9.600133283875865
Loss in iteration 163 : 11.693433963712272
Loss in iteration 164 : 10.854378045509295
Loss in iteration 165 : 13.15243804474229
Loss in iteration 166 : 9.05717940167412
Loss in iteration 167 : 10.179870306990692
Loss in iteration 168 : 9.615226725238319
Loss in iteration 169 : 11.610841995986078
Loss in iteration 170 : 10.70709872951626
Loss in iteration 171 : 12.074987719689332
Loss in iteration 172 : 8.526372049007623
Loss in iteration 173 : 10.467100962998824
Loss in iteration 174 : 9.61672582984534
Loss in iteration 175 : 12.678589338408864
Loss in iteration 176 : 9.195810143800387
Loss in iteration 177 : 11.959826883406606
Loss in iteration 178 : 10.549098016591712
Loss in iteration 179 : 12.861117908810877
Loss in iteration 180 : 10.169295503466453
Loss in iteration 181 : 12.633065443785158
Loss in iteration 182 : 9.454604423089824
Loss in iteration 183 : 11.568336560236164
Loss in iteration 184 : 8.847558918562665
Loss in iteration 185 : 10.806472366275907
Loss in iteration 186 : 9.558665585515044
Loss in iteration 187 : 10.985263228279555
Loss in iteration 188 : 9.666985885256157
Loss in iteration 189 : 11.505499497427405
Loss in iteration 190 : 9.711650833518775
Loss in iteration 191 : 11.237280499416787
Loss in iteration 192 : 10.678443080123827
Loss in iteration 193 : 12.859992933275654
Loss in iteration 194 : 9.880132839506473
Loss in iteration 195 : 11.91632654924015
Loss in iteration 196 : 9.945589835403569
Loss in iteration 197 : 11.507899804397233
Loss in iteration 198 : 9.138184467256599
Loss in iteration 199 : 10.349321068827702
Loss in iteration 200 : 9.23412444524881
Loss in iteration 201 : 11.919586619494943
Loss in iteration 202 : 11.165766190288497
Loss in iteration 203 : 12.143168887048047
Loss in iteration 204 : 9.31787070737786
Loss in iteration 205 : 11.617347009982135
Loss in iteration 206 : 10.062429245782958
Loss in iteration 207 : 11.205316654946293
Loss in iteration 208 : 9.609545397000083
Loss in iteration 209 : 10.934315115602393
Loss in iteration 210 : 9.104248117504506
Loss in iteration 211 : 11.246053391822203
Loss in iteration 212 : 9.988678112292126
Loss in iteration 213 : 11.526255277024863
Loss in iteration 214 : 10.208350361386604
Loss in iteration 215 : 12.27160371911818
Loss in iteration 216 : 10.544732942202918
Loss in iteration 217 : 12.020642078046926
Loss in iteration 218 : 8.860580159923456
Loss in iteration 219 : 11.380391888814103
Loss in iteration 220 : 9.234848251489412
Loss in iteration 221 : 11.161049019956938
Loss in iteration 222 : 8.109851403740523
Loss in iteration 223 : 10.743958519547547
Loss in iteration 224 : 11.362851658094234
Loss in iteration 225 : 13.324389293432594
Loss in iteration 226 : 9.896671171670349
Loss in iteration 227 : 10.998928175275745
Loss in iteration 228 : 9.74986312515892
Loss in iteration 229 : 11.044198427204487
Loss in iteration 230 : 9.080776811918943
Loss in iteration 231 : 11.282915149795624
Loss in iteration 232 : 9.722694418566183
Loss in iteration 233 : 12.326193780391312
Loss in iteration 234 : 10.553333858943093
Loss in iteration 235 : 11.354398406715728
Loss in iteration 236 : 9.17501160037316
Loss in iteration 237 : 10.71059952502474
Loss in iteration 238 : 9.219893785692534
Loss in iteration 239 : 11.00102818574676
Loss in iteration 240 : 8.850293095463863
Loss in iteration 241 : 11.632353847971988
Loss in iteration 242 : 10.229151971312094
Loss in iteration 243 : 12.78450923422559
Loss in iteration 244 : 10.916312399440722
Loss in iteration 245 : 12.810662547567667
Loss in iteration 246 : 9.911498874238864
Loss in iteration 247 : 10.619118058588729
Loss in iteration 248 : 9.288208938478038
Loss in iteration 249 : 10.996104283546078
Loss in iteration 250 : 9.691266301417201
Loss in iteration 251 : 11.929005170390358
Loss in iteration 252 : 9.587024071791003
Loss in iteration 253 : 12.308134509060443
Loss in iteration 254 : 8.947582164078959
Loss in iteration 255 : 11.255813395170176
Loss in iteration 256 : 10.541901797922545
Loss in iteration 257 : 11.92457570129317
Loss in iteration 258 : 9.636156717310751
Loss in iteration 259 : 11.13460127381822
Loss in iteration 260 : 9.987212961377974
Loss in iteration 261 : 11.82279145222424
Loss in iteration 262 : 10.146580146163322
Loss in iteration 263 : 12.081256029218405
Loss in iteration 264 : 9.788664847347158
Loss in iteration 265 : 11.004702190236106
Loss in iteration 266 : 8.545572580182338
Loss in iteration 267 : 9.415549529084021
Loss in iteration 268 : 9.816241209660165
Loss in iteration 269 : 12.116452072094408
Loss in iteration 270 : 10.338973520721842
Loss in iteration 271 : 12.558407598312233
Loss in iteration 272 : 11.01038596981624
Loss in iteration 273 : 12.199102296851652
Loss in iteration 274 : 9.117928987689496
Loss in iteration 275 : 10.57103445037852
Loss in iteration 276 : 9.570713951663592
Loss in iteration 277 : 11.478341530731107
Loss in iteration 278 : 9.43514978021025
Loss in iteration 279 : 11.312252246936934
Loss in iteration 280 : 10.305396989345589
Loss in iteration 281 : 11.286059880077294
Loss in iteration 282 : 8.847090165477601
Loss in iteration 283 : 11.099166177151918
Loss in iteration 284 : 9.486913325305988
Loss in iteration 285 : 12.168181792478187
Loss in iteration 286 : 10.246894968969881
Loss in iteration 287 : 11.758387092607165
Loss in iteration 288 : 9.586958440289308
Loss in iteration 289 : 10.058312397927386
Loss in iteration 290 : 8.724169963329071
Loss in iteration 291 : 10.617091416665298
Loss in iteration 292 : 9.697583209718857
Loss in iteration 293 : 12.066783202461894
Loss in iteration 294 : 10.057166368227554
Loss in iteration 295 : 11.443918066503455
Loss in iteration 296 : 10.877853519143933
Loss in iteration 297 : 12.579955001517268
Loss in iteration 298 : 10.025533660789176
Loss in iteration 299 : 11.624892817811853
Loss in iteration 300 : 10.101559230389123
Loss in iteration 301 : 11.826165335798501
Loss in iteration 302 : 9.040658352203554
Loss in iteration 303 : 10.461313968290165
Loss in iteration 304 : 10.084044314626555
Loss in iteration 305 : 11.268897271271854
Loss in iteration 306 : 9.459167823568654
Loss in iteration 307 : 10.90235398895179
Loss in iteration 308 : 10.402430526432493
Loss in iteration 309 : 11.704132212280161
Loss in iteration 310 : 10.08206735441131
Loss in iteration 311 : 11.66836491751281
Loss in iteration 312 : 9.905810307121731
Loss in iteration 313 : 10.959113373292832
Loss in iteration 314 : 8.50895614472654
Loss in iteration 315 : 10.360853706516142
Loss in iteration 316 : 10.366397597211819
Loss in iteration 317 : 11.862797597949836
Loss in iteration 318 : 10.71797400069384
Loss in iteration 319 : 11.602226117827096
Loss in iteration 320 : 10.405984869691574
Loss in iteration 321 : 12.198876581332234
Loss in iteration 322 : 9.650954441238662
Loss in iteration 323 : 10.593298448718519
Loss in iteration 324 : 9.707696559398439
Loss in iteration 325 : 11.053796202730917
Loss in iteration 326 : 10.002597757287937
Loss in iteration 327 : 10.707152097904645
Loss in iteration 328 : 8.874749081542594
Loss in iteration 329 : 11.16948214579861
Loss in iteration 330 : 10.183457828773077
Loss in iteration 331 : 11.797971411709195
Loss in iteration 332 : 8.727842516608677
Loss in iteration 333 : 10.763196906282552
Loss in iteration 334 : 9.783519068582558
Loss in iteration 335 : 11.751223796436124
Loss in iteration 336 : 9.019144204523494
Loss in iteration 337 : 11.821008958916256
Loss in iteration 338 : 10.893824519845024
Loss in iteration 339 : 12.268302080346515
Loss in iteration 340 : 9.428791248969478
Loss in iteration 341 : 10.468801403761466
Loss in iteration 342 : 9.55557223578471
Loss in iteration 343 : 11.543172588920116
Loss in iteration 344 : 9.639847962224323
Loss in iteration 345 : 11.442112580101774
Loss in iteration 346 : 10.06685829315896
Loss in iteration 347 : 10.726485806169588
Loss in iteration 348 : 10.880662639585063
Loss in iteration 349 : 12.587908713514413
Loss in iteration 350 : 9.895973415170316
Loss in iteration 351 : 10.81959242806612
Loss in iteration 352 : 9.507397255731604
Loss in iteration 353 : 10.141894426309738
Loss in iteration 354 : 10.044872132057641
Loss in iteration 355 : 10.979414425792983
Loss in iteration 356 : 9.68987680822604
Loss in iteration 357 : 11.59212088940493
Loss in iteration 358 : 9.592656646469889
Loss in iteration 359 : 10.326816902339514
Loss in iteration 360 : 10.010746433987096
Loss in iteration 361 : 11.117788343428831
Loss in iteration 362 : 9.518278226612772
Loss in iteration 363 : 11.241286987140606
Loss in iteration 364 : 10.437186405460597
Loss in iteration 365 : 12.66723904816775
Loss in iteration 366 : 10.131113484211692
Loss in iteration 367 : 11.435814350628762
Loss in iteration 368 : 9.565518428360516
Loss in iteration 369 : 11.551189923803596
Loss in iteration 370 : 9.8317878244658
Loss in iteration 371 : 11.957831436838418
Loss in iteration 372 : 9.43212933051196
Loss in iteration 373 : 11.478189032572658
Loss in iteration 374 : 10.918844818910413
Loss in iteration 375 : 11.835040209842978
Loss in iteration 376 : 10.045976804194517
Loss in iteration 377 : 10.920479566290938
Loss in iteration 378 : 9.093462461071702
Loss in iteration 379 : 10.303845274096279
Loss in iteration 380 : 10.127627082237382
Loss in iteration 381 : 11.822859876790472
Loss in iteration 382 : 8.74486898960402
Loss in iteration 383 : 10.834636990818643
Loss in iteration 384 : 9.981591270538656
Loss in iteration 385 : 11.026614615747635
Loss in iteration 386 : 10.161335706464742
Loss in iteration 387 : 11.745925325225532
Loss in iteration 388 : 10.190466425390664
Loss in iteration 389 : 10.447612263164569
Loss in iteration 390 : 9.248243849770525
Loss in iteration 391 : 10.539227150417103
Loss in iteration 392 : 10.039988305322876
Loss in iteration 393 : 11.251651465501858
Loss in iteration 394 : 9.150373516758428
Loss in iteration 395 : 11.859412290732715
Loss in iteration 396 : 9.270054162160537
Loss in iteration 397 : 11.257254080155532
Loss in iteration 398 : 9.9288495591219
Loss in iteration 399 : 11.476038404674322
Loss in iteration 400 : 9.714946329417517
Testing accuracy  of updater 5 on alg 0 with rate 7.0 = 0.716625, training accuracy 0.716625, time elapsed: 4424 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 3.8909446087793294
Loss in iteration 3 : 36.9927139139705
Loss in iteration 4 : 5.66148238052083
Loss in iteration 5 : 22.61894080488411
Loss in iteration 6 : 8.876521954266781
Loss in iteration 7 : 16.45815998669097
Loss in iteration 8 : 9.14173057715217
Loss in iteration 9 : 11.575417017617331
Loss in iteration 10 : 11.100455838171676
Loss in iteration 11 : 8.64239514159014
Loss in iteration 12 : 11.801484985006777
Loss in iteration 13 : 5.638722676853904
Loss in iteration 14 : 9.927498025881743
Loss in iteration 15 : 6.4887212460034975
Loss in iteration 16 : 10.162379973668829
Loss in iteration 17 : 6.521011280879942
Loss in iteration 18 : 8.994931390265977
Loss in iteration 19 : 6.633945056325771
Loss in iteration 20 : 7.903803364962418
Loss in iteration 21 : 7.8999335927286625
Loss in iteration 22 : 7.656538053086945
Loss in iteration 23 : 7.293232851378724
Loss in iteration 24 : 6.838363260618217
Loss in iteration 25 : 7.290442933859993
Loss in iteration 26 : 6.304188606824735
Loss in iteration 27 : 6.584250004954827
Loss in iteration 28 : 6.014279803671149
Loss in iteration 29 : 7.410239549412962
Loss in iteration 30 : 6.425157363064966
Loss in iteration 31 : 8.147965774408638
Loss in iteration 32 : 7.28985725977551
Loss in iteration 33 : 6.966099100203035
Loss in iteration 34 : 6.349895411488817
Loss in iteration 35 : 7.296522600114366
Loss in iteration 36 : 6.145158839020346
Loss in iteration 37 : 6.967240890456031
Loss in iteration 38 : 6.197355045452591
Loss in iteration 39 : 7.049273750966695
Loss in iteration 40 : 5.9310852429585275
Loss in iteration 41 : 6.769320672749521
Loss in iteration 42 : 5.017987956030516
Loss in iteration 43 : 6.547854805426454
Loss in iteration 44 : 5.59753964612795
Loss in iteration 45 : 7.140063293483019
Loss in iteration 46 : 5.549873729152306
Loss in iteration 47 : 7.364211808378726
Loss in iteration 48 : 5.25957071364535
Loss in iteration 49 : 6.6665551244016275
Loss in iteration 50 : 6.047479048195831
Loss in iteration 51 : 7.694186142659715
Loss in iteration 52 : 6.237502829544064
Loss in iteration 53 : 7.3419071851544695
Loss in iteration 54 : 5.86732480452443
Loss in iteration 55 : 6.7811014424836324
Loss in iteration 56 : 6.094690755866307
Loss in iteration 57 : 6.7989517770959935
Loss in iteration 58 : 5.530179629112554
Loss in iteration 59 : 6.475533896049009
Loss in iteration 60 : 5.4728076222649475
Loss in iteration 61 : 6.512888418447932
Loss in iteration 62 : 5.508160133861337
Loss in iteration 63 : 6.8375188752258955
Loss in iteration 64 : 4.9835763365042105
Loss in iteration 65 : 6.916510983195642
Loss in iteration 66 : 4.992602930231753
Loss in iteration 67 : 6.803025606763534
Loss in iteration 68 : 5.21652149561694
Loss in iteration 69 : 6.983721501665698
Loss in iteration 70 : 5.926830610474485
Loss in iteration 71 : 7.514052056684978
Loss in iteration 72 : 6.121154556168978
Loss in iteration 73 : 6.728807656541386
Loss in iteration 74 : 5.7457621668002
Loss in iteration 75 : 6.840630645061611
Loss in iteration 76 : 5.542284279283445
Loss in iteration 77 : 6.647944619129772
Loss in iteration 78 : 4.838390391915971
Loss in iteration 79 : 5.888834118623852
Loss in iteration 80 : 5.145173991067816
Loss in iteration 81 : 6.564596001541871
Loss in iteration 82 : 6.035882870723143
Loss in iteration 83 : 7.2160324183700455
Loss in iteration 84 : 6.511195182765391
Loss in iteration 85 : 6.854934931190322
Loss in iteration 86 : 5.322705114384355
Loss in iteration 87 : 6.780000020586853
Loss in iteration 88 : 6.028351700723181
Loss in iteration 89 : 7.1085404103313
Loss in iteration 90 : 5.588440369584467
Loss in iteration 91 : 6.828808784209744
Loss in iteration 92 : 5.438153335093638
Loss in iteration 93 : 6.569240839257719
Loss in iteration 94 : 4.9656113420348955
Loss in iteration 95 : 6.389641769079363
Loss in iteration 96 : 5.662645088664525
Loss in iteration 97 : 7.579819028964988
Loss in iteration 98 : 6.579548940776468
Loss in iteration 99 : 7.356365378732994
Loss in iteration 100 : 5.803806838993935
Loss in iteration 101 : 6.707732078997495
Loss in iteration 102 : 5.114616315838147
Loss in iteration 103 : 6.5276565245350575
Loss in iteration 104 : 5.805376237841522
Loss in iteration 105 : 7.004749835019617
Loss in iteration 106 : 5.905411216863765
Loss in iteration 107 : 6.89762844977736
Loss in iteration 108 : 6.164699429489237
Loss in iteration 109 : 7.061993402661945
Loss in iteration 110 : 6.19888417859366
Loss in iteration 111 : 6.9877451468553895
Loss in iteration 112 : 4.332491191919155
Loss in iteration 113 : 5.219538669127887
Loss in iteration 114 : 5.662711684714886
Loss in iteration 115 : 6.9459088205431465
Loss in iteration 116 : 4.928228871646589
Loss in iteration 117 : 6.781335606422831
Loss in iteration 118 : 6.218472668573792
Loss in iteration 119 : 7.005953302507528
Loss in iteration 120 : 6.136637439711117
Loss in iteration 121 : 6.873702089507174
Loss in iteration 122 : 5.707058994095379
Loss in iteration 123 : 6.368716523001783
Loss in iteration 124 : 5.329579228592975
Loss in iteration 125 : 6.622064183179382
Loss in iteration 126 : 5.584310228453505
Loss in iteration 127 : 7.001399675093766
Loss in iteration 128 : 5.461272262485768
Loss in iteration 129 : 6.567658471725678
Loss in iteration 130 : 5.411275488935449
Loss in iteration 131 : 6.80849071333099
Loss in iteration 132 : 4.519065403483595
Loss in iteration 133 : 6.120826355597533
Loss in iteration 134 : 6.160813214277419
Loss in iteration 135 : 7.451636396529593
Loss in iteration 136 : 5.603008945969868
Loss in iteration 137 : 6.79794611943673
Loss in iteration 138 : 5.551208285813342
Loss in iteration 139 : 7.093726853635223
Loss in iteration 140 : 5.311029530605231
Loss in iteration 141 : 6.444225572318147
Loss in iteration 142 : 5.979904027674732
Loss in iteration 143 : 7.119689505792339
Loss in iteration 144 : 5.506571744188988
Loss in iteration 145 : 6.319715032589656
Loss in iteration 146 : 5.056059677132902
Loss in iteration 147 : 6.138898656160523
Loss in iteration 148 : 5.099465186010461
Loss in iteration 149 : 6.84690976836153
Loss in iteration 150 : 5.508908435578408
Loss in iteration 151 : 6.960390116430211
Loss in iteration 152 : 6.270701998777415
Loss in iteration 153 : 7.129975162872816
Loss in iteration 154 : 6.0343169234310965
Loss in iteration 155 : 6.781445267784729
Loss in iteration 156 : 5.51788655759682
Loss in iteration 157 : 7.275665010454481
Loss in iteration 158 : 5.891866696552497
Loss in iteration 159 : 6.603275483320066
Loss in iteration 160 : 4.8054415756981514
Loss in iteration 161 : 5.98920267254601
Loss in iteration 162 : 5.522539917633729
Loss in iteration 163 : 6.697944171562666
Loss in iteration 164 : 6.196170072719644
Loss in iteration 165 : 7.478709444338625
Loss in iteration 166 : 5.203995896401475
Loss in iteration 167 : 5.8512944336199615
Loss in iteration 168 : 5.525548698398033
Loss in iteration 169 : 6.643321217876554
Loss in iteration 170 : 6.158684807391787
Loss in iteration 171 : 6.899088533734336
Loss in iteration 172 : 4.888059028247165
Loss in iteration 173 : 5.9840425128617705
Loss in iteration 174 : 5.518903780589679
Loss in iteration 175 : 7.2435245006203735
Loss in iteration 176 : 5.320482447546944
Loss in iteration 177 : 6.801355039712947
Loss in iteration 178 : 6.069021173720163
Loss in iteration 179 : 7.365196433691147
Loss in iteration 180 : 5.814904072362641
Loss in iteration 181 : 7.180359733437844
Loss in iteration 182 : 5.39478873687227
Loss in iteration 183 : 6.623081461217077
Loss in iteration 184 : 5.044056673023611
Loss in iteration 185 : 6.167332804000198
Loss in iteration 186 : 5.5389806464067135
Loss in iteration 187 : 6.317033554753389
Loss in iteration 188 : 5.5932095509206325
Loss in iteration 189 : 6.586748801932822
Loss in iteration 190 : 5.551877080920194
Loss in iteration 191 : 6.386739722017551
Loss in iteration 192 : 6.133195171626604
Loss in iteration 193 : 7.3462302277515485
Loss in iteration 194 : 5.6674253717956775
Loss in iteration 195 : 6.773322289968732
Loss in iteration 196 : 5.728661989765376
Loss in iteration 197 : 6.564936629375479
Loss in iteration 198 : 5.207891372101246
Loss in iteration 199 : 5.904926999012531
Loss in iteration 200 : 5.262622634455461
Loss in iteration 201 : 6.802174464643097
Loss in iteration 202 : 6.5118548364404925
Loss in iteration 203 : 6.975383411685386
Loss in iteration 204 : 5.306459675361398
Loss in iteration 205 : 6.5775047414826755
Loss in iteration 206 : 5.678809043085752
Loss in iteration 207 : 6.342686361590704
Loss in iteration 208 : 5.680307000086378
Loss in iteration 209 : 6.402104604158195
Loss in iteration 210 : 5.107688246721825
Loss in iteration 211 : 6.325590580348204
Loss in iteration 212 : 5.645569040951658
Loss in iteration 213 : 6.563521639696922
Loss in iteration 214 : 5.955287447251314
Loss in iteration 215 : 7.092099592641165
Loss in iteration 216 : 6.119291410149537
Loss in iteration 217 : 6.885532941238545
Loss in iteration 218 : 5.1002980290025635
Loss in iteration 219 : 6.479029679916909
Loss in iteration 220 : 5.311899650711088
Loss in iteration 221 : 6.37200250918491
Loss in iteration 222 : 4.684339927285999
Loss in iteration 223 : 6.139920860516632
Loss in iteration 224 : 6.4693341681810494
Loss in iteration 225 : 7.612961119613734
Loss in iteration 226 : 5.646508789079646
Loss in iteration 227 : 6.272314404536135
Loss in iteration 228 : 5.54458821770397
Loss in iteration 229 : 6.314254015989529
Loss in iteration 230 : 5.248707601837532
Loss in iteration 231 : 6.492880630458126
Loss in iteration 232 : 5.597126908065039
Loss in iteration 233 : 7.017846195733461
Loss in iteration 234 : 6.0410613216245626
Loss in iteration 235 : 6.473005127427659
Loss in iteration 236 : 5.2925947470113
Loss in iteration 237 : 6.166876904430394
Loss in iteration 238 : 5.27459432671298
Loss in iteration 239 : 6.309383158888067
Loss in iteration 240 : 5.0521032730291955
Loss in iteration 241 : 6.620187429531789
Loss in iteration 242 : 5.871523519074692
Loss in iteration 243 : 7.336544391263991
Loss in iteration 244 : 6.289772601074136
Loss in iteration 245 : 7.31768280373572
Loss in iteration 246 : 5.682703433497022
Loss in iteration 247 : 6.072109417918895
Loss in iteration 248 : 5.349286657809095
Loss in iteration 249 : 6.268945756974913
Loss in iteration 250 : 5.606029547585242
Loss in iteration 251 : 6.830431669235467
Loss in iteration 252 : 5.523884992567698
Loss in iteration 253 : 7.030093589101046
Loss in iteration 254 : 5.05040953962761
Loss in iteration 255 : 6.357955441384675
Loss in iteration 256 : 6.0156181067406225
Loss in iteration 257 : 6.827347386380935
Loss in iteration 258 : 5.514677818683598
Loss in iteration 259 : 6.3775479634252825
Loss in iteration 260 : 5.738812678990559
Loss in iteration 261 : 6.787512848897542
Loss in iteration 262 : 5.8460245431222155
Loss in iteration 263 : 6.880398758758761
Loss in iteration 264 : 5.678946420109407
Loss in iteration 265 : 6.299974209229776
Loss in iteration 266 : 4.904394290485645
Loss in iteration 267 : 5.369633020765922
Loss in iteration 268 : 5.615367096179231
Loss in iteration 269 : 6.923073436661376
Loss in iteration 270 : 5.892752363068515
Loss in iteration 271 : 7.1896034384549425
Loss in iteration 272 : 6.2968967839435805
Loss in iteration 273 : 6.963294847345982
Loss in iteration 274 : 5.31883326659085
Loss in iteration 275 : 6.121504835044726
Loss in iteration 276 : 5.467882156585583
Loss in iteration 277 : 6.524005856909762
Loss in iteration 278 : 5.367574389492722
Loss in iteration 279 : 6.422146653592846
Loss in iteration 280 : 5.884171163903398
Loss in iteration 281 : 6.4295551081425915
Loss in iteration 282 : 5.164275687627559
Loss in iteration 283 : 6.450462429063571
Loss in iteration 284 : 5.453648988319719
Loss in iteration 285 : 6.938878853352297
Loss in iteration 286 : 5.818543353481876
Loss in iteration 287 : 6.658558145456508
Loss in iteration 288 : 5.508423635868685
Loss in iteration 289 : 5.772116921955319
Loss in iteration 290 : 4.966733752990692
Loss in iteration 291 : 6.037388793927505
Loss in iteration 292 : 5.575575441296176
Loss in iteration 293 : 6.914670741878438
Loss in iteration 294 : 5.783734283725133
Loss in iteration 295 : 6.574649513741805
Loss in iteration 296 : 6.258154738007205
Loss in iteration 297 : 7.16833593292919
Loss in iteration 298 : 5.771659399011454
Loss in iteration 299 : 6.6301738868641635
Loss in iteration 300 : 5.622915504205226
Loss in iteration 301 : 6.802143950229342
Loss in iteration 302 : 5.136724968021661
Loss in iteration 303 : 5.969981753351913
Loss in iteration 304 : 5.762390775445052
Loss in iteration 305 : 6.437911903141989
Loss in iteration 306 : 5.433663857143203
Loss in iteration 307 : 6.263955501194691
Loss in iteration 308 : 6.003485587727505
Loss in iteration 309 : 6.764666677946844
Loss in iteration 310 : 5.777559301371389
Loss in iteration 311 : 6.666167388180032
Loss in iteration 312 : 5.65993718516003
Loss in iteration 313 : 6.290587108111484
Loss in iteration 314 : 4.950217598310678
Loss in iteration 315 : 5.999827061876835
Loss in iteration 316 : 5.912954380639427
Loss in iteration 317 : 6.752615120702792
Loss in iteration 318 : 6.104412930248523
Loss in iteration 319 : 6.631105263151086
Loss in iteration 320 : 5.938613704282457
Loss in iteration 321 : 6.968063148825967
Loss in iteration 322 : 5.536462540860782
Loss in iteration 323 : 6.054322598058105
Loss in iteration 324 : 5.549059144814286
Loss in iteration 325 : 6.283978713266765
Loss in iteration 326 : 5.783098147461001
Loss in iteration 327 : 6.1397907363706965
Loss in iteration 328 : 5.067754817045485
Loss in iteration 329 : 6.337695373068334
Loss in iteration 330 : 5.84352791438746
Loss in iteration 331 : 6.7822011604643135
Loss in iteration 332 : 5.005443228194038
Loss in iteration 333 : 6.16466153579651
Loss in iteration 334 : 5.596240061220915
Loss in iteration 335 : 6.721318679604599
Loss in iteration 336 : 5.202740884142153
Loss in iteration 337 : 6.790954770827109
Loss in iteration 338 : 6.250301317591619
Loss in iteration 339 : 7.010962989532246
Loss in iteration 340 : 5.431632396775723
Loss in iteration 341 : 5.995526358462484
Loss in iteration 342 : 5.467696951268429
Loss in iteration 343 : 6.604503826143147
Loss in iteration 344 : 5.526570284003558
Loss in iteration 345 : 6.520653816723137
Loss in iteration 346 : 5.766381921944151
Loss in iteration 347 : 6.145100162466505
Loss in iteration 348 : 6.200205081016834
Loss in iteration 349 : 7.188789367481817
Loss in iteration 350 : 5.690146733491997
Loss in iteration 351 : 6.2093584960224355
Loss in iteration 352 : 5.4458053609409856
Loss in iteration 353 : 5.761042582991918
Loss in iteration 354 : 5.73703660422286
Loss in iteration 355 : 6.306835481162434
Loss in iteration 356 : 5.533390826511512
Loss in iteration 357 : 6.6158378741316906
Loss in iteration 358 : 5.510400600230313
Loss in iteration 359 : 5.896541066424799
Loss in iteration 360 : 5.7781433820360375
Loss in iteration 361 : 6.366293347533955
Loss in iteration 362 : 5.455355474159092
Loss in iteration 363 : 6.439100900353298
Loss in iteration 364 : 5.996446443148215
Loss in iteration 365 : 7.246220353737935
Loss in iteration 366 : 5.821665881423759
Loss in iteration 367 : 6.574170134942361
Loss in iteration 368 : 5.47791569409771
Loss in iteration 369 : 6.549697450813293
Loss in iteration 370 : 5.661081166134709
Loss in iteration 371 : 6.821434638168521
Loss in iteration 372 : 5.389442331602575
Loss in iteration 373 : 6.527538924218923
Loss in iteration 374 : 6.277076398600055
Loss in iteration 375 : 6.762189708391806
Loss in iteration 376 : 5.7836782360588455
Loss in iteration 377 : 6.268710488602775
Loss in iteration 378 : 5.197140055603739
Loss in iteration 379 : 5.932087018314194
Loss in iteration 380 : 5.778520260998923
Loss in iteration 381 : 6.735213155005854
Loss in iteration 382 : 5.0470547469997165
Loss in iteration 383 : 6.238868544143861
Loss in iteration 384 : 5.733793539487464
Loss in iteration 385 : 6.271223652833571
Loss in iteration 386 : 5.823772728285546
Loss in iteration 387 : 6.7046426544724165
Loss in iteration 388 : 5.827672626672142
Loss in iteration 389 : 5.9837617719102845
Loss in iteration 390 : 5.262372787518787
Loss in iteration 391 : 5.965811787503373
Loss in iteration 392 : 5.753923843641753
Loss in iteration 393 : 6.472883925226562
Loss in iteration 394 : 5.588503286061531
Loss in iteration 395 : 6.548932513601275
Loss in iteration 396 : 5.632615366836568
Loss in iteration 397 : 6.429266101106114
Loss in iteration 398 : 5.407136985748115
Loss in iteration 399 : 6.391424349553797
Loss in iteration 400 : 4.976117963649205
Testing accuracy  of updater 5 on alg 0 with rate 4.0 = 0.71825, training accuracy 0.71825, time elapsed: 4407 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.131182355904736
Loss in iteration 3 : 7.688225228188776
Loss in iteration 4 : 3.132315355463401
Loss in iteration 5 : 4.358098742037852
Loss in iteration 6 : 3.244833114911541
Loss in iteration 7 : 3.0816357291549994
Loss in iteration 8 : 3.1468839411482694
Loss in iteration 9 : 2.1345341361899135
Loss in iteration 10 : 3.408276888832001
Loss in iteration 11 : 1.6087194778858214
Loss in iteration 12 : 3.072022461253258
Loss in iteration 13 : 1.3412248570436234
Loss in iteration 14 : 2.582075965050493
Loss in iteration 15 : 1.7146365774631116
Loss in iteration 16 : 2.468812715805624
Loss in iteration 17 : 1.6845825493394206
Loss in iteration 18 : 2.2806730904518493
Loss in iteration 19 : 1.818207544035919
Loss in iteration 20 : 2.3101100025816144
Loss in iteration 21 : 1.8563435200152103
Loss in iteration 22 : 2.1225439556930183
Loss in iteration 23 : 1.7388029106419636
Loss in iteration 24 : 1.9072215138251278
Loss in iteration 25 : 1.7560851425270407
Loss in iteration 26 : 1.7229423098194292
Loss in iteration 27 : 1.6216719840472313
Loss in iteration 28 : 1.7201856524582801
Loss in iteration 29 : 1.8341482724139409
Loss in iteration 30 : 1.8001430767278181
Loss in iteration 31 : 1.963081686886618
Loss in iteration 32 : 1.963558683962936
Loss in iteration 33 : 1.711486232079921
Loss in iteration 34 : 1.7732082937624325
Loss in iteration 35 : 1.7709147029729846
Loss in iteration 36 : 1.7181095068915955
Loss in iteration 37 : 1.7115326204976342
Loss in iteration 38 : 1.7109781065623846
Loss in iteration 39 : 1.7430536804002317
Loss in iteration 40 : 1.6765568812021934
Loss in iteration 41 : 1.6684780162509765
Loss in iteration 42 : 1.4878824128034538
Loss in iteration 43 : 1.6507429772163444
Loss in iteration 44 : 1.503490107669521
Loss in iteration 45 : 1.6879693004783434
Loss in iteration 46 : 1.590033402867974
Loss in iteration 47 : 1.8896557886784724
Loss in iteration 48 : 1.7288725937373324
Loss in iteration 49 : 1.67388942229755
Loss in iteration 50 : 1.6297865568191137
Loss in iteration 51 : 1.8101662701480576
Loss in iteration 52 : 1.7166261741468554
Loss in iteration 53 : 1.7910649690743459
Loss in iteration 54 : 1.6114080702775895
Loss in iteration 55 : 1.6803633260351827
Loss in iteration 56 : 1.6506190041434026
Loss in iteration 57 : 1.6934336159867969
Loss in iteration 58 : 1.5379697763372673
Loss in iteration 59 : 1.6352478900616747
Loss in iteration 60 : 1.5605218077222933
Loss in iteration 61 : 1.6282575404152484
Loss in iteration 62 : 1.5007776489094675
Loss in iteration 63 : 1.6977918291105052
Loss in iteration 64 : 1.3809560980500393
Loss in iteration 65 : 1.7410823665858337
Loss in iteration 66 : 1.5510906245544924
Loss in iteration 67 : 1.7754951264789554
Loss in iteration 68 : 1.5062634250342462
Loss in iteration 69 : 1.694599889626424
Loss in iteration 70 : 1.5905686615107346
Loss in iteration 71 : 1.8085858059562294
Loss in iteration 72 : 1.6598531933301626
Loss in iteration 73 : 1.6473860326560588
Loss in iteration 74 : 1.543164869248419
Loss in iteration 75 : 1.7226428293606775
Loss in iteration 76 : 1.5531879677218245
Loss in iteration 77 : 1.6829214726511526
Loss in iteration 78 : 1.3043823996220274
Loss in iteration 79 : 1.4700389453196263
Loss in iteration 80 : 1.327666740519462
Loss in iteration 81 : 1.6544088895717164
Loss in iteration 82 : 1.708157315678703
Loss in iteration 83 : 1.8695996894533595
Loss in iteration 84 : 1.8315142578396983
Loss in iteration 85 : 1.742747327372207
Loss in iteration 86 : 1.5759849353984892
Loss in iteration 87 : 1.7302087673138307
Loss in iteration 88 : 1.6963654345945243
Loss in iteration 89 : 1.7618106323884362
Loss in iteration 90 : 1.4823164094514805
Loss in iteration 91 : 1.6493090687899914
Loss in iteration 92 : 1.4222023999591527
Loss in iteration 93 : 1.6014401613019056
Loss in iteration 94 : 1.4251048426785127
Loss in iteration 95 : 1.6473254733265434
Loss in iteration 96 : 1.5138220269897344
Loss in iteration 97 : 1.844273629281477
Loss in iteration 98 : 1.6729167122112012
Loss in iteration 99 : 1.8339900691060913
Loss in iteration 100 : 1.6075604245747859
Loss in iteration 101 : 1.7361153813180334
Loss in iteration 102 : 1.4505743844173125
Loss in iteration 103 : 1.679492883963553
Loss in iteration 104 : 1.6228076407133645
Loss in iteration 105 : 1.7586100887258875
Loss in iteration 106 : 1.5818662546258577
Loss in iteration 107 : 1.7296549541071227
Loss in iteration 108 : 1.6586678236501875
Loss in iteration 109 : 1.7906342229335974
Loss in iteration 110 : 1.6232763697403512
Loss in iteration 111 : 1.7330952276578273
Loss in iteration 112 : 1.2424599495505975
Loss in iteration 113 : 1.3598670991692097
Loss in iteration 114 : 1.522259767666651
Loss in iteration 115 : 1.7753745596578636
Loss in iteration 116 : 1.361275325447799
Loss in iteration 117 : 1.7389604938911458
Loss in iteration 118 : 1.6358165537890774
Loss in iteration 119 : 1.7183828369905016
Loss in iteration 120 : 1.6670447024565178
Loss in iteration 121 : 1.7801882633405754
Loss in iteration 122 : 1.5719587425485766
Loss in iteration 123 : 1.608056395136322
Loss in iteration 124 : 1.4761534982145628
Loss in iteration 125 : 1.6974685465808586
Loss in iteration 126 : 1.4793581384088137
Loss in iteration 127 : 1.7287348997902825
Loss in iteration 128 : 1.4851044628911223
Loss in iteration 129 : 1.6348189688270496
Loss in iteration 130 : 1.5287990096951467
Loss in iteration 131 : 1.7583802117928735
Loss in iteration 132 : 1.2481835722553316
Loss in iteration 133 : 1.5399086872847554
Loss in iteration 134 : 1.5730079454745938
Loss in iteration 135 : 1.818017395793494
Loss in iteration 136 : 1.5471708824137749
Loss in iteration 137 : 1.7493890917842738
Loss in iteration 138 : 1.5606378769135218
Loss in iteration 139 : 1.7970088853519917
Loss in iteration 140 : 1.4995893620685616
Loss in iteration 141 : 1.6578405261103526
Loss in iteration 142 : 1.5659253779632551
Loss in iteration 143 : 1.756713234289375
Loss in iteration 144 : 1.4819250681494287
Loss in iteration 145 : 1.611180434873966
Loss in iteration 146 : 1.392577755922617
Loss in iteration 147 : 1.5588004327577474
Loss in iteration 148 : 1.3820937033002727
Loss in iteration 149 : 1.7404665017476
Loss in iteration 150 : 1.5136928185273342
Loss in iteration 151 : 1.7864201873830785
Loss in iteration 152 : 1.6759738652914427
Loss in iteration 153 : 1.7893680838684756
Loss in iteration 154 : 1.6386063241918771
Loss in iteration 155 : 1.7058622098973177
Loss in iteration 156 : 1.5155348398958866
Loss in iteration 157 : 1.8391356242475738
Loss in iteration 158 : 1.5795789262556583
Loss in iteration 159 : 1.669929780566734
Loss in iteration 160 : 1.297297106701438
Loss in iteration 161 : 1.521320504065121
Loss in iteration 162 : 1.4765902088875469
Loss in iteration 163 : 1.6975688440492036
Loss in iteration 164 : 1.6707037704421952
Loss in iteration 165 : 1.8585116664016215
Loss in iteration 166 : 1.4488215984770334
Loss in iteration 167 : 1.5378551875841189
Loss in iteration 168 : 1.4954682929308152
Loss in iteration 169 : 1.6714769936967222
Loss in iteration 170 : 1.638257533008639
Loss in iteration 171 : 1.7153455520440144
Loss in iteration 172 : 1.3836581116328333
Loss in iteration 173 : 1.5798057510366843
Loss in iteration 174 : 1.4926685470468781
Loss in iteration 175 : 1.79649545425406
Loss in iteration 176 : 1.4897137089589427
Loss in iteration 177 : 1.7009485471930266
Loss in iteration 178 : 1.6243491883163628
Loss in iteration 179 : 1.822016952692105
Loss in iteration 180 : 1.5723605421257079
Loss in iteration 181 : 1.781570257133187
Loss in iteration 182 : 1.4490063446106427
Loss in iteration 183 : 1.6661575124127848
Loss in iteration 184 : 1.375744411275003
Loss in iteration 185 : 1.5700339141362316
Loss in iteration 186 : 1.4850974648644657
Loss in iteration 187 : 1.608770185402559
Loss in iteration 188 : 1.524956581114451
Loss in iteration 189 : 1.6945190217297414
Loss in iteration 190 : 1.5106220734652678
Loss in iteration 191 : 1.6171735629991861
Loss in iteration 192 : 1.628807880566832
Loss in iteration 193 : 1.8223832504651143
Loss in iteration 194 : 1.5440502048245692
Loss in iteration 195 : 1.7110912424828546
Loss in iteration 196 : 1.5711833946352385
Loss in iteration 197 : 1.6470518780500694
Loss in iteration 198 : 1.4285970092521965
Loss in iteration 199 : 1.5207507540233702
Loss in iteration 200 : 1.3855087231299494
Loss in iteration 201 : 1.6854033515138611
Loss in iteration 202 : 1.7280554141121778
Loss in iteration 203 : 1.7573913050083572
Loss in iteration 204 : 1.4845028352024219
Loss in iteration 205 : 1.7352600721745475
Loss in iteration 206 : 1.4840812997490074
Loss in iteration 207 : 1.596502455846475
Loss in iteration 208 : 1.4372031641735297
Loss in iteration 209 : 1.6454244275623224
Loss in iteration 210 : 1.4352359476857979
Loss in iteration 211 : 1.6549434107321725
Loss in iteration 212 : 1.4214788747506626
Loss in iteration 213 : 1.6009032557939418
Loss in iteration 214 : 1.6123262892280985
Loss in iteration 215 : 1.817230652577522
Loss in iteration 216 : 1.6979076156398
Loss in iteration 217 : 1.7546684633511573
Loss in iteration 218 : 1.384779689455525
Loss in iteration 219 : 1.6201848937418912
Loss in iteration 220 : 1.464799486382639
Loss in iteration 221 : 1.6381532910536047
Loss in iteration 222 : 1.319411625174756
Loss in iteration 223 : 1.6181914365983814
Loss in iteration 224 : 1.6551272933926688
Loss in iteration 225 : 1.8775515532591094
Loss in iteration 226 : 1.50951016577398
Loss in iteration 227 : 1.612122299302711
Loss in iteration 228 : 1.5018329539362525
Loss in iteration 229 : 1.6241299579139776
Loss in iteration 230 : 1.4888078312427717
Loss in iteration 231 : 1.6860898056359885
Loss in iteration 232 : 1.5237834370437817
Loss in iteration 233 : 1.7456476007157418
Loss in iteration 234 : 1.6230496737528048
Loss in iteration 235 : 1.628768262489253
Loss in iteration 236 : 1.4299958037704577
Loss in iteration 237 : 1.5747189279621232
Loss in iteration 238 : 1.3714512043592784
Loss in iteration 239 : 1.578813730867261
Loss in iteration 240 : 1.3515315283786895
Loss in iteration 241 : 1.6761936178508445
Loss in iteration 242 : 1.62963407288995
Loss in iteration 243 : 1.8888308516927166
Loss in iteration 244 : 1.7144358027212838
Loss in iteration 245 : 1.8567410820887569
Loss in iteration 246 : 1.5169139532944542
Loss in iteration 247 : 1.5318310476324382
Loss in iteration 248 : 1.4808570095005973
Loss in iteration 249 : 1.6054131678443833
Loss in iteration 250 : 1.5034853511762887
Loss in iteration 251 : 1.7102588902472902
Loss in iteration 252 : 1.4792109446210426
Loss in iteration 253 : 1.76741183464681
Loss in iteration 254 : 1.3599466364092472
Loss in iteration 255 : 1.6155635323001794
Loss in iteration 256 : 1.6232021000506227
Loss in iteration 257 : 1.7121501725895893
Loss in iteration 258 : 1.4799731474455213
Loss in iteration 259 : 1.626989147756533
Loss in iteration 260 : 1.552089326269995
Loss in iteration 261 : 1.724288772193378
Loss in iteration 262 : 1.5931276063723145
Loss in iteration 263 : 1.7177408504299219
Loss in iteration 264 : 1.5549495490413816
Loss in iteration 265 : 1.59985086158821
Loss in iteration 266 : 1.3158466290917037
Loss in iteration 267 : 1.3885660809284304
Loss in iteration 268 : 1.46909465351659
Loss in iteration 269 : 1.7153262066101542
Loss in iteration 270 : 1.6342300223090547
Loss in iteration 271 : 1.8657556551778878
Loss in iteration 272 : 1.7005230830280196
Loss in iteration 273 : 1.7343404520093253
Loss in iteration 274 : 1.434456963262045
Loss in iteration 275 : 1.5665630486068924
Loss in iteration 276 : 1.4485209900556035
Loss in iteration 277 : 1.65148264586733
Loss in iteration 278 : 1.4382943867437974
Loss in iteration 279 : 1.6276459879042635
Loss in iteration 280 : 1.5366559084165796
Loss in iteration 281 : 1.6211712692310296
Loss in iteration 282 : 1.4566163105991374
Loss in iteration 283 : 1.6805972142865024
Loss in iteration 284 : 1.4946326803226044
Loss in iteration 285 : 1.7704507337374429
Loss in iteration 286 : 1.5004402527584044
Loss in iteration 287 : 1.6286077846619396
Loss in iteration 288 : 1.484548336177392
Loss in iteration 289 : 1.4964904154803957
Loss in iteration 290 : 1.2938900251893721
Loss in iteration 291 : 1.5171576058111578
Loss in iteration 292 : 1.5856119893620004
Loss in iteration 293 : 1.8825558890713177
Loss in iteration 294 : 1.4192064340220443
Loss in iteration 295 : 1.609944305924367
Loss in iteration 296 : 1.613034054828882
Loss in iteration 297 : 1.862612384096928
Loss in iteration 298 : 1.6607811283257814
Loss in iteration 299 : 1.7381416011311193
Loss in iteration 300 : 1.6170815077868446
Loss in iteration 301 : 1.7396593487660326
Loss in iteration 302 : 1.4485878224569222
Loss in iteration 303 : 1.5008207201013444
Loss in iteration 304 : 1.4902012521785064
Loss in iteration 305 : 1.5765572086134694
Loss in iteration 306 : 1.396493249510483
Loss in iteration 307 : 1.5655740379766212
Loss in iteration 308 : 1.545541503461915
Loss in iteration 309 : 1.7018567820989468
Loss in iteration 310 : 1.5825899549704845
Loss in iteration 311 : 1.7374557992529758
Loss in iteration 312 : 1.5404540918490743
Loss in iteration 313 : 1.5954769040539396
Loss in iteration 314 : 1.3874194825839434
Loss in iteration 315 : 1.5724687697403064
Loss in iteration 316 : 1.5312993702724833
Loss in iteration 317 : 1.651991095245257
Loss in iteration 318 : 1.6356242736097053
Loss in iteration 319 : 1.6919660177232987
Loss in iteration 320 : 1.5757918985461201
Loss in iteration 321 : 1.7744801698225012
Loss in iteration 322 : 1.5080830283140125
Loss in iteration 323 : 1.5623874756035045
Loss in iteration 324 : 1.445827237780394
Loss in iteration 325 : 1.5589062799190443
Loss in iteration 326 : 1.5980881939078404
Loss in iteration 327 : 1.610101357844439
Loss in iteration 328 : 1.346692401256094
Loss in iteration 329 : 1.5988079148246228
Loss in iteration 330 : 1.5234809843510004
Loss in iteration 331 : 1.7289921887822868
Loss in iteration 332 : 1.3751318255112916
Loss in iteration 333 : 1.586994036988603
Loss in iteration 334 : 1.5579170957372646
Loss in iteration 335 : 1.7189697757257387
Loss in iteration 336 : 1.4927543294895784
Loss in iteration 337 : 1.746277106069593
Loss in iteration 338 : 1.6157608454536947
Loss in iteration 339 : 1.7158756194653888
Loss in iteration 340 : 1.4220071112073907
Loss in iteration 341 : 1.5245648194366366
Loss in iteration 342 : 1.4042413458509384
Loss in iteration 343 : 1.6523667744842665
Loss in iteration 344 : 1.5111791783877013
Loss in iteration 345 : 1.692905863969084
Loss in iteration 346 : 1.5742897259212556
Loss in iteration 347 : 1.618480092850063
Loss in iteration 348 : 1.6407096631225226
Loss in iteration 349 : 1.8004582615402542
Loss in iteration 350 : 1.5797604294245853
Loss in iteration 351 : 1.6192108623568617
Loss in iteration 352 : 1.4223032439247294
Loss in iteration 353 : 1.4242313650116596
Loss in iteration 354 : 1.4695557573241251
Loss in iteration 355 : 1.6034393616512723
Loss in iteration 356 : 1.4888515217648934
Loss in iteration 357 : 1.6929025717270298
Loss in iteration 358 : 1.519333042779218
Loss in iteration 359 : 1.5637113187224272
Loss in iteration 360 : 1.4997596722072382
Loss in iteration 361 : 1.6325808768033465
Loss in iteration 362 : 1.4467420410168121
Loss in iteration 363 : 1.651247084880824
Loss in iteration 364 : 1.5827268248942605
Loss in iteration 365 : 1.8125389953588749
Loss in iteration 366 : 1.6011616570932803
Loss in iteration 367 : 1.7038093198988116
Loss in iteration 368 : 1.4485513675484325
Loss in iteration 369 : 1.623059626035664
Loss in iteration 370 : 1.5447933641012803
Loss in iteration 371 : 1.7569613863862115
Loss in iteration 372 : 1.4355201144422638
Loss in iteration 373 : 1.6752905110872192
Loss in iteration 374 : 1.657091966145757
Loss in iteration 375 : 1.7053720001784263
Loss in iteration 376 : 1.553410336787904
Loss in iteration 377 : 1.617195430948902
Loss in iteration 378 : 1.4033475344322601
Loss in iteration 379 : 1.5271709153441553
Loss in iteration 380 : 1.5415218414881569
Loss in iteration 381 : 1.6941516481939478
Loss in iteration 382 : 1.4032532225188814
Loss in iteration 383 : 1.6304291815069012
Loss in iteration 384 : 1.5335380602738664
Loss in iteration 385 : 1.587463060012933
Loss in iteration 386 : 1.519481197207462
Loss in iteration 387 : 1.6930795989884921
Loss in iteration 388 : 1.5632047895361718
Loss in iteration 389 : 1.54627839075949
Loss in iteration 390 : 1.4077252205022617
Loss in iteration 391 : 1.6273260913152015
Loss in iteration 392 : 1.4156721985736311
Loss in iteration 393 : 1.6004490740035384
Loss in iteration 394 : 1.4769735788121439
Loss in iteration 395 : 1.7246946760255701
Loss in iteration 396 : 1.5419572776283106
Loss in iteration 397 : 1.7036589414546452
Loss in iteration 398 : 1.5421411783640577
Loss in iteration 399 : 1.6587544352368675
Loss in iteration 400 : 1.4235100188345802
Testing accuracy  of updater 5 on alg 0 with rate 1.0 = 0.722375, training accuracy 0.722375, time elapsed: 4467 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.9216188276251687
Loss in iteration 3 : 4.673723950937576
Loss in iteration 4 : 2.965333193448165
Loss in iteration 5 : 2.391856031776342
Loss in iteration 6 : 2.7028179412809012
Loss in iteration 7 : 1.8078627203060724
Loss in iteration 8 : 2.516194483811925
Loss in iteration 9 : 1.2949930298725558
Loss in iteration 10 : 2.4150997842654096
Loss in iteration 11 : 1.2339450507142988
Loss in iteration 12 : 2.0382261095907923
Loss in iteration 13 : 1.053190855725483
Loss in iteration 14 : 1.8045059903716307
Loss in iteration 15 : 1.354131486543535
Loss in iteration 16 : 1.8886335716044895
Loss in iteration 17 : 1.255899585967572
Loss in iteration 18 : 1.7163220102172967
Loss in iteration 19 : 1.1837433618623707
Loss in iteration 20 : 1.5036359651752107
Loss in iteration 21 : 1.3548104235368648
Loss in iteration 22 : 1.4435951299887115
Loss in iteration 23 : 1.2478015254440813
Loss in iteration 24 : 1.4068859648025702
Loss in iteration 25 : 1.2382838613215066
Loss in iteration 26 : 1.3169713328583146
Loss in iteration 27 : 1.1702566237581478
Loss in iteration 28 : 1.2604644745558498
Loss in iteration 29 : 1.2979989780170904
Loss in iteration 30 : 1.2956872889431676
Loss in iteration 31 : 1.3960884683592136
Loss in iteration 32 : 1.4455042923242687
Loss in iteration 33 : 1.2489599694136855
Loss in iteration 34 : 1.3369901835818008
Loss in iteration 35 : 1.2798582213510326
Loss in iteration 36 : 1.3002291858879513
Loss in iteration 37 : 1.225715151605603
Loss in iteration 38 : 1.278289201153229
Loss in iteration 39 : 1.2469849407464069
Loss in iteration 40 : 1.2641189646989337
Loss in iteration 41 : 1.1918783611630432
Loss in iteration 42 : 1.1347821407993657
Loss in iteration 43 : 1.1855982741847848
Loss in iteration 44 : 1.194469117025211
Loss in iteration 45 : 1.2024720365514756
Loss in iteration 46 : 1.1560464325076485
Loss in iteration 47 : 1.3020492607032788
Loss in iteration 48 : 1.26044205994498
Loss in iteration 49 : 1.2172688296790026
Loss in iteration 50 : 1.2465214330230285
Loss in iteration 51 : 1.3006852614122146
Loss in iteration 52 : 1.313714344360159
Loss in iteration 53 : 1.2841447458589594
Loss in iteration 54 : 1.2155397525457114
Loss in iteration 55 : 1.2015297409836017
Loss in iteration 56 : 1.2354121772262032
Loss in iteration 57 : 1.2100285752580435
Loss in iteration 58 : 1.1603018399922047
Loss in iteration 59 : 1.1723814799570282
Loss in iteration 60 : 1.168914403306835
Loss in iteration 61 : 1.1677040101308684
Loss in iteration 62 : 1.142550633771738
Loss in iteration 63 : 1.213332591906568
Loss in iteration 64 : 1.064434641234645
Loss in iteration 65 : 1.2463622884231573
Loss in iteration 66 : 1.158243214479299
Loss in iteration 67 : 1.2598206369146423
Loss in iteration 68 : 1.1281975236377537
Loss in iteration 69 : 1.2071101851787185
Loss in iteration 70 : 1.1955994167082968
Loss in iteration 71 : 1.2887437178893464
Loss in iteration 72 : 1.2444422673389222
Loss in iteration 73 : 1.179434602578063
Loss in iteration 74 : 1.1570956463818383
Loss in iteration 75 : 1.2321872764696844
Loss in iteration 76 : 1.185348981167803
Loss in iteration 77 : 1.2008061626282927
Loss in iteration 78 : 1.018494291375259
Loss in iteration 79 : 1.0737587472318226
Loss in iteration 80 : 0.9996874140384254
Loss in iteration 81 : 1.1742689227985095
Loss in iteration 82 : 1.2343780879417294
Loss in iteration 83 : 1.3185971756360502
Loss in iteration 84 : 1.3476075794627338
Loss in iteration 85 : 1.2564369663699357
Loss in iteration 86 : 1.195571715161403
Loss in iteration 87 : 1.2440474084565811
Loss in iteration 88 : 1.2649960513767096
Loss in iteration 89 : 1.258329612126217
Loss in iteration 90 : 1.1184043805805084
Loss in iteration 91 : 1.200164762752609
Loss in iteration 92 : 1.0831076200813927
Loss in iteration 93 : 1.1826263240147945
Loss in iteration 94 : 1.0463378492231978
Loss in iteration 95 : 1.1450722544392617
Loss in iteration 96 : 1.1022054995080424
Loss in iteration 97 : 1.3176568388752659
Loss in iteration 98 : 1.3156993148484653
Loss in iteration 99 : 1.3343992658625585
Loss in iteration 100 : 1.2124884193835153
Loss in iteration 101 : 1.234216614104758
Loss in iteration 102 : 1.0902647765490192
Loss in iteration 103 : 1.1907362163839081
Loss in iteration 104 : 1.1877629858587322
Loss in iteration 105 : 1.2354514174155338
Loss in iteration 106 : 1.1769252051071408
Loss in iteration 107 : 1.2429540509155363
Loss in iteration 108 : 1.2373907961668182
Loss in iteration 109 : 1.2867684350440647
Loss in iteration 110 : 1.2028436465365369
Loss in iteration 111 : 1.2291290259948195
Loss in iteration 112 : 0.9459367844072761
Loss in iteration 113 : 0.9800468727990204
Loss in iteration 114 : 1.1082602973118674
Loss in iteration 115 : 1.2680944030750558
Loss in iteration 116 : 1.0760835428442088
Loss in iteration 117 : 1.2871309067429857
Loss in iteration 118 : 1.1905023464542892
Loss in iteration 119 : 1.2043338819916567
Loss in iteration 120 : 1.1943826666470794
Loss in iteration 121 : 1.2753548656316185
Loss in iteration 122 : 1.1811917785294455
Loss in iteration 123 : 1.1690193984485655
Loss in iteration 124 : 1.1217668306502622
Loss in iteration 125 : 1.2332246225591317
Loss in iteration 126 : 1.1164963627811597
Loss in iteration 127 : 1.255721567604202
Loss in iteration 128 : 1.1045617628097608
Loss in iteration 129 : 1.1744187113911053
Loss in iteration 130 : 1.124018930769895
Loss in iteration 131 : 1.2284248514591094
Loss in iteration 132 : 0.981521085398037
Loss in iteration 133 : 1.1533827426557124
Loss in iteration 134 : 1.2072458008000868
Loss in iteration 135 : 1.297838373386534
Loss in iteration 136 : 1.1451676599501202
Loss in iteration 137 : 1.229028130875523
Loss in iteration 138 : 1.1513088063485817
Loss in iteration 139 : 1.2644460867794243
Loss in iteration 140 : 1.1256338917184905
Loss in iteration 141 : 1.1900689321564537
Loss in iteration 142 : 1.1551719302486005
Loss in iteration 143 : 1.2597117539089617
Loss in iteration 144 : 1.1001786093070685
Loss in iteration 145 : 1.155747389090932
Loss in iteration 146 : 1.0471504282957387
Loss in iteration 147 : 1.1293544489966247
Loss in iteration 148 : 1.053923380807009
Loss in iteration 149 : 1.2515209224555297
Loss in iteration 150 : 1.1531144068934096
Loss in iteration 151 : 1.288480942675557
Loss in iteration 152 : 1.239904721079139
Loss in iteration 153 : 1.2662070698504773
Loss in iteration 154 : 1.2126526328758538
Loss in iteration 155 : 1.2149826235114702
Loss in iteration 156 : 1.1252094670563086
Loss in iteration 157 : 1.3090975111120877
Loss in iteration 158 : 1.180185657153494
Loss in iteration 159 : 1.2040883460022518
Loss in iteration 160 : 0.9812837886179607
Loss in iteration 161 : 1.0975033445809081
Loss in iteration 162 : 1.0910478460289752
Loss in iteration 163 : 1.2105410062897874
Loss in iteration 164 : 1.23637011274337
Loss in iteration 165 : 1.3266674942595047
Loss in iteration 166 : 1.0989492944422963
Loss in iteration 167 : 1.1229665500734742
Loss in iteration 168 : 1.1083488932628687
Loss in iteration 169 : 1.187812553501345
Loss in iteration 170 : 1.2150196727945322
Loss in iteration 171 : 1.2261823841631865
Loss in iteration 172 : 1.0650110912813215
Loss in iteration 173 : 1.1553868323822782
Loss in iteration 174 : 1.1111869720684546
Loss in iteration 175 : 1.2732989094115512
Loss in iteration 176 : 1.1103019976488508
Loss in iteration 177 : 1.2102417725780585
Loss in iteration 178 : 1.1902707792092106
Loss in iteration 179 : 1.2893374950165362
Loss in iteration 180 : 1.1687006852138264
Loss in iteration 181 : 1.2687315319222199
Loss in iteration 182 : 1.1168179320263825
Loss in iteration 183 : 1.2128452632968465
Loss in iteration 184 : 1.0350922469857649
Loss in iteration 185 : 1.1207531650058997
Loss in iteration 186 : 1.0878637173815777
Loss in iteration 187 : 1.1442184445133516
Loss in iteration 188 : 1.1260267990812878
Loss in iteration 189 : 1.217243430383058
Loss in iteration 190 : 1.1330833371440654
Loss in iteration 191 : 1.1709399604870254
Loss in iteration 192 : 1.21044570146772
Loss in iteration 193 : 1.2992971548164156
Loss in iteration 194 : 1.157994205408765
Loss in iteration 195 : 1.2262559870084895
Loss in iteration 196 : 1.1721425323620154
Loss in iteration 197 : 1.1779535093780702
Loss in iteration 198 : 1.0763778191721016
Loss in iteration 199 : 1.1032321770800917
Loss in iteration 200 : 1.028432889082422
Loss in iteration 201 : 1.2106383751620344
Loss in iteration 202 : 1.2625731827569249
Loss in iteration 203 : 1.2726838888016134
Loss in iteration 204 : 1.0512313954143275
Loss in iteration 205 : 1.2239339017534747
Loss in iteration 206 : 1.1520688585242733
Loss in iteration 207 : 1.194334154311639
Loss in iteration 208 : 1.1400966244629194
Loss in iteration 209 : 1.189904794194864
Loss in iteration 210 : 1.1037071601540756
Loss in iteration 211 : 1.1824207165759757
Loss in iteration 212 : 1.0992164823066974
Loss in iteration 213 : 1.1414044190931973
Loss in iteration 214 : 1.1468098850941895
Loss in iteration 215 : 1.2431782361469652
Loss in iteration 216 : 1.2120326206099252
Loss in iteration 217 : 1.2305483220187856
Loss in iteration 218 : 1.0423755230408664
Loss in iteration 219 : 1.1925436348113296
Loss in iteration 220 : 1.0921283106105815
Loss in iteration 221 : 1.190339867774523
Loss in iteration 222 : 0.9495342707141948
Loss in iteration 223 : 1.1338124844065645
Loss in iteration 224 : 1.2545250891738984
Loss in iteration 225 : 1.3744454356695068
Loss in iteration 226 : 1.1844646990688943
Loss in iteration 227 : 1.1770816421392378
Loss in iteration 228 : 1.1179419244030353
Loss in iteration 229 : 1.1601223240709597
Loss in iteration 230 : 1.0984489597708527
Loss in iteration 231 : 1.2052784075737373
Loss in iteration 232 : 1.120239380290725
Loss in iteration 233 : 1.2380350401788431
Loss in iteration 234 : 1.2054566177922121
Loss in iteration 235 : 1.1713424892013613
Loss in iteration 236 : 1.0726729757492421
Loss in iteration 237 : 1.134032219426595
Loss in iteration 238 : 1.023451599520292
Loss in iteration 239 : 1.1319890385337092
Loss in iteration 240 : 1.0167826845641148
Loss in iteration 241 : 1.2092089151328616
Loss in iteration 242 : 1.226056977666864
Loss in iteration 243 : 1.3499283830539235
Loss in iteration 244 : 1.26551323378677
Loss in iteration 245 : 1.3258757615110355
Loss in iteration 246 : 1.1261490942039993
Loss in iteration 247 : 1.1003951701736867
Loss in iteration 248 : 1.1011652447543168
Loss in iteration 249 : 1.156913048107319
Loss in iteration 250 : 1.1077451100821194
Loss in iteration 251 : 1.2272887406247663
Loss in iteration 252 : 1.1104463986532878
Loss in iteration 253 : 1.2643510388555907
Loss in iteration 254 : 1.0302922569714166
Loss in iteration 255 : 1.1697984736093767
Loss in iteration 256 : 1.211364699923717
Loss in iteration 257 : 1.2223114101958896
Loss in iteration 258 : 1.1071571739896764
Loss in iteration 259 : 1.1692929253162172
Loss in iteration 260 : 1.1421783123615912
Loss in iteration 261 : 1.22424399095763
Loss in iteration 262 : 1.1822985877750376
Loss in iteration 263 : 1.2264809612759466
Loss in iteration 264 : 1.1601776080880712
Loss in iteration 265 : 1.1518046403769067
Loss in iteration 266 : 0.991695317011931
Loss in iteration 267 : 1.019506173986978
Loss in iteration 268 : 1.0798657289414044
Loss in iteration 269 : 1.2141624111994567
Loss in iteration 270 : 1.2316519163430328
Loss in iteration 271 : 1.345963025368864
Loss in iteration 272 : 1.2627830433170477
Loss in iteration 273 : 1.2350947187720605
Loss in iteration 274 : 1.07194361569548
Loss in iteration 275 : 1.1292579375857867
Loss in iteration 276 : 1.0790877428941261
Loss in iteration 277 : 1.1825924903507083
Loss in iteration 278 : 1.0755421234516602
Loss in iteration 279 : 1.1686230150461079
Loss in iteration 280 : 1.1252380857869424
Loss in iteration 281 : 1.1552077461553607
Loss in iteration 282 : 1.0985825351197813
Loss in iteration 283 : 1.2139356784455877
Loss in iteration 284 : 1.1243287940725966
Loss in iteration 285 : 1.276950087781489
Loss in iteration 286 : 1.1094867044265406
Loss in iteration 287 : 1.1651345970206677
Loss in iteration 288 : 1.088572888168937
Loss in iteration 289 : 1.0781205326305217
Loss in iteration 290 : 0.9714570441940684
Loss in iteration 291 : 1.127389598710496
Loss in iteration 292 : 1.1426814890714883
Loss in iteration 293 : 1.309411275464658
Loss in iteration 294 : 1.121678700887735
Loss in iteration 295 : 1.2076226119125013
Loss in iteration 296 : 1.2440882239244855
Loss in iteration 297 : 1.3302076906838323
Loss in iteration 298 : 1.215947442174998
Loss in iteration 299 : 1.2191260400502744
Loss in iteration 300 : 1.1701104986170494
Loss in iteration 301 : 1.224684497712965
Loss in iteration 302 : 1.074813133395117
Loss in iteration 303 : 1.0895025183897145
Loss in iteration 304 : 1.0902066991137227
Loss in iteration 305 : 1.131026767752155
Loss in iteration 306 : 1.0288553188311915
Loss in iteration 307 : 1.139131601642789
Loss in iteration 308 : 1.1624333638004336
Loss in iteration 309 : 1.2380196138227872
Loss in iteration 310 : 1.15377184959914
Loss in iteration 311 : 1.2508450624609515
Loss in iteration 312 : 1.1863526761298857
Loss in iteration 313 : 1.1657330831365371
Loss in iteration 314 : 1.0621099073294176
Loss in iteration 315 : 1.131633992872491
Loss in iteration 316 : 1.1207561677614737
Loss in iteration 317 : 1.1659454280267196
Loss in iteration 318 : 1.1981764624825535
Loss in iteration 319 : 1.2107315980922846
Loss in iteration 320 : 1.159751637539901
Loss in iteration 321 : 1.2709077210641835
Loss in iteration 322 : 1.1260778381969576
Loss in iteration 323 : 1.1344469768884815
Loss in iteration 324 : 1.076249287915797
Loss in iteration 325 : 1.1250092055592769
Loss in iteration 326 : 1.1835896646088717
Loss in iteration 327 : 1.1641507193133231
Loss in iteration 328 : 1.0074792762282374
Loss in iteration 329 : 1.1485993433904997
Loss in iteration 330 : 1.1376860165387808
Loss in iteration 331 : 1.2402100933217952
Loss in iteration 332 : 1.0510367280230397
Loss in iteration 333 : 1.1550316506916956
Loss in iteration 334 : 1.1470969428055988
Loss in iteration 335 : 1.2264952921593804
Loss in iteration 336 : 1.1044942684257215
Loss in iteration 337 : 1.24615561450834
Loss in iteration 338 : 1.1929689949865836
Loss in iteration 339 : 1.2282755340834182
Loss in iteration 340 : 1.0600368325073872
Loss in iteration 341 : 1.0994271558097288
Loss in iteration 342 : 1.044516784681309
Loss in iteration 343 : 1.1830683326773115
Loss in iteration 344 : 1.1317581701214445
Loss in iteration 345 : 1.2237130072604068
Loss in iteration 346 : 1.1715039177341322
Loss in iteration 347 : 1.1727251060546526
Loss in iteration 348 : 1.2027376898305964
Loss in iteration 349 : 1.2820517699415983
Loss in iteration 350 : 1.1844353016454494
Loss in iteration 351 : 1.1777013398556166
Loss in iteration 352 : 1.047213721755921
Loss in iteration 353 : 1.0233307235991476
Loss in iteration 354 : 1.0725262091309125
Loss in iteration 355 : 1.1522838600523886
Loss in iteration 356 : 1.104661994186535
Loss in iteration 357 : 1.2285038789997325
Loss in iteration 358 : 1.1253685758545318
Loss in iteration 359 : 1.168209340597407
Loss in iteration 360 : 1.1691802783755372
Loss in iteration 361 : 1.1560668707990374
Loss in iteration 362 : 1.1172027146462624
Loss in iteration 363 : 1.2079199600826895
Loss in iteration 364 : 1.1837119970140746
Loss in iteration 365 : 1.2830368424140346
Loss in iteration 366 : 1.177521599230607
Loss in iteration 367 : 1.2159353410982752
Loss in iteration 368 : 1.0645483262660236
Loss in iteration 369 : 1.1530452289657185
Loss in iteration 370 : 1.1188067086332696
Loss in iteration 371 : 1.2421418733019038
Loss in iteration 372 : 1.1006918844324445
Loss in iteration 373 : 1.2279795111407372
Loss in iteration 374 : 1.2308399933855059
Loss in iteration 375 : 1.2028543743755544
Loss in iteration 376 : 1.1589421757762752
Loss in iteration 377 : 1.153754926321644
Loss in iteration 378 : 1.067265217361338
Loss in iteration 379 : 1.1205015472515514
Loss in iteration 380 : 1.1063688052528178
Loss in iteration 381 : 1.1803269143026194
Loss in iteration 382 : 1.0357455808505434
Loss in iteration 383 : 1.18008948538382
Loss in iteration 384 : 1.151016734199597
Loss in iteration 385 : 1.162404756831401
Loss in iteration 386 : 1.1346924806137326
Loss in iteration 387 : 1.2322395403198416
Loss in iteration 388 : 1.1657243531845052
Loss in iteration 389 : 1.119248639125691
Loss in iteration 390 : 1.0229862624064239
Loss in iteration 391 : 1.150859394579997
Loss in iteration 392 : 1.082314681080112
Loss in iteration 393 : 1.174164508470314
Loss in iteration 394 : 1.1104854202478887
Loss in iteration 395 : 1.2323431043153512
Loss in iteration 396 : 1.1476394535846846
Loss in iteration 397 : 1.218744346603078
Loss in iteration 398 : 1.1359583546829632
Loss in iteration 399 : 1.183857471741908
Loss in iteration 400 : 1.0454775835784678
Testing accuracy  of updater 5 on alg 0 with rate 0.7 = 0.7275, training accuracy 0.7275, time elapsed: 4464 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.7540268260710024
Loss in iteration 3 : 1.9224241388535193
Loss in iteration 4 : 2.388191134009031
Loss in iteration 5 : 0.9650455816398462
Loss in iteration 6 : 1.5667756694178772
Loss in iteration 7 : 1.2046612090840505
Loss in iteration 8 : 1.5150945131924074
Loss in iteration 9 : 0.8771490960230907
Loss in iteration 10 : 1.2455218463197915
Loss in iteration 11 : 0.9434089263212756
Loss in iteration 12 : 1.1129108215246304
Loss in iteration 13 : 0.8317072277542347
Loss in iteration 14 : 1.063725152860216
Loss in iteration 15 : 0.8935144332122723
Loss in iteration 16 : 1.0676715064930742
Loss in iteration 17 : 0.8605939777975615
Loss in iteration 18 : 0.9984658513194083
Loss in iteration 19 : 0.8141061685799564
Loss in iteration 20 : 0.9137835626281396
Loss in iteration 21 : 0.8691990107635165
Loss in iteration 22 : 0.9108238653578175
Loss in iteration 23 : 0.8189859961296776
Loss in iteration 24 : 0.8787766115266168
Loss in iteration 25 : 0.8282513865609634
Loss in iteration 26 : 0.8340214212775827
Loss in iteration 27 : 0.7675607566206761
Loss in iteration 28 : 0.858364064340244
Loss in iteration 29 : 0.8529857030328418
Loss in iteration 30 : 0.9190232837756376
Loss in iteration 31 : 0.8852126258070946
Loss in iteration 32 : 0.9399515793074626
Loss in iteration 33 : 0.8099636646090986
Loss in iteration 34 : 0.8607419411984257
Loss in iteration 35 : 0.8279989601359259
Loss in iteration 36 : 0.8569426050777706
Loss in iteration 37 : 0.7975944506860314
Loss in iteration 38 : 0.8484286937141431
Loss in iteration 39 : 0.8137134664821354
Loss in iteration 40 : 0.8497939513771207
Loss in iteration 41 : 0.7987808548827555
Loss in iteration 42 : 0.7738073027581137
Loss in iteration 43 : 0.770525993200003
Loss in iteration 44 : 0.7798427332014201
Loss in iteration 45 : 0.7983316536536174
Loss in iteration 46 : 0.8496143891459079
Loss in iteration 47 : 0.8577193027715766
Loss in iteration 48 : 0.8733635004059193
Loss in iteration 49 : 0.7884861667944171
Loss in iteration 50 : 0.8116763752600206
Loss in iteration 51 : 0.8212882282956161
Loss in iteration 52 : 0.8649293752412295
Loss in iteration 53 : 0.8249772951354061
Loss in iteration 54 : 0.8232337345508065
Loss in iteration 55 : 0.7840966526475055
Loss in iteration 56 : 0.8279452719800966
Loss in iteration 57 : 0.7833202167355332
Loss in iteration 58 : 0.7911792446331568
Loss in iteration 59 : 0.7715283340522808
Loss in iteration 60 : 0.7971287024822415
Loss in iteration 61 : 0.7605939470589851
Loss in iteration 62 : 0.7763923323386646
Loss in iteration 63 : 0.7806657634090534
Loss in iteration 64 : 0.7713381114015736
Loss in iteration 65 : 0.8133394913544724
Loss in iteration 66 : 0.8058645567742824
Loss in iteration 67 : 0.8082715056791223
Loss in iteration 68 : 0.7702630080330557
Loss in iteration 69 : 0.7672930558282999
Loss in iteration 70 : 0.7819373276451367
Loss in iteration 71 : 0.8124061382171764
Loss in iteration 72 : 0.8250690198196029
Loss in iteration 73 : 0.7729712294318068
Loss in iteration 74 : 0.7864338530861817
Loss in iteration 75 : 0.7930937224405232
Loss in iteration 76 : 0.815194818775949
Loss in iteration 77 : 0.7726061809209649
Loss in iteration 78 : 0.7447217953489919
Loss in iteration 79 : 0.7343597056083503
Loss in iteration 80 : 0.7360293466025207
Loss in iteration 81 : 0.7537436392447401
Loss in iteration 82 : 0.7938672721326373
Loss in iteration 83 : 0.828466601261044
Loss in iteration 84 : 0.8856631207292179
Loss in iteration 85 : 0.8048505679317105
Loss in iteration 86 : 0.8131073015341165
Loss in iteration 87 : 0.7975215611594229
Loss in iteration 88 : 0.8494982605163931
Loss in iteration 89 : 0.8078337498859655
Loss in iteration 90 : 0.7727769592358134
Loss in iteration 91 : 0.7795206499124017
Loss in iteration 92 : 0.749188587582225
Loss in iteration 93 : 0.7500710125666521
Loss in iteration 94 : 0.7279381327547816
Loss in iteration 95 : 0.7417328286984953
Loss in iteration 96 : 0.7777826592752323
Loss in iteration 97 : 0.8403514649962949
Loss in iteration 98 : 0.8763385473880414
Loss in iteration 99 : 0.8408204778489
Loss in iteration 100 : 0.808680433067145
Loss in iteration 101 : 0.7799587156551596
Loss in iteration 102 : 0.7426371014636094
Loss in iteration 103 : 0.7637268055552133
Loss in iteration 104 : 0.7860404247658895
Loss in iteration 105 : 0.7812203392184776
Loss in iteration 106 : 0.7922039481448776
Loss in iteration 107 : 0.8086454311507332
Loss in iteration 108 : 0.843785925905376
Loss in iteration 109 : 0.8229933121830294
Loss in iteration 110 : 0.8261315996990364
Loss in iteration 111 : 0.7900326326524831
Loss in iteration 112 : 0.6689952385866219
Loss in iteration 113 : 0.6521916720790633
Loss in iteration 114 : 0.7046605295661144
Loss in iteration 115 : 0.7546502302231053
Loss in iteration 116 : 0.7857884365573786
Loss in iteration 117 : 0.850771062242374
Loss in iteration 118 : 0.8178357322777854
Loss in iteration 119 : 0.7775929373645754
Loss in iteration 120 : 0.8252359613879612
Loss in iteration 121 : 0.8109944027137078
Loss in iteration 122 : 0.7987628765910986
Loss in iteration 123 : 0.7390819126612143
Loss in iteration 124 : 0.7369845381051
Loss in iteration 125 : 0.7791338653844412
Loss in iteration 126 : 0.7629814571530884
Loss in iteration 127 : 0.7978749461933035
Loss in iteration 128 : 0.7862041365719025
Loss in iteration 129 : 0.7637859701968585
Loss in iteration 130 : 0.7790813208053287
Loss in iteration 131 : 0.7777251238551237
Loss in iteration 132 : 0.7126915090715398
Loss in iteration 133 : 0.7500011621020385
Loss in iteration 134 : 0.7682257017938043
Loss in iteration 135 : 0.7908055647310368
Loss in iteration 136 : 0.7654236206833473
Loss in iteration 137 : 0.7939032651670476
Loss in iteration 138 : 0.7928436999857609
Loss in iteration 139 : 0.8169224782077251
Loss in iteration 140 : 0.772184548949764
Loss in iteration 141 : 0.7719545482453908
Loss in iteration 142 : 0.7775537354439879
Loss in iteration 143 : 0.8110688075751883
Loss in iteration 144 : 0.7605582028426426
Loss in iteration 145 : 0.7493589857298574
Loss in iteration 146 : 0.7372414810253544
Loss in iteration 147 : 0.7373427965030678
Loss in iteration 148 : 0.7398482071564347
Loss in iteration 149 : 0.7950932606228442
Loss in iteration 150 : 0.7821995056280209
Loss in iteration 151 : 0.8152244177339681
Loss in iteration 152 : 0.817877458371778
Loss in iteration 153 : 0.7957092393492131
Loss in iteration 154 : 0.8103310927123519
Loss in iteration 155 : 0.7810457481761619
Loss in iteration 156 : 0.755122804652267
Loss in iteration 157 : 0.8270287972884182
Loss in iteration 158 : 0.8044458002390854
Loss in iteration 159 : 0.7819384771012644
Loss in iteration 160 : 0.6980229701864248
Loss in iteration 161 : 0.7164111803356941
Loss in iteration 162 : 0.7283941678878207
Loss in iteration 163 : 0.7612738862331999
Loss in iteration 164 : 0.8126940995718087
Loss in iteration 165 : 0.8387980431176423
Loss in iteration 166 : 0.7695330932729811
Loss in iteration 167 : 0.7450721364861694
Loss in iteration 168 : 0.7464757976463412
Loss in iteration 169 : 0.7530074875717223
Loss in iteration 170 : 0.8035047246385466
Loss in iteration 171 : 0.7814851708637552
Loss in iteration 172 : 0.7536701784838306
Loss in iteration 173 : 0.7602196451129893
Loss in iteration 174 : 0.7536030963766153
Loss in iteration 175 : 0.8070724238520751
Loss in iteration 176 : 0.7535244972448364
Loss in iteration 177 : 0.7721871836181541
Loss in iteration 178 : 0.7849824723777173
Loss in iteration 179 : 0.813098977313165
Loss in iteration 180 : 0.7789469959614123
Loss in iteration 181 : 0.7926896836458287
Loss in iteration 182 : 0.7753447445212694
Loss in iteration 183 : 0.8004353740861139
Loss in iteration 184 : 0.7377714371728825
Loss in iteration 185 : 0.726559420242103
Loss in iteration 186 : 0.7153621716628934
Loss in iteration 187 : 0.7203025835125335
Loss in iteration 188 : 0.7405707943195685
Loss in iteration 189 : 0.7748211766972741
Loss in iteration 190 : 0.765465256990686
Loss in iteration 191 : 0.7683412254288303
Loss in iteration 192 : 0.8197219912927021
Loss in iteration 193 : 0.8376853513842161
Loss in iteration 194 : 0.7945346855216324
Loss in iteration 195 : 0.7923986881667956
Loss in iteration 196 : 0.7889729622215053
Loss in iteration 197 : 0.7545718697554187
Loss in iteration 198 : 0.7430206618180546
Loss in iteration 199 : 0.7267271273533622
Loss in iteration 200 : 0.6927296333597455
Loss in iteration 201 : 0.7676942629124539
Loss in iteration 202 : 0.8182114887881241
Loss in iteration 203 : 0.8049338440357549
Loss in iteration 204 : 0.7687748072448868
Loss in iteration 205 : 0.8143265512708766
Loss in iteration 206 : 0.8071964362706316
Loss in iteration 207 : 0.7653940428263115
Loss in iteration 208 : 0.7629176011386117
Loss in iteration 209 : 0.7516615640343559
Loss in iteration 210 : 0.740433615489814
Loss in iteration 211 : 0.7474438378293804
Loss in iteration 212 : 0.7308480345283966
Loss in iteration 213 : 0.7352629717116791
Loss in iteration 214 : 0.7455166906241483
Loss in iteration 215 : 0.7909249732911965
Loss in iteration 216 : 0.8046238888152365
Loss in iteration 217 : 0.8051265585977018
Loss in iteration 218 : 0.7292344278281755
Loss in iteration 219 : 0.7799853706118642
Loss in iteration 220 : 0.765026087241498
Loss in iteration 221 : 0.7674799060443653
Loss in iteration 222 : 0.7067533259297589
Loss in iteration 223 : 0.7637853158721202
Loss in iteration 224 : 0.832739265535834
Loss in iteration 225 : 0.841151470815909
Loss in iteration 226 : 0.7803182185771456
Loss in iteration 227 : 0.7417542619475451
Loss in iteration 228 : 0.7325678020641939
Loss in iteration 229 : 0.7414187284910457
Loss in iteration 230 : 0.7442398612626333
Loss in iteration 231 : 0.7823850434605688
Loss in iteration 232 : 0.767647203427338
Loss in iteration 233 : 0.8029210873085786
Loss in iteration 234 : 0.8116022128608262
Loss in iteration 235 : 0.7564904689650894
Loss in iteration 236 : 0.7308965779675793
Loss in iteration 237 : 0.7284692363299252
Loss in iteration 238 : 0.7013460944214989
Loss in iteration 239 : 0.7272905182790546
Loss in iteration 240 : 0.7055575061641837
Loss in iteration 241 : 0.7812203061515558
Loss in iteration 242 : 0.8340313828572127
Loss in iteration 243 : 0.8515318413185533
Loss in iteration 244 : 0.8365716526473955
Loss in iteration 245 : 0.8430609809290233
Loss in iteration 246 : 0.7613048835575597
Loss in iteration 247 : 0.7164003958736883
Loss in iteration 248 : 0.7363062019218034
Loss in iteration 249 : 0.7422717148358481
Loss in iteration 250 : 0.737698096469042
Loss in iteration 251 : 0.7815352883855302
Loss in iteration 252 : 0.757875040695661
Loss in iteration 253 : 0.8135076822420929
Loss in iteration 254 : 0.7248175660228933
Loss in iteration 255 : 0.7629785681361851
Loss in iteration 256 : 0.8108340187450043
Loss in iteration 257 : 0.7738232757538132
Loss in iteration 258 : 0.7558183339494868
Loss in iteration 259 : 0.7539074924409139
Loss in iteration 260 : 0.7550077449203827
Loss in iteration 261 : 0.7708954851889617
Loss in iteration 262 : 0.7888905306397621
Loss in iteration 263 : 0.7852336340046029
Loss in iteration 264 : 0.7856333352776403
Loss in iteration 265 : 0.7444334507092147
Loss in iteration 266 : 0.7026580824303589
Loss in iteration 267 : 0.6944677346342648
Loss in iteration 268 : 0.7347556147690559
Loss in iteration 269 : 0.761809011894845
Loss in iteration 270 : 0.7916853183706061
Loss in iteration 271 : 0.8355417936537911
Loss in iteration 272 : 0.8524743980086469
Loss in iteration 273 : 0.7997125084210797
Loss in iteration 274 : 0.7452939462033831
Loss in iteration 275 : 0.7427841945637789
Loss in iteration 276 : 0.7381455510748771
Loss in iteration 277 : 0.7537294880654603
Loss in iteration 278 : 0.7417195916326842
Loss in iteration 279 : 0.7541627364436789
Loss in iteration 280 : 0.7463268185850939
Loss in iteration 281 : 0.7356494553321092
Loss in iteration 282 : 0.7325932063080461
Loss in iteration 283 : 0.7712807506071293
Loss in iteration 284 : 0.758151155438985
Loss in iteration 285 : 0.8108143035796486
Loss in iteration 286 : 0.7786566669634071
Loss in iteration 287 : 0.7625848764096563
Loss in iteration 288 : 0.734612343672346
Loss in iteration 289 : 0.6958531223579992
Loss in iteration 290 : 0.6701100128263879
Loss in iteration 291 : 0.722657560373825
Loss in iteration 292 : 0.7211704136767599
Loss in iteration 293 : 0.7862333027580387
Loss in iteration 294 : 0.8089117350706797
Loss in iteration 295 : 0.8117180672601509
Loss in iteration 296 : 0.838333212219439
Loss in iteration 297 : 0.8438812746017793
Loss in iteration 298 : 0.8033566999083769
Loss in iteration 299 : 0.7776255133115011
Loss in iteration 300 : 0.7813092381626463
Loss in iteration 301 : 0.7803030400400919
Loss in iteration 302 : 0.7266736706871596
Loss in iteration 303 : 0.7155574645267848
Loss in iteration 304 : 0.7327498976502034
Loss in iteration 305 : 0.7344050186265146
Loss in iteration 306 : 0.6950756155661618
Loss in iteration 307 : 0.7377260181677366
Loss in iteration 308 : 0.7713679073663057
Loss in iteration 309 : 0.7897030002928107
Loss in iteration 310 : 0.8005731839601582
Loss in iteration 311 : 0.817404452666887
Loss in iteration 312 : 0.8127847471672769
Loss in iteration 313 : 0.7475310672153068
Loss in iteration 314 : 0.7163747534783765
Loss in iteration 315 : 0.7232263668887865
Loss in iteration 316 : 0.7444506906202064
Loss in iteration 317 : 0.7428153891023516
Loss in iteration 318 : 0.7940383402513971
Loss in iteration 319 : 0.7772632807968519
Loss in iteration 320 : 0.7868444382586988
Loss in iteration 321 : 0.8209271978308491
Loss in iteration 322 : 0.7732431272972273
Loss in iteration 323 : 0.7438016552842137
Loss in iteration 324 : 0.725052166068938
Loss in iteration 325 : 0.7253609468369869
Loss in iteration 326 : 0.7814777495840495
Loss in iteration 327 : 0.7477447323956966
Loss in iteration 328 : 0.7002426363615323
Loss in iteration 329 : 0.7476383588377282
Loss in iteration 330 : 0.7742380168050785
Loss in iteration 331 : 0.7936014270491357
Loss in iteration 332 : 0.7311096869143813
Loss in iteration 333 : 0.7506473995136371
Loss in iteration 334 : 0.7623754642372939
Loss in iteration 335 : 0.779095456037765
Loss in iteration 336 : 0.7432866481435192
Loss in iteration 337 : 0.7915156468267989
Loss in iteration 338 : 0.7966807454104906
Loss in iteration 339 : 0.7861009622733336
Loss in iteration 340 : 0.7253313181218424
Loss in iteration 341 : 0.7144442532816611
Loss in iteration 342 : 0.7109738245763819
Loss in iteration 343 : 0.7548257368433331
Loss in iteration 344 : 0.7694951391008533
Loss in iteration 345 : 0.79059795898255
Loss in iteration 346 : 0.7868519480120039
Loss in iteration 347 : 0.7656090952960969
Loss in iteration 348 : 0.793893221728375
Loss in iteration 349 : 0.8131174130961027
Loss in iteration 350 : 0.805069873868022
Loss in iteration 351 : 0.7711614781562279
Loss in iteration 352 : 0.7102801803039555
Loss in iteration 353 : 0.6729195815308632
Loss in iteration 354 : 0.7026662522878739
Loss in iteration 355 : 0.7356978700989085
Loss in iteration 356 : 0.7473881902460084
Loss in iteration 357 : 0.7837374653599908
Loss in iteration 358 : 0.7478831496587491
Loss in iteration 359 : 0.7348763856144934
Loss in iteration 360 : 0.8149929975431639
Loss in iteration 361 : 0.7905020772489011
Loss in iteration 362 : 0.7482905539406028
Loss in iteration 363 : 0.7633511290715681
Loss in iteration 364 : 0.7650520106162962
Loss in iteration 365 : 0.8062402166986388
Loss in iteration 366 : 0.7912192565454733
Loss in iteration 367 : 0.7944448351251636
Loss in iteration 368 : 0.7325416999932941
Loss in iteration 369 : 0.7507783208969049
Loss in iteration 370 : 0.7510674286400953
Loss in iteration 371 : 0.8009781814051535
Loss in iteration 372 : 0.7551224598626257
Loss in iteration 373 : 0.7952001990416683
Loss in iteration 374 : 0.8162081334570332
Loss in iteration 375 : 0.7781237352785225
Loss in iteration 376 : 0.7715201591970118
Loss in iteration 377 : 0.7473556657215733
Loss in iteration 378 : 0.7170670649536196
Loss in iteration 379 : 0.7232120572558758
Loss in iteration 380 : 0.7394449032610744
Loss in iteration 381 : 0.7537761817257429
Loss in iteration 382 : 0.7288040742042805
Loss in iteration 383 : 0.780384376569582
Loss in iteration 384 : 0.7805137526543213
Loss in iteration 385 : 0.7532019961810712
Loss in iteration 386 : 0.7450780016987514
Loss in iteration 387 : 0.794252600963447
Loss in iteration 388 : 0.7721952788228597
Loss in iteration 389 : 0.7207185984332138
Loss in iteration 390 : 0.6983634665697496
Loss in iteration 391 : 0.7459738521032564
Loss in iteration 392 : 0.7458571647449707
Loss in iteration 393 : 0.7706988439977089
Loss in iteration 394 : 0.7598630328002876
Loss in iteration 395 : 0.788955290669854
Loss in iteration 396 : 0.7719152396967702
Loss in iteration 397 : 0.7768998282801393
Loss in iteration 398 : 0.7564190843638383
Loss in iteration 399 : 0.7466715967471259
Loss in iteration 400 : 0.7177947183200181
Testing accuracy  of updater 5 on alg 0 with rate 0.4 = 0.73725, training accuracy 0.73725, time elapsed: 5095 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6765346151674537
Loss in iteration 3 : 0.6617205973776359
Loss in iteration 4 : 0.6563507070452513
Loss in iteration 5 : 0.6438121563900288
Loss in iteration 6 : 0.6400283863655174
Loss in iteration 7 : 0.6325318429697622
Loss in iteration 8 : 0.6257605716460645
Loss in iteration 9 : 0.6175243917633008
Loss in iteration 10 : 0.6155379491697873
Loss in iteration 11 : 0.6124704630264166
Loss in iteration 12 : 0.6152045526155268
Loss in iteration 13 : 0.6044215628819912
Loss in iteration 14 : 0.605262653358135
Loss in iteration 15 : 0.6037292140688975
Loss in iteration 16 : 0.6047895840034301
Loss in iteration 17 : 0.5963049635873527
Loss in iteration 18 : 0.5962302471966419
Loss in iteration 19 : 0.5806912105362344
Loss in iteration 20 : 0.5773446350056786
Loss in iteration 21 : 0.5800639102533868
Loss in iteration 22 : 0.5743532147434921
Loss in iteration 23 : 0.5663232684033489
Loss in iteration 24 : 0.5561714557198804
Loss in iteration 25 : 0.5569096278620276
Loss in iteration 26 : 0.5459142871512964
Loss in iteration 27 : 0.5423710870108478
Loss in iteration 28 : 0.5507865790732335
Loss in iteration 29 : 0.5603581966012561
Loss in iteration 30 : 0.5704952767364294
Loss in iteration 31 : 0.573413712098635
Loss in iteration 32 : 0.5786318433576624
Loss in iteration 33 : 0.5538362057768816
Loss in iteration 34 : 0.5492687047584018
Loss in iteration 35 : 0.5478225934744224
Loss in iteration 36 : 0.5417063318156736
Loss in iteration 37 : 0.5334177539756659
Loss in iteration 38 : 0.536388768637232
Loss in iteration 39 : 0.5351475572133565
Loss in iteration 40 : 0.5389697563266921
Loss in iteration 41 : 0.5365790323582662
Loss in iteration 42 : 0.5265531368294369
Loss in iteration 43 : 0.5300111168989861
Loss in iteration 44 : 0.5291447497400216
Loss in iteration 45 : 0.5281634369040871
Loss in iteration 46 : 0.5355651704257683
Loss in iteration 47 : 0.5415278874267304
Loss in iteration 48 : 0.5486481645638059
Loss in iteration 49 : 0.5305596021933756
Loss in iteration 50 : 0.5261249970600178
Loss in iteration 51 : 0.5341094851846877
Loss in iteration 52 : 0.5283715143301974
Loss in iteration 53 : 0.5267100977897126
Loss in iteration 54 : 0.5237600281884148
Loss in iteration 55 : 0.520873463301752
Loss in iteration 56 : 0.5263086979957974
Loss in iteration 57 : 0.5116641507118447
Loss in iteration 58 : 0.5166102490994926
Loss in iteration 59 : 0.5149974161815124
Loss in iteration 60 : 0.5167094196098794
Loss in iteration 61 : 0.5127062643093158
Loss in iteration 62 : 0.511651035751577
Loss in iteration 63 : 0.5090088582184771
Loss in iteration 64 : 0.5105235523193399
Loss in iteration 65 : 0.5223719806107562
Loss in iteration 66 : 0.5118914563374775
Loss in iteration 67 : 0.515568443397057
Loss in iteration 68 : 0.5069196997252947
Loss in iteration 69 : 0.5120216199745039
Loss in iteration 70 : 0.5042902243977823
Loss in iteration 71 : 0.52229371893059
Loss in iteration 72 : 0.5158016519801475
Loss in iteration 73 : 0.5162972645139676
Loss in iteration 74 : 0.5121080706878643
Loss in iteration 75 : 0.5099542424184649
Loss in iteration 76 : 0.5192001960739692
Loss in iteration 77 : 0.500648549111615
Loss in iteration 78 : 0.5027300413664136
Loss in iteration 79 : 0.4829975113823796
Loss in iteration 80 : 0.49523658905774764
Loss in iteration 81 : 0.5006839785671546
Loss in iteration 82 : 0.5043662301640923
Loss in iteration 83 : 0.5171501082398768
Loss in iteration 84 : 0.5354543311922875
Loss in iteration 85 : 0.5214633775574079
Loss in iteration 86 : 0.5240818670515791
Loss in iteration 87 : 0.5073525125429689
Loss in iteration 88 : 0.5277103842680353
Loss in iteration 89 : 0.5088210853627368
Loss in iteration 90 : 0.5067463376784012
Loss in iteration 91 : 0.5046051361067065
Loss in iteration 92 : 0.49039681367867916
Loss in iteration 93 : 0.49124610367761706
Loss in iteration 94 : 0.4908417539811194
Loss in iteration 95 : 0.4857980071552906
Loss in iteration 96 : 0.5029558063096813
Loss in iteration 97 : 0.50666733470165
Loss in iteration 98 : 0.5274359823567731
Loss in iteration 99 : 0.5258345886857553
Loss in iteration 100 : 0.5252376179528677
Loss in iteration 101 : 0.5048917445296414
Loss in iteration 102 : 0.49827394419126925
Loss in iteration 103 : 0.5024761594523606
Loss in iteration 104 : 0.49975498506273525
Loss in iteration 105 : 0.4868526768367887
Loss in iteration 106 : 0.4981964582992269
Loss in iteration 107 : 0.508671923342209
Loss in iteration 108 : 0.5219898367333309
Loss in iteration 109 : 0.5104319462357677
Loss in iteration 110 : 0.5097080304711082
Loss in iteration 111 : 0.5069147700007831
Loss in iteration 112 : 0.4836097368533858
Loss in iteration 113 : 0.47420743178017893
Loss in iteration 114 : 0.48034731706989764
Loss in iteration 115 : 0.4849005082669183
Loss in iteration 116 : 0.49826373963717935
Loss in iteration 117 : 0.5158612358298723
Loss in iteration 118 : 0.5141087778315631
Loss in iteration 119 : 0.4962584238747312
Loss in iteration 120 : 0.5166476412564212
Loss in iteration 121 : 0.5137002283898495
Loss in iteration 122 : 0.5114517096915813
Loss in iteration 123 : 0.4915670812723037
Loss in iteration 124 : 0.48591963376837083
Loss in iteration 125 : 0.4876784076517301
Loss in iteration 126 : 0.49218963322817366
Loss in iteration 127 : 0.4975779523593168
Loss in iteration 128 : 0.5005566652722222
Loss in iteration 129 : 0.49209450875269267
Loss in iteration 130 : 0.5000461301755494
Loss in iteration 131 : 0.49176292120925874
Loss in iteration 132 : 0.48912148730760935
Loss in iteration 133 : 0.49491705168289424
Loss in iteration 134 : 0.5011030900061603
Loss in iteration 135 : 0.5006573591028347
Loss in iteration 136 : 0.498079737161291
Loss in iteration 137 : 0.4999294060952168
Loss in iteration 138 : 0.5031268458011763
Loss in iteration 139 : 0.5102234259476663
Loss in iteration 140 : 0.499591292128325
Loss in iteration 141 : 0.4954658555048086
Loss in iteration 142 : 0.4905820676719895
Loss in iteration 143 : 0.499907917056367
Loss in iteration 144 : 0.4946467062309445
Loss in iteration 145 : 0.4836675804674608
Loss in iteration 146 : 0.4910154597577648
Loss in iteration 147 : 0.4925627836881805
Loss in iteration 148 : 0.4848256866217213
Loss in iteration 149 : 0.49871881677702723
Loss in iteration 150 : 0.4950725148554629
Loss in iteration 151 : 0.49960369375972696
Loss in iteration 152 : 0.5193018272788983
Loss in iteration 153 : 0.5014289465643007
Loss in iteration 154 : 0.5191996074779978
Loss in iteration 155 : 0.502906568583762
Loss in iteration 156 : 0.48356091859738803
Loss in iteration 157 : 0.5019988154139777
Loss in iteration 158 : 0.4955423242017293
Loss in iteration 159 : 0.49253407234299695
Loss in iteration 160 : 0.48536644205545126
Loss in iteration 161 : 0.4808133179729904
Loss in iteration 162 : 0.48091431159098
Loss in iteration 163 : 0.48566123146043383
Loss in iteration 164 : 0.49085027121279834
Loss in iteration 165 : 0.49222545889586294
Loss in iteration 166 : 0.497632915008875
Loss in iteration 167 : 0.499971317119049
Loss in iteration 168 : 0.4985539231676147
Loss in iteration 169 : 0.4888222823306073
Loss in iteration 170 : 0.4993586453424069
Loss in iteration 171 : 0.4875674357757213
Loss in iteration 172 : 0.49708567529105896
Loss in iteration 173 : 0.4920040903839469
Loss in iteration 174 : 0.4909331210740249
Loss in iteration 175 : 0.5083203791106272
Loss in iteration 176 : 0.4895581746053075
Loss in iteration 177 : 0.4978277484080715
Loss in iteration 178 : 0.4921405878583455
Loss in iteration 179 : 0.5010255930394575
Loss in iteration 180 : 0.4892341452967928
Loss in iteration 181 : 0.4967809635380991
Loss in iteration 182 : 0.4963563990649845
Loss in iteration 183 : 0.4930586568762022
Loss in iteration 184 : 0.4995818053167563
Loss in iteration 185 : 0.4820933569883478
Loss in iteration 186 : 0.47538454286526993
Loss in iteration 187 : 0.4739411378916733
Loss in iteration 188 : 0.4849462967447846
Loss in iteration 189 : 0.4866512118681329
Loss in iteration 190 : 0.4784165316437416
Loss in iteration 191 : 0.4896937463629585
Loss in iteration 192 : 0.5058672435329923
Loss in iteration 193 : 0.5170253868533988
Loss in iteration 194 : 0.5146206977869923
Loss in iteration 195 : 0.5087114376858705
Loss in iteration 196 : 0.5090565231759302
Loss in iteration 197 : 0.477649305555711
Loss in iteration 198 : 0.49044831161783997
Loss in iteration 199 : 0.47481047798869824
Loss in iteration 200 : 0.4657360439658157
Loss in iteration 201 : 0.47634285610752186
Loss in iteration 202 : 0.4920223717306575
Loss in iteration 203 : 0.4817812971578197
Loss in iteration 204 : 0.5003711346699632
Loss in iteration 205 : 0.518204516242462
Loss in iteration 206 : 0.5195956993385908
Loss in iteration 207 : 0.4983822058631856
Loss in iteration 208 : 0.48849158885950583
Loss in iteration 209 : 0.4885632806169777
Loss in iteration 210 : 0.4876494427902472
Loss in iteration 211 : 0.47875404938532445
Loss in iteration 212 : 0.48116566577944997
Loss in iteration 213 : 0.47953833492482617
Loss in iteration 214 : 0.49171654439662643
Loss in iteration 215 : 0.48509036875816697
Loss in iteration 216 : 0.48443588746124544
Loss in iteration 217 : 0.48736654636003934
Loss in iteration 218 : 0.484996375403236
Loss in iteration 219 : 0.508279554751823
Loss in iteration 220 : 0.5184030268009536
Loss in iteration 221 : 0.5047848214167353
Loss in iteration 222 : 0.48617087492961775
Loss in iteration 223 : 0.49910253512018254
Loss in iteration 224 : 0.5057134544770409
Loss in iteration 225 : 0.49318564013692134
Loss in iteration 226 : 0.48633454882785454
Loss in iteration 227 : 0.46961062165316403
Loss in iteration 228 : 0.4792745970293525
Loss in iteration 229 : 0.4800019104603225
Loss in iteration 230 : 0.4774165035140656
Loss in iteration 231 : 0.49049875466839316
Loss in iteration 232 : 0.4973519455729601
Loss in iteration 233 : 0.5043274551060425
Loss in iteration 234 : 0.5162054395930171
Loss in iteration 235 : 0.49107393468919597
Loss in iteration 236 : 0.4859716844788453
Loss in iteration 237 : 0.4785336892087122
Loss in iteration 238 : 0.4719802132550803
Loss in iteration 239 : 0.4688132153284277
Loss in iteration 240 : 0.4629612923163341
Loss in iteration 241 : 0.47222239441996827
Loss in iteration 242 : 0.5081412647518757
Loss in iteration 243 : 0.5080716930628839
Loss in iteration 244 : 0.5156712586254676
Loss in iteration 245 : 0.5350716029297493
Loss in iteration 246 : 0.5068939732475
Loss in iteration 247 : 0.48419508919260645
Loss in iteration 248 : 0.48698465355931353
Loss in iteration 249 : 0.484247313497971
Loss in iteration 250 : 0.47236895262717493
Loss in iteration 251 : 0.48335900373636587
Loss in iteration 252 : 0.4778541882832891
Loss in iteration 253 : 0.491687057933267
Loss in iteration 254 : 0.47024239430431164
Loss in iteration 255 : 0.48223010854499637
Loss in iteration 256 : 0.5130418677466453
Loss in iteration 257 : 0.490013485647348
Loss in iteration 258 : 0.4968587780639322
Loss in iteration 259 : 0.48792027495085577
Loss in iteration 260 : 0.4882455713827815
Loss in iteration 261 : 0.4774355946345684
Loss in iteration 262 : 0.4888865797731492
Loss in iteration 263 : 0.4892736893505831
Loss in iteration 264 : 0.5023146448632926
Loss in iteration 265 : 0.4774457356732234
Loss in iteration 266 : 0.4865688712010804
Loss in iteration 267 : 0.478318497544491
Loss in iteration 268 : 0.4852328351803347
Loss in iteration 269 : 0.4719939873588958
Loss in iteration 270 : 0.48975603501905973
Loss in iteration 271 : 0.4924166857967553
Loss in iteration 272 : 0.5149263942452641
Loss in iteration 273 : 0.49350792799912546
Loss in iteration 274 : 0.4925344943880081
Loss in iteration 275 : 0.4911030712304993
Loss in iteration 276 : 0.4935278501985909
Loss in iteration 277 : 0.4818048947997595
Loss in iteration 278 : 0.47877890150064467
Loss in iteration 279 : 0.4751224810907946
Loss in iteration 280 : 0.4792141844045524
Loss in iteration 281 : 0.4820999985079411
Loss in iteration 282 : 0.4746437580855077
Loss in iteration 283 : 0.4851616187924952
Loss in iteration 284 : 0.4773621710553883
Loss in iteration 285 : 0.48910027615176566
Loss in iteration 286 : 0.49721347846975605
Loss in iteration 287 : 0.4939244459361614
Loss in iteration 288 : 0.48750352013290055
Loss in iteration 289 : 0.47075157859480454
Loss in iteration 290 : 0.462040758063024
Loss in iteration 291 : 0.47562273530265786
Loss in iteration 292 : 0.47539025452500927
Loss in iteration 293 : 0.47935564684928844
Loss in iteration 294 : 0.4997796957179238
Loss in iteration 295 : 0.4943627118378469
Loss in iteration 296 : 0.5119863601095325
Loss in iteration 297 : 0.5193679634609218
Loss in iteration 298 : 0.5006298908966588
Loss in iteration 299 : 0.4899622484770382
Loss in iteration 300 : 0.49012927878671214
Loss in iteration 301 : 0.4872734656073005
Loss in iteration 302 : 0.47929146546004636
Loss in iteration 303 : 0.4708920438363247
Loss in iteration 304 : 0.47567216707909393
Loss in iteration 305 : 0.4742121675075939
Loss in iteration 306 : 0.47009485534050616
Loss in iteration 307 : 0.4745231704390939
Loss in iteration 308 : 0.4841586637452975
Loss in iteration 309 : 0.4834349006507346
Loss in iteration 310 : 0.5008068701064381
Loss in iteration 311 : 0.5138105008806516
Loss in iteration 312 : 0.5224957437801723
Loss in iteration 313 : 0.4805693067186383
Loss in iteration 314 : 0.48033801922696423
Loss in iteration 315 : 0.4758696371749062
Loss in iteration 316 : 0.4900777555003436
Loss in iteration 317 : 0.4759678762437269
Loss in iteration 318 : 0.4921236928662928
Loss in iteration 319 : 0.4775315802722152
Loss in iteration 320 : 0.48539037881175784
Loss in iteration 321 : 0.49531457721345123
Loss in iteration 322 : 0.5000832357958304
Loss in iteration 323 : 0.488559367672307
Loss in iteration 324 : 0.48416785022765485
Loss in iteration 325 : 0.47627880768027026
Loss in iteration 326 : 0.49421369286740424
Loss in iteration 327 : 0.471746342293093
Loss in iteration 328 : 0.4721611543110092
Loss in iteration 329 : 0.4773852806111477
Loss in iteration 330 : 0.48091726255979717
Loss in iteration 331 : 0.4892553666224634
Loss in iteration 332 : 0.47486534433369054
Loss in iteration 333 : 0.476529549557598
Loss in iteration 334 : 0.48847756644839446
Loss in iteration 335 : 0.4841528575839771
Loss in iteration 336 : 0.4881527593204168
Loss in iteration 337 : 0.4959945395355883
Loss in iteration 338 : 0.5024501231301146
Loss in iteration 339 : 0.4969063214030247
Loss in iteration 340 : 0.4824564242681314
Loss in iteration 341 : 0.4702973360660561
Loss in iteration 342 : 0.47214544868004854
Loss in iteration 343 : 0.47419392874412014
Loss in iteration 344 : 0.4851928074736448
Loss in iteration 345 : 0.48631100391320564
Loss in iteration 346 : 0.4904952277494526
Loss in iteration 347 : 0.49720817835582476
Loss in iteration 348 : 0.506216970381757
Loss in iteration 349 : 0.5089806773099113
Loss in iteration 350 : 0.5061647882027511
Loss in iteration 351 : 0.4905086531464207
Loss in iteration 352 : 0.4769049216810423
Loss in iteration 353 : 0.4638472872547554
Loss in iteration 354 : 0.4713154122280388
Loss in iteration 355 : 0.4722138357667259
Loss in iteration 356 : 0.4662362889983287
Loss in iteration 357 : 0.4731530329063024
Loss in iteration 358 : 0.4828117899630846
Loss in iteration 359 : 0.4751135171547315
Loss in iteration 360 : 0.4968286931738953
Loss in iteration 361 : 0.49146546081532044
Loss in iteration 362 : 0.5154395592750642
Loss in iteration 363 : 0.5061505074916521
Loss in iteration 364 : 0.497225102313706
Loss in iteration 365 : 0.49234610782789684
Loss in iteration 366 : 0.4834411815427923
Loss in iteration 367 : 0.4865694790349296
Loss in iteration 368 : 0.4780413053953384
Loss in iteration 369 : 0.48209865713466205
Loss in iteration 370 : 0.47060727394866786
Loss in iteration 371 : 0.4926492497899497
Loss in iteration 372 : 0.4772556834761559
Loss in iteration 373 : 0.4951761921779829
Loss in iteration 374 : 0.5089957915878042
Loss in iteration 375 : 0.5037360132873676
Loss in iteration 376 : 0.5040451627579898
Loss in iteration 377 : 0.4855898502338313
Loss in iteration 378 : 0.4820238857222536
Loss in iteration 379 : 0.4779384954515409
Loss in iteration 380 : 0.4715470356170932
Loss in iteration 381 : 0.4638343348005109
Loss in iteration 382 : 0.4725020611416089
Loss in iteration 383 : 0.4869121466028535
Loss in iteration 384 : 0.4991004739878055
Loss in iteration 385 : 0.4890074402056689
Loss in iteration 386 : 0.4788000668277414
Loss in iteration 387 : 0.49732715157665974
Loss in iteration 388 : 0.499941746144508
Loss in iteration 389 : 0.4772568389783826
Loss in iteration 390 : 0.47367196213563206
Loss in iteration 391 : 0.47835848366670575
Loss in iteration 392 : 0.4717753095740887
Loss in iteration 393 : 0.4790432669475054
Loss in iteration 394 : 0.4859649797797181
Loss in iteration 395 : 0.4975481198764321
Loss in iteration 396 : 0.4927687260847432
Loss in iteration 397 : 0.5000011171227668
Loss in iteration 398 : 0.49122505930813043
Loss in iteration 399 : 0.47450212377193146
Loss in iteration 400 : 0.466130465631158
Testing accuracy  of updater 5 on alg 0 with rate 0.09999999999999998 = 0.780125, training accuracy 0.780125, time elapsed: 5946 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 4.5042166849363845
Loss in iteration 3 : 1.4440754588390716
Loss in iteration 4 : 2.0838307083794687
Loss in iteration 5 : 0.9664283267468418
Loss in iteration 6 : 1.353580868280173
Loss in iteration 7 : 0.9250611567849766
Loss in iteration 8 : 1.245627374954869
Loss in iteration 9 : 1.034485359063487
Loss in iteration 10 : 0.6814225760643117
Loss in iteration 11 : 0.9720794923686802
Loss in iteration 12 : 0.781047848176317
Loss in iteration 13 : 0.6396447928764439
Loss in iteration 14 : 0.8490867965007425
Loss in iteration 15 : 0.7252788975538943
Loss in iteration 16 : 0.7773842724357265
Loss in iteration 17 : 0.7952155582991942
Loss in iteration 18 : 0.6173420813017738
Loss in iteration 19 : 0.6883838290077969
Loss in iteration 20 : 0.7246376368231476
Loss in iteration 21 : 0.6680671759801587
Loss in iteration 22 : 0.6470134397823787
Loss in iteration 23 : 0.6712050692432483
Loss in iteration 24 : 0.629653360817343
Loss in iteration 25 : 0.6422412996670649
Loss in iteration 26 : 0.6022330141812413
Loss in iteration 27 : 0.5427620664847024
Loss in iteration 28 : 0.5938265305343544
Loss in iteration 29 : 0.5647743079527603
Loss in iteration 30 : 0.5800271047641073
Loss in iteration 31 : 0.5710856980289832
Loss in iteration 32 : 0.5204189463849954
Loss in iteration 33 : 0.5490833183129343
Loss in iteration 34 : 0.498249676043972
Loss in iteration 35 : 0.5368920574618784
Loss in iteration 36 : 0.5141472962072625
Loss in iteration 37 : 0.5209625939032848
Loss in iteration 38 : 0.5176023684551829
Loss in iteration 39 : 0.49374715472204533
Loss in iteration 40 : 0.5099335841883696
Loss in iteration 41 : 0.5043422445188597
Loss in iteration 42 : 0.4923752594760098
Loss in iteration 43 : 0.48689744499139787
Loss in iteration 44 : 0.49471303681169204
Loss in iteration 45 : 0.49539418445680405
Loss in iteration 46 : 0.49806503072058056
Loss in iteration 47 : 0.4909466470801402
Loss in iteration 48 : 0.4809623952664559
Loss in iteration 49 : 0.5083491302139388
Loss in iteration 50 : 0.45848964123961333
Loss in iteration 51 : 0.5042639396591779
Loss in iteration 52 : 0.47887977567116197
Loss in iteration 53 : 0.4740715928268708
Loss in iteration 54 : 0.4691372599112791
Loss in iteration 55 : 0.4740816907082729
Loss in iteration 56 : 0.48307225782857793
Loss in iteration 57 : 0.45692945548200214
Loss in iteration 58 : 0.4620803306532943
Loss in iteration 59 : 0.45572432927721146
Loss in iteration 60 : 0.4683967564486006
Loss in iteration 61 : 0.4638819076504041
Loss in iteration 62 : 0.4656297031341967
Loss in iteration 63 : 0.46529594794575735
Loss in iteration 64 : 0.46124444912493545
Loss in iteration 65 : 0.48063028467320623
Loss in iteration 66 : 0.4562072756882424
Loss in iteration 67 : 0.47010126544692743
Loss in iteration 68 : 0.46217083878886916
Loss in iteration 69 : 0.46819537149356205
Loss in iteration 70 : 0.455119101716563
Loss in iteration 71 : 0.47139935441948816
Loss in iteration 72 : 0.45605199922000433
Loss in iteration 73 : 0.4664416147278495
Loss in iteration 74 : 0.4609764029553599
Loss in iteration 75 : 0.4615932769658553
Loss in iteration 76 : 0.47195862852393766
Loss in iteration 77 : 0.45352890608541574
Loss in iteration 78 : 0.46431761615254474
Loss in iteration 79 : 0.4494887288181635
Loss in iteration 80 : 0.4700790050807098
Loss in iteration 81 : 0.46697596644966705
Loss in iteration 82 : 0.46046230426269913
Loss in iteration 83 : 0.4696621606140095
Loss in iteration 84 : 0.480179335134517
Loss in iteration 85 : 0.46210935793586866
Loss in iteration 86 : 0.4794666308839175
Loss in iteration 87 : 0.463861175834155
Loss in iteration 88 : 0.4709069098155391
Loss in iteration 89 : 0.4626558516367957
Loss in iteration 90 : 0.48004664884933507
Loss in iteration 91 : 0.4751340293632291
Loss in iteration 92 : 0.45481316861098775
Loss in iteration 93 : 0.48605394219885883
Loss in iteration 94 : 0.48356392105868384
Loss in iteration 95 : 0.47400911767028675
Loss in iteration 96 : 0.46846542013391423
Loss in iteration 97 : 0.4830508532520864
Loss in iteration 98 : 0.5083575527338834
Loss in iteration 99 : 0.4733674350523314
Loss in iteration 100 : 0.46384358688239785
Loss in iteration 101 : 0.47895150358243954
Loss in iteration 102 : 0.4765884897647019
Loss in iteration 103 : 0.46650116940206593
Loss in iteration 104 : 0.47800797577596715
Loss in iteration 105 : 0.47702157071058754
Loss in iteration 106 : 0.4765422318039783
Loss in iteration 107 : 0.477793515134492
Loss in iteration 108 : 0.48589909536974246
Loss in iteration 109 : 0.4676971441166626
Loss in iteration 110 : 0.45523598981139124
Loss in iteration 111 : 0.4672668125217335
Loss in iteration 112 : 0.4610923097690389
Loss in iteration 113 : 0.4552193668485102
Loss in iteration 114 : 0.46357585770530063
Loss in iteration 115 : 0.46265391765466823
Loss in iteration 116 : 0.46567241359595485
Loss in iteration 117 : 0.48715375611663597
Loss in iteration 118 : 0.4880902270712209
Loss in iteration 119 : 0.4579840866328227
Loss in iteration 120 : 0.4743741189774203
Loss in iteration 121 : 0.474474762280254
Loss in iteration 122 : 0.48505219180890274
Loss in iteration 123 : 0.4861041460643349
Loss in iteration 124 : 0.47388413409884855
Loss in iteration 125 : 0.4680849749367291
Loss in iteration 126 : 0.45606192144346663
Loss in iteration 127 : 0.4611276467845905
Loss in iteration 128 : 0.4869696653747234
Loss in iteration 129 : 0.47968072180809607
Loss in iteration 130 : 0.47475734537922387
Loss in iteration 131 : 0.45792984470406856
Loss in iteration 132 : 0.46877658397595295
Loss in iteration 133 : 0.5005296921262793
Loss in iteration 134 : 0.4859270931671171
Loss in iteration 135 : 0.46361145536472714
Loss in iteration 136 : 0.46301132940911904
Loss in iteration 137 : 0.48084208307197057
Loss in iteration 138 : 0.48516908242275164
Loss in iteration 139 : 0.4897240081036402
Loss in iteration 140 : 0.4773760862749705
Loss in iteration 141 : 0.46865142054142855
Loss in iteration 142 : 0.4630705861684774
Loss in iteration 143 : 0.4795129821883694
Loss in iteration 144 : 0.46701474252346004
Loss in iteration 145 : 0.451573421377393
Loss in iteration 146 : 0.4711923707248652
Loss in iteration 147 : 0.47549772736228774
Loss in iteration 148 : 0.4699523176974733
Loss in iteration 149 : 0.4835055616643525
Loss in iteration 150 : 0.47252826039838747
Loss in iteration 151 : 0.46910583448985854
Loss in iteration 152 : 0.4751698751131608
Loss in iteration 153 : 0.4534242290278753
Loss in iteration 154 : 0.4744220078248456
Loss in iteration 155 : 0.47476262041008216
Loss in iteration 156 : 0.46873575382238214
Loss in iteration 157 : 0.5017208654533675
Loss in iteration 158 : 0.5286017249925985
Loss in iteration 159 : 0.5359924749519253
Loss in iteration 160 : 0.5128747753735582
Loss in iteration 161 : 0.4699351356456598
Loss in iteration 162 : 0.47506638754881275
Loss in iteration 163 : 0.49096682345138687
Loss in iteration 164 : 0.5246379183957413
Loss in iteration 165 : 0.49754634869836234
Loss in iteration 166 : 0.4678718096670881
Loss in iteration 167 : 0.4762252014024052
Loss in iteration 168 : 0.4779321634028491
Loss in iteration 169 : 0.4664051314968464
Loss in iteration 170 : 0.47159210534028134
Loss in iteration 171 : 0.4563133079756507
Loss in iteration 172 : 0.47477006935028887
Loss in iteration 173 : 0.46642404559993117
Loss in iteration 174 : 0.47246430925592575
Loss in iteration 175 : 0.49430408547445215
Loss in iteration 176 : 0.46988610738678244
Loss in iteration 177 : 0.4917303355620193
Loss in iteration 178 : 0.46543273870891905
Loss in iteration 179 : 0.47191245890729433
Loss in iteration 180 : 0.464371083916797
Loss in iteration 181 : 0.4757636493592336
Loss in iteration 182 : 0.49602923993750514
Loss in iteration 183 : 0.5209457160564389
Loss in iteration 184 : 0.5675567109893718
Loss in iteration 185 : 0.5206397666491682
Loss in iteration 186 : 0.4782049596468298
Loss in iteration 187 : 0.4570832536888993
Loss in iteration 188 : 0.47952666700474517
Loss in iteration 189 : 0.5161349021710743
Loss in iteration 190 : 0.5261853658143608
Loss in iteration 191 : 0.5008020076196573
Loss in iteration 192 : 0.46313030807086225
Loss in iteration 193 : 0.4867993801569613
Loss in iteration 194 : 0.5334307180147551
Loss in iteration 195 : 0.564054066339263
Loss in iteration 196 : 0.5455710382836794
Loss in iteration 197 : 0.4662811608379296
Loss in iteration 198 : 0.4849789243107237
Loss in iteration 199 : 0.5068350351191258
Loss in iteration 200 : 0.5345262666667062
Loss in iteration 201 : 0.5162353930570279
Loss in iteration 202 : 0.4818813775195109
Loss in iteration 203 : 0.467954611101052
Loss in iteration 204 : 0.5354715633836257
Loss in iteration 205 : 0.568272295125535
Loss in iteration 206 : 0.5495472380785692
Loss in iteration 207 : 0.4790920522779666
Loss in iteration 208 : 0.4609089385212745
Loss in iteration 209 : 0.494259396314189
Loss in iteration 210 : 0.5209869101075627
Loss in iteration 211 : 0.4991740662085394
Loss in iteration 212 : 0.4847203889637182
Loss in iteration 213 : 0.47261198072319144
Loss in iteration 214 : 0.49095953416529664
Loss in iteration 215 : 0.47879335740093953
Loss in iteration 216 : 0.471230372897398
Loss in iteration 217 : 0.4745233802540192
Loss in iteration 218 : 0.46033650272169874
Loss in iteration 219 : 0.47253442811885377
Loss in iteration 220 : 0.4878399839077901
Loss in iteration 221 : 0.5124623867235736
Loss in iteration 222 : 0.5391720431877275
Loss in iteration 223 : 0.5500820744397082
Loss in iteration 224 : 0.4923269197418199
Loss in iteration 225 : 0.4628004009162433
Loss in iteration 226 : 0.4900974519000731
Loss in iteration 227 : 0.48965921598052087
Loss in iteration 228 : 0.4888070018647276
Loss in iteration 229 : 0.4676242950047635
Loss in iteration 230 : 0.46234143549876017
Loss in iteration 231 : 0.4732757446524455
Loss in iteration 232 : 0.4897238531541025
Loss in iteration 233 : 0.4672128664755267
Loss in iteration 234 : 0.46730889757475047
Loss in iteration 235 : 0.46071429186036056
Loss in iteration 236 : 0.47024297623927463
Loss in iteration 237 : 0.4654156029569784
Loss in iteration 238 : 0.4588149883567313
Loss in iteration 239 : 0.4906339127646929
Loss in iteration 240 : 0.5224075852432667
Loss in iteration 241 : 0.5315523901809638
Loss in iteration 242 : 0.514247648067967
Loss in iteration 243 : 0.4745709418628819
Loss in iteration 244 : 0.46123824966792126
Loss in iteration 245 : 0.4989314840311414
Loss in iteration 246 : 0.523119971928092
Loss in iteration 247 : 0.5214709195738592
Loss in iteration 248 : 0.5210945171363504
Loss in iteration 249 : 0.48731122606484656
Loss in iteration 250 : 0.4556315994275729
Loss in iteration 251 : 0.47713435756243866
Loss in iteration 252 : 0.5014785833838943
Loss in iteration 253 : 0.5334191772549974
Loss in iteration 254 : 0.49333932795708774
Loss in iteration 255 : 0.4695198058079147
Loss in iteration 256 : 0.4768203093203632
Loss in iteration 257 : 0.4590240575731529
Loss in iteration 258 : 0.49168106978726206
Loss in iteration 259 : 0.4983883046092414
Loss in iteration 260 : 0.48987438031095315
Loss in iteration 261 : 0.46281200792748306
Loss in iteration 262 : 0.4627972934965829
Loss in iteration 263 : 0.461325213132306
Loss in iteration 264 : 0.4805936318362884
Loss in iteration 265 : 0.46268211691029204
Loss in iteration 266 : 0.48410228510907793
Loss in iteration 267 : 0.49742811903597367
Loss in iteration 268 : 0.5127291048275476
Loss in iteration 269 : 0.5022232983515563
Loss in iteration 270 : 0.4845224450062116
Loss in iteration 271 : 0.4563344408314958
Loss in iteration 272 : 0.4631937007620478
Loss in iteration 273 : 0.4753383848539389
Loss in iteration 274 : 0.5152389179806423
Loss in iteration 275 : 0.5315733813877268
Loss in iteration 276 : 0.5294464044424414
Loss in iteration 277 : 0.48587332464702326
Loss in iteration 278 : 0.4680923018034824
Loss in iteration 279 : 0.45939404775207243
Loss in iteration 280 : 0.46197031019942364
Loss in iteration 281 : 0.46526651019283854
Loss in iteration 282 : 0.46555434953540376
Loss in iteration 283 : 0.4724874720818575
Loss in iteration 284 : 0.4532080613269401
Loss in iteration 285 : 0.4489808969580492
Loss in iteration 286 : 0.46744805473621337
Loss in iteration 287 : 0.4838654176309546
Loss in iteration 288 : 0.49222901318430234
Loss in iteration 289 : 0.4678680894274442
Loss in iteration 290 : 0.45513805486692577
Loss in iteration 291 : 0.4836184647370341
Loss in iteration 292 : 0.5115779460158169
Loss in iteration 293 : 0.526773339828622
Loss in iteration 294 : 0.5075220947828043
Loss in iteration 295 : 0.4826474930373986
Loss in iteration 296 : 0.45835500521571293
Loss in iteration 297 : 0.47729534556983855
Loss in iteration 298 : 0.49872557226101816
Loss in iteration 299 : 0.5376932284258754
Loss in iteration 300 : 0.5760427349287599
Loss in iteration 301 : 0.5488915072507459
Loss in iteration 302 : 0.5202832928007524
Loss in iteration 303 : 0.45923884272587634
Loss in iteration 304 : 0.48749119304853233
Loss in iteration 305 : 0.5012459648554676
Loss in iteration 306 : 0.5416118421995508
Loss in iteration 307 : 0.521567781669735
Loss in iteration 308 : 0.4878304184841391
Loss in iteration 309 : 0.4720537457185436
Loss in iteration 310 : 0.5687738078387259
Loss in iteration 311 : 0.5758334276678143
Loss in iteration 312 : 0.5504654633989682
Loss in iteration 313 : 0.46357424589986634
Loss in iteration 314 : 0.4846131028116017
Loss in iteration 315 : 0.5436833048551607
Loss in iteration 316 : 0.5486229704206694
Loss in iteration 317 : 0.5142567342190264
Loss in iteration 318 : 0.47726437642999286
Loss in iteration 319 : 0.495152825200347
Loss in iteration 320 : 0.5288402065006891
Loss in iteration 321 : 0.5270316855410742
Loss in iteration 322 : 0.5077954933916564
Loss in iteration 323 : 0.46835761326432573
Loss in iteration 324 : 0.49080624383692023
Loss in iteration 325 : 0.5098056164739955
Loss in iteration 326 : 0.5252312136486893
Loss in iteration 327 : 0.4596248043012539
Loss in iteration 328 : 0.46768949805331683
Loss in iteration 329 : 0.48373875657253895
Loss in iteration 330 : 0.5341596257932657
Loss in iteration 331 : 0.5298400598064474
Loss in iteration 332 : 0.4812632156335518
Loss in iteration 333 : 0.4473705469893668
Loss in iteration 334 : 0.484461567246285
Loss in iteration 335 : 0.5088929369249418
Loss in iteration 336 : 0.5067578017320997
Loss in iteration 337 : 0.4687147523282941
Loss in iteration 338 : 0.4882736047320087
Loss in iteration 339 : 0.540070261592659
Loss in iteration 340 : 0.5267823083280155
Loss in iteration 341 : 0.46484352249618077
Loss in iteration 342 : 0.4683623096253265
Loss in iteration 343 : 0.5115021652209585
Loss in iteration 344 : 0.5320603832300995
Loss in iteration 345 : 0.5031657424113621
Loss in iteration 346 : 0.46080158319986647
Loss in iteration 347 : 0.4685520966839323
Loss in iteration 348 : 0.508356421169453
Loss in iteration 349 : 0.5526367609890038
Loss in iteration 350 : 0.570658631011684
Loss in iteration 351 : 0.5132723342473383
Loss in iteration 352 : 0.46659637632087614
Loss in iteration 353 : 0.47505514895695344
Loss in iteration 354 : 0.528138893794029
Loss in iteration 355 : 0.524570513484564
Loss in iteration 356 : 0.4766070267755695
Loss in iteration 357 : 0.46409125712286536
Loss in iteration 358 : 0.5047304084791384
Loss in iteration 359 : 0.4928154221040246
Loss in iteration 360 : 0.5035187509356981
Loss in iteration 361 : 0.46230493775386056
Loss in iteration 362 : 0.4908360439449032
Loss in iteration 363 : 0.47258178997868905
Loss in iteration 364 : 0.4708810543741152
Loss in iteration 365 : 0.4732833974332115
Loss in iteration 366 : 0.4673520602671623
Loss in iteration 367 : 0.47869864806194196
Loss in iteration 368 : 0.4800851707221205
Loss in iteration 369 : 0.4767684498459276
Loss in iteration 370 : 0.47232707290255843
Loss in iteration 371 : 0.48125014180806264
Loss in iteration 372 : 0.4672181371830066
Loss in iteration 373 : 0.46176795214370275
Loss in iteration 374 : 0.46597780250646875
Loss in iteration 375 : 0.4759197707618942
Loss in iteration 376 : 0.5121408404213544
Loss in iteration 377 : 0.531125702326962
Loss in iteration 378 : 0.5360090357576479
Loss in iteration 379 : 0.4935816826976144
Loss in iteration 380 : 0.47635197103467597
Loss in iteration 381 : 0.4582499864442018
Loss in iteration 382 : 0.4760047602771837
Loss in iteration 383 : 0.48847514198822384
Loss in iteration 384 : 0.48363812607018314
Loss in iteration 385 : 0.46870408611449527
Loss in iteration 386 : 0.4512230387016402
Loss in iteration 387 : 0.4738219742034836
Loss in iteration 388 : 0.4801322621937381
Loss in iteration 389 : 0.4633179076534022
Loss in iteration 390 : 0.46313900952302034
Loss in iteration 391 : 0.4718314939180882
Loss in iteration 392 : 0.4771383098805794
Loss in iteration 393 : 0.4792935105010303
Loss in iteration 394 : 0.4747808299220546
Loss in iteration 395 : 0.47328068405773926
Loss in iteration 396 : 0.45781609878834945
Loss in iteration 397 : 0.4784994791883913
Loss in iteration 398 : 0.47689881713699556
Loss in iteration 399 : 0.46039708837609333
Loss in iteration 400 : 0.4550425715112775
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.78425, training accuracy 0.78425, time elapsed: 8546 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.7675106816084767
Loss in iteration 3 : 1.2325238204477573
Loss in iteration 4 : 1.252261505356409
Loss in iteration 5 : 0.5467760235811359
Loss in iteration 6 : 1.3735071251124065
Loss in iteration 7 : 0.5180799927613045
Loss in iteration 8 : 0.9473518598732334
Loss in iteration 9 : 0.5301756502440517
Loss in iteration 10 : 0.8694616072146215
Loss in iteration 11 : 0.532767992970983
Loss in iteration 12 : 0.7934379254830419
Loss in iteration 13 : 0.5988159693304561
Loss in iteration 14 : 0.6911376194357818
Loss in iteration 15 : 0.7087852199823842
Loss in iteration 16 : 0.6096600713775251
Loss in iteration 17 : 0.697951709504534
Loss in iteration 18 : 0.5534560678966386
Loss in iteration 19 : 0.6867224652357897
Loss in iteration 20 : 0.576413439204877
Loss in iteration 21 : 0.6133144254527638
Loss in iteration 22 : 0.6276773879431092
Loss in iteration 23 : 0.5625327671289979
Loss in iteration 24 : 0.61723164473151
Loss in iteration 25 : 0.5435378933122617
Loss in iteration 26 : 0.5720607837729186
Loss in iteration 27 : 0.5325832210171465
Loss in iteration 28 : 0.5627671600038682
Loss in iteration 29 : 0.5446905236380994
Loss in iteration 30 : 0.5397106036841953
Loss in iteration 31 : 0.5364863285567935
Loss in iteration 32 : 0.5103287257452196
Loss in iteration 33 : 0.5117915497602938
Loss in iteration 34 : 0.48432413011464803
Loss in iteration 35 : 0.5132765608669356
Loss in iteration 36 : 0.4972746730637307
Loss in iteration 37 : 0.5038734533531393
Loss in iteration 38 : 0.4928216002121303
Loss in iteration 39 : 0.48395130428628447
Loss in iteration 40 : 0.4921413797107053
Loss in iteration 41 : 0.49063197426657607
Loss in iteration 42 : 0.4803118071267581
Loss in iteration 43 : 0.4787356252712287
Loss in iteration 44 : 0.49152312863971714
Loss in iteration 45 : 0.48306054222904155
Loss in iteration 46 : 0.49666016230811993
Loss in iteration 47 : 0.4756841028823702
Loss in iteration 48 : 0.477121344374619
Loss in iteration 49 : 0.48590748731028105
Loss in iteration 50 : 0.4543581409102706
Loss in iteration 51 : 0.4864154224695562
Loss in iteration 52 : 0.4715386206796411
Loss in iteration 53 : 0.466107679305644
Loss in iteration 54 : 0.4633728383728783
Loss in iteration 55 : 0.4644710860998998
Loss in iteration 56 : 0.475987522199766
Loss in iteration 57 : 0.4497057400214576
Loss in iteration 58 : 0.45605472657955337
Loss in iteration 59 : 0.45093269806216546
Loss in iteration 60 : 0.4646747324575337
Loss in iteration 61 : 0.4601483514647641
Loss in iteration 62 : 0.46213417975808174
Loss in iteration 63 : 0.46264850884216957
Loss in iteration 64 : 0.4606925036716562
Loss in iteration 65 : 0.47336542861539066
Loss in iteration 66 : 0.4555491769226552
Loss in iteration 67 : 0.4680916862214797
Loss in iteration 68 : 0.4591109964629193
Loss in iteration 69 : 0.4652868543034533
Loss in iteration 70 : 0.4548539121378297
Loss in iteration 71 : 0.47054919192311284
Loss in iteration 72 : 0.4555555788042382
Loss in iteration 73 : 0.46770455512406284
Loss in iteration 74 : 0.4567762504715679
Loss in iteration 75 : 0.46117054447294953
Loss in iteration 76 : 0.4718171457260956
Loss in iteration 77 : 0.45493553367904216
Loss in iteration 78 : 0.4672236681282631
Loss in iteration 79 : 0.44524699344509505
Loss in iteration 80 : 0.46507062032520774
Loss in iteration 81 : 0.46517078427444297
Loss in iteration 82 : 0.4623996723559217
Loss in iteration 83 : 0.4664496793965239
Loss in iteration 84 : 0.4761264855755769
Loss in iteration 85 : 0.4663811504394422
Loss in iteration 86 : 0.472539411552226
Loss in iteration 87 : 0.45203905465571853
Loss in iteration 88 : 0.4743805553474755
Loss in iteration 89 : 0.45945556468847665
Loss in iteration 90 : 0.46138922401467153
Loss in iteration 91 : 0.4711696878363128
Loss in iteration 92 : 0.4572038222286684
Loss in iteration 93 : 0.46428550528631357
Loss in iteration 94 : 0.46198007371534233
Loss in iteration 95 : 0.46216504611082987
Loss in iteration 96 : 0.4680544532243549
Loss in iteration 97 : 0.4605384358423888
Loss in iteration 98 : 0.4687354958745661
Loss in iteration 99 : 0.4628426713984366
Loss in iteration 100 : 0.4617724845098644
Loss in iteration 101 : 0.46077552842954883
Loss in iteration 102 : 0.45624746957361684
Loss in iteration 103 : 0.46591386419812686
Loss in iteration 104 : 0.47259483352123816
Loss in iteration 105 : 0.4445071696769294
Loss in iteration 106 : 0.4650869803838024
Loss in iteration 107 : 0.47753895360884
Loss in iteration 108 : 0.48084655865770815
Loss in iteration 109 : 0.475597989411294
Loss in iteration 110 : 0.454086191957306
Loss in iteration 111 : 0.46792138865219185
Loss in iteration 112 : 0.47442031178097577
Loss in iteration 113 : 0.4589983742719249
Loss in iteration 114 : 0.45587005053202057
Loss in iteration 115 : 0.476554319078791
Loss in iteration 116 : 0.46935084292845214
Loss in iteration 117 : 0.4770307306070484
Loss in iteration 118 : 0.48242034125393396
Loss in iteration 119 : 0.4567738223238279
Loss in iteration 120 : 0.47794368086857525
Loss in iteration 121 : 0.47974082572660887
Loss in iteration 122 : 0.4737059657895801
Loss in iteration 123 : 0.4826269034186757
Loss in iteration 124 : 0.4719915969485735
Loss in iteration 125 : 0.4562636313689987
Loss in iteration 126 : 0.47584947710132974
Loss in iteration 127 : 0.4617963241930131
Loss in iteration 128 : 0.469198572777754
Loss in iteration 129 : 0.4710549122311459
Loss in iteration 130 : 0.4653038625364791
Loss in iteration 131 : 0.46422815276528645
Loss in iteration 132 : 0.45737479404782916
Loss in iteration 133 : 0.466679421314765
Loss in iteration 134 : 0.46465148766083475
Loss in iteration 135 : 0.46286486551481526
Loss in iteration 136 : 0.46325601552085655
Loss in iteration 137 : 0.46612782890263793
Loss in iteration 138 : 0.46616696647220923
Loss in iteration 139 : 0.4708660120706878
Loss in iteration 140 : 0.4700037603450141
Loss in iteration 141 : 0.4695062061072138
Loss in iteration 142 : 0.45605384475050753
Loss in iteration 143 : 0.463181379905436
Loss in iteration 144 : 0.4580613812910076
Loss in iteration 145 : 0.4505829675346699
Loss in iteration 146 : 0.46977216208571465
Loss in iteration 147 : 0.48255762165668714
Loss in iteration 148 : 0.4651015240822199
Loss in iteration 149 : 0.4811901526934038
Loss in iteration 150 : 0.47093914941459536
Loss in iteration 151 : 0.46510039645744833
Loss in iteration 152 : 0.4735744294721154
Loss in iteration 153 : 0.45705946453329077
Loss in iteration 154 : 0.47146079554112036
Loss in iteration 155 : 0.46827725156686273
Loss in iteration 156 : 0.45277056212869315
Loss in iteration 157 : 0.46455297538165985
Loss in iteration 158 : 0.46226671172042216
Loss in iteration 159 : 0.4634561072825472
Loss in iteration 160 : 0.46200102317878006
Loss in iteration 161 : 0.45776204946137306
Loss in iteration 162 : 0.46893116333587903
Loss in iteration 163 : 0.4660170795079708
Loss in iteration 164 : 0.46699426965331775
Loss in iteration 165 : 0.46194065238365833
Loss in iteration 166 : 0.46363305056435195
Loss in iteration 167 : 0.4710254981853622
Loss in iteration 168 : 0.487882317757643
Loss in iteration 169 : 0.4724080827902062
Loss in iteration 170 : 0.46503061768086124
Loss in iteration 171 : 0.4654478837680042
Loss in iteration 172 : 0.4726454316118828
Loss in iteration 173 : 0.4589173038006659
Loss in iteration 174 : 0.4659680568130338
Loss in iteration 175 : 0.4742807889635114
Loss in iteration 176 : 0.4560244087286844
Loss in iteration 177 : 0.4768941266227339
Loss in iteration 178 : 0.4589234229887583
Loss in iteration 179 : 0.46764992053430715
Loss in iteration 180 : 0.45925571316932823
Loss in iteration 181 : 0.4645913184528687
Loss in iteration 182 : 0.4638972924449795
Loss in iteration 183 : 0.4657834933078031
Loss in iteration 184 : 0.47496887190416365
Loss in iteration 185 : 0.4660801027279949
Loss in iteration 186 : 0.45350683665671687
Loss in iteration 187 : 0.454810102053383
Loss in iteration 188 : 0.4685912269997555
Loss in iteration 189 : 0.4644512640672697
Loss in iteration 190 : 0.4490626689872545
Loss in iteration 191 : 0.457112663005776
Loss in iteration 192 : 0.45906142523123816
Loss in iteration 193 : 0.47232000964434695
Loss in iteration 194 : 0.46610422647736516
Loss in iteration 195 : 0.47162588362678703
Loss in iteration 196 : 0.47893043239181554
Loss in iteration 197 : 0.4498802725212211
Loss in iteration 198 : 0.4771996053478727
Loss in iteration 199 : 0.46085671781858106
Loss in iteration 200 : 0.4568944736435147
Loss in iteration 201 : 0.45992247117427737
Loss in iteration 202 : 0.47330408608536056
Loss in iteration 203 : 0.45306287328856903
Loss in iteration 204 : 0.46242570050260395
Loss in iteration 205 : 0.46468756881747236
Loss in iteration 206 : 0.4616191022011956
Loss in iteration 207 : 0.45986297975590906
Loss in iteration 208 : 0.45057331388341904
Loss in iteration 209 : 0.4687664691675282
Loss in iteration 210 : 0.4712231830810947
Loss in iteration 211 : 0.4677135233837111
Loss in iteration 212 : 0.4716647275076677
Loss in iteration 213 : 0.4716552775440384
Loss in iteration 214 : 0.4833295398315897
Loss in iteration 215 : 0.4694004922374374
Loss in iteration 216 : 0.46384414770968946
Loss in iteration 217 : 0.4599336765967026
Loss in iteration 218 : 0.4626657812540325
Loss in iteration 219 : 0.4809938013008659
Loss in iteration 220 : 0.4735061935750732
Loss in iteration 221 : 0.4638369278892692
Loss in iteration 222 : 0.45661584634324653
Loss in iteration 223 : 0.4664637199319512
Loss in iteration 224 : 0.47004313614185195
Loss in iteration 225 : 0.4608192781900761
Loss in iteration 226 : 0.4643439079576909
Loss in iteration 227 : 0.45732116725499017
Loss in iteration 228 : 0.475292413436736
Loss in iteration 229 : 0.46319450129554973
Loss in iteration 230 : 0.46479445687798376
Loss in iteration 231 : 0.4674779911791581
Loss in iteration 232 : 0.46931217465350394
Loss in iteration 233 : 0.4576221584309852
Loss in iteration 234 : 0.46686764758496985
Loss in iteration 235 : 0.45560092378857536
Loss in iteration 236 : 0.464808501356749
Loss in iteration 237 : 0.46424334777968906
Loss in iteration 238 : 0.4633957665999255
Loss in iteration 239 : 0.4674435110483413
Loss in iteration 240 : 0.45508631453169923
Loss in iteration 241 : 0.45885261939968053
Loss in iteration 242 : 0.4952294673373822
Loss in iteration 243 : 0.47380388780010085
Loss in iteration 244 : 0.4601155060474403
Loss in iteration 245 : 0.4806574536821542
Loss in iteration 246 : 0.4602372871158945
Loss in iteration 247 : 0.4565990427638121
Loss in iteration 248 : 0.4664759294522528
Loss in iteration 249 : 0.469237169939315
Loss in iteration 250 : 0.4584935352004865
Loss in iteration 251 : 0.4678037816626961
Loss in iteration 252 : 0.4713786143734613
Loss in iteration 253 : 0.4695093232143888
Loss in iteration 254 : 0.45291524658992804
Loss in iteration 255 : 0.4537968268190704
Loss in iteration 256 : 0.475482473349018
Loss in iteration 257 : 0.45083538910414045
Loss in iteration 258 : 0.4586631881780066
Loss in iteration 259 : 0.458907294783419
Loss in iteration 260 : 0.4603208395071913
Loss in iteration 261 : 0.45028312520306046
Loss in iteration 262 : 0.46247012708185326
Loss in iteration 263 : 0.4625384347544682
Loss in iteration 264 : 0.47707689410294246
Loss in iteration 265 : 0.4563776293093707
Loss in iteration 266 : 0.47536772879050876
Loss in iteration 267 : 0.480484697715502
Loss in iteration 268 : 0.47605689779409527
Loss in iteration 269 : 0.45604751110912123
Loss in iteration 270 : 0.4687687377672896
Loss in iteration 271 : 0.4598687551547699
Loss in iteration 272 : 0.4686204810755935
Loss in iteration 273 : 0.44729394786564636
Loss in iteration 274 : 0.4626323729520962
Loss in iteration 275 : 0.46931054569036573
Loss in iteration 276 : 0.47053806093277645
Loss in iteration 277 : 0.46678204810835466
Loss in iteration 278 : 0.4717561021414204
Loss in iteration 279 : 0.45646535835492663
Loss in iteration 280 : 0.4580327152167461
Loss in iteration 281 : 0.4723694736526183
Loss in iteration 282 : 0.46882770394069473
Loss in iteration 283 : 0.464181551538469
Loss in iteration 284 : 0.4602969183940239
Loss in iteration 285 : 0.477612122394824
Loss in iteration 286 : 0.4704628797214503
Loss in iteration 287 : 0.4671682921486643
Loss in iteration 288 : 0.4858609073912146
Loss in iteration 289 : 0.47672960400602893
Loss in iteration 290 : 0.45700605572119607
Loss in iteration 291 : 0.4664486828204649
Loss in iteration 292 : 0.4686807915356109
Loss in iteration 293 : 0.4677523658009867
Loss in iteration 294 : 0.4702237214207048
Loss in iteration 295 : 0.4527421096448782
Loss in iteration 296 : 0.4612073896895755
Loss in iteration 297 : 0.4649738143058137
Loss in iteration 298 : 0.46384615542880653
Loss in iteration 299 : 0.4698802473171123
Loss in iteration 300 : 0.47581009014869236
Loss in iteration 301 : 0.46576107735884115
Loss in iteration 302 : 0.4611727945101208
Loss in iteration 303 : 0.4524567424210139
Loss in iteration 304 : 0.46474307119261615
Loss in iteration 305 : 0.45903322486195103
Loss in iteration 306 : 0.46646511326285717
Loss in iteration 307 : 0.46124868873321945
Loss in iteration 308 : 0.4678525195687558
Loss in iteration 309 : 0.458618491172512
Loss in iteration 310 : 0.47311045724933676
Loss in iteration 311 : 0.4725722665138984
Loss in iteration 312 : 0.4828084128487003
Loss in iteration 313 : 0.4492543407251237
Loss in iteration 314 : 0.4617393703588694
Loss in iteration 315 : 0.4588528009622682
Loss in iteration 316 : 0.4801871520790772
Loss in iteration 317 : 0.4667263469523508
Loss in iteration 318 : 0.4754337474322189
Loss in iteration 319 : 0.4586754983157794
Loss in iteration 320 : 0.45449277257950416
Loss in iteration 321 : 0.45606972637794174
Loss in iteration 322 : 0.4642895762215641
Loss in iteration 323 : 0.45852779899556845
Loss in iteration 324 : 0.46381416837153894
Loss in iteration 325 : 0.459537750145382
Loss in iteration 326 : 0.47930197230037697
Loss in iteration 327 : 0.4530392703307297
Loss in iteration 328 : 0.4537905707492249
Loss in iteration 329 : 0.460292956276303
Loss in iteration 330 : 0.45744426136124
Loss in iteration 331 : 0.46428954310521786
Loss in iteration 332 : 0.4483525772219104
Loss in iteration 333 : 0.44939104280146847
Loss in iteration 334 : 0.46652854804764887
Loss in iteration 335 : 0.4546649608026988
Loss in iteration 336 : 0.4634039505108737
Loss in iteration 337 : 0.47462455507833223
Loss in iteration 338 : 0.49580695219432896
Loss in iteration 339 : 0.47820618365642015
Loss in iteration 340 : 0.4735861897398226
Loss in iteration 341 : 0.4895705500077019
Loss in iteration 342 : 0.4853889108698785
Loss in iteration 343 : 0.4636860436048868
Loss in iteration 344 : 0.47732601481446046
Loss in iteration 345 : 0.48730485271370033
Loss in iteration 346 : 0.4699972697753841
Loss in iteration 347 : 0.4646382414420417
Loss in iteration 348 : 0.4778405500274429
Loss in iteration 349 : 0.4729796786461559
Loss in iteration 350 : 0.4716171783326399
Loss in iteration 351 : 0.48104607378843534
Loss in iteration 352 : 0.48116030171093155
Loss in iteration 353 : 0.4535403182930774
Loss in iteration 354 : 0.4737180931177226
Loss in iteration 355 : 0.48190511082657095
Loss in iteration 356 : 0.46522346073832466
Loss in iteration 357 : 0.4651975929380226
Loss in iteration 358 : 0.4838160792145053
Loss in iteration 359 : 0.4628097360617392
Loss in iteration 360 : 0.47581591418993324
Loss in iteration 361 : 0.46055433712479565
Loss in iteration 362 : 0.4805384848176454
Loss in iteration 363 : 0.4674555180570633
Loss in iteration 364 : 0.46942083922378736
Loss in iteration 365 : 0.48346871618250414
Loss in iteration 366 : 0.46084542228353187
Loss in iteration 367 : 0.47578185907575476
Loss in iteration 368 : 0.4828953800496536
Loss in iteration 369 : 0.4997415557809913
Loss in iteration 370 : 0.46810199174306955
Loss in iteration 371 : 0.4817834387820371
Loss in iteration 372 : 0.4811354114890217
Loss in iteration 373 : 0.489787968087932
Loss in iteration 374 : 0.47605535325159287
Loss in iteration 375 : 0.45708079418696407
Loss in iteration 376 : 0.47144721337101336
Loss in iteration 377 : 0.47476726243951956
Loss in iteration 378 : 0.4755509261047815
Loss in iteration 379 : 0.46696286478420795
Loss in iteration 380 : 0.46023383895121894
Loss in iteration 381 : 0.45953088460695984
Loss in iteration 382 : 0.45825304132238476
Loss in iteration 383 : 0.4654718159772537
Loss in iteration 384 : 0.47052995011303717
Loss in iteration 385 : 0.46299075450282
Loss in iteration 386 : 0.45266817798458686
Loss in iteration 387 : 0.46769965982689965
Loss in iteration 388 : 0.47217881446397914
Loss in iteration 389 : 0.46059951098044777
Loss in iteration 390 : 0.46020434176297553
Loss in iteration 391 : 0.46358591024439005
Loss in iteration 392 : 0.4582357236885318
Loss in iteration 393 : 0.45475312086426334
Loss in iteration 394 : 0.46318802975336976
Loss in iteration 395 : 0.47131574564063966
Loss in iteration 396 : 0.4600154565614679
Loss in iteration 397 : 0.47411532595759937
Loss in iteration 398 : 0.47561391406537373
Loss in iteration 399 : 0.4767701102221574
Loss in iteration 400 : 0.46901170120299135
Testing accuracy  of updater 6 on alg 0 with rate 1.4000000000000001 = 0.7835, training accuracy 0.7835, time elapsed: 5634 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.7552827183754061
Loss in iteration 3 : 0.7709150572408134
Loss in iteration 4 : 0.6601640826597129
Loss in iteration 5 : 0.534336444507911
Loss in iteration 6 : 0.5976046292429147
Loss in iteration 7 : 0.5237092200904553
Loss in iteration 8 : 0.5258716625290132
Loss in iteration 9 : 0.5145465298474569
Loss in iteration 10 : 0.5082764974158273
Loss in iteration 11 : 0.49347344814626903
Loss in iteration 12 : 0.5026891433407483
Loss in iteration 13 : 0.48546206347581955
Loss in iteration 14 : 0.504214793292638
Loss in iteration 15 : 0.4978996682862915
Loss in iteration 16 : 0.5160431059193786
Loss in iteration 17 : 0.4843584406670657
Loss in iteration 18 : 0.49463264330080664
Loss in iteration 19 : 0.4977470234550581
Loss in iteration 20 : 0.4863484010923261
Loss in iteration 21 : 0.5049753875378368
Loss in iteration 22 : 0.4936655015146126
Loss in iteration 23 : 0.4987680693385071
Loss in iteration 24 : 0.4838680132860825
Loss in iteration 25 : 0.48879318871427907
Loss in iteration 26 : 0.4670059993221666
Loss in iteration 27 : 0.4750341777217782
Loss in iteration 28 : 0.48333971569882384
Loss in iteration 29 : 0.4778118381451089
Loss in iteration 30 : 0.48556613190577497
Loss in iteration 31 : 0.4754718398513318
Loss in iteration 32 : 0.4731739090165553
Loss in iteration 33 : 0.4634204579018658
Loss in iteration 34 : 0.4621660196000654
Loss in iteration 35 : 0.4753001690991867
Loss in iteration 36 : 0.47521767926898967
Loss in iteration 37 : 0.47028239518720183
Loss in iteration 38 : 0.47061725704967305
Loss in iteration 39 : 0.46406134406775185
Loss in iteration 40 : 0.472376748442
Loss in iteration 41 : 0.47556157604577853
Loss in iteration 42 : 0.46068358023870326
Loss in iteration 43 : 0.46761669829423685
Loss in iteration 44 : 0.4668801832668237
Loss in iteration 45 : 0.47055024834219283
Loss in iteration 46 : 0.4729951077080806
Loss in iteration 47 : 0.46447469714297407
Loss in iteration 48 : 0.46768388386951215
Loss in iteration 49 : 0.46728314526727915
Loss in iteration 50 : 0.4525785487318449
Loss in iteration 51 : 0.47407109994650853
Loss in iteration 52 : 0.45954833663152966
Loss in iteration 53 : 0.4615085788935506
Loss in iteration 54 : 0.45786994570546996
Loss in iteration 55 : 0.4626287806528264
Loss in iteration 56 : 0.4673646716028159
Loss in iteration 57 : 0.45096270519316234
Loss in iteration 58 : 0.45481988276165247
Loss in iteration 59 : 0.45307430793463244
Loss in iteration 60 : 0.4633374638047809
Loss in iteration 61 : 0.46022513108896296
Loss in iteration 62 : 0.461510843667271
Loss in iteration 63 : 0.46507827394708884
Loss in iteration 64 : 0.4588344167906501
Loss in iteration 65 : 0.47164544083913235
Loss in iteration 66 : 0.45560006218492444
Loss in iteration 67 : 0.4666698050748231
Loss in iteration 68 : 0.456778861374667
Loss in iteration 69 : 0.4683171382048347
Loss in iteration 70 : 0.4534409616836875
Loss in iteration 71 : 0.4707081090316306
Loss in iteration 72 : 0.4553182376596032
Loss in iteration 73 : 0.46734980596103703
Loss in iteration 74 : 0.45780256782082585
Loss in iteration 75 : 0.4598665830105728
Loss in iteration 76 : 0.4729612015624725
Loss in iteration 77 : 0.4549313694038131
Loss in iteration 78 : 0.46887129034043423
Loss in iteration 79 : 0.4420127512737096
Loss in iteration 80 : 0.4604178022123242
Loss in iteration 81 : 0.46600789277807975
Loss in iteration 82 : 0.46052096510789087
Loss in iteration 83 : 0.46122713611620486
Loss in iteration 84 : 0.47599952080846725
Loss in iteration 85 : 0.46227077063923294
Loss in iteration 86 : 0.47254452138220715
Loss in iteration 87 : 0.4500855480103942
Loss in iteration 88 : 0.4751980809475254
Loss in iteration 89 : 0.45583720171737585
Loss in iteration 90 : 0.4612555283217984
Loss in iteration 91 : 0.4672594901449769
Loss in iteration 92 : 0.4541969689400618
Loss in iteration 93 : 0.46116552914609715
Loss in iteration 94 : 0.4619222392831495
Loss in iteration 95 : 0.4582610308365511
Loss in iteration 96 : 0.4691231652020849
Loss in iteration 97 : 0.46172741404569384
Loss in iteration 98 : 0.4652059008372294
Loss in iteration 99 : 0.4576382889603138
Loss in iteration 100 : 0.4619856637643913
Loss in iteration 101 : 0.4571944833252218
Loss in iteration 102 : 0.45275285450084435
Loss in iteration 103 : 0.46418952720193946
Loss in iteration 104 : 0.4618146269871272
Loss in iteration 105 : 0.44479032396802776
Loss in iteration 106 : 0.46063177316097625
Loss in iteration 107 : 0.4758287774958598
Loss in iteration 108 : 0.478265979209404
Loss in iteration 109 : 0.462608489433903
Loss in iteration 110 : 0.4544997715014278
Loss in iteration 111 : 0.46534318053716245
Loss in iteration 112 : 0.45595867303618715
Loss in iteration 113 : 0.4523079760969582
Loss in iteration 114 : 0.45451308672825075
Loss in iteration 115 : 0.455952152865892
Loss in iteration 116 : 0.4657155308498924
Loss in iteration 117 : 0.47106355496834557
Loss in iteration 118 : 0.467258348931787
Loss in iteration 119 : 0.4558598920507821
Loss in iteration 120 : 0.47296828726765744
Loss in iteration 121 : 0.46532858844110325
Loss in iteration 122 : 0.4769228594148647
Loss in iteration 123 : 0.46613200927780457
Loss in iteration 124 : 0.4553282843355256
Loss in iteration 125 : 0.4590134822193312
Loss in iteration 126 : 0.45936915397185335
Loss in iteration 127 : 0.45587176408608754
Loss in iteration 128 : 0.46597622441529585
Loss in iteration 129 : 0.4594789298520812
Loss in iteration 130 : 0.46535607762072584
Loss in iteration 131 : 0.45707959876077225
Loss in iteration 132 : 0.4572750043717879
Loss in iteration 133 : 0.4631903385577356
Loss in iteration 134 : 0.4656597783720222
Loss in iteration 135 : 0.4619755319498434
Loss in iteration 136 : 0.4617263642928367
Loss in iteration 137 : 0.46754344452202906
Loss in iteration 138 : 0.4657774910510876
Loss in iteration 139 : 0.47356553725799433
Loss in iteration 140 : 0.4678320111870387
Loss in iteration 141 : 0.47349396678535033
Loss in iteration 142 : 0.4557878246609479
Loss in iteration 143 : 0.4700204682272032
Loss in iteration 144 : 0.4553227336954568
Loss in iteration 145 : 0.4554657448634943
Loss in iteration 146 : 0.4691700411784994
Loss in iteration 147 : 0.47395699166857147
Loss in iteration 148 : 0.4616996352581586
Loss in iteration 149 : 0.4763739264525995
Loss in iteration 150 : 0.47186859917772994
Loss in iteration 151 : 0.4588971723067229
Loss in iteration 152 : 0.4801244952852699
Loss in iteration 153 : 0.44787860222065073
Loss in iteration 154 : 0.47048799168913336
Loss in iteration 155 : 0.46798872772625905
Loss in iteration 156 : 0.44964528157113476
Loss in iteration 157 : 0.46619842493791347
Loss in iteration 158 : 0.4618833643403958
Loss in iteration 159 : 0.4621120856042231
Loss in iteration 160 : 0.45825169968638957
Loss in iteration 161 : 0.4574712341104305
Loss in iteration 162 : 0.4633910476327317
Loss in iteration 163 : 0.46671537760568976
Loss in iteration 164 : 0.46371542599064863
Loss in iteration 165 : 0.4640708212809805
Loss in iteration 166 : 0.46042647264049574
Loss in iteration 167 : 0.4774711887344718
Loss in iteration 168 : 0.47573560001694015
Loss in iteration 169 : 0.4598312501949025
Loss in iteration 170 : 0.46653835427577306
Loss in iteration 171 : 0.4560640150420557
Loss in iteration 172 : 0.4721212387945928
Loss in iteration 173 : 0.45612933059723626
Loss in iteration 174 : 0.4607725110854477
Loss in iteration 175 : 0.4679942460647908
Loss in iteration 176 : 0.4581162691558258
Loss in iteration 177 : 0.46845032332396874
Loss in iteration 178 : 0.463851274758136
Loss in iteration 179 : 0.4680717029318813
Loss in iteration 180 : 0.45850429361179257
Loss in iteration 181 : 0.46798926141415903
Loss in iteration 182 : 0.4630559579398489
Loss in iteration 183 : 0.4635616590991602
Loss in iteration 184 : 0.47852726013368235
Loss in iteration 185 : 0.46693564716066194
Loss in iteration 186 : 0.450624988375505
Loss in iteration 187 : 0.46093416451609853
Loss in iteration 188 : 0.471021603946762
Loss in iteration 189 : 0.4752511739062861
Loss in iteration 190 : 0.4505262857837249
Loss in iteration 191 : 0.46487591137537393
Loss in iteration 192 : 0.4613023089795301
Loss in iteration 193 : 0.4678422019279903
Loss in iteration 194 : 0.46624906567319496
Loss in iteration 195 : 0.4746465686845223
Loss in iteration 196 : 0.47485262858174715
Loss in iteration 197 : 0.45787381405957556
Loss in iteration 198 : 0.4704445669574294
Loss in iteration 199 : 0.4612224964951114
Loss in iteration 200 : 0.45072461979145817
Loss in iteration 201 : 0.45960691346290583
Loss in iteration 202 : 0.46985138778012164
Loss in iteration 203 : 0.4503827278661634
Loss in iteration 204 : 0.45750493624354766
Loss in iteration 205 : 0.4602247945994837
Loss in iteration 206 : 0.46045084104789225
Loss in iteration 207 : 0.4543270977349355
Loss in iteration 208 : 0.44959729625327405
Loss in iteration 209 : 0.4667579166525294
Loss in iteration 210 : 0.4712786756303824
Loss in iteration 211 : 0.46293419845172035
Loss in iteration 212 : 0.46842849147332066
Loss in iteration 213 : 0.4685131601640841
Loss in iteration 214 : 0.48297392430909875
Loss in iteration 215 : 0.4732504595162652
Loss in iteration 216 : 0.45996021973141044
Loss in iteration 217 : 0.46014021512563913
Loss in iteration 218 : 0.46242371312461594
Loss in iteration 219 : 0.47016951610998353
Loss in iteration 220 : 0.47226660688819316
Loss in iteration 221 : 0.463781273646214
Loss in iteration 222 : 0.4611855889932914
Loss in iteration 223 : 0.4706133117663294
Loss in iteration 224 : 0.47908564806275833
Loss in iteration 225 : 0.4676001776983833
Loss in iteration 226 : 0.4702150965339022
Loss in iteration 227 : 0.4706833111616085
Loss in iteration 228 : 0.4662292539110523
Loss in iteration 229 : 0.47980678728221987
Loss in iteration 230 : 0.45583607798666953
Loss in iteration 231 : 0.47326566106431206
Loss in iteration 232 : 0.4671787755948911
Loss in iteration 233 : 0.47240897779115865
Loss in iteration 234 : 0.47215116987158
Loss in iteration 235 : 0.4668826420505035
Loss in iteration 236 : 0.4806155423915061
Loss in iteration 237 : 0.46377599744866277
Loss in iteration 238 : 0.4654578067029439
Loss in iteration 239 : 0.466540626954948
Loss in iteration 240 : 0.4633259899331533
Loss in iteration 241 : 0.45965398126569124
Loss in iteration 242 : 0.4997604271350845
Loss in iteration 243 : 0.4697837485218236
Loss in iteration 244 : 0.4733839342425263
Loss in iteration 245 : 0.47381695126214984
Loss in iteration 246 : 0.4686015783627935
Loss in iteration 247 : 0.45131912370732763
Loss in iteration 248 : 0.47294464750109205
Loss in iteration 249 : 0.4669340682787804
Loss in iteration 250 : 0.4611343818962095
Loss in iteration 251 : 0.46835517939356547
Loss in iteration 252 : 0.4650927740482249
Loss in iteration 253 : 0.4733477744180388
Loss in iteration 254 : 0.4498803959477037
Loss in iteration 255 : 0.4573657839123643
Loss in iteration 256 : 0.4729704543609152
Loss in iteration 257 : 0.4538723747440171
Loss in iteration 258 : 0.4574934400379085
Loss in iteration 259 : 0.459010665196173
Loss in iteration 260 : 0.4600585147458184
Loss in iteration 261 : 0.4500097612887548
Loss in iteration 262 : 0.4625138030339496
Loss in iteration 263 : 0.46254324345621706
Loss in iteration 264 : 0.4771291559270756
Loss in iteration 265 : 0.45406539095204634
Loss in iteration 266 : 0.47671960207005304
Loss in iteration 267 : 0.4662992523870334
Loss in iteration 268 : 0.4735938280096844
Loss in iteration 269 : 0.4562965584667129
Loss in iteration 270 : 0.4585749623910421
Loss in iteration 271 : 0.4480970225319552
Loss in iteration 272 : 0.4594043716103654
Loss in iteration 273 : 0.4477153296973002
Loss in iteration 274 : 0.45623134774684554
Loss in iteration 275 : 0.46464824405299815
Loss in iteration 276 : 0.4683479674937099
Loss in iteration 277 : 0.45792735801537787
Loss in iteration 278 : 0.4512086898700319
Loss in iteration 279 : 0.4474816373830695
Loss in iteration 280 : 0.4581368459079758
Loss in iteration 281 : 0.46325767397299256
Loss in iteration 282 : 0.4587857547027793
Loss in iteration 283 : 0.46315080467587855
Loss in iteration 284 : 0.4481431216852183
Loss in iteration 285 : 0.44929512439243147
Loss in iteration 286 : 0.45979690683570634
Loss in iteration 287 : 0.46617961491756
Loss in iteration 288 : 0.4642576253486454
Loss in iteration 289 : 0.4555243176699193
Loss in iteration 290 : 0.4524321667565908
Loss in iteration 291 : 0.46777607934476306
Loss in iteration 292 : 0.46377370253307054
Loss in iteration 293 : 0.4627465506670722
Loss in iteration 294 : 0.4665922497056219
Loss in iteration 295 : 0.4568684633198909
Loss in iteration 296 : 0.4539540829778635
Loss in iteration 297 : 0.47427408615349237
Loss in iteration 298 : 0.4636246488813407
Loss in iteration 299 : 0.4640232756167934
Loss in iteration 300 : 0.46716777146774585
Loss in iteration 301 : 0.4704698825522285
Loss in iteration 302 : 0.4654027466083213
Loss in iteration 303 : 0.4534723514592075
Loss in iteration 304 : 0.4657878139578807
Loss in iteration 305 : 0.45970864011004803
Loss in iteration 306 : 0.4610431349089689
Loss in iteration 307 : 0.46136526518453713
Loss in iteration 308 : 0.4591637304028592
Loss in iteration 309 : 0.4512528113186129
Loss in iteration 310 : 0.4608472290937366
Loss in iteration 311 : 0.46872904116389463
Loss in iteration 312 : 0.4782737588016173
Loss in iteration 313 : 0.44790282336396564
Loss in iteration 314 : 0.4596454540267046
Loss in iteration 315 : 0.4584628005099892
Loss in iteration 316 : 0.4784236194038665
Loss in iteration 317 : 0.46922550495066945
Loss in iteration 318 : 0.4715964710403698
Loss in iteration 319 : 0.45619670839547966
Loss in iteration 320 : 0.4574049229830349
Loss in iteration 321 : 0.45525736524085336
Loss in iteration 322 : 0.46397179409163136
Loss in iteration 323 : 0.4611599141052807
Loss in iteration 324 : 0.45821424989774767
Loss in iteration 325 : 0.4592257590608412
Loss in iteration 326 : 0.4752816127488013
Loss in iteration 327 : 0.45238770134621253
Loss in iteration 328 : 0.4545179390887353
Loss in iteration 329 : 0.4594885921404316
Loss in iteration 330 : 0.4502019028384534
Loss in iteration 331 : 0.46420404127852877
Loss in iteration 332 : 0.4476943315616948
Loss in iteration 333 : 0.4516741440812678
Loss in iteration 334 : 0.4581737937585526
Loss in iteration 335 : 0.4513611966638517
Loss in iteration 336 : 0.4591360440355201
Loss in iteration 337 : 0.4612704764469805
Loss in iteration 338 : 0.4681438029392648
Loss in iteration 339 : 0.4711716017434666
Loss in iteration 340 : 0.46559948923110084
Loss in iteration 341 : 0.4563948729769621
Loss in iteration 342 : 0.4582717515743918
Loss in iteration 343 : 0.4593784764508093
Loss in iteration 344 : 0.46384630266140436
Loss in iteration 345 : 0.4570346417791841
Loss in iteration 346 : 0.4518836474137837
Loss in iteration 347 : 0.46108111756161624
Loss in iteration 348 : 0.4653525163143828
Loss in iteration 349 : 0.4691270984534593
Loss in iteration 350 : 0.46575344480678105
Loss in iteration 351 : 0.4621532631339009
Loss in iteration 352 : 0.4608058289106999
Loss in iteration 353 : 0.45279413348929437
Loss in iteration 354 : 0.4587842145442641
Loss in iteration 355 : 0.4611424978137808
Loss in iteration 356 : 0.4533767936796131
Loss in iteration 357 : 0.4571711755899134
Loss in iteration 358 : 0.4702999885740317
Loss in iteration 359 : 0.45775988849925686
Loss in iteration 360 : 0.4657398444572984
Loss in iteration 361 : 0.4526703898141872
Loss in iteration 362 : 0.47110157153132687
Loss in iteration 363 : 0.4612034380766167
Loss in iteration 364 : 0.46288898710552523
Loss in iteration 365 : 0.4637143320445616
Loss in iteration 366 : 0.4557851188753612
Loss in iteration 367 : 0.47164598967066135
Loss in iteration 368 : 0.4696720031828854
Loss in iteration 369 : 0.4772080968507389
Loss in iteration 370 : 0.45674529627695953
Loss in iteration 371 : 0.4812044602452213
Loss in iteration 372 : 0.4595056219606727
Loss in iteration 373 : 0.4646668574119852
Loss in iteration 374 : 0.45924098401576186
Loss in iteration 375 : 0.45675364294213044
Loss in iteration 376 : 0.47496834806137106
Loss in iteration 377 : 0.4617576838926056
Loss in iteration 378 : 0.4696797288799122
Loss in iteration 379 : 0.46697263923124294
Loss in iteration 380 : 0.46357858378095174
Loss in iteration 381 : 0.452919439517787
Loss in iteration 382 : 0.4609406131628044
Loss in iteration 383 : 0.46375400529701666
Loss in iteration 384 : 0.47171080940575083
Loss in iteration 385 : 0.465029169621895
Loss in iteration 386 : 0.4505116783988168
Loss in iteration 387 : 0.47175749960692404
Loss in iteration 388 : 0.47176314823067106
Loss in iteration 389 : 0.4602012765462611
Loss in iteration 390 : 0.4630486695606053
Loss in iteration 391 : 0.46335515261602184
Loss in iteration 392 : 0.45306464554567594
Loss in iteration 393 : 0.457966653357889
Loss in iteration 394 : 0.4585315622775092
Loss in iteration 395 : 0.46634199259520376
Loss in iteration 396 : 0.45891625181593865
Loss in iteration 397 : 0.4729353290895454
Loss in iteration 398 : 0.4683473964292331
Loss in iteration 399 : 0.4564823823771438
Loss in iteration 400 : 0.4528430472579311
Testing accuracy  of updater 6 on alg 0 with rate 0.8 = 0.788875, training accuracy 0.788875, time elapsed: 6211 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6802502867664142
Loss in iteration 3 : 0.6594515320278449
Loss in iteration 4 : 0.6415711670456183
Loss in iteration 5 : 0.6175248662120846
Loss in iteration 6 : 0.6051146435810946
Loss in iteration 7 : 0.5795494318852037
Loss in iteration 8 : 0.5618519050554621
Loss in iteration 9 : 0.5511138904170692
Loss in iteration 10 : 0.5322407495828883
Loss in iteration 11 : 0.5220648101319824
Loss in iteration 12 : 0.513036236040151
Loss in iteration 13 : 0.4999856634717506
Loss in iteration 14 : 0.4930402103404154
Loss in iteration 15 : 0.5030500782198164
Loss in iteration 16 : 0.4953229680200197
Loss in iteration 17 : 0.4844868202762044
Loss in iteration 18 : 0.48492156001655157
Loss in iteration 19 : 0.48754182550639685
Loss in iteration 20 : 0.4779825411068059
Loss in iteration 21 : 0.4938116471873182
Loss in iteration 22 : 0.4906423776271648
Loss in iteration 23 : 0.48831734097794643
Loss in iteration 24 : 0.48015430697269174
Loss in iteration 25 : 0.484951033380672
Loss in iteration 26 : 0.4712404104634527
Loss in iteration 27 : 0.475207229202476
Loss in iteration 28 : 0.4819917219888485
Loss in iteration 29 : 0.4882766547575059
Loss in iteration 30 : 0.4858393572431443
Loss in iteration 31 : 0.4790784269264641
Loss in iteration 32 : 0.47900845797758196
Loss in iteration 33 : 0.4722800426700637
Loss in iteration 34 : 0.47196129932412495
Loss in iteration 35 : 0.4825691001887275
Loss in iteration 36 : 0.4817461407022
Loss in iteration 37 : 0.47322178464211534
Loss in iteration 38 : 0.4729513271423591
Loss in iteration 39 : 0.4656201811709768
Loss in iteration 40 : 0.4742711250348058
Loss in iteration 41 : 0.47732258316847015
Loss in iteration 42 : 0.4636496031808604
Loss in iteration 43 : 0.4718509358532024
Loss in iteration 44 : 0.47199483915580615
Loss in iteration 45 : 0.473770682658261
Loss in iteration 46 : 0.47291981687435974
Loss in iteration 47 : 0.46601504963025064
Loss in iteration 48 : 0.4711979830126309
Loss in iteration 49 : 0.46559557442413835
Loss in iteration 50 : 0.45680808045578364
Loss in iteration 51 : 0.47513258686654064
Loss in iteration 52 : 0.4625452045041339
Loss in iteration 53 : 0.4633250015179362
Loss in iteration 54 : 0.46080691042219163
Loss in iteration 55 : 0.4648235041701696
Loss in iteration 56 : 0.4702424467926205
Loss in iteration 57 : 0.452162101097003
Loss in iteration 58 : 0.4571936559563574
Loss in iteration 59 : 0.45544512479191734
Loss in iteration 60 : 0.46514648536711406
Loss in iteration 61 : 0.4618385217534892
Loss in iteration 62 : 0.4647762812540332
Loss in iteration 63 : 0.4635527345673981
Loss in iteration 64 : 0.4625194742057157
Loss in iteration 65 : 0.4734978146503464
Loss in iteration 66 : 0.4562659272332026
Loss in iteration 67 : 0.46816017379172503
Loss in iteration 68 : 0.45899888338440525
Loss in iteration 69 : 0.46628521520392885
Loss in iteration 70 : 0.4572982114770081
Loss in iteration 71 : 0.47128339683350723
Loss in iteration 72 : 0.457177807691083
Loss in iteration 73 : 0.4698501404499418
Loss in iteration 74 : 0.4599184016041317
Loss in iteration 75 : 0.4613711707022402
Loss in iteration 76 : 0.4740567251699322
Loss in iteration 77 : 0.45608180205876486
Loss in iteration 78 : 0.46600734626153023
Loss in iteration 79 : 0.4449003695241952
Loss in iteration 80 : 0.46293717966727066
Loss in iteration 81 : 0.46525485123996696
Loss in iteration 82 : 0.46276814321621407
Loss in iteration 83 : 0.46319414557602245
Loss in iteration 84 : 0.4750757272515881
Loss in iteration 85 : 0.46297637570512323
Loss in iteration 86 : 0.47260948065462743
Loss in iteration 87 : 0.45224589696059453
Loss in iteration 88 : 0.4720181762702758
Loss in iteration 89 : 0.45561142103150104
Loss in iteration 90 : 0.4595326470759258
Loss in iteration 91 : 0.46851982256151026
Loss in iteration 92 : 0.45718071813321814
Loss in iteration 93 : 0.46145948363389827
Loss in iteration 94 : 0.46011725753578153
Loss in iteration 95 : 0.45925934304245597
Loss in iteration 96 : 0.46931685869677725
Loss in iteration 97 : 0.46124630302117714
Loss in iteration 98 : 0.4672314763416493
Loss in iteration 99 : 0.4591387091986191
Loss in iteration 100 : 0.46444365494273493
Loss in iteration 101 : 0.45934999109854113
Loss in iteration 102 : 0.45442460543803426
Loss in iteration 103 : 0.4666196393747462
Loss in iteration 104 : 0.46153677984677305
Loss in iteration 105 : 0.4466167827923023
Loss in iteration 106 : 0.4624003469601341
Loss in iteration 107 : 0.47694431691964
Loss in iteration 108 : 0.4771950999889548
Loss in iteration 109 : 0.4604983000856653
Loss in iteration 110 : 0.454519978407328
Loss in iteration 111 : 0.46482814879934187
Loss in iteration 112 : 0.4581685079577831
Loss in iteration 113 : 0.4525524948379718
Loss in iteration 114 : 0.45600454398941126
Loss in iteration 115 : 0.45271072822640424
Loss in iteration 116 : 0.464444729157913
Loss in iteration 117 : 0.4708038768741524
Loss in iteration 118 : 0.47015162111186637
Loss in iteration 119 : 0.45465548880537643
Loss in iteration 120 : 0.4710531383316597
Loss in iteration 121 : 0.46590828148515695
Loss in iteration 122 : 0.47389711948665014
Loss in iteration 123 : 0.46537155528177826
Loss in iteration 124 : 0.45746399126856985
Loss in iteration 125 : 0.45693405938232334
Loss in iteration 126 : 0.45751245197829615
Loss in iteration 127 : 0.4582646474354303
Loss in iteration 128 : 0.46440759542468435
Loss in iteration 129 : 0.4601627115751764
Loss in iteration 130 : 0.4669342860140335
Loss in iteration 131 : 0.4593900057898412
Loss in iteration 132 : 0.4566549338254652
Loss in iteration 133 : 0.46603382220518064
Loss in iteration 134 : 0.46246573070817387
Loss in iteration 135 : 0.46222093645638024
Loss in iteration 136 : 0.4607221480044709
Loss in iteration 137 : 0.4676719573207458
Loss in iteration 138 : 0.46572336837265554
Loss in iteration 139 : 0.47150728949880466
Loss in iteration 140 : 0.4678941253433433
Loss in iteration 141 : 0.46840453590243464
Loss in iteration 142 : 0.45613199706466667
Loss in iteration 143 : 0.4630700472424766
Loss in iteration 144 : 0.4551538702002936
Loss in iteration 145 : 0.4504367245821658
Loss in iteration 146 : 0.4666248753514223
Loss in iteration 147 : 0.4707962223592162
Loss in iteration 148 : 0.4608144110507444
Loss in iteration 149 : 0.47496962365091605
Loss in iteration 150 : 0.46563463251167
Loss in iteration 151 : 0.46117082469318754
Loss in iteration 152 : 0.47130309396770953
Loss in iteration 153 : 0.4479656615014457
Loss in iteration 154 : 0.4699915881557089
Loss in iteration 155 : 0.4657558420711334
Loss in iteration 156 : 0.4482780185523132
Loss in iteration 157 : 0.4658436912852559
Loss in iteration 158 : 0.4581280221521693
Loss in iteration 159 : 0.4591974084336434
Loss in iteration 160 : 0.45952142759026815
Loss in iteration 161 : 0.4568439695289595
Loss in iteration 162 : 0.4596606532827304
Loss in iteration 163 : 0.46504796309999596
Loss in iteration 164 : 0.4640366592600611
Loss in iteration 165 : 0.4573338395723866
Loss in iteration 166 : 0.45961441303460887
Loss in iteration 167 : 0.4695775910001375
Loss in iteration 168 : 0.4711909267980082
Loss in iteration 169 : 0.45824960241415913
Loss in iteration 170 : 0.46445468520618555
Loss in iteration 171 : 0.45515764806784476
Loss in iteration 172 : 0.46548870995247166
Loss in iteration 173 : 0.45687754031492783
Loss in iteration 174 : 0.45498993269413096
Loss in iteration 175 : 0.46946735452481914
Loss in iteration 176 : 0.45463168663289577
Loss in iteration 177 : 0.4681319871654572
Loss in iteration 178 : 0.4574483888172983
Loss in iteration 179 : 0.46602914833386366
Loss in iteration 180 : 0.454134696940397
Loss in iteration 181 : 0.4648785156474867
Loss in iteration 182 : 0.4598000930563912
Loss in iteration 183 : 0.4580975375718398
Loss in iteration 184 : 0.47550026680052654
Loss in iteration 185 : 0.4590258501761267
Loss in iteration 186 : 0.4499824210046959
Loss in iteration 187 : 0.4555094669284804
Loss in iteration 188 : 0.46789012977683964
Loss in iteration 189 : 0.46526556351825976
Loss in iteration 190 : 0.44931599783567355
Loss in iteration 191 : 0.4535080607154717
Loss in iteration 192 : 0.4587379233542198
Loss in iteration 193 : 0.46257702313166854
Loss in iteration 194 : 0.46199349361103575
Loss in iteration 195 : 0.46900981231834976
Loss in iteration 196 : 0.4753111562997071
Loss in iteration 197 : 0.45133749357612574
Loss in iteration 198 : 0.47029441451459897
Loss in iteration 199 : 0.4610983893697013
Loss in iteration 200 : 0.45142307145051724
Loss in iteration 201 : 0.4578086208965139
Loss in iteration 202 : 0.47013082480937907
Loss in iteration 203 : 0.45007670277970097
Loss in iteration 204 : 0.45652138544148335
Loss in iteration 205 : 0.46093995499656526
Loss in iteration 206 : 0.4592721850172865
Loss in iteration 207 : 0.4545514353565929
Loss in iteration 208 : 0.4508764421767713
Loss in iteration 209 : 0.46547767292675424
Loss in iteration 210 : 0.47158276785636843
Loss in iteration 211 : 0.46316083240486183
Loss in iteration 212 : 0.4686057143216707
Loss in iteration 213 : 0.46918499381848194
Loss in iteration 214 : 0.48243695865558084
Loss in iteration 215 : 0.46975191674574834
Loss in iteration 216 : 0.4585892835039553
Loss in iteration 217 : 0.45762940817974246
Loss in iteration 218 : 0.4557388335287301
Loss in iteration 219 : 0.4686446651025332
Loss in iteration 220 : 0.46743569270464674
Loss in iteration 221 : 0.46393139670524886
Loss in iteration 222 : 0.4548200668105723
Loss in iteration 223 : 0.4659740264748504
Loss in iteration 224 : 0.46892274053858096
Loss in iteration 225 : 0.45907597882252765
Loss in iteration 226 : 0.46372861087938394
Loss in iteration 227 : 0.45284037088520285
Loss in iteration 228 : 0.4646121911170025
Loss in iteration 229 : 0.46406279817079055
Loss in iteration 230 : 0.4550533763276155
Loss in iteration 231 : 0.4628769267125928
Loss in iteration 232 : 0.4668704593179133
Loss in iteration 233 : 0.4586707631218504
Loss in iteration 234 : 0.46510167983622736
Loss in iteration 235 : 0.4568527503400447
Loss in iteration 236 : 0.4627191695066062
Loss in iteration 237 : 0.4625834196189322
Loss in iteration 238 : 0.45910308746519096
Loss in iteration 239 : 0.4615464503081582
Loss in iteration 240 : 0.4519591108794948
Loss in iteration 241 : 0.45619218417828905
Loss in iteration 242 : 0.48310627803485556
Loss in iteration 243 : 0.4697284788015003
Loss in iteration 244 : 0.46075881146566017
Loss in iteration 245 : 0.47378313973765923
Loss in iteration 246 : 0.4571796111766298
Loss in iteration 247 : 0.453302726165486
Loss in iteration 248 : 0.4631016116400759
Loss in iteration 249 : 0.4646374347769171
Loss in iteration 250 : 0.4550771921882035
Loss in iteration 251 : 0.4667197983181834
Loss in iteration 252 : 0.46130549016739436
Loss in iteration 253 : 0.47125207902895777
Loss in iteration 254 : 0.44718127839705446
Loss in iteration 255 : 0.4515786457119546
Loss in iteration 256 : 0.4751212395092052
Loss in iteration 257 : 0.44981630766099934
Loss in iteration 258 : 0.4575840114655725
Loss in iteration 259 : 0.4561348373455134
Loss in iteration 260 : 0.45927403188331206
Loss in iteration 261 : 0.4479684060203975
Loss in iteration 262 : 0.4608453982383692
Loss in iteration 263 : 0.46111601807954544
Loss in iteration 264 : 0.47458596206213377
Loss in iteration 265 : 0.45042509644427503
Loss in iteration 266 : 0.46982825219853047
Loss in iteration 267 : 0.46594926833478184
Loss in iteration 268 : 0.4721230474787269
Loss in iteration 269 : 0.4538901503223341
Loss in iteration 270 : 0.4589797567103748
Loss in iteration 271 : 0.44679248648001096
Loss in iteration 272 : 0.459545497117601
Loss in iteration 273 : 0.4462884220586707
Loss in iteration 274 : 0.455652360190954
Loss in iteration 275 : 0.464726491190263
Loss in iteration 276 : 0.46784196430984487
Loss in iteration 277 : 0.45392582514175395
Loss in iteration 278 : 0.45045132217477063
Loss in iteration 279 : 0.4494643263031258
Loss in iteration 280 : 0.45617709316272614
Loss in iteration 281 : 0.4632395186462272
Loss in iteration 282 : 0.4577931280682165
Loss in iteration 283 : 0.46287331340876275
Loss in iteration 284 : 0.4482690099575738
Loss in iteration 285 : 0.4494727048602987
Loss in iteration 286 : 0.45935955535220296
Loss in iteration 287 : 0.46565190624027947
Loss in iteration 288 : 0.4648757977417493
Loss in iteration 289 : 0.4566222602100728
Loss in iteration 290 : 0.4515837353306872
Loss in iteration 291 : 0.4657770590645783
Loss in iteration 292 : 0.46117918971578997
Loss in iteration 293 : 0.4560258627615146
Loss in iteration 294 : 0.46540290651389954
Loss in iteration 295 : 0.4524816698803954
Loss in iteration 296 : 0.4559775824589018
Loss in iteration 297 : 0.46431044398874843
Loss in iteration 298 : 0.46151560206519043
Loss in iteration 299 : 0.4623912545061293
Loss in iteration 300 : 0.4649815700272216
Loss in iteration 301 : 0.4642276192676348
Loss in iteration 302 : 0.45757597672482603
Loss in iteration 303 : 0.4518137852386223
Loss in iteration 304 : 0.45437125358195424
Loss in iteration 305 : 0.4580591192585817
Loss in iteration 306 : 0.4588680399311464
Loss in iteration 307 : 0.45905060317948854
Loss in iteration 308 : 0.4599521823912629
Loss in iteration 309 : 0.4507457018973817
Loss in iteration 310 : 0.45714059955493214
Loss in iteration 311 : 0.46729722072846835
Loss in iteration 312 : 0.4783998606844336
Loss in iteration 313 : 0.4471312153154354
Loss in iteration 314 : 0.4552337938180234
Loss in iteration 315 : 0.45915530973411056
Loss in iteration 316 : 0.4779870856689173
Loss in iteration 317 : 0.4607820541885209
Loss in iteration 318 : 0.47056262080381195
Loss in iteration 319 : 0.4558538577526108
Loss in iteration 320 : 0.45436636380584994
Loss in iteration 321 : 0.45407946438231117
Loss in iteration 322 : 0.45955329150731555
Loss in iteration 323 : 0.4565688884541416
Loss in iteration 324 : 0.4575755903033838
Loss in iteration 325 : 0.45751466899462484
Loss in iteration 326 : 0.47217547956667283
Loss in iteration 327 : 0.4507319324164664
Loss in iteration 328 : 0.4534970894201218
Loss in iteration 329 : 0.45673694785865576
Loss in iteration 330 : 0.4509197807266794
Loss in iteration 331 : 0.461398768928596
Loss in iteration 332 : 0.4468956226311517
Loss in iteration 333 : 0.4470149173591474
Loss in iteration 334 : 0.45753524083894337
Loss in iteration 335 : 0.4527992174360904
Loss in iteration 336 : 0.45879127475023546
Loss in iteration 337 : 0.46295928708673467
Loss in iteration 338 : 0.46781422321683247
Loss in iteration 339 : 0.47281670711079704
Loss in iteration 340 : 0.46512717951589383
Loss in iteration 341 : 0.4565455962991585
Loss in iteration 342 : 0.4581611001384129
Loss in iteration 343 : 0.45800411164424837
Loss in iteration 344 : 0.4647281048599444
Loss in iteration 345 : 0.45840719147685743
Loss in iteration 346 : 0.4523501061516669
Loss in iteration 347 : 0.45912418789522874
Loss in iteration 348 : 0.46129633512404344
Loss in iteration 349 : 0.4660297855113384
Loss in iteration 350 : 0.4664761303066338
Loss in iteration 351 : 0.4620860994476656
Loss in iteration 352 : 0.45688893565703453
Loss in iteration 353 : 0.4516020779152334
Loss in iteration 354 : 0.45990719016744014
Loss in iteration 355 : 0.4601547667182833
Loss in iteration 356 : 0.4526120533067618
Loss in iteration 357 : 0.45782075854649074
Loss in iteration 358 : 0.46700063974967354
Loss in iteration 359 : 0.4558393962183549
Loss in iteration 360 : 0.4662583037542409
Loss in iteration 361 : 0.4497551739226929
Loss in iteration 362 : 0.46965916823270126
Loss in iteration 363 : 0.46231876643459197
Loss in iteration 364 : 0.46004594304331686
Loss in iteration 365 : 0.461607107137207
Loss in iteration 366 : 0.4572472792333765
Loss in iteration 367 : 0.467671835603523
Loss in iteration 368 : 0.4666304180436137
Loss in iteration 369 : 0.47156272505115
Loss in iteration 370 : 0.45518806840227033
Loss in iteration 371 : 0.47353871689792915
Loss in iteration 372 : 0.4541010991221093
Loss in iteration 373 : 0.4599418922885862
Loss in iteration 374 : 0.457634596036173
Loss in iteration 375 : 0.4561159615611587
Loss in iteration 376 : 0.46386810439147913
Loss in iteration 377 : 0.45980341124986823
Loss in iteration 378 : 0.46296923917848576
Loss in iteration 379 : 0.4630706959064817
Loss in iteration 380 : 0.45789316013898995
Loss in iteration 381 : 0.4479332717491345
Loss in iteration 382 : 0.454496091567294
Loss in iteration 383 : 0.46338648349567957
Loss in iteration 384 : 0.46775613340794653
Loss in iteration 385 : 0.4606870626988281
Loss in iteration 386 : 0.4477095980248576
Loss in iteration 387 : 0.4651132020712242
Loss in iteration 388 : 0.4696129735182885
Loss in iteration 389 : 0.4562451018581734
Loss in iteration 390 : 0.45728173484316975
Loss in iteration 391 : 0.4603029448535576
Loss in iteration 392 : 0.4502299580141721
Loss in iteration 393 : 0.45379040616798777
Loss in iteration 394 : 0.456779238800144
Loss in iteration 395 : 0.4647934898854314
Loss in iteration 396 : 0.45771664829577874
Loss in iteration 397 : 0.4717613110766902
Loss in iteration 398 : 0.4678393114149264
Loss in iteration 399 : 0.4558445796279157
Loss in iteration 400 : 0.4499630251649997
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.788, training accuracy 0.788, time elapsed: 5393 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6793201114280726
Loss in iteration 3 : 0.6599995822541526
Loss in iteration 4 : 0.6465266630428788
Loss in iteration 5 : 0.6194166425818104
Loss in iteration 6 : 0.6092741153550975
Loss in iteration 7 : 0.5888905073223488
Loss in iteration 8 : 0.5677482668432073
Loss in iteration 9 : 0.5566931126648962
Loss in iteration 10 : 0.541429847102153
Loss in iteration 11 : 0.5252721481145951
Loss in iteration 12 : 0.517317031591002
Loss in iteration 13 : 0.5071939324649185
Loss in iteration 14 : 0.4976959359227693
Loss in iteration 15 : 0.505470879335775
Loss in iteration 16 : 0.4995142637377811
Loss in iteration 17 : 0.48856843092229574
Loss in iteration 18 : 0.4861874958956814
Loss in iteration 19 : 0.4894673583507708
Loss in iteration 20 : 0.48037233449312
Loss in iteration 21 : 0.49557973304992387
Loss in iteration 22 : 0.49181340277784946
Loss in iteration 23 : 0.48974225171781766
Loss in iteration 24 : 0.4815908424627286
Loss in iteration 25 : 0.4864561900042776
Loss in iteration 26 : 0.47255934881934225
Loss in iteration 27 : 0.4762387962181174
Loss in iteration 28 : 0.4823030647998972
Loss in iteration 29 : 0.4898553425938545
Loss in iteration 30 : 0.48681996127837085
Loss in iteration 31 : 0.4800936785935612
Loss in iteration 32 : 0.4793954561550205
Loss in iteration 33 : 0.4736096117385003
Loss in iteration 34 : 0.4736132232700462
Loss in iteration 35 : 0.48379848694978356
Loss in iteration 36 : 0.48331538544462344
Loss in iteration 37 : 0.47503482582508516
Loss in iteration 38 : 0.47384932172207783
Loss in iteration 39 : 0.4668840456112306
Loss in iteration 40 : 0.4751696340691737
Loss in iteration 41 : 0.4786461134572121
Loss in iteration 42 : 0.46523664824774796
Loss in iteration 43 : 0.4731580962479862
Loss in iteration 44 : 0.47359656286629453
Loss in iteration 45 : 0.47499479779252796
Loss in iteration 46 : 0.4740307178772109
Loss in iteration 47 : 0.4676181190109009
Loss in iteration 48 : 0.47269299925434877
Loss in iteration 49 : 0.46661570816952624
Loss in iteration 50 : 0.45826819352078346
Loss in iteration 51 : 0.47661780690689803
Loss in iteration 52 : 0.4637761389972509
Loss in iteration 53 : 0.46447573085839
Loss in iteration 54 : 0.46163783335580066
Loss in iteration 55 : 0.4656942974323715
Loss in iteration 56 : 0.4709614728888731
Loss in iteration 57 : 0.45305078804247384
Loss in iteration 58 : 0.4582571041019081
Loss in iteration 59 : 0.4564675011880415
Loss in iteration 60 : 0.46601706611172666
Loss in iteration 61 : 0.46298458171024715
Loss in iteration 62 : 0.4658102986042006
Loss in iteration 63 : 0.46410542230178115
Loss in iteration 64 : 0.4630022024444342
Loss in iteration 65 : 0.47426185193942794
Loss in iteration 66 : 0.45703308021600364
Loss in iteration 67 : 0.4688843749691362
Loss in iteration 68 : 0.45922592774803184
Loss in iteration 69 : 0.46703851802674307
Loss in iteration 70 : 0.4579130576766164
Loss in iteration 71 : 0.4722022615828309
Loss in iteration 72 : 0.4578682205587642
Loss in iteration 73 : 0.4699989386350238
Loss in iteration 74 : 0.4606808735710942
Loss in iteration 75 : 0.462207445194879
Loss in iteration 76 : 0.4742360906262492
Loss in iteration 77 : 0.4565981073840552
Loss in iteration 78 : 0.46651939930757974
Loss in iteration 79 : 0.4459122162488633
Loss in iteration 80 : 0.46344314702059247
Loss in iteration 81 : 0.4656774209841361
Loss in iteration 82 : 0.4631641935789934
Loss in iteration 83 : 0.4637795337332449
Loss in iteration 84 : 0.4753132654880576
Loss in iteration 85 : 0.4635611170141997
Loss in iteration 86 : 0.4727910141906631
Loss in iteration 87 : 0.45281771068023224
Loss in iteration 88 : 0.4724367673420109
Loss in iteration 89 : 0.4562571248753335
Loss in iteration 90 : 0.4600336577856057
Loss in iteration 91 : 0.4684830225983601
Loss in iteration 92 : 0.457313763239296
Loss in iteration 93 : 0.4618947663868328
Loss in iteration 94 : 0.4606970931215962
Loss in iteration 95 : 0.45938901819075284
Loss in iteration 96 : 0.46928802176896783
Loss in iteration 97 : 0.46137161243156716
Loss in iteration 98 : 0.4675959484310887
Loss in iteration 99 : 0.4594768137687731
Loss in iteration 100 : 0.4647834465648297
Loss in iteration 101 : 0.4594770296292512
Loss in iteration 102 : 0.454946949621202
Loss in iteration 103 : 0.46718245989298707
Loss in iteration 104 : 0.46202766003569873
Loss in iteration 105 : 0.4471295154683302
Loss in iteration 106 : 0.462460954794048
Loss in iteration 107 : 0.4765924043423171
Loss in iteration 108 : 0.4773341186008463
Loss in iteration 109 : 0.4607469227650317
Loss in iteration 110 : 0.45496387517324155
Loss in iteration 111 : 0.4650166432930474
Loss in iteration 112 : 0.4581724102988625
Loss in iteration 113 : 0.4529240658017972
Loss in iteration 114 : 0.456285454184959
Loss in iteration 115 : 0.4531260802697842
Loss in iteration 116 : 0.4644194768771858
Loss in iteration 117 : 0.47102509994565167
Loss in iteration 118 : 0.4701878771969826
Loss in iteration 119 : 0.4552281608685137
Loss in iteration 120 : 0.4710092306256162
Loss in iteration 121 : 0.46585222730582987
Loss in iteration 122 : 0.47412110249535255
Loss in iteration 123 : 0.4658365285643041
Loss in iteration 124 : 0.45789635795105715
Loss in iteration 125 : 0.4570830285431414
Loss in iteration 126 : 0.45797378367106695
Loss in iteration 127 : 0.4592022849702498
Loss in iteration 128 : 0.4645903881233125
Loss in iteration 129 : 0.4599374209841672
Loss in iteration 130 : 0.46728908428104005
Loss in iteration 131 : 0.4608324328105421
Loss in iteration 132 : 0.4566116048380691
Loss in iteration 133 : 0.4655340793509749
Loss in iteration 134 : 0.46299576617805493
Loss in iteration 135 : 0.4629801639912584
Loss in iteration 136 : 0.4611485595929783
Loss in iteration 137 : 0.4678951722752105
Loss in iteration 138 : 0.4672324022183144
Loss in iteration 139 : 0.471804115591681
Loss in iteration 140 : 0.46654650010541904
Loss in iteration 141 : 0.46829654592417674
Loss in iteration 142 : 0.4566947854986779
Loss in iteration 143 : 0.463976129782189
Loss in iteration 144 : 0.45532288557264916
Loss in iteration 145 : 0.45048957789260546
Loss in iteration 146 : 0.4664304750082354
Loss in iteration 147 : 0.47130074199214383
Loss in iteration 148 : 0.46136293298328496
Loss in iteration 149 : 0.47490810170224673
Loss in iteration 150 : 0.4659835450517855
Loss in iteration 151 : 0.46192866071402305
Loss in iteration 152 : 0.4713847701271119
Loss in iteration 153 : 0.44814941210232706
Loss in iteration 154 : 0.4708239212427882
Loss in iteration 155 : 0.46673518604494196
Loss in iteration 156 : 0.4487255899391682
Loss in iteration 157 : 0.46615011202389045
Loss in iteration 158 : 0.4590298968148533
Loss in iteration 159 : 0.4595567320113392
Loss in iteration 160 : 0.45872760406766216
Loss in iteration 161 : 0.4574968990402221
Loss in iteration 162 : 0.46009314140491153
Loss in iteration 163 : 0.46521261846638173
Loss in iteration 164 : 0.4643205683546836
Loss in iteration 165 : 0.4580911231425556
Loss in iteration 166 : 0.46074922155957687
Loss in iteration 167 : 0.46853958266970785
Loss in iteration 168 : 0.47092130128770987
Loss in iteration 169 : 0.45882917967486264
Loss in iteration 170 : 0.46442409524995854
Loss in iteration 171 : 0.4548260417403715
Loss in iteration 172 : 0.4656385440045077
Loss in iteration 173 : 0.45760157906061166
Loss in iteration 174 : 0.4549653391535977
Loss in iteration 175 : 0.46926129925851257
Loss in iteration 176 : 0.45488423814356127
Loss in iteration 177 : 0.46845140853379036
Loss in iteration 178 : 0.45830219619755797
Loss in iteration 179 : 0.466282509696551
Loss in iteration 180 : 0.454513408752547
Loss in iteration 181 : 0.4660038332141682
Loss in iteration 182 : 0.45977700013643497
Loss in iteration 183 : 0.4581749704916018
Loss in iteration 184 : 0.475555327205129
Loss in iteration 185 : 0.45930105607596355
Loss in iteration 186 : 0.4505509096129571
Loss in iteration 187 : 0.4559860248500693
Loss in iteration 188 : 0.46824759487453804
Loss in iteration 189 : 0.4659813946759582
Loss in iteration 190 : 0.44991504386954556
Loss in iteration 191 : 0.45405385604784676
Loss in iteration 192 : 0.45873480889537815
Loss in iteration 193 : 0.46294311283971945
Loss in iteration 194 : 0.46273654664080177
Loss in iteration 195 : 0.469582489854536
Loss in iteration 196 : 0.47586835466957395
Loss in iteration 197 : 0.45220429121625705
Loss in iteration 198 : 0.4707724818876685
Loss in iteration 199 : 0.4609760992814155
Loss in iteration 200 : 0.4518064555248187
Loss in iteration 201 : 0.4583287225657777
Loss in iteration 202 : 0.47114730455949233
Loss in iteration 203 : 0.4497227997983446
Loss in iteration 204 : 0.4566436801920477
Loss in iteration 205 : 0.461471557499383
Loss in iteration 206 : 0.4600679526911058
Loss in iteration 207 : 0.4551555963058413
Loss in iteration 208 : 0.4507654993168946
Loss in iteration 209 : 0.465859890128197
Loss in iteration 210 : 0.471547508345982
Loss in iteration 211 : 0.46307823122106034
Loss in iteration 212 : 0.46877230607125253
Loss in iteration 213 : 0.46901024395647234
Loss in iteration 214 : 0.4816649764899255
Loss in iteration 215 : 0.4700205378468546
Loss in iteration 216 : 0.45888094845349076
Loss in iteration 217 : 0.45881916720725413
Loss in iteration 218 : 0.45506937670976977
Loss in iteration 219 : 0.46858258887379417
Loss in iteration 220 : 0.4675512187453555
Loss in iteration 221 : 0.4631291039281357
Loss in iteration 222 : 0.45505692993235847
Loss in iteration 223 : 0.466088009825313
Loss in iteration 224 : 0.46933262549880583
Loss in iteration 225 : 0.459470288530451
Loss in iteration 226 : 0.4632085272844357
Loss in iteration 227 : 0.45296618331043487
Loss in iteration 228 : 0.4648564015718792
Loss in iteration 229 : 0.46380065685546495
Loss in iteration 230 : 0.4553385902940651
Loss in iteration 231 : 0.46279085406760934
Loss in iteration 232 : 0.4668643809284881
Loss in iteration 233 : 0.4586736112437815
Loss in iteration 234 : 0.4650074504831088
Loss in iteration 235 : 0.4566935911391046
Loss in iteration 236 : 0.46246517665880144
Loss in iteration 237 : 0.46262709279166925
Loss in iteration 238 : 0.45888195277212096
Loss in iteration 239 : 0.4617961751010352
Loss in iteration 240 : 0.4517809764495905
Loss in iteration 241 : 0.45609214726023917
Loss in iteration 242 : 0.48343350850196487
Loss in iteration 243 : 0.4699200152275692
Loss in iteration 244 : 0.4601987515304627
Loss in iteration 245 : 0.4735624283662521
Loss in iteration 246 : 0.4571422028460624
Loss in iteration 247 : 0.45302869699310006
Loss in iteration 248 : 0.4631729819882036
Loss in iteration 249 : 0.4649080244343458
Loss in iteration 250 : 0.45496441408478944
Loss in iteration 251 : 0.46643197200629555
Loss in iteration 252 : 0.46131043190464
Loss in iteration 253 : 0.4713930846649934
Loss in iteration 254 : 0.44757258214310747
Loss in iteration 255 : 0.45157744170599423
Loss in iteration 256 : 0.47494182805088847
Loss in iteration 257 : 0.44978496947518254
Loss in iteration 258 : 0.45741620506366615
Loss in iteration 259 : 0.45657646625758475
Loss in iteration 260 : 0.45995572535612955
Loss in iteration 261 : 0.44809350298562795
Loss in iteration 262 : 0.4610371508707855
Loss in iteration 263 : 0.4609136418140414
Loss in iteration 264 : 0.4743812719504376
Loss in iteration 265 : 0.4504872714678662
Loss in iteration 266 : 0.4700733873743648
Loss in iteration 267 : 0.46674491374913757
Loss in iteration 268 : 0.47199634540452545
Loss in iteration 269 : 0.45377585951559124
Loss in iteration 270 : 0.4591766014251523
Loss in iteration 271 : 0.4475429373094561
Loss in iteration 272 : 0.459538515890189
Loss in iteration 273 : 0.4462021114417245
Loss in iteration 274 : 0.45606834819997394
Loss in iteration 275 : 0.46539990102134027
Loss in iteration 276 : 0.4685817422724829
Loss in iteration 277 : 0.45366492493020827
Loss in iteration 278 : 0.4504798236606243
Loss in iteration 279 : 0.44990101703955426
Loss in iteration 280 : 0.4562695256910813
Loss in iteration 281 : 0.4633995007360013
Loss in iteration 282 : 0.45787132844230044
Loss in iteration 283 : 0.46216218255070995
Loss in iteration 284 : 0.4477843069766329
Loss in iteration 285 : 0.4497849162680004
Loss in iteration 286 : 0.45980365272757695
Loss in iteration 287 : 0.4650754346695837
Loss in iteration 288 : 0.46418095641502083
Loss in iteration 289 : 0.4575197628934496
Loss in iteration 290 : 0.45074155307405556
Loss in iteration 291 : 0.46542914661994395
Loss in iteration 292 : 0.4610737801380332
Loss in iteration 293 : 0.45681128992166076
Loss in iteration 294 : 0.46592446200742504
Loss in iteration 295 : 0.45201274391406043
Loss in iteration 296 : 0.456332495261871
Loss in iteration 297 : 0.46418010731045023
Loss in iteration 298 : 0.46029400760074135
Loss in iteration 299 : 0.462363070672551
Loss in iteration 300 : 0.465202517172764
Loss in iteration 301 : 0.4639393776637005
Loss in iteration 302 : 0.45693331625740097
Loss in iteration 303 : 0.45216093254352685
Loss in iteration 304 : 0.45440194199361417
Loss in iteration 305 : 0.4580729973803727
Loss in iteration 306 : 0.4589032034459782
Loss in iteration 307 : 0.4595906861409291
Loss in iteration 308 : 0.4590902202865573
Loss in iteration 309 : 0.45063906406382614
Loss in iteration 310 : 0.4575234350523512
Loss in iteration 311 : 0.46740019147313366
Loss in iteration 312 : 0.47841151689926786
Loss in iteration 313 : 0.44731560280166727
Loss in iteration 314 : 0.4552117914794142
Loss in iteration 315 : 0.45888450255477253
Loss in iteration 316 : 0.4783149887745607
Loss in iteration 317 : 0.46084602547384185
Loss in iteration 318 : 0.47076258836705565
Loss in iteration 319 : 0.45568566610335154
Loss in iteration 320 : 0.45445326129808983
Loss in iteration 321 : 0.454046393114516
Loss in iteration 322 : 0.4599742077225127
Loss in iteration 323 : 0.45670997964567445
Loss in iteration 324 : 0.4573527049759988
Loss in iteration 325 : 0.45736675845052904
Loss in iteration 326 : 0.4731928342743635
Loss in iteration 327 : 0.451117061086683
Loss in iteration 328 : 0.4532818628835929
Loss in iteration 329 : 0.45711396362475903
Loss in iteration 330 : 0.4510217795174641
Loss in iteration 331 : 0.46040123062577243
Loss in iteration 332 : 0.44701779982945117
Loss in iteration 333 : 0.4476309659893265
Loss in iteration 334 : 0.45746009556141765
Loss in iteration 335 : 0.4517080790791285
Loss in iteration 336 : 0.4586472314333044
Loss in iteration 337 : 0.46381295693209773
Loss in iteration 338 : 0.4679662629724591
Loss in iteration 339 : 0.4720581589761365
Loss in iteration 340 : 0.4643905947418724
Loss in iteration 341 : 0.4570336412358991
Loss in iteration 342 : 0.45844615472216615
Loss in iteration 343 : 0.4580892009818948
Loss in iteration 344 : 0.4648217367318801
Loss in iteration 345 : 0.4583476183574864
Loss in iteration 346 : 0.4518357853593412
Loss in iteration 347 : 0.45906049756496886
Loss in iteration 348 : 0.4624275868153494
Loss in iteration 349 : 0.4657821004117512
Loss in iteration 350 : 0.46622733407236044
Loss in iteration 351 : 0.4617955499624651
Loss in iteration 352 : 0.4577602085841086
Loss in iteration 353 : 0.45152634846399237
Loss in iteration 354 : 0.45969861849539334
Loss in iteration 355 : 0.46031976245612455
Loss in iteration 356 : 0.4526300192512521
Loss in iteration 357 : 0.4570600589326457
Loss in iteration 358 : 0.467439722185168
Loss in iteration 359 : 0.4559859781355883
Loss in iteration 360 : 0.4645769045070162
Loss in iteration 361 : 0.4500446892492269
Loss in iteration 362 : 0.4702982790170171
Loss in iteration 363 : 0.46096305063187126
Loss in iteration 364 : 0.45896386853761845
Loss in iteration 365 : 0.46195427275473866
Loss in iteration 366 : 0.45599378861977125
Loss in iteration 367 : 0.4666300626327623
Loss in iteration 368 : 0.4666302047042299
Loss in iteration 369 : 0.4716469902143807
Loss in iteration 370 : 0.4559146449130829
Loss in iteration 371 : 0.47367874138111105
Loss in iteration 372 : 0.4543034712533052
Loss in iteration 373 : 0.460057300381323
Loss in iteration 374 : 0.45682881656009783
Loss in iteration 375 : 0.4561792497407676
Loss in iteration 376 : 0.4637660954235384
Loss in iteration 377 : 0.4600804200712722
Loss in iteration 378 : 0.4624838304772319
Loss in iteration 379 : 0.46268579116457165
Loss in iteration 380 : 0.4574523957889961
Loss in iteration 381 : 0.44815422344753414
Loss in iteration 382 : 0.4549103649151686
Loss in iteration 383 : 0.4620863400311425
Loss in iteration 384 : 0.4673526898096809
Loss in iteration 385 : 0.4602902515686735
Loss in iteration 386 : 0.44757160348319625
Loss in iteration 387 : 0.46512891217184393
Loss in iteration 388 : 0.4696037981699949
Loss in iteration 389 : 0.45597315980530273
Loss in iteration 390 : 0.4572336061283664
Loss in iteration 391 : 0.46041825278206316
Loss in iteration 392 : 0.4499777225928204
Loss in iteration 393 : 0.45377922980163005
Loss in iteration 394 : 0.45699046232398305
Loss in iteration 395 : 0.46482650968474615
Loss in iteration 396 : 0.45765562850645336
Loss in iteration 397 : 0.47184300912226845
Loss in iteration 398 : 0.4679604564857233
Loss in iteration 399 : 0.4555884787770194
Loss in iteration 400 : 0.4502759567414524
Testing accuracy  of updater 6 on alg 0 with rate 0.14 = 0.78825, training accuracy 0.78825, time elapsed: 5572 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.682325726375691
Loss in iteration 3 : 0.6663556389869715
Loss in iteration 4 : 0.6593212937351979
Loss in iteration 5 : 0.6363221796933091
Loss in iteration 6 : 0.6250249779083067
Loss in iteration 7 : 0.6127389777167908
Loss in iteration 8 : 0.5955018563358017
Loss in iteration 9 : 0.5769591632640472
Loss in iteration 10 : 0.565724109508477
Loss in iteration 11 : 0.5500350662131482
Loss in iteration 12 : 0.5379170136634529
Loss in iteration 13 : 0.5275835240133232
Loss in iteration 14 : 0.5198423107677537
Loss in iteration 15 : 0.5225573746981613
Loss in iteration 16 : 0.5124682419691875
Loss in iteration 17 : 0.5043935266440375
Loss in iteration 18 : 0.5006672812610237
Loss in iteration 19 : 0.5020390200508339
Loss in iteration 20 : 0.4936409367026509
Loss in iteration 21 : 0.5033693865712096
Loss in iteration 22 : 0.49990615741579847
Loss in iteration 23 : 0.49788427770529275
Loss in iteration 24 : 0.48817614510382296
Loss in iteration 25 : 0.49282560508808165
Loss in iteration 26 : 0.4801529940796924
Loss in iteration 27 : 0.48263372664433557
Loss in iteration 28 : 0.4868744714284737
Loss in iteration 29 : 0.49423915244176997
Loss in iteration 30 : 0.49120116331426145
Loss in iteration 31 : 0.48591063937091655
Loss in iteration 32 : 0.48420847320693605
Loss in iteration 33 : 0.4775948694843866
Loss in iteration 34 : 0.4778727196654226
Loss in iteration 35 : 0.4870513185222723
Loss in iteration 36 : 0.48700007649565374
Loss in iteration 37 : 0.4798064031452203
Loss in iteration 38 : 0.47896813698868557
Loss in iteration 39 : 0.47229390285462314
Loss in iteration 40 : 0.4784722968694746
Loss in iteration 41 : 0.4826964355260979
Loss in iteration 42 : 0.4706318801956303
Loss in iteration 43 : 0.4788940370314569
Loss in iteration 44 : 0.47910430744834104
Loss in iteration 45 : 0.4789184797287011
Loss in iteration 46 : 0.47871421137420533
Loss in iteration 47 : 0.4732839957624358
Loss in iteration 48 : 0.47817774951183245
Loss in iteration 49 : 0.4708414770809723
Loss in iteration 50 : 0.4637840818500528
Loss in iteration 51 : 0.481147198920798
Loss in iteration 52 : 0.4687759682776168
Loss in iteration 53 : 0.46864833332555955
Loss in iteration 54 : 0.4662848086349293
Loss in iteration 55 : 0.4700314362418084
Loss in iteration 56 : 0.4745959625773198
Loss in iteration 57 : 0.45762691971955016
Loss in iteration 58 : 0.462680331389918
Loss in iteration 59 : 0.46158480768418003
Loss in iteration 60 : 0.4695147942877801
Loss in iteration 61 : 0.4674464396241768
Loss in iteration 62 : 0.46984432167964707
Loss in iteration 63 : 0.4675070588619856
Loss in iteration 64 : 0.46614206395587127
Loss in iteration 65 : 0.47734565390617095
Loss in iteration 66 : 0.46028693646919533
Loss in iteration 67 : 0.47138898302884064
Loss in iteration 68 : 0.46164874644957177
Loss in iteration 69 : 0.47084494440099417
Loss in iteration 70 : 0.4610211811145836
Loss in iteration 71 : 0.4759786730181265
Loss in iteration 72 : 0.46164357608811724
Loss in iteration 73 : 0.4719924700064621
Loss in iteration 74 : 0.4639391708222512
Loss in iteration 75 : 0.46539637569693515
Loss in iteration 76 : 0.4764574687473983
Loss in iteration 77 : 0.4597973478771097
Loss in iteration 78 : 0.46860731587115084
Loss in iteration 79 : 0.4495152857227619
Loss in iteration 80 : 0.46618708514159046
Loss in iteration 81 : 0.46811095721667956
Loss in iteration 82 : 0.46520447175867213
Loss in iteration 83 : 0.4667736370508787
Loss in iteration 84 : 0.47713897113114084
Loss in iteration 85 : 0.4669494212638712
Loss in iteration 86 : 0.47463976520869855
Loss in iteration 87 : 0.4553205180975367
Loss in iteration 88 : 0.47490463295863367
Loss in iteration 89 : 0.4586326261452677
Loss in iteration 90 : 0.46201298381554357
Loss in iteration 91 : 0.47059598069777286
Loss in iteration 92 : 0.4596176250631893
Loss in iteration 93 : 0.4641816767312785
Loss in iteration 94 : 0.4630744573671099
Loss in iteration 95 : 0.46130440728755273
Loss in iteration 96 : 0.471670276547919
Loss in iteration 97 : 0.46340653224249406
Loss in iteration 98 : 0.4692128544673616
Loss in iteration 99 : 0.46173292893836404
Loss in iteration 100 : 0.46707067024458243
Loss in iteration 101 : 0.4612249126885372
Loss in iteration 102 : 0.4578521493084158
Loss in iteration 103 : 0.469567170173813
Loss in iteration 104 : 0.464170784728536
Loss in iteration 105 : 0.45014759022447365
Loss in iteration 106 : 0.4634581672456675
Loss in iteration 107 : 0.47682407732773974
Loss in iteration 108 : 0.4781988573187717
Loss in iteration 109 : 0.4625900472081986
Loss in iteration 110 : 0.45697152576482747
Loss in iteration 111 : 0.46702869169506184
Loss in iteration 112 : 0.4598126688271133
Loss in iteration 113 : 0.45471517418172075
Loss in iteration 114 : 0.4573621322995734
Loss in iteration 115 : 0.45472487926358474
Loss in iteration 116 : 0.4655446332321979
Loss in iteration 117 : 0.47246124620577684
Loss in iteration 118 : 0.4706943006016964
Loss in iteration 119 : 0.4566716085692714
Loss in iteration 120 : 0.47271990192439195
Loss in iteration 121 : 0.4671147333259512
Loss in iteration 122 : 0.47486613127675276
Loss in iteration 123 : 0.46663390408958916
Loss in iteration 124 : 0.4595550826415267
Loss in iteration 125 : 0.45856417986176
Loss in iteration 126 : 0.45847485841093194
Loss in iteration 127 : 0.4604811163479313
Loss in iteration 128 : 0.46644211300728844
Loss in iteration 129 : 0.460652578466941
Loss in iteration 130 : 0.4665769928461497
Loss in iteration 131 : 0.46135187175283293
Loss in iteration 132 : 0.45875614818856103
Loss in iteration 133 : 0.46585556071918977
Loss in iteration 134 : 0.4634995697464895
Loss in iteration 135 : 0.46361695803017455
Loss in iteration 136 : 0.46325059744588337
Loss in iteration 137 : 0.46900180586557955
Loss in iteration 138 : 0.46694471715078156
Loss in iteration 139 : 0.4720593757261228
Loss in iteration 140 : 0.4664812887848698
Loss in iteration 141 : 0.46884736552079476
Loss in iteration 142 : 0.45783888210165674
Loss in iteration 143 : 0.46439096030968574
Loss in iteration 144 : 0.45713889472896524
Loss in iteration 145 : 0.451357154743343
Loss in iteration 146 : 0.4685014878443399
Loss in iteration 147 : 0.4712964412772061
Loss in iteration 148 : 0.46187612276443607
Loss in iteration 149 : 0.4755960775166349
Loss in iteration 150 : 0.46595853576746427
Loss in iteration 151 : 0.4620345805518494
Loss in iteration 152 : 0.47310105785877804
Loss in iteration 153 : 0.44924083055365344
Loss in iteration 154 : 0.47055419377659374
Loss in iteration 155 : 0.4668377979182313
Loss in iteration 156 : 0.45003687623376437
Loss in iteration 157 : 0.46554451034631306
Loss in iteration 158 : 0.4583090136594494
Loss in iteration 159 : 0.46001029765014806
Loss in iteration 160 : 0.4588310704140992
Loss in iteration 161 : 0.45706768500361233
Loss in iteration 162 : 0.4608386832039796
Loss in iteration 163 : 0.465781484467031
Loss in iteration 164 : 0.46538571499176185
Loss in iteration 165 : 0.4578954117149538
Loss in iteration 166 : 0.46075381706105417
Loss in iteration 167 : 0.4696240381139045
Loss in iteration 168 : 0.471541939554098
Loss in iteration 169 : 0.459416522054911
Loss in iteration 170 : 0.4655245130561368
Loss in iteration 171 : 0.4562968997519118
Loss in iteration 172 : 0.46576557482466163
Loss in iteration 173 : 0.45875936633553566
Loss in iteration 174 : 0.45581328614310673
Loss in iteration 175 : 0.4701854344654383
Loss in iteration 176 : 0.4557912704448129
Loss in iteration 177 : 0.4694226542196724
Loss in iteration 178 : 0.45891185207308666
Loss in iteration 179 : 0.4671389459104354
Loss in iteration 180 : 0.45565115280455537
Loss in iteration 181 : 0.46584501637968995
Loss in iteration 182 : 0.46038708978768955
Loss in iteration 183 : 0.4588713523657519
Loss in iteration 184 : 0.47676934539501475
Loss in iteration 185 : 0.46063232628937717
Loss in iteration 186 : 0.4513736173265782
Loss in iteration 187 : 0.45732628513494694
Loss in iteration 188 : 0.4687300075640928
Loss in iteration 189 : 0.4660681501469191
Loss in iteration 190 : 0.45067153893894524
Loss in iteration 191 : 0.455627678134346
Loss in iteration 192 : 0.460101572330739
Loss in iteration 193 : 0.46371478303443187
Loss in iteration 194 : 0.46219604078602605
Loss in iteration 195 : 0.4704027353965433
Loss in iteration 196 : 0.47709480784658553
Loss in iteration 197 : 0.45292718153746564
Loss in iteration 198 : 0.47074879466061725
Loss in iteration 199 : 0.46122570403044605
Loss in iteration 200 : 0.45228008032610795
Loss in iteration 201 : 0.45962388224063766
Loss in iteration 202 : 0.47091481635164795
Loss in iteration 203 : 0.45044389446269845
Loss in iteration 204 : 0.4583140806542875
Loss in iteration 205 : 0.4607670305163037
Loss in iteration 206 : 0.4608994449051932
Loss in iteration 207 : 0.45674245964695753
Loss in iteration 208 : 0.45159973163484685
Loss in iteration 209 : 0.4656464500961241
Loss in iteration 210 : 0.47130117324722653
Loss in iteration 211 : 0.4633751305100406
Loss in iteration 212 : 0.46855264977160216
Loss in iteration 213 : 0.47013255994206826
Loss in iteration 214 : 0.48269152538604176
Loss in iteration 215 : 0.47142395523973313
Loss in iteration 216 : 0.4594589389968817
Loss in iteration 217 : 0.45881213379261043
Loss in iteration 218 : 0.45602382029428357
Loss in iteration 219 : 0.4688565810655665
Loss in iteration 220 : 0.4680151282620267
Loss in iteration 221 : 0.4642472008357065
Loss in iteration 222 : 0.4552191991413451
Loss in iteration 223 : 0.4669099029526721
Loss in iteration 224 : 0.46960007371197054
Loss in iteration 225 : 0.46015893525155116
Loss in iteration 226 : 0.46382471828830124
Loss in iteration 227 : 0.45346524123992693
Loss in iteration 228 : 0.46530311628466303
Loss in iteration 229 : 0.46429831844187547
Loss in iteration 230 : 0.45646726750095323
Loss in iteration 231 : 0.46365005423384353
Loss in iteration 232 : 0.46711158792529034
Loss in iteration 233 : 0.4592823936936045
Loss in iteration 234 : 0.465809558962316
Loss in iteration 235 : 0.45736551598725805
Loss in iteration 236 : 0.4630855237313819
Loss in iteration 237 : 0.46325559894100626
Loss in iteration 238 : 0.459728397621337
Loss in iteration 239 : 0.4622222112239617
Loss in iteration 240 : 0.4529315181050646
Loss in iteration 241 : 0.45648797911860656
Loss in iteration 242 : 0.4838537020157224
Loss in iteration 243 : 0.47073450972880004
Loss in iteration 244 : 0.4602750621673033
Loss in iteration 245 : 0.4737980844395894
Loss in iteration 246 : 0.45819992115557084
Loss in iteration 247 : 0.45369542315739114
Loss in iteration 248 : 0.4636212623925299
Loss in iteration 249 : 0.46559148975400183
Loss in iteration 250 : 0.45537822813510304
Loss in iteration 251 : 0.4671898511035774
Loss in iteration 252 : 0.46172631975999767
Loss in iteration 253 : 0.47177881474130584
Loss in iteration 254 : 0.4479172547852598
Loss in iteration 255 : 0.4526503166457409
Loss in iteration 256 : 0.4758479713146229
Loss in iteration 257 : 0.450935448931075
Loss in iteration 258 : 0.45767557504337375
Loss in iteration 259 : 0.45702768780772546
Loss in iteration 260 : 0.46041171319459273
Loss in iteration 261 : 0.44938639358064414
Loss in iteration 262 : 0.4622761795798553
Loss in iteration 263 : 0.46127549816615443
Loss in iteration 264 : 0.47384630794486265
Loss in iteration 265 : 0.45075978524816385
Loss in iteration 266 : 0.4706844002043786
Loss in iteration 267 : 0.46835692005636526
Loss in iteration 268 : 0.47300523465930533
Loss in iteration 269 : 0.4546793357982492
Loss in iteration 270 : 0.4595210805031873
Loss in iteration 271 : 0.44852014864991774
Loss in iteration 272 : 0.4608711660398522
Loss in iteration 273 : 0.4468282849105089
Loss in iteration 274 : 0.4562781208564659
Loss in iteration 275 : 0.4660706436972188
Loss in iteration 276 : 0.4700347491639285
Loss in iteration 277 : 0.4533652224289896
Loss in iteration 278 : 0.4510618481165224
Loss in iteration 279 : 0.450111520742165
Loss in iteration 280 : 0.45673550118251455
Loss in iteration 281 : 0.4630230557134334
Loss in iteration 282 : 0.45735903708589226
Loss in iteration 283 : 0.4628818327175314
Loss in iteration 284 : 0.44896251018313643
Loss in iteration 285 : 0.4494157647521775
Loss in iteration 286 : 0.4600602184563006
Loss in iteration 287 : 0.4659068647718789
Loss in iteration 288 : 0.464423942987816
Loss in iteration 289 : 0.45608604221578763
Loss in iteration 290 : 0.45138882353708165
Loss in iteration 291 : 0.46567751223568077
Loss in iteration 292 : 0.46183082582346136
Loss in iteration 293 : 0.45656693683021393
Loss in iteration 294 : 0.46690894398861005
Loss in iteration 295 : 0.4524260021257142
Loss in iteration 296 : 0.4555902511625562
Loss in iteration 297 : 0.4647791105709205
Loss in iteration 298 : 0.4607597535998803
Loss in iteration 299 : 0.46253339847782066
Loss in iteration 300 : 0.46551092393431365
Loss in iteration 301 : 0.4644703435449686
Loss in iteration 302 : 0.4576569466175785
Loss in iteration 303 : 0.4524516346560393
Loss in iteration 304 : 0.45511465694052894
Loss in iteration 305 : 0.45855748614319747
Loss in iteration 306 : 0.4590167426402505
Loss in iteration 307 : 0.4598216678959015
Loss in iteration 308 : 0.4598306381012131
Loss in iteration 309 : 0.4515617970554075
Loss in iteration 310 : 0.45799236808259153
Loss in iteration 311 : 0.4671377811252596
Loss in iteration 312 : 0.4787569636303361
Loss in iteration 313 : 0.44836451427302987
Loss in iteration 314 : 0.45542934271768715
Loss in iteration 315 : 0.4584913266089438
Loss in iteration 316 : 0.47712629560835756
Loss in iteration 317 : 0.4614947792787113
Loss in iteration 318 : 0.471053253544106
Loss in iteration 319 : 0.4555511937833168
Loss in iteration 320 : 0.4548222996013762
Loss in iteration 321 : 0.45379371942464136
Loss in iteration 322 : 0.45932480329200776
Loss in iteration 323 : 0.4567920261664595
Loss in iteration 324 : 0.45832035933216053
Loss in iteration 325 : 0.45719258673458174
Loss in iteration 326 : 0.47282561440943427
Loss in iteration 327 : 0.4513721776006505
Loss in iteration 328 : 0.4534810126899528
Loss in iteration 329 : 0.45593916493327746
Loss in iteration 330 : 0.45110647846031815
Loss in iteration 331 : 0.46042354066995067
Loss in iteration 332 : 0.44710103609982305
Loss in iteration 333 : 0.4478490803378804
Loss in iteration 334 : 0.4577865769961567
Loss in iteration 335 : 0.4519405752283471
Loss in iteration 336 : 0.45892664929200727
Loss in iteration 337 : 0.46312490957045316
Loss in iteration 338 : 0.4681606598323433
Loss in iteration 339 : 0.4718096262101198
Loss in iteration 340 : 0.46448557325325507
Loss in iteration 341 : 0.45684148622918763
Loss in iteration 342 : 0.4588919153107172
Loss in iteration 343 : 0.4585109749516297
Loss in iteration 344 : 0.46504307567358316
Loss in iteration 345 : 0.4583468311793582
Loss in iteration 346 : 0.45217018499333195
Loss in iteration 347 : 0.45970911891614874
Loss in iteration 348 : 0.4614586697749437
Loss in iteration 349 : 0.466110079767491
Loss in iteration 350 : 0.4666808301381883
Loss in iteration 351 : 0.46116016178287983
Loss in iteration 352 : 0.4575902768492093
Loss in iteration 353 : 0.452169287270256
Loss in iteration 354 : 0.459847516509738
Loss in iteration 355 : 0.4594119342901799
Loss in iteration 356 : 0.45298426054796415
Loss in iteration 357 : 0.45752316325920755
Loss in iteration 358 : 0.4668040945823321
Loss in iteration 359 : 0.4565501321038043
Loss in iteration 360 : 0.4651170666568327
Loss in iteration 361 : 0.4496533992533784
Loss in iteration 362 : 0.47015635436806824
Loss in iteration 363 : 0.46145208174407343
Loss in iteration 364 : 0.45904111927586083
Loss in iteration 365 : 0.46242955966449456
Loss in iteration 366 : 0.45657084395553515
Loss in iteration 367 : 0.46630074059575344
Loss in iteration 368 : 0.46692289623063626
Loss in iteration 369 : 0.47152046717117235
Loss in iteration 370 : 0.4566157420100792
Loss in iteration 371 : 0.4741416105366266
Loss in iteration 372 : 0.45424851690983964
Loss in iteration 373 : 0.4598469197855894
Loss in iteration 374 : 0.4573058520710648
Loss in iteration 375 : 0.4560053637242629
Loss in iteration 376 : 0.46474936674389966
Loss in iteration 377 : 0.4603873060513496
Loss in iteration 378 : 0.4628913474244163
Loss in iteration 379 : 0.46292963978937585
Loss in iteration 380 : 0.4577940329196578
Loss in iteration 381 : 0.4487241359376944
Loss in iteration 382 : 0.45495541060802486
Loss in iteration 383 : 0.4623203837418654
Loss in iteration 384 : 0.46774614726892716
Loss in iteration 385 : 0.46047912027853666
Loss in iteration 386 : 0.44795843397626006
Loss in iteration 387 : 0.4656294402229889
Loss in iteration 388 : 0.4693845568221784
Loss in iteration 389 : 0.4564648674174785
Loss in iteration 390 : 0.45758377708631653
Loss in iteration 391 : 0.46073135366884305
Loss in iteration 392 : 0.4501320868905926
Loss in iteration 393 : 0.45402641219124706
Loss in iteration 394 : 0.457433150433435
Loss in iteration 395 : 0.46510454311236743
Loss in iteration 396 : 0.4583318251985976
Loss in iteration 397 : 0.4721700153544451
Loss in iteration 398 : 0.4679902018342299
Loss in iteration 399 : 0.4562801207226263
Loss in iteration 400 : 0.45055584536019294
Testing accuracy  of updater 6 on alg 0 with rate 0.08000000000000002 = 0.788125, training accuracy 0.788125, time elapsed: 5194 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6892787252606047
Loss in iteration 3 : 0.6815099379725457
Loss in iteration 4 : 0.6775763791485008
Loss in iteration 5 : 0.669855080337541
Loss in iteration 6 : 0.6669651374008356
Loss in iteration 7 : 0.6571832816635348
Loss in iteration 8 : 0.648016669672382
Loss in iteration 9 : 0.641658813859937
Loss in iteration 10 : 0.6351440337077107
Loss in iteration 11 : 0.6236743295951943
Loss in iteration 12 : 0.6194492110349216
Loss in iteration 13 : 0.6120734086505543
Loss in iteration 14 : 0.604214876263879
Loss in iteration 15 : 0.5987490093190009
Loss in iteration 16 : 0.5889389884164187
Loss in iteration 17 : 0.5820260706214361
Loss in iteration 18 : 0.5766952339179994
Loss in iteration 19 : 0.5739636335062649
Loss in iteration 20 : 0.5669103397579628
Loss in iteration 21 : 0.566597886623349
Loss in iteration 22 : 0.561937524629792
Loss in iteration 23 : 0.5585149765163895
Loss in iteration 24 : 0.5483747737894755
Loss in iteration 25 : 0.549154681759218
Loss in iteration 26 : 0.5403833856231779
Loss in iteration 27 : 0.5380314050047033
Loss in iteration 28 : 0.5377127301861846
Loss in iteration 29 : 0.5406534204482973
Loss in iteration 30 : 0.5375679575747998
Loss in iteration 31 : 0.531858276627532
Loss in iteration 32 : 0.5300889170365038
Loss in iteration 33 : 0.5246904898453083
Loss in iteration 34 : 0.522863561763945
Loss in iteration 35 : 0.5259858910982911
Loss in iteration 36 : 0.5232307571879217
Loss in iteration 37 : 0.5184929783011522
Loss in iteration 38 : 0.5159149202340096
Loss in iteration 39 : 0.5126398092235358
Loss in iteration 40 : 0.5142220888613632
Loss in iteration 41 : 0.5154820791978396
Loss in iteration 42 : 0.5068718775187575
Loss in iteration 43 : 0.512987044151322
Loss in iteration 44 : 0.5113614090022348
Loss in iteration 45 : 0.509832879488548
Loss in iteration 46 : 0.5091080518877041
Loss in iteration 47 : 0.5063489091094221
Loss in iteration 48 : 0.5093623189019646
Loss in iteration 49 : 0.5026711125142846
Loss in iteration 50 : 0.4983276039672037
Loss in iteration 51 : 0.5086531984919004
Loss in iteration 52 : 0.49950149150373147
Loss in iteration 53 : 0.4985960798298867
Loss in iteration 54 : 0.49547746281762367
Loss in iteration 55 : 0.4978806880189949
Loss in iteration 56 : 0.500969768760125
Loss in iteration 57 : 0.4883668109921696
Loss in iteration 58 : 0.4912815185009331
Loss in iteration 59 : 0.4917840771531991
Loss in iteration 60 : 0.49526885750393457
Loss in iteration 61 : 0.49579998054298496
Loss in iteration 62 : 0.49507660053916763
Loss in iteration 63 : 0.4924266502114878
Loss in iteration 64 : 0.4915792114018241
Loss in iteration 65 : 0.4995411598663078
Loss in iteration 66 : 0.4866901880503072
Loss in iteration 67 : 0.4922883319304364
Loss in iteration 68 : 0.4854044573640637
Loss in iteration 69 : 0.4947318506718459
Loss in iteration 70 : 0.48665778088255385
Loss in iteration 71 : 0.5000413199874443
Loss in iteration 72 : 0.48709052024561444
Loss in iteration 73 : 0.4924959220084293
Loss in iteration 74 : 0.4886235374303926
Loss in iteration 75 : 0.48853651416736443
Loss in iteration 76 : 0.49542959715218593
Loss in iteration 77 : 0.4845654405643916
Loss in iteration 78 : 0.48979007225708776
Loss in iteration 79 : 0.4750942987973467
Loss in iteration 80 : 0.48663837209439037
Loss in iteration 81 : 0.4895597239034867
Loss in iteration 82 : 0.4851282762190259
Loss in iteration 83 : 0.4894112376431901
Loss in iteration 84 : 0.4945391975617089
Loss in iteration 85 : 0.48773960498842467
Loss in iteration 86 : 0.4925443178492599
Loss in iteration 87 : 0.4772044675391159
Loss in iteration 88 : 0.4942664653048989
Loss in iteration 89 : 0.47990846517341684
Loss in iteration 90 : 0.4829377328967474
Loss in iteration 91 : 0.48996999075318975
Loss in iteration 92 : 0.47903499147494205
Loss in iteration 93 : 0.48282195553719603
Loss in iteration 94 : 0.48352395193711434
Loss in iteration 95 : 0.47898977360177675
Loss in iteration 96 : 0.49101926024480363
Loss in iteration 97 : 0.48163236422913613
Loss in iteration 98 : 0.4851632846977732
Loss in iteration 99 : 0.48228869912013567
Loss in iteration 100 : 0.4864486139247539
Loss in iteration 101 : 0.47913602964510027
Loss in iteration 102 : 0.4784796066924823
Loss in iteration 103 : 0.48686693183557606
Loss in iteration 104 : 0.48240263026105473
Loss in iteration 105 : 0.4712198152432524
Loss in iteration 106 : 0.48046273804735345
Loss in iteration 107 : 0.48857027053911467
Loss in iteration 108 : 0.49330850511161206
Loss in iteration 109 : 0.47980082848106587
Loss in iteration 110 : 0.475070647259096
Loss in iteration 111 : 0.4836734626329617
Loss in iteration 112 : 0.4768401533574608
Loss in iteration 113 : 0.4729963416283232
Loss in iteration 114 : 0.47320573904711044
Loss in iteration 115 : 0.4721980927214295
Loss in iteration 116 : 0.4789982343799049
Loss in iteration 117 : 0.4871790007863573
Loss in iteration 118 : 0.485591989636474
Loss in iteration 119 : 0.47196018140477025
Loss in iteration 120 : 0.48599303364817276
Loss in iteration 121 : 0.48317672090551084
Loss in iteration 122 : 0.48639248901957743
Loss in iteration 123 : 0.4798627271890684
Loss in iteration 124 : 0.47411773010625174
Loss in iteration 125 : 0.4737707662997051
Loss in iteration 126 : 0.4729232621121571
Loss in iteration 127 : 0.47448634223254293
Loss in iteration 128 : 0.4792675227302567
Loss in iteration 129 : 0.4737033397987216
Loss in iteration 130 : 0.47854187919715996
Loss in iteration 131 : 0.4744944995656189
Loss in iteration 132 : 0.4738003126060597
Loss in iteration 133 : 0.478398032587
Loss in iteration 134 : 0.47832329289831294
Loss in iteration 135 : 0.47603822499272525
Loss in iteration 136 : 0.47607173926192975
Loss in iteration 137 : 0.4783140143121589
Loss in iteration 138 : 0.47920598449266805
Loss in iteration 139 : 0.4838871455557966
Loss in iteration 140 : 0.4790318848145572
Loss in iteration 141 : 0.4798608669093496
Loss in iteration 142 : 0.4721036310807105
Loss in iteration 143 : 0.4754445473235274
Loss in iteration 144 : 0.471900418252535
Loss in iteration 145 : 0.4662831435453547
Loss in iteration 146 : 0.4785893126629659
Loss in iteration 147 : 0.4829299903307158
Loss in iteration 148 : 0.4745308542216038
Loss in iteration 149 : 0.48571445208725467
Loss in iteration 150 : 0.47902124795577933
Loss in iteration 151 : 0.472836738735398
Loss in iteration 152 : 0.48215061844451673
Loss in iteration 153 : 0.46274322912068
Loss in iteration 154 : 0.4814845889996217
Loss in iteration 155 : 0.4780584733936255
Loss in iteration 156 : 0.4628965990911046
Loss in iteration 157 : 0.4763638103894954
Loss in iteration 158 : 0.4678826746762846
Loss in iteration 159 : 0.4704920543299979
Loss in iteration 160 : 0.4696600985667597
Loss in iteration 161 : 0.4705297536465581
Loss in iteration 162 : 0.47203473553339614
Loss in iteration 163 : 0.47615881705779495
Loss in iteration 164 : 0.47559452244771894
Loss in iteration 165 : 0.468453763453601
Loss in iteration 166 : 0.471069130162476
Loss in iteration 167 : 0.4806007556552407
Loss in iteration 168 : 0.4799800406283346
Loss in iteration 169 : 0.4719492063398784
Loss in iteration 170 : 0.47723080326691686
Loss in iteration 171 : 0.4672523079350133
Loss in iteration 172 : 0.4742826947011501
Loss in iteration 173 : 0.47119910298469
Loss in iteration 174 : 0.46652930959851335
Loss in iteration 175 : 0.4802412285027694
Loss in iteration 176 : 0.46732228215767424
Loss in iteration 177 : 0.47877123520222975
Loss in iteration 178 : 0.46827961393552775
Loss in iteration 179 : 0.4770445479108921
Loss in iteration 180 : 0.4662652843689579
Loss in iteration 181 : 0.4735124740083401
Loss in iteration 182 : 0.47048628733098674
Loss in iteration 183 : 0.4694465975757609
Loss in iteration 184 : 0.48339522943586744
Loss in iteration 185 : 0.4703973845611965
Loss in iteration 186 : 0.46307836107615813
Loss in iteration 187 : 0.46553361782891634
Loss in iteration 188 : 0.47591968583250194
Loss in iteration 189 : 0.47575760523841315
Loss in iteration 190 : 0.4626548796909704
Loss in iteration 191 : 0.4651099407137716
Loss in iteration 192 : 0.4684472384611183
Loss in iteration 193 : 0.4720027825213523
Loss in iteration 194 : 0.47120017599624225
Loss in iteration 195 : 0.4783908687112722
Loss in iteration 196 : 0.4843261698696662
Loss in iteration 197 : 0.46153543120006024
Loss in iteration 198 : 0.4791303834899774
Loss in iteration 199 : 0.46949720549408186
Loss in iteration 200 : 0.46257758038546737
Loss in iteration 201 : 0.4699228161302978
Loss in iteration 202 : 0.4768322489186417
Loss in iteration 203 : 0.4608027915275647
Loss in iteration 204 : 0.46531682961187343
Loss in iteration 205 : 0.4702465915328659
Loss in iteration 206 : 0.46847525328478634
Loss in iteration 207 : 0.46580224183534236
Loss in iteration 208 : 0.46110544981083285
Loss in iteration 209 : 0.47373258696134285
Loss in iteration 210 : 0.47838416707734077
Loss in iteration 211 : 0.4721549624918412
Loss in iteration 212 : 0.47598602622628505
Loss in iteration 213 : 0.4762324263616431
Loss in iteration 214 : 0.48809597238525915
Loss in iteration 215 : 0.4776748672013733
Loss in iteration 216 : 0.46989205469243484
Loss in iteration 217 : 0.4676092334184147
Loss in iteration 218 : 0.46349907484152264
Loss in iteration 219 : 0.47445196067801704
Loss in iteration 220 : 0.4737503629261311
Loss in iteration 221 : 0.4720828524232767
Loss in iteration 222 : 0.46183216082030376
Loss in iteration 223 : 0.47532995538040396
Loss in iteration 224 : 0.47458347005615453
Loss in iteration 225 : 0.46753569288552516
Loss in iteration 226 : 0.47055516256930563
Loss in iteration 227 : 0.4609387129951475
Loss in iteration 228 : 0.47084559863625763
Loss in iteration 229 : 0.4713678299937564
Loss in iteration 230 : 0.46273533111352444
Loss in iteration 231 : 0.47107935872931944
Loss in iteration 232 : 0.4724262740645565
Loss in iteration 233 : 0.46679848046470673
Loss in iteration 234 : 0.47316171660728684
Loss in iteration 235 : 0.4630658663480031
Loss in iteration 236 : 0.46864487763180196
Loss in iteration 237 : 0.46953682673543196
Loss in iteration 238 : 0.4676713560090716
Loss in iteration 239 : 0.46739937861471503
Loss in iteration 240 : 0.4623585664141108
Loss in iteration 241 : 0.4635408484857839
Loss in iteration 242 : 0.48889053449443143
Loss in iteration 243 : 0.47713404712611573
Loss in iteration 244 : 0.4671635460175769
Loss in iteration 245 : 0.47974033415583694
Loss in iteration 246 : 0.46609715108784255
Loss in iteration 247 : 0.46119434076625054
Loss in iteration 248 : 0.4692021179947306
Loss in iteration 249 : 0.47229693858922944
Loss in iteration 250 : 0.46208023509776464
Loss in iteration 251 : 0.47304578597610425
Loss in iteration 252 : 0.4664966972912149
Loss in iteration 253 : 0.47662424201124404
Loss in iteration 254 : 0.454736473966232
Loss in iteration 255 : 0.4606961843206912
Loss in iteration 256 : 0.48069370068321376
Loss in iteration 257 : 0.45913567076747785
Loss in iteration 258 : 0.46369109297834915
Loss in iteration 259 : 0.4629149214338817
Loss in iteration 260 : 0.4666078323439294
Loss in iteration 261 : 0.45559270640065297
Loss in iteration 262 : 0.4661507341560803
Loss in iteration 263 : 0.4673936539592403
Loss in iteration 264 : 0.479785315146713
Loss in iteration 265 : 0.4587835788173763
Loss in iteration 266 : 0.47614857196497085
Loss in iteration 267 : 0.4722375025718314
Loss in iteration 268 : 0.47735608225909965
Loss in iteration 269 : 0.4619598222570542
Loss in iteration 270 : 0.467322814036098
Loss in iteration 271 : 0.4555997119180899
Loss in iteration 272 : 0.4677183985952579
Loss in iteration 273 : 0.4528303030641518
Loss in iteration 274 : 0.46174278762230175
Loss in iteration 275 : 0.4698359949388662
Loss in iteration 276 : 0.4727673134203914
Loss in iteration 277 : 0.4602705089086013
Loss in iteration 278 : 0.45871026116251834
Loss in iteration 279 : 0.45695230248206564
Loss in iteration 280 : 0.46225219852333793
Loss in iteration 281 : 0.46987186461029984
Loss in iteration 282 : 0.4627617773709413
Loss in iteration 283 : 0.46922807398896943
Loss in iteration 284 : 0.4542658413794555
Loss in iteration 285 : 0.4565367810389347
Loss in iteration 286 : 0.46565354879374204
Loss in iteration 287 : 0.4696956797650764
Loss in iteration 288 : 0.46911707607094283
Loss in iteration 289 : 0.46202776503975856
Loss in iteration 290 : 0.45719814254810437
Loss in iteration 291 : 0.47005818676296474
Loss in iteration 292 : 0.46758919535066296
Loss in iteration 293 : 0.46143785430819373
Loss in iteration 294 : 0.47145877072678577
Loss in iteration 295 : 0.45818401925416535
Loss in iteration 296 : 0.46069863657143467
Loss in iteration 297 : 0.46913922116201673
Loss in iteration 298 : 0.4663814648274246
Loss in iteration 299 : 0.46751679264665963
Loss in iteration 300 : 0.4704155308935234
Loss in iteration 301 : 0.46917163577722204
Loss in iteration 302 : 0.4638291420288168
Loss in iteration 303 : 0.45792248525473095
Loss in iteration 304 : 0.46126481302794775
Loss in iteration 305 : 0.4623770226992072
Loss in iteration 306 : 0.4629747975244568
Loss in iteration 307 : 0.46473772923658935
Loss in iteration 308 : 0.4666673901015453
Loss in iteration 309 : 0.4574895585784598
Loss in iteration 310 : 0.46318762785262485
Loss in iteration 311 : 0.4720915264305725
Loss in iteration 312 : 0.48126258011793077
Loss in iteration 313 : 0.45355623307591997
Loss in iteration 314 : 0.4602726888022884
Loss in iteration 315 : 0.46264056308417206
Loss in iteration 316 : 0.479421201346078
Loss in iteration 317 : 0.4638282224570433
Loss in iteration 318 : 0.47533919871179314
Loss in iteration 319 : 0.4602717001519272
Loss in iteration 320 : 0.45892143595354634
Loss in iteration 321 : 0.45977195311080105
Loss in iteration 322 : 0.46282007540141945
Loss in iteration 323 : 0.4614980346672933
Loss in iteration 324 : 0.46298605947715155
Loss in iteration 325 : 0.46178897961893406
Loss in iteration 326 : 0.47703002481233103
Loss in iteration 327 : 0.4564393926343022
Loss in iteration 328 : 0.45903138474394284
Loss in iteration 329 : 0.46038965952525285
Loss in iteration 330 : 0.4557343549863112
Loss in iteration 331 : 0.46412766164852537
Loss in iteration 332 : 0.45183842514090655
Loss in iteration 333 : 0.4532901029498576
Loss in iteration 334 : 0.4633591525257959
Loss in iteration 335 : 0.456525172622916
Loss in iteration 336 : 0.4633914545639616
Loss in iteration 337 : 0.4666263454791111
Loss in iteration 338 : 0.4719880092805653
Loss in iteration 339 : 0.47499629828153506
Loss in iteration 340 : 0.4687062721505488
Loss in iteration 341 : 0.4615016743040721
Loss in iteration 342 : 0.46343511140372357
Loss in iteration 343 : 0.4627113433530226
Loss in iteration 344 : 0.4690722827307756
Loss in iteration 345 : 0.46302058558655307
Loss in iteration 346 : 0.4577836711100219
Loss in iteration 347 : 0.4646233233003587
Loss in iteration 348 : 0.46555300782648473
Loss in iteration 349 : 0.47032139634122677
Loss in iteration 350 : 0.4694934189349453
Loss in iteration 351 : 0.46493436582511777
Loss in iteration 352 : 0.46189293813477333
Loss in iteration 353 : 0.4572694108907915
Loss in iteration 354 : 0.46373683199236776
Loss in iteration 355 : 0.4635804218852377
Loss in iteration 356 : 0.4569973527693599
Loss in iteration 357 : 0.4617137504205561
Loss in iteration 358 : 0.46910015513266595
Loss in iteration 359 : 0.46211787100105234
Loss in iteration 360 : 0.46859214367886604
Loss in iteration 361 : 0.45512516123238717
Loss in iteration 362 : 0.47350035272101554
Loss in iteration 363 : 0.46441755213828345
Loss in iteration 364 : 0.46331903137968145
Loss in iteration 365 : 0.466197422740608
Loss in iteration 366 : 0.46103753799359876
Loss in iteration 367 : 0.46937542058728754
Loss in iteration 368 : 0.47141060604684676
Loss in iteration 369 : 0.47530279643631135
Loss in iteration 370 : 0.4604837655742543
Loss in iteration 371 : 0.47771490162055397
Loss in iteration 372 : 0.4570626698114989
Loss in iteration 373 : 0.46387640859705886
Loss in iteration 374 : 0.46221806826204825
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.79125, training accuracy 0.79125, time elapsed: 5183 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 31.640444342255385
Loss in iteration 3 : 7.297672066142016
Loss in iteration 4 : 18.01301033212898
Loss in iteration 5 : 8.990031130702372
Loss in iteration 6 : 10.07148346508921
Loss in iteration 7 : 11.200306809490714
Loss in iteration 8 : 4.779415229336995
Loss in iteration 9 : 7.741133672300495
Loss in iteration 10 : 9.626141918691479
Loss in iteration 11 : 4.786626015530068
Loss in iteration 12 : 4.652321980328687
Loss in iteration 13 : 6.4443249167806105
Loss in iteration 14 : 4.198574918480264
Loss in iteration 15 : 3.7531835506048785
Loss in iteration 16 : 5.509577787295525
Loss in iteration 17 : 4.715384520118818
Loss in iteration 18 : 2.9693342147324757
Loss in iteration 19 : 4.076124428840386
Loss in iteration 20 : 4.934906930046724
Loss in iteration 21 : 3.2303356059257933
Loss in iteration 22 : 3.2940716953293534
Loss in iteration 23 : 4.1901929811645005
Loss in iteration 24 : 3.5768206744762607
Loss in iteration 25 : 2.826560052076922
Loss in iteration 26 : 3.2438533249643218
Loss in iteration 27 : 3.3784651748375802
Loss in iteration 28 : 2.754113425862544
Loss in iteration 29 : 2.9287552593913357
Loss in iteration 30 : 3.2319595125009095
Loss in iteration 31 : 2.6215637977425805
Loss in iteration 32 : 2.3101737788364334
Loss in iteration 33 : 2.689634549556739
Loss in iteration 34 : 2.284663181741038
Loss in iteration 35 : 2.157493507426868
Loss in iteration 36 : 2.46520497259208
Loss in iteration 37 : 2.0709436599085063
Loss in iteration 38 : 1.9864379236953464
Loss in iteration 39 : 2.091785817908332
Loss in iteration 40 : 1.7625714553077751
Loss in iteration 41 : 1.8724858532487065
Loss in iteration 42 : 1.8473871901068957
Loss in iteration 43 : 1.5648639352472533
Loss in iteration 44 : 1.6796961950497753
Loss in iteration 45 : 1.5708945280759785
Loss in iteration 46 : 1.4716605889937417
Loss in iteration 47 : 1.5412083290174219
Loss in iteration 48 : 1.3122789891554865
Loss in iteration 49 : 1.4967204369415856
Loss in iteration 50 : 1.221193641148768
Loss in iteration 51 : 1.3696261841844422
Loss in iteration 52 : 1.2270826020100585
Loss in iteration 53 : 1.2438732824990024
Loss in iteration 54 : 1.1733554381726141
Loss in iteration 55 : 1.177525982087822
Loss in iteration 56 : 1.1841089878739908
Loss in iteration 57 : 1.0994543019257865
Loss in iteration 58 : 1.0153480928781373
Loss in iteration 59 : 1.0620128982338009
Loss in iteration 60 : 0.9888464571680896
Loss in iteration 61 : 1.0848354558085471
Loss in iteration 62 : 0.9707500334006128
Loss in iteration 63 : 0.96363478289188
Loss in iteration 64 : 1.0413225495167981
Loss in iteration 65 : 0.8817569967143413
Loss in iteration 66 : 0.8726071846026642
Loss in iteration 67 : 0.9449760826857313
Loss in iteration 68 : 0.8010192780513964
Loss in iteration 69 : 0.7567397063720348
Loss in iteration 70 : 0.7821329739709681
Loss in iteration 71 : 0.772620336596081
Loss in iteration 72 : 0.6711443395361177
Loss in iteration 73 : 0.6550366501347928
Loss in iteration 74 : 0.6796500842221791
Loss in iteration 75 : 0.7787834640023007
Loss in iteration 76 : 0.9609366411634055
Loss in iteration 77 : 1.054145619739855
Loss in iteration 78 : 1.1231548499243362
Loss in iteration 79 : 1.0418294069389211
Loss in iteration 80 : 0.8362847079086033
Loss in iteration 81 : 0.7861671501926043
Loss in iteration 82 : 0.6665410504678567
Loss in iteration 83 : 0.6882551124120917
Loss in iteration 84 : 0.6487800513280915
Loss in iteration 85 : 0.709240149883944
Loss in iteration 86 : 0.8494531414837464
Loss in iteration 87 : 1.410382818206731
Loss in iteration 88 : 1.471063091786434
Loss in iteration 89 : 1.551152304756875
Loss in iteration 90 : 0.9289155016235049
Loss in iteration 91 : 0.9059150092271455
Loss in iteration 92 : 0.7887342809727894
Loss in iteration 93 : 0.9803921377506085
Loss in iteration 94 : 0.9361294275548576
Loss in iteration 95 : 1.0032236346024905
Loss in iteration 96 : 0.8005412903271186
Loss in iteration 97 : 0.7103672508754559
Loss in iteration 98 : 0.6193466379849764
Loss in iteration 99 : 0.6324658366502747
Loss in iteration 100 : 0.6059368912408775
Loss in iteration 101 : 0.7133641996959419
Loss in iteration 102 : 1.1758784165544953
Loss in iteration 103 : 2.133474272189823
Loss in iteration 104 : 1.1750701760578057
Loss in iteration 105 : 0.7355126949504383
Loss in iteration 106 : 0.6317933086492569
Loss in iteration 107 : 0.6761319922116025
Loss in iteration 108 : 0.6550422423465907
Loss in iteration 109 : 0.7216881723996503
Loss in iteration 110 : 1.1180322452374403
Loss in iteration 111 : 1.756452868150066
Loss in iteration 112 : 1.2956780067837
Loss in iteration 113 : 0.7266818135001694
Loss in iteration 114 : 0.5967198652387214
Loss in iteration 115 : 0.593814701465291
Loss in iteration 116 : 0.5991973164039238
Loss in iteration 117 : 0.6668592084390488
Loss in iteration 118 : 0.7176257068699026
Loss in iteration 119 : 1.0010190328534578
Loss in iteration 120 : 1.356935189191076
Loss in iteration 121 : 1.3950195448315452
Loss in iteration 122 : 0.9253801089808547
Loss in iteration 123 : 0.9289406617011444
Loss in iteration 124 : 0.7884859392418656
Loss in iteration 125 : 1.0129094033180512
Loss in iteration 126 : 0.8435760248440117
Loss in iteration 127 : 0.9722164167353011
Loss in iteration 128 : 0.7948491316232871
Loss in iteration 129 : 0.8667088202222499
Loss in iteration 130 : 0.8631186416342139
Loss in iteration 131 : 1.0143740257185425
Loss in iteration 132 : 1.1923617913511593
Loss in iteration 133 : 1.2717029182154074
Loss in iteration 134 : 0.9882294854775857
Loss in iteration 135 : 0.7306126643342892
Loss in iteration 136 : 0.6010049788592264
Loss in iteration 137 : 0.6218399951603759
Loss in iteration 138 : 0.5881528503974521
Loss in iteration 139 : 0.6855917225117417
Loss in iteration 140 : 0.9111810136164343
Loss in iteration 141 : 1.6481747483601792
Loss in iteration 142 : 1.921055273192454
Loss in iteration 143 : 0.9896150009806995
Loss in iteration 144 : 0.6925833438684605
Loss in iteration 145 : 0.594948321962788
Loss in iteration 146 : 0.6474847582103329
Loss in iteration 147 : 0.6101284763357252
Loss in iteration 148 : 0.5853000189302431
Loss in iteration 149 : 0.5824783162076839
Loss in iteration 150 : 0.5668706224161622
Loss in iteration 151 : 0.6538712548629442
Loss in iteration 152 : 1.2131815298897732
Loss in iteration 153 : 3.011970823883281
Loss in iteration 154 : 0.9147258409912316
Loss in iteration 155 : 0.9030416535894074
Loss in iteration 156 : 3.1102157532162167
Loss in iteration 157 : 0.9063129060415007
Loss in iteration 158 : 2.143096471185154
Loss in iteration 159 : 2.531199204121352
Loss in iteration 160 : 1.1845319745960825
Loss in iteration 161 : 3.2657210995145496
Loss in iteration 162 : 1.0842962966066068
Loss in iteration 163 : 2.984473225739413
Loss in iteration 164 : 0.8447785552640319
Loss in iteration 165 : 2.681295372001207
Loss in iteration 166 : 1.0790776167601093
Loss in iteration 167 : 2.4085170388668145
Loss in iteration 168 : 1.044111501049739
Loss in iteration 169 : 2.016076712199538
Loss in iteration 170 : 1.1575261575963514
Loss in iteration 171 : 1.6295706964359653
Loss in iteration 172 : 1.0885911505017134
Loss in iteration 173 : 1.5786939095406287
Loss in iteration 174 : 1.0069905656078288
Loss in iteration 175 : 1.3909879888157788
Loss in iteration 176 : 1.032451303506221
Loss in iteration 177 : 1.2817074867342806
Loss in iteration 178 : 1.0395761073089151
Loss in iteration 179 : 0.9488460897843554
Loss in iteration 180 : 1.090031204077002
Loss in iteration 181 : 0.8838776200880643
Loss in iteration 182 : 1.0091727143878888
Loss in iteration 183 : 0.9641669353048664
Loss in iteration 184 : 0.854217010618388
Loss in iteration 185 : 0.8924858501886463
Loss in iteration 186 : 0.90006913692001
Loss in iteration 187 : 0.8576359444288657
Loss in iteration 188 : 0.6674203019602611
Loss in iteration 189 : 0.7714468431464915
Loss in iteration 190 : 0.6866219474963847
Loss in iteration 191 : 0.6984292764168575
Loss in iteration 192 : 0.9285399266967345
Loss in iteration 193 : 1.1860089559934077
Loss in iteration 194 : 1.9992603643981053
Loss in iteration 195 : 1.2441498499737074
Loss in iteration 196 : 0.8836242823072363
Loss in iteration 197 : 0.6692989063482746
Loss in iteration 198 : 0.7171132661817704
Loss in iteration 199 : 0.5815503938591661
Loss in iteration 200 : 0.6364186917321476
Loss in iteration 201 : 0.5996684076348614
Loss in iteration 202 : 0.7387120717195481
Loss in iteration 203 : 1.0103973832943158
Loss in iteration 204 : 1.4566118819404448
Loss in iteration 205 : 1.7361050038758685
Loss in iteration 206 : 0.7972786567721729
Loss in iteration 207 : 0.6999910884183924
Loss in iteration 208 : 0.6934268573494089
Loss in iteration 209 : 0.9779726075660233
Loss in iteration 210 : 1.2311726872429225
Loss in iteration 211 : 0.9267571751666557
Loss in iteration 212 : 1.0732370933873745
Loss in iteration 213 : 0.6563191977261403
Loss in iteration 214 : 0.7959342883716763
Loss in iteration 215 : 0.781516048791507
Loss in iteration 216 : 0.615580508850912
Loss in iteration 217 : 0.8648798220111781
Loss in iteration 218 : 0.9779980927989942
Loss in iteration 219 : 2.3209262803874178
Loss in iteration 220 : 1.3805079013920338
Loss in iteration 221 : 0.6919900327407272
Loss in iteration 222 : 0.6873533771464595
Loss in iteration 223 : 0.9312128652355615
Loss in iteration 224 : 1.1019837581668992
Loss in iteration 225 : 1.6162064980667772
Loss in iteration 226 : 1.6414874523328369
Loss in iteration 227 : 0.6315377762275644
Loss in iteration 228 : 0.7894675706720502
Loss in iteration 229 : 1.2707043135929206
Loss in iteration 230 : 1.2227659409562903
Loss in iteration 231 : 1.2232360996147227
Loss in iteration 232 : 0.765873442513962
Loss in iteration 233 : 0.6439532701206913
Loss in iteration 234 : 1.0517797033571703
Loss in iteration 235 : 1.2285934424719782
Loss in iteration 236 : 1.2149660560824502
Loss in iteration 237 : 0.7162412064576187
Loss in iteration 238 : 0.6724099859080057
Loss in iteration 239 : 1.2072663112185817
Loss in iteration 240 : 1.3996023804645676
Loss in iteration 241 : 0.9692732902015481
Loss in iteration 242 : 0.6610234783127936
Loss in iteration 243 : 0.856971999105816
Loss in iteration 244 : 1.3584444567837743
Loss in iteration 245 : 1.5522832169785903
Loss in iteration 246 : 1.3313846544323598
Loss in iteration 247 : 0.6133111097083551
Loss in iteration 248 : 0.8300429824905524
Loss in iteration 249 : 1.0579228472086102
Loss in iteration 250 : 1.3152471942044428
Loss in iteration 251 : 1.5076421065911232
Loss in iteration 252 : 0.6713462108630421
Loss in iteration 253 : 0.8542190490972968
Loss in iteration 254 : 0.8790531056906524
Loss in iteration 255 : 1.0680435819969982
Loss in iteration 256 : 1.6594173164001735
Loss in iteration 257 : 0.8907593386509453
Loss in iteration 258 : 0.8128053068912293
Loss in iteration 259 : 0.6609339528232833
Loss in iteration 260 : 0.6577832751139647
Loss in iteration 261 : 0.7131584025125187
Loss in iteration 262 : 0.779890272326213
Loss in iteration 263 : 1.308679075521804
Loss in iteration 264 : 1.669680637924042
Loss in iteration 265 : 1.728751468126629
Loss in iteration 266 : 1.0566832417020597
Loss in iteration 267 : 0.6922776890464073
Loss in iteration 268 : 0.706762565061203
Loss in iteration 269 : 0.7165554146591141
Loss in iteration 270 : 0.986521732895949
Loss in iteration 271 : 1.5013996152054814
Loss in iteration 272 : 1.684859935250332
Loss in iteration 273 : 1.17996686334506
Loss in iteration 274 : 0.8149822028703785
Loss in iteration 275 : 0.6395289142038223
Loss in iteration 276 : 0.7019762379871032
Loss in iteration 277 : 0.6291144312936472
Loss in iteration 278 : 0.6530079555205582
Loss in iteration 279 : 0.681365191016219
Loss in iteration 280 : 0.7740567752659243
Loss in iteration 281 : 1.3117539649713479
Loss in iteration 282 : 2.070631507320868
Loss in iteration 283 : 1.5165450777904825
Loss in iteration 284 : 0.6998102719121349
Loss in iteration 285 : 0.5901089762252301
Loss in iteration 286 : 0.9257951851346761
Loss in iteration 287 : 1.4633807370828806
Loss in iteration 288 : 1.4309294071848089
Loss in iteration 289 : 0.7896183923922531
Loss in iteration 290 : 0.6169278918847381
Loss in iteration 291 : 0.9927078177653362
Loss in iteration 292 : 1.6460090021854776
Loss in iteration 293 : 1.3149771406570632
Loss in iteration 294 : 0.7707398095062797
Loss in iteration 295 : 0.6248373222811606
Loss in iteration 296 : 0.6912615584973002
Loss in iteration 297 : 1.0312827731573646
Loss in iteration 298 : 1.481987679541692
Loss in iteration 299 : 1.346497542599306
Loss in iteration 300 : 0.9413229683826738
Loss in iteration 301 : 0.7360461946306279
Loss in iteration 302 : 0.6542489750108
Loss in iteration 303 : 0.583513477834746
Loss in iteration 304 : 0.6009210904065929
Loss in iteration 305 : 0.5768852392783416
Loss in iteration 306 : 0.675382123393685
Loss in iteration 307 : 1.2017580793811473
Loss in iteration 308 : 2.5804407935864986
Loss in iteration 309 : 1.5090515162288485
Loss in iteration 310 : 0.5834176765176523
Loss in iteration 311 : 0.9605950224399975
Loss in iteration 312 : 1.8308032774078247
Loss in iteration 313 : 1.1446048921434497
Loss in iteration 314 : 0.7081568193341646
Loss in iteration 315 : 0.9718250890933456
Loss in iteration 316 : 1.4342186232753709
Loss in iteration 317 : 1.1303945359628356
Loss in iteration 318 : 0.6855908992627746
Loss in iteration 319 : 1.2266458077139735
Loss in iteration 320 : 1.323026154297166
Loss in iteration 321 : 0.6487741215383631
Loss in iteration 322 : 0.916803951319134
Loss in iteration 323 : 0.8409508041596558
Loss in iteration 324 : 0.7245242229377084
Loss in iteration 325 : 1.0541869865605609
Loss in iteration 326 : 0.846747181347021
Loss in iteration 327 : 0.8952129003590273
Loss in iteration 328 : 0.8300491686283222
Loss in iteration 329 : 0.6722474032120818
Loss in iteration 330 : 0.6736792556081932
Loss in iteration 331 : 0.6139293648924297
Loss in iteration 332 : 0.5609259469420003
Loss in iteration 333 : 0.6089082407345444
Loss in iteration 334 : 0.6642282496845797
Loss in iteration 335 : 1.356496556188169
Loss in iteration 336 : 2.875083548369993
Loss in iteration 337 : 0.9644041758558006
Loss in iteration 338 : 0.585070002477471
Loss in iteration 339 : 1.3299427334436422
Loss in iteration 340 : 2.0073510200852085
Loss in iteration 341 : 1.3714452650431337
Loss in iteration 342 : 0.6397452647879489
Loss in iteration 343 : 0.947947696382511
Loss in iteration 344 : 1.4812915701281526
Loss in iteration 345 : 0.8872353642875195
Loss in iteration 346 : 0.6425993298708499
Loss in iteration 347 : 0.9391274870894052
Loss in iteration 348 : 1.1161621574802543
Loss in iteration 349 : 0.8315607769646274
Loss in iteration 350 : 0.6907932361280845
Loss in iteration 351 : 1.0106326858695145
Loss in iteration 352 : 1.0507214652126862
Loss in iteration 353 : 0.692510751952024
Loss in iteration 354 : 0.7554473173303253
Loss in iteration 355 : 0.6667665235767154
Loss in iteration 356 : 0.7268790405483675
Loss in iteration 357 : 0.7689129730411913
Loss in iteration 358 : 0.7101220076232839
Loss in iteration 359 : 1.0453231986448157
Loss in iteration 360 : 1.2126911788677388
Loss in iteration 361 : 1.6724670246440316
Loss in iteration 362 : 1.2200127608191105
Loss in iteration 363 : 0.7999444682510273
Loss in iteration 364 : 0.6573151758613175
Loss in iteration 365 : 0.6911341207926724
Loss in iteration 366 : 0.6595089875614316
Loss in iteration 367 : 0.9448833075881728
Loss in iteration 368 : 1.4077777388743973
Loss in iteration 369 : 1.949640088767509
Loss in iteration 370 : 0.9989008238634732
Loss in iteration 371 : 0.6440219553518005
Loss in iteration 372 : 0.729175403236621
Loss in iteration 373 : 1.2617700220364911
Loss in iteration 374 : 1.564597551914457
Loss in iteration 375 : 0.8904120539481414
Loss in iteration 376 : 0.6651503742598676
Loss in iteration 377 : 0.782363538966304
Loss in iteration 378 : 1.0612281272905633
Loss in iteration 379 : 1.0357309777805863
Loss in iteration 380 : 0.6501805019365821
Loss in iteration 381 : 0.613986355688393
Loss in iteration 382 : 0.6627886671599902
Loss in iteration 383 : 0.9082706903936958
Loss in iteration 384 : 1.3917945907632658
Loss in iteration 385 : 1.2046857028536486
Loss in iteration 386 : 1.1786639452897525
Loss in iteration 387 : 0.8974631860404308
Loss in iteration 388 : 0.9262899995194909
Loss in iteration 389 : 0.770414658213506
Loss in iteration 390 : 0.711691679565156
Loss in iteration 391 : 0.6594004251193782
Loss in iteration 392 : 0.7222044090260634
Loss in iteration 393 : 0.9568861580773429
Loss in iteration 394 : 1.495156081059536
Loss in iteration 395 : 1.672230999107271
Loss in iteration 396 : 1.2615127962786328
Loss in iteration 397 : 0.7957610184579952
Loss in iteration 398 : 0.67977768182058
Loss in iteration 399 : 0.5824796144948059
Loss in iteration 400 : 0.6953288938647435
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.712375, training accuracy 0.712375, time elapsed: 5697 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.8713469854759814
Loss in iteration 3 : 4.081758473573131
Loss in iteration 4 : 0.8380440201425978
Loss in iteration 5 : 4.875176369588857
Loss in iteration 6 : 4.043679835964921
Loss in iteration 7 : 1.4277625520959594
Loss in iteration 8 : 2.3551991103621894
Loss in iteration 9 : 1.186843854117398
Loss in iteration 10 : 1.8117089130371513
Loss in iteration 11 : 0.8636469136832918
Loss in iteration 12 : 1.5897699868566875
Loss in iteration 13 : 0.9461359990602811
Loss in iteration 14 : 1.261449611295947
Loss in iteration 15 : 1.2223849423537057
Loss in iteration 16 : 1.035967668093103
Loss in iteration 17 : 1.2927713383082087
Loss in iteration 18 : 0.9170572980337584
Loss in iteration 19 : 1.1410815056095325
Loss in iteration 20 : 1.0413395588143035
Loss in iteration 21 : 0.9967390915814983
Loss in iteration 22 : 1.1419176079558278
Loss in iteration 23 : 0.8834061649748756
Loss in iteration 24 : 1.0824858116466836
Loss in iteration 25 : 0.8649857904723245
Loss in iteration 26 : 0.8878921367198767
Loss in iteration 27 : 0.857316499284978
Loss in iteration 28 : 0.8229451020320074
Loss in iteration 29 : 0.8195653397279367
Loss in iteration 30 : 0.7591437513057516
Loss in iteration 31 : 0.7849467028553453
Loss in iteration 32 : 0.6946996398397548
Loss in iteration 33 : 0.6981398886695465
Loss in iteration 34 : 0.6517376306764543
Loss in iteration 35 : 0.6890473194992017
Loss in iteration 36 : 0.6693184055786381
Loss in iteration 37 : 0.623423272459119
Loss in iteration 38 : 0.6412135300144585
Loss in iteration 39 : 0.5795541369619166
Loss in iteration 40 : 0.6526356836738693
Loss in iteration 41 : 0.6021598270517511
Loss in iteration 42 : 0.5828635236411122
Loss in iteration 43 : 0.6336921152639938
Loss in iteration 44 : 0.6138041057272303
Loss in iteration 45 : 0.549301601422319
Loss in iteration 46 : 0.6232519836901668
Loss in iteration 47 : 0.6561282537042282
Loss in iteration 48 : 0.6048073489081285
Loss in iteration 49 : 0.51950988857403
Loss in iteration 50 : 0.554944731691425
Loss in iteration 51 : 0.6121919225214357
Loss in iteration 52 : 0.525769389676648
Loss in iteration 53 : 0.5086717269649003
Loss in iteration 54 : 0.5817435396412018
Loss in iteration 55 : 0.5961915788398848
Loss in iteration 56 : 0.5318082261363001
Loss in iteration 57 : 0.46935694267603834
Loss in iteration 58 : 0.49720903745815054
Loss in iteration 59 : 0.5125860301637689
Loss in iteration 60 : 0.5096644425676801
Loss in iteration 61 : 0.4789009950250206
Loss in iteration 62 : 0.4781834228852574
Loss in iteration 63 : 0.49711648610125836
Loss in iteration 64 : 0.5135839825326245
Loss in iteration 65 : 0.5390118961008921
Loss in iteration 66 : 0.5056643461216357
Loss in iteration 67 : 0.4973870630210792
Loss in iteration 68 : 0.466556664258674
Loss in iteration 69 : 0.4827264508282175
Loss in iteration 70 : 0.5077485922529688
Loss in iteration 71 : 0.614749241765497
Loss in iteration 72 : 0.6955006023383359
Loss in iteration 73 : 0.764148411730014
Loss in iteration 74 : 0.6646021055631239
Loss in iteration 75 : 0.6150383454901918
Loss in iteration 76 : 0.5643685998923997
Loss in iteration 77 : 0.5229014083465976
Loss in iteration 78 : 0.5177838413692566
Loss in iteration 79 : 0.4987570720529158
Loss in iteration 80 : 0.5195040823238931
Loss in iteration 81 : 0.54486243545633
Loss in iteration 82 : 0.5555398732427203
Loss in iteration 83 : 0.5630179954166423
Loss in iteration 84 : 0.5855956035541581
Loss in iteration 85 : 0.5990046430037101
Loss in iteration 86 : 0.6230623393904148
Loss in iteration 87 : 0.5804224481714042
Loss in iteration 88 : 0.5380901415034509
Loss in iteration 89 : 0.47241680673232966
Loss in iteration 90 : 0.47398527239173255
Loss in iteration 91 : 0.512174455919125
Loss in iteration 92 : 0.5296089548659693
Loss in iteration 93 : 0.5465681288959044
Loss in iteration 94 : 0.5432279562941655
Loss in iteration 95 : 0.5379487997378876
Loss in iteration 96 : 0.5377012437315918
Loss in iteration 97 : 0.539942380156648
Loss in iteration 98 : 0.6075734925669082
Loss in iteration 99 : 0.6684441797325859
Loss in iteration 100 : 0.6893869557015698
Loss in iteration 101 : 0.6159800616379868
Loss in iteration 102 : 0.5383515262579861
Loss in iteration 103 : 0.4920151564721505
Loss in iteration 104 : 0.4750366880248828
Loss in iteration 105 : 0.4621584423410465
Loss in iteration 106 : 0.5036372490769079
Loss in iteration 107 : 0.5334945285732305
Loss in iteration 108 : 0.5302139598845517
Loss in iteration 109 : 0.5044092015301622
Loss in iteration 110 : 0.493813004226156
Loss in iteration 111 : 0.5112899820113354
Loss in iteration 112 : 0.5567520637135741
Loss in iteration 113 : 0.6957632320316288
Loss in iteration 114 : 0.7805450191988308
Loss in iteration 115 : 0.6627820301443808
Loss in iteration 116 : 0.5058157594326806
Loss in iteration 117 : 0.5340259807984659
Loss in iteration 118 : 0.6351354597157629
Loss in iteration 119 : 0.6288666394956567
Loss in iteration 120 : 0.5969196636955264
Loss in iteration 121 : 0.5106794743983182
Loss in iteration 122 : 0.5064812994909751
Loss in iteration 123 : 0.4912157987391889
Loss in iteration 124 : 0.545310974318376
Loss in iteration 125 : 0.6582090560397866
Loss in iteration 126 : 0.7219618263301866
Loss in iteration 127 : 0.6352023107880964
Loss in iteration 128 : 0.5119065724047007
Loss in iteration 129 : 0.5015194915358587
Loss in iteration 130 : 0.531145003138732
Loss in iteration 131 : 0.5828457187469936
Loss in iteration 132 : 0.5246486060960347
Loss in iteration 133 : 0.5182279041787132
Loss in iteration 134 : 0.4869269179696856
Loss in iteration 135 : 0.5138537072188049
Loss in iteration 136 : 0.539779726132989
Loss in iteration 137 : 0.6584621105185808
Loss in iteration 138 : 0.7093278157291708
Loss in iteration 139 : 0.7051381579941705
Loss in iteration 140 : 0.6407516030925077
Loss in iteration 141 : 0.6280088423899808
Loss in iteration 142 : 0.6011113997523865
Loss in iteration 143 : 0.578820221396432
Loss in iteration 144 : 0.5008831559390213
Loss in iteration 145 : 0.4719476285567413
Loss in iteration 146 : 0.5012176816230588
Loss in iteration 147 : 0.511499695568174
Loss in iteration 148 : 0.4909292419628425
Loss in iteration 149 : 0.5137974612104569
Loss in iteration 150 : 0.4985677522567991
Loss in iteration 151 : 0.509691715973999
Loss in iteration 152 : 0.5637572193179686
Loss in iteration 153 : 0.6386741514028013
Loss in iteration 154 : 0.8744709202027583
Loss in iteration 155 : 0.8938194886229395
Loss in iteration 156 : 0.7514005301318616
Loss in iteration 157 : 0.5913014945629235
Loss in iteration 158 : 0.551745746295383
Loss in iteration 159 : 0.5061128089966098
Loss in iteration 160 : 0.5326664882443759
Loss in iteration 161 : 0.4951250909327731
Loss in iteration 162 : 0.5165286553732832
Loss in iteration 163 : 0.4933176025542253
Loss in iteration 164 : 0.5112797901982215
Loss in iteration 165 : 0.4802712841452119
Loss in iteration 166 : 0.500921034572927
Loss in iteration 167 : 0.502960924088869
Loss in iteration 168 : 0.5483901397098202
Loss in iteration 169 : 0.5820555168232455
Loss in iteration 170 : 0.7293509244719205
Loss in iteration 171 : 0.7401522745573786
Loss in iteration 172 : 0.7753261847351931
Loss in iteration 173 : 0.5794775983659735
Loss in iteration 174 : 0.5662832158655098
Loss in iteration 175 : 0.5300374807353417
Loss in iteration 176 : 0.5321681048373496
Loss in iteration 177 : 0.5363408505045488
Loss in iteration 178 : 0.5420304407726618
Loss in iteration 179 : 0.5815839234447416
Loss in iteration 180 : 0.6392235594593157
Loss in iteration 181 : 0.7977033695843431
Loss in iteration 182 : 0.8292086117858319
Loss in iteration 183 : 0.7289151443830351
Loss in iteration 184 : 0.6573441041953344
Loss in iteration 185 : 0.5490720473231924
Loss in iteration 186 : 0.4836924360442827
Loss in iteration 187 : 0.511640123257719
Loss in iteration 188 : 0.5616429842245478
Loss in iteration 189 : 0.5357108467509809
Loss in iteration 190 : 0.49449091007028273
Loss in iteration 191 : 0.4839221697914104
Loss in iteration 192 : 0.4784728940785258
Loss in iteration 193 : 0.49758031378495043
Loss in iteration 194 : 0.49155727694187157
Loss in iteration 195 : 0.496235732970741
Loss in iteration 196 : 0.5160064405875515
Loss in iteration 197 : 0.47541879094975864
Loss in iteration 198 : 0.5417601820872398
Loss in iteration 199 : 0.6823168418498147
Loss in iteration 200 : 0.9869873903032551
Loss in iteration 201 : 1.1302136889050662
Loss in iteration 202 : 0.6391131403948521
Loss in iteration 203 : 0.5434511848153281
Loss in iteration 204 : 0.653595828138814
Loss in iteration 205 : 0.9472703949526744
Loss in iteration 206 : 0.8984018450833965
Loss in iteration 207 : 0.6066477108806323
Loss in iteration 208 : 0.5484067025325354
Loss in iteration 209 : 0.6424725415341798
Loss in iteration 210 : 0.7492814665862475
Loss in iteration 211 : 0.6270063469525401
Loss in iteration 212 : 0.5717170164726533
Loss in iteration 213 : 0.5224098235168676
Loss in iteration 214 : 0.6091731826915234
Loss in iteration 215 : 0.6126891667218557
Loss in iteration 216 : 0.6278727539936456
Loss in iteration 217 : 0.5886289823333395
Loss in iteration 218 : 0.5037936382517538
Loss in iteration 219 : 0.5117887156218108
Loss in iteration 220 : 0.5738201123259641
Loss in iteration 221 : 0.6998698772450146
Loss in iteration 222 : 0.8846685205841165
Loss in iteration 223 : 0.8478309069561747
Loss in iteration 224 : 0.6022781653832605
Loss in iteration 225 : 0.4912735051472806
Loss in iteration 226 : 0.5033391625381377
Loss in iteration 227 : 0.5235872708260342
Loss in iteration 228 : 0.6292623164559209
Loss in iteration 229 : 0.6948002008020856
Loss in iteration 230 : 0.7904333684395682
Loss in iteration 231 : 0.7767688363366724
Loss in iteration 232 : 0.7469761113482905
Loss in iteration 233 : 0.6179090050266843
Loss in iteration 234 : 0.620084740703716
Loss in iteration 235 : 0.5440662229346906
Loss in iteration 236 : 0.5232427274246877
Loss in iteration 237 : 0.5374698662504532
Loss in iteration 238 : 0.622298015480569
Loss in iteration 239 : 0.8021926943749279
Loss in iteration 240 : 0.8513453757658045
Loss in iteration 241 : 0.6261634989989252
Loss in iteration 242 : 0.5345738909298166
Loss in iteration 243 : 0.5613537967045026
Loss in iteration 244 : 0.742476753719346
Loss in iteration 245 : 0.9737372470099629
Loss in iteration 246 : 0.7918287375439631
Loss in iteration 247 : 0.5435767572206738
Loss in iteration 248 : 0.5192344571056186
Loss in iteration 249 : 0.6006406869775224
Loss in iteration 250 : 0.7184099471459365
Loss in iteration 251 : 0.7618499086574433
Loss in iteration 252 : 0.6753517148161989
Loss in iteration 253 : 0.5961201916422414
Loss in iteration 254 : 0.49752639251156544
Loss in iteration 255 : 0.4930080302766581
Loss in iteration 256 : 0.5568616610771854
Loss in iteration 257 : 0.6326291935656742
Loss in iteration 258 : 0.8360762965297031
Loss in iteration 259 : 0.9412974879498737
Loss in iteration 260 : 0.7849476445043663
Loss in iteration 261 : 0.5881454799371666
Loss in iteration 262 : 0.5555719853539239
Loss in iteration 263 : 0.49878413652768894
Loss in iteration 264 : 0.5652361880260329
Loss in iteration 265 : 0.501227733793116
Loss in iteration 266 : 0.5703504997600876
Loss in iteration 267 : 0.6200015588415213
Loss in iteration 268 : 0.793113835025719
Loss in iteration 269 : 0.9020362251690642
Loss in iteration 270 : 0.8219397304052097
Loss in iteration 271 : 0.746102365631768
Loss in iteration 272 : 0.5413560493725342
Loss in iteration 273 : 0.567695458568276
Loss in iteration 274 : 0.49883886950810347
Loss in iteration 275 : 0.579753297765408
Loss in iteration 276 : 0.554268724448712
Loss in iteration 277 : 0.5711669228083057
Loss in iteration 278 : 0.6124568920143864
Loss in iteration 279 : 0.6907046158622344
Loss in iteration 280 : 0.9415495522307507
Loss in iteration 281 : 0.7745284293311208
Loss in iteration 282 : 0.7724437491606271
Loss in iteration 283 : 0.6206808972360119
Loss in iteration 284 : 0.5612778063221102
Loss in iteration 285 : 0.5720236059734068
Loss in iteration 286 : 0.577637622893793
Loss in iteration 287 : 0.5894971689629456
Loss in iteration 288 : 0.5801697912618088
Loss in iteration 289 : 0.5311238554566959
Loss in iteration 290 : 0.5367131949463319
Loss in iteration 291 : 0.520502758420634
Loss in iteration 292 : 0.520167749459695
Loss in iteration 293 : 0.5474568511348615
Loss in iteration 294 : 0.4890375508524476
Loss in iteration 295 : 0.5368166019009377
Loss in iteration 296 : 0.46747741965982736
Loss in iteration 297 : 0.5362025258751973
Loss in iteration 298 : 0.48702064917066856
Loss in iteration 299 : 0.5075729459652832
Loss in iteration 300 : 0.5468695339593169
Loss in iteration 301 : 0.6250753736760787
Loss in iteration 302 : 1.0044519090652118
Loss in iteration 303 : 1.6525743093952567
Loss in iteration 304 : 0.6596490688459202
Loss in iteration 305 : 0.5643150334560532
Loss in iteration 306 : 0.7222630871430759
Loss in iteration 307 : 1.0261782229891392
Loss in iteration 308 : 0.9976859885603995
Loss in iteration 309 : 0.5503683701385466
Loss in iteration 310 : 0.6738738104968861
Loss in iteration 311 : 0.6939199126454505
Loss in iteration 312 : 0.7420610439874346
Loss in iteration 313 : 0.55328941912049
Loss in iteration 314 : 0.61991006886849
Loss in iteration 315 : 0.6647154673206971
Loss in iteration 316 : 0.6807111207980365
Loss in iteration 317 : 0.6183060728442697
Loss in iteration 318 : 0.5665935918638728
Loss in iteration 319 : 0.5445020877267489
Loss in iteration 320 : 0.4992956351494445
Loss in iteration 321 : 0.5397036524417383
Loss in iteration 322 : 0.5194134336946084
Loss in iteration 323 : 0.5786489759944566
Loss in iteration 324 : 0.622869686318867
Loss in iteration 325 : 0.8723419914958168
Loss in iteration 326 : 1.096347914486676
Loss in iteration 327 : 1.2087761846159364
Loss in iteration 328 : 0.7374409387017271
Loss in iteration 329 : 0.5725960142791628
Loss in iteration 330 : 0.49114138892520737
Loss in iteration 331 : 0.6121315486767454
Loss in iteration 332 : 0.6686384923824719
Loss in iteration 333 : 0.7675846768044045
Loss in iteration 334 : 0.8297266507047396
Loss in iteration 335 : 0.6625446265896324
Loss in iteration 336 : 0.6488309907516556
Loss in iteration 337 : 0.5274119213599203
Loss in iteration 338 : 0.6353329135969069
Loss in iteration 339 : 0.5363992462142396
Loss in iteration 340 : 0.6193924219548202
Loss in iteration 341 : 0.5242632763590279
Loss in iteration 342 : 0.6116346619491322
Loss in iteration 343 : 0.5797536771617106
Loss in iteration 344 : 0.7193515935946048
Loss in iteration 345 : 0.8209343527436392
Loss in iteration 346 : 0.9828126933701493
Loss in iteration 347 : 0.8839989113373304
Loss in iteration 348 : 0.7782762165424172
Loss in iteration 349 : 0.6622578118084067
Loss in iteration 350 : 0.6545592356312153
Loss in iteration 351 : 0.571644678239236
Loss in iteration 352 : 0.6255147332500945
Loss in iteration 353 : 0.5341840771332336
Loss in iteration 354 : 0.6438783397478951
Loss in iteration 355 : 0.6159350652872796
Loss in iteration 356 : 0.8450470639353732
Loss in iteration 357 : 0.8723544975667881
Loss in iteration 358 : 1.0400575715741385
Loss in iteration 359 : 0.7076119056843577
Loss in iteration 360 : 0.604119325877153
Loss in iteration 361 : 0.5023616127999866
Loss in iteration 362 : 0.5551037690965489
Loss in iteration 363 : 0.509915049232241
Loss in iteration 364 : 0.5107251413438781
Loss in iteration 365 : 0.5299357783107356
Loss in iteration 366 : 0.556606010321153
Loss in iteration 367 : 0.7104249222661199
Loss in iteration 368 : 1.1472983377863055
Loss in iteration 369 : 1.5127850908979217
Loss in iteration 370 : 0.7407682555827216
Loss in iteration 371 : 0.5660474788326016
Loss in iteration 372 : 0.5451520268142277
Loss in iteration 373 : 0.8094468342024672
Loss in iteration 374 : 1.1419095643162243
Loss in iteration 375 : 0.8675954591062037
Loss in iteration 376 : 0.6121827699713099
Loss in iteration 377 : 0.5258292020904864
Loss in iteration 378 : 0.6412340535784677
Loss in iteration 379 : 0.786505892254646
Loss in iteration 380 : 0.779355129287453
Loss in iteration 381 : 0.6844167679082893
Loss in iteration 382 : 0.6226740310471315
Loss in iteration 383 : 0.5582532327455276
Loss in iteration 384 : 0.5210506599643869
Loss in iteration 385 : 0.5112179203047569
Loss in iteration 386 : 0.5021594710349758
Loss in iteration 387 : 0.5796469325433149
Loss in iteration 388 : 0.7311750747699598
Loss in iteration 389 : 0.9328886807027891
Loss in iteration 390 : 1.048005484977643
Loss in iteration 391 : 0.7967514097673527
Loss in iteration 392 : 0.73326362126545
Loss in iteration 393 : 0.6114016408232513
Loss in iteration 394 : 0.6105110047844597
Loss in iteration 395 : 0.5651972193519341
Loss in iteration 396 : 0.5748215814445173
Loss in iteration 397 : 0.6247828688700402
Loss in iteration 398 : 0.7766237733136012
Loss in iteration 399 : 0.9446022161266464
Loss in iteration 400 : 1.0726018472018863
Testing accuracy  of updater 7 on alg 0 with rate 14.0 = 0.70775, training accuracy 0.70775, time elapsed: 5695 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.0571452546074425
Loss in iteration 3 : 2.222027560362819
Loss in iteration 4 : 1.0818691257862205
Loss in iteration 5 : 3.1121198207249416
Loss in iteration 6 : 2.345811061914742
Loss in iteration 7 : 1.2014675820442675
Loss in iteration 8 : 2.0652933683275405
Loss in iteration 9 : 0.6812972660757315
Loss in iteration 10 : 1.6103526034634985
Loss in iteration 11 : 1.4792207597824463
Loss in iteration 12 : 0.6755861715378604
Loss in iteration 13 : 1.4934558619624383
Loss in iteration 14 : 1.0255551868385888
Loss in iteration 15 : 0.898005028356028
Loss in iteration 16 : 1.334681530436319
Loss in iteration 17 : 0.9378335687714591
Loss in iteration 18 : 0.8426265383796185
Loss in iteration 19 : 1.1654687429698287
Loss in iteration 20 : 0.8268316286324023
Loss in iteration 21 : 0.9184704089847602
Loss in iteration 22 : 1.095216769279019
Loss in iteration 23 : 0.815893417774881
Loss in iteration 24 : 0.8654338687589392
Loss in iteration 25 : 0.9584699790203186
Loss in iteration 26 : 0.696763957172162
Loss in iteration 27 : 0.8268490255481015
Loss in iteration 28 : 0.8511157990260357
Loss in iteration 29 : 0.7034474368534561
Loss in iteration 30 : 0.8086361613590161
Loss in iteration 31 : 0.7100377782754924
Loss in iteration 32 : 0.6859337975421803
Loss in iteration 33 : 0.7047475274995392
Loss in iteration 34 : 0.5937908300228791
Loss in iteration 35 : 0.6810482082512068
Loss in iteration 36 : 0.634013501795117
Loss in iteration 37 : 0.6019419356792758
Loss in iteration 38 : 0.6175853319467567
Loss in iteration 39 : 0.5369321810548159
Loss in iteration 40 : 0.5956578902411721
Loss in iteration 41 : 0.5542646371861216
Loss in iteration 42 : 0.5587295216182124
Loss in iteration 43 : 0.5290722806291417
Loss in iteration 44 : 0.5386746653173491
Loss in iteration 45 : 0.5345550168843136
Loss in iteration 46 : 0.531408286992964
Loss in iteration 47 : 0.5206381420605148
Loss in iteration 48 : 0.5224026286197699
Loss in iteration 49 : 0.5212457280600972
Loss in iteration 50 : 0.5074302869360239
Loss in iteration 51 : 0.5131578308883993
Loss in iteration 52 : 0.528140033019794
Loss in iteration 53 : 0.4997469998684659
Loss in iteration 54 : 0.5056834182128835
Loss in iteration 55 : 0.4912248007010101
Loss in iteration 56 : 0.5130222686852343
Loss in iteration 57 : 0.48184583798620956
Loss in iteration 58 : 0.4778379370164069
Loss in iteration 59 : 0.4753048598292313
Loss in iteration 60 : 0.48389826308579287
Loss in iteration 61 : 0.4799952235243538
Loss in iteration 62 : 0.4778158023638963
Loss in iteration 63 : 0.475986277961027
Loss in iteration 64 : 0.46865605743510863
Loss in iteration 65 : 0.4823308163172412
Loss in iteration 66 : 0.463350250206004
Loss in iteration 67 : 0.4752076897282818
Loss in iteration 68 : 0.4677185213978487
Loss in iteration 69 : 0.4715246703310431
Loss in iteration 70 : 0.45934175920942694
Loss in iteration 71 : 0.47401944804771723
Loss in iteration 72 : 0.4589497028000615
Loss in iteration 73 : 0.4723440419096329
Loss in iteration 74 : 0.4610840538023449
Loss in iteration 75 : 0.46045278663128997
Loss in iteration 76 : 0.477033662179542
Loss in iteration 77 : 0.4558986050129892
Loss in iteration 78 : 0.47269442765458075
Loss in iteration 79 : 0.44773464280694447
Loss in iteration 80 : 0.4630408996838905
Loss in iteration 81 : 0.46776602006441875
Loss in iteration 82 : 0.46118916819859673
Loss in iteration 83 : 0.4631717620230211
Loss in iteration 84 : 0.4764390742090961
Loss in iteration 85 : 0.4650159038046154
Loss in iteration 86 : 0.4712648126781625
Loss in iteration 87 : 0.4494572740363239
Loss in iteration 88 : 0.47609769289587706
Loss in iteration 89 : 0.45887963130042486
Loss in iteration 90 : 0.4613004287899049
Loss in iteration 91 : 0.4731333392631451
Loss in iteration 92 : 0.45643285996317406
Loss in iteration 93 : 0.4665654431948991
Loss in iteration 94 : 0.4599794504016791
Loss in iteration 95 : 0.46380386837446885
Loss in iteration 96 : 0.46698792744119233
Loss in iteration 97 : 0.46021093242372046
Loss in iteration 98 : 0.4672734253760198
Loss in iteration 99 : 0.46183940277106705
Loss in iteration 100 : 0.46109756181345934
Loss in iteration 101 : 0.4599559001377581
Loss in iteration 102 : 0.4540395355188022
Loss in iteration 103 : 0.46445120716138005
Loss in iteration 104 : 0.4669141705037749
Loss in iteration 105 : 0.4499182991830259
Loss in iteration 106 : 0.46182951542244005
Loss in iteration 107 : 0.4772937279704535
Loss in iteration 108 : 0.47734416236269317
Loss in iteration 109 : 0.46843715891360677
Loss in iteration 110 : 0.46296484805222055
Loss in iteration 111 : 0.4644860039359772
Loss in iteration 112 : 0.4740783044687861
Loss in iteration 113 : 0.4814107341718052
Loss in iteration 114 : 0.45388079273684906
Loss in iteration 115 : 0.4835587051704594
Loss in iteration 116 : 0.4958549449485437
Loss in iteration 117 : 0.47299724458676246
Loss in iteration 118 : 0.4968347517532316
Loss in iteration 119 : 0.4773121632932769
Loss in iteration 120 : 0.474188337600213
Loss in iteration 121 : 0.5024170456407581
Loss in iteration 122 : 0.48422484868393323
Loss in iteration 123 : 0.48010372817212593
Loss in iteration 124 : 0.5065786687322323
Loss in iteration 125 : 0.4628835654299091
Loss in iteration 126 : 0.4755901937885177
Loss in iteration 127 : 0.4916375058970619
Loss in iteration 128 : 0.46771444092901365
Loss in iteration 129 : 0.47860852318149083
Loss in iteration 130 : 0.484507224683271
Loss in iteration 131 : 0.46072037257255744
Loss in iteration 132 : 0.46575403880440974
Loss in iteration 133 : 0.46916590447925915
Loss in iteration 134 : 0.464837658868259
Loss in iteration 135 : 0.46664567110223865
Loss in iteration 136 : 0.4628068550519216
Loss in iteration 137 : 0.4679749788794211
Loss in iteration 138 : 0.4665392194238577
Loss in iteration 139 : 0.47441203009657174
Loss in iteration 140 : 0.46928452846339475
Loss in iteration 141 : 0.47490886347077327
Loss in iteration 142 : 0.46129832547551813
Loss in iteration 143 : 0.46209168744787943
Loss in iteration 144 : 0.47070474981548793
Loss in iteration 145 : 0.4545589359100101
Loss in iteration 146 : 0.4727240084957117
Loss in iteration 147 : 0.4992698991658599
Loss in iteration 148 : 0.47290376957407004
Loss in iteration 149 : 0.48051168578257286
Loss in iteration 150 : 0.48229097687542083
Loss in iteration 151 : 0.466822444743099
Loss in iteration 152 : 0.47589066357285437
Loss in iteration 153 : 0.4648416718785671
Loss in iteration 154 : 0.46885832957613244
Loss in iteration 155 : 0.47130703862590756
Loss in iteration 156 : 0.4518370967574338
Loss in iteration 157 : 0.4703136510948091
Loss in iteration 158 : 0.4586396646027088
Loss in iteration 159 : 0.46158490476722835
Loss in iteration 160 : 0.45719606554355896
Loss in iteration 161 : 0.45621767219786874
Loss in iteration 162 : 0.46146867513165285
Loss in iteration 163 : 0.4704393948116581
Loss in iteration 164 : 0.4639014931398247
Loss in iteration 165 : 0.4584094977242625
Loss in iteration 166 : 0.46336821560905794
Loss in iteration 167 : 0.46861291165372576
Loss in iteration 168 : 0.4790632662592021
Loss in iteration 169 : 0.47081413227678826
Loss in iteration 170 : 0.469741560686475
Loss in iteration 171 : 0.45768058182930943
Loss in iteration 172 : 0.476181590095765
Loss in iteration 173 : 0.46010342499412965
Loss in iteration 174 : 0.45908488620938537
Loss in iteration 175 : 0.47701112044062255
Loss in iteration 176 : 0.4576044468002296
Loss in iteration 177 : 0.47260002206096585
Loss in iteration 178 : 0.4618047820826332
Loss in iteration 179 : 0.47468790453857934
Loss in iteration 180 : 0.45654192424200374
Loss in iteration 181 : 0.46724093449756643
Loss in iteration 182 : 0.46752620108479875
Loss in iteration 183 : 0.45982390084553987
Loss in iteration 184 : 0.48086308296733343
Loss in iteration 185 : 0.45970120220446536
Loss in iteration 186 : 0.4531881174939129
Loss in iteration 187 : 0.45617228422541817
Loss in iteration 188 : 0.47345723382900257
Loss in iteration 189 : 0.46484999030442087
Loss in iteration 190 : 0.45116231333529666
Loss in iteration 191 : 0.4571234393373455
Loss in iteration 192 : 0.46211148366133153
Loss in iteration 193 : 0.47065641378228107
Loss in iteration 194 : 0.4659200664649642
Loss in iteration 195 : 0.47310976577752095
Loss in iteration 196 : 0.4767998382581678
Loss in iteration 197 : 0.45033265313714554
Loss in iteration 198 : 0.4712517144302779
Loss in iteration 199 : 0.46390417144803786
Loss in iteration 200 : 0.4550613141223502
Loss in iteration 201 : 0.4583745105918651
Loss in iteration 202 : 0.4717984904420375
Loss in iteration 203 : 0.4565757026239884
Loss in iteration 204 : 0.4670919924685501
Loss in iteration 205 : 0.4608554169962548
Loss in iteration 206 : 0.4642060732115917
Loss in iteration 207 : 0.4547240332068262
Loss in iteration 208 : 0.45251031957449606
Loss in iteration 209 : 0.465276397318946
Loss in iteration 210 : 0.4715386355405814
Loss in iteration 211 : 0.4652021355588388
Loss in iteration 212 : 0.4720277360474272
Loss in iteration 213 : 0.4760766402671772
Loss in iteration 214 : 0.4822559484600071
Loss in iteration 215 : 0.4707981644072007
Loss in iteration 216 : 0.460793771975088
Loss in iteration 217 : 0.461682656256696
Loss in iteration 218 : 0.459195717148079
Loss in iteration 219 : 0.4830099827703834
Loss in iteration 220 : 0.48413799849227207
Loss in iteration 221 : 0.46382343168865114
Loss in iteration 222 : 0.46005490669382076
Loss in iteration 223 : 0.4672626924485919
Loss in iteration 224 : 0.47645692554517527
Loss in iteration 225 : 0.46534746322119375
Loss in iteration 226 : 0.4631166770987805
Loss in iteration 227 : 0.4647826703276291
Loss in iteration 228 : 0.4863957505617229
Loss in iteration 229 : 0.47160342396434785
Loss in iteration 230 : 0.4604894764763299
Loss in iteration 231 : 0.4777039560767704
Loss in iteration 232 : 0.47867185254723105
Loss in iteration 233 : 0.4570777008251884
Loss in iteration 234 : 0.4675368687831723
Loss in iteration 235 : 0.46292024574905666
Loss in iteration 236 : 0.47141577542226304
Loss in iteration 237 : 0.46754637227888657
Loss in iteration 238 : 0.4617011889763228
Loss in iteration 239 : 0.46275212818415806
Loss in iteration 240 : 0.4555346105922039
Loss in iteration 241 : 0.456209596509659
Loss in iteration 242 : 0.4896486615459657
Loss in iteration 243 : 0.47312988676224177
Loss in iteration 244 : 0.4602151199649733
Loss in iteration 245 : 0.47524242751400064
Loss in iteration 246 : 0.45938598393930247
Loss in iteration 247 : 0.45618563639371296
Loss in iteration 248 : 0.4633066934381943
Loss in iteration 249 : 0.4689774060067415
Loss in iteration 250 : 0.45533246615777245
Loss in iteration 251 : 0.46678487880333963
Loss in iteration 252 : 0.4642912985515669
Loss in iteration 253 : 0.473630875502873
Loss in iteration 254 : 0.44858888578761785
Loss in iteration 255 : 0.4544767127763849
Loss in iteration 256 : 0.4747957611918115
Loss in iteration 257 : 0.4524139878553408
Loss in iteration 258 : 0.45822680437429353
Loss in iteration 259 : 0.45656379756785304
Loss in iteration 260 : 0.4616972721995777
Loss in iteration 261 : 0.45264894477383616
Loss in iteration 262 : 0.4610450174575819
Loss in iteration 263 : 0.4611499297195951
Loss in iteration 264 : 0.4780336062124964
Loss in iteration 265 : 0.458808354084124
Loss in iteration 266 : 0.4711669879187619
Loss in iteration 267 : 0.4821646995509005
Loss in iteration 268 : 0.49481819158341345
Loss in iteration 269 : 0.4626752771319082
Loss in iteration 270 : 0.46297217835974547
Loss in iteration 271 : 0.47330555533101487
Loss in iteration 272 : 0.4974538248996922
Loss in iteration 273 : 0.46093060150917314
Loss in iteration 274 : 0.4605072084467547
Loss in iteration 275 : 0.48152882107382633
Loss in iteration 276 : 0.4826957548294523
Loss in iteration 277 : 0.4546344439140811
Loss in iteration 278 : 0.47430361277085503
Loss in iteration 279 : 0.48905898224608296
Loss in iteration 280 : 0.47574027779397204
Loss in iteration 281 : 0.4642486410771797
Loss in iteration 282 : 0.4869519702968119
Loss in iteration 283 : 0.48287555602271404
Loss in iteration 284 : 0.4475528246498646
Loss in iteration 285 : 0.47760379320273894
Loss in iteration 286 : 0.5126793793442126
Loss in iteration 287 : 0.47868427467799757
Loss in iteration 288 : 0.4739228628120631
Loss in iteration 289 : 0.5033813749279629
Loss in iteration 290 : 0.5105308542301917
Loss in iteration 291 : 0.4820078665086908
Loss in iteration 292 : 0.47169747076655033
Loss in iteration 293 : 0.5121881689427648
Loss in iteration 294 : 0.5203310789025564
Loss in iteration 295 : 0.46006327732948865
Loss in iteration 296 : 0.47752602254922405
Loss in iteration 297 : 0.5013414712361098
Loss in iteration 298 : 0.46663708402678483
Loss in iteration 299 : 0.47891689067021376
Loss in iteration 300 : 0.5021155240043458
Loss in iteration 301 : 0.5014464645559554
Loss in iteration 302 : 0.46348167912917143
Loss in iteration 303 : 0.4679329770537347
Loss in iteration 304 : 0.479342129076475
Loss in iteration 305 : 0.4938521683814049
Loss in iteration 306 : 0.4714088477507044
Loss in iteration 307 : 0.4717544082629941
Loss in iteration 308 : 0.47350499210944813
Loss in iteration 309 : 0.48535718695292834
Loss in iteration 310 : 0.4871726758956744
Loss in iteration 311 : 0.49431351813480084
Loss in iteration 312 : 0.4934702657873927
Loss in iteration 313 : 0.4662092471057679
Loss in iteration 314 : 0.47594250769148005
Loss in iteration 315 : 0.46570957268288476
Loss in iteration 316 : 0.489550099082491
Loss in iteration 317 : 0.4711597832145862
Loss in iteration 318 : 0.4853780727382764
Loss in iteration 319 : 0.4603249702783041
Loss in iteration 320 : 0.4632139912952577
Loss in iteration 321 : 0.4545100626404527
Loss in iteration 322 : 0.4723770698079377
Loss in iteration 323 : 0.46185124287607365
Loss in iteration 324 : 0.4638530796975038
Loss in iteration 325 : 0.4607708598948477
Loss in iteration 326 : 0.4772297292334512
Loss in iteration 327 : 0.4578231783488678
Loss in iteration 328 : 0.4590001651339415
Loss in iteration 329 : 0.46330047476184794
Loss in iteration 330 : 0.45579750875287445
Loss in iteration 331 : 0.4738521040781159
Loss in iteration 332 : 0.45285339262746727
Loss in iteration 333 : 0.45010232268116485
Loss in iteration 334 : 0.46091038023342384
Loss in iteration 335 : 0.46613086594463465
Loss in iteration 336 : 0.4690744847039194
Loss in iteration 337 : 0.46710220609694575
Loss in iteration 338 : 0.4906165122308717
Loss in iteration 339 : 0.49455604184502866
Loss in iteration 340 : 0.4785296018518255
Loss in iteration 341 : 0.4605506402665014
Loss in iteration 342 : 0.48926640062504756
Loss in iteration 343 : 0.4910835078207594
Loss in iteration 344 : 0.4799314013271682
Loss in iteration 345 : 0.4590735957517931
Loss in iteration 346 : 0.4659555650821404
Loss in iteration 347 : 0.49258649032135027
Loss in iteration 348 : 0.4922070003292075
Loss in iteration 349 : 0.47955217073958634
Loss in iteration 350 : 0.4727318188577513
Loss in iteration 351 : 0.4648581091760135
Loss in iteration 352 : 0.4648789205336635
Loss in iteration 353 : 0.4590150103298868
Loss in iteration 354 : 0.46926474910034927
Loss in iteration 355 : 0.4628369008077891
Loss in iteration 356 : 0.45650287160825587
Loss in iteration 357 : 0.4603940652281635
Loss in iteration 358 : 0.4759378127281533
Loss in iteration 359 : 0.46114585802165126
Loss in iteration 360 : 0.47236749328825395
Loss in iteration 361 : 0.45278296853771655
Loss in iteration 362 : 0.4762729987175579
Loss in iteration 363 : 0.4619964209554424
Loss in iteration 364 : 0.4637204881089047
Loss in iteration 365 : 0.46990651128149297
Loss in iteration 366 : 0.46735884694678925
Loss in iteration 367 : 0.47105207175195357
Loss in iteration 368 : 0.47721621801142494
Loss in iteration 369 : 0.48236021598113205
Loss in iteration 370 : 0.4943366765311199
Loss in iteration 371 : 0.4971746322186871
Loss in iteration 372 : 0.46783499741012446
Loss in iteration 373 : 0.469227842609243
Loss in iteration 374 : 0.47342400017686337
Loss in iteration 375 : 0.5023239350157335
Loss in iteration 376 : 0.5112830418172303
Loss in iteration 377 : 0.49563316298975585
Loss in iteration 378 : 0.4704796682206371
Loss in iteration 379 : 0.4839456342598125
Loss in iteration 380 : 0.49619350621324027
Loss in iteration 381 : 0.4824865712994438
Loss in iteration 382 : 0.4705254197269093
Loss in iteration 383 : 0.4661903840103426
Loss in iteration 384 : 0.49326533423918734
Loss in iteration 385 : 0.5009610590540733
Loss in iteration 386 : 0.4796425225630902
Loss in iteration 387 : 0.4744039621895198
Loss in iteration 388 : 0.4773938239269294
Loss in iteration 389 : 0.4864222077571251
Loss in iteration 390 : 0.5117653727253266
Loss in iteration 391 : 0.5161914519212989
Loss in iteration 392 : 0.4710567835551785
Loss in iteration 393 : 0.45858238740714125
Loss in iteration 394 : 0.479904445407028
Loss in iteration 395 : 0.5098840164808254
Loss in iteration 396 : 0.48557191480360273
Loss in iteration 397 : 0.47989775372770604
Loss in iteration 398 : 0.47876181406117024
Loss in iteration 399 : 0.5090883418025345
Loss in iteration 400 : 0.5337819836045691
Testing accuracy  of updater 7 on alg 0 with rate 8.0 = 0.7455, training accuracy 0.7455, time elapsed: 5430 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.7216225659913922
Loss in iteration 3 : 0.652246575259969
Loss in iteration 4 : 0.6436344161464446
Loss in iteration 5 : 0.6072313111582609
Loss in iteration 6 : 0.5851494642247491
Loss in iteration 7 : 0.5653112560699106
Loss in iteration 8 : 0.537644230611875
Loss in iteration 9 : 0.5333836263071835
Loss in iteration 10 : 0.5133341549087101
Loss in iteration 11 : 0.50330381426886
Loss in iteration 12 : 0.4960501292999128
Loss in iteration 13 : 0.48766747942895866
Loss in iteration 14 : 0.485076515903031
Loss in iteration 15 : 0.4983833617854374
Loss in iteration 16 : 0.488770339222693
Loss in iteration 17 : 0.48126893994288766
Loss in iteration 18 : 0.4829290396160463
Loss in iteration 19 : 0.4859108259254318
Loss in iteration 20 : 0.4752981598805447
Loss in iteration 21 : 0.4979013646533171
Loss in iteration 22 : 0.49071656672813924
Loss in iteration 23 : 0.49087425399393797
Loss in iteration 24 : 0.4841641637578933
Loss in iteration 25 : 0.48624538286414004
Loss in iteration 26 : 0.4697859810443444
Loss in iteration 27 : 0.47661121694333664
Loss in iteration 28 : 0.4819600699199548
Loss in iteration 29 : 0.4859325376803306
Loss in iteration 30 : 0.48464355279866667
Loss in iteration 31 : 0.4785723547862816
Loss in iteration 32 : 0.4768417041653047
Loss in iteration 33 : 0.4684651704340728
Loss in iteration 34 : 0.4677281595933798
Loss in iteration 35 : 0.4807773881024106
Loss in iteration 36 : 0.47828292139193923
Loss in iteration 37 : 0.4707203583557577
Loss in iteration 38 : 0.4701439636533531
Loss in iteration 39 : 0.4614756335563649
Loss in iteration 40 : 0.4709813688352598
Loss in iteration 41 : 0.4744845426322575
Loss in iteration 42 : 0.4592036791379496
Loss in iteration 43 : 0.4682722746437508
Loss in iteration 44 : 0.4679828269487365
Loss in iteration 45 : 0.4703743476809544
Loss in iteration 46 : 0.4701223540212976
Loss in iteration 47 : 0.46197054727177095
Loss in iteration 48 : 0.4677817964522407
Loss in iteration 49 : 0.4642487537752531
Loss in iteration 50 : 0.45449370749735485
Loss in iteration 51 : 0.47078787333678324
Loss in iteration 52 : 0.4601789576253725
Loss in iteration 53 : 0.4627201100725616
Loss in iteration 54 : 0.4593701647166486
Loss in iteration 55 : 0.4638553303882519
Loss in iteration 56 : 0.46913476655353215
Loss in iteration 57 : 0.45168976486268747
Loss in iteration 58 : 0.4551539742815879
Loss in iteration 59 : 0.4545834811128561
Loss in iteration 60 : 0.46431873268101315
Loss in iteration 61 : 0.46169081366362974
Loss in iteration 62 : 0.46317349844522115
Loss in iteration 63 : 0.4636520219024194
Loss in iteration 64 : 0.46129254622217725
Loss in iteration 65 : 0.47270115728084566
Loss in iteration 66 : 0.45648079150819293
Loss in iteration 67 : 0.4678398188889058
Loss in iteration 68 : 0.4606071495760214
Loss in iteration 69 : 0.46526674993644235
Loss in iteration 70 : 0.45572464396449325
Loss in iteration 71 : 0.47096207628366943
Loss in iteration 72 : 0.45758859736138796
Loss in iteration 73 : 0.4694878644863937
Loss in iteration 74 : 0.458764646608381
Loss in iteration 75 : 0.46226509500935686
Loss in iteration 76 : 0.47489105938152687
Loss in iteration 77 : 0.4570985342173101
Loss in iteration 78 : 0.46504689374606284
Loss in iteration 79 : 0.44405188227075804
Loss in iteration 80 : 0.4636479520500935
Loss in iteration 81 : 0.4647465624334512
Loss in iteration 82 : 0.461323380289511
Loss in iteration 83 : 0.4631624713480714
Loss in iteration 84 : 0.47492131517643926
Loss in iteration 85 : 0.4625748602200248
Loss in iteration 86 : 0.47242180839862463
Loss in iteration 87 : 0.4518461728662503
Loss in iteration 88 : 0.4723234314392403
Loss in iteration 89 : 0.45611768363605365
Loss in iteration 90 : 0.45880024968263544
Loss in iteration 91 : 0.4666335993785604
Loss in iteration 92 : 0.45576433237680175
Loss in iteration 93 : 0.4606999992094673
Loss in iteration 94 : 0.45989637899671737
Loss in iteration 95 : 0.45937269631402217
Loss in iteration 96 : 0.46797578346013835
Loss in iteration 97 : 0.4607958448200428
Loss in iteration 98 : 0.46626335990002277
Loss in iteration 99 : 0.45933064882692864
Loss in iteration 100 : 0.46338681886379535
Loss in iteration 101 : 0.4595022023694056
Loss in iteration 102 : 0.4537189204874913
Loss in iteration 103 : 0.4664307741870213
Loss in iteration 104 : 0.4619483762448313
Loss in iteration 105 : 0.44560675196002264
Loss in iteration 106 : 0.4633771323274836
Loss in iteration 107 : 0.4760215445946782
Loss in iteration 108 : 0.47714415908725155
Loss in iteration 109 : 0.46088457638914754
Loss in iteration 110 : 0.45461494204822356
Loss in iteration 111 : 0.4640209900593202
Loss in iteration 112 : 0.45810182005677413
Loss in iteration 113 : 0.45214826293570964
Loss in iteration 114 : 0.45805413956947916
Loss in iteration 115 : 0.45158374362306497
Loss in iteration 116 : 0.4651777609219445
Loss in iteration 117 : 0.47262419318358245
Loss in iteration 118 : 0.46820882412618514
Loss in iteration 119 : 0.4564468821518014
Loss in iteration 120 : 0.4746389765734337
Loss in iteration 121 : 0.4661123364719411
Loss in iteration 122 : 0.47923303149191415
Loss in iteration 123 : 0.46539862408504046
Loss in iteration 124 : 0.457192747018028
Loss in iteration 125 : 0.4550575148907644
Loss in iteration 126 : 0.45894784740162986
Loss in iteration 127 : 0.45977379785105904
Loss in iteration 128 : 0.4650955571748861
Loss in iteration 129 : 0.45900883358462463
Loss in iteration 130 : 0.4666979341564801
Loss in iteration 131 : 0.45906014848197413
Loss in iteration 132 : 0.4573755283548916
Loss in iteration 133 : 0.46673150734677027
Loss in iteration 134 : 0.4622487534227492
Loss in iteration 135 : 0.4628249710416102
Loss in iteration 136 : 0.4603672054900025
Loss in iteration 137 : 0.4675709380637661
Loss in iteration 138 : 0.46442506799698613
Loss in iteration 139 : 0.4732221482412424
Loss in iteration 140 : 0.4667867990604443
Loss in iteration 141 : 0.46799611569869076
Loss in iteration 142 : 0.4557343941416995
Loss in iteration 143 : 0.46469084683799566
Loss in iteration 144 : 0.4546942711086319
Loss in iteration 145 : 0.44908394820841857
Loss in iteration 146 : 0.46686451103216203
Loss in iteration 147 : 0.47095083170658336
Loss in iteration 148 : 0.4607724434926349
Loss in iteration 149 : 0.4743976569429052
Loss in iteration 150 : 0.4654064766719019
Loss in iteration 151 : 0.4610105445304449
Loss in iteration 152 : 0.4736668238632641
Loss in iteration 153 : 0.44919147520223324
Loss in iteration 154 : 0.4691650692941681
Loss in iteration 155 : 0.4661945683665949
Loss in iteration 156 : 0.4497343255811695
Loss in iteration 157 : 0.46470871454095886
Loss in iteration 158 : 0.4614956290298355
Loss in iteration 159 : 0.461144445320653
Loss in iteration 160 : 0.4584931038093607
Loss in iteration 161 : 0.4564555393011146
Loss in iteration 162 : 0.4653746169544528
Loss in iteration 163 : 0.46412360538705094
Loss in iteration 164 : 0.4685074043197809
Loss in iteration 165 : 0.4583267558829959
Loss in iteration 166 : 0.46033919098250603
Loss in iteration 167 : 0.4673033555750409
Loss in iteration 168 : 0.47159732627595863
Loss in iteration 169 : 0.45763613599166403
Loss in iteration 170 : 0.46536894120894945
Loss in iteration 171 : 0.45598140980152063
Loss in iteration 172 : 0.46581869973060996
Loss in iteration 173 : 0.45843011729658073
Loss in iteration 174 : 0.45463838960490144
Loss in iteration 175 : 0.46924559440561636
Loss in iteration 176 : 0.45448061540369983
Loss in iteration 177 : 0.46874012448419666
Loss in iteration 178 : 0.45800200305887306
Loss in iteration 179 : 0.46608216583980755
Loss in iteration 180 : 0.4550002946249515
Loss in iteration 181 : 0.46579568018025785
Loss in iteration 182 : 0.46012920842758825
Loss in iteration 183 : 0.4585085063682967
Loss in iteration 184 : 0.47704857176853865
Loss in iteration 185 : 0.45988506252942857
Loss in iteration 186 : 0.44980309844328603
Loss in iteration 187 : 0.45680285604825227
Loss in iteration 188 : 0.4685474684440192
Loss in iteration 189 : 0.466620216689745
Loss in iteration 190 : 0.44889856227886327
Loss in iteration 191 : 0.45318899380941263
Loss in iteration 192 : 0.462754996935189
Loss in iteration 193 : 0.4623651173011976
Loss in iteration 194 : 0.46155016273654154
Loss in iteration 195 : 0.4690314835706727
Loss in iteration 196 : 0.4751606278553099
Loss in iteration 197 : 0.45089611587152084
Loss in iteration 198 : 0.46991175784480294
Loss in iteration 199 : 0.4617320272648696
Loss in iteration 200 : 0.4500450387420328
Loss in iteration 201 : 0.4581197026963162
Loss in iteration 202 : 0.46939370429601185
Loss in iteration 203 : 0.45055675750465235
Loss in iteration 204 : 0.45704587720478385
Loss in iteration 205 : 0.46037443884141716
Loss in iteration 206 : 0.4588696097862477
Loss in iteration 207 : 0.4543665631294597
Loss in iteration 208 : 0.4503910431885228
Loss in iteration 209 : 0.4646200944683967
Loss in iteration 210 : 0.47160323292494816
Loss in iteration 211 : 0.46157959446911423
Loss in iteration 212 : 0.46734689686316827
Loss in iteration 213 : 0.4708161665930808
Loss in iteration 214 : 0.4811022481245393
Loss in iteration 215 : 0.47138449599938104
Loss in iteration 216 : 0.4584617611038492
Loss in iteration 217 : 0.4570013162207287
Loss in iteration 218 : 0.45452029754319767
Loss in iteration 219 : 0.46898780435415316
Loss in iteration 220 : 0.46692779489157993
Loss in iteration 221 : 0.46208934671860635
Loss in iteration 222 : 0.45506586937464055
Loss in iteration 223 : 0.4665742124302497
Loss in iteration 224 : 0.4706388732450116
Loss in iteration 225 : 0.46071038417544086
Loss in iteration 226 : 0.4634615438568295
Loss in iteration 227 : 0.45254296016273876
Loss in iteration 228 : 0.46534145619367767
Loss in iteration 229 : 0.46558782772938595
Loss in iteration 230 : 0.4554197522693844
Loss in iteration 231 : 0.4617705719435002
Loss in iteration 232 : 0.46680663814598905
Loss in iteration 233 : 0.4600459331061224
Loss in iteration 234 : 0.46517254114646966
Loss in iteration 235 : 0.4605848039607209
Loss in iteration 236 : 0.4628229678916845
Loss in iteration 237 : 0.46259004819094274
Loss in iteration 238 : 0.462111712800258
Loss in iteration 239 : 0.46241571162332096
Loss in iteration 240 : 0.4534807518919679
Loss in iteration 241 : 0.45581650391831
Loss in iteration 242 : 0.4833863460327422
Loss in iteration 243 : 0.4690794684063325
Loss in iteration 244 : 0.45996195057025613
Loss in iteration 245 : 0.47334170545758963
Loss in iteration 246 : 0.4564814398495576
Loss in iteration 247 : 0.4535893037635557
Loss in iteration 248 : 0.4635878188461805
Loss in iteration 249 : 0.4651692817612136
Loss in iteration 250 : 0.45436247419666204
Loss in iteration 251 : 0.46667143860889754
Loss in iteration 252 : 0.46101064087116916
Loss in iteration 253 : 0.4708918592016577
Loss in iteration 254 : 0.4472321085140975
Loss in iteration 255 : 0.4517455269168507
Loss in iteration 256 : 0.47448011170527177
Loss in iteration 257 : 0.4491149139871765
Loss in iteration 258 : 0.4567374604210501
Loss in iteration 259 : 0.4573476781194059
Loss in iteration 260 : 0.459179839359218
Loss in iteration 261 : 0.44764941680455383
Loss in iteration 262 : 0.46191432194581655
Loss in iteration 263 : 0.4605716864306321
Loss in iteration 264 : 0.47466783499489296
Loss in iteration 265 : 0.44987260965466724
Loss in iteration 266 : 0.47203792997595845
Loss in iteration 267 : 0.46681435780302577
Loss in iteration 268 : 0.4715177231908452
Loss in iteration 269 : 0.45573527494385563
Loss in iteration 270 : 0.45970732754523996
Loss in iteration 271 : 0.447293451387896
Loss in iteration 272 : 0.4602138383678864
Loss in iteration 273 : 0.44734066294607666
Loss in iteration 274 : 0.45599061214919123
Loss in iteration 275 : 0.46561464887142684
Loss in iteration 276 : 0.468319058831817
Loss in iteration 277 : 0.4544943591918779
Loss in iteration 278 : 0.4496817239460901
Loss in iteration 279 : 0.449348717468645
Loss in iteration 280 : 0.45611111475024696
Loss in iteration 281 : 0.4625598184073569
Loss in iteration 282 : 0.4582887134996674
Loss in iteration 283 : 0.4625030060496222
Loss in iteration 284 : 0.44761674415252856
Loss in iteration 285 : 0.44875072605786037
Loss in iteration 286 : 0.4595215213594188
Loss in iteration 287 : 0.4651933414918306
Loss in iteration 288 : 0.4650671954384494
Loss in iteration 289 : 0.4572186591708917
Loss in iteration 290 : 0.45159638791543005
Loss in iteration 291 : 0.46608413420408074
Loss in iteration 292 : 0.462870103108752
Loss in iteration 293 : 0.45686017715863075
Loss in iteration 294 : 0.4648238798338683
Loss in iteration 295 : 0.4527517203086737
Loss in iteration 296 : 0.45651758071074505
Loss in iteration 297 : 0.46416380026595544
Loss in iteration 298 : 0.4621308392417819
Loss in iteration 299 : 0.4615764115262983
Loss in iteration 300 : 0.4687074441848487
Loss in iteration 301 : 0.46523857133505303
Loss in iteration 302 : 0.4561980928081791
Loss in iteration 303 : 0.4537159132660671
Loss in iteration 304 : 0.4551742530120727
Loss in iteration 305 : 0.4581607876656373
Loss in iteration 306 : 0.4581391458153809
Loss in iteration 307 : 0.4595096024879317
Loss in iteration 308 : 0.4582875618362973
Loss in iteration 309 : 0.45059296401100235
Loss in iteration 310 : 0.45780476062094805
Loss in iteration 311 : 0.4663264023064152
Loss in iteration 312 : 0.4782072912939529
Loss in iteration 313 : 0.4465131767251342
Loss in iteration 314 : 0.4557371048602433
Loss in iteration 315 : 0.45715696239221126
Loss in iteration 316 : 0.47696771565427176
Loss in iteration 317 : 0.4629207252205262
Loss in iteration 318 : 0.4711147043010824
Loss in iteration 319 : 0.4540540563680515
Loss in iteration 320 : 0.4531947012818595
Loss in iteration 321 : 0.45695972279874975
Loss in iteration 322 : 0.46000112816089034
Loss in iteration 323 : 0.456995088173265
Loss in iteration 324 : 0.459082914593275
Loss in iteration 325 : 0.45655514300603645
Loss in iteration 326 : 0.4723441964230802
Loss in iteration 327 : 0.4504311937053749
Loss in iteration 328 : 0.4546950092343843
Loss in iteration 329 : 0.45548884307173776
Loss in iteration 330 : 0.4522818344767818
Loss in iteration 331 : 0.46136917253597964
Loss in iteration 332 : 0.4464291793050265
Loss in iteration 333 : 0.4478558830580113
Loss in iteration 334 : 0.4577965722535846
Loss in iteration 335 : 0.45252472778246
Loss in iteration 336 : 0.4584309500127945
Loss in iteration 337 : 0.4663275381128969
Loss in iteration 338 : 0.4684007818183731
Loss in iteration 339 : 0.47196626229535216
Loss in iteration 340 : 0.4665758312713862
Loss in iteration 341 : 0.4572749827764779
Loss in iteration 342 : 0.4580284586383163
Loss in iteration 343 : 0.45819504368321
Loss in iteration 344 : 0.4648380875338586
Loss in iteration 345 : 0.4574115323523679
Loss in iteration 346 : 0.45057079450102655
Loss in iteration 347 : 0.45979973836302024
Loss in iteration 348 : 0.4631463436038682
Loss in iteration 349 : 0.4673287784582181
Loss in iteration 350 : 0.46652379343920486
Loss in iteration 351 : 0.46161188591237845
Loss in iteration 352 : 0.45740430374618973
Loss in iteration 353 : 0.4517409631330902
Loss in iteration 354 : 0.4596806276133895
Loss in iteration 355 : 0.45989422619929876
Loss in iteration 356 : 0.4533182775957479
Loss in iteration 357 : 0.45790102796726045
Loss in iteration 358 : 0.4656799654748587
Loss in iteration 359 : 0.4569724151243353
Loss in iteration 360 : 0.4658776510747702
Loss in iteration 361 : 0.4500783161256737
Loss in iteration 362 : 0.4699382690586802
Loss in iteration 363 : 0.4614845133996736
Loss in iteration 364 : 0.4591978476979274
Loss in iteration 365 : 0.46313892645149174
Loss in iteration 366 : 0.45706513627270434
Loss in iteration 367 : 0.46786222626836693
Loss in iteration 368 : 0.46914520201494325
Loss in iteration 369 : 0.4728594737907301
Loss in iteration 370 : 0.460448767420261
Loss in iteration 371 : 0.4745794442659456
Loss in iteration 372 : 0.4578215350196609
Loss in iteration 373 : 0.4611280600566963
Loss in iteration 374 : 0.46226812274252804
Loss in iteration 375 : 0.45546106453882995
Loss in iteration 376 : 0.4659560846025608
Loss in iteration 377 : 0.46225843018375984
Loss in iteration 378 : 0.46236799268232165
Loss in iteration 379 : 0.4651121813149185
Loss in iteration 380 : 0.46128788535104226
Loss in iteration 381 : 0.44882710699084105
Loss in iteration 382 : 0.45724880785400596
Loss in iteration 383 : 0.4632376692702706
Loss in iteration 384 : 0.4709403402717404
Loss in iteration 385 : 0.4647264958919127
Loss in iteration 386 : 0.4479897360842375
Loss in iteration 387 : 0.469918453860295
Loss in iteration 388 : 0.47145038614678625
Loss in iteration 389 : 0.46143177890910336
Loss in iteration 390 : 0.4581948542861497
Loss in iteration 391 : 0.4620364172886488
Loss in iteration 392 : 0.4516324519724899
Loss in iteration 393 : 0.45378003742480505
Loss in iteration 394 : 0.45999547192224816
Loss in iteration 395 : 0.46570919510180386
Loss in iteration 396 : 0.4573715378896704
Loss in iteration 397 : 0.47283542445611265
Loss in iteration 398 : 0.46829064777117996
Loss in iteration 399 : 0.4587717037724766
Loss in iteration 400 : 0.4504265727680228
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.79, training accuracy 0.79, time elapsed: 6275 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.67489077325063
Loss in iteration 3 : 0.6450309978824464
Loss in iteration 4 : 0.6198264031152237
Loss in iteration 5 : 0.5917094623991344
Loss in iteration 6 : 0.5705322371606508
Loss in iteration 7 : 0.5475498942322108
Loss in iteration 8 : 0.5339388051061688
Loss in iteration 9 : 0.5226357863096216
Loss in iteration 10 : 0.5096992834951201
Loss in iteration 11 : 0.5011152329701215
Loss in iteration 12 : 0.4898460502872461
Loss in iteration 13 : 0.48801889586124775
Loss in iteration 14 : 0.48220285103059657
Loss in iteration 15 : 0.49411656967979967
Loss in iteration 16 : 0.48930731993351007
Loss in iteration 17 : 0.4800597704758889
Loss in iteration 18 : 0.47984918141184685
Loss in iteration 19 : 0.4864174531000471
Loss in iteration 20 : 0.47538372810299845
Loss in iteration 21 : 0.4952946694520905
Loss in iteration 22 : 0.4911421305004232
Loss in iteration 23 : 0.4899078197634295
Loss in iteration 24 : 0.48245589512615467
Loss in iteration 25 : 0.48612819034085475
Loss in iteration 26 : 0.4685314632475945
Loss in iteration 27 : 0.47508688391479154
Loss in iteration 28 : 0.48189212428442846
Loss in iteration 29 : 0.48506407264794865
Loss in iteration 30 : 0.4835020074046032
Loss in iteration 31 : 0.4781784615308729
Loss in iteration 32 : 0.4761930410353747
Loss in iteration 33 : 0.467362339499702
Loss in iteration 34 : 0.4668977236188263
Loss in iteration 35 : 0.4807253384735661
Loss in iteration 36 : 0.4774132248422888
Loss in iteration 37 : 0.470157504299007
Loss in iteration 38 : 0.4704666178339781
Loss in iteration 39 : 0.4607631834477958
Loss in iteration 40 : 0.4706960317999249
Loss in iteration 41 : 0.47402029749437696
Loss in iteration 42 : 0.45874684817991146
Loss in iteration 43 : 0.46815471698011957
Loss in iteration 44 : 0.4674417380308978
Loss in iteration 45 : 0.47027228437342006
Loss in iteration 46 : 0.46993348886642855
Loss in iteration 47 : 0.46151253520792607
Loss in iteration 48 : 0.4674586017989036
Loss in iteration 49 : 0.46462044461577623
Loss in iteration 50 : 0.45422069711878915
Loss in iteration 51 : 0.47061122695925556
Loss in iteration 52 : 0.46010294697008675
Loss in iteration 53 : 0.4626103854458046
Loss in iteration 54 : 0.45958472275421536
Loss in iteration 55 : 0.4637060770552363
Loss in iteration 56 : 0.4690741662178596
Loss in iteration 57 : 0.45185304707093216
Loss in iteration 58 : 0.45512044716114236
Loss in iteration 59 : 0.4542596658248428
Loss in iteration 60 : 0.46433682766303486
Loss in iteration 61 : 0.4616610028857448
Loss in iteration 62 : 0.46296338290363326
Loss in iteration 63 : 0.46379742677271907
Loss in iteration 64 : 0.46105659842018737
Loss in iteration 65 : 0.4726569598259109
Loss in iteration 66 : 0.45668040295406875
Loss in iteration 67 : 0.46776877020602076
Loss in iteration 68 : 0.46071352597379756
Loss in iteration 69 : 0.46526980434413073
Loss in iteration 70 : 0.45524511967338854
Loss in iteration 71 : 0.4710770403746307
Loss in iteration 72 : 0.45756941757914593
Loss in iteration 73 : 0.4688428562689417
Loss in iteration 74 : 0.4588892420990474
Loss in iteration 75 : 0.46234282852688763
Loss in iteration 76 : 0.4748281087471337
Loss in iteration 77 : 0.45781445599695836
Loss in iteration 78 : 0.46489404536055645
Loss in iteration 79 : 0.4438782851494441
Loss in iteration 80 : 0.4641726014347009
Loss in iteration 81 : 0.4648093374643966
Loss in iteration 82 : 0.46136966293413834
Loss in iteration 83 : 0.463429160062093
Loss in iteration 84 : 0.47491090403761643
Loss in iteration 85 : 0.46221261251735235
Loss in iteration 86 : 0.4726499039157819
Loss in iteration 87 : 0.451422135725566
Loss in iteration 88 : 0.47274167811315926
Loss in iteration 89 : 0.4562106606679605
Loss in iteration 90 : 0.4585876806185883
Loss in iteration 91 : 0.4666870944227296
Loss in iteration 92 : 0.4559151107533259
Loss in iteration 93 : 0.460510779918592
Loss in iteration 94 : 0.45979813095905
Loss in iteration 95 : 0.45908058481583697
Loss in iteration 96 : 0.46761517413185005
Loss in iteration 97 : 0.4612437967120602
Loss in iteration 98 : 0.4659742764966303
Loss in iteration 99 : 0.4591594553563341
Loss in iteration 100 : 0.46341428358123865
Loss in iteration 101 : 0.4594314211161092
Loss in iteration 102 : 0.45333077553731976
Loss in iteration 103 : 0.46633464261164403
Loss in iteration 104 : 0.4619060043000429
Loss in iteration 105 : 0.44569892319953286
Loss in iteration 106 : 0.4631268979784331
Loss in iteration 107 : 0.4761817348333306
Loss in iteration 108 : 0.47684273826035706
Loss in iteration 109 : 0.46077464783617295
Loss in iteration 110 : 0.45432295032095377
Loss in iteration 111 : 0.4639067044842473
Loss in iteration 112 : 0.4582288493386949
Loss in iteration 113 : 0.45186127132002235
Loss in iteration 114 : 0.4577704725484928
Loss in iteration 115 : 0.4515286280401993
Loss in iteration 116 : 0.4650215049055335
Loss in iteration 117 : 0.47224254364482826
Loss in iteration 118 : 0.46807200157309015
Loss in iteration 119 : 0.45634176104737895
Loss in iteration 120 : 0.4739955417406515
Loss in iteration 121 : 0.4664811966268609
Loss in iteration 122 : 0.47935690897725547
Loss in iteration 123 : 0.46515375341425275
Loss in iteration 124 : 0.45748412785163484
Loss in iteration 125 : 0.45470275184251785
Loss in iteration 126 : 0.4591928859342177
Loss in iteration 127 : 0.46026963525933445
Loss in iteration 128 : 0.4646240284174053
Loss in iteration 129 : 0.4593410199856884
Loss in iteration 130 : 0.4670651476420247
Loss in iteration 131 : 0.4585231893282066
Loss in iteration 132 : 0.4582674339864526
Loss in iteration 133 : 0.4667816874262859
Loss in iteration 134 : 0.4624721507516664
Loss in iteration 135 : 0.46358532166161526
Loss in iteration 136 : 0.4602388707033996
Loss in iteration 137 : 0.46792659347700416
Loss in iteration 138 : 0.4644788614634698
Loss in iteration 139 : 0.4730932095353669
Loss in iteration 140 : 0.4665709499517759
Loss in iteration 141 : 0.4680623470303946
Loss in iteration 142 : 0.45556467818693225
Loss in iteration 143 : 0.4655209277137649
Loss in iteration 144 : 0.4543885625891911
Loss in iteration 145 : 0.4493569835499996
Loss in iteration 146 : 0.46687804818010564
Loss in iteration 147 : 0.4707371959914698
Loss in iteration 148 : 0.46054480393536207
Loss in iteration 149 : 0.4743749943850179
Loss in iteration 150 : 0.4649204462806971
Loss in iteration 151 : 0.4604952503066027
Loss in iteration 152 : 0.473655962654861
Loss in iteration 153 : 0.448349379918823
Loss in iteration 154 : 0.4688214966587877
Loss in iteration 155 : 0.46633938195328917
Loss in iteration 156 : 0.4489805924776979
Loss in iteration 157 : 0.4638503369276424
Loss in iteration 158 : 0.4615642205863063
Loss in iteration 159 : 0.4601622356822401
Loss in iteration 160 : 0.45817354372315966
Loss in iteration 161 : 0.4563690909290863
Loss in iteration 162 : 0.4635488452986096
Loss in iteration 163 : 0.4644129259071376
Loss in iteration 164 : 0.4681475481344267
Loss in iteration 165 : 0.4578070330573437
Loss in iteration 166 : 0.46089182677695156
Loss in iteration 167 : 0.4674082296530187
Loss in iteration 168 : 0.4710399449720885
Loss in iteration 169 : 0.45734800359120126
Loss in iteration 170 : 0.4646897631227134
Loss in iteration 171 : 0.455956290247736
Loss in iteration 172 : 0.465830097156187
Loss in iteration 173 : 0.4575583716589295
Loss in iteration 174 : 0.4543009145399312
Loss in iteration 175 : 0.4694548345695414
Loss in iteration 176 : 0.45434095246860223
Loss in iteration 177 : 0.4683485800123451
Loss in iteration 178 : 0.45800101245389074
Loss in iteration 179 : 0.4659584276918449
Loss in iteration 180 : 0.45474917687053623
Loss in iteration 181 : 0.46536769472458156
Loss in iteration 182 : 0.4597101755153456
Loss in iteration 183 : 0.4587887554327698
Loss in iteration 184 : 0.47665340123538996
Loss in iteration 185 : 0.45923155604228344
Loss in iteration 186 : 0.44964430619563805
Loss in iteration 187 : 0.45661922804973265
Loss in iteration 188 : 0.46873123698173325
Loss in iteration 189 : 0.46645811773917
Loss in iteration 190 : 0.44885438291957924
Loss in iteration 191 : 0.4533036234134729
Loss in iteration 192 : 0.46254581172716186
Loss in iteration 193 : 0.4622670987125693
Loss in iteration 194 : 0.4617937997722843
Loss in iteration 195 : 0.4687421252716108
Loss in iteration 196 : 0.4752434870711784
Loss in iteration 197 : 0.4509058686909241
Loss in iteration 198 : 0.4702056124842973
Loss in iteration 199 : 0.46178271996627956
Loss in iteration 200 : 0.4499976467910704
Loss in iteration 201 : 0.4578241804081288
Loss in iteration 202 : 0.4696295861048557
Loss in iteration 203 : 0.45087366470002505
Loss in iteration 204 : 0.4566667671646546
Loss in iteration 205 : 0.46073476709820094
Loss in iteration 206 : 0.45914156211798923
Loss in iteration 207 : 0.4545256426063922
Loss in iteration 208 : 0.4505728176186083
Loss in iteration 209 : 0.46468841617577306
Loss in iteration 210 : 0.4724702993889666
Loss in iteration 211 : 0.46125002978844976
Loss in iteration 212 : 0.4674180245941317
Loss in iteration 213 : 0.47151714911321424
Loss in iteration 214 : 0.48092828696166534
Loss in iteration 215 : 0.47248207049889296
Loss in iteration 216 : 0.45822516874960467
Loss in iteration 217 : 0.4576915964997087
Loss in iteration 218 : 0.45454319971688195
Loss in iteration 219 : 0.4690444986433803
Loss in iteration 220 : 0.46684829295297464
Loss in iteration 221 : 0.4622030614702229
Loss in iteration 222 : 0.45442236041404716
Loss in iteration 223 : 0.4661978429515625
Loss in iteration 224 : 0.4704628269486112
Loss in iteration 225 : 0.46015643461964395
Loss in iteration 226 : 0.4641650496987138
Loss in iteration 227 : 0.452238710712335
Loss in iteration 228 : 0.46543523892820515
Loss in iteration 229 : 0.46578051057399583
Loss in iteration 230 : 0.4551589018904027
Loss in iteration 231 : 0.46176467146432587
Loss in iteration 232 : 0.4669734849308571
Loss in iteration 233 : 0.45870012388626547
Loss in iteration 234 : 0.46521325934566454
Loss in iteration 235 : 0.4605734430631912
Loss in iteration 236 : 0.46307301825584324
Loss in iteration 237 : 0.46296478290219867
Loss in iteration 238 : 0.4611780828912531
Loss in iteration 239 : 0.4636926342454186
Loss in iteration 240 : 0.4534688525423991
Loss in iteration 241 : 0.45652657482651715
Loss in iteration 242 : 0.4844070937943137
Loss in iteration 243 : 0.46898151217162753
Loss in iteration 244 : 0.4610685333771794
Loss in iteration 245 : 0.4731029589384247
Loss in iteration 246 : 0.456859852253147
Loss in iteration 247 : 0.4540177967022379
Loss in iteration 248 : 0.46353134533451584
Loss in iteration 249 : 0.4655936070413311
Loss in iteration 250 : 0.4542878377030127
Loss in iteration 251 : 0.4671402517997689
Loss in iteration 252 : 0.4613840918625638
Loss in iteration 253 : 0.470746326445878
Loss in iteration 254 : 0.4477598412908147
Loss in iteration 255 : 0.45171624502837443
Loss in iteration 256 : 0.47422245525651646
Loss in iteration 257 : 0.449178996945295
Loss in iteration 258 : 0.45705407040731905
Loss in iteration 259 : 0.457608626096899
Loss in iteration 260 : 0.4590484517809013
Loss in iteration 261 : 0.44822754556650385
Loss in iteration 262 : 0.4620904298031682
Loss in iteration 263 : 0.4604665654026555
Loss in iteration 264 : 0.47569122903490946
Loss in iteration 265 : 0.4500481778664426
Loss in iteration 266 : 0.472434440874626
Loss in iteration 267 : 0.46642437024238415
Loss in iteration 268 : 0.47197520792339714
Loss in iteration 269 : 0.45599991406805107
Loss in iteration 270 : 0.45954644978440157
Loss in iteration 271 : 0.44756424443951653
Loss in iteration 272 : 0.4604050464834129
Loss in iteration 273 : 0.4475012269433122
Loss in iteration 274 : 0.45611968352151905
Loss in iteration 275 : 0.4659170340933162
Loss in iteration 276 : 0.4682769378724433
Loss in iteration 277 : 0.4546709005214534
Loss in iteration 278 : 0.4501327632078271
Loss in iteration 279 : 0.44947164086550917
Loss in iteration 280 : 0.45602306525689423
Loss in iteration 281 : 0.4629338178406814
Loss in iteration 282 : 0.4584150241686783
Loss in iteration 283 : 0.46211163817269607
Loss in iteration 284 : 0.4483662197022553
Loss in iteration 285 : 0.4487956097778316
Loss in iteration 286 : 0.45949939646965166
Loss in iteration 287 : 0.4656202515049946
Loss in iteration 288 : 0.4651235364471193
Loss in iteration 289 : 0.45723778172720636
Loss in iteration 290 : 0.4521319988161641
Loss in iteration 291 : 0.4667847345663555
Loss in iteration 292 : 0.46304917715975785
Loss in iteration 293 : 0.4568144233041351
Loss in iteration 294 : 0.4649578459060175
Loss in iteration 295 : 0.45240267624773195
Loss in iteration 296 : 0.4558445290302633
Loss in iteration 297 : 0.4648727670071148
Loss in iteration 298 : 0.46223411430939326
Loss in iteration 299 : 0.4613551636024659
Loss in iteration 300 : 0.46965268371919777
Loss in iteration 301 : 0.46511140533071127
Loss in iteration 302 : 0.4565494047255315
Loss in iteration 303 : 0.45465288810788035
Loss in iteration 304 : 0.45516349093193104
Loss in iteration 305 : 0.45853536921235954
Loss in iteration 306 : 0.45852485229664997
Loss in iteration 307 : 0.45979980883317334
Loss in iteration 308 : 0.45789229092311906
Loss in iteration 309 : 0.45069707733853775
Loss in iteration 310 : 0.45810444556340024
Loss in iteration 311 : 0.46635118150534893
Loss in iteration 312 : 0.4781344152933904
Loss in iteration 313 : 0.446574507541763
Loss in iteration 314 : 0.45546110631699405
Loss in iteration 315 : 0.45746537013887073
Loss in iteration 316 : 0.4771281527361949
Loss in iteration 317 : 0.4630996393828187
Loss in iteration 318 : 0.4713176654583185
Loss in iteration 319 : 0.4540902254993233
Loss in iteration 320 : 0.45345983821850777
Loss in iteration 321 : 0.4573441868459684
Loss in iteration 322 : 0.4601045266660069
Loss in iteration 323 : 0.45703607943341173
Loss in iteration 324 : 0.45951685224203087
Loss in iteration 325 : 0.45643167341558016
Loss in iteration 326 : 0.47247036210495047
Loss in iteration 327 : 0.4507839467560136
Loss in iteration 328 : 0.45465411370942965
Loss in iteration 329 : 0.45543958505315574
Loss in iteration 330 : 0.4527692077833544
Loss in iteration 331 : 0.4613681530663498
Loss in iteration 332 : 0.44635882045642855
Loss in iteration 333 : 0.44846905026159933
Loss in iteration 334 : 0.45779874786844665
Loss in iteration 335 : 0.45161597128366304
Loss in iteration 336 : 0.4588051209579724
Loss in iteration 337 : 0.4668425432699202
Loss in iteration 338 : 0.46792119812674376
Loss in iteration 339 : 0.4726403366842838
Loss in iteration 340 : 0.4667891789877709
Loss in iteration 341 : 0.4565741225654769
Loss in iteration 342 : 0.458117693977968
Loss in iteration 343 : 0.45845624117834555
Loss in iteration 344 : 0.46455360859090605
Loss in iteration 345 : 0.45737813907656494
Loss in iteration 346 : 0.45039026067285626
Loss in iteration 347 : 0.45983166394410885
Loss in iteration 348 : 0.46255899764832764
Loss in iteration 349 : 0.46669739462655424
Loss in iteration 350 : 0.46647264825480184
Loss in iteration 351 : 0.46082653409442514
Loss in iteration 352 : 0.45730695248374176
Loss in iteration 353 : 0.45197897474564336
Loss in iteration 354 : 0.459415164166591
Loss in iteration 355 : 0.460028605171468
Loss in iteration 356 : 0.45343917543439927
Loss in iteration 357 : 0.4576705765157608
Loss in iteration 358 : 0.46573416265238055
Loss in iteration 359 : 0.4575964433251757
Loss in iteration 360 : 0.46580250613175306
Loss in iteration 361 : 0.4500052603724877
Loss in iteration 362 : 0.46994772231002796
Loss in iteration 363 : 0.46128288214485574
Loss in iteration 364 : 0.4591817313342375
Loss in iteration 365 : 0.4630702342049651
Loss in iteration 366 : 0.4570489603281917
Loss in iteration 367 : 0.4680001918386195
Loss in iteration 368 : 0.4684264032954766
Loss in iteration 369 : 0.47351252288624757
Loss in iteration 370 : 0.4594527179274454
Loss in iteration 371 : 0.47531112418350147
Loss in iteration 372 : 0.4578456532531485
Loss in iteration 373 : 0.4608458575083259
Loss in iteration 374 : 0.4637220175497742
Loss in iteration 375 : 0.45465436072205445
Loss in iteration 376 : 0.4678586459946572
Loss in iteration 377 : 0.4622275882324129
Loss in iteration 378 : 0.4621154734128451
Loss in iteration 379 : 0.46622255782681976
Loss in iteration 380 : 0.4606505796147272
Loss in iteration 381 : 0.4491942471966055
Loss in iteration 382 : 0.45738547019487
Loss in iteration 383 : 0.46253649446259915
Loss in iteration 384 : 0.4715928793172519
Loss in iteration 385 : 0.4632812955017959
Loss in iteration 386 : 0.44947996763481696
Loss in iteration 387 : 0.46889723392497484
Loss in iteration 388 : 0.47184197816701523
Loss in iteration 389 : 0.4621755614360308
Loss in iteration 390 : 0.4579921705840521
Loss in iteration 391 : 0.46378597148371653
Loss in iteration 392 : 0.4504601829335681
Loss in iteration 393 : 0.45497437725668366
Loss in iteration 394 : 0.46014554767225524
Loss in iteration 395 : 0.46593337988485045
Loss in iteration 396 : 0.4581920954624987
Loss in iteration 397 : 0.47285855219275363
Loss in iteration 398 : 0.46815406253237757
Loss in iteration 399 : 0.45878124784689145
Loss in iteration 400 : 0.4509775581705618
Testing accuracy  of updater 7 on alg 0 with rate 1.4 = 0.79, training accuracy 0.79, time elapsed: 5202 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6775218550202687
Loss in iteration 3 : 0.6509977988203013
Loss in iteration 4 : 0.6282162313325096
Loss in iteration 5 : 0.6029504079908138
Loss in iteration 6 : 0.5826244851751151
Loss in iteration 7 : 0.5591340667727456
Loss in iteration 8 : 0.5447021578877479
Loss in iteration 9 : 0.5315404464949844
Loss in iteration 10 : 0.5179220602936472
Loss in iteration 11 : 0.510059572368112
Loss in iteration 12 : 0.49601826988055814
Loss in iteration 13 : 0.49287206636239933
Loss in iteration 14 : 0.48739818377529526
Loss in iteration 15 : 0.49570899627462167
Loss in iteration 16 : 0.4914223292378207
Loss in iteration 17 : 0.48324672946367914
Loss in iteration 18 : 0.48080317595129424
Loss in iteration 19 : 0.4874612308929823
Loss in iteration 20 : 0.47942895086234566
Loss in iteration 21 : 0.4944051992105003
Loss in iteration 22 : 0.491910349840695
Loss in iteration 23 : 0.4912997563209469
Loss in iteration 24 : 0.4816861796992713
Loss in iteration 25 : 0.4866646442366216
Loss in iteration 26 : 0.47110247470290234
Loss in iteration 27 : 0.4760907657936549
Loss in iteration 28 : 0.4822404276830197
Loss in iteration 29 : 0.48854962254017503
Loss in iteration 30 : 0.4853800600188192
Loss in iteration 31 : 0.4788671375295285
Loss in iteration 32 : 0.47871513124946213
Loss in iteration 33 : 0.47155409357420713
Loss in iteration 34 : 0.4691296557036559
Loss in iteration 35 : 0.48345055535539416
Loss in iteration 36 : 0.4808410068377576
Loss in iteration 37 : 0.47184006067966816
Loss in iteration 38 : 0.47277835041582367
Loss in iteration 39 : 0.46299651973096617
Loss in iteration 40 : 0.47150554252038834
Loss in iteration 41 : 0.4763242746110864
Loss in iteration 42 : 0.4614558785376848
Loss in iteration 43 : 0.468576150847089
Loss in iteration 44 : 0.46962462268409216
Loss in iteration 45 : 0.4708788633326363
Loss in iteration 46 : 0.47111211798415803
Loss in iteration 47 : 0.4633554455623237
Loss in iteration 48 : 0.46977007283751765
Loss in iteration 49 : 0.46362576637805375
Loss in iteration 50 : 0.4553726392500062
Loss in iteration 51 : 0.4714533717650466
Loss in iteration 52 : 0.4607218786129681
Loss in iteration 53 : 0.4634384346539365
Loss in iteration 54 : 0.4594760755085587
Loss in iteration 55 : 0.4637167713310534
Loss in iteration 56 : 0.46954874611704606
Loss in iteration 57 : 0.45195689581201337
Loss in iteration 58 : 0.45521858859736536
Loss in iteration 59 : 0.45510016952314025
Loss in iteration 60 : 0.4648267961295109
Loss in iteration 61 : 0.4612860746037399
Loss in iteration 62 : 0.46375862338337154
Loss in iteration 63 : 0.46307948555641915
Loss in iteration 64 : 0.46159750079459205
Loss in iteration 65 : 0.47272204611377683
Loss in iteration 66 : 0.4564390305857832
Loss in iteration 67 : 0.4673577404392243
Loss in iteration 68 : 0.46045697337662544
Loss in iteration 69 : 0.4656097171636993
Loss in iteration 70 : 0.45644197852049273
Loss in iteration 71 : 0.47090771934397374
Loss in iteration 72 : 0.4569735902357781
Loss in iteration 73 : 0.46952718074939287
Loss in iteration 74 : 0.45914121449573425
Loss in iteration 75 : 0.4614723724175935
Loss in iteration 76 : 0.4745134780719891
Loss in iteration 77 : 0.45632212280041246
Loss in iteration 78 : 0.4654080833939147
Loss in iteration 79 : 0.4440648835217047
Loss in iteration 80 : 0.4633376910037991
Loss in iteration 81 : 0.46448655790295174
Loss in iteration 82 : 0.461473671997445
Loss in iteration 83 : 0.4630823200492697
Loss in iteration 84 : 0.4748713941217024
Loss in iteration 85 : 0.46309022641614467
Loss in iteration 86 : 0.47225066720681713
Loss in iteration 87 : 0.4515466219258741
Loss in iteration 88 : 0.4722050301527575
Loss in iteration 89 : 0.45587466241124014
Loss in iteration 90 : 0.458862568401856
Loss in iteration 91 : 0.4670064281402092
Loss in iteration 92 : 0.4551267036004871
Loss in iteration 93 : 0.46124846732219577
Loss in iteration 94 : 0.4604441294340151
Loss in iteration 95 : 0.45954527372899134
Loss in iteration 96 : 0.4682116809782847
Loss in iteration 97 : 0.4604503452178616
Loss in iteration 98 : 0.466600608132891
Loss in iteration 99 : 0.4600125037793817
Loss in iteration 100 : 0.46377886910084537
Loss in iteration 101 : 0.459423548072826
Loss in iteration 102 : 0.45464886689308914
Loss in iteration 103 : 0.4671165233066958
Loss in iteration 104 : 0.4618879356335175
Loss in iteration 105 : 0.4456826130024028
Loss in iteration 106 : 0.4643386594537881
Loss in iteration 107 : 0.476534026812219
Loss in iteration 108 : 0.47725516132982826
Loss in iteration 109 : 0.46126396884935744
Loss in iteration 110 : 0.4554168658339232
Loss in iteration 111 : 0.4643604853898676
Loss in iteration 112 : 0.458855393495133
Loss in iteration 113 : 0.45196797195434474
Loss in iteration 114 : 0.45806897355260323
Loss in iteration 115 : 0.4523324336807139
Loss in iteration 116 : 0.4645432484295585
Loss in iteration 117 : 0.4728848086020727
Loss in iteration 118 : 0.46975221949720686
Loss in iteration 119 : 0.4554452491696698
Loss in iteration 120 : 0.47572182848690725
Loss in iteration 121 : 0.4663989748396396
Loss in iteration 122 : 0.4767612848965804
Loss in iteration 123 : 0.4664166222414817
Loss in iteration 124 : 0.45771840206270736
Loss in iteration 125 : 0.4554287460086063
Loss in iteration 126 : 0.45874256465664104
Loss in iteration 127 : 0.45903781264239113
Loss in iteration 128 : 0.4652086461155699
Loss in iteration 129 : 0.45941801187407505
Loss in iteration 130 : 0.4658192264448987
Loss in iteration 131 : 0.45904886740088535
Loss in iteration 132 : 0.4566071061319376
Loss in iteration 133 : 0.4652488412505097
Loss in iteration 134 : 0.46270438380504236
Loss in iteration 135 : 0.46207233182400054
Loss in iteration 136 : 0.4604361563489706
Loss in iteration 137 : 0.46747351253152997
Loss in iteration 138 : 0.4647828849754873
Loss in iteration 139 : 0.47251012905677514
Loss in iteration 140 : 0.466784625023668
Loss in iteration 141 : 0.4677326051586945
Loss in iteration 142 : 0.45513639932188515
Loss in iteration 143 : 0.46384515045254715
Loss in iteration 144 : 0.4547183764247869
Loss in iteration 145 : 0.4495937171472418
Loss in iteration 146 : 0.4669696444756042
Loss in iteration 147 : 0.4705152274875679
Loss in iteration 148 : 0.4606063697610342
Loss in iteration 149 : 0.47497008964544785
Loss in iteration 150 : 0.46502899121498215
Loss in iteration 151 : 0.46120821261345174
Loss in iteration 152 : 0.47381955415433435
Loss in iteration 153 : 0.44902331803313716
Loss in iteration 154 : 0.470259555719351
Loss in iteration 155 : 0.46548882714375767
Loss in iteration 156 : 0.4494729165131379
Loss in iteration 157 : 0.4665938835380366
Loss in iteration 158 : 0.46091413249915797
Loss in iteration 159 : 0.46109412216211604
Loss in iteration 160 : 0.46044419597529923
Loss in iteration 161 : 0.4559678864199671
Loss in iteration 162 : 0.4653231527879557
Loss in iteration 163 : 0.46532140229040925
Loss in iteration 164 : 0.46600943496695535
Loss in iteration 165 : 0.458517030273752
Loss in iteration 166 : 0.4603692773130147
Loss in iteration 167 : 0.467968688119202
Loss in iteration 168 : 0.47175431697921527
Loss in iteration 169 : 0.45836812854342435
Loss in iteration 170 : 0.4665688398433161
Loss in iteration 171 : 0.4574972408061445
Loss in iteration 172 : 0.46464653153798363
Loss in iteration 173 : 0.45805879495790924
Loss in iteration 174 : 0.4567187736827587
Loss in iteration 175 : 0.4696861642871209
Loss in iteration 176 : 0.45421587377803807
Loss in iteration 177 : 0.4689442691719334
Loss in iteration 178 : 0.46070868782815394
Loss in iteration 179 : 0.46676259295136263
Loss in iteration 180 : 0.4550365326970799
Loss in iteration 181 : 0.46739547354306343
Loss in iteration 182 : 0.46064298159250067
Loss in iteration 183 : 0.4593085510346211
Loss in iteration 184 : 0.475235438129551
Loss in iteration 185 : 0.4600025706495473
Loss in iteration 186 : 0.4502400116509813
Loss in iteration 187 : 0.45557312507366343
Loss in iteration 188 : 0.46776365108738277
Loss in iteration 189 : 0.4662408618773265
Loss in iteration 190 : 0.4496438301215569
Loss in iteration 191 : 0.45302243906620726
Loss in iteration 192 : 0.4608301228813544
Loss in iteration 193 : 0.463096364405994
Loss in iteration 194 : 0.4617507955005332
Loss in iteration 195 : 0.4685111865841858
Loss in iteration 196 : 0.4748715617314327
Loss in iteration 197 : 0.45057852288128947
Loss in iteration 198 : 0.4707932091832077
Loss in iteration 199 : 0.4617443203214796
Loss in iteration 200 : 0.4502684360269518
Loss in iteration 201 : 0.45892816054422164
Loss in iteration 202 : 0.46971323296998285
Loss in iteration 203 : 0.4503589070964002
Loss in iteration 204 : 0.4572421066111529
Loss in iteration 205 : 0.46018767444216013
Loss in iteration 206 : 0.4594708483960959
Loss in iteration 207 : 0.4543697397047099
Loss in iteration 208 : 0.44993942870372944
Loss in iteration 209 : 0.4654287140930439
Loss in iteration 210 : 0.47139285291924743
Loss in iteration 211 : 0.4616368284970007
Loss in iteration 212 : 0.46773318018579474
Loss in iteration 213 : 0.4702620999247943
Loss in iteration 214 : 0.4812925365458412
Loss in iteration 215 : 0.47105564375888254
Loss in iteration 216 : 0.45884541648243704
Loss in iteration 217 : 0.45695884811472726
Loss in iteration 218 : 0.45521741105210184
Loss in iteration 219 : 0.46879807781717736
Loss in iteration 220 : 0.4673210708528284
Loss in iteration 221 : 0.46247301832933585
Loss in iteration 222 : 0.45527043222446184
Loss in iteration 223 : 0.46722108846212096
Loss in iteration 224 : 0.469989810046336
Loss in iteration 225 : 0.4606260432464413
Loss in iteration 226 : 0.46248809738710117
Loss in iteration 227 : 0.45281421220566587
Loss in iteration 228 : 0.4651178656194564
Loss in iteration 229 : 0.46471624609828704
Loss in iteration 230 : 0.45516130737616667
Loss in iteration 231 : 0.4624488580460227
Loss in iteration 232 : 0.466053271166474
Loss in iteration 233 : 0.4603655962602292
Loss in iteration 234 : 0.4657975883093827
Loss in iteration 235 : 0.4577706507200919
Loss in iteration 236 : 0.46254347929596035
Loss in iteration 237 : 0.4622596228467784
Loss in iteration 238 : 0.4610463013543586
Loss in iteration 239 : 0.46183689178883014
Loss in iteration 240 : 0.4516374489926826
Loss in iteration 241 : 0.4554551689725782
Loss in iteration 242 : 0.48255397568932057
Loss in iteration 243 : 0.4692563847549836
Loss in iteration 244 : 0.4598699238598778
Loss in iteration 245 : 0.4736910573259188
Loss in iteration 246 : 0.4569426357575037
Loss in iteration 247 : 0.453534782447752
Loss in iteration 248 : 0.463515066876237
Loss in iteration 249 : 0.46517783061197304
Loss in iteration 250 : 0.4549126854721795
Loss in iteration 251 : 0.46636921157302375
Loss in iteration 252 : 0.46078812554698184
Loss in iteration 253 : 0.47116588236820006
Loss in iteration 254 : 0.44750351099741026
Loss in iteration 255 : 0.4512212120566425
Loss in iteration 256 : 0.4751926022541155
Loss in iteration 257 : 0.44959667609686643
Loss in iteration 258 : 0.4565315140634681
Loss in iteration 259 : 0.456499978933471
Loss in iteration 260 : 0.4592792941401284
Loss in iteration 261 : 0.4477395093513736
Loss in iteration 262 : 0.46113055813163634
Loss in iteration 263 : 0.4607141866398768
Loss in iteration 264 : 0.47474497381857317
Loss in iteration 265 : 0.4501257640118122
Loss in iteration 266 : 0.47114679290660016
Loss in iteration 267 : 0.4677263919752179
Loss in iteration 268 : 0.47181423116138727
Loss in iteration 269 : 0.45426226944697706
Loss in iteration 270 : 0.4601227772430252
Loss in iteration 271 : 0.4479259420030711
Loss in iteration 272 : 0.4591638844790597
Loss in iteration 273 : 0.44650875156258063
Loss in iteration 274 : 0.45647197976040726
Loss in iteration 275 : 0.46504334166298344
Loss in iteration 276 : 0.4680253471752458
Loss in iteration 277 : 0.45416295077236357
Loss in iteration 278 : 0.44926130047465895
Loss in iteration 279 : 0.4483852369701285
Loss in iteration 280 : 0.4561989055735367
Loss in iteration 281 : 0.46272181286846115
Loss in iteration 282 : 0.45708293476272044
Loss in iteration 283 : 0.463087529365242
Loss in iteration 284 : 0.44691993105413014
Loss in iteration 285 : 0.4482069380614692
Loss in iteration 286 : 0.4596035273739455
Loss in iteration 287 : 0.4650725053356517
Loss in iteration 288 : 0.46440213273407815
Loss in iteration 289 : 0.4563496422652487
Loss in iteration 290 : 0.4524834903243994
Loss in iteration 291 : 0.4651600181080289
Loss in iteration 292 : 0.4631955041875659
Loss in iteration 293 : 0.4565476381342603
Loss in iteration 294 : 0.4650059153557466
Loss in iteration 295 : 0.4528733859939056
Loss in iteration 296 : 0.45644061215003107
Loss in iteration 297 : 0.46356990633867806
Loss in iteration 298 : 0.46033441051313373
Loss in iteration 299 : 0.46149471702336164
Loss in iteration 300 : 0.46659383848151864
Loss in iteration 301 : 0.46451644611244114
Loss in iteration 302 : 0.4564390029707385
Loss in iteration 303 : 0.45252543928535943
Loss in iteration 304 : 0.4544915040879878
Loss in iteration 305 : 0.45751410326158637
Loss in iteration 306 : 0.457934156443966
Loss in iteration 307 : 0.45906247075866186
Loss in iteration 308 : 0.4589542598505778
Loss in iteration 309 : 0.4508280563195596
Loss in iteration 310 : 0.45741278311236017
Loss in iteration 311 : 0.46652278321408686
Loss in iteration 312 : 0.47839155133798833
Loss in iteration 313 : 0.44693266505923
Loss in iteration 314 : 0.45608011265253384
Loss in iteration 315 : 0.4580375801786188
Loss in iteration 316 : 0.47665143737539917
Loss in iteration 317 : 0.46314621611573137
Loss in iteration 318 : 0.4723494099284521
Loss in iteration 319 : 0.4549509203120418
Loss in iteration 320 : 0.45302455497764155
Loss in iteration 321 : 0.45872943283872464
Loss in iteration 322 : 0.4604454806894573
Loss in iteration 323 : 0.4567838016877816
Loss in iteration 324 : 0.459656145282122
Loss in iteration 325 : 0.4574727656392826
Loss in iteration 326 : 0.4729288527200063
Loss in iteration 327 : 0.45032110714404044
Loss in iteration 328 : 0.4551687838905255
Loss in iteration 329 : 0.4569226126800589
Loss in iteration 330 : 0.4519209462329239
Loss in iteration 331 : 0.4607637114913474
Loss in iteration 332 : 0.4483382848947642
Loss in iteration 333 : 0.4479079774982352
Loss in iteration 334 : 0.45689298564175573
Loss in iteration 335 : 0.4545167595751661
Loss in iteration 336 : 0.459979151602812
Loss in iteration 337 : 0.4635254826126183
Loss in iteration 338 : 0.46913257459770225
Loss in iteration 339 : 0.4723721367818793
Loss in iteration 340 : 0.4663266408727978
Loss in iteration 341 : 0.4575110865746622
Loss in iteration 342 : 0.45923116353064397
Loss in iteration 343 : 0.4591842991285839
Loss in iteration 344 : 0.46429235796065754
Loss in iteration 345 : 0.4576367450347352
Loss in iteration 346 : 0.45297294894807877
Loss in iteration 347 : 0.463202607150579
Loss in iteration 348 : 0.4620243445784638
Loss in iteration 349 : 0.46906775003665724
Loss in iteration 350 : 0.467399768044857
Loss in iteration 351 : 0.46066840353377897
Loss in iteration 352 : 0.4580004429148675
Loss in iteration 353 : 0.4521633616382464
Loss in iteration 354 : 0.45948906496607866
Loss in iteration 355 : 0.46099748814515745
Loss in iteration 356 : 0.45315907691195223
Loss in iteration 357 : 0.45888969550414566
Loss in iteration 358 : 0.46686264120714366
Loss in iteration 359 : 0.45711430414527027
Loss in iteration 360 : 0.46580250009211766
Loss in iteration 361 : 0.45048394902640243
Loss in iteration 362 : 0.4702072704970335
Loss in iteration 363 : 0.4610767348181559
Loss in iteration 364 : 0.4588885926317131
Loss in iteration 365 : 0.46444253355029275
Loss in iteration 366 : 0.45780653891374307
Loss in iteration 367 : 0.4669577448399135
Loss in iteration 368 : 0.4701809049053917
Loss in iteration 369 : 0.47142321985471786
Loss in iteration 370 : 0.46061553997642635
Loss in iteration 371 : 0.4747410462607312
Loss in iteration 372 : 0.4561228007620438
Loss in iteration 373 : 0.46206973278557767
Loss in iteration 374 : 0.45801669945647916
Loss in iteration 375 : 0.4559856186925801
Loss in iteration 376 : 0.4644518176721857
Loss in iteration 377 : 0.4600778308996984
Loss in iteration 378 : 0.46425463196383665
Loss in iteration 379 : 0.46300736782555435
Loss in iteration 380 : 0.46015003156336776
Loss in iteration 381 : 0.4490799684324773
Loss in iteration 382 : 0.45467004177655956
Loss in iteration 383 : 0.4656236367736194
Loss in iteration 384 : 0.46731083785064464
Loss in iteration 385 : 0.4632978766939994
Loss in iteration 386 : 0.44749310964958167
Loss in iteration 387 : 0.46622567686075855
Loss in iteration 388 : 0.4717251241345177
Loss in iteration 389 : 0.45644773767077057
Loss in iteration 390 : 0.4576872873666459
Loss in iteration 391 : 0.4602876835174656
Loss in iteration 392 : 0.4499312854858721
Loss in iteration 393 : 0.45434699285784735
Loss in iteration 394 : 0.4567843844238161
Loss in iteration 395 : 0.4652460252833819
Loss in iteration 396 : 0.4576420706404921
Loss in iteration 397 : 0.47135451647767534
Loss in iteration 398 : 0.4699226416179568
Loss in iteration 399 : 0.4561643580202611
Loss in iteration 400 : 0.44984513583893254
Testing accuracy  of updater 7 on alg 0 with rate 0.8 = 0.78825, training accuracy 0.78825, time elapsed: 5408 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6854344848296791
Loss in iteration 3 : 0.6727475807177227
Loss in iteration 4 : 0.6680455258363059
Loss in iteration 5 : 0.6510508069415365
Loss in iteration 6 : 0.640799483407733
Loss in iteration 7 : 0.6294244724143886
Loss in iteration 8 : 0.6177858529523516
Loss in iteration 9 : 0.6021876619294271
Loss in iteration 10 : 0.5897496518550556
Loss in iteration 11 : 0.574997160066869
Loss in iteration 12 : 0.5657327309813955
Loss in iteration 13 : 0.5556768616192262
Loss in iteration 14 : 0.5450919002617101
Loss in iteration 15 : 0.5450282227527488
Loss in iteration 16 : 0.5348308633019505
Loss in iteration 17 : 0.5283690631055443
Loss in iteration 18 : 0.5225510999878044
Loss in iteration 19 : 0.5221254174292552
Loss in iteration 20 : 0.5133262144560159
Loss in iteration 21 : 0.520558071280724
Loss in iteration 22 : 0.5162660955896499
Loss in iteration 23 : 0.5135841916037415
Loss in iteration 24 : 0.5026486932884969
Loss in iteration 25 : 0.5073509443330468
Loss in iteration 26 : 0.49524902887758687
Loss in iteration 27 : 0.4954576770911361
Loss in iteration 28 : 0.49803384504913023
Loss in iteration 29 : 0.5068618523812979
Loss in iteration 30 : 0.5024852636014798
Loss in iteration 31 : 0.4974552317736483
Loss in iteration 32 : 0.4951111920042332
Loss in iteration 33 : 0.4896329192382979
Loss in iteration 34 : 0.48997524880707716
Loss in iteration 35 : 0.4975850527589023
Loss in iteration 36 : 0.4969209912190458
Loss in iteration 37 : 0.49006761738455973
Loss in iteration 38 : 0.48797342451281256
Loss in iteration 39 : 0.48345478813347603
Loss in iteration 40 : 0.4876860126534019
Loss in iteration 41 : 0.4911970878121991
Loss in iteration 42 : 0.4801671687814865
Loss in iteration 43 : 0.4884981212913467
Loss in iteration 44 : 0.48862926471701507
Loss in iteration 45 : 0.48736302571183227
Loss in iteration 46 : 0.4866324914157848
Loss in iteration 47 : 0.4817966606075006
Loss in iteration 48 : 0.4876708164613391
Loss in iteration 49 : 0.47996486845552566
Loss in iteration 50 : 0.4739620338892092
Loss in iteration 51 : 0.48972786965542625
Loss in iteration 52 : 0.47712036283813314
Loss in iteration 53 : 0.47676210169087485
Loss in iteration 54 : 0.4746291567818061
Loss in iteration 55 : 0.47825694445501793
Loss in iteration 56 : 0.48118169890942286
Loss in iteration 57 : 0.4664392723450469
Loss in iteration 58 : 0.4705275628760005
Loss in iteration 59 : 0.4704121377925271
Loss in iteration 60 : 0.4769264930395697
Loss in iteration 61 : 0.4756539358709973
Loss in iteration 62 : 0.47700146483670847
Loss in iteration 63 : 0.473996661787512
Loss in iteration 64 : 0.47334254361455996
Loss in iteration 65 : 0.48402171560968615
Loss in iteration 66 : 0.46781580004014445
Loss in iteration 67 : 0.47687949350840253
Loss in iteration 68 : 0.4680968165576707
Loss in iteration 69 : 0.47814920344050443
Loss in iteration 70 : 0.46843697490121256
Loss in iteration 71 : 0.48319963444806635
Loss in iteration 72 : 0.46897713931793056
Loss in iteration 73 : 0.47750354055628635
Loss in iteration 74 : 0.4708652696373406
Loss in iteration 75 : 0.47227804549630703
Loss in iteration 76 : 0.481128117072967
Loss in iteration 77 : 0.46698007856844614
Loss in iteration 78 : 0.4740830134886984
Loss in iteration 79 : 0.4563110236041627
Loss in iteration 80 : 0.4715097676710488
Loss in iteration 81 : 0.47392504809094904
Loss in iteration 82 : 0.4701448185255658
Loss in iteration 83 : 0.47344593964344395
Loss in iteration 84 : 0.4819808971468033
Loss in iteration 85 : 0.4727112767238905
Loss in iteration 86 : 0.47902589158709913
Loss in iteration 87 : 0.4610819761479738
Loss in iteration 88 : 0.4800483886195958
Loss in iteration 89 : 0.4639768417513216
Loss in iteration 90 : 0.46770602962336755
Loss in iteration 91 : 0.4763710551693349
Loss in iteration 92 : 0.46514302582207706
Loss in iteration 93 : 0.46867040478710437
Loss in iteration 94 : 0.46838241666866287
Loss in iteration 95 : 0.4655909765961508
Loss in iteration 96 : 0.47705555822058754
Loss in iteration 97 : 0.4681251453484737
Loss in iteration 98 : 0.4731869006333086
Loss in iteration 99 : 0.4670747948831088
Loss in iteration 100 : 0.4724373731705585
Loss in iteration 101 : 0.4658257199061724
Loss in iteration 102 : 0.4629907107559613
Loss in iteration 103 : 0.4747364687240903
Loss in iteration 104 : 0.4691717018101256
Loss in iteration 105 : 0.45580844736354614
Loss in iteration 106 : 0.4681957608877884
Loss in iteration 107 : 0.47913706908420384
Loss in iteration 108 : 0.4818379540728596
Loss in iteration 109 : 0.46650009703894363
Loss in iteration 110 : 0.4616322879830733
Loss in iteration 111 : 0.4708511603482137
Loss in iteration 112 : 0.4642342868890856
Loss in iteration 113 : 0.4591201003395645
Loss in iteration 114 : 0.4613513715074384
Loss in iteration 115 : 0.45910975926356357
Loss in iteration 116 : 0.4691075797078809
Loss in iteration 117 : 0.47555273858062785
Loss in iteration 118 : 0.47402931035315815
Loss in iteration 119 : 0.46042227776309863
Loss in iteration 120 : 0.47574418719258194
Loss in iteration 121 : 0.47089264843732614
Loss in iteration 122 : 0.47792193501727603
Loss in iteration 123 : 0.4699914189606209
Loss in iteration 124 : 0.46349437550890016
Loss in iteration 125 : 0.46230216124659623
Loss in iteration 126 : 0.4620735618963973
Loss in iteration 127 : 0.4638849831305898
Loss in iteration 128 : 0.46935744919136624
Loss in iteration 129 : 0.4637736461562431
Loss in iteration 130 : 0.46902417919484596
Loss in iteration 131 : 0.46468292082203594
Loss in iteration 132 : 0.46273881079757373
Loss in iteration 133 : 0.4688626691549647
Loss in iteration 134 : 0.4670072469203672
Loss in iteration 135 : 0.4662631201361594
Loss in iteration 136 : 0.46594728400720126
Loss in iteration 137 : 0.4714246691164462
Loss in iteration 138 : 0.47038022937483326
Loss in iteration 139 : 0.47562901427167714
Loss in iteration 140 : 0.46977288857998406
Loss in iteration 141 : 0.47184082722593407
Loss in iteration 142 : 0.46125458208812886
Loss in iteration 143 : 0.4666121285477823
Loss in iteration 144 : 0.4607801017730899
Loss in iteration 145 : 0.4545948863765008
Loss in iteration 146 : 0.47076557389164914
Loss in iteration 147 : 0.47378341646744954
Loss in iteration 148 : 0.4646870920379003
Loss in iteration 149 : 0.4773850439240253
Loss in iteration 150 : 0.4687403599600125
Loss in iteration 151 : 0.464568603900651
Loss in iteration 152 : 0.47465091933002673
Loss in iteration 153 : 0.4520777638125629
Loss in iteration 154 : 0.4731953729740471
Loss in iteration 155 : 0.4700066775060064
Loss in iteration 156 : 0.453662923460335
Loss in iteration 157 : 0.46825591828586544
Loss in iteration 158 : 0.4604450916077598
Loss in iteration 159 : 0.4615080636151547
Loss in iteration 160 : 0.4606601352658963
Loss in iteration 161 : 0.4600497664657187
Loss in iteration 162 : 0.46337777195439456
Loss in iteration 163 : 0.4683004631226266
Loss in iteration 164 : 0.46794435686516606
Loss in iteration 165 : 0.46013074375640695
Loss in iteration 166 : 0.4624379512512661
Loss in iteration 167 : 0.4719893039632992
Loss in iteration 168 : 0.47327598613736604
Loss in iteration 169 : 0.46219123062869066
Loss in iteration 170 : 0.46811628327638477
Loss in iteration 171 : 0.4589492718043465
Loss in iteration 172 : 0.46764825510758345
Loss in iteration 173 : 0.46189705868821807
Loss in iteration 174 : 0.45813582812572595
Loss in iteration 175 : 0.47255045109589566
Loss in iteration 176 : 0.45838752303888025
Loss in iteration 177 : 0.47156179380032875
Loss in iteration 178 : 0.460891374643379
Loss in iteration 179 : 0.469389556363511
Loss in iteration 180 : 0.4578078869174644
Loss in iteration 181 : 0.46800714703721297
Loss in iteration 182 : 0.46255175485708716
Loss in iteration 183 : 0.46142428647511485
Loss in iteration 184 : 0.4782096889149919
Loss in iteration 185 : 0.46291219441257214
Loss in iteration 186 : 0.4539229164964024
Loss in iteration 187 : 0.45959896444526216
Loss in iteration 188 : 0.47071874281319503
Loss in iteration 189 : 0.4684425377365961
Loss in iteration 190 : 0.4535878779849482
Loss in iteration 191 : 0.45775515102002284
Loss in iteration 192 : 0.4616679369177659
Loss in iteration 193 : 0.4660382403796844
Loss in iteration 194 : 0.4638320905534809
Loss in iteration 195 : 0.4725052822793639
Loss in iteration 196 : 0.47924357715458515
Loss in iteration 197 : 0.45554952167595386
Loss in iteration 198 : 0.47248819362266703
Loss in iteration 199 : 0.4631194244474056
Loss in iteration 200 : 0.4550408252635174
Loss in iteration 201 : 0.4617288075014933
Loss in iteration 202 : 0.4728738793445686
Loss in iteration 203 : 0.4530403102410912
Loss in iteration 204 : 0.45976655861937615
Loss in iteration 205 : 0.4631603770741077
Loss in iteration 206 : 0.46262759735491565
Loss in iteration 207 : 0.45832439986675283
Loss in iteration 208 : 0.4539295747767593
Loss in iteration 209 : 0.46730663281185797
Loss in iteration 210 : 0.47329534332108364
Loss in iteration 211 : 0.4658167002399126
Loss in iteration 212 : 0.47066610379865076
Loss in iteration 213 : 0.4715255677860566
Loss in iteration 214 : 0.4833987071156363
Loss in iteration 215 : 0.472937259227818
Loss in iteration 216 : 0.4620264702272834
Loss in iteration 217 : 0.4611227050662604
Loss in iteration 218 : 0.4575718203686957
Loss in iteration 219 : 0.47033251433852036
Loss in iteration 220 : 0.46938251519377266
Loss in iteration 221 : 0.4657377903939479
Loss in iteration 222 : 0.4567044030227606
Loss in iteration 223 : 0.4686396230750599
Loss in iteration 224 : 0.47092767668627317
Loss in iteration 225 : 0.46194475971984317
Loss in iteration 226 : 0.4651902193611075
Loss in iteration 227 : 0.4553094402220093
Loss in iteration 228 : 0.4664683189458003
Loss in iteration 229 : 0.46608228165624843
Loss in iteration 230 : 0.4578740407427996
Loss in iteration 231 : 0.4657009049482975
Loss in iteration 232 : 0.4684818085316696
Loss in iteration 233 : 0.4615466406182036
Loss in iteration 234 : 0.46744165843350743
Loss in iteration 235 : 0.45879367237140717
Loss in iteration 236 : 0.4642824670862543
Loss in iteration 237 : 0.4647069830497502
Loss in iteration 238 : 0.4617556380272047
Loss in iteration 239 : 0.4635114214162704
Loss in iteration 240 : 0.45516794986025855
Loss in iteration 241 : 0.45813666946684684
Loss in iteration 242 : 0.4851293519075812
Loss in iteration 243 : 0.4726102802485635
Loss in iteration 244 : 0.4617844078240019
Loss in iteration 245 : 0.47499979144055937
Loss in iteration 246 : 0.4602023395977226
Loss in iteration 247 : 0.4557892590299849
Loss in iteration 248 : 0.46500530741535673
Loss in iteration 249 : 0.4672274324532686
Loss in iteration 250 : 0.4568600545704396
Loss in iteration 251 : 0.46877071173116636
Loss in iteration 252 : 0.4629795942530317
Loss in iteration 253 : 0.47330584645300716
Loss in iteration 254 : 0.44934711541668315
Loss in iteration 255 : 0.45469670744229224
Loss in iteration 256 : 0.477245784744322
Loss in iteration 257 : 0.4530255468178151
Loss in iteration 258 : 0.45909797050950246
Loss in iteration 259 : 0.4589050692956082
Loss in iteration 260 : 0.46197113250214666
Loss in iteration 261 : 0.4505755910019814
Loss in iteration 262 : 0.46325333031686633
Loss in iteration 263 : 0.46331918386541265
Loss in iteration 264 : 0.4756868269467088
Loss in iteration 265 : 0.45249533750650195
Loss in iteration 266 : 0.4719741572418927
Loss in iteration 267 : 0.47011877372477145
Loss in iteration 268 : 0.4748102298468197
Loss in iteration 269 : 0.4565079212912872
Loss in iteration 270 : 0.46170336993279004
Loss in iteration 271 : 0.45038416152993305
Loss in iteration 272 : 0.4629846370888505
Loss in iteration 273 : 0.44829034842939025
Loss in iteration 274 : 0.45785968193371834
Loss in iteration 275 : 0.4670195604095142
Loss in iteration 276 : 0.4711672818226025
Loss in iteration 277 : 0.4552319806719776
Loss in iteration 278 : 0.4531716629595352
Loss in iteration 279 : 0.4512372690539532
Loss in iteration 280 : 0.45776395972401673
Loss in iteration 281 : 0.4648995655738222
Loss in iteration 282 : 0.4578095927168917
Loss in iteration 283 : 0.4642013399935912
Loss in iteration 284 : 0.44987247276283615
Loss in iteration 285 : 0.4510831204153613
Loss in iteration 286 : 0.46144286616071234
Loss in iteration 287 : 0.4670588323801076
Loss in iteration 288 : 0.4654773703931786
Loss in iteration 289 : 0.4574474229261782
Loss in iteration 290 : 0.45286021636603846
Loss in iteration 291 : 0.4668110876790765
Loss in iteration 292 : 0.46346090491912745
Loss in iteration 293 : 0.4579877042667542
Loss in iteration 294 : 0.4685018570566591
Loss in iteration 295 : 0.4540380781173636
Loss in iteration 296 : 0.4571197907931809
Loss in iteration 297 : 0.46601060175682707
Loss in iteration 298 : 0.4618781605127536
Loss in iteration 299 : 0.4642547942640226
Loss in iteration 300 : 0.4664872910194293
Loss in iteration 301 : 0.46538928169547866
Loss in iteration 302 : 0.45886020123605525
Loss in iteration 303 : 0.4539998804515747
Loss in iteration 304 : 0.4564363162269122
Loss in iteration 305 : 0.45980554322738254
Loss in iteration 306 : 0.4601632810926727
Loss in iteration 307 : 0.46099300873548055
Loss in iteration 308 : 0.461458661347292
Loss in iteration 309 : 0.45277059946235515
Loss in iteration 310 : 0.459162753890986
Loss in iteration 311 : 0.4687483734376808
Loss in iteration 312 : 0.4799921291576319
Loss in iteration 313 : 0.449805721086439
Loss in iteration 314 : 0.4567509924985459
Loss in iteration 315 : 0.46022732168624086
Loss in iteration 316 : 0.478031476163727
Loss in iteration 317 : 0.46191993454849223
Loss in iteration 318 : 0.47227962588717054
Loss in iteration 319 : 0.4567396037349404
Loss in iteration 320 : 0.45562905373719975
Loss in iteration 321 : 0.45539601471667973
Loss in iteration 322 : 0.46012270138316186
Loss in iteration 323 : 0.4579047718853838
Loss in iteration 324 : 0.4598688068683705
Loss in iteration 325 : 0.4584521225251949
Loss in iteration 326 : 0.47396177484790836
Loss in iteration 327 : 0.4529691634945181
Loss in iteration 328 : 0.4546613521869931
Loss in iteration 329 : 0.45701502934083654
Loss in iteration 330 : 0.452414717223573
Loss in iteration 331 : 0.46121281248275925
Loss in iteration 332 : 0.4486019673905979
Loss in iteration 333 : 0.4492017025428782
Loss in iteration 334 : 0.45906381358877596
Loss in iteration 335 : 0.4534729888369127
Loss in iteration 336 : 0.46045600481670984
Loss in iteration 337 : 0.4644753095110481
Loss in iteration 338 : 0.46909499672348653
Loss in iteration 339 : 0.4725566802691324
Loss in iteration 340 : 0.4657972305640536
Loss in iteration 341 : 0.45824456647813117
Loss in iteration 342 : 0.46018691861814437
Loss in iteration 343 : 0.4594549880674546
Loss in iteration 344 : 0.4664252026167671
Loss in iteration 345 : 0.4594737472776811
Loss in iteration 346 : 0.45385607303828085
Loss in iteration 347 : 0.4604493120663394
Loss in iteration 348 : 0.462509071649127
Loss in iteration 349 : 0.4673437669640136
Loss in iteration 350 : 0.4675859930414283
Loss in iteration 351 : 0.4621821197018599
Loss in iteration 352 : 0.45853750480250965
Loss in iteration 353 : 0.45336701625862924
Loss in iteration 354 : 0.46093585707749163
Loss in iteration 355 : 0.4608040127043327
Loss in iteration 356 : 0.45434450458828024
Loss in iteration 357 : 0.4585251500843436
Loss in iteration 358 : 0.4676950689221077
Loss in iteration 359 : 0.4580208392135011
Loss in iteration 360 : 0.4654600800881361
Loss in iteration 361 : 0.45133758708987104
Loss in iteration 362 : 0.47119237128492664
Loss in iteration 363 : 0.46250044791887357
Loss in iteration 364 : 0.45980843096590135
Loss in iteration 365 : 0.4632402042437047
Loss in iteration 366 : 0.457522281603432
Loss in iteration 367 : 0.46706542231514053
Loss in iteration 368 : 0.46780760154898055
Loss in iteration 369 : 0.47275295793888855
Loss in iteration 370 : 0.4579463463938385
Loss in iteration 371 : 0.47514360430808583
Loss in iteration 372 : 0.4551462966636795
Loss in iteration 373 : 0.4609111967198779
Loss in iteration 374 : 0.4583413383616681
Loss in iteration 375 : 0.45758309017658233
Loss in iteration 376 : 0.4660424972339851
Loss in iteration 377 : 0.461628611999776
Loss in iteration 378 : 0.46393193785809
Loss in iteration 379 : 0.463737296219257
Loss in iteration 380 : 0.45875555869469636
Loss in iteration 381 : 0.45004045486126937
Loss in iteration 382 : 0.4561265401481649
Loss in iteration 383 : 0.46298763380495206
Loss in iteration 384 : 0.468481522110689
Loss in iteration 385 : 0.46153420840455145
Loss in iteration 386 : 0.4492624629587537
Loss in iteration 387 : 0.4665428623150384
Loss in iteration 388 : 0.47021287342685847
Loss in iteration 389 : 0.4571312690964362
Loss in iteration 390 : 0.45876510649725016
Loss in iteration 391 : 0.46171366190125057
Loss in iteration 392 : 0.45092530832799615
Loss in iteration 393 : 0.4548323620624667
Loss in iteration 394 : 0.4583694633889438
Loss in iteration 395 : 0.4663276893189154
Loss in iteration 396 : 0.45959045686024796
Loss in iteration 397 : 0.47279163074111497
Loss in iteration 398 : 0.46877839865886695
Loss in iteration 399 : 0.4573070314544471
Loss in iteration 400 : 0.45149759402355966
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.78675, training accuracy 0.78675, time elapsed: 5947 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 7.314219254326836
Loss in iteration 3 : 2.920346396290651
Loss in iteration 4 : 1.6316011682933673
Loss in iteration 5 : 1.3338071913169063
Loss in iteration 6 : 1.0886858122797816
Loss in iteration 7 : 0.9358036553025848
Loss in iteration 8 : 0.8991519586267934
Loss in iteration 9 : 0.7691634723646363
Loss in iteration 10 : 0.6518223142225834
Loss in iteration 11 : 0.6104367020621686
Loss in iteration 12 : 0.5621729326267059
Loss in iteration 13 : 0.5509385932110952
Loss in iteration 14 : 0.5454095837263581
Loss in iteration 15 : 0.5704991367932284
Loss in iteration 16 : 0.5778317127754352
Loss in iteration 17 : 0.5591181935757088
Loss in iteration 18 : 0.5679516024881801
Loss in iteration 19 : 0.5723694870861019
Loss in iteration 20 : 0.5599824632658635
Loss in iteration 21 : 0.5987311119769884
Loss in iteration 22 : 0.6096495422551074
Loss in iteration 23 : 0.6665565910915415
Loss in iteration 24 : 0.6949850427408876
Loss in iteration 25 : 0.766423146028537
Loss in iteration 26 : 0.7546312429395937
Loss in iteration 27 : 0.7517850550937357
Loss in iteration 28 : 0.7451915602293927
Loss in iteration 29 : 0.7473775049074476
Loss in iteration 30 : 0.7331487092926449
Loss in iteration 31 : 0.7300037322066959
Loss in iteration 32 : 0.731108223356534
Loss in iteration 33 : 0.670416734396858
Loss in iteration 34 : 0.6347255337847615
Loss in iteration 35 : 0.645608908402735
Loss in iteration 36 : 0.6473659788204055
Loss in iteration 37 : 0.6403947306609696
Loss in iteration 38 : 0.6621714733099201
Loss in iteration 39 : 0.6762322424149879
Loss in iteration 40 : 0.7189336525598853
Loss in iteration 41 : 0.6885243022244392
Loss in iteration 42 : 0.6348684772225212
Loss in iteration 43 : 0.6122902256891382
Loss in iteration 44 : 0.609756836724345
Loss in iteration 45 : 0.6157816895704606
Loss in iteration 46 : 0.6353148577837324
Loss in iteration 47 : 0.6572284894276574
Loss in iteration 48 : 0.6646073457666527
Loss in iteration 49 : 0.6258035971194433
Loss in iteration 50 : 0.5998039515624463
Loss in iteration 51 : 0.6320585343760304
Loss in iteration 52 : 0.6280022959003502
Loss in iteration 53 : 0.6372243988774429
Loss in iteration 54 : 0.634580398073411
Loss in iteration 55 : 0.6348394343872261
Loss in iteration 56 : 0.6588451757838283
Loss in iteration 57 : 0.6417137419548298
Loss in iteration 58 : 0.6408041477148433
Loss in iteration 59 : 0.6241521939127433
Loss in iteration 60 : 0.6063345907435973
Loss in iteration 61 : 0.5852336720046264
Loss in iteration 62 : 0.5946776052234077
Loss in iteration 63 : 0.6163075423129968
Loss in iteration 64 : 0.6314387867340756
Loss in iteration 65 : 0.6539019044525702
Loss in iteration 66 : 0.6188729808331095
Loss in iteration 67 : 0.6447051894426246
Loss in iteration 68 : 0.6038608669791743
Loss in iteration 69 : 0.5998589931774134
Loss in iteration 70 : 0.5801148555295866
Loss in iteration 71 : 0.608084827874327
Loss in iteration 72 : 0.6192286316091078
Loss in iteration 73 : 0.6191077027442807
Loss in iteration 74 : 0.6113736693281662
Loss in iteration 75 : 0.6457660305483945
Loss in iteration 76 : 0.6888801707544063
Loss in iteration 77 : 0.6537029595051669
Loss in iteration 78 : 0.6146471378442079
Loss in iteration 79 : 0.5679302280211193
Loss in iteration 80 : 0.5881041025897361
Loss in iteration 81 : 0.5946592820225224
Loss in iteration 82 : 0.6164542941946797
Loss in iteration 83 : 0.6404577025010695
Loss in iteration 84 : 0.717978500343775
Loss in iteration 85 : 0.6769759176731238
Loss in iteration 86 : 0.6731217975738336
Loss in iteration 87 : 0.6649777181164567
Loss in iteration 88 : 0.7480644696366421
Loss in iteration 89 : 0.7122839227764388
Loss in iteration 90 : 0.6898063195109946
Loss in iteration 91 : 0.6692747209509163
Loss in iteration 92 : 0.6263767092213353
Loss in iteration 93 : 0.6234884687252562
Loss in iteration 94 : 0.5933120205500038
Loss in iteration 95 : 0.5907504990215983
Loss in iteration 96 : 0.5909146613265667
Loss in iteration 97 : 0.6185846003973084
Loss in iteration 98 : 0.6690105228625748
Loss in iteration 99 : 0.6764692772009517
Loss in iteration 100 : 0.6691894871966999
Loss in iteration 101 : 0.6635557305374004
Loss in iteration 102 : 0.6295222326411304
Loss in iteration 103 : 0.6389886842074352
Loss in iteration 104 : 0.6688186510575469
Loss in iteration 105 : 0.6648021494083284
Loss in iteration 106 : 0.6681920019494378
Loss in iteration 107 : 0.6743039966866051
Loss in iteration 108 : 0.6912390524874336
Loss in iteration 109 : 0.6806019465214559
Loss in iteration 110 : 0.6952854068516947
Loss in iteration 111 : 0.6779897692619171
Loss in iteration 112 : 0.5856416989328154
Loss in iteration 113 : 0.537564419967207
Loss in iteration 114 : 0.5678551173614039
Loss in iteration 115 : 0.6073811125577026
Loss in iteration 116 : 0.5957728128208728
Loss in iteration 117 : 0.6243287140379487
Loss in iteration 118 : 0.6069772009083081
Loss in iteration 119 : 0.584519713488264
Loss in iteration 120 : 0.6198356232559453
Loss in iteration 121 : 0.6468995090964296
Loss in iteration 122 : 0.6926475644276682
Loss in iteration 123 : 0.6526498277440801
Loss in iteration 124 : 0.6506425178106936
Loss in iteration 125 : 0.6876337398113439
Loss in iteration 126 : 0.6986770018911428
Loss in iteration 127 : 0.7019509677060192
Loss in iteration 128 : 0.6667108560617806
Loss in iteration 129 : 0.6345335271345179
Loss in iteration 130 : 0.6501680918355366
Loss in iteration 131 : 0.6263617880891372
Loss in iteration 132 : 0.5836776487725176
Loss in iteration 133 : 0.5880954372795116
Loss in iteration 134 : 0.6034876385636814
Loss in iteration 135 : 0.6147882932230463
Loss in iteration 136 : 0.5913843051317507
Loss in iteration 137 : 0.6020056478629804
Loss in iteration 138 : 0.6172260797561988
Loss in iteration 139 : 0.6531678294802156
Loss in iteration 140 : 0.6539075738301656
Loss in iteration 141 : 0.6461052212511899
Loss in iteration 142 : 0.6588154694352166
Loss in iteration 143 : 0.699929183979787
Loss in iteration 144 : 0.6993828724754054
Loss in iteration 145 : 0.6699105792414181
Loss in iteration 146 : 0.6599479768830845
Loss in iteration 147 : 0.6233113826821104
Loss in iteration 148 : 0.6058390994867378
Loss in iteration 149 : 0.6290020917773091
Loss in iteration 150 : 0.6082378114352238
Loss in iteration 151 : 0.618227614066483
Loss in iteration 152 : 0.6479037197240385
Loss in iteration 153 : 0.6267337721879055
Loss in iteration 154 : 0.667640966016913
Loss in iteration 155 : 0.6553415977607585
Loss in iteration 156 : 0.6528220475496592
Loss in iteration 157 : 0.7161599615597846
Loss in iteration 158 : 0.7175174337319128
Loss in iteration 159 : 0.6897086560302048
Loss in iteration 160 : 0.6301751822383905
Loss in iteration 161 : 0.6104699562090963
Loss in iteration 162 : 0.6199879669735155
Loss in iteration 163 : 0.6176169123724099
Loss in iteration 164 : 0.6341031682396995
Loss in iteration 165 : 0.6567571346533854
Loss in iteration 166 : 0.6252296362975094
Loss in iteration 167 : 0.5970477340260192
Loss in iteration 168 : 0.5931499948557794
Loss in iteration 169 : 0.5783444460866421
Loss in iteration 170 : 0.615521850667612
Loss in iteration 171 : 0.6166118691025962
Loss in iteration 172 : 0.6076423917796853
Loss in iteration 173 : 0.5985892491446897
Loss in iteration 174 : 0.600852476862046
Loss in iteration 175 : 0.6512891289642857
Loss in iteration 176 : 0.6385090901480989
Loss in iteration 177 : 0.6603401975160134
Loss in iteration 178 : 0.6719980273161503
Loss in iteration 179 : 0.7062333944180829
Loss in iteration 180 : 0.7144742960042559
Loss in iteration 181 : 0.7225316499377769
Loss in iteration 182 : 0.6916024375460705
Loss in iteration 183 : 0.6922734825564989
Loss in iteration 184 : 0.6619572362789787
Loss in iteration 185 : 0.6103634464232404
Loss in iteration 186 : 0.5892203783536046
Loss in iteration 187 : 0.5781127838100718
Loss in iteration 188 : 0.5763862075838654
Loss in iteration 189 : 0.5714422797849129
Loss in iteration 190 : 0.562146117486713
Loss in iteration 191 : 0.5673554132954457
Loss in iteration 192 : 0.6070902149200356
Loss in iteration 193 : 0.6535575357004954
Loss in iteration 194 : 0.6654181502051626
Loss in iteration 195 : 0.6832111201259123
Loss in iteration 196 : 0.7289896226429302
Loss in iteration 197 : 0.6893340723555655
Loss in iteration 198 : 0.7021695594969253
Loss in iteration 199 : 0.6514079561973795
Loss in iteration 200 : 0.6101779013648427
Loss in iteration 201 : 0.6263903006313983
Loss in iteration 202 : 0.6690326004284873
Loss in iteration 203 : 0.641941973658739
Loss in iteration 204 : 0.6452642674409683
Loss in iteration 205 : 0.6703177527482078
Loss in iteration 206 : 0.6875958201771973
Loss in iteration 207 : 0.6416009253170158
Loss in iteration 208 : 0.6415942003295293
Loss in iteration 209 : 0.6419179135351037
Loss in iteration 210 : 0.6343305502328563
Loss in iteration 211 : 0.6158248357846342
Loss in iteration 212 : 0.6156540176047405
Loss in iteration 213 : 0.5927438514673841
Loss in iteration 214 : 0.6059316888594232
Loss in iteration 215 : 0.609168589318241
Loss in iteration 216 : 0.6182967196165164
Loss in iteration 217 : 0.6207570663528412
Loss in iteration 218 : 0.5862928306592052
Loss in iteration 219 : 0.612339994711129
Loss in iteration 220 : 0.6247663893790558
Loss in iteration 221 : 0.6236113626093219
Loss in iteration 222 : 0.5826144118577996
Loss in iteration 223 : 0.6109523046219569
Loss in iteration 224 : 0.6539752138603878
Loss in iteration 225 : 0.6854081757755559
Loss in iteration 226 : 0.6626847782946119
Loss in iteration 227 : 0.6350489113252199
Loss in iteration 228 : 0.6489046102529439
Loss in iteration 229 : 0.650328066982013
Loss in iteration 230 : 0.6484293642491638
Loss in iteration 231 : 0.6793674543934833
Loss in iteration 232 : 0.66110918507264
Loss in iteration 233 : 0.6701329997765184
Loss in iteration 234 : 0.7068680843349703
Loss in iteration 235 : 0.6534942425432283
Loss in iteration 236 : 0.6333698568071914
Loss in iteration 237 : 0.6075414135889728
Loss in iteration 238 : 0.5807860089989348
Loss in iteration 239 : 0.5828296524816451
Loss in iteration 240 : 0.5523478766860354
Loss in iteration 241 : 0.5889403516875573
Loss in iteration 242 : 0.6435621078962384
Loss in iteration 243 : 0.6542497694672609
Loss in iteration 244 : 0.6576372201506759
Loss in iteration 245 : 0.710289593289199
Loss in iteration 246 : 0.6905085495938141
Loss in iteration 247 : 0.6496974264076751
Loss in iteration 248 : 0.6760273788891334
Loss in iteration 249 : 0.6594814512102631
Loss in iteration 250 : 0.6529190346155209
Loss in iteration 251 : 0.6665597834375855
Loss in iteration 252 : 0.6578439451654988
Loss in iteration 253 : 0.6681273106069296
Loss in iteration 254 : 0.6261812335582295
Loss in iteration 255 : 0.637358684894027
Loss in iteration 256 : 0.6805125718095035
Loss in iteration 257 : 0.6338817964811719
Loss in iteration 258 : 0.630436956321011
Loss in iteration 259 : 0.6234264243391026
Loss in iteration 260 : 0.6270810890639177
Loss in iteration 261 : 0.6326115448800089
Loss in iteration 262 : 0.6772487541632349
Loss in iteration 263 : 0.6652862996208776
Loss in iteration 264 : 0.6831977343007589
Loss in iteration 265 : 0.641155071356311
Loss in iteration 266 : 0.6051731040469861
Loss in iteration 267 : 0.5642967462699179
Loss in iteration 268 : 0.5788026006142588
Loss in iteration 269 : 0.5721771943603007
Loss in iteration 270 : 0.6024956746971206
Loss in iteration 271 : 0.6275601288569524
Loss in iteration 272 : 0.6858064822018607
Loss in iteration 273 : 0.6872231603290307
Loss in iteration 274 : 0.6845311301538194
Loss in iteration 275 : 0.6770934430980214
Loss in iteration 276 : 0.6757544792948291
Loss in iteration 277 : 0.6706856086896785
Loss in iteration 278 : 0.6634250467769187
Loss in iteration 279 : 0.6547356694927543
Loss in iteration 280 : 0.6663154563605559
Loss in iteration 281 : 0.632003445882032
Loss in iteration 282 : 0.6021476393949275
Loss in iteration 283 : 0.611320896697671
Loss in iteration 284 : 0.6032343809913423
Loss in iteration 285 : 0.6423829079371962
Loss in iteration 286 : 0.6484521926916692
Loss in iteration 287 : 0.6344630416036141
Loss in iteration 288 : 0.6148257493690223
Loss in iteration 289 : 0.5706036484665701
Loss in iteration 290 : 0.5451335410524707
Loss in iteration 291 : 0.5587401084922154
Loss in iteration 292 : 0.5750519489889853
Loss in iteration 293 : 0.6170859353221776
Loss in iteration 294 : 0.6347784567448589
Loss in iteration 295 : 0.632192589892357
Loss in iteration 296 : 0.6701959498376352
Loss in iteration 297 : 0.7187397136748042
Loss in iteration 298 : 0.7192061365896193
Loss in iteration 299 : 0.6970028340980096
Loss in iteration 300 : 0.7231502183661235
Loss in iteration 301 : 0.71812667629952
Loss in iteration 302 : 0.6622261379534021
Loss in iteration 303 : 0.6191293652190112
Loss in iteration 304 : 0.635123056295089
Loss in iteration 305 : 0.6217954750251912
Loss in iteration 306 : 0.5753649598207561
Loss in iteration 307 : 0.5783483847250466
Loss in iteration 308 : 0.5993128274575817
Loss in iteration 309 : 0.612557653121673
Loss in iteration 310 : 0.6268314137810674
Loss in iteration 311 : 0.6433264715919647
Loss in iteration 312 : 0.6610355869963815
Loss in iteration 313 : 0.6225847165633377
Loss in iteration 314 : 0.6122223244004121
Loss in iteration 315 : 0.6127790623485013
Loss in iteration 316 : 0.6345571650765877
Loss in iteration 317 : 0.6176110735005614
Loss in iteration 318 : 0.6642076720324583
Loss in iteration 319 : 0.649560725663672
Loss in iteration 320 : 0.6695955660875135
Loss in iteration 321 : 0.7052955080722646
Loss in iteration 322 : 0.7133022686261522
Loss in iteration 323 : 0.6651743810859033
Loss in iteration 324 : 0.6540465131482184
Loss in iteration 325 : 0.6265910362702207
Loss in iteration 326 : 0.6523304713808916
Loss in iteration 327 : 0.6065044750823794
Loss in iteration 328 : 0.5837766994607855
Loss in iteration 329 : 0.5901238164256503
Loss in iteration 330 : 0.6110067609853126
Loss in iteration 331 : 0.6215049447779587
Loss in iteration 332 : 0.5858142071622823
Loss in iteration 333 : 0.5906262193956284
Loss in iteration 334 : 0.6131572390212882
Loss in iteration 335 : 0.6282112660553975
Loss in iteration 336 : 0.6203444603912365
Loss in iteration 337 : 0.6511124399811231
Loss in iteration 338 : 0.6771442767062384
Loss in iteration 339 : 0.6701940149868146
Loss in iteration 340 : 0.6188583851496866
Loss in iteration 341 : 0.5939019121078655
Loss in iteration 342 : 0.5880467818195672
Loss in iteration 343 : 0.6114088626924461
Loss in iteration 344 : 0.6150935077717546
Loss in iteration 345 : 0.6243606570936695
Loss in iteration 346 : 0.6322969792389921
Loss in iteration 347 : 0.634869152136676
Loss in iteration 348 : 0.650017955505394
Loss in iteration 349 : 0.6830086041397513
Loss in iteration 350 : 0.7247260219772125
Loss in iteration 351 : 0.6962142819949185
Loss in iteration 352 : 0.6708691953776412
Loss in iteration 353 : 0.6062870021744322
Loss in iteration 354 : 0.6185507636399254
Loss in iteration 355 : 0.6236157026398402
Loss in iteration 356 : 0.6228436206957016
Loss in iteration 357 : 0.6316435893778612
Loss in iteration 358 : 0.6303991387612117
Loss in iteration 359 : 0.5909750068985116
Loss in iteration 360 : 0.6333106526589719
Loss in iteration 361 : 0.6442686463612504
Loss in iteration 362 : 0.6141928531446406
Loss in iteration 363 : 0.6142628943955383
Loss in iteration 364 : 0.6178736795373985
Loss in iteration 365 : 0.6563891253487089
Loss in iteration 366 : 0.6492303943434857
Loss in iteration 367 : 0.655437386748104
Loss in iteration 368 : 0.6291284505998225
Loss in iteration 369 : 0.6262398763993355
Loss in iteration 370 : 0.6480773180306386
Loss in iteration 371 : 0.698813659105882
Loss in iteration 372 : 0.6693276656888326
Loss in iteration 373 : 0.6906253566591536
Loss in iteration 374 : 0.7357857407388515
Loss in iteration 375 : 0.7005045861467681
Loss in iteration 376 : 0.7047570739435696
Loss in iteration 377 : 0.6643260997524438
Loss in iteration 378 : 0.6377214607141368
Loss in iteration 379 : 0.6081287787854692
Loss in iteration 380 : 0.6079687039342264
Loss in iteration 381 : 0.605009195154854
Loss in iteration 382 : 0.6020966501936215
Loss in iteration 383 : 0.6347041671064261
Loss in iteration 384 : 0.6167586813556021
Loss in iteration 385 : 0.5936093750720264
Loss in iteration 386 : 0.5691759242106395
Loss in iteration 387 : 0.6128563004690493
Loss in iteration 388 : 0.6198541860899354
Loss in iteration 389 : 0.5958175937143939
Loss in iteration 390 : 0.568980701607189
Loss in iteration 391 : 0.5919349003472929
Loss in iteration 392 : 0.6114076003780152
Loss in iteration 393 : 0.6388520502006774
Loss in iteration 394 : 0.6429553202117219
Loss in iteration 395 : 0.6644601924380064
Loss in iteration 396 : 0.6738390886562606
Loss in iteration 397 : 0.6769886938766806
Loss in iteration 398 : 0.6659169220215637
Loss in iteration 399 : 0.6330323306436639
Loss in iteration 400 : 0.6105469552699733
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.747625, training accuracy 0.747625, time elapsed: 5795 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6658318251701436
Loss in iteration 3 : 0.7350862092198318
Loss in iteration 4 : 1.1739768467832596
Loss in iteration 5 : 1.118663746006835
Loss in iteration 6 : 1.4159866382149258
Loss in iteration 7 : 0.7108288872326853
Loss in iteration 8 : 0.7104219968070536
Loss in iteration 9 : 0.6916460549286816
Loss in iteration 10 : 0.6099488998191098
Loss in iteration 11 : 0.5665976624343089
Loss in iteration 12 : 0.5202641385796858
Loss in iteration 13 : 0.4978527957561146
Loss in iteration 14 : 0.4862676492702378
Loss in iteration 15 : 0.5067370311708511
Loss in iteration 16 : 0.5038451313708598
Loss in iteration 17 : 0.49171394238979005
Loss in iteration 18 : 0.49298518844829764
Loss in iteration 19 : 0.4987827150867071
Loss in iteration 20 : 0.48785911981725955
Loss in iteration 21 : 0.5127206617966246
Loss in iteration 22 : 0.5035735159855164
Loss in iteration 23 : 0.5058641985266009
Loss in iteration 24 : 0.4968047551108177
Loss in iteration 25 : 0.4955333458663977
Loss in iteration 26 : 0.4794022035067462
Loss in iteration 27 : 0.48571720984351024
Loss in iteration 28 : 0.49105034454127994
Loss in iteration 29 : 0.4867192977541568
Loss in iteration 30 : 0.48906917771217917
Loss in iteration 31 : 0.48151915836868714
Loss in iteration 32 : 0.4780176102886738
Loss in iteration 33 : 0.46806308406785274
Loss in iteration 34 : 0.4645673576414634
Loss in iteration 35 : 0.4780516550079671
Loss in iteration 36 : 0.4772585856058781
Loss in iteration 37 : 0.4684996497147075
Loss in iteration 38 : 0.47065598883417475
Loss in iteration 39 : 0.4613333102718656
Loss in iteration 40 : 0.4729945008177992
Loss in iteration 41 : 0.4764238094077792
Loss in iteration 42 : 0.459120936688945
Loss in iteration 43 : 0.4655983177827708
Loss in iteration 44 : 0.46687407692862554
Loss in iteration 45 : 0.4714299127000857
Loss in iteration 46 : 0.4726426340714646
Loss in iteration 47 : 0.4633205007342833
Loss in iteration 48 : 0.46999450091745404
Loss in iteration 49 : 0.4688339992488191
Loss in iteration 50 : 0.45899239581511003
Loss in iteration 51 : 0.4767788824700165
Loss in iteration 52 : 0.4681450976225389
Loss in iteration 53 : 0.47019373429694367
Loss in iteration 54 : 0.46707348156642403
Loss in iteration 55 : 0.47351449765202025
Loss in iteration 56 : 0.48235227831496263
Loss in iteration 57 : 0.47056190977395135
Loss in iteration 58 : 0.48042405520873294
Loss in iteration 59 : 0.4819661777548189
Loss in iteration 60 : 0.48650295429793616
Loss in iteration 61 : 0.47973659194443136
Loss in iteration 62 : 0.48046939735744504
Loss in iteration 63 : 0.4837846972836188
Loss in iteration 64 : 0.4855704860759108
Loss in iteration 65 : 0.4983474239901557
Loss in iteration 66 : 0.4777839656393868
Loss in iteration 67 : 0.487404689770815
Loss in iteration 68 : 0.4728862929319539
Loss in iteration 69 : 0.4746548884327017
Loss in iteration 70 : 0.4597013060729541
Loss in iteration 71 : 0.4768993585322429
Loss in iteration 72 : 0.4623805706737842
Loss in iteration 73 : 0.4711577271911941
Loss in iteration 74 : 0.458376767705525
Loss in iteration 75 : 0.46011239808272897
Loss in iteration 76 : 0.47370273674324337
Loss in iteration 77 : 0.45311190533243223
Loss in iteration 78 : 0.4654211712691158
Loss in iteration 79 : 0.4454861792197094
Loss in iteration 80 : 0.4706019154471235
Loss in iteration 81 : 0.47655569842453793
Loss in iteration 82 : 0.48139115102685487
Loss in iteration 83 : 0.48207702631874993
Loss in iteration 84 : 0.4984425227443421
Loss in iteration 85 : 0.4937674669233399
Loss in iteration 86 : 0.5146366119664618
Loss in iteration 87 : 0.49337816026495185
Loss in iteration 88 : 0.5130474102530773
Loss in iteration 89 : 0.49230476443556853
Loss in iteration 90 : 0.49620902780819254
Loss in iteration 91 : 0.4966345990252182
Loss in iteration 92 : 0.4900253729366487
Loss in iteration 93 : 0.5082806302723204
Loss in iteration 94 : 0.5122779970947767
Loss in iteration 95 : 0.5150225723747324
Loss in iteration 96 : 0.5143731700357154
Loss in iteration 97 : 0.4999199097317011
Loss in iteration 98 : 0.49971586996426964
Loss in iteration 99 : 0.4908255536465778
Loss in iteration 100 : 0.49857119649867615
Loss in iteration 101 : 0.49701863175731636
Loss in iteration 102 : 0.4933612701960733
Loss in iteration 103 : 0.4945467154584902
Loss in iteration 104 : 0.48506882497244264
Loss in iteration 105 : 0.47086107636821944
Loss in iteration 106 : 0.49504500156057285
Loss in iteration 107 : 0.5058510903492025
Loss in iteration 108 : 0.49742653601737186
Loss in iteration 109 : 0.47061526552030236
Loss in iteration 110 : 0.4619555556873039
Loss in iteration 111 : 0.47125090529190394
Loss in iteration 112 : 0.4743143171629498
Loss in iteration 113 : 0.48831053955340376
Loss in iteration 114 : 0.4878298994351438
Loss in iteration 115 : 0.47498412216667596
Loss in iteration 116 : 0.4836510908488061
Loss in iteration 117 : 0.4896473464441054
Loss in iteration 118 : 0.4892933537540636
Loss in iteration 119 : 0.4815889806525932
Loss in iteration 120 : 0.4995633511796681
Loss in iteration 121 : 0.49428624476771776
Loss in iteration 122 : 0.5126954223960388
Loss in iteration 123 : 0.5246800424930541
Loss in iteration 124 : 0.5311142142641647
Loss in iteration 125 : 0.5410206056023253
Loss in iteration 126 : 0.5447122105248182
Loss in iteration 127 : 0.5232671457439413
Loss in iteration 128 : 0.5220300915146433
Loss in iteration 129 : 0.5122582534921344
Loss in iteration 130 : 0.5135699478140459
Loss in iteration 131 : 0.5088112623310453
Loss in iteration 132 : 0.5148783625804287
Loss in iteration 133 : 0.5219069126847643
Loss in iteration 134 : 0.5076069946464754
Loss in iteration 135 : 0.4968114474353811
Loss in iteration 136 : 0.4951614138283323
Loss in iteration 137 : 0.5061796906263162
Loss in iteration 138 : 0.4948303539461635
Loss in iteration 139 : 0.49877125441027315
Loss in iteration 140 : 0.499360821868148
Loss in iteration 141 : 0.5091727591589641
Loss in iteration 142 : 0.4997164858978308
Loss in iteration 143 : 0.49973597750302734
Loss in iteration 144 : 0.4810665744218743
Loss in iteration 145 : 0.4839346644283173
Loss in iteration 146 : 0.521270554520943
Loss in iteration 147 : 0.5507659840363811
Loss in iteration 148 : 0.5382582215756481
Loss in iteration 149 : 0.5545311360857534
Loss in iteration 150 : 0.5501823345969005
Loss in iteration 151 : 0.5427177436275362
Loss in iteration 152 : 0.5402612404664396
Loss in iteration 153 : 0.5143891401089364
Loss in iteration 154 : 0.5284965372877011
Loss in iteration 155 : 0.5188468303013405
Loss in iteration 156 : 0.4971552295691787
Loss in iteration 157 : 0.5025706012606871
Loss in iteration 158 : 0.4837010485649409
Loss in iteration 159 : 0.48392212096869314
Loss in iteration 160 : 0.47707888192324294
Loss in iteration 161 : 0.47263416614440407
Loss in iteration 162 : 0.4830065081474185
Loss in iteration 163 : 0.4978611162573758
Loss in iteration 164 : 0.4984147210477114
Loss in iteration 165 : 0.487554368192387
Loss in iteration 166 : 0.49210542629145404
Loss in iteration 167 : 0.5154297964698734
Loss in iteration 168 : 0.5378709462533193
Loss in iteration 169 : 0.5457148726503376
Loss in iteration 170 : 0.5669883635409549
Loss in iteration 171 : 0.5652039576679011
Loss in iteration 172 : 0.5784356249758142
Loss in iteration 173 : 0.5688318043997505
Loss in iteration 174 : 0.5425776894036511
Loss in iteration 175 : 0.5470238129482167
Loss in iteration 176 : 0.5209239140225448
Loss in iteration 177 : 0.5351022675833089
Loss in iteration 178 : 0.5060363851479878
Loss in iteration 179 : 0.4995732243085976
Loss in iteration 180 : 0.4869257167146964
Loss in iteration 181 : 0.49653790441628626
Loss in iteration 182 : 0.48365289463083905
Loss in iteration 183 : 0.47954947462024516
Loss in iteration 184 : 0.510044844559135
Loss in iteration 185 : 0.49789179202998074
Loss in iteration 186 : 0.4818242843408558
Loss in iteration 187 : 0.49786950142326186
Loss in iteration 188 : 0.5309056611287922
Loss in iteration 189 : 0.5364876502571001
Loss in iteration 190 : 0.5196780051584411
Loss in iteration 191 : 0.5188426225346994
Loss in iteration 192 : 0.5130325710691065
Loss in iteration 193 : 0.5084183696085316
Loss in iteration 194 : 0.490252717708937
Loss in iteration 195 : 0.4976150170315046
Loss in iteration 196 : 0.5107516051892118
Loss in iteration 197 : 0.48345305220320606
Loss in iteration 198 : 0.5151627921490369
Loss in iteration 199 : 0.5253493260402343
Loss in iteration 200 : 0.5199335127497192
Loss in iteration 201 : 0.5291184428179594
Loss in iteration 202 : 0.5405428837967136
Loss in iteration 203 : 0.5112143750640519
Loss in iteration 204 : 0.5120707321346107
Loss in iteration 205 : 0.4961362833210111
Loss in iteration 206 : 0.4925946277453812
Loss in iteration 207 : 0.478244273436295
Loss in iteration 208 : 0.46571274491052234
Loss in iteration 209 : 0.4831958232670958
Loss in iteration 210 : 0.49845509203821814
Loss in iteration 211 : 0.49672044980576446
Loss in iteration 212 : 0.5155722079758419
Loss in iteration 213 : 0.5382788688796478
Loss in iteration 214 : 0.5766239896956568
Loss in iteration 215 : 0.5815397963434831
Loss in iteration 216 : 0.5597441402899834
Loss in iteration 217 : 0.5702260889590549
Loss in iteration 218 : 0.580788742702025
Loss in iteration 219 : 0.6155128726248853
Loss in iteration 220 : 0.5936393196896343
Loss in iteration 221 : 0.586825317561768
Loss in iteration 222 : 0.5796525468737089
Loss in iteration 223 : 0.5731843650038914
Loss in iteration 224 : 0.5383496537434854
Loss in iteration 225 : 0.5129351707375521
Loss in iteration 226 : 0.527493171644761
Loss in iteration 227 : 0.5315879715765565
Loss in iteration 228 : 0.5390508187320425
Loss in iteration 229 : 0.527415554672862
Loss in iteration 230 : 0.5087053864239157
Loss in iteration 231 : 0.513012520941193
Loss in iteration 232 : 0.5177297067321879
Loss in iteration 233 : 0.48403329664489864
Loss in iteration 234 : 0.4900521678507315
Loss in iteration 235 : 0.49002078290535284
Loss in iteration 236 : 0.5157052557580966
Loss in iteration 237 : 0.5167642564074729
Loss in iteration 238 : 0.5254940476900283
Loss in iteration 239 : 0.5619488821209898
Loss in iteration 240 : 0.5483872984032524
Loss in iteration 241 : 0.5296702166107804
Loss in iteration 242 : 0.5534677351603103
Loss in iteration 243 : 0.5421289494700163
Loss in iteration 244 : 0.520703802317903
Loss in iteration 245 : 0.5317704431324999
Loss in iteration 246 : 0.5093301299800702
Loss in iteration 247 : 0.5099463014076218
Loss in iteration 248 : 0.52698405505837
Loss in iteration 249 : 0.5321613979596576
Loss in iteration 250 : 0.5255402194973703
Loss in iteration 251 : 0.5455400282159351
Loss in iteration 252 : 0.5437418914390515
Loss in iteration 253 : 0.5663970977827896
Loss in iteration 254 : 0.5296573629250367
Loss in iteration 255 : 0.5333195392081588
Loss in iteration 256 : 0.5611030520765875
Loss in iteration 257 : 0.537674918815598
Loss in iteration 258 : 0.5402954104058806
Loss in iteration 259 : 0.5372179312341385
Loss in iteration 260 : 0.5406394658590075
Loss in iteration 261 : 0.533509968945871
Loss in iteration 262 : 0.5526798281161324
Loss in iteration 263 : 0.54279357349184
Loss in iteration 264 : 0.547740243247491
Loss in iteration 265 : 0.5226443897923848
Loss in iteration 266 : 0.5494724189230653
Loss in iteration 267 : 0.5545190149928929
Loss in iteration 268 : 0.5571457256777474
Loss in iteration 269 : 0.5329657594543058
Loss in iteration 270 : 0.5262864369841741
Loss in iteration 271 : 0.5014863020930942
Loss in iteration 272 : 0.5093979302835112
Loss in iteration 273 : 0.4914016958129642
Loss in iteration 274 : 0.5006938989538728
Loss in iteration 275 : 0.5104117609170297
Loss in iteration 276 : 0.50765218211198
Loss in iteration 277 : 0.4740339135962222
Loss in iteration 278 : 0.46623633306728207
Loss in iteration 279 : 0.4611553280142042
Loss in iteration 280 : 0.47121975766748514
Loss in iteration 281 : 0.48326298559392183
Loss in iteration 282 : 0.4807653220168436
Loss in iteration 283 : 0.48438838497092346
Loss in iteration 284 : 0.4676101101062929
Loss in iteration 285 : 0.46856685003920007
Loss in iteration 286 : 0.48259421177397477
Loss in iteration 287 : 0.49460111434669835
Loss in iteration 288 : 0.5008561941719003
Loss in iteration 289 : 0.5169109271306337
Loss in iteration 290 : 0.5591843371828585
Loss in iteration 291 : 0.6056226775967182
Loss in iteration 292 : 0.592936975297095
Loss in iteration 293 : 0.592992685528113
Loss in iteration 294 : 0.5875575662455914
Loss in iteration 295 : 0.5915867232666814
Loss in iteration 296 : 0.577955188477289
Loss in iteration 297 : 0.5613212304373263
Loss in iteration 298 : 0.5505066950431567
Loss in iteration 299 : 0.5558984442466429
Loss in iteration 300 : 0.5384897769520799
Loss in iteration 301 : 0.5261664380079341
Loss in iteration 302 : 0.5091965502139852
Loss in iteration 303 : 0.4977314066969347
Loss in iteration 304 : 0.5019674881413526
Loss in iteration 305 : 0.5208406521038929
Loss in iteration 306 : 0.5362612230119227
Loss in iteration 307 : 0.548327137021893
Loss in iteration 308 : 0.5296776773195612
Loss in iteration 309 : 0.5037420894802948
Loss in iteration 310 : 0.504637713004783
Loss in iteration 311 : 0.5161488751059108
Loss in iteration 312 : 0.5268850843989146
Loss in iteration 313 : 0.49042098510923526
Loss in iteration 314 : 0.4987672555635937
Loss in iteration 315 : 0.5060145127267965
Loss in iteration 316 : 0.5412145268407342
Loss in iteration 317 : 0.5307355535289032
Loss in iteration 318 : 0.537096000353584
Loss in iteration 319 : 0.5198671176891775
Loss in iteration 320 : 0.5156928247219876
Loss in iteration 321 : 0.5053421356358639
Loss in iteration 322 : 0.5123194947156059
Loss in iteration 323 : 0.5038295906921918
Loss in iteration 324 : 0.5114259951043347
Loss in iteration 325 : 0.5303635016423941
Loss in iteration 326 : 0.5523604335852251
Loss in iteration 327 : 0.5358740991624213
Loss in iteration 328 : 0.5518885934620292
Loss in iteration 329 : 0.5559413777544923
Loss in iteration 330 : 0.5390593128055772
Loss in iteration 331 : 0.5502709322238717
Loss in iteration 332 : 0.540294430032337
Loss in iteration 333 : 0.543261596887009
Loss in iteration 334 : 0.5538362524293049
Loss in iteration 335 : 0.5661259645523237
Loss in iteration 336 : 0.5617111206883627
Loss in iteration 337 : 0.5256925310752434
Loss in iteration 338 : 0.5169556577883346
Loss in iteration 339 : 0.5269826740396193
Loss in iteration 340 : 0.5320266862653017
Loss in iteration 341 : 0.5374418871520913
Loss in iteration 342 : 0.5358806852640435
Loss in iteration 343 : 0.5389645967522205
Loss in iteration 344 : 0.5411463350550241
Loss in iteration 345 : 0.5383823562199188
Loss in iteration 346 : 0.5374229474244824
Loss in iteration 347 : 0.5408797716013917
Loss in iteration 348 : 0.539136075771861
Loss in iteration 349 : 0.5216552079613914
Loss in iteration 350 : 0.5184892600396132
Loss in iteration 351 : 0.5050621167276843
Loss in iteration 352 : 0.5093066999297217
Loss in iteration 353 : 0.5080231591460946
Loss in iteration 354 : 0.5245930216773909
Loss in iteration 355 : 0.5328151744752428
Loss in iteration 356 : 0.5251161677146886
Loss in iteration 357 : 0.5335747649549228
Loss in iteration 358 : 0.5634066023333639
Loss in iteration 359 : 0.5874324686444456
Loss in iteration 360 : 0.594925619256388
Loss in iteration 361 : 0.5756428799541955
Loss in iteration 362 : 0.5828780939743411
Loss in iteration 363 : 0.5416871464184856
Loss in iteration 364 : 0.5370430310808186
Loss in iteration 365 : 0.5610841703918191
Loss in iteration 366 : 0.5547902570122636
Loss in iteration 367 : 0.5571121706046254
Loss in iteration 368 : 0.5745731911989839
Loss in iteration 369 : 0.6019203341296402
Loss in iteration 370 : 0.5595086772748326
Loss in iteration 371 : 0.5647459894251537
Loss in iteration 372 : 0.5412252281887894
Loss in iteration 373 : 0.5408415203686253
Loss in iteration 374 : 0.5295438872274548
Loss in iteration 375 : 0.512180844701106
Loss in iteration 376 : 0.5291824774808727
Loss in iteration 377 : 0.5336091370226148
Loss in iteration 378 : 0.5437521797258774
Loss in iteration 379 : 0.5398962699708996
Loss in iteration 380 : 0.5260374213645775
Loss in iteration 381 : 0.5206224155752863
Loss in iteration 382 : 0.533670453186415
Loss in iteration 383 : 0.5354731052257359
Loss in iteration 384 : 0.5289180687770298
Loss in iteration 385 : 0.5230011142754808
Loss in iteration 386 : 0.5215114995454091
Loss in iteration 387 : 0.5415491207630303
Loss in iteration 388 : 0.5573699601863693
Loss in iteration 389 : 0.5656834046819554
Loss in iteration 390 : 0.5707689007954937
Loss in iteration 391 : 0.5842798483321058
Loss in iteration 392 : 0.5624552030166804
Loss in iteration 393 : 0.5614386585690014
Loss in iteration 394 : 0.545635664946518
Loss in iteration 395 : 0.5483068746190809
Loss in iteration 396 : 0.5324116099359267
Loss in iteration 397 : 0.5551278752062861
Loss in iteration 398 : 0.5591292605727841
Loss in iteration 399 : 0.554536875601426
Loss in iteration 400 : 0.5468990881391018
Testing accuracy  of updater 8 on alg 0 with rate 1.4000000000000001 = 0.735125, training accuracy 0.735125, time elapsed: 5831 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.75411408673773
Loss in iteration 3 : 0.7976078879202937
Loss in iteration 4 : 0.9732632845005431
Loss in iteration 5 : 0.7129677244541462
Loss in iteration 6 : 0.708962774225328
Loss in iteration 7 : 0.6350241253753857
Loss in iteration 8 : 0.5895796338605582
Loss in iteration 9 : 0.5321046273130464
Loss in iteration 10 : 0.5009149417416215
Loss in iteration 11 : 0.49149857598567115
Loss in iteration 12 : 0.4814176791258775
Loss in iteration 13 : 0.47839104829703544
Loss in iteration 14 : 0.47482940818883335
Loss in iteration 15 : 0.48873226757062666
Loss in iteration 16 : 0.48383763759284115
Loss in iteration 17 : 0.4749902820727055
Loss in iteration 18 : 0.4756912724786865
Loss in iteration 19 : 0.48040593147558464
Loss in iteration 20 : 0.47206679371352195
Loss in iteration 21 : 0.49196726551309594
Loss in iteration 22 : 0.486303247264556
Loss in iteration 23 : 0.4871764022213698
Loss in iteration 24 : 0.47881005691904877
Loss in iteration 25 : 0.48220884729682084
Loss in iteration 26 : 0.46769300161290395
Loss in iteration 27 : 0.47278853944568716
Loss in iteration 28 : 0.47926423209832425
Loss in iteration 29 : 0.48101528351055517
Loss in iteration 30 : 0.48305791547261806
Loss in iteration 31 : 0.47560127344621883
Loss in iteration 32 : 0.47330568781929727
Loss in iteration 33 : 0.46565185681287313
Loss in iteration 34 : 0.4647240588615969
Loss in iteration 35 : 0.47708468171912277
Loss in iteration 36 : 0.4763677282842361
Loss in iteration 37 : 0.46823113447954445
Loss in iteration 38 : 0.4692170330940687
Loss in iteration 39 : 0.4602535828111163
Loss in iteration 40 : 0.4694081788442696
Loss in iteration 41 : 0.4734645087252778
Loss in iteration 42 : 0.45847957093667857
Loss in iteration 43 : 0.46617590095596273
Loss in iteration 44 : 0.46691047495998134
Loss in iteration 45 : 0.4698250557467968
Loss in iteration 46 : 0.4699799503617508
Loss in iteration 47 : 0.46231366934839985
Loss in iteration 48 : 0.4676334416313326
Loss in iteration 49 : 0.46405905754202315
Loss in iteration 50 : 0.45367997224308226
Loss in iteration 51 : 0.4704534263982588
Loss in iteration 52 : 0.46043539422927854
Loss in iteration 53 : 0.46227608686098276
Loss in iteration 54 : 0.45828053756233234
Loss in iteration 55 : 0.46307951783584944
Loss in iteration 56 : 0.4687469051928613
Loss in iteration 57 : 0.45166403272455535
Loss in iteration 58 : 0.4559987843388873
Loss in iteration 59 : 0.45348345584466804
Loss in iteration 60 : 0.46358859561240967
Loss in iteration 61 : 0.46063659013736813
Loss in iteration 62 : 0.4627337641811077
Loss in iteration 63 : 0.4625157762936517
Loss in iteration 64 : 0.4601668500771465
Loss in iteration 65 : 0.4719544879633709
Loss in iteration 66 : 0.45599531290926154
Loss in iteration 67 : 0.46710260397834674
Loss in iteration 68 : 0.458279152333119
Loss in iteration 69 : 0.4653933378300816
Loss in iteration 70 : 0.45617579136340614
Loss in iteration 71 : 0.47106455987904
Loss in iteration 72 : 0.45553784286284527
Loss in iteration 73 : 0.46739438223343793
Loss in iteration 74 : 0.4578742606070523
Loss in iteration 75 : 0.4599903286724078
Loss in iteration 76 : 0.4730444329619211
Loss in iteration 77 : 0.4546042363233714
Loss in iteration 78 : 0.46632333881868127
Loss in iteration 79 : 0.4443093417949101
Loss in iteration 80 : 0.4631458977526782
Loss in iteration 81 : 0.46414076833081847
Loss in iteration 82 : 0.462149040443677
Loss in iteration 83 : 0.4616582593055552
Loss in iteration 84 : 0.47483683107551955
Loss in iteration 85 : 0.4619090267053445
Loss in iteration 86 : 0.47243073166432425
Loss in iteration 87 : 0.4499650924614936
Loss in iteration 88 : 0.47152557426288905
Loss in iteration 89 : 0.45572084053362644
Loss in iteration 90 : 0.45995780521281265
Loss in iteration 91 : 0.46654565160802425
Loss in iteration 92 : 0.45488248954053995
Loss in iteration 93 : 0.46150435952940805
Loss in iteration 94 : 0.4594859627704478
Loss in iteration 95 : 0.45871004347535543
Loss in iteration 96 : 0.4673750332050823
Loss in iteration 97 : 0.4602246033764559
Loss in iteration 98 : 0.46635871884155444
Loss in iteration 99 : 0.459160157703856
Loss in iteration 100 : 0.46438794253204707
Loss in iteration 101 : 0.457771668700949
Loss in iteration 102 : 0.45338206370996725
Loss in iteration 103 : 0.464921583893109
Loss in iteration 104 : 0.4624009162474267
Loss in iteration 105 : 0.44727845431921914
Loss in iteration 106 : 0.46311001882842473
Loss in iteration 107 : 0.47668071575651594
Loss in iteration 108 : 0.47718072330721395
Loss in iteration 109 : 0.46098968383322364
Loss in iteration 110 : 0.4548643579969913
Loss in iteration 111 : 0.4637933190084353
Loss in iteration 112 : 0.4573199116055086
Loss in iteration 113 : 0.45490975156024943
Loss in iteration 114 : 0.45333589313513994
Loss in iteration 115 : 0.4535367271259407
Loss in iteration 116 : 0.4644845427847043
Loss in iteration 117 : 0.47122237474650386
Loss in iteration 118 : 0.4678598126803962
Loss in iteration 119 : 0.45412230761759875
Loss in iteration 120 : 0.4708275269303362
Loss in iteration 121 : 0.46606490091072494
Loss in iteration 122 : 0.47338769208290093
Loss in iteration 123 : 0.46563263396455024
Loss in iteration 124 : 0.4565487914434924
Loss in iteration 125 : 0.45494836132189753
Loss in iteration 126 : 0.459249136752085
Loss in iteration 127 : 0.45791248205397844
Loss in iteration 128 : 0.46361578910510454
Loss in iteration 129 : 0.4582221098868413
Loss in iteration 130 : 0.46480361778043877
Loss in iteration 131 : 0.4570413775992142
Loss in iteration 132 : 0.45626497704148233
Loss in iteration 133 : 0.46352241077353007
Loss in iteration 134 : 0.46153841036268256
Loss in iteration 135 : 0.46153217555737475
Loss in iteration 136 : 0.4593433705010587
Loss in iteration 137 : 0.4663186641747913
Loss in iteration 138 : 0.46325897832341134
Loss in iteration 139 : 0.47134946509584585
Loss in iteration 140 : 0.465406100192565
Loss in iteration 141 : 0.4675976017191123
Loss in iteration 142 : 0.4544591182302775
Loss in iteration 143 : 0.4626837172244228
Loss in iteration 144 : 0.45445152318286136
Loss in iteration 145 : 0.4482970297031743
Loss in iteration 146 : 0.46669547680031637
Loss in iteration 147 : 0.4725843209879062
Loss in iteration 148 : 0.45981211137409533
Loss in iteration 149 : 0.47418385416475084
Loss in iteration 150 : 0.46418816173319993
Loss in iteration 151 : 0.45973415382501204
Loss in iteration 152 : 0.47162047386213435
Loss in iteration 153 : 0.44860173634059936
Loss in iteration 154 : 0.46874217427761444
Loss in iteration 155 : 0.46395722935372774
Loss in iteration 156 : 0.44698599699728425
Loss in iteration 157 : 0.46362711210152524
Loss in iteration 158 : 0.4598047315580846
Loss in iteration 159 : 0.46138122907648593
Loss in iteration 160 : 0.45729308414664566
Loss in iteration 161 : 0.4549236814833298
Loss in iteration 162 : 0.4601565976559595
Loss in iteration 163 : 0.4659984607260664
Loss in iteration 164 : 0.4633853628647053
Loss in iteration 165 : 0.45753432776666486
Loss in iteration 166 : 0.4587729652610469
Loss in iteration 167 : 0.46810301738917554
Loss in iteration 168 : 0.4715062620464818
Loss in iteration 169 : 0.4573972018765782
Loss in iteration 170 : 0.4646386040905424
Loss in iteration 171 : 0.4545126288397465
Loss in iteration 172 : 0.4665456597847633
Loss in iteration 173 : 0.4562764369539579
Loss in iteration 174 : 0.4540143913406711
Loss in iteration 175 : 0.46858888715827374
Loss in iteration 176 : 0.4536714881766819
Loss in iteration 177 : 0.46761906702302963
Loss in iteration 178 : 0.4572387622801909
Loss in iteration 179 : 0.4659854988829597
Loss in iteration 180 : 0.45466751289248275
Loss in iteration 181 : 0.4659821006059404
Loss in iteration 182 : 0.46086807064581137
Loss in iteration 183 : 0.45818112800198996
Loss in iteration 184 : 0.4755352648580915
Loss in iteration 185 : 0.4584448355515963
Loss in iteration 186 : 0.4489868532870691
Loss in iteration 187 : 0.45509468017195637
Loss in iteration 188 : 0.4692616413083551
Loss in iteration 189 : 0.46455567167192724
Loss in iteration 190 : 0.44807275052828965
Loss in iteration 191 : 0.4540543954569831
Loss in iteration 192 : 0.4582850651220587
Loss in iteration 193 : 0.464637529648052
Loss in iteration 194 : 0.4615923117392732
Loss in iteration 195 : 0.4682485140128308
Loss in iteration 196 : 0.47449697275069014
Loss in iteration 197 : 0.45034209229191546
Loss in iteration 198 : 0.4696363059246634
Loss in iteration 199 : 0.4609806051800988
Loss in iteration 200 : 0.45077626528089354
Loss in iteration 201 : 0.4574735856947558
Loss in iteration 202 : 0.4697641485211771
Loss in iteration 203 : 0.4496898706140329
Loss in iteration 204 : 0.458641834483811
Loss in iteration 205 : 0.4618944858231535
Loss in iteration 206 : 0.4620269752756805
Loss in iteration 207 : 0.45545227092317575
Loss in iteration 208 : 0.45043545665264884
Loss in iteration 209 : 0.46473507411876047
Loss in iteration 210 : 0.47022371280009345
Loss in iteration 211 : 0.4614299581065849
Loss in iteration 212 : 0.4678356205193246
Loss in iteration 213 : 0.46942741835287116
Loss in iteration 214 : 0.48190195219687604
Loss in iteration 215 : 0.4707135532401776
Loss in iteration 216 : 0.45880678103652217
Loss in iteration 217 : 0.4567944365693502
Loss in iteration 218 : 0.45652943487224307
Loss in iteration 219 : 0.4720669407660512
Loss in iteration 220 : 0.4701567877427274
Loss in iteration 221 : 0.46311286525741097
Loss in iteration 222 : 0.4566004439746952
Loss in iteration 223 : 0.4669557933485331
Loss in iteration 224 : 0.46980588614274416
Loss in iteration 225 : 0.46018378393655945
Loss in iteration 226 : 0.46269086662387504
Loss in iteration 227 : 0.4533564106316334
Loss in iteration 228 : 0.4657771464245701
Loss in iteration 229 : 0.46340064820081484
Loss in iteration 230 : 0.45677675690409464
Loss in iteration 231 : 0.4630241730624676
Loss in iteration 232 : 0.4660570251999746
Loss in iteration 233 : 0.45782520896266865
Loss in iteration 234 : 0.4650959879019826
Loss in iteration 235 : 0.4565584224578502
Loss in iteration 236 : 0.4648810066827369
Loss in iteration 237 : 0.4630650724143499
Loss in iteration 238 : 0.46158067190343255
Loss in iteration 239 : 0.46522890944219203
Loss in iteration 240 : 0.45270111775191285
Loss in iteration 241 : 0.4562210947317946
Loss in iteration 242 : 0.48307525326211526
Loss in iteration 243 : 0.46873073700669765
Loss in iteration 244 : 0.4588944195354607
Loss in iteration 245 : 0.4739507406218229
Loss in iteration 246 : 0.4572518790531148
Loss in iteration 247 : 0.453109465353817
Loss in iteration 248 : 0.4625191928273182
Loss in iteration 249 : 0.46448432117929156
Loss in iteration 250 : 0.4543030330577371
Loss in iteration 251 : 0.46636071789333294
Loss in iteration 252 : 0.4620093641122875
Loss in iteration 253 : 0.47135003314873963
Loss in iteration 254 : 0.44734213408104984
Loss in iteration 255 : 0.4509443970315451
Loss in iteration 256 : 0.47418637652732437
Loss in iteration 257 : 0.44916429310042555
Loss in iteration 258 : 0.45660450378185263
Loss in iteration 259 : 0.4551082423017133
Loss in iteration 260 : 0.4585982373958537
Loss in iteration 261 : 0.4476339779104006
Loss in iteration 262 : 0.4605709733088462
Loss in iteration 263 : 0.45982631675794683
Loss in iteration 264 : 0.47367772218527493
Loss in iteration 265 : 0.4529190343180815
Loss in iteration 266 : 0.4702619668445741
Loss in iteration 267 : 0.46764251390958933
Loss in iteration 268 : 0.4727067414324823
Loss in iteration 269 : 0.45300550710449883
Loss in iteration 270 : 0.4587991248984247
Loss in iteration 271 : 0.44765887320892694
Loss in iteration 272 : 0.45927172699720265
Loss in iteration 273 : 0.4466471239824069
Loss in iteration 274 : 0.45557232400095177
Loss in iteration 275 : 0.4640890305545042
Loss in iteration 276 : 0.46790727602403115
Loss in iteration 277 : 0.4550551603737123
Loss in iteration 278 : 0.45106452027868243
Loss in iteration 279 : 0.44987234005097404
Loss in iteration 280 : 0.4572978513045807
Loss in iteration 281 : 0.4622685110289501
Loss in iteration 282 : 0.4568465573369656
Loss in iteration 283 : 0.4621887046248416
Loss in iteration 284 : 0.4478562775909995
Loss in iteration 285 : 0.45003058895892967
Loss in iteration 286 : 0.45968380027051536
Loss in iteration 287 : 0.46472570759800763
Loss in iteration 288 : 0.4633002908556779
Loss in iteration 289 : 0.45580980033597845
Loss in iteration 290 : 0.45426240857934075
Loss in iteration 291 : 0.4678637053817733
Loss in iteration 292 : 0.4612967775524815
Loss in iteration 293 : 0.45786849687946796
Loss in iteration 294 : 0.46485543336996005
Loss in iteration 295 : 0.45180772249401613
Loss in iteration 296 : 0.4551227163263164
Loss in iteration 297 : 0.4659935106680676
Loss in iteration 298 : 0.46009663957841046
Loss in iteration 299 : 0.46071095434197973
Loss in iteration 300 : 0.4658521571148289
Loss in iteration 301 : 0.46594969334401587
Loss in iteration 302 : 0.4577309009371931
Loss in iteration 303 : 0.4513556141141093
Loss in iteration 304 : 0.45631616544035875
Loss in iteration 305 : 0.4597323596626286
Loss in iteration 306 : 0.45861142078146744
Loss in iteration 307 : 0.4591867855494092
Loss in iteration 308 : 0.45962337084210514
Loss in iteration 309 : 0.45186827793610845
Loss in iteration 310 : 0.45888393275547984
Loss in iteration 311 : 0.46876662779879785
Loss in iteration 312 : 0.4799209934870794
Loss in iteration 313 : 0.4479838364552426
Loss in iteration 314 : 0.45635978894057166
Loss in iteration 315 : 0.4578264352968663
Loss in iteration 316 : 0.4778776584470951
Loss in iteration 317 : 0.4622949143720578
Loss in iteration 318 : 0.47094374367657954
Loss in iteration 319 : 0.45432443659320904
Loss in iteration 320 : 0.4536924832947229
Loss in iteration 321 : 0.45493851499855265
Loss in iteration 322 : 0.46351403729629775
Loss in iteration 323 : 0.456785054294741
Loss in iteration 324 : 0.4578501564854667
Loss in iteration 325 : 0.4569162133953746
Loss in iteration 326 : 0.4719558752042053
Loss in iteration 327 : 0.45086090709097176
Loss in iteration 328 : 0.45336212973219947
Loss in iteration 329 : 0.4551399945849039
Loss in iteration 330 : 0.45090937193506
Loss in iteration 331 : 0.46054992056503163
Loss in iteration 332 : 0.44581076444550755
Loss in iteration 333 : 0.44639978359736565
Loss in iteration 334 : 0.45773354413961775
Loss in iteration 335 : 0.45350606225820267
Loss in iteration 336 : 0.4598789250885797
Loss in iteration 337 : 0.46266747871281383
Loss in iteration 338 : 0.4701494334823298
Loss in iteration 339 : 0.4718350533331796
Loss in iteration 340 : 0.46495776196249716
Loss in iteration 341 : 0.45735886421192584
Loss in iteration 342 : 0.45807792664947444
Loss in iteration 343 : 0.4579803083175256
Loss in iteration 344 : 0.46369742668019487
Loss in iteration 345 : 0.4572226093173596
Loss in iteration 346 : 0.4527171246382995
Loss in iteration 347 : 0.4612494833875701
Loss in iteration 348 : 0.46356455444440586
Loss in iteration 349 : 0.46750360918472295
Loss in iteration 350 : 0.4678283108643051
Loss in iteration 351 : 0.4624126970670016
Loss in iteration 352 : 0.45774888936803093
Loss in iteration 353 : 0.4514829842517451
Loss in iteration 354 : 0.4595001905347214
Loss in iteration 355 : 0.4595374102371335
Loss in iteration 356 : 0.4528706673960741
Loss in iteration 357 : 0.45612890163279846
Loss in iteration 358 : 0.46621909417629703
Loss in iteration 359 : 0.45620227913808925
Loss in iteration 360 : 0.46737157719023825
Loss in iteration 361 : 0.45201461212377614
Loss in iteration 362 : 0.4710369994584714
Loss in iteration 363 : 0.46074254002502685
Loss in iteration 364 : 0.4586551379360256
Loss in iteration 365 : 0.46272684356518
Loss in iteration 366 : 0.45657464701407835
Loss in iteration 367 : 0.4659245960468298
Loss in iteration 368 : 0.4676424176238105
Loss in iteration 369 : 0.47343265036679477
Loss in iteration 370 : 0.45748255691133866
Loss in iteration 371 : 0.47402637638086365
Loss in iteration 372 : 0.4545201497643587
Loss in iteration 373 : 0.4603116170987583
Loss in iteration 374 : 0.4580113108122696
Loss in iteration 375 : 0.45715345421477394
Loss in iteration 376 : 0.465649921550814
Loss in iteration 377 : 0.4607821123147563
Loss in iteration 378 : 0.4632540704327389
Loss in iteration 379 : 0.4638491518353927
Loss in iteration 380 : 0.4573342183220724
Loss in iteration 381 : 0.4486017244893894
Loss in iteration 382 : 0.4562302027754385
Loss in iteration 383 : 0.46389765304022473
Loss in iteration 384 : 0.4699078160772798
Loss in iteration 385 : 0.4607445834832437
Loss in iteration 386 : 0.44781641269686767
Loss in iteration 387 : 0.46515927928169903
Loss in iteration 388 : 0.4702079608714642
Loss in iteration 389 : 0.4591055862192498
Loss in iteration 390 : 0.4595641284396101
Loss in iteration 391 : 0.46202881385045885
Loss in iteration 392 : 0.4518873367318043
Loss in iteration 393 : 0.4546602873129031
Loss in iteration 394 : 0.4575326462432693
Loss in iteration 395 : 0.4645995739695303
Loss in iteration 396 : 0.4578389489375362
Loss in iteration 397 : 0.47266207116685854
Loss in iteration 398 : 0.46879942615909387
Loss in iteration 399 : 0.45787175872489916
Loss in iteration 400 : 0.45334293251621033
Testing accuracy  of updater 8 on alg 0 with rate 0.8 = 0.7875, training accuracy 0.7875, time elapsed: 4926 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6790774825000224
Loss in iteration 3 : 0.6608145485408072
Loss in iteration 4 : 0.6511795565193073
Loss in iteration 5 : 0.6319291465214472
Loss in iteration 6 : 0.622506901487988
Loss in iteration 7 : 0.6071772054982969
Loss in iteration 8 : 0.5926884153133988
Loss in iteration 9 : 0.5807656848452256
Loss in iteration 10 : 0.5683870999549621
Loss in iteration 11 : 0.5546588464282411
Loss in iteration 12 : 0.5452223735162091
Loss in iteration 13 : 0.5354507367433917
Loss in iteration 14 : 0.526721890526421
Loss in iteration 15 : 0.5286752256148357
Loss in iteration 16 : 0.5193872636967711
Loss in iteration 17 : 0.511349220173815
Loss in iteration 18 : 0.5072977022735788
Loss in iteration 19 : 0.5079615448858134
Loss in iteration 20 : 0.4989759472603841
Loss in iteration 21 : 0.5083792192867211
Loss in iteration 22 : 0.504378368798201
Loss in iteration 23 : 0.501772731017237
Loss in iteration 24 : 0.4921310725837841
Loss in iteration 25 : 0.4963214685390899
Loss in iteration 26 : 0.48442244858465394
Loss in iteration 27 : 0.4858266016484823
Loss in iteration 28 : 0.48943355246128
Loss in iteration 29 : 0.49619103797188874
Loss in iteration 30 : 0.49382945496514064
Loss in iteration 31 : 0.48763323818578935
Loss in iteration 32 : 0.48666607896017305
Loss in iteration 33 : 0.4807967374107028
Loss in iteration 34 : 0.4806814570573143
Loss in iteration 35 : 0.4892089918562688
Loss in iteration 36 : 0.4879885932317033
Loss in iteration 37 : 0.4816580925531706
Loss in iteration 38 : 0.48036554950165394
Loss in iteration 39 : 0.47480728114583837
Loss in iteration 40 : 0.4805989135233059
Loss in iteration 41 : 0.48422195337691715
Loss in iteration 42 : 0.472445565211075
Loss in iteration 43 : 0.48104026699340346
Loss in iteration 44 : 0.4805681733554698
Loss in iteration 45 : 0.4805637057377359
Loss in iteration 46 : 0.4799717890410212
Loss in iteration 47 : 0.4759573749158752
Loss in iteration 48 : 0.48021381677817276
Loss in iteration 49 : 0.4735756959253564
Loss in iteration 50 : 0.46666133470900445
Loss in iteration 51 : 0.48308438317876073
Loss in iteration 52 : 0.470952864318781
Loss in iteration 53 : 0.470987882764435
Loss in iteration 54 : 0.4686285731118862
Loss in iteration 55 : 0.4721154152504177
Loss in iteration 56 : 0.4764886456453017
Loss in iteration 57 : 0.4604483344994994
Loss in iteration 58 : 0.4648414142370549
Loss in iteration 59 : 0.4641093222496629
Loss in iteration 60 : 0.47146640712579274
Loss in iteration 61 : 0.4696747150508798
Loss in iteration 62 : 0.4716597405748091
Loss in iteration 63 : 0.469490872907323
Loss in iteration 64 : 0.46865794393616195
Loss in iteration 65 : 0.4790411563003201
Loss in iteration 66 : 0.46259205632419836
Loss in iteration 67 : 0.4726852848534793
Loss in iteration 68 : 0.46364305803686734
Loss in iteration 69 : 0.47278498309542094
Loss in iteration 70 : 0.46339210144080795
Loss in iteration 71 : 0.4781204588901233
Loss in iteration 72 : 0.4636259822480041
Loss in iteration 73 : 0.47370144035745654
Loss in iteration 74 : 0.466135938649965
Loss in iteration 75 : 0.4673397068817994
Loss in iteration 76 : 0.47824016050722473
Loss in iteration 77 : 0.4616354316755694
Loss in iteration 78 : 0.4701834952106446
Loss in iteration 79 : 0.45209258640516803
Loss in iteration 80 : 0.4677314870275529
Loss in iteration 81 : 0.47013675737759597
Loss in iteration 82 : 0.4667408520029036
Loss in iteration 83 : 0.4686388625630967
Loss in iteration 84 : 0.4783061370968168
Loss in iteration 85 : 0.46832764140599725
Loss in iteration 86 : 0.47578103912735503
Loss in iteration 87 : 0.4570521929652743
Loss in iteration 88 : 0.47664982316825494
Loss in iteration 89 : 0.4600903284831783
Loss in iteration 90 : 0.46344524284843575
Loss in iteration 91 : 0.4724262879854841
Loss in iteration 92 : 0.46026277200719307
Loss in iteration 93 : 0.46542962729658843
Loss in iteration 94 : 0.46483293790527846
Loss in iteration 95 : 0.4623423749442609
Loss in iteration 96 : 0.47324385732506474
Loss in iteration 97 : 0.4642713735555379
Loss in iteration 98 : 0.469511458269022
Loss in iteration 99 : 0.463956673796695
Loss in iteration 100 : 0.4684709936772307
Loss in iteration 101 : 0.4622163871112621
Loss in iteration 102 : 0.45950869677426714
Loss in iteration 103 : 0.4706173425195724
Loss in iteration 104 : 0.4653698893613447
Loss in iteration 105 : 0.4512275256938561
Loss in iteration 106 : 0.46511027617942696
Loss in iteration 107 : 0.47724540748831595
Loss in iteration 108 : 0.47939505922388587
Loss in iteration 109 : 0.464143188068557
Loss in iteration 110 : 0.4579948385115062
Loss in iteration 111 : 0.46805900836370257
Loss in iteration 112 : 0.46097504330160927
Loss in iteration 113 : 0.4558344803973019
Loss in iteration 114 : 0.4587220330103781
Loss in iteration 115 : 0.45568983846676436
Loss in iteration 116 : 0.46607409690043017
Loss in iteration 117 : 0.47376025195258126
Loss in iteration 118 : 0.47170303435329836
Loss in iteration 119 : 0.4567529896465147
Loss in iteration 120 : 0.47317429776856446
Loss in iteration 121 : 0.46836526688243574
Loss in iteration 122 : 0.4757453063311755
Loss in iteration 123 : 0.46715332193101905
Loss in iteration 124 : 0.45976346061216894
Loss in iteration 125 : 0.4587642652328012
Loss in iteration 126 : 0.45953749678410144
Loss in iteration 127 : 0.46035882733415123
Loss in iteration 128 : 0.46656654280484017
Loss in iteration 129 : 0.4608617590556293
Loss in iteration 130 : 0.46685080000027573
Loss in iteration 131 : 0.46135201744461457
Loss in iteration 132 : 0.45908022991590053
Loss in iteration 133 : 0.4662781504095287
Loss in iteration 134 : 0.46427997988818337
Loss in iteration 135 : 0.4636991175149685
Loss in iteration 136 : 0.4630460968744759
Loss in iteration 137 : 0.46805993789930705
Loss in iteration 138 : 0.4665018045375506
Loss in iteration 139 : 0.4729258279382952
Loss in iteration 140 : 0.467731742834338
Loss in iteration 141 : 0.4687336512300516
Loss in iteration 142 : 0.4584275823190173
Loss in iteration 143 : 0.46500089964072827
Loss in iteration 144 : 0.45774619896034674
Loss in iteration 145 : 0.4525163284939724
Loss in iteration 146 : 0.4681684551449996
Loss in iteration 147 : 0.47227467317989896
Loss in iteration 148 : 0.46251658115395006
Loss in iteration 149 : 0.4762519168850894
Loss in iteration 150 : 0.46708517511077824
Loss in iteration 151 : 0.4622529225392376
Loss in iteration 152 : 0.4733076826707903
Loss in iteration 153 : 0.44985819717265535
Loss in iteration 154 : 0.47129930259858727
Loss in iteration 155 : 0.4673356407639254
Loss in iteration 156 : 0.45014999087040625
Loss in iteration 157 : 0.4655787707779014
Loss in iteration 158 : 0.45928968462752845
Loss in iteration 159 : 0.46034631845819624
Loss in iteration 160 : 0.45982397461398944
Loss in iteration 161 : 0.45788676340849827
Loss in iteration 162 : 0.46179841875647365
Loss in iteration 163 : 0.4661973785310211
Loss in iteration 164 : 0.46579147458973214
Loss in iteration 165 : 0.4584824561288003
Loss in iteration 166 : 0.4613022329258387
Loss in iteration 167 : 0.4703557316581934
Loss in iteration 168 : 0.47179482553065744
Loss in iteration 169 : 0.46001847525185785
Loss in iteration 170 : 0.46663510258451546
Loss in iteration 171 : 0.45670317300243046
Loss in iteration 172 : 0.46582676876194895
Loss in iteration 173 : 0.45910498752433404
Loss in iteration 174 : 0.4565072685777737
Loss in iteration 175 : 0.4706644277276272
Loss in iteration 176 : 0.45640607887755147
Loss in iteration 177 : 0.46977103283448646
Loss in iteration 178 : 0.45922311744381794
Loss in iteration 179 : 0.46783781817167563
Loss in iteration 180 : 0.4560630964285374
Loss in iteration 181 : 0.46601331344573255
Loss in iteration 182 : 0.46074440318870824
Loss in iteration 183 : 0.45950584418189283
Loss in iteration 184 : 0.47680866025200314
Loss in iteration 185 : 0.4607320042562293
Loss in iteration 186 : 0.4520298649832603
Loss in iteration 187 : 0.45715104102450355
Loss in iteration 188 : 0.46888401000232627
Loss in iteration 189 : 0.46662136352926903
Loss in iteration 190 : 0.4512910691770897
Loss in iteration 191 : 0.4552776311632969
Loss in iteration 192 : 0.46011831977352213
Loss in iteration 193 : 0.4638458013026982
Loss in iteration 194 : 0.4627779340896118
Loss in iteration 195 : 0.47021593586050403
Loss in iteration 196 : 0.47633117731457725
Loss in iteration 197 : 0.45247055226648675
Loss in iteration 198 : 0.4711619095415286
Loss in iteration 199 : 0.4615714772494068
Loss in iteration 200 : 0.4526040156584307
Loss in iteration 201 : 0.4597048799613808
Loss in iteration 202 : 0.47094549631576166
Loss in iteration 203 : 0.4512950716006088
Loss in iteration 204 : 0.45791414720475865
Loss in iteration 205 : 0.46149514519128204
Loss in iteration 206 : 0.4607741274088623
Loss in iteration 207 : 0.45653286568837836
Loss in iteration 208 : 0.45182323153274145
Loss in iteration 209 : 0.46579388764294016
Loss in iteration 210 : 0.47178482185100007
Loss in iteration 211 : 0.46403700466175785
Loss in iteration 212 : 0.4686037729645311
Loss in iteration 213 : 0.4700066876950461
Loss in iteration 214 : 0.48241338234435943
Loss in iteration 215 : 0.47140873137709316
Loss in iteration 216 : 0.46043510513267477
Loss in iteration 217 : 0.45930942533106656
Loss in iteration 218 : 0.4564300545183053
Loss in iteration 219 : 0.46893849660101117
Loss in iteration 220 : 0.4684758555316957
Loss in iteration 221 : 0.4647473032990377
Loss in iteration 222 : 0.4555412001255715
Loss in iteration 223 : 0.4676245019376815
Loss in iteration 224 : 0.46990747291587615
Loss in iteration 225 : 0.4603952988042175
Loss in iteration 226 : 0.464284050826155
Loss in iteration 227 : 0.4535271546600049
Loss in iteration 228 : 0.46544778970405515
Loss in iteration 229 : 0.4650180380286929
Loss in iteration 230 : 0.4562965115001181
Loss in iteration 231 : 0.46421234575718745
Loss in iteration 232 : 0.4671168008566116
Loss in iteration 233 : 0.46002312998929623
Loss in iteration 234 : 0.4662455933720749
Loss in iteration 235 : 0.4578165121701564
Loss in iteration 236 : 0.4633078317727775
Loss in iteration 237 : 0.4631059170445855
Loss in iteration 238 : 0.46077907632929305
Loss in iteration 239 : 0.4625415394153804
Loss in iteration 240 : 0.4532334608498813
Loss in iteration 241 : 0.45705499388434423
Loss in iteration 242 : 0.48359561629363
Loss in iteration 243 : 0.47067212564565813
Loss in iteration 244 : 0.46130533045170125
Loss in iteration 245 : 0.4741588648169611
Loss in iteration 246 : 0.45859420475679513
Loss in iteration 247 : 0.45397892520788885
Loss in iteration 248 : 0.4635368527434757
Loss in iteration 249 : 0.46584111445556303
Loss in iteration 250 : 0.455629992233839
Loss in iteration 251 : 0.46741530845296686
Loss in iteration 252 : 0.46158355325886763
Loss in iteration 253 : 0.4715950511485696
Loss in iteration 254 : 0.4479427105780917
Loss in iteration 255 : 0.4529624558514525
Loss in iteration 256 : 0.47614120060779114
Loss in iteration 257 : 0.45107645911671373
Loss in iteration 258 : 0.45784239285534
Loss in iteration 259 : 0.45733060495736905
Loss in iteration 260 : 0.46013319329350827
Loss in iteration 261 : 0.44892120145753345
Loss in iteration 262 : 0.46166007767645073
Loss in iteration 263 : 0.46157560213557436
Loss in iteration 264 : 0.47486650178880063
Loss in iteration 265 : 0.4510213923653908
Loss in iteration 266 : 0.47094764578375625
Loss in iteration 267 : 0.46785264061943194
Loss in iteration 268 : 0.4729453745581287
Loss in iteration 269 : 0.4549027751118086
Loss in iteration 270 : 0.460302976148878
Loss in iteration 271 : 0.4481123978416772
Loss in iteration 272 : 0.4606574574849062
Loss in iteration 273 : 0.446823916643899
Loss in iteration 274 : 0.45621901887204636
Loss in iteration 275 : 0.46579391145690124
Loss in iteration 276 : 0.4690956336129032
Loss in iteration 277 : 0.45405835687568713
Loss in iteration 278 : 0.4514704611497721
Loss in iteration 279 : 0.44980062243470015
Loss in iteration 280 : 0.45639055557263924
Loss in iteration 281 : 0.463536693844897
Loss in iteration 282 : 0.45782405687989863
Loss in iteration 283 : 0.46316722192378157
Loss in iteration 284 : 0.44847011054689523
Loss in iteration 285 : 0.44991048061146655
Loss in iteration 286 : 0.4603615191220665
Loss in iteration 287 : 0.4656918965257833
Loss in iteration 288 : 0.4644533154267478
Loss in iteration 289 : 0.45674258825540026
Loss in iteration 290 : 0.45215204812451076
Loss in iteration 291 : 0.4657671059638308
Loss in iteration 292 : 0.4624201428453664
Loss in iteration 293 : 0.456912790376378
Loss in iteration 294 : 0.46646617629462694
Loss in iteration 295 : 0.45287694494217273
Loss in iteration 296 : 0.45621842478329044
Loss in iteration 297 : 0.4650596060375107
Loss in iteration 298 : 0.4608238128448164
Loss in iteration 299 : 0.46261331720125093
Loss in iteration 300 : 0.4661151157858361
Loss in iteration 301 : 0.46423298292683435
Loss in iteration 302 : 0.4578638433697547
Loss in iteration 303 : 0.4530544394089151
Loss in iteration 304 : 0.45551641397200027
Loss in iteration 305 : 0.45875763033598366
Loss in iteration 306 : 0.4590215855935317
Loss in iteration 307 : 0.4596997190971097
Loss in iteration 308 : 0.4608705234698352
Loss in iteration 309 : 0.4516528384661013
Loss in iteration 310 : 0.45822528981070126
Loss in iteration 311 : 0.46769202035773305
Loss in iteration 312 : 0.47868139784930874
Loss in iteration 313 : 0.44801961264325013
Loss in iteration 314 : 0.45643390271042983
Loss in iteration 315 : 0.4593315414303779
Loss in iteration 316 : 0.47687569560044873
Loss in iteration 317 : 0.4616970428865995
Loss in iteration 318 : 0.4715675602562387
Loss in iteration 319 : 0.4557391715972346
Loss in iteration 320 : 0.454179721478977
Loss in iteration 321 : 0.45482770148607926
Loss in iteration 322 : 0.4596063438421875
Loss in iteration 323 : 0.45693712108060786
Loss in iteration 324 : 0.4588365074135724
Loss in iteration 325 : 0.4572914563527312
Loss in iteration 326 : 0.47329226017326664
Loss in iteration 327 : 0.45159798492887
Loss in iteration 328 : 0.4537878199521237
Loss in iteration 329 : 0.45642134161117354
Loss in iteration 330 : 0.4514159383556581
Loss in iteration 331 : 0.4607621621775157
Loss in iteration 332 : 0.44706044554650126
Loss in iteration 333 : 0.4480231033036046
Loss in iteration 334 : 0.4578523481651597
Loss in iteration 335 : 0.45253748910008124
Loss in iteration 336 : 0.45941589120907067
Loss in iteration 337 : 0.46357009052357356
Loss in iteration 338 : 0.46819600698826663
Loss in iteration 339 : 0.47191312638822996
Loss in iteration 340 : 0.46542885414923246
Loss in iteration 341 : 0.4570692023013062
Loss in iteration 342 : 0.4590348650607786
Loss in iteration 343 : 0.45846434187371804
Loss in iteration 344 : 0.46531814760533224
Loss in iteration 345 : 0.4583170336807723
Loss in iteration 346 : 0.4526738727758748
Loss in iteration 347 : 0.46021237553480016
Loss in iteration 348 : 0.46189231642607304
Loss in iteration 349 : 0.4666855369305352
Loss in iteration 350 : 0.466734699487868
Loss in iteration 351 : 0.46142132587066786
Loss in iteration 352 : 0.45783766589317465
Loss in iteration 353 : 0.4521911182372285
Loss in iteration 354 : 0.4600022297583983
Loss in iteration 355 : 0.4601511320325444
Loss in iteration 356 : 0.4533685038830925
Loss in iteration 357 : 0.45788559858066424
Loss in iteration 358 : 0.4666717250367409
Loss in iteration 359 : 0.4571273006423923
Loss in iteration 360 : 0.4653227464394633
Loss in iteration 361 : 0.45015803008990146
Loss in iteration 362 : 0.47010259092387513
Loss in iteration 363 : 0.4614955639371078
Loss in iteration 364 : 0.4591368493378342
Loss in iteration 365 : 0.46260982016532187
Loss in iteration 366 : 0.4570776461607329
Loss in iteration 367 : 0.4666797778502473
Loss in iteration 368 : 0.4676551312074141
Loss in iteration 369 : 0.4721959220436303
Loss in iteration 370 : 0.4565823194538569
Loss in iteration 371 : 0.4743332698396809
Loss in iteration 372 : 0.4539379203852111
Loss in iteration 373 : 0.4606576847831416
Loss in iteration 374 : 0.4581631341127138
Loss in iteration 375 : 0.4559564747618135
Loss in iteration 376 : 0.4653400215533751
Loss in iteration 377 : 0.4604549291046583
Loss in iteration 378 : 0.4637936590758631
Loss in iteration 379 : 0.4631550732168192
Loss in iteration 380 : 0.45808409267873534
Loss in iteration 381 : 0.44866654219027785
Loss in iteration 382 : 0.4549042777129831
Loss in iteration 383 : 0.46283099739077854
Loss in iteration 384 : 0.4677868302995098
Loss in iteration 385 : 0.4609185395884195
Loss in iteration 386 : 0.4481752960999261
Loss in iteration 387 : 0.46593573804696653
Loss in iteration 388 : 0.4698203690817175
Loss in iteration 389 : 0.4565472137073312
Loss in iteration 390 : 0.45784480979952347
Loss in iteration 391 : 0.4608837932229449
Loss in iteration 392 : 0.4505358448098411
Loss in iteration 393 : 0.4537816782951106
Loss in iteration 394 : 0.45753318483861105
Loss in iteration 395 : 0.46554327843228466
Loss in iteration 396 : 0.45863660711352044
Loss in iteration 397 : 0.47205194854738947
Loss in iteration 398 : 0.4682958007924034
Loss in iteration 399 : 0.4568924075717255
Loss in iteration 400 : 0.45069324515767667
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.788, training accuracy 0.788, time elapsed: 5433 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6790945110953196
Loss in iteration 3 : 0.6624165655564761
Loss in iteration 4 : 0.6540135519580339
Loss in iteration 5 : 0.6354786560678681
Loss in iteration 6 : 0.6267103843913401
Loss in iteration 7 : 0.6123377548605172
Loss in iteration 8 : 0.5982652205469643
Loss in iteration 9 : 0.5864641204572522
Loss in iteration 10 : 0.574676773040019
Loss in iteration 11 : 0.5607892392662881
Loss in iteration 12 : 0.5516148266851573
Loss in iteration 13 : 0.5417745846172044
Loss in iteration 14 : 0.5328932907926502
Loss in iteration 15 : 0.534113208804084
Loss in iteration 16 : 0.5246863026717218
Loss in iteration 17 : 0.5167145788999994
Loss in iteration 18 : 0.5123380726496658
Loss in iteration 19 : 0.5126094750956819
Loss in iteration 20 : 0.5035954059915674
Loss in iteration 21 : 0.5121026996970428
Loss in iteration 22 : 0.508028259434548
Loss in iteration 23 : 0.5052844432355598
Loss in iteration 24 : 0.49544371170364493
Loss in iteration 25 : 0.49937647876531815
Loss in iteration 26 : 0.4877553690684927
Loss in iteration 27 : 0.48877502051243055
Loss in iteration 28 : 0.4920492914877816
Loss in iteration 29 : 0.49871826874947983
Loss in iteration 30 : 0.49618269346693744
Loss in iteration 31 : 0.49007477905826907
Loss in iteration 32 : 0.4889926451415717
Loss in iteration 33 : 0.4831828044213686
Loss in iteration 34 : 0.48298040247097024
Loss in iteration 35 : 0.4910548288609119
Loss in iteration 36 : 0.4897881504644616
Loss in iteration 37 : 0.4834852502321402
Loss in iteration 38 : 0.48218415997902353
Loss in iteration 39 : 0.4769569034282821
Loss in iteration 40 : 0.48233497425468586
Loss in iteration 41 : 0.48579090165593314
Loss in iteration 42 : 0.4743224389884533
Loss in iteration 43 : 0.48284157088391083
Loss in iteration 44 : 0.4822601973348629
Loss in iteration 45 : 0.4820965737336617
Loss in iteration 46 : 0.48151299357932337
Loss in iteration 47 : 0.4776825627104071
Loss in iteration 48 : 0.48189768286640783
Loss in iteration 49 : 0.47519556070650876
Loss in iteration 50 : 0.4685672860310023
Loss in iteration 51 : 0.4846365911445253
Loss in iteration 52 : 0.4726081890119845
Loss in iteration 53 : 0.47249889154619024
Loss in iteration 54 : 0.47015257093697727
Loss in iteration 55 : 0.47356796187858974
Loss in iteration 56 : 0.47781003527119376
Loss in iteration 57 : 0.46209545129600893
Loss in iteration 58 : 0.4664056987459011
Loss in iteration 59 : 0.4657898328781195
Loss in iteration 60 : 0.47279196736489
Loss in iteration 61 : 0.471293887797435
Loss in iteration 62 : 0.47304291972819046
Loss in iteration 63 : 0.4707706952778427
Loss in iteration 64 : 0.4699929638963459
Loss in iteration 65 : 0.48025829538558945
Loss in iteration 66 : 0.46390208785961384
Loss in iteration 67 : 0.47377820256706726
Loss in iteration 68 : 0.46472939531800683
Loss in iteration 69 : 0.4741526280618505
Loss in iteration 70 : 0.46475856158201995
Loss in iteration 71 : 0.47947599273571484
Loss in iteration 72 : 0.4650262865635066
Loss in iteration 73 : 0.4747511748988045
Loss in iteration 74 : 0.4675242438783661
Loss in iteration 75 : 0.4685861546850054
Loss in iteration 76 : 0.4791496336339977
Loss in iteration 77 : 0.46303193730041897
Loss in iteration 78 : 0.4712746706934795
Loss in iteration 79 : 0.4534813907831697
Loss in iteration 80 : 0.46878173026844605
Loss in iteration 81 : 0.471294050290754
Loss in iteration 82 : 0.46777614135406115
Loss in iteration 83 : 0.46988573811485645
Loss in iteration 84 : 0.47921703117178605
Loss in iteration 85 : 0.4694997932512031
Loss in iteration 86 : 0.47665719256522754
Loss in iteration 87 : 0.45821894770943566
Loss in iteration 88 : 0.4777089734899373
Loss in iteration 89 : 0.46110305710672983
Loss in iteration 90 : 0.464495008002281
Loss in iteration 91 : 0.4734168770766879
Loss in iteration 92 : 0.4614274509122732
Loss in iteration 93 : 0.46630235846237356
Loss in iteration 94 : 0.46582107406843004
Loss in iteration 95 : 0.4631802381185768
Loss in iteration 96 : 0.474397428256845
Loss in iteration 97 : 0.465245298765441
Loss in iteration 98 : 0.4703753391157003
Loss in iteration 99 : 0.464865070988955
Loss in iteration 100 : 0.46946495995090537
Loss in iteration 101 : 0.4630943710685901
Loss in iteration 102 : 0.460671917702635
Loss in iteration 103 : 0.4715506846273842
Loss in iteration 104 : 0.46627001966698467
Loss in iteration 105 : 0.45239368054667595
Loss in iteration 106 : 0.46582510323720544
Loss in iteration 107 : 0.4776627816753515
Loss in iteration 108 : 0.4800631834293785
Loss in iteration 109 : 0.4648831523590797
Loss in iteration 110 : 0.4588679214196055
Loss in iteration 111 : 0.4689409658708701
Loss in iteration 112 : 0.4618145170743854
Loss in iteration 113 : 0.45672340135455003
Loss in iteration 114 : 0.4593824651172356
Loss in iteration 115 : 0.45658057584224787
Loss in iteration 116 : 0.46667510082869756
Loss in iteration 117 : 0.4743774705111907
Loss in iteration 118 : 0.47242101160275185
Loss in iteration 119 : 0.457470302169201
Loss in iteration 120 : 0.4737019578111968
Loss in iteration 121 : 0.469034674446074
Loss in iteration 122 : 0.4761123851672322
Loss in iteration 123 : 0.46776289807084204
Loss in iteration 124 : 0.46052327852531716
Loss in iteration 125 : 0.45963192990487817
Loss in iteration 126 : 0.45999699881460493
Loss in iteration 127 : 0.4610423507158438
Loss in iteration 128 : 0.46715916407668673
Loss in iteration 129 : 0.4614281209966969
Loss in iteration 130 : 0.46734052370206003
Loss in iteration 131 : 0.462038098813138
Loss in iteration 132 : 0.4598071107406971
Loss in iteration 133 : 0.466785621099922
Loss in iteration 134 : 0.4649483630541386
Loss in iteration 135 : 0.4642550514728567
Loss in iteration 136 : 0.4637133692785578
Loss in iteration 137 : 0.4685737858586922
Loss in iteration 138 : 0.46714019712818966
Loss in iteration 139 : 0.47327647576045684
Loss in iteration 140 : 0.46816386108450425
Loss in iteration 141 : 0.46922874827959354
Loss in iteration 142 : 0.459152511035777
Loss in iteration 143 : 0.46536522870216857
Loss in iteration 144 : 0.4584565618393512
Loss in iteration 145 : 0.4532176085825555
Loss in iteration 146 : 0.46856473617694255
Loss in iteration 147 : 0.47269230017527475
Loss in iteration 148 : 0.4630957261330688
Loss in iteration 149 : 0.4765919271370891
Loss in iteration 150 : 0.46769164161908483
Loss in iteration 151 : 0.46267668804482076
Loss in iteration 152 : 0.4735073065732933
Loss in iteration 153 : 0.45047619832597596
Loss in iteration 154 : 0.4716959176842681
Loss in iteration 155 : 0.46784101033780073
Loss in iteration 156 : 0.4507998757856258
Loss in iteration 157 : 0.46599652106949424
Loss in iteration 158 : 0.4594445767587766
Loss in iteration 159 : 0.46070117211971096
Loss in iteration 160 : 0.4601583217723448
Loss in iteration 161 : 0.4584946134105665
Loss in iteration 162 : 0.46216067865827515
Loss in iteration 163 : 0.466634420484859
Loss in iteration 164 : 0.46616287884835383
Loss in iteration 165 : 0.45884090476361095
Loss in iteration 166 : 0.46171116590881833
Loss in iteration 167 : 0.4708064094278084
Loss in iteration 168 : 0.4720860016108429
Loss in iteration 169 : 0.46060878035501096
Loss in iteration 170 : 0.46706126171111334
Loss in iteration 171 : 0.45705549929101863
Loss in iteration 172 : 0.4661794960669116
Loss in iteration 173 : 0.4596813072160275
Loss in iteration 174 : 0.45689091319965797
Loss in iteration 175 : 0.47106055322966717
Loss in iteration 176 : 0.4569583005238459
Loss in iteration 177 : 0.4701481080644905
Loss in iteration 178 : 0.45953577797502076
Loss in iteration 179 : 0.4681985900178429
Loss in iteration 180 : 0.45649210709478877
Loss in iteration 181 : 0.46622730645394483
Loss in iteration 182 : 0.4611418743777551
Loss in iteration 183 : 0.4599141445960772
Loss in iteration 184 : 0.47704504228037564
Loss in iteration 185 : 0.46117669256271127
Loss in iteration 186 : 0.4525427799913629
Loss in iteration 187 : 0.45743110781710505
Loss in iteration 188 : 0.469094564293008
Loss in iteration 189 : 0.4669981961705589
Loss in iteration 190 : 0.45183496602194595
Loss in iteration 191 : 0.4557172447988771
Loss in iteration 192 : 0.460288146703818
Loss in iteration 193 : 0.4641298900179962
Loss in iteration 194 : 0.4630800054913239
Loss in iteration 195 : 0.4706130497481238
Loss in iteration 196 : 0.4767386681986058
Loss in iteration 197 : 0.4528869128171685
Loss in iteration 198 : 0.4713537869495706
Loss in iteration 199 : 0.4618197835073229
Loss in iteration 200 : 0.45312560282040376
Loss in iteration 201 : 0.4601396792800757
Loss in iteration 202 : 0.4711709001959941
Loss in iteration 203 : 0.45162845410740293
Loss in iteration 204 : 0.45816980245312605
Loss in iteration 205 : 0.4617897376523853
Loss in iteration 206 : 0.46111939829237475
Loss in iteration 207 : 0.45698418160724297
Loss in iteration 208 : 0.4522599021029821
Loss in iteration 209 : 0.4661070465303227
Loss in iteration 210 : 0.4719890821346702
Loss in iteration 211 : 0.4643960446967996
Loss in iteration 212 : 0.4688877602314284
Loss in iteration 213 : 0.47022529590178325
Loss in iteration 214 : 0.48263087769922147
Loss in iteration 215 : 0.47156688553350473
Loss in iteration 216 : 0.4608065984392096
Loss in iteration 217 : 0.45974166193773514
Loss in iteration 218 : 0.4565841404463659
Loss in iteration 219 : 0.46911149464710444
Loss in iteration 220 : 0.4686092341895419
Loss in iteration 221 : 0.46506126824344285
Loss in iteration 222 : 0.4557731505020789
Loss in iteration 223 : 0.4678819394806236
Loss in iteration 224 : 0.46998430384691137
Loss in iteration 225 : 0.4606772475275472
Loss in iteration 226 : 0.46456182983515043
Loss in iteration 227 : 0.45384259489447276
Loss in iteration 228 : 0.465557156131253
Loss in iteration 229 : 0.4651627988995212
Loss in iteration 230 : 0.45653478834173994
Loss in iteration 231 : 0.46454692445873164
Loss in iteration 232 : 0.46730556049744726
Loss in iteration 233 : 0.46026597489991183
Loss in iteration 234 : 0.4665509242629022
Loss in iteration 235 : 0.45796364353999885
Loss in iteration 236 : 0.4634951961678264
Loss in iteration 237 : 0.46334891767846004
Loss in iteration 238 : 0.46102368458724324
Loss in iteration 239 : 0.4626706885040641
Loss in iteration 240 : 0.4537052494226891
Loss in iteration 241 : 0.4572525450262057
Loss in iteration 242 : 0.48378778855495524
Loss in iteration 243 : 0.4709378633803222
Loss in iteration 244 : 0.4614340594557165
Loss in iteration 245 : 0.47438073446321194
Loss in iteration 246 : 0.45892244985144903
Loss in iteration 247 : 0.45425324277497564
Loss in iteration 248 : 0.46374025891411186
Loss in iteration 249 : 0.46609461130708074
Loss in iteration 250 : 0.4558993754004453
Loss in iteration 251 : 0.4676119352435457
Loss in iteration 252 : 0.4617529559888741
Loss in iteration 253 : 0.4718318749819612
Loss in iteration 254 : 0.4482193729068706
Loss in iteration 255 : 0.45326062357218455
Loss in iteration 256 : 0.476319212911172
Loss in iteration 257 : 0.45147433602755715
Loss in iteration 258 : 0.4580941208008259
Loss in iteration 259 : 0.45758799054372434
Loss in iteration 260 : 0.46043917290156167
Loss in iteration 261 : 0.44920273591288457
Loss in iteration 262 : 0.4618230062501832
Loss in iteration 263 : 0.46188561797660155
Loss in iteration 264 : 0.4750534445162504
Loss in iteration 265 : 0.4512461549254428
Loss in iteration 266 : 0.4711000749880777
Loss in iteration 267 : 0.46805651783868973
Loss in iteration 268 : 0.4731764713661847
Loss in iteration 269 : 0.4551761553341022
Loss in iteration 270 : 0.4605982095412026
Loss in iteration 271 : 0.44838978865575724
Loss in iteration 272 : 0.460966581376123
Loss in iteration 273 : 0.4470440084539494
Loss in iteration 274 : 0.4564295575615143
Loss in iteration 275 : 0.4659602640539425
Loss in iteration 276 : 0.4692228404832404
Loss in iteration 277 : 0.45424706170242973
Loss in iteration 278 : 0.4518532189977585
Loss in iteration 279 : 0.4501225166240266
Loss in iteration 280 : 0.4566237973164843
Loss in iteration 281 : 0.46373280402728073
Loss in iteration 282 : 0.45796658912704097
Loss in iteration 283 : 0.4634463434183296
Loss in iteration 284 : 0.4487167256632271
Loss in iteration 285 : 0.4501412338704529
Loss in iteration 286 : 0.46062563751283936
Loss in iteration 287 : 0.465889627921287
Loss in iteration 288 : 0.46462039735441174
Loss in iteration 289 : 0.4568379063764132
Loss in iteration 290 : 0.45226235553529104
Loss in iteration 291 : 0.4658694848504192
Loss in iteration 292 : 0.46262478749584435
Loss in iteration 293 : 0.4570568697974567
Loss in iteration 294 : 0.4667624060772867
Loss in iteration 295 : 0.453056952422643
Loss in iteration 296 : 0.45633153180929364
Loss in iteration 297 : 0.4652441054433431
Loss in iteration 298 : 0.4610425226618351
Loss in iteration 299 : 0.46288068116053965
Loss in iteration 300 : 0.4661856181513106
Loss in iteration 301 : 0.46448731888253686
Loss in iteration 302 : 0.4580889707482488
Loss in iteration 303 : 0.45320026982954476
Loss in iteration 304 : 0.45570455428673495
Loss in iteration 305 : 0.45892017681860375
Loss in iteration 306 : 0.45921217288005745
Loss in iteration 307 : 0.45993891505089296
Loss in iteration 308 : 0.4610704248803169
Loss in iteration 309 : 0.45187046691665606
Loss in iteration 310 : 0.45838488149362594
Loss in iteration 311 : 0.4679064625531618
Loss in iteration 312 : 0.478825964551752
Loss in iteration 313 : 0.4483242697330807
Loss in iteration 314 : 0.456404369218213
Loss in iteration 315 : 0.4595379966880097
Loss in iteration 316 : 0.47705828626644586
Loss in iteration 317 : 0.4615503142156022
Loss in iteration 318 : 0.47169026750657905
Loss in iteration 319 : 0.4560228256148638
Loss in iteration 320 : 0.45451074618313686
Loss in iteration 321 : 0.454858363093126
Loss in iteration 322 : 0.45967556030099105
Loss in iteration 323 : 0.45720356418921076
Loss in iteration 324 : 0.4589783387956198
Loss in iteration 325 : 0.45753534697743425
Loss in iteration 326 : 0.4734297666989658
Loss in iteration 327 : 0.4519282568529416
Loss in iteration 328 : 0.45393297161554924
Loss in iteration 329 : 0.4565088013300803
Loss in iteration 330 : 0.45161804734278127
Loss in iteration 331 : 0.4609314336899626
Loss in iteration 332 : 0.4472938527496116
Loss in iteration 333 : 0.4482269998174648
Loss in iteration 334 : 0.45816462007483055
Loss in iteration 335 : 0.45271286886041606
Loss in iteration 336 : 0.4595948749625892
Loss in iteration 337 : 0.46368573504959476
Loss in iteration 338 : 0.4683330036299211
Loss in iteration 339 : 0.4720752831774713
Loss in iteration 340 : 0.4655701354600465
Loss in iteration 341 : 0.45730923772891313
Loss in iteration 342 : 0.459195601592048
Loss in iteration 343 : 0.4586368637463417
Loss in iteration 344 : 0.4655191976850074
Loss in iteration 345 : 0.45860029961013327
Loss in iteration 346 : 0.452993188623794
Loss in iteration 347 : 0.4602319960158423
Loss in iteration 348 : 0.46197520826480476
Loss in iteration 349 : 0.46676119766114144
Loss in iteration 350 : 0.4668840346472663
Loss in iteration 351 : 0.46157489937575813
Loss in iteration 352 : 0.45797345513215953
Loss in iteration 353 : 0.452459423294903
Loss in iteration 354 : 0.46016550605244716
Loss in iteration 355 : 0.46029132453460925
Loss in iteration 356 : 0.45348803293173096
Loss in iteration 357 : 0.4579978370597417
Loss in iteration 358 : 0.4667422067459122
Loss in iteration 359 : 0.4573163320121663
Loss in iteration 360 : 0.4653739785294646
Loss in iteration 361 : 0.4503885984383318
Loss in iteration 362 : 0.47028181880691894
Loss in iteration 363 : 0.46162089557070224
Loss in iteration 364 : 0.4593191146478948
Loss in iteration 365 : 0.4627158356867349
Loss in iteration 366 : 0.4572115067432382
Loss in iteration 367 : 0.46674595467458047
Loss in iteration 368 : 0.4677239829279242
Loss in iteration 369 : 0.47229164433952536
Loss in iteration 370 : 0.45677455275050177
Loss in iteration 371 : 0.4744741576837388
Loss in iteration 372 : 0.45413244414303927
Loss in iteration 373 : 0.46067686851235623
Loss in iteration 374 : 0.4583373257743807
Loss in iteration 375 : 0.4563205991539912
Loss in iteration 376 : 0.4654703251068638
Loss in iteration 377 : 0.4605937129670732
Loss in iteration 378 : 0.46395852414208294
Loss in iteration 379 : 0.46327858280532613
Loss in iteration 380 : 0.4581264110897728
Loss in iteration 381 : 0.44891151135660057
Loss in iteration 382 : 0.4551056843523021
Loss in iteration 383 : 0.4628290594217013
Loss in iteration 384 : 0.46792037241090273
Loss in iteration 385 : 0.46111299735288275
Loss in iteration 386 : 0.44843533448835965
Loss in iteration 387 : 0.4660595701299551
Loss in iteration 388 : 0.4699554859623139
Loss in iteration 389 : 0.4565710162083213
Loss in iteration 390 : 0.45799754025044903
Loss in iteration 391 : 0.4611009140997493
Loss in iteration 392 : 0.45063122465483424
Loss in iteration 393 : 0.45394428144884635
Loss in iteration 394 : 0.45769210372751884
Loss in iteration 395 : 0.4657051112841319
Loss in iteration 396 : 0.4589121298244723
Loss in iteration 397 : 0.4722225791724714
Loss in iteration 398 : 0.4684269124600045
Loss in iteration 399 : 0.45697263165906316
Loss in iteration 400 : 0.4508648810461204
Testing accuracy  of updater 8 on alg 0 with rate 0.14 = 0.786875, training accuracy 0.786875, time elapsed: 5504 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6816940147397285
Loss in iteration 3 : 0.6681481372678689
Loss in iteration 4 : 0.6629344945496907
Loss in iteration 5 : 0.6475604118423569
Loss in iteration 6 : 0.6405417704045132
Loss in iteration 7 : 0.6285700074123988
Loss in iteration 8 : 0.616637994728875
Loss in iteration 9 : 0.6059554212793616
Loss in iteration 10 : 0.5960940715036962
Loss in iteration 11 : 0.582401351682266
Loss in iteration 12 : 0.5743858565095586
Loss in iteration 13 : 0.5649028585590211
Loss in iteration 14 : 0.5556528103982784
Loss in iteration 15 : 0.5544099085304062
Loss in iteration 16 : 0.5447113162729903
Loss in iteration 17 : 0.5371309747684272
Loss in iteration 18 : 0.5319513213607212
Loss in iteration 19 : 0.5310020884006256
Loss in iteration 20 : 0.5221014939743016
Loss in iteration 21 : 0.5276268467335997
Loss in iteration 22 : 0.5233147857711425
Loss in iteration 23 : 0.520278392318623
Loss in iteration 24 : 0.50976868893258
Loss in iteration 25 : 0.5127198781837117
Loss in iteration 26 : 0.501882279508106
Loss in iteration 27 : 0.5016534204747334
Loss in iteration 28 : 0.5038474629290363
Loss in iteration 29 : 0.5097717969258931
Loss in iteration 30 : 0.506761783766589
Loss in iteration 31 : 0.5009009065070847
Loss in iteration 32 : 0.49933264348138656
Loss in iteration 33 : 0.49359211694332517
Loss in iteration 34 : 0.4930183744534435
Loss in iteration 35 : 0.4995892955692985
Loss in iteration 36 : 0.4979224638550217
Loss in iteration 37 : 0.49186157806431
Loss in iteration 38 : 0.49045875399747085
Loss in iteration 39 : 0.4860599146379277
Loss in iteration 40 : 0.49000925162676784
Loss in iteration 41 : 0.49284575726083
Loss in iteration 42 : 0.4823140292247518
Loss in iteration 43 : 0.49029465337840555
Loss in iteration 44 : 0.48944399850331055
Loss in iteration 45 : 0.488763370486379
Loss in iteration 46 : 0.48823663727032096
Loss in iteration 47 : 0.48484508543739296
Loss in iteration 48 : 0.4888448350247961
Loss in iteration 49 : 0.48199073024747574
Loss in iteration 50 : 0.4762242808354308
Loss in iteration 51 : 0.49071573581390227
Loss in iteration 52 : 0.4794326825581485
Loss in iteration 53 : 0.47896557485008634
Loss in iteration 54 : 0.47638818229774044
Loss in iteration 55 : 0.47960098855233807
Loss in iteration 56 : 0.4834572217835219
Loss in iteration 57 : 0.4687853551504898
Loss in iteration 58 : 0.4727420642547896
Loss in iteration 59 : 0.47259384752389405
Loss in iteration 60 : 0.4783475785714087
Loss in iteration 61 : 0.4777730071894473
Loss in iteration 62 : 0.47861859793774975
Loss in iteration 63 : 0.4761591697986003
Loss in iteration 64 : 0.4753913595645487
Loss in iteration 65 : 0.48508625897789426
Loss in iteration 66 : 0.4695853873800526
Loss in iteration 67 : 0.478297925146767
Loss in iteration 68 : 0.46960380284622205
Loss in iteration 69 : 0.4795606064025688
Loss in iteration 70 : 0.47033863630607925
Loss in iteration 71 : 0.4849794187453959
Loss in iteration 72 : 0.47073839260326
Loss in iteration 73 : 0.47908406213617327
Loss in iteration 74 : 0.47306569502965856
Loss in iteration 75 : 0.4737595201384033
Loss in iteration 76 : 0.4831286827517552
Loss in iteration 77 : 0.468665069076676
Loss in iteration 78 : 0.475917083175836
Loss in iteration 79 : 0.45925944754270204
Loss in iteration 80 : 0.47326501546875055
Loss in iteration 81 : 0.4759869934200314
Loss in iteration 82 : 0.4720806293138575
Loss in iteration 83 : 0.47498947990312
Loss in iteration 84 : 0.4830835162633533
Loss in iteration 85 : 0.47433451095826107
Loss in iteration 86 : 0.48049719042306327
Loss in iteration 87 : 0.46301226726856054
Loss in iteration 88 : 0.48205432234716644
Loss in iteration 89 : 0.4657147414579445
Loss in iteration 90 : 0.46901629675081924
Loss in iteration 91 : 0.4775104794936573
Loss in iteration 92 : 0.4660611926779625
Loss in iteration 93 : 0.47031949502448195
Loss in iteration 94 : 0.4702038962335898
Loss in iteration 95 : 0.4669675915887454
Loss in iteration 96 : 0.4788302603778523
Loss in iteration 97 : 0.4693836618495459
Loss in iteration 98 : 0.4741360092075441
Loss in iteration 99 : 0.46901816367627625
Loss in iteration 100 : 0.47369333993428187
Loss in iteration 101 : 0.46696262514756065
Loss in iteration 102 : 0.46544467286960434
Loss in iteration 103 : 0.4755242693802243
Loss in iteration 104 : 0.4703046077387343
Loss in iteration 105 : 0.4572222918392712
Loss in iteration 106 : 0.4691761872203502
Loss in iteration 107 : 0.47988741384659495
Loss in iteration 108 : 0.4832067226348355
Loss in iteration 109 : 0.46841746557018965
Loss in iteration 110 : 0.46279817098318204
Loss in iteration 111 : 0.472684991188514
Loss in iteration 112 : 0.46559641462187373
Loss in iteration 113 : 0.46066695205358643
Loss in iteration 114 : 0.46250439987946906
Loss in iteration 115 : 0.46043321502332113
Loss in iteration 116 : 0.4694800608874329
Loss in iteration 117 : 0.4774156764442399
Loss in iteration 118 : 0.47556207991307753
Loss in iteration 119 : 0.4608504956418486
Loss in iteration 120 : 0.4764653150222049
Loss in iteration 121 : 0.4722969460888678
Loss in iteration 122 : 0.47825310843569535
Loss in iteration 123 : 0.4705659738142051
Loss in iteration 124 : 0.46375242656360094
Loss in iteration 125 : 0.4632002787759868
Loss in iteration 126 : 0.4627173539182329
Loss in iteration 127 : 0.46414399746872775
Loss in iteration 128 : 0.4699534072575627
Loss in iteration 129 : 0.46413684503804614
Loss in iteration 130 : 0.4697418829867068
Loss in iteration 131 : 0.4649414077399964
Loss in iteration 132 : 0.4631452052148184
Loss in iteration 133 : 0.46936181400335797
Loss in iteration 134 : 0.4680911430290143
Loss in iteration 135 : 0.466861872627716
Loss in iteration 136 : 0.46662209818145944
Loss in iteration 137 : 0.47089470963913427
Loss in iteration 138 : 0.4700742697170071
Loss in iteration 139 : 0.47541576649328554
Loss in iteration 140 : 0.47031831994663315
Loss in iteration 141 : 0.47144168479228477
Loss in iteration 142 : 0.4622322612521995
Loss in iteration 143 : 0.4674327505813489
Loss in iteration 144 : 0.46164998270020136
Loss in iteration 145 : 0.4563904631430326
Loss in iteration 146 : 0.470577843810447
Loss in iteration 147 : 0.47486616281588756
Loss in iteration 148 : 0.46568552128074986
Loss in iteration 149 : 0.47832505607924264
Loss in iteration 150 : 0.47030006180345085
Loss in iteration 151 : 0.464880592564538
Loss in iteration 152 : 0.4752363933524448
Loss in iteration 153 : 0.45326349285338874
Loss in iteration 154 : 0.47369970907165443
Loss in iteration 155 : 0.4700779049717086
Loss in iteration 156 : 0.45360220550424657
Loss in iteration 157 : 0.4681360011302789
Loss in iteration 158 : 0.4608467036804749
Loss in iteration 159 : 0.4626515533226163
Loss in iteration 160 : 0.4620118155329511
Loss in iteration 161 : 0.461197209964136
Loss in iteration 162 : 0.4641879516660628
Loss in iteration 163 : 0.4685628374673384
Loss in iteration 164 : 0.46805862451966845
Loss in iteration 165 : 0.4608473759359688
Loss in iteration 166 : 0.4637073200233119
Loss in iteration 167 : 0.47298669938553484
Loss in iteration 168 : 0.47364055455424553
Loss in iteration 169 : 0.4631522266753995
Loss in iteration 170 : 0.46922674517894575
Loss in iteration 171 : 0.4591282881024355
Loss in iteration 172 : 0.46781828507025924
Loss in iteration 173 : 0.4623000558144282
Loss in iteration 174 : 0.45884342512103365
Loss in iteration 175 : 0.472974240943941
Loss in iteration 176 : 0.45929499510497424
Loss in iteration 177 : 0.4720148317832428
Loss in iteration 178 : 0.4611792582810451
Loss in iteration 179 : 0.4699848670470899
Loss in iteration 180 : 0.45864368843014663
Loss in iteration 181 : 0.4674597019318157
Loss in iteration 182 : 0.46306157286203187
Loss in iteration 183 : 0.4618675138229751
Loss in iteration 184 : 0.4782789275964979
Loss in iteration 185 : 0.4631854336051232
Loss in iteration 186 : 0.45479779709717966
Loss in iteration 187 : 0.45889984999786904
Loss in iteration 188 : 0.4703117535102277
Loss in iteration 189 : 0.46880406666801083
Loss in iteration 190 : 0.4543227686027095
Loss in iteration 191 : 0.4577073562800015
Loss in iteration 192 : 0.46173991738365155
Loss in iteration 193 : 0.46560168961419507
Loss in iteration 194 : 0.4646330490805412
Loss in iteration 195 : 0.4722599621721628
Loss in iteration 196 : 0.47845237982321254
Loss in iteration 197 : 0.4548443591808167
Loss in iteration 198 : 0.47273611765561163
Loss in iteration 199 : 0.46313199821102735
Loss in iteration 200 : 0.4551666703072102
Loss in iteration 201 : 0.462291350453271
Loss in iteration 202 : 0.4722483945524464
Loss in iteration 203 : 0.45323427778259273
Loss in iteration 204 : 0.4595176579682323
Loss in iteration 205 : 0.4633843096980945
Loss in iteration 206 : 0.46274649478203284
Loss in iteration 207 : 0.4589689723905789
Loss in iteration 208 : 0.4542142810564676
Loss in iteration 209 : 0.467628998838996
Loss in iteration 210 : 0.47300987809640416
Loss in iteration 211 : 0.4659051365804375
Loss in iteration 212 : 0.4702813245241941
Loss in iteration 213 : 0.47130081630472287
Loss in iteration 214 : 0.4836742394531465
Loss in iteration 215 : 0.4727017261238593
Loss in iteration 216 : 0.4626025813072178
Loss in iteration 217 : 0.4614942685161548
Loss in iteration 218 : 0.45776198997549505
Loss in iteration 219 : 0.4700711565599121
Loss in iteration 220 : 0.4693632471836619
Loss in iteration 221 : 0.46651737790740727
Loss in iteration 222 : 0.456841521281004
Loss in iteration 223 : 0.46928980383412855
Loss in iteration 224 : 0.4706300967412727
Loss in iteration 225 : 0.4620331400905357
Loss in iteration 226 : 0.4656997349380836
Loss in iteration 227 : 0.4552478794302007
Loss in iteration 228 : 0.46641004893143884
Loss in iteration 229 : 0.46614609240323446
Loss in iteration 230 : 0.4577890986674462
Loss in iteration 231 : 0.46586726965363134
Loss in iteration 232 : 0.46812499908609895
Loss in iteration 233 : 0.46147546989307126
Loss in iteration 234 : 0.46782617228379797
Loss in iteration 235 : 0.45892583886486743
Loss in iteration 236 : 0.46441409920875937
Loss in iteration 237 : 0.4645088656084081
Loss in iteration 238 : 0.46229617350401386
Loss in iteration 239 : 0.4634056968989536
Loss in iteration 240 : 0.45563360430981825
Loss in iteration 241 : 0.4583983981103322
Loss in iteration 242 : 0.4846836601759438
Loss in iteration 243 : 0.4721100804806893
Loss in iteration 244 : 0.46235788357919466
Loss in iteration 245 : 0.47535952292248973
Loss in iteration 246 : 0.4604426419719414
Loss in iteration 247 : 0.45560207884292536
Loss in iteration 248 : 0.4647606979101602
Loss in iteration 249 : 0.46728128107658157
Loss in iteration 250 : 0.4571150146611867
Loss in iteration 251 : 0.46855599623347033
Loss in iteration 252 : 0.46252819540989315
Loss in iteration 253 : 0.47280039314997296
Loss in iteration 254 : 0.44943784406086335
Loss in iteration 255 : 0.45470359119777887
Loss in iteration 256 : 0.4771283883581485
Loss in iteration 257 : 0.45307644838062505
Loss in iteration 258 : 0.4591808017373767
Loss in iteration 259 : 0.4586820147861813
Loss in iteration 260 : 0.4617678097418881
Loss in iteration 261 : 0.4505824965385232
Loss in iteration 262 : 0.4626266207253682
Loss in iteration 263 : 0.46291258960663717
Loss in iteration 264 : 0.4759295899833466
Loss in iteration 265 : 0.4525509883532408
Loss in iteration 266 : 0.4719714615180678
Loss in iteration 267 : 0.4688962224213069
Loss in iteration 268 : 0.47398561815716767
Loss in iteration 269 : 0.4566370767523957
Loss in iteration 270 : 0.46195364374022546
Loss in iteration 271 : 0.4497349885636099
Loss in iteration 272 : 0.4622226824611095
Loss in iteration 273 : 0.4481032729313097
Loss in iteration 274 : 0.45736718689292677
Loss in iteration 275 : 0.46670173260840275
Loss in iteration 276 : 0.4696831056332188
Loss in iteration 277 : 0.4553258106254194
Loss in iteration 278 : 0.4532914315682675
Loss in iteration 279 : 0.451423851007134
Loss in iteration 280 : 0.45763132553131497
Loss in iteration 281 : 0.46486168603712846
Loss in iteration 282 : 0.45880090013201635
Loss in iteration 283 : 0.4645692288678391
Loss in iteration 284 : 0.4497816740562863
Loss in iteration 285 : 0.45131565136784935
Loss in iteration 286 : 0.46163986181492617
Loss in iteration 287 : 0.46658643785442555
Loss in iteration 288 : 0.46542589605168255
Loss in iteration 289 : 0.4575815192734041
Loss in iteration 290 : 0.453058758661142
Loss in iteration 291 : 0.46653397837044824
Loss in iteration 292 : 0.4635046091399275
Loss in iteration 293 : 0.4577993077296457
Loss in iteration 294 : 0.46780960314706366
Loss in iteration 295 : 0.45400167584422896
Loss in iteration 296 : 0.45711397585821006
Loss in iteration 297 : 0.46598492738467623
Loss in iteration 298 : 0.4620342650026553
Loss in iteration 299 : 0.4638561427725975
Loss in iteration 300 : 0.46691569852877357
Loss in iteration 301 : 0.465431418421328
Loss in iteration 302 : 0.45923234787794226
Loss in iteration 303 : 0.4540357448973967
Loss in iteration 304 : 0.45672722788915565
Loss in iteration 305 : 0.45953196419374387
Loss in iteration 306 : 0.45994284661626694
Loss in iteration 307 : 0.4609698556543353
Loss in iteration 308 : 0.46215016088209815
Loss in iteration 309 : 0.45294806097539525
Loss in iteration 310 : 0.45930227935461393
Loss in iteration 311 : 0.468693321847266
Loss in iteration 312 : 0.47931552316297255
Loss in iteration 313 : 0.4495082837010647
Loss in iteration 314 : 0.4568138649566402
Loss in iteration 315 : 0.4600889326546778
Loss in iteration 316 : 0.47739932903783366
Loss in iteration 317 : 0.46169698348502364
Loss in iteration 318 : 0.472301139620606
Loss in iteration 319 : 0.4567941206395008
Loss in iteration 320 : 0.45552407678520457
Loss in iteration 321 : 0.4556181182456986
Loss in iteration 322 : 0.4601225927367993
Loss in iteration 323 : 0.45803797325875784
Loss in iteration 324 : 0.4597039876477581
Loss in iteration 325 : 0.45841946546560874
Loss in iteration 326 : 0.4741127106357034
Loss in iteration 327 : 0.4528705216054879
Loss in iteration 328 : 0.454914281241231
Loss in iteration 329 : 0.4571113392256587
Loss in iteration 330 : 0.45243619484655934
Loss in iteration 331 : 0.4615061776652285
Loss in iteration 332 : 0.4481936170548129
Loss in iteration 333 : 0.4491782597326278
Loss in iteration 334 : 0.4593446312909333
Loss in iteration 335 : 0.45337259809358377
Loss in iteration 336 : 0.4603504076351362
Loss in iteration 337 : 0.46418657021809767
Loss in iteration 338 : 0.46901010604996635
Loss in iteration 339 : 0.47255215296905656
Loss in iteration 340 : 0.4661394722523614
Loss in iteration 341 : 0.4582020204705318
Loss in iteration 342 : 0.460032322737848
Loss in iteration 343 : 0.459459301177295
Loss in iteration 344 : 0.466215179273989
Loss in iteration 345 : 0.45950209016275373
Loss in iteration 346 : 0.4540644422602835
Loss in iteration 347 : 0.4609195660500637
Loss in iteration 348 : 0.46259698797973964
Loss in iteration 349 : 0.4674116027919022
Loss in iteration 350 : 0.46739842319128316
Loss in iteration 351 : 0.4620760693950833
Loss in iteration 352 : 0.45874298342417824
Loss in iteration 353 : 0.4535149031594662
Loss in iteration 354 : 0.46080716926821813
Loss in iteration 355 : 0.46081463363619546
Loss in iteration 356 : 0.45421162954341215
Loss in iteration 357 : 0.45865334109913153
Loss in iteration 358 : 0.46715010331531
Loss in iteration 359 : 0.45819999511111104
Loss in iteration 360 : 0.4659391962947635
Loss in iteration 361 : 0.4513292685548055
Loss in iteration 362 : 0.47092635676085787
Loss in iteration 363 : 0.4621895538584967
Loss in iteration 364 : 0.46002848727919515
Loss in iteration 365 : 0.463427738996083
Loss in iteration 366 : 0.45796324450811965
Loss in iteration 367 : 0.4671864470413908
Loss in iteration 368 : 0.46828666735625163
Loss in iteration 369 : 0.47282053714429095
Loss in iteration 370 : 0.4575456962295426
Loss in iteration 371 : 0.47514719284829915
Loss in iteration 372 : 0.45480917808972765
Loss in iteration 373 : 0.4610547363293135
Loss in iteration 374 : 0.4591136439015457
Loss in iteration 375 : 0.4572023175621295
Loss in iteration 376 : 0.46641072467269196
Loss in iteration 377 : 0.4612446590536556
Loss in iteration 378 : 0.46453815249832275
Loss in iteration 379 : 0.46384025574405013
Loss in iteration 380 : 0.4585901213774593
Loss in iteration 381 : 0.4497987506765766
Loss in iteration 382 : 0.4559552785070882
Loss in iteration 383 : 0.46305064012445485
Loss in iteration 384 : 0.46847004759601
Loss in iteration 385 : 0.4618750676664438
Loss in iteration 386 : 0.44942849004542496
Loss in iteration 387 : 0.46656560497563443
Loss in iteration 388 : 0.47044629066872007
Loss in iteration 389 : 0.45687703938791135
Loss in iteration 390 : 0.4587588053069487
Loss in iteration 391 : 0.4618446273503998
Loss in iteration 392 : 0.45114168703066243
Loss in iteration 393 : 0.4545000070312793
Loss in iteration 394 : 0.4584608400635064
Loss in iteration 395 : 0.4663412850485453
Loss in iteration 396 : 0.45976071586503103
Loss in iteration 397 : 0.4727821526468062
Loss in iteration 398 : 0.4690228298100817
Loss in iteration 399 : 0.45750586012600436
Loss in iteration 400 : 0.45160751839307445
Testing accuracy  of updater 8 on alg 0 with rate 0.08000000000000002 = 0.787, training accuracy 0.787, time elapsed: 5122 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6888571697935494
Loss in iteration 3 : 0.6823488228058612
Loss in iteration 4 : 0.6793396833704153
Loss in iteration 5 : 0.6729828262732345
Loss in iteration 6 : 0.6711154569332838
Loss in iteration 7 : 0.6643024872159731
Loss in iteration 8 : 0.6580346446261411
Loss in iteration 9 : 0.6549667492700768
Loss in iteration 10 : 0.6514039737282761
Loss in iteration 11 : 0.6416269418880293
Loss in iteration 12 : 0.6383842450121048
Loss in iteration 13 : 0.6341079996905313
Loss in iteration 14 : 0.6278905532402121
Loss in iteration 15 : 0.6239911372970065
Loss in iteration 16 : 0.6168363784215984
Loss in iteration 17 : 0.6116249827463348
Loss in iteration 18 : 0.6075219677308489
Loss in iteration 19 : 0.604734885623445
Loss in iteration 20 : 0.5996361155893464
Loss in iteration 21 : 0.5978850711542376
Loss in iteration 22 : 0.5937279504680467
Loss in iteration 23 : 0.5910253119656206
Loss in iteration 24 : 0.5823423547351665
Loss in iteration 25 : 0.5823832612288331
Loss in iteration 26 : 0.574528050151279
Loss in iteration 27 : 0.5723926932663358
Loss in iteration 28 : 0.5717825793523992
Loss in iteration 29 : 0.5725879592202642
Loss in iteration 30 : 0.5702805303227065
Loss in iteration 31 : 0.5655111032990094
Loss in iteration 32 : 0.5634257449398459
Loss in iteration 33 : 0.5582537908848398
Loss in iteration 34 : 0.555752657931553
Loss in iteration 35 : 0.5570664294517399
Loss in iteration 36 : 0.5532492275728926
Loss in iteration 37 : 0.549726920325162
Loss in iteration 38 : 0.5467899031886858
Loss in iteration 39 : 0.5442417179891402
Loss in iteration 40 : 0.5442070301197787
Loss in iteration 41 : 0.544179723992858
Loss in iteration 42 : 0.5374157010201724
Loss in iteration 43 : 0.5414772484988056
Loss in iteration 44 : 0.5389576964293568
Loss in iteration 45 : 0.5374937652175594
Loss in iteration 46 : 0.5364544947178759
Loss in iteration 47 : 0.5334516276670633
Loss in iteration 48 : 0.5355801266225346
Loss in iteration 49 : 0.5296482810588834
Loss in iteration 50 : 0.526083600284321
Loss in iteration 51 : 0.5326076899409149
Loss in iteration 52 : 0.525434465166019
Loss in iteration 53 : 0.5244481397791563
Loss in iteration 54 : 0.5204388198583628
Loss in iteration 55 : 0.5217496572163594
Loss in iteration 56 : 0.524769059233834
Loss in iteration 57 : 0.5142166903514009
Loss in iteration 58 : 0.5159514115344337
Loss in iteration 59 : 0.5162403545058241
Loss in iteration 60 : 0.5181666555159171
Loss in iteration 61 : 0.5189302526054391
Loss in iteration 62 : 0.5170330770616653
Loss in iteration 63 : 0.5148896405651403
Loss in iteration 64 : 0.5137186893824225
Loss in iteration 65 : 0.5192221618493998
Loss in iteration 66 : 0.5091576818464955
Loss in iteration 67 : 0.5119363927489243
Loss in iteration 68 : 0.5069762999152964
Loss in iteration 69 : 0.5145699593058545
Loss in iteration 70 : 0.5077758995671927
Loss in iteration 71 : 0.518863451973013
Loss in iteration 72 : 0.5076735341903751
Loss in iteration 73 : 0.5108109456982537
Loss in iteration 74 : 0.508290034610549
Loss in iteration 75 : 0.5076426169296557
Loss in iteration 76 : 0.5128058620055354
Loss in iteration 77 : 0.5046447632030515
Loss in iteration 78 : 0.5081481053236818
Loss in iteration 79 : 0.49506144490873105
Loss in iteration 80 : 0.504195088671065
Loss in iteration 81 : 0.5073173362597941
Loss in iteration 82 : 0.5026041526273994
Loss in iteration 83 : 0.5070087085879644
Loss in iteration 84 : 0.5095981437145605
Loss in iteration 85 : 0.5046772030073566
Loss in iteration 86 : 0.5085455390741731
Loss in iteration 87 : 0.4954235291219799
Loss in iteration 88 : 0.5097182589471596
Loss in iteration 89 : 0.49708937303024686
Loss in iteration 90 : 0.500235495653001
Loss in iteration 91 : 0.5054616305358987
Loss in iteration 92 : 0.4959094354450675
Loss in iteration 93 : 0.4982660405609895
Loss in iteration 94 : 0.4997965858077654
Loss in iteration 95 : 0.4950739510715639
Loss in iteration 96 : 0.5058656264082556
Loss in iteration 97 : 0.49717080808968755
Loss in iteration 98 : 0.499595754763381
Loss in iteration 99 : 0.49762606120264297
Loss in iteration 100 : 0.5014292028602865
Loss in iteration 101 : 0.49440468981560354
Loss in iteration 102 : 0.49401373716290875
Loss in iteration 103 : 0.5007152383353053
Loss in iteration 104 : 0.49673010232128384
Loss in iteration 105 : 0.48801154351172565
Loss in iteration 106 : 0.4946923660288879
Loss in iteration 107 : 0.5012148088622739
Loss in iteration 108 : 0.5060391599985994
Loss in iteration 109 : 0.49439195565311117
Loss in iteration 110 : 0.4900003855911968
Loss in iteration 111 : 0.49702793745638973
Loss in iteration 112 : 0.4911909490445268
Loss in iteration 113 : 0.48817310142214465
Loss in iteration 114 : 0.48738042650241026
Loss in iteration 115 : 0.4867946641532442
Loss in iteration 116 : 0.49195217963439747
Loss in iteration 117 : 0.49920897609717807
Loss in iteration 118 : 0.4978281020091506
Loss in iteration 119 : 0.48620071930474357
Loss in iteration 120 : 0.4977617521625328
Loss in iteration 121 : 0.4962714745742485
Loss in iteration 122 : 0.49761061392771244
Loss in iteration 123 : 0.49173925424572107
Loss in iteration 124 : 0.487154749264572
Loss in iteration 125 : 0.48747204362453234
Loss in iteration 126 : 0.48580545594103824
Loss in iteration 127 : 0.48762000664544397
Loss in iteration 128 : 0.4909418098829907
Loss in iteration 129 : 0.4862198429521343
Loss in iteration 130 : 0.49051638682034754
Loss in iteration 131 : 0.4867213478162203
Loss in iteration 132 : 0.4866798646404615
Loss in iteration 133 : 0.4896730934813956
Loss in iteration 134 : 0.49033123232547876
Loss in iteration 135 : 0.4879972161235747
Loss in iteration 136 : 0.48797354974116985
Loss in iteration 137 : 0.48886661585650165
Loss in iteration 138 : 0.4906943158382677
Loss in iteration 139 : 0.494074147588528
Loss in iteration 140 : 0.4900498685803926
Loss in iteration 141 : 0.4908294667767213
Loss in iteration 142 : 0.4845558881648541
Loss in iteration 143 : 0.4864406950525957
Loss in iteration 144 : 0.48415154501667634
Loss in iteration 145 : 0.47868106099369884
Loss in iteration 146 : 0.48906022588393944
Loss in iteration 147 : 0.4931413050216321
Loss in iteration 148 : 0.4854983972121415
Loss in iteration 149 : 0.49505679556626087
Loss in iteration 150 : 0.48984135826094405
Loss in iteration 151 : 0.4840999863137959
Loss in iteration 152 : 0.4915684595378211
Loss in iteration 153 : 0.47457169602422844
Loss in iteration 154 : 0.4912796555553244
Loss in iteration 155 : 0.4885029973125635
Loss in iteration 156 : 0.4754185524603626
Loss in iteration 157 : 0.48683681650208627
Loss in iteration 158 : 0.47848834142665253
Loss in iteration 159 : 0.4815709541606526
Loss in iteration 160 : 0.4800466773955426
Loss in iteration 161 : 0.4819714996271406
Loss in iteration 162 : 0.4824589889008531
Loss in iteration 163 : 0.48615838237448533
Loss in iteration 164 : 0.48564515640468436
Loss in iteration 165 : 0.4790613020897471
Loss in iteration 166 : 0.4813242697065606
Loss in iteration 167 : 0.49042720931794986
Loss in iteration 168 : 0.48885526301951987
Loss in iteration 169 : 0.4828762705272394
Loss in iteration 170 : 0.4871462514990858
Loss in iteration 171 : 0.4777982253768837
Loss in iteration 172 : 0.48381538928382734
Loss in iteration 173 : 0.4817661987000836
Loss in iteration 174 : 0.4768846319598998
Loss in iteration 175 : 0.4895238545145757
Loss in iteration 176 : 0.47777411633512307
Loss in iteration 177 : 0.48772751105079504
Loss in iteration 178 : 0.47804847165513414
Loss in iteration 179 : 0.486170463486012
Loss in iteration 180 : 0.47632741806662054
Loss in iteration 181 : 0.4823910803838316
Loss in iteration 182 : 0.48025284425810577
Loss in iteration 183 : 0.47904708603037727
Loss in iteration 184 : 0.4910052765549181
Loss in iteration 185 : 0.479688629656895
Loss in iteration 186 : 0.473833608519994
Loss in iteration 187 : 0.47503615179111514
Loss in iteration 188 : 0.4838993061406338
Loss in iteration 189 : 0.48491523161853517
Loss in iteration 190 : 0.473683201905224
Loss in iteration 191 : 0.4751903683696102
Loss in iteration 192 : 0.4774419372724246
Loss in iteration 193 : 0.4801607180099517
Loss in iteration 194 : 0.48051769345679146
Loss in iteration 195 : 0.4867014114873521
Loss in iteration 196 : 0.49198292182364634
Loss in iteration 197 : 0.47124830945915097
Loss in iteration 198 : 0.48776947880349936
Loss in iteration 199 : 0.47832339710054245
Loss in iteration 200 : 0.4725138888349277
Loss in iteration 201 : 0.4797731493705479
Loss in iteration 202 : 0.4840939919182044
Loss in iteration 203 : 0.4703762861928946
Loss in iteration 204 : 0.4740850877947251
Loss in iteration 205 : 0.47886836788299725
Loss in iteration 206 : 0.4770487882332346
Loss in iteration 207 : 0.4754292429174923
Loss in iteration 208 : 0.470309013838461
Loss in iteration 209 : 0.48203133372566703
Loss in iteration 210 : 0.4859300271543774
Loss in iteration 211 : 0.4804825576980481
Loss in iteration 212 : 0.48417762276160015
Loss in iteration 213 : 0.4840120761423171
Loss in iteration 214 : 0.4950268578229002
Loss in iteration 215 : 0.48529012695037194
Loss in iteration 216 : 0.4789422688406975
Loss in iteration 217 : 0.476178187144952
Loss in iteration 218 : 0.4719939381040102
Loss in iteration 219 : 0.4818057139307727
Loss in iteration 220 : 0.48063790429476805
Loss in iteration 221 : 0.4799599085891068
Loss in iteration 222 : 0.46973706091580786
Loss in iteration 223 : 0.483658026403182
Loss in iteration 224 : 0.48136049971631983
Loss in iteration 225 : 0.4756294873696714
Loss in iteration 226 : 0.4780841408604254
Loss in iteration 227 : 0.4694820555978767
Loss in iteration 228 : 0.4779106544450481
Loss in iteration 229 : 0.47891497029256086
Loss in iteration 230 : 0.4701840319979043
Loss in iteration 231 : 0.47851333021482545
Loss in iteration 232 : 0.4788811994858513
Loss in iteration 233 : 0.4748844672228741
Loss in iteration 234 : 0.48088015708886656
Loss in iteration 235 : 0.4702532809240698
Loss in iteration 236 : 0.475528750146726
Loss in iteration 237 : 0.477247017259067
Loss in iteration 238 : 0.4752095707499316
Loss in iteration 239 : 0.47428777266427646
Loss in iteration 240 : 0.47086867164010165
Loss in iteration 241 : 0.47103281686317766
Loss in iteration 242 : 0.4950899735770154
Loss in iteration 243 : 0.48388224504859095
Loss in iteration 244 : 0.47445051108168645
Loss in iteration 245 : 0.48628132577729677
Loss in iteration 246 : 0.4739766425901711
Loss in iteration 247 : 0.46900880516894783
Loss in iteration 248 : 0.4760792312557173
Loss in iteration 249 : 0.47899485003352443
Loss in iteration 250 : 0.46980031660067473
Loss in iteration 251 : 0.4798902125694341
Loss in iteration 252 : 0.4730192561127459
Loss in iteration 253 : 0.4826054441432949
Loss in iteration 254 : 0.4627116036244801
Loss in iteration 255 : 0.46848659685201743
Loss in iteration 256 : 0.4862893437943795
Loss in iteration 257 : 0.46718050762051905
Loss in iteration 258 : 0.4705221905903836
Loss in iteration 259 : 0.469217773950077
Loss in iteration 260 : 0.47408608240871586
Loss in iteration 261 : 0.46382936310504075
Loss in iteration 262 : 0.47180530856175174
Loss in iteration 263 : 0.47428861878213896
Loss in iteration 264 : 0.48633283128748206
Loss in iteration 265 : 0.46670554002967113
Loss in iteration 266 : 0.48174615414642663
Loss in iteration 267 : 0.4783690749013328
Loss in iteration 268 : 0.4828783997032968
Loss in iteration 269 : 0.46921063412591796
Loss in iteration 270 : 0.47426369731757595
Loss in iteration 271 : 0.46333605659947186
Loss in iteration 272 : 0.47450930664528856
Loss in iteration 273 : 0.46021130413390365
Loss in iteration 274 : 0.46817678897351983
Loss in iteration 275 : 0.4753246091049959
Loss in iteration 276 : 0.47817122342588764
Loss in iteration 277 : 0.4668659768517152
Loss in iteration 278 : 0.4659748125079488
Loss in iteration 279 : 0.46424870154689735
Loss in iteration 280 : 0.46875375811213
Loss in iteration 281 : 0.47688368633577555
Loss in iteration 282 : 0.46961990446333784
Loss in iteration 283 : 0.4758494596296482
Loss in iteration 284 : 0.4612000614001525
Loss in iteration 285 : 0.4645387416199501
Loss in iteration 286 : 0.4718052037401888
Loss in iteration 287 : 0.47459747318230955
Loss in iteration 288 : 0.47486422776813036
Loss in iteration 289 : 0.4683675243581777
Loss in iteration 290 : 0.4638369143907266
Loss in iteration 291 : 0.4752021843889351
Loss in iteration 292 : 0.4738136365892606
Loss in iteration 293 : 0.46743453490521725
Loss in iteration 294 : 0.47705979580521707
Loss in iteration 295 : 0.46461035820966534
Loss in iteration 296 : 0.4671609111074167
Loss in iteration 297 : 0.4742847419726994
Loss in iteration 298 : 0.47292755359834243
Loss in iteration 299 : 0.47293693027240474
Loss in iteration 300 : 0.4765704181222325
Loss in iteration 301 : 0.47521504327682323
Loss in iteration 302 : 0.47046824105922525
Loss in iteration 303 : 0.46460193870565236
Loss in iteration 304 : 0.46787401037593823
Loss in iteration 305 : 0.46734968503334734
Loss in iteration 306 : 0.4684359437312365
Loss in iteration 307 : 0.4710133384395609
Loss in iteration 308 : 0.4728848407569486
Loss in iteration 309 : 0.4644326734992721
Loss in iteration 310 : 0.4693007974277419
Loss in iteration 311 : 0.4776657429074591
Loss in iteration 312 : 0.48539458691228626
Loss in iteration 313 : 0.45965421492129455
Loss in iteration 314 : 0.4661685258587089
Loss in iteration 315 : 0.46761988834095775
Loss in iteration 316 : 0.4835122406576129
Loss in iteration 317 : 0.4684190887203407
Loss in iteration 318 : 0.4803433369361862
Loss in iteration 319 : 0.4661142425191465
Loss in iteration 320 : 0.4646017042765016
Loss in iteration 321 : 0.46643085384991295
Loss in iteration 322 : 0.46783389765368555
Loss in iteration 323 : 0.4671762309843429
Loss in iteration 324 : 0.4683903795250831
Loss in iteration 325 : 0.4673346189391292
Loss in iteration 326 : 0.4811807633643436
Loss in iteration 327 : 0.46199077264585586
Loss in iteration 328 : 0.46540598389120375
Loss in iteration 329 : 0.46593829282605814
Loss in iteration 330 : 0.46113589552503903
Loss in iteration 331 : 0.4690504756797738
Loss in iteration 332 : 0.45734053541725117
Loss in iteration 333 : 0.45949893797584274
Loss in iteration 334 : 0.46934005398561673
Loss in iteration 335 : 0.4615836330057136
Loss in iteration 336 : 0.46867074182826457
Loss in iteration 337 : 0.4710407792799216
Loss in iteration 338 : 0.4769810795717351
Loss in iteration 339 : 0.47953615451765724
Loss in iteration 340 : 0.4736905074623227
Loss in iteration 341 : 0.4666115803784371
Loss in iteration 342 : 0.4686661900095049
Loss in iteration 343 : 0.46811787281777634
Loss in iteration 344 : 0.4737761518296466
Loss in iteration 345 : 0.4687174901626756
Loss in iteration 346 : 0.4637078507268535
Loss in iteration 347 : 0.4704179301077776
Loss in iteration 348 : 0.47089499705029386
Loss in iteration 349 : 0.47521876207087393
Loss in iteration 350 : 0.47332645980951565
Loss in iteration 351 : 0.4693426075509896
Loss in iteration 352 : 0.4668134169750673
Loss in iteration 353 : 0.4628089574432486
Loss in iteration 354 : 0.4688787385141061
Loss in iteration 355 : 0.4684010799771694
Loss in iteration 356 : 0.46161070049021724
Loss in iteration 357 : 0.46690262292919055
Loss in iteration 358 : 0.4730669548629995
Loss in iteration 359 : 0.466968902948243
Loss in iteration 360 : 0.4733823026721099
Loss in iteration 361 : 0.4607256822547808
Loss in iteration 362 : 0.4773504252853379
Loss in iteration 363 : 0.46895909975760997
Loss in iteration 364 : 0.469081614826415
Loss in iteration 365 : 0.4709391898960174
Loss in iteration 366 : 0.4661378942479377
Loss in iteration 367 : 0.4732253877778245
Loss in iteration 368 : 0.474979571381837
Loss in iteration 369 : 0.4801881905795615
Loss in iteration 370 : 0.4654041189519778
Loss in iteration 371 : 0.4822653527782752
Loss in iteration 372 : 0.4613870845632864
Loss in iteration 373 : 0.4687747170200756
Loss in iteration 374 : 0.46644685457738505
Loss in iteration 375 : 0.4657571969664141
Loss in iteration 376 : 0.47500467303581206
Loss in iteration 377 : 0.4686228857598624
Loss in iteration 378 : 0.47182226979505504
Loss in iteration 379 : 0.4701724004296247
Loss in iteration 380 : 0.46559181512526826
Loss in iteration 381 : 0.45882548234256276
Loss in iteration 382 : 0.46391544248770045
Loss in iteration 383 : 0.46892630731067675
Loss in iteration 384 : 0.47463155832023934
Loss in iteration 385 : 0.4689802791920826
Loss in iteration 386 : 0.45818985351796165
Loss in iteration 387 : 0.47190470811509305
Loss in iteration 388 : 0.47654331908661235
Loss in iteration 389 : 0.4630304161799299
Loss in iteration 390 : 0.4662420248021938
Loss in iteration 391 : 0.4689365615438604
Loss in iteration 392 : 0.4589175090619634
Loss in iteration 393 : 0.4606099068810837
Loss in iteration 394 : 0.46582902053514785
Loss in iteration 395 : 0.4725779292363639
Loss in iteration 396 : 0.46687081469507286
Loss in iteration 397 : 0.4782548214521962
Loss in iteration 398 : 0.4755319926222727
Loss in iteration 399 : 0.46343244087546487
Loss in iteration 400 : 0.4599465945850406
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.7865, training accuracy 0.7865, time elapsed: 4918 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 19.014320314906367
Loss in iteration 3 : 8.794385484388371
Loss in iteration 4 : 6.177784866811049
Loss in iteration 5 : 7.099938855169081
Loss in iteration 6 : 5.0124405031015655
Loss in iteration 7 : 6.010940356093322
Loss in iteration 8 : 4.562731890303383
Loss in iteration 9 : 4.334357913784713
Loss in iteration 10 : 5.39853556069984
Loss in iteration 11 : 2.2261633542961152
Loss in iteration 12 : 4.382575615082878
Loss in iteration 13 : 3.62154052771387
Loss in iteration 14 : 2.9185633804357662
Loss in iteration 15 : 3.633421592282181
Loss in iteration 16 : 2.7882976501610695
Loss in iteration 17 : 3.3787398651127316
Loss in iteration 18 : 3.419904730189457
Loss in iteration 19 : 2.218186208064888
Loss in iteration 20 : 3.2169010447595183
Loss in iteration 21 : 2.9250545119650977
Loss in iteration 22 : 2.3584320323430776
Loss in iteration 23 : 2.9866333099614857
Loss in iteration 24 : 2.3422713416816148
Loss in iteration 25 : 2.413073082778957
Loss in iteration 26 : 2.188029373943801
Loss in iteration 27 : 1.8623725846402828
Loss in iteration 28 : 2.3358108214437614
Loss in iteration 29 : 2.0143588467753157
Loss in iteration 30 : 1.8980270116968334
Loss in iteration 31 : 1.9460537422727677
Loss in iteration 32 : 1.6189273244850035
Loss in iteration 33 : 1.763364818619143
Loss in iteration 34 : 1.4208432079489324
Loss in iteration 35 : 1.680629861883661
Loss in iteration 36 : 1.3486583942463162
Loss in iteration 37 : 1.530895693225235
Loss in iteration 38 : 1.3004468180711288
Loss in iteration 39 : 1.296389041441093
Loss in iteration 40 : 1.1971510558944292
Loss in iteration 41 : 1.2789001888167644
Loss in iteration 42 : 1.1839120601460495
Loss in iteration 43 : 1.0639724817799228
Loss in iteration 44 : 1.2298895274016346
Loss in iteration 45 : 1.084018429903856
Loss in iteration 46 : 1.0548023555483805
Loss in iteration 47 : 1.1934758644771466
Loss in iteration 48 : 1.1559413414417112
Loss in iteration 49 : 0.9517773170231747
Loss in iteration 50 : 0.9172763643954114
Loss in iteration 51 : 1.1583614268254272
Loss in iteration 52 : 1.0880631728981132
Loss in iteration 53 : 0.7793087336494572
Loss in iteration 54 : 0.8660979765745052
Loss in iteration 55 : 1.2301816978757174
Loss in iteration 56 : 1.529015993124039
Loss in iteration 57 : 0.9403662911417547
Loss in iteration 58 : 0.6854501476254737
Loss in iteration 59 : 0.7110385465785479
Loss in iteration 60 : 0.9377887023674905
Loss in iteration 61 : 0.9290975076439697
Loss in iteration 62 : 0.764055654087389
Loss in iteration 63 : 0.6779553364621004
Loss in iteration 64 : 0.6737354567553517
Loss in iteration 65 : 0.77273497955317
Loss in iteration 66 : 0.8898266650154526
Loss in iteration 67 : 1.0780084626484963
Loss in iteration 68 : 1.0126035423701412
Loss in iteration 69 : 0.8439397290347344
Loss in iteration 70 : 0.7022335495130042
Loss in iteration 71 : 0.6930110796617688
Loss in iteration 72 : 0.617557879188285
Loss in iteration 73 : 0.6779384069460835
Loss in iteration 74 : 0.6879953840514172
Loss in iteration 75 : 1.0104406266905523
Loss in iteration 76 : 1.7763026876078034
Loss in iteration 77 : 1.1892648779077295
Loss in iteration 78 : 1.0663664444859668
Loss in iteration 79 : 0.5347061437939419
Loss in iteration 80 : 0.8289234754159717
Loss in iteration 81 : 0.9061945680511544
Loss in iteration 82 : 1.0779061745736078
Loss in iteration 83 : 0.852117166147875
Loss in iteration 84 : 0.6793285819202693
Loss in iteration 85 : 0.9405869787653307
Loss in iteration 86 : 0.6941563730891708
Loss in iteration 87 : 0.7047642964060996
Loss in iteration 88 : 0.7272421273829331
Loss in iteration 89 : 0.6081761140581037
Loss in iteration 90 : 0.7737127424459903
Loss in iteration 91 : 0.5872376424884465
Loss in iteration 92 : 0.6793131076318261
Loss in iteration 93 : 0.6145955710945078
Loss in iteration 94 : 0.7172451156561754
Loss in iteration 95 : 0.8092672803661097
Loss in iteration 96 : 0.9778869345869385
Loss in iteration 97 : 1.045096317396596
Loss in iteration 98 : 1.2914349620356032
Loss in iteration 99 : 0.6398314231556751
Loss in iteration 100 : 0.7122983476105804
Loss in iteration 101 : 0.6877725365316315
Loss in iteration 102 : 0.7632639407332183
Loss in iteration 103 : 0.9820883463584568
Loss in iteration 104 : 0.5895441613194766
Loss in iteration 105 : 0.6737289373786891
Loss in iteration 106 : 0.6102810761392933
Loss in iteration 107 : 0.5836269863678633
Loss in iteration 108 : 0.7184610524107974
Loss in iteration 109 : 0.789192392565728
Loss in iteration 110 : 1.2499643348945209
Loss in iteration 111 : 0.8901621699422726
Loss in iteration 112 : 0.614881973866812
Loss in iteration 113 : 0.6217963302778091
Loss in iteration 114 : 0.9258513926568559
Loss in iteration 115 : 0.8213148605380843
Loss in iteration 116 : 0.5937973255446841
Loss in iteration 117 : 0.7262033315702652
Loss in iteration 118 : 0.8280904494650385
Loss in iteration 119 : 0.7302482355314562
Loss in iteration 120 : 0.6076576348264313
Loss in iteration 121 : 0.5472228498782117
Loss in iteration 122 : 0.6373918665869119
Loss in iteration 123 : 0.654748269093846
Loss in iteration 124 : 0.8443806862785399
Loss in iteration 125 : 0.8936482829150217
Loss in iteration 126 : 0.7010439946906101
Loss in iteration 127 : 0.6205228701914288
Loss in iteration 128 : 0.5199602840598729
Loss in iteration 129 : 0.6341321799602156
Loss in iteration 130 : 0.6664391078289745
Loss in iteration 131 : 0.6233214257888666
Loss in iteration 132 : 0.6304098957470813
Loss in iteration 133 : 0.4988091174839243
Loss in iteration 134 : 0.5902587246013404
Loss in iteration 135 : 0.5072592181302191
Loss in iteration 136 : 0.5459803075429847
Loss in iteration 137 : 0.5412980010699308
Loss in iteration 138 : 0.5151988080458224
Loss in iteration 139 : 0.6262402965368175
Loss in iteration 140 : 0.8038573142713797
Loss in iteration 141 : 1.2230115036750249
Loss in iteration 142 : 1.2723456465583636
Loss in iteration 143 : 0.5512917012456585
Loss in iteration 144 : 0.7780434175258353
Loss in iteration 145 : 1.236019295663978
Loss in iteration 146 : 0.7262500399439662
Loss in iteration 147 : 0.6104625942648595
Loss in iteration 148 : 0.7669324304236987
Loss in iteration 149 : 0.7467966771550587
Loss in iteration 150 : 0.641741490805615
Loss in iteration 151 : 0.5823369674050612
Loss in iteration 152 : 0.6435867449618485
Loss in iteration 153 : 0.7352834918801627
Loss in iteration 154 : 0.5619747927978629
Loss in iteration 155 : 0.589056643988142
Loss in iteration 156 : 0.5504553183849731
Loss in iteration 157 : 0.5837189312481088
Loss in iteration 158 : 0.6377348524253645
Loss in iteration 159 : 0.6061824454975624
Loss in iteration 160 : 0.5289464022737128
Loss in iteration 161 : 0.5328268038275402
Loss in iteration 162 : 0.5734443732901611
Loss in iteration 163 : 0.7194127104481695
Loss in iteration 164 : 0.7951350370276288
Loss in iteration 165 : 0.7904318451804891
Loss in iteration 166 : 0.616794435760804
Loss in iteration 167 : 0.61494090311286
Loss in iteration 168 : 0.5642299785494311
Loss in iteration 169 : 0.5790944881345818
Loss in iteration 170 : 0.6055768143611667
Loss in iteration 171 : 0.5815545214140985
Loss in iteration 172 : 0.6540958270746048
Loss in iteration 173 : 0.6358054325213908
Loss in iteration 174 : 0.630707082717609
Loss in iteration 175 : 0.6309504340400693
Loss in iteration 176 : 0.540065920659142
Loss in iteration 177 : 0.5627915800011435
Loss in iteration 178 : 0.4990935470038405
Loss in iteration 179 : 0.522588737362811
Loss in iteration 180 : 0.5007544379000107
Loss in iteration 181 : 0.5004587302195553
Loss in iteration 182 : 0.5518727580288268
Loss in iteration 183 : 0.5979738075299261
Loss in iteration 184 : 0.8181990609271883
Loss in iteration 185 : 0.8305754935515225
Loss in iteration 186 : 0.7701169845701032
Loss in iteration 187 : 0.5591063142845961
Loss in iteration 188 : 0.5168021155119574
Loss in iteration 189 : 0.5572649517742972
Loss in iteration 190 : 0.6541443307433649
Loss in iteration 191 : 0.6887549140723002
Loss in iteration 192 : 0.529801335753772
Loss in iteration 193 : 0.5271767782801136
Loss in iteration 194 : 0.5426904998511429
Loss in iteration 195 : 0.6972886627790906
Loss in iteration 196 : 0.8310657115023379
Loss in iteration 197 : 0.6496611584431905
Loss in iteration 198 : 0.5937618624109833
Loss in iteration 199 : 0.4927949817454759
Loss in iteration 200 : 0.577495441466691
Loss in iteration 201 : 0.6334675440244405
Loss in iteration 202 : 0.6007973633321774
Loss in iteration 203 : 0.5287690163597457
Loss in iteration 204 : 0.5005421070755641
Loss in iteration 205 : 0.6105394493819526
Loss in iteration 206 : 0.7174282392031904
Loss in iteration 207 : 0.6767854837233573
Loss in iteration 208 : 0.6355030360219165
Loss in iteration 209 : 0.5108666354358178
Loss in iteration 210 : 0.5611132901107728
Loss in iteration 211 : 0.4886515240900173
Loss in iteration 212 : 0.5632875589444614
Loss in iteration 213 : 0.5737825636505723
Loss in iteration 214 : 0.6020519460986291
Loss in iteration 215 : 0.6935601092756245
Loss in iteration 216 : 0.531863276307453
Loss in iteration 217 : 0.616011298665544
Loss in iteration 218 : 0.5231633452054391
Loss in iteration 219 : 0.598240061678834
Loss in iteration 220 : 0.6000524947477966
Loss in iteration 221 : 0.5657077598375069
Loss in iteration 222 : 0.6152442428498539
Loss in iteration 223 : 0.5839090809703651
Loss in iteration 224 : 0.5376298259492732
Loss in iteration 225 : 0.49614903319838166
Loss in iteration 226 : 0.4839257992111451
Loss in iteration 227 : 0.4975389766341545
Loss in iteration 228 : 0.48803437688575496
Loss in iteration 229 : 0.5001343574394259
Loss in iteration 230 : 0.478985556163559
Loss in iteration 231 : 0.4918624776797839
Loss in iteration 232 : 0.5354468486923478
Loss in iteration 233 : 0.5874010269815282
Loss in iteration 234 : 0.8405903082084875
Loss in iteration 235 : 0.8331879703493776
Loss in iteration 236 : 0.7128510766042264
Loss in iteration 237 : 0.514756496692687
Loss in iteration 238 : 0.520170714111959
Loss in iteration 239 : 0.7938760399859603
Loss in iteration 240 : 0.77273093578878
Loss in iteration 241 : 0.5154347976540237
Loss in iteration 242 : 0.5616844585007895
Loss in iteration 243 : 0.7777626545699989
Loss in iteration 244 : 0.7819397846944485
Loss in iteration 245 : 0.5637717678180137
Loss in iteration 246 : 0.5086986155106429
Loss in iteration 247 : 0.674815978629041
Loss in iteration 248 : 0.6551221797082988
Loss in iteration 249 : 0.5071677493482359
Loss in iteration 250 : 0.5174149821929275
Loss in iteration 251 : 0.6182266383997268
Loss in iteration 252 : 0.5814164260858384
Loss in iteration 253 : 0.501634762022678
Loss in iteration 254 : 0.48606455441981483
Loss in iteration 255 : 0.5520086417602043
Loss in iteration 256 : 0.5898433401477284
Loss in iteration 257 : 0.5176507090922079
Loss in iteration 258 : 0.4835862301644743
Loss in iteration 259 : 0.46443899646145637
Loss in iteration 260 : 0.4980522800680096
Loss in iteration 261 : 0.5392778063487684
Loss in iteration 262 : 0.6478329791492988
Loss in iteration 263 : 0.6418347652559293
Loss in iteration 264 : 0.6160298484807026
Loss in iteration 265 : 0.4979866663703305
Loss in iteration 266 : 0.48728339814755794
Loss in iteration 267 : 0.5338703343511042
Loss in iteration 268 : 0.6611463559721987
Loss in iteration 269 : 0.7149345798170038
Loss in iteration 270 : 0.57007822473487
Loss in iteration 271 : 0.46490925871391936
Loss in iteration 272 : 0.517051343537644
Loss in iteration 273 : 0.6355679759927398
Loss in iteration 274 : 0.6814677945779833
Loss in iteration 275 : 0.5530547886232137
Loss in iteration 276 : 0.4852282252686316
Loss in iteration 277 : 0.5140192584086691
Loss in iteration 278 : 0.5336053546156578
Loss in iteration 279 : 0.5092299581518629
Loss in iteration 280 : 0.46639879800208695
Loss in iteration 281 : 0.49849316205188815
Loss in iteration 282 : 0.517903291308944
Loss in iteration 283 : 0.5371675267090733
Loss in iteration 284 : 0.5152094859161546
Loss in iteration 285 : 0.5102627266636549
Loss in iteration 286 : 0.5077872812651997
Loss in iteration 287 : 0.4950845079914186
Loss in iteration 288 : 0.4737602147469616
Loss in iteration 289 : 0.4650633277360517
Loss in iteration 290 : 0.5014893699174973
Loss in iteration 291 : 0.6231071500146679
Loss in iteration 292 : 0.6765324869825975
Loss in iteration 293 : 0.65691119527012
Loss in iteration 294 : 0.5269999073193234
Loss in iteration 295 : 0.4655358446480029
Loss in iteration 296 : 0.4854400273640165
Loss in iteration 297 : 0.5890550503359425
Loss in iteration 298 : 0.6826669166734525
Loss in iteration 299 : 0.6121796792494465
Loss in iteration 300 : 0.5314953505397086
Loss in iteration 301 : 0.4838696446998325
Loss in iteration 302 : 0.4762378617174463
Loss in iteration 303 : 0.4946642012877506
Loss in iteration 304 : 0.5114624938967242
Loss in iteration 305 : 0.5227247235671783
Loss in iteration 306 : 0.5077462111708966
Loss in iteration 307 : 0.49559555255114
Loss in iteration 308 : 0.4783581064481874
Loss in iteration 309 : 0.47280293628698705
Loss in iteration 310 : 0.5311234314176393
Loss in iteration 311 : 0.5934140786091886
Loss in iteration 312 : 0.6030559467722737
Loss in iteration 313 : 0.5183059084292324
Loss in iteration 314 : 0.4742342046000342
Loss in iteration 315 : 0.4775079473551393
Loss in iteration 316 : 0.5368257237264205
Loss in iteration 317 : 0.5928985666229276
Loss in iteration 318 : 0.5901622636947113
Loss in iteration 319 : 0.5295093935050844
Loss in iteration 320 : 0.47352109265007186
Loss in iteration 321 : 0.4615965547401456
Loss in iteration 322 : 0.5323156659431929
Loss in iteration 323 : 0.5956130968961502
Loss in iteration 324 : 0.6123984299626895
Loss in iteration 325 : 0.5118949375414393
Loss in iteration 326 : 0.48384743341135367
Loss in iteration 327 : 0.4709560021074653
Loss in iteration 328 : 0.5259426000312312
Loss in iteration 329 : 0.5640084741764709
Loss in iteration 330 : 0.5160031262744386
Loss in iteration 331 : 0.47949932338255447
Loss in iteration 332 : 0.45143270448561207
Loss in iteration 333 : 0.4745222159885931
Loss in iteration 334 : 0.5209000006969198
Loss in iteration 335 : 0.5193058744143095
Loss in iteration 336 : 0.5128597411287279
Loss in iteration 337 : 0.48650375056422224
Loss in iteration 338 : 0.4898119522641819
Loss in iteration 339 : 0.47724227309023654
Loss in iteration 340 : 0.4773199375804321
Loss in iteration 341 : 0.48492912836023183
Loss in iteration 342 : 0.525810965310631
Loss in iteration 343 : 0.5676083411070935
Loss in iteration 344 : 0.5720173002410295
Loss in iteration 345 : 0.556909008926612
Loss in iteration 346 : 0.5024280272355394
Loss in iteration 347 : 0.48745462125894384
Loss in iteration 348 : 0.46944652975526985
Loss in iteration 349 : 0.4943160296511163
Loss in iteration 350 : 0.5664129881812762
Loss in iteration 351 : 0.6196128264438795
Loss in iteration 352 : 0.631317849411614
Loss in iteration 353 : 0.49300339750495725
Loss in iteration 354 : 0.48892188322069857
Loss in iteration 355 : 0.5109649093654718
Loss in iteration 356 : 0.5665108982018838
Loss in iteration 357 : 0.5424733345191272
Loss in iteration 358 : 0.5177743265232243
Loss in iteration 359 : 0.4820101225551817
Loss in iteration 360 : 0.47968452697610914
Loss in iteration 361 : 0.4867929003823678
Loss in iteration 362 : 0.5011815878697417
Loss in iteration 363 : 0.498299537438479
Loss in iteration 364 : 0.4808354359442769
Loss in iteration 365 : 0.4866485863076453
Loss in iteration 366 : 0.4694067109739483
Loss in iteration 367 : 0.48539395926298057
Loss in iteration 368 : 0.504544222064039
Loss in iteration 369 : 0.5270644321685538
Loss in iteration 370 : 0.5534385549816419
Loss in iteration 371 : 0.5511993205247276
Loss in iteration 372 : 0.526692661155898
Loss in iteration 373 : 0.49291454023446585
Loss in iteration 374 : 0.47588475168895056
Loss in iteration 375 : 0.47785884479331614
Loss in iteration 376 : 0.505465486972002
Loss in iteration 377 : 0.5292180600522953
Loss in iteration 378 : 0.5318716775252438
Loss in iteration 379 : 0.5078320133606338
Loss in iteration 380 : 0.5008815202947187
Loss in iteration 381 : 0.46310419243621775
Loss in iteration 382 : 0.4790502009969333
Loss in iteration 383 : 0.4655788846173236
Loss in iteration 384 : 0.4981349713866679
Loss in iteration 385 : 0.4676674719025012
Loss in iteration 386 : 0.47018444603715515
Loss in iteration 387 : 0.47940069384783085
Loss in iteration 388 : 0.5001125256755131
Loss in iteration 389 : 0.4697116063532822
Loss in iteration 390 : 0.47118439618345687
Loss in iteration 391 : 0.4710775414012378
Loss in iteration 392 : 0.46086927362962865
Loss in iteration 393 : 0.4667416550340541
Loss in iteration 394 : 0.46749747774628536
Loss in iteration 395 : 0.47757811430168584
Loss in iteration 396 : 0.46527287695303315
Loss in iteration 397 : 0.4920566296220491
Loss in iteration 398 : 0.4929925670302879
Loss in iteration 399 : 0.5283106541951877
Loss in iteration 400 : 0.5806468884556759
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.654125, training accuracy 0.654125, time elapsed: 5084 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 1.329580598641966
Loss in iteration 3 : 1.3171790348671353
Loss in iteration 4 : 0.866221612735751
Loss in iteration 5 : 0.5103323596700733
Loss in iteration 6 : 0.6705604505834251
Loss in iteration 7 : 0.6817886398487649
Loss in iteration 8 : 0.5170630365844094
Loss in iteration 9 : 0.6662634409278787
Loss in iteration 10 : 0.5158418281756083
Loss in iteration 11 : 0.6066114617474504
Loss in iteration 12 : 0.518739258920902
Loss in iteration 13 : 0.5930667743446404
Loss in iteration 14 : 0.5245986711210523
Loss in iteration 15 : 0.5952678873113787
Loss in iteration 16 : 0.5406472463488038
Loss in iteration 17 : 0.569662903843503
Loss in iteration 18 : 0.5215187074042463
Loss in iteration 19 : 0.5614651078634827
Loss in iteration 20 : 0.5135747334040847
Loss in iteration 21 : 0.5682092408847249
Loss in iteration 22 : 0.5208154480829436
Loss in iteration 23 : 0.5459971579412147
Loss in iteration 24 : 0.5047022404199079
Loss in iteration 25 : 0.5258881918992376
Loss in iteration 26 : 0.48008165118917545
Loss in iteration 27 : 0.5068210745091895
Loss in iteration 28 : 0.4992126329267922
Loss in iteration 29 : 0.5019686389437011
Loss in iteration 30 : 0.5093343980331471
Loss in iteration 31 : 0.48719332834128803
Loss in iteration 32 : 0.49754270427947744
Loss in iteration 33 : 0.4751991617107564
Loss in iteration 34 : 0.4769379346491324
Loss in iteration 35 : 0.48779696062033806
Loss in iteration 36 : 0.4859056683488249
Loss in iteration 37 : 0.4836342898989586
Loss in iteration 38 : 0.48210295961954974
Loss in iteration 39 : 0.4826617535647017
Loss in iteration 40 : 0.49724746668613196
Loss in iteration 41 : 0.4836260869617736
Loss in iteration 42 : 0.4839548312329066
Loss in iteration 43 : 0.4822684947941817
Loss in iteration 44 : 0.4729472543447652
Loss in iteration 45 : 0.48523106583870745
Loss in iteration 46 : 0.4833481805921723
Loss in iteration 47 : 0.46385945803540823
Loss in iteration 48 : 0.47082071401029535
Loss in iteration 49 : 0.47263818002492564
Loss in iteration 50 : 0.45079521183994364
Loss in iteration 51 : 0.4771953773848685
Loss in iteration 52 : 0.4676806820553034
Loss in iteration 53 : 0.4603078072876464
Loss in iteration 54 : 0.45981643563254493
Loss in iteration 55 : 0.46430840876678664
Loss in iteration 56 : 0.4679254613555537
Loss in iteration 57 : 0.45046679959152885
Loss in iteration 58 : 0.45819862529022853
Loss in iteration 59 : 0.4510810674216189
Loss in iteration 60 : 0.46349105511091154
Loss in iteration 61 : 0.4616259300721628
Loss in iteration 62 : 0.460476680964375
Loss in iteration 63 : 0.46145894521081365
Loss in iteration 64 : 0.4635284483325819
Loss in iteration 65 : 0.47113467321839136
Loss in iteration 66 : 0.45437718572134034
Loss in iteration 67 : 0.46674786893219733
Loss in iteration 68 : 0.4575543876464503
Loss in iteration 69 : 0.46406319667916357
Loss in iteration 70 : 0.45374550523496293
Loss in iteration 71 : 0.46990899364341143
Loss in iteration 72 : 0.45486067083160997
Loss in iteration 73 : 0.4655422781706687
Loss in iteration 74 : 0.4577992925303382
Loss in iteration 75 : 0.46067065993224265
Loss in iteration 76 : 0.4719454517410478
Loss in iteration 77 : 0.45532213691916323
Loss in iteration 78 : 0.46521627327268583
Loss in iteration 79 : 0.4453816036113895
Loss in iteration 80 : 0.46524440961522906
Loss in iteration 81 : 0.4655479058646982
Loss in iteration 82 : 0.460538189979514
Loss in iteration 83 : 0.4657791000619326
Loss in iteration 84 : 0.4744829024440024
Loss in iteration 85 : 0.4628096087418419
Loss in iteration 86 : 0.4789757817597044
Loss in iteration 87 : 0.45126718820077466
Loss in iteration 88 : 0.4745314139632402
Loss in iteration 89 : 0.4689790196623766
Loss in iteration 90 : 0.46205215002571143
Loss in iteration 91 : 0.4686507681298249
Loss in iteration 92 : 0.4681199130528913
Loss in iteration 93 : 0.46938144916745284
Loss in iteration 94 : 0.4582633294799771
Loss in iteration 95 : 0.4666298803872343
Loss in iteration 96 : 0.46945885674490084
Loss in iteration 97 : 0.45900914951513017
Loss in iteration 98 : 0.46518474306997276
Loss in iteration 99 : 0.45985335385011644
Loss in iteration 100 : 0.46119037518892814
Loss in iteration 101 : 0.4586761376331645
Loss in iteration 102 : 0.4528165557542107
Loss in iteration 103 : 0.46362028590323817
Loss in iteration 104 : 0.4639856252735348
Loss in iteration 105 : 0.4451391749124557
Loss in iteration 106 : 0.4610718454880765
Loss in iteration 107 : 0.4742016324440241
Loss in iteration 108 : 0.4775721302402726
Loss in iteration 109 : 0.4664668143271838
Loss in iteration 110 : 0.4576120015160419
Loss in iteration 111 : 0.46332441269891816
Loss in iteration 112 : 0.4672610270091653
Loss in iteration 113 : 0.47506680817297314
Loss in iteration 114 : 0.453353579516168
Loss in iteration 115 : 0.47135535308472803
Loss in iteration 116 : 0.49746929213846175
Loss in iteration 117 : 0.47158381824382317
Loss in iteration 118 : 0.4852941234417026
Loss in iteration 119 : 0.48573579096782893
Loss in iteration 120 : 0.47073826425926946
Loss in iteration 121 : 0.49355706528482673
Loss in iteration 122 : 0.5001545042134456
Loss in iteration 123 : 0.46670930015403633
Loss in iteration 124 : 0.5063424366399967
Loss in iteration 125 : 0.477077258571699
Loss in iteration 126 : 0.4619694067133241
Loss in iteration 127 : 0.5010815531621808
Loss in iteration 128 : 0.47822609526285637
Loss in iteration 129 : 0.46955124830502837
Loss in iteration 130 : 0.4983297937355294
Loss in iteration 131 : 0.46334608083681295
Loss in iteration 132 : 0.4625648292936562
Loss in iteration 133 : 0.48008332984852264
Loss in iteration 134 : 0.4639304467365996
Loss in iteration 135 : 0.4673038962207897
Loss in iteration 136 : 0.4706496167004922
Loss in iteration 137 : 0.46787545475416936
Loss in iteration 138 : 0.469566152111842
Loss in iteration 139 : 0.48388473909016055
Loss in iteration 140 : 0.4699719390833793
Loss in iteration 141 : 0.4778578134330792
Loss in iteration 142 : 0.4687706555077958
Loss in iteration 143 : 0.4651203712015941
Loss in iteration 144 : 0.4683975617069665
Loss in iteration 145 : 0.46400984416414387
Loss in iteration 146 : 0.4672783638344276
Loss in iteration 147 : 0.5027287261959752
Loss in iteration 148 : 0.4812437085359539
Loss in iteration 149 : 0.4790418087635267
Loss in iteration 150 : 0.48506158123134085
Loss in iteration 151 : 0.47296773945261866
Loss in iteration 152 : 0.4722899923680204
Loss in iteration 153 : 0.46472711237432296
Loss in iteration 154 : 0.47416135501709933
Loss in iteration 155 : 0.4678548210281854
Loss in iteration 156 : 0.45793203861846044
Loss in iteration 157 : 0.4710874465707586
Loss in iteration 158 : 0.4617096234588286
Loss in iteration 159 : 0.46366414691888724
Loss in iteration 160 : 0.4634043866834228
Loss in iteration 161 : 0.45582398802973384
Loss in iteration 162 : 0.4703456332814865
Loss in iteration 163 : 0.4636796592520707
Loss in iteration 164 : 0.46769933868030217
Loss in iteration 165 : 0.4584016284507469
Loss in iteration 166 : 0.46138714846486184
Loss in iteration 167 : 0.469867501499486
Loss in iteration 168 : 0.4785808899074115
Loss in iteration 169 : 0.47124910434067263
Loss in iteration 170 : 0.46671610333826785
Loss in iteration 171 : 0.45962082038410645
Loss in iteration 172 : 0.4716992676429927
Loss in iteration 173 : 0.4581019634127302
Loss in iteration 174 : 0.45852413667734976
Loss in iteration 175 : 0.47475365765436195
Loss in iteration 176 : 0.45370567981909554
Loss in iteration 177 : 0.47145527917872504
Loss in iteration 178 : 0.4623030680574275
Loss in iteration 179 : 0.4675541289994916
Loss in iteration 180 : 0.4577194224321485
Loss in iteration 181 : 0.4676155585629919
Loss in iteration 182 : 0.4606950767626468
Loss in iteration 183 : 0.46167899338086377
Loss in iteration 184 : 0.47594060324413945
Loss in iteration 185 : 0.4588509506932293
Loss in iteration 186 : 0.45084083500529093
Loss in iteration 187 : 0.4558550548560312
Loss in iteration 188 : 0.4685503527170342
Loss in iteration 189 : 0.4641384700003955
Loss in iteration 190 : 0.4481494922903473
Loss in iteration 191 : 0.4556782555727883
Loss in iteration 192 : 0.4604877638453382
Loss in iteration 193 : 0.46608717052845816
Loss in iteration 194 : 0.4633183456742326
Loss in iteration 195 : 0.46877415429363795
Loss in iteration 196 : 0.477246621234545
Loss in iteration 197 : 0.45010263170104864
Loss in iteration 198 : 0.4744680243424792
Loss in iteration 199 : 0.4631220332918253
Loss in iteration 200 : 0.4566917266251234
Loss in iteration 201 : 0.4600929425056914
Loss in iteration 202 : 0.4743751436617445
Loss in iteration 203 : 0.455706844672835
Loss in iteration 204 : 0.46197550258061726
Loss in iteration 205 : 0.4633246805571477
Loss in iteration 206 : 0.4592646264568597
Loss in iteration 207 : 0.45675025213933734
Loss in iteration 208 : 0.45049429682524916
Loss in iteration 209 : 0.468772170536657
Loss in iteration 210 : 0.4712921462917423
Loss in iteration 211 : 0.46692489177607055
Loss in iteration 212 : 0.47166184458050836
Loss in iteration 213 : 0.4721747698385896
Loss in iteration 214 : 0.4833711148854809
Loss in iteration 215 : 0.4717136730339271
Loss in iteration 216 : 0.46089723987941006
Loss in iteration 217 : 0.4627410416982414
Loss in iteration 218 : 0.46234258925385885
Loss in iteration 219 : 0.4831587494883263
Loss in iteration 220 : 0.472847771530107
Loss in iteration 221 : 0.46346987471672807
Loss in iteration 222 : 0.4575566489647888
Loss in iteration 223 : 0.4662423219722541
Loss in iteration 224 : 0.47109183907672836
Loss in iteration 225 : 0.4607411932608647
Loss in iteration 226 : 0.4636875839376034
Loss in iteration 227 : 0.4608262810502103
Loss in iteration 228 : 0.47350576227303437
Loss in iteration 229 : 0.46624114551997575
Loss in iteration 230 : 0.45991563649942974
Loss in iteration 231 : 0.4674927857394801
Loss in iteration 232 : 0.467832859692817
Loss in iteration 233 : 0.4564365116398284
Loss in iteration 234 : 0.464653847963622
Loss in iteration 235 : 0.45658055979081674
Loss in iteration 236 : 0.46464785435640626
Loss in iteration 237 : 0.46568428701222137
Loss in iteration 238 : 0.46217418461858223
Loss in iteration 239 : 0.46639247916790966
Loss in iteration 240 : 0.4536128328000253
Loss in iteration 241 : 0.45879303477567723
Loss in iteration 242 : 0.4945605018190759
Loss in iteration 243 : 0.472398488859304
Loss in iteration 244 : 0.4588241742375227
Loss in iteration 245 : 0.47901401351763656
Loss in iteration 246 : 0.4590590706075111
Loss in iteration 247 : 0.4547382779630267
Loss in iteration 248 : 0.4657413731527153
Loss in iteration 249 : 0.46762974796563883
Loss in iteration 250 : 0.4577160805272813
Loss in iteration 251 : 0.4677874068367193
Loss in iteration 252 : 0.46837987763169464
Loss in iteration 253 : 0.4700509628333192
Loss in iteration 254 : 0.45008768613375105
Loss in iteration 255 : 0.45350848377364433
Loss in iteration 256 : 0.4758979581698091
Loss in iteration 257 : 0.45018112881587324
Loss in iteration 258 : 0.4590706406982934
Loss in iteration 259 : 0.45705046198170846
Loss in iteration 260 : 0.4603187912343704
Loss in iteration 261 : 0.44948471665610207
Loss in iteration 262 : 0.46138056267766075
Loss in iteration 263 : 0.46223933892387564
Loss in iteration 264 : 0.4751439528744128
Loss in iteration 265 : 0.4541034346665472
Loss in iteration 266 : 0.47360194302247277
Loss in iteration 267 : 0.4785114422416748
Loss in iteration 268 : 0.475421892264839
Loss in iteration 269 : 0.45649005084302635
Loss in iteration 270 : 0.4679105318565609
Loss in iteration 271 : 0.45968963060825874
Loss in iteration 272 : 0.4665284176766337
Loss in iteration 273 : 0.4483351843304399
Loss in iteration 274 : 0.46177273270431474
Loss in iteration 275 : 0.4683118228328838
Loss in iteration 276 : 0.47087440633909233
Loss in iteration 277 : 0.46646965167765986
Loss in iteration 278 : 0.47056532755614716
Loss in iteration 279 : 0.453096396952895
Loss in iteration 280 : 0.4575447886815622
Loss in iteration 281 : 0.4730070989449887
Loss in iteration 282 : 0.4655084913404736
Loss in iteration 283 : 0.4641484029379326
Loss in iteration 284 : 0.46227635048466115
Loss in iteration 285 : 0.472644086670896
Loss in iteration 286 : 0.46576173264688636
Loss in iteration 287 : 0.46976922193448584
Loss in iteration 288 : 0.48365164183586223
Loss in iteration 289 : 0.46780651269802453
Loss in iteration 290 : 0.45409043842020735
Loss in iteration 291 : 0.4651898435560255
Loss in iteration 292 : 0.46703152046093804
Loss in iteration 293 : 0.46337897307000686
Loss in iteration 294 : 0.469079681620408
Loss in iteration 295 : 0.4530374781959218
Loss in iteration 296 : 0.4591084517912221
Loss in iteration 297 : 0.4644516630523453
Loss in iteration 298 : 0.46567390831070504
Loss in iteration 299 : 0.46724299792152324
Loss in iteration 300 : 0.47360484125402835
Loss in iteration 301 : 0.46514971380516223
Loss in iteration 302 : 0.46227641540655323
Loss in iteration 303 : 0.45204857905195617
Loss in iteration 304 : 0.4641510695587893
Loss in iteration 305 : 0.4593600274023297
Loss in iteration 306 : 0.46680974554673876
Loss in iteration 307 : 0.4623314449281532
Loss in iteration 308 : 0.4663425425634511
Loss in iteration 309 : 0.4606417745529484
Loss in iteration 310 : 0.4707905923860344
Loss in iteration 311 : 0.47184019038587194
Loss in iteration 312 : 0.4792761861951067
Loss in iteration 313 : 0.45191913405512846
Loss in iteration 314 : 0.4598184952812607
Loss in iteration 315 : 0.4589764285301159
Loss in iteration 316 : 0.4820008453682737
Loss in iteration 317 : 0.46975520465603177
Loss in iteration 318 : 0.47326559158056825
Loss in iteration 319 : 0.4577083932177765
Loss in iteration 320 : 0.45423896502008715
Loss in iteration 321 : 0.45491469690511993
Loss in iteration 322 : 0.4669076906307343
Loss in iteration 323 : 0.4593817525827404
Loss in iteration 324 : 0.4656210235142925
Loss in iteration 325 : 0.46022073352076615
Loss in iteration 326 : 0.4797934243591096
Loss in iteration 327 : 0.4531541008711219
Loss in iteration 328 : 0.45528524592683495
Loss in iteration 329 : 0.4606860225848652
Loss in iteration 330 : 0.4596144932347718
Loss in iteration 331 : 0.46120515915075816
Loss in iteration 332 : 0.4505843982860764
Loss in iteration 333 : 0.4528692472730057
Loss in iteration 334 : 0.4632347571416945
Loss in iteration 335 : 0.457000792316236
Loss in iteration 336 : 0.46037881727486185
Loss in iteration 337 : 0.4754577935033471
Loss in iteration 338 : 0.4793304193551649
Loss in iteration 339 : 0.4735861216139996
Loss in iteration 340 : 0.4785564787996536
Loss in iteration 341 : 0.4800444098258068
Loss in iteration 342 : 0.4688890874279797
Loss in iteration 343 : 0.4615533078537934
Loss in iteration 344 : 0.4832941340788239
Loss in iteration 345 : 0.47350498610092545
Loss in iteration 346 : 0.4551697206332635
Loss in iteration 347 : 0.463682500093331
Loss in iteration 348 : 0.47357813874140026
Loss in iteration 349 : 0.4689131439125186
Loss in iteration 350 : 0.47772943367280973
Loss in iteration 351 : 0.4859295148272187
Loss in iteration 352 : 0.46726724184297946
Loss in iteration 353 : 0.45885686890638555
Loss in iteration 354 : 0.4782523920135661
Loss in iteration 355 : 0.4696901840452149
Loss in iteration 356 : 0.45405742402276206
Loss in iteration 357 : 0.4722847337895021
Loss in iteration 358 : 0.47449396175932324
Loss in iteration 359 : 0.45723699657695355
Loss in iteration 360 : 0.4872517758907987
Loss in iteration 361 : 0.46368038540495565
Loss in iteration 362 : 0.4748450575090494
Loss in iteration 363 : 0.47851559223626594
Loss in iteration 364 : 0.48366038899543523
Loss in iteration 365 : 0.4738016333514927
Loss in iteration 366 : 0.46384361104061045
Loss in iteration 367 : 0.4849887222520137
Loss in iteration 368 : 0.48157284193052097
Loss in iteration 369 : 0.4807585055203801
Loss in iteration 370 : 0.4598279605873734
Loss in iteration 371 : 0.49341658412626693
Loss in iteration 372 : 0.4698310204765642
Loss in iteration 373 : 0.4637351827428328
Loss in iteration 374 : 0.4633659704265808
Loss in iteration 375 : 0.4562724365519323
Loss in iteration 376 : 0.46671673426791177
Loss in iteration 377 : 0.4648797779128848
Loss in iteration 378 : 0.46527599190074376
Loss in iteration 379 : 0.465430379567744
Loss in iteration 380 : 0.4589269753050876
Loss in iteration 381 : 0.44868915730732273
Loss in iteration 382 : 0.4550181284426077
Loss in iteration 383 : 0.4652100663064691
Loss in iteration 384 : 0.47410892044814257
Loss in iteration 385 : 0.4625541755666491
Loss in iteration 386 : 0.44964931326892504
Loss in iteration 387 : 0.46920318161043867
Loss in iteration 388 : 0.47496367649691473
Loss in iteration 389 : 0.4612824464847201
Loss in iteration 390 : 0.4586562949799512
Loss in iteration 391 : 0.46055320368359803
Loss in iteration 392 : 0.4532245703881874
Loss in iteration 393 : 0.4564443076646994
Loss in iteration 394 : 0.4595737673678374
Loss in iteration 395 : 0.4662096189163969
Loss in iteration 396 : 0.4570893287414491
Loss in iteration 397 : 0.47481765000631393
Loss in iteration 398 : 0.4725895003741908
Loss in iteration 399 : 0.4627214596092558
Loss in iteration 400 : 0.45508792198571085
Testing accuracy  of updater 9 on alg 0 with rate 1.4000000000000001 = 0.78775, training accuracy 0.78775, time elapsed: 5403 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.715087442865266
Loss in iteration 3 : 0.7112088592445752
Loss in iteration 4 : 0.6391303869727916
Loss in iteration 5 : 0.5366222665969028
Loss in iteration 6 : 0.5654681085060946
Loss in iteration 7 : 0.5196766120104402
Loss in iteration 8 : 0.5118485468690951
Loss in iteration 9 : 0.5076975327666478
Loss in iteration 10 : 0.4967470148458654
Loss in iteration 11 : 0.48836248373235536
Loss in iteration 12 : 0.49139691925020135
Loss in iteration 13 : 0.4819513051411689
Loss in iteration 14 : 0.4918501292354381
Loss in iteration 15 : 0.4952080796489965
Loss in iteration 16 : 0.505019033634859
Loss in iteration 17 : 0.4819697895326349
Loss in iteration 18 : 0.48762402231083896
Loss in iteration 19 : 0.49277160649996293
Loss in iteration 20 : 0.4812232411890075
Loss in iteration 21 : 0.5002408419142368
Loss in iteration 22 : 0.491158271326823
Loss in iteration 23 : 0.4937717173453733
Loss in iteration 24 : 0.4820001857113411
Loss in iteration 25 : 0.48539180290945877
Loss in iteration 26 : 0.4652208990360451
Loss in iteration 27 : 0.47249412721766026
Loss in iteration 28 : 0.48224812694437075
Loss in iteration 29 : 0.47739027484195506
Loss in iteration 30 : 0.4831204762851628
Loss in iteration 31 : 0.4745603928818123
Loss in iteration 32 : 0.4727869068635818
Loss in iteration 33 : 0.46208723771508503
Loss in iteration 34 : 0.4610055804387696
Loss in iteration 35 : 0.47588754861033183
Loss in iteration 36 : 0.4741879272343433
Loss in iteration 37 : 0.4684756406457369
Loss in iteration 38 : 0.4706444557649238
Loss in iteration 39 : 0.4620807232537323
Loss in iteration 40 : 0.4699711556088916
Loss in iteration 41 : 0.4751109054501726
Loss in iteration 42 : 0.4593020698756547
Loss in iteration 43 : 0.4659248155335536
Loss in iteration 44 : 0.4662814435267138
Loss in iteration 45 : 0.47014215748372473
Loss in iteration 46 : 0.4720200704743948
Loss in iteration 47 : 0.4638200149897855
Loss in iteration 48 : 0.46655702751162015
Loss in iteration 49 : 0.46610198118921214
Loss in iteration 50 : 0.4525672640036463
Loss in iteration 51 : 0.47179136409462324
Loss in iteration 52 : 0.45962745550452977
Loss in iteration 53 : 0.4610818084495682
Loss in iteration 54 : 0.4578880963643986
Loss in iteration 55 : 0.46213484633315893
Loss in iteration 56 : 0.4678801514244223
Loss in iteration 57 : 0.4489700047648886
Loss in iteration 58 : 0.45535638449118654
Loss in iteration 59 : 0.4523813932334117
Loss in iteration 60 : 0.4629089585930862
Loss in iteration 61 : 0.45918270631957486
Loss in iteration 62 : 0.46178268605665496
Loss in iteration 63 : 0.4634527477780076
Loss in iteration 64 : 0.45883347766213844
Loss in iteration 65 : 0.4706475814967334
Loss in iteration 66 : 0.45579964577604226
Loss in iteration 67 : 0.46636950107410385
Loss in iteration 68 : 0.45735372240078587
Loss in iteration 69 : 0.4663380961877896
Loss in iteration 70 : 0.4537527767544371
Loss in iteration 71 : 0.47055198228853207
Loss in iteration 72 : 0.45478896547526054
Loss in iteration 73 : 0.4667759992444499
Loss in iteration 74 : 0.4576019141381012
Loss in iteration 75 : 0.45976999775428107
Loss in iteration 76 : 0.4727611871858926
Loss in iteration 77 : 0.4546815426335834
Loss in iteration 78 : 0.46732079236386226
Loss in iteration 79 : 0.4423899101498542
Loss in iteration 80 : 0.4604779531095889
Loss in iteration 81 : 0.46482423947397383
Loss in iteration 82 : 0.46072761139268387
Loss in iteration 83 : 0.4608288983460025
Loss in iteration 84 : 0.47521394748867796
Loss in iteration 85 : 0.46302368687498197
Loss in iteration 86 : 0.4718305730031182
Loss in iteration 87 : 0.4499305731485453
Loss in iteration 88 : 0.4737422303120602
Loss in iteration 89 : 0.4553884131884429
Loss in iteration 90 : 0.4597472969891242
Loss in iteration 91 : 0.46689351266366463
Loss in iteration 92 : 0.45329164044553033
Loss in iteration 93 : 0.46154712953947535
Loss in iteration 94 : 0.4608810226869021
Loss in iteration 95 : 0.45837718253722853
Loss in iteration 96 : 0.46803232637571923
Loss in iteration 97 : 0.46024484983760394
Loss in iteration 98 : 0.46487144222808136
Loss in iteration 99 : 0.4572968068620053
Loss in iteration 100 : 0.46237138027381525
Loss in iteration 101 : 0.4572613428600125
Loss in iteration 102 : 0.45234583244548426
Loss in iteration 103 : 0.4645970814579058
Loss in iteration 104 : 0.46234361486181724
Loss in iteration 105 : 0.4442527007391682
Loss in iteration 106 : 0.4604886080510173
Loss in iteration 107 : 0.47679535899576214
Loss in iteration 108 : 0.47858176682830705
Loss in iteration 109 : 0.4610948992458722
Loss in iteration 110 : 0.4550737348709938
Loss in iteration 111 : 0.4639696215408282
Loss in iteration 112 : 0.45561684034683503
Loss in iteration 113 : 0.4535356408615908
Loss in iteration 114 : 0.4547121566992368
Loss in iteration 115 : 0.45761940442679744
Loss in iteration 116 : 0.4653706335566757
Loss in iteration 117 : 0.47312381002561704
Loss in iteration 118 : 0.46719099010898046
Loss in iteration 119 : 0.4560112180078639
Loss in iteration 120 : 0.4724424155269509
Loss in iteration 121 : 0.46504639565435474
Loss in iteration 122 : 0.47604399940978764
Loss in iteration 123 : 0.46557574945877667
Loss in iteration 124 : 0.4551126680150876
Loss in iteration 125 : 0.4585183472875647
Loss in iteration 126 : 0.45842556339002277
Loss in iteration 127 : 0.45569639558503366
Loss in iteration 128 : 0.46540768075931493
Loss in iteration 129 : 0.45895709541169616
Loss in iteration 130 : 0.46507911517696043
Loss in iteration 131 : 0.45743975947074333
Loss in iteration 132 : 0.456343691365749
Loss in iteration 133 : 0.46296655206339854
Loss in iteration 134 : 0.46468779503027574
Loss in iteration 135 : 0.46235436748964015
Loss in iteration 136 : 0.4606351925986173
Loss in iteration 137 : 0.4667301992517538
Loss in iteration 138 : 0.46610378571372646
Loss in iteration 139 : 0.4717329432274369
Loss in iteration 140 : 0.46776591353702784
Loss in iteration 141 : 0.4711842831225703
Loss in iteration 142 : 0.457027014014765
Loss in iteration 143 : 0.46597906115783216
Loss in iteration 144 : 0.4557260419276625
Loss in iteration 145 : 0.4535090536218008
Loss in iteration 146 : 0.4660065425535866
Loss in iteration 147 : 0.47437149611783463
Loss in iteration 148 : 0.46086161209940324
Loss in iteration 149 : 0.47498634810795526
Loss in iteration 150 : 0.46983472354168954
Loss in iteration 151 : 0.45956681188582776
Loss in iteration 152 : 0.47717672457126153
Loss in iteration 153 : 0.4471993141892435
Loss in iteration 154 : 0.4685367746868295
Loss in iteration 155 : 0.4666079722153233
Loss in iteration 156 : 0.4495606752607233
Loss in iteration 157 : 0.46375429810074925
Loss in iteration 158 : 0.4605764445540452
Loss in iteration 159 : 0.46129051225011686
Loss in iteration 160 : 0.4579413625816488
Loss in iteration 161 : 0.4560044625815437
Loss in iteration 162 : 0.4647207069955555
Loss in iteration 163 : 0.46440223251712653
Loss in iteration 164 : 0.4636868909725471
Loss in iteration 165 : 0.46199698118102217
Loss in iteration 166 : 0.4602277553717583
Loss in iteration 167 : 0.47468279650289125
Loss in iteration 168 : 0.47133740769239224
Loss in iteration 169 : 0.45877520890312457
Loss in iteration 170 : 0.4637446089834352
Loss in iteration 171 : 0.4574122288918327
Loss in iteration 172 : 0.46742155280961906
Loss in iteration 173 : 0.45695555091366963
Loss in iteration 174 : 0.4556327282346574
Loss in iteration 175 : 0.4689069882685164
Loss in iteration 176 : 0.4550164838628109
Loss in iteration 177 : 0.46819932181323604
Loss in iteration 178 : 0.46014966617566966
Loss in iteration 179 : 0.46550760772515426
Loss in iteration 180 : 0.4560507702905449
Loss in iteration 181 : 0.46468642443659197
Loss in iteration 182 : 0.46313269745514435
Loss in iteration 183 : 0.4583413438479742
Loss in iteration 184 : 0.48008438796020547
Loss in iteration 185 : 0.4597469463490155
Loss in iteration 186 : 0.45282846191330856
Loss in iteration 187 : 0.45569688750008536
Loss in iteration 188 : 0.47664326619859787
Loss in iteration 189 : 0.46680242080031736
Loss in iteration 190 : 0.4560011073041259
Loss in iteration 191 : 0.45527388405618024
Loss in iteration 192 : 0.4638268396726408
Loss in iteration 193 : 0.4628535713673405
Loss in iteration 194 : 0.4693969342948593
Loss in iteration 195 : 0.46892079490657784
Loss in iteration 196 : 0.47700203551751624
Loss in iteration 197 : 0.45321024691582495
Loss in iteration 198 : 0.47269248552143767
Loss in iteration 199 : 0.45997672839435094
Loss in iteration 200 : 0.45356105402592617
Loss in iteration 201 : 0.45949288027132823
Loss in iteration 202 : 0.47499704009344285
Loss in iteration 203 : 0.4497462304554585
Loss in iteration 204 : 0.4582837667782105
Loss in iteration 205 : 0.4602690446507874
Loss in iteration 206 : 0.4663035847306115
Loss in iteration 207 : 0.4546379292304136
Loss in iteration 208 : 0.45104181047849795
Loss in iteration 209 : 0.4661885047538177
Loss in iteration 210 : 0.4714860165929166
Loss in iteration 211 : 0.4622082525156516
Loss in iteration 212 : 0.46958770487607393
Loss in iteration 213 : 0.46957721621018
Loss in iteration 214 : 0.480569753274796
Loss in iteration 215 : 0.4719744439476659
Loss in iteration 216 : 0.4586282488552606
Loss in iteration 217 : 0.4571636180106833
Loss in iteration 218 : 0.45659604419756555
Loss in iteration 219 : 0.4688183690233391
Loss in iteration 220 : 0.46743124662869245
Loss in iteration 221 : 0.4626543497805365
Loss in iteration 222 : 0.46007202815156745
Loss in iteration 223 : 0.46473298901685384
Loss in iteration 224 : 0.4781602781120101
Loss in iteration 225 : 0.460628376218391
Loss in iteration 226 : 0.47397768901631154
Loss in iteration 227 : 0.45723225743555335
Loss in iteration 228 : 0.4704593167204163
Loss in iteration 229 : 0.46927227231651114
Loss in iteration 230 : 0.4571486779718947
Loss in iteration 231 : 0.46604082626470655
Loss in iteration 232 : 0.467773070739934
Loss in iteration 233 : 0.46532035610518113
Loss in iteration 234 : 0.4633462555343468
Loss in iteration 235 : 0.46665791172984905
Loss in iteration 236 : 0.4648538312467136
Loss in iteration 237 : 0.4653718253259803
Loss in iteration 238 : 0.45909037901900573
Loss in iteration 239 : 0.47132260637037054
Loss in iteration 240 : 0.45147292836617126
Loss in iteration 241 : 0.46859709579090164
Loss in iteration 242 : 0.4861302943252692
Loss in iteration 243 : 0.4766107434785547
Loss in iteration 244 : 0.46293028890873916
Loss in iteration 245 : 0.4831550735553394
Loss in iteration 246 : 0.45898777294875986
Loss in iteration 247 : 0.46079211510041934
Loss in iteration 248 : 0.4660751422396908
Loss in iteration 249 : 0.47137431102855665
Loss in iteration 250 : 0.45792955070242386
Loss in iteration 251 : 0.470821009667941
Loss in iteration 252 : 0.4650512604824232
Loss in iteration 253 : 0.47352196914149364
Loss in iteration 254 : 0.4504533837106185
Loss in iteration 255 : 0.45283984630660684
Loss in iteration 256 : 0.4752423311078555
Loss in iteration 257 : 0.4496027135572281
Loss in iteration 258 : 0.4603780175251222
Loss in iteration 259 : 0.4559615768963469
Loss in iteration 260 : 0.4617948738830435
Loss in iteration 261 : 0.44903522925427736
Loss in iteration 262 : 0.464742269057073
Loss in iteration 263 : 0.4598935653343344
Loss in iteration 264 : 0.47827488239438204
Loss in iteration 265 : 0.45009594553197574
Loss in iteration 266 : 0.470742413074401
Loss in iteration 267 : 0.46725565095377447
Loss in iteration 268 : 0.47309082201780683
Loss in iteration 269 : 0.45567399478070303
Loss in iteration 270 : 0.46221828173738844
Loss in iteration 271 : 0.4466788283935227
Loss in iteration 272 : 0.4603100864644564
Loss in iteration 273 : 0.4488790412668783
Loss in iteration 274 : 0.4564940860611617
Loss in iteration 275 : 0.46455372810590684
Loss in iteration 276 : 0.46966722094858193
Loss in iteration 277 : 0.4550421460732301
Loss in iteration 278 : 0.4522869276118916
Loss in iteration 279 : 0.44843251831699027
Loss in iteration 280 : 0.45741869664851675
Loss in iteration 281 : 0.46325276795957376
Loss in iteration 282 : 0.4606274791829609
Loss in iteration 283 : 0.46266553456099113
Loss in iteration 284 : 0.4476792725761601
Loss in iteration 285 : 0.45056395039404723
Loss in iteration 286 : 0.4598900961221796
Loss in iteration 287 : 0.46583815136904144
Loss in iteration 288 : 0.46465977924180196
Loss in iteration 289 : 0.4572210221054388
Loss in iteration 290 : 0.45286154482280594
Loss in iteration 291 : 0.4700401000375507
Loss in iteration 292 : 0.46399777152828003
Loss in iteration 293 : 0.45939917519322165
Loss in iteration 294 : 0.46735725677768714
Loss in iteration 295 : 0.4551226469653978
Loss in iteration 296 : 0.45396157557364375
Loss in iteration 297 : 0.4732586156977628
Loss in iteration 298 : 0.46133028677487314
Loss in iteration 299 : 0.4649232183688053
Loss in iteration 300 : 0.46728922740387846
Loss in iteration 301 : 0.4741416094544054
Loss in iteration 302 : 0.45840922278373614
Loss in iteration 303 : 0.4590726299902109
Loss in iteration 304 : 0.4563124529218653
Loss in iteration 305 : 0.46063530044836665
Loss in iteration 306 : 0.458597117253172
Loss in iteration 307 : 0.4606277149545114
Loss in iteration 308 : 0.4592062015852598
Loss in iteration 309 : 0.45654435145729017
Loss in iteration 310 : 0.45871343301961975
Loss in iteration 311 : 0.47069988203012897
Loss in iteration 312 : 0.4802586332193815
Loss in iteration 313 : 0.4489145839469201
Loss in iteration 314 : 0.45943346899633025
Loss in iteration 315 : 0.4600522585909619
Loss in iteration 316 : 0.4786358777883034
Loss in iteration 317 : 0.4698165251358988
Loss in iteration 318 : 0.47455324114403186
Loss in iteration 319 : 0.4544227505200792
Loss in iteration 320 : 0.4583167747047571
Loss in iteration 321 : 0.45771928096197056
Loss in iteration 322 : 0.4666514976501346
Loss in iteration 323 : 0.4599482412414013
Loss in iteration 324 : 0.4627299982209601
Loss in iteration 325 : 0.4570011367593182
Loss in iteration 326 : 0.47817601894856454
Loss in iteration 327 : 0.4506648567073068
Loss in iteration 328 : 0.4573243809713689
Loss in iteration 329 : 0.45739465489473863
Loss in iteration 330 : 0.45667461511979357
Loss in iteration 331 : 0.4618094051478142
Loss in iteration 332 : 0.4530261400077195
Loss in iteration 333 : 0.4503990245098463
Loss in iteration 334 : 0.45945238211303197
Loss in iteration 335 : 0.4522434121955338
Loss in iteration 336 : 0.4589777624812145
Loss in iteration 337 : 0.46140093902346524
Loss in iteration 338 : 0.47202152880043446
Loss in iteration 339 : 0.47141472218674646
Loss in iteration 340 : 0.47109076544920436
Loss in iteration 341 : 0.45907671535755834
Loss in iteration 342 : 0.4611063626297309
Loss in iteration 343 : 0.4618354394996541
Loss in iteration 344 : 0.4651728287037745
Loss in iteration 345 : 0.46082964536482723
Loss in iteration 346 : 0.4516904833967059
Loss in iteration 347 : 0.4657382783918032
Loss in iteration 348 : 0.46220338603092437
Loss in iteration 349 : 0.46781245835492036
Loss in iteration 350 : 0.46734625983675304
Loss in iteration 351 : 0.4622834599843807
Loss in iteration 352 : 0.45917284209044207
Loss in iteration 353 : 0.45552137132405013
Loss in iteration 354 : 0.45866107865541433
Loss in iteration 355 : 0.462424441446307
Loss in iteration 356 : 0.4543054636273545
Loss in iteration 357 : 0.46016513188734404
Loss in iteration 358 : 0.4685940188490352
Loss in iteration 359 : 0.45842232044265674
Loss in iteration 360 : 0.46593605278196587
Loss in iteration 361 : 0.44966998949193987
Loss in iteration 362 : 0.4716327969239077
Loss in iteration 363 : 0.4621519038586836
Loss in iteration 364 : 0.4615476619185565
Loss in iteration 365 : 0.4630833585889045
Loss in iteration 366 : 0.45631980265220123
Loss in iteration 367 : 0.46895596954738444
Loss in iteration 368 : 0.4689968710241433
Loss in iteration 369 : 0.4765123792069766
Loss in iteration 370 : 0.45530066384056433
Loss in iteration 371 : 0.4798980499174239
Loss in iteration 372 : 0.4567335057587486
Loss in iteration 373 : 0.46486387994879275
Loss in iteration 374 : 0.4574919391153431
Loss in iteration 375 : 0.46054767434082794
Loss in iteration 376 : 0.4695502099987426
Loss in iteration 377 : 0.4652666871099687
Loss in iteration 378 : 0.46698020154093106
Loss in iteration 379 : 0.46733796583978826
Loss in iteration 380 : 0.4639525683648332
Loss in iteration 381 : 0.4490723113977581
Loss in iteration 382 : 0.46404399752053016
Loss in iteration 383 : 0.4628554373903877
Loss in iteration 384 : 0.47865033525016787
Loss in iteration 385 : 0.46233356184056423
Loss in iteration 386 : 0.4601859689342078
Loss in iteration 387 : 0.4676750355912982
Loss in iteration 388 : 0.48133546119312315
Loss in iteration 389 : 0.4572503916890067
Loss in iteration 390 : 0.4733766749171052
Loss in iteration 391 : 0.4632539918476873
Loss in iteration 392 : 0.460267406577988
Loss in iteration 393 : 0.4580826207773989
Loss in iteration 394 : 0.4636314489476256
Loss in iteration 395 : 0.4699898192921199
Loss in iteration 396 : 0.46094436159887164
Loss in iteration 397 : 0.4768461332526765
Loss in iteration 398 : 0.473990611160575
Loss in iteration 399 : 0.46554573282974177
Loss in iteration 400 : 0.45111698093095765
Testing accuracy  of updater 9 on alg 0 with rate 0.8 = 0.78425, training accuracy 0.78425, time elapsed: 5230 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6789281275167313
Loss in iteration 3 : 0.6585600745608061
Loss in iteration 4 : 0.6468981762392343
Loss in iteration 5 : 0.6196317379536892
Loss in iteration 6 : 0.6076716794661211
Loss in iteration 7 : 0.5888452098635278
Loss in iteration 8 : 0.5667269398250311
Loss in iteration 9 : 0.5542406167774426
Loss in iteration 10 : 0.5411927701580325
Loss in iteration 11 : 0.5248254134041554
Loss in iteration 12 : 0.5151831449589309
Loss in iteration 13 : 0.5063530961032068
Loss in iteration 14 : 0.49693858626630655
Loss in iteration 15 : 0.5038947292033875
Loss in iteration 16 : 0.49817147493312375
Loss in iteration 17 : 0.487403047563174
Loss in iteration 18 : 0.48542364375238994
Loss in iteration 19 : 0.4882741192859871
Loss in iteration 20 : 0.47970202279648283
Loss in iteration 21 : 0.4949569394948341
Loss in iteration 22 : 0.49124802130564454
Loss in iteration 23 : 0.48897024267495776
Loss in iteration 24 : 0.4809668014728651
Loss in iteration 25 : 0.48607623533142436
Loss in iteration 26 : 0.47196908066705917
Loss in iteration 27 : 0.4757137972507418
Loss in iteration 28 : 0.48199855644875883
Loss in iteration 29 : 0.4895084189650234
Loss in iteration 30 : 0.4866260892547389
Loss in iteration 31 : 0.4796846150137726
Loss in iteration 32 : 0.47901425385157165
Loss in iteration 33 : 0.473197806993213
Loss in iteration 34 : 0.47316651257627096
Loss in iteration 35 : 0.4833634940681783
Loss in iteration 36 : 0.482930309333351
Loss in iteration 37 : 0.47472738693598593
Loss in iteration 38 : 0.47349034509144144
Loss in iteration 39 : 0.46631705092803216
Loss in iteration 40 : 0.47482461430462836
Loss in iteration 41 : 0.47845097386208757
Loss in iteration 42 : 0.46500549681367576
Loss in iteration 43 : 0.4729109541386404
Loss in iteration 44 : 0.4732834648290492
Loss in iteration 45 : 0.47459497334627293
Loss in iteration 46 : 0.47372264292155575
Loss in iteration 47 : 0.4674125784599712
Loss in iteration 48 : 0.4724445433336137
Loss in iteration 49 : 0.46639282309906305
Loss in iteration 50 : 0.4580678302941514
Loss in iteration 51 : 0.4763454517855417
Loss in iteration 52 : 0.4635678584154285
Loss in iteration 53 : 0.46428883338115207
Loss in iteration 54 : 0.46153556256986905
Loss in iteration 55 : 0.46553402631847385
Loss in iteration 56 : 0.4707127167992181
Loss in iteration 57 : 0.45295447471288874
Loss in iteration 58 : 0.4581396020212766
Loss in iteration 59 : 0.4562654983552815
Loss in iteration 60 : 0.46580302406926855
Loss in iteration 61 : 0.4627838267850755
Loss in iteration 62 : 0.4657260983395293
Loss in iteration 63 : 0.464078587121417
Loss in iteration 64 : 0.4629162551922775
Loss in iteration 65 : 0.4740988534472872
Loss in iteration 66 : 0.45683808058639636
Loss in iteration 67 : 0.4687733094959785
Loss in iteration 68 : 0.45917619283635436
Loss in iteration 69 : 0.46697035099114187
Loss in iteration 70 : 0.4577788113073978
Loss in iteration 71 : 0.4720687135860033
Loss in iteration 72 : 0.457709935364634
Loss in iteration 73 : 0.46998416253573544
Loss in iteration 74 : 0.4606295285691031
Loss in iteration 75 : 0.4620625812750124
Loss in iteration 76 : 0.4742328261724704
Loss in iteration 77 : 0.45643155326275825
Loss in iteration 78 : 0.466450475063707
Loss in iteration 79 : 0.44568674278516424
Loss in iteration 80 : 0.4632878498386771
Loss in iteration 81 : 0.46575543554790894
Loss in iteration 82 : 0.46316990508524214
Loss in iteration 83 : 0.463607080112369
Loss in iteration 84 : 0.4751980398807938
Loss in iteration 85 : 0.4634602776536025
Loss in iteration 86 : 0.47283242369519674
Loss in iteration 87 : 0.4528047758127223
Loss in iteration 88 : 0.4723790747489083
Loss in iteration 89 : 0.45607957110275277
Loss in iteration 90 : 0.45985975507350585
Loss in iteration 91 : 0.4687093994556106
Loss in iteration 92 : 0.4573814153210309
Loss in iteration 93 : 0.4618230930650796
Loss in iteration 94 : 0.46061946679107146
Loss in iteration 95 : 0.45935298899751104
Loss in iteration 96 : 0.4694146921882736
Loss in iteration 97 : 0.46142784593338554
Loss in iteration 98 : 0.4675848364192574
Loss in iteration 99 : 0.4594017990022868
Loss in iteration 100 : 0.4647364492629887
Loss in iteration 101 : 0.45952158374815805
Loss in iteration 102 : 0.4549060342363009
Loss in iteration 103 : 0.46704616617892636
Loss in iteration 104 : 0.46189128791786965
Loss in iteration 105 : 0.4470798528537143
Loss in iteration 106 : 0.4623938801453044
Loss in iteration 107 : 0.4766050399859036
Loss in iteration 108 : 0.47730191699258934
Loss in iteration 109 : 0.46068982634827293
Loss in iteration 110 : 0.4548316477301886
Loss in iteration 111 : 0.4650241387600182
Loss in iteration 112 : 0.45805767555313454
Loss in iteration 113 : 0.45283814259405963
Loss in iteration 114 : 0.456153809819785
Loss in iteration 115 : 0.45302783833502885
Loss in iteration 116 : 0.4643641266367508
Loss in iteration 117 : 0.4710366163166519
Loss in iteration 118 : 0.4701492238839447
Loss in iteration 119 : 0.4550825524808308
Loss in iteration 120 : 0.47090345300065856
Loss in iteration 121 : 0.4658001275790047
Loss in iteration 122 : 0.47406686851997565
Loss in iteration 123 : 0.4657671876679697
Loss in iteration 124 : 0.45786638404073055
Loss in iteration 125 : 0.45695385384578
Loss in iteration 126 : 0.4579119008460715
Loss in iteration 127 : 0.4591411515235619
Loss in iteration 128 : 0.46462817531070455
Loss in iteration 129 : 0.45990916296833534
Loss in iteration 130 : 0.4671996033329172
Loss in iteration 131 : 0.46068060290187446
Loss in iteration 132 : 0.45649735380178685
Loss in iteration 133 : 0.4653641432229771
Loss in iteration 134 : 0.46297791899317137
Loss in iteration 135 : 0.46304543766688666
Loss in iteration 136 : 0.46110888659993
Loss in iteration 137 : 0.4676855798995456
Loss in iteration 138 : 0.4668878162287747
Loss in iteration 139 : 0.4717855419222478
Loss in iteration 140 : 0.4666651854097261
Loss in iteration 141 : 0.4682070088370808
Loss in iteration 142 : 0.45655237972322443
Loss in iteration 143 : 0.4640000334789296
Loss in iteration 144 : 0.45516322226812866
Loss in iteration 145 : 0.45038475377140175
Loss in iteration 146 : 0.46642802019272617
Loss in iteration 147 : 0.47126234620237456
Loss in iteration 148 : 0.46120984295694223
Loss in iteration 149 : 0.4748443858241915
Loss in iteration 150 : 0.4661115327100563
Loss in iteration 151 : 0.46212179679220505
Loss in iteration 152 : 0.47130054466930077
Loss in iteration 153 : 0.44818379013118076
Loss in iteration 154 : 0.4707275163788725
Loss in iteration 155 : 0.46669846466398696
Loss in iteration 156 : 0.44860419985909816
Loss in iteration 157 : 0.4660020244746967
Loss in iteration 158 : 0.4588846724512302
Loss in iteration 159 : 0.4596272365994451
Loss in iteration 160 : 0.45866857533907135
Loss in iteration 161 : 0.45725111781271993
Loss in iteration 162 : 0.46017009904365075
Loss in iteration 163 : 0.4652394795483031
Loss in iteration 164 : 0.46429500181016464
Loss in iteration 165 : 0.4579298871096612
Loss in iteration 166 : 0.4607069687150752
Loss in iteration 167 : 0.46845014230661447
Loss in iteration 168 : 0.4709848282366817
Loss in iteration 169 : 0.45890258146023083
Loss in iteration 170 : 0.46440004713112
Loss in iteration 171 : 0.45483935699739947
Loss in iteration 172 : 0.4655177201528647
Loss in iteration 173 : 0.457482223198468
Loss in iteration 174 : 0.4549586899331099
Loss in iteration 175 : 0.4691943368991627
Loss in iteration 176 : 0.4548807137712736
Loss in iteration 177 : 0.46849898986182503
Loss in iteration 178 : 0.4584003930562216
Loss in iteration 179 : 0.4662035112123776
Loss in iteration 180 : 0.4544189214457781
Loss in iteration 181 : 0.4658924756182477
Loss in iteration 182 : 0.45972967462947306
Loss in iteration 183 : 0.45812432159840355
Loss in iteration 184 : 0.4755715401784147
Loss in iteration 185 : 0.4593086904325111
Loss in iteration 186 : 0.45053117488850586
Loss in iteration 187 : 0.45599387191454593
Loss in iteration 188 : 0.4681989340932018
Loss in iteration 189 : 0.465987180968927
Loss in iteration 190 : 0.44982177791385497
Loss in iteration 191 : 0.45403293917398074
Loss in iteration 192 : 0.4588041908653572
Loss in iteration 193 : 0.4628666593965986
Loss in iteration 194 : 0.4628103003456083
Loss in iteration 195 : 0.46963785470376523
Loss in iteration 196 : 0.47574797297685584
Loss in iteration 197 : 0.45197129485859894
Loss in iteration 198 : 0.47088225496630437
Loss in iteration 199 : 0.4609208158235787
Loss in iteration 200 : 0.4518200503002729
Loss in iteration 201 : 0.4583607473516034
Loss in iteration 202 : 0.4712356513388078
Loss in iteration 203 : 0.44972193562180385
Loss in iteration 204 : 0.45658026420089287
Loss in iteration 205 : 0.4614280783165387
Loss in iteration 206 : 0.4600428918288955
Loss in iteration 207 : 0.45527885565761517
Loss in iteration 208 : 0.4506897298607707
Loss in iteration 209 : 0.4657543253058791
Loss in iteration 210 : 0.4714201724796551
Loss in iteration 211 : 0.46299558362682547
Loss in iteration 212 : 0.4686038182241934
Loss in iteration 213 : 0.4690036019749991
Loss in iteration 214 : 0.4815344033495027
Loss in iteration 215 : 0.4700433480017297
Loss in iteration 216 : 0.45883923944631455
Loss in iteration 217 : 0.45878760153925674
Loss in iteration 218 : 0.4550355728270841
Loss in iteration 219 : 0.4685536524471371
Loss in iteration 220 : 0.4675843280749116
Loss in iteration 221 : 0.4629893884983847
Loss in iteration 222 : 0.4550316214330508
Loss in iteration 223 : 0.4661113809800002
Loss in iteration 224 : 0.4694578904777099
Loss in iteration 225 : 0.4594519922735747
Loss in iteration 226 : 0.46318947361427737
Loss in iteration 227 : 0.45284563579725745
Loss in iteration 228 : 0.46481532185916785
Loss in iteration 229 : 0.46376875848617477
Loss in iteration 230 : 0.4553227813588259
Loss in iteration 231 : 0.4626488875656582
Loss in iteration 232 : 0.466823788802747
Loss in iteration 233 : 0.45846372643365985
Loss in iteration 234 : 0.46498273979630195
Loss in iteration 235 : 0.45670803503426105
Loss in iteration 236 : 0.4624303201102055
Loss in iteration 237 : 0.4624581767225825
Loss in iteration 238 : 0.4588749073333173
Loss in iteration 239 : 0.46176021911598863
Loss in iteration 240 : 0.451796041420933
Loss in iteration 241 : 0.4559722124242701
Loss in iteration 242 : 0.4833503748730429
Loss in iteration 243 : 0.4698733548843999
Loss in iteration 244 : 0.4601181234276565
Loss in iteration 245 : 0.473612047053562
Loss in iteration 246 : 0.45703161763086925
Loss in iteration 247 : 0.4529804395372991
Loss in iteration 248 : 0.4631177899497906
Loss in iteration 249 : 0.46490014536436003
Loss in iteration 250 : 0.4550073213796546
Loss in iteration 251 : 0.4663886735215791
Loss in iteration 252 : 0.46127163572944374
Loss in iteration 253 : 0.47136762052130526
Loss in iteration 254 : 0.4476317594344924
Loss in iteration 255 : 0.4515177061861469
Loss in iteration 256 : 0.47495307409193843
Loss in iteration 257 : 0.44976120756288457
Loss in iteration 258 : 0.45745647506654885
Loss in iteration 259 : 0.4565393813035294
Loss in iteration 260 : 0.45993744179410767
Loss in iteration 261 : 0.4481136254718389
Loss in iteration 262 : 0.46109011701331504
Loss in iteration 263 : 0.46098295523425303
Loss in iteration 264 : 0.47422827307226817
Loss in iteration 265 : 0.45040224877397844
Loss in iteration 266 : 0.4700947165545857
Loss in iteration 267 : 0.46680900241230383
Loss in iteration 268 : 0.47192967135228925
Loss in iteration 269 : 0.4535622128561973
Loss in iteration 270 : 0.4591642416929266
Loss in iteration 271 : 0.4475308587335551
Loss in iteration 272 : 0.4595067139046861
Loss in iteration 273 : 0.44620135263053046
Loss in iteration 274 : 0.4559896406142158
Loss in iteration 275 : 0.4652912674147489
Loss in iteration 276 : 0.46859115112312827
Loss in iteration 277 : 0.45357551986106914
Loss in iteration 278 : 0.45046965617209483
Loss in iteration 279 : 0.4499931727263375
Loss in iteration 280 : 0.4561236875803818
Loss in iteration 281 : 0.4632091715398355
Loss in iteration 282 : 0.45798088626072275
Loss in iteration 283 : 0.46224139067388587
Loss in iteration 284 : 0.4477577390193948
Loss in iteration 285 : 0.4496444897077213
Loss in iteration 286 : 0.4598468312130137
Loss in iteration 287 : 0.4650835693004296
Loss in iteration 288 : 0.4640502307371335
Loss in iteration 289 : 0.45733598413662285
Loss in iteration 290 : 0.4507429294106303
Loss in iteration 291 : 0.4652854149061842
Loss in iteration 292 : 0.4610500999062443
Loss in iteration 293 : 0.4567021925570418
Loss in iteration 294 : 0.46578684546821375
Loss in iteration 295 : 0.45192195806634566
Loss in iteration 296 : 0.45626595815337984
Loss in iteration 297 : 0.4641595723871338
Loss in iteration 298 : 0.46018596457206223
Loss in iteration 299 : 0.4621808918198262
Loss in iteration 300 : 0.4650221498676207
Loss in iteration 301 : 0.4638190855385299
Loss in iteration 302 : 0.45681357953523
Loss in iteration 303 : 0.4521187548292274
Loss in iteration 304 : 0.45434512744556416
Loss in iteration 305 : 0.45807701771437875
Loss in iteration 306 : 0.4586586778663186
Loss in iteration 307 : 0.4594362826126894
Loss in iteration 308 : 0.4591597246773112
Loss in iteration 309 : 0.4506612817959353
Loss in iteration 310 : 0.4574349351114714
Loss in iteration 311 : 0.46723918474922815
Loss in iteration 312 : 0.4782122113385019
Loss in iteration 313 : 0.4471537110633767
Loss in iteration 314 : 0.4553577905739685
Loss in iteration 315 : 0.45909459455815904
Loss in iteration 316 : 0.47833718300975814
Loss in iteration 317 : 0.46081606205060277
Loss in iteration 318 : 0.47073715605682864
Loss in iteration 319 : 0.45613797666603983
Loss in iteration 320 : 0.45490218378531716
Loss in iteration 321 : 0.453894861924697
Loss in iteration 322 : 0.45987373075816884
Loss in iteration 323 : 0.4569037519529909
Loss in iteration 324 : 0.4575881212901726
Loss in iteration 325 : 0.457484628845679
Loss in iteration 326 : 0.4732244558982216
Loss in iteration 327 : 0.4512433592603022
Loss in iteration 328 : 0.4530296766913214
Loss in iteration 329 : 0.45693195816531884
Loss in iteration 330 : 0.4512197230844953
Loss in iteration 331 : 0.46060571915663473
Loss in iteration 332 : 0.44685586125939253
Loss in iteration 333 : 0.4475202243580285
Loss in iteration 334 : 0.4573512552859037
Loss in iteration 335 : 0.45210063292773406
Loss in iteration 336 : 0.4588481601019189
Loss in iteration 337 : 0.46388458480152006
Loss in iteration 338 : 0.4679850713748869
Loss in iteration 339 : 0.47205750507942223
Loss in iteration 340 : 0.46465514071421543
Loss in iteration 341 : 0.4572421331220199
Loss in iteration 342 : 0.458645190572649
Loss in iteration 343 : 0.4580065669778987
Loss in iteration 344 : 0.4648281223449196
Loss in iteration 345 : 0.4588182356745531
Loss in iteration 346 : 0.4520570998275761
Loss in iteration 347 : 0.4589177809810865
Loss in iteration 348 : 0.46234570582408646
Loss in iteration 349 : 0.46568146401687865
Loss in iteration 350 : 0.46614673382059557
Loss in iteration 351 : 0.4618319690986108
Loss in iteration 352 : 0.4577816075859126
Loss in iteration 353 : 0.4515552795736408
Loss in iteration 354 : 0.4594932261719339
Loss in iteration 355 : 0.46004501442267937
Loss in iteration 356 : 0.4525514374876316
Loss in iteration 357 : 0.45699398250674084
Loss in iteration 358 : 0.46714404280032795
Loss in iteration 359 : 0.45601614630295545
Loss in iteration 360 : 0.4645134006118672
Loss in iteration 361 : 0.4497040141049916
Loss in iteration 362 : 0.4700885921873097
Loss in iteration 363 : 0.4608794977689548
Loss in iteration 364 : 0.4588715780780875
Loss in iteration 365 : 0.4619370497214173
Loss in iteration 366 : 0.4560248613549182
Loss in iteration 367 : 0.4663768752484871
Loss in iteration 368 : 0.4666500066456351
Loss in iteration 369 : 0.4716832150537229
Loss in iteration 370 : 0.45591740935593866
Loss in iteration 371 : 0.4736079635727079
Loss in iteration 372 : 0.4540155543012795
Loss in iteration 373 : 0.46016693819084586
Loss in iteration 374 : 0.45706502697274054
Loss in iteration 375 : 0.4564035731085438
Loss in iteration 376 : 0.4637701825526079
Loss in iteration 377 : 0.4599437560911032
Loss in iteration 378 : 0.4628506536203111
Loss in iteration 379 : 0.4628715663714634
Loss in iteration 380 : 0.4574825575793069
Loss in iteration 381 : 0.4481439237638978
Loss in iteration 382 : 0.45471048889265675
Loss in iteration 383 : 0.4622923037879959
Loss in iteration 384 : 0.46740122776774995
Loss in iteration 385 : 0.46056626739630196
Loss in iteration 386 : 0.44766787973061584
Loss in iteration 387 : 0.4651090769341275
Loss in iteration 388 : 0.46969738151826923
Loss in iteration 389 : 0.4560819940385348
Loss in iteration 390 : 0.45714657915322915
Loss in iteration 391 : 0.46057491341712065
Loss in iteration 392 : 0.4500388298113463
Loss in iteration 393 : 0.45388974242608005
Loss in iteration 394 : 0.45698734968696786
Loss in iteration 395 : 0.46481847368006907
Loss in iteration 396 : 0.45765059198792046
Loss in iteration 397 : 0.4719001072495086
Loss in iteration 398 : 0.4681405376123615
Loss in iteration 399 : 0.45553607690689435
Loss in iteration 400 : 0.450169212261016
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.787375, training accuracy 0.787375, time elapsed: 6129 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6806601158956832
Loss in iteration 3 : 0.6630370613324145
Loss in iteration 4 : 0.6530966420277673
Loss in iteration 5 : 0.6271155132249803
Loss in iteration 6 : 0.616960020703347
Loss in iteration 7 : 0.6012576573175114
Loss in iteration 8 : 0.5798391538676869
Loss in iteration 9 : 0.5649133782629014
Loss in iteration 10 : 0.553418820800779
Loss in iteration 11 : 0.5355440903824226
Loss in iteration 12 : 0.5244624637541871
Loss in iteration 13 : 0.5159759951066826
Loss in iteration 14 : 0.5077635959077539
Loss in iteration 15 : 0.5112112895790134
Loss in iteration 16 : 0.5035604343208693
Loss in iteration 17 : 0.49489338436705427
Loss in iteration 18 : 0.49180838356045287
Loss in iteration 19 : 0.4929691945365835
Loss in iteration 20 : 0.484317428047646
Loss in iteration 21 : 0.498428832360315
Loss in iteration 22 : 0.4951292357485903
Loss in iteration 23 : 0.4919484162718673
Loss in iteration 24 : 0.4830881787407674
Loss in iteration 25 : 0.48903327548794784
Loss in iteration 26 : 0.47569158057255095
Loss in iteration 27 : 0.47830408250971373
Loss in iteration 28 : 0.4829202063550828
Loss in iteration 29 : 0.49110977838592806
Loss in iteration 30 : 0.4887691719645206
Loss in iteration 31 : 0.48252604873519805
Loss in iteration 32 : 0.48051064340713934
Loss in iteration 33 : 0.47453620085908665
Loss in iteration 34 : 0.4754817100162402
Loss in iteration 35 : 0.4848671568482234
Loss in iteration 36 : 0.48443416679205226
Loss in iteration 37 : 0.47688025686804913
Loss in iteration 38 : 0.4755402085361052
Loss in iteration 39 : 0.46898679587770997
Loss in iteration 40 : 0.4760068975713598
Loss in iteration 41 : 0.4798457726442374
Loss in iteration 42 : 0.46718690476204766
Loss in iteration 43 : 0.4759086245648424
Loss in iteration 44 : 0.4760114921084531
Loss in iteration 45 : 0.47627607046729914
Loss in iteration 46 : 0.47542083396976587
Loss in iteration 47 : 0.47040218036833453
Loss in iteration 48 : 0.47528926845335484
Loss in iteration 49 : 0.46808093634873804
Loss in iteration 50 : 0.460558782082339
Loss in iteration 51 : 0.47859867195110234
Loss in iteration 52 : 0.4662631215148333
Loss in iteration 53 : 0.46570085685232754
Loss in iteration 54 : 0.4632487032788183
Loss in iteration 55 : 0.46747170072112815
Loss in iteration 56 : 0.47214099785030306
Loss in iteration 57 : 0.4550091259109048
Loss in iteration 58 : 0.4599305139079457
Loss in iteration 59 : 0.45838224983540343
Loss in iteration 60 : 0.46731250649496914
Loss in iteration 61 : 0.4646768748460212
Loss in iteration 62 : 0.46740332141402996
Loss in iteration 63 : 0.46536849492018056
Loss in iteration 64 : 0.46425751456874104
Loss in iteration 65 : 0.47571852550405425
Loss in iteration 66 : 0.457951115985491
Loss in iteration 67 : 0.46985993249761704
Loss in iteration 68 : 0.45989672006764376
Loss in iteration 69 : 0.4687238797479378
Loss in iteration 70 : 0.45874170201154923
Loss in iteration 71 : 0.473765099531457
Loss in iteration 72 : 0.45928821888169463
Loss in iteration 73 : 0.47025085897048674
Loss in iteration 74 : 0.46172127756192355
Loss in iteration 75 : 0.4634594436257774
Loss in iteration 76 : 0.47500481298011393
Loss in iteration 77 : 0.4577341269137386
Loss in iteration 78 : 0.46694526702759587
Loss in iteration 79 : 0.4473835077156905
Loss in iteration 80 : 0.4645048902378649
Loss in iteration 81 : 0.4665179120715201
Loss in iteration 82 : 0.4637472592187315
Loss in iteration 83 : 0.46470282082504605
Loss in iteration 84 : 0.47589221152159944
Loss in iteration 85 : 0.46498679364341866
Loss in iteration 86 : 0.47325200603706163
Loss in iteration 87 : 0.4535878307024858
Loss in iteration 88 : 0.4733000930174256
Loss in iteration 89 : 0.45691793868237895
Loss in iteration 90 : 0.46065807520740204
Loss in iteration 91 : 0.46920907724883354
Loss in iteration 92 : 0.45799416623469313
Loss in iteration 93 : 0.4627056257122782
Loss in iteration 94 : 0.4614195863587874
Loss in iteration 95 : 0.4599031826592932
Loss in iteration 96 : 0.47003401875941186
Loss in iteration 97 : 0.4622265181912849
Loss in iteration 98 : 0.4681523602727456
Loss in iteration 99 : 0.4600923458135245
Loss in iteration 100 : 0.46553194988477464
Loss in iteration 101 : 0.4600308825055657
Loss in iteration 102 : 0.45601335577291313
Loss in iteration 103 : 0.4679684764968776
Loss in iteration 104 : 0.4626857367205662
Loss in iteration 105 : 0.4481507493778498
Loss in iteration 106 : 0.4627864615269978
Loss in iteration 107 : 0.47656193305724914
Loss in iteration 108 : 0.4775898777402032
Loss in iteration 109 : 0.4613496399848226
Loss in iteration 110 : 0.4557556016320488
Loss in iteration 111 : 0.46575820392630013
Loss in iteration 112 : 0.45851969305264395
Loss in iteration 113 : 0.45352124528935
Loss in iteration 114 : 0.45632641153457093
Loss in iteration 115 : 0.45363808715333487
Loss in iteration 116 : 0.464681162561609
Loss in iteration 117 : 0.47160071146963123
Loss in iteration 118 : 0.4700332942033619
Loss in iteration 119 : 0.45558627981488065
Loss in iteration 120 : 0.47143937310831585
Loss in iteration 121 : 0.4660581696277453
Loss in iteration 122 : 0.47433733640043674
Loss in iteration 123 : 0.465962392011039
Loss in iteration 124 : 0.4585926590414166
Loss in iteration 125 : 0.4576489766087053
Loss in iteration 126 : 0.4578036110896954
Loss in iteration 127 : 0.4597212361416087
Loss in iteration 128 : 0.4653846099024049
Loss in iteration 129 : 0.4599146343443488
Loss in iteration 130 : 0.46658818687460224
Loss in iteration 131 : 0.4612046113787344
Loss in iteration 132 : 0.457553113013612
Loss in iteration 133 : 0.46490221098496703
Loss in iteration 134 : 0.46274607684992275
Loss in iteration 135 : 0.4634802501599521
Loss in iteration 136 : 0.4624196951725995
Loss in iteration 137 : 0.4680615995403791
Loss in iteration 138 : 0.46655968459105346
Loss in iteration 139 : 0.4719716768205133
Loss in iteration 140 : 0.4663223762537874
Loss in iteration 141 : 0.46838782006098373
Loss in iteration 142 : 0.456674808592338
Loss in iteration 143 : 0.4642631047409902
Loss in iteration 144 : 0.4562709787547291
Loss in iteration 145 : 0.4501691142827324
Loss in iteration 146 : 0.46698566632655936
Loss in iteration 147 : 0.47149874080478965
Loss in iteration 148 : 0.46229057628499287
Loss in iteration 149 : 0.47532333321298836
Loss in iteration 150 : 0.4653892243150842
Loss in iteration 151 : 0.4621056784087569
Loss in iteration 152 : 0.4723658028641265
Loss in iteration 153 : 0.44825043148636773
Loss in iteration 154 : 0.47002949936427346
Loss in iteration 155 : 0.4671134402759122
Loss in iteration 156 : 0.4501187136700998
Loss in iteration 157 : 0.46485783097170347
Loss in iteration 158 : 0.458183733073884
Loss in iteration 159 : 0.4598677765382856
Loss in iteration 160 : 0.45854655029608427
Loss in iteration 161 : 0.456569445249781
Loss in iteration 162 : 0.4600025410339754
Loss in iteration 163 : 0.4656836153871756
Loss in iteration 164 : 0.4650428959755718
Loss in iteration 165 : 0.4571268918829077
Loss in iteration 166 : 0.4602680452276432
Loss in iteration 167 : 0.46880851339507396
Loss in iteration 168 : 0.4710111824414241
Loss in iteration 169 : 0.4584218299495964
Loss in iteration 170 : 0.4648405096668987
Loss in iteration 171 : 0.4555162505154998
Loss in iteration 172 : 0.4653201499183557
Loss in iteration 173 : 0.45777225823401985
Loss in iteration 174 : 0.4552497920688901
Loss in iteration 175 : 0.4696038551990612
Loss in iteration 176 : 0.4554022682853978
Loss in iteration 177 : 0.46892685547730123
Loss in iteration 178 : 0.4583934299459322
Loss in iteration 179 : 0.4665832323247168
Loss in iteration 180 : 0.454754771545861
Loss in iteration 181 : 0.46546762966379457
Loss in iteration 182 : 0.4601025627016261
Loss in iteration 183 : 0.45842500102938033
Loss in iteration 184 : 0.4763002057717344
Loss in iteration 185 : 0.45988488432622826
Loss in iteration 186 : 0.4508031906371699
Loss in iteration 187 : 0.4567135096729391
Loss in iteration 188 : 0.46842993979451186
Loss in iteration 189 : 0.4659120651298541
Loss in iteration 190 : 0.45022393046168807
Loss in iteration 191 : 0.4550726155446894
Loss in iteration 192 : 0.4591564229485374
Loss in iteration 193 : 0.46289455259217294
Loss in iteration 194 : 0.4624699084848649
Loss in iteration 195 : 0.4704142569380396
Loss in iteration 196 : 0.47690429091025066
Loss in iteration 197 : 0.4523856472024018
Loss in iteration 198 : 0.4707158533412743
Loss in iteration 199 : 0.46089673469859627
Loss in iteration 200 : 0.45187187198301243
Loss in iteration 201 : 0.45865866882129014
Loss in iteration 202 : 0.4714621725184533
Loss in iteration 203 : 0.45007039034515156
Loss in iteration 204 : 0.4578162729806986
Loss in iteration 205 : 0.4602496522266294
Loss in iteration 206 : 0.4602077143018667
Loss in iteration 207 : 0.4562594443545541
Loss in iteration 208 : 0.4505839080993583
Loss in iteration 209 : 0.4651648470650064
Loss in iteration 210 : 0.47117354151227664
Loss in iteration 211 : 0.462980240475258
Loss in iteration 212 : 0.4681252986782655
Loss in iteration 213 : 0.4694950571063061
Loss in iteration 214 : 0.48172477733298386
Loss in iteration 215 : 0.4708598823847034
Loss in iteration 216 : 0.4588228526524836
Loss in iteration 217 : 0.4585192969119255
Loss in iteration 218 : 0.4552536885064685
Loss in iteration 219 : 0.4686745264830271
Loss in iteration 220 : 0.46770995509575264
Loss in iteration 221 : 0.46371767081834603
Loss in iteration 222 : 0.4548137450718252
Loss in iteration 223 : 0.4666942355341556
Loss in iteration 224 : 0.4692560910783111
Loss in iteration 225 : 0.4596316722232234
Loss in iteration 226 : 0.46317053010053205
Loss in iteration 227 : 0.45286384694320475
Loss in iteration 228 : 0.4649016349659634
Loss in iteration 229 : 0.46403546232288867
Loss in iteration 230 : 0.4556813212630484
Loss in iteration 231 : 0.4629296119652792
Loss in iteration 232 : 0.46670345857221296
Loss in iteration 233 : 0.4591382042081385
Loss in iteration 234 : 0.4654668888867686
Loss in iteration 235 : 0.456945446387404
Loss in iteration 236 : 0.4626854041443133
Loss in iteration 237 : 0.4629441929263011
Loss in iteration 238 : 0.4590511587725711
Loss in iteration 239 : 0.46190094216796884
Loss in iteration 240 : 0.4520159997738163
Loss in iteration 241 : 0.456412608947678
Loss in iteration 242 : 0.4837383182578364
Loss in iteration 243 : 0.4702744205654115
Loss in iteration 244 : 0.46015196335699415
Loss in iteration 245 : 0.4736772446871177
Loss in iteration 246 : 0.45763355198106315
Loss in iteration 247 : 0.4531963737196359
Loss in iteration 248 : 0.46322034793497435
Loss in iteration 249 : 0.46517903038169595
Loss in iteration 250 : 0.45533367725496
Loss in iteration 251 : 0.4667127461383745
Loss in iteration 252 : 0.4614540112491681
Loss in iteration 253 : 0.47149808577125624
Loss in iteration 254 : 0.44800414224134844
Loss in iteration 255 : 0.4519951242448114
Loss in iteration 256 : 0.4752357529421653
Loss in iteration 257 : 0.4500753556465398
Loss in iteration 258 : 0.45737000809411604
Loss in iteration 259 : 0.4567079936166231
Loss in iteration 260 : 0.4602396762280837
Loss in iteration 261 : 0.4486722409248128
Loss in iteration 262 : 0.46155246435820885
Loss in iteration 263 : 0.4607470947113566
Loss in iteration 264 : 0.4738386150471706
Loss in iteration 265 : 0.4505860083220969
Loss in iteration 266 : 0.4703876990952982
Loss in iteration 267 : 0.4673151168594407
Loss in iteration 268 : 0.4722318446150423
Loss in iteration 269 : 0.453857176261381
Loss in iteration 270 : 0.45913076757215354
Loss in iteration 271 : 0.44760239406862773
Loss in iteration 272 : 0.4599477564999628
Loss in iteration 273 : 0.4464362986500284
Loss in iteration 274 : 0.4561149174131302
Loss in iteration 275 : 0.4657712525327242
Loss in iteration 276 : 0.46918405303012795
Loss in iteration 277 : 0.453094702626142
Loss in iteration 278 : 0.4504277174921339
Loss in iteration 279 : 0.45003533110917043
Loss in iteration 280 : 0.45637815089235434
Loss in iteration 281 : 0.46274228937976847
Loss in iteration 282 : 0.4574919671360649
Loss in iteration 283 : 0.46249136341598407
Loss in iteration 284 : 0.44826731627406635
Loss in iteration 285 : 0.44919315284541844
Loss in iteration 286 : 0.4599515381128472
Loss in iteration 287 : 0.4656114840155161
Loss in iteration 288 : 0.4642311643497674
Loss in iteration 289 : 0.456375719091609
Loss in iteration 290 : 0.45082702651062634
Loss in iteration 291 : 0.4654658810975354
Loss in iteration 292 : 0.46153127584663134
Loss in iteration 293 : 0.4565869688129385
Loss in iteration 294 : 0.4667121514859249
Loss in iteration 295 : 0.45202473910123186
Loss in iteration 296 : 0.45504837846947027
Loss in iteration 297 : 0.464360278479637
Loss in iteration 298 : 0.4602090165762136
Loss in iteration 299 : 0.46188011753005265
Loss in iteration 300 : 0.4654465655305774
Loss in iteration 301 : 0.4641099043110388
Loss in iteration 302 : 0.4569675860026644
Loss in iteration 303 : 0.4520021852407571
Loss in iteration 304 : 0.45469399405424143
Loss in iteration 305 : 0.45829531684718205
Loss in iteration 306 : 0.45892655055001713
Loss in iteration 307 : 0.4594710954967331
Loss in iteration 308 : 0.4593471588836608
Loss in iteration 309 : 0.4508398181958471
Loss in iteration 310 : 0.4575885211062923
Loss in iteration 311 : 0.4675142984485209
Loss in iteration 312 : 0.47869958746436436
Loss in iteration 313 : 0.4478762821659344
Loss in iteration 314 : 0.4551851869487685
Loss in iteration 315 : 0.45857526699643975
Loss in iteration 316 : 0.47799889075353785
Loss in iteration 317 : 0.46124629521403504
Loss in iteration 318 : 0.4707288280643062
Loss in iteration 319 : 0.45527045097115276
Loss in iteration 320 : 0.4545808191252351
Loss in iteration 321 : 0.4536041632200072
Loss in iteration 322 : 0.4594711797759263
Loss in iteration 323 : 0.45679511902116643
Loss in iteration 324 : 0.4578328739522979
Loss in iteration 325 : 0.4568748876388831
Loss in iteration 326 : 0.4727292597441183
Loss in iteration 327 : 0.4513487646774781
Loss in iteration 328 : 0.4532120821309677
Loss in iteration 329 : 0.4559676398665019
Loss in iteration 330 : 0.45096134190126747
Loss in iteration 331 : 0.4602141607525686
Loss in iteration 332 : 0.44682122766739785
Loss in iteration 333 : 0.4474841748922388
Loss in iteration 334 : 0.457602053121183
Loss in iteration 335 : 0.4516582102426596
Loss in iteration 336 : 0.4587341900835589
Loss in iteration 337 : 0.46313444832988937
Loss in iteration 338 : 0.4681195338115189
Loss in iteration 339 : 0.4717083188292751
Loss in iteration 340 : 0.4642955822879825
Loss in iteration 341 : 0.45673503637780927
Loss in iteration 342 : 0.4587749213243515
Loss in iteration 343 : 0.45827177168431926
Loss in iteration 344 : 0.4646817561546256
Loss in iteration 345 : 0.45825731822932614
Loss in iteration 346 : 0.451987330705334
Loss in iteration 347 : 0.4591793285486661
Loss in iteration 348 : 0.4615355509129847
Loss in iteration 349 : 0.4658665380752345
Loss in iteration 350 : 0.4665292575484171
Loss in iteration 351 : 0.46110580012478863
Loss in iteration 352 : 0.4573478666450956
Loss in iteration 353 : 0.4517869259992971
Loss in iteration 354 : 0.4593637064009224
Loss in iteration 355 : 0.45939216545028994
Loss in iteration 356 : 0.45271470468325076
Loss in iteration 357 : 0.45712381280142916
Loss in iteration 358 : 0.4664809085357613
Loss in iteration 359 : 0.4561556686990994
Loss in iteration 360 : 0.4645490979434991
Loss in iteration 361 : 0.4496752964552472
Loss in iteration 362 : 0.4701050218686737
Loss in iteration 363 : 0.46128338576509353
Loss in iteration 364 : 0.45878086837754845
Loss in iteration 365 : 0.4620042103615787
Loss in iteration 366 : 0.4562559957137788
Loss in iteration 367 : 0.46648818140380405
Loss in iteration 368 : 0.46679670036477544
Loss in iteration 369 : 0.47164529154163304
Loss in iteration 370 : 0.4561714131305096
Loss in iteration 371 : 0.47381484784212297
Loss in iteration 372 : 0.45416185211696064
Loss in iteration 373 : 0.45984063429211613
Loss in iteration 374 : 0.4568865712705742
Loss in iteration 375 : 0.4559973523599099
Loss in iteration 376 : 0.46414390496721775
Loss in iteration 377 : 0.46030577732073474
Loss in iteration 378 : 0.46257950299208883
Loss in iteration 379 : 0.46262990547169325
Loss in iteration 380 : 0.4575196437904987
Loss in iteration 381 : 0.44829878709674537
Loss in iteration 382 : 0.45492570172215613
Loss in iteration 383 : 0.46213257919215306
Loss in iteration 384 : 0.46749357357469157
Loss in iteration 385 : 0.46024838725833345
Loss in iteration 386 : 0.4476455429943401
Loss in iteration 387 : 0.4653105086024706
Loss in iteration 388 : 0.46960478541533784
Loss in iteration 389 : 0.45615400373853143
Loss in iteration 390 : 0.4574124341548847
Loss in iteration 391 : 0.46048694461193296
Loss in iteration 392 : 0.44994794712151204
Loss in iteration 393 : 0.4538640689082702
Loss in iteration 394 : 0.4571895695277364
Loss in iteration 395 : 0.46487323988816764
Loss in iteration 396 : 0.4578751192869485
Loss in iteration 397 : 0.4719483130555453
Loss in iteration 398 : 0.4679287675617746
Loss in iteration 399 : 0.45572843579214145
Loss in iteration 400 : 0.4504285821584242
Testing accuracy  of updater 9 on alg 0 with rate 0.14 = 0.788625, training accuracy 0.788625, time elapsed: 6699 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6835996204294794
Loss in iteration 3 : 0.6688873634677985
Loss in iteration 4 : 0.6638472800385141
Loss in iteration 5 : 0.6446008801509197
Loss in iteration 6 : 0.633093861531496
Loss in iteration 7 : 0.6205962756496477
Loss in iteration 8 : 0.6075100706187925
Loss in iteration 9 : 0.5896122701129046
Loss in iteration 10 : 0.5765227013780508
Loss in iteration 11 : 0.5616089790633797
Loss in iteration 12 : 0.551332445095152
Loss in iteration 13 : 0.5407641181645284
Loss in iteration 14 : 0.530581474635186
Loss in iteration 15 : 0.5317793458731603
Loss in iteration 16 : 0.5222368938027475
Loss in iteration 17 : 0.514993161388146
Loss in iteration 18 : 0.5092733701301818
Loss in iteration 19 : 0.50943115920568
Loss in iteration 20 : 0.5014273446582849
Loss in iteration 21 : 0.5097037079740515
Loss in iteration 22 : 0.5050591424311516
Loss in iteration 23 : 0.5022317052637322
Loss in iteration 24 : 0.49268394272112204
Loss in iteration 25 : 0.49713513041174184
Loss in iteration 26 : 0.4852335785431778
Loss in iteration 27 : 0.4862143538067683
Loss in iteration 28 : 0.48946521141485877
Loss in iteration 29 : 0.49714082070054966
Loss in iteration 30 : 0.49444295598716514
Loss in iteration 31 : 0.4885438888342639
Loss in iteration 32 : 0.486667676349275
Loss in iteration 33 : 0.48085647248516494
Loss in iteration 34 : 0.4808717798211443
Loss in iteration 35 : 0.48954324842941555
Loss in iteration 36 : 0.4889127001341278
Loss in iteration 37 : 0.48198769567697664
Loss in iteration 38 : 0.4811565556523604
Loss in iteration 39 : 0.4752628344844931
Loss in iteration 40 : 0.48057135754431884
Loss in iteration 41 : 0.4847646625871481
Loss in iteration 42 : 0.4729190086758821
Loss in iteration 43 : 0.48173602933930987
Loss in iteration 44 : 0.481513833701447
Loss in iteration 45 : 0.48106165267492657
Loss in iteration 46 : 0.48069500295429746
Loss in iteration 47 : 0.47622982331686536
Loss in iteration 48 : 0.4807571072941695
Loss in iteration 49 : 0.4734852544755395
Loss in iteration 50 : 0.46683168243827244
Loss in iteration 51 : 0.483443945202939
Loss in iteration 52 : 0.4713296897154147
Loss in iteration 53 : 0.47112380572203694
Loss in iteration 54 : 0.4688383573530546
Loss in iteration 55 : 0.4725889598049041
Loss in iteration 56 : 0.4768913920293191
Loss in iteration 57 : 0.460055538561475
Loss in iteration 58 : 0.46518344816529167
Loss in iteration 59 : 0.4643038637411767
Loss in iteration 60 : 0.4715065018410732
Loss in iteration 61 : 0.47004386652369784
Loss in iteration 62 : 0.47201451225119956
Loss in iteration 63 : 0.4696786395825024
Loss in iteration 64 : 0.46830281534066426
Loss in iteration 65 : 0.47915811934489
Loss in iteration 66 : 0.46246450896414115
Loss in iteration 67 : 0.4729634311013153
Loss in iteration 68 : 0.4633968077178154
Loss in iteration 69 : 0.4731035968483621
Loss in iteration 70 : 0.46321087374132675
Loss in iteration 71 : 0.47801512121964224
Loss in iteration 72 : 0.46377178343036646
Loss in iteration 73 : 0.4737461280572376
Loss in iteration 74 : 0.46622946035753804
Loss in iteration 75 : 0.467331637632479
Loss in iteration 76 : 0.4780545644651063
Loss in iteration 77 : 0.4617910547470292
Loss in iteration 78 : 0.47023282074719186
Loss in iteration 79 : 0.45185643405523096
Loss in iteration 80 : 0.4677166518880941
Loss in iteration 81 : 0.4700185170140033
Loss in iteration 82 : 0.4668379335567788
Loss in iteration 83 : 0.46877232664339696
Loss in iteration 84 : 0.4785341354180484
Loss in iteration 85 : 0.46862079759808334
Loss in iteration 86 : 0.47596932517283475
Loss in iteration 87 : 0.45699514417437026
Loss in iteration 88 : 0.4767026773547272
Loss in iteration 89 : 0.460336292394997
Loss in iteration 90 : 0.4634706325630066
Loss in iteration 91 : 0.47198133872274795
Loss in iteration 92 : 0.4609864419279037
Loss in iteration 93 : 0.4655609789429841
Loss in iteration 94 : 0.46472962852141253
Loss in iteration 95 : 0.46254293641835365
Loss in iteration 96 : 0.4733794078082844
Loss in iteration 97 : 0.4647052117420421
Loss in iteration 98 : 0.47030523229782395
Loss in iteration 99 : 0.46348211904827835
Loss in iteration 100 : 0.4685059934140092
Loss in iteration 101 : 0.46235895060412086
Loss in iteration 102 : 0.45961552320585636
Loss in iteration 103 : 0.4709539932926981
Loss in iteration 104 : 0.46552624842431584
Loss in iteration 105 : 0.4518517929939014
Loss in iteration 106 : 0.46441781396225873
Loss in iteration 107 : 0.47727373387909344
Loss in iteration 108 : 0.47924646379911945
Loss in iteration 109 : 0.46395316410290943
Loss in iteration 110 : 0.4581073504150477
Loss in iteration 111 : 0.46831826340484906
Loss in iteration 112 : 0.46114274876565264
Loss in iteration 113 : 0.4560897633159788
Loss in iteration 114 : 0.45830842345005
Loss in iteration 115 : 0.4558938134914902
Loss in iteration 116 : 0.46641011639893454
Loss in iteration 117 : 0.47354945562754736
Loss in iteration 118 : 0.4715707399592434
Loss in iteration 119 : 0.4573303987502038
Loss in iteration 120 : 0.47350995807208424
Loss in iteration 121 : 0.4682604368910959
Loss in iteration 122 : 0.4754757914958713
Loss in iteration 123 : 0.46721681621467326
Loss in iteration 124 : 0.46033114758591703
Loss in iteration 125 : 0.459328364272272
Loss in iteration 126 : 0.4592602955604857
Loss in iteration 127 : 0.46088408991547536
Loss in iteration 128 : 0.4671264152775269
Loss in iteration 129 : 0.4614265263524371
Loss in iteration 130 : 0.4670511685184093
Loss in iteration 131 : 0.46155237248251496
Loss in iteration 132 : 0.4595947225593837
Loss in iteration 133 : 0.4667268314797716
Loss in iteration 134 : 0.46461458565423813
Loss in iteration 135 : 0.4639571366808378
Loss in iteration 136 : 0.46352015665910357
Loss in iteration 137 : 0.4692794447121789
Loss in iteration 138 : 0.46757666177278684
Loss in iteration 139 : 0.47264606583867247
Loss in iteration 140 : 0.46724358679159456
Loss in iteration 141 : 0.4689776762888561
Loss in iteration 142 : 0.45876125492904424
Loss in iteration 143 : 0.4648695922285524
Loss in iteration 144 : 0.45804386772911954
Loss in iteration 145 : 0.45268252468705644
Loss in iteration 146 : 0.4687321202925405
Loss in iteration 147 : 0.47192161821580614
Loss in iteration 148 : 0.4623622686976224
Loss in iteration 149 : 0.47593356347974175
Loss in iteration 150 : 0.4670289836559836
Loss in iteration 151 : 0.46266162313423986
Loss in iteration 152 : 0.47354257992617077
Loss in iteration 153 : 0.4500082849307716
Loss in iteration 154 : 0.47137781832253023
Loss in iteration 155 : 0.46762750988424084
Loss in iteration 156 : 0.45075058515487143
Loss in iteration 157 : 0.4660187072373864
Loss in iteration 158 : 0.45860993209810186
Loss in iteration 159 : 0.46064643691195706
Loss in iteration 160 : 0.45944441104474804
Loss in iteration 161 : 0.4578306209959648
Loss in iteration 162 : 0.46162911611625324
Loss in iteration 163 : 0.4662498044384609
Loss in iteration 164 : 0.4661197892803408
Loss in iteration 165 : 0.4584711934097294
Loss in iteration 166 : 0.4613168911616358
Loss in iteration 167 : 0.4705223035348581
Loss in iteration 168 : 0.47200484503732804
Loss in iteration 169 : 0.4603368760019757
Loss in iteration 170 : 0.46628332209276685
Loss in iteration 171 : 0.45670376735634205
Loss in iteration 172 : 0.4661534243057866
Loss in iteration 173 : 0.45962560231589367
Loss in iteration 174 : 0.45642482862657296
Loss in iteration 175 : 0.4708669524485874
Loss in iteration 176 : 0.45640964902830267
Loss in iteration 177 : 0.4698027613033557
Loss in iteration 178 : 0.459283125907326
Loss in iteration 179 : 0.4676980333048326
Loss in iteration 180 : 0.4564149281381793
Loss in iteration 181 : 0.4660406203520763
Loss in iteration 182 : 0.4609687659113067
Loss in iteration 183 : 0.45939376439309143
Loss in iteration 184 : 0.4771270261111816
Loss in iteration 185 : 0.46108413464314685
Loss in iteration 186 : 0.4520064837875171
Loss in iteration 187 : 0.4574938326071129
Loss in iteration 188 : 0.4689831489147936
Loss in iteration 189 : 0.4665762403092756
Loss in iteration 190 : 0.45136980112484887
Loss in iteration 191 : 0.45587705779304594
Loss in iteration 192 : 0.4604619521507459
Loss in iteration 193 : 0.46430418214348435
Loss in iteration 194 : 0.46247332291778187
Loss in iteration 195 : 0.4705951057439968
Loss in iteration 196 : 0.4770971208541942
Loss in iteration 197 : 0.45331701162320714
Loss in iteration 198 : 0.47091072132845607
Loss in iteration 199 : 0.4615995103896027
Loss in iteration 200 : 0.45274735147876255
Loss in iteration 201 : 0.4603051046621633
Loss in iteration 202 : 0.47099596066717603
Loss in iteration 203 : 0.45097075028323425
Loss in iteration 204 : 0.4581768770191202
Loss in iteration 205 : 0.46142725912589294
Loss in iteration 206 : 0.4613657370966105
Loss in iteration 207 : 0.45718795799203826
Loss in iteration 208 : 0.4524856085333926
Loss in iteration 209 : 0.4663046170273355
Loss in iteration 210 : 0.47154769506653754
Loss in iteration 211 : 0.46394121343095024
Loss in iteration 212 : 0.46891314908134474
Loss in iteration 213 : 0.4700717382279403
Loss in iteration 214 : 0.48293210795068925
Loss in iteration 215 : 0.4716840981645238
Loss in iteration 216 : 0.46022837869883965
Loss in iteration 217 : 0.4596920688752395
Loss in iteration 218 : 0.45639664158758786
Loss in iteration 219 : 0.46882408436194645
Loss in iteration 220 : 0.4683297769251503
Loss in iteration 221 : 0.46463965323842643
Loss in iteration 222 : 0.45569413752985016
Loss in iteration 223 : 0.4677288181856272
Loss in iteration 224 : 0.4697579842652415
Loss in iteration 225 : 0.46055197030335077
Loss in iteration 226 : 0.4642647297644751
Loss in iteration 227 : 0.4539234189593983
Loss in iteration 228 : 0.4655279167959842
Loss in iteration 229 : 0.46457461932850014
Loss in iteration 230 : 0.45701734857204707
Loss in iteration 231 : 0.46400706038633494
Loss in iteration 232 : 0.46717805311546357
Loss in iteration 233 : 0.45973843019329613
Loss in iteration 234 : 0.46627785954966056
Loss in iteration 235 : 0.4578428295397308
Loss in iteration 236 : 0.46338240930835733
Loss in iteration 237 : 0.4635341745168269
Loss in iteration 238 : 0.46021631363955423
Loss in iteration 239 : 0.4624845256476347
Loss in iteration 240 : 0.45370605752009713
Loss in iteration 241 : 0.45676566096125115
Loss in iteration 242 : 0.48398344338835564
Loss in iteration 243 : 0.4711235630626012
Loss in iteration 244 : 0.46067621591125224
Loss in iteration 245 : 0.4740685667107785
Loss in iteration 246 : 0.45880362741115543
Loss in iteration 247 : 0.45405543179971725
Loss in iteration 248 : 0.4638673944048133
Loss in iteration 249 : 0.4661437235201197
Loss in iteration 250 : 0.45572530333913686
Loss in iteration 251 : 0.46740113233566066
Loss in iteration 252 : 0.4618232087592928
Loss in iteration 253 : 0.4720419793826111
Loss in iteration 254 : 0.448146016383933
Loss in iteration 255 : 0.45311488350308565
Loss in iteration 256 : 0.4761925298220267
Loss in iteration 257 : 0.45149602691561963
Loss in iteration 258 : 0.4581931849261094
Loss in iteration 259 : 0.4576592167111462
Loss in iteration 260 : 0.46063569076577515
Loss in iteration 261 : 0.44951159123565804
Loss in iteration 262 : 0.4627018923742047
Loss in iteration 263 : 0.4622474885802914
Loss in iteration 264 : 0.47419209001021706
Loss in iteration 265 : 0.45091233400707065
Loss in iteration 266 : 0.47087794899474317
Loss in iteration 267 : 0.4685986674069387
Loss in iteration 268 : 0.47394689139537455
Loss in iteration 269 : 0.455564194503583
Loss in iteration 270 : 0.46030605838379174
Loss in iteration 271 : 0.4485067767961083
Loss in iteration 272 : 0.46152971212766
Loss in iteration 273 : 0.4477638871939989
Loss in iteration 274 : 0.4567422029465395
Loss in iteration 275 : 0.46602623490435985
Loss in iteration 276 : 0.4695176545121961
Loss in iteration 277 : 0.4537966379949029
Loss in iteration 278 : 0.4520991766293956
Loss in iteration 279 : 0.4499610755071024
Loss in iteration 280 : 0.45676743677485937
Loss in iteration 281 : 0.463496504501304
Loss in iteration 282 : 0.45752312754345403
Loss in iteration 283 : 0.4633983147607726
Loss in iteration 284 : 0.44869918951607973
Loss in iteration 285 : 0.449868339534158
Loss in iteration 286 : 0.46039046739705686
Loss in iteration 287 : 0.46604960147206614
Loss in iteration 288 : 0.4643749713592284
Loss in iteration 289 : 0.45683599263740093
Loss in iteration 290 : 0.45141997007133083
Loss in iteration 291 : 0.4658816366590595
Loss in iteration 292 : 0.4620792822987913
Loss in iteration 293 : 0.4569464520367619
Loss in iteration 294 : 0.467070330872428
Loss in iteration 295 : 0.45277662329473284
Loss in iteration 296 : 0.4559634475911899
Loss in iteration 297 : 0.4651111649165004
Loss in iteration 298 : 0.4609846956948107
Loss in iteration 299 : 0.4626992410973211
Loss in iteration 300 : 0.465721639781286
Loss in iteration 301 : 0.46484820942702415
Loss in iteration 302 : 0.4578825861615526
Loss in iteration 303 : 0.4525367250425263
Loss in iteration 304 : 0.45555786522001296
Loss in iteration 305 : 0.4589059642568174
Loss in iteration 306 : 0.45910197148513393
Loss in iteration 307 : 0.4599344084030048
Loss in iteration 308 : 0.46038591419656455
Loss in iteration 309 : 0.45184270602823035
Loss in iteration 310 : 0.4584427347429927
Loss in iteration 311 : 0.4674804100015474
Loss in iteration 312 : 0.4787106304503163
Loss in iteration 313 : 0.44858595604949003
Loss in iteration 314 : 0.455684030575472
Loss in iteration 315 : 0.45900136144069364
Loss in iteration 316 : 0.47703725346337705
Loss in iteration 317 : 0.46130779717371573
Loss in iteration 318 : 0.47133816741338136
Loss in iteration 319 : 0.45579866503570016
Loss in iteration 320 : 0.4547354453498888
Loss in iteration 321 : 0.4542301782380271
Loss in iteration 322 : 0.45948227456605034
Loss in iteration 323 : 0.45700053659350487
Loss in iteration 324 : 0.45856382657318884
Loss in iteration 325 : 0.4575000083066689
Loss in iteration 326 : 0.47314766225612814
Loss in iteration 327 : 0.4517076026238817
Loss in iteration 328 : 0.45388979688866743
Loss in iteration 329 : 0.45632263207532586
Loss in iteration 330 : 0.4513351176525122
Loss in iteration 331 : 0.460663826355723
Loss in iteration 332 : 0.44737144393730466
Loss in iteration 333 : 0.44811363546369665
Loss in iteration 334 : 0.45818982485102633
Loss in iteration 335 : 0.4521771166036447
Loss in iteration 336 : 0.4591898468801229
Loss in iteration 337 : 0.4633617788530771
Loss in iteration 338 : 0.4683329406265429
Loss in iteration 339 : 0.47199531710904913
Loss in iteration 340 : 0.4648045526109125
Loss in iteration 341 : 0.4571273485897731
Loss in iteration 342 : 0.45912920605247864
Loss in iteration 343 : 0.4587682741550154
Loss in iteration 344 : 0.46540446348388703
Loss in iteration 345 : 0.45872644276847346
Loss in iteration 346 : 0.4524486083927763
Loss in iteration 347 : 0.459957563426011
Loss in iteration 348 : 0.46154905144384406
Loss in iteration 349 : 0.46641762759896355
Loss in iteration 350 : 0.4667888082985807
Loss in iteration 351 : 0.46133588205170784
Loss in iteration 352 : 0.45778605012718365
Loss in iteration 353 : 0.45243894495489245
Loss in iteration 354 : 0.4602824326843267
Loss in iteration 355 : 0.45980307177714774
Loss in iteration 356 : 0.4532380298084967
Loss in iteration 357 : 0.4577474798064177
Loss in iteration 358 : 0.46702232732388654
Loss in iteration 359 : 0.4570204492156802
Loss in iteration 360 : 0.46536483533913703
Loss in iteration 361 : 0.44991040043429265
Loss in iteration 362 : 0.47036050182785377
Loss in iteration 363 : 0.46154221861756894
Loss in iteration 364 : 0.45909772392521286
Loss in iteration 365 : 0.4626529344866513
Loss in iteration 366 : 0.45692892644147715
Loss in iteration 367 : 0.4664840922816946
Loss in iteration 368 : 0.46744779607250747
Loss in iteration 369 : 0.4717081380285878
Loss in iteration 370 : 0.45686490951855485
Loss in iteration 371 : 0.47450697194201485
Loss in iteration 372 : 0.454533194553773
Loss in iteration 373 : 0.459999766465719
Loss in iteration 374 : 0.45779870073500867
Loss in iteration 375 : 0.4562408409116665
Loss in iteration 376 : 0.4653975506736979
Loss in iteration 377 : 0.46068795619695163
Loss in iteration 378 : 0.4633746518077049
Loss in iteration 379 : 0.4631264262720362
Loss in iteration 380 : 0.458002258861304
Loss in iteration 381 : 0.44928046982986375
Loss in iteration 382 : 0.4554825135329853
Loss in iteration 383 : 0.46221246016978196
Loss in iteration 384 : 0.46773697965978317
Loss in iteration 385 : 0.46072708152874303
Loss in iteration 386 : 0.4483958714926497
Loss in iteration 387 : 0.46602752676671333
Loss in iteration 388 : 0.46985469493898335
Loss in iteration 389 : 0.4564564132905554
Loss in iteration 390 : 0.4578412763886047
Loss in iteration 391 : 0.461007680093062
Loss in iteration 392 : 0.4504831479756788
Loss in iteration 393 : 0.4541557496349734
Loss in iteration 394 : 0.4577263164081837
Loss in iteration 395 : 0.4654051629376204
Loss in iteration 396 : 0.4588132988950128
Loss in iteration 397 : 0.4721007992743795
Loss in iteration 398 : 0.4683282428522826
Loss in iteration 399 : 0.45667358646627065
Loss in iteration 400 : 0.45076128679455413
Testing accuracy  of updater 9 on alg 0 with rate 0.08000000000000002 = 0.7875, training accuracy 0.7875, time elapsed: 5385 millisecond.
Loss in iteration 1 : 0.6931471805599116
Loss in iteration 2 : 0.6898122500359531
Loss in iteration 3 : 0.682997327852835
Loss in iteration 4 : 0.679309789966082
Loss in iteration 5 : 0.672308172216191
Loss in iteration 6 : 0.6701251761814361
Loss in iteration 7 : 0.6616601713628536
Loss in iteration 8 : 0.6536786467919777
Loss in iteration 9 : 0.6487209658116274
Loss in iteration 10 : 0.6431914430928695
Loss in iteration 11 : 0.6317351497417644
Loss in iteration 12 : 0.6278041235088974
Loss in iteration 13 : 0.6217481380686525
Loss in iteration 14 : 0.6149187282081027
Loss in iteration 15 : 0.6096820403808464
Loss in iteration 16 : 0.6006484413284586
Loss in iteration 17 : 0.5942270273580804
Loss in iteration 18 : 0.5887818893587422
Loss in iteration 19 : 0.5855308445992001
Loss in iteration 20 : 0.5791227521471926
Loss in iteration 21 : 0.5780903122616446
Loss in iteration 22 : 0.5734858830333597
Loss in iteration 23 : 0.5701806609737796
Loss in iteration 24 : 0.5601248009757058
Loss in iteration 25 : 0.5606555265772006
Loss in iteration 26 : 0.5518443492237306
Loss in iteration 27 : 0.549709189226202
Loss in iteration 28 : 0.5491585234929879
Loss in iteration 29 : 0.5511129071987497
Loss in iteration 30 : 0.5483393177839462
Loss in iteration 31 : 0.5427301463785296
Loss in iteration 32 : 0.540895382053731
Loss in iteration 33 : 0.5352434307035081
Loss in iteration 34 : 0.5331114830009756
Loss in iteration 35 : 0.5356375206705182
Loss in iteration 36 : 0.5323598688165214
Loss in iteration 37 : 0.5280722128942933
Loss in iteration 38 : 0.5253760165318204
Loss in iteration 39 : 0.5221393565351987
Loss in iteration 40 : 0.5231642911587241
Loss in iteration 41 : 0.5239771584258579
Loss in iteration 42 : 0.5159074225631419
Loss in iteration 43 : 0.5213352682852187
Loss in iteration 44 : 0.5194064283309516
Loss in iteration 45 : 0.5178585580326563
Loss in iteration 46 : 0.5169637749006208
Loss in iteration 47 : 0.514248206002127
Loss in iteration 48 : 0.5169348754146074
Loss in iteration 49 : 0.5103264904922382
Loss in iteration 50 : 0.5063014349715305
Loss in iteration 51 : 0.5153554631875312
Loss in iteration 52 : 0.5069061679099756
Loss in iteration 53 : 0.5059229354068803
Loss in iteration 54 : 0.5024901315766659
Loss in iteration 55 : 0.504476890538418
Loss in iteration 56 : 0.5076059756351858
Loss in iteration 57 : 0.4955887385698803
Loss in iteration 58 : 0.4982281835099294
Loss in iteration 59 : 0.49861450579046385
Loss in iteration 60 : 0.5015322828106147
Loss in iteration 61 : 0.5022705941846852
Loss in iteration 62 : 0.5011073247697057
Loss in iteration 63 : 0.49864130649098765
Loss in iteration 64 : 0.4976597086852549
Loss in iteration 65 : 0.5048481968250977
Loss in iteration 66 : 0.4928858589173787
Loss in iteration 67 : 0.49752206082320005
Loss in iteration 68 : 0.4912843989237648
Loss in iteration 69 : 0.5001094453056748
Loss in iteration 70 : 0.4924021217039194
Loss in iteration 71 : 0.5051092391505437
Loss in iteration 72 : 0.4927156452127227
Loss in iteration 73 : 0.4974307552514897
Loss in iteration 74 : 0.49400564566845234
Loss in iteration 75 : 0.49373930158259755
Loss in iteration 76 : 0.5000943037580006
Loss in iteration 77 : 0.4900524321075733
Loss in iteration 78 : 0.4947541666979346
Loss in iteration 79 : 0.4805758397787687
Loss in iteration 80 : 0.49135453664778156
Loss in iteration 81 : 0.49437584305036825
Loss in iteration 82 : 0.4898426333902472
Loss in iteration 83 : 0.4941572393751954
Loss in iteration 84 : 0.4985117010630124
Loss in iteration 85 : 0.4922907271564988
Loss in iteration 86 : 0.49683820307599474
Loss in iteration 87 : 0.48220373862854954
Loss in iteration 88 : 0.49841942617739576
Loss in iteration 89 : 0.48459066420468966
Loss in iteration 90 : 0.4876737541105374
Loss in iteration 91 : 0.49413519853275184
Loss in iteration 92 : 0.4835863042794503
Loss in iteration 93 : 0.4870841190303384
Loss in iteration 94 : 0.4879474963901123
Loss in iteration 95 : 0.483326079928669
Loss in iteration 96 : 0.4950856229317776
Loss in iteration 97 : 0.4857445963600723
Loss in iteration 98 : 0.48894584040014183
Loss in iteration 99 : 0.48644729937206643
Loss in iteration 100 : 0.49051639484881815
Loss in iteration 101 : 0.48330791858962097
Loss in iteration 102 : 0.48276282875480403
Loss in iteration 103 : 0.4905663814912203
Loss in iteration 104 : 0.4863249457995993
Loss in iteration 105 : 0.4757696929521494
Loss in iteration 106 : 0.4843869494203549
Loss in iteration 107 : 0.4919715959024996
Loss in iteration 108 : 0.4968020037182601
Loss in iteration 109 : 0.4837252228736648
Loss in iteration 110 : 0.4792188196732667
Loss in iteration 111 : 0.487393074076545
Loss in iteration 112 : 0.48060990413085364
Loss in iteration 113 : 0.47713376116452505
Loss in iteration 114 : 0.4770837372542908
Loss in iteration 115 : 0.4761917556508419
Loss in iteration 116 : 0.4824175770215369
Loss in iteration 117 : 0.4904587244982454
Loss in iteration 118 : 0.48897235564799907
Loss in iteration 119 : 0.47589205645850363
Loss in iteration 120 : 0.48924774721026365
Loss in iteration 121 : 0.48684998906852733
Loss in iteration 122 : 0.4893535447338826
Loss in iteration 123 : 0.48304976220468104
Loss in iteration 124 : 0.47760607411552447
Loss in iteration 125 : 0.4774379328528333
Loss in iteration 126 : 0.4764997719118702
Loss in iteration 127 : 0.47805547116060926
Loss in iteration 128 : 0.4823647948096691
Loss in iteration 129 : 0.4770499701351559
Loss in iteration 130 : 0.481741501233791
Loss in iteration 131 : 0.4778688925903559
Loss in iteration 132 : 0.47735331919183704
Loss in iteration 133 : 0.48140910826876615
Loss in iteration 134 : 0.48164695814620967
Loss in iteration 135 : 0.4793055933540035
Loss in iteration 136 : 0.4793600993895695
Loss in iteration 137 : 0.4810784493778378
Loss in iteration 138 : 0.4822856943135534
Loss in iteration 139 : 0.48664392905036047
Loss in iteration 140 : 0.4819665559140445
Loss in iteration 141 : 0.4827827380297651
Loss in iteration 142 : 0.4755849867790545
Loss in iteration 143 : 0.47844075003630065
Loss in iteration 144 : 0.4753566975814901
Loss in iteration 145 : 0.46974995645136797
Loss in iteration 146 : 0.4814237437363234
Loss in iteration 147 : 0.4857680590689792
Loss in iteration 148 : 0.4775781318833164
Loss in iteration 149 : 0.48826793183008915
Loss in iteration 150 : 0.4820230776409035
Loss in iteration 151 : 0.4758206926532589
Loss in iteration 152 : 0.4846306810426662
Loss in iteration 153 : 0.46600231873072867
Loss in iteration 154 : 0.48417246976316547
Loss in iteration 155 : 0.4809268718140089
Loss in iteration 156 : 0.46636513139041985
Loss in iteration 157 : 0.47927131725926375
Loss in iteration 158 : 0.4707468458184853
Loss in iteration 159 : 0.47358770459318306
Loss in iteration 160 : 0.47250282022981843
Loss in iteration 161 : 0.47373553653744394
Loss in iteration 162 : 0.47487373919257564
Loss in iteration 163 : 0.47887546944898235
Loss in iteration 164 : 0.4783199609445566
Loss in iteration 165 : 0.47131718935288247
Loss in iteration 166 : 0.47392980230683057
Loss in iteration 167 : 0.4833894899975478
Loss in iteration 168 : 0.48235907354067303
Loss in iteration 169 : 0.4749899177458388
Loss in iteration 170 : 0.48001927492061197
Loss in iteration 171 : 0.4701803520347583
Loss in iteration 172 : 0.4768499653043775
Loss in iteration 173 : 0.47418182816155513
Loss in iteration 174 : 0.4694109280230677
Loss in iteration 175 : 0.48282529256700424
Loss in iteration 176 : 0.4703216355998359
Loss in iteration 177 : 0.48131995406105466
Loss in iteration 178 : 0.4708850181938352
Loss in iteration 179 : 0.4795334737863801
Loss in iteration 180 : 0.46902286522456965
Loss in iteration 181 : 0.47580859344742765
Loss in iteration 182 : 0.4732105667512659
Loss in iteration 183 : 0.47214322996280045
Loss in iteration 184 : 0.4853621912661292
Loss in iteration 185 : 0.47290827275253655
Loss in iteration 186 : 0.4660796901319938
Loss in iteration 187 : 0.4681218136067476
Loss in iteration 188 : 0.4780247967264339
Loss in iteration 189 : 0.47829695338620715
Loss in iteration 190 : 0.4657934939783671
Loss in iteration 191 : 0.4678993934156204
Loss in iteration 192 : 0.47084495303767565
Loss in iteration 193 : 0.47426361313653387
Loss in iteration 194 : 0.4737834325217884
Loss in iteration 195 : 0.4806183644977967
Loss in iteration 196 : 0.4863971390563225
Loss in iteration 197 : 0.4641150265467049
Loss in iteration 198 : 0.4814872087415336
Loss in iteration 199 : 0.47189748126467895
Loss in iteration 200 : 0.46534370277795706
Loss in iteration 201 : 0.4726394073862803
Loss in iteration 202 : 0.47879109151340726
Loss in iteration 203 : 0.46339057711329856
Loss in iteration 204 : 0.46771564749995365
Loss in iteration 205 : 0.4725173660414383
Loss in iteration 206 : 0.4708093522817128
Loss in iteration 207 : 0.46854846636215597
Loss in iteration 208 : 0.46359348819964663
Loss in iteration 209 : 0.4759422057548356
Loss in iteration 210 : 0.48038504855779374
Loss in iteration 211 : 0.474389518374533
Loss in iteration 212 : 0.47827726356974004
Loss in iteration 213 : 0.4782884818266241
Loss in iteration 214 : 0.4900090691579738
Loss in iteration 215 : 0.479715208043402
Loss in iteration 216 : 0.47247434439086217
Loss in iteration 217 : 0.4700476239724055
Loss in iteration 218 : 0.4656871344431772
Loss in iteration 219 : 0.47641536487262065
Loss in iteration 220 : 0.4755142482294132
Loss in iteration 221 : 0.4742906584054033
Loss in iteration 222 : 0.4639174747363435
Loss in iteration 223 : 0.4775945659393599
Loss in iteration 224 : 0.47634959408378713
Loss in iteration 225 : 0.46971292344864995
Loss in iteration 226 : 0.47256887651277946
Loss in iteration 227 : 0.4632097521404889
Loss in iteration 228 : 0.4727300002135456
Loss in iteration 229 : 0.47337434143736484
Loss in iteration 230 : 0.4647101583682102
Loss in iteration 231 : 0.47307294725912014
Loss in iteration 232 : 0.47408022603350236
Loss in iteration 233 : 0.4689579107517673
Loss in iteration 234 : 0.47535258635111444
Loss in iteration 235 : 0.46490518178265855
Loss in iteration 236 : 0.4704569960176122
Loss in iteration 237 : 0.4716445600147228
Loss in iteration 238 : 0.46969597076189723
Loss in iteration 239 : 0.46919935444438304
Loss in iteration 240 : 0.4647399125546437
Loss in iteration 241 : 0.46552520548106296
Loss in iteration 242 : 0.4905851681253437
Loss in iteration 243 : 0.4789639532289623
Loss in iteration 244 : 0.4691350938507851
Loss in iteration 245 : 0.4815731004867334
Loss in iteration 246 : 0.4682685801649399
Loss in iteration 247 : 0.46328090343683204
Loss in iteration 248 : 0.47102147007649386
Loss in iteration 249 : 0.47413611207797157
Loss in iteration 250 : 0.4641691312562839
Loss in iteration 251 : 0.4748392967372761
Loss in iteration 252 : 0.4681464424556979
Loss in iteration 253 : 0.4781459592761334
Loss in iteration 254 : 0.45688626440793245
Loss in iteration 255 : 0.46280839193378026
Loss in iteration 256 : 0.4821617687923128
Loss in iteration 257 : 0.4613267434146209
Loss in iteration 258 : 0.4654579351464517
Loss in iteration 259 : 0.46446076140550596
Loss in iteration 260 : 0.4685894525171347
Loss in iteration 261 : 0.4578248928760787
Loss in iteration 262 : 0.46752173944250847
Loss in iteration 263 : 0.46917165448290443
Loss in iteration 264 : 0.4815673183936263
Loss in iteration 265 : 0.4609545239082191
Loss in iteration 266 : 0.4775726521804294
Loss in iteration 267 : 0.47381743665509923
Loss in iteration 268 : 0.47879129026644585
Loss in iteration 269 : 0.46397822779156145
Loss in iteration 270 : 0.4691290394474642
Loss in iteration 271 : 0.45767823269171803
Loss in iteration 272 : 0.469507882190794
Loss in iteration 273 : 0.4547610432328651
Loss in iteration 274 : 0.4634721456017928
Loss in iteration 275 : 0.471219666307994
Loss in iteration 276 : 0.4741509569249401
Loss in iteration 277 : 0.46190660116023124
Loss in iteration 278 : 0.46063979253223825
Loss in iteration 279 : 0.45884611453476737
Loss in iteration 280 : 0.4639925402137912
Loss in iteration 281 : 0.4717807403981405
Loss in iteration 282 : 0.46466013005613593
Loss in iteration 283 : 0.47114417667981195
Loss in iteration 284 : 0.4560678212651475
Loss in iteration 285 : 0.4587144891281689
Loss in iteration 286 : 0.46724786640417676
Loss in iteration 287 : 0.47083362152178637
Loss in iteration 288 : 0.4705647533915778
Loss in iteration 289 : 0.46380264751666345
Loss in iteration 290 : 0.45892797854998224
Loss in iteration 291 : 0.47143400521585777
Loss in iteration 292 : 0.46935513175942295
Loss in iteration 293 : 0.4629380322251161
Loss in iteration 294 : 0.47286532842817214
Loss in iteration 295 : 0.4598193268961783
Loss in iteration 296 : 0.4623862177103908
Loss in iteration 297 : 0.4704568771876096
Loss in iteration 298 : 0.46815084461395645
Loss in iteration 299 : 0.4689927171110356
Loss in iteration 300 : 0.47203908364110936
Loss in iteration 301 : 0.47079649036888943
Loss in iteration 302 : 0.465626253733984
Loss in iteration 303 : 0.45967275316473305
Loss in iteration 304 : 0.46308074230897245
Loss in iteration 305 : 0.46361402130591606
Loss in iteration 306 : 0.46435842418320783
Loss in iteration 307 : 0.4663953453963479
Loss in iteration 308 : 0.4684781272490105
Loss in iteration 309 : 0.45934261555614764
Loss in iteration 310 : 0.4647920684246844
Loss in iteration 311 : 0.47347253885129414
Loss in iteration 312 : 0.48228275849107866
Loss in iteration 313 : 0.4551139545394916
Loss in iteration 314 : 0.4617440205733741
Loss in iteration 315 : 0.46379708336141245
Loss in iteration 316 : 0.4803894420502995
Loss in iteration 317 : 0.4650515009270763
Loss in iteration 318 : 0.4766763486141912
Loss in iteration 319 : 0.4617911553090068
Loss in iteration 320 : 0.46029385325597033
Loss in iteration 321 : 0.46148218512535283
Loss in iteration 322 : 0.46404798656644475
Loss in iteration 323 : 0.4629555929026163
Loss in iteration 324 : 0.46432460478249715
Loss in iteration 325 : 0.4632426023262859
Loss in iteration 326 : 0.4781016559075049
Loss in iteration 327 : 0.4579251211873068
Loss in iteration 328 : 0.46076254840721576
Loss in iteration 329 : 0.4616902833924234
Loss in iteration 330 : 0.45711046590872634
Loss in iteration 331 : 0.4654185380620882
Loss in iteration 332 : 0.45321717482240764
Loss in iteration 333 : 0.45491745337170203
Loss in iteration 334 : 0.46499726430220145
Loss in iteration 335 : 0.45786304043813486
Loss in iteration 336 : 0.464649506089938
Loss in iteration 337 : 0.46770280455890223
Loss in iteration 338 : 0.4732872038813562
Loss in iteration 339 : 0.47607333726229367
Loss in iteration 340 : 0.4700326612177225
Loss in iteration 341 : 0.462818362990894
Loss in iteration 342 : 0.4647997316803882
Loss in iteration 343 : 0.4641506580870883
Loss in iteration 344 : 0.470271671105223
Loss in iteration 345 : 0.46448243341774526
Loss in iteration 346 : 0.4594573693821453
Loss in iteration 347 : 0.46619288197904696
Loss in iteration 348 : 0.4670483887083734
Loss in iteration 349 : 0.4715616921580642
Loss in iteration 350 : 0.4704372418320582
Loss in iteration 351 : 0.46602905093573355
Loss in iteration 352 : 0.4631848863067536
Loss in iteration 353 : 0.45875152731321556
Loss in iteration 354 : 0.4650216698547042
Loss in iteration 355 : 0.46480119713592233
Loss in iteration 356 : 0.45810103511927114
Loss in iteration 357 : 0.46305170506610804
Loss in iteration 358 : 0.4700549584429767
Loss in iteration 359 : 0.46332639298358086
Loss in iteration 360 : 0.46980477103879786
Loss in iteration 361 : 0.4566642908803152
Loss in iteration 362 : 0.474632498288918
Loss in iteration 363 : 0.46554599816641595
Loss in iteration 364 : 0.46482540112544685
Loss in iteration 365 : 0.46742184969636436
Loss in iteration 366 : 0.4624057841658368
Loss in iteration 367 : 0.4702635414122558
Loss in iteration 368 : 0.4721630662726632
Loss in iteration 369 : 0.4766309290899125
Loss in iteration 370 : 0.461744605137946
Loss in iteration 371 : 0.47891787018319837
Loss in iteration 372 : 0.4580702625823038
Loss in iteration 373 : 0.46516271102211354
Loss in iteration 374 : 0.46313162515207557
Loss in iteration 375 : 0.46180480163482623
Loss in iteration 376 : 0.4712038351635655
Loss in iteration 377 : 0.4652345109865285
Loss in iteration 378 : 0.4683401937627332
Loss in iteration 379 : 0.46714347743688506
Loss in iteration 380 : 0.4621544934855769
Loss in iteration 381 : 0.4546701125655177
Loss in iteration 382 : 0.4602683682352563
Loss in iteration 383 : 0.4660115769516761
Loss in iteration 384 : 0.4716006447385854
Loss in iteration 385 : 0.465635772997887
Loss in iteration 386 : 0.4542398129445119
Loss in iteration 387 : 0.4693200853662467
Loss in iteration 388 : 0.47335927899647545
Loss in iteration 389 : 0.46000355335983706
Loss in iteration 390 : 0.4627668095139213
Loss in iteration 391 : 0.4655354143233876
Loss in iteration 392 : 0.4553582759355792
Loss in iteration 393 : 0.45753827441789024
Loss in iteration 394 : 0.4623836057404811
Loss in iteration 395 : 0.46956230029122
Loss in iteration 396 : 0.4636218627263168
Loss in iteration 397 : 0.4754849376319935
Loss in iteration 398 : 0.47245464794490094
Loss in iteration 399 : 0.46078700069150674
Loss in iteration 400 : 0.4561571511437471
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.79, training accuracy 0.79, time elapsed: 5158 millisecond.
