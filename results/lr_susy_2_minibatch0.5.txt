objc[3030]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x106b6f4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x106bf34e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/03/02 13:29:31 INFO SparkContext: Running Spark version 2.0.0
18/03/02 13:29:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/02 13:29:32 INFO SecurityManager: Changing view acls to: Aitor
18/03/02 13:29:32 INFO SecurityManager: Changing modify acls to: Aitor
18/03/02 13:29:32 INFO SecurityManager: Changing view acls groups to: 
18/03/02 13:29:32 INFO SecurityManager: Changing modify acls groups to: 
18/03/02 13:29:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/03/02 13:29:32 INFO Utils: Successfully started service 'sparkDriver' on port 52727.
18/03/02 13:29:33 INFO SparkEnv: Registering MapOutputTracker
18/03/02 13:29:33 INFO SparkEnv: Registering BlockManagerMaster
18/03/02 13:29:33 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-157c5efa-f78f-40a1-a48a-bf8a9248586b
18/03/02 13:29:33 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/03/02 13:29:33 INFO SparkEnv: Registering OutputCommitCoordinator
18/03/02 13:29:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/03/02 13:29:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/03/02 13:29:33 INFO Executor: Starting executor ID driver on host localhost
18/03/02 13:29:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52728.
18/03/02 13:29:33 INFO NettyBlockTransferService: Server created on 192.168.2.140:52728
18/03/02 13:29:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 52728)
18/03/02 13:29:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:52728 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 52728)
18/03/02 13:29:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 52728)
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 2.36925253869775
Loss in iteration 3 : 23.846187844670148
Loss in iteration 4 : 6.942696927364602
Loss in iteration 5 : 16.901053933642952
Loss in iteration 6 : 10.396685603115177
Loss in iteration 7 : 11.535234584572377
Loss in iteration 8 : 13.682048722377143
Loss in iteration 9 : 6.8797627418463945
Loss in iteration 10 : 14.238132449908095
Loss in iteration 11 : 4.962245826646053
Loss in iteration 12 : 12.211896074199135
Loss in iteration 13 : 6.003165486608508
Loss in iteration 14 : 12.062695548882605
Loss in iteration 15 : 6.157720598428067
Loss in iteration 16 : 10.527271119020034
Loss in iteration 17 : 7.0221625353431865
Loss in iteration 18 : 9.719616038441865
Loss in iteration 19 : 7.846906376568725
Loss in iteration 20 : 9.509260302284696
Loss in iteration 21 : 7.084867915918831
Loss in iteration 22 : 8.568017392493656
Loss in iteration 23 : 6.8020351449893885
Loss in iteration 24 : 6.927043603950074
Loss in iteration 25 : 6.775561961941404
Loss in iteration 26 : 6.454880135732909
Loss in iteration 27 : 6.943871496709977
Loss in iteration 28 : 6.349364264509575
Loss in iteration 29 : 6.404775392755178
Loss in iteration 30 : 5.678209144355839
Loss in iteration 31 : 6.416549592289823
Loss in iteration 32 : 5.664891764661087
Loss in iteration 33 : 6.198141618245352
Loss in iteration 34 : 5.908366106699408
Loss in iteration 35 : 6.026164098924574
Loss in iteration 36 : 4.728431814370367
Loss in iteration 37 : 5.658824209349814
Loss in iteration 38 : 4.76662314411322
Loss in iteration 39 : 5.685128161693308
Loss in iteration 40 : 4.408965757608515
Loss in iteration 41 : 5.517498558671979
Loss in iteration 42 : 4.732262719387332
Loss in iteration 43 : 5.230288414629132
Loss in iteration 44 : 4.246109303386377
Loss in iteration 45 : 5.495665702833269
Loss in iteration 46 : 4.747803854346009
Loss in iteration 47 : 5.675058666728333
Loss in iteration 48 : 4.501199954081886
Loss in iteration 49 : 5.717878678978893
Loss in iteration 50 : 4.405901512112844
Loss in iteration 51 : 5.011615638394452
Loss in iteration 52 : 4.4789601655264875
Loss in iteration 53 : 5.48091773461226
Loss in iteration 54 : 4.1798044862704185
Loss in iteration 55 : 5.116519132055647
Loss in iteration 56 : 4.4975709752638275
Loss in iteration 57 : 5.376014735624979
Loss in iteration 58 : 4.106727987128225
Loss in iteration 59 : 5.075043346523498
Loss in iteration 60 : 4.0983833486618595
Loss in iteration 61 : 5.0196594041367835
Loss in iteration 62 : 3.8984546669391227
Loss in iteration 63 : 4.7701097033525794
Loss in iteration 64 : 3.7632100037826914
Loss in iteration 65 : 4.629065711638818
Loss in iteration 66 : 4.237526062600264
Loss in iteration 67 : 5.006078572246598
Loss in iteration 68 : 4.422059630951059
Loss in iteration 69 : 5.0604103832167375
Loss in iteration 70 : 3.746131399560354
Loss in iteration 71 : 4.778682927460733
Loss in iteration 72 : 3.9994073589166117
Loss in iteration 73 : 4.844037149038715
Loss in iteration 74 : 4.152464782058033
Loss in iteration 75 : 5.067831451838656
Loss in iteration 76 : 3.9999007153009187
Loss in iteration 77 : 4.758078099287721
Loss in iteration 78 : 3.7977697138435107
Loss in iteration 79 : 4.4606681853995775
Loss in iteration 80 : 3.641559131869075
Loss in iteration 81 : 4.550964311939669
Loss in iteration 82 : 3.716378796091142
Loss in iteration 83 : 4.711144976310117
Loss in iteration 84 : 4.001047776557092
Loss in iteration 85 : 4.747855398767762
Loss in iteration 86 : 3.799757706515385
Loss in iteration 87 : 4.618979591795289
Loss in iteration 88 : 3.818711069155514
Loss in iteration 89 : 4.4544191623126626
Loss in iteration 90 : 3.627156220948703
Loss in iteration 91 : 4.322703578705045
Loss in iteration 92 : 3.7793027459646007
Loss in iteration 93 : 4.250253526871562
Loss in iteration 94 : 3.605655855453224
Loss in iteration 95 : 4.273612993067427
Loss in iteration 96 : 3.713052524041126
Loss in iteration 97 : 4.702651252753874
Loss in iteration 98 : 3.685025822114465
Loss in iteration 99 : 4.293261797641213
Loss in iteration 100 : 3.5127050903449297
Loss in iteration 101 : 4.263605076056179
Loss in iteration 102 : 3.5029938703588734
Loss in iteration 103 : 4.245591414618849
Loss in iteration 104 : 3.809641271734411
Loss in iteration 105 : 4.35009954303322
Loss in iteration 106 : 3.7008890934760608
Loss in iteration 107 : 4.484094548875961
Loss in iteration 108 : 3.685039755067315
Loss in iteration 109 : 4.596524786298517
Loss in iteration 110 : 4.146180458296877
Loss in iteration 111 : 4.970911335313022
Loss in iteration 112 : 3.8819574428809416
Loss in iteration 113 : 4.804186355586547
Loss in iteration 114 : 4.058611211392677
Loss in iteration 115 : 4.5987237812478865
Loss in iteration 116 : 3.691911503952583
Loss in iteration 117 : 4.474342509550481
Loss in iteration 118 : 3.764691901458149
Loss in iteration 119 : 4.407969695091786
Loss in iteration 120 : 3.8634649617150973
Loss in iteration 121 : 4.5080005504318486
Loss in iteration 122 : 3.372782025672901
Loss in iteration 123 : 3.9610800099773034
Loss in iteration 124 : 3.747034199037749
Loss in iteration 125 : 4.540955829890231
Loss in iteration 126 : 3.8046909375388864
Loss in iteration 127 : 4.824897392321579
Loss in iteration 128 : 3.4521416514232697
Loss in iteration 129 : 4.12985707824591
Loss in iteration 130 : 3.647032090561224
Loss in iteration 131 : 4.414826547887917
Loss in iteration 132 : 3.49700078902711
Loss in iteration 133 : 4.041445744835308
Loss in iteration 134 : 3.4496324647284635
Loss in iteration 135 : 4.237806523692449
Loss in iteration 136 : 3.7534638493819066
Loss in iteration 137 : 4.336532835602327
Loss in iteration 138 : 3.514039274815954
Loss in iteration 139 : 4.166153329273404
Loss in iteration 140 : 3.691444252678001
Loss in iteration 141 : 4.327076829939137
Loss in iteration 142 : 3.79664691157551
Loss in iteration 143 : 4.4481688317994665
Loss in iteration 144 : 3.623530682869541
Loss in iteration 145 : 4.087034566436173
Loss in iteration 146 : 3.6974827223184814
Loss in iteration 147 : 4.429986676004605
Loss in iteration 148 : 3.7879852878727096
Loss in iteration 149 : 4.697484314937863
Loss in iteration 150 : 4.081718195253622
Loss in iteration 151 : 4.793756636048784
Loss in iteration 152 : 3.916369709318702
Loss in iteration 153 : 4.446302559757472
Loss in iteration 154 : 3.5662308651103305
Loss in iteration 155 : 4.123253689582778
Loss in iteration 156 : 3.688253292791491
Loss in iteration 157 : 4.242983884758969
Loss in iteration 158 : 3.3985155793497652
Loss in iteration 159 : 3.9869869686331154
Loss in iteration 160 : 3.5437012245869504
Loss in iteration 161 : 4.254556139390338
Loss in iteration 162 : 3.466313949060267
Loss in iteration 163 : 4.070378035407076
Loss in iteration 164 : 3.5489075813799698
Loss in iteration 165 : 4.2955936629290825
Loss in iteration 166 : 3.7974807511197723
Loss in iteration 167 : 4.688292484728606
Loss in iteration 168 : 3.8860940101926174
Loss in iteration 169 : 4.652823335114243
Loss in iteration 170 : 3.572271126528941
Loss in iteration 171 : 4.326214722092762
Loss in iteration 172 : 3.865094433248423
Loss in iteration 173 : 4.294247780795311
Loss in iteration 174 : 3.9002212342008886
Loss in iteration 175 : 4.608432799984938
Loss in iteration 176 : 3.620446274729484
Loss in iteration 177 : 4.286859525852399
Loss in iteration 178 : 3.8187024851243327
Loss in iteration 179 : 4.369166835826695
Loss in iteration 180 : 3.5868619886424207
Loss in iteration 181 : 3.961844331893151
Loss in iteration 182 : 3.7631149163372064
Loss in iteration 183 : 4.4109055180983985
Loss in iteration 184 : 3.552404701395846
Loss in iteration 185 : 4.056342812678171
Loss in iteration 186 : 3.4602337136605805
Loss in iteration 187 : 4.035058478728849
Loss in iteration 188 : 3.5275067272760974
Loss in iteration 189 : 4.195717984721292
Loss in iteration 190 : 3.998860639326361
Loss in iteration 191 : 4.616881464815638
Loss in iteration 192 : 3.8475793543190204
Loss in iteration 193 : 4.258176049073303
Loss in iteration 194 : 3.482921626330997
Loss in iteration 195 : 3.9182456741765033
Loss in iteration 196 : 3.7582951162658613
Loss in iteration 197 : 4.415480504126782
Loss in iteration 198 : 3.751141850796422
Loss in iteration 199 : 4.49015546144784
Loss in iteration 200 : 3.9720154981006726
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.717, training accuracy 0.708375, time elapsed: 9667 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 1.698262261146221
Loss in iteration 3 : 15.787242933761615
Loss in iteration 4 : 5.632224768324169
Loss in iteration 5 : 10.941462482187124
Loss in iteration 6 : 8.054694444328748
Loss in iteration 7 : 7.194267847784383
Loss in iteration 8 : 10.159617185053303
Loss in iteration 9 : 4.246513843005644
Loss in iteration 10 : 10.10083726702117
Loss in iteration 11 : 3.3872326328321165
Loss in iteration 12 : 8.57454041786995
Loss in iteration 13 : 4.2190290265534705
Loss in iteration 14 : 8.554012784846998
Loss in iteration 15 : 4.243869717591424
Loss in iteration 16 : 7.450807068787944
Loss in iteration 17 : 4.8699480090020675
Loss in iteration 18 : 6.9121451617840615
Loss in iteration 19 : 5.415420553892036
Loss in iteration 20 : 6.758007735890531
Loss in iteration 21 : 4.897776861791837
Loss in iteration 22 : 6.068681334074587
Loss in iteration 23 : 4.723856721448077
Loss in iteration 24 : 4.926832981306352
Loss in iteration 25 : 4.734857486249003
Loss in iteration 26 : 4.590479773294758
Loss in iteration 27 : 4.849174252588037
Loss in iteration 28 : 4.524526677493863
Loss in iteration 29 : 4.468836819843351
Loss in iteration 30 : 4.044464680071477
Loss in iteration 31 : 4.470544229680766
Loss in iteration 32 : 4.013795982336639
Loss in iteration 33 : 4.320956137263906
Loss in iteration 34 : 4.177444539100089
Loss in iteration 35 : 4.201061740123506
Loss in iteration 36 : 3.3622458731350777
Loss in iteration 37 : 3.967340221852903
Loss in iteration 38 : 3.3861356622899494
Loss in iteration 39 : 3.9568936289221406
Loss in iteration 40 : 3.13263008475039
Loss in iteration 41 : 3.8617336922423875
Loss in iteration 42 : 3.3566358636020452
Loss in iteration 43 : 3.662208633774941
Loss in iteration 44 : 3.001507486813105
Loss in iteration 45 : 3.814161893061578
Loss in iteration 46 : 3.351460604060942
Loss in iteration 47 : 3.949150297753199
Loss in iteration 48 : 3.186029624217917
Loss in iteration 49 : 3.986732883979946
Loss in iteration 50 : 3.1182015918717427
Loss in iteration 51 : 3.4908422922223
Loss in iteration 52 : 3.156744805023107
Loss in iteration 53 : 3.8063897776956885
Loss in iteration 54 : 2.954724934072439
Loss in iteration 55 : 3.5604077113223336
Loss in iteration 56 : 3.1715730860876956
Loss in iteration 57 : 3.7345974837687743
Loss in iteration 58 : 2.900703827325623
Loss in iteration 59 : 3.530561267431483
Loss in iteration 60 : 2.8985978019112975
Loss in iteration 61 : 3.500568336254297
Loss in iteration 62 : 2.7636086971360987
Loss in iteration 63 : 3.3290019017503187
Loss in iteration 64 : 2.6705121452565486
Loss in iteration 65 : 3.2408113424940432
Loss in iteration 66 : 3.0044513691139807
Loss in iteration 67 : 3.499696764884965
Loss in iteration 68 : 3.1245625914631514
Loss in iteration 69 : 3.520735217914068
Loss in iteration 70 : 2.655759441764944
Loss in iteration 71 : 3.329777964007713
Loss in iteration 72 : 2.828137252912979
Loss in iteration 73 : 3.3712776187235702
Loss in iteration 74 : 2.9287953675885037
Loss in iteration 75 : 3.5193368840662487
Loss in iteration 76 : 2.81792976055087
Loss in iteration 77 : 3.3044687047213874
Loss in iteration 78 : 2.677339456863728
Loss in iteration 79 : 3.096579198652524
Loss in iteration 80 : 2.5727097199176496
Loss in iteration 81 : 3.175844952318188
Loss in iteration 82 : 2.619243078481452
Loss in iteration 83 : 3.2705297004524496
Loss in iteration 84 : 2.8169753039262213
Loss in iteration 85 : 3.2970594422308883
Loss in iteration 86 : 2.687825116031827
Loss in iteration 87 : 3.2205586260965644
Loss in iteration 88 : 2.693057827412224
Loss in iteration 89 : 3.093805920729649
Loss in iteration 90 : 2.576411980320362
Loss in iteration 91 : 3.032555785083121
Loss in iteration 92 : 2.667087899292055
Loss in iteration 93 : 2.9613083739339445
Loss in iteration 94 : 2.5378240082301207
Loss in iteration 95 : 2.962312095584663
Loss in iteration 96 : 2.6172933853146283
Loss in iteration 97 : 3.270094199285152
Loss in iteration 98 : 2.605380221128858
Loss in iteration 99 : 2.9824274820907544
Loss in iteration 100 : 2.4877009882663725
Loss in iteration 101 : 2.984537646355767
Loss in iteration 102 : 2.4770276477051607
Loss in iteration 103 : 2.9494101987401313
Loss in iteration 104 : 2.677368080223387
Loss in iteration 105 : 3.0156196370955306
Loss in iteration 106 : 2.610961923271541
Loss in iteration 107 : 3.118936298619741
Loss in iteration 108 : 2.603928928067501
Loss in iteration 109 : 3.191183296747111
Loss in iteration 110 : 2.914913719601535
Loss in iteration 111 : 3.4510043083885793
Loss in iteration 112 : 2.752122630395321
Loss in iteration 113 : 3.3453194030322377
Loss in iteration 114 : 2.8647496643724195
Loss in iteration 115 : 3.208274870671353
Loss in iteration 116 : 2.61014534125838
Loss in iteration 117 : 3.1130705757730572
Loss in iteration 118 : 2.655701339635948
Loss in iteration 119 : 3.06667223504236
Loss in iteration 120 : 2.72532875618622
Loss in iteration 121 : 3.137329467609425
Loss in iteration 122 : 2.3906292643481413
Loss in iteration 123 : 2.781706597843073
Loss in iteration 124 : 2.6424081889628748
Loss in iteration 125 : 3.1544380765448317
Loss in iteration 126 : 2.689252408251691
Loss in iteration 127 : 3.3580252387534526
Loss in iteration 128 : 2.4477492872462587
Loss in iteration 129 : 2.878282584932366
Loss in iteration 130 : 2.5651532317491066
Loss in iteration 131 : 3.0663040039450835
Loss in iteration 132 : 2.4700010344150316
Loss in iteration 133 : 2.8246293205672948
Loss in iteration 134 : 2.4345532548557975
Loss in iteration 135 : 2.946949146289009
Loss in iteration 136 : 2.6413689650501064
Loss in iteration 137 : 3.006173786480933
Loss in iteration 138 : 2.4833506805708967
Loss in iteration 139 : 2.8983820914890472
Loss in iteration 140 : 2.6047822631878135
Loss in iteration 141 : 3.004657841809112
Loss in iteration 142 : 2.6722019916230946
Loss in iteration 143 : 3.0846317185233296
Loss in iteration 144 : 2.555288055313489
Loss in iteration 145 : 2.846164890860663
Loss in iteration 146 : 2.59932363402219
Loss in iteration 147 : 3.079999566045238
Loss in iteration 148 : 2.667900503606313
Loss in iteration 149 : 3.2732919887523044
Loss in iteration 150 : 2.88030062982439
Loss in iteration 151 : 3.332836306584225
Loss in iteration 152 : 2.7763503587558906
Loss in iteration 153 : 3.103645574590151
Loss in iteration 154 : 2.522367172997155
Loss in iteration 155 : 2.8984668637911652
Loss in iteration 156 : 2.595775404381268
Loss in iteration 157 : 2.9394883085109877
Loss in iteration 158 : 2.4046885715766653
Loss in iteration 159 : 2.7888365533127377
Loss in iteration 160 : 2.4978850978486498
Loss in iteration 161 : 2.959028526526223
Loss in iteration 162 : 2.435973936414712
Loss in iteration 163 : 2.818443901704716
Loss in iteration 164 : 2.492718238377026
Loss in iteration 165 : 2.986353075794088
Loss in iteration 166 : 2.6661434870363037
Loss in iteration 167 : 3.2565287328387353
Loss in iteration 168 : 2.7315866801337103
Loss in iteration 169 : 3.2281574416021597
Loss in iteration 170 : 2.5232577946013426
Loss in iteration 171 : 3.0167182762659266
Loss in iteration 172 : 2.7109132025445115
Loss in iteration 173 : 2.98798634760048
Loss in iteration 174 : 2.7437849396086973
Loss in iteration 175 : 3.202751503000366
Loss in iteration 176 : 2.5670104742850315
Loss in iteration 177 : 2.9981250850509618
Loss in iteration 178 : 2.6884385852169657
Loss in iteration 179 : 3.0385417106618324
Loss in iteration 180 : 2.5354520321029788
Loss in iteration 181 : 2.7788305099234436
Loss in iteration 182 : 2.655739192909952
Loss in iteration 183 : 3.0777735112617397
Loss in iteration 184 : 2.504092127761021
Loss in iteration 185 : 2.8179268551363994
Loss in iteration 186 : 2.4406089188707027
Loss in iteration 187 : 2.814171340848376
Loss in iteration 188 : 2.483260902026498
Loss in iteration 189 : 2.9145209647911865
Loss in iteration 190 : 2.7992283691218285
Loss in iteration 191 : 3.203570826257615
Loss in iteration 192 : 2.718847362936638
Loss in iteration 193 : 2.96120609094845
Loss in iteration 194 : 2.4589379855751012
Loss in iteration 195 : 2.7441715805150184
Loss in iteration 196 : 2.647358686906422
Loss in iteration 197 : 3.0815621360725456
Loss in iteration 198 : 2.6550285646575547
Loss in iteration 199 : 3.135589523780302
Loss in iteration 200 : 2.8039659302483457
Testing accuracy  of updater 0 on alg 0 with rate 7.0 = 0.7175, training accuracy 0.709, time elapsed: 7048 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 1.0873486084939135
Loss in iteration 3 : 7.401160297943523
Loss in iteration 4 : 4.5857025998719125
Loss in iteration 5 : 4.670536277879749
Loss in iteration 6 : 5.939533955382478
Loss in iteration 7 : 2.6525196270132816
Loss in iteration 8 : 6.354616897286555
Loss in iteration 9 : 1.9430049560875977
Loss in iteration 10 : 5.714504118130928
Loss in iteration 11 : 2.0538235281264035
Loss in iteration 12 : 5.034153992025908
Loss in iteration 13 : 2.326475011283154
Loss in iteration 14 : 4.948349889209212
Loss in iteration 15 : 2.4082025516345285
Loss in iteration 16 : 4.35466017742393
Loss in iteration 17 : 2.7135149280182063
Loss in iteration 18 : 4.0642918052940855
Loss in iteration 19 : 2.9940481651448105
Loss in iteration 20 : 3.9758073542414007
Loss in iteration 21 : 2.7150475552168394
Loss in iteration 22 : 3.559269499471548
Loss in iteration 23 : 2.6331685515802645
Loss in iteration 24 : 2.9202225575602703
Loss in iteration 25 : 2.66581199736644
Loss in iteration 26 : 2.7261133628545786
Loss in iteration 27 : 2.722188451456519
Loss in iteration 28 : 2.7059444223961946
Loss in iteration 29 : 2.5104582931581034
Loss in iteration 30 : 2.4148776701532744
Loss in iteration 31 : 2.5138299375607427
Loss in iteration 32 : 2.3726345915487093
Loss in iteration 33 : 2.4356863802558624
Loss in iteration 34 : 2.456500004248043
Loss in iteration 35 : 2.363080578594149
Loss in iteration 36 : 2.014013790080659
Loss in iteration 37 : 2.2596238539854543
Loss in iteration 38 : 2.015374628653296
Loss in iteration 39 : 2.2244438253086893
Loss in iteration 40 : 1.8723383721430977
Loss in iteration 41 : 2.1903656115822776
Loss in iteration 42 : 1.9885492320659868
Loss in iteration 43 : 2.0774392495257956
Loss in iteration 44 : 1.7674084031894461
Loss in iteration 45 : 2.126622342794893
Loss in iteration 46 : 1.9684975031997771
Loss in iteration 47 : 2.2121312573677296
Loss in iteration 48 : 1.8833133689915762
Loss in iteration 49 : 2.24416934050713
Loss in iteration 50 : 1.8443277455763927
Loss in iteration 51 : 1.9656799207389515
Loss in iteration 52 : 1.8543377664019842
Loss in iteration 53 : 2.127546433760864
Loss in iteration 54 : 1.7418254628253653
Loss in iteration 55 : 1.9917902252155217
Loss in iteration 56 : 1.8560290908360702
Loss in iteration 57 : 2.08230356554132
Loss in iteration 58 : 1.7152763919317227
Loss in iteration 59 : 1.9828035844484442
Loss in iteration 60 : 1.7092670282714741
Loss in iteration 61 : 1.9693004910715923
Loss in iteration 62 : 1.638468410812477
Loss in iteration 63 : 1.8746578681323292
Loss in iteration 64 : 1.5834429754514068
Loss in iteration 65 : 1.8315238241942196
Loss in iteration 66 : 1.7725264980637268
Loss in iteration 67 : 1.9696739287747453
Loss in iteration 68 : 1.8348561657860731
Loss in iteration 69 : 1.9754250954544417
Loss in iteration 70 : 1.576762546879354
Loss in iteration 71 : 1.873240536067133
Loss in iteration 72 : 1.6638893134559907
Loss in iteration 73 : 1.8918657527892617
Loss in iteration 74 : 1.7086109716928746
Loss in iteration 75 : 1.9579126544071417
Loss in iteration 76 : 1.6506996076796172
Loss in iteration 77 : 1.8462247351760297
Loss in iteration 78 : 1.565823077584762
Loss in iteration 79 : 1.722751575263224
Loss in iteration 80 : 1.5131586743984993
Loss in iteration 81 : 1.784765318623562
Loss in iteration 82 : 1.5361126263040166
Loss in iteration 83 : 1.8315713277831442
Loss in iteration 84 : 1.646881749240259
Loss in iteration 85 : 1.8436657581546925
Loss in iteration 86 : 1.5838613178055638
Loss in iteration 87 : 1.8069775447357277
Loss in iteration 88 : 1.578228115543154
Loss in iteration 89 : 1.7261673083152609
Loss in iteration 90 : 1.5274182495512225
Loss in iteration 91 : 1.7250972861364524
Loss in iteration 92 : 1.5693505871189763
Loss in iteration 93 : 1.6677168892579055
Loss in iteration 94 : 1.482817943025283
Loss in iteration 95 : 1.6496618375579142
Loss in iteration 96 : 1.5263322152107146
Loss in iteration 97 : 1.8233937979478678
Loss in iteration 98 : 1.5349977223081288
Loss in iteration 99 : 1.6704705165688478
Loss in iteration 100 : 1.4709515829853024
Loss in iteration 101 : 1.6913458857680124
Loss in iteration 102 : 1.4641602238395641
Loss in iteration 103 : 1.6538728092660442
Loss in iteration 104 : 1.5572588994491818
Loss in iteration 105 : 1.6839090475523935
Loss in iteration 106 : 1.535996618823576
Loss in iteration 107 : 1.7511035279807186
Loss in iteration 108 : 1.532381660972362
Loss in iteration 109 : 1.7878366228063616
Loss in iteration 110 : 1.6967200460343934
Loss in iteration 111 : 1.922318884145736
Loss in iteration 112 : 1.6311726028529632
Loss in iteration 113 : 1.882101076657455
Loss in iteration 114 : 1.6821879947917446
Loss in iteration 115 : 1.80597871760593
Loss in iteration 116 : 1.5320937204991658
Loss in iteration 117 : 1.738819795608177
Loss in iteration 118 : 1.554210679399601
Loss in iteration 119 : 1.719035711739737
Loss in iteration 120 : 1.5993507072627116
Loss in iteration 121 : 1.7708794023496173
Loss in iteration 122 : 1.4224329201729302
Loss in iteration 123 : 1.5915345792942595
Loss in iteration 124 : 1.5466157921326797
Loss in iteration 125 : 1.7679525448591615
Loss in iteration 126 : 1.5791942085268615
Loss in iteration 127 : 1.8839555884591983
Loss in iteration 128 : 1.4478879254281511
Loss in iteration 129 : 1.6169586779885128
Loss in iteration 130 : 1.4965912883387273
Loss in iteration 131 : 1.712835562329696
Loss in iteration 132 : 1.4493351686031226
Loss in iteration 133 : 1.5963246541768534
Loss in iteration 134 : 1.424999583547273
Loss in iteration 135 : 1.644234009739736
Loss in iteration 136 : 1.5392534245679863
Loss in iteration 137 : 1.6774011036504017
Loss in iteration 138 : 1.4602532815907225
Loss in iteration 139 : 1.6261745832166559
Loss in iteration 140 : 1.529227745902471
Loss in iteration 141 : 1.6849261877485031
Loss in iteration 142 : 1.5609176503358841
Loss in iteration 143 : 1.7217342477129074
Loss in iteration 144 : 1.5001908265122557
Loss in iteration 145 : 1.6090255355084826
Loss in iteration 146 : 1.5170000240707604
Loss in iteration 147 : 1.7304823275139152
Loss in iteration 148 : 1.5568100021553228
Loss in iteration 149 : 1.8358697768255947
Loss in iteration 150 : 1.6806996132980816
Loss in iteration 151 : 1.8579120702066485
Loss in iteration 152 : 1.6360192483906706
Loss in iteration 153 : 1.7523456241606066
Loss in iteration 154 : 1.4873522719208374
Loss in iteration 155 : 1.655690701969711
Loss in iteration 156 : 1.5215089400071018
Loss in iteration 157 : 1.646536922568492
Loss in iteration 158 : 1.4192604307750838
Loss in iteration 159 : 1.5842018261283584
Loss in iteration 160 : 1.460575943243335
Loss in iteration 161 : 1.6590020527283669
Loss in iteration 162 : 1.4191237970096766
Loss in iteration 163 : 1.5676552716784817
Loss in iteration 164 : 1.4451975253971445
Loss in iteration 165 : 1.6695838466577362
Loss in iteration 166 : 1.54574741916349
Loss in iteration 167 : 1.8191377119926526
Loss in iteration 168 : 1.589744358378796
Loss in iteration 169 : 1.796182698970521
Loss in iteration 170 : 1.4830688249262378
Loss in iteration 171 : 1.7031417630564685
Loss in iteration 172 : 1.570597154337749
Loss in iteration 173 : 1.6791257802208726
Loss in iteration 174 : 1.5946055962968473
Loss in iteration 175 : 1.7901153083310646
Loss in iteration 176 : 1.5194484516337683
Loss in iteration 177 : 1.6997375037006317
Loss in iteration 178 : 1.5682898326550747
Loss in iteration 179 : 1.7011903109952604
Loss in iteration 180 : 1.4892198937762247
Loss in iteration 181 : 1.5833171774940775
Loss in iteration 182 : 1.5523439658209917
Loss in iteration 183 : 1.7296341663762256
Loss in iteration 184 : 1.4663516711233051
Loss in iteration 185 : 1.578722632290136
Loss in iteration 186 : 1.4312662012012065
Loss in iteration 187 : 1.5853729144530522
Loss in iteration 188 : 1.4529474408863354
Loss in iteration 189 : 1.6326994016339
Loss in iteration 190 : 1.612657535887334
Loss in iteration 191 : 1.7858532362568142
Loss in iteration 192 : 1.5933672770236944
Loss in iteration 193 : 1.6604517993641197
Loss in iteration 194 : 1.4459774115922837
Loss in iteration 195 : 1.5605645866207083
Loss in iteration 196 : 1.5432427040085814
Loss in iteration 197 : 1.7329664700216134
Loss in iteration 198 : 1.562135343644365
Loss in iteration 199 : 1.7695574027616157
Loss in iteration 200 : 1.6412692574489347
Testing accuracy  of updater 0 on alg 0 with rate 4.0 = 0.718, training accuracy 0.70975, time elapsed: 5329 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6945156839104583
Loss in iteration 3 : 0.7758981624057808
Loss in iteration 4 : 1.040111314301903
Loss in iteration 5 : 1.0169087378637338
Loss in iteration 6 : 1.196571586801268
Loss in iteration 7 : 0.8949522062847507
Loss in iteration 8 : 1.0859039064049258
Loss in iteration 9 : 0.8726834188598844
Loss in iteration 10 : 1.021237511917044
Loss in iteration 11 : 0.7886465386654142
Loss in iteration 12 : 0.8723118137252067
Loss in iteration 13 : 0.7534999005899774
Loss in iteration 14 : 0.8396372217622212
Loss in iteration 15 : 0.7406800106433674
Loss in iteration 16 : 0.778935987806371
Loss in iteration 17 : 0.7106306376622107
Loss in iteration 18 : 0.74689839850406
Loss in iteration 19 : 0.7088675716416859
Loss in iteration 20 : 0.7499069559958315
Loss in iteration 21 : 0.6784501226765974
Loss in iteration 22 : 0.7121318273143519
Loss in iteration 23 : 0.6326224901025538
Loss in iteration 24 : 0.630130474892049
Loss in iteration 25 : 0.5832627641583328
Loss in iteration 26 : 0.5858068027393006
Loss in iteration 27 : 0.5706320052979637
Loss in iteration 28 : 0.5904903543726876
Loss in iteration 29 : 0.55787687783188
Loss in iteration 30 : 0.5589419850315677
Loss in iteration 31 : 0.5434327935936081
Loss in iteration 32 : 0.5456163259227303
Loss in iteration 33 : 0.5523146696542317
Loss in iteration 34 : 0.5576514658715586
Loss in iteration 35 : 0.5407964305040345
Loss in iteration 36 : 0.5364195940071718
Loss in iteration 37 : 0.528537632323686
Loss in iteration 38 : 0.5190310200471262
Loss in iteration 39 : 0.5092985287061365
Loss in iteration 40 : 0.5017076064145684
Loss in iteration 41 : 0.5042552931883544
Loss in iteration 42 : 0.5028808220207833
Loss in iteration 43 : 0.4998807611550991
Loss in iteration 44 : 0.4876848024496635
Loss in iteration 45 : 0.4988797784755733
Loss in iteration 46 : 0.48610126625188155
Loss in iteration 47 : 0.4920970110881527
Loss in iteration 48 : 0.4912795045979748
Loss in iteration 49 : 0.4996683292114316
Loss in iteration 50 : 0.4946359968052653
Loss in iteration 51 : 0.48074140718202407
Loss in iteration 52 : 0.49392033663354257
Loss in iteration 53 : 0.48840268195133607
Loss in iteration 54 : 0.4813078128821828
Loss in iteration 55 : 0.48224766670594804
Loss in iteration 56 : 0.48526975705177683
Loss in iteration 57 : 0.4827197056789816
Loss in iteration 58 : 0.4902941462007766
Loss in iteration 59 : 0.4935672068735478
Loss in iteration 60 : 0.48723822312093223
Loss in iteration 61 : 0.48384971114086
Loss in iteration 62 : 0.48165761891461323
Loss in iteration 63 : 0.48543084154581045
Loss in iteration 64 : 0.4807823852488577
Loss in iteration 65 : 0.47669503290350357
Loss in iteration 66 : 0.4899808928044409
Loss in iteration 67 : 0.4760534386814118
Loss in iteration 68 : 0.49299975900840504
Loss in iteration 69 : 0.4859351226792918
Loss in iteration 70 : 0.47582276891115755
Loss in iteration 71 : 0.486333047966308
Loss in iteration 72 : 0.4842377352857594
Loss in iteration 73 : 0.4920741656020076
Loss in iteration 74 : 0.4861905158369143
Loss in iteration 75 : 0.48859298439981397
Loss in iteration 76 : 0.47654949215321735
Loss in iteration 77 : 0.4730978217767832
Loss in iteration 78 : 0.46968693425142183
Loss in iteration 79 : 0.4796462978876245
Loss in iteration 80 : 0.47820505009619996
Loss in iteration 81 : 0.4832578270780472
Loss in iteration 82 : 0.47361031033757045
Loss in iteration 83 : 0.48195941770145273
Loss in iteration 84 : 0.4705799475407647
Loss in iteration 85 : 0.4797375890101709
Loss in iteration 86 : 0.47673804395383335
Loss in iteration 87 : 0.4731701071097744
Loss in iteration 88 : 0.4777852013702307
Loss in iteration 89 : 0.4735717617214315
Loss in iteration 90 : 0.4788822588846962
Loss in iteration 91 : 0.47569505700480497
Loss in iteration 92 : 0.4767114876472508
Loss in iteration 93 : 0.4801801589841281
Loss in iteration 94 : 0.47578232655636027
Loss in iteration 95 : 0.4792264714964117
Loss in iteration 96 : 0.4801348559093016
Loss in iteration 97 : 0.4827585821689169
Loss in iteration 98 : 0.4766543831807426
Loss in iteration 99 : 0.46970736354256054
Loss in iteration 100 : 0.4706588584766867
Loss in iteration 101 : 0.4739857780528678
Loss in iteration 102 : 0.4763639907308661
Loss in iteration 103 : 0.4822810280470337
Loss in iteration 104 : 0.4789292226015594
Loss in iteration 105 : 0.4668729427515046
Loss in iteration 106 : 0.482988212634634
Loss in iteration 107 : 0.4782685419732883
Loss in iteration 108 : 0.46746137610717375
Loss in iteration 109 : 0.4818455827991608
Loss in iteration 110 : 0.47248224247529286
Loss in iteration 111 : 0.46689597090231016
Loss in iteration 112 : 0.474591785187099
Loss in iteration 113 : 0.4717941952912026
Loss in iteration 114 : 0.47999980922088437
Loss in iteration 115 : 0.47480860111857737
Loss in iteration 116 : 0.46938542038985587
Loss in iteration 117 : 0.4722466676789717
Loss in iteration 118 : 0.47336510198494175
Loss in iteration 119 : 0.4749290981883669
Loss in iteration 120 : 0.47795989423047086
Loss in iteration 121 : 0.4710612673646648
Loss in iteration 122 : 0.4776823645631411
Loss in iteration 123 : 0.4748672398175884
Loss in iteration 124 : 0.46772231718393165
Loss in iteration 125 : 0.4716717582055987
Loss in iteration 126 : 0.47686434586098336
Loss in iteration 127 : 0.484266492530516
Loss in iteration 128 : 0.46845582748056785
Loss in iteration 129 : 0.4743784888738271
Loss in iteration 130 : 0.47228805302905125
Loss in iteration 131 : 0.4757426500531986
Loss in iteration 132 : 0.4701497747989927
Loss in iteration 133 : 0.4732988590753856
Loss in iteration 134 : 0.47095158124602404
Loss in iteration 135 : 0.46954048374177776
Loss in iteration 136 : 0.47577112754959333
Loss in iteration 137 : 0.46673294153725375
Loss in iteration 138 : 0.4707107303496345
Loss in iteration 139 : 0.4623367916172949
Loss in iteration 140 : 0.47450306295019656
Loss in iteration 141 : 0.476812186653845
Loss in iteration 142 : 0.4743911135005822
Loss in iteration 143 : 0.46164249189286066
Loss in iteration 144 : 0.46987434410725154
Loss in iteration 145 : 0.47042747800142065
Loss in iteration 146 : 0.46364374562430866
Loss in iteration 147 : 0.4719606292864071
Loss in iteration 148 : 0.4684502529344906
Loss in iteration 149 : 0.47481615817949435
Loss in iteration 150 : 0.4721945121332896
Loss in iteration 151 : 0.4710844205580806
Loss in iteration 152 : 0.4795364675648736
Loss in iteration 153 : 0.4701305967759844
Loss in iteration 154 : 0.4715349848747645
Loss in iteration 155 : 0.46438094932395246
Loss in iteration 156 : 0.4677475703885136
Loss in iteration 157 : 0.46289529568570426
Loss in iteration 158 : 0.45863565188751704
Loss in iteration 159 : 0.4692444631584507
Loss in iteration 160 : 0.4620432631482822
Loss in iteration 161 : 0.46424674360294865
Loss in iteration 162 : 0.46756912668347733
Loss in iteration 163 : 0.47579532331851615
Loss in iteration 164 : 0.47578169779534213
Loss in iteration 165 : 0.47819871165000366
Loss in iteration 166 : 0.4714665938997097
Loss in iteration 167 : 0.4809570754318862
Loss in iteration 168 : 0.46185023481469273
Loss in iteration 169 : 0.4701928767892544
Loss in iteration 170 : 0.4638238862947276
Loss in iteration 171 : 0.4719630423678636
Loss in iteration 172 : 0.46597345704490434
Loss in iteration 173 : 0.46506463890051275
Loss in iteration 174 : 0.46453496003304695
Loss in iteration 175 : 0.46789243612311676
Loss in iteration 176 : 0.4747302363347045
Loss in iteration 177 : 0.4722705787490953
Loss in iteration 178 : 0.4681268353271898
Loss in iteration 179 : 0.4693007914659138
Loss in iteration 180 : 0.47210417504976687
Loss in iteration 181 : 0.47386896906959713
Loss in iteration 182 : 0.4741483331945941
Loss in iteration 183 : 0.4728042127344846
Loss in iteration 184 : 0.4663955786195084
Loss in iteration 185 : 0.4626792226378239
Loss in iteration 186 : 0.46169432205292343
Loss in iteration 187 : 0.45697665067285104
Loss in iteration 188 : 0.476862517334232
Loss in iteration 189 : 0.47312429522336247
Loss in iteration 190 : 0.47321233436878707
Loss in iteration 191 : 0.4721477106622062
Loss in iteration 192 : 0.47162574750707337
Loss in iteration 193 : 0.4627400426404299
Loss in iteration 194 : 0.465479524527585
Loss in iteration 195 : 0.4690457204852183
Loss in iteration 196 : 0.47447445145520684
Loss in iteration 197 : 0.4626136820337484
Loss in iteration 198 : 0.4650636776406851
Loss in iteration 199 : 0.47104391629338643
Loss in iteration 200 : 0.4650752046473075
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.7825, training accuracy 0.787875, time elapsed: 4625 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6838437658874919
Loss in iteration 3 : 0.6762182180857186
Loss in iteration 4 : 0.6802504581487581
Loss in iteration 5 : 0.6686965581908452
Loss in iteration 6 : 0.6669793295621637
Loss in iteration 7 : 0.6487249851339029
Loss in iteration 8 : 0.6448321130912347
Loss in iteration 9 : 0.6307004512294901
Loss in iteration 10 : 0.6273672326323779
Loss in iteration 11 : 0.6024506668317153
Loss in iteration 12 : 0.5895422581642052
Loss in iteration 13 : 0.5813009129808514
Loss in iteration 14 : 0.5730902574813348
Loss in iteration 15 : 0.57186358569451
Loss in iteration 16 : 0.5593934045925554
Loss in iteration 17 : 0.5611794920314719
Loss in iteration 18 : 0.5556111156810348
Loss in iteration 19 : 0.5542224042353305
Loss in iteration 20 : 0.5520200665692456
Loss in iteration 21 : 0.5463828816858973
Loss in iteration 22 : 0.5479410408346401
Loss in iteration 23 : 0.5407223572401629
Loss in iteration 24 : 0.5426459925480902
Loss in iteration 25 : 0.5320976153845027
Loss in iteration 26 : 0.5341031630730246
Loss in iteration 27 : 0.5252289482966879
Loss in iteration 28 : 0.5332014308973752
Loss in iteration 29 : 0.5235712961028878
Loss in iteration 30 : 0.5252101709893208
Loss in iteration 31 : 0.5173842503218209
Loss in iteration 32 : 0.5181141818908762
Loss in iteration 33 : 0.5259259275222155
Loss in iteration 34 : 0.5241731407106404
Loss in iteration 35 : 0.520120540972626
Loss in iteration 36 : 0.5195813606160268
Loss in iteration 37 : 0.5206185968563082
Loss in iteration 38 : 0.516024772879293
Loss in iteration 39 : 0.5118214993365708
Loss in iteration 40 : 0.5072645988843356
Loss in iteration 41 : 0.5100319596576721
Loss in iteration 42 : 0.5080148600451607
Loss in iteration 43 : 0.509643500599864
Loss in iteration 44 : 0.5034778326027107
Loss in iteration 45 : 0.5138075307227912
Loss in iteration 46 : 0.5012432451903972
Loss in iteration 47 : 0.5065530109967666
Loss in iteration 48 : 0.5051628855400311
Loss in iteration 49 : 0.5105996011640077
Loss in iteration 50 : 0.5076935147553205
Loss in iteration 51 : 0.49766560363928664
Loss in iteration 52 : 0.5083364817951431
Loss in iteration 53 : 0.5027170804040043
Loss in iteration 54 : 0.497893657734975
Loss in iteration 55 : 0.49843152460027806
Loss in iteration 56 : 0.49969201148161063
Loss in iteration 57 : 0.49639406759719595
Loss in iteration 58 : 0.5033638756321971
Loss in iteration 59 : 0.5054248047728865
Loss in iteration 60 : 0.5004259124632281
Loss in iteration 61 : 0.4964428287198973
Loss in iteration 62 : 0.4956069933368103
Loss in iteration 63 : 0.4993655936713312
Loss in iteration 64 : 0.49425045649573296
Loss in iteration 65 : 0.4908360331569377
Loss in iteration 66 : 0.5010377833973132
Loss in iteration 67 : 0.4887261364075748
Loss in iteration 68 : 0.5009308408399751
Loss in iteration 69 : 0.49522553336762437
Loss in iteration 70 : 0.48730745933950814
Loss in iteration 71 : 0.4968666839686645
Loss in iteration 72 : 0.49501863570956045
Loss in iteration 73 : 0.5014832217799574
Loss in iteration 74 : 0.49626652094491175
Loss in iteration 75 : 0.4976322011707808
Loss in iteration 76 : 0.4880819291754684
Loss in iteration 77 : 0.4855156467571567
Loss in iteration 78 : 0.482303488029584
Loss in iteration 79 : 0.49080765015633965
Loss in iteration 80 : 0.4892651252522224
Loss in iteration 81 : 0.49386421910719636
Loss in iteration 82 : 0.485604763578044
Loss in iteration 83 : 0.4927424572768115
Loss in iteration 84 : 0.48227807867189826
Loss in iteration 85 : 0.4904206898976243
Loss in iteration 86 : 0.4874066556443526
Loss in iteration 87 : 0.48329771107407676
Loss in iteration 88 : 0.487777163463083
Loss in iteration 89 : 0.48416730256278045
Loss in iteration 90 : 0.48813284779289945
Loss in iteration 91 : 0.48601967058868384
Loss in iteration 92 : 0.4859140327818333
Loss in iteration 93 : 0.48949132987121324
Loss in iteration 94 : 0.48493328173300293
Loss in iteration 95 : 0.48743258906062337
Loss in iteration 96 : 0.4884407432855261
Loss in iteration 97 : 0.49060680565820275
Loss in iteration 98 : 0.4855515279797828
Loss in iteration 99 : 0.4790753053995747
Loss in iteration 100 : 0.4786413805676679
Loss in iteration 101 : 0.48194351254524487
Loss in iteration 102 : 0.48386937790068807
Loss in iteration 103 : 0.48940697418680656
Loss in iteration 104 : 0.48656572002892684
Loss in iteration 105 : 0.4755721601459824
Loss in iteration 106 : 0.4898681169471189
Loss in iteration 107 : 0.48569087650206416
Loss in iteration 108 : 0.47630448816365195
Loss in iteration 109 : 0.4896620865744736
Loss in iteration 110 : 0.4816937001746228
Loss in iteration 111 : 0.4758513259557773
Loss in iteration 112 : 0.4826668173250171
Loss in iteration 113 : 0.47979896389289345
Loss in iteration 114 : 0.4860714124932796
Loss in iteration 115 : 0.4821703374675229
Loss in iteration 116 : 0.47811270374090425
Loss in iteration 117 : 0.48019196751858384
Loss in iteration 118 : 0.48117531586668827
Loss in iteration 119 : 0.4825084747836118
Loss in iteration 120 : 0.48530050078896747
Loss in iteration 121 : 0.4774198265789238
Loss in iteration 122 : 0.48446945680985254
Loss in iteration 123 : 0.48224807450367335
Loss in iteration 124 : 0.47525084975764736
Loss in iteration 125 : 0.478392815103112
Loss in iteration 126 : 0.48341510636232815
Loss in iteration 127 : 0.4897256958303572
Loss in iteration 128 : 0.4756455861001794
Loss in iteration 129 : 0.48184645927810604
Loss in iteration 130 : 0.47874902278021036
Loss in iteration 131 : 0.4819553993153514
Loss in iteration 132 : 0.4768926069157958
Loss in iteration 133 : 0.4794995625279881
Loss in iteration 134 : 0.47703733951131655
Loss in iteration 135 : 0.4747398958291006
Loss in iteration 136 : 0.4819018338922782
Loss in iteration 137 : 0.4726014909619147
Loss in iteration 138 : 0.47559645599734257
Loss in iteration 139 : 0.46872528061658114
Loss in iteration 140 : 0.4795701620107213
Loss in iteration 141 : 0.4819496218464834
Loss in iteration 142 : 0.4797342263674974
Loss in iteration 143 : 0.46877511732046084
Loss in iteration 144 : 0.47596991522712406
Loss in iteration 145 : 0.4763203009799501
Loss in iteration 146 : 0.4705300871006002
Loss in iteration 147 : 0.47807286462217713
Loss in iteration 148 : 0.4754514487877838
Loss in iteration 149 : 0.4805171835379982
Loss in iteration 150 : 0.47766244811866104
Loss in iteration 151 : 0.47686523235077694
Loss in iteration 152 : 0.4845540396094643
Loss in iteration 153 : 0.47638638350005713
Loss in iteration 154 : 0.477630767869316
Loss in iteration 155 : 0.4701376728385928
Loss in iteration 156 : 0.47313217976917593
Loss in iteration 157 : 0.46899793612374663
Loss in iteration 158 : 0.46508432156160173
Loss in iteration 159 : 0.4748433025975059
Loss in iteration 160 : 0.46843736067963987
Loss in iteration 161 : 0.47010450860303615
Loss in iteration 162 : 0.47389861103157854
Loss in iteration 163 : 0.48136174946833044
Loss in iteration 164 : 0.48011583175973466
Loss in iteration 165 : 0.48132448204361056
Loss in iteration 166 : 0.4760641797621971
Loss in iteration 167 : 0.48571983539434516
Loss in iteration 168 : 0.4684434330861965
Loss in iteration 169 : 0.47535088537925674
Loss in iteration 170 : 0.4698121210027704
Loss in iteration 171 : 0.47753732182535
Loss in iteration 172 : 0.4713690339292937
Loss in iteration 173 : 0.47025868604247556
Loss in iteration 174 : 0.4695784102172829
Loss in iteration 175 : 0.47265954292702606
Loss in iteration 176 : 0.4790939953744208
Loss in iteration 177 : 0.4766042048367344
Loss in iteration 178 : 0.4721193643644872
Loss in iteration 179 : 0.4733937019174311
Loss in iteration 180 : 0.4767721960458249
Loss in iteration 181 : 0.4785987851273475
Loss in iteration 182 : 0.4782879964196701
Loss in iteration 183 : 0.4769755191042846
Loss in iteration 184 : 0.4719993223175574
Loss in iteration 185 : 0.4682124048150406
Loss in iteration 186 : 0.4671527105680529
Loss in iteration 187 : 0.462164121478783
Loss in iteration 188 : 0.4807652160333436
Loss in iteration 189 : 0.47819915319431266
Loss in iteration 190 : 0.47707426702330197
Loss in iteration 191 : 0.4768594093875047
Loss in iteration 192 : 0.47577400198145425
Loss in iteration 193 : 0.467387497991329
Loss in iteration 194 : 0.47009228005077425
Loss in iteration 195 : 0.47424065271660265
Loss in iteration 196 : 0.4792687177143174
Loss in iteration 197 : 0.46717485733501146
Loss in iteration 198 : 0.46927786339340083
Loss in iteration 199 : 0.47443583612152257
Loss in iteration 200 : 0.4690170256705568
Testing accuracy  of updater 0 on alg 0 with rate 0.7 = 0.7805, training accuracy 0.785375, time elapsed: 4450 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6816054441845584
Loss in iteration 3 : 0.668001043954389
Loss in iteration 4 : 0.6604991281971028
Loss in iteration 5 : 0.6523203879747769
Loss in iteration 6 : 0.644402339299084
Loss in iteration 7 : 0.636365171422467
Loss in iteration 8 : 0.6307793868607176
Loss in iteration 9 : 0.6240817962713723
Loss in iteration 10 : 0.6206831009514996
Loss in iteration 11 : 0.6139876270894241
Loss in iteration 12 : 0.6080267051089536
Loss in iteration 13 : 0.6057978307987822
Loss in iteration 14 : 0.6004552112537679
Loss in iteration 15 : 0.6007094721780115
Loss in iteration 16 : 0.5922514436328739
Loss in iteration 17 : 0.5932236224908009
Loss in iteration 18 : 0.5883446651823502
Loss in iteration 19 : 0.5854589494267471
Loss in iteration 20 : 0.5833101143399061
Loss in iteration 21 : 0.5786253782996701
Loss in iteration 22 : 0.5782349763676013
Loss in iteration 23 : 0.573439568816183
Loss in iteration 24 : 0.5739327552712451
Loss in iteration 25 : 0.565236593966821
Loss in iteration 26 : 0.5656937384499777
Loss in iteration 27 : 0.5579808933590937
Loss in iteration 28 : 0.5636705794087975
Loss in iteration 29 : 0.5557486685888398
Loss in iteration 30 : 0.5561367419751818
Loss in iteration 31 : 0.5490778037497205
Loss in iteration 32 : 0.5499008969918177
Loss in iteration 33 : 0.554263568815033
Loss in iteration 34 : 0.5525170321191433
Loss in iteration 35 : 0.5486156123654266
Loss in iteration 36 : 0.5472901277675755
Loss in iteration 37 : 0.5484427207738265
Loss in iteration 38 : 0.5445589456795417
Loss in iteration 39 : 0.5407707736875766
Loss in iteration 40 : 0.5367286464667917
Loss in iteration 41 : 0.5381641191844121
Loss in iteration 42 : 0.5348418655747864
Loss in iteration 43 : 0.5365765110854858
Loss in iteration 44 : 0.5312118892906531
Loss in iteration 45 : 0.5389877768736965
Loss in iteration 46 : 0.5281993420129514
Loss in iteration 47 : 0.5326140954003432
Loss in iteration 48 : 0.530945403351393
Loss in iteration 49 : 0.5346652035701059
Loss in iteration 50 : 0.5317241655155602
Loss in iteration 51 : 0.5233235232372908
Loss in iteration 52 : 0.5326029305671766
Loss in iteration 53 : 0.5268238362473149
Loss in iteration 54 : 0.5227900628447189
Loss in iteration 55 : 0.5228660736567221
Loss in iteration 56 : 0.5234120136859013
Loss in iteration 57 : 0.5198782706058476
Loss in iteration 58 : 0.5257857750476669
Loss in iteration 59 : 0.5266681751773375
Loss in iteration 60 : 0.5222732979820264
Loss in iteration 61 : 0.5182139181377504
Loss in iteration 62 : 0.5180631197187975
Loss in iteration 63 : 0.5208844937993061
Loss in iteration 64 : 0.5156024210243235
Loss in iteration 65 : 0.5132683068597078
Loss in iteration 66 : 0.521279850928116
Loss in iteration 67 : 0.5108517580775915
Loss in iteration 68 : 0.5193567566773057
Loss in iteration 69 : 0.5147690431997115
Loss in iteration 70 : 0.5088877594611794
Loss in iteration 71 : 0.517296847461328
Loss in iteration 72 : 0.5148170592819958
Loss in iteration 73 : 0.5194094555058939
Loss in iteration 74 : 0.515088213046144
Loss in iteration 75 : 0.5154324826947085
Loss in iteration 76 : 0.5079416996686285
Loss in iteration 77 : 0.5059790422305281
Loss in iteration 78 : 0.5026966548682202
Loss in iteration 79 : 0.509272710028324
Loss in iteration 80 : 0.5079872759963484
Loss in iteration 81 : 0.5120821161393864
Loss in iteration 82 : 0.5053499375033288
Loss in iteration 83 : 0.510910365153859
Loss in iteration 84 : 0.5015430218813398
Loss in iteration 85 : 0.5086224518225003
Loss in iteration 86 : 0.5054420417087315
Loss in iteration 87 : 0.5011073739292375
Loss in iteration 88 : 0.505368444149014
Loss in iteration 89 : 0.5022326867014255
Loss in iteration 90 : 0.5048215678161604
Loss in iteration 91 : 0.5035341100621301
Loss in iteration 92 : 0.5024777650099705
Loss in iteration 93 : 0.50604242949977
Loss in iteration 94 : 0.5015788714398316
Loss in iteration 95 : 0.5034562320202266
Loss in iteration 96 : 0.5046804596183202
Loss in iteration 97 : 0.5057068384135639
Loss in iteration 98 : 0.5019948381813307
Loss in iteration 99 : 0.4965251622121038
Loss in iteration 100 : 0.49486634203633756
Loss in iteration 101 : 0.49816146683013035
Loss in iteration 102 : 0.49947642557689564
Loss in iteration 103 : 0.5042086426703505
Loss in iteration 104 : 0.5018739909386645
Loss in iteration 105 : 0.49256075425861295
Loss in iteration 106 : 0.5044737550360756
Loss in iteration 107 : 0.5007799063627677
Loss in iteration 108 : 0.49304976004235324
Loss in iteration 109 : 0.5042674366797983
Loss in iteration 110 : 0.497454679082932
Loss in iteration 111 : 0.49180695544489017
Loss in iteration 112 : 0.497707199754607
Loss in iteration 113 : 0.49492479779867626
Loss in iteration 114 : 0.49925791250790624
Loss in iteration 115 : 0.49728889623394673
Loss in iteration 116 : 0.49415237965553643
Loss in iteration 117 : 0.4951560571515939
Loss in iteration 118 : 0.49583723516173717
Loss in iteration 119 : 0.4966717940702287
Loss in iteration 120 : 0.4994375704058071
Loss in iteration 121 : 0.4909487134792134
Loss in iteration 122 : 0.4976352611795497
Loss in iteration 123 : 0.49624777763537364
Loss in iteration 124 : 0.4896625594951371
Loss in iteration 125 : 0.49184794097135043
Loss in iteration 126 : 0.49697434491973946
Loss in iteration 127 : 0.5016622733981289
Loss in iteration 128 : 0.48920630316306496
Loss in iteration 129 : 0.4952463137570851
Loss in iteration 130 : 0.491731732332562
Loss in iteration 131 : 0.49437526447529573
Loss in iteration 132 : 0.4902154755011389
Loss in iteration 133 : 0.49243792129084396
Loss in iteration 134 : 0.49034629908517025
Loss in iteration 135 : 0.4873616215830693
Loss in iteration 136 : 0.4950236431877882
Loss in iteration 137 : 0.4861386733456353
Loss in iteration 138 : 0.4880649343659331
Loss in iteration 139 : 0.48335996284619215
Loss in iteration 140 : 0.491716880056403
Loss in iteration 141 : 0.4941705360656829
Loss in iteration 142 : 0.4917284408461303
Loss in iteration 143 : 0.4826993036826481
Loss in iteration 144 : 0.4883748612496769
Loss in iteration 145 : 0.488676349300158
Loss in iteration 146 : 0.48412923108945577
Loss in iteration 147 : 0.4904013551858852
Loss in iteration 148 : 0.4884861869492204
Loss in iteration 149 : 0.49209799596956316
Loss in iteration 150 : 0.4895973610970447
Loss in iteration 151 : 0.48949418055199606
Loss in iteration 152 : 0.4954673101859307
Loss in iteration 153 : 0.4883154000882386
Loss in iteration 154 : 0.4894853925704085
Loss in iteration 155 : 0.4821079047596238
Loss in iteration 156 : 0.484430569225207
Loss in iteration 157 : 0.4816518554180729
Loss in iteration 158 : 0.47798264945715113
Loss in iteration 159 : 0.48640305840883213
Loss in iteration 160 : 0.48098370151282654
Loss in iteration 161 : 0.48196022474689226
Loss in iteration 162 : 0.48629175716986545
Loss in iteration 163 : 0.49230612719436617
Loss in iteration 164 : 0.490754291127255
Loss in iteration 165 : 0.49109179154840066
Loss in iteration 166 : 0.4869279389020195
Loss in iteration 167 : 0.49540984487982515
Loss in iteration 168 : 0.48083008399203014
Loss in iteration 169 : 0.48625800691979826
Loss in iteration 170 : 0.48173538209306477
Loss in iteration 171 : 0.488675522839133
Loss in iteration 172 : 0.48254356698035544
Loss in iteration 173 : 0.48163045546984595
Loss in iteration 174 : 0.4811334085195682
Loss in iteration 175 : 0.4832545847764251
Loss in iteration 176 : 0.4889336794125299
Loss in iteration 177 : 0.48660579346290167
Loss in iteration 178 : 0.4818744129839856
Loss in iteration 179 : 0.4830874172144122
Loss in iteration 180 : 0.48674737154874786
Loss in iteration 181 : 0.48898108525914535
Loss in iteration 182 : 0.48784658533895975
Loss in iteration 183 : 0.48671796837535075
Loss in iteration 184 : 0.4830517932345102
Loss in iteration 185 : 0.4794265565245762
Loss in iteration 186 : 0.47833116345434223
Loss in iteration 187 : 0.473267433578954
Loss in iteration 188 : 0.48972576350706987
Loss in iteration 189 : 0.4885527410261304
Loss in iteration 190 : 0.4861384051254328
Loss in iteration 191 : 0.4870859635836976
Loss in iteration 192 : 0.48527938058071163
Loss in iteration 193 : 0.4779743035135266
Loss in iteration 194 : 0.4800216799042267
Loss in iteration 195 : 0.4845983757894996
Loss in iteration 196 : 0.4892695352873634
Loss in iteration 197 : 0.4772757195877065
Loss in iteration 198 : 0.4791304883185282
Loss in iteration 199 : 0.48346399087985537
Loss in iteration 200 : 0.4790743541268122
Testing accuracy  of updater 0 on alg 0 with rate 0.4 = 0.781, training accuracy 0.7815, time elapsed: 4804 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6886417315104332
Loss in iteration 3 : 0.683961868470794
Loss in iteration 4 : 0.681752741921946
Loss in iteration 5 : 0.6792657330014904
Loss in iteration 6 : 0.676163096590202
Loss in iteration 7 : 0.6732243977113445
Loss in iteration 8 : 0.6707164445534357
Loss in iteration 9 : 0.6674647824669869
Loss in iteration 10 : 0.665978577864914
Loss in iteration 11 : 0.6642596664371888
Loss in iteration 12 : 0.6599187738246493
Loss in iteration 13 : 0.6590093692130942
Loss in iteration 14 : 0.6573506388947077
Loss in iteration 15 : 0.6571728032613047
Loss in iteration 16 : 0.6526571092544449
Loss in iteration 17 : 0.65269988215151
Loss in iteration 18 : 0.6506048661719291
Loss in iteration 19 : 0.6474636707266512
Loss in iteration 20 : 0.6471874566498953
Loss in iteration 21 : 0.6446340900638591
Loss in iteration 22 : 0.6434720897311028
Loss in iteration 23 : 0.6417507686866879
Loss in iteration 24 : 0.6397657109459536
Loss in iteration 25 : 0.6368402267290274
Loss in iteration 26 : 0.6356351092511836
Loss in iteration 27 : 0.6308223293322627
Loss in iteration 28 : 0.6333778069637246
Loss in iteration 29 : 0.6296500003134626
Loss in iteration 30 : 0.6288639814615373
Loss in iteration 31 : 0.624517972425553
Loss in iteration 32 : 0.6257817332275949
Loss in iteration 33 : 0.6252155889165846
Loss in iteration 34 : 0.6249861360406608
Loss in iteration 35 : 0.6223270546952697
Loss in iteration 36 : 0.620900893348429
Loss in iteration 37 : 0.6222468770785832
Loss in iteration 38 : 0.6196145229285054
Loss in iteration 39 : 0.6181796706903855
Loss in iteration 40 : 0.615485855283826
Loss in iteration 41 : 0.6145151569237122
Loss in iteration 42 : 0.6112238754042154
Loss in iteration 43 : 0.6131321994546841
Loss in iteration 44 : 0.6086613809168507
Loss in iteration 45 : 0.6131783007690684
Loss in iteration 46 : 0.6069752986174674
Loss in iteration 47 : 0.6089665222778995
Loss in iteration 48 : 0.6077603483014892
Loss in iteration 49 : 0.6080757224669969
Loss in iteration 50 : 0.6054931472162342
Loss in iteration 51 : 0.601079788071364
Loss in iteration 52 : 0.6071636643535769
Loss in iteration 53 : 0.6020621180924657
Loss in iteration 54 : 0.5993843585371573
Loss in iteration 55 : 0.5989385317111567
Loss in iteration 56 : 0.5986620605984874
Loss in iteration 57 : 0.5966997343581311
Loss in iteration 58 : 0.5997644229632372
Loss in iteration 59 : 0.5991894308330762
Loss in iteration 60 : 0.5956381343625387
Loss in iteration 61 : 0.5925131877463845
Loss in iteration 62 : 0.5935226510437868
Loss in iteration 63 : 0.5941593807814041
Loss in iteration 64 : 0.5899191058884338
Loss in iteration 65 : 0.5880044383671347
Loss in iteration 66 : 0.5936215786943971
Loss in iteration 67 : 0.5865085643122578
Loss in iteration 68 : 0.5888812795258553
Loss in iteration 69 : 0.5866908619206626
Loss in iteration 70 : 0.5842896318747904
Loss in iteration 71 : 0.5902960867468956
Loss in iteration 72 : 0.5866723912042187
Loss in iteration 73 : 0.5869620491996557
Loss in iteration 74 : 0.5856521488090937
Loss in iteration 75 : 0.584195248588701
Loss in iteration 76 : 0.5800188534417371
Loss in iteration 77 : 0.5796539126642288
Loss in iteration 78 : 0.5769045178500064
Loss in iteration 79 : 0.5803115644182278
Loss in iteration 80 : 0.579033287278901
Loss in iteration 81 : 0.5813092062489129
Loss in iteration 82 : 0.5782068434770606
Loss in iteration 83 : 0.5797867234933315
Loss in iteration 84 : 0.5739606362149278
Loss in iteration 85 : 0.5782578581938866
Loss in iteration 86 : 0.5751698456028063
Loss in iteration 87 : 0.5712744274388848
Loss in iteration 88 : 0.5746842563033713
Loss in iteration 89 : 0.5722176083126501
Loss in iteration 90 : 0.5722944186132162
Loss in iteration 91 : 0.5711657237142932
Loss in iteration 92 : 0.5703680145902771
Loss in iteration 93 : 0.5722982188278792
Loss in iteration 94 : 0.5687885534605813
Loss in iteration 95 : 0.5698605945779015
Loss in iteration 96 : 0.5708135544984956
Loss in iteration 97 : 0.5693760814909756
Loss in iteration 98 : 0.5684140438810921
Loss in iteration 99 : 0.5658971054772166
Loss in iteration 100 : 0.5624919623526913
Loss in iteration 101 : 0.5649095286665196
Loss in iteration 102 : 0.5653034415908376
Loss in iteration 103 : 0.5670191476778224
Loss in iteration 104 : 0.5657280683394663
Loss in iteration 105 : 0.5606418698633675
Loss in iteration 106 : 0.5664216241330001
Loss in iteration 107 : 0.5640889242043853
Loss in iteration 108 : 0.5602330655211006
Loss in iteration 109 : 0.56503886688469
Loss in iteration 110 : 0.5624396855609255
Loss in iteration 111 : 0.5584524365550125
Loss in iteration 112 : 0.5614451437592742
Loss in iteration 113 : 0.5586243946703661
Loss in iteration 114 : 0.5594891776755037
Loss in iteration 115 : 0.5606588431101444
Loss in iteration 116 : 0.5588615808760594
Loss in iteration 117 : 0.5582947323154095
Loss in iteration 118 : 0.5581237057991445
Loss in iteration 119 : 0.5575627974523378
Loss in iteration 120 : 0.5597753600299769
Loss in iteration 121 : 0.5530911381268067
Loss in iteration 122 : 0.557096198501345
Loss in iteration 123 : 0.5574425699397292
Loss in iteration 124 : 0.5520753160305928
Loss in iteration 125 : 0.5523660518103513
Loss in iteration 126 : 0.557295148893164
Loss in iteration 127 : 0.5573409995041675
Loss in iteration 128 : 0.5500290482945367
Loss in iteration 129 : 0.5544725414052518
Loss in iteration 130 : 0.5511819540788526
Loss in iteration 131 : 0.5518268543825544
Loss in iteration 132 : 0.5493435684157905
Loss in iteration 133 : 0.5510366223053156
Loss in iteration 134 : 0.5493450719134536
Loss in iteration 135 : 0.5461287702019895
Loss in iteration 136 : 0.5526290805253307
Loss in iteration 137 : 0.5472499945742517
Loss in iteration 138 : 0.5464616363836988
Loss in iteration 139 : 0.5451886374128682
Loss in iteration 140 : 0.548386908764771
Loss in iteration 141 : 0.550578862707065
Loss in iteration 142 : 0.547025856310887
Loss in iteration 143 : 0.5427228853306435
Loss in iteration 144 : 0.5452497231700342
Loss in iteration 145 : 0.5454068756711195
Loss in iteration 146 : 0.5433321926580862
Loss in iteration 147 : 0.5465902128734565
Loss in iteration 148 : 0.5456456770421474
Loss in iteration 149 : 0.5472548939429893
Loss in iteration 150 : 0.5459647771054451
Loss in iteration 151 : 0.5461574123815383
Loss in iteration 152 : 0.5478274873019915
Loss in iteration 153 : 0.5431680615364124
Loss in iteration 154 : 0.5439881487723373
Loss in iteration 155 : 0.538876223177448
Loss in iteration 156 : 0.5391030959483526
Loss in iteration 157 : 0.5392799724040639
Loss in iteration 158 : 0.5366249923454768
Loss in iteration 159 : 0.5405298131100659
Loss in iteration 160 : 0.5374344051006862
Loss in iteration 161 : 0.5373517162862368
Loss in iteration 162 : 0.5416018627414726
Loss in iteration 163 : 0.5450959547574127
Loss in iteration 164 : 0.5424524328012019
Loss in iteration 165 : 0.541246660452187
Loss in iteration 166 : 0.5399735913374086
Loss in iteration 167 : 0.5437676628313186
Loss in iteration 168 : 0.5366710785077132
Loss in iteration 169 : 0.5382791392091996
Loss in iteration 170 : 0.5363619275754564
Loss in iteration 171 : 0.5390945044291345
Loss in iteration 172 : 0.5353309842987324
Loss in iteration 173 : 0.5348733316279259
Loss in iteration 174 : 0.5362006349158579
Loss in iteration 175 : 0.5343559505701245
Loss in iteration 176 : 0.5389527365874461
Loss in iteration 177 : 0.5366260039964249
Loss in iteration 178 : 0.5323678248973281
Loss in iteration 179 : 0.5318926609372536
Loss in iteration 180 : 0.5352635294271773
Loss in iteration 181 : 0.5386288189693199
Loss in iteration 182 : 0.5363399876327392
Loss in iteration 183 : 0.5355418036677049
Loss in iteration 184 : 0.533599429611444
Loss in iteration 185 : 0.5322603535947422
Loss in iteration 186 : 0.5298567082151381
Loss in iteration 187 : 0.5263181641696082
Loss in iteration 188 : 0.5360896616623982
Loss in iteration 189 : 0.5366252267502475
Loss in iteration 190 : 0.5336374043290211
Loss in iteration 191 : 0.5351390285343839
Loss in iteration 192 : 0.531569021222165
Loss in iteration 193 : 0.5290866263826896
Loss in iteration 194 : 0.5287938140340206
Loss in iteration 195 : 0.5331021591358525
Loss in iteration 196 : 0.5365896545591984
Loss in iteration 197 : 0.5275781571179513
Loss in iteration 198 : 0.5285940261138617
Loss in iteration 199 : 0.5308352018358821
Loss in iteration 200 : 0.5285415768310228
Testing accuracy  of updater 0 on alg 0 with rate 0.09999999999999998 = 0.77, training accuracy 0.771875, time elapsed: 5151 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 2.36925253869775
Loss in iteration 3 : 20.695514455281167
Loss in iteration 4 : 8.048040775290472
Loss in iteration 5 : 23.676036626811257
Loss in iteration 6 : 30.19526156334139
Loss in iteration 7 : 12.509263808789152
Loss in iteration 8 : 18.675140475691887
Loss in iteration 9 : 28.577094814069078
Loss in iteration 10 : 11.364576861419689
Loss in iteration 11 : 10.667791175872308
Loss in iteration 12 : 19.46935473319377
Loss in iteration 13 : 10.326021551485548
Loss in iteration 14 : 6.9061414805122014
Loss in iteration 15 : 15.136039086854947
Loss in iteration 16 : 12.290315639853427
Loss in iteration 17 : 5.700550580195759
Loss in iteration 18 : 10.608741425553276
Loss in iteration 19 : 12.240510848745085
Loss in iteration 20 : 6.962113282890734
Loss in iteration 21 : 7.1150439942930825
Loss in iteration 22 : 10.724258146068726
Loss in iteration 23 : 8.651113306653976
Loss in iteration 24 : 6.160597799798005
Loss in iteration 25 : 7.811284324019526
Loss in iteration 26 : 8.623148040593557
Loss in iteration 27 : 5.713959234775724
Loss in iteration 28 : 6.403740055525916
Loss in iteration 29 : 7.066158626094162
Loss in iteration 30 : 6.4804752381260196
Loss in iteration 31 : 5.048499352091494
Loss in iteration 32 : 6.159087858607782
Loss in iteration 33 : 6.457419078258913
Loss in iteration 34 : 5.267923736001597
Loss in iteration 35 : 5.520718249677283
Loss in iteration 36 : 6.062999527572959
Loss in iteration 37 : 4.829477935256484
Loss in iteration 38 : 4.660145188109743
Loss in iteration 39 : 5.072626559477846
Loss in iteration 40 : 3.9910601548412976
Loss in iteration 41 : 4.094486843228432
Loss in iteration 42 : 4.5091872940515785
Loss in iteration 43 : 3.685964849437713
Loss in iteration 44 : 3.591785976795051
Loss in iteration 45 : 3.8398914527011674
Loss in iteration 46 : 3.17547286218034
Loss in iteration 47 : 3.744927695229109
Loss in iteration 48 : 2.9583190510215114
Loss in iteration 49 : 3.3522020134974486
Loss in iteration 50 : 3.060909537713128
Loss in iteration 51 : 2.735439275717333
Loss in iteration 52 : 2.8357483236431404
Loss in iteration 53 : 2.4499136911594572
Loss in iteration 54 : 2.5362251832871103
Loss in iteration 55 : 2.2529793563133973
Loss in iteration 56 : 2.4425526694152926
Loss in iteration 57 : 2.179560797144599
Loss in iteration 58 : 2.3189820990219547
Loss in iteration 59 : 2.3557593934246945
Loss in iteration 60 : 2.0386441700645634
Loss in iteration 61 : 2.1814740443513814
Loss in iteration 62 : 1.877430090581072
Loss in iteration 63 : 1.8757500192430996
Loss in iteration 64 : 1.9184281658581683
Loss in iteration 65 : 1.727948138807209
Loss in iteration 66 : 1.7464467876437686
Loss in iteration 67 : 1.551841597140995
Loss in iteration 68 : 1.5203397787499038
Loss in iteration 69 : 1.3962663382265172
Loss in iteration 70 : 1.2972180750982076
Loss in iteration 71 : 1.2600538808845392
Loss in iteration 72 : 1.3184490378873086
Loss in iteration 73 : 1.7377798975635668
Loss in iteration 74 : 3.2064851202410387
Loss in iteration 75 : 3.3627127052142023
Loss in iteration 76 : 2.6314575940236233
Loss in iteration 77 : 1.4640930605120788
Loss in iteration 78 : 1.1355028890839838
Loss in iteration 79 : 1.2138312666472848
Loss in iteration 80 : 1.4828503230024903
Loss in iteration 81 : 1.7869885259307747
Loss in iteration 82 : 2.54581924129131
Loss in iteration 83 : 2.5552943456571886
Loss in iteration 84 : 2.515144019756764
Loss in iteration 85 : 1.8449636140223413
Loss in iteration 86 : 1.9062083979876299
Loss in iteration 87 : 1.627511238091697
Loss in iteration 88 : 1.5883560215843167
Loss in iteration 89 : 1.6333581596292712
Loss in iteration 90 : 2.3444891523560942
Loss in iteration 91 : 2.334425926774478
Loss in iteration 92 : 2.581004156055121
Loss in iteration 93 : 2.097454959857684
Loss in iteration 94 : 2.4548303301633623
Loss in iteration 95 : 2.2398730423554207
Loss in iteration 96 : 2.215146221573959
Loss in iteration 97 : 1.6289619904840187
Loss in iteration 98 : 1.3038549340498997
Loss in iteration 99 : 1.2088505078321274
Loss in iteration 100 : 1.6625013766844483
Loss in iteration 101 : 2.6852461779849133
Loss in iteration 102 : 4.129425513313911
Loss in iteration 103 : 1.869456698507147
Loss in iteration 104 : 1.2211950243705272
Loss in iteration 105 : 1.0889591678907347
Loss in iteration 106 : 1.3355783264634395
Loss in iteration 107 : 1.9932473188460151
Loss in iteration 108 : 3.663076917975428
Loss in iteration 109 : 2.8925822649040573
Loss in iteration 110 : 1.8446951480366058
Loss in iteration 111 : 1.2725244892402416
Loss in iteration 112 : 1.2553829803598962
Loss in iteration 113 : 1.2307227676163508
Loss in iteration 114 : 1.4772267369627523
Loss in iteration 115 : 1.7032816206038894
Loss in iteration 116 : 2.5912496490634345
Loss in iteration 117 : 2.6950393744147227
Loss in iteration 118 : 2.447235195498988
Loss in iteration 119 : 1.8254976166959118
Loss in iteration 120 : 1.6187497274151634
Loss in iteration 121 : 1.4478891879789966
Loss in iteration 122 : 1.5129118306224003
Loss in iteration 123 : 1.5682163945186938
Loss in iteration 124 : 1.915037483581504
Loss in iteration 125 : 2.0738158345201305
Loss in iteration 126 : 2.2833306468394188
Loss in iteration 127 : 2.201842868078672
Loss in iteration 128 : 2.337805122801387
Loss in iteration 129 : 2.010850372889322
Loss in iteration 130 : 1.9489533010529911
Loss in iteration 131 : 1.4939077816813273
Loss in iteration 132 : 1.291928197932167
Loss in iteration 133 : 1.3406802752052456
Loss in iteration 134 : 1.7837942434828111
Loss in iteration 135 : 2.7849462138789773
Loss in iteration 136 : 3.415820732769847
Loss in iteration 137 : 2.0747886621234155
Loss in iteration 138 : 1.7226172008066714
Loss in iteration 139 : 1.5198375878844979
Loss in iteration 140 : 1.6374787377429836
Loss in iteration 141 : 1.6088331401045743
Loss in iteration 142 : 1.7263148981798542
Loss in iteration 143 : 1.7953214044742931
Loss in iteration 144 : 2.150254047787665
Loss in iteration 145 : 2.140802316642149
Loss in iteration 146 : 2.269099952142306
Loss in iteration 147 : 2.0784847000142133
Loss in iteration 148 : 2.023484750184516
Loss in iteration 149 : 1.7080097195822292
Loss in iteration 150 : 1.669679177354981
Loss in iteration 151 : 1.7740341358278822
Loss in iteration 152 : 2.367730310162814
Loss in iteration 153 : 2.65709166190133
Loss in iteration 154 : 2.6300161166541574
Loss in iteration 155 : 1.5088928306412923
Loss in iteration 156 : 1.099785391163676
Loss in iteration 157 : 1.0394468260901277
Loss in iteration 158 : 1.275342620033405
Loss in iteration 159 : 2.0158192685784395
Loss in iteration 160 : 2.810761655333521
Loss in iteration 161 : 3.3306910454771232
Loss in iteration 162 : 1.5880520801948164
Loss in iteration 163 : 1.086856953015005
Loss in iteration 164 : 0.9663564963026618
Loss in iteration 165 : 1.0916858692214235
Loss in iteration 166 : 1.5634507573890164
Loss in iteration 167 : 3.164212533263377
Loss in iteration 168 : 3.9861640780362535
Loss in iteration 169 : 2.048520898165896
Loss in iteration 170 : 1.2788719762987253
Loss in iteration 171 : 1.1741953896522366
Loss in iteration 172 : 1.1416590541842928
Loss in iteration 173 : 1.1927877772953066
Loss in iteration 174 : 1.5909869695359418
Loss in iteration 175 : 2.804094254173963
Loss in iteration 176 : 4.106810354426718
Loss in iteration 177 : 1.6503919976906385
Loss in iteration 178 : 1.0077242823221684
Loss in iteration 179 : 1.0448031742707187
Loss in iteration 180 : 1.4835457484367554
Loss in iteration 181 : 2.943412029581616
Loss in iteration 182 : 3.7942381132829968
Loss in iteration 183 : 2.8946870801316384
Loss in iteration 184 : 1.2375374659295475
Loss in iteration 185 : 0.9738013157726791
Loss in iteration 186 : 1.4666399011508198
Loss in iteration 187 : 2.165539320196356
Loss in iteration 188 : 2.8278300501098776
Loss in iteration 189 : 1.7003258079606856
Loss in iteration 190 : 1.2045681069585967
Loss in iteration 191 : 0.9951844166464912
Loss in iteration 192 : 1.0408838779722285
Loss in iteration 193 : 1.3179378785295224
Loss in iteration 194 : 2.5225280055526578
Loss in iteration 195 : 4.7755755133368325
Loss in iteration 196 : 2.1447288063452925
Loss in iteration 197 : 1.5394442412459115
Loss in iteration 198 : 1.754889017243873
Loss in iteration 199 : 2.996604213140986
Loss in iteration 200 : 3.1441726166934516
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.7245, training accuracy 0.718875, time elapsed: 5095 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 1.698262261146221
Loss in iteration 3 : 13.58177761090313
Loss in iteration 4 : 4.101250808458793
Loss in iteration 5 : 17.616321805982633
Loss in iteration 6 : 21.798962999219512
Loss in iteration 7 : 9.081539688432024
Loss in iteration 8 : 13.30492846688601
Loss in iteration 9 : 20.249516527235393
Loss in iteration 10 : 7.672038325213617
Loss in iteration 11 : 8.154064199197421
Loss in iteration 12 : 14.165880385553749
Loss in iteration 13 : 7.345095102499774
Loss in iteration 14 : 4.974412160580856
Loss in iteration 15 : 11.021944583981673
Loss in iteration 16 : 8.78947598148985
Loss in iteration 17 : 3.9875294105059496
Loss in iteration 18 : 7.704724340211016
Loss in iteration 19 : 8.835960819969166
Loss in iteration 20 : 4.927737524104733
Loss in iteration 21 : 5.041796628597139
Loss in iteration 22 : 7.701452577495219
Loss in iteration 23 : 6.2201591559733815
Loss in iteration 24 : 4.342020901099835
Loss in iteration 25 : 5.532972726288161
Loss in iteration 26 : 6.217572273716397
Loss in iteration 27 : 4.097321837727975
Loss in iteration 28 : 4.51202509621729
Loss in iteration 29 : 5.0559515758354125
Loss in iteration 30 : 4.704045827403282
Loss in iteration 31 : 3.584814539107945
Loss in iteration 32 : 4.333055404506024
Loss in iteration 33 : 4.678678486685353
Loss in iteration 34 : 3.7957751140225167
Loss in iteration 35 : 3.8802569633891224
Loss in iteration 36 : 4.3726467030648655
Loss in iteration 37 : 3.535231206405234
Loss in iteration 38 : 3.2660788911720817
Loss in iteration 39 : 3.6582742483979014
Loss in iteration 40 : 2.9395574887050153
Loss in iteration 41 : 2.866857677227966
Loss in iteration 42 : 3.2905447647486956
Loss in iteration 43 : 2.7115575339529356
Loss in iteration 44 : 2.523264801708459
Loss in iteration 45 : 2.8841003870814363
Loss in iteration 46 : 2.265919992804075
Loss in iteration 47 : 2.7168988767871682
Loss in iteration 48 : 2.2328612460970314
Loss in iteration 49 : 2.343310189031168
Loss in iteration 50 : 2.3478923045100255
Loss in iteration 51 : 1.8927578944739607
Loss in iteration 52 : 2.143764845746842
Loss in iteration 53 : 1.7371954019881213
Loss in iteration 54 : 1.8723400795902914
Loss in iteration 55 : 1.6191864813798527
Loss in iteration 56 : 1.7838990827172896
Loss in iteration 57 : 1.58464350171404
Loss in iteration 58 : 1.7744932036387846
Loss in iteration 59 : 1.6027129250772565
Loss in iteration 60 : 1.6372421794461085
Loss in iteration 61 : 1.4710365789854605
Loss in iteration 62 : 1.4022361874269034
Loss in iteration 63 : 1.5069330028553016
Loss in iteration 64 : 1.3112385645855698
Loss in iteration 65 : 1.2382476932248452
Loss in iteration 66 : 1.2683753787407148
Loss in iteration 67 : 1.1314771077607353
Loss in iteration 68 : 1.2234117754705252
Loss in iteration 69 : 1.203310302095689
Loss in iteration 70 : 1.1059039690541492
Loss in iteration 71 : 0.996843985133614
Loss in iteration 72 : 0.9163475402383369
Loss in iteration 73 : 0.8817285591052753
Loss in iteration 74 : 0.9151414095884565
Loss in iteration 75 : 1.2648781294364575
Loss in iteration 76 : 2.2098384464502505
Loss in iteration 77 : 2.222656706596988
Loss in iteration 78 : 1.9620884469664845
Loss in iteration 79 : 1.3399912197692854
Loss in iteration 80 : 1.1528261986942543
Loss in iteration 81 : 0.9497145204802315
Loss in iteration 82 : 0.8220850622854314
Loss in iteration 83 : 0.8161437243003876
Loss in iteration 84 : 0.7438882599592017
Loss in iteration 85 : 0.7498268565247286
Loss in iteration 86 : 0.7812553091667377
Loss in iteration 87 : 1.10128506949445
Loss in iteration 88 : 2.6953837230329105
Loss in iteration 89 : 3.363100934223248
Loss in iteration 90 : 1.9675983840179152
Loss in iteration 91 : 1.0541857887939559
Loss in iteration 92 : 1.0368190322566617
Loss in iteration 93 : 1.4292311830231823
Loss in iteration 94 : 2.5700716442333804
Loss in iteration 95 : 2.0073323187507013
Loss in iteration 96 : 1.155973798690342
Loss in iteration 97 : 0.8811489100653215
Loss in iteration 98 : 1.0700417055732347
Loss in iteration 99 : 1.5970663267529341
Loss in iteration 100 : 1.4292592168813383
Loss in iteration 101 : 1.19520742126074
Loss in iteration 102 : 0.9103425071030401
Loss in iteration 103 : 0.8301716635398105
Loss in iteration 104 : 0.7601475387270289
Loss in iteration 105 : 0.7365871985546105
Loss in iteration 106 : 1.0101677412389636
Loss in iteration 107 : 2.5154377764656113
Loss in iteration 108 : 3.6199798598333266
Loss in iteration 109 : 1.4110701109390076
Loss in iteration 110 : 0.8814488841429532
Loss in iteration 111 : 0.8279491185683113
Loss in iteration 112 : 1.018356652638283
Loss in iteration 113 : 1.5430610492909482
Loss in iteration 114 : 2.4790647494232636
Loss in iteration 115 : 1.9633412590857622
Loss in iteration 116 : 1.028006238672868
Loss in iteration 117 : 0.7912138202709655
Loss in iteration 118 : 0.876591605184425
Loss in iteration 119 : 1.3138898822100968
Loss in iteration 120 : 1.810752938386117
Loss in iteration 121 : 1.3410800132937302
Loss in iteration 122 : 1.1095044428709095
Loss in iteration 123 : 0.9515006034495616
Loss in iteration 124 : 0.9495790351064982
Loss in iteration 125 : 1.0102251729651996
Loss in iteration 126 : 1.3779038244530801
Loss in iteration 127 : 1.949473719842401
Loss in iteration 128 : 2.425839903816398
Loss in iteration 129 : 1.4305948223428375
Loss in iteration 130 : 0.9872608759298734
Loss in iteration 131 : 0.8018826339023646
Loss in iteration 132 : 0.7356242648144441
Loss in iteration 133 : 0.7907250187130253
Loss in iteration 134 : 0.9039266364128627
Loss in iteration 135 : 1.4822321001148937
Loss in iteration 136 : 2.591836089943371
Loss in iteration 137 : 2.7261548144240297
Loss in iteration 138 : 1.1368031188557086
Loss in iteration 139 : 0.7362396408335882
Loss in iteration 140 : 0.7405102700704986
Loss in iteration 141 : 0.7519882983711093
Loss in iteration 142 : 0.8725870652481398
Loss in iteration 143 : 1.3441639106080363
Loss in iteration 144 : 2.6145689822741134
Loss in iteration 145 : 2.0693396361317227
Loss in iteration 146 : 1.4074803397069773
Loss in iteration 147 : 1.037375955377267
Loss in iteration 148 : 0.891445109721381
Loss in iteration 149 : 0.8369687637669664
Loss in iteration 150 : 0.8518643328655179
Loss in iteration 151 : 0.988963514406225
Loss in iteration 152 : 1.6775437779943443
Loss in iteration 153 : 2.811469949786675
Loss in iteration 154 : 2.537206019727359
Loss in iteration 155 : 0.9228513570060222
Loss in iteration 156 : 0.771457736379284
Loss in iteration 157 : 1.3789673803546947
Loss in iteration 158 : 2.1544695872676134
Loss in iteration 159 : 1.8473220438695257
Loss in iteration 160 : 0.9533057970054468
Loss in iteration 161 : 0.7779693528780988
Loss in iteration 162 : 1.0005407766401435
Loss in iteration 163 : 1.6820381275474845
Loss in iteration 164 : 2.3951982010736237
Loss in iteration 165 : 1.260485788815801
Loss in iteration 166 : 0.785802997583348
Loss in iteration 167 : 0.9824652218039074
Loss in iteration 168 : 1.7068341268952218
Loss in iteration 169 : 2.6914883584771276
Loss in iteration 170 : 1.0658161822819467
Loss in iteration 171 : 0.749751410024371
Loss in iteration 172 : 1.0862123703684616
Loss in iteration 173 : 1.7738779032945944
Loss in iteration 174 : 2.2379890375397276
Loss in iteration 175 : 1.294363108714618
Loss in iteration 176 : 0.8505776433639147
Loss in iteration 177 : 0.7940384105870332
Loss in iteration 178 : 1.1668491406022432
Loss in iteration 179 : 2.0757596455488105
Loss in iteration 180 : 1.924061930336878
Loss in iteration 181 : 1.333484259530767
Loss in iteration 182 : 1.0703683749544723
Loss in iteration 183 : 1.1301990128012385
Loss in iteration 184 : 1.2036874667994828
Loss in iteration 185 : 1.5473950491867647
Loss in iteration 186 : 1.56631735831248
Loss in iteration 187 : 1.6632943259604855
Loss in iteration 188 : 1.410499082553021
Loss in iteration 189 : 1.2427598586083568
Loss in iteration 190 : 1.1861655648546579
Loss in iteration 191 : 1.237864361614849
Loss in iteration 192 : 1.337587167417323
Loss in iteration 193 : 1.3766278259066942
Loss in iteration 194 : 1.2028313771884913
Loss in iteration 195 : 1.0870216797731955
Loss in iteration 196 : 1.1035351893384804
Loss in iteration 197 : 1.4285905118666202
Loss in iteration 198 : 1.9589303877167012
Loss in iteration 199 : 2.42091505790987
Loss in iteration 200 : 1.6209354496316142
Testing accuracy  of updater 1 on alg 0 with rate 7.0 = 0.754, training accuracy 0.74575, time elapsed: 4899 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 1.0873486084939135
Loss in iteration 3 : 6.141365856785726
Loss in iteration 4 : 0.5616038188458452
Loss in iteration 5 : 5.2369322071020505
Loss in iteration 6 : 1.2497606667893566
Loss in iteration 7 : 7.754164331536471
Loss in iteration 8 : 6.073741302998894
Loss in iteration 9 : 2.185507648340741
Loss in iteration 10 : 4.453341406605054
Loss in iteration 11 : 0.9950489950281068
Loss in iteration 12 : 3.243717079650605
Loss in iteration 13 : 2.290326752954597
Loss in iteration 14 : 1.65512959506291
Loss in iteration 15 : 2.947007485754245
Loss in iteration 16 : 1.439925168599375
Loss in iteration 17 : 2.3032450305087124
Loss in iteration 18 : 2.2999686652740072
Loss in iteration 19 : 1.6173969103383623
Loss in iteration 20 : 2.303499136644133
Loss in iteration 21 : 1.818244218123522
Loss in iteration 22 : 1.7597351390222762
Loss in iteration 23 : 2.03522959587305
Loss in iteration 24 : 1.6925251393345924
Loss in iteration 25 : 1.5903506524485216
Loss in iteration 26 : 1.7799757491537098
Loss in iteration 27 : 1.310454737085552
Loss in iteration 28 : 1.6699364421756313
Loss in iteration 29 : 1.3930869854524814
Loss in iteration 30 : 1.267396774085231
Loss in iteration 31 : 1.3478595551512795
Loss in iteration 32 : 1.1323990640958022
Loss in iteration 33 : 1.2991080231530339
Loss in iteration 34 : 1.1411025439024993
Loss in iteration 35 : 1.1221399250387925
Loss in iteration 36 : 1.0982775598463648
Loss in iteration 37 : 1.034235218970524
Loss in iteration 38 : 0.9951506696188459
Loss in iteration 39 : 0.9324301075791656
Loss in iteration 40 : 0.8572976833875289
Loss in iteration 41 : 0.8542073958004565
Loss in iteration 42 : 0.8399876141561802
Loss in iteration 43 : 0.8884487957550826
Loss in iteration 44 : 0.7356917015826813
Loss in iteration 45 : 0.7794471333631026
Loss in iteration 46 : 0.8005417898132633
Loss in iteration 47 : 0.7662653436180477
Loss in iteration 48 : 0.682896773591824
Loss in iteration 49 : 0.7743524659886638
Loss in iteration 50 : 0.8697117431164606
Loss in iteration 51 : 0.8754295145682464
Loss in iteration 52 : 0.7736158827496709
Loss in iteration 53 : 0.6528832156899295
Loss in iteration 54 : 0.5847744775121426
Loss in iteration 55 : 0.5955359591763636
Loss in iteration 56 : 0.6746727558805118
Loss in iteration 57 : 0.8605917280494462
Loss in iteration 58 : 1.0567148207863386
Loss in iteration 59 : 1.0654527299521723
Loss in iteration 60 : 0.7698897503015135
Loss in iteration 61 : 0.6229375925991528
Loss in iteration 62 : 0.5410987277394349
Loss in iteration 63 : 0.5506595472464527
Loss in iteration 64 : 0.6004094643003004
Loss in iteration 65 : 0.6273426163488991
Loss in iteration 66 : 0.681608150971
Loss in iteration 67 : 0.6779616502936175
Loss in iteration 68 : 0.8429651600028087
Loss in iteration 69 : 0.9886616730972201
Loss in iteration 70 : 1.104468318175972
Loss in iteration 71 : 0.9407987211161908
Loss in iteration 72 : 0.8426028013899549
Loss in iteration 73 : 0.790623445246904
Loss in iteration 74 : 0.77538812958452
Loss in iteration 75 : 0.7859579840292352
Loss in iteration 76 : 0.7880567334787063
Loss in iteration 77 : 0.8122973953163666
Loss in iteration 78 : 0.888589942616592
Loss in iteration 79 : 0.9445484714412034
Loss in iteration 80 : 0.9566040835734548
Loss in iteration 81 : 0.7960748132933436
Loss in iteration 82 : 0.6744306411587975
Loss in iteration 83 : 0.6612939589858989
Loss in iteration 84 : 0.6583618805875837
Loss in iteration 85 : 0.756316227548427
Loss in iteration 86 : 0.9414503090799756
Loss in iteration 87 : 1.0171441080775452
Loss in iteration 88 : 0.8265576784571563
Loss in iteration 89 : 0.7083151171687679
Loss in iteration 90 : 0.6984459320264653
Loss in iteration 91 : 0.6622272644865135
Loss in iteration 92 : 0.6510015886257872
Loss in iteration 93 : 0.6762347621091315
Loss in iteration 94 : 0.7990404271732295
Loss in iteration 95 : 0.9892497421976955
Loss in iteration 96 : 1.0551874003203043
Loss in iteration 97 : 0.8798802178926227
Loss in iteration 98 : 0.6958070514797853
Loss in iteration 99 : 0.6281429872725434
Loss in iteration 100 : 0.6572371498082193
Loss in iteration 101 : 0.7224944040785455
Loss in iteration 102 : 0.8318800858510829
Loss in iteration 103 : 0.9204850133877598
Loss in iteration 104 : 0.9251933683920158
Loss in iteration 105 : 0.8792178225292547
Loss in iteration 106 : 0.830665270167552
Loss in iteration 107 : 0.7301896011925902
Loss in iteration 108 : 0.6198312640286769
Loss in iteration 109 : 0.6061437112217404
Loss in iteration 110 : 0.548590096204957
Loss in iteration 111 : 0.518578062195976
Loss in iteration 112 : 0.5296997339459758
Loss in iteration 113 : 0.5143990917233835
Loss in iteration 114 : 0.547378256319918
Loss in iteration 115 : 0.5964372952789151
Loss in iteration 116 : 0.9032770224726463
Loss in iteration 117 : 1.6410491860919936
Loss in iteration 118 : 1.3253730064375193
Loss in iteration 119 : 0.8653318015102701
Loss in iteration 120 : 0.6310148336046735
Loss in iteration 121 : 0.5523095979847463
Loss in iteration 122 : 0.5546148718544553
Loss in iteration 123 : 0.5528557927660455
Loss in iteration 124 : 0.6165930508872574
Loss in iteration 125 : 0.804651434469411
Loss in iteration 126 : 1.1132091894153484
Loss in iteration 127 : 1.335692092151933
Loss in iteration 128 : 0.8692273295610826
Loss in iteration 129 : 0.5978336144293053
Loss in iteration 130 : 0.5427922958632909
Loss in iteration 131 : 0.655554402505941
Loss in iteration 132 : 0.9443618891178234
Loss in iteration 133 : 1.1045048534744306
Loss in iteration 134 : 0.9049801525118603
Loss in iteration 135 : 0.6760735307982318
Loss in iteration 136 : 0.5517757682062625
Loss in iteration 137 : 0.5355979019012426
Loss in iteration 138 : 0.6040327671739218
Loss in iteration 139 : 0.6652158534812276
Loss in iteration 140 : 0.8116902408681874
Loss in iteration 141 : 0.9295299098907946
Loss in iteration 142 : 1.0304533696651101
Loss in iteration 143 : 1.0453782445304869
Loss in iteration 144 : 0.9010523622682821
Loss in iteration 145 : 0.7464118710476569
Loss in iteration 146 : 0.6848308877425183
Loss in iteration 147 : 0.6979547348270599
Loss in iteration 148 : 0.722399440886043
Loss in iteration 149 : 0.823664112446055
Loss in iteration 150 : 0.9367868693282465
Loss in iteration 151 : 0.9256002726430421
Loss in iteration 152 : 0.8040403464422878
Loss in iteration 153 : 0.6542602367751437
Loss in iteration 154 : 0.5678629258388596
Loss in iteration 155 : 0.5192316670505669
Loss in iteration 156 : 0.5240482374345277
Loss in iteration 157 : 0.5117194934807254
Loss in iteration 158 : 0.5344921307054987
Loss in iteration 159 : 0.6805845520897021
Loss in iteration 160 : 1.0863535583910742
Loss in iteration 161 : 1.5025888838207961
Loss in iteration 162 : 1.2258986738879594
Loss in iteration 163 : 0.9128235340393048
Loss in iteration 164 : 0.7766278484351763
Loss in iteration 165 : 0.708047343191465
Loss in iteration 166 : 0.6108167113831788
Loss in iteration 167 : 0.5798816705816998
Loss in iteration 168 : 0.5350289304908502
Loss in iteration 169 : 0.6502848177393192
Loss in iteration 170 : 0.8062187780245644
Loss in iteration 171 : 0.9992731915696164
Loss in iteration 172 : 0.9590540339351292
Loss in iteration 173 : 0.8302879898434335
Loss in iteration 174 : 0.7107115692437043
Loss in iteration 175 : 0.6845091700329334
Loss in iteration 176 : 0.6747508029116239
Loss in iteration 177 : 0.6875500610301082
Loss in iteration 178 : 0.7466270064267001
Loss in iteration 179 : 0.8189999924698474
Loss in iteration 180 : 0.8556364616890743
Loss in iteration 181 : 0.8028114433849485
Loss in iteration 182 : 0.8101731360512557
Loss in iteration 183 : 0.8245700048459523
Loss in iteration 184 : 0.7596458715736261
Loss in iteration 185 : 0.7180929424274196
Loss in iteration 186 : 0.7274541305948422
Loss in iteration 187 : 0.7981158080132007
Loss in iteration 188 : 0.9403944799261663
Loss in iteration 189 : 0.9636410954150254
Loss in iteration 190 : 0.9568621521901707
Loss in iteration 191 : 0.8668191490520336
Loss in iteration 192 : 0.7509884138998235
Loss in iteration 193 : 0.6072532384734741
Loss in iteration 194 : 0.5395211127329599
Loss in iteration 195 : 0.5372911394011133
Loss in iteration 196 : 0.6249513706032143
Loss in iteration 197 : 0.7556715338564326
Loss in iteration 198 : 0.9262957141808129
Loss in iteration 199 : 1.042804673583547
Loss in iteration 200 : 1.034533952177491
Testing accuracy  of updater 1 on alg 0 with rate 4.0 = 0.6575, training accuracy 0.66225, time elapsed: 5635 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6945156839104583
Loss in iteration 3 : 0.6514900343167045
Loss in iteration 4 : 0.5920665305735815
Loss in iteration 5 : 0.5615642624544503
Loss in iteration 6 : 0.5456033944816365
Loss in iteration 7 : 0.5093672393324372
Loss in iteration 8 : 0.5049325137287334
Loss in iteration 9 : 0.4867141702441016
Loss in iteration 10 : 0.4947394038113815
Loss in iteration 11 : 0.4736710690606923
Loss in iteration 12 : 0.47345991144456556
Loss in iteration 13 : 0.4871445440288721
Loss in iteration 14 : 0.4766604429476295
Loss in iteration 15 : 0.49077786839843784
Loss in iteration 16 : 0.4775738632305429
Loss in iteration 17 : 0.4973741944033922
Loss in iteration 18 : 0.49294919973381757
Loss in iteration 19 : 0.4949975347332983
Loss in iteration 20 : 0.49471449988978405
Loss in iteration 21 : 0.47751405693627663
Loss in iteration 22 : 0.5006205759213018
Loss in iteration 23 : 0.47819314867282364
Loss in iteration 24 : 0.49060483076994693
Loss in iteration 25 : 0.46644513894329126
Loss in iteration 26 : 0.4702536896468483
Loss in iteration 27 : 0.463081987482488
Loss in iteration 28 : 0.4831335765608286
Loss in iteration 29 : 0.4598345735850676
Loss in iteration 30 : 0.4660969863379748
Loss in iteration 31 : 0.45571728184966764
Loss in iteration 32 : 0.45577522303421897
Loss in iteration 33 : 0.47393663486432347
Loss in iteration 34 : 0.4733180385013273
Loss in iteration 35 : 0.4644991474086907
Loss in iteration 36 : 0.47269014706105683
Loss in iteration 37 : 0.46920936861342916
Loss in iteration 38 : 0.4649202659205241
Loss in iteration 39 : 0.4584235660320052
Loss in iteration 40 : 0.4515568345766609
Loss in iteration 41 : 0.4534048361645825
Loss in iteration 42 : 0.46596186872221174
Loss in iteration 43 : 0.46236108217270694
Loss in iteration 44 : 0.4514431683529667
Loss in iteration 45 : 0.4701870402722539
Loss in iteration 46 : 0.4541495526107817
Loss in iteration 47 : 0.4632262997249507
Loss in iteration 48 : 0.46076924445206885
Loss in iteration 49 : 0.4719812157513941
Loss in iteration 50 : 0.47161569932858066
Loss in iteration 51 : 0.4574957405997119
Loss in iteration 52 : 0.4634809587591528
Loss in iteration 53 : 0.46032480873524406
Loss in iteration 54 : 0.4537709846156073
Loss in iteration 55 : 0.4549929447782505
Loss in iteration 56 : 0.4588867277265983
Loss in iteration 57 : 0.46064395162068444
Loss in iteration 58 : 0.4683818858857463
Loss in iteration 59 : 0.4791892812593356
Loss in iteration 60 : 0.4670211914422842
Loss in iteration 61 : 0.4635424179312368
Loss in iteration 62 : 0.4617544554950894
Loss in iteration 63 : 0.46701084407643595
Loss in iteration 64 : 0.4618294406061911
Loss in iteration 65 : 0.45697593008693976
Loss in iteration 66 : 0.46866955728620174
Loss in iteration 67 : 0.4503991541703765
Loss in iteration 68 : 0.4745536665921295
Loss in iteration 69 : 0.46696474367934065
Loss in iteration 70 : 0.45373046066288714
Loss in iteration 71 : 0.46338596385386194
Loss in iteration 72 : 0.4646988007567133
Loss in iteration 73 : 0.4737145142794368
Loss in iteration 74 : 0.46592501331338454
Loss in iteration 75 : 0.47487328434706183
Loss in iteration 76 : 0.4569665172836817
Loss in iteration 77 : 0.45225553868201346
Loss in iteration 78 : 0.4466154918825604
Loss in iteration 79 : 0.46306136028474476
Loss in iteration 80 : 0.4625926445545292
Loss in iteration 81 : 0.46595125870952897
Loss in iteration 82 : 0.4534967897322341
Loss in iteration 83 : 0.46597820882773033
Loss in iteration 84 : 0.45375711881473624
Loss in iteration 85 : 0.4622708887176642
Loss in iteration 86 : 0.46079025971207593
Loss in iteration 87 : 0.46066170909700915
Loss in iteration 88 : 0.46662488971110594
Loss in iteration 89 : 0.4599358392794133
Loss in iteration 90 : 0.4675405876911496
Loss in iteration 91 : 0.462933549834707
Loss in iteration 92 : 0.46723874304440094
Loss in iteration 93 : 0.4745621781107584
Loss in iteration 94 : 0.4605451449415856
Loss in iteration 95 : 0.47274157693654867
Loss in iteration 96 : 0.4647460178569019
Loss in iteration 97 : 0.4758846489794551
Loss in iteration 98 : 0.46072545546542726
Loss in iteration 99 : 0.4518597367878344
Loss in iteration 100 : 0.45863907710282265
Loss in iteration 101 : 0.4572121895877525
Loss in iteration 102 : 0.4653904809112666
Loss in iteration 103 : 0.4674578751918074
Loss in iteration 104 : 0.4678573137403973
Loss in iteration 105 : 0.4485579639406524
Loss in iteration 106 : 0.46827571072816127
Loss in iteration 107 : 0.46388364966209944
Loss in iteration 108 : 0.4518912494574303
Loss in iteration 109 : 0.46798249343794696
Loss in iteration 110 : 0.4599969981147112
Loss in iteration 111 : 0.45279339806530666
Loss in iteration 112 : 0.4604479699557654
Loss in iteration 113 : 0.4584366701327138
Loss in iteration 114 : 0.4694008869362396
Loss in iteration 115 : 0.4606593679354665
Loss in iteration 116 : 0.4542917148228316
Loss in iteration 117 : 0.45997216558933673
Loss in iteration 118 : 0.4596615449268087
Loss in iteration 119 : 0.46229674712900537
Loss in iteration 120 : 0.46289714609538507
Loss in iteration 121 : 0.4633360699860808
Loss in iteration 122 : 0.4697697294591772
Loss in iteration 123 : 0.4648534734568388
Loss in iteration 124 : 0.4609373670429368
Loss in iteration 125 : 0.46063860039080334
Loss in iteration 126 : 0.4682411032535963
Loss in iteration 127 : 0.4741453661045151
Loss in iteration 128 : 0.4603423905979914
Loss in iteration 129 : 0.4625435001618314
Loss in iteration 130 : 0.4654189882576567
Loss in iteration 131 : 0.4652802354298777
Loss in iteration 132 : 0.4575630378166435
Loss in iteration 133 : 0.46699810005443465
Loss in iteration 134 : 0.45874460469337497
Loss in iteration 135 : 0.4588502388006341
Loss in iteration 136 : 0.4645192535644814
Loss in iteration 137 : 0.45707753383147276
Loss in iteration 138 : 0.4585786280437174
Loss in iteration 139 : 0.4476837730921964
Loss in iteration 140 : 0.46511518832926374
Loss in iteration 141 : 0.4669552623645934
Loss in iteration 142 : 0.46306716486241956
Loss in iteration 143 : 0.4508645846773712
Loss in iteration 144 : 0.45851892372245584
Loss in iteration 145 : 0.461407725804035
Loss in iteration 146 : 0.45238102627670357
Loss in iteration 147 : 0.464885777300988
Loss in iteration 148 : 0.45667944157728124
Loss in iteration 149 : 0.4663372306480551
Loss in iteration 150 : 0.4626752008169299
Loss in iteration 151 : 0.45978028002294824
Loss in iteration 152 : 0.47308302559022286
Loss in iteration 153 : 0.45981611030178243
Loss in iteration 154 : 0.46096172580730005
Loss in iteration 155 : 0.45578697523047307
Loss in iteration 156 : 0.4590190415795797
Loss in iteration 157 : 0.4519739284267835
Loss in iteration 158 : 0.44839030610772684
Loss in iteration 159 : 0.4615824294187375
Loss in iteration 160 : 0.4496465164474896
Loss in iteration 161 : 0.4542402094028771
Loss in iteration 162 : 0.4591260161123766
Loss in iteration 163 : 0.4678986455277035
Loss in iteration 164 : 0.4630354743462364
Loss in iteration 165 : 0.47069886056179305
Loss in iteration 166 : 0.4624503711971366
Loss in iteration 167 : 0.47511182142315883
Loss in iteration 168 : 0.45188080021613186
Loss in iteration 169 : 0.461396788941779
Loss in iteration 170 : 0.45409937834994046
Loss in iteration 171 : 0.46112613213390985
Loss in iteration 172 : 0.45762786877195993
Loss in iteration 173 : 0.45527072110964134
Loss in iteration 174 : 0.45681832343111034
Loss in iteration 175 : 0.4603185167551946
Loss in iteration 176 : 0.4690044770757374
Loss in iteration 177 : 0.4653412529448388
Loss in iteration 178 : 0.46072245812233975
Loss in iteration 179 : 0.4616613623351956
Loss in iteration 180 : 0.46461826251276855
Loss in iteration 181 : 0.46499943159258833
Loss in iteration 182 : 0.46855563821624185
Loss in iteration 183 : 0.46598387149252196
Loss in iteration 184 : 0.4584581342827171
Loss in iteration 185 : 0.453844286491369
Loss in iteration 186 : 0.45487271587853834
Loss in iteration 187 : 0.44894165335346176
Loss in iteration 188 : 0.4715587967838257
Loss in iteration 189 : 0.4630154344023989
Loss in iteration 190 : 0.4695080311153446
Loss in iteration 191 : 0.46197334833993825
Loss in iteration 192 : 0.46318086970347694
Loss in iteration 193 : 0.45338967693620874
Loss in iteration 194 : 0.45873077985635485
Loss in iteration 195 : 0.460431401149336
Loss in iteration 196 : 0.4657609368529009
Loss in iteration 197 : 0.4567578115567901
Loss in iteration 198 : 0.4570170247739691
Loss in iteration 199 : 0.463642027557296
Loss in iteration 200 : 0.4558618569852468
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.7825, training accuracy 0.786875, time elapsed: 4379 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6838437658874919
Loss in iteration 3 : 0.6387852414238034
Loss in iteration 4 : 0.6136990849083455
Loss in iteration 5 : 0.5814488261937559
Loss in iteration 6 : 0.5596591002545597
Loss in iteration 7 : 0.532307640331829
Loss in iteration 8 : 0.5199725062646318
Loss in iteration 9 : 0.5030878874006969
Loss in iteration 10 : 0.5026563621086363
Loss in iteration 11 : 0.4851002945332982
Loss in iteration 12 : 0.48018014426946504
Loss in iteration 13 : 0.48889424930996556
Loss in iteration 14 : 0.4767224416448973
Loss in iteration 15 : 0.4910214052796363
Loss in iteration 16 : 0.4768642414710343
Loss in iteration 17 : 0.49223412955909757
Loss in iteration 18 : 0.48881699241330556
Loss in iteration 19 : 0.49084367453221095
Loss in iteration 20 : 0.49019928131293483
Loss in iteration 21 : 0.47631922545727434
Loss in iteration 22 : 0.4965725418843608
Loss in iteration 23 : 0.4786535107308591
Loss in iteration 24 : 0.48817924758888115
Loss in iteration 25 : 0.4686102426345278
Loss in iteration 26 : 0.47256996099395276
Loss in iteration 27 : 0.46395129084152803
Loss in iteration 28 : 0.4850233211355636
Loss in iteration 29 : 0.46322341496003067
Loss in iteration 30 : 0.4687074129576424
Loss in iteration 31 : 0.45944859641875013
Loss in iteration 32 : 0.4586156531872885
Loss in iteration 33 : 0.4773183309946011
Loss in iteration 34 : 0.478364824571446
Loss in iteration 35 : 0.4686511694431603
Loss in iteration 36 : 0.47588710658187466
Loss in iteration 37 : 0.47254333140313765
Loss in iteration 38 : 0.4657382102123663
Loss in iteration 39 : 0.4613343200716658
Loss in iteration 40 : 0.4532085841909706
Loss in iteration 41 : 0.45546016938451284
Loss in iteration 42 : 0.466438724679445
Loss in iteration 43 : 0.46296529066127
Loss in iteration 44 : 0.45021858495611156
Loss in iteration 45 : 0.4717851257957042
Loss in iteration 46 : 0.45481763921742235
Loss in iteration 47 : 0.463459955636863
Loss in iteration 48 : 0.46243624860781674
Loss in iteration 49 : 0.4717935597852722
Loss in iteration 50 : 0.47114491779953693
Loss in iteration 51 : 0.4580896971447145
Loss in iteration 52 : 0.46490638608559537
Loss in iteration 53 : 0.4617931823203523
Loss in iteration 54 : 0.45579746458007653
Loss in iteration 55 : 0.45555591982069826
Loss in iteration 56 : 0.462288529803206
Loss in iteration 57 : 0.4603066673689767
Loss in iteration 58 : 0.46864924928328366
Loss in iteration 59 : 0.4802615936457856
Loss in iteration 60 : 0.46685658928038487
Loss in iteration 61 : 0.46401516185013014
Loss in iteration 62 : 0.46151636497232484
Loss in iteration 63 : 0.46698401472479667
Loss in iteration 64 : 0.4616633962646562
Loss in iteration 65 : 0.458841901037946
Loss in iteration 66 : 0.4701131527099874
Loss in iteration 67 : 0.4526110551694996
Loss in iteration 68 : 0.47545367324640736
Loss in iteration 69 : 0.46826729739747314
Loss in iteration 70 : 0.4548873940084939
Loss in iteration 71 : 0.4633022379522192
Loss in iteration 72 : 0.46534711877441837
Loss in iteration 73 : 0.47451734704000387
Loss in iteration 74 : 0.4667448296663811
Loss in iteration 75 : 0.4755071657256575
Loss in iteration 76 : 0.4579451426117064
Loss in iteration 77 : 0.4535898985588928
Loss in iteration 78 : 0.447303188221866
Loss in iteration 79 : 0.46428949388897983
Loss in iteration 80 : 0.46271948767277443
Loss in iteration 81 : 0.4672505152131599
Loss in iteration 82 : 0.45534701384121257
Loss in iteration 83 : 0.46595564665746797
Loss in iteration 84 : 0.45607662686384615
Loss in iteration 85 : 0.46415279694261097
Loss in iteration 86 : 0.4606862035550301
Loss in iteration 87 : 0.46486999607795465
Loss in iteration 88 : 0.4647897786951369
Loss in iteration 89 : 0.46395104089502753
Loss in iteration 90 : 0.46519298614223537
Loss in iteration 91 : 0.46146971471039816
Loss in iteration 92 : 0.4673748514412628
Loss in iteration 93 : 0.46528036493427405
Loss in iteration 94 : 0.4604730871819624
Loss in iteration 95 : 0.4650524237207635
Loss in iteration 96 : 0.46579131767592685
Loss in iteration 97 : 0.4727844714078526
Loss in iteration 98 : 0.46124119061874
Loss in iteration 99 : 0.45448908082300155
Loss in iteration 100 : 0.45664412991439574
Loss in iteration 101 : 0.45794555002937126
Loss in iteration 102 : 0.4655371200645227
Loss in iteration 103 : 0.46788529832979214
Loss in iteration 104 : 0.4653200531544015
Loss in iteration 105 : 0.4510271970791272
Loss in iteration 106 : 0.4682179160736487
Loss in iteration 107 : 0.46554057166103807
Loss in iteration 108 : 0.4526027738704629
Loss in iteration 109 : 0.46789510830779213
Loss in iteration 110 : 0.46247437578441436
Loss in iteration 111 : 0.4533019154368119
Loss in iteration 112 : 0.4625809178566408
Loss in iteration 113 : 0.46113549562919437
Loss in iteration 114 : 0.4688705393216398
Loss in iteration 115 : 0.46275587882672115
Loss in iteration 116 : 0.4537781146124426
Loss in iteration 117 : 0.46077201714481103
Loss in iteration 118 : 0.4613871701930935
Loss in iteration 119 : 0.4619229148118466
Loss in iteration 120 : 0.4655072339782732
Loss in iteration 121 : 0.46115482081717607
Loss in iteration 122 : 0.47083903487246853
Loss in iteration 123 : 0.46419670172361843
Loss in iteration 124 : 0.45730200155031925
Loss in iteration 125 : 0.4601486615995461
Loss in iteration 126 : 0.46538700986212006
Loss in iteration 127 : 0.4759743600908847
Loss in iteration 128 : 0.4604461440056792
Loss in iteration 129 : 0.4627360888700857
Loss in iteration 130 : 0.46460828802114507
Loss in iteration 131 : 0.46416929913908556
Loss in iteration 132 : 0.45966219974428985
Loss in iteration 133 : 0.4634913055176791
Loss in iteration 134 : 0.4591561679092499
Loss in iteration 135 : 0.4590556585099069
Loss in iteration 136 : 0.4671565922957678
Loss in iteration 137 : 0.4552547900066687
Loss in iteration 138 : 0.4620749501686642
Loss in iteration 139 : 0.44805854378099513
Loss in iteration 140 : 0.4653040267469232
Loss in iteration 141 : 0.4689020525162485
Loss in iteration 142 : 0.4638882715018018
Loss in iteration 143 : 0.45158456415667014
Loss in iteration 144 : 0.4624018266229725
Loss in iteration 145 : 0.46039467215010593
Loss in iteration 146 : 0.4544134369989125
Loss in iteration 147 : 0.4631928523058702
Loss in iteration 148 : 0.4564663898866205
Loss in iteration 149 : 0.4701562156052067
Loss in iteration 150 : 0.4611432412039663
Loss in iteration 151 : 0.46090958175383684
Loss in iteration 152 : 0.4723866917065983
Loss in iteration 153 : 0.4603797118907215
Loss in iteration 154 : 0.46187801776243764
Loss in iteration 155 : 0.4579102668554389
Loss in iteration 156 : 0.4594078796602334
Loss in iteration 157 : 0.453892685001177
Loss in iteration 158 : 0.4491699489042226
Loss in iteration 159 : 0.46180564108098504
Loss in iteration 160 : 0.4510868396062646
Loss in iteration 161 : 0.454346775335626
Loss in iteration 162 : 0.4579550294282566
Loss in iteration 163 : 0.4680797918305985
Loss in iteration 164 : 0.46316284855568907
Loss in iteration 165 : 0.47155996582937765
Loss in iteration 166 : 0.4633060094167096
Loss in iteration 167 : 0.4749069893738784
Loss in iteration 168 : 0.4527925981584947
Loss in iteration 169 : 0.46193702514682555
Loss in iteration 170 : 0.4538278058248298
Loss in iteration 171 : 0.4632080284782555
Loss in iteration 172 : 0.45594189331359464
Loss in iteration 173 : 0.4563084618251706
Loss in iteration 174 : 0.45654635933085014
Loss in iteration 175 : 0.4600430588849961
Loss in iteration 176 : 0.4696411900860132
Loss in iteration 177 : 0.4658930183225149
Loss in iteration 178 : 0.4622648044317414
Loss in iteration 179 : 0.46372474152494364
Loss in iteration 180 : 0.4637586935144294
Loss in iteration 181 : 0.467405646226094
Loss in iteration 182 : 0.4666309565379345
Loss in iteration 183 : 0.46684187056722004
Loss in iteration 184 : 0.4574117423469409
Loss in iteration 185 : 0.4542036842425402
Loss in iteration 186 : 0.45354231698440534
Loss in iteration 187 : 0.4505187478979686
Loss in iteration 188 : 0.47108492187600237
Loss in iteration 189 : 0.4645738126849053
Loss in iteration 190 : 0.4701315699268255
Loss in iteration 191 : 0.4620185641125236
Loss in iteration 192 : 0.46412934778005477
Loss in iteration 193 : 0.4546636898132689
Loss in iteration 194 : 0.4588650854949461
Loss in iteration 195 : 0.4622135710999058
Loss in iteration 196 : 0.4671243455771586
Loss in iteration 197 : 0.45640802762090155
Loss in iteration 198 : 0.45833674296340854
Loss in iteration 199 : 0.46356219626889716
Loss in iteration 200 : 0.45729340252206624
Testing accuracy  of updater 1 on alg 0 with rate 0.7 = 0.7825, training accuracy 0.78875, time elapsed: 4123 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6816054441845584
Loss in iteration 3 : 0.6595283050016377
Loss in iteration 4 : 0.6379351193225908
Loss in iteration 5 : 0.6166199410166111
Loss in iteration 6 : 0.5968926206854711
Loss in iteration 7 : 0.5711320054445289
Loss in iteration 8 : 0.5559948060636706
Loss in iteration 9 : 0.5391329516324936
Loss in iteration 10 : 0.5304810305094585
Loss in iteration 11 : 0.5118305542631262
Loss in iteration 12 : 0.5048706232808869
Loss in iteration 13 : 0.5045425683446644
Loss in iteration 14 : 0.4928775970013608
Loss in iteration 15 : 0.5005885180774121
Loss in iteration 16 : 0.4863695086213338
Loss in iteration 17 : 0.4965617268308003
Loss in iteration 18 : 0.4911545168072032
Loss in iteration 19 : 0.49325760539741303
Loss in iteration 20 : 0.4908755720716072
Loss in iteration 21 : 0.47963419930266754
Loss in iteration 22 : 0.49538244959496663
Loss in iteration 23 : 0.482596112586286
Loss in iteration 24 : 0.488661439987163
Loss in iteration 25 : 0.4728503049319184
Loss in iteration 26 : 0.47814314263998875
Loss in iteration 27 : 0.4679347615869404
Loss in iteration 28 : 0.4858809530023058
Loss in iteration 29 : 0.46898971607408363
Loss in iteration 30 : 0.4725716229471778
Loss in iteration 31 : 0.4637135453716766
Loss in iteration 32 : 0.4642664705781872
Loss in iteration 33 : 0.4831662879674535
Loss in iteration 34 : 0.4802964768410684
Loss in iteration 35 : 0.4760373172753081
Loss in iteration 36 : 0.47961116483168587
Loss in iteration 37 : 0.4769670935685218
Loss in iteration 38 : 0.4708909444203285
Loss in iteration 39 : 0.46637091289140714
Loss in iteration 40 : 0.45835841615573836
Loss in iteration 41 : 0.46334996698529046
Loss in iteration 42 : 0.4687169846299947
Loss in iteration 43 : 0.4687249979633489
Loss in iteration 44 : 0.45504566722491635
Loss in iteration 45 : 0.4753287161557467
Loss in iteration 46 : 0.4593111891759319
Loss in iteration 47 : 0.4668447081758062
Loss in iteration 48 : 0.46497246796083674
Loss in iteration 49 : 0.47588039250624725
Loss in iteration 50 : 0.473782172005474
Loss in iteration 51 : 0.4584079869996096
Loss in iteration 52 : 0.47093718667235396
Loss in iteration 53 : 0.46431469965322003
Loss in iteration 54 : 0.4568646076430551
Loss in iteration 55 : 0.45949252702938487
Loss in iteration 56 : 0.46195335111096264
Loss in iteration 57 : 0.46218035558014336
Loss in iteration 58 : 0.46997258917208085
Loss in iteration 59 : 0.4790472625096939
Loss in iteration 60 : 0.46844605829930297
Loss in iteration 61 : 0.46512010012542004
Loss in iteration 62 : 0.4630280690602583
Loss in iteration 63 : 0.4681462964825556
Loss in iteration 64 : 0.46359059199863545
Loss in iteration 65 : 0.4574227810655546
Loss in iteration 66 : 0.47146546975945036
Loss in iteration 67 : 0.4533044325871616
Loss in iteration 68 : 0.4759313506174441
Loss in iteration 69 : 0.4679588584633463
Loss in iteration 70 : 0.45570121430828914
Loss in iteration 71 : 0.4656741503381338
Loss in iteration 72 : 0.4669020677685171
Loss in iteration 73 : 0.4755951504408732
Loss in iteration 74 : 0.46882538640079424
Loss in iteration 75 : 0.475714510635388
Loss in iteration 76 : 0.45925733866599083
Loss in iteration 77 : 0.4556509422012902
Loss in iteration 78 : 0.4501845165027175
Loss in iteration 79 : 0.4646536966888922
Loss in iteration 80 : 0.4640802268102673
Loss in iteration 81 : 0.46721655996659933
Loss in iteration 82 : 0.4567856345143752
Loss in iteration 83 : 0.46794247786177695
Loss in iteration 84 : 0.45536576276834934
Loss in iteration 85 : 0.46497142729210333
Loss in iteration 86 : 0.4617540310787082
Loss in iteration 87 : 0.46203298971489865
Loss in iteration 88 : 0.4656502246907953
Loss in iteration 89 : 0.4603290448039247
Loss in iteration 90 : 0.4651370051320459
Loss in iteration 91 : 0.46111128756727876
Loss in iteration 92 : 0.46623453885977656
Loss in iteration 93 : 0.46678550619262993
Loss in iteration 94 : 0.46092801884607676
Loss in iteration 95 : 0.46578222309852824
Loss in iteration 96 : 0.46678579647214036
Loss in iteration 97 : 0.4720784639588949
Loss in iteration 98 : 0.46306271971005436
Loss in iteration 99 : 0.45352009147511657
Loss in iteration 100 : 0.45724269723679245
Loss in iteration 101 : 0.45924983141284764
Loss in iteration 102 : 0.46435813645952084
Loss in iteration 103 : 0.4689611363850538
Loss in iteration 104 : 0.4659347280098771
Loss in iteration 105 : 0.45045726463110797
Loss in iteration 106 : 0.46947379886879237
Loss in iteration 107 : 0.46574079017219694
Loss in iteration 108 : 0.4528655331323645
Loss in iteration 109 : 0.46919894325512124
Loss in iteration 110 : 0.4608991952854042
Loss in iteration 111 : 0.45461543088325057
Loss in iteration 112 : 0.462715035445362
Loss in iteration 113 : 0.45989779782935686
Loss in iteration 114 : 0.4694018672603833
Loss in iteration 115 : 0.46186297536348
Loss in iteration 116 : 0.4550349977365188
Loss in iteration 117 : 0.4617654428133827
Loss in iteration 118 : 0.46201940943964825
Loss in iteration 119 : 0.4633369422933939
Loss in iteration 120 : 0.4654254538033991
Loss in iteration 121 : 0.4619363294752611
Loss in iteration 122 : 0.47204561201571826
Loss in iteration 123 : 0.4656841914035308
Loss in iteration 124 : 0.4570810034721853
Loss in iteration 125 : 0.4616831146541408
Loss in iteration 126 : 0.46774799674519296
Loss in iteration 127 : 0.47550226715589106
Loss in iteration 128 : 0.4610624962412809
Loss in iteration 129 : 0.46405293402015285
Loss in iteration 130 : 0.46437447041275526
Loss in iteration 131 : 0.465798021882615
Loss in iteration 132 : 0.45952681922583183
Loss in iteration 133 : 0.4638520196097901
Loss in iteration 134 : 0.459632884706178
Loss in iteration 135 : 0.4592168588634475
Loss in iteration 136 : 0.4665312116678755
Loss in iteration 137 : 0.4566066917640658
Loss in iteration 138 : 0.45989764219407
Loss in iteration 139 : 0.4486357748255416
Loss in iteration 140 : 0.4655943432759575
Loss in iteration 141 : 0.4677177348011729
Loss in iteration 142 : 0.4643230792561898
Loss in iteration 143 : 0.4503861710822058
Loss in iteration 144 : 0.46032243663448486
Loss in iteration 145 : 0.4614858315268567
Loss in iteration 146 : 0.45289827857477316
Loss in iteration 147 : 0.4636770020096013
Loss in iteration 148 : 0.45833146145909615
Loss in iteration 149 : 0.4674914344306103
Loss in iteration 150 : 0.46284490367340175
Loss in iteration 151 : 0.46119977007052887
Loss in iteration 152 : 0.47251924020065117
Loss in iteration 153 : 0.461370578519567
Loss in iteration 154 : 0.46248484392033407
Loss in iteration 155 : 0.4568126561126513
Loss in iteration 156 : 0.4598929302084929
Loss in iteration 157 : 0.453688596060789
Loss in iteration 158 : 0.45046050894573375
Loss in iteration 159 : 0.46241238857543243
Loss in iteration 160 : 0.45197794177697953
Loss in iteration 161 : 0.4554183696036078
Loss in iteration 162 : 0.4592493537529941
Loss in iteration 163 : 0.4671091274275797
Loss in iteration 164 : 0.46425818225711946
Loss in iteration 165 : 0.4717150443354412
Loss in iteration 166 : 0.46441984653175
Loss in iteration 167 : 0.47574186407590086
Loss in iteration 168 : 0.4522775491113082
Loss in iteration 169 : 0.4629090261124186
Loss in iteration 170 : 0.4549241910317259
Loss in iteration 171 : 0.4634427733155322
Loss in iteration 172 : 0.4575049642309204
Loss in iteration 173 : 0.4574020459090961
Loss in iteration 174 : 0.4572181769378459
Loss in iteration 175 : 0.46072336772229777
Loss in iteration 176 : 0.4693918395096248
Loss in iteration 177 : 0.4662160508470859
Loss in iteration 178 : 0.46164808097254606
Loss in iteration 179 : 0.463730022811383
Loss in iteration 180 : 0.4649151237275253
Loss in iteration 181 : 0.46655642098538763
Loss in iteration 182 : 0.46765782995966726
Loss in iteration 183 : 0.4672397417246506
Loss in iteration 184 : 0.45794272698840577
Loss in iteration 185 : 0.4551162274105105
Loss in iteration 186 : 0.4545744133464017
Loss in iteration 187 : 0.4498650953564806
Loss in iteration 188 : 0.4721321646245222
Loss in iteration 189 : 0.46556222302602046
Loss in iteration 190 : 0.46806618623603746
Loss in iteration 191 : 0.4633837758406368
Loss in iteration 192 : 0.46407876897353406
Loss in iteration 193 : 0.4554472389901514
Loss in iteration 194 : 0.4595115251395573
Loss in iteration 195 : 0.4613882015474492
Loss in iteration 196 : 0.46730934963811555
Loss in iteration 197 : 0.45579368156699435
Loss in iteration 198 : 0.4579372976443303
Loss in iteration 199 : 0.46458715459620553
Loss in iteration 200 : 0.4568999730977124
Testing accuracy  of updater 1 on alg 0 with rate 0.4 = 0.787, training accuracy 0.78925, time elapsed: 4491 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6886417315104332
Loss in iteration 3 : 0.6801437436627537
Loss in iteration 4 : 0.6753423646585895
Loss in iteration 5 : 0.668624868879682
Loss in iteration 6 : 0.6585890662455827
Loss in iteration 7 : 0.6463885418053652
Loss in iteration 8 : 0.6358761561830139
Loss in iteration 9 : 0.6256122196631808
Loss in iteration 10 : 0.6186217958640298
Loss in iteration 11 : 0.6068936562965307
Loss in iteration 12 : 0.5980013053737735
Loss in iteration 13 : 0.5909718215177174
Loss in iteration 14 : 0.5816091559544363
Loss in iteration 15 : 0.5790622409957352
Loss in iteration 16 : 0.5659771033769487
Loss in iteration 17 : 0.5663354684108715
Loss in iteration 18 : 0.5581613035807651
Loss in iteration 19 : 0.5524434214252663
Loss in iteration 20 : 0.5484740517637481
Loss in iteration 21 : 0.5411939041787421
Loss in iteration 22 : 0.5411551429952022
Loss in iteration 23 : 0.5326525868567735
Loss in iteration 24 : 0.5349723498224648
Loss in iteration 25 : 0.5209324462086907
Loss in iteration 26 : 0.522566049475436
Loss in iteration 27 : 0.513187367276564
Loss in iteration 28 : 0.5212800131129226
Loss in iteration 29 : 0.5101157091963913
Loss in iteration 30 : 0.5116646941600695
Loss in iteration 31 : 0.5020049128544288
Loss in iteration 32 : 0.5024128598942814
Loss in iteration 33 : 0.5122690457003942
Loss in iteration 34 : 0.5091356555828787
Loss in iteration 35 : 0.5058117664375213
Loss in iteration 36 : 0.5052928450036024
Loss in iteration 37 : 0.5060193775256357
Loss in iteration 38 : 0.5003942727457225
Loss in iteration 39 : 0.4955613559958575
Loss in iteration 40 : 0.49053070538510957
Loss in iteration 41 : 0.4944417653054937
Loss in iteration 42 : 0.49346244113323184
Loss in iteration 43 : 0.4947379181248958
Loss in iteration 44 : 0.4872038688656049
Loss in iteration 45 : 0.4992900228888635
Loss in iteration 46 : 0.485923658740484
Loss in iteration 47 : 0.491652964976152
Loss in iteration 48 : 0.4908834545190843
Loss in iteration 49 : 0.4972443440333533
Loss in iteration 50 : 0.4946440372443829
Loss in iteration 51 : 0.4822432944924427
Loss in iteration 52 : 0.4948981182882482
Loss in iteration 53 : 0.48897939075081737
Loss in iteration 54 : 0.48334815648842977
Loss in iteration 55 : 0.48448462589717467
Loss in iteration 56 : 0.4862698813663301
Loss in iteration 57 : 0.48320784644658626
Loss in iteration 58 : 0.4907760327091562
Loss in iteration 59 : 0.4944547773044855
Loss in iteration 60 : 0.48830944874109805
Loss in iteration 61 : 0.48453805238221
Loss in iteration 62 : 0.4829036052300027
Loss in iteration 63 : 0.48728466574871
Loss in iteration 64 : 0.48260204270184026
Loss in iteration 65 : 0.4782326988079732
Loss in iteration 66 : 0.4889501934585608
Loss in iteration 67 : 0.47511744354292695
Loss in iteration 68 : 0.49126232646253326
Loss in iteration 69 : 0.484549928863191
Loss in iteration 70 : 0.47481272647633177
Loss in iteration 71 : 0.4861760605334719
Loss in iteration 72 : 0.4845929994620684
Loss in iteration 73 : 0.4923616741712756
Loss in iteration 74 : 0.4867775067934338
Loss in iteration 75 : 0.48843748894250544
Loss in iteration 76 : 0.47713867678614413
Loss in iteration 77 : 0.4737144388204075
Loss in iteration 78 : 0.4713817956866521
Loss in iteration 79 : 0.4808013641004678
Loss in iteration 80 : 0.4794260621261857
Loss in iteration 81 : 0.48430949092842746
Loss in iteration 82 : 0.47461423002639175
Loss in iteration 83 : 0.4829345962024464
Loss in iteration 84 : 0.47138355175569996
Loss in iteration 85 : 0.4808288883014773
Loss in iteration 86 : 0.47771805848946713
Loss in iteration 87 : 0.4738453758967138
Loss in iteration 88 : 0.47843911321084626
Loss in iteration 89 : 0.4744392271380567
Loss in iteration 90 : 0.47945540355348043
Loss in iteration 91 : 0.47693789478445925
Loss in iteration 92 : 0.4772255896593601
Loss in iteration 93 : 0.48068190655955195
Loss in iteration 94 : 0.47579289418068604
Loss in iteration 95 : 0.4788241718443459
Loss in iteration 96 : 0.4799415794193523
Loss in iteration 97 : 0.48321994828733145
Loss in iteration 98 : 0.477141031925444
Loss in iteration 99 : 0.46902770745961153
Loss in iteration 100 : 0.4695869388196448
Loss in iteration 101 : 0.4735042856610966
Loss in iteration 102 : 0.4759339956592582
Loss in iteration 103 : 0.4820487715708382
Loss in iteration 104 : 0.4784988601972554
Loss in iteration 105 : 0.46623109425565473
Loss in iteration 106 : 0.4822939180887467
Loss in iteration 107 : 0.47807366902378623
Loss in iteration 108 : 0.46718404015920434
Loss in iteration 109 : 0.4821539124506503
Loss in iteration 110 : 0.4731408419902069
Loss in iteration 111 : 0.46715806890191924
Loss in iteration 112 : 0.4748546715174345
Loss in iteration 113 : 0.47175063139261836
Loss in iteration 114 : 0.4791836492069375
Loss in iteration 115 : 0.4739705262367548
Loss in iteration 116 : 0.469857548483811
Loss in iteration 117 : 0.4726451100037792
Loss in iteration 118 : 0.47364638253646285
Loss in iteration 119 : 0.47529549231086016
Loss in iteration 120 : 0.47871304298659584
Loss in iteration 121 : 0.4710751383061439
Loss in iteration 122 : 0.47797383074169697
Loss in iteration 123 : 0.47540819869776124
Loss in iteration 124 : 0.46738896810249064
Loss in iteration 125 : 0.4715437651813683
Loss in iteration 126 : 0.47628406163087544
Loss in iteration 127 : 0.48394445138863434
Loss in iteration 128 : 0.46912365352767155
Loss in iteration 129 : 0.47436047550278154
Loss in iteration 130 : 0.47237183857369136
Loss in iteration 131 : 0.47578352672146174
Loss in iteration 132 : 0.47000956246451453
Loss in iteration 133 : 0.47278730566920135
Loss in iteration 134 : 0.46971566468913906
Loss in iteration 135 : 0.46821453223482723
Loss in iteration 136 : 0.47527317527406265
Loss in iteration 137 : 0.46587997320591756
Loss in iteration 138 : 0.4685722951964991
Loss in iteration 139 : 0.4606635178365124
Loss in iteration 140 : 0.47359911286095696
Loss in iteration 141 : 0.47599303010376864
Loss in iteration 142 : 0.47367715297045315
Loss in iteration 143 : 0.4615267848663939
Loss in iteration 144 : 0.4699171159345374
Loss in iteration 145 : 0.47024097292293077
Loss in iteration 146 : 0.4634717421383143
Loss in iteration 147 : 0.4721313771576953
Loss in iteration 148 : 0.46859559150469016
Loss in iteration 149 : 0.47446828815999564
Loss in iteration 150 : 0.47124737410707473
Loss in iteration 151 : 0.4704311988879222
Loss in iteration 152 : 0.4792954200565604
Loss in iteration 153 : 0.47060338907010835
Loss in iteration 154 : 0.47200950339683034
Loss in iteration 155 : 0.46411855670832797
Loss in iteration 156 : 0.4674342885832855
Loss in iteration 157 : 0.46275179224685764
Loss in iteration 158 : 0.45883072546568315
Loss in iteration 159 : 0.4694186236979781
Loss in iteration 160 : 0.4622722121145235
Loss in iteration 161 : 0.4644769965370079
Loss in iteration 162 : 0.46859104081711855
Loss in iteration 163 : 0.4742044940687415
Loss in iteration 164 : 0.47355305106243406
Loss in iteration 165 : 0.4767368252757621
Loss in iteration 166 : 0.4711775424142967
Loss in iteration 167 : 0.48091228568222316
Loss in iteration 168 : 0.46191386877210494
Loss in iteration 169 : 0.4703585719930323
Loss in iteration 170 : 0.4637225182622717
Loss in iteration 171 : 0.4719231964011864
Loss in iteration 172 : 0.465645906979653
Loss in iteration 173 : 0.4646191310662157
Loss in iteration 174 : 0.46388074259521406
Loss in iteration 175 : 0.4676753330981973
Loss in iteration 176 : 0.4746794086829196
Loss in iteration 177 : 0.4721776527197014
Loss in iteration 178 : 0.4670625693344466
Loss in iteration 179 : 0.4688156584809257
Loss in iteration 180 : 0.47191758098670816
Loss in iteration 181 : 0.473530064158238
Loss in iteration 182 : 0.4738323614287015
Loss in iteration 183 : 0.47224357425446706
Loss in iteration 184 : 0.4662524165149013
Loss in iteration 185 : 0.46268612362019756
Loss in iteration 186 : 0.46172748150879195
Loss in iteration 187 : 0.4565759267344589
Loss in iteration 188 : 0.4767941230223749
Loss in iteration 189 : 0.47316592509742994
Loss in iteration 190 : 0.47284774813887
Loss in iteration 191 : 0.47144371122532447
Loss in iteration 192 : 0.47081627280427063
Loss in iteration 193 : 0.46239569389577423
Loss in iteration 194 : 0.46532386075464477
Loss in iteration 195 : 0.4690505569543384
Loss in iteration 196 : 0.47467253062320053
Loss in iteration 197 : 0.46203853064827777
Loss in iteration 198 : 0.4644717679373287
Loss in iteration 199 : 0.47010097195217215
Loss in iteration 200 : 0.46386684056538513
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.786, training accuracy 0.788875, time elapsed: 4562 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 4.45825047051948
Loss in iteration 3 : 43.895136572707784
Loss in iteration 4 : 1.7179531007968896
Loss in iteration 5 : 26.3956976261265
Loss in iteration 6 : 1.2757962620896768
Loss in iteration 7 : 1.7502592533255168
Loss in iteration 8 : 6.675719557789265
Loss in iteration 9 : 23.699823290637212
Loss in iteration 10 : 6.1935151686777425
Loss in iteration 11 : 11.42803934359953
Loss in iteration 12 : 9.671535559538576
Loss in iteration 13 : 7.982566667906544
Loss in iteration 14 : 6.017160669381048
Loss in iteration 15 : 6.448027866406731
Loss in iteration 16 : 4.666729266311332
Loss in iteration 17 : 4.620629997706583
Loss in iteration 18 : 4.211916143559073
Loss in iteration 19 : 4.146220882958188
Loss in iteration 20 : 3.868929537032952
Loss in iteration 21 : 3.607624621900786
Loss in iteration 22 : 3.909303224271119
Loss in iteration 23 : 3.506925690065577
Loss in iteration 24 : 3.361096123634515
Loss in iteration 25 : 3.1293177988008014
Loss in iteration 26 : 2.9766303889697707
Loss in iteration 27 : 2.846206209429135
Loss in iteration 28 : 3.145678349523927
Loss in iteration 29 : 2.923661883950178
Loss in iteration 30 : 3.0794738801986674
Loss in iteration 31 : 4.002309727056745
Loss in iteration 32 : 4.97613350719522
Loss in iteration 33 : 8.916165085158543
Loss in iteration 34 : 7.972543968029852
Loss in iteration 35 : 9.26755982094149
Loss in iteration 36 : 6.392349926768759
Loss in iteration 37 : 7.839709430755758
Loss in iteration 38 : 6.86085296165573
Loss in iteration 39 : 7.4890781740318575
Loss in iteration 40 : 5.550210862405951
Loss in iteration 41 : 6.527120707752774
Loss in iteration 42 : 5.941033510617162
Loss in iteration 43 : 5.726312585639374
Loss in iteration 44 : 4.702879996423171
Loss in iteration 45 : 5.4153506444508075
Loss in iteration 46 : 5.369017090043382
Loss in iteration 47 : 5.974706953613962
Loss in iteration 48 : 5.047985687299662
Loss in iteration 49 : 5.853252620521333
Loss in iteration 50 : 5.1315744878437615
Loss in iteration 51 : 5.047679411414624
Loss in iteration 52 : 4.893201694411728
Loss in iteration 53 : 5.70613100769393
Loss in iteration 54 : 4.979072327504335
Loss in iteration 55 : 5.577866595858493
Loss in iteration 56 : 5.3240014936187485
Loss in iteration 57 : 6.230497285027828
Loss in iteration 58 : 5.032409262851496
Loss in iteration 59 : 6.092992971831931
Loss in iteration 60 : 5.288934737915798
Loss in iteration 61 : 6.2336170550205425
Loss in iteration 62 : 5.07959591292481
Loss in iteration 63 : 5.637639806682546
Loss in iteration 64 : 4.861053140215815
Loss in iteration 65 : 5.334811050105235
Loss in iteration 66 : 5.517154240056024
Loss in iteration 67 : 5.968224416059934
Loss in iteration 68 : 5.767492215353105
Loss in iteration 69 : 6.068645247074532
Loss in iteration 70 : 5.030185351796443
Loss in iteration 71 : 5.445810093726616
Loss in iteration 72 : 5.14516275676391
Loss in iteration 73 : 5.50480137953582
Loss in iteration 74 : 5.235694669886537
Loss in iteration 75 : 6.110008736499065
Loss in iteration 76 : 5.314114827810637
Loss in iteration 77 : 5.885978762068745
Loss in iteration 78 : 5.228857391409675
Loss in iteration 79 : 5.588734041433543
Loss in iteration 80 : 4.964266379982838
Loss in iteration 81 : 5.759666906125187
Loss in iteration 82 : 4.908895774658755
Loss in iteration 83 : 5.8054132893012635
Loss in iteration 84 : 5.5367401054776755
Loss in iteration 85 : 6.2204301851097155
Loss in iteration 86 : 5.190489403503614
Loss in iteration 87 : 5.860713461720741
Loss in iteration 88 : 5.098632152005415
Loss in iteration 89 : 5.3593458894184405
Loss in iteration 90 : 4.930850064515635
Loss in iteration 91 : 5.229424932427907
Loss in iteration 92 : 5.0914404488764795
Loss in iteration 93 : 5.224328830495632
Loss in iteration 94 : 4.744613159495391
Loss in iteration 95 : 5.334537802830011
Loss in iteration 96 : 5.425953476752884
Loss in iteration 97 : 6.582916645859435
Loss in iteration 98 : 5.417225687719967
Loss in iteration 99 : 5.723636514930787
Loss in iteration 100 : 5.022609534913191
Loss in iteration 101 : 5.521199784302367
Loss in iteration 102 : 4.806769690725453
Loss in iteration 103 : 5.190158934108031
Loss in iteration 104 : 5.16569706434029
Loss in iteration 105 : 5.570335989853627
Loss in iteration 106 : 5.051497987105457
Loss in iteration 107 : 5.849782060993991
Loss in iteration 108 : 5.141914736694637
Loss in iteration 109 : 5.991041438729893
Loss in iteration 110 : 5.881169219480561
Loss in iteration 111 : 6.668815575235961
Loss in iteration 112 : 5.5034726992609615
Loss in iteration 113 : 6.1529311490216205
Loss in iteration 114 : 5.855743327031389
Loss in iteration 115 : 5.893547205679758
Loss in iteration 116 : 4.948820499248103
Loss in iteration 117 : 5.405493722266904
Loss in iteration 118 : 4.568266671720552
Loss in iteration 119 : 4.846594664901553
Loss in iteration 120 : 4.974315132515595
Loss in iteration 121 : 5.68655548738883
Loss in iteration 122 : 4.612796470168878
Loss in iteration 123 : 4.873593720411329
Loss in iteration 124 : 5.263131536816242
Loss in iteration 125 : 6.373987340689795
Loss in iteration 126 : 5.535811841754381
Loss in iteration 127 : 6.878363419780595
Loss in iteration 128 : 5.18705051050412
Loss in iteration 129 : 5.6993230298894035
Loss in iteration 130 : 5.385623226003209
Loss in iteration 131 : 6.100209190021591
Loss in iteration 132 : 4.830486456845697
Loss in iteration 133 : 5.187686421705243
Loss in iteration 134 : 4.815289420450306
Loss in iteration 135 : 5.618281013554498
Loss in iteration 136 : 5.243950397293493
Loss in iteration 137 : 5.679891405559651
Loss in iteration 138 : 4.97888895278731
Loss in iteration 139 : 5.471627087238685
Loss in iteration 140 : 5.2190056947976
Loss in iteration 141 : 5.570476282519252
Loss in iteration 142 : 5.246464743553626
Loss in iteration 143 : 5.721852619666083
Loss in iteration 144 : 4.940815805431799
Loss in iteration 145 : 5.146418650367719
Loss in iteration 146 : 5.187166899981408
Loss in iteration 147 : 6.1253962305509795
Loss in iteration 148 : 5.52162241588652
Loss in iteration 149 : 6.706996270851409
Loss in iteration 150 : 6.041173759966973
Loss in iteration 151 : 6.591146885417179
Loss in iteration 152 : 5.60706559352448
Loss in iteration 153 : 5.79956741533752
Loss in iteration 154 : 4.788603452617865
Loss in iteration 155 : 5.096309852136884
Loss in iteration 156 : 4.9152862014629415
Loss in iteration 157 : 5.27281566872309
Loss in iteration 158 : 4.4276316655382795
Loss in iteration 159 : 4.793132775961279
Loss in iteration 160 : 4.594125779165902
Loss in iteration 161 : 5.434291750810045
Loss in iteration 162 : 4.660683096878352
Loss in iteration 163 : 5.220507676779866
Loss in iteration 164 : 5.007846296233682
Loss in iteration 165 : 6.088090402413491
Loss in iteration 166 : 5.831092456413333
Loss in iteration 167 : 6.927411258348995
Loss in iteration 168 : 6.028191658135494
Loss in iteration 169 : 6.759013728821285
Loss in iteration 170 : 4.943552941807279
Loss in iteration 171 : 5.578372787899644
Loss in iteration 172 : 5.421450536708149
Loss in iteration 173 : 5.811857606900616
Loss in iteration 174 : 5.538944363520996
Loss in iteration 175 : 6.155843118105091
Loss in iteration 176 : 5.076730046571418
Loss in iteration 177 : 5.408338242571206
Loss in iteration 178 : 5.105201526320721
Loss in iteration 179 : 5.473201206876181
Loss in iteration 180 : 4.908722886688179
Loss in iteration 181 : 5.1389831046255114
Loss in iteration 182 : 5.264634890903957
Loss in iteration 183 : 6.141258877877302
Loss in iteration 184 : 5.020022583384932
Loss in iteration 185 : 5.491638085549888
Loss in iteration 186 : 4.937340106822976
Loss in iteration 187 : 5.638905572743283
Loss in iteration 188 : 4.994495655833767
Loss in iteration 189 : 5.653100068212045
Loss in iteration 190 : 5.875832775826518
Loss in iteration 191 : 6.655915907976598
Loss in iteration 192 : 5.661648903475383
Loss in iteration 193 : 5.8604740505806
Loss in iteration 194 : 5.00096029835195
Loss in iteration 195 : 5.248771857217376
Loss in iteration 196 : 5.271357642989478
Loss in iteration 197 : 6.0412375198024835
Loss in iteration 198 : 5.291668455288535
Loss in iteration 199 : 5.9600650557422155
Loss in iteration 200 : 5.689504417396681
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.781, training accuracy 0.78025, time elapsed: 4034 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 3.129316785492818
Loss in iteration 3 : 30.394295758331232
Loss in iteration 4 : 1.0644116515820574
Loss in iteration 5 : 15.478047624766821
Loss in iteration 6 : 2.946260092305074
Loss in iteration 7 : 12.65954482611658
Loss in iteration 8 : 7.4440573382379505
Loss in iteration 9 : 8.60121012477224
Loss in iteration 10 : 7.416335710409474
Loss in iteration 11 : 5.93478999284456
Loss in iteration 12 : 5.87608510067401
Loss in iteration 13 : 4.1844799357171425
Loss in iteration 14 : 3.683649406627747
Loss in iteration 15 : 3.2782443625981332
Loss in iteration 16 : 3.0025446729668555
Loss in iteration 17 : 2.966191280251829
Loss in iteration 18 : 2.762899347669185
Loss in iteration 19 : 2.6442189183871454
Loss in iteration 20 : 2.5771663152438533
Loss in iteration 21 : 2.3516652423563484
Loss in iteration 22 : 2.550450381036523
Loss in iteration 23 : 2.281038066725493
Loss in iteration 24 : 2.2481609286773008
Loss in iteration 25 : 2.1029682254166975
Loss in iteration 26 : 2.0152411986530105
Loss in iteration 27 : 1.938807483907696
Loss in iteration 28 : 2.086768116414523
Loss in iteration 29 : 2.0194393171660545
Loss in iteration 30 : 2.5671176737308525
Loss in iteration 31 : 3.5403608004320803
Loss in iteration 32 : 6.252287859691543
Loss in iteration 33 : 4.997758490705439
Loss in iteration 34 : 6.351142224806997
Loss in iteration 35 : 5.138045195144691
Loss in iteration 36 : 6.38499432294112
Loss in iteration 37 : 4.5993672457536166
Loss in iteration 38 : 5.545289545109935
Loss in iteration 39 : 4.931438249973869
Loss in iteration 40 : 4.851402263106136
Loss in iteration 41 : 3.9097013574170543
Loss in iteration 42 : 4.139297615732474
Loss in iteration 43 : 3.626760964275562
Loss in iteration 44 : 3.5041485136490733
Loss in iteration 45 : 3.559551747831695
Loss in iteration 46 : 3.539483801147513
Loss in iteration 47 : 3.118393777933678
Loss in iteration 48 : 3.3893735715443967
Loss in iteration 49 : 3.389064346662082
Loss in iteration 50 : 3.6488708361516315
Loss in iteration 51 : 3.59249984279609
Loss in iteration 52 : 4.093239274694255
Loss in iteration 53 : 3.475427346522233
Loss in iteration 54 : 3.882168852927272
Loss in iteration 55 : 3.605517911521171
Loss in iteration 56 : 4.051145198926387
Loss in iteration 57 : 3.4833103501655005
Loss in iteration 58 : 4.05419948608628
Loss in iteration 59 : 3.6526769867150306
Loss in iteration 60 : 4.019456868973966
Loss in iteration 61 : 3.7415705847111322
Loss in iteration 62 : 4.429408397068445
Loss in iteration 63 : 3.8653268636013682
Loss in iteration 64 : 4.24593495333782
Loss in iteration 65 : 3.245551530720811
Loss in iteration 66 : 3.5945466213225825
Loss in iteration 67 : 3.1981908438382427
Loss in iteration 68 : 3.906280561482357
Loss in iteration 69 : 3.431597792531436
Loss in iteration 70 : 3.6672233071583964
Loss in iteration 71 : 3.3048674592773515
Loss in iteration 72 : 3.778078707456865
Loss in iteration 73 : 3.761571939874522
Loss in iteration 74 : 4.182958371125984
Loss in iteration 75 : 3.8743827350144255
Loss in iteration 76 : 4.256245075273103
Loss in iteration 77 : 3.724680594505776
Loss in iteration 78 : 4.361039000618463
Loss in iteration 79 : 4.0776477509824085
Loss in iteration 80 : 4.453863718856468
Loss in iteration 81 : 3.6470666689835882
Loss in iteration 82 : 3.955172960506693
Loss in iteration 83 : 3.960215272407428
Loss in iteration 84 : 4.183435511915502
Loss in iteration 85 : 4.067579531453234
Loss in iteration 86 : 4.543025036712157
Loss in iteration 87 : 3.6220512688535997
Loss in iteration 88 : 3.3895349775145402
Loss in iteration 89 : 3.5627621716185853
Loss in iteration 90 : 4.111000973209901
Loss in iteration 91 : 3.369110475990458
Loss in iteration 92 : 3.578503056518624
Loss in iteration 93 : 3.5730344216582046
Loss in iteration 94 : 4.220990130363511
Loss in iteration 95 : 3.714220356942483
Loss in iteration 96 : 3.8654253832854306
Loss in iteration 97 : 3.5302270122959247
Loss in iteration 98 : 3.7757409705505394
Loss in iteration 99 : 3.771938309536577
Loss in iteration 100 : 4.484159423665208
Loss in iteration 101 : 3.6618661080262953
Loss in iteration 102 : 4.047231966498659
Loss in iteration 103 : 3.7912566806222734
Loss in iteration 104 : 4.268438208448449
Loss in iteration 105 : 3.9495082532657606
Loss in iteration 106 : 4.449594835732692
Loss in iteration 107 : 3.8211340441619646
Loss in iteration 108 : 3.9694484633182525
Loss in iteration 109 : 3.8046319799194572
Loss in iteration 110 : 3.890350850622156
Loss in iteration 111 : 3.45910877069804
Loss in iteration 112 : 4.130955094362873
Loss in iteration 113 : 3.462640120090848
Loss in iteration 114 : 3.8256938711906825
Loss in iteration 115 : 3.4192665625235117
Loss in iteration 116 : 3.9108884215647284
Loss in iteration 117 : 3.680921733788885
Loss in iteration 118 : 3.8323477479900876
Loss in iteration 119 : 3.6615969669008406
Loss in iteration 120 : 3.9299237243404477
Loss in iteration 121 : 3.5162523537782433
Loss in iteration 122 : 4.276180337493863
Loss in iteration 123 : 3.7622692136809457
Loss in iteration 124 : 4.057817531964018
Loss in iteration 125 : 3.6339841642422877
Loss in iteration 126 : 4.006969782001257
Loss in iteration 127 : 3.9485960299992153
Loss in iteration 128 : 4.446582772591621
Loss in iteration 129 : 3.7992386092483805
Loss in iteration 130 : 4.15085980734129
Loss in iteration 131 : 3.589420125623065
Loss in iteration 132 : 3.8215192167559184
Loss in iteration 133 : 3.619791598986663
Loss in iteration 134 : 3.9852816654443273
Loss in iteration 135 : 3.9061790670987215
Loss in iteration 136 : 3.894790246229953
Loss in iteration 137 : 3.777973978276904
Loss in iteration 138 : 4.493482209593438
Loss in iteration 139 : 3.7920434275129344
Loss in iteration 140 : 4.066866858350879
Loss in iteration 141 : 3.7080421937032515
Loss in iteration 142 : 3.9739057863257794
Loss in iteration 143 : 3.419404218536732
Loss in iteration 144 : 3.8240389147705947
Loss in iteration 145 : 3.543388961486255
Loss in iteration 146 : 3.724879520279732
Loss in iteration 147 : 3.4857537922172406
Loss in iteration 148 : 3.6566610290581125
Loss in iteration 149 : 3.3601410945469015
Loss in iteration 150 : 3.71812057058757
Loss in iteration 151 : 3.6100906373208574
Loss in iteration 152 : 4.446902460252717
Loss in iteration 153 : 4.327307462022332
Loss in iteration 154 : 4.698455316422213
Loss in iteration 155 : 3.552704053280987
Loss in iteration 156 : 3.7360594693224454
Loss in iteration 157 : 3.2398883610860176
Loss in iteration 158 : 3.607361618954908
Loss in iteration 159 : 3.690855441559845
Loss in iteration 160 : 3.9266455623610743
Loss in iteration 161 : 3.333243648008841
Loss in iteration 162 : 3.817710934212444
Loss in iteration 163 : 4.183316685954122
Loss in iteration 164 : 4.9882183815559555
Loss in iteration 165 : 3.993679026097094
Loss in iteration 166 : 4.0221981131791
Loss in iteration 167 : 3.5816647626601963
Loss in iteration 168 : 3.3924364022579585
Loss in iteration 169 : 3.327879080152308
Loss in iteration 170 : 3.587123642375141
Loss in iteration 171 : 3.3763463115236387
Loss in iteration 172 : 3.519258818664198
Loss in iteration 173 : 3.32879033345122
Loss in iteration 174 : 3.8976715074938393
Loss in iteration 175 : 3.920359019842769
Loss in iteration 176 : 4.3875319702399525
Loss in iteration 177 : 3.6095317451949986
Loss in iteration 178 : 3.7778825804093157
Loss in iteration 179 : 3.52967260161337
Loss in iteration 180 : 3.966660590284632
Loss in iteration 181 : 3.5278443004744826
Loss in iteration 182 : 4.002504834951101
Loss in iteration 183 : 3.7388594895334237
Loss in iteration 184 : 4.2563440796532355
Loss in iteration 185 : 3.912214582406351
Loss in iteration 186 : 4.060739034890948
Loss in iteration 187 : 3.3561345187004825
Loss in iteration 188 : 3.8251878964152994
Loss in iteration 189 : 3.650710816882016
Loss in iteration 190 : 4.041131849794314
Loss in iteration 191 : 3.5018060603036045
Loss in iteration 192 : 3.933914020639581
Loss in iteration 193 : 3.5078881750091506
Loss in iteration 194 : 4.019452657156967
Loss in iteration 195 : 3.817229771789418
Loss in iteration 196 : 4.0920641213274935
Loss in iteration 197 : 3.6846378535403965
Loss in iteration 198 : 3.9242557276463343
Loss in iteration 199 : 3.6107157189863917
Loss in iteration 200 : 4.058700432807053
Testing accuracy  of updater 2 on alg 0 with rate 7.0 = 0.759, training accuracy 0.76775, time elapsed: 3959 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 1.8296948892493274
Loss in iteration 3 : 16.294888399922744
Loss in iteration 4 : 0.6873337597052429
Loss in iteration 5 : 1.2522161479631115
Loss in iteration 6 : 9.99034358870705
Loss in iteration 7 : 0.926068548510787
Loss in iteration 8 : 2.1072482216617336
Loss in iteration 9 : 8.494019382852196
Loss in iteration 10 : 3.00195694599199
Loss in iteration 11 : 4.103532132367922
Loss in iteration 12 : 3.3852103459651226
Loss in iteration 13 : 3.0169117109134023
Loss in iteration 14 : 2.1965161594759213
Loss in iteration 15 : 2.2036776233658695
Loss in iteration 16 : 1.7431695801023108
Loss in iteration 17 : 1.7250908289164515
Loss in iteration 18 : 1.6423011119230202
Loss in iteration 19 : 1.6261770096922687
Loss in iteration 20 : 1.5503218364923685
Loss in iteration 21 : 1.4272251396404314
Loss in iteration 22 : 1.5583320561076917
Loss in iteration 23 : 1.403732424453229
Loss in iteration 24 : 1.360318666839475
Loss in iteration 25 : 1.2685502262205057
Loss in iteration 26 : 1.2122495412609513
Loss in iteration 27 : 1.1649960930097243
Loss in iteration 28 : 1.2810043580067723
Loss in iteration 29 : 1.2008545954843721
Loss in iteration 30 : 1.2574021047348243
Loss in iteration 31 : 1.6188798712176784
Loss in iteration 32 : 2.0865591542385595
Loss in iteration 33 : 3.5824264102626846
Loss in iteration 34 : 3.3255335159651067
Loss in iteration 35 : 3.6277563966960678
Loss in iteration 36 : 2.67307644721154
Loss in iteration 37 : 3.152534463379469
Loss in iteration 38 : 2.7974189496246358
Loss in iteration 39 : 2.945159798286398
Loss in iteration 40 : 2.3093513152194265
Loss in iteration 41 : 2.587200871405596
Loss in iteration 42 : 2.4046095410494406
Loss in iteration 43 : 2.266730598860556
Loss in iteration 44 : 1.9014453105873863
Loss in iteration 45 : 2.132831480264465
Loss in iteration 46 : 2.1610687714286545
Loss in iteration 47 : 2.3709093422788587
Loss in iteration 48 : 2.056099816954067
Loss in iteration 49 : 2.333993703016977
Loss in iteration 50 : 2.096707661221324
Loss in iteration 51 : 2.0328079838576336
Loss in iteration 52 : 2.0099303722664144
Loss in iteration 53 : 2.288551938225236
Loss in iteration 54 : 2.009032661005631
Loss in iteration 55 : 2.194605804388766
Loss in iteration 56 : 2.151758608673056
Loss in iteration 57 : 2.4706879691802075
Loss in iteration 58 : 2.022773963280859
Loss in iteration 59 : 2.370292095780592
Loss in iteration 60 : 2.107083459942232
Loss in iteration 61 : 2.422828853837421
Loss in iteration 62 : 2.049462962357473
Loss in iteration 63 : 2.219668617668809
Loss in iteration 64 : 1.9416797317018433
Loss in iteration 65 : 2.1033975030789582
Loss in iteration 66 : 2.231725809704111
Loss in iteration 67 : 2.3724545204748995
Loss in iteration 68 : 2.321923671654025
Loss in iteration 69 : 2.3956358948763454
Loss in iteration 70 : 2.0292213804241417
Loss in iteration 71 : 2.1854603767109535
Loss in iteration 72 : 2.086105261090525
Loss in iteration 73 : 2.2252454746950194
Loss in iteration 74 : 2.1523518717469865
Loss in iteration 75 : 2.4572740371823256
Loss in iteration 76 : 2.1571875761610673
Loss in iteration 77 : 2.3260638213839497
Loss in iteration 78 : 2.092114560795287
Loss in iteration 79 : 2.182335944611148
Loss in iteration 80 : 2.0035606430465647
Loss in iteration 81 : 2.2701769598384614
Loss in iteration 82 : 1.990157903116125
Loss in iteration 83 : 2.3176683456760383
Loss in iteration 84 : 2.2371141869258424
Loss in iteration 85 : 2.4613898096292988
Loss in iteration 86 : 2.1002855662929734
Loss in iteration 87 : 2.3388003584855013
Loss in iteration 88 : 2.0711776708952896
Loss in iteration 89 : 2.117925626124312
Loss in iteration 90 : 1.9919010125634837
Loss in iteration 91 : 2.115336456661619
Loss in iteration 92 : 2.092178952337207
Loss in iteration 93 : 2.0956106378237247
Loss in iteration 94 : 1.9131276897814897
Loss in iteration 95 : 2.1064096440629876
Loss in iteration 96 : 2.126442765417606
Loss in iteration 97 : 2.5362926181493233
Loss in iteration 98 : 2.2064436115405877
Loss in iteration 99 : 2.275840864369563
Loss in iteration 100 : 2.0350201261881975
Loss in iteration 101 : 2.2027953780168357
Loss in iteration 102 : 1.9525208458292753
Loss in iteration 103 : 2.0664243260067816
Loss in iteration 104 : 2.079021746925742
Loss in iteration 105 : 2.2025894697048556
Loss in iteration 106 : 2.035995422727655
Loss in iteration 107 : 2.3103821852456368
Loss in iteration 108 : 2.0701125096515107
Loss in iteration 109 : 2.3525191978374282
Loss in iteration 110 : 2.3571517218614892
Loss in iteration 111 : 2.6452272952114
Loss in iteration 112 : 2.2778954164232292
Loss in iteration 113 : 2.489590404007297
Loss in iteration 114 : 2.378779981476002
Loss in iteration 115 : 2.364714571080386
Loss in iteration 116 : 2.013671643055157
Loss in iteration 117 : 2.1525419728102966
Loss in iteration 118 : 1.8655756568354553
Loss in iteration 119 : 1.9517616138372873
Loss in iteration 120 : 1.9994124137602878
Loss in iteration 121 : 2.239859877300522
Loss in iteration 122 : 1.8618829301250208
Loss in iteration 123 : 1.9364371891879795
Loss in iteration 124 : 2.092561659937885
Loss in iteration 125 : 2.466263368826521
Loss in iteration 126 : 2.2235417387471665
Loss in iteration 127 : 2.6961449246362093
Loss in iteration 128 : 2.109986407157656
Loss in iteration 129 : 2.2485403381252227
Loss in iteration 130 : 2.1492981379744447
Loss in iteration 131 : 2.3886806313748394
Loss in iteration 132 : 1.9735179457723908
Loss in iteration 133 : 2.1066504600917964
Loss in iteration 134 : 1.9598579315621998
Loss in iteration 135 : 2.237023526427241
Loss in iteration 136 : 2.1278677963831916
Loss in iteration 137 : 2.2703521006652063
Loss in iteration 138 : 2.026892700515264
Loss in iteration 139 : 2.175422695029973
Loss in iteration 140 : 2.125683531267626
Loss in iteration 141 : 2.2272438598964004
Loss in iteration 142 : 2.127742027041892
Loss in iteration 143 : 2.268077102146025
Loss in iteration 144 : 1.9987646989032397
Loss in iteration 145 : 2.0292167756873973
Loss in iteration 146 : 2.0831269226845057
Loss in iteration 147 : 2.4092296157137585
Loss in iteration 148 : 2.206388751900312
Loss in iteration 149 : 2.6067025473598386
Loss in iteration 150 : 2.424537873252208
Loss in iteration 151 : 2.592421628882295
Loss in iteration 152 : 2.27108622016434
Loss in iteration 153 : 2.3228765878374067
Loss in iteration 154 : 1.947172179641631
Loss in iteration 155 : 2.077486531968165
Loss in iteration 156 : 2.008388755843021
Loss in iteration 157 : 2.1147549234296896
Loss in iteration 158 : 1.8228587799108404
Loss in iteration 159 : 1.9519032626637485
Loss in iteration 160 : 1.8632828026587758
Loss in iteration 161 : 2.1432536819380297
Loss in iteration 162 : 1.8847152570098031
Loss in iteration 163 : 2.050463583369591
Loss in iteration 164 : 1.9953613060278113
Loss in iteration 165 : 2.366670388046492
Loss in iteration 166 : 2.307893504950948
Loss in iteration 167 : 2.709325450088615
Loss in iteration 168 : 2.4237099443934644
Loss in iteration 169 : 2.663945356397411
Loss in iteration 170 : 2.0501021812128233
Loss in iteration 171 : 2.2688553709424686
Loss in iteration 172 : 2.2187505737804547
Loss in iteration 173 : 2.316626881598065
Loss in iteration 174 : 2.2218274638789226
Loss in iteration 175 : 2.4158419286333963
Loss in iteration 176 : 2.0599492901504766
Loss in iteration 177 : 2.1578334530388914
Loss in iteration 178 : 2.0538850393090655
Loss in iteration 179 : 2.1588023662004243
Loss in iteration 180 : 1.9631817304948294
Loss in iteration 181 : 2.0224081060414005
Loss in iteration 182 : 2.112551008043308
Loss in iteration 183 : 2.40874684543734
Loss in iteration 184 : 2.0255365416259377
Loss in iteration 185 : 2.1707458793517773
Loss in iteration 186 : 2.0055656005560403
Loss in iteration 187 : 2.248456330413402
Loss in iteration 188 : 2.0551101372895846
Loss in iteration 189 : 2.2734918897491756
Loss in iteration 190 : 2.3620401642815367
Loss in iteration 191 : 2.635255645167542
Loss in iteration 192 : 2.304880575083832
Loss in iteration 193 : 2.332185712262356
Loss in iteration 194 : 2.021952606428441
Loss in iteration 195 : 2.102818291373939
Loss in iteration 196 : 2.1482651477972423
Loss in iteration 197 : 2.4091357966768423
Loss in iteration 198 : 2.157225402469187
Loss in iteration 199 : 2.375162978220521
Loss in iteration 200 : 2.3008056124923444
Testing accuracy  of updater 2 on alg 0 with rate 4.0 = 0.7815, training accuracy 0.7805, time elapsed: 4479 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.767735722167926
Loss in iteration 3 : 1.764961001912834
Loss in iteration 4 : 2.4566746023833828
Loss in iteration 5 : 0.584041451108754
Loss in iteration 6 : 0.5919456080293449
Loss in iteration 7 : 0.9506147709255168
Loss in iteration 8 : 1.2795246768916675
Loss in iteration 9 : 0.9566237922376971
Loss in iteration 10 : 0.9275641155230289
Loss in iteration 11 : 0.763425411289091
Loss in iteration 12 : 0.6319246054719313
Loss in iteration 13 : 0.5859257362256538
Loss in iteration 14 : 0.5357167675852554
Loss in iteration 15 : 0.5404211265066837
Loss in iteration 16 : 0.5179543343452958
Loss in iteration 17 : 0.5408439599299639
Loss in iteration 18 : 0.5388260475074323
Loss in iteration 19 : 0.539736044557168
Loss in iteration 20 : 0.5403401596820249
Loss in iteration 21 : 0.5144132002835291
Loss in iteration 22 : 0.5446010208243378
Loss in iteration 23 : 0.5141587649057132
Loss in iteration 24 : 0.5198345096943385
Loss in iteration 25 : 0.4984519662196462
Loss in iteration 26 : 0.49877105107894476
Loss in iteration 27 : 0.4857123950246199
Loss in iteration 28 : 0.5104882082285704
Loss in iteration 29 : 0.4796208695746555
Loss in iteration 30 : 0.48353513473695764
Loss in iteration 31 : 0.46969207938924196
Loss in iteration 32 : 0.46825415742179416
Loss in iteration 33 : 0.4874021791833427
Loss in iteration 34 : 0.4863000462138347
Loss in iteration 35 : 0.47432724117432057
Loss in iteration 36 : 0.4826744840049687
Loss in iteration 37 : 0.47587800488722154
Loss in iteration 38 : 0.47033267602757217
Loss in iteration 39 : 0.4629710655538229
Loss in iteration 40 : 0.4546967258085694
Loss in iteration 41 : 0.45570779492563696
Loss in iteration 42 : 0.4694425229548378
Loss in iteration 43 : 0.46439798563382606
Loss in iteration 44 : 0.451843475756529
Loss in iteration 45 : 0.4723838249585199
Loss in iteration 46 : 0.4554790434249239
Loss in iteration 47 : 0.46487952066105676
Loss in iteration 48 : 0.4621453379743657
Loss in iteration 49 : 0.4737396416345457
Loss in iteration 50 : 0.4741074035203397
Loss in iteration 51 : 0.46200527188591534
Loss in iteration 52 : 0.47053099151327105
Loss in iteration 53 : 0.4713484322222519
Loss in iteration 54 : 0.4765960747108429
Loss in iteration 55 : 0.5015233696031812
Loss in iteration 56 : 0.5301934175802938
Loss in iteration 57 : 0.5815124213220417
Loss in iteration 58 : 0.6382823891134962
Loss in iteration 59 : 0.7157609838370248
Loss in iteration 60 : 0.7047820825349317
Loss in iteration 61 : 0.7225319857130853
Loss in iteration 62 : 0.6986477858443091
Loss in iteration 63 : 0.7070251748206349
Loss in iteration 64 : 0.6501693620379783
Loss in iteration 65 : 0.5854122417720294
Loss in iteration 66 : 0.5568772973590187
Loss in iteration 67 : 0.5200759251372682
Loss in iteration 68 : 0.5426861396768435
Loss in iteration 69 : 0.520389937134438
Loss in iteration 70 : 0.4890117812805076
Loss in iteration 71 : 0.49309859509761184
Loss in iteration 72 : 0.4940523871304154
Loss in iteration 73 : 0.5084117761205733
Loss in iteration 74 : 0.49601090390261793
Loss in iteration 75 : 0.5040957540616823
Loss in iteration 76 : 0.4816504918759605
Loss in iteration 77 : 0.4752322075622421
Loss in iteration 78 : 0.4754541237768448
Loss in iteration 79 : 0.5078289129167319
Loss in iteration 80 : 0.5153517491778397
Loss in iteration 81 : 0.5147298861115102
Loss in iteration 82 : 0.5066083933799611
Loss in iteration 83 : 0.5297851457877791
Loss in iteration 84 : 0.5209675390530555
Loss in iteration 85 : 0.5530147314926859
Loss in iteration 86 : 0.5736723772476131
Loss in iteration 87 : 0.5815982455716645
Loss in iteration 88 : 0.5606312389452156
Loss in iteration 89 : 0.5877473357276187
Loss in iteration 90 : 0.6101788920976475
Loss in iteration 91 : 0.5856911218768118
Loss in iteration 92 : 0.5682611714012833
Loss in iteration 93 : 0.5878743610629021
Loss in iteration 94 : 0.5914870580557985
Loss in iteration 95 : 0.5957812675727212
Loss in iteration 96 : 0.577543683582518
Loss in iteration 97 : 0.5677910723539316
Loss in iteration 98 : 0.5436480158904354
Loss in iteration 99 : 0.5490766639281109
Loss in iteration 100 : 0.5589999472699245
Loss in iteration 101 : 0.5546164580589894
Loss in iteration 102 : 0.5502324395154755
Loss in iteration 103 : 0.5545116964957271
Loss in iteration 104 : 0.550460308587653
Loss in iteration 105 : 0.5430864352108995
Loss in iteration 106 : 0.5668213030806019
Loss in iteration 107 : 0.5624323632166164
Loss in iteration 108 : 0.5415095070728839
Loss in iteration 109 : 0.5603805771202625
Loss in iteration 110 : 0.5279172618981942
Loss in iteration 111 : 0.5141898999755057
Loss in iteration 112 : 0.5268131485530378
Loss in iteration 113 : 0.515886156520258
Loss in iteration 114 : 0.5214988967211408
Loss in iteration 115 : 0.508111290913927
Loss in iteration 116 : 0.5069051173372008
Loss in iteration 117 : 0.5249347954858142
Loss in iteration 118 : 0.5229468646131379
Loss in iteration 119 : 0.5332052173180869
Loss in iteration 120 : 0.5241634154208629
Loss in iteration 121 : 0.5173047489607712
Loss in iteration 122 : 0.5397555659778059
Loss in iteration 123 : 0.5421372164202577
Loss in iteration 124 : 0.5263679739424454
Loss in iteration 125 : 0.529164189388076
Loss in iteration 126 : 0.5294336700081499
Loss in iteration 127 : 0.5510085368856041
Loss in iteration 128 : 0.5474309615318957
Loss in iteration 129 : 0.5766498992466845
Loss in iteration 130 : 0.5845902583678525
Loss in iteration 131 : 0.5882134047913932
Loss in iteration 132 : 0.5802310557723886
Loss in iteration 133 : 0.5940882614732872
Loss in iteration 134 : 0.5919597121673362
Loss in iteration 135 : 0.6002402623651933
Loss in iteration 136 : 0.5744634615598256
Loss in iteration 137 : 0.5749660161268914
Loss in iteration 138 : 0.5886603800132033
Loss in iteration 139 : 0.5770683732445867
Loss in iteration 140 : 0.5744185289155355
Loss in iteration 141 : 0.5682552722141972
Loss in iteration 142 : 0.5565441508872093
Loss in iteration 143 : 0.528013226398175
Loss in iteration 144 : 0.5312162343646879
Loss in iteration 145 : 0.5321412044010163
Loss in iteration 146 : 0.5178299840317813
Loss in iteration 147 : 0.5251244851012743
Loss in iteration 148 : 0.5033254008494298
Loss in iteration 149 : 0.5026338622492184
Loss in iteration 150 : 0.4887590559923229
Loss in iteration 151 : 0.48565745666037075
Loss in iteration 152 : 0.5026595728297042
Loss in iteration 153 : 0.5030387977787375
Loss in iteration 154 : 0.5125298293261452
Loss in iteration 155 : 0.5061551391093969
Loss in iteration 156 : 0.5032395476614937
Loss in iteration 157 : 0.49443995112203504
Loss in iteration 158 : 0.49931694564545454
Loss in iteration 159 : 0.5248991953369231
Loss in iteration 160 : 0.5163187252594031
Loss in iteration 161 : 0.5268337152982209
Loss in iteration 162 : 0.5466825587454937
Loss in iteration 163 : 0.6002576080611567
Loss in iteration 164 : 0.6335656761751207
Loss in iteration 165 : 0.6538175362636021
Loss in iteration 166 : 0.6128755703501185
Loss in iteration 167 : 0.6035259427998573
Loss in iteration 168 : 0.5368786844514443
Loss in iteration 169 : 0.5446196626344767
Loss in iteration 170 : 0.5302334446605778
Loss in iteration 171 : 0.5383254870197502
Loss in iteration 172 : 0.5127428208900057
Loss in iteration 173 : 0.5053306486626743
Loss in iteration 174 : 0.5028179234200074
Loss in iteration 175 : 0.5111170103117773
Loss in iteration 176 : 0.5227171674086517
Loss in iteration 177 : 0.5143462317297235
Loss in iteration 178 : 0.49946111732126575
Loss in iteration 179 : 0.5021632558684991
Loss in iteration 180 : 0.5098743897382422
Loss in iteration 181 : 0.5087858386966838
Loss in iteration 182 : 0.5102266003524172
Loss in iteration 183 : 0.5127223613421531
Loss in iteration 184 : 0.5115087299819685
Loss in iteration 185 : 0.5260691856338051
Loss in iteration 186 : 0.5262609474135698
Loss in iteration 187 : 0.517226163883592
Loss in iteration 188 : 0.5452807869949746
Loss in iteration 189 : 0.5498313651287234
Loss in iteration 190 : 0.5493165686066425
Loss in iteration 191 : 0.5431798237942324
Loss in iteration 192 : 0.5422621038624845
Loss in iteration 193 : 0.5393087577255059
Loss in iteration 194 : 0.5530755198436049
Loss in iteration 195 : 0.5730229152713452
Loss in iteration 196 : 0.5748440548163634
Loss in iteration 197 : 0.5588062332285221
Loss in iteration 198 : 0.5461866281273497
Loss in iteration 199 : 0.5538398756361916
Loss in iteration 200 : 0.5444307471739482
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.7765, training accuracy 0.784625, time elapsed: 5707 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.7148539948171944
Loss in iteration 3 : 0.9297694532244074
Loss in iteration 4 : 1.403956884100903
Loss in iteration 5 : 0.7238701962980004
Loss in iteration 6 : 0.7774398946788543
Loss in iteration 7 : 0.8017208499175672
Loss in iteration 8 : 0.7445247779010474
Loss in iteration 9 : 0.626612074665598
Loss in iteration 10 : 0.5796460628207663
Loss in iteration 11 : 0.513433066677404
Loss in iteration 12 : 0.4846231483218268
Loss in iteration 13 : 0.4906320998877649
Loss in iteration 14 : 0.47867556737589617
Loss in iteration 15 : 0.49504018790695886
Loss in iteration 16 : 0.483025456110273
Loss in iteration 17 : 0.5004813335026348
Loss in iteration 18 : 0.49726278812202457
Loss in iteration 19 : 0.49886749847856
Loss in iteration 20 : 0.4998605994605318
Loss in iteration 21 : 0.48245575479428726
Loss in iteration 22 : 0.5047403584271414
Loss in iteration 23 : 0.48451186363305115
Loss in iteration 24 : 0.4926715366595283
Loss in iteration 25 : 0.474734401316853
Loss in iteration 26 : 0.47747663937196483
Loss in iteration 27 : 0.46733715464011344
Loss in iteration 28 : 0.4897513268120831
Loss in iteration 29 : 0.4654490328360928
Loss in iteration 30 : 0.47106481952673324
Loss in iteration 31 : 0.459752902016548
Loss in iteration 32 : 0.46002600211197736
Loss in iteration 33 : 0.4793714996301247
Loss in iteration 34 : 0.4786462048441048
Loss in iteration 35 : 0.46999313463824266
Loss in iteration 36 : 0.4762888338557069
Loss in iteration 37 : 0.4717814258285408
Loss in iteration 38 : 0.4662793487473168
Loss in iteration 39 : 0.4604254697429251
Loss in iteration 40 : 0.45252031427211925
Loss in iteration 41 : 0.45546212163395106
Loss in iteration 42 : 0.46545880727575156
Loss in iteration 43 : 0.46304485612981466
Loss in iteration 44 : 0.4505777569684577
Loss in iteration 45 : 0.4711062769641379
Loss in iteration 46 : 0.4540017200145531
Loss in iteration 47 : 0.46283727235835
Loss in iteration 48 : 0.4605521403805389
Loss in iteration 49 : 0.47183793062654256
Loss in iteration 50 : 0.47083656030007753
Loss in iteration 51 : 0.4565635470392556
Loss in iteration 52 : 0.4647382123512368
Loss in iteration 53 : 0.4602488763019383
Loss in iteration 54 : 0.45354343398793384
Loss in iteration 55 : 0.45526872809356533
Loss in iteration 56 : 0.45890879256131345
Loss in iteration 57 : 0.460743012993328
Loss in iteration 58 : 0.46833905977890117
Loss in iteration 59 : 0.47873389818179174
Loss in iteration 60 : 0.4667093954191018
Loss in iteration 61 : 0.4638363119825513
Loss in iteration 62 : 0.46092719144782496
Loss in iteration 63 : 0.4667977433652109
Loss in iteration 64 : 0.4616440252713207
Loss in iteration 65 : 0.45661676038844934
Loss in iteration 66 : 0.4732649772216871
Loss in iteration 67 : 0.4538059966098223
Loss in iteration 68 : 0.47592250618640103
Loss in iteration 69 : 0.4675874415781061
Loss in iteration 70 : 0.4552074465171488
Loss in iteration 71 : 0.46387986463827235
Loss in iteration 72 : 0.4651825508348693
Loss in iteration 73 : 0.4741727944183415
Loss in iteration 74 : 0.4665368269586007
Loss in iteration 75 : 0.475249542459967
Loss in iteration 76 : 0.4574680133028468
Loss in iteration 77 : 0.45368064355797005
Loss in iteration 78 : 0.44726669004886166
Loss in iteration 79 : 0.464040503746195
Loss in iteration 80 : 0.4636515855721913
Loss in iteration 81 : 0.4653761005425479
Loss in iteration 82 : 0.45439410154671517
Loss in iteration 83 : 0.4658572796101128
Loss in iteration 84 : 0.4537201871453196
Loss in iteration 85 : 0.46276680826479594
Loss in iteration 86 : 0.459862250433868
Loss in iteration 87 : 0.4612191337763945
Loss in iteration 88 : 0.46479771993095303
Loss in iteration 89 : 0.4590875850973895
Loss in iteration 90 : 0.46493442957685543
Loss in iteration 91 : 0.458810153126487
Loss in iteration 92 : 0.4653810492482613
Loss in iteration 93 : 0.46486905080068197
Loss in iteration 94 : 0.460483747170584
Loss in iteration 95 : 0.46538505438997346
Loss in iteration 96 : 0.46556619382230624
Loss in iteration 97 : 0.47073831273811323
Loss in iteration 98 : 0.46107301194319245
Loss in iteration 99 : 0.45180337303325213
Loss in iteration 100 : 0.45668573914750926
Loss in iteration 101 : 0.45768131070125434
Loss in iteration 102 : 0.463417609466575
Loss in iteration 103 : 0.46818847769255306
Loss in iteration 104 : 0.4650733469435686
Loss in iteration 105 : 0.4494790592090374
Loss in iteration 106 : 0.46893855227069114
Loss in iteration 107 : 0.46435616606254526
Loss in iteration 108 : 0.451747684724151
Loss in iteration 109 : 0.4678001331269557
Loss in iteration 110 : 0.4593996220460735
Loss in iteration 111 : 0.4541644324566513
Loss in iteration 112 : 0.4614175406400254
Loss in iteration 113 : 0.45846334490458407
Loss in iteration 114 : 0.46993694463373153
Loss in iteration 115 : 0.46143103117793205
Loss in iteration 116 : 0.4535849399183415
Loss in iteration 117 : 0.4602431402845728
Loss in iteration 118 : 0.46035470978605536
Loss in iteration 119 : 0.4620205229882113
Loss in iteration 120 : 0.4638024409208302
Loss in iteration 121 : 0.4622534021418896
Loss in iteration 122 : 0.46882275453309263
Loss in iteration 123 : 0.46341156157921987
Loss in iteration 124 : 0.45606949080052783
Loss in iteration 125 : 0.4601077774471162
Loss in iteration 126 : 0.465920751423015
Loss in iteration 127 : 0.47507566973813614
Loss in iteration 128 : 0.45825035106638207
Loss in iteration 129 : 0.4627395241328128
Loss in iteration 130 : 0.4635101519773124
Loss in iteration 131 : 0.4642956943052341
Loss in iteration 132 : 0.4584616260370645
Loss in iteration 133 : 0.46344897473691693
Loss in iteration 134 : 0.45991873955733453
Loss in iteration 135 : 0.45860739236185627
Loss in iteration 136 : 0.46523439865566446
Loss in iteration 137 : 0.4550275444670344
Loss in iteration 138 : 0.4597984924686974
Loss in iteration 139 : 0.447862155588242
Loss in iteration 140 : 0.46516135851709106
Loss in iteration 141 : 0.4658886380606634
Loss in iteration 142 : 0.46359512735743713
Loss in iteration 143 : 0.44948349637882795
Loss in iteration 144 : 0.4587812516063979
Loss in iteration 145 : 0.4603032951297469
Loss in iteration 146 : 0.4520259644180247
Loss in iteration 147 : 0.46283456520461735
Loss in iteration 148 : 0.4569316665479323
Loss in iteration 149 : 0.4681258224224131
Loss in iteration 150 : 0.46326843323273564
Loss in iteration 151 : 0.4610785297756136
Loss in iteration 152 : 0.47152597443433447
Loss in iteration 153 : 0.4601154929153412
Loss in iteration 154 : 0.4610367747677046
Loss in iteration 155 : 0.45575931456480273
Loss in iteration 156 : 0.45975679664752095
Loss in iteration 157 : 0.4529305906784101
Loss in iteration 158 : 0.44885712855015664
Loss in iteration 159 : 0.46136485357755774
Loss in iteration 160 : 0.45043657078842175
Loss in iteration 161 : 0.45427882274821013
Loss in iteration 162 : 0.4581302561081125
Loss in iteration 163 : 0.4676912270294221
Loss in iteration 164 : 0.4668149093648507
Loss in iteration 165 : 0.47187810562072663
Loss in iteration 166 : 0.4629155763536637
Loss in iteration 167 : 0.47514755940327524
Loss in iteration 168 : 0.4524915728446906
Loss in iteration 169 : 0.4623169228873822
Loss in iteration 170 : 0.4536241543534082
Loss in iteration 171 : 0.46167197664632303
Loss in iteration 172 : 0.4564907068227487
Loss in iteration 173 : 0.45640157801435843
Loss in iteration 174 : 0.4564650358227841
Loss in iteration 175 : 0.460239224535543
Loss in iteration 176 : 0.46863674545372763
Loss in iteration 177 : 0.4653312245053034
Loss in iteration 178 : 0.4616679446535438
Loss in iteration 179 : 0.46210058340476096
Loss in iteration 180 : 0.46385975995664014
Loss in iteration 181 : 0.4651238266161415
Loss in iteration 182 : 0.46692715283152647
Loss in iteration 183 : 0.46657409506863023
Loss in iteration 184 : 0.45688267602519045
Loss in iteration 185 : 0.4542809341286998
Loss in iteration 186 : 0.4532609793888752
Loss in iteration 187 : 0.44946599681939875
Loss in iteration 188 : 0.47088880588610627
Loss in iteration 189 : 0.46377720652768845
Loss in iteration 190 : 0.46760272818756693
Loss in iteration 191 : 0.46301014623107806
Loss in iteration 192 : 0.4638282636099396
Loss in iteration 193 : 0.45411377612242887
Loss in iteration 194 : 0.4586863676756458
Loss in iteration 195 : 0.46046809947253375
Loss in iteration 196 : 0.46609652263617735
Loss in iteration 197 : 0.4555193741512587
Loss in iteration 198 : 0.4578285940347366
Loss in iteration 199 : 0.4642759208979365
Loss in iteration 200 : 0.4567850920833807
Testing accuracy  of updater 2 on alg 0 with rate 0.7 = 0.7845, training accuracy 0.7895, time elapsed: 3962 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6853335969180427
Loss in iteration 3 : 0.6564102173293027
Loss in iteration 4 : 0.6341539464644386
Loss in iteration 5 : 0.6044702159330118
Loss in iteration 6 : 0.5809500871848617
Loss in iteration 7 : 0.5589642475419625
Loss in iteration 8 : 0.5452414842272664
Loss in iteration 9 : 0.5305952390613792
Loss in iteration 10 : 0.5250147260374637
Loss in iteration 11 : 0.5075734248529594
Loss in iteration 12 : 0.5008978741466273
Loss in iteration 13 : 0.5029567926415054
Loss in iteration 14 : 0.49161755214821257
Loss in iteration 15 : 0.4990032176282464
Loss in iteration 16 : 0.48572667505756184
Loss in iteration 17 : 0.49598466314143425
Loss in iteration 18 : 0.49062148422095675
Loss in iteration 19 : 0.49167544395199125
Loss in iteration 20 : 0.4899018184485631
Loss in iteration 21 : 0.4791112879538988
Loss in iteration 22 : 0.49319476931566303
Loss in iteration 23 : 0.4805423706826944
Loss in iteration 24 : 0.48797514146462606
Loss in iteration 25 : 0.47199703472297255
Loss in iteration 26 : 0.47623414209208825
Loss in iteration 27 : 0.4666821349056627
Loss in iteration 28 : 0.48448866154196263
Loss in iteration 29 : 0.46673478993581813
Loss in iteration 30 : 0.4714963972581079
Loss in iteration 31 : 0.46277193616163825
Loss in iteration 32 : 0.4630142783368145
Loss in iteration 33 : 0.4807469031060725
Loss in iteration 34 : 0.47942517864082085
Loss in iteration 35 : 0.4743580793765682
Loss in iteration 36 : 0.4779728794114636
Loss in iteration 37 : 0.47594761684807446
Loss in iteration 38 : 0.4700556679176234
Loss in iteration 39 : 0.46522449376590447
Loss in iteration 40 : 0.45788715785391076
Loss in iteration 41 : 0.46263688262620817
Loss in iteration 42 : 0.46811198878427074
Loss in iteration 43 : 0.4678720243131876
Loss in iteration 44 : 0.45526085485497175
Loss in iteration 45 : 0.47488948006057424
Loss in iteration 46 : 0.4591157651633137
Loss in iteration 47 : 0.4664886141921379
Loss in iteration 48 : 0.464628277697281
Loss in iteration 49 : 0.4753230324089535
Loss in iteration 50 : 0.47347381708674485
Loss in iteration 51 : 0.45840365854500126
Loss in iteration 52 : 0.470008887527903
Loss in iteration 53 : 0.46440872436720787
Loss in iteration 54 : 0.45718315674837523
Loss in iteration 55 : 0.4589913184820141
Loss in iteration 56 : 0.462524694169881
Loss in iteration 57 : 0.4622225589570344
Loss in iteration 58 : 0.47003397900594507
Loss in iteration 59 : 0.478800343696576
Loss in iteration 60 : 0.4683587796790224
Loss in iteration 61 : 0.46518266190616786
Loss in iteration 62 : 0.46275335626559705
Loss in iteration 63 : 0.4684326839168139
Loss in iteration 64 : 0.46350455562764287
Loss in iteration 65 : 0.4582740286099467
Loss in iteration 66 : 0.4717922179292711
Loss in iteration 67 : 0.4532738523808245
Loss in iteration 68 : 0.47602774429222283
Loss in iteration 69 : 0.4681023779430945
Loss in iteration 70 : 0.4559320905695673
Loss in iteration 71 : 0.46581972442423486
Loss in iteration 72 : 0.4669196404246872
Loss in iteration 73 : 0.4757028754026781
Loss in iteration 74 : 0.46876698834766134
Loss in iteration 75 : 0.47560117587796397
Loss in iteration 76 : 0.45925399322251714
Loss in iteration 77 : 0.4555684608183706
Loss in iteration 78 : 0.4500790199640342
Loss in iteration 79 : 0.4650709671630114
Loss in iteration 80 : 0.46380348450698844
Loss in iteration 81 : 0.46755703483597466
Loss in iteration 82 : 0.45664579554348805
Loss in iteration 83 : 0.467405479630921
Loss in iteration 84 : 0.45547677815459364
Loss in iteration 85 : 0.46469206697380194
Loss in iteration 86 : 0.4617053677194738
Loss in iteration 87 : 0.4621411299972496
Loss in iteration 88 : 0.46557773740141667
Loss in iteration 89 : 0.46085925368346853
Loss in iteration 90 : 0.46533504998779057
Loss in iteration 91 : 0.46136857995348135
Loss in iteration 92 : 0.4659602600156312
Loss in iteration 93 : 0.46728397811937145
Loss in iteration 94 : 0.4608823400150238
Loss in iteration 95 : 0.46571478600927346
Loss in iteration 96 : 0.46668665709127216
Loss in iteration 97 : 0.4714679040901108
Loss in iteration 98 : 0.4625472448931496
Loss in iteration 99 : 0.4540069422247718
Loss in iteration 100 : 0.4569896589746269
Loss in iteration 101 : 0.4590683775719474
Loss in iteration 102 : 0.46439313843730823
Loss in iteration 103 : 0.46912846450811
Loss in iteration 104 : 0.46577350895849917
Loss in iteration 105 : 0.45067478427643015
Loss in iteration 106 : 0.46959142800222653
Loss in iteration 107 : 0.46567361902675714
Loss in iteration 108 : 0.4529443520954156
Loss in iteration 109 : 0.4691631865047549
Loss in iteration 110 : 0.4610461567065553
Loss in iteration 111 : 0.45477296105302845
Loss in iteration 112 : 0.4627027491192671
Loss in iteration 113 : 0.45943964651489594
Loss in iteration 114 : 0.4697609322218262
Loss in iteration 115 : 0.4619010732655463
Loss in iteration 116 : 0.4552242967536882
Loss in iteration 117 : 0.4616219483973857
Loss in iteration 118 : 0.46187298984888775
Loss in iteration 119 : 0.46336331590974483
Loss in iteration 120 : 0.4656646205605111
Loss in iteration 121 : 0.4621145787575366
Loss in iteration 122 : 0.47101103032948644
Loss in iteration 123 : 0.46484398087323603
Loss in iteration 124 : 0.4575082530532108
Loss in iteration 125 : 0.4608854996880274
Loss in iteration 126 : 0.4664646840682861
Loss in iteration 127 : 0.47545281712144394
Loss in iteration 128 : 0.45976765443222184
Loss in iteration 129 : 0.4638281010324444
Loss in iteration 130 : 0.4640472199804977
Loss in iteration 131 : 0.4656716770433466
Loss in iteration 132 : 0.4596207812070462
Loss in iteration 133 : 0.4635190301159547
Loss in iteration 134 : 0.4598205174099038
Loss in iteration 135 : 0.4591664331599455
Loss in iteration 136 : 0.4665202241926022
Loss in iteration 137 : 0.4564021982236152
Loss in iteration 138 : 0.46025819645426164
Loss in iteration 139 : 0.44845080608530763
Loss in iteration 140 : 0.46559281437411376
Loss in iteration 141 : 0.467368629538825
Loss in iteration 142 : 0.464406488013027
Loss in iteration 143 : 0.4505145164114183
Loss in iteration 144 : 0.4601787676830625
Loss in iteration 145 : 0.4615709061067067
Loss in iteration 146 : 0.45291082964294693
Loss in iteration 147 : 0.46375081497966736
Loss in iteration 148 : 0.4581173934015212
Loss in iteration 149 : 0.4678282199245216
Loss in iteration 150 : 0.46290798914172515
Loss in iteration 151 : 0.4611073828936604
Loss in iteration 152 : 0.4723955484928939
Loss in iteration 153 : 0.46143833042599064
Loss in iteration 154 : 0.46229113662859017
Loss in iteration 155 : 0.4570998758743788
Loss in iteration 156 : 0.460161122192758
Loss in iteration 157 : 0.4536657914528762
Loss in iteration 158 : 0.4502928523249871
Loss in iteration 159 : 0.4622684130971284
Loss in iteration 160 : 0.4521082208261838
Loss in iteration 161 : 0.45542193403833575
Loss in iteration 162 : 0.4594164096074937
Loss in iteration 163 : 0.46752348307775127
Loss in iteration 164 : 0.46452439268957463
Loss in iteration 165 : 0.4715721243868201
Loss in iteration 166 : 0.4644395292662578
Loss in iteration 167 : 0.4756897974444202
Loss in iteration 168 : 0.45250517996373113
Loss in iteration 169 : 0.4627658461999193
Loss in iteration 170 : 0.45491053021584765
Loss in iteration 171 : 0.46328906481454973
Loss in iteration 172 : 0.45767915626587746
Loss in iteration 173 : 0.4572354543397099
Loss in iteration 174 : 0.4571796751929275
Loss in iteration 175 : 0.4607460912746306
Loss in iteration 176 : 0.4693586152525514
Loss in iteration 177 : 0.4661369318866034
Loss in iteration 178 : 0.46183107512010835
Loss in iteration 179 : 0.46311858994002514
Loss in iteration 180 : 0.46497617725256446
Loss in iteration 181 : 0.4665564742669801
Loss in iteration 182 : 0.4676587823007558
Loss in iteration 183 : 0.4670230928188431
Loss in iteration 184 : 0.4579761836577958
Loss in iteration 185 : 0.4550285453037261
Loss in iteration 186 : 0.454035963514524
Loss in iteration 187 : 0.4501497806805917
Loss in iteration 188 : 0.4719232509791557
Loss in iteration 189 : 0.46523761061635577
Loss in iteration 190 : 0.46849009297865335
Loss in iteration 191 : 0.4636068053861264
Loss in iteration 192 : 0.4639529614909252
Loss in iteration 193 : 0.45534655649713895
Loss in iteration 194 : 0.45947698209976484
Loss in iteration 195 : 0.46147685482840417
Loss in iteration 196 : 0.46738236918107295
Loss in iteration 197 : 0.4561044214572167
Loss in iteration 198 : 0.45797834318297004
Loss in iteration 199 : 0.46460776484370847
Loss in iteration 200 : 0.4569983518067137
Testing accuracy  of updater 2 on alg 0 with rate 0.4 = 0.786, training accuracy 0.789625, time elapsed: 3882 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6855183764303943
Loss in iteration 3 : 0.6752071143881596
Loss in iteration 4 : 0.6694538546572179
Loss in iteration 5 : 0.6603412770986979
Loss in iteration 6 : 0.649276131184747
Loss in iteration 7 : 0.6376753246725527
Loss in iteration 8 : 0.628589959322313
Loss in iteration 9 : 0.6182774663573497
Loss in iteration 10 : 0.6109086568335786
Loss in iteration 11 : 0.599760470297303
Loss in iteration 12 : 0.5907290741029715
Loss in iteration 13 : 0.5854907696988202
Loss in iteration 14 : 0.5765086917573979
Loss in iteration 15 : 0.5744289286167581
Loss in iteration 16 : 0.5618708632953224
Loss in iteration 17 : 0.5624891244012655
Loss in iteration 18 : 0.5548493346359661
Loss in iteration 19 : 0.5504818291684279
Loss in iteration 20 : 0.5464790803137286
Loss in iteration 21 : 0.5390602114248428
Loss in iteration 22 : 0.5397040417253669
Loss in iteration 23 : 0.5320040821792011
Loss in iteration 24 : 0.534013610344122
Loss in iteration 25 : 0.520600681185935
Loss in iteration 26 : 0.52223843839508
Loss in iteration 27 : 0.5131348432351412
Loss in iteration 28 : 0.5211317911647115
Loss in iteration 29 : 0.5099326986113923
Loss in iteration 30 : 0.5116261293945886
Loss in iteration 31 : 0.5024489220611849
Loss in iteration 32 : 0.5027812984575248
Loss in iteration 33 : 0.5124522052740171
Loss in iteration 34 : 0.5096498503451384
Loss in iteration 35 : 0.5063304806788846
Loss in iteration 36 : 0.5056782762158981
Loss in iteration 37 : 0.5064240821890379
Loss in iteration 38 : 0.5009278855512711
Loss in iteration 39 : 0.4960936499827721
Loss in iteration 40 : 0.4911104127443085
Loss in iteration 41 : 0.49491473798777774
Loss in iteration 42 : 0.493878060730064
Loss in iteration 43 : 0.49519051867266384
Loss in iteration 44 : 0.4876628585264642
Loss in iteration 45 : 0.4997476434957288
Loss in iteration 46 : 0.48641776490574884
Loss in iteration 47 : 0.49210498794720864
Loss in iteration 48 : 0.4911631782253397
Loss in iteration 49 : 0.49770560671504094
Loss in iteration 50 : 0.4951658025595899
Loss in iteration 51 : 0.48276157652996293
Loss in iteration 52 : 0.49527630211422996
Loss in iteration 53 : 0.48941199068141084
Loss in iteration 54 : 0.4837915993672641
Loss in iteration 55 : 0.48487009259048747
Loss in iteration 56 : 0.48668137505620807
Loss in iteration 57 : 0.48354661755021233
Loss in iteration 58 : 0.4910540611721614
Loss in iteration 59 : 0.49463448926834247
Loss in iteration 60 : 0.48854941181554634
Loss in iteration 61 : 0.4848356780858832
Loss in iteration 62 : 0.48327693707884
Loss in iteration 63 : 0.48759580145390885
Loss in iteration 64 : 0.482873853339595
Loss in iteration 65 : 0.4783287761461201
Loss in iteration 66 : 0.4892193730102019
Loss in iteration 67 : 0.47540434258338915
Loss in iteration 68 : 0.49143268574357724
Loss in iteration 69 : 0.4848082978934197
Loss in iteration 70 : 0.4750372424089796
Loss in iteration 71 : 0.4861933750832507
Loss in iteration 72 : 0.48470444941463636
Loss in iteration 73 : 0.49245920693518863
Loss in iteration 74 : 0.48684917328598476
Loss in iteration 75 : 0.48862802651461246
Loss in iteration 76 : 0.4773492337175135
Loss in iteration 77 : 0.47392678643925923
Loss in iteration 78 : 0.4713243237619892
Loss in iteration 79 : 0.48106793441805756
Loss in iteration 80 : 0.47968150168700074
Loss in iteration 81 : 0.4844798851433337
Loss in iteration 82 : 0.4748832857239054
Loss in iteration 83 : 0.4832375298997907
Loss in iteration 84 : 0.471671260894524
Loss in iteration 85 : 0.48095488407527165
Loss in iteration 86 : 0.4778800194990572
Loss in iteration 87 : 0.47412788843524306
Loss in iteration 88 : 0.4786419747116743
Loss in iteration 89 : 0.4746791399304861
Loss in iteration 90 : 0.47956209294868235
Loss in iteration 91 : 0.47708599267286306
Loss in iteration 92 : 0.47737045646172993
Loss in iteration 93 : 0.48100533979370297
Loss in iteration 94 : 0.47590549253058756
Loss in iteration 95 : 0.4789670973686044
Loss in iteration 96 : 0.4800690037835047
Loss in iteration 97 : 0.4833460031981065
Loss in iteration 98 : 0.4772751589625743
Loss in iteration 99 : 0.469267549840712
Loss in iteration 100 : 0.46976515927182894
Loss in iteration 101 : 0.4736373127076488
Loss in iteration 102 : 0.47606172847168643
Loss in iteration 103 : 0.4821249586586187
Loss in iteration 104 : 0.47863567940211804
Loss in iteration 105 : 0.4664057802135751
Loss in iteration 106 : 0.4823921038362009
Loss in iteration 107 : 0.47817121603428253
Loss in iteration 108 : 0.46738232382441885
Loss in iteration 109 : 0.4822717828991575
Loss in iteration 110 : 0.47329883354517616
Loss in iteration 111 : 0.467285898083123
Loss in iteration 112 : 0.4749863696510767
Loss in iteration 113 : 0.4719093800545134
Loss in iteration 114 : 0.4792658249986746
Loss in iteration 115 : 0.4740811986795033
Loss in iteration 116 : 0.46986325475925317
Loss in iteration 117 : 0.4726563514853085
Loss in iteration 118 : 0.47372847728085776
Loss in iteration 119 : 0.4754254649150772
Loss in iteration 120 : 0.47871295370728406
Loss in iteration 121 : 0.4708622305764378
Loss in iteration 122 : 0.4782093069700479
Loss in iteration 123 : 0.47564978690549886
Loss in iteration 124 : 0.4675242623161155
Loss in iteration 125 : 0.47162245181842394
Loss in iteration 126 : 0.4764233419405997
Loss in iteration 127 : 0.4840138827554333
Loss in iteration 128 : 0.46906838554395175
Loss in iteration 129 : 0.474502619617245
Loss in iteration 130 : 0.47245870401811857
Loss in iteration 131 : 0.47585680326162816
Loss in iteration 132 : 0.4701098466773982
Loss in iteration 133 : 0.472860314808726
Loss in iteration 134 : 0.4699132030144145
Loss in iteration 135 : 0.4682969791264953
Loss in iteration 136 : 0.47536900872861887
Loss in iteration 137 : 0.46593348963030456
Loss in iteration 138 : 0.468675418917696
Loss in iteration 139 : 0.46077903233374806
Loss in iteration 140 : 0.4736773477714385
Loss in iteration 141 : 0.47607655585165504
Loss in iteration 142 : 0.473757730027629
Loss in iteration 143 : 0.46162650041123343
Loss in iteration 144 : 0.4699210806586517
Loss in iteration 145 : 0.4702701040600195
Loss in iteration 146 : 0.4635103900864649
Loss in iteration 147 : 0.47212329001995074
Loss in iteration 148 : 0.4687276270942998
Loss in iteration 149 : 0.47454901386804615
Loss in iteration 150 : 0.47132655512692256
Loss in iteration 151 : 0.47049012070171314
Loss in iteration 152 : 0.47935712431337213
Loss in iteration 153 : 0.4705084152280218
Loss in iteration 154 : 0.47189925337830674
Loss in iteration 155 : 0.46425405171163414
Loss in iteration 156 : 0.4674574089722556
Loss in iteration 157 : 0.4628233221173874
Loss in iteration 158 : 0.458884297388873
Loss in iteration 159 : 0.4693736087549837
Loss in iteration 160 : 0.4622273966363527
Loss in iteration 161 : 0.4643246391116882
Loss in iteration 162 : 0.46822008948455435
Loss in iteration 163 : 0.4745005671428762
Loss in iteration 164 : 0.4735832530361638
Loss in iteration 165 : 0.47677352990987665
Loss in iteration 166 : 0.4710943903236224
Loss in iteration 167 : 0.481006341412674
Loss in iteration 168 : 0.4617749177948228
Loss in iteration 169 : 0.4702150741011787
Loss in iteration 170 : 0.4637668607172683
Loss in iteration 171 : 0.47203047187830355
Loss in iteration 172 : 0.4657413745050697
Loss in iteration 173 : 0.46467548597425584
Loss in iteration 174 : 0.46400433139588243
Loss in iteration 175 : 0.4677160986514821
Loss in iteration 176 : 0.4746755190339601
Loss in iteration 177 : 0.47205082645166735
Loss in iteration 178 : 0.4671159046894178
Loss in iteration 179 : 0.46889066798977563
Loss in iteration 180 : 0.47195942487260356
Loss in iteration 181 : 0.47365592245043553
Loss in iteration 182 : 0.4738069521698137
Loss in iteration 183 : 0.4722995018522471
Loss in iteration 184 : 0.46640266455040447
Loss in iteration 185 : 0.4627291926904901
Loss in iteration 186 : 0.46175461253998573
Loss in iteration 187 : 0.45665881878596365
Loss in iteration 188 : 0.47683818516330784
Loss in iteration 189 : 0.47318325111262594
Loss in iteration 190 : 0.47296776612955665
Loss in iteration 191 : 0.4715006366830641
Loss in iteration 192 : 0.47089712926350247
Loss in iteration 193 : 0.4624639899442688
Loss in iteration 194 : 0.46539704216328065
Loss in iteration 195 : 0.4691060527574808
Loss in iteration 196 : 0.4746644809906399
Loss in iteration 197 : 0.4621148834098712
Loss in iteration 198 : 0.4644836200537676
Loss in iteration 199 : 0.47018268544899766
Loss in iteration 200 : 0.46393074295582065
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.7865, training accuracy 0.7885, time elapsed: 4025 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 21.49369814880065
Loss in iteration 3 : 28.019512815476478
Loss in iteration 4 : 10.18947472357765
Loss in iteration 5 : 10.825813692782644
Loss in iteration 6 : 7.8674241181111615
Loss in iteration 7 : 7.6832158354415
Loss in iteration 8 : 6.155086565516692
Loss in iteration 9 : 5.893867616529649
Loss in iteration 10 : 5.488784524637726
Loss in iteration 11 : 4.751005344344645
Loss in iteration 12 : 4.039818815270974
Loss in iteration 13 : 3.891891422911427
Loss in iteration 14 : 3.6195368755152724
Loss in iteration 15 : 3.7549687751819283
Loss in iteration 16 : 3.1266982513622286
Loss in iteration 17 : 3.6158749793855613
Loss in iteration 18 : 2.8498465241843403
Loss in iteration 19 : 3.1519829770718966
Loss in iteration 20 : 2.777145544909849
Loss in iteration 21 : 3.0267021453153395
Loss in iteration 22 : 3.057289822358468
Loss in iteration 23 : 3.0267717180569123
Loss in iteration 24 : 2.4229309927292544
Loss in iteration 25 : 2.5220085859996875
Loss in iteration 26 : 2.1727341562482536
Loss in iteration 27 : 2.386253957936897
Loss in iteration 28 : 2.4352103393373516
Loss in iteration 29 : 2.4941312556634534
Loss in iteration 30 : 2.151272436260779
Loss in iteration 31 : 2.3481501678307697
Loss in iteration 32 : 2.2254342305346384
Loss in iteration 33 : 2.4879995049042543
Loss in iteration 34 : 2.502826105866256
Loss in iteration 35 : 2.589841391327318
Loss in iteration 36 : 2.135488441365389
Loss in iteration 37 : 2.214652009073257
Loss in iteration 38 : 2.0521777794389697
Loss in iteration 39 : 2.1593077594931662
Loss in iteration 40 : 1.9558882006549363
Loss in iteration 41 : 2.11464346650271
Loss in iteration 42 : 2.0433473640754207
Loss in iteration 43 : 2.1050321641354515
Loss in iteration 44 : 1.7819268827566015
Loss in iteration 45 : 2.0360302278212217
Loss in iteration 46 : 2.002391891707996
Loss in iteration 47 : 2.1334556064754353
Loss in iteration 48 : 1.8918604708403473
Loss in iteration 49 : 2.1014588140719983
Loss in iteration 50 : 1.8919759186041545
Loss in iteration 51 : 1.9028757528350824
Loss in iteration 52 : 1.9082602527334946
Loss in iteration 53 : 1.9876482044538233
Loss in iteration 54 : 1.751098650033142
Loss in iteration 55 : 1.9162192788171244
Loss in iteration 56 : 1.8348591604022042
Loss in iteration 57 : 2.0304021127701186
Loss in iteration 58 : 1.772378511629518
Loss in iteration 59 : 1.9388039261820083
Loss in iteration 60 : 1.653773574389543
Loss in iteration 61 : 1.8226792145184367
Loss in iteration 62 : 1.5958498483870682
Loss in iteration 63 : 1.7181804217802161
Loss in iteration 64 : 1.6134020744738253
Loss in iteration 65 : 1.7554699429391971
Loss in iteration 66 : 1.716079497390408
Loss in iteration 67 : 1.8515948060989096
Loss in iteration 68 : 1.8408468670722433
Loss in iteration 69 : 1.9057315740428293
Loss in iteration 70 : 1.584294741890099
Loss in iteration 71 : 1.8000599590288169
Loss in iteration 72 : 1.767769023159719
Loss in iteration 73 : 1.9086288959910525
Loss in iteration 74 : 1.7670049342235918
Loss in iteration 75 : 1.8994151037934812
Loss in iteration 76 : 1.6874653365793497
Loss in iteration 77 : 1.7630036925897858
Loss in iteration 78 : 1.6697039248502619
Loss in iteration 79 : 1.7145632432538636
Loss in iteration 80 : 1.565187247475826
Loss in iteration 81 : 1.6943626449297775
Loss in iteration 82 : 1.5554170263020477
Loss in iteration 83 : 1.6940287271812633
Loss in iteration 84 : 1.6237240070765493
Loss in iteration 85 : 1.7443960530860008
Loss in iteration 86 : 1.5982424256937418
Loss in iteration 87 : 1.7250470923150927
Loss in iteration 88 : 1.5262203132115906
Loss in iteration 89 : 1.5135749459782841
Loss in iteration 90 : 1.4412524437010035
Loss in iteration 91 : 1.5109362349244577
Loss in iteration 92 : 1.4849698176933053
Loss in iteration 93 : 1.488043784403122
Loss in iteration 94 : 1.4706807203668513
Loss in iteration 95 : 1.5527341061547013
Loss in iteration 96 : 1.4291779478490803
Loss in iteration 97 : 1.7163786690742728
Loss in iteration 98 : 1.4276316150885027
Loss in iteration 99 : 1.5030470253851844
Loss in iteration 100 : 1.2666311891343445
Loss in iteration 101 : 1.4346063041769832
Loss in iteration 102 : 1.3010077614255826
Loss in iteration 103 : 1.445730811725752
Loss in iteration 104 : 1.3185002805954058
Loss in iteration 105 : 1.4314416724194607
Loss in iteration 106 : 1.393196582241885
Loss in iteration 107 : 1.5248494345097898
Loss in iteration 108 : 1.3060865735960538
Loss in iteration 109 : 1.5524316651902679
Loss in iteration 110 : 1.500451268174857
Loss in iteration 111 : 1.7285593186652346
Loss in iteration 112 : 1.578674184871351
Loss in iteration 113 : 1.7569110378898178
Loss in iteration 114 : 1.6812680723551805
Loss in iteration 115 : 1.6686002687920625
Loss in iteration 116 : 1.5149799514100915
Loss in iteration 117 : 1.5953763783620425
Loss in iteration 118 : 1.4682638003378388
Loss in iteration 119 : 1.52703468880191
Loss in iteration 120 : 1.5215120076627653
Loss in iteration 121 : 1.5638765630228102
Loss in iteration 122 : 1.390894697453479
Loss in iteration 123 : 1.4343792248961815
Loss in iteration 124 : 1.3969686466760993
Loss in iteration 125 : 1.497967427141597
Loss in iteration 126 : 1.4224217360403317
Loss in iteration 127 : 1.5596602757637357
Loss in iteration 128 : 1.3037623059368915
Loss in iteration 129 : 1.3407504054986619
Loss in iteration 130 : 1.3417975715888344
Loss in iteration 131 : 1.4165826696122918
Loss in iteration 132 : 1.3040514597522213
Loss in iteration 133 : 1.330709158155336
Loss in iteration 134 : 1.2299428530020546
Loss in iteration 135 : 1.3145289394714257
Loss in iteration 136 : 1.3095375866042203
Loss in iteration 137 : 1.327991734144008
Loss in iteration 138 : 1.272161320226347
Loss in iteration 139 : 1.2966909718219848
Loss in iteration 140 : 1.3346401411442077
Loss in iteration 141 : 1.334115549908799
Loss in iteration 142 : 1.3207606887984789
Loss in iteration 143 : 1.3164687197875808
Loss in iteration 144 : 1.2781254115137572
Loss in iteration 145 : 1.2627736427032263
Loss in iteration 146 : 1.245732136232693
Loss in iteration 147 : 1.343726488408391
Loss in iteration 148 : 1.3237669447205531
Loss in iteration 149 : 1.454793642938275
Loss in iteration 150 : 1.4285686173002785
Loss in iteration 151 : 1.4464226687959552
Loss in iteration 152 : 1.3985671415752168
Loss in iteration 153 : 1.3841620017470628
Loss in iteration 154 : 1.2559999451433455
Loss in iteration 155 : 1.2932029706138124
Loss in iteration 156 : 1.2693097722355122
Loss in iteration 157 : 1.2773949334749137
Loss in iteration 158 : 1.1272908864518858
Loss in iteration 159 : 1.171231243520758
Loss in iteration 160 : 1.12245519969594
Loss in iteration 161 : 1.2035966152589703
Loss in iteration 162 : 1.106576506877312
Loss in iteration 163 : 1.160796927350532
Loss in iteration 164 : 1.1303256884224424
Loss in iteration 165 : 1.2550816597948569
Loss in iteration 166 : 1.243155976775407
Loss in iteration 167 : 1.3807639688328013
Loss in iteration 168 : 1.281567765104663
Loss in iteration 169 : 1.3726876427769779
Loss in iteration 170 : 1.265038306283944
Loss in iteration 171 : 1.3557347181313928
Loss in iteration 172 : 1.2239340844240898
Loss in iteration 173 : 1.2426474156356364
Loss in iteration 174 : 1.227559764432726
Loss in iteration 175 : 1.3270222718071754
Loss in iteration 176 : 1.2359102099380475
Loss in iteration 177 : 1.2963153332751138
Loss in iteration 178 : 1.1904680568774548
Loss in iteration 179 : 1.2665957224892612
Loss in iteration 180 : 1.129635487269583
Loss in iteration 181 : 1.1629055060437774
Loss in iteration 182 : 1.142148124602243
Loss in iteration 183 : 1.2386982942737523
Loss in iteration 184 : 1.0435146748879804
Loss in iteration 185 : 1.1116181242501744
Loss in iteration 186 : 1.03878258482607
Loss in iteration 187 : 1.1322196997679723
Loss in iteration 188 : 1.0608605594621454
Loss in iteration 189 : 1.1334548354831642
Loss in iteration 190 : 1.1352233771880547
Loss in iteration 191 : 1.2544102639192782
Loss in iteration 192 : 1.1650418841552872
Loss in iteration 193 : 1.1853489877369383
Loss in iteration 194 : 1.0255166555243553
Loss in iteration 195 : 1.111382726779416
Loss in iteration 196 : 1.0793114083821551
Loss in iteration 197 : 1.1649232565171939
Loss in iteration 198 : 1.1789650494954298
Loss in iteration 199 : 1.287763932723725
Loss in iteration 200 : 1.2420907194813569
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.732, training accuracy 0.727875, time elapsed: 4095 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.8473878562989546
Loss in iteration 3 : 3.0251901007598527
Loss in iteration 4 : 3.632531199542587
Loss in iteration 5 : 1.6484815429321875
Loss in iteration 6 : 3.3411039379875493
Loss in iteration 7 : 1.5211006599121573
Loss in iteration 8 : 3.3464890825856863
Loss in iteration 9 : 1.230166313978702
Loss in iteration 10 : 2.8018895271589264
Loss in iteration 11 : 1.3238746662122993
Loss in iteration 12 : 2.5190212589973084
Loss in iteration 13 : 1.2699838281009017
Loss in iteration 14 : 2.249719797938565
Loss in iteration 15 : 1.3776510556808905
Loss in iteration 16 : 2.013806601701109
Loss in iteration 17 : 1.3890802005168408
Loss in iteration 18 : 1.8128287895081003
Loss in iteration 19 : 1.4570498838738235
Loss in iteration 20 : 1.7667874125854874
Loss in iteration 21 : 1.2951661804495758
Loss in iteration 22 : 1.5789901763957377
Loss in iteration 23 : 1.1955008916081065
Loss in iteration 24 : 1.2333486832593596
Loss in iteration 25 : 1.153777594925833
Loss in iteration 26 : 1.1796624178143678
Loss in iteration 27 : 1.1405943951782442
Loss in iteration 28 : 1.1882166964041487
Loss in iteration 29 : 1.056852473513586
Loss in iteration 30 : 1.0549901452978256
Loss in iteration 31 : 1.027131736696938
Loss in iteration 32 : 1.0324035738246058
Loss in iteration 33 : 1.0059038459679093
Loss in iteration 34 : 1.0701237348994133
Loss in iteration 35 : 0.986689129035218
Loss in iteration 36 : 0.8959902348975958
Loss in iteration 37 : 0.9150190632514624
Loss in iteration 38 : 0.8790833157906598
Loss in iteration 39 : 0.8943659494387658
Loss in iteration 40 : 0.8448568553274884
Loss in iteration 41 : 0.8811780940825731
Loss in iteration 42 : 0.8609003654492285
Loss in iteration 43 : 0.8467042345668234
Loss in iteration 44 : 0.7781783905968386
Loss in iteration 45 : 0.8375506829268021
Loss in iteration 46 : 0.8348464244656341
Loss in iteration 47 : 0.8746919979591562
Loss in iteration 48 : 0.8061130641962186
Loss in iteration 49 : 0.8720761850860957
Loss in iteration 50 : 0.8156767245317178
Loss in iteration 51 : 0.800961317356117
Loss in iteration 52 : 0.8159200813715048
Loss in iteration 53 : 0.8364730243602115
Loss in iteration 54 : 0.7402632271556996
Loss in iteration 55 : 0.7720866230468714
Loss in iteration 56 : 0.7528526364876765
Loss in iteration 57 : 0.7738162841820944
Loss in iteration 58 : 0.7391558953862594
Loss in iteration 59 : 0.7745485734731418
Loss in iteration 60 : 0.7533107056224015
Loss in iteration 61 : 0.7927919670618025
Loss in iteration 62 : 0.755998198130226
Loss in iteration 63 : 0.7721102648007908
Loss in iteration 64 : 0.7053380082259532
Loss in iteration 65 : 0.7375841888723467
Loss in iteration 66 : 0.7721391696835372
Loss in iteration 67 : 0.8007782720537222
Loss in iteration 68 : 0.7724265565830755
Loss in iteration 69 : 0.7608177587146436
Loss in iteration 70 : 0.6705604776408026
Loss in iteration 71 : 0.7031375363345164
Loss in iteration 72 : 0.6866471254276718
Loss in iteration 73 : 0.7379908231774153
Loss in iteration 74 : 0.7048274798887759
Loss in iteration 75 : 0.736090227032931
Loss in iteration 76 : 0.6986189517187944
Loss in iteration 77 : 0.7189612265620133
Loss in iteration 78 : 0.6692553666609675
Loss in iteration 79 : 0.6845986449541042
Loss in iteration 80 : 0.6222816945614684
Loss in iteration 81 : 0.6606218428616154
Loss in iteration 82 : 0.6102857639778996
Loss in iteration 83 : 0.6554640294802461
Loss in iteration 84 : 0.6169871593669526
Loss in iteration 85 : 0.6449155380734477
Loss in iteration 86 : 0.6051504319378406
Loss in iteration 87 : 0.6238650361983681
Loss in iteration 88 : 0.6223774053389586
Loss in iteration 89 : 0.6282579019292734
Loss in iteration 90 : 0.6518507840752491
Loss in iteration 91 : 0.6884838227679213
Loss in iteration 92 : 0.7151556775460864
Loss in iteration 93 : 0.703345108519293
Loss in iteration 94 : 0.6886221973965764
Loss in iteration 95 : 0.7023268677721886
Loss in iteration 96 : 0.7162751350357305
Loss in iteration 97 : 0.7804178594000087
Loss in iteration 98 : 0.7257725492017476
Loss in iteration 99 : 0.7080559725123824
Loss in iteration 100 : 0.6660568813041116
Loss in iteration 101 : 0.7018307291394703
Loss in iteration 102 : 0.6812862560540516
Loss in iteration 103 : 0.6992910195624232
Loss in iteration 104 : 0.6811651920453212
Loss in iteration 105 : 0.6822307816906116
Loss in iteration 106 : 0.6846172288000812
Loss in iteration 107 : 0.6976419003285114
Loss in iteration 108 : 0.6546708953370579
Loss in iteration 109 : 0.7120214751687214
Loss in iteration 110 : 0.7086475438923839
Loss in iteration 111 : 0.7425919216009977
Loss in iteration 112 : 0.7154934408214573
Loss in iteration 113 : 0.7522357413594823
Loss in iteration 114 : 0.732046438196144
Loss in iteration 115 : 0.7141152475259215
Loss in iteration 116 : 0.6863928196567546
Loss in iteration 117 : 0.697364309687264
Loss in iteration 118 : 0.6785181488231351
Loss in iteration 119 : 0.6810063462492143
Loss in iteration 120 : 0.6989827119777442
Loss in iteration 121 : 0.7054454549225216
Loss in iteration 122 : 0.6644089229578104
Loss in iteration 123 : 0.659050163064067
Loss in iteration 124 : 0.6698408128224571
Loss in iteration 125 : 0.6928166463026525
Loss in iteration 126 : 0.6942417363364354
Loss in iteration 127 : 0.7311263213002587
Loss in iteration 128 : 0.6585644414418427
Loss in iteration 129 : 0.6481908072564289
Loss in iteration 130 : 0.6490679351154292
Loss in iteration 131 : 0.6585106974368907
Loss in iteration 132 : 0.6258302328475114
Loss in iteration 133 : 0.6211538018431857
Loss in iteration 134 : 0.6060712421075918
Loss in iteration 135 : 0.6141273683622429
Loss in iteration 136 : 0.6314267993270596
Loss in iteration 137 : 0.6190383016011388
Loss in iteration 138 : 0.6188309531016105
Loss in iteration 139 : 0.6117577373690433
Loss in iteration 140 : 0.6462273527996809
Loss in iteration 141 : 0.6424568484478432
Loss in iteration 142 : 0.6503734521538334
Loss in iteration 143 : 0.6372818677578402
Loss in iteration 144 : 0.6423812951319948
Loss in iteration 145 : 0.632042452266407
Loss in iteration 146 : 0.636301613229986
Loss in iteration 147 : 0.6577831096695023
Loss in iteration 148 : 0.6604232086880464
Loss in iteration 149 : 0.6932522884608847
Loss in iteration 150 : 0.6975887580475347
Loss in iteration 151 : 0.6856180897704293
Loss in iteration 152 : 0.696538291226675
Loss in iteration 153 : 0.6689438878613948
Loss in iteration 154 : 0.6524553742650845
Loss in iteration 155 : 0.6530704957778783
Loss in iteration 156 : 0.6603293387197573
Loss in iteration 157 : 0.6437945833872737
Loss in iteration 158 : 0.6083450731878829
Loss in iteration 159 : 0.617590521419385
Loss in iteration 160 : 0.6105280715140476
Loss in iteration 161 : 0.6228830508591859
Loss in iteration 162 : 0.5975323290536408
Loss in iteration 163 : 0.5994027069896013
Loss in iteration 164 : 0.5855449150969526
Loss in iteration 165 : 0.6031966806208916
Loss in iteration 166 : 0.5995296218101946
Loss in iteration 167 : 0.63944441002117
Loss in iteration 168 : 0.6217057891243157
Loss in iteration 169 : 0.6422273295533292
Loss in iteration 170 : 0.6194476398176961
Loss in iteration 171 : 0.6479866650674374
Loss in iteration 172 : 0.6318275050066404
Loss in iteration 173 : 0.6281628186356107
Loss in iteration 174 : 0.6368432483229479
Loss in iteration 175 : 0.6582559983679304
Loss in iteration 176 : 0.6475710661116579
Loss in iteration 177 : 0.6591578278999434
Loss in iteration 178 : 0.6459564035994113
Loss in iteration 179 : 0.6515408535665138
Loss in iteration 180 : 0.6148033467110612
Loss in iteration 181 : 0.6172940217209086
Loss in iteration 182 : 0.6219035447682157
Loss in iteration 183 : 0.6327484041941304
Loss in iteration 184 : 0.5901975108159782
Loss in iteration 185 : 0.587121944036096
Loss in iteration 186 : 0.5758848042150905
Loss in iteration 187 : 0.585945189549598
Loss in iteration 188 : 0.5938434263954968
Loss in iteration 189 : 0.5986770853176698
Loss in iteration 190 : 0.6116098765891741
Loss in iteration 191 : 0.6369746481753704
Loss in iteration 192 : 0.6396637597051597
Loss in iteration 193 : 0.617504448486731
Loss in iteration 194 : 0.5819831243910742
Loss in iteration 195 : 0.5929748591608649
Loss in iteration 196 : 0.5904481662997814
Loss in iteration 197 : 0.594292750477566
Loss in iteration 198 : 0.5950392648839079
Loss in iteration 199 : 0.6205711923615699
Loss in iteration 200 : 0.6122961651505394
Testing accuracy  of updater 3 on alg 0 with rate 7.0 = 0.749, training accuracy 0.7415, time elapsed: 3878 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6914702074211885
Loss in iteration 3 : 0.7393941684674962
Loss in iteration 4 : 0.8837709986618277
Loss in iteration 5 : 0.9069644252748318
Loss in iteration 6 : 1.0299553648356212
Loss in iteration 7 : 0.8364339555841679
Loss in iteration 8 : 0.9476875691166297
Loss in iteration 9 : 0.7992440719366241
Loss in iteration 10 : 0.884422022173411
Loss in iteration 11 : 0.72157262773795
Loss in iteration 12 : 0.7602869394743537
Loss in iteration 13 : 0.6796400881774098
Loss in iteration 14 : 0.7163269125600223
Loss in iteration 15 : 0.660734861284557
Loss in iteration 16 : 0.6664132596708021
Loss in iteration 17 : 0.633031025441937
Loss in iteration 18 : 0.6412259241011531
Loss in iteration 19 : 0.6246718668550971
Loss in iteration 20 : 0.6373356125138726
Loss in iteration 21 : 0.6015407185067327
Loss in iteration 22 : 0.614402905256007
Loss in iteration 23 : 0.5716001027278982
Loss in iteration 24 : 0.5619628774811618
Loss in iteration 25 : 0.534035360740884
Loss in iteration 26 : 0.5308191225869541
Loss in iteration 27 : 0.5214981398375177
Loss in iteration 28 : 0.5324516282674999
Loss in iteration 29 : 0.5147163298463708
Loss in iteration 30 : 0.5133690749781216
Loss in iteration 31 : 0.504584480235401
Loss in iteration 32 : 0.5049562618772192
Loss in iteration 33 : 0.5148308015750702
Loss in iteration 34 : 0.5139425828579215
Loss in iteration 35 : 0.5072088436462195
Loss in iteration 36 : 0.5061276563105449
Loss in iteration 37 : 0.5053826101661882
Loss in iteration 38 : 0.4992712171346499
Loss in iteration 39 : 0.49476818097741293
Loss in iteration 40 : 0.48859880509790016
Loss in iteration 41 : 0.4921869211716446
Loss in iteration 42 : 0.4914837041122135
Loss in iteration 43 : 0.4930720158176965
Loss in iteration 44 : 0.4858978761316309
Loss in iteration 45 : 0.49908159753401576
Loss in iteration 46 : 0.4862562456593985
Loss in iteration 47 : 0.49188843525121473
Loss in iteration 48 : 0.4903044404368435
Loss in iteration 49 : 0.49731624029142146
Loss in iteration 50 : 0.49470553569119263
Loss in iteration 51 : 0.4828998026328295
Loss in iteration 52 : 0.49488517946540045
Loss in iteration 53 : 0.49158416244619896
Loss in iteration 54 : 0.48411204978013844
Loss in iteration 55 : 0.484782505714675
Loss in iteration 56 : 0.4877202995307294
Loss in iteration 57 : 0.4861505022156012
Loss in iteration 58 : 0.49307517925140115
Loss in iteration 59 : 0.4943957255632654
Loss in iteration 60 : 0.4896432672942814
Loss in iteration 61 : 0.48554671398470933
Loss in iteration 62 : 0.48381363761835855
Loss in iteration 63 : 0.4884192485600486
Loss in iteration 64 : 0.48341115164987425
Loss in iteration 65 : 0.4792747981977587
Loss in iteration 66 : 0.4910085999560458
Loss in iteration 67 : 0.4787381231193363
Loss in iteration 68 : 0.49296662687381965
Loss in iteration 69 : 0.4859042547459487
Loss in iteration 70 : 0.477718862995385
Loss in iteration 71 : 0.48829289442921425
Loss in iteration 72 : 0.4864726598293821
Loss in iteration 73 : 0.4935922554012385
Loss in iteration 74 : 0.48825335728541314
Loss in iteration 75 : 0.49109737877341203
Loss in iteration 76 : 0.4796603670024005
Loss in iteration 77 : 0.47586624950799616
Loss in iteration 78 : 0.4733673463509718
Loss in iteration 79 : 0.4825832573201256
Loss in iteration 80 : 0.48060996257666005
Loss in iteration 81 : 0.48605449039051185
Loss in iteration 82 : 0.4768863025760725
Loss in iteration 83 : 0.48451138403975985
Loss in iteration 84 : 0.47304870818306133
Loss in iteration 85 : 0.4818239985211505
Loss in iteration 86 : 0.47919892585276996
Loss in iteration 87 : 0.4764135058480471
Loss in iteration 88 : 0.48104022908230526
Loss in iteration 89 : 0.4762822191158053
Loss in iteration 90 : 0.48100103868486244
Loss in iteration 91 : 0.47760310544710105
Loss in iteration 92 : 0.47950121870004475
Loss in iteration 93 : 0.4823955493763254
Loss in iteration 94 : 0.47761522407692963
Loss in iteration 95 : 0.480921984998165
Loss in iteration 96 : 0.48198090530133975
Loss in iteration 97 : 0.48584758230186326
Loss in iteration 98 : 0.47879020688533913
Loss in iteration 99 : 0.471127606467432
Loss in iteration 100 : 0.47162982881437815
Loss in iteration 101 : 0.4747237213730656
Loss in iteration 102 : 0.47733481449858395
Loss in iteration 103 : 0.48350903390304667
Loss in iteration 104 : 0.48075377455791724
Loss in iteration 105 : 0.46836689865487663
Loss in iteration 106 : 0.48400932539089053
Loss in iteration 107 : 0.47905017160768365
Loss in iteration 108 : 0.46894724405615174
Loss in iteration 109 : 0.4831956670598747
Loss in iteration 110 : 0.47448076870425615
Loss in iteration 111 : 0.4699701345952865
Loss in iteration 112 : 0.477645306051132
Loss in iteration 113 : 0.475207337795534
Loss in iteration 114 : 0.48172121567266923
Loss in iteration 115 : 0.4757990796461278
Loss in iteration 116 : 0.4708880542347668
Loss in iteration 117 : 0.4739925503934579
Loss in iteration 118 : 0.47568133760662157
Loss in iteration 119 : 0.47651898196376585
Loss in iteration 120 : 0.4790034311965688
Loss in iteration 121 : 0.4718839614233493
Loss in iteration 122 : 0.47958275470360584
Loss in iteration 123 : 0.47665324064214526
Loss in iteration 124 : 0.47024174351129366
Loss in iteration 125 : 0.4744111697332662
Loss in iteration 126 : 0.4778975416564905
Loss in iteration 127 : 0.4841874541409177
Loss in iteration 128 : 0.4698053138668552
Loss in iteration 129 : 0.4766850980983664
Loss in iteration 130 : 0.4730800105395134
Loss in iteration 131 : 0.47671751603245704
Loss in iteration 132 : 0.4718111050512765
Loss in iteration 133 : 0.47502652550171304
Loss in iteration 134 : 0.4722199016744222
Loss in iteration 135 : 0.46998948405155083
Loss in iteration 136 : 0.476426982805512
Loss in iteration 137 : 0.46706819576718533
Loss in iteration 138 : 0.47173774694386006
Loss in iteration 139 : 0.462736249267808
Loss in iteration 140 : 0.47510298094706416
Loss in iteration 141 : 0.47725292801237845
Loss in iteration 142 : 0.4750680108322319
Loss in iteration 143 : 0.46323468021574826
Loss in iteration 144 : 0.47111465732549485
Loss in iteration 145 : 0.471376042107705
Loss in iteration 146 : 0.4659164145355129
Loss in iteration 147 : 0.4739722725011255
Loss in iteration 148 : 0.4705267800223707
Loss in iteration 149 : 0.47648643554758696
Loss in iteration 150 : 0.47363101687883075
Loss in iteration 151 : 0.4718083196389442
Loss in iteration 152 : 0.4807618141635124
Loss in iteration 153 : 0.4714358391791774
Loss in iteration 154 : 0.47280024670532134
Loss in iteration 155 : 0.46646692595618566
Loss in iteration 156 : 0.4691266737497581
Loss in iteration 157 : 0.4642033730447237
Loss in iteration 158 : 0.4599590466883352
Loss in iteration 159 : 0.4698814761587097
Loss in iteration 160 : 0.4630030942612374
Loss in iteration 161 : 0.4651447577331616
Loss in iteration 162 : 0.46912466983237816
Loss in iteration 163 : 0.47806059899712766
Loss in iteration 164 : 0.47691748095540165
Loss in iteration 165 : 0.4788308564881449
Loss in iteration 166 : 0.47154257281323103
Loss in iteration 167 : 0.4815301895343711
Loss in iteration 168 : 0.46365551140012545
Loss in iteration 169 : 0.4708264385065768
Loss in iteration 170 : 0.4647872905433221
Loss in iteration 171 : 0.4728326247440049
Loss in iteration 172 : 0.46678989290796724
Loss in iteration 173 : 0.46560453104672433
Loss in iteration 174 : 0.46474861531025524
Loss in iteration 175 : 0.46861781215969484
Loss in iteration 176 : 0.4756359791907307
Loss in iteration 177 : 0.47307354405296076
Loss in iteration 178 : 0.46967777877559974
Loss in iteration 179 : 0.4709550918041693
Loss in iteration 180 : 0.4734050551686779
Loss in iteration 181 : 0.4745652805076111
Loss in iteration 182 : 0.4747525081028922
Loss in iteration 183 : 0.47377458474673173
Loss in iteration 184 : 0.4679308693321511
Loss in iteration 185 : 0.4635298894536169
Loss in iteration 186 : 0.4627376009297674
Loss in iteration 187 : 0.4585246891701759
Loss in iteration 188 : 0.47753495989160977
Loss in iteration 189 : 0.47487491105859025
Loss in iteration 190 : 0.47476834958408215
Loss in iteration 191 : 0.4730315001548688
Loss in iteration 192 : 0.47456689549573156
Loss in iteration 193 : 0.46386909065038484
Loss in iteration 194 : 0.4668458893693523
Loss in iteration 195 : 0.4705515243182069
Loss in iteration 196 : 0.47518519755851124
Loss in iteration 197 : 0.46366747241762407
Loss in iteration 198 : 0.46566493813194
Loss in iteration 199 : 0.47110659019740597
Loss in iteration 200 : 0.464783441829796
Testing accuracy  of updater 3 on alg 0 with rate 4.0 = 0.781, training accuracy 0.78725, time elapsed: 4062 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6830454316525673
Loss in iteration 3 : 0.6741956047247659
Loss in iteration 4 : 0.669881932634058
Loss in iteration 5 : 0.6661969035916472
Loss in iteration 6 : 0.659391600571104
Loss in iteration 7 : 0.6542124585112905
Loss in iteration 8 : 0.6502398607868402
Loss in iteration 9 : 0.6454532904131269
Loss in iteration 10 : 0.6428425809293836
Loss in iteration 11 : 0.6390859406425202
Loss in iteration 12 : 0.6332732367719943
Loss in iteration 13 : 0.6314097900592182
Loss in iteration 14 : 0.6278590708028472
Loss in iteration 15 : 0.627467956806076
Loss in iteration 16 : 0.6204692200126686
Loss in iteration 17 : 0.620743617138428
Loss in iteration 18 : 0.6172440497336203
Loss in iteration 19 : 0.6139670453262729
Loss in iteration 20 : 0.6126006442916763
Loss in iteration 21 : 0.6088232887271644
Loss in iteration 22 : 0.6078115284069687
Loss in iteration 23 : 0.6037981604871918
Loss in iteration 24 : 0.6030268470190738
Loss in iteration 25 : 0.5967653219925553
Loss in iteration 26 : 0.5963313137353656
Loss in iteration 27 : 0.5897794635080365
Loss in iteration 28 : 0.5939683735595873
Loss in iteration 29 : 0.5877270160882129
Loss in iteration 30 : 0.5869500650565453
Loss in iteration 31 : 0.5816346242764179
Loss in iteration 32 : 0.582602209617992
Loss in iteration 33 : 0.583840732188797
Loss in iteration 34 : 0.5825421231393332
Loss in iteration 35 : 0.5788743984016563
Loss in iteration 36 : 0.5777086844904166
Loss in iteration 37 : 0.5784010953530405
Loss in iteration 38 : 0.5754439565653799
Loss in iteration 39 : 0.5723505396838265
Loss in iteration 40 : 0.5688300864689599
Loss in iteration 41 : 0.5685408315426566
Loss in iteration 42 : 0.5646660271579442
Loss in iteration 43 : 0.5666899758227179
Loss in iteration 44 : 0.561899752111731
Loss in iteration 45 : 0.5678312773702664
Loss in iteration 46 : 0.5591272439596314
Loss in iteration 47 : 0.5627600639955734
Loss in iteration 48 : 0.5605979690133575
Loss in iteration 49 : 0.5627857759418777
Loss in iteration 50 : 0.5601162311830611
Loss in iteration 51 : 0.5531461574985002
Loss in iteration 52 : 0.5608532576546126
Loss in iteration 53 : 0.5557697318222501
Loss in iteration 54 : 0.551874911717708
Loss in iteration 55 : 0.5514285159272138
Loss in iteration 56 : 0.55184354668462
Loss in iteration 57 : 0.5488194653898693
Loss in iteration 58 : 0.5530521755885409
Loss in iteration 59 : 0.5528110013507989
Loss in iteration 60 : 0.5494753528732244
Loss in iteration 61 : 0.5452859756850534
Loss in iteration 62 : 0.5460184195449093
Loss in iteration 63 : 0.5475632845281706
Loss in iteration 64 : 0.5423242054342159
Loss in iteration 65 : 0.5406670681645489
Loss in iteration 66 : 0.5474121238640011
Loss in iteration 67 : 0.5382602865024754
Loss in iteration 68 : 0.543653491514525
Loss in iteration 69 : 0.5398391045137172
Loss in iteration 70 : 0.5363546109883619
Loss in iteration 71 : 0.5431676729887296
Loss in iteration 72 : 0.5399787630765005
Loss in iteration 73 : 0.5425333528779361
Loss in iteration 74 : 0.5396412786317636
Loss in iteration 75 : 0.5390345481832256
Loss in iteration 76 : 0.5330990787011815
Loss in iteration 77 : 0.531991803830582
Loss in iteration 78 : 0.5284880261585067
Loss in iteration 79 : 0.5330835142786116
Loss in iteration 80 : 0.5323438063405856
Loss in iteration 81 : 0.5356590573415115
Loss in iteration 82 : 0.5301351230595582
Loss in iteration 83 : 0.5342692697147144
Loss in iteration 84 : 0.5266002716663573
Loss in iteration 85 : 0.5321759237616445
Loss in iteration 86 : 0.5289794956905512
Loss in iteration 87 : 0.5245799197768346
Loss in iteration 88 : 0.5284220613566748
Loss in iteration 89 : 0.5254459020135391
Loss in iteration 90 : 0.5274840651082985
Loss in iteration 91 : 0.5262323557568365
Loss in iteration 92 : 0.5251413088863077
Loss in iteration 93 : 0.527624973100489
Loss in iteration 94 : 0.5236342901749671
Loss in iteration 95 : 0.5256854528895328
Loss in iteration 96 : 0.5265984437903917
Loss in iteration 97 : 0.5265236243608237
Loss in iteration 98 : 0.5237116407251498
Loss in iteration 99 : 0.5198083033163298
Loss in iteration 100 : 0.5170512429925371
Loss in iteration 101 : 0.5196463348307886
Loss in iteration 102 : 0.520806851769902
Loss in iteration 103 : 0.5242463618023391
Loss in iteration 104 : 0.5227031148357493
Loss in iteration 105 : 0.5148943273274812
Loss in iteration 106 : 0.5242196364121409
Loss in iteration 107 : 0.5210132063265213
Loss in iteration 108 : 0.5151543517676792
Loss in iteration 109 : 0.5235866534952104
Loss in iteration 110 : 0.5179833616008948
Loss in iteration 111 : 0.5132042059647762
Loss in iteration 112 : 0.5181192017317116
Loss in iteration 113 : 0.5152033013589961
Loss in iteration 114 : 0.5179163300032101
Loss in iteration 115 : 0.5175103325953929
Loss in iteration 116 : 0.5148219288554926
Loss in iteration 117 : 0.5148814061591288
Loss in iteration 118 : 0.5160107067028649
Loss in iteration 119 : 0.5157010854529536
Loss in iteration 120 : 0.518136643804577
Loss in iteration 121 : 0.5102419592408969
Loss in iteration 122 : 0.5161984527222199
Loss in iteration 123 : 0.5151728236925652
Loss in iteration 124 : 0.5094485512267918
Loss in iteration 125 : 0.5102656204488877
Loss in iteration 126 : 0.5153364758053111
Loss in iteration 127 : 0.5185626407248655
Loss in iteration 128 : 0.5080493953606603
Loss in iteration 129 : 0.5139937060270044
Loss in iteration 130 : 0.5096691007331902
Loss in iteration 131 : 0.5118204035422121
Loss in iteration 132 : 0.508462723380324
Loss in iteration 133 : 0.5106345218360673
Loss in iteration 134 : 0.5086359447989215
Loss in iteration 135 : 0.5049702092577193
Loss in iteration 136 : 0.5125931243152778
Loss in iteration 137 : 0.5048972572112269
Loss in iteration 138 : 0.5064098376333906
Loss in iteration 139 : 0.5026898722483084
Loss in iteration 140 : 0.5088391281279206
Loss in iteration 141 : 0.5112146347783096
Loss in iteration 142 : 0.508196676308586
Loss in iteration 143 : 0.5018901036269185
Loss in iteration 144 : 0.5060186191260513
Loss in iteration 145 : 0.505696776397324
Loss in iteration 146 : 0.5026891565798548
Loss in iteration 147 : 0.5073829469995572
Loss in iteration 148 : 0.50576989237737
Loss in iteration 149 : 0.5086961949105355
Loss in iteration 150 : 0.5065885777156884
Loss in iteration 151 : 0.5066049484989286
Loss in iteration 152 : 0.5111645524033774
Loss in iteration 153 : 0.5045963325673315
Loss in iteration 154 : 0.5056521538107674
Loss in iteration 155 : 0.4993051640083245
Loss in iteration 156 : 0.5006774428754421
Loss in iteration 157 : 0.49922060510280536
Loss in iteration 158 : 0.49585675522104317
Loss in iteration 159 : 0.5022250497705908
Loss in iteration 160 : 0.4980217407777929
Loss in iteration 161 : 0.49853591219369475
Loss in iteration 162 : 0.5030090956223516
Loss in iteration 163 : 0.507922848853
Loss in iteration 164 : 0.5064364344721362
Loss in iteration 165 : 0.5052914518107239
Loss in iteration 166 : 0.5020172227936656
Loss in iteration 167 : 0.509162321208921
Loss in iteration 168 : 0.4976895638931093
Loss in iteration 169 : 0.5015289053679253
Loss in iteration 170 : 0.49797524848370783
Loss in iteration 171 : 0.5033447999849906
Loss in iteration 172 : 0.49793881761939857
Loss in iteration 173 : 0.49721807349675246
Loss in iteration 174 : 0.4973730852866554
Loss in iteration 175 : 0.4982953967401308
Loss in iteration 176 : 0.5031242263093293
Loss in iteration 177 : 0.5010282692618089
Loss in iteration 178 : 0.49631464117362056
Loss in iteration 179 : 0.49731113772898605
Loss in iteration 180 : 0.5007874362666289
Loss in iteration 181 : 0.5033178285459047
Loss in iteration 182 : 0.5014748084764533
Loss in iteration 183 : 0.5009508155644243
Loss in iteration 184 : 0.49832331114364004
Loss in iteration 185 : 0.49471897833814754
Loss in iteration 186 : 0.4938156677878646
Loss in iteration 187 : 0.489127184460994
Loss in iteration 188 : 0.5030631202148582
Loss in iteration 189 : 0.5030815811631776
Loss in iteration 190 : 0.49956859623316363
Loss in iteration 191 : 0.500909219666602
Loss in iteration 192 : 0.4991721935468363
Loss in iteration 193 : 0.49218441532800233
Loss in iteration 194 : 0.49402365951571453
Loss in iteration 195 : 0.4988358490137593
Loss in iteration 196 : 0.5029667967429746
Loss in iteration 197 : 0.491747901472835
Loss in iteration 198 : 0.49321575198106066
Loss in iteration 199 : 0.4969105780051697
Loss in iteration 200 : 0.4933121203088005
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.778, training accuracy 0.777, time elapsed: 4040 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.685411753846097
Loss in iteration 3 : 0.6784288998394769
Loss in iteration 4 : 0.6750412996258883
Loss in iteration 5 : 0.6723414839497234
Loss in iteration 6 : 0.6669036663667696
Loss in iteration 7 : 0.6629334061704314
Loss in iteration 8 : 0.6597476167281332
Loss in iteration 9 : 0.6558107635379676
Loss in iteration 10 : 0.6537483546972911
Loss in iteration 11 : 0.651400485102083
Loss in iteration 12 : 0.6462159062367906
Loss in iteration 13 : 0.6448187079204987
Loss in iteration 14 : 0.6423631195371483
Loss in iteration 15 : 0.6417043219564129
Loss in iteration 16 : 0.6357504241958994
Loss in iteration 17 : 0.636021260654991
Loss in iteration 18 : 0.6333381526657345
Loss in iteration 19 : 0.6299733969648097
Loss in iteration 20 : 0.6293514594265668
Loss in iteration 21 : 0.626184702932133
Loss in iteration 22 : 0.6250208497057179
Loss in iteration 23 : 0.6216978934364084
Loss in iteration 24 : 0.6204106411234868
Loss in iteration 25 : 0.6156135828603736
Loss in iteration 26 : 0.61493523273632
Loss in iteration 27 : 0.6091243906353614
Loss in iteration 28 : 0.6125667692925377
Loss in iteration 29 : 0.6073587595281441
Loss in iteration 30 : 0.6063908972989993
Loss in iteration 31 : 0.6018236356661277
Loss in iteration 32 : 0.6029843879876792
Loss in iteration 33 : 0.6030338361437515
Loss in iteration 34 : 0.6020818894502085
Loss in iteration 35 : 0.5987863541851715
Loss in iteration 36 : 0.5976406173869743
Loss in iteration 37 : 0.5984555165992952
Loss in iteration 38 : 0.5958703431897465
Loss in iteration 39 : 0.5933824322059139
Loss in iteration 40 : 0.5902628518792699
Loss in iteration 41 : 0.5892577612843839
Loss in iteration 42 : 0.5855824266540407
Loss in iteration 43 : 0.5875872677050037
Loss in iteration 44 : 0.5829524998503753
Loss in iteration 45 : 0.5881352088772669
Loss in iteration 46 : 0.580627349258554
Loss in iteration 47 : 0.5836416250693552
Loss in iteration 48 : 0.5815926826184061
Loss in iteration 49 : 0.5829680196284698
Loss in iteration 50 : 0.5804047160171811
Loss in iteration 51 : 0.5744201479553636
Loss in iteration 52 : 0.5812717660485929
Loss in iteration 53 : 0.5763182337370563
Loss in iteration 54 : 0.5728541652673131
Loss in iteration 55 : 0.5723075188366546
Loss in iteration 56 : 0.5724383169354148
Loss in iteration 57 : 0.569982557943591
Loss in iteration 58 : 0.5734173696318836
Loss in iteration 59 : 0.5728344307767443
Loss in iteration 60 : 0.5695119838126667
Loss in iteration 61 : 0.5656500090460798
Loss in iteration 62 : 0.566743181483998
Loss in iteration 63 : 0.5676922304076132
Loss in iteration 64 : 0.5627520816711823
Loss in iteration 65 : 0.5611766098757495
Loss in iteration 66 : 0.5673490658357352
Loss in iteration 67 : 0.5590458679409281
Loss in iteration 68 : 0.5627957575069013
Loss in iteration 69 : 0.5596140772022448
Loss in iteration 70 : 0.5569652163851034
Loss in iteration 71 : 0.5632198955726303
Loss in iteration 72 : 0.5597226457875321
Loss in iteration 73 : 0.5610542073260715
Loss in iteration 74 : 0.5589963451951093
Loss in iteration 75 : 0.5579608451445096
Loss in iteration 76 : 0.5528220083162174
Loss in iteration 77 : 0.5521579787881195
Loss in iteration 78 : 0.548843682201255
Loss in iteration 79 : 0.5526084557613818
Loss in iteration 80 : 0.5518854903664159
Loss in iteration 81 : 0.5546553308995317
Loss in iteration 82 : 0.5501444581439959
Loss in iteration 83 : 0.5531495427028393
Loss in iteration 84 : 0.5464737277936369
Loss in iteration 85 : 0.5512577643106356
Loss in iteration 86 : 0.5481397936389955
Loss in iteration 87 : 0.5438253368274975
Loss in iteration 88 : 0.5474706572798009
Loss in iteration 89 : 0.5446606940329854
Loss in iteration 90 : 0.5459866762741235
Loss in iteration 91 : 0.5447427113467455
Loss in iteration 92 : 0.5436430998805377
Loss in iteration 93 : 0.5458414868890896
Loss in iteration 94 : 0.5421051170062363
Loss in iteration 95 : 0.5438219013678292
Loss in iteration 96 : 0.5446871727489586
Loss in iteration 97 : 0.5438704508248158
Loss in iteration 98 : 0.5418904692916772
Loss in iteration 99 : 0.53873320327285
Loss in iteration 100 : 0.5356211308611625
Loss in iteration 101 : 0.5378901450215668
Loss in iteration 102 : 0.5387554454065302
Loss in iteration 103 : 0.5413774952945378
Loss in iteration 104 : 0.5400990988955425
Loss in iteration 105 : 0.5334589252475709
Loss in iteration 106 : 0.5410950439396213
Loss in iteration 107 : 0.5382749738181053
Loss in iteration 108 : 0.5334599445587347
Loss in iteration 109 : 0.5401294509161718
Loss in iteration 110 : 0.535732755583298
Loss in iteration 111 : 0.5313530026711376
Loss in iteration 112 : 0.5355152044716284
Loss in iteration 113 : 0.532476756800625
Loss in iteration 114 : 0.5343551842720741
Loss in iteration 115 : 0.5347886091173463
Loss in iteration 116 : 0.5324325787239406
Loss in iteration 117 : 0.5320788309096788
Loss in iteration 118 : 0.5328731105051643
Loss in iteration 119 : 0.5322491853481331
Loss in iteration 120 : 0.534521000854408
Loss in iteration 121 : 0.527205119101447
Loss in iteration 122 : 0.5323034015134248
Loss in iteration 123 : 0.5317721507443696
Loss in iteration 124 : 0.5262920561721611
Loss in iteration 125 : 0.5267191982207541
Loss in iteration 126 : 0.5317452344256862
Loss in iteration 127 : 0.5336993463649653
Loss in iteration 128 : 0.5245537231218508
Loss in iteration 129 : 0.5300120841933832
Loss in iteration 130 : 0.5258542176117295
Loss in iteration 131 : 0.5273696065482407
Loss in iteration 132 : 0.5244788151478362
Loss in iteration 133 : 0.5264696587737362
Loss in iteration 134 : 0.5245925450234502
Loss in iteration 135 : 0.5209139275251491
Loss in iteration 136 : 0.5282238155528954
Loss in iteration 137 : 0.5214689092628896
Loss in iteration 138 : 0.5221914947205445
Loss in iteration 139 : 0.5193588498974856
Loss in iteration 140 : 0.5241367967399496
Loss in iteration 141 : 0.5264504923028273
Loss in iteration 142 : 0.5230612327257457
Loss in iteration 143 : 0.5180389748246587
Loss in iteration 144 : 0.5213627925113867
Loss in iteration 145 : 0.5210506913979207
Loss in iteration 146 : 0.5185613700339117
Loss in iteration 147 : 0.5224883347814807
Loss in iteration 148 : 0.5211174423944498
Loss in iteration 149 : 0.5235673554742688
Loss in iteration 150 : 0.5217589626419791
Loss in iteration 151 : 0.5218212722922952
Loss in iteration 152 : 0.5251849311283827
Loss in iteration 153 : 0.5193318007211035
Loss in iteration 154 : 0.5203108940033383
Loss in iteration 155 : 0.5145236662309397
Loss in iteration 156 : 0.5153878199095102
Loss in iteration 157 : 0.5147240701883623
Loss in iteration 158 : 0.5115988643018794
Loss in iteration 159 : 0.5167561439390956
Loss in iteration 160 : 0.5132054919912648
Loss in iteration 161 : 0.5134309103773197
Loss in iteration 162 : 0.5178181929302011
Loss in iteration 163 : 0.5220737740815815
Loss in iteration 164 : 0.5203209542352338
Loss in iteration 165 : 0.5186909363338861
Loss in iteration 166 : 0.5162600782745109
Loss in iteration 167 : 0.5220800281568824
Loss in iteration 168 : 0.5126022032405764
Loss in iteration 169 : 0.5154597725776842
Loss in iteration 170 : 0.5125596963060405
Loss in iteration 171 : 0.5167973560691258
Loss in iteration 172 : 0.5120455010272669
Loss in iteration 173 : 0.5114644701052217
Loss in iteration 174 : 0.5121857124275215
Loss in iteration 175 : 0.5119901890554088
Loss in iteration 176 : 0.5164701601467753
Loss in iteration 177 : 0.5143717137293776
Loss in iteration 178 : 0.509812910419961
Loss in iteration 179 : 0.5103543552341009
Loss in iteration 180 : 0.5136898917906603
Loss in iteration 181 : 0.5165724529073231
Loss in iteration 182 : 0.5144932105185194
Loss in iteration 183 : 0.513948596642192
Loss in iteration 184 : 0.5117731816212507
Loss in iteration 185 : 0.5088338180353137
Loss in iteration 186 : 0.5074576258786555
Loss in iteration 187 : 0.5032925955395304
Loss in iteration 188 : 0.5153854053156619
Loss in iteration 189 : 0.5158377380126646
Loss in iteration 190 : 0.5122503102950804
Loss in iteration 191 : 0.5136646977424204
Loss in iteration 192 : 0.511383949195084
Loss in iteration 193 : 0.5058596832771447
Loss in iteration 194 : 0.5070224839511844
Loss in iteration 195 : 0.5116986451776162
Loss in iteration 196 : 0.515559361408027
Loss in iteration 197 : 0.5050745105504999
Loss in iteration 198 : 0.5064381349894064
Loss in iteration 199 : 0.5094850231588015
Loss in iteration 200 : 0.5064978294926505
Testing accuracy  of updater 3 on alg 0 with rate 0.7 = 0.776, training accuracy 0.774375, time elapsed: 4372 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6883568611346683
Loss in iteration 3 : 0.6836222238068644
Loss in iteration 4 : 0.6810777930016955
Loss in iteration 5 : 0.6792591270863813
Loss in iteration 6 : 0.6754809921261362
Loss in iteration 7 : 0.6728435484432835
Loss in iteration 8 : 0.6705831150724029
Loss in iteration 9 : 0.6676444795000204
Loss in iteration 10 : 0.6662556474011523
Loss in iteration 11 : 0.6653492665390701
Loss in iteration 12 : 0.6610462901001499
Loss in iteration 13 : 0.6603985126347444
Loss in iteration 14 : 0.6591770825365368
Loss in iteration 15 : 0.6585920066858917
Loss in iteration 16 : 0.653981897278082
Loss in iteration 17 : 0.6544126552933386
Loss in iteration 18 : 0.6528853473171079
Loss in iteration 19 : 0.6495879774676712
Loss in iteration 20 : 0.6499150925358278
Loss in iteration 21 : 0.6476028792003965
Loss in iteration 22 : 0.6466166418689386
Loss in iteration 23 : 0.6443051285415003
Loss in iteration 24 : 0.642628305289359
Loss in iteration 25 : 0.6398428394258118
Loss in iteration 26 : 0.6389220421898505
Loss in iteration 27 : 0.6342478859885282
Loss in iteration 28 : 0.6368963347637586
Loss in iteration 29 : 0.633212864738249
Loss in iteration 30 : 0.6321538057117176
Loss in iteration 31 : 0.6287890053408002
Loss in iteration 32 : 0.6303345257661166
Loss in iteration 33 : 0.6290451655188359
Loss in iteration 34 : 0.6287873890153204
Loss in iteration 35 : 0.6261312187219428
Loss in iteration 36 : 0.6252952867689143
Loss in iteration 37 : 0.6264594200976269
Loss in iteration 38 : 0.6242262643259034
Loss in iteration 39 : 0.6229017241081312
Loss in iteration 40 : 0.6203140418953849
Loss in iteration 41 : 0.6185660163334726
Loss in iteration 42 : 0.615622758835081
Loss in iteration 43 : 0.6176288148591523
Loss in iteration 44 : 0.6131757279489759
Loss in iteration 45 : 0.6176444760324995
Loss in iteration 46 : 0.611860453637286
Loss in iteration 47 : 0.6140773229241128
Loss in iteration 48 : 0.6125589481563748
Loss in iteration 49 : 0.612808011538829
Loss in iteration 50 : 0.6104916526492784
Loss in iteration 51 : 0.6060211500567377
Loss in iteration 52 : 0.6119593873953948
Loss in iteration 53 : 0.6072334376700714
Loss in iteration 54 : 0.6043606747907614
Loss in iteration 55 : 0.6038497042990555
Loss in iteration 56 : 0.6037839309220707
Loss in iteration 57 : 0.6024793817940226
Loss in iteration 58 : 0.6048123519774007
Loss in iteration 59 : 0.6037855037380221
Loss in iteration 60 : 0.6007058603940859
Loss in iteration 61 : 0.5975522824565129
Loss in iteration 62 : 0.5989866655798624
Loss in iteration 63 : 0.5992338119334103
Loss in iteration 64 : 0.5951192267405966
Loss in iteration 65 : 0.5933512473086586
Loss in iteration 66 : 0.5990129246925016
Loss in iteration 67 : 0.5918374667810342
Loss in iteration 68 : 0.5937514268502794
Loss in iteration 69 : 0.5915722753368932
Loss in iteration 70 : 0.589903662683938
Loss in iteration 71 : 0.5955199273818583
Loss in iteration 72 : 0.5917907285034759
Loss in iteration 73 : 0.5916088436290186
Loss in iteration 74 : 0.5908260261782091
Loss in iteration 75 : 0.5892500207832769
Loss in iteration 76 : 0.5850761021803466
Loss in iteration 77 : 0.5851785906807496
Loss in iteration 78 : 0.5822607577318523
Loss in iteration 79 : 0.5851436033513112
Loss in iteration 80 : 0.5843489704319876
Loss in iteration 81 : 0.5863994308014574
Loss in iteration 82 : 0.5832924323168932
Loss in iteration 83 : 0.5846768960316631
Loss in iteration 84 : 0.5796082963741596
Loss in iteration 85 : 0.5833471373835255
Loss in iteration 86 : 0.5804514882564633
Loss in iteration 87 : 0.5764660320334296
Loss in iteration 88 : 0.5797628273074811
Loss in iteration 89 : 0.5772672771282936
Loss in iteration 90 : 0.5776101811987915
Loss in iteration 91 : 0.5764750817964536
Loss in iteration 92 : 0.5755349911254577
Loss in iteration 93 : 0.5774082969825031
Loss in iteration 94 : 0.5739294679916587
Loss in iteration 95 : 0.5752868933478256
Loss in iteration 96 : 0.5761094820427078
Loss in iteration 97 : 0.5745059322730195
Loss in iteration 98 : 0.573522802816543
Loss in iteration 99 : 0.5715726345951905
Loss in iteration 100 : 0.5680365782720443
Loss in iteration 101 : 0.569950383304359
Loss in iteration 102 : 0.5704971823074292
Loss in iteration 103 : 0.571916021654749
Loss in iteration 104 : 0.5709170985296674
Loss in iteration 105 : 0.5659313571518476
Loss in iteration 106 : 0.5712862091570479
Loss in iteration 107 : 0.5689794541234003
Loss in iteration 108 : 0.565663287484255
Loss in iteration 109 : 0.5698031496226159
Loss in iteration 110 : 0.5674093118989498
Loss in iteration 111 : 0.5636359130453411
Loss in iteration 112 : 0.566721516364969
Loss in iteration 113 : 0.5636322561847972
Loss in iteration 114 : 0.5644208385868036
Loss in iteration 115 : 0.5657039831886254
Loss in iteration 116 : 0.5638861087433024
Loss in iteration 117 : 0.5629910848926969
Loss in iteration 118 : 0.5634434213884958
Loss in iteration 119 : 0.5624735047714359
Loss in iteration 120 : 0.5644921271198347
Loss in iteration 121 : 0.5583646523759933
Loss in iteration 122 : 0.5621438039781974
Loss in iteration 123 : 0.5621687016954917
Loss in iteration 124 : 0.5572540097836635
Loss in iteration 125 : 0.5571036137239072
Loss in iteration 126 : 0.5620283604788828
Loss in iteration 127 : 0.5621967515604761
Loss in iteration 128 : 0.5552082190412467
Loss in iteration 129 : 0.5597918475178998
Loss in iteration 130 : 0.5560061199975731
Loss in iteration 131 : 0.5567763378779401
Loss in iteration 132 : 0.5544758176842831
Loss in iteration 133 : 0.556054422807873
Loss in iteration 134 : 0.5541764507332548
Loss in iteration 135 : 0.5509242326026073
Loss in iteration 136 : 0.5573461493555301
Loss in iteration 137 : 0.5524279550092778
Loss in iteration 138 : 0.551920631780108
Loss in iteration 139 : 0.5501248423038634
Loss in iteration 140 : 0.5532295809945268
Loss in iteration 141 : 0.5553609884877957
Loss in iteration 142 : 0.551490082395333
Loss in iteration 143 : 0.5481199060228246
Loss in iteration 144 : 0.5505114714011585
Loss in iteration 145 : 0.5502032677116234
Loss in iteration 146 : 0.5483484824588222
Loss in iteration 147 : 0.5512072984428763
Loss in iteration 148 : 0.5502689392407467
Loss in iteration 149 : 0.5519712651872344
Loss in iteration 150 : 0.5507169403214593
Loss in iteration 151 : 0.5507144186594385
Loss in iteration 152 : 0.5524510612805923
Loss in iteration 153 : 0.5477341883379665
Loss in iteration 154 : 0.5485934670595397
Loss in iteration 155 : 0.5437871574267472
Loss in iteration 156 : 0.5440232359376722
Loss in iteration 157 : 0.5442824317593705
Loss in iteration 158 : 0.5416751164320932
Loss in iteration 159 : 0.5448992276120486
Loss in iteration 160 : 0.5423518105271568
Loss in iteration 161 : 0.5422151610764806
Loss in iteration 162 : 0.5460535728700064
Loss in iteration 163 : 0.5497252569298063
Loss in iteration 164 : 0.5473515637729109
Loss in iteration 165 : 0.5453954980483104
Loss in iteration 166 : 0.5441677577255852
Loss in iteration 167 : 0.5478282071459604
Loss in iteration 168 : 0.5413153823127155
Loss in iteration 169 : 0.5426682817166115
Loss in iteration 170 : 0.5409269302035993
Loss in iteration 171 : 0.5432027773088027
Loss in iteration 172 : 0.5397412722030085
Loss in iteration 173 : 0.5391886294889836
Loss in iteration 174 : 0.5408088603107833
Loss in iteration 175 : 0.5390375716600667
Loss in iteration 176 : 0.543178866646952
Loss in iteration 177 : 0.5409877772950377
Loss in iteration 178 : 0.5369034602202135
Loss in iteration 179 : 0.536509242272555
Loss in iteration 180 : 0.5396301314088088
Loss in iteration 181 : 0.5428322086900811
Loss in iteration 182 : 0.5405974140622362
Loss in iteration 183 : 0.5399661104166873
Loss in iteration 184 : 0.5383873771145157
Loss in iteration 185 : 0.5366077010697162
Loss in iteration 186 : 0.5344600725932365
Loss in iteration 187 : 0.5310728665112452
Loss in iteration 188 : 0.5404378362906016
Loss in iteration 189 : 0.5411565857406233
Loss in iteration 190 : 0.5380485992546574
Loss in iteration 191 : 0.5391398251024497
Loss in iteration 192 : 0.5361768358761377
Loss in iteration 193 : 0.5330034570909788
Loss in iteration 194 : 0.533122835794151
Loss in iteration 195 : 0.5373373403121504
Loss in iteration 196 : 0.5408221645287585
Loss in iteration 197 : 0.5318622646885901
Loss in iteration 198 : 0.5329230052507345
Loss in iteration 199 : 0.5351315836692663
Loss in iteration 200 : 0.532913988039703
Testing accuracy  of updater 3 on alg 0 with rate 0.4 = 0.7715, training accuracy 0.772125, time elapsed: 3948 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6918619078591464
Loss in iteration 3 : 0.6903986092888126
Loss in iteration 4 : 0.6893920977715858
Loss in iteration 5 : 0.6886275590663576
Loss in iteration 6 : 0.6871971016937466
Loss in iteration 7 : 0.686161008276475
Loss in iteration 8 : 0.6851689215891847
Loss in iteration 9 : 0.6838636559736089
Loss in iteration 10 : 0.6831795293390543
Loss in iteration 11 : 0.6830901067007331
Loss in iteration 12 : 0.6808216087069588
Loss in iteration 13 : 0.6806944104465823
Loss in iteration 14 : 0.6804491226377708
Loss in iteration 15 : 0.6803237705843865
Loss in iteration 16 : 0.6781298009933467
Loss in iteration 17 : 0.6784633519980936
Loss in iteration 18 : 0.6781814760014337
Loss in iteration 19 : 0.6759801102608655
Loss in iteration 20 : 0.6768070412913509
Loss in iteration 21 : 0.6757103464087096
Loss in iteration 22 : 0.6752388283212373
Loss in iteration 23 : 0.6748326987563604
Loss in iteration 24 : 0.6733908864933748
Loss in iteration 25 : 0.6731195417689807
Loss in iteration 26 : 0.671977205169106
Loss in iteration 27 : 0.6694143697532124
Loss in iteration 28 : 0.6710257727477563
Loss in iteration 29 : 0.6698382452991609
Loss in iteration 30 : 0.6690903109461845
Loss in iteration 31 : 0.6673192559969598
Loss in iteration 32 : 0.6689896260893191
Loss in iteration 33 : 0.6669327142400348
Loss in iteration 34 : 0.6678710328013266
Loss in iteration 35 : 0.666730760815556
Loss in iteration 36 : 0.6666656844981597
Loss in iteration 37 : 0.6683173674753593
Loss in iteration 38 : 0.6664287204511417
Loss in iteration 39 : 0.6672635175161289
Loss in iteration 40 : 0.6655154343409374
Loss in iteration 41 : 0.6638433985087173
Loss in iteration 42 : 0.6622385065995574
Loss in iteration 43 : 0.6643035498052412
Loss in iteration 44 : 0.660681207010059
Loss in iteration 45 : 0.664298588923838
Loss in iteration 46 : 0.6614119249587772
Loss in iteration 47 : 0.6627385959983447
Loss in iteration 48 : 0.6625895873176094
Loss in iteration 49 : 0.6613739162147271
Loss in iteration 50 : 0.6595901897270965
Loss in iteration 51 : 0.6578669610457671
Loss in iteration 52 : 0.6628777662549588
Loss in iteration 53 : 0.6589220352984637
Loss in iteration 54 : 0.6569296655332354
Loss in iteration 55 : 0.656888167309957
Loss in iteration 56 : 0.6571495210576277
Loss in iteration 57 : 0.6576372797751772
Loss in iteration 58 : 0.6590052385838472
Loss in iteration 59 : 0.6573647766430131
Loss in iteration 60 : 0.6558288170735108
Loss in iteration 61 : 0.6542947575392856
Loss in iteration 62 : 0.6557039112420417
Loss in iteration 63 : 0.6555994465793713
Loss in iteration 64 : 0.6535577748141759
Loss in iteration 65 : 0.6512355125635206
Loss in iteration 66 : 0.6563087337311533
Loss in iteration 67 : 0.6509378020676578
Loss in iteration 68 : 0.6514322732272113
Loss in iteration 69 : 0.6510562080108121
Loss in iteration 70 : 0.6507907442319539
Loss in iteration 71 : 0.655383785324779
Loss in iteration 72 : 0.6521013965605005
Loss in iteration 73 : 0.6504704717467933
Loss in iteration 74 : 0.651720416307612
Loss in iteration 75 : 0.6496410602979256
Loss in iteration 76 : 0.6472216348320725
Loss in iteration 77 : 0.6486457399912388
Loss in iteration 78 : 0.6464179390227205
Loss in iteration 79 : 0.6489621707341345
Loss in iteration 80 : 0.6481979186639567
Loss in iteration 81 : 0.6487928522196704
Loss in iteration 82 : 0.6480602476662504
Loss in iteration 83 : 0.6475821550143444
Loss in iteration 84 : 0.6454883916170501
Loss in iteration 85 : 0.6475487239978749
Loss in iteration 86 : 0.6453783389094981
Loss in iteration 87 : 0.642799955501679
Loss in iteration 88 : 0.6452374039581744
Loss in iteration 89 : 0.6438997057243515
Loss in iteration 90 : 0.6429103319831652
Loss in iteration 91 : 0.6424115046499267
Loss in iteration 92 : 0.6423303831198411
Loss in iteration 93 : 0.6438818240474924
Loss in iteration 94 : 0.6405109668897603
Loss in iteration 95 : 0.6422322625815332
Loss in iteration 96 : 0.6428663213930654
Loss in iteration 97 : 0.6408267281909397
Loss in iteration 98 : 0.6408940065267037
Loss in iteration 99 : 0.6414839855839475
Loss in iteration 100 : 0.6380972748126933
Loss in iteration 101 : 0.63942723131284
Loss in iteration 102 : 0.6397563617826062
Loss in iteration 103 : 0.6396379210240206
Loss in iteration 104 : 0.6389327685778083
Loss in iteration 105 : 0.6367533233833403
Loss in iteration 106 : 0.6390119183781398
Loss in iteration 107 : 0.637343052661045
Loss in iteration 108 : 0.6366970063527927
Loss in iteration 109 : 0.636901876670584
Loss in iteration 110 : 0.6386234780578953
Loss in iteration 111 : 0.6360720225533737
Loss in iteration 112 : 0.6374883652576435
Loss in iteration 113 : 0.6349103483834114
Loss in iteration 114 : 0.6345460349037965
Loss in iteration 115 : 0.6358997493794922
Loss in iteration 116 : 0.6355501097923758
Loss in iteration 117 : 0.6334811122299835
Loss in iteration 118 : 0.634461808546337
Loss in iteration 119 : 0.6333062204176056
Loss in iteration 120 : 0.6353997443382701
Loss in iteration 121 : 0.6319820996744249
Loss in iteration 122 : 0.6332573249297354
Loss in iteration 123 : 0.6341330115128482
Loss in iteration 124 : 0.6308790631265219
Loss in iteration 125 : 0.6299051224030957
Loss in iteration 126 : 0.6344286950667521
Loss in iteration 127 : 0.6326008004184145
Loss in iteration 128 : 0.6294584714817464
Loss in iteration 129 : 0.6325046551666275
Loss in iteration 130 : 0.6293646670538005
Loss in iteration 131 : 0.6302900519109629
Loss in iteration 132 : 0.6284987342353949
Loss in iteration 133 : 0.6291735690245296
Loss in iteration 134 : 0.6269750307470285
Loss in iteration 135 : 0.6257168712197904
Loss in iteration 136 : 0.629600937300019
Loss in iteration 137 : 0.6291687509936866
Loss in iteration 138 : 0.6261499025898384
Loss in iteration 139 : 0.6258037368267181
Loss in iteration 140 : 0.6270728796430118
Loss in iteration 141 : 0.6291318880569903
Loss in iteration 142 : 0.625115179606585
Loss in iteration 143 : 0.6236015529673796
Loss in iteration 144 : 0.6253178172145566
Loss in iteration 145 : 0.6248230025969149
Loss in iteration 146 : 0.6240789689430578
Loss in iteration 147 : 0.6254141222199113
Loss in iteration 148 : 0.6259760216994019
Loss in iteration 149 : 0.6255759008615062
Loss in iteration 150 : 0.6256813756412773
Loss in iteration 151 : 0.6256039300925628
Loss in iteration 152 : 0.6249874981142077
Loss in iteration 153 : 0.6231186487403318
Loss in iteration 154 : 0.6237964984468802
Loss in iteration 155 : 0.62065278414703
Loss in iteration 156 : 0.6210062580509507
Loss in iteration 157 : 0.6215905438416868
Loss in iteration 158 : 0.6205181273591945
Loss in iteration 159 : 0.620515838741143
Loss in iteration 160 : 0.6201534206791447
Loss in iteration 161 : 0.6196639813181323
Loss in iteration 162 : 0.6205011518449673
Loss in iteration 163 : 0.6249986123065787
Loss in iteration 164 : 0.6211979688586312
Loss in iteration 165 : 0.6201144997797585
Loss in iteration 166 : 0.6203128594304321
Loss in iteration 167 : 0.6201621382709218
Loss in iteration 168 : 0.6191968531361808
Loss in iteration 169 : 0.617759047491427
Loss in iteration 170 : 0.6184877472478619
Loss in iteration 171 : 0.6171335606827573
Loss in iteration 172 : 0.6167843511535755
Loss in iteration 173 : 0.6155436210398219
Loss in iteration 174 : 0.6185402994785352
Loss in iteration 175 : 0.6153343991107861
Loss in iteration 176 : 0.6192459508641216
Loss in iteration 177 : 0.6163457056147541
Loss in iteration 178 : 0.6142876726650212
Loss in iteration 179 : 0.6124084885816367
Loss in iteration 180 : 0.6149134808735216
Loss in iteration 181 : 0.6173756913039564
Loss in iteration 182 : 0.6155863498875918
Loss in iteration 183 : 0.615296895558969
Loss in iteration 184 : 0.6148781300008215
Loss in iteration 185 : 0.6150183929693072
Loss in iteration 186 : 0.6120319580170057
Loss in iteration 187 : 0.6098951002056515
Loss in iteration 188 : 0.6148600952959665
Loss in iteration 189 : 0.6147779650680639
Loss in iteration 190 : 0.6143769427054282
Loss in iteration 191 : 0.6141010661032081
Loss in iteration 192 : 0.6105527131177516
Loss in iteration 193 : 0.6117081705899622
Loss in iteration 194 : 0.6100231603201954
Loss in iteration 195 : 0.6128486840052405
Loss in iteration 196 : 0.6154017166147573
Loss in iteration 197 : 0.6108804769443443
Loss in iteration 198 : 0.6107823428184852
Loss in iteration 199 : 0.6119279589213129
Loss in iteration 200 : 0.6108669588549279
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.7425, training accuracy 0.74875, time elapsed: 4025 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.692954415660519
Loss in iteration 3 : 0.6927134963987032
Loss in iteration 4 : 0.692523095794886
Loss in iteration 5 : 0.6923455354187664
Loss in iteration 6 : 0.6921216694434115
Loss in iteration 7 : 0.6918865755017567
Loss in iteration 8 : 0.6916468334493496
Loss in iteration 9 : 0.6913409841999592
Loss in iteration 10 : 0.6911699977835887
Loss in iteration 11 : 0.6910998932353517
Loss in iteration 12 : 0.6905747285637485
Loss in iteration 13 : 0.6904901262275003
Loss in iteration 14 : 0.6903743550492953
Loss in iteration 15 : 0.690310302304313
Loss in iteration 16 : 0.689806463256547
Loss in iteration 17 : 0.6899069296808503
Loss in iteration 18 : 0.6896688976433597
Loss in iteration 19 : 0.6890710372875193
Loss in iteration 20 : 0.6892362936799045
Loss in iteration 21 : 0.6888117175152009
Loss in iteration 22 : 0.6887006592618843
Loss in iteration 23 : 0.6886442248161144
Loss in iteration 24 : 0.6880380642530143
Loss in iteration 25 : 0.6881967489006112
Loss in iteration 26 : 0.6875872874376608
Loss in iteration 27 : 0.6867817857287847
Loss in iteration 28 : 0.6872354582639569
Loss in iteration 29 : 0.6869142857890587
Loss in iteration 30 : 0.6867542545457949
Loss in iteration 31 : 0.6860850094038604
Loss in iteration 32 : 0.6863243776619262
Loss in iteration 33 : 0.6855634357555872
Loss in iteration 34 : 0.6860530403303732
Loss in iteration 35 : 0.6858173235597693
Loss in iteration 36 : 0.6854738346376515
Loss in iteration 37 : 0.6861668711429727
Loss in iteration 38 : 0.685358320393502
Loss in iteration 39 : 0.6855984786956603
Loss in iteration 40 : 0.6852109543777539
Loss in iteration 41 : 0.6846730663473309
Loss in iteration 42 : 0.6838751925922859
Loss in iteration 43 : 0.6848140061066534
Loss in iteration 44 : 0.6833706927310454
Loss in iteration 45 : 0.6844238279258036
Loss in iteration 46 : 0.6836049473296173
Loss in iteration 47 : 0.6838840618100708
Loss in iteration 48 : 0.6841418489596726
Loss in iteration 49 : 0.6833423515938891
Loss in iteration 50 : 0.6822570849897197
Loss in iteration 51 : 0.6820410809464507
Loss in iteration 52 : 0.6842172403635418
Loss in iteration 53 : 0.6824610636130277
Loss in iteration 54 : 0.6814335935960143
Loss in iteration 55 : 0.681496265657095
Loss in iteration 56 : 0.6816584272795216
Loss in iteration 57 : 0.6814044548699174
Loss in iteration 58 : 0.6823142482767517
Loss in iteration 59 : 0.6818166439067789
Loss in iteration 60 : 0.680984571327607
Loss in iteration 61 : 0.6804299540116885
Loss in iteration 62 : 0.6806588082776646
Loss in iteration 63 : 0.6810059076383056
Loss in iteration 64 : 0.6798089601646751
Loss in iteration 65 : 0.6786934721217232
Loss in iteration 66 : 0.6812692904962416
Loss in iteration 67 : 0.6786282044899703
Loss in iteration 68 : 0.6789186893388243
Loss in iteration 69 : 0.6789550517114518
Loss in iteration 70 : 0.6787557362498237
Loss in iteration 71 : 0.6808884493413297
Loss in iteration 72 : 0.6794239541283359
Loss in iteration 73 : 0.6786152869511552
Loss in iteration 74 : 0.6792640097908187
Loss in iteration 75 : 0.678063131292884
Loss in iteration 76 : 0.6775461117949462
Loss in iteration 77 : 0.6781283652027353
Loss in iteration 78 : 0.6766907491829647
Loss in iteration 79 : 0.6782379624016732
Loss in iteration 80 : 0.6776193295328654
Loss in iteration 81 : 0.6776972753818514
Loss in iteration 82 : 0.6773557613648032
Loss in iteration 83 : 0.6769748744780512
Loss in iteration 84 : 0.675924312165286
Loss in iteration 85 : 0.6772908846710026
Loss in iteration 86 : 0.6757809063258772
Loss in iteration 87 : 0.6749744219115691
Loss in iteration 88 : 0.67601480271449
Loss in iteration 89 : 0.675646293112358
Loss in iteration 90 : 0.6744101227227716
Loss in iteration 91 : 0.6744567095708259
Loss in iteration 92 : 0.6745414373605534
Loss in iteration 93 : 0.6752314855747859
Loss in iteration 94 : 0.6734759837070062
Loss in iteration 95 : 0.6745101817448461
Loss in iteration 96 : 0.6745331278123725
Loss in iteration 97 : 0.6738027902430818
Loss in iteration 98 : 0.6737669629575883
Loss in iteration 99 : 0.6743777506444127
Loss in iteration 100 : 0.6722784137692199
Loss in iteration 101 : 0.6733694449586813
Loss in iteration 102 : 0.6731121565405521
Loss in iteration 103 : 0.6734136350357758
Loss in iteration 104 : 0.6727738778926896
Loss in iteration 105 : 0.6718771121349132
Loss in iteration 106 : 0.6732421991393129
Loss in iteration 107 : 0.6721677338723379
Loss in iteration 108 : 0.6713905854082072
Loss in iteration 109 : 0.6715185252493262
Loss in iteration 110 : 0.6729158469874029
Loss in iteration 111 : 0.6713837988523518
Loss in iteration 112 : 0.6719477970102387
Loss in iteration 113 : 0.6706368320117776
Loss in iteration 114 : 0.670109245424604
Loss in iteration 115 : 0.6706213127118003
Loss in iteration 116 : 0.6709175313620888
Loss in iteration 117 : 0.6698958158812931
Loss in iteration 118 : 0.6699510089080012
Loss in iteration 119 : 0.6694416343555145
Loss in iteration 120 : 0.6712451589260533
Loss in iteration 121 : 0.6690761871716152
Loss in iteration 122 : 0.6700555742631813
Loss in iteration 123 : 0.6706273357948886
Loss in iteration 124 : 0.6686718133851148
Loss in iteration 125 : 0.6682278689377027
Loss in iteration 126 : 0.6704673764411873
Loss in iteration 127 : 0.6691975530081467
Loss in iteration 128 : 0.6675086804082131
Loss in iteration 129 : 0.6689740917958047
Loss in iteration 130 : 0.6672363585655108
Loss in iteration 131 : 0.6685671867080794
Loss in iteration 132 : 0.666224556026815
Loss in iteration 133 : 0.6674589690227553
Loss in iteration 134 : 0.6657901826537936
Loss in iteration 135 : 0.6655242208632174
Loss in iteration 136 : 0.6674135142211685
Loss in iteration 137 : 0.6677434385493181
Loss in iteration 138 : 0.6651925196940215
Loss in iteration 139 : 0.6660466112817626
Loss in iteration 140 : 0.6658852474079276
Loss in iteration 141 : 0.6681548550692326
Loss in iteration 142 : 0.6654469281457099
Loss in iteration 143 : 0.6639040221704651
Loss in iteration 144 : 0.6652330975164438
Loss in iteration 145 : 0.6641597007569381
Loss in iteration 146 : 0.6649920837938817
Loss in iteration 147 : 0.6655410378185769
Loss in iteration 148 : 0.6660876763155313
Loss in iteration 149 : 0.6647996070340219
Loss in iteration 150 : 0.6651731423845115
Loss in iteration 151 : 0.6654523794303495
Loss in iteration 152 : 0.6643060982785727
Loss in iteration 153 : 0.6641407082625759
Loss in iteration 154 : 0.6646133933149727
Loss in iteration 155 : 0.6626030214529978
Loss in iteration 156 : 0.6626971996014777
Loss in iteration 157 : 0.6634448131879315
Loss in iteration 158 : 0.6624733777586596
Loss in iteration 159 : 0.6621274482667638
Loss in iteration 160 : 0.6616120184454752
Loss in iteration 161 : 0.6619756065026701
Loss in iteration 162 : 0.6623729175143266
Loss in iteration 163 : 0.6650991563988393
Loss in iteration 164 : 0.6617219644637925
Loss in iteration 165 : 0.6621637697629604
Loss in iteration 166 : 0.6624473959923729
Loss in iteration 167 : 0.6609477889145687
Loss in iteration 168 : 0.6614633676128009
Loss in iteration 169 : 0.6601729706600771
Loss in iteration 170 : 0.661011104768248
Loss in iteration 171 : 0.6595563660802913
Loss in iteration 172 : 0.6603329381052991
Loss in iteration 173 : 0.6586281552288722
Loss in iteration 174 : 0.660319162516109
Loss in iteration 175 : 0.6585619824769439
Loss in iteration 176 : 0.6616700948390346
Loss in iteration 177 : 0.6589459474672856
Loss in iteration 178 : 0.6580609643273624
Loss in iteration 179 : 0.6573943295267652
Loss in iteration 180 : 0.6584059769392654
Loss in iteration 181 : 0.6592770366513977
Loss in iteration 182 : 0.6583673488944718
Loss in iteration 183 : 0.6589572967198181
Loss in iteration 184 : 0.6573712755651815
Loss in iteration 185 : 0.6587859450298591
Loss in iteration 186 : 0.6567822460751354
Loss in iteration 187 : 0.6553766976342923
Loss in iteration 188 : 0.6578602337204229
Loss in iteration 189 : 0.657672962740154
Loss in iteration 190 : 0.6575353124075957
Loss in iteration 191 : 0.6573975035774434
Loss in iteration 192 : 0.6548825985343616
Loss in iteration 193 : 0.6565150221975925
Loss in iteration 194 : 0.6549944829341455
Loss in iteration 195 : 0.6570131850173415
Loss in iteration 196 : 0.6589547357083592
Loss in iteration 197 : 0.6562908359928611
Loss in iteration 198 : 0.6555151258113502
Loss in iteration 199 : 0.6563159438510385
Loss in iteration 200 : 0.6560198182292187
Testing accuracy  of updater 4 on alg 0 with rate 100.0 = 0.6495, training accuracy 0.66525, time elapsed: 3982 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.692954415660519
Loss in iteration 3 : 0.6927134963987032
Loss in iteration 4 : 0.692523095794886
Loss in iteration 5 : 0.6923455354187665
Loss in iteration 6 : 0.6921216694434115
Loss in iteration 7 : 0.6918865755017567
Loss in iteration 8 : 0.6916468334493497
Loss in iteration 9 : 0.6913409841999592
Loss in iteration 10 : 0.6911699977835886
Loss in iteration 11 : 0.6910998932353517
Loss in iteration 12 : 0.6905747285637485
Loss in iteration 13 : 0.6904901262275003
Loss in iteration 14 : 0.6903743550492953
Loss in iteration 15 : 0.690310302304313
Loss in iteration 16 : 0.689806463256547
Loss in iteration 17 : 0.6899069296808502
Loss in iteration 18 : 0.6896688976433597
Loss in iteration 19 : 0.6890710372875193
Loss in iteration 20 : 0.6892362936799045
Loss in iteration 21 : 0.6888117175152009
Loss in iteration 22 : 0.6887006592618843
Loss in iteration 23 : 0.6886442248161144
Loss in iteration 24 : 0.6880380642530143
Loss in iteration 25 : 0.6881967489006112
Loss in iteration 26 : 0.6875872874376608
Loss in iteration 27 : 0.6867817857287847
Loss in iteration 28 : 0.6872354582639569
Loss in iteration 29 : 0.6869142857890588
Loss in iteration 30 : 0.6867542545457949
Loss in iteration 31 : 0.6860850094038603
Loss in iteration 32 : 0.6863243776619261
Loss in iteration 33 : 0.6855634357555874
Loss in iteration 34 : 0.6860530403303732
Loss in iteration 35 : 0.6858173235597692
Loss in iteration 36 : 0.6854738346376515
Loss in iteration 37 : 0.6861668711429725
Loss in iteration 38 : 0.6853583203935021
Loss in iteration 39 : 0.6855984786956603
Loss in iteration 40 : 0.6852109543777539
Loss in iteration 41 : 0.6846730663473308
Loss in iteration 42 : 0.6838751925922858
Loss in iteration 43 : 0.6848140061066534
Loss in iteration 44 : 0.6833706927310454
Loss in iteration 45 : 0.6844238279258036
Loss in iteration 46 : 0.6836049473296173
Loss in iteration 47 : 0.6838840618100707
Loss in iteration 48 : 0.6841418489596727
Loss in iteration 49 : 0.6833423515938891
Loss in iteration 50 : 0.6822570849897197
Loss in iteration 51 : 0.6820410809464507
Loss in iteration 52 : 0.6842172403635419
Loss in iteration 53 : 0.6824610636130277
Loss in iteration 54 : 0.6814335935960143
Loss in iteration 55 : 0.681496265657095
Loss in iteration 56 : 0.6816584272795218
Loss in iteration 57 : 0.6814044548699174
Loss in iteration 58 : 0.6823142482767517
Loss in iteration 59 : 0.6818166439067789
Loss in iteration 60 : 0.680984571327607
Loss in iteration 61 : 0.6804299540116885
Loss in iteration 62 : 0.6806588082776647
Loss in iteration 63 : 0.6810059076383057
Loss in iteration 64 : 0.6798089601646751
Loss in iteration 65 : 0.6786934721217232
Loss in iteration 66 : 0.6812692904962416
Loss in iteration 67 : 0.6786282044899703
Loss in iteration 68 : 0.6789186893388243
Loss in iteration 69 : 0.6789550517114518
Loss in iteration 70 : 0.6787557362498238
Loss in iteration 71 : 0.6808884493413296
Loss in iteration 72 : 0.6794239541283359
Loss in iteration 73 : 0.6786152869511552
Loss in iteration 74 : 0.6792640097908187
Loss in iteration 75 : 0.6780631312928841
Loss in iteration 76 : 0.6775461117949462
Loss in iteration 77 : 0.6781283652027355
Loss in iteration 78 : 0.6766907491829647
Loss in iteration 79 : 0.6782379624016731
Loss in iteration 80 : 0.6776193295328654
Loss in iteration 81 : 0.6776972753818513
Loss in iteration 82 : 0.6773557613648034
Loss in iteration 83 : 0.6769748744780513
Loss in iteration 84 : 0.6759243121652859
Loss in iteration 85 : 0.6772908846710026
Loss in iteration 86 : 0.6757809063258772
Loss in iteration 87 : 0.6749744219115692
Loss in iteration 88 : 0.6760148027144901
Loss in iteration 89 : 0.675646293112358
Loss in iteration 90 : 0.6744101227227716
Loss in iteration 91 : 0.6744567095708259
Loss in iteration 92 : 0.6745414373605535
Loss in iteration 93 : 0.6752314855747859
Loss in iteration 94 : 0.6734759837070062
Loss in iteration 95 : 0.6745101817448462
Loss in iteration 96 : 0.6745331278123725
Loss in iteration 97 : 0.6738027902430819
Loss in iteration 98 : 0.6737669629575883
Loss in iteration 99 : 0.6743777506444127
Loss in iteration 100 : 0.6722784137692199
Loss in iteration 101 : 0.6733694449586813
Loss in iteration 102 : 0.673112156540552
Loss in iteration 103 : 0.6734136350357758
Loss in iteration 104 : 0.6727738778926896
Loss in iteration 105 : 0.6718771121349132
Loss in iteration 106 : 0.6732421991393129
Loss in iteration 107 : 0.6721677338723379
Loss in iteration 108 : 0.6713905854082072
Loss in iteration 109 : 0.6715185252493263
Loss in iteration 110 : 0.6729158469874029
Loss in iteration 111 : 0.6713837988523517
Loss in iteration 112 : 0.6719477970102385
Loss in iteration 113 : 0.6706368320117777
Loss in iteration 114 : 0.6701092454246038
Loss in iteration 115 : 0.6706213127118003
Loss in iteration 116 : 0.6709175313620888
Loss in iteration 117 : 0.6698958158812931
Loss in iteration 118 : 0.6699510089080012
Loss in iteration 119 : 0.6694416343555145
Loss in iteration 120 : 0.6712451589260533
Loss in iteration 121 : 0.6690761871716151
Loss in iteration 122 : 0.6700555742631813
Loss in iteration 123 : 0.6706273357948886
Loss in iteration 124 : 0.6686718133851149
Loss in iteration 125 : 0.6682278689377027
Loss in iteration 126 : 0.6704673764411873
Loss in iteration 127 : 0.6691975530081467
Loss in iteration 128 : 0.6675086804082131
Loss in iteration 129 : 0.6689740917958047
Loss in iteration 130 : 0.6672363585655108
Loss in iteration 131 : 0.6685671867080792
Loss in iteration 132 : 0.666224556026815
Loss in iteration 133 : 0.6674589690227553
Loss in iteration 134 : 0.6657901826537936
Loss in iteration 135 : 0.6655242208632173
Loss in iteration 136 : 0.6674135142211683
Loss in iteration 137 : 0.6677434385493181
Loss in iteration 138 : 0.6651925196940216
Loss in iteration 139 : 0.6660466112817627
Loss in iteration 140 : 0.6658852474079276
Loss in iteration 141 : 0.6681548550692326
Loss in iteration 142 : 0.6654469281457099
Loss in iteration 143 : 0.6639040221704651
Loss in iteration 144 : 0.6652330975164437
Loss in iteration 145 : 0.6641597007569381
Loss in iteration 146 : 0.6649920837938817
Loss in iteration 147 : 0.6655410378185769
Loss in iteration 148 : 0.6660876763155313
Loss in iteration 149 : 0.6647996070340219
Loss in iteration 150 : 0.6651731423845115
Loss in iteration 151 : 0.6654523794303494
Loss in iteration 152 : 0.6643060982785727
Loss in iteration 153 : 0.6641407082625759
Loss in iteration 154 : 0.6646133933149727
Loss in iteration 155 : 0.6626030214529977
Loss in iteration 156 : 0.6626971996014778
Loss in iteration 157 : 0.6634448131879316
Loss in iteration 158 : 0.6624733777586596
Loss in iteration 159 : 0.662127448266764
Loss in iteration 160 : 0.6616120184454752
Loss in iteration 161 : 0.6619756065026701
Loss in iteration 162 : 0.6623729175143266
Loss in iteration 163 : 0.6650991563988393
Loss in iteration 164 : 0.6617219644637926
Loss in iteration 165 : 0.6621637697629604
Loss in iteration 166 : 0.6624473959923729
Loss in iteration 167 : 0.6609477889145686
Loss in iteration 168 : 0.6614633676128009
Loss in iteration 169 : 0.6601729706600771
Loss in iteration 170 : 0.661011104768248
Loss in iteration 171 : 0.6595563660802912
Loss in iteration 172 : 0.6603329381052992
Loss in iteration 173 : 0.6586281552288724
Loss in iteration 174 : 0.660319162516109
Loss in iteration 175 : 0.6585619824769439
Loss in iteration 176 : 0.6616700948390346
Loss in iteration 177 : 0.6589459474672857
Loss in iteration 178 : 0.6580609643273624
Loss in iteration 179 : 0.6573943295267652
Loss in iteration 180 : 0.6584059769392654
Loss in iteration 181 : 0.6592770366513976
Loss in iteration 182 : 0.6583673488944718
Loss in iteration 183 : 0.6589572967198181
Loss in iteration 184 : 0.6573712755651815
Loss in iteration 185 : 0.6587859450298591
Loss in iteration 186 : 0.6567822460751354
Loss in iteration 187 : 0.6553766976342922
Loss in iteration 188 : 0.6578602337204229
Loss in iteration 189 : 0.657672962740154
Loss in iteration 190 : 0.6575353124075958
Loss in iteration 191 : 0.6573975035774434
Loss in iteration 192 : 0.6548825985343616
Loss in iteration 193 : 0.6565150221975925
Loss in iteration 194 : 0.6549944829341455
Loss in iteration 195 : 0.6570131850173415
Loss in iteration 196 : 0.6589547357083592
Loss in iteration 197 : 0.6562908359928611
Loss in iteration 198 : 0.6555151258113502
Loss in iteration 199 : 0.6563159438510384
Loss in iteration 200 : 0.6560198182292187
Testing accuracy  of updater 4 on alg 0 with rate 70.0 = 0.6495, training accuracy 0.66525, time elapsed: 3912 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.692954415660519
Loss in iteration 3 : 0.6927134963987032
Loss in iteration 4 : 0.692523095794886
Loss in iteration 5 : 0.6923455354187666
Loss in iteration 6 : 0.6921216694434115
Loss in iteration 7 : 0.6918865755017567
Loss in iteration 8 : 0.6916468334493496
Loss in iteration 9 : 0.6913409841999592
Loss in iteration 10 : 0.6911699977835887
Loss in iteration 11 : 0.6910998932353517
Loss in iteration 12 : 0.6905747285637485
Loss in iteration 13 : 0.6904901262275003
Loss in iteration 14 : 0.6903743550492953
Loss in iteration 15 : 0.690310302304313
Loss in iteration 16 : 0.689806463256547
Loss in iteration 17 : 0.6899069296808502
Loss in iteration 18 : 0.6896688976433597
Loss in iteration 19 : 0.6890710372875193
Loss in iteration 20 : 0.6892362936799045
Loss in iteration 21 : 0.6888117175152009
Loss in iteration 22 : 0.6887006592618844
Loss in iteration 23 : 0.6886442248161144
Loss in iteration 24 : 0.6880380642530143
Loss in iteration 25 : 0.6881967489006112
Loss in iteration 26 : 0.6875872874376607
Loss in iteration 27 : 0.6867817857287847
Loss in iteration 28 : 0.6872354582639568
Loss in iteration 29 : 0.6869142857890588
Loss in iteration 30 : 0.6867542545457949
Loss in iteration 31 : 0.6860850094038604
Loss in iteration 32 : 0.6863243776619261
Loss in iteration 33 : 0.6855634357555873
Loss in iteration 34 : 0.6860530403303732
Loss in iteration 35 : 0.6858173235597693
Loss in iteration 36 : 0.6854738346376517
Loss in iteration 37 : 0.6861668711429727
Loss in iteration 38 : 0.6853583203935021
Loss in iteration 39 : 0.6855984786956603
Loss in iteration 40 : 0.6852109543777539
Loss in iteration 41 : 0.6846730663473308
Loss in iteration 42 : 0.6838751925922858
Loss in iteration 43 : 0.6848140061066534
Loss in iteration 44 : 0.6833706927310454
Loss in iteration 45 : 0.6844238279258037
Loss in iteration 46 : 0.6836049473296173
Loss in iteration 47 : 0.6838840618100708
Loss in iteration 48 : 0.6841418489596727
Loss in iteration 49 : 0.6833423515938891
Loss in iteration 50 : 0.6822570849897197
Loss in iteration 51 : 0.6820410809464506
Loss in iteration 52 : 0.6842172403635419
Loss in iteration 53 : 0.6824610636130277
Loss in iteration 54 : 0.6814335935960143
Loss in iteration 55 : 0.681496265657095
Loss in iteration 56 : 0.6816584272795216
Loss in iteration 57 : 0.6814044548699173
Loss in iteration 58 : 0.6823142482767517
Loss in iteration 59 : 0.6818166439067789
Loss in iteration 60 : 0.680984571327607
Loss in iteration 61 : 0.6804299540116885
Loss in iteration 62 : 0.6806588082776646
Loss in iteration 63 : 0.6810059076383056
Loss in iteration 64 : 0.6798089601646751
Loss in iteration 65 : 0.6786934721217232
Loss in iteration 66 : 0.6812692904962416
Loss in iteration 67 : 0.6786282044899703
Loss in iteration 68 : 0.6789186893388243
Loss in iteration 69 : 0.6789550517114518
Loss in iteration 70 : 0.6787557362498238
Loss in iteration 71 : 0.6808884493413297
Loss in iteration 72 : 0.6794239541283359
Loss in iteration 73 : 0.6786152869511552
Loss in iteration 74 : 0.6792640097908187
Loss in iteration 75 : 0.6780631312928841
Loss in iteration 76 : 0.6775461117949462
Loss in iteration 77 : 0.6781283652027353
Loss in iteration 78 : 0.6766907491829647
Loss in iteration 79 : 0.6782379624016731
Loss in iteration 80 : 0.6776193295328653
Loss in iteration 81 : 0.6776972753818514
Loss in iteration 82 : 0.6773557613648032
Loss in iteration 83 : 0.6769748744780513
Loss in iteration 84 : 0.675924312165286
Loss in iteration 85 : 0.6772908846710025
Loss in iteration 86 : 0.6757809063258772
Loss in iteration 87 : 0.6749744219115691
Loss in iteration 88 : 0.6760148027144901
Loss in iteration 89 : 0.675646293112358
Loss in iteration 90 : 0.6744101227227716
Loss in iteration 91 : 0.6744567095708259
Loss in iteration 92 : 0.6745414373605534
Loss in iteration 93 : 0.6752314855747859
Loss in iteration 94 : 0.6734759837070062
Loss in iteration 95 : 0.6745101817448461
Loss in iteration 96 : 0.6745331278123725
Loss in iteration 97 : 0.6738027902430818
Loss in iteration 98 : 0.6737669629575883
Loss in iteration 99 : 0.6743777506444127
Loss in iteration 100 : 0.6722784137692199
Loss in iteration 101 : 0.6733694449586813
Loss in iteration 102 : 0.6731121565405521
Loss in iteration 103 : 0.6734136350357758
Loss in iteration 104 : 0.6727738778926896
Loss in iteration 105 : 0.6718771121349132
Loss in iteration 106 : 0.6732421991393129
Loss in iteration 107 : 0.6721677338723379
Loss in iteration 108 : 0.6713905854082072
Loss in iteration 109 : 0.6715185252493262
Loss in iteration 110 : 0.6729158469874029
Loss in iteration 111 : 0.6713837988523517
Loss in iteration 112 : 0.6719477970102387
Loss in iteration 113 : 0.6706368320117777
Loss in iteration 114 : 0.6701092454246038
Loss in iteration 115 : 0.6706213127118004
Loss in iteration 116 : 0.6709175313620888
Loss in iteration 117 : 0.669895815881293
Loss in iteration 118 : 0.6699510089080012
Loss in iteration 119 : 0.6694416343555145
Loss in iteration 120 : 0.6712451589260533
Loss in iteration 121 : 0.6690761871716151
Loss in iteration 122 : 0.6700555742631813
Loss in iteration 123 : 0.6706273357948886
Loss in iteration 124 : 0.6686718133851148
Loss in iteration 125 : 0.6682278689377027
Loss in iteration 126 : 0.6704673764411873
Loss in iteration 127 : 0.6691975530081467
Loss in iteration 128 : 0.6675086804082131
Loss in iteration 129 : 0.6689740917958047
Loss in iteration 130 : 0.6672363585655108
Loss in iteration 131 : 0.6685671867080792
Loss in iteration 132 : 0.666224556026815
Loss in iteration 133 : 0.6674589690227553
Loss in iteration 134 : 0.6657901826537936
Loss in iteration 135 : 0.6655242208632173
Loss in iteration 136 : 0.6674135142211685
Loss in iteration 137 : 0.667743438549318
Loss in iteration 138 : 0.6651925196940214
Loss in iteration 139 : 0.6660466112817627
Loss in iteration 140 : 0.6658852474079276
Loss in iteration 141 : 0.6681548550692327
Loss in iteration 142 : 0.66544692814571
Loss in iteration 143 : 0.6639040221704651
Loss in iteration 144 : 0.6652330975164438
Loss in iteration 145 : 0.6641597007569381
Loss in iteration 146 : 0.6649920837938817
Loss in iteration 147 : 0.6655410378185769
Loss in iteration 148 : 0.6660876763155313
Loss in iteration 149 : 0.6647996070340219
Loss in iteration 150 : 0.6651731423845115
Loss in iteration 151 : 0.6654523794303494
Loss in iteration 152 : 0.6643060982785727
Loss in iteration 153 : 0.6641407082625759
Loss in iteration 154 : 0.6646133933149727
Loss in iteration 155 : 0.6626030214529978
Loss in iteration 156 : 0.6626971996014778
Loss in iteration 157 : 0.6634448131879316
Loss in iteration 158 : 0.6624733777586597
Loss in iteration 159 : 0.6621274482667638
Loss in iteration 160 : 0.6616120184454752
Loss in iteration 161 : 0.6619756065026701
Loss in iteration 162 : 0.6623729175143266
Loss in iteration 163 : 0.6650991563988393
Loss in iteration 164 : 0.6617219644637926
Loss in iteration 165 : 0.6621637697629604
Loss in iteration 166 : 0.6624473959923729
Loss in iteration 167 : 0.6609477889145687
Loss in iteration 168 : 0.661463367612801
Loss in iteration 169 : 0.660172970660077
Loss in iteration 170 : 0.661011104768248
Loss in iteration 171 : 0.6595563660802912
Loss in iteration 172 : 0.6603329381052992
Loss in iteration 173 : 0.6586281552288724
Loss in iteration 174 : 0.6603191625161091
Loss in iteration 175 : 0.658561982476944
Loss in iteration 176 : 0.6616700948390346
Loss in iteration 177 : 0.6589459474672856
Loss in iteration 178 : 0.6580609643273624
Loss in iteration 179 : 0.6573943295267654
Loss in iteration 180 : 0.6584059769392654
Loss in iteration 181 : 0.6592770366513975
Loss in iteration 182 : 0.6583673488944717
Loss in iteration 183 : 0.6589572967198181
Loss in iteration 184 : 0.6573712755651815
Loss in iteration 185 : 0.6587859450298591
Loss in iteration 186 : 0.6567822460751354
Loss in iteration 187 : 0.6553766976342922
Loss in iteration 188 : 0.6578602337204229
Loss in iteration 189 : 0.6576729627401541
Loss in iteration 190 : 0.6575353124075958
Loss in iteration 191 : 0.6573975035774432
Loss in iteration 192 : 0.6548825985343616
Loss in iteration 193 : 0.6565150221975925
Loss in iteration 194 : 0.6549944829341457
Loss in iteration 195 : 0.6570131850173415
Loss in iteration 196 : 0.6589547357083592
Loss in iteration 197 : 0.6562908359928611
Loss in iteration 198 : 0.6555151258113502
Loss in iteration 199 : 0.6563159438510384
Loss in iteration 200 : 0.6560198182292187
Testing accuracy  of updater 4 on alg 0 with rate 40.0 = 0.6495, training accuracy 0.66525, time elapsed: 3893 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.692954415660519
Loss in iteration 3 : 0.6927134963987032
Loss in iteration 4 : 0.692523095794886
Loss in iteration 5 : 0.6923455354187666
Loss in iteration 6 : 0.6921216694434115
Loss in iteration 7 : 0.6918865755017567
Loss in iteration 8 : 0.6916468334493497
Loss in iteration 9 : 0.6913409841999593
Loss in iteration 10 : 0.6911699977835886
Loss in iteration 11 : 0.6910998932353517
Loss in iteration 12 : 0.6905747285637486
Loss in iteration 13 : 0.6904901262275003
Loss in iteration 14 : 0.6903743550492953
Loss in iteration 15 : 0.690310302304313
Loss in iteration 16 : 0.689806463256547
Loss in iteration 17 : 0.6899069296808502
Loss in iteration 18 : 0.6896688976433597
Loss in iteration 19 : 0.6890710372875193
Loss in iteration 20 : 0.6892362936799045
Loss in iteration 21 : 0.6888117175152009
Loss in iteration 22 : 0.6887006592618844
Loss in iteration 23 : 0.6886442248161144
Loss in iteration 24 : 0.6880380642530143
Loss in iteration 25 : 0.6881967489006112
Loss in iteration 26 : 0.6875872874376607
Loss in iteration 27 : 0.6867817857287846
Loss in iteration 28 : 0.6872354582639569
Loss in iteration 29 : 0.6869142857890588
Loss in iteration 30 : 0.686754254545795
Loss in iteration 31 : 0.6860850094038603
Loss in iteration 32 : 0.6863243776619261
Loss in iteration 33 : 0.6855634357555874
Loss in iteration 34 : 0.6860530403303732
Loss in iteration 35 : 0.6858173235597691
Loss in iteration 36 : 0.6854738346376517
Loss in iteration 37 : 0.6861668711429726
Loss in iteration 38 : 0.685358320393502
Loss in iteration 39 : 0.6855984786956603
Loss in iteration 40 : 0.6852109543777539
Loss in iteration 41 : 0.6846730663473308
Loss in iteration 42 : 0.6838751925922859
Loss in iteration 43 : 0.6848140061066535
Loss in iteration 44 : 0.6833706927310454
Loss in iteration 45 : 0.6844238279258036
Loss in iteration 46 : 0.6836049473296173
Loss in iteration 47 : 0.6838840618100708
Loss in iteration 48 : 0.6841418489596726
Loss in iteration 49 : 0.6833423515938891
Loss in iteration 50 : 0.6822570849897197
Loss in iteration 51 : 0.6820410809464507
Loss in iteration 52 : 0.6842172403635419
Loss in iteration 53 : 0.6824610636130277
Loss in iteration 54 : 0.6814335935960143
Loss in iteration 55 : 0.6814962656570949
Loss in iteration 56 : 0.6816584272795218
Loss in iteration 57 : 0.6814044548699173
Loss in iteration 58 : 0.6823142482767517
Loss in iteration 59 : 0.6818166439067789
Loss in iteration 60 : 0.680984571327607
Loss in iteration 61 : 0.6804299540116885
Loss in iteration 62 : 0.6806588082776646
Loss in iteration 63 : 0.6810059076383056
Loss in iteration 64 : 0.679808960164675
Loss in iteration 65 : 0.6786934721217232
Loss in iteration 66 : 0.6812692904962417
Loss in iteration 67 : 0.6786282044899703
Loss in iteration 68 : 0.6789186893388243
Loss in iteration 69 : 0.6789550517114518
Loss in iteration 70 : 0.6787557362498237
Loss in iteration 71 : 0.6808884493413297
Loss in iteration 72 : 0.679423954128336
Loss in iteration 73 : 0.6786152869511552
Loss in iteration 74 : 0.6792640097908187
Loss in iteration 75 : 0.678063131292884
Loss in iteration 76 : 0.6775461117949462
Loss in iteration 77 : 0.6781283652027355
Loss in iteration 78 : 0.6766907491829647
Loss in iteration 79 : 0.6782379624016731
Loss in iteration 80 : 0.6776193295328653
Loss in iteration 81 : 0.6776972753818514
Loss in iteration 82 : 0.6773557613648032
Loss in iteration 83 : 0.6769748744780513
Loss in iteration 84 : 0.6759243121652861
Loss in iteration 85 : 0.6772908846710025
Loss in iteration 86 : 0.6757809063258772
Loss in iteration 87 : 0.6749744219115691
Loss in iteration 88 : 0.6760148027144902
Loss in iteration 89 : 0.675646293112358
Loss in iteration 90 : 0.6744101227227716
Loss in iteration 91 : 0.6744567095708259
Loss in iteration 92 : 0.6745414373605535
Loss in iteration 93 : 0.6752314855747859
Loss in iteration 94 : 0.6734759837070062
Loss in iteration 95 : 0.6745101817448461
Loss in iteration 96 : 0.6745331278123725
Loss in iteration 97 : 0.6738027902430819
Loss in iteration 98 : 0.6737669629575883
Loss in iteration 99 : 0.6743777506444127
Loss in iteration 100 : 0.6722784137692198
Loss in iteration 101 : 0.6733694449586813
Loss in iteration 102 : 0.6731121565405521
Loss in iteration 103 : 0.6734136350357758
Loss in iteration 104 : 0.6727738778926896
Loss in iteration 105 : 0.6718771121349132
Loss in iteration 106 : 0.6732421991393129
Loss in iteration 107 : 0.6721677338723379
Loss in iteration 108 : 0.6713905854082072
Loss in iteration 109 : 0.6715185252493263
Loss in iteration 110 : 0.6729158469874029
Loss in iteration 111 : 0.6713837988523517
Loss in iteration 112 : 0.6719477970102385
Loss in iteration 113 : 0.6706368320117777
Loss in iteration 114 : 0.6701092454246038
Loss in iteration 115 : 0.6706213127118004
Loss in iteration 116 : 0.6709175313620888
Loss in iteration 117 : 0.6698958158812931
Loss in iteration 118 : 0.6699510089080012
Loss in iteration 119 : 0.6694416343555145
Loss in iteration 120 : 0.6712451589260533
Loss in iteration 121 : 0.6690761871716151
Loss in iteration 122 : 0.6700555742631813
Loss in iteration 123 : 0.6706273357948886
Loss in iteration 124 : 0.6686718133851148
Loss in iteration 125 : 0.6682278689377027
Loss in iteration 126 : 0.6704673764411873
Loss in iteration 127 : 0.6691975530081468
Loss in iteration 128 : 0.6675086804082131
Loss in iteration 129 : 0.6689740917958047
Loss in iteration 130 : 0.6672363585655108
Loss in iteration 131 : 0.6685671867080794
Loss in iteration 132 : 0.666224556026815
Loss in iteration 133 : 0.6674589690227553
Loss in iteration 134 : 0.6657901826537936
Loss in iteration 135 : 0.6655242208632173
Loss in iteration 136 : 0.6674135142211683
Loss in iteration 137 : 0.667743438549318
Loss in iteration 138 : 0.6651925196940215
Loss in iteration 139 : 0.6660466112817627
Loss in iteration 140 : 0.6658852474079276
Loss in iteration 141 : 0.6681548550692327
Loss in iteration 142 : 0.66544692814571
Loss in iteration 143 : 0.6639040221704652
Loss in iteration 144 : 0.6652330975164438
Loss in iteration 145 : 0.6641597007569381
Loss in iteration 146 : 0.6649920837938819
Loss in iteration 147 : 0.6655410378185769
Loss in iteration 148 : 0.6660876763155313
Loss in iteration 149 : 0.6647996070340219
Loss in iteration 150 : 0.6651731423845115
Loss in iteration 151 : 0.6654523794303494
Loss in iteration 152 : 0.6643060982785727
Loss in iteration 153 : 0.6641407082625759
Loss in iteration 154 : 0.6646133933149727
Loss in iteration 155 : 0.6626030214529978
Loss in iteration 156 : 0.6626971996014778
Loss in iteration 157 : 0.6634448131879316
Loss in iteration 158 : 0.6624733777586597
Loss in iteration 159 : 0.662127448266764
Loss in iteration 160 : 0.6616120184454752
Loss in iteration 161 : 0.6619756065026701
Loss in iteration 162 : 0.6623729175143265
Loss in iteration 163 : 0.6650991563988393
Loss in iteration 164 : 0.6617219644637926
Loss in iteration 165 : 0.6621637697629604
Loss in iteration 166 : 0.6624473959923729
Loss in iteration 167 : 0.6609477889145687
Loss in iteration 168 : 0.6614633676128009
Loss in iteration 169 : 0.6601729706600771
Loss in iteration 170 : 0.661011104768248
Loss in iteration 171 : 0.6595563660802913
Loss in iteration 172 : 0.6603329381052992
Loss in iteration 173 : 0.6586281552288724
Loss in iteration 174 : 0.660319162516109
Loss in iteration 175 : 0.6585619824769439
Loss in iteration 176 : 0.6616700948390346
Loss in iteration 177 : 0.6589459474672856
Loss in iteration 178 : 0.6580609643273624
Loss in iteration 179 : 0.6573943295267652
Loss in iteration 180 : 0.6584059769392654
Loss in iteration 181 : 0.6592770366513976
Loss in iteration 182 : 0.6583673488944717
Loss in iteration 183 : 0.6589572967198181
Loss in iteration 184 : 0.6573712755651815
Loss in iteration 185 : 0.6587859450298591
Loss in iteration 186 : 0.6567822460751354
Loss in iteration 187 : 0.6553766976342923
Loss in iteration 188 : 0.6578602337204229
Loss in iteration 189 : 0.657672962740154
Loss in iteration 190 : 0.6575353124075958
Loss in iteration 191 : 0.6573975035774434
Loss in iteration 192 : 0.6548825985343616
Loss in iteration 193 : 0.6565150221975925
Loss in iteration 194 : 0.6549944829341457
Loss in iteration 195 : 0.6570131850173416
Loss in iteration 196 : 0.6589547357083592
Loss in iteration 197 : 0.6562908359928611
Loss in iteration 198 : 0.6555151258113502
Loss in iteration 199 : 0.6563159438510383
Loss in iteration 200 : 0.6560198182292188
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.6495, training accuracy 0.66525, time elapsed: 4104 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.692954415660519
Loss in iteration 3 : 0.6927134963987032
Loss in iteration 4 : 0.692523095794886
Loss in iteration 5 : 0.6923455354187665
Loss in iteration 6 : 0.6921216694434115
Loss in iteration 7 : 0.6918865755017567
Loss in iteration 8 : 0.6916468334493496
Loss in iteration 9 : 0.6913409841999592
Loss in iteration 10 : 0.6911699977835886
Loss in iteration 11 : 0.6910998932353517
Loss in iteration 12 : 0.6905747285637486
Loss in iteration 13 : 0.6904901262275003
Loss in iteration 14 : 0.6903743550492952
Loss in iteration 15 : 0.690310302304313
Loss in iteration 16 : 0.689806463256547
Loss in iteration 17 : 0.6899069296808503
Loss in iteration 18 : 0.6896688976433596
Loss in iteration 19 : 0.6890710372875193
Loss in iteration 20 : 0.6892362936799045
Loss in iteration 21 : 0.6888117175152009
Loss in iteration 22 : 0.6887006592618844
Loss in iteration 23 : 0.6886442248161144
Loss in iteration 24 : 0.6880380642530143
Loss in iteration 25 : 0.6881967489006112
Loss in iteration 26 : 0.6875872874376608
Loss in iteration 27 : 0.6867817857287846
Loss in iteration 28 : 0.6872354582639569
Loss in iteration 29 : 0.6869142857890588
Loss in iteration 30 : 0.686754254545795
Loss in iteration 31 : 0.6860850094038603
Loss in iteration 32 : 0.6863243776619261
Loss in iteration 33 : 0.6855634357555872
Loss in iteration 34 : 0.6860530403303732
Loss in iteration 35 : 0.6858173235597693
Loss in iteration 36 : 0.6854738346376517
Loss in iteration 37 : 0.6861668711429726
Loss in iteration 38 : 0.685358320393502
Loss in iteration 39 : 0.6855984786956603
Loss in iteration 40 : 0.6852109543777539
Loss in iteration 41 : 0.6846730663473309
Loss in iteration 42 : 0.6838751925922859
Loss in iteration 43 : 0.6848140061066534
Loss in iteration 44 : 0.6833706927310454
Loss in iteration 45 : 0.6844238279258037
Loss in iteration 46 : 0.6836049473296173
Loss in iteration 47 : 0.6838840618100707
Loss in iteration 48 : 0.6841418489596726
Loss in iteration 49 : 0.6833423515938891
Loss in iteration 50 : 0.6822570849897197
Loss in iteration 51 : 0.6820410809464506
Loss in iteration 52 : 0.6842172403635419
Loss in iteration 53 : 0.6824610636130277
Loss in iteration 54 : 0.6814335935960143
Loss in iteration 55 : 0.681496265657095
Loss in iteration 56 : 0.6816584272795216
Loss in iteration 57 : 0.6814044548699174
Loss in iteration 58 : 0.6823142482767517
Loss in iteration 59 : 0.6818166439067789
Loss in iteration 60 : 0.680984571327607
Loss in iteration 61 : 0.6804299540116885
Loss in iteration 62 : 0.6806588082776646
Loss in iteration 63 : 0.6810059076383056
Loss in iteration 64 : 0.679808960164675
Loss in iteration 65 : 0.6786934721217232
Loss in iteration 66 : 0.6812692904962416
Loss in iteration 67 : 0.6786282044899703
Loss in iteration 68 : 0.6789186893388243
Loss in iteration 69 : 0.6789550517114518
Loss in iteration 70 : 0.6787557362498238
Loss in iteration 71 : 0.6808884493413297
Loss in iteration 72 : 0.679423954128336
Loss in iteration 73 : 0.6786152869511552
Loss in iteration 74 : 0.6792640097908187
Loss in iteration 75 : 0.678063131292884
Loss in iteration 76 : 0.6775461117949462
Loss in iteration 77 : 0.6781283652027353
Loss in iteration 78 : 0.6766907491829647
Loss in iteration 79 : 0.6782379624016731
Loss in iteration 80 : 0.6776193295328653
Loss in iteration 81 : 0.6776972753818513
Loss in iteration 82 : 0.6773557613648032
Loss in iteration 83 : 0.6769748744780512
Loss in iteration 84 : 0.675924312165286
Loss in iteration 85 : 0.6772908846710026
Loss in iteration 86 : 0.6757809063258772
Loss in iteration 87 : 0.6749744219115692
Loss in iteration 88 : 0.6760148027144901
Loss in iteration 89 : 0.6756462931123579
Loss in iteration 90 : 0.6744101227227716
Loss in iteration 91 : 0.6744567095708259
Loss in iteration 92 : 0.6745414373605535
Loss in iteration 93 : 0.6752314855747859
Loss in iteration 94 : 0.6734759837070062
Loss in iteration 95 : 0.6745101817448461
Loss in iteration 96 : 0.6745331278123725
Loss in iteration 97 : 0.6738027902430818
Loss in iteration 98 : 0.6737669629575883
Loss in iteration 99 : 0.6743777506444127
Loss in iteration 100 : 0.6722784137692198
Loss in iteration 101 : 0.6733694449586813
Loss in iteration 102 : 0.6731121565405521
Loss in iteration 103 : 0.6734136350357758
Loss in iteration 104 : 0.6727738778926897
Loss in iteration 105 : 0.6718771121349132
Loss in iteration 106 : 0.6732421991393129
Loss in iteration 107 : 0.6721677338723377
Loss in iteration 108 : 0.6713905854082072
Loss in iteration 109 : 0.6715185252493263
Loss in iteration 110 : 0.6729158469874029
Loss in iteration 111 : 0.6713837988523518
Loss in iteration 112 : 0.6719477970102385
Loss in iteration 113 : 0.6706368320117777
Loss in iteration 114 : 0.6701092454246038
Loss in iteration 115 : 0.6706213127118003
Loss in iteration 116 : 0.6709175313620888
Loss in iteration 117 : 0.6698958158812931
Loss in iteration 118 : 0.6699510089080012
Loss in iteration 119 : 0.6694416343555145
Loss in iteration 120 : 0.6712451589260533
Loss in iteration 121 : 0.6690761871716152
Loss in iteration 122 : 0.6700555742631813
Loss in iteration 123 : 0.6706273357948886
Loss in iteration 124 : 0.6686718133851149
Loss in iteration 125 : 0.6682278689377027
Loss in iteration 126 : 0.6704673764411875
Loss in iteration 127 : 0.6691975530081468
Loss in iteration 128 : 0.6675086804082131
Loss in iteration 129 : 0.6689740917958047
Loss in iteration 130 : 0.6672363585655108
Loss in iteration 131 : 0.6685671867080794
Loss in iteration 132 : 0.666224556026815
Loss in iteration 133 : 0.6674589690227553
Loss in iteration 134 : 0.6657901826537936
Loss in iteration 135 : 0.6655242208632173
Loss in iteration 136 : 0.6674135142211683
Loss in iteration 137 : 0.667743438549318
Loss in iteration 138 : 0.6651925196940214
Loss in iteration 139 : 0.6660466112817627
Loss in iteration 140 : 0.6658852474079276
Loss in iteration 141 : 0.6681548550692327
Loss in iteration 142 : 0.66544692814571
Loss in iteration 143 : 0.6639040221704651
Loss in iteration 144 : 0.6652330975164438
Loss in iteration 145 : 0.6641597007569381
Loss in iteration 146 : 0.6649920837938819
Loss in iteration 147 : 0.6655410378185769
Loss in iteration 148 : 0.6660876763155313
Loss in iteration 149 : 0.6647996070340219
Loss in iteration 150 : 0.6651731423845115
Loss in iteration 151 : 0.6654523794303495
Loss in iteration 152 : 0.6643060982785727
Loss in iteration 153 : 0.6641407082625759
Loss in iteration 154 : 0.6646133933149727
Loss in iteration 155 : 0.6626030214529977
Loss in iteration 156 : 0.6626971996014777
Loss in iteration 157 : 0.6634448131879316
Loss in iteration 158 : 0.6624733777586597
Loss in iteration 159 : 0.662127448266764
Loss in iteration 160 : 0.6616120184454752
Loss in iteration 161 : 0.6619756065026701
Loss in iteration 162 : 0.6623729175143265
Loss in iteration 163 : 0.6650991563988393
Loss in iteration 164 : 0.6617219644637926
Loss in iteration 165 : 0.6621637697629604
Loss in iteration 166 : 0.6624473959923729
Loss in iteration 167 : 0.6609477889145687
Loss in iteration 168 : 0.6614633676128009
Loss in iteration 169 : 0.6601729706600771
Loss in iteration 170 : 0.661011104768248
Loss in iteration 171 : 0.6595563660802912
Loss in iteration 172 : 0.6603329381052992
Loss in iteration 173 : 0.6586281552288722
Loss in iteration 174 : 0.660319162516109
Loss in iteration 175 : 0.6585619824769439
Loss in iteration 176 : 0.6616700948390346
Loss in iteration 177 : 0.6589459474672856
Loss in iteration 178 : 0.6580609643273624
Loss in iteration 179 : 0.6573943295267652
Loss in iteration 180 : 0.6584059769392654
Loss in iteration 181 : 0.6592770366513976
Loss in iteration 182 : 0.6583673488944717
Loss in iteration 183 : 0.6589572967198181
Loss in iteration 184 : 0.6573712755651815
Loss in iteration 185 : 0.6587859450298591
Loss in iteration 186 : 0.6567822460751352
Loss in iteration 187 : 0.6553766976342923
Loss in iteration 188 : 0.657860233720423
Loss in iteration 189 : 0.6576729627401541
Loss in iteration 190 : 0.6575353124075958
Loss in iteration 191 : 0.6573975035774434
Loss in iteration 192 : 0.6548825985343616
Loss in iteration 193 : 0.6565150221975924
Loss in iteration 194 : 0.6549944829341455
Loss in iteration 195 : 0.6570131850173415
Loss in iteration 196 : 0.6589547357083592
Loss in iteration 197 : 0.6562908359928612
Loss in iteration 198 : 0.6555151258113502
Loss in iteration 199 : 0.6563159438510384
Loss in iteration 200 : 0.6560198182292188
Testing accuracy  of updater 4 on alg 0 with rate 7.0 = 0.6495, training accuracy 0.66525, time elapsed: 4913 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.692954415660519
Loss in iteration 3 : 0.6927134963987032
Loss in iteration 4 : 0.692523095794886
Loss in iteration 5 : 0.6923455354187666
Loss in iteration 6 : 0.6921216694434115
Loss in iteration 7 : 0.6918865755017567
Loss in iteration 8 : 0.6916468334493497
Loss in iteration 9 : 0.6913409841999592
Loss in iteration 10 : 0.6911699977835887
Loss in iteration 11 : 0.6910998932353517
Loss in iteration 12 : 0.6905747285637486
Loss in iteration 13 : 0.6904901262275003
Loss in iteration 14 : 0.6903743550492953
Loss in iteration 15 : 0.690310302304313
Loss in iteration 16 : 0.689806463256547
Loss in iteration 17 : 0.6899069296808502
Loss in iteration 18 : 0.6896688976433597
Loss in iteration 19 : 0.6890710372875193
Loss in iteration 20 : 0.6892362936799045
Loss in iteration 21 : 0.688811717515201
Loss in iteration 22 : 0.6887006592618844
Loss in iteration 23 : 0.6886442248161144
Loss in iteration 24 : 0.6880380642530143
Loss in iteration 25 : 0.6881967489006112
Loss in iteration 26 : 0.6875872874376607
Loss in iteration 27 : 0.6867817857287847
Loss in iteration 28 : 0.6872354582639568
Loss in iteration 29 : 0.6869142857890588
Loss in iteration 30 : 0.6867542545457949
Loss in iteration 31 : 0.6860850094038603
Loss in iteration 32 : 0.6863243776619262
Loss in iteration 33 : 0.6855634357555872
Loss in iteration 34 : 0.6860530403303732
Loss in iteration 35 : 0.6858173235597693
Loss in iteration 36 : 0.6854738346376517
Loss in iteration 37 : 0.6861668711429726
Loss in iteration 38 : 0.685358320393502
Loss in iteration 39 : 0.6855984786956603
Loss in iteration 40 : 0.6852109543777539
Loss in iteration 41 : 0.6846730663473309
Loss in iteration 42 : 0.6838751925922859
Loss in iteration 43 : 0.6848140061066534
Loss in iteration 44 : 0.6833706927310454
Loss in iteration 45 : 0.6844238279258037
Loss in iteration 46 : 0.6836049473296173
Loss in iteration 47 : 0.6838840618100708
Loss in iteration 48 : 0.6841418489596727
Loss in iteration 49 : 0.6833423515938892
Loss in iteration 50 : 0.6822570849897197
Loss in iteration 51 : 0.6820410809464507
Loss in iteration 52 : 0.6842172403635419
Loss in iteration 53 : 0.6824610636130277
Loss in iteration 54 : 0.6814335935960143
Loss in iteration 55 : 0.6814962656570949
Loss in iteration 56 : 0.6816584272795216
Loss in iteration 57 : 0.6814044548699174
Loss in iteration 58 : 0.6823142482767517
Loss in iteration 59 : 0.6818166439067789
Loss in iteration 60 : 0.680984571327607
Loss in iteration 61 : 0.6804299540116885
Loss in iteration 62 : 0.6806588082776646
Loss in iteration 63 : 0.6810059076383056
Loss in iteration 64 : 0.6798089601646751
Loss in iteration 65 : 0.6786934721217232
Loss in iteration 66 : 0.6812692904962416
Loss in iteration 67 : 0.6786282044899703
Loss in iteration 68 : 0.6789186893388243
Loss in iteration 69 : 0.6789550517114518
Loss in iteration 70 : 0.6787557362498238
Loss in iteration 71 : 0.6808884493413297
Loss in iteration 72 : 0.679423954128336
Loss in iteration 73 : 0.6786152869511552
Loss in iteration 74 : 0.6792640097908187
Loss in iteration 75 : 0.6780631312928841
Loss in iteration 76 : 0.6775461117949461
Loss in iteration 77 : 0.6781283652027353
Loss in iteration 78 : 0.6766907491829648
Loss in iteration 79 : 0.6782379624016732
Loss in iteration 80 : 0.6776193295328653
Loss in iteration 81 : 0.6776972753818514
Loss in iteration 82 : 0.6773557613648034
Loss in iteration 83 : 0.6769748744780512
Loss in iteration 84 : 0.675924312165286
Loss in iteration 85 : 0.6772908846710025
Loss in iteration 86 : 0.6757809063258772
Loss in iteration 87 : 0.6749744219115691
Loss in iteration 88 : 0.6760148027144901
Loss in iteration 89 : 0.6756462931123579
Loss in iteration 90 : 0.6744101227227716
Loss in iteration 91 : 0.6744567095708259
Loss in iteration 92 : 0.6745414373605534
Loss in iteration 93 : 0.6752314855747859
Loss in iteration 94 : 0.6734759837070062
Loss in iteration 95 : 0.674510181744846
Loss in iteration 96 : 0.6745331278123725
Loss in iteration 97 : 0.6738027902430818
Loss in iteration 98 : 0.6737669629575883
Loss in iteration 99 : 0.6743777506444127
Loss in iteration 100 : 0.6722784137692199
Loss in iteration 101 : 0.6733694449586813
Loss in iteration 102 : 0.673112156540552
Loss in iteration 103 : 0.6734136350357758
Loss in iteration 104 : 0.6727738778926896
Loss in iteration 105 : 0.6718771121349132
Loss in iteration 106 : 0.6732421991393129
Loss in iteration 107 : 0.6721677338723379
Loss in iteration 108 : 0.6713905854082072
Loss in iteration 109 : 0.6715185252493262
Loss in iteration 110 : 0.6729158469874029
Loss in iteration 111 : 0.6713837988523518
Loss in iteration 112 : 0.6719477970102385
Loss in iteration 113 : 0.6706368320117777
Loss in iteration 114 : 0.6701092454246038
Loss in iteration 115 : 0.6706213127118004
Loss in iteration 116 : 0.6709175313620888
Loss in iteration 117 : 0.6698958158812931
Loss in iteration 118 : 0.6699510089080012
Loss in iteration 119 : 0.6694416343555145
Loss in iteration 120 : 0.6712451589260533
Loss in iteration 121 : 0.6690761871716152
Loss in iteration 122 : 0.6700555742631813
Loss in iteration 123 : 0.6706273357948886
Loss in iteration 124 : 0.6686718133851148
Loss in iteration 125 : 0.6682278689377027
Loss in iteration 126 : 0.6704673764411873
Loss in iteration 127 : 0.6691975530081467
Loss in iteration 128 : 0.6675086804082131
Loss in iteration 129 : 0.6689740917958047
Loss in iteration 130 : 0.6672363585655108
Loss in iteration 131 : 0.6685671867080792
Loss in iteration 132 : 0.666224556026815
Loss in iteration 133 : 0.6674589690227553
Loss in iteration 134 : 0.6657901826537936
Loss in iteration 135 : 0.6655242208632173
Loss in iteration 136 : 0.6674135142211685
Loss in iteration 137 : 0.667743438549318
Loss in iteration 138 : 0.6651925196940215
Loss in iteration 139 : 0.6660466112817627
Loss in iteration 140 : 0.6658852474079276
Loss in iteration 141 : 0.6681548550692326
Loss in iteration 142 : 0.66544692814571
Loss in iteration 143 : 0.6639040221704652
Loss in iteration 144 : 0.6652330975164438
Loss in iteration 145 : 0.6641597007569381
Loss in iteration 146 : 0.6649920837938817
Loss in iteration 147 : 0.6655410378185769
Loss in iteration 148 : 0.6660876763155313
Loss in iteration 149 : 0.6647996070340219
Loss in iteration 150 : 0.6651731423845115
Loss in iteration 151 : 0.6654523794303495
Loss in iteration 152 : 0.6643060982785726
Loss in iteration 153 : 0.6641407082625759
Loss in iteration 154 : 0.6646133933149728
Loss in iteration 155 : 0.6626030214529978
Loss in iteration 156 : 0.6626971996014777
Loss in iteration 157 : 0.6634448131879316
Loss in iteration 158 : 0.6624733777586597
Loss in iteration 159 : 0.6621274482667638
Loss in iteration 160 : 0.6616120184454752
Loss in iteration 161 : 0.6619756065026701
Loss in iteration 162 : 0.6623729175143265
Loss in iteration 163 : 0.6650991563988393
Loss in iteration 164 : 0.6617219644637925
Loss in iteration 165 : 0.6621637697629603
Loss in iteration 166 : 0.6624473959923729
Loss in iteration 167 : 0.6609477889145687
Loss in iteration 168 : 0.6614633676128009
Loss in iteration 169 : 0.6601729706600771
Loss in iteration 170 : 0.661011104768248
Loss in iteration 171 : 0.6595563660802912
Loss in iteration 172 : 0.6603329381052992
Loss in iteration 173 : 0.6586281552288724
Loss in iteration 174 : 0.6603191625161091
Loss in iteration 175 : 0.6585619824769439
Loss in iteration 176 : 0.6616700948390346
Loss in iteration 177 : 0.6589459474672857
Loss in iteration 178 : 0.6580609643273624
Loss in iteration 179 : 0.6573943295267654
Loss in iteration 180 : 0.6584059769392654
Loss in iteration 181 : 0.6592770366513976
Loss in iteration 182 : 0.6583673488944717
Loss in iteration 183 : 0.6589572967198181
Loss in iteration 184 : 0.6573712755651815
Loss in iteration 185 : 0.6587859450298591
Loss in iteration 186 : 0.6567822460751354
Loss in iteration 187 : 0.6553766976342923
Loss in iteration 188 : 0.657860233720423
Loss in iteration 189 : 0.657672962740154
Loss in iteration 190 : 0.6575353124075958
Loss in iteration 191 : 0.6573975035774433
Loss in iteration 192 : 0.6548825985343616
Loss in iteration 193 : 0.6565150221975925
Loss in iteration 194 : 0.6549944829341457
Loss in iteration 195 : 0.6570131850173416
Loss in iteration 196 : 0.6589547357083592
Loss in iteration 197 : 0.6562908359928611
Loss in iteration 198 : 0.6555151258113502
Loss in iteration 199 : 0.6563159438510385
Loss in iteration 200 : 0.6560198182292187
Testing accuracy  of updater 4 on alg 0 with rate 4.0 = 0.6495, training accuracy 0.66525, time elapsed: 4682 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.692954415660519
Loss in iteration 3 : 0.6927134963987032
Loss in iteration 4 : 0.692523095794886
Loss in iteration 5 : 0.6923455354187666
Loss in iteration 6 : 0.6921216694434115
Loss in iteration 7 : 0.6918865755017567
Loss in iteration 8 : 0.6916468334493496
Loss in iteration 9 : 0.6913409841999593
Loss in iteration 10 : 0.6911699977835887
Loss in iteration 11 : 0.6910998932353517
Loss in iteration 12 : 0.6905747285637485
Loss in iteration 13 : 0.6904901262275003
Loss in iteration 14 : 0.6903743550492953
Loss in iteration 15 : 0.690310302304313
Loss in iteration 16 : 0.689806463256547
Loss in iteration 17 : 0.6899069296808503
Loss in iteration 18 : 0.6896688976433597
Loss in iteration 19 : 0.6890710372875193
Loss in iteration 20 : 0.6892362936799045
Loss in iteration 21 : 0.6888117175152009
Loss in iteration 22 : 0.6887006592618844
Loss in iteration 23 : 0.6886442248161144
Loss in iteration 24 : 0.6880380642530143
Loss in iteration 25 : 0.6881967489006112
Loss in iteration 26 : 0.6875872874376607
Loss in iteration 27 : 0.6867817857287846
Loss in iteration 28 : 0.6872354582639568
Loss in iteration 29 : 0.6869142857890588
Loss in iteration 30 : 0.6867542545457949
Loss in iteration 31 : 0.6860850094038604
Loss in iteration 32 : 0.6863243776619262
Loss in iteration 33 : 0.6855634357555872
Loss in iteration 34 : 0.6860530403303732
Loss in iteration 35 : 0.6858173235597693
Loss in iteration 36 : 0.6854738346376518
Loss in iteration 37 : 0.6861668711429727
Loss in iteration 38 : 0.6853583203935021
Loss in iteration 39 : 0.6855984786956603
Loss in iteration 40 : 0.6852109543777539
Loss in iteration 41 : 0.6846730663473308
Loss in iteration 42 : 0.6838751925922859
Loss in iteration 43 : 0.6848140061066534
Loss in iteration 44 : 0.6833706927310454
Loss in iteration 45 : 0.6844238279258036
Loss in iteration 46 : 0.6836049473296173
Loss in iteration 47 : 0.6838840618100707
Loss in iteration 48 : 0.6841418489596727
Loss in iteration 49 : 0.6833423515938892
Loss in iteration 50 : 0.6822570849897197
Loss in iteration 51 : 0.6820410809464507
Loss in iteration 52 : 0.6842172403635419
Loss in iteration 53 : 0.6824610636130277
Loss in iteration 54 : 0.6814335935960143
Loss in iteration 55 : 0.6814962656570949
Loss in iteration 56 : 0.6816584272795216
Loss in iteration 57 : 0.6814044548699174
Loss in iteration 58 : 0.6823142482767517
Loss in iteration 59 : 0.6818166439067788
Loss in iteration 60 : 0.680984571327607
Loss in iteration 61 : 0.6804299540116885
Loss in iteration 62 : 0.6806588082776646
Loss in iteration 63 : 0.6810059076383056
Loss in iteration 64 : 0.679808960164675
Loss in iteration 65 : 0.6786934721217232
Loss in iteration 66 : 0.6812692904962416
Loss in iteration 67 : 0.6786282044899703
Loss in iteration 68 : 0.6789186893388243
Loss in iteration 69 : 0.6789550517114518
Loss in iteration 70 : 0.6787557362498237
Loss in iteration 71 : 0.6808884493413297
Loss in iteration 72 : 0.679423954128336
Loss in iteration 73 : 0.6786152869511552
Loss in iteration 74 : 0.6792640097908187
Loss in iteration 75 : 0.6780631312928841
Loss in iteration 76 : 0.6775461117949462
Loss in iteration 77 : 0.6781283652027355
Loss in iteration 78 : 0.6766907491829647
Loss in iteration 79 : 0.6782379624016732
Loss in iteration 80 : 0.6776193295328653
Loss in iteration 81 : 0.6776972753818513
Loss in iteration 82 : 0.6773557613648034
Loss in iteration 83 : 0.6769748744780512
Loss in iteration 84 : 0.675924312165286
Loss in iteration 85 : 0.6772908846710025
Loss in iteration 86 : 0.6757809063258772
Loss in iteration 87 : 0.6749744219115691
Loss in iteration 88 : 0.6760148027144902
Loss in iteration 89 : 0.6756462931123579
Loss in iteration 90 : 0.6744101227227716
Loss in iteration 91 : 0.6744567095708258
Loss in iteration 92 : 0.6745414373605534
Loss in iteration 93 : 0.6752314855747859
Loss in iteration 94 : 0.6734759837070062
Loss in iteration 95 : 0.6745101817448461
Loss in iteration 96 : 0.6745331278123725
Loss in iteration 97 : 0.6738027902430818
Loss in iteration 98 : 0.6737669629575883
Loss in iteration 99 : 0.6743777506444127
Loss in iteration 100 : 0.6722784137692198
Loss in iteration 101 : 0.6733694449586813
Loss in iteration 102 : 0.6731121565405521
Loss in iteration 103 : 0.6734136350357758
Loss in iteration 104 : 0.6727738778926896
Loss in iteration 105 : 0.6718771121349132
Loss in iteration 106 : 0.6732421991393129
Loss in iteration 107 : 0.6721677338723377
Loss in iteration 108 : 0.6713905854082073
Loss in iteration 109 : 0.6715185252493262
Loss in iteration 110 : 0.6729158469874029
Loss in iteration 111 : 0.6713837988523517
Loss in iteration 112 : 0.6719477970102387
Loss in iteration 113 : 0.6706368320117777
Loss in iteration 114 : 0.6701092454246038
Loss in iteration 115 : 0.6706213127118003
Loss in iteration 116 : 0.6709175313620888
Loss in iteration 117 : 0.669895815881293
Loss in iteration 118 : 0.6699510089080012
Loss in iteration 119 : 0.6694416343555145
Loss in iteration 120 : 0.6712451589260533
Loss in iteration 121 : 0.6690761871716152
Loss in iteration 122 : 0.6700555742631813
Loss in iteration 123 : 0.6706273357948886
Loss in iteration 124 : 0.6686718133851148
Loss in iteration 125 : 0.6682278689377027
Loss in iteration 126 : 0.6704673764411873
Loss in iteration 127 : 0.6691975530081468
Loss in iteration 128 : 0.667508680408213
Loss in iteration 129 : 0.6689740917958047
Loss in iteration 130 : 0.6672363585655108
Loss in iteration 131 : 0.6685671867080792
Loss in iteration 132 : 0.666224556026815
Loss in iteration 133 : 0.6674589690227553
Loss in iteration 134 : 0.6657901826537936
Loss in iteration 135 : 0.6655242208632173
Loss in iteration 136 : 0.6674135142211685
Loss in iteration 137 : 0.6677434385493181
Loss in iteration 138 : 0.6651925196940214
Loss in iteration 139 : 0.6660466112817627
Loss in iteration 140 : 0.6658852474079276
Loss in iteration 141 : 0.6681548550692327
Loss in iteration 142 : 0.66544692814571
Loss in iteration 143 : 0.6639040221704651
Loss in iteration 144 : 0.6652330975164438
Loss in iteration 145 : 0.664159700756938
Loss in iteration 146 : 0.6649920837938817
Loss in iteration 147 : 0.6655410378185769
Loss in iteration 148 : 0.6660876763155313
Loss in iteration 149 : 0.6647996070340219
Loss in iteration 150 : 0.6651731423845115
Loss in iteration 151 : 0.6654523794303494
Loss in iteration 152 : 0.6643060982785726
Loss in iteration 153 : 0.6641407082625759
Loss in iteration 154 : 0.6646133933149728
Loss in iteration 155 : 0.6626030214529977
Loss in iteration 156 : 0.6626971996014778
Loss in iteration 157 : 0.6634448131879316
Loss in iteration 158 : 0.6624733777586596
Loss in iteration 159 : 0.662127448266764
Loss in iteration 160 : 0.6616120184454752
Loss in iteration 161 : 0.66197560650267
Loss in iteration 162 : 0.6623729175143265
Loss in iteration 163 : 0.6650991563988393
Loss in iteration 164 : 0.6617219644637926
Loss in iteration 165 : 0.6621637697629604
Loss in iteration 166 : 0.6624473959923729
Loss in iteration 167 : 0.6609477889145687
Loss in iteration 168 : 0.6614633676128009
Loss in iteration 169 : 0.6601729706600771
Loss in iteration 170 : 0.661011104768248
Loss in iteration 171 : 0.6595563660802913
Loss in iteration 172 : 0.6603329381052992
Loss in iteration 173 : 0.6586281552288724
Loss in iteration 174 : 0.660319162516109
Loss in iteration 175 : 0.6585619824769439
Loss in iteration 176 : 0.6616700948390346
Loss in iteration 177 : 0.6589459474672856
Loss in iteration 178 : 0.6580609643273624
Loss in iteration 179 : 0.6573943295267654
Loss in iteration 180 : 0.6584059769392654
Loss in iteration 181 : 0.6592770366513976
Loss in iteration 182 : 0.6583673488944717
Loss in iteration 183 : 0.6589572967198181
Loss in iteration 184 : 0.6573712755651815
Loss in iteration 185 : 0.6587859450298591
Loss in iteration 186 : 0.6567822460751352
Loss in iteration 187 : 0.6553766976342923
Loss in iteration 188 : 0.657860233720423
Loss in iteration 189 : 0.6576729627401541
Loss in iteration 190 : 0.6575353124075958
Loss in iteration 191 : 0.6573975035774433
Loss in iteration 192 : 0.6548825985343616
Loss in iteration 193 : 0.6565150221975925
Loss in iteration 194 : 0.6549944829341455
Loss in iteration 195 : 0.6570131850173415
Loss in iteration 196 : 0.6589547357083592
Loss in iteration 197 : 0.6562908359928611
Loss in iteration 198 : 0.6555151258113502
Loss in iteration 199 : 0.6563159438510384
Loss in iteration 200 : 0.6560198182292188
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.6495, training accuracy 0.66525, time elapsed: 4805 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 1.4903898020537105
Loss in iteration 3 : 1.8552754657457025
Loss in iteration 4 : 0.9425050256776626
Loss in iteration 5 : 0.8027112085660816
Loss in iteration 6 : 0.7585507908339341
Loss in iteration 7 : 0.6977202377597254
Loss in iteration 8 : 0.6999957358439237
Loss in iteration 9 : 0.6716942217648872
Loss in iteration 10 : 0.7052537683164068
Loss in iteration 11 : 0.6369685680252607
Loss in iteration 12 : 0.6385940095513253
Loss in iteration 13 : 0.61235076868375
Loss in iteration 14 : 0.62291242009179
Loss in iteration 15 : 0.6180656026743276
Loss in iteration 16 : 0.6223927499290997
Loss in iteration 17 : 0.6246886933365251
Loss in iteration 18 : 0.6413932400221639
Loss in iteration 19 : 0.6464822121928718
Loss in iteration 20 : 0.6617653590978637
Loss in iteration 21 : 0.6246346907752951
Loss in iteration 22 : 0.6482184668091152
Loss in iteration 23 : 0.5956930964692075
Loss in iteration 24 : 0.5893762525818735
Loss in iteration 25 : 0.5657249557519451
Loss in iteration 26 : 0.5801037131124631
Loss in iteration 27 : 0.5805368278104703
Loss in iteration 28 : 0.6222079281846433
Loss in iteration 29 : 0.6012711984416952
Loss in iteration 30 : 0.618743125179276
Loss in iteration 31 : 0.6040075035787209
Loss in iteration 32 : 0.623074633834997
Loss in iteration 33 : 0.6123098332838719
Loss in iteration 34 : 0.6353256955728378
Loss in iteration 35 : 0.5980005849158574
Loss in iteration 36 : 0.5927696009681661
Loss in iteration 37 : 0.5820977655122449
Loss in iteration 38 : 0.5773400343709394
Loss in iteration 39 : 0.5706573917328902
Loss in iteration 40 : 0.5747390985921074
Loss in iteration 41 : 0.5851002656727647
Loss in iteration 42 : 0.6005935459344064
Loss in iteration 43 : 0.5886153724100802
Loss in iteration 44 : 0.5790289367039866
Loss in iteration 45 : 0.5880702635264535
Loss in iteration 46 : 0.5932997701369646
Loss in iteration 47 : 0.5987316957826168
Loss in iteration 48 : 0.6057498306472912
Loss in iteration 49 : 0.6151496617493746
Loss in iteration 50 : 0.6034935455872331
Loss in iteration 51 : 0.5697044570991588
Loss in iteration 52 : 0.5990969012696493
Loss in iteration 53 : 0.5863488818767943
Loss in iteration 54 : 0.5628330338697831
Loss in iteration 55 : 0.5602612886976307
Loss in iteration 56 : 0.5833593222615635
Loss in iteration 57 : 0.5822840822367893
Loss in iteration 58 : 0.6003844950676933
Loss in iteration 59 : 0.5895000589931205
Loss in iteration 60 : 0.5904834838704868
Loss in iteration 61 : 0.5879189307884651
Loss in iteration 62 : 0.5812908341000018
Loss in iteration 63 : 0.5703391841316013
Loss in iteration 64 : 0.5671153106737659
Loss in iteration 65 : 0.5741528238400596
Loss in iteration 66 : 0.6037113497852937
Loss in iteration 67 : 0.5886054608001166
Loss in iteration 68 : 0.6193200616563893
Loss in iteration 69 : 0.5863786256349873
Loss in iteration 70 : 0.5637633369647039
Loss in iteration 71 : 0.5657485244000956
Loss in iteration 72 : 0.5706982431662665
Loss in iteration 73 : 0.5750493321242305
Loss in iteration 74 : 0.5774574965471113
Loss in iteration 75 : 0.5838117434984523
Loss in iteration 76 : 0.5787891933168816
Loss in iteration 77 : 0.5742744591013315
Loss in iteration 78 : 0.5723944356694519
Loss in iteration 79 : 0.5579068755699451
Loss in iteration 80 : 0.5590698740099068
Loss in iteration 81 : 0.5756669272845619
Loss in iteration 82 : 0.5697877350967402
Loss in iteration 83 : 0.5884010287732331
Loss in iteration 84 : 0.5899750668825119
Loss in iteration 85 : 0.5819661850118487
Loss in iteration 86 : 0.5799747236649327
Loss in iteration 87 : 0.574401851729093
Loss in iteration 88 : 0.5689062432343263
Loss in iteration 89 : 0.5442332598029327
Loss in iteration 90 : 0.5531148139993511
Loss in iteration 91 : 0.5610512911219371
Loss in iteration 92 : 0.573468359332008
Loss in iteration 93 : 0.5616449691964467
Loss in iteration 94 : 0.5584432475044897
Loss in iteration 95 : 0.5609349649999275
Loss in iteration 96 : 0.5748524734616838
Loss in iteration 97 : 0.5980669695894766
Loss in iteration 98 : 0.5919115564844595
Loss in iteration 99 : 0.5622111947468328
Loss in iteration 100 : 0.5613778565252547
Loss in iteration 101 : 0.5629090510637108
Loss in iteration 102 : 0.5680838141484277
Loss in iteration 103 : 0.5690870390636962
Loss in iteration 104 : 0.5694658871344402
Loss in iteration 105 : 0.5550067640145759
Loss in iteration 106 : 0.5782715364371098
Loss in iteration 107 : 0.5743799103936862
Loss in iteration 108 : 0.5691390221726523
Loss in iteration 109 : 0.5877306464529463
Loss in iteration 110 : 0.5995114124900182
Loss in iteration 111 : 0.5888241790132321
Loss in iteration 112 : 0.5865381923820581
Loss in iteration 113 : 0.5764099759413743
Loss in iteration 114 : 0.5791122780261483
Loss in iteration 115 : 0.554965004739172
Loss in iteration 116 : 0.542795711692712
Loss in iteration 117 : 0.5423220421411914
Loss in iteration 118 : 0.5465538251067571
Loss in iteration 119 : 0.5478359892138418
Loss in iteration 120 : 0.5747735664548826
Loss in iteration 121 : 0.575899245935757
Loss in iteration 122 : 0.5688679752913804
Loss in iteration 123 : 0.5606127828333125
Loss in iteration 124 : 0.5710206046353459
Loss in iteration 125 : 0.5823656716793306
Loss in iteration 126 : 0.5910517032239672
Loss in iteration 127 : 0.5966905729001438
Loss in iteration 128 : 0.5513449823376353
Loss in iteration 129 : 0.5404710325516238
Loss in iteration 130 : 0.543362915370665
Loss in iteration 131 : 0.5545465690623349
Loss in iteration 132 : 0.538861445822284
Loss in iteration 133 : 0.5419486273243355
Loss in iteration 134 : 0.5419761018693692
Loss in iteration 135 : 0.5544825996827573
Loss in iteration 136 : 0.5853818292277146
Loss in iteration 137 : 0.5711099368985189
Loss in iteration 138 : 0.5712471808696398
Loss in iteration 139 : 0.557109933182692
Loss in iteration 140 : 0.5792169393493501
Loss in iteration 141 : 0.5671880686168587
Loss in iteration 142 : 0.5688114155184841
Loss in iteration 143 : 0.552116956985823
Loss in iteration 144 : 0.5594138494641109
Loss in iteration 145 : 0.5514761410238519
Loss in iteration 146 : 0.5480294997235584
Loss in iteration 147 : 0.5615269923303219
Loss in iteration 148 : 0.5700997073080628
Loss in iteration 149 : 0.5874001555911309
Loss in iteration 150 : 0.5966141234470135
Loss in iteration 151 : 0.5745907118624961
Loss in iteration 152 : 0.578724941795287
Loss in iteration 153 : 0.5512369524800902
Loss in iteration 154 : 0.5484683184469759
Loss in iteration 155 : 0.5516187245611085
Loss in iteration 156 : 0.5549929943466774
Loss in iteration 157 : 0.5440103321338434
Loss in iteration 158 : 0.5355614639812551
Loss in iteration 159 : 0.5497659818694449
Loss in iteration 160 : 0.5558255642580464
Loss in iteration 161 : 0.5634974606434672
Loss in iteration 162 : 0.556607259698447
Loss in iteration 163 : 0.5466774176754234
Loss in iteration 164 : 0.5425665569977228
Loss in iteration 165 : 0.5545564881671803
Loss in iteration 166 : 0.5624018728671885
Loss in iteration 167 : 0.5934593500594377
Loss in iteration 168 : 0.5864614982930119
Loss in iteration 169 : 0.5855207075759714
Loss in iteration 170 : 0.5695221751589862
Loss in iteration 171 : 0.5764459258504977
Loss in iteration 172 : 0.5571030422649585
Loss in iteration 173 : 0.5441382958067943
Loss in iteration 174 : 0.5490999160639957
Loss in iteration 175 : 0.5560960557579648
Loss in iteration 176 : 0.5629561853539298
Loss in iteration 177 : 0.5658962612603693
Loss in iteration 178 : 0.5707928720337602
Loss in iteration 179 : 0.5646145853079709
Loss in iteration 180 : 0.5612141708356847
Loss in iteration 181 : 0.5591138567996248
Loss in iteration 182 : 0.5700249794122242
Loss in iteration 183 : 0.5684363834329462
Loss in iteration 184 : 0.5486420784804514
Loss in iteration 185 : 0.5374703526270534
Loss in iteration 186 : 0.544273273888177
Loss in iteration 187 : 0.5498288535494089
Loss in iteration 188 : 0.5730914400953677
Loss in iteration 189 : 0.5680635949462637
Loss in iteration 190 : 0.5849467526466987
Loss in iteration 191 : 0.5850316876951126
Loss in iteration 192 : 0.5881169953584893
Loss in iteration 193 : 0.5522916220918427
Loss in iteration 194 : 0.5450182415958271
Loss in iteration 195 : 0.5402831081018766
Loss in iteration 196 : 0.5562431133747291
Loss in iteration 197 : 0.5571952688276146
Loss in iteration 198 : 0.5741124240745386
Loss in iteration 199 : 0.5791881531558324
Loss in iteration 200 : 0.5803505722905553
Testing accuracy  of updater 5 on alg 0 with rate 0.19999999999999998 = 0.7435, training accuracy 0.738875, time elapsed: 4496 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6970982466841156
Loss in iteration 3 : 0.7914391103096963
Loss in iteration 4 : 0.9484627114877845
Loss in iteration 5 : 0.7808543518602586
Loss in iteration 6 : 0.774692709804548
Loss in iteration 7 : 0.6830493051195189
Loss in iteration 8 : 0.6811752879272076
Loss in iteration 9 : 0.6469100641655621
Loss in iteration 10 : 0.6467499116605859
Loss in iteration 11 : 0.6124317241817899
Loss in iteration 12 : 0.6023622855673381
Loss in iteration 13 : 0.594178002132722
Loss in iteration 14 : 0.5963837669762494
Loss in iteration 15 : 0.5996176944454452
Loss in iteration 16 : 0.5973677657295826
Loss in iteration 17 : 0.6019011297289417
Loss in iteration 18 : 0.6140310156735171
Loss in iteration 19 : 0.6214341102942624
Loss in iteration 20 : 0.6402744980767829
Loss in iteration 21 : 0.6125412958689717
Loss in iteration 22 : 0.6181290210775592
Loss in iteration 23 : 0.5780710859141068
Loss in iteration 24 : 0.5762351543519251
Loss in iteration 25 : 0.5581838073779819
Loss in iteration 26 : 0.5582262824010278
Loss in iteration 27 : 0.5533003186862058
Loss in iteration 28 : 0.5750284710035588
Loss in iteration 29 : 0.5662134184258579
Loss in iteration 30 : 0.5783785070537806
Loss in iteration 31 : 0.5714232852670563
Loss in iteration 32 : 0.5870697409877227
Loss in iteration 33 : 0.5844760639334148
Loss in iteration 34 : 0.5955590866373052
Loss in iteration 35 : 0.5664235841957966
Loss in iteration 36 : 0.5630752164247896
Loss in iteration 37 : 0.5527811066640527
Loss in iteration 38 : 0.5463137743858046
Loss in iteration 39 : 0.5399213994880285
Loss in iteration 40 : 0.5380914925633327
Loss in iteration 41 : 0.5449962223960135
Loss in iteration 42 : 0.553780350955502
Loss in iteration 43 : 0.5529116389372984
Loss in iteration 44 : 0.5453324881520705
Loss in iteration 45 : 0.5531820514276266
Loss in iteration 46 : 0.5544804205476609
Loss in iteration 47 : 0.5616510458326248
Loss in iteration 48 : 0.5671781269948708
Loss in iteration 49 : 0.5719286019868949
Loss in iteration 50 : 0.5611372707395135
Loss in iteration 51 : 0.533549726371935
Loss in iteration 52 : 0.5515510233074783
Loss in iteration 53 : 0.5451966316572594
Loss in iteration 54 : 0.5282343813895816
Loss in iteration 55 : 0.5268308649555807
Loss in iteration 56 : 0.5405495132372679
Loss in iteration 57 : 0.5408171334494385
Loss in iteration 58 : 0.5536856299219096
Loss in iteration 59 : 0.5471254790001103
Loss in iteration 60 : 0.5472397597645
Loss in iteration 61 : 0.5437261684842734
Loss in iteration 62 : 0.5411374715405386
Loss in iteration 63 : 0.5347007599552286
Loss in iteration 64 : 0.5291735082390343
Loss in iteration 65 : 0.5334885897279352
Loss in iteration 66 : 0.5570744828599433
Loss in iteration 67 : 0.5448914468971727
Loss in iteration 68 : 0.5660896678856641
Loss in iteration 69 : 0.5423430928811406
Loss in iteration 70 : 0.5259717479070284
Loss in iteration 71 : 0.529660749358238
Loss in iteration 72 : 0.5287499589348824
Loss in iteration 73 : 0.5312546173614142
Loss in iteration 74 : 0.530092357106838
Loss in iteration 75 : 0.5356762837201648
Loss in iteration 76 : 0.5313819927371277
Loss in iteration 77 : 0.526907199274124
Loss in iteration 78 : 0.5262180824578031
Loss in iteration 79 : 0.5241092464984635
Loss in iteration 80 : 0.5263401516782402
Loss in iteration 81 : 0.5362991860883373
Loss in iteration 82 : 0.527885963689647
Loss in iteration 83 : 0.5428507948208205
Loss in iteration 84 : 0.5401335374044661
Loss in iteration 85 : 0.5387089723254873
Loss in iteration 86 : 0.5333553937550705
Loss in iteration 87 : 0.5279831218506391
Loss in iteration 88 : 0.527739061925798
Loss in iteration 89 : 0.5104619253879202
Loss in iteration 90 : 0.5160486941076117
Loss in iteration 91 : 0.5204220913097257
Loss in iteration 92 : 0.5281215313566391
Loss in iteration 93 : 0.5215263408355991
Loss in iteration 94 : 0.5165992223021462
Loss in iteration 95 : 0.519526170243324
Loss in iteration 96 : 0.5294735884273432
Loss in iteration 97 : 0.5463760555141125
Loss in iteration 98 : 0.542688603740027
Loss in iteration 99 : 0.521396022255771
Loss in iteration 100 : 0.5206865814315882
Loss in iteration 101 : 0.5214873839211283
Loss in iteration 102 : 0.5232437238753025
Loss in iteration 103 : 0.5257859215987922
Loss in iteration 104 : 0.525839362619356
Loss in iteration 105 : 0.5129140746918937
Loss in iteration 106 : 0.53310350480441
Loss in iteration 107 : 0.5288653479991211
Loss in iteration 108 : 0.520799805040147
Loss in iteration 109 : 0.5387863962777364
Loss in iteration 110 : 0.544268177920942
Loss in iteration 111 : 0.5369280283922175
Loss in iteration 112 : 0.538433255982533
Loss in iteration 113 : 0.5308314675272741
Loss in iteration 114 : 0.534427304913328
Loss in iteration 115 : 0.5172329461736415
Loss in iteration 116 : 0.5056773728243398
Loss in iteration 117 : 0.5048465572536008
Loss in iteration 118 : 0.507308376078955
Loss in iteration 119 : 0.5073941700996726
Loss in iteration 120 : 0.5243558209541703
Loss in iteration 121 : 0.523969342405703
Loss in iteration 122 : 0.5248807230766755
Loss in iteration 123 : 0.521149828822505
Loss in iteration 124 : 0.5268274952849928
Loss in iteration 125 : 0.5363940346938947
Loss in iteration 126 : 0.5405421419224546
Loss in iteration 127 : 0.5436979713484444
Loss in iteration 128 : 0.5116750573317448
Loss in iteration 129 : 0.506026856518813
Loss in iteration 130 : 0.5046781854984492
Loss in iteration 131 : 0.5103145773653782
Loss in iteration 132 : 0.5001673643908862
Loss in iteration 133 : 0.5032774935703833
Loss in iteration 134 : 0.5015765384143261
Loss in iteration 135 : 0.5084158702144809
Loss in iteration 136 : 0.5320313855054112
Loss in iteration 137 : 0.5236456796905585
Loss in iteration 138 : 0.5278049148209417
Loss in iteration 139 : 0.5181740414425146
Loss in iteration 140 : 0.5356483855965761
Loss in iteration 141 : 0.5276229759807701
Loss in iteration 142 : 0.525099260460552
Loss in iteration 143 : 0.5085686491281375
Loss in iteration 144 : 0.5134874621441866
Loss in iteration 145 : 0.5077880960356336
Loss in iteration 146 : 0.5039324147918699
Loss in iteration 147 : 0.5153179837804428
Loss in iteration 148 : 0.5204808197089932
Loss in iteration 149 : 0.5370882884931372
Loss in iteration 150 : 0.5402455532364069
Loss in iteration 151 : 0.5276575552094882
Loss in iteration 152 : 0.5287416366250949
Loss in iteration 153 : 0.5090041556464792
Loss in iteration 154 : 0.5079006834485279
Loss in iteration 155 : 0.5096136114602566
Loss in iteration 156 : 0.5143702451297621
Loss in iteration 157 : 0.505744054492953
Loss in iteration 158 : 0.4996795648556559
Loss in iteration 159 : 0.5118824113207505
Loss in iteration 160 : 0.5108707951830279
Loss in iteration 161 : 0.5156533120536491
Loss in iteration 162 : 0.5102588996411825
Loss in iteration 163 : 0.5061849920388085
Loss in iteration 164 : 0.5030350763432697
Loss in iteration 165 : 0.5110823678697846
Loss in iteration 166 : 0.5141979753975121
Loss in iteration 167 : 0.541660667850282
Loss in iteration 168 : 0.5360892485834872
Loss in iteration 169 : 0.5384381396901632
Loss in iteration 170 : 0.5198114193335314
Loss in iteration 171 : 0.5240099608138387
Loss in iteration 172 : 0.5155774115362675
Loss in iteration 173 : 0.5081659393765758
Loss in iteration 174 : 0.5106146864777049
Loss in iteration 175 : 0.5139011962245005
Loss in iteration 176 : 0.5178268171925202
Loss in iteration 177 : 0.5205250051069386
Loss in iteration 178 : 0.523514217330941
Loss in iteration 179 : 0.5175316392502314
Loss in iteration 180 : 0.5149773019631974
Loss in iteration 181 : 0.5157494649730401
Loss in iteration 182 : 0.5229843402146859
Loss in iteration 183 : 0.521073714406695
Loss in iteration 184 : 0.509694996821906
Loss in iteration 185 : 0.49950971763590596
Loss in iteration 186 : 0.5032082048673259
Loss in iteration 187 : 0.5044475497992311
Loss in iteration 188 : 0.5248869577123337
Loss in iteration 189 : 0.5201641745051084
Loss in iteration 190 : 0.5318827355851444
Loss in iteration 191 : 0.5340058225867996
Loss in iteration 192 : 0.5362023582236087
Loss in iteration 193 : 0.5102304857400806
Loss in iteration 194 : 0.5060570431265343
Loss in iteration 195 : 0.5029597567863903
Loss in iteration 196 : 0.5137131223715814
Loss in iteration 197 : 0.5093606202604212
Loss in iteration 198 : 0.5206220817285505
Loss in iteration 199 : 0.5282511073476682
Loss in iteration 200 : 0.5271201163206343
Testing accuracy  of updater 5 on alg 0 with rate 0.13999999999999999 = 0.754, training accuracy 0.74925, time elapsed: 4720 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6846426255950097
Loss in iteration 3 : 0.6854775029524198
Loss in iteration 4 : 0.6988427584143216
Loss in iteration 5 : 0.6825715006296273
Loss in iteration 6 : 0.6697393106958894
Loss in iteration 7 : 0.6403558854880702
Loss in iteration 8 : 0.6291179962976259
Loss in iteration 9 : 0.6158339806979299
Loss in iteration 10 : 0.6102036539735128
Loss in iteration 11 : 0.5944090874308257
Loss in iteration 12 : 0.5839566803808443
Loss in iteration 13 : 0.5816319241438197
Loss in iteration 14 : 0.5756685836415941
Loss in iteration 15 : 0.5777825132631614
Loss in iteration 16 : 0.5672711058213975
Loss in iteration 17 : 0.5710448391177807
Loss in iteration 18 : 0.5707143548659408
Loss in iteration 19 : 0.5810764458488167
Loss in iteration 20 : 0.5956851320486338
Loss in iteration 21 : 0.5895688648914006
Loss in iteration 22 : 0.5898173546234918
Loss in iteration 23 : 0.562058372675437
Loss in iteration 24 : 0.5550340154769327
Loss in iteration 25 : 0.5387273805146342
Loss in iteration 26 : 0.5370306985007542
Loss in iteration 27 : 0.5304515304991053
Loss in iteration 28 : 0.5414887156443049
Loss in iteration 29 : 0.5335311128028438
Loss in iteration 30 : 0.5360957030778298
Loss in iteration 31 : 0.5335734416148189
Loss in iteration 32 : 0.5432237573936597
Loss in iteration 33 : 0.5545632838052247
Loss in iteration 34 : 0.5599994514119521
Loss in iteration 35 : 0.5433375440224516
Loss in iteration 36 : 0.5383296150570264
Loss in iteration 37 : 0.532144605779205
Loss in iteration 38 : 0.5244683502542858
Loss in iteration 39 : 0.5181120268690076
Loss in iteration 40 : 0.5119543459039789
Loss in iteration 41 : 0.5154154887311684
Loss in iteration 42 : 0.5154937921619897
Loss in iteration 43 : 0.5184066009178537
Loss in iteration 44 : 0.5125707116798083
Loss in iteration 45 : 0.5245226672944645
Loss in iteration 46 : 0.5211217549157638
Loss in iteration 47 : 0.5306639598345602
Loss in iteration 48 : 0.5333806485876471
Loss in iteration 49 : 0.5391590782733984
Loss in iteration 50 : 0.5271728223605358
Loss in iteration 51 : 0.5075839379591015
Loss in iteration 52 : 0.5183147075835929
Loss in iteration 53 : 0.5126784763652656
Loss in iteration 54 : 0.5021357813083834
Loss in iteration 55 : 0.5026365106140144
Loss in iteration 56 : 0.5093631985542386
Loss in iteration 57 : 0.5099979234650404
Loss in iteration 58 : 0.5187274317727201
Loss in iteration 59 : 0.517525203575742
Loss in iteration 60 : 0.5152676525545928
Loss in iteration 61 : 0.5112991794455974
Loss in iteration 62 : 0.5090654870428404
Loss in iteration 63 : 0.5081018267318825
Loss in iteration 64 : 0.5023294098743938
Loss in iteration 65 : 0.5037373529097912
Loss in iteration 66 : 0.5211795136152086
Loss in iteration 67 : 0.5097504445396341
Loss in iteration 68 : 0.5220066029278558
Loss in iteration 69 : 0.5092743847719269
Loss in iteration 70 : 0.5000317365254519
Loss in iteration 71 : 0.5058322534408934
Loss in iteration 72 : 0.5007440135350444
Loss in iteration 73 : 0.5061340159170276
Loss in iteration 74 : 0.5026280993529751
Loss in iteration 75 : 0.507046727099577
Loss in iteration 76 : 0.4986188575845185
Loss in iteration 77 : 0.4969489957853481
Loss in iteration 78 : 0.49501722827599914
Loss in iteration 79 : 0.4982580172569832
Loss in iteration 80 : 0.497664850932809
Loss in iteration 81 : 0.5068259141589715
Loss in iteration 82 : 0.4991361098690628
Loss in iteration 83 : 0.5104706518880582
Loss in iteration 84 : 0.5016140031157577
Loss in iteration 85 : 0.5041255249124487
Loss in iteration 86 : 0.4987102420442721
Loss in iteration 87 : 0.49505673872406725
Loss in iteration 88 : 0.4971801663821518
Loss in iteration 89 : 0.4871044047027323
Loss in iteration 90 : 0.4918061996421508
Loss in iteration 91 : 0.4931702133137621
Loss in iteration 92 : 0.49744062776798914
Loss in iteration 93 : 0.4943916201356468
Loss in iteration 94 : 0.4881693118533685
Loss in iteration 95 : 0.4911934355368876
Loss in iteration 96 : 0.49680093654743457
Loss in iteration 97 : 0.5102084515780737
Loss in iteration 98 : 0.5067069288904331
Loss in iteration 99 : 0.4927021791623287
Loss in iteration 100 : 0.49148223585359024
Loss in iteration 101 : 0.4937784049495808
Loss in iteration 102 : 0.49429546021076365
Loss in iteration 103 : 0.4983307971435449
Loss in iteration 104 : 0.4948119250463904
Loss in iteration 105 : 0.4815425489355952
Loss in iteration 106 : 0.49835347472535446
Loss in iteration 107 : 0.4951753750122883
Loss in iteration 108 : 0.4872848607979479
Loss in iteration 109 : 0.5052887663857002
Loss in iteration 110 : 0.5045905081962295
Loss in iteration 111 : 0.4994818371757824
Loss in iteration 112 : 0.5026065740509015
Loss in iteration 113 : 0.4973841880478597
Loss in iteration 114 : 0.5012322237866859
Loss in iteration 115 : 0.4906944475133414
Loss in iteration 116 : 0.48116772922928364
Loss in iteration 117 : 0.4819730742177133
Loss in iteration 118 : 0.48332548047630247
Loss in iteration 119 : 0.48364097737294215
Loss in iteration 120 : 0.4915996169132571
Loss in iteration 121 : 0.48826532195763017
Loss in iteration 122 : 0.4936682444028432
Loss in iteration 123 : 0.49284120061361847
Loss in iteration 124 : 0.49330179426597354
Loss in iteration 125 : 0.5000458291790961
Loss in iteration 126 : 0.5052947235570483
Loss in iteration 127 : 0.5083525068895964
Loss in iteration 128 : 0.4837128186386636
Loss in iteration 129 : 0.48353178195163266
Loss in iteration 130 : 0.47953330910614017
Loss in iteration 131 : 0.48374893339769537
Loss in iteration 132 : 0.47684879572656325
Loss in iteration 133 : 0.47999511145981766
Loss in iteration 134 : 0.4763053595725903
Loss in iteration 135 : 0.47622000660324865
Loss in iteration 136 : 0.48882313880373607
Loss in iteration 137 : 0.48367137159391593
Loss in iteration 138 : 0.49367442480439677
Loss in iteration 139 : 0.4920493624728981
Loss in iteration 140 : 0.5082568699749882
Loss in iteration 141 : 0.5008353219543056
Loss in iteration 142 : 0.49553927142186793
Loss in iteration 143 : 0.4800210741074854
Loss in iteration 144 : 0.48346307637410285
Loss in iteration 145 : 0.4799324446630312
Loss in iteration 146 : 0.474717855785462
Loss in iteration 147 : 0.4849988224672499
Loss in iteration 148 : 0.4848797339267508
Loss in iteration 149 : 0.4979672988556849
Loss in iteration 150 : 0.49933264233233543
Loss in iteration 151 : 0.4950967136062609
Loss in iteration 152 : 0.5008130019118351
Loss in iteration 153 : 0.4836878146225049
Loss in iteration 154 : 0.4826730676035447
Loss in iteration 155 : 0.4774177741931479
Loss in iteration 156 : 0.48376738603516484
Loss in iteration 157 : 0.4781606995825522
Loss in iteration 158 : 0.4718375663715174
Loss in iteration 159 : 0.48250763277725284
Loss in iteration 160 : 0.4779589211144101
Loss in iteration 161 : 0.48222259258222255
Loss in iteration 162 : 0.4812094793858043
Loss in iteration 163 : 0.4835555246126188
Loss in iteration 164 : 0.4798625777380623
Loss in iteration 165 : 0.4846989862490926
Loss in iteration 166 : 0.48289228218465136
Loss in iteration 167 : 0.5034743535488019
Loss in iteration 168 : 0.49568937647451566
Loss in iteration 169 : 0.5019091306061375
Loss in iteration 170 : 0.48874563060938797
Loss in iteration 171 : 0.492986363039956
Loss in iteration 172 : 0.48429617422704185
Loss in iteration 173 : 0.47887560552151637
Loss in iteration 174 : 0.477646880770482
Loss in iteration 175 : 0.4801852937585329
Loss in iteration 176 : 0.48624059954865856
Loss in iteration 177 : 0.48573362040701323
Loss in iteration 178 : 0.48522437208056085
Loss in iteration 179 : 0.4856113432050047
Loss in iteration 180 : 0.4878349546033942
Loss in iteration 181 : 0.4891793278495964
Loss in iteration 182 : 0.4925054542767681
Loss in iteration 183 : 0.4920333667889865
Loss in iteration 184 : 0.48165770054623946
Loss in iteration 185 : 0.4742203696977116
Loss in iteration 186 : 0.4755649874611445
Loss in iteration 187 : 0.474619452412314
Loss in iteration 188 : 0.49350492997489787
Loss in iteration 189 : 0.4899925287661193
Loss in iteration 190 : 0.49436391553492975
Loss in iteration 191 : 0.4943514633384864
Loss in iteration 192 : 0.4964114092176858
Loss in iteration 193 : 0.47922042636583034
Loss in iteration 194 : 0.47873211365482266
Loss in iteration 195 : 0.4786242757014075
Loss in iteration 196 : 0.48614514379578994
Loss in iteration 197 : 0.47792768911523187
Loss in iteration 198 : 0.4838336380662616
Loss in iteration 199 : 0.4916721376320789
Loss in iteration 200 : 0.4876638330671773
Testing accuracy  of updater 5 on alg 0 with rate 0.08 = 0.778, training accuracy 0.767375, time elapsed: 4569 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6823847825892725
Loss in iteration 3 : 0.6719951425031518
Loss in iteration 4 : 0.6665479994661749
Loss in iteration 5 : 0.6616040256213842
Loss in iteration 6 : 0.6539379677786225
Loss in iteration 7 : 0.647599802117189
Loss in iteration 8 : 0.642672111239279
Loss in iteration 9 : 0.6369291728538667
Loss in iteration 10 : 0.6337693896447919
Loss in iteration 11 : 0.6281566941821838
Loss in iteration 12 : 0.6218146394130314
Loss in iteration 13 : 0.6195025708244987
Loss in iteration 14 : 0.6143286458568333
Loss in iteration 15 : 0.614085019187118
Loss in iteration 16 : 0.6055979229548296
Loss in iteration 17 : 0.6058992995522875
Loss in iteration 18 : 0.6011061395437631
Loss in iteration 19 : 0.5974745385677227
Loss in iteration 20 : 0.5951774432869245
Loss in iteration 21 : 0.5902140931881285
Loss in iteration 22 : 0.589277536362287
Loss in iteration 23 : 0.583163995350091
Loss in iteration 24 : 0.5830582090918732
Loss in iteration 25 : 0.5753540283241273
Loss in iteration 26 : 0.5748556316474305
Loss in iteration 27 : 0.5665817059862385
Loss in iteration 28 : 0.5711765790137414
Loss in iteration 29 : 0.5632806611134683
Loss in iteration 30 : 0.5621970962995156
Loss in iteration 31 : 0.5553235756133436
Loss in iteration 32 : 0.5560747954189452
Loss in iteration 33 : 0.5579067918686744
Loss in iteration 34 : 0.5572625908529923
Loss in iteration 35 : 0.5522200395913804
Loss in iteration 36 : 0.5514277137637518
Loss in iteration 37 : 0.5510308516226634
Loss in iteration 38 : 0.546478689236025
Loss in iteration 39 : 0.5423436701066618
Loss in iteration 40 : 0.5376029423039823
Loss in iteration 41 : 0.5388470753288034
Loss in iteration 42 : 0.5338996523510157
Loss in iteration 43 : 0.5360873052861023
Loss in iteration 44 : 0.5291803128550695
Loss in iteration 45 : 0.5368778467450008
Loss in iteration 46 : 0.5265457224105465
Loss in iteration 47 : 0.5300950628404832
Loss in iteration 48 : 0.5276467138049945
Loss in iteration 49 : 0.5333896909289099
Loss in iteration 50 : 0.5295952182622169
Loss in iteration 51 : 0.5204988461539609
Loss in iteration 52 : 0.5312582391513214
Loss in iteration 53 : 0.5255532992606662
Loss in iteration 54 : 0.5182316389957667
Loss in iteration 55 : 0.5188360398034263
Loss in iteration 56 : 0.5196183336030171
Loss in iteration 57 : 0.5160791584446974
Loss in iteration 58 : 0.5214356758365871
Loss in iteration 59 : 0.5214355274204114
Loss in iteration 60 : 0.5175873205817251
Loss in iteration 61 : 0.5128959833259282
Loss in iteration 62 : 0.5130220269956903
Loss in iteration 63 : 0.5145919342906224
Loss in iteration 64 : 0.5090570033043851
Loss in iteration 65 : 0.5068405101607433
Loss in iteration 66 : 0.5175333939864086
Loss in iteration 67 : 0.5062960975505352
Loss in iteration 68 : 0.5162476857416763
Loss in iteration 69 : 0.5093527712074546
Loss in iteration 70 : 0.5026651315087444
Loss in iteration 71 : 0.5104968029378462
Loss in iteration 72 : 0.507583308993796
Loss in iteration 73 : 0.5130357273626956
Loss in iteration 74 : 0.5083821956409409
Loss in iteration 75 : 0.508764182876683
Loss in iteration 76 : 0.4998276375938467
Loss in iteration 77 : 0.4981175920081642
Loss in iteration 78 : 0.49325589076030585
Loss in iteration 79 : 0.49950591546602097
Loss in iteration 80 : 0.4992295766703892
Loss in iteration 81 : 0.5044461867541561
Loss in iteration 82 : 0.4955290065685421
Loss in iteration 83 : 0.5013263327823917
Loss in iteration 84 : 0.49290320563644036
Loss in iteration 85 : 0.5005765958989475
Loss in iteration 86 : 0.4978364897049269
Loss in iteration 87 : 0.4949234197977822
Loss in iteration 88 : 0.49811790998900723
Loss in iteration 89 : 0.49165554128050737
Loss in iteration 90 : 0.495916991784286
Loss in iteration 91 : 0.49417607346687564
Loss in iteration 92 : 0.4939540216666942
Loss in iteration 93 : 0.49622723388162576
Loss in iteration 94 : 0.4916098915011124
Loss in iteration 95 : 0.49433998969476606
Loss in iteration 96 : 0.4954284086459149
Loss in iteration 97 : 0.49805534954641145
Loss in iteration 98 : 0.4931470371997275
Loss in iteration 99 : 0.4879982891774106
Loss in iteration 100 : 0.4878927928338666
Loss in iteration 101 : 0.4895633778769737
Loss in iteration 102 : 0.4898838572498801
Loss in iteration 103 : 0.4953823357357205
Loss in iteration 104 : 0.49247419936954306
Loss in iteration 105 : 0.48268180754955176
Loss in iteration 106 : 0.49534327761377084
Loss in iteration 107 : 0.49075612148958836
Loss in iteration 108 : 0.4819051282143254
Loss in iteration 109 : 0.4952139343735396
Loss in iteration 110 : 0.48535402173519965
Loss in iteration 111 : 0.4805256693307626
Loss in iteration 112 : 0.48756310739679476
Loss in iteration 113 : 0.4854822703652532
Loss in iteration 114 : 0.49299114230936375
Loss in iteration 115 : 0.489856736581083
Loss in iteration 116 : 0.4837041756456332
Loss in iteration 117 : 0.4848412142761577
Loss in iteration 118 : 0.4855333335639357
Loss in iteration 119 : 0.4862323388451544
Loss in iteration 120 : 0.4893255474219299
Loss in iteration 121 : 0.4821662231279551
Loss in iteration 122 : 0.4874134642156279
Loss in iteration 123 : 0.48480439597630043
Loss in iteration 124 : 0.4799274845832535
Loss in iteration 125 : 0.4819120394532769
Loss in iteration 126 : 0.4890702777854589
Loss in iteration 127 : 0.49464828381518566
Loss in iteration 128 : 0.4784829200332084
Loss in iteration 129 : 0.4848602680696353
Loss in iteration 130 : 0.48016908503930383
Loss in iteration 131 : 0.48510425186861256
Loss in iteration 132 : 0.47926481747441496
Loss in iteration 133 : 0.4820277945710023
Loss in iteration 134 : 0.4813424467923234
Loss in iteration 135 : 0.48073429007883695
Loss in iteration 136 : 0.48604903762214263
Loss in iteration 137 : 0.4771363520704114
Loss in iteration 138 : 0.4802512278335898
Loss in iteration 139 : 0.4721272123026267
Loss in iteration 140 : 0.48200045705925354
Loss in iteration 141 : 0.4843516367401172
Loss in iteration 142 : 0.48149861929074267
Loss in iteration 143 : 0.47067419749930095
Loss in iteration 144 : 0.47813028898311505
Loss in iteration 145 : 0.47725866595546856
Loss in iteration 146 : 0.4723419069417306
Loss in iteration 147 : 0.480136209585147
Loss in iteration 148 : 0.4766705554594715
Loss in iteration 149 : 0.4808790114271584
Loss in iteration 150 : 0.47924861656193884
Loss in iteration 151 : 0.47950647408974795
Loss in iteration 152 : 0.48825724271837334
Loss in iteration 153 : 0.47812801745790356
Loss in iteration 154 : 0.47979988629166753
Loss in iteration 155 : 0.47305707795242913
Loss in iteration 156 : 0.47693566442452984
Loss in iteration 157 : 0.471344927203653
Loss in iteration 158 : 0.4654218892515163
Loss in iteration 159 : 0.4752028169858434
Loss in iteration 160 : 0.4696549510797589
Loss in iteration 161 : 0.47151235587807466
Loss in iteration 162 : 0.47431640032462175
Loss in iteration 163 : 0.48143365036658153
Loss in iteration 164 : 0.48209391289654435
Loss in iteration 165 : 0.48431681663067727
Loss in iteration 166 : 0.4773371208612711
Loss in iteration 167 : 0.48627443907737666
Loss in iteration 168 : 0.46793168697856546
Loss in iteration 169 : 0.47538980937884634
Loss in iteration 170 : 0.47067161588894063
Loss in iteration 171 : 0.4775462930056562
Loss in iteration 172 : 0.47178054611800935
Loss in iteration 173 : 0.47143718289094116
Loss in iteration 174 : 0.4725206923393035
Loss in iteration 175 : 0.47541388666310086
Loss in iteration 176 : 0.4808114477867023
Loss in iteration 177 : 0.4793219565013218
Loss in iteration 178 : 0.47442437726823494
Loss in iteration 179 : 0.47452511986242013
Loss in iteration 180 : 0.4768479178900075
Loss in iteration 181 : 0.4784417720139633
Loss in iteration 182 : 0.47868078473574516
Loss in iteration 183 : 0.4780027550516707
Loss in iteration 184 : 0.4717971492556085
Loss in iteration 185 : 0.46725876443057746
Loss in iteration 186 : 0.46667464831101485
Loss in iteration 187 : 0.46235328972974
Loss in iteration 188 : 0.48167780475390354
Loss in iteration 189 : 0.4790290877739959
Loss in iteration 190 : 0.4789041970048324
Loss in iteration 191 : 0.47962361453777164
Loss in iteration 192 : 0.47882194379411647
Loss in iteration 193 : 0.46739559356282007
Loss in iteration 194 : 0.46969838930921376
Loss in iteration 195 : 0.4733876958948487
Loss in iteration 196 : 0.479089914198409
Loss in iteration 197 : 0.46739667642696014
Loss in iteration 198 : 0.4696191201161693
Loss in iteration 199 : 0.4758420602135476
Loss in iteration 200 : 0.47043487202535667
Testing accuracy  of updater 5 on alg 0 with rate 0.02 = 0.7825, training accuracy 0.78525, time elapsed: 4419 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6823401496523103
Loss in iteration 3 : 0.6700587938866532
Loss in iteration 4 : 0.6633006241545623
Loss in iteration 5 : 0.657611861477307
Loss in iteration 6 : 0.6502947471162323
Loss in iteration 7 : 0.6440346019984446
Loss in iteration 8 : 0.6392879772125458
Loss in iteration 9 : 0.6340187396204612
Loss in iteration 10 : 0.6315050941824425
Loss in iteration 11 : 0.6260210692548165
Loss in iteration 12 : 0.6203498848712758
Loss in iteration 13 : 0.618873649357941
Loss in iteration 14 : 0.6141729152174851
Loss in iteration 15 : 0.6145573019868614
Loss in iteration 16 : 0.6065605304387796
Loss in iteration 17 : 0.6076351344578369
Loss in iteration 18 : 0.6033789394166941
Loss in iteration 19 : 0.6002210197700224
Loss in iteration 20 : 0.5985786433932022
Loss in iteration 21 : 0.5941011076488582
Loss in iteration 22 : 0.5935133362385704
Loss in iteration 23 : 0.587888133149375
Loss in iteration 24 : 0.588451863479252
Loss in iteration 25 : 0.5813353301470693
Loss in iteration 26 : 0.5811511269384935
Loss in iteration 27 : 0.5735316907725729
Loss in iteration 28 : 0.5784540956811574
Loss in iteration 29 : 0.5710906261856407
Loss in iteration 30 : 0.5704011194428773
Loss in iteration 31 : 0.564029026475092
Loss in iteration 32 : 0.5654066712583554
Loss in iteration 33 : 0.5669766843849093
Loss in iteration 34 : 0.5668527796340996
Loss in iteration 35 : 0.5617134872266982
Loss in iteration 36 : 0.5606420856788549
Loss in iteration 37 : 0.5603925855866493
Loss in iteration 38 : 0.5565564365454929
Loss in iteration 39 : 0.5534184572241522
Loss in iteration 40 : 0.5491982828108983
Loss in iteration 41 : 0.5502084921953375
Loss in iteration 42 : 0.5448307816742649
Loss in iteration 43 : 0.5479235399972939
Loss in iteration 44 : 0.542364959205399
Loss in iteration 45 : 0.5501183896164877
Loss in iteration 46 : 0.5396776307771496
Loss in iteration 47 : 0.5426293680685348
Loss in iteration 48 : 0.5398100801813805
Loss in iteration 49 : 0.5432636731256304
Loss in iteration 50 : 0.5396575047668599
Loss in iteration 51 : 0.5319987471682582
Loss in iteration 52 : 0.5413162554177033
Loss in iteration 53 : 0.5358076286382274
Loss in iteration 54 : 0.5301838466322325
Loss in iteration 55 : 0.5309419058445293
Loss in iteration 56 : 0.5308689366892554
Loss in iteration 57 : 0.5274244203136687
Loss in iteration 58 : 0.5328050986891474
Loss in iteration 59 : 0.5328636249735459
Loss in iteration 60 : 0.5290249827196514
Loss in iteration 61 : 0.524421500886352
Loss in iteration 62 : 0.5249284378225847
Loss in iteration 63 : 0.5257685439909626
Loss in iteration 64 : 0.5202570916529233
Loss in iteration 65 : 0.5181200155182114
Loss in iteration 66 : 0.5274288736813213
Loss in iteration 67 : 0.5159840569305915
Loss in iteration 68 : 0.5246848582151825
Loss in iteration 69 : 0.5190768180322498
Loss in iteration 70 : 0.5132060319258595
Loss in iteration 71 : 0.5211054702902221
Loss in iteration 72 : 0.5179290730535062
Loss in iteration 73 : 0.5226453804069331
Loss in iteration 74 : 0.5182298251384202
Loss in iteration 75 : 0.5181213104683977
Loss in iteration 76 : 0.5098272223983816
Loss in iteration 77 : 0.5084633969624094
Loss in iteration 78 : 0.5036880569627689
Loss in iteration 79 : 0.5093187637388484
Loss in iteration 80 : 0.5093327846759316
Loss in iteration 81 : 0.5142565271239415
Loss in iteration 82 : 0.5061054444485712
Loss in iteration 83 : 0.5107567258491775
Loss in iteration 84 : 0.5020172891701136
Loss in iteration 85 : 0.508795494073609
Loss in iteration 86 : 0.5064908957538515
Loss in iteration 87 : 0.5011029421157532
Loss in iteration 88 : 0.5048854365157317
Loss in iteration 89 : 0.5018732170958284
Loss in iteration 90 : 0.5058295604665182
Loss in iteration 91 : 0.5023843430410164
Loss in iteration 92 : 0.5015254208054887
Loss in iteration 93 : 0.5055952771951471
Loss in iteration 94 : 0.50153935136276
Loss in iteration 95 : 0.5034649683047062
Loss in iteration 96 : 0.5032378669069699
Loss in iteration 97 : 0.50510919316508
Loss in iteration 98 : 0.5007411492738535
Loss in iteration 99 : 0.49456681933403535
Loss in iteration 100 : 0.49399441764373836
Loss in iteration 101 : 0.4964155289282522
Loss in iteration 102 : 0.49682351049648993
Loss in iteration 103 : 0.5025905918780611
Loss in iteration 104 : 0.49990709984875614
Loss in iteration 105 : 0.4912117918454048
Loss in iteration 106 : 0.5024689688856644
Loss in iteration 107 : 0.49808755516402203
Loss in iteration 108 : 0.48986356535684317
Loss in iteration 109 : 0.5026671737325182
Loss in iteration 110 : 0.4931297808261975
Loss in iteration 111 : 0.48852195135175924
Loss in iteration 112 : 0.49531235113754163
Loss in iteration 113 : 0.49285832144607256
Loss in iteration 114 : 0.4985145710544012
Loss in iteration 115 : 0.49512115525225464
Loss in iteration 116 : 0.4904818424770556
Loss in iteration 117 : 0.4917243937176477
Loss in iteration 118 : 0.49227745046640875
Loss in iteration 119 : 0.49309021634465466
Loss in iteration 120 : 0.49584081594451235
Loss in iteration 121 : 0.4885030262926561
Loss in iteration 122 : 0.4936565101067429
Loss in iteration 123 : 0.4913255948434614
Loss in iteration 124 : 0.486513412328508
Loss in iteration 125 : 0.48760023026266086
Loss in iteration 126 : 0.49480259141459043
Loss in iteration 127 : 0.4994918253439694
Loss in iteration 128 : 0.48460161673585234
Loss in iteration 129 : 0.49146567682849035
Loss in iteration 130 : 0.4862477795275216
Loss in iteration 131 : 0.4915671071779328
Loss in iteration 132 : 0.4859179006787559
Loss in iteration 133 : 0.48875150402448375
Loss in iteration 134 : 0.487939329422475
Loss in iteration 135 : 0.48485536699019655
Loss in iteration 136 : 0.49053192935296186
Loss in iteration 137 : 0.481496853642943
Loss in iteration 138 : 0.4845867114874294
Loss in iteration 139 : 0.4784342515829669
Loss in iteration 140 : 0.48753112751426886
Loss in iteration 141 : 0.4900343977200047
Loss in iteration 142 : 0.4867958656064671
Loss in iteration 143 : 0.4769860692393785
Loss in iteration 144 : 0.48400667107250833
Loss in iteration 145 : 0.4826347487091541
Loss in iteration 146 : 0.4781894772158782
Loss in iteration 147 : 0.4857580102925319
Loss in iteration 148 : 0.482582139155287
Loss in iteration 149 : 0.4863580880523533
Loss in iteration 150 : 0.48545449168992794
Loss in iteration 151 : 0.4853263179518657
Loss in iteration 152 : 0.49198471647683356
Loss in iteration 153 : 0.4822972749990133
Loss in iteration 154 : 0.48395445075435306
Loss in iteration 155 : 0.47631899706319303
Loss in iteration 156 : 0.4800691340020602
Loss in iteration 157 : 0.4759306589432073
Loss in iteration 158 : 0.4703382116650652
Loss in iteration 159 : 0.47988193709014676
Loss in iteration 160 : 0.4752557093753768
Loss in iteration 161 : 0.4768675274016872
Loss in iteration 162 : 0.479599038637941
Loss in iteration 163 : 0.48644472992608495
Loss in iteration 164 : 0.4871554351391564
Loss in iteration 165 : 0.487663597841322
Loss in iteration 166 : 0.4811886780421744
Loss in iteration 167 : 0.49057561561656055
Loss in iteration 168 : 0.47319156192877665
Loss in iteration 169 : 0.48027803314365264
Loss in iteration 170 : 0.4760450752335104
Loss in iteration 171 : 0.4826116965874204
Loss in iteration 172 : 0.4768320931551457
Loss in iteration 173 : 0.4759982533044835
Loss in iteration 174 : 0.4760269307900415
Loss in iteration 175 : 0.47777426842447035
Loss in iteration 176 : 0.4831665364821473
Loss in iteration 177 : 0.48223817123546586
Loss in iteration 178 : 0.4771886959323275
Loss in iteration 179 : 0.4783473445042611
Loss in iteration 180 : 0.48086667131428157
Loss in iteration 181 : 0.4825287965226581
Loss in iteration 182 : 0.48263009848879956
Loss in iteration 183 : 0.48212087772912937
Loss in iteration 184 : 0.4762875657541915
Loss in iteration 185 : 0.4716079440387511
Loss in iteration 186 : 0.47088064137201885
Loss in iteration 187 : 0.46643930966554115
Loss in iteration 188 : 0.4849686552714802
Loss in iteration 189 : 0.48267051663326715
Loss in iteration 190 : 0.4812509514250759
Loss in iteration 191 : 0.482277981652055
Loss in iteration 192 : 0.48129733674685005
Loss in iteration 193 : 0.47092004931576703
Loss in iteration 194 : 0.4733412360910198
Loss in iteration 195 : 0.4774752145327464
Loss in iteration 196 : 0.4830995870759649
Loss in iteration 197 : 0.47107620089699587
Loss in iteration 198 : 0.47299816780566
Loss in iteration 199 : 0.4784666191942783
Loss in iteration 200 : 0.4734273882339777
Testing accuracy  of updater 5 on alg 0 with rate 0.014 = 0.78, training accuracy 0.78375, time elapsed: 4354 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6843560930064337
Loss in iteration 3 : 0.6757676553855089
Loss in iteration 4 : 0.671580345836224
Loss in iteration 5 : 0.6678615014220082
Loss in iteration 6 : 0.6626735315600051
Loss in iteration 7 : 0.6582579023264672
Loss in iteration 8 : 0.6547222130201946
Loss in iteration 9 : 0.6507150392275879
Loss in iteration 10 : 0.6489704012171353
Loss in iteration 11 : 0.6457626035454103
Loss in iteration 12 : 0.6408865543171353
Loss in iteration 13 : 0.639865390339403
Loss in iteration 14 : 0.6369740388557553
Loss in iteration 15 : 0.6368639784980209
Loss in iteration 16 : 0.6307620208356285
Loss in iteration 17 : 0.631512468391615
Loss in iteration 18 : 0.628501350192587
Loss in iteration 19 : 0.6257305830590328
Loss in iteration 20 : 0.6248796886347071
Loss in iteration 21 : 0.6212937057526682
Loss in iteration 22 : 0.6204091900097477
Loss in iteration 23 : 0.6165380700925398
Loss in iteration 24 : 0.616326800476736
Loss in iteration 25 : 0.6109903943398888
Loss in iteration 26 : 0.610395005142098
Loss in iteration 27 : 0.6043933492170551
Loss in iteration 28 : 0.6083592698045839
Loss in iteration 29 : 0.6026093991062063
Loss in iteration 30 : 0.6017827520164631
Loss in iteration 31 : 0.5966891678049804
Loss in iteration 32 : 0.5983645012707797
Loss in iteration 33 : 0.598060191285379
Loss in iteration 34 : 0.5979525079774531
Loss in iteration 35 : 0.5938133599784542
Loss in iteration 36 : 0.5926840284564306
Loss in iteration 37 : 0.5929672325930767
Loss in iteration 38 : 0.590148673414488
Loss in iteration 39 : 0.5879816382466323
Loss in iteration 40 : 0.5842438973761154
Loss in iteration 41 : 0.5841105010130337
Loss in iteration 42 : 0.5789080980079969
Loss in iteration 43 : 0.5814390296164731
Loss in iteration 44 : 0.5763695428324764
Loss in iteration 45 : 0.5821335634120349
Loss in iteration 46 : 0.5742890236161587
Loss in iteration 47 : 0.5766398677377127
Loss in iteration 48 : 0.5741097218790434
Loss in iteration 49 : 0.5766048805935191
Loss in iteration 50 : 0.5731441642979173
Loss in iteration 51 : 0.5669015070589014
Loss in iteration 52 : 0.5746563791630891
Loss in iteration 53 : 0.5699253159817352
Loss in iteration 54 : 0.5648603710218443
Loss in iteration 55 : 0.5651561597845846
Loss in iteration 56 : 0.5649245406980428
Loss in iteration 57 : 0.5618914992215996
Loss in iteration 58 : 0.5653987762374354
Loss in iteration 59 : 0.5651107592709159
Loss in iteration 60 : 0.5611118831738926
Loss in iteration 61 : 0.5569912069969791
Loss in iteration 62 : 0.5585231533405993
Loss in iteration 63 : 0.5585971967736031
Loss in iteration 64 : 0.553572765225249
Loss in iteration 65 : 0.5515333895442815
Loss in iteration 66 : 0.5588939654679345
Loss in iteration 67 : 0.5488190192582949
Loss in iteration 68 : 0.5543054403872238
Loss in iteration 69 : 0.5503528626852234
Loss in iteration 70 : 0.5460215969186089
Loss in iteration 71 : 0.5532995021556519
Loss in iteration 72 : 0.5491109326227266
Loss in iteration 73 : 0.5518644044169894
Loss in iteration 74 : 0.5484498453616768
Loss in iteration 75 : 0.5478164451961384
Loss in iteration 76 : 0.5410476657578411
Loss in iteration 77 : 0.5401290715134446
Loss in iteration 78 : 0.5365290287596021
Loss in iteration 79 : 0.5402794967822547
Loss in iteration 80 : 0.5397095905830299
Loss in iteration 81 : 0.5439301547554831
Loss in iteration 82 : 0.5378712867689068
Loss in iteration 83 : 0.5406724067200875
Loss in iteration 84 : 0.5333987567828334
Loss in iteration 85 : 0.5381070501340853
Loss in iteration 86 : 0.5356034959028995
Loss in iteration 87 : 0.5298223901506222
Loss in iteration 88 : 0.5338304139222039
Loss in iteration 89 : 0.5299232090141197
Loss in iteration 90 : 0.533513235309805
Loss in iteration 91 : 0.5309276500295316
Loss in iteration 92 : 0.5295072747554522
Loss in iteration 93 : 0.5324009246402359
Loss in iteration 94 : 0.5288791135797235
Loss in iteration 95 : 0.5297379151894224
Loss in iteration 96 : 0.5295951576370171
Loss in iteration 97 : 0.5296462478545874
Loss in iteration 98 : 0.52804129952884
Loss in iteration 99 : 0.5226901378918984
Loss in iteration 100 : 0.521461901369651
Loss in iteration 101 : 0.5230750671750105
Loss in iteration 102 : 0.5228185964969864
Loss in iteration 103 : 0.5272146591171893
Loss in iteration 104 : 0.5250325548322065
Loss in iteration 105 : 0.5181869555759563
Loss in iteration 106 : 0.5257964425122271
Loss in iteration 107 : 0.5223984123046412
Loss in iteration 108 : 0.5165470436848344
Loss in iteration 109 : 0.5258069723047021
Loss in iteration 110 : 0.5184509622204272
Loss in iteration 111 : 0.5140470885306965
Loss in iteration 112 : 0.519594259804733
Loss in iteration 113 : 0.5155755011729166
Loss in iteration 114 : 0.5191965817587606
Loss in iteration 115 : 0.5182099744959607
Loss in iteration 116 : 0.5152902454504968
Loss in iteration 117 : 0.5149582498880552
Loss in iteration 118 : 0.5155357645321599
Loss in iteration 119 : 0.5153955499817617
Loss in iteration 120 : 0.5179230273510149
Loss in iteration 121 : 0.5106004226850808
Loss in iteration 122 : 0.5149412588891766
Loss in iteration 123 : 0.5133715148083308
Loss in iteration 124 : 0.5089758719141104
Loss in iteration 125 : 0.5088751342090706
Loss in iteration 126 : 0.5155143559482798
Loss in iteration 127 : 0.517349865770358
Loss in iteration 128 : 0.5060941599186671
Loss in iteration 129 : 0.5126779949214356
Loss in iteration 130 : 0.5068495705564815
Loss in iteration 131 : 0.5114658101458457
Loss in iteration 132 : 0.5061013154929017
Loss in iteration 133 : 0.5079507860505611
Loss in iteration 134 : 0.5067285945697578
Loss in iteration 135 : 0.5034335404448328
Loss in iteration 136 : 0.5100978765760207
Loss in iteration 137 : 0.5018862047221225
Loss in iteration 138 : 0.5040141414378427
Loss in iteration 139 : 0.49973313888603377
Loss in iteration 140 : 0.5061161236532873
Loss in iteration 141 : 0.5085240899291671
Loss in iteration 142 : 0.5046457357217332
Loss in iteration 143 : 0.49731996759801106
Loss in iteration 144 : 0.502919996848118
Loss in iteration 145 : 0.5008667379314593
Loss in iteration 146 : 0.49784162653282854
Loss in iteration 147 : 0.5038069714909325
Loss in iteration 148 : 0.5014337152144284
Loss in iteration 149 : 0.5040307815820333
Loss in iteration 150 : 0.5027368755457358
Loss in iteration 151 : 0.5024977031769632
Loss in iteration 152 : 0.5072934551691508
Loss in iteration 153 : 0.4995062931453484
Loss in iteration 154 : 0.5012199182470654
Loss in iteration 155 : 0.4935064133745688
Loss in iteration 156 : 0.49643449766631736
Loss in iteration 157 : 0.49339695035773046
Loss in iteration 158 : 0.48834539472580146
Loss in iteration 159 : 0.4961170377366146
Loss in iteration 160 : 0.49279351283058176
Loss in iteration 161 : 0.4935955390830257
Loss in iteration 162 : 0.49700475730798366
Loss in iteration 163 : 0.5037504236579817
Loss in iteration 164 : 0.5025950539318697
Loss in iteration 165 : 0.5006498342994699
Loss in iteration 166 : 0.4963600956134036
Loss in iteration 167 : 0.5049915620091839
Loss in iteration 168 : 0.4903902876492572
Loss in iteration 169 : 0.4959163539025688
Loss in iteration 170 : 0.4930019974373879
Loss in iteration 171 : 0.49809858153367753
Loss in iteration 172 : 0.4922448587016455
Loss in iteration 173 : 0.4911228314990353
Loss in iteration 174 : 0.4914084402598754
Loss in iteration 175 : 0.4922446688428614
Loss in iteration 176 : 0.49679046453274034
Loss in iteration 177 : 0.4963189765436483
Loss in iteration 178 : 0.49048838801858846
Loss in iteration 179 : 0.4919487984646858
Loss in iteration 180 : 0.4948644949375625
Loss in iteration 181 : 0.4966482195782651
Loss in iteration 182 : 0.49632012544279797
Loss in iteration 183 : 0.49603063555776034
Loss in iteration 184 : 0.4914721947386353
Loss in iteration 185 : 0.48678820527654765
Loss in iteration 186 : 0.48591563080541067
Loss in iteration 187 : 0.48161381492552136
Loss in iteration 188 : 0.4972711456815661
Loss in iteration 189 : 0.49658443512983946
Loss in iteration 190 : 0.49327166740195294
Loss in iteration 191 : 0.49512370383671706
Loss in iteration 192 : 0.4936224189948553
Loss in iteration 193 : 0.4848055447593852
Loss in iteration 194 : 0.4867458313651484
Loss in iteration 195 : 0.4915824989580346
Loss in iteration 196 : 0.49690597479557946
Loss in iteration 197 : 0.48448305448782847
Loss in iteration 198 : 0.4861199566810235
Loss in iteration 199 : 0.4901535956676125
Loss in iteration 200 : 0.48635231726263306
Testing accuracy  of updater 5 on alg 0 with rate 0.008 = 0.7815, training accuracy 0.777875, time elapsed: 4413 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6901052715381909
Loss in iteration 3 : 0.6871114016808124
Loss in iteration 4 : 0.6857155587785105
Loss in iteration 5 : 0.6845035752266339
Loss in iteration 6 : 0.6827509890191847
Loss in iteration 7 : 0.6812565335952484
Loss in iteration 8 : 0.6798078003208895
Loss in iteration 9 : 0.6779862928767035
Loss in iteration 10 : 0.6774031330087191
Loss in iteration 11 : 0.6771338093270978
Loss in iteration 12 : 0.6742183766012958
Loss in iteration 13 : 0.6742724232348161
Loss in iteration 14 : 0.6737450888498093
Loss in iteration 15 : 0.6737215487755356
Loss in iteration 16 : 0.6709868115349956
Loss in iteration 17 : 0.6718132225019591
Loss in iteration 18 : 0.6707291327831612
Loss in iteration 19 : 0.6685121999002221
Loss in iteration 20 : 0.6690809300259573
Loss in iteration 21 : 0.6674149486284358
Loss in iteration 22 : 0.6671797610563666
Loss in iteration 23 : 0.6661095441804424
Loss in iteration 24 : 0.6645613904890613
Loss in iteration 25 : 0.6640774721461107
Loss in iteration 26 : 0.6626622374201872
Loss in iteration 27 : 0.6594444022193092
Loss in iteration 28 : 0.661600242368256
Loss in iteration 29 : 0.659731825727364
Loss in iteration 30 : 0.6590824116202061
Loss in iteration 31 : 0.6568159755225701
Loss in iteration 32 : 0.6580026337727894
Loss in iteration 33 : 0.6561755737434126
Loss in iteration 34 : 0.657306443071278
Loss in iteration 35 : 0.6558497834501394
Loss in iteration 36 : 0.6548652326115494
Loss in iteration 37 : 0.6563658706746001
Loss in iteration 38 : 0.6544550380412912
Loss in iteration 39 : 0.6543204689198506
Loss in iteration 40 : 0.6527051361394253
Loss in iteration 41 : 0.6514450759669074
Loss in iteration 42 : 0.6489526906384743
Loss in iteration 43 : 0.6509350562285917
Loss in iteration 44 : 0.6473634827561425
Loss in iteration 45 : 0.6506594599644203
Loss in iteration 46 : 0.6472085874636077
Loss in iteration 47 : 0.6482293172770767
Loss in iteration 48 : 0.6478233571318066
Loss in iteration 49 : 0.6475520866347243
Loss in iteration 50 : 0.6454760374001276
Loss in iteration 51 : 0.6427257767390597
Loss in iteration 52 : 0.6480866390649636
Loss in iteration 53 : 0.6441498132453293
Loss in iteration 54 : 0.6417677741770696
Loss in iteration 55 : 0.641762369099583
Loss in iteration 56 : 0.6414536313436786
Loss in iteration 57 : 0.6411794299349776
Loss in iteration 58 : 0.6427376224554466
Loss in iteration 59 : 0.6415893236955602
Loss in iteration 60 : 0.6390417410599682
Loss in iteration 61 : 0.6370837928612758
Loss in iteration 62 : 0.6384797534692934
Loss in iteration 63 : 0.6382193401932842
Loss in iteration 64 : 0.6354331397207941
Loss in iteration 65 : 0.6336375000104182
Loss in iteration 66 : 0.6386073341515237
Loss in iteration 67 : 0.6326502005280294
Loss in iteration 68 : 0.6337222603741723
Loss in iteration 69 : 0.6325864976049962
Loss in iteration 70 : 0.6315129741747082
Loss in iteration 71 : 0.6364535092272374
Loss in iteration 72 : 0.6329382023813998
Loss in iteration 73 : 0.6320507675074947
Loss in iteration 74 : 0.6319463158465242
Loss in iteration 75 : 0.6304295179624594
Loss in iteration 76 : 0.6270238344137408
Loss in iteration 77 : 0.6279078525031475
Loss in iteration 78 : 0.6257896603042398
Loss in iteration 79 : 0.627846602129147
Loss in iteration 80 : 0.6271465484924366
Loss in iteration 81 : 0.6287677982076751
Loss in iteration 82 : 0.6267995343335272
Loss in iteration 83 : 0.6265100898741819
Loss in iteration 84 : 0.6234135332371279
Loss in iteration 85 : 0.6259290199694025
Loss in iteration 86 : 0.6239916006759949
Loss in iteration 87 : 0.6201793205351411
Loss in iteration 88 : 0.6227911128530952
Loss in iteration 89 : 0.6209622181842406
Loss in iteration 90 : 0.6207512160098125
Loss in iteration 91 : 0.6198122893015654
Loss in iteration 92 : 0.6191773088564184
Loss in iteration 93 : 0.620879810282188
Loss in iteration 94 : 0.617773130501269
Loss in iteration 95 : 0.6186203307282491
Loss in iteration 96 : 0.6192229410057883
Loss in iteration 97 : 0.6169042929680765
Loss in iteration 98 : 0.6175675450089507
Loss in iteration 99 : 0.616239828802714
Loss in iteration 100 : 0.6137993872074871
Loss in iteration 101 : 0.6150500491905267
Loss in iteration 102 : 0.615064970239094
Loss in iteration 103 : 0.6152953952252506
Loss in iteration 104 : 0.6142889190981798
Loss in iteration 105 : 0.6117027314250417
Loss in iteration 106 : 0.6140316763804412
Loss in iteration 107 : 0.6124002345226397
Loss in iteration 108 : 0.6109509603925884
Loss in iteration 109 : 0.612852774873287
Loss in iteration 110 : 0.6119539789140592
Loss in iteration 111 : 0.6086502159274186
Loss in iteration 112 : 0.6115024528629004
Loss in iteration 113 : 0.6080998975613234
Loss in iteration 114 : 0.6083541365565539
Loss in iteration 115 : 0.6099380221992875
Loss in iteration 116 : 0.6086692270549245
Loss in iteration 117 : 0.6068830344581987
Loss in iteration 118 : 0.6075606539397373
Loss in iteration 119 : 0.6063032127443534
Loss in iteration 120 : 0.6083192339695915
Loss in iteration 121 : 0.6039101307082287
Loss in iteration 122 : 0.6052190135249916
Loss in iteration 123 : 0.605402718676133
Loss in iteration 124 : 0.6027431455913236
Loss in iteration 125 : 0.6012535437528685
Loss in iteration 126 : 0.6064323702522968
Loss in iteration 127 : 0.6039184491507236
Loss in iteration 128 : 0.600152467421608
Loss in iteration 129 : 0.6038399649813325
Loss in iteration 130 : 0.599811209135408
Loss in iteration 131 : 0.6010487725197481
Loss in iteration 132 : 0.5986947105465548
Loss in iteration 133 : 0.5993600473526238
Loss in iteration 134 : 0.5976294509556295
Loss in iteration 135 : 0.595385560271295
Loss in iteration 136 : 0.6001833093040152
Loss in iteration 137 : 0.5974928730739844
Loss in iteration 138 : 0.5957341422100834
Loss in iteration 139 : 0.595119735159348
Loss in iteration 140 : 0.5965150214714069
Loss in iteration 141 : 0.5984704141535105
Loss in iteration 142 : 0.5943000681077082
Loss in iteration 143 : 0.5918312329827371
Loss in iteration 144 : 0.5941020487041948
Loss in iteration 145 : 0.592737240369664
Loss in iteration 146 : 0.5916672946758351
Loss in iteration 147 : 0.5931558306696445
Loss in iteration 148 : 0.593618050229273
Loss in iteration 149 : 0.593924538048776
Loss in iteration 150 : 0.5934403356151884
Loss in iteration 151 : 0.5937944932426146
Loss in iteration 152 : 0.593547980073388
Loss in iteration 153 : 0.5895279166902
Loss in iteration 154 : 0.5911859291268536
Loss in iteration 155 : 0.586259239314699
Loss in iteration 156 : 0.5875437800109812
Loss in iteration 157 : 0.5875207799303406
Loss in iteration 158 : 0.5853538932117172
Loss in iteration 159 : 0.5868102434645309
Loss in iteration 160 : 0.5862237545471469
Loss in iteration 161 : 0.5851504586139925
Loss in iteration 162 : 0.5870163004996354
Loss in iteration 163 : 0.5913590524966997
Loss in iteration 164 : 0.588671062819101
Loss in iteration 165 : 0.5857295940817505
Loss in iteration 166 : 0.5853734229731298
Loss in iteration 167 : 0.5877736282282858
Loss in iteration 168 : 0.5835857055438602
Loss in iteration 169 : 0.5834607629749646
Loss in iteration 170 : 0.5845336141554232
Loss in iteration 171 : 0.5837253506289557
Loss in iteration 172 : 0.5813662069932174
Loss in iteration 173 : 0.5801599518707272
Loss in iteration 174 : 0.5828657136561247
Loss in iteration 175 : 0.5792525024480304
Loss in iteration 176 : 0.5832617074466394
Loss in iteration 177 : 0.5819135923593924
Loss in iteration 178 : 0.5779670106306554
Loss in iteration 179 : 0.576418586297272
Loss in iteration 180 : 0.5789991966691421
Loss in iteration 181 : 0.5817791117102298
Loss in iteration 182 : 0.5801408670751441
Loss in iteration 183 : 0.579517558932572
Loss in iteration 184 : 0.5791779910628237
Loss in iteration 185 : 0.5781572114351701
Loss in iteration 186 : 0.5751323529573282
Loss in iteration 187 : 0.5720417859189993
Loss in iteration 188 : 0.5778633555339782
Loss in iteration 189 : 0.5792733094401302
Loss in iteration 190 : 0.5777401679117091
Loss in iteration 191 : 0.5780694015085073
Loss in iteration 192 : 0.5740562399642981
Loss in iteration 193 : 0.5725876981208704
Loss in iteration 194 : 0.5722220831759885
Loss in iteration 195 : 0.5744712169844454
Loss in iteration 196 : 0.5787954055621928
Loss in iteration 197 : 0.5714295941466889
Loss in iteration 198 : 0.5720971720885629
Loss in iteration 199 : 0.5735833781258678
Loss in iteration 200 : 0.5722070942736079
Testing accuracy  of updater 5 on alg 0 with rate 0.0019999999999999983 = 0.756, training accuracy 0.761, time elapsed: 4455 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 3.2504356180465335
Loss in iteration 3 : 2.450869043735894
Loss in iteration 4 : 1.7321740464303998
Loss in iteration 5 : 1.3879075589066834
Loss in iteration 6 : 1.4017244868976593
Loss in iteration 7 : 0.7082868968375441
Loss in iteration 8 : 1.4518148060344696
Loss in iteration 9 : 1.1368651277177615
Loss in iteration 10 : 0.8681152874764353
Loss in iteration 11 : 1.1290953790207077
Loss in iteration 12 : 0.8703764322244054
Loss in iteration 13 : 0.7408052908542141
Loss in iteration 14 : 0.9660775316413327
Loss in iteration 15 : 0.7881060367890327
Loss in iteration 16 : 0.6490027178322753
Loss in iteration 17 : 0.9675767067353716
Loss in iteration 18 : 0.8433624448732818
Loss in iteration 19 : 0.6863932495780238
Loss in iteration 20 : 0.8483222545493502
Loss in iteration 21 : 0.7604436160279541
Loss in iteration 22 : 0.6605510689780378
Loss in iteration 23 : 0.7407439324163665
Loss in iteration 24 : 0.6791013048276632
Loss in iteration 25 : 0.5978272303491095
Loss in iteration 26 : 0.7264704047319601
Loss in iteration 27 : 0.6515041387020307
Loss in iteration 28 : 0.5986635819522154
Loss in iteration 29 : 0.6719509794593882
Loss in iteration 30 : 0.5700185490357448
Loss in iteration 31 : 0.5729755291881845
Loss in iteration 32 : 0.6038807833154506
Loss in iteration 33 : 0.542875019957562
Loss in iteration 34 : 0.6278636969044799
Loss in iteration 35 : 0.5640332011181185
Loss in iteration 36 : 0.5317040987130194
Loss in iteration 37 : 0.5801150513046284
Loss in iteration 38 : 0.5055749829689505
Loss in iteration 39 : 0.5414014078075987
Loss in iteration 40 : 0.494695513892896
Loss in iteration 41 : 0.5091337203070976
Loss in iteration 42 : 0.5327262217882632
Loss in iteration 43 : 0.488229946307034
Loss in iteration 44 : 0.4992487967711693
Loss in iteration 45 : 0.491679799508245
Loss in iteration 46 : 0.49657712894099254
Loss in iteration 47 : 0.48229550202352617
Loss in iteration 48 : 0.5028701692155819
Loss in iteration 49 : 0.4871091165563205
Loss in iteration 50 : 0.5110135770735161
Loss in iteration 51 : 0.47314394954793415
Loss in iteration 52 : 0.4963459323939967
Loss in iteration 53 : 0.4769496120826789
Loss in iteration 54 : 0.4820427883656324
Loss in iteration 55 : 0.4657328494379619
Loss in iteration 56 : 0.4879732676077023
Loss in iteration 57 : 0.4762198999531237
Loss in iteration 58 : 0.48424156964595994
Loss in iteration 59 : 0.4987367927195389
Loss in iteration 60 : 0.48173392106453544
Loss in iteration 61 : 0.4850931275804543
Loss in iteration 62 : 0.4696519335685985
Loss in iteration 63 : 0.49357514020318943
Loss in iteration 64 : 0.46386557828818314
Loss in iteration 65 : 0.4883366112809022
Loss in iteration 66 : 0.49005624413673443
Loss in iteration 67 : 0.4604053925752652
Loss in iteration 68 : 0.5004261251362457
Loss in iteration 69 : 0.4762101549240312
Loss in iteration 70 : 0.4815815034083502
Loss in iteration 71 : 0.4691806752299931
Loss in iteration 72 : 0.4905254642889299
Loss in iteration 73 : 0.49402509466633093
Loss in iteration 74 : 0.4819664712342986
Loss in iteration 75 : 0.49257062558947917
Loss in iteration 76 : 0.4630008152231147
Loss in iteration 77 : 0.4658625651845761
Loss in iteration 78 : 0.4588705713416497
Loss in iteration 79 : 0.4673639544705408
Loss in iteration 80 : 0.4682518033824983
Loss in iteration 81 : 0.47472947012566663
Loss in iteration 82 : 0.455937490621774
Loss in iteration 83 : 0.47158966805002533
Loss in iteration 84 : 0.45782764723718844
Loss in iteration 85 : 0.4680795464162695
Loss in iteration 86 : 0.46217293672509063
Loss in iteration 87 : 0.4683979380554139
Loss in iteration 88 : 0.46556463733578335
Loss in iteration 89 : 0.46306603104719896
Loss in iteration 90 : 0.46773445559489946
Loss in iteration 91 : 0.4592334408363216
Loss in iteration 92 : 0.47305444180759
Loss in iteration 93 : 0.46389097216656666
Loss in iteration 94 : 0.4697539326814027
Loss in iteration 95 : 0.47172714901269713
Loss in iteration 96 : 0.4730475106696876
Loss in iteration 97 : 0.4781281733485178
Loss in iteration 98 : 0.4789640044650491
Loss in iteration 99 : 0.45319981875934834
Loss in iteration 100 : 0.4717300050214592
Loss in iteration 101 : 0.47692438067468823
Loss in iteration 102 : 0.4672750759515333
Loss in iteration 103 : 0.47661866073911235
Loss in iteration 104 : 0.47550623596526126
Loss in iteration 105 : 0.45254057980501
Loss in iteration 106 : 0.47649903637453817
Loss in iteration 107 : 0.47755785652070626
Loss in iteration 108 : 0.4559352395107903
Loss in iteration 109 : 0.47307358453964643
Loss in iteration 110 : 0.47044652506041124
Loss in iteration 111 : 0.4719088100044295
Loss in iteration 112 : 0.46476013606826594
Loss in iteration 113 : 0.46624906151822315
Loss in iteration 114 : 0.4709467804936773
Loss in iteration 115 : 0.46644551630349224
Loss in iteration 116 : 0.45256475349440883
Loss in iteration 117 : 0.4665271141776265
Loss in iteration 118 : 0.4622539155407232
Loss in iteration 119 : 0.4695749714965131
Loss in iteration 120 : 0.4661197753746492
Loss in iteration 121 : 0.46532447017510387
Loss in iteration 122 : 0.4786582776713909
Loss in iteration 123 : 0.4662849579738157
Loss in iteration 124 : 0.4617671128010199
Loss in iteration 125 : 0.45821995008521466
Loss in iteration 126 : 0.47552270190921375
Loss in iteration 127 : 0.4815449664124474
Loss in iteration 128 : 0.4724707788398647
Loss in iteration 129 : 0.4661818982677845
Loss in iteration 130 : 0.48175269521025327
Loss in iteration 131 : 0.48472979009765743
Loss in iteration 132 : 0.46554098842635916
Loss in iteration 133 : 0.4680816872879359
Loss in iteration 134 : 0.46084402612829833
Loss in iteration 135 : 0.46148558584327914
Loss in iteration 136 : 0.4672062900625636
Loss in iteration 137 : 0.45699265529575894
Loss in iteration 138 : 0.4630390722342211
Loss in iteration 139 : 0.4581178948042554
Loss in iteration 140 : 0.47562025250458645
Loss in iteration 141 : 0.4727102126524912
Loss in iteration 142 : 0.4665735976742023
Loss in iteration 143 : 0.4520168139660202
Loss in iteration 144 : 0.45994938236669525
Loss in iteration 145 : 0.4629263686906324
Loss in iteration 146 : 0.45420772103287854
Loss in iteration 147 : 0.463035309756508
Loss in iteration 148 : 0.4568249449457836
Loss in iteration 149 : 0.46941850131151636
Loss in iteration 150 : 0.4748356978432403
Loss in iteration 151 : 0.48089732711418254
Loss in iteration 152 : 0.4983620060900708
Loss in iteration 153 : 0.46825380913103487
Loss in iteration 154 : 0.4636658008766208
Loss in iteration 155 : 0.4640808416926295
Loss in iteration 156 : 0.4651458932130631
Loss in iteration 157 : 0.4626084704230207
Loss in iteration 158 : 0.4515357500044595
Loss in iteration 159 : 0.46953710916760216
Loss in iteration 160 : 0.4543534174704178
Loss in iteration 161 : 0.4624370489997361
Loss in iteration 162 : 0.4592827242882846
Loss in iteration 163 : 0.47352343033247896
Loss in iteration 164 : 0.4900987929008296
Loss in iteration 165 : 0.5351187989675956
Loss in iteration 166 : 0.516826850389789
Loss in iteration 167 : 0.49797909474231056
Loss in iteration 168 : 0.45473793298500653
Loss in iteration 169 : 0.5053466942084326
Loss in iteration 170 : 0.5440796362456887
Loss in iteration 171 : 0.5177795733490794
Loss in iteration 172 : 0.48233858204394925
Loss in iteration 173 : 0.4572006857541768
Loss in iteration 174 : 0.4783251999825629
Loss in iteration 175 : 0.4812717310387989
Loss in iteration 176 : 0.4880126904430892
Loss in iteration 177 : 0.468554396900807
Loss in iteration 178 : 0.4713531023202626
Loss in iteration 179 : 0.4893437880475377
Loss in iteration 180 : 0.49916690083300946
Loss in iteration 181 : 0.48458353883230665
Loss in iteration 182 : 0.4782436718679407
Loss in iteration 183 : 0.47219461372886384
Loss in iteration 184 : 0.45904229448339523
Loss in iteration 185 : 0.4668443996967268
Loss in iteration 186 : 0.47303545599192076
Loss in iteration 187 : 0.46380487244561247
Loss in iteration 188 : 0.4766067497128185
Loss in iteration 189 : 0.4653672643088776
Loss in iteration 190 : 0.47551702704871
Loss in iteration 191 : 0.4843257507925311
Loss in iteration 192 : 0.5082775925106824
Loss in iteration 193 : 0.5096580380817252
Loss in iteration 194 : 0.5056888853073235
Loss in iteration 195 : 0.4803146546568037
Loss in iteration 196 : 0.469078047541116
Loss in iteration 197 : 0.457100352054283
Loss in iteration 198 : 0.4627760709240718
Loss in iteration 199 : 0.46738824534835527
Loss in iteration 200 : 0.4601612252572452
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.788, training accuracy 0.786375, time elapsed: 4127 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.7683935500344153
Loss in iteration 3 : 1.1546140444967088
Loss in iteration 4 : 1.0936914246415634
Loss in iteration 5 : 0.5521028083654487
Loss in iteration 6 : 1.1433371737147284
Loss in iteration 7 : 0.5301192745420225
Loss in iteration 8 : 0.8112926336449613
Loss in iteration 9 : 0.5278366370478487
Loss in iteration 10 : 0.7316030852987342
Loss in iteration 11 : 0.5035155586326349
Loss in iteration 12 : 0.6507062992763306
Loss in iteration 13 : 0.5439882990971718
Loss in iteration 14 : 0.6393013413880824
Loss in iteration 15 : 0.5779670861801517
Loss in iteration 16 : 0.5697137287284987
Loss in iteration 17 : 0.6283101138406737
Loss in iteration 18 : 0.5552502939021746
Loss in iteration 19 : 0.6372187269737482
Loss in iteration 20 : 0.5551184095457244
Loss in iteration 21 : 0.5817891878778955
Loss in iteration 22 : 0.5748133444369397
Loss in iteration 23 : 0.5454594707876359
Loss in iteration 24 : 0.576638917161719
Loss in iteration 25 : 0.5111318197711233
Loss in iteration 26 : 0.5408692761681287
Loss in iteration 27 : 0.48907646201275945
Loss in iteration 28 : 0.5431984145670189
Loss in iteration 29 : 0.47652678629596074
Loss in iteration 30 : 0.5020170159876938
Loss in iteration 31 : 0.4688983198245226
Loss in iteration 32 : 0.48019631785235556
Loss in iteration 33 : 0.4892535278643351
Loss in iteration 34 : 0.5114699988877994
Loss in iteration 35 : 0.4767890099251121
Loss in iteration 36 : 0.5010102506763928
Loss in iteration 37 : 0.47741027399335834
Loss in iteration 38 : 0.48897295913558547
Loss in iteration 39 : 0.4694991839566488
Loss in iteration 40 : 0.47607140113333557
Loss in iteration 41 : 0.46249156897515653
Loss in iteration 42 : 0.4848884474866086
Loss in iteration 43 : 0.4713467596525813
Loss in iteration 44 : 0.46463423726512687
Loss in iteration 45 : 0.48946536491764153
Loss in iteration 46 : 0.4616101497496776
Loss in iteration 47 : 0.483616744898831
Loss in iteration 48 : 0.46686415425757954
Loss in iteration 49 : 0.48454552533256984
Loss in iteration 50 : 0.4812156863419923
Loss in iteration 51 : 0.461390057063546
Loss in iteration 52 : 0.47093507115144434
Loss in iteration 53 : 0.4634778254259799
Loss in iteration 54 : 0.4604531854119634
Loss in iteration 55 : 0.45827058475467347
Loss in iteration 56 : 0.4672465296300125
Loss in iteration 57 : 0.4692858946857131
Loss in iteration 58 : 0.46770061575054744
Loss in iteration 59 : 0.4874146087771287
Loss in iteration 60 : 0.47048198579642736
Loss in iteration 61 : 0.4742464423750692
Loss in iteration 62 : 0.46236179636680264
Loss in iteration 63 : 0.47936825025966456
Loss in iteration 64 : 0.4693545138816351
Loss in iteration 65 : 0.46365532812802573
Loss in iteration 66 : 0.5010586260990342
Loss in iteration 67 : 0.45128509338882855
Loss in iteration 68 : 0.49729185849645374
Loss in iteration 69 : 0.47035858667921354
Loss in iteration 70 : 0.47078405668381246
Loss in iteration 71 : 0.4784280181164359
Loss in iteration 72 : 0.47211559692983723
Loss in iteration 73 : 0.5003110591318478
Loss in iteration 74 : 0.4696000762547501
Loss in iteration 75 : 0.49162399971516607
Loss in iteration 76 : 0.46695180266336317
Loss in iteration 77 : 0.46057109978552224
Loss in iteration 78 : 0.4634801797519107
Loss in iteration 79 : 0.46383386919482716
Loss in iteration 80 : 0.46976006611165744
Loss in iteration 81 : 0.4739932693054345
Loss in iteration 82 : 0.4548311761040784
Loss in iteration 83 : 0.46981333114806684
Loss in iteration 84 : 0.4564667783270723
Loss in iteration 85 : 0.4680677516490985
Loss in iteration 86 : 0.4604645444223328
Loss in iteration 87 : 0.4631956438579618
Loss in iteration 88 : 0.464797322843294
Loss in iteration 89 : 0.4602514144240471
Loss in iteration 90 : 0.4654552748483215
Loss in iteration 91 : 0.4584497216784173
Loss in iteration 92 : 0.4678889577945169
Loss in iteration 93 : 0.46225243391000914
Loss in iteration 94 : 0.4654672129661825
Loss in iteration 95 : 0.4666128920166248
Loss in iteration 96 : 0.46601282448104137
Loss in iteration 97 : 0.4793916755052032
Loss in iteration 98 : 0.46375387377674343
Loss in iteration 99 : 0.4574907722019273
Loss in iteration 100 : 0.46912368806529603
Loss in iteration 101 : 0.46022798019116407
Loss in iteration 102 : 0.4696168376537984
Loss in iteration 103 : 0.47281295067323914
Loss in iteration 104 : 0.46995162768799753
Loss in iteration 105 : 0.4603498326389945
Loss in iteration 106 : 0.4694351739310873
Loss in iteration 107 : 0.46678774110386084
Loss in iteration 108 : 0.45695598751905
Loss in iteration 109 : 0.4672410348699785
Loss in iteration 110 : 0.45903669467228037
Loss in iteration 111 : 0.45279553798451283
Loss in iteration 112 : 0.4651379325173859
Loss in iteration 113 : 0.46190595629134745
Loss in iteration 114 : 0.46866632126406454
Loss in iteration 115 : 0.4604973277691159
Loss in iteration 116 : 0.4554345271501523
Loss in iteration 117 : 0.46112137873784015
Loss in iteration 118 : 0.4603821774525595
Loss in iteration 119 : 0.465463208813608
Loss in iteration 120 : 0.4649657958727511
Loss in iteration 121 : 0.46166306776793253
Loss in iteration 122 : 0.47283696537508507
Loss in iteration 123 : 0.46500519274466373
Loss in iteration 124 : 0.45751658985769894
Loss in iteration 125 : 0.46561189612019005
Loss in iteration 126 : 0.47473851718732185
Loss in iteration 127 : 0.4775955155143975
Loss in iteration 128 : 0.4684125265681581
Loss in iteration 129 : 0.4773350678188487
Loss in iteration 130 : 0.46392242785408316
Loss in iteration 131 : 0.46916097169763754
Loss in iteration 132 : 0.46233859692762175
Loss in iteration 133 : 0.4656454496848734
Loss in iteration 134 : 0.4726709940253025
Loss in iteration 135 : 0.46110705164444993
Loss in iteration 136 : 0.4716707581709038
Loss in iteration 137 : 0.4631797365001899
Loss in iteration 138 : 0.46048375642893385
Loss in iteration 139 : 0.45863077586418627
Loss in iteration 140 : 0.47305615347289426
Loss in iteration 141 : 0.46897637733697656
Loss in iteration 142 : 0.4681202355836222
Loss in iteration 143 : 0.4540221802724352
Loss in iteration 144 : 0.45957214371414534
Loss in iteration 145 : 0.4697840209488655
Loss in iteration 146 : 0.45556755191675075
Loss in iteration 147 : 0.465159101587153
Loss in iteration 148 : 0.46801196434617215
Loss in iteration 149 : 0.4745191675433803
Loss in iteration 150 : 0.4619091259043508
Loss in iteration 151 : 0.4614752766753339
Loss in iteration 152 : 0.47622015459865996
Loss in iteration 153 : 0.4611530507921138
Loss in iteration 154 : 0.461806099706224
Loss in iteration 155 : 0.4591216022946611
Loss in iteration 156 : 0.4611201700591467
Loss in iteration 157 : 0.45442351392450614
Loss in iteration 158 : 0.4510375393741046
Loss in iteration 159 : 0.46585535082746926
Loss in iteration 160 : 0.45168961107786726
Loss in iteration 161 : 0.4578163180449856
Loss in iteration 162 : 0.4588379285082769
Loss in iteration 163 : 0.47325444772585035
Loss in iteration 164 : 0.48203027632691625
Loss in iteration 165 : 0.48131610554376114
Loss in iteration 166 : 0.466233911186564
Loss in iteration 167 : 0.5007132757873933
Loss in iteration 168 : 0.46776457048515485
Loss in iteration 169 : 0.4613755316457673
Loss in iteration 170 : 0.4723767062809214
Loss in iteration 171 : 0.4688431316918885
Loss in iteration 172 : 0.4608428457682549
Loss in iteration 173 : 0.4800862618754548
Loss in iteration 174 : 0.46746415444791456
Loss in iteration 175 : 0.4624582261872483
Loss in iteration 176 : 0.4845198562266141
Loss in iteration 177 : 0.4703452637185229
Loss in iteration 178 : 0.47203270112043666
Loss in iteration 179 : 0.4822710462235297
Loss in iteration 180 : 0.47142352877277677
Loss in iteration 181 : 0.4715203363737571
Loss in iteration 182 : 0.4821880631242444
Loss in iteration 183 : 0.4683994719698956
Loss in iteration 184 : 0.4660497588491511
Loss in iteration 185 : 0.46022374602078053
Loss in iteration 186 : 0.4557237284973398
Loss in iteration 187 : 0.45410706368797726
Loss in iteration 188 : 0.4738218184946854
Loss in iteration 189 : 0.4672472673301982
Loss in iteration 190 : 0.47205374188858445
Loss in iteration 191 : 0.4672758214373529
Loss in iteration 192 : 0.46688026549877004
Loss in iteration 193 : 0.4549659409615183
Loss in iteration 194 : 0.4633088798033563
Loss in iteration 195 : 0.4649058127910936
Loss in iteration 196 : 0.4678048063929805
Loss in iteration 197 : 0.46072793931711886
Loss in iteration 198 : 0.4696657764026391
Loss in iteration 199 : 0.4706097849502067
Loss in iteration 200 : 0.4586149710048792
Testing accuracy  of updater 6 on alg 0 with rate 1.4000000000000001 = 0.782, training accuracy 0.78575, time elapsed: 4219 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.7089306942083785
Loss in iteration 3 : 0.6770656243790889
Loss in iteration 4 : 0.5992788182846404
Loss in iteration 5 : 0.556719932018907
Loss in iteration 6 : 0.5552496102068663
Loss in iteration 7 : 0.5100296307306044
Loss in iteration 8 : 0.5134884522164742
Loss in iteration 9 : 0.4890023105031532
Loss in iteration 10 : 0.49703569631696887
Loss in iteration 11 : 0.47544639819323314
Loss in iteration 12 : 0.4795949256677518
Loss in iteration 13 : 0.48969215768521285
Loss in iteration 14 : 0.4821608632424802
Loss in iteration 15 : 0.49301515902064574
Loss in iteration 16 : 0.48011376622487417
Loss in iteration 17 : 0.5002216622089255
Loss in iteration 18 : 0.49645715574046195
Loss in iteration 19 : 0.5001137712722079
Loss in iteration 20 : 0.49749525306268916
Loss in iteration 21 : 0.48178856343733234
Loss in iteration 22 : 0.5037989007795077
Loss in iteration 23 : 0.4791006809013489
Loss in iteration 24 : 0.4930830879200047
Loss in iteration 25 : 0.46743982419433955
Loss in iteration 26 : 0.4720423148037358
Loss in iteration 27 : 0.46342919830012264
Loss in iteration 28 : 0.48328830178461407
Loss in iteration 29 : 0.4595048968180097
Loss in iteration 30 : 0.46560690109822694
Loss in iteration 31 : 0.4558676028165231
Loss in iteration 32 : 0.4541819270364616
Loss in iteration 33 : 0.4756035309979221
Loss in iteration 34 : 0.47530795953677596
Loss in iteration 35 : 0.46624381398855186
Loss in iteration 36 : 0.4753878081846319
Loss in iteration 37 : 0.47004497306538123
Loss in iteration 38 : 0.46478991234956435
Loss in iteration 39 : 0.46022030485418786
Loss in iteration 40 : 0.45528583489469127
Loss in iteration 41 : 0.45494070637402956
Loss in iteration 42 : 0.4670679753984416
Loss in iteration 43 : 0.4635562157648388
Loss in iteration 44 : 0.4519249778955433
Loss in iteration 45 : 0.47161701908732584
Loss in iteration 46 : 0.4559459406470154
Loss in iteration 47 : 0.4644171073461215
Loss in iteration 48 : 0.460826960382056
Loss in iteration 49 : 0.4723721252384626
Loss in iteration 50 : 0.4716734521937851
Loss in iteration 51 : 0.4575783477239006
Loss in iteration 52 : 0.46639795391837546
Loss in iteration 53 : 0.45852199776936303
Loss in iteration 54 : 0.4541085258431889
Loss in iteration 55 : 0.45473897314978357
Loss in iteration 56 : 0.46002298511696665
Loss in iteration 57 : 0.46308006025254855
Loss in iteration 58 : 0.4670956866729006
Loss in iteration 59 : 0.479504044787202
Loss in iteration 60 : 0.471583815841248
Loss in iteration 61 : 0.4648413164299564
Loss in iteration 62 : 0.46173838559710156
Loss in iteration 63 : 0.4688333548942475
Loss in iteration 64 : 0.4626842233833727
Loss in iteration 65 : 0.46112577933999793
Loss in iteration 66 : 0.469406849988975
Loss in iteration 67 : 0.4503148759718277
Loss in iteration 68 : 0.4748082813042486
Loss in iteration 69 : 0.4692715626272854
Loss in iteration 70 : 0.45712652537504656
Loss in iteration 71 : 0.46480260582488886
Loss in iteration 72 : 0.4662295746274516
Loss in iteration 73 : 0.4784630609101917
Loss in iteration 74 : 0.4654051163758886
Loss in iteration 75 : 0.4751734960934526
Loss in iteration 76 : 0.45972060978576135
Loss in iteration 77 : 0.45269281979714077
Loss in iteration 78 : 0.4486368621173952
Loss in iteration 79 : 0.4673215649553611
Loss in iteration 80 : 0.4619452403221326
Loss in iteration 81 : 0.4670192070099827
Loss in iteration 82 : 0.4569069747699688
Loss in iteration 83 : 0.46598783635408175
Loss in iteration 84 : 0.4557521274908236
Loss in iteration 85 : 0.4651881709215483
Loss in iteration 86 : 0.46020063233366704
Loss in iteration 87 : 0.4618603991677337
Loss in iteration 88 : 0.46808495932632066
Loss in iteration 89 : 0.4586016217560009
Loss in iteration 90 : 0.46916771437358495
Loss in iteration 91 : 0.4592952922371233
Loss in iteration 92 : 0.47031810944003805
Loss in iteration 93 : 0.4662896326362184
Loss in iteration 94 : 0.46843805815595807
Loss in iteration 95 : 0.46561402733340623
Loss in iteration 96 : 0.4721039875268847
Loss in iteration 97 : 0.4716942793731232
Loss in iteration 98 : 0.46626863361480386
Loss in iteration 99 : 0.45022836294921914
Loss in iteration 100 : 0.4567793320354299
Loss in iteration 101 : 0.45673269012501916
Loss in iteration 102 : 0.4636683374201646
Loss in iteration 103 : 0.4684291771357674
Loss in iteration 104 : 0.4642607553293104
Loss in iteration 105 : 0.44742846666457076
Loss in iteration 106 : 0.469159027924771
Loss in iteration 107 : 0.46308328835044854
Loss in iteration 108 : 0.45130976666645223
Loss in iteration 109 : 0.4679182195726781
Loss in iteration 110 : 0.4583822241927976
Loss in iteration 111 : 0.452760578582096
Loss in iteration 112 : 0.461395967224834
Loss in iteration 113 : 0.4569205850666222
Loss in iteration 114 : 0.4689318043349781
Loss in iteration 115 : 0.4600129046375177
Loss in iteration 116 : 0.45359471332038587
Loss in iteration 117 : 0.46078580310931777
Loss in iteration 118 : 0.4594570464987286
Loss in iteration 119 : 0.4627503174359831
Loss in iteration 120 : 0.4641816158537658
Loss in iteration 121 : 0.46068800145149014
Loss in iteration 122 : 0.46957039941552525
Loss in iteration 123 : 0.46370998702030014
Loss in iteration 124 : 0.4597502208004164
Loss in iteration 125 : 0.4621628239889993
Loss in iteration 126 : 0.46605304692070393
Loss in iteration 127 : 0.4785357060623798
Loss in iteration 128 : 0.4588705813183753
Loss in iteration 129 : 0.46385850587441996
Loss in iteration 130 : 0.4643058575589326
Loss in iteration 131 : 0.46481341168602336
Loss in iteration 132 : 0.45905590565149523
Loss in iteration 133 : 0.4646229079039532
Loss in iteration 134 : 0.4597155710069692
Loss in iteration 135 : 0.45730720483374554
Loss in iteration 136 : 0.4668284741360898
Loss in iteration 137 : 0.4545780764816605
Loss in iteration 138 : 0.4625375288261669
Loss in iteration 139 : 0.4476987024922153
Loss in iteration 140 : 0.4686137276154155
Loss in iteration 141 : 0.46535204563463906
Loss in iteration 142 : 0.46624336372296477
Loss in iteration 143 : 0.4515094330395732
Loss in iteration 144 : 0.4583614867059546
Loss in iteration 145 : 0.45919787203095214
Loss in iteration 146 : 0.455453802007125
Loss in iteration 147 : 0.46289622041442235
Loss in iteration 148 : 0.46054477232566954
Loss in iteration 149 : 0.46951559429567313
Loss in iteration 150 : 0.4614806563229335
Loss in iteration 151 : 0.4616716324263962
Loss in iteration 152 : 0.471562800633484
Loss in iteration 153 : 0.4600155554487526
Loss in iteration 154 : 0.46145235199561013
Loss in iteration 155 : 0.45542643524216214
Loss in iteration 156 : 0.4625257548882669
Loss in iteration 157 : 0.45334685082328186
Loss in iteration 158 : 0.4518032924248859
Loss in iteration 159 : 0.4630012834779533
Loss in iteration 160 : 0.4527716248689178
Loss in iteration 161 : 0.4554480586883791
Loss in iteration 162 : 0.46245914996570414
Loss in iteration 163 : 0.472056211823063
Loss in iteration 164 : 0.46334457375467947
Loss in iteration 165 : 0.47591446412465677
Loss in iteration 166 : 0.4629141949602573
Loss in iteration 167 : 0.47629222255923076
Loss in iteration 168 : 0.45111060375263823
Loss in iteration 169 : 0.46334795884824687
Loss in iteration 170 : 0.45330385180737526
Loss in iteration 171 : 0.46245183522620975
Loss in iteration 172 : 0.4565704792533081
Loss in iteration 173 : 0.4579998630296401
Loss in iteration 174 : 0.45588707069417966
Loss in iteration 175 : 0.46115298801715215
Loss in iteration 176 : 0.46863671121956585
Loss in iteration 177 : 0.4669574232904505
Loss in iteration 178 : 0.4638759885111795
Loss in iteration 179 : 0.4637092627449084
Loss in iteration 180 : 0.4670449584440707
Loss in iteration 181 : 0.46626136488346226
Loss in iteration 182 : 0.4715790962857197
Loss in iteration 183 : 0.4674413286179762
Loss in iteration 184 : 0.46125791683501727
Loss in iteration 185 : 0.45559634798445975
Loss in iteration 186 : 0.4589201779719233
Loss in iteration 187 : 0.4520004455171916
Loss in iteration 188 : 0.47397209804199
Loss in iteration 189 : 0.4679433038401388
Loss in iteration 190 : 0.4689546994317864
Loss in iteration 191 : 0.46977635615125646
Loss in iteration 192 : 0.4648914179251158
Loss in iteration 193 : 0.4599004440749807
Loss in iteration 194 : 0.4597700909336872
Loss in iteration 195 : 0.4653271467910898
Loss in iteration 196 : 0.46753933210337534
Loss in iteration 197 : 0.4549358435583169
Loss in iteration 198 : 0.45932211997916106
Loss in iteration 199 : 0.4694604414507248
Loss in iteration 200 : 0.456738932144646
Testing accuracy  of updater 6 on alg 0 with rate 0.8 = 0.7785, training accuracy 0.785625, time elapsed: 4301 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6802762244746343
Loss in iteration 3 : 0.6631806001482412
Loss in iteration 4 : 0.648166111473523
Loss in iteration 5 : 0.6276229492127334
Loss in iteration 6 : 0.6073863765995332
Loss in iteration 7 : 0.5847410305069063
Loss in iteration 8 : 0.565565665645647
Loss in iteration 9 : 0.5506185242796868
Loss in iteration 10 : 0.5425079314089769
Loss in iteration 11 : 0.5229268154421514
Loss in iteration 12 : 0.5118419526659006
Loss in iteration 13 : 0.5136730239395858
Loss in iteration 14 : 0.5007751814638486
Loss in iteration 15 : 0.5089566161686633
Loss in iteration 16 : 0.4922761362721367
Loss in iteration 17 : 0.5023336969798923
Loss in iteration 18 : 0.49565109485468395
Loss in iteration 19 : 0.49514078304574133
Loss in iteration 20 : 0.4929859809305018
Loss in iteration 21 : 0.4840550541444859
Loss in iteration 22 : 0.4961413271406924
Loss in iteration 23 : 0.48411575876360946
Loss in iteration 24 : 0.4894739302380695
Loss in iteration 25 : 0.475488027599493
Loss in iteration 26 : 0.4793643805442615
Loss in iteration 27 : 0.47049875464489566
Loss in iteration 28 : 0.4867017845454688
Loss in iteration 29 : 0.4696918116338408
Loss in iteration 30 : 0.4740409948626235
Loss in iteration 31 : 0.46519120699349026
Loss in iteration 32 : 0.4660839761972058
Loss in iteration 33 : 0.4832173324556217
Loss in iteration 34 : 0.4809748889601579
Loss in iteration 35 : 0.47686081040234873
Loss in iteration 36 : 0.48053230087023374
Loss in iteration 37 : 0.4785693390296442
Loss in iteration 38 : 0.47227075808699504
Loss in iteration 39 : 0.46726104722638034
Loss in iteration 40 : 0.4600599672208457
Loss in iteration 41 : 0.4644355793576827
Loss in iteration 42 : 0.4697656879199748
Loss in iteration 43 : 0.4697165346690014
Loss in iteration 44 : 0.45727241154814613
Loss in iteration 45 : 0.47627034604221147
Loss in iteration 46 : 0.4607174281813197
Loss in iteration 47 : 0.4676238783061562
Loss in iteration 48 : 0.46621791408455815
Loss in iteration 49 : 0.47700979592921494
Loss in iteration 50 : 0.4750722714257425
Loss in iteration 51 : 0.4591805152924516
Loss in iteration 52 : 0.47189695170384355
Loss in iteration 53 : 0.4665128253335926
Loss in iteration 54 : 0.4590348933997627
Loss in iteration 55 : 0.4603083886333244
Loss in iteration 56 : 0.4631674987029719
Loss in iteration 57 : 0.463062823360256
Loss in iteration 58 : 0.4713191818428297
Loss in iteration 59 : 0.47944685040882895
Loss in iteration 60 : 0.4687605131360176
Loss in iteration 61 : 0.4657759059847922
Loss in iteration 62 : 0.46360107629885083
Loss in iteration 63 : 0.46888544212120087
Loss in iteration 64 : 0.46393770748572066
Loss in iteration 65 : 0.4583502844880686
Loss in iteration 66 : 0.4714432256850587
Loss in iteration 67 : 0.45421899064022775
Loss in iteration 68 : 0.4770236282762103
Loss in iteration 69 : 0.46847139112879926
Loss in iteration 70 : 0.4562137044437147
Loss in iteration 71 : 0.4670042209801928
Loss in iteration 72 : 0.4680493286775749
Loss in iteration 73 : 0.47655742211691404
Loss in iteration 74 : 0.46942748764065123
Loss in iteration 75 : 0.4761619086484978
Loss in iteration 76 : 0.45967949012761294
Loss in iteration 77 : 0.456411542690237
Loss in iteration 78 : 0.4511144067549784
Loss in iteration 79 : 0.46519276384376745
Loss in iteration 80 : 0.46450716253303287
Loss in iteration 81 : 0.4677333303991476
Loss in iteration 82 : 0.4572763462828708
Loss in iteration 83 : 0.4687193095217549
Loss in iteration 84 : 0.4562665164009446
Loss in iteration 85 : 0.46544003375468185
Loss in iteration 86 : 0.4627971563727344
Loss in iteration 87 : 0.4621472037430931
Loss in iteration 88 : 0.4659213596805698
Loss in iteration 89 : 0.46064905459151256
Loss in iteration 90 : 0.46558897293794255
Loss in iteration 91 : 0.46127280444433266
Loss in iteration 92 : 0.4671432839158452
Loss in iteration 93 : 0.4673331873683953
Loss in iteration 94 : 0.46112546276634153
Loss in iteration 95 : 0.46672754694672947
Loss in iteration 96 : 0.4672629218349322
Loss in iteration 97 : 0.47246200364023583
Loss in iteration 98 : 0.4635384569335325
Loss in iteration 99 : 0.453718612918422
Loss in iteration 100 : 0.4567766220979031
Loss in iteration 101 : 0.4592283405569262
Loss in iteration 102 : 0.46487994478026035
Loss in iteration 103 : 0.4699490856672199
Loss in iteration 104 : 0.4665156787851361
Loss in iteration 105 : 0.45054207191348905
Loss in iteration 106 : 0.47015511871622184
Loss in iteration 107 : 0.46623982957460564
Loss in iteration 108 : 0.4535017030532743
Loss in iteration 109 : 0.4694726273952687
Loss in iteration 110 : 0.4614320058860127
Loss in iteration 111 : 0.45524406025614333
Loss in iteration 112 : 0.4627623499247707
Loss in iteration 113 : 0.4607551602228454
Loss in iteration 114 : 0.469870504024246
Loss in iteration 115 : 0.4625167522996765
Loss in iteration 116 : 0.4557051829129886
Loss in iteration 117 : 0.4612916543772621
Loss in iteration 118 : 0.46296887603553166
Loss in iteration 119 : 0.4643772519847211
Loss in iteration 120 : 0.4661067675349654
Loss in iteration 121 : 0.46192494358476394
Loss in iteration 122 : 0.4723914151667598
Loss in iteration 123 : 0.4660686686387009
Loss in iteration 124 : 0.457839900437475
Loss in iteration 125 : 0.46208717156518353
Loss in iteration 126 : 0.46810177150546106
Loss in iteration 127 : 0.4754603510408065
Loss in iteration 128 : 0.46048197256141565
Loss in iteration 129 : 0.4646679136607179
Loss in iteration 130 : 0.4647605866241189
Loss in iteration 131 : 0.46548574556979583
Loss in iteration 132 : 0.45947752434156985
Loss in iteration 133 : 0.4643206765021678
Loss in iteration 134 : 0.4596231876836108
Loss in iteration 135 : 0.4593631214821484
Loss in iteration 136 : 0.4663392682699633
Loss in iteration 137 : 0.4560247006564272
Loss in iteration 138 : 0.46029531400686297
Loss in iteration 139 : 0.44920593330879816
Loss in iteration 140 : 0.4666881727635978
Loss in iteration 141 : 0.4677301495738877
Loss in iteration 142 : 0.4652989486766806
Loss in iteration 143 : 0.45122693293732713
Loss in iteration 144 : 0.46057044424117216
Loss in iteration 145 : 0.46173277379433364
Loss in iteration 146 : 0.4531230624670794
Loss in iteration 147 : 0.4643304047878802
Loss in iteration 148 : 0.45924609512653414
Loss in iteration 149 : 0.4675254736601418
Loss in iteration 150 : 0.4628275968943485
Loss in iteration 151 : 0.4614088540402963
Loss in iteration 152 : 0.47244487351831216
Loss in iteration 153 : 0.46140848112310934
Loss in iteration 154 : 0.4622627280120955
Loss in iteration 155 : 0.45749757308819144
Loss in iteration 156 : 0.46031028440139754
Loss in iteration 157 : 0.45372635668868694
Loss in iteration 158 : 0.4505411755015586
Loss in iteration 159 : 0.46232819874855297
Loss in iteration 160 : 0.45241131876270707
Loss in iteration 161 : 0.45567560074922014
Loss in iteration 162 : 0.4590543171992127
Loss in iteration 163 : 0.4674187777330384
Loss in iteration 164 : 0.4650280137522671
Loss in iteration 165 : 0.47179711622002085
Loss in iteration 166 : 0.4643696088242798
Loss in iteration 167 : 0.47562384239637545
Loss in iteration 168 : 0.45240318010393044
Loss in iteration 169 : 0.46342259153046383
Loss in iteration 170 : 0.4558099925326076
Loss in iteration 171 : 0.46348816288020284
Loss in iteration 172 : 0.45779584920216454
Loss in iteration 173 : 0.4573556312822875
Loss in iteration 174 : 0.45700273123898677
Loss in iteration 175 : 0.46141876632467704
Loss in iteration 176 : 0.4694967646524849
Loss in iteration 177 : 0.4663431708854518
Loss in iteration 178 : 0.4617938470122288
Loss in iteration 179 : 0.4634895343592725
Loss in iteration 180 : 0.46502956689691116
Loss in iteration 181 : 0.46672506082282705
Loss in iteration 182 : 0.46748800826356446
Loss in iteration 183 : 0.4675476499582312
Loss in iteration 184 : 0.4582648003810111
Loss in iteration 185 : 0.45509986504019484
Loss in iteration 186 : 0.4546716406801886
Loss in iteration 187 : 0.4498429374491097
Loss in iteration 188 : 0.4724908744820505
Loss in iteration 189 : 0.4656138050595765
Loss in iteration 190 : 0.46816235236598946
Loss in iteration 191 : 0.4632188157131631
Loss in iteration 192 : 0.4643860800372159
Loss in iteration 193 : 0.4549388889615511
Loss in iteration 194 : 0.45950587242441654
Loss in iteration 195 : 0.4618312231309202
Loss in iteration 196 : 0.46803355080210396
Loss in iteration 197 : 0.4568513704919109
Loss in iteration 198 : 0.4583778600853011
Loss in iteration 199 : 0.46462134904897007
Loss in iteration 200 : 0.4575456286474688
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.7855, training accuracy 0.7885, time elapsed: 3746 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6822955972141519
Loss in iteration 3 : 0.665972267586973
Loss in iteration 4 : 0.6531180490980686
Loss in iteration 5 : 0.6346664614483906
Loss in iteration 6 : 0.6161697731656104
Loss in iteration 7 : 0.5972557714330575
Loss in iteration 8 : 0.5794894834513745
Loss in iteration 9 : 0.5629084872147174
Loss in iteration 10 : 0.5549700339729904
Loss in iteration 11 : 0.5372016666045046
Loss in iteration 12 : 0.522730374321384
Loss in iteration 13 : 0.5222986399495945
Loss in iteration 14 : 0.5112128917956157
Loss in iteration 15 : 0.51629653946668
Loss in iteration 16 : 0.4998881026429481
Loss in iteration 17 : 0.5079899416552756
Loss in iteration 18 : 0.5024669918617216
Loss in iteration 19 : 0.4995313100275879
Loss in iteration 20 : 0.49635609672350867
Loss in iteration 21 : 0.4879589860922536
Loss in iteration 22 : 0.4983852239754864
Loss in iteration 23 : 0.4867290356619684
Loss in iteration 24 : 0.4922681820768798
Loss in iteration 25 : 0.4788022553039888
Loss in iteration 26 : 0.48244239133455047
Loss in iteration 27 : 0.4720031292488269
Loss in iteration 28 : 0.4883591810213052
Loss in iteration 29 : 0.4722348982480095
Loss in iteration 30 : 0.475972310788633
Loss in iteration 31 : 0.46837959989395206
Loss in iteration 32 : 0.46892162517271757
Loss in iteration 33 : 0.48510057803553924
Loss in iteration 34 : 0.48264409555269994
Loss in iteration 35 : 0.4793421581135291
Loss in iteration 36 : 0.4822916548993246
Loss in iteration 37 : 0.48067594136483865
Loss in iteration 38 : 0.4747112379453388
Loss in iteration 39 : 0.469899683369624
Loss in iteration 40 : 0.46268418776091524
Loss in iteration 41 : 0.46713627300765825
Loss in iteration 42 : 0.47125981390399563
Loss in iteration 43 : 0.4722713477564917
Loss in iteration 44 : 0.4601492828802354
Loss in iteration 45 : 0.4786602068203472
Loss in iteration 46 : 0.4630083080028846
Loss in iteration 47 : 0.4698840337952045
Loss in iteration 48 : 0.46864442441214693
Loss in iteration 49 : 0.4792413168530795
Loss in iteration 50 : 0.47731859173430774
Loss in iteration 51 : 0.46119804084003524
Loss in iteration 52 : 0.4743908481423717
Loss in iteration 53 : 0.46927699503819803
Loss in iteration 54 : 0.4614637542375737
Loss in iteration 55 : 0.462668881911153
Loss in iteration 56 : 0.4657268400130424
Loss in iteration 57 : 0.4649396628429885
Loss in iteration 58 : 0.47356503357178253
Loss in iteration 59 : 0.4804979216644682
Loss in iteration 60 : 0.4704714439695367
Loss in iteration 61 : 0.46736177966736453
Loss in iteration 62 : 0.46531499120586367
Loss in iteration 63 : 0.4708683757368114
Loss in iteration 64 : 0.46547243004494193
Loss in iteration 65 : 0.4599422204375407
Loss in iteration 66 : 0.4726054137982597
Loss in iteration 67 : 0.4555796847880894
Loss in iteration 68 : 0.4782691275807999
Loss in iteration 69 : 0.4699690882698451
Loss in iteration 70 : 0.457961244693266
Loss in iteration 71 : 0.46854146303673216
Loss in iteration 72 : 0.46900602227828225
Loss in iteration 73 : 0.4780285224148795
Loss in iteration 74 : 0.47103844075225687
Loss in iteration 75 : 0.4771296579929469
Loss in iteration 76 : 0.4612088129455723
Loss in iteration 77 : 0.45688939457140176
Loss in iteration 78 : 0.4523134784373888
Loss in iteration 79 : 0.46666002340395646
Loss in iteration 80 : 0.4656334594804923
Loss in iteration 81 : 0.468797967488842
Loss in iteration 82 : 0.45872497854282485
Loss in iteration 83 : 0.4697703179985965
Loss in iteration 84 : 0.4570220867697569
Loss in iteration 85 : 0.46643254689563707
Loss in iteration 86 : 0.46352991267214
Loss in iteration 87 : 0.4627657370399303
Loss in iteration 88 : 0.4660551374953089
Loss in iteration 89 : 0.4615541301876924
Loss in iteration 90 : 0.46658207557797404
Loss in iteration 91 : 0.4629629751973366
Loss in iteration 92 : 0.46734283341273214
Loss in iteration 93 : 0.46773257641379257
Loss in iteration 94 : 0.4619239636036612
Loss in iteration 95 : 0.4673974158371295
Loss in iteration 96 : 0.4679903267975697
Loss in iteration 97 : 0.4724508701451727
Loss in iteration 98 : 0.4644181880848952
Loss in iteration 99 : 0.4549777457010687
Loss in iteration 100 : 0.4576792270912508
Loss in iteration 101 : 0.4599638808549302
Loss in iteration 102 : 0.46512563120853384
Loss in iteration 103 : 0.47071491488905887
Loss in iteration 104 : 0.4674089005021084
Loss in iteration 105 : 0.45223079491412094
Loss in iteration 106 : 0.47067587631113367
Loss in iteration 107 : 0.4667085540461674
Loss in iteration 108 : 0.45441904151620083
Loss in iteration 109 : 0.4704677027656004
Loss in iteration 110 : 0.46254654521969557
Loss in iteration 111 : 0.45602365080769125
Loss in iteration 112 : 0.4634431913559807
Loss in iteration 113 : 0.46048703242887734
Loss in iteration 114 : 0.47018533115241096
Loss in iteration 115 : 0.4634612386345575
Loss in iteration 116 : 0.4570393337297286
Loss in iteration 117 : 0.46204581696033936
Loss in iteration 118 : 0.46291022362380047
Loss in iteration 119 : 0.4648883405458018
Loss in iteration 120 : 0.46781733380151985
Loss in iteration 121 : 0.46225511535027847
Loss in iteration 122 : 0.47238889723633287
Loss in iteration 123 : 0.4676107081266108
Loss in iteration 124 : 0.45742431201848216
Loss in iteration 125 : 0.4615254335404424
Loss in iteration 126 : 0.46801454366140083
Loss in iteration 127 : 0.4765650795394627
Loss in iteration 128 : 0.460138184019966
Loss in iteration 129 : 0.46461057849192683
Loss in iteration 130 : 0.46495654477806536
Loss in iteration 131 : 0.4664035948083973
Loss in iteration 132 : 0.4607549592895208
Loss in iteration 133 : 0.4643643880758632
Loss in iteration 134 : 0.4600591179013878
Loss in iteration 135 : 0.459813657518329
Loss in iteration 136 : 0.46729395906973603
Loss in iteration 137 : 0.4574174890452575
Loss in iteration 138 : 0.4604629948577293
Loss in iteration 139 : 0.4502165258556904
Loss in iteration 140 : 0.46762679395752327
Loss in iteration 141 : 0.46801417489358715
Loss in iteration 142 : 0.4655822797320344
Loss in iteration 143 : 0.4515674570532535
Loss in iteration 144 : 0.4609244063726451
Loss in iteration 145 : 0.46233963740826106
Loss in iteration 146 : 0.453479147455091
Loss in iteration 147 : 0.4646582182226822
Loss in iteration 148 : 0.45914710172944423
Loss in iteration 149 : 0.46841511577136735
Loss in iteration 150 : 0.46327306696566095
Loss in iteration 151 : 0.4617619143896428
Loss in iteration 152 : 0.4727871336086498
Loss in iteration 153 : 0.4623924982522794
Loss in iteration 154 : 0.462923286893083
Loss in iteration 155 : 0.45816596182277775
Loss in iteration 156 : 0.4606754504087282
Loss in iteration 157 : 0.4542242275510781
Loss in iteration 158 : 0.4510328892649661
Loss in iteration 159 : 0.4628481405066834
Loss in iteration 160 : 0.4529405788922442
Loss in iteration 161 : 0.456378086065241
Loss in iteration 162 : 0.4598484510823485
Loss in iteration 163 : 0.4681666703242686
Loss in iteration 164 : 0.46550666293534987
Loss in iteration 165 : 0.4719198457976989
Loss in iteration 166 : 0.46498836075816746
Loss in iteration 167 : 0.4760702470684943
Loss in iteration 168 : 0.4528088859856528
Loss in iteration 169 : 0.4638396147189857
Loss in iteration 170 : 0.4561162657800587
Loss in iteration 171 : 0.4641330362255053
Loss in iteration 172 : 0.4583466823532981
Loss in iteration 173 : 0.4577671573264525
Loss in iteration 174 : 0.45744833505371046
Loss in iteration 175 : 0.4611678072347886
Loss in iteration 176 : 0.4696600209255358
Loss in iteration 177 : 0.46705910991790645
Loss in iteration 178 : 0.4613819367594539
Loss in iteration 179 : 0.46372431022747646
Loss in iteration 180 : 0.4658168924706626
Loss in iteration 181 : 0.4667971474440885
Loss in iteration 182 : 0.4679384582096195
Loss in iteration 183 : 0.4674531280177312
Loss in iteration 184 : 0.45874952568328525
Loss in iteration 185 : 0.45547046758139637
Loss in iteration 186 : 0.454675035771918
Loss in iteration 187 : 0.45040920338412693
Loss in iteration 188 : 0.47251796782439814
Loss in iteration 189 : 0.4657997490316244
Loss in iteration 190 : 0.4685363642871763
Loss in iteration 191 : 0.4641325734748254
Loss in iteration 192 : 0.46483251045676416
Loss in iteration 193 : 0.45557129966036797
Loss in iteration 194 : 0.4600357096619894
Loss in iteration 195 : 0.4622507615424544
Loss in iteration 196 : 0.4685809665379982
Loss in iteration 197 : 0.4564909182329629
Loss in iteration 198 : 0.4587136132403408
Loss in iteration 199 : 0.465105356996457
Loss in iteration 200 : 0.4573279686569483
Testing accuracy  of updater 6 on alg 0 with rate 0.14 = 0.785, training accuracy 0.788875, time elapsed: 3816 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6848936046685685
Loss in iteration 3 : 0.6719766514230758
Loss in iteration 4 : 0.6645977695907889
Loss in iteration 5 : 0.6517560733231165
Loss in iteration 6 : 0.6337579259418024
Loss in iteration 7 : 0.6191906770103558
Loss in iteration 8 : 0.6085913701294198
Loss in iteration 9 : 0.5945518289707108
Loss in iteration 10 : 0.5817901566723159
Loss in iteration 11 : 0.5670809671708652
Loss in iteration 12 : 0.5546689841152633
Loss in iteration 13 : 0.5499377786158747
Loss in iteration 14 : 0.5366287278125086
Loss in iteration 15 : 0.5379375986233864
Loss in iteration 16 : 0.5246354781795993
Loss in iteration 17 : 0.5282115321934956
Loss in iteration 18 : 0.5193856452631819
Loss in iteration 19 : 0.5168623380873628
Loss in iteration 20 : 0.5124495979093415
Loss in iteration 21 : 0.5039312471752001
Loss in iteration 22 : 0.5101448631309786
Loss in iteration 23 : 0.5012513852620221
Loss in iteration 24 : 0.5041240835239386
Loss in iteration 25 : 0.49067147422706303
Loss in iteration 26 : 0.4940882106523518
Loss in iteration 27 : 0.484853496246126
Loss in iteration 28 : 0.49665100671005735
Loss in iteration 29 : 0.4823790182247587
Loss in iteration 30 : 0.4852138792049696
Loss in iteration 31 : 0.4785656558590396
Loss in iteration 32 : 0.47868086162264617
Loss in iteration 33 : 0.49153886578938993
Loss in iteration 34 : 0.48932525007823313
Loss in iteration 35 : 0.4862962344200583
Loss in iteration 36 : 0.4873137194828633
Loss in iteration 37 : 0.4873437808557849
Loss in iteration 38 : 0.4818504116069087
Loss in iteration 39 : 0.4767542536682167
Loss in iteration 40 : 0.470341341451056
Loss in iteration 41 : 0.47494182378499156
Loss in iteration 42 : 0.4767116168778423
Loss in iteration 43 : 0.4784969456811113
Loss in iteration 44 : 0.4682512345523545
Loss in iteration 45 : 0.4846326154998142
Loss in iteration 46 : 0.4699701111654715
Loss in iteration 47 : 0.4766496214424256
Loss in iteration 48 : 0.4751417993200856
Loss in iteration 49 : 0.48433673449930853
Loss in iteration 50 : 0.48261295917563224
Loss in iteration 51 : 0.46718845482322213
Loss in iteration 52 : 0.48045984217541426
Loss in iteration 53 : 0.4761853445217272
Loss in iteration 54 : 0.46887819581607554
Loss in iteration 55 : 0.4700585140028773
Loss in iteration 56 : 0.47276014560199364
Loss in iteration 57 : 0.4703657860925286
Loss in iteration 58 : 0.4787531128487842
Loss in iteration 59 : 0.4845374812374752
Loss in iteration 60 : 0.4762876303685446
Loss in iteration 61 : 0.4730479391276588
Loss in iteration 62 : 0.47141800495955727
Loss in iteration 63 : 0.4761283078588026
Loss in iteration 64 : 0.47087356471416475
Loss in iteration 65 : 0.464531372794965
Loss in iteration 66 : 0.47790031646486125
Loss in iteration 67 : 0.46099709816853435
Loss in iteration 68 : 0.4825845593532932
Loss in iteration 69 : 0.4750310580361289
Loss in iteration 70 : 0.46224117208069965
Loss in iteration 71 : 0.47328773030872295
Loss in iteration 72 : 0.4733561469350206
Loss in iteration 73 : 0.4824767127050965
Loss in iteration 74 : 0.4760238779445667
Loss in iteration 75 : 0.47980331819937316
Loss in iteration 76 : 0.46572704787489194
Loss in iteration 77 : 0.4614078019232524
Loss in iteration 78 : 0.4578479780034911
Loss in iteration 79 : 0.47054506367201776
Loss in iteration 80 : 0.4692968550931968
Loss in iteration 81 : 0.4736572236615933
Loss in iteration 82 : 0.46259363529826025
Loss in iteration 83 : 0.4730145115265313
Loss in iteration 84 : 0.460688293441202
Loss in iteration 85 : 0.4701826846360033
Loss in iteration 86 : 0.4671800742593376
Loss in iteration 87 : 0.4650574527531783
Loss in iteration 88 : 0.4688333789814036
Loss in iteration 89 : 0.4648180444065078
Loss in iteration 90 : 0.47044424607465657
Loss in iteration 91 : 0.4662757900943497
Loss in iteration 92 : 0.469389821640892
Loss in iteration 93 : 0.4709195845397189
Loss in iteration 94 : 0.46555666845578353
Loss in iteration 95 : 0.46968922902190513
Loss in iteration 96 : 0.47072674415186294
Loss in iteration 97 : 0.4749697403209619
Loss in iteration 98 : 0.4674542453546208
Loss in iteration 99 : 0.45871461336294317
Loss in iteration 100 : 0.46076101726534563
Loss in iteration 101 : 0.4639283716814762
Loss in iteration 102 : 0.4675988515579763
Loss in iteration 103 : 0.47315181990554656
Loss in iteration 104 : 0.46976112255639746
Loss in iteration 105 : 0.4561904967844766
Loss in iteration 106 : 0.4735898764659099
Loss in iteration 107 : 0.4695410429403874
Loss in iteration 108 : 0.45762570934370916
Loss in iteration 109 : 0.47314231231500237
Loss in iteration 110 : 0.46435564781847993
Loss in iteration 111 : 0.4583695761014598
Loss in iteration 112 : 0.46630817037497624
Loss in iteration 113 : 0.46322990459151475
Loss in iteration 114 : 0.47202778797067424
Loss in iteration 115 : 0.46539045203401236
Loss in iteration 116 : 0.459751561395599
Loss in iteration 117 : 0.4645451613533016
Loss in iteration 118 : 0.465169224085642
Loss in iteration 119 : 0.4671342989375444
Loss in iteration 120 : 0.47047298384359326
Loss in iteration 121 : 0.46441914047761973
Loss in iteration 122 : 0.47216618171727265
Loss in iteration 123 : 0.4690754902393569
Loss in iteration 124 : 0.45962401319514073
Loss in iteration 125 : 0.4640597765422421
Loss in iteration 126 : 0.4690221477102965
Loss in iteration 127 : 0.4774475618765152
Loss in iteration 128 : 0.46174908405883036
Loss in iteration 129 : 0.4671810024955854
Loss in iteration 130 : 0.46632428334153947
Loss in iteration 131 : 0.46820823204815265
Loss in iteration 132 : 0.46290260117236076
Loss in iteration 133 : 0.46620739401362293
Loss in iteration 134 : 0.4630255304057097
Loss in iteration 135 : 0.46178126181944057
Loss in iteration 136 : 0.46842354762621324
Loss in iteration 137 : 0.4596586040086066
Loss in iteration 138 : 0.46231676717785886
Loss in iteration 139 : 0.4524633207834819
Loss in iteration 140 : 0.46779795085056936
Loss in iteration 141 : 0.4696749301895388
Loss in iteration 142 : 0.46725627971410394
Loss in iteration 143 : 0.45401915218590966
Loss in iteration 144 : 0.4639386551481631
Loss in iteration 145 : 0.4639498202326184
Loss in iteration 146 : 0.4560202396688834
Loss in iteration 147 : 0.46573819967654984
Loss in iteration 148 : 0.46136069059387014
Loss in iteration 149 : 0.46898815350602635
Loss in iteration 150 : 0.46511784909338855
Loss in iteration 151 : 0.4637106395679593
Loss in iteration 152 : 0.4744106395100162
Loss in iteration 153 : 0.4638728326880008
Loss in iteration 154 : 0.4650830703788839
Loss in iteration 155 : 0.45933011687211217
Loss in iteration 156 : 0.46197832836966213
Loss in iteration 157 : 0.4560685241741706
Loss in iteration 158 : 0.4525677700860176
Loss in iteration 159 : 0.464089679347267
Loss in iteration 160 : 0.4550849658253577
Loss in iteration 161 : 0.4580044820493653
Loss in iteration 162 : 0.46175934853107975
Loss in iteration 163 : 0.46899166159767236
Loss in iteration 164 : 0.4670655668580769
Loss in iteration 165 : 0.4727042751072314
Loss in iteration 166 : 0.46634206996613503
Loss in iteration 167 : 0.4769154492868213
Loss in iteration 168 : 0.45492141064777525
Loss in iteration 169 : 0.46478654571623834
Loss in iteration 170 : 0.4573615415727516
Loss in iteration 171 : 0.4656470460906511
Loss in iteration 172 : 0.4596411766147352
Loss in iteration 173 : 0.45919410390399973
Loss in iteration 174 : 0.45911532237633695
Loss in iteration 175 : 0.4623712601382415
Loss in iteration 176 : 0.4705146921329473
Loss in iteration 177 : 0.46799781363760296
Loss in iteration 178 : 0.4628327993575995
Loss in iteration 179 : 0.4645214455586226
Loss in iteration 180 : 0.4665938266545472
Loss in iteration 181 : 0.4684972070896863
Loss in iteration 182 : 0.4690118656131718
Loss in iteration 183 : 0.46850010744192266
Loss in iteration 184 : 0.4600502318279184
Loss in iteration 185 : 0.45682945695406996
Loss in iteration 186 : 0.45648898191146353
Loss in iteration 187 : 0.4515646714820985
Loss in iteration 188 : 0.473203390126982
Loss in iteration 189 : 0.46761037357655494
Loss in iteration 190 : 0.4692458603380619
Loss in iteration 191 : 0.4655107173476029
Loss in iteration 192 : 0.46601359857389774
Loss in iteration 193 : 0.45707557363630547
Loss in iteration 194 : 0.46091587011554946
Loss in iteration 195 : 0.4636267152691474
Loss in iteration 196 : 0.46954781890953906
Loss in iteration 197 : 0.457652905178121
Loss in iteration 198 : 0.4598978557301543
Loss in iteration 199 : 0.4662959974753073
Loss in iteration 200 : 0.45879786387934846
Testing accuracy  of updater 6 on alg 0 with rate 0.08000000000000002 = 0.7865, training accuracy 0.790125, time elapsed: 3804 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6903205893597892
Loss in iteration 3 : 0.6846563040410966
Loss in iteration 4 : 0.6805613752697377
Loss in iteration 5 : 0.6764850893030575
Loss in iteration 6 : 0.6702573991906088
Loss in iteration 7 : 0.6642683525613858
Loss in iteration 8 : 0.6580408966248469
Loss in iteration 9 : 0.6500846812692113
Loss in iteration 10 : 0.6450396380648381
Loss in iteration 11 : 0.6392045913265064
Loss in iteration 12 : 0.6312779209308295
Loss in iteration 13 : 0.6271101569262588
Loss in iteration 14 : 0.6214238595789234
Loss in iteration 15 : 0.6171322137507571
Loss in iteration 16 : 0.6073585154220841
Loss in iteration 17 : 0.605761439904561
Loss in iteration 18 : 0.5992667272137222
Loss in iteration 19 : 0.5936391898210915
Loss in iteration 20 : 0.5902241999488936
Loss in iteration 21 : 0.5839655354463524
Loss in iteration 22 : 0.5817040013317432
Loss in iteration 23 : 0.5753609856828191
Loss in iteration 24 : 0.5741465302538307
Loss in iteration 25 : 0.5640549243344964
Loss in iteration 26 : 0.5631131587879207
Loss in iteration 27 : 0.5546123627662096
Loss in iteration 28 : 0.5592079996779273
Loss in iteration 29 : 0.5501436866132388
Loss in iteration 30 : 0.5498611431580389
Loss in iteration 31 : 0.5420906106329947
Loss in iteration 32 : 0.5415484436144559
Loss in iteration 33 : 0.546438204682335
Loss in iteration 34 : 0.5435223126138644
Loss in iteration 35 : 0.5392404512460718
Loss in iteration 36 : 0.5377472923878598
Loss in iteration 37 : 0.5382293039959088
Loss in iteration 38 : 0.5329480901599694
Loss in iteration 39 : 0.5289054849374208
Loss in iteration 40 : 0.5239881866938183
Loss in iteration 41 : 0.5258348964134434
Loss in iteration 42 : 0.5221165822809815
Loss in iteration 43 : 0.5239790878247829
Loss in iteration 44 : 0.5178343194507544
Loss in iteration 45 : 0.5261561048859108
Loss in iteration 46 : 0.5144632650223875
Loss in iteration 47 : 0.5194061810079691
Loss in iteration 48 : 0.5177247813565009
Loss in iteration 49 : 0.521523502852403
Loss in iteration 50 : 0.5190354910698616
Loss in iteration 51 : 0.5089406079870841
Loss in iteration 52 : 0.5191659051734575
Loss in iteration 53 : 0.5137209521337548
Loss in iteration 54 : 0.5086219740990393
Loss in iteration 55 : 0.5095123916563296
Loss in iteration 56 : 0.5099300773102695
Loss in iteration 57 : 0.5065474260623731
Loss in iteration 58 : 0.5125908418684667
Loss in iteration 59 : 0.5138421637960137
Loss in iteration 60 : 0.5097145313445102
Loss in iteration 61 : 0.5052984096388513
Loss in iteration 62 : 0.504677874618744
Loss in iteration 63 : 0.5079690116205082
Loss in iteration 64 : 0.5024755960869267
Loss in iteration 65 : 0.499903769286663
Loss in iteration 66 : 0.5086495755029034
Loss in iteration 67 : 0.496952194858275
Loss in iteration 68 : 0.5076147880844195
Loss in iteration 69 : 0.502500073350033
Loss in iteration 70 : 0.4954643171679458
Loss in iteration 71 : 0.5049928204696104
Loss in iteration 72 : 0.502597320687709
Loss in iteration 73 : 0.5083172521890665
Loss in iteration 74 : 0.5038219448254874
Loss in iteration 75 : 0.504118395338987
Loss in iteration 76 : 0.49547658556292656
Loss in iteration 77 : 0.49276146288412503
Loss in iteration 78 : 0.4896967177266336
Loss in iteration 79 : 0.496894128749251
Loss in iteration 80 : 0.49636720771087134
Loss in iteration 81 : 0.5007908125224905
Loss in iteration 82 : 0.4922591668375766
Loss in iteration 83 : 0.49914020922850916
Loss in iteration 84 : 0.48933924400211043
Loss in iteration 85 : 0.4970548328608877
Loss in iteration 86 : 0.4937265845609617
Loss in iteration 87 : 0.4893618236504382
Loss in iteration 88 : 0.4936624128495646
Loss in iteration 89 : 0.4901716028717337
Loss in iteration 90 : 0.49446335881260284
Loss in iteration 91 : 0.4925480446524116
Loss in iteration 92 : 0.49165970888772914
Loss in iteration 93 : 0.49560524090335084
Loss in iteration 94 : 0.4905924938027837
Loss in iteration 95 : 0.4933022292696345
Loss in iteration 96 : 0.4943406374784681
Loss in iteration 97 : 0.496469807511811
Loss in iteration 98 : 0.4914752907947713
Loss in iteration 99 : 0.4850551136871169
Loss in iteration 100 : 0.48455316851405944
Loss in iteration 101 : 0.4881340363251637
Loss in iteration 102 : 0.4895486572594815
Loss in iteration 103 : 0.4949042136223494
Loss in iteration 104 : 0.4923624498728357
Loss in iteration 105 : 0.481978091377502
Loss in iteration 106 : 0.4949498188824467
Loss in iteration 107 : 0.4911964782475646
Loss in iteration 108 : 0.4824822964994603
Loss in iteration 109 : 0.49515686226085276
Loss in iteration 110 : 0.4867636774832324
Loss in iteration 111 : 0.48109047591269805
Loss in iteration 112 : 0.4884662302191045
Loss in iteration 113 : 0.484813034165256
Loss in iteration 114 : 0.4905019903533909
Loss in iteration 115 : 0.487368083428926
Loss in iteration 116 : 0.4841431448154858
Loss in iteration 117 : 0.48560999214224876
Loss in iteration 118 : 0.4866991328706489
Loss in iteration 119 : 0.4875783918679318
Loss in iteration 120 : 0.4903716967577524
Loss in iteration 121 : 0.48220208443993245
Loss in iteration 122 : 0.48892138217053854
Loss in iteration 123 : 0.4872892345212597
Loss in iteration 124 : 0.48012442861706583
Loss in iteration 125 : 0.48272304130071286
Loss in iteration 126 : 0.4880865720970235
Loss in iteration 127 : 0.49415540615473
Loss in iteration 128 : 0.48018344148013437
Loss in iteration 129 : 0.48660143457255506
Loss in iteration 130 : 0.4826713151347653
Loss in iteration 131 : 0.48655906856084635
Loss in iteration 132 : 0.4815061595629605
Loss in iteration 133 : 0.48403084046544625
Loss in iteration 134 : 0.4816801941503591
Loss in iteration 135 : 0.4790351448605618
Loss in iteration 136 : 0.4864684832987436
Loss in iteration 137 : 0.4771064460018091
Loss in iteration 138 : 0.47991051416989283
Loss in iteration 139 : 0.4739424625057155
Loss in iteration 140 : 0.48380854360836506
Loss in iteration 141 : 0.4866203603979653
Loss in iteration 142 : 0.48394320380630695
Loss in iteration 143 : 0.47411950774938244
Loss in iteration 144 : 0.4807213457786783
Loss in iteration 145 : 0.480155641800254
Loss in iteration 146 : 0.47548373715720466
Loss in iteration 147 : 0.4825122462983529
Loss in iteration 148 : 0.48008877626020263
Loss in iteration 149 : 0.48422514157115154
Loss in iteration 150 : 0.48157509328138415
Loss in iteration 151 : 0.481537182937736
Loss in iteration 152 : 0.4884293350449677
Loss in iteration 153 : 0.480340470893592
Loss in iteration 154 : 0.4821296424974154
Loss in iteration 155 : 0.47384398414277695
Loss in iteration 156 : 0.4771148851754472
Loss in iteration 157 : 0.4736443501689843
Loss in iteration 158 : 0.46941752942838877
Loss in iteration 159 : 0.4785719828193481
Loss in iteration 160 : 0.4728806653206086
Loss in iteration 161 : 0.474299080105498
Loss in iteration 162 : 0.4785868728843927
Loss in iteration 163 : 0.48400423839330703
Loss in iteration 164 : 0.48374124079722436
Loss in iteration 165 : 0.48463639941447473
Loss in iteration 166 : 0.47926910405276035
Loss in iteration 167 : 0.48895356830729586
Loss in iteration 168 : 0.472329818913846
Loss in iteration 169 : 0.4791072108383726
Loss in iteration 170 : 0.47393554389599957
Loss in iteration 171 : 0.4815479659277871
Loss in iteration 172 : 0.47548185374997703
Loss in iteration 173 : 0.4740398306521485
Loss in iteration 174 : 0.47331129870982164
Loss in iteration 175 : 0.47664583704240265
Loss in iteration 176 : 0.48243691192253785
Loss in iteration 177 : 0.4798765951529545
Loss in iteration 178 : 0.4748035829678081
Loss in iteration 179 : 0.4768514832655764
Loss in iteration 180 : 0.47992847460428445
Loss in iteration 181 : 0.48187178322578933
Loss in iteration 182 : 0.4812061954964158
Loss in iteration 183 : 0.48052746485476
Loss in iteration 184 : 0.47622596703968256
Loss in iteration 185 : 0.47165319995748883
Loss in iteration 186 : 0.4708571155483238
Loss in iteration 187 : 0.46580996011429876
Loss in iteration 188 : 0.4839032825263873
Loss in iteration 189 : 0.4820509675838152
Loss in iteration 190 : 0.47979877039942803
Loss in iteration 191 : 0.4802464107119504
Loss in iteration 192 : 0.47934095919447867
Loss in iteration 193 : 0.4703181033292387
Loss in iteration 194 : 0.4734121288745267
Loss in iteration 195 : 0.4778510744169276
Loss in iteration 196 : 0.4831928530716378
Loss in iteration 197 : 0.47010354836143153
Loss in iteration 198 : 0.47228560884235254
Loss in iteration 199 : 0.4771717425739361
Loss in iteration 200 : 0.47213647584615126
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.781, training accuracy 0.78325, time elapsed: 3778 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 22.62012566726576
Loss in iteration 3 : 12.467842659252312
Loss in iteration 4 : 11.402566221410284
Loss in iteration 5 : 7.406179052963869
Loss in iteration 6 : 7.6626064472475885
Loss in iteration 7 : 3.23234157405867
Loss in iteration 8 : 6.3912539270592434
Loss in iteration 9 : 5.596315945860311
Loss in iteration 10 : 5.873087174616311
Loss in iteration 11 : 5.158977646003453
Loss in iteration 12 : 3.1151414188622493
Loss in iteration 13 : 3.9138639748874424
Loss in iteration 14 : 4.21068327895325
Loss in iteration 15 : 3.825047825197974
Loss in iteration 16 : 3.3481722389777486
Loss in iteration 17 : 3.4750766876559074
Loss in iteration 18 : 3.390534881479578
Loss in iteration 19 : 3.6693788313807056
Loss in iteration 20 : 3.8657615722802454
Loss in iteration 21 : 2.9757223548011464
Loss in iteration 22 : 2.751641299076226
Loss in iteration 23 : 2.659352214987938
Loss in iteration 24 : 2.6111347962194484
Loss in iteration 25 : 2.5533569923854467
Loss in iteration 26 : 2.6818595106357974
Loss in iteration 27 : 2.1813292087192733
Loss in iteration 28 : 2.3668767023500115
Loss in iteration 29 : 2.308700507941127
Loss in iteration 30 : 1.8236645735366472
Loss in iteration 31 : 2.096331126125171
Loss in iteration 32 : 1.6403939242235275
Loss in iteration 33 : 1.9578098895337417
Loss in iteration 34 : 1.8998120657897328
Loss in iteration 35 : 1.683118321849743
Loss in iteration 36 : 1.770944667644246
Loss in iteration 37 : 1.428977829715293
Loss in iteration 38 : 1.6428275494939029
Loss in iteration 39 : 1.2680288971437412
Loss in iteration 40 : 1.4453438126615166
Loss in iteration 41 : 1.2735764498936042
Loss in iteration 42 : 1.3409519480443561
Loss in iteration 43 : 1.1741126794239014
Loss in iteration 44 : 1.150055521741822
Loss in iteration 45 : 1.2151934003175124
Loss in iteration 46 : 1.0284784305415937
Loss in iteration 47 : 1.215332880412786
Loss in iteration 48 : 1.0910840779613777
Loss in iteration 49 : 0.9923701536042419
Loss in iteration 50 : 1.147389748798307
Loss in iteration 51 : 1.118766785190442
Loss in iteration 52 : 1.034371289913518
Loss in iteration 53 : 0.8924146392704064
Loss in iteration 54 : 0.7520991114599234
Loss in iteration 55 : 0.9068187542583636
Loss in iteration 56 : 0.8444949529359923
Loss in iteration 57 : 0.8349168511156729
Loss in iteration 58 : 0.944397952904262
Loss in iteration 59 : 0.9577717476414124
Loss in iteration 60 : 1.396955917322289
Loss in iteration 61 : 1.3995579455307454
Loss in iteration 62 : 1.622690940564774
Loss in iteration 63 : 1.1913183874792297
Loss in iteration 64 : 0.8485752835332626
Loss in iteration 65 : 0.7442510376764034
Loss in iteration 66 : 0.6845633091022025
Loss in iteration 67 : 0.6290619367664589
Loss in iteration 68 : 0.7029495303435891
Loss in iteration 69 : 0.6529670973474898
Loss in iteration 70 : 0.6883588670835729
Loss in iteration 71 : 0.6410456473661562
Loss in iteration 72 : 0.9099389782403615
Loss in iteration 73 : 2.3448199612823912
Loss in iteration 74 : 2.752847565123957
Loss in iteration 75 : 0.9400375593385016
Loss in iteration 76 : 0.7645473937396376
Loss in iteration 77 : 1.8640690091877703
Loss in iteration 78 : 1.455212602414259
Loss in iteration 79 : 0.6878253743804114
Loss in iteration 80 : 1.4846556933888757
Loss in iteration 81 : 1.3750409830322652
Loss in iteration 82 : 0.6883862519918831
Loss in iteration 83 : 1.304657283431382
Loss in iteration 84 : 1.277188229854867
Loss in iteration 85 : 0.6988736487189352
Loss in iteration 86 : 1.1694973873576766
Loss in iteration 87 : 1.1705401304023109
Loss in iteration 88 : 0.7035625564561095
Loss in iteration 89 : 1.0437206052622963
Loss in iteration 90 : 1.070037996610822
Loss in iteration 91 : 0.6403513526740239
Loss in iteration 92 : 0.8604168557170017
Loss in iteration 93 : 0.8232698145590854
Loss in iteration 94 : 0.6822420566801732
Loss in iteration 95 : 0.8936752781257812
Loss in iteration 96 : 0.8186452676126266
Loss in iteration 97 : 0.7921335305519102
Loss in iteration 98 : 0.8012912460977377
Loss in iteration 99 : 0.6444500275209998
Loss in iteration 100 : 0.7336620998880036
Loss in iteration 101 : 0.9071274094225157
Loss in iteration 102 : 1.184781889821582
Loss in iteration 103 : 2.008778660408272
Loss in iteration 104 : 0.972450559434394
Loss in iteration 105 : 0.5580341271725711
Loss in iteration 106 : 1.1598558012262408
Loss in iteration 107 : 1.705608966456176
Loss in iteration 108 : 1.4799691350708792
Loss in iteration 109 : 0.7602508571471663
Loss in iteration 110 : 0.770133610024345
Loss in iteration 111 : 0.8939893260812425
Loss in iteration 112 : 1.0775777729488856
Loss in iteration 113 : 0.9225774305990623
Loss in iteration 114 : 0.642694030085249
Loss in iteration 115 : 0.8775040845286481
Loss in iteration 116 : 0.8204547450655123
Loss in iteration 117 : 0.7280704373571716
Loss in iteration 118 : 0.7778891299034322
Loss in iteration 119 : 0.6359044619157038
Loss in iteration 120 : 0.7121964350852865
Loss in iteration 121 : 0.6448280189410558
Loss in iteration 122 : 0.6685018486438496
Loss in iteration 123 : 0.6847939808909624
Loss in iteration 124 : 0.9508286341242227
Loss in iteration 125 : 1.4331075349612061
Loss in iteration 126 : 2.2636287420845402
Loss in iteration 127 : 1.0484844312688775
Loss in iteration 128 : 0.762038061651091
Loss in iteration 129 : 2.6719782445369695
Loss in iteration 130 : 1.0982598108165575
Loss in iteration 131 : 0.9596934893221659
Loss in iteration 132 : 2.398275900768244
Loss in iteration 133 : 0.9295711705287975
Loss in iteration 134 : 1.4160072508980364
Loss in iteration 135 : 2.331178781672807
Loss in iteration 136 : 0.7513642142884287
Loss in iteration 137 : 2.2178909957869224
Loss in iteration 138 : 1.1057609223277636
Loss in iteration 139 : 1.0156880946461164
Loss in iteration 140 : 1.845155486313651
Loss in iteration 141 : 0.8921706674244305
Loss in iteration 142 : 1.4719073473181072
Loss in iteration 143 : 1.232029656048786
Loss in iteration 144 : 1.0893361305925942
Loss in iteration 145 : 1.5020252567171908
Loss in iteration 146 : 0.822868446265403
Loss in iteration 147 : 1.2527471797097898
Loss in iteration 148 : 1.1308616744843791
Loss in iteration 149 : 0.807478439973379
Loss in iteration 150 : 1.1019990925673708
Loss in iteration 151 : 0.8457325492518833
Loss in iteration 152 : 0.8232882776589988
Loss in iteration 153 : 0.9697699896453309
Loss in iteration 154 : 0.8401562802438622
Loss in iteration 155 : 0.7623846726181601
Loss in iteration 156 : 0.6859937172108402
Loss in iteration 157 : 0.78997320495394
Loss in iteration 158 : 0.9668253981612464
Loss in iteration 159 : 1.3105009726358339
Loss in iteration 160 : 1.1965120328812502
Loss in iteration 161 : 1.2086795445029108
Loss in iteration 162 : 0.882703982938962
Loss in iteration 163 : 1.0380287082770787
Loss in iteration 164 : 0.976444760290809
Loss in iteration 165 : 1.4214504638949015
Loss in iteration 166 : 1.0489526319096496
Loss in iteration 167 : 0.8815382172150334
Loss in iteration 168 : 0.6769613323707355
Loss in iteration 169 : 0.6580668209740377
Loss in iteration 170 : 0.8460274632933406
Loss in iteration 171 : 1.2719588456969748
Loss in iteration 172 : 1.5235151344819233
Loss in iteration 173 : 1.0210338683127147
Loss in iteration 174 : 0.7661412710015469
Loss in iteration 175 : 0.6422821872982925
Loss in iteration 176 : 0.7011791996557424
Loss in iteration 177 : 0.6065190178265698
Loss in iteration 178 : 0.6413339685927102
Loss in iteration 179 : 0.6130113512702207
Loss in iteration 180 : 0.6912689055034797
Loss in iteration 181 : 0.8140498466186219
Loss in iteration 182 : 1.799165989710538
Loss in iteration 183 : 2.4543843157865055
Loss in iteration 184 : 1.0554627586190701
Loss in iteration 185 : 0.5856743103828478
Loss in iteration 186 : 0.6891648408614365
Loss in iteration 187 : 1.1316992031293465
Loss in iteration 188 : 1.4600430757927372
Loss in iteration 189 : 1.0019628532616023
Loss in iteration 190 : 0.6720784921954073
Loss in iteration 191 : 0.6648871191705936
Loss in iteration 192 : 0.966391336179869
Loss in iteration 193 : 1.297845524493517
Loss in iteration 194 : 1.0752033710068376
Loss in iteration 195 : 0.7658452234820836
Loss in iteration 196 : 0.6902299605693653
Loss in iteration 197 : 0.6226162232042779
Loss in iteration 198 : 0.6543750859162605
Loss in iteration 199 : 0.7522089125720646
Loss in iteration 200 : 1.1533718735331588
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.67, training accuracy 0.660375, time elapsed: 3748 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 1.7502824842332465
Loss in iteration 3 : 4.056784370481943
Loss in iteration 4 : 2.5853545381604093
Loss in iteration 5 : 4.769786576296644
Loss in iteration 6 : 4.0077134875094425
Loss in iteration 7 : 1.8761363902196886
Loss in iteration 8 : 2.932550001878594
Loss in iteration 9 : 0.8842254968465733
Loss in iteration 10 : 2.2463910757909775
Loss in iteration 11 : 1.0307295441995838
Loss in iteration 12 : 1.5754989426556318
Loss in iteration 13 : 1.662691675087816
Loss in iteration 14 : 1.1102987973928888
Loss in iteration 15 : 1.7918386750296398
Loss in iteration 16 : 1.2393851386286838
Loss in iteration 17 : 1.3762948351403894
Loss in iteration 18 : 1.5806489873987835
Loss in iteration 19 : 1.1031278837391343
Loss in iteration 20 : 1.4621244686038486
Loss in iteration 21 : 1.2821268687383167
Loss in iteration 22 : 1.1832430579219635
Loss in iteration 23 : 1.3346094318053259
Loss in iteration 24 : 1.0191907285268995
Loss in iteration 25 : 1.1349617572067594
Loss in iteration 26 : 1.1040337123691975
Loss in iteration 27 : 0.8726705819944865
Loss in iteration 28 : 1.1650085697552006
Loss in iteration 29 : 0.8574320880512543
Loss in iteration 30 : 1.0433135182255533
Loss in iteration 31 : 0.8550630217115913
Loss in iteration 32 : 0.8431822139655881
Loss in iteration 33 : 0.882024948834413
Loss in iteration 34 : 0.8006603065389473
Loss in iteration 35 : 0.8520107814468872
Loss in iteration 36 : 0.7286094903634479
Loss in iteration 37 : 0.7989568402264692
Loss in iteration 38 : 0.6909396623568516
Loss in iteration 39 : 0.7126579272332192
Loss in iteration 40 : 0.6523105040726158
Loss in iteration 41 : 0.6408117456772979
Loss in iteration 42 : 0.7390947048400668
Loss in iteration 43 : 0.5940944830941877
Loss in iteration 44 : 0.6546057482066509
Loss in iteration 45 : 0.6230642180677042
Loss in iteration 46 : 0.5752461907998325
Loss in iteration 47 : 0.6292193027097909
Loss in iteration 48 : 0.5812217395179238
Loss in iteration 49 : 0.5697474748412056
Loss in iteration 50 : 0.5722866152305432
Loss in iteration 51 : 0.5549149188548553
Loss in iteration 52 : 0.5333518833018833
Loss in iteration 53 : 0.5214805403696671
Loss in iteration 54 : 0.5017263533305433
Loss in iteration 55 : 0.515396523581134
Loss in iteration 56 : 0.5091294937526153
Loss in iteration 57 : 0.5035676529097638
Loss in iteration 58 : 0.49662915879147085
Loss in iteration 59 : 0.5266757527670223
Loss in iteration 60 : 0.5125028268450035
Loss in iteration 61 : 0.5245402356643379
Loss in iteration 62 : 0.5280112129672782
Loss in iteration 63 : 0.5241175996032177
Loss in iteration 64 : 0.4997447619058525
Loss in iteration 65 : 0.4877281904550822
Loss in iteration 66 : 0.5264804847853767
Loss in iteration 67 : 0.5633984812050946
Loss in iteration 68 : 0.7095341499768151
Loss in iteration 69 : 0.7769278511076843
Loss in iteration 70 : 0.7215706970117729
Loss in iteration 71 : 0.5894428338140035
Loss in iteration 72 : 0.5417448302449496
Loss in iteration 73 : 0.4984696215200305
Loss in iteration 74 : 0.5219106633083994
Loss in iteration 75 : 0.5480379891034444
Loss in iteration 76 : 0.6077962527796672
Loss in iteration 77 : 0.7137984802578871
Loss in iteration 78 : 0.8282756026588516
Loss in iteration 79 : 0.8472728225144358
Loss in iteration 80 : 0.6285233980969234
Loss in iteration 81 : 0.5415154991557214
Loss in iteration 82 : 0.4784957430923978
Loss in iteration 83 : 0.584840558769154
Loss in iteration 84 : 0.6360836106002079
Loss in iteration 85 : 0.6688663580379964
Loss in iteration 86 : 0.5496222645204677
Loss in iteration 87 : 0.5231534448744372
Loss in iteration 88 : 0.49563259667120513
Loss in iteration 89 : 0.6056033438128586
Loss in iteration 90 : 0.7534875413649336
Loss in iteration 91 : 0.795759756669162
Loss in iteration 92 : 0.5846444660389122
Loss in iteration 93 : 0.5022984395140905
Loss in iteration 94 : 0.48620444224025905
Loss in iteration 95 : 0.5143972536274529
Loss in iteration 96 : 0.5578453282286964
Loss in iteration 97 : 0.6132884223113511
Loss in iteration 98 : 0.6452103359327123
Loss in iteration 99 : 0.6034400224170994
Loss in iteration 100 : 0.5827775047257929
Loss in iteration 101 : 0.5331211106815799
Loss in iteration 102 : 0.514187378003261
Loss in iteration 103 : 0.502294277865833
Loss in iteration 104 : 0.5018159494131003
Loss in iteration 105 : 0.5636678025617806
Loss in iteration 106 : 0.7237877343238767
Loss in iteration 107 : 0.8956385937558431
Loss in iteration 108 : 0.7408696268565282
Loss in iteration 109 : 0.6420849547220642
Loss in iteration 110 : 0.5081516645817467
Loss in iteration 111 : 0.514064734784839
Loss in iteration 112 : 0.64264675546001
Loss in iteration 113 : 0.7720824424508073
Loss in iteration 114 : 0.6987681811657979
Loss in iteration 115 : 0.5615123196956857
Loss in iteration 116 : 0.483704849374099
Loss in iteration 117 : 0.5977946966138877
Loss in iteration 118 : 0.6631609986784457
Loss in iteration 119 : 0.7030827427784578
Loss in iteration 120 : 0.5525230354041932
Loss in iteration 121 : 0.5037276880902027
Loss in iteration 122 : 0.5224281398003988
Loss in iteration 123 : 0.5816566854863405
Loss in iteration 124 : 0.6431945618256081
Loss in iteration 125 : 0.707995440996185
Loss in iteration 126 : 0.7293132957687726
Loss in iteration 127 : 0.6671418375557846
Loss in iteration 128 : 0.5979179142053772
Loss in iteration 129 : 0.5072745681813161
Loss in iteration 130 : 0.5067408514140426
Loss in iteration 131 : 0.5220791932070714
Loss in iteration 132 : 0.5948501826307498
Loss in iteration 133 : 0.71136842316635
Loss in iteration 134 : 0.7898379340636154
Loss in iteration 135 : 0.7810209649743869
Loss in iteration 136 : 0.6250987201303867
Loss in iteration 137 : 0.5417608562228927
Loss in iteration 138 : 0.5098561474611006
Loss in iteration 139 : 0.48142201894685377
Loss in iteration 140 : 0.49804243586015123
Loss in iteration 141 : 0.4937824238519281
Loss in iteration 142 : 0.49996315000909375
Loss in iteration 143 : 0.46347526203669265
Loss in iteration 144 : 0.4902644766284294
Loss in iteration 145 : 0.4694462491146317
Loss in iteration 146 : 0.48001108159047895
Loss in iteration 147 : 0.49236586064394944
Loss in iteration 148 : 0.5105969218438248
Loss in iteration 149 : 0.7199109524731718
Loss in iteration 150 : 1.301404331264337
Loss in iteration 151 : 1.07162676471893
Loss in iteration 152 : 0.7501792076034981
Loss in iteration 153 : 0.5447763256579592
Loss in iteration 154 : 0.5558513579780434
Loss in iteration 155 : 0.6620433455414096
Loss in iteration 156 : 0.6974615832011215
Loss in iteration 157 : 0.5728160548241473
Loss in iteration 158 : 0.4914323764045228
Loss in iteration 159 : 0.503451769413931
Loss in iteration 160 : 0.5601518884543771
Loss in iteration 161 : 0.6622326849304491
Loss in iteration 162 : 0.711044754089688
Loss in iteration 163 : 0.6257062539227564
Loss in iteration 164 : 0.5474690930617743
Loss in iteration 165 : 0.5077850839763586
Loss in iteration 166 : 0.5237167285186776
Loss in iteration 167 : 0.5460469777223711
Loss in iteration 168 : 0.512079587066417
Loss in iteration 169 : 0.5041910769901002
Loss in iteration 170 : 0.4881105935473755
Loss in iteration 171 : 0.4990461970325637
Loss in iteration 172 : 0.49718397796658903
Loss in iteration 173 : 0.54344451654716
Loss in iteration 174 : 0.6934201019384111
Loss in iteration 175 : 1.1854688500214932
Loss in iteration 176 : 1.2010170936970508
Loss in iteration 177 : 0.819203808856311
Loss in iteration 178 : 0.5285172267455109
Loss in iteration 179 : 0.5685878600897437
Loss in iteration 180 : 0.79839338371088
Loss in iteration 181 : 0.8016674903783492
Loss in iteration 182 : 0.6623095382509587
Loss in iteration 183 : 0.5287255871138912
Loss in iteration 184 : 0.5237546498331006
Loss in iteration 185 : 0.6471465469672993
Loss in iteration 186 : 0.7158802800149343
Loss in iteration 187 : 0.6350806059706415
Loss in iteration 188 : 0.5322756275800075
Loss in iteration 189 : 0.5232959327532118
Loss in iteration 190 : 0.5460881997749283
Loss in iteration 191 : 0.6060805061205615
Loss in iteration 192 : 0.7249809422770702
Loss in iteration 193 : 0.7154988487669584
Loss in iteration 194 : 0.6734054327740494
Loss in iteration 195 : 0.5478841196879122
Loss in iteration 196 : 0.517420135842101
Loss in iteration 197 : 0.47910190207020137
Loss in iteration 198 : 0.48371540680801317
Loss in iteration 199 : 0.48748310930743344
Loss in iteration 200 : 0.47253491363915107
Testing accuracy  of updater 7 on alg 0 with rate 14.0 = 0.7675, training accuracy 0.7705, time elapsed: 4060 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.7987508457223725
Loss in iteration 3 : 1.4787056202522117
Loss in iteration 4 : 1.219663717827634
Loss in iteration 5 : 0.5781322097392411
Loss in iteration 6 : 1.2372894455266603
Loss in iteration 7 : 0.6659838896199533
Loss in iteration 8 : 0.6588100364800537
Loss in iteration 9 : 0.8013426786465232
Loss in iteration 10 : 0.562281896601669
Loss in iteration 11 : 0.7834876380665008
Loss in iteration 12 : 0.5348344217329098
Loss in iteration 13 : 0.7261600358159573
Loss in iteration 14 : 0.5781831158379555
Loss in iteration 15 : 0.7029317842238104
Loss in iteration 16 : 0.5791528750422843
Loss in iteration 17 : 0.6533784463275419
Loss in iteration 18 : 0.6161617281530922
Loss in iteration 19 : 0.6045521485257944
Loss in iteration 20 : 0.6288670182535614
Loss in iteration 21 : 0.5699111489339181
Loss in iteration 22 : 0.6035779461728213
Loss in iteration 23 : 0.5440910999890364
Loss in iteration 24 : 0.5578037059227232
Loss in iteration 25 : 0.511534446546601
Loss in iteration 26 : 0.5291559332091452
Loss in iteration 27 : 0.49846085480164803
Loss in iteration 28 : 0.547799266281521
Loss in iteration 29 : 0.48652757701573435
Loss in iteration 30 : 0.5185052038312078
Loss in iteration 31 : 0.48894246428575644
Loss in iteration 32 : 0.4872027649656165
Loss in iteration 33 : 0.5246530220752545
Loss in iteration 34 : 0.4980768672994045
Loss in iteration 35 : 0.5047835792921676
Loss in iteration 36 : 0.49193597229328156
Loss in iteration 37 : 0.5006194091530382
Loss in iteration 38 : 0.4958872029929979
Loss in iteration 39 : 0.48026350882500407
Loss in iteration 40 : 0.4903794450421896
Loss in iteration 41 : 0.46973457963424026
Loss in iteration 42 : 0.4915583086622296
Loss in iteration 43 : 0.48252616081121696
Loss in iteration 44 : 0.4618457152953874
Loss in iteration 45 : 0.49729211497287146
Loss in iteration 46 : 0.471260742355073
Loss in iteration 47 : 0.4750887346894169
Loss in iteration 48 : 0.48794206080244545
Loss in iteration 49 : 0.4808901407729682
Loss in iteration 50 : 0.48460518317994344
Loss in iteration 51 : 0.4816824511436683
Loss in iteration 52 : 0.4669862718607465
Loss in iteration 53 : 0.47599881854122833
Loss in iteration 54 : 0.4642154014550443
Loss in iteration 55 : 0.45889017298652585
Loss in iteration 56 : 0.47023086677632714
Loss in iteration 57 : 0.46513298801955943
Loss in iteration 58 : 0.4743047330558886
Loss in iteration 59 : 0.48932629356405694
Loss in iteration 60 : 0.471604732398036
Loss in iteration 61 : 0.4675360732865008
Loss in iteration 62 : 0.46367174349884327
Loss in iteration 63 : 0.47132252587129436
Loss in iteration 64 : 0.46253422394444765
Loss in iteration 65 : 0.45969020062765
Loss in iteration 66 : 0.48162059433617366
Loss in iteration 67 : 0.45651769339229503
Loss in iteration 68 : 0.4800568145350234
Loss in iteration 69 : 0.47183129672804647
Loss in iteration 70 : 0.46087228953981835
Loss in iteration 71 : 0.46522771259078416
Loss in iteration 72 : 0.4761727112219067
Loss in iteration 73 : 0.4773891164090878
Loss in iteration 74 : 0.47718439405328456
Loss in iteration 75 : 0.47704846936793616
Loss in iteration 76 : 0.46047037848077255
Loss in iteration 77 : 0.46138081814968357
Loss in iteration 78 : 0.448962715076056
Loss in iteration 79 : 0.4663651961474959
Loss in iteration 80 : 0.46699377876146936
Loss in iteration 81 : 0.47354178981021167
Loss in iteration 82 : 0.4537765025457215
Loss in iteration 83 : 0.4715057615009217
Loss in iteration 84 : 0.4641287865823425
Loss in iteration 85 : 0.4648452254380103
Loss in iteration 86 : 0.46935878172477985
Loss in iteration 87 : 0.46991833080464396
Loss in iteration 88 : 0.4649817251577919
Loss in iteration 89 : 0.4612963004866687
Loss in iteration 90 : 0.4681599212117307
Loss in iteration 91 : 0.4583625080592848
Loss in iteration 92 : 0.4710269687183631
Loss in iteration 93 : 0.4633234937779129
Loss in iteration 94 : 0.4685923869184975
Loss in iteration 95 : 0.4679516005055079
Loss in iteration 96 : 0.470781346153129
Loss in iteration 97 : 0.47707297333455445
Loss in iteration 98 : 0.469342261157069
Loss in iteration 99 : 0.4533964069023412
Loss in iteration 100 : 0.46565462679489633
Loss in iteration 101 : 0.47273534749636925
Loss in iteration 102 : 0.46577953166749914
Loss in iteration 103 : 0.4740588641366872
Loss in iteration 104 : 0.47077402850283107
Loss in iteration 105 : 0.4521087015113048
Loss in iteration 106 : 0.471473629221858
Loss in iteration 107 : 0.47448608091713224
Loss in iteration 108 : 0.4567716042480023
Loss in iteration 109 : 0.4701869855033648
Loss in iteration 110 : 0.4667501485236086
Loss in iteration 111 : 0.4696885594453757
Loss in iteration 112 : 0.47055036685682644
Loss in iteration 113 : 0.46100699478143187
Loss in iteration 114 : 0.4706786016247709
Loss in iteration 115 : 0.46254140281850115
Loss in iteration 116 : 0.4540179243799488
Loss in iteration 117 : 0.462779372616953
Loss in iteration 118 : 0.4615135391378125
Loss in iteration 119 : 0.4650330738894274
Loss in iteration 120 : 0.4641658966250128
Loss in iteration 121 : 0.46323461913049524
Loss in iteration 122 : 0.47375127417727314
Loss in iteration 123 : 0.4643878938021104
Loss in iteration 124 : 0.4586623747487853
Loss in iteration 125 : 0.4583965177149716
Loss in iteration 126 : 0.4742491942387529
Loss in iteration 127 : 0.4843516392429179
Loss in iteration 128 : 0.464706063496358
Loss in iteration 129 : 0.47283791419105387
Loss in iteration 130 : 0.4847337599139355
Loss in iteration 131 : 0.4770578352999126
Loss in iteration 132 : 0.4586590425376682
Loss in iteration 133 : 0.47077767978312535
Loss in iteration 134 : 0.4641042198474669
Loss in iteration 135 : 0.45980888597696196
Loss in iteration 136 : 0.4702218663978897
Loss in iteration 137 : 0.46209374248116586
Loss in iteration 138 : 0.4643329942277536
Loss in iteration 139 : 0.44908620991625503
Loss in iteration 140 : 0.46675817871091263
Loss in iteration 141 : 0.4678155356063137
Loss in iteration 142 : 0.46670919207076306
Loss in iteration 143 : 0.4501894907520951
Loss in iteration 144 : 0.45939120205335504
Loss in iteration 145 : 0.46400126502169164
Loss in iteration 146 : 0.4570159735899686
Loss in iteration 147 : 0.46334763708840115
Loss in iteration 148 : 0.46047765480632713
Loss in iteration 149 : 0.4797883715229353
Loss in iteration 150 : 0.47717686231903284
Loss in iteration 151 : 0.4646327596112166
Loss in iteration 152 : 0.4740660136412024
Loss in iteration 153 : 0.47116003748003527
Loss in iteration 154 : 0.476037400721691
Loss in iteration 155 : 0.461073894401333
Loss in iteration 156 : 0.46615973060338567
Loss in iteration 157 : 0.4827985265118551
Loss in iteration 158 : 0.4659380223843865
Loss in iteration 159 : 0.46592274521779514
Loss in iteration 160 : 0.4663113137892142
Loss in iteration 161 : 0.47416117411968006
Loss in iteration 162 : 0.4614488654666227
Loss in iteration 163 : 0.47362019157564367
Loss in iteration 164 : 0.4631802234054315
Loss in iteration 165 : 0.47595999721923676
Loss in iteration 166 : 0.4640348167599582
Loss in iteration 167 : 0.4787455441593985
Loss in iteration 168 : 0.46329229671142647
Loss in iteration 169 : 0.47147096608682904
Loss in iteration 170 : 0.4645866758886917
Loss in iteration 171 : 0.46223169413160564
Loss in iteration 172 : 0.4654910520541147
Loss in iteration 173 : 0.45548141555539434
Loss in iteration 174 : 0.46282770128253153
Loss in iteration 175 : 0.4665721261850609
Loss in iteration 176 : 0.4761000090359885
Loss in iteration 177 : 0.46588474123672063
Loss in iteration 178 : 0.4625122169578326
Loss in iteration 179 : 0.4644984913744057
Loss in iteration 180 : 0.4652173339957698
Loss in iteration 181 : 0.46766823692963444
Loss in iteration 182 : 0.4680961966112466
Loss in iteration 183 : 0.4745116292755216
Loss in iteration 184 : 0.46183409435765405
Loss in iteration 185 : 0.4577191145326933
Loss in iteration 186 : 0.4572507992574239
Loss in iteration 187 : 0.45301295424993177
Loss in iteration 188 : 0.47207460063551626
Loss in iteration 189 : 0.46716442862113006
Loss in iteration 190 : 0.4741981254108145
Loss in iteration 191 : 0.4683688024216706
Loss in iteration 192 : 0.47216601901885696
Loss in iteration 193 : 0.45624060719952875
Loss in iteration 194 : 0.4599531880402207
Loss in iteration 195 : 0.4658035618357741
Loss in iteration 196 : 0.4739666896405766
Loss in iteration 197 : 0.4587785664787071
Loss in iteration 198 : 0.45829341362846643
Loss in iteration 199 : 0.4762814996788192
Loss in iteration 200 : 0.47937740222772246
Testing accuracy  of updater 7 on alg 0 with rate 8.0 = 0.7785, training accuracy 0.774875, time elapsed: 3709 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6751486646113697
Loss in iteration 3 : 0.6500363427119794
Loss in iteration 4 : 0.6293995466955707
Loss in iteration 5 : 0.6032285424033866
Loss in iteration 6 : 0.5801668575813318
Loss in iteration 7 : 0.5535149708144284
Loss in iteration 8 : 0.5365230162336166
Loss in iteration 9 : 0.5222916237763734
Loss in iteration 10 : 0.5180236292438983
Loss in iteration 11 : 0.49915504137080025
Loss in iteration 12 : 0.49129539118530635
Loss in iteration 13 : 0.49806681021931876
Loss in iteration 14 : 0.4860919278657784
Loss in iteration 15 : 0.4979862138696915
Loss in iteration 16 : 0.4822652168019994
Loss in iteration 17 : 0.4967028518598119
Loss in iteration 18 : 0.4927821939564918
Loss in iteration 19 : 0.4931855770007356
Loss in iteration 20 : 0.4923649325569262
Loss in iteration 21 : 0.47922896152799277
Loss in iteration 22 : 0.4972804946108712
Loss in iteration 23 : 0.4812512749991381
Loss in iteration 24 : 0.49051364550515963
Loss in iteration 25 : 0.4726322500407863
Loss in iteration 26 : 0.4772002507884348
Loss in iteration 27 : 0.4681843777641884
Loss in iteration 28 : 0.4871578817492556
Loss in iteration 29 : 0.46831938889920194
Loss in iteration 30 : 0.4718604209439065
Loss in iteration 31 : 0.4624202418812464
Loss in iteration 32 : 0.46358112660700856
Loss in iteration 33 : 0.4797662033324563
Loss in iteration 34 : 0.48043146723327057
Loss in iteration 35 : 0.47392637327301945
Loss in iteration 36 : 0.4767458411241854
Loss in iteration 37 : 0.4748359202214134
Loss in iteration 38 : 0.46978617081411495
Loss in iteration 39 : 0.46304596281349913
Loss in iteration 40 : 0.45605524851135965
Loss in iteration 41 : 0.4606309791568744
Loss in iteration 42 : 0.4665167703421401
Loss in iteration 43 : 0.4655709955982691
Loss in iteration 44 : 0.45396258243478826
Loss in iteration 45 : 0.4730302017719647
Loss in iteration 46 : 0.4569254256156937
Loss in iteration 47 : 0.4636117399864721
Loss in iteration 48 : 0.46250793077733615
Loss in iteration 49 : 0.47352663417970764
Loss in iteration 50 : 0.4719495793128401
Loss in iteration 51 : 0.4575229927783909
Loss in iteration 52 : 0.469403878385163
Loss in iteration 53 : 0.46295462234148693
Loss in iteration 54 : 0.4554817343441283
Loss in iteration 55 : 0.4575576993186802
Loss in iteration 56 : 0.4610484886294874
Loss in iteration 57 : 0.46265788014854997
Loss in iteration 58 : 0.46988615420646707
Loss in iteration 59 : 0.47891234521769244
Loss in iteration 60 : 0.4673363595087484
Loss in iteration 61 : 0.46466263580167694
Loss in iteration 62 : 0.46264215079056076
Loss in iteration 63 : 0.4673825444852108
Loss in iteration 64 : 0.4642832141415915
Loss in iteration 65 : 0.45760880386921227
Loss in iteration 66 : 0.47010747834418415
Loss in iteration 67 : 0.45260122085641796
Loss in iteration 68 : 0.47648289256303167
Loss in iteration 69 : 0.46901402710493184
Loss in iteration 70 : 0.455534814017863
Loss in iteration 71 : 0.46609001175268283
Loss in iteration 72 : 0.4663841752014712
Loss in iteration 73 : 0.4750965844487521
Loss in iteration 74 : 0.46838011647108335
Loss in iteration 75 : 0.476622326008229
Loss in iteration 76 : 0.4581762854685012
Loss in iteration 77 : 0.45492898169048973
Loss in iteration 78 : 0.44926310280640475
Loss in iteration 79 : 0.46399962689202795
Loss in iteration 80 : 0.4636225450876282
Loss in iteration 81 : 0.4674538642647593
Loss in iteration 82 : 0.45671578623329734
Loss in iteration 83 : 0.46734491225015584
Loss in iteration 84 : 0.4553656893736341
Loss in iteration 85 : 0.4653288064170066
Loss in iteration 86 : 0.4617892508095551
Loss in iteration 87 : 0.46379326973859497
Loss in iteration 88 : 0.46549006987940345
Loss in iteration 89 : 0.45999764092100104
Loss in iteration 90 : 0.4656633074258347
Loss in iteration 91 : 0.46018152969425735
Loss in iteration 92 : 0.4670448757596827
Loss in iteration 93 : 0.4657079186324314
Loss in iteration 94 : 0.4598012648402395
Loss in iteration 95 : 0.4661472505561584
Loss in iteration 96 : 0.46688737858180784
Loss in iteration 97 : 0.472045794956743
Loss in iteration 98 : 0.46288277856899374
Loss in iteration 99 : 0.4541365791873321
Loss in iteration 100 : 0.4561005462604103
Loss in iteration 101 : 0.4581823876084256
Loss in iteration 102 : 0.4648613834412433
Loss in iteration 103 : 0.46919798718826194
Loss in iteration 104 : 0.4655658768371081
Loss in iteration 105 : 0.4503574307161694
Loss in iteration 106 : 0.46904693447012163
Loss in iteration 107 : 0.4657389291406587
Loss in iteration 108 : 0.4530927652503549
Loss in iteration 109 : 0.46892625872363936
Loss in iteration 110 : 0.46045220734475495
Loss in iteration 111 : 0.4548522026676015
Loss in iteration 112 : 0.46332869073186633
Loss in iteration 113 : 0.46019377430325636
Loss in iteration 114 : 0.4690096428199745
Loss in iteration 115 : 0.4619427030882525
Loss in iteration 116 : 0.45454980136579537
Loss in iteration 117 : 0.4613287089220062
Loss in iteration 118 : 0.46210553065728593
Loss in iteration 119 : 0.46321081551478743
Loss in iteration 120 : 0.46526883079594
Loss in iteration 121 : 0.46296964381613
Loss in iteration 122 : 0.4707709727615848
Loss in iteration 123 : 0.46444199357273075
Loss in iteration 124 : 0.4588282907128183
Loss in iteration 125 : 0.4601604641593832
Loss in iteration 126 : 0.46601218737926725
Loss in iteration 127 : 0.47595717997893533
Loss in iteration 128 : 0.46068546795184023
Loss in iteration 129 : 0.46394100954008444
Loss in iteration 130 : 0.46448010309671145
Loss in iteration 131 : 0.4660512896963036
Loss in iteration 132 : 0.4597968239264004
Loss in iteration 133 : 0.46412158414508675
Loss in iteration 134 : 0.460078310515047
Loss in iteration 135 : 0.45866385243655805
Loss in iteration 136 : 0.4672743201347561
Loss in iteration 137 : 0.45600074131647345
Loss in iteration 138 : 0.460655883848649
Loss in iteration 139 : 0.4494157208147494
Loss in iteration 140 : 0.4663484232541996
Loss in iteration 141 : 0.46832624839191
Loss in iteration 142 : 0.4657731247859056
Loss in iteration 143 : 0.4513114623754885
Loss in iteration 144 : 0.4611567214590409
Loss in iteration 145 : 0.4610259046150603
Loss in iteration 146 : 0.45328170404398654
Loss in iteration 147 : 0.4643337944544858
Loss in iteration 148 : 0.4588085940633336
Loss in iteration 149 : 0.46825627208346265
Loss in iteration 150 : 0.4630510950698615
Loss in iteration 151 : 0.4613088534000647
Loss in iteration 152 : 0.4723589627099257
Loss in iteration 153 : 0.4622114504612728
Loss in iteration 154 : 0.4616979269518615
Loss in iteration 155 : 0.45794660752657756
Loss in iteration 156 : 0.46027388950548914
Loss in iteration 157 : 0.4536246470617149
Loss in iteration 158 : 0.44986103330907734
Loss in iteration 159 : 0.46166810544519177
Loss in iteration 160 : 0.45119870970389325
Loss in iteration 161 : 0.4553005432493345
Loss in iteration 162 : 0.4596562718698844
Loss in iteration 163 : 0.4666629976408077
Loss in iteration 164 : 0.46437906544341556
Loss in iteration 165 : 0.47144640395985277
Loss in iteration 166 : 0.4635115100121493
Loss in iteration 167 : 0.4759263229852751
Loss in iteration 168 : 0.4515764776001254
Loss in iteration 169 : 0.4626459791169626
Loss in iteration 170 : 0.4551076526084037
Loss in iteration 171 : 0.4626755396569968
Loss in iteration 172 : 0.45714465716230523
Loss in iteration 173 : 0.45674533231316966
Loss in iteration 174 : 0.4569353326340986
Loss in iteration 175 : 0.4608786014068979
Loss in iteration 176 : 0.4694982358279169
Loss in iteration 177 : 0.4665147805724604
Loss in iteration 178 : 0.46171620259945534
Loss in iteration 179 : 0.4629959833574469
Loss in iteration 180 : 0.4651319836186744
Loss in iteration 181 : 0.4667265006770183
Loss in iteration 182 : 0.46705398255527497
Loss in iteration 183 : 0.4667377587085365
Loss in iteration 184 : 0.45824368517101444
Loss in iteration 185 : 0.4550617609483355
Loss in iteration 186 : 0.4543278159354204
Loss in iteration 187 : 0.45025738579023256
Loss in iteration 188 : 0.47299960081518583
Loss in iteration 189 : 0.46475575668054125
Loss in iteration 190 : 0.4701936150129377
Loss in iteration 191 : 0.4623990794789143
Loss in iteration 192 : 0.4642782164366311
Loss in iteration 193 : 0.4553180446980863
Loss in iteration 194 : 0.4593953428116243
Loss in iteration 195 : 0.4624689279314644
Loss in iteration 196 : 0.46964704628537207
Loss in iteration 197 : 0.4575109715818953
Loss in iteration 198 : 0.45839800705096506
Loss in iteration 199 : 0.46634727814468213
Loss in iteration 200 : 0.4599860140745088
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.7835, training accuracy 0.789625, time elapsed: 3843 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6815174951173947
Loss in iteration 3 : 0.6581502888281278
Loss in iteration 4 : 0.6370115488500713
Loss in iteration 5 : 0.618173864463105
Loss in iteration 6 : 0.5913259641525268
Loss in iteration 7 : 0.5676053153246371
Loss in iteration 8 : 0.5496698296742866
Loss in iteration 9 : 0.5334053696009069
Loss in iteration 10 : 0.5279531289046923
Loss in iteration 11 : 0.5083423449416966
Loss in iteration 12 : 0.49830104088455823
Loss in iteration 13 : 0.5042219960390065
Loss in iteration 14 : 0.4915430945256496
Loss in iteration 15 : 0.5025486152820449
Loss in iteration 16 : 0.48640842508329835
Loss in iteration 17 : 0.498643251048393
Loss in iteration 18 : 0.49386768634643613
Loss in iteration 19 : 0.4944868128915195
Loss in iteration 20 : 0.49309220157466477
Loss in iteration 21 : 0.4810269389169728
Loss in iteration 22 : 0.4969612195585318
Loss in iteration 23 : 0.4823259004227811
Loss in iteration 24 : 0.490855318071112
Loss in iteration 25 : 0.47407375466003027
Loss in iteration 26 : 0.4784178941448638
Loss in iteration 27 : 0.46944713627564577
Loss in iteration 28 : 0.4876498400554724
Loss in iteration 29 : 0.4695595586714178
Loss in iteration 30 : 0.47320177289287557
Loss in iteration 31 : 0.4639992633461863
Loss in iteration 32 : 0.46487775936790743
Loss in iteration 33 : 0.481438062946207
Loss in iteration 34 : 0.4810666579184527
Loss in iteration 35 : 0.4757609726340704
Loss in iteration 36 : 0.47891020320074157
Loss in iteration 37 : 0.47593159167583143
Loss in iteration 38 : 0.47071208164215655
Loss in iteration 39 : 0.46523201393924013
Loss in iteration 40 : 0.45795713673561095
Loss in iteration 41 : 0.4623868275512175
Loss in iteration 42 : 0.46784352158556375
Loss in iteration 43 : 0.46704798784133045
Loss in iteration 44 : 0.45466913501394113
Loss in iteration 45 : 0.4747618494162812
Loss in iteration 46 : 0.45810795327960807
Loss in iteration 47 : 0.4646233653539855
Loss in iteration 48 : 0.46378015572466497
Loss in iteration 49 : 0.47440602500382495
Loss in iteration 50 : 0.4721045493046628
Loss in iteration 51 : 0.45808730924868274
Loss in iteration 52 : 0.4703730182337593
Loss in iteration 53 : 0.4641309338087249
Loss in iteration 54 : 0.4565982041473109
Loss in iteration 55 : 0.4581346851267729
Loss in iteration 56 : 0.4618304514037667
Loss in iteration 57 : 0.46280468333543306
Loss in iteration 58 : 0.47046491367484533
Loss in iteration 59 : 0.47884613890379574
Loss in iteration 60 : 0.46759841709046596
Loss in iteration 61 : 0.4649215849676765
Loss in iteration 62 : 0.46315282642143846
Loss in iteration 63 : 0.4675561347651958
Loss in iteration 64 : 0.4638180061129779
Loss in iteration 65 : 0.4583300548131896
Loss in iteration 66 : 0.4706420168150402
Loss in iteration 67 : 0.4532400503423581
Loss in iteration 68 : 0.47612032126493137
Loss in iteration 69 : 0.46875451205325225
Loss in iteration 70 : 0.4553829783845478
Loss in iteration 71 : 0.4663777574238456
Loss in iteration 72 : 0.46673474759415423
Loss in iteration 73 : 0.4752202993613581
Loss in iteration 74 : 0.4685572974976219
Loss in iteration 75 : 0.47623152894641696
Loss in iteration 76 : 0.4585813753002922
Loss in iteration 77 : 0.45549118638194697
Loss in iteration 78 : 0.44949527425862523
Loss in iteration 79 : 0.4642950875662411
Loss in iteration 80 : 0.4641688378159284
Loss in iteration 81 : 0.4675015580636749
Loss in iteration 82 : 0.45691142370383664
Loss in iteration 83 : 0.46805819163725343
Loss in iteration 84 : 0.4555788326000039
Loss in iteration 85 : 0.46523257080226693
Loss in iteration 86 : 0.46236804822407956
Loss in iteration 87 : 0.46297957487365465
Loss in iteration 88 : 0.4655881742158948
Loss in iteration 89 : 0.4597507470863189
Loss in iteration 90 : 0.46528917689405347
Loss in iteration 91 : 0.4603044605817843
Loss in iteration 92 : 0.46689758876416954
Loss in iteration 93 : 0.4664933344912723
Loss in iteration 94 : 0.4603542434866245
Loss in iteration 95 : 0.466782764081239
Loss in iteration 96 : 0.4672922279447384
Loss in iteration 97 : 0.4722670155125
Loss in iteration 98 : 0.46289903968640395
Loss in iteration 99 : 0.4549820778876003
Loss in iteration 100 : 0.4569677775038957
Loss in iteration 101 : 0.45840788387608095
Loss in iteration 102 : 0.464945416364829
Loss in iteration 103 : 0.4700529501762738
Loss in iteration 104 : 0.4661050764856348
Loss in iteration 105 : 0.4505344227777658
Loss in iteration 106 : 0.46965900923381027
Loss in iteration 107 : 0.46589893276696265
Loss in iteration 108 : 0.4532546360964734
Loss in iteration 109 : 0.47002709263359443
Loss in iteration 110 : 0.45989027736778537
Loss in iteration 111 : 0.45521722049606134
Loss in iteration 112 : 0.4636499933204805
Loss in iteration 113 : 0.4594569939761291
Loss in iteration 114 : 0.46943298965682834
Loss in iteration 115 : 0.46222473058505664
Loss in iteration 116 : 0.45504638633980454
Loss in iteration 117 : 0.4617971210054195
Loss in iteration 118 : 0.46328522643284736
Loss in iteration 119 : 0.4634356137828576
Loss in iteration 120 : 0.4654144640117338
Loss in iteration 121 : 0.4635467316713569
Loss in iteration 122 : 0.4720395333837909
Loss in iteration 123 : 0.4652711432691452
Loss in iteration 124 : 0.4583542957403681
Loss in iteration 125 : 0.4612641119236516
Loss in iteration 126 : 0.46685920288253663
Loss in iteration 127 : 0.4766274861618821
Loss in iteration 128 : 0.46213261496840025
Loss in iteration 129 : 0.4639158040442522
Loss in iteration 130 : 0.46440846668406494
Loss in iteration 131 : 0.4677253378422093
Loss in iteration 132 : 0.45893371527047855
Loss in iteration 133 : 0.46343713108582935
Loss in iteration 134 : 0.4610149328034733
Loss in iteration 135 : 0.45898726145016483
Loss in iteration 136 : 0.4665101038922498
Loss in iteration 137 : 0.4574461033128418
Loss in iteration 138 : 0.45961251599522684
Loss in iteration 139 : 0.448906899941116
Loss in iteration 140 : 0.467540672135336
Loss in iteration 141 : 0.46664772608283367
Loss in iteration 142 : 0.46500142566358893
Loss in iteration 143 : 0.45198525878678575
Loss in iteration 144 : 0.4596215407975038
Loss in iteration 145 : 0.46162858210250746
Loss in iteration 146 : 0.45297258841189775
Loss in iteration 147 : 0.4639274858454662
Loss in iteration 148 : 0.4584424874882789
Loss in iteration 149 : 0.467142321194021
Loss in iteration 150 : 0.46305240993851005
Loss in iteration 151 : 0.461365801158316
Loss in iteration 152 : 0.4726820336904596
Loss in iteration 153 : 0.4609119259503313
Loss in iteration 154 : 0.4620146851938812
Loss in iteration 155 : 0.4569123552640107
Loss in iteration 156 : 0.46042495042351295
Loss in iteration 157 : 0.45339574829800056
Loss in iteration 158 : 0.45001593412345614
Loss in iteration 159 : 0.4616317768845874
Loss in iteration 160 : 0.4519292444313409
Loss in iteration 161 : 0.4554430723223778
Loss in iteration 162 : 0.4588957310847522
Loss in iteration 163 : 0.4665755739496362
Loss in iteration 164 : 0.46441172861732205
Loss in iteration 165 : 0.47190365009366914
Loss in iteration 166 : 0.4640505651779864
Loss in iteration 167 : 0.4753424593325572
Loss in iteration 168 : 0.451896035169715
Loss in iteration 169 : 0.46265201201438944
Loss in iteration 170 : 0.455631805941006
Loss in iteration 171 : 0.46315319078149647
Loss in iteration 172 : 0.45746802009103427
Loss in iteration 173 : 0.4572352507824962
Loss in iteration 174 : 0.4572286384678531
Loss in iteration 175 : 0.46108287198570563
Loss in iteration 176 : 0.46953284262414885
Loss in iteration 177 : 0.46647315903692166
Loss in iteration 178 : 0.4620089695405489
Loss in iteration 179 : 0.46301065999463337
Loss in iteration 180 : 0.46515958915512085
Loss in iteration 181 : 0.4665408571425514
Loss in iteration 182 : 0.46738071051974966
Loss in iteration 183 : 0.46710357522184576
Loss in iteration 184 : 0.45826622926616517
Loss in iteration 185 : 0.45563554198038264
Loss in iteration 186 : 0.45470008900669134
Loss in iteration 187 : 0.450180726744108
Loss in iteration 188 : 0.4732683275232962
Loss in iteration 189 : 0.4651780928632974
Loss in iteration 190 : 0.469360920544362
Loss in iteration 191 : 0.46324439501477827
Loss in iteration 192 : 0.46424809744286794
Loss in iteration 193 : 0.4551418034707501
Loss in iteration 194 : 0.46076049170219086
Loss in iteration 195 : 0.46199740766825753
Loss in iteration 196 : 0.46912082917460873
Loss in iteration 197 : 0.4577022541414252
Loss in iteration 198 : 0.4584125379891203
Loss in iteration 199 : 0.46525417806257297
Loss in iteration 200 : 0.4590755304890568
Testing accuracy  of updater 7 on alg 0 with rate 1.4 = 0.7825, training accuracy 0.788625, time elapsed: 4963 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.682740821597939
Loss in iteration 3 : 0.6659101476938483
Loss in iteration 4 : 0.6509846017937867
Loss in iteration 5 : 0.6328690144168472
Loss in iteration 6 : 0.6141153699958801
Loss in iteration 7 : 0.5923908976404177
Loss in iteration 8 : 0.5742808665023607
Loss in iteration 9 : 0.559028051351184
Loss in iteration 10 : 0.5509896512902502
Loss in iteration 11 : 0.531407323546014
Loss in iteration 12 : 0.51969437983915
Loss in iteration 13 : 0.5208976413188858
Loss in iteration 14 : 0.5081676507048549
Loss in iteration 15 : 0.5152115368199601
Loss in iteration 16 : 0.4986747633786657
Loss in iteration 17 : 0.5080594170251549
Loss in iteration 18 : 0.5011156448788145
Loss in iteration 19 : 0.5003965531672205
Loss in iteration 20 : 0.4975469760290413
Loss in iteration 21 : 0.4885219900264552
Loss in iteration 22 : 0.49944662827019987
Loss in iteration 23 : 0.4878777394484085
Loss in iteration 24 : 0.4933234082965647
Loss in iteration 25 : 0.47970365852070074
Loss in iteration 26 : 0.48335190010676093
Loss in iteration 27 : 0.4740568107891834
Loss in iteration 28 : 0.4895975923747722
Loss in iteration 29 : 0.47340650898499487
Loss in iteration 30 : 0.4771848051170837
Loss in iteration 31 : 0.4688606249186646
Loss in iteration 32 : 0.4696002972355564
Loss in iteration 33 : 0.48547615947926764
Loss in iteration 34 : 0.4836603886244497
Loss in iteration 35 : 0.4797834306955132
Loss in iteration 36 : 0.48299804054052964
Loss in iteration 37 : 0.4811248218723371
Loss in iteration 38 : 0.47492342191222786
Loss in iteration 39 : 0.46981451520636297
Loss in iteration 40 : 0.46323065708488487
Loss in iteration 41 : 0.46772927877176795
Loss in iteration 42 : 0.47139428479967826
Loss in iteration 43 : 0.4722083930464541
Loss in iteration 44 : 0.4598368922939259
Loss in iteration 45 : 0.4783043363211639
Loss in iteration 46 : 0.4632464173236836
Loss in iteration 47 : 0.4695205359874359
Loss in iteration 48 : 0.46825699461502307
Loss in iteration 49 : 0.4787253473609272
Loss in iteration 50 : 0.47609536188854096
Loss in iteration 51 : 0.46109244641955616
Loss in iteration 52 : 0.47448104893158927
Loss in iteration 53 : 0.4687489846813419
Loss in iteration 54 : 0.4610987011674262
Loss in iteration 55 : 0.4628218511294281
Loss in iteration 56 : 0.46515199492977766
Loss in iteration 57 : 0.46468999356438473
Loss in iteration 58 : 0.4730075453371694
Loss in iteration 59 : 0.4803501850036248
Loss in iteration 60 : 0.4703725737398118
Loss in iteration 61 : 0.46698795165129847
Loss in iteration 62 : 0.46556085873115777
Loss in iteration 63 : 0.47029343218607716
Loss in iteration 64 : 0.4650774223763129
Loss in iteration 65 : 0.45993911247962027
Loss in iteration 66 : 0.4726651206259601
Loss in iteration 67 : 0.45565782657778975
Loss in iteration 68 : 0.4782076312738088
Loss in iteration 69 : 0.4697675956570497
Loss in iteration 70 : 0.45759989398099915
Loss in iteration 71 : 0.4684062823980195
Loss in iteration 72 : 0.46916397132419774
Loss in iteration 73 : 0.4779155496463312
Loss in iteration 74 : 0.4704773606500682
Loss in iteration 75 : 0.47685235681593524
Loss in iteration 76 : 0.4607546487329082
Loss in iteration 77 : 0.45788217819518867
Loss in iteration 78 : 0.4523038792283929
Loss in iteration 79 : 0.46583066662453243
Loss in iteration 80 : 0.46544539270254537
Loss in iteration 81 : 0.4691205375508525
Loss in iteration 82 : 0.45842960834125374
Loss in iteration 83 : 0.46944456787115896
Loss in iteration 84 : 0.4576124198908356
Loss in iteration 85 : 0.466495218818433
Loss in iteration 86 : 0.4640245609659064
Loss in iteration 87 : 0.4629551756075477
Loss in iteration 88 : 0.4665115825394755
Loss in iteration 89 : 0.46129707730344277
Loss in iteration 90 : 0.4666206820744665
Loss in iteration 91 : 0.4622309783300281
Loss in iteration 92 : 0.4677804778748668
Loss in iteration 93 : 0.46828960820916965
Loss in iteration 94 : 0.4620030563609124
Loss in iteration 95 : 0.46754219036392974
Loss in iteration 96 : 0.4677279761147528
Loss in iteration 97 : 0.4731639019490609
Loss in iteration 98 : 0.46474505830187585
Loss in iteration 99 : 0.4546681154700864
Loss in iteration 100 : 0.457607746598411
Loss in iteration 101 : 0.46008931048078244
Loss in iteration 102 : 0.46547299656894714
Loss in iteration 103 : 0.47057615902243843
Loss in iteration 104 : 0.4671915275302062
Loss in iteration 105 : 0.45169794536392605
Loss in iteration 106 : 0.47085856645045304
Loss in iteration 107 : 0.46704642908619576
Loss in iteration 108 : 0.4544501166827514
Loss in iteration 109 : 0.4704701525248977
Loss in iteration 110 : 0.46220203275248384
Loss in iteration 111 : 0.4562042414744417
Loss in iteration 112 : 0.4635639929502459
Loss in iteration 113 : 0.46145311460940025
Loss in iteration 114 : 0.47041294525018695
Loss in iteration 115 : 0.46336531614341825
Loss in iteration 116 : 0.4566955358420207
Loss in iteration 117 : 0.46206942930049566
Loss in iteration 118 : 0.4638380675874543
Loss in iteration 119 : 0.46518812240810803
Loss in iteration 120 : 0.46698502437552597
Loss in iteration 121 : 0.4626453016195095
Loss in iteration 122 : 0.4728947629298435
Loss in iteration 123 : 0.4670450702508225
Loss in iteration 124 : 0.4583453002655499
Loss in iteration 125 : 0.4625085619680496
Loss in iteration 126 : 0.4693665549433301
Loss in iteration 127 : 0.47618963430087896
Loss in iteration 128 : 0.46082067608648697
Loss in iteration 129 : 0.46551197366324437
Loss in iteration 130 : 0.4652327938933119
Loss in iteration 131 : 0.46631501263002395
Loss in iteration 132 : 0.46026724748352144
Loss in iteration 133 : 0.4646853571698343
Loss in iteration 134 : 0.46029865168491224
Loss in iteration 135 : 0.46012239728008125
Loss in iteration 136 : 0.46692197546379316
Loss in iteration 137 : 0.4568809043090325
Loss in iteration 138 : 0.46108347158185314
Loss in iteration 139 : 0.45035385461926436
Loss in iteration 140 : 0.46725255526148446
Loss in iteration 141 : 0.46832838671778115
Loss in iteration 142 : 0.46594459532079646
Loss in iteration 143 : 0.4519690104277733
Loss in iteration 144 : 0.46109616226016636
Loss in iteration 145 : 0.4622667602357607
Loss in iteration 146 : 0.4537528577070864
Loss in iteration 147 : 0.46516673159331307
Loss in iteration 148 : 0.45988471144268617
Loss in iteration 149 : 0.4678422303489371
Loss in iteration 150 : 0.46358036895403076
Loss in iteration 151 : 0.46200502154415507
Loss in iteration 152 : 0.4731829594359253
Loss in iteration 153 : 0.46191014324426494
Loss in iteration 154 : 0.46293445513640863
Loss in iteration 155 : 0.45789308469107304
Loss in iteration 156 : 0.4609278503902105
Loss in iteration 157 : 0.4543407039428704
Loss in iteration 158 : 0.4509292432992185
Loss in iteration 159 : 0.4626675263025255
Loss in iteration 160 : 0.4532469933815318
Loss in iteration 161 : 0.45625215067477287
Loss in iteration 162 : 0.45968410802247534
Loss in iteration 163 : 0.4678666097837738
Loss in iteration 164 : 0.46561089416991125
Loss in iteration 165 : 0.4724119218725335
Loss in iteration 166 : 0.4648855205757106
Loss in iteration 167 : 0.4761091570296829
Loss in iteration 168 : 0.4529840206871728
Loss in iteration 169 : 0.46387589226542797
Loss in iteration 170 : 0.45657831966431256
Loss in iteration 171 : 0.46414556661176287
Loss in iteration 172 : 0.4585697529003872
Loss in iteration 173 : 0.4582510292023598
Loss in iteration 174 : 0.4578237581636458
Loss in iteration 175 : 0.46184002923960044
Loss in iteration 176 : 0.4698651343005474
Loss in iteration 177 : 0.4668432776095
Loss in iteration 178 : 0.46243718413851576
Loss in iteration 179 : 0.46396299046672596
Loss in iteration 180 : 0.4655878516793087
Loss in iteration 181 : 0.4671997911354418
Loss in iteration 182 : 0.46792381440347863
Loss in iteration 183 : 0.4684552034684773
Loss in iteration 184 : 0.45873137416382065
Loss in iteration 185 : 0.45545263773322586
Loss in iteration 186 : 0.4549470243664741
Loss in iteration 187 : 0.4504195128778961
Loss in iteration 188 : 0.47302045253079045
Loss in iteration 189 : 0.4662145644643045
Loss in iteration 190 : 0.4684419996826497
Loss in iteration 191 : 0.4639928687898779
Loss in iteration 192 : 0.4647592087147562
Loss in iteration 193 : 0.4555722594991963
Loss in iteration 194 : 0.4599026013156863
Loss in iteration 195 : 0.46225006578758593
Loss in iteration 196 : 0.4685102302444639
Loss in iteration 197 : 0.45734641687515803
Loss in iteration 198 : 0.45883609130868913
Loss in iteration 199 : 0.46514962162322315
Loss in iteration 200 : 0.4580172516088882
Testing accuracy  of updater 7 on alg 0 with rate 0.8 = 0.7865, training accuracy 0.788875, time elapsed: 5020 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6888235945178324
Loss in iteration 3 : 0.6806590148041163
Loss in iteration 4 : 0.6754911826900768
Loss in iteration 5 : 0.6697178641191434
Loss in iteration 6 : 0.6595165378340371
Loss in iteration 7 : 0.6489506480141799
Loss in iteration 8 : 0.6393418315853964
Loss in iteration 9 : 0.6304137452640288
Loss in iteration 10 : 0.6236579262652434
Loss in iteration 11 : 0.613235229214132
Loss in iteration 12 : 0.6036492807995703
Loss in iteration 13 : 0.5963534420466247
Loss in iteration 14 : 0.5871737773864033
Loss in iteration 15 : 0.5840320669200082
Loss in iteration 16 : 0.5707152899177165
Loss in iteration 17 : 0.5710026208053226
Loss in iteration 18 : 0.5623093462627632
Loss in iteration 19 : 0.5576902797679821
Loss in iteration 20 : 0.5526625070292162
Loss in iteration 21 : 0.5455456098078683
Loss in iteration 22 : 0.5453830996300365
Loss in iteration 23 : 0.5378817346179908
Loss in iteration 24 : 0.5393026718407692
Loss in iteration 25 : 0.5270404425353732
Loss in iteration 26 : 0.5275988034468438
Loss in iteration 27 : 0.5171854326263585
Loss in iteration 28 : 0.5256449246558478
Loss in iteration 29 : 0.5147784566211564
Loss in iteration 30 : 0.515408002488777
Loss in iteration 31 : 0.5077805398265778
Loss in iteration 32 : 0.5068987724802376
Loss in iteration 33 : 0.5160497674953791
Loss in iteration 34 : 0.5129363110534754
Loss in iteration 35 : 0.5099465910707488
Loss in iteration 36 : 0.5088555655321295
Loss in iteration 37 : 0.5098347755947104
Loss in iteration 38 : 0.5051440636700427
Loss in iteration 39 : 0.49968069534415316
Loss in iteration 40 : 0.4948298142192868
Loss in iteration 41 : 0.4982532493716908
Loss in iteration 42 : 0.49628371101787677
Loss in iteration 43 : 0.49841612600425805
Loss in iteration 44 : 0.4909413204832327
Loss in iteration 45 : 0.5022266058446747
Loss in iteration 46 : 0.48974733515127566
Loss in iteration 47 : 0.49573693395990615
Loss in iteration 48 : 0.49470353604176204
Loss in iteration 49 : 0.5002408344934683
Loss in iteration 50 : 0.4976057987894405
Loss in iteration 51 : 0.4852488646419565
Loss in iteration 52 : 0.49799824771627416
Loss in iteration 53 : 0.4931751726723897
Loss in iteration 54 : 0.48710617200256257
Loss in iteration 55 : 0.48850298440209516
Loss in iteration 56 : 0.489851292949872
Loss in iteration 57 : 0.48634580545025613
Loss in iteration 58 : 0.4935563679044054
Loss in iteration 59 : 0.49651066995934706
Loss in iteration 60 : 0.4917327413838757
Loss in iteration 61 : 0.48736669939403277
Loss in iteration 62 : 0.4868062424059523
Loss in iteration 63 : 0.49062731978281515
Loss in iteration 64 : 0.4852169648768431
Loss in iteration 65 : 0.4811958910939274
Loss in iteration 66 : 0.4920057257841956
Loss in iteration 67 : 0.477449626920211
Loss in iteration 68 : 0.4933127366586647
Loss in iteration 69 : 0.48726072397511505
Loss in iteration 70 : 0.47711060569272984
Loss in iteration 71 : 0.48811207765525916
Loss in iteration 72 : 0.486639935537443
Loss in iteration 73 : 0.494761053613521
Loss in iteration 74 : 0.4888089225740383
Loss in iteration 75 : 0.49098308341905256
Loss in iteration 76 : 0.4794589314942769
Loss in iteration 77 : 0.4765222341130268
Loss in iteration 78 : 0.47340765961794773
Loss in iteration 79 : 0.4822455561514874
Loss in iteration 80 : 0.48141658215886973
Loss in iteration 81 : 0.4863202045518604
Loss in iteration 82 : 0.4762864623690697
Loss in iteration 83 : 0.4846393277357272
Loss in iteration 84 : 0.47371610593810737
Loss in iteration 85 : 0.4832010513819007
Loss in iteration 86 : 0.4794120209619357
Loss in iteration 87 : 0.47574241557028424
Loss in iteration 88 : 0.479460380621309
Loss in iteration 89 : 0.4755473881437438
Loss in iteration 90 : 0.48149011543996173
Loss in iteration 91 : 0.47859678863982785
Loss in iteration 92 : 0.47893550053602546
Loss in iteration 93 : 0.48223274551770007
Loss in iteration 94 : 0.47722525621801987
Loss in iteration 95 : 0.48059464362487037
Loss in iteration 96 : 0.4813504540083029
Loss in iteration 97 : 0.48514323321362657
Loss in iteration 98 : 0.4786874764656648
Loss in iteration 99 : 0.4707080458607142
Loss in iteration 100 : 0.47129938534917976
Loss in iteration 101 : 0.47527784266551454
Loss in iteration 102 : 0.4771815869009084
Loss in iteration 103 : 0.4833087294428761
Loss in iteration 104 : 0.47967767882958945
Loss in iteration 105 : 0.4682477333692315
Loss in iteration 106 : 0.48334725050021665
Loss in iteration 107 : 0.4794262080628251
Loss in iteration 108 : 0.4691090255021117
Loss in iteration 109 : 0.4835157274326995
Loss in iteration 110 : 0.4739778087429964
Loss in iteration 111 : 0.4680850895788105
Loss in iteration 112 : 0.47614054221758567
Loss in iteration 113 : 0.47271670998331106
Loss in iteration 114 : 0.47994353413982316
Loss in iteration 115 : 0.47521291737314464
Loss in iteration 116 : 0.4711128862082453
Loss in iteration 117 : 0.47379675692512335
Loss in iteration 118 : 0.4747125022420456
Loss in iteration 119 : 0.4763782325386204
Loss in iteration 120 : 0.4798743914059513
Loss in iteration 121 : 0.4722742779534203
Loss in iteration 122 : 0.47858930509418013
Loss in iteration 123 : 0.4763898260403223
Loss in iteration 124 : 0.4685271511984642
Loss in iteration 125 : 0.47216170620858666
Loss in iteration 126 : 0.47785658461091873
Loss in iteration 127 : 0.4845291113722518
Loss in iteration 128 : 0.46986038011057724
Loss in iteration 129 : 0.47547304138167606
Loss in iteration 130 : 0.4726006715283088
Loss in iteration 131 : 0.47641586433870226
Loss in iteration 132 : 0.4708887773130755
Loss in iteration 133 : 0.47374021576060876
Loss in iteration 134 : 0.47067429396069194
Loss in iteration 135 : 0.469140958464352
Loss in iteration 136 : 0.47603818840741824
Loss in iteration 137 : 0.4669388516629374
Loss in iteration 138 : 0.4696375569037152
Loss in iteration 139 : 0.4619041083553273
Loss in iteration 140 : 0.47424068301263517
Loss in iteration 141 : 0.4767231116615676
Loss in iteration 142 : 0.4741443821384463
Loss in iteration 143 : 0.4625613387279897
Loss in iteration 144 : 0.4708770971398923
Loss in iteration 145 : 0.4703579130448159
Loss in iteration 146 : 0.46429568566926477
Loss in iteration 147 : 0.4729404520198944
Loss in iteration 148 : 0.4692566381265255
Loss in iteration 149 : 0.4747443656617927
Loss in iteration 150 : 0.47184770152208355
Loss in iteration 151 : 0.4711976601587852
Loss in iteration 152 : 0.48024114526868306
Loss in iteration 153 : 0.4709241638452499
Loss in iteration 154 : 0.47280230419894925
Loss in iteration 155 : 0.46453574925538044
Loss in iteration 156 : 0.4677086299073972
Loss in iteration 157 : 0.4632369104797731
Loss in iteration 158 : 0.4592121852726284
Loss in iteration 159 : 0.4698270544974823
Loss in iteration 160 : 0.4629504618953344
Loss in iteration 161 : 0.46513455944783244
Loss in iteration 162 : 0.4687524415547321
Loss in iteration 163 : 0.47483337848936263
Loss in iteration 164 : 0.4736516543818324
Loss in iteration 165 : 0.47736784408575655
Loss in iteration 166 : 0.4715288713050711
Loss in iteration 167 : 0.4814064857851367
Loss in iteration 168 : 0.46225569466264016
Loss in iteration 169 : 0.47046208034343484
Loss in iteration 170 : 0.4644523848759537
Loss in iteration 171 : 0.4725347429922533
Loss in iteration 172 : 0.46617302665215193
Loss in iteration 173 : 0.4652264798128798
Loss in iteration 174 : 0.46441591183242154
Loss in iteration 175 : 0.4677900328639917
Loss in iteration 176 : 0.4749372646638233
Loss in iteration 177 : 0.4726601344459194
Loss in iteration 178 : 0.4670670553101726
Loss in iteration 179 : 0.469601860659041
Loss in iteration 180 : 0.4715689302940663
Loss in iteration 181 : 0.4734452564522242
Loss in iteration 182 : 0.47367713304495723
Loss in iteration 183 : 0.47292826551519906
Loss in iteration 184 : 0.46654591920523764
Loss in iteration 185 : 0.46244324917134116
Loss in iteration 186 : 0.4619462807461716
Loss in iteration 187 : 0.45702517976634044
Loss in iteration 188 : 0.4772763171540477
Loss in iteration 189 : 0.47345057203692786
Loss in iteration 190 : 0.47270500107780816
Loss in iteration 191 : 0.4712576137600706
Loss in iteration 192 : 0.4712839921105774
Loss in iteration 193 : 0.46215784750360017
Loss in iteration 194 : 0.4657663009031582
Loss in iteration 195 : 0.46932455677465407
Loss in iteration 196 : 0.4749336513334103
Loss in iteration 197 : 0.46236732616555426
Loss in iteration 198 : 0.4645979534486189
Loss in iteration 199 : 0.47021903114744035
Loss in iteration 200 : 0.46374412701012435
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.7805, training accuracy 0.788125, time elapsed: 4415 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 5.246912620554848
Loss in iteration 3 : 4.947996978169312
Loss in iteration 4 : 1.4970144274448698
Loss in iteration 5 : 1.1395698296318575
Loss in iteration 6 : 0.8417608954432746
Loss in iteration 7 : 0.7935532484609094
Loss in iteration 8 : 0.899986735485158
Loss in iteration 9 : 0.8819373609160917
Loss in iteration 10 : 0.8204444190547348
Loss in iteration 11 : 0.7021686906972617
Loss in iteration 12 : 0.6403693698239344
Loss in iteration 13 : 0.659080836456554
Loss in iteration 14 : 0.6314582417107848
Loss in iteration 15 : 0.6554090164026666
Loss in iteration 16 : 0.665748688329764
Loss in iteration 17 : 0.6897734504309437
Loss in iteration 18 : 0.6692986277568824
Loss in iteration 19 : 0.6467288435350328
Loss in iteration 20 : 0.6502391866234736
Loss in iteration 21 : 0.6021929728383907
Loss in iteration 22 : 0.6418002346797483
Loss in iteration 23 : 0.6326136245714081
Loss in iteration 24 : 0.7049145000339121
Loss in iteration 25 : 0.785131000410847
Loss in iteration 26 : 0.8249322398239805
Loss in iteration 27 : 0.7381037371066497
Loss in iteration 28 : 0.6918343603955598
Loss in iteration 29 : 0.66892270698391
Loss in iteration 30 : 0.700941571390029
Loss in iteration 31 : 0.6618722939378274
Loss in iteration 32 : 0.6358369187907111
Loss in iteration 33 : 0.6466591767913659
Loss in iteration 34 : 0.6219921995612019
Loss in iteration 35 : 0.6241493836465659
Loss in iteration 36 : 0.6482457273760186
Loss in iteration 37 : 0.6654469395909579
Loss in iteration 38 : 0.707944775617796
Loss in iteration 39 : 0.7458378352238677
Loss in iteration 40 : 0.7408131226157998
Loss in iteration 41 : 0.7251468834754249
Loss in iteration 42 : 0.7286016680812517
Loss in iteration 43 : 0.6988285891032245
Loss in iteration 44 : 0.6571469058117889
Loss in iteration 45 : 0.686866076364838
Loss in iteration 46 : 0.6441108741073986
Loss in iteration 47 : 0.614308574932775
Loss in iteration 48 : 0.6072219279004077
Loss in iteration 49 : 0.6133803642149485
Loss in iteration 50 : 0.6069636246578816
Loss in iteration 51 : 0.597282973519302
Loss in iteration 52 : 0.6092022394451367
Loss in iteration 53 : 0.607660566589815
Loss in iteration 54 : 0.6037849491000769
Loss in iteration 55 : 0.58011796127138
Loss in iteration 56 : 0.5844109800553767
Loss in iteration 57 : 0.5668685266245728
Loss in iteration 58 : 0.5844734181995279
Loss in iteration 59 : 0.6073626674402957
Loss in iteration 60 : 0.603044037849741
Loss in iteration 61 : 0.6106246064979447
Loss in iteration 62 : 0.6625735890284806
Loss in iteration 63 : 0.7064367148258393
Loss in iteration 64 : 0.7109933269012263
Loss in iteration 65 : 0.639859216409033
Loss in iteration 66 : 0.6293009125455027
Loss in iteration 67 : 0.6299991968060628
Loss in iteration 68 : 0.6806650685086054
Loss in iteration 69 : 0.6451271477964674
Loss in iteration 70 : 0.6232163446604746
Loss in iteration 71 : 0.6347020224524749
Loss in iteration 72 : 0.6638973851215687
Loss in iteration 73 : 0.6958892453642973
Loss in iteration 74 : 0.6886303179104869
Loss in iteration 75 : 0.6968218667900576
Loss in iteration 76 : 0.68993420777106
Loss in iteration 77 : 0.6527428629524653
Loss in iteration 78 : 0.6614106937655698
Loss in iteration 79 : 0.6843894773011251
Loss in iteration 80 : 0.6816358265319192
Loss in iteration 81 : 0.6427728974265281
Loss in iteration 82 : 0.6244818419792321
Loss in iteration 83 : 0.6418270932430898
Loss in iteration 84 : 0.6133688375810029
Loss in iteration 85 : 0.648050191602025
Loss in iteration 86 : 0.6604463801939103
Loss in iteration 87 : 0.6554509226714791
Loss in iteration 88 : 0.6235650681136741
Loss in iteration 89 : 0.6275284038967169
Loss in iteration 90 : 0.648783057008924
Loss in iteration 91 : 0.6025842719090581
Loss in iteration 92 : 0.598803503526078
Loss in iteration 93 : 0.6333105510762078
Loss in iteration 94 : 0.6633578288204014
Loss in iteration 95 : 0.6748798388246149
Loss in iteration 96 : 0.6729683164667823
Loss in iteration 97 : 0.6649970583492647
Loss in iteration 98 : 0.6444776565690442
Loss in iteration 99 : 0.6706160420331749
Loss in iteration 100 : 0.6958210472520199
Loss in iteration 101 : 0.6476719750746667
Loss in iteration 102 : 0.6409978366942668
Loss in iteration 103 : 0.6493349276720041
Loss in iteration 104 : 0.6492055416526669
Loss in iteration 105 : 0.6588588590832284
Loss in iteration 106 : 0.6809567307644158
Loss in iteration 107 : 0.6814642650868623
Loss in iteration 108 : 0.6503483749065693
Loss in iteration 109 : 0.6701350851575113
Loss in iteration 110 : 0.616791576847828
Loss in iteration 111 : 0.6104816138946476
Loss in iteration 112 : 0.6367007172841459
Loss in iteration 113 : 0.6165491288678031
Loss in iteration 114 : 0.6243797594657978
Loss in iteration 115 : 0.592048249903395
Loss in iteration 116 : 0.5938039653427601
Loss in iteration 117 : 0.6320057040045162
Loss in iteration 118 : 0.6189452561912387
Loss in iteration 119 : 0.6383890173759701
Loss in iteration 120 : 0.6236238714599343
Loss in iteration 121 : 0.6188979575702044
Loss in iteration 122 : 0.6594255777003749
Loss in iteration 123 : 0.6587908245198616
Loss in iteration 124 : 0.6440261988094648
Loss in iteration 125 : 0.6378679911978952
Loss in iteration 126 : 0.6220764041418883
Loss in iteration 127 : 0.6390410130048554
Loss in iteration 128 : 0.6458554962177099
Loss in iteration 129 : 0.6733047078664094
Loss in iteration 130 : 0.6812937773196613
Loss in iteration 131 : 0.6838903928254665
Loss in iteration 132 : 0.6742825408078557
Loss in iteration 133 : 0.6752522096698998
Loss in iteration 134 : 0.67437149979611
Loss in iteration 135 : 0.6908594033582849
Loss in iteration 136 : 0.6572753040311887
Loss in iteration 137 : 0.6467875796262501
Loss in iteration 138 : 0.6786990847544448
Loss in iteration 139 : 0.6237731361108598
Loss in iteration 140 : 0.6294864963910426
Loss in iteration 141 : 0.6398475254670621
Loss in iteration 142 : 0.6425164945562669
Loss in iteration 143 : 0.5944423092633874
Loss in iteration 144 : 0.6105102116401578
Loss in iteration 145 : 0.6283017333218723
Loss in iteration 146 : 0.6382517577304593
Loss in iteration 147 : 0.6354409889513959
Loss in iteration 148 : 0.6143267971498687
Loss in iteration 149 : 0.602026684043026
Loss in iteration 150 : 0.5943960443812739
Loss in iteration 151 : 0.6041525753220763
Loss in iteration 152 : 0.6402960886735601
Loss in iteration 153 : 0.6733384366004126
Loss in iteration 154 : 0.6953110294652566
Loss in iteration 155 : 0.6667356393566978
Loss in iteration 156 : 0.6289071248716401
Loss in iteration 157 : 0.6008860763487377
Loss in iteration 158 : 0.6021906264542486
Loss in iteration 159 : 0.633616331650804
Loss in iteration 160 : 0.6089909234974256
Loss in iteration 161 : 0.6060055450088304
Loss in iteration 162 : 0.6269660201881699
Loss in iteration 163 : 0.7047364519921768
Loss in iteration 164 : 0.7513795065929675
Loss in iteration 165 : 0.6867946599550424
Loss in iteration 166 : 0.639045157511958
Loss in iteration 167 : 0.6349981388263843
Loss in iteration 168 : 0.5650692017460452
Loss in iteration 169 : 0.5843791590455404
Loss in iteration 170 : 0.5855480592747864
Loss in iteration 171 : 0.6128527407499931
Loss in iteration 172 : 0.5955913335891027
Loss in iteration 173 : 0.5870102018836517
Loss in iteration 174 : 0.6047121083494373
Loss in iteration 175 : 0.6448593066624642
Loss in iteration 176 : 0.6912918801772218
Loss in iteration 177 : 0.640343848580135
Loss in iteration 178 : 0.6080074959535754
Loss in iteration 179 : 0.6211785720954351
Loss in iteration 180 : 0.6471649854680884
Loss in iteration 181 : 0.6504685915815481
Loss in iteration 182 : 0.6580186490182192
Loss in iteration 183 : 0.648119856796931
Loss in iteration 184 : 0.6491989317529103
Loss in iteration 185 : 0.6751088907843091
Loss in iteration 186 : 0.64577232923324
Loss in iteration 187 : 0.6214764347143341
Loss in iteration 188 : 0.6372432529510594
Loss in iteration 189 : 0.6380081020017466
Loss in iteration 190 : 0.6341424010027306
Loss in iteration 191 : 0.6003810880004745
Loss in iteration 192 : 0.5960091365651633
Loss in iteration 193 : 0.5931117553802655
Loss in iteration 194 : 0.6173171958630082
Loss in iteration 195 : 0.6449015558026314
Loss in iteration 196 : 0.6547488547600662
Loss in iteration 197 : 0.6416542903899234
Loss in iteration 198 : 0.6300936668825678
Loss in iteration 199 : 0.6422964768790322
Loss in iteration 200 : 0.6488005295217346
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.696, training accuracy 0.70375, time elapsed: 4258 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.9230250673453906
Loss in iteration 3 : 1.4363789257677655
Loss in iteration 4 : 1.7245801871649358
Loss in iteration 5 : 0.6990468508787184
Loss in iteration 6 : 0.7525337359021501
Loss in iteration 7 : 0.9161447001569395
Loss in iteration 8 : 0.885607574716622
Loss in iteration 9 : 0.6626063828538956
Loss in iteration 10 : 0.6120210873248008
Loss in iteration 11 : 0.53363119837367
Loss in iteration 12 : 0.4943276140061816
Loss in iteration 13 : 0.5011476928983476
Loss in iteration 14 : 0.48505524002934897
Loss in iteration 15 : 0.5029242510497468
Loss in iteration 16 : 0.49005489461252727
Loss in iteration 17 : 0.5110931756170973
Loss in iteration 18 : 0.5078746080047549
Loss in iteration 19 : 0.5105706045064141
Loss in iteration 20 : 0.5124560958456684
Loss in iteration 21 : 0.4922291683281001
Loss in iteration 22 : 0.5164422444527759
Loss in iteration 23 : 0.49450099791528557
Loss in iteration 24 : 0.5008291112293606
Loss in iteration 25 : 0.4794380278736158
Loss in iteration 26 : 0.48136968941884783
Loss in iteration 27 : 0.47178227259386823
Loss in iteration 28 : 0.4945014609991631
Loss in iteration 29 : 0.46832462537979935
Loss in iteration 30 : 0.4756437341555502
Loss in iteration 31 : 0.4621659092365934
Loss in iteration 32 : 0.4603108090212942
Loss in iteration 33 : 0.48159864187435913
Loss in iteration 34 : 0.48084232643621616
Loss in iteration 35 : 0.4725830196942505
Loss in iteration 36 : 0.4776181960149122
Loss in iteration 37 : 0.47276913495278483
Loss in iteration 38 : 0.4659215937711073
Loss in iteration 39 : 0.46022831911308776
Loss in iteration 40 : 0.4528232781800463
Loss in iteration 41 : 0.4555250647611397
Loss in iteration 42 : 0.4675836051090233
Loss in iteration 43 : 0.46362016721833055
Loss in iteration 44 : 0.45231025506973827
Loss in iteration 45 : 0.4756925918769443
Loss in iteration 46 : 0.4590327380344957
Loss in iteration 47 : 0.4665593487742984
Loss in iteration 48 : 0.4627682313983888
Loss in iteration 49 : 0.47395700601711305
Loss in iteration 50 : 0.47454032691242665
Loss in iteration 51 : 0.4617761312624077
Loss in iteration 52 : 0.47440846655784724
Loss in iteration 53 : 0.48238065934707103
Loss in iteration 54 : 0.4772960751477342
Loss in iteration 55 : 0.4688313420010698
Loss in iteration 56 : 0.4675353726408907
Loss in iteration 57 : 0.46596749424056993
Loss in iteration 58 : 0.4755675395667103
Loss in iteration 59 : 0.48925144906088686
Loss in iteration 60 : 0.4794413218049353
Loss in iteration 61 : 0.47281777816804843
Loss in iteration 62 : 0.47291178447489357
Loss in iteration 63 : 0.485343218888627
Loss in iteration 64 : 0.4796437771014958
Loss in iteration 65 : 0.4640104028968478
Loss in iteration 66 : 0.4781229493464291
Loss in iteration 67 : 0.4642125339962133
Loss in iteration 68 : 0.48211863464998045
Loss in iteration 69 : 0.46831824908165204
Loss in iteration 70 : 0.45894249978796975
Loss in iteration 71 : 0.4701282425273694
Loss in iteration 72 : 0.4692888563231097
Loss in iteration 73 : 0.47826734819965044
Loss in iteration 74 : 0.47266869827578956
Loss in iteration 75 : 0.4868173437720099
Loss in iteration 76 : 0.46804760447641264
Loss in iteration 77 : 0.46052687567895056
Loss in iteration 78 : 0.45470144504315335
Loss in iteration 79 : 0.47168603182555174
Loss in iteration 80 : 0.47277190040880906
Loss in iteration 81 : 0.47488029390450603
Loss in iteration 82 : 0.46705588661119307
Loss in iteration 83 : 0.4784435612071939
Loss in iteration 84 : 0.46384771542291364
Loss in iteration 85 : 0.4739577088355491
Loss in iteration 86 : 0.47609222158553954
Loss in iteration 87 : 0.4860303827123312
Loss in iteration 88 : 0.49685865159540576
Loss in iteration 89 : 0.508867417505717
Loss in iteration 90 : 0.5310016631204534
Loss in iteration 91 : 0.5208137240143228
Loss in iteration 92 : 0.529817767427298
Loss in iteration 93 : 0.5617555343846518
Loss in iteration 94 : 0.5820654836790833
Loss in iteration 95 : 0.6075245540836139
Loss in iteration 96 : 0.594205722094233
Loss in iteration 97 : 0.5855316216698031
Loss in iteration 98 : 0.5574248422284518
Loss in iteration 99 : 0.5680277791811843
Loss in iteration 100 : 0.5686185813720543
Loss in iteration 101 : 0.544962217065056
Loss in iteration 102 : 0.5330575375512486
Loss in iteration 103 : 0.5329154200512729
Loss in iteration 104 : 0.524445476544415
Loss in iteration 105 : 0.5102999213910117
Loss in iteration 106 : 0.528216762319617
Loss in iteration 107 : 0.5210923089232906
Loss in iteration 108 : 0.5021604515909412
Loss in iteration 109 : 0.5174510818062688
Loss in iteration 110 : 0.4903604563250923
Loss in iteration 111 : 0.47980757461987183
Loss in iteration 112 : 0.49152611826027715
Loss in iteration 113 : 0.4813423683733036
Loss in iteration 114 : 0.4897034481011336
Loss in iteration 115 : 0.47403837492770395
Loss in iteration 116 : 0.46884552995445217
Loss in iteration 117 : 0.48219690667058773
Loss in iteration 118 : 0.4832336706678904
Loss in iteration 119 : 0.48704033302023125
Loss in iteration 120 : 0.4828456757723073
Loss in iteration 121 : 0.4776095708054454
Loss in iteration 122 : 0.49624556203640063
Loss in iteration 123 : 0.4984112064970159
Loss in iteration 124 : 0.49194991516359116
Loss in iteration 125 : 0.5008413411176637
Loss in iteration 126 : 0.5024235313547656
Loss in iteration 127 : 0.5164755934506831
Loss in iteration 128 : 0.5167530849314188
Loss in iteration 129 : 0.551534715320041
Loss in iteration 130 : 0.5706386038574568
Loss in iteration 131 : 0.5940123586001765
Loss in iteration 132 : 0.5947993244486302
Loss in iteration 133 : 0.6112329553003484
Loss in iteration 134 : 0.6025848291901879
Loss in iteration 135 : 0.6116958260834312
Loss in iteration 136 : 0.5770678718961704
Loss in iteration 137 : 0.5707764268287066
Loss in iteration 138 : 0.5797663540325876
Loss in iteration 139 : 0.5498556704152854
Loss in iteration 140 : 0.5467980228199442
Loss in iteration 141 : 0.5447410064993222
Loss in iteration 142 : 0.5351976173278
Loss in iteration 143 : 0.5035918113852488
Loss in iteration 144 : 0.5090261790947683
Loss in iteration 145 : 0.5122896613344076
Loss in iteration 146 : 0.5074275797885837
Loss in iteration 147 : 0.5146233665857834
Loss in iteration 148 : 0.49838510161240124
Loss in iteration 149 : 0.4945599900217039
Loss in iteration 150 : 0.48503172207778017
Loss in iteration 151 : 0.4811323747122137
Loss in iteration 152 : 0.4953919865314127
Loss in iteration 153 : 0.48907660932902863
Loss in iteration 154 : 0.49725414037116633
Loss in iteration 155 : 0.48963583128231575
Loss in iteration 156 : 0.48778909134951004
Loss in iteration 157 : 0.4747608051075262
Loss in iteration 158 : 0.4757087775599269
Loss in iteration 159 : 0.4963852035174169
Loss in iteration 160 : 0.48724069841001316
Loss in iteration 161 : 0.4953228717846202
Loss in iteration 162 : 0.5149510549642518
Loss in iteration 163 : 0.5683454095832698
Loss in iteration 164 : 0.6068963308667624
Loss in iteration 165 : 0.6178250490859406
Loss in iteration 166 : 0.5864643530066803
Loss in iteration 167 : 0.5875976012419604
Loss in iteration 168 : 0.5284333441527022
Loss in iteration 169 : 0.5407191576355601
Loss in iteration 170 : 0.5306708975542771
Loss in iteration 171 : 0.5443459554717882
Loss in iteration 172 : 0.5214872097277583
Loss in iteration 173 : 0.5125822896974903
Loss in iteration 174 : 0.5117312030029963
Loss in iteration 175 : 0.527289080990208
Loss in iteration 176 : 0.5451968144000443
Loss in iteration 177 : 0.5304160489036494
Loss in iteration 178 : 0.5089082837790411
Loss in iteration 179 : 0.5134819699173075
Loss in iteration 180 : 0.5225225215417854
Loss in iteration 181 : 0.5191582171528316
Loss in iteration 182 : 0.5205718102132242
Loss in iteration 183 : 0.5208946236974206
Loss in iteration 184 : 0.51866251680318
Loss in iteration 185 : 0.5271781570489021
Loss in iteration 186 : 0.5189501920309397
Loss in iteration 187 : 0.5104047531733943
Loss in iteration 188 : 0.5317702275390559
Loss in iteration 189 : 0.5343199535321352
Loss in iteration 190 : 0.5348694922665718
Loss in iteration 191 : 0.516564733792425
Loss in iteration 192 : 0.5146922819124743
Loss in iteration 193 : 0.5077259397355157
Loss in iteration 194 : 0.5202393466670209
Loss in iteration 195 : 0.5354376734972303
Loss in iteration 196 : 0.5422017461792297
Loss in iteration 197 : 0.5325321736288379
Loss in iteration 198 : 0.5290550739676064
Loss in iteration 199 : 0.5396753656211549
Loss in iteration 200 : 0.5379515771031393
Testing accuracy  of updater 8 on alg 0 with rate 1.4000000000000001 = 0.729, training accuracy 0.7365, time elapsed: 4882 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.7649697220355623
Loss in iteration 3 : 0.8015292591993812
Loss in iteration 4 : 0.9183350099987033
Loss in iteration 5 : 0.7119655852079569
Loss in iteration 6 : 0.686560707580821
Loss in iteration 7 : 0.6087355967427658
Loss in iteration 8 : 0.5639108751146246
Loss in iteration 9 : 0.525837015596807
Loss in iteration 10 : 0.5091876690127387
Loss in iteration 11 : 0.4831282017699551
Loss in iteration 12 : 0.4781408590276954
Loss in iteration 13 : 0.48861002684056065
Loss in iteration 14 : 0.47645799663152427
Loss in iteration 15 : 0.48939621981457165
Loss in iteration 16 : 0.47629923772134286
Loss in iteration 17 : 0.491086663888908
Loss in iteration 18 : 0.4874144016707307
Loss in iteration 19 : 0.4892941991630255
Loss in iteration 20 : 0.4891494896865878
Loss in iteration 21 : 0.47568831936214123
Loss in iteration 22 : 0.4947762870209498
Loss in iteration 23 : 0.4784677136418419
Loss in iteration 24 : 0.486201209564954
Loss in iteration 25 : 0.46941727434927233
Loss in iteration 26 : 0.4724899543726227
Loss in iteration 27 : 0.464335455121129
Loss in iteration 28 : 0.48398433196918866
Loss in iteration 29 : 0.46321206454974373
Loss in iteration 30 : 0.4689719948964082
Loss in iteration 31 : 0.45897245559833644
Loss in iteration 32 : 0.4597398398813615
Loss in iteration 33 : 0.4772773280691604
Loss in iteration 34 : 0.47768329717630303
Loss in iteration 35 : 0.4705539942007019
Loss in iteration 36 : 0.4749994340023521
Loss in iteration 37 : 0.47143283188515983
Loss in iteration 38 : 0.465447549632458
Loss in iteration 39 : 0.46041397868645023
Loss in iteration 40 : 0.45299303638491867
Loss in iteration 41 : 0.4569044496101475
Loss in iteration 42 : 0.4655934705520563
Loss in iteration 43 : 0.4631553671476006
Loss in iteration 44 : 0.4509788350680986
Loss in iteration 45 : 0.47187994822713425
Loss in iteration 46 : 0.45471717780651677
Loss in iteration 47 : 0.4630328685409543
Loss in iteration 48 : 0.46067543920976334
Loss in iteration 49 : 0.4724326244731949
Loss in iteration 50 : 0.4707893027067298
Loss in iteration 51 : 0.45679325092893064
Loss in iteration 52 : 0.46631474556335767
Loss in iteration 53 : 0.4628042998317769
Loss in iteration 54 : 0.4536783630291794
Loss in iteration 55 : 0.4560257325772474
Loss in iteration 56 : 0.46016062120404505
Loss in iteration 57 : 0.46193406687903776
Loss in iteration 58 : 0.46833134933083614
Loss in iteration 59 : 0.4778384310423846
Loss in iteration 60 : 0.4676598011239776
Loss in iteration 61 : 0.4634428831985079
Loss in iteration 62 : 0.46038132769281087
Loss in iteration 63 : 0.466981531596978
Loss in iteration 64 : 0.461754842654348
Loss in iteration 65 : 0.4571097204742225
Loss in iteration 66 : 0.47246969915029285
Loss in iteration 67 : 0.4536173785796627
Loss in iteration 68 : 0.47515967886257965
Loss in iteration 69 : 0.46755857167475706
Loss in iteration 70 : 0.45679574619074303
Loss in iteration 71 : 0.4645901959389
Loss in iteration 72 : 0.46612614894470406
Loss in iteration 73 : 0.4744093564688736
Loss in iteration 74 : 0.46796052153399265
Loss in iteration 75 : 0.4760352781302447
Loss in iteration 76 : 0.4584808932220688
Loss in iteration 77 : 0.4543845434987181
Loss in iteration 78 : 0.44893313804069757
Loss in iteration 79 : 0.46386602868888926
Loss in iteration 80 : 0.4628050309232836
Loss in iteration 81 : 0.46674436368503747
Loss in iteration 82 : 0.45542528143561645
Loss in iteration 83 : 0.46575717618933243
Loss in iteration 84 : 0.4539973763673249
Loss in iteration 85 : 0.462932531489498
Loss in iteration 86 : 0.460273273941912
Loss in iteration 87 : 0.46281341657658054
Loss in iteration 88 : 0.4657753531719046
Loss in iteration 89 : 0.459591756629686
Loss in iteration 90 : 0.4651483375571306
Loss in iteration 91 : 0.45911827789098675
Loss in iteration 92 : 0.4665856663390433
Loss in iteration 93 : 0.46538712195231524
Loss in iteration 94 : 0.46088905935276026
Loss in iteration 95 : 0.4650019281387519
Loss in iteration 96 : 0.46629864128380116
Loss in iteration 97 : 0.4729231639714142
Loss in iteration 98 : 0.4619189887537817
Loss in iteration 99 : 0.45262502081928474
Loss in iteration 100 : 0.45666537711671434
Loss in iteration 101 : 0.4578549261873357
Loss in iteration 102 : 0.4634824842197575
Loss in iteration 103 : 0.46895671930432414
Loss in iteration 104 : 0.4656277732731598
Loss in iteration 105 : 0.44963790215315763
Loss in iteration 106 : 0.46924819671867607
Loss in iteration 107 : 0.46429347211818567
Loss in iteration 108 : 0.4519333403160114
Loss in iteration 109 : 0.4681133550920204
Loss in iteration 110 : 0.4596334178972948
Loss in iteration 111 : 0.45532028146077486
Loss in iteration 112 : 0.4625622192363809
Loss in iteration 113 : 0.46012036131522943
Loss in iteration 114 : 0.47024708903850054
Loss in iteration 115 : 0.46096558663958354
Loss in iteration 116 : 0.4534676840739541
Loss in iteration 117 : 0.46101143441584824
Loss in iteration 118 : 0.46119695067653416
Loss in iteration 119 : 0.46259124692139053
Loss in iteration 120 : 0.46425809751071945
Loss in iteration 121 : 0.4618606417747661
Loss in iteration 122 : 0.4698786958261031
Loss in iteration 123 : 0.46419642501255404
Loss in iteration 124 : 0.45749062099937055
Loss in iteration 125 : 0.46191445902307454
Loss in iteration 126 : 0.46577090362268975
Loss in iteration 127 : 0.4744395614913456
Loss in iteration 128 : 0.4596504860898865
Loss in iteration 129 : 0.4640733301768992
Loss in iteration 130 : 0.46298404718431474
Loss in iteration 131 : 0.4652882665150697
Loss in iteration 132 : 0.4598244818396144
Loss in iteration 133 : 0.46414324247139543
Loss in iteration 134 : 0.45997199806609496
Loss in iteration 135 : 0.45812322439800846
Loss in iteration 136 : 0.4650109465191787
Loss in iteration 137 : 0.45445534224475487
Loss in iteration 138 : 0.46059267348323135
Loss in iteration 139 : 0.4473338346916893
Loss in iteration 140 : 0.4654891897580224
Loss in iteration 141 : 0.46652854099716135
Loss in iteration 142 : 0.46394003963034003
Loss in iteration 143 : 0.45006200033387594
Loss in iteration 144 : 0.45900159512275096
Loss in iteration 145 : 0.4602669288658408
Loss in iteration 146 : 0.4533373770672839
Loss in iteration 147 : 0.4641885709744381
Loss in iteration 148 : 0.4577128013440211
Loss in iteration 149 : 0.46849559058168855
Loss in iteration 150 : 0.46399332373608143
Loss in iteration 151 : 0.4608710364713098
Loss in iteration 152 : 0.4730314610066556
Loss in iteration 153 : 0.4607801361016686
Loss in iteration 154 : 0.4612476228275364
Loss in iteration 155 : 0.45705296494872943
Loss in iteration 156 : 0.46081111002333985
Loss in iteration 157 : 0.4543017188987964
Loss in iteration 158 : 0.4497247774705457
Loss in iteration 159 : 0.4615904210827944
Loss in iteration 160 : 0.4510266186733237
Loss in iteration 161 : 0.45477296421754393
Loss in iteration 162 : 0.4591159775214385
Loss in iteration 163 : 0.4694678639388161
Loss in iteration 164 : 0.46697611142411005
Loss in iteration 165 : 0.4733176100278023
Loss in iteration 166 : 0.4631178914773473
Loss in iteration 167 : 0.47542049768647215
Loss in iteration 168 : 0.453228089949119
Loss in iteration 169 : 0.462250401995638
Loss in iteration 170 : 0.4547591218300549
Loss in iteration 171 : 0.4628226388061837
Loss in iteration 172 : 0.45698452021082664
Loss in iteration 173 : 0.45649554864668335
Loss in iteration 174 : 0.45649256296745966
Loss in iteration 175 : 0.46082377741645975
Loss in iteration 176 : 0.4690496444390831
Loss in iteration 177 : 0.4660455056176451
Loss in iteration 178 : 0.4636389363316792
Loss in iteration 179 : 0.46452436892855625
Loss in iteration 180 : 0.465049987665947
Loss in iteration 181 : 0.4655893615648699
Loss in iteration 182 : 0.46798091199178127
Loss in iteration 183 : 0.46765053478662144
Loss in iteration 184 : 0.45832900996900156
Loss in iteration 185 : 0.4546538424606491
Loss in iteration 186 : 0.4535798740820874
Loss in iteration 187 : 0.45120456507591233
Loss in iteration 188 : 0.4718972185436631
Loss in iteration 189 : 0.4655730678224848
Loss in iteration 190 : 0.4692385883305077
Loss in iteration 191 : 0.4636415135514822
Loss in iteration 192 : 0.4673803365407371
Loss in iteration 193 : 0.45571563202105275
Loss in iteration 194 : 0.4591336967891547
Loss in iteration 195 : 0.46124875441153274
Loss in iteration 196 : 0.46637189498070625
Loss in iteration 197 : 0.45608470142247637
Loss in iteration 198 : 0.45866703783044044
Loss in iteration 199 : 0.4648709179438757
Loss in iteration 200 : 0.4566672772979696
Testing accuracy  of updater 8 on alg 0 with rate 0.8 = 0.781, training accuracy 0.788125, time elapsed: 4716 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6801045397854995
Loss in iteration 3 : 0.6639982235320826
Loss in iteration 4 : 0.653281450564995
Loss in iteration 5 : 0.6407239425831498
Loss in iteration 6 : 0.6239628075865854
Loss in iteration 7 : 0.6082565996893035
Loss in iteration 8 : 0.595883841871985
Loss in iteration 9 : 0.5830386288375842
Loss in iteration 10 : 0.5743784032267584
Loss in iteration 11 : 0.5591367586817523
Loss in iteration 12 : 0.5476011522144923
Loss in iteration 13 : 0.544468226867819
Loss in iteration 14 : 0.532977221747193
Loss in iteration 15 : 0.5360334636504989
Loss in iteration 16 : 0.5209197258061071
Loss in iteration 17 : 0.5249501569082491
Loss in iteration 18 : 0.5178203243175278
Loss in iteration 19 : 0.5154723013727172
Loss in iteration 20 : 0.5112250717156024
Loss in iteration 21 : 0.502852846497615
Loss in iteration 22 : 0.5092805302146424
Loss in iteration 23 : 0.4998241011171454
Loss in iteration 24 : 0.5039376826744447
Loss in iteration 25 : 0.49046353524463154
Loss in iteration 26 : 0.493061295934383
Loss in iteration 27 : 0.48352938690507985
Loss in iteration 28 : 0.49600099495496713
Loss in iteration 29 : 0.4826769408436395
Loss in iteration 30 : 0.48526227612543815
Loss in iteration 31 : 0.47778342853676276
Loss in iteration 32 : 0.47811853287556
Loss in iteration 33 : 0.4911972488391675
Loss in iteration 34 : 0.48918699872594873
Loss in iteration 35 : 0.4857858976030393
Loss in iteration 36 : 0.48699091686816587
Loss in iteration 37 : 0.4867540652701196
Loss in iteration 38 : 0.48089936316886667
Loss in iteration 39 : 0.4763793976615935
Loss in iteration 40 : 0.4703983057194659
Loss in iteration 41 : 0.4745440038073638
Loss in iteration 42 : 0.4765173215226845
Loss in iteration 43 : 0.47772950059173547
Loss in iteration 44 : 0.46802411983645753
Loss in iteration 45 : 0.4839708469030058
Loss in iteration 46 : 0.46934693440685327
Loss in iteration 47 : 0.47598730452541993
Loss in iteration 48 : 0.4747603726313157
Loss in iteration 49 : 0.4834466030204372
Loss in iteration 50 : 0.48147661531422214
Loss in iteration 51 : 0.466962311277456
Loss in iteration 52 : 0.48064681508627205
Loss in iteration 53 : 0.4752248663582253
Loss in iteration 54 : 0.4682013979309832
Loss in iteration 55 : 0.4689736959532551
Loss in iteration 56 : 0.4721265492612784
Loss in iteration 57 : 0.4706963881714497
Loss in iteration 58 : 0.4782988638469556
Loss in iteration 59 : 0.4837770010925274
Loss in iteration 60 : 0.47573540833623745
Loss in iteration 61 : 0.472455368537125
Loss in iteration 62 : 0.47052275642511293
Loss in iteration 63 : 0.4752889644675398
Loss in iteration 64 : 0.4706041410628676
Loss in iteration 65 : 0.46496185837812287
Loss in iteration 66 : 0.4774278719886007
Loss in iteration 67 : 0.46107879441861693
Loss in iteration 68 : 0.4818232835773516
Loss in iteration 69 : 0.4741758416609295
Loss in iteration 70 : 0.4622954467707642
Loss in iteration 71 : 0.4739079436427254
Loss in iteration 72 : 0.47308736811521573
Loss in iteration 73 : 0.4823605092295234
Loss in iteration 74 : 0.47552702451002865
Loss in iteration 75 : 0.48005719906934885
Loss in iteration 76 : 0.4655289296631766
Loss in iteration 77 : 0.46166727072561264
Loss in iteration 78 : 0.457990071892732
Loss in iteration 79 : 0.4705609318718674
Loss in iteration 80 : 0.468887322306364
Loss in iteration 81 : 0.4735379604660883
Loss in iteration 82 : 0.46269100070345015
Loss in iteration 83 : 0.47268346456724
Loss in iteration 84 : 0.4604090018790024
Loss in iteration 85 : 0.47008990416405305
Loss in iteration 86 : 0.4674578414192343
Loss in iteration 87 : 0.4651305664888636
Loss in iteration 88 : 0.46874829299239545
Loss in iteration 89 : 0.46437557380861305
Loss in iteration 90 : 0.47003184138250653
Loss in iteration 91 : 0.4660471606055201
Loss in iteration 92 : 0.46957567146166246
Loss in iteration 93 : 0.47101086354387606
Loss in iteration 94 : 0.46547896050766646
Loss in iteration 95 : 0.4702909453167492
Loss in iteration 96 : 0.4708376413433971
Loss in iteration 97 : 0.4751196129962403
Loss in iteration 98 : 0.4674514050322376
Loss in iteration 99 : 0.45880005213028174
Loss in iteration 100 : 0.4601668271786356
Loss in iteration 101 : 0.4635476527516762
Loss in iteration 102 : 0.4677011874417038
Loss in iteration 103 : 0.47339953629039355
Loss in iteration 104 : 0.46958047621719323
Loss in iteration 105 : 0.45532658790684194
Loss in iteration 106 : 0.4737590182930054
Loss in iteration 107 : 0.46938680769456986
Loss in iteration 108 : 0.45722146191938706
Loss in iteration 109 : 0.4730466925688296
Loss in iteration 110 : 0.4640359430982507
Loss in iteration 111 : 0.45836321676076724
Loss in iteration 112 : 0.4660602726943007
Loss in iteration 113 : 0.4630804475663703
Loss in iteration 114 : 0.4719736173394484
Loss in iteration 115 : 0.46535255401709513
Loss in iteration 116 : 0.4595231950615615
Loss in iteration 117 : 0.46399888914950893
Loss in iteration 118 : 0.4655880845097086
Loss in iteration 119 : 0.46699486464268686
Loss in iteration 120 : 0.4696357803932942
Loss in iteration 121 : 0.46371205747720373
Loss in iteration 122 : 0.4725705524981931
Loss in iteration 123 : 0.46798894248893813
Loss in iteration 124 : 0.45984220544280874
Loss in iteration 125 : 0.463641071672003
Loss in iteration 126 : 0.46876170738256695
Loss in iteration 127 : 0.4772509407833293
Loss in iteration 128 : 0.46183054951537494
Loss in iteration 129 : 0.4668657560141728
Loss in iteration 130 : 0.46594911009764167
Loss in iteration 131 : 0.46818012275899507
Loss in iteration 132 : 0.46255626938086636
Loss in iteration 133 : 0.4659190293144262
Loss in iteration 134 : 0.4621748537589779
Loss in iteration 135 : 0.46117045547447394
Loss in iteration 136 : 0.46807691233282
Loss in iteration 137 : 0.45878542325709243
Loss in iteration 138 : 0.4620852875315009
Loss in iteration 139 : 0.4520346880319066
Loss in iteration 140 : 0.46802767345378854
Loss in iteration 141 : 0.46924968702845676
Loss in iteration 142 : 0.46705776745453714
Loss in iteration 143 : 0.45388688074608774
Loss in iteration 144 : 0.4626265323870385
Loss in iteration 145 : 0.4636553491826737
Loss in iteration 146 : 0.45583068032790874
Loss in iteration 147 : 0.4658870381680892
Loss in iteration 148 : 0.46109533461682306
Loss in iteration 149 : 0.46890998066745115
Loss in iteration 150 : 0.46488622959466963
Loss in iteration 151 : 0.46339488999197287
Loss in iteration 152 : 0.47400979817760386
Loss in iteration 153 : 0.4636991429197162
Loss in iteration 154 : 0.4646460558129725
Loss in iteration 155 : 0.4591146960015927
Loss in iteration 156 : 0.4617184859443826
Loss in iteration 157 : 0.4559655697875967
Loss in iteration 158 : 0.4524192734066215
Loss in iteration 159 : 0.46365735663676744
Loss in iteration 160 : 0.45478560881312496
Loss in iteration 161 : 0.45771714241991973
Loss in iteration 162 : 0.4614760335060855
Loss in iteration 163 : 0.46908401862189364
Loss in iteration 164 : 0.4668996069104727
Loss in iteration 165 : 0.4725433416215812
Loss in iteration 166 : 0.4658578905221775
Loss in iteration 167 : 0.47643733373608155
Loss in iteration 168 : 0.45467208580664376
Loss in iteration 169 : 0.46463119935805697
Loss in iteration 170 : 0.45752018749632256
Loss in iteration 171 : 0.46542159670597494
Loss in iteration 172 : 0.4596456119893519
Loss in iteration 173 : 0.458927606890209
Loss in iteration 174 : 0.45858524988727106
Loss in iteration 175 : 0.46232753264295257
Loss in iteration 176 : 0.47029903258352757
Loss in iteration 177 : 0.46764404078942234
Loss in iteration 178 : 0.46282266516702153
Loss in iteration 179 : 0.46430357213227097
Loss in iteration 180 : 0.46686132785365825
Loss in iteration 181 : 0.46836950451637527
Loss in iteration 182 : 0.46879468180894507
Loss in iteration 183 : 0.4682286949089516
Loss in iteration 184 : 0.46017985459035154
Loss in iteration 185 : 0.45685276813571757
Loss in iteration 186 : 0.4563412968959326
Loss in iteration 187 : 0.4513830063580817
Loss in iteration 188 : 0.47329182934469755
Loss in iteration 189 : 0.4673353321473005
Loss in iteration 190 : 0.4693224928003523
Loss in iteration 191 : 0.46542633382472515
Loss in iteration 192 : 0.4660740545505633
Loss in iteration 193 : 0.4567045413621617
Loss in iteration 194 : 0.4607075101966337
Loss in iteration 195 : 0.4636818365826936
Loss in iteration 196 : 0.46953944993366586
Loss in iteration 197 : 0.4577967984114716
Loss in iteration 198 : 0.45944927031725785
Loss in iteration 199 : 0.4663055136151281
Loss in iteration 200 : 0.459149442604313
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.785, training accuracy 0.79, time elapsed: 4602 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6812551548184673
Loss in iteration 3 : 0.6668132293513445
Loss in iteration 4 : 0.6577895712409733
Loss in iteration 5 : 0.6463709273224654
Loss in iteration 6 : 0.6309217071616362
Loss in iteration 7 : 0.6167190558495168
Loss in iteration 8 : 0.6053387653290585
Loss in iteration 9 : 0.5932728720568518
Loss in iteration 10 : 0.5846172572491823
Loss in iteration 11 : 0.5704503387111203
Loss in iteration 12 : 0.558936164726348
Loss in iteration 13 : 0.5548025908555715
Loss in iteration 14 : 0.5437450635024421
Loss in iteration 15 : 0.5454995178848472
Loss in iteration 16 : 0.5309056558000917
Loss in iteration 17 : 0.5336842025398942
Loss in iteration 18 : 0.5262387590909898
Loss in iteration 19 : 0.5234681326708658
Loss in iteration 20 : 0.5189120219930976
Loss in iteration 21 : 0.5108833518234751
Loss in iteration 22 : 0.5154681131487371
Loss in iteration 23 : 0.5067658195216957
Loss in iteration 24 : 0.5101372620970406
Loss in iteration 25 : 0.49692123079205947
Loss in iteration 26 : 0.4990548796190789
Loss in iteration 27 : 0.4894862147106323
Loss in iteration 28 : 0.5008555655191366
Loss in iteration 29 : 0.488069396602764
Loss in iteration 30 : 0.4903532754156656
Loss in iteration 31 : 0.4828850341685182
Loss in iteration 32 : 0.4831696903186951
Loss in iteration 33 : 0.49512851302311683
Loss in iteration 34 : 0.49285910599756577
Loss in iteration 35 : 0.48964451519136065
Loss in iteration 36 : 0.49036663988104107
Loss in iteration 37 : 0.490417807507594
Loss in iteration 38 : 0.48474967796741586
Loss in iteration 39 : 0.48012399362530805
Loss in iteration 40 : 0.4743964073923771
Loss in iteration 41 : 0.47834786877225205
Loss in iteration 42 : 0.4795490210757742
Loss in iteration 43 : 0.48097912442944446
Loss in iteration 44 : 0.47194778107372
Loss in iteration 45 : 0.48682493340823885
Loss in iteration 46 : 0.47262162418847914
Loss in iteration 47 : 0.47916166028319246
Loss in iteration 48 : 0.4779006302557485
Loss in iteration 49 : 0.4860276739243725
Loss in iteration 50 : 0.4841040636322356
Loss in iteration 51 : 0.4699397830367772
Loss in iteration 52 : 0.4833054711485488
Loss in iteration 53 : 0.4782148710281541
Loss in iteration 54 : 0.4714504474072917
Loss in iteration 55 : 0.47225462387836886
Loss in iteration 56 : 0.4750706610276943
Loss in iteration 57 : 0.4730658679514706
Loss in iteration 58 : 0.48078198450086224
Loss in iteration 59 : 0.48561247092309157
Loss in iteration 60 : 0.4782721564798488
Loss in iteration 61 : 0.47487197357521943
Loss in iteration 62 : 0.4731647482444259
Loss in iteration 63 : 0.4778213962399751
Loss in iteration 64 : 0.4728988746385777
Loss in iteration 65 : 0.4675686935796271
Loss in iteration 66 : 0.4796243477963864
Loss in iteration 67 : 0.4638353649662341
Loss in iteration 68 : 0.4836410620001489
Loss in iteration 69 : 0.47624183237924206
Loss in iteration 70 : 0.46476182438129077
Loss in iteration 71 : 0.4763008652704533
Loss in iteration 72 : 0.4753315935885669
Loss in iteration 73 : 0.4843398560338484
Loss in iteration 74 : 0.47780473230769627
Loss in iteration 75 : 0.48172063733525883
Loss in iteration 76 : 0.4678539700407793
Loss in iteration 77 : 0.46401959735398973
Loss in iteration 78 : 0.46064568436989295
Loss in iteration 79 : 0.47253738325982225
Loss in iteration 80 : 0.4710169815927742
Loss in iteration 81 : 0.47561606069421847
Loss in iteration 82 : 0.4649374316499552
Loss in iteration 83 : 0.47471652006005965
Loss in iteration 84 : 0.4625065992790676
Loss in iteration 85 : 0.4722045334006576
Loss in iteration 86 : 0.46940217133632745
Loss in iteration 87 : 0.4666619535219285
Loss in iteration 88 : 0.4704507737306323
Loss in iteration 89 : 0.46625997172748
Loss in iteration 90 : 0.4719304614337246
Loss in iteration 91 : 0.4681363989017073
Loss in iteration 92 : 0.47098945033845646
Loss in iteration 93 : 0.4728212753315291
Loss in iteration 94 : 0.46738392698179254
Loss in iteration 95 : 0.4719182787597126
Loss in iteration 96 : 0.47255150143517993
Loss in iteration 97 : 0.4766538669222695
Loss in iteration 98 : 0.4692618900574958
Loss in iteration 99 : 0.46075453715618436
Loss in iteration 100 : 0.4619996135605364
Loss in iteration 101 : 0.4655060093971618
Loss in iteration 102 : 0.469201858101139
Loss in iteration 103 : 0.4750382466757096
Loss in iteration 104 : 0.47130097636338547
Loss in iteration 105 : 0.4575348571887916
Loss in iteration 106 : 0.47527224974851456
Loss in iteration 107 : 0.4710067296920053
Loss in iteration 108 : 0.4592208754016977
Loss in iteration 109 : 0.474717279718625
Loss in iteration 110 : 0.4656731440011991
Loss in iteration 111 : 0.4598463183316816
Loss in iteration 112 : 0.46757284013264555
Loss in iteration 113 : 0.46459988192465
Loss in iteration 114 : 0.473172226959141
Loss in iteration 115 : 0.4669401678596477
Loss in iteration 116 : 0.4614589393507352
Loss in iteration 117 : 0.4653918303270115
Loss in iteration 118 : 0.46694476839724025
Loss in iteration 119 : 0.4685192969396795
Loss in iteration 120 : 0.47140135662189003
Loss in iteration 121 : 0.46480862782408155
Loss in iteration 122 : 0.4732451019266523
Loss in iteration 123 : 0.46934015644979793
Loss in iteration 124 : 0.46109605522818997
Loss in iteration 125 : 0.46498390849687765
Loss in iteration 126 : 0.4699784984824416
Loss in iteration 127 : 0.4782752236325041
Loss in iteration 128 : 0.4629011527422504
Loss in iteration 129 : 0.46822385114033144
Loss in iteration 130 : 0.4669540800795799
Loss in iteration 131 : 0.46934927640198515
Loss in iteration 132 : 0.46384416952571456
Loss in iteration 133 : 0.46713119631149935
Loss in iteration 134 : 0.4634565597921279
Loss in iteration 135 : 0.46232130229006435
Loss in iteration 136 : 0.46918622671994425
Loss in iteration 137 : 0.4599953459061014
Loss in iteration 138 : 0.46315049589869506
Loss in iteration 139 : 0.4536028540849892
Loss in iteration 140 : 0.46893585544891975
Loss in iteration 141 : 0.4703274193356674
Loss in iteration 142 : 0.46821565435705875
Loss in iteration 143 : 0.4552110111460488
Loss in iteration 144 : 0.4639366858336832
Loss in iteration 145 : 0.46463551303013123
Loss in iteration 146 : 0.4570583357341143
Loss in iteration 147 : 0.4668564552839751
Loss in iteration 148 : 0.4624012613901746
Loss in iteration 149 : 0.4697213041855915
Loss in iteration 150 : 0.4658471704548208
Loss in iteration 151 : 0.4645696753309024
Loss in iteration 152 : 0.47479433265033527
Loss in iteration 153 : 0.4648072279774681
Loss in iteration 154 : 0.46583733851578574
Loss in iteration 155 : 0.4599513991576377
Loss in iteration 156 : 0.46256717536600644
Loss in iteration 157 : 0.45694618406324544
Loss in iteration 158 : 0.4534026733685143
Loss in iteration 159 : 0.46451295369773665
Loss in iteration 160 : 0.45598909478806
Loss in iteration 161 : 0.4588011060144377
Loss in iteration 162 : 0.4625131846755434
Loss in iteration 163 : 0.46996425736560266
Loss in iteration 164 : 0.46802174111178074
Loss in iteration 165 : 0.4730032005080294
Loss in iteration 166 : 0.4666475574397406
Loss in iteration 167 : 0.4770145418022182
Loss in iteration 168 : 0.45586329147492466
Loss in iteration 169 : 0.4654995135346433
Loss in iteration 170 : 0.45845327847555867
Loss in iteration 171 : 0.46648481421976323
Loss in iteration 172 : 0.4605486310793691
Loss in iteration 173 : 0.4597843285980592
Loss in iteration 174 : 0.45935891136235063
Loss in iteration 175 : 0.46308697472455895
Loss in iteration 176 : 0.4708405368757088
Loss in iteration 177 : 0.4682548267429641
Loss in iteration 178 : 0.463289262688746
Loss in iteration 179 : 0.46490765394652095
Loss in iteration 180 : 0.46759213605708844
Loss in iteration 181 : 0.4691630707898507
Loss in iteration 182 : 0.4694674943467801
Loss in iteration 183 : 0.46873885949245603
Loss in iteration 184 : 0.46115609283972686
Loss in iteration 185 : 0.4577015129829626
Loss in iteration 186 : 0.4571184060282678
Loss in iteration 187 : 0.45216102612983605
Loss in iteration 188 : 0.47369132195080255
Loss in iteration 189 : 0.4682291760264221
Loss in iteration 190 : 0.46974310170230765
Loss in iteration 191 : 0.46634461995254456
Loss in iteration 192 : 0.4668329262307247
Loss in iteration 193 : 0.45753926553058855
Loss in iteration 194 : 0.4614004601139437
Loss in iteration 195 : 0.46453137707103426
Loss in iteration 196 : 0.4703479108724486
Loss in iteration 197 : 0.4583788687736729
Loss in iteration 198 : 0.4602569203312822
Loss in iteration 199 : 0.4667482953730818
Loss in iteration 200 : 0.4597952401257847
Testing accuracy  of updater 8 on alg 0 with rate 0.14 = 0.7835, training accuracy 0.789375, time elapsed: 4293 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.6839514002335411
Loss in iteration 3 : 0.6727822365281534
Loss in iteration 4 : 0.6665420054246066
Loss in iteration 5 : 0.6585619960153438
Loss in iteration 6 : 0.6460730010556955
Loss in iteration 7 : 0.6348414255964233
Loss in iteration 8 : 0.6258464639559138
Loss in iteration 9 : 0.6161430943543861
Loss in iteration 10 : 0.6084032727345142
Loss in iteration 11 : 0.597001003515685
Loss in iteration 12 : 0.5865793635936981
Loss in iteration 13 : 0.5809923678975741
Loss in iteration 14 : 0.5715217537300722
Loss in iteration 15 : 0.5704309923351594
Loss in iteration 16 : 0.5573064704535641
Loss in iteration 17 : 0.5578964621866275
Loss in iteration 18 : 0.5502162016279326
Loss in iteration 19 : 0.5465206023592384
Loss in iteration 20 : 0.5416653963634697
Loss in iteration 21 : 0.5343888793141718
Loss in iteration 22 : 0.5352611167108883
Loss in iteration 23 : 0.5279919420225858
Loss in iteration 24 : 0.5296839582972038
Loss in iteration 25 : 0.5171829651472651
Loss in iteration 26 : 0.5181517655656048
Loss in iteration 27 : 0.5085819655371882
Loss in iteration 28 : 0.517328820961209
Loss in iteration 29 : 0.5057851217104736
Loss in iteration 30 : 0.5070913879059757
Loss in iteration 31 : 0.4994685646442806
Loss in iteration 32 : 0.49953400908185996
Loss in iteration 33 : 0.5087629328545304
Loss in iteration 34 : 0.5060364373076573
Loss in iteration 35 : 0.502739797290019
Loss in iteration 36 : 0.5025116767306105
Loss in iteration 37 : 0.5029492280029453
Loss in iteration 38 : 0.4977015359966087
Loss in iteration 39 : 0.49288862352425583
Loss in iteration 40 : 0.4876038232373073
Loss in iteration 41 : 0.4909120196651705
Loss in iteration 42 : 0.4902548964626419
Loss in iteration 43 : 0.4919342830793802
Loss in iteration 44 : 0.48441407693208643
Loss in iteration 45 : 0.49654762936712954
Loss in iteration 46 : 0.4834204826193413
Loss in iteration 47 : 0.48960203860669876
Loss in iteration 48 : 0.488221175650839
Loss in iteration 49 : 0.4946929225344572
Loss in iteration 50 : 0.49275272077246324
Loss in iteration 51 : 0.47985259489700904
Loss in iteration 52 : 0.49211128186272207
Loss in iteration 53 : 0.48749920041726547
Loss in iteration 54 : 0.48145479373699196
Loss in iteration 55 : 0.4821697895413547
Loss in iteration 56 : 0.48416725466827243
Loss in iteration 57 : 0.4810728968198902
Loss in iteration 58 : 0.48867460458425216
Loss in iteration 59 : 0.4920598413021192
Loss in iteration 60 : 0.4863779062130562
Loss in iteration 61 : 0.4825223240525432
Loss in iteration 62 : 0.4813736235599802
Loss in iteration 63 : 0.4855758527746041
Loss in iteration 64 : 0.4802567112242321
Loss in iteration 65 : 0.4758684879855748
Loss in iteration 66 : 0.48678657315173396
Loss in iteration 67 : 0.4724147424934746
Loss in iteration 68 : 0.48949623294299144
Loss in iteration 69 : 0.4828862670348673
Loss in iteration 70 : 0.47259851265637204
Loss in iteration 71 : 0.4835428959869707
Loss in iteration 72 : 0.4822155755781213
Loss in iteration 73 : 0.4903586731699222
Loss in iteration 74 : 0.48461064819155264
Loss in iteration 75 : 0.48703884956445287
Loss in iteration 76 : 0.47502511976689216
Loss in iteration 77 : 0.47152556004191926
Loss in iteration 78 : 0.4685990595664633
Loss in iteration 79 : 0.4786604888005796
Loss in iteration 80 : 0.4774582701432557
Loss in iteration 81 : 0.4820788969478671
Loss in iteration 82 : 0.47200303683586137
Loss in iteration 83 : 0.4810405670694379
Loss in iteration 84 : 0.4693322987088885
Loss in iteration 85 : 0.478820539320773
Loss in iteration 86 : 0.475514814173703
Loss in iteration 87 : 0.4720292039383132
Loss in iteration 88 : 0.4761239936251666
Loss in iteration 89 : 0.4722363390527389
Loss in iteration 90 : 0.4777549526837124
Loss in iteration 91 : 0.47468226666996616
Loss in iteration 92 : 0.47581774621097556
Loss in iteration 93 : 0.4786129987160842
Loss in iteration 94 : 0.4734164774868534
Loss in iteration 95 : 0.47725385749369253
Loss in iteration 96 : 0.47809297408426943
Loss in iteration 97 : 0.4816693421068048
Loss in iteration 98 : 0.47510116033151745
Loss in iteration 99 : 0.46717043785169565
Loss in iteration 100 : 0.4678343508200131
Loss in iteration 101 : 0.47152370072255445
Loss in iteration 102 : 0.4742262428971249
Loss in iteration 103 : 0.4801558131551892
Loss in iteration 104 : 0.47675949181932914
Loss in iteration 105 : 0.4642614167586662
Loss in iteration 106 : 0.48032363546944307
Loss in iteration 107 : 0.4761939624265095
Loss in iteration 108 : 0.4654321161861045
Loss in iteration 109 : 0.48005880701412723
Loss in iteration 110 : 0.47100541217328984
Loss in iteration 111 : 0.4650056464322256
Loss in iteration 112 : 0.47275302707045563
Loss in iteration 113 : 0.46976133429403333
Loss in iteration 114 : 0.47737385799135956
Loss in iteration 115 : 0.4720992277979616
Loss in iteration 116 : 0.46748414653164116
Loss in iteration 117 : 0.4703799362416128
Loss in iteration 118 : 0.47181233905187236
Loss in iteration 119 : 0.47337709356386676
Loss in iteration 120 : 0.47663957423618314
Loss in iteration 121 : 0.4690172334654493
Loss in iteration 122 : 0.4764801901311127
Loss in iteration 123 : 0.4737431866565678
Loss in iteration 124 : 0.4657247965267457
Loss in iteration 125 : 0.4694702636008716
Loss in iteration 126 : 0.4743044283057763
Loss in iteration 127 : 0.4820640045572269
Loss in iteration 128 : 0.4669950490674414
Loss in iteration 129 : 0.4727926535606193
Loss in iteration 130 : 0.47058550836977364
Loss in iteration 131 : 0.47352394369758927
Loss in iteration 132 : 0.4682566079756068
Loss in iteration 133 : 0.4713059901449093
Loss in iteration 134 : 0.4678642123278034
Loss in iteration 135 : 0.4663298155351258
Loss in iteration 136 : 0.4732711172925608
Loss in iteration 137 : 0.46414917586774357
Loss in iteration 138 : 0.4671313863286268
Loss in iteration 139 : 0.4587166975375397
Loss in iteration 140 : 0.4722288918084733
Loss in iteration 141 : 0.4741473012888704
Loss in iteration 142 : 0.47211223670072283
Loss in iteration 143 : 0.4598290803445486
Loss in iteration 144 : 0.468267606737045
Loss in iteration 145 : 0.4682591934602401
Loss in iteration 146 : 0.4616012918607102
Loss in iteration 147 : 0.4703442220149921
Loss in iteration 148 : 0.46669441595832545
Loss in iteration 149 : 0.47290852321956844
Loss in iteration 150 : 0.46942170644643866
Loss in iteration 151 : 0.46863697737652993
Loss in iteration 152 : 0.4779128363031577
Loss in iteration 153 : 0.46863923616634157
Loss in iteration 154 : 0.4699600311057421
Loss in iteration 155 : 0.4629376255065855
Loss in iteration 156 : 0.46584244984184964
Loss in iteration 157 : 0.46072286119302663
Loss in iteration 158 : 0.4571067923725523
Loss in iteration 159 : 0.4677300886390858
Loss in iteration 160 : 0.460077610982902
Loss in iteration 161 : 0.4625023151739562
Loss in iteration 162 : 0.46622347859069196
Loss in iteration 163 : 0.4731340040757595
Loss in iteration 164 : 0.4717402316036768
Loss in iteration 165 : 0.4752540717328727
Loss in iteration 166 : 0.46942290903348494
Loss in iteration 167 : 0.47959531114778436
Loss in iteration 168 : 0.4598867132991264
Loss in iteration 169 : 0.4685029572799068
Loss in iteration 170 : 0.461901782128329
Loss in iteration 171 : 0.4700877228490564
Loss in iteration 172 : 0.46388858242274084
Loss in iteration 173 : 0.46293316643033733
Loss in iteration 174 : 0.4623008012887281
Loss in iteration 175 : 0.465935709412733
Loss in iteration 176 : 0.47325662971740995
Loss in iteration 177 : 0.4707276423012011
Loss in iteration 178 : 0.4656065102966736
Loss in iteration 179 : 0.46740292164463626
Loss in iteration 180 : 0.4703079446235061
Loss in iteration 181 : 0.4719568473519079
Loss in iteration 182 : 0.471965692005998
Loss in iteration 183 : 0.47105556681600574
Loss in iteration 184 : 0.4646118592831906
Loss in iteration 185 : 0.4607882746896054
Loss in iteration 186 : 0.46033130477656486
Loss in iteration 187 : 0.4550618643342252
Loss in iteration 188 : 0.4755489576663965
Loss in iteration 189 : 0.4714416525385061
Loss in iteration 190 : 0.4716038160384696
Loss in iteration 191 : 0.4695126466622711
Loss in iteration 192 : 0.4696380064713382
Loss in iteration 193 : 0.4603637564064152
Loss in iteration 194 : 0.4639415709946005
Loss in iteration 195 : 0.4675226977098549
Loss in iteration 196 : 0.47317830874908734
Loss in iteration 197 : 0.46073352410861623
Loss in iteration 198 : 0.46294174263801846
Loss in iteration 199 : 0.4688172870012174
Loss in iteration 200 : 0.46230887370809376
Testing accuracy  of updater 8 on alg 0 with rate 0.08000000000000002 = 0.7845, training accuracy 0.78925, time elapsed: 4236 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6899742063348278
Loss in iteration 3 : 0.6852591808864946
Loss in iteration 4 : 0.6818356809940128
Loss in iteration 5 : 0.6786680379723053
Loss in iteration 6 : 0.6734785293203696
Loss in iteration 7 : 0.6691886414140419
Loss in iteration 8 : 0.6651660828219336
Loss in iteration 9 : 0.6598138401396524
Loss in iteration 10 : 0.6567722940298651
Loss in iteration 11 : 0.6538575703558096
Loss in iteration 12 : 0.6467380891604828
Loss in iteration 13 : 0.6442697693872471
Loss in iteration 14 : 0.6406810438552145
Loss in iteration 15 : 0.6377448057956154
Loss in iteration 16 : 0.6298919999116452
Loss in iteration 17 : 0.6291339292777418
Loss in iteration 18 : 0.6247906877479747
Loss in iteration 19 : 0.6199316680282737
Loss in iteration 20 : 0.6180193300952362
Loss in iteration 21 : 0.6135261018787274
Loss in iteration 22 : 0.6109827091448207
Loss in iteration 23 : 0.6053732250683819
Loss in iteration 24 : 0.6037280206456282
Loss in iteration 25 : 0.5961323814793896
Loss in iteration 26 : 0.5951560417649026
Loss in iteration 27 : 0.5879378160044653
Loss in iteration 28 : 0.5910802910005225
Loss in iteration 29 : 0.583715885912943
Loss in iteration 30 : 0.5825706945155756
Loss in iteration 31 : 0.5761112490281001
Loss in iteration 32 : 0.5762581420507957
Loss in iteration 33 : 0.5777804303127406
Loss in iteration 34 : 0.5756222123019333
Loss in iteration 35 : 0.5713219670076866
Loss in iteration 36 : 0.5695959569110749
Loss in iteration 37 : 0.5699244225052679
Loss in iteration 38 : 0.5657099464339405
Loss in iteration 39 : 0.5622643081778301
Loss in iteration 40 : 0.5579299926162306
Loss in iteration 41 : 0.5580198553838188
Loss in iteration 42 : 0.553716977126161
Loss in iteration 43 : 0.55531430899056
Loss in iteration 44 : 0.5499518222157865
Loss in iteration 45 : 0.5561378691092144
Loss in iteration 46 : 0.5462611208034929
Loss in iteration 47 : 0.550168278937309
Loss in iteration 48 : 0.547873411131567
Loss in iteration 49 : 0.550222025494156
Loss in iteration 50 : 0.5477076744883664
Loss in iteration 51 : 0.5393662449810005
Loss in iteration 52 : 0.547335331389285
Loss in iteration 53 : 0.5420751907193618
Loss in iteration 54 : 0.5377282344239619
Loss in iteration 55 : 0.5377885440535582
Loss in iteration 56 : 0.5375276794458151
Loss in iteration 57 : 0.5342470411451918
Loss in iteration 58 : 0.5390156912222972
Loss in iteration 59 : 0.5392207550133153
Loss in iteration 60 : 0.5354823827242411
Loss in iteration 61 : 0.531059895889535
Loss in iteration 62 : 0.531062055247321
Loss in iteration 63 : 0.5332534395916624
Loss in iteration 64 : 0.5276435876221751
Loss in iteration 65 : 0.5255719377640763
Loss in iteration 66 : 0.532875547119076
Loss in iteration 67 : 0.5228923315725871
Loss in iteration 68 : 0.5296361067030299
Loss in iteration 69 : 0.5254808976939116
Loss in iteration 70 : 0.5206743807815329
Loss in iteration 71 : 0.5284848208326709
Loss in iteration 72 : 0.525428582847739
Loss in iteration 73 : 0.5289307096169634
Loss in iteration 74 : 0.5253648620546596
Loss in iteration 75 : 0.525006705444129
Loss in iteration 76 : 0.5182376622803019
Loss in iteration 77 : 0.5163708666817651
Loss in iteration 78 : 0.5130546190410779
Loss in iteration 79 : 0.5184203134069977
Loss in iteration 80 : 0.5178817024972228
Loss in iteration 81 : 0.5212633579847675
Loss in iteration 82 : 0.5147166730646605
Loss in iteration 83 : 0.5198243051296128
Loss in iteration 84 : 0.5112942796275716
Loss in iteration 85 : 0.5176816589862461
Loss in iteration 86 : 0.5142666236947906
Loss in iteration 87 : 0.5098609831902353
Loss in iteration 88 : 0.5139521798116432
Loss in iteration 89 : 0.5107213415854592
Loss in iteration 90 : 0.513469254948511
Loss in iteration 91 : 0.5121342086275915
Loss in iteration 92 : 0.5108084089966636
Loss in iteration 93 : 0.513866553375581
Loss in iteration 94 : 0.5094877721561607
Loss in iteration 95 : 0.5118026129161277
Loss in iteration 96 : 0.512871010994442
Loss in iteration 97 : 0.513305304433311
Loss in iteration 98 : 0.5097145668481288
Loss in iteration 99 : 0.5049418491261207
Loss in iteration 100 : 0.5030945637651534
Loss in iteration 101 : 0.5060688892740415
Loss in iteration 102 : 0.5070711905961636
Loss in iteration 103 : 0.5113008607935776
Loss in iteration 104 : 0.5094667261497137
Loss in iteration 105 : 0.5006917060455963
Loss in iteration 106 : 0.5112954338182967
Loss in iteration 107 : 0.5079709747124106
Loss in iteration 108 : 0.5010382659670402
Loss in iteration 109 : 0.5110871929924137
Loss in iteration 110 : 0.5040499158428611
Loss in iteration 111 : 0.4989689572402147
Loss in iteration 112 : 0.5050141107514721
Loss in iteration 113 : 0.501706133147879
Loss in iteration 114 : 0.5055518321739254
Loss in iteration 115 : 0.5041375666474517
Loss in iteration 116 : 0.5013436028476338
Loss in iteration 117 : 0.5018772124701751
Loss in iteration 118 : 0.5029127134550969
Loss in iteration 119 : 0.5030483009971571
Loss in iteration 120 : 0.5057869652361544
Loss in iteration 121 : 0.49761728903685687
Loss in iteration 122 : 0.5036358364279105
Loss in iteration 123 : 0.5026248560256806
Loss in iteration 124 : 0.49608916441892326
Loss in iteration 125 : 0.49778767697941084
Loss in iteration 126 : 0.5029083256468699
Loss in iteration 127 : 0.5073381430566706
Loss in iteration 128 : 0.49527655468350085
Loss in iteration 129 : 0.5015240556341849
Loss in iteration 130 : 0.49728514486058556
Loss in iteration 131 : 0.5002201006948964
Loss in iteration 132 : 0.4961236513139217
Loss in iteration 133 : 0.49848693192177906
Loss in iteration 134 : 0.49644691496363547
Loss in iteration 135 : 0.4930527097733781
Loss in iteration 136 : 0.5007913453428636
Loss in iteration 137 : 0.4920602422223974
Loss in iteration 138 : 0.4942114877720268
Loss in iteration 139 : 0.48978725971716397
Loss in iteration 140 : 0.49721079302008175
Loss in iteration 141 : 0.49995338438664333
Loss in iteration 142 : 0.4970081758141478
Loss in iteration 143 : 0.4892614981941908
Loss in iteration 144 : 0.4943959337560889
Loss in iteration 145 : 0.49391173582709513
Loss in iteration 146 : 0.4903530870976741
Loss in iteration 147 : 0.4958227996421306
Loss in iteration 148 : 0.49408348730321267
Loss in iteration 149 : 0.497262028482759
Loss in iteration 150 : 0.4949648640171385
Loss in iteration 151 : 0.49511709404187104
Loss in iteration 152 : 0.5003956380817438
Loss in iteration 153 : 0.49332408159962765
Loss in iteration 154 : 0.4947690440550687
Loss in iteration 155 : 0.48727803111932105
Loss in iteration 156 : 0.48957347416625946
Loss in iteration 157 : 0.48736856842245546
Loss in iteration 158 : 0.48365465147246806
Loss in iteration 159 : 0.491229474347026
Loss in iteration 160 : 0.4863655893909884
Loss in iteration 161 : 0.4871783706736074
Loss in iteration 162 : 0.4916010408685086
Loss in iteration 163 : 0.49655349011435923
Loss in iteration 164 : 0.495934981652754
Loss in iteration 165 : 0.4953716852826592
Loss in iteration 166 : 0.4911573733482441
Loss in iteration 167 : 0.49950542598245856
Loss in iteration 168 : 0.48595767838461257
Loss in iteration 169 : 0.490986351445548
Loss in iteration 170 : 0.4866605558308147
Loss in iteration 171 : 0.4932689843440128
Loss in iteration 172 : 0.4873440180055997
Loss in iteration 173 : 0.48624154258789265
Loss in iteration 174 : 0.4858957345840159
Loss in iteration 175 : 0.48810102221090246
Loss in iteration 176 : 0.49325289289025503
Loss in iteration 177 : 0.49069048533524157
Loss in iteration 178 : 0.4859890915742596
Loss in iteration 179 : 0.48764046359945246
Loss in iteration 180 : 0.490818670129411
Loss in iteration 181 : 0.4930139018090271
Loss in iteration 182 : 0.4916389597213164
Loss in iteration 183 : 0.4911340891291967
Loss in iteration 184 : 0.48801387234132104
Loss in iteration 185 : 0.4838343514603105
Loss in iteration 186 : 0.4830204496270034
Loss in iteration 187 : 0.4781195546216295
Loss in iteration 188 : 0.49376946095433843
Loss in iteration 189 : 0.4932736131627449
Loss in iteration 190 : 0.4898639368829245
Loss in iteration 191 : 0.49123149514466363
Loss in iteration 192 : 0.489657439164768
Loss in iteration 193 : 0.4816639903729674
Loss in iteration 194 : 0.48416661363455893
Loss in iteration 195 : 0.488896979438244
Loss in iteration 196 : 0.49368823218176267
Loss in iteration 197 : 0.4812805186058902
Loss in iteration 198 : 0.4832839836544389
Loss in iteration 199 : 0.4871773537942154
Loss in iteration 200 : 0.48317386747973123
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.78, training accuracy 0.78025, time elapsed: 4360 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 13.599364892863527
Loss in iteration 3 : 13.744102535395784
Loss in iteration 4 : 7.4274875964612335
Loss in iteration 5 : 9.571185825814888
Loss in iteration 6 : 6.816112003701588
Loss in iteration 7 : 5.007185737484414
Loss in iteration 8 : 9.063837961411576
Loss in iteration 9 : 3.772370851389749
Loss in iteration 10 : 5.4254560400963205
Loss in iteration 11 : 5.941032774600767
Loss in iteration 12 : 2.8891477409156736
Loss in iteration 13 : 4.894221261746288
Loss in iteration 14 : 4.588707413983463
Loss in iteration 15 : 3.2381057271771976
Loss in iteration 16 : 4.140197416501437
Loss in iteration 17 : 3.7841993377594054
Loss in iteration 18 : 3.0167866782808623
Loss in iteration 19 : 4.150946428288583
Loss in iteration 20 : 3.7981419549570483
Loss in iteration 21 : 2.489059977362437
Loss in iteration 22 : 3.40369382534527
Loss in iteration 23 : 3.2629666462671327
Loss in iteration 24 : 2.568043905795325
Loss in iteration 25 : 2.9554508381519495
Loss in iteration 26 : 2.8193285551133207
Loss in iteration 27 : 2.1810579380376462
Loss in iteration 28 : 2.905482135938956
Loss in iteration 29 : 2.319268331906828
Loss in iteration 30 : 2.0517691400448923
Loss in iteration 31 : 2.568462465624825
Loss in iteration 32 : 2.0540651373131387
Loss in iteration 33 : 1.9842769449451951
Loss in iteration 34 : 2.318447082156098
Loss in iteration 35 : 1.736121817775436
Loss in iteration 36 : 2.0782922336347345
Loss in iteration 37 : 1.720438425048087
Loss in iteration 38 : 1.655843511527005
Loss in iteration 39 : 1.7061015939559314
Loss in iteration 40 : 1.3199849592275805
Loss in iteration 41 : 1.536258973378633
Loss in iteration 42 : 1.3042998688138252
Loss in iteration 43 : 1.4784033149912235
Loss in iteration 44 : 1.1077689868779204
Loss in iteration 45 : 1.35906885877183
Loss in iteration 46 : 1.094061754686989
Loss in iteration 47 : 1.1895539296438615
Loss in iteration 48 : 1.0823350523564867
Loss in iteration 49 : 1.066219491982845
Loss in iteration 50 : 1.077382030854272
Loss in iteration 51 : 0.9826741544675061
Loss in iteration 52 : 0.9399921245530899
Loss in iteration 53 : 0.9438004233999467
Loss in iteration 54 : 0.8355214181372492
Loss in iteration 55 : 0.8451453360222135
Loss in iteration 56 : 0.8574533828734878
Loss in iteration 57 : 0.8993342844706267
Loss in iteration 58 : 0.8639330418592521
Loss in iteration 59 : 0.884439602351515
Loss in iteration 60 : 0.7807222090728304
Loss in iteration 61 : 0.77770443699496
Loss in iteration 62 : 0.7703078220886809
Loss in iteration 63 : 0.8729018039949138
Loss in iteration 64 : 0.9814450144153641
Loss in iteration 65 : 1.1896428212060752
Loss in iteration 66 : 1.5289778520046633
Loss in iteration 67 : 0.850456769458523
Loss in iteration 68 : 0.7683372284298321
Loss in iteration 69 : 0.7908899802655686
Loss in iteration 70 : 0.8222123413650323
Loss in iteration 71 : 0.8935586157633164
Loss in iteration 72 : 0.8126154556732146
Loss in iteration 73 : 0.7750390823989888
Loss in iteration 74 : 0.7091408502739813
Loss in iteration 75 : 0.7378869400688034
Loss in iteration 76 : 0.7395196525615697
Loss in iteration 77 : 0.7086992657162259
Loss in iteration 78 : 0.9528917357193712
Loss in iteration 79 : 1.1322627299856602
Loss in iteration 80 : 1.6780729086502892
Loss in iteration 81 : 0.7971855872286397
Loss in iteration 82 : 0.7849886374709288
Loss in iteration 83 : 0.638375665863107
Loss in iteration 84 : 0.6208630819633858
Loss in iteration 85 : 0.6701385721158183
Loss in iteration 86 : 0.6574787054929752
Loss in iteration 87 : 1.0655845971256042
Loss in iteration 88 : 1.4348013844162582
Loss in iteration 89 : 1.7159999849349625
Loss in iteration 90 : 0.7554105393017951
Loss in iteration 91 : 0.6725179330514484
Loss in iteration 92 : 1.0732028221806937
Loss in iteration 93 : 1.0851323601034228
Loss in iteration 94 : 0.6775343261765066
Loss in iteration 95 : 0.736209320974565
Loss in iteration 96 : 1.089530640455734
Loss in iteration 97 : 0.7830992477337583
Loss in iteration 98 : 0.6669246725179916
Loss in iteration 99 : 0.8789883130220695
Loss in iteration 100 : 0.8016478337080012
Loss in iteration 101 : 0.5952012246722722
Loss in iteration 102 : 0.6471511234992608
Loss in iteration 103 : 0.8553123939298087
Loss in iteration 104 : 0.9348304932194575
Loss in iteration 105 : 0.7543811898873193
Loss in iteration 106 : 0.6119161578788164
Loss in iteration 107 : 0.6084932795829566
Loss in iteration 108 : 0.7292086703555679
Loss in iteration 109 : 0.7269744396645935
Loss in iteration 110 : 0.7634949376366196
Loss in iteration 111 : 0.604663658948328
Loss in iteration 112 : 0.5836732319015403
Loss in iteration 113 : 0.5688932574280835
Loss in iteration 114 : 0.5397940766597533
Loss in iteration 115 : 0.5743926899553324
Loss in iteration 116 : 0.48412936133140066
Loss in iteration 117 : 0.5551936826467436
Loss in iteration 118 : 0.4964891399446174
Loss in iteration 119 : 0.6260897660144115
Loss in iteration 120 : 0.8788357077708433
Loss in iteration 121 : 1.5946055817208962
Loss in iteration 122 : 0.9328959780542696
Loss in iteration 123 : 0.6146573717660935
Loss in iteration 124 : 0.5574494319709332
Loss in iteration 125 : 0.8522287232129981
Loss in iteration 126 : 1.0514745533412806
Loss in iteration 127 : 0.7871456977684618
Loss in iteration 128 : 0.5652306592966021
Loss in iteration 129 : 0.8161586957285887
Loss in iteration 130 : 0.9952781018081616
Loss in iteration 131 : 0.7476864050971881
Loss in iteration 132 : 0.5319170401109072
Loss in iteration 133 : 0.7238580926170899
Loss in iteration 134 : 0.7646114237530391
Loss in iteration 135 : 0.5909033281952887
Loss in iteration 136 : 0.5952048033729618
Loss in iteration 137 : 0.6449124587293261
Loss in iteration 138 : 0.685487393531537
Loss in iteration 139 : 0.6102751776140908
Loss in iteration 140 : 0.5425180826187015
Loss in iteration 141 : 0.549855094902456
Loss in iteration 142 : 0.5689504614948352
Loss in iteration 143 : 0.5893485568534773
Loss in iteration 144 : 0.6689089137709578
Loss in iteration 145 : 0.7109583723687621
Loss in iteration 146 : 0.8665465841796005
Loss in iteration 147 : 0.8612584773192974
Loss in iteration 148 : 0.7574525761642245
Loss in iteration 149 : 0.6362241229626535
Loss in iteration 150 : 0.5402643818948808
Loss in iteration 151 : 0.5028823134979804
Loss in iteration 152 : 0.5122158619449418
Loss in iteration 153 : 0.533924593718118
Loss in iteration 154 : 0.6155585493383993
Loss in iteration 155 : 0.7179315227242269
Loss in iteration 156 : 0.7001848785007085
Loss in iteration 157 : 0.6161988281607015
Loss in iteration 158 : 0.5349495338611067
Loss in iteration 159 : 0.5170939183368078
Loss in iteration 160 : 0.48357102709557026
Loss in iteration 161 : 0.48052417790845603
Loss in iteration 162 : 0.4895343841349519
Loss in iteration 163 : 0.49517347043241144
Loss in iteration 164 : 0.4891484758609044
Loss in iteration 165 : 0.49345495381154914
Loss in iteration 166 : 0.4924403179152048
Loss in iteration 167 : 0.49239957621139263
Loss in iteration 168 : 0.4741948123722549
Loss in iteration 169 : 0.478893152226406
Loss in iteration 170 : 0.45860631996235146
Loss in iteration 171 : 0.47578748711036456
Loss in iteration 172 : 0.4600840964156753
Loss in iteration 173 : 0.46362068502495624
Loss in iteration 174 : 0.48124853197930606
Loss in iteration 175 : 0.563425547062445
Loss in iteration 176 : 0.954936770680351
Loss in iteration 177 : 1.5070236789353524
Loss in iteration 178 : 1.2780831840937785
Loss in iteration 179 : 0.5222755054943053
Loss in iteration 180 : 0.8654394917327056
Loss in iteration 181 : 1.3184710051080502
Loss in iteration 182 : 0.54611048908593
Loss in iteration 183 : 0.8868483928531279
Loss in iteration 184 : 0.9741186161414503
Loss in iteration 185 : 0.5203050071941254
Loss in iteration 186 : 0.9154608411785999
Loss in iteration 187 : 0.6301527946428772
Loss in iteration 188 : 0.6618163932477588
Loss in iteration 189 : 0.815716362105665
Loss in iteration 190 : 0.574171852737809
Loss in iteration 191 : 0.6977310980084617
Loss in iteration 192 : 0.6611532229972552
Loss in iteration 193 : 0.5369085874661953
Loss in iteration 194 : 0.6816549929187738
Loss in iteration 195 : 0.5814201868301958
Loss in iteration 196 : 0.5464923573770979
Loss in iteration 197 : 0.6130905442904663
Loss in iteration 198 : 0.5422097403972331
Loss in iteration 199 : 0.5102883702092628
Loss in iteration 200 : 0.595875308933364
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.742, training accuracy 0.737375, time elapsed: 4147 millisecond.
Loss in iteration 1 : 0.6931471805599322
Loss in iteration 2 : 0.7483476870199799
Loss in iteration 3 : 1.333583611608698
Loss in iteration 4 : 1.0785388257228388
Loss in iteration 5 : 0.7521550504581574
Loss in iteration 6 : 0.5357150082209755
Loss in iteration 7 : 0.5438103228280419
Loss in iteration 8 : 0.62092999694262
Loss in iteration 9 : 0.5066569166044317
Loss in iteration 10 : 0.556018775881879
Loss in iteration 11 : 0.5471971809147733
Loss in iteration 12 : 0.5083760266272851
Loss in iteration 13 : 0.5639364895340493
Loss in iteration 14 : 0.5183571213193533
Loss in iteration 15 : 0.5685671822133186
Loss in iteration 16 : 0.5271105830415961
Loss in iteration 17 : 0.5534742770283341
Loss in iteration 18 : 0.5422603764693327
Loss in iteration 19 : 0.5350767343753504
Loss in iteration 20 : 0.540307696435759
Loss in iteration 21 : 0.5068028455712807
Loss in iteration 22 : 0.5331185801453336
Loss in iteration 23 : 0.5023087551328357
Loss in iteration 24 : 0.5145236754598584
Loss in iteration 25 : 0.48370163775704206
Loss in iteration 26 : 0.47988534604371885
Loss in iteration 27 : 0.4860368302299135
Loss in iteration 28 : 0.499422234711347
Loss in iteration 29 : 0.47366409000320175
Loss in iteration 30 : 0.4928557851927595
Loss in iteration 31 : 0.4651245776170737
Loss in iteration 32 : 0.4739717085553813
Loss in iteration 33 : 0.50384515436912
Loss in iteration 34 : 0.4987568104018606
Loss in iteration 35 : 0.47293628466818477
Loss in iteration 36 : 0.49715973106234956
Loss in iteration 37 : 0.4826075424562952
Loss in iteration 38 : 0.4837930802463428
Loss in iteration 39 : 0.4701843122566153
Loss in iteration 40 : 0.46223151252991357
Loss in iteration 41 : 0.4596375902354692
Loss in iteration 42 : 0.47922657032950244
Loss in iteration 43 : 0.46687310914963165
Loss in iteration 44 : 0.454968925502704
Loss in iteration 45 : 0.4716083564151435
Loss in iteration 46 : 0.4568971579404428
Loss in iteration 47 : 0.4663510694095314
Loss in iteration 48 : 0.465375985141111
Loss in iteration 49 : 0.4717071302636555
Loss in iteration 50 : 0.4756923209671719
Loss in iteration 51 : 0.46072251467251574
Loss in iteration 52 : 0.46644425304063436
Loss in iteration 53 : 0.46149732616529265
Loss in iteration 54 : 0.45548621632391034
Loss in iteration 55 : 0.45533645027690434
Loss in iteration 56 : 0.45993735676220193
Loss in iteration 57 : 0.46106519298657855
Loss in iteration 58 : 0.46812907951181193
Loss in iteration 59 : 0.48027361836228577
Loss in iteration 60 : 0.4669473457842983
Loss in iteration 61 : 0.4630853025016895
Loss in iteration 62 : 0.4612739572173385
Loss in iteration 63 : 0.46975336450177707
Loss in iteration 64 : 0.46160413111718657
Loss in iteration 65 : 0.45760505891393705
Loss in iteration 66 : 0.47746692338334057
Loss in iteration 67 : 0.4730610126246613
Loss in iteration 68 : 0.4899593757434732
Loss in iteration 69 : 0.47550420646315156
Loss in iteration 70 : 0.4609047830784786
Loss in iteration 71 : 0.4748307568296774
Loss in iteration 72 : 0.48024945717326656
Loss in iteration 73 : 0.474934536737508
Loss in iteration 74 : 0.4784437372242085
Loss in iteration 75 : 0.4829131209724103
Loss in iteration 76 : 0.4667022511648629
Loss in iteration 77 : 0.4568500554239207
Loss in iteration 78 : 0.45480033226982364
Loss in iteration 79 : 0.4730039840796675
Loss in iteration 80 : 0.47450598153700335
Loss in iteration 81 : 0.4674565232380348
Loss in iteration 82 : 0.45748527752043805
Loss in iteration 83 : 0.47660542615628654
Loss in iteration 84 : 0.46566297098136755
Loss in iteration 85 : 0.4676395114019435
Loss in iteration 86 : 0.4668761262382564
Loss in iteration 87 : 0.4821159091537282
Loss in iteration 88 : 0.4656803616388977
Loss in iteration 89 : 0.46247792417179484
Loss in iteration 90 : 0.4668660864707788
Loss in iteration 91 : 0.4614433309052286
Loss in iteration 92 : 0.46889886985679075
Loss in iteration 93 : 0.4658490634328678
Loss in iteration 94 : 0.47110014769879816
Loss in iteration 95 : 0.47370919685486995
Loss in iteration 96 : 0.47312027985463156
Loss in iteration 97 : 0.4760967696372615
Loss in iteration 98 : 0.47649194389888155
Loss in iteration 99 : 0.4561887848178828
Loss in iteration 100 : 0.462414178640649
Loss in iteration 101 : 0.4739264962793498
Loss in iteration 102 : 0.47561365135284356
Loss in iteration 103 : 0.4718710761996368
Loss in iteration 104 : 0.4686645787762805
Loss in iteration 105 : 0.45477931990655224
Loss in iteration 106 : 0.4694792203249384
Loss in iteration 107 : 0.4670662366439675
Loss in iteration 108 : 0.4575466213029198
Loss in iteration 109 : 0.4756497878939217
Loss in iteration 110 : 0.4574666905148061
Loss in iteration 111 : 0.4642963588798019
Loss in iteration 112 : 0.47923067894105353
Loss in iteration 113 : 0.4736333165143775
Loss in iteration 114 : 0.46892436504965723
Loss in iteration 115 : 0.46630686021607437
Loss in iteration 116 : 0.4594462185003198
Loss in iteration 117 : 0.47223783870549235
Loss in iteration 118 : 0.4605694611229229
Loss in iteration 119 : 0.469533145979174
Loss in iteration 120 : 0.4712467751308572
Loss in iteration 121 : 0.47260960602581914
Loss in iteration 122 : 0.47694023080691866
Loss in iteration 123 : 0.4693044052901644
Loss in iteration 124 : 0.4693915017269024
Loss in iteration 125 : 0.4585783995289888
Loss in iteration 126 : 0.4774694563396151
Loss in iteration 127 : 0.4923887518389337
Loss in iteration 128 : 0.47627993318290773
Loss in iteration 129 : 0.4687644214352705
Loss in iteration 130 : 0.49294156360652086
Loss in iteration 131 : 0.4938051464986044
Loss in iteration 132 : 0.4614509502507768
Loss in iteration 133 : 0.4722528130726228
Loss in iteration 134 : 0.46913069076841846
Loss in iteration 135 : 0.46246887836653733
Loss in iteration 136 : 0.4676455263564267
Loss in iteration 137 : 0.46503902384889334
Loss in iteration 138 : 0.47345119297338084
Loss in iteration 139 : 0.45740514037794144
Loss in iteration 140 : 0.4669255460591751
Loss in iteration 141 : 0.46960511867823534
Loss in iteration 142 : 0.4715067678076928
Loss in iteration 143 : 0.4523756730065634
Loss in iteration 144 : 0.45846235142759006
Loss in iteration 145 : 0.4651646015405994
Loss in iteration 146 : 0.46413830621950214
Loss in iteration 147 : 0.46555636111538595
Loss in iteration 148 : 0.4587682509922451
Loss in iteration 149 : 0.4867191144349183
Loss in iteration 150 : 0.4925863156945307
Loss in iteration 151 : 0.4715085599333246
Loss in iteration 152 : 0.4738900761193398
Loss in iteration 153 : 0.47793656031772136
Loss in iteration 154 : 0.48740101838139976
Loss in iteration 155 : 0.4653267362153447
Loss in iteration 156 : 0.4658041641842061
Loss in iteration 157 : 0.4931188930422593
Loss in iteration 158 : 0.474508045032897
Loss in iteration 159 : 0.4649790491500167
Loss in iteration 160 : 0.46962654509943785
Loss in iteration 161 : 0.48107263263193084
Loss in iteration 162 : 0.46313977384373023
Loss in iteration 163 : 0.4741281131020781
Loss in iteration 164 : 0.4636898003604668
Loss in iteration 165 : 0.47539160272008496
Loss in iteration 166 : 0.46455751145329865
Loss in iteration 167 : 0.4779343212974312
Loss in iteration 168 : 0.4601685843356642
Loss in iteration 169 : 0.46868281074182133
Loss in iteration 170 : 0.46389532792340227
Loss in iteration 171 : 0.4612463298801408
Loss in iteration 172 : 0.46252856365987555
Loss in iteration 173 : 0.45545567286736544
Loss in iteration 174 : 0.4614329312504604
Loss in iteration 175 : 0.46476816331107207
Loss in iteration 176 : 0.47283986105947207
Loss in iteration 177 : 0.46600180995861185
Loss in iteration 178 : 0.46249758150232295
Loss in iteration 179 : 0.46439643744630227
Loss in iteration 180 : 0.4660799697828995
Loss in iteration 181 : 0.46707286650313995
Loss in iteration 182 : 0.4691931007618636
Loss in iteration 183 : 0.4730163690508764
Loss in iteration 184 : 0.46152686660963665
Loss in iteration 185 : 0.457579190414155
Loss in iteration 186 : 0.4569395791833439
Loss in iteration 187 : 0.45304568225058406
Loss in iteration 188 : 0.4724602215839151
Loss in iteration 189 : 0.4677883242310698
Loss in iteration 190 : 0.4727864632301774
Loss in iteration 191 : 0.46856911191532313
Loss in iteration 192 : 0.46861310825883107
Loss in iteration 193 : 0.4569220973000205
Loss in iteration 194 : 0.45891261618383644
Loss in iteration 195 : 0.46682815603580097
Loss in iteration 196 : 0.472580878561487
Loss in iteration 197 : 0.45897405195715835
Loss in iteration 198 : 0.4586555758309312
Loss in iteration 199 : 0.4770192793712267
Loss in iteration 200 : 0.4787293189883321
Testing accuracy  of updater 9 on alg 0 with rate 1.4000000000000001 = 0.7815, training accuracy 0.777875, time elapsed: 4240 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6635753538409672
Loss in iteration 3 : 0.6211656620836696
Loss in iteration 4 : 0.586930323426197
Loss in iteration 5 : 0.5451302329492538
Loss in iteration 6 : 0.5236152340832648
Loss in iteration 7 : 0.5108428527981982
Loss in iteration 8 : 0.4981511773074992
Loss in iteration 9 : 0.48856603383126695
Loss in iteration 10 : 0.4876455246363796
Loss in iteration 11 : 0.4792676688186446
Loss in iteration 12 : 0.4807471162178965
Loss in iteration 13 : 0.4893058574231999
Loss in iteration 14 : 0.4781430179362321
Loss in iteration 15 : 0.4962752582343164
Loss in iteration 16 : 0.48316074871350817
Loss in iteration 17 : 0.4994655783880029
Loss in iteration 18 : 0.4985608778364182
Loss in iteration 19 : 0.49439870170058253
Loss in iteration 20 : 0.4971803219820443
Loss in iteration 21 : 0.48063218075864217
Loss in iteration 22 : 0.5027199412131677
Loss in iteration 23 : 0.47763107403345123
Loss in iteration 24 : 0.4860722767208443
Loss in iteration 25 : 0.46565735995426627
Loss in iteration 26 : 0.4673914046548317
Loss in iteration 27 : 0.4618103511272768
Loss in iteration 28 : 0.4814098663587332
Loss in iteration 29 : 0.4592010974097128
Loss in iteration 30 : 0.46635526897037627
Loss in iteration 31 : 0.4563345466917676
Loss in iteration 32 : 0.45488861741909775
Loss in iteration 33 : 0.47459811543096236
Loss in iteration 34 : 0.47480707656697513
Loss in iteration 35 : 0.4667220994452031
Loss in iteration 36 : 0.4752199145216486
Loss in iteration 37 : 0.47053579898308967
Loss in iteration 38 : 0.46520267713770275
Loss in iteration 39 : 0.46100444415548175
Loss in iteration 40 : 0.4557587841867544
Loss in iteration 41 : 0.4547174890882637
Loss in iteration 42 : 0.4676614736106667
Loss in iteration 43 : 0.46353750231260643
Loss in iteration 44 : 0.4522879929064623
Loss in iteration 45 : 0.47192155900930743
Loss in iteration 46 : 0.4555108827852777
Loss in iteration 47 : 0.46516233849021915
Loss in iteration 48 : 0.4614339351769291
Loss in iteration 49 : 0.47358907050682236
Loss in iteration 50 : 0.47157431537137556
Loss in iteration 51 : 0.4569904742512385
Loss in iteration 52 : 0.46563472170181136
Loss in iteration 53 : 0.45862008716108543
Loss in iteration 54 : 0.45413377935389804
Loss in iteration 55 : 0.45395031040957884
Loss in iteration 56 : 0.4593103629817669
Loss in iteration 57 : 0.462294196900317
Loss in iteration 58 : 0.4666960073969057
Loss in iteration 59 : 0.4792285648176146
Loss in iteration 60 : 0.4702331348079515
Loss in iteration 61 : 0.46400267314392024
Loss in iteration 62 : 0.46141171146216925
Loss in iteration 63 : 0.46900458584113125
Loss in iteration 64 : 0.46218484378452784
Loss in iteration 65 : 0.461391334228875
Loss in iteration 66 : 0.4704899000799107
Loss in iteration 67 : 0.4503274333496751
Loss in iteration 68 : 0.4751825484642803
Loss in iteration 69 : 0.46877865962737264
Loss in iteration 70 : 0.4578429048983659
Loss in iteration 71 : 0.464458326215806
Loss in iteration 72 : 0.4679758678478843
Loss in iteration 73 : 0.4776158004569634
Loss in iteration 74 : 0.466177658247307
Loss in iteration 75 : 0.4745548895741707
Loss in iteration 76 : 0.46008707876280086
Loss in iteration 77 : 0.4519147480511033
Loss in iteration 78 : 0.4492654324719927
Loss in iteration 79 : 0.46550700137136075
Loss in iteration 80 : 0.4633970239379566
Loss in iteration 81 : 0.46602910978939444
Loss in iteration 82 : 0.4573313466299653
Loss in iteration 83 : 0.4646147751397491
Loss in iteration 84 : 0.4567188580192279
Loss in iteration 85 : 0.4641062560937974
Loss in iteration 86 : 0.4596359620588637
Loss in iteration 87 : 0.4617097473501307
Loss in iteration 88 : 0.46569644209426403
Loss in iteration 89 : 0.45872875051546685
Loss in iteration 90 : 0.4664377395454038
Loss in iteration 91 : 0.4588449864885012
Loss in iteration 92 : 0.46801685348361166
Loss in iteration 93 : 0.46527921933935773
Loss in iteration 94 : 0.4656097945814041
Loss in iteration 95 : 0.4656896412701787
Loss in iteration 96 : 0.4692878475485106
Loss in iteration 97 : 0.4726423320561309
Loss in iteration 98 : 0.4650112792735343
Loss in iteration 99 : 0.45157964819795987
Loss in iteration 100 : 0.4565311552236806
Loss in iteration 101 : 0.45738391509066745
Loss in iteration 102 : 0.4631301603424853
Loss in iteration 103 : 0.4699604836370861
Loss in iteration 104 : 0.4640097004217037
Loss in iteration 105 : 0.44767431982897127
Loss in iteration 106 : 0.4684428081691674
Loss in iteration 107 : 0.4635829004207013
Loss in iteration 108 : 0.45097183888991377
Loss in iteration 109 : 0.4682494598675819
Loss in iteration 110 : 0.4588116954910044
Loss in iteration 111 : 0.45204411809730977
Loss in iteration 112 : 0.4614801081187259
Loss in iteration 113 : 0.45725003917464396
Loss in iteration 114 : 0.4699176962935833
Loss in iteration 115 : 0.4595174373389807
Loss in iteration 116 : 0.45452423594766883
Loss in iteration 117 : 0.4604139995956222
Loss in iteration 118 : 0.45975184700354904
Loss in iteration 119 : 0.4624990035728128
Loss in iteration 120 : 0.4641414642623435
Loss in iteration 121 : 0.46074341661412116
Loss in iteration 122 : 0.46956229936954236
Loss in iteration 123 : 0.4636329310890991
Loss in iteration 124 : 0.45961922912607234
Loss in iteration 125 : 0.46267178068217624
Loss in iteration 126 : 0.46549872462550895
Loss in iteration 127 : 0.4791894038428106
Loss in iteration 128 : 0.45939079049590825
Loss in iteration 129 : 0.46349816338500427
Loss in iteration 130 : 0.4640643261088679
Loss in iteration 131 : 0.46476755740300857
Loss in iteration 132 : 0.45886027385939904
Loss in iteration 133 : 0.4645091451662407
Loss in iteration 134 : 0.459488646863862
Loss in iteration 135 : 0.4572038840808676
Loss in iteration 136 : 0.4661373041077494
Loss in iteration 137 : 0.45492676843373875
Loss in iteration 138 : 0.46164815430938894
Loss in iteration 139 : 0.44729910045863513
Loss in iteration 140 : 0.46811287469096713
Loss in iteration 141 : 0.4652665948605007
Loss in iteration 142 : 0.4648456946465599
Loss in iteration 143 : 0.4516329939302651
Loss in iteration 144 : 0.4575720311598847
Loss in iteration 145 : 0.45864442928669286
Loss in iteration 146 : 0.45472321215599215
Loss in iteration 147 : 0.46296762540033
Loss in iteration 148 : 0.45925236931352525
Loss in iteration 149 : 0.4697762583942694
Loss in iteration 150 : 0.46098117073406625
Loss in iteration 151 : 0.46109778558016357
Loss in iteration 152 : 0.47120361061045957
Loss in iteration 153 : 0.4600811265808636
Loss in iteration 154 : 0.46074520880225317
Loss in iteration 155 : 0.4554277439053451
Loss in iteration 156 : 0.4618375391679083
Loss in iteration 157 : 0.45315244614136524
Loss in iteration 158 : 0.4510738878765657
Loss in iteration 159 : 0.4631305109209387
Loss in iteration 160 : 0.4521301696910796
Loss in iteration 161 : 0.45555753417533257
Loss in iteration 162 : 0.46174906163773194
Loss in iteration 163 : 0.47241639032028715
Loss in iteration 164 : 0.4635942656740539
Loss in iteration 165 : 0.4760125664361806
Loss in iteration 166 : 0.46309684354419334
Loss in iteration 167 : 0.47652398904504406
Loss in iteration 168 : 0.45096552380311805
Loss in iteration 169 : 0.4631764747208626
Loss in iteration 170 : 0.4531621882495891
Loss in iteration 171 : 0.46218472943492644
Loss in iteration 172 : 0.4561884732606899
Loss in iteration 173 : 0.4576986686295788
Loss in iteration 174 : 0.45595291020966244
Loss in iteration 175 : 0.46091207436823944
Loss in iteration 176 : 0.4686805625053284
Loss in iteration 177 : 0.46667350467397894
Loss in iteration 178 : 0.4641767300107778
Loss in iteration 179 : 0.46342705039841753
Loss in iteration 180 : 0.46724979742494144
Loss in iteration 181 : 0.4658079170643546
Loss in iteration 182 : 0.4718619650417893
Loss in iteration 183 : 0.4675548459525181
Loss in iteration 184 : 0.46149028799509806
Loss in iteration 185 : 0.45546600921385655
Loss in iteration 186 : 0.45893355369995165
Loss in iteration 187 : 0.4521414087670331
Loss in iteration 188 : 0.47368947171056675
Loss in iteration 189 : 0.46794782381233024
Loss in iteration 190 : 0.4694557630507325
Loss in iteration 191 : 0.46951024885285475
Loss in iteration 192 : 0.46473867652829665
Loss in iteration 193 : 0.460304994223818
Loss in iteration 194 : 0.4593736025375762
Loss in iteration 195 : 0.464902232267387
Loss in iteration 196 : 0.46729517806329085
Loss in iteration 197 : 0.45539441854674495
Loss in iteration 198 : 0.4588986018951428
Loss in iteration 199 : 0.46990630497441316
Loss in iteration 200 : 0.45704596794836383
Testing accuracy  of updater 9 on alg 0 with rate 0.8 = 0.7795, training accuracy 0.78525, time elapsed: 4157 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6802032200251656
Loss in iteration 3 : 0.6634882183565415
Loss in iteration 4 : 0.6489449542830761
Loss in iteration 5 : 0.6286243888221221
Loss in iteration 6 : 0.6080772235673282
Loss in iteration 7 : 0.5851626425134284
Loss in iteration 8 : 0.5659811277047297
Loss in iteration 9 : 0.5511147767092364
Loss in iteration 10 : 0.543166849623856
Loss in iteration 11 : 0.5236356219114667
Loss in iteration 12 : 0.5124467186873349
Loss in iteration 13 : 0.5140884173792383
Loss in iteration 14 : 0.5009532888804272
Loss in iteration 15 : 0.5086408109872579
Loss in iteration 16 : 0.49205517349131483
Loss in iteration 17 : 0.5021741448512572
Loss in iteration 18 : 0.4954294353285612
Loss in iteration 19 : 0.49456623906117536
Loss in iteration 20 : 0.49269218794541036
Loss in iteration 21 : 0.4839623848400748
Loss in iteration 22 : 0.49570559721150065
Loss in iteration 23 : 0.48367430860191113
Loss in iteration 24 : 0.489158520583596
Loss in iteration 25 : 0.4750341252348931
Loss in iteration 26 : 0.47900154896257874
Loss in iteration 27 : 0.4700710890999092
Loss in iteration 28 : 0.4863323745643271
Loss in iteration 29 : 0.4693915629386639
Loss in iteration 30 : 0.4738569162269848
Loss in iteration 31 : 0.46484578837901147
Loss in iteration 32 : 0.46581327022913255
Loss in iteration 33 : 0.48307328371323854
Loss in iteration 34 : 0.48080588946162794
Loss in iteration 35 : 0.47678833024058775
Loss in iteration 36 : 0.48022975704790793
Loss in iteration 37 : 0.4784373624939805
Loss in iteration 38 : 0.4722640034498597
Loss in iteration 39 : 0.46724283421299634
Loss in iteration 40 : 0.4600159082157764
Loss in iteration 41 : 0.4643438914142708
Loss in iteration 42 : 0.46980631488767277
Loss in iteration 43 : 0.4697384534469995
Loss in iteration 44 : 0.45732552323950953
Loss in iteration 45 : 0.47638009776631185
Loss in iteration 46 : 0.4607012898617749
Loss in iteration 47 : 0.46774023142329263
Loss in iteration 48 : 0.46633029999839065
Loss in iteration 49 : 0.47711145628520674
Loss in iteration 50 : 0.4753192740127973
Loss in iteration 51 : 0.4591878167036297
Loss in iteration 52 : 0.4718734791519425
Loss in iteration 53 : 0.4666429396875152
Loss in iteration 54 : 0.4591850787852273
Loss in iteration 55 : 0.4603514285096462
Loss in iteration 56 : 0.4633967321395068
Loss in iteration 57 : 0.4631409135232134
Loss in iteration 58 : 0.47142205417175864
Loss in iteration 59 : 0.4794193091887157
Loss in iteration 60 : 0.46882532064941274
Loss in iteration 61 : 0.46590874670775206
Loss in iteration 62 : 0.46366594177558623
Loss in iteration 63 : 0.4690700361008478
Loss in iteration 64 : 0.46403146509201076
Loss in iteration 65 : 0.45838154666094705
Loss in iteration 66 : 0.47147791694061014
Loss in iteration 67 : 0.45422623183217653
Loss in iteration 68 : 0.47712747896711644
Loss in iteration 69 : 0.4685503247719872
Loss in iteration 70 : 0.45628191499092235
Loss in iteration 71 : 0.4670569432554456
Loss in iteration 72 : 0.4681288949730942
Loss in iteration 73 : 0.47674881067155944
Loss in iteration 74 : 0.469616728496548
Loss in iteration 75 : 0.4762490954655012
Loss in iteration 76 : 0.45979680771289727
Loss in iteration 77 : 0.456285642233647
Loss in iteration 78 : 0.45116285082405005
Loss in iteration 79 : 0.46544689059654776
Loss in iteration 80 : 0.4645953771297996
Loss in iteration 81 : 0.4676851588352041
Loss in iteration 82 : 0.45736943871775426
Loss in iteration 83 : 0.4688151094831073
Loss in iteration 84 : 0.4562568093891243
Loss in iteration 85 : 0.46557633557597294
Loss in iteration 86 : 0.4627567679080016
Loss in iteration 87 : 0.46208827181540296
Loss in iteration 88 : 0.4658528596583675
Loss in iteration 89 : 0.46071987976707096
Loss in iteration 90 : 0.4656709724300368
Loss in iteration 91 : 0.4615125174181799
Loss in iteration 92 : 0.46700485483253484
Loss in iteration 93 : 0.4673139271925862
Loss in iteration 94 : 0.46118094446616187
Loss in iteration 95 : 0.4667394635038117
Loss in iteration 96 : 0.4673790596169053
Loss in iteration 97 : 0.4723112597663724
Loss in iteration 98 : 0.46365157434133647
Loss in iteration 99 : 0.45377575464179526
Loss in iteration 100 : 0.4566565100063595
Loss in iteration 101 : 0.4592321127799866
Loss in iteration 102 : 0.46487357253727485
Loss in iteration 103 : 0.470125023429996
Loss in iteration 104 : 0.4665965145670802
Loss in iteration 105 : 0.4507092946137757
Loss in iteration 106 : 0.4701036899543479
Loss in iteration 107 : 0.46620190483182067
Loss in iteration 108 : 0.45358893453716487
Loss in iteration 109 : 0.46954994577202236
Loss in iteration 110 : 0.4617613148099337
Loss in iteration 111 : 0.45515704269753804
Loss in iteration 112 : 0.46268914163308855
Loss in iteration 113 : 0.4605697868964704
Loss in iteration 114 : 0.47000287849404077
Loss in iteration 115 : 0.46269085327423637
Loss in iteration 116 : 0.4557850749418666
Loss in iteration 117 : 0.4612498704596182
Loss in iteration 118 : 0.4629528232321519
Loss in iteration 119 : 0.4645780075469934
Loss in iteration 120 : 0.46659052180245747
Loss in iteration 121 : 0.46180882364701903
Loss in iteration 122 : 0.47262265102769846
Loss in iteration 123 : 0.46671050862483876
Loss in iteration 124 : 0.4573723748915485
Loss in iteration 125 : 0.4618330208085348
Loss in iteration 126 : 0.4682497695281481
Loss in iteration 127 : 0.47584145631687924
Loss in iteration 128 : 0.4598156250860696
Loss in iteration 129 : 0.46433252588381774
Loss in iteration 130 : 0.464964208177466
Loss in iteration 131 : 0.46569948105175196
Loss in iteration 132 : 0.4596436399912761
Loss in iteration 133 : 0.4641546017687307
Loss in iteration 134 : 0.45963208874214273
Loss in iteration 135 : 0.4594649997497331
Loss in iteration 136 : 0.46651005830871883
Loss in iteration 137 : 0.4560673673822507
Loss in iteration 138 : 0.46038625500952135
Loss in iteration 139 : 0.44952487657367557
Loss in iteration 140 : 0.46681123202667213
Loss in iteration 141 : 0.46760601360385257
Loss in iteration 142 : 0.46538753573373104
Loss in iteration 143 : 0.45124708157380405
Loss in iteration 144 : 0.4603754675384775
Loss in iteration 145 : 0.46176484404715945
Loss in iteration 146 : 0.4531938861796369
Loss in iteration 147 : 0.46428451876067717
Loss in iteration 148 : 0.4588663051023384
Loss in iteration 149 : 0.46762266991257945
Loss in iteration 150 : 0.4629224413476704
Loss in iteration 151 : 0.4613729910655252
Loss in iteration 152 : 0.47227597702025337
Loss in iteration 153 : 0.4614629578194845
Loss in iteration 154 : 0.4622176685417171
Loss in iteration 155 : 0.4577280046437758
Loss in iteration 156 : 0.46027585874139487
Loss in iteration 157 : 0.45379059028754176
Loss in iteration 158 : 0.45052637970799686
Loss in iteration 159 : 0.4623321750938544
Loss in iteration 160 : 0.452427753612292
Loss in iteration 161 : 0.45576872347421893
Loss in iteration 162 : 0.45930523827696373
Loss in iteration 163 : 0.4673100504148339
Loss in iteration 164 : 0.46500079080925083
Loss in iteration 165 : 0.4716784212283578
Loss in iteration 166 : 0.46449795929062815
Loss in iteration 167 : 0.47549951873522533
Loss in iteration 168 : 0.45241319870309715
Loss in iteration 169 : 0.46347871072767005
Loss in iteration 170 : 0.4557561884389453
Loss in iteration 171 : 0.4635998489157047
Loss in iteration 172 : 0.45781263299167646
Loss in iteration 173 : 0.45727846020200297
Loss in iteration 174 : 0.4569760366757885
Loss in iteration 175 : 0.4612312953711594
Loss in iteration 176 : 0.46954450795721636
Loss in iteration 177 : 0.4666379992038321
Loss in iteration 178 : 0.46141143002185175
Loss in iteration 179 : 0.4633444854622018
Loss in iteration 180 : 0.46520510795544673
Loss in iteration 181 : 0.4665786892727768
Loss in iteration 182 : 0.4675544841517867
Loss in iteration 183 : 0.46728567332620047
Loss in iteration 184 : 0.4584212243644377
Loss in iteration 185 : 0.45508384845658095
Loss in iteration 186 : 0.4544646640301293
Loss in iteration 187 : 0.4499579134501131
Loss in iteration 188 : 0.47233183251588146
Loss in iteration 189 : 0.4654804313785547
Loss in iteration 190 : 0.4682496665274027
Loss in iteration 191 : 0.463368984747284
Loss in iteration 192 : 0.46450294986886737
Loss in iteration 193 : 0.4550452821152992
Loss in iteration 194 : 0.4596701365526948
Loss in iteration 195 : 0.4619396249108968
Loss in iteration 196 : 0.4682497574892126
Loss in iteration 197 : 0.4566811506772109
Loss in iteration 198 : 0.45850890974735364
Loss in iteration 199 : 0.464764636492892
Loss in iteration 200 : 0.45726789407019586
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.786, training accuracy 0.7885, time elapsed: 4187 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6827541971119389
Loss in iteration 3 : 0.6674382404795777
Loss in iteration 4 : 0.6565200537847677
Loss in iteration 5 : 0.6393742791383306
Loss in iteration 6 : 0.620587394569892
Loss in iteration 7 : 0.6035557441331216
Loss in iteration 8 : 0.5871209512887493
Loss in iteration 9 : 0.5697357014095348
Loss in iteration 10 : 0.5608621414693373
Loss in iteration 11 : 0.545029701823506
Loss in iteration 12 : 0.5296824655129045
Loss in iteration 13 : 0.5274733756080237
Loss in iteration 14 : 0.516406845539907
Loss in iteration 15 : 0.5216250286447315
Loss in iteration 16 : 0.5053577975742963
Loss in iteration 17 : 0.5110387240393015
Loss in iteration 18 : 0.5052245286290926
Loss in iteration 19 : 0.5024017845823981
Loss in iteration 20 : 0.4990928564748397
Loss in iteration 21 : 0.4899355095486678
Loss in iteration 22 : 0.4994504700918297
Loss in iteration 23 : 0.48859572892970093
Loss in iteration 24 : 0.49437264453070406
Loss in iteration 25 : 0.4799730641085514
Loss in iteration 26 : 0.4837030155624418
Loss in iteration 27 : 0.4734736913292952
Loss in iteration 28 : 0.4893304587117623
Loss in iteration 29 : 0.47403162290289336
Loss in iteration 30 : 0.4771172887917721
Loss in iteration 31 : 0.4696064608154975
Loss in iteration 32 : 0.4701168397911853
Loss in iteration 33 : 0.4862931264139241
Loss in iteration 34 : 0.4830989353536375
Loss in iteration 35 : 0.48001002451644936
Loss in iteration 36 : 0.4829537372874234
Loss in iteration 37 : 0.48173267825994254
Loss in iteration 38 : 0.4754769455296433
Loss in iteration 39 : 0.4707911318635887
Loss in iteration 40 : 0.46378640724471737
Loss in iteration 41 : 0.4684466338819552
Loss in iteration 42 : 0.4721081570356851
Loss in iteration 43 : 0.47294082374623714
Loss in iteration 44 : 0.46161405778491793
Loss in iteration 45 : 0.4797724146325322
Loss in iteration 46 : 0.46448392169886377
Loss in iteration 47 : 0.4710280971141994
Loss in iteration 48 : 0.4697372713139611
Loss in iteration 49 : 0.4800676159923399
Loss in iteration 50 : 0.47868889591863634
Loss in iteration 51 : 0.46223873811986377
Loss in iteration 52 : 0.4751149182455444
Loss in iteration 53 : 0.4705560016093959
Loss in iteration 54 : 0.4629867127387295
Loss in iteration 55 : 0.463865761608078
Loss in iteration 56 : 0.4671808138502298
Loss in iteration 57 : 0.4657650177382957
Loss in iteration 58 : 0.4746539438134662
Loss in iteration 59 : 0.48141873138141994
Loss in iteration 60 : 0.47146036794837165
Loss in iteration 61 : 0.46837850862232944
Loss in iteration 62 : 0.46633246534866957
Loss in iteration 63 : 0.47199861449213343
Loss in iteration 64 : 0.4667120848986593
Loss in iteration 65 : 0.4603141766399568
Loss in iteration 66 : 0.47348521421786316
Loss in iteration 67 : 0.4564936064261182
Loss in iteration 68 : 0.47903737976078686
Loss in iteration 69 : 0.47091136434190634
Loss in iteration 70 : 0.4586570767706982
Loss in iteration 71 : 0.4695372119469372
Loss in iteration 72 : 0.46970556776716044
Loss in iteration 73 : 0.4786505297970377
Loss in iteration 74 : 0.4720277348057684
Loss in iteration 75 : 0.4775164265951842
Loss in iteration 76 : 0.4621604595774726
Loss in iteration 77 : 0.4573732340728552
Loss in iteration 78 : 0.4534305709482075
Loss in iteration 79 : 0.467383972134643
Loss in iteration 80 : 0.466183053640799
Loss in iteration 81 : 0.4696232806317271
Loss in iteration 82 : 0.45920597058453144
Loss in iteration 83 : 0.47019432059263194
Loss in iteration 84 : 0.457467505397897
Loss in iteration 85 : 0.46687845700955516
Loss in iteration 86 : 0.4639767951339038
Loss in iteration 87 : 0.46299437213969163
Loss in iteration 88 : 0.4663401189183292
Loss in iteration 89 : 0.4622438034109524
Loss in iteration 90 : 0.46733043356497705
Loss in iteration 91 : 0.46333962961987185
Loss in iteration 92 : 0.46745073024343287
Loss in iteration 93 : 0.4679632430064242
Loss in iteration 94 : 0.4624189878338114
Loss in iteration 95 : 0.4676259063724067
Loss in iteration 96 : 0.4683764758867312
Loss in iteration 97 : 0.4727697831866178
Loss in iteration 98 : 0.46459335531530266
Loss in iteration 99 : 0.45560222260012484
Loss in iteration 100 : 0.45823624369931837
Loss in iteration 101 : 0.46076150793428206
Loss in iteration 102 : 0.46545070766795354
Loss in iteration 103 : 0.47084946009200634
Loss in iteration 104 : 0.46771472483494825
Loss in iteration 105 : 0.45296465809846975
Loss in iteration 106 : 0.4712314597850839
Loss in iteration 107 : 0.4670959812078311
Loss in iteration 108 : 0.454833665380505
Loss in iteration 109 : 0.470826725087156
Loss in iteration 110 : 0.4625576352283747
Loss in iteration 111 : 0.4562805663145566
Loss in iteration 112 : 0.463864650783429
Loss in iteration 113 : 0.4608816793652411
Loss in iteration 114 : 0.4703023894657091
Loss in iteration 115 : 0.46343720522765924
Loss in iteration 116 : 0.45739153703764
Loss in iteration 117 : 0.46247592876950566
Loss in iteration 118 : 0.46312356231926216
Loss in iteration 119 : 0.4650672444331053
Loss in iteration 120 : 0.46810659888099265
Loss in iteration 121 : 0.4625441673768609
Loss in iteration 122 : 0.47184912860810657
Loss in iteration 123 : 0.467682679301946
Loss in iteration 124 : 0.45772259735494836
Loss in iteration 125 : 0.4617516141961231
Loss in iteration 126 : 0.46740728941647086
Loss in iteration 127 : 0.4765409582073853
Loss in iteration 128 : 0.4603813008772267
Loss in iteration 129 : 0.4651581138731335
Loss in iteration 130 : 0.4650749737293269
Loss in iteration 131 : 0.4665469446125365
Loss in iteration 132 : 0.4612986799285293
Loss in iteration 133 : 0.4647240236155864
Loss in iteration 134 : 0.46065269406666964
Loss in iteration 135 : 0.45984629712027014
Loss in iteration 136 : 0.4673774721891529
Loss in iteration 137 : 0.4585613351885546
Loss in iteration 138 : 0.4607655980303119
Loss in iteration 139 : 0.4502098056812811
Loss in iteration 140 : 0.46733446483847707
Loss in iteration 141 : 0.46838061245572304
Loss in iteration 142 : 0.46614293706132204
Loss in iteration 143 : 0.45200350570012615
Loss in iteration 144 : 0.46158090024227505
Loss in iteration 145 : 0.4626419086211569
Loss in iteration 146 : 0.45415421410521806
Loss in iteration 147 : 0.46475100634520594
Loss in iteration 148 : 0.45963387087591195
Loss in iteration 149 : 0.4685257149787943
Loss in iteration 150 : 0.4635998066241647
Loss in iteration 151 : 0.46221965644887464
Loss in iteration 152 : 0.47321632604387337
Loss in iteration 153 : 0.4625943533015914
Loss in iteration 154 : 0.46354180660380495
Loss in iteration 155 : 0.45844571538687107
Loss in iteration 156 : 0.46107537067718074
Loss in iteration 157 : 0.45448158815359974
Loss in iteration 158 : 0.4513376080602302
Loss in iteration 159 : 0.46337188124003215
Loss in iteration 160 : 0.4533219297947367
Loss in iteration 161 : 0.45672472758623606
Loss in iteration 162 : 0.4605397763770431
Loss in iteration 163 : 0.4686558583902946
Loss in iteration 164 : 0.46597726527768846
Loss in iteration 165 : 0.47195187623716034
Loss in iteration 166 : 0.46533169949665276
Loss in iteration 167 : 0.47636252051566547
Loss in iteration 168 : 0.45329761128123136
Loss in iteration 169 : 0.4639916948933463
Loss in iteration 170 : 0.45600180585123773
Loss in iteration 171 : 0.4642767185975738
Loss in iteration 172 : 0.45870374326863644
Loss in iteration 173 : 0.4582224564048915
Loss in iteration 174 : 0.45810635016671203
Loss in iteration 175 : 0.46117089973848463
Loss in iteration 176 : 0.46978798086658013
Loss in iteration 177 : 0.4675445063767617
Loss in iteration 178 : 0.4615440063270444
Loss in iteration 179 : 0.4635017441787372
Loss in iteration 180 : 0.4661264556521802
Loss in iteration 181 : 0.4671329309346512
Loss in iteration 182 : 0.4682900990655759
Loss in iteration 183 : 0.46757419937164124
Loss in iteration 184 : 0.4586812245468706
Loss in iteration 185 : 0.4557778265397655
Loss in iteration 186 : 0.45493465930160426
Loss in iteration 187 : 0.4507151620522486
Loss in iteration 188 : 0.47261911537121887
Loss in iteration 189 : 0.46619652116755833
Loss in iteration 190 : 0.46883184869454936
Loss in iteration 191 : 0.4645654369099915
Loss in iteration 192 : 0.4650068438608318
Loss in iteration 193 : 0.45586171780133417
Loss in iteration 194 : 0.46011002909001475
Loss in iteration 195 : 0.4623746240654422
Loss in iteration 196 : 0.46848154443833523
Loss in iteration 197 : 0.45645627360382
Loss in iteration 198 : 0.4587636599712745
Loss in iteration 199 : 0.46530230995690336
Loss in iteration 200 : 0.457681607101924
Testing accuracy  of updater 9 on alg 0 with rate 0.14 = 0.785, training accuracy 0.789125, time elapsed: 3973 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6855641552295813
Loss in iteration 3 : 0.6734942784808758
Loss in iteration 4 : 0.667110659918715
Loss in iteration 5 : 0.656924217355455
Loss in iteration 6 : 0.6404620991166012
Loss in iteration 7 : 0.625771332583653
Loss in iteration 8 : 0.6159903618285901
Loss in iteration 9 : 0.6047494180769719
Loss in iteration 10 : 0.5925493310120395
Loss in iteration 11 : 0.5766483623721519
Loss in iteration 12 : 0.5645863407212219
Loss in iteration 13 : 0.5601917140981997
Loss in iteration 14 : 0.5481195924078225
Loss in iteration 15 : 0.5475876681278057
Loss in iteration 16 : 0.532804751186265
Loss in iteration 17 : 0.5358968960154689
Loss in iteration 18 : 0.5278122855874948
Loss in iteration 19 : 0.525765286989787
Loss in iteration 20 : 0.5194579567817724
Loss in iteration 21 : 0.5108574728068159
Loss in iteration 22 : 0.5153977833515369
Loss in iteration 23 : 0.5081179721471705
Loss in iteration 24 : 0.5098096186484202
Loss in iteration 25 : 0.49672845240954355
Loss in iteration 26 : 0.4983764043402899
Loss in iteration 27 : 0.4893515807932938
Loss in iteration 28 : 0.5009826378597992
Loss in iteration 29 : 0.4869314951097274
Loss in iteration 30 : 0.48934744117456586
Loss in iteration 31 : 0.48250229212034257
Loss in iteration 32 : 0.48264136074997604
Loss in iteration 33 : 0.4946392801087122
Loss in iteration 34 : 0.4929130958441939
Loss in iteration 35 : 0.4899772917108522
Loss in iteration 36 : 0.49003867817913144
Loss in iteration 37 : 0.4900314721000218
Loss in iteration 38 : 0.484667347813833
Loss in iteration 39 : 0.47998787770646595
Loss in iteration 40 : 0.47394107473774527
Loss in iteration 41 : 0.47823003946746606
Loss in iteration 42 : 0.4791725226860441
Loss in iteration 43 : 0.48063616814182636
Loss in iteration 44 : 0.4713365476540052
Loss in iteration 45 : 0.4871510435350495
Loss in iteration 46 : 0.4730855639504431
Loss in iteration 47 : 0.4793740372701523
Loss in iteration 48 : 0.4778256651950518
Loss in iteration 49 : 0.48616459101403675
Loss in iteration 50 : 0.4849512357066082
Loss in iteration 51 : 0.46988596222772133
Loss in iteration 52 : 0.4827154480432842
Loss in iteration 53 : 0.4782714383524873
Loss in iteration 54 : 0.47134530833900024
Loss in iteration 55 : 0.4725853348884467
Loss in iteration 56 : 0.47558134370683586
Loss in iteration 57 : 0.4728277940586809
Loss in iteration 58 : 0.48064187527141533
Loss in iteration 59 : 0.4858640086792402
Loss in iteration 60 : 0.4785079756765729
Loss in iteration 61 : 0.47525454694532443
Loss in iteration 62 : 0.4739483078595712
Loss in iteration 63 : 0.47820772495226627
Loss in iteration 64 : 0.47283783710024246
Loss in iteration 65 : 0.4670518065580907
Loss in iteration 66 : 0.47986121799876613
Loss in iteration 67 : 0.4633202492040975
Loss in iteration 68 : 0.48412133127099544
Loss in iteration 69 : 0.4770205482634325
Loss in iteration 70 : 0.46428391607036734
Loss in iteration 71 : 0.47544712653293847
Loss in iteration 72 : 0.47511522621180624
Loss in iteration 73 : 0.4843505286077506
Loss in iteration 74 : 0.4779373994608731
Loss in iteration 75 : 0.4815899711779923
Loss in iteration 76 : 0.46778711115663296
Loss in iteration 77 : 0.46364332575584666
Loss in iteration 78 : 0.4603100299135493
Loss in iteration 79 : 0.4724899466725032
Loss in iteration 80 : 0.47125598827132303
Loss in iteration 81 : 0.4754663508179708
Loss in iteration 82 : 0.4644924608036588
Loss in iteration 83 : 0.47461520506775456
Loss in iteration 84 : 0.462438980930434
Loss in iteration 85 : 0.47209799564257604
Loss in iteration 86 : 0.4688620961797039
Loss in iteration 87 : 0.4665328572718643
Loss in iteration 88 : 0.4702323467143152
Loss in iteration 89 : 0.46618689378682243
Loss in iteration 90 : 0.4721524282930154
Loss in iteration 91 : 0.46801469114490074
Loss in iteration 92 : 0.47033632669735964
Loss in iteration 93 : 0.4725936523559638
Loss in iteration 94 : 0.46732708932350403
Loss in iteration 95 : 0.47126312269500437
Loss in iteration 96 : 0.4721888022718707
Loss in iteration 97 : 0.4765127305172349
Loss in iteration 98 : 0.4693142891043299
Loss in iteration 99 : 0.4604782573615976
Loss in iteration 100 : 0.4622934908590028
Loss in iteration 101 : 0.4657471321654428
Loss in iteration 102 : 0.46896595278957737
Loss in iteration 103 : 0.47468938527393445
Loss in iteration 104 : 0.471112359296247
Loss in iteration 105 : 0.45793072270739005
Loss in iteration 106 : 0.4750299667842474
Loss in iteration 107 : 0.47100138733590063
Loss in iteration 108 : 0.45942151267619374
Loss in iteration 109 : 0.4746450015926476
Loss in iteration 110 : 0.46539202261504914
Loss in iteration 111 : 0.45944416100684726
Loss in iteration 112 : 0.4675984549223711
Loss in iteration 113 : 0.4644468916903717
Loss in iteration 114 : 0.4731306497873289
Loss in iteration 115 : 0.4666271207517279
Loss in iteration 116 : 0.46126075103218395
Loss in iteration 117 : 0.4656462592572974
Loss in iteration 118 : 0.46658797214779246
Loss in iteration 119 : 0.4683528009592187
Loss in iteration 120 : 0.4716895675926761
Loss in iteration 121 : 0.4652686453047303
Loss in iteration 122 : 0.47270817603384213
Loss in iteration 123 : 0.4697789112806045
Loss in iteration 124 : 0.46066103329723845
Loss in iteration 125 : 0.4653722465442
Loss in iteration 126 : 0.4700383472352368
Loss in iteration 127 : 0.4781314814012849
Loss in iteration 128 : 0.4627381597467577
Loss in iteration 129 : 0.468095946658636
Loss in iteration 130 : 0.46709694325381307
Loss in iteration 131 : 0.4692836632164739
Loss in iteration 132 : 0.46385411869010595
Loss in iteration 133 : 0.4671870885546041
Loss in iteration 134 : 0.4638429427395443
Loss in iteration 135 : 0.46267966868298865
Loss in iteration 136 : 0.46938058656935683
Loss in iteration 137 : 0.4601141680858224
Loss in iteration 138 : 0.46315555163036637
Loss in iteration 139 : 0.4536342981986703
Loss in iteration 140 : 0.46851865739993714
Loss in iteration 141 : 0.47059039691232063
Loss in iteration 142 : 0.4681522209445113
Loss in iteration 143 : 0.45501717240104367
Loss in iteration 144 : 0.46462749654982505
Loss in iteration 145 : 0.46481816645833735
Loss in iteration 146 : 0.4573182564075404
Loss in iteration 147 : 0.4666256765478978
Loss in iteration 148 : 0.4621253559077791
Loss in iteration 149 : 0.4696424112567898
Loss in iteration 150 : 0.46591640683308155
Loss in iteration 151 : 0.4646602931747071
Loss in iteration 152 : 0.4750282628733955
Loss in iteration 153 : 0.4649647555017557
Loss in iteration 154 : 0.466254684587318
Loss in iteration 155 : 0.45972766109706126
Loss in iteration 156 : 0.46254293739824087
Loss in iteration 157 : 0.45696000046741114
Loss in iteration 158 : 0.4536924955142452
Loss in iteration 159 : 0.46493484004523583
Loss in iteration 160 : 0.45624007246277515
Loss in iteration 161 : 0.4589299634874809
Loss in iteration 162 : 0.46241236603258007
Loss in iteration 163 : 0.4697682440481235
Loss in iteration 164 : 0.46795450000454186
Loss in iteration 165 : 0.4731056610198816
Loss in iteration 166 : 0.46690604763455257
Loss in iteration 167 : 0.4773124596841961
Loss in iteration 168 : 0.45598689769344175
Loss in iteration 169 : 0.4654315364369304
Loss in iteration 170 : 0.4583369093735331
Loss in iteration 171 : 0.46678095613959497
Loss in iteration 172 : 0.46048186719882495
Loss in iteration 173 : 0.45979893022911655
Loss in iteration 174 : 0.45953716552817475
Loss in iteration 175 : 0.46311993315507677
Loss in iteration 176 : 0.4710694365026171
Loss in iteration 177 : 0.46827242060830526
Loss in iteration 178 : 0.463408960717074
Loss in iteration 179 : 0.4653289558415655
Loss in iteration 180 : 0.46719277365330325
Loss in iteration 181 : 0.4691872643804376
Loss in iteration 182 : 0.4694819341974056
Loss in iteration 183 : 0.46879437325908035
Loss in iteration 184 : 0.4611128416912133
Loss in iteration 185 : 0.45763285299231515
Loss in iteration 186 : 0.457409126997863
Loss in iteration 187 : 0.4523144092969377
Loss in iteration 188 : 0.473437072389267
Loss in iteration 189 : 0.46842014534798965
Loss in iteration 190 : 0.469571237530768
Loss in iteration 191 : 0.4662307985367238
Loss in iteration 192 : 0.4667046398394457
Loss in iteration 193 : 0.45780258457939227
Loss in iteration 194 : 0.4615590616374227
Loss in iteration 195 : 0.46445677604194024
Loss in iteration 196 : 0.4701891497394217
Loss in iteration 197 : 0.458158798573494
Loss in iteration 198 : 0.4605761893121334
Loss in iteration 199 : 0.4667050243629099
Loss in iteration 200 : 0.4593478432403796
Testing accuracy  of updater 9 on alg 0 with rate 0.08000000000000002 = 0.7845, training accuracy 0.789875, time elapsed: 3780 millisecond.
Loss in iteration 1 : 0.6931471805599321
Loss in iteration 2 : 0.6907246067617776
Loss in iteration 3 : 0.6857986618358446
Loss in iteration 4 : 0.681888597497846
Loss in iteration 5 : 0.6780737942539387
Loss in iteration 6 : 0.6723337240138376
Loss in iteration 7 : 0.6672795968843331
Loss in iteration 8 : 0.6621813721641775
Loss in iteration 9 : 0.6553016966760599
Loss in iteration 10 : 0.6510273072735989
Loss in iteration 11 : 0.6465014038806258
Loss in iteration 12 : 0.6384607562208173
Loss in iteration 13 : 0.6347521256818489
Loss in iteration 14 : 0.6299413309192415
Loss in iteration 15 : 0.6258964473333987
Loss in iteration 16 : 0.6170497611732746
Loss in iteration 17 : 0.6155616989889939
Loss in iteration 18 : 0.609918767304494
Loss in iteration 19 : 0.6045348521441342
Loss in iteration 20 : 0.6013242230767912
Loss in iteration 21 : 0.5956391992082436
Loss in iteration 22 : 0.5927927313188034
Loss in iteration 23 : 0.5865883256194179
Loss in iteration 24 : 0.5852910446024795
Loss in iteration 25 : 0.5759364688734974
Loss in iteration 26 : 0.574792521175995
Loss in iteration 27 : 0.5665556058945063
Loss in iteration 28 : 0.570489900625352
Loss in iteration 29 : 0.5621116910378183
Loss in iteration 30 : 0.5613962432436144
Loss in iteration 31 : 0.5535837186624173
Loss in iteration 32 : 0.5533768556706322
Loss in iteration 33 : 0.5570568025058914
Loss in iteration 34 : 0.554345041438667
Loss in iteration 35 : 0.5498294904750808
Loss in iteration 36 : 0.5481655333904167
Loss in iteration 37 : 0.5484710168867556
Loss in iteration 38 : 0.5434691195375365
Loss in iteration 39 : 0.539691076487073
Loss in iteration 40 : 0.5349245429929131
Loss in iteration 41 : 0.5361040644469524
Loss in iteration 42 : 0.5321040184375642
Loss in iteration 43 : 0.533755822236769
Loss in iteration 44 : 0.5279116240511346
Loss in iteration 45 : 0.5354580971907895
Loss in iteration 46 : 0.5242952547273383
Loss in iteration 47 : 0.5289152563358518
Loss in iteration 48 : 0.5270067699049766
Loss in iteration 49 : 0.5301802152387868
Loss in iteration 50 : 0.5277976596780596
Loss in iteration 51 : 0.5182750030452349
Loss in iteration 52 : 0.5275152467527559
Loss in iteration 53 : 0.5221968520403405
Loss in iteration 54 : 0.5174206468205288
Loss in iteration 55 : 0.517928094676044
Loss in iteration 56 : 0.5180224503443945
Loss in iteration 57 : 0.5147280307615902
Loss in iteration 58 : 0.5203279545391287
Loss in iteration 59 : 0.5212589960812916
Loss in iteration 60 : 0.5172789162133308
Loss in iteration 61 : 0.5128623426211937
Loss in iteration 62 : 0.5122498008630426
Loss in iteration 63 : 0.5152752625918966
Loss in iteration 64 : 0.509707208072933
Loss in iteration 65 : 0.5072385215299351
Loss in iteration 66 : 0.5155061677380525
Loss in iteration 67 : 0.5045110810766434
Loss in iteration 68 : 0.5137389898287543
Loss in iteration 69 : 0.509030913259708
Loss in iteration 70 : 0.5027085031256487
Loss in iteration 71 : 0.5117435293048964
Loss in iteration 72 : 0.5090425288005999
Loss in iteration 73 : 0.5140223896816385
Loss in iteration 74 : 0.5097796330316922
Loss in iteration 75 : 0.5098326141372401
Loss in iteration 76 : 0.5019518967940878
Loss in iteration 77 : 0.49939059459740154
Loss in iteration 78 : 0.4963189550939201
Loss in iteration 79 : 0.5029605156667498
Loss in iteration 80 : 0.5023935346101328
Loss in iteration 81 : 0.5063435510527293
Loss in iteration 82 : 0.4985211309830925
Loss in iteration 83 : 0.5048680157643158
Loss in iteration 84 : 0.49547064789342776
Loss in iteration 85 : 0.5027752462761549
Loss in iteration 86 : 0.49946281805868004
Loss in iteration 87 : 0.4950800283245429
Loss in iteration 88 : 0.4993864713992461
Loss in iteration 89 : 0.4959009280619027
Loss in iteration 90 : 0.4996088671586954
Loss in iteration 91 : 0.4981143344343246
Loss in iteration 92 : 0.4968770485807746
Loss in iteration 93 : 0.5004653430009716
Loss in iteration 94 : 0.4957998149323912
Loss in iteration 95 : 0.4984910812342161
Loss in iteration 96 : 0.49950699342568744
Loss in iteration 97 : 0.5009999099402753
Loss in iteration 98 : 0.4964558800486478
Loss in iteration 99 : 0.49066479140352676
Loss in iteration 100 : 0.48958372351756996
Loss in iteration 101 : 0.49302410561339177
Loss in iteration 102 : 0.4943965746223023
Loss in iteration 103 : 0.49942382601645097
Loss in iteration 104 : 0.49714313425896317
Loss in iteration 105 : 0.48713938421330205
Loss in iteration 106 : 0.4994267793305632
Loss in iteration 107 : 0.4958213462718089
Loss in iteration 108 : 0.4876978937424149
Loss in iteration 109 : 0.4995125149880102
Loss in iteration 110 : 0.4914465936811281
Loss in iteration 111 : 0.48603487969249454
Loss in iteration 112 : 0.49304424998758184
Loss in iteration 113 : 0.4894937136739982
Loss in iteration 114 : 0.49462279980088764
Loss in iteration 115 : 0.4919792270693132
Loss in iteration 116 : 0.4888672941617078
Loss in iteration 117 : 0.4899448936443559
Loss in iteration 118 : 0.4911552732702925
Loss in iteration 119 : 0.4918179692344628
Loss in iteration 120 : 0.4946615960283836
Loss in iteration 121 : 0.4863827355869906
Loss in iteration 122 : 0.4929387268632961
Loss in iteration 123 : 0.4915922237805505
Loss in iteration 124 : 0.48445145159299147
Loss in iteration 125 : 0.4869067470485068
Loss in iteration 126 : 0.4920097646028095
Loss in iteration 127 : 0.4976755823064089
Loss in iteration 128 : 0.4844142236191173
Loss in iteration 129 : 0.4906912124911588
Loss in iteration 130 : 0.48671296914682377
Loss in iteration 131 : 0.4902556869316973
Loss in iteration 132 : 0.4855650742041885
Loss in iteration 133 : 0.488048077528407
Loss in iteration 134 : 0.4857415509668238
Loss in iteration 135 : 0.4827852639527541
Loss in iteration 136 : 0.4904447089442354
Loss in iteration 137 : 0.48121750462860124
Loss in iteration 138 : 0.4838723668582112
Loss in iteration 139 : 0.47840015498999505
Loss in iteration 140 : 0.4874911124951842
Loss in iteration 141 : 0.49028914976580285
Loss in iteration 142 : 0.48763165077565934
Loss in iteration 143 : 0.4784488014451537
Loss in iteration 144 : 0.4844820088377866
Loss in iteration 145 : 0.4840337246962705
Loss in iteration 146 : 0.47972344535392325
Loss in iteration 147 : 0.48613618568178996
Loss in iteration 148 : 0.4840607455641469
Loss in iteration 149 : 0.4879223996076952
Loss in iteration 150 : 0.48529946486057246
Loss in iteration 151 : 0.4853670344591618
Loss in iteration 152 : 0.4917241958515826
Loss in iteration 153 : 0.48398990073035486
Loss in iteration 154 : 0.48563431250829375
Loss in iteration 155 : 0.4775600156505751
Loss in iteration 156 : 0.48057678619651306
Loss in iteration 157 : 0.47751158947759526
Loss in iteration 158 : 0.4735310378676446
Loss in iteration 159 : 0.48218215877265197
Loss in iteration 160 : 0.4766758802604619
Loss in iteration 161 : 0.4778287676278778
Loss in iteration 162 : 0.48216085879055093
Loss in iteration 163 : 0.48752065838216396
Loss in iteration 164 : 0.48725832875739694
Loss in iteration 165 : 0.487554099957923
Loss in iteration 166 : 0.4825027703734545
Loss in iteration 167 : 0.49187879419064
Loss in iteration 168 : 0.4762457876300275
Loss in iteration 169 : 0.4823956764229942
Loss in iteration 170 : 0.47746945318517203
Loss in iteration 171 : 0.4848797635796911
Loss in iteration 172 : 0.4787986874706072
Loss in iteration 173 : 0.4774484735382356
Loss in iteration 174 : 0.476760694148052
Loss in iteration 175 : 0.47993557637364337
Loss in iteration 176 : 0.4855047161369886
Loss in iteration 177 : 0.48270969857674434
Loss in iteration 178 : 0.4780214420184901
Loss in iteration 179 : 0.4798250316978037
Loss in iteration 180 : 0.483058543883856
Loss in iteration 181 : 0.48498434072559365
Loss in iteration 182 : 0.4840997494216761
Loss in iteration 183 : 0.4834115748278571
Loss in iteration 184 : 0.47967125702105307
Loss in iteration 185 : 0.47524594857823754
Loss in iteration 186 : 0.4744285464540357
Loss in iteration 187 : 0.46933944658467797
Loss in iteration 188 : 0.4866603556028247
Loss in iteration 189 : 0.48531905401506353
Loss in iteration 190 : 0.48258753155604905
Loss in iteration 191 : 0.483477385177348
Loss in iteration 192 : 0.48222976897047554
Loss in iteration 193 : 0.473522099742839
Loss in iteration 194 : 0.47641951944797467
Loss in iteration 195 : 0.48108309702197166
Loss in iteration 196 : 0.4863114810096184
Loss in iteration 197 : 0.4733687902390439
Loss in iteration 198 : 0.4755535119604348
Loss in iteration 199 : 0.4799126661402765
Loss in iteration 200 : 0.47527934405128397
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.78, training accuracy 0.782, time elapsed: 3845 millisecond.
