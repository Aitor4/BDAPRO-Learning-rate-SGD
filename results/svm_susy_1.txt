objc[1567]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10cf344c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10df594e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 19:27:21 INFO SparkContext: Running Spark version 2.0.0
18/02/26 19:27:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 19:27:22 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 19:27:22 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 19:27:22 INFO SecurityManager: Changing view acls groups to: 
18/02/26 19:27:22 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 19:27:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 19:27:23 INFO Utils: Successfully started service 'sparkDriver' on port 50498.
18/02/26 19:27:23 INFO SparkEnv: Registering MapOutputTracker
18/02/26 19:27:23 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 19:27:23 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-3d29d763-7a29-415a-bb31-9cb34338f40e
18/02/26 19:27:23 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 19:27:24 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 19:27:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 19:27:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 19:27:24 INFO Executor: Starting executor ID driver on host localhost
18/02/26 19:27:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50500.
18/02/26 19:27:24 INFO NettyBlockTransferService: Server created on 192.168.2.140:50500
18/02/26 19:27:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50500)
18/02/26 19:27:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50500 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50500)
18/02/26 19:27:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50500)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.7637457863079735
Loss in iteration 3 : 21.073859681496845
Loss in iteration 4 : 8.986991572615942
Loss in iteration 5 : 14.539457668301038
Loss in iteration 6 : 13.180878550547261
Loss in iteration 7 : 8.31430970145108
Loss in iteration 8 : 15.467231900578147
Loss in iteration 9 : 5.128244719867838
Loss in iteration 10 : 14.429444280300459
Loss in iteration 11 : 5.609646364734405
Loss in iteration 12 : 13.404569908653487
Loss in iteration 13 : 6.090355794218036
Loss in iteration 14 : 12.350347249643866
Loss in iteration 15 : 6.620416747589253
Loss in iteration 16 : 11.2444088994146
Loss in iteration 17 : 7.135186949482002
Loss in iteration 18 : 10.217983425796046
Loss in iteration 19 : 7.45735657246114
Loss in iteration 20 : 9.29746286206531
Loss in iteration 21 : 7.646755299167509
Loss in iteration 22 : 8.44593545163489
Loss in iteration 23 : 7.530899366888362
Loss in iteration 24 : 7.847418723898605
Loss in iteration 25 : 7.457627820413414
Loss in iteration 26 : 7.30611060889198
Loss in iteration 27 : 7.207506416636299
Loss in iteration 28 : 6.6616844008067675
Loss in iteration 29 : 7.037439357003943
Loss in iteration 30 : 6.252023953166852
Loss in iteration 31 : 6.831899996520338
Loss in iteration 32 : 5.972679515853862
Loss in iteration 33 : 6.737907178346471
Loss in iteration 34 : 5.780961473893163
Loss in iteration 35 : 6.549874616111649
Loss in iteration 36 : 5.523591380977136
Loss in iteration 37 : 6.299326492178905
Loss in iteration 38 : 5.273852742862302
Loss in iteration 39 : 6.107990589937874
Loss in iteration 40 : 5.122511380044223
Loss in iteration 41 : 5.989726630515679
Loss in iteration 42 : 5.026547718688752
Loss in iteration 43 : 5.925331878366122
Loss in iteration 44 : 4.958876821144649
Loss in iteration 45 : 5.862019272591648
Loss in iteration 46 : 4.8324067723187145
Loss in iteration 47 : 5.756963680293318
Loss in iteration 48 : 4.787472670976448
Loss in iteration 49 : 5.736279567501351
Loss in iteration 50 : 4.754745110603243
Loss in iteration 51 : 5.71162060545296
Loss in iteration 52 : 4.653556509103433
Loss in iteration 53 : 5.600267092625632
Loss in iteration 54 : 4.604644622973347
Loss in iteration 55 : 5.532634184065326
Loss in iteration 56 : 4.557206139375867
Loss in iteration 57 : 5.501742400283313
Loss in iteration 58 : 4.476306914197321
Loss in iteration 59 : 5.441937422357072
Loss in iteration 60 : 4.417435359079882
Loss in iteration 61 : 5.42003104367061
Loss in iteration 62 : 4.361772880733001
Loss in iteration 63 : 5.388736849712374
Loss in iteration 64 : 4.336115402609534
Loss in iteration 65 : 5.358088510714322
Loss in iteration 66 : 4.297504315039146
Loss in iteration 67 : 5.284362366135054
Loss in iteration 68 : 4.2519633333367075
Loss in iteration 69 : 5.2501913811580945
Loss in iteration 70 : 4.21316119016555
Loss in iteration 71 : 5.2271246699063205
Loss in iteration 72 : 4.191361642117168
Loss in iteration 73 : 5.1604303593723655
Loss in iteration 74 : 4.14561593145424
Loss in iteration 75 : 5.131556650741372
Loss in iteration 76 : 4.115039527627378
Loss in iteration 77 : 5.114596214060617
Loss in iteration 78 : 4.108744469230069
Loss in iteration 79 : 5.111584348394231
Loss in iteration 80 : 4.0918057991757895
Loss in iteration 81 : 5.07292416550608
Loss in iteration 82 : 4.086678941450259
Loss in iteration 83 : 5.059458921199759
Loss in iteration 84 : 4.063528047183307
Loss in iteration 85 : 5.037219636569904
Loss in iteration 86 : 4.060901073880819
Loss in iteration 87 : 5.037660049594021
Loss in iteration 88 : 4.047880792320621
Loss in iteration 89 : 5.021939891860394
Loss in iteration 90 : 4.053756979211876
Loss in iteration 91 : 5.017771684751386
Loss in iteration 92 : 4.058108945365712
Loss in iteration 93 : 5.021887970890784
Loss in iteration 94 : 4.053072656439576
Loss in iteration 95 : 5.007488485687583
Loss in iteration 96 : 4.042916497813709
Loss in iteration 97 : 4.977871389322759
Loss in iteration 98 : 4.041055623252816
Loss in iteration 99 : 4.972971552028124
Loss in iteration 100 : 4.03006185802126
Loss in iteration 101 : 4.946095166054909
Loss in iteration 102 : 4.034754839980284
Loss in iteration 103 : 4.964582462709315
Loss in iteration 104 : 4.027079946502418
Loss in iteration 105 : 4.922923559124493
Loss in iteration 106 : 4.032893020298143
Loss in iteration 107 : 4.944848496916388
Loss in iteration 108 : 4.0143613215276455
Loss in iteration 109 : 4.913506048256819
Loss in iteration 110 : 4.025410044884187
Loss in iteration 111 : 4.905704160537403
Loss in iteration 112 : 4.02218902453677
Loss in iteration 113 : 4.887976036292067
Loss in iteration 114 : 4.019507581604434
Loss in iteration 115 : 4.884958957109068
Loss in iteration 116 : 4.0133123134511575
Loss in iteration 117 : 4.8651717824445315
Loss in iteration 118 : 3.984581107984163
Loss in iteration 119 : 4.804898924392775
Loss in iteration 120 : 4.007695767225246
Loss in iteration 121 : 4.853506693462905
Loss in iteration 122 : 3.997023197299671
Loss in iteration 123 : 4.80481334519937
Loss in iteration 124 : 3.986976597379876
Loss in iteration 125 : 4.790591412842825
Loss in iteration 126 : 3.9951236373451904
Loss in iteration 127 : 4.797712238312409
Loss in iteration 128 : 3.99249844383471
Loss in iteration 129 : 4.789234256253425
Loss in iteration 130 : 3.9885007054267225
Loss in iteration 131 : 4.777753257090199
Loss in iteration 132 : 3.985630469840826
Loss in iteration 133 : 4.772637496335458
Loss in iteration 134 : 3.974630771294288
Loss in iteration 135 : 4.75658112211782
Loss in iteration 136 : 3.971908958030609
Loss in iteration 137 : 4.7495712711868165
Loss in iteration 138 : 3.973667668313514
Loss in iteration 139 : 4.746495554798297
Loss in iteration 140 : 3.961242292448459
Loss in iteration 141 : 4.728277269554199
Loss in iteration 142 : 3.946349273693087
Loss in iteration 143 : 4.719034221934319
Loss in iteration 144 : 3.9398237591948444
Loss in iteration 145 : 4.712738913379388
Loss in iteration 146 : 3.931041811671029
Loss in iteration 147 : 4.699496671523974
Loss in iteration 148 : 3.9383938658788193
Loss in iteration 149 : 4.705158323741234
Loss in iteration 150 : 3.9485477199749655
Loss in iteration 151 : 4.7054084930883775
Loss in iteration 152 : 3.946430017554818
Loss in iteration 153 : 4.698762547493677
Loss in iteration 154 : 3.948310437130929
Loss in iteration 155 : 4.68267958725389
Loss in iteration 156 : 3.9429839546220857
Loss in iteration 157 : 4.6685811941678965
Loss in iteration 158 : 3.9353178164775
Loss in iteration 159 : 4.655237989979297
Loss in iteration 160 : 3.946652529783849
Loss in iteration 161 : 4.684478794131281
Loss in iteration 162 : 3.9442238943051025
Loss in iteration 163 : 4.675157526238884
Loss in iteration 164 : 3.950933403332451
Loss in iteration 165 : 4.675958413890166
Loss in iteration 166 : 3.9406037921969257
Loss in iteration 167 : 4.650633913018056
Loss in iteration 168 : 3.936559719461767
Loss in iteration 169 : 4.6362617773451005
Loss in iteration 170 : 3.948456325900986
Loss in iteration 171 : 4.643206581266717
Loss in iteration 172 : 3.9499390607425147
Loss in iteration 173 : 4.643317732990812
Loss in iteration 174 : 3.9473898700031222
Loss in iteration 175 : 4.634175575804012
Loss in iteration 176 : 3.94833608258591
Loss in iteration 177 : 4.620066570404606
Loss in iteration 178 : 3.949010413641022
Loss in iteration 179 : 4.626202467249317
Loss in iteration 180 : 3.953944927579341
Loss in iteration 181 : 4.629251827954871
Loss in iteration 182 : 3.950694248827658
Loss in iteration 183 : 4.621008305097477
Loss in iteration 184 : 3.939518726149555
Loss in iteration 185 : 4.587832975419765
Loss in iteration 186 : 3.9191928102538696
Loss in iteration 187 : 4.588738381173485
Loss in iteration 188 : 3.929328907203869
Loss in iteration 189 : 4.592670370498437
Loss in iteration 190 : 3.922056958882966
Loss in iteration 191 : 4.58444068111851
Loss in iteration 192 : 3.918854013637841
Loss in iteration 193 : 4.582408429768928
Loss in iteration 194 : 3.9171067008551583
Loss in iteration 195 : 4.578853428226943
Loss in iteration 196 : 3.9207632230427443
Loss in iteration 197 : 4.589281717881202
Loss in iteration 198 : 3.9357007323899307
Loss in iteration 199 : 4.594718862462198
Loss in iteration 200 : 3.934217312940936
Testing accuracy  of updater 0 on alg 1 with rate 10.0 = 0.718875, training accuracy 0.727, time elapsed: 8813 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0004925100680415
Loss in iteration 3 : 1.8713901205952803
Loss in iteration 4 : 1.9119880475500086
Loss in iteration 5 : 1.2520403570475873
Loss in iteration 6 : 2.1852011019027135
Loss in iteration 7 : 0.8596258100324305
Loss in iteration 8 : 1.7721198456322778
Loss in iteration 9 : 1.1777353020709829
Loss in iteration 10 : 2.0225043278343557
Loss in iteration 11 : 0.8407361852396762
Loss in iteration 12 : 1.754172740998644
Loss in iteration 13 : 1.030388872004367
Loss in iteration 14 : 1.8113729182665674
Loss in iteration 15 : 0.9143429687935397
Loss in iteration 16 : 1.6387628355474018
Loss in iteration 17 : 1.0120698429925923
Loss in iteration 18 : 1.582446795181391
Loss in iteration 19 : 1.001307276508493
Loss in iteration 20 : 1.4928154306456027
Loss in iteration 21 : 1.0232831470054922
Loss in iteration 22 : 1.4030287236623307
Loss in iteration 23 : 1.0419446901612044
Loss in iteration 24 : 1.323526128898755
Loss in iteration 25 : 1.0572317357874668
Loss in iteration 26 : 1.2516400781261832
Loss in iteration 27 : 1.055513251602917
Loss in iteration 28 : 1.1905146889031708
Loss in iteration 29 : 1.0586073111438168
Loss in iteration 30 : 1.1319685329611735
Loss in iteration 31 : 1.055327134345227
Loss in iteration 32 : 1.090040830151232
Loss in iteration 33 : 1.0341038422327415
Loss in iteration 34 : 1.0412020236900492
Loss in iteration 35 : 1.0146848895526432
Loss in iteration 36 : 1.0082259266968676
Loss in iteration 37 : 0.9916609642996843
Loss in iteration 38 : 0.9844889532171096
Loss in iteration 39 : 0.9744100418874161
Loss in iteration 40 : 0.9554011049807685
Loss in iteration 41 : 0.9569852748824796
Loss in iteration 42 : 0.9302547767867451
Loss in iteration 43 : 0.9425594427049363
Loss in iteration 44 : 0.9067112724589906
Loss in iteration 45 : 0.9244971841935824
Loss in iteration 46 : 0.890305168857069
Loss in iteration 47 : 0.9149620110943999
Loss in iteration 48 : 0.877158254347853
Loss in iteration 49 : 0.9067780522155895
Loss in iteration 50 : 0.8665155702980942
Loss in iteration 51 : 0.8988476355321668
Loss in iteration 52 : 0.8536317534469678
Loss in iteration 53 : 0.8850842498281487
Loss in iteration 54 : 0.8416640309963002
Loss in iteration 55 : 0.874700134244194
Loss in iteration 56 : 0.827794990484818
Loss in iteration 57 : 0.8646725126013611
Loss in iteration 58 : 0.8164309906582419
Loss in iteration 59 : 0.8549030162814933
Loss in iteration 60 : 0.8105281440021285
Loss in iteration 61 : 0.8458975197082156
Loss in iteration 62 : 0.8040649216378509
Loss in iteration 63 : 0.8398390317995617
Loss in iteration 64 : 0.7959726482514509
Loss in iteration 65 : 0.8256270427714201
Loss in iteration 66 : 0.7907640190667657
Loss in iteration 67 : 0.8209671387219292
Loss in iteration 68 : 0.7849826418722684
Loss in iteration 69 : 0.8172470809114527
Loss in iteration 70 : 0.7800313961829543
Loss in iteration 71 : 0.8145541384555394
Loss in iteration 72 : 0.7783995552811322
Loss in iteration 73 : 0.8105655736506019
Loss in iteration 74 : 0.7728580630501738
Loss in iteration 75 : 0.8051225182339585
Loss in iteration 76 : 0.7709144497669121
Loss in iteration 77 : 0.8035499701355427
Loss in iteration 78 : 0.769752169025809
Loss in iteration 79 : 0.8007463192511923
Loss in iteration 80 : 0.7656337360407709
Loss in iteration 81 : 0.7965185382620404
Loss in iteration 82 : 0.7626827758014524
Loss in iteration 83 : 0.792840380975734
Loss in iteration 84 : 0.7606685316676514
Loss in iteration 85 : 0.7899732254253055
Loss in iteration 86 : 0.7574014665486013
Loss in iteration 87 : 0.7862671243833094
Loss in iteration 88 : 0.7521987114320718
Loss in iteration 89 : 0.7842953436666285
Loss in iteration 90 : 0.7497583637838838
Loss in iteration 91 : 0.7802639824746188
Loss in iteration 92 : 0.7486017065220694
Loss in iteration 93 : 0.780030998719143
Loss in iteration 94 : 0.7480103130259431
Loss in iteration 95 : 0.7789397197913215
Loss in iteration 96 : 0.7463601942328316
Loss in iteration 97 : 0.7774063951949857
Loss in iteration 98 : 0.7444168823093235
Loss in iteration 99 : 0.7764577424834836
Loss in iteration 100 : 0.7426586969745831
Loss in iteration 101 : 0.7733681043176125
Loss in iteration 102 : 0.7409923961586499
Loss in iteration 103 : 0.7729065430368871
Loss in iteration 104 : 0.738933493550683
Loss in iteration 105 : 0.7689185813397391
Loss in iteration 106 : 0.7396468843847195
Loss in iteration 107 : 0.7688079274373671
Loss in iteration 108 : 0.7381042597349491
Loss in iteration 109 : 0.7652765244856516
Loss in iteration 110 : 0.7390215697553927
Loss in iteration 111 : 0.7654445372668143
Loss in iteration 112 : 0.7368683722581499
Loss in iteration 113 : 0.7641890687922623
Loss in iteration 114 : 0.7367331642971631
Loss in iteration 115 : 0.7629764821717925
Loss in iteration 116 : 0.7357667126959684
Loss in iteration 117 : 0.7607690675216611
Loss in iteration 118 : 0.7358138788919262
Loss in iteration 119 : 0.7587459489517073
Loss in iteration 120 : 0.7355628413117281
Loss in iteration 121 : 0.7582659345991958
Loss in iteration 122 : 0.7342661035421384
Loss in iteration 123 : 0.7544348541934146
Loss in iteration 124 : 0.7320388147804051
Loss in iteration 125 : 0.7512644573671976
Loss in iteration 126 : 0.7307708629978611
Loss in iteration 127 : 0.7502417748217137
Loss in iteration 128 : 0.7296083744859045
Loss in iteration 129 : 0.7491611383661408
Loss in iteration 130 : 0.7255438709450348
Loss in iteration 131 : 0.7469924482815654
Loss in iteration 132 : 0.7252574210639605
Loss in iteration 133 : 0.7460889921502804
Loss in iteration 134 : 0.7257549275459663
Loss in iteration 135 : 0.7460877630605796
Loss in iteration 136 : 0.7255557953272596
Loss in iteration 137 : 0.7433481839564738
Loss in iteration 138 : 0.7238613749357967
Loss in iteration 139 : 0.7422950936049313
Loss in iteration 140 : 0.7229828328084343
Loss in iteration 141 : 0.7405962325639641
Loss in iteration 142 : 0.7221823234754002
Loss in iteration 143 : 0.741195306558644
Loss in iteration 144 : 0.72124371301299
Loss in iteration 145 : 0.7391476799589214
Loss in iteration 146 : 0.7207577372204428
Loss in iteration 147 : 0.739132393480869
Loss in iteration 148 : 0.7201701728448222
Loss in iteration 149 : 0.7386926693225642
Loss in iteration 150 : 0.7192851756710844
Loss in iteration 151 : 0.7374362033338269
Loss in iteration 152 : 0.7178497456597269
Loss in iteration 153 : 0.7340725044282141
Loss in iteration 154 : 0.717785224267388
Loss in iteration 155 : 0.7340066539127262
Loss in iteration 156 : 0.7162617368211517
Loss in iteration 157 : 0.7326715771368397
Loss in iteration 158 : 0.713269769108226
Loss in iteration 159 : 0.7307835346700957
Loss in iteration 160 : 0.7129857720834422
Loss in iteration 161 : 0.7292616320044787
Loss in iteration 162 : 0.7123627329320044
Loss in iteration 163 : 0.7286799522140941
Loss in iteration 164 : 0.7117084420316458
Loss in iteration 165 : 0.728453010745233
Loss in iteration 166 : 0.7120163005342507
Loss in iteration 167 : 0.7282370389362173
Loss in iteration 168 : 0.7121982709593776
Loss in iteration 169 : 0.7285113345069275
Loss in iteration 170 : 0.7117914121657156
Loss in iteration 171 : 0.7281394428436194
Loss in iteration 172 : 0.7123503559233629
Loss in iteration 173 : 0.7275154405643156
Loss in iteration 174 : 0.7115052603118829
Loss in iteration 175 : 0.7268022181908962
Loss in iteration 176 : 0.7096309504796691
Loss in iteration 177 : 0.7256181924832024
Loss in iteration 178 : 0.7092160408330579
Loss in iteration 179 : 0.7250597116042249
Loss in iteration 180 : 0.7087014807820452
Loss in iteration 181 : 0.7247232338795588
Loss in iteration 182 : 0.7084803051471172
Loss in iteration 183 : 0.7245551697149875
Loss in iteration 184 : 0.7079237988743846
Loss in iteration 185 : 0.7227068305544272
Loss in iteration 186 : 0.7089680565050387
Loss in iteration 187 : 0.7228883228742953
Loss in iteration 188 : 0.7087828984097446
Loss in iteration 189 : 0.7225038315850991
Loss in iteration 190 : 0.7085847504114572
Loss in iteration 191 : 0.7218285852594587
Loss in iteration 192 : 0.7093241970933906
Loss in iteration 193 : 0.7218143438270023
Loss in iteration 194 : 0.7096159658616411
Loss in iteration 195 : 0.721798306010899
Loss in iteration 196 : 0.7092242140181788
Loss in iteration 197 : 0.7210188220482028
Loss in iteration 198 : 0.7091317368755472
Loss in iteration 199 : 0.7206232795959229
Loss in iteration 200 : 0.7088359100970554
Testing accuracy  of updater 0 on alg 1 with rate 1.0 = 0.744125, training accuracy 0.752, time elapsed: 5675 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.976881816203008
Loss in iteration 3 : 0.9540353271130189
Loss in iteration 4 : 0.9344581318744214
Loss in iteration 5 : 0.9221497410136664
Loss in iteration 6 : 0.9143530761802388
Loss in iteration 7 : 0.9082381952924466
Loss in iteration 8 : 0.9026218690588094
Loss in iteration 9 : 0.8971800762964935
Loss in iteration 10 : 0.8917851439620087
Loss in iteration 11 : 0.8864230045702859
Loss in iteration 12 : 0.88109634892903
Loss in iteration 13 : 0.8758051471196118
Loss in iteration 14 : 0.8705334109506992
Loss in iteration 15 : 0.865280788548079
Loss in iteration 16 : 0.8600421729291213
Loss in iteration 17 : 0.854823364922182
Loss in iteration 18 : 0.8496257479882564
Loss in iteration 19 : 0.8444419733592461
Loss in iteration 20 : 0.8392667990722289
Loss in iteration 21 : 0.834109664422836
Loss in iteration 22 : 0.8289574393506801
Loss in iteration 23 : 0.8238127159923616
Loss in iteration 24 : 0.8186811650058066
Loss in iteration 25 : 0.8135548376893412
Loss in iteration 26 : 0.8084335701575626
Loss in iteration 27 : 0.803320886686303
Loss in iteration 28 : 0.7982216660665351
Loss in iteration 29 : 0.7931277302443953
Loss in iteration 30 : 0.7880391315578832
Loss in iteration 31 : 0.782954929559514
Loss in iteration 32 : 0.7778796071107059
Loss in iteration 33 : 0.7728117112657528
Loss in iteration 34 : 0.7677498742027626
Loss in iteration 35 : 0.7626969619725378
Loss in iteration 36 : 0.7576452340070586
Loss in iteration 37 : 0.7525944502048142
Loss in iteration 38 : 0.7475515560015034
Loss in iteration 39 : 0.7425395443519981
Loss in iteration 40 : 0.7375833282478741
Loss in iteration 41 : 0.7326778353343928
Loss in iteration 42 : 0.727854356934529
Loss in iteration 43 : 0.7231104001753051
Loss in iteration 44 : 0.7185275717167302
Loss in iteration 45 : 0.7141020116082806
Loss in iteration 46 : 0.7098355798282466
Loss in iteration 47 : 0.7057244290310137
Loss in iteration 48 : 0.7017284722007328
Loss in iteration 49 : 0.6978711137199117
Loss in iteration 50 : 0.6941418606970855
Loss in iteration 51 : 0.6905048517424704
Loss in iteration 52 : 0.6869937516422738
Loss in iteration 53 : 0.6836154290269818
Loss in iteration 54 : 0.6803796762048855
Loss in iteration 55 : 0.6772288071195333
Loss in iteration 56 : 0.6741642505388665
Loss in iteration 57 : 0.671224733713188
Loss in iteration 58 : 0.6683890578257632
Loss in iteration 59 : 0.6656646454200995
Loss in iteration 60 : 0.6630202227900445
Loss in iteration 61 : 0.6604474562241838
Loss in iteration 62 : 0.6579705425385021
Loss in iteration 63 : 0.6556048405159804
Loss in iteration 64 : 0.6533087785352083
Loss in iteration 65 : 0.6510666394916207
Loss in iteration 66 : 0.6488957642680073
Loss in iteration 67 : 0.6467899630334112
Loss in iteration 68 : 0.6447520701279329
Loss in iteration 69 : 0.6427742717960465
Loss in iteration 70 : 0.6408656923805018
Loss in iteration 71 : 0.6390324073271771
Loss in iteration 72 : 0.6372668349740596
Loss in iteration 73 : 0.635566089943357
Loss in iteration 74 : 0.6339232291830386
Loss in iteration 75 : 0.6323269190871124
Loss in iteration 76 : 0.6307808279922488
Loss in iteration 77 : 0.6292691265812249
Loss in iteration 78 : 0.6278096985531643
Loss in iteration 79 : 0.6263931802206946
Loss in iteration 80 : 0.625010289364214
Loss in iteration 81 : 0.6236607304122588
Loss in iteration 82 : 0.6223348983981568
Loss in iteration 83 : 0.6210496535826034
Loss in iteration 84 : 0.6197881130619779
Loss in iteration 85 : 0.6185554486912231
Loss in iteration 86 : 0.6173534137994836
Loss in iteration 87 : 0.6161906925620487
Loss in iteration 88 : 0.6150557722981612
Loss in iteration 89 : 0.6139459197216028
Loss in iteration 90 : 0.6128648432678294
Loss in iteration 91 : 0.6118055613920708
Loss in iteration 92 : 0.6107690968381484
Loss in iteration 93 : 0.6097575631501635
Loss in iteration 94 : 0.6087664638095618
Loss in iteration 95 : 0.6077895598116524
Loss in iteration 96 : 0.6068414488302794
Loss in iteration 97 : 0.6059175059536203
Loss in iteration 98 : 0.6050134690971443
Loss in iteration 99 : 0.6041213249735556
Loss in iteration 100 : 0.6032476192202547
Loss in iteration 101 : 0.602394188269129
Loss in iteration 102 : 0.6015541980526111
Loss in iteration 103 : 0.6007292268735476
Loss in iteration 104 : 0.5999213757054559
Loss in iteration 105 : 0.5991273903416613
Loss in iteration 106 : 0.5983529472777718
Loss in iteration 107 : 0.5976002157681316
Loss in iteration 108 : 0.5968618343453261
Loss in iteration 109 : 0.5961395588836699
Loss in iteration 110 : 0.5954332584134133
Loss in iteration 111 : 0.594741396173236
Loss in iteration 112 : 0.594062814137143
Loss in iteration 113 : 0.5933959656808235
Loss in iteration 114 : 0.592745198844615
Loss in iteration 115 : 0.5921017972571608
Loss in iteration 116 : 0.591470219227903
Loss in iteration 117 : 0.5908469220832122
Loss in iteration 118 : 0.5902346623988055
Loss in iteration 119 : 0.5896288756688193
Loss in iteration 120 : 0.5890309415278903
Loss in iteration 121 : 0.5884385173958556
Loss in iteration 122 : 0.5878501178690434
Loss in iteration 123 : 0.5872657016380114
Loss in iteration 124 : 0.5866904441041235
Loss in iteration 125 : 0.5861210277759517
Loss in iteration 126 : 0.5855580672800509
Loss in iteration 127 : 0.5850005428105156
Loss in iteration 128 : 0.584449768151603
Loss in iteration 129 : 0.5839086785084401
Loss in iteration 130 : 0.5833757297284778
Loss in iteration 131 : 0.5828536464309698
Loss in iteration 132 : 0.5823377883671274
Loss in iteration 133 : 0.5818275035365075
Loss in iteration 134 : 0.5813254380179335
Loss in iteration 135 : 0.5808266925158718
Loss in iteration 136 : 0.5803354579837913
Loss in iteration 137 : 0.579855052258014
Loss in iteration 138 : 0.579384264040464
Loss in iteration 139 : 0.5789213963604204
Loss in iteration 140 : 0.578461339432818
Loss in iteration 141 : 0.578006428368585
Loss in iteration 142 : 0.5775577675861145
Loss in iteration 143 : 0.5771154489502065
Loss in iteration 144 : 0.5766836342513532
Loss in iteration 145 : 0.5762550796307689
Loss in iteration 146 : 0.5758313884091636
Loss in iteration 147 : 0.5754123071326208
Loss in iteration 148 : 0.5749964275751158
Loss in iteration 149 : 0.5745824004267945
Loss in iteration 150 : 0.5741724170063303
Loss in iteration 151 : 0.5737641833053696
Loss in iteration 152 : 0.5733614980507671
Loss in iteration 153 : 0.5729646939763404
Loss in iteration 154 : 0.5725708318268393
Loss in iteration 155 : 0.5721821111010843
Loss in iteration 156 : 0.5717965387496619
Loss in iteration 157 : 0.5714151337766942
Loss in iteration 158 : 0.571035654572044
Loss in iteration 159 : 0.5706590610174801
Loss in iteration 160 : 0.5702859704304021
Loss in iteration 161 : 0.5699189782785123
Loss in iteration 162 : 0.5695591279320286
Loss in iteration 163 : 0.5692033588248764
Loss in iteration 164 : 0.568852513986146
Loss in iteration 165 : 0.5685051955112499
Loss in iteration 166 : 0.5681612904615734
Loss in iteration 167 : 0.5678228890298125
Loss in iteration 168 : 0.5674892425105553
Loss in iteration 169 : 0.5671598261841658
Loss in iteration 170 : 0.566832871973573
Loss in iteration 171 : 0.5665086672379424
Loss in iteration 172 : 0.5661882267158198
Loss in iteration 173 : 0.5658710031046773
Loss in iteration 174 : 0.5655581303415607
Loss in iteration 175 : 0.5652506228883595
Loss in iteration 176 : 0.5649455808335483
Loss in iteration 177 : 0.5646454772196811
Loss in iteration 178 : 0.5643483484485323
Loss in iteration 179 : 0.5640527271512826
Loss in iteration 180 : 0.5637598120321955
Loss in iteration 181 : 0.563471747505615
Loss in iteration 182 : 0.5631870259776178
Loss in iteration 183 : 0.5629066020969926
Loss in iteration 184 : 0.562630565819872
Loss in iteration 185 : 0.5623572095127584
Loss in iteration 186 : 0.5620858443576899
Loss in iteration 187 : 0.5618155878515385
Loss in iteration 188 : 0.5615468836052727
Loss in iteration 189 : 0.5612808029366281
Loss in iteration 190 : 0.5610158959352954
Loss in iteration 191 : 0.5607538627099131
Loss in iteration 192 : 0.5604938551616356
Loss in iteration 193 : 0.5602374878308594
Loss in iteration 194 : 0.5599861607145096
Loss in iteration 195 : 0.5597377303166586
Loss in iteration 196 : 0.5594914884703726
Loss in iteration 197 : 0.5592474750903991
Loss in iteration 198 : 0.5590067737584142
Loss in iteration 199 : 0.5587681573814878
Loss in iteration 200 : 0.5585317191084164
Testing accuracy  of updater 0 on alg 1 with rate 0.09999999999999998 = 0.77075, training accuracy 0.767, time elapsed: 5501 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.7637457863079735
Loss in iteration 3 : 15.192325455668692
Loss in iteration 4 : 2.6929239994843783
Loss in iteration 5 : 20.89676763382901
Loss in iteration 6 : 20.77063440192338
Loss in iteration 7 : 2.106060880186158
Loss in iteration 8 : 16.871060475937245
Loss in iteration 9 : 12.27287415940914
Loss in iteration 10 : 4.82730347104377
Loss in iteration 11 : 11.683156849039793
Loss in iteration 12 : 4.302199090496558
Loss in iteration 13 : 7.861934492625087
Loss in iteration 14 : 9.418995854610039
Loss in iteration 15 : 4.2400410371208475
Loss in iteration 16 : 7.685587630432155
Loss in iteration 17 : 7.4249816584603705
Loss in iteration 18 : 4.4031381343574285
Loss in iteration 19 : 7.30192410605331
Loss in iteration 20 : 6.464083078903441
Loss in iteration 21 : 4.354875461567414
Loss in iteration 22 : 6.722209838319623
Loss in iteration 23 : 5.38096438743282
Loss in iteration 24 : 4.573165513889946
Loss in iteration 25 : 6.025006308866337
Loss in iteration 26 : 4.757596189720532
Loss in iteration 27 : 4.359734642620248
Loss in iteration 28 : 5.37174620589969
Loss in iteration 29 : 3.8301961418425194
Loss in iteration 30 : 4.55254236474214
Loss in iteration 31 : 4.4421193403035835
Loss in iteration 32 : 3.457379699549796
Loss in iteration 33 : 4.298867443609007
Loss in iteration 34 : 3.2740785942442825
Loss in iteration 35 : 3.6818549255668573
Loss in iteration 36 : 3.3745076166739
Loss in iteration 37 : 3.070649746689444
Loss in iteration 38 : 3.2605517527058647
Loss in iteration 39 : 2.7337030061927248
Loss in iteration 40 : 3.01265446046496
Loss in iteration 41 : 2.4677317549655693
Loss in iteration 42 : 2.7526586428919004
Loss in iteration 43 : 2.3732020558323743
Loss in iteration 44 : 2.424493362733565
Loss in iteration 45 : 2.418755224359629
Loss in iteration 46 : 2.029453906107663
Loss in iteration 47 : 2.368044802040289
Loss in iteration 48 : 2.05536464229833
Loss in iteration 49 : 1.8660205381991282
Loss in iteration 50 : 2.119330618378924
Loss in iteration 51 : 2.2097816697092485
Loss in iteration 52 : 2.1036766617918112
Loss in iteration 53 : 1.8602884745385777
Loss in iteration 54 : 1.7284034126309589
Loss in iteration 55 : 1.671849884531015
Loss in iteration 56 : 1.7899298215992316
Loss in iteration 57 : 2.3575262865905025
Loss in iteration 58 : 3.976917000380753
Loss in iteration 59 : 3.086528630773133
Loss in iteration 60 : 2.286055152518592
Loss in iteration 61 : 1.7558669083556613
Loss in iteration 62 : 1.5824399173849448
Loss in iteration 63 : 1.5229186864776143
Loss in iteration 64 : 1.5909234754095656
Loss in iteration 65 : 1.732944348564559
Loss in iteration 66 : 2.4007623522038988
Loss in iteration 67 : 2.7122111643343234
Loss in iteration 68 : 3.480935401111573
Loss in iteration 69 : 1.871169400502077
Loss in iteration 70 : 1.6038480054782358
Loss in iteration 71 : 1.5124455013002454
Loss in iteration 72 : 1.9748340040010772
Loss in iteration 73 : 2.7489022630334548
Loss in iteration 74 : 4.324744334698547
Loss in iteration 75 : 1.9511460525232944
Loss in iteration 76 : 1.38324294242923
Loss in iteration 77 : 1.2490837284986287
Loss in iteration 78 : 1.1993018366501487
Loss in iteration 79 : 1.145102093229748
Loss in iteration 80 : 1.1570578264657225
Loss in iteration 81 : 1.8452656265109624
Loss in iteration 82 : 7.23886984894903
Loss in iteration 83 : 2.9709407269834935
Loss in iteration 84 : 2.998059532255133
Loss in iteration 85 : 3.97383152851622
Loss in iteration 86 : 4.866998944217206
Loss in iteration 87 : 1.5184132860093156
Loss in iteration 88 : 2.037965433915516
Loss in iteration 89 : 4.249214206645173
Loss in iteration 90 : 1.7890847519650097
Loss in iteration 91 : 2.0996565362035557
Loss in iteration 92 : 3.5311194188404693
Loss in iteration 93 : 1.6258704784650886
Loss in iteration 94 : 2.690588602759476
Loss in iteration 95 : 3.7154045328376943
Loss in iteration 96 : 1.5957126780964503
Loss in iteration 97 : 4.40637823280035
Loss in iteration 98 : 3.5454945022993325
Loss in iteration 99 : 2.0349732231250997
Loss in iteration 100 : 5.498940147454716
Loss in iteration 101 : 2.0515515643936015
Loss in iteration 102 : 2.8147489190969766
Loss in iteration 103 : 3.746634207596639
Loss in iteration 104 : 1.663664656211943
Loss in iteration 105 : 2.7275534179690086
Loss in iteration 106 : 2.6240508585546847
Loss in iteration 107 : 1.592228183736659
Loss in iteration 108 : 2.201394924378456
Loss in iteration 109 : 2.476458856321262
Loss in iteration 110 : 1.7804783681704794
Loss in iteration 111 : 1.3838225016679442
Loss in iteration 112 : 1.8550816447828178
Loss in iteration 113 : 3.1058045452297445
Loss in iteration 114 : 2.6546831838413754
Loss in iteration 115 : 2.4562379911570758
Loss in iteration 116 : 1.972354804609607
Loss in iteration 117 : 2.2453098607143427
Loss in iteration 118 : 2.560040034832701
Loss in iteration 119 : 3.0669794020788874
Loss in iteration 120 : 2.2379555266221627
Loss in iteration 121 : 1.7777406467599854
Loss in iteration 122 : 1.4917033733952854
Loss in iteration 123 : 1.3521697048726058
Loss in iteration 124 : 1.265413411748172
Loss in iteration 125 : 1.2026855298580599
Loss in iteration 126 : 1.153606550146997
Loss in iteration 127 : 1.168976995828292
Loss in iteration 128 : 1.7842335486490646
Loss in iteration 129 : 5.393986570872347
Loss in iteration 130 : 5.8428008534850795
Loss in iteration 131 : 1.2108223454137306
Loss in iteration 132 : 1.472716609692458
Loss in iteration 133 : 5.684729920781864
Loss in iteration 134 : 3.210576847230983
Loss in iteration 135 : 1.6681301734866172
Loss in iteration 136 : 1.3324848312654096
Loss in iteration 137 : 1.5812183491406904
Loss in iteration 138 : 2.198099183857141
Loss in iteration 139 : 2.1929514858378685
Loss in iteration 140 : 1.9970766787938288
Loss in iteration 141 : 1.5142635112961267
Loss in iteration 142 : 1.3464486337038457
Loss in iteration 143 : 1.2516982670574062
Loss in iteration 144 : 1.19087369229048
Loss in iteration 145 : 1.1324047796530234
Loss in iteration 146 : 1.0720122707376787
Loss in iteration 147 : 1.0786747097956413
Loss in iteration 148 : 2.4342148744949528
Loss in iteration 149 : 10.306194163411925
Loss in iteration 150 : 0.9692145581001437
Loss in iteration 151 : 10.823992707736753
Loss in iteration 152 : 1.0811188733265626
Loss in iteration 153 : 11.840723363078178
Loss in iteration 154 : 1.2163897084513986
Loss in iteration 155 : 11.495266301139575
Loss in iteration 156 : 2.507333624693679
Loss in iteration 157 : 15.644733213362663
Loss in iteration 158 : 9.79894887666484
Loss in iteration 159 : 9.627066616366866
Loss in iteration 160 : 11.204789987593443
Loss in iteration 161 : 3.4828751006461696
Loss in iteration 162 : 8.934432491751235
Loss in iteration 163 : 3.738870373443627
Loss in iteration 164 : 7.061438967731776
Loss in iteration 165 : 6.052004095234907
Loss in iteration 166 : 4.205559665131726
Loss in iteration 167 : 6.956458586996726
Loss in iteration 168 : 3.969473657310368
Loss in iteration 169 : 5.255855507369654
Loss in iteration 170 : 5.385085974659237
Loss in iteration 171 : 3.6827452968351344
Loss in iteration 172 : 5.429751894378371
Loss in iteration 173 : 3.972335416196867
Loss in iteration 174 : 4.068532608611462
Loss in iteration 175 : 4.4943634143927556
Loss in iteration 176 : 3.2917601832330665
Loss in iteration 177 : 4.301611088652888
Loss in iteration 178 : 3.2466927016562597
Loss in iteration 179 : 3.715476965165409
Loss in iteration 180 : 3.28851518358697
Loss in iteration 181 : 3.193010441683863
Loss in iteration 182 : 3.292382485670427
Loss in iteration 183 : 2.687507267611606
Loss in iteration 184 : 3.043940832534674
Loss in iteration 185 : 2.5633342194742332
Loss in iteration 186 : 2.687301104182065
Loss in iteration 187 : 2.4412791296339136
Loss in iteration 188 : 2.2476389985531804
Loss in iteration 189 : 2.5277206167919593
Loss in iteration 190 : 1.9068270074305702
Loss in iteration 191 : 2.415785692399336
Loss in iteration 192 : 2.2271476548480207
Loss in iteration 193 : 1.7244078265026492
Loss in iteration 194 : 2.1400765250173635
Loss in iteration 195 : 2.67329199666601
Loss in iteration 196 : 2.1656193280990284
Loss in iteration 197 : 1.7602190761767382
Loss in iteration 198 : 1.5551671059765586
Loss in iteration 199 : 1.46604287990826
Loss in iteration 200 : 1.404982539202144
Testing accuracy  of updater 1 on alg 1 with rate 10.0 = 0.762125, training accuracy 0.7545, time elapsed: 5618 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0004925100680415
Loss in iteration 3 : 1.2941204768357544
Loss in iteration 4 : 0.9869333759951523
Loss in iteration 5 : 0.6279215581784685
Loss in iteration 6 : 0.9669243396348883
Loss in iteration 7 : 0.7506677840090303
Loss in iteration 8 : 0.5807722048770447
Loss in iteration 9 : 0.80079120898707
Loss in iteration 10 : 0.5797757439926047
Loss in iteration 11 : 0.7613239690791646
Loss in iteration 12 : 0.5990487646239733
Loss in iteration 13 : 0.7215683046466114
Loss in iteration 14 : 0.6128323791985686
Loss in iteration 15 : 0.7020067403734149
Loss in iteration 16 : 0.6257380716081998
Loss in iteration 17 : 0.6734535018716433
Loss in iteration 18 : 0.6407173742476837
Loss in iteration 19 : 0.6391008925753726
Loss in iteration 20 : 0.6450098827148335
Loss in iteration 21 : 0.6116940975356475
Loss in iteration 22 : 0.6355327057543004
Loss in iteration 23 : 0.5915596420721986
Loss in iteration 24 : 0.6216931028532577
Loss in iteration 25 : 0.5780843959379723
Loss in iteration 26 : 0.6022288914744495
Loss in iteration 27 : 0.5638436930433156
Loss in iteration 28 : 0.583177973221154
Loss in iteration 29 : 0.5564606507872395
Loss in iteration 30 : 0.5621221414329337
Loss in iteration 31 : 0.5550539263016312
Loss in iteration 32 : 0.5407641891124879
Loss in iteration 33 : 0.5538893047661586
Loss in iteration 34 : 0.5309323559725353
Loss in iteration 35 : 0.5477819095212355
Loss in iteration 36 : 0.5424085326720218
Loss in iteration 37 : 0.5286521707939135
Loss in iteration 38 : 0.5507845839484271
Loss in iteration 39 : 0.5403750222870712
Loss in iteration 40 : 0.525618813866677
Loss in iteration 41 : 0.5427454663006522
Loss in iteration 42 : 0.5465949274197861
Loss in iteration 43 : 0.5262500365373608
Loss in iteration 44 : 0.5265489616269472
Loss in iteration 45 : 0.5402802587087517
Loss in iteration 46 : 0.5345808077033746
Loss in iteration 47 : 0.518947714469766
Loss in iteration 48 : 0.5196290041124025
Loss in iteration 49 : 0.5280865260535832
Loss in iteration 50 : 0.5233489003169282
Loss in iteration 51 : 0.5129999399401804
Loss in iteration 52 : 0.5158373111738019
Loss in iteration 53 : 0.5228165285562438
Loss in iteration 54 : 0.5165511570401122
Loss in iteration 55 : 0.5104449923545228
Loss in iteration 56 : 0.5106460729604464
Loss in iteration 57 : 0.5143668759754292
Loss in iteration 58 : 0.5160173236045666
Loss in iteration 59 : 0.5101039220951792
Loss in iteration 60 : 0.5092287714886349
Loss in iteration 61 : 0.5125556447434624
Loss in iteration 62 : 0.5117122291270024
Loss in iteration 63 : 0.5096033221871044
Loss in iteration 64 : 0.5080081037062807
Loss in iteration 65 : 0.508636783304695
Loss in iteration 66 : 0.5104960626474698
Loss in iteration 67 : 0.510879026501632
Loss in iteration 68 : 0.5105998819743242
Loss in iteration 69 : 0.5084032817627879
Loss in iteration 70 : 0.5072521432865742
Loss in iteration 71 : 0.5075260547233387
Loss in iteration 72 : 0.5084062243476072
Loss in iteration 73 : 0.509147714976146
Loss in iteration 74 : 0.5085963175756893
Loss in iteration 75 : 0.5079420091868232
Loss in iteration 76 : 0.5073018892374094
Loss in iteration 77 : 0.5072093595726443
Loss in iteration 78 : 0.507655887882906
Loss in iteration 79 : 0.5081630069990708
Loss in iteration 80 : 0.5092139040307132
Loss in iteration 81 : 0.5087033809755792
Loss in iteration 82 : 0.508532085078392
Loss in iteration 83 : 0.507314109721098
Loss in iteration 84 : 0.5070850402550414
Loss in iteration 85 : 0.5079273310211038
Loss in iteration 86 : 0.5082591644842733
Loss in iteration 87 : 0.5086062463960392
Loss in iteration 88 : 0.5075710963596113
Loss in iteration 89 : 0.507049450856526
Loss in iteration 90 : 0.5067927400496992
Loss in iteration 91 : 0.506874223669715
Loss in iteration 92 : 0.5070502949566913
Loss in iteration 93 : 0.507188889802531
Loss in iteration 94 : 0.507603889613731
Loss in iteration 95 : 0.5075802583899349
Loss in iteration 96 : 0.5072597278169395
Loss in iteration 97 : 0.5067189755440775
Loss in iteration 98 : 0.506774975385833
Loss in iteration 99 : 0.5073323963149995
Loss in iteration 100 : 0.5078930635218597
Loss in iteration 101 : 0.5080272574496011
Loss in iteration 102 : 0.5073975459802429
Loss in iteration 103 : 0.5068968376518495
Loss in iteration 104 : 0.5065987579341747
Loss in iteration 105 : 0.5067942045855243
Loss in iteration 106 : 0.507309443656994
Loss in iteration 107 : 0.5076270600754297
Loss in iteration 108 : 0.5074608772724924
Loss in iteration 109 : 0.5068912171888923
Loss in iteration 110 : 0.5065821209886998
Loss in iteration 111 : 0.5069016771597012
Loss in iteration 112 : 0.5071770765341479
Loss in iteration 113 : 0.5074094437682111
Loss in iteration 114 : 0.5071950829491869
Loss in iteration 115 : 0.5068737336064637
Loss in iteration 116 : 0.5065667926509024
Loss in iteration 117 : 0.5065079898196476
Loss in iteration 118 : 0.5066815473952303
Loss in iteration 119 : 0.5068127311727966
Loss in iteration 120 : 0.506791069943819
Loss in iteration 121 : 0.5065934847624014
Loss in iteration 122 : 0.506474670562981
Loss in iteration 123 : 0.5064905789743274
Loss in iteration 124 : 0.5065372455325201
Loss in iteration 125 : 0.5065223851103634
Loss in iteration 126 : 0.5064654117578865
Loss in iteration 127 : 0.506431094339356
Loss in iteration 128 : 0.5064816300001824
Loss in iteration 129 : 0.5065736463436304
Loss in iteration 130 : 0.5066024471216517
Loss in iteration 131 : 0.5066028499756152
Loss in iteration 132 : 0.5065189260666949
Loss in iteration 133 : 0.5064131568478372
Loss in iteration 134 : 0.5064188241176903
Loss in iteration 135 : 0.5065625076068785
Loss in iteration 136 : 0.5067104507148472
Loss in iteration 137 : 0.5066922799775285
Loss in iteration 138 : 0.5065613946208984
Loss in iteration 139 : 0.5064156277244121
Loss in iteration 140 : 0.5063900422841597
Loss in iteration 141 : 0.5064207643124754
Loss in iteration 142 : 0.5064708104002221
Loss in iteration 143 : 0.5064873507836267
Loss in iteration 144 : 0.5065210700562855
Loss in iteration 145 : 0.5065118844236355
Loss in iteration 146 : 0.5064747319661196
Loss in iteration 147 : 0.5063646894957289
Loss in iteration 148 : 0.506466401006958
Loss in iteration 149 : 0.506865532134009
Loss in iteration 150 : 0.5073085012377629
Loss in iteration 151 : 0.5076965170496895
Loss in iteration 152 : 0.507013625291557
Loss in iteration 153 : 0.5065066718826103
Loss in iteration 154 : 0.5064272018827987
Loss in iteration 155 : 0.5068306656111295
Loss in iteration 156 : 0.5072833767188879
Loss in iteration 157 : 0.5071297973607936
Loss in iteration 158 : 0.5068109714984527
Loss in iteration 159 : 0.50645209476562
Loss in iteration 160 : 0.5065010878866912
Loss in iteration 161 : 0.5070344418411347
Loss in iteration 162 : 0.5073191616618453
Loss in iteration 163 : 0.5071845210008296
Loss in iteration 164 : 0.5066377422350575
Loss in iteration 165 : 0.5063813750006521
Loss in iteration 166 : 0.5064889074869815
Loss in iteration 167 : 0.5068023208876073
Loss in iteration 168 : 0.5070199730738517
Loss in iteration 169 : 0.5068256374635011
Loss in iteration 170 : 0.5065513151437704
Loss in iteration 171 : 0.5063422967775874
Loss in iteration 172 : 0.5063877416312308
Loss in iteration 173 : 0.5066355298386688
Loss in iteration 174 : 0.5069756625988244
Loss in iteration 175 : 0.5070560642861912
Loss in iteration 176 : 0.5066486933360962
Loss in iteration 177 : 0.5063847854966292
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.788875, training accuracy 0.7825, time elapsed: 4359 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.976881816203008
Loss in iteration 3 : 0.9356914387655066
Loss in iteration 4 : 0.9118945900533192
Loss in iteration 5 : 0.913758314477594
Loss in iteration 6 : 0.8870448132840141
Loss in iteration 7 : 0.8429505789818151
Loss in iteration 8 : 0.8069669450114523
Loss in iteration 9 : 0.7779934522664059
Loss in iteration 10 : 0.7538595475209127
Loss in iteration 11 : 0.7383310836441096
Loss in iteration 12 : 0.7035617414134796
Loss in iteration 13 : 0.6618248637977694
Loss in iteration 14 : 0.6406558394247837
Loss in iteration 15 : 0.6316318372492505
Loss in iteration 16 : 0.6171921963174084
Loss in iteration 17 : 0.5970656732583002
Loss in iteration 18 : 0.5840599181380626
Loss in iteration 19 : 0.5816541615321487
Loss in iteration 20 : 0.579813591709692
Loss in iteration 21 : 0.5717031420861949
Loss in iteration 22 : 0.5633752403047252
Loss in iteration 23 : 0.561086972292831
Loss in iteration 24 : 0.5617294982070872
Loss in iteration 25 : 0.5595358890657881
Loss in iteration 26 : 0.5545461156644516
Loss in iteration 27 : 0.550616691004272
Loss in iteration 28 : 0.5500289237521467
Loss in iteration 29 : 0.5500566778729477
Loss in iteration 30 : 0.5481488047672374
Loss in iteration 31 : 0.5449878226399112
Loss in iteration 32 : 0.5428543627859019
Loss in iteration 33 : 0.5425921156233078
Loss in iteration 34 : 0.5424061035815967
Loss in iteration 35 : 0.5409248306601873
Loss in iteration 36 : 0.5388872365484635
Loss in iteration 37 : 0.5378430468893448
Loss in iteration 38 : 0.5376280551586715
Loss in iteration 39 : 0.537236028398642
Loss in iteration 40 : 0.5360200029800762
Loss in iteration 41 : 0.5346109733948733
Loss in iteration 42 : 0.5338398653953543
Loss in iteration 43 : 0.5335936315334537
Loss in iteration 44 : 0.5330940146361557
Loss in iteration 45 : 0.5321101215152494
Loss in iteration 46 : 0.5311355842255249
Loss in iteration 47 : 0.5306263181185144
Loss in iteration 48 : 0.5304025871888743
Loss in iteration 49 : 0.5298591391006245
Loss in iteration 50 : 0.5290228827269586
Loss in iteration 51 : 0.5284254692534928
Loss in iteration 52 : 0.5280927855142997
Loss in iteration 53 : 0.5277085918039941
Loss in iteration 54 : 0.5270999818608841
Loss in iteration 55 : 0.5264641262675765
Loss in iteration 56 : 0.5259901996146206
Loss in iteration 57 : 0.5256328547693905
Loss in iteration 58 : 0.5251853053862838
Loss in iteration 59 : 0.5246363087431132
Loss in iteration 60 : 0.5241641720334892
Loss in iteration 61 : 0.5238278413828216
Loss in iteration 62 : 0.5234912060010786
Loss in iteration 63 : 0.5230791411973098
Loss in iteration 64 : 0.5226689013868611
Loss in iteration 65 : 0.52233287885761
Loss in iteration 66 : 0.5220444162193921
Loss in iteration 67 : 0.5217247447340495
Loss in iteration 68 : 0.5213758728750941
Loss in iteration 69 : 0.521044918799071
Loss in iteration 70 : 0.5207569364242894
Loss in iteration 71 : 0.5204934227913238
Loss in iteration 72 : 0.5202114988026854
Loss in iteration 73 : 0.5199185473974526
Loss in iteration 74 : 0.5196558186208108
Loss in iteration 75 : 0.5194263913757095
Loss in iteration 76 : 0.5192056997823388
Loss in iteration 77 : 0.5189767232641759
Loss in iteration 78 : 0.5187489010466199
Loss in iteration 79 : 0.5185279597002427
Loss in iteration 80 : 0.5183285564975517
Loss in iteration 81 : 0.5181414708771677
Loss in iteration 82 : 0.5179484290408806
Loss in iteration 83 : 0.5177514612005635
Loss in iteration 84 : 0.5175636245621219
Loss in iteration 85 : 0.5173900629331267
Loss in iteration 86 : 0.5172207704329213
Loss in iteration 87 : 0.5170522486004128
Loss in iteration 88 : 0.5168857317618961
Loss in iteration 89 : 0.516730152024403
Loss in iteration 90 : 0.5165802634470976
Loss in iteration 91 : 0.5164316938747536
Loss in iteration 92 : 0.5162869447497833
Loss in iteration 93 : 0.5161476630115578
Loss in iteration 94 : 0.5160139611916218
Loss in iteration 95 : 0.515881266374929
Loss in iteration 96 : 0.5157522341064699
Loss in iteration 97 : 0.5156282845463818
Loss in iteration 98 : 0.5155066290880965
Loss in iteration 99 : 0.5153880852851007
Loss in iteration 100 : 0.5152727041671321
Loss in iteration 101 : 0.5151591522951883
Loss in iteration 102 : 0.5150480154956075
Loss in iteration 103 : 0.5149388938285981
Loss in iteration 104 : 0.5148318408623977
Loss in iteration 105 : 0.5147278278915917
Loss in iteration 106 : 0.5146269827971114
Loss in iteration 107 : 0.5145279504063904
Loss in iteration 108 : 0.5144299691796375
Loss in iteration 109 : 0.5143344821589194
Loss in iteration 110 : 0.5142409068327458
Loss in iteration 111 : 0.5141488256537434
Loss in iteration 112 : 0.5140586358855695
Loss in iteration 113 : 0.5139705551439765
Loss in iteration 114 : 0.5138851376669683
Loss in iteration 115 : 0.5138020826615147
Loss in iteration 116 : 0.5137214074037922
Loss in iteration 117 : 0.5136427997467613
Loss in iteration 118 : 0.5135653231987396
Loss in iteration 119 : 0.5134896571216022
Loss in iteration 120 : 0.5134153236654053
Loss in iteration 121 : 0.5133417392161334
Loss in iteration 122 : 0.5132689812616517
Loss in iteration 123 : 0.5131975423806407
Loss in iteration 124 : 0.5131272297795271
Loss in iteration 125 : 0.5130583414601958
Loss in iteration 126 : 0.5129902493149718
Loss in iteration 127 : 0.5129233980628936
Loss in iteration 128 : 0.5128574485353694
Loss in iteration 129 : 0.5127925384387861
Loss in iteration 130 : 0.5127279787547174
Loss in iteration 131 : 0.5126643248452979
Loss in iteration 132 : 0.5126022477673436
Loss in iteration 133 : 0.5125407886648996
Loss in iteration 134 : 0.5124806698107043
Loss in iteration 135 : 0.5124218557527768
Loss in iteration 136 : 0.5123645700880354
Loss in iteration 137 : 0.5123086180863088
Loss in iteration 138 : 0.5122534931524256
Loss in iteration 139 : 0.5121997278821795
Loss in iteration 140 : 0.5121475734698833
Loss in iteration 141 : 0.512096045948283
Loss in iteration 142 : 0.5120455452958926
Loss in iteration 143 : 0.511996364001815
Loss in iteration 144 : 0.5119476996184432
Loss in iteration 145 : 0.511899988661466
Loss in iteration 146 : 0.5118530805534997
Loss in iteration 147 : 0.511807210408652
Loss in iteration 148 : 0.5117623715376299
Loss in iteration 149 : 0.5117181394850719
Loss in iteration 150 : 0.5116744444167967
Loss in iteration 151 : 0.5116311401379959
Loss in iteration 152 : 0.5115893230926534
Loss in iteration 153 : 0.5115490116619426
Loss in iteration 154 : 0.5115098419634111
Loss in iteration 155 : 0.5114718652234452
Loss in iteration 156 : 0.5114346690057592
Loss in iteration 157 : 0.5113976931252473
Loss in iteration 158 : 0.511361440283057
Loss in iteration 159 : 0.5113265085397333
Loss in iteration 160 : 0.5112920422147635
Loss in iteration 161 : 0.5112580364476336
Loss in iteration 162 : 0.5112249815228986
Loss in iteration 163 : 0.5111926290925302
Loss in iteration 164 : 0.5111608248312969
Loss in iteration 165 : 0.5111292346707927
Loss in iteration 166 : 0.5110981254517781
Loss in iteration 167 : 0.511068687321561
Loss in iteration 168 : 0.511040222160672
Loss in iteration 169 : 0.5110117690614436
Loss in iteration 170 : 0.5109841201322194
Loss in iteration 171 : 0.5109570071405124
Loss in iteration 172 : 0.5109300788876554
Loss in iteration 173 : 0.5109034570448473
Loss in iteration 174 : 0.5108769792660861
Loss in iteration 175 : 0.5108506412190572
Loss in iteration 176 : 0.510824619533084
Loss in iteration 177 : 0.510798706750691
Loss in iteration 178 : 0.5107728682503521
Loss in iteration 179 : 0.5107472270844599
Loss in iteration 180 : 0.5107221209434535
Loss in iteration 181 : 0.5106972508596846
Loss in iteration 182 : 0.5106725229620911
Loss in iteration 183 : 0.5106484382644443
Loss in iteration 184 : 0.510624930681474
Loss in iteration 185 : 0.5106019416315282
Loss in iteration 186 : 0.5105793725652108
Loss in iteration 187 : 0.5105567415951761
Loss in iteration 188 : 0.5105347740823052
Loss in iteration 189 : 0.510513581205505
Loss in iteration 190 : 0.510492316951238
Loss in iteration 191 : 0.510471462638957
Loss in iteration 192 : 0.5104508203077848
Loss in iteration 193 : 0.5104301502043035
Loss in iteration 194 : 0.5104097217125991
Loss in iteration 195 : 0.5103894860284999
Loss in iteration 196 : 0.5103695804694949
Loss in iteration 197 : 0.510349881301142
Loss in iteration 198 : 0.5103304099765965
Loss in iteration 199 : 0.5103111835235554
Loss in iteration 200 : 0.5102920331198268
Testing accuracy  of updater 1 on alg 1 with rate 0.09999999999999998 = 0.7875, training accuracy 0.781, time elapsed: 4336 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 8.564666993985131
Loss in iteration 3 : 34.3333814203782
Loss in iteration 4 : 4.675319581639311
Loss in iteration 5 : 22.95346769180393
Loss in iteration 6 : 12.961440335483166
Loss in iteration 7 : 9.647490828672863
Loss in iteration 8 : 10.751406546504128
Loss in iteration 9 : 9.555753878871467
Loss in iteration 10 : 5.625458640322837
Loss in iteration 11 : 5.360597582341721
Loss in iteration 12 : 4.733616280922366
Loss in iteration 13 : 4.579349251092106
Loss in iteration 14 : 4.3417756514577155
Loss in iteration 15 : 4.271199082621461
Loss in iteration 16 : 4.219333724372291
Loss in iteration 17 : 4.178292215404045
Loss in iteration 18 : 4.117333531076332
Loss in iteration 19 : 4.033339444146747
Loss in iteration 20 : 3.937244605103238
Loss in iteration 21 : 3.8299868930183734
Loss in iteration 22 : 3.7155963667657237
Loss in iteration 23 : 3.5930746813132344
Loss in iteration 24 : 3.4617693338312048
Loss in iteration 25 : 3.324493154547905
Loss in iteration 26 : 3.182940610949715
Loss in iteration 27 : 3.041212380012097
Loss in iteration 28 : 2.904819881709289
Loss in iteration 29 : 2.7936501573113177
Loss in iteration 30 : 2.752456647228205
Loss in iteration 31 : 3.0253581654595623
Loss in iteration 32 : 4.501382875350706
Loss in iteration 33 : 10.275495316780647
Loss in iteration 34 : 8.933854090581281
Loss in iteration 35 : 11.422724318824724
Loss in iteration 36 : 8.35835980804362
Loss in iteration 37 : 10.369866298604705
Loss in iteration 38 : 8.157048997875922
Loss in iteration 39 : 8.740367728602926
Loss in iteration 40 : 7.356908734100553
Loss in iteration 41 : 7.3653320239796525
Loss in iteration 42 : 6.381863119179957
Loss in iteration 43 : 6.24744739586346
Loss in iteration 44 : 5.558873853708959
Loss in iteration 45 : 5.478567134885128
Loss in iteration 46 : 4.9920193991256285
Loss in iteration 47 : 5.074329619644611
Loss in iteration 48 : 4.890390829514635
Loss in iteration 49 : 5.135074523009483
Loss in iteration 50 : 4.8242989776153005
Loss in iteration 51 : 5.229855788897721
Loss in iteration 52 : 4.837597994665723
Loss in iteration 53 : 5.524062792634054
Loss in iteration 54 : 5.008089971636971
Loss in iteration 55 : 5.949635315938989
Loss in iteration 56 : 5.308502309621947
Loss in iteration 57 : 6.536130915087352
Loss in iteration 58 : 5.755417674157725
Loss in iteration 59 : 6.934088427095316
Loss in iteration 60 : 5.700820673140869
Loss in iteration 61 : 6.803160548799965
Loss in iteration 62 : 5.686224452070626
Loss in iteration 63 : 6.4486727022705015
Loss in iteration 64 : 5.5735628379910525
Loss in iteration 65 : 6.09243772498076
Loss in iteration 66 : 5.3306343933813185
Loss in iteration 67 : 5.7771484915662095
Loss in iteration 68 : 5.184449071873613
Loss in iteration 69 : 5.611140240571332
Loss in iteration 70 : 5.071702899641793
Loss in iteration 71 : 5.626310935547047
Loss in iteration 72 : 5.174513351700938
Loss in iteration 73 : 5.855915267361339
Loss in iteration 74 : 5.442165917535229
Loss in iteration 75 : 6.1005241162307575
Loss in iteration 76 : 5.525985999252462
Loss in iteration 77 : 6.238603092004397
Loss in iteration 78 : 5.641743793679616
Loss in iteration 79 : 6.390798293246624
Loss in iteration 80 : 5.636981310594733
Loss in iteration 81 : 6.277679077034399
Loss in iteration 82 : 5.617634844736032
Loss in iteration 83 : 6.2126280580108935
Loss in iteration 84 : 5.500129880360948
Loss in iteration 85 : 6.096194043327063
Loss in iteration 86 : 5.4775825900926876
Loss in iteration 87 : 6.089225680785721
Loss in iteration 88 : 5.4775356078507444
Loss in iteration 89 : 6.049938212798779
Loss in iteration 90 : 5.424624398566004
Loss in iteration 91 : 6.027714827031473
Loss in iteration 92 : 5.399603141739552
Loss in iteration 93 : 6.011976174341561
Loss in iteration 94 : 5.4236989156953515
Loss in iteration 95 : 6.049960484782728
Loss in iteration 96 : 5.428592027105092
Loss in iteration 97 : 6.056752094350627
Loss in iteration 98 : 5.470473231178453
Loss in iteration 99 : 6.100932394464898
Loss in iteration 100 : 5.462703354380048
Loss in iteration 101 : 6.058933425394791
Loss in iteration 102 : 5.463821103926831
Loss in iteration 103 : 6.061640069969781
Loss in iteration 104 : 5.48626137073135
Loss in iteration 105 : 6.105615346630857
Loss in iteration 106 : 5.508588205125216
Loss in iteration 107 : 6.115802675872158
Loss in iteration 108 : 5.501167047080609
Loss in iteration 109 : 6.081635319901741
Loss in iteration 110 : 5.505787934668404
Loss in iteration 111 : 6.085718348325197
Loss in iteration 112 : 5.509262596074585
Loss in iteration 113 : 6.067307625675303
Loss in iteration 114 : 5.478460462293291
Loss in iteration 115 : 6.0627647213515425
Loss in iteration 116 : 5.497779218420282
Loss in iteration 117 : 6.060540942995401
Loss in iteration 118 : 5.493342193257369
Loss in iteration 119 : 6.0724582097638695
Loss in iteration 120 : 5.4944294955882595
Loss in iteration 121 : 6.084245837200607
Loss in iteration 122 : 5.494667741110768
Loss in iteration 123 : 6.07200472852143
Loss in iteration 124 : 5.497790292819622
Loss in iteration 125 : 6.073384644544713
Loss in iteration 126 : 5.489798773965968
Loss in iteration 127 : 6.060795852804225
Loss in iteration 128 : 5.502231244807697
Loss in iteration 129 : 6.07157719738749
Loss in iteration 130 : 5.488017305987152
Loss in iteration 131 : 6.053920492325484
Loss in iteration 132 : 5.475906594609576
Loss in iteration 133 : 6.037582281024243
Loss in iteration 134 : 5.44260603447792
Loss in iteration 135 : 6.013920410612313
Loss in iteration 136 : 5.458154643276867
Loss in iteration 137 : 6.066357508588373
Loss in iteration 138 : 5.518293295727933
Loss in iteration 139 : 6.085163399663215
Loss in iteration 140 : 5.509663953957521
Loss in iteration 141 : 6.089175519555021
Loss in iteration 142 : 5.496408485286713
Loss in iteration 143 : 6.067313181459438
Loss in iteration 144 : 5.492483499744022
Loss in iteration 145 : 6.0472198025764685
Loss in iteration 146 : 5.444406401253751
Loss in iteration 147 : 6.034277343838952
Loss in iteration 148 : 5.496722716868189
Loss in iteration 149 : 6.064499249486935
Loss in iteration 150 : 5.46363757049185
Loss in iteration 151 : 6.053928312989239
Loss in iteration 152 : 5.479451227817186
Loss in iteration 153 : 6.034430504049214
Loss in iteration 154 : 5.434322914577797
Loss in iteration 155 : 6.0346266527396715
Loss in iteration 156 : 5.503610568883477
Loss in iteration 157 : 6.063147721751247
Loss in iteration 158 : 5.477523108838098
Loss in iteration 159 : 6.04925817735656
Loss in iteration 160 : 5.435542796442096
Loss in iteration 161 : 6.033462957240361
Loss in iteration 162 : 5.460375574084287
Loss in iteration 163 : 6.060097472640269
Loss in iteration 164 : 5.505462797661603
Loss in iteration 165 : 6.067058203917955
Loss in iteration 166 : 5.517512048868055
Loss in iteration 167 : 6.05999718757912
Loss in iteration 168 : 5.4401076438442635
Loss in iteration 169 : 6.011433747742296
Loss in iteration 170 : 5.454495033702292
Loss in iteration 171 : 6.022927627022019
Loss in iteration 172 : 5.464179924616257
Loss in iteration 173 : 6.040094737965845
Loss in iteration 174 : 5.5062889968123745
Loss in iteration 175 : 6.064185372512791
Loss in iteration 176 : 5.484089812513756
Loss in iteration 177 : 6.025597621690895
Loss in iteration 178 : 5.466910037090344
Loss in iteration 179 : 6.025755485490119
Loss in iteration 180 : 5.4671284329275585
Loss in iteration 181 : 6.027228706956526
Loss in iteration 182 : 5.460637336808949
Loss in iteration 183 : 6.027698409027238
Loss in iteration 184 : 5.475894798530335
Loss in iteration 185 : 6.0685979460113995
Loss in iteration 186 : 5.493790793969661
Loss in iteration 187 : 6.062867005431866
Loss in iteration 188 : 5.475554203842092
Loss in iteration 189 : 6.015710853738764
Loss in iteration 190 : 5.47058415308986
Loss in iteration 191 : 6.043107142433791
Loss in iteration 192 : 5.446660943254439
Loss in iteration 193 : 6.012794647348879
Loss in iteration 194 : 5.464334861130845
Loss in iteration 195 : 6.040052633909656
Loss in iteration 196 : 5.476394628454421
Loss in iteration 197 : 6.054836794819143
Loss in iteration 198 : 5.481713846548257
Loss in iteration 199 : 6.044773673508349
Loss in iteration 200 : 5.458466750090149
Testing accuracy  of updater 2 on alg 1 with rate 10.0 = 0.782625, training accuracy 0.789, time elapsed: 4364 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3464210846018327
Loss in iteration 3 : 3.73545723363827
Loss in iteration 4 : 1.1015662955488137
Loss in iteration 5 : 2.4103402991671796
Loss in iteration 6 : 2.0750304424941897
Loss in iteration 7 : 1.0367438584063136
Loss in iteration 8 : 1.506062757812067
Loss in iteration 9 : 1.5129532821095226
Loss in iteration 10 : 0.962946016049446
Loss in iteration 11 : 1.0419816040657284
Loss in iteration 12 : 0.9077084580175715
Loss in iteration 13 : 0.8789252821355418
Loss in iteration 14 : 0.8062029833960266
Loss in iteration 15 : 0.7790282617310742
Loss in iteration 16 : 0.7490426125583877
Loss in iteration 17 : 0.7363790230028867
Loss in iteration 18 : 0.7298830657895318
Loss in iteration 19 : 0.7227081670322479
Loss in iteration 20 : 0.7150003070837585
Loss in iteration 21 : 0.7075906651297396
Loss in iteration 22 : 0.6991166036476597
Loss in iteration 23 : 0.6898936258726432
Loss in iteration 24 : 0.6800997132850607
Loss in iteration 25 : 0.6696282618429329
Loss in iteration 26 : 0.658556867887071
Loss in iteration 27 : 0.6469302589344407
Loss in iteration 28 : 0.6350865495863613
Loss in iteration 29 : 0.6235430390655182
Loss in iteration 30 : 0.6122647811939546
Loss in iteration 31 : 0.6017724398501557
Loss in iteration 32 : 0.5925039168776313
Loss in iteration 33 : 0.5861162244422093
Loss in iteration 34 : 0.5877844087561037
Loss in iteration 35 : 0.6320191282174369
Loss in iteration 36 : 0.8461005593770822
Loss in iteration 37 : 1.2977153549721927
Loss in iteration 38 : 1.1561055718633055
Loss in iteration 39 : 1.2930330058239348
Loss in iteration 40 : 1.1333441035432883
Loss in iteration 41 : 1.2462246835013138
Loss in iteration 42 : 1.0734983142441474
Loss in iteration 43 : 1.130882013213547
Loss in iteration 44 : 1.0483396424750957
Loss in iteration 45 : 1.0291064813087447
Loss in iteration 46 : 0.9498052420240642
Loss in iteration 47 : 0.9440975140625927
Loss in iteration 48 : 0.894406650322532
Loss in iteration 49 : 0.888782851804398
Loss in iteration 50 : 0.8478106301688815
Loss in iteration 51 : 0.8403980569769626
Loss in iteration 52 : 0.8183180555001275
Loss in iteration 53 : 0.8176232589709086
Loss in iteration 54 : 0.7929841611424492
Loss in iteration 55 : 0.8045955543442945
Loss in iteration 56 : 0.7965702562400802
Loss in iteration 57 : 0.8288591733202163
Loss in iteration 58 : 0.7957538403582748
Loss in iteration 59 : 0.840497605588017
Loss in iteration 60 : 0.8005865589918885
Loss in iteration 61 : 0.8574803251408118
Loss in iteration 62 : 0.8117099867770111
Loss in iteration 63 : 0.8713698266169511
Loss in iteration 64 : 0.8164380039629165
Loss in iteration 65 : 0.8817951722053601
Loss in iteration 66 : 0.8260197077037417
Loss in iteration 67 : 0.8874642451515884
Loss in iteration 68 : 0.8276313662370055
Loss in iteration 69 : 0.88058556460988
Loss in iteration 70 : 0.8281406283232507
Loss in iteration 71 : 0.8710511999581984
Loss in iteration 72 : 0.8229715088554267
Loss in iteration 73 : 0.8586018459527087
Loss in iteration 74 : 0.8232193714405602
Loss in iteration 75 : 0.8599308838807481
Loss in iteration 76 : 0.8246491367985557
Loss in iteration 77 : 0.861629426993466
Loss in iteration 78 : 0.8250096829567262
Loss in iteration 79 : 0.8691968705340333
Loss in iteration 80 : 0.8367290539265937
Loss in iteration 81 : 0.877976457663047
Loss in iteration 82 : 0.842393375094076
Loss in iteration 83 : 0.8851550279855209
Loss in iteration 84 : 0.8486560210788591
Loss in iteration 85 : 0.8865895559226905
Loss in iteration 86 : 0.846340291032814
Loss in iteration 87 : 0.8844419408487949
Loss in iteration 88 : 0.8456767535903363
Loss in iteration 89 : 0.880697482174486
Loss in iteration 90 : 0.8366672180586678
Loss in iteration 91 : 0.8709596694703596
Loss in iteration 92 : 0.8370202431684943
Loss in iteration 93 : 0.8738118571760999
Loss in iteration 94 : 0.8352332588096781
Loss in iteration 95 : 0.8703157747006581
Loss in iteration 96 : 0.8282785761161765
Loss in iteration 97 : 0.8694486040939701
Loss in iteration 98 : 0.829431161512136
Loss in iteration 99 : 0.8688021534683666
Loss in iteration 100 : 0.8278649716849465
Loss in iteration 101 : 0.8698639998474696
Loss in iteration 102 : 0.8307467309680572
Loss in iteration 103 : 0.8712890702763291
Loss in iteration 104 : 0.8298883450297425
Loss in iteration 105 : 0.8713960895246892
Loss in iteration 106 : 0.8298121686829127
Loss in iteration 107 : 0.8718198964549067
Loss in iteration 108 : 0.832688429651201
Loss in iteration 109 : 0.8744753479483404
Loss in iteration 110 : 0.8353587701827039
Loss in iteration 111 : 0.8759256478932355
Loss in iteration 112 : 0.8349514571128022
Loss in iteration 113 : 0.8746665579571761
Loss in iteration 114 : 0.832716460386563
Loss in iteration 115 : 0.8734262155021493
Loss in iteration 116 : 0.8331274761693644
Loss in iteration 117 : 0.8724763108704693
Loss in iteration 118 : 0.833159498818756
Loss in iteration 119 : 0.8725260143214234
Loss in iteration 120 : 0.8340741001378535
Loss in iteration 121 : 0.8750291441014726
Loss in iteration 122 : 0.8344826507037139
Loss in iteration 123 : 0.8742614556948033
Loss in iteration 124 : 0.8332100323444943
Loss in iteration 125 : 0.8702923049812873
Loss in iteration 126 : 0.8274449743447726
Loss in iteration 127 : 0.8667555176005586
Loss in iteration 128 : 0.8242938825022322
Loss in iteration 129 : 0.8667687662370028
Loss in iteration 130 : 0.827149113332563
Loss in iteration 131 : 0.8710408530493768
Loss in iteration 132 : 0.83711806470846
Loss in iteration 133 : 0.8777557195449308
Loss in iteration 134 : 0.8399936346806152
Loss in iteration 135 : 0.8769933236712373
Loss in iteration 136 : 0.8382147183113022
Loss in iteration 137 : 0.8772258614245553
Loss in iteration 138 : 0.8362809647187216
Loss in iteration 139 : 0.8710229664684459
Loss in iteration 140 : 0.8309785241008133
Loss in iteration 141 : 0.8675844873080537
Loss in iteration 142 : 0.8221885935351565
Loss in iteration 143 : 0.8619041853441809
Loss in iteration 144 : 0.8176964172695425
Loss in iteration 145 : 0.8638550131215984
Loss in iteration 146 : 0.8247000560763782
Loss in iteration 147 : 0.8678257246215351
Loss in iteration 148 : 0.8323697122163539
Loss in iteration 149 : 0.8725136716969337
Loss in iteration 150 : 0.8377051463150529
Loss in iteration 151 : 0.878937592167601
Loss in iteration 152 : 0.839728966392096
Loss in iteration 153 : 0.8776089391225878
Loss in iteration 154 : 0.835673655040446
Loss in iteration 155 : 0.8702178964512202
Loss in iteration 156 : 0.8290775225721654
Loss in iteration 157 : 0.8637350341403298
Loss in iteration 158 : 0.8227463591929004
Loss in iteration 159 : 0.8618569710620779
Loss in iteration 160 : 0.8217967656174696
Loss in iteration 161 : 0.8639193659009097
Loss in iteration 162 : 0.826227062691155
Loss in iteration 163 : 0.8682703210578888
Loss in iteration 164 : 0.8295197527432093
Loss in iteration 165 : 0.869663131394361
Loss in iteration 166 : 0.8355317185943142
Loss in iteration 167 : 0.8772723008613262
Loss in iteration 168 : 0.8365980394142335
Loss in iteration 169 : 0.8729789016337443
Loss in iteration 170 : 0.835130599414519
Loss in iteration 171 : 0.8699208760583363
Loss in iteration 172 : 0.8303875751429932
Loss in iteration 173 : 0.8673178662319706
Loss in iteration 174 : 0.8248593714804151
Loss in iteration 175 : 0.8654015693757405
Loss in iteration 176 : 0.825133468461031
Loss in iteration 177 : 0.865596136355724
Loss in iteration 178 : 0.8262226764658301
Loss in iteration 179 : 0.8698404596418065
Loss in iteration 180 : 0.8342436598160994
Loss in iteration 181 : 0.873334351411888
Loss in iteration 182 : 0.8386240759010349
Loss in iteration 183 : 0.8746401355309569
Loss in iteration 184 : 0.8334417857536064
Loss in iteration 185 : 0.8684360010330232
Loss in iteration 186 : 0.8279967534216685
Loss in iteration 187 : 0.8676860642966926
Loss in iteration 188 : 0.8268462313564824
Loss in iteration 189 : 0.8667319521967729
Loss in iteration 190 : 0.8271485136904273
Loss in iteration 191 : 0.8662770095107198
Loss in iteration 192 : 0.8279225496536644
Loss in iteration 193 : 0.871021187602517
Loss in iteration 194 : 0.8352023200482362
Loss in iteration 195 : 0.8712157538654471
Loss in iteration 196 : 0.833305248076441
Loss in iteration 197 : 0.8693873960873654
Loss in iteration 198 : 0.829747574851693
Loss in iteration 199 : 0.8676126071877297
Loss in iteration 200 : 0.8255845848790385
Testing accuracy  of updater 2 on alg 1 with rate 1.0 = 0.784125, training accuracy 0.79, time elapsed: 3486 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9562833339385256
Loss in iteration 3 : 0.9151041458516825
Loss in iteration 4 : 0.9031458434782357
Loss in iteration 5 : 0.8756135068483786
Loss in iteration 6 : 0.8464201034407032
Loss in iteration 7 : 0.816819352751688
Loss in iteration 8 : 0.7862626647888489
Loss in iteration 9 : 0.7541128418533997
Loss in iteration 10 : 0.7214528212914176
Loss in iteration 11 : 0.693580418968255
Loss in iteration 12 : 0.6679083150732179
Loss in iteration 13 : 0.6442895948804949
Loss in iteration 14 : 0.6250960080791542
Loss in iteration 15 : 0.6101215133606692
Loss in iteration 16 : 0.5981960898365524
Loss in iteration 17 : 0.5885878773270697
Loss in iteration 18 : 0.5808346356886062
Loss in iteration 19 : 0.5745479929062344
Loss in iteration 20 : 0.5693767377336548
Loss in iteration 21 : 0.5650182243441499
Loss in iteration 22 : 0.561335049164717
Loss in iteration 23 : 0.5581914818118894
Loss in iteration 24 : 0.5554844156707134
Loss in iteration 25 : 0.5531237585654805
Loss in iteration 26 : 0.5509658150612174
Loss in iteration 27 : 0.5489913212494532
Loss in iteration 28 : 0.547201620779488
Loss in iteration 29 : 0.5455856887609409
Loss in iteration 30 : 0.5441170948437498
Loss in iteration 31 : 0.5427484664836967
Loss in iteration 32 : 0.5414626074222828
Loss in iteration 33 : 0.5403038467162743
Loss in iteration 34 : 0.5392375215597603
Loss in iteration 35 : 0.5382536369908166
Loss in iteration 36 : 0.5373666280655288
Loss in iteration 37 : 0.536541176079028
Loss in iteration 38 : 0.535750652903614
Loss in iteration 39 : 0.5349892395359334
Loss in iteration 40 : 0.5342613055504877
Loss in iteration 41 : 0.5335609316952007
Loss in iteration 42 : 0.5328791821819124
Loss in iteration 43 : 0.5322211793826275
Loss in iteration 44 : 0.5315984527887246
Loss in iteration 45 : 0.5309975987516
Loss in iteration 46 : 0.5304247045075984
Loss in iteration 47 : 0.5298777944603968
Loss in iteration 48 : 0.5293476465479109
Loss in iteration 49 : 0.5288338013007163
Loss in iteration 50 : 0.5283343921824617
Loss in iteration 51 : 0.5278435240709318
Loss in iteration 52 : 0.5273634671845503
Loss in iteration 53 : 0.526891721183445
Loss in iteration 54 : 0.5264307720131595
Loss in iteration 55 : 0.525982189158807
Loss in iteration 56 : 0.5255463471063619
Loss in iteration 57 : 0.5251274432256464
Loss in iteration 58 : 0.5247223325506546
Loss in iteration 59 : 0.5243301628067814
Loss in iteration 60 : 0.5239502994208501
Loss in iteration 61 : 0.5235851477323936
Loss in iteration 62 : 0.5232342104141385
Loss in iteration 63 : 0.5228943625936185
Loss in iteration 64 : 0.5225665950413078
Loss in iteration 65 : 0.5222476836097101
Loss in iteration 66 : 0.5219370081744762
Loss in iteration 67 : 0.5216328552512255
Loss in iteration 68 : 0.5213372295978648
Loss in iteration 69 : 0.5210488001542298
Loss in iteration 70 : 0.5207704829168756
Loss in iteration 71 : 0.5205010082489713
Loss in iteration 72 : 0.5202412028281093
Loss in iteration 73 : 0.5199903724343076
Loss in iteration 74 : 0.5197500026898333
Loss in iteration 75 : 0.5195184289227381
Loss in iteration 76 : 0.5192945001053135
Loss in iteration 77 : 0.5190765190359147
Loss in iteration 78 : 0.5188649440952571
Loss in iteration 79 : 0.5186589814073536
Loss in iteration 80 : 0.5184575507668283
Loss in iteration 81 : 0.5182619470261945
Loss in iteration 82 : 0.5180737541888061
Loss in iteration 83 : 0.5178907089904564
Loss in iteration 84 : 0.5177126878499384
Loss in iteration 85 : 0.517539897906775
Loss in iteration 86 : 0.5173711936878099
Loss in iteration 87 : 0.5172069549616397
Loss in iteration 88 : 0.5170462010376484
Loss in iteration 89 : 0.516888596880037
Loss in iteration 90 : 0.5167354161603687
Loss in iteration 91 : 0.5165863314951792
Loss in iteration 92 : 0.5164423556122502
Loss in iteration 93 : 0.5163029250753682
Loss in iteration 94 : 0.5161676742423507
Loss in iteration 95 : 0.5160355886394398
Loss in iteration 96 : 0.5159068691124981
Loss in iteration 97 : 0.5157814365612896
Loss in iteration 98 : 0.515659234309014
Loss in iteration 99 : 0.5155401889705169
Loss in iteration 100 : 0.5154230509278062
Loss in iteration 101 : 0.5153085334907402
Loss in iteration 102 : 0.5151966750956896
Loss in iteration 103 : 0.5150872211198386
Loss in iteration 104 : 0.5149797805083951
Loss in iteration 105 : 0.514874995149283
Loss in iteration 106 : 0.5147725435339733
Loss in iteration 107 : 0.5146717737542184
Loss in iteration 108 : 0.5145731738231846
Loss in iteration 109 : 0.5144763070763109
Loss in iteration 110 : 0.5143810899057123
Loss in iteration 111 : 0.5142875618897954
Loss in iteration 112 : 0.5141955429872155
Loss in iteration 113 : 0.5141053698857949
Loss in iteration 114 : 0.5140177184402653
Loss in iteration 115 : 0.5139320179715271
Loss in iteration 116 : 0.5138485180450766
Loss in iteration 117 : 0.5137670108233108
Loss in iteration 118 : 0.513686461577049
Loss in iteration 119 : 0.5136071660427088
Loss in iteration 120 : 0.5135303166944966
Loss in iteration 121 : 0.5134564420786284
Loss in iteration 122 : 0.5133839690442081
Loss in iteration 123 : 0.5133125698448636
Loss in iteration 124 : 0.5132418171982429
Loss in iteration 125 : 0.5131721263318937
Loss in iteration 126 : 0.513103575795147
Loss in iteration 127 : 0.5130363413816429
Loss in iteration 128 : 0.5129696326084844
Loss in iteration 129 : 0.5129037187216504
Loss in iteration 130 : 0.5128382963857145
Loss in iteration 131 : 0.5127736722463775
Loss in iteration 132 : 0.5127100612133872
Loss in iteration 133 : 0.5126473703116414
Loss in iteration 134 : 0.5125854661926451
Loss in iteration 135 : 0.5125247875075427
Loss in iteration 136 : 0.5124656803639521
Loss in iteration 137 : 0.5124078153171883
Loss in iteration 138 : 0.5123514806729833
Loss in iteration 139 : 0.5122961710615956
Loss in iteration 140 : 0.5122417290643178
Loss in iteration 141 : 0.5121877573057368
Loss in iteration 142 : 0.5121355225584362
Loss in iteration 143 : 0.5120843667661805
Loss in iteration 144 : 0.5120344377432138
Loss in iteration 145 : 0.5119849126599051
Loss in iteration 146 : 0.5119360044381188
Loss in iteration 147 : 0.5118879054936578
Loss in iteration 148 : 0.5118405129287322
Loss in iteration 149 : 0.5117941965001244
Loss in iteration 150 : 0.5117486495060333
Loss in iteration 151 : 0.5117037451103726
Loss in iteration 152 : 0.511660157853045
Loss in iteration 153 : 0.5116180422217781
Loss in iteration 154 : 0.5115766504944779
Loss in iteration 155 : 0.5115358627770702
Loss in iteration 156 : 0.5114965332797671
Loss in iteration 157 : 0.5114583321741848
Loss in iteration 158 : 0.5114209142746665
Loss in iteration 159 : 0.5113844404623379
Loss in iteration 160 : 0.5113485527327954
Loss in iteration 161 : 0.5113130580419626
Loss in iteration 162 : 0.511278209373588
Loss in iteration 163 : 0.5112436735700808
Loss in iteration 164 : 0.5112102225007448
Loss in iteration 165 : 0.5111779921959471
Loss in iteration 166 : 0.5111468159087657
Loss in iteration 167 : 0.5111162456995654
Loss in iteration 168 : 0.5110861877272569
Loss in iteration 169 : 0.5110564696462476
Loss in iteration 170 : 0.5110277330255028
Loss in iteration 171 : 0.5109993143552236
Loss in iteration 172 : 0.5109713301654754
Loss in iteration 173 : 0.5109437129066436
Loss in iteration 174 : 0.510916745086902
Loss in iteration 175 : 0.5108899577930197
Loss in iteration 176 : 0.5108634189551202
Loss in iteration 177 : 0.51083710096005
Loss in iteration 178 : 0.5108110342987573
Loss in iteration 179 : 0.5107852116846721
Loss in iteration 180 : 0.5107599463547148
Loss in iteration 181 : 0.5107349103882687
Loss in iteration 182 : 0.5107101540891067
Loss in iteration 183 : 0.5106859547899284
Loss in iteration 184 : 0.5106622893703789
Loss in iteration 185 : 0.5106388624769201
Loss in iteration 186 : 0.5106158415483404
Loss in iteration 187 : 0.510592903435241
Loss in iteration 188 : 0.5105701828953872
Loss in iteration 189 : 0.5105477189568396
Loss in iteration 190 : 0.5105255469678736
Loss in iteration 191 : 0.5105039186046715
Loss in iteration 192 : 0.5104828496046954
Loss in iteration 193 : 0.5104621524222175
Loss in iteration 194 : 0.5104414843204208
Loss in iteration 195 : 0.5104210909490898
Loss in iteration 196 : 0.51040097778258
Loss in iteration 197 : 0.5103810764723461
Loss in iteration 198 : 0.5103614552783097
Loss in iteration 199 : 0.5103419904793192
Loss in iteration 200 : 0.5103227624443522
Testing accuracy  of updater 2 on alg 1 with rate 0.09999999999999998 = 0.787, training accuracy 0.781, time elapsed: 3307 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 20.37732908901907
Loss in iteration 3 : 25.287366070202793
Loss in iteration 4 : 11.168884937204485
Loss in iteration 5 : 11.6784906900165
Loss in iteration 6 : 7.350552661719799
Loss in iteration 7 : 7.762225700903877
Loss in iteration 8 : 6.32013226186656
Loss in iteration 9 : 6.340921394894471
Loss in iteration 10 : 5.4520296409480595
Loss in iteration 11 : 5.33377793124557
Loss in iteration 12 : 4.767078403366352
Loss in iteration 13 : 4.647737190252947
Loss in iteration 14 : 4.271200710520914
Loss in iteration 15 : 4.189880425630291
Loss in iteration 16 : 3.777859235508905
Loss in iteration 17 : 3.817027300148857
Loss in iteration 18 : 3.3929439800858185
Loss in iteration 19 : 3.4839866173252148
Loss in iteration 20 : 3.1075214095075236
Loss in iteration 21 : 3.278737428036524
Loss in iteration 22 : 2.8955764294197324
Loss in iteration 23 : 3.11895623164707
Loss in iteration 24 : 2.796890261406155
Loss in iteration 25 : 3.078568865605482
Loss in iteration 26 : 2.6691947509800924
Loss in iteration 27 : 3.0350723374253175
Loss in iteration 28 : 2.488808211776199
Loss in iteration 29 : 2.811912752103233
Loss in iteration 30 : 2.290016793936932
Loss in iteration 31 : 2.540316316626215
Loss in iteration 32 : 2.222048607136492
Loss in iteration 33 : 2.4972018230670634
Loss in iteration 34 : 2.269598651278266
Loss in iteration 35 : 2.553689105753858
Loss in iteration 36 : 2.351304993973123
Loss in iteration 37 : 2.6405132400325173
Loss in iteration 38 : 2.4774894825254727
Loss in iteration 39 : 2.796378462882507
Loss in iteration 40 : 2.4950328096823524
Loss in iteration 41 : 2.7472021765997496
Loss in iteration 42 : 2.4373392677924235
Loss in iteration 43 : 2.6396004740931485
Loss in iteration 44 : 2.342377434466678
Loss in iteration 45 : 2.4921293254847465
Loss in iteration 46 : 2.2988065104151625
Loss in iteration 47 : 2.4287923681263837
Loss in iteration 48 : 2.2310171139916575
Loss in iteration 49 : 2.344514610565474
Loss in iteration 50 : 2.16578533885449
Loss in iteration 51 : 2.272322763005821
Loss in iteration 52 : 2.1320045079311756
Loss in iteration 53 : 2.248051530445119
Loss in iteration 54 : 2.105302061487764
Loss in iteration 55 : 2.2095529662394147
Loss in iteration 56 : 2.0775645976600576
Loss in iteration 57 : 2.187791519455366
Loss in iteration 58 : 2.0451971519311463
Loss in iteration 59 : 2.152560100152242
Loss in iteration 60 : 2.015986034121985
Loss in iteration 61 : 2.1294860395648567
Loss in iteration 62 : 2.0012179889014963
Loss in iteration 63 : 2.122708045273517
Loss in iteration 64 : 1.9805129497851182
Loss in iteration 65 : 2.101263472928125
Loss in iteration 66 : 1.959394448210694
Loss in iteration 67 : 2.076501749893059
Loss in iteration 68 : 1.9514101830440367
Loss in iteration 69 : 2.084914708842053
Loss in iteration 70 : 1.928894735027179
Loss in iteration 71 : 2.0475498850841856
Loss in iteration 72 : 1.9205984870163566
Loss in iteration 73 : 2.0595547646113235
Loss in iteration 74 : 1.8958175307185807
Loss in iteration 75 : 2.0262887028798096
Loss in iteration 76 : 1.887623874793267
Loss in iteration 77 : 2.0143611110725947
Loss in iteration 78 : 1.8785311613782192
Loss in iteration 79 : 1.9998813615697228
Loss in iteration 80 : 1.8684294088536149
Loss in iteration 81 : 1.9847595934732962
Loss in iteration 82 : 1.850735254709397
Loss in iteration 83 : 1.9721781099377471
Loss in iteration 84 : 1.8342405315602985
Loss in iteration 85 : 1.9584170635280889
Loss in iteration 86 : 1.8207103826181037
Loss in iteration 87 : 1.9394424025852186
Loss in iteration 88 : 1.8151300906435608
Loss in iteration 89 : 1.930336990418779
Loss in iteration 90 : 1.78372258003892
Loss in iteration 91 : 1.896043090206903
Loss in iteration 92 : 1.7845123018770488
Loss in iteration 93 : 1.896473088109551
Loss in iteration 94 : 1.766639433714873
Loss in iteration 95 : 1.869718067870824
Loss in iteration 96 : 1.766503008617947
Loss in iteration 97 : 1.8752982959795628
Loss in iteration 98 : 1.753121340339268
Loss in iteration 99 : 1.8537668509831842
Loss in iteration 100 : 1.7369839289586653
Loss in iteration 101 : 1.8342307438431924
Loss in iteration 102 : 1.7257022652771439
Loss in iteration 103 : 1.8228782808871788
Loss in iteration 104 : 1.7105798189422243
Loss in iteration 105 : 1.8064713118694216
Loss in iteration 106 : 1.6992908988317295
Loss in iteration 107 : 1.7953393631475973
Loss in iteration 108 : 1.6854915924568556
Loss in iteration 109 : 1.7807423586972921
Loss in iteration 110 : 1.6815108551883569
Loss in iteration 111 : 1.7796695363713253
Loss in iteration 112 : 1.670589385902886
Loss in iteration 113 : 1.7690288944149715
Loss in iteration 114 : 1.6577188524512518
Loss in iteration 115 : 1.7539862678691365
Loss in iteration 116 : 1.6485113698021305
Loss in iteration 117 : 1.7463895711740935
Loss in iteration 118 : 1.6396747083228762
Loss in iteration 119 : 1.7366525753314446
Loss in iteration 120 : 1.6275367817959583
Loss in iteration 121 : 1.7346866141451553
Loss in iteration 122 : 1.61975581278652
Loss in iteration 123 : 1.718179071899761
Loss in iteration 124 : 1.6138485498604613
Loss in iteration 125 : 1.7148470642522107
Loss in iteration 126 : 1.601545359353319
Loss in iteration 127 : 1.7079213280545193
Loss in iteration 128 : 1.5913303173583022
Loss in iteration 129 : 1.7046142293373883
Loss in iteration 130 : 1.5784724934952292
Loss in iteration 131 : 1.6841281946860935
Loss in iteration 132 : 1.576730761221078
Loss in iteration 133 : 1.680174281193655
Loss in iteration 134 : 1.5652652738521637
Loss in iteration 135 : 1.6667885464333787
Loss in iteration 136 : 1.5561943554028799
Loss in iteration 137 : 1.6648555994957754
Loss in iteration 138 : 1.5475422643163534
Loss in iteration 139 : 1.6572296395303252
Loss in iteration 140 : 1.5373487304668398
Loss in iteration 141 : 1.6467372699682843
Loss in iteration 142 : 1.529735129890627
Loss in iteration 143 : 1.636027962782095
Loss in iteration 144 : 1.522113858183323
Loss in iteration 145 : 1.6270071868847036
Loss in iteration 146 : 1.5127781949071253
Loss in iteration 147 : 1.6165133830899572
Loss in iteration 148 : 1.5033634360201558
Loss in iteration 149 : 1.6062087817646649
Loss in iteration 150 : 1.4932748020769824
Loss in iteration 151 : 1.5973769470806747
Loss in iteration 152 : 1.4860839959334593
Loss in iteration 153 : 1.5908183318564293
Loss in iteration 154 : 1.4725471509133319
Loss in iteration 155 : 1.581552631618283
Loss in iteration 156 : 1.4672470418529369
Loss in iteration 157 : 1.574727424793487
Loss in iteration 158 : 1.4605706886015868
Loss in iteration 159 : 1.5626740118799503
Loss in iteration 160 : 1.457366699242572
Loss in iteration 161 : 1.562472758191955
Loss in iteration 162 : 1.4490247281747841
Loss in iteration 163 : 1.5524718799275647
Loss in iteration 164 : 1.454251308787513
Loss in iteration 165 : 1.5613282792933492
Loss in iteration 166 : 1.4451792156714516
Loss in iteration 167 : 1.5438504168739073
Loss in iteration 168 : 1.4464940555270946
Loss in iteration 169 : 1.5493754151944286
Loss in iteration 170 : 1.4383181953977022
Loss in iteration 171 : 1.5310353221870818
Loss in iteration 172 : 1.4329806090597867
Loss in iteration 173 : 1.526241352072294
Loss in iteration 174 : 1.4257532447545413
Loss in iteration 175 : 1.5218535959242558
Loss in iteration 176 : 1.4323957647438967
Loss in iteration 177 : 1.5256940181178962
Loss in iteration 178 : 1.4346679859415221
Loss in iteration 179 : 1.5283548502935855
Loss in iteration 180 : 1.426735163331838
Loss in iteration 181 : 1.518156297836681
Loss in iteration 182 : 1.4237953887765682
Loss in iteration 183 : 1.5138552878629539
Loss in iteration 184 : 1.41606364545525
Loss in iteration 185 : 1.5069336406231841
Loss in iteration 186 : 1.3797982175358543
Loss in iteration 187 : 1.4819210306195405
Loss in iteration 188 : 1.363644632041734
Loss in iteration 189 : 1.5572141786007256
Loss in iteration 190 : 1.3886981555989357
Loss in iteration 191 : 1.7100707819971603
Loss in iteration 192 : 1.3923167082414616
Loss in iteration 193 : 1.4790467306071942
Loss in iteration 194 : 1.2565894607126407
Loss in iteration 195 : 1.2351014007354124
Loss in iteration 196 : 1.116821687189522
Loss in iteration 197 : 1.1106368637260302
Loss in iteration 198 : 1.0382780064651882
Loss in iteration 199 : 1.0668448454071267
Loss in iteration 200 : 1.0424252678853534
Testing accuracy  of updater 3 on alg 1 with rate 10.0 = 0.7615, training accuracy 0.753, time elapsed: 2911 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9218248671950956
Loss in iteration 3 : 0.8998444202083427
Loss in iteration 4 : 0.8805787016344643
Loss in iteration 5 : 0.8617557129342106
Loss in iteration 6 : 0.8436253579091461
Loss in iteration 7 : 0.8257573828795344
Loss in iteration 8 : 0.8080500150522815
Loss in iteration 9 : 0.7904743757655244
Loss in iteration 10 : 0.7731475741020419
Loss in iteration 11 : 0.7565356970761752
Loss in iteration 12 : 0.7407185192056958
Loss in iteration 13 : 0.7255456210810589
Loss in iteration 14 : 0.711031462250631
Loss in iteration 15 : 0.6972566947170056
Loss in iteration 16 : 0.6843131143977121
Loss in iteration 17 : 0.6726683442404887
Loss in iteration 18 : 0.6624793557213225
Loss in iteration 19 : 0.6534940528543811
Loss in iteration 20 : 0.6455347601739148
Loss in iteration 21 : 0.638574521785023
Loss in iteration 22 : 0.6323839740226143
Loss in iteration 23 : 0.6269458074914708
Loss in iteration 24 : 0.6220506329157609
Loss in iteration 25 : 0.61757635970812
Loss in iteration 26 : 0.6134827518298459
Loss in iteration 27 : 0.6097286562324662
Loss in iteration 28 : 0.6064073919384916
Loss in iteration 29 : 0.6032839498880276
Loss in iteration 30 : 0.6005368632114765
Loss in iteration 31 : 0.5977611148121452
Loss in iteration 32 : 0.5952256641101403
Loss in iteration 33 : 0.5927350480469687
Loss in iteration 34 : 0.5905506021427512
Loss in iteration 35 : 0.5882782875836982
Loss in iteration 36 : 0.5861001167580677
Loss in iteration 37 : 0.584046961937571
Loss in iteration 38 : 0.5820679204023657
Loss in iteration 39 : 0.5802527326933944
Loss in iteration 40 : 0.5785429245157951
Loss in iteration 41 : 0.5768951212645386
Loss in iteration 42 : 0.5753499482944161
Loss in iteration 43 : 0.5739151783433285
Loss in iteration 44 : 0.5724973815434716
Loss in iteration 45 : 0.5711816362073625
Loss in iteration 46 : 0.5698853638978407
Loss in iteration 47 : 0.5686523919736823
Loss in iteration 48 : 0.5674859033817595
Loss in iteration 49 : 0.5663539144864143
Loss in iteration 50 : 0.5652872686995439
Loss in iteration 51 : 0.564259720568913
Loss in iteration 52 : 0.5632627127713019
Loss in iteration 53 : 0.5623057977688856
Loss in iteration 54 : 0.561387485566011
Loss in iteration 55 : 0.5605039267321336
Loss in iteration 56 : 0.5596464476958495
Loss in iteration 57 : 0.5588133398799054
Loss in iteration 58 : 0.5579995952701453
Loss in iteration 59 : 0.5572086745540255
Loss in iteration 60 : 0.5564457083772857
Loss in iteration 61 : 0.5557163712775177
Loss in iteration 62 : 0.555002693964725
Loss in iteration 63 : 0.5543106752649979
Loss in iteration 64 : 0.5536338708860303
Loss in iteration 65 : 0.55298586402803
Loss in iteration 66 : 0.5523643555021228
Loss in iteration 67 : 0.5517632288441804
Loss in iteration 68 : 0.551181938848822
Loss in iteration 69 : 0.550614720366181
Loss in iteration 70 : 0.5500599204505852
Loss in iteration 71 : 0.549516631621165
Loss in iteration 72 : 0.5489770738273733
Loss in iteration 73 : 0.5484445702128428
Loss in iteration 74 : 0.5479236067198158
Loss in iteration 75 : 0.5474146944752369
Loss in iteration 76 : 0.546915958747238
Loss in iteration 77 : 0.5464299194067187
Loss in iteration 78 : 0.5459596951970314
Loss in iteration 79 : 0.5454971971910719
Loss in iteration 80 : 0.5450449731740786
Loss in iteration 81 : 0.5446005496952554
Loss in iteration 82 : 0.5441692061318187
Loss in iteration 83 : 0.543737750753253
Loss in iteration 84 : 0.5433219831503284
Loss in iteration 85 : 0.5429144602645799
Loss in iteration 86 : 0.5425211647499909
Loss in iteration 87 : 0.5421357242489327
Loss in iteration 88 : 0.5417811257791167
Loss in iteration 89 : 0.5414321900159623
Loss in iteration 90 : 0.5411153757461085
Loss in iteration 91 : 0.5407436055995918
Loss in iteration 92 : 0.5404299659480609
Loss in iteration 93 : 0.540084927654679
Loss in iteration 94 : 0.5397351630410769
Loss in iteration 95 : 0.539403545946188
Loss in iteration 96 : 0.5390662647794083
Loss in iteration 97 : 0.5387963832168223
Loss in iteration 98 : 0.5384375005352494
Loss in iteration 99 : 0.5381425547437769
Loss in iteration 100 : 0.5378313667075845
Loss in iteration 101 : 0.5375824451629816
Loss in iteration 102 : 0.5372682153131384
Loss in iteration 103 : 0.5370009944978663
Loss in iteration 104 : 0.5366867322642824
Loss in iteration 105 : 0.5364047829791212
Loss in iteration 106 : 0.5361003265842514
Loss in iteration 107 : 0.5358430249536307
Loss in iteration 108 : 0.5355490873723556
Loss in iteration 109 : 0.5353036338246354
Loss in iteration 110 : 0.5350181863396268
Loss in iteration 111 : 0.534801410554077
Loss in iteration 112 : 0.5345134591754518
Loss in iteration 113 : 0.534263256766063
Loss in iteration 114 : 0.5340188494093857
Loss in iteration 115 : 0.533812335813556
Loss in iteration 116 : 0.533530313262084
Loss in iteration 117 : 0.5333411000359278
Loss in iteration 118 : 0.533100082149707
Loss in iteration 119 : 0.5329018643410336
Loss in iteration 120 : 0.5326625156484058
Loss in iteration 121 : 0.5324743629403654
Loss in iteration 122 : 0.5322281192650089
Loss in iteration 123 : 0.5320708608916619
Loss in iteration 124 : 0.5318212102506957
Loss in iteration 125 : 0.5316263560205848
Loss in iteration 126 : 0.531417574331386
Loss in iteration 127 : 0.5312122601357784
Loss in iteration 128 : 0.5310171199863142
Loss in iteration 129 : 0.5308241723562471
Loss in iteration 130 : 0.5306353792791205
Loss in iteration 131 : 0.5304539052734313
Loss in iteration 132 : 0.5302768208863281
Loss in iteration 133 : 0.5300986723531808
Loss in iteration 134 : 0.5299218669301299
Loss in iteration 135 : 0.5297462607293426
Loss in iteration 136 : 0.5295749615923889
Loss in iteration 137 : 0.529408730262274
Loss in iteration 138 : 0.529243353692314
Loss in iteration 139 : 0.52908242939749
Loss in iteration 140 : 0.5289122662724567
Loss in iteration 141 : 0.5287467186704895
Loss in iteration 142 : 0.5285807362537507
Loss in iteration 143 : 0.5284193428852654
Loss in iteration 144 : 0.5282618781286351
Loss in iteration 145 : 0.5281022818135493
Loss in iteration 146 : 0.5279539817688712
Loss in iteration 147 : 0.5278004666262622
Loss in iteration 148 : 0.5276409053563613
Loss in iteration 149 : 0.5274864294572801
Loss in iteration 150 : 0.5273370994051627
Loss in iteration 151 : 0.5271959305437749
Loss in iteration 152 : 0.527047501577227
Loss in iteration 153 : 0.5269083647194485
Loss in iteration 154 : 0.5267717299456073
Loss in iteration 155 : 0.5266364150044397
Loss in iteration 156 : 0.5264953566267698
Loss in iteration 157 : 0.5263581606719631
Loss in iteration 158 : 0.5262298184711119
Loss in iteration 159 : 0.526096271532861
Loss in iteration 160 : 0.5259765350576769
Loss in iteration 161 : 0.5258416851832515
Loss in iteration 162 : 0.5257150774223338
Loss in iteration 163 : 0.5255721544113648
Loss in iteration 164 : 0.525446565705193
Loss in iteration 165 : 0.5253158308496809
Loss in iteration 166 : 0.5251921988073858
Loss in iteration 167 : 0.5250684920327076
Loss in iteration 168 : 0.5249456221738976
Loss in iteration 169 : 0.5248247135645224
Loss in iteration 170 : 0.5247062003993317
Loss in iteration 171 : 0.5245879939244407
Loss in iteration 172 : 0.5244725511395429
Loss in iteration 173 : 0.5243560355224421
Loss in iteration 174 : 0.524239294487049
Loss in iteration 175 : 0.5241271266329302
Loss in iteration 176 : 0.5240075534336104
Loss in iteration 177 : 0.5238946287611089
Loss in iteration 178 : 0.5237799519252737
Loss in iteration 179 : 0.523667460924752
Loss in iteration 180 : 0.5235576653483821
Loss in iteration 181 : 0.5234491275576834
Loss in iteration 182 : 0.5233417269348688
Loss in iteration 183 : 0.5232322564140729
Loss in iteration 184 : 0.5231257422074439
Loss in iteration 185 : 0.5230211728606095
Loss in iteration 186 : 0.5229176101086626
Loss in iteration 187 : 0.5228151245964299
Loss in iteration 188 : 0.5227109888248925
Loss in iteration 189 : 0.5226082246068691
Loss in iteration 190 : 0.5225066449286999
Loss in iteration 191 : 0.5224064580584779
Loss in iteration 192 : 0.5223059291071026
Loss in iteration 193 : 0.5222085897781221
Loss in iteration 194 : 0.522112827105438
Loss in iteration 195 : 0.5220157622417605
Loss in iteration 196 : 0.5219200333189219
Loss in iteration 197 : 0.5218267224163707
Loss in iteration 198 : 0.5217311017070985
Loss in iteration 199 : 0.5216394504126662
Loss in iteration 200 : 0.52154410735903
Testing accuracy  of updater 3 on alg 1 with rate 1.0 = 0.783, training accuracy 0.7765, time elapsed: 2921 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9911196871832337
Loss in iteration 3 : 0.9822730055802166
Loss in iteration 4 : 0.973478412195194
Loss in iteration 5 : 0.9648670928653896
Loss in iteration 6 : 0.9566342099293867
Loss in iteration 7 : 0.9490952834503458
Loss in iteration 8 : 0.9422804036027701
Loss in iteration 9 : 0.9362751133454527
Loss in iteration 10 : 0.9309994941592763
Loss in iteration 11 : 0.9263805872103494
Loss in iteration 12 : 0.9224283119061983
Loss in iteration 13 : 0.9191089776824777
Loss in iteration 14 : 0.9162460295208202
Loss in iteration 15 : 0.9136761077601282
Loss in iteration 16 : 0.9113283771546946
Loss in iteration 17 : 0.9091371267379752
Loss in iteration 18 : 0.9070707820522648
Loss in iteration 19 : 0.9051022372252112
Loss in iteration 20 : 0.903186004489612
Loss in iteration 21 : 0.9013013413711781
Loss in iteration 22 : 0.8994404787801811
Loss in iteration 23 : 0.8975919418514756
Loss in iteration 24 : 0.8957578072517374
Loss in iteration 25 : 0.8939543216817076
Loss in iteration 26 : 0.8921673457632521
Loss in iteration 27 : 0.8903829996548144
Loss in iteration 28 : 0.8886020865736254
Loss in iteration 29 : 0.8868239081313347
Loss in iteration 30 : 0.8850477812352113
Loss in iteration 31 : 0.8832779968226883
Loss in iteration 32 : 0.881512987371098
Loss in iteration 33 : 0.879750200652886
Loss in iteration 34 : 0.8779898967381314
Loss in iteration 35 : 0.8762324617418553
Loss in iteration 36 : 0.874478514565449
Loss in iteration 37 : 0.8727276637085712
Loss in iteration 38 : 0.8709795461912023
Loss in iteration 39 : 0.8692352583955671
Loss in iteration 40 : 0.8674963944913124
Loss in iteration 41 : 0.8657607030086836
Loss in iteration 42 : 0.8640289280457887
Loss in iteration 43 : 0.862301093598489
Loss in iteration 44 : 0.8605789724502968
Loss in iteration 45 : 0.8588595625914338
Loss in iteration 46 : 0.8571433984090443
Loss in iteration 47 : 0.8554292288238722
Loss in iteration 48 : 0.8537166581363058
Loss in iteration 49 : 0.8520055610273598
Loss in iteration 50 : 0.850297003800483
Loss in iteration 51 : 0.8485911666196333
Loss in iteration 52 : 0.8468874386687999
Loss in iteration 53 : 0.8451863123779887
Loss in iteration 54 : 0.8434881376483118
Loss in iteration 55 : 0.8417947185278174
Loss in iteration 56 : 0.8401047886471713
Loss in iteration 57 : 0.8384170109941547
Loss in iteration 58 : 0.8367308806505139
Loss in iteration 59 : 0.8350462602571656
Loss in iteration 60 : 0.8333639275467848
Loss in iteration 61 : 0.8316842743656541
Loss in iteration 62 : 0.8300072738650562
Loss in iteration 63 : 0.8283329514288064
Loss in iteration 64 : 0.8266602946981108
Loss in iteration 65 : 0.8249889908336968
Loss in iteration 66 : 0.8233201277298438
Loss in iteration 67 : 0.8216545479769527
Loss in iteration 68 : 0.819990064207011
Loss in iteration 69 : 0.8183277841241047
Loss in iteration 70 : 0.8166674217315182
Loss in iteration 71 : 0.815010040503802
Loss in iteration 72 : 0.8133565795055442
Loss in iteration 73 : 0.8117045066785991
Loss in iteration 74 : 0.81005421182957
Loss in iteration 75 : 0.8084065972204779
Loss in iteration 76 : 0.8067617748789063
Loss in iteration 77 : 0.8051180129910644
Loss in iteration 78 : 0.8034756895499416
Loss in iteration 79 : 0.8018348490476497
Loss in iteration 80 : 0.8001955883737915
Loss in iteration 81 : 0.7985581805694137
Loss in iteration 82 : 0.7969237243092367
Loss in iteration 83 : 0.795292495969785
Loss in iteration 84 : 0.7936622722608395
Loss in iteration 85 : 0.7920335627410255
Loss in iteration 86 : 0.7904091422013112
Loss in iteration 87 : 0.7887858397944946
Loss in iteration 88 : 0.7871683183398214
Loss in iteration 89 : 0.7855586579113226
Loss in iteration 90 : 0.7839544250565585
Loss in iteration 91 : 0.7823561273845041
Loss in iteration 92 : 0.780764000963606
Loss in iteration 93 : 0.7791758536727669
Loss in iteration 94 : 0.7775951344796777
Loss in iteration 95 : 0.7760178191941498
Loss in iteration 96 : 0.7744451113334188
Loss in iteration 97 : 0.7728775580834792
Loss in iteration 98 : 0.7713145639212091
Loss in iteration 99 : 0.769760546831843
Loss in iteration 100 : 0.7682167068845747
Loss in iteration 101 : 0.7666828865228972
Loss in iteration 102 : 0.7651644591333838
Loss in iteration 103 : 0.763650917548295
Loss in iteration 104 : 0.7621433889221503
Loss in iteration 105 : 0.7606490024744481
Loss in iteration 106 : 0.7591589688460187
Loss in iteration 107 : 0.7576772014092943
Loss in iteration 108 : 0.7561993044558695
Loss in iteration 109 : 0.7547249140945387
Loss in iteration 110 : 0.7532579495825243
Loss in iteration 111 : 0.7517997798515271
Loss in iteration 112 : 0.7503504000929745
Loss in iteration 113 : 0.7489052068639762
Loss in iteration 114 : 0.7474639067665477
Loss in iteration 115 : 0.7460288387422807
Loss in iteration 116 : 0.74460353646482
Loss in iteration 117 : 0.7431877704580652
Loss in iteration 118 : 0.7417759873392151
Loss in iteration 119 : 0.7403692969455372
Loss in iteration 120 : 0.7389715108257058
Loss in iteration 121 : 0.7375772339437088
Loss in iteration 122 : 0.736187027996697
Loss in iteration 123 : 0.7348013049541191
Loss in iteration 124 : 0.7334233402806041
Loss in iteration 125 : 0.7320507969056748
Loss in iteration 126 : 0.730683888467046
Loss in iteration 127 : 0.7293253099583438
Loss in iteration 128 : 0.7279702363540476
Loss in iteration 129 : 0.72662086595933
Loss in iteration 130 : 0.7252782439089792
Loss in iteration 131 : 0.7239438307575203
Loss in iteration 132 : 0.722615427596648
Loss in iteration 133 : 0.7212932818931269
Loss in iteration 134 : 0.7199785069054314
Loss in iteration 135 : 0.7186682769902947
Loss in iteration 136 : 0.7173637973300027
Loss in iteration 137 : 0.7160641803629849
Loss in iteration 138 : 0.7147714687112157
Loss in iteration 139 : 0.7134883112440589
Loss in iteration 140 : 0.712211638273194
Loss in iteration 141 : 0.7109411091645053
Loss in iteration 142 : 0.7096776828111954
Loss in iteration 143 : 0.7084198219679454
Loss in iteration 144 : 0.7071671092439479
Loss in iteration 145 : 0.7059202189812177
Loss in iteration 146 : 0.7046781174816984
Loss in iteration 147 : 0.7034374708076621
Loss in iteration 148 : 0.7022011450611383
Loss in iteration 149 : 0.7009717256682226
Loss in iteration 150 : 0.6997497075828136
Loss in iteration 151 : 0.6985328110840638
Loss in iteration 152 : 0.6973281100438963
Loss in iteration 153 : 0.696137995591817
Loss in iteration 154 : 0.6949575861649803
Loss in iteration 155 : 0.6937933561383197
Loss in iteration 156 : 0.6926408688457693
Loss in iteration 157 : 0.6914980677288354
Loss in iteration 158 : 0.6903622051947244
Loss in iteration 159 : 0.689236517789084
Loss in iteration 160 : 0.6881135486282917
Loss in iteration 161 : 0.686998022071797
Loss in iteration 162 : 0.6858871756227388
Loss in iteration 163 : 0.6847864481764528
Loss in iteration 164 : 0.6836991055676718
Loss in iteration 165 : 0.682625900099488
Loss in iteration 166 : 0.6815702054758453
Loss in iteration 167 : 0.6805272929797136
Loss in iteration 168 : 0.6794995592195835
Loss in iteration 169 : 0.6784823037805309
Loss in iteration 170 : 0.6774770368692923
Loss in iteration 171 : 0.6764799955455716
Loss in iteration 172 : 0.6754890757968929
Loss in iteration 173 : 0.6745065997147376
Loss in iteration 174 : 0.6735346110301196
Loss in iteration 175 : 0.6725855154130429
Loss in iteration 176 : 0.6716512116057611
Loss in iteration 177 : 0.6707259397825966
Loss in iteration 178 : 0.6698156168266239
Loss in iteration 179 : 0.6689141922779572
Loss in iteration 180 : 0.6680182541764093
Loss in iteration 181 : 0.6671306495070071
Loss in iteration 182 : 0.6662509707557245
Loss in iteration 183 : 0.6653837913293139
Loss in iteration 184 : 0.6645255512965788
Loss in iteration 185 : 0.6636736623339768
Loss in iteration 186 : 0.6628283127362401
Loss in iteration 187 : 0.6619936895554938
Loss in iteration 188 : 0.6611681663005885
Loss in iteration 189 : 0.6603479036987214
Loss in iteration 190 : 0.659535681445854
Loss in iteration 191 : 0.6587337589035791
Loss in iteration 192 : 0.6579457749118283
Loss in iteration 193 : 0.6571644398244019
Loss in iteration 194 : 0.6563899257095499
Loss in iteration 195 : 0.6556207438075926
Loss in iteration 196 : 0.6548615862572013
Loss in iteration 197 : 0.6541104881014996
Loss in iteration 198 : 0.6533675371088435
Loss in iteration 199 : 0.6526323674022203
Loss in iteration 200 : 0.65190643228614
Testing accuracy  of updater 3 on alg 1 with rate 0.09999999999999998 = 0.74525, training accuracy 0.744, time elapsed: 3215 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.6015, training accuracy 0.595, time elapsed: 3990 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.6015, training accuracy 0.595, time elapsed: 3349 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302124
Loss in iteration 3 : 0.9990911666950771
Loss in iteration 4 : 0.9986229266098012
Loss in iteration 5 : 0.9981484796078872
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516703
Loss in iteration 8 : 0.9966968064934897
Loss in iteration 9 : 0.9962051782244066
Loss in iteration 10 : 0.9957102204541362
Loss in iteration 11 : 0.9952121515419887
Loss in iteration 12 : 0.9947111474515812
Loss in iteration 13 : 0.9942073525436084
Loss in iteration 14 : 0.9937008870216676
Loss in iteration 15 : 0.9931918522344851
Loss in iteration 16 : 0.9926803345536424
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306334
Loss in iteration 19 : 0.9911315795097323
Loss in iteration 20 : 0.9906107828087424
Loss in iteration 21 : 0.9900877915131123
Loss in iteration 22 : 0.9895626445693696
Loss in iteration 23 : 0.9890353767970491
Loss in iteration 24 : 0.9885060194751375
Loss in iteration 25 : 0.9879746008291871
Loss in iteration 26 : 0.9874411464389787
Loss in iteration 27 : 0.9869056795820381
Loss in iteration 28 : 0.9863682215247993
Loss in iteration 29 : 0.9858287917708686
Loss in iteration 30 : 0.9852874082736689
Loss in iteration 31 : 0.9847440876194403
Loss in iteration 32 : 0.9841988451854286
Loss in iteration 33 : 0.9836516952770065
Loss in iteration 34 : 0.9831026512470877
Loss in iteration 35 : 0.98255172560032
Loss in iteration 36 : 0.9819989300843128
Loss in iteration 37 : 0.9814442757697035
Loss in iteration 38 : 0.9808877731205848
Loss in iteration 39 : 0.9803294320566008
Loss in iteration 40 : 0.9797692620077592
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131212
Loss in iteration 43 : 0.9780778658890487
Loss in iteration 44 : 0.9775104659956844
Loss in iteration 45 : 0.9769412784421333
Loss in iteration 46 : 0.9763703105686186
Loss in iteration 47 : 0.9757975694704585
Loss in iteration 48 : 0.9752230620195429
Loss in iteration 49 : 0.9746467948834653
Loss in iteration 50 : 0.9740687745426849
Loss in iteration 51 : 0.9734890073058864
Loss in iteration 52 : 0.9729074993237554
Loss in iteration 53 : 0.9723242566013293
Loss in iteration 54 : 0.9717392850090891
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264176
Loss in iteration 57 : 0.9699853256383415
Loss in iteration 58 : 0.9694030412738993
Loss in iteration 59 : 0.96881907840498
Loss in iteration 60 : 0.9682334401416729
Loss in iteration 61 : 0.9676461297304232
Loss in iteration 62 : 0.9670571505375632
Loss in iteration 63 : 0.9664665060343812
Loss in iteration 64 : 0.965874199783518
Loss in iteration 65 : 0.9652802354266974
Loss in iteration 66 : 0.964684616673578
Loss in iteration 67 : 0.9640873472917237
Loss in iteration 68 : 0.9634884723407172
Loss in iteration 69 : 0.9628921941325822
Loss in iteration 70 : 0.9622977454567908
Loss in iteration 71 : 0.9617016800571683
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874623
Loss in iteration 74 : 0.9599051388851504
Loss in iteration 75 : 0.9593062298501525
Loss in iteration 76 : 0.9587057257699184
Loss in iteration 77 : 0.958103628798543
Loss in iteration 78 : 0.9574999412695994
Loss in iteration 79 : 0.9568946656766395
Loss in iteration 80 : 0.9562878046553551
Loss in iteration 81 : 0.9556816523135653
Loss in iteration 82 : 0.9550814459110015
Loss in iteration 83 : 0.9544815984559709
Loss in iteration 84 : 0.9538846209944896
Loss in iteration 85 : 0.9532872686338312
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201408
Loss in iteration 88 : 0.9515032521975604
Loss in iteration 89 : 0.9509194294980812
Loss in iteration 90 : 0.9503342683949643
Loss in iteration 91 : 0.9497511738393177
Loss in iteration 92 : 0.9491708501216707
Loss in iteration 93 : 0.9485982816500561
Loss in iteration 94 : 0.9480263044402891
Loss in iteration 95 : 0.9474547293733717
Loss in iteration 96 : 0.9468880307025399
Loss in iteration 97 : 0.9463243837048171
Loss in iteration 98 : 0.9457618628085943
Loss in iteration 99 : 0.9452048428904507
Loss in iteration 100 : 0.9446515461397315
Loss in iteration 101 : 0.9441012671116764
Loss in iteration 102 : 0.9435518629128601
Loss in iteration 103 : 0.9430016508564786
Loss in iteration 104 : 0.9424557183301122
Loss in iteration 105 : 0.9419166132179775
Loss in iteration 106 : 0.9413841271762255
Loss in iteration 107 : 0.9408530340533833
Loss in iteration 108 : 0.9403236524482216
Loss in iteration 109 : 0.9397929218514743
Loss in iteration 110 : 0.9392608655180982
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118596
Loss in iteration 113 : 0.937676664436899
Loss in iteration 114 : 0.9371508081019349
Loss in iteration 115 : 0.9366254633121418
Loss in iteration 116 : 0.936104619795406
Loss in iteration 117 : 0.9355860091709157
Loss in iteration 118 : 0.9350716258591176
Loss in iteration 119 : 0.9345612315882861
Loss in iteration 120 : 0.9340512504682456
Loss in iteration 121 : 0.9335446904439622
Loss in iteration 122 : 0.9330455555866898
Loss in iteration 123 : 0.9325469560645522
Loss in iteration 124 : 0.9320503331151226
Loss in iteration 125 : 0.931558602387324
Loss in iteration 126 : 0.9310695652406191
Loss in iteration 127 : 0.9305809233902159
Loss in iteration 128 : 0.9300954437442285
Loss in iteration 129 : 0.9296147801287616
Loss in iteration 130 : 0.9291363398323608
Loss in iteration 131 : 0.9286582107499012
Loss in iteration 132 : 0.9281839212992995
Loss in iteration 133 : 0.9277110478830776
Loss in iteration 134 : 0.9272409858205116
Loss in iteration 135 : 0.9267724677135254
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082243
Loss in iteration 138 : 0.9253838076792628
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739476
Loss in iteration 141 : 0.9240078888647761
Loss in iteration 142 : 0.9235514384530814
Loss in iteration 143 : 0.9230946045257101
Loss in iteration 144 : 0.9226378387174597
Loss in iteration 145 : 0.9221806569055571
Loss in iteration 146 : 0.9217241933501489
Loss in iteration 147 : 0.9212687202551174
Loss in iteration 148 : 0.9208131844890628
Loss in iteration 149 : 0.9203572340558847
Loss in iteration 150 : 0.9199031582037847
Loss in iteration 151 : 0.9194513495399588
Loss in iteration 152 : 0.9189995040212228
Loss in iteration 153 : 0.9185479726262287
Loss in iteration 154 : 0.9180971823926425
Loss in iteration 155 : 0.9176477919076037
Loss in iteration 156 : 0.9171988772718441
Loss in iteration 157 : 0.9167508097553874
Loss in iteration 158 : 0.9163036803089585
Loss in iteration 159 : 0.9158567756420074
Loss in iteration 160 : 0.915411897700953
Loss in iteration 161 : 0.9149669701718814
Loss in iteration 162 : 0.9145250016896184
Loss in iteration 163 : 0.9140835362539845
Loss in iteration 164 : 0.9136431976261008
Loss in iteration 165 : 0.913203643781806
Loss in iteration 166 : 0.9127638919433501
Loss in iteration 167 : 0.9123240985898379
Loss in iteration 168 : 0.9118836296821612
Loss in iteration 169 : 0.9114434971424128
Loss in iteration 170 : 0.9110041982043735
Loss in iteration 171 : 0.9105655186486221
Loss in iteration 172 : 0.9101275270446161
Loss in iteration 173 : 0.9096900524816638
Loss in iteration 174 : 0.909252552265134
Loss in iteration 175 : 0.9088155499936313
Loss in iteration 176 : 0.9083794631841505
Loss in iteration 177 : 0.9079423132434219
Loss in iteration 178 : 0.9075043152484309
Loss in iteration 179 : 0.9070663591650334
Loss in iteration 180 : 0.9066278643844529
Loss in iteration 181 : 0.9061902002084037
Loss in iteration 182 : 0.90575261682308
Loss in iteration 183 : 0.9053142227477837
Loss in iteration 184 : 0.9048752314844969
Loss in iteration 185 : 0.9044351889099906
Loss in iteration 186 : 0.9039944390416038
Loss in iteration 187 : 0.9035536750934497
Loss in iteration 188 : 0.9031125484537375
Loss in iteration 189 : 0.9026718497227744
Loss in iteration 190 : 0.9022315665777417
Loss in iteration 191 : 0.9017921622328409
Loss in iteration 192 : 0.9013538286391164
Loss in iteration 193 : 0.9009151507017396
Loss in iteration 194 : 0.9004763855730444
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.8995966107246026
Loss in iteration 197 : 0.8991560274165029
Loss in iteration 198 : 0.8987144392382059
Loss in iteration 199 : 0.8982727192963393
Loss in iteration 200 : 0.897832213979834
Testing accuracy  of updater 4 on alg 1 with rate 1.0 = 0.6015, training accuracy 0.595, time elapsed: 3148 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 64.30045502706703
Loss in iteration 3 : 79.06506149471694
Loss in iteration 4 : 36.677579900242385
Loss in iteration 5 : 39.820089461413865
Loss in iteration 6 : 26.684949464211044
Loss in iteration 7 : 28.55588702662394
Loss in iteration 8 : 24.45032375982337
Loss in iteration 9 : 23.723246198465247
Loss in iteration 10 : 22.152359768722953
Loss in iteration 11 : 22.535489841433954
Loss in iteration 12 : 21.14255661384474
Loss in iteration 13 : 20.600424611749325
Loss in iteration 14 : 18.842393116305256
Loss in iteration 15 : 19.51376795019422
Loss in iteration 16 : 17.50030545830891
Loss in iteration 17 : 18.790475971246632
Loss in iteration 18 : 16.779874354714003
Loss in iteration 19 : 18.553853758522127
Loss in iteration 20 : 16.3138797410506
Loss in iteration 21 : 18.526475530791824
Loss in iteration 22 : 15.37985953388752
Loss in iteration 23 : 18.199576244343174
Loss in iteration 24 : 14.549514607847524
Loss in iteration 25 : 18.170106252601176
Loss in iteration 26 : 15.814090654505318
Loss in iteration 27 : 18.873393225672928
Loss in iteration 28 : 16.354635670332115
Loss in iteration 29 : 18.36283510078367
Loss in iteration 30 : 16.158626182935045
Loss in iteration 31 : 17.94143329930406
Loss in iteration 32 : 15.345260349497087
Loss in iteration 33 : 17.511447663718638
Loss in iteration 34 : 15.302173219873916
Loss in iteration 35 : 17.644363633830356
Loss in iteration 36 : 14.907976148503883
Loss in iteration 37 : 17.466702084712875
Loss in iteration 38 : 14.712031743720095
Loss in iteration 39 : 18.06858771547656
Loss in iteration 40 : 13.464446397460971
Loss in iteration 41 : 17.190993675228178
Loss in iteration 42 : 12.88711385301534
Loss in iteration 43 : 17.567378182103276
Loss in iteration 44 : 15.479936796952327
Loss in iteration 45 : 19.111431010839887
Loss in iteration 46 : 16.17922639829965
Loss in iteration 47 : 18.55869676865204
Loss in iteration 48 : 15.436026013858998
Loss in iteration 49 : 17.637918324162477
Loss in iteration 50 : 14.966666275635275
Loss in iteration 51 : 17.41119849365089
Loss in iteration 52 : 14.569666591596112
Loss in iteration 53 : 17.35136503694931
Loss in iteration 54 : 14.099328192706722
Loss in iteration 55 : 17.465071589823804
Loss in iteration 56 : 13.854938443704995
Loss in iteration 57 : 17.379584676485113
Loss in iteration 58 : 12.898932075109405
Loss in iteration 59 : 16.97606353942662
Loss in iteration 60 : 14.124642790638454
Loss in iteration 61 : 18.282106375160524
Loss in iteration 62 : 15.623251281350942
Loss in iteration 63 : 18.71018329423727
Loss in iteration 64 : 15.387185178520902
Loss in iteration 65 : 18.09659884637336
Loss in iteration 66 : 14.61078962986547
Loss in iteration 67 : 17.40727867194827
Loss in iteration 68 : 14.492027951862498
Loss in iteration 69 : 17.228529352048106
Loss in iteration 70 : 13.966372808265469
Loss in iteration 71 : 17.47946452025525
Loss in iteration 72 : 13.503937492134362
Loss in iteration 73 : 17.194725346823898
Loss in iteration 74 : 12.564858955149338
Loss in iteration 75 : 16.839828047123593
Loss in iteration 76 : 14.414734461386445
Loss in iteration 77 : 18.50296570835017
Loss in iteration 78 : 15.620644681613564
Loss in iteration 79 : 18.506238125724227
Loss in iteration 80 : 15.296020868787021
Loss in iteration 81 : 17.724959738661813
Loss in iteration 82 : 14.634578587434463
Loss in iteration 83 : 17.07643024515189
Loss in iteration 84 : 14.14946626782541
Loss in iteration 85 : 16.975317688540642
Loss in iteration 86 : 14.08966029538561
Loss in iteration 87 : 17.569760118238047
Loss in iteration 88 : 12.921374142516797
Loss in iteration 89 : 16.647446022296375
Loss in iteration 90 : 13.362434397923655
Loss in iteration 91 : 17.580229503268708
Loss in iteration 92 : 14.844754394019237
Loss in iteration 93 : 18.30630194573129
Loss in iteration 94 : 15.290220164926083
Loss in iteration 95 : 17.997840436256308
Loss in iteration 96 : 14.703917622532582
Loss in iteration 97 : 17.214168563728286
Loss in iteration 98 : 14.426259799471607
Loss in iteration 99 : 17.094449455601406
Loss in iteration 100 : 13.99045121729753
Loss in iteration 101 : 17.20343930751837
Loss in iteration 102 : 13.5238106331097
Loss in iteration 103 : 17.009850088881652
Loss in iteration 104 : 13.17802017369082
Loss in iteration 105 : 17.15576718147879
Loss in iteration 106 : 14.474665580048416
Loss in iteration 107 : 17.915634190754542
Loss in iteration 108 : 15.073737892212401
Loss in iteration 109 : 17.832544485104954
Loss in iteration 110 : 14.913449861032165
Loss in iteration 111 : 17.419812023204447
Loss in iteration 112 : 14.391614036463874
Loss in iteration 113 : 17.096430225397757
Loss in iteration 114 : 14.011098191217199
Loss in iteration 115 : 17.181855665529387
Loss in iteration 116 : 13.624974553589277
Loss in iteration 117 : 16.963098500157233
Loss in iteration 118 : 13.006982713404165
Loss in iteration 119 : 16.735907805804832
Loss in iteration 120 : 14.035570744459939
Loss in iteration 121 : 17.809357784366792
Loss in iteration 122 : 15.162772984631811
Loss in iteration 123 : 17.929878561544648
Loss in iteration 124 : 15.110336041438048
Loss in iteration 125 : 17.63306103714039
Loss in iteration 126 : 14.402494698881442
Loss in iteration 127 : 16.86547164989261
Loss in iteration 128 : 14.298775457089627
Loss in iteration 129 : 16.983757735201117
Loss in iteration 130 : 13.912985681199912
Loss in iteration 131 : 17.23674289623987
Loss in iteration 132 : 12.994688250603451
Loss in iteration 133 : 16.55006999754788
Loss in iteration 134 : 13.06908377647456
Loss in iteration 135 : 17.0863218351669
Loss in iteration 136 : 14.83504867609447
Loss in iteration 137 : 18.06747238260784
Loss in iteration 138 : 15.348886574511724
Loss in iteration 139 : 17.854659563145418
Loss in iteration 140 : 14.705527921845972
Loss in iteration 141 : 17.06575055461965
Loss in iteration 142 : 14.410061069335113
Loss in iteration 143 : 16.75470277411763
Loss in iteration 144 : 14.03791222977687
Loss in iteration 145 : 17.001374254851587
Loss in iteration 146 : 13.61339056158295
Loss in iteration 147 : 16.94469542816362
Loss in iteration 148 : 12.820145130925754
Loss in iteration 149 : 16.476378397211068
Loss in iteration 150 : 13.76697608722985
Loss in iteration 151 : 17.57075833994407
Loss in iteration 152 : 15.145823355285897
Loss in iteration 153 : 17.92117778546954
Loss in iteration 154 : 15.174174903967415
Loss in iteration 155 : 17.43429210945755
Loss in iteration 156 : 14.43972321963328
Loss in iteration 157 : 16.806164674401213
Loss in iteration 158 : 14.261909606302245
Loss in iteration 159 : 16.801813040292725
Loss in iteration 160 : 13.8996178376766
Loss in iteration 161 : 17.086872009302024
Loss in iteration 162 : 12.826097174875107
Loss in iteration 163 : 16.291355739852666
Loss in iteration 164 : 13.151298475424866
Loss in iteration 165 : 17.058036509140074
Loss in iteration 166 : 14.879974265971832
Loss in iteration 167 : 18.048700030966394
Loss in iteration 168 : 15.439550895964553
Loss in iteration 169 : 17.69744429829733
Loss in iteration 170 : 14.71451085771269
Loss in iteration 171 : 16.83902306713781
Loss in iteration 172 : 14.29967971358381
Loss in iteration 173 : 16.58170101268581
Loss in iteration 174 : 14.003083925360196
Loss in iteration 175 : 16.828085706713065
Loss in iteration 176 : 13.598115686385013
Loss in iteration 177 : 16.84416269789404
Loss in iteration 178 : 12.881579642065482
Loss in iteration 179 : 16.488551216133292
Loss in iteration 180 : 13.93928038271374
Loss in iteration 181 : 17.430409800367155
Loss in iteration 182 : 15.094030949867888
Loss in iteration 183 : 17.727905969414227
Loss in iteration 184 : 14.96000609415487
Loss in iteration 185 : 17.196478240026416
Loss in iteration 186 : 14.531421081570496
Loss in iteration 187 : 16.677603587515407
Loss in iteration 188 : 14.25968425455129
Loss in iteration 189 : 16.717458948830274
Loss in iteration 190 : 13.79344606098393
Loss in iteration 191 : 16.789669425172733
Loss in iteration 192 : 13.427530566743902
Loss in iteration 193 : 16.66228672405129
Loss in iteration 194 : 12.613032711696262
Loss in iteration 195 : 16.25531855519129
Loss in iteration 196 : 14.768365139208614
Loss in iteration 197 : 18.014604843360953
Loss in iteration 198 : 15.390029384091271
Loss in iteration 199 : 17.591848047715022
Loss in iteration 200 : 14.802687984256943
Testing accuracy  of updater 5 on alg 1 with rate 10.0 = 0.711375, training accuracy 0.722, time elapsed: 3343 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.283203151366727
Loss in iteration 3 : 7.525362058309006
Loss in iteration 4 : 2.7477080416994504
Loss in iteration 5 : 4.895420395778751
Loss in iteration 6 : 3.2083094772328105
Loss in iteration 7 : 3.561528324465902
Loss in iteration 8 : 3.31762659978941
Loss in iteration 9 : 2.797380349484285
Loss in iteration 10 : 3.5414546975730654
Loss in iteration 11 : 2.0507558523406617
Loss in iteration 12 : 3.490880614647716
Loss in iteration 13 : 1.8138492753877726
Loss in iteration 14 : 3.223868669800636
Loss in iteration 15 : 1.8702491653935502
Loss in iteration 16 : 2.9038120111661834
Loss in iteration 17 : 2.0134026648662253
Loss in iteration 18 : 2.6771286097009317
Loss in iteration 19 : 1.9518906433049008
Loss in iteration 20 : 1.8786876451541923
Loss in iteration 21 : 2.2099371715649276
Loss in iteration 22 : 2.4014137876627073
Loss in iteration 23 : 2.3746165822988186
Loss in iteration 24 : 2.4662527815019004
Loss in iteration 25 : 2.1041662149305367
Loss in iteration 26 : 2.2591313521558853
Loss in iteration 27 : 2.118740071299483
Loss in iteration 28 : 2.1174610330551387
Loss in iteration 29 : 2.098587599881074
Loss in iteration 30 : 2.0170304719932224
Loss in iteration 31 : 2.07642782306434
Loss in iteration 32 : 1.9401090005949877
Loss in iteration 33 : 2.089424621581009
Loss in iteration 34 : 1.8852594066957744
Loss in iteration 35 : 2.0775433778128702
Loss in iteration 36 : 1.844246683438445
Loss in iteration 37 : 2.089059665929958
Loss in iteration 38 : 1.7927156628157643
Loss in iteration 39 : 2.102879723714473
Loss in iteration 40 : 1.709748296127788
Loss in iteration 41 : 2.060674892065677
Loss in iteration 42 : 1.6778808082715262
Loss in iteration 43 : 2.14209942618138
Loss in iteration 44 : 1.8387262362261048
Loss in iteration 45 : 2.1631762693611183
Loss in iteration 46 : 1.8655458452463494
Loss in iteration 47 : 2.131170940247409
Loss in iteration 48 : 1.7917731854012284
Loss in iteration 49 : 2.092692984775341
Loss in iteration 50 : 1.8011207225426524
Loss in iteration 51 : 2.0904406067496244
Loss in iteration 52 : 1.7699020868271143
Loss in iteration 53 : 2.0666758041256377
Loss in iteration 54 : 1.7377557410681344
Loss in iteration 55 : 2.066723227700067
Loss in iteration 56 : 1.729141853028042
Loss in iteration 57 : 2.0890097357899067
Loss in iteration 58 : 1.717777370199706
Loss in iteration 59 : 2.1200118399525647
Loss in iteration 60 : 1.5633215156347977
Loss in iteration 61 : 2.030083084944723
Loss in iteration 62 : 1.6162636393003509
Loss in iteration 63 : 2.146172930704011
Loss in iteration 64 : 1.8514137976434277
Loss in iteration 65 : 2.1736082224586006
Loss in iteration 66 : 1.7897954266294118
Loss in iteration 67 : 2.126609845541574
Loss in iteration 68 : 1.7415012221156805
Loss in iteration 69 : 2.078316497483334
Loss in iteration 70 : 1.7174256095670164
Loss in iteration 71 : 2.058312135737063
Loss in iteration 72 : 1.7163233232382744
Loss in iteration 73 : 2.0665058213042737
Loss in iteration 74 : 1.7070389126463092
Loss in iteration 75 : 2.064252057729505
Loss in iteration 76 : 1.7003779434350008
Loss in iteration 77 : 2.086315197600897
Loss in iteration 78 : 1.6543994745814694
Loss in iteration 79 : 2.124112988149528
Loss in iteration 80 : 1.4858069819130824
Loss in iteration 81 : 1.9685731472850854
Loss in iteration 82 : 1.5964037734347898
Loss in iteration 83 : 2.131817439454698
Loss in iteration 84 : 1.8417318415450668
Loss in iteration 85 : 2.2220319357585017
Loss in iteration 86 : 1.7963242995991282
Loss in iteration 87 : 2.124009306775693
Loss in iteration 88 : 1.7478321186907377
Loss in iteration 89 : 2.0810855119393827
Loss in iteration 90 : 1.7160770603639894
Loss in iteration 91 : 2.051787727252533
Loss in iteration 92 : 1.7035570168997063
Loss in iteration 93 : 2.0348242348087946
Loss in iteration 94 : 1.7008250109880219
Loss in iteration 95 : 2.0428267499180106
Loss in iteration 96 : 1.6718952777182325
Loss in iteration 97 : 2.035362909341032
Loss in iteration 98 : 1.6512770748271914
Loss in iteration 99 : 2.0460270121432678
Loss in iteration 100 : 1.6812050482760832
Loss in iteration 101 : 2.055671204985334
Loss in iteration 102 : 1.5327882127682404
Loss in iteration 103 : 1.9812307526209636
Loss in iteration 104 : 1.7404371758050396
Loss in iteration 105 : 2.1652003965864197
Loss in iteration 106 : 1.7885590900906982
Loss in iteration 107 : 2.1226452538886895
Loss in iteration 108 : 1.7341805374150736
Loss in iteration 109 : 2.049437796480972
Loss in iteration 110 : 1.68854195818964
Loss in iteration 111 : 2.021422732466287
Loss in iteration 112 : 1.667275590524219
Loss in iteration 113 : 2.0384486165338385
Loss in iteration 114 : 1.6508395771760296
Loss in iteration 115 : 2.025521275877238
Loss in iteration 116 : 1.680388703033998
Loss in iteration 117 : 2.0996458594584806
Loss in iteration 118 : 1.5347663557965652
Loss in iteration 119 : 1.9323508107951184
Loss in iteration 120 : 1.551774483289929
Loss in iteration 121 : 2.0504250949279244
Loss in iteration 122 : 1.815954373088182
Loss in iteration 123 : 2.2063771083389248
Loss in iteration 124 : 1.797599348615721
Loss in iteration 125 : 2.1165976541012834
Loss in iteration 126 : 1.730351510774386
Loss in iteration 127 : 2.033124709571096
Loss in iteration 128 : 1.707857681419684
Loss in iteration 129 : 2.000452095037876
Loss in iteration 130 : 1.6825805536763105
Loss in iteration 131 : 2.0066834364114885
Loss in iteration 132 : 1.6736405072598113
Loss in iteration 133 : 1.9949419079234025
Loss in iteration 134 : 1.6661787385446587
Loss in iteration 135 : 2.0177889239795648
Loss in iteration 136 : 1.6761631044773966
Loss in iteration 137 : 2.0518718897305486
Loss in iteration 138 : 1.6640519378056142
Loss in iteration 139 : 1.9877006866753053
Loss in iteration 140 : 1.56617760058395
Loss in iteration 141 : 1.940755158296005
Loss in iteration 142 : 1.7254068957864568
Loss in iteration 143 : 2.132250445724334
Loss in iteration 144 : 1.7893420291288815
Loss in iteration 145 : 2.108012294870345
Loss in iteration 146 : 1.737990850508596
Loss in iteration 147 : 2.025414568828349
Loss in iteration 148 : 1.7003558239815
Loss in iteration 149 : 1.9999221590287894
Loss in iteration 150 : 1.7002890453499024
Loss in iteration 151 : 2.001629615773591
Loss in iteration 152 : 1.6622988764911615
Loss in iteration 153 : 1.9808766469589367
Loss in iteration 154 : 1.6626138945683082
Loss in iteration 155 : 2.015809037217704
Loss in iteration 156 : 1.6425206550926665
Loss in iteration 157 : 2.0361813972151044
Loss in iteration 158 : 1.578305749679368
Loss in iteration 159 : 1.9524286948951708
Loss in iteration 160 : 1.6396229728622203
Loss in iteration 161 : 2.0677218696087856
Loss in iteration 162 : 1.7935925155038435
Loss in iteration 163 : 2.1207539059504943
Loss in iteration 164 : 1.7866640330013737
Loss in iteration 165 : 2.0614334277871285
Loss in iteration 166 : 1.7128367549983123
Loss in iteration 167 : 1.9804074177468594
Loss in iteration 168 : 1.722409060639567
Loss in iteration 169 : 1.9936169844222353
Loss in iteration 170 : 1.6673993277437005
Loss in iteration 171 : 1.9562931608243703
Loss in iteration 172 : 1.6876029138594668
Loss in iteration 173 : 1.9915822555102245
Loss in iteration 174 : 1.6878442577396178
Loss in iteration 175 : 2.0116284578600467
Loss in iteration 176 : 1.624633484695856
Loss in iteration 177 : 1.962719716438262
Loss in iteration 178 : 1.6159185650415189
Loss in iteration 179 : 1.9754966153738254
Loss in iteration 180 : 1.7305684291714998
Loss in iteration 181 : 2.0775785109266827
Loss in iteration 182 : 1.8172339994413258
Loss in iteration 183 : 2.0647935898075276
Loss in iteration 184 : 1.7328814750760202
Loss in iteration 185 : 1.9769084546489748
Loss in iteration 186 : 1.7358772404941174
Loss in iteration 187 : 1.974905395387194
Loss in iteration 188 : 1.6617788722867444
Loss in iteration 189 : 1.96170892941071
Loss in iteration 190 : 1.6807238775714033
Loss in iteration 191 : 1.9809690571106162
Loss in iteration 192 : 1.6492134375339242
Loss in iteration 193 : 2.0038104142801365
Loss in iteration 194 : 1.6058421845006157
Loss in iteration 195 : 1.9980514610878284
Loss in iteration 196 : 1.5640160687501032
Loss in iteration 197 : 1.9597206374929355
Loss in iteration 198 : 1.7578483015509223
Loss in iteration 199 : 2.1234307624703996
Loss in iteration 200 : 1.8003049857997029
Testing accuracy  of updater 5 on alg 1 with rate 1.0 = 0.71325, training accuracy 0.725, time elapsed: 3344 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.914670140246707
Loss in iteration 3 : 0.8900497745224872
Loss in iteration 4 : 0.8656480866080274
Loss in iteration 5 : 0.8406919126770374
Loss in iteration 6 : 0.8152151002307307
Loss in iteration 7 : 0.788951060202002
Loss in iteration 8 : 0.7628444004960916
Loss in iteration 9 : 0.7375228104105084
Loss in iteration 10 : 0.7132525567925118
Loss in iteration 11 : 0.6913866582595313
Loss in iteration 12 : 0.6774707048768601
Loss in iteration 13 : 0.6961254892883135
Loss in iteration 14 : 0.709452365581042
Loss in iteration 15 : 0.8831118772092802
Loss in iteration 16 : 0.7564966108325698
Loss in iteration 17 : 0.8319058107046956
Loss in iteration 18 : 0.6628172507722345
Loss in iteration 19 : 0.7001826807412811
Loss in iteration 20 : 0.647731912394868
Loss in iteration 21 : 0.6738025612129636
Loss in iteration 22 : 0.6478481153032706
Loss in iteration 23 : 0.6823891284357128
Loss in iteration 24 : 0.6452241814446906
Loss in iteration 25 : 0.6800364994067217
Loss in iteration 26 : 0.6420509234375572
Loss in iteration 27 : 0.6724287185643568
Loss in iteration 28 : 0.634078896824255
Loss in iteration 29 : 0.6592518363746901
Loss in iteration 30 : 0.6303148550356066
Loss in iteration 31 : 0.6516147771484676
Loss in iteration 32 : 0.624936329352638
Loss in iteration 33 : 0.6412273506964759
Loss in iteration 34 : 0.6190155930864021
Loss in iteration 35 : 0.6363957977617999
Loss in iteration 36 : 0.6162125101330441
Loss in iteration 37 : 0.6311570746294684
Loss in iteration 38 : 0.6168851324695138
Loss in iteration 39 : 0.6333730993908671
Loss in iteration 40 : 0.6184761000528443
Loss in iteration 41 : 0.6353113864227516
Loss in iteration 42 : 0.615479780508152
Loss in iteration 43 : 0.6282969627461332
Loss in iteration 44 : 0.610119321794904
Loss in iteration 45 : 0.622732808760143
Loss in iteration 46 : 0.6065012342962149
Loss in iteration 47 : 0.6196364933649811
Loss in iteration 48 : 0.6035403039098256
Loss in iteration 49 : 0.6156529284864891
Loss in iteration 50 : 0.6046684583071779
Loss in iteration 51 : 0.6226902459944627
Loss in iteration 52 : 0.6007151777829133
Loss in iteration 53 : 0.6122297895317096
Loss in iteration 54 : 0.5979276365522128
Loss in iteration 55 : 0.6103101808198252
Loss in iteration 56 : 0.6001217268850524
Loss in iteration 57 : 0.6111208429418343
Loss in iteration 58 : 0.6000032106849063
Loss in iteration 59 : 0.6088933513133941
Loss in iteration 60 : 0.5977353276086407
Loss in iteration 61 : 0.6056601136262231
Loss in iteration 62 : 0.5966959674470284
Loss in iteration 63 : 0.6044139470010311
Loss in iteration 64 : 0.5947907174823311
Loss in iteration 65 : 0.6044578364306652
Loss in iteration 66 : 0.5912831116110819
Loss in iteration 67 : 0.5993948909705215
Loss in iteration 68 : 0.5877993837479475
Loss in iteration 69 : 0.5984718905923907
Loss in iteration 70 : 0.5881381252114238
Loss in iteration 71 : 0.5971317508492578
Loss in iteration 72 : 0.5873929428632308
Loss in iteration 73 : 0.5974395624691964
Loss in iteration 74 : 0.5872394569322605
Loss in iteration 75 : 0.5975111911464426
Loss in iteration 76 : 0.5872952640596214
Loss in iteration 77 : 0.5969943024723003
Loss in iteration 78 : 0.5874421770503315
Loss in iteration 79 : 0.5967495439804352
Loss in iteration 80 : 0.5860978113187013
Loss in iteration 81 : 0.5938401730149502
Loss in iteration 82 : 0.5846336901954416
Loss in iteration 83 : 0.5933401056800964
Loss in iteration 84 : 0.5848747891174064
Loss in iteration 85 : 0.5942043213313266
Loss in iteration 86 : 0.5856580994564828
Loss in iteration 87 : 0.5961744014329137
Loss in iteration 88 : 0.5847338196750486
Loss in iteration 89 : 0.5911160393146979
Loss in iteration 90 : 0.5823826177944784
Loss in iteration 91 : 0.5903127391857137
Loss in iteration 92 : 0.5834774076382475
Loss in iteration 93 : 0.5888093481402347
Loss in iteration 94 : 0.5819042150885878
Loss in iteration 95 : 0.5884251919690515
Loss in iteration 96 : 0.5808086604205212
Loss in iteration 97 : 0.5872155024337607
Loss in iteration 98 : 0.5792882961930644
Loss in iteration 99 : 0.5867864020353496
Loss in iteration 100 : 0.5793809656580525
Loss in iteration 101 : 0.5870733423837812
Loss in iteration 102 : 0.5803174353865306
Loss in iteration 103 : 0.5862525525975169
Loss in iteration 104 : 0.579492038689626
Loss in iteration 105 : 0.5869867235725273
Loss in iteration 106 : 0.5786803216389774
Loss in iteration 107 : 0.5863980063944292
Loss in iteration 108 : 0.5791883655190434
Loss in iteration 109 : 0.5855835433947201
Loss in iteration 110 : 0.5782369979644816
Loss in iteration 111 : 0.5840668984873141
Loss in iteration 112 : 0.5780278002268117
Loss in iteration 113 : 0.5852183791339495
Loss in iteration 114 : 0.5791963563717805
Loss in iteration 115 : 0.584428763245094
Loss in iteration 116 : 0.5778193045675553
Loss in iteration 117 : 0.5817294929134014
Loss in iteration 118 : 0.5757040893249595
Loss in iteration 119 : 0.5806518756605249
Loss in iteration 120 : 0.5752691665686193
Loss in iteration 121 : 0.580795570191327
Loss in iteration 122 : 0.5734609312312821
Loss in iteration 123 : 0.5776215201359253
Loss in iteration 124 : 0.5724472684193966
Loss in iteration 125 : 0.5774315659359963
Loss in iteration 126 : 0.574359490352405
Loss in iteration 127 : 0.5783395996416778
Loss in iteration 128 : 0.5738498267320208
Loss in iteration 129 : 0.5768559908422721
Loss in iteration 130 : 0.5732717070007075
Loss in iteration 131 : 0.5783741981846089
Loss in iteration 132 : 0.5751101211692674
Loss in iteration 133 : 0.5806099493283232
Loss in iteration 134 : 0.576362291481957
Loss in iteration 135 : 0.579301835025888
Loss in iteration 136 : 0.5734294206933565
Loss in iteration 137 : 0.5775083988038516
Loss in iteration 138 : 0.5739878580416935
Loss in iteration 139 : 0.5769097837278212
Loss in iteration 140 : 0.5730618601962479
Loss in iteration 141 : 0.5758958916608283
Loss in iteration 142 : 0.5722291886864537
Loss in iteration 143 : 0.5751374064230432
Loss in iteration 144 : 0.5722461947000056
Loss in iteration 145 : 0.5752804033114366
Loss in iteration 146 : 0.5720452887776886
Loss in iteration 147 : 0.5737940032315741
Loss in iteration 148 : 0.5713780080829599
Loss in iteration 149 : 0.5737551461786372
Loss in iteration 150 : 0.5713747081120041
Loss in iteration 151 : 0.574639558236571
Loss in iteration 152 : 0.5715183353321682
Loss in iteration 153 : 0.573705752130902
Loss in iteration 154 : 0.5702867401574849
Loss in iteration 155 : 0.5722706456734802
Loss in iteration 156 : 0.5695962279325147
Loss in iteration 157 : 0.5724090412933992
Loss in iteration 158 : 0.5698860436288831
Loss in iteration 159 : 0.5721375669434265
Loss in iteration 160 : 0.5690589020288601
Loss in iteration 161 : 0.5726580065648983
Loss in iteration 162 : 0.5700635828726176
Loss in iteration 163 : 0.5722614647497328
Loss in iteration 164 : 0.5697525307451761
Loss in iteration 165 : 0.5715384265395673
Loss in iteration 166 : 0.5692681407758389
Loss in iteration 167 : 0.5716457191030293
Loss in iteration 168 : 0.5689750175609709
Loss in iteration 169 : 0.5713397160973648
Loss in iteration 170 : 0.5682716414376573
Loss in iteration 171 : 0.5707440292714663
Loss in iteration 172 : 0.5684680946455513
Loss in iteration 173 : 0.5702379420597001
Loss in iteration 174 : 0.5679080724705651
Loss in iteration 175 : 0.5697652399052211
Loss in iteration 176 : 0.566852729029262
Loss in iteration 177 : 0.5696023587833381
Loss in iteration 178 : 0.5671984576836645
Loss in iteration 179 : 0.569697335707652
Loss in iteration 180 : 0.5677860973217166
Loss in iteration 181 : 0.5696522350687389
Loss in iteration 182 : 0.566498786116217
Loss in iteration 183 : 0.56722537042763
Loss in iteration 184 : 0.5660772861045769
Loss in iteration 185 : 0.5694955381949228
Loss in iteration 186 : 0.5669212805435242
Loss in iteration 187 : 0.5674003020188355
Loss in iteration 188 : 0.5673388234295879
Loss in iteration 189 : 0.570918134105495
Loss in iteration 190 : 0.5679906897268437
Loss in iteration 191 : 0.5699686324536435
Loss in iteration 192 : 0.5661884231780279
Loss in iteration 193 : 0.5688386379175304
Loss in iteration 194 : 0.5650995363282102
Loss in iteration 195 : 0.5677585435321489
Loss in iteration 196 : 0.5642613607696437
Loss in iteration 197 : 0.56696121276653
Loss in iteration 198 : 0.5644038004466684
Loss in iteration 199 : 0.5678025835550673
Loss in iteration 200 : 0.5633900427456222
Testing accuracy  of updater 5 on alg 1 with rate 0.09999999999999998 = 0.765, training accuracy 0.76, time elapsed: 3429 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.460044302182312
Loss in iteration 3 : 2.2244825109156827
Loss in iteration 4 : 2.0088085444956767
Loss in iteration 5 : 1.234012072936522
Loss in iteration 6 : 1.3590801679122244
Loss in iteration 7 : 0.9807856666172464
Loss in iteration 8 : 1.466392703898998
Loss in iteration 9 : 1.1323154234982136
Loss in iteration 10 : 1.1628364404795293
Loss in iteration 11 : 1.2684883983739539
Loss in iteration 12 : 0.9119434604656899
Loss in iteration 13 : 0.9922206494129455
Loss in iteration 14 : 0.9980408901016481
Loss in iteration 15 : 0.8784500343724315
Loss in iteration 16 : 1.0564179973084709
Loss in iteration 17 : 0.9513930005350312
Loss in iteration 18 : 0.8689812880881195
Loss in iteration 19 : 0.9681013683195133
Loss in iteration 20 : 0.8596155148976407
Loss in iteration 21 : 0.8165591262843809
Loss in iteration 22 : 0.8346791088194923
Loss in iteration 23 : 0.7586721605025527
Loss in iteration 24 : 0.8305721900733606
Loss in iteration 25 : 0.8018700881761608
Loss in iteration 26 : 0.7611599883645147
Loss in iteration 27 : 0.7547368263687115
Loss in iteration 28 : 0.6626662050745735
Loss in iteration 29 : 0.6993242528426133
Loss in iteration 30 : 0.6783497624572121
Loss in iteration 31 : 0.6994341542406785
Loss in iteration 32 : 0.638747905945967
Loss in iteration 33 : 0.645862597020047
Loss in iteration 34 : 0.6219730364717657
Loss in iteration 35 : 0.6293912854012867
Loss in iteration 36 : 0.6017394520929934
Loss in iteration 37 : 0.5998166665795431
Loss in iteration 38 : 0.6115250355575771
Loss in iteration 39 : 0.5901698082327739
Loss in iteration 40 : 0.5797088419335801
Loss in iteration 41 : 0.5933429438777924
Loss in iteration 42 : 0.5947884889895049
Loss in iteration 43 : 0.580198983315943
Loss in iteration 44 : 0.5589713758298138
Loss in iteration 45 : 0.5702359413359221
Loss in iteration 46 : 0.5934146724079488
Loss in iteration 47 : 0.5983104356360306
Loss in iteration 48 : 0.5911466030632161
Loss in iteration 49 : 0.5985790152084179
Loss in iteration 50 : 0.5981369224492716
Loss in iteration 51 : 0.5781464077956558
Loss in iteration 52 : 0.5644237436266051
Loss in iteration 53 : 0.5443724594566106
Loss in iteration 54 : 0.5358226708106725
Loss in iteration 55 : 0.5288357931675152
Loss in iteration 56 : 0.5299581743561714
Loss in iteration 57 : 0.522023021049818
Loss in iteration 58 : 0.5292982132137922
Loss in iteration 59 : 0.5485692425780592
Loss in iteration 60 : 0.5898345899975175
Loss in iteration 61 : 0.6613812040040666
Loss in iteration 62 : 0.7649324649301298
Loss in iteration 63 : 0.6748476529164428
Loss in iteration 64 : 0.5595616187976074
Loss in iteration 65 : 0.5246154861289274
Loss in iteration 66 : 0.5650351200840176
Loss in iteration 67 : 0.635856299411628
Loss in iteration 68 : 0.6505866524952175
Loss in iteration 69 : 0.5870332738712054
Loss in iteration 70 : 0.529021912976193
Loss in iteration 71 : 0.5234227977741439
Loss in iteration 72 : 0.5378841438571494
Loss in iteration 73 : 0.6002567880194135
Loss in iteration 74 : 0.697488665024244
Loss in iteration 75 : 0.7098542314065905
Loss in iteration 76 : 0.5906480738276662
Loss in iteration 77 : 0.5235858813235678
Loss in iteration 78 : 0.5379541991130161
Loss in iteration 79 : 0.6090959574736944
Loss in iteration 80 : 0.6552447617383241
Loss in iteration 81 : 0.5987266359743126
Loss in iteration 82 : 0.5313910676617819
Loss in iteration 83 : 0.5218746626266477
Loss in iteration 84 : 0.5576252106654901
Loss in iteration 85 : 0.6098909302945326
Loss in iteration 86 : 0.6472641375026095
Loss in iteration 87 : 0.5897965655392174
Loss in iteration 88 : 0.5305719737790896
Loss in iteration 89 : 0.5180207281105931
Loss in iteration 90 : 0.5343684219244946
Loss in iteration 91 : 0.560753230734058
Loss in iteration 92 : 0.6098028746816685
Loss in iteration 93 : 0.6226470501984774
Loss in iteration 94 : 0.5787838131211835
Loss in iteration 95 : 0.5340279244146697
Loss in iteration 96 : 0.5221908764852647
Loss in iteration 97 : 0.5250820006982939
Loss in iteration 98 : 0.5584170828625769
Loss in iteration 99 : 0.604641186746093
Loss in iteration 100 : 0.61789934548921
Loss in iteration 101 : 0.5929759320320246
Loss in iteration 102 : 0.5517778027780389
Loss in iteration 103 : 0.5391477256456624
Loss in iteration 104 : 0.5227751261856838
Loss in iteration 105 : 0.5187683160023617
Loss in iteration 106 : 0.518687677827196
Loss in iteration 107 : 0.5156997358619342
Loss in iteration 108 : 0.5276293690530388
Loss in iteration 109 : 0.533003769420651
Loss in iteration 110 : 0.5541977526270035
Loss in iteration 111 : 0.6387049836653135
Loss in iteration 112 : 0.7094939597692056
Loss in iteration 113 : 0.6842476084756904
Loss in iteration 114 : 0.5633673858611089
Loss in iteration 115 : 0.5176237902228368
Loss in iteration 116 : 0.5838709811152543
Loss in iteration 117 : 0.6301766942522297
Loss in iteration 118 : 0.6367182450314405
Loss in iteration 119 : 0.557532607260791
Loss in iteration 120 : 0.5268219748414273
Loss in iteration 121 : 0.6082978140945977
Loss in iteration 122 : 0.6060467558765703
Loss in iteration 123 : 0.6187869295173299
Loss in iteration 124 : 0.5299328530967734
Loss in iteration 125 : 0.5376141167720124
Loss in iteration 126 : 0.5408754812197759
Loss in iteration 127 : 0.5664754303163936
Loss in iteration 128 : 0.5777501360344659
Loss in iteration 129 : 0.5626520944828549
Loss in iteration 130 : 0.5396682239088013
Loss in iteration 131 : 0.522000761595393
Loss in iteration 132 : 0.5193611712996767
Loss in iteration 133 : 0.5142134137978951
Loss in iteration 134 : 0.5134936049749622
Loss in iteration 135 : 0.5143459310575629
Loss in iteration 136 : 0.5098535864685715
Loss in iteration 137 : 0.514523202128903
Loss in iteration 138 : 0.5301639808719082
Loss in iteration 139 : 0.5859829451429552
Loss in iteration 140 : 0.76591326950274
Loss in iteration 141 : 0.7943534493912764
Loss in iteration 142 : 0.6169039273695335
Loss in iteration 143 : 0.5232493663114495
Loss in iteration 144 : 0.5472544347102218
Loss in iteration 145 : 0.650156532099219
Loss in iteration 146 : 0.6558302923966471
Loss in iteration 147 : 0.5590097388589492
Loss in iteration 148 : 0.5206290252692655
Loss in iteration 149 : 0.5621518502880879
Loss in iteration 150 : 0.6369823843598131
Loss in iteration 151 : 0.6062500591100707
Loss in iteration 152 : 0.5415468769224507
Loss in iteration 153 : 0.5203152306909292
Loss in iteration 154 : 0.5458768658678064
Loss in iteration 155 : 0.5925882529038937
Loss in iteration 156 : 0.594957703791293
Loss in iteration 157 : 0.5516104868727517
Loss in iteration 158 : 0.5274709477563622
Loss in iteration 159 : 0.5238858305536748
Loss in iteration 160 : 0.5536843500489321
Loss in iteration 161 : 0.5919487554306033
Loss in iteration 162 : 0.5865144904679109
Loss in iteration 163 : 0.5703676362349939
Loss in iteration 164 : 0.5484446573062278
Loss in iteration 165 : 0.5424833890604357
Loss in iteration 166 : 0.5297349360312857
Loss in iteration 167 : 0.5367681376289372
Loss in iteration 168 : 0.5303289399997425
Loss in iteration 169 : 0.5670655014427025
Loss in iteration 170 : 0.6011124551323199
Loss in iteration 171 : 0.6507944120302105
Loss in iteration 172 : 0.6681204205707124
Loss in iteration 173 : 0.5952593448429399
Loss in iteration 174 : 0.5361997129528343
Loss in iteration 175 : 0.5300583176103492
Loss in iteration 176 : 0.5374091694563629
Loss in iteration 177 : 0.5974322411474317
Loss in iteration 178 : 0.6063697981699611
Loss in iteration 179 : 0.6053069001553117
Loss in iteration 180 : 0.5425658245671191
Loss in iteration 181 : 0.5296016099590358
Loss in iteration 182 : 0.5210424956115806
Loss in iteration 183 : 0.5331724806969254
Loss in iteration 184 : 0.561566743806332
Loss in iteration 185 : 0.5875239556515776
Loss in iteration 186 : 0.5990399798620217
Loss in iteration 187 : 0.5679030013620384
Loss in iteration 188 : 0.5361298440893466
Loss in iteration 189 : 0.521608736088573
Loss in iteration 190 : 0.5169791286832445
Loss in iteration 191 : 0.5133696361907063
Loss in iteration 192 : 0.5151353031896888
Loss in iteration 193 : 0.5189152527297927
Loss in iteration 194 : 0.5220096243834517
Loss in iteration 195 : 0.5597040437248463
Loss in iteration 196 : 0.665353322279056
Loss in iteration 197 : 0.7650102622666299
Loss in iteration 198 : 0.6765454692753355
Loss in iteration 199 : 0.5529176044337725
Loss in iteration 200 : 0.5160601753893997
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.775625, training accuracy 0.769, time elapsed: 3204 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9586565706569578
Loss in iteration 3 : 0.9153967064211498
Loss in iteration 4 : 0.9235793559332917
Loss in iteration 5 : 0.8940406637610682
Loss in iteration 6 : 0.8396914259198907
Loss in iteration 7 : 0.7996923556585355
Loss in iteration 8 : 0.7681023136421309
Loss in iteration 9 : 0.7490057323652375
Loss in iteration 10 : 0.698232503919585
Loss in iteration 11 : 0.650083732786294
Loss in iteration 12 : 0.6354068268980497
Loss in iteration 13 : 0.6245548572404425
Loss in iteration 14 : 0.6013687282288182
Loss in iteration 15 : 0.5817955343612161
Loss in iteration 16 : 0.578402963453366
Loss in iteration 17 : 0.5752444771012939
Loss in iteration 18 : 0.5634915555705615
Loss in iteration 19 : 0.5563171470395999
Loss in iteration 20 : 0.5568957803187731
Loss in iteration 21 : 0.5555718105216588
Loss in iteration 22 : 0.5499091839539542
Loss in iteration 23 : 0.5469241474853648
Loss in iteration 24 : 0.548132726032706
Loss in iteration 25 : 0.5478455282888507
Loss in iteration 26 : 0.5442382555452921
Loss in iteration 27 : 0.5413357296970055
Loss in iteration 28 : 0.5412299991839864
Loss in iteration 29 : 0.5412787482228898
Loss in iteration 30 : 0.5394061781211653
Loss in iteration 31 : 0.5372880158151695
Loss in iteration 32 : 0.53676439630059
Loss in iteration 33 : 0.5368670473124475
Loss in iteration 34 : 0.5358029815598202
Loss in iteration 35 : 0.5340735766911564
Loss in iteration 36 : 0.5332010055019059
Loss in iteration 37 : 0.5328492124305031
Loss in iteration 38 : 0.531821672650961
Loss in iteration 39 : 0.5302623706423844
Loss in iteration 40 : 0.5293171779154066
Loss in iteration 41 : 0.5291027168905152
Loss in iteration 42 : 0.5282061379710908
Loss in iteration 43 : 0.526736471683071
Loss in iteration 44 : 0.525766622073044
Loss in iteration 45 : 0.5254203494053582
Loss in iteration 46 : 0.5247601807385723
Loss in iteration 47 : 0.5236092397400098
Loss in iteration 48 : 0.5226266937142559
Loss in iteration 49 : 0.5222461034359699
Loss in iteration 50 : 0.5217017684721769
Loss in iteration 51 : 0.520797790176269
Loss in iteration 52 : 0.5201535209779844
Loss in iteration 53 : 0.5197435566881066
Loss in iteration 54 : 0.5192697031983295
Loss in iteration 55 : 0.5186326880725882
Loss in iteration 56 : 0.5181107754468216
Loss in iteration 57 : 0.517765812567744
Loss in iteration 58 : 0.5173795819972047
Loss in iteration 59 : 0.5169043091911626
Loss in iteration 60 : 0.5164882636180373
Loss in iteration 61 : 0.5162129759007879
Loss in iteration 62 : 0.5159555126415524
Loss in iteration 63 : 0.5156171276423688
Loss in iteration 64 : 0.5152897956001102
Loss in iteration 65 : 0.5150850722425251
Loss in iteration 66 : 0.5148947348538916
Loss in iteration 67 : 0.5146293892430269
Loss in iteration 68 : 0.514382490027846
Loss in iteration 69 : 0.5142218022514687
Loss in iteration 70 : 0.5140716131964412
Loss in iteration 71 : 0.5138859763154267
Loss in iteration 72 : 0.5137037341962163
Loss in iteration 73 : 0.5135595173750321
Loss in iteration 74 : 0.5134180012902075
Loss in iteration 75 : 0.5132638060623171
Loss in iteration 76 : 0.5131129644219131
Loss in iteration 77 : 0.512982601965718
Loss in iteration 78 : 0.5128599321262295
Loss in iteration 79 : 0.5127249092094651
Loss in iteration 80 : 0.5126059826819911
Loss in iteration 81 : 0.5125092329087679
Loss in iteration 82 : 0.5124033303462519
Loss in iteration 83 : 0.5122954885188017
Loss in iteration 84 : 0.5122046204638204
Loss in iteration 85 : 0.5121132004317424
Loss in iteration 86 : 0.5120145111423727
Loss in iteration 87 : 0.5119288447117246
Loss in iteration 88 : 0.5118520865241372
Loss in iteration 89 : 0.5117660783077362
Loss in iteration 90 : 0.5116900993952136
Loss in iteration 91 : 0.5116165974542932
Loss in iteration 92 : 0.511541933121766
Loss in iteration 93 : 0.511472579226533
Loss in iteration 94 : 0.5114049228767016
Loss in iteration 95 : 0.5113373962499959
Loss in iteration 96 : 0.5112708159923149
Loss in iteration 97 : 0.5112045235731305
Loss in iteration 98 : 0.5111437142574669
Loss in iteration 99 : 0.5110846136801698
Loss in iteration 100 : 0.5110253271761289
Loss in iteration 101 : 0.5109670237650431
Loss in iteration 102 : 0.5109110300452753
Loss in iteration 103 : 0.5108573948042722
Loss in iteration 104 : 0.5108054903482503
Loss in iteration 105 : 0.5107537650867748
Loss in iteration 106 : 0.5107058259478477
Loss in iteration 107 : 0.5106595399688052
Loss in iteration 108 : 0.5106122355955584
Loss in iteration 109 : 0.510564677648896
Loss in iteration 110 : 0.5105200535890212
Loss in iteration 111 : 0.5104774276030084
Loss in iteration 112 : 0.5104351284587261
Loss in iteration 113 : 0.5103944287574899
Loss in iteration 114 : 0.5103534453557602
Loss in iteration 115 : 0.5103138386727051
Loss in iteration 116 : 0.5102745455530036
Loss in iteration 117 : 0.5102364937683097
Loss in iteration 118 : 0.5101997498073679
Loss in iteration 119 : 0.510161845102218
Loss in iteration 120 : 0.5101269032682488
Loss in iteration 121 : 0.5100921513449762
Loss in iteration 122 : 0.5100569729329733
Loss in iteration 123 : 0.5100231937673663
Loss in iteration 124 : 0.5099896963834688
Loss in iteration 125 : 0.5099563346763077
Loss in iteration 126 : 0.5099229006553773
Loss in iteration 127 : 0.5098909809712128
Loss in iteration 128 : 0.509858506163723
Loss in iteration 129 : 0.5098267483779044
Loss in iteration 130 : 0.5097967835207858
Loss in iteration 131 : 0.5097668728745633
Loss in iteration 132 : 0.5097377564178722
Loss in iteration 133 : 0.5097104030443893
Loss in iteration 134 : 0.5096817284147976
Loss in iteration 135 : 0.5096541030654328
Loss in iteration 136 : 0.5096280091145317
Loss in iteration 137 : 0.5096019679546565
Loss in iteration 138 : 0.5095752550879183
Loss in iteration 139 : 0.5095490725434635
Loss in iteration 140 : 0.5095232111691057
Loss in iteration 141 : 0.5094988148562856
Loss in iteration 142 : 0.5094736819978674
Loss in iteration 143 : 0.5094490586528269
Loss in iteration 144 : 0.5094246513162395
Loss in iteration 145 : 0.5094008697622419
Loss in iteration 146 : 0.5093778440927711
Loss in iteration 147 : 0.5093536518052348
Loss in iteration 148 : 0.5093314221821285
Loss in iteration 149 : 0.5093092189542847
Loss in iteration 150 : 0.5092871177300909
Loss in iteration 151 : 0.5092652734271313
Loss in iteration 152 : 0.5092438431912824
Loss in iteration 153 : 0.509222626018602
Loss in iteration 154 : 0.5092023683310534
Loss in iteration 155 : 0.50918249776003
Loss in iteration 156 : 0.5091616081369176
Loss in iteration 157 : 0.5091417546948381
Loss in iteration 158 : 0.5091230922970122
Loss in iteration 159 : 0.509105562981527
Loss in iteration 160 : 0.5090859791256216
Loss in iteration 161 : 0.5090694581210102
Loss in iteration 162 : 0.5090511680896259
Loss in iteration 163 : 0.5090332440405738
Loss in iteration 164 : 0.5090167177301337
Loss in iteration 165 : 0.5089986523799434
Loss in iteration 166 : 0.5089821016256808
Loss in iteration 167 : 0.5089649162494638
Loss in iteration 168 : 0.5089488662071008
Loss in iteration 169 : 0.5089325194475353
Loss in iteration 170 : 0.508916385442129
Loss in iteration 171 : 0.5089004719029486
Loss in iteration 172 : 0.5088839179761215
Loss in iteration 173 : 0.5088679666702274
Loss in iteration 174 : 0.5088521909987179
Loss in iteration 175 : 0.5088366810840028
Loss in iteration 176 : 0.5088211265162527
Loss in iteration 177 : 0.5088059801399124
Loss in iteration 178 : 0.5087904395498786
Loss in iteration 179 : 0.508775354953012
Loss in iteration 180 : 0.5087604387021935
Loss in iteration 181 : 0.5087452999760956
Loss in iteration 182 : 0.508730250239348
Loss in iteration 183 : 0.5087157292159398
Loss in iteration 184 : 0.508700492825475
Loss in iteration 185 : 0.5086859366704425
Loss in iteration 186 : 0.5086711595085256
Loss in iteration 187 : 0.5086567932333812
Loss in iteration 188 : 0.5086425554082032
Loss in iteration 189 : 0.5086285175847552
Loss in iteration 190 : 0.5086142019874142
Loss in iteration 191 : 0.5086006874344561
Loss in iteration 192 : 0.5085867407244892
Loss in iteration 193 : 0.5085728562069152
Loss in iteration 194 : 0.5085595017685799
Loss in iteration 195 : 0.5085459200387155
Loss in iteration 196 : 0.5085320483957377
Loss in iteration 197 : 0.5085188391407489
Loss in iteration 198 : 0.5085055690653042
Loss in iteration 199 : 0.5084924778144744
Loss in iteration 200 : 0.508478839541549
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.78775, training accuracy 0.781, time elapsed: 4170 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9948904156307504
Loss in iteration 3 : 0.9852809910674648
Loss in iteration 4 : 0.9717110627192994
Loss in iteration 5 : 0.9550888090905536
Loss in iteration 6 : 0.9376227906730928
Loss in iteration 7 : 0.9226642981461957
Loss in iteration 8 : 0.9132726076634365
Loss in iteration 9 : 0.9088497279205925
Loss in iteration 10 : 0.9073489743125558
Loss in iteration 11 : 0.9053956345195449
Loss in iteration 12 : 0.9002908445940355
Loss in iteration 13 : 0.8915135476768634
Loss in iteration 14 : 0.8802226250230439
Loss in iteration 15 : 0.8680710108523402
Loss in iteration 16 : 0.8562076286711446
Loss in iteration 17 : 0.8453931010757674
Loss in iteration 18 : 0.8356410438666577
Loss in iteration 19 : 0.82661109295401
Loss in iteration 20 : 0.817960415110834
Loss in iteration 21 : 0.8093807347274368
Loss in iteration 22 : 0.8007018923939715
Loss in iteration 23 : 0.7917857534500427
Loss in iteration 24 : 0.7826215826391107
Loss in iteration 25 : 0.7733594827532914
Loss in iteration 26 : 0.764269762682516
Loss in iteration 27 : 0.7552968441915102
Loss in iteration 28 : 0.7464025089225192
Loss in iteration 29 : 0.737496546461128
Loss in iteration 30 : 0.728600676476912
Loss in iteration 31 : 0.7198787237020747
Loss in iteration 32 : 0.7114142034352913
Loss in iteration 33 : 0.7032517883722603
Loss in iteration 34 : 0.6954130937398065
Loss in iteration 35 : 0.6878720804758053
Loss in iteration 36 : 0.6806594113889921
Loss in iteration 37 : 0.6737435782243087
Loss in iteration 38 : 0.6670676838148506
Loss in iteration 39 : 0.6606482588951376
Loss in iteration 40 : 0.6545233963066062
Loss in iteration 41 : 0.6488245263465839
Loss in iteration 42 : 0.6436389401625606
Loss in iteration 43 : 0.6388922631165265
Loss in iteration 44 : 0.634554471926521
Loss in iteration 45 : 0.6304642262994342
Loss in iteration 46 : 0.6265335084033511
Loss in iteration 47 : 0.62279869737916
Loss in iteration 48 : 0.6192222043514607
Loss in iteration 49 : 0.6158425176435067
Loss in iteration 50 : 0.6126813238195571
Loss in iteration 51 : 0.6097711206172658
Loss in iteration 52 : 0.6071112059758227
Loss in iteration 53 : 0.6046520210342311
Loss in iteration 54 : 0.6023730189425113
Loss in iteration 55 : 0.600248592938608
Loss in iteration 56 : 0.5982343991413128
Loss in iteration 57 : 0.5963251414460995
Loss in iteration 58 : 0.5944850236847253
Loss in iteration 59 : 0.5926874612016662
Loss in iteration 60 : 0.5909492805964901
Loss in iteration 61 : 0.5892791932744262
Loss in iteration 62 : 0.5876932049881255
Loss in iteration 63 : 0.586187287642502
Loss in iteration 64 : 0.5847576930796704
Loss in iteration 65 : 0.5834011060118897
Loss in iteration 66 : 0.5821037381652686
Loss in iteration 67 : 0.5808573320958003
Loss in iteration 68 : 0.5796562476621684
Loss in iteration 69 : 0.5785013942891815
Loss in iteration 70 : 0.5773925489557455
Loss in iteration 71 : 0.5763193522004094
Loss in iteration 72 : 0.5752825083461135
Loss in iteration 73 : 0.574288713629202
Loss in iteration 74 : 0.5733381101362842
Loss in iteration 75 : 0.5724310165210409
Loss in iteration 76 : 0.571563711284728
Loss in iteration 77 : 0.570722459365505
Loss in iteration 78 : 0.5699018562707524
Loss in iteration 79 : 0.5690989262686658
Loss in iteration 80 : 0.5683140398762709
Loss in iteration 81 : 0.5675467095934935
Loss in iteration 82 : 0.5668014109984026
Loss in iteration 83 : 0.5660801065850124
Loss in iteration 84 : 0.5653852721013197
Loss in iteration 85 : 0.564713093014896
Loss in iteration 86 : 0.5640572017259561
Loss in iteration 87 : 0.5634216178762466
Loss in iteration 88 : 0.5628031366068692
Loss in iteration 89 : 0.5622012288962762
Loss in iteration 90 : 0.5616098728511731
Loss in iteration 91 : 0.5610332390235142
Loss in iteration 92 : 0.5604709834418438
Loss in iteration 93 : 0.5599226089462225
Loss in iteration 94 : 0.5593902015418273
Loss in iteration 95 : 0.558868674513437
Loss in iteration 96 : 0.5583571160081819
Loss in iteration 97 : 0.5578551208499225
Loss in iteration 98 : 0.5573657818494533
Loss in iteration 99 : 0.5568865441989633
Loss in iteration 100 : 0.5564181382850081
Loss in iteration 101 : 0.5559591756937835
Loss in iteration 102 : 0.5555106875725304
Loss in iteration 103 : 0.5550728275397976
Loss in iteration 104 : 0.5546438213359571
Loss in iteration 105 : 0.5542244662867563
Loss in iteration 106 : 0.5538125550797126
Loss in iteration 107 : 0.5534095348802077
Loss in iteration 108 : 0.5530139857434206
Loss in iteration 109 : 0.5526240895879072
Loss in iteration 110 : 0.5522401531173983
Loss in iteration 111 : 0.5518630458745957
Loss in iteration 112 : 0.5514942826272061
Loss in iteration 113 : 0.5511306931361372
Loss in iteration 114 : 0.5507713005704752
Loss in iteration 115 : 0.5504158018955183
Loss in iteration 116 : 0.5500646583263239
Loss in iteration 117 : 0.5497160428317243
Loss in iteration 118 : 0.5493726013817135
Loss in iteration 119 : 0.549034637279982
Loss in iteration 120 : 0.5487024430853733
Loss in iteration 121 : 0.548376363620375
Loss in iteration 122 : 0.5480573721726986
Loss in iteration 123 : 0.5477455108638936
Loss in iteration 124 : 0.5474384483196097
Loss in iteration 125 : 0.5471344420817663
Loss in iteration 126 : 0.5468334743358668
Loss in iteration 127 : 0.5465372495242934
Loss in iteration 128 : 0.5462457531986472
Loss in iteration 129 : 0.5459568547717342
Loss in iteration 130 : 0.5456710385991048
Loss in iteration 131 : 0.5453899180065837
Loss in iteration 132 : 0.5451125419132223
Loss in iteration 133 : 0.5448390871599125
Loss in iteration 134 : 0.5445673439615843
Loss in iteration 135 : 0.5442981516227989
Loss in iteration 136 : 0.5440315930487709
Loss in iteration 137 : 0.5437672899718498
Loss in iteration 138 : 0.5435057384318841
Loss in iteration 139 : 0.5432483518218423
Loss in iteration 140 : 0.5429930115457264
Loss in iteration 141 : 0.5427402585321143
Loss in iteration 142 : 0.5424908886252268
Loss in iteration 143 : 0.5422442036842193
Loss in iteration 144 : 0.5419996052270292
Loss in iteration 145 : 0.5417568009865398
Loss in iteration 146 : 0.5415162550776011
Loss in iteration 147 : 0.5412783748127544
Loss in iteration 148 : 0.54104378849548
Loss in iteration 149 : 0.5408122058335522
Loss in iteration 150 : 0.5405837518105063
Loss in iteration 151 : 0.5403579843097537
Loss in iteration 152 : 0.5401344770067668
Loss in iteration 153 : 0.5399127516058645
Loss in iteration 154 : 0.5396931572744833
Loss in iteration 155 : 0.5394762006033101
Loss in iteration 156 : 0.5392626037061854
Loss in iteration 157 : 0.5390526024628088
Loss in iteration 158 : 0.5388454782820272
Loss in iteration 159 : 0.5386400541280625
Loss in iteration 160 : 0.5384368291020932
Loss in iteration 161 : 0.5382360153893762
Loss in iteration 162 : 0.53803761328404
Loss in iteration 163 : 0.5378409456682787
Loss in iteration 164 : 0.5376461690551455
Loss in iteration 165 : 0.5374536297956405
Loss in iteration 166 : 0.5372623692731308
Loss in iteration 167 : 0.5370728266436829
Loss in iteration 168 : 0.5368855255624501
Loss in iteration 169 : 0.5367004570282868
Loss in iteration 170 : 0.5365185266848068
Loss in iteration 171 : 0.5363375394652542
Loss in iteration 172 : 0.5361574113211832
Loss in iteration 173 : 0.5359785969884399
Loss in iteration 174 : 0.5358016320879396
Loss in iteration 175 : 0.5356293524015723
Loss in iteration 176 : 0.5354596187831266
Loss in iteration 177 : 0.5352905978403603
Loss in iteration 178 : 0.5351221917881872
Loss in iteration 179 : 0.5349548500424459
Loss in iteration 180 : 0.534788724776311
Loss in iteration 181 : 0.53462453754845
Loss in iteration 182 : 0.5344616416386553
Loss in iteration 183 : 0.5343005441598913
Loss in iteration 184 : 0.5341415823476844
Loss in iteration 185 : 0.5339837616420469
Loss in iteration 186 : 0.533826538827913
Loss in iteration 187 : 0.5336704809601885
Loss in iteration 188 : 0.5335161639860944
Loss in iteration 189 : 0.5333639624765344
Loss in iteration 190 : 0.5332147534030407
Loss in iteration 191 : 0.5330675136957875
Loss in iteration 192 : 0.5329218422908791
Loss in iteration 193 : 0.5327769073994109
Loss in iteration 194 : 0.5326328465127501
Loss in iteration 195 : 0.5324914638014061
Loss in iteration 196 : 0.5323519464786199
Loss in iteration 197 : 0.5322134799990286
Loss in iteration 198 : 0.5320766598880232
Loss in iteration 199 : 0.5319410802924197
Loss in iteration 200 : 0.5318061015512144
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.778875, training accuracy 0.775, time elapsed: 3691 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 21.385250216909082
Loss in iteration 3 : 10.12475362748073
Loss in iteration 4 : 11.207942337773282
Loss in iteration 5 : 5.711671050904876
Loss in iteration 6 : 6.785650977398408
Loss in iteration 7 : 3.63435823185863
Loss in iteration 8 : 6.221338542391411
Loss in iteration 9 : 6.346555094729656
Loss in iteration 10 : 5.71551391524631
Loss in iteration 11 : 5.759177408470966
Loss in iteration 12 : 4.469657742779541
Loss in iteration 13 : 3.7057073511137006
Loss in iteration 14 : 4.281324775730311
Loss in iteration 15 : 4.328738072010546
Loss in iteration 16 : 4.141671538302031
Loss in iteration 17 : 4.0919041276255905
Loss in iteration 18 : 3.7501647955578377
Loss in iteration 19 : 3.6031593684272245
Loss in iteration 20 : 3.8302117667971824
Loss in iteration 21 : 3.7389073561929607
Loss in iteration 22 : 3.297896118699141
Loss in iteration 23 : 2.950818866908053
Loss in iteration 24 : 2.8342248262308596
Loss in iteration 25 : 2.851573846452566
Loss in iteration 26 : 2.976126714671964
Loss in iteration 27 : 2.9719485538869166
Loss in iteration 28 : 2.7014937312761758
Loss in iteration 29 : 2.4530618816033103
Loss in iteration 30 : 2.3166870215930238
Loss in iteration 31 : 2.2518336661054263
Loss in iteration 32 : 2.2402688198232603
Loss in iteration 33 : 2.130757877766546
Loss in iteration 34 : 1.9997790199060939
Loss in iteration 35 : 2.0112534172736614
Loss in iteration 36 : 1.9426148522819178
Loss in iteration 37 : 1.8746624082658305
Loss in iteration 38 : 1.710501901622516
Loss in iteration 39 : 1.675766404550031
Loss in iteration 40 : 1.6483643052109107
Loss in iteration 41 : 1.660402642970369
Loss in iteration 42 : 1.583505321045811
Loss in iteration 43 : 1.5307562359880726
Loss in iteration 44 : 1.4392075166394545
Loss in iteration 45 : 1.4337885573545577
Loss in iteration 46 : 1.3831217134083482
Loss in iteration 47 : 1.3740292703797212
Loss in iteration 48 : 1.3464774017603818
Loss in iteration 49 : 1.2723032851136669
Loss in iteration 50 : 1.2429934003988978
Loss in iteration 51 : 1.1871771617398508
Loss in iteration 52 : 1.1883961803909264
Loss in iteration 53 : 1.1996578977927401
Loss in iteration 54 : 1.074255036444661
Loss in iteration 55 : 1.0676177359081758
Loss in iteration 56 : 1.05188774526563
Loss in iteration 57 : 1.107637360026485
Loss in iteration 58 : 1.580958817131649
Loss in iteration 59 : 2.61309257717875
Loss in iteration 60 : 1.660064664154791
Loss in iteration 61 : 0.8635142462096406
Loss in iteration 62 : 1.2722946365068846
Loss in iteration 63 : 1.9969351864955363
Loss in iteration 64 : 1.5552023222605076
Loss in iteration 65 : 1.165823640326692
Loss in iteration 66 : 0.985626687518311
Loss in iteration 67 : 1.4077817104548802
Loss in iteration 68 : 1.5874720205736252
Loss in iteration 69 : 1.032032518113201
Loss in iteration 70 : 1.1065960738318943
Loss in iteration 71 : 0.9639236923000286
Loss in iteration 72 : 0.8588683247575878
Loss in iteration 73 : 1.0685123007758652
Loss in iteration 74 : 1.1345245342039862
Loss in iteration 75 : 2.0177772987786216
Loss in iteration 76 : 2.856070875568699
Loss in iteration 77 : 1.1904888769458366
Loss in iteration 78 : 0.8062196533418552
Loss in iteration 79 : 1.5637132016452306
Loss in iteration 80 : 2.321898677444902
Loss in iteration 81 : 1.2110140559928473
Loss in iteration 82 : 0.8681268331307145
Loss in iteration 83 : 1.0033051436527423
Loss in iteration 84 : 1.6634611983324856
Loss in iteration 85 : 2.095586689602664
Loss in iteration 86 : 1.3025635281905656
Loss in iteration 87 : 0.838798847524954
Loss in iteration 88 : 0.8233396282836966
Loss in iteration 89 : 1.0041195179117026
Loss in iteration 90 : 1.753520525673015
Loss in iteration 91 : 2.5103411612378173
Loss in iteration 92 : 1.0790971609470112
Loss in iteration 93 : 0.8353336929620679
Loss in iteration 94 : 1.6074193245352384
Loss in iteration 95 : 1.9498118033310456
Loss in iteration 96 : 1.1801699465249456
Loss in iteration 97 : 0.9208119279373252
Loss in iteration 98 : 0.9106764433003011
Loss in iteration 99 : 1.145257331160545
Loss in iteration 100 : 1.657921299719665
Loss in iteration 101 : 1.5357570513753034
Loss in iteration 102 : 1.1334448043179213
Loss in iteration 103 : 0.9471616119984781
Loss in iteration 104 : 0.9180180187308535
Loss in iteration 105 : 1.068415271544221
Loss in iteration 106 : 1.5892562485764696
Loss in iteration 107 : 2.311246986495408
Loss in iteration 108 : 1.2927399034062805
Loss in iteration 109 : 0.9074236557979523
Loss in iteration 110 : 0.8552787878340133
Loss in iteration 111 : 0.8610839779869097
Loss in iteration 112 : 0.9612253676104499
Loss in iteration 113 : 1.4419442021453848
Loss in iteration 114 : 2.0864520492573257
Loss in iteration 115 : 1.9999645105629256
Loss in iteration 116 : 1.0317919434047917
Loss in iteration 117 : 0.8179042522974311
Loss in iteration 118 : 0.77406725817238
Loss in iteration 119 : 0.7712338453859054
Loss in iteration 120 : 0.7905120628098938
Loss in iteration 121 : 0.9914765048565805
Loss in iteration 122 : 2.1678986734020254
Loss in iteration 123 : 2.3846308065710637
Loss in iteration 124 : 1.4772838153526153
Loss in iteration 125 : 0.9234164279042911
Loss in iteration 126 : 0.8266838643783019
Loss in iteration 127 : 0.8286372730710259
Loss in iteration 128 : 0.8766176823413473
Loss in iteration 129 : 1.1804062387345287
Loss in iteration 130 : 1.9648529011774845
Loss in iteration 131 : 1.6727551284066626
Loss in iteration 132 : 1.3492173186176062
Loss in iteration 133 : 1.067931620529894
Loss in iteration 134 : 1.086472282979274
Loss in iteration 135 : 1.188404701642189
Loss in iteration 136 : 1.4642383807955324
Loss in iteration 137 : 1.357416204380847
Loss in iteration 138 : 1.2530789747889057
Loss in iteration 139 : 1.0107451147231281
Loss in iteration 140 : 0.9233783667619945
Loss in iteration 141 : 0.9252605116126187
Loss in iteration 142 : 1.0707328040558663
Loss in iteration 143 : 1.4436118763049108
Loss in iteration 144 : 2.0871167075211807
Loss in iteration 145 : 1.3429345045831633
Loss in iteration 146 : 1.0569459350024204
Loss in iteration 147 : 1.0214383189486198
Loss in iteration 148 : 1.240686675113362
Loss in iteration 149 : 1.483006107255022
Loss in iteration 150 : 1.6955220294548352
Loss in iteration 151 : 1.2270705421864327
Loss in iteration 152 : 1.0023381310163344
Loss in iteration 153 : 0.9396217080870632
Loss in iteration 154 : 0.9644073957955134
Loss in iteration 155 : 1.1779743708928732
Loss in iteration 156 : 1.7927597913221185
Loss in iteration 157 : 1.577434991613583
Loss in iteration 158 : 1.3976537190881777
Loss in iteration 159 : 1.1717526969002876
Loss in iteration 160 : 1.2303184868829002
Loss in iteration 161 : 1.240214619933061
Loss in iteration 162 : 1.3325833798231557
Loss in iteration 163 : 1.2004362794185761
Loss in iteration 164 : 1.1497816838971124
Loss in iteration 165 : 1.0671267617693148
Loss in iteration 166 : 1.0703165615403099
Loss in iteration 167 : 1.170195661531976
Loss in iteration 168 : 1.5020943825528497
Loss in iteration 169 : 1.5520494288627185
Loss in iteration 170 : 1.5747328981210587
Loss in iteration 171 : 1.2566181441847182
Loss in iteration 172 : 1.1782448960919147
Loss in iteration 173 : 1.1303930321075586
Loss in iteration 174 : 1.2666598648556973
Loss in iteration 175 : 1.324292622440422
Loss in iteration 176 : 1.4652503479862697
Loss in iteration 177 : 1.2877459875836863
Loss in iteration 178 : 1.2293138327214297
Loss in iteration 179 : 1.086665565786655
Loss in iteration 180 : 1.0871271006624557
Loss in iteration 181 : 1.161089210088872
Loss in iteration 182 : 1.547364865174156
Loss in iteration 183 : 1.580926173734155
Loss in iteration 184 : 1.5935113007131503
Loss in iteration 185 : 1.1927437256806366
Loss in iteration 186 : 1.1087759172245282
Loss in iteration 187 : 1.097213793324043
Loss in iteration 188 : 1.2064100437776188
Loss in iteration 189 : 1.2894789524914643
Loss in iteration 190 : 1.4811949800309303
Loss in iteration 191 : 1.3814824097890992
Loss in iteration 192 : 1.3847824760529028
Loss in iteration 193 : 1.2072210099023992
Loss in iteration 194 : 1.2099648902890723
Loss in iteration 195 : 1.1837329451754859
Loss in iteration 196 : 1.3406196585055181
Loss in iteration 197 : 1.3807832665467332
Loss in iteration 198 : 1.5127997298803055
Loss in iteration 199 : 1.33744016689723
Loss in iteration 200 : 1.3002391922858363
Testing accuracy  of updater 7 on alg 1 with rate 20.0 = 0.71775, training accuracy 0.708, time elapsed: 4069 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0597863668588998
Loss in iteration 3 : 0.8597859613161406
Loss in iteration 4 : 0.8579021928878379
Loss in iteration 5 : 0.6996730321407936
Loss in iteration 6 : 0.6692698727439372
Loss in iteration 7 : 0.5925983715778793
Loss in iteration 8 : 0.5820669244074029
Loss in iteration 9 : 0.5718817703616488
Loss in iteration 10 : 0.5647919951934329
Loss in iteration 11 : 0.5615439968987016
Loss in iteration 12 : 0.566086881687345
Loss in iteration 13 : 0.5567770565574691
Loss in iteration 14 : 0.5686776408717614
Loss in iteration 15 : 0.5581692568331649
Loss in iteration 16 : 0.5642684450500094
Loss in iteration 17 : 0.5632647094483004
Loss in iteration 18 : 0.5578956379194873
Loss in iteration 19 : 0.5629412588112968
Loss in iteration 20 : 0.5563596148774099
Loss in iteration 21 : 0.5559555133578901
Loss in iteration 22 : 0.5557231300257751
Loss in iteration 23 : 0.5492219199868051
Loss in iteration 24 : 0.5495381573419793
Loss in iteration 25 : 0.5447845194476417
Loss in iteration 26 : 0.540501881678971
Loss in iteration 27 : 0.5392281232123567
Loss in iteration 28 : 0.5330992350696581
Loss in iteration 29 : 0.5322029969345197
Loss in iteration 30 : 0.5281958653658132
Loss in iteration 31 : 0.5252985858320687
Loss in iteration 32 : 0.5238323521229318
Loss in iteration 33 : 0.5201349558115916
Loss in iteration 34 : 0.5203185639934335
Loss in iteration 35 : 0.5173768433530572
Loss in iteration 36 : 0.5177904943132798
Loss in iteration 37 : 0.5156889946773392
Loss in iteration 38 : 0.5163575496172222
Loss in iteration 39 : 0.5146620820039085
Loss in iteration 40 : 0.5154706022409897
Loss in iteration 41 : 0.5142488135683045
Loss in iteration 42 : 0.5149597011697206
Loss in iteration 43 : 0.5140676599915769
Loss in iteration 44 : 0.5145906156549969
Loss in iteration 45 : 0.5139256025004462
Loss in iteration 46 : 0.5142127322931799
Loss in iteration 47 : 0.513651537882142
Loss in iteration 48 : 0.5137900398819929
Loss in iteration 49 : 0.5132093927108219
Loss in iteration 50 : 0.5131606656330716
Loss in iteration 51 : 0.5125260172968135
Loss in iteration 52 : 0.5123898335183208
Loss in iteration 53 : 0.5118096574028531
Loss in iteration 54 : 0.5116403302745228
Loss in iteration 55 : 0.5111228056788881
Loss in iteration 56 : 0.5109740303378347
Loss in iteration 57 : 0.510487056511463
Loss in iteration 58 : 0.5103528955268367
Loss in iteration 59 : 0.5099788536206644
Loss in iteration 60 : 0.509879081836098
Loss in iteration 61 : 0.5095714796915206
Loss in iteration 62 : 0.5094638704077453
Loss in iteration 63 : 0.5092362146841319
Loss in iteration 64 : 0.5091708705922052
Loss in iteration 65 : 0.5090028248720132
Loss in iteration 66 : 0.5089290381422229
Loss in iteration 67 : 0.5088199265456013
Loss in iteration 68 : 0.5087819360875327
Loss in iteration 69 : 0.5086847317443017
Loss in iteration 70 : 0.5086533741416971
Loss in iteration 71 : 0.5085852035475709
Loss in iteration 72 : 0.5085203551620647
Loss in iteration 73 : 0.5084502266535491
Loss in iteration 74 : 0.5084216201356465
Loss in iteration 75 : 0.5083466898679833
Loss in iteration 76 : 0.5083292783633253
Loss in iteration 77 : 0.5082565898828043
Loss in iteration 78 : 0.5082169450143592
Loss in iteration 79 : 0.5081515094233976
Loss in iteration 80 : 0.5081237164410419
Loss in iteration 81 : 0.5080502422022671
Loss in iteration 82 : 0.5080349382589643
Loss in iteration 83 : 0.5079672440680463
Loss in iteration 84 : 0.5079383997302236
Loss in iteration 85 : 0.5078831704801635
Loss in iteration 86 : 0.5078607190619296
Loss in iteration 87 : 0.5078220070146359
Loss in iteration 88 : 0.5077899138505111
Loss in iteration 89 : 0.5077713891017946
Loss in iteration 90 : 0.5077323264983454
Loss in iteration 91 : 0.5077130877969931
Loss in iteration 92 : 0.5076860851666436
Loss in iteration 93 : 0.5076637811712479
Loss in iteration 94 : 0.5076368100246412
Loss in iteration 95 : 0.507619114096089
Loss in iteration 96 : 0.5075878031595091
Loss in iteration 97 : 0.5075708145591957
Loss in iteration 98 : 0.50754584426385
Loss in iteration 99 : 0.507520685327979
Loss in iteration 100 : 0.5074925188398125
Loss in iteration 101 : 0.5074706119628134
Loss in iteration 102 : 0.5074512552882129
Loss in iteration 103 : 0.5074256462995562
Loss in iteration 104 : 0.5074097942511068
Loss in iteration 105 : 0.5073843120552008
Loss in iteration 106 : 0.5073660937963578
Loss in iteration 107 : 0.5073462887913146
Loss in iteration 108 : 0.5073328664081864
Loss in iteration 109 : 0.5073136436479598
Loss in iteration 110 : 0.5072960185218337
Loss in iteration 111 : 0.5072794471702743
Loss in iteration 112 : 0.5072646403551786
Loss in iteration 113 : 0.5072445490883771
Loss in iteration 114 : 0.5072353242186012
Loss in iteration 115 : 0.5072134964358989
Loss in iteration 116 : 0.5072037171208904
Loss in iteration 117 : 0.5071862884899383
Loss in iteration 118 : 0.5071712242948622
Loss in iteration 119 : 0.5071591000804759
Loss in iteration 120 : 0.5071389513047783
Loss in iteration 121 : 0.5071268677185222
Loss in iteration 122 : 0.5071128513341802
Loss in iteration 123 : 0.5071021538208564
Loss in iteration 124 : 0.5070840573113385
Loss in iteration 125 : 0.5070765531076923
Loss in iteration 126 : 0.5070574458229664
Loss in iteration 127 : 0.5070494247171167
Loss in iteration 128 : 0.5070363240691467
Loss in iteration 129 : 0.5070205274446555
Loss in iteration 130 : 0.5070115174937417
Loss in iteration 131 : 0.5069984683891703
Loss in iteration 132 : 0.5069876498375643
Loss in iteration 133 : 0.5069764094468834
Loss in iteration 134 : 0.5069621296741318
Loss in iteration 135 : 0.5069549432469855
Loss in iteration 136 : 0.5069498091215817
Loss in iteration 137 : 0.5069293730277334
Loss in iteration 138 : 0.5069275595187467
Loss in iteration 139 : 0.5069162689763498
Loss in iteration 140 : 0.5069000494228895
Loss in iteration 141 : 0.5068928209620992
Loss in iteration 142 : 0.506880909039098
Loss in iteration 143 : 0.506866152089869
Loss in iteration 144 : 0.5068585209175622
Loss in iteration 145 : 0.5068529533181334
Loss in iteration 146 : 0.5068394088538211
Loss in iteration 147 : 0.5068324108341884
Loss in iteration 148 : 0.5068239325444864
Loss in iteration 149 : 0.5068119709132567
Loss in iteration 150 : 0.5068044021751948
Loss in iteration 151 : 0.5067962677729985
Loss in iteration 152 : 0.5067826235198485
Loss in iteration 153 : 0.5067816879125925
Loss in iteration 154 : 0.5067819164347517
Loss in iteration 155 : 0.5067564124581092
Loss in iteration 156 : 0.5067734940732179
Loss in iteration 157 : 0.5067428448240227
Loss in iteration 158 : 0.5067351140318341
Loss in iteration 159 : 0.5067337701332748
Loss in iteration 160 : 0.5067147494465277
Loss in iteration 161 : 0.5067131820001175
Loss in iteration 162 : 0.50670196432201
Loss in iteration 163 : 0.5066889829540325
Loss in iteration 164 : 0.5066821817128889
Loss in iteration 165 : 0.5066760178953083
Loss in iteration 166 : 0.5066723889615484
Loss in iteration 167 : 0.5066644901585802
Loss in iteration 168 : 0.5066567713724374
Loss in iteration 169 : 0.5066481968379205
Loss in iteration 170 : 0.506648017830624
Loss in iteration 171 : 0.5066319155596288
Loss in iteration 172 : 0.5066314315231739
Loss in iteration 173 : 0.5066286317828701
Loss in iteration 174 : 0.5066128899951606
Loss in iteration 175 : 0.5066082587999354
Loss in iteration 176 : 0.5066087838579432
Loss in iteration 177 : 0.5065977694481286
Loss in iteration 178 : 0.5066023243445366
Loss in iteration 179 : 0.5065856226457892
Loss in iteration 180 : 0.506591262811465
Loss in iteration 181 : 0.506577548303172
Loss in iteration 182 : 0.5065662406403697
Loss in iteration 183 : 0.5065673763439746
Loss in iteration 184 : 0.5065598028287708
Loss in iteration 185 : 0.5065512413191743
Loss in iteration 186 : 0.5065506219858227
Loss in iteration 187 : 0.5065458436360812
Loss in iteration 188 : 0.5065401394340393
Loss in iteration 189 : 0.5065353428377503
Loss in iteration 190 : 0.5065339762144767
Loss in iteration 191 : 0.5065269549846028
Loss in iteration 192 : 0.5065190326213544
Loss in iteration 193 : 0.5065136577574668
Loss in iteration 194 : 0.5065149327139409
Loss in iteration 195 : 0.5065081151300141
Loss in iteration 196 : 0.5065009223286971
Loss in iteration 197 : 0.5065003740025825
Loss in iteration 198 : 0.506494986777565
Loss in iteration 199 : 0.5064883258396522
Loss in iteration 200 : 0.5064893340125282
Testing accuracy  of updater 7 on alg 1 with rate 2.0 = 0.787625, training accuracy 0.7815, time elapsed: 4883 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9844056292713239
Loss in iteration 3 : 0.9550135525822948
Loss in iteration 4 : 0.922536526823836
Loss in iteration 5 : 0.9113931327845107
Loss in iteration 6 : 0.9139413054155957
Loss in iteration 7 : 0.9014218770353002
Loss in iteration 8 : 0.872776812228813
Loss in iteration 9 : 0.8436212175491926
Loss in iteration 10 : 0.820662289580449
Loss in iteration 11 : 0.8010409798375565
Loss in iteration 12 : 0.7815981039458891
Loss in iteration 13 : 0.7637545124814671
Loss in iteration 14 : 0.7461453647988384
Loss in iteration 15 : 0.7221865052767131
Loss in iteration 16 : 0.6940246918972254
Loss in iteration 17 : 0.6699757786629268
Loss in iteration 18 : 0.6531974059432575
Loss in iteration 19 : 0.6407412567909205
Loss in iteration 20 : 0.6282653656622814
Loss in iteration 21 : 0.6152584743438827
Loss in iteration 22 : 0.6036908597761902
Loss in iteration 23 : 0.5956853376645317
Loss in iteration 24 : 0.5907656429079563
Loss in iteration 25 : 0.5861041878259342
Loss in iteration 26 : 0.5803224109263174
Loss in iteration 27 : 0.5741036007691763
Loss in iteration 28 : 0.5693588507964725
Loss in iteration 29 : 0.5663889482809824
Loss in iteration 30 : 0.5643526289679653
Loss in iteration 31 : 0.5622224548558676
Loss in iteration 32 : 0.5595970217612776
Loss in iteration 33 : 0.5570096970099923
Loss in iteration 34 : 0.5551376494065089
Loss in iteration 35 : 0.5539421764370617
Loss in iteration 36 : 0.5528276811493741
Loss in iteration 37 : 0.551395618089528
Loss in iteration 38 : 0.5497421715944532
Loss in iteration 39 : 0.5482578382563277
Loss in iteration 40 : 0.5472539674812317
Loss in iteration 41 : 0.5465894737505209
Loss in iteration 42 : 0.5458126247987679
Loss in iteration 43 : 0.5447783050715521
Loss in iteration 44 : 0.5436280109748534
Loss in iteration 45 : 0.5426502735740546
Loss in iteration 46 : 0.5419475889493033
Loss in iteration 47 : 0.5413357264803507
Loss in iteration 48 : 0.540660625124746
Loss in iteration 49 : 0.5399139973437589
Loss in iteration 50 : 0.5391920366762892
Loss in iteration 51 : 0.5386140275276782
Loss in iteration 52 : 0.5381016013879666
Loss in iteration 53 : 0.5375685704487503
Loss in iteration 54 : 0.5369781080672228
Loss in iteration 55 : 0.5363417477598381
Loss in iteration 56 : 0.5357295248827638
Loss in iteration 57 : 0.5351942333463832
Loss in iteration 58 : 0.5347261548017198
Loss in iteration 59 : 0.5342707165005766
Loss in iteration 60 : 0.5337781137472127
Loss in iteration 61 : 0.5332719042194936
Loss in iteration 62 : 0.5327891472482271
Loss in iteration 63 : 0.5323441355606519
Loss in iteration 64 : 0.5319277550917711
Loss in iteration 65 : 0.5315059373107099
Loss in iteration 66 : 0.5310706976293134
Loss in iteration 67 : 0.5306479782228626
Loss in iteration 68 : 0.5302499021334307
Loss in iteration 69 : 0.529877415434255
Loss in iteration 70 : 0.5295070287827792
Loss in iteration 71 : 0.5291282071750684
Loss in iteration 72 : 0.528759803068082
Loss in iteration 73 : 0.528413341808489
Loss in iteration 74 : 0.5280796989408211
Loss in iteration 75 : 0.5277520508146154
Loss in iteration 76 : 0.5274261201799645
Loss in iteration 77 : 0.5271063905478791
Loss in iteration 78 : 0.5267912726023571
Loss in iteration 79 : 0.5264831303448173
Loss in iteration 80 : 0.5261829871418329
Loss in iteration 81 : 0.52589055161702
Loss in iteration 82 : 0.5256070584024441
Loss in iteration 83 : 0.5253323463327894
Loss in iteration 84 : 0.5250614225403369
Loss in iteration 85 : 0.5247984216279019
Loss in iteration 86 : 0.524539135538736
Loss in iteration 87 : 0.5242856817323276
Loss in iteration 88 : 0.5240386684920482
Loss in iteration 89 : 0.5237955235204982
Loss in iteration 90 : 0.5235585302527261
Loss in iteration 91 : 0.5233253495265694
Loss in iteration 92 : 0.5230953825800959
Loss in iteration 93 : 0.5228706149935605
Loss in iteration 94 : 0.5226519207436122
Loss in iteration 95 : 0.5224383539154777
Loss in iteration 96 : 0.5222305787647902
Loss in iteration 97 : 0.5220284648628865
Loss in iteration 98 : 0.5218302786183262
Loss in iteration 99 : 0.5216355299306181
Loss in iteration 100 : 0.5214434076785438
Loss in iteration 101 : 0.5212543634845562
Loss in iteration 102 : 0.5210693051745888
Loss in iteration 103 : 0.5208884032482552
Loss in iteration 104 : 0.5207100851866902
Loss in iteration 105 : 0.5205350669776162
Loss in iteration 106 : 0.5203655347026251
Loss in iteration 107 : 0.5201995976689786
Loss in iteration 108 : 0.5200351522315321
Loss in iteration 109 : 0.5198725908309545
Loss in iteration 110 : 0.5197135245287868
Loss in iteration 111 : 0.5195576767671825
Loss in iteration 112 : 0.5194047728182297
Loss in iteration 113 : 0.5192561923068355
Loss in iteration 114 : 0.5191121767453867
Loss in iteration 115 : 0.5189705061456371
Loss in iteration 116 : 0.5188310700430456
Loss in iteration 117 : 0.5186939508333133
Loss in iteration 118 : 0.5185600693019158
Loss in iteration 119 : 0.5184281973881285
Loss in iteration 120 : 0.5182979759741679
Loss in iteration 121 : 0.518169381709726
Loss in iteration 122 : 0.5180428398446558
Loss in iteration 123 : 0.5179183820907126
Loss in iteration 124 : 0.517796060614181
Loss in iteration 125 : 0.5176751513033075
Loss in iteration 126 : 0.5175571374087758
Loss in iteration 127 : 0.5174414252838797
Loss in iteration 128 : 0.5173278504586247
Loss in iteration 129 : 0.5172161886912005
Loss in iteration 130 : 0.5171069618916324
Loss in iteration 131 : 0.516999957061167
Loss in iteration 132 : 0.5168942466936196
Loss in iteration 133 : 0.5167896538804827
Loss in iteration 134 : 0.5166861401645634
Loss in iteration 135 : 0.5165836761809496
Loss in iteration 136 : 0.516482484535574
Loss in iteration 137 : 0.5163839523036441
Loss in iteration 138 : 0.5162879393380722
Loss in iteration 139 : 0.5161927231662208
Loss in iteration 140 : 0.5160988793733743
Loss in iteration 141 : 0.5160056732287389
Loss in iteration 142 : 0.5159136211979983
Loss in iteration 143 : 0.5158227828311124
Loss in iteration 144 : 0.5157344312144889
Loss in iteration 145 : 0.5156477824656411
Loss in iteration 146 : 0.5155619661987142
Loss in iteration 147 : 0.5154779885953007
Loss in iteration 148 : 0.5153964819044152
Loss in iteration 149 : 0.5153159726926683
Loss in iteration 150 : 0.5152356348590117
Loss in iteration 151 : 0.515155960378708
Loss in iteration 152 : 0.5150778257408419
Loss in iteration 153 : 0.5150001967883636
Loss in iteration 154 : 0.5149239265443832
Loss in iteration 155 : 0.5148484019259141
Loss in iteration 156 : 0.5147741783197872
Loss in iteration 157 : 0.5147007454841593
Loss in iteration 158 : 0.5146281619892037
Loss in iteration 159 : 0.5145564276529988
Loss in iteration 160 : 0.5144856819970257
Loss in iteration 161 : 0.5144158345126816
Loss in iteration 162 : 0.514347123732197
Loss in iteration 163 : 0.5142789751922557
Loss in iteration 164 : 0.5142115454116811
Loss in iteration 165 : 0.5141455260782447
Loss in iteration 166 : 0.5140816924007272
Loss in iteration 167 : 0.5140183105394648
Loss in iteration 168 : 0.5139558157379743
Loss in iteration 169 : 0.5138943867455658
Loss in iteration 170 : 0.5138337782539922
Loss in iteration 171 : 0.5137736462965751
Loss in iteration 172 : 0.513714057094858
Loss in iteration 173 : 0.5136550013878279
Loss in iteration 174 : 0.5135965790294013
Loss in iteration 175 : 0.5135387902212597
Loss in iteration 176 : 0.5134814792398988
Loss in iteration 177 : 0.5134244590375585
Loss in iteration 178 : 0.5133689505161899
Loss in iteration 179 : 0.5133149739173217
Loss in iteration 180 : 0.5132621631569477
Loss in iteration 181 : 0.5132100745302209
Loss in iteration 182 : 0.5131584633476179
Loss in iteration 183 : 0.5131074384522712
Loss in iteration 184 : 0.5130572468980884
Loss in iteration 185 : 0.5130075682129505
Loss in iteration 186 : 0.5129583716129241
Loss in iteration 187 : 0.5129093536929122
Loss in iteration 188 : 0.5128608139917349
Loss in iteration 189 : 0.5128129769697201
Loss in iteration 190 : 0.5127653990780827
Loss in iteration 191 : 0.512718039484597
Loss in iteration 192 : 0.5126710952318333
Loss in iteration 193 : 0.5126246994684008
Loss in iteration 194 : 0.5125785778366185
Loss in iteration 195 : 0.5125327159665233
Loss in iteration 196 : 0.5124873290574822
Loss in iteration 197 : 0.5124426848707494
Loss in iteration 198 : 0.5123982987399749
Loss in iteration 199 : 0.5123543617477345
Loss in iteration 200 : 0.5123108739454756
Testing accuracy  of updater 7 on alg 1 with rate 0.19999999999999996 = 0.787375, training accuracy 0.784, time elapsed: 2896 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 5.317080503371858
Loss in iteration 3 : 4.4022018535258765
Loss in iteration 4 : 1.719849137863844
Loss in iteration 5 : 1.2885613304599701
Loss in iteration 6 : 0.9719300710097416
Loss in iteration 7 : 0.8834084793480049
Loss in iteration 8 : 0.9601726007635646
Loss in iteration 9 : 0.9941139015206363
Loss in iteration 10 : 0.9789862838117378
Loss in iteration 11 : 0.9551068136753127
Loss in iteration 12 : 0.9818275955241995
Loss in iteration 13 : 1.0100109605359642
Loss in iteration 14 : 1.1280754358561558
Loss in iteration 15 : 0.9887421333369939
Loss in iteration 16 : 1.0101170442287228
Loss in iteration 17 : 0.90545390318794
Loss in iteration 18 : 0.8894371009770757
Loss in iteration 19 : 0.850282250715574
Loss in iteration 20 : 0.8538981909709233
Loss in iteration 21 : 0.8326321592882296
Loss in iteration 22 : 0.8598196535824011
Loss in iteration 23 : 0.84290697097177
Loss in iteration 24 : 0.88737912635849
Loss in iteration 25 : 0.8684708688705759
Loss in iteration 26 : 0.9176858983953893
Loss in iteration 27 : 0.9031681989710817
Loss in iteration 28 : 0.9554580641854085
Loss in iteration 29 : 0.8851154082615726
Loss in iteration 30 : 0.9285139127424795
Loss in iteration 31 : 0.8754802109546993
Loss in iteration 32 : 0.9072765667462896
Loss in iteration 33 : 0.8682240647011795
Loss in iteration 34 : 0.9085874518381053
Loss in iteration 35 : 0.868397646892454
Loss in iteration 36 : 0.9024840892811835
Loss in iteration 37 : 0.8723344936789638
Loss in iteration 38 : 0.9031257322734547
Loss in iteration 39 : 0.8599191106433071
Loss in iteration 40 : 0.8866847655311637
Loss in iteration 41 : 0.8447364603668968
Loss in iteration 42 : 0.8698669331266895
Loss in iteration 43 : 0.8272258983704692
Loss in iteration 44 : 0.8499887957556443
Loss in iteration 45 : 0.8169664336504489
Loss in iteration 46 : 0.8340543739506189
Loss in iteration 47 : 0.8012072206852751
Loss in iteration 48 : 0.8305549655514495
Loss in iteration 49 : 0.7936699468133904
Loss in iteration 50 : 0.8488620296051524
Loss in iteration 51 : 0.7512484885533705
Loss in iteration 52 : 0.8046961199815446
Loss in iteration 53 : 0.7145272994261264
Loss in iteration 54 : 0.760418336192736
Loss in iteration 55 : 0.7261337255855558
Loss in iteration 56 : 0.7942959359070921
Loss in iteration 57 : 0.7848140278930328
Loss in iteration 58 : 0.8897585042237128
Loss in iteration 59 : 0.9238160572093138
Loss in iteration 60 : 0.9507152818053901
Loss in iteration 61 : 0.8965023752713643
Loss in iteration 62 : 0.8960290818461988
Loss in iteration 63 : 0.8492593165557998
Loss in iteration 64 : 0.8552551624336595
Loss in iteration 65 : 0.8193866588253386
Loss in iteration 66 : 0.8336315899598453
Loss in iteration 67 : 0.8111585665840275
Loss in iteration 68 : 0.8126576253781559
Loss in iteration 69 : 0.7969740898202505
Loss in iteration 70 : 0.8200977174890833
Loss in iteration 71 : 0.7997294929532317
Loss in iteration 72 : 0.8213935400511415
Loss in iteration 73 : 0.7937843898103091
Loss in iteration 74 : 0.8230565182011621
Loss in iteration 75 : 0.804382297815191
Loss in iteration 76 : 0.8283846689967364
Loss in iteration 77 : 0.7967618369661832
Loss in iteration 78 : 0.8290118519257045
Loss in iteration 79 : 0.8114861551798931
Loss in iteration 80 : 0.8407591680182036
Loss in iteration 81 : 0.8015983652770542
Loss in iteration 82 : 0.8290130712745304
Loss in iteration 83 : 0.7985643584505971
Loss in iteration 84 : 0.8195309753208164
Loss in iteration 85 : 0.7900212650646559
Loss in iteration 86 : 0.8246964046319237
Loss in iteration 87 : 0.7959047573945184
Loss in iteration 88 : 0.8141556861431181
Loss in iteration 89 : 0.784628082760894
Loss in iteration 90 : 0.8219587705709639
Loss in iteration 91 : 0.801934990888482
Loss in iteration 92 : 0.8221075854811372
Loss in iteration 93 : 0.7973076287647137
Loss in iteration 94 : 0.8339990729621015
Loss in iteration 95 : 0.8085943419996717
Loss in iteration 96 : 0.830252276491305
Loss in iteration 97 : 0.7985133850961653
Loss in iteration 98 : 0.8301954515829295
Loss in iteration 99 : 0.8086832506489193
Loss in iteration 100 : 0.8264514362900602
Loss in iteration 101 : 0.7977585944982531
Loss in iteration 102 : 0.8258730791648633
Loss in iteration 103 : 0.7991480999775231
Loss in iteration 104 : 0.8199858754608269
Loss in iteration 105 : 0.7919288736356206
Loss in iteration 106 : 0.8195144226859156
Loss in iteration 107 : 0.7913131721065831
Loss in iteration 108 : 0.8167105440301653
Loss in iteration 109 : 0.7926523205789963
Loss in iteration 110 : 0.8192164972998605
Loss in iteration 111 : 0.7898464074440567
Loss in iteration 112 : 0.8186049041002833
Loss in iteration 113 : 0.8054952526794367
Loss in iteration 114 : 0.8359996776588379
Loss in iteration 115 : 0.803745117294944
Loss in iteration 116 : 0.8269243538397147
Loss in iteration 117 : 0.8036668959518948
Loss in iteration 118 : 0.8244707969731258
Loss in iteration 119 : 0.7914250575494596
Loss in iteration 120 : 0.8117912166785511
Loss in iteration 121 : 0.7910726216757284
Loss in iteration 122 : 0.8140986743302492
Loss in iteration 123 : 0.7811291442096169
Loss in iteration 124 : 0.8092212902069875
Loss in iteration 125 : 0.7864027943833747
Loss in iteration 126 : 0.8114093864676847
Loss in iteration 127 : 0.7826239468193356
Loss in iteration 128 : 0.8123991423815631
Loss in iteration 129 : 0.7938625692135921
Loss in iteration 130 : 0.8280499097922452
Loss in iteration 131 : 0.800061157678168
Loss in iteration 132 : 0.8258363740948156
Loss in iteration 133 : 0.803152080812088
Loss in iteration 134 : 0.8246583842222881
Loss in iteration 135 : 0.7870828351145019
Loss in iteration 136 : 0.8080119259798904
Loss in iteration 137 : 0.779491310248751
Loss in iteration 138 : 0.8082267953141824
Loss in iteration 139 : 0.7757445683441649
Loss in iteration 140 : 0.8040777643566526
Loss in iteration 141 : 0.771917277499535
Loss in iteration 142 : 0.8073383516776369
Loss in iteration 143 : 0.7841547909396275
Loss in iteration 144 : 0.82227024494529
Loss in iteration 145 : 0.7868609582991186
Loss in iteration 146 : 0.8286665037080362
Loss in iteration 147 : 0.7979576104033592
Loss in iteration 148 : 0.8337248807236134
Loss in iteration 149 : 0.7950389959830013
Loss in iteration 150 : 0.8283589108748065
Loss in iteration 151 : 0.7955656658824605
Loss in iteration 152 : 0.8267407901350557
Loss in iteration 153 : 0.792116593034653
Loss in iteration 154 : 0.8164761527903918
Loss in iteration 155 : 0.7786846683532048
Loss in iteration 156 : 0.7982231659557117
Loss in iteration 157 : 0.779210181603687
Loss in iteration 158 : 0.8098061663923074
Loss in iteration 159 : 0.7739497392117048
Loss in iteration 160 : 0.805621333143521
Loss in iteration 161 : 0.7810804236438695
Loss in iteration 162 : 0.815488910629155
Loss in iteration 163 : 0.7818198059156698
Loss in iteration 164 : 0.8194376894122553
Loss in iteration 165 : 0.7810372392637261
Loss in iteration 166 : 0.8308089452885371
Loss in iteration 167 : 0.7624411873531254
Loss in iteration 168 : 0.8531777079577435
Loss in iteration 169 : 0.7582943495209113
Loss in iteration 170 : 0.841275933265277
Loss in iteration 171 : 0.7482427078467174
Loss in iteration 172 : 0.7120268478435514
Loss in iteration 173 : 0.7031135601663087
Loss in iteration 174 : 0.6927098876018433
Loss in iteration 175 : 0.6866582996351911
Loss in iteration 176 : 0.6944947556184656
Loss in iteration 177 : 0.6558598630716301
Loss in iteration 178 : 0.6630872490769321
Loss in iteration 179 : 0.6372870632520826
Loss in iteration 180 : 0.6728226519181968
Loss in iteration 181 : 0.6855627017800059
Loss in iteration 182 : 0.8443504169511029
Loss in iteration 183 : 0.9904156862656489
Loss in iteration 184 : 1.117535908955716
Loss in iteration 185 : 1.0102130318237155
Loss in iteration 186 : 0.9848270272860666
Loss in iteration 187 : 0.9453585415462973
Loss in iteration 188 : 0.9457325925546579
Loss in iteration 189 : 0.8880121677920165
Loss in iteration 190 : 0.8734514455639993
Loss in iteration 191 : 0.8258651437300863
Loss in iteration 192 : 0.8136433759742713
Loss in iteration 193 : 0.7945675720877288
Loss in iteration 194 : 0.8001681463176494
Loss in iteration 195 : 0.7730033573039893
Loss in iteration 196 : 0.7877381112643017
Loss in iteration 197 : 0.7773594719720652
Loss in iteration 198 : 0.7982688331856234
Loss in iteration 199 : 0.7715614575317242
Loss in iteration 200 : 0.7985127540119711
Testing accuracy  of updater 8 on alg 1 with rate 2.0 = 0.7295, training accuracy 0.7215, time elapsed: 2528 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9451210358102378
Loss in iteration 3 : 0.9204396662139911
Loss in iteration 4 : 0.9102754012001915
Loss in iteration 5 : 0.8808486657000707
Loss in iteration 6 : 0.8528258498356907
Loss in iteration 7 : 0.8246866812856252
Loss in iteration 8 : 0.7950847637718216
Loss in iteration 9 : 0.7641596023327379
Loss in iteration 10 : 0.7350210273890214
Loss in iteration 11 : 0.7061802653615207
Loss in iteration 12 : 0.6769576822394363
Loss in iteration 13 : 0.6515542428611156
Loss in iteration 14 : 0.6318035777913189
Loss in iteration 15 : 0.614946388711035
Loss in iteration 16 : 0.6012287577735826
Loss in iteration 17 : 0.5904990056414378
Loss in iteration 18 : 0.5820924694369164
Loss in iteration 19 : 0.5752532532687141
Loss in iteration 20 : 0.5694589935889124
Loss in iteration 21 : 0.5645967901763059
Loss in iteration 22 : 0.5605030400838552
Loss in iteration 23 : 0.5570409642022929
Loss in iteration 24 : 0.554139224517396
Loss in iteration 25 : 0.5516855473497724
Loss in iteration 26 : 0.5495390164840508
Loss in iteration 27 : 0.5476649908204727
Loss in iteration 28 : 0.5459771166840021
Loss in iteration 29 : 0.5444376884512255
Loss in iteration 30 : 0.5430047098237138
Loss in iteration 31 : 0.5416837131480365
Loss in iteration 32 : 0.5404890936946866
Loss in iteration 33 : 0.539396758138535
Loss in iteration 34 : 0.5383695336875107
Loss in iteration 35 : 0.5374083413450637
Loss in iteration 36 : 0.5365224282053457
Loss in iteration 37 : 0.5356765746342195
Loss in iteration 38 : 0.5348611864940949
Loss in iteration 39 : 0.5340698598626492
Loss in iteration 40 : 0.5333066159975158
Loss in iteration 41 : 0.532568704447196
Loss in iteration 42 : 0.5318499708475116
Loss in iteration 43 : 0.5311522969118165
Loss in iteration 44 : 0.5304753309314414
Loss in iteration 45 : 0.5298165905016736
Loss in iteration 46 : 0.5291751596475065
Loss in iteration 47 : 0.5285440225385619
Loss in iteration 48 : 0.5279277288992481
Loss in iteration 49 : 0.527336923070931
Loss in iteration 50 : 0.5267622732394465
Loss in iteration 51 : 0.5262108510710763
Loss in iteration 52 : 0.5256726484942797
Loss in iteration 53 : 0.5251568564568361
Loss in iteration 54 : 0.5246612786216459
Loss in iteration 55 : 0.5241821336229634
Loss in iteration 56 : 0.5237257088662518
Loss in iteration 57 : 0.5232873002009265
Loss in iteration 58 : 0.5228715185907117
Loss in iteration 59 : 0.5224728637177568
Loss in iteration 60 : 0.5220893290045179
Loss in iteration 61 : 0.5217188136514007
Loss in iteration 62 : 0.5213592602649053
Loss in iteration 63 : 0.5210074239225455
Loss in iteration 64 : 0.5206672294455222
Loss in iteration 65 : 0.5203368922106647
Loss in iteration 66 : 0.5200145342553205
Loss in iteration 67 : 0.5197042698267567
Loss in iteration 68 : 0.5194038684218508
Loss in iteration 69 : 0.5191187669911955
Loss in iteration 70 : 0.5188480978017688
Loss in iteration 71 : 0.518591864406355
Loss in iteration 72 : 0.5183501085226851
Loss in iteration 73 : 0.5181151626642454
Loss in iteration 74 : 0.5178853013739155
Loss in iteration 75 : 0.5176622650798804
Loss in iteration 76 : 0.5174507854621441
Loss in iteration 77 : 0.5172460438822465
Loss in iteration 78 : 0.5170462100912488
Loss in iteration 79 : 0.5168504703662532
Loss in iteration 80 : 0.5166638356643368
Loss in iteration 81 : 0.5164814625841332
Loss in iteration 82 : 0.5163067861227308
Loss in iteration 83 : 0.516136465833506
Loss in iteration 84 : 0.5159728308234983
Loss in iteration 85 : 0.5158112676727629
Loss in iteration 86 : 0.5156554981652027
Loss in iteration 87 : 0.515503522602262
Loss in iteration 88 : 0.5153552513855189
Loss in iteration 89 : 0.515212056024518
Loss in iteration 90 : 0.5150712326445273
Loss in iteration 91 : 0.5149340284064386
Loss in iteration 92 : 0.5148023668885963
Loss in iteration 93 : 0.5146739309957015
Loss in iteration 94 : 0.5145482538087204
Loss in iteration 95 : 0.514425291759415
Loss in iteration 96 : 0.5143058086319402
Loss in iteration 97 : 0.514189780735835
Loss in iteration 98 : 0.5140765785069382
Loss in iteration 99 : 0.5139657417188258
Loss in iteration 100 : 0.513856904550076
Loss in iteration 101 : 0.5137500248111345
Loss in iteration 102 : 0.5136455754201453
Loss in iteration 103 : 0.5135430795366729
Loss in iteration 104 : 0.5134449376989794
Loss in iteration 105 : 0.5133463852161667
Loss in iteration 106 : 0.5132510185131344
Loss in iteration 107 : 0.5131574581307662
Loss in iteration 108 : 0.5130646710817843
Loss in iteration 109 : 0.5129733266961288
Loss in iteration 110 : 0.5128838217087551
Loss in iteration 111 : 0.5127951683316465
Loss in iteration 112 : 0.5127082237718346
Loss in iteration 113 : 0.5126231644026554
Loss in iteration 114 : 0.5125391220140798
Loss in iteration 115 : 0.5124570538244231
Loss in iteration 116 : 0.512377272715562
Loss in iteration 117 : 0.5122995418914104
Loss in iteration 118 : 0.5122230675412006
Loss in iteration 119 : 0.51214797573147
Loss in iteration 120 : 0.5120750227999356
Loss in iteration 121 : 0.5120031909392433
Loss in iteration 122 : 0.5119330566788985
Loss in iteration 123 : 0.5118652009576321
Loss in iteration 124 : 0.5117986928378128
Loss in iteration 125 : 0.5117344850590727
Loss in iteration 126 : 0.5116720548913334
Loss in iteration 127 : 0.5116116771461484
Loss in iteration 128 : 0.5115522159938117
Loss in iteration 129 : 0.5114934717084945
Loss in iteration 130 : 0.5114368791777978
Loss in iteration 131 : 0.5113809192166474
Loss in iteration 132 : 0.5113258732980904
Loss in iteration 133 : 0.5112720319819697
Loss in iteration 134 : 0.5112183433750805
Loss in iteration 135 : 0.5111673385453284
Loss in iteration 136 : 0.5111171158023076
Loss in iteration 137 : 0.5110682614115842
Loss in iteration 138 : 0.5110196324599201
Loss in iteration 139 : 0.5109725583841226
Loss in iteration 140 : 0.5109259195410476
Loss in iteration 141 : 0.5108808301672001
Loss in iteration 142 : 0.5108362979529392
Loss in iteration 143 : 0.5107934572011
Loss in iteration 144 : 0.5107506291488892
Loss in iteration 145 : 0.5107089970594252
Loss in iteration 146 : 0.5106684959225191
Loss in iteration 147 : 0.51062841303697
Loss in iteration 148 : 0.5105890785826405
Loss in iteration 149 : 0.5105516974618142
Loss in iteration 150 : 0.5105151921985326
Loss in iteration 151 : 0.510479103012562
Loss in iteration 152 : 0.5104443571578307
Loss in iteration 153 : 0.5104110815207863
Loss in iteration 154 : 0.510379521822983
Loss in iteration 155 : 0.5103472494923098
Loss in iteration 156 : 0.5103157558981702
Loss in iteration 157 : 0.5102842979595446
Loss in iteration 158 : 0.5102539007873379
Loss in iteration 159 : 0.5102227542507013
Loss in iteration 160 : 0.510193447542009
Loss in iteration 161 : 0.51016389282978
Loss in iteration 162 : 0.5101356081863566
Loss in iteration 163 : 0.5101074691845282
Loss in iteration 164 : 0.5100794854108371
Loss in iteration 165 : 0.510052672103409
Loss in iteration 166 : 0.5100254876921478
Loss in iteration 167 : 0.5099993511322471
Loss in iteration 168 : 0.5099736514736207
Loss in iteration 169 : 0.5099481840437068
Loss in iteration 170 : 0.509923268182004
Loss in iteration 171 : 0.5098987202520293
Loss in iteration 172 : 0.5098745437346159
Loss in iteration 173 : 0.509850396813999
Loss in iteration 174 : 0.5098262310905018
Loss in iteration 175 : 0.50980243766176
Loss in iteration 176 : 0.5097797067289606
Loss in iteration 177 : 0.5097556542250645
Loss in iteration 178 : 0.509732330097339
Loss in iteration 179 : 0.5097090120147754
Loss in iteration 180 : 0.5096871208723684
Loss in iteration 181 : 0.5096646935584657
Loss in iteration 182 : 0.5096427356326027
Loss in iteration 183 : 0.5096210071946226
Loss in iteration 184 : 0.5095996915967136
Loss in iteration 185 : 0.5095791918821928
Loss in iteration 186 : 0.5095595744898377
Loss in iteration 187 : 0.509536974656202
Loss in iteration 188 : 0.5095171477848206
Loss in iteration 189 : 0.5094973661630007
Loss in iteration 190 : 0.5094773711385286
Loss in iteration 191 : 0.509457101163168
Loss in iteration 192 : 0.5094376492685538
Loss in iteration 193 : 0.5094192816516316
Loss in iteration 194 : 0.5094014191420677
Loss in iteration 195 : 0.5093835809928104
Loss in iteration 196 : 0.5093653758524593
Loss in iteration 197 : 0.5093475940078838
Loss in iteration 198 : 0.5093300800182212
Loss in iteration 199 : 0.5093132726665497
Loss in iteration 200 : 0.509296253032443
Testing accuracy  of updater 8 on alg 1 with rate 0.2 = 0.787875, training accuracy 0.781, time elapsed: 2528 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9929225366571991
Loss in iteration 3 : 0.9828816428248189
Loss in iteration 4 : 0.9702422711403204
Loss in iteration 5 : 0.9557685486491971
Loss in iteration 6 : 0.9410206716179174
Loss in iteration 7 : 0.9278437778454902
Loss in iteration 8 : 0.917774097914065
Loss in iteration 9 : 0.9111371762361339
Loss in iteration 10 : 0.9068294208896948
Loss in iteration 11 : 0.9034489321153123
Loss in iteration 12 : 0.899844895270457
Loss in iteration 13 : 0.8951708894361378
Loss in iteration 14 : 0.8891798421959151
Loss in iteration 15 : 0.8821224808361596
Loss in iteration 16 : 0.8744209759889111
Loss in iteration 17 : 0.8664948021466163
Loss in iteration 18 : 0.8586754073949228
Loss in iteration 19 : 0.8511460623910325
Loss in iteration 20 : 0.8438452304394561
Loss in iteration 21 : 0.8367073388957074
Loss in iteration 22 : 0.8297333403212848
Loss in iteration 23 : 0.8229022068422286
Loss in iteration 24 : 0.8160974994456085
Loss in iteration 25 : 0.8092536756061129
Loss in iteration 26 : 0.8023486511648057
Loss in iteration 27 : 0.7953824666109547
Loss in iteration 28 : 0.7883784479695889
Loss in iteration 29 : 0.781359239097862
Loss in iteration 30 : 0.7743596270383125
Loss in iteration 31 : 0.7674310513481071
Loss in iteration 32 : 0.7606690300167971
Loss in iteration 33 : 0.7540824313769845
Loss in iteration 34 : 0.7476361188525116
Loss in iteration 35 : 0.7412589337212863
Loss in iteration 36 : 0.7349570408530363
Loss in iteration 37 : 0.728704454077957
Loss in iteration 38 : 0.7224899694613984
Loss in iteration 39 : 0.716307110229867
Loss in iteration 40 : 0.7101959568880516
Loss in iteration 41 : 0.7041626887876732
Loss in iteration 42 : 0.6982281420307566
Loss in iteration 43 : 0.6924096316808583
Loss in iteration 44 : 0.6867635687482646
Loss in iteration 45 : 0.6813030731595522
Loss in iteration 46 : 0.6760299781519192
Loss in iteration 47 : 0.6710079749964021
Loss in iteration 48 : 0.6662407016877396
Loss in iteration 49 : 0.6616909619935986
Loss in iteration 50 : 0.6573966407205604
Loss in iteration 51 : 0.653334878989293
Loss in iteration 52 : 0.6494327310287614
Loss in iteration 53 : 0.6456879409255185
Loss in iteration 54 : 0.6421207200306415
Loss in iteration 55 : 0.6386830570749467
Loss in iteration 56 : 0.6353821071819258
Loss in iteration 57 : 0.6322232163960986
Loss in iteration 58 : 0.6292443717515657
Loss in iteration 59 : 0.6264331678264317
Loss in iteration 60 : 0.6237866160815101
Loss in iteration 61 : 0.6213122674229427
Loss in iteration 62 : 0.6189718759240442
Loss in iteration 63 : 0.6167657848264754
Loss in iteration 64 : 0.6146635550337096
Loss in iteration 65 : 0.6126336262125769
Loss in iteration 66 : 0.6106859931772096
Loss in iteration 67 : 0.6088081706299946
Loss in iteration 68 : 0.6070119636366
Loss in iteration 69 : 0.6052905895406905
Loss in iteration 70 : 0.6036458481785318
Loss in iteration 71 : 0.6020665618555517
Loss in iteration 72 : 0.6005416744816618
Loss in iteration 73 : 0.5990818269698772
Loss in iteration 74 : 0.5976773714272504
Loss in iteration 75 : 0.5963189340216749
Loss in iteration 76 : 0.5950025594890254
Loss in iteration 77 : 0.5937217815974075
Loss in iteration 78 : 0.5924888510560199
Loss in iteration 79 : 0.5912972889394261
Loss in iteration 80 : 0.5901434070953937
Loss in iteration 81 : 0.5890288018304003
Loss in iteration 82 : 0.5879547203343191
Loss in iteration 83 : 0.5869128938795383
Loss in iteration 84 : 0.5859008122608004
Loss in iteration 85 : 0.5849187024891647
Loss in iteration 86 : 0.5839676034426997
Loss in iteration 87 : 0.5830388960498383
Loss in iteration 88 : 0.5821338613919663
Loss in iteration 89 : 0.5812546339634365
Loss in iteration 90 : 0.580395532705633
Loss in iteration 91 : 0.5795572225463491
Loss in iteration 92 : 0.5787397995699476
Loss in iteration 93 : 0.5779423638884348
Loss in iteration 94 : 0.5771625079973239
Loss in iteration 95 : 0.5763996725678616
Loss in iteration 96 : 0.575655427363394
Loss in iteration 97 : 0.5749340833630973
Loss in iteration 98 : 0.5742301609361421
Loss in iteration 99 : 0.573545352219431
Loss in iteration 100 : 0.5728749718927076
Loss in iteration 101 : 0.5722156415987999
Loss in iteration 102 : 0.5715689362792773
Loss in iteration 103 : 0.5709335044686138
Loss in iteration 104 : 0.5703126815331191
Loss in iteration 105 : 0.5697062690885675
Loss in iteration 106 : 0.5691151347888901
Loss in iteration 107 : 0.5685348469961272
Loss in iteration 108 : 0.5679672124075175
Loss in iteration 109 : 0.5674138327147192
Loss in iteration 110 : 0.5668696671769823
Loss in iteration 111 : 0.566335263734219
Loss in iteration 112 : 0.5658119963587535
Loss in iteration 113 : 0.5652982180004056
Loss in iteration 114 : 0.5647935716985305
Loss in iteration 115 : 0.5642952629940646
Loss in iteration 116 : 0.5638072698650178
Loss in iteration 117 : 0.5633299399711242
Loss in iteration 118 : 0.5628640643278118
Loss in iteration 119 : 0.5624067709284089
Loss in iteration 120 : 0.5619591514126661
Loss in iteration 121 : 0.5615169466946072
Loss in iteration 122 : 0.5610790560678267
Loss in iteration 123 : 0.5606502370813694
Loss in iteration 124 : 0.560229377122906
Loss in iteration 125 : 0.5598148636458687
Loss in iteration 126 : 0.5594060137594461
Loss in iteration 127 : 0.5590052811549778
Loss in iteration 128 : 0.5586090047244308
Loss in iteration 129 : 0.5582175323111239
Loss in iteration 130 : 0.5578323589276343
Loss in iteration 131 : 0.5574529391184578
Loss in iteration 132 : 0.5570801326842805
Loss in iteration 133 : 0.55671304842251
Loss in iteration 134 : 0.55635277322916
Loss in iteration 135 : 0.556000234449468
Loss in iteration 136 : 0.555652646984534
Loss in iteration 137 : 0.555308387007931
Loss in iteration 138 : 0.5549688048161663
Loss in iteration 139 : 0.5546344939882237
Loss in iteration 140 : 0.5543046533372273
Loss in iteration 141 : 0.5539800747766273
Loss in iteration 142 : 0.5536603247605874
Loss in iteration 143 : 0.5533439367644192
Loss in iteration 144 : 0.5530317264771929
Loss in iteration 145 : 0.552726371929105
Loss in iteration 146 : 0.5524254846972115
Loss in iteration 147 : 0.5521296331209585
Loss in iteration 148 : 0.5518379199774301
Loss in iteration 149 : 0.5515500111629098
Loss in iteration 150 : 0.5512667644413607
Loss in iteration 151 : 0.5509870411313775
Loss in iteration 152 : 0.5507103845458766
Loss in iteration 153 : 0.5504371231624233
Loss in iteration 154 : 0.5501655041356617
Loss in iteration 155 : 0.5498960103661497
Loss in iteration 156 : 0.5496292085675648
Loss in iteration 157 : 0.5493654885460024
Loss in iteration 158 : 0.5491052354945396
Loss in iteration 159 : 0.54884755902757
Loss in iteration 160 : 0.548592226819868
Loss in iteration 161 : 0.5483398351136958
Loss in iteration 162 : 0.5480900947371206
Loss in iteration 163 : 0.5478435518917512
Loss in iteration 164 : 0.5475986970924922
Loss in iteration 165 : 0.5473554878051907
Loss in iteration 166 : 0.5471134710942295
Loss in iteration 167 : 0.5468743642500602
Loss in iteration 168 : 0.5466384442206187
Loss in iteration 169 : 0.5464043600916356
Loss in iteration 170 : 0.5461729205136234
Loss in iteration 171 : 0.5459438069769713
Loss in iteration 172 : 0.545716486971774
Loss in iteration 173 : 0.545491163080603
Loss in iteration 174 : 0.5452677772697251
Loss in iteration 175 : 0.5450459020419087
Loss in iteration 176 : 0.544826288549633
Loss in iteration 177 : 0.5446077608419239
Loss in iteration 178 : 0.5443909148483213
Loss in iteration 179 : 0.5441775736826411
Loss in iteration 180 : 0.5439667167621643
Loss in iteration 181 : 0.5437571444867773
Loss in iteration 182 : 0.5435496350166997
Loss in iteration 183 : 0.5433440995774883
Loss in iteration 184 : 0.543140857501793
Loss in iteration 185 : 0.5429388578606388
Loss in iteration 186 : 0.5427377620100119
Loss in iteration 187 : 0.5425379106324489
Loss in iteration 188 : 0.5423401713908749
Loss in iteration 189 : 0.5421442972239952
Loss in iteration 190 : 0.5419505097915996
Loss in iteration 191 : 0.5417589999958163
Loss in iteration 192 : 0.5415696082175755
Loss in iteration 193 : 0.5413819361716894
Loss in iteration 194 : 0.541196466633214
Loss in iteration 195 : 0.5410136587550101
Loss in iteration 196 : 0.5408324424539019
Loss in iteration 197 : 0.5406525673160647
Loss in iteration 198 : 0.5404736472491338
Loss in iteration 199 : 0.5402955218578643
Loss in iteration 200 : 0.5401190576266213
Testing accuracy  of updater 8 on alg 1 with rate 0.01999999999999999 = 0.77425, training accuracy 0.7715, time elapsed: 2722 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 13.047744702165827
Loss in iteration 3 : 10.216536773725506
Loss in iteration 4 : 6.486844360594467
Loss in iteration 5 : 7.671989240130641
Loss in iteration 6 : 4.3204167355236605
Loss in iteration 7 : 6.255170096458657
Loss in iteration 8 : 6.199987679365051
Loss in iteration 9 : 4.239830259317618
Loss in iteration 10 : 6.324036980703771
Loss in iteration 11 : 3.9984511808048526
Loss in iteration 12 : 3.885736332985842
Loss in iteration 13 : 4.965166516276351
Loss in iteration 14 : 3.522618090273562
Loss in iteration 15 : 4.197415808326391
Loss in iteration 16 : 4.1164348960709045
Loss in iteration 17 : 3.2533089002784545
Loss in iteration 18 : 4.066106106371875
Loss in iteration 19 : 3.7307224031030475
Loss in iteration 20 : 3.0849693553754616
Loss in iteration 21 : 3.3757666768431287
Loss in iteration 22 : 2.8246755029094737
Loss in iteration 23 : 2.9882170593981483
Loss in iteration 24 : 3.257030203975566
Loss in iteration 25 : 2.7825086141365523
Loss in iteration 26 : 2.797003033311104
Loss in iteration 27 : 2.364418591610278
Loss in iteration 28 : 2.208893830858278
Loss in iteration 29 : 2.389167973950283
Loss in iteration 30 : 2.325971815316354
Loss in iteration 31 : 2.22055467525548
Loss in iteration 32 : 1.9057445661228116
Loss in iteration 33 : 1.8604344867977307
Loss in iteration 34 : 1.8593102477276862
Loss in iteration 35 : 1.8781288002645333
Loss in iteration 36 : 1.712800276993076
Loss in iteration 37 : 1.6298570302826154
Loss in iteration 38 : 1.5948465238542426
Loss in iteration 39 : 1.585317051923322
Loss in iteration 40 : 1.5557943180752931
Loss in iteration 41 : 1.438606809126518
Loss in iteration 42 : 1.4392819256459484
Loss in iteration 43 : 1.4645576820493575
Loss in iteration 44 : 1.3684031749970873
Loss in iteration 45 : 1.2792734505394292
Loss in iteration 46 : 1.2688840102727896
Loss in iteration 47 : 1.3238129931114606
Loss in iteration 48 : 1.4364383276836539
Loss in iteration 49 : 1.7562121069424745
Loss in iteration 50 : 2.130847022959369
Loss in iteration 51 : 1.5670263263978645
Loss in iteration 52 : 1.1901650744451553
Loss in iteration 53 : 1.1704749847738263
Loss in iteration 54 : 1.792762166720469
Loss in iteration 55 : 1.6055172781553892
Loss in iteration 56 : 1.1777918737255881
Loss in iteration 57 : 1.0799764840578558
Loss in iteration 58 : 1.5099725138717186
Loss in iteration 59 : 1.5202693960389846
Loss in iteration 60 : 1.1309928454984937
Loss in iteration 61 : 0.9185439435976864
Loss in iteration 62 : 1.192066476184243
Loss in iteration 63 : 1.4698941512311277
Loss in iteration 64 : 1.3309982399795224
Loss in iteration 65 : 0.9732056242103226
Loss in iteration 66 : 1.0135806355046606
Loss in iteration 67 : 1.1826674186340846
Loss in iteration 68 : 1.566012424409449
Loss in iteration 69 : 1.2858803530507148
Loss in iteration 70 : 0.9643549408081958
Loss in iteration 71 : 0.9382117566153001
Loss in iteration 72 : 0.8484046911365287
Loss in iteration 73 : 0.8914149571067653
Loss in iteration 74 : 0.7664403417910671
Loss in iteration 75 : 1.2032621193207669
Loss in iteration 76 : 2.3513822360444663
Loss in iteration 77 : 2.5715657099126004
Loss in iteration 78 : 0.9993505299868861
Loss in iteration 79 : 2.227372276572423
Loss in iteration 80 : 3.0498000902749043
Loss in iteration 81 : 2.1248609562834773
Loss in iteration 82 : 3.1097963302796616
Loss in iteration 83 : 1.3259888920868215
Loss in iteration 84 : 2.8663181722249926
Loss in iteration 85 : 1.2361776177221655
Loss in iteration 86 : 2.6989742468504856
Loss in iteration 87 : 1.6271777022947937
Loss in iteration 88 : 2.5398856843711783
Loss in iteration 89 : 1.509780164903629
Loss in iteration 90 : 2.0204630769032845
Loss in iteration 91 : 2.00191473735274
Loss in iteration 92 : 2.1144653651822805
Loss in iteration 93 : 1.8909001932564533
Loss in iteration 94 : 1.1913864715268452
Loss in iteration 95 : 1.7132183906765186
Loss in iteration 96 : 1.81389473217584
Loss in iteration 97 : 1.5941511508348813
Loss in iteration 98 : 1.1082747148088015
Loss in iteration 99 : 1.2726703128467443
Loss in iteration 100 : 1.5650665361306089
Loss in iteration 101 : 1.4073840914561202
Loss in iteration 102 : 1.0939783677360102
Loss in iteration 103 : 1.0724306642889678
Loss in iteration 104 : 1.2810235994589212
Loss in iteration 105 : 1.2140477659973985
Loss in iteration 106 : 0.9916260470657662
Loss in iteration 107 : 1.0279904434918912
Loss in iteration 108 : 1.0110462120733048
Loss in iteration 109 : 1.101038199657839
Loss in iteration 110 : 0.8825045603645226
Loss in iteration 111 : 0.8603176336789929
Loss in iteration 112 : 1.0558008857383383
Loss in iteration 113 : 0.9245549458257312
Loss in iteration 114 : 0.7750443913650585
Loss in iteration 115 : 0.8605281930620456
Loss in iteration 116 : 0.9449280802903879
Loss in iteration 117 : 1.0009605086456395
Loss in iteration 118 : 1.2988790577753904
Loss in iteration 119 : 1.176308470911717
Loss in iteration 120 : 0.84906110318741
Loss in iteration 121 : 1.0412400383253624
Loss in iteration 122 : 0.9672049557672153
Loss in iteration 123 : 1.4949369109957555
Loss in iteration 124 : 0.9175514766636765
Loss in iteration 125 : 0.9176348559758444
Loss in iteration 126 : 0.7003902379480837
Loss in iteration 127 : 0.769715018794104
Loss in iteration 128 : 0.700987958113466
Loss in iteration 129 : 0.7382218623011391
Loss in iteration 130 : 0.6932431832282503
Loss in iteration 131 : 0.7629789672461353
Loss in iteration 132 : 1.0606775262244352
Loss in iteration 133 : 1.6889793751804196
Loss in iteration 134 : 1.139248801420346
Loss in iteration 135 : 0.7161143441664278
Loss in iteration 136 : 0.6852198511057014
Loss in iteration 137 : 0.8268948572441379
Loss in iteration 138 : 1.0301453443224555
Loss in iteration 139 : 1.0614197534668475
Loss in iteration 140 : 0.9839767796963076
Loss in iteration 141 : 0.8346994417090725
Loss in iteration 142 : 0.7587202370432538
Loss in iteration 143 : 0.7493005417769443
Loss in iteration 144 : 0.8355872918493067
Loss in iteration 145 : 0.8053262665837956
Loss in iteration 146 : 0.9909158176029849
Loss in iteration 147 : 0.9639389041720711
Loss in iteration 148 : 0.8874307188893583
Loss in iteration 149 : 0.932992009124941
Loss in iteration 150 : 0.685423648784685
Loss in iteration 151 : 0.8505861289873755
Loss in iteration 152 : 0.644933367663198
Loss in iteration 153 : 0.9917465114606707
Loss in iteration 154 : 1.1686797904122317
Loss in iteration 155 : 1.7389217940760893
Loss in iteration 156 : 0.8640526446251079
Loss in iteration 157 : 0.686948680037783
Loss in iteration 158 : 1.4338688424321513
Loss in iteration 159 : 1.2360544725780522
Loss in iteration 160 : 0.7536619321321741
Loss in iteration 161 : 0.6738415854468139
Loss in iteration 162 : 0.9865291721060291
Loss in iteration 163 : 1.1018414801772665
Loss in iteration 164 : 0.7586452543174025
Loss in iteration 165 : 0.7400910085363066
Loss in iteration 166 : 0.6609147399221079
Loss in iteration 167 : 0.7836095073918307
Loss in iteration 168 : 0.7691151077542042
Loss in iteration 169 : 0.7818604220862451
Loss in iteration 170 : 0.8351333889006288
Loss in iteration 171 : 0.7172487316455082
Loss in iteration 172 : 0.9665911086674125
Loss in iteration 173 : 0.8040184331297299
Loss in iteration 174 : 1.0822070866173228
Loss in iteration 175 : 0.8948641983621701
Loss in iteration 176 : 0.7923780350368386
Loss in iteration 177 : 0.7444011006213195
Loss in iteration 178 : 0.7613415629361284
Loss in iteration 179 : 0.8924707985939991
Loss in iteration 180 : 1.053074670225178
Loss in iteration 181 : 0.9955098613241478
Loss in iteration 182 : 1.0177176769686045
Loss in iteration 183 : 0.7227770241358499
Loss in iteration 184 : 0.7778458777470432
Loss in iteration 185 : 0.681445880618169
Loss in iteration 186 : 0.7981644656026767
Loss in iteration 187 : 0.7721794049530801
Loss in iteration 188 : 0.9125177698658834
Loss in iteration 189 : 0.8376733974513272
Loss in iteration 190 : 0.8659625406556336
Loss in iteration 191 : 0.674625391963777
Loss in iteration 192 : 0.7491623924131402
Loss in iteration 193 : 0.6393788027739673
Loss in iteration 194 : 0.7791996292211264
Loss in iteration 195 : 0.7749563787116011
Loss in iteration 196 : 1.106118794844254
Loss in iteration 197 : 0.9410899141039344
Loss in iteration 198 : 0.8194259408857588
Loss in iteration 199 : 0.6975562280833472
Loss in iteration 200 : 0.7029659205213218
Testing accuracy  of updater 9 on alg 1 with rate 2.0 = 0.756, training accuracy 0.766, time elapsed: 2794 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9532818580908876
Loss in iteration 3 : 0.8945576035138886
Loss in iteration 4 : 0.8786096900275049
Loss in iteration 5 : 0.8149678851685723
Loss in iteration 6 : 0.73311368595441
Loss in iteration 7 : 0.7134233036673464
Loss in iteration 8 : 0.6506443599972872
Loss in iteration 9 : 0.6103183442487933
Loss in iteration 10 : 0.6185797187226472
Loss in iteration 11 : 0.5818997558226567
Loss in iteration 12 : 0.5743022675953838
Loss in iteration 13 : 0.5750953654818747
Loss in iteration 14 : 0.5530717922888245
Loss in iteration 15 : 0.5573273536157551
Loss in iteration 16 : 0.5627845508347639
Loss in iteration 17 : 0.550261805113941
Loss in iteration 18 : 0.5494551289777683
Loss in iteration 19 : 0.5557783808784236
Loss in iteration 20 : 0.5494901715125788
Loss in iteration 21 : 0.5442655341774681
Loss in iteration 22 : 0.5479409325078503
Loss in iteration 23 : 0.5466978676857966
Loss in iteration 24 : 0.5417501119235135
Loss in iteration 25 : 0.5433804091573734
Loss in iteration 26 : 0.5445845050113687
Loss in iteration 27 : 0.5399512552149657
Loss in iteration 28 : 0.5379455089281592
Loss in iteration 29 : 0.5391381009542103
Loss in iteration 30 : 0.5358967645530618
Loss in iteration 31 : 0.5324092029514309
Loss in iteration 32 : 0.532569983278715
Loss in iteration 33 : 0.5311978741638775
Loss in iteration 34 : 0.5282715766815186
Loss in iteration 35 : 0.5275683005921764
Loss in iteration 36 : 0.5266426460907131
Loss in iteration 37 : 0.5236264486143596
Loss in iteration 38 : 0.5223640904711747
Loss in iteration 39 : 0.5221765883226371
Loss in iteration 40 : 0.5201719630752812
Loss in iteration 41 : 0.5187876848301806
Loss in iteration 42 : 0.5185406796720238
Loss in iteration 43 : 0.5172196221337372
Loss in iteration 44 : 0.516065211213599
Loss in iteration 45 : 0.5159152667539821
Loss in iteration 46 : 0.5147473374469154
Loss in iteration 47 : 0.5138663476693662
Loss in iteration 48 : 0.5138256785393664
Loss in iteration 49 : 0.5130738830257844
Loss in iteration 50 : 0.5127327618389284
Loss in iteration 51 : 0.5127392551361981
Loss in iteration 52 : 0.5122672760114736
Loss in iteration 53 : 0.5120785353971719
Loss in iteration 54 : 0.5120339753238152
Loss in iteration 55 : 0.5117005400616792
Loss in iteration 56 : 0.5115916240219429
Loss in iteration 57 : 0.5115899435058582
Loss in iteration 58 : 0.5113500536857865
Loss in iteration 59 : 0.5112383922207706
Loss in iteration 60 : 0.5112630261778887
Loss in iteration 61 : 0.5111543460457595
Loss in iteration 62 : 0.5110235066056503
Loss in iteration 63 : 0.5109598675760096
Loss in iteration 64 : 0.510838940478863
Loss in iteration 65 : 0.5107592957801187
Loss in iteration 66 : 0.5107644259596441
Loss in iteration 67 : 0.5106747535896157
Loss in iteration 68 : 0.5105763742215524
Loss in iteration 69 : 0.5105385501224703
Loss in iteration 70 : 0.5104521578608064
Loss in iteration 71 : 0.510375262968763
Loss in iteration 72 : 0.51033393362747
Loss in iteration 73 : 0.5102243426622649
Loss in iteration 74 : 0.5101574989398283
Loss in iteration 75 : 0.5101132991256495
Loss in iteration 76 : 0.5100110718734253
Loss in iteration 77 : 0.5099672167513812
Loss in iteration 78 : 0.5098989465737829
Loss in iteration 79 : 0.5098393757513908
Loss in iteration 80 : 0.5098009732976534
Loss in iteration 81 : 0.5097266088076797
Loss in iteration 82 : 0.5096883824208507
Loss in iteration 83 : 0.5096399604747778
Loss in iteration 84 : 0.509586969694404
Loss in iteration 85 : 0.5095511948970896
Loss in iteration 86 : 0.5095017450468993
Loss in iteration 87 : 0.5094532108550074
Loss in iteration 88 : 0.5094181891696926
Loss in iteration 89 : 0.5093784526417247
Loss in iteration 90 : 0.5093491630759925
Loss in iteration 91 : 0.5093149657156192
Loss in iteration 92 : 0.5092769444135526
Loss in iteration 93 : 0.5092491124513019
Loss in iteration 94 : 0.5092131406604813
Loss in iteration 95 : 0.5091893299776907
Loss in iteration 96 : 0.5091568415434427
Loss in iteration 97 : 0.5091292720241021
Loss in iteration 98 : 0.5091011495273806
Loss in iteration 99 : 0.5090749277111374
Loss in iteration 100 : 0.5090492740612558
Loss in iteration 101 : 0.5090197288124723
Loss in iteration 102 : 0.5089976668568607
Loss in iteration 103 : 0.5089694576904443
Loss in iteration 104 : 0.5089487116623911
Loss in iteration 105 : 0.5089222097195591
Loss in iteration 106 : 0.5088959554397584
Loss in iteration 107 : 0.5088734798681388
Loss in iteration 108 : 0.5088480530098978
Loss in iteration 109 : 0.5088268527877068
Loss in iteration 110 : 0.5088029869836908
Loss in iteration 111 : 0.5087813939005268
Loss in iteration 112 : 0.508760713150758
Loss in iteration 113 : 0.5087384839059863
Loss in iteration 114 : 0.5087147683875199
Loss in iteration 115 : 0.508695503915728
Loss in iteration 116 : 0.5086752621679601
Loss in iteration 117 : 0.5086547970790404
Loss in iteration 118 : 0.5086333761894498
Loss in iteration 119 : 0.508612151940682
Loss in iteration 120 : 0.5085934020488552
Loss in iteration 121 : 0.5085736511809749
Loss in iteration 122 : 0.5085536206920162
Loss in iteration 123 : 0.5085345497241429
Loss in iteration 124 : 0.5085149259088142
Loss in iteration 125 : 0.5084961756641432
Loss in iteration 126 : 0.5084776633492417
Loss in iteration 127 : 0.508459759250485
Loss in iteration 128 : 0.5084414171957846
Loss in iteration 129 : 0.5084244146953752
Loss in iteration 130 : 0.5084064624513623
Loss in iteration 131 : 0.508388677539589
Loss in iteration 132 : 0.5083718498351635
Loss in iteration 133 : 0.5083551763170221
Loss in iteration 134 : 0.5083384155385027
Loss in iteration 135 : 0.5083219254036448
Loss in iteration 136 : 0.5083048479117191
Loss in iteration 137 : 0.5082887359179258
Loss in iteration 138 : 0.5082728384713465
Loss in iteration 139 : 0.5082568615436152
Loss in iteration 140 : 0.5082404399736762
Loss in iteration 141 : 0.508224551058198
Loss in iteration 142 : 0.5082090991658502
Loss in iteration 143 : 0.5081930914573496
Loss in iteration 144 : 0.5081774382154358
Loss in iteration 145 : 0.508162279981815
Loss in iteration 146 : 0.5081465822329431
Loss in iteration 147 : 0.508131440732905
Loss in iteration 148 : 0.5081154805859626
Loss in iteration 149 : 0.5081001306915147
Loss in iteration 150 : 0.5080852462458576
Loss in iteration 151 : 0.5080708105961457
Loss in iteration 152 : 0.508055879713894
Loss in iteration 153 : 0.5080406666551849
Loss in iteration 154 : 0.5080264958745476
Loss in iteration 155 : 0.508012262124375
Loss in iteration 156 : 0.5079975265318984
Loss in iteration 157 : 0.5079848598483234
Loss in iteration 158 : 0.5079701136551866
Loss in iteration 159 : 0.5079564476913812
Loss in iteration 160 : 0.5079429301504305
Loss in iteration 161 : 0.507929089757452
Loss in iteration 162 : 0.5079150621782531
Loss in iteration 163 : 0.5079017503942221
Loss in iteration 164 : 0.5078885894360875
Loss in iteration 165 : 0.5078748626209286
Loss in iteration 166 : 0.5078620347125549
Loss in iteration 167 : 0.5078479668250296
Loss in iteration 168 : 0.5078351409160025
Loss in iteration 169 : 0.507821233839031
Loss in iteration 170 : 0.5078083176481268
Loss in iteration 171 : 0.5077956381019681
Loss in iteration 172 : 0.507782508374699
Loss in iteration 173 : 0.5077689540923557
Loss in iteration 174 : 0.5077561206909519
Loss in iteration 175 : 0.5077427895776768
Loss in iteration 176 : 0.5077300833856092
Loss in iteration 177 : 0.5077164472809873
Loss in iteration 178 : 0.5077037938863239
Loss in iteration 179 : 0.5076912550357942
Loss in iteration 180 : 0.5076779396642427
Loss in iteration 181 : 0.5076655007446848
Loss in iteration 182 : 0.5076523941380352
Loss in iteration 183 : 0.5076394163540793
Loss in iteration 184 : 0.5076264932493271
Loss in iteration 185 : 0.50761395993889
Loss in iteration 186 : 0.5076011608628184
Loss in iteration 187 : 0.5075884295893358
Loss in iteration 188 : 0.507576048350491
Loss in iteration 189 : 0.5075637377705974
Loss in iteration 190 : 0.5075515768161007
Loss in iteration 191 : 0.5075403801920261
Loss in iteration 192 : 0.5075293034907719
Loss in iteration 193 : 0.50751835115222
Loss in iteration 194 : 0.5075072226712047
Loss in iteration 195 : 0.5074969657879042
Loss in iteration 196 : 0.5074866132455245
Loss in iteration 197 : 0.5074764462325251
Loss in iteration 198 : 0.5074666660451613
Loss in iteration 199 : 0.5074562053374401
Loss in iteration 200 : 0.5074459297152512
Testing accuracy  of updater 9 on alg 1 with rate 0.2 = 0.78725, training accuracy 0.784, time elapsed: 3061 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9926077042547176
Loss in iteration 3 : 0.9786545527430146
Loss in iteration 4 : 0.9591632452520445
Loss in iteration 5 : 0.9373736654183445
Loss in iteration 6 : 0.9196256273608485
Loss in iteration 7 : 0.9105086417069244
Loss in iteration 8 : 0.9082097760658909
Loss in iteration 9 : 0.9069523362368749
Loss in iteration 10 : 0.9008797370145865
Loss in iteration 11 : 0.8885715898252752
Loss in iteration 12 : 0.8724794400025141
Loss in iteration 13 : 0.8558311310752156
Loss in iteration 14 : 0.8404411886802327
Loss in iteration 15 : 0.826873129850796
Loss in iteration 16 : 0.814563525146712
Loss in iteration 17 : 0.8026673349116746
Loss in iteration 18 : 0.7908236745529419
Loss in iteration 19 : 0.7787345652312374
Loss in iteration 20 : 0.7664680435656239
Loss in iteration 21 : 0.7543832416489764
Loss in iteration 22 : 0.7424253880700629
Loss in iteration 23 : 0.7303597012087355
Loss in iteration 24 : 0.7180723598823283
Loss in iteration 25 : 0.7056280975502903
Loss in iteration 26 : 0.6936210422674037
Loss in iteration 27 : 0.682456356746389
Loss in iteration 28 : 0.6723636468999311
Loss in iteration 29 : 0.6633360642656654
Loss in iteration 30 : 0.6550409216610176
Loss in iteration 31 : 0.6472056237714164
Loss in iteration 32 : 0.6397656795509923
Loss in iteration 33 : 0.6328752070268433
Loss in iteration 34 : 0.6266445933010355
Loss in iteration 35 : 0.6210981861279969
Loss in iteration 36 : 0.616140504131954
Loss in iteration 37 : 0.6117599297958006
Loss in iteration 38 : 0.607780978157761
Loss in iteration 39 : 0.604001503762437
Loss in iteration 40 : 0.6003375422810723
Loss in iteration 41 : 0.5968540276936131
Loss in iteration 42 : 0.5936125081294915
Loss in iteration 43 : 0.590699605608553
Loss in iteration 44 : 0.5881528487635022
Loss in iteration 45 : 0.5858624718099404
Loss in iteration 46 : 0.5837749375873311
Loss in iteration 47 : 0.5818094300452505
Loss in iteration 48 : 0.5799184735070484
Loss in iteration 49 : 0.57809162590385
Loss in iteration 50 : 0.5763306833171489
Loss in iteration 51 : 0.5746504669349777
Loss in iteration 52 : 0.5731117134493003
Loss in iteration 53 : 0.5716843189391609
Loss in iteration 54 : 0.5703409502225587
Loss in iteration 55 : 0.5690680841328062
Loss in iteration 56 : 0.5678533439928062
Loss in iteration 57 : 0.5666798139711448
Loss in iteration 58 : 0.5655523036613375
Loss in iteration 59 : 0.5644941656371684
Loss in iteration 60 : 0.563516327163768
Loss in iteration 61 : 0.5625932571879353
Loss in iteration 62 : 0.5617152253429554
Loss in iteration 63 : 0.5608749495071036
Loss in iteration 64 : 0.5600676716154425
Loss in iteration 65 : 0.5592791834070446
Loss in iteration 66 : 0.5585151288840304
Loss in iteration 67 : 0.5577692513662653
Loss in iteration 68 : 0.5570499697981868
Loss in iteration 69 : 0.5563643806664691
Loss in iteration 70 : 0.5557136270570777
Loss in iteration 71 : 0.5550931358688088
Loss in iteration 72 : 0.5545001912528864
Loss in iteration 73 : 0.5539246744276974
Loss in iteration 74 : 0.5533626805026606
Loss in iteration 75 : 0.552810995482121
Loss in iteration 76 : 0.5522671866803215
Loss in iteration 77 : 0.5517373696626398
Loss in iteration 78 : 0.55121977168835
Loss in iteration 79 : 0.5507141846202182
Loss in iteration 80 : 0.5502200747085864
Loss in iteration 81 : 0.5497367077101049
Loss in iteration 82 : 0.5492683877609409
Loss in iteration 83 : 0.548812475888931
Loss in iteration 84 : 0.5483627897209075
Loss in iteration 85 : 0.547922146273061
Loss in iteration 86 : 0.5474896529429711
Loss in iteration 87 : 0.5470673273925066
Loss in iteration 88 : 0.5466565330284302
Loss in iteration 89 : 0.5462554528065556
Loss in iteration 90 : 0.545864093423271
Loss in iteration 91 : 0.5454759588150547
Loss in iteration 92 : 0.5450943235591237
Loss in iteration 93 : 0.5447179078258535
Loss in iteration 94 : 0.5443489502257838
Loss in iteration 95 : 0.5439884680559164
Loss in iteration 96 : 0.5436366777125166
Loss in iteration 97 : 0.5432911609701814
Loss in iteration 98 : 0.5429509360455288
Loss in iteration 99 : 0.5426146752370355
Loss in iteration 100 : 0.5422852798291963
Loss in iteration 101 : 0.541963289452393
Loss in iteration 102 : 0.5416460006032002
Loss in iteration 103 : 0.5413343320307403
Loss in iteration 104 : 0.5410277875946717
Loss in iteration 105 : 0.5407277095035564
Loss in iteration 106 : 0.5404329844328009
Loss in iteration 107 : 0.5401419180259132
Loss in iteration 108 : 0.5398554714259786
Loss in iteration 109 : 0.5395734327688079
Loss in iteration 110 : 0.5392943091149136
Loss in iteration 111 : 0.5390187783142282
Loss in iteration 112 : 0.5387469560800345
Loss in iteration 113 : 0.5384792293598996
Loss in iteration 114 : 0.5382159837943344
Loss in iteration 115 : 0.5379585633900021
Loss in iteration 116 : 0.5377054178996821
Loss in iteration 117 : 0.5374561885844491
Loss in iteration 118 : 0.5372103371642861
Loss in iteration 119 : 0.5369682527594426
Loss in iteration 120 : 0.5367290429067276
Loss in iteration 121 : 0.5364938062763948
Loss in iteration 122 : 0.5362617946746812
Loss in iteration 123 : 0.5360321684053927
Loss in iteration 124 : 0.5358059575843668
Loss in iteration 125 : 0.5355828427391611
Loss in iteration 126 : 0.5353643441671744
Loss in iteration 127 : 0.5351486441963211
Loss in iteration 128 : 0.5349350075830137
Loss in iteration 129 : 0.5347233899166185
Loss in iteration 130 : 0.5345133563799038
Loss in iteration 131 : 0.5343066863696067
Loss in iteration 132 : 0.5341020532153816
Loss in iteration 133 : 0.5338996226023809
Loss in iteration 134 : 0.5336996460494647
Loss in iteration 135 : 0.5335022866022182
Loss in iteration 136 : 0.5333080981539651
Loss in iteration 137 : 0.5331153845682118
Loss in iteration 138 : 0.532925110101664
Loss in iteration 139 : 0.5327379650418537
Loss in iteration 140 : 0.5325529987881256
Loss in iteration 141 : 0.5323703226782179
Loss in iteration 142 : 0.5321900121905526
Loss in iteration 143 : 0.5320115338334407
Loss in iteration 144 : 0.5318353686699265
Loss in iteration 145 : 0.5316622855880369
Loss in iteration 146 : 0.5314914018318119
Loss in iteration 147 : 0.5313223035997903
Loss in iteration 148 : 0.5311546608359576
Loss in iteration 149 : 0.5309887680166692
Loss in iteration 150 : 0.5308260751996355
Loss in iteration 151 : 0.5306649651993129
Loss in iteration 152 : 0.5305053080135708
Loss in iteration 153 : 0.5303474630837036
Loss in iteration 154 : 0.5301906852235609
Loss in iteration 155 : 0.5300361422224387
Loss in iteration 156 : 0.5298828402999185
Loss in iteration 157 : 0.5297300006689293
Loss in iteration 158 : 0.5295777036180636
Loss in iteration 159 : 0.5294265803917094
Loss in iteration 160 : 0.5292770678544243
Loss in iteration 161 : 0.529128808331092
Loss in iteration 162 : 0.5289821092015665
Loss in iteration 163 : 0.5288376019928768
Loss in iteration 164 : 0.528695356303906
Loss in iteration 165 : 0.5285548212129152
Loss in iteration 166 : 0.5284149027000749
Loss in iteration 167 : 0.5282753704834117
Loss in iteration 168 : 0.5281361574482066
Loss in iteration 169 : 0.5279974934119072
Loss in iteration 170 : 0.5278593820114581
Loss in iteration 171 : 0.5277226163629594
Loss in iteration 172 : 0.5275869052542352
Loss in iteration 173 : 0.5274526639948629
Loss in iteration 174 : 0.5273205661461946
Loss in iteration 175 : 0.5271900675001838
Loss in iteration 176 : 0.5270616297573992
Loss in iteration 177 : 0.5269350112842995
Loss in iteration 178 : 0.5268095286942936
Loss in iteration 179 : 0.5266849283346499
Loss in iteration 180 : 0.526561218430963
Loss in iteration 181 : 0.5264388583315148
Loss in iteration 182 : 0.5263189633719886
Loss in iteration 183 : 0.5261999836471885
Loss in iteration 184 : 0.5260817634667554
Loss in iteration 185 : 0.5259644559942066
Loss in iteration 186 : 0.5258481761675468
Loss in iteration 187 : 0.5257331172754255
Loss in iteration 188 : 0.5256189342105335
Loss in iteration 189 : 0.5255052178997432
Loss in iteration 190 : 0.5253920600278371
Loss in iteration 191 : 0.5252797095227709
Loss in iteration 192 : 0.5251684093193717
Loss in iteration 193 : 0.5250579289410932
Loss in iteration 194 : 0.524948162775946
Loss in iteration 195 : 0.5248389218162487
Loss in iteration 196 : 0.5247304908123863
Loss in iteration 197 : 0.5246232469878417
Loss in iteration 198 : 0.5245164413229774
Loss in iteration 199 : 0.5244102815110258
Loss in iteration 200 : 0.5243047171193396
Testing accuracy  of updater 9 on alg 1 with rate 0.01999999999999999 = 0.78275, training accuracy 0.7775, time elapsed: 3193 millisecond.
