objc[3090]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1095614c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x1095e54e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/03/02 13:44:40 INFO SparkContext: Running Spark version 2.0.0
18/03/02 13:44:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/03/02 13:44:41 INFO SecurityManager: Changing view acls to: Aitor
18/03/02 13:44:41 INFO SecurityManager: Changing modify acls to: Aitor
18/03/02 13:44:41 INFO SecurityManager: Changing view acls groups to: 
18/03/02 13:44:41 INFO SecurityManager: Changing modify acls groups to: 
18/03/02 13:44:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/03/02 13:44:42 INFO Utils: Successfully started service 'sparkDriver' on port 52828.
18/03/02 13:44:42 INFO SparkEnv: Registering MapOutputTracker
18/03/02 13:44:42 INFO SparkEnv: Registering BlockManagerMaster
18/03/02 13:44:42 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-490f4079-15bb-47ec-bf97-66a16a78bbde
18/03/02 13:44:42 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/03/02 13:44:42 INFO SparkEnv: Registering OutputCommitCoordinator
18/03/02 13:44:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/03/02 13:44:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/03/02 13:44:43 INFO Executor: Starting executor ID driver on host localhost
18/03/02 13:44:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52829.
18/03/02 13:44:43 INFO NettyBlockTransferService: Server created on 192.168.2.140:52829
18/03/02 13:44:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 52829)
18/03/02 13:44:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:52829 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 52829)
18/03/02 13:44:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 52829)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.763745786307966
Loss in iteration 3 : 21.073859681496824
Loss in iteration 4 : 8.986991572615931
Loss in iteration 5 : 14.539457668301027
Loss in iteration 6 : 13.18087855054728
Loss in iteration 7 : 8.314309701451075
Loss in iteration 8 : 15.467231900578131
Loss in iteration 9 : 5.128244719867834
Loss in iteration 10 : 14.429444280300457
Loss in iteration 11 : 5.609646364734408
Loss in iteration 12 : 13.404569908653484
Loss in iteration 13 : 6.090355794218039
Loss in iteration 14 : 12.350347249643852
Loss in iteration 15 : 6.620416747589248
Loss in iteration 16 : 11.244408899414605
Loss in iteration 17 : 7.135186949482012
Loss in iteration 18 : 10.217983425796048
Loss in iteration 19 : 7.457356572461145
Loss in iteration 20 : 9.297462862065306
Loss in iteration 21 : 7.646755299167503
Loss in iteration 22 : 8.445935451634906
Loss in iteration 23 : 7.530899366888367
Loss in iteration 24 : 7.847418723898613
Loss in iteration 25 : 7.457627820413405
Loss in iteration 26 : 7.3061106088919905
Loss in iteration 27 : 7.2075064166363
Loss in iteration 28 : 6.661684400806765
Loss in iteration 29 : 7.037439357003949
Loss in iteration 30 : 6.252023953166858
Loss in iteration 31 : 6.831899996520333
Loss in iteration 32 : 5.972679515853859
Loss in iteration 33 : 6.737907178346466
Loss in iteration 34 : 5.78096147389316
Loss in iteration 35 : 6.549874616111653
Loss in iteration 36 : 5.523591380977136
Loss in iteration 37 : 6.299326492178904
Loss in iteration 38 : 5.273852742862309
Loss in iteration 39 : 6.107990589937877
Loss in iteration 40 : 5.122511380044219
Loss in iteration 41 : 5.989726630515679
Loss in iteration 42 : 5.026547718688764
Loss in iteration 43 : 5.925331878366126
Loss in iteration 44 : 4.958876821144648
Loss in iteration 45 : 5.862019272591654
Loss in iteration 46 : 4.832406772318722
Loss in iteration 47 : 5.75696368029332
Loss in iteration 48 : 4.787472670976445
Loss in iteration 49 : 5.736279567501353
Loss in iteration 50 : 4.75474511060325
Loss in iteration 51 : 5.711620605452963
Loss in iteration 52 : 4.6535565091034385
Loss in iteration 53 : 5.600267092625623
Loss in iteration 54 : 4.604644622973348
Loss in iteration 55 : 5.532634184065325
Loss in iteration 56 : 4.557206139375867
Loss in iteration 57 : 5.501742400283314
Loss in iteration 58 : 4.476306914197325
Loss in iteration 59 : 5.441937422357068
Loss in iteration 60 : 4.417435359079882
Loss in iteration 61 : 5.420031043670609
Loss in iteration 62 : 4.361772880733005
Loss in iteration 63 : 5.388736849712381
Loss in iteration 64 : 4.33611540260953
Loss in iteration 65 : 5.358088510714328
Loss in iteration 66 : 4.2975043150391485
Loss in iteration 67 : 5.284362366135052
Loss in iteration 68 : 4.251963333336707
Loss in iteration 69 : 5.250191381158101
Loss in iteration 70 : 4.213161190165552
Loss in iteration 71 : 5.227124669906319
Loss in iteration 72 : 4.19136164211717
Loss in iteration 73 : 5.160430359372366
Loss in iteration 74 : 4.145615931454244
Loss in iteration 75 : 5.131556650741368
Loss in iteration 76 : 4.11503952762738
Loss in iteration 77 : 5.1145962140606205
Loss in iteration 78 : 4.108744469230074
Loss in iteration 79 : 5.111584348394231
Loss in iteration 80 : 4.091805799175789
Loss in iteration 81 : 5.07292416550608
Loss in iteration 82 : 4.086678941450263
Loss in iteration 83 : 5.059458921199764
Loss in iteration 84 : 4.063528047183302
Loss in iteration 85 : 5.037219636569899
Loss in iteration 86 : 4.060901073880821
Loss in iteration 87 : 5.037660049594022
Loss in iteration 88 : 4.047880792320619
Loss in iteration 89 : 5.021939891860394
Loss in iteration 90 : 4.053756979211877
Loss in iteration 91 : 5.017771684751378
Loss in iteration 92 : 4.058108945365712
Loss in iteration 93 : 5.021887970890781
Loss in iteration 94 : 4.053072656439578
Loss in iteration 95 : 5.007488485687582
Loss in iteration 96 : 4.04291649781371
Loss in iteration 97 : 4.977871389322766
Loss in iteration 98 : 4.041055623252816
Loss in iteration 99 : 4.972971552028119
Loss in iteration 100 : 4.030061858021258
Loss in iteration 101 : 4.946095166054912
Loss in iteration 102 : 4.034754839980283
Loss in iteration 103 : 4.964582462709317
Loss in iteration 104 : 4.027079946502416
Loss in iteration 105 : 4.922923559124496
Loss in iteration 106 : 4.032893020298142
Loss in iteration 107 : 4.944848496916391
Loss in iteration 108 : 4.014361321527646
Loss in iteration 109 : 4.913506048256819
Loss in iteration 110 : 4.02541004488419
Loss in iteration 111 : 4.905704160537405
Loss in iteration 112 : 4.0221890245367735
Loss in iteration 113 : 4.887976036292066
Loss in iteration 114 : 4.019507581604436
Loss in iteration 115 : 4.884958957109069
Loss in iteration 116 : 4.013312313451155
Loss in iteration 117 : 4.86517178244453
Loss in iteration 118 : 3.984581107984161
Loss in iteration 119 : 4.804898924392768
Loss in iteration 120 : 4.007695767225246
Loss in iteration 121 : 4.853506693462899
Loss in iteration 122 : 3.9970231972996726
Loss in iteration 123 : 4.804813345199369
Loss in iteration 124 : 3.986976597379873
Loss in iteration 125 : 4.7905914128428195
Loss in iteration 126 : 3.9951236373451846
Loss in iteration 127 : 4.797712238312406
Loss in iteration 128 : 3.9924984438347084
Loss in iteration 129 : 4.7892342562534305
Loss in iteration 130 : 3.988500705426726
Loss in iteration 131 : 4.777753257090193
Loss in iteration 132 : 3.9856304698408294
Loss in iteration 133 : 4.772637496335456
Loss in iteration 134 : 3.9746307712942905
Loss in iteration 135 : 4.756581122117823
Loss in iteration 136 : 3.9719089580306064
Loss in iteration 137 : 4.749571271186821
Loss in iteration 138 : 3.9736676683135124
Loss in iteration 139 : 4.746495554798296
Loss in iteration 140 : 3.961242292448456
Loss in iteration 141 : 4.728277269554199
Loss in iteration 142 : 3.9463492736930847
Loss in iteration 143 : 4.719034221934323
Loss in iteration 144 : 3.9398237591948497
Loss in iteration 145 : 4.712738913379391
Loss in iteration 146 : 3.931041811671029
Loss in iteration 147 : 4.6994966715239705
Loss in iteration 148 : 3.9383938658788176
Loss in iteration 149 : 4.705158323741232
Loss in iteration 150 : 3.948547719974964
Loss in iteration 151 : 4.705408493088375
Loss in iteration 152 : 3.94643001755482
Loss in iteration 153 : 4.6987625474936765
Loss in iteration 154 : 3.94831043713093
Loss in iteration 155 : 4.6826795872538876
Loss in iteration 156 : 3.9429839546220817
Loss in iteration 157 : 4.6685811941679
Loss in iteration 158 : 3.9353178164774953
Loss in iteration 159 : 4.6552379899793
Loss in iteration 160 : 3.9466525297838477
Loss in iteration 161 : 4.684478794131283
Loss in iteration 162 : 3.944223894305103
Loss in iteration 163 : 4.675157526238878
Loss in iteration 164 : 3.9509334033324497
Loss in iteration 165 : 4.6759584138901635
Loss in iteration 166 : 3.9406037921969275
Loss in iteration 167 : 4.650633913018052
Loss in iteration 168 : 3.9365597194617656
Loss in iteration 169 : 4.636261777345099
Loss in iteration 170 : 3.948456325900984
Loss in iteration 171 : 4.643206581266712
Loss in iteration 172 : 3.9499390607425107
Loss in iteration 173 : 4.643317732990812
Loss in iteration 174 : 3.947389870003124
Loss in iteration 175 : 4.634175575804014
Loss in iteration 176 : 3.9483360825859073
Loss in iteration 177 : 4.620066570404605
Loss in iteration 178 : 3.949010413641021
Loss in iteration 179 : 4.626202467249318
Loss in iteration 180 : 3.95394492757934
Loss in iteration 181 : 4.629251827954869
Loss in iteration 182 : 3.95069424882766
Loss in iteration 183 : 4.621008305097477
Loss in iteration 184 : 3.939518726149553
Loss in iteration 185 : 4.587832975419764
Loss in iteration 186 : 3.9191928102538727
Loss in iteration 187 : 4.588738381173487
Loss in iteration 188 : 3.9293289072038653
Loss in iteration 189 : 4.59267037049844
Loss in iteration 190 : 3.9220569588829632
Loss in iteration 191 : 4.584440681118514
Loss in iteration 192 : 3.9188540136378425
Loss in iteration 193 : 4.582408429768924
Loss in iteration 194 : 3.917106700855151
Loss in iteration 195 : 4.578853428226945
Loss in iteration 196 : 3.9207632230427443
Loss in iteration 197 : 4.589281717881203
Loss in iteration 198 : 3.935700732389927
Loss in iteration 199 : 4.594718862462194
Loss in iteration 200 : 3.9342173129409344
Testing accuracy  of updater 0 on alg 1 with rate 10.0 = 0.727, training accuracy 0.718875, time elapsed: 8899 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0004925100680404
Loss in iteration 3 : 1.8713901205952803
Loss in iteration 4 : 1.9119880475500073
Loss in iteration 5 : 1.2520403570475869
Loss in iteration 6 : 2.185201101902715
Loss in iteration 7 : 0.8596258100324302
Loss in iteration 8 : 1.7721198456322769
Loss in iteration 9 : 1.1777353020709833
Loss in iteration 10 : 2.022504327834357
Loss in iteration 11 : 0.8407361852396759
Loss in iteration 12 : 1.7541727409986443
Loss in iteration 13 : 1.0303888720043668
Loss in iteration 14 : 1.8113729182665652
Loss in iteration 15 : 0.9143429687935393
Loss in iteration 16 : 1.6387628355473995
Loss in iteration 17 : 1.0120698429925932
Loss in iteration 18 : 1.5824467951813903
Loss in iteration 19 : 1.001307276508493
Loss in iteration 20 : 1.4928154306456007
Loss in iteration 21 : 1.0232831470054922
Loss in iteration 22 : 1.4030287236623307
Loss in iteration 23 : 1.0419446901612057
Loss in iteration 24 : 1.3235261288987537
Loss in iteration 25 : 1.0572317357874688
Loss in iteration 26 : 1.251640078126184
Loss in iteration 27 : 1.0555132516029162
Loss in iteration 28 : 1.1905146889031695
Loss in iteration 29 : 1.058607311143815
Loss in iteration 30 : 1.1319685329611735
Loss in iteration 31 : 1.0553271343452264
Loss in iteration 32 : 1.0900408301512308
Loss in iteration 33 : 1.034103842232741
Loss in iteration 34 : 1.041202023690048
Loss in iteration 35 : 1.014684889552642
Loss in iteration 36 : 1.0082259266968687
Loss in iteration 37 : 0.9916609642996832
Loss in iteration 38 : 0.9844889532171102
Loss in iteration 39 : 0.974410041887416
Loss in iteration 40 : 0.9554011049807687
Loss in iteration 41 : 0.9569852748824802
Loss in iteration 42 : 0.9302547767867441
Loss in iteration 43 : 0.9425594427049344
Loss in iteration 44 : 0.9067112724589901
Loss in iteration 45 : 0.9244971841935838
Loss in iteration 46 : 0.8903051688570698
Loss in iteration 47 : 0.9149620110943995
Loss in iteration 48 : 0.8771582543478522
Loss in iteration 49 : 0.9067780522155898
Loss in iteration 50 : 0.8665155702980941
Loss in iteration 51 : 0.8988476355321667
Loss in iteration 52 : 0.8536317534469678
Loss in iteration 53 : 0.8850842498281482
Loss in iteration 54 : 0.8416640309963006
Loss in iteration 55 : 0.8747001342441936
Loss in iteration 56 : 0.8277949904848185
Loss in iteration 57 : 0.8646725126013611
Loss in iteration 58 : 0.8164309906582413
Loss in iteration 59 : 0.8549030162814932
Loss in iteration 60 : 0.8105281440021285
Loss in iteration 61 : 0.8458975197082156
Loss in iteration 62 : 0.8040649216378523
Loss in iteration 63 : 0.8398390317995605
Loss in iteration 64 : 0.7959726482514518
Loss in iteration 65 : 0.8256270427714187
Loss in iteration 66 : 0.7907640190667646
Loss in iteration 67 : 0.8209671387219304
Loss in iteration 68 : 0.7849826418722672
Loss in iteration 69 : 0.8172470809114533
Loss in iteration 70 : 0.780031396182955
Loss in iteration 71 : 0.8145541384555399
Loss in iteration 72 : 0.778399555281131
Loss in iteration 73 : 0.8105655736506019
Loss in iteration 74 : 0.7728580630501745
Loss in iteration 75 : 0.8051225182339585
Loss in iteration 76 : 0.7709144497669117
Loss in iteration 77 : 0.8035499701355442
Loss in iteration 78 : 0.7697521690258077
Loss in iteration 79 : 0.8007463192511928
Loss in iteration 80 : 0.765633736040771
Loss in iteration 81 : 0.7965185382620407
Loss in iteration 82 : 0.762682775801452
Loss in iteration 83 : 0.7928403809757331
Loss in iteration 84 : 0.7606685316676515
Loss in iteration 85 : 0.789973225425306
Loss in iteration 86 : 0.7574014665486016
Loss in iteration 87 : 0.7862671243833091
Loss in iteration 88 : 0.7521987114320712
Loss in iteration 89 : 0.7842953436666296
Loss in iteration 90 : 0.7497583637838832
Loss in iteration 91 : 0.7802639824746179
Loss in iteration 92 : 0.7486017065220681
Loss in iteration 93 : 0.7800309987191426
Loss in iteration 94 : 0.7480103130259421
Loss in iteration 95 : 0.778939719791322
Loss in iteration 96 : 0.7463601942328318
Loss in iteration 97 : 0.7774063951949854
Loss in iteration 98 : 0.7444168823093242
Loss in iteration 99 : 0.7764577424834831
Loss in iteration 100 : 0.7426586969745829
Loss in iteration 101 : 0.7733681043176134
Loss in iteration 102 : 0.7409923961586502
Loss in iteration 103 : 0.7729065430368873
Loss in iteration 104 : 0.738933493550684
Loss in iteration 105 : 0.7689185813397384
Loss in iteration 106 : 0.7396468843847191
Loss in iteration 107 : 0.7688079274373668
Loss in iteration 108 : 0.7381042597349501
Loss in iteration 109 : 0.7652765244856509
Loss in iteration 110 : 0.7390215697553927
Loss in iteration 111 : 0.7654445372668144
Loss in iteration 112 : 0.7368683722581493
Loss in iteration 113 : 0.7641890687922627
Loss in iteration 114 : 0.7367331642971632
Loss in iteration 115 : 0.7629764821717929
Loss in iteration 116 : 0.7357667126959689
Loss in iteration 117 : 0.7607690675216616
Loss in iteration 118 : 0.7358138788919256
Loss in iteration 119 : 0.7587459489517078
Loss in iteration 120 : 0.7355628413117272
Loss in iteration 121 : 0.7582659345991972
Loss in iteration 122 : 0.734266103542138
Loss in iteration 123 : 0.7544348541934149
Loss in iteration 124 : 0.7320388147804032
Loss in iteration 125 : 0.7512644573671984
Loss in iteration 126 : 0.7307708629978616
Loss in iteration 127 : 0.7502417748217135
Loss in iteration 128 : 0.7296083744859037
Loss in iteration 129 : 0.7491611383661405
Loss in iteration 130 : 0.7255438709450354
Loss in iteration 131 : 0.746992448281565
Loss in iteration 132 : 0.7252574210639593
Loss in iteration 133 : 0.74608899215028
Loss in iteration 134 : 0.7257549275459653
Loss in iteration 135 : 0.7460877630605789
Loss in iteration 136 : 0.7255557953272583
Loss in iteration 137 : 0.7433481839564742
Loss in iteration 138 : 0.723861374935796
Loss in iteration 139 : 0.742295093604931
Loss in iteration 140 : 0.7229828328084341
Loss in iteration 141 : 0.7405962325639631
Loss in iteration 142 : 0.7221823234753999
Loss in iteration 143 : 0.7411953065586447
Loss in iteration 144 : 0.7212437130129901
Loss in iteration 145 : 0.7391476799589212
Loss in iteration 146 : 0.7207577372204428
Loss in iteration 147 : 0.7391323934808689
Loss in iteration 148 : 0.7201701728448215
Loss in iteration 149 : 0.7386926693225643
Loss in iteration 150 : 0.7192851756710851
Loss in iteration 151 : 0.7374362033338259
Loss in iteration 152 : 0.7178497456597273
Loss in iteration 153 : 0.7340725044282131
Loss in iteration 154 : 0.7177852242673879
Loss in iteration 155 : 0.7340066539127266
Loss in iteration 156 : 0.7162617368211518
Loss in iteration 157 : 0.7326715771368404
Loss in iteration 158 : 0.7132697691082265
Loss in iteration 159 : 0.730783534670095
Loss in iteration 160 : 0.7129857720834422
Loss in iteration 161 : 0.7292616320044781
Loss in iteration 162 : 0.712362732932005
Loss in iteration 163 : 0.7286799522140941
Loss in iteration 164 : 0.711708442031647
Loss in iteration 165 : 0.7284530107452333
Loss in iteration 166 : 0.7120163005342508
Loss in iteration 167 : 0.7282370389362173
Loss in iteration 168 : 0.7121982709593772
Loss in iteration 169 : 0.7285113345069278
Loss in iteration 170 : 0.7117914121657158
Loss in iteration 171 : 0.7281394428436191
Loss in iteration 172 : 0.7123503559233628
Loss in iteration 173 : 0.7275154405643157
Loss in iteration 174 : 0.7115052603118814
Loss in iteration 175 : 0.7268022181908962
Loss in iteration 176 : 0.7096309504796693
Loss in iteration 177 : 0.7256181924832031
Loss in iteration 178 : 0.7092160408330588
Loss in iteration 179 : 0.7250597116042246
Loss in iteration 180 : 0.7087014807820445
Loss in iteration 181 : 0.7247232338795585
Loss in iteration 182 : 0.7084803051471168
Loss in iteration 183 : 0.7245551697149881
Loss in iteration 184 : 0.7079237988743843
Loss in iteration 185 : 0.7227068305544263
Loss in iteration 186 : 0.7089680565050382
Loss in iteration 187 : 0.7228883228742956
Loss in iteration 188 : 0.7087828984097436
Loss in iteration 189 : 0.7225038315851
Loss in iteration 190 : 0.7085847504114562
Loss in iteration 191 : 0.7218285852594595
Loss in iteration 192 : 0.7093241970933919
Loss in iteration 193 : 0.7218143438270033
Loss in iteration 194 : 0.7096159658616419
Loss in iteration 195 : 0.7217983060108986
Loss in iteration 196 : 0.7092242140181781
Loss in iteration 197 : 0.721018822048203
Loss in iteration 198 : 0.7091317368755469
Loss in iteration 199 : 0.7206232795959232
Loss in iteration 200 : 0.7088359100970555
Testing accuracy  of updater 0 on alg 1 with rate 1.0 = 0.752, training accuracy 0.744125, time elapsed: 6052 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9768818162030087
Loss in iteration 3 : 0.9540353271130204
Loss in iteration 4 : 0.9344581318744213
Loss in iteration 5 : 0.9221497410136664
Loss in iteration 6 : 0.9143530761802396
Loss in iteration 7 : 0.9082381952924466
Loss in iteration 8 : 0.9026218690588098
Loss in iteration 9 : 0.8971800762964925
Loss in iteration 10 : 0.8917851439620083
Loss in iteration 11 : 0.8864230045702868
Loss in iteration 12 : 0.881096348929029
Loss in iteration 13 : 0.8758051471196119
Loss in iteration 14 : 0.8705334109506986
Loss in iteration 15 : 0.8652807885480801
Loss in iteration 16 : 0.8600421729291227
Loss in iteration 17 : 0.8548233649221797
Loss in iteration 18 : 0.8496257479882556
Loss in iteration 19 : 0.8444419733592468
Loss in iteration 20 : 0.839266799072227
Loss in iteration 21 : 0.8341096644228357
Loss in iteration 22 : 0.8289574393506797
Loss in iteration 23 : 0.8238127159923625
Loss in iteration 24 : 0.8186811650058041
Loss in iteration 25 : 0.81355483768934
Loss in iteration 26 : 0.8084335701575615
Loss in iteration 27 : 0.8033208866863023
Loss in iteration 28 : 0.7982216660665336
Loss in iteration 29 : 0.7931277302443951
Loss in iteration 30 : 0.7880391315578827
Loss in iteration 31 : 0.7829549295595153
Loss in iteration 32 : 0.7778796071107055
Loss in iteration 33 : 0.7728117112657532
Loss in iteration 34 : 0.7677498742027622
Loss in iteration 35 : 0.7626969619725388
Loss in iteration 36 : 0.7576452340070612
Loss in iteration 37 : 0.7525944502048147
Loss in iteration 38 : 0.7475515560015031
Loss in iteration 39 : 0.7425395443519985
Loss in iteration 40 : 0.7375833282478745
Loss in iteration 41 : 0.7326778353343938
Loss in iteration 42 : 0.7278543569345304
Loss in iteration 43 : 0.7231104001753053
Loss in iteration 44 : 0.7185275717167283
Loss in iteration 45 : 0.7141020116082804
Loss in iteration 46 : 0.7098355798282474
Loss in iteration 47 : 0.7057244290310132
Loss in iteration 48 : 0.7017284722007301
Loss in iteration 49 : 0.6978711137199115
Loss in iteration 50 : 0.6941418606970853
Loss in iteration 51 : 0.6905048517424697
Loss in iteration 52 : 0.6869937516422737
Loss in iteration 53 : 0.6836154290269811
Loss in iteration 54 : 0.6803796762048855
Loss in iteration 55 : 0.6772288071195333
Loss in iteration 56 : 0.6741642505388656
Loss in iteration 57 : 0.6712247337131892
Loss in iteration 58 : 0.6683890578257644
Loss in iteration 59 : 0.6656646454201001
Loss in iteration 60 : 0.6630202227900438
Loss in iteration 61 : 0.6604474562241837
Loss in iteration 62 : 0.6579705425385022
Loss in iteration 63 : 0.6556048405159802
Loss in iteration 64 : 0.6533087785352085
Loss in iteration 65 : 0.6510666394916199
Loss in iteration 66 : 0.6488957642680065
Loss in iteration 67 : 0.6467899630334111
Loss in iteration 68 : 0.6447520701279311
Loss in iteration 69 : 0.6427742717960461
Loss in iteration 70 : 0.6408656923805006
Loss in iteration 71 : 0.6390324073271778
Loss in iteration 72 : 0.6372668349740604
Loss in iteration 73 : 0.6355660899433555
Loss in iteration 74 : 0.6339232291830366
Loss in iteration 75 : 0.6323269190871117
Loss in iteration 76 : 0.6307808279922492
Loss in iteration 77 : 0.6292691265812255
Loss in iteration 78 : 0.6278096985531639
Loss in iteration 79 : 0.6263931802206942
Loss in iteration 80 : 0.6250102893642144
Loss in iteration 81 : 0.623660730412259
Loss in iteration 82 : 0.6223348983981557
Loss in iteration 83 : 0.6210496535826022
Loss in iteration 84 : 0.6197881130619789
Loss in iteration 85 : 0.6185554486912236
Loss in iteration 86 : 0.6173534137994832
Loss in iteration 87 : 0.6161906925620483
Loss in iteration 88 : 0.6150557722981597
Loss in iteration 89 : 0.6139459197216024
Loss in iteration 90 : 0.6128648432678308
Loss in iteration 91 : 0.6118055613920709
Loss in iteration 92 : 0.6107690968381484
Loss in iteration 93 : 0.6097575631501633
Loss in iteration 94 : 0.6087664638095619
Loss in iteration 95 : 0.6077895598116527
Loss in iteration 96 : 0.6068414488302792
Loss in iteration 97 : 0.6059175059536204
Loss in iteration 98 : 0.6050134690971449
Loss in iteration 99 : 0.6041213249735566
Loss in iteration 100 : 0.6032476192202547
Loss in iteration 101 : 0.6023941882691285
Loss in iteration 102 : 0.6015541980526109
Loss in iteration 103 : 0.600729226873549
Loss in iteration 104 : 0.5999213757054569
Loss in iteration 105 : 0.5991273903416608
Loss in iteration 106 : 0.5983529472777721
Loss in iteration 107 : 0.5976002157681319
Loss in iteration 108 : 0.5968618343453254
Loss in iteration 109 : 0.5961395588836693
Loss in iteration 110 : 0.5954332584134141
Loss in iteration 111 : 0.5947413961732363
Loss in iteration 112 : 0.5940628141371442
Loss in iteration 113 : 0.5933959656808235
Loss in iteration 114 : 0.5927451988446159
Loss in iteration 115 : 0.5921017972571604
Loss in iteration 116 : 0.5914702192279031
Loss in iteration 117 : 0.5908469220832125
Loss in iteration 118 : 0.5902346623988042
Loss in iteration 119 : 0.5896288756688197
Loss in iteration 120 : 0.5890309415278903
Loss in iteration 121 : 0.5884385173958564
Loss in iteration 122 : 0.5878501178690428
Loss in iteration 123 : 0.5872657016380116
Loss in iteration 124 : 0.5866904441041234
Loss in iteration 125 : 0.5861210277759518
Loss in iteration 126 : 0.5855580672800502
Loss in iteration 127 : 0.5850005428105152
Loss in iteration 128 : 0.5844497681516041
Loss in iteration 129 : 0.5839086785084401
Loss in iteration 130 : 0.5833757297284786
Loss in iteration 131 : 0.5828536464309707
Loss in iteration 132 : 0.582337788367127
Loss in iteration 133 : 0.5818275035365094
Loss in iteration 134 : 0.5813254380179337
Loss in iteration 135 : 0.5808266925158709
Loss in iteration 136 : 0.5803354579837908
Loss in iteration 137 : 0.5798550522580137
Loss in iteration 138 : 0.579384264040465
Loss in iteration 139 : 0.5789213963604205
Loss in iteration 140 : 0.5784613394328177
Loss in iteration 141 : 0.5780064283685853
Loss in iteration 142 : 0.5775577675861143
Loss in iteration 143 : 0.577115448950207
Loss in iteration 144 : 0.5766836342513529
Loss in iteration 145 : 0.576255079630768
Loss in iteration 146 : 0.5758313884091636
Loss in iteration 147 : 0.5754123071326214
Loss in iteration 148 : 0.5749964275751158
Loss in iteration 149 : 0.5745824004267943
Loss in iteration 150 : 0.57417241700633
Loss in iteration 151 : 0.5737641833053687
Loss in iteration 152 : 0.5733614980507666
Loss in iteration 153 : 0.5729646939763389
Loss in iteration 154 : 0.5725708318268393
Loss in iteration 155 : 0.5721821111010846
Loss in iteration 156 : 0.5717965387496607
Loss in iteration 157 : 0.571415133776694
Loss in iteration 158 : 0.5710356545720454
Loss in iteration 159 : 0.5706590610174812
Loss in iteration 160 : 0.5702859704304012
Loss in iteration 161 : 0.5699189782785132
Loss in iteration 162 : 0.5695591279320282
Loss in iteration 163 : 0.5692033588248759
Loss in iteration 164 : 0.5688525139861458
Loss in iteration 165 : 0.5685051955112512
Loss in iteration 166 : 0.5681612904615738
Loss in iteration 167 : 0.567822889029813
Loss in iteration 168 : 0.567489242510555
Loss in iteration 169 : 0.5671598261841653
Loss in iteration 170 : 0.5668328719735743
Loss in iteration 171 : 0.5665086672379414
Loss in iteration 172 : 0.5661882267158205
Loss in iteration 173 : 0.5658710031046781
Loss in iteration 174 : 0.5655581303415604
Loss in iteration 175 : 0.5652506228883598
Loss in iteration 176 : 0.5649455808335483
Loss in iteration 177 : 0.564645477219682
Loss in iteration 178 : 0.5643483484485321
Loss in iteration 179 : 0.5640527271512823
Loss in iteration 180 : 0.5637598120321968
Loss in iteration 181 : 0.5634717475056155
Loss in iteration 182 : 0.5631870259776186
Loss in iteration 183 : 0.562906602096993
Loss in iteration 184 : 0.5626305658198724
Loss in iteration 185 : 0.562357209512757
Loss in iteration 186 : 0.5620858443576899
Loss in iteration 187 : 0.5618155878515395
Loss in iteration 188 : 0.5615468836052713
Loss in iteration 189 : 0.5612808029366286
Loss in iteration 190 : 0.5610158959352969
Loss in iteration 191 : 0.5607538627099125
Loss in iteration 192 : 0.5604938551616351
Loss in iteration 193 : 0.5602374878308592
Loss in iteration 194 : 0.5599861607145097
Loss in iteration 195 : 0.5597377303166589
Loss in iteration 196 : 0.5594914884703724
Loss in iteration 197 : 0.5592474750903995
Loss in iteration 198 : 0.5590067737584141
Loss in iteration 199 : 0.5587681573814882
Loss in iteration 200 : 0.5585317191084153
Testing accuracy  of updater 0 on alg 1 with rate 0.09999999999999998 = 0.767, training accuracy 0.77075, time elapsed: 5124 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.763745786307966
Loss in iteration 3 : 15.192325455668712
Loss in iteration 4 : 2.692923999484379
Loss in iteration 5 : 20.89676763382906
Loss in iteration 6 : 20.77063440192336
Loss in iteration 7 : 2.1060608801861576
Loss in iteration 8 : 16.87106047593724
Loss in iteration 9 : 12.272874159409149
Loss in iteration 10 : 4.827303471043771
Loss in iteration 11 : 11.683156849039792
Loss in iteration 12 : 4.30219909049656
Loss in iteration 13 : 7.861934492625091
Loss in iteration 14 : 9.418995854610037
Loss in iteration 15 : 4.2400410371208475
Loss in iteration 16 : 7.685587630432154
Loss in iteration 17 : 7.4249816584603705
Loss in iteration 18 : 4.403138134357423
Loss in iteration 19 : 7.301924106053308
Loss in iteration 20 : 6.464083078903441
Loss in iteration 21 : 4.354875461567417
Loss in iteration 22 : 6.72220983831963
Loss in iteration 23 : 5.38096438743282
Loss in iteration 24 : 4.573165513889944
Loss in iteration 25 : 6.025006308866331
Loss in iteration 26 : 4.757596189720537
Loss in iteration 27 : 4.359734642620251
Loss in iteration 28 : 5.371746205899686
Loss in iteration 29 : 3.8301961418425177
Loss in iteration 30 : 4.552542364742138
Loss in iteration 31 : 4.442119340303581
Loss in iteration 32 : 3.4573796995497967
Loss in iteration 33 : 4.2988674436090095
Loss in iteration 34 : 3.274078594244283
Loss in iteration 35 : 3.6818549255668596
Loss in iteration 36 : 3.3745076166739008
Loss in iteration 37 : 3.0706497466894436
Loss in iteration 38 : 3.260551752705866
Loss in iteration 39 : 2.733703006192725
Loss in iteration 40 : 3.0126544604649568
Loss in iteration 41 : 2.46773175496557
Loss in iteration 42 : 2.7526586428919013
Loss in iteration 43 : 2.3732020558323748
Loss in iteration 44 : 2.4244933627335645
Loss in iteration 45 : 2.4187552243596278
Loss in iteration 46 : 2.0294539061076633
Loss in iteration 47 : 2.368044802040291
Loss in iteration 48 : 2.0553646422983296
Loss in iteration 49 : 1.8660205381991266
Loss in iteration 50 : 2.1193306183789233
Loss in iteration 51 : 2.20978166970925
Loss in iteration 52 : 2.1036766617918095
Loss in iteration 53 : 1.86028847453858
Loss in iteration 54 : 1.7284034126309606
Loss in iteration 55 : 1.671849884531017
Loss in iteration 56 : 1.789929821599233
Loss in iteration 57 : 2.3575262865905
Loss in iteration 58 : 3.9769170003807526
Loss in iteration 59 : 3.0865286307731323
Loss in iteration 60 : 2.286055152518595
Loss in iteration 61 : 1.7558669083556624
Loss in iteration 62 : 1.5824399173849446
Loss in iteration 63 : 1.5229186864776165
Loss in iteration 64 : 1.590923475409566
Loss in iteration 65 : 1.7329443485645593
Loss in iteration 66 : 2.4007623522039
Loss in iteration 67 : 2.712211164334326
Loss in iteration 68 : 3.480935401111575
Loss in iteration 69 : 1.8711694005020751
Loss in iteration 70 : 1.6038480054782351
Loss in iteration 71 : 1.5124455013002454
Loss in iteration 72 : 1.9748340040010772
Loss in iteration 73 : 2.7489022630334587
Loss in iteration 74 : 4.324744334698546
Loss in iteration 75 : 1.9511460525232946
Loss in iteration 76 : 1.3832429424292314
Loss in iteration 77 : 1.2490837284986298
Loss in iteration 78 : 1.1993018366501482
Loss in iteration 79 : 1.1451020932297489
Loss in iteration 80 : 1.1570578264657223
Loss in iteration 81 : 1.8452656265109628
Loss in iteration 82 : 7.238869848949034
Loss in iteration 83 : 2.970940726983489
Loss in iteration 84 : 2.998059532255129
Loss in iteration 85 : 3.9738315285162207
Loss in iteration 86 : 4.866998944217209
Loss in iteration 87 : 1.5184132860093147
Loss in iteration 88 : 2.037965433915515
Loss in iteration 89 : 4.249214206645174
Loss in iteration 90 : 1.7890847519650104
Loss in iteration 91 : 2.0996565362035575
Loss in iteration 92 : 3.5311194188404738
Loss in iteration 93 : 1.6258704784650895
Loss in iteration 94 : 2.6905886027594805
Loss in iteration 95 : 3.7154045328376943
Loss in iteration 96 : 1.5957126780964501
Loss in iteration 97 : 4.406378232800349
Loss in iteration 98 : 3.5454945022993325
Loss in iteration 99 : 2.034973223125101
Loss in iteration 100 : 5.498940147454719
Loss in iteration 101 : 2.0515515643936006
Loss in iteration 102 : 2.81474891909698
Loss in iteration 103 : 3.7466342075966397
Loss in iteration 104 : 1.6636646562119421
Loss in iteration 105 : 2.7275534179690104
Loss in iteration 106 : 2.6240508585546847
Loss in iteration 107 : 1.5922281837366588
Loss in iteration 108 : 2.2013949243784525
Loss in iteration 109 : 2.476458856321261
Loss in iteration 110 : 1.7804783681704803
Loss in iteration 111 : 1.383822501667944
Loss in iteration 112 : 1.8550816447828158
Loss in iteration 113 : 3.1058045452297436
Loss in iteration 114 : 2.654683183841378
Loss in iteration 115 : 2.4562379911570766
Loss in iteration 116 : 1.9723548046096069
Loss in iteration 117 : 2.2453098607143414
Loss in iteration 118 : 2.560040034832702
Loss in iteration 119 : 3.066979402078886
Loss in iteration 120 : 2.2379555266221636
Loss in iteration 121 : 1.777740646759987
Loss in iteration 122 : 1.4917033733952847
Loss in iteration 123 : 1.352169704872606
Loss in iteration 124 : 1.2654134117481706
Loss in iteration 125 : 1.2026855298580603
Loss in iteration 126 : 1.1536065501469996
Loss in iteration 127 : 1.1689769958282918
Loss in iteration 128 : 1.7842335486490652
Loss in iteration 129 : 5.393986570872348
Loss in iteration 130 : 5.842800853485076
Loss in iteration 131 : 1.2108223454137315
Loss in iteration 132 : 1.4727166096924575
Loss in iteration 133 : 5.684729920781866
Loss in iteration 134 : 3.210576847230981
Loss in iteration 135 : 1.6681301734866156
Loss in iteration 136 : 1.3324848312654085
Loss in iteration 137 : 1.5812183491406893
Loss in iteration 138 : 2.1980991838571406
Loss in iteration 139 : 2.192951485837869
Loss in iteration 140 : 1.9970766787938279
Loss in iteration 141 : 1.5142635112961274
Loss in iteration 142 : 1.3464486337038455
Loss in iteration 143 : 1.2516982670574044
Loss in iteration 144 : 1.1908736922904803
Loss in iteration 145 : 1.1324047796530228
Loss in iteration 146 : 1.0720122707376782
Loss in iteration 147 : 1.0786747097956417
Loss in iteration 148 : 2.4342148744949537
Loss in iteration 149 : 10.306194163411929
Loss in iteration 150 : 0.9692145581001439
Loss in iteration 151 : 10.823992707736757
Loss in iteration 152 : 1.0811188733265622
Loss in iteration 153 : 11.840723363078181
Loss in iteration 154 : 1.2163897084513993
Loss in iteration 155 : 11.495266301139592
Loss in iteration 156 : 2.5073336246936795
Loss in iteration 157 : 15.644733213362688
Loss in iteration 158 : 9.798948876664854
Loss in iteration 159 : 9.627066616366868
Loss in iteration 160 : 11.20478998759346
Loss in iteration 161 : 3.4828751006461705
Loss in iteration 162 : 8.934432491751227
Loss in iteration 163 : 3.7388703734436284
Loss in iteration 164 : 7.061438967731774
Loss in iteration 165 : 6.052004095234912
Loss in iteration 166 : 4.205559665131723
Loss in iteration 167 : 6.956458586996725
Loss in iteration 168 : 3.9694736573103686
Loss in iteration 169 : 5.255855507369655
Loss in iteration 170 : 5.38508597465924
Loss in iteration 171 : 3.6827452968351375
Loss in iteration 172 : 5.4297518943783745
Loss in iteration 173 : 3.972335416196869
Loss in iteration 174 : 4.068532608611467
Loss in iteration 175 : 4.494363414392748
Loss in iteration 176 : 3.2917601832330656
Loss in iteration 177 : 4.301611088652886
Loss in iteration 178 : 3.2466927016562606
Loss in iteration 179 : 3.7154769651654105
Loss in iteration 180 : 3.288515183586972
Loss in iteration 181 : 3.1930104416838607
Loss in iteration 182 : 3.292382485670427
Loss in iteration 183 : 2.6875072676116045
Loss in iteration 184 : 3.043940832534675
Loss in iteration 185 : 2.563334219474232
Loss in iteration 186 : 2.687301104182067
Loss in iteration 187 : 2.4412791296339136
Loss in iteration 188 : 2.247638998553178
Loss in iteration 189 : 2.527720616791961
Loss in iteration 190 : 1.90682700743057
Loss in iteration 191 : 2.415785692399336
Loss in iteration 192 : 2.2271476548480194
Loss in iteration 193 : 1.7244078265026483
Loss in iteration 194 : 2.140076525017364
Loss in iteration 195 : 2.673291996666012
Loss in iteration 196 : 2.1656193280990275
Loss in iteration 197 : 1.7602190761767393
Loss in iteration 198 : 1.5551671059765582
Loss in iteration 199 : 1.4660428799082619
Loss in iteration 200 : 1.4049825392021447
Testing accuracy  of updater 1 on alg 1 with rate 10.0 = 0.7545, training accuracy 0.762125, time elapsed: 5206 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0004925100680404
Loss in iteration 3 : 1.2941204768357535
Loss in iteration 4 : 0.9869333759951521
Loss in iteration 5 : 0.6279215581784676
Loss in iteration 6 : 0.9669243396348881
Loss in iteration 7 : 0.7506677840090307
Loss in iteration 8 : 0.5807722048770438
Loss in iteration 9 : 0.8007912089870712
Loss in iteration 10 : 0.5797757439926052
Loss in iteration 11 : 0.7613239690791653
Loss in iteration 12 : 0.599048764623973
Loss in iteration 13 : 0.7215683046466113
Loss in iteration 14 : 0.6128323791985684
Loss in iteration 15 : 0.7020067403734138
Loss in iteration 16 : 0.6257380716081996
Loss in iteration 17 : 0.6734535018716433
Loss in iteration 18 : 0.6407173742476836
Loss in iteration 19 : 0.6391008925753726
Loss in iteration 20 : 0.6450098827148334
Loss in iteration 21 : 0.6116940975356472
Loss in iteration 22 : 0.6355327057543007
Loss in iteration 23 : 0.5915596420721986
Loss in iteration 24 : 0.6216931028532579
Loss in iteration 25 : 0.5780843959379727
Loss in iteration 26 : 0.6022288914744489
Loss in iteration 27 : 0.5638436930433155
Loss in iteration 28 : 0.5831779732211546
Loss in iteration 29 : 0.5564606507872388
Loss in iteration 30 : 0.5621221414329329
Loss in iteration 31 : 0.5550539263016318
Loss in iteration 32 : 0.5407641891124875
Loss in iteration 33 : 0.5538893047661586
Loss in iteration 34 : 0.530932355972536
Loss in iteration 35 : 0.5477819095212357
Loss in iteration 36 : 0.5424085326720222
Loss in iteration 37 : 0.5286521707939144
Loss in iteration 38 : 0.5507845839484278
Loss in iteration 39 : 0.5403750222870715
Loss in iteration 40 : 0.5256188138666762
Loss in iteration 41 : 0.5427454663006516
Loss in iteration 42 : 0.5465949274197858
Loss in iteration 43 : 0.5262500365373607
Loss in iteration 44 : 0.5265489616269465
Loss in iteration 45 : 0.5402802587087515
Loss in iteration 46 : 0.5345808077033747
Loss in iteration 47 : 0.5189477144697664
Loss in iteration 48 : 0.5196290041124025
Loss in iteration 49 : 0.5280865260535829
Loss in iteration 50 : 0.5233489003169283
Loss in iteration 51 : 0.51299993994018
Loss in iteration 52 : 0.5158373111738023
Loss in iteration 53 : 0.5228165285562442
Loss in iteration 54 : 0.5165511570401125
Loss in iteration 55 : 0.5104449923545228
Loss in iteration 56 : 0.5106460729604465
Loss in iteration 57 : 0.5143668759754285
Loss in iteration 58 : 0.5160173236045664
Loss in iteration 59 : 0.5101039220951796
Loss in iteration 60 : 0.5092287714886348
Loss in iteration 61 : 0.5125556447434619
Loss in iteration 62 : 0.5117122291270029
Loss in iteration 63 : 0.5096033221871046
Loss in iteration 64 : 0.5080081037062799
Loss in iteration 65 : 0.5086367833046952
Loss in iteration 66 : 0.5104960626474697
Loss in iteration 67 : 0.5108790265016329
Loss in iteration 68 : 0.5105998819743236
Loss in iteration 69 : 0.5084032817627884
Loss in iteration 70 : 0.5072521432865736
Loss in iteration 71 : 0.5075260547233383
Loss in iteration 72 : 0.5084062243476082
Loss in iteration 73 : 0.5091477149761464
Loss in iteration 74 : 0.5085963175756895
Loss in iteration 75 : 0.5079420091868233
Loss in iteration 76 : 0.5073018892374089
Loss in iteration 77 : 0.5072093595726445
Loss in iteration 78 : 0.507655887882906
Loss in iteration 79 : 0.5081630069990711
Loss in iteration 80 : 0.5092139040307131
Loss in iteration 81 : 0.5087033809755791
Loss in iteration 82 : 0.5085320850783918
Loss in iteration 83 : 0.5073141097210982
Loss in iteration 84 : 0.5070850402550411
Loss in iteration 85 : 0.5079273310211041
Loss in iteration 86 : 0.5082591644842733
Loss in iteration 87 : 0.5086062463960387
Loss in iteration 88 : 0.5075710963596121
Loss in iteration 89 : 0.5070494508565258
Loss in iteration 90 : 0.5067927400496998
Loss in iteration 91 : 0.5068742236697148
Loss in iteration 92 : 0.5070502949566917
Loss in iteration 93 : 0.5071888898025313
Loss in iteration 94 : 0.5076038896137306
Loss in iteration 95 : 0.5075802583899345
Loss in iteration 96 : 0.507259727816939
Loss in iteration 97 : 0.5067189755440773
Loss in iteration 98 : 0.506774975385833
Loss in iteration 99 : 0.5073323963149983
Loss in iteration 100 : 0.5078930635218593
Loss in iteration 101 : 0.5080272574496012
Loss in iteration 102 : 0.5073975459802432
Loss in iteration 103 : 0.5068968376518496
Loss in iteration 104 : 0.5065987579341747
Loss in iteration 105 : 0.506794204585523
Loss in iteration 106 : 0.5073094436569942
Loss in iteration 107 : 0.50762706007543
Loss in iteration 108 : 0.5074608772724923
Loss in iteration 109 : 0.5068912171888929
Loss in iteration 110 : 0.5065821209887
Loss in iteration 111 : 0.5069016771597015
Loss in iteration 112 : 0.5071770765341476
Loss in iteration 113 : 0.5074094437682114
Loss in iteration 114 : 0.5071950829491878
Loss in iteration 115 : 0.5068737336064636
Loss in iteration 116 : 0.5065667926509019
Loss in iteration 117 : 0.5065079898196478
Loss in iteration 118 : 0.5066815473952302
Loss in iteration 119 : 0.5068127311727967
Loss in iteration 120 : 0.5067910699438192
Loss in iteration 121 : 0.5065934847624017
Loss in iteration 122 : 0.506474670562981
Loss in iteration 123 : 0.5064905789743274
Loss in iteration 124 : 0.5065372455325206
Loss in iteration 125 : 0.5065223851103634
Loss in iteration 126 : 0.5064654117578864
Loss in iteration 127 : 0.5064310943393564
Loss in iteration 128 : 0.5064816300001822
Loss in iteration 129 : 0.5065736463436308
Loss in iteration 130 : 0.5066024471216516
Loss in iteration 131 : 0.5066028499756157
Loss in iteration 132 : 0.5065189260666947
Loss in iteration 133 : 0.5064131568478375
Loss in iteration 134 : 0.5064188241176905
Loss in iteration 135 : 0.5065625076068777
Loss in iteration 136 : 0.5067104507148474
Loss in iteration 137 : 0.5066922799775286
Loss in iteration 138 : 0.5065613946208984
Loss in iteration 139 : 0.5064156277244112
Loss in iteration 140 : 0.5063900422841593
Loss in iteration 141 : 0.5064207643124757
Loss in iteration 142 : 0.5064708104002221
Loss in iteration 143 : 0.5064873507836274
Loss in iteration 144 : 0.5065210700562859
Loss in iteration 145 : 0.506511884423635
Loss in iteration 146 : 0.5064747319661204
Loss in iteration 147 : 0.5063646894957297
Loss in iteration 148 : 0.5064664010069586
Loss in iteration 149 : 0.5068655321340081
Loss in iteration 150 : 0.5073085012377624
Loss in iteration 151 : 0.50769651704969
Loss in iteration 152 : 0.5070136252915569
Loss in iteration 153 : 0.5065066718826107
Loss in iteration 154 : 0.5064272018827993
Loss in iteration 155 : 0.5068306656111297
Loss in iteration 156 : 0.5072833767188879
Loss in iteration 157 : 0.5071297973607936
Loss in iteration 158 : 0.5068109714984529
Loss in iteration 159 : 0.50645209476562
Loss in iteration 160 : 0.5065010878866916
Loss in iteration 161 : 0.5070344418411347
Loss in iteration 162 : 0.5073191616618453
Loss in iteration 163 : 0.5071845210008288
Loss in iteration 164 : 0.5066377422350579
Loss in iteration 165 : 0.5063813750006521
Loss in iteration 166 : 0.5064889074869816
Loss in iteration 167 : 0.5068023208876077
Loss in iteration 168 : 0.5070199730738513
Loss in iteration 169 : 0.5068256374635007
Loss in iteration 170 : 0.5065513151437712
Loss in iteration 171 : 0.5063422967775875
Loss in iteration 172 : 0.5063877416312307
Loss in iteration 173 : 0.5066355298386686
Loss in iteration 174 : 0.5069756625988251
Loss in iteration 175 : 0.5070560642861914
Loss in iteration 176 : 0.5066486933360959
Loss in iteration 177 : 0.5063847854966291
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.7825, training accuracy 0.788875, time elapsed: 4730 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9768818162030087
Loss in iteration 3 : 0.9356914387655049
Loss in iteration 4 : 0.9118945900533202
Loss in iteration 5 : 0.9137583144775945
Loss in iteration 6 : 0.8870448132840135
Loss in iteration 7 : 0.8429505789818175
Loss in iteration 8 : 0.8069669450114517
Loss in iteration 9 : 0.777993452266407
Loss in iteration 10 : 0.7538595475209122
Loss in iteration 11 : 0.7383310836441089
Loss in iteration 12 : 0.7035617414134803
Loss in iteration 13 : 0.6618248637977703
Loss in iteration 14 : 0.6406558394247839
Loss in iteration 15 : 0.6316318372492509
Loss in iteration 16 : 0.6171921963174088
Loss in iteration 17 : 0.5970656732582997
Loss in iteration 18 : 0.5840599181380626
Loss in iteration 19 : 0.5816541615321488
Loss in iteration 20 : 0.579813591709691
Loss in iteration 21 : 0.5717031420861944
Loss in iteration 22 : 0.563375240304724
Loss in iteration 23 : 0.5610869722928307
Loss in iteration 24 : 0.5617294982070873
Loss in iteration 25 : 0.5595358890657884
Loss in iteration 26 : 0.554546115664452
Loss in iteration 27 : 0.5506166910042721
Loss in iteration 28 : 0.5500289237521467
Loss in iteration 29 : 0.5500566778729481
Loss in iteration 30 : 0.5481488047672384
Loss in iteration 31 : 0.544987822639911
Loss in iteration 32 : 0.5428543627859023
Loss in iteration 33 : 0.5425921156233072
Loss in iteration 34 : 0.5424061035815974
Loss in iteration 35 : 0.5409248306601878
Loss in iteration 36 : 0.5388872365484637
Loss in iteration 37 : 0.5378430468893446
Loss in iteration 38 : 0.5376280551586711
Loss in iteration 39 : 0.5372360283986425
Loss in iteration 40 : 0.5360200029800752
Loss in iteration 41 : 0.5346109733948743
Loss in iteration 42 : 0.5338398653953543
Loss in iteration 43 : 0.5335936315334538
Loss in iteration 44 : 0.5330940146361564
Loss in iteration 45 : 0.5321101215152496
Loss in iteration 46 : 0.531135584225525
Loss in iteration 47 : 0.5306263181185141
Loss in iteration 48 : 0.5304025871888745
Loss in iteration 49 : 0.5298591391006243
Loss in iteration 50 : 0.5290228827269582
Loss in iteration 51 : 0.528425469253492
Loss in iteration 52 : 0.5280927855143002
Loss in iteration 53 : 0.527708591803994
Loss in iteration 54 : 0.5270999818608847
Loss in iteration 55 : 0.5264641262675764
Loss in iteration 56 : 0.5259901996146205
Loss in iteration 57 : 0.5256328547693906
Loss in iteration 58 : 0.5251853053862833
Loss in iteration 59 : 0.5246363087431127
Loss in iteration 60 : 0.5241641720334889
Loss in iteration 61 : 0.5238278413828213
Loss in iteration 62 : 0.5234912060010783
Loss in iteration 63 : 0.5230791411973096
Loss in iteration 64 : 0.5226689013868606
Loss in iteration 65 : 0.5223328788576102
Loss in iteration 66 : 0.5220444162193919
Loss in iteration 67 : 0.5217247447340492
Loss in iteration 68 : 0.5213758728750943
Loss in iteration 69 : 0.5210449187990712
Loss in iteration 70 : 0.5207569364242892
Loss in iteration 71 : 0.5204934227913243
Loss in iteration 72 : 0.5202114988026856
Loss in iteration 73 : 0.519918547397452
Loss in iteration 74 : 0.5196558186208099
Loss in iteration 75 : 0.5194263913757091
Loss in iteration 76 : 0.5192056997823393
Loss in iteration 77 : 0.5189767232641759
Loss in iteration 78 : 0.5187489010466194
Loss in iteration 79 : 0.5185279597002425
Loss in iteration 80 : 0.5183285564975517
Loss in iteration 81 : 0.518141470877168
Loss in iteration 82 : 0.5179484290408807
Loss in iteration 83 : 0.5177514612005636
Loss in iteration 84 : 0.5175636245621216
Loss in iteration 85 : 0.5173900629331264
Loss in iteration 86 : 0.5172207704329215
Loss in iteration 87 : 0.5170522486004123
Loss in iteration 88 : 0.5168857317618957
Loss in iteration 89 : 0.5167301520244031
Loss in iteration 90 : 0.5165802634470972
Loss in iteration 91 : 0.5164316938747536
Loss in iteration 92 : 0.5162869447497831
Loss in iteration 93 : 0.5161476630115575
Loss in iteration 94 : 0.5160139611916227
Loss in iteration 95 : 0.5158812663749297
Loss in iteration 96 : 0.5157522341064703
Loss in iteration 97 : 0.5156282845463822
Loss in iteration 98 : 0.5155066290880973
Loss in iteration 99 : 0.5153880852851
Loss in iteration 100 : 0.5152727041671321
Loss in iteration 101 : 0.5151591522951884
Loss in iteration 102 : 0.5150480154956079
Loss in iteration 103 : 0.5149388938285988
Loss in iteration 104 : 0.5148318408623969
Loss in iteration 105 : 0.5147278278915919
Loss in iteration 106 : 0.5146269827971119
Loss in iteration 107 : 0.5145279504063905
Loss in iteration 108 : 0.5144299691796376
Loss in iteration 109 : 0.5143344821589192
Loss in iteration 110 : 0.5142409068327464
Loss in iteration 111 : 0.5141488256537433
Loss in iteration 112 : 0.5140586358855698
Loss in iteration 113 : 0.513970555143977
Loss in iteration 114 : 0.5138851376669684
Loss in iteration 115 : 0.5138020826615145
Loss in iteration 116 : 0.5137214074037925
Loss in iteration 117 : 0.5136427997467621
Loss in iteration 118 : 0.5135653231987395
Loss in iteration 119 : 0.5134896571216034
Loss in iteration 120 : 0.5134153236654053
Loss in iteration 121 : 0.513341739216133
Loss in iteration 122 : 0.5132689812616512
Loss in iteration 123 : 0.51319754238064
Loss in iteration 124 : 0.513127229779527
Loss in iteration 125 : 0.5130583414601947
Loss in iteration 126 : 0.5129902493149708
Loss in iteration 127 : 0.5129233980628927
Loss in iteration 128 : 0.51285744853537
Loss in iteration 129 : 0.5127925384387859
Loss in iteration 130 : 0.5127279787547178
Loss in iteration 131 : 0.5126643248452984
Loss in iteration 132 : 0.5126022477673436
Loss in iteration 133 : 0.5125407886648998
Loss in iteration 134 : 0.512480669810705
Loss in iteration 135 : 0.5124218557527769
Loss in iteration 136 : 0.512364570088035
Loss in iteration 137 : 0.5123086180863085
Loss in iteration 138 : 0.512253493152425
Loss in iteration 139 : 0.5121997278821793
Loss in iteration 140 : 0.5121475734698838
Loss in iteration 141 : 0.5120960459482822
Loss in iteration 142 : 0.5120455452958932
Loss in iteration 143 : 0.511996364001815
Loss in iteration 144 : 0.511947699618443
Loss in iteration 145 : 0.5118999886614668
Loss in iteration 146 : 0.5118530805534999
Loss in iteration 147 : 0.5118072104086521
Loss in iteration 148 : 0.5117623715376299
Loss in iteration 149 : 0.5117181394850724
Loss in iteration 150 : 0.5116744444167968
Loss in iteration 151 : 0.5116311401379962
Loss in iteration 152 : 0.5115893230926527
Loss in iteration 153 : 0.5115490116619428
Loss in iteration 154 : 0.5115098419634116
Loss in iteration 155 : 0.5114718652234453
Loss in iteration 156 : 0.5114346690057584
Loss in iteration 157 : 0.5113976931252474
Loss in iteration 158 : 0.5113614402830569
Loss in iteration 159 : 0.5113265085397329
Loss in iteration 160 : 0.5112920422147638
Loss in iteration 161 : 0.5112580364476331
Loss in iteration 162 : 0.5112249815228985
Loss in iteration 163 : 0.5111926290925295
Loss in iteration 164 : 0.5111608248312959
Loss in iteration 165 : 0.5111292346707919
Loss in iteration 166 : 0.511098125451779
Loss in iteration 167 : 0.5110686873215605
Loss in iteration 168 : 0.5110402221606719
Loss in iteration 169 : 0.5110117690614433
Loss in iteration 170 : 0.5109841201322194
Loss in iteration 171 : 0.5109570071405121
Loss in iteration 172 : 0.5109300788876555
Loss in iteration 173 : 0.5109034570448474
Loss in iteration 174 : 0.5108769792660854
Loss in iteration 175 : 0.5108506412190572
Loss in iteration 176 : 0.5108246195330843
Loss in iteration 177 : 0.5107987067506901
Loss in iteration 178 : 0.5107728682503518
Loss in iteration 179 : 0.5107472270844594
Loss in iteration 180 : 0.5107221209434536
Loss in iteration 181 : 0.510697250859685
Loss in iteration 182 : 0.5106725229620908
Loss in iteration 183 : 0.5106484382644434
Loss in iteration 184 : 0.5106249306814737
Loss in iteration 185 : 0.5106019416315288
Loss in iteration 186 : 0.5105793725652115
Loss in iteration 187 : 0.5105567415951761
Loss in iteration 188 : 0.5105347740823046
Loss in iteration 189 : 0.5105135812055054
Loss in iteration 190 : 0.5104923169512385
Loss in iteration 191 : 0.5104714626389569
Loss in iteration 192 : 0.5104508203077847
Loss in iteration 193 : 0.5104301502043033
Loss in iteration 194 : 0.5104097217125989
Loss in iteration 195 : 0.5103894860284994
Loss in iteration 196 : 0.510369580469495
Loss in iteration 197 : 0.510349881301142
Loss in iteration 198 : 0.5103304099765968
Loss in iteration 199 : 0.5103111835235562
Loss in iteration 200 : 0.5102920331198262
Testing accuracy  of updater 1 on alg 1 with rate 0.09999999999999998 = 0.781, training accuracy 0.7875, time elapsed: 4756 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 8.564666993985139
Loss in iteration 3 : 34.33338142037817
Loss in iteration 4 : 4.675319581639313
Loss in iteration 5 : 22.953467691803933
Loss in iteration 6 : 12.961440335483202
Loss in iteration 7 : 9.647490828672865
Loss in iteration 8 : 10.751406546504127
Loss in iteration 9 : 9.555753878871464
Loss in iteration 10 : 5.625458640322839
Loss in iteration 11 : 5.360597582341722
Loss in iteration 12 : 4.733616280922366
Loss in iteration 13 : 4.579349251092102
Loss in iteration 14 : 4.341775651457716
Loss in iteration 15 : 4.271199082621461
Loss in iteration 16 : 4.219333724372289
Loss in iteration 17 : 4.17829221540404
Loss in iteration 18 : 4.117333531076335
Loss in iteration 19 : 4.033339444146747
Loss in iteration 20 : 3.9372446051032406
Loss in iteration 21 : 3.8299868930183716
Loss in iteration 22 : 3.715596366765727
Loss in iteration 23 : 3.593074681313239
Loss in iteration 24 : 3.4617693338312057
Loss in iteration 25 : 3.3244931545479046
Loss in iteration 26 : 3.182940610949713
Loss in iteration 27 : 3.041212380012098
Loss in iteration 28 : 2.9048198817092863
Loss in iteration 29 : 2.7936501573113213
Loss in iteration 30 : 2.7524566472282035
Loss in iteration 31 : 3.0253581654595627
Loss in iteration 32 : 4.501382875350711
Loss in iteration 33 : 10.275495316780649
Loss in iteration 34 : 8.933854090581281
Loss in iteration 35 : 11.422724318824715
Loss in iteration 36 : 8.358359808043629
Loss in iteration 37 : 10.369866298604702
Loss in iteration 38 : 8.157048997875915
Loss in iteration 39 : 8.740367728602926
Loss in iteration 40 : 7.356908734100551
Loss in iteration 41 : 7.365332023979656
Loss in iteration 42 : 6.381863119179969
Loss in iteration 43 : 6.247447395863469
Loss in iteration 44 : 5.5588738537089615
Loss in iteration 45 : 5.478567134885135
Loss in iteration 46 : 4.992019399125635
Loss in iteration 47 : 5.074329619644608
Loss in iteration 48 : 4.890390829514634
Loss in iteration 49 : 5.1350745230094805
Loss in iteration 50 : 4.824298977615294
Loss in iteration 51 : 5.229855788897719
Loss in iteration 52 : 4.8375979946657255
Loss in iteration 53 : 5.524062792634052
Loss in iteration 54 : 5.008089971636968
Loss in iteration 55 : 5.949635315938987
Loss in iteration 56 : 5.3085023096219395
Loss in iteration 57 : 6.536130915087358
Loss in iteration 58 : 5.755417674157729
Loss in iteration 59 : 6.934088427095312
Loss in iteration 60 : 5.700820673140859
Loss in iteration 61 : 6.803160548799963
Loss in iteration 62 : 5.68622445207063
Loss in iteration 63 : 6.448672702270495
Loss in iteration 64 : 5.573562837991056
Loss in iteration 65 : 6.09243772498076
Loss in iteration 66 : 5.3306343933813185
Loss in iteration 67 : 5.777148491566213
Loss in iteration 68 : 5.1844490718736145
Loss in iteration 69 : 5.611140240571337
Loss in iteration 70 : 5.071702899641797
Loss in iteration 71 : 5.626310935547045
Loss in iteration 72 : 5.174513351700941
Loss in iteration 73 : 5.8559152673613335
Loss in iteration 74 : 5.442165917535233
Loss in iteration 75 : 6.100524116230759
Loss in iteration 76 : 5.525985999252464
Loss in iteration 77 : 6.238603092004396
Loss in iteration 78 : 5.641743793679617
Loss in iteration 79 : 6.390798293246624
Loss in iteration 80 : 5.636981310594732
Loss in iteration 81 : 6.277679077034407
Loss in iteration 82 : 5.617634844736025
Loss in iteration 83 : 6.2126280580108935
Loss in iteration 84 : 5.500129880360942
Loss in iteration 85 : 6.096194043327065
Loss in iteration 86 : 5.477582590092687
Loss in iteration 87 : 6.089225680785725
Loss in iteration 88 : 5.477535607850748
Loss in iteration 89 : 6.049938212798776
Loss in iteration 90 : 5.424624398566012
Loss in iteration 91 : 6.027714827031472
Loss in iteration 92 : 5.399603141739544
Loss in iteration 93 : 6.011976174341553
Loss in iteration 94 : 5.423698915695345
Loss in iteration 95 : 6.049960484782728
Loss in iteration 96 : 5.4285920271050925
Loss in iteration 97 : 6.056752094350621
Loss in iteration 98 : 5.4704732311784525
Loss in iteration 99 : 6.1009323944649
Loss in iteration 100 : 5.462703354380049
Loss in iteration 101 : 6.058933425394786
Loss in iteration 102 : 5.4638211039268345
Loss in iteration 103 : 6.06164006996979
Loss in iteration 104 : 5.486261370731347
Loss in iteration 105 : 6.105615346630856
Loss in iteration 106 : 5.5085882051252195
Loss in iteration 107 : 6.11580267587216
Loss in iteration 108 : 5.501167047080606
Loss in iteration 109 : 6.0816353199017374
Loss in iteration 110 : 5.5057879346684
Loss in iteration 111 : 6.085718348325192
Loss in iteration 112 : 5.509262596074578
Loss in iteration 113 : 6.067307625675298
Loss in iteration 114 : 5.478460462293285
Loss in iteration 115 : 6.062764721351535
Loss in iteration 116 : 5.497779218420283
Loss in iteration 117 : 6.060540942995402
Loss in iteration 118 : 5.4933421932573685
Loss in iteration 119 : 6.072458209763873
Loss in iteration 120 : 5.494429495588256
Loss in iteration 121 : 6.084245837200605
Loss in iteration 122 : 5.494667741110771
Loss in iteration 123 : 6.0720047285214305
Loss in iteration 124 : 5.4977902928196265
Loss in iteration 125 : 6.073384644544704
Loss in iteration 126 : 5.489798773965958
Loss in iteration 127 : 6.060795852804229
Loss in iteration 128 : 5.502231244807698
Loss in iteration 129 : 6.0715771973874935
Loss in iteration 130 : 5.488017305987156
Loss in iteration 131 : 6.053920492325474
Loss in iteration 132 : 5.475906594609575
Loss in iteration 133 : 6.037582281024238
Loss in iteration 134 : 5.442606034477912
Loss in iteration 135 : 6.013920410612315
Loss in iteration 136 : 5.45815464327687
Loss in iteration 137 : 6.066357508588369
Loss in iteration 138 : 5.518293295727925
Loss in iteration 139 : 6.085163399663221
Loss in iteration 140 : 5.509663953957517
Loss in iteration 141 : 6.089175519555024
Loss in iteration 142 : 5.4964084852867146
Loss in iteration 143 : 6.067313181459438
Loss in iteration 144 : 5.492483499744015
Loss in iteration 145 : 6.04721980257647
Loss in iteration 146 : 5.44440640125375
Loss in iteration 147 : 6.034277343838956
Loss in iteration 148 : 5.496722716868191
Loss in iteration 149 : 6.064499249486924
Loss in iteration 150 : 5.463637570491856
Loss in iteration 151 : 6.05392831298924
Loss in iteration 152 : 5.479451227817197
Loss in iteration 153 : 6.03443050404922
Loss in iteration 154 : 5.434322914577792
Loss in iteration 155 : 6.034626652739669
Loss in iteration 156 : 5.5036105688834756
Loss in iteration 157 : 6.06314772175124
Loss in iteration 158 : 5.477523108838094
Loss in iteration 159 : 6.0492581773565615
Loss in iteration 160 : 5.435542796442101
Loss in iteration 161 : 6.033462957240349
Loss in iteration 162 : 5.4603755740842885
Loss in iteration 163 : 6.060097472640276
Loss in iteration 164 : 5.505462797661603
Loss in iteration 165 : 6.067058203917948
Loss in iteration 166 : 5.517512048868055
Loss in iteration 167 : 6.05999718757912
Loss in iteration 168 : 5.440107643844262
Loss in iteration 169 : 6.011433747742303
Loss in iteration 170 : 5.454495033702291
Loss in iteration 171 : 6.022927627022018
Loss in iteration 172 : 5.464179924616258
Loss in iteration 173 : 6.040094737965842
Loss in iteration 174 : 5.5062889968123745
Loss in iteration 175 : 6.064185372512792
Loss in iteration 176 : 5.484089812513757
Loss in iteration 177 : 6.025597621690891
Loss in iteration 178 : 5.466910037090343
Loss in iteration 179 : 6.025755485490119
Loss in iteration 180 : 5.467128432927561
Loss in iteration 181 : 6.027228706956528
Loss in iteration 182 : 5.46063733680895
Loss in iteration 183 : 6.027698409027241
Loss in iteration 184 : 5.4758947985303354
Loss in iteration 185 : 6.068597946011399
Loss in iteration 186 : 5.493790793969662
Loss in iteration 187 : 6.062867005431861
Loss in iteration 188 : 5.475554203842094
Loss in iteration 189 : 6.01571085373876
Loss in iteration 190 : 5.470584153089862
Loss in iteration 191 : 6.04310714243379
Loss in iteration 192 : 5.446660943254442
Loss in iteration 193 : 6.012794647348872
Loss in iteration 194 : 5.464334861130846
Loss in iteration 195 : 6.040052633909656
Loss in iteration 196 : 5.476394628454418
Loss in iteration 197 : 6.054836794819137
Loss in iteration 198 : 5.48171384654826
Loss in iteration 199 : 6.044773673508353
Loss in iteration 200 : 5.458466750090156
Testing accuracy  of updater 2 on alg 1 with rate 10.0 = 0.789, training accuracy 0.782625, time elapsed: 5602 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3464210846018347
Loss in iteration 3 : 3.7354572336382685
Loss in iteration 4 : 1.1015662955488141
Loss in iteration 5 : 2.4103402991671783
Loss in iteration 6 : 2.0750304424941906
Loss in iteration 7 : 1.0367438584063136
Loss in iteration 8 : 1.5060627578120713
Loss in iteration 9 : 1.5129532821095246
Loss in iteration 10 : 0.9629460160494465
Loss in iteration 11 : 1.0419816040657284
Loss in iteration 12 : 0.907708458017572
Loss in iteration 13 : 0.8789252821355434
Loss in iteration 14 : 0.806202983396027
Loss in iteration 15 : 0.7790282617310741
Loss in iteration 16 : 0.7490426125583874
Loss in iteration 17 : 0.7363790230028867
Loss in iteration 18 : 0.7298830657895325
Loss in iteration 19 : 0.7227081670322477
Loss in iteration 20 : 0.7150003070837574
Loss in iteration 21 : 0.707590665129739
Loss in iteration 22 : 0.6991166036476592
Loss in iteration 23 : 0.6898936258726438
Loss in iteration 24 : 0.6800997132850604
Loss in iteration 25 : 0.6696282618429337
Loss in iteration 26 : 0.6585568678870709
Loss in iteration 27 : 0.6469302589344412
Loss in iteration 28 : 0.635086549586362
Loss in iteration 29 : 0.6235430390655187
Loss in iteration 30 : 0.6122647811939556
Loss in iteration 31 : 0.6017724398501553
Loss in iteration 32 : 0.5925039168776309
Loss in iteration 33 : 0.5861162244422092
Loss in iteration 34 : 0.5877844087561042
Loss in iteration 35 : 0.6320191282174373
Loss in iteration 36 : 0.8461005593770804
Loss in iteration 37 : 1.2977153549721925
Loss in iteration 38 : 1.1561055718633046
Loss in iteration 39 : 1.2930330058239352
Loss in iteration 40 : 1.1333441035432863
Loss in iteration 41 : 1.2462246835013118
Loss in iteration 42 : 1.073498314244147
Loss in iteration 43 : 1.1308820132135475
Loss in iteration 44 : 1.0483396424750961
Loss in iteration 45 : 1.0291064813087452
Loss in iteration 46 : 0.949805242024064
Loss in iteration 47 : 0.944097514062593
Loss in iteration 48 : 0.8944066503225314
Loss in iteration 49 : 0.8887828518043982
Loss in iteration 50 : 0.8478106301688816
Loss in iteration 51 : 0.840398056976962
Loss in iteration 52 : 0.8183180555001254
Loss in iteration 53 : 0.8176232589709086
Loss in iteration 54 : 0.792984161142449
Loss in iteration 55 : 0.8045955543442936
Loss in iteration 56 : 0.7965702562400806
Loss in iteration 57 : 0.828859173320217
Loss in iteration 58 : 0.7957538403582746
Loss in iteration 59 : 0.8404976055880169
Loss in iteration 60 : 0.8005865589918881
Loss in iteration 61 : 0.8574803251408114
Loss in iteration 62 : 0.8117099867770128
Loss in iteration 63 : 0.8713698266169512
Loss in iteration 64 : 0.8164380039629165
Loss in iteration 65 : 0.8817951722053611
Loss in iteration 66 : 0.8260197077037414
Loss in iteration 67 : 0.8874642451515894
Loss in iteration 68 : 0.8276313662370042
Loss in iteration 69 : 0.8805855646098805
Loss in iteration 70 : 0.8281406283232514
Loss in iteration 71 : 0.8710511999581987
Loss in iteration 72 : 0.8229715088554266
Loss in iteration 73 : 0.8586018459527098
Loss in iteration 74 : 0.8232193714405593
Loss in iteration 75 : 0.8599308838807482
Loss in iteration 76 : 0.824649136798555
Loss in iteration 77 : 0.8616294269934665
Loss in iteration 78 : 0.8250096829567272
Loss in iteration 79 : 0.8691968705340325
Loss in iteration 80 : 0.8367290539265939
Loss in iteration 81 : 0.8779764576630473
Loss in iteration 82 : 0.8423933750940755
Loss in iteration 83 : 0.8851550279855209
Loss in iteration 84 : 0.8486560210788598
Loss in iteration 85 : 0.8865895559226913
Loss in iteration 86 : 0.8463402910328143
Loss in iteration 87 : 0.8844419408487941
Loss in iteration 88 : 0.8456767535903359
Loss in iteration 89 : 0.8806974821744856
Loss in iteration 90 : 0.8366672180586676
Loss in iteration 91 : 0.8709596694703603
Loss in iteration 92 : 0.8370202431684947
Loss in iteration 93 : 0.8738118571760998
Loss in iteration 94 : 0.8352332588096775
Loss in iteration 95 : 0.8703157747006585
Loss in iteration 96 : 0.8282785761161754
Loss in iteration 97 : 0.8694486040939695
Loss in iteration 98 : 0.8294311615121358
Loss in iteration 99 : 0.8688021534683671
Loss in iteration 100 : 0.8278649716849465
Loss in iteration 101 : 0.8698639998474692
Loss in iteration 102 : 0.8307467309680567
Loss in iteration 103 : 0.8712890702763292
Loss in iteration 104 : 0.8298883450297407
Loss in iteration 105 : 0.8713960895246903
Loss in iteration 106 : 0.8298121686829143
Loss in iteration 107 : 0.8718198964549071
Loss in iteration 108 : 0.8326884296511995
Loss in iteration 109 : 0.8744753479483406
Loss in iteration 110 : 0.8353587701827038
Loss in iteration 111 : 0.8759256478932345
Loss in iteration 112 : 0.8349514571128018
Loss in iteration 113 : 0.8746665579571763
Loss in iteration 114 : 0.8327164603865648
Loss in iteration 115 : 0.8734262155021495
Loss in iteration 116 : 0.8331274761693644
Loss in iteration 117 : 0.8724763108704696
Loss in iteration 118 : 0.833159498818757
Loss in iteration 119 : 0.8725260143214238
Loss in iteration 120 : 0.8340741001378535
Loss in iteration 121 : 0.8750291441014725
Loss in iteration 122 : 0.8344826507037132
Loss in iteration 123 : 0.8742614556948041
Loss in iteration 124 : 0.8332100323444924
Loss in iteration 125 : 0.8702923049812881
Loss in iteration 126 : 0.8274449743447724
Loss in iteration 127 : 0.866755517600558
Loss in iteration 128 : 0.8242938825022318
Loss in iteration 129 : 0.8667687662370039
Loss in iteration 130 : 0.8271491133325634
Loss in iteration 131 : 0.8710408530493754
Loss in iteration 132 : 0.8371180647084596
Loss in iteration 133 : 0.8777557195449309
Loss in iteration 134 : 0.839993634680615
Loss in iteration 135 : 0.8769933236712368
Loss in iteration 136 : 0.8382147183113027
Loss in iteration 137 : 0.8772258614245533
Loss in iteration 138 : 0.8362809647187218
Loss in iteration 139 : 0.8710229664684455
Loss in iteration 140 : 0.8309785241008134
Loss in iteration 141 : 0.8675844873080525
Loss in iteration 142 : 0.8221885935351552
Loss in iteration 143 : 0.8619041853441795
Loss in iteration 144 : 0.8176964172695439
Loss in iteration 145 : 0.863855013121598
Loss in iteration 146 : 0.8247000560763776
Loss in iteration 147 : 0.8678257246215355
Loss in iteration 148 : 0.8323697122163538
Loss in iteration 149 : 0.8725136716969341
Loss in iteration 150 : 0.8377051463150521
Loss in iteration 151 : 0.878937592167602
Loss in iteration 152 : 0.839728966392096
Loss in iteration 153 : 0.8776089391225885
Loss in iteration 154 : 0.8356736550404453
Loss in iteration 155 : 0.8702178964512204
Loss in iteration 156 : 0.8290775225721673
Loss in iteration 157 : 0.8637350341403294
Loss in iteration 158 : 0.822746359192901
Loss in iteration 159 : 0.8618569710620773
Loss in iteration 160 : 0.82179676561747
Loss in iteration 161 : 0.8639193659009093
Loss in iteration 162 : 0.8262270626911543
Loss in iteration 163 : 0.8682703210578895
Loss in iteration 164 : 0.8295197527432094
Loss in iteration 165 : 0.8696631313943605
Loss in iteration 166 : 0.8355317185943139
Loss in iteration 167 : 0.8772723008613241
Loss in iteration 168 : 0.8365980394142328
Loss in iteration 169 : 0.8729789016337445
Loss in iteration 170 : 0.8351305994145186
Loss in iteration 171 : 0.8699208760583367
Loss in iteration 172 : 0.8303875751429937
Loss in iteration 173 : 0.8673178662319706
Loss in iteration 174 : 0.8248593714804151
Loss in iteration 175 : 0.8654015693757414
Loss in iteration 176 : 0.825133468461032
Loss in iteration 177 : 0.8655961363557237
Loss in iteration 178 : 0.8262226764658304
Loss in iteration 179 : 0.869840459641806
Loss in iteration 180 : 0.8342436598160998
Loss in iteration 181 : 0.8733343514118876
Loss in iteration 182 : 0.8386240759010353
Loss in iteration 183 : 0.8746401355309574
Loss in iteration 184 : 0.8334417857536068
Loss in iteration 185 : 0.8684360010330239
Loss in iteration 186 : 0.8279967534216678
Loss in iteration 187 : 0.8676860642966925
Loss in iteration 188 : 0.8268462313564824
Loss in iteration 189 : 0.8667319521967726
Loss in iteration 190 : 0.8271485136904274
Loss in iteration 191 : 0.866277009510719
Loss in iteration 192 : 0.827922549653665
Loss in iteration 193 : 0.8710211876025172
Loss in iteration 194 : 0.8352023200482362
Loss in iteration 195 : 0.8712157538654469
Loss in iteration 196 : 0.8333052480764407
Loss in iteration 197 : 0.8693873960873657
Loss in iteration 198 : 0.8297475748516918
Loss in iteration 199 : 0.8676126071877288
Loss in iteration 200 : 0.8255845848790381
Testing accuracy  of updater 2 on alg 1 with rate 1.0 = 0.79, training accuracy 0.784125, time elapsed: 6245 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9562833339385243
Loss in iteration 3 : 0.9151041458516824
Loss in iteration 4 : 0.903145843478236
Loss in iteration 5 : 0.8756135068483792
Loss in iteration 6 : 0.8464201034407038
Loss in iteration 7 : 0.8168193527516876
Loss in iteration 8 : 0.786262664788849
Loss in iteration 9 : 0.7541128418534018
Loss in iteration 10 : 0.7214528212914189
Loss in iteration 11 : 0.6935804189682547
Loss in iteration 12 : 0.6679083150732168
Loss in iteration 13 : 0.6442895948804958
Loss in iteration 14 : 0.6250960080791537
Loss in iteration 15 : 0.6101215133606693
Loss in iteration 16 : 0.5981960898365524
Loss in iteration 17 : 0.5885878773270691
Loss in iteration 18 : 0.5808346356886058
Loss in iteration 19 : 0.5745479929062357
Loss in iteration 20 : 0.5693767377336549
Loss in iteration 21 : 0.5650182243441497
Loss in iteration 22 : 0.5613350491647174
Loss in iteration 23 : 0.5581914818118883
Loss in iteration 24 : 0.5554844156707136
Loss in iteration 25 : 0.553123758565479
Loss in iteration 26 : 0.550965815061217
Loss in iteration 27 : 0.5489913212494536
Loss in iteration 28 : 0.5472016207794883
Loss in iteration 29 : 0.5455856887609409
Loss in iteration 30 : 0.5441170948437487
Loss in iteration 31 : 0.5427484664836976
Loss in iteration 32 : 0.5414626074222828
Loss in iteration 33 : 0.5403038467162737
Loss in iteration 34 : 0.53923752155976
Loss in iteration 35 : 0.5382536369908153
Loss in iteration 36 : 0.5373666280655284
Loss in iteration 37 : 0.536541176079028
Loss in iteration 38 : 0.5357506529036143
Loss in iteration 39 : 0.5349892395359334
Loss in iteration 40 : 0.5342613055504873
Loss in iteration 41 : 0.5335609316952006
Loss in iteration 42 : 0.5328791821819128
Loss in iteration 43 : 0.5322211793826285
Loss in iteration 44 : 0.5315984527887244
Loss in iteration 45 : 0.5309975987516
Loss in iteration 46 : 0.5304247045075982
Loss in iteration 47 : 0.5298777944603967
Loss in iteration 48 : 0.5293476465479098
Loss in iteration 49 : 0.5288338013007162
Loss in iteration 50 : 0.5283343921824616
Loss in iteration 51 : 0.5278435240709319
Loss in iteration 52 : 0.5273634671845504
Loss in iteration 53 : 0.5268917211834447
Loss in iteration 54 : 0.526430772013159
Loss in iteration 55 : 0.5259821891588069
Loss in iteration 56 : 0.5255463471063625
Loss in iteration 57 : 0.5251274432256451
Loss in iteration 58 : 0.5247223325506553
Loss in iteration 59 : 0.5243301628067818
Loss in iteration 60 : 0.52395029942085
Loss in iteration 61 : 0.5235851477323938
Loss in iteration 62 : 0.523234210414138
Loss in iteration 63 : 0.5228943625936188
Loss in iteration 64 : 0.5225665950413076
Loss in iteration 65 : 0.5222476836097105
Loss in iteration 66 : 0.5219370081744766
Loss in iteration 67 : 0.5216328552512249
Loss in iteration 68 : 0.5213372295978647
Loss in iteration 69 : 0.5210488001542292
Loss in iteration 70 : 0.5207704829168756
Loss in iteration 71 : 0.5205010082489719
Loss in iteration 72 : 0.5202412028281086
Loss in iteration 73 : 0.5199903724343073
Loss in iteration 74 : 0.5197500026898331
Loss in iteration 75 : 0.5195184289227384
Loss in iteration 76 : 0.5192945001053141
Loss in iteration 77 : 0.519076519035915
Loss in iteration 78 : 0.5188649440952567
Loss in iteration 79 : 0.5186589814073531
Loss in iteration 80 : 0.5184575507668279
Loss in iteration 81 : 0.5182619470261948
Loss in iteration 82 : 0.518073754188806
Loss in iteration 83 : 0.5178907089904556
Loss in iteration 84 : 0.5177126878499387
Loss in iteration 85 : 0.5175398979067752
Loss in iteration 86 : 0.5173711936878099
Loss in iteration 87 : 0.5172069549616394
Loss in iteration 88 : 0.5170462010376488
Loss in iteration 89 : 0.5168885968800371
Loss in iteration 90 : 0.516735416160369
Loss in iteration 91 : 0.5165863314951799
Loss in iteration 92 : 0.5164423556122499
Loss in iteration 93 : 0.516302925075368
Loss in iteration 94 : 0.5161676742423515
Loss in iteration 95 : 0.5160355886394402
Loss in iteration 96 : 0.5159068691124976
Loss in iteration 97 : 0.5157814365612894
Loss in iteration 98 : 0.5156592343090146
Loss in iteration 99 : 0.5155401889705168
Loss in iteration 100 : 0.5154230509278064
Loss in iteration 101 : 0.5153085334907405
Loss in iteration 102 : 0.5151966750956893
Loss in iteration 103 : 0.515087221119839
Loss in iteration 104 : 0.5149797805083949
Loss in iteration 105 : 0.5148749951492826
Loss in iteration 106 : 0.5147725435339734
Loss in iteration 107 : 0.5146717737542181
Loss in iteration 108 : 0.5145731738231842
Loss in iteration 109 : 0.5144763070763113
Loss in iteration 110 : 0.5143810899057119
Loss in iteration 111 : 0.5142875618897956
Loss in iteration 112 : 0.5141955429872157
Loss in iteration 113 : 0.5141053698857949
Loss in iteration 114 : 0.5140177184402653
Loss in iteration 115 : 0.5139320179715273
Loss in iteration 116 : 0.5138485180450768
Loss in iteration 117 : 0.51376701082331
Loss in iteration 118 : 0.513686461577049
Loss in iteration 119 : 0.5136071660427096
Loss in iteration 120 : 0.5135303166944971
Loss in iteration 121 : 0.5134564420786286
Loss in iteration 122 : 0.5133839690442078
Loss in iteration 123 : 0.5133125698448628
Loss in iteration 124 : 0.5132418171982427
Loss in iteration 125 : 0.5131721263318944
Loss in iteration 126 : 0.5131035757951463
Loss in iteration 127 : 0.5130363413816427
Loss in iteration 128 : 0.5129696326084843
Loss in iteration 129 : 0.5129037187216495
Loss in iteration 130 : 0.5128382963857147
Loss in iteration 131 : 0.512773672246378
Loss in iteration 132 : 0.5127100612133876
Loss in iteration 133 : 0.5126473703116409
Loss in iteration 134 : 0.5125854661926451
Loss in iteration 135 : 0.5125247875075424
Loss in iteration 136 : 0.5124656803639522
Loss in iteration 137 : 0.5124078153171884
Loss in iteration 138 : 0.5123514806729829
Loss in iteration 139 : 0.5122961710615959
Loss in iteration 140 : 0.5122417290643178
Loss in iteration 141 : 0.5121877573057367
Loss in iteration 142 : 0.5121355225584364
Loss in iteration 143 : 0.5120843667661805
Loss in iteration 144 : 0.5120344377432134
Loss in iteration 145 : 0.5119849126599053
Loss in iteration 146 : 0.511936004438119
Loss in iteration 147 : 0.5118879054936581
Loss in iteration 148 : 0.5118405129287323
Loss in iteration 149 : 0.5117941965001247
Loss in iteration 150 : 0.5117486495060332
Loss in iteration 151 : 0.5117037451103731
Loss in iteration 152 : 0.5116601578530449
Loss in iteration 153 : 0.5116180422217785
Loss in iteration 154 : 0.5115766504944784
Loss in iteration 155 : 0.5115358627770699
Loss in iteration 156 : 0.5114965332797675
Loss in iteration 157 : 0.5114583321741848
Loss in iteration 158 : 0.5114209142746662
Loss in iteration 159 : 0.5113844404623376
Loss in iteration 160 : 0.5113485527327959
Loss in iteration 161 : 0.511313058041963
Loss in iteration 162 : 0.5112782093735883
Loss in iteration 163 : 0.5112436735700805
Loss in iteration 164 : 0.511210222500745
Loss in iteration 165 : 0.5111779921959467
Loss in iteration 166 : 0.5111468159087654
Loss in iteration 167 : 0.5111162456995652
Loss in iteration 168 : 0.5110861877272559
Loss in iteration 169 : 0.5110564696462474
Loss in iteration 170 : 0.5110277330255029
Loss in iteration 171 : 0.5109993143552238
Loss in iteration 172 : 0.5109713301654754
Loss in iteration 173 : 0.5109437129066432
Loss in iteration 174 : 0.5109167450869027
Loss in iteration 175 : 0.5108899577930199
Loss in iteration 176 : 0.5108634189551209
Loss in iteration 177 : 0.5108371009600502
Loss in iteration 178 : 0.5108110342987571
Loss in iteration 179 : 0.5107852116846732
Loss in iteration 180 : 0.5107599463547151
Loss in iteration 181 : 0.5107349103882699
Loss in iteration 182 : 0.5107101540891066
Loss in iteration 183 : 0.5106859547899282
Loss in iteration 184 : 0.5106622893703783
Loss in iteration 185 : 0.5106388624769199
Loss in iteration 186 : 0.5106158415483391
Loss in iteration 187 : 0.5105929034352408
Loss in iteration 188 : 0.5105701828953879
Loss in iteration 189 : 0.5105477189568393
Loss in iteration 190 : 0.5105255469678734
Loss in iteration 191 : 0.5105039186046713
Loss in iteration 192 : 0.5104828496046955
Loss in iteration 193 : 0.5104621524222175
Loss in iteration 194 : 0.5104414843204215
Loss in iteration 195 : 0.5104210909490903
Loss in iteration 196 : 0.5104009777825795
Loss in iteration 197 : 0.5103810764723467
Loss in iteration 198 : 0.51036145527831
Loss in iteration 199 : 0.5103419904793185
Loss in iteration 200 : 0.5103227624443519
Testing accuracy  of updater 2 on alg 1 with rate 0.09999999999999998 = 0.781, training accuracy 0.787, time elapsed: 5083 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 20.37732908901909
Loss in iteration 3 : 25.287366070202783
Loss in iteration 4 : 11.168884937204485
Loss in iteration 5 : 11.678490690016512
Loss in iteration 6 : 7.350552661719791
Loss in iteration 7 : 7.762225700903869
Loss in iteration 8 : 6.320132261866561
Loss in iteration 9 : 6.340921394894469
Loss in iteration 10 : 5.452029640948059
Loss in iteration 11 : 5.33377793124557
Loss in iteration 12 : 4.76707840336635
Loss in iteration 13 : 4.647737190252951
Loss in iteration 14 : 4.271200710520913
Loss in iteration 15 : 4.189880425630288
Loss in iteration 16 : 3.7778592355089033
Loss in iteration 17 : 3.817027300148862
Loss in iteration 18 : 3.392943980085815
Loss in iteration 19 : 3.4839866173252148
Loss in iteration 20 : 3.1075214095075236
Loss in iteration 21 : 3.2787374280365236
Loss in iteration 22 : 2.895576429419735
Loss in iteration 23 : 3.1189562316470667
Loss in iteration 24 : 2.796890261406153
Loss in iteration 25 : 3.0785688656054826
Loss in iteration 26 : 2.669194750980093
Loss in iteration 27 : 3.035072337425321
Loss in iteration 28 : 2.4888082117761963
Loss in iteration 29 : 2.8119127521032294
Loss in iteration 30 : 2.2900167939369322
Loss in iteration 31 : 2.540316316626215
Loss in iteration 32 : 2.2220486071364927
Loss in iteration 33 : 2.497201823067064
Loss in iteration 34 : 2.2695986512782658
Loss in iteration 35 : 2.5536891057538558
Loss in iteration 36 : 2.3513049939731228
Loss in iteration 37 : 2.6405132400325217
Loss in iteration 38 : 2.4774894825254723
Loss in iteration 39 : 2.7963784628825095
Loss in iteration 40 : 2.4950328096823533
Loss in iteration 41 : 2.747202176599744
Loss in iteration 42 : 2.4373392677924226
Loss in iteration 43 : 2.639600474093149
Loss in iteration 44 : 2.3423774344666803
Loss in iteration 45 : 2.492129325484746
Loss in iteration 46 : 2.298806510415162
Loss in iteration 47 : 2.428792368126381
Loss in iteration 48 : 2.231017113991659
Loss in iteration 49 : 2.344514610565474
Loss in iteration 50 : 2.1657853388544894
Loss in iteration 51 : 2.272322763005821
Loss in iteration 52 : 2.1320045079311756
Loss in iteration 53 : 2.248051530445118
Loss in iteration 54 : 2.105302061487765
Loss in iteration 55 : 2.2095529662394138
Loss in iteration 56 : 2.077564597660057
Loss in iteration 57 : 2.187791519455365
Loss in iteration 58 : 2.0451971519311467
Loss in iteration 59 : 2.152560100152242
Loss in iteration 60 : 2.015986034121986
Loss in iteration 61 : 2.1294860395648554
Loss in iteration 62 : 2.001217988901496
Loss in iteration 63 : 2.122708045273518
Loss in iteration 64 : 1.9805129497851175
Loss in iteration 65 : 2.1012634729281245
Loss in iteration 66 : 1.9593944482106942
Loss in iteration 67 : 2.07650174989306
Loss in iteration 68 : 1.951410183044036
Loss in iteration 69 : 2.0849147088420534
Loss in iteration 70 : 1.9288947350271766
Loss in iteration 71 : 2.0475498850841864
Loss in iteration 72 : 1.920598487016357
Loss in iteration 73 : 2.0595547646113226
Loss in iteration 74 : 1.8958175307185812
Loss in iteration 75 : 2.026288702879807
Loss in iteration 76 : 1.887623874793267
Loss in iteration 77 : 2.0143611110725943
Loss in iteration 78 : 1.878531161378219
Loss in iteration 79 : 1.9998813615697233
Loss in iteration 80 : 1.8684294088536175
Loss in iteration 81 : 1.9847595934732942
Loss in iteration 82 : 1.8507352547093971
Loss in iteration 83 : 1.9721781099377471
Loss in iteration 84 : 1.8342405315602979
Loss in iteration 85 : 1.9584170635280884
Loss in iteration 86 : 1.8207103826181028
Loss in iteration 87 : 1.93944240258522
Loss in iteration 88 : 1.8151300906435615
Loss in iteration 89 : 1.9303369904187786
Loss in iteration 90 : 1.7837225800389231
Loss in iteration 91 : 1.896043090206904
Loss in iteration 92 : 1.784512301877045
Loss in iteration 93 : 1.8964730881095495
Loss in iteration 94 : 1.7666394337148703
Loss in iteration 95 : 1.869718067870827
Loss in iteration 96 : 1.7665030086179467
Loss in iteration 97 : 1.875298295979564
Loss in iteration 98 : 1.7531213403392711
Loss in iteration 99 : 1.853766850983186
Loss in iteration 100 : 1.7369839289586655
Loss in iteration 101 : 1.8342307438431908
Loss in iteration 102 : 1.7257022652771454
Loss in iteration 103 : 1.8228782808871808
Loss in iteration 104 : 1.7105798189422234
Loss in iteration 105 : 1.8064713118694213
Loss in iteration 106 : 1.6992908988317303
Loss in iteration 107 : 1.7953393631475976
Loss in iteration 108 : 1.6854915924568572
Loss in iteration 109 : 1.7807423586972888
Loss in iteration 110 : 1.6815108551883557
Loss in iteration 111 : 1.7796695363713255
Loss in iteration 112 : 1.6705893859028853
Loss in iteration 113 : 1.7690288944149704
Loss in iteration 114 : 1.6577188524512525
Loss in iteration 115 : 1.753986267869138
Loss in iteration 116 : 1.6485113698021325
Loss in iteration 117 : 1.7463895711740947
Loss in iteration 118 : 1.6396747083228753
Loss in iteration 119 : 1.7366525753314423
Loss in iteration 120 : 1.6275367817959583
Loss in iteration 121 : 1.7346866141451542
Loss in iteration 122 : 1.6197558127865188
Loss in iteration 123 : 1.7181790718997605
Loss in iteration 124 : 1.6138485498604613
Loss in iteration 125 : 1.7148470642522098
Loss in iteration 126 : 1.6015453593533184
Loss in iteration 127 : 1.7079213280545196
Loss in iteration 128 : 1.5913303173583029
Loss in iteration 129 : 1.7046142293373878
Loss in iteration 130 : 1.5784724934952303
Loss in iteration 131 : 1.6841281946860922
Loss in iteration 132 : 1.5767307612210766
Loss in iteration 133 : 1.680174281193656
Loss in iteration 134 : 1.565265273852164
Loss in iteration 135 : 1.6667885464333776
Loss in iteration 136 : 1.5561943554028808
Loss in iteration 137 : 1.6648555994957766
Loss in iteration 138 : 1.5475422643163528
Loss in iteration 139 : 1.6572296395303245
Loss in iteration 140 : 1.5373487304668392
Loss in iteration 141 : 1.6467372699682865
Loss in iteration 142 : 1.5297351298906279
Loss in iteration 143 : 1.636027962782096
Loss in iteration 144 : 1.5221138581833242
Loss in iteration 145 : 1.6270071868847025
Loss in iteration 146 : 1.5127781949071262
Loss in iteration 147 : 1.6165133830899578
Loss in iteration 148 : 1.5033634360201582
Loss in iteration 149 : 1.6062087817646649
Loss in iteration 150 : 1.4932748020769817
Loss in iteration 151 : 1.5973769470806747
Loss in iteration 152 : 1.4860839959334582
Loss in iteration 153 : 1.590818331856428
Loss in iteration 154 : 1.4725471509133305
Loss in iteration 155 : 1.5815526316182822
Loss in iteration 156 : 1.4672470418529355
Loss in iteration 157 : 1.574727424793487
Loss in iteration 158 : 1.460570688601588
Loss in iteration 159 : 1.5626740118799487
Loss in iteration 160 : 1.457366699242572
Loss in iteration 161 : 1.5624727581919537
Loss in iteration 162 : 1.4490247281747837
Loss in iteration 163 : 1.5524718799275627
Loss in iteration 164 : 1.4542513087875126
Loss in iteration 165 : 1.561328279293349
Loss in iteration 166 : 1.4451792156714498
Loss in iteration 167 : 1.543850416873907
Loss in iteration 168 : 1.446494055527093
Loss in iteration 169 : 1.549375415194427
Loss in iteration 170 : 1.4383181953977044
Loss in iteration 171 : 1.5310353221870827
Loss in iteration 172 : 1.4329806090597847
Loss in iteration 173 : 1.526241352072293
Loss in iteration 174 : 1.4257532447545422
Loss in iteration 175 : 1.5218535959242543
Loss in iteration 176 : 1.4323957647438972
Loss in iteration 177 : 1.5256940181178966
Loss in iteration 178 : 1.4346679859415232
Loss in iteration 179 : 1.5283548502935853
Loss in iteration 180 : 1.4267351633318384
Loss in iteration 181 : 1.5181562978366798
Loss in iteration 182 : 1.4237953887765682
Loss in iteration 183 : 1.5138552878629539
Loss in iteration 184 : 1.4160636454552502
Loss in iteration 185 : 1.5069336406231855
Loss in iteration 186 : 1.3797982175358527
Loss in iteration 187 : 1.4819210306195396
Loss in iteration 188 : 1.363644632041733
Loss in iteration 189 : 1.5572141786007272
Loss in iteration 190 : 1.3886981555989348
Loss in iteration 191 : 1.7100707819971608
Loss in iteration 192 : 1.3923167082414634
Loss in iteration 193 : 1.4790467306071933
Loss in iteration 194 : 1.2565894607126387
Loss in iteration 195 : 1.2351014007354133
Loss in iteration 196 : 1.1168216871895214
Loss in iteration 197 : 1.1106368637260298
Loss in iteration 198 : 1.0382780064651884
Loss in iteration 199 : 1.0668448454071258
Loss in iteration 200 : 1.0424252678853523
Testing accuracy  of updater 3 on alg 1 with rate 10.0 = 0.753, training accuracy 0.7615, time elapsed: 4837 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9218248671950953
Loss in iteration 3 : 0.8998444202083441
Loss in iteration 4 : 0.880578701634463
Loss in iteration 5 : 0.8617557129342109
Loss in iteration 6 : 0.8436253579091465
Loss in iteration 7 : 0.8257573828795355
Loss in iteration 8 : 0.8080500150522816
Loss in iteration 9 : 0.7904743757655242
Loss in iteration 10 : 0.7731475741020419
Loss in iteration 11 : 0.756535697076176
Loss in iteration 12 : 0.7407185192056959
Loss in iteration 13 : 0.7255456210810589
Loss in iteration 14 : 0.7110314622506302
Loss in iteration 15 : 0.6972566947170036
Loss in iteration 16 : 0.684313114397712
Loss in iteration 17 : 0.6726683442404887
Loss in iteration 18 : 0.6624793557213225
Loss in iteration 19 : 0.6534940528543802
Loss in iteration 20 : 0.6455347601739144
Loss in iteration 21 : 0.638574521785023
Loss in iteration 22 : 0.6323839740226128
Loss in iteration 23 : 0.6269458074914713
Loss in iteration 24 : 0.6220506329157595
Loss in iteration 25 : 0.6175763597081197
Loss in iteration 26 : 0.6134827518298462
Loss in iteration 27 : 0.6097286562324666
Loss in iteration 28 : 0.606407391938491
Loss in iteration 29 : 0.6032839498880266
Loss in iteration 30 : 0.6005368632114769
Loss in iteration 31 : 0.5977611148121446
Loss in iteration 32 : 0.5952256641101388
Loss in iteration 33 : 0.5927350480469692
Loss in iteration 34 : 0.5905506021427523
Loss in iteration 35 : 0.5882782875836965
Loss in iteration 36 : 0.5861001167580675
Loss in iteration 37 : 0.5840469619375708
Loss in iteration 38 : 0.5820679204023652
Loss in iteration 39 : 0.5802527326933942
Loss in iteration 40 : 0.5785429245157956
Loss in iteration 41 : 0.5768951212645383
Loss in iteration 42 : 0.5753499482944157
Loss in iteration 43 : 0.5739151783433285
Loss in iteration 44 : 0.5724973815434712
Loss in iteration 45 : 0.5711816362073616
Loss in iteration 46 : 0.5698853638978403
Loss in iteration 47 : 0.5686523919736821
Loss in iteration 48 : 0.5674859033817606
Loss in iteration 49 : 0.5663539144864149
Loss in iteration 50 : 0.5652872686995454
Loss in iteration 51 : 0.5642597205689128
Loss in iteration 52 : 0.5632627127713015
Loss in iteration 53 : 0.5623057977688865
Loss in iteration 54 : 0.5613874855660118
Loss in iteration 55 : 0.5605039267321338
Loss in iteration 56 : 0.5596464476958509
Loss in iteration 57 : 0.5588133398799066
Loss in iteration 58 : 0.5579995952701444
Loss in iteration 59 : 0.5572086745540259
Loss in iteration 60 : 0.5564457083772852
Loss in iteration 61 : 0.5557163712775178
Loss in iteration 62 : 0.5550026939647251
Loss in iteration 63 : 0.5543106752649964
Loss in iteration 64 : 0.5536338708860296
Loss in iteration 65 : 0.5529858640280303
Loss in iteration 66 : 0.5523643555021234
Loss in iteration 67 : 0.5517632288441805
Loss in iteration 68 : 0.5511819388488226
Loss in iteration 69 : 0.550614720366182
Loss in iteration 70 : 0.5500599204505845
Loss in iteration 71 : 0.5495166316211646
Loss in iteration 72 : 0.5489770738273739
Loss in iteration 73 : 0.5484445702128434
Loss in iteration 74 : 0.5479236067198151
Loss in iteration 75 : 0.5474146944752363
Loss in iteration 76 : 0.5469159587472386
Loss in iteration 77 : 0.5464299194067185
Loss in iteration 78 : 0.5459596951970304
Loss in iteration 79 : 0.5454971971910718
Loss in iteration 80 : 0.5450449731740789
Loss in iteration 81 : 0.5446005496952548
Loss in iteration 82 : 0.5441692061318179
Loss in iteration 83 : 0.5437377507532539
Loss in iteration 84 : 0.5433219831503279
Loss in iteration 85 : 0.5429144602645797
Loss in iteration 86 : 0.5425211647499916
Loss in iteration 87 : 0.5421357242489312
Loss in iteration 88 : 0.5417811257791163
Loss in iteration 89 : 0.5414321900159621
Loss in iteration 90 : 0.5411153757461081
Loss in iteration 91 : 0.5407436055995907
Loss in iteration 92 : 0.5404299659480601
Loss in iteration 93 : 0.540084927654679
Loss in iteration 94 : 0.5397351630410772
Loss in iteration 95 : 0.5394035459461883
Loss in iteration 96 : 0.5390662647794078
Loss in iteration 97 : 0.5387963832168223
Loss in iteration 98 : 0.53843750053525
Loss in iteration 99 : 0.5381425547437764
Loss in iteration 100 : 0.5378313667075841
Loss in iteration 101 : 0.5375824451629821
Loss in iteration 102 : 0.5372682153131384
Loss in iteration 103 : 0.5370009944978653
Loss in iteration 104 : 0.5366867322642813
Loss in iteration 105 : 0.5364047829791221
Loss in iteration 106 : 0.5361003265842506
Loss in iteration 107 : 0.5358430249536309
Loss in iteration 108 : 0.535549087372355
Loss in iteration 109 : 0.5353036338246354
Loss in iteration 110 : 0.5350181863396272
Loss in iteration 111 : 0.5348014105540773
Loss in iteration 112 : 0.5345134591754517
Loss in iteration 113 : 0.5342632567660636
Loss in iteration 114 : 0.5340188494093849
Loss in iteration 115 : 0.533812335813555
Loss in iteration 116 : 0.5335303132620838
Loss in iteration 117 : 0.5333411000359283
Loss in iteration 118 : 0.5331000821497074
Loss in iteration 119 : 0.532901864341034
Loss in iteration 120 : 0.5326625156484048
Loss in iteration 121 : 0.5324743629403659
Loss in iteration 122 : 0.532228119265009
Loss in iteration 123 : 0.5320708608916611
Loss in iteration 124 : 0.531821210250695
Loss in iteration 125 : 0.5316263560205851
Loss in iteration 126 : 0.5314175743313863
Loss in iteration 127 : 0.5312122601357783
Loss in iteration 128 : 0.5310171199863138
Loss in iteration 129 : 0.5308241723562477
Loss in iteration 130 : 0.530635379279121
Loss in iteration 131 : 0.5304539052734308
Loss in iteration 132 : 0.530276820886328
Loss in iteration 133 : 0.5300986723531805
Loss in iteration 134 : 0.5299218669301303
Loss in iteration 135 : 0.529746260729341
Loss in iteration 136 : 0.5295749615923888
Loss in iteration 137 : 0.5294087302622743
Loss in iteration 138 : 0.5292433536923137
Loss in iteration 139 : 0.5290824293974897
Loss in iteration 140 : 0.5289122662724562
Loss in iteration 141 : 0.5287467186704894
Loss in iteration 142 : 0.5285807362537492
Loss in iteration 143 : 0.5284193428852657
Loss in iteration 144 : 0.5282618781286357
Loss in iteration 145 : 0.528102281813549
Loss in iteration 146 : 0.5279539817688711
Loss in iteration 147 : 0.5278004666262636
Loss in iteration 148 : 0.5276409053563618
Loss in iteration 149 : 0.5274864294572793
Loss in iteration 150 : 0.5273370994051625
Loss in iteration 151 : 0.5271959305437747
Loss in iteration 152 : 0.5270475015772267
Loss in iteration 153 : 0.5269083647194496
Loss in iteration 154 : 0.5267717299456073
Loss in iteration 155 : 0.5266364150044397
Loss in iteration 156 : 0.5264953566267699
Loss in iteration 157 : 0.5263581606719636
Loss in iteration 158 : 0.5262298184711126
Loss in iteration 159 : 0.5260962715328608
Loss in iteration 160 : 0.5259765350576767
Loss in iteration 161 : 0.5258416851832508
Loss in iteration 162 : 0.525715077422334
Loss in iteration 163 : 0.5255721544113645
Loss in iteration 164 : 0.5254465657051934
Loss in iteration 165 : 0.5253158308496809
Loss in iteration 166 : 0.5251921988073864
Loss in iteration 167 : 0.5250684920327077
Loss in iteration 168 : 0.524945622173898
Loss in iteration 169 : 0.5248247135645219
Loss in iteration 170 : 0.5247062003993318
Loss in iteration 171 : 0.5245879939244407
Loss in iteration 172 : 0.5244725511395427
Loss in iteration 173 : 0.5243560355224424
Loss in iteration 174 : 0.5242392944870488
Loss in iteration 175 : 0.5241271266329296
Loss in iteration 176 : 0.5240075534336097
Loss in iteration 177 : 0.5238946287611086
Loss in iteration 178 : 0.5237799519252732
Loss in iteration 179 : 0.523667460924752
Loss in iteration 180 : 0.5235576653483826
Loss in iteration 181 : 0.5234491275576829
Loss in iteration 182 : 0.5233417269348687
Loss in iteration 183 : 0.5232322564140734
Loss in iteration 184 : 0.5231257422074443
Loss in iteration 185 : 0.5230211728606088
Loss in iteration 186 : 0.522917610108663
Loss in iteration 187 : 0.5228151245964293
Loss in iteration 188 : 0.5227109888248933
Loss in iteration 189 : 0.5226082246068694
Loss in iteration 190 : 0.5225066449287
Loss in iteration 191 : 0.522406458058478
Loss in iteration 192 : 0.5223059291071025
Loss in iteration 193 : 0.5222085897781223
Loss in iteration 194 : 0.5221128271054392
Loss in iteration 195 : 0.52201576224176
Loss in iteration 196 : 0.5219200333189213
Loss in iteration 197 : 0.5218267224163708
Loss in iteration 198 : 0.5217311017070987
Loss in iteration 199 : 0.5216394504126665
Loss in iteration 200 : 0.5215441073590298
Testing accuracy  of updater 3 on alg 1 with rate 1.0 = 0.7765, training accuracy 0.783, time elapsed: 4401 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9911196871832347
Loss in iteration 3 : 0.9822730055802156
Loss in iteration 4 : 0.9734784121951932
Loss in iteration 5 : 0.964867092865391
Loss in iteration 6 : 0.9566342099293881
Loss in iteration 7 : 0.9490952834503464
Loss in iteration 8 : 0.9422804036027714
Loss in iteration 9 : 0.9362751133454514
Loss in iteration 10 : 0.9309994941592751
Loss in iteration 11 : 0.9263805872103483
Loss in iteration 12 : 0.9224283119061992
Loss in iteration 13 : 0.9191089776824771
Loss in iteration 14 : 0.9162460295208189
Loss in iteration 15 : 0.9136761077601248
Loss in iteration 16 : 0.9113283771546948
Loss in iteration 17 : 0.9091371267379786
Loss in iteration 18 : 0.9070707820522637
Loss in iteration 19 : 0.905102237225209
Loss in iteration 20 : 0.9031860044896138
Loss in iteration 21 : 0.9013013413711777
Loss in iteration 22 : 0.8994404787801806
Loss in iteration 23 : 0.8975919418514763
Loss in iteration 24 : 0.895757807251737
Loss in iteration 25 : 0.8939543216817069
Loss in iteration 26 : 0.8921673457632521
Loss in iteration 27 : 0.8903829996548145
Loss in iteration 28 : 0.8886020865736259
Loss in iteration 29 : 0.8868239081313355
Loss in iteration 30 : 0.885047781235211
Loss in iteration 31 : 0.8832779968226891
Loss in iteration 32 : 0.8815129873710975
Loss in iteration 33 : 0.8797502006528862
Loss in iteration 34 : 0.8779898967381323
Loss in iteration 35 : 0.8762324617418551
Loss in iteration 36 : 0.8744785145654458
Loss in iteration 37 : 0.8727276637085702
Loss in iteration 38 : 0.8709795461912035
Loss in iteration 39 : 0.8692352583955694
Loss in iteration 40 : 0.8674963944913122
Loss in iteration 41 : 0.8657607030086841
Loss in iteration 42 : 0.8640289280457908
Loss in iteration 43 : 0.8623010935984885
Loss in iteration 44 : 0.8605789724502979
Loss in iteration 45 : 0.858859562591434
Loss in iteration 46 : 0.8571433984090446
Loss in iteration 47 : 0.8554292288238744
Loss in iteration 48 : 0.8537166581363083
Loss in iteration 49 : 0.8520055610273591
Loss in iteration 50 : 0.8502970038004812
Loss in iteration 51 : 0.8485911666196325
Loss in iteration 52 : 0.8468874386687981
Loss in iteration 53 : 0.8451863123779894
Loss in iteration 54 : 0.843488137648311
Loss in iteration 55 : 0.8417947185278176
Loss in iteration 56 : 0.8401047886471724
Loss in iteration 57 : 0.8384170109941554
Loss in iteration 58 : 0.8367308806505135
Loss in iteration 59 : 0.8350462602571662
Loss in iteration 60 : 0.833363927546784
Loss in iteration 61 : 0.8316842743656531
Loss in iteration 62 : 0.8300072738650559
Loss in iteration 63 : 0.8283329514288055
Loss in iteration 64 : 0.8266602946981106
Loss in iteration 65 : 0.824988990833697
Loss in iteration 66 : 0.8233201277298426
Loss in iteration 67 : 0.8216545479769549
Loss in iteration 68 : 0.8199900642070123
Loss in iteration 69 : 0.8183277841241069
Loss in iteration 70 : 0.816667421731517
Loss in iteration 71 : 0.8150100405038028
Loss in iteration 72 : 0.8133565795055452
Loss in iteration 73 : 0.811704506678598
Loss in iteration 74 : 0.8100542118295702
Loss in iteration 75 : 0.8084065972204783
Loss in iteration 76 : 0.8067617748789068
Loss in iteration 77 : 0.8051180129910649
Loss in iteration 78 : 0.8034756895499418
Loss in iteration 79 : 0.8018348490476511
Loss in iteration 80 : 0.8001955883737908
Loss in iteration 81 : 0.7985581805694121
Loss in iteration 82 : 0.7969237243092362
Loss in iteration 83 : 0.7952924959697848
Loss in iteration 84 : 0.7936622722608411
Loss in iteration 85 : 0.7920335627410267
Loss in iteration 86 : 0.7904091422013103
Loss in iteration 87 : 0.7887858397944955
Loss in iteration 88 : 0.7871683183398204
Loss in iteration 89 : 0.785558657911325
Loss in iteration 90 : 0.7839544250565585
Loss in iteration 91 : 0.7823561273845032
Loss in iteration 92 : 0.7807640009636082
Loss in iteration 93 : 0.7791758536727672
Loss in iteration 94 : 0.7775951344796768
Loss in iteration 95 : 0.77601781919415
Loss in iteration 96 : 0.7744451113334192
Loss in iteration 97 : 0.7728775580834777
Loss in iteration 98 : 0.7713145639212087
Loss in iteration 99 : 0.7697605468318447
Loss in iteration 100 : 0.7682167068845727
Loss in iteration 101 : 0.7666828865228956
Loss in iteration 102 : 0.7651644591333838
Loss in iteration 103 : 0.7636509175482951
Loss in iteration 104 : 0.7621433889221497
Loss in iteration 105 : 0.7606490024744478
Loss in iteration 106 : 0.759158968846019
Loss in iteration 107 : 0.7576772014092924
Loss in iteration 108 : 0.7561993044558704
Loss in iteration 109 : 0.7547249140945401
Loss in iteration 110 : 0.7532579495825257
Loss in iteration 111 : 0.7517997798515264
Loss in iteration 112 : 0.7503504000929738
Loss in iteration 113 : 0.748905206863976
Loss in iteration 114 : 0.7474639067665494
Loss in iteration 115 : 0.7460288387422809
Loss in iteration 116 : 0.7446035364648209
Loss in iteration 117 : 0.7431877704580644
Loss in iteration 118 : 0.7417759873392152
Loss in iteration 119 : 0.7403692969455369
Loss in iteration 120 : 0.7389715108257056
Loss in iteration 121 : 0.7375772339437088
Loss in iteration 122 : 0.7361870279966971
Loss in iteration 123 : 0.7348013049541184
Loss in iteration 124 : 0.7334233402806024
Loss in iteration 125 : 0.7320507969056753
Loss in iteration 126 : 0.7306838884670476
Loss in iteration 127 : 0.7293253099583437
Loss in iteration 128 : 0.7279702363540476
Loss in iteration 129 : 0.7266208659593311
Loss in iteration 130 : 0.7252782439089791
Loss in iteration 131 : 0.7239438307575186
Loss in iteration 132 : 0.7226154275966478
Loss in iteration 133 : 0.7212932818931275
Loss in iteration 134 : 0.7199785069054322
Loss in iteration 135 : 0.7186682769902963
Loss in iteration 136 : 0.7173637973300035
Loss in iteration 137 : 0.7160641803629851
Loss in iteration 138 : 0.714771468711217
Loss in iteration 139 : 0.7134883112440574
Loss in iteration 140 : 0.7122116382731938
Loss in iteration 141 : 0.7109411091645065
Loss in iteration 142 : 0.7096776828111946
Loss in iteration 143 : 0.7084198219679458
Loss in iteration 144 : 0.707167109243948
Loss in iteration 145 : 0.7059202189812182
Loss in iteration 146 : 0.7046781174816975
Loss in iteration 147 : 0.7034374708076626
Loss in iteration 148 : 0.702201145061138
Loss in iteration 149 : 0.7009717256682226
Loss in iteration 150 : 0.6997497075828137
Loss in iteration 151 : 0.6985328110840638
Loss in iteration 152 : 0.6973281100438957
Loss in iteration 153 : 0.696137995591818
Loss in iteration 154 : 0.6949575861649798
Loss in iteration 155 : 0.6937933561383226
Loss in iteration 156 : 0.6926408688457699
Loss in iteration 157 : 0.6914980677288349
Loss in iteration 158 : 0.6903622051947227
Loss in iteration 159 : 0.6892365177890836
Loss in iteration 160 : 0.6881135486282923
Loss in iteration 161 : 0.6869980220717967
Loss in iteration 162 : 0.6858871756227386
Loss in iteration 163 : 0.6847864481764534
Loss in iteration 164 : 0.6836991055676716
Loss in iteration 165 : 0.682625900099488
Loss in iteration 166 : 0.6815702054758452
Loss in iteration 167 : 0.6805272929797145
Loss in iteration 168 : 0.6794995592195844
Loss in iteration 169 : 0.6784823037805304
Loss in iteration 170 : 0.677477036869292
Loss in iteration 171 : 0.676479995545571
Loss in iteration 172 : 0.675489075796895
Loss in iteration 173 : 0.6745065997147378
Loss in iteration 174 : 0.6735346110301187
Loss in iteration 175 : 0.6725855154130431
Loss in iteration 176 : 0.6716512116057601
Loss in iteration 177 : 0.6707259397825958
Loss in iteration 178 : 0.6698156168266239
Loss in iteration 179 : 0.6689141922779575
Loss in iteration 180 : 0.6680182541764103
Loss in iteration 181 : 0.6671306495070064
Loss in iteration 182 : 0.6662509707557247
Loss in iteration 183 : 0.6653837913293139
Loss in iteration 184 : 0.6645255512965808
Loss in iteration 185 : 0.6636736623339772
Loss in iteration 186 : 0.6628283127362409
Loss in iteration 187 : 0.6619936895554935
Loss in iteration 188 : 0.6611681663005886
Loss in iteration 189 : 0.6603479036987223
Loss in iteration 190 : 0.6595356814458544
Loss in iteration 191 : 0.6587337589035779
Loss in iteration 192 : 0.6579457749118288
Loss in iteration 193 : 0.657164439824402
Loss in iteration 194 : 0.6563899257095509
Loss in iteration 195 : 0.6556207438075933
Loss in iteration 196 : 0.654861586257201
Loss in iteration 197 : 0.6541104881015
Loss in iteration 198 : 0.6533675371088431
Loss in iteration 199 : 0.6526323674022187
Loss in iteration 200 : 0.6519064322861394
Testing accuracy  of updater 3 on alg 1 with rate 0.09999999999999998 = 0.744, training accuracy 0.74525, time elapsed: 4398 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302137
Loss in iteration 3 : 0.9990911666950768
Loss in iteration 4 : 0.9986229266098003
Loss in iteration 5 : 0.9981484796078843
Loss in iteration 6 : 0.9976688821745493
Loss in iteration 7 : 0.9971848282516708
Loss in iteration 8 : 0.996696806493488
Loss in iteration 9 : 0.9962051782244059
Loss in iteration 10 : 0.9957102204541372
Loss in iteration 11 : 0.9952121515419889
Loss in iteration 12 : 0.9947111474515832
Loss in iteration 13 : 0.9942073525436069
Loss in iteration 14 : 0.993700887021666
Loss in iteration 15 : 0.9931918522344857
Loss in iteration 16 : 0.99268033455364
Loss in iteration 17 : 0.9921664082746888
Loss in iteration 18 : 0.9916501378306335
Loss in iteration 19 : 0.9911315795097306
Loss in iteration 20 : 0.9906107828087408
Loss in iteration 21 : 0.9900877915131128
Loss in iteration 22 : 0.9895626445693705
Loss in iteration 23 : 0.989035376797049
Loss in iteration 24 : 0.9885060194751382
Loss in iteration 25 : 0.9879746008291856
Loss in iteration 26 : 0.987441146438981
Loss in iteration 27 : 0.9869056795820369
Loss in iteration 28 : 0.9863682215248007
Loss in iteration 29 : 0.9858287917708691
Loss in iteration 30 : 0.9852874082736669
Loss in iteration 31 : 0.9847440876194421
Loss in iteration 32 : 0.9841988451854279
Loss in iteration 33 : 0.9836516952770068
Loss in iteration 34 : 0.9831026512470905
Loss in iteration 35 : 0.9825517256003204
Loss in iteration 36 : 0.9819989300843119
Loss in iteration 37 : 0.9814442757697015
Loss in iteration 38 : 0.9808877731205827
Loss in iteration 39 : 0.980329432056597
Loss in iteration 40 : 0.9797692620077609
Loss in iteration 41 : 0.979207271962993
Loss in iteration 42 : 0.9786434705131218
Loss in iteration 43 : 0.9780778658890503
Loss in iteration 44 : 0.9775104659956854
Loss in iteration 45 : 0.9769412784421365
Loss in iteration 46 : 0.9763703105686172
Loss in iteration 47 : 0.97579756947046
Loss in iteration 48 : 0.9752230620195441
Loss in iteration 49 : 0.9746467948834651
Loss in iteration 50 : 0.9740687745426835
Loss in iteration 51 : 0.9734890073058868
Loss in iteration 52 : 0.9729074993237569
Loss in iteration 53 : 0.9723242566013284
Loss in iteration 54 : 0.9717392850090887
Loss in iteration 55 : 0.9711529670183002
Loss in iteration 56 : 0.9705693932264201
Loss in iteration 57 : 0.9699853256383412
Loss in iteration 58 : 0.9694030412738988
Loss in iteration 59 : 0.9688190784049796
Loss in iteration 60 : 0.9682334401416767
Loss in iteration 61 : 0.9676461297304226
Loss in iteration 62 : 0.967057150537562
Loss in iteration 63 : 0.9664665060343796
Loss in iteration 64 : 0.9658741997835194
Loss in iteration 65 : 0.965280235426698
Loss in iteration 66 : 0.9646846166735777
Loss in iteration 67 : 0.9640873472917261
Loss in iteration 68 : 0.9634884723407168
Loss in iteration 69 : 0.9628921941325861
Loss in iteration 70 : 0.9622977454567893
Loss in iteration 71 : 0.9617016800571655
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874628
Loss in iteration 74 : 0.9599051388851518
Loss in iteration 75 : 0.9593062298501533
Loss in iteration 76 : 0.9587057257699186
Loss in iteration 77 : 0.9581036287985416
Loss in iteration 78 : 0.9574999412695979
Loss in iteration 79 : 0.956894665676639
Loss in iteration 80 : 0.9562878046553546
Loss in iteration 81 : 0.9556816523135638
Loss in iteration 82 : 0.9550814459110005
Loss in iteration 83 : 0.9544815984559714
Loss in iteration 84 : 0.9538846209944875
Loss in iteration 85 : 0.9532872686338298
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201398
Loss in iteration 88 : 0.951503252197559
Loss in iteration 89 : 0.9509194294980792
Loss in iteration 90 : 0.9503342683949609
Loss in iteration 91 : 0.9497511738393183
Loss in iteration 92 : 0.9491708501216721
Loss in iteration 93 : 0.9485982816500576
Loss in iteration 94 : 0.9480263044402895
Loss in iteration 95 : 0.9474547293733718
Loss in iteration 96 : 0.9468880307025385
Loss in iteration 97 : 0.9463243837048178
Loss in iteration 98 : 0.9457618628085926
Loss in iteration 99 : 0.9452048428904497
Loss in iteration 100 : 0.9446515461397307
Loss in iteration 101 : 0.9441012671116744
Loss in iteration 102 : 0.9435518629128592
Loss in iteration 103 : 0.9430016508564804
Loss in iteration 104 : 0.9424557183301124
Loss in iteration 105 : 0.9419166132179784
Loss in iteration 106 : 0.9413841271762252
Loss in iteration 107 : 0.9408530340533816
Loss in iteration 108 : 0.9403236524482214
Loss in iteration 109 : 0.9397929218514739
Loss in iteration 110 : 0.9392608655180985
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118603
Loss in iteration 113 : 0.9376766644368997
Loss in iteration 114 : 0.9371508081019355
Loss in iteration 115 : 0.9366254633121422
Loss in iteration 116 : 0.9361046197954084
Loss in iteration 117 : 0.9355860091709182
Loss in iteration 118 : 0.9350716258591197
Loss in iteration 119 : 0.9345612315882873
Loss in iteration 120 : 0.9340512504682469
Loss in iteration 121 : 0.933544690443962
Loss in iteration 122 : 0.9330455555866902
Loss in iteration 123 : 0.9325469560645513
Loss in iteration 124 : 0.9320503331151222
Loss in iteration 125 : 0.9315586023873229
Loss in iteration 126 : 0.9310695652406196
Loss in iteration 127 : 0.9305809233902171
Loss in iteration 128 : 0.9300954437442299
Loss in iteration 129 : 0.9296147801287639
Loss in iteration 130 : 0.9291363398323597
Loss in iteration 131 : 0.9286582107499006
Loss in iteration 132 : 0.9281839212992987
Loss in iteration 133 : 0.9277110478830762
Loss in iteration 134 : 0.9272409858205105
Loss in iteration 135 : 0.9267724677135242
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082241
Loss in iteration 138 : 0.9253838076792636
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739487
Loss in iteration 141 : 0.9240078888647766
Loss in iteration 142 : 0.923551438453082
Loss in iteration 143 : 0.923094604525712
Loss in iteration 144 : 0.9226378387174606
Loss in iteration 145 : 0.9221806569055581
Loss in iteration 146 : 0.9217241933501482
Loss in iteration 147 : 0.9212687202551162
Loss in iteration 148 : 0.9208131844890631
Loss in iteration 149 : 0.9203572340558853
Loss in iteration 150 : 0.9199031582037827
Loss in iteration 151 : 0.9194513495399594
Loss in iteration 152 : 0.9189995040212217
Loss in iteration 153 : 0.918547972626229
Loss in iteration 154 : 0.9180971823926409
Loss in iteration 155 : 0.9176477919076033
Loss in iteration 156 : 0.9171988772718451
Loss in iteration 157 : 0.9167508097553855
Loss in iteration 158 : 0.9163036803089568
Loss in iteration 159 : 0.9158567756420073
Loss in iteration 160 : 0.9154118977009528
Loss in iteration 161 : 0.9149669701718807
Loss in iteration 162 : 0.9145250016896176
Loss in iteration 163 : 0.9140835362539844
Loss in iteration 164 : 0.9136431976261
Loss in iteration 165 : 0.9132036437818055
Loss in iteration 166 : 0.9127638919433495
Loss in iteration 167 : 0.9123240985898368
Loss in iteration 168 : 0.9118836296821626
Loss in iteration 169 : 0.9114434971424152
Loss in iteration 170 : 0.911004198204372
Loss in iteration 171 : 0.9105655186486219
Loss in iteration 172 : 0.9101275270446175
Loss in iteration 173 : 0.9096900524816632
Loss in iteration 174 : 0.9092525522651342
Loss in iteration 175 : 0.9088155499936301
Loss in iteration 176 : 0.9083794631841514
Loss in iteration 177 : 0.9079423132434211
Loss in iteration 178 : 0.9075043152484322
Loss in iteration 179 : 0.9070663591650343
Loss in iteration 180 : 0.9066278643844525
Loss in iteration 181 : 0.9061902002084017
Loss in iteration 182 : 0.9057526168230802
Loss in iteration 183 : 0.9053142227477843
Loss in iteration 184 : 0.904875231484499
Loss in iteration 185 : 0.9044351889099915
Loss in iteration 186 : 0.9039944390416046
Loss in iteration 187 : 0.9035536750934489
Loss in iteration 188 : 0.9031125484537357
Loss in iteration 189 : 0.9026718497227764
Loss in iteration 190 : 0.9022315665777413
Loss in iteration 191 : 0.9017921622328392
Loss in iteration 192 : 0.9013538286391136
Loss in iteration 193 : 0.9009151507017394
Loss in iteration 194 : 0.9004763855730438
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.899596610724603
Loss in iteration 197 : 0.8991560274165034
Loss in iteration 198 : 0.8987144392382084
Loss in iteration 199 : 0.898272719296338
Loss in iteration 200 : 0.8978322139798338
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.595, training accuracy 0.6015, time elapsed: 4498 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302137
Loss in iteration 3 : 0.9990911666950768
Loss in iteration 4 : 0.9986229266098005
Loss in iteration 5 : 0.9981484796078844
Loss in iteration 6 : 0.9976688821745494
Loss in iteration 7 : 0.9971848282516708
Loss in iteration 8 : 0.9966968064934881
Loss in iteration 9 : 0.9962051782244059
Loss in iteration 10 : 0.9957102204541372
Loss in iteration 11 : 0.9952121515419889
Loss in iteration 12 : 0.9947111474515833
Loss in iteration 13 : 0.9942073525436071
Loss in iteration 14 : 0.993700887021666
Loss in iteration 15 : 0.9931918522344856
Loss in iteration 16 : 0.99268033455364
Loss in iteration 17 : 0.9921664082746888
Loss in iteration 18 : 0.9916501378306335
Loss in iteration 19 : 0.9911315795097306
Loss in iteration 20 : 0.990610782808741
Loss in iteration 21 : 0.9900877915131129
Loss in iteration 22 : 0.9895626445693706
Loss in iteration 23 : 0.989035376797049
Loss in iteration 24 : 0.9885060194751382
Loss in iteration 25 : 0.9879746008291856
Loss in iteration 26 : 0.987441146438981
Loss in iteration 27 : 0.9869056795820369
Loss in iteration 28 : 0.9863682215248009
Loss in iteration 29 : 0.9858287917708691
Loss in iteration 30 : 0.9852874082736669
Loss in iteration 31 : 0.9847440876194421
Loss in iteration 32 : 0.9841988451854278
Loss in iteration 33 : 0.9836516952770068
Loss in iteration 34 : 0.9831026512470905
Loss in iteration 35 : 0.9825517256003203
Loss in iteration 36 : 0.981998930084312
Loss in iteration 37 : 0.9814442757697015
Loss in iteration 38 : 0.9808877731205827
Loss in iteration 39 : 0.9803294320565971
Loss in iteration 40 : 0.9797692620077608
Loss in iteration 41 : 0.9792072719629928
Loss in iteration 42 : 0.9786434705131218
Loss in iteration 43 : 0.9780778658890503
Loss in iteration 44 : 0.9775104659956854
Loss in iteration 45 : 0.9769412784421365
Loss in iteration 46 : 0.9763703105686173
Loss in iteration 47 : 0.97579756947046
Loss in iteration 48 : 0.9752230620195441
Loss in iteration 49 : 0.974646794883465
Loss in iteration 50 : 0.9740687745426835
Loss in iteration 51 : 0.9734890073058868
Loss in iteration 52 : 0.9729074993237569
Loss in iteration 53 : 0.9723242566013285
Loss in iteration 54 : 0.9717392850090887
Loss in iteration 55 : 0.9711529670183002
Loss in iteration 56 : 0.9705693932264202
Loss in iteration 57 : 0.9699853256383412
Loss in iteration 58 : 0.9694030412738988
Loss in iteration 59 : 0.9688190784049796
Loss in iteration 60 : 0.9682334401416767
Loss in iteration 61 : 0.9676461297304229
Loss in iteration 62 : 0.9670571505375621
Loss in iteration 63 : 0.9664665060343796
Loss in iteration 64 : 0.9658741997835194
Loss in iteration 65 : 0.965280235426698
Loss in iteration 66 : 0.9646846166735779
Loss in iteration 67 : 0.9640873472917261
Loss in iteration 68 : 0.9634884723407168
Loss in iteration 69 : 0.9628921941325861
Loss in iteration 70 : 0.9622977454567893
Loss in iteration 71 : 0.9617016800571656
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874628
Loss in iteration 74 : 0.9599051388851518
Loss in iteration 75 : 0.9593062298501533
Loss in iteration 76 : 0.9587057257699186
Loss in iteration 77 : 0.9581036287985416
Loss in iteration 78 : 0.9574999412695979
Loss in iteration 79 : 0.9568946656766391
Loss in iteration 80 : 0.9562878046553546
Loss in iteration 81 : 0.9556816523135638
Loss in iteration 82 : 0.9550814459110004
Loss in iteration 83 : 0.9544815984559714
Loss in iteration 84 : 0.9538846209944875
Loss in iteration 85 : 0.95328726863383
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201397
Loss in iteration 88 : 0.951503252197559
Loss in iteration 89 : 0.9509194294980793
Loss in iteration 90 : 0.9503342683949608
Loss in iteration 91 : 0.9497511738393182
Loss in iteration 92 : 0.9491708501216721
Loss in iteration 93 : 0.9485982816500576
Loss in iteration 94 : 0.9480263044402895
Loss in iteration 95 : 0.9474547293733718
Loss in iteration 96 : 0.9468880307025385
Loss in iteration 97 : 0.9463243837048178
Loss in iteration 98 : 0.9457618628085928
Loss in iteration 99 : 0.9452048428904497
Loss in iteration 100 : 0.9446515461397307
Loss in iteration 101 : 0.9441012671116742
Loss in iteration 102 : 0.9435518629128592
Loss in iteration 103 : 0.9430016508564804
Loss in iteration 104 : 0.9424557183301124
Loss in iteration 105 : 0.9419166132179783
Loss in iteration 106 : 0.9413841271762252
Loss in iteration 107 : 0.9408530340533816
Loss in iteration 108 : 0.9403236524482215
Loss in iteration 109 : 0.9397929218514739
Loss in iteration 110 : 0.9392608655180985
Loss in iteration 111 : 0.9387300940724488
Loss in iteration 112 : 0.9382011791118603
Loss in iteration 113 : 0.9376766644368997
Loss in iteration 114 : 0.9371508081019355
Loss in iteration 115 : 0.9366254633121422
Loss in iteration 116 : 0.9361046197954084
Loss in iteration 117 : 0.935586009170918
Loss in iteration 118 : 0.9350716258591197
Loss in iteration 119 : 0.9345612315882874
Loss in iteration 120 : 0.934051250468247
Loss in iteration 121 : 0.933544690443962
Loss in iteration 122 : 0.9330455555866902
Loss in iteration 123 : 0.9325469560645513
Loss in iteration 124 : 0.9320503331151223
Loss in iteration 125 : 0.9315586023873229
Loss in iteration 126 : 0.9310695652406198
Loss in iteration 127 : 0.9305809233902171
Loss in iteration 128 : 0.9300954437442298
Loss in iteration 129 : 0.9296147801287639
Loss in iteration 130 : 0.9291363398323597
Loss in iteration 131 : 0.9286582107499006
Loss in iteration 132 : 0.9281839212992987
Loss in iteration 133 : 0.9277110478830762
Loss in iteration 134 : 0.9272409858205105
Loss in iteration 135 : 0.9267724677135242
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082239
Loss in iteration 138 : 0.9253838076792636
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739486
Loss in iteration 141 : 0.9240078888647766
Loss in iteration 142 : 0.923551438453082
Loss in iteration 143 : 0.9230946045257119
Loss in iteration 144 : 0.9226378387174606
Loss in iteration 145 : 0.9221806569055581
Loss in iteration 146 : 0.9217241933501483
Loss in iteration 147 : 0.921268720255116
Loss in iteration 148 : 0.920813184489063
Loss in iteration 149 : 0.9203572340558853
Loss in iteration 150 : 0.9199031582037829
Loss in iteration 151 : 0.9194513495399594
Loss in iteration 152 : 0.9189995040212217
Loss in iteration 153 : 0.918547972626229
Loss in iteration 154 : 0.9180971823926409
Loss in iteration 155 : 0.9176477919076034
Loss in iteration 156 : 0.9171988772718452
Loss in iteration 157 : 0.9167508097553855
Loss in iteration 158 : 0.9163036803089568
Loss in iteration 159 : 0.9158567756420072
Loss in iteration 160 : 0.9154118977009529
Loss in iteration 161 : 0.9149669701718807
Loss in iteration 162 : 0.9145250016896177
Loss in iteration 163 : 0.9140835362539844
Loss in iteration 164 : 0.9136431976261
Loss in iteration 165 : 0.9132036437818054
Loss in iteration 166 : 0.9127638919433493
Loss in iteration 167 : 0.9123240985898369
Loss in iteration 168 : 0.9118836296821627
Loss in iteration 169 : 0.9114434971424152
Loss in iteration 170 : 0.911004198204372
Loss in iteration 171 : 0.9105655186486218
Loss in iteration 172 : 0.9101275270446175
Loss in iteration 173 : 0.9096900524816632
Loss in iteration 174 : 0.9092525522651344
Loss in iteration 175 : 0.9088155499936301
Loss in iteration 176 : 0.9083794631841515
Loss in iteration 177 : 0.9079423132434212
Loss in iteration 178 : 0.9075043152484323
Loss in iteration 179 : 0.9070663591650343
Loss in iteration 180 : 0.9066278643844525
Loss in iteration 181 : 0.9061902002084017
Loss in iteration 182 : 0.9057526168230802
Loss in iteration 183 : 0.9053142227477843
Loss in iteration 184 : 0.904875231484499
Loss in iteration 185 : 0.9044351889099916
Loss in iteration 186 : 0.9039944390416046
Loss in iteration 187 : 0.9035536750934489
Loss in iteration 188 : 0.9031125484537357
Loss in iteration 189 : 0.9026718497227765
Loss in iteration 190 : 0.9022315665777413
Loss in iteration 191 : 0.9017921622328392
Loss in iteration 192 : 0.9013538286391136
Loss in iteration 193 : 0.9009151507017394
Loss in iteration 194 : 0.9004763855730438
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.899596610724603
Loss in iteration 197 : 0.8991560274165034
Loss in iteration 198 : 0.8987144392382084
Loss in iteration 199 : 0.898272719296338
Loss in iteration 200 : 0.8978322139798338
Testing accuracy  of updater 4 on alg 1 with rate 1.0 = 0.595, training accuracy 0.6015, time elapsed: 4898 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9995514097302138
Loss in iteration 3 : 0.9990911666950768
Loss in iteration 4 : 0.9986229266098003
Loss in iteration 5 : 0.9981484796078843
Loss in iteration 6 : 0.9976688821745494
Loss in iteration 7 : 0.9971848282516707
Loss in iteration 8 : 0.9966968064934881
Loss in iteration 9 : 0.9962051782244059
Loss in iteration 10 : 0.9957102204541372
Loss in iteration 11 : 0.9952121515419889
Loss in iteration 12 : 0.9947111474515832
Loss in iteration 13 : 0.9942073525436071
Loss in iteration 14 : 0.993700887021666
Loss in iteration 15 : 0.9931918522344856
Loss in iteration 16 : 0.99268033455364
Loss in iteration 17 : 0.9921664082746886
Loss in iteration 18 : 0.9916501378306335
Loss in iteration 19 : 0.9911315795097305
Loss in iteration 20 : 0.9906107828087409
Loss in iteration 21 : 0.9900877915131128
Loss in iteration 22 : 0.9895626445693705
Loss in iteration 23 : 0.989035376797049
Loss in iteration 24 : 0.9885060194751382
Loss in iteration 25 : 0.9879746008291856
Loss in iteration 26 : 0.987441146438981
Loss in iteration 27 : 0.9869056795820369
Loss in iteration 28 : 0.9863682215248009
Loss in iteration 29 : 0.9858287917708692
Loss in iteration 30 : 0.9852874082736669
Loss in iteration 31 : 0.9847440876194421
Loss in iteration 32 : 0.9841988451854278
Loss in iteration 33 : 0.9836516952770068
Loss in iteration 34 : 0.9831026512470904
Loss in iteration 35 : 0.9825517256003204
Loss in iteration 36 : 0.981998930084312
Loss in iteration 37 : 0.9814442757697015
Loss in iteration 38 : 0.9808877731205827
Loss in iteration 39 : 0.9803294320565971
Loss in iteration 40 : 0.9797692620077608
Loss in iteration 41 : 0.979207271962993
Loss in iteration 42 : 0.9786434705131218
Loss in iteration 43 : 0.9780778658890503
Loss in iteration 44 : 0.9775104659956854
Loss in iteration 45 : 0.9769412784421364
Loss in iteration 46 : 0.9763703105686173
Loss in iteration 47 : 0.97579756947046
Loss in iteration 48 : 0.9752230620195441
Loss in iteration 49 : 0.974646794883465
Loss in iteration 50 : 0.9740687745426835
Loss in iteration 51 : 0.9734890073058868
Loss in iteration 52 : 0.972907499323757
Loss in iteration 53 : 0.9723242566013285
Loss in iteration 54 : 0.9717392850090887
Loss in iteration 55 : 0.9711529670183003
Loss in iteration 56 : 0.9705693932264201
Loss in iteration 57 : 0.9699853256383412
Loss in iteration 58 : 0.9694030412738988
Loss in iteration 59 : 0.9688190784049796
Loss in iteration 60 : 0.9682334401416767
Loss in iteration 61 : 0.9676461297304229
Loss in iteration 62 : 0.9670571505375621
Loss in iteration 63 : 0.9664665060343794
Loss in iteration 64 : 0.9658741997835194
Loss in iteration 65 : 0.965280235426698
Loss in iteration 66 : 0.9646846166735779
Loss in iteration 67 : 0.9640873472917261
Loss in iteration 68 : 0.9634884723407168
Loss in iteration 69 : 0.9628921941325861
Loss in iteration 70 : 0.9622977454567893
Loss in iteration 71 : 0.9617016800571656
Loss in iteration 72 : 0.9611040001626104
Loss in iteration 73 : 0.9605047081874628
Loss in iteration 74 : 0.9599051388851518
Loss in iteration 75 : 0.9593062298501533
Loss in iteration 76 : 0.9587057257699186
Loss in iteration 77 : 0.9581036287985416
Loss in iteration 78 : 0.9574999412695979
Loss in iteration 79 : 0.9568946656766391
Loss in iteration 80 : 0.9562878046553546
Loss in iteration 81 : 0.955681652313564
Loss in iteration 82 : 0.9550814459110004
Loss in iteration 83 : 0.9544815984559714
Loss in iteration 84 : 0.9538846209944875
Loss in iteration 85 : 0.9532872686338298
Loss in iteration 86 : 0.9526914020736182
Loss in iteration 87 : 0.9520959175201398
Loss in iteration 88 : 0.951503252197559
Loss in iteration 89 : 0.9509194294980793
Loss in iteration 90 : 0.9503342683949609
Loss in iteration 91 : 0.9497511738393183
Loss in iteration 92 : 0.9491708501216721
Loss in iteration 93 : 0.9485982816500576
Loss in iteration 94 : 0.9480263044402895
Loss in iteration 95 : 0.9474547293733718
Loss in iteration 96 : 0.9468880307025384
Loss in iteration 97 : 0.9463243837048178
Loss in iteration 98 : 0.9457618628085928
Loss in iteration 99 : 0.9452048428904497
Loss in iteration 100 : 0.9446515461397307
Loss in iteration 101 : 0.9441012671116744
Loss in iteration 102 : 0.9435518629128592
Loss in iteration 103 : 0.9430016508564802
Loss in iteration 104 : 0.9424557183301124
Loss in iteration 105 : 0.9419166132179783
Loss in iteration 106 : 0.9413841271762252
Loss in iteration 107 : 0.9408530340533816
Loss in iteration 108 : 0.9403236524482214
Loss in iteration 109 : 0.9397929218514739
Loss in iteration 110 : 0.9392608655180985
Loss in iteration 111 : 0.9387300940724487
Loss in iteration 112 : 0.9382011791118603
Loss in iteration 113 : 0.9376766644368997
Loss in iteration 114 : 0.9371508081019355
Loss in iteration 115 : 0.9366254633121422
Loss in iteration 116 : 0.9361046197954084
Loss in iteration 117 : 0.935586009170918
Loss in iteration 118 : 0.9350716258591197
Loss in iteration 119 : 0.9345612315882874
Loss in iteration 120 : 0.934051250468247
Loss in iteration 121 : 0.933544690443962
Loss in iteration 122 : 0.9330455555866902
Loss in iteration 123 : 0.9325469560645513
Loss in iteration 124 : 0.9320503331151223
Loss in iteration 125 : 0.9315586023873228
Loss in iteration 126 : 0.9310695652406197
Loss in iteration 127 : 0.930580923390217
Loss in iteration 128 : 0.9300954437442299
Loss in iteration 129 : 0.9296147801287639
Loss in iteration 130 : 0.9291363398323597
Loss in iteration 131 : 0.9286582107499007
Loss in iteration 132 : 0.9281839212992987
Loss in iteration 133 : 0.9277110478830762
Loss in iteration 134 : 0.9272409858205105
Loss in iteration 135 : 0.9267724677135241
Loss in iteration 136 : 0.9263072804824651
Loss in iteration 137 : 0.9258456656082239
Loss in iteration 138 : 0.9253838076792636
Loss in iteration 139 : 0.9249236713963239
Loss in iteration 140 : 0.9244657199739487
Loss in iteration 141 : 0.9240078888647766
Loss in iteration 142 : 0.923551438453082
Loss in iteration 143 : 0.9230946045257119
Loss in iteration 144 : 0.9226378387174606
Loss in iteration 145 : 0.9221806569055581
Loss in iteration 146 : 0.9217241933501483
Loss in iteration 147 : 0.921268720255116
Loss in iteration 148 : 0.9208131844890631
Loss in iteration 149 : 0.9203572340558853
Loss in iteration 150 : 0.9199031582037827
Loss in iteration 151 : 0.9194513495399594
Loss in iteration 152 : 0.9189995040212217
Loss in iteration 153 : 0.918547972626229
Loss in iteration 154 : 0.918097182392641
Loss in iteration 155 : 0.9176477919076034
Loss in iteration 156 : 0.9171988772718452
Loss in iteration 157 : 0.9167508097553855
Loss in iteration 158 : 0.9163036803089568
Loss in iteration 159 : 0.9158567756420073
Loss in iteration 160 : 0.9154118977009528
Loss in iteration 161 : 0.9149669701718807
Loss in iteration 162 : 0.9145250016896177
Loss in iteration 163 : 0.9140835362539844
Loss in iteration 164 : 0.9136431976261
Loss in iteration 165 : 0.9132036437818055
Loss in iteration 166 : 0.9127638919433493
Loss in iteration 167 : 0.9123240985898369
Loss in iteration 168 : 0.9118836296821627
Loss in iteration 169 : 0.9114434971424153
Loss in iteration 170 : 0.911004198204372
Loss in iteration 171 : 0.9105655186486219
Loss in iteration 172 : 0.9101275270446175
Loss in iteration 173 : 0.9096900524816632
Loss in iteration 174 : 0.9092525522651342
Loss in iteration 175 : 0.9088155499936301
Loss in iteration 176 : 0.9083794631841515
Loss in iteration 177 : 0.9079423132434212
Loss in iteration 178 : 0.9075043152484322
Loss in iteration 179 : 0.9070663591650343
Loss in iteration 180 : 0.9066278643844525
Loss in iteration 181 : 0.9061902002084018
Loss in iteration 182 : 0.9057526168230802
Loss in iteration 183 : 0.9053142227477843
Loss in iteration 184 : 0.904875231484499
Loss in iteration 185 : 0.9044351889099915
Loss in iteration 186 : 0.9039944390416047
Loss in iteration 187 : 0.9035536750934489
Loss in iteration 188 : 0.9031125484537357
Loss in iteration 189 : 0.9026718497227765
Loss in iteration 190 : 0.9022315665777413
Loss in iteration 191 : 0.9017921622328392
Loss in iteration 192 : 0.9013538286391136
Loss in iteration 193 : 0.9009151507017394
Loss in iteration 194 : 0.9004763855730438
Loss in iteration 195 : 0.9000369437447143
Loss in iteration 196 : 0.899596610724603
Loss in iteration 197 : 0.8991560274165034
Loss in iteration 198 : 0.8987144392382084
Loss in iteration 199 : 0.898272719296338
Loss in iteration 200 : 0.8978322139798338
Testing accuracy  of updater 4 on alg 1 with rate 0.09999999999999998 = 0.595, training accuracy 0.6015, time elapsed: 4517 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 64.30045502706706
Loss in iteration 3 : 79.06506149471691
Loss in iteration 4 : 36.67757990024234
Loss in iteration 5 : 39.820089461413865
Loss in iteration 6 : 26.684949464211012
Loss in iteration 7 : 28.555887026623935
Loss in iteration 8 : 24.45032375982332
Loss in iteration 9 : 23.72324619846518
Loss in iteration 10 : 22.152359768722956
Loss in iteration 11 : 22.53548984143395
Loss in iteration 12 : 21.142556613844754
Loss in iteration 13 : 20.600424611749332
Loss in iteration 14 : 18.84239311630524
Loss in iteration 15 : 19.513767950194218
Loss in iteration 16 : 17.500305458308926
Loss in iteration 17 : 18.790475971246636
Loss in iteration 18 : 16.77987435471399
Loss in iteration 19 : 18.553853758522134
Loss in iteration 20 : 16.313879741050577
Loss in iteration 21 : 18.526475530791824
Loss in iteration 22 : 15.379859533887524
Loss in iteration 23 : 18.199576244343184
Loss in iteration 24 : 14.54951460784752
Loss in iteration 25 : 18.170106252601176
Loss in iteration 26 : 15.814090654505318
Loss in iteration 27 : 18.87339322567295
Loss in iteration 28 : 16.354635670332122
Loss in iteration 29 : 18.362835100783656
Loss in iteration 30 : 16.158626182935052
Loss in iteration 31 : 17.94143329930405
Loss in iteration 32 : 15.345260349497087
Loss in iteration 33 : 17.51144766371865
Loss in iteration 34 : 15.302173219873904
Loss in iteration 35 : 17.644363633830356
Loss in iteration 36 : 14.907976148503888
Loss in iteration 37 : 17.466702084712878
Loss in iteration 38 : 14.71203174372012
Loss in iteration 39 : 18.068587715476557
Loss in iteration 40 : 13.464446397460964
Loss in iteration 41 : 17.19099367522816
Loss in iteration 42 : 12.88711385301533
Loss in iteration 43 : 17.56737818210328
Loss in iteration 44 : 15.479936796952307
Loss in iteration 45 : 19.111431010839887
Loss in iteration 46 : 16.179226398299647
Loss in iteration 47 : 18.55869676865205
Loss in iteration 48 : 15.436026013858992
Loss in iteration 49 : 17.63791832416247
Loss in iteration 50 : 14.966666275635253
Loss in iteration 51 : 17.411198493650897
Loss in iteration 52 : 14.569666591596139
Loss in iteration 53 : 17.351365036949318
Loss in iteration 54 : 14.09932819270673
Loss in iteration 55 : 17.465071589823808
Loss in iteration 56 : 13.854938443704977
Loss in iteration 57 : 17.3795846764851
Loss in iteration 58 : 12.89893207510941
Loss in iteration 59 : 16.976063539426637
Loss in iteration 60 : 14.124642790638449
Loss in iteration 61 : 18.28210637516053
Loss in iteration 62 : 15.623251281350944
Loss in iteration 63 : 18.710183294237275
Loss in iteration 64 : 15.387185178520904
Loss in iteration 65 : 18.096598846373343
Loss in iteration 66 : 14.610789629865469
Loss in iteration 67 : 17.40727867194828
Loss in iteration 68 : 14.492027951862479
Loss in iteration 69 : 17.2285293520481
Loss in iteration 70 : 13.96637280826546
Loss in iteration 71 : 17.479464520255267
Loss in iteration 72 : 13.503937492134346
Loss in iteration 73 : 17.194725346823912
Loss in iteration 74 : 12.56485895514934
Loss in iteration 75 : 16.839828047123586
Loss in iteration 76 : 14.414734461386441
Loss in iteration 77 : 18.50296570835016
Loss in iteration 78 : 15.62064468161356
Loss in iteration 79 : 18.506238125724234
Loss in iteration 80 : 15.296020868787009
Loss in iteration 81 : 17.724959738661806
Loss in iteration 82 : 14.634578587434447
Loss in iteration 83 : 17.076430245151897
Loss in iteration 84 : 14.149466267825426
Loss in iteration 85 : 16.975317688540642
Loss in iteration 86 : 14.089660295385606
Loss in iteration 87 : 17.569760118238047
Loss in iteration 88 : 12.921374142516797
Loss in iteration 89 : 16.647446022296382
Loss in iteration 90 : 13.362434397923655
Loss in iteration 91 : 17.58022950326872
Loss in iteration 92 : 14.844754394019231
Loss in iteration 93 : 18.306301945731306
Loss in iteration 94 : 15.290220164926083
Loss in iteration 95 : 17.997840436256297
Loss in iteration 96 : 14.703917622532584
Loss in iteration 97 : 17.21416856372829
Loss in iteration 98 : 14.426259799471584
Loss in iteration 99 : 17.094449455601417
Loss in iteration 100 : 13.990451217297535
Loss in iteration 101 : 17.203439307518362
Loss in iteration 102 : 13.523810633109695
Loss in iteration 103 : 17.009850088881635
Loss in iteration 104 : 13.17802017369083
Loss in iteration 105 : 17.155767181478797
Loss in iteration 106 : 14.474665580048425
Loss in iteration 107 : 17.915634190754545
Loss in iteration 108 : 15.073737892212403
Loss in iteration 109 : 17.83254448510496
Loss in iteration 110 : 14.913449861032165
Loss in iteration 111 : 17.419812023204436
Loss in iteration 112 : 14.391614036463888
Loss in iteration 113 : 17.096430225397764
Loss in iteration 114 : 14.011098191217188
Loss in iteration 115 : 17.18185566552939
Loss in iteration 116 : 13.62497455358929
Loss in iteration 117 : 16.96309850015724
Loss in iteration 118 : 13.006982713404163
Loss in iteration 119 : 16.735907805804818
Loss in iteration 120 : 14.03557074445994
Loss in iteration 121 : 17.809357784366775
Loss in iteration 122 : 15.162772984631811
Loss in iteration 123 : 17.92987856154464
Loss in iteration 124 : 15.110336041438037
Loss in iteration 125 : 17.63306103714039
Loss in iteration 126 : 14.402494698881425
Loss in iteration 127 : 16.86547164989261
Loss in iteration 128 : 14.298775457089608
Loss in iteration 129 : 16.983757735201117
Loss in iteration 130 : 13.912985681199931
Loss in iteration 131 : 17.236742896239868
Loss in iteration 132 : 12.99468825060346
Loss in iteration 133 : 16.550069997547894
Loss in iteration 134 : 13.069083776474567
Loss in iteration 135 : 17.086321835166917
Loss in iteration 136 : 14.835048676094493
Loss in iteration 137 : 18.067472382607857
Loss in iteration 138 : 15.34888657451173
Loss in iteration 139 : 17.854659563145447
Loss in iteration 140 : 14.705527921845977
Loss in iteration 141 : 17.06575055461964
Loss in iteration 142 : 14.410061069335116
Loss in iteration 143 : 16.75470277411763
Loss in iteration 144 : 14.037912229776884
Loss in iteration 145 : 17.001374254851573
Loss in iteration 146 : 13.613390561582957
Loss in iteration 147 : 16.944695428163634
Loss in iteration 148 : 12.820145130925754
Loss in iteration 149 : 16.476378397211054
Loss in iteration 150 : 13.766976087229867
Loss in iteration 151 : 17.57075833994407
Loss in iteration 152 : 15.145823355285895
Loss in iteration 153 : 17.92117778546953
Loss in iteration 154 : 15.17417490396744
Loss in iteration 155 : 17.434292109457544
Loss in iteration 156 : 14.439723219633283
Loss in iteration 157 : 16.80616467440121
Loss in iteration 158 : 14.261909606302243
Loss in iteration 159 : 16.801813040292732
Loss in iteration 160 : 13.899617837676606
Loss in iteration 161 : 17.086872009302017
Loss in iteration 162 : 12.826097174875102
Loss in iteration 163 : 16.291355739852648
Loss in iteration 164 : 13.15129847542487
Loss in iteration 165 : 17.05803650914007
Loss in iteration 166 : 14.879974265971834
Loss in iteration 167 : 18.048700030966387
Loss in iteration 168 : 15.439550895964533
Loss in iteration 169 : 17.69744429829731
Loss in iteration 170 : 14.71451085771267
Loss in iteration 171 : 16.839023067137813
Loss in iteration 172 : 14.299679713583814
Loss in iteration 173 : 16.581701012685826
Loss in iteration 174 : 14.00308392536018
Loss in iteration 175 : 16.828085706713086
Loss in iteration 176 : 13.598115686384999
Loss in iteration 177 : 16.844162697894024
Loss in iteration 178 : 12.88157964206548
Loss in iteration 179 : 16.48855121613328
Loss in iteration 180 : 13.93928038271374
Loss in iteration 181 : 17.430409800367137
Loss in iteration 182 : 15.094030949867896
Loss in iteration 183 : 17.727905969414234
Loss in iteration 184 : 14.960006094154874
Loss in iteration 185 : 17.196478240026412
Loss in iteration 186 : 14.531421081570505
Loss in iteration 187 : 16.67760358751542
Loss in iteration 188 : 14.259684254551276
Loss in iteration 189 : 16.71745894883029
Loss in iteration 190 : 13.79344606098392
Loss in iteration 191 : 16.789669425172733
Loss in iteration 192 : 13.427530566743904
Loss in iteration 193 : 16.662286724051278
Loss in iteration 194 : 12.613032711696262
Loss in iteration 195 : 16.25531855519128
Loss in iteration 196 : 14.768365139208619
Loss in iteration 197 : 18.01460484336095
Loss in iteration 198 : 15.390029384091248
Loss in iteration 199 : 17.591848047715022
Loss in iteration 200 : 14.80268798425696
Testing accuracy  of updater 5 on alg 1 with rate 10.0 = 0.722, training accuracy 0.711375, time elapsed: 4043 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.283203151366729
Loss in iteration 3 : 7.5253620583090015
Loss in iteration 4 : 2.747708041699455
Loss in iteration 5 : 4.895420395778746
Loss in iteration 6 : 3.208309477232811
Loss in iteration 7 : 3.561528324465906
Loss in iteration 8 : 3.3176265997894108
Loss in iteration 9 : 2.7973803494842833
Loss in iteration 10 : 3.541454697573074
Loss in iteration 11 : 2.0507558523406626
Loss in iteration 12 : 3.4908806146477183
Loss in iteration 13 : 1.8138492753877742
Loss in iteration 14 : 3.223868669800636
Loss in iteration 15 : 1.8702491653935513
Loss in iteration 16 : 2.9038120111661927
Loss in iteration 17 : 2.0134026648662227
Loss in iteration 18 : 2.677128609700931
Loss in iteration 19 : 1.9518906433049004
Loss in iteration 20 : 1.8786876451541932
Loss in iteration 21 : 2.2099371715649294
Loss in iteration 22 : 2.4014137876627086
Loss in iteration 23 : 2.3746165822988194
Loss in iteration 24 : 2.4662527815019004
Loss in iteration 25 : 2.1041662149305362
Loss in iteration 26 : 2.259131352155886
Loss in iteration 27 : 2.118740071299484
Loss in iteration 28 : 2.117461033055138
Loss in iteration 29 : 2.098587599881073
Loss in iteration 30 : 2.017030471993221
Loss in iteration 31 : 2.0764278230643396
Loss in iteration 32 : 1.9401090005949846
Loss in iteration 33 : 2.0894246215810077
Loss in iteration 34 : 1.8852594066957733
Loss in iteration 35 : 2.0775433778128716
Loss in iteration 36 : 1.8442466834384441
Loss in iteration 37 : 2.0890596659299625
Loss in iteration 38 : 1.7927156628157637
Loss in iteration 39 : 2.1028797237144743
Loss in iteration 40 : 1.70974829612779
Loss in iteration 41 : 2.060674892065681
Loss in iteration 42 : 1.6778808082715266
Loss in iteration 43 : 2.1420994261813817
Loss in iteration 44 : 1.8387262362261063
Loss in iteration 45 : 2.1631762693611187
Loss in iteration 46 : 1.8655458452463498
Loss in iteration 47 : 2.131170940247411
Loss in iteration 48 : 1.7917731854012293
Loss in iteration 49 : 2.092692984775341
Loss in iteration 50 : 1.8011207225426527
Loss in iteration 51 : 2.0904406067496235
Loss in iteration 52 : 1.7699020868271145
Loss in iteration 53 : 2.066675804125636
Loss in iteration 54 : 1.7377557410681346
Loss in iteration 55 : 2.0667232277000673
Loss in iteration 56 : 1.7291418530280422
Loss in iteration 57 : 2.089009735789907
Loss in iteration 58 : 1.717777370199703
Loss in iteration 59 : 2.1200118399525643
Loss in iteration 60 : 1.5633215156347993
Loss in iteration 61 : 2.0300830849447227
Loss in iteration 62 : 1.6162636393003529
Loss in iteration 63 : 2.146172930704011
Loss in iteration 64 : 1.8514137976434286
Loss in iteration 65 : 2.1736082224586015
Loss in iteration 66 : 1.7897954266294096
Loss in iteration 67 : 2.126609845541574
Loss in iteration 68 : 1.7415012221156803
Loss in iteration 69 : 2.0783164974833297
Loss in iteration 70 : 1.7174256095670166
Loss in iteration 71 : 2.0583121357370664
Loss in iteration 72 : 1.716323323238274
Loss in iteration 73 : 2.066505821304277
Loss in iteration 74 : 1.707038912646312
Loss in iteration 75 : 2.0642520577295036
Loss in iteration 76 : 1.7003779434350013
Loss in iteration 77 : 2.086315197600896
Loss in iteration 78 : 1.654399474581468
Loss in iteration 79 : 2.1241129881495313
Loss in iteration 80 : 1.485806981913082
Loss in iteration 81 : 1.9685731472850863
Loss in iteration 82 : 1.5964037734347878
Loss in iteration 83 : 2.1318174394546965
Loss in iteration 84 : 1.8417318415450665
Loss in iteration 85 : 2.222031935758504
Loss in iteration 86 : 1.7963242995991293
Loss in iteration 87 : 2.12400930677569
Loss in iteration 88 : 1.7478321186907368
Loss in iteration 89 : 2.081085511939382
Loss in iteration 90 : 1.716077060363991
Loss in iteration 91 : 2.0517877272525316
Loss in iteration 92 : 1.7035570168997052
Loss in iteration 93 : 2.0348242348087973
Loss in iteration 94 : 1.700825010988021
Loss in iteration 95 : 2.04282674991801
Loss in iteration 96 : 1.6718952777182325
Loss in iteration 97 : 2.035362909341033
Loss in iteration 98 : 1.6512770748271917
Loss in iteration 99 : 2.0460270121432695
Loss in iteration 100 : 1.6812050482760836
Loss in iteration 101 : 2.055671204985333
Loss in iteration 102 : 1.532788212768242
Loss in iteration 103 : 1.9812307526209647
Loss in iteration 104 : 1.7404371758050392
Loss in iteration 105 : 2.1652003965864197
Loss in iteration 106 : 1.788559090090698
Loss in iteration 107 : 2.1226452538886904
Loss in iteration 108 : 1.7341805374150763
Loss in iteration 109 : 2.049437796480971
Loss in iteration 110 : 1.6885419581896386
Loss in iteration 111 : 2.021422732466286
Loss in iteration 112 : 1.6672755905242191
Loss in iteration 113 : 2.038448616533838
Loss in iteration 114 : 1.6508395771760311
Loss in iteration 115 : 2.0255212758772365
Loss in iteration 116 : 1.6803887030339992
Loss in iteration 117 : 2.0996458594584815
Loss in iteration 118 : 1.534766355796566
Loss in iteration 119 : 1.9323508107951168
Loss in iteration 120 : 1.55177448328993
Loss in iteration 121 : 2.0504250949279266
Loss in iteration 122 : 1.8159543730881813
Loss in iteration 123 : 2.206377108338924
Loss in iteration 124 : 1.7975993486157211
Loss in iteration 125 : 2.1165976541012843
Loss in iteration 126 : 1.7303515107743859
Loss in iteration 127 : 2.0331247095710956
Loss in iteration 128 : 1.7078576814196849
Loss in iteration 129 : 2.0004520950378755
Loss in iteration 130 : 1.6825805536763099
Loss in iteration 131 : 2.006683436411489
Loss in iteration 132 : 1.673640507259813
Loss in iteration 133 : 1.9949419079234028
Loss in iteration 134 : 1.6661787385446594
Loss in iteration 135 : 2.0177889239795626
Loss in iteration 136 : 1.6761631044773972
Loss in iteration 137 : 2.0518718897305486
Loss in iteration 138 : 1.6640519378056147
Loss in iteration 139 : 1.987700686675305
Loss in iteration 140 : 1.5661776005839496
Loss in iteration 141 : 1.9407551582960068
Loss in iteration 142 : 1.7254068957864583
Loss in iteration 143 : 2.1322504457243325
Loss in iteration 144 : 1.7893420291288813
Loss in iteration 145 : 2.1080122948703437
Loss in iteration 146 : 1.7379908505085946
Loss in iteration 147 : 2.0254145688283467
Loss in iteration 148 : 1.7003558239814986
Loss in iteration 149 : 1.9999221590287883
Loss in iteration 150 : 1.7002890453499016
Loss in iteration 151 : 2.00162961577359
Loss in iteration 152 : 1.6622988764911597
Loss in iteration 153 : 1.9808766469589392
Loss in iteration 154 : 1.6626138945683087
Loss in iteration 155 : 2.0158090372177035
Loss in iteration 156 : 1.6425206550926659
Loss in iteration 157 : 2.0361813972151035
Loss in iteration 158 : 1.5783057496793704
Loss in iteration 159 : 1.9524286948951686
Loss in iteration 160 : 1.6396229728622194
Loss in iteration 161 : 2.067721869608786
Loss in iteration 162 : 1.793592515503842
Loss in iteration 163 : 2.120753905950495
Loss in iteration 164 : 1.7866640330013732
Loss in iteration 165 : 2.0614334277871267
Loss in iteration 166 : 1.712836754998311
Loss in iteration 167 : 1.9804074177468587
Loss in iteration 168 : 1.7224090606395694
Loss in iteration 169 : 1.9936169844222364
Loss in iteration 170 : 1.6673993277436996
Loss in iteration 171 : 1.9562931608243728
Loss in iteration 172 : 1.6876029138594688
Loss in iteration 173 : 1.9915822555102263
Loss in iteration 174 : 1.6878442577396178
Loss in iteration 175 : 2.0116284578600463
Loss in iteration 176 : 1.624633484695857
Loss in iteration 177 : 1.9627197164382613
Loss in iteration 178 : 1.6159185650415195
Loss in iteration 179 : 1.9754966153738247
Loss in iteration 180 : 1.7305684291714993
Loss in iteration 181 : 2.0775785109266836
Loss in iteration 182 : 1.817233999441324
Loss in iteration 183 : 2.0647935898075276
Loss in iteration 184 : 1.732881475076023
Loss in iteration 185 : 1.9769084546489748
Loss in iteration 186 : 1.7358772404941154
Loss in iteration 187 : 1.974905395387194
Loss in iteration 188 : 1.6617788722867446
Loss in iteration 189 : 1.9617089294107086
Loss in iteration 190 : 1.6807238775714004
Loss in iteration 191 : 1.9809690571106167
Loss in iteration 192 : 1.649213437533926
Loss in iteration 193 : 2.0038104142801356
Loss in iteration 194 : 1.6058421845006137
Loss in iteration 195 : 1.9980514610878264
Loss in iteration 196 : 1.564016068750103
Loss in iteration 197 : 1.9597206374929372
Loss in iteration 198 : 1.7578483015509248
Loss in iteration 199 : 2.123430762470402
Loss in iteration 200 : 1.8003049857997002
Testing accuracy  of updater 5 on alg 1 with rate 1.0 = 0.725, training accuracy 0.71325, time elapsed: 4616 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9146701402467067
Loss in iteration 3 : 0.8900497745224877
Loss in iteration 4 : 0.8656480866080262
Loss in iteration 5 : 0.8406919126770399
Loss in iteration 6 : 0.81521510023073
Loss in iteration 7 : 0.7889510602020035
Loss in iteration 8 : 0.7628444004960927
Loss in iteration 9 : 0.7375228104105065
Loss in iteration 10 : 0.7132525567925123
Loss in iteration 11 : 0.6913866582595317
Loss in iteration 12 : 0.6774707048768598
Loss in iteration 13 : 0.6961254892883141
Loss in iteration 14 : 0.7094523655810406
Loss in iteration 15 : 0.8831118772092806
Loss in iteration 16 : 0.7564966108325689
Loss in iteration 17 : 0.8319058107046953
Loss in iteration 18 : 0.6628172507722343
Loss in iteration 19 : 0.7001826807412803
Loss in iteration 20 : 0.6477319123948668
Loss in iteration 21 : 0.6738025612129627
Loss in iteration 22 : 0.6478481153032697
Loss in iteration 23 : 0.6823891284357132
Loss in iteration 24 : 0.6452241814446903
Loss in iteration 25 : 0.6800364994067226
Loss in iteration 26 : 0.642050923437557
Loss in iteration 27 : 0.672428718564357
Loss in iteration 28 : 0.6340788968242557
Loss in iteration 29 : 0.6592518363746889
Loss in iteration 30 : 0.6303148550356078
Loss in iteration 31 : 0.6516147771484688
Loss in iteration 32 : 0.6249363293526377
Loss in iteration 33 : 0.6412273506964754
Loss in iteration 34 : 0.6190155930864019
Loss in iteration 35 : 0.6363957977617988
Loss in iteration 36 : 0.6162125101330443
Loss in iteration 37 : 0.6311570746294688
Loss in iteration 38 : 0.6168851324695136
Loss in iteration 39 : 0.6333730993908667
Loss in iteration 40 : 0.6184761000528433
Loss in iteration 41 : 0.635311386422752
Loss in iteration 42 : 0.6154797805081519
Loss in iteration 43 : 0.6282969627461336
Loss in iteration 44 : 0.6101193217949041
Loss in iteration 45 : 0.622732808760144
Loss in iteration 46 : 0.6065012342962149
Loss in iteration 47 : 0.6196364933649817
Loss in iteration 48 : 0.6035403039098257
Loss in iteration 49 : 0.6156529284864893
Loss in iteration 50 : 0.6046684583071793
Loss in iteration 51 : 0.6226902459944624
Loss in iteration 52 : 0.6007151777829131
Loss in iteration 53 : 0.6122297895317098
Loss in iteration 54 : 0.5979276365522123
Loss in iteration 55 : 0.6103101808198239
Loss in iteration 56 : 0.6001217268850516
Loss in iteration 57 : 0.6111208429418351
Loss in iteration 58 : 0.600003210684907
Loss in iteration 59 : 0.6088933513133941
Loss in iteration 60 : 0.5977353276086408
Loss in iteration 61 : 0.6056601136262223
Loss in iteration 62 : 0.5966959674470289
Loss in iteration 63 : 0.6044139470010318
Loss in iteration 64 : 0.5947907174823314
Loss in iteration 65 : 0.6044578364306646
Loss in iteration 66 : 0.5912831116110815
Loss in iteration 67 : 0.5993948909705213
Loss in iteration 68 : 0.5877993837479476
Loss in iteration 69 : 0.5984718905923895
Loss in iteration 70 : 0.5881381252114235
Loss in iteration 71 : 0.5971317508492566
Loss in iteration 72 : 0.5873929428632313
Loss in iteration 73 : 0.5974395624691967
Loss in iteration 74 : 0.5872394569322608
Loss in iteration 75 : 0.5975111911464431
Loss in iteration 76 : 0.5872952640596226
Loss in iteration 77 : 0.5969943024722999
Loss in iteration 78 : 0.5874421770503315
Loss in iteration 79 : 0.5967495439804341
Loss in iteration 80 : 0.5860978113187025
Loss in iteration 81 : 0.5938401730149495
Loss in iteration 82 : 0.5846336901954414
Loss in iteration 83 : 0.5933401056800967
Loss in iteration 84 : 0.5848747891174062
Loss in iteration 85 : 0.5942043213313266
Loss in iteration 86 : 0.5856580994564838
Loss in iteration 87 : 0.5961744014329141
Loss in iteration 88 : 0.5847338196750496
Loss in iteration 89 : 0.5911160393146986
Loss in iteration 90 : 0.5823826177944785
Loss in iteration 91 : 0.5903127391857136
Loss in iteration 92 : 0.5834774076382471
Loss in iteration 93 : 0.5888093481402344
Loss in iteration 94 : 0.5819042150885875
Loss in iteration 95 : 0.5884251919690527
Loss in iteration 96 : 0.5808086604205205
Loss in iteration 97 : 0.5872155024337605
Loss in iteration 98 : 0.5792882961930642
Loss in iteration 99 : 0.5867864020353497
Loss in iteration 100 : 0.5793809656580522
Loss in iteration 101 : 0.5870733423837806
Loss in iteration 102 : 0.5803174353865302
Loss in iteration 103 : 0.5862525525975163
Loss in iteration 104 : 0.5794920386896267
Loss in iteration 105 : 0.5869867235725278
Loss in iteration 106 : 0.5786803216389778
Loss in iteration 107 : 0.5863980063944292
Loss in iteration 108 : 0.5791883655190435
Loss in iteration 109 : 0.5855835433947206
Loss in iteration 110 : 0.5782369979644819
Loss in iteration 111 : 0.5840668984873139
Loss in iteration 112 : 0.5780278002268112
Loss in iteration 113 : 0.5852183791339503
Loss in iteration 114 : 0.5791963563717801
Loss in iteration 115 : 0.584428763245094
Loss in iteration 116 : 0.5778193045675556
Loss in iteration 117 : 0.5817294929134009
Loss in iteration 118 : 0.5757040893249594
Loss in iteration 119 : 0.5806518756605237
Loss in iteration 120 : 0.5752691665686186
Loss in iteration 121 : 0.5807955701913265
Loss in iteration 122 : 0.5734609312312823
Loss in iteration 123 : 0.5776215201359255
Loss in iteration 124 : 0.5724472684193963
Loss in iteration 125 : 0.5774315659359959
Loss in iteration 126 : 0.5743594903524041
Loss in iteration 127 : 0.5783395996416768
Loss in iteration 128 : 0.5738498267320196
Loss in iteration 129 : 0.5768559908422719
Loss in iteration 130 : 0.573271707000708
Loss in iteration 131 : 0.5783741981846083
Loss in iteration 132 : 0.5751101211692674
Loss in iteration 133 : 0.5806099493283234
Loss in iteration 134 : 0.5763622914819577
Loss in iteration 135 : 0.5793018350258872
Loss in iteration 136 : 0.5734294206933565
Loss in iteration 137 : 0.5775083988038517
Loss in iteration 138 : 0.5739878580416934
Loss in iteration 139 : 0.5769097837278212
Loss in iteration 140 : 0.573061860196248
Loss in iteration 141 : 0.575895891660829
Loss in iteration 142 : 0.5722291886864534
Loss in iteration 143 : 0.575137406423043
Loss in iteration 144 : 0.5722461947000055
Loss in iteration 145 : 0.5752804033114367
Loss in iteration 146 : 0.5720452887776885
Loss in iteration 147 : 0.5737940032315737
Loss in iteration 148 : 0.57137800808296
Loss in iteration 149 : 0.5737551461786373
Loss in iteration 150 : 0.5713747081120046
Loss in iteration 151 : 0.5746395582365708
Loss in iteration 152 : 0.5715183353321672
Loss in iteration 153 : 0.5737057521309025
Loss in iteration 154 : 0.5702867401574856
Loss in iteration 155 : 0.57227064567348
Loss in iteration 156 : 0.5695962279325136
Loss in iteration 157 : 0.5724090412933998
Loss in iteration 158 : 0.5698860436288843
Loss in iteration 159 : 0.572137566943426
Loss in iteration 160 : 0.5690589020288598
Loss in iteration 161 : 0.5726580065648973
Loss in iteration 162 : 0.5700635828726168
Loss in iteration 163 : 0.5722614647497326
Loss in iteration 164 : 0.5697525307451745
Loss in iteration 165 : 0.571538426539568
Loss in iteration 166 : 0.5692681407758391
Loss in iteration 167 : 0.5716457191030297
Loss in iteration 168 : 0.5689750175609704
Loss in iteration 169 : 0.5713397160973657
Loss in iteration 170 : 0.5682716414376575
Loss in iteration 171 : 0.5707440292714658
Loss in iteration 172 : 0.5684680946455514
Loss in iteration 173 : 0.5702379420597004
Loss in iteration 174 : 0.5679080724705649
Loss in iteration 175 : 0.5697652399052218
Loss in iteration 176 : 0.5668527290292615
Loss in iteration 177 : 0.5696023587833374
Loss in iteration 178 : 0.5671984576836646
Loss in iteration 179 : 0.5696973357076526
Loss in iteration 180 : 0.5677860973217168
Loss in iteration 181 : 0.5696522350687385
Loss in iteration 182 : 0.566498786116217
Loss in iteration 183 : 0.5672253704276309
Loss in iteration 184 : 0.5660772861045771
Loss in iteration 185 : 0.5694955381949232
Loss in iteration 186 : 0.5669212805435238
Loss in iteration 187 : 0.567400302018836
Loss in iteration 188 : 0.5673388234295873
Loss in iteration 189 : 0.5709181341054945
Loss in iteration 190 : 0.5679906897268437
Loss in iteration 191 : 0.5699686324536424
Loss in iteration 192 : 0.5661884231780271
Loss in iteration 193 : 0.5688386379175301
Loss in iteration 194 : 0.56509953632821
Loss in iteration 195 : 0.5677585435321492
Loss in iteration 196 : 0.5642613607696443
Loss in iteration 197 : 0.5669612127665298
Loss in iteration 198 : 0.5644038004466679
Loss in iteration 199 : 0.5678025835550674
Loss in iteration 200 : 0.5633900427456215
Testing accuracy  of updater 5 on alg 1 with rate 0.09999999999999998 = 0.76, training accuracy 0.765, time elapsed: 4157 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.4600443021823173
Loss in iteration 3 : 2.2244825109156845
Loss in iteration 4 : 2.0088085444956767
Loss in iteration 5 : 1.2340120729365216
Loss in iteration 6 : 1.3590801679122233
Loss in iteration 7 : 0.980785666617246
Loss in iteration 8 : 1.4663927038989977
Loss in iteration 9 : 1.1323154234982153
Loss in iteration 10 : 1.1628364404795288
Loss in iteration 11 : 1.2684883983739537
Loss in iteration 12 : 0.9119434604656901
Loss in iteration 13 : 0.9922206494129464
Loss in iteration 14 : 0.9980408901016483
Loss in iteration 15 : 0.8784500343724324
Loss in iteration 16 : 1.0564179973084704
Loss in iteration 17 : 0.9513930005350325
Loss in iteration 18 : 0.8689812880881189
Loss in iteration 19 : 0.9681013683195131
Loss in iteration 20 : 0.8596155148976403
Loss in iteration 21 : 0.8165591262843801
Loss in iteration 22 : 0.8346791088194923
Loss in iteration 23 : 0.7586721605025535
Loss in iteration 24 : 0.8305721900733612
Loss in iteration 25 : 0.801870088176161
Loss in iteration 26 : 0.7611599883645158
Loss in iteration 27 : 0.7547368263687115
Loss in iteration 28 : 0.6626662050745735
Loss in iteration 29 : 0.6993242528426131
Loss in iteration 30 : 0.6783497624572118
Loss in iteration 31 : 0.6994341542406783
Loss in iteration 32 : 0.6387479059459681
Loss in iteration 33 : 0.6458625970200462
Loss in iteration 34 : 0.6219730364717657
Loss in iteration 35 : 0.6293912854012872
Loss in iteration 36 : 0.6017394520929934
Loss in iteration 37 : 0.599816666579543
Loss in iteration 38 : 0.6115250355575772
Loss in iteration 39 : 0.5901698082327739
Loss in iteration 40 : 0.5797088419335803
Loss in iteration 41 : 0.5933429438777923
Loss in iteration 42 : 0.5947884889895052
Loss in iteration 43 : 0.580198983315943
Loss in iteration 44 : 0.5589713758298132
Loss in iteration 45 : 0.5702359413359214
Loss in iteration 46 : 0.5934146724079485
Loss in iteration 47 : 0.5983104356360308
Loss in iteration 48 : 0.591146603063216
Loss in iteration 49 : 0.5985790152084176
Loss in iteration 50 : 0.5981369224492726
Loss in iteration 51 : 0.5781464077956554
Loss in iteration 52 : 0.5644237436266046
Loss in iteration 53 : 0.5443724594566097
Loss in iteration 54 : 0.5358226708106723
Loss in iteration 55 : 0.5288357931675153
Loss in iteration 56 : 0.5299581743561718
Loss in iteration 57 : 0.5220230210498182
Loss in iteration 58 : 0.5292982132137923
Loss in iteration 59 : 0.5485692425780594
Loss in iteration 60 : 0.5898345899975184
Loss in iteration 61 : 0.6613812040040667
Loss in iteration 62 : 0.7649324649301297
Loss in iteration 63 : 0.6748476529164432
Loss in iteration 64 : 0.5595616187976075
Loss in iteration 65 : 0.5246154861289282
Loss in iteration 66 : 0.5650351200840175
Loss in iteration 67 : 0.6358562994116269
Loss in iteration 68 : 0.6505866524952183
Loss in iteration 69 : 0.5870332738712052
Loss in iteration 70 : 0.5290219129761943
Loss in iteration 71 : 0.5234227977741436
Loss in iteration 72 : 0.5378841438571503
Loss in iteration 73 : 0.6002567880194136
Loss in iteration 74 : 0.6974886650242438
Loss in iteration 75 : 0.7098542314065894
Loss in iteration 76 : 0.5906480738276659
Loss in iteration 77 : 0.5235858813235683
Loss in iteration 78 : 0.5379541991130163
Loss in iteration 79 : 0.6090959574736944
Loss in iteration 80 : 0.6552447617383235
Loss in iteration 81 : 0.5987266359743134
Loss in iteration 82 : 0.5313910676617825
Loss in iteration 83 : 0.5218746626266476
Loss in iteration 84 : 0.5576252106654914
Loss in iteration 85 : 0.6098909302945326
Loss in iteration 86 : 0.6472641375026085
Loss in iteration 87 : 0.5897965655392171
Loss in iteration 88 : 0.5305719737790889
Loss in iteration 89 : 0.5180207281105932
Loss in iteration 90 : 0.5343684219244942
Loss in iteration 91 : 0.5607532307340578
Loss in iteration 92 : 0.6098028746816685
Loss in iteration 93 : 0.6226470501984771
Loss in iteration 94 : 0.5787838131211837
Loss in iteration 95 : 0.5340279244146712
Loss in iteration 96 : 0.5221908764852644
Loss in iteration 97 : 0.5250820006982934
Loss in iteration 98 : 0.5584170828625764
Loss in iteration 99 : 0.604641186746093
Loss in iteration 100 : 0.61789934548921
Loss in iteration 101 : 0.5929759320320238
Loss in iteration 102 : 0.551777802778039
Loss in iteration 103 : 0.5391477256456624
Loss in iteration 104 : 0.5227751261856842
Loss in iteration 105 : 0.5187683160023622
Loss in iteration 106 : 0.5186876778271955
Loss in iteration 107 : 0.5156997358619341
Loss in iteration 108 : 0.5276293690530393
Loss in iteration 109 : 0.5330037694206508
Loss in iteration 110 : 0.554197752627003
Loss in iteration 111 : 0.6387049836653136
Loss in iteration 112 : 0.7094939597692055
Loss in iteration 113 : 0.6842476084756908
Loss in iteration 114 : 0.5633673858611102
Loss in iteration 115 : 0.5176237902228366
Loss in iteration 116 : 0.5838709811152539
Loss in iteration 117 : 0.6301766942522296
Loss in iteration 118 : 0.6367182450314404
Loss in iteration 119 : 0.5575326072607909
Loss in iteration 120 : 0.5268219748414269
Loss in iteration 121 : 0.6082978140945969
Loss in iteration 122 : 0.6060467558765711
Loss in iteration 123 : 0.61878692951733
Loss in iteration 124 : 0.5299328530967735
Loss in iteration 125 : 0.5376141167720122
Loss in iteration 126 : 0.5408754812197762
Loss in iteration 127 : 0.5664754303163934
Loss in iteration 128 : 0.577750136034466
Loss in iteration 129 : 0.562652094482855
Loss in iteration 130 : 0.5396682239088013
Loss in iteration 131 : 0.5220007615953924
Loss in iteration 132 : 0.5193611712996766
Loss in iteration 133 : 0.5142134137978949
Loss in iteration 134 : 0.5134936049749619
Loss in iteration 135 : 0.5143459310575622
Loss in iteration 136 : 0.5098535864685715
Loss in iteration 137 : 0.514523202128903
Loss in iteration 138 : 0.5301639808719081
Loss in iteration 139 : 0.5859829451429556
Loss in iteration 140 : 0.7659132695027397
Loss in iteration 141 : 0.7943534493912762
Loss in iteration 142 : 0.6169039273695339
Loss in iteration 143 : 0.5232493663114497
Loss in iteration 144 : 0.5472544347102216
Loss in iteration 145 : 0.6501565320992182
Loss in iteration 146 : 0.6558302923966459
Loss in iteration 147 : 0.5590097388589491
Loss in iteration 148 : 0.5206290252692648
Loss in iteration 149 : 0.5621518502880872
Loss in iteration 150 : 0.636982384359813
Loss in iteration 151 : 0.6062500591100702
Loss in iteration 152 : 0.5415468769224505
Loss in iteration 153 : 0.5203152306909293
Loss in iteration 154 : 0.5458768658678063
Loss in iteration 155 : 0.5925882529038939
Loss in iteration 156 : 0.5949577037912931
Loss in iteration 157 : 0.5516104868727515
Loss in iteration 158 : 0.5274709477563623
Loss in iteration 159 : 0.5238858305536747
Loss in iteration 160 : 0.553684350048932
Loss in iteration 161 : 0.5919487554306037
Loss in iteration 162 : 0.5865144904679108
Loss in iteration 163 : 0.570367636234993
Loss in iteration 164 : 0.548444657306227
Loss in iteration 165 : 0.5424833890604356
Loss in iteration 166 : 0.5297349360312862
Loss in iteration 167 : 0.5367681376289367
Loss in iteration 168 : 0.5303289399997428
Loss in iteration 169 : 0.567065501442703
Loss in iteration 170 : 0.6011124551323195
Loss in iteration 171 : 0.6507944120302093
Loss in iteration 172 : 0.668120420570712
Loss in iteration 173 : 0.5952593448429406
Loss in iteration 174 : 0.5361997129528341
Loss in iteration 175 : 0.5300583176103497
Loss in iteration 176 : 0.5374091694563626
Loss in iteration 177 : 0.5974322411474307
Loss in iteration 178 : 0.6063697981699615
Loss in iteration 179 : 0.605306900155312
Loss in iteration 180 : 0.5425658245671195
Loss in iteration 181 : 0.5296016099590356
Loss in iteration 182 : 0.5210424956115797
Loss in iteration 183 : 0.5331724806969257
Loss in iteration 184 : 0.5615667438063324
Loss in iteration 185 : 0.587523955651578
Loss in iteration 186 : 0.5990399798620222
Loss in iteration 187 : 0.5679030013620394
Loss in iteration 188 : 0.5361298440893472
Loss in iteration 189 : 0.5216087360885739
Loss in iteration 190 : 0.5169791286832439
Loss in iteration 191 : 0.5133696361907067
Loss in iteration 192 : 0.5151353031896888
Loss in iteration 193 : 0.5189152527297926
Loss in iteration 194 : 0.5220096243834513
Loss in iteration 195 : 0.559704043724847
Loss in iteration 196 : 0.6653533222790554
Loss in iteration 197 : 0.7650102622666307
Loss in iteration 198 : 0.6765454692753361
Loss in iteration 199 : 0.5529176044337718
Loss in iteration 200 : 0.5160601753893997
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.769, training accuracy 0.775625, time elapsed: 4532 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9586565706569572
Loss in iteration 3 : 0.9153967064211506
Loss in iteration 4 : 0.9235793559332909
Loss in iteration 5 : 0.8940406637610684
Loss in iteration 6 : 0.839691425919892
Loss in iteration 7 : 0.799692355658536
Loss in iteration 8 : 0.7681023136421289
Loss in iteration 9 : 0.749005732365239
Loss in iteration 10 : 0.6982325039195856
Loss in iteration 11 : 0.6500837327862942
Loss in iteration 12 : 0.6354068268980512
Loss in iteration 13 : 0.6245548572404421
Loss in iteration 14 : 0.6013687282288187
Loss in iteration 15 : 0.581795534361215
Loss in iteration 16 : 0.5784029634533662
Loss in iteration 17 : 0.5752444771012938
Loss in iteration 18 : 0.5634915555705604
Loss in iteration 19 : 0.5563171470395991
Loss in iteration 20 : 0.5568957803187732
Loss in iteration 21 : 0.5555718105216588
Loss in iteration 22 : 0.5499091839539542
Loss in iteration 23 : 0.5469241474853644
Loss in iteration 24 : 0.5481327260327068
Loss in iteration 25 : 0.5478455282888508
Loss in iteration 26 : 0.5442382555452918
Loss in iteration 27 : 0.5413357296970055
Loss in iteration 28 : 0.5412299991839857
Loss in iteration 29 : 0.5412787482228901
Loss in iteration 30 : 0.5394061781211649
Loss in iteration 31 : 0.5372880158151698
Loss in iteration 32 : 0.53676439630059
Loss in iteration 33 : 0.536867047312448
Loss in iteration 34 : 0.5358029815598206
Loss in iteration 35 : 0.5340735766911562
Loss in iteration 36 : 0.5332010055019057
Loss in iteration 37 : 0.5328492124305025
Loss in iteration 38 : 0.5318216726509601
Loss in iteration 39 : 0.5302623706423839
Loss in iteration 40 : 0.5293171779154067
Loss in iteration 41 : 0.5291027168905156
Loss in iteration 42 : 0.528206137971091
Loss in iteration 43 : 0.5267364716830709
Loss in iteration 44 : 0.5257666220730443
Loss in iteration 45 : 0.5254203494053573
Loss in iteration 46 : 0.524760180738572
Loss in iteration 47 : 0.5236092397400094
Loss in iteration 48 : 0.5226266937142555
Loss in iteration 49 : 0.5222461034359698
Loss in iteration 50 : 0.5217017684721764
Loss in iteration 51 : 0.5207977901762694
Loss in iteration 52 : 0.520153520977984
Loss in iteration 53 : 0.519743556688107
Loss in iteration 54 : 0.5192697031983293
Loss in iteration 55 : 0.518632688072588
Loss in iteration 56 : 0.5181107754468219
Loss in iteration 57 : 0.5177658125677439
Loss in iteration 58 : 0.5173795819972047
Loss in iteration 59 : 0.516904309191163
Loss in iteration 60 : 0.5164882636180371
Loss in iteration 61 : 0.5162129759007871
Loss in iteration 62 : 0.5159555126415519
Loss in iteration 63 : 0.5156171276423689
Loss in iteration 64 : 0.5152897956001098
Loss in iteration 65 : 0.5150850722425251
Loss in iteration 66 : 0.5148947348538917
Loss in iteration 67 : 0.5146293892430271
Loss in iteration 68 : 0.5143824900278458
Loss in iteration 69 : 0.5142218022514694
Loss in iteration 70 : 0.5140716131964405
Loss in iteration 71 : 0.5138859763154265
Loss in iteration 72 : 0.5137037341962167
Loss in iteration 73 : 0.5135595173750325
Loss in iteration 74 : 0.5134180012902065
Loss in iteration 75 : 0.513263806062317
Loss in iteration 76 : 0.5131129644219131
Loss in iteration 77 : 0.5129826019657184
Loss in iteration 78 : 0.5128599321262295
Loss in iteration 79 : 0.5127249092094655
Loss in iteration 80 : 0.512605982681991
Loss in iteration 81 : 0.5125092329087683
Loss in iteration 82 : 0.5124033303462521
Loss in iteration 83 : 0.5122954885188015
Loss in iteration 84 : 0.5122046204638212
Loss in iteration 85 : 0.5121132004317416
Loss in iteration 86 : 0.5120145111423721
Loss in iteration 87 : 0.5119288447117241
Loss in iteration 88 : 0.5118520865241372
Loss in iteration 89 : 0.5117660783077368
Loss in iteration 90 : 0.5116900993952133
Loss in iteration 91 : 0.5116165974542936
Loss in iteration 92 : 0.5115419331217659
Loss in iteration 93 : 0.5114725792265334
Loss in iteration 94 : 0.5114049228767013
Loss in iteration 95 : 0.5113373962499962
Loss in iteration 96 : 0.5112708159923152
Loss in iteration 97 : 0.5112045235731304
Loss in iteration 98 : 0.5111437142574672
Loss in iteration 99 : 0.5110846136801691
Loss in iteration 100 : 0.5110253271761288
Loss in iteration 101 : 0.5109670237650432
Loss in iteration 102 : 0.5109110300452758
Loss in iteration 103 : 0.5108573948042718
Loss in iteration 104 : 0.5108054903482501
Loss in iteration 105 : 0.5107537650867748
Loss in iteration 106 : 0.5107058259478474
Loss in iteration 107 : 0.510659539968805
Loss in iteration 108 : 0.5106122355955588
Loss in iteration 109 : 0.5105646776488965
Loss in iteration 110 : 0.5105200535890215
Loss in iteration 111 : 0.5104774276030097
Loss in iteration 112 : 0.510435128458726
Loss in iteration 113 : 0.5103944287574904
Loss in iteration 114 : 0.5103534453557599
Loss in iteration 115 : 0.5103138386727056
Loss in iteration 116 : 0.5102745455530036
Loss in iteration 117 : 0.5102364937683096
Loss in iteration 118 : 0.5101997498073689
Loss in iteration 119 : 0.5101618451022175
Loss in iteration 120 : 0.5101269032682485
Loss in iteration 121 : 0.5100921513449763
Loss in iteration 122 : 0.5100569729329734
Loss in iteration 123 : 0.5100231937673658
Loss in iteration 124 : 0.5099896963834685
Loss in iteration 125 : 0.5099563346763073
Loss in iteration 126 : 0.5099229006553773
Loss in iteration 127 : 0.5098909809712124
Loss in iteration 128 : 0.5098585061637236
Loss in iteration 129 : 0.509826748377905
Loss in iteration 130 : 0.5097967835207865
Loss in iteration 131 : 0.5097668728745637
Loss in iteration 132 : 0.5097377564178721
Loss in iteration 133 : 0.5097104030443895
Loss in iteration 134 : 0.509681728414798
Loss in iteration 135 : 0.5096541030654329
Loss in iteration 136 : 0.5096280091145318
Loss in iteration 137 : 0.5096019679546568
Loss in iteration 138 : 0.5095752550879181
Loss in iteration 139 : 0.5095490725434629
Loss in iteration 140 : 0.5095232111691047
Loss in iteration 141 : 0.509498814856286
Loss in iteration 142 : 0.5094736819978669
Loss in iteration 143 : 0.5094490586528265
Loss in iteration 144 : 0.5094246513162395
Loss in iteration 145 : 0.5094008697622417
Loss in iteration 146 : 0.5093778440927705
Loss in iteration 147 : 0.5093536518052356
Loss in iteration 148 : 0.509331422182128
Loss in iteration 149 : 0.5093092189542843
Loss in iteration 150 : 0.5092871177300906
Loss in iteration 151 : 0.5092652734271322
Loss in iteration 152 : 0.5092438431912831
Loss in iteration 153 : 0.5092226260186021
Loss in iteration 154 : 0.5092023683310539
Loss in iteration 155 : 0.5091824977600304
Loss in iteration 156 : 0.5091616081369184
Loss in iteration 157 : 0.5091417546948381
Loss in iteration 158 : 0.5091230922970118
Loss in iteration 159 : 0.5091055629815264
Loss in iteration 160 : 0.5090859791256218
Loss in iteration 161 : 0.50906945812101
Loss in iteration 162 : 0.509051168089626
Loss in iteration 163 : 0.509033244040575
Loss in iteration 164 : 0.5090167177301339
Loss in iteration 165 : 0.508998652379943
Loss in iteration 166 : 0.5089821016256817
Loss in iteration 167 : 0.5089649162494637
Loss in iteration 168 : 0.5089488662071003
Loss in iteration 169 : 0.5089325194475351
Loss in iteration 170 : 0.5089163854421292
Loss in iteration 171 : 0.5089004719029486
Loss in iteration 172 : 0.5088839179761219
Loss in iteration 173 : 0.5088679666702269
Loss in iteration 174 : 0.5088521909987174
Loss in iteration 175 : 0.5088366810840035
Loss in iteration 176 : 0.5088211265162533
Loss in iteration 177 : 0.5088059801399122
Loss in iteration 178 : 0.5087904395498781
Loss in iteration 179 : 0.5087753549530107
Loss in iteration 180 : 0.5087604387021939
Loss in iteration 181 : 0.5087452999760957
Loss in iteration 182 : 0.5087302502393481
Loss in iteration 183 : 0.50871572921594
Loss in iteration 184 : 0.5087004928254752
Loss in iteration 185 : 0.5086859366704423
Loss in iteration 186 : 0.5086711595085259
Loss in iteration 187 : 0.5086567932333811
Loss in iteration 188 : 0.5086425554082032
Loss in iteration 189 : 0.5086285175847551
Loss in iteration 190 : 0.5086142019874137
Loss in iteration 191 : 0.5086006874344559
Loss in iteration 192 : 0.5085867407244886
Loss in iteration 193 : 0.5085728562069163
Loss in iteration 194 : 0.5085595017685798
Loss in iteration 195 : 0.5085459200387156
Loss in iteration 196 : 0.5085320483957374
Loss in iteration 197 : 0.5085188391407498
Loss in iteration 198 : 0.5085055690653039
Loss in iteration 199 : 0.5084924778144747
Loss in iteration 200 : 0.5084788395415496
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.781, training accuracy 0.78775, time elapsed: 3984 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9948904156307494
Loss in iteration 3 : 0.9852809910674639
Loss in iteration 4 : 0.9717110627192994
Loss in iteration 5 : 0.9550888090905523
Loss in iteration 6 : 0.937622790673094
Loss in iteration 7 : 0.9226642981461946
Loss in iteration 8 : 0.913272607663436
Loss in iteration 9 : 0.9088497279205947
Loss in iteration 10 : 0.9073489743125547
Loss in iteration 11 : 0.9053956345195451
Loss in iteration 12 : 0.9002908445940337
Loss in iteration 13 : 0.8915135476768619
Loss in iteration 14 : 0.8802226250230439
Loss in iteration 15 : 0.8680710108523397
Loss in iteration 16 : 0.8562076286711453
Loss in iteration 17 : 0.8453931010757677
Loss in iteration 18 : 0.8356410438666576
Loss in iteration 19 : 0.8266110929540105
Loss in iteration 20 : 0.8179604151108342
Loss in iteration 21 : 0.8093807347274377
Loss in iteration 22 : 0.8007018923939718
Loss in iteration 23 : 0.7917857534500427
Loss in iteration 24 : 0.7826215826391124
Loss in iteration 25 : 0.7733594827532924
Loss in iteration 26 : 0.7642697626825162
Loss in iteration 27 : 0.7552968441915109
Loss in iteration 28 : 0.7464025089225184
Loss in iteration 29 : 0.7374965464611282
Loss in iteration 30 : 0.7286006764769121
Loss in iteration 31 : 0.7198787237020743
Loss in iteration 32 : 0.711414203435291
Loss in iteration 33 : 0.7032517883722621
Loss in iteration 34 : 0.6954130937398074
Loss in iteration 35 : 0.687872080475805
Loss in iteration 36 : 0.6806594113889893
Loss in iteration 37 : 0.6737435782243073
Loss in iteration 38 : 0.6670676838148513
Loss in iteration 39 : 0.6606482588951366
Loss in iteration 40 : 0.6545233963066059
Loss in iteration 41 : 0.6488245263465862
Loss in iteration 42 : 0.6436389401625632
Loss in iteration 43 : 0.6388922631165264
Loss in iteration 44 : 0.6345544719265201
Loss in iteration 45 : 0.6304642262994324
Loss in iteration 46 : 0.6265335084033511
Loss in iteration 47 : 0.6227986973791607
Loss in iteration 48 : 0.6192222043514604
Loss in iteration 49 : 0.6158425176435083
Loss in iteration 50 : 0.6126813238195574
Loss in iteration 51 : 0.6097711206172657
Loss in iteration 52 : 0.6071112059758228
Loss in iteration 53 : 0.6046520210342323
Loss in iteration 54 : 0.6023730189425116
Loss in iteration 55 : 0.6002485929386079
Loss in iteration 56 : 0.5982343991413116
Loss in iteration 57 : 0.5963251414460999
Loss in iteration 58 : 0.5944850236847251
Loss in iteration 59 : 0.5926874612016655
Loss in iteration 60 : 0.5909492805964904
Loss in iteration 61 : 0.589279193274426
Loss in iteration 62 : 0.5876932049881262
Loss in iteration 63 : 0.5861872876425023
Loss in iteration 64 : 0.58475769307967
Loss in iteration 65 : 0.5834011060118898
Loss in iteration 66 : 0.5821037381652677
Loss in iteration 67 : 0.5808573320958
Loss in iteration 68 : 0.5796562476621687
Loss in iteration 69 : 0.5785013942891812
Loss in iteration 70 : 0.5773925489557464
Loss in iteration 71 : 0.5763193522004092
Loss in iteration 72 : 0.5752825083461143
Loss in iteration 73 : 0.5742887136292016
Loss in iteration 74 : 0.5733381101362841
Loss in iteration 75 : 0.5724310165210418
Loss in iteration 76 : 0.5715637112847275
Loss in iteration 77 : 0.5707224593655048
Loss in iteration 78 : 0.5699018562707523
Loss in iteration 79 : 0.5690989262686665
Loss in iteration 80 : 0.5683140398762716
Loss in iteration 81 : 0.5675467095934937
Loss in iteration 82 : 0.5668014109984011
Loss in iteration 83 : 0.5660801065850124
Loss in iteration 84 : 0.5653852721013201
Loss in iteration 85 : 0.5647130930148954
Loss in iteration 86 : 0.5640572017259564
Loss in iteration 87 : 0.5634216178762471
Loss in iteration 88 : 0.5628031366068701
Loss in iteration 89 : 0.5622012288962762
Loss in iteration 90 : 0.5616098728511726
Loss in iteration 91 : 0.561033239023514
Loss in iteration 92 : 0.5604709834418441
Loss in iteration 93 : 0.5599226089462231
Loss in iteration 94 : 0.5593902015418266
Loss in iteration 95 : 0.558868674513437
Loss in iteration 96 : 0.558357116008181
Loss in iteration 97 : 0.5578551208499225
Loss in iteration 98 : 0.5573657818494526
Loss in iteration 99 : 0.5568865441989644
Loss in iteration 100 : 0.5564181382850075
Loss in iteration 101 : 0.5559591756937833
Loss in iteration 102 : 0.5555106875725303
Loss in iteration 103 : 0.5550728275397976
Loss in iteration 104 : 0.554643821335958
Loss in iteration 105 : 0.5542244662867555
Loss in iteration 106 : 0.5538125550797124
Loss in iteration 107 : 0.5534095348802086
Loss in iteration 108 : 0.5530139857434218
Loss in iteration 109 : 0.5526240895879058
Loss in iteration 110 : 0.5522401531173987
Loss in iteration 111 : 0.5518630458745959
Loss in iteration 112 : 0.5514942826272066
Loss in iteration 113 : 0.551130693136137
Loss in iteration 114 : 0.5507713005704747
Loss in iteration 115 : 0.5504158018955178
Loss in iteration 116 : 0.5500646583263239
Loss in iteration 117 : 0.5497160428317256
Loss in iteration 118 : 0.5493726013817132
Loss in iteration 119 : 0.5490346372799815
Loss in iteration 120 : 0.5487024430853722
Loss in iteration 121 : 0.5483763636203746
Loss in iteration 122 : 0.5480573721726981
Loss in iteration 123 : 0.5477455108638942
Loss in iteration 124 : 0.5474384483196105
Loss in iteration 125 : 0.5471344420817662
Loss in iteration 126 : 0.5468334743358665
Loss in iteration 127 : 0.5465372495242922
Loss in iteration 128 : 0.5462457531986465
Loss in iteration 129 : 0.545956854771734
Loss in iteration 130 : 0.545671038599104
Loss in iteration 131 : 0.5453899180065844
Loss in iteration 132 : 0.5451125419132226
Loss in iteration 133 : 0.5448390871599119
Loss in iteration 134 : 0.5445673439615843
Loss in iteration 135 : 0.5442981516227986
Loss in iteration 136 : 0.544031593048771
Loss in iteration 137 : 0.5437672899718501
Loss in iteration 138 : 0.5435057384318843
Loss in iteration 139 : 0.5432483518218417
Loss in iteration 140 : 0.5429930115457267
Loss in iteration 141 : 0.5427402585321148
Loss in iteration 142 : 0.5424908886252273
Loss in iteration 143 : 0.5422442036842183
Loss in iteration 144 : 0.5419996052270294
Loss in iteration 145 : 0.5417568009865396
Loss in iteration 146 : 0.5415162550776005
Loss in iteration 147 : 0.5412783748127542
Loss in iteration 148 : 0.54104378849548
Loss in iteration 149 : 0.5408122058335516
Loss in iteration 150 : 0.5405837518105052
Loss in iteration 151 : 0.5403579843097532
Loss in iteration 152 : 0.5401344770067655
Loss in iteration 153 : 0.539912751605865
Loss in iteration 154 : 0.5396931572744825
Loss in iteration 155 : 0.5394762006033118
Loss in iteration 156 : 0.5392626037061853
Loss in iteration 157 : 0.5390526024628083
Loss in iteration 158 : 0.5388454782820272
Loss in iteration 159 : 0.5386400541280626
Loss in iteration 160 : 0.5384368291020931
Loss in iteration 161 : 0.5382360153893757
Loss in iteration 162 : 0.5380376132840407
Loss in iteration 163 : 0.5378409456682782
Loss in iteration 164 : 0.5376461690551448
Loss in iteration 165 : 0.5374536297956407
Loss in iteration 166 : 0.5372623692731311
Loss in iteration 167 : 0.5370728266436835
Loss in iteration 168 : 0.5368855255624503
Loss in iteration 169 : 0.5367004570282861
Loss in iteration 170 : 0.5365185266848066
Loss in iteration 171 : 0.5363375394652541
Loss in iteration 172 : 0.5361574113211829
Loss in iteration 173 : 0.5359785969884396
Loss in iteration 174 : 0.5358016320879397
Loss in iteration 175 : 0.5356293524015732
Loss in iteration 176 : 0.5354596187831261
Loss in iteration 177 : 0.535290597840361
Loss in iteration 178 : 0.5351221917881871
Loss in iteration 179 : 0.5349548500424457
Loss in iteration 180 : 0.5347887247763108
Loss in iteration 181 : 0.5346245375484501
Loss in iteration 182 : 0.5344616416386546
Loss in iteration 183 : 0.5343005441598911
Loss in iteration 184 : 0.534141582347684
Loss in iteration 185 : 0.5339837616420466
Loss in iteration 186 : 0.5338265388279131
Loss in iteration 187 : 0.5336704809601888
Loss in iteration 188 : 0.5335161639860952
Loss in iteration 189 : 0.5333639624765341
Loss in iteration 190 : 0.5332147534030405
Loss in iteration 191 : 0.5330675136957875
Loss in iteration 192 : 0.5329218422908791
Loss in iteration 193 : 0.5327769073994124
Loss in iteration 194 : 0.5326328465127503
Loss in iteration 195 : 0.5324914638014072
Loss in iteration 196 : 0.5323519464786196
Loss in iteration 197 : 0.5322134799990288
Loss in iteration 198 : 0.5320766598880234
Loss in iteration 199 : 0.5319410802924194
Loss in iteration 200 : 0.5318061015512137
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.775, training accuracy 0.778875, time elapsed: 4587 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 21.385250216909075
Loss in iteration 3 : 10.124753627480736
Loss in iteration 4 : 11.207942337773286
Loss in iteration 5 : 5.711671050904873
Loss in iteration 6 : 6.785650977398401
Loss in iteration 7 : 3.634358231858629
Loss in iteration 8 : 6.221338542391407
Loss in iteration 9 : 6.34655509472966
Loss in iteration 10 : 5.715513915246307
Loss in iteration 11 : 5.759177408470961
Loss in iteration 12 : 4.469657742779539
Loss in iteration 13 : 3.705707351113702
Loss in iteration 14 : 4.281324775730312
Loss in iteration 15 : 4.328738072010544
Loss in iteration 16 : 4.141671538302027
Loss in iteration 17 : 4.091904127625589
Loss in iteration 18 : 3.750164795557842
Loss in iteration 19 : 3.6031593684272254
Loss in iteration 20 : 3.8302117667971856
Loss in iteration 21 : 3.738907356192962
Loss in iteration 22 : 3.297896118699143
Loss in iteration 23 : 2.950818866908053
Loss in iteration 24 : 2.8342248262308605
Loss in iteration 25 : 2.8515738464525695
Loss in iteration 26 : 2.976126714671963
Loss in iteration 27 : 2.9719485538869166
Loss in iteration 28 : 2.7014937312761735
Loss in iteration 29 : 2.4530618816033134
Loss in iteration 30 : 2.3166870215930255
Loss in iteration 31 : 2.251833666105427
Loss in iteration 32 : 2.2402688198232625
Loss in iteration 33 : 2.1307578777665475
Loss in iteration 34 : 1.9997790199060945
Loss in iteration 35 : 2.011253417273661
Loss in iteration 36 : 1.9426148522819193
Loss in iteration 37 : 1.87466240826583
Loss in iteration 38 : 1.7105019016225158
Loss in iteration 39 : 1.6757664045500331
Loss in iteration 40 : 1.6483643052109107
Loss in iteration 41 : 1.6604026429703682
Loss in iteration 42 : 1.5835053210458114
Loss in iteration 43 : 1.530756235988073
Loss in iteration 44 : 1.4392075166394542
Loss in iteration 45 : 1.4337885573545581
Loss in iteration 46 : 1.383121713408347
Loss in iteration 47 : 1.3740292703797203
Loss in iteration 48 : 1.3464774017603818
Loss in iteration 49 : 1.2723032851136669
Loss in iteration 50 : 1.2429934003988978
Loss in iteration 51 : 1.1871771617398512
Loss in iteration 52 : 1.1883961803909273
Loss in iteration 53 : 1.199657897792741
Loss in iteration 54 : 1.07425503644466
Loss in iteration 55 : 1.0676177359081749
Loss in iteration 56 : 1.0518877452656301
Loss in iteration 57 : 1.1076373600264855
Loss in iteration 58 : 1.5809588171316522
Loss in iteration 59 : 2.613092577178751
Loss in iteration 60 : 1.6600646641547878
Loss in iteration 61 : 0.8635142462096415
Loss in iteration 62 : 1.2722946365068857
Loss in iteration 63 : 1.9969351864955383
Loss in iteration 64 : 1.5552023222605065
Loss in iteration 65 : 1.1658236403266937
Loss in iteration 66 : 0.9856266875183116
Loss in iteration 67 : 1.4077817104548793
Loss in iteration 68 : 1.5874720205736252
Loss in iteration 69 : 1.0320325181132006
Loss in iteration 70 : 1.1065960738318958
Loss in iteration 71 : 0.9639236923000281
Loss in iteration 72 : 0.8588683247575877
Loss in iteration 73 : 1.0685123007758657
Loss in iteration 74 : 1.1345245342039858
Loss in iteration 75 : 2.017777298778619
Loss in iteration 76 : 2.856070875568698
Loss in iteration 77 : 1.190488876945836
Loss in iteration 78 : 0.8062196533418546
Loss in iteration 79 : 1.5637132016452298
Loss in iteration 80 : 2.321898677444904
Loss in iteration 81 : 1.211014055992848
Loss in iteration 82 : 0.8681268331307154
Loss in iteration 83 : 1.0033051436527436
Loss in iteration 84 : 1.6634611983324858
Loss in iteration 85 : 2.095586689602663
Loss in iteration 86 : 1.3025635281905663
Loss in iteration 87 : 0.8387988475249529
Loss in iteration 88 : 0.8233396282836961
Loss in iteration 89 : 1.0041195179117035
Loss in iteration 90 : 1.7535205256730138
Loss in iteration 91 : 2.510341161237816
Loss in iteration 92 : 1.0790971609470112
Loss in iteration 93 : 0.8353336929620679
Loss in iteration 94 : 1.6074193245352364
Loss in iteration 95 : 1.9498118033310452
Loss in iteration 96 : 1.1801699465249453
Loss in iteration 97 : 0.920811927937325
Loss in iteration 98 : 0.9106764433003023
Loss in iteration 99 : 1.1452573311605454
Loss in iteration 100 : 1.657921299719665
Loss in iteration 101 : 1.5357570513753032
Loss in iteration 102 : 1.1334448043179217
Loss in iteration 103 : 0.9471616119984781
Loss in iteration 104 : 0.9180180187308528
Loss in iteration 105 : 1.068415271544221
Loss in iteration 106 : 1.5892562485764694
Loss in iteration 107 : 2.31124698649541
Loss in iteration 108 : 1.2927399034062812
Loss in iteration 109 : 0.9074236557979523
Loss in iteration 110 : 0.8552787878340132
Loss in iteration 111 : 0.8610839779869097
Loss in iteration 112 : 0.9612253676104499
Loss in iteration 113 : 1.4419442021453832
Loss in iteration 114 : 2.0864520492573266
Loss in iteration 115 : 1.9999645105629273
Loss in iteration 116 : 1.0317919434047917
Loss in iteration 117 : 0.8179042522974312
Loss in iteration 118 : 0.7740672581723811
Loss in iteration 119 : 0.7712338453859048
Loss in iteration 120 : 0.7905120628098941
Loss in iteration 121 : 0.9914765048565805
Loss in iteration 122 : 2.167898673402028
Loss in iteration 123 : 2.3846308065710606
Loss in iteration 124 : 1.4772838153526135
Loss in iteration 125 : 0.9234164279042909
Loss in iteration 126 : 0.8266838643783022
Loss in iteration 127 : 0.8286372730710263
Loss in iteration 128 : 0.8766176823413466
Loss in iteration 129 : 1.1804062387345298
Loss in iteration 130 : 1.9648529011774853
Loss in iteration 131 : 1.6727551284066569
Loss in iteration 132 : 1.3492173186176057
Loss in iteration 133 : 1.0679316205298957
Loss in iteration 134 : 1.0864722829792732
Loss in iteration 135 : 1.1884047016421897
Loss in iteration 136 : 1.464238380795534
Loss in iteration 137 : 1.3574162043808478
Loss in iteration 138 : 1.253078974788906
Loss in iteration 139 : 1.0107451147231288
Loss in iteration 140 : 0.9233783667619957
Loss in iteration 141 : 0.925260511612618
Loss in iteration 142 : 1.0707328040558668
Loss in iteration 143 : 1.443611876304912
Loss in iteration 144 : 2.0871167075211807
Loss in iteration 145 : 1.3429345045831613
Loss in iteration 146 : 1.0569459350024202
Loss in iteration 147 : 1.0214383189486198
Loss in iteration 148 : 1.2406866751133625
Loss in iteration 149 : 1.4830061072550194
Loss in iteration 150 : 1.6955220294548363
Loss in iteration 151 : 1.227070542186431
Loss in iteration 152 : 1.002338131016334
Loss in iteration 153 : 0.9396217080870641
Loss in iteration 154 : 0.9644073957955134
Loss in iteration 155 : 1.1779743708928714
Loss in iteration 156 : 1.7927597913221185
Loss in iteration 157 : 1.577434991613583
Loss in iteration 158 : 1.397653719088178
Loss in iteration 159 : 1.1717526969002896
Loss in iteration 160 : 1.2303184868829007
Loss in iteration 161 : 1.240214619933061
Loss in iteration 162 : 1.3325833798231541
Loss in iteration 163 : 1.2004362794185774
Loss in iteration 164 : 1.149781683897113
Loss in iteration 165 : 1.0671267617693154
Loss in iteration 166 : 1.0703165615403076
Loss in iteration 167 : 1.170195661531976
Loss in iteration 168 : 1.5020943825528492
Loss in iteration 169 : 1.5520494288627178
Loss in iteration 170 : 1.5747328981210593
Loss in iteration 171 : 1.2566181441847166
Loss in iteration 172 : 1.1782448960919154
Loss in iteration 173 : 1.1303930321075564
Loss in iteration 174 : 1.2666598648556984
Loss in iteration 175 : 1.3242926224404206
Loss in iteration 176 : 1.4652503479862689
Loss in iteration 177 : 1.2877459875836863
Loss in iteration 178 : 1.2293138327214301
Loss in iteration 179 : 1.0866655657866549
Loss in iteration 180 : 1.0871271006624552
Loss in iteration 181 : 1.161089210088872
Loss in iteration 182 : 1.5473648651741572
Loss in iteration 183 : 1.5809261737341564
Loss in iteration 184 : 1.5935113007131498
Loss in iteration 185 : 1.1927437256806361
Loss in iteration 186 : 1.1087759172245286
Loss in iteration 187 : 1.097213793324043
Loss in iteration 188 : 1.2064100437776173
Loss in iteration 189 : 1.2894789524914645
Loss in iteration 190 : 1.481194980030932
Loss in iteration 191 : 1.3814824097890994
Loss in iteration 192 : 1.3847824760529026
Loss in iteration 193 : 1.2072210099023999
Loss in iteration 194 : 1.2099648902890725
Loss in iteration 195 : 1.1837329451754854
Loss in iteration 196 : 1.3406196585055183
Loss in iteration 197 : 1.380783266546733
Loss in iteration 198 : 1.5127997298803073
Loss in iteration 199 : 1.337440166897231
Loss in iteration 200 : 1.3002391922858345
Testing accuracy  of updater 7 on alg 1 with rate 20.0 = 0.708, training accuracy 0.71775, time elapsed: 4297 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0597863668588985
Loss in iteration 3 : 0.8597859613161395
Loss in iteration 4 : 0.857902192887839
Loss in iteration 5 : 0.6996730321407921
Loss in iteration 6 : 0.6692698727439367
Loss in iteration 7 : 0.5925983715778794
Loss in iteration 8 : 0.5820669244074037
Loss in iteration 9 : 0.5718817703616491
Loss in iteration 10 : 0.5647919951934319
Loss in iteration 11 : 0.5615439968987013
Loss in iteration 12 : 0.5660868816873442
Loss in iteration 13 : 0.5567770565574688
Loss in iteration 14 : 0.5686776408717616
Loss in iteration 15 : 0.5581692568331655
Loss in iteration 16 : 0.5642684450500094
Loss in iteration 17 : 0.5632647094483002
Loss in iteration 18 : 0.5578956379194877
Loss in iteration 19 : 0.5629412588112959
Loss in iteration 20 : 0.5563596148774095
Loss in iteration 21 : 0.5559555133578906
Loss in iteration 22 : 0.5557231300257759
Loss in iteration 23 : 0.5492219199868047
Loss in iteration 24 : 0.5495381573419797
Loss in iteration 25 : 0.5447845194476418
Loss in iteration 26 : 0.5405018816789705
Loss in iteration 27 : 0.5392281232123566
Loss in iteration 28 : 0.5330992350696583
Loss in iteration 29 : 0.5322029969345197
Loss in iteration 30 : 0.5281958653658123
Loss in iteration 31 : 0.525298585832069
Loss in iteration 32 : 0.5238323521229318
Loss in iteration 33 : 0.5201349558115921
Loss in iteration 34 : 0.5203185639934335
Loss in iteration 35 : 0.5173768433530572
Loss in iteration 36 : 0.5177904943132793
Loss in iteration 37 : 0.5156889946773391
Loss in iteration 38 : 0.5163575496172221
Loss in iteration 39 : 0.5146620820039081
Loss in iteration 40 : 0.5154706022409895
Loss in iteration 41 : 0.5142488135683048
Loss in iteration 42 : 0.514959701169721
Loss in iteration 43 : 0.5140676599915764
Loss in iteration 44 : 0.5145906156549974
Loss in iteration 45 : 0.5139256025004462
Loss in iteration 46 : 0.5142127322931799
Loss in iteration 47 : 0.5136515378821417
Loss in iteration 48 : 0.5137900398819939
Loss in iteration 49 : 0.5132093927108217
Loss in iteration 50 : 0.5131606656330706
Loss in iteration 51 : 0.5125260172968136
Loss in iteration 52 : 0.5123898335183217
Loss in iteration 53 : 0.5118096574028536
Loss in iteration 54 : 0.5116403302745225
Loss in iteration 55 : 0.5111228056788887
Loss in iteration 56 : 0.5109740303378337
Loss in iteration 57 : 0.5104870565114634
Loss in iteration 58 : 0.5103528955268366
Loss in iteration 59 : 0.5099788536206653
Loss in iteration 60 : 0.5098790818360982
Loss in iteration 61 : 0.5095714796915207
Loss in iteration 62 : 0.5094638704077457
Loss in iteration 63 : 0.5092362146841322
Loss in iteration 64 : 0.5091708705922053
Loss in iteration 65 : 0.5090028248720132
Loss in iteration 66 : 0.5089290381422227
Loss in iteration 67 : 0.5088199265456005
Loss in iteration 68 : 0.5087819360875329
Loss in iteration 69 : 0.5086847317443014
Loss in iteration 70 : 0.5086533741416971
Loss in iteration 71 : 0.508585203547571
Loss in iteration 72 : 0.5085203551620651
Loss in iteration 73 : 0.5084502266535487
Loss in iteration 74 : 0.5084216201356466
Loss in iteration 75 : 0.5083466898679838
Loss in iteration 76 : 0.5083292783633246
Loss in iteration 77 : 0.5082565898828048
Loss in iteration 78 : 0.5082169450143591
Loss in iteration 79 : 0.5081515094233979
Loss in iteration 80 : 0.5081237164410413
Loss in iteration 81 : 0.5080502422022671
Loss in iteration 82 : 0.508034938258964
Loss in iteration 83 : 0.5079672440680467
Loss in iteration 84 : 0.5079383997302235
Loss in iteration 85 : 0.5078831704801638
Loss in iteration 86 : 0.5078607190619295
Loss in iteration 87 : 0.5078220070146356
Loss in iteration 88 : 0.5077899138505109
Loss in iteration 89 : 0.5077713891017951
Loss in iteration 90 : 0.5077323264983459
Loss in iteration 91 : 0.5077130877969936
Loss in iteration 92 : 0.5076860851666435
Loss in iteration 93 : 0.5076637811712481
Loss in iteration 94 : 0.5076368100246416
Loss in iteration 95 : 0.5076191140960884
Loss in iteration 96 : 0.5075878031595091
Loss in iteration 97 : 0.5075708145591955
Loss in iteration 98 : 0.5075458442638494
Loss in iteration 99 : 0.5075206853279781
Loss in iteration 100 : 0.5074925188398121
Loss in iteration 101 : 0.5074706119628144
Loss in iteration 102 : 0.5074512552882131
Loss in iteration 103 : 0.507425646299556
Loss in iteration 104 : 0.5074097942511068
Loss in iteration 105 : 0.507384312055201
Loss in iteration 106 : 0.5073660937963578
Loss in iteration 107 : 0.507346288791315
Loss in iteration 108 : 0.5073328664081856
Loss in iteration 109 : 0.5073136436479598
Loss in iteration 110 : 0.507296018521834
Loss in iteration 111 : 0.5072794471702737
Loss in iteration 112 : 0.5072646403551792
Loss in iteration 113 : 0.5072445490883775
Loss in iteration 114 : 0.5072353242186017
Loss in iteration 115 : 0.5072134964358983
Loss in iteration 116 : 0.5072037171208897
Loss in iteration 117 : 0.5071862884899386
Loss in iteration 118 : 0.5071712242948619
Loss in iteration 119 : 0.5071591000804755
Loss in iteration 120 : 0.5071389513047785
Loss in iteration 121 : 0.5071268677185227
Loss in iteration 122 : 0.5071128513341806
Loss in iteration 123 : 0.5071021538208562
Loss in iteration 124 : 0.507084057311338
Loss in iteration 125 : 0.507076553107692
Loss in iteration 126 : 0.507057445822965
Loss in iteration 127 : 0.507049424717117
Loss in iteration 128 : 0.5070363240691467
Loss in iteration 129 : 0.5070205274446559
Loss in iteration 130 : 0.5070115174937414
Loss in iteration 131 : 0.5069984683891703
Loss in iteration 132 : 0.5069876498375644
Loss in iteration 133 : 0.5069764094468838
Loss in iteration 134 : 0.5069621296741316
Loss in iteration 135 : 0.5069549432469855
Loss in iteration 136 : 0.5069498091215823
Loss in iteration 137 : 0.5069293730277337
Loss in iteration 138 : 0.5069275595187467
Loss in iteration 139 : 0.5069162689763498
Loss in iteration 140 : 0.5069000494228898
Loss in iteration 141 : 0.5068928209620992
Loss in iteration 142 : 0.506880909039098
Loss in iteration 143 : 0.5068661520898697
Loss in iteration 144 : 0.5068585209175617
Loss in iteration 145 : 0.5068529533181331
Loss in iteration 146 : 0.5068394088538212
Loss in iteration 147 : 0.5068324108341883
Loss in iteration 148 : 0.506823932544487
Loss in iteration 149 : 0.5068119709132564
Loss in iteration 150 : 0.5068044021751951
Loss in iteration 151 : 0.5067962677729988
Loss in iteration 152 : 0.5067826235198482
Loss in iteration 153 : 0.5067816879125928
Loss in iteration 154 : 0.5067819164347517
Loss in iteration 155 : 0.5067564124581092
Loss in iteration 156 : 0.5067734940732178
Loss in iteration 157 : 0.5067428448240221
Loss in iteration 158 : 0.5067351140318348
Loss in iteration 159 : 0.5067337701332741
Loss in iteration 160 : 0.5067147494465278
Loss in iteration 161 : 0.5067131820001174
Loss in iteration 162 : 0.5067019643220096
Loss in iteration 163 : 0.5066889829540323
Loss in iteration 164 : 0.5066821817128893
Loss in iteration 165 : 0.5066760178953084
Loss in iteration 166 : 0.506672388961549
Loss in iteration 167 : 0.5066644901585794
Loss in iteration 168 : 0.5066567713724373
Loss in iteration 169 : 0.5066481968379194
Loss in iteration 170 : 0.5066480178306237
Loss in iteration 171 : 0.5066319155596287
Loss in iteration 172 : 0.5066314315231736
Loss in iteration 173 : 0.5066286317828692
Loss in iteration 174 : 0.5066128899951607
Loss in iteration 175 : 0.5066082587999355
Loss in iteration 176 : 0.5066087838579433
Loss in iteration 177 : 0.5065977694481284
Loss in iteration 178 : 0.5066023243445371
Loss in iteration 179 : 0.506585622645789
Loss in iteration 180 : 0.5065912628114654
Loss in iteration 181 : 0.5065775483031717
Loss in iteration 182 : 0.5065662406403709
Loss in iteration 183 : 0.5065673763439741
Loss in iteration 184 : 0.5065598028287702
Loss in iteration 185 : 0.5065512413191744
Loss in iteration 186 : 0.5065506219858227
Loss in iteration 187 : 0.5065458436360812
Loss in iteration 188 : 0.5065401394340386
Loss in iteration 189 : 0.5065353428377509
Loss in iteration 190 : 0.5065339762144773
Loss in iteration 191 : 0.5065269549846022
Loss in iteration 192 : 0.5065190326213547
Loss in iteration 193 : 0.5065136577574665
Loss in iteration 194 : 0.5065149327139405
Loss in iteration 195 : 0.5065081151300141
Loss in iteration 196 : 0.5065009223286971
Loss in iteration 197 : 0.5065003740025817
Loss in iteration 198 : 0.5064949867775649
Loss in iteration 199 : 0.5064883258396524
Loss in iteration 200 : 0.5064893340125285
Testing accuracy  of updater 7 on alg 1 with rate 2.0 = 0.7815, training accuracy 0.787625, time elapsed: 3806 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9844056292713212
Loss in iteration 3 : 0.9550135525822953
Loss in iteration 4 : 0.9225365268238354
Loss in iteration 5 : 0.9113931327845101
Loss in iteration 6 : 0.9139413054155944
Loss in iteration 7 : 0.9014218770353004
Loss in iteration 8 : 0.8727768122288151
Loss in iteration 9 : 0.8436212175491936
Loss in iteration 10 : 0.8206622895804456
Loss in iteration 11 : 0.8010409798375563
Loss in iteration 12 : 0.7815981039458877
Loss in iteration 13 : 0.7637545124814676
Loss in iteration 14 : 0.7461453647988374
Loss in iteration 15 : 0.7221865052767129
Loss in iteration 16 : 0.694024691897225
Loss in iteration 17 : 0.6699757786629285
Loss in iteration 18 : 0.6531974059432584
Loss in iteration 19 : 0.6407412567909223
Loss in iteration 20 : 0.6282653656622817
Loss in iteration 21 : 0.6152584743438819
Loss in iteration 22 : 0.6036908597761911
Loss in iteration 23 : 0.5956853376645329
Loss in iteration 24 : 0.5907656429079559
Loss in iteration 25 : 0.586104187825933
Loss in iteration 26 : 0.580322410926317
Loss in iteration 27 : 0.5741036007691768
Loss in iteration 28 : 0.569358850796473
Loss in iteration 29 : 0.5663889482809814
Loss in iteration 30 : 0.5643526289679653
Loss in iteration 31 : 0.5622224548558676
Loss in iteration 32 : 0.5595970217612776
Loss in iteration 33 : 0.5570096970099923
Loss in iteration 34 : 0.5551376494065098
Loss in iteration 35 : 0.5539421764370619
Loss in iteration 36 : 0.5528276811493752
Loss in iteration 37 : 0.5513956180895281
Loss in iteration 38 : 0.5497421715944533
Loss in iteration 39 : 0.5482578382563282
Loss in iteration 40 : 0.547253967481231
Loss in iteration 41 : 0.5465894737505212
Loss in iteration 42 : 0.5458126247987677
Loss in iteration 43 : 0.5447783050715521
Loss in iteration 44 : 0.5436280109748526
Loss in iteration 45 : 0.5426502735740543
Loss in iteration 46 : 0.5419475889493036
Loss in iteration 47 : 0.5413357264803506
Loss in iteration 48 : 0.540660625124746
Loss in iteration 49 : 0.5399139973437593
Loss in iteration 50 : 0.5391920366762892
Loss in iteration 51 : 0.5386140275276786
Loss in iteration 52 : 0.5381016013879669
Loss in iteration 53 : 0.5375685704487508
Loss in iteration 54 : 0.536978108067223
Loss in iteration 55 : 0.5363417477598387
Loss in iteration 56 : 0.5357295248827642
Loss in iteration 57 : 0.5351942333463824
Loss in iteration 58 : 0.5347261548017207
Loss in iteration 59 : 0.5342707165005767
Loss in iteration 60 : 0.5337781137472132
Loss in iteration 61 : 0.5332719042194938
Loss in iteration 62 : 0.5327891472482272
Loss in iteration 63 : 0.5323441355606523
Loss in iteration 64 : 0.5319277550917709
Loss in iteration 65 : 0.5315059373107104
Loss in iteration 66 : 0.5310706976293138
Loss in iteration 67 : 0.5306479782228618
Loss in iteration 68 : 0.5302499021334315
Loss in iteration 69 : 0.5298774154342546
Loss in iteration 70 : 0.5295070287827796
Loss in iteration 71 : 0.5291282071750684
Loss in iteration 72 : 0.5287598030680816
Loss in iteration 73 : 0.5284133418084884
Loss in iteration 74 : 0.52807969894082
Loss in iteration 75 : 0.5277520508146145
Loss in iteration 76 : 0.5274261201799644
Loss in iteration 77 : 0.5271063905478786
Loss in iteration 78 : 0.5267912726023577
Loss in iteration 79 : 0.526483130344817
Loss in iteration 80 : 0.5261829871418329
Loss in iteration 81 : 0.52589055161702
Loss in iteration 82 : 0.5256070584024446
Loss in iteration 83 : 0.52533234633279
Loss in iteration 84 : 0.5250614225403366
Loss in iteration 85 : 0.5247984216279017
Loss in iteration 86 : 0.5245391355387367
Loss in iteration 87 : 0.5242856817323273
Loss in iteration 88 : 0.524038668492049
Loss in iteration 89 : 0.5237955235204986
Loss in iteration 90 : 0.5235585302527266
Loss in iteration 91 : 0.5233253495265693
Loss in iteration 92 : 0.5230953825800961
Loss in iteration 93 : 0.5228706149935608
Loss in iteration 94 : 0.5226519207436122
Loss in iteration 95 : 0.5224383539154778
Loss in iteration 96 : 0.5222305787647905
Loss in iteration 97 : 0.5220284648628865
Loss in iteration 98 : 0.5218302786183261
Loss in iteration 99 : 0.5216355299306176
Loss in iteration 100 : 0.5214434076785444
Loss in iteration 101 : 0.521254363484557
Loss in iteration 102 : 0.5210693051745889
Loss in iteration 103 : 0.5208884032482546
Loss in iteration 104 : 0.5207100851866898
Loss in iteration 105 : 0.520535066977616
Loss in iteration 106 : 0.5203655347026247
Loss in iteration 107 : 0.5201995976689778
Loss in iteration 108 : 0.5200351522315316
Loss in iteration 109 : 0.5198725908309542
Loss in iteration 110 : 0.519713524528787
Loss in iteration 111 : 0.5195576767671828
Loss in iteration 112 : 0.5194047728182305
Loss in iteration 113 : 0.5192561923068355
Loss in iteration 114 : 0.5191121767453868
Loss in iteration 115 : 0.5189705061456368
Loss in iteration 116 : 0.5188310700430464
Loss in iteration 117 : 0.5186939508333128
Loss in iteration 118 : 0.5185600693019163
Loss in iteration 119 : 0.5184281973881278
Loss in iteration 120 : 0.5182979759741679
Loss in iteration 121 : 0.5181693817097262
Loss in iteration 122 : 0.518042839844655
Loss in iteration 123 : 0.5179183820907126
Loss in iteration 124 : 0.5177960606141807
Loss in iteration 125 : 0.5176751513033071
Loss in iteration 126 : 0.517557137408776
Loss in iteration 127 : 0.5174414252838789
Loss in iteration 128 : 0.5173278504586244
Loss in iteration 129 : 0.5172161886912016
Loss in iteration 130 : 0.5171069618916321
Loss in iteration 131 : 0.5169999570611663
Loss in iteration 132 : 0.5168942466936187
Loss in iteration 133 : 0.5167896538804827
Loss in iteration 134 : 0.5166861401645635
Loss in iteration 135 : 0.5165836761809497
Loss in iteration 136 : 0.5164824845355742
Loss in iteration 137 : 0.5163839523036449
Loss in iteration 138 : 0.516287939338072
Loss in iteration 139 : 0.5161927231662206
Loss in iteration 140 : 0.5160988793733743
Loss in iteration 141 : 0.5160056732287387
Loss in iteration 142 : 0.5159136211979986
Loss in iteration 143 : 0.5158227828311124
Loss in iteration 144 : 0.5157344312144888
Loss in iteration 145 : 0.5156477824656407
Loss in iteration 146 : 0.5155619661987143
Loss in iteration 147 : 0.5154779885952999
Loss in iteration 148 : 0.5153964819044159
Loss in iteration 149 : 0.5153159726926685
Loss in iteration 150 : 0.5152356348590116
Loss in iteration 151 : 0.5151559603787077
Loss in iteration 152 : 0.5150778257408422
Loss in iteration 153 : 0.5150001967883638
Loss in iteration 154 : 0.5149239265443828
Loss in iteration 155 : 0.5148484019259143
Loss in iteration 156 : 0.5147741783197873
Loss in iteration 157 : 0.5147007454841599
Loss in iteration 158 : 0.514628161989204
Loss in iteration 159 : 0.5145564276529988
Loss in iteration 160 : 0.5144856819970255
Loss in iteration 161 : 0.5144158345126814
Loss in iteration 162 : 0.5143471237321974
Loss in iteration 163 : 0.5142789751922554
Loss in iteration 164 : 0.5142115454116813
Loss in iteration 165 : 0.5141455260782453
Loss in iteration 166 : 0.514081692400727
Loss in iteration 167 : 0.514018310539465
Loss in iteration 168 : 0.5139558157379752
Loss in iteration 169 : 0.513894386745566
Loss in iteration 170 : 0.5138337782539925
Loss in iteration 171 : 0.5137736462965754
Loss in iteration 172 : 0.5137140570948583
Loss in iteration 173 : 0.513655001387828
Loss in iteration 174 : 0.513596579029401
Loss in iteration 175 : 0.5135387902212601
Loss in iteration 176 : 0.5134814792398993
Loss in iteration 177 : 0.5134244590375584
Loss in iteration 178 : 0.5133689505161897
Loss in iteration 179 : 0.5133149739173207
Loss in iteration 180 : 0.5132621631569486
Loss in iteration 181 : 0.5132100745302212
Loss in iteration 182 : 0.5131584633476175
Loss in iteration 183 : 0.5131074384522719
Loss in iteration 184 : 0.5130572468980881
Loss in iteration 185 : 0.5130075682129507
Loss in iteration 186 : 0.5129583716129241
Loss in iteration 187 : 0.5129093536929125
Loss in iteration 188 : 0.5128608139917349
Loss in iteration 189 : 0.5128129769697202
Loss in iteration 190 : 0.5127653990780838
Loss in iteration 191 : 0.5127180394845966
Loss in iteration 192 : 0.5126710952318334
Loss in iteration 193 : 0.5126246994684012
Loss in iteration 194 : 0.512578577836619
Loss in iteration 195 : 0.5125327159665232
Loss in iteration 196 : 0.5124873290574828
Loss in iteration 197 : 0.5124426848707491
Loss in iteration 198 : 0.5123982987399751
Loss in iteration 199 : 0.512354361747735
Loss in iteration 200 : 0.5123108739454758
Testing accuracy  of updater 7 on alg 1 with rate 0.19999999999999996 = 0.784, training accuracy 0.787375, time elapsed: 4111 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 5.3170805033718604
Loss in iteration 3 : 4.402201853525879
Loss in iteration 4 : 1.7198491378638427
Loss in iteration 5 : 1.2885613304599712
Loss in iteration 6 : 0.9719300710097416
Loss in iteration 7 : 0.883408479348004
Loss in iteration 8 : 0.9601726007635631
Loss in iteration 9 : 0.9941139015206367
Loss in iteration 10 : 0.9789862838117368
Loss in iteration 11 : 0.9551068136753113
Loss in iteration 12 : 0.9818275955242
Loss in iteration 13 : 1.010010960535963
Loss in iteration 14 : 1.1280754358561564
Loss in iteration 15 : 0.9887421333369937
Loss in iteration 16 : 1.0101170442287226
Loss in iteration 17 : 0.9054539031879392
Loss in iteration 18 : 0.8894371009770764
Loss in iteration 19 : 0.8502822507155735
Loss in iteration 20 : 0.8538981909709241
Loss in iteration 21 : 0.8326321592882298
Loss in iteration 22 : 0.859819653582401
Loss in iteration 23 : 0.8429069709717695
Loss in iteration 24 : 0.88737912635849
Loss in iteration 25 : 0.8684708688705743
Loss in iteration 26 : 0.91768589839539
Loss in iteration 27 : 0.9031681989710821
Loss in iteration 28 : 0.9554580641854072
Loss in iteration 29 : 0.8851154082615712
Loss in iteration 30 : 0.9285139127424781
Loss in iteration 31 : 0.8754802109546976
Loss in iteration 32 : 0.9072765667462894
Loss in iteration 33 : 0.8682240647011797
Loss in iteration 34 : 0.9085874518381056
Loss in iteration 35 : 0.8683976468924531
Loss in iteration 36 : 0.9024840892811835
Loss in iteration 37 : 0.8723344936789634
Loss in iteration 38 : 0.9031257322734545
Loss in iteration 39 : 0.8599191106433084
Loss in iteration 40 : 0.8866847655311638
Loss in iteration 41 : 0.8447364603668971
Loss in iteration 42 : 0.8698669331266896
Loss in iteration 43 : 0.8272258983704694
Loss in iteration 44 : 0.8499887957556452
Loss in iteration 45 : 0.8169664336504481
Loss in iteration 46 : 0.8340543739506197
Loss in iteration 47 : 0.8012072206852754
Loss in iteration 48 : 0.8305549655514494
Loss in iteration 49 : 0.7936699468133915
Loss in iteration 50 : 0.8488620296051533
Loss in iteration 51 : 0.751248488553371
Loss in iteration 52 : 0.8046961199815446
Loss in iteration 53 : 0.714527299426127
Loss in iteration 54 : 0.7604183361927362
Loss in iteration 55 : 0.7261337255855558
Loss in iteration 56 : 0.7942959359070909
Loss in iteration 57 : 0.7848140278930327
Loss in iteration 58 : 0.8897585042237136
Loss in iteration 59 : 0.923816057209312
Loss in iteration 60 : 0.9507152818053897
Loss in iteration 61 : 0.8965023752713637
Loss in iteration 62 : 0.896029081846199
Loss in iteration 63 : 0.8492593165557993
Loss in iteration 64 : 0.8552551624336587
Loss in iteration 65 : 0.8193866588253391
Loss in iteration 66 : 0.8336315899598451
Loss in iteration 67 : 0.8111585665840272
Loss in iteration 68 : 0.8126576253781553
Loss in iteration 69 : 0.7969740898202512
Loss in iteration 70 : 0.8200977174890842
Loss in iteration 71 : 0.7997294929532318
Loss in iteration 72 : 0.8213935400511412
Loss in iteration 73 : 0.7937843898103094
Loss in iteration 74 : 0.8230565182011611
Loss in iteration 75 : 0.8043822978151912
Loss in iteration 76 : 0.8283846689967364
Loss in iteration 77 : 0.7967618369661835
Loss in iteration 78 : 0.8290118519257047
Loss in iteration 79 : 0.811486155179894
Loss in iteration 80 : 0.840759168018204
Loss in iteration 81 : 0.8015983652770549
Loss in iteration 82 : 0.8290130712745295
Loss in iteration 83 : 0.7985643584505969
Loss in iteration 84 : 0.8195309753208162
Loss in iteration 85 : 0.7900212650646553
Loss in iteration 86 : 0.8246964046319247
Loss in iteration 87 : 0.7959047573945168
Loss in iteration 88 : 0.8141556861431185
Loss in iteration 89 : 0.784628082760894
Loss in iteration 90 : 0.8219587705709644
Loss in iteration 91 : 0.801934990888481
Loss in iteration 92 : 0.8221075854811363
Loss in iteration 93 : 0.7973076287647126
Loss in iteration 94 : 0.8339990729621032
Loss in iteration 95 : 0.8085943419996727
Loss in iteration 96 : 0.8302522764913048
Loss in iteration 97 : 0.798513385096165
Loss in iteration 98 : 0.8301954515829311
Loss in iteration 99 : 0.8086832506489204
Loss in iteration 100 : 0.8264514362900608
Loss in iteration 101 : 0.7977585944982534
Loss in iteration 102 : 0.8258730791648636
Loss in iteration 103 : 0.7991480999775223
Loss in iteration 104 : 0.819985875460827
Loss in iteration 105 : 0.7919288736356201
Loss in iteration 106 : 0.8195144226859149
Loss in iteration 107 : 0.7913131721065834
Loss in iteration 108 : 0.8167105440301643
Loss in iteration 109 : 0.7926523205789953
Loss in iteration 110 : 0.8192164972998608
Loss in iteration 111 : 0.7898464074440569
Loss in iteration 112 : 0.818604904100283
Loss in iteration 113 : 0.8054952526794368
Loss in iteration 114 : 0.8359996776588379
Loss in iteration 115 : 0.8037451172949438
Loss in iteration 116 : 0.8269243538397145
Loss in iteration 117 : 0.8036668959518944
Loss in iteration 118 : 0.824470796973127
Loss in iteration 119 : 0.7914250575494605
Loss in iteration 120 : 0.8117912166785505
Loss in iteration 121 : 0.791072621675728
Loss in iteration 122 : 0.8140986743302501
Loss in iteration 123 : 0.7811291442096173
Loss in iteration 124 : 0.8092212902069867
Loss in iteration 125 : 0.7864027943833748
Loss in iteration 126 : 0.811409386467685
Loss in iteration 127 : 0.7826239468193359
Loss in iteration 128 : 0.8123991423815627
Loss in iteration 129 : 0.7938625692135927
Loss in iteration 130 : 0.828049909792244
Loss in iteration 131 : 0.8000611576781671
Loss in iteration 132 : 0.8258363740948165
Loss in iteration 133 : 0.803152080812088
Loss in iteration 134 : 0.8246583842222888
Loss in iteration 135 : 0.787082835114502
Loss in iteration 136 : 0.8080119259798909
Loss in iteration 137 : 0.779491310248751
Loss in iteration 138 : 0.8082267953141817
Loss in iteration 139 : 0.7757445683441656
Loss in iteration 140 : 0.8040777643566532
Loss in iteration 141 : 0.7719172774995354
Loss in iteration 142 : 0.8073383516776376
Loss in iteration 143 : 0.784154790939627
Loss in iteration 144 : 0.822270244945289
Loss in iteration 145 : 0.7868609582991191
Loss in iteration 146 : 0.8286665037080357
Loss in iteration 147 : 0.7979576104033594
Loss in iteration 148 : 0.8337248807236137
Loss in iteration 149 : 0.795038995983
Loss in iteration 150 : 0.8283589108748081
Loss in iteration 151 : 0.7955656658824619
Loss in iteration 152 : 0.8267407901350556
Loss in iteration 153 : 0.7921165930346526
Loss in iteration 154 : 0.8164761527903919
Loss in iteration 155 : 0.7786846683532045
Loss in iteration 156 : 0.7982231659557124
Loss in iteration 157 : 0.7792101816036877
Loss in iteration 158 : 0.8098061663923077
Loss in iteration 159 : 0.7739497392117051
Loss in iteration 160 : 0.8056213331435204
Loss in iteration 161 : 0.7810804236438712
Loss in iteration 162 : 0.8154889106291541
Loss in iteration 163 : 0.7818198059156694
Loss in iteration 164 : 0.8194376894122549
Loss in iteration 165 : 0.7810372392637261
Loss in iteration 166 : 0.8308089452885381
Loss in iteration 167 : 0.7624411873531259
Loss in iteration 168 : 0.8531777079577431
Loss in iteration 169 : 0.7582943495209115
Loss in iteration 170 : 0.8412759332652769
Loss in iteration 171 : 0.7482427078467184
Loss in iteration 172 : 0.7120268478435527
Loss in iteration 173 : 0.7031135601663083
Loss in iteration 174 : 0.6927098876018437
Loss in iteration 175 : 0.6866582996351903
Loss in iteration 176 : 0.6944947556184663
Loss in iteration 177 : 0.65585986307163
Loss in iteration 178 : 0.6630872490769326
Loss in iteration 179 : 0.6372870632520822
Loss in iteration 180 : 0.6728226519181975
Loss in iteration 181 : 0.6855627017800068
Loss in iteration 182 : 0.844350416951102
Loss in iteration 183 : 0.9904156862656487
Loss in iteration 184 : 1.1175359089557162
Loss in iteration 185 : 1.0102130318237144
Loss in iteration 186 : 0.9848270272860663
Loss in iteration 187 : 0.9453585415462972
Loss in iteration 188 : 0.945732592554658
Loss in iteration 189 : 0.8880121677920172
Loss in iteration 190 : 0.8734514455640006
Loss in iteration 191 : 0.8258651437300852
Loss in iteration 192 : 0.8136433759742717
Loss in iteration 193 : 0.7945675720877291
Loss in iteration 194 : 0.800168146317649
Loss in iteration 195 : 0.77300335730399
Loss in iteration 196 : 0.7877381112643018
Loss in iteration 197 : 0.7773594719720648
Loss in iteration 198 : 0.7982688331856231
Loss in iteration 199 : 0.7715614575317227
Loss in iteration 200 : 0.7985127540119715
Testing accuracy  of updater 8 on alg 1 with rate 2.0 = 0.7215, training accuracy 0.7295, time elapsed: 3992 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9451210358102389
Loss in iteration 3 : 0.9204396662139925
Loss in iteration 4 : 0.9102754012001932
Loss in iteration 5 : 0.8808486657000695
Loss in iteration 6 : 0.8528258498356892
Loss in iteration 7 : 0.8246866812856254
Loss in iteration 8 : 0.7950847637718218
Loss in iteration 9 : 0.7641596023327395
Loss in iteration 10 : 0.7350210273890202
Loss in iteration 11 : 0.7061802653615202
Loss in iteration 12 : 0.6769576822394369
Loss in iteration 13 : 0.6515542428611167
Loss in iteration 14 : 0.6318035777913176
Loss in iteration 15 : 0.6149463887110366
Loss in iteration 16 : 0.6012287577735829
Loss in iteration 17 : 0.5904990056414381
Loss in iteration 18 : 0.5820924694369175
Loss in iteration 19 : 0.5752532532687139
Loss in iteration 20 : 0.5694589935889112
Loss in iteration 21 : 0.5645967901763054
Loss in iteration 22 : 0.5605030400838557
Loss in iteration 23 : 0.5570409642022925
Loss in iteration 24 : 0.5541392245173951
Loss in iteration 25 : 0.5516855473497718
Loss in iteration 26 : 0.5495390164840513
Loss in iteration 27 : 0.5476649908204732
Loss in iteration 28 : 0.5459771166840021
Loss in iteration 29 : 0.5444376884512262
Loss in iteration 30 : 0.5430047098237144
Loss in iteration 31 : 0.5416837131480372
Loss in iteration 32 : 0.5404890936946866
Loss in iteration 33 : 0.5393967581385343
Loss in iteration 34 : 0.5383695336875105
Loss in iteration 35 : 0.5374083413450638
Loss in iteration 36 : 0.5365224282053459
Loss in iteration 37 : 0.5356765746342194
Loss in iteration 38 : 0.5348611864940955
Loss in iteration 39 : 0.5340698598626498
Loss in iteration 40 : 0.5333066159975145
Loss in iteration 41 : 0.5325687044471962
Loss in iteration 42 : 0.5318499708475123
Loss in iteration 43 : 0.5311522969118161
Loss in iteration 44 : 0.5304753309314415
Loss in iteration 45 : 0.5298165905016737
Loss in iteration 46 : 0.5291751596475067
Loss in iteration 47 : 0.5285440225385624
Loss in iteration 48 : 0.5279277288992489
Loss in iteration 49 : 0.5273369230709323
Loss in iteration 50 : 0.5267622732394462
Loss in iteration 51 : 0.5262108510710761
Loss in iteration 52 : 0.5256726484942801
Loss in iteration 53 : 0.5251568564568363
Loss in iteration 54 : 0.524661278621646
Loss in iteration 55 : 0.5241821336229632
Loss in iteration 56 : 0.5237257088662511
Loss in iteration 57 : 0.5232873002009266
Loss in iteration 58 : 0.5228715185907116
Loss in iteration 59 : 0.5224728637177567
Loss in iteration 60 : 0.5220893290045173
Loss in iteration 61 : 0.5217188136514009
Loss in iteration 62 : 0.5213592602649049
Loss in iteration 63 : 0.5210074239225456
Loss in iteration 64 : 0.5206672294455225
Loss in iteration 65 : 0.5203368922106647
Loss in iteration 66 : 0.5200145342553213
Loss in iteration 67 : 0.5197042698267562
Loss in iteration 68 : 0.519403868421851
Loss in iteration 69 : 0.5191187669911961
Loss in iteration 70 : 0.5188480978017679
Loss in iteration 71 : 0.5185918644063547
Loss in iteration 72 : 0.5183501085226839
Loss in iteration 73 : 0.5181151626642447
Loss in iteration 74 : 0.5178853013739152
Loss in iteration 75 : 0.5176622650798804
Loss in iteration 76 : 0.5174507854621443
Loss in iteration 77 : 0.5172460438822465
Loss in iteration 78 : 0.5170462100912491
Loss in iteration 79 : 0.5168504703662531
Loss in iteration 80 : 0.5166638356643375
Loss in iteration 81 : 0.5164814625841331
Loss in iteration 82 : 0.516306786122731
Loss in iteration 83 : 0.5161364658335061
Loss in iteration 84 : 0.5159728308234982
Loss in iteration 85 : 0.5158112676727628
Loss in iteration 86 : 0.5156554981652027
Loss in iteration 87 : 0.5155035226022621
Loss in iteration 88 : 0.5153552513855192
Loss in iteration 89 : 0.515212056024518
Loss in iteration 90 : 0.5150712326445266
Loss in iteration 91 : 0.5149340284064391
Loss in iteration 92 : 0.5148023668885963
Loss in iteration 93 : 0.5146739309957017
Loss in iteration 94 : 0.5145482538087202
Loss in iteration 95 : 0.5144252917594153
Loss in iteration 96 : 0.5143058086319404
Loss in iteration 97 : 0.5141897807358358
Loss in iteration 98 : 0.5140765785069387
Loss in iteration 99 : 0.5139657417188255
Loss in iteration 100 : 0.5138569045500759
Loss in iteration 101 : 0.5137500248111346
Loss in iteration 102 : 0.513645575420145
Loss in iteration 103 : 0.5135430795366723
Loss in iteration 104 : 0.5134449376989803
Loss in iteration 105 : 0.5133463852161668
Loss in iteration 106 : 0.5132510185131344
Loss in iteration 107 : 0.5131574581307666
Loss in iteration 108 : 0.5130646710817846
Loss in iteration 109 : 0.5129733266961294
Loss in iteration 110 : 0.5128838217087555
Loss in iteration 111 : 0.512795168331646
Loss in iteration 112 : 0.512708223771835
Loss in iteration 113 : 0.512623164402656
Loss in iteration 114 : 0.5125391220140796
Loss in iteration 115 : 0.5124570538244227
Loss in iteration 116 : 0.5123772727155619
Loss in iteration 117 : 0.5122995418914102
Loss in iteration 118 : 0.5122230675412007
Loss in iteration 119 : 0.5121479757314694
Loss in iteration 120 : 0.5120750227999353
Loss in iteration 121 : 0.5120031909392425
Loss in iteration 122 : 0.5119330566788987
Loss in iteration 123 : 0.5118652009576319
Loss in iteration 124 : 0.511798692837813
Loss in iteration 125 : 0.511734485059072
Loss in iteration 126 : 0.5116720548913334
Loss in iteration 127 : 0.5116116771461485
Loss in iteration 128 : 0.5115522159938111
Loss in iteration 129 : 0.5114934717084944
Loss in iteration 130 : 0.5114368791777978
Loss in iteration 131 : 0.5113809192166475
Loss in iteration 132 : 0.5113258732980911
Loss in iteration 133 : 0.5112720319819694
Loss in iteration 134 : 0.5112183433750809
Loss in iteration 135 : 0.5111673385453284
Loss in iteration 136 : 0.5111171158023077
Loss in iteration 137 : 0.5110682614115845
Loss in iteration 138 : 0.5110196324599199
Loss in iteration 139 : 0.510972558384122
Loss in iteration 140 : 0.5109259195410478
Loss in iteration 141 : 0.5108808301671997
Loss in iteration 142 : 0.5108362979529395
Loss in iteration 143 : 0.5107934572011006
Loss in iteration 144 : 0.5107506291488888
Loss in iteration 145 : 0.5107089970594253
Loss in iteration 146 : 0.5106684959225192
Loss in iteration 147 : 0.5106284130369708
Loss in iteration 148 : 0.5105890785826408
Loss in iteration 149 : 0.5105516974618143
Loss in iteration 150 : 0.5105151921985324
Loss in iteration 151 : 0.510479103012562
Loss in iteration 152 : 0.5104443571578309
Loss in iteration 153 : 0.5104110815207863
Loss in iteration 154 : 0.5103795218229831
Loss in iteration 155 : 0.5103472494923091
Loss in iteration 156 : 0.5103157558981699
Loss in iteration 157 : 0.5102842979595452
Loss in iteration 158 : 0.5102539007873376
Loss in iteration 159 : 0.510222754250702
Loss in iteration 160 : 0.5101934475420091
Loss in iteration 161 : 0.5101638928297809
Loss in iteration 162 : 0.5101356081863567
Loss in iteration 163 : 0.5101074691845273
Loss in iteration 164 : 0.5100794854108384
Loss in iteration 165 : 0.5100526721034095
Loss in iteration 166 : 0.5100254876921475
Loss in iteration 167 : 0.5099993511322477
Loss in iteration 168 : 0.5099736514736206
Loss in iteration 169 : 0.5099481840437066
Loss in iteration 170 : 0.5099232681820045
Loss in iteration 171 : 0.5098987202520292
Loss in iteration 172 : 0.5098745437346159
Loss in iteration 173 : 0.5098503968139987
Loss in iteration 174 : 0.5098262310905018
Loss in iteration 175 : 0.5098024376617597
Loss in iteration 176 : 0.5097797067289604
Loss in iteration 177 : 0.5097556542250641
Loss in iteration 178 : 0.5097323300973389
Loss in iteration 179 : 0.5097090120147755
Loss in iteration 180 : 0.5096871208723688
Loss in iteration 181 : 0.5096646935584662
Loss in iteration 182 : 0.5096427356326031
Loss in iteration 183 : 0.5096210071946227
Loss in iteration 184 : 0.5095996915967135
Loss in iteration 185 : 0.5095791918821929
Loss in iteration 186 : 0.5095595744898376
Loss in iteration 187 : 0.5095369746562014
Loss in iteration 188 : 0.5095171477848205
Loss in iteration 189 : 0.5094973661630003
Loss in iteration 190 : 0.5094773711385284
Loss in iteration 191 : 0.509457101163168
Loss in iteration 192 : 0.5094376492685541
Loss in iteration 193 : 0.509419281651631
Loss in iteration 194 : 0.5094014191420679
Loss in iteration 195 : 0.5093835809928103
Loss in iteration 196 : 0.5093653758524599
Loss in iteration 197 : 0.5093475940078839
Loss in iteration 198 : 0.5093300800182212
Loss in iteration 199 : 0.5093132726665501
Loss in iteration 200 : 0.5092962530324434
Testing accuracy  of updater 8 on alg 1 with rate 0.2 = 0.781, training accuracy 0.787875, time elapsed: 4113 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9929225366571984
Loss in iteration 3 : 0.9828816428248183
Loss in iteration 4 : 0.9702422711403218
Loss in iteration 5 : 0.9557685486491985
Loss in iteration 6 : 0.9410206716179182
Loss in iteration 7 : 0.9278437778454891
Loss in iteration 8 : 0.917774097914064
Loss in iteration 9 : 0.911137176236134
Loss in iteration 10 : 0.9068294208896955
Loss in iteration 11 : 0.9034489321153111
Loss in iteration 12 : 0.8998448952704582
Loss in iteration 13 : 0.8951708894361379
Loss in iteration 14 : 0.8891798421959151
Loss in iteration 15 : 0.882122480836161
Loss in iteration 16 : 0.8744209759889104
Loss in iteration 17 : 0.8664948021466155
Loss in iteration 18 : 0.8586754073949218
Loss in iteration 19 : 0.8511460623910325
Loss in iteration 20 : 0.8438452304394571
Loss in iteration 21 : 0.8367073388957041
Loss in iteration 22 : 0.8297333403212853
Loss in iteration 23 : 0.8229022068422303
Loss in iteration 24 : 0.8160974994456074
Loss in iteration 25 : 0.8092536756061135
Loss in iteration 26 : 0.802348651164807
Loss in iteration 27 : 0.7953824666109547
Loss in iteration 28 : 0.7883784479695908
Loss in iteration 29 : 0.7813592390978604
Loss in iteration 30 : 0.7743596270383127
Loss in iteration 31 : 0.767431051348106
Loss in iteration 32 : 0.7606690300167981
Loss in iteration 33 : 0.7540824313769835
Loss in iteration 34 : 0.747636118852511
Loss in iteration 35 : 0.7412589337212863
Loss in iteration 36 : 0.7349570408530364
Loss in iteration 37 : 0.7287044540779568
Loss in iteration 38 : 0.722489969461398
Loss in iteration 39 : 0.7163071102298677
Loss in iteration 40 : 0.7101959568880519
Loss in iteration 41 : 0.7041626887876737
Loss in iteration 42 : 0.6982281420307579
Loss in iteration 43 : 0.6924096316808582
Loss in iteration 44 : 0.6867635687482645
Loss in iteration 45 : 0.6813030731595525
Loss in iteration 46 : 0.6760299781519179
Loss in iteration 47 : 0.6710079749964029
Loss in iteration 48 : 0.6662407016877396
Loss in iteration 49 : 0.6616909619935984
Loss in iteration 50 : 0.6573966407205598
Loss in iteration 51 : 0.6533348789892935
Loss in iteration 52 : 0.6494327310287614
Loss in iteration 53 : 0.6456879409255184
Loss in iteration 54 : 0.6421207200306424
Loss in iteration 55 : 0.6386830570749462
Loss in iteration 56 : 0.635382107181925
Loss in iteration 57 : 0.632223216396098
Loss in iteration 58 : 0.6292443717515648
Loss in iteration 59 : 0.6264331678264323
Loss in iteration 60 : 0.6237866160815099
Loss in iteration 61 : 0.6213122674229422
Loss in iteration 62 : 0.6189718759240433
Loss in iteration 63 : 0.6167657848264744
Loss in iteration 64 : 0.6146635550337097
Loss in iteration 65 : 0.6126336262125766
Loss in iteration 66 : 0.610685993177209
Loss in iteration 67 : 0.6088081706299936
Loss in iteration 68 : 0.6070119636365989
Loss in iteration 69 : 0.6052905895406913
Loss in iteration 70 : 0.6036458481785314
Loss in iteration 71 : 0.6020665618555525
Loss in iteration 72 : 0.6005416744816611
Loss in iteration 73 : 0.5990818269698762
Loss in iteration 74 : 0.5976773714272501
Loss in iteration 75 : 0.5963189340216752
Loss in iteration 76 : 0.5950025594890254
Loss in iteration 77 : 0.5937217815974083
Loss in iteration 78 : 0.5924888510560189
Loss in iteration 79 : 0.5912972889394243
Loss in iteration 80 : 0.5901434070953939
Loss in iteration 81 : 0.5890288018303997
Loss in iteration 82 : 0.5879547203343205
Loss in iteration 83 : 0.5869128938795392
Loss in iteration 84 : 0.5859008122608007
Loss in iteration 85 : 0.5849187024891651
Loss in iteration 86 : 0.5839676034426988
Loss in iteration 87 : 0.5830388960498384
Loss in iteration 88 : 0.582133861391967
Loss in iteration 89 : 0.5812546339634342
Loss in iteration 90 : 0.5803955327056337
Loss in iteration 91 : 0.5795572225463492
Loss in iteration 92 : 0.578739799569948
Loss in iteration 93 : 0.5779423638884349
Loss in iteration 94 : 0.5771625079973246
Loss in iteration 95 : 0.5763996725678624
Loss in iteration 96 : 0.5756554273633931
Loss in iteration 97 : 0.5749340833630968
Loss in iteration 98 : 0.5742301609361422
Loss in iteration 99 : 0.5735453522194309
Loss in iteration 100 : 0.5728749718927085
Loss in iteration 101 : 0.5722156415988005
Loss in iteration 102 : 0.5715689362792773
Loss in iteration 103 : 0.5709335044686146
Loss in iteration 104 : 0.5703126815331193
Loss in iteration 105 : 0.5697062690885683
Loss in iteration 106 : 0.5691151347888894
Loss in iteration 107 : 0.5685348469961276
Loss in iteration 108 : 0.5679672124075167
Loss in iteration 109 : 0.5674138327147191
Loss in iteration 110 : 0.5668696671769828
Loss in iteration 111 : 0.5663352637342199
Loss in iteration 112 : 0.5658119963587528
Loss in iteration 113 : 0.5652982180004067
Loss in iteration 114 : 0.564793571698531
Loss in iteration 115 : 0.5642952629940635
Loss in iteration 116 : 0.5638072698650174
Loss in iteration 117 : 0.5633299399711231
Loss in iteration 118 : 0.5628640643278114
Loss in iteration 119 : 0.5624067709284101
Loss in iteration 120 : 0.5619591514126656
Loss in iteration 121 : 0.561516946694607
Loss in iteration 122 : 0.5610790560678262
Loss in iteration 123 : 0.5606502370813694
Loss in iteration 124 : 0.5602293771229067
Loss in iteration 125 : 0.5598148636458689
Loss in iteration 126 : 0.5594060137594468
Loss in iteration 127 : 0.559005281154978
Loss in iteration 128 : 0.5586090047244315
Loss in iteration 129 : 0.5582175323111229
Loss in iteration 130 : 0.5578323589276338
Loss in iteration 131 : 0.5574529391184577
Loss in iteration 132 : 0.5570801326842809
Loss in iteration 133 : 0.5567130484225096
Loss in iteration 134 : 0.5563527732291597
Loss in iteration 135 : 0.5560002344494678
Loss in iteration 136 : 0.5556526469845349
Loss in iteration 137 : 0.5553083870079304
Loss in iteration 138 : 0.5549688048161664
Loss in iteration 139 : 0.5546344939882237
Loss in iteration 140 : 0.5543046533372278
Loss in iteration 141 : 0.5539800747766274
Loss in iteration 142 : 0.5536603247605867
Loss in iteration 143 : 0.5533439367644192
Loss in iteration 144 : 0.5530317264771932
Loss in iteration 145 : 0.5527263719291036
Loss in iteration 146 : 0.5524254846972123
Loss in iteration 147 : 0.5521296331209578
Loss in iteration 148 : 0.5518379199774299
Loss in iteration 149 : 0.5515500111629097
Loss in iteration 150 : 0.5512667644413608
Loss in iteration 151 : 0.5509870411313772
Loss in iteration 152 : 0.5507103845458778
Loss in iteration 153 : 0.5504371231624232
Loss in iteration 154 : 0.550165504135662
Loss in iteration 155 : 0.5498960103661492
Loss in iteration 156 : 0.5496292085675637
Loss in iteration 157 : 0.5493654885460014
Loss in iteration 158 : 0.5491052354945402
Loss in iteration 159 : 0.5488475590275705
Loss in iteration 160 : 0.5485922268198679
Loss in iteration 161 : 0.5483398351136965
Loss in iteration 162 : 0.5480900947371208
Loss in iteration 163 : 0.5478435518917513
Loss in iteration 164 : 0.5475986970924928
Loss in iteration 165 : 0.5473554878051918
Loss in iteration 166 : 0.5471134710942297
Loss in iteration 167 : 0.5468743642500603
Loss in iteration 168 : 0.5466384442206188
Loss in iteration 169 : 0.5464043600916354
Loss in iteration 170 : 0.546172920513623
Loss in iteration 171 : 0.5459438069769705
Loss in iteration 172 : 0.5457164869717746
Loss in iteration 173 : 0.545491163080603
Loss in iteration 174 : 0.5452677772697245
Loss in iteration 175 : 0.5450459020419091
Loss in iteration 176 : 0.5448262885496337
Loss in iteration 177 : 0.5446077608419242
Loss in iteration 178 : 0.5443909148483204
Loss in iteration 179 : 0.5441775736826407
Loss in iteration 180 : 0.5439667167621635
Loss in iteration 181 : 0.543757144486777
Loss in iteration 182 : 0.5435496350166998
Loss in iteration 183 : 0.5433440995774889
Loss in iteration 184 : 0.5431408575017938
Loss in iteration 185 : 0.5429388578606386
Loss in iteration 186 : 0.5427377620100117
Loss in iteration 187 : 0.5425379106324492
Loss in iteration 188 : 0.542340171390875
Loss in iteration 189 : 0.5421442972239946
Loss in iteration 190 : 0.5419505097916001
Loss in iteration 191 : 0.5417589999958171
Loss in iteration 192 : 0.5415696082175745
Loss in iteration 193 : 0.5413819361716892
Loss in iteration 194 : 0.5411964666332139
Loss in iteration 195 : 0.5410136587550094
Loss in iteration 196 : 0.540832442453902
Loss in iteration 197 : 0.5406525673160649
Loss in iteration 198 : 0.5404736472491347
Loss in iteration 199 : 0.5402955218578636
Loss in iteration 200 : 0.5401190576266219
Testing accuracy  of updater 8 on alg 1 with rate 0.01999999999999999 = 0.7715, training accuracy 0.77425, time elapsed: 5958 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 13.047744702165843
Loss in iteration 3 : 10.2165367737255
Loss in iteration 4 : 6.486844360594467
Loss in iteration 5 : 7.671989240130641
Loss in iteration 6 : 4.320416735523655
Loss in iteration 7 : 6.255170096458656
Loss in iteration 8 : 6.1999876793650435
Loss in iteration 9 : 4.239830259317616
Loss in iteration 10 : 6.32403698070377
Loss in iteration 11 : 3.9984511808048557
Loss in iteration 12 : 3.8857363329858416
Loss in iteration 13 : 4.965166516276352
Loss in iteration 14 : 3.522618090273561
Loss in iteration 15 : 4.1974158083263875
Loss in iteration 16 : 4.1164348960709045
Loss in iteration 17 : 3.253308900278452
Loss in iteration 18 : 4.066106106371877
Loss in iteration 19 : 3.7307224031030413
Loss in iteration 20 : 3.0849693553754634
Loss in iteration 21 : 3.37576667684313
Loss in iteration 22 : 2.8246755029094737
Loss in iteration 23 : 2.9882170593981505
Loss in iteration 24 : 3.257030203975567
Loss in iteration 25 : 2.78250861413655
Loss in iteration 26 : 2.7970030333111056
Loss in iteration 27 : 2.3644185916102796
Loss in iteration 28 : 2.2088938308582775
Loss in iteration 29 : 2.3891679739502822
Loss in iteration 30 : 2.3259718153163553
Loss in iteration 31 : 2.2205546752554786
Loss in iteration 32 : 1.9057445661228105
Loss in iteration 33 : 1.860434486797731
Loss in iteration 34 : 1.8593102477276862
Loss in iteration 35 : 1.8781288002645324
Loss in iteration 36 : 1.7128002769930768
Loss in iteration 37 : 1.6298570302826152
Loss in iteration 38 : 1.5948465238542449
Loss in iteration 39 : 1.5853170519233244
Loss in iteration 40 : 1.5557943180752942
Loss in iteration 41 : 1.4386068091265176
Loss in iteration 42 : 1.439281925645949
Loss in iteration 43 : 1.464557682049356
Loss in iteration 44 : 1.368403174997089
Loss in iteration 45 : 1.2792734505394288
Loss in iteration 46 : 1.268884010272789
Loss in iteration 47 : 1.3238129931114604
Loss in iteration 48 : 1.436438327683653
Loss in iteration 49 : 1.7562121069424752
Loss in iteration 50 : 2.13084702295937
Loss in iteration 51 : 1.5670263263978639
Loss in iteration 52 : 1.1901650744451557
Loss in iteration 53 : 1.1704749847738254
Loss in iteration 54 : 1.7927621667204718
Loss in iteration 55 : 1.6055172781553906
Loss in iteration 56 : 1.177791873725588
Loss in iteration 57 : 1.0799764840578567
Loss in iteration 58 : 1.509972513871719
Loss in iteration 59 : 1.520269396038982
Loss in iteration 60 : 1.1309928454984943
Loss in iteration 61 : 0.9185439435976869
Loss in iteration 62 : 1.1920664761842423
Loss in iteration 63 : 1.4698941512311299
Loss in iteration 64 : 1.330998239979522
Loss in iteration 65 : 0.9732056242103226
Loss in iteration 66 : 1.0135806355046606
Loss in iteration 67 : 1.1826674186340862
Loss in iteration 68 : 1.5660124244094482
Loss in iteration 69 : 1.2858803530507135
Loss in iteration 70 : 0.964354940808195
Loss in iteration 71 : 0.9382117566153002
Loss in iteration 72 : 0.8484046911365276
Loss in iteration 73 : 0.8914149571067654
Loss in iteration 74 : 0.7664403417910678
Loss in iteration 75 : 1.2032621193207687
Loss in iteration 76 : 2.3513822360444703
Loss in iteration 77 : 2.571565709912602
Loss in iteration 78 : 0.9993505299868854
Loss in iteration 79 : 2.2273722765724213
Loss in iteration 80 : 3.0498000902749065
Loss in iteration 81 : 2.124860956283474
Loss in iteration 82 : 3.1097963302796607
Loss in iteration 83 : 1.325988892086821
Loss in iteration 84 : 2.8663181722249975
Loss in iteration 85 : 1.2361776177221666
Loss in iteration 86 : 2.6989742468504865
Loss in iteration 87 : 1.6271777022947933
Loss in iteration 88 : 2.5398856843711757
Loss in iteration 89 : 1.5097801649036304
Loss in iteration 90 : 2.0204630769032836
Loss in iteration 91 : 2.0019147373527364
Loss in iteration 92 : 2.1144653651822787
Loss in iteration 93 : 1.8909001932564538
Loss in iteration 94 : 1.1913864715268445
Loss in iteration 95 : 1.7132183906765186
Loss in iteration 96 : 1.8138947321758396
Loss in iteration 97 : 1.5941511508348818
Loss in iteration 98 : 1.1082747148088015
Loss in iteration 99 : 1.2726703128467425
Loss in iteration 100 : 1.5650665361306095
Loss in iteration 101 : 1.40738409145612
Loss in iteration 102 : 1.0939783677360104
Loss in iteration 103 : 1.0724306642889676
Loss in iteration 104 : 1.281023599458922
Loss in iteration 105 : 1.214047765997398
Loss in iteration 106 : 0.9916260470657657
Loss in iteration 107 : 1.027990443491892
Loss in iteration 108 : 1.011046212073304
Loss in iteration 109 : 1.1010381996578404
Loss in iteration 110 : 0.8825045603645231
Loss in iteration 111 : 0.8603176336789929
Loss in iteration 112 : 1.0558008857383387
Loss in iteration 113 : 0.9245549458257319
Loss in iteration 114 : 0.7750443913650584
Loss in iteration 115 : 0.8605281930620468
Loss in iteration 116 : 0.9449280802903872
Loss in iteration 117 : 1.0009605086456392
Loss in iteration 118 : 1.2988790577753886
Loss in iteration 119 : 1.1763084709117186
Loss in iteration 120 : 0.84906110318741
Loss in iteration 121 : 1.0412400383253626
Loss in iteration 122 : 0.9672049557672151
Loss in iteration 123 : 1.494936910995758
Loss in iteration 124 : 0.9175514766636756
Loss in iteration 125 : 0.9176348559758437
Loss in iteration 126 : 0.7003902379480837
Loss in iteration 127 : 0.7697150187941049
Loss in iteration 128 : 0.7009879581134653
Loss in iteration 129 : 0.7382218623011391
Loss in iteration 130 : 0.6932431832282503
Loss in iteration 131 : 0.7629789672461363
Loss in iteration 132 : 1.060677526224436
Loss in iteration 133 : 1.6889793751804199
Loss in iteration 134 : 1.1392488014203452
Loss in iteration 135 : 0.7161143441664277
Loss in iteration 136 : 0.685219851105701
Loss in iteration 137 : 0.8268948572441385
Loss in iteration 138 : 1.0301453443224562
Loss in iteration 139 : 1.061419753466848
Loss in iteration 140 : 0.9839767796963065
Loss in iteration 141 : 0.8346994417090722
Loss in iteration 142 : 0.7587202370432534
Loss in iteration 143 : 0.7493005417769446
Loss in iteration 144 : 0.8355872918493062
Loss in iteration 145 : 0.8053262665837957
Loss in iteration 146 : 0.9909158176029845
Loss in iteration 147 : 0.9639389041720711
Loss in iteration 148 : 0.8874307188893592
Loss in iteration 149 : 0.9329920091249401
Loss in iteration 150 : 0.6854236487846843
Loss in iteration 151 : 0.850586128987375
Loss in iteration 152 : 0.6449333676631973
Loss in iteration 153 : 0.9917465114606709
Loss in iteration 154 : 1.1686797904122332
Loss in iteration 155 : 1.738921794076091
Loss in iteration 156 : 0.8640526446251083
Loss in iteration 157 : 0.6869486800377818
Loss in iteration 158 : 1.4338688424321515
Loss in iteration 159 : 1.236054472578052
Loss in iteration 160 : 0.7536619321321726
Loss in iteration 161 : 0.6738415854468138
Loss in iteration 162 : 0.9865291721060294
Loss in iteration 163 : 1.1018414801772676
Loss in iteration 164 : 0.7586452543174023
Loss in iteration 165 : 0.740091008536307
Loss in iteration 166 : 0.6609147399221083
Loss in iteration 167 : 0.7836095073918312
Loss in iteration 168 : 0.769115107754205
Loss in iteration 169 : 0.7818604220862457
Loss in iteration 170 : 0.8351333889006287
Loss in iteration 171 : 0.7172487316455076
Loss in iteration 172 : 0.9665911086674118
Loss in iteration 173 : 0.8040184331297295
Loss in iteration 174 : 1.0822070866173223
Loss in iteration 175 : 0.894864198362171
Loss in iteration 176 : 0.7923780350368386
Loss in iteration 177 : 0.7444011006213207
Loss in iteration 178 : 0.7613415629361293
Loss in iteration 179 : 0.8924707985939998
Loss in iteration 180 : 1.0530746702251776
Loss in iteration 181 : 0.9955098613241482
Loss in iteration 182 : 1.0177176769686056
Loss in iteration 183 : 0.7227770241358492
Loss in iteration 184 : 0.7778458777470443
Loss in iteration 185 : 0.6814458806181688
Loss in iteration 186 : 0.798164465602677
Loss in iteration 187 : 0.7721794049530796
Loss in iteration 188 : 0.9125177698658843
Loss in iteration 189 : 0.837673397451327
Loss in iteration 190 : 0.8659625406556336
Loss in iteration 191 : 0.674625391963777
Loss in iteration 192 : 0.7491623924131396
Loss in iteration 193 : 0.6393788027739672
Loss in iteration 194 : 0.7791996292211254
Loss in iteration 195 : 0.7749563787116012
Loss in iteration 196 : 1.1061187948442535
Loss in iteration 197 : 0.9410899141039338
Loss in iteration 198 : 0.8194259408857585
Loss in iteration 199 : 0.6975562280833474
Loss in iteration 200 : 0.7029659205213216
Testing accuracy  of updater 9 on alg 1 with rate 2.0 = 0.766, training accuracy 0.756, time elapsed: 4981 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9532818580908878
Loss in iteration 3 : 0.8945576035138875
Loss in iteration 4 : 0.8786096900275049
Loss in iteration 5 : 0.8149678851685721
Loss in iteration 6 : 0.7331136859544112
Loss in iteration 7 : 0.7134233036673454
Loss in iteration 8 : 0.6506443599972863
Loss in iteration 9 : 0.6103183442487943
Loss in iteration 10 : 0.6185797187226474
Loss in iteration 11 : 0.5818997558226571
Loss in iteration 12 : 0.5743022675953844
Loss in iteration 13 : 0.5750953654818755
Loss in iteration 14 : 0.5530717922888241
Loss in iteration 15 : 0.5573273536157555
Loss in iteration 16 : 0.5627845508347634
Loss in iteration 17 : 0.5502618051139413
Loss in iteration 18 : 0.5494551289777683
Loss in iteration 19 : 0.5557783808784234
Loss in iteration 20 : 0.5494901715125784
Loss in iteration 21 : 0.5442655341774689
Loss in iteration 22 : 0.5479409325078507
Loss in iteration 23 : 0.5466978676857961
Loss in iteration 24 : 0.5417501119235129
Loss in iteration 25 : 0.5433804091573721
Loss in iteration 26 : 0.5445845050113687
Loss in iteration 27 : 0.5399512552149652
Loss in iteration 28 : 0.537945508928159
Loss in iteration 29 : 0.5391381009542101
Loss in iteration 30 : 0.5358967645530616
Loss in iteration 31 : 0.5324092029514303
Loss in iteration 32 : 0.5325699832787155
Loss in iteration 33 : 0.5311978741638779
Loss in iteration 34 : 0.5282715766815181
Loss in iteration 35 : 0.5275683005921762
Loss in iteration 36 : 0.5266426460907123
Loss in iteration 37 : 0.5236264486143594
Loss in iteration 38 : 0.5223640904711745
Loss in iteration 39 : 0.5221765883226366
Loss in iteration 40 : 0.5201719630752811
Loss in iteration 41 : 0.5187876848301803
Loss in iteration 42 : 0.5185406796720241
Loss in iteration 43 : 0.5172196221337365
Loss in iteration 44 : 0.5160652112135992
Loss in iteration 45 : 0.5159152667539822
Loss in iteration 46 : 0.5147473374469153
Loss in iteration 47 : 0.5138663476693652
Loss in iteration 48 : 0.5138256785393663
Loss in iteration 49 : 0.5130738830257844
Loss in iteration 50 : 0.5127327618389282
Loss in iteration 51 : 0.5127392551361973
Loss in iteration 52 : 0.5122672760114733
Loss in iteration 53 : 0.5120785353971713
Loss in iteration 54 : 0.512033975323816
Loss in iteration 55 : 0.5117005400616801
Loss in iteration 56 : 0.511591624021943
Loss in iteration 57 : 0.5115899435058585
Loss in iteration 58 : 0.5113500536857867
Loss in iteration 59 : 0.5112383922207698
Loss in iteration 60 : 0.5112630261778883
Loss in iteration 61 : 0.5111543460457598
Loss in iteration 62 : 0.5110235066056495
Loss in iteration 63 : 0.5109598675760099
Loss in iteration 64 : 0.5108389404788631
Loss in iteration 65 : 0.5107592957801187
Loss in iteration 66 : 0.5107644259596446
Loss in iteration 67 : 0.5106747535896161
Loss in iteration 68 : 0.5105763742215522
Loss in iteration 69 : 0.51053855012247
Loss in iteration 70 : 0.5104521578608068
Loss in iteration 71 : 0.5103752629687635
Loss in iteration 72 : 0.5103339336274696
Loss in iteration 73 : 0.5102243426622659
Loss in iteration 74 : 0.5101574989398286
Loss in iteration 75 : 0.5101132991256501
Loss in iteration 76 : 0.5100110718734259
Loss in iteration 77 : 0.509967216751381
Loss in iteration 78 : 0.5098989465737832
Loss in iteration 79 : 0.5098393757513915
Loss in iteration 80 : 0.509800973297654
Loss in iteration 81 : 0.5097266088076796
Loss in iteration 82 : 0.5096883824208516
Loss in iteration 83 : 0.509639960474778
Loss in iteration 84 : 0.5095869696944033
Loss in iteration 85 : 0.5095511948970899
Loss in iteration 86 : 0.509501745046899
Loss in iteration 87 : 0.5094532108550079
Loss in iteration 88 : 0.5094181891696922
Loss in iteration 89 : 0.5093784526417242
Loss in iteration 90 : 0.5093491630759927
Loss in iteration 91 : 0.5093149657156187
Loss in iteration 92 : 0.5092769444135523
Loss in iteration 93 : 0.509249112451302
Loss in iteration 94 : 0.5092131406604812
Loss in iteration 95 : 0.5091893299776916
Loss in iteration 96 : 0.5091568415434434
Loss in iteration 97 : 0.5091292720241024
Loss in iteration 98 : 0.5091011495273806
Loss in iteration 99 : 0.5090749277111378
Loss in iteration 100 : 0.5090492740612561
Loss in iteration 101 : 0.5090197288124715
Loss in iteration 102 : 0.508997666856861
Loss in iteration 103 : 0.5089694576904445
Loss in iteration 104 : 0.5089487116623904
Loss in iteration 105 : 0.5089222097195584
Loss in iteration 106 : 0.5088959554397586
Loss in iteration 107 : 0.5088734798681391
Loss in iteration 108 : 0.508848053009897
Loss in iteration 109 : 0.5088268527877063
Loss in iteration 110 : 0.5088029869836906
Loss in iteration 111 : 0.5087813939005269
Loss in iteration 112 : 0.5087607131507579
Loss in iteration 113 : 0.5087384839059866
Loss in iteration 114 : 0.5087147683875203
Loss in iteration 115 : 0.508695503915728
Loss in iteration 116 : 0.5086752621679602
Loss in iteration 117 : 0.50865479707904
Loss in iteration 118 : 0.5086333761894501
Loss in iteration 119 : 0.5086121519406818
Loss in iteration 120 : 0.5085934020488551
Loss in iteration 121 : 0.5085736511809748
Loss in iteration 122 : 0.5085536206920166
Loss in iteration 123 : 0.5085345497241426
Loss in iteration 124 : 0.5085149259088141
Loss in iteration 125 : 0.5084961756641431
Loss in iteration 126 : 0.5084776633492423
Loss in iteration 127 : 0.5084597592504847
Loss in iteration 128 : 0.5084414171957847
Loss in iteration 129 : 0.5084244146953757
Loss in iteration 130 : 0.5084064624513627
Loss in iteration 131 : 0.5083886775395893
Loss in iteration 132 : 0.5083718498351638
Loss in iteration 133 : 0.5083551763170217
Loss in iteration 134 : 0.5083384155385022
Loss in iteration 135 : 0.5083219254036451
Loss in iteration 136 : 0.5083048479117193
Loss in iteration 137 : 0.5082887359179253
Loss in iteration 138 : 0.5082728384713459
Loss in iteration 139 : 0.5082568615436152
Loss in iteration 140 : 0.5082404399736759
Loss in iteration 141 : 0.5082245510581975
Loss in iteration 142 : 0.5082090991658506
Loss in iteration 143 : 0.5081930914573497
Loss in iteration 144 : 0.508177438215436
Loss in iteration 145 : 0.508162279981814
Loss in iteration 146 : 0.5081465822329424
Loss in iteration 147 : 0.508131440732905
Loss in iteration 148 : 0.5081154805859621
Loss in iteration 149 : 0.5081001306915156
Loss in iteration 150 : 0.5080852462458569
Loss in iteration 151 : 0.5080708105961461
Loss in iteration 152 : 0.5080558797138939
Loss in iteration 153 : 0.5080406666551853
Loss in iteration 154 : 0.5080264958745472
Loss in iteration 155 : 0.5080122621243746
Loss in iteration 156 : 0.5079975265318983
Loss in iteration 157 : 0.5079848598483229
Loss in iteration 158 : 0.5079701136551867
Loss in iteration 159 : 0.5079564476913808
Loss in iteration 160 : 0.5079429301504311
Loss in iteration 161 : 0.5079290897574527
Loss in iteration 162 : 0.5079150621782519
Loss in iteration 163 : 0.5079017503942218
Loss in iteration 164 : 0.5078885894360881
Loss in iteration 165 : 0.5078748626209283
Loss in iteration 166 : 0.507862034712555
Loss in iteration 167 : 0.5078479668250293
Loss in iteration 168 : 0.5078351409160019
Loss in iteration 169 : 0.5078212338390298
Loss in iteration 170 : 0.5078083176481264
Loss in iteration 171 : 0.507795638101968
Loss in iteration 172 : 0.5077825083746988
Loss in iteration 173 : 0.5077689540923566
Loss in iteration 174 : 0.5077561206909518
Loss in iteration 175 : 0.5077427895776775
Loss in iteration 176 : 0.5077300833856095
Loss in iteration 177 : 0.5077164472809872
Loss in iteration 178 : 0.5077037938863239
Loss in iteration 179 : 0.5076912550357936
Loss in iteration 180 : 0.5076779396642426
Loss in iteration 181 : 0.5076655007446846
Loss in iteration 182 : 0.5076523941380344
Loss in iteration 183 : 0.5076394163540793
Loss in iteration 184 : 0.5076264932493268
Loss in iteration 185 : 0.507613959938891
Loss in iteration 186 : 0.5076011608628185
Loss in iteration 187 : 0.5075884295893353
Loss in iteration 188 : 0.5075760483504916
Loss in iteration 189 : 0.5075637377705979
Loss in iteration 190 : 0.507551576816101
Loss in iteration 191 : 0.5075403801920262
Loss in iteration 192 : 0.5075293034907725
Loss in iteration 193 : 0.50751835115222
Loss in iteration 194 : 0.5075072226712058
Loss in iteration 195 : 0.5074969657879043
Loss in iteration 196 : 0.5074866132455245
Loss in iteration 197 : 0.5074764462325253
Loss in iteration 198 : 0.5074666660451608
Loss in iteration 199 : 0.5074562053374401
Loss in iteration 200 : 0.5074459297152509
Testing accuracy  of updater 9 on alg 1 with rate 0.2 = 0.784, training accuracy 0.78725, time elapsed: 4925 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9926077042547164
Loss in iteration 3 : 0.9786545527430169
Loss in iteration 4 : 0.9591632452520432
Loss in iteration 5 : 0.9373736654183451
Loss in iteration 6 : 0.919625627360849
Loss in iteration 7 : 0.9105086417069246
Loss in iteration 8 : 0.9082097760658933
Loss in iteration 9 : 0.9069523362368759
Loss in iteration 10 : 0.9008797370145859
Loss in iteration 11 : 0.8885715898252726
Loss in iteration 12 : 0.8724794400025133
Loss in iteration 13 : 0.8558311310752157
Loss in iteration 14 : 0.8404411886802323
Loss in iteration 15 : 0.8268731298507962
Loss in iteration 16 : 0.8145635251467116
Loss in iteration 17 : 0.8026673349116752
Loss in iteration 18 : 0.7908236745529408
Loss in iteration 19 : 0.7787345652312381
Loss in iteration 20 : 0.7664680435656228
Loss in iteration 21 : 0.7543832416489761
Loss in iteration 22 : 0.7424253880700632
Loss in iteration 23 : 0.730359701208736
Loss in iteration 24 : 0.71807235988233
Loss in iteration 25 : 0.7056280975502904
Loss in iteration 26 : 0.6936210422674035
Loss in iteration 27 : 0.6824563567463877
Loss in iteration 28 : 0.6723636468999307
Loss in iteration 29 : 0.6633360642656654
Loss in iteration 30 : 0.6550409216610186
Loss in iteration 31 : 0.6472056237714173
Loss in iteration 32 : 0.6397656795509927
Loss in iteration 33 : 0.6328752070268433
Loss in iteration 34 : 0.6266445933010355
Loss in iteration 35 : 0.6210981861279954
Loss in iteration 36 : 0.6161405041319522
Loss in iteration 37 : 0.6117599297958006
Loss in iteration 38 : 0.6077809781577629
Loss in iteration 39 : 0.6040015037624366
Loss in iteration 40 : 0.6003375422810725
Loss in iteration 41 : 0.5968540276936122
Loss in iteration 42 : 0.593612508129492
Loss in iteration 43 : 0.5906996056085523
Loss in iteration 44 : 0.5881528487635014
Loss in iteration 45 : 0.58586247180994
Loss in iteration 46 : 0.5837749375873311
Loss in iteration 47 : 0.5818094300452503
Loss in iteration 48 : 0.579918473507048
Loss in iteration 49 : 0.5780916259038488
Loss in iteration 50 : 0.5763306833171493
Loss in iteration 51 : 0.574650466934977
Loss in iteration 52 : 0.5731117134493012
Loss in iteration 53 : 0.5716843189391606
Loss in iteration 54 : 0.5703409502225586
Loss in iteration 55 : 0.5690680841328046
Loss in iteration 56 : 0.5678533439928063
Loss in iteration 57 : 0.5666798139711443
Loss in iteration 58 : 0.5655523036613372
Loss in iteration 59 : 0.5644941656371687
Loss in iteration 60 : 0.5635163271637685
Loss in iteration 61 : 0.5625932571879357
Loss in iteration 62 : 0.561715225342956
Loss in iteration 63 : 0.5608749495071046
Loss in iteration 64 : 0.5600676716154425
Loss in iteration 65 : 0.5592791834070446
Loss in iteration 66 : 0.5585151288840304
Loss in iteration 67 : 0.5577692513662664
Loss in iteration 68 : 0.5570499697981857
Loss in iteration 69 : 0.5563643806664693
Loss in iteration 70 : 0.5557136270570778
Loss in iteration 71 : 0.5550931358688093
Loss in iteration 72 : 0.5545001912528864
Loss in iteration 73 : 0.5539246744276972
Loss in iteration 74 : 0.5533626805026611
Loss in iteration 75 : 0.5528109954821211
Loss in iteration 76 : 0.5522671866803219
Loss in iteration 77 : 0.5517373696626391
Loss in iteration 78 : 0.5512197716883491
Loss in iteration 79 : 0.5507141846202185
Loss in iteration 80 : 0.5502200747085871
Loss in iteration 81 : 0.5497367077101052
Loss in iteration 82 : 0.5492683877609396
Loss in iteration 83 : 0.548812475888931
Loss in iteration 84 : 0.5483627897209077
Loss in iteration 85 : 0.5479221462730606
Loss in iteration 86 : 0.5474896529429706
Loss in iteration 87 : 0.5470673273925063
Loss in iteration 88 : 0.5466565330284315
Loss in iteration 89 : 0.5462554528065561
Loss in iteration 90 : 0.5458640934232712
Loss in iteration 91 : 0.5454759588150553
Loss in iteration 92 : 0.5450943235591242
Loss in iteration 93 : 0.5447179078258539
Loss in iteration 94 : 0.5443489502257831
Loss in iteration 95 : 0.5439884680559165
Loss in iteration 96 : 0.543636677712517
Loss in iteration 97 : 0.5432911609701814
Loss in iteration 98 : 0.542950936045529
Loss in iteration 99 : 0.5426146752370357
Loss in iteration 100 : 0.5422852798291965
Loss in iteration 101 : 0.5419632894523931
Loss in iteration 102 : 0.5416460006032
Loss in iteration 103 : 0.5413343320307406
Loss in iteration 104 : 0.5410277875946718
Loss in iteration 105 : 0.5407277095035565
Loss in iteration 106 : 0.5404329844328002
Loss in iteration 107 : 0.5401419180259123
Loss in iteration 108 : 0.5398554714259783
Loss in iteration 109 : 0.539573432768808
Loss in iteration 110 : 0.5392943091149143
Loss in iteration 111 : 0.5390187783142271
Loss in iteration 112 : 0.5387469560800348
Loss in iteration 113 : 0.5384792293598997
Loss in iteration 114 : 0.5382159837943339
Loss in iteration 115 : 0.5379585633900019
Loss in iteration 116 : 0.5377054178996814
Loss in iteration 117 : 0.5374561885844489
Loss in iteration 118 : 0.5372103371642879
Loss in iteration 119 : 0.5369682527594432
Loss in iteration 120 : 0.5367290429067273
Loss in iteration 121 : 0.536493806276394
Loss in iteration 122 : 0.5362617946746807
Loss in iteration 123 : 0.536032168405392
Loss in iteration 124 : 0.5358059575843668
Loss in iteration 125 : 0.5355828427391611
Loss in iteration 126 : 0.5353643441671738
Loss in iteration 127 : 0.5351486441963214
Loss in iteration 128 : 0.534935007583013
Loss in iteration 129 : 0.5347233899166184
Loss in iteration 130 : 0.5345133563799035
Loss in iteration 131 : 0.5343066863696069
Loss in iteration 132 : 0.5341020532153824
Loss in iteration 133 : 0.5338996226023809
Loss in iteration 134 : 0.5336996460494642
Loss in iteration 135 : 0.5335022866022184
Loss in iteration 136 : 0.5333080981539647
Loss in iteration 137 : 0.5331153845682121
Loss in iteration 138 : 0.5329251101016641
Loss in iteration 139 : 0.5327379650418547
Loss in iteration 140 : 0.5325529987881256
Loss in iteration 141 : 0.5323703226782188
Loss in iteration 142 : 0.5321900121905527
Loss in iteration 143 : 0.5320115338334398
Loss in iteration 144 : 0.531835368669927
Loss in iteration 145 : 0.5316622855880372
Loss in iteration 146 : 0.5314914018318121
Loss in iteration 147 : 0.5313223035997892
Loss in iteration 148 : 0.5311546608359576
Loss in iteration 149 : 0.5309887680166692
Loss in iteration 150 : 0.5308260751996352
Loss in iteration 151 : 0.5306649651993134
Loss in iteration 152 : 0.5305053080135704
Loss in iteration 153 : 0.5303474630837044
Loss in iteration 154 : 0.5301906852235606
Loss in iteration 155 : 0.5300361422224387
Loss in iteration 156 : 0.5298828402999175
Loss in iteration 157 : 0.5297300006689297
Loss in iteration 158 : 0.5295777036180638
Loss in iteration 159 : 0.5294265803917101
Loss in iteration 160 : 0.5292770678544237
Loss in iteration 161 : 0.529128808331092
Loss in iteration 162 : 0.528982109201566
Loss in iteration 163 : 0.5288376019928761
Loss in iteration 164 : 0.528695356303907
Loss in iteration 165 : 0.528554821212915
Loss in iteration 166 : 0.5284149027000747
Loss in iteration 167 : 0.5282753704834113
Loss in iteration 168 : 0.5281361574482062
Loss in iteration 169 : 0.5279974934119075
Loss in iteration 170 : 0.5278593820114581
Loss in iteration 171 : 0.5277226163629594
Loss in iteration 172 : 0.5275869052542361
Loss in iteration 173 : 0.5274526639948636
Loss in iteration 174 : 0.5273205661461943
Loss in iteration 175 : 0.5271900675001839
Loss in iteration 176 : 0.5270616297573999
Loss in iteration 177 : 0.5269350112842996
Loss in iteration 178 : 0.5268095286942935
Loss in iteration 179 : 0.5266849283346506
Loss in iteration 180 : 0.5265612184309627
Loss in iteration 181 : 0.5264388583315148
Loss in iteration 182 : 0.5263189633719885
Loss in iteration 183 : 0.5261999836471887
Loss in iteration 184 : 0.5260817634667553
Loss in iteration 185 : 0.5259644559942072
Loss in iteration 186 : 0.5258481761675471
Loss in iteration 187 : 0.5257331172754257
Loss in iteration 188 : 0.5256189342105346
Loss in iteration 189 : 0.5255052178997429
Loss in iteration 190 : 0.5253920600278374
Loss in iteration 191 : 0.5252797095227703
Loss in iteration 192 : 0.5251684093193713
Loss in iteration 193 : 0.5250579289410936
Loss in iteration 194 : 0.5249481627759456
Loss in iteration 195 : 0.5248389218162494
Loss in iteration 196 : 0.5247304908123859
Loss in iteration 197 : 0.5246232469878406
Loss in iteration 198 : 0.5245164413229774
Loss in iteration 199 : 0.5244102815110258
Loss in iteration 200 : 0.5243047171193397
Testing accuracy  of updater 9 on alg 1 with rate 0.01999999999999999 = 0.7775, training accuracy 0.78275, time elapsed: 4785 millisecond.
