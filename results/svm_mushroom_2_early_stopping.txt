objc[2924]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10d42f4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10d4b34e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 09:20:04 INFO SparkContext: Running Spark version 2.0.0
18/02/27 09:20:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 09:20:05 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 09:20:05 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 09:20:05 INFO SecurityManager: Changing view acls groups to: 
18/02/27 09:20:05 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 09:20:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 09:20:05 INFO Utils: Successfully started service 'sparkDriver' on port 50443.
18/02/27 09:20:05 INFO SparkEnv: Registering MapOutputTracker
18/02/27 09:20:05 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 09:20:05 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-8357a07c-47f6-4a70-90f4-953632b63b68
18/02/27 09:20:05 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 09:20:05 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 09:20:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 09:20:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 09:20:06 INFO Executor: Starting executor ID driver on host localhost
18/02/27 09:20:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50444.
18/02/27 09:20:06 INFO NettyBlockTransferService: Server created on 192.168.2.140:50444
18/02/27 09:20:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50444)
18/02/27 09:20:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50444 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50444)
18/02/27 09:20:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50444)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 12.263371166865788
Loss in iteration 3 : 1.483416515717938
Loss in iteration 4 : 1.125961575519328
Loss in iteration 5 : 0.9598691563571898
Loss in iteration 6 : 0.8067146490923804
Loss in iteration 7 : 0.6730309220076823
Loss in iteration 8 : 0.5500481751262188
Loss in iteration 9 : 0.43451683560077603
Loss in iteration 10 : 0.3295583304428257
Loss in iteration 11 : 0.255206125306715
Loss in iteration 12 : 0.215472565871456
Loss in iteration 13 : 0.18876941457586613
Loss in iteration 14 : 0.1917954548374251
Testing accuracy  of updater 0 on alg 1 with rate 70.0 = 0.8746666666666667, training accuracy 0.9791368962560731, time elapsed: 2852 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 8.627545637902907
Loss in iteration 3 : 1.0553737426292056
Loss in iteration 4 : 0.8028891586695956
Loss in iteration 5 : 0.6834118909058208
Loss in iteration 6 : 0.5761744453437135
Loss in iteration 7 : 0.4811157359232281
Loss in iteration 8 : 0.3943438868976503
Loss in iteration 9 : 0.312651815881091
Loss in iteration 10 : 0.23877834303259202
Loss in iteration 11 : 0.18422659652368287
Loss in iteration 12 : 0.1553748024819825
Loss in iteration 13 : 0.13492420446808237
Loss in iteration 14 : 0.12945491450520932
Testing accuracy  of updater 0 on alg 1 with rate 49.0 = 0.8835555555555555, training accuracy 0.9807087739354101, time elapsed: 1376 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.992402332960777
Loss in iteration 3 : 0.6379315527806684
Loss in iteration 4 : 0.48001143356328935
Loss in iteration 5 : 0.41208116545265344
Loss in iteration 6 : 0.3487619634896777
Loss in iteration 7 : 0.2922258192983132
Loss in iteration 8 : 0.24178928842126893
Loss in iteration 9 : 0.19394008298968402
Loss in iteration 10 : 0.14979303965776386
Loss in iteration 11 : 0.11745763800567822
Loss in iteration 12 : 0.10490628332112897
Loss in iteration 13 : 0.0919986896365668
Testing accuracy  of updater 0 on alg 1 with rate 28.0 = 0.8862222222222222, training accuracy 0.9794226921977708, time elapsed: 1091 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3611948715972297
Loss in iteration 3 : 0.21971409668426667
Loss in iteration 4 : 0.2190956565306201
Loss in iteration 5 : 0.3159732981333746
Loss in iteration 6 : 0.9618296591770406
Loss in iteration 7 : 0.16838189386056196
Loss in iteration 8 : 0.12128290361120742
Loss in iteration 9 : 0.1056421423157462
Loss in iteration 10 : 0.09112058555260079
Loss in iteration 11 : 0.07741630374738248
Loss in iteration 12 : 0.06551919939363575
Loss in iteration 13 : 0.055142373556352035
Loss in iteration 14 : 0.046186617592271405
Loss in iteration 15 : 0.0421479361775928
Loss in iteration 16 : 0.0428475353605426
Loss in iteration 17 : 0.05033317917292949
Testing accuracy  of updater 0 on alg 1 with rate 7.0 = 0.8924444444444445, training accuracy 0.981566161760503, time elapsed: 1159 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.000926767384797
Loss in iteration 3 : 0.20820514253414013
Loss in iteration 4 : 0.5126791761411081
Loss in iteration 5 : 0.19899463329093917
Loss in iteration 6 : 0.36553285864775636
Loss in iteration 7 : 0.22071013347721635
Loss in iteration 8 : 0.348493204095528
Loss in iteration 9 : 0.16475536028571058
Loss in iteration 10 : 0.09107596253902187
Loss in iteration 11 : 0.07979268316182989
Loss in iteration 12 : 0.0691909918937321
Loss in iteration 13 : 0.06033871931244459
Loss in iteration 14 : 0.052439080947058736
Loss in iteration 15 : 0.04580329774797343
Loss in iteration 16 : 0.03974137986408191
Loss in iteration 17 : 0.0344088815662697
Loss in iteration 18 : 0.030712686758905976
Loss in iteration 19 : 0.028061930730591843
Loss in iteration 20 : 0.026139441297002873
Loss in iteration 21 : 0.024682753947148667
Testing accuracy  of updater 0 on alg 1 with rate 4.9 = 0.9333333333333333, training accuracy 0.9911403258073735, time elapsed: 1240 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6465804396140848
Loss in iteration 3 : 0.3045430942562402
Loss in iteration 4 : 0.9931115350522215
Loss in iteration 5 : 0.13261186264308006
Loss in iteration 6 : 0.11685731169948994
Loss in iteration 7 : 0.10326703151295587
Loss in iteration 8 : 0.09221656967407921
Loss in iteration 9 : 0.08367124009198236
Loss in iteration 10 : 0.07781827057720223
Loss in iteration 11 : 0.07338622319857142
Loss in iteration 12 : 0.07275527035880837
Testing accuracy  of updater 0 on alg 1 with rate 2.8 = 0.9351111111111111, training accuracy 0.9655615890254359, time elapsed: 683 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38074159825798715
Loss in iteration 3 : 1.142330918153673
Loss in iteration 4 : 0.4972097288061558
Loss in iteration 5 : 0.18355902737631835
Loss in iteration 6 : 0.17314304158476898
Loss in iteration 7 : 0.19237577883120716
Loss in iteration 8 : 0.1195697447360645
Loss in iteration 9 : 0.10888216685294377
Loss in iteration 10 : 0.10341665060829126
Loss in iteration 11 : 0.11359030266825965
Testing accuracy  of updater 0 on alg 1 with rate 0.7000000000000002 = 0.9395555555555556, training accuracy 0.959274078308088, time elapsed: 603 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.8786272658367702
Loss in iteration 3 : 1.1400941984301334
Loss in iteration 4 : 0.5841908473683534
Loss in iteration 5 : 1.1263789349443099
Loss in iteration 6 : 0.9968534141711954
Loss in iteration 7 : 0.8539044492202054
Loss in iteration 8 : 0.7508607605243383
Loss in iteration 9 : 0.7269024981357192
Loss in iteration 10 : 0.7873382148367026
Loss in iteration 11 : 0.8219859588170042
Testing accuracy  of updater 1 on alg 1 with rate 10.0 = 0.9688888888888889, training accuracy 0.9525578736781938, time elapsed: 616 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3611948715972297
Loss in iteration 3 : 0.806217314782698
Loss in iteration 4 : 0.4375304354389075
Loss in iteration 5 : 0.8963809049215711
Loss in iteration 6 : 0.6799385999665986
Loss in iteration 7 : 0.5862382115270232
Loss in iteration 8 : 0.5387754798813928
Loss in iteration 9 : 0.5622179471051311
Loss in iteration 10 : 0.6227030213062126
Loss in iteration 11 : 0.6100273730790284
Loss in iteration 12 : 0.5264568066525314
Loss in iteration 13 : 0.4378690227145219
Loss in iteration 14 : 0.38175970077674365
Loss in iteration 15 : 0.3354375196935274
Loss in iteration 16 : 0.29005622780270296
Loss in iteration 17 : 0.25229441090969157
Loss in iteration 18 : 0.21850272011787322
Loss in iteration 19 : 0.18678714601482693
Loss in iteration 20 : 0.15887831025676652
Loss in iteration 21 : 0.13966071565638988
Loss in iteration 22 : 0.12814430718572195
Loss in iteration 23 : 0.12107794952130099
Loss in iteration 24 : 0.11681355208157078
Loss in iteration 25 : 0.11475864207681914
Loss in iteration 26 : 0.11307582713138226
Loss in iteration 27 : 0.11006084567894363
Loss in iteration 28 : 0.1056606233303676
Loss in iteration 29 : 0.09918660546215594
Loss in iteration 30 : 0.091349324763692
Loss in iteration 31 : 0.08361229329532989
Loss in iteration 32 : 0.07830136509310795
Loss in iteration 33 : 0.07506882460134583
Loss in iteration 34 : 0.07180681434166257
Loss in iteration 35 : 0.0674325177145811
Loss in iteration 36 : 0.06216112540631196
Loss in iteration 37 : 0.05775572773016854
Loss in iteration 38 : 0.05334560422272335
Loss in iteration 39 : 0.04983252325077795
Loss in iteration 40 : 0.04802992350034802
Loss in iteration 41 : 0.047166307571479445
Loss in iteration 42 : 0.045927953044081885
Loss in iteration 43 : 0.04435922447978305
Loss in iteration 44 : 0.042596833005497374
Loss in iteration 45 : 0.04100877028870976
Loss in iteration 46 : 0.03981229978375014
Loss in iteration 47 : 0.03887795383821127
Loss in iteration 48 : 0.037949378907810534
Loss in iteration 49 : 0.037040696393747824
Loss in iteration 50 : 0.03609269272948269
Loss in iteration 51 : 0.03509988669400379
Loss in iteration 52 : 0.034012283148503485
Loss in iteration 53 : 0.03288811587374065
Loss in iteration 54 : 0.031786647717206365
Loss in iteration 55 : 0.030673475423742692
Loss in iteration 56 : 0.029686440712927604
Loss in iteration 57 : 0.02876601030795465
Loss in iteration 58 : 0.027821167605032664
Loss in iteration 59 : 0.02686660315494842
Loss in iteration 60 : 0.025937594447565097
Loss in iteration 61 : 0.025027748364661595
Loss in iteration 62 : 0.02408478776490874
Loss in iteration 63 : 0.023133256493243874
Loss in iteration 64 : 0.02229716330922729
Loss in iteration 65 : 0.02153004102355536
Loss in iteration 66 : 0.020757176645156573
Loss in iteration 67 : 0.01995357136357536
Loss in iteration 68 : 0.019120186329778962
Loss in iteration 69 : 0.018304340666535027
Loss in iteration 70 : 0.01750182863899157
Loss in iteration 71 : 0.01676579641550043
Loss in iteration 72 : 0.016085992912261093
Loss in iteration 73 : 0.015400204222132088
Loss in iteration 74 : 0.014712164655814902
Loss in iteration 75 : 0.014059493223015328
Loss in iteration 76 : 0.013438508570409359
Loss in iteration 77 : 0.01286290714196627
Loss in iteration 78 : 0.012362308228456631
Loss in iteration 79 : 0.01186883482932723
Loss in iteration 80 : 0.011361541793588142
Loss in iteration 81 : 0.010837757701972296
Loss in iteration 82 : 0.010299131660067358
Loss in iteration 83 : 0.009757756954069462
Loss in iteration 84 : 0.009229063463189045
Loss in iteration 85 : 0.008703023806660874
Loss in iteration 86 : 0.008173608619119232
Loss in iteration 87 : 0.007641155453665469
Loss in iteration 88 : 0.007109612022601594
Loss in iteration 89 : 0.006584701916366681
Loss in iteration 90 : 0.006120088432634586
Loss in iteration 91 : 0.00564191009000704
Loss in iteration 92 : 0.00520900593290961
Loss in iteration 93 : 0.00481045657217136
Loss in iteration 94 : 0.0044396677204937155
Loss in iteration 95 : 0.004119021336291257
Loss in iteration 96 : 0.0038300942415234033
Loss in iteration 97 : 0.0035585389058407813
Loss in iteration 98 : 0.0033700775204917313
Loss in iteration 99 : 0.0031858491520578795
Loss in iteration 100 : 0.003024549896494725
Loss in iteration 101 : 0.002920551194839916
Loss in iteration 102 : 0.002853448696926844
Loss in iteration 103 : 0.002794366336708601
Loss in iteration 104 : 0.002736695867398425
Loss in iteration 105 : 0.0026802960999055114
Loss in iteration 106 : 0.0026250399640481282
Loss in iteration 107 : 0.0025708130966627285
Loss in iteration 108 : 0.00251751257090211
Loss in iteration 109 : 0.0024650457526037956
Testing accuracy  of updater 1 on alg 1 with rate 7.0 = 0.9644444444444444, training accuracy 0.9991426121749071, time elapsed: 5547 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8477017253118535
Loss in iteration 3 : 0.4711445253786564
Loss in iteration 4 : 0.2946304672344908
Loss in iteration 5 : 0.6645027066712913
Loss in iteration 6 : 0.3838152572423273
Loss in iteration 7 : 0.3328377851774772
Loss in iteration 8 : 0.3667788308765304
Loss in iteration 9 : 0.43293921534993135
Loss in iteration 10 : 0.4238177248482832
Loss in iteration 11 : 0.353611126652902
Loss in iteration 12 : 0.30312467277780486
Loss in iteration 13 : 0.2819888989536729
Loss in iteration 14 : 0.2619944453465708
Loss in iteration 15 : 0.23685777407246916
Loss in iteration 16 : 0.20720650676271357
Loss in iteration 17 : 0.175498408735383
Loss in iteration 18 : 0.14514170560968626
Loss in iteration 19 : 0.11903823021708003
Loss in iteration 20 : 0.10115148757262589
Loss in iteration 21 : 0.08988914433318598
Loss in iteration 22 : 0.08255340982809788
Loss in iteration 23 : 0.0777463147552763
Loss in iteration 24 : 0.07429040084615969
Loss in iteration 25 : 0.07187637000994283
Loss in iteration 26 : 0.0699297434406432
Loss in iteration 27 : 0.06782096121280591
Loss in iteration 28 : 0.06528705978138333
Loss in iteration 29 : 0.06280019400087838
Loss in iteration 30 : 0.05990835653737026
Loss in iteration 31 : 0.056821051726407654
Loss in iteration 32 : 0.053280089433333865
Loss in iteration 33 : 0.04953594005186496
Loss in iteration 34 : 0.045965464683347765
Loss in iteration 35 : 0.042552832110064824
Loss in iteration 36 : 0.039563870848480065
Loss in iteration 37 : 0.036770029173072885
Loss in iteration 38 : 0.03427415671859668
Loss in iteration 39 : 0.03197971087405656
Loss in iteration 40 : 0.02988256387259339
Loss in iteration 41 : 0.02838016732388469
Loss in iteration 42 : 0.027418300815390022
Loss in iteration 43 : 0.026570260370237363
Loss in iteration 44 : 0.02573015468697629
Loss in iteration 45 : 0.0249770355405142
Loss in iteration 46 : 0.02429128673453949
Loss in iteration 47 : 0.023699592153834187
Loss in iteration 48 : 0.023166431236712217
Loss in iteration 49 : 0.022617237143757456
Loss in iteration 50 : 0.022053333344336554
Loss in iteration 51 : 0.021476190809096126
Loss in iteration 52 : 0.020903083015874617
Loss in iteration 53 : 0.020350001329364174
Loss in iteration 54 : 0.019795968809896234
Loss in iteration 55 : 0.019230793634691483
Loss in iteration 56 : 0.01866636761437362
Loss in iteration 57 : 0.018116266015015894
Loss in iteration 58 : 0.017590034770015455
Loss in iteration 59 : 0.017123839928170955
Loss in iteration 60 : 0.016605590038819524
Loss in iteration 61 : 0.016074264654403946
Loss in iteration 62 : 0.01560961897204384
Loss in iteration 63 : 0.015155071859979058
Loss in iteration 64 : 0.014692544466121539
Loss in iteration 65 : 0.014220940354891524
Loss in iteration 66 : 0.013753418374626734
Loss in iteration 67 : 0.013291199105050734
Loss in iteration 68 : 0.012831286938536626
Loss in iteration 69 : 0.01236906239545184
Loss in iteration 70 : 0.011904756713453448
Loss in iteration 71 : 0.0114390819753607
Loss in iteration 72 : 0.010977197429689974
Loss in iteration 73 : 0.010521116620510142
Loss in iteration 74 : 0.010063523701811232
Loss in iteration 75 : 0.009615674460311812
Loss in iteration 76 : 0.009226001256426024
Loss in iteration 77 : 0.008849707062104962
Loss in iteration 78 : 0.00847639795295641
Loss in iteration 79 : 0.00811774985567901
Loss in iteration 80 : 0.00778457869071979
Loss in iteration 81 : 0.007446056074278173
Loss in iteration 82 : 0.00710272793882494
Loss in iteration 83 : 0.0067704291505202956
Loss in iteration 84 : 0.006430034017485455
Loss in iteration 85 : 0.006124186256693301
Loss in iteration 86 : 0.005813151220648977
Loss in iteration 87 : 0.005497284766557745
Loss in iteration 88 : 0.005189674232702031
Loss in iteration 89 : 0.004882613131293058
Loss in iteration 90 : 0.0045742698562156806
Loss in iteration 91 : 0.0042647726248367405
Loss in iteration 92 : 0.003954236832786384
Loss in iteration 93 : 0.0036439368031757242
Loss in iteration 94 : 0.0033679993082952053
Loss in iteration 95 : 0.003107588910287982
Loss in iteration 96 : 0.0028891526244115896
Loss in iteration 97 : 0.0026802941062143842
Loss in iteration 98 : 0.0024848485524293163
Loss in iteration 99 : 0.002289677503535226
Loss in iteration 100 : 0.002117231250799795
Loss in iteration 101 : 0.0020015844950153593
Loss in iteration 102 : 0.0018942434553621027
Loss in iteration 103 : 0.0018038513527459138
Loss in iteration 104 : 0.0017172428132828238
Loss in iteration 105 : 0.0016305219179162203
Loss in iteration 106 : 0.0015917130029411038
Loss in iteration 107 : 0.001560880238893959
Loss in iteration 108 : 0.0015305614111865225
Loss in iteration 109 : 0.0015007051261848264
Loss in iteration 110 : 0.0014712651296182972
Loss in iteration 111 : 0.0014421997926434148
Testing accuracy  of updater 1 on alg 1 with rate 4.0 = 0.968, training accuracy 0.9994284081166047, time elapsed: 5217 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3800799385927725
Loss in iteration 3 : 0.1582145366837528
Loss in iteration 4 : 0.3256458871288908
Loss in iteration 5 : 0.1374297154455878
Loss in iteration 6 : 0.19448450870685116
Loss in iteration 7 : 0.186489711434848
Loss in iteration 8 : 0.14247656543046083
Loss in iteration 9 : 0.12696417160132514
Loss in iteration 10 : 0.13196082910091883
Loss in iteration 11 : 0.13442856785911136
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.944, training accuracy 0.9635610174335525, time elapsed: 606 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38074159825798715
Loss in iteration 3 : 0.4068550240233296
Loss in iteration 4 : 0.21177735981038273
Loss in iteration 5 : 0.2615353355461774
Loss in iteration 6 : 0.21733982318219197
Loss in iteration 7 : 0.14877074929089426
Loss in iteration 8 : 0.14173549680718714
Loss in iteration 9 : 0.16445623258652992
Loss in iteration 10 : 0.18048975952895882
Loss in iteration 11 : 0.17082698499089602
Loss in iteration 12 : 0.15172556751436667
Loss in iteration 13 : 0.13662264178131575
Loss in iteration 14 : 0.12699734219336345
Loss in iteration 15 : 0.12187399569876896
Loss in iteration 16 : 0.12043169591494295
Loss in iteration 17 : 0.12076237285156781
Loss in iteration 18 : 0.11859472536238173
Loss in iteration 19 : 0.11269278550181619
Loss in iteration 20 : 0.10403764546310902
Loss in iteration 21 : 0.0940857548977697
Loss in iteration 22 : 0.08466889412320734
Loss in iteration 23 : 0.07632743449555215
Loss in iteration 24 : 0.07065345219313371
Loss in iteration 25 : 0.06710595453798786
Loss in iteration 26 : 0.06613526996289384
Testing accuracy  of updater 1 on alg 1 with rate 0.7 = 0.9128888888888889, training accuracy 0.9764218348099457, time elapsed: 1129 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4798002338099459
Loss in iteration 3 : 0.22705036548862426
Loss in iteration 4 : 0.1760387930525045
Loss in iteration 5 : 0.143444842370987
Loss in iteration 6 : 0.1376977917292944
Loss in iteration 7 : 0.11955410437956863
Loss in iteration 8 : 0.11710904126728845
Loss in iteration 9 : 0.11660917354090777
Loss in iteration 10 : 0.11425267898096969
Loss in iteration 11 : 0.11036859902392283
Loss in iteration 12 : 0.10566717444431063
Loss in iteration 13 : 0.10112250650759524
Loss in iteration 14 : 0.09727664131901942
Loss in iteration 15 : 0.09361909573278095
Loss in iteration 16 : 0.08986879909988171
Loss in iteration 17 : 0.08573813356076332
Loss in iteration 18 : 0.08146242080572734
Loss in iteration 19 : 0.07711992866083293
Loss in iteration 20 : 0.07296128387859761
Loss in iteration 21 : 0.06907305884300745
Loss in iteration 22 : 0.06546987450619494
Loss in iteration 23 : 0.06229652214847671
Loss in iteration 24 : 0.05916335169311665
Loss in iteration 25 : 0.056076459764748586
Loss in iteration 26 : 0.05311192135453118
Loss in iteration 27 : 0.05037941832996577
Loss in iteration 28 : 0.048122672557345585
Loss in iteration 29 : 0.04609525610508308
Loss in iteration 30 : 0.043897839623911535
Loss in iteration 31 : 0.041546059975405336
Loss in iteration 32 : 0.03896139993115927
Loss in iteration 33 : 0.03640955918271117
Loss in iteration 34 : 0.033928445560129604
Loss in iteration 35 : 0.031525148909555156
Loss in iteration 36 : 0.02919706396667057
Loss in iteration 37 : 0.02690211279238144
Loss in iteration 38 : 0.024846732439523768
Loss in iteration 39 : 0.023052657865930058
Loss in iteration 40 : 0.021334459964145354
Loss in iteration 41 : 0.01973300745074034
Loss in iteration 42 : 0.018377816557883325
Loss in iteration 43 : 0.017295555110035134
Loss in iteration 44 : 0.016372245663426526
Loss in iteration 45 : 0.015520211261386524
Loss in iteration 46 : 0.01473394709132239
Loss in iteration 47 : 0.01397978103180851
Loss in iteration 48 : 0.01328561198500016
Loss in iteration 49 : 0.012664233925250672
Loss in iteration 50 : 0.012148219625174438
Loss in iteration 51 : 0.011756830806189375
Loss in iteration 52 : 0.011418265534651991
Loss in iteration 53 : 0.011107329436395136
Loss in iteration 54 : 0.010804325432151396
Loss in iteration 55 : 0.010472457270174935
Loss in iteration 56 : 0.010108553737449141
Loss in iteration 57 : 0.009747211769117796
Loss in iteration 58 : 0.009406080820769878
Loss in iteration 59 : 0.009098882301478787
Loss in iteration 60 : 0.008874198190747617
Loss in iteration 61 : 0.008711777544890352
Loss in iteration 62 : 0.008561870703272877
Loss in iteration 63 : 0.008383863113338805
Loss in iteration 64 : 0.008198776679560508
Loss in iteration 65 : 0.008025055081805533
Loss in iteration 66 : 0.00788347710691146
Loss in iteration 67 : 0.0077646044244032986
Loss in iteration 68 : 0.007655060488954686
Loss in iteration 69 : 0.007531940843111593
Loss in iteration 70 : 0.007389530279053254
Loss in iteration 71 : 0.00724835092569963
Loss in iteration 72 : 0.007120928178434445
Loss in iteration 73 : 0.0070087377256685315
Loss in iteration 74 : 0.0068938839237911225
Loss in iteration 75 : 0.006783436349400621
Loss in iteration 76 : 0.006678529717301629
Loss in iteration 77 : 0.006568162037892018
Loss in iteration 78 : 0.00647166440439747
Loss in iteration 79 : 0.006392151095035417
Loss in iteration 80 : 0.006315325985660343
Loss in iteration 81 : 0.0062367031195100335
Loss in iteration 82 : 0.006159912295113801
Loss in iteration 83 : 0.00608651430053003
Loss in iteration 84 : 0.006014752434538448
Loss in iteration 85 : 0.005946190973574963
Loss in iteration 86 : 0.005875426039340742
Loss in iteration 87 : 0.005809673980101086
Loss in iteration 88 : 0.005749055186113342
Loss in iteration 89 : 0.005688817384700361
Loss in iteration 90 : 0.005628613592299875
Loss in iteration 91 : 0.005568816001586749
Loss in iteration 92 : 0.005509642057434371
Loss in iteration 93 : 0.005454766031441964
Loss in iteration 94 : 0.00540013615431619
Loss in iteration 95 : 0.005346149813872126
Loss in iteration 96 : 0.005292669453057618
Loss in iteration 97 : 0.005239308521292277
Loss in iteration 98 : 0.005186968711298165
Loss in iteration 99 : 0.005135339323885155
Loss in iteration 100 : 0.005083798428585065
Loss in iteration 101 : 0.005033135000048807
Loss in iteration 102 : 0.004986394027250506
Loss in iteration 103 : 0.004941276911059012
Loss in iteration 104 : 0.004897073312569656
Loss in iteration 105 : 0.004853011932978554
Loss in iteration 106 : 0.004808941546413684
Loss in iteration 107 : 0.004765723390684023
Loss in iteration 108 : 0.00472253448708022
Loss in iteration 109 : 0.004679580363354073
Loss in iteration 110 : 0.004637388829648364
Loss in iteration 111 : 0.004595290937604571
Loss in iteration 112 : 0.004553562231814464
Loss in iteration 113 : 0.004511598535011198
Loss in iteration 114 : 0.004470594573788761
Loss in iteration 115 : 0.004429773474087714
Loss in iteration 116 : 0.004389449218090436
Loss in iteration 117 : 0.004349201640868436
Loss in iteration 118 : 0.004308944957466577
Loss in iteration 119 : 0.00427101131012528
Loss in iteration 120 : 0.0042334863664690294
Loss in iteration 121 : 0.004196191429511961
Loss in iteration 122 : 0.004158928628334379
Loss in iteration 123 : 0.004121447490807391
Loss in iteration 124 : 0.004084233690712404
Loss in iteration 125 : 0.004047303217769811
Loss in iteration 126 : 0.004010267118102749
Loss in iteration 127 : 0.0039735550903392544
Loss in iteration 128 : 0.003936858157005828
Loss in iteration 129 : 0.0039003363037414945
Loss in iteration 130 : 0.0038636126257060865
Loss in iteration 131 : 0.0038267073053767126
Loss in iteration 132 : 0.003790575381696111
Loss in iteration 133 : 0.0037544585976971275
Loss in iteration 134 : 0.0037182041666089577
Loss in iteration 135 : 0.003681721168532099
Loss in iteration 136 : 0.0036452201902656954
Loss in iteration 137 : 0.003608601742334691
Loss in iteration 138 : 0.003572312625629794
Loss in iteration 139 : 0.0035360448931784643
Loss in iteration 140 : 0.003499615185263285
Loss in iteration 141 : 0.003463251941717263
Loss in iteration 142 : 0.0034271759820912033
Loss in iteration 143 : 0.003391006698071022
Loss in iteration 144 : 0.003354844118784827
Loss in iteration 145 : 0.0033187242689066954
Loss in iteration 146 : 0.003282875599125359
Loss in iteration 147 : 0.0032474213273861133
Loss in iteration 148 : 0.0032117563633257172
Loss in iteration 149 : 0.0031762997503109007
Loss in iteration 150 : 0.0031418606135073905
Loss in iteration 151 : 0.003107420926926725
Loss in iteration 152 : 0.003072712990623537
Loss in iteration 153 : 0.003038060304817935
Loss in iteration 154 : 0.0030035106461943684
Loss in iteration 155 : 0.002968997044215232
Loss in iteration 156 : 0.002934669342891116
Loss in iteration 157 : 0.002900739440513006
Loss in iteration 158 : 0.0028670273346264787
Loss in iteration 159 : 0.0028335485108524007
Loss in iteration 160 : 0.0028001670542869596
Loss in iteration 161 : 0.002766958426880421
Loss in iteration 162 : 0.002734208045945331
Loss in iteration 163 : 0.002701604047823117
Loss in iteration 164 : 0.0026687838446204154
Loss in iteration 165 : 0.002636408540817481
Loss in iteration 166 : 0.002604290710594774
Loss in iteration 167 : 0.002572527584839562
Loss in iteration 168 : 0.0025412739784733836
Loss in iteration 169 : 0.0025098441307602934
Loss in iteration 170 : 0.0024786480000888902
Loss in iteration 171 : 0.002447791657182468
Loss in iteration 172 : 0.0024171902734273967
Loss in iteration 173 : 0.0023869998506179245
Loss in iteration 174 : 0.0023572452508624437
Loss in iteration 175 : 0.0023274424873367845
Loss in iteration 176 : 0.0022973934763312943
Loss in iteration 177 : 0.0022670991512441243
Loss in iteration 178 : 0.002237224444541551
Loss in iteration 179 : 0.0022073201678909963
Loss in iteration 180 : 0.002177130410322615
Loss in iteration 181 : 0.0021468234460448763
Loss in iteration 182 : 0.002116802216117102
Loss in iteration 183 : 0.0020865430964953453
Loss in iteration 184 : 0.0020560082868522345
Loss in iteration 185 : 0.0020257077752288714
Loss in iteration 186 : 0.001995531479489933
Loss in iteration 187 : 0.001965182221521799
Loss in iteration 188 : 0.0019347574617406972
Loss in iteration 189 : 0.0019044780911946944
Loss in iteration 190 : 0.0018741558464523014
Loss in iteration 191 : 0.0018439574733068946
Loss in iteration 192 : 0.001814098855168061
Loss in iteration 193 : 0.0017841001816125725
Loss in iteration 194 : 0.0017540158096629073
Loss in iteration 195 : 0.0017239223512382476
Loss in iteration 196 : 0.0016935887181785216
Loss in iteration 197 : 0.0016636109887148763
Loss in iteration 198 : 0.0016335877167117571
Loss in iteration 199 : 0.0016032796691250095
Loss in iteration 200 : 0.0015733392235339058
Testing accuracy  of updater 1 on alg 1 with rate 0.4 = 0.9857777777777778, training accuracy 0.9988568162332095, time elapsed: 9369 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8646938438612042
Loss in iteration 3 : 0.6076121471974923
Loss in iteration 4 : 0.40458091956680947
Loss in iteration 5 : 0.26132472976644694
Loss in iteration 6 : 0.20739011877577365
Loss in iteration 7 : 0.18379754854173888
Loss in iteration 8 : 0.16801229074297605
Loss in iteration 9 : 0.15628023741677788
Loss in iteration 10 : 0.14102378472666127
Loss in iteration 11 : 0.1254616373279207
Loss in iteration 12 : 0.11695915476915143
Loss in iteration 13 : 0.11201976458894702
Loss in iteration 14 : 0.10819014480009302
Loss in iteration 15 : 0.1050246104960433
Loss in iteration 16 : 0.10202298935215823
Loss in iteration 17 : 0.09932183867610263
Loss in iteration 18 : 0.09667789580550529
Loss in iteration 19 : 0.09422414001896551
Loss in iteration 20 : 0.09190215130373434
Loss in iteration 21 : 0.0895872599772257
Loss in iteration 22 : 0.08724889067929449
Loss in iteration 23 : 0.08486222511048902
Loss in iteration 24 : 0.08244431036352473
Loss in iteration 25 : 0.0800480510654976
Loss in iteration 26 : 0.07770742111291606
Loss in iteration 27 : 0.0754478785904489
Loss in iteration 28 : 0.07337205711711038
Loss in iteration 29 : 0.07139083511747298
Loss in iteration 30 : 0.06952442968370705
Loss in iteration 31 : 0.06771196856636856
Loss in iteration 32 : 0.06591493094119796
Loss in iteration 33 : 0.06414808906061543
Loss in iteration 34 : 0.06238024984549003
Loss in iteration 35 : 0.060652389664157334
Loss in iteration 36 : 0.05894599268074408
Loss in iteration 37 : 0.05729004784045614
Loss in iteration 38 : 0.05569778284126225
Loss in iteration 39 : 0.05419061105109057
Loss in iteration 40 : 0.052756996496880634
Loss in iteration 41 : 0.0514185105675988
Loss in iteration 42 : 0.05017835716919766
Loss in iteration 43 : 0.0490109669505461
Loss in iteration 44 : 0.04790953061074177
Loss in iteration 45 : 0.04686229754973783
Loss in iteration 46 : 0.04585419521773878
Loss in iteration 47 : 0.04489553682821638
Loss in iteration 48 : 0.043950827241874886
Loss in iteration 49 : 0.043058860344690524
Loss in iteration 50 : 0.04220191149925719
Loss in iteration 51 : 0.04136245763837031
Loss in iteration 52 : 0.04052212839764807
Loss in iteration 53 : 0.039672271473691484
Loss in iteration 54 : 0.03881302309859306
Loss in iteration 55 : 0.03794968513097776
Loss in iteration 56 : 0.037078180776808876
Loss in iteration 57 : 0.03621097273784493
Loss in iteration 58 : 0.03535376085659594
Loss in iteration 59 : 0.03450310885020449
Loss in iteration 60 : 0.03365976694949526
Loss in iteration 61 : 0.032832764557821914
Loss in iteration 62 : 0.03202094514128144
Loss in iteration 63 : 0.031220749492497833
Loss in iteration 64 : 0.030430971500972924
Loss in iteration 65 : 0.02966286528901893
Loss in iteration 66 : 0.028936079233024586
Loss in iteration 67 : 0.028244884309690483
Loss in iteration 68 : 0.027585426008606338
Loss in iteration 69 : 0.02694860296434244
Loss in iteration 70 : 0.026337810865729772
Loss in iteration 71 : 0.025753710232413535
Loss in iteration 72 : 0.025187511726125406
Loss in iteration 73 : 0.024640282503297323
Loss in iteration 74 : 0.024105429878057492
Loss in iteration 75 : 0.023573950287244426
Loss in iteration 76 : 0.023038606912762175
Loss in iteration 77 : 0.022501133064369273
Loss in iteration 78 : 0.021962760660963927
Loss in iteration 79 : 0.021428590767255076
Loss in iteration 80 : 0.02090374721686322
Loss in iteration 81 : 0.020385083430470734
Loss in iteration 82 : 0.019868739027445168
Loss in iteration 83 : 0.01935700083785717
Loss in iteration 84 : 0.018859634579772737
Loss in iteration 85 : 0.01838081563382496
Loss in iteration 86 : 0.017915855053676703
Loss in iteration 87 : 0.01746517388158096
Loss in iteration 88 : 0.017034308402962505
Loss in iteration 89 : 0.016627571465378778
Loss in iteration 90 : 0.01625152476513047
Loss in iteration 91 : 0.015896909406654558
Loss in iteration 92 : 0.01556292477256279
Loss in iteration 93 : 0.015248363149375487
Loss in iteration 94 : 0.014952451757837637
Loss in iteration 95 : 0.014671764391003742
Loss in iteration 96 : 0.014409258801869863
Loss in iteration 97 : 0.014162074042138468
Loss in iteration 98 : 0.013929441470603852
Loss in iteration 99 : 0.01370795758824098
Loss in iteration 100 : 0.01349593470795547
Loss in iteration 101 : 0.01328421799563082
Loss in iteration 102 : 0.013074186540558143
Loss in iteration 103 : 0.01287091998792537
Loss in iteration 104 : 0.012680763768260436
Loss in iteration 105 : 0.012501341852011136
Loss in iteration 106 : 0.012328822106946625
Loss in iteration 107 : 0.012163304791042905
Loss in iteration 108 : 0.011999882917702967
Loss in iteration 109 : 0.011841128304226203
Loss in iteration 110 : 0.011692038891758579
Loss in iteration 111 : 0.011552810942568744
Loss in iteration 112 : 0.011416690975375784
Loss in iteration 113 : 0.01127823314420399
Loss in iteration 114 : 0.011139584948780977
Loss in iteration 115 : 0.011001111212149911
Loss in iteration 116 : 0.010862460657646835
Loss in iteration 117 : 0.010728794870811666
Loss in iteration 118 : 0.010607016754835942
Loss in iteration 119 : 0.01048899490251008
Loss in iteration 120 : 0.010368275388059326
Loss in iteration 121 : 0.010247548311842353
Loss in iteration 122 : 0.010129047030745532
Loss in iteration 123 : 0.010014079275985964
Loss in iteration 124 : 0.009902229010689571
Loss in iteration 125 : 0.00979096449915826
Loss in iteration 126 : 0.00967974615974975
Loss in iteration 127 : 0.009568569375251757
Loss in iteration 128 : 0.009458538353481415
Loss in iteration 129 : 0.009349903978314925
Loss in iteration 130 : 0.009242854068399418
Loss in iteration 131 : 0.009135824349436043
Loss in iteration 132 : 0.009028931016691723
Loss in iteration 133 : 0.008924162449299078
Loss in iteration 134 : 0.008820135356615416
Loss in iteration 135 : 0.008716503783048652
Loss in iteration 136 : 0.008613741265896651
Loss in iteration 137 : 0.008511221727506004
Loss in iteration 138 : 0.008409177361709295
Loss in iteration 139 : 0.008307673944470547
Loss in iteration 140 : 0.00820638073457919
Loss in iteration 141 : 0.008107229687622364
Loss in iteration 142 : 0.008014348542993889
Loss in iteration 143 : 0.007923266987050601
Loss in iteration 144 : 0.007835536289407408
Loss in iteration 145 : 0.007750498250569768
Loss in iteration 146 : 0.007667974939580646
Loss in iteration 147 : 0.007588324659039601
Loss in iteration 148 : 0.007510746602711734
Loss in iteration 149 : 0.007434018291104651
Loss in iteration 150 : 0.0073574939718683684
Loss in iteration 151 : 0.007283069834943902
Loss in iteration 152 : 0.007212634720456801
Loss in iteration 153 : 0.00714608123768123
Loss in iteration 154 : 0.007082447579476306
Loss in iteration 155 : 0.007018163495043264
Loss in iteration 156 : 0.006956410917192829
Loss in iteration 157 : 0.00689642767501657
Loss in iteration 158 : 0.006839963808930495
Loss in iteration 159 : 0.006786898419330631
Loss in iteration 160 : 0.006735393555281192
Loss in iteration 161 : 0.006684140983379074
Loss in iteration 162 : 0.006632889569074215
Loss in iteration 163 : 0.0065839148173745575
Loss in iteration 164 : 0.006536355709108247
Loss in iteration 165 : 0.006489323932733705
Loss in iteration 166 : 0.006442876878799603
Loss in iteration 167 : 0.0063967136274491795
Loss in iteration 168 : 0.006350890622253628
Loss in iteration 169 : 0.006306154281893075
Loss in iteration 170 : 0.006263638312857684
Loss in iteration 171 : 0.006221920543946694
Loss in iteration 172 : 0.00618005061009392
Loss in iteration 173 : 0.006139242356225003
Loss in iteration 174 : 0.006100110884863234
Loss in iteration 175 : 0.0060629940494230155
Loss in iteration 176 : 0.0060260282861348555
Loss in iteration 177 : 0.005989394424768851
Loss in iteration 178 : 0.0059526554935906365
Loss in iteration 179 : 0.005915662576905871
Loss in iteration 180 : 0.005878441073265205
Loss in iteration 181 : 0.005841196851462834
Loss in iteration 182 : 0.005804581540876595
Loss in iteration 183 : 0.005768976222269976
Loss in iteration 184 : 0.005734270257984906
Loss in iteration 185 : 0.0057003699792379185
Loss in iteration 186 : 0.005666223450347153
Loss in iteration 187 : 0.005631586177446662
Loss in iteration 188 : 0.005596924534128336
Loss in iteration 189 : 0.005563055227404959
Loss in iteration 190 : 0.00552936373762777
Loss in iteration 191 : 0.005497094732365572
Loss in iteration 192 : 0.005465065530451327
Loss in iteration 193 : 0.005432215924175437
Loss in iteration 194 : 0.005399336040387145
Loss in iteration 195 : 0.00536739369778617
Loss in iteration 196 : 0.005335696657885125
Loss in iteration 197 : 0.005304478819430673
Loss in iteration 198 : 0.005273391814146434
Loss in iteration 199 : 0.005242339936337862
Loss in iteration 200 : 0.00521133880048791
Testing accuracy  of updater 1 on alg 1 with rate 0.09999999999999998 = 0.9724444444444444, training accuracy 0.998428122320663, time elapsed: 9159 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.448921920967099
Loss in iteration 3 : 0.5890972893462313
Loss in iteration 4 : 0.5775028909250911
Loss in iteration 5 : 0.6426139384289771
Loss in iteration 6 : 0.6058792575898305
Loss in iteration 7 : 0.5439839338550702
Loss in iteration 8 : 0.4976462905000549
Loss in iteration 9 : 0.4560838320182999
Loss in iteration 10 : 0.4196880432730047
Loss in iteration 11 : 0.3921172893173572
Loss in iteration 12 : 0.35665482489251193
Loss in iteration 13 : 0.3102967258477023
Loss in iteration 14 : 0.26045937410725606
Loss in iteration 15 : 0.21957816850266715
Loss in iteration 16 : 0.1927037297147546
Loss in iteration 17 : 0.1797932219738131
Loss in iteration 18 : 0.1725814761033508
Loss in iteration 19 : 0.16377714474326804
Loss in iteration 20 : 0.1528016452540528
Loss in iteration 21 : 0.1421650077312843
Loss in iteration 22 : 0.13241682653828518
Loss in iteration 23 : 0.12181410985354751
Loss in iteration 24 : 0.11164904589146302
Loss in iteration 25 : 0.10338694402055743
Loss in iteration 26 : 0.09640570453485464
Loss in iteration 27 : 0.08944739315585597
Loss in iteration 28 : 0.08229462472332397
Loss in iteration 29 : 0.07557958274422544
Loss in iteration 30 : 0.06876350161227575
Loss in iteration 31 : 0.06252387926112447
Loss in iteration 32 : 0.057808142595631884
Loss in iteration 33 : 0.054086094761598126
Loss in iteration 34 : 0.050976783318384636
Loss in iteration 35 : 0.0485099386637576
Loss in iteration 36 : 0.04663747514016974
Loss in iteration 37 : 0.045095759668638526
Loss in iteration 38 : 0.04379255102275227
Loss in iteration 39 : 0.04251212075336714
Loss in iteration 40 : 0.04124128239418959
Loss in iteration 41 : 0.04005130614712562
Loss in iteration 42 : 0.03890232155480718
Loss in iteration 43 : 0.037772228657557985
Loss in iteration 44 : 0.03662834811782575
Loss in iteration 45 : 0.03549920064219156
Loss in iteration 46 : 0.03452086257588864
Loss in iteration 47 : 0.033562041435654374
Loss in iteration 48 : 0.032564436873286515
Loss in iteration 49 : 0.0315319272309984
Loss in iteration 50 : 0.03046800301678203
Loss in iteration 51 : 0.02947901772965944
Loss in iteration 52 : 0.028580960516711135
Loss in iteration 53 : 0.027693367842452722
Loss in iteration 54 : 0.026822141131433712
Loss in iteration 55 : 0.025966930976087193
Loss in iteration 56 : 0.02505362323313344
Loss in iteration 57 : 0.02419136672809068
Loss in iteration 58 : 0.023334860826387797
Loss in iteration 59 : 0.022568179062281046
Loss in iteration 60 : 0.0218165996244441
Loss in iteration 61 : 0.021111746769977693
Loss in iteration 62 : 0.020438258734811334
Loss in iteration 63 : 0.019793586490321206
Loss in iteration 64 : 0.019203999940825323
Loss in iteration 65 : 0.018616043646078623
Loss in iteration 66 : 0.018029713606141192
Loss in iteration 67 : 0.017451540231164468
Loss in iteration 68 : 0.016887567811846412
Loss in iteration 69 : 0.016366313202745418
Loss in iteration 70 : 0.01584244208699745
Loss in iteration 71 : 0.015318699413478392
Loss in iteration 72 : 0.014796937579230314
Loss in iteration 73 : 0.014275670616979106
Loss in iteration 74 : 0.013757440965944598
Loss in iteration 75 : 0.013239368276449223
Loss in iteration 76 : 0.012720495618072425
Loss in iteration 77 : 0.012200902987702352
Loss in iteration 78 : 0.011683008488466085
Loss in iteration 79 : 0.011165639892841984
Loss in iteration 80 : 0.010654288768451209
Loss in iteration 81 : 0.010136510073970754
Loss in iteration 82 : 0.00961726754025722
Loss in iteration 83 : 0.00909993210871063
Loss in iteration 84 : 0.008584096559879894
Loss in iteration 85 : 0.008071681662314813
Loss in iteration 86 : 0.007553438998068147
Loss in iteration 87 : 0.007041429192514973
Loss in iteration 88 : 0.0065313682705162674
Loss in iteration 89 : 0.0060204893795137715
Loss in iteration 90 : 0.005508540774291936
Loss in iteration 91 : 0.005061270962052684
Loss in iteration 92 : 0.004688682700608117
Loss in iteration 93 : 0.004362377475729151
Loss in iteration 94 : 0.004042399154422603
Loss in iteration 95 : 0.0037281150463312263
Loss in iteration 96 : 0.003440143663551412
Loss in iteration 97 : 0.0031675148834654776
Loss in iteration 98 : 0.0029660497333129417
Loss in iteration 99 : 0.0028405039185361108
Loss in iteration 100 : 0.002713838452364339
Loss in iteration 101 : 0.0026044300630514443
Loss in iteration 102 : 0.002515352404056841
Loss in iteration 103 : 0.002446841451916762
Loss in iteration 104 : 0.0023928986898008454
Loss in iteration 105 : 0.0023398538587827583
Loss in iteration 106 : 0.0022876171657527276
Loss in iteration 107 : 0.0022361077969119372
Loss in iteration 108 : 0.0021852530198414708
Loss in iteration 109 : 0.002134987375364293
Loss in iteration 110 : 0.002085251950221076
Testing accuracy  of updater 2 on alg 1 with rate 7.0 = 0.968, training accuracy 0.999285510145756, time elapsed: 5880 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.759472178543441
Loss in iteration 3 : 0.4269290790200536
Loss in iteration 4 : 0.42669340151719537
Loss in iteration 5 : 0.4615755009201449
Loss in iteration 6 : 0.4321809899479066
Loss in iteration 7 : 0.38949593668641525
Loss in iteration 8 : 0.36008223161108444
Loss in iteration 9 : 0.33264040673649287
Loss in iteration 10 : 0.3105019170769787
Loss in iteration 11 : 0.2907287503058367
Loss in iteration 12 : 0.26347831872108807
Loss in iteration 13 : 0.22981344411852606
Loss in iteration 14 : 0.1934426197861087
Loss in iteration 15 : 0.16189923797037303
Loss in iteration 16 : 0.14269353863665063
Loss in iteration 17 : 0.13094170188766951
Loss in iteration 18 : 0.12450579151573522
Loss in iteration 19 : 0.11753280760131918
Loss in iteration 20 : 0.10977361043098437
Loss in iteration 21 : 0.10240163821637949
Loss in iteration 22 : 0.0955199662487137
Loss in iteration 23 : 0.08801597870645948
Loss in iteration 24 : 0.0808577440256361
Loss in iteration 25 : 0.0749852203919017
Loss in iteration 26 : 0.06987242997273169
Loss in iteration 27 : 0.064873325810367
Loss in iteration 28 : 0.05978009227443055
Loss in iteration 29 : 0.05496494049324841
Loss in iteration 30 : 0.05015060926373949
Loss in iteration 31 : 0.04579643458330771
Loss in iteration 32 : 0.04244055510147759
Loss in iteration 33 : 0.0397078624517303
Loss in iteration 34 : 0.03743066194956579
Loss in iteration 35 : 0.03551624783713615
Loss in iteration 36 : 0.0341952348827085
Loss in iteration 37 : 0.03301784635062649
Loss in iteration 38 : 0.03192857594366439
Loss in iteration 39 : 0.030925843237789272
Loss in iteration 40 : 0.02997772951868888
Loss in iteration 41 : 0.029114831345526657
Loss in iteration 42 : 0.028289817998454033
Loss in iteration 43 : 0.027483570361838808
Loss in iteration 44 : 0.02667615417778992
Loss in iteration 45 : 0.025929215008710937
Loss in iteration 46 : 0.02524423758630231
Loss in iteration 47 : 0.024554817232047676
Loss in iteration 48 : 0.02385697032749241
Loss in iteration 49 : 0.023132698828844743
Loss in iteration 50 : 0.022386588604751907
Loss in iteration 51 : 0.02166836224695247
Loss in iteration 52 : 0.02104468913841396
Loss in iteration 53 : 0.020414586239911513
Loss in iteration 54 : 0.019780065999633552
Loss in iteration 55 : 0.01914804480581863
Loss in iteration 56 : 0.018548803855049877
Loss in iteration 57 : 0.017939696117298003
Loss in iteration 58 : 0.017338137638518707
Loss in iteration 59 : 0.016725760199996675
Loss in iteration 60 : 0.016172898911970345
Loss in iteration 61 : 0.01563386777414429
Loss in iteration 62 : 0.01510524760570847
Loss in iteration 63 : 0.014609049760012623
Loss in iteration 64 : 0.014194969400858532
Loss in iteration 65 : 0.013782167197479555
Loss in iteration 66 : 0.013370515334298186
Loss in iteration 67 : 0.012959898777294676
Loss in iteration 68 : 0.012551003724945591
Loss in iteration 69 : 0.012159916822234912
Loss in iteration 70 : 0.011791995886790108
Loss in iteration 71 : 0.011426757413879468
Loss in iteration 72 : 0.011064585536738426
Loss in iteration 73 : 0.010698207641173478
Loss in iteration 74 : 0.01033450447575091
Loss in iteration 75 : 0.009970346560388914
Loss in iteration 76 : 0.009606974733988843
Loss in iteration 77 : 0.009244393614381702
Loss in iteration 78 : 0.00888660535113955
Loss in iteration 79 : 0.00852343733511954
Loss in iteration 80 : 0.008159726519992494
Loss in iteration 81 : 0.007794956774235974
Loss in iteration 82 : 0.0074355759213945985
Loss in iteration 83 : 0.0070772285051470905
Loss in iteration 84 : 0.006714300492828189
Loss in iteration 85 : 0.006349423866548672
Loss in iteration 86 : 0.005997844552472063
Loss in iteration 87 : 0.005633082634864195
Loss in iteration 88 : 0.005275732028653503
Loss in iteration 89 : 0.004918127573796661
Loss in iteration 90 : 0.0045602775920447845
Loss in iteration 91 : 0.004201463383539808
Loss in iteration 92 : 0.0038420202735616198
Loss in iteration 93 : 0.0035222646559584215
Loss in iteration 94 : 0.003283663193572044
Loss in iteration 95 : 0.003058469688027614
Loss in iteration 96 : 0.0028373829997967877
Loss in iteration 97 : 0.0026227805024233264
Loss in iteration 98 : 0.002425299888305892
Loss in iteration 99 : 0.0022209865847696813
Loss in iteration 100 : 0.002041448516673694
Loss in iteration 101 : 0.001942218495680655
Loss in iteration 102 : 0.0018627571834313372
Loss in iteration 103 : 0.0018023370553298503
Loss in iteration 104 : 0.0017430804055900849
Loss in iteration 105 : 0.0016982658326384335
Loss in iteration 106 : 0.0016596490231441975
Loss in iteration 107 : 0.0016217464530197561
Loss in iteration 108 : 0.0015844866983281277
Loss in iteration 109 : 0.0015478054775260302
Loss in iteration 110 : 0.0015116449372245147
Loss in iteration 111 : 0.0014759530093735207
Loss in iteration 112 : 0.0014406828327279946
Testing accuracy  of updater 2 on alg 1 with rate 4.8999999999999995 = 0.9697777777777777, training accuracy 0.9995713060874536, time elapsed: 5939 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0727509654295293
Loss in iteration 3 : 0.26238048432060224
Loss in iteration 4 : 0.29592405543351197
Loss in iteration 5 : 0.2610200257319408
Loss in iteration 6 : 0.2501000285234003
Loss in iteration 7 : 0.23581175335731816
Loss in iteration 8 : 0.22694718301311265
Loss in iteration 9 : 0.21673154244903173
Loss in iteration 10 : 0.20597207387465188
Loss in iteration 11 : 0.19172795103397916
Loss in iteration 12 : 0.17357048227185642
Loss in iteration 13 : 0.15337335078056333
Loss in iteration 14 : 0.13208904088102363
Loss in iteration 15 : 0.10974486490533053
Loss in iteration 16 : 0.09398610710614781
Loss in iteration 17 : 0.08358228910980756
Loss in iteration 18 : 0.07633641590859186
Loss in iteration 19 : 0.07164067509292635
Loss in iteration 20 : 0.06731969355266057
Loss in iteration 21 : 0.06288386791505503
Loss in iteration 22 : 0.05894861914991449
Loss in iteration 23 : 0.0548354518753988
Loss in iteration 24 : 0.05070726705011886
Loss in iteration 25 : 0.04706189443926833
Loss in iteration 26 : 0.04400844430720455
Loss in iteration 27 : 0.040777526525976444
Loss in iteration 28 : 0.037526963945602836
Loss in iteration 29 : 0.03444994094972091
Loss in iteration 30 : 0.031846072513860534
Loss in iteration 31 : 0.02936892688294592
Loss in iteration 32 : 0.02704563407921762
Loss in iteration 33 : 0.025202465891280904
Loss in iteration 34 : 0.023691412729349205
Loss in iteration 35 : 0.022359942104705183
Loss in iteration 36 : 0.021273176490023624
Loss in iteration 37 : 0.020460471875744714
Loss in iteration 38 : 0.01971262983221801
Loss in iteration 39 : 0.019028380434417947
Loss in iteration 40 : 0.018444120028162942
Loss in iteration 41 : 0.017917700324210854
Loss in iteration 42 : 0.01739822597000576
Loss in iteration 43 : 0.016918229829124488
Loss in iteration 44 : 0.016457565308577782
Loss in iteration 45 : 0.01604054941568714
Loss in iteration 46 : 0.015645687020475287
Loss in iteration 47 : 0.015252865855072965
Loss in iteration 48 : 0.0148507662789823
Loss in iteration 49 : 0.01444031613327212
Loss in iteration 50 : 0.014022350474904377
Loss in iteration 51 : 0.013597620855144823
Loss in iteration 52 : 0.013183699091999746
Loss in iteration 53 : 0.012782877274561701
Loss in iteration 54 : 0.01241625241838476
Loss in iteration 55 : 0.012051754911287611
Loss in iteration 56 : 0.011684588952410488
Loss in iteration 57 : 0.011330391744458497
Loss in iteration 58 : 0.010987712125924367
Loss in iteration 59 : 0.010642006784849695
Loss in iteration 60 : 0.010284764847023115
Loss in iteration 61 : 0.009935459036757653
Loss in iteration 62 : 0.009590487539709432
Loss in iteration 63 : 0.00927957794119641
Loss in iteration 64 : 0.008975546743656337
Loss in iteration 65 : 0.008703235835429363
Loss in iteration 66 : 0.008434023199571525
Loss in iteration 67 : 0.00817155337457295
Loss in iteration 68 : 0.00791226856806978
Loss in iteration 69 : 0.007658289746500337
Loss in iteration 70 : 0.007423768180542382
Loss in iteration 71 : 0.0071897674111000565
Loss in iteration 72 : 0.006967505691641327
Loss in iteration 73 : 0.006758340331140375
Loss in iteration 74 : 0.006547378280657598
Loss in iteration 75 : 0.0063356601979702365
Loss in iteration 76 : 0.006131444808427353
Loss in iteration 77 : 0.005924684888707309
Loss in iteration 78 : 0.005718292329552923
Loss in iteration 79 : 0.0055085807466792045
Loss in iteration 80 : 0.005300040830455068
Loss in iteration 81 : 0.005090269361082991
Loss in iteration 82 : 0.004876301559554828
Loss in iteration 83 : 0.004665678103577424
Loss in iteration 84 : 0.004458505964611754
Loss in iteration 85 : 0.004249472726612055
Loss in iteration 86 : 0.004046371063674389
Loss in iteration 87 : 0.0038399263313252585
Loss in iteration 88 : 0.003635484320615364
Loss in iteration 89 : 0.0034306234696486215
Loss in iteration 90 : 0.003225510648930437
Loss in iteration 91 : 0.0030206629002163866
Loss in iteration 92 : 0.002815144701817413
Loss in iteration 93 : 0.0026093152819305072
Loss in iteration 94 : 0.0024032057627044467
Loss in iteration 95 : 0.002203697700637631
Loss in iteration 96 : 0.002039716232485842
Loss in iteration 97 : 0.0019149071975058626
Loss in iteration 98 : 0.0017958847142887223
Loss in iteration 99 : 0.0016788725201430288
Loss in iteration 100 : 0.0015636695861616396
Loss in iteration 101 : 0.0014511468086759736
Loss in iteration 102 : 0.0013593567421823077
Loss in iteration 103 : 0.0012780852197576415
Loss in iteration 104 : 0.0012244578685809456
Loss in iteration 105 : 0.001175478414924666
Loss in iteration 106 : 0.0011392297585209488
Loss in iteration 107 : 0.0011026002398443788
Loss in iteration 108 : 0.0010669289942744193
Loss in iteration 109 : 0.001038575639654034
Loss in iteration 110 : 0.0010112407308446548
Loss in iteration 111 : 9.889180749912982E-4
Loss in iteration 112 : 9.658527587623079E-4
Loss in iteration 113 : 9.43164272495666E-4
Loss in iteration 114 : 9.195072663737826E-4
Loss in iteration 115 : 8.949785923821818E-4
Testing accuracy  of updater 2 on alg 1 with rate 2.8 = 0.9688888888888889, training accuracy 0.9994284081166047, time elapsed: 5155 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4190660211199753
Loss in iteration 3 : 0.16654120659421132
Loss in iteration 4 : 0.1811595831855963
Loss in iteration 5 : 0.10963205886775611
Loss in iteration 6 : 0.11245351567705325
Loss in iteration 7 : 0.10679830704293367
Loss in iteration 8 : 0.09892361991001562
Loss in iteration 9 : 0.09330450792052115
Loss in iteration 10 : 0.08957653674499895
Loss in iteration 11 : 0.0850539916703518
Loss in iteration 12 : 0.0797491490336439
Loss in iteration 13 : 0.07387609441605673
Loss in iteration 14 : 0.0678868828244974
Loss in iteration 15 : 0.06194670152984507
Loss in iteration 16 : 0.05637214829047409
Loss in iteration 17 : 0.05108722846233639
Loss in iteration 18 : 0.046305868790588706
Loss in iteration 19 : 0.042105008882012976
Loss in iteration 20 : 0.038849753461275865
Loss in iteration 21 : 0.03578133547471271
Loss in iteration 22 : 0.032518050657275394
Loss in iteration 23 : 0.02964951692349961
Loss in iteration 24 : 0.02722523564763388
Loss in iteration 25 : 0.024815138519628573
Loss in iteration 26 : 0.02241835428287353
Loss in iteration 27 : 0.02037570492391776
Loss in iteration 28 : 0.01869364879685139
Loss in iteration 29 : 0.017238159241693623
Loss in iteration 30 : 0.015952583328722177
Loss in iteration 31 : 0.014787015503370056
Loss in iteration 32 : 0.01373199808341841
Loss in iteration 33 : 0.012781016453792846
Loss in iteration 34 : 0.011953751698071684
Loss in iteration 35 : 0.011229040779445053
Loss in iteration 36 : 0.010540108826307383
Loss in iteration 37 : 0.009918329251774457
Loss in iteration 38 : 0.009403332140569293
Loss in iteration 39 : 0.008923305423469757
Loss in iteration 40 : 0.008568025195356151
Loss in iteration 41 : 0.008291084966096688
Loss in iteration 42 : 0.008036489767345847
Loss in iteration 43 : 0.00785912546468138
Loss in iteration 44 : 0.00769757178038985
Loss in iteration 45 : 0.007534258738685037
Loss in iteration 46 : 0.007384012532817332
Loss in iteration 47 : 0.00722520057804704
Loss in iteration 48 : 0.007072361167475752
Loss in iteration 49 : 0.006914252021524014
Loss in iteration 50 : 0.006748269849432807
Loss in iteration 51 : 0.0065853862691004445
Loss in iteration 52 : 0.00644034406438436
Loss in iteration 53 : 0.006299598628392307
Loss in iteration 54 : 0.006166519057752321
Loss in iteration 55 : 0.006044504225893861
Loss in iteration 56 : 0.005929366172190071
Loss in iteration 57 : 0.005826277851736424
Loss in iteration 58 : 0.005727251337305156
Loss in iteration 59 : 0.005637447392110329
Loss in iteration 60 : 0.005548906594266956
Loss in iteration 61 : 0.00546018222252927
Loss in iteration 62 : 0.005371292634286697
Loss in iteration 63 : 0.005284565393467452
Loss in iteration 64 : 0.005196411201212856
Loss in iteration 65 : 0.005107427767914712
Loss in iteration 66 : 0.005017698017677369
Loss in iteration 67 : 0.004927560871894974
Loss in iteration 68 : 0.004838021972785074
Loss in iteration 69 : 0.004747556603253655
Loss in iteration 70 : 0.004658283402146063
Loss in iteration 71 : 0.004580129688317083
Loss in iteration 72 : 0.004505187016629552
Loss in iteration 73 : 0.0044324876218331845
Loss in iteration 74 : 0.0043566594060953645
Loss in iteration 75 : 0.0042850976374460194
Loss in iteration 76 : 0.004212372396524029
Loss in iteration 77 : 0.00413979781536779
Loss in iteration 78 : 0.004066836905633788
Loss in iteration 79 : 0.003995513703233033
Loss in iteration 80 : 0.003926271527411069
Loss in iteration 81 : 0.0038596362682038875
Loss in iteration 82 : 0.0037924209016990704
Loss in iteration 83 : 0.003726111840065586
Loss in iteration 84 : 0.0036595988175129207
Loss in iteration 85 : 0.0035932124938001368
Loss in iteration 86 : 0.003526995370379912
Loss in iteration 87 : 0.0034607227736777
Loss in iteration 88 : 0.0033948467506946522
Loss in iteration 89 : 0.00333008148161147
Loss in iteration 90 : 0.0032662220930490616
Loss in iteration 91 : 0.003202139807662199
Loss in iteration 92 : 0.0031380813233567125
Loss in iteration 93 : 0.0030739539505179646
Loss in iteration 94 : 0.0030099311269425576
Loss in iteration 95 : 0.0029469494619615423
Loss in iteration 96 : 0.002884405263931807
Loss in iteration 97 : 0.0028219575338608916
Loss in iteration 98 : 0.002761417547209858
Loss in iteration 99 : 0.002701726986265839
Loss in iteration 100 : 0.0026463211628215593
Loss in iteration 101 : 0.0025904788326783384
Loss in iteration 102 : 0.002535148499901862
Loss in iteration 103 : 0.0024793785232188233
Loss in iteration 104 : 0.0024242085663335046
Loss in iteration 105 : 0.0023685889947066435
Loss in iteration 106 : 0.0023128738057888168
Loss in iteration 107 : 0.0022568673148619443
Loss in iteration 108 : 0.0022013359544653228
Loss in iteration 109 : 0.002150722152833445
Loss in iteration 110 : 0.002096652319200095
Loss in iteration 111 : 0.002040751484769112
Loss in iteration 112 : 0.001987992995215486
Loss in iteration 113 : 0.0019316285816470646
Loss in iteration 114 : 0.0018772283420569534
Loss in iteration 115 : 0.0018234618561353423
Loss in iteration 116 : 0.0017680125067962345
Loss in iteration 117 : 0.001714080287758913
Loss in iteration 118 : 0.0016628081111796087
Loss in iteration 119 : 0.0016081914104765364
Loss in iteration 120 : 0.001554937185919739
Loss in iteration 121 : 0.001506255306650523
Loss in iteration 122 : 0.0014609416480871752
Loss in iteration 123 : 0.0014048709151517763
Loss in iteration 124 : 0.0013523213236879337
Loss in iteration 125 : 0.0013006395901042306
Loss in iteration 126 : 0.0012497598995133742
Loss in iteration 127 : 0.001199319061021529
Loss in iteration 128 : 0.0011482089443852807
Loss in iteration 129 : 0.0010968917790829486
Loss in iteration 130 : 0.0010522951284698803
Loss in iteration 131 : 0.0010098450228901209
Loss in iteration 132 : 9.70988869610964E-4
Loss in iteration 133 : 9.336752468513159E-4
Loss in iteration 134 : 8.844928169112E-4
Loss in iteration 135 : 8.447074097637536E-4
Loss in iteration 136 : 8.026562158165668E-4
Loss in iteration 137 : 7.592709969697335E-4
Loss in iteration 138 : 7.193804780720438E-4
Loss in iteration 139 : 6.874332320206891E-4
Loss in iteration 140 : 6.604619184279511E-4
Loss in iteration 141 : 6.339562565813496E-4
Loss in iteration 142 : 6.08813554124837E-4
Loss in iteration 143 : 5.843385887065659E-4
Loss in iteration 144 : 5.625462932024286E-4
Loss in iteration 145 : 5.384638335234115E-4
Loss in iteration 146 : 5.175880506127069E-4
Loss in iteration 147 : 4.926040520999278E-4
Loss in iteration 148 : 4.7053057573710066E-4
Loss in iteration 149 : 4.495174585547009E-4
Loss in iteration 150 : 4.2925900185392693E-4
Loss in iteration 151 : 4.155128688840574E-4
Loss in iteration 152 : 4.0027458421355703E-4
Loss in iteration 153 : 3.873087981103938E-4
Loss in iteration 154 : 3.721044275300786E-4
Loss in iteration 155 : 3.603844860127621E-4
Loss in iteration 156 : 3.4820409428687195E-4
Loss in iteration 157 : 3.333779463939297E-4
Loss in iteration 158 : 3.202334416865951E-4
Loss in iteration 159 : 3.1238790638030584E-4
Testing accuracy  of updater 2 on alg 1 with rate 0.7 = 0.9875555555555555, training accuracy 0.9997142040583024, time elapsed: 7132 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3747422549170744
Loss in iteration 3 : 0.3825708769350344
Loss in iteration 4 : 0.42880098815116713
Loss in iteration 5 : 0.36970874532017184
Loss in iteration 6 : 0.26013661579139524
Loss in iteration 7 : 0.12250312198893
Loss in iteration 8 : 0.11192514032976227
Loss in iteration 9 : 0.1268502387796437
Loss in iteration 10 : 0.1256765756300938
Loss in iteration 11 : 0.11411490769476929
Loss in iteration 12 : 0.10269342212609259
Loss in iteration 13 : 0.09234343531567851
Loss in iteration 14 : 0.08587995204760007
Loss in iteration 15 : 0.0813994812108671
Loss in iteration 16 : 0.07763012522782532
Loss in iteration 17 : 0.07442091355054256
Loss in iteration 18 : 0.07056149917143305
Loss in iteration 19 : 0.06630515284736851
Loss in iteration 20 : 0.061541264719465624
Loss in iteration 21 : 0.056584302877894135
Loss in iteration 22 : 0.05210672175877779
Loss in iteration 23 : 0.04840736823841189
Loss in iteration 24 : 0.04525158219659061
Loss in iteration 25 : 0.042728025229923215
Loss in iteration 26 : 0.04035766640682932
Loss in iteration 27 : 0.03788544767091529
Loss in iteration 28 : 0.03538458581122106
Loss in iteration 29 : 0.032861448264175
Loss in iteration 30 : 0.030495426223039867
Loss in iteration 31 : 0.028524349395117266
Loss in iteration 32 : 0.02667402983620213
Loss in iteration 33 : 0.02476477964593661
Loss in iteration 34 : 0.02284860471700415
Loss in iteration 35 : 0.02112351754452622
Loss in iteration 36 : 0.01952360462466431
Loss in iteration 37 : 0.01811160048972507
Loss in iteration 38 : 0.016959883146736892
Loss in iteration 39 : 0.015920263590342088
Loss in iteration 40 : 0.014949908326048362
Loss in iteration 41 : 0.014068122252008042
Loss in iteration 42 : 0.013292597653898742
Loss in iteration 43 : 0.012553685081091337
Loss in iteration 44 : 0.011894764250774952
Loss in iteration 45 : 0.011290486598256445
Loss in iteration 46 : 0.010740369662128029
Loss in iteration 47 : 0.010239189868228847
Loss in iteration 48 : 0.009780965110808056
Loss in iteration 49 : 0.009386861682654773
Loss in iteration 50 : 0.009029667761508774
Loss in iteration 51 : 0.008707743806998645
Loss in iteration 52 : 0.00842807149823466
Loss in iteration 53 : 0.008195727563289188
Loss in iteration 54 : 0.007985422261299632
Loss in iteration 55 : 0.007790533517243601
Loss in iteration 56 : 0.007601004477410523
Loss in iteration 57 : 0.007416390085993815
Loss in iteration 58 : 0.007244276628302354
Loss in iteration 59 : 0.007084585985484133
Loss in iteration 60 : 0.006929088150448483
Loss in iteration 61 : 0.006777421596037715
Loss in iteration 62 : 0.006636043591388866
Loss in iteration 63 : 0.0064969880136559275
Loss in iteration 64 : 0.0063596962342967345
Loss in iteration 65 : 0.006223273280641788
Loss in iteration 66 : 0.006090406045046178
Loss in iteration 67 : 0.00596297545972172
Loss in iteration 68 : 0.005848663878581647
Loss in iteration 69 : 0.005746174605048722
Loss in iteration 70 : 0.005646949861548732
Loss in iteration 71 : 0.005555053855199436
Loss in iteration 72 : 0.005471279782649452
Loss in iteration 73 : 0.005395658151248025
Loss in iteration 74 : 0.005322529557928743
Loss in iteration 75 : 0.005250348098780672
Loss in iteration 76 : 0.005180589096147061
Loss in iteration 77 : 0.005111924577222324
Loss in iteration 78 : 0.0050454857995317105
Loss in iteration 79 : 0.004979998564594172
Loss in iteration 80 : 0.004915196344124181
Loss in iteration 81 : 0.004852676995225128
Loss in iteration 82 : 0.004790298619779654
Loss in iteration 83 : 0.004730416322079406
Loss in iteration 84 : 0.004672852890837871
Loss in iteration 85 : 0.004613957868912498
Loss in iteration 86 : 0.004555373036402615
Loss in iteration 87 : 0.0044965563876871355
Loss in iteration 88 : 0.004438654153774203
Loss in iteration 89 : 0.0043808816714269345
Loss in iteration 90 : 0.004323000262270305
Loss in iteration 91 : 0.004265298556383876
Loss in iteration 92 : 0.004207829326946928
Loss in iteration 93 : 0.00415168209887197
Loss in iteration 94 : 0.0040955042729078055
Loss in iteration 95 : 0.004039878384099316
Loss in iteration 96 : 0.003984635243430704
Loss in iteration 97 : 0.0039295279208625975
Loss in iteration 98 : 0.003874405807059136
Loss in iteration 99 : 0.003819484785454469
Loss in iteration 100 : 0.0037644219455827276
Loss in iteration 101 : 0.003709231469270624
Loss in iteration 102 : 0.003658275062946921
Loss in iteration 103 : 0.0036078488134662196
Loss in iteration 104 : 0.003557217539310027
Loss in iteration 105 : 0.0035068994507143313
Loss in iteration 106 : 0.0034569338573823384
Loss in iteration 107 : 0.0034079655323170205
Loss in iteration 108 : 0.003359124421086943
Loss in iteration 109 : 0.0033100784067431684
Loss in iteration 110 : 0.003261259512284168
Loss in iteration 111 : 0.003213006683836806
Loss in iteration 112 : 0.0031642023532710162
Loss in iteration 113 : 0.0031154053717278203
Loss in iteration 114 : 0.003067940062029944
Loss in iteration 115 : 0.003020336930324029
Loss in iteration 116 : 0.002969863309119166
Loss in iteration 117 : 0.0029216575871489356
Loss in iteration 118 : 0.0028731827887845114
Loss in iteration 119 : 0.002826673052577378
Loss in iteration 120 : 0.0027811532206461573
Loss in iteration 121 : 0.0027361258598824446
Loss in iteration 122 : 0.0026915090486124074
Loss in iteration 123 : 0.0026474237288094497
Loss in iteration 124 : 0.002608033018939354
Loss in iteration 125 : 0.002562881595917192
Loss in iteration 126 : 0.0025189692766069095
Loss in iteration 127 : 0.0024780944753303646
Loss in iteration 128 : 0.0024353755854129495
Loss in iteration 129 : 0.0023935360291073944
Loss in iteration 130 : 0.0023516257993912407
Loss in iteration 131 : 0.002309383546661786
Loss in iteration 132 : 0.002267627588406754
Loss in iteration 133 : 0.0022260026816347464
Loss in iteration 134 : 0.0021845094116970734
Loss in iteration 135 : 0.0021432160202821816
Loss in iteration 136 : 0.002103630918135214
Loss in iteration 137 : 0.0020647702731206178
Loss in iteration 138 : 0.0020213553571444202
Loss in iteration 139 : 0.0019842491434026947
Loss in iteration 140 : 0.0019438204387357764
Loss in iteration 141 : 0.0019015160210957439
Loss in iteration 142 : 0.0018615048986495946
Loss in iteration 143 : 0.0018205549545159013
Loss in iteration 144 : 0.001780106498448417
Loss in iteration 145 : 0.0017403223966765697
Loss in iteration 146 : 0.001702260348610309
Loss in iteration 147 : 0.001662036677360269
Loss in iteration 148 : 0.0016222431981094416
Loss in iteration 149 : 0.001580807170672542
Loss in iteration 150 : 0.0015400055472810316
Loss in iteration 151 : 0.0015017834578396592
Loss in iteration 152 : 0.001464061319162521
Loss in iteration 153 : 0.001424752088866417
Testing accuracy  of updater 2 on alg 1 with rate 0.49 = 0.984, training accuracy 0.999285510145756, time elapsed: 6290 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.41205189373209483
Loss in iteration 3 : 0.5570691569866781
Loss in iteration 4 : 0.29298237567862695
Loss in iteration 5 : 0.25503336784766373
Loss in iteration 6 : 0.19638583395167075
Loss in iteration 7 : 0.1215248811473337
Loss in iteration 8 : 0.10618651455802687
Loss in iteration 9 : 0.10711140038092555
Loss in iteration 10 : 0.09709835761485872
Loss in iteration 11 : 0.09185650677747807
Loss in iteration 12 : 0.08822415764878265
Loss in iteration 13 : 0.08514704961658189
Loss in iteration 14 : 0.08238321655951701
Loss in iteration 15 : 0.07974207444331598
Loss in iteration 16 : 0.07696618071362703
Loss in iteration 17 : 0.07402043298933429
Loss in iteration 18 : 0.0708286050549988
Loss in iteration 19 : 0.06764508050331251
Loss in iteration 20 : 0.06433580574860213
Loss in iteration 21 : 0.06111502374537962
Loss in iteration 22 : 0.05803650137995413
Loss in iteration 23 : 0.05503015553702023
Loss in iteration 24 : 0.05232383317949985
Loss in iteration 25 : 0.049899068185980035
Loss in iteration 26 : 0.04765006159650093
Loss in iteration 27 : 0.045506802182643434
Loss in iteration 28 : 0.04351740201062503
Loss in iteration 29 : 0.04160979361168266
Loss in iteration 30 : 0.03988193811829664
Loss in iteration 31 : 0.038232948040486825
Loss in iteration 32 : 0.036591015611120235
Loss in iteration 33 : 0.03498384083588167
Loss in iteration 34 : 0.0333129733212484
Loss in iteration 35 : 0.03156884306003638
Loss in iteration 36 : 0.029805445152198205
Loss in iteration 37 : 0.02805070524269787
Loss in iteration 38 : 0.026342501574230587
Loss in iteration 39 : 0.024706783263607344
Loss in iteration 40 : 0.023207206762725432
Loss in iteration 41 : 0.02180962337980413
Loss in iteration 42 : 0.020557940783591383
Loss in iteration 43 : 0.01940192001543889
Loss in iteration 44 : 0.018323940334243838
Loss in iteration 45 : 0.017318507408680763
Loss in iteration 46 : 0.016413761519562434
Loss in iteration 47 : 0.015637543554771295
Loss in iteration 48 : 0.014957993015621921
Loss in iteration 49 : 0.014350262897833296
Loss in iteration 50 : 0.013813320497638708
Loss in iteration 51 : 0.013348252699683446
Loss in iteration 52 : 0.012937160076787888
Loss in iteration 53 : 0.012561592247086186
Loss in iteration 54 : 0.012208021841484095
Loss in iteration 55 : 0.011872033175811845
Loss in iteration 56 : 0.011581389713924648
Loss in iteration 57 : 0.011297277498854003
Loss in iteration 58 : 0.011011061019087533
Loss in iteration 59 : 0.010726662009719065
Loss in iteration 60 : 0.010443366401131155
Loss in iteration 61 : 0.010164070698618573
Loss in iteration 62 : 0.009892440271171638
Loss in iteration 63 : 0.009626137176373424
Loss in iteration 64 : 0.009377956900904878
Loss in iteration 65 : 0.009141587366386268
Loss in iteration 66 : 0.008908696421139065
Loss in iteration 67 : 0.00869199562155544
Loss in iteration 68 : 0.008489002479312025
Loss in iteration 69 : 0.008294581402783436
Loss in iteration 70 : 0.008106857689228924
Loss in iteration 71 : 0.007923054231406565
Loss in iteration 72 : 0.007743344766441726
Loss in iteration 73 : 0.007569027210368695
Loss in iteration 74 : 0.00739809503462294
Loss in iteration 75 : 0.007230515780853952
Loss in iteration 76 : 0.007075945017462404
Loss in iteration 77 : 0.006929377923853421
Loss in iteration 78 : 0.006786714741655955
Loss in iteration 79 : 0.006660886828111302
Loss in iteration 80 : 0.006552421535405851
Loss in iteration 81 : 0.006450408461261066
Loss in iteration 82 : 0.0063528334937173365
Loss in iteration 83 : 0.006260463286122491
Loss in iteration 84 : 0.006168232255046341
Loss in iteration 85 : 0.006076188059962426
Loss in iteration 86 : 0.005987239708428016
Loss in iteration 87 : 0.0059038795010484
Loss in iteration 88 : 0.00582416269213243
Loss in iteration 89 : 0.005748577085212561
Loss in iteration 90 : 0.005673855489490769
Loss in iteration 91 : 0.0056034866881094405
Loss in iteration 92 : 0.0055338261761451865
Loss in iteration 93 : 0.00546710837082872
Loss in iteration 94 : 0.005400864768236667
Loss in iteration 95 : 0.005335499214505734
Loss in iteration 96 : 0.005272103869384599
Loss in iteration 97 : 0.005211081634249167
Loss in iteration 98 : 0.005152414566757018
Loss in iteration 99 : 0.00509398789920273
Loss in iteration 100 : 0.005036037202568507
Loss in iteration 101 : 0.004980881767773294
Loss in iteration 102 : 0.004927776023006733
Loss in iteration 103 : 0.004876622932612591
Loss in iteration 104 : 0.004826195965944806
Loss in iteration 105 : 0.004777048774439344
Loss in iteration 106 : 0.004728339099398768
Loss in iteration 107 : 0.004680182137304033
Loss in iteration 108 : 0.004633698893626683
Loss in iteration 109 : 0.0045876880179555385
Loss in iteration 110 : 0.004542156070773875
Loss in iteration 111 : 0.004498655128305938
Loss in iteration 112 : 0.004456571207262683
Loss in iteration 113 : 0.004415019121158273
Loss in iteration 114 : 0.004373943267387204
Loss in iteration 115 : 0.004333483278812552
Loss in iteration 116 : 0.00429340486944213
Loss in iteration 117 : 0.004253710752355609
Loss in iteration 118 : 0.00421424724817529
Loss in iteration 119 : 0.004176784081211784
Loss in iteration 120 : 0.0041404242789713445
Loss in iteration 121 : 0.004104354916572096
Loss in iteration 122 : 0.004068202521036285
Loss in iteration 123 : 0.004033185719343421
Loss in iteration 124 : 0.003999569205252035
Loss in iteration 125 : 0.003962445906900988
Loss in iteration 126 : 0.003927189959173857
Loss in iteration 127 : 0.003891412401637611
Loss in iteration 128 : 0.003856600661804997
Loss in iteration 129 : 0.003823734029364042
Loss in iteration 130 : 0.003793719254499967
Loss in iteration 131 : 0.0037640267491177806
Loss in iteration 132 : 0.0037338698120625074
Loss in iteration 133 : 0.0037041424698703584
Loss in iteration 134 : 0.00367443023854781
Loss in iteration 135 : 0.0036452859461100916
Loss in iteration 136 : 0.0036164629639179337
Loss in iteration 137 : 0.0035879884735257267
Loss in iteration 138 : 0.0035590641024037728
Loss in iteration 139 : 0.0035304024144276686
Loss in iteration 140 : 0.0035021307995375026
Loss in iteration 141 : 0.003474237300701181
Loss in iteration 142 : 0.003446307986309355
Loss in iteration 143 : 0.003418968822264673
Loss in iteration 144 : 0.003391454328383114
Loss in iteration 145 : 0.00336410524965125
Loss in iteration 146 : 0.003336982106574123
Loss in iteration 147 : 0.0033102474298560185
Loss in iteration 148 : 0.003283462354911681
Loss in iteration 149 : 0.003256797456616021
Loss in iteration 150 : 0.003231429142921008
Loss in iteration 151 : 0.0032043987480863028
Loss in iteration 152 : 0.0031781806844054837
Loss in iteration 153 : 0.003152307709040333
Loss in iteration 154 : 0.0031259018480029005
Loss in iteration 155 : 0.003099696738439539
Loss in iteration 156 : 0.0030735947875517922
Loss in iteration 157 : 0.003047406623353339
Loss in iteration 158 : 0.003021652234018023
Loss in iteration 159 : 0.0029953599821051415
Loss in iteration 160 : 0.002970315614470844
Loss in iteration 161 : 0.0029461535330246457
Loss in iteration 162 : 0.00292272635440362
Loss in iteration 163 : 0.0028996278781239088
Loss in iteration 164 : 0.002876318802404602
Loss in iteration 165 : 0.0028533615197561366
Loss in iteration 166 : 0.0028305140347902953
Loss in iteration 167 : 0.002807812608118414
Loss in iteration 168 : 0.0027849369403136794
Loss in iteration 169 : 0.0027621149360088043
Loss in iteration 170 : 0.002739257989661034
Loss in iteration 171 : 0.0027166686810834745
Loss in iteration 172 : 0.0026937439631999137
Loss in iteration 173 : 0.0026711949399070975
Loss in iteration 174 : 0.002649090077995152
Loss in iteration 175 : 0.002627225618035392
Loss in iteration 176 : 0.0026048520060655417
Loss in iteration 177 : 0.0025825102086668786
Loss in iteration 178 : 0.002560863557511336
Loss in iteration 179 : 0.002539120668929543
Loss in iteration 180 : 0.0025167231529142015
Loss in iteration 181 : 0.002494726999801851
Loss in iteration 182 : 0.002472490198041789
Loss in iteration 183 : 0.0024502937854754404
Loss in iteration 184 : 0.00242827417368631
Loss in iteration 185 : 0.002406041553160873
Loss in iteration 186 : 0.0023841959110429225
Loss in iteration 187 : 0.002363797737995247
Loss in iteration 188 : 0.002340726950217108
Loss in iteration 189 : 0.0023205747816171856
Loss in iteration 190 : 0.0022965719874638783
Loss in iteration 191 : 0.0022750081197392283
Loss in iteration 192 : 0.0022533500696540545
Loss in iteration 193 : 0.002231095466750585
Loss in iteration 194 : 0.0022091756457329904
Loss in iteration 195 : 0.002187619875611454
Loss in iteration 196 : 0.0021674001981502166
Loss in iteration 197 : 0.0021461133355437567
Loss in iteration 198 : 0.0021238009666835088
Loss in iteration 199 : 0.002102248805225662
Loss in iteration 200 : 0.0020793939005505197
Testing accuracy  of updater 2 on alg 1 with rate 0.27999999999999997 = 0.9866666666666667, training accuracy 0.9987139182623607, time elapsed: 7433 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8200428123354024
Loss in iteration 3 : 0.5633694758289338
Loss in iteration 4 : 0.40366371441817295
Loss in iteration 5 : 0.2388523721051568
Loss in iteration 6 : 0.20545842995367808
Loss in iteration 7 : 0.18839672878611344
Loss in iteration 8 : 0.17453669484912893
Loss in iteration 9 : 0.16036790252122174
Loss in iteration 10 : 0.14627064676222137
Loss in iteration 11 : 0.13445660056788686
Loss in iteration 12 : 0.12495851409844042
Loss in iteration 13 : 0.11672036462178037
Loss in iteration 14 : 0.1098436040895029
Loss in iteration 15 : 0.1043246153515001
Loss in iteration 16 : 0.09982170336176818
Loss in iteration 17 : 0.09627229239002513
Loss in iteration 18 : 0.09332606971232515
Loss in iteration 19 : 0.09077335390944252
Loss in iteration 20 : 0.08839405096607489
Loss in iteration 21 : 0.0861545451101782
Loss in iteration 22 : 0.08400440890882355
Loss in iteration 23 : 0.08195599986373675
Loss in iteration 24 : 0.07993281148990908
Loss in iteration 25 : 0.07792508501674512
Loss in iteration 26 : 0.07593953907067769
Loss in iteration 27 : 0.07396895708513072
Loss in iteration 28 : 0.07199264497362218
Loss in iteration 29 : 0.0700507860019919
Loss in iteration 30 : 0.06816471176212725
Loss in iteration 31 : 0.06630462039027375
Loss in iteration 32 : 0.06450334009738318
Loss in iteration 33 : 0.06277058218326344
Loss in iteration 34 : 0.0611274204318821
Loss in iteration 35 : 0.0595498878049557
Loss in iteration 36 : 0.05804289098669027
Loss in iteration 37 : 0.056636021770713134
Loss in iteration 38 : 0.0552798136399122
Loss in iteration 39 : 0.05396518769041793
Loss in iteration 40 : 0.05271003846088097
Loss in iteration 41 : 0.051577154989095125
Loss in iteration 42 : 0.05051103853320855
Loss in iteration 43 : 0.049476935302868494
Loss in iteration 44 : 0.04854731759462464
Loss in iteration 45 : 0.047654970962753694
Loss in iteration 46 : 0.04680695516808074
Loss in iteration 47 : 0.0460153842610464
Loss in iteration 48 : 0.04525509843452721
Loss in iteration 49 : 0.04451843015229841
Loss in iteration 50 : 0.04379540534409362
Loss in iteration 51 : 0.04308532909772825
Loss in iteration 52 : 0.04238554263497151
Loss in iteration 53 : 0.04170501033482934
Loss in iteration 54 : 0.04103578638729574
Loss in iteration 55 : 0.04037689203537072
Loss in iteration 56 : 0.03972120548358198
Loss in iteration 57 : 0.03907311736879289
Loss in iteration 58 : 0.03843039144340693
Loss in iteration 59 : 0.03778816499613493
Loss in iteration 60 : 0.037159134059784334
Loss in iteration 61 : 0.03653246216957323
Loss in iteration 62 : 0.035916158551508516
Loss in iteration 63 : 0.03531135954330077
Loss in iteration 64 : 0.034714418059967965
Loss in iteration 65 : 0.03412108305731602
Loss in iteration 66 : 0.033533477581700034
Loss in iteration 67 : 0.03295623696001427
Loss in iteration 68 : 0.03238928652963771
Loss in iteration 69 : 0.031830571643778026
Loss in iteration 70 : 0.031278698478729564
Loss in iteration 71 : 0.030731793429642752
Loss in iteration 72 : 0.030197541130197597
Loss in iteration 73 : 0.029675686908946827
Loss in iteration 74 : 0.029166838799633967
Loss in iteration 75 : 0.028665189137486606
Loss in iteration 76 : 0.02817413516576792
Loss in iteration 77 : 0.027696975246637293
Loss in iteration 78 : 0.027228104379732724
Loss in iteration 79 : 0.026771756632223434
Loss in iteration 80 : 0.0263237822569632
Loss in iteration 81 : 0.025882204153807792
Loss in iteration 82 : 0.02544441426571817
Loss in iteration 83 : 0.025016368937994478
Loss in iteration 84 : 0.0246005428315478
Loss in iteration 85 : 0.02419040710286961
Loss in iteration 86 : 0.023783792331355285
Loss in iteration 87 : 0.023380802772122275
Loss in iteration 88 : 0.022981251833988525
Loss in iteration 89 : 0.022585684919468
Loss in iteration 90 : 0.02219654284118886
Loss in iteration 91 : 0.021813232603102823
Loss in iteration 92 : 0.021432821597515316
Loss in iteration 93 : 0.0210566789722052
Loss in iteration 94 : 0.020682942812552542
Loss in iteration 95 : 0.020311830030583927
Loss in iteration 96 : 0.019944353139985552
Loss in iteration 97 : 0.019579784254187327
Loss in iteration 98 : 0.01921904311215782
Loss in iteration 99 : 0.0188622565860342
Loss in iteration 100 : 0.01851077281311642
Loss in iteration 101 : 0.018167686391629086
Loss in iteration 102 : 0.017835584856228232
Loss in iteration 103 : 0.017514187493385708
Loss in iteration 104 : 0.017205230984981033
Loss in iteration 105 : 0.01690443866045902
Loss in iteration 106 : 0.016606918571271143
Loss in iteration 107 : 0.016314258125953527
Loss in iteration 108 : 0.016030579617015494
Loss in iteration 109 : 0.015760520588276393
Loss in iteration 110 : 0.015502986656925942
Loss in iteration 111 : 0.015258379552207683
Loss in iteration 112 : 0.015029911442468303
Loss in iteration 113 : 0.014808294644443306
Loss in iteration 114 : 0.014593728666204045
Loss in iteration 115 : 0.014385300774797964
Loss in iteration 116 : 0.014185577116965605
Loss in iteration 117 : 0.013994675705111906
Loss in iteration 118 : 0.013807308446729878
Loss in iteration 119 : 0.013624673260280759
Loss in iteration 120 : 0.013447145790622176
Loss in iteration 121 : 0.013271193729607198
Loss in iteration 122 : 0.013098146601931458
Loss in iteration 123 : 0.012929665136544382
Loss in iteration 124 : 0.012766282215928841
Loss in iteration 125 : 0.012605588346479086
Loss in iteration 126 : 0.012447559834268346
Loss in iteration 127 : 0.012292697204153524
Loss in iteration 128 : 0.012140534875759116
Loss in iteration 129 : 0.011991334282313826
Loss in iteration 130 : 0.011846108576232112
Loss in iteration 131 : 0.011704240757223875
Loss in iteration 132 : 0.011568800241277984
Loss in iteration 133 : 0.011437783195533393
Loss in iteration 134 : 0.011311303838352584
Loss in iteration 135 : 0.011186978996662697
Loss in iteration 136 : 0.011065565179854278
Loss in iteration 137 : 0.010947412263558508
Loss in iteration 138 : 0.010831219610601338
Loss in iteration 139 : 0.010718086926031482
Loss in iteration 140 : 0.010609952580275653
Loss in iteration 141 : 0.010505873229396133
Loss in iteration 142 : 0.01040452150811118
Loss in iteration 143 : 0.010304423934131932
Loss in iteration 144 : 0.010207714556614587
Loss in iteration 145 : 0.010113510344046511
Loss in iteration 146 : 0.010020595527989975
Loss in iteration 147 : 0.0099297221182092
Loss in iteration 148 : 0.009841788753627461
Loss in iteration 149 : 0.009755335609084039
Loss in iteration 150 : 0.009669105403733764
Loss in iteration 151 : 0.009583842155743545
Loss in iteration 152 : 0.009499979400763645
Loss in iteration 153 : 0.009417199606561318
Loss in iteration 154 : 0.009334997065701692
Loss in iteration 155 : 0.00925326328976459
Loss in iteration 156 : 0.009171858286526816
Loss in iteration 157 : 0.009091353568665554
Loss in iteration 158 : 0.00901302345522511
Loss in iteration 159 : 0.008936425527114409
Loss in iteration 160 : 0.008860342836658238
Loss in iteration 161 : 0.00878493477172853
Loss in iteration 162 : 0.008711488782557076
Loss in iteration 163 : 0.00863924378766503
Loss in iteration 164 : 0.008567538785998675
Loss in iteration 165 : 0.008496039705554369
Loss in iteration 166 : 0.00842473060184595
Loss in iteration 167 : 0.008353627802834777
Loss in iteration 168 : 0.008282905931826482
Loss in iteration 169 : 0.008213112010722227
Loss in iteration 170 : 0.008144472339234972
Loss in iteration 171 : 0.008076395185137985
Loss in iteration 172 : 0.008008886767201004
Loss in iteration 173 : 0.007941632817174128
Loss in iteration 174 : 0.007875009146682998
Loss in iteration 175 : 0.007809605540190834
Loss in iteration 176 : 0.007744549256350188
Loss in iteration 177 : 0.007679642351680984
Loss in iteration 178 : 0.007616249396207278
Loss in iteration 179 : 0.007552928461772021
Loss in iteration 180 : 0.007490509914209359
Loss in iteration 181 : 0.007428507655061746
Loss in iteration 182 : 0.007366677053571689
Loss in iteration 183 : 0.007305168084890987
Loss in iteration 184 : 0.007243868704206094
Loss in iteration 185 : 0.007182866051676698
Loss in iteration 186 : 0.007122589483407733
Loss in iteration 187 : 0.007062442418404209
Loss in iteration 188 : 0.007002750747280012
Loss in iteration 189 : 0.006943179029394808
Loss in iteration 190 : 0.006883766477760506
Loss in iteration 191 : 0.006825404857500395
Loss in iteration 192 : 0.006769615661272259
Loss in iteration 193 : 0.0067155110897595275
Loss in iteration 194 : 0.006662138356552708
Loss in iteration 195 : 0.006609631545245039
Loss in iteration 196 : 0.006557198314725989
Loss in iteration 197 : 0.006505033229466826
Loss in iteration 198 : 0.006453304110678762
Loss in iteration 199 : 0.006401722966984168
Loss in iteration 200 : 0.006350147765011627
Testing accuracy  of updater 2 on alg 1 with rate 0.06999999999999995 = 0.9742222222222222, training accuracy 0.998428122320663, time elapsed: 7542 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 48.78732955364872
Loss in iteration 3 : 5.825503660664924
Loss in iteration 4 : 3.098331879524825
Loss in iteration 5 : 2.340109625483003
Loss in iteration 6 : 1.0484943274646659
Loss in iteration 7 : 0.7839382292685946
Loss in iteration 8 : 0.622651382020842
Loss in iteration 9 : 0.5133022987249037
Loss in iteration 10 : 0.415486094508399
Loss in iteration 11 : 0.3483050285743925
Loss in iteration 12 : 0.30643849094406506
Loss in iteration 13 : 0.2761918982061904
Loss in iteration 14 : 0.2480481453123284
Loss in iteration 15 : 0.22127479237792613
Loss in iteration 16 : 0.20028944410962024
Loss in iteration 17 : 0.18605210919405038
Loss in iteration 18 : 0.17352905322214507
Loss in iteration 19 : 0.1617657389697134
Loss in iteration 20 : 0.15204914793291488
Loss in iteration 21 : 0.1435522904754629
Loss in iteration 22 : 0.13690008100336182
Loss in iteration 23 : 0.13103380562942551
Loss in iteration 24 : 0.1267426822467167
Loss in iteration 25 : 0.1233087463795923
Loss in iteration 26 : 0.12027537655043925
Loss in iteration 27 : 0.11733029276552065
Loss in iteration 28 : 0.11463483796708476
Loss in iteration 29 : 0.1119436444427106
Loss in iteration 30 : 0.1094673457273012
Loss in iteration 31 : 0.10680691049550274
Loss in iteration 32 : 0.1048430492142853
Loss in iteration 33 : 0.10250722269955284
Loss in iteration 34 : 0.10049911874849006
Loss in iteration 35 : 0.09857703683693181
Loss in iteration 36 : 0.09664484300496369
Loss in iteration 37 : 0.09486573065596252
Loss in iteration 38 : 0.09311545474258738
Loss in iteration 39 : 0.09138582307763242
Loss in iteration 40 : 0.08975999767802541
Loss in iteration 41 : 0.08797281057119036
Loss in iteration 42 : 0.08624326459255048
Loss in iteration 43 : 0.08460584831982347
Loss in iteration 44 : 0.08283085217820445
Loss in iteration 45 : 0.08121314021910622
Loss in iteration 46 : 0.07979176986485756
Loss in iteration 47 : 0.07845797552229036
Testing accuracy  of updater 3 on alg 1 with rate 40.0 = 0.9742222222222222, training accuracy 0.9974278365247213, time elapsed: 1785 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 16.151288007118097
Loss in iteration 3 : 2.315687344386939
Loss in iteration 4 : 1.1119707133664716
Loss in iteration 5 : 0.7717632411372327
Loss in iteration 6 : 0.3114840956635973
Loss in iteration 7 : 0.2208677724873863
Loss in iteration 8 : 0.17357100539137071
Loss in iteration 9 : 0.1457384234612017
Loss in iteration 10 : 0.12774388972582365
Loss in iteration 11 : 0.11093299720993997
Loss in iteration 12 : 0.09765064115180801
Loss in iteration 13 : 0.08720672714074032
Loss in iteration 14 : 0.07759133725280092
Loss in iteration 15 : 0.07036164381703662
Loss in iteration 16 : 0.06404949712854797
Loss in iteration 17 : 0.059177276829449135
Loss in iteration 18 : 0.054771419543513236
Loss in iteration 19 : 0.05196125981478486
Loss in iteration 20 : 0.0501145830230015
Loss in iteration 21 : 0.04691126394733394
Testing accuracy  of updater 3 on alg 1 with rate 28.0 = 0.9582222222222222, training accuracy 0.9958559588453844, time elapsed: 732 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.530329907248895
Loss in iteration 3 : 1.239342068330604
Loss in iteration 4 : 0.847284272487333
Loss in iteration 5 : 0.6134649349325001
Loss in iteration 6 : 0.15393215410073982
Loss in iteration 7 : 0.11150351409931297
Loss in iteration 8 : 0.0888084263766573
Loss in iteration 9 : 0.07342010948769775
Loss in iteration 10 : 0.06151576367060177
Loss in iteration 11 : 0.05379256138410783
Loss in iteration 12 : 0.04794148186745891
Loss in iteration 13 : 0.04299504603540515
Loss in iteration 14 : 0.03872043573425447
Loss in iteration 15 : 0.035288876040585455
Loss in iteration 16 : 0.03212413572393318
Loss in iteration 17 : 0.029486655243112267
Loss in iteration 18 : 0.027256150766493896
Loss in iteration 19 : 0.02533607071226485
Loss in iteration 20 : 0.02415485490800875
Loss in iteration 21 : 0.023304456867714814
Loss in iteration 22 : 0.022527539159794146
Loss in iteration 23 : 0.021803119747456245
Loss in iteration 24 : 0.020972812150022977
Loss in iteration 25 : 0.020364804129793045
Loss in iteration 26 : 0.019735816028483764
Loss in iteration 27 : 0.01924942571666553
Testing accuracy  of updater 3 on alg 1 with rate 16.0 = 0.9608888888888889, training accuracy 0.9965704486996285, time elapsed: 905 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.7071000822799016
Loss in iteration 3 : 0.3593665138919858
Loss in iteration 4 : 0.5915910583014341
Loss in iteration 5 : 0.5266310070996282
Loss in iteration 6 : 0.7652398968804113
Loss in iteration 7 : 0.21556154200427438
Loss in iteration 8 : 0.06688425522181193
Loss in iteration 9 : 0.03933227796774857
Loss in iteration 10 : 0.033029406078488345
Loss in iteration 11 : 0.028447419005918725
Loss in iteration 12 : 0.024465227442423403
Loss in iteration 13 : 0.020679591520140886
Loss in iteration 14 : 0.01757934458767076
Loss in iteration 15 : 0.015503252173677965
Loss in iteration 16 : 0.01383289462861893
Loss in iteration 17 : 0.012351718107022427
Loss in iteration 18 : 0.011169352036244515
Loss in iteration 19 : 0.010150561014641636
Loss in iteration 20 : 0.009421414030944476
Loss in iteration 21 : 0.00882847150032347
Loss in iteration 22 : 0.008329935500888737
Loss in iteration 23 : 0.007842569130270769
Loss in iteration 24 : 0.007426466303330443
Loss in iteration 25 : 0.007053416832910007
Loss in iteration 26 : 0.00668864021455106
Loss in iteration 27 : 0.006425135785416244
Testing accuracy  of updater 3 on alg 1 with rate 4.0 = 0.9724444444444444, training accuracy 0.9972849385538726, time elapsed: 996 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0272437765809324
Loss in iteration 3 : 0.2012525122407305
Loss in iteration 4 : 0.3047615108314084
Loss in iteration 5 : 0.31825854856427926
Loss in iteration 6 : 0.581688651966946
Loss in iteration 7 : 0.14111009367355148
Loss in iteration 8 : 0.038496260722936826
Loss in iteration 9 : 0.02835116622124964
Loss in iteration 10 : 0.023649412834114675
Loss in iteration 11 : 0.020841086817010905
Loss in iteration 12 : 0.018349804934620143
Loss in iteration 13 : 0.016157546781531897
Loss in iteration 14 : 0.014121483461233464
Loss in iteration 15 : 0.012368266277797725
Loss in iteration 16 : 0.011129628060355458
Loss in iteration 17 : 0.010015461240469615
Loss in iteration 18 : 0.008994417525535942
Loss in iteration 19 : 0.008051599040505857
Loss in iteration 20 : 0.007331323277506785
Loss in iteration 21 : 0.006690525905475669
Loss in iteration 22 : 0.006200076412921869
Loss in iteration 23 : 0.005820250820687726
Loss in iteration 24 : 0.005477874876044997
Loss in iteration 25 : 0.005154128225769874
Loss in iteration 26 : 0.0048829832563022065
Loss in iteration 27 : 0.004631958411374933
Loss in iteration 28 : 0.004432333244848432
Loss in iteration 29 : 0.0042528376929024565
Loss in iteration 30 : 0.004106891115850345
Loss in iteration 31 : 0.003961197946625755
Loss in iteration 32 : 0.0038202491370136382
Loss in iteration 33 : 0.0037062448001819017
Loss in iteration 34 : 0.0035942768261071952
Loss in iteration 35 : 0.0034903376152854307
Loss in iteration 36 : 0.0033904391978009357
Loss in iteration 37 : 0.0032858510309419844
Loss in iteration 38 : 0.003197907696377549
Loss in iteration 39 : 0.0031173511862587393
Loss in iteration 40 : 0.003042757235337022
Loss in iteration 41 : 0.0029722568334586055
Loss in iteration 42 : 0.002903171449704035
Loss in iteration 43 : 0.002834761752834229
Loss in iteration 44 : 0.0027707255924916334
Loss in iteration 45 : 0.002700588417831815
Loss in iteration 46 : 0.0026439671738425464
Loss in iteration 47 : 0.002583525409411791
Loss in iteration 48 : 0.0025262635348595208
Loss in iteration 49 : 0.0024693003283725585
Loss in iteration 50 : 0.002417064491718638
Loss in iteration 51 : 0.002361670732850592
Loss in iteration 52 : 0.002306521193760645
Loss in iteration 53 : 0.0022516902854293236
Loss in iteration 54 : 0.002207709368097688
Testing accuracy  of updater 3 on alg 1 with rate 2.8 = 0.9768888888888889, training accuracy 0.9988568162332095, time elapsed: 2670 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5783127212296497
Loss in iteration 3 : 0.18806153828280428
Loss in iteration 4 : 0.6445752620034475
Loss in iteration 5 : 0.14695434306776792
Loss in iteration 6 : 0.19035205688725174
Loss in iteration 7 : 0.08369958802062705
Loss in iteration 8 : 0.036150154089724544
Loss in iteration 9 : 0.027254047646923518
Loss in iteration 10 : 0.02191197526458571
Loss in iteration 11 : 0.018498097598995445
Loss in iteration 12 : 0.015900154477748493
Loss in iteration 13 : 0.014009663470237911
Loss in iteration 14 : 0.01259972484047244
Loss in iteration 15 : 0.011552795304449016
Loss in iteration 16 : 0.01057075231692168
Loss in iteration 17 : 0.00962464782551276
Loss in iteration 18 : 0.008743456042257687
Loss in iteration 19 : 0.008003582775981
Loss in iteration 20 : 0.007371060991621616
Loss in iteration 21 : 0.006783377200811308
Loss in iteration 22 : 0.006189348462193641
Loss in iteration 23 : 0.005637091733413296
Loss in iteration 24 : 0.0051682720712973125
Loss in iteration 25 : 0.004742287484770457
Loss in iteration 26 : 0.004376712992219966
Loss in iteration 27 : 0.004100321880965701
Loss in iteration 28 : 0.003856313649929156
Loss in iteration 29 : 0.003664681168788807
Testing accuracy  of updater 3 on alg 1 with rate 1.6 = 0.9706666666666667, training accuracy 0.9991426121749071, time elapsed: 1266 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.30782612428581485
Loss in iteration 3 : 1.0423213025777367
Loss in iteration 4 : 0.41168998602417894
Loss in iteration 5 : 0.1550829763305106
Loss in iteration 6 : 0.11979165269647826
Loss in iteration 7 : 0.09595653228408586
Loss in iteration 8 : 0.08474886542141155
Loss in iteration 9 : 0.07360581871507203
Loss in iteration 10 : 0.07594990546577624
Loss in iteration 11 : 0.05352447393582462
Loss in iteration 12 : 0.04989805818998622
Loss in iteration 13 : 0.041837590444082265
Loss in iteration 14 : 0.038182831087255543
Loss in iteration 15 : 0.03330287577372422
Loss in iteration 16 : 0.03018217087509491
Loss in iteration 17 : 0.0276896311785281
Loss in iteration 18 : 0.02581652843519293
Loss in iteration 19 : 0.024009152578144875
Loss in iteration 20 : 0.02230398155400056
Loss in iteration 21 : 0.020902821350430142
Loss in iteration 22 : 0.019607923434972842
Loss in iteration 23 : 0.01849102940094966
Loss in iteration 24 : 0.017509115409505432
Loss in iteration 25 : 0.016620373994734507
Loss in iteration 26 : 0.015774918498679208
Loss in iteration 27 : 0.014991413621024149
Loss in iteration 28 : 0.01429429384863184
Loss in iteration 29 : 0.013685232656391119
Loss in iteration 30 : 0.013126776979698442
Loss in iteration 31 : 0.012619769021957722
Loss in iteration 32 : 0.012136575943461148
Loss in iteration 33 : 0.011677061406659325
Loss in iteration 34 : 0.011249947765086305
Loss in iteration 35 : 0.010865056754825127
Loss in iteration 36 : 0.010526741417920161
Loss in iteration 37 : 0.010219422859223972
Loss in iteration 38 : 0.009918202710646005
Loss in iteration 39 : 0.009634039701271174
Loss in iteration 40 : 0.009390961213370488
Loss in iteration 41 : 0.009070042990655352
Loss in iteration 42 : 0.008834202518170015
Loss in iteration 43 : 0.008530362278755355
Loss in iteration 44 : 0.008321064643633901
Loss in iteration 45 : 0.008007989585904597
Loss in iteration 46 : 0.0078038583381554545
Loss in iteration 47 : 0.007523769594776279
Loss in iteration 48 : 0.007331278336977809
Loss in iteration 49 : 0.007062107864480431
Loss in iteration 50 : 0.006871663972418418
Loss in iteration 51 : 0.006663598751380637
Testing accuracy  of updater 3 on alg 1 with rate 0.3999999999999999 = 0.9866666666666667, training accuracy 0.9994284081166047, time elapsed: 3505 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975479612481092
Loss in iteration 3 : 0.9950322333076048
Loss in iteration 4 : 0.9924727961555131
Loss in iteration 5 : 0.989879433835418
Loss in iteration 6 : 0.9872579206900947
Loss in iteration 7 : 0.9846120495765806
Loss in iteration 8 : 0.9819444913370551
Loss in iteration 9 : 0.9792572208774807
Loss in iteration 10 : 0.9765517522691183
Loss in iteration 11 : 0.9738292790190761
Loss in iteration 12 : 0.971090762912426
Loss in iteration 13 : 0.9683369930016623
Loss in iteration 14 : 0.9655686263032196
Loss in iteration 15 : 0.9627862167776796
Loss in iteration 16 : 0.9599902365242617
Loss in iteration 17 : 0.9571810916376241
Loss in iteration 18 : 0.9543591343059925
Loss in iteration 19 : 0.9515246722002975
Loss in iteration 20 : 0.9486779758706282
Loss in iteration 21 : 0.9458192846502996
Loss in iteration 22 : 0.94294881142408
Loss in iteration 23 : 0.9400667465194705
Loss in iteration 24 : 0.9371732609120244
Loss in iteration 25 : 0.9342685088878165
Loss in iteration 26 : 0.9313526302716557
Loss in iteration 27 : 0.9284257523045572
Loss in iteration 28 : 0.925487991235358
Loss in iteration 29 : 0.9225394536774878
Loss in iteration 30 : 0.9195802377713136
Loss in iteration 31 : 0.9166104341844653
Loss in iteration 32 : 0.9136301269762293
Loss in iteration 33 : 0.9106393943472861
Loss in iteration 34 : 0.9076383092921944
Loss in iteration 35 : 0.9046269401689891
Loss in iteration 36 : 0.9016053511977964
Loss in iteration 37 : 0.8985736028984481
Loss in iteration 38 : 0.8955317524754294
Loss in iteration 39 : 0.8924798541572285
Loss in iteration 40 : 0.8894179594960614
Loss in iteration 41 : 0.8863461176330928
Loss in iteration 42 : 0.8832643755334691
Loss in iteration 43 : 0.880172778194964
Loss in iteration 44 : 0.877071368833413
Loss in iteration 45 : 0.8739601890477618
Loss in iteration 46 : 0.8708392789671417
Loss in iteration 47 : 0.8677086773820696
Loss in iteration 48 : 0.8645684218616234
Loss in iteration 49 : 0.8614185488581949
Loss in iteration 50 : 0.8582590938012205
Loss in iteration 51 : 0.8550900911811472
Loss in iteration 52 : 0.8519115746246982
Loss in iteration 53 : 0.8487235769624152
Loss in iteration 54 : 0.8455261302893157
Loss in iteration 55 : 0.8423192660194206
Loss in iteration 56 : 0.8391030149348122
Loss in iteration 57 : 0.8358774072297956
Loss in iteration 58 : 0.8326424725507212
Loss in iteration 59 : 0.8293982400318961
Loss in iteration 60 : 0.8261447383280074
Loss in iteration 61 : 0.8228819956434407
Loss in iteration 62 : 0.8196100397587865
Loss in iteration 63 : 0.8163288980548684
Loss in iteration 64 : 0.8130385975345078
Loss in iteration 65 : 0.8097391648422936
Loss in iteration 66 : 0.8064306262825424
Loss in iteration 67 : 0.8031130078356398
Loss in iteration 68 : 0.7997863351729336
Loss in iteration 69 : 0.796450633670317
Loss in iteration 70 : 0.7931059284206402
Loss in iteration 71 : 0.7897522442450646
Loss in iteration 72 : 0.7863896057034743
Loss in iteration 73 : 0.7830180371040264
Loss in iteration 74 : 0.7796375625119359
Loss in iteration 75 : 0.7762482057575628
Loss in iteration 76 : 0.7728499904438836
Loss in iteration 77 : 0.7694429399533872
Loss in iteration 78 : 0.766027077454468
Loss in iteration 79 : 0.7626024259073629
Loss in iteration 80 : 0.7591690080696658
Loss in iteration 81 : 0.7557268465014654
Loss in iteration 82 : 0.7522759635701524
Loss in iteration 83 : 0.7488163814549087
Loss in iteration 84 : 0.7453481221509194
Loss in iteration 85 : 0.7418712074733346
Loss in iteration 86 : 0.7383856590609948
Loss in iteration 87 : 0.7348914983799548
Loss in iteration 88 : 0.7313887467268049
Loss in iteration 89 : 0.7278774252318317
Loss in iteration 90 : 0.7243575548620056
Loss in iteration 91 : 0.720829156423839
Loss in iteration 92 : 0.717292250566094
Loss in iteration 93 : 0.7137468577823854
Loss in iteration 94 : 0.7101929984136613
Loss in iteration 95 : 0.7066306926505824
Loss in iteration 96 : 0.7030599605358115
Loss in iteration 97 : 0.6994808219662065
Loss in iteration 98 : 0.6958932966949499
Loss in iteration 99 : 0.6922974043335803
Loss in iteration 100 : 0.6886931643539774
Loss in iteration 101 : 0.6850805960902664
Loss in iteration 102 : 0.6814597187406735
Loss in iteration 103 : 0.6778305513693202
Loss in iteration 104 : 0.6741931129079652
Loss in iteration 105 : 0.670547422157701
Loss in iteration 106 : 0.6668934977906062
Loss in iteration 107 : 0.6632313583513435
Loss in iteration 108 : 0.6595610222587374
Loss in iteration 109 : 0.6558825078072923
Loss in iteration 110 : 0.652195833168691
Loss in iteration 111 : 0.6485010163932499
Loss in iteration 112 : 0.6447980754113448
Loss in iteration 113 : 0.6410870280348077
Loss in iteration 114 : 0.6373678919582917
Loss in iteration 115 : 0.6336406847606086
Loss in iteration 116 : 0.6299054239060394
Loss in iteration 117 : 0.626162126745623
Loss in iteration 118 : 0.6224108105184135
Loss in iteration 119 : 0.6186514923527208
Loss in iteration 120 : 0.6148841892673238
Loss in iteration 121 : 0.6111089181726667
Loss in iteration 122 : 0.6073256958720276
Loss in iteration 123 : 0.603534539062673
Loss in iteration 124 : 0.5997354643369908
Loss in iteration 125 : 0.5959284881836016
Loss in iteration 126 : 0.5921136269884575
Loss in iteration 127 : 0.5887746220416947
Loss in iteration 128 : 0.5856145256028997
Loss in iteration 129 : 0.5824479333998543
Loss in iteration 130 : 0.579274814340544
Loss in iteration 131 : 0.5760951317512577
Loss in iteration 132 : 0.5729088468924131
Loss in iteration 133 : 0.5697159208420516
Loss in iteration 134 : 0.566516315589082
Loss in iteration 135 : 0.5633099946967912
Loss in iteration 136 : 0.5600969237107037
Loss in iteration 137 : 0.5568770704033169
Loss in iteration 138 : 0.5536504049090315
Loss in iteration 139 : 0.5504168997822806
Loss in iteration 140 : 0.5471765300006273
Loss in iteration 141 : 0.5439292729280102
Loss in iteration 142 : 0.5406751082492303
Loss in iteration 143 : 0.5374140178840927
Loss in iteration 144 : 0.5341459858877636
Loss in iteration 145 : 0.5308717374355844
Loss in iteration 146 : 0.527620628895536
Loss in iteration 147 : 0.5244298321340509
Loss in iteration 148 : 0.5212631581157064
Loss in iteration 149 : 0.5181256321043095
Loss in iteration 150 : 0.5150053790567005
Loss in iteration 151 : 0.5119065708949074
Loss in iteration 152 : 0.508806806599097
Loss in iteration 153 : 0.5057030402901448
Loss in iteration 154 : 0.5025979567277621
Loss in iteration 155 : 0.4994898956318766
Loss in iteration 156 : 0.4963765846180785
Loss in iteration 157 : 0.4932570950636704
Loss in iteration 158 : 0.49013085728337874
Loss in iteration 159 : 0.4869980183880255
Loss in iteration 160 : 0.4838589315268839
Loss in iteration 161 : 0.48071379519537455
Loss in iteration 162 : 0.47756300935916207
Loss in iteration 163 : 0.4744083850505218
Loss in iteration 164 : 0.4712510204445991
Loss in iteration 165 : 0.4680889883036269
Loss in iteration 166 : 0.46492146346045804
Loss in iteration 167 : 0.46174744793099076
Loss in iteration 168 : 0.45856712634431257
Loss in iteration 169 : 0.45538025558349804
Loss in iteration 170 : 0.4521868825313078
Loss in iteration 171 : 0.4489869744549742
Loss in iteration 172 : 0.4457804092941689
Loss in iteration 173 : 0.44256726267648766
Loss in iteration 174 : 0.4393474935107087
Loss in iteration 175 : 0.4361210544763015
Loss in iteration 176 : 0.4328879990210173
Loss in iteration 177 : 0.4296509724762815
Loss in iteration 178 : 0.4264091288572455
Loss in iteration 179 : 0.42316106881265336
Loss in iteration 180 : 0.4199066918626695
Loss in iteration 181 : 0.41664570815500673
Loss in iteration 182 : 0.4133781389952566
Loss in iteration 183 : 0.4101041526520685
Loss in iteration 184 : 0.40682386245703106
Loss in iteration 185 : 0.40353832953693464
Loss in iteration 186 : 0.40025504624799346
Loss in iteration 187 : 0.39697274530255794
Loss in iteration 188 : 0.39369066412008763
Loss in iteration 189 : 0.39042769837453917
Loss in iteration 190 : 0.38720221686387674
Loss in iteration 191 : 0.38397089674374213
Loss in iteration 192 : 0.3807590580667263
Loss in iteration 193 : 0.3775947049975074
Loss in iteration 194 : 0.37444532161853544
Loss in iteration 195 : 0.3712997120899868
Loss in iteration 196 : 0.36820653008791815
Loss in iteration 197 : 0.3651184989071489
Loss in iteration 198 : 0.3620259201462592
Loss in iteration 199 : 0.3589536459719925
Loss in iteration 200 : 0.35591521314951147
Testing accuracy  of updater 4 on alg 1 with rate 10000.0 = 0.9982222222222222, training accuracy 0.8796799085452986, time elapsed: 13658 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975479612481092
Loss in iteration 3 : 0.9950322333076049
Loss in iteration 4 : 0.9924727961555131
Loss in iteration 5 : 0.989879433835418
Loss in iteration 6 : 0.9872579206900947
Loss in iteration 7 : 0.9846120495765806
Loss in iteration 8 : 0.9819444913370551
Loss in iteration 9 : 0.9792572208774805
Loss in iteration 10 : 0.9765517522691183
Loss in iteration 11 : 0.9738292790190762
Loss in iteration 12 : 0.9710907629124258
Loss in iteration 13 : 0.9683369930016623
Loss in iteration 14 : 0.9655686263032193
Loss in iteration 15 : 0.9627862167776797
Loss in iteration 16 : 0.9599902365242617
Loss in iteration 17 : 0.9571810916376241
Loss in iteration 18 : 0.9543591343059925
Loss in iteration 19 : 0.9515246722002975
Loss in iteration 20 : 0.9486779758706282
Loss in iteration 21 : 0.9458192846502997
Loss in iteration 22 : 0.94294881142408
Loss in iteration 23 : 0.9400667465194705
Loss in iteration 24 : 0.9371732609120244
Loss in iteration 25 : 0.9342685088878165
Loss in iteration 26 : 0.9313526302716557
Loss in iteration 27 : 0.928425752304557
Loss in iteration 28 : 0.9254879912353581
Loss in iteration 29 : 0.9225394536774878
Loss in iteration 30 : 0.9195802377713136
Loss in iteration 31 : 0.9166104341844653
Loss in iteration 32 : 0.9136301269762295
Loss in iteration 33 : 0.9106393943472861
Loss in iteration 34 : 0.9076383092921944
Loss in iteration 35 : 0.9046269401689891
Loss in iteration 36 : 0.9016053511977964
Loss in iteration 37 : 0.8985736028984481
Loss in iteration 38 : 0.8955317524754294
Loss in iteration 39 : 0.8924798541572285
Loss in iteration 40 : 0.8894179594960614
Loss in iteration 41 : 0.8863461176330928
Loss in iteration 42 : 0.8832643755334691
Loss in iteration 43 : 0.880172778194964
Loss in iteration 44 : 0.877071368833413
Loss in iteration 45 : 0.8739601890477618
Loss in iteration 46 : 0.8708392789671417
Loss in iteration 47 : 0.8677086773820697
Loss in iteration 48 : 0.8645684218616234
Loss in iteration 49 : 0.8614185488581949
Loss in iteration 50 : 0.8582590938012205
Loss in iteration 51 : 0.8550900911811471
Loss in iteration 52 : 0.8519115746246984
Loss in iteration 53 : 0.8487235769624152
Loss in iteration 54 : 0.8455261302893158
Loss in iteration 55 : 0.8423192660194206
Loss in iteration 56 : 0.8391030149348122
Loss in iteration 57 : 0.8358774072297956
Loss in iteration 58 : 0.8326424725507212
Loss in iteration 59 : 0.8293982400318961
Loss in iteration 60 : 0.8261447383280074
Loss in iteration 61 : 0.8228819956434407
Loss in iteration 62 : 0.8196100397587865
Loss in iteration 63 : 0.8163288980548684
Loss in iteration 64 : 0.8130385975345076
Loss in iteration 65 : 0.8097391648422936
Loss in iteration 66 : 0.8064306262825424
Loss in iteration 67 : 0.8031130078356399
Loss in iteration 68 : 0.7997863351729336
Loss in iteration 69 : 0.796450633670317
Loss in iteration 70 : 0.7931059284206402
Loss in iteration 71 : 0.7897522442450646
Loss in iteration 72 : 0.786389605703474
Loss in iteration 73 : 0.7830180371040264
Loss in iteration 74 : 0.7796375625119359
Loss in iteration 75 : 0.776248205757563
Loss in iteration 76 : 0.7728499904438836
Loss in iteration 77 : 0.7694429399533871
Loss in iteration 78 : 0.7660270774544679
Loss in iteration 79 : 0.7626024259073629
Loss in iteration 80 : 0.7591690080696658
Loss in iteration 81 : 0.7557268465014655
Loss in iteration 82 : 0.7522759635701524
Loss in iteration 83 : 0.7488163814549087
Loss in iteration 84 : 0.7453481221509196
Loss in iteration 85 : 0.7418712074733346
Loss in iteration 86 : 0.7383856590609948
Loss in iteration 87 : 0.7348914983799548
Loss in iteration 88 : 0.7313887467268049
Loss in iteration 89 : 0.7278774252318317
Loss in iteration 90 : 0.7243575548620057
Loss in iteration 91 : 0.720829156423839
Loss in iteration 92 : 0.717292250566094
Loss in iteration 93 : 0.7137468577823854
Loss in iteration 94 : 0.7101929984136613
Loss in iteration 95 : 0.7066306926505822
Loss in iteration 96 : 0.7030599605358115
Loss in iteration 97 : 0.6994808219662066
Loss in iteration 98 : 0.69589329669495
Loss in iteration 99 : 0.6922974043335803
Loss in iteration 100 : 0.6886931643539774
Loss in iteration 101 : 0.6850805960902664
Loss in iteration 102 : 0.6814597187406735
Loss in iteration 103 : 0.6778305513693202
Loss in iteration 104 : 0.6741931129079652
Loss in iteration 105 : 0.670547422157701
Loss in iteration 106 : 0.6668934977906062
Loss in iteration 107 : 0.6632313583513435
Loss in iteration 108 : 0.6595610222587374
Loss in iteration 109 : 0.6558825078072923
Loss in iteration 110 : 0.652195833168691
Loss in iteration 111 : 0.6485010163932499
Loss in iteration 112 : 0.6447980754113449
Loss in iteration 113 : 0.6410870280348077
Loss in iteration 114 : 0.6373678919582917
Loss in iteration 115 : 0.6336406847606086
Loss in iteration 116 : 0.6299054239060394
Loss in iteration 117 : 0.626162126745623
Loss in iteration 118 : 0.6224108105184135
Loss in iteration 119 : 0.6186514923527207
Loss in iteration 120 : 0.6148841892673238
Loss in iteration 121 : 0.6111089181726667
Loss in iteration 122 : 0.6073256958720276
Loss in iteration 123 : 0.603534539062673
Loss in iteration 124 : 0.5997354643369908
Loss in iteration 125 : 0.5959284881836016
Loss in iteration 126 : 0.5921136269884575
Loss in iteration 127 : 0.5887746220416947
Loss in iteration 128 : 0.5856145256028997
Loss in iteration 129 : 0.5824479333998543
Loss in iteration 130 : 0.579274814340544
Loss in iteration 131 : 0.5760951317512577
Loss in iteration 132 : 0.5729088468924131
Loss in iteration 133 : 0.5697159208420516
Loss in iteration 134 : 0.566516315589082
Loss in iteration 135 : 0.5633099946967912
Loss in iteration 136 : 0.5600969237107037
Loss in iteration 137 : 0.5568770704033169
Loss in iteration 138 : 0.5536504049090315
Loss in iteration 139 : 0.5504168997822806
Loss in iteration 140 : 0.5471765300006274
Loss in iteration 141 : 0.5439292729280102
Loss in iteration 142 : 0.5406751082492303
Loss in iteration 143 : 0.5374140178840927
Loss in iteration 144 : 0.5341459858877636
Loss in iteration 145 : 0.5308717374355844
Loss in iteration 146 : 0.527620628895536
Loss in iteration 147 : 0.524429832134051
Loss in iteration 148 : 0.5212631581157066
Loss in iteration 149 : 0.5181256321043096
Loss in iteration 150 : 0.5150053790567005
Loss in iteration 151 : 0.5119065708949074
Loss in iteration 152 : 0.508806806599097
Loss in iteration 153 : 0.5057030402901448
Loss in iteration 154 : 0.5025979567277621
Loss in iteration 155 : 0.4994898956318766
Loss in iteration 156 : 0.4963765846180785
Loss in iteration 157 : 0.4932570950636704
Loss in iteration 158 : 0.49013085728337874
Loss in iteration 159 : 0.4869980183880255
Loss in iteration 160 : 0.48385893152688386
Loss in iteration 161 : 0.48071379519537455
Loss in iteration 162 : 0.47756300935916207
Loss in iteration 163 : 0.4744083850505218
Loss in iteration 164 : 0.4712510204445991
Loss in iteration 165 : 0.4680889883036269
Loss in iteration 166 : 0.46492146346045804
Loss in iteration 167 : 0.46174744793099076
Loss in iteration 168 : 0.4585671263443126
Loss in iteration 169 : 0.45538025558349793
Loss in iteration 170 : 0.4521868825313078
Loss in iteration 171 : 0.44898697445497415
Loss in iteration 172 : 0.4457804092941689
Loss in iteration 173 : 0.44256726267648766
Loss in iteration 174 : 0.4393474935107087
Loss in iteration 175 : 0.4361210544763015
Loss in iteration 176 : 0.4328879990210173
Loss in iteration 177 : 0.4296509724762815
Loss in iteration 178 : 0.42640912885724547
Loss in iteration 179 : 0.42316106881265336
Loss in iteration 180 : 0.4199066918626694
Loss in iteration 181 : 0.41664570815500673
Loss in iteration 182 : 0.4133781389952566
Loss in iteration 183 : 0.4101041526520685
Loss in iteration 184 : 0.40682386245703106
Loss in iteration 185 : 0.40353832953693464
Loss in iteration 186 : 0.40025504624799346
Loss in iteration 187 : 0.39697274530255794
Loss in iteration 188 : 0.3936906641200877
Loss in iteration 189 : 0.39042769837453917
Loss in iteration 190 : 0.38720221686387674
Loss in iteration 191 : 0.38397089674374213
Loss in iteration 192 : 0.3807590580667263
Loss in iteration 193 : 0.3775947049975074
Loss in iteration 194 : 0.37444532161853544
Loss in iteration 195 : 0.3712997120899868
Loss in iteration 196 : 0.36820653008791815
Loss in iteration 197 : 0.365118498907149
Loss in iteration 198 : 0.3620259201462592
Loss in iteration 199 : 0.3589536459719925
Loss in iteration 200 : 0.3559152131495115
Testing accuracy  of updater 4 on alg 1 with rate 7000.0 = 0.9982222222222222, training accuracy 0.8796799085452986, time elapsed: 7795 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975479612481092
Loss in iteration 3 : 0.9950322333076048
Loss in iteration 4 : 0.9924727961555131
Loss in iteration 5 : 0.9898794338354182
Loss in iteration 6 : 0.9872579206900947
Loss in iteration 7 : 0.9846120495765806
Loss in iteration 8 : 0.9819444913370551
Loss in iteration 9 : 0.9792572208774807
Loss in iteration 10 : 0.9765517522691183
Loss in iteration 11 : 0.9738292790190761
Loss in iteration 12 : 0.9710907629124258
Loss in iteration 13 : 0.9683369930016623
Loss in iteration 14 : 0.9655686263032196
Loss in iteration 15 : 0.9627862167776797
Loss in iteration 16 : 0.9599902365242617
Loss in iteration 17 : 0.9571810916376241
Loss in iteration 18 : 0.9543591343059925
Loss in iteration 19 : 0.9515246722002976
Loss in iteration 20 : 0.9486779758706282
Loss in iteration 21 : 0.9458192846502997
Loss in iteration 22 : 0.94294881142408
Loss in iteration 23 : 0.9400667465194705
Loss in iteration 24 : 0.9371732609120242
Loss in iteration 25 : 0.9342685088878165
Loss in iteration 26 : 0.9313526302716557
Loss in iteration 27 : 0.928425752304557
Loss in iteration 28 : 0.9254879912353581
Loss in iteration 29 : 0.9225394536774879
Loss in iteration 30 : 0.9195802377713136
Loss in iteration 31 : 0.9166104341844653
Loss in iteration 32 : 0.9136301269762295
Loss in iteration 33 : 0.9106393943472861
Loss in iteration 34 : 0.9076383092921944
Loss in iteration 35 : 0.9046269401689891
Loss in iteration 36 : 0.9016053511977964
Loss in iteration 37 : 0.8985736028984481
Loss in iteration 38 : 0.8955317524754294
Loss in iteration 39 : 0.8924798541572285
Loss in iteration 40 : 0.8894179594960614
Loss in iteration 41 : 0.8863461176330926
Loss in iteration 42 : 0.8832643755334691
Loss in iteration 43 : 0.880172778194964
Loss in iteration 44 : 0.877071368833413
Loss in iteration 45 : 0.8739601890477618
Loss in iteration 46 : 0.8708392789671417
Loss in iteration 47 : 0.8677086773820697
Loss in iteration 48 : 0.8645684218616234
Loss in iteration 49 : 0.8614185488581949
Loss in iteration 50 : 0.8582590938012205
Loss in iteration 51 : 0.8550900911811472
Loss in iteration 52 : 0.8519115746246982
Loss in iteration 53 : 0.8487235769624152
Loss in iteration 54 : 0.8455261302893157
Loss in iteration 55 : 0.8423192660194208
Loss in iteration 56 : 0.8391030149348122
Loss in iteration 57 : 0.8358774072297956
Loss in iteration 58 : 0.8326424725507212
Loss in iteration 59 : 0.8293982400318961
Loss in iteration 60 : 0.8261447383280074
Loss in iteration 61 : 0.8228819956434407
Loss in iteration 62 : 0.8196100397587865
Loss in iteration 63 : 0.8163288980548684
Loss in iteration 64 : 0.8130385975345078
Loss in iteration 65 : 0.8097391648422936
Loss in iteration 66 : 0.8064306262825424
Loss in iteration 67 : 0.8031130078356398
Loss in iteration 68 : 0.7997863351729336
Loss in iteration 69 : 0.796450633670317
Loss in iteration 70 : 0.7931059284206402
Loss in iteration 71 : 0.7897522442450646
Loss in iteration 72 : 0.7863896057034743
Loss in iteration 73 : 0.7830180371040263
Loss in iteration 74 : 0.7796375625119359
Loss in iteration 75 : 0.776248205757563
Loss in iteration 76 : 0.7728499904438834
Loss in iteration 77 : 0.7694429399533872
Loss in iteration 78 : 0.7660270774544679
Loss in iteration 79 : 0.7626024259073629
Loss in iteration 80 : 0.7591690080696658
Loss in iteration 81 : 0.7557268465014655
Loss in iteration 82 : 0.7522759635701524
Loss in iteration 83 : 0.7488163814549087
Loss in iteration 84 : 0.7453481221509194
Loss in iteration 85 : 0.7418712074733346
Loss in iteration 86 : 0.7383856590609948
Loss in iteration 87 : 0.7348914983799548
Loss in iteration 88 : 0.7313887467268049
Loss in iteration 89 : 0.7278774252318317
Loss in iteration 90 : 0.7243575548620057
Loss in iteration 91 : 0.720829156423839
Loss in iteration 92 : 0.717292250566094
Loss in iteration 93 : 0.7137468577823854
Loss in iteration 94 : 0.7101929984136613
Loss in iteration 95 : 0.7066306926505824
Loss in iteration 96 : 0.7030599605358115
Loss in iteration 97 : 0.6994808219662065
Loss in iteration 98 : 0.6958932966949501
Loss in iteration 99 : 0.6922974043335803
Loss in iteration 100 : 0.6886931643539774
Loss in iteration 101 : 0.6850805960902664
Loss in iteration 102 : 0.6814597187406735
Loss in iteration 103 : 0.6778305513693202
Loss in iteration 104 : 0.6741931129079652
Loss in iteration 105 : 0.670547422157701
Loss in iteration 106 : 0.6668934977906062
Loss in iteration 107 : 0.6632313583513435
Loss in iteration 108 : 0.6595610222587374
Loss in iteration 109 : 0.6558825078072923
Loss in iteration 110 : 0.6521958331686911
Loss in iteration 111 : 0.6485010163932499
Loss in iteration 112 : 0.6447980754113449
Loss in iteration 113 : 0.6410870280348077
Loss in iteration 114 : 0.6373678919582917
Loss in iteration 115 : 0.6336406847606086
Loss in iteration 116 : 0.6299054239060394
Loss in iteration 117 : 0.626162126745623
Loss in iteration 118 : 0.6224108105184135
Loss in iteration 119 : 0.6186514923527208
Loss in iteration 120 : 0.6148841892673238
Loss in iteration 121 : 0.6111089181726667
Loss in iteration 122 : 0.6073256958720276
Loss in iteration 123 : 0.603534539062673
Loss in iteration 124 : 0.5997354643369908
Loss in iteration 125 : 0.5959284881836016
Loss in iteration 126 : 0.5921136269884575
Loss in iteration 127 : 0.5887746220416947
Loss in iteration 128 : 0.5856145256028997
Loss in iteration 129 : 0.5824479333998543
Loss in iteration 130 : 0.5792748143405438
Loss in iteration 131 : 0.5760951317512577
Loss in iteration 132 : 0.5729088468924131
Loss in iteration 133 : 0.5697159208420516
Loss in iteration 134 : 0.5665163155890819
Loss in iteration 135 : 0.5633099946967912
Loss in iteration 136 : 0.5600969237107037
Loss in iteration 137 : 0.5568770704033169
Loss in iteration 138 : 0.5536504049090315
Loss in iteration 139 : 0.5504168997822806
Loss in iteration 140 : 0.5471765300006274
Loss in iteration 141 : 0.5439292729280102
Loss in iteration 142 : 0.5406751082492303
Loss in iteration 143 : 0.5374140178840927
Loss in iteration 144 : 0.5341459858877636
Loss in iteration 145 : 0.5308717374355844
Loss in iteration 146 : 0.527620628895536
Loss in iteration 147 : 0.524429832134051
Loss in iteration 148 : 0.5212631581157066
Loss in iteration 149 : 0.5181256321043096
Loss in iteration 150 : 0.5150053790567005
Loss in iteration 151 : 0.5119065708949074
Loss in iteration 152 : 0.508806806599097
Loss in iteration 153 : 0.5057030402901448
Loss in iteration 154 : 0.5025979567277621
Loss in iteration 155 : 0.4994898956318766
Loss in iteration 156 : 0.4963765846180785
Loss in iteration 157 : 0.4932570950636704
Loss in iteration 158 : 0.49013085728337874
Loss in iteration 159 : 0.4869980183880255
Loss in iteration 160 : 0.4838589315268839
Loss in iteration 161 : 0.48071379519537455
Loss in iteration 162 : 0.4775630093591621
Loss in iteration 163 : 0.4744083850505218
Loss in iteration 164 : 0.4712510204445991
Loss in iteration 165 : 0.4680889883036269
Loss in iteration 166 : 0.46492146346045804
Loss in iteration 167 : 0.46174744793099076
Loss in iteration 168 : 0.45856712634431257
Loss in iteration 169 : 0.45538025558349804
Loss in iteration 170 : 0.4521868825313078
Loss in iteration 171 : 0.4489869744549742
Loss in iteration 172 : 0.4457804092941689
Loss in iteration 173 : 0.44256726267648766
Loss in iteration 174 : 0.4393474935107087
Loss in iteration 175 : 0.4361210544763015
Loss in iteration 176 : 0.4328879990210173
Loss in iteration 177 : 0.4296509724762815
Loss in iteration 178 : 0.4264091288572455
Loss in iteration 179 : 0.42316106881265336
Loss in iteration 180 : 0.4199066918626695
Loss in iteration 181 : 0.41664570815500684
Loss in iteration 182 : 0.4133781389952566
Loss in iteration 183 : 0.4101041526520685
Loss in iteration 184 : 0.40682386245703106
Loss in iteration 185 : 0.40353832953693464
Loss in iteration 186 : 0.40025504624799346
Loss in iteration 187 : 0.39697274530255794
Loss in iteration 188 : 0.3936906641200877
Loss in iteration 189 : 0.39042769837453917
Loss in iteration 190 : 0.38720221686387674
Loss in iteration 191 : 0.38397089674374213
Loss in iteration 192 : 0.3807590580667263
Loss in iteration 193 : 0.3775947049975074
Loss in iteration 194 : 0.37444532161853544
Loss in iteration 195 : 0.3712997120899868
Loss in iteration 196 : 0.36820653008791815
Loss in iteration 197 : 0.3651184989071489
Loss in iteration 198 : 0.3620259201462592
Loss in iteration 199 : 0.3589536459719925
Loss in iteration 200 : 0.35591521314951147
Testing accuracy  of updater 4 on alg 1 with rate 4000.0 = 0.9982222222222222, training accuracy 0.8796799085452986, time elapsed: 9141 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975479612481092
Loss in iteration 3 : 0.9950322333076049
Loss in iteration 4 : 0.9924727961555131
Loss in iteration 5 : 0.9898794338354182
Loss in iteration 6 : 0.9872579206900947
Loss in iteration 7 : 0.9846120495765806
Loss in iteration 8 : 0.9819444913370551
Loss in iteration 9 : 0.9792572208774807
Loss in iteration 10 : 0.9765517522691183
Loss in iteration 11 : 0.9738292790190762
Loss in iteration 12 : 0.971090762912426
Loss in iteration 13 : 0.9683369930016623
Loss in iteration 14 : 0.9655686263032195
Loss in iteration 15 : 0.9627862167776797
Loss in iteration 16 : 0.9599902365242617
Loss in iteration 17 : 0.9571810916376241
Loss in iteration 18 : 0.9543591343059925
Loss in iteration 19 : 0.9515246722002975
Loss in iteration 20 : 0.9486779758706282
Loss in iteration 21 : 0.9458192846502996
Loss in iteration 22 : 0.94294881142408
Loss in iteration 23 : 0.9400667465194705
Loss in iteration 24 : 0.9371732609120244
Loss in iteration 25 : 0.9342685088878165
Loss in iteration 26 : 0.9313526302716557
Loss in iteration 27 : 0.928425752304557
Loss in iteration 28 : 0.925487991235358
Loss in iteration 29 : 0.9225394536774878
Loss in iteration 30 : 0.9195802377713136
Loss in iteration 31 : 0.9166104341844652
Loss in iteration 32 : 0.9136301269762295
Loss in iteration 33 : 0.9106393943472859
Loss in iteration 34 : 0.9076383092921944
Loss in iteration 35 : 0.904626940168989
Loss in iteration 36 : 0.9016053511977964
Loss in iteration 37 : 0.8985736028984481
Loss in iteration 38 : 0.8955317524754294
Loss in iteration 39 : 0.8924798541572285
Loss in iteration 40 : 0.8894179594960614
Loss in iteration 41 : 0.8863461176330928
Loss in iteration 42 : 0.8832643755334691
Loss in iteration 43 : 0.880172778194964
Loss in iteration 44 : 0.8770713688334127
Loss in iteration 45 : 0.8739601890477618
Loss in iteration 46 : 0.8708392789671417
Loss in iteration 47 : 0.8677086773820696
Loss in iteration 48 : 0.8645684218616234
Loss in iteration 49 : 0.8614185488581949
Loss in iteration 50 : 0.8582590938012205
Loss in iteration 51 : 0.8550900911811472
Loss in iteration 52 : 0.8519115746246982
Loss in iteration 53 : 0.8487235769624152
Loss in iteration 54 : 0.8455261302893158
Loss in iteration 55 : 0.8423192660194208
Loss in iteration 56 : 0.8391030149348122
Loss in iteration 57 : 0.8358774072297956
Loss in iteration 58 : 0.8326424725507212
Loss in iteration 59 : 0.8293982400318961
Loss in iteration 60 : 0.8261447383280074
Loss in iteration 61 : 0.8228819956434407
Loss in iteration 62 : 0.8196100397587865
Loss in iteration 63 : 0.8163288980548684
Loss in iteration 64 : 0.8130385975345076
Loss in iteration 65 : 0.8097391648422936
Loss in iteration 66 : 0.8064306262825424
Loss in iteration 67 : 0.8031130078356399
Loss in iteration 68 : 0.7997863351729336
Loss in iteration 69 : 0.796450633670317
Loss in iteration 70 : 0.7931059284206402
Loss in iteration 71 : 0.7897522442450646
Loss in iteration 72 : 0.7863896057034743
Loss in iteration 73 : 0.7830180371040263
Loss in iteration 74 : 0.7796375625119358
Loss in iteration 75 : 0.776248205757563
Loss in iteration 76 : 0.7728499904438836
Loss in iteration 77 : 0.7694429399533872
Loss in iteration 78 : 0.766027077454468
Loss in iteration 79 : 0.7626024259073629
Loss in iteration 80 : 0.7591690080696658
Loss in iteration 81 : 0.7557268465014654
Loss in iteration 82 : 0.7522759635701524
Loss in iteration 83 : 0.7488163814549087
Loss in iteration 84 : 0.7453481221509196
Loss in iteration 85 : 0.7418712074733346
Loss in iteration 86 : 0.7383856590609948
Loss in iteration 87 : 0.7348914983799548
Loss in iteration 88 : 0.7313887467268049
Loss in iteration 89 : 0.7278774252318317
Loss in iteration 90 : 0.7243575548620057
Loss in iteration 91 : 0.720829156423839
Loss in iteration 92 : 0.717292250566094
Loss in iteration 93 : 0.7137468577823854
Loss in iteration 94 : 0.7101929984136613
Loss in iteration 95 : 0.7066306926505824
Loss in iteration 96 : 0.7030599605358115
Loss in iteration 97 : 0.6994808219662066
Loss in iteration 98 : 0.69589329669495
Loss in iteration 99 : 0.6922974043335803
Loss in iteration 100 : 0.6886931643539773
Loss in iteration 101 : 0.6850805960902664
Loss in iteration 102 : 0.6814597187406733
Loss in iteration 103 : 0.6778305513693202
Loss in iteration 104 : 0.6741931129079652
Loss in iteration 105 : 0.670547422157701
Loss in iteration 106 : 0.6668934977906062
Loss in iteration 107 : 0.6632313583513435
Loss in iteration 108 : 0.6595610222587375
Loss in iteration 109 : 0.6558825078072923
Loss in iteration 110 : 0.6521958331686911
Loss in iteration 111 : 0.6485010163932499
Loss in iteration 112 : 0.6447980754113449
Loss in iteration 113 : 0.6410870280348077
Loss in iteration 114 : 0.6373678919582917
Loss in iteration 115 : 0.6336406847606086
Loss in iteration 116 : 0.6299054239060394
Loss in iteration 117 : 0.626162126745623
Loss in iteration 118 : 0.6224108105184135
Loss in iteration 119 : 0.6186514923527207
Loss in iteration 120 : 0.6148841892673238
Loss in iteration 121 : 0.6111089181726667
Loss in iteration 122 : 0.6073256958720276
Loss in iteration 123 : 0.603534539062673
Loss in iteration 124 : 0.5997354643369908
Loss in iteration 125 : 0.5959284881836016
Loss in iteration 126 : 0.5921136269884575
Loss in iteration 127 : 0.5887746220416947
Loss in iteration 128 : 0.5856145256028997
Loss in iteration 129 : 0.5824479333998543
Loss in iteration 130 : 0.579274814340544
Loss in iteration 131 : 0.5760951317512577
Loss in iteration 132 : 0.5729088468924131
Loss in iteration 133 : 0.5697159208420516
Loss in iteration 134 : 0.566516315589082
Loss in iteration 135 : 0.5633099946967912
Loss in iteration 136 : 0.5600969237107037
Loss in iteration 137 : 0.5568770704033169
Loss in iteration 138 : 0.5536504049090315
Loss in iteration 139 : 0.5504168997822805
Loss in iteration 140 : 0.5471765300006273
Loss in iteration 141 : 0.5439292729280103
Loss in iteration 142 : 0.5406751082492303
Loss in iteration 143 : 0.5374140178840927
Loss in iteration 144 : 0.5341459858877636
Loss in iteration 145 : 0.5308717374355844
Loss in iteration 146 : 0.527620628895536
Loss in iteration 147 : 0.5244298321340509
Loss in iteration 148 : 0.5212631581157064
Loss in iteration 149 : 0.5181256321043095
Loss in iteration 150 : 0.5150053790567006
Loss in iteration 151 : 0.5119065708949074
Loss in iteration 152 : 0.508806806599097
Loss in iteration 153 : 0.5057030402901448
Loss in iteration 154 : 0.5025979567277621
Loss in iteration 155 : 0.4994898956318766
Loss in iteration 156 : 0.4963765846180785
Loss in iteration 157 : 0.4932570950636704
Loss in iteration 158 : 0.49013085728337874
Loss in iteration 159 : 0.4869980183880255
Loss in iteration 160 : 0.4838589315268839
Loss in iteration 161 : 0.48071379519537455
Loss in iteration 162 : 0.4775630093591621
Loss in iteration 163 : 0.4744083850505218
Loss in iteration 164 : 0.4712510204445991
Loss in iteration 165 : 0.4680889883036269
Loss in iteration 166 : 0.46492146346045804
Loss in iteration 167 : 0.46174744793099076
Loss in iteration 168 : 0.4585671263443126
Loss in iteration 169 : 0.45538025558349793
Loss in iteration 170 : 0.4521868825313078
Loss in iteration 171 : 0.44898697445497415
Loss in iteration 172 : 0.44578040929416896
Loss in iteration 173 : 0.44256726267648766
Loss in iteration 174 : 0.4393474935107087
Loss in iteration 175 : 0.4361210544763015
Loss in iteration 176 : 0.4328879990210173
Loss in iteration 177 : 0.4296509724762815
Loss in iteration 178 : 0.42640912885724547
Loss in iteration 179 : 0.42316106881265336
Loss in iteration 180 : 0.4199066918626695
Loss in iteration 181 : 0.41664570815500684
Loss in iteration 182 : 0.41337813899525666
Loss in iteration 183 : 0.4101041526520685
Loss in iteration 184 : 0.40682386245703106
Loss in iteration 185 : 0.40353832953693464
Loss in iteration 186 : 0.40025504624799346
Loss in iteration 187 : 0.39697274530255794
Loss in iteration 188 : 0.3936906641200877
Loss in iteration 189 : 0.39042769837453917
Loss in iteration 190 : 0.38720221686387674
Loss in iteration 191 : 0.38397089674374213
Loss in iteration 192 : 0.3807590580667263
Loss in iteration 193 : 0.3775947049975074
Loss in iteration 194 : 0.37444532161853544
Loss in iteration 195 : 0.3712997120899868
Loss in iteration 196 : 0.36820653008791815
Loss in iteration 197 : 0.365118498907149
Loss in iteration 198 : 0.36202592014625923
Loss in iteration 199 : 0.3589536459719925
Loss in iteration 200 : 0.3559152131495115
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.9982222222222222, training accuracy 0.8796799085452986, time elapsed: 8609 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975479612481092
Loss in iteration 3 : 0.9950322333076049
Loss in iteration 4 : 0.9924727961555131
Loss in iteration 5 : 0.989879433835418
Loss in iteration 6 : 0.9872579206900947
Loss in iteration 7 : 0.9846120495765806
Loss in iteration 8 : 0.9819444913370551
Loss in iteration 9 : 0.9792572208774805
Loss in iteration 10 : 0.9765517522691183
Loss in iteration 11 : 0.9738292790190761
Loss in iteration 12 : 0.9710907629124258
Loss in iteration 13 : 0.9683369930016623
Loss in iteration 14 : 0.9655686263032195
Loss in iteration 15 : 0.9627862167776797
Loss in iteration 16 : 0.9599902365242617
Loss in iteration 17 : 0.9571810916376241
Loss in iteration 18 : 0.9543591343059925
Loss in iteration 19 : 0.9515246722002975
Loss in iteration 20 : 0.9486779758706282
Loss in iteration 21 : 0.9458192846502997
Loss in iteration 22 : 0.94294881142408
Loss in iteration 23 : 0.9400667465194705
Loss in iteration 24 : 0.9371732609120242
Loss in iteration 25 : 0.9342685088878165
Loss in iteration 26 : 0.9313526302716557
Loss in iteration 27 : 0.928425752304557
Loss in iteration 28 : 0.9254879912353581
Loss in iteration 29 : 0.9225394536774878
Loss in iteration 30 : 0.9195802377713136
Loss in iteration 31 : 0.9166104341844653
Loss in iteration 32 : 0.9136301269762293
Loss in iteration 33 : 0.9106393943472861
Loss in iteration 34 : 0.9076383092921944
Loss in iteration 35 : 0.904626940168989
Loss in iteration 36 : 0.9016053511977964
Loss in iteration 37 : 0.8985736028984481
Loss in iteration 38 : 0.8955317524754294
Loss in iteration 39 : 0.8924798541572285
Loss in iteration 40 : 0.8894179594960616
Loss in iteration 41 : 0.8863461176330928
Loss in iteration 42 : 0.8832643755334691
Loss in iteration 43 : 0.880172778194964
Loss in iteration 44 : 0.8770713688334127
Loss in iteration 45 : 0.8739601890477618
Loss in iteration 46 : 0.8708392789671417
Loss in iteration 47 : 0.8677086773820696
Loss in iteration 48 : 0.8645684218616234
Loss in iteration 49 : 0.8614185488581949
Loss in iteration 50 : 0.8582590938012205
Loss in iteration 51 : 0.8550900911811471
Loss in iteration 52 : 0.8519115746246982
Loss in iteration 53 : 0.8487235769624152
Loss in iteration 54 : 0.8455261302893157
Loss in iteration 55 : 0.8423192660194206
Loss in iteration 56 : 0.8391030149348122
Loss in iteration 57 : 0.8358774072297956
Loss in iteration 58 : 0.8326424725507212
Loss in iteration 59 : 0.8293982400318961
Loss in iteration 60 : 0.8261447383280074
Loss in iteration 61 : 0.8228819956434406
Loss in iteration 62 : 0.8196100397587865
Loss in iteration 63 : 0.8163288980548684
Loss in iteration 64 : 0.8130385975345078
Loss in iteration 65 : 0.8097391648422936
Loss in iteration 66 : 0.8064306262825424
Loss in iteration 67 : 0.8031130078356399
Loss in iteration 68 : 0.7997863351729336
Loss in iteration 69 : 0.796450633670317
Loss in iteration 70 : 0.7931059284206402
Loss in iteration 71 : 0.7897522442450646
Loss in iteration 72 : 0.7863896057034743
Loss in iteration 73 : 0.7830180371040263
Loss in iteration 74 : 0.7796375625119359
Loss in iteration 75 : 0.776248205757563
Loss in iteration 76 : 0.7728499904438834
Loss in iteration 77 : 0.7694429399533871
Loss in iteration 78 : 0.7660270774544679
Loss in iteration 79 : 0.7626024259073627
Loss in iteration 80 : 0.7591690080696658
Loss in iteration 81 : 0.7557268465014654
Loss in iteration 82 : 0.7522759635701524
Loss in iteration 83 : 0.7488163814549087
Loss in iteration 84 : 0.7453481221509194
Loss in iteration 85 : 0.7418712074733346
Loss in iteration 86 : 0.7383856590609948
Loss in iteration 87 : 0.7348914983799548
Loss in iteration 88 : 0.7313887467268049
Loss in iteration 89 : 0.7278774252318317
Loss in iteration 90 : 0.7243575548620056
Loss in iteration 91 : 0.720829156423839
Loss in iteration 92 : 0.717292250566094
Loss in iteration 93 : 0.7137468577823854
Loss in iteration 94 : 0.7101929984136613
Loss in iteration 95 : 0.7066306926505823
Loss in iteration 96 : 0.7030599605358115
Loss in iteration 97 : 0.6994808219662066
Loss in iteration 98 : 0.6958932966949499
Loss in iteration 99 : 0.6922974043335803
Loss in iteration 100 : 0.6886931643539774
Loss in iteration 101 : 0.6850805960902664
Loss in iteration 102 : 0.6814597187406735
Loss in iteration 103 : 0.6778305513693202
Loss in iteration 104 : 0.6741931129079652
Loss in iteration 105 : 0.670547422157701
Loss in iteration 106 : 0.6668934977906062
Loss in iteration 107 : 0.6632313583513435
Loss in iteration 108 : 0.6595610222587375
Loss in iteration 109 : 0.6558825078072924
Loss in iteration 110 : 0.652195833168691
Loss in iteration 111 : 0.6485010163932499
Loss in iteration 112 : 0.6447980754113449
Loss in iteration 113 : 0.6410870280348077
Loss in iteration 114 : 0.6373678919582917
Loss in iteration 115 : 0.6336406847606086
Loss in iteration 116 : 0.6299054239060394
Loss in iteration 117 : 0.626162126745623
Loss in iteration 118 : 0.6224108105184135
Loss in iteration 119 : 0.6186514923527207
Loss in iteration 120 : 0.6148841892673238
Loss in iteration 121 : 0.6111089181726667
Loss in iteration 122 : 0.6073256958720276
Loss in iteration 123 : 0.603534539062673
Loss in iteration 124 : 0.5997354643369908
Loss in iteration 125 : 0.5959284881836016
Loss in iteration 126 : 0.5921136269884575
Loss in iteration 127 : 0.5887746220416947
Loss in iteration 128 : 0.5856145256028997
Loss in iteration 129 : 0.5824479333998543
Loss in iteration 130 : 0.579274814340544
Loss in iteration 131 : 0.5760951317512577
Loss in iteration 132 : 0.5729088468924131
Loss in iteration 133 : 0.5697159208420516
Loss in iteration 134 : 0.566516315589082
Loss in iteration 135 : 0.5633099946967912
Loss in iteration 136 : 0.5600969237107036
Loss in iteration 137 : 0.5568770704033169
Loss in iteration 138 : 0.5536504049090315
Loss in iteration 139 : 0.5504168997822806
Loss in iteration 140 : 0.5471765300006273
Loss in iteration 141 : 0.5439292729280102
Loss in iteration 142 : 0.5406751082492303
Loss in iteration 143 : 0.5374140178840927
Loss in iteration 144 : 0.5341459858877636
Loss in iteration 145 : 0.5308717374355844
Loss in iteration 146 : 0.527620628895536
Loss in iteration 147 : 0.524429832134051
Loss in iteration 148 : 0.5212631581157064
Loss in iteration 149 : 0.5181256321043096
Loss in iteration 150 : 0.5150053790567005
Loss in iteration 151 : 0.5119065708949074
Loss in iteration 152 : 0.508806806599097
Loss in iteration 153 : 0.5057030402901448
Loss in iteration 154 : 0.5025979567277621
Loss in iteration 155 : 0.4994898956318766
Loss in iteration 156 : 0.4963765846180785
Loss in iteration 157 : 0.4932570950636704
Loss in iteration 158 : 0.4901308572833788
Loss in iteration 159 : 0.4869980183880255
Loss in iteration 160 : 0.48385893152688386
Loss in iteration 161 : 0.48071379519537455
Loss in iteration 162 : 0.47756300935916207
Loss in iteration 163 : 0.4744083850505218
Loss in iteration 164 : 0.4712510204445991
Loss in iteration 165 : 0.4680889883036269
Loss in iteration 166 : 0.46492146346045804
Loss in iteration 167 : 0.4617474479309909
Loss in iteration 168 : 0.45856712634431257
Loss in iteration 169 : 0.45538025558349804
Loss in iteration 170 : 0.4521868825313078
Loss in iteration 171 : 0.4489869744549742
Loss in iteration 172 : 0.4457804092941689
Loss in iteration 173 : 0.4425672626764876
Loss in iteration 174 : 0.4393474935107087
Loss in iteration 175 : 0.4361210544763015
Loss in iteration 176 : 0.4328879990210173
Loss in iteration 177 : 0.4296509724762815
Loss in iteration 178 : 0.4264091288572455
Loss in iteration 179 : 0.42316106881265336
Loss in iteration 180 : 0.4199066918626694
Loss in iteration 181 : 0.41664570815500673
Loss in iteration 182 : 0.4133781389952566
Loss in iteration 183 : 0.4101041526520685
Loss in iteration 184 : 0.40682386245703106
Loss in iteration 185 : 0.40353832953693464
Loss in iteration 186 : 0.4002550462479935
Loss in iteration 187 : 0.39697274530255794
Loss in iteration 188 : 0.3936906641200877
Loss in iteration 189 : 0.39042769837453917
Loss in iteration 190 : 0.38720221686387674
Loss in iteration 191 : 0.38397089674374213
Loss in iteration 192 : 0.3807590580667263
Loss in iteration 193 : 0.3775947049975074
Loss in iteration 194 : 0.37444532161853544
Loss in iteration 195 : 0.3712997120899868
Loss in iteration 196 : 0.36820653008791815
Loss in iteration 197 : 0.365118498907149
Loss in iteration 198 : 0.3620259201462592
Loss in iteration 199 : 0.3589536459719925
Loss in iteration 200 : 0.3559152131495115
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.9982222222222222, training accuracy 0.8796799085452986, time elapsed: 8408 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975479612481092
Loss in iteration 3 : 0.9950322333076048
Loss in iteration 4 : 0.9924727961555131
Loss in iteration 5 : 0.989879433835418
Loss in iteration 6 : 0.9872579206900947
Loss in iteration 7 : 0.9846120495765806
Loss in iteration 8 : 0.9819444913370551
Loss in iteration 9 : 0.9792572208774807
Loss in iteration 10 : 0.9765517522691183
Loss in iteration 11 : 0.9738292790190761
Loss in iteration 12 : 0.9710907629124258
Loss in iteration 13 : 0.9683369930016623
Loss in iteration 14 : 0.9655686263032195
Loss in iteration 15 : 0.9627862167776797
Loss in iteration 16 : 0.9599902365242617
Loss in iteration 17 : 0.9571810916376241
Loss in iteration 18 : 0.9543591343059925
Loss in iteration 19 : 0.9515246722002976
Loss in iteration 20 : 0.9486779758706282
Loss in iteration 21 : 0.9458192846502997
Loss in iteration 22 : 0.94294881142408
Loss in iteration 23 : 0.9400667465194705
Loss in iteration 24 : 0.9371732609120244
Loss in iteration 25 : 0.9342685088878165
Loss in iteration 26 : 0.9313526302716557
Loss in iteration 27 : 0.928425752304557
Loss in iteration 28 : 0.9254879912353581
Loss in iteration 29 : 0.9225394536774879
Loss in iteration 30 : 0.9195802377713136
Loss in iteration 31 : 0.9166104341844652
Loss in iteration 32 : 0.9136301269762293
Loss in iteration 33 : 0.9106393943472861
Loss in iteration 34 : 0.9076383092921944
Loss in iteration 35 : 0.904626940168989
Loss in iteration 36 : 0.9016053511977964
Loss in iteration 37 : 0.8985736028984481
Loss in iteration 38 : 0.8955317524754294
Loss in iteration 39 : 0.8924798541572285
Loss in iteration 40 : 0.8894179594960614
Loss in iteration 41 : 0.8863461176330928
Loss in iteration 42 : 0.8832643755334691
Loss in iteration 43 : 0.880172778194964
Loss in iteration 44 : 0.8770713688334127
Loss in iteration 45 : 0.8739601890477618
Loss in iteration 46 : 0.8708392789671417
Loss in iteration 47 : 0.8677086773820696
Loss in iteration 48 : 0.8645684218616234
Loss in iteration 49 : 0.8614185488581949
Loss in iteration 50 : 0.8582590938012205
Loss in iteration 51 : 0.8550900911811472
Loss in iteration 52 : 0.8519115746246982
Loss in iteration 53 : 0.8487235769624152
Loss in iteration 54 : 0.8455261302893157
Loss in iteration 55 : 0.8423192660194208
Loss in iteration 56 : 0.8391030149348123
Loss in iteration 57 : 0.8358774072297956
Loss in iteration 58 : 0.8326424725507212
Loss in iteration 59 : 0.8293982400318961
Loss in iteration 60 : 0.8261447383280074
Loss in iteration 61 : 0.8228819956434407
Loss in iteration 62 : 0.8196100397587865
Loss in iteration 63 : 0.8163288980548684
Loss in iteration 64 : 0.8130385975345076
Loss in iteration 65 : 0.8097391648422936
Loss in iteration 66 : 0.8064306262825424
Loss in iteration 67 : 0.8031130078356399
Loss in iteration 68 : 0.7997863351729336
Loss in iteration 69 : 0.7964506336703168
Loss in iteration 70 : 0.7931059284206402
Loss in iteration 71 : 0.7897522442450646
Loss in iteration 72 : 0.786389605703474
Loss in iteration 73 : 0.7830180371040264
Loss in iteration 74 : 0.7796375625119359
Loss in iteration 75 : 0.776248205757563
Loss in iteration 76 : 0.7728499904438834
Loss in iteration 77 : 0.7694429399533871
Loss in iteration 78 : 0.7660270774544679
Loss in iteration 79 : 0.7626024259073629
Loss in iteration 80 : 0.7591690080696658
Loss in iteration 81 : 0.7557268465014655
Loss in iteration 82 : 0.7522759635701524
Loss in iteration 83 : 0.7488163814549087
Loss in iteration 84 : 0.7453481221509194
Loss in iteration 85 : 0.7418712074733346
Loss in iteration 86 : 0.7383856590609948
Loss in iteration 87 : 0.7348914983799548
Loss in iteration 88 : 0.7313887467268049
Loss in iteration 89 : 0.7278774252318317
Loss in iteration 90 : 0.7243575548620057
Loss in iteration 91 : 0.720829156423839
Loss in iteration 92 : 0.717292250566094
Loss in iteration 93 : 0.7137468577823854
Loss in iteration 94 : 0.7101929984136613
Loss in iteration 95 : 0.7066306926505824
Loss in iteration 96 : 0.7030599605358115
Loss in iteration 97 : 0.6994808219662066
Loss in iteration 98 : 0.6958932966949499
Loss in iteration 99 : 0.6922974043335803
Loss in iteration 100 : 0.6886931643539774
Loss in iteration 101 : 0.6850805960902665
Loss in iteration 102 : 0.6814597187406735
Loss in iteration 103 : 0.6778305513693202
Loss in iteration 104 : 0.6741931129079652
Loss in iteration 105 : 0.670547422157701
Loss in iteration 106 : 0.6668934977906062
Loss in iteration 107 : 0.6632313583513435
Loss in iteration 108 : 0.6595610222587374
Loss in iteration 109 : 0.6558825078072923
Loss in iteration 110 : 0.652195833168691
Loss in iteration 111 : 0.6485010163932499
Loss in iteration 112 : 0.6447980754113449
Loss in iteration 113 : 0.6410870280348077
Loss in iteration 114 : 0.6373678919582917
Loss in iteration 115 : 0.6336406847606086
Loss in iteration 116 : 0.6299054239060394
Loss in iteration 117 : 0.626162126745623
Loss in iteration 118 : 0.6224108105184135
Loss in iteration 119 : 0.6186514923527207
Loss in iteration 120 : 0.6148841892673238
Loss in iteration 121 : 0.6111089181726667
Loss in iteration 122 : 0.6073256958720276
Loss in iteration 123 : 0.603534539062673
Loss in iteration 124 : 0.5997354643369908
Loss in iteration 125 : 0.5959284881836016
Loss in iteration 126 : 0.5921136269884575
Loss in iteration 127 : 0.5887746220416947
Loss in iteration 128 : 0.5856145256028997
Loss in iteration 129 : 0.5824479333998543
Loss in iteration 130 : 0.579274814340544
Loss in iteration 131 : 0.5760951317512577
Loss in iteration 132 : 0.5729088468924131
Loss in iteration 133 : 0.5697159208420516
Loss in iteration 134 : 0.5665163155890819
Loss in iteration 135 : 0.5633099946967912
Loss in iteration 136 : 0.5600969237107037
Loss in iteration 137 : 0.5568770704033169
Loss in iteration 138 : 0.5536504049090315
Loss in iteration 139 : 0.5504168997822806
Loss in iteration 140 : 0.5471765300006274
Loss in iteration 141 : 0.5439292729280102
Loss in iteration 142 : 0.5406751082492303
Loss in iteration 143 : 0.5374140178840927
Loss in iteration 144 : 0.5341459858877636
Loss in iteration 145 : 0.5308717374355844
Loss in iteration 146 : 0.527620628895536
Loss in iteration 147 : 0.5244298321340509
Loss in iteration 148 : 0.5212631581157066
Loss in iteration 149 : 0.5181256321043096
Loss in iteration 150 : 0.5150053790567005
Loss in iteration 151 : 0.5119065708949074
Loss in iteration 152 : 0.508806806599097
Loss in iteration 153 : 0.5057030402901448
Loss in iteration 154 : 0.5025979567277621
Loss in iteration 155 : 0.4994898956318766
Loss in iteration 156 : 0.4963765846180785
Loss in iteration 157 : 0.49325709506367027
Loss in iteration 158 : 0.49013085728337874
Loss in iteration 159 : 0.4869980183880255
Loss in iteration 160 : 0.48385893152688386
Loss in iteration 161 : 0.48071379519537455
Loss in iteration 162 : 0.47756300935916207
Loss in iteration 163 : 0.4744083850505218
Loss in iteration 164 : 0.471251020444599
Loss in iteration 165 : 0.4680889883036269
Loss in iteration 166 : 0.46492146346045804
Loss in iteration 167 : 0.46174744793099076
Loss in iteration 168 : 0.45856712634431257
Loss in iteration 169 : 0.45538025558349804
Loss in iteration 170 : 0.4521868825313078
Loss in iteration 171 : 0.44898697445497415
Loss in iteration 172 : 0.4457804092941689
Loss in iteration 173 : 0.44256726267648766
Loss in iteration 174 : 0.4393474935107087
Loss in iteration 175 : 0.4361210544763015
Loss in iteration 176 : 0.4328879990210173
Loss in iteration 177 : 0.4296509724762815
Loss in iteration 178 : 0.4264091288572455
Loss in iteration 179 : 0.42316106881265336
Loss in iteration 180 : 0.4199066918626695
Loss in iteration 181 : 0.41664570815500673
Loss in iteration 182 : 0.41337813899525666
Loss in iteration 183 : 0.4101041526520685
Loss in iteration 184 : 0.40682386245703106
Loss in iteration 185 : 0.40353832953693464
Loss in iteration 186 : 0.40025504624799346
Loss in iteration 187 : 0.39697274530255794
Loss in iteration 188 : 0.3936906641200877
Loss in iteration 189 : 0.39042769837453917
Loss in iteration 190 : 0.38720221686387674
Loss in iteration 191 : 0.38397089674374213
Loss in iteration 192 : 0.3807590580667263
Loss in iteration 193 : 0.3775947049975074
Loss in iteration 194 : 0.37444532161853544
Loss in iteration 195 : 0.3712997120899868
Loss in iteration 196 : 0.36820653008791815
Loss in iteration 197 : 0.3651184989071489
Loss in iteration 198 : 0.3620259201462592
Loss in iteration 199 : 0.3589536459719925
Loss in iteration 200 : 0.35591521314951147
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.9982222222222222, training accuracy 0.8796799085452986, time elapsed: 7543 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975479612481092
Loss in iteration 3 : 0.9950322333076049
Loss in iteration 4 : 0.9924727961555131
Loss in iteration 5 : 0.9898794338354182
Loss in iteration 6 : 0.9872579206900947
Loss in iteration 7 : 0.9846120495765806
Loss in iteration 8 : 0.9819444913370551
Loss in iteration 9 : 0.9792572208774805
Loss in iteration 10 : 0.9765517522691183
Loss in iteration 11 : 0.9738292790190762
Loss in iteration 12 : 0.9710907629124258
Loss in iteration 13 : 0.9683369930016623
Loss in iteration 14 : 0.9655686263032196
Loss in iteration 15 : 0.9627862167776796
Loss in iteration 16 : 0.9599902365242617
Loss in iteration 17 : 0.9571810916376241
Loss in iteration 18 : 0.9543591343059925
Loss in iteration 19 : 0.9515246722002975
Loss in iteration 20 : 0.9486779758706282
Loss in iteration 21 : 0.9458192846502996
Loss in iteration 22 : 0.94294881142408
Loss in iteration 23 : 0.9400667465194705
Loss in iteration 24 : 0.9371732609120242
Loss in iteration 25 : 0.9342685088878165
Loss in iteration 26 : 0.9313526302716557
Loss in iteration 27 : 0.928425752304557
Loss in iteration 28 : 0.9254879912353581
Loss in iteration 29 : 0.9225394536774878
Loss in iteration 30 : 0.9195802377713136
Loss in iteration 31 : 0.9166104341844653
Loss in iteration 32 : 0.9136301269762293
Loss in iteration 33 : 0.9106393943472861
Loss in iteration 34 : 0.9076383092921944
Loss in iteration 35 : 0.9046269401689891
Loss in iteration 36 : 0.9016053511977964
Loss in iteration 37 : 0.8985736028984481
Loss in iteration 38 : 0.8955317524754294
Loss in iteration 39 : 0.8924798541572287
Loss in iteration 40 : 0.8894179594960614
Loss in iteration 41 : 0.8863461176330928
Loss in iteration 42 : 0.8832643755334691
Loss in iteration 43 : 0.880172778194964
Loss in iteration 44 : 0.877071368833413
Loss in iteration 45 : 0.8739601890477618
Loss in iteration 46 : 0.8708392789671417
Loss in iteration 47 : 0.8677086773820697
Loss in iteration 48 : 0.8645684218616234
Loss in iteration 49 : 0.8614185488581949
Loss in iteration 50 : 0.8582590938012205
Loss in iteration 51 : 0.8550900911811472
Loss in iteration 52 : 0.8519115746246984
Loss in iteration 53 : 0.8487235769624152
Loss in iteration 54 : 0.8455261302893157
Loss in iteration 55 : 0.8423192660194208
Loss in iteration 56 : 0.8391030149348122
Loss in iteration 57 : 0.8358774072297956
Loss in iteration 58 : 0.8326424725507212
Loss in iteration 59 : 0.8293982400318961
Loss in iteration 60 : 0.8261447383280074
Loss in iteration 61 : 0.8228819956434407
Loss in iteration 62 : 0.8196100397587865
Loss in iteration 63 : 0.8163288980548684
Loss in iteration 64 : 0.8130385975345078
Loss in iteration 65 : 0.8097391648422936
Loss in iteration 66 : 0.8064306262825424
Loss in iteration 67 : 0.8031130078356398
Loss in iteration 68 : 0.7997863351729336
Loss in iteration 69 : 0.7964506336703168
Loss in iteration 70 : 0.7931059284206402
Loss in iteration 71 : 0.7897522442450646
Loss in iteration 72 : 0.786389605703474
Loss in iteration 73 : 0.7830180371040264
Loss in iteration 74 : 0.7796375625119359
Loss in iteration 75 : 0.776248205757563
Loss in iteration 76 : 0.7728499904438836
Loss in iteration 77 : 0.7694429399533871
Loss in iteration 78 : 0.7660270774544679
Loss in iteration 79 : 0.7626024259073629
Loss in iteration 80 : 0.7591690080696658
Loss in iteration 81 : 0.7557268465014654
Loss in iteration 82 : 0.7522759635701524
Loss in iteration 83 : 0.7488163814549087
Loss in iteration 84 : 0.7453481221509196
Loss in iteration 85 : 0.7418712074733346
Loss in iteration 86 : 0.7383856590609948
Loss in iteration 87 : 0.734891498379955
Loss in iteration 88 : 0.7313887467268049
Loss in iteration 89 : 0.7278774252318317
Loss in iteration 90 : 0.7243575548620057
Loss in iteration 91 : 0.720829156423839
Loss in iteration 92 : 0.717292250566094
Loss in iteration 93 : 0.7137468577823854
Loss in iteration 94 : 0.7101929984136613
Loss in iteration 95 : 0.7066306926505824
Loss in iteration 96 : 0.7030599605358115
Loss in iteration 97 : 0.6994808219662065
Loss in iteration 98 : 0.69589329669495
Loss in iteration 99 : 0.6922974043335803
Loss in iteration 100 : 0.6886931643539773
Loss in iteration 101 : 0.6850805960902665
Loss in iteration 102 : 0.6814597187406735
Loss in iteration 103 : 0.6778305513693202
Loss in iteration 104 : 0.6741931129079652
Loss in iteration 105 : 0.670547422157701
Loss in iteration 106 : 0.6668934977906062
Loss in iteration 107 : 0.6632313583513435
Loss in iteration 108 : 0.6595610222587375
Loss in iteration 109 : 0.6558825078072924
Loss in iteration 110 : 0.6521958331686911
Loss in iteration 111 : 0.6485010163932499
Loss in iteration 112 : 0.6447980754113449
Loss in iteration 113 : 0.6410870280348077
Loss in iteration 114 : 0.6373678919582917
Loss in iteration 115 : 0.6336406847606085
Loss in iteration 116 : 0.6299054239060394
Loss in iteration 117 : 0.626162126745623
Loss in iteration 118 : 0.6224108105184135
Loss in iteration 119 : 0.6186514923527207
Loss in iteration 120 : 0.6148841892673238
Loss in iteration 121 : 0.6111089181726667
Loss in iteration 122 : 0.6073256958720276
Loss in iteration 123 : 0.603534539062673
Loss in iteration 124 : 0.5997354643369908
Loss in iteration 125 : 0.5959284881836016
Loss in iteration 126 : 0.5921136269884575
Loss in iteration 127 : 0.5887746220416947
Loss in iteration 128 : 0.5856145256028997
Loss in iteration 129 : 0.5824479333998543
Loss in iteration 130 : 0.579274814340544
Loss in iteration 131 : 0.5760951317512577
Loss in iteration 132 : 0.5729088468924131
Loss in iteration 133 : 0.5697159208420516
Loss in iteration 134 : 0.566516315589082
Loss in iteration 135 : 0.5633099946967912
Loss in iteration 136 : 0.5600969237107037
Loss in iteration 137 : 0.5568770704033169
Loss in iteration 138 : 0.5536504049090315
Loss in iteration 139 : 0.5504168997822806
Loss in iteration 140 : 0.5471765300006274
Loss in iteration 141 : 0.5439292729280102
Loss in iteration 142 : 0.5406751082492303
Loss in iteration 143 : 0.5374140178840927
Loss in iteration 144 : 0.5341459858877636
Loss in iteration 145 : 0.5308717374355844
Loss in iteration 146 : 0.527620628895536
Loss in iteration 147 : 0.524429832134051
Loss in iteration 148 : 0.5212631581157066
Loss in iteration 149 : 0.5181256321043096
Loss in iteration 150 : 0.5150053790567005
Loss in iteration 151 : 0.5119065708949074
Loss in iteration 152 : 0.508806806599097
Loss in iteration 153 : 0.5057030402901448
Loss in iteration 154 : 0.5025979567277621
Loss in iteration 155 : 0.4994898956318766
Loss in iteration 156 : 0.4963765846180785
Loss in iteration 157 : 0.4932570950636704
Loss in iteration 158 : 0.49013085728337874
Loss in iteration 159 : 0.4869980183880255
Loss in iteration 160 : 0.4838589315268839
Loss in iteration 161 : 0.48071379519537455
Loss in iteration 162 : 0.47756300935916207
Loss in iteration 163 : 0.4744083850505218
Loss in iteration 164 : 0.4712510204445991
Loss in iteration 165 : 0.4680889883036269
Loss in iteration 166 : 0.46492146346045804
Loss in iteration 167 : 0.46174744793099076
Loss in iteration 168 : 0.45856712634431257
Loss in iteration 169 : 0.45538025558349793
Loss in iteration 170 : 0.4521868825313078
Loss in iteration 171 : 0.44898697445497415
Loss in iteration 172 : 0.4457804092941689
Loss in iteration 173 : 0.4425672626764876
Loss in iteration 174 : 0.4393474935107087
Loss in iteration 175 : 0.4361210544763015
Loss in iteration 176 : 0.43288799902101743
Loss in iteration 177 : 0.4296509724762815
Loss in iteration 178 : 0.42640912885724547
Loss in iteration 179 : 0.42316106881265336
Loss in iteration 180 : 0.4199066918626695
Loss in iteration 181 : 0.41664570815500684
Loss in iteration 182 : 0.4133781389952566
Loss in iteration 183 : 0.4101041526520685
Loss in iteration 184 : 0.40682386245703106
Loss in iteration 185 : 0.40353832953693464
Loss in iteration 186 : 0.40025504624799346
Loss in iteration 187 : 0.39697274530255794
Loss in iteration 188 : 0.3936906641200877
Loss in iteration 189 : 0.39042769837453917
Loss in iteration 190 : 0.38720221686387674
Loss in iteration 191 : 0.38397089674374213
Loss in iteration 192 : 0.3807590580667263
Loss in iteration 193 : 0.3775947049975074
Loss in iteration 194 : 0.37444532161853544
Loss in iteration 195 : 0.3712997120899868
Loss in iteration 196 : 0.36820653008791815
Loss in iteration 197 : 0.3651184989071489
Loss in iteration 198 : 0.3620259201462592
Loss in iteration 199 : 0.3589536459719925
Loss in iteration 200 : 0.35591521314951147
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.9982222222222222, training accuracy 0.8796799085452986, time elapsed: 7538 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 15.55186606234135
Loss in iteration 3 : 2.0307170212230266
Loss in iteration 4 : 1.2446124745437626
Loss in iteration 5 : 0.8919191386312977
Loss in iteration 6 : 0.3293197731609368
Loss in iteration 7 : 0.23697553317291226
Loss in iteration 8 : 0.1738849043247694
Loss in iteration 9 : 0.13077912389665708
Loss in iteration 10 : 0.10364925333557266
Loss in iteration 11 : 0.08727666966382397
Testing accuracy  of updater 5 on alg 1 with rate 4.0 = 0.9617777777777777, training accuracy 0.9908545298656759, time elapsed: 513 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 6.876129923392854
Loss in iteration 3 : 1.1590397482068648
Loss in iteration 4 : 1.0113947655815074
Loss in iteration 5 : 0.8658851926171318
Loss in iteration 6 : 0.25974113457832204
Loss in iteration 7 : 0.154479296847216
Loss in iteration 8 : 0.07731562247590316
Loss in iteration 9 : 0.061804571603934204
Loss in iteration 10 : 0.04754708298006697
Loss in iteration 11 : 0.03812805848598918
Loss in iteration 12 : 0.03260760563006042
Loss in iteration 13 : 0.02830790635096713
Loss in iteration 14 : 0.024767452455336743
Loss in iteration 15 : 0.02184360260635668
Loss in iteration 16 : 0.020053523833173
Loss in iteration 17 : 0.018880201740949353
Testing accuracy  of updater 5 on alg 1 with rate 2.8000000000000003 = 0.9635555555555556, training accuracy 0.9964275507287796, time elapsed: 1220 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.323358125867041
Loss in iteration 3 : 0.7398850954525326
Loss in iteration 4 : 0.7643465487684734
Loss in iteration 5 : 0.7922919806901189
Loss in iteration 6 : 0.7416663933539815
Loss in iteration 7 : 0.30969411958451704
Loss in iteration 8 : 0.07901572766540267
Loss in iteration 9 : 0.055785553764555205
Loss in iteration 10 : 0.04338529449780614
Loss in iteration 11 : 0.035615690757313766
Loss in iteration 12 : 0.02874352704742468
Loss in iteration 13 : 0.02284039191070541
Loss in iteration 14 : 0.019557774764391418
Loss in iteration 15 : 0.01704296925878209
Loss in iteration 16 : 0.014900759626994822
Loss in iteration 17 : 0.013491556289865288
Loss in iteration 18 : 0.012323264235598902
Loss in iteration 19 : 0.011500212498676107
Loss in iteration 20 : 0.010679703869828682
Loss in iteration 21 : 0.009967251913849848
Loss in iteration 22 : 0.00924471230966031
Loss in iteration 23 : 0.008625307662173665
Loss in iteration 24 : 0.007979540128155573
Loss in iteration 25 : 0.007470332213811012
Testing accuracy  of updater 5 on alg 1 with rate 1.6 = 0.976, training accuracy 0.9979994284081166, time elapsed: 1266 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.2973461820052943
Loss in iteration 3 : 0.30941868204162365
Loss in iteration 4 : 0.5542811508384288
Loss in iteration 5 : 0.16467588892259574
Loss in iteration 6 : 0.06720504607278328
Loss in iteration 7 : 0.041013907755585645
Loss in iteration 8 : 0.03003564660351234
Loss in iteration 9 : 0.022895336956397422
Loss in iteration 10 : 0.018067871615826868
Loss in iteration 11 : 0.01500871539528021
Loss in iteration 12 : 0.012588429945451333
Loss in iteration 13 : 0.010295663842058118
Loss in iteration 14 : 0.00867427477551404
Loss in iteration 15 : 0.007391214698878048
Testing accuracy  of updater 5 on alg 1 with rate 0.4 = 0.9724444444444444, training accuracy 0.9975707344955702, time elapsed: 634 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8238884221814344
Loss in iteration 3 : 0.2736612940522108
Loss in iteration 4 : 0.6908383248667374
Loss in iteration 5 : 0.08376094784580367
Loss in iteration 6 : 0.05591382497002158
Loss in iteration 7 : 0.03896679170326832
Loss in iteration 8 : 0.027861441964421547
Loss in iteration 9 : 0.021221780950716453
Loss in iteration 10 : 0.01654608209639685
Loss in iteration 11 : 0.01345421004615106
Loss in iteration 12 : 0.01114463336917757
Loss in iteration 13 : 0.0095354012486302
Loss in iteration 14 : 0.008140366131828442
Loss in iteration 15 : 0.006860119623820268
Loss in iteration 16 : 0.005754110638104719
Loss in iteration 17 : 0.004914615758766945
Loss in iteration 18 : 0.004873963617847519
Loss in iteration 19 : 0.005278595754785015
Testing accuracy  of updater 5 on alg 1 with rate 0.28 = 0.9671111111111111, training accuracy 0.9982852243498143, time elapsed: 790 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5410724283198536
Loss in iteration 3 : 0.3543358430707112
Loss in iteration 4 : 0.7165290025904093
Loss in iteration 5 : 0.11674708726807512
Loss in iteration 6 : 0.06778540407016609
Loss in iteration 7 : 0.051304757340980556
Loss in iteration 8 : 0.03956905193379238
Loss in iteration 9 : 0.030678946358282177
Loss in iteration 10 : 0.024298672396342477
Loss in iteration 11 : 0.019988197614225527
Loss in iteration 12 : 0.016580582504516515
Loss in iteration 13 : 0.014111983861675606
Loss in iteration 14 : 0.011951736446917445
Loss in iteration 15 : 0.010343761545036351
Loss in iteration 16 : 0.008977022556767836
Loss in iteration 17 : 0.007855211070079065
Loss in iteration 18 : 0.006756174782193535
Loss in iteration 19 : 0.005769130022867196
Loss in iteration 20 : 0.005117440354786525
Loss in iteration 21 : 0.0046113329529631425
Testing accuracy  of updater 5 on alg 1 with rate 0.16000000000000003 = 0.9768888888888889, training accuracy 0.9991426121749071, time elapsed: 869 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3510579713893099
Loss in iteration 3 : 0.6806559165262822
Loss in iteration 4 : 0.2147733202531721
Loss in iteration 5 : 0.15305312373789426
Loss in iteration 6 : 0.13071808156434112
Loss in iteration 7 : 0.11089492577329867
Loss in iteration 8 : 0.09579081758502808
Loss in iteration 9 : 0.08483635123732788
Loss in iteration 10 : 0.0861684166672991
Loss in iteration 11 : 0.06811465351038871
Loss in iteration 12 : 0.07326511018672377
Loss in iteration 13 : 0.05349136743574032
Loss in iteration 14 : 0.050972783769930134
Loss in iteration 15 : 0.042329111738653814
Loss in iteration 16 : 0.03929004202732973
Loss in iteration 17 : 0.035336172744597674
Loss in iteration 18 : 0.033085864265779794
Loss in iteration 19 : 0.03008454257224071
Testing accuracy  of updater 5 on alg 1 with rate 0.03999999999999998 = 0.9848888888888889, training accuracy 0.9927122034867105, time elapsed: 956 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.984405313533882
Loss in iteration 3 : 0.7061889303652208
Loss in iteration 4 : 0.35961697981483726
Loss in iteration 5 : 0.772786965286696
Loss in iteration 6 : 0.670404549447574
Loss in iteration 7 : 0.43574343732099274
Loss in iteration 8 : 0.29147437334411147
Loss in iteration 9 : 0.20555378941135477
Loss in iteration 10 : 0.18874074644035527
Loss in iteration 11 : 0.21686054349855993
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.9911111111111112, training accuracy 0.9727064875678766, time elapsed: 944 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6928901114374328
Loss in iteration 3 : 0.33397816771590816
Loss in iteration 4 : 0.06040468095518821
Loss in iteration 5 : 0.14514485404849653
Loss in iteration 6 : 0.2219936674725122
Loss in iteration 7 : 0.1875746797602777
Loss in iteration 8 : 0.12047140197360005
Loss in iteration 9 : 0.077106954535244
Loss in iteration 10 : 0.0519823525701081
Loss in iteration 11 : 0.04163459895471757
Loss in iteration 12 : 0.045372936605742924
Loss in iteration 13 : 0.05372189862574752
Testing accuracy  of updater 6 on alg 1 with rate 1.4000000000000001 = 0.9848888888888889, training accuracy 0.9872820805944555, time elapsed: 762 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3629680526088043
Loss in iteration 3 : 0.160345622648035
Loss in iteration 4 : 0.06435739162060596
Loss in iteration 5 : 0.13525784362619034
Loss in iteration 6 : 0.11354444956664815
Loss in iteration 7 : 0.06993105590454587
Loss in iteration 8 : 0.04554330298463812
Loss in iteration 9 : 0.03398725067878651
Loss in iteration 10 : 0.032593917587493665
Loss in iteration 11 : 0.03736914489936297
Loss in iteration 12 : 0.04101664010391024
Loss in iteration 13 : 0.04035137040460763
Loss in iteration 14 : 0.03492328478416433
Loss in iteration 15 : 0.02715325622937869
Loss in iteration 16 : 0.01952544806315389
Loss in iteration 17 : 0.014500789809892941
Loss in iteration 18 : 0.012591081642417231
Loss in iteration 19 : 0.01176458442383959
Testing accuracy  of updater 6 on alg 1 with rate 0.8 = 0.9644444444444444, training accuracy 0.996141754787082, time elapsed: 931 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.47685633207295025
Loss in iteration 3 : 0.24113086180422677
Loss in iteration 4 : 0.19020065291335725
Loss in iteration 5 : 0.1205576857350958
Loss in iteration 6 : 0.07752395503326744
Loss in iteration 7 : 0.09770283975469689
Loss in iteration 8 : 0.09084152184297221
Loss in iteration 9 : 0.06728280702593988
Loss in iteration 10 : 0.050933374572681364
Loss in iteration 11 : 0.04269236014951779
Loss in iteration 12 : 0.038932690605525386
Loss in iteration 13 : 0.037651666800461114
Loss in iteration 14 : 0.03615541317726522
Loss in iteration 15 : 0.033090257630582605
Loss in iteration 16 : 0.02844993982039223
Loss in iteration 17 : 0.02351074366564627
Loss in iteration 18 : 0.019575815963719774
Loss in iteration 19 : 0.016505899417624405
Loss in iteration 20 : 0.014389780613458983
Loss in iteration 21 : 0.013105421608685736
Loss in iteration 22 : 0.012368610477909598
Loss in iteration 23 : 0.01198216556514721
Loss in iteration 24 : 0.011590280610679467
Loss in iteration 25 : 0.011125825238394177
Loss in iteration 26 : 0.010604581299367638
Loss in iteration 27 : 0.00999178618070344
Loss in iteration 28 : 0.009261428957873961
Loss in iteration 29 : 0.008459311763204592
Loss in iteration 30 : 0.007761308276851225
Loss in iteration 31 : 0.0072387602794583925
Loss in iteration 32 : 0.006833832260725685
Loss in iteration 33 : 0.006470864497685644
Loss in iteration 34 : 0.006156131537595183
Loss in iteration 35 : 0.005874876241587465
Loss in iteration 36 : 0.005667187275205748
Loss in iteration 37 : 0.0054656341932251525
Loss in iteration 38 : 0.00527834110764195
Loss in iteration 39 : 0.00509284193828516
Loss in iteration 40 : 0.004893562508054442
Loss in iteration 41 : 0.004686895024712212
Loss in iteration 42 : 0.004484543081787942
Loss in iteration 43 : 0.004288477002656876
Loss in iteration 44 : 0.004116130213996299
Loss in iteration 45 : 0.003963615718792715
Loss in iteration 46 : 0.0038289425265935597
Loss in iteration 47 : 0.003699293164257598
Loss in iteration 48 : 0.0035906727114583953
Loss in iteration 49 : 0.003524777313501761
Loss in iteration 50 : 0.0034626418021938667
Loss in iteration 51 : 0.0034025737673536805
Loss in iteration 52 : 0.003338984305021295
Loss in iteration 53 : 0.0032777275889694563
Loss in iteration 54 : 0.003223497821839387
Loss in iteration 55 : 0.0031728051503986726
Loss in iteration 56 : 0.003130908362812596
Loss in iteration 57 : 0.003089079291286954
Loss in iteration 58 : 0.0030472928549801115
Loss in iteration 59 : 0.003012914007760144
Loss in iteration 60 : 0.002981137797156142
Loss in iteration 61 : 0.0029451738584760252
Loss in iteration 62 : 0.0029053870482502134
Loss in iteration 63 : 0.0028638334523275444
Loss in iteration 64 : 0.0028222760389972606
Loss in iteration 65 : 0.002779958409967526
Loss in iteration 66 : 0.0027397831456480006
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.9777777777777777, training accuracy 0.9991426121749071, time elapsed: 2856 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5877006351389138
Loss in iteration 3 : 0.319771344589268
Loss in iteration 4 : 0.20695019068535742
Loss in iteration 5 : 0.14603463970415606
Loss in iteration 6 : 0.1397503625587465
Loss in iteration 7 : 0.13036144674523972
Loss in iteration 8 : 0.09351187829406378
Loss in iteration 9 : 0.07701331882811414
Loss in iteration 10 : 0.07108379874308324
Loss in iteration 11 : 0.0661914544958226
Loss in iteration 12 : 0.06036124536436603
Loss in iteration 13 : 0.05339180408543613
Loss in iteration 14 : 0.045949088277702055
Loss in iteration 15 : 0.039372527820269766
Loss in iteration 16 : 0.034075020132251395
Loss in iteration 17 : 0.029564609806048546
Loss in iteration 18 : 0.02587117698237651
Loss in iteration 19 : 0.022776699265181593
Loss in iteration 20 : 0.020246883187424375
Loss in iteration 21 : 0.018149904611808813
Loss in iteration 22 : 0.016202254063652888
Loss in iteration 23 : 0.014663769834925773
Loss in iteration 24 : 0.013463441250436445
Loss in iteration 25 : 0.012387867571450483
Loss in iteration 26 : 0.011396956626373328
Loss in iteration 27 : 0.010672081518822733
Loss in iteration 28 : 0.010062623848718892
Loss in iteration 29 : 0.009548518559564458
Loss in iteration 30 : 0.009097886759529437
Loss in iteration 31 : 0.008708212869868916
Loss in iteration 32 : 0.008340700138721655
Loss in iteration 33 : 0.007979703908217246
Loss in iteration 34 : 0.007618831791438342
Loss in iteration 35 : 0.007239276642657586
Loss in iteration 36 : 0.00684855097228948
Loss in iteration 37 : 0.006494369539293486
Loss in iteration 38 : 0.006203135523763937
Loss in iteration 39 : 0.005918268624680543
Loss in iteration 40 : 0.00563666949941681
Loss in iteration 41 : 0.00537663712033202
Loss in iteration 42 : 0.005164128251927694
Loss in iteration 43 : 0.005008344881337756
Loss in iteration 44 : 0.004850743007015323
Loss in iteration 45 : 0.004707736972556229
Loss in iteration 46 : 0.0045640231366604065
Loss in iteration 47 : 0.0044254402117762486
Loss in iteration 48 : 0.004300631765524463
Loss in iteration 49 : 0.004175464068233059
Loss in iteration 50 : 0.004052670098449748
Loss in iteration 51 : 0.003938488292464454
Loss in iteration 52 : 0.0038520562700241016
Loss in iteration 53 : 0.0037682096727966836
Loss in iteration 54 : 0.0036895542165725197
Loss in iteration 55 : 0.0036127879961046316
Loss in iteration 56 : 0.003547003312965213
Loss in iteration 57 : 0.003493234718394375
Loss in iteration 58 : 0.0034396568300098917
Loss in iteration 59 : 0.0033859792989321524
Loss in iteration 60 : 0.0033334818519460225
Loss in iteration 61 : 0.0032778596131626545
Loss in iteration 62 : 0.0032217872137714513
Loss in iteration 63 : 0.0031610138514054516
Loss in iteration 64 : 0.003102533781851834
Loss in iteration 65 : 0.0030496106748071836
Loss in iteration 66 : 0.0030019405468320356
Loss in iteration 67 : 0.0029594834339703724
Loss in iteration 68 : 0.002927893073231507
Loss in iteration 69 : 0.0028959156090446084
Loss in iteration 70 : 0.002865904820647211
Loss in iteration 71 : 0.002836498458784058
Loss in iteration 72 : 0.0028087079367074518
Loss in iteration 73 : 0.0027809487975563522
Loss in iteration 74 : 0.002753210433691464
Loss in iteration 75 : 0.0027254832809761476
Loss in iteration 76 : 0.002697758717599724
Loss in iteration 77 : 0.0026700289726826064
Loss in iteration 78 : 0.0026422870437195654
Loss in iteration 79 : 0.002614526622008277
Loss in iteration 80 : 0.0025867420252923927
Loss in iteration 81 : 0.0025589281369226375
Loss in iteration 82 : 0.0025328043305965714
Loss in iteration 83 : 0.0025069911316482054
Loss in iteration 84 : 0.0024809609317670606
Loss in iteration 85 : 0.0024547274385507602
Loss in iteration 86 : 0.0024293470991848306
Loss in iteration 87 : 0.002403655518309762
Loss in iteration 88 : 0.0023776157098763287
Loss in iteration 89 : 0.0023512539791724664
Loss in iteration 90 : 0.0023245940862664607
Loss in iteration 91 : 0.0022976574930808156
Loss in iteration 92 : 0.002270463586507956
Loss in iteration 93 : 0.0022430298798872114
Loss in iteration 94 : 0.002215492933727478
Loss in iteration 95 : 0.0021885218249777633
Loss in iteration 96 : 0.0021613581811323643
Loss in iteration 97 : 0.002134013495594137
Loss in iteration 98 : 0.0021072696281157653
Loss in iteration 99 : 0.002081445239888648
Loss in iteration 100 : 0.002057704116699384
Loss in iteration 101 : 0.0020334450601081195
Loss in iteration 102 : 0.002009635659449366
Loss in iteration 103 : 0.0019899825434673453
Loss in iteration 104 : 0.0019700629473505637
Loss in iteration 105 : 0.001950081034001861
Loss in iteration 106 : 0.00192984039909221
Loss in iteration 107 : 0.0019089565851031602
Loss in iteration 108 : 0.0018874859036857984
Loss in iteration 109 : 0.001865479186154845
Loss in iteration 110 : 0.001843237711606551
Loss in iteration 111 : 0.001821191452498546
Loss in iteration 112 : 0.0017999437434010526
Loss in iteration 113 : 0.001778830450361561
Loss in iteration 114 : 0.001757335781524639
Testing accuracy  of updater 6 on alg 1 with rate 0.14 = 0.9822222222222222, training accuracy 0.999285510145756, time elapsed: 5198 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7108312728227363
Loss in iteration 3 : 0.35225751886160234
Loss in iteration 4 : 0.19528931913776582
Loss in iteration 5 : 0.16960470710555733
Loss in iteration 6 : 0.14019535865371413
Loss in iteration 7 : 0.11399856688686971
Loss in iteration 8 : 0.09439029038762028
Loss in iteration 9 : 0.0818861179526842
Loss in iteration 10 : 0.07307696655074873
Loss in iteration 11 : 0.0660994392757946
Loss in iteration 12 : 0.05981666600366979
Loss in iteration 13 : 0.053991054930547594
Loss in iteration 14 : 0.048324996646563846
Loss in iteration 15 : 0.04289857495105801
Loss in iteration 16 : 0.03805735162649919
Loss in iteration 17 : 0.033781584114341304
Loss in iteration 18 : 0.029908285690483743
Loss in iteration 19 : 0.026417487837631558
Loss in iteration 20 : 0.02335448648553755
Loss in iteration 21 : 0.020877620254128395
Loss in iteration 22 : 0.018730413650513963
Loss in iteration 23 : 0.016895537430735862
Loss in iteration 24 : 0.015378619063806782
Loss in iteration 25 : 0.014210986449336186
Loss in iteration 26 : 0.013234511352748507
Loss in iteration 27 : 0.012385506167909151
Loss in iteration 28 : 0.011634604624063034
Loss in iteration 29 : 0.010963049065107509
Loss in iteration 30 : 0.010333130202765044
Loss in iteration 31 : 0.009768761044092933
Loss in iteration 32 : 0.009233196587755534
Loss in iteration 33 : 0.008704058626032306
Loss in iteration 34 : 0.008205896354606163
Loss in iteration 35 : 0.007779471214604194
Loss in iteration 36 : 0.007378402078926429
Loss in iteration 37 : 0.007009842241848048
Loss in iteration 38 : 0.006682514951791858
Loss in iteration 39 : 0.006376008616108654
Loss in iteration 40 : 0.006098682718207823
Loss in iteration 41 : 0.005841563397960809
Loss in iteration 42 : 0.0055969512757096445
Loss in iteration 43 : 0.005362610267465604
Loss in iteration 44 : 0.005136497887259768
Loss in iteration 45 : 0.004912294346267523
Loss in iteration 46 : 0.004687888109988871
Loss in iteration 47 : 0.0044944151456629755
Loss in iteration 48 : 0.004356058727308829
Loss in iteration 49 : 0.00422583460604028
Loss in iteration 50 : 0.00411210584795095
Loss in iteration 51 : 0.0040040892842022205
Loss in iteration 52 : 0.003899821050659931
Loss in iteration 53 : 0.0038015223535011098
Loss in iteration 54 : 0.0037159065866376913
Loss in iteration 55 : 0.0036448926980259125
Loss in iteration 56 : 0.003581040360752629
Loss in iteration 57 : 0.003522650149789018
Loss in iteration 58 : 0.003470044515121486
Loss in iteration 59 : 0.0034174739380316696
Loss in iteration 60 : 0.003365098499545736
Loss in iteration 61 : 0.003312591870170261
Loss in iteration 62 : 0.00325931795427021
Loss in iteration 63 : 0.0032097523722557243
Loss in iteration 64 : 0.0031608324644453595
Loss in iteration 65 : 0.0031143774706729947
Loss in iteration 66 : 0.003072631495866199
Loss in iteration 67 : 0.0030318501360899986
Loss in iteration 68 : 0.002991759946500044
Loss in iteration 69 : 0.002952369147946224
Loss in iteration 70 : 0.002915014543095255
Loss in iteration 71 : 0.0028778504775820984
Loss in iteration 72 : 0.0028408516257520233
Loss in iteration 73 : 0.0028039951535300577
Loss in iteration 74 : 0.0027685899402916632
Loss in iteration 75 : 0.0027348171505563665
Loss in iteration 76 : 0.002705083240482928
Loss in iteration 77 : 0.002676397896821275
Loss in iteration 78 : 0.0026476121598293283
Loss in iteration 79 : 0.002618319479710041
Loss in iteration 80 : 0.0025887353346401897
Loss in iteration 81 : 0.0025587164843858356
Loss in iteration 82 : 0.002529047488075567
Loss in iteration 83 : 0.0024990488520783105
Loss in iteration 84 : 0.0024700672742125193
Loss in iteration 85 : 0.0024412195227367624
Loss in iteration 86 : 0.002412570949927383
Loss in iteration 87 : 0.002387238102564719
Loss in iteration 88 : 0.0023618789546172745
Loss in iteration 89 : 0.002338582209632124
Loss in iteration 90 : 0.002317665713730241
Loss in iteration 91 : 0.002296877320906422
Loss in iteration 92 : 0.002276110010926983
Loss in iteration 93 : 0.0022560900161241386
Loss in iteration 94 : 0.002236158825598135
Loss in iteration 95 : 0.0022161109557142693
Loss in iteration 96 : 0.0021959542099521927
Loss in iteration 97 : 0.0021757678219274856
Loss in iteration 98 : 0.002155918438609489
Loss in iteration 99 : 0.0021360176540865202
Loss in iteration 100 : 0.0021160670225071885
Loss in iteration 101 : 0.002096067948080901
Loss in iteration 102 : 0.002076021699802406
Loss in iteration 103 : 0.0020559294247294007
Loss in iteration 104 : 0.002035792159955356
Loss in iteration 105 : 0.002015686524548442
Loss in iteration 106 : 0.001995645745978741
Loss in iteration 107 : 0.001977433338636586
Loss in iteration 108 : 0.001959272054512054
Loss in iteration 109 : 0.0019409589376895857
Loss in iteration 110 : 0.0019224350070782528
Loss in iteration 111 : 0.0019037175464015797
Loss in iteration 112 : 0.001884841191735535
Loss in iteration 113 : 0.0018659035570077007
Loss in iteration 114 : 0.0018468590800731304
Loss in iteration 115 : 0.0018277514895353698
Loss in iteration 116 : 0.00181065482440828
Loss in iteration 117 : 0.001793818453452921
Loss in iteration 118 : 0.0017769201952326202
Loss in iteration 119 : 0.0017600204583572503
Loss in iteration 120 : 0.0017441207219871665
Loss in iteration 121 : 0.0017284462937280217
Loss in iteration 122 : 0.0017132871474110666
Loss in iteration 123 : 0.0016975587901372833
Loss in iteration 124 : 0.0016823141762588494
Loss in iteration 125 : 0.00166658785950034
Loss in iteration 126 : 0.0016500655883518133
Loss in iteration 127 : 0.0016328218390225665
Loss in iteration 128 : 0.001616027551395153
Testing accuracy  of updater 6 on alg 1 with rate 0.08000000000000002 = 0.9822222222222222, training accuracy 0.999285510145756, time elapsed: 5458 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9197727162366521
Loss in iteration 3 : 0.7739074112845075
Loss in iteration 4 : 0.5742212366384969
Loss in iteration 5 : 0.3966353045373099
Loss in iteration 6 : 0.27508900445049383
Loss in iteration 7 : 0.21208097406787907
Loss in iteration 8 : 0.19067478021540277
Loss in iteration 9 : 0.17702173161903112
Loss in iteration 10 : 0.16436732333076842
Loss in iteration 11 : 0.1512064077172546
Loss in iteration 12 : 0.13738194363422448
Loss in iteration 13 : 0.12326012664072665
Loss in iteration 14 : 0.1102010990762203
Loss in iteration 15 : 0.09872041284837152
Loss in iteration 16 : 0.08966976440408518
Loss in iteration 17 : 0.08211502593807403
Loss in iteration 18 : 0.07597372692782459
Loss in iteration 19 : 0.07078856303937932
Loss in iteration 20 : 0.06631951114537582
Loss in iteration 21 : 0.062277724637945
Loss in iteration 22 : 0.05851370321640941
Loss in iteration 23 : 0.055015970986426556
Loss in iteration 24 : 0.05186310349674969
Loss in iteration 25 : 0.04890786658466841
Loss in iteration 26 : 0.04611413170106327
Loss in iteration 27 : 0.0434485361621954
Loss in iteration 28 : 0.04091336681811285
Loss in iteration 29 : 0.038474627976476466
Loss in iteration 30 : 0.03612935691648296
Loss in iteration 31 : 0.0339268971717108
Loss in iteration 32 : 0.03179178593746833
Loss in iteration 33 : 0.029854664027975625
Loss in iteration 34 : 0.028180272104546274
Loss in iteration 35 : 0.026636511448087298
Loss in iteration 36 : 0.025274493123459536
Loss in iteration 37 : 0.024046663934383993
Loss in iteration 38 : 0.022905838702151512
Loss in iteration 39 : 0.02181290502485593
Loss in iteration 40 : 0.02079123223787441
Loss in iteration 41 : 0.019872172957956888
Loss in iteration 42 : 0.019038255882502686
Loss in iteration 43 : 0.018313945462399884
Loss in iteration 44 : 0.017658065188491185
Loss in iteration 45 : 0.01707549582142053
Loss in iteration 46 : 0.016553397939278673
Loss in iteration 47 : 0.016069973405559143
Loss in iteration 48 : 0.015626773299965632
Loss in iteration 49 : 0.015214017037447977
Loss in iteration 50 : 0.014809920478942565
Loss in iteration 51 : 0.014408569461537056
Loss in iteration 52 : 0.014012828579754022
Loss in iteration 53 : 0.01363843813747781
Loss in iteration 54 : 0.01327030294766896
Loss in iteration 55 : 0.012905033124398044
Loss in iteration 56 : 0.012547675432935178
Loss in iteration 57 : 0.012205482342632683
Loss in iteration 58 : 0.011891225855348875
Loss in iteration 59 : 0.011590495498442174
Loss in iteration 60 : 0.011299420250917921
Loss in iteration 61 : 0.011016791614494693
Loss in iteration 62 : 0.010736937351644058
Loss in iteration 63 : 0.010460536870592275
Loss in iteration 64 : 0.010184122715434072
Loss in iteration 65 : 0.009909874454242318
Loss in iteration 66 : 0.00964530766239395
Loss in iteration 67 : 0.009389871075892071
Loss in iteration 68 : 0.009140167516137426
Loss in iteration 69 : 0.008895527307878568
Loss in iteration 70 : 0.008653334977588387
Loss in iteration 71 : 0.0084182651196726
Loss in iteration 72 : 0.008197122348166807
Loss in iteration 73 : 0.007989201953190259
Loss in iteration 74 : 0.0077858168102637685
Loss in iteration 75 : 0.007586491225265236
Loss in iteration 76 : 0.007390861542638303
Loss in iteration 77 : 0.007203573432533052
Loss in iteration 78 : 0.007025489144208746
Loss in iteration 79 : 0.0068663899492720955
Loss in iteration 80 : 0.006720556100087721
Loss in iteration 81 : 0.006579304662827649
Loss in iteration 82 : 0.006443527644134767
Loss in iteration 83 : 0.006309982753707324
Loss in iteration 84 : 0.006180733952853292
Loss in iteration 85 : 0.006055528521763099
Loss in iteration 86 : 0.005933553786533953
Loss in iteration 87 : 0.0058144477229659965
Loss in iteration 88 : 0.005696824255739536
Loss in iteration 89 : 0.005582652845751746
Loss in iteration 90 : 0.005473490843401181
Loss in iteration 91 : 0.005366493824668982
Loss in iteration 92 : 0.005261879905596901
Loss in iteration 93 : 0.005162777237502299
Loss in iteration 94 : 0.005069568432571612
Loss in iteration 95 : 0.004979695532920606
Loss in iteration 96 : 0.004892603432113215
Loss in iteration 97 : 0.004806980207652085
Loss in iteration 98 : 0.0047258580993625844
Loss in iteration 99 : 0.004645028326754876
Loss in iteration 100 : 0.0045653352059577285
Loss in iteration 101 : 0.004489034582030937
Loss in iteration 102 : 0.004416388880734573
Loss in iteration 103 : 0.004349895310298087
Loss in iteration 104 : 0.004285873760921681
Loss in iteration 105 : 0.0042240818408044644
Loss in iteration 106 : 0.004162989422010002
Loss in iteration 107 : 0.004103705352029872
Loss in iteration 108 : 0.00404512429898952
Loss in iteration 109 : 0.003987085551497384
Loss in iteration 110 : 0.0039293311486617985
Loss in iteration 111 : 0.003871729230195555
Loss in iteration 112 : 0.0038149414817984445
Loss in iteration 113 : 0.003758396083184389
Loss in iteration 114 : 0.0037023500973287836
Loss in iteration 115 : 0.003647874927890971
Loss in iteration 116 : 0.0035948130245702812
Loss in iteration 117 : 0.0035419440989909235
Loss in iteration 118 : 0.0034891328380649523
Loss in iteration 119 : 0.003436836074695171
Loss in iteration 120 : 0.0033862197948597347
Loss in iteration 121 : 0.003335719804437934
Loss in iteration 122 : 0.0032853903873156583
Loss in iteration 123 : 0.003237565266435601
Loss in iteration 124 : 0.0031931654917386676
Loss in iteration 125 : 0.003151482632088799
Loss in iteration 126 : 0.003112713948123151
Loss in iteration 127 : 0.0030758716274532187
Loss in iteration 128 : 0.0030414516698426056
Loss in iteration 129 : 0.003007580885250059
Loss in iteration 130 : 0.002973376296376902
Loss in iteration 131 : 0.002939204382503165
Loss in iteration 132 : 0.0029051673612439733
Loss in iteration 133 : 0.002872892481394306
Loss in iteration 134 : 0.0028429652845266334
Loss in iteration 135 : 0.002813866649916864
Loss in iteration 136 : 0.0027840738489301434
Loss in iteration 137 : 0.002753405774282336
Loss in iteration 138 : 0.0027239630226745123
Loss in iteration 139 : 0.0026982458654405956
Loss in iteration 140 : 0.0026728632698148694
Loss in iteration 141 : 0.0026481087883020687
Loss in iteration 142 : 0.0026235095640386417
Loss in iteration 143 : 0.0025988490414429965
Loss in iteration 144 : 0.0025744671368810755
Loss in iteration 145 : 0.002550245942102995
Loss in iteration 146 : 0.002526026598096668
Loss in iteration 147 : 0.002501929888143258
Loss in iteration 148 : 0.0024785392299657713
Loss in iteration 149 : 0.0024558727987994597
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.9822222222222222, training accuracy 0.999285510145756, time elapsed: 6184 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 9.130154687794766
Loss in iteration 3 : 3.2436830187470402
Loss in iteration 4 : 1.3682695575346209
Loss in iteration 5 : 2.747990765287853
Loss in iteration 6 : 3.1184804332375475
Loss in iteration 7 : 2.2164051004062753
Loss in iteration 8 : 1.472516451628934
Loss in iteration 9 : 0.9506470753695284
Loss in iteration 10 : 0.6398437974805065
Loss in iteration 11 : 0.5386251674637845
Loss in iteration 12 : 0.5779648773888055
Loss in iteration 13 : 0.6528378837025984
Testing accuracy  of updater 7 on alg 1 with rate 14.0 = 0.9973333333333333, training accuracy 0.9774221206058874, time elapsed: 609 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.7767367794941082
Loss in iteration 3 : 0.7261060756355951
Loss in iteration 4 : 0.09822670213300531
Loss in iteration 5 : 0.3742485795768412
Loss in iteration 6 : 0.5522327784868654
Loss in iteration 7 : 0.3775365055247397
Loss in iteration 8 : 0.22054662350683177
Loss in iteration 9 : 0.13896583740511514
Loss in iteration 10 : 0.09042208449742081
Loss in iteration 11 : 0.06737304598894547
Loss in iteration 12 : 0.07416865154444444
Loss in iteration 13 : 0.09785782794606464
Testing accuracy  of updater 7 on alg 1 with rate 9.799999999999999 = 0.9937777777777778, training accuracy 0.9878536724778508, time elapsed: 696 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7736202586761761
Loss in iteration 3 : 0.3311151207716915
Loss in iteration 4 : 0.08687249215084394
Loss in iteration 5 : 0.21023035282102295
Loss in iteration 6 : 0.26918602555979115
Loss in iteration 7 : 0.1968402726480246
Loss in iteration 8 : 0.1223803897446074
Loss in iteration 9 : 0.07614371097847195
Loss in iteration 10 : 0.04917436414379364
Loss in iteration 11 : 0.03791230974370801
Loss in iteration 12 : 0.04170319387592071
Loss in iteration 13 : 0.051802673731980414
Testing accuracy  of updater 7 on alg 1 with rate 5.6 = 0.9928888888888889, training accuracy 0.9907116318948271, time elapsed: 587 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.35053930661453087
Loss in iteration 3 : 0.10655384771254148
Loss in iteration 4 : 0.138135519818054
Loss in iteration 5 : 0.09831485249543401
Loss in iteration 6 : 0.05344032400388261
Loss in iteration 7 : 0.04023971515356098
Loss in iteration 8 : 0.03854584965665551
Loss in iteration 9 : 0.039292657814217744
Loss in iteration 10 : 0.038257671586313116
Loss in iteration 11 : 0.033424219308539775
Loss in iteration 12 : 0.027113869931883233
Loss in iteration 13 : 0.02122288951902163
Loss in iteration 14 : 0.01668987461502903
Loss in iteration 15 : 0.014328324400917988
Loss in iteration 16 : 0.012941996685234486
Loss in iteration 17 : 0.011840764542192654
Loss in iteration 18 : 0.010773006795010422
Loss in iteration 19 : 0.009852404169737624
Loss in iteration 20 : 0.009129932657299937
Loss in iteration 21 : 0.008483567645542887
Loss in iteration 22 : 0.007967439321719616
Loss in iteration 23 : 0.007536900367931516
Loss in iteration 24 : 0.007147356517141372
Loss in iteration 25 : 0.0068058583565284375
Loss in iteration 26 : 0.006531769371538715
Loss in iteration 27 : 0.0062761004444220085
Loss in iteration 28 : 0.006146235697064487
Loss in iteration 29 : 0.006074045116523413
Loss in iteration 30 : 0.005999028508226672
Loss in iteration 31 : 0.005921540236620866
Loss in iteration 32 : 0.005871124655712348
Loss in iteration 33 : 0.005838916417342038
Loss in iteration 34 : 0.005810205191796161
Loss in iteration 35 : 0.005781926977401582
Loss in iteration 36 : 0.0057437410644789475
Loss in iteration 37 : 0.005692940931220782
Loss in iteration 38 : 0.005630761017367179
Loss in iteration 39 : 0.0055604612171638685
Loss in iteration 40 : 0.005492856358999095
Loss in iteration 41 : 0.005425286402565646
Loss in iteration 42 : 0.005351480025450121
Loss in iteration 43 : 0.0052806332308645365
Loss in iteration 44 : 0.005207732693439347
Loss in iteration 45 : 0.005129258649101114
Testing accuracy  of updater 7 on alg 1 with rate 1.4 = 0.9706666666666667, training accuracy 0.9989997142040583, time elapsed: 1940 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.36299865355514765
Loss in iteration 3 : 0.19596321361099175
Loss in iteration 4 : 0.11868941042775467
Loss in iteration 5 : 0.07845439438351465
Loss in iteration 6 : 0.061786145540785674
Loss in iteration 7 : 0.05703265851989495
Loss in iteration 8 : 0.05065639665437698
Loss in iteration 9 : 0.04284097656759507
Loss in iteration 10 : 0.03447197134681949
Loss in iteration 11 : 0.027233418560392582
Loss in iteration 12 : 0.02177721922736479
Loss in iteration 13 : 0.0180285041993883
Loss in iteration 14 : 0.01573135571050841
Loss in iteration 15 : 0.014315303192541763
Loss in iteration 16 : 0.013023043804989141
Loss in iteration 17 : 0.011518380913922502
Loss in iteration 18 : 0.01035686225717662
Loss in iteration 19 : 0.00936943762359466
Loss in iteration 20 : 0.008528614617019582
Loss in iteration 21 : 0.007707003913966001
Loss in iteration 22 : 0.007000560004140639
Loss in iteration 23 : 0.006393497790939053
Loss in iteration 24 : 0.005918083317602142
Loss in iteration 25 : 0.0055873205249954115
Loss in iteration 26 : 0.005310253800761265
Loss in iteration 27 : 0.005111944680631153
Loss in iteration 28 : 0.004990018559114765
Loss in iteration 29 : 0.004900102411229836
Loss in iteration 30 : 0.004829230071072088
Loss in iteration 31 : 0.004786656572472877
Loss in iteration 32 : 0.004749243960156003
Loss in iteration 33 : 0.004700229764081592
Loss in iteration 34 : 0.004640748376026382
Loss in iteration 35 : 0.004571821753390742
Loss in iteration 36 : 0.004494370561675614
Loss in iteration 37 : 0.004409224212684371
Loss in iteration 38 : 0.004317129907894702
Loss in iteration 39 : 0.0042187607855973175
Loss in iteration 40 : 0.004114723260625561
Loss in iteration 41 : 0.004005747608066285
Loss in iteration 42 : 0.00390376585097467
Loss in iteration 43 : 0.0038555033533342296
Loss in iteration 44 : 0.003812182548669445
Loss in iteration 45 : 0.0037687294276975815
Loss in iteration 46 : 0.00372539859489959
Loss in iteration 47 : 0.0036872349218250305
Loss in iteration 48 : 0.003658102655326665
Loss in iteration 49 : 0.0036277572541675847
Loss in iteration 50 : 0.003595617827082406
Loss in iteration 51 : 0.0035618573546380835
Loss in iteration 52 : 0.0035266316703257542
Loss in iteration 53 : 0.0034900811598244977
Loss in iteration 54 : 0.003452332291867938
Loss in iteration 55 : 0.003413498997401967
Loss in iteration 56 : 0.0033736839120680874
Loss in iteration 57 : 0.003334305931402274
Loss in iteration 58 : 0.0032961822719007935
Loss in iteration 59 : 0.0032578683129621732
Loss in iteration 60 : 0.003219378940805434
Testing accuracy  of updater 7 on alg 1 with rate 0.98 = 0.9813333333333333, training accuracy 0.9989997142040583, time elapsed: 2487 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6030388633773238
Loss in iteration 3 : 0.34009752017829353
Loss in iteration 4 : 0.2201356429053476
Loss in iteration 5 : 0.14923087724720557
Loss in iteration 6 : 0.09328776949183477
Loss in iteration 7 : 0.11374911285181713
Loss in iteration 8 : 0.10997971635236249
Loss in iteration 9 : 0.08070803370838375
Loss in iteration 10 : 0.0600156239207619
Loss in iteration 11 : 0.049139039300542364
Loss in iteration 12 : 0.043532735150067794
Loss in iteration 13 : 0.04033850187327924
Loss in iteration 14 : 0.038180258655890094
Loss in iteration 15 : 0.03522371944699184
Loss in iteration 16 : 0.03095470642281482
Loss in iteration 17 : 0.02632065959427876
Loss in iteration 18 : 0.02218293117932006
Loss in iteration 19 : 0.01861143119294397
Loss in iteration 20 : 0.01567383326230603
Loss in iteration 21 : 0.01336544715155089
Loss in iteration 22 : 0.011876727118130162
Loss in iteration 23 : 0.010794946104644291
Loss in iteration 24 : 0.009925600103865668
Loss in iteration 25 : 0.009161707114365248
Loss in iteration 26 : 0.008495790034305931
Loss in iteration 27 : 0.007838378903774739
Loss in iteration 28 : 0.007221888657123971
Loss in iteration 29 : 0.00675530456982729
Loss in iteration 30 : 0.0063528862924855465
Loss in iteration 31 : 0.005947793614265856
Loss in iteration 32 : 0.005673290780925701
Loss in iteration 33 : 0.00552278361040155
Loss in iteration 34 : 0.005396323867580028
Loss in iteration 35 : 0.005281044370159196
Loss in iteration 36 : 0.00516861991473742
Loss in iteration 37 : 0.005058080209801342
Loss in iteration 38 : 0.004939684183124806
Loss in iteration 39 : 0.004813630882830969
Loss in iteration 40 : 0.004680659728220106
Loss in iteration 41 : 0.004567267175726873
Loss in iteration 42 : 0.004456886814866689
Loss in iteration 43 : 0.0043453228202393985
Loss in iteration 44 : 0.00423597362092355
Loss in iteration 45 : 0.004141988917607028
Loss in iteration 46 : 0.0040513849253671725
Loss in iteration 47 : 0.003961458334830388
Loss in iteration 48 : 0.003868542402267016
Loss in iteration 49 : 0.0037786802341198316
Loss in iteration 50 : 0.0036943740828306146
Loss in iteration 51 : 0.0036235580781049602
Loss in iteration 52 : 0.0035535870780073497
Loss in iteration 53 : 0.003485750299053866
Loss in iteration 54 : 0.0034392453072125953
Loss in iteration 55 : 0.003397202383091805
Loss in iteration 56 : 0.0033557475446473924
Loss in iteration 57 : 0.003314818931206409
Loss in iteration 58 : 0.003274571183420403
Loss in iteration 59 : 0.003240740314310171
Loss in iteration 60 : 0.003208990281392708
Loss in iteration 61 : 0.003177666736790315
Loss in iteration 62 : 0.003148169710111916
Loss in iteration 63 : 0.0031229245153565764
Loss in iteration 64 : 0.0030978085749737987
Loss in iteration 65 : 0.0030728067050751202
Loss in iteration 66 : 0.0030479052241044694
Loss in iteration 67 : 0.0030230918039560063
Loss in iteration 68 : 0.002998355335846037
Loss in iteration 69 : 0.002973685809476905
Loss in iteration 70 : 0.0029490742041756264
Loss in iteration 71 : 0.0029245123908205914
Loss in iteration 72 : 0.002899993043487229
Loss in iteration 73 : 0.0028761959830461287
Testing accuracy  of updater 7 on alg 1 with rate 0.5599999999999999 = 0.9822222222222222, training accuracy 0.9991426121749071, time elapsed: 2905 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9005660348902746
Loss in iteration 3 : 0.7114198928634466
Loss in iteration 4 : 0.4509533775312212
Loss in iteration 5 : 0.31774540410967744
Loss in iteration 6 : 0.2314426856051768
Loss in iteration 7 : 0.19594332601246275
Loss in iteration 8 : 0.1688235120493178
Loss in iteration 9 : 0.14071670530908675
Loss in iteration 10 : 0.11745006706864874
Loss in iteration 11 : 0.10697369378050951
Loss in iteration 12 : 0.10321612713486791
Loss in iteration 13 : 0.09396070586735523
Loss in iteration 14 : 0.07921784457077857
Loss in iteration 15 : 0.06592435390546121
Loss in iteration 16 : 0.05699758737238922
Loss in iteration 17 : 0.05143415757246223
Loss in iteration 18 : 0.047314974825701475
Loss in iteration 19 : 0.04401984657228838
Loss in iteration 20 : 0.04089234798326178
Loss in iteration 21 : 0.03763188246348412
Loss in iteration 22 : 0.034339537460962986
Loss in iteration 23 : 0.03128569648102152
Loss in iteration 24 : 0.02844065245667775
Loss in iteration 25 : 0.025942529708139928
Loss in iteration 26 : 0.023707194368073455
Loss in iteration 27 : 0.02174524547843311
Loss in iteration 28 : 0.02006810900064741
Loss in iteration 29 : 0.01856984493886263
Loss in iteration 30 : 0.01723731470871744
Loss in iteration 31 : 0.01611715365291622
Loss in iteration 32 : 0.015100220549229151
Loss in iteration 33 : 0.014144508742265307
Loss in iteration 34 : 0.013247094437977384
Loss in iteration 35 : 0.012431615381754673
Loss in iteration 36 : 0.011672246664364937
Loss in iteration 37 : 0.010971248959288188
Loss in iteration 38 : 0.01031695871311441
Loss in iteration 39 : 0.009699363360709028
Loss in iteration 40 : 0.009216595284535441
Loss in iteration 41 : 0.008822840179007469
Loss in iteration 42 : 0.008480114766539155
Loss in iteration 43 : 0.008151746748521128
Loss in iteration 44 : 0.007835199194479489
Loss in iteration 45 : 0.007552789795513104
Loss in iteration 46 : 0.007311615426187883
Loss in iteration 47 : 0.007076628588089491
Loss in iteration 48 : 0.006838798593676464
Loss in iteration 49 : 0.006596358964184042
Loss in iteration 50 : 0.006358086126984228
Loss in iteration 51 : 0.006127899161202954
Loss in iteration 52 : 0.005910886025035594
Loss in iteration 53 : 0.005700453884562089
Loss in iteration 54 : 0.005499919756558493
Loss in iteration 55 : 0.005325562205768112
Loss in iteration 56 : 0.005160581879203241
Loss in iteration 57 : 0.0050226889124866175
Loss in iteration 58 : 0.004894086497433284
Loss in iteration 59 : 0.004775648340713576
Loss in iteration 60 : 0.004663809351887975
Loss in iteration 61 : 0.0045538363160500936
Loss in iteration 62 : 0.0044455411020427925
Loss in iteration 63 : 0.004345968090717055
Loss in iteration 64 : 0.004248839126350208
Loss in iteration 65 : 0.004153310399255079
Loss in iteration 66 : 0.004061811443330074
Loss in iteration 67 : 0.003973329796988627
Loss in iteration 68 : 0.0038927781515810172
Loss in iteration 69 : 0.003818506125489146
Loss in iteration 70 : 0.003745851092018416
Loss in iteration 71 : 0.0036798663527787163
Loss in iteration 72 : 0.0036197561780935985
Loss in iteration 73 : 0.003562183120843512
Loss in iteration 74 : 0.0035069239026849133
Loss in iteration 75 : 0.0034531792675134487
Loss in iteration 76 : 0.003400369505120853
Loss in iteration 77 : 0.003348076008508448
Loss in iteration 78 : 0.003298156606721049
Loss in iteration 79 : 0.003249442728936279
Loss in iteration 80 : 0.0032026218639848833
Loss in iteration 81 : 0.003158459729453736
Loss in iteration 82 : 0.0031152983492444605
Loss in iteration 83 : 0.003072668283721875
Loss in iteration 84 : 0.0030302213937719157
Loss in iteration 85 : 0.0029880110326805914
Loss in iteration 86 : 0.0029458088698369692
Loss in iteration 87 : 0.0029034389333085667
Loss in iteration 88 : 0.0028613454449395274
Loss in iteration 89 : 0.0028205618549980646
Loss in iteration 90 : 0.0027827062617549873
Loss in iteration 91 : 0.002744669981052236
Loss in iteration 92 : 0.0027087350742865586
Loss in iteration 93 : 0.002679923161006677
Loss in iteration 94 : 0.002651284814917592
Loss in iteration 95 : 0.002622800151315252
Loss in iteration 96 : 0.0025946840234152904
Loss in iteration 97 : 0.0025666488539657652
Loss in iteration 98 : 0.002538675319039172
Loss in iteration 99 : 0.002510754591364956
Loss in iteration 100 : 0.002483264634051568
Loss in iteration 101 : 0.0024570753923335445
Loss in iteration 102 : 0.0024332266379772996
Loss in iteration 103 : 0.0024096506504766506
Testing accuracy  of updater 7 on alg 1 with rate 0.1399999999999999 = 0.9866666666666667, training accuracy 0.999285510145756, time elapsed: 4550 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.240679100610508
Loss in iteration 3 : 0.31686414403581026
Loss in iteration 4 : 0.2917798535761096
Loss in iteration 5 : 0.26003005498593484
Loss in iteration 6 : 0.21587023076410322
Loss in iteration 7 : 0.172209099430216
Loss in iteration 8 : 0.13789686804034254
Loss in iteration 9 : 0.10926397612559771
Loss in iteration 10 : 0.09190297983669617
Loss in iteration 11 : 0.07936691938165097
Loss in iteration 12 : 0.0681872703933321
Loss in iteration 13 : 0.05860159757083706
Loss in iteration 14 : 0.049108185972346806
Loss in iteration 15 : 0.04185146699998454
Loss in iteration 16 : 0.03614916494843415
Loss in iteration 17 : 0.03209629110066446
Loss in iteration 18 : 0.0287433605474286
Loss in iteration 19 : 0.025790134939392174
Loss in iteration 20 : 0.023635529780424876
Loss in iteration 21 : 0.02174713131409141
Loss in iteration 22 : 0.02038191951961814
Loss in iteration 23 : 0.01911393749739957
Loss in iteration 24 : 0.018314461660458022
Loss in iteration 25 : 0.017697705007967655
Loss in iteration 26 : 0.01729922199935778
Loss in iteration 27 : 0.016880579331883052
Loss in iteration 28 : 0.016538545881300548
Loss in iteration 29 : 0.016215791523500847
Loss in iteration 30 : 0.015905688964318064
Loss in iteration 31 : 0.015604681299905632
Loss in iteration 32 : 0.015354632802020056
Testing accuracy  of updater 8 on alg 1 with rate 1.4000000000000004 = 0.9564444444444444, training accuracy 0.9982852243498143, time elapsed: 1359 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7566740428012529
Loss in iteration 3 : 0.11811565127471842
Loss in iteration 4 : 0.14301782735538532
Loss in iteration 5 : 0.09591926717858335
Loss in iteration 6 : 0.07725438897054437
Loss in iteration 7 : 0.06158383760827213
Loss in iteration 8 : 0.04802223604926687
Loss in iteration 9 : 0.037857626564361174
Loss in iteration 10 : 0.030615736925585176
Loss in iteration 11 : 0.025229011099828693
Loss in iteration 12 : 0.020998029434539732
Loss in iteration 13 : 0.017834809931761553
Loss in iteration 14 : 0.01577550897482448
Loss in iteration 15 : 0.01426773071776568
Loss in iteration 16 : 0.012965527285121162
Loss in iteration 17 : 0.011726408940378644
Loss in iteration 18 : 0.010631550576511148
Loss in iteration 19 : 0.010030287073589538
Loss in iteration 20 : 0.009677648785187305
Loss in iteration 21 : 0.009398279454399281
Loss in iteration 22 : 0.00912637540069423
Loss in iteration 23 : 0.008885875306182013
Loss in iteration 24 : 0.008617227371855267
Loss in iteration 25 : 0.008405372818731743
Loss in iteration 26 : 0.008217716584260447
Loss in iteration 27 : 0.008037247030321645
Loss in iteration 28 : 0.007864700138213885
Loss in iteration 29 : 0.0076763834679304845
Loss in iteration 30 : 0.007479584404968715
Loss in iteration 31 : 0.0072873611107532035
Loss in iteration 32 : 0.007098468492961455
Loss in iteration 33 : 0.0069016017337703225
Testing accuracy  of updater 8 on alg 1 with rate 0.9800000000000001 = 0.96, training accuracy 0.9982852243498143, time elapsed: 1349 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5327921243544168
Loss in iteration 3 : 0.10338220134652532
Loss in iteration 4 : 0.12435504883541988
Loss in iteration 5 : 0.07452470614155106
Loss in iteration 6 : 0.06210721935617727
Loss in iteration 7 : 0.049932324992714196
Loss in iteration 8 : 0.03886306739321828
Loss in iteration 9 : 0.030625875500887613
Loss in iteration 10 : 0.0241282372390158
Loss in iteration 11 : 0.019921257104324176
Loss in iteration 12 : 0.016604010419782166
Loss in iteration 13 : 0.014105656944339767
Loss in iteration 14 : 0.01234997343175253
Loss in iteration 15 : 0.011002880529716331
Loss in iteration 16 : 0.009878314778734456
Loss in iteration 17 : 0.008904240717008441
Loss in iteration 18 : 0.008205424256500027
Loss in iteration 19 : 0.007755956489874123
Loss in iteration 20 : 0.0074458026370521095
Loss in iteration 21 : 0.007119958288059383
Loss in iteration 22 : 0.006807124542464479
Loss in iteration 23 : 0.00655390895232807
Loss in iteration 24 : 0.006315641847042884
Loss in iteration 25 : 0.006132966437053017
Loss in iteration 26 : 0.005964579960424494
Loss in iteration 27 : 0.005807302051460534
Loss in iteration 28 : 0.005657190591056691
Loss in iteration 29 : 0.005523276094644538
Loss in iteration 30 : 0.005409206349520885
Loss in iteration 31 : 0.0052885379262705475
Loss in iteration 32 : 0.005161810818253378
Loss in iteration 33 : 0.00503788893835344
Loss in iteration 34 : 0.0049113481725109225
Loss in iteration 35 : 0.004781902211414504
Loss in iteration 36 : 0.00464975694445211
Loss in iteration 37 : 0.004515099094389302
Testing accuracy  of updater 8 on alg 1 with rate 0.56 = 0.9635555555555556, training accuracy 0.9988568162332095, time elapsed: 1430 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3488670800125576
Loss in iteration 3 : 0.7530921076967785
Loss in iteration 4 : 0.23011829737381198
Loss in iteration 5 : 0.20930614737180045
Loss in iteration 6 : 0.14856961742291264
Loss in iteration 7 : 0.08309798187931784
Loss in iteration 8 : 0.057842834356747634
Loss in iteration 9 : 0.05468707940721372
Loss in iteration 10 : 0.050949277493165794
Loss in iteration 11 : 0.04439127047850441
Loss in iteration 12 : 0.03743978280524099
Loss in iteration 13 : 0.030992185049803764
Loss in iteration 14 : 0.025562530588413717
Loss in iteration 15 : 0.021113569964525566
Loss in iteration 16 : 0.01805229249185807
Loss in iteration 17 : 0.01583513627737684
Loss in iteration 18 : 0.014663875534004715
Loss in iteration 19 : 0.013613536224040788
Loss in iteration 20 : 0.012645119100156666
Loss in iteration 21 : 0.011661055612148322
Loss in iteration 22 : 0.010731431576894879
Loss in iteration 23 : 0.009864594446964994
Loss in iteration 24 : 0.009072230551477198
Loss in iteration 25 : 0.008306863402924568
Loss in iteration 26 : 0.0076579724751163905
Loss in iteration 27 : 0.0071453344181809575
Loss in iteration 28 : 0.00673895251819881
Loss in iteration 29 : 0.006353872261789427
Loss in iteration 30 : 0.005996821468619706
Loss in iteration 31 : 0.0056509638935851375
Loss in iteration 32 : 0.005344620302616056
Loss in iteration 33 : 0.005089121169304054
Loss in iteration 34 : 0.0048596237346918885
Loss in iteration 35 : 0.004652647470275863
Loss in iteration 36 : 0.004466354806501236
Loss in iteration 37 : 0.004281368141046071
Loss in iteration 38 : 0.004103560703918059
Loss in iteration 39 : 0.003928831910281797
Loss in iteration 40 : 0.0037519388495185994
Loss in iteration 41 : 0.003578027855690287
Loss in iteration 42 : 0.003423867326951817
Loss in iteration 43 : 0.003309994218328955
Loss in iteration 44 : 0.0032149241035664784
Loss in iteration 45 : 0.0031471217192379428
Loss in iteration 46 : 0.00308453335643147
Loss in iteration 47 : 0.0030228678103593194
Loss in iteration 48 : 0.0029717320970078643
Loss in iteration 49 : 0.0029238625036416576
Loss in iteration 50 : 0.0028760632398616886
Loss in iteration 51 : 0.002827916393769962
Loss in iteration 52 : 0.002779470840577368
Loss in iteration 53 : 0.0027308989612373417
Loss in iteration 54 : 0.002683227789328281
Loss in iteration 55 : 0.002637765362771889
Loss in iteration 56 : 0.0025920562155671506
Loss in iteration 57 : 0.0025479948780671867
Loss in iteration 58 : 0.0025065772533148355
Loss in iteration 59 : 0.002467825923255794
Loss in iteration 60 : 0.0024310253029034283
Loss in iteration 61 : 0.002394043609633421
Loss in iteration 62 : 0.0023582151415740943
Loss in iteration 63 : 0.002324669492059334
Loss in iteration 64 : 0.002292993192776321
Loss in iteration 65 : 0.002260947751906554
Loss in iteration 66 : 0.0022294912426452
Loss in iteration 67 : 0.00220065112762906
Loss in iteration 68 : 0.002171702321929517
Loss in iteration 69 : 0.002142578766431435
Loss in iteration 70 : 0.002113288439528374
Loss in iteration 71 : 0.0020876753080935713
Loss in iteration 72 : 0.002062288871520548
Loss in iteration 73 : 0.002036423671817149
Loss in iteration 74 : 0.002009972770813671
Loss in iteration 75 : 0.0019829839368961564
Loss in iteration 76 : 0.0019555003268993485
Loss in iteration 77 : 0.0019279387467183236
Loss in iteration 78 : 0.0019013075341995638
Loss in iteration 79 : 0.001874495490369897
Loss in iteration 80 : 0.0018475785785941404
Loss in iteration 81 : 0.0018205755393814452
Loss in iteration 82 : 0.0017937194414145957
Loss in iteration 83 : 0.0017665899993671538
Loss in iteration 84 : 0.0017393342906857825
Loss in iteration 85 : 0.0017119694724585352
Loss in iteration 86 : 0.0016848073673099897
Loss in iteration 87 : 0.0016573463276233463
Loss in iteration 88 : 0.0016303972592194192
Loss in iteration 89 : 0.0016073230250900528
Loss in iteration 90 : 0.0015841756231745855
Loss in iteration 91 : 0.001560870389216699
Loss in iteration 92 : 0.001537416157259569
Loss in iteration 93 : 0.0015138209142606066
Loss in iteration 94 : 0.0014900918822064989
Loss in iteration 95 : 0.001466235592272282
Loss in iteration 96 : 0.0014427712032674556
Loss in iteration 97 : 0.00141845100995781
Loss in iteration 98 : 0.001394509223314373
Loss in iteration 99 : 0.0013715789395059843
Loss in iteration 100 : 0.0013467088572296404
Loss in iteration 101 : 0.0013227873135664414
Loss in iteration 102 : 0.001298306270749848
Loss in iteration 103 : 0.0012752638663476382
Loss in iteration 104 : 0.0012516967107782373
Loss in iteration 105 : 0.0012277601512359715
Loss in iteration 106 : 0.00120417063747954
Loss in iteration 107 : 0.001179953890225144
Loss in iteration 108 : 0.0011558060405630365
Loss in iteration 109 : 0.001131331263700577
Loss in iteration 110 : 0.0011069830762644524
Loss in iteration 111 : 0.0010820911453533586
Loss in iteration 112 : 0.0010588240938283538
Testing accuracy  of updater 8 on alg 1 with rate 0.14 = 0.9822222222222222, training accuracy 0.999285510145756, time elapsed: 4628 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.32298908929735903
Loss in iteration 3 : 0.3648170580794175
Loss in iteration 4 : 0.24292749596501303
Loss in iteration 5 : 0.20980975421622985
Loss in iteration 6 : 0.14594983682499676
Loss in iteration 7 : 0.08170674229581956
Loss in iteration 8 : 0.0579081967599805
Loss in iteration 9 : 0.05521119873595113
Loss in iteration 10 : 0.05399886540363517
Loss in iteration 11 : 0.049152913079800055
Loss in iteration 12 : 0.0416482234597875
Loss in iteration 13 : 0.03429518593883857
Loss in iteration 14 : 0.028045048633915933
Loss in iteration 15 : 0.02321050054543498
Loss in iteration 16 : 0.01985045610168011
Loss in iteration 17 : 0.017622231244297803
Loss in iteration 18 : 0.01611836325175597
Loss in iteration 19 : 0.014930780834294895
Loss in iteration 20 : 0.014072837028489573
Loss in iteration 21 : 0.013200997384222285
Loss in iteration 22 : 0.012281557066952538
Loss in iteration 23 : 0.011320521338773851
Loss in iteration 24 : 0.010310055388665355
Loss in iteration 25 : 0.00942219905738803
Loss in iteration 26 : 0.008563784378436239
Loss in iteration 27 : 0.00777153158774341
Loss in iteration 28 : 0.007120689075498962
Loss in iteration 29 : 0.006627026676486489
Loss in iteration 30 : 0.006258389682778871
Loss in iteration 31 : 0.005898318002513178
Loss in iteration 32 : 0.005568572422098998
Loss in iteration 33 : 0.005277630077505178
Loss in iteration 34 : 0.005001716709000475
Loss in iteration 35 : 0.004741285350163717
Loss in iteration 36 : 0.004493843674724192
Loss in iteration 37 : 0.004272895897705405
Loss in iteration 38 : 0.004082279289874922
Loss in iteration 39 : 0.003909830357315194
Loss in iteration 40 : 0.003742674165702698
Loss in iteration 41 : 0.003583757663955766
Loss in iteration 42 : 0.0034575441533529704
Loss in iteration 43 : 0.003348743527947808
Loss in iteration 44 : 0.003257601521542056
Loss in iteration 45 : 0.0031817869142941315
Loss in iteration 46 : 0.0031070719132712363
Loss in iteration 47 : 0.00304462946691565
Loss in iteration 48 : 0.0029911623053966332
Loss in iteration 49 : 0.0029480060965225625
Loss in iteration 50 : 0.002909640734416673
Loss in iteration 51 : 0.0028712824010306173
Loss in iteration 52 : 0.002832922626438113
Loss in iteration 53 : 0.0027945537792682788
Loss in iteration 54 : 0.002756168984533994
Loss in iteration 55 : 0.0027180477075086262
Loss in iteration 56 : 0.0026800151911335543
Loss in iteration 57 : 0.002641654521651832
Loss in iteration 58 : 0.00260313342370099
Loss in iteration 59 : 0.0025648003025718377
Loss in iteration 60 : 0.002526850248205097
Loss in iteration 61 : 0.0024904592680572543
Loss in iteration 62 : 0.002453925158517149
Loss in iteration 63 : 0.002419615936910439
Loss in iteration 64 : 0.0023857345226680642
Loss in iteration 65 : 0.002352837426930028
Loss in iteration 66 : 0.0023252553838217457
Loss in iteration 67 : 0.0022979554186626657
Loss in iteration 68 : 0.0022715451090407905
Loss in iteration 69 : 0.0022457870469860183
Loss in iteration 70 : 0.002220000107210804
Loss in iteration 71 : 0.0021941821226479064
Loss in iteration 72 : 0.002168331143696458
Loss in iteration 73 : 0.0021424454168739193
Loss in iteration 74 : 0.002116523365559427
Loss in iteration 75 : 0.002092284159396685
Loss in iteration 76 : 0.0020693404921418752
Loss in iteration 77 : 0.0020471353259407447
Loss in iteration 78 : 0.0020247570045031603
Loss in iteration 79 : 0.002003900013003112
Loss in iteration 80 : 0.0019846945910185427
Loss in iteration 81 : 0.001965828975661269
Loss in iteration 82 : 0.0019465085772113657
Loss in iteration 83 : 0.0019267199726659841
Loss in iteration 84 : 0.0019065046859170897
Loss in iteration 85 : 0.0018859001693575293
Loss in iteration 86 : 0.0018649402032598701
Loss in iteration 87 : 0.0018436552559996993
Loss in iteration 88 : 0.0018234909992726161
Loss in iteration 89 : 0.0018032686444133005
Loss in iteration 90 : 0.0017832369130313804
Loss in iteration 91 : 0.001762535554334406
Loss in iteration 92 : 0.001741840849370284
Loss in iteration 93 : 0.0017217041259390907
Testing accuracy  of updater 8 on alg 1 with rate 0.098 = 0.9848888888888889, training accuracy 0.999285510145756, time elapsed: 3584 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5610315949802971
Loss in iteration 3 : 0.2763581182002906
Loss in iteration 4 : 0.18334734410849524
Loss in iteration 5 : 0.1531724230260779
Loss in iteration 6 : 0.12070892088770449
Loss in iteration 7 : 0.0956861632802614
Loss in iteration 8 : 0.08049888090257219
Loss in iteration 9 : 0.06867586514163578
Loss in iteration 10 : 0.05990471205885163
Loss in iteration 11 : 0.05297845739876055
Loss in iteration 12 : 0.047005026345977606
Loss in iteration 13 : 0.04176780170104788
Loss in iteration 14 : 0.03690228371492831
Loss in iteration 15 : 0.032397826801167574
Loss in iteration 16 : 0.028376976858235838
Loss in iteration 17 : 0.02495107890641916
Loss in iteration 18 : 0.02210781633790327
Loss in iteration 19 : 0.0196779498798429
Loss in iteration 20 : 0.01766808875112383
Loss in iteration 21 : 0.01601767559645638
Loss in iteration 22 : 0.01456624558678133
Loss in iteration 23 : 0.013412689470130853
Loss in iteration 24 : 0.012408386382442854
Loss in iteration 25 : 0.011492163881299677
Loss in iteration 26 : 0.010702056191855549
Loss in iteration 27 : 0.009994558838671088
Loss in iteration 28 : 0.009373725110225172
Loss in iteration 29 : 0.008784273608152166
Loss in iteration 30 : 0.008197418327340581
Loss in iteration 31 : 0.007638480349933867
Loss in iteration 32 : 0.007107404567807308
Loss in iteration 33 : 0.006613554632346067
Loss in iteration 34 : 0.006181626658044522
Loss in iteration 35 : 0.005778805017285316
Loss in iteration 36 : 0.0054206365960111245
Loss in iteration 37 : 0.005134449768353577
Loss in iteration 38 : 0.004864256849451116
Loss in iteration 39 : 0.0046157785242885345
Loss in iteration 40 : 0.004407956936552761
Loss in iteration 41 : 0.004232239508905539
Loss in iteration 42 : 0.004062907995256721
Loss in iteration 43 : 0.0038993256922564
Loss in iteration 44 : 0.0037518356626721244
Loss in iteration 45 : 0.0036120794068951775
Loss in iteration 46 : 0.00347428312493142
Loss in iteration 47 : 0.00334771756014543
Loss in iteration 48 : 0.003235890739910778
Loss in iteration 49 : 0.0031356911036898587
Loss in iteration 50 : 0.003037097016468864
Loss in iteration 51 : 0.002943397913519335
Loss in iteration 52 : 0.002861842817179743
Loss in iteration 53 : 0.0027901355664522566
Loss in iteration 54 : 0.002726267261000498
Loss in iteration 55 : 0.002668874437460836
Loss in iteration 56 : 0.0026127794691483477
Loss in iteration 57 : 0.0025562046503381976
Loss in iteration 58 : 0.00250198177444431
Loss in iteration 59 : 0.002451442504793437
Loss in iteration 60 : 0.0024098731374196792
Loss in iteration 61 : 0.0023692255653719476
Loss in iteration 62 : 0.002328603693312421
Loss in iteration 63 : 0.0022896889578368318
Loss in iteration 64 : 0.0022548140345100224
Loss in iteration 65 : 0.002222596923982374
Loss in iteration 66 : 0.0021916789287702615
Loss in iteration 67 : 0.002161258577740337
Loss in iteration 68 : 0.002134959862115816
Loss in iteration 69 : 0.002109264260682777
Loss in iteration 70 : 0.0020866853263296916
Loss in iteration 71 : 0.002065964857316983
Loss in iteration 72 : 0.0020454829500182485
Loss in iteration 73 : 0.002025896862151002
Loss in iteration 74 : 0.0020074529806601237
Loss in iteration 75 : 0.001988921009295624
Loss in iteration 76 : 0.001970306730953308
Loss in iteration 77 : 0.0019516153597958644
Loss in iteration 78 : 0.001932851597328529
Loss in iteration 79 : 0.001914019682946752
Loss in iteration 80 : 0.0018951234395006426
Loss in iteration 81 : 0.0018761663143672936
Loss in iteration 82 : 0.0018573830905334885
Loss in iteration 83 : 0.0018394512517663972
Loss in iteration 84 : 0.001821989732225596
Loss in iteration 85 : 0.0018044952689268015
Loss in iteration 86 : 0.0017868534704450002
Loss in iteration 87 : 0.0017702961280798603
Loss in iteration 88 : 0.0017543340943776626
Loss in iteration 89 : 0.001738314353765807
Loss in iteration 90 : 0.001722240186767428
Loss in iteration 91 : 0.0017064892080254975
Loss in iteration 92 : 0.0016910029907368786
Loss in iteration 93 : 0.0016754371652450562
Loss in iteration 94 : 0.0016597972023766508
Loss in iteration 95 : 0.0016441907736316562
Loss in iteration 96 : 0.0016295342530767889
Loss in iteration 97 : 0.001616882231953716
Loss in iteration 98 : 0.0016038480735797254
Loss in iteration 99 : 0.0015905732448009279
Loss in iteration 100 : 0.0015770792474271507
Loss in iteration 101 : 0.0015634172814201265
Loss in iteration 102 : 0.0015501491762898382
Loss in iteration 103 : 0.0015368891606563433
Loss in iteration 104 : 0.0015238773872038342
Loss in iteration 105 : 0.0015106226453316441
Loss in iteration 106 : 0.0014973198162979645
Loss in iteration 107 : 0.0014840444091282108
Loss in iteration 108 : 0.0014707782214428444
Loss in iteration 109 : 0.0014576959154330253
Loss in iteration 110 : 0.0014442109490769002
Loss in iteration 111 : 0.0014311300047102353
Loss in iteration 112 : 0.0014180227209123245
Loss in iteration 113 : 0.001404941371607535
Loss in iteration 114 : 0.0013916274922511095
Loss in iteration 115 : 0.0013781445614998633
Loss in iteration 116 : 0.0013648915232955976
Loss in iteration 117 : 0.0013517053236654852
Loss in iteration 118 : 0.0013384594668347989
Loss in iteration 119 : 0.0013250560546769125
Loss in iteration 120 : 0.0013119045616722046
Loss in iteration 121 : 0.0012995350939396801
Loss in iteration 122 : 0.0012874517042051642
Loss in iteration 123 : 0.0012752188731937932
Loss in iteration 124 : 0.001262924564508502
Loss in iteration 125 : 0.0012507654849388695
Loss in iteration 126 : 0.001239097056524837
Loss in iteration 127 : 0.0012269334644999339
Loss in iteration 128 : 0.0012150108099432417
Loss in iteration 129 : 0.0012027145910799531
Testing accuracy  of updater 8 on alg 1 with rate 0.05600000000000001 = 0.9893333333333333, training accuracy 0.999285510145756, time elapsed: 4753 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8818280795171701
Loss in iteration 3 : 0.7212296086177382
Loss in iteration 4 : 0.5261358609685904
Loss in iteration 5 : 0.3543114998634129
Loss in iteration 6 : 0.24405893898977638
Loss in iteration 7 : 0.207186346224682
Loss in iteration 8 : 0.1916116661905176
Loss in iteration 9 : 0.17946871731833083
Loss in iteration 10 : 0.1670474508672443
Loss in iteration 11 : 0.15406269474189135
Loss in iteration 12 : 0.14077479935759685
Loss in iteration 13 : 0.1275881693066262
Loss in iteration 14 : 0.11525635280717572
Loss in iteration 15 : 0.10467144407700363
Loss in iteration 16 : 0.09555195300837464
Loss in iteration 17 : 0.08808699590965174
Loss in iteration 18 : 0.08138182330134427
Loss in iteration 19 : 0.07522289638677905
Loss in iteration 20 : 0.06993184879922384
Loss in iteration 21 : 0.06515460243259553
Loss in iteration 22 : 0.06110455672954677
Loss in iteration 23 : 0.057503157358595014
Loss in iteration 24 : 0.054147321070976404
Loss in iteration 25 : 0.05102426999724084
Loss in iteration 26 : 0.04807701847046696
Loss in iteration 27 : 0.045501122185062054
Loss in iteration 28 : 0.043095550142857826
Loss in iteration 29 : 0.040903820153791744
Loss in iteration 30 : 0.0387956723322106
Loss in iteration 31 : 0.03681713148672714
Loss in iteration 32 : 0.0349777501824368
Loss in iteration 33 : 0.03323233050969249
Loss in iteration 34 : 0.03156725391996581
Loss in iteration 35 : 0.02999750757683736
Loss in iteration 36 : 0.028551395240471095
Loss in iteration 37 : 0.027222532672615304
Loss in iteration 38 : 0.0259484728572533
Loss in iteration 39 : 0.024767236240537286
Loss in iteration 40 : 0.02365200334126858
Loss in iteration 41 : 0.02261227177040729
Loss in iteration 42 : 0.02169553285583827
Loss in iteration 43 : 0.020871163848107728
Loss in iteration 44 : 0.02009114676204735
Loss in iteration 45 : 0.019374469210591987
Loss in iteration 46 : 0.018714748334083466
Loss in iteration 47 : 0.018116565813779913
Loss in iteration 48 : 0.017550292079945178
Loss in iteration 49 : 0.017021346036883146
Loss in iteration 50 : 0.016519016810163666
Loss in iteration 51 : 0.016070400064121984
Loss in iteration 52 : 0.015651844492978805
Loss in iteration 53 : 0.015249594136568788
Loss in iteration 54 : 0.014865079036736692
Loss in iteration 55 : 0.014497840075329758
Loss in iteration 56 : 0.014153476455601699
Loss in iteration 57 : 0.013827051391718708
Loss in iteration 58 : 0.013509200553024682
Loss in iteration 59 : 0.013200465880802545
Loss in iteration 60 : 0.012904127491012374
Loss in iteration 61 : 0.01262426087141013
Loss in iteration 62 : 0.012348442951774369
Loss in iteration 63 : 0.012077661000311596
Loss in iteration 64 : 0.011813484276577404
Loss in iteration 65 : 0.011559361871764031
Loss in iteration 66 : 0.011311995839707087
Loss in iteration 67 : 0.011073745829984624
Loss in iteration 68 : 0.010847434334852376
Loss in iteration 69 : 0.01063011193230361
Loss in iteration 70 : 0.010421160148006913
Loss in iteration 71 : 0.01021620980111947
Loss in iteration 72 : 0.010013257191190986
Loss in iteration 73 : 0.009812065026954444
Loss in iteration 74 : 0.009612442309554972
Loss in iteration 75 : 0.009414878968504736
Loss in iteration 76 : 0.009222001032697017
Loss in iteration 77 : 0.009032388457653537
Loss in iteration 78 : 0.008849034967164743
Loss in iteration 79 : 0.008668249567949891
Loss in iteration 80 : 0.008490076314617505
Loss in iteration 81 : 0.008314120934700294
Loss in iteration 82 : 0.008140775401482761
Loss in iteration 83 : 0.007969894662521885
Loss in iteration 84 : 0.007801672105387597
Loss in iteration 85 : 0.007637403676746464
Loss in iteration 86 : 0.007476496309139235
Loss in iteration 87 : 0.007317337621916957
Loss in iteration 88 : 0.007160371437791294
Loss in iteration 89 : 0.007005890367634043
Loss in iteration 90 : 0.006855716273905115
Loss in iteration 91 : 0.006709644967384249
Loss in iteration 92 : 0.006567282993649668
Loss in iteration 93 : 0.006426305163915139
Loss in iteration 94 : 0.006289331723935562
Loss in iteration 95 : 0.006157213156762427
Loss in iteration 96 : 0.0060293868363938165
Loss in iteration 97 : 0.005906513553648608
Loss in iteration 98 : 0.0057917346499422135
Loss in iteration 99 : 0.005681422959076113
Loss in iteration 100 : 0.005574810880177624
Loss in iteration 101 : 0.005472731749586845
Loss in iteration 102 : 0.005374498761321917
Loss in iteration 103 : 0.005277977028653919
Loss in iteration 104 : 0.005183053558050433
Loss in iteration 105 : 0.005091900619308385
Loss in iteration 106 : 0.005002349960005036
Loss in iteration 107 : 0.004915991934800845
Loss in iteration 108 : 0.004831049318766643
Loss in iteration 109 : 0.004746652809737216
Loss in iteration 110 : 0.004662716575981134
Loss in iteration 111 : 0.004580450472703129
Loss in iteration 112 : 0.0044988122043906065
Loss in iteration 113 : 0.004418345210158755
Loss in iteration 114 : 0.004339522369421962
Loss in iteration 115 : 0.004263721302588686
Loss in iteration 116 : 0.004194689355530222
Loss in iteration 117 : 0.004132469919870323
Loss in iteration 118 : 0.0040793372505093075
Loss in iteration 119 : 0.004029731648862853
Loss in iteration 120 : 0.003982303491548353
Loss in iteration 121 : 0.00393574485775996
Loss in iteration 122 : 0.003890013944383761
Loss in iteration 123 : 0.0038455244226968084
Loss in iteration 124 : 0.0038016645344926487
Loss in iteration 125 : 0.003758138445941919
Loss in iteration 126 : 0.0037153737327279652
Loss in iteration 127 : 0.0036732606582880988
Loss in iteration 128 : 0.003631993408792113
Loss in iteration 129 : 0.003590968502524031
Loss in iteration 130 : 0.003550597100822932
Loss in iteration 131 : 0.0035104113207629997
Loss in iteration 132 : 0.003470388692015551
Loss in iteration 133 : 0.0034305812666810837
Loss in iteration 134 : 0.003391636789765543
Loss in iteration 135 : 0.003354077716890173
Loss in iteration 136 : 0.0033172855940445107
Loss in iteration 137 : 0.0032807369527526123
Loss in iteration 138 : 0.0032445681447170684
Loss in iteration 139 : 0.0032088445005645123
Loss in iteration 140 : 0.0031730872084572656
Loss in iteration 141 : 0.0031376633903539685
Loss in iteration 142 : 0.0031022111697124326
Loss in iteration 143 : 0.0030666759415170467
Loss in iteration 144 : 0.0030311633981171112
Loss in iteration 145 : 0.0029956222825536115
Loss in iteration 146 : 0.002960300586586823
Loss in iteration 147 : 0.0029248778388566207
Loss in iteration 148 : 0.002889585628114768
Loss in iteration 149 : 0.0028543742691354026
Loss in iteration 150 : 0.0028206545328587364
Loss in iteration 151 : 0.002789312124125867
Loss in iteration 152 : 0.002760406654231056
Loss in iteration 153 : 0.002734446756645947
Loss in iteration 154 : 0.002709400084722782
Loss in iteration 155 : 0.002685655759412881
Loss in iteration 156 : 0.002661940567793302
Loss in iteration 157 : 0.002638139584924658
Loss in iteration 158 : 0.002614630885214863
Loss in iteration 159 : 0.0025913526350632636
Loss in iteration 160 : 0.0025682103455880504
Loss in iteration 161 : 0.002545184230698766
Loss in iteration 162 : 0.0025224404975502748
Loss in iteration 163 : 0.0025003912617350514
Loss in iteration 164 : 0.0024782913592652635
Testing accuracy  of updater 8 on alg 1 with rate 0.013999999999999985 = 0.9857777777777778, training accuracy 0.9994284081166047, time elapsed: 5949 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.250116421648715
Loss in iteration 3 : 0.9094185621931243
Loss in iteration 4 : 0.9040198703541753
Loss in iteration 5 : 1.6971999619183062
Loss in iteration 6 : 1.026610265160977
Loss in iteration 7 : 0.6466483099835654
Loss in iteration 8 : 0.4232049633234976
Loss in iteration 9 : 0.3855270799620333
Loss in iteration 10 : 0.4791845252337125
Loss in iteration 11 : 0.534979406190993
Testing accuracy  of updater 9 on alg 1 with rate 0.7999999999999999 = 0.9973333333333333, training accuracy 0.9668476707630752, time elapsed: 642 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.1332360286202008
Loss in iteration 3 : 0.4852473834776998
Loss in iteration 4 : 0.10176594846135804
Loss in iteration 5 : 0.24258492312577537
Loss in iteration 6 : 0.3524477093449604
Loss in iteration 7 : 0.35043963862011307
Loss in iteration 8 : 0.26524324965294166
Loss in iteration 9 : 0.1826876907427083
Loss in iteration 10 : 0.11747677077571833
Loss in iteration 11 : 0.08167498920834267
Loss in iteration 12 : 0.06784618582612127
Loss in iteration 13 : 0.07248523104100132
Loss in iteration 14 : 0.08487105062489685
Loss in iteration 15 : 0.09835602240412127
Testing accuracy  of updater 9 on alg 1 with rate 0.5599999999999999 = 0.9768888888888889, training accuracy 0.9902829379822806, time elapsed: 623 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5974932705131573
Loss in iteration 3 : 0.2590699890752241
Loss in iteration 4 : 0.08794450556494617
Loss in iteration 5 : 0.18967303805155772
Loss in iteration 6 : 0.21511076390003367
Loss in iteration 7 : 0.15316692271708157
Loss in iteration 8 : 0.09257717115040084
Loss in iteration 9 : 0.05551040317462279
Loss in iteration 10 : 0.04043679136535526
Loss in iteration 11 : 0.045274176584624945
Loss in iteration 12 : 0.055310055990463666
Testing accuracy  of updater 9 on alg 1 with rate 0.32 = 0.9822222222222222, training accuracy 0.9885681623320949, time elapsed: 545 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.32902723670584444
Loss in iteration 3 : 0.1185058246138428
Loss in iteration 4 : 0.1388874011664158
Loss in iteration 5 : 0.06463822037007944
Loss in iteration 6 : 0.05848860692716811
Loss in iteration 7 : 0.05753323685059595
Loss in iteration 8 : 0.051136468426676907
Loss in iteration 9 : 0.04156499349087505
Loss in iteration 10 : 0.03367504607356892
Loss in iteration 11 : 0.028411845403956273
Loss in iteration 12 : 0.024463557499208505
Loss in iteration 13 : 0.021525038194640977
Loss in iteration 14 : 0.01905359972183558
Loss in iteration 15 : 0.01692168271113754
Loss in iteration 16 : 0.01492974999159613
Loss in iteration 17 : 0.013151629101137456
Loss in iteration 18 : 0.011833889208430128
Loss in iteration 19 : 0.010851250970857672
Loss in iteration 20 : 0.00999831721743973
Loss in iteration 21 : 0.009272033918594538
Loss in iteration 22 : 0.008725635673628248
Loss in iteration 23 : 0.008435203716962839
Loss in iteration 24 : 0.00822004700071188
Loss in iteration 25 : 0.008016503361258363
Loss in iteration 26 : 0.007777734345508346
Loss in iteration 27 : 0.007513308078685957
Loss in iteration 28 : 0.0072642292917208745
Loss in iteration 29 : 0.007024384768430238
Loss in iteration 30 : 0.0067637679977893056
Loss in iteration 31 : 0.006491569183463736
Loss in iteration 32 : 0.006299593088563856
Loss in iteration 33 : 0.006188358589319067
Loss in iteration 34 : 0.006077300094455581
Loss in iteration 35 : 0.005955193706913169
Loss in iteration 36 : 0.00582408801086003
Loss in iteration 37 : 0.005720657723493383
Loss in iteration 38 : 0.005633332323822243
Loss in iteration 39 : 0.005556262251854188
Loss in iteration 40 : 0.005484741523484724
Loss in iteration 41 : 0.005410750871981608
Loss in iteration 42 : 0.005334531614569878
Loss in iteration 43 : 0.00525630104153787
Loss in iteration 44 : 0.005176254808296688
Loss in iteration 45 : 0.005102669624880944
Loss in iteration 46 : 0.005031832070539333
Loss in iteration 47 : 0.004960878756873169
Loss in iteration 48 : 0.0048898177407533175
Loss in iteration 49 : 0.004822805203739702
Loss in iteration 50 : 0.004758402847671554
Loss in iteration 51 : 0.004693759208017535
Loss in iteration 52 : 0.004632036089280527
Loss in iteration 53 : 0.004568108718716115
Loss in iteration 54 : 0.004503459567919948
Loss in iteration 55 : 0.004439146539631597
Loss in iteration 56 : 0.004374371987679761
Loss in iteration 57 : 0.0043091785364364745
Loss in iteration 58 : 0.004249018308211474
Loss in iteration 59 : 0.0041921336273070065
Loss in iteration 60 : 0.004137297317310003
Loss in iteration 61 : 0.004084819343020746
Loss in iteration 62 : 0.004032147953189548
Loss in iteration 63 : 0.003979299999683693
Loss in iteration 64 : 0.003926657707179252
Loss in iteration 65 : 0.0038748272108535863
Loss in iteration 66 : 0.0038223397871367166
Loss in iteration 67 : 0.0037692582247259077
Loss in iteration 68 : 0.0037206922341342014
Loss in iteration 69 : 0.003676123072488947
Loss in iteration 70 : 0.0036319163859454707
Loss in iteration 71 : 0.0035874968685619627
Loss in iteration 72 : 0.0035428836686665047
Loss in iteration 73 : 0.0034980940254298516
Loss in iteration 74 : 0.003453143459139574
Loss in iteration 75 : 0.003408310852994912
Testing accuracy  of updater 9 on alg 1 with rate 0.08 = 0.976, training accuracy 0.9991426121749071, time elapsed: 3190 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4655583919069932
Loss in iteration 3 : 0.27538556815583587
Loss in iteration 4 : 0.22402866060853704
Loss in iteration 5 : 0.1467993199820384
Loss in iteration 6 : 0.09994417658587378
Loss in iteration 7 : 0.12614691596535707
Loss in iteration 8 : 0.10931412742796229
Loss in iteration 9 : 0.0803003192527428
Loss in iteration 10 : 0.06398263698849087
Loss in iteration 11 : 0.05507371095518528
Loss in iteration 12 : 0.05097436049665735
Loss in iteration 13 : 0.04864088986419037
Loss in iteration 14 : 0.044672527876214105
Loss in iteration 15 : 0.03916183436874469
Loss in iteration 16 : 0.03269390026920472
Loss in iteration 17 : 0.026466869713058155
Loss in iteration 18 : 0.021848567683991654
Loss in iteration 19 : 0.018178338837879195
Loss in iteration 20 : 0.01585935679701521
Loss in iteration 21 : 0.014518836158514988
Loss in iteration 22 : 0.013571695004051098
Loss in iteration 23 : 0.013041990269972593
Loss in iteration 24 : 0.01250887974663459
Loss in iteration 25 : 0.011967914709273855
Loss in iteration 26 : 0.011321800663075235
Loss in iteration 27 : 0.01059857988014538
Loss in iteration 28 : 0.00984272488912475
Loss in iteration 29 : 0.009165595986332133
Loss in iteration 30 : 0.00859318923809473
Loss in iteration 31 : 0.008054636348155229
Loss in iteration 32 : 0.007569421959826993
Loss in iteration 33 : 0.007173577345192299
Loss in iteration 34 : 0.006840036843338727
Loss in iteration 35 : 0.006569769599597077
Loss in iteration 36 : 0.006344941243911361
Loss in iteration 37 : 0.006138039177346961
Loss in iteration 38 : 0.005927250740237258
Loss in iteration 39 : 0.005698255035772149
Loss in iteration 40 : 0.005445198140247342
Loss in iteration 41 : 0.0052091194570205905
Loss in iteration 42 : 0.004971701428842263
Loss in iteration 43 : 0.0047391650076249936
Loss in iteration 44 : 0.004543893005190282
Loss in iteration 45 : 0.004353936163020958
Loss in iteration 46 : 0.004180582110766982
Loss in iteration 47 : 0.0040456244220282555
Loss in iteration 48 : 0.003933249980392109
Loss in iteration 49 : 0.003839324600636896
Loss in iteration 50 : 0.0037727040674374324
Loss in iteration 51 : 0.0037228588022405382
Loss in iteration 52 : 0.0036732876202809094
Loss in iteration 53 : 0.003626309955072187
Loss in iteration 54 : 0.0035767132481404505
Loss in iteration 55 : 0.0035247033458050184
Loss in iteration 56 : 0.0034705169078711744
Loss in iteration 57 : 0.003414367032616006
Loss in iteration 58 : 0.003356445602376753
Loss in iteration 59 : 0.003296925395631988
Loss in iteration 60 : 0.003235961988821681
Loss in iteration 61 : 0.003190631485797446
Loss in iteration 62 : 0.0031496312721555233
Loss in iteration 63 : 0.003113692250501287
Loss in iteration 64 : 0.003078983260957695
Loss in iteration 65 : 0.0030429541296039778
Loss in iteration 66 : 0.0030057339301393683
Loss in iteration 67 : 0.0029674388858027716
Loss in iteration 68 : 0.0029300732345573463
Loss in iteration 69 : 0.0028957624589882745
Loss in iteration 70 : 0.002864844561394694
Loss in iteration 71 : 0.0028358686488483215
Loss in iteration 72 : 0.002806118882111794
Loss in iteration 73 : 0.002776176639008149
Loss in iteration 74 : 0.002748512051181407
Loss in iteration 75 : 0.0027210316324915574
Loss in iteration 76 : 0.0026934045278945436
Loss in iteration 77 : 0.002665644040773014
Loss in iteration 78 : 0.0026377621476261007
Loss in iteration 79 : 0.002611947704727761
Loss in iteration 80 : 0.0025883586993907344
Loss in iteration 81 : 0.0025643708741576623
Loss in iteration 82 : 0.002539845684507875
Loss in iteration 83 : 0.0025148351944241226
Loss in iteration 84 : 0.0024901187600071526
Loss in iteration 85 : 0.0024654967148983473
Loss in iteration 86 : 0.0024408154833198453
Loss in iteration 87 : 0.002416079733014269
Loss in iteration 88 : 0.002391293666628512
Loss in iteration 89 : 0.0023664610679997294
Loss in iteration 90 : 0.002341585343834934
Loss in iteration 91 : 0.0023166695612426495
Loss in iteration 92 : 0.0022917164815294236
Loss in iteration 93 : 0.0022667285906328976
Loss in iteration 94 : 0.002244023787088825
Loss in iteration 95 : 0.0022224537887477586
Loss in iteration 96 : 0.0022005249933801344
Loss in iteration 97 : 0.0021786077438114546
Loss in iteration 98 : 0.0021579535539693424
Loss in iteration 99 : 0.002137158345742169
Loss in iteration 100 : 0.002116235089927181
Loss in iteration 101 : 0.0020951954654782673
Loss in iteration 102 : 0.0020740499881195637
Loss in iteration 103 : 0.0020528081261545
Loss in iteration 104 : 0.0020314784047451113
Loss in iteration 105 : 0.0020102052878350116
Loss in iteration 106 : 0.001989165727677651
Loss in iteration 107 : 0.0019693278421257783
Loss in iteration 108 : 0.0019492440564798077
Loss in iteration 109 : 0.0019288302958720023
Loss in iteration 110 : 0.0019077146041413821
Testing accuracy  of updater 9 on alg 1 with rate 0.056 = 0.9831111111111112, training accuracy 0.9991426121749071, time elapsed: 4640 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7243514352863123
Loss in iteration 3 : 0.36592285365448235
Loss in iteration 4 : 0.1972119249089415
Loss in iteration 5 : 0.17161145483168658
Loss in iteration 6 : 0.14338388713633482
Loss in iteration 7 : 0.11729785524268532
Loss in iteration 8 : 0.09656220448487318
Loss in iteration 9 : 0.08270593431694027
Loss in iteration 10 : 0.07306753776224488
Loss in iteration 11 : 0.06587995839625133
Loss in iteration 12 : 0.05955374126240656
Loss in iteration 13 : 0.053929563918393535
Loss in iteration 14 : 0.048482070761733026
Loss in iteration 15 : 0.04321021132877054
Loss in iteration 16 : 0.03837391730102407
Loss in iteration 17 : 0.03407720252227756
Loss in iteration 18 : 0.03024238267825205
Loss in iteration 19 : 0.026831684653529624
Loss in iteration 20 : 0.0237454694189652
Loss in iteration 21 : 0.02122966539272664
Loss in iteration 22 : 0.0191221556007636
Loss in iteration 23 : 0.017319810621809206
Loss in iteration 24 : 0.015784171399820628
Loss in iteration 25 : 0.014531955989305526
Loss in iteration 26 : 0.013512643935873665
Loss in iteration 27 : 0.012702847453720627
Loss in iteration 28 : 0.011955210532408066
Loss in iteration 29 : 0.011264866261797912
Loss in iteration 30 : 0.010650571949874128
Loss in iteration 31 : 0.010071922598433244
Loss in iteration 32 : 0.00952155485377592
Loss in iteration 33 : 0.009003919521186548
Loss in iteration 34 : 0.00850059362610554
Loss in iteration 35 : 0.008025496904295205
Loss in iteration 36 : 0.007616508645117034
Loss in iteration 37 : 0.007252833896194139
Loss in iteration 38 : 0.0069003611691461535
Loss in iteration 39 : 0.006607671248967993
Loss in iteration 40 : 0.0063246697610003155
Loss in iteration 41 : 0.006058463593662844
Loss in iteration 42 : 0.005796379251747365
Loss in iteration 43 : 0.005558829599408966
Loss in iteration 44 : 0.005332744717355036
Loss in iteration 45 : 0.005113485180878301
Loss in iteration 46 : 0.0049092130402814994
Loss in iteration 47 : 0.004709267833779882
Loss in iteration 48 : 0.0045157522461029935
Loss in iteration 49 : 0.0043518902110955575
Loss in iteration 50 : 0.004214247590031467
Loss in iteration 51 : 0.004097181894002595
Loss in iteration 52 : 0.003982796244112458
Loss in iteration 53 : 0.0038748152248015253
Loss in iteration 54 : 0.003784436147588404
Loss in iteration 55 : 0.003695911181957364
Loss in iteration 56 : 0.0036131699206149565
Loss in iteration 57 : 0.003536673137368652
Loss in iteration 58 : 0.0034807665594418316
Loss in iteration 59 : 0.0034278317224952083
Loss in iteration 60 : 0.003380595173132491
Loss in iteration 61 : 0.0033344420008186264
Loss in iteration 62 : 0.003287247588776924
Loss in iteration 63 : 0.0032391128243457004
Loss in iteration 64 : 0.0031928824846156247
Loss in iteration 65 : 0.003148582877238635
Loss in iteration 66 : 0.0031037033667096797
Loss in iteration 67 : 0.0030617702705133076
Loss in iteration 68 : 0.003026413056480777
Loss in iteration 69 : 0.0029911339372319114
Loss in iteration 70 : 0.0029553430003315885
Loss in iteration 71 : 0.002919579177785786
Loss in iteration 72 : 0.0028860980227994647
Loss in iteration 73 : 0.002852998962664713
Loss in iteration 74 : 0.0028197507600058
Loss in iteration 75 : 0.0027877630852284874
Loss in iteration 76 : 0.0027576229584209714
Loss in iteration 77 : 0.0027280728915279402
Loss in iteration 78 : 0.0026987575552244554
Loss in iteration 79 : 0.002669450869870797
Loss in iteration 80 : 0.0026397437915053382
Loss in iteration 81 : 0.0026096745463660836
Loss in iteration 82 : 0.002579916783697651
Loss in iteration 83 : 0.0025502251613140417
Loss in iteration 84 : 0.0025205767877399202
Loss in iteration 85 : 0.0024924721219137485
Loss in iteration 86 : 0.002465876573098526
Loss in iteration 87 : 0.0024431327899984805
Loss in iteration 88 : 0.002419668898447401
Loss in iteration 89 : 0.00239554930389141
Loss in iteration 90 : 0.0023708378538811713
Loss in iteration 91 : 0.0023461686239615457
Loss in iteration 92 : 0.0023230979132576653
Loss in iteration 93 : 0.0023010880997855862
Loss in iteration 94 : 0.0022805908044061323
Loss in iteration 95 : 0.0022613488674332618
Loss in iteration 96 : 0.002242602301990584
Loss in iteration 97 : 0.0022254633377332715
Loss in iteration 98 : 0.002208245455534694
Loss in iteration 99 : 0.002190955676193846
Loss in iteration 100 : 0.0021736003206730046
Loss in iteration 101 : 0.0021561850798219482
Loss in iteration 102 : 0.0021387150771556293
Loss in iteration 103 : 0.0021211949253773787
Loss in iteration 104 : 0.0021036287772707657
Loss in iteration 105 : 0.002086020371521149
Loss in iteration 106 : 0.0020699584607326723
Loss in iteration 107 : 0.0020546361071492095
Loss in iteration 108 : 0.0020390508279577745
Loss in iteration 109 : 0.002023227916890688
Loss in iteration 110 : 0.002007287065982604
Loss in iteration 111 : 0.00199207648965365
Loss in iteration 112 : 0.001976804860909757
Loss in iteration 113 : 0.001962012983421109
Loss in iteration 114 : 0.0019472479247813381
Testing accuracy  of updater 9 on alg 1 with rate 0.032 = 0.9831111111111112, training accuracy 0.999285510145756, time elapsed: 4967 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9385720367062065
Loss in iteration 3 : 0.8274937366201985
Loss in iteration 4 : 0.6761324262782693
Loss in iteration 5 : 0.4946047020529215
Loss in iteration 6 : 0.3829433981636307
Loss in iteration 7 : 0.28211600000367615
Loss in iteration 8 : 0.22111768648086713
Loss in iteration 9 : 0.19714679486428738
Loss in iteration 10 : 0.18424233499408757
Loss in iteration 11 : 0.17413834856964178
Loss in iteration 12 : 0.1643654566763695
Loss in iteration 13 : 0.1537337630596193
Loss in iteration 14 : 0.1417250831559714
Loss in iteration 15 : 0.1293023458803146
Loss in iteration 16 : 0.1175311570096003
Loss in iteration 17 : 0.10690103762579617
Loss in iteration 18 : 0.09781211398034413
Loss in iteration 19 : 0.09042444640559046
Loss in iteration 20 : 0.08406140024424508
Loss in iteration 21 : 0.07859108170776062
Loss in iteration 22 : 0.07387789211650182
Loss in iteration 23 : 0.06965077414238255
Loss in iteration 24 : 0.06583277502448197
Loss in iteration 25 : 0.06244496974342743
Loss in iteration 26 : 0.05930654918882975
Loss in iteration 27 : 0.05636104379603121
Loss in iteration 28 : 0.053586181562853004
Loss in iteration 29 : 0.05103008079825249
Loss in iteration 30 : 0.04863806862210645
Loss in iteration 31 : 0.04635554001601811
Loss in iteration 32 : 0.044118548442120015
Loss in iteration 33 : 0.042012269289223476
Loss in iteration 34 : 0.04002113579430611
Loss in iteration 35 : 0.03810976991704183
Loss in iteration 36 : 0.036306593729397686
Loss in iteration 37 : 0.03462148963519839
Loss in iteration 38 : 0.033023509698367065
Loss in iteration 39 : 0.03148662854422097
Loss in iteration 40 : 0.030067493630329675
Loss in iteration 41 : 0.02877770264366513
Loss in iteration 42 : 0.027559134443054804
Loss in iteration 43 : 0.02645145171552377
Loss in iteration 44 : 0.02541688050103043
Loss in iteration 45 : 0.024437282384862147
Loss in iteration 46 : 0.023518412744394663
Loss in iteration 47 : 0.022632408998379785
Loss in iteration 48 : 0.021811069547890246
Loss in iteration 49 : 0.021079046007005123
Loss in iteration 50 : 0.020418917033806466
Loss in iteration 51 : 0.019786340453919634
Loss in iteration 52 : 0.019199091777872997
Loss in iteration 53 : 0.018651653528671756
Loss in iteration 54 : 0.018134153642325804
Loss in iteration 55 : 0.017652338557992107
Loss in iteration 56 : 0.017208552566467045
Loss in iteration 57 : 0.016796273180168998
Loss in iteration 58 : 0.016406148232167843
Loss in iteration 59 : 0.016034947840718794
Loss in iteration 60 : 0.01568490057668432
Loss in iteration 61 : 0.01534553942421791
Loss in iteration 62 : 0.015021120960963258
Loss in iteration 63 : 0.014710975717173237
Loss in iteration 64 : 0.014409774305808976
Loss in iteration 65 : 0.014113950947399936
Loss in iteration 66 : 0.013829754402991118
Loss in iteration 67 : 0.0135523956898674
Loss in iteration 68 : 0.013290562280784858
Loss in iteration 69 : 0.013043636445287327
Loss in iteration 70 : 0.012802883901267556
Loss in iteration 71 : 0.012570938116242847
Loss in iteration 72 : 0.012344589072263424
Loss in iteration 73 : 0.012121665843721315
Loss in iteration 74 : 0.011901108791345484
Loss in iteration 75 : 0.01168426977506577
Loss in iteration 76 : 0.011470421593030078
Loss in iteration 77 : 0.011258773646897641
Loss in iteration 78 : 0.011048849343379503
Loss in iteration 79 : 0.010842351651579851
Loss in iteration 80 : 0.010638222824617752
Loss in iteration 81 : 0.010437667809893527
Loss in iteration 82 : 0.010246444557391265
Loss in iteration 83 : 0.010060128551265807
Loss in iteration 84 : 0.009877049955902833
Loss in iteration 85 : 0.009697925333716146
Loss in iteration 86 : 0.00952083097820114
Loss in iteration 87 : 0.009347323404944093
Loss in iteration 88 : 0.009176724179212111
Loss in iteration 89 : 0.009008436312196198
Loss in iteration 90 : 0.00884344427431296
Loss in iteration 91 : 0.008679473249332223
Loss in iteration 92 : 0.008518572784213972
Loss in iteration 93 : 0.008359335209636089
Loss in iteration 94 : 0.008205403778056801
Loss in iteration 95 : 0.00805685919800809
Loss in iteration 96 : 0.0079092478100661
Loss in iteration 97 : 0.007763500243547534
Loss in iteration 98 : 0.0076207823754309385
Loss in iteration 99 : 0.0074788961188008655
Loss in iteration 100 : 0.007342266417227122
Loss in iteration 101 : 0.007211929778343131
Loss in iteration 102 : 0.007084491098531236
Loss in iteration 103 : 0.006965251579808632
Loss in iteration 104 : 0.006849035476407768
Loss in iteration 105 : 0.006742504198817301
Loss in iteration 106 : 0.006640088475178639
Loss in iteration 107 : 0.0065406198897768626
Loss in iteration 108 : 0.00644217533297033
Loss in iteration 109 : 0.00634440889194504
Loss in iteration 110 : 0.0062489396061813345
Loss in iteration 111 : 0.006155158036818274
Loss in iteration 112 : 0.006065809125163656
Loss in iteration 113 : 0.0059823645453166855
Loss in iteration 114 : 0.00590260999301986
Loss in iteration 115 : 0.00582386143076298
Loss in iteration 116 : 0.005745578308307955
Loss in iteration 117 : 0.0056687590494516014
Loss in iteration 118 : 0.0055930994807167666
Loss in iteration 119 : 0.005517563803843149
Loss in iteration 120 : 0.005442027298695661
Loss in iteration 121 : 0.005366486660196236
Loss in iteration 122 : 0.005290938912404185
Loss in iteration 123 : 0.005215705526571632
Loss in iteration 124 : 0.005140569963352295
Loss in iteration 125 : 0.005065610419016588
Loss in iteration 126 : 0.004992305526674932
Loss in iteration 127 : 0.0049210399036397024
Loss in iteration 128 : 0.004851927225178861
Loss in iteration 129 : 0.004783602601842177
Loss in iteration 130 : 0.00471776417885571
Loss in iteration 131 : 0.004658220771973516
Loss in iteration 132 : 0.00460280410098577
Loss in iteration 133 : 0.0045513825514921094
Loss in iteration 134 : 0.004501178386827663
Loss in iteration 135 : 0.004453527128758391
Loss in iteration 136 : 0.004406024186864539
Loss in iteration 137 : 0.004360113400585391
Loss in iteration 138 : 0.004315646220271166
Loss in iteration 139 : 0.004272375347110713
Loss in iteration 140 : 0.004231489798912692
Loss in iteration 141 : 0.004190936038636716
Loss in iteration 142 : 0.004150710033751334
Loss in iteration 143 : 0.004110692862669267
Loss in iteration 144 : 0.004071355017808875
Loss in iteration 145 : 0.004032534382751902
Loss in iteration 146 : 0.0039939007024042
Loss in iteration 147 : 0.003955433766373989
Loss in iteration 148 : 0.003917160824742351
Loss in iteration 149 : 0.003879407992651978
Loss in iteration 150 : 0.0038416636221534823
Loss in iteration 151 : 0.0038044021653992036
Loss in iteration 152 : 0.0037676159781881565
Loss in iteration 153 : 0.003731687866100745
Loss in iteration 154 : 0.0036959761978744756
Loss in iteration 155 : 0.003660255016203082
Loss in iteration 156 : 0.00362475882331876
Loss in iteration 157 : 0.0035897530913603475
Loss in iteration 158 : 0.003554898409361152
Loss in iteration 159 : 0.003520726971913226
Loss in iteration 160 : 0.0034873016465828996
Loss in iteration 161 : 0.0034545210816447278
Loss in iteration 162 : 0.0034225457833969666
Loss in iteration 163 : 0.0033898056452831647
Loss in iteration 164 : 0.0033563495970465444
Loss in iteration 165 : 0.003324857582714337
Loss in iteration 166 : 0.003293824779960697
Loss in iteration 167 : 0.00326271953115748
Loss in iteration 168 : 0.0032314897462515217
Loss in iteration 169 : 0.00319980749947296
Loss in iteration 170 : 0.003168602842115907
Loss in iteration 171 : 0.0031379441403712573
Loss in iteration 172 : 0.0031078137724661176
Loss in iteration 173 : 0.003077971903616153
Loss in iteration 174 : 0.003047713225986881
Loss in iteration 175 : 0.0030174187153944347
Loss in iteration 176 : 0.00298802120095414
Loss in iteration 177 : 0.0029608409170122883
Loss in iteration 178 : 0.0029346487799579326
Loss in iteration 179 : 0.0029091500184125554
Loss in iteration 180 : 0.0028843117531790422
Loss in iteration 181 : 0.0028607887312530186
Loss in iteration 182 : 0.0028380192890893573
Loss in iteration 183 : 0.002815389220814297
Testing accuracy  of updater 9 on alg 1 with rate 0.007999999999999993 = 0.9831111111111112, training accuracy 0.999285510145756, time elapsed: 8257 millisecond.
