objc[2920]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1038314c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10505c4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 21:31:58 INFO SparkContext: Running Spark version 2.0.0
18/02/26 21:31:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 21:31:59 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 21:31:59 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 21:31:59 INFO SecurityManager: Changing view acls groups to: 
18/02/26 21:31:59 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 21:31:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 21:31:59 INFO Utils: Successfully started service 'sparkDriver' on port 51622.
18/02/26 21:31:59 INFO SparkEnv: Registering MapOutputTracker
18/02/26 21:31:59 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 21:31:59 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-caf7b65c-4880-47cd-8603-cbfc03331204
18/02/26 21:31:59 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 21:32:00 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 21:32:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 21:32:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 21:32:00 INFO Executor: Starting executor ID driver on host localhost
18/02/26 21:32:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51623.
18/02/26 21:32:00 INFO NettyBlockTransferService: Server created on 192.168.2.140:51623
18/02/26 21:32:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 51623)
18/02/26 21:32:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:51623 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 51623)
18/02/26 21:32:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 51623)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 11.355315348062131
Loss in iteration 3 : 6.978040126136736
Loss in iteration 4 : 2.6170298043616325
Loss in iteration 5 : 1.0852194563557118
Loss in iteration 6 : 0.9871481532773367
Loss in iteration 7 : 0.9116758286279951
Loss in iteration 8 : 0.8721419436765999
Loss in iteration 9 : 0.8399436699680379
Loss in iteration 10 : 0.8082423793568553
Loss in iteration 11 : 0.7778870814027321
Loss in iteration 12 : 0.7494679143893128
Loss in iteration 13 : 0.7211065895614791
Loss in iteration 14 : 0.6940132845920329
Loss in iteration 15 : 0.6692306358489809
Loss in iteration 16 : 0.6518492165676092
Loss in iteration 17 : 0.6615224407762587
Loss in iteration 18 : 0.802186309116701
Loss in iteration 19 : 1.4212745175151618
Loss in iteration 20 : 3.428185867263587
Loss in iteration 21 : 0.772557796712292
Loss in iteration 22 : 0.9864707918034494
Loss in iteration 23 : 1.4181075304221051
Loss in iteration 24 : 2.9077107342708786
Loss in iteration 25 : 1.0263802500372485
Loss in iteration 26 : 1.412138219925712
Loss in iteration 27 : 1.526305568985732
Loss in iteration 28 : 2.351970285026589
Loss in iteration 29 : 1.3093246298467283
Loss in iteration 30 : 1.6054806513992492
Loss in iteration 31 : 1.2321818491294065
Loss in iteration 32 : 1.3704976837148013
Loss in iteration 33 : 1.1144993703632986
Loss in iteration 34 : 1.1860028039204271
Loss in iteration 35 : 1.0123020657727244
Loss in iteration 36 : 1.1338576931401785
Loss in iteration 37 : 1.0237856672987766
Loss in iteration 38 : 1.18542836227343
Loss in iteration 39 : 1.109269129192406
Loss in iteration 40 : 1.41084596638967
Loss in iteration 41 : 1.3163269614741058
Loss in iteration 42 : 1.792235026351039
Loss in iteration 43 : 1.3471288947663052
Loss in iteration 44 : 1.6416477437148984
Loss in iteration 45 : 1.2043408312914794
Loss in iteration 46 : 1.2995467198879203
Loss in iteration 47 : 1.0212757126984335
Loss in iteration 48 : 1.1121357846112132
Loss in iteration 49 : 0.9820498734065106
Loss in iteration 50 : 1.1068482470131549
Loss in iteration 51 : 0.9994149458546031
Loss in iteration 52 : 1.1854872086323367
Loss in iteration 53 : 1.1131968103664591
Loss in iteration 54 : 1.540337294155232
Loss in iteration 55 : 1.3512781727829077
Loss in iteration 56 : 1.8245811905380411
Loss in iteration 57 : 1.3285091564585165
Loss in iteration 58 : 1.5868676296764082
Loss in iteration 59 : 1.174888135901474
Loss in iteration 60 : 1.2725212831717243
Loss in iteration 61 : 1.0098226169469675
Loss in iteration 62 : 1.0840506676418693
Loss in iteration 63 : 0.9573876284742504
Loss in iteration 64 : 1.0716784042821372
Loss in iteration 65 : 0.9963111377424818
Loss in iteration 66 : 1.201183299172508
Loss in iteration 67 : 1.133815838461617
Loss in iteration 68 : 1.5822537763027462
Loss in iteration 69 : 1.365976499767202
Loss in iteration 70 : 1.835369817948691
Loss in iteration 71 : 1.2909168543194176
Loss in iteration 72 : 1.5074726674938586
Loss in iteration 73 : 1.1346653290909223
Loss in iteration 74 : 1.2446762862290042
Loss in iteration 75 : 0.9961411815988609
Loss in iteration 76 : 1.0805946179746646
Loss in iteration 77 : 0.9640664271553547
Loss in iteration 78 : 1.0953131982722148
Loss in iteration 79 : 1.0039397099073573
Loss in iteration 80 : 1.2369992463141515
Loss in iteration 81 : 1.1628608327368322
Loss in iteration 82 : 1.5887769656528103
Loss in iteration 83 : 1.3491060230522587
Loss in iteration 84 : 1.805637252498155
Loss in iteration 85 : 1.2750362194745395
Loss in iteration 86 : 1.4929763521963024
Loss in iteration 87 : 1.1251660748081225
Loss in iteration 88 : 1.2390448876524045
Loss in iteration 89 : 0.9871372677724972
Loss in iteration 90 : 1.0594056478679286
Loss in iteration 91 : 0.9539479438524286
Loss in iteration 92 : 1.0598714930412076
Loss in iteration 93 : 0.9974791705659646
Loss in iteration 94 : 1.2333520879342112
Loss in iteration 95 : 1.1708002541168565
Loss in iteration 96 : 1.603364224007815
Loss in iteration 97 : 1.3639486774231935
Loss in iteration 98 : 1.81138731902718
Loss in iteration 99 : 1.277116602509789
Loss in iteration 100 : 1.4874982568242228
Loss in iteration 101 : 1.1182040101525295
Loss in iteration 102 : 1.232642993786262
Loss in iteration 103 : 0.9828411201742194
Loss in iteration 104 : 1.0439471763828803
Loss in iteration 105 : 0.947167287902766
Loss in iteration 106 : 1.0532013848030177
Loss in iteration 107 : 0.9876876039010611
Loss in iteration 108 : 1.2284531332351525
Loss in iteration 109 : 1.1768119266512833
Loss in iteration 110 : 1.6299043304069194
Loss in iteration 111 : 1.3688140695243358
Loss in iteration 112 : 1.800098821976772
Loss in iteration 113 : 1.2783998824830078
Loss in iteration 114 : 1.4831147206258624
Loss in iteration 115 : 1.1076658471510554
Loss in iteration 116 : 1.2081891790365016
Loss in iteration 117 : 0.9715208407325995
Loss in iteration 118 : 1.0436810227343127
Loss in iteration 119 : 0.95391110148832
Loss in iteration 120 : 1.0682418917648868
Loss in iteration 121 : 1.0031579746629329
Loss in iteration 122 : 1.2556799020875282
Loss in iteration 123 : 1.1807419902975116
Loss in iteration 124 : 1.639509937867286
Loss in iteration 125 : 1.3562067856954203
Loss in iteration 126 : 1.7920513832888867
Loss in iteration 127 : 1.2727858883941017
Loss in iteration 128 : 1.4769813081380043
Loss in iteration 129 : 1.1010146585234222
Loss in iteration 130 : 1.2141807145098127
Loss in iteration 131 : 0.9690012026382787
Loss in iteration 132 : 1.0429415888356568
Loss in iteration 133 : 0.9550002494232844
Loss in iteration 134 : 1.0670992622635136
Loss in iteration 135 : 1.006548721185328
Loss in iteration 136 : 1.2582327749344246
Loss in iteration 137 : 1.177481953488549
Loss in iteration 138 : 1.623413296763625
Loss in iteration 139 : 1.33951558226308
Loss in iteration 140 : 1.763837683160561
Loss in iteration 141 : 1.277299439171708
Loss in iteration 142 : 1.4893303176646433
Loss in iteration 143 : 1.122869579755239
Loss in iteration 144 : 1.2571712948962395
Loss in iteration 145 : 0.9919980926110372
Loss in iteration 146 : 1.0520537250835154
Loss in iteration 147 : 0.9573736224977333
Loss in iteration 148 : 1.0540687820176469
Loss in iteration 149 : 0.9887594048243853
Loss in iteration 150 : 1.2061976812337913
Loss in iteration 151 : 1.122728369403736
Loss in iteration 152 : 1.5559690556003791
Loss in iteration 153 : 1.3581071883422582
Loss in iteration 154 : 1.813814393765954
Loss in iteration 155 : 1.289682388667855
Loss in iteration 156 : 1.521468640756216
Loss in iteration 157 : 1.1334976871229643
Loss in iteration 158 : 1.267413387482387
Loss in iteration 159 : 0.9931140409785312
Loss in iteration 160 : 1.0473531322709835
Loss in iteration 161 : 0.9526828793412522
Loss in iteration 162 : 1.0502741294179951
Loss in iteration 163 : 0.9878390006787118
Loss in iteration 164 : 1.1997888688925584
Loss in iteration 165 : 1.1221506051149635
Loss in iteration 166 : 1.5481816354063
Loss in iteration 167 : 1.3423893310307777
Loss in iteration 168 : 1.8045890240853266
Loss in iteration 169 : 1.294144216535222
Loss in iteration 170 : 1.5245354483455038
Loss in iteration 171 : 1.136704780989568
Loss in iteration 172 : 1.2728606374858922
Loss in iteration 173 : 1.0040630692495058
Loss in iteration 174 : 1.0586185751516564
Loss in iteration 175 : 0.9546762600335512
Loss in iteration 176 : 1.0490563131640582
Loss in iteration 177 : 0.9899869243229165
Loss in iteration 178 : 1.200355456534208
Loss in iteration 179 : 1.1215871990863486
Loss in iteration 180 : 1.5359725795912205
Loss in iteration 181 : 1.331664114621436
Loss in iteration 182 : 1.7840638734378538
Loss in iteration 183 : 1.2891320558382682
Loss in iteration 184 : 1.5243503127585771
Loss in iteration 185 : 1.1368926044046206
Loss in iteration 186 : 1.2739014150853445
Loss in iteration 187 : 1.0081513639039434
Loss in iteration 188 : 1.0698251961344867
Loss in iteration 189 : 0.957675554288906
Loss in iteration 190 : 1.051685166343748
Loss in iteration 191 : 0.9896586933809907
Loss in iteration 192 : 1.1968739599263598
Loss in iteration 193 : 1.1182106502738793
Loss in iteration 194 : 1.5335492503132824
Loss in iteration 195 : 1.3293581113214787
Loss in iteration 196 : 1.7798620995261774
Loss in iteration 197 : 1.2930912936407104
Loss in iteration 198 : 1.5332597255132907
Loss in iteration 199 : 1.1454939826369481
Loss in iteration 200 : 1.2767234315171914
Testing accuracy  of updater 0 on alg 1 with rate 0.004 = 0.7885, training accuracy 0.7937843962447394, time elapsed: 5960 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 8.054483126291602
Loss in iteration 3 : 4.990390470943832
Loss in iteration 4 : 1.9337903070392757
Loss in iteration 5 : 0.8505976305276779
Loss in iteration 6 : 0.7640492985462887
Loss in iteration 7 : 0.7155531013303975
Loss in iteration 8 : 0.6986432025004268
Loss in iteration 9 : 0.6707175021347181
Loss in iteration 10 : 0.6556984140588551
Loss in iteration 11 : 0.6349473598256421
Loss in iteration 12 : 0.6263681312065082
Loss in iteration 13 : 0.6261822466365137
Loss in iteration 14 : 0.6547144261031703
Loss in iteration 15 : 0.7142039795545186
Loss in iteration 16 : 0.9594306803799376
Loss in iteration 17 : 1.2928662662592607
Loss in iteration 18 : 2.1037111205199475
Loss in iteration 19 : 0.8012626571590369
Loss in iteration 20 : 0.906659224131505
Loss in iteration 21 : 0.887319497854663
Loss in iteration 22 : 1.1231271699222183
Loss in iteration 23 : 1.0855008510440922
Loss in iteration 24 : 1.3957014805547672
Loss in iteration 25 : 1.0620450045065009
Loss in iteration 26 : 1.2157505163140365
Loss in iteration 27 : 0.9364623076745716
Loss in iteration 28 : 0.9929376308934655
Loss in iteration 29 : 0.816673401012327
Loss in iteration 30 : 0.8475814265314622
Loss in iteration 31 : 0.7568914332285879
Loss in iteration 32 : 0.8020077734852581
Loss in iteration 33 : 0.7551149789520863
Loss in iteration 34 : 0.852836636423489
Loss in iteration 35 : 0.8397668433340362
Loss in iteration 36 : 1.0664523087498399
Loss in iteration 37 : 1.0642327940405605
Loss in iteration 38 : 1.427135518542454
Loss in iteration 39 : 1.0426446104288738
Loss in iteration 40 : 1.2234632753071866
Loss in iteration 41 : 0.9326673307033406
Loss in iteration 42 : 1.0123870407056086
Loss in iteration 43 : 0.8344089341116601
Loss in iteration 44 : 0.8645887423332164
Loss in iteration 45 : 0.7693004641174526
Loss in iteration 46 : 0.8342193893087433
Loss in iteration 47 : 0.7705611728520684
Loss in iteration 48 : 0.8630408091530004
Loss in iteration 49 : 0.8368921754654101
Loss in iteration 50 : 1.049336339607573
Loss in iteration 51 : 0.9916654658289237
Loss in iteration 52 : 1.2986611756501494
Loss in iteration 53 : 1.0308008917193259
Loss in iteration 54 : 1.2378149233340616
Loss in iteration 55 : 0.9262730979823756
Loss in iteration 56 : 1.0360693071314835
Loss in iteration 57 : 0.8419988317045072
Loss in iteration 58 : 0.8833962432506134
Loss in iteration 59 : 0.76518653886994
Loss in iteration 60 : 0.8235027755355833
Loss in iteration 61 : 0.766872640498904
Loss in iteration 62 : 0.8550563183454644
Loss in iteration 63 : 0.8209189476853135
Loss in iteration 64 : 1.0086064784554472
Loss in iteration 65 : 0.9790110093881653
Loss in iteration 66 : 1.30553346506225
Loss in iteration 67 : 1.028470755045753
Loss in iteration 68 : 1.2537255534386753
Loss in iteration 69 : 0.9319983564353203
Loss in iteration 70 : 1.0503738544703893
Loss in iteration 71 : 0.8450003726802415
Loss in iteration 72 : 0.8985295506639954
Loss in iteration 73 : 0.7719311265181569
Loss in iteration 74 : 0.8211823428183463
Loss in iteration 75 : 0.7636189388736616
Loss in iteration 76 : 0.8355426395208168
Loss in iteration 77 : 0.7900285492396425
Loss in iteration 78 : 0.9468634594902504
Loss in iteration 79 : 0.9348247893359419
Loss in iteration 80 : 1.226263471145588
Loss in iteration 81 : 1.0429198092406748
Loss in iteration 82 : 1.319254174930674
Loss in iteration 83 : 0.9703114966261466
Loss in iteration 84 : 1.0996160843245912
Loss in iteration 85 : 0.8601321052650707
Loss in iteration 86 : 0.9194047448765225
Loss in iteration 87 : 0.7767930712182198
Loss in iteration 88 : 0.8161883885193774
Loss in iteration 89 : 0.7494902022742886
Loss in iteration 90 : 0.8088123180609772
Loss in iteration 91 : 0.7783516040497054
Loss in iteration 92 : 0.9199500187331027
Loss in iteration 93 : 0.9023661536271674
Loss in iteration 94 : 1.1793914602384306
Loss in iteration 95 : 1.0111498458365362
Loss in iteration 96 : 1.3068436665468215
Loss in iteration 97 : 0.9899737636216612
Loss in iteration 98 : 1.1324214808425128
Loss in iteration 99 : 0.8920129439499921
Loss in iteration 100 : 0.9538355935694334
Loss in iteration 101 : 0.8008868190420351
Loss in iteration 102 : 0.8353796843076048
Loss in iteration 103 : 0.7565784692744788
Loss in iteration 104 : 0.8062637451524998
Loss in iteration 105 : 0.7634462965127639
Loss in iteration 106 : 0.8709407621326678
Loss in iteration 107 : 0.8409668355948906
Loss in iteration 108 : 1.0939719551022724
Loss in iteration 109 : 1.0100783807893365
Loss in iteration 110 : 1.3434970893183957
Loss in iteration 111 : 1.0009757868235565
Loss in iteration 112 : 1.1596132526541296
Loss in iteration 113 : 0.8924435280231521
Loss in iteration 114 : 0.9674096536168779
Loss in iteration 115 : 0.8054123603027336
Loss in iteration 116 : 0.8497933981155189
Loss in iteration 117 : 0.7567896669118007
Loss in iteration 118 : 0.8061147178362235
Loss in iteration 119 : 0.762644522752249
Loss in iteration 120 : 0.8677691334357186
Loss in iteration 121 : 0.8448410241023517
Loss in iteration 122 : 1.1025760340059219
Loss in iteration 123 : 1.0168774922695347
Loss in iteration 124 : 1.332841885610442
Loss in iteration 125 : 0.9986343160806458
Loss in iteration 126 : 1.158731914598592
Loss in iteration 127 : 0.8832388205092541
Loss in iteration 128 : 0.9514670459461877
Loss in iteration 129 : 0.801381063660404
Loss in iteration 130 : 0.845635488302643
Loss in iteration 131 : 0.7593881531926368
Loss in iteration 132 : 0.8154338808509706
Loss in iteration 133 : 0.776783886178816
Loss in iteration 134 : 0.9010461395716625
Loss in iteration 135 : 0.8688118250130669
Loss in iteration 136 : 1.1245554036727892
Loss in iteration 137 : 0.9982561871647418
Loss in iteration 138 : 1.3004749198226644
Loss in iteration 139 : 0.9901432073859058
Loss in iteration 140 : 1.149942042410748
Loss in iteration 141 : 0.8849144794898108
Loss in iteration 142 : 0.9510215442170826
Loss in iteration 143 : 0.7989723410597894
Loss in iteration 144 : 0.8391791930816712
Loss in iteration 145 : 0.7567872878568274
Loss in iteration 146 : 0.8107288746636461
Loss in iteration 147 : 0.7661751936141775
Loss in iteration 148 : 0.8825791562084813
Loss in iteration 149 : 0.8483594733268366
Loss in iteration 150 : 1.1079343177592196
Loss in iteration 151 : 1.003873018363876
Loss in iteration 152 : 1.3150923530107443
Loss in iteration 153 : 1.0046226225531256
Loss in iteration 154 : 1.1751164140870014
Loss in iteration 155 : 0.8867419065708082
Loss in iteration 156 : 0.9485876205993348
Loss in iteration 157 : 0.798810121477005
Loss in iteration 158 : 0.8397770597939775
Loss in iteration 159 : 0.7535008331394863
Loss in iteration 160 : 0.8068141437731438
Loss in iteration 161 : 0.7672998818393227
Loss in iteration 162 : 0.8850188352222261
Loss in iteration 163 : 0.8508072972257034
Loss in iteration 164 : 1.1050145949244123
Loss in iteration 165 : 0.9975489971049206
Loss in iteration 166 : 1.2989236944773024
Loss in iteration 167 : 0.9931232424415112
Loss in iteration 168 : 1.1691179523388426
Loss in iteration 169 : 0.8878714705647519
Loss in iteration 170 : 0.9556739869009918
Loss in iteration 171 : 0.7981841866067979
Loss in iteration 172 : 0.8422012582694341
Loss in iteration 173 : 0.7591431787988601
Loss in iteration 174 : 0.8215409902152175
Loss in iteration 175 : 0.772212317024312
Loss in iteration 176 : 0.8887071310216776
Loss in iteration 177 : 0.8577725794013579
Loss in iteration 178 : 1.1144171898062325
Loss in iteration 179 : 0.9984269067883694
Loss in iteration 180 : 1.2960755029346964
Loss in iteration 181 : 0.994128191062456
Loss in iteration 182 : 1.164182949887417
Loss in iteration 183 : 0.8778768097497336
Loss in iteration 184 : 0.9408048268417257
Loss in iteration 185 : 0.7826820455740634
Loss in iteration 186 : 0.8215671943418569
Loss in iteration 187 : 0.7586065774087465
Loss in iteration 188 : 0.8242423960748233
Loss in iteration 189 : 0.7703838898164905
Loss in iteration 190 : 0.8900143806913325
Loss in iteration 191 : 0.8630660090474589
Loss in iteration 192 : 1.1194369215903568
Loss in iteration 193 : 0.9955042446600784
Loss in iteration 194 : 1.295322707106779
Loss in iteration 195 : 0.9957725399774063
Loss in iteration 196 : 1.1637343829183369
Loss in iteration 197 : 0.8783617655657577
Loss in iteration 198 : 0.9449475077788027
Loss in iteration 199 : 0.7961741221289094
Loss in iteration 200 : 0.8437241443444007
Testing accuracy  of updater 0 on alg 1 with rate 0.0028000000000000004 = 0.7905, training accuracy 0.7996115247652962, time elapsed: 3451 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.753650904521067
Loss in iteration 3 : 3.002740815750913
Loss in iteration 4 : 1.253652369559031
Loss in iteration 5 : 0.6134458918911159
Loss in iteration 6 : 0.5797513686108936
Loss in iteration 7 : 0.5486551355205334
Loss in iteration 8 : 0.5275747864159605
Loss in iteration 9 : 0.5178544282240132
Loss in iteration 10 : 0.50674540872534
Loss in iteration 11 : 0.5039883163074371
Loss in iteration 12 : 0.5070552450336834
Loss in iteration 13 : 0.527558625957314
Loss in iteration 14 : 0.5808773399665444
Loss in iteration 15 : 0.66902850527566
Loss in iteration 16 : 0.8866668542714247
Loss in iteration 17 : 0.8382794752097673
Loss in iteration 18 : 1.0427126359898602
Loss in iteration 19 : 0.7075720889682425
Loss in iteration 20 : 0.7540651533600231
Loss in iteration 21 : 0.6655212483561797
Loss in iteration 22 : 0.6817889288762528
Loss in iteration 23 : 0.6219557925565905
Loss in iteration 24 : 0.6297634934858889
Loss in iteration 25 : 0.5891423923277412
Loss in iteration 26 : 0.5979315271577373
Loss in iteration 27 : 0.5800161604475744
Loss in iteration 28 : 0.60423301332318
Loss in iteration 29 : 0.6083742489503992
Loss in iteration 30 : 0.6723434524590504
Loss in iteration 31 : 0.6712772510874631
Loss in iteration 32 : 0.7812652615253924
Loss in iteration 33 : 0.7115038258663032
Loss in iteration 34 : 0.7948924998088482
Loss in iteration 35 : 0.6783615394913582
Loss in iteration 36 : 0.7191703386869418
Loss in iteration 37 : 0.6280239119043203
Loss in iteration 38 : 0.6472154466776702
Loss in iteration 39 : 0.5948495655149184
Loss in iteration 40 : 0.6000954837049678
Loss in iteration 41 : 0.5713188962105747
Loss in iteration 42 : 0.5809488402937237
Loss in iteration 43 : 0.5751799520304762
Loss in iteration 44 : 0.6093459488591814
Loss in iteration 45 : 0.6203526578356848
Loss in iteration 46 : 0.7267668818594727
Loss in iteration 47 : 0.7099312771720049
Loss in iteration 48 : 0.8390225598838231
Loss in iteration 49 : 0.7148660941069495
Loss in iteration 50 : 0.7821757323478689
Loss in iteration 51 : 0.6530403731983737
Loss in iteration 52 : 0.6721388134805335
Loss in iteration 53 : 0.5935214896543987
Loss in iteration 54 : 0.58609705232339
Loss in iteration 55 : 0.557894061530691
Loss in iteration 56 : 0.5649055868829306
Loss in iteration 57 : 0.556981284531115
Loss in iteration 58 : 0.5757848327063492
Loss in iteration 59 : 0.571522879015525
Loss in iteration 60 : 0.6235274008473859
Loss in iteration 61 : 0.6406521800000939
Loss in iteration 62 : 0.7545204777605629
Loss in iteration 63 : 0.7132061778166481
Loss in iteration 64 : 0.8482605069938419
Loss in iteration 65 : 0.7046167521495091
Loss in iteration 66 : 0.7603681293750315
Loss in iteration 67 : 0.6470124553133685
Loss in iteration 68 : 0.6775656869689289
Loss in iteration 69 : 0.5956904121403339
Loss in iteration 70 : 0.5962660370165347
Loss in iteration 71 : 0.5549584532015746
Loss in iteration 72 : 0.5626493195433288
Loss in iteration 73 : 0.5574688556020725
Loss in iteration 74 : 0.5809904735856014
Loss in iteration 75 : 0.5757472987514449
Loss in iteration 76 : 0.6298246919358298
Loss in iteration 77 : 0.6394887289097959
Loss in iteration 78 : 0.7390818738022243
Loss in iteration 79 : 0.703689674434359
Loss in iteration 80 : 0.8332218704942864
Loss in iteration 81 : 0.6944901756734105
Loss in iteration 82 : 0.7440176828451305
Loss in iteration 83 : 0.639071324143735
Loss in iteration 84 : 0.6683461921000099
Loss in iteration 85 : 0.5955736929572744
Loss in iteration 86 : 0.5988926306843722
Loss in iteration 87 : 0.5614966701667671
Loss in iteration 88 : 0.5664971723682761
Loss in iteration 89 : 0.5568377390135454
Loss in iteration 90 : 0.5830790571219219
Loss in iteration 91 : 0.5793081568962626
Loss in iteration 92 : 0.6353938696161355
Loss in iteration 93 : 0.642798145232951
Loss in iteration 94 : 0.744149301816267
Loss in iteration 95 : 0.6964305648153806
Loss in iteration 96 : 0.8174963946557213
Loss in iteration 97 : 0.677371036436953
Loss in iteration 98 : 0.7183671882860353
Loss in iteration 99 : 0.6312570660551227
Loss in iteration 100 : 0.6644950414705174
Loss in iteration 101 : 0.5963791608244523
Loss in iteration 102 : 0.6068460840512194
Loss in iteration 103 : 0.5664163382076459
Loss in iteration 104 : 0.5810515562646803
Loss in iteration 105 : 0.5631357643964756
Loss in iteration 106 : 0.59516091688463
Loss in iteration 107 : 0.579819082722134
Loss in iteration 108 : 0.6336719561680353
Loss in iteration 109 : 0.6371502055468928
Loss in iteration 110 : 0.7359817374746114
Loss in iteration 111 : 0.6940765516424963
Loss in iteration 112 : 0.8106021565321072
Loss in iteration 113 : 0.6750039179634316
Loss in iteration 114 : 0.7149740626811776
Loss in iteration 115 : 0.6202735471708896
Loss in iteration 116 : 0.6483052751284548
Loss in iteration 117 : 0.5887964709893467
Loss in iteration 118 : 0.5989585658008233
Loss in iteration 119 : 0.5653992728951356
Loss in iteration 120 : 0.5860699760852882
Loss in iteration 121 : 0.5722115703328433
Loss in iteration 122 : 0.6143449960788898
Loss in iteration 123 : 0.607892176244986
Loss in iteration 124 : 0.6952891663850147
Loss in iteration 125 : 0.6784459519644296
Loss in iteration 126 : 0.7774901849214674
Loss in iteration 127 : 0.6687158951013149
Loss in iteration 128 : 0.7165540214345845
Loss in iteration 129 : 0.6345226705429123
Loss in iteration 130 : 0.6819184235053183
Loss in iteration 131 : 0.6103685145829312
Loss in iteration 132 : 0.6267199804462722
Loss in iteration 133 : 0.5729984350649331
Loss in iteration 134 : 0.5939082982752537
Loss in iteration 135 : 0.5657903321006089
Loss in iteration 136 : 0.5924823265515212
Loss in iteration 137 : 0.5744685598791271
Loss in iteration 138 : 0.6319515364435833
Loss in iteration 139 : 0.6335864311843438
Loss in iteration 140 : 0.728945160072224
Loss in iteration 141 : 0.6910037367590627
Loss in iteration 142 : 0.7997099166134829
Loss in iteration 143 : 0.6628361141617946
Loss in iteration 144 : 0.7038720144334567
Loss in iteration 145 : 0.6147235624920794
Loss in iteration 146 : 0.6307274216882971
Loss in iteration 147 : 0.573497361099901
Loss in iteration 148 : 0.590021310451759
Loss in iteration 149 : 0.5650354941464907
Loss in iteration 150 : 0.5918779449370324
Loss in iteration 151 : 0.5713111157711188
Loss in iteration 152 : 0.6255427570498655
Loss in iteration 153 : 0.6213007648499512
Loss in iteration 154 : 0.7082705020826608
Loss in iteration 155 : 0.6884074638445833
Loss in iteration 156 : 0.7967739715760808
Loss in iteration 157 : 0.6679574521980943
Loss in iteration 158 : 0.7083301994415065
Loss in iteration 159 : 0.6224298854651384
Loss in iteration 160 : 0.6614527886264226
Loss in iteration 161 : 0.5983600046237556
Loss in iteration 162 : 0.6216958524624101
Loss in iteration 163 : 0.5737493243834603
Loss in iteration 164 : 0.5927532139572025
Loss in iteration 165 : 0.5661023469000679
Loss in iteration 166 : 0.596510722934844
Loss in iteration 167 : 0.5772837577812835
Loss in iteration 168 : 0.6333555196327668
Loss in iteration 169 : 0.6346319363259587
Loss in iteration 170 : 0.73062866623541
Loss in iteration 171 : 0.6926476652976145
Loss in iteration 172 : 0.7977828590452857
Loss in iteration 173 : 0.664690936441973
Loss in iteration 174 : 0.7066580263595967
Loss in iteration 175 : 0.616867842179984
Loss in iteration 176 : 0.6474397401923678
Loss in iteration 177 : 0.5905235302398566
Loss in iteration 178 : 0.6069530723479648
Loss in iteration 179 : 0.5653640311458636
Loss in iteration 180 : 0.5874120994124418
Loss in iteration 181 : 0.5681545587894921
Loss in iteration 182 : 0.6065809426313822
Loss in iteration 183 : 0.6003481014768396
Loss in iteration 184 : 0.6868981928955615
Loss in iteration 185 : 0.6607861280914544
Loss in iteration 186 : 0.755864360373006
Loss in iteration 187 : 0.6765870175362686
Loss in iteration 188 : 0.7478458730615443
Loss in iteration 189 : 0.6559300399562025
Loss in iteration 190 : 0.6911932006443796
Loss in iteration 191 : 0.608940244149094
Loss in iteration 192 : 0.6209758566555197
Loss in iteration 193 : 0.5665630372835331
Loss in iteration 194 : 0.5806601623505279
Loss in iteration 195 : 0.5622804112136558
Loss in iteration 196 : 0.5892893574694875
Loss in iteration 197 : 0.5713338067056924
Loss in iteration 198 : 0.6310579910387728
Loss in iteration 199 : 0.6290436470962156
Loss in iteration 200 : 0.7168160731235909
Testing accuracy  of updater 0 on alg 1 with rate 0.0016 = 0.78, training accuracy 0.7695046940757526, time elapsed: 2915 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.4528186827505332
Loss in iteration 3 : 1.0150911605579938
Loss in iteration 4 : 0.5815190272260541
Loss in iteration 5 : 0.43749722331685226
Loss in iteration 6 : 0.45802274662801545
Loss in iteration 7 : 0.4413393927432192
Loss in iteration 8 : 0.4549328263583529
Loss in iteration 9 : 0.43208776042093444
Loss in iteration 10 : 0.4329442096283661
Loss in iteration 11 : 0.41810978237453483
Loss in iteration 12 : 0.4125560500451749
Loss in iteration 13 : 0.40565235252501103
Loss in iteration 14 : 0.3996970294240689
Loss in iteration 15 : 0.39501966525096616
Loss in iteration 16 : 0.39170281573629506
Loss in iteration 17 : 0.3897550240316182
Loss in iteration 18 : 0.3862940509668942
Loss in iteration 19 : 0.3846800144201073
Loss in iteration 20 : 0.38408674849638
Testing accuracy  of updater 0 on alg 1 with rate 4.0E-4 = 0.7795, training accuracy 0.8336031078018776, time elapsed: 210 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.122735460573479
Loss in iteration 3 : 0.8163261950387023
Loss in iteration 4 : 0.522550521383128
Loss in iteration 5 : 0.4486281416706398
Loss in iteration 6 : 0.4343367116452023
Loss in iteration 7 : 0.4231954318256695
Loss in iteration 8 : 0.41565562450042215
Loss in iteration 9 : 0.4100548639020771
Loss in iteration 10 : 0.40526058481239746
Loss in iteration 11 : 0.4021891814902965
Loss in iteration 12 : 0.3995550815873623
Loss in iteration 13 : 0.39736270767923665
Loss in iteration 14 : 0.39533520587177967
Testing accuracy  of updater 0 on alg 1 with rate 2.8000000000000003E-4 = 0.77925, training accuracy 0.8332793784396245, time elapsed: 151 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.792653020718533
Loss in iteration 3 : 0.6190444738790621
Loss in iteration 4 : 0.5027113039806883
Loss in iteration 5 : 0.49448907449822627
Loss in iteration 6 : 0.4873657723747309
Loss in iteration 7 : 0.48036223326177624
Loss in iteration 8 : 0.4734098918487609
Loss in iteration 9 : 0.4665451059000256
Loss in iteration 10 : 0.45971873159083615
Loss in iteration 11 : 0.45291651548129624
Loss in iteration 12 : 0.446179200612459
Loss in iteration 13 : 0.4395554167883102
Testing accuracy  of updater 0 on alg 1 with rate 1.6E-4 = 0.766, training accuracy 0.830042084817093, time elapsed: 275 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5974296869061922
Loss in iteration 3 : 0.5549720815024687
Testing accuracy  of updater 0 on alg 1 with rate 3.9999999999999996E-5 = 0.5, training accuracy 0.6474587245063127, time elapsed: 49 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.7278880345647418
Loss in iteration 3 : 2.418540714988017
Loss in iteration 4 : 2.4929687246282977
Loss in iteration 5 : 2.012794530563873
Loss in iteration 6 : 1.0817015316494
Loss in iteration 7 : 0.6429260467231581
Loss in iteration 8 : 1.0996920806624757
Loss in iteration 9 : 1.594641976693005
Loss in iteration 10 : 1.5303033477713324
Loss in iteration 11 : 1.1429297780969865
Loss in iteration 12 : 0.8910558245568359
Loss in iteration 13 : 0.9242629764627767
Loss in iteration 14 : 1.1086449578933724
Loss in iteration 15 : 1.2707779585590702
Loss in iteration 16 : 1.3188925421396713
Loss in iteration 17 : 1.24556265804395
Loss in iteration 18 : 1.1109336870054844
Loss in iteration 19 : 0.999007879845344
Loss in iteration 20 : 0.9679077434919506
Loss in iteration 21 : 1.0193435907090318
Loss in iteration 22 : 1.0962459656671428
Loss in iteration 23 : 1.1222744843852723
Loss in iteration 24 : 1.0731368817463338
Loss in iteration 25 : 0.9841917124422028
Loss in iteration 26 : 0.9146341807965016
Loss in iteration 27 : 0.8961800784793975
Loss in iteration 28 : 0.9148121148460784
Loss in iteration 29 : 0.9355210900649249
Loss in iteration 30 : 0.9247232574238154
Loss in iteration 31 : 0.8771720136296758
Loss in iteration 32 : 0.8126935527311621
Loss in iteration 33 : 0.7666120308641694
Loss in iteration 34 : 0.7527657819808795
Loss in iteration 35 : 0.7617517329301504
Loss in iteration 36 : 0.7513475148473752
Loss in iteration 37 : 0.7045058720352042
Loss in iteration 38 : 0.6506026676709404
Loss in iteration 39 : 0.6251118690782368
Loss in iteration 40 : 0.622582268361826
Loss in iteration 41 : 0.6119254230150852
Loss in iteration 42 : 0.5759283145963486
Loss in iteration 43 : 0.5305037309691935
Loss in iteration 44 : 0.5128046355062896
Loss in iteration 45 : 0.5135979448473484
Loss in iteration 46 : 0.4889599029327282
Loss in iteration 47 : 0.4491855931759711
Loss in iteration 48 : 0.44709227580021454
Loss in iteration 49 : 0.4442510976364457
Loss in iteration 50 : 0.4111583631100555
Loss in iteration 51 : 0.4289185426574394
Loss in iteration 52 : 0.4113216082704221
Loss in iteration 53 : 0.43359268100718884
Loss in iteration 54 : 0.4173696847924007
Loss in iteration 55 : 0.46384432968124856
Loss in iteration 56 : 0.4335374675663553
Loss in iteration 57 : 0.4228733777487799
Loss in iteration 58 : 0.43713652226190725
Loss in iteration 59 : 0.3975136754161653
Loss in iteration 60 : 0.4050180386411846
Loss in iteration 61 : 0.3895745220261392
Loss in iteration 62 : 0.39760061185997553
Loss in iteration 63 : 0.38730063740906207
Loss in iteration 64 : 0.3957228212437304
Loss in iteration 65 : 0.3912193312762383
Loss in iteration 66 : 0.3918688094426114
Loss in iteration 67 : 0.39483359268174373
Loss in iteration 68 : 0.3896106557546415
Loss in iteration 69 : 0.39202473905820945
Loss in iteration 70 : 0.38928079034795154
Loss in iteration 71 : 0.3857728344418134
Loss in iteration 72 : 0.3869101063839323
Loss in iteration 73 : 0.3811051503026097
Loss in iteration 74 : 0.3810469945366768
Loss in iteration 75 : 0.37831699790213213
Loss in iteration 76 : 0.37731537217363487
Loss in iteration 77 : 0.37632772792854224
Loss in iteration 78 : 0.3772450800862319
Loss in iteration 79 : 0.3760050810550151
Loss in iteration 80 : 0.3791159393386867
Loss in iteration 81 : 0.3769491395178353
Loss in iteration 82 : 0.38009939320864256
Loss in iteration 83 : 0.3780093928172033
Testing accuracy  of updater 1 on alg 1 with rate 5.0E-4 = 0.786, training accuracy 0.8387827775979282, time elapsed: 1178 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3152840068434255
Loss in iteration 3 : 1.7987408831397187
Loss in iteration 4 : 1.850840489887914
Loss in iteration 5 : 1.5147185540428199
Loss in iteration 6 : 0.8528302635835923
Loss in iteration 7 : 0.5231617668300272
Loss in iteration 8 : 0.877535521376189
Loss in iteration 9 : 1.2168867640729855
Loss in iteration 10 : 1.1409062545274309
Loss in iteration 11 : 0.8541828745800157
Loss in iteration 12 : 0.686059765674355
Loss in iteration 13 : 0.7264168749023048
Loss in iteration 14 : 0.8674794951485354
Loss in iteration 15 : 0.9783216884171916
Loss in iteration 16 : 0.997285018100345
Loss in iteration 17 : 0.9321061796971831
Loss in iteration 18 : 0.8331573886060221
Loss in iteration 19 : 0.7633122293927553
Loss in iteration 20 : 0.7570078868259774
Loss in iteration 21 : 0.8043739167041285
Loss in iteration 22 : 0.856578642133516
Loss in iteration 23 : 0.8663301834680628
Loss in iteration 24 : 0.8243383261248463
Loss in iteration 25 : 0.7612240992252932
Loss in iteration 26 : 0.717863039472556
Loss in iteration 27 : 0.710465830374552
Loss in iteration 28 : 0.7267248392686494
Loss in iteration 29 : 0.7393350602321526
Loss in iteration 30 : 0.7272631711650908
Loss in iteration 31 : 0.6908158555947397
Loss in iteration 32 : 0.6486703700928902
Loss in iteration 33 : 0.6216197023262189
Loss in iteration 34 : 0.6176295705725062
Loss in iteration 35 : 0.622532328255606
Loss in iteration 36 : 0.6119374799334507
Loss in iteration 37 : 0.5798872309679266
Loss in iteration 38 : 0.544678530228411
Loss in iteration 39 : 0.5296998237528818
Loss in iteration 40 : 0.5284362736512289
Loss in iteration 41 : 0.521960471304642
Loss in iteration 42 : 0.49841354880392275
Loss in iteration 43 : 0.46796166859090166
Loss in iteration 44 : 0.4551622795138624
Loss in iteration 45 : 0.45634499250391775
Loss in iteration 46 : 0.4436157114033616
Loss in iteration 47 : 0.4179566439725243
Loss in iteration 48 : 0.41089953165166704
Loss in iteration 49 : 0.41525246806738114
Loss in iteration 50 : 0.39939059473902305
Loss in iteration 51 : 0.39120448071485786
Loss in iteration 52 : 0.40591162315117435
Loss in iteration 53 : 0.39015317268290556
Loss in iteration 54 : 0.4065424433467802
Loss in iteration 55 : 0.40103439030920757
Loss in iteration 56 : 0.409372725227841
Loss in iteration 57 : 0.40239144197345283
Loss in iteration 58 : 0.4076117693549728
Loss in iteration 59 : 0.3980856106955786
Loss in iteration 60 : 0.3982779261151174
Loss in iteration 61 : 0.38950721382911757
Loss in iteration 62 : 0.38748768269024175
Loss in iteration 63 : 0.3862249422999646
Loss in iteration 64 : 0.3810538826019307
Loss in iteration 65 : 0.38508360708591427
Loss in iteration 66 : 0.3817337485415674
Loss in iteration 67 : 0.38180568187285546
Loss in iteration 68 : 0.38435610916570806
Loss in iteration 69 : 0.38181968746172806
Loss in iteration 70 : 0.38263794650749317
Loss in iteration 71 : 0.38371960434389285
Loss in iteration 72 : 0.3814111847195287
Loss in iteration 73 : 0.3817517693928148
Loss in iteration 74 : 0.38166710199138587
Loss in iteration 75 : 0.3797302243363676
Loss in iteration 76 : 0.3795491046367871
Loss in iteration 77 : 0.37877005884227954
Loss in iteration 78 : 0.37732397707347437
Loss in iteration 79 : 0.37727102240366556
Loss in iteration 80 : 0.37616343464576363
Loss in iteration 81 : 0.3760061221228505
Loss in iteration 82 : 0.37577672472571294
Loss in iteration 83 : 0.3753350751999353
Loss in iteration 84 : 0.37574290440611874
Loss in iteration 85 : 0.3754090862826364
Testing accuracy  of updater 1 on alg 1 with rate 3.5000000000000005E-4 = 0.7865, training accuracy 0.8387827775979282, time elapsed: 1255 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9026799791221102
Loss in iteration 3 : 1.178941051291423
Loss in iteration 4 : 1.208712255147529
Loss in iteration 5 : 1.0166425775217598
Loss in iteration 6 : 0.6328126207304918
Loss in iteration 7 : 0.4213654373423672
Loss in iteration 8 : 0.6498408718598012
Loss in iteration 9 : 0.8243122154930598
Loss in iteration 10 : 0.747126956732451
Loss in iteration 11 : 0.5740004594208519
Loss in iteration 12 : 0.4942447985218138
Loss in iteration 13 : 0.5377537704360151
Loss in iteration 14 : 0.6270326078015139
Loss in iteration 15 : 0.6800068566718513
Loss in iteration 16 : 0.6706317706493323
Loss in iteration 17 : 0.6171904964485958
Loss in iteration 18 : 0.5623617666393685
Loss in iteration 19 : 0.5414217957089643
Loss in iteration 20 : 0.5588009674578546
Loss in iteration 21 : 0.5942309443529032
Loss in iteration 22 : 0.6152570854845596
Loss in iteration 23 : 0.6052689836735162
Loss in iteration 24 : 0.5730473591378646
Loss in iteration 25 : 0.5414272728733307
Loss in iteration 26 : 0.5279262645563386
Loss in iteration 27 : 0.5343985223787785
Loss in iteration 28 : 0.5468165479428085
Loss in iteration 29 : 0.5500275773802336
Loss in iteration 30 : 0.5372062297581712
Loss in iteration 31 : 0.5142956862078706
Loss in iteration 32 : 0.4939076828329306
Loss in iteration 33 : 0.484326129324352
Loss in iteration 34 : 0.48770418888812395
Loss in iteration 35 : 0.49020877935267054
Loss in iteration 36 : 0.4816842384711172
Loss in iteration 37 : 0.46361753779293846
Loss in iteration 38 : 0.4470090013490337
Loss in iteration 39 : 0.4410045892520426
Loss in iteration 40 : 0.441771044602831
Loss in iteration 41 : 0.43966718533503685
Loss in iteration 42 : 0.42756304978339316
Loss in iteration 43 : 0.4121760579753547
Loss in iteration 44 : 0.40628673041003394
Loss in iteration 45 : 0.4079097524619645
Loss in iteration 46 : 0.40518065792288005
Loss in iteration 47 : 0.39427060271401093
Loss in iteration 48 : 0.3861349711674975
Loss in iteration 49 : 0.38773947461695574
Loss in iteration 50 : 0.3889855970063088
Loss in iteration 51 : 0.3825101601373754
Loss in iteration 52 : 0.3780351964100991
Loss in iteration 53 : 0.38240795368248115
Loss in iteration 54 : 0.3833455033923223
Loss in iteration 55 : 0.3786545763089662
Loss in iteration 56 : 0.38214268495998654
Loss in iteration 57 : 0.3850043318814159
Loss in iteration 58 : 0.3813285288930954
Loss in iteration 59 : 0.38417632656228057
Testing accuracy  of updater 1 on alg 1 with rate 2.0E-4 = 0.791, training accuracy 0.8394302363224344, time elapsed: 867 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5624922659103616
Loss in iteration 3 : 0.6353367091236468
Loss in iteration 4 : 0.7103781853452057
Loss in iteration 5 : 0.724324370809546
Loss in iteration 6 : 0.6827544957093412
Loss in iteration 7 : 0.5930821022021145
Loss in iteration 8 : 0.4830279953253102
Loss in iteration 9 : 0.41972379040460955
Loss in iteration 10 : 0.4590737879686528
Loss in iteration 11 : 0.5167373755772148
Loss in iteration 12 : 0.5121411163415667
Loss in iteration 13 : 0.4590451439685478
Loss in iteration 14 : 0.4051133149382155
Loss in iteration 15 : 0.3875403414729742
Loss in iteration 16 : 0.40241322661115914
Loss in iteration 17 : 0.42517591857777937
Loss in iteration 18 : 0.4358399335177618
Loss in iteration 19 : 0.42949411630768214
Loss in iteration 20 : 0.4135585614967798
Loss in iteration 21 : 0.3981396302233844
Loss in iteration 22 : 0.39018170971299515
Loss in iteration 23 : 0.39300414116662435
Loss in iteration 24 : 0.4014149164890065
Loss in iteration 25 : 0.409049569949542
Testing accuracy  of updater 1 on alg 1 with rate 5.0E-5 = 0.782, training accuracy 0.8209776626740045, time elapsed: 360 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6267485974473342
Loss in iteration 3 : 0.5953025987406623
Loss in iteration 4 : 0.6861093632055716
Loss in iteration 5 : 0.7329011256594602
Loss in iteration 6 : 0.7370791909179659
Loss in iteration 7 : 0.702805256385469
Loss in iteration 8 : 0.6346628556991125
Loss in iteration 9 : 0.541983591067788
Loss in iteration 10 : 0.4559652261075423
Loss in iteration 11 : 0.41801675036625013
Loss in iteration 12 : 0.45418707697591454
Loss in iteration 13 : 0.5047257651354274
Loss in iteration 14 : 0.5164388267672659
Loss in iteration 15 : 0.485651242427613
Loss in iteration 16 : 0.437035783963119
Loss in iteration 17 : 0.39958376413215857
Loss in iteration 18 : 0.3883478663961137
Loss in iteration 19 : 0.3983443714685744
Loss in iteration 20 : 0.41591695713885735
Loss in iteration 21 : 0.4273989617786483
Loss in iteration 22 : 0.42765186175869424
Loss in iteration 23 : 0.4183943292395477
Loss in iteration 24 : 0.405176009405211
Loss in iteration 25 : 0.39387096250713083
Loss in iteration 26 : 0.3880994701606637
Loss in iteration 27 : 0.3889098042386161
Loss in iteration 28 : 0.3938299126384395
Testing accuracy  of updater 1 on alg 1 with rate 3.5000000000000004E-5 = 0.77975, training accuracy 0.8229200388475235, time elapsed: 353 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7635519275264456
Loss in iteration 3 : 0.5545269375624008
Loss in iteration 4 : 0.6042705240567514
Loss in iteration 5 : 0.6640491124969861
Loss in iteration 6 : 0.6978268787615527
Loss in iteration 7 : 0.7067618065859148
Loss in iteration 8 : 0.6931760608334915
Loss in iteration 9 : 0.6595419947315918
Loss in iteration 10 : 0.6085758585606414
Loss in iteration 11 : 0.5464237923729337
Loss in iteration 12 : 0.4873625339546142
Loss in iteration 13 : 0.4464786769287026
Loss in iteration 14 : 0.43309698900425997
Loss in iteration 15 : 0.4486649268919943
Loss in iteration 16 : 0.4761616719618397
Loss in iteration 17 : 0.4901331161788934
Loss in iteration 18 : 0.4829323795059871
Loss in iteration 19 : 0.4594705497227044
Loss in iteration 20 : 0.4310499102787568
Loss in iteration 21 : 0.40775204841001295
Loss in iteration 22 : 0.3952506772568206
Loss in iteration 23 : 0.39463521699637866
Loss in iteration 24 : 0.400975240550282
Loss in iteration 25 : 0.4088099403780902
Testing accuracy  of updater 1 on alg 1 with rate 2.0E-5 = 0.75075, training accuracy 0.817740369051473, time elapsed: 380 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9406648675344318
Loss in iteration 3 : 0.8279281158498556
Loss in iteration 4 : 0.6809799310369014
Loss in iteration 5 : 0.5794772878709906
Loss in iteration 6 : 0.5541935577860287
Loss in iteration 7 : 0.5686003372292696
Loss in iteration 8 : 0.5923861375583068
Loss in iteration 9 : 0.6132779899611643
Testing accuracy  of updater 1 on alg 1 with rate 4.9999999999999996E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 115 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.0109636546226985
Loss in iteration 3 : 4.115162868119083
Loss in iteration 4 : 3.442918996428885
Loss in iteration 5 : 2.0762971278691635
Loss in iteration 6 : 0.8816388107045456
Loss in iteration 7 : 1.0044929548296535
Loss in iteration 8 : 1.2846319534126192
Loss in iteration 9 : 1.279422336304991
Loss in iteration 10 : 1.163855174713761
Loss in iteration 11 : 1.0855404800030357
Loss in iteration 12 : 1.0819737598057704
Loss in iteration 13 : 1.104435533285832
Loss in iteration 14 : 1.1266360914744151
Loss in iteration 15 : 1.1305332644438673
Loss in iteration 16 : 1.1192528922961227
Loss in iteration 17 : 1.104252569209408
Loss in iteration 18 : 1.0882984797844877
Loss in iteration 19 : 1.0718847506185711
Loss in iteration 20 : 1.0539784724994552
Loss in iteration 21 : 1.03279489965115
Loss in iteration 22 : 1.0072780595855209
Loss in iteration 23 : 0.9791059228193993
Loss in iteration 24 : 0.9496556241673287
Loss in iteration 25 : 0.9190499822602286
Loss in iteration 26 : 0.8872995548058794
Loss in iteration 27 : 0.854371234945748
Loss in iteration 28 : 0.8203912142278815
Loss in iteration 29 : 0.7856943388047687
Loss in iteration 30 : 0.7502980003060555
Loss in iteration 31 : 0.7146092422273322
Loss in iteration 32 : 0.6788968476117329
Loss in iteration 33 : 0.6433224061593974
Loss in iteration 34 : 0.6086970102140422
Loss in iteration 35 : 0.5750456976877953
Loss in iteration 36 : 0.5427830335054853
Loss in iteration 37 : 0.5118922966214688
Loss in iteration 38 : 0.48364832277607067
Loss in iteration 39 : 0.4588110240715644
Loss in iteration 40 : 0.4378347590304612
Loss in iteration 41 : 0.4229007643901943
Loss in iteration 42 : 0.4165636003985375
Loss in iteration 43 : 0.4199283474611373
Loss in iteration 44 : 0.45332870470174647
Loss in iteration 45 : 0.8198346993013682
Loss in iteration 46 : 1.3371287231551512
Loss in iteration 47 : 2.7546176006163563
Loss in iteration 48 : 2.4448550318856257
Loss in iteration 49 : 1.4016292851989876
Loss in iteration 50 : 0.553230650519105
Loss in iteration 51 : 0.8003789340699992
Loss in iteration 52 : 0.7186117709693914
Loss in iteration 53 : 0.6621961446199601
Loss in iteration 54 : 0.6673764969154043
Loss in iteration 55 : 0.684281521067724
Loss in iteration 56 : 0.6916761335646038
Loss in iteration 57 : 0.6912190555030624
Loss in iteration 58 : 0.6876473180452425
Loss in iteration 59 : 0.6810262201303365
Loss in iteration 60 : 0.6707819837184112
Loss in iteration 61 : 0.6571118502447777
Loss in iteration 62 : 0.640527907475794
Loss in iteration 63 : 0.6214308297971026
Loss in iteration 64 : 0.6002585388483113
Loss in iteration 65 : 0.5771952056428303
Loss in iteration 66 : 0.5530564845543369
Loss in iteration 67 : 0.5283688921899135
Loss in iteration 68 : 0.5035687939738714
Loss in iteration 69 : 0.4789440768008967
Loss in iteration 70 : 0.4550187208498781
Loss in iteration 71 : 0.432905435777052
Loss in iteration 72 : 0.41376262288800647
Loss in iteration 73 : 0.3985899074853795
Loss in iteration 74 : 0.38991122021693986
Loss in iteration 75 : 0.39020151807336484
Loss in iteration 76 : 0.4022905686633871
Loss in iteration 77 : 0.5295624654537409
Loss in iteration 78 : 2.3757315268175163
Loss in iteration 79 : 3.475324638579118
Loss in iteration 80 : 3.574166410503556
Loss in iteration 81 : 2.8971008413986197
Loss in iteration 82 : 1.5523166840507163
Loss in iteration 83 : 0.7261002490544474
Loss in iteration 84 : 0.9871901067102541
Loss in iteration 85 : 1.1224664321166282
Loss in iteration 86 : 1.0138018187278262
Loss in iteration 87 : 0.9247000325912643
Loss in iteration 88 : 0.9094984357687613
Loss in iteration 89 : 0.9298708542111914
Loss in iteration 90 : 0.9478356485588835
Loss in iteration 91 : 0.9505311618535685
Loss in iteration 92 : 0.9407880595296576
Loss in iteration 93 : 0.9267381496597021
Loss in iteration 94 : 0.9123889493228181
Loss in iteration 95 : 0.8977587129931157
Loss in iteration 96 : 0.8805817228767853
Loss in iteration 97 : 0.8596289542886815
Loss in iteration 98 : 0.8355376026311337
Loss in iteration 99 : 0.8095985238788244
Loss in iteration 100 : 0.7819366998531371
Loss in iteration 101 : 0.7529613733229744
Loss in iteration 102 : 0.7229147803183205
Loss in iteration 103 : 0.6916609108883033
Loss in iteration 104 : 0.6597809212778051
Loss in iteration 105 : 0.627340784758717
Loss in iteration 106 : 0.5949740569290076
Loss in iteration 107 : 0.5636559872616681
Loss in iteration 108 : 0.5330516462751684
Loss in iteration 109 : 0.5033196651307396
Loss in iteration 110 : 0.47559297122753297
Loss in iteration 111 : 0.4510675288209542
Loss in iteration 112 : 0.4301562557177564
Loss in iteration 113 : 0.41399042787452495
Loss in iteration 114 : 0.40773006823581837
Loss in iteration 115 : 0.4169100756224103
Loss in iteration 116 : 0.617628205912634
Loss in iteration 117 : 1.8249570428679263
Loss in iteration 118 : 1.1068742232833377
Loss in iteration 119 : 0.670823963213875
Loss in iteration 120 : 0.7500233671374907
Loss in iteration 121 : 0.6131282140839379
Loss in iteration 122 : 0.5051951495730028
Loss in iteration 123 : 0.4663188957277922
Loss in iteration 124 : 0.46360019856963863
Loss in iteration 125 : 0.4682098315874379
Loss in iteration 126 : 0.46949254802083107
Loss in iteration 127 : 0.4676150409963895
Loss in iteration 128 : 0.4629760116605563
Loss in iteration 129 : 0.4559285328920117
Loss in iteration 130 : 0.44701955909692054
Loss in iteration 131 : 0.4365634755263178
Loss in iteration 132 : 0.4250020753161525
Loss in iteration 133 : 0.41306354258579014
Loss in iteration 134 : 0.40166564146317424
Loss in iteration 135 : 0.39149022687912394
Loss in iteration 136 : 0.38386424936316954
Loss in iteration 137 : 0.37927551806976467
Loss in iteration 138 : 0.38137896522736703
Loss in iteration 139 : 0.42116417411467005
Loss in iteration 140 : 1.113983557276795
Loss in iteration 141 : 2.75688110375275
Loss in iteration 142 : 2.491481686650836
Loss in iteration 143 : 1.4865990474221773
Loss in iteration 144 : 0.5156354143362955
Loss in iteration 145 : 0.7669125590407397
Loss in iteration 146 : 0.6228633048023837
Loss in iteration 147 : 0.5911309729048579
Loss in iteration 148 : 0.6022967493724325
Loss in iteration 149 : 0.6100733002554646
Loss in iteration 150 : 0.610204979219543
Loss in iteration 151 : 0.6068775417163693
Loss in iteration 152 : 0.5998273459846671
Loss in iteration 153 : 0.589090885665205
Loss in iteration 154 : 0.5751620286873542
Loss in iteration 155 : 0.5585881845503803
Loss in iteration 156 : 0.539846966508382
Loss in iteration 157 : 0.5193087560562181
Loss in iteration 158 : 0.49773334236672745
Loss in iteration 159 : 0.4753585912002213
Loss in iteration 160 : 0.45322855600259715
Loss in iteration 161 : 0.4325118584641942
Loss in iteration 162 : 0.4132582867553813
Loss in iteration 163 : 0.3973294036970472
Loss in iteration 164 : 0.38619470832464764
Loss in iteration 165 : 0.382150874472241
Loss in iteration 166 : 0.3878155123551447
Loss in iteration 167 : 0.44316245472394267
Loss in iteration 168 : 1.0320528623771812
Loss in iteration 169 : 0.6208097000595599
Loss in iteration 170 : 1.611681133524523
Loss in iteration 171 : 0.7319201465359362
Loss in iteration 172 : 1.1149290411651094
Loss in iteration 173 : 0.798807512464015
Loss in iteration 174 : 0.482249442815116
Loss in iteration 175 : 0.513262648410144
Loss in iteration 176 : 0.5224823229572645
Loss in iteration 177 : 0.5340707414169487
Loss in iteration 178 : 0.5409866826198194
Loss in iteration 179 : 0.5431577854177105
Loss in iteration 180 : 0.5410267250568415
Testing accuracy  of updater 2 on alg 1 with rate 6.999999999999999E-4 = 0.78075, training accuracy 0.8316607316283587, time elapsed: 2458 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.913436940883992
Loss in iteration 3 : 2.986376390331459
Loss in iteration 4 : 2.5158056801483286
Loss in iteration 5 : 1.5578563576900586
Loss in iteration 6 : 0.6804947884772491
Loss in iteration 7 : 0.7800330731897331
Loss in iteration 8 : 0.9812544801981162
Loss in iteration 9 : 0.9616044070611943
Loss in iteration 10 : 0.8739755039269458
Loss in iteration 11 : 0.8205157984771541
Loss in iteration 12 : 0.8213523547177706
Loss in iteration 13 : 0.8397538216008467
Loss in iteration 14 : 0.8540811524704012
Loss in iteration 15 : 0.8575780875188413
Loss in iteration 16 : 0.8507765419454044
Loss in iteration 17 : 0.8397770987534884
Loss in iteration 18 : 0.8285279842899342
Loss in iteration 19 : 0.8172843625146252
Loss in iteration 20 : 0.8057570540620046
Loss in iteration 21 : 0.7914066994957143
Loss in iteration 22 : 0.7743541641943398
Loss in iteration 23 : 0.7550660895051025
Loss in iteration 24 : 0.734890262197611
Loss in iteration 25 : 0.7143120731743617
Loss in iteration 26 : 0.6930200319611461
Loss in iteration 27 : 0.6708411490738349
Loss in iteration 28 : 0.6479744713124571
Loss in iteration 29 : 0.6244267203717218
Loss in iteration 30 : 0.6007007391961962
Loss in iteration 31 : 0.5767974667224147
Loss in iteration 32 : 0.5531622951229248
Loss in iteration 33 : 0.5301318415485209
Loss in iteration 34 : 0.5078188254748212
Loss in iteration 35 : 0.4863510339572391
Loss in iteration 36 : 0.46583800157992084
Loss in iteration 37 : 0.44669784485635317
Loss in iteration 38 : 0.4298334766376491
Loss in iteration 39 : 0.41529345039022647
Loss in iteration 40 : 0.40348545271682446
Loss in iteration 41 : 0.3960307848394792
Loss in iteration 42 : 0.39413851859928223
Loss in iteration 43 : 0.39676458188646946
Loss in iteration 44 : 0.408104407655813
Loss in iteration 45 : 0.46594826050765825
Loss in iteration 46 : 0.8176925585912213
Loss in iteration 47 : 0.48191085182385623
Loss in iteration 48 : 0.7027060855508626
Loss in iteration 49 : 0.5586553577564072
Loss in iteration 50 : 0.6040360198283174
Loss in iteration 51 : 0.4587163337821237
Loss in iteration 52 : 0.4184609476323187
Loss in iteration 53 : 0.4138061380122104
Loss in iteration 54 : 0.41706682500995046
Loss in iteration 55 : 0.41982240208939625
Loss in iteration 56 : 0.42091144653857643
Loss in iteration 57 : 0.4203455723329143
Loss in iteration 58 : 0.4183157418424862
Loss in iteration 59 : 0.4150591522799666
Loss in iteration 60 : 0.41085969015266466
Loss in iteration 61 : 0.40588048395297993
Loss in iteration 62 : 0.40047096692724404
Loss in iteration 63 : 0.3948627319995663
Loss in iteration 64 : 0.38970262643642517
Loss in iteration 65 : 0.3850364585518957
Loss in iteration 66 : 0.38125692275129963
Loss in iteration 67 : 0.37856433187630506
Loss in iteration 68 : 0.3772260966380472
Loss in iteration 69 : 0.37752966143844685
Loss in iteration 70 : 0.3813334623229076
Loss in iteration 71 : 0.4091693235724967
Loss in iteration 72 : 0.5875022705276198
Loss in iteration 73 : 0.7910073442403979
Loss in iteration 74 : 1.148835154239604
Loss in iteration 75 : 0.6071997131121909
Loss in iteration 76 : 0.5610172279187443
Loss in iteration 77 : 0.41957970455995386
Loss in iteration 78 : 0.42912951194209176
Loss in iteration 79 : 0.43618837989781467
Loss in iteration 80 : 0.4404069780102432
Loss in iteration 81 : 0.4420292934651868
Testing accuracy  of updater 2 on alg 1 with rate 4.9E-4 = 0.779, training accuracy 0.8319844609906119, time elapsed: 1047 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.815910227145291
Loss in iteration 3 : 1.8575899125438422
Loss in iteration 4 : 1.5886923638677692
Loss in iteration 5 : 1.0407474194241153
Loss in iteration 6 : 0.49204125246312896
Loss in iteration 7 : 0.5634913831783773
Loss in iteration 8 : 0.6636515124407106
Loss in iteration 9 : 0.6419230299498504
Loss in iteration 10 : 0.5898469487960162
Loss in iteration 11 : 0.5647473502975429
Loss in iteration 12 : 0.5693296154765839
Loss in iteration 13 : 0.5811485868002977
Loss in iteration 14 : 0.5892005676221632
Loss in iteration 15 : 0.5899119000379183
Loss in iteration 16 : 0.5860976834547625
Loss in iteration 17 : 0.5809766159956309
Loss in iteration 18 : 0.5761143916936572
Loss in iteration 19 : 0.5712632865052943
Loss in iteration 20 : 0.5655086669689618
Loss in iteration 21 : 0.5580917942391872
Loss in iteration 22 : 0.5491362725099738
Loss in iteration 23 : 0.539533759741964
Loss in iteration 24 : 0.5297353964141642
Loss in iteration 25 : 0.5195361303357006
Loss in iteration 26 : 0.508875355529816
Loss in iteration 27 : 0.49787820907971037
Loss in iteration 28 : 0.48675157074317316
Loss in iteration 29 : 0.47563164046210377
Loss in iteration 30 : 0.4644707943994083
Loss in iteration 31 : 0.453357173072809
Loss in iteration 32 : 0.4423260943357409
Loss in iteration 33 : 0.4317625262731042
Loss in iteration 34 : 0.42188302464884697
Loss in iteration 35 : 0.4127969963037047
Loss in iteration 36 : 0.4043963871941033
Loss in iteration 37 : 0.39690485837591594
Loss in iteration 38 : 0.39059122144213887
Loss in iteration 39 : 0.38566312176470796
Loss in iteration 40 : 0.38211093260216167
Loss in iteration 41 : 0.3797850055502804
Loss in iteration 42 : 0.37917564286052385
Loss in iteration 43 : 0.3797087455965829
Loss in iteration 44 : 0.3812729710783005
Loss in iteration 45 : 0.3832626443658632
Loss in iteration 46 : 0.3851843366409262
Loss in iteration 47 : 0.38627760471463224
Testing accuracy  of updater 2 on alg 1 with rate 2.8E-4 = 0.789, training accuracy 0.8429912593072192, time elapsed: 546 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7185432341540061
Loss in iteration 3 : 0.7294257159278489
Loss in iteration 4 : 0.663317174656249
Loss in iteration 5 : 0.5371070841251006
Loss in iteration 6 : 0.4307594296066546
Loss in iteration 7 : 0.42518101939745784
Loss in iteration 8 : 0.42755801175399677
Loss in iteration 9 : 0.4088998788383447
Loss in iteration 10 : 0.39398551094261586
Loss in iteration 11 : 0.3876458556597646
Loss in iteration 12 : 0.38731591546453825
Loss in iteration 13 : 0.3880127620492531
Loss in iteration 14 : 0.38842756805092704
Loss in iteration 15 : 0.38836868043645933
Loss in iteration 16 : 0.388254419981795
Loss in iteration 17 : 0.388478473169955
Loss in iteration 18 : 0.3889841178149869
Loss in iteration 19 : 0.38942884228281294
Loss in iteration 20 : 0.38970566499438036
Testing accuracy  of updater 2 on alg 1 with rate 7.0E-5 = 0.77875, training accuracy 0.8329556490773713, time elapsed: 249 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6109464762328297
Loss in iteration 3 : 0.6240178955463592
Loss in iteration 4 : 0.5844476162207393
Loss in iteration 5 : 0.5181596376215527
Loss in iteration 6 : 0.4699611483016798
Loss in iteration 7 : 0.449266839043701
Loss in iteration 8 : 0.4375791777368766
Loss in iteration 9 : 0.4268570914310641
Loss in iteration 10 : 0.412634382249027
Loss in iteration 11 : 0.4008906729398937
Loss in iteration 12 : 0.3937462952572943
Loss in iteration 13 : 0.3903064220466758
Loss in iteration 14 : 0.38859998249762084
Loss in iteration 15 : 0.38743716126427746
Loss in iteration 16 : 0.38653831747685413
Loss in iteration 17 : 0.3857662804498578
Loss in iteration 18 : 0.38523490446158326
Loss in iteration 19 : 0.38498711147911624
Loss in iteration 20 : 0.38488379349295515
Loss in iteration 21 : 0.3848223694004582
Testing accuracy  of updater 2 on alg 1 with rate 4.9E-5 = 0.7795, training accuracy 0.8329556490773713, time elapsed: 273 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5574494820533261
Loss in iteration 3 : 0.5782001777312115
Loss in iteration 4 : 0.5887387094980089
Loss in iteration 5 : 0.5735254229247184
Loss in iteration 6 : 0.5407114747148114
Loss in iteration 7 : 0.5082482146747155
Loss in iteration 8 : 0.48787309065204737
Loss in iteration 9 : 0.4766469505230246
Loss in iteration 10 : 0.46737675092308223
Loss in iteration 11 : 0.45701043374439093
Loss in iteration 12 : 0.44570338059117204
Loss in iteration 13 : 0.4342078483521723
Loss in iteration 14 : 0.42368201644891185
Loss in iteration 15 : 0.4147736483661218
Loss in iteration 16 : 0.4075102760031847
Loss in iteration 17 : 0.4018011475944377
Loss in iteration 18 : 0.3976082388964635
Loss in iteration 19 : 0.39452882882469925
Loss in iteration 20 : 0.3920414704030918
Loss in iteration 21 : 0.39005210658237877
Loss in iteration 22 : 0.388562290287156
Loss in iteration 23 : 0.38741653060364395
Loss in iteration 24 : 0.3865587130860883
Testing accuracy  of updater 2 on alg 1 with rate 2.7999999999999996E-5 = 0.77875, training accuracy 0.8319844609906119, time elapsed: 265 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8421685476415909
Loss in iteration 3 : 0.645963378273028
Loss in iteration 4 : 0.559917566911478
Loss in iteration 5 : 0.557774521084823
Loss in iteration 6 : 0.5734520869252581
Loss in iteration 7 : 0.58695294287514
Testing accuracy  of updater 2 on alg 1 with rate 6.999999999999994E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 75 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 89.84829860215356
Loss in iteration 3 : 55.659436352767244
Loss in iteration 4 : 24.740372580822143
Loss in iteration 5 : 4.437831437142936
Loss in iteration 6 : 4.116182690579972
Loss in iteration 7 : 3.957882514783732
Loss in iteration 8 : 3.792772398598835
Loss in iteration 9 : 3.6711792110946084
Loss in iteration 10 : 3.5184714684180762
Loss in iteration 11 : 3.4050558482002558
Loss in iteration 12 : 3.295971975209122
Loss in iteration 13 : 3.3288170019914225
Loss in iteration 14 : 3.3016932944197923
Loss in iteration 15 : 3.9299421920224407
Loss in iteration 16 : 4.34727451657303
Loss in iteration 17 : 7.574748231341259
Loss in iteration 18 : 8.173704269479062
Loss in iteration 19 : 14.004834381333369
Loss in iteration 20 : 3.844691499689864
Loss in iteration 21 : 3.970855133688928
Loss in iteration 22 : 3.3401063358372323
Loss in iteration 23 : 3.4491304004856778
Loss in iteration 24 : 3.01719091019758
Loss in iteration 25 : 3.1762389827011774
Loss in iteration 26 : 2.9298466216909675
Loss in iteration 27 : 3.504105453617575
Loss in iteration 28 : 3.5774688871063933
Loss in iteration 29 : 5.645340153175172
Loss in iteration 30 : 5.59283112009848
Loss in iteration 31 : 10.365597626068759
Loss in iteration 32 : 4.1716278421354875
Loss in iteration 33 : 5.017600252062992
Loss in iteration 34 : 3.2139091668586146
Loss in iteration 35 : 3.581247425637965
Loss in iteration 36 : 2.980491034432524
Loss in iteration 37 : 3.4652435636716192
Loss in iteration 38 : 3.085726828335648
Loss in iteration 39 : 3.8857421735446898
Loss in iteration 40 : 3.394510847798554
Loss in iteration 41 : 5.218901914210196
Loss in iteration 42 : 4.153128060935088
Loss in iteration 43 : 6.879163013262611
Loss in iteration 44 : 4.019493050005512
Loss in iteration 45 : 5.338258404128286
Loss in iteration 46 : 3.1119371057768883
Loss in iteration 47 : 3.4633332943371076
Loss in iteration 48 : 2.7416629411860174
Loss in iteration 49 : 3.234439621768823
Loss in iteration 50 : 2.7751845575125715
Loss in iteration 51 : 3.546649237957204
Loss in iteration 52 : 2.997747844767562
Loss in iteration 53 : 4.49422499150037
Loss in iteration 54 : 3.561534776029424
Loss in iteration 55 : 6.033222041534575
Loss in iteration 56 : 3.6891905922360535
Loss in iteration 57 : 5.408454786811682
Loss in iteration 58 : 3.0617355755977336
Loss in iteration 59 : 3.594869955504231
Loss in iteration 60 : 2.610814620829382
Loss in iteration 61 : 3.1034301429839255
Loss in iteration 62 : 2.5234234069578245
Loss in iteration 63 : 3.187429542334678
Loss in iteration 64 : 2.7455304973589563
Loss in iteration 65 : 4.029479420904867
Loss in iteration 66 : 3.208370387301253
Loss in iteration 67 : 5.502050071391595
Loss in iteration 68 : 3.4026977630991007
Loss in iteration 69 : 5.134402535835155
Loss in iteration 70 : 2.822704993684324
Loss in iteration 71 : 3.459254845581868
Loss in iteration 72 : 2.467119496308304
Loss in iteration 73 : 2.9174096098637925
Loss in iteration 74 : 2.3859624103619406
Loss in iteration 75 : 3.019018273044907
Loss in iteration 76 : 2.601611312967162
Loss in iteration 77 : 3.976048862591862
Loss in iteration 78 : 2.9916687082303235
Loss in iteration 79 : 4.974915470466993
Loss in iteration 80 : 3.1535778169300586
Loss in iteration 81 : 4.785100353173988
Loss in iteration 82 : 2.639005351177872
Loss in iteration 83 : 3.238686042126879
Loss in iteration 84 : 2.335574740517328
Loss in iteration 85 : 2.8323061858271723
Loss in iteration 86 : 2.326335498269563
Loss in iteration 87 : 3.1977638635178556
Loss in iteration 88 : 2.4526200166557484
Loss in iteration 89 : 3.647613439555645
Loss in iteration 90 : 2.7468525160281114
Loss in iteration 91 : 4.563139428156399
Loss in iteration 92 : 2.7199495943941012
Loss in iteration 93 : 4.015163626756898
Loss in iteration 94 : 2.4192724804931727
Loss in iteration 95 : 3.12089828646061
Loss in iteration 96 : 2.260733968711652
Loss in iteration 97 : 3.0522150569929636
Loss in iteration 98 : 2.3057421765997255
Loss in iteration 99 : 3.272714974446892
Loss in iteration 100 : 2.4734552829777905
Loss in iteration 101 : 3.889340092765623
Loss in iteration 102 : 2.5444512606607685
Loss in iteration 103 : 3.9160280106519596
Loss in iteration 104 : 2.453459421470566
Loss in iteration 105 : 3.431607803012641
Loss in iteration 106 : 2.3395321151583968
Loss in iteration 107 : 3.2432317827570203
Loss in iteration 108 : 2.2985009367344045
Loss in iteration 109 : 3.2143634718457834
Loss in iteration 110 : 2.31081120818822
Loss in iteration 111 : 3.294212166684342
Loss in iteration 112 : 2.3499003834879857
Loss in iteration 113 : 3.4557232026357383
Loss in iteration 114 : 2.348281967970774
Loss in iteration 115 : 3.4154333933508045
Loss in iteration 116 : 2.2868937365842066
Loss in iteration 117 : 3.2296540594492953
Loss in iteration 118 : 2.2534605582657443
Loss in iteration 119 : 3.185671485343582
Loss in iteration 120 : 2.2387094912554324
Loss in iteration 121 : 3.2091622827465995
Loss in iteration 122 : 2.261528636693299
Loss in iteration 123 : 3.352562200793234
Loss in iteration 124 : 2.292372984411232
Loss in iteration 125 : 3.3475037269171173
Loss in iteration 126 : 2.2237328306830233
Loss in iteration 127 : 3.1375010482072168
Loss in iteration 128 : 2.1625083997000387
Loss in iteration 129 : 3.0802241208786834
Loss in iteration 130 : 2.1624398545694117
Loss in iteration 131 : 3.106317723867621
Loss in iteration 132 : 2.181693429803203
Loss in iteration 133 : 3.250434171113303
Loss in iteration 134 : 2.198524389219279
Loss in iteration 135 : 3.2451521353544783
Loss in iteration 136 : 2.140924045614229
Loss in iteration 137 : 3.0588365888533784
Loss in iteration 138 : 2.1058619729558745
Loss in iteration 139 : 2.984217788645844
Loss in iteration 140 : 2.089996587785831
Loss in iteration 141 : 3.02498252864722
Loss in iteration 142 : 2.1131219685597946
Loss in iteration 143 : 3.170230938164191
Loss in iteration 144 : 2.12955373873637
Loss in iteration 145 : 3.164169629227626
Loss in iteration 146 : 2.099533029141794
Loss in iteration 147 : 3.044613582599695
Loss in iteration 148 : 2.066818911630992
Loss in iteration 149 : 2.952021116958622
Loss in iteration 150 : 2.025991013433975
Loss in iteration 151 : 2.8791264258604183
Loss in iteration 152 : 2.039562088166124
Loss in iteration 153 : 3.022881615121414
Loss in iteration 154 : 2.080476118349362
Loss in iteration 155 : 3.120160754174507
Loss in iteration 156 : 2.050726967954937
Loss in iteration 157 : 2.9849204461420262
Loss in iteration 158 : 2.0159374495682507
Loss in iteration 159 : 2.8730233275461305
Loss in iteration 160 : 1.967385424475926
Loss in iteration 161 : 2.7919488311159326
Loss in iteration 162 : 1.9626518290698283
Loss in iteration 163 : 2.8252872223031273
Loss in iteration 164 : 1.9847309258036547
Loss in iteration 165 : 2.9943187677504315
Loss in iteration 166 : 2.0149352043802575
Loss in iteration 167 : 3.055931384838862
Loss in iteration 168 : 1.9665839228208832
Loss in iteration 169 : 2.8755645676399317
Loss in iteration 170 : 1.9674890780087575
Loss in iteration 171 : 2.8836688607612575
Loss in iteration 172 : 1.9628139822543549
Loss in iteration 173 : 2.8658910552312595
Loss in iteration 174 : 1.9469347701155186
Loss in iteration 175 : 2.819719842569911
Loss in iteration 176 : 1.8938898399549515
Loss in iteration 177 : 2.649738527714464
Loss in iteration 178 : 1.881126068183726
Loss in iteration 179 : 2.7022182876349654
Loss in iteration 180 : 1.906047183290365
Loss in iteration 181 : 2.8945870507998044
Loss in iteration 182 : 1.9357210680743953
Loss in iteration 183 : 2.9470927162235
Loss in iteration 184 : 1.9073745939937592
Loss in iteration 185 : 2.8022692503886573
Loss in iteration 186 : 1.9083156158128927
Loss in iteration 187 : 2.8163425800680786
Loss in iteration 188 : 1.8914815745809137
Loss in iteration 189 : 2.7444684633896825
Loss in iteration 190 : 1.8381022604466073
Loss in iteration 191 : 2.5539007582151143
Loss in iteration 192 : 1.8142257723887685
Loss in iteration 193 : 2.5670410013033487
Loss in iteration 194 : 1.834080208230024
Loss in iteration 195 : 2.7990207724234817
Loss in iteration 196 : 1.8733037092919982
Loss in iteration 197 : 2.900824260795098
Loss in iteration 198 : 1.8317095713241736
Loss in iteration 199 : 2.6748869066099257
Loss in iteration 200 : 1.8333436419869678
Testing accuracy  of updater 3 on alg 1 with rate 1.96 = 0.67, training accuracy 0.759792813208158, time elapsed: 2565 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 18.914301625097295
Loss in iteration 3 : 12.736912765907832
Loss in iteration 4 : 6.59832043066221
Loss in iteration 5 : 2.041173406185313
Loss in iteration 6 : 1.4570364846760868
Loss in iteration 7 : 1.4100539988249572
Loss in iteration 8 : 1.3627316546473078
Loss in iteration 9 : 1.3166056363758063
Loss in iteration 10 : 1.2701979279287283
Loss in iteration 11 : 1.2244638476349274
Loss in iteration 12 : 1.1788898749488963
Loss in iteration 13 : 1.1345584826897654
Loss in iteration 14 : 1.0897594288365247
Loss in iteration 15 : 1.0456094019339153
Loss in iteration 16 : 1.0025560733521695
Loss in iteration 17 : 0.9614945061588195
Loss in iteration 18 : 0.9222563440016737
Loss in iteration 19 : 0.8921834768406562
Loss in iteration 20 : 0.8993438986313975
Loss in iteration 21 : 0.9466525723022993
Loss in iteration 22 : 1.3367863462628795
Loss in iteration 23 : 1.819959211292118
Loss in iteration 24 : 4.672977508598255
Loss in iteration 25 : 0.9161964187795408
Loss in iteration 26 : 0.9177456278399022
Loss in iteration 27 : 0.9333771235566496
Loss in iteration 28 : 1.2169084043902225
Loss in iteration 29 : 1.4871970898284526
Loss in iteration 30 : 3.494093576866068
Loss in iteration 31 : 1.404487958357718
Loss in iteration 32 : 2.3315830370378454
Loss in iteration 33 : 1.5697861288440496
Loss in iteration 34 : 2.3858957847815043
Loss in iteration 35 : 1.410495257660744
Loss in iteration 36 : 1.8452980235143253
Loss in iteration 37 : 1.2243592765165154
Loss in iteration 38 : 1.4472669468009736
Loss in iteration 39 : 1.1610569321809385
Loss in iteration 40 : 1.4189272053771314
Loss in iteration 41 : 1.2235691003514222
Loss in iteration 42 : 1.7906801113421422
Loss in iteration 43 : 1.374889208315624
Loss in iteration 44 : 2.154069207452467
Loss in iteration 45 : 1.4495532272841034
Loss in iteration 46 : 2.1859696254781857
Loss in iteration 47 : 1.3347462383074173
Loss in iteration 48 : 1.7574731891807525
Loss in iteration 49 : 1.1911664756544353
Loss in iteration 50 : 1.4296192129253966
Loss in iteration 51 : 1.160614755542689
Loss in iteration 52 : 1.5074377277077688
Loss in iteration 53 : 1.2743269709264051
Loss in iteration 54 : 1.9277273023259978
Loss in iteration 55 : 1.4020687782575205
Loss in iteration 56 : 2.162540862600021
Loss in iteration 57 : 1.3550895794940196
Loss in iteration 58 : 1.8664434234348055
Loss in iteration 59 : 1.209296236052702
Loss in iteration 60 : 1.504108630808615
Loss in iteration 61 : 1.1654927200036336
Loss in iteration 62 : 1.5072008919202917
Loss in iteration 63 : 1.2381586072253068
Loss in iteration 64 : 1.7921063791768757
Loss in iteration 65 : 1.2944755876887382
Loss in iteration 66 : 1.9136378106355638
Loss in iteration 67 : 1.2980010073955375
Loss in iteration 68 : 1.847458310948695
Loss in iteration 69 : 1.2165301554808985
Loss in iteration 70 : 1.6266793231012568
Loss in iteration 71 : 1.1808123816109801
Loss in iteration 72 : 1.570176881430482
Loss in iteration 73 : 1.192739140724657
Loss in iteration 74 : 1.713178759529402
Loss in iteration 75 : 1.2710381872395877
Loss in iteration 76 : 1.8704244930311513
Loss in iteration 77 : 1.2619674874168283
Loss in iteration 78 : 1.7707591734638866
Loss in iteration 79 : 1.2076034694552644
Loss in iteration 80 : 1.662520141491675
Loss in iteration 81 : 1.1598297034018903
Loss in iteration 82 : 1.5375230197554792
Loss in iteration 83 : 1.1739023350553395
Loss in iteration 84 : 1.6925587895134175
Loss in iteration 85 : 1.249487793434747
Loss in iteration 86 : 1.8418209696420034
Loss in iteration 87 : 1.2331371200640444
Loss in iteration 88 : 1.7308029163130823
Loss in iteration 89 : 1.2001622399038274
Loss in iteration 90 : 1.6482497108924319
Loss in iteration 91 : 1.1490147433365554
Loss in iteration 92 : 1.5217431956832677
Loss in iteration 93 : 1.1557611875025324
Loss in iteration 94 : 1.6196895323205425
Loss in iteration 95 : 1.1909104576136615
Loss in iteration 96 : 1.7401556124587525
Loss in iteration 97 : 1.2113618228023881
Loss in iteration 98 : 1.7496479396898699
Loss in iteration 99 : 1.1856500984249245
Loss in iteration 100 : 1.6495646202333958
Loss in iteration 101 : 1.158316785829169
Loss in iteration 102 : 1.581323579688829
Loss in iteration 103 : 1.1542012476735366
Loss in iteration 104 : 1.6126827346709205
Loss in iteration 105 : 1.1637217990937758
Loss in iteration 106 : 1.6596470242541972
Loss in iteration 107 : 1.1864128142934403
Loss in iteration 108 : 1.7072608413299195
Loss in iteration 109 : 1.1746448707758306
Loss in iteration 110 : 1.6547099078193952
Loss in iteration 111 : 1.141005884432402
Loss in iteration 112 : 1.5148378915156615
Loss in iteration 113 : 1.1094122267644209
Loss in iteration 114 : 1.4957522886489925
Loss in iteration 115 : 1.130886077350902
Loss in iteration 116 : 1.6361597155609882
Loss in iteration 117 : 1.1916209909222082
Loss in iteration 118 : 1.7642632777385197
Loss in iteration 119 : 1.1654624231680937
Loss in iteration 120 : 1.6362354727237078
Loss in iteration 121 : 1.1409707621160772
Loss in iteration 122 : 1.554802832790893
Loss in iteration 123 : 1.127397611792669
Loss in iteration 124 : 1.5480641224649307
Loss in iteration 125 : 1.1398452211266246
Loss in iteration 126 : 1.6059364803693106
Loss in iteration 127 : 1.1444929349091653
Loss in iteration 128 : 1.6205729462506109
Loss in iteration 129 : 1.1303489302916117
Loss in iteration 130 : 1.5499575965093118
Loss in iteration 131 : 1.1234610228263315
Loss in iteration 132 : 1.550527573932815
Loss in iteration 133 : 1.1277684078991672
Loss in iteration 134 : 1.5702788612663232
Loss in iteration 135 : 1.1242164594393615
Loss in iteration 136 : 1.5600757523012116
Loss in iteration 137 : 1.1232986181207596
Loss in iteration 138 : 1.5691077994459184
Loss in iteration 139 : 1.1210889635354986
Loss in iteration 140 : 1.560320213014698
Loss in iteration 141 : 1.111059770298318
Loss in iteration 142 : 1.5275187323168826
Loss in iteration 143 : 1.1259348477000062
Loss in iteration 144 : 1.5919410368114102
Loss in iteration 145 : 1.1126076163444785
Loss in iteration 146 : 1.545666444569675
Loss in iteration 147 : 1.1044435139512645
Loss in iteration 148 : 1.5121228246971314
Loss in iteration 149 : 1.112248868207994
Loss in iteration 150 : 1.5466209471226975
Loss in iteration 151 : 1.1035297621587847
Loss in iteration 152 : 1.5387171322433768
Loss in iteration 153 : 1.09791036227337
Loss in iteration 154 : 1.5114190178378868
Loss in iteration 155 : 1.1080321127356212
Loss in iteration 156 : 1.5542669964866915
Loss in iteration 157 : 1.100355768980036
Loss in iteration 158 : 1.5334728085440505
Loss in iteration 159 : 1.090058431176674
Loss in iteration 160 : 1.492128325614252
Loss in iteration 161 : 1.0962682020005785
Loss in iteration 162 : 1.5469035125105548
Loss in iteration 163 : 1.0854592542943564
Loss in iteration 164 : 1.4931680723427991
Loss in iteration 165 : 1.092452241797286
Loss in iteration 166 : 1.5341746500704028
Loss in iteration 167 : 1.0825311276302507
Loss in iteration 168 : 1.4873460161574819
Loss in iteration 169 : 1.0878303334258217
Loss in iteration 170 : 1.5223473355919586
Loss in iteration 171 : 1.080253146173865
Loss in iteration 172 : 1.4905376083859105
Loss in iteration 173 : 1.085075197307882
Loss in iteration 174 : 1.5165469977178683
Loss in iteration 175 : 1.06446612227227
Loss in iteration 176 : 1.4532863524938024
Loss in iteration 177 : 1.070098994229518
Loss in iteration 178 : 1.4916771666120356
Loss in iteration 179 : 1.0729691208640386
Loss in iteration 180 : 1.5102413534670671
Loss in iteration 181 : 1.0704131771621932
Loss in iteration 182 : 1.4912820581527424
Loss in iteration 183 : 1.0633991282112984
Loss in iteration 184 : 1.4754586093644315
Loss in iteration 185 : 1.0703041110930285
Loss in iteration 186 : 1.507392005213362
Loss in iteration 187 : 1.0584263230714586
Loss in iteration 188 : 1.4624202409960574
Loss in iteration 189 : 1.0715405813201886
Loss in iteration 190 : 1.5112887106702222
Loss in iteration 191 : 1.0451181019499851
Loss in iteration 192 : 1.4169061967037089
Loss in iteration 193 : 1.0407694399673435
Loss in iteration 194 : 1.4402276882619285
Loss in iteration 195 : 1.0560696224725947
Loss in iteration 196 : 1.507932991248253
Loss in iteration 197 : 1.0561408978498168
Loss in iteration 198 : 1.4802218718506328
Loss in iteration 199 : 1.0445700138501222
Loss in iteration 200 : 1.4422390410850796
Testing accuracy  of updater 3 on alg 1 with rate 1.372 = 0.7865, training accuracy 0.7843962447393978, time elapsed: 2388 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 8.102057142901115
Loss in iteration 3 : 5.570358224942099
Loss in iteration 4 : 3.04648685574063
Loss in iteration 5 : 1.0331150116600079
Loss in iteration 6 : 0.7354071959268954
Loss in iteration 7 : 0.7161933329909128
Loss in iteration 8 : 0.6973376580688514
Loss in iteration 9 : 0.6785980715146434
Loss in iteration 10 : 0.6600357314987549
Loss in iteration 11 : 0.6416704052543089
Loss in iteration 12 : 0.6236431175908397
Loss in iteration 13 : 0.6057506083454088
Loss in iteration 14 : 0.5882695024533945
Loss in iteration 15 : 0.5711672712749879
Loss in iteration 16 : 0.5543213088287606
Loss in iteration 17 : 0.5379539029982947
Loss in iteration 18 : 0.5221451699546575
Loss in iteration 19 : 0.50729463213413
Loss in iteration 20 : 0.4956902905531235
Loss in iteration 21 : 0.48929294078059915
Loss in iteration 22 : 0.5036323253122768
Loss in iteration 23 : 0.6237603067212955
Loss in iteration 24 : 1.2992474859521237
Loss in iteration 25 : 1.1980683166708674
Loss in iteration 26 : 2.3706861801927803
Loss in iteration 27 : 0.5534997540696668
Loss in iteration 28 : 0.5457308026254829
Loss in iteration 29 : 0.5468148336455367
Loss in iteration 30 : 0.561781600907887
Loss in iteration 31 : 0.6537359142504582
Loss in iteration 32 : 0.7445304449284093
Loss in iteration 33 : 1.3023270781750638
Loss in iteration 34 : 0.9009445771059121
Loss in iteration 35 : 1.4441983262835982
Loss in iteration 36 : 0.7132753970392611
Loss in iteration 37 : 0.8285912109336367
Loss in iteration 38 : 0.6816828872513402
Loss in iteration 39 : 0.7802480559342083
Loss in iteration 40 : 0.6878100778404909
Loss in iteration 41 : 0.8558418550228841
Loss in iteration 42 : 0.7288455551833185
Loss in iteration 43 : 0.9300966725343394
Loss in iteration 44 : 0.7297905570229447
Loss in iteration 45 : 0.9265615150799039
Loss in iteration 46 : 0.719947899202207
Loss in iteration 47 : 0.9104697313506613
Loss in iteration 48 : 0.7201529488475328
Loss in iteration 49 : 0.9017377119256803
Loss in iteration 50 : 0.7193961818734931
Loss in iteration 51 : 0.8988763100924312
Loss in iteration 52 : 0.7136766476006507
Loss in iteration 53 : 0.8879030949677258
Loss in iteration 54 : 0.7085125672345824
Loss in iteration 55 : 0.8799903923548074
Loss in iteration 56 : 0.7050777052147247
Loss in iteration 57 : 0.8745753281929463
Loss in iteration 58 : 0.7023486702644754
Loss in iteration 59 : 0.8761353754004135
Loss in iteration 60 : 0.7055414886231802
Loss in iteration 61 : 0.8780784892169144
Loss in iteration 62 : 0.709911066530148
Loss in iteration 63 : 0.8917231498955445
Loss in iteration 64 : 0.7055701923548597
Loss in iteration 65 : 0.8779985341331468
Loss in iteration 66 : 0.700929333743404
Loss in iteration 67 : 0.8736447054790392
Loss in iteration 68 : 0.697339681046851
Loss in iteration 69 : 0.8641905555461246
Loss in iteration 70 : 0.6959197313080655
Loss in iteration 71 : 0.8656513896604128
Loss in iteration 72 : 0.7004198292167354
Loss in iteration 73 : 0.8824915605559156
Loss in iteration 74 : 0.6947759998816275
Loss in iteration 75 : 0.8603522581046004
Loss in iteration 76 : 0.7008757197447322
Loss in iteration 77 : 0.8841047533532619
Loss in iteration 78 : 0.7013757732210022
Loss in iteration 79 : 0.8820028518954821
Loss in iteration 80 : 0.6920955984814541
Loss in iteration 81 : 0.8504337230355747
Loss in iteration 82 : 0.6866018232960623
Loss in iteration 83 : 0.8513865017431614
Loss in iteration 84 : 0.6856356213902561
Loss in iteration 85 : 0.858323482242797
Loss in iteration 86 : 0.6926347724033798
Loss in iteration 87 : 0.8730786974697289
Loss in iteration 88 : 0.6909859085782274
Loss in iteration 89 : 0.8641956348074423
Loss in iteration 90 : 0.6895670402922894
Loss in iteration 91 : 0.8628751158984141
Loss in iteration 92 : 0.6886813605269361
Loss in iteration 93 : 0.8582466961435922
Loss in iteration 94 : 0.6891496842033502
Loss in iteration 95 : 0.8566897527634657
Loss in iteration 96 : 0.6885444796911848
Loss in iteration 97 : 0.8551398760490844
Loss in iteration 98 : 0.6864314934674245
Loss in iteration 99 : 0.855109361976591
Loss in iteration 100 : 0.6843308192675618
Loss in iteration 101 : 0.8515944970744532
Loss in iteration 102 : 0.6841624848751141
Loss in iteration 103 : 0.8524920539768573
Loss in iteration 104 : 0.6827395928287308
Loss in iteration 105 : 0.8493573777638063
Loss in iteration 106 : 0.679928627908343
Loss in iteration 107 : 0.8441921378141579
Loss in iteration 108 : 0.6745336639168779
Loss in iteration 109 : 0.8409957569165633
Loss in iteration 110 : 0.6757265588455843
Loss in iteration 111 : 0.8421990624408804
Loss in iteration 112 : 0.6761956076376477
Loss in iteration 113 : 0.8480485940942737
Loss in iteration 114 : 0.68352589206572
Loss in iteration 115 : 0.8583837059749084
Loss in iteration 116 : 0.676228150748705
Loss in iteration 117 : 0.84185445950522
Loss in iteration 118 : 0.6689058666719293
Loss in iteration 119 : 0.8280595777199306
Loss in iteration 120 : 0.6665483058530647
Loss in iteration 121 : 0.8261902132871287
Loss in iteration 122 : 0.670044967099883
Loss in iteration 123 : 0.8381126972052122
Loss in iteration 124 : 0.6770464177349033
Loss in iteration 125 : 0.8620072821002357
Loss in iteration 126 : 0.6711097681305
Loss in iteration 127 : 0.83325192023049
Loss in iteration 128 : 0.6699687294682489
Loss in iteration 129 : 0.8325683680741158
Loss in iteration 130 : 0.6688385821436869
Loss in iteration 131 : 0.8324259654905881
Loss in iteration 132 : 0.6678835008435625
Loss in iteration 133 : 0.8341368803440369
Loss in iteration 134 : 0.6659653592491045
Loss in iteration 135 : 0.8318438533632679
Loss in iteration 136 : 0.6649149032279834
Loss in iteration 137 : 0.8336806741415753
Loss in iteration 138 : 0.6685474488015769
Loss in iteration 139 : 0.8416152734654452
Loss in iteration 140 : 0.6672334891444378
Loss in iteration 141 : 0.8358516085659455
Loss in iteration 142 : 0.662846215386351
Loss in iteration 143 : 0.8139633385075354
Loss in iteration 144 : 0.6542568500100054
Loss in iteration 145 : 0.8078658942138703
Loss in iteration 146 : 0.6546614085926077
Loss in iteration 147 : 0.8131892090508643
Loss in iteration 148 : 0.6610825707771378
Loss in iteration 149 : 0.8414872650950993
Loss in iteration 150 : 0.6644363018668543
Loss in iteration 151 : 0.8394752728633154
Loss in iteration 152 : 0.6645966541868978
Loss in iteration 153 : 0.8308073230119547
Loss in iteration 154 : 0.657780116865392
Loss in iteration 155 : 0.8176498787445519
Loss in iteration 156 : 0.6533153422284121
Loss in iteration 157 : 0.7987459578175379
Loss in iteration 158 : 0.6483140652319178
Loss in iteration 159 : 0.792850921279608
Loss in iteration 160 : 0.6513052538583338
Loss in iteration 161 : 0.8225824050306065
Loss in iteration 162 : 0.6654507063052784
Loss in iteration 163 : 0.8519790267988319
Loss in iteration 164 : 0.6559111564000009
Loss in iteration 165 : 0.8157385353831117
Loss in iteration 166 : 0.6519900593362695
Loss in iteration 167 : 0.8019752931517722
Loss in iteration 168 : 0.6452933853759375
Loss in iteration 169 : 0.7924947984691609
Loss in iteration 170 : 0.6476628180312055
Loss in iteration 171 : 0.8039099471971914
Loss in iteration 172 : 0.6515501701189623
Loss in iteration 173 : 0.8224721566924745
Loss in iteration 174 : 0.6592949655594957
Loss in iteration 175 : 0.8368947931741973
Loss in iteration 176 : 0.6489782196566268
Loss in iteration 177 : 0.7988547835653488
Loss in iteration 178 : 0.6410771473322591
Loss in iteration 179 : 0.7889142890599921
Loss in iteration 180 : 0.641117877183826
Loss in iteration 181 : 0.7941340407164497
Loss in iteration 182 : 0.645882898833804
Loss in iteration 183 : 0.8097787491227594
Loss in iteration 184 : 0.6605600029272146
Loss in iteration 185 : 0.8494477517597283
Loss in iteration 186 : 0.6416094429277778
Loss in iteration 187 : 0.7871717024030862
Loss in iteration 188 : 0.6349579880217101
Loss in iteration 189 : 0.7784415607997719
Loss in iteration 190 : 0.6405906329374903
Loss in iteration 191 : 0.8028221876070066
Loss in iteration 192 : 0.6448216848566913
Loss in iteration 193 : 0.8082655833609063
Loss in iteration 194 : 0.6519416943983758
Loss in iteration 195 : 0.8352968377688271
Loss in iteration 196 : 0.6366709955092595
Loss in iteration 197 : 0.7758383386636654
Loss in iteration 198 : 0.633267431438684
Loss in iteration 199 : 0.7755833389796393
Loss in iteration 200 : 0.6420211075335108
Testing accuracy  of updater 3 on alg 1 with rate 0.784 = 0.7045, training accuracy 0.7831013272903853, time elapsed: 2421 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.958608594937329
Loss in iteration 3 : 1.4367606306260987
Loss in iteration 4 : 0.9159124849685826
Loss in iteration 5 : 0.4398676591000665
Loss in iteration 6 : 0.40089689942923734
Loss in iteration 7 : 0.3926301703716335
Loss in iteration 8 : 0.3828348555224128
Loss in iteration 9 : 0.3799673020282342
Loss in iteration 10 : 0.37744328265753674
Loss in iteration 11 : 0.37642244900615535
Loss in iteration 12 : 0.37546474910736144
Loss in iteration 13 : 0.3749708488156844
Loss in iteration 14 : 0.37458481678915234
Loss in iteration 15 : 0.374181473308723
Loss in iteration 16 : 0.3738315509454418
Loss in iteration 17 : 0.37353853792521996
Loss in iteration 18 : 0.37325393061346696
Loss in iteration 19 : 0.37300651944744595
Loss in iteration 20 : 0.3727958348165726
Loss in iteration 21 : 0.37249648705153926
Loss in iteration 22 : 0.37230828867588656
Loss in iteration 23 : 0.37207255845169984
Loss in iteration 24 : 0.3718904827352723
Loss in iteration 25 : 0.37173905493339715
Loss in iteration 26 : 0.3715794139458934
Loss in iteration 27 : 0.37141818149051425
Loss in iteration 28 : 0.37128280517415585
Loss in iteration 29 : 0.37111819412740404
Loss in iteration 30 : 0.37100037510075295
Loss in iteration 31 : 0.3708768449701677
Loss in iteration 32 : 0.3707859474829095
Loss in iteration 33 : 0.3706915456215643
Loss in iteration 34 : 0.37068771471544043
Loss in iteration 35 : 0.370537126643165
Loss in iteration 36 : 0.37055376472361407
Loss in iteration 37 : 0.3704632035964003
Loss in iteration 38 : 0.3706261828089959
Loss in iteration 39 : 0.3706341529080923
Loss in iteration 40 : 0.3711643683410535
Loss in iteration 41 : 0.37110725446369164
Loss in iteration 42 : 0.3719127781864452
Loss in iteration 43 : 0.3718988877718814
Loss in iteration 44 : 0.37247518202020574
Loss in iteration 45 : 0.37264165212334305
Loss in iteration 46 : 0.37347777897868684
Loss in iteration 47 : 0.37392744286693264
Loss in iteration 48 : 0.3739498392412895
Loss in iteration 49 : 0.37461547529762484
Loss in iteration 50 : 0.373877646359289
Loss in iteration 51 : 0.3744195110109005
Loss in iteration 52 : 0.37374816378592557
Loss in iteration 53 : 0.37422373141972526
Loss in iteration 54 : 0.3736196349906578
Loss in iteration 55 : 0.37398594909981636
Loss in iteration 56 : 0.3734541753296287
Loss in iteration 57 : 0.3739661966541267
Loss in iteration 58 : 0.37317616411916393
Loss in iteration 59 : 0.37343054110237545
Loss in iteration 60 : 0.3728766598821991
Loss in iteration 61 : 0.3732175983444665
Loss in iteration 62 : 0.3728104422658335
Loss in iteration 63 : 0.37315996735821083
Loss in iteration 64 : 0.37258946453697167
Loss in iteration 65 : 0.372950344201793
Loss in iteration 66 : 0.37263150579891197
Loss in iteration 67 : 0.37288580098365115
Loss in iteration 68 : 0.37247277778779697
Loss in iteration 69 : 0.372490346270161
Loss in iteration 70 : 0.37189314370135823
Loss in iteration 71 : 0.37237144198815253
Loss in iteration 72 : 0.37176774472845253
Loss in iteration 73 : 0.37211097329771464
Loss in iteration 74 : 0.3716605377012565
Loss in iteration 75 : 0.371867161575621
Loss in iteration 76 : 0.37146335409722986
Loss in iteration 77 : 0.37183125139514656
Loss in iteration 78 : 0.37210429722652666
Loss in iteration 79 : 0.37283593011469096
Loss in iteration 80 : 0.3731629245060677
Loss in iteration 81 : 0.37416643823507745
Loss in iteration 82 : 0.37402552898126445
Loss in iteration 83 : 0.37454483010720824
Loss in iteration 84 : 0.3738466553036933
Loss in iteration 85 : 0.3745622377000482
Loss in iteration 86 : 0.37379078695209456
Loss in iteration 87 : 0.37441765915432923
Loss in iteration 88 : 0.373731932244166
Loss in iteration 89 : 0.3742808047329871
Loss in iteration 90 : 0.3734464860759031
Loss in iteration 91 : 0.3743726439921817
Loss in iteration 92 : 0.3737305763906092
Loss in iteration 93 : 0.3749196089774069
Loss in iteration 94 : 0.37403436246135574
Loss in iteration 95 : 0.3752597461824475
Loss in iteration 96 : 0.37447736203150966
Loss in iteration 97 : 0.3754494223211921
Loss in iteration 98 : 0.37433624594838594
Loss in iteration 99 : 0.3751257658858415
Loss in iteration 100 : 0.3734167693565903
Loss in iteration 101 : 0.3744219904867159
Loss in iteration 102 : 0.3734030406466427
Loss in iteration 103 : 0.3742598783393896
Loss in iteration 104 : 0.3732269957922236
Loss in iteration 105 : 0.37390353518139136
Loss in iteration 106 : 0.37263753715319914
Loss in iteration 107 : 0.3734721311694318
Loss in iteration 108 : 0.372613969572005
Loss in iteration 109 : 0.3736081573412892
Loss in iteration 110 : 0.3723163378766587
Loss in iteration 111 : 0.37277358669905847
Loss in iteration 112 : 0.3714825304953756
Loss in iteration 113 : 0.37143947382330555
Loss in iteration 114 : 0.37095864182601207
Loss in iteration 115 : 0.37075803492989695
Loss in iteration 116 : 0.37094930999376935
Loss in iteration 117 : 0.3706203534795936
Loss in iteration 118 : 0.3712139975222499
Loss in iteration 119 : 0.37130289505652664
Loss in iteration 120 : 0.37090096654803856
Loss in iteration 121 : 0.3708162263521875
Loss in iteration 122 : 0.3710257325445072
Loss in iteration 123 : 0.3709906830715205
Loss in iteration 124 : 0.3708184155315109
Loss in iteration 125 : 0.3705427416120724
Loss in iteration 126 : 0.3710330689660461
Loss in iteration 127 : 0.37102215306395614
Loss in iteration 128 : 0.3709376020923717
Loss in iteration 129 : 0.370892987498207
Loss in iteration 130 : 0.3708330399892146
Loss in iteration 131 : 0.3707174631847397
Loss in iteration 132 : 0.3708849396726297
Loss in iteration 133 : 0.3705423429546253
Loss in iteration 134 : 0.37083654016403256
Loss in iteration 135 : 0.37046938666613666
Loss in iteration 136 : 0.37078815885507665
Loss in iteration 137 : 0.3702202163134686
Loss in iteration 138 : 0.37077027657101336
Loss in iteration 139 : 0.3702948719077754
Loss in iteration 140 : 0.3707873514160992
Loss in iteration 141 : 0.37052081799695624
Loss in iteration 142 : 0.37074052666165236
Loss in iteration 143 : 0.3706328577544739
Loss in iteration 144 : 0.37051680056249037
Loss in iteration 145 : 0.36990008362777826
Loss in iteration 146 : 0.3704736478080912
Loss in iteration 147 : 0.3702623438012644
Loss in iteration 148 : 0.3705196121944769
Loss in iteration 149 : 0.3705258833629611
Loss in iteration 150 : 0.3703000837133313
Loss in iteration 151 : 0.37012060305807254
Loss in iteration 152 : 0.3700588420759499
Loss in iteration 153 : 0.3699893611460801
Loss in iteration 154 : 0.36990321029288237
Loss in iteration 155 : 0.3699099398115474
Loss in iteration 156 : 0.3697001305687346
Loss in iteration 157 : 0.36974386846403456
Loss in iteration 158 : 0.36959893313468856
Loss in iteration 159 : 0.3694417974744952
Loss in iteration 160 : 0.3693986452924439
Loss in iteration 161 : 0.36947556477291627
Loss in iteration 162 : 0.3692757940790747
Loss in iteration 163 : 0.3693451611428851
Loss in iteration 164 : 0.36931766653812376
Loss in iteration 165 : 0.36958293516785534
Loss in iteration 166 : 0.3691364762366207
Loss in iteration 167 : 0.36891045442717707
Loss in iteration 168 : 0.3692168079369531
Loss in iteration 169 : 0.3697970098352384
Loss in iteration 170 : 0.3691517884996174
Loss in iteration 171 : 0.3694528735971103
Loss in iteration 172 : 0.3690118652535398
Loss in iteration 173 : 0.36932418380763843
Loss in iteration 174 : 0.3690599279277188
Loss in iteration 175 : 0.3693833460083062
Loss in iteration 176 : 0.36906362672407056
Loss in iteration 177 : 0.3693717278074649
Loss in iteration 178 : 0.36911794717711294
Loss in iteration 179 : 0.36912089540387183
Loss in iteration 180 : 0.36889853124685545
Loss in iteration 181 : 0.36916292189491706
Loss in iteration 182 : 0.36905102250476757
Loss in iteration 183 : 0.369417179017924
Loss in iteration 184 : 0.3688499798079633
Loss in iteration 185 : 0.3690663667903169
Loss in iteration 186 : 0.36859069075443956
Loss in iteration 187 : 0.36889560895155815
Loss in iteration 188 : 0.3685551529974973
Loss in iteration 189 : 0.3689317841630834
Loss in iteration 190 : 0.3684511627579397
Loss in iteration 191 : 0.3689516730359309
Loss in iteration 192 : 0.36836393591112
Loss in iteration 193 : 0.36880997011821576
Loss in iteration 194 : 0.36844029423512015
Loss in iteration 195 : 0.3691206939834848
Loss in iteration 196 : 0.36816937196925414
Loss in iteration 197 : 0.3688699096345262
Loss in iteration 198 : 0.3683565529168006
Loss in iteration 199 : 0.3690735641733334
Loss in iteration 200 : 0.36809098549551017
Testing accuracy  of updater 3 on alg 1 with rate 0.196 = 0.77875, training accuracy 0.8397539656846876, time elapsed: 2055 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.4280255430411282
Loss in iteration 3 : 1.0747007256945456
Loss in iteration 4 : 0.7222730503580578
Loss in iteration 5 : 0.4148678952997656
Loss in iteration 6 : 0.39828298699359477
Loss in iteration 7 : 0.3924972229424621
Loss in iteration 8 : 0.38645330570486264
Loss in iteration 9 : 0.3822338743946679
Loss in iteration 10 : 0.3794264016780373
Loss in iteration 11 : 0.3779618687973859
Loss in iteration 12 : 0.3771143088572074
Loss in iteration 13 : 0.37656627060462644
Loss in iteration 14 : 0.3761172562071527
Loss in iteration 15 : 0.37571462582482557
Loss in iteration 16 : 0.3753444438133666
Loss in iteration 17 : 0.3749985707204928
Loss in iteration 18 : 0.3746828283022792
Loss in iteration 19 : 0.3743838642961116
Loss in iteration 20 : 0.37410020173348785
Loss in iteration 21 : 0.37384022007580064
Loss in iteration 22 : 0.3735902687757762
Loss in iteration 23 : 0.3733527471760865
Loss in iteration 24 : 0.37313896510560374
Loss in iteration 25 : 0.37294973489684957
Loss in iteration 26 : 0.3727728239792999
Loss in iteration 27 : 0.3726158169423989
Loss in iteration 28 : 0.37247296548651976
Loss in iteration 29 : 0.3723663593278216
Loss in iteration 30 : 0.37225083277956383
Loss in iteration 31 : 0.3721550138144182
Loss in iteration 32 : 0.3720108160285738
Loss in iteration 33 : 0.37189951718427944
Loss in iteration 34 : 0.37179528425155506
Loss in iteration 35 : 0.37170418940927735
Loss in iteration 36 : 0.3716165292093995
Loss in iteration 37 : 0.37154270895602753
Loss in iteration 38 : 0.37143202050872764
Loss in iteration 39 : 0.3713429675299973
Loss in iteration 40 : 0.3712601721200681
Loss in iteration 41 : 0.37117854003514783
Loss in iteration 42 : 0.37109954121998434
Loss in iteration 43 : 0.37102107944885043
Loss in iteration 44 : 0.37094313048617783
Loss in iteration 45 : 0.3708661645162615
Loss in iteration 46 : 0.3707898069883032
Loss in iteration 47 : 0.3707137048721486
Loss in iteration 48 : 0.37063816826659457
Loss in iteration 49 : 0.3705631271486371
Loss in iteration 50 : 0.3704908801666585
Loss in iteration 51 : 0.3704191158450756
Loss in iteration 52 : 0.37035095293909387
Loss in iteration 53 : 0.3702836379804691
Loss in iteration 54 : 0.37021655051604846
Loss in iteration 55 : 0.37014967484820493
Loss in iteration 56 : 0.37008334380011665
Loss in iteration 57 : 0.3700195462589044
Loss in iteration 58 : 0.3699533334810454
Loss in iteration 59 : 0.3698874385335115
Loss in iteration 60 : 0.369823742396638
Loss in iteration 61 : 0.3697604113197574
Loss in iteration 62 : 0.36969710175323856
Loss in iteration 63 : 0.3696342629156388
Loss in iteration 64 : 0.3695730360907023
Loss in iteration 65 : 0.3695117949799219
Loss in iteration 66 : 0.36945106443302794
Loss in iteration 67 : 0.36939219045065796
Loss in iteration 68 : 0.36933510948888654
Loss in iteration 69 : 0.3692783234047924
Loss in iteration 70 : 0.3692217659546979
Loss in iteration 71 : 0.3691662906487286
Loss in iteration 72 : 0.3691111188645998
Loss in iteration 73 : 0.3690559239658629
Loss in iteration 74 : 0.3690005430049155
Loss in iteration 75 : 0.36894562746114457
Loss in iteration 76 : 0.36889135070458484
Loss in iteration 77 : 0.36883716708964837
Loss in iteration 78 : 0.3687840658757418
Loss in iteration 79 : 0.36873048306298634
Loss in iteration 80 : 0.36867848809229914
Loss in iteration 81 : 0.36863036131415927
Loss in iteration 82 : 0.36857789934228935
Loss in iteration 83 : 0.36852953639099023
Loss in iteration 84 : 0.36847946268698956
Loss in iteration 85 : 0.3684317220611687
Loss in iteration 86 : 0.3683865623705497
Loss in iteration 87 : 0.3683387442070652
Loss in iteration 88 : 0.36829278403858556
Loss in iteration 89 : 0.3682439574525472
Loss in iteration 90 : 0.3682021310016402
Loss in iteration 91 : 0.36815653989341574
Loss in iteration 92 : 0.36811958387479965
Loss in iteration 93 : 0.3680759940678232
Loss in iteration 94 : 0.368031966725371
Loss in iteration 95 : 0.36798781867852975
Loss in iteration 96 : 0.36794468689096815
Loss in iteration 97 : 0.3679040203422026
Loss in iteration 98 : 0.36785962494910013
Loss in iteration 99 : 0.3678079456414092
Loss in iteration 100 : 0.367764988561972
Loss in iteration 101 : 0.3677253584865785
Loss in iteration 102 : 0.36768490048659475
Loss in iteration 103 : 0.3676453983154432
Loss in iteration 104 : 0.36760618495543107
Loss in iteration 105 : 0.3675676493034293
Loss in iteration 106 : 0.36752960801498463
Loss in iteration 107 : 0.36749219245813514
Loss in iteration 108 : 0.36745478625765093
Loss in iteration 109 : 0.36741738940645224
Loss in iteration 110 : 0.3673800018974679
Loss in iteration 111 : 0.367342796531136
Loss in iteration 112 : 0.3673064879248428
Loss in iteration 113 : 0.3672701072371113
Loss in iteration 114 : 0.3672338829656275
Loss in iteration 115 : 0.3671975878059628
Loss in iteration 116 : 0.3671618042311028
Loss in iteration 117 : 0.3671300345199293
Loss in iteration 118 : 0.3670959305568271
Loss in iteration 119 : 0.3670627030446351
Loss in iteration 120 : 0.3670326054395203
Loss in iteration 121 : 0.36700600431672786
Loss in iteration 122 : 0.366969490563143
Loss in iteration 123 : 0.36693384528150885
Loss in iteration 124 : 0.3669018388359535
Loss in iteration 125 : 0.3668706624556814
Loss in iteration 126 : 0.3668424906572065
Loss in iteration 127 : 0.3668123264274089
Loss in iteration 128 : 0.3667805985761733
Loss in iteration 129 : 0.3667472130327509
Loss in iteration 130 : 0.3667159843570243
Loss in iteration 131 : 0.3666879221868865
Loss in iteration 132 : 0.3666580855113943
Loss in iteration 133 : 0.3666294874858017
Loss in iteration 134 : 0.3666002084967348
Loss in iteration 135 : 0.36657107534529226
Loss in iteration 136 : 0.3665423532876754
Loss in iteration 137 : 0.3665132523974298
Loss in iteration 138 : 0.3664846315967737
Loss in iteration 139 : 0.3664564244001212
Loss in iteration 140 : 0.366427861819821
Loss in iteration 141 : 0.3664003637874739
Loss in iteration 142 : 0.3663738112538831
Loss in iteration 143 : 0.36634373682155863
Loss in iteration 144 : 0.36631641811516175
Loss in iteration 145 : 0.36628890869939584
Loss in iteration 146 : 0.36626177976378727
Loss in iteration 147 : 0.36623508850629194
Loss in iteration 148 : 0.3662080819484426
Loss in iteration 149 : 0.3661811424144773
Loss in iteration 150 : 0.3661543560420909
Loss in iteration 151 : 0.36612757436477633
Loss in iteration 152 : 0.3661007973800292
Loss in iteration 153 : 0.3660741321609638
Loss in iteration 154 : 0.3660522841550144
Loss in iteration 155 : 0.3660247055231562
Loss in iteration 156 : 0.36599780770624485
Loss in iteration 157 : 0.36597262795549784
Loss in iteration 158 : 0.3659478750145965
Loss in iteration 159 : 0.3659231260900875
Loss in iteration 160 : 0.365898381179992
Loss in iteration 161 : 0.36587364028232777
Loss in iteration 162 : 0.3658493675761761
Loss in iteration 163 : 0.3658248321846159
Loss in iteration 164 : 0.36580040299939703
Loss in iteration 165 : 0.36577605648096106
Loss in iteration 166 : 0.3657515329054879
Loss in iteration 167 : 0.36572742128532215
Loss in iteration 168 : 0.36570322123688204
Loss in iteration 169 : 0.36567912100919975
Loss in iteration 170 : 0.36565503922941706
Loss in iteration 171 : 0.3656310476353143
Loss in iteration 172 : 0.36560760425900646
Loss in iteration 173 : 0.36558608058609676
Loss in iteration 174 : 0.3655630649951389
Loss in iteration 175 : 0.36553919126269996
Loss in iteration 176 : 0.3655178585306882
Loss in iteration 177 : 0.36549376939359424
Loss in iteration 178 : 0.3654717504136826
Loss in iteration 179 : 0.3654500091795514
Loss in iteration 180 : 0.36542832020527316
Loss in iteration 181 : 0.3654069601105982
Loss in iteration 182 : 0.3653859474651563
Loss in iteration 183 : 0.3653649377238383
Loss in iteration 184 : 0.36534393088542866
Loss in iteration 185 : 0.3653229269487122
Loss in iteration 186 : 0.3653019259124736
Loss in iteration 187 : 0.36528126326331856
Loss in iteration 188 : 0.3652609151855368
Loss in iteration 189 : 0.3652414552780695
Loss in iteration 190 : 0.36522206908074095
Loss in iteration 191 : 0.3652025861588283
Loss in iteration 192 : 0.3651838709896361
Loss in iteration 193 : 0.36516442008449945
Loss in iteration 194 : 0.3651452842021959
Loss in iteration 195 : 0.36512632768357595
Loss in iteration 196 : 0.3651085323341793
Loss in iteration 197 : 0.36509003835555043
Loss in iteration 198 : 0.36507333251393087
Loss in iteration 199 : 0.36505590547127614
Loss in iteration 200 : 0.36503689220416374
Testing accuracy  of updater 3 on alg 1 with rate 0.13720000000000002 = 0.788, training accuracy 0.8416963418582065, time elapsed: 2018 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9483613404798847
Loss in iteration 3 : 0.7505343215361276
Loss in iteration 4 : 0.5565770534443907
Loss in iteration 5 : 0.4487763758202568
Loss in iteration 6 : 0.4385804872387903
Loss in iteration 7 : 0.4287146335919804
Loss in iteration 8 : 0.4200046343343946
Loss in iteration 9 : 0.41260377478009463
Loss in iteration 10 : 0.406190539338065
Loss in iteration 11 : 0.4010132342119393
Loss in iteration 12 : 0.3968662283701544
Loss in iteration 13 : 0.3937162340106923
Loss in iteration 14 : 0.3911819996598983
Loss in iteration 15 : 0.3891881966008065
Loss in iteration 16 : 0.38753083347308226
Loss in iteration 17 : 0.38614228870572237
Loss in iteration 18 : 0.38499490665294084
Loss in iteration 19 : 0.38394360537446975
Loss in iteration 20 : 0.38295928897163956
Loss in iteration 21 : 0.38203655778020906
Loss in iteration 22 : 0.38123509904222375
Loss in iteration 23 : 0.3805086958781793
Loss in iteration 24 : 0.37988568649120674
Loss in iteration 25 : 0.3793334620452644
Loss in iteration 26 : 0.3788312375956297
Loss in iteration 27 : 0.3783690848388497
Loss in iteration 28 : 0.3779412627650002
Loss in iteration 29 : 0.3775546904508561
Loss in iteration 30 : 0.3771995241623014
Loss in iteration 31 : 0.37687177495459384
Loss in iteration 32 : 0.3765719871736554
Loss in iteration 33 : 0.37629445463842104
Loss in iteration 34 : 0.37602445495996745
Loss in iteration 35 : 0.375758455399854
Loss in iteration 36 : 0.3754965532003971
Loss in iteration 37 : 0.37525022670989083
Loss in iteration 38 : 0.37502840919715286
Loss in iteration 39 : 0.3748205427241628
Loss in iteration 40 : 0.374622313158387
Loss in iteration 41 : 0.3744326368396563
Loss in iteration 42 : 0.3742468390019455
Loss in iteration 43 : 0.3740732850855375
Loss in iteration 44 : 0.3739035320888019
Loss in iteration 45 : 0.3737406255846126
Loss in iteration 46 : 0.37358403363564285
Loss in iteration 47 : 0.3734329309188549
Loss in iteration 48 : 0.3732836702825376
Loss in iteration 49 : 0.37313810950387116
Loss in iteration 50 : 0.3729987992176748
Loss in iteration 51 : 0.37286504812682403
Loss in iteration 52 : 0.3727380788388614
Loss in iteration 53 : 0.3726277604723385
Loss in iteration 54 : 0.37252330751655316
Loss in iteration 55 : 0.3724250496552088
Loss in iteration 56 : 0.37233837736776465
Loss in iteration 57 : 0.3722563185010653
Loss in iteration 58 : 0.3721794447607861
Loss in iteration 59 : 0.37210424268744063
Loss in iteration 60 : 0.37202936045685187
Loss in iteration 61 : 0.37195617249003526
Loss in iteration 62 : 0.37188722668509155
Loss in iteration 63 : 0.3718205844500243
Loss in iteration 64 : 0.3717559455213052
Loss in iteration 65 : 0.3716928215854667
Loss in iteration 66 : 0.3716302814746962
Loss in iteration 67 : 0.3715689393918792
Loss in iteration 68 : 0.37151275335610706
Loss in iteration 69 : 0.3714617109696985
Loss in iteration 70 : 0.37140595281796185
Loss in iteration 71 : 0.37135448959760253
Loss in iteration 72 : 0.37130578088433125
Loss in iteration 73 : 0.3712580718831103
Loss in iteration 74 : 0.37121026057463763
Loss in iteration 75 : 0.37116262482195556
Loss in iteration 76 : 0.3711181863635229
Loss in iteration 77 : 0.3710743802379912
Loss in iteration 78 : 0.37103095743016706
Loss in iteration 79 : 0.3709882668241186
Loss in iteration 80 : 0.3709452489202975
Loss in iteration 81 : 0.3709027230663635
Loss in iteration 82 : 0.37086058861365256
Loss in iteration 83 : 0.37081864327212904
Loss in iteration 84 : 0.37077649754782466
Loss in iteration 85 : 0.37073489653204755
Loss in iteration 86 : 0.37069420336468517
Loss in iteration 87 : 0.37065434501159295
Loss in iteration 88 : 0.3706144975589249
Loss in iteration 89 : 0.37057466099546077
Loss in iteration 90 : 0.370534835309998
Loss in iteration 91 : 0.37049579326858373
Loss in iteration 92 : 0.3704564203971742
Loss in iteration 93 : 0.3704176464233788
Loss in iteration 94 : 0.370378923314379
Loss in iteration 95 : 0.3703402108342335
Loss in iteration 96 : 0.37030207956455574
Loss in iteration 97 : 0.3702643980902205
Loss in iteration 98 : 0.3702285015380558
Loss in iteration 99 : 0.3701942121663054
Loss in iteration 100 : 0.37015877549067483
Loss in iteration 101 : 0.3701243060475112
Loss in iteration 102 : 0.37009002318258083
Loss in iteration 103 : 0.37005617126876034
Loss in iteration 104 : 0.3700223288213844
Loss in iteration 105 : 0.369988495831395
Loss in iteration 106 : 0.3699546856403237
Loss in iteration 107 : 0.36992096876175024
Loss in iteration 108 : 0.36988731687259635
Loss in iteration 109 : 0.36985431308429717
Loss in iteration 110 : 0.36982107993454044
Loss in iteration 111 : 0.36978831331909545
Loss in iteration 112 : 0.3697555556752008
Loss in iteration 113 : 0.36972280699450916
Loss in iteration 114 : 0.36969006726868503
Loss in iteration 115 : 0.36965733648940285
Loss in iteration 116 : 0.36962461464835417
Loss in iteration 117 : 0.3695921303799069
Loss in iteration 118 : 0.36956063876447787
Loss in iteration 119 : 0.36952915560573857
Loss in iteration 120 : 0.36949768089605584
Loss in iteration 121 : 0.36946621462780777
Loss in iteration 122 : 0.369434868333853
Loss in iteration 123 : 0.36940353546482113
Loss in iteration 124 : 0.3693728405036056
Loss in iteration 125 : 0.36934232958640056
Loss in iteration 126 : 0.36931182487114733
Loss in iteration 127 : 0.36928145231382314
Loss in iteration 128 : 0.3692522461765957
Loss in iteration 129 : 0.3692232696111897
Loss in iteration 130 : 0.369195008461498
Loss in iteration 131 : 0.3691666852972592
Loss in iteration 132 : 0.36913861634694967
Loss in iteration 133 : 0.3691105543798596
Loss in iteration 134 : 0.36908249939027354
Loss in iteration 135 : 0.36905445137248283
Loss in iteration 136 : 0.36902641032078537
Loss in iteration 137 : 0.3689983762294882
Loss in iteration 138 : 0.3689703490929064
Loss in iteration 139 : 0.36894264685783285
Loss in iteration 140 : 0.3689151832375372
Loss in iteration 141 : 0.36888801301730156
Loss in iteration 142 : 0.3688608493983314
Loss in iteration 143 : 0.36883394136938824
Loss in iteration 144 : 0.3688069332511948
Loss in iteration 145 : 0.3687798994577293
Loss in iteration 146 : 0.3687539590879797
Loss in iteration 147 : 0.3687280251373535
Loss in iteration 148 : 0.36870209760082195
Loss in iteration 149 : 0.36867617647336154
Loss in iteration 150 : 0.36865026174995735
Loss in iteration 151 : 0.3686243534255992
Loss in iteration 152 : 0.3685985755334105
Loss in iteration 153 : 0.36857346420560066
Loss in iteration 154 : 0.36854870551106594
Loss in iteration 155 : 0.36852395296345963
Loss in iteration 156 : 0.36849920655807283
Loss in iteration 157 : 0.3684746149350731
Loss in iteration 158 : 0.3684499878309129
Loss in iteration 159 : 0.3684252904666131
Loss in iteration 160 : 0.3684009526693266
Loss in iteration 161 : 0.36837645348568643
Loss in iteration 162 : 0.3683521707954262
Loss in iteration 163 : 0.36832813140199955
Loss in iteration 164 : 0.3683048994380688
Loss in iteration 165 : 0.36828039015890524
Loss in iteration 166 : 0.3682568429538687
Loss in iteration 167 : 0.36823358872416534
Loss in iteration 168 : 0.3682109841081785
Loss in iteration 169 : 0.36818988163613564
Loss in iteration 170 : 0.3681674931129023
Loss in iteration 171 : 0.3681436617669718
Loss in iteration 172 : 0.36812068113618135
Loss in iteration 173 : 0.36809857946852026
Loss in iteration 174 : 0.3680764827599305
Loss in iteration 175 : 0.3680547900475779
Loss in iteration 176 : 0.36803316143325904
Loss in iteration 177 : 0.36801074779899484
Loss in iteration 178 : 0.36798876432022765
Loss in iteration 179 : 0.36796707040045973
Loss in iteration 180 : 0.3679453782222754
Loss in iteration 181 : 0.3679237136072243
Loss in iteration 182 : 0.3679031984432515
Loss in iteration 183 : 0.3678820649302008
Loss in iteration 184 : 0.3678611421516031
Loss in iteration 185 : 0.36784042716126736
Loss in iteration 186 : 0.36781973853000405
Loss in iteration 187 : 0.36779918828801916
Loss in iteration 188 : 0.367778718705118
Loss in iteration 189 : 0.36775776775299035
Loss in iteration 190 : 0.3677370875080902
Loss in iteration 191 : 0.3677170392427012
Loss in iteration 192 : 0.36769709306145526
Loss in iteration 193 : 0.3676773424184099
Loss in iteration 194 : 0.3676574966696147
Loss in iteration 195 : 0.36763756268585035
Loss in iteration 196 : 0.3676176327330873
Loss in iteration 197 : 0.3675979231944352
Loss in iteration 198 : 0.3675784918799355
Loss in iteration 199 : 0.36755935004062673
Loss in iteration 200 : 0.36754012771551375
Testing accuracy  of updater 3 on alg 1 with rate 0.07840000000000001 = 0.7865, training accuracy 0.8397539656846876, time elapsed: 2055 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5422311805262409
Loss in iteration 3 : 0.5388867931158878
Loss in iteration 4 : 0.5357222121682533
Loss in iteration 5 : 0.5326498865713639
Loss in iteration 6 : 0.5296382963201195
Loss in iteration 7 : 0.5266555913592128
Loss in iteration 8 : 0.5237111579727425
Loss in iteration 9 : 0.5208160616393844
Loss in iteration 10 : 0.5179886295553442
Loss in iteration 11 : 0.515226702277747
Loss in iteration 12 : 0.5125100860049885
Loss in iteration 13 : 0.5098253665589377
Loss in iteration 14 : 0.5071507863434477
Loss in iteration 15 : 0.50449637206525
Loss in iteration 16 : 0.5018687584907977
Loss in iteration 17 : 0.4992563782534722
Loss in iteration 18 : 0.49666143816376834
Loss in iteration 19 : 0.494078027027116
Loss in iteration 20 : 0.4915056541289333
Loss in iteration 21 : 0.4889485248762963
Loss in iteration 22 : 0.48640305046852794
Loss in iteration 23 : 0.4838662392464277
Loss in iteration 24 : 0.4813329314781169
Loss in iteration 25 : 0.478806973035666
Loss in iteration 26 : 0.4762879872122651
Loss in iteration 27 : 0.4737752445339245
Loss in iteration 28 : 0.4712649993779285
Loss in iteration 29 : 0.46875745515711575
Loss in iteration 30 : 0.4662591502538404
Loss in iteration 31 : 0.46377087005186296
Loss in iteration 32 : 0.4612969241676028
Loss in iteration 33 : 0.4588283051863111
Loss in iteration 34 : 0.456368508467443
Loss in iteration 35 : 0.45391433958034305
Loss in iteration 36 : 0.45146550546403674
Loss in iteration 37 : 0.4490232824240051
Loss in iteration 38 : 0.4465870466005765
Loss in iteration 39 : 0.44415303943823814
Loss in iteration 40 : 0.4417260214304449
Loss in iteration 41 : 0.43930243505361927
Loss in iteration 42 : 0.4368791247844003
Loss in iteration 43 : 0.43446867069198286
Loss in iteration 44 : 0.432103816026816
Loss in iteration 45 : 0.4297933236375849
Loss in iteration 46 : 0.42752434116044824
Loss in iteration 47 : 0.42534657698164813
Loss in iteration 48 : 0.42324914732577434
Loss in iteration 49 : 0.42121819920080505
Loss in iteration 50 : 0.419276346616281
Loss in iteration 51 : 0.4174442558213567
Loss in iteration 52 : 0.41566620550644795
Loss in iteration 53 : 0.4139436895417035
Loss in iteration 54 : 0.41229713190234657
Loss in iteration 55 : 0.41071407173675384
Loss in iteration 56 : 0.4091859554489937
Loss in iteration 57 : 0.4077132139154065
Loss in iteration 58 : 0.4063305298735516
Loss in iteration 59 : 0.4050204311350022
Loss in iteration 60 : 0.40376805593906234
Loss in iteration 61 : 0.4025934907622149
Loss in iteration 62 : 0.4014790063172874
Loss in iteration 63 : 0.4004104500450211
Loss in iteration 64 : 0.39937331762545825
Loss in iteration 65 : 0.39839754769423397
Loss in iteration 66 : 0.3974780936506747
Testing accuracy  of updater 3 on alg 1 with rate 0.019600000000000006 = 0.775, training accuracy 0.8378115895111686, time elapsed: 792 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 10000.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 42 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 7000.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 31 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 4000.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 42 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 28 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 44 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 51 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 30 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.427802406469979
Loss in iteration 3 : 4.613860794203664
Loss in iteration 4 : 1.9779117461842128
Loss in iteration 5 : 0.5840935756776677
Loss in iteration 6 : 0.6345545967757517
Loss in iteration 7 : 0.7245489611123007
Loss in iteration 8 : 1.1409071671706121
Loss in iteration 9 : 1.1248313272257378
Loss in iteration 10 : 1.6084995432417757
Loss in iteration 11 : 0.6771498374875408
Loss in iteration 12 : 0.7064780593089711
Loss in iteration 13 : 0.6741720941411641
Loss in iteration 14 : 0.7935769944861238
Loss in iteration 15 : 0.8313503903464068
Loss in iteration 16 : 1.271707720990257
Loss in iteration 17 : 0.9143813522890762
Loss in iteration 18 : 1.283291565386213
Loss in iteration 19 : 0.791895647570385
Loss in iteration 20 : 0.9615275440541875
Loss in iteration 21 : 0.7770687008169342
Loss in iteration 22 : 0.9982808967416219
Loss in iteration 23 : 0.8106025671532072
Loss in iteration 24 : 1.0962178016545634
Loss in iteration 25 : 0.8464614946075177
Loss in iteration 26 : 1.1804217988246948
Loss in iteration 27 : 0.829762105831937
Loss in iteration 28 : 1.0998426227505835
Loss in iteration 29 : 0.8087512696375898
Loss in iteration 30 : 1.0817776274490833
Loss in iteration 31 : 0.8216981971684666
Loss in iteration 32 : 1.1362104294595667
Loss in iteration 33 : 0.8498040103442155
Loss in iteration 34 : 1.1993766586114414
Loss in iteration 35 : 0.8456874307748243
Loss in iteration 36 : 1.1458206182188695
Loss in iteration 37 : 0.8148932007305196
Loss in iteration 38 : 1.1149673410376202
Loss in iteration 39 : 0.841619747218699
Loss in iteration 40 : 1.1812318303077451
Loss in iteration 41 : 0.8472994836205613
Loss in iteration 42 : 1.1806215812756864
Loss in iteration 43 : 0.8422439657550342
Loss in iteration 44 : 1.1688151588019726
Loss in iteration 45 : 0.8310300277044148
Loss in iteration 46 : 1.1637720029048535
Loss in iteration 47 : 0.843174402788527
Loss in iteration 48 : 1.1886186140239556
Loss in iteration 49 : 0.8430142226808218
Loss in iteration 50 : 1.1926359328468212
Loss in iteration 51 : 0.8435662798873308
Loss in iteration 52 : 1.1967253721232458
Loss in iteration 53 : 0.8422584191410851
Loss in iteration 54 : 1.1880710723734096
Loss in iteration 55 : 0.8486449622720774
Loss in iteration 56 : 1.1921962294848951
Loss in iteration 57 : 0.8482283961081614
Loss in iteration 58 : 1.1958827050176342
Loss in iteration 59 : 0.8473983546065985
Loss in iteration 60 : 1.195113751510178
Loss in iteration 61 : 0.845659519806019
Loss in iteration 62 : 1.1806841100027536
Loss in iteration 63 : 0.8508394863631077
Loss in iteration 64 : 1.197328523574504
Loss in iteration 65 : 0.8497428203645911
Loss in iteration 66 : 1.194253407408938
Loss in iteration 67 : 0.8503362730133022
Loss in iteration 68 : 1.1853114086589187
Loss in iteration 69 : 0.8520784186882231
Loss in iteration 70 : 1.1948075420442705
Loss in iteration 71 : 0.8555192422103619
Loss in iteration 72 : 1.1991618532619868
Loss in iteration 73 : 0.8533571863172571
Loss in iteration 74 : 1.178375282071529
Loss in iteration 75 : 0.8603673738965999
Loss in iteration 76 : 1.2063161821765576
Loss in iteration 77 : 0.850475285839543
Loss in iteration 78 : 1.1744599482017084
Loss in iteration 79 : 0.8634006474768106
Loss in iteration 80 : 1.2075567973770447
Loss in iteration 81 : 0.8472933062228949
Loss in iteration 82 : 1.157098696062482
Loss in iteration 83 : 0.8494370102120262
Loss in iteration 84 : 1.195981928075276
Loss in iteration 85 : 0.862433538315683
Loss in iteration 86 : 1.2168123571012965
Loss in iteration 87 : 0.854215017236534
Loss in iteration 88 : 1.1851447679587923
Loss in iteration 89 : 0.856168460057871
Loss in iteration 90 : 1.1865903302808891
Loss in iteration 91 : 0.8583963535438789
Loss in iteration 92 : 1.1908770973801475
Loss in iteration 93 : 0.8540920289021313
Loss in iteration 94 : 1.1844612454304144
Loss in iteration 95 : 0.8614697345423724
Loss in iteration 96 : 1.1996839369174157
Loss in iteration 97 : 0.8448578857897434
Loss in iteration 98 : 1.166112672092305
Loss in iteration 99 : 0.8533163251718403
Loss in iteration 100 : 1.2013576646778634
Loss in iteration 101 : 0.8607595646143831
Loss in iteration 102 : 1.2109966953462603
Loss in iteration 103 : 0.8514956088583132
Loss in iteration 104 : 1.1779909150074588
Loss in iteration 105 : 0.8431530291566902
Loss in iteration 106 : 1.1669465258830074
Loss in iteration 107 : 0.8577145490373496
Loss in iteration 108 : 1.2115659882615368
Loss in iteration 109 : 0.853580468982003
Loss in iteration 110 : 1.1824504754840568
Loss in iteration 111 : 0.8547845278442113
Loss in iteration 112 : 1.192986045372928
Loss in iteration 113 : 0.8536097206144994
Loss in iteration 114 : 1.1828635210822573
Loss in iteration 115 : 0.8516609259134376
Loss in iteration 116 : 1.1846569054273877
Loss in iteration 117 : 0.8561436656720814
Loss in iteration 118 : 1.197411135180167
Loss in iteration 119 : 0.8564117320196087
Loss in iteration 120 : 1.2006125706222317
Loss in iteration 121 : 0.8504344740385648
Loss in iteration 122 : 1.1771223677927722
Loss in iteration 123 : 0.8482121159780577
Loss in iteration 124 : 1.1779315007027025
Loss in iteration 125 : 0.8503754658758446
Loss in iteration 126 : 1.1981807900580381
Loss in iteration 127 : 0.8552924609363863
Loss in iteration 128 : 1.198124756142168
Loss in iteration 129 : 0.8498356372540187
Loss in iteration 130 : 1.1886863372775365
Loss in iteration 131 : 0.841100073442464
Loss in iteration 132 : 1.1647471797776345
Loss in iteration 133 : 0.8596571024808056
Loss in iteration 134 : 1.212665373686629
Loss in iteration 135 : 0.8517974262484717
Loss in iteration 136 : 1.1908195582710135
Loss in iteration 137 : 0.8415315321478812
Loss in iteration 138 : 1.1740512736992783
Loss in iteration 139 : 0.8532013523758112
Loss in iteration 140 : 1.2055661265745397
Loss in iteration 141 : 0.8500025843708681
Loss in iteration 142 : 1.1890965735072905
Loss in iteration 143 : 0.8442610140854871
Loss in iteration 144 : 1.184514328707148
Loss in iteration 145 : 0.8502655690827645
Loss in iteration 146 : 1.19270918958028
Loss in iteration 147 : 0.8440464282228026
Loss in iteration 148 : 1.1805925384684581
Loss in iteration 149 : 0.8518917115288824
Loss in iteration 150 : 1.1980761337751784
Loss in iteration 151 : 0.8514276190959442
Loss in iteration 152 : 1.1895100056225945
Loss in iteration 153 : 0.8453677551404237
Loss in iteration 154 : 1.1778632955146686
Loss in iteration 155 : 0.8539725342000125
Loss in iteration 156 : 1.19807394323502
Loss in iteration 157 : 0.8454012440300878
Loss in iteration 158 : 1.1796140518115326
Loss in iteration 159 : 0.8538153623083293
Loss in iteration 160 : 1.1897787014314154
Loss in iteration 161 : 0.8432702302850189
Loss in iteration 162 : 1.1795461503312445
Loss in iteration 163 : 0.855208469410469
Loss in iteration 164 : 1.1965619145484605
Loss in iteration 165 : 0.846642668730801
Loss in iteration 166 : 1.1828522441591032
Loss in iteration 167 : 0.850387554569093
Loss in iteration 168 : 1.1879612246443583
Loss in iteration 169 : 0.8452946062002018
Loss in iteration 170 : 1.172491103810154
Loss in iteration 171 : 0.8548812484150159
Loss in iteration 172 : 1.2012320371574394
Loss in iteration 173 : 0.8477731786621645
Loss in iteration 174 : 1.1770529120002204
Loss in iteration 175 : 0.8502913087531363
Loss in iteration 176 : 1.1900334619626298
Loss in iteration 177 : 0.8492359106781009
Loss in iteration 178 : 1.1847222399966362
Loss in iteration 179 : 0.8500212590223802
Loss in iteration 180 : 1.186458966229814
Loss in iteration 181 : 0.8481584519564861
Loss in iteration 182 : 1.1700063407522048
Loss in iteration 183 : 0.8458746757490258
Loss in iteration 184 : 1.1737384252611016
Loss in iteration 185 : 0.857991855106073
Loss in iteration 186 : 1.2137417603524236
Loss in iteration 187 : 0.8461241845959521
Loss in iteration 188 : 1.1599245121460104
Loss in iteration 189 : 0.8431194749039217
Loss in iteration 190 : 1.1698723971991072
Loss in iteration 191 : 0.8569305057713766
Loss in iteration 192 : 1.218729278151706
Loss in iteration 193 : 0.847807617046857
Loss in iteration 194 : 1.1741326658754756
Loss in iteration 195 : 0.84709700574201
Loss in iteration 196 : 1.1830676284398014
Loss in iteration 197 : 0.8521580769714908
Loss in iteration 198 : 1.1954484203489906
Loss in iteration 199 : 0.8481595860437297
Loss in iteration 200 : 1.1788222614526367
Testing accuracy  of updater 5 on alg 1 with rate 0.049 = 0.78825, training accuracy 0.7869860796374231, time elapsed: 2274 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.239345163435184
Loss in iteration 3 : 2.7059668028113943
Loss in iteration 4 : 1.1991784831977295
Loss in iteration 5 : 0.5053752548419055
Loss in iteration 6 : 0.545411727596768
Loss in iteration 7 : 0.5848148629319291
Loss in iteration 8 : 0.7639935814187817
Loss in iteration 9 : 0.7685774562913485
Loss in iteration 10 : 1.0374429259557263
Loss in iteration 11 : 0.5875984733966695
Loss in iteration 12 : 0.6519093058014174
Loss in iteration 13 : 0.6063035688323113
Loss in iteration 14 : 0.7227381305265352
Loss in iteration 15 : 0.6567008226449499
Loss in iteration 16 : 0.8262567231542177
Loss in iteration 17 : 0.659501382103146
Loss in iteration 18 : 0.8035245721181647
Loss in iteration 19 : 0.6542021632467095
Loss in iteration 20 : 0.803136702509944
Loss in iteration 21 : 0.6520680137990293
Loss in iteration 22 : 0.8072268433982746
Loss in iteration 23 : 0.659968969755057
Loss in iteration 24 : 0.8294448706251716
Loss in iteration 25 : 0.6608177338075599
Loss in iteration 26 : 0.8315640895645522
Loss in iteration 27 : 0.6669058118185134
Loss in iteration 28 : 0.8475178880032357
Loss in iteration 29 : 0.6678658482798197
Loss in iteration 30 : 0.8517743562830545
Loss in iteration 31 : 0.6677851829682385
Loss in iteration 32 : 0.8359293677026716
Loss in iteration 33 : 0.6707620640223573
Loss in iteration 34 : 0.8576182995762763
Loss in iteration 35 : 0.6849216821022093
Loss in iteration 36 : 0.8948908098737838
Loss in iteration 37 : 0.6831560225135053
Loss in iteration 38 : 0.8713935186607162
Loss in iteration 39 : 0.6719635333719924
Loss in iteration 40 : 0.8569932913236082
Loss in iteration 41 : 0.6757980662658921
Loss in iteration 42 : 0.8685571015224639
Loss in iteration 43 : 0.6899519649082432
Loss in iteration 44 : 0.9019278331823911
Loss in iteration 45 : 0.6931571393701429
Loss in iteration 46 : 0.8946043910482457
Loss in iteration 47 : 0.6828258358125807
Loss in iteration 48 : 0.87738013358788
Loss in iteration 49 : 0.6899608508664206
Loss in iteration 50 : 0.8935713607426016
Loss in iteration 51 : 0.68944521159796
Loss in iteration 52 : 0.8987001634157965
Loss in iteration 53 : 0.6884177652172218
Loss in iteration 54 : 0.8938821579777115
Loss in iteration 55 : 0.6916404314527754
Loss in iteration 56 : 0.8978087153566354
Loss in iteration 57 : 0.6905039204989458
Loss in iteration 58 : 0.897432947142581
Loss in iteration 59 : 0.6935888056622828
Loss in iteration 60 : 0.900183601833473
Loss in iteration 61 : 0.693138838612159
Loss in iteration 62 : 0.9045916957040613
Loss in iteration 63 : 0.6913256703761756
Loss in iteration 64 : 0.8967844287535645
Loss in iteration 65 : 0.6966064200442343
Loss in iteration 66 : 0.8999666897285353
Loss in iteration 67 : 0.6948621072038593
Loss in iteration 68 : 0.8983161463278767
Loss in iteration 69 : 0.6949633901117355
Loss in iteration 70 : 0.9013203989500242
Loss in iteration 71 : 0.6942333196364094
Loss in iteration 72 : 0.8981156132128951
Loss in iteration 73 : 0.6938740678105522
Loss in iteration 74 : 0.8968783668373148
Loss in iteration 75 : 0.69216458805578
Loss in iteration 76 : 0.8995420639035777
Loss in iteration 77 : 0.6933957426463699
Loss in iteration 78 : 0.9035370106993115
Loss in iteration 79 : 0.6947681204788461
Loss in iteration 80 : 0.9027459451589622
Loss in iteration 81 : 0.6935525308803026
Loss in iteration 82 : 0.9001964380357154
Loss in iteration 83 : 0.6934006105316415
Loss in iteration 84 : 0.8988906192604555
Loss in iteration 85 : 0.6915878461256648
Loss in iteration 86 : 0.8956883350352052
Loss in iteration 87 : 0.6946110998589359
Loss in iteration 88 : 0.9153899184650738
Loss in iteration 89 : 0.6948986696016328
Loss in iteration 90 : 0.9034537008844938
Loss in iteration 91 : 0.6878945691336883
Loss in iteration 92 : 0.8800381287155743
Loss in iteration 93 : 0.6910981335292176
Loss in iteration 94 : 0.9095803064804958
Loss in iteration 95 : 0.7006354481889289
Loss in iteration 96 : 0.9227555140221458
Loss in iteration 97 : 0.6934837259140564
Loss in iteration 98 : 0.8911343279122078
Loss in iteration 99 : 0.6858918683331164
Loss in iteration 100 : 0.8838608987813876
Loss in iteration 101 : 0.692786071921765
Loss in iteration 102 : 0.9054734002249549
Loss in iteration 103 : 0.6966020395346709
Loss in iteration 104 : 0.9110288668103064
Loss in iteration 105 : 0.6908906174806984
Loss in iteration 106 : 0.8964947775895913
Loss in iteration 107 : 0.6895262132752676
Loss in iteration 108 : 0.894973607576183
Loss in iteration 109 : 0.6928618245081152
Loss in iteration 110 : 0.9045567069072805
Loss in iteration 111 : 0.6911287185104219
Loss in iteration 112 : 0.8972418899279062
Loss in iteration 113 : 0.6932436118527009
Loss in iteration 114 : 0.9020220737952319
Loss in iteration 115 : 0.6882652877507108
Loss in iteration 116 : 0.8980061263251954
Loss in iteration 117 : 0.6914130932554229
Loss in iteration 118 : 0.9012854387993113
Loss in iteration 119 : 0.6896597728417874
Loss in iteration 120 : 0.8993635242223105
Loss in iteration 121 : 0.6917624752101965
Loss in iteration 122 : 0.9042172590341659
Loss in iteration 123 : 0.6888295763632356
Loss in iteration 124 : 0.8993847025588698
Loss in iteration 125 : 0.6895631432853262
Loss in iteration 126 : 0.901628642322415
Loss in iteration 127 : 0.6930093637234671
Loss in iteration 128 : 0.9088379085059703
Loss in iteration 129 : 0.689923041233835
Loss in iteration 130 : 0.8975134708302784
Loss in iteration 131 : 0.6858062318516829
Loss in iteration 132 : 0.8921450384325442
Loss in iteration 133 : 0.6914349013761376
Loss in iteration 134 : 0.9099273207632953
Loss in iteration 135 : 0.6908263893984401
Loss in iteration 136 : 0.9023452662608868
Loss in iteration 137 : 0.6862436296182061
Loss in iteration 138 : 0.8909434930553308
Loss in iteration 139 : 0.6892165288737571
Loss in iteration 140 : 0.9036137176457095
Loss in iteration 141 : 0.6881145558674393
Loss in iteration 142 : 0.899374537865783
Loss in iteration 143 : 0.6888873932287839
Loss in iteration 144 : 0.9039122049219747
Loss in iteration 145 : 0.6869850793060872
Loss in iteration 146 : 0.8922450999124126
Loss in iteration 147 : 0.692709703619305
Loss in iteration 148 : 0.9140757480679719
Loss in iteration 149 : 0.688871122882799
Loss in iteration 150 : 0.8925578412704137
Loss in iteration 151 : 0.68539865773305
Loss in iteration 152 : 0.8861312074668358
Loss in iteration 153 : 0.6922871927328966
Loss in iteration 154 : 0.9157501648242706
Loss in iteration 155 : 0.6857037174086132
Loss in iteration 156 : 0.8860294035608511
Loss in iteration 157 : 0.6895102617769445
Loss in iteration 158 : 0.9077543494664975
Loss in iteration 159 : 0.684349538292614
Loss in iteration 160 : 0.8855730411428983
Loss in iteration 161 : 0.6889298923764661
Loss in iteration 162 : 0.9066540581888123
Loss in iteration 163 : 0.6862073040838155
Loss in iteration 164 : 0.9036153914746977
Loss in iteration 165 : 0.6845334799349215
Loss in iteration 166 : 0.891008686844132
Loss in iteration 167 : 0.6890750140463455
Loss in iteration 168 : 0.9095011666819091
Loss in iteration 169 : 0.683184366810917
Loss in iteration 170 : 0.8839037862616308
Loss in iteration 171 : 0.6900586119403211
Loss in iteration 172 : 0.9129682577585906
Loss in iteration 173 : 0.6839233898649248
Loss in iteration 174 : 0.8912962951995266
Loss in iteration 175 : 0.6920501766735094
Loss in iteration 176 : 0.9141404315697254
Loss in iteration 177 : 0.6783107701302952
Loss in iteration 178 : 0.8707367288735994
Loss in iteration 179 : 0.6824920081150537
Loss in iteration 180 : 0.8982198078708931
Loss in iteration 181 : 0.6939925364303606
Loss in iteration 182 : 0.9295287079798821
Loss in iteration 183 : 0.6843339440125263
Loss in iteration 184 : 0.8925138905338644
Loss in iteration 185 : 0.6872051145953076
Loss in iteration 186 : 0.9033668551580758
Loss in iteration 187 : 0.6819071038922632
Loss in iteration 188 : 0.8777259439968446
Loss in iteration 189 : 0.6792930443951357
Loss in iteration 190 : 0.8885069469148819
Loss in iteration 191 : 0.6983581131009592
Loss in iteration 192 : 0.9432342268255413
Loss in iteration 193 : 0.6767244912936077
Loss in iteration 194 : 0.8635988230537222
Loss in iteration 195 : 0.6786498306608426
Loss in iteration 196 : 0.893757464092055
Loss in iteration 197 : 0.6992030819371078
Loss in iteration 198 : 0.9443393850036531
Loss in iteration 199 : 0.6750620910012259
Loss in iteration 200 : 0.8555869280653348
Testing accuracy  of updater 5 on alg 1 with rate 0.0343 = 0.78875, training accuracy 0.7924894787957267, time elapsed: 2084 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.5870731033164462
Loss in iteration 3 : 1.7047453045983576
Loss in iteration 4 : 0.8386479838840722
Loss in iteration 5 : 0.4367516642315969
Loss in iteration 6 : 0.5218031871278531
Loss in iteration 7 : 0.5991401387010137
Loss in iteration 8 : 0.7181224180679592
Loss in iteration 9 : 0.47809169268421386
Loss in iteration 10 : 0.5127790795980878
Loss in iteration 11 : 0.4951556156994546
Loss in iteration 12 : 0.5438517272863198
Loss in iteration 13 : 0.5000414594303526
Loss in iteration 14 : 0.5501847886224475
Loss in iteration 15 : 0.5032138859984309
Loss in iteration 16 : 0.5575171001244368
Loss in iteration 17 : 0.5074238129226742
Loss in iteration 18 : 0.5606122523844159
Loss in iteration 19 : 0.5097423189837703
Loss in iteration 20 : 0.5646646167066692
Loss in iteration 21 : 0.5055803439620269
Loss in iteration 22 : 0.5576332007086061
Loss in iteration 23 : 0.5096086224445044
Loss in iteration 24 : 0.5728999892889245
Loss in iteration 25 : 0.5174252862596465
Loss in iteration 26 : 0.5932460512404332
Loss in iteration 27 : 0.5191362251258874
Loss in iteration 28 : 0.5872035293547686
Loss in iteration 29 : 0.51725928791598
Loss in iteration 30 : 0.5893090753291857
Loss in iteration 31 : 0.5184093945851677
Loss in iteration 32 : 0.5925343565043313
Loss in iteration 33 : 0.5243706743065306
Loss in iteration 34 : 0.6088823022319265
Loss in iteration 35 : 0.5255264171010161
Loss in iteration 36 : 0.6017743568766671
Loss in iteration 37 : 0.5237188684817841
Loss in iteration 38 : 0.6038045478590247
Loss in iteration 39 : 0.5263699320384663
Loss in iteration 40 : 0.6107552396084538
Loss in iteration 41 : 0.5284015412755015
Loss in iteration 42 : 0.6134253750263757
Loss in iteration 43 : 0.528285644942694
Loss in iteration 44 : 0.612488044203237
Loss in iteration 45 : 0.5316554048889228
Loss in iteration 46 : 0.61808493682552
Loss in iteration 47 : 0.5316232217467147
Loss in iteration 48 : 0.6110093418856039
Loss in iteration 49 : 0.5308863574885871
Loss in iteration 50 : 0.6119874569403208
Loss in iteration 51 : 0.5313085952947489
Loss in iteration 52 : 0.613437210173352
Loss in iteration 53 : 0.5334871089156653
Loss in iteration 54 : 0.618599458308967
Loss in iteration 55 : 0.5337824892943939
Loss in iteration 56 : 0.6190698898146509
Loss in iteration 57 : 0.5343800467188411
Loss in iteration 58 : 0.6192427916901656
Loss in iteration 59 : 0.5344170326772858
Loss in iteration 60 : 0.621014787677202
Loss in iteration 61 : 0.5331843694255799
Loss in iteration 62 : 0.6192219073756198
Loss in iteration 63 : 0.5318447150773001
Loss in iteration 64 : 0.6183362596644073
Loss in iteration 65 : 0.5334423267086483
Loss in iteration 66 : 0.6212626237366297
Loss in iteration 67 : 0.5329568260646287
Loss in iteration 68 : 0.620323028513675
Loss in iteration 69 : 0.5338208453780833
Loss in iteration 70 : 0.6217769128969917
Loss in iteration 71 : 0.5331383677562952
Loss in iteration 72 : 0.6204575009277197
Loss in iteration 73 : 0.5324214305103221
Loss in iteration 74 : 0.6220875272378167
Loss in iteration 75 : 0.532520357302574
Loss in iteration 76 : 0.6232203846197597
Loss in iteration 77 : 0.5318796183168032
Loss in iteration 78 : 0.6227807284308347
Loss in iteration 79 : 0.5335944802149963
Loss in iteration 80 : 0.628546195833708
Loss in iteration 81 : 0.5313304865556032
Loss in iteration 82 : 0.6231843018203523
Loss in iteration 83 : 0.5337099060881955
Loss in iteration 84 : 0.6267340611708999
Loss in iteration 85 : 0.5309114422696406
Loss in iteration 86 : 0.6220735615689802
Loss in iteration 87 : 0.5319024516448996
Loss in iteration 88 : 0.6226503838804183
Loss in iteration 89 : 0.5319615450088142
Loss in iteration 90 : 0.6219783712693896
Loss in iteration 91 : 0.5328371691915466
Loss in iteration 92 : 0.6248840120204093
Loss in iteration 93 : 0.5322815364966837
Loss in iteration 94 : 0.6233904098551061
Loss in iteration 95 : 0.5323175323100721
Loss in iteration 96 : 0.6205924910055628
Loss in iteration 97 : 0.5310227795284873
Loss in iteration 98 : 0.6199039039969001
Loss in iteration 99 : 0.5312450774844383
Loss in iteration 100 : 0.6224177810046262
Loss in iteration 101 : 0.5313052224872656
Loss in iteration 102 : 0.6252602841566588
Loss in iteration 103 : 0.529260006930091
Loss in iteration 104 : 0.6192104384858558
Loss in iteration 105 : 0.530441624965584
Loss in iteration 106 : 0.6259071883649203
Loss in iteration 107 : 0.5300667200683679
Loss in iteration 108 : 0.6211223923355574
Loss in iteration 109 : 0.5306841550770277
Loss in iteration 110 : 0.6302406148478271
Loss in iteration 111 : 0.5271284753579795
Loss in iteration 112 : 0.609509551347082
Loss in iteration 113 : 0.5226462708809035
Loss in iteration 114 : 0.611160154586771
Loss in iteration 115 : 0.5282982800864034
Loss in iteration 116 : 0.6377527639025538
Loss in iteration 117 : 0.5326460456725441
Loss in iteration 118 : 0.6398517321409292
Loss in iteration 119 : 0.5254290178065976
Loss in iteration 120 : 0.6075726891238669
Loss in iteration 121 : 0.5246570999736324
Loss in iteration 122 : 0.6131353744728166
Loss in iteration 123 : 0.531027371715163
Loss in iteration 124 : 0.6384035611504648
Loss in iteration 125 : 0.5267768498864279
Loss in iteration 126 : 0.6204709701345726
Loss in iteration 127 : 0.5310354813322621
Loss in iteration 128 : 0.6335459745189732
Loss in iteration 129 : 0.5257367481746241
Loss in iteration 130 : 0.6177647531759906
Loss in iteration 131 : 0.5264593659584869
Loss in iteration 132 : 0.6201442408603813
Loss in iteration 133 : 0.5269376154965284
Loss in iteration 134 : 0.6241363249758818
Loss in iteration 135 : 0.5291206783518282
Loss in iteration 136 : 0.6235781788977596
Loss in iteration 137 : 0.5264422214409741
Loss in iteration 138 : 0.6211942256173187
Loss in iteration 139 : 0.5248903544426462
Loss in iteration 140 : 0.6233076376235723
Loss in iteration 141 : 0.5273908126552268
Loss in iteration 142 : 0.6227833660664541
Loss in iteration 143 : 0.5277093369103394
Loss in iteration 144 : 0.6235524798890506
Loss in iteration 145 : 0.5276075182522354
Loss in iteration 146 : 0.6253408066080185
Loss in iteration 147 : 0.5252327634893023
Loss in iteration 148 : 0.6222724457124332
Loss in iteration 149 : 0.5238170696281974
Loss in iteration 150 : 0.6217105615932401
Loss in iteration 151 : 0.5241721601476558
Loss in iteration 152 : 0.6226817263769827
Loss in iteration 153 : 0.5268559277375742
Loss in iteration 154 : 0.6306763363731479
Loss in iteration 155 : 0.5288509135840755
Loss in iteration 156 : 0.629151961672326
Loss in iteration 157 : 0.5247052199388926
Loss in iteration 158 : 0.6201311283105005
Loss in iteration 159 : 0.5196139261615633
Loss in iteration 160 : 0.6112716381102855
Loss in iteration 161 : 0.5202051965874448
Loss in iteration 162 : 0.61793594792723
Loss in iteration 163 : 0.531220039538411
Loss in iteration 164 : 0.6346244177032421
Loss in iteration 165 : 0.5271020542736164
Loss in iteration 166 : 0.6277794600919638
Loss in iteration 167 : 0.524031859292326
Loss in iteration 168 : 0.6177564647835493
Loss in iteration 169 : 0.5250847679556379
Loss in iteration 170 : 0.6269903077380783
Loss in iteration 171 : 0.5237443752312562
Loss in iteration 172 : 0.6177618934112666
Loss in iteration 173 : 0.5253590904812352
Loss in iteration 174 : 0.6263784003326648
Loss in iteration 175 : 0.5237325245394469
Loss in iteration 176 : 0.6172793081821085
Loss in iteration 177 : 0.5219211624581109
Loss in iteration 178 : 0.6170938338715402
Loss in iteration 179 : 0.5269742366976821
Loss in iteration 180 : 0.6321424988093548
Loss in iteration 181 : 0.5263754702607646
Loss in iteration 182 : 0.6190587509967915
Loss in iteration 183 : 0.5229588044480749
Loss in iteration 184 : 0.6161225718668165
Loss in iteration 185 : 0.5214890790070378
Loss in iteration 186 : 0.617573639182721
Loss in iteration 187 : 0.5275352321831197
Loss in iteration 188 : 0.6328575468232008
Loss in iteration 189 : 0.5269918125376307
Loss in iteration 190 : 0.6238036526337883
Loss in iteration 191 : 0.5227782315935746
Loss in iteration 192 : 0.6178804396929356
Loss in iteration 193 : 0.5201951824930567
Loss in iteration 194 : 0.6163797716791615
Loss in iteration 195 : 0.5234636586697082
Loss in iteration 196 : 0.6248472345173293
Loss in iteration 197 : 0.5255897316069418
Loss in iteration 198 : 0.6261697178867944
Loss in iteration 199 : 0.5233013636915522
Loss in iteration 200 : 0.6189071823660001
Testing accuracy  of updater 5 on alg 1 with rate 0.0196 = 0.79125, training accuracy 0.8025250890255746, time elapsed: 2139 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9203915956546849
Loss in iteration 3 : 0.6957287064003678
Loss in iteration 4 : 0.5029177717867397
Loss in iteration 5 : 0.48572000121500025
Loss in iteration 6 : 0.4709162341863581
Loss in iteration 7 : 0.4580574215512454
Loss in iteration 8 : 0.4452622011347767
Loss in iteration 9 : 0.43319130427152613
Loss in iteration 10 : 0.4219045647046437
Loss in iteration 11 : 0.41222938444137175
Loss in iteration 12 : 0.40428530697148407
Loss in iteration 13 : 0.3982877798102897
Loss in iteration 14 : 0.39434859358959407
Loss in iteration 15 : 0.391429216985399
Loss in iteration 16 : 0.3901347791561174
Loss in iteration 17 : 0.38995159075261404
Loss in iteration 18 : 0.3925322616454297
Loss in iteration 19 : 0.3970000078350899
Loss in iteration 20 : 0.4063341801755772
Loss in iteration 21 : 0.40799527110040146
Loss in iteration 22 : 0.4097811779616382
Loss in iteration 23 : 0.3996355321577604
Loss in iteration 24 : 0.3972918371467466
Loss in iteration 25 : 0.38723147463371915
Loss in iteration 26 : 0.38393601877748457
Loss in iteration 27 : 0.3815622384954812
Loss in iteration 28 : 0.3814728912728314
Loss in iteration 29 : 0.3803156753870735
Loss in iteration 30 : 0.382896392363412
Loss in iteration 31 : 0.38412682454977
Loss in iteration 32 : 0.3895907886187317
Loss in iteration 33 : 0.3920747038974479
Loss in iteration 34 : 0.4018873932619422
Loss in iteration 35 : 0.4021562762531686
Loss in iteration 36 : 0.4116222372201465
Loss in iteration 37 : 0.39732741665838267
Loss in iteration 38 : 0.3991015606840968
Loss in iteration 39 : 0.39036733736330065
Loss in iteration 40 : 0.3901326760279644
Loss in iteration 41 : 0.38362666301603504
Loss in iteration 42 : 0.38647749892450634
Loss in iteration 43 : 0.3836038126521157
Loss in iteration 44 : 0.38870800686098445
Loss in iteration 45 : 0.3872726825164467
Loss in iteration 46 : 0.397084448471256
Loss in iteration 47 : 0.39736349598742193
Loss in iteration 48 : 0.4074979613575588
Loss in iteration 49 : 0.39837381551954976
Loss in iteration 50 : 0.4027786949214469
Loss in iteration 51 : 0.39308927410064415
Loss in iteration 52 : 0.3937165565387436
Loss in iteration 53 : 0.3857591105280101
Loss in iteration 54 : 0.3873042224390357
Loss in iteration 55 : 0.383641519116723
Loss in iteration 56 : 0.386863748619306
Loss in iteration 57 : 0.3860210346409597
Loss in iteration 58 : 0.39524590413864963
Loss in iteration 59 : 0.3958647339759707
Loss in iteration 60 : 0.4072510496240363
Loss in iteration 61 : 0.4012986687462246
Loss in iteration 62 : 0.4077789280988544
Loss in iteration 63 : 0.3954150572109961
Loss in iteration 64 : 0.3958524083094668
Loss in iteration 65 : 0.3873431926613198
Loss in iteration 66 : 0.38834329618673663
Loss in iteration 67 : 0.3817370114605509
Loss in iteration 68 : 0.3823804601527423
Loss in iteration 69 : 0.38245364508026247
Loss in iteration 70 : 0.3868705406323522
Loss in iteration 71 : 0.38603767882355344
Loss in iteration 72 : 0.3980105936909943
Loss in iteration 73 : 0.4028447204319094
Loss in iteration 74 : 0.4185712939440463
Loss in iteration 75 : 0.40345390887340343
Loss in iteration 76 : 0.4066071217261802
Loss in iteration 77 : 0.3923529747443263
Loss in iteration 78 : 0.3921791071629613
Loss in iteration 79 : 0.3837918274274693
Loss in iteration 80 : 0.3826655482766512
Loss in iteration 81 : 0.3799432761852694
Loss in iteration 82 : 0.3817939739022339
Loss in iteration 83 : 0.38257872369732654
Loss in iteration 84 : 0.387059419052395
Loss in iteration 85 : 0.387507805476344
Loss in iteration 86 : 0.40060940924624266
Loss in iteration 87 : 0.40555238638521546
Loss in iteration 88 : 0.42042236494250634
Loss in iteration 89 : 0.39885228199261147
Loss in iteration 90 : 0.4006810579012137
Loss in iteration 91 : 0.38921990940944823
Loss in iteration 92 : 0.3890265721699262
Loss in iteration 93 : 0.38124317620234
Loss in iteration 94 : 0.38078629013174153
Loss in iteration 95 : 0.37975657582580624
Loss in iteration 96 : 0.38287095500561613
Loss in iteration 97 : 0.38327887258880705
Loss in iteration 98 : 0.39212053228240656
Loss in iteration 99 : 0.39635457550690656
Loss in iteration 100 : 0.41552414061252474
Loss in iteration 101 : 0.4041519363579268
Loss in iteration 102 : 0.410997167820322
Loss in iteration 103 : 0.39284411239121614
Loss in iteration 104 : 0.3926559506512217
Loss in iteration 105 : 0.3828555676723891
Loss in iteration 106 : 0.38116191909570485
Loss in iteration 107 : 0.3778586625073004
Loss in iteration 108 : 0.38097936292662066
Loss in iteration 109 : 0.38105341627856304
Loss in iteration 110 : 0.38652289106612214
Loss in iteration 111 : 0.3876655159984141
Loss in iteration 112 : 0.40120747484713754
Loss in iteration 113 : 0.4051499524687564
Loss in iteration 114 : 0.4186369312945558
Loss in iteration 115 : 0.3972944282497858
Loss in iteration 116 : 0.3999725241850369
Loss in iteration 117 : 0.38835373886874724
Loss in iteration 118 : 0.38701512076705286
Loss in iteration 119 : 0.3787586462191288
Loss in iteration 120 : 0.37985728575754274
Loss in iteration 121 : 0.37817459435521117
Loss in iteration 122 : 0.3828577166128623
Loss in iteration 123 : 0.3820701965151484
Loss in iteration 124 : 0.39013006846343257
Loss in iteration 125 : 0.39273215007947443
Loss in iteration 126 : 0.4100107012359424
Loss in iteration 127 : 0.4040935714767066
Loss in iteration 128 : 0.4110821880691174
Loss in iteration 129 : 0.39202413622198967
Loss in iteration 130 : 0.3928654170301325
Loss in iteration 131 : 0.3833108357534573
Loss in iteration 132 : 0.3827514193926117
Loss in iteration 133 : 0.37811558356369646
Loss in iteration 134 : 0.3816041257408666
Loss in iteration 135 : 0.37990369185385386
Loss in iteration 136 : 0.3863559855233457
Loss in iteration 137 : 0.38703885146733313
Loss in iteration 138 : 0.3991004260799296
Loss in iteration 139 : 0.39846401826666467
Loss in iteration 140 : 0.41048816687646694
Loss in iteration 141 : 0.3966086662049543
Loss in iteration 142 : 0.4001739727365763
Loss in iteration 143 : 0.3867010983002247
Loss in iteration 144 : 0.38579598925333475
Loss in iteration 145 : 0.37793381948446114
Loss in iteration 146 : 0.38030397416179296
Loss in iteration 147 : 0.37855168567773034
Loss in iteration 148 : 0.3849247780457846
Loss in iteration 149 : 0.3837079715367326
Loss in iteration 150 : 0.39189397228580025
Loss in iteration 151 : 0.3947839522073807
Loss in iteration 152 : 0.41156456893304333
Loss in iteration 153 : 0.39924956712208215
Loss in iteration 154 : 0.40394005641432995
Loss in iteration 155 : 0.3877800383538714
Loss in iteration 156 : 0.38789647082811757
Loss in iteration 157 : 0.3798547878328521
Loss in iteration 158 : 0.3818833257033046
Loss in iteration 159 : 0.37767993424801544
Loss in iteration 160 : 0.3822728873429969
Loss in iteration 161 : 0.38096016845776215
Loss in iteration 162 : 0.3899434051749621
Loss in iteration 163 : 0.3913131211581462
Loss in iteration 164 : 0.404453712284195
Loss in iteration 165 : 0.3972137407637711
Loss in iteration 166 : 0.4063424370074376
Loss in iteration 167 : 0.3907234234358379
Loss in iteration 168 : 0.3905632146443204
Loss in iteration 169 : 0.38280216700587616
Loss in iteration 170 : 0.3837853300802166
Loss in iteration 171 : 0.37901679391113297
Loss in iteration 172 : 0.3835750757640226
Loss in iteration 173 : 0.38137310380523975
Loss in iteration 174 : 0.3881076733524638
Loss in iteration 175 : 0.38726098894337846
Loss in iteration 176 : 0.3951093283177712
Loss in iteration 177 : 0.39311110941748245
Loss in iteration 178 : 0.404630412027314
Loss in iteration 179 : 0.39214884641862063
Loss in iteration 180 : 0.39558595142719816
Loss in iteration 181 : 0.3864265620952924
Loss in iteration 182 : 0.3860553623095819
Loss in iteration 183 : 0.3787442318663615
Loss in iteration 184 : 0.3820972786354729
Loss in iteration 185 : 0.37983820402935214
Loss in iteration 186 : 0.3866750908933582
Loss in iteration 187 : 0.3847810007334706
Loss in iteration 188 : 0.39273051469792286
Loss in iteration 189 : 0.39145597205828236
Loss in iteration 190 : 0.40303631291342706
Loss in iteration 191 : 0.39229051279033417
Loss in iteration 192 : 0.3978578234610648
Loss in iteration 193 : 0.38731658200541
Loss in iteration 194 : 0.38779707962958343
Loss in iteration 195 : 0.38067342402108784
Loss in iteration 196 : 0.3848559934563502
Loss in iteration 197 : 0.38012825700128977
Loss in iteration 198 : 0.3872389311023977
Loss in iteration 199 : 0.38412399536200786
Loss in iteration 200 : 0.39011908431064873
Testing accuracy  of updater 5 on alg 1 with rate 0.0049 = 0.79475, training accuracy 0.8313370022661055, time elapsed: 2090 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8044027224779735
Loss in iteration 3 : 0.6251760096453969
Loss in iteration 4 : 0.512370497266313
Loss in iteration 5 : 0.5041742034290748
Loss in iteration 6 : 0.4958170737345406
Loss in iteration 7 : 0.4872293147850864
Loss in iteration 8 : 0.4783296751387477
Loss in iteration 9 : 0.4691060431228618
Loss in iteration 10 : 0.45950866412247815
Loss in iteration 11 : 0.4495235063500379
Loss in iteration 12 : 0.439184482729802
Loss in iteration 13 : 0.4286616020358455
Loss in iteration 14 : 0.4190879076541205
Loss in iteration 15 : 0.4106963297439711
Loss in iteration 16 : 0.4036994821624916
Loss in iteration 17 : 0.3981727890950325
Loss in iteration 18 : 0.39394965617745714
Loss in iteration 19 : 0.39065910926256575
Loss in iteration 20 : 0.3881435747912118
Loss in iteration 21 : 0.38620582261150915
Loss in iteration 22 : 0.3843901272957428
Loss in iteration 23 : 0.38282270961527376
Loss in iteration 24 : 0.3814597184513402
Loss in iteration 25 : 0.38031374648387395
Loss in iteration 26 : 0.3794722224411565
Loss in iteration 27 : 0.3790014086594812
Loss in iteration 28 : 0.37913774388054167
Loss in iteration 29 : 0.3795801407661501
Loss in iteration 30 : 0.38422911770061907
Loss in iteration 31 : 0.39622332856545217
Loss in iteration 32 : 0.41055289308141124
Loss in iteration 33 : 0.41667621883380834
Loss in iteration 34 : 0.39304360480673134
Loss in iteration 35 : 0.3876080376398315
Loss in iteration 36 : 0.37986834076177545
Loss in iteration 37 : 0.37785614318961036
Loss in iteration 38 : 0.3762724163915762
Loss in iteration 39 : 0.3759832431439755
Loss in iteration 40 : 0.3761920303698205
Loss in iteration 41 : 0.3768400479222966
Loss in iteration 42 : 0.3776233552430008
Loss in iteration 43 : 0.37951005885050665
Loss in iteration 44 : 0.38092508609695613
Loss in iteration 45 : 0.3850232964576015
Loss in iteration 46 : 0.38690881825969164
Loss in iteration 47 : 0.39304096863207366
Loss in iteration 48 : 0.38665946401957135
Loss in iteration 49 : 0.38939094025306087
Loss in iteration 50 : 0.38275486496757516
Loss in iteration 51 : 0.38426521437702515
Loss in iteration 52 : 0.38105959296112685
Loss in iteration 53 : 0.3824693623490774
Loss in iteration 54 : 0.38120269684570673
Loss in iteration 55 : 0.3829255781741887
Loss in iteration 56 : 0.3821549478245446
Loss in iteration 57 : 0.3844462318673342
Loss in iteration 58 : 0.38286336774981544
Loss in iteration 59 : 0.38557273212471416
Loss in iteration 60 : 0.382716170166731
Loss in iteration 61 : 0.3851473368697279
Loss in iteration 62 : 0.3821088820278481
Loss in iteration 63 : 0.38377710337910576
Loss in iteration 64 : 0.38214758365829643
Loss in iteration 65 : 0.38408620793019804
Loss in iteration 66 : 0.3822860690000398
Loss in iteration 67 : 0.38462217200563303
Loss in iteration 68 : 0.38247330189133183
Loss in iteration 69 : 0.3847860264624038
Loss in iteration 70 : 0.38208789391070375
Loss in iteration 71 : 0.38326884521168925
Loss in iteration 72 : 0.38165651435472364
Loss in iteration 73 : 0.38305360300006697
Loss in iteration 74 : 0.38195981341518237
Loss in iteration 75 : 0.383968274595878
Loss in iteration 76 : 0.3823040978621909
Loss in iteration 77 : 0.384559252674803
Loss in iteration 78 : 0.38189113190136714
Loss in iteration 79 : 0.38409971853295577
Loss in iteration 80 : 0.381857914666938
Loss in iteration 81 : 0.3839026954799506
Loss in iteration 82 : 0.38189913113601537
Loss in iteration 83 : 0.38382691285996895
Loss in iteration 84 : 0.3818018440659744
Loss in iteration 85 : 0.3837496766192833
Loss in iteration 86 : 0.381704125070091
Loss in iteration 87 : 0.383348016453893
Loss in iteration 88 : 0.38113794915460825
Loss in iteration 89 : 0.3830435222238106
Loss in iteration 90 : 0.38110931994448954
Loss in iteration 91 : 0.3829542501084316
Loss in iteration 92 : 0.3814185175975534
Loss in iteration 93 : 0.38448582617407806
Loss in iteration 94 : 0.3814628082498168
Loss in iteration 95 : 0.3842261138734728
Loss in iteration 96 : 0.3813280134720463
Loss in iteration 97 : 0.38344413625933776
Loss in iteration 98 : 0.3808664937790264
Loss in iteration 99 : 0.38286180369162776
Loss in iteration 100 : 0.3807114989501923
Loss in iteration 101 : 0.3829288893980072
Loss in iteration 102 : 0.3806058252716771
Loss in iteration 103 : 0.382984759209966
Loss in iteration 104 : 0.3806744179555221
Loss in iteration 105 : 0.3834468908782477
Loss in iteration 106 : 0.38006167282383724
Loss in iteration 107 : 0.3817486641740534
Loss in iteration 108 : 0.38000418263457064
Loss in iteration 109 : 0.382737712908017
Loss in iteration 110 : 0.3809416452144222
Loss in iteration 111 : 0.38452653450543134
Loss in iteration 112 : 0.3810209396284352
Loss in iteration 113 : 0.3847251291972433
Loss in iteration 114 : 0.38032991591085247
Loss in iteration 115 : 0.3829456458375653
Loss in iteration 116 : 0.3792134464995776
Loss in iteration 117 : 0.3807217876957739
Loss in iteration 118 : 0.3787826601346449
Loss in iteration 119 : 0.38154663118789767
Loss in iteration 120 : 0.3800361251519459
Loss in iteration 121 : 0.383816742278806
Loss in iteration 122 : 0.3806973433558342
Loss in iteration 123 : 0.38432112239457555
Loss in iteration 124 : 0.3797225260607329
Loss in iteration 125 : 0.3821138733160308
Loss in iteration 126 : 0.37918944799228865
Loss in iteration 127 : 0.3819460414208179
Loss in iteration 128 : 0.3791988464007695
Loss in iteration 129 : 0.3825203439052906
Loss in iteration 130 : 0.37920647730596785
Loss in iteration 131 : 0.3826804094233747
Loss in iteration 132 : 0.3789116188283172
Loss in iteration 133 : 0.38232966841470495
Loss in iteration 134 : 0.3790211186354261
Loss in iteration 135 : 0.38292461524227656
Loss in iteration 136 : 0.3788429706060202
Loss in iteration 137 : 0.38288129007030214
Loss in iteration 138 : 0.37887834904685214
Loss in iteration 139 : 0.38262018685857196
Loss in iteration 140 : 0.37866544722408774
Loss in iteration 141 : 0.3824472542261627
Loss in iteration 142 : 0.3785236695779581
Loss in iteration 143 : 0.3822611636997272
Loss in iteration 144 : 0.37858613855859935
Loss in iteration 145 : 0.3820728849646974
Loss in iteration 146 : 0.37849301493981236
Loss in iteration 147 : 0.3820512905371473
Loss in iteration 148 : 0.3783967209236959
Loss in iteration 149 : 0.38232680774417244
Loss in iteration 150 : 0.3781230428670496
Loss in iteration 151 : 0.38185545038497637
Loss in iteration 152 : 0.3781205187759249
Loss in iteration 153 : 0.3817842362205368
Loss in iteration 154 : 0.37811093068533735
Loss in iteration 155 : 0.3817075515158914
Loss in iteration 156 : 0.37823799516252365
Loss in iteration 157 : 0.38245678560281876
Loss in iteration 158 : 0.3788251304211792
Loss in iteration 159 : 0.38210533150402637
Loss in iteration 160 : 0.3781222950102638
Loss in iteration 161 : 0.38162330433128094
Loss in iteration 162 : 0.3779046203194942
Loss in iteration 163 : 0.3812412323977233
Loss in iteration 164 : 0.3778848826789312
Loss in iteration 165 : 0.38150085277862633
Loss in iteration 166 : 0.3778435089720877
Loss in iteration 167 : 0.38168094242489836
Loss in iteration 168 : 0.3779384948654605
Loss in iteration 169 : 0.38108695807676984
Loss in iteration 170 : 0.37762121258606995
Loss in iteration 171 : 0.38113978236203633
Loss in iteration 172 : 0.37769801413126664
Loss in iteration 173 : 0.38148490163583004
Loss in iteration 174 : 0.37812425305993685
Loss in iteration 175 : 0.38121838989146767
Loss in iteration 176 : 0.37780320166119813
Loss in iteration 177 : 0.3810212622005709
Loss in iteration 178 : 0.377781838641948
Loss in iteration 179 : 0.3809632866948214
Loss in iteration 180 : 0.3776653757222732
Loss in iteration 181 : 0.38070549491081235
Loss in iteration 182 : 0.37777998764928494
Loss in iteration 183 : 0.38063147136903025
Loss in iteration 184 : 0.3776967808109934
Loss in iteration 185 : 0.38026052779778907
Loss in iteration 186 : 0.37756871506674045
Loss in iteration 187 : 0.38060553916331474
Loss in iteration 188 : 0.3774838681875164
Loss in iteration 189 : 0.38053347851127134
Loss in iteration 190 : 0.37749587190974293
Loss in iteration 191 : 0.38097773137656205
Loss in iteration 192 : 0.37728744291496025
Loss in iteration 193 : 0.380397136363901
Loss in iteration 194 : 0.3767790141680243
Loss in iteration 195 : 0.38043201516656994
Loss in iteration 196 : 0.3770820112272685
Loss in iteration 197 : 0.3808708356138587
Loss in iteration 198 : 0.3768448156043077
Loss in iteration 199 : 0.38035955219543405
Loss in iteration 200 : 0.37667503992951334
Testing accuracy  of updater 5 on alg 1 with rate 0.00343 = 0.7705, training accuracy 0.8365166720621561, time elapsed: 1977 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6208247370947881
Loss in iteration 3 : 0.546458492338776
Loss in iteration 4 : 0.538945753922286
Loss in iteration 5 : 0.5329407084956843
Loss in iteration 6 : 0.5271529919957112
Loss in iteration 7 : 0.5213024747386669
Loss in iteration 8 : 0.5153514567194727
Loss in iteration 9 : 0.5092519360898142
Loss in iteration 10 : 0.5030266137373713
Loss in iteration 11 : 0.49660606032241594
Loss in iteration 12 : 0.4899782600427008
Loss in iteration 13 : 0.48313020813162494
Loss in iteration 14 : 0.4760297471013503
Loss in iteration 15 : 0.4686744634577482
Loss in iteration 16 : 0.4610228240712702
Loss in iteration 17 : 0.45306635831950753
Loss in iteration 18 : 0.44482938329054345
Loss in iteration 19 : 0.43631333954493606
Loss in iteration 20 : 0.4278534550253735
Loss in iteration 21 : 0.4201414659610363
Loss in iteration 22 : 0.4132131581708771
Loss in iteration 23 : 0.4070365010470972
Loss in iteration 24 : 0.4019992935457364
Loss in iteration 25 : 0.39776368565898784
Loss in iteration 26 : 0.3943438969932316
Loss in iteration 27 : 0.3915730263471131
Loss in iteration 28 : 0.3893187528416328
Loss in iteration 29 : 0.38748658687907417
Loss in iteration 30 : 0.3859072726623966
Loss in iteration 31 : 0.3844596054919121
Loss in iteration 32 : 0.38314416529885653
Loss in iteration 33 : 0.3819589326785989
Loss in iteration 34 : 0.38090970435864163
Loss in iteration 35 : 0.380009261642029
Loss in iteration 36 : 0.379149430391019
Loss in iteration 37 : 0.3783843993629272
Loss in iteration 38 : 0.3778054873692753
Loss in iteration 39 : 0.3775109655990634
Loss in iteration 40 : 0.3774470179381698
Loss in iteration 41 : 0.37805037981138306
Loss in iteration 42 : 0.38160881757824633
Loss in iteration 43 : 0.3885234309658868
Loss in iteration 44 : 0.387565394085734
Loss in iteration 45 : 0.3835831116029912
Loss in iteration 46 : 0.37882194901131383
Loss in iteration 47 : 0.3771125912561089
Loss in iteration 48 : 0.3760418299508683
Loss in iteration 49 : 0.37589962766762636
Loss in iteration 50 : 0.3755901612460039
Loss in iteration 51 : 0.375533235198488
Loss in iteration 52 : 0.3756831885241054
Loss in iteration 53 : 0.3760517290517492
Loss in iteration 54 : 0.3768400466318461
Loss in iteration 55 : 0.3771683544955357
Loss in iteration 56 : 0.3781653799377376
Loss in iteration 57 : 0.3784739882776476
Loss in iteration 58 : 0.3782262123076891
Loss in iteration 59 : 0.37846856912676424
Loss in iteration 60 : 0.3776732637118994
Loss in iteration 61 : 0.37713074509443667
Loss in iteration 62 : 0.37704820937444283
Loss in iteration 63 : 0.37687366648804077
Loss in iteration 64 : 0.37684233508592513
Loss in iteration 65 : 0.3766611292273268
Loss in iteration 66 : 0.37695678649171704
Loss in iteration 67 : 0.37681853650154623
Loss in iteration 68 : 0.3771416836056695
Loss in iteration 69 : 0.37702908624696463
Loss in iteration 70 : 0.3770456867601644
Loss in iteration 71 : 0.3764251012588584
Loss in iteration 72 : 0.37671150056773356
Loss in iteration 73 : 0.3758521408785822
Loss in iteration 74 : 0.37630608357217094
Loss in iteration 75 : 0.3762872453738864
Loss in iteration 76 : 0.37678866735659006
Loss in iteration 77 : 0.37667870944122117
Loss in iteration 78 : 0.3770157915180249
Loss in iteration 79 : 0.3767516463923824
Loss in iteration 80 : 0.3769721817615514
Loss in iteration 81 : 0.3765409253598793
Loss in iteration 82 : 0.37668422917224675
Loss in iteration 83 : 0.37613050231674056
Loss in iteration 84 : 0.3766783965528134
Loss in iteration 85 : 0.37614187861509246
Loss in iteration 86 : 0.3765820392167962
Loss in iteration 87 : 0.37614958949892124
Loss in iteration 88 : 0.3765845283609107
Loss in iteration 89 : 0.37604311318299677
Loss in iteration 90 : 0.37622514055779227
Loss in iteration 91 : 0.37599523543212765
Loss in iteration 92 : 0.37622670450740164
Loss in iteration 93 : 0.3760457427738643
Loss in iteration 94 : 0.3762677890100176
Loss in iteration 95 : 0.3759140574668294
Loss in iteration 96 : 0.3761971649978304
Loss in iteration 97 : 0.37589756648393524
Loss in iteration 98 : 0.37612260473894926
Loss in iteration 99 : 0.3758292162051725
Loss in iteration 100 : 0.37609830644232567
Loss in iteration 101 : 0.3758600595726166
Loss in iteration 102 : 0.3760755588422176
Loss in iteration 103 : 0.37580788521408287
Loss in iteration 104 : 0.37600198353184594
Loss in iteration 105 : 0.37575549715297096
Loss in iteration 106 : 0.3759282995115193
Loss in iteration 107 : 0.37570293253643855
Loss in iteration 108 : 0.3758545793279137
Loss in iteration 109 : 0.3757797398716767
Loss in iteration 110 : 0.3758740661902486
Loss in iteration 111 : 0.3758244176447314
Loss in iteration 112 : 0.375774526868102
Loss in iteration 113 : 0.3755826591891426
Loss in iteration 114 : 0.37546281284388905
Loss in iteration 115 : 0.3755748439126615
Loss in iteration 116 : 0.3756467409485284
Loss in iteration 117 : 0.3754572264981462
Loss in iteration 118 : 0.37545852349735204
Loss in iteration 119 : 0.3753836767726606
Loss in iteration 120 : 0.3754128976773815
Loss in iteration 121 : 0.3754382220365705
Loss in iteration 122 : 0.37552831709320245
Loss in iteration 123 : 0.3759423853560525
Loss in iteration 124 : 0.3752697541112098
Loss in iteration 125 : 0.37500725951103797
Loss in iteration 126 : 0.37518918798004003
Loss in iteration 127 : 0.37501071405498043
Loss in iteration 128 : 0.3751041808438515
Loss in iteration 129 : 0.37525396443232706
Loss in iteration 130 : 0.3750491163531718
Loss in iteration 131 : 0.3750836862997527
Loss in iteration 132 : 0.375089585075316
Loss in iteration 133 : 0.375410193067245
Loss in iteration 134 : 0.3749325656478895
Loss in iteration 135 : 0.37492690758611
Loss in iteration 136 : 0.3749116112148488
Loss in iteration 137 : 0.3753723001299773
Loss in iteration 138 : 0.37487889701979904
Loss in iteration 139 : 0.3753190028501912
Loss in iteration 140 : 0.37491873218950617
Loss in iteration 141 : 0.3751925549873054
Loss in iteration 142 : 0.3746071415770373
Loss in iteration 143 : 0.3747041000306383
Loss in iteration 144 : 0.37421548453956616
Loss in iteration 145 : 0.37396259555345907
Loss in iteration 146 : 0.3740377565784953
Loss in iteration 147 : 0.3740670822558695
Loss in iteration 148 : 0.3741354425595623
Loss in iteration 149 : 0.3742588252986839
Loss in iteration 150 : 0.37434038745045695
Loss in iteration 151 : 0.37521454343130384
Loss in iteration 152 : 0.3750434340146862
Loss in iteration 153 : 0.3757511313640028
Loss in iteration 154 : 0.37469601408225606
Loss in iteration 155 : 0.3748317170176986
Loss in iteration 156 : 0.3742815802365705
Loss in iteration 157 : 0.37441772806185525
Loss in iteration 158 : 0.3742455169941927
Loss in iteration 159 : 0.3744013961044794
Loss in iteration 160 : 0.37412404489216244
Loss in iteration 161 : 0.3739952687428814
Loss in iteration 162 : 0.37368848261537857
Loss in iteration 163 : 0.37334060238216865
Loss in iteration 164 : 0.3735772116587823
Loss in iteration 165 : 0.37355686653288317
Loss in iteration 166 : 0.3736272401871706
Loss in iteration 167 : 0.37399441531562694
Loss in iteration 168 : 0.3743970822373347
Loss in iteration 169 : 0.37514784396487444
Loss in iteration 170 : 0.37435308190807093
Loss in iteration 171 : 0.3746794876633631
Loss in iteration 172 : 0.37394498395226056
Loss in iteration 173 : 0.3741087799210089
Loss in iteration 174 : 0.3733429312060047
Loss in iteration 175 : 0.37335668037345765
Loss in iteration 176 : 0.373466009871365
Loss in iteration 177 : 0.3735977757609775
Loss in iteration 178 : 0.37359703127854205
Loss in iteration 179 : 0.37365643611432026
Loss in iteration 180 : 0.37347051214519705
Loss in iteration 181 : 0.3737119957583167
Loss in iteration 182 : 0.3735008676281193
Loss in iteration 183 : 0.3735868659768626
Loss in iteration 184 : 0.3733689449856653
Loss in iteration 185 : 0.37363881068292226
Loss in iteration 186 : 0.3735077582999331
Loss in iteration 187 : 0.3741566757592711
Loss in iteration 188 : 0.37340478129729704
Loss in iteration 189 : 0.37387222206873466
Loss in iteration 190 : 0.37314814587741785
Loss in iteration 191 : 0.37314465132806396
Loss in iteration 192 : 0.3732548251350126
Loss in iteration 193 : 0.3737096368336337
Loss in iteration 194 : 0.37312624713625026
Loss in iteration 195 : 0.37318281458609925
Loss in iteration 196 : 0.37311505978594445
Loss in iteration 197 : 0.3735542410701721
Loss in iteration 198 : 0.37307285496849013
Loss in iteration 199 : 0.3734943865375754
Loss in iteration 200 : 0.37298349591562874
Testing accuracy  of updater 5 on alg 1 with rate 0.00196 = 0.78025, training accuracy 0.8361929426999029, time elapsed: 2447 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7426733636392217
Loss in iteration 3 : 0.6120601451577221
Loss in iteration 4 : 0.5754477902366294
Testing accuracy  of updater 5 on alg 1 with rate 4.9E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 40 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.255315226924871
Loss in iteration 3 : 2.964394697481796
Loss in iteration 4 : 3.002448526420269
Loss in iteration 5 : 2.60516507256122
Loss in iteration 6 : 1.900306715729107
Loss in iteration 7 : 1.0095218402391004
Loss in iteration 8 : 0.5521297555312628
Loss in iteration 9 : 0.8492357708222547
Loss in iteration 10 : 1.368175910103327
Loss in iteration 11 : 1.4891324955089666
Loss in iteration 12 : 1.2212116941434097
Loss in iteration 13 : 0.8811017693887412
Loss in iteration 14 : 0.7135465545812254
Loss in iteration 15 : 0.7451835272389278
Loss in iteration 16 : 0.8738308186377741
Loss in iteration 17 : 1.0045947348727426
Loss in iteration 18 : 1.0801884544384073
Loss in iteration 19 : 1.078433378508401
Loss in iteration 20 : 1.0114190468679063
Loss in iteration 21 : 0.9146864689282802
Loss in iteration 22 : 0.8242245641586543
Loss in iteration 23 : 0.7721517164577775
Loss in iteration 24 : 0.7800974512868041
Loss in iteration 25 : 0.8359479098066217
Loss in iteration 26 : 0.8908085563749465
Loss in iteration 27 : 0.9029468182490549
Loss in iteration 28 : 0.8651783514272422
Loss in iteration 29 : 0.7987092730128097
Loss in iteration 30 : 0.7446003218535258
Loss in iteration 31 : 0.7269785152470144
Loss in iteration 32 : 0.7408254203822419
Loss in iteration 33 : 0.7610154786410676
Loss in iteration 34 : 0.7676478602112187
Loss in iteration 35 : 0.7510377790777044
Loss in iteration 36 : 0.7140550048346637
Loss in iteration 37 : 0.6708373290027322
Loss in iteration 38 : 0.6394542068462352
Loss in iteration 39 : 0.6339606996813779
Loss in iteration 40 : 0.6417975029274883
Loss in iteration 41 : 0.6388322665901197
Loss in iteration 42 : 0.6097851808527192
Loss in iteration 43 : 0.5710088648703509
Loss in iteration 44 : 0.5453990954661727
Loss in iteration 45 : 0.539850635875805
Loss in iteration 46 : 0.5388558318348029
Loss in iteration 47 : 0.5207122907216186
Loss in iteration 48 : 0.48634104960122587
Loss in iteration 49 : 0.46229408869200994
Loss in iteration 50 : 0.45803290758886955
Loss in iteration 51 : 0.45345509059809325
Loss in iteration 52 : 0.4248071502590181
Loss in iteration 53 : 0.40206082715636415
Loss in iteration 54 : 0.4050725341357943
Loss in iteration 55 : 0.39617388603215353
Loss in iteration 56 : 0.36978002828239975
Loss in iteration 57 : 0.3884399966257077
Loss in iteration 58 : 0.3707924322817941
Loss in iteration 59 : 0.38638707902410135
Loss in iteration 60 : 0.3822945204971191
Loss in iteration 61 : 0.40228818337271854
Loss in iteration 62 : 0.38137296515657937
Loss in iteration 63 : 0.39556511112049453
Loss in iteration 64 : 0.38073953889266093
Loss in iteration 65 : 0.37899840890504
Loss in iteration 66 : 0.3741657446803799
Loss in iteration 67 : 0.36996027273265336
Loss in iteration 68 : 0.3682448155015317
Loss in iteration 69 : 0.37018631228550986
Loss in iteration 70 : 0.3667392661045107
Loss in iteration 71 : 0.3728220918068055
Loss in iteration 72 : 0.36852887359907116
Loss in iteration 73 : 0.3718427035921303
Loss in iteration 74 : 0.3718625742751309
Loss in iteration 75 : 0.3692803364351071
Loss in iteration 76 : 0.3715551288984039
Loss in iteration 77 : 0.3680720326391685
Loss in iteration 78 : 0.3686041941398574
Loss in iteration 79 : 0.3671780996217001
Loss in iteration 80 : 0.365554896228241
Loss in iteration 81 : 0.3660371169155914
Loss in iteration 82 : 0.36358539031285103
Loss in iteration 83 : 0.36585220608188757
Loss in iteration 84 : 0.36364275629166753
Loss in iteration 85 : 0.3660083365996717
Loss in iteration 86 : 0.36400332770454535
Loss in iteration 87 : 0.36645781679514156
Loss in iteration 88 : 0.36421986940844747
Loss in iteration 89 : 0.3655755517139541
Loss in iteration 90 : 0.36401957269470664
Loss in iteration 91 : 0.36388905992379655
Loss in iteration 92 : 0.36366057511256344
Loss in iteration 93 : 0.3630426555838562
Loss in iteration 94 : 0.3632235632340774
Loss in iteration 95 : 0.36291242767692256
Loss in iteration 96 : 0.3631076418976748
Loss in iteration 97 : 0.3631571264395752
Loss in iteration 98 : 0.36328628992351275
Loss in iteration 99 : 0.3632115773940817
Loss in iteration 100 : 0.3632975620726735
Loss in iteration 101 : 0.36318726884832075
Loss in iteration 102 : 0.36309058636291375
Loss in iteration 103 : 0.36297245863371186
Loss in iteration 104 : 0.36288374339113416
Loss in iteration 105 : 0.3627914061501677
Loss in iteration 106 : 0.362754435129033
Loss in iteration 107 : 0.36272430532151684
Loss in iteration 108 : 0.36272745482778046
Loss in iteration 109 : 0.3627406977007464
Loss in iteration 110 : 0.3628102374095584
Loss in iteration 111 : 0.36279462325896095
Loss in iteration 112 : 0.36283914851284416
Loss in iteration 113 : 0.3628446521245181
Loss in iteration 114 : 0.3628248976981743
Loss in iteration 115 : 0.3628492076693293
Loss in iteration 116 : 0.36280417615916
Loss in iteration 117 : 0.3627823586516364
Loss in iteration 118 : 0.3627986148181028
Loss in iteration 119 : 0.3627279496913674
Loss in iteration 120 : 0.362798292842529
Loss in iteration 121 : 0.3627588736352711
Testing accuracy  of updater 6 on alg 1 with rate 0.056 = 0.78975, training accuracy 0.842667529944966, time elapsed: 1479 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3200684674055576
Loss in iteration 3 : 1.8131351591562785
Loss in iteration 4 : 1.8947502870117503
Loss in iteration 5 : 1.61904087485805
Loss in iteration 6 : 1.0373739660498313
Loss in iteration 7 : 0.5215431613339977
Loss in iteration 8 : 0.6506057125703619
Loss in iteration 9 : 1.067601377676463
Loss in iteration 10 : 1.1642532159100278
Loss in iteration 11 : 0.9259627842557068
Loss in iteration 12 : 0.6917994084787197
Loss in iteration 13 : 0.6502171275986113
Loss in iteration 14 : 0.7510771622542682
Loss in iteration 15 : 0.8729813950595011
Loss in iteration 16 : 0.9442982432521472
Loss in iteration 17 : 0.9383335144151436
Loss in iteration 18 : 0.872716495947172
Loss in iteration 19 : 0.7904623777241792
Loss in iteration 20 : 0.7295083194657532
Loss in iteration 21 : 0.7196004327753037
Loss in iteration 22 : 0.7623453094469326
Loss in iteration 23 : 0.8142905339589775
Loss in iteration 24 : 0.827551088796477
Loss in iteration 25 : 0.7939534937411278
Loss in iteration 26 : 0.7363164708448089
Loss in iteration 27 : 0.6943446322946917
Loss in iteration 28 : 0.6872148444975547
Loss in iteration 29 : 0.7021666362384469
Loss in iteration 30 : 0.7157455221182935
Loss in iteration 31 : 0.7120673401539596
Loss in iteration 32 : 0.6866696472964356
Loss in iteration 33 : 0.6491329914036955
Loss in iteration 34 : 0.6158439620200984
Loss in iteration 35 : 0.6042345017698151
Loss in iteration 36 : 0.6077539327898424
Loss in iteration 37 : 0.6066407905497182
Loss in iteration 38 : 0.5856891003993472
Loss in iteration 39 : 0.5535420083345051
Loss in iteration 40 : 0.5283302298829702
Loss in iteration 41 : 0.5190361299361875
Loss in iteration 42 : 0.5172438523242582
Loss in iteration 43 : 0.503674453549465
Loss in iteration 44 : 0.47636132266323394
Loss in iteration 45 : 0.4536394632703702
Loss in iteration 46 : 0.4458675243812162
Loss in iteration 47 : 0.44210883076814145
Loss in iteration 48 : 0.4233085461725868
Loss in iteration 49 : 0.40076467980911495
Loss in iteration 50 : 0.39605782453584754
Loss in iteration 51 : 0.39405503441228557
Loss in iteration 52 : 0.3751284307074867
Loss in iteration 53 : 0.37276607736923956
Loss in iteration 54 : 0.37734473748013897
Loss in iteration 55 : 0.36609354863213445
Loss in iteration 56 : 0.3821055195492537
Loss in iteration 57 : 0.37341592324210915
Loss in iteration 58 : 0.3894102451033781
Loss in iteration 59 : 0.3787340937623205
Loss in iteration 60 : 0.3877029209052846
Loss in iteration 61 : 0.3758888594123189
Loss in iteration 62 : 0.3790919593116258
Loss in iteration 63 : 0.36899464803777204
Loss in iteration 64 : 0.3708137196210859
Loss in iteration 65 : 0.36501739114111
Loss in iteration 66 : 0.36770157124613445
Loss in iteration 67 : 0.3661613663746369
Loss in iteration 68 : 0.3661071727531981
Loss in iteration 69 : 0.3688973752396509
Loss in iteration 70 : 0.36639582009826727
Loss in iteration 71 : 0.36829303612664566
Loss in iteration 72 : 0.3684218308735872
Loss in iteration 73 : 0.3669269426321414
Loss in iteration 74 : 0.3680590076832314
Testing accuracy  of updater 6 on alg 1 with rate 0.0392 = 0.791, training accuracy 0.8442861767562317, time elapsed: 921 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8914613487763373
Loss in iteration 3 : 1.1674721961371248
Loss in iteration 4 : 1.2121042977185794
Loss in iteration 5 : 1.0530862895340538
Loss in iteration 6 : 0.7162341816264022
Loss in iteration 7 : 0.40353609537780694
Loss in iteration 8 : 0.5246663847255526
Loss in iteration 9 : 0.7544624309380129
Loss in iteration 10 : 0.7627798529914273
Loss in iteration 11 : 0.6019606069553997
Loss in iteration 12 : 0.484069291031556
Loss in iteration 13 : 0.48365867072397584
Loss in iteration 14 : 0.5561478269325476
Loss in iteration 15 : 0.624442381594876
Loss in iteration 16 : 0.6489670095455349
Loss in iteration 17 : 0.6273155904255963
Loss in iteration 18 : 0.5788315115700062
Loss in iteration 19 : 0.534089343058735
Loss in iteration 20 : 0.5168768160117432
Loss in iteration 21 : 0.5353650629475899
Loss in iteration 22 : 0.5680550120416054
Loss in iteration 23 : 0.5869722334419495
Loss in iteration 24 : 0.5791857978741534
Loss in iteration 25 : 0.5513154298683557
Loss in iteration 26 : 0.5226560635191925
Loss in iteration 27 : 0.5095864474131471
Loss in iteration 28 : 0.5149939597966028
Loss in iteration 29 : 0.526119267227912
Loss in iteration 30 : 0.5305914779582539
Loss in iteration 31 : 0.5223446749463605
Loss in iteration 32 : 0.5040324170849545
Loss in iteration 33 : 0.4845185579240664
Loss in iteration 34 : 0.47465566772730494
Loss in iteration 35 : 0.47447546708825566
Loss in iteration 36 : 0.4766186193041341
Loss in iteration 37 : 0.4733405080351442
Loss in iteration 38 : 0.4607854414179851
Loss in iteration 39 : 0.4455714142927964
Loss in iteration 40 : 0.43519262749366916
Loss in iteration 41 : 0.4323960956526264
Loss in iteration 42 : 0.4324624214340887
Loss in iteration 43 : 0.4267581964389939
Loss in iteration 44 : 0.41449975471898476
Loss in iteration 45 : 0.4032521861495464
Loss in iteration 46 : 0.39826116247248916
Loss in iteration 47 : 0.3983673356755489
Loss in iteration 48 : 0.3947063247999658
Loss in iteration 49 : 0.3841122558705419
Loss in iteration 50 : 0.3770296676043693
Loss in iteration 51 : 0.37759567773916003
Loss in iteration 52 : 0.37779151850430165
Loss in iteration 53 : 0.37100534133988444
Loss in iteration 54 : 0.3658884939322095
Loss in iteration 55 : 0.369915605305471
Loss in iteration 56 : 0.3691389539791104
Loss in iteration 57 : 0.3648238425086657
Loss in iteration 58 : 0.36903822411181747
Loss in iteration 59 : 0.3706180350484543
Loss in iteration 60 : 0.36744418779131477
Loss in iteration 61 : 0.37028390097762137
Loss in iteration 62 : 0.37107140761869467
Loss in iteration 63 : 0.36810385595274353
Loss in iteration 64 : 0.36983274909091113
Loss in iteration 65 : 0.36897719105978305
Loss in iteration 66 : 0.3664053044341966
Loss in iteration 67 : 0.367508233563783
Loss in iteration 68 : 0.36613559594776923
Loss in iteration 69 : 0.36456345295572107
Loss in iteration 70 : 0.36501544497822225
Loss in iteration 71 : 0.36473572480483507
Loss in iteration 72 : 0.3637339897685736
Loss in iteration 73 : 0.3639543141412785
Loss in iteration 74 : 0.364274725500584
Loss in iteration 75 : 0.36378499345170046
Loss in iteration 76 : 0.36365962226212734
Loss in iteration 77 : 0.36410369496135336
Loss in iteration 78 : 0.3639875577032521
Loss in iteration 79 : 0.3636569750306732
Loss in iteration 80 : 0.3638682947078798
Loss in iteration 81 : 0.36392835803588874
Loss in iteration 82 : 0.3636626744098338
Loss in iteration 83 : 0.3636639589092671
Loss in iteration 84 : 0.3637472691838438
Loss in iteration 85 : 0.36356005973527716
Loss in iteration 86 : 0.3633692359896238
Loss in iteration 87 : 0.3634231690608733
Loss in iteration 88 : 0.3633237025969819
Loss in iteration 89 : 0.3631312205617311
Loss in iteration 90 : 0.36314335920394
Loss in iteration 91 : 0.3631268638776036
Loss in iteration 92 : 0.3629658371079252
Loss in iteration 93 : 0.3629422350525029
Loss in iteration 94 : 0.36297033097796355
Loss in iteration 95 : 0.3628627212845607
Loss in iteration 96 : 0.3628620848641968
Loss in iteration 97 : 0.36290988895957416
Loss in iteration 98 : 0.36283091053149497
Loss in iteration 99 : 0.3628791122042943
Loss in iteration 100 : 0.3628940918098435
Loss in iteration 101 : 0.3628460730388041
Loss in iteration 102 : 0.36290038008306974
Loss in iteration 103 : 0.3628810243087317
Loss in iteration 104 : 0.3628567411086228
Loss in iteration 105 : 0.3628790638215003
Loss in iteration 106 : 0.36283779333192046
Loss in iteration 107 : 0.36282326579844615
Loss in iteration 108 : 0.36282082292828693
Loss in iteration 109 : 0.36278422519302483
Loss in iteration 110 : 0.3627899604400356
Loss in iteration 111 : 0.36277136791576015
Loss in iteration 112 : 0.36275419780968543
Loss in iteration 113 : 0.36277164738344403
Loss in iteration 114 : 0.36274260198155067
Loss in iteration 115 : 0.36275794400822065
Loss in iteration 116 : 0.36275825509259124
Loss in iteration 117 : 0.3627359478687011
Loss in iteration 118 : 0.3627548345026313
Loss in iteration 119 : 0.3627386590858946
Loss in iteration 120 : 0.3627430932105304
Loss in iteration 121 : 0.36273999272510027
Loss in iteration 122 : 0.3627393537663302
Loss in iteration 123 : 0.36274781336326795
Loss in iteration 124 : 0.3627308764437462
Loss in iteration 125 : 0.36275194143025197
Loss in iteration 126 : 0.36273866233874197
Loss in iteration 127 : 0.3627372251547141
Loss in iteration 128 : 0.3627477029773276
Loss in iteration 129 : 0.3627252037221874
Loss in iteration 130 : 0.36275190169683785
Loss in iteration 131 : 0.36273466857835923
Loss in iteration 132 : 0.36273335619195596
Loss in iteration 133 : 0.36274006724867813
Loss in iteration 134 : 0.36272015308849187
Testing accuracy  of updater 6 on alg 1 with rate 0.0224 = 0.7905, training accuracy 0.8423438005827129, time elapsed: 1334 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5505277434314294
Loss in iteration 3 : 0.6417567820325407
Loss in iteration 4 : 0.7072058739461672
Loss in iteration 5 : 0.7100831568751239
Loss in iteration 6 : 0.6573360784730992
Loss in iteration 7 : 0.5581282919173358
Loss in iteration 8 : 0.44846166185391534
Loss in iteration 9 : 0.4077178086441538
Loss in iteration 10 : 0.4654154964612923
Loss in iteration 11 : 0.5051765207227493
Loss in iteration 12 : 0.47988622649372703
Loss in iteration 13 : 0.4214073172013871
Loss in iteration 14 : 0.3801898337678097
Loss in iteration 15 : 0.37767123386742285
Loss in iteration 16 : 0.39703016885817083
Loss in iteration 17 : 0.41625867511721404
Loss in iteration 18 : 0.4220125586977992
Loss in iteration 19 : 0.4132489968252526
Loss in iteration 20 : 0.39730490345693903
Loss in iteration 21 : 0.38368462975732254
Loss in iteration 22 : 0.37914255027648264
Loss in iteration 23 : 0.3830296165755795
Loss in iteration 24 : 0.3917049470158254
Loss in iteration 25 : 0.3985295656154455
Loss in iteration 26 : 0.4005445015343647
Loss in iteration 27 : 0.3975996124465539
Loss in iteration 28 : 0.39163715417150885
Loss in iteration 29 : 0.3859118241697174
Loss in iteration 30 : 0.3831851125659318
Loss in iteration 31 : 0.38395911161995133
Loss in iteration 32 : 0.38662590159165733
Loss in iteration 33 : 0.3894000292675379
Loss in iteration 34 : 0.3905367206852974
Loss in iteration 35 : 0.38964585270460106
Loss in iteration 36 : 0.38720850065302986
Loss in iteration 37 : 0.3842479060851624
Loss in iteration 38 : 0.3821618202700305
Loss in iteration 39 : 0.3812065604169311
Loss in iteration 40 : 0.3816428159748225
Loss in iteration 41 : 0.3825564300218766
Loss in iteration 42 : 0.3830894663295442
Loss in iteration 43 : 0.38253294324232784
Loss in iteration 44 : 0.3809675690391178
Loss in iteration 45 : 0.3791131312317051
Loss in iteration 46 : 0.37767410009866287
Loss in iteration 47 : 0.3771949959781009
Loss in iteration 48 : 0.3771882435895334
Loss in iteration 49 : 0.37725384109618315
Loss in iteration 50 : 0.3770537764105528
Loss in iteration 51 : 0.37640784472265315
Loss in iteration 52 : 0.3753915451231143
Loss in iteration 53 : 0.37428635568776036
Loss in iteration 54 : 0.37337588491671714
Loss in iteration 55 : 0.37293971667946013
Loss in iteration 56 : 0.3728448208706288
Loss in iteration 57 : 0.37276707517512137
Loss in iteration 58 : 0.3724152034370117
Loss in iteration 59 : 0.3717364630366527
Loss in iteration 60 : 0.3709622949611886
Loss in iteration 61 : 0.37047842019451727
Loss in iteration 62 : 0.37033562622000593
Loss in iteration 63 : 0.37028185064198443
Loss in iteration 64 : 0.37011423081125533
Loss in iteration 65 : 0.3697733698206464
Loss in iteration 66 : 0.3693584997130418
Loss in iteration 67 : 0.3690678757513274
Loss in iteration 68 : 0.3689421819513698
Loss in iteration 69 : 0.36890294158377046
Loss in iteration 70 : 0.36883001983998165
Loss in iteration 71 : 0.3686767442476523
Loss in iteration 72 : 0.3684649572983245
Loss in iteration 73 : 0.36828348593776483
Loss in iteration 74 : 0.3681996868072365
Loss in iteration 75 : 0.36817493547873703
Loss in iteration 76 : 0.3681456674465909
Loss in iteration 77 : 0.3680540789771809
Loss in iteration 78 : 0.36792920570818427
Loss in iteration 79 : 0.3678310593288047
Loss in iteration 80 : 0.3677694113677842
Loss in iteration 81 : 0.3677445254892163
Loss in iteration 82 : 0.3677030299420105
Loss in iteration 83 : 0.36763325032531435
Loss in iteration 84 : 0.3675532522162236
Loss in iteration 85 : 0.3674892235085586
Loss in iteration 86 : 0.36744688702001044
Loss in iteration 87 : 0.3674068941297892
Loss in iteration 88 : 0.3673590810052433
Loss in iteration 89 : 0.36730076607356477
Loss in iteration 90 : 0.36723444718038534
Loss in iteration 91 : 0.3671682926525649
Loss in iteration 92 : 0.3671065201135005
Loss in iteration 93 : 0.3670557330383554
Loss in iteration 94 : 0.3670138178942858
Loss in iteration 95 : 0.36695490044329715
Loss in iteration 96 : 0.36688775175976085
Loss in iteration 97 : 0.3668348504189131
Loss in iteration 98 : 0.36678485112871045
Loss in iteration 99 : 0.3667362102419649
Loss in iteration 100 : 0.3666847132822735
Loss in iteration 101 : 0.3666301852312211
Loss in iteration 102 : 0.36657485727967964
Loss in iteration 103 : 0.3665273412546564
Loss in iteration 104 : 0.36648388038054763
Loss in iteration 105 : 0.36643862903179225
Loss in iteration 106 : 0.3663893239111097
Loss in iteration 107 : 0.3663400315186048
Loss in iteration 108 : 0.3662951801233216
Loss in iteration 109 : 0.36625295514729334
Loss in iteration 110 : 0.36621286033742995
Loss in iteration 111 : 0.36617078247286644
Loss in iteration 112 : 0.36612585740690273
Loss in iteration 113 : 0.366080826283878
Loss in iteration 114 : 0.3660377204944338
Loss in iteration 115 : 0.36599936729647503
Loss in iteration 116 : 0.36596146953057923
Loss in iteration 117 : 0.3659209079136461
Loss in iteration 118 : 0.36587969162036527
Loss in iteration 119 : 0.36583950567005125
Loss in iteration 120 : 0.36579938286613617
Loss in iteration 121 : 0.36575957789644004
Loss in iteration 122 : 0.3657224675691249
Loss in iteration 123 : 0.3656851399005655
Loss in iteration 124 : 0.36564873314927565
Loss in iteration 125 : 0.36561075157531076
Loss in iteration 126 : 0.3655727685939697
Loss in iteration 127 : 0.36553473289180644
Loss in iteration 128 : 0.3654972579635405
Loss in iteration 129 : 0.36546259880285714
Loss in iteration 130 : 0.3654274873407137
Loss in iteration 131 : 0.36539354829262655
Loss in iteration 132 : 0.36535850963329825
Loss in iteration 133 : 0.3653237412978402
Loss in iteration 134 : 0.3652896194890415
Loss in iteration 135 : 0.3652556288333807
Loss in iteration 136 : 0.3652221993068897
Loss in iteration 137 : 0.36519174531835397
Loss in iteration 138 : 0.3651613915713027
Loss in iteration 139 : 0.36513028964357497
Loss in iteration 140 : 0.365098847367119
Loss in iteration 141 : 0.3650685201003808
Loss in iteration 142 : 0.36503908806915747
Loss in iteration 143 : 0.365010186413672
Loss in iteration 144 : 0.36498132606792205
Loss in iteration 145 : 0.36495234845569074
Loss in iteration 146 : 0.364923544700363
Loss in iteration 147 : 0.36489530907617523
Loss in iteration 148 : 0.36486800844171635
Loss in iteration 149 : 0.3648406876180072
Loss in iteration 150 : 0.3648142199345073
Loss in iteration 151 : 0.3647881698465246
Loss in iteration 152 : 0.36476314823410644
Loss in iteration 153 : 0.3647381280914947
Loss in iteration 154 : 0.3647129781210684
Loss in iteration 155 : 0.3646881145724705
Loss in iteration 156 : 0.3646640992421819
Loss in iteration 157 : 0.36464037035069125
Loss in iteration 158 : 0.36461684893539215
Loss in iteration 159 : 0.3645934453724819
Loss in iteration 160 : 0.3645711123731146
Loss in iteration 161 : 0.3645486453766508
Loss in iteration 162 : 0.3645263495523458
Loss in iteration 163 : 0.36450436524621954
Loss in iteration 164 : 0.3644824518092005
Loss in iteration 165 : 0.36446060019937015
Loss in iteration 166 : 0.36443909054686885
Loss in iteration 167 : 0.3644186718839014
Loss in iteration 168 : 0.36439826077766174
Loss in iteration 169 : 0.36437690991245697
Loss in iteration 170 : 0.36435623357141
Loss in iteration 171 : 0.36433743837130117
Loss in iteration 172 : 0.36431752101137815
Loss in iteration 173 : 0.36429818088389454
Loss in iteration 174 : 0.3642800071597013
Loss in iteration 175 : 0.3642621095099605
Loss in iteration 176 : 0.3642440399755632
Loss in iteration 177 : 0.36422601830485124
Loss in iteration 178 : 0.3642084386057735
Loss in iteration 179 : 0.36419096380476185
Loss in iteration 180 : 0.36417466165551293
Loss in iteration 181 : 0.3641570955412374
Loss in iteration 182 : 0.36413999925025564
Loss in iteration 183 : 0.3641231632475458
Loss in iteration 184 : 0.364106564600793
Loss in iteration 185 : 0.3640902165149269
Loss in iteration 186 : 0.36407372855196357
Loss in iteration 187 : 0.3640572099076541
Loss in iteration 188 : 0.36404085299681854
Loss in iteration 189 : 0.3640249267153613
Loss in iteration 190 : 0.36400935110759086
Loss in iteration 191 : 0.36399350506586137
Loss in iteration 192 : 0.3639782563623613
Loss in iteration 193 : 0.36396304078582226
Loss in iteration 194 : 0.3639480061706701
Loss in iteration 195 : 0.36393327245958673
Loss in iteration 196 : 0.3639184568700365
Loss in iteration 197 : 0.36390435418884465
Loss in iteration 198 : 0.3638902236334268
Loss in iteration 199 : 0.3638762929295948
Loss in iteration 200 : 0.36386272336222264
Testing accuracy  of updater 6 on alg 1 with rate 0.0056 = 0.78875, training accuracy 0.8420200712204597, time elapsed: 1977 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5718556000436639
Loss in iteration 3 : 0.6197559484275003
Loss in iteration 4 : 0.7031357496969074
Loss in iteration 5 : 0.7336524423478532
Loss in iteration 6 : 0.7164698391634382
Loss in iteration 7 : 0.6573285951306448
Loss in iteration 8 : 0.563352804633392
Loss in iteration 9 : 0.46092384001251935
Loss in iteration 10 : 0.4065801197660419
Loss in iteration 11 : 0.4433378708285453
Loss in iteration 12 : 0.4971373225633514
Loss in iteration 13 : 0.5034214077041368
Loss in iteration 14 : 0.4631401418768958
Loss in iteration 15 : 0.41004059154437295
Loss in iteration 16 : 0.37860431148825907
Loss in iteration 17 : 0.37843501457173
Loss in iteration 18 : 0.3954763617200087
Loss in iteration 19 : 0.4131234218398119
Loss in iteration 20 : 0.4207883517313141
Loss in iteration 21 : 0.4157081512091623
Loss in iteration 22 : 0.4026611082188054
Loss in iteration 23 : 0.3881776442838234
Loss in iteration 24 : 0.37899079217242004
Loss in iteration 25 : 0.37736090722448334
Loss in iteration 26 : 0.3820045765637481
Loss in iteration 27 : 0.38921028797020746
Loss in iteration 28 : 0.39456391358187104
Loss in iteration 29 : 0.395987169530691
Loss in iteration 30 : 0.3934554444868402
Loss in iteration 31 : 0.38844829561226
Loss in iteration 32 : 0.38319860706913633
Loss in iteration 33 : 0.38024508565038473
Loss in iteration 34 : 0.379973312143955
Loss in iteration 35 : 0.3815917018273426
Loss in iteration 36 : 0.38380696692493266
Loss in iteration 37 : 0.3855604724815781
Loss in iteration 38 : 0.3859230101560098
Loss in iteration 39 : 0.3848185763802756
Loss in iteration 40 : 0.3827627977387559
Loss in iteration 41 : 0.3807024977345867
Loss in iteration 42 : 0.3791380954862663
Loss in iteration 43 : 0.3784094673155801
Loss in iteration 44 : 0.3785176959818289
Loss in iteration 45 : 0.37915048244648497
Loss in iteration 46 : 0.3797490724365194
Loss in iteration 47 : 0.3797447811770187
Loss in iteration 48 : 0.3789644522751803
Loss in iteration 49 : 0.37773298188168686
Loss in iteration 50 : 0.3765676581516865
Loss in iteration 51 : 0.3758193231615782
Loss in iteration 52 : 0.37550525232643767
Loss in iteration 53 : 0.3755305743387573
Loss in iteration 54 : 0.37558486100099114
Loss in iteration 55 : 0.37551283552647535
Loss in iteration 56 : 0.3751453082574349
Loss in iteration 57 : 0.3745123361554404
Loss in iteration 58 : 0.3737600069494084
Loss in iteration 59 : 0.3731521226521856
Loss in iteration 60 : 0.37278632138167395
Loss in iteration 61 : 0.3726832908372449
Loss in iteration 62 : 0.3726400287484907
Loss in iteration 63 : 0.372509232353445
Loss in iteration 64 : 0.37220522939508016
Loss in iteration 65 : 0.3717711045857758
Loss in iteration 66 : 0.3713090344193716
Loss in iteration 67 : 0.3710177295876658
Loss in iteration 68 : 0.3708900685719527
Loss in iteration 69 : 0.3708222188800111
Loss in iteration 70 : 0.3707298505503962
Loss in iteration 71 : 0.3705548890828056
Loss in iteration 72 : 0.3703145848038525
Loss in iteration 73 : 0.37007553858504244
Loss in iteration 74 : 0.36989180832101304
Loss in iteration 75 : 0.36977996224934545
Loss in iteration 76 : 0.369727519051047
Loss in iteration 77 : 0.36964536097620004
Loss in iteration 78 : 0.36952873707773415
Loss in iteration 79 : 0.36940041365915716
Loss in iteration 80 : 0.36928106452244513
Loss in iteration 81 : 0.3691951043868505
Loss in iteration 82 : 0.3691411412873387
Loss in iteration 83 : 0.36909234281263764
Loss in iteration 84 : 0.3690283845569897
Loss in iteration 85 : 0.3689533196992189
Loss in iteration 86 : 0.3688725297810862
Loss in iteration 87 : 0.3688050674622148
Loss in iteration 88 : 0.3687485585325333
Loss in iteration 89 : 0.3687021874661779
Loss in iteration 90 : 0.3686590753850425
Loss in iteration 91 : 0.36860617819348546
Loss in iteration 92 : 0.3685482029616237
Loss in iteration 93 : 0.3684881466288944
Loss in iteration 94 : 0.3684369450288246
Loss in iteration 95 : 0.3683953813966302
Loss in iteration 96 : 0.3683573795996268
Loss in iteration 97 : 0.3683114434078299
Loss in iteration 98 : 0.36825847543874696
Loss in iteration 99 : 0.36820645898042753
Loss in iteration 100 : 0.3681631162638582
Loss in iteration 101 : 0.368121379868344
Loss in iteration 102 : 0.3680781790572216
Loss in iteration 103 : 0.368033257154476
Loss in iteration 104 : 0.3679867288183765
Loss in iteration 105 : 0.3679397327125858
Loss in iteration 106 : 0.367894744955515
Loss in iteration 107 : 0.3678516087481861
Loss in iteration 108 : 0.3678082601685504
Loss in iteration 109 : 0.3677626092698509
Loss in iteration 110 : 0.36771777326788335
Loss in iteration 111 : 0.367675535276836
Loss in iteration 112 : 0.36763303103164197
Loss in iteration 113 : 0.3675896000602803
Loss in iteration 114 : 0.36754825359621945
Loss in iteration 115 : 0.3675083920841369
Loss in iteration 116 : 0.3674689424625189
Loss in iteration 117 : 0.3674297209081212
Loss in iteration 118 : 0.3673906246906185
Loss in iteration 119 : 0.3673511910988723
Loss in iteration 120 : 0.367312121467217
Loss in iteration 121 : 0.3672731219731033
Loss in iteration 122 : 0.36723482626815057
Loss in iteration 123 : 0.3671972434120336
Loss in iteration 124 : 0.3671597895625043
Loss in iteration 125 : 0.3671223261176027
Loss in iteration 126 : 0.3670853184925866
Loss in iteration 127 : 0.36704920949960673
Loss in iteration 128 : 0.36701326088876796
Loss in iteration 129 : 0.3669778150539708
Loss in iteration 130 : 0.36694310046941786
Loss in iteration 131 : 0.3669077776456895
Loss in iteration 132 : 0.36687319484859476
Loss in iteration 133 : 0.36683943142527975
Loss in iteration 134 : 0.36680566065153525
Loss in iteration 135 : 0.36677239915263926
Loss in iteration 136 : 0.36673976174542805
Loss in iteration 137 : 0.3667072538883214
Loss in iteration 138 : 0.366675301098995
Loss in iteration 139 : 0.36664380361790416
Loss in iteration 140 : 0.3666124384109166
Loss in iteration 141 : 0.3665809910415642
Loss in iteration 142 : 0.3665495321500069
Loss in iteration 143 : 0.3665182585312175
Loss in iteration 144 : 0.36648768308218893
Loss in iteration 145 : 0.3664572039347975
Loss in iteration 146 : 0.3664268534641668
Loss in iteration 147 : 0.36639659135803626
Loss in iteration 148 : 0.36636621681617487
Loss in iteration 149 : 0.3663361234831033
Loss in iteration 150 : 0.36630629640695694
Loss in iteration 151 : 0.3662768200334664
Loss in iteration 152 : 0.3662475858639084
Loss in iteration 153 : 0.3662183265441262
Loss in iteration 154 : 0.3661890429658521
Loss in iteration 155 : 0.3661598820370017
Loss in iteration 156 : 0.36613101318559654
Loss in iteration 157 : 0.36610224620832016
Loss in iteration 158 : 0.36607393863833565
Loss in iteration 159 : 0.36604567801569765
Loss in iteration 160 : 0.3660178002285222
Loss in iteration 161 : 0.3659904811404282
Loss in iteration 162 : 0.3659632952538181
Loss in iteration 163 : 0.36593619629429364
Loss in iteration 164 : 0.3659091741805239
Loss in iteration 165 : 0.36588221983617086
Loss in iteration 166 : 0.36585532508989693
Loss in iteration 167 : 0.36582848258531264
Loss in iteration 168 : 0.36580199970187355
Loss in iteration 169 : 0.3657757054595581
Loss in iteration 170 : 0.3657497932182542
Loss in iteration 171 : 0.3657237496790595
Loss in iteration 172 : 0.3656979511855451
Loss in iteration 173 : 0.3656715794727648
Loss in iteration 174 : 0.3656445732086664
Loss in iteration 175 : 0.36561861831780496
Loss in iteration 176 : 0.36559452589651276
Loss in iteration 177 : 0.365569752514919
Loss in iteration 178 : 0.36554470526301736
Loss in iteration 179 : 0.36551993581123454
Loss in iteration 180 : 0.36549579412637706
Loss in iteration 181 : 0.3654717855678728
Loss in iteration 182 : 0.3654478318136418
Loss in iteration 183 : 0.36542392603922097
Loss in iteration 184 : 0.3654000621015441
Loss in iteration 185 : 0.3653762344710697
Loss in iteration 186 : 0.3653524381706543
Loss in iteration 187 : 0.3653291066686072
Loss in iteration 188 : 0.36530591505300103
Loss in iteration 189 : 0.3652830415889703
Loss in iteration 190 : 0.36526089656080624
Loss in iteration 191 : 0.3652388306606725
Loss in iteration 192 : 0.3652168770705732
Loss in iteration 193 : 0.3651948296844441
Loss in iteration 194 : 0.3651728122499752
Loss in iteration 195 : 0.3651510709251179
Loss in iteration 196 : 0.36512978264329027
Loss in iteration 197 : 0.3651085873688443
Loss in iteration 198 : 0.365087398608771
Loss in iteration 199 : 0.36506592682947886
Loss in iteration 200 : 0.36504557970677737
Testing accuracy  of updater 6 on alg 1 with rate 0.00392 = 0.788, training accuracy 0.8416963418582065, time elapsed: 1957 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6828202493552278
Loss in iteration 3 : 0.5595649649914104
Loss in iteration 4 : 0.6370670627241208
Loss in iteration 5 : 0.6906604843707115
Loss in iteration 6 : 0.7125520895288886
Loss in iteration 7 : 0.7058713094846987
Loss in iteration 8 : 0.6738468313608329
Loss in iteration 9 : 0.6198352504349727
Loss in iteration 10 : 0.54897519225275
Loss in iteration 11 : 0.47741802755470586
Loss in iteration 12 : 0.4284905820569069
Loss in iteration 13 : 0.419220644076224
Loss in iteration 14 : 0.4484785864419326
Loss in iteration 15 : 0.47855668578977467
Loss in iteration 16 : 0.4849000635955876
Loss in iteration 17 : 0.4651664530332244
Loss in iteration 18 : 0.43150547796875255
Loss in iteration 19 : 0.40001763793292877
Loss in iteration 20 : 0.38228752527360543
Loss in iteration 21 : 0.3800395579879366
Loss in iteration 22 : 0.38774042920288254
Loss in iteration 23 : 0.3978430808172309
Loss in iteration 24 : 0.4045902815158066
Loss in iteration 25 : 0.4054082078089357
Loss in iteration 26 : 0.4008054081454421
Loss in iteration 27 : 0.3928546004902096
Loss in iteration 28 : 0.3843523894978733
Loss in iteration 29 : 0.37785370030185184
Loss in iteration 30 : 0.374363896384049
Loss in iteration 31 : 0.37453559617884674
Loss in iteration 32 : 0.37704338575270624
Loss in iteration 33 : 0.38025456513373374
Loss in iteration 34 : 0.38283512993490676
Loss in iteration 35 : 0.3838003414442575
Loss in iteration 36 : 0.38293374797315494
Loss in iteration 37 : 0.3808411416681083
Loss in iteration 38 : 0.3782763444071821
Loss in iteration 39 : 0.3759419489700467
Loss in iteration 40 : 0.37452823030865495
Loss in iteration 41 : 0.3742400307170485
Loss in iteration 42 : 0.374841117390221
Loss in iteration 43 : 0.37572750674995703
Loss in iteration 44 : 0.37659510460282564
Loss in iteration 45 : 0.3771289064354408
Loss in iteration 46 : 0.3771767038221098
Loss in iteration 47 : 0.37675506913618717
Loss in iteration 48 : 0.3760150195562269
Loss in iteration 49 : 0.3752116152676657
Loss in iteration 50 : 0.3744874696569061
Loss in iteration 51 : 0.37395168949506713
Loss in iteration 52 : 0.3737120984209139
Loss in iteration 53 : 0.3738433071982287
Loss in iteration 54 : 0.3741233461164545
Loss in iteration 55 : 0.3743661676671809
Loss in iteration 56 : 0.37444938323742805
Loss in iteration 57 : 0.37434041335554996
Loss in iteration 58 : 0.37406034126258764
Loss in iteration 59 : 0.37369627134927563
Loss in iteration 60 : 0.37330613399995416
Loss in iteration 61 : 0.3730273362007132
Loss in iteration 62 : 0.37289150842282864
Loss in iteration 63 : 0.37286320340348034
Loss in iteration 64 : 0.3729165716191162
Loss in iteration 65 : 0.37295426348288435
Loss in iteration 66 : 0.3729253618106736
Loss in iteration 67 : 0.37282271583120763
Loss in iteration 68 : 0.37266090665954404
Loss in iteration 69 : 0.37245808393974406
Loss in iteration 70 : 0.3722784976611812
Loss in iteration 71 : 0.3721391138861847
Loss in iteration 72 : 0.3720447835143818
Loss in iteration 73 : 0.37199660140946744
Loss in iteration 74 : 0.3719782774227882
Loss in iteration 75 : 0.3719523788249272
Loss in iteration 76 : 0.37189212227641494
Loss in iteration 77 : 0.37180175252611913
Loss in iteration 78 : 0.3716969891521089
Loss in iteration 79 : 0.3715902896734683
Loss in iteration 80 : 0.3714969973559892
Loss in iteration 81 : 0.37141713918797553
Loss in iteration 82 : 0.3713567273679491
Loss in iteration 83 : 0.3713057082549959
Loss in iteration 84 : 0.3712636117733391
Loss in iteration 85 : 0.37121891651068234
Loss in iteration 86 : 0.3711591888922859
Loss in iteration 87 : 0.3710898487201516
Loss in iteration 88 : 0.3710210368733158
Loss in iteration 89 : 0.3709592047439651
Loss in iteration 90 : 0.3709021315186354
Loss in iteration 91 : 0.37084933935194114
Loss in iteration 92 : 0.37080333577676017
Loss in iteration 93 : 0.37076341094119597
Loss in iteration 94 : 0.3707193317879099
Loss in iteration 95 : 0.3706732787093231
Loss in iteration 96 : 0.370625342528249
Loss in iteration 97 : 0.37057624248523136
Loss in iteration 98 : 0.3705277601090264
Loss in iteration 99 : 0.3704794824050917
Loss in iteration 100 : 0.3704333214208922
Loss in iteration 101 : 0.3703910365530577
Loss in iteration 102 : 0.3703531315252897
Loss in iteration 103 : 0.37031560380620826
Loss in iteration 104 : 0.37027701346398706
Loss in iteration 105 : 0.37023679227186596
Loss in iteration 106 : 0.37019502250066577
Loss in iteration 107 : 0.3701554682768549
Loss in iteration 108 : 0.3701177846934479
Loss in iteration 109 : 0.3700810416126657
Loss in iteration 110 : 0.37004521780437793
Loss in iteration 111 : 0.370010005343517
Loss in iteration 112 : 0.36997475503789495
Loss in iteration 113 : 0.36993946960991214
Loss in iteration 114 : 0.3699041515183027
Loss in iteration 115 : 0.36986888085474734
Loss in iteration 116 : 0.3698342099249272
Loss in iteration 117 : 0.36979963660220355
Loss in iteration 118 : 0.3697653150342095
Loss in iteration 119 : 0.36973222605812445
Loss in iteration 120 : 0.3696993632911047
Loss in iteration 121 : 0.36966656007091914
Loss in iteration 122 : 0.369634586213575
Loss in iteration 123 : 0.36960275527031594
Loss in iteration 124 : 0.36957105860512623
Loss in iteration 125 : 0.3695399455937294
Loss in iteration 126 : 0.3695090537942085
Loss in iteration 127 : 0.3694782763391792
Loss in iteration 128 : 0.36944760119678377
Loss in iteration 129 : 0.36941701753338707
Loss in iteration 130 : 0.3693865155943439
Loss in iteration 131 : 0.36935608659663327
Loss in iteration 132 : 0.3693257226321733
Loss in iteration 133 : 0.36929541658075504
Loss in iteration 134 : 0.36926516203164306
Loss in iteration 135 : 0.36923533856547974
Loss in iteration 136 : 0.3692055707290885
Loss in iteration 137 : 0.36917583404061716
Loss in iteration 138 : 0.36914620895767297
Loss in iteration 139 : 0.3691167007884625
Loss in iteration 140 : 0.36908723142150157
Loss in iteration 141 : 0.3690577963137092
Loss in iteration 142 : 0.3690283913761527
Loss in iteration 143 : 0.36899919994601643
Loss in iteration 144 : 0.3689704993543705
Loss in iteration 145 : 0.36894204126537494
Loss in iteration 146 : 0.3689136917052436
Loss in iteration 147 : 0.3688859467770745
Loss in iteration 148 : 0.3688582889906877
Loss in iteration 149 : 0.36883067711376205
Loss in iteration 150 : 0.36880310590134213
Loss in iteration 151 : 0.36877573095224847
Loss in iteration 152 : 0.36874842764205745
Loss in iteration 153 : 0.3687212154083395
Loss in iteration 154 : 0.36869399838458317
Loss in iteration 155 : 0.36866709988803753
Loss in iteration 156 : 0.36864029887471716
Loss in iteration 157 : 0.36861347407136574
Loss in iteration 158 : 0.3685866582873024
Loss in iteration 159 : 0.36856011245885845
Loss in iteration 160 : 0.3685336941612165
Loss in iteration 161 : 0.3685075294567072
Loss in iteration 162 : 0.36848136725001673
Loss in iteration 163 : 0.36845520663718895
Loss in iteration 164 : 0.3684292100615406
Loss in iteration 165 : 0.3684037213227939
Loss in iteration 166 : 0.368378312067029
Loss in iteration 167 : 0.368353016510844
Loss in iteration 168 : 0.36832800862848647
Loss in iteration 169 : 0.36830338970272564
Loss in iteration 170 : 0.3682786583844569
Loss in iteration 171 : 0.36825369879609116
Loss in iteration 172 : 0.36822870721744894
Loss in iteration 173 : 0.36820409114633107
Loss in iteration 174 : 0.3681800857971828
Loss in iteration 175 : 0.3681558761703844
Loss in iteration 176 : 0.3681315436695638
Loss in iteration 177 : 0.3681074160666805
Loss in iteration 178 : 0.36808348458131834
Loss in iteration 179 : 0.36805958187129467
Loss in iteration 180 : 0.3680357043819695
Loss in iteration 181 : 0.3680119796069654
Loss in iteration 182 : 0.36798853833308026
Loss in iteration 183 : 0.3679651389232793
Loss in iteration 184 : 0.36794200193380416
Loss in iteration 185 : 0.3679190121904778
Loss in iteration 186 : 0.3678961269636087
Loss in iteration 187 : 0.36787325162555984
Loss in iteration 188 : 0.3678504286929147
Loss in iteration 189 : 0.3678277274404672
Loss in iteration 190 : 0.3678050458653389
Loss in iteration 191 : 0.36778238126803964
Loss in iteration 192 : 0.36775973122063504
Loss in iteration 193 : 0.36773733477894727
Loss in iteration 194 : 0.3677150145526564
Loss in iteration 195 : 0.367692711784749
Loss in iteration 196 : 0.3676704239704375
Loss in iteration 197 : 0.3676481488571991
Loss in iteration 198 : 0.3676261038318664
Loss in iteration 199 : 0.3676044622288777
Loss in iteration 200 : 0.36758274788974377
Testing accuracy  of updater 6 on alg 1 with rate 0.0022400000000000002 = 0.78625, training accuracy 0.8397539656846876, time elapsed: 1842 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9164507178862157
Loss in iteration 3 : 0.7688333933744828
Loss in iteration 4 : 0.6200317304950411
Loss in iteration 5 : 0.554352064019294
Loss in iteration 6 : 0.5600400401513229
Loss in iteration 7 : 0.5868897339811376
Loss in iteration 8 : 0.6119702052314164
Loss in iteration 9 : 0.6298700755958855
Loss in iteration 10 : 0.6398752853146026
Loss in iteration 11 : 0.6426755784595106
Loss in iteration 12 : 0.6390523937812342
Loss in iteration 13 : 0.6297250448302395
Loss in iteration 14 : 0.6153864251790718
Loss in iteration 15 : 0.5967765170807983
Loss in iteration 16 : 0.5749307603258659
Loss in iteration 17 : 0.5514794379931852
Loss in iteration 18 : 0.5284933465519549
Loss in iteration 19 : 0.5084797218137701
Loss in iteration 20 : 0.49261596531832585
Loss in iteration 21 : 0.4826701137590962
Loss in iteration 22 : 0.47804775836389063
Loss in iteration 23 : 0.4768635922794124
Loss in iteration 24 : 0.4777225238984017
Loss in iteration 25 : 0.4791145655833538
Loss in iteration 26 : 0.47982402092488946
Loss in iteration 27 : 0.4790031135525144
Loss in iteration 28 : 0.4764706161075332
Loss in iteration 29 : 0.47234222691983924
Loss in iteration 30 : 0.467000037879032
Loss in iteration 31 : 0.46077111594264847
Loss in iteration 32 : 0.45414535704976844
Loss in iteration 33 : 0.44784207192178754
Loss in iteration 34 : 0.4420712283197115
Loss in iteration 35 : 0.4370409158154799
Loss in iteration 36 : 0.4327475584689146
Loss in iteration 37 : 0.42912750794194515
Loss in iteration 38 : 0.4261936158821423
Loss in iteration 39 : 0.4238427214376635
Loss in iteration 40 : 0.4218194980330904
Loss in iteration 41 : 0.4199217895600568
Loss in iteration 42 : 0.4180378102632378
Loss in iteration 43 : 0.41609537725901014
Loss in iteration 44 : 0.41407650601539686
Loss in iteration 45 : 0.41194668938870976
Loss in iteration 46 : 0.40971333107683355
Loss in iteration 47 : 0.4074422006340914
Loss in iteration 48 : 0.40519506431621327
Loss in iteration 49 : 0.40308381513266944
Loss in iteration 50 : 0.40113615357046445
Loss in iteration 51 : 0.39939876581754097
Loss in iteration 52 : 0.39788948407915486
Loss in iteration 53 : 0.39664271190429223
Loss in iteration 54 : 0.3955879768493262
Loss in iteration 55 : 0.3947060320462059
Loss in iteration 56 : 0.39393858653083974
Loss in iteration 57 : 0.3932359369240174
Loss in iteration 58 : 0.3925629253377342
Loss in iteration 59 : 0.39186619200261835
Loss in iteration 60 : 0.3911502585534795
Testing accuracy  of updater 6 on alg 1 with rate 5.599999999999997E-4 = 0.78, training accuracy 0.8358692133376497, time elapsed: 564 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 19.578267791999746
Loss in iteration 3 : 25.852128651777925
Loss in iteration 4 : 25.672713908769197
Loss in iteration 5 : 21.64204057284392
Loss in iteration 6 : 15.060943338073793
Loss in iteration 7 : 7.191624662433019
Loss in iteration 8 : 3.3312812974432533
Loss in iteration 9 : 4.507070294109651
Loss in iteration 10 : 8.296363013426118
Loss in iteration 11 : 10.73180429649503
Loss in iteration 12 : 10.265094455305022
Loss in iteration 13 : 7.894131683696081
Loss in iteration 14 : 5.46059330932037
Loss in iteration 15 : 4.13900428178139
Loss in iteration 16 : 4.137937996299073
Loss in iteration 17 : 4.818957294107743
Loss in iteration 18 : 5.6399500313996525
Loss in iteration 19 : 6.2783448171630765
Loss in iteration 20 : 6.553836761492215
Loss in iteration 21 : 6.414301883318767
Loss in iteration 22 : 5.965570705123134
Loss in iteration 23 : 5.374892731451016
Loss in iteration 24 : 4.813094791025913
Loss in iteration 25 : 4.384189260700973
Loss in iteration 26 : 4.184045055166953
Loss in iteration 27 : 4.242202492099961
Loss in iteration 28 : 4.492248902030029
Loss in iteration 29 : 4.756251911327523
Loss in iteration 30 : 4.883161291831058
Loss in iteration 31 : 4.803501297065148
Loss in iteration 32 : 4.557723979885963
Loss in iteration 33 : 4.255821405573591
Loss in iteration 34 : 4.008478916536911
Loss in iteration 35 : 3.8899351946927854
Loss in iteration 36 : 3.881213339145101
Loss in iteration 37 : 3.932662161302197
Loss in iteration 38 : 3.986816119511765
Loss in iteration 39 : 4.001565979941152
Loss in iteration 40 : 3.958386595190768
Loss in iteration 41 : 3.858409732510825
Loss in iteration 42 : 3.7220312400019004
Loss in iteration 43 : 3.5778684880399414
Loss in iteration 44 : 3.453341680777356
Loss in iteration 45 : 3.371895107688187
Loss in iteration 46 : 3.334937714837521
Loss in iteration 47 : 3.329936245617476
Loss in iteration 48 : 3.316419727218464
Loss in iteration 49 : 3.2643822564502782
Loss in iteration 50 : 3.1714286421450164
Loss in iteration 51 : 3.0586702249058746
Loss in iteration 52 : 2.9596892392941263
Loss in iteration 53 : 2.890519674574784
Loss in iteration 54 : 2.844924135826242
Loss in iteration 55 : 2.8076031198962386
Loss in iteration 56 : 2.762083930869227
Loss in iteration 57 : 2.700091084982221
Loss in iteration 58 : 2.622385878417531
Loss in iteration 59 : 2.5353306184140174
Loss in iteration 60 : 2.45176700623664
Loss in iteration 61 : 2.380897169908811
Loss in iteration 62 : 2.3297150127645536
Loss in iteration 63 : 2.2814517151800566
Loss in iteration 64 : 2.2231795822774916
Loss in iteration 65 : 2.1494004623602496
Loss in iteration 66 : 2.0683934882505226
Loss in iteration 67 : 1.993545302937112
Loss in iteration 68 : 1.9339772777380646
Loss in iteration 69 : 1.8800110520342772
Loss in iteration 70 : 1.82068351234951
Loss in iteration 71 : 1.752061808947937
Loss in iteration 72 : 1.6780738094892262
Loss in iteration 73 : 1.6077211234909632
Loss in iteration 74 : 1.5483207073231944
Loss in iteration 75 : 1.4928437471573657
Loss in iteration 76 : 1.430418024566738
Loss in iteration 77 : 1.3610789529690552
Loss in iteration 78 : 1.2933874310520463
Loss in iteration 79 : 1.234106521307329
Loss in iteration 80 : 1.178594217243
Loss in iteration 81 : 1.1181084646145554
Loss in iteration 82 : 1.054265615893529
Loss in iteration 83 : 0.9950450466109891
Loss in iteration 84 : 0.9426124217135022
Loss in iteration 85 : 0.8901140147364456
Loss in iteration 86 : 0.8338080379672114
Loss in iteration 87 : 0.7814790935359021
Loss in iteration 88 : 0.7387061946545252
Loss in iteration 89 : 0.696993230275124
Loss in iteration 90 : 0.6556489556499252
Loss in iteration 91 : 0.632341896183739
Loss in iteration 92 : 0.6127001317077194
Loss in iteration 93 : 0.5919709972624158
Loss in iteration 94 : 0.5864278404077792
Loss in iteration 95 : 0.5741575667915714
Loss in iteration 96 : 0.5728203584245992
Loss in iteration 97 : 0.5667460695566681
Loss in iteration 98 : 0.572081543446233
Loss in iteration 99 : 0.6062572370535205
Loss in iteration 100 : 0.8946154031864196
Loss in iteration 101 : 0.6132778842436796
Loss in iteration 102 : 0.4720425955847162
Loss in iteration 103 : 0.5063732109315593
Loss in iteration 104 : 0.45944881796704784
Loss in iteration 105 : 0.4883885339039886
Loss in iteration 106 : 0.4845516351205446
Loss in iteration 107 : 0.4968815637469397
Loss in iteration 108 : 0.4796756980107275
Loss in iteration 109 : 0.49652214979894777
Loss in iteration 110 : 0.47727380056579133
Loss in iteration 111 : 0.48598004769087655
Loss in iteration 112 : 0.45428826112132265
Loss in iteration 113 : 0.46551287531748825
Loss in iteration 114 : 0.4344977803743947
Loss in iteration 115 : 0.4300255870514876
Loss in iteration 116 : 0.42685389509126936
Loss in iteration 117 : 0.3910024703504596
Loss in iteration 118 : 0.3692248841065446
Loss in iteration 119 : 0.36697834088937226
Loss in iteration 120 : 0.44629292416025873
Loss in iteration 121 : 1.9032190040175672
Loss in iteration 122 : 2.4121116541013943
Loss in iteration 123 : 3.98680153346482
Loss in iteration 124 : 4.2476257150917895
Loss in iteration 125 : 3.323628661709793
Loss in iteration 126 : 1.5279767605865449
Loss in iteration 127 : 0.9010156604878168
Loss in iteration 128 : 2.043168351771868
Loss in iteration 129 : 2.6837910583109865
Loss in iteration 130 : 1.9748927234270994
Loss in iteration 131 : 1.2596659821570306
Loss in iteration 132 : 1.312290953626209
Loss in iteration 133 : 1.7112116167353604
Loss in iteration 134 : 2.0294320646239075
Loss in iteration 135 : 2.066301917579022
Loss in iteration 136 : 1.8541702598330565
Loss in iteration 137 : 1.574123339496534
Loss in iteration 138 : 1.4002527149023047
Loss in iteration 139 : 1.4626052232653763
Loss in iteration 140 : 1.6745415790797797
Loss in iteration 141 : 1.7713385963471477
Loss in iteration 142 : 1.648864606941719
Loss in iteration 143 : 1.4373567526505042
Loss in iteration 144 : 1.337079490346313
Loss in iteration 145 : 1.3838358478963924
Loss in iteration 146 : 1.4606471365144216
Loss in iteration 147 : 1.4772980394084383
Loss in iteration 148 : 1.4052192193800415
Loss in iteration 149 : 1.2750198091834368
Loss in iteration 150 : 1.1617067609602096
Loss in iteration 151 : 1.1476882657506917
Loss in iteration 152 : 1.1930545884615271
Loss in iteration 153 : 1.1708294017954752
Loss in iteration 154 : 1.0514255926729155
Loss in iteration 155 : 0.9496457115925905
Loss in iteration 156 : 0.9376108037179605
Loss in iteration 157 : 0.9448057383400956
Loss in iteration 158 : 0.8967875578080338
Loss in iteration 159 : 0.7932182597024006
Loss in iteration 160 : 0.7217840589790979
Loss in iteration 161 : 0.7316735242737816
Loss in iteration 162 : 0.6913478920688885
Loss in iteration 163 : 0.5838901263117725
Loss in iteration 164 : 0.5738429508562186
Loss in iteration 165 : 0.5673939501469911
Loss in iteration 166 : 0.47108435144111355
Loss in iteration 167 : 0.5024162957223809
Loss in iteration 168 : 0.43100192748385324
Loss in iteration 169 : 0.4821655936561887
Loss in iteration 170 : 0.4284990320121285
Loss in iteration 171 : 0.5995283714618163
Loss in iteration 172 : 1.3869616152323607
Loss in iteration 173 : 1.397023581229828
Loss in iteration 174 : 0.4093351444993659
Loss in iteration 175 : 1.8927805025626536
Loss in iteration 176 : 0.886651258289908
Loss in iteration 177 : 1.5995712564499869
Loss in iteration 178 : 1.2156087306085492
Loss in iteration 179 : 0.7272038912810594
Loss in iteration 180 : 0.9688540448758334
Loss in iteration 181 : 1.333731365529883
Loss in iteration 182 : 1.1451673594801544
Loss in iteration 183 : 0.9232202125403548
Loss in iteration 184 : 1.0327375561793917
Loss in iteration 185 : 1.214563456735013
Loss in iteration 186 : 1.270117765435335
Loss in iteration 187 : 1.173150468328202
Loss in iteration 188 : 1.0431190918763276
Loss in iteration 189 : 1.019368014628953
Loss in iteration 190 : 1.1193269823707044
Loss in iteration 191 : 1.1531524146739722
Loss in iteration 192 : 1.0502608678305538
Loss in iteration 193 : 0.951895828990144
Loss in iteration 194 : 0.9662979408740664
Loss in iteration 195 : 1.0015132362946815
Loss in iteration 196 : 0.9719593179871959
Loss in iteration 197 : 0.8783545654179409
Loss in iteration 198 : 0.802983503222561
Loss in iteration 199 : 0.8131594533787033
Loss in iteration 200 : 0.8064054174231459
Testing accuracy  of updater 7 on alg 1 with rate 0.7999999999999999 = 0.79625, training accuracy 0.8326319197151182, time elapsed: 2159 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.680973936546168
Loss in iteration 3 : 3.915377749332908
Loss in iteration 4 : 4.163305839785311
Loss in iteration 5 : 3.521652050919123
Loss in iteration 6 : 2.0781541489994413
Loss in iteration 7 : 0.7047729864502862
Loss in iteration 8 : 1.4580437795866144
Loss in iteration 9 : 2.5190620878162138
Loss in iteration 10 : 2.1449762807156127
Loss in iteration 11 : 1.2118974206367943
Loss in iteration 12 : 0.9761452178440361
Loss in iteration 13 : 1.3027974314189175
Loss in iteration 14 : 1.6838244105805862
Loss in iteration 15 : 1.8376943264480765
Loss in iteration 16 : 1.7257765786341386
Loss in iteration 17 : 1.4596663786687127
Loss in iteration 18 : 1.2306855153742593
Loss in iteration 19 : 1.160042332468515
Loss in iteration 20 : 1.3035215777614
Loss in iteration 21 : 1.490285600538686
Loss in iteration 22 : 1.5271484133080386
Loss in iteration 23 : 1.3901959911001547
Loss in iteration 24 : 1.2163052934391922
Loss in iteration 25 : 1.148372656879655
Loss in iteration 26 : 1.1964339377140667
Loss in iteration 27 : 1.2660306318601315
Loss in iteration 28 : 1.2918377743905192
Loss in iteration 29 : 1.2473979941835183
Loss in iteration 30 : 1.1538964935037515
Loss in iteration 31 : 1.0587668237869852
Loss in iteration 32 : 1.0180174307213323
Loss in iteration 33 : 1.0453359611811457
Loss in iteration 34 : 1.0684254814569663
Loss in iteration 35 : 1.0269492211088838
Loss in iteration 36 : 0.9387363619767083
Loss in iteration 37 : 0.8727834456877531
Loss in iteration 38 : 0.8678443830124474
Loss in iteration 39 : 0.8755654696870953
Loss in iteration 40 : 0.8509016386935785
Loss in iteration 41 : 0.7851481742246087
Loss in iteration 42 : 0.7150028280187715
Loss in iteration 43 : 0.6964184846148883
Loss in iteration 44 : 0.699347548283287
Loss in iteration 45 : 0.6542374157021661
Loss in iteration 46 : 0.5859659622619884
Loss in iteration 47 : 0.5687421862361353
Loss in iteration 48 : 0.5656017793823058
Loss in iteration 49 : 0.5186105920637888
Loss in iteration 50 : 0.4665516803635871
Loss in iteration 51 : 0.48335272074635643
Loss in iteration 52 : 0.437676267373853
Loss in iteration 53 : 0.4314860566852498
Loss in iteration 54 : 0.4428163489984043
Loss in iteration 55 : 0.411959610193015
Loss in iteration 56 : 0.4507703389291458
Loss in iteration 57 : 0.539758724764626
Loss in iteration 58 : 0.4145919638296868
Loss in iteration 59 : 0.5338442827682568
Loss in iteration 60 : 0.8007747656172424
Loss in iteration 61 : 0.6083993170082521
Loss in iteration 62 : 0.7165824468115309
Loss in iteration 63 : 0.43785719842438403
Loss in iteration 64 : 0.5431394835211557
Loss in iteration 65 : 0.5880072420131204
Loss in iteration 66 : 0.4531070819565333
Loss in iteration 67 : 0.5577713642277755
Loss in iteration 68 : 0.566422282807664
Loss in iteration 69 : 0.4867867669961797
Loss in iteration 70 : 0.550875406571388
Loss in iteration 71 : 0.5753744191000361
Loss in iteration 72 : 0.5091393818409595
Loss in iteration 73 : 0.5121080766754739
Loss in iteration 74 : 0.5527255211775137
Loss in iteration 75 : 0.4915398908631032
Loss in iteration 76 : 0.48226684852268176
Loss in iteration 77 : 0.5066319286457598
Loss in iteration 78 : 0.45724569397490034
Loss in iteration 79 : 0.4427655797458808
Loss in iteration 80 : 0.4533887644648314
Loss in iteration 81 : 0.4044351275911301
Loss in iteration 82 : 0.42906445306512436
Loss in iteration 83 : 0.38277653759299457
Loss in iteration 84 : 0.42504534563254637
Loss in iteration 85 : 0.37541504992286345
Loss in iteration 86 : 0.3906823846346017
Loss in iteration 87 : 0.4359559475152734
Loss in iteration 88 : 0.4830136783117526
Loss in iteration 89 : 0.39460286503843345
Loss in iteration 90 : 0.36878711038991097
Loss in iteration 91 : 0.37987451489340807
Loss in iteration 92 : 0.39045724110035174
Loss in iteration 93 : 0.37789792028409325
Loss in iteration 94 : 0.36642952283401514
Loss in iteration 95 : 0.375088394565962
Loss in iteration 96 : 0.37162185479751714
Loss in iteration 97 : 0.3709425082711765
Loss in iteration 98 : 0.3759227983684263
Loss in iteration 99 : 0.3686973852266644
Loss in iteration 100 : 0.37275237317442567
Loss in iteration 101 : 0.37139798720023687
Loss in iteration 102 : 0.364832353499376
Loss in iteration 103 : 0.3678569221857591
Loss in iteration 104 : 0.3679724923526932
Loss in iteration 105 : 0.3658989629153425
Loss in iteration 106 : 0.3631167224252075
Loss in iteration 107 : 0.36489188914934306
Loss in iteration 108 : 0.37094325777365206
Loss in iteration 109 : 0.3799315526972707
Loss in iteration 110 : 0.3894498086630755
Loss in iteration 111 : 0.38116076622494943
Loss in iteration 112 : 0.3724298760792032
Loss in iteration 113 : 0.3638935935026344
Loss in iteration 114 : 0.3655017228369808
Loss in iteration 115 : 0.37410931852185025
Loss in iteration 116 : 0.37020885739509407
Loss in iteration 117 : 0.3641933567996732
Loss in iteration 118 : 0.36973853472843377
Loss in iteration 119 : 0.3693596888233264
Loss in iteration 120 : 0.36409001018767023
Loss in iteration 121 : 0.3648599926202183
Loss in iteration 122 : 0.3687257345914628
Loss in iteration 123 : 0.36814096499907334
Loss in iteration 124 : 0.3634300899642602
Loss in iteration 125 : 0.36359587725653814
Loss in iteration 126 : 0.36770712444030695
Loss in iteration 127 : 0.37187561094361565
Loss in iteration 128 : 0.37540913295634937
Loss in iteration 129 : 0.3741130696646258
Loss in iteration 130 : 0.37267672526710627
Loss in iteration 131 : 0.3690710448044409
Loss in iteration 132 : 0.3647784988765192
Loss in iteration 133 : 0.36323027917218864
Loss in iteration 134 : 0.3680618183559975
Loss in iteration 135 : 0.3718682423773666
Loss in iteration 136 : 0.3664397356495274
Loss in iteration 137 : 0.363203651264429
Testing accuracy  of updater 7 on alg 1 with rate 0.5599999999999999 = 0.7825, training accuracy 0.8407251537714471, time elapsed: 1474 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.6947659402851107
Loss in iteration 3 : 2.3466744171907306
Loss in iteration 4 : 2.3760931593087635
Loss in iteration 5 : 1.8441548332391482
Loss in iteration 6 : 0.860470893741358
Loss in iteration 7 : 0.5926366856485967
Loss in iteration 8 : 1.2806466178513094
Loss in iteration 9 : 1.5331044883817504
Loss in iteration 10 : 1.1215536247426041
Loss in iteration 11 : 0.737975372852287
Loss in iteration 12 : 0.7618135523991233
Loss in iteration 13 : 0.9840521141635773
Loss in iteration 14 : 1.1588160349940546
Loss in iteration 15 : 1.1788252559402717
Loss in iteration 16 : 1.0633145504247439
Loss in iteration 17 : 0.91356521150714
Loss in iteration 18 : 0.8272401153115598
Loss in iteration 19 : 0.8581656137019525
Loss in iteration 20 : 0.9636646875082057
Loss in iteration 21 : 1.028250529804067
Loss in iteration 22 : 0.9958576650140516
Loss in iteration 23 : 0.9016039860782318
Loss in iteration 24 : 0.8294770626201471
Loss in iteration 25 : 0.8289654365995858
Loss in iteration 26 : 0.8680015730341634
Loss in iteration 27 : 0.89834743454898
Loss in iteration 28 : 0.8919143822542015
Loss in iteration 29 : 0.849405245557712
Loss in iteration 30 : 0.7916423204361518
Loss in iteration 31 : 0.7505606065758947
Loss in iteration 32 : 0.7535219538235632
Loss in iteration 33 : 0.7715653708878482
Loss in iteration 34 : 0.7653586335932409
Loss in iteration 35 : 0.7246266993677646
Loss in iteration 36 : 0.6772861569317701
Loss in iteration 37 : 0.6560135710915534
Loss in iteration 38 : 0.6602892701652469
Loss in iteration 39 : 0.6572269839298476
Loss in iteration 40 : 0.6310609730210048
Loss in iteration 41 : 0.5891267591192767
Loss in iteration 42 : 0.5625899770582148
Loss in iteration 43 : 0.5602253986522848
Loss in iteration 44 : 0.5537244896573003
Loss in iteration 45 : 0.5201556013812976
Loss in iteration 46 : 0.4882610647489999
Loss in iteration 47 : 0.4812893719539641
Loss in iteration 48 : 0.4772300236744004
Loss in iteration 49 : 0.4501585631477441
Loss in iteration 50 : 0.42248001649599126
Loss in iteration 51 : 0.42258252150288916
Loss in iteration 52 : 0.41290862352144436
Loss in iteration 53 : 0.3853698885104831
Loss in iteration 54 : 0.39628749245083306
Loss in iteration 55 : 0.3805486940157146
Loss in iteration 56 : 0.3867270561965713
Loss in iteration 57 : 0.3838544079686467
Loss in iteration 58 : 0.39635352046934935
Loss in iteration 59 : 0.3882233475391572
Loss in iteration 60 : 0.41519066400521976
Loss in iteration 61 : 0.3999125952483501
Loss in iteration 62 : 0.38820952120956354
Loss in iteration 63 : 0.40317526097558515
Loss in iteration 64 : 0.3787893805638521
Loss in iteration 65 : 0.37730726101715306
Loss in iteration 66 : 0.37622486644572595
Loss in iteration 67 : 0.3684742446251184
Loss in iteration 68 : 0.37243454507985946
Loss in iteration 69 : 0.3680417860263958
Loss in iteration 70 : 0.3712055856230113
Loss in iteration 71 : 0.37059081799273674
Loss in iteration 72 : 0.3700645881434414
Loss in iteration 73 : 0.37213175095193923
Loss in iteration 74 : 0.3696608031374784
Loss in iteration 75 : 0.37132320922054385
Loss in iteration 76 : 0.3684395855963236
Loss in iteration 77 : 0.37009770964050365
Loss in iteration 78 : 0.3666932923079899
Loss in iteration 79 : 0.36818607647815543
Loss in iteration 80 : 0.36495963184801483
Loss in iteration 81 : 0.36648269554301693
Loss in iteration 82 : 0.36398153291568125
Loss in iteration 83 : 0.3658722603197383
Loss in iteration 84 : 0.36302201994515443
Loss in iteration 85 : 0.3654843520313918
Loss in iteration 86 : 0.36380206426960304
Loss in iteration 87 : 0.36479440935023716
Loss in iteration 88 : 0.36545491945595554
Loss in iteration 89 : 0.3637526795999276
Loss in iteration 90 : 0.3653881655657147
Loss in iteration 91 : 0.3635335773682094
Loss in iteration 92 : 0.364342938439496
Loss in iteration 93 : 0.3635592429353623
Loss in iteration 94 : 0.36349373727221523
Loss in iteration 95 : 0.3633162079349064
Loss in iteration 96 : 0.36307247772909307
Loss in iteration 97 : 0.36339861775418386
Loss in iteration 98 : 0.36306723288934334
Loss in iteration 99 : 0.3634146710406913
Loss in iteration 100 : 0.36316172426994203
Loss in iteration 101 : 0.3635512825272308
Loss in iteration 102 : 0.3630733997084274
Loss in iteration 103 : 0.36319350080329865
Loss in iteration 104 : 0.36303240058063885
Loss in iteration 105 : 0.3632621381620663
Loss in iteration 106 : 0.36302770410873997
Loss in iteration 107 : 0.3630670064712048
Loss in iteration 108 : 0.36292447576491177
Loss in iteration 109 : 0.36287329613150654
Loss in iteration 110 : 0.362907638059917
Loss in iteration 111 : 0.36275011706775623
Loss in iteration 112 : 0.36288415407931474
Loss in iteration 113 : 0.36273020715947346
Loss in iteration 114 : 0.3630351509387775
Loss in iteration 115 : 0.36277088016135955
Loss in iteration 116 : 0.36291232634655424
Loss in iteration 117 : 0.3628353357914807
Loss in iteration 118 : 0.3628185392089516
Loss in iteration 119 : 0.36290038144856535
Loss in iteration 120 : 0.3627634707977901
Loss in iteration 121 : 0.36294210916443265
Loss in iteration 122 : 0.3627621166751476
Loss in iteration 123 : 0.3628064779504831
Loss in iteration 124 : 0.36276452336429865
Loss in iteration 125 : 0.3627510937109464
Loss in iteration 126 : 0.3627887898224079
Testing accuracy  of updater 7 on alg 1 with rate 0.32 = 0.791, training accuracy 0.8420200712204597, time elapsed: 1350 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.713567776340315
Loss in iteration 3 : 0.882791795172168
Loss in iteration 4 : 0.8792510168073429
Loss in iteration 5 : 0.7196577918347554
Loss in iteration 6 : 0.44369745613299816
Loss in iteration 7 : 0.4642564242555375
Loss in iteration 8 : 0.6342021030120575
Loss in iteration 9 : 0.5783799516294565
Loss in iteration 10 : 0.4248910662920295
Loss in iteration 11 : 0.3849476481218598
Loss in iteration 12 : 0.45788832353386905
Loss in iteration 13 : 0.5126569456449198
Loss in iteration 14 : 0.4971160810992535
Loss in iteration 15 : 0.44255309525076125
Loss in iteration 16 : 0.40773628189143757
Loss in iteration 17 : 0.419771111731422
Loss in iteration 18 : 0.45293909494006807
Loss in iteration 19 : 0.4734677500880881
Loss in iteration 20 : 0.4676515241482178
Loss in iteration 21 : 0.444525741363645
Loss in iteration 22 : 0.425602738150183
Loss in iteration 23 : 0.42316550332103464
Loss in iteration 24 : 0.43652053908876587
Loss in iteration 25 : 0.44935183093614794
Loss in iteration 26 : 0.4495273632299792
Loss in iteration 27 : 0.4378089058235703
Loss in iteration 28 : 0.4239446743328388
Loss in iteration 29 : 0.4199042256183422
Loss in iteration 30 : 0.4237160646162489
Loss in iteration 31 : 0.4295835243756314
Loss in iteration 32 : 0.43016695135409655
Loss in iteration 33 : 0.42320235647222454
Loss in iteration 34 : 0.41404450700664225
Loss in iteration 35 : 0.40848507377908877
Loss in iteration 36 : 0.4080011595000851
Loss in iteration 37 : 0.40973471898188246
Loss in iteration 38 : 0.40924620182805016
Loss in iteration 39 : 0.4042602744088609
Loss in iteration 40 : 0.398266501953674
Loss in iteration 41 : 0.39393667884422096
Loss in iteration 42 : 0.39287550008470534
Loss in iteration 43 : 0.39310270069323316
Loss in iteration 44 : 0.39138125982467625
Loss in iteration 45 : 0.3866955581115337
Loss in iteration 46 : 0.38214212818134546
Loss in iteration 47 : 0.3802552503489276
Loss in iteration 48 : 0.38009714905025943
Loss in iteration 49 : 0.37903909657268453
Loss in iteration 50 : 0.37585300522651005
Loss in iteration 51 : 0.3720566726143582
Loss in iteration 52 : 0.3708775603875672
Loss in iteration 53 : 0.37127602689936223
Loss in iteration 54 : 0.3698832389568968
Loss in iteration 55 : 0.3670609474075366
Loss in iteration 56 : 0.3656042335386305
Loss in iteration 57 : 0.36640919220807894
Loss in iteration 58 : 0.36587666165310917
Loss in iteration 59 : 0.3640673310887603
Loss in iteration 60 : 0.3637286177508511
Loss in iteration 61 : 0.36446956133237257
Loss in iteration 62 : 0.36439399342424866
Loss in iteration 63 : 0.36360153167494014
Loss in iteration 64 : 0.3633661193008361
Loss in iteration 65 : 0.3641025938210844
Loss in iteration 66 : 0.3642340835289872
Loss in iteration 67 : 0.3636746614813176
Loss in iteration 68 : 0.36378266094437195
Loss in iteration 69 : 0.3643358093888801
Loss in iteration 70 : 0.36421300143413216
Loss in iteration 71 : 0.3638552514322021
Loss in iteration 72 : 0.3639911282837078
Loss in iteration 73 : 0.36427396656247174
Loss in iteration 74 : 0.36399604519523726
Loss in iteration 75 : 0.3637146671345368
Loss in iteration 76 : 0.363792756685135
Loss in iteration 77 : 0.36384223049815756
Loss in iteration 78 : 0.3635828046563687
Loss in iteration 79 : 0.36339094841220126
Loss in iteration 80 : 0.3634234005917393
Loss in iteration 81 : 0.36338099468870294
Loss in iteration 82 : 0.3631634425479104
Loss in iteration 83 : 0.3631029418113139
Loss in iteration 84 : 0.3631240238233355
Loss in iteration 85 : 0.3630708470431369
Loss in iteration 86 : 0.3629435850288936
Loss in iteration 87 : 0.3629009073922349
Loss in iteration 88 : 0.3629565500478324
Loss in iteration 89 : 0.36291195729733305
Loss in iteration 90 : 0.3628216724322505
Loss in iteration 91 : 0.36285152968177115
Loss in iteration 92 : 0.3628763206341254
Loss in iteration 93 : 0.3628373179745131
Loss in iteration 94 : 0.3627890659175567
Loss in iteration 95 : 0.36283244926342617
Loss in iteration 96 : 0.3628389146131607
Loss in iteration 97 : 0.3627883935887182
Loss in iteration 98 : 0.3627974943977596
Loss in iteration 99 : 0.36282211032703027
Loss in iteration 100 : 0.36280373385590214
Loss in iteration 101 : 0.3627778052989985
Loss in iteration 102 : 0.3627913868535523
Loss in iteration 103 : 0.36279272865793594
Loss in iteration 104 : 0.36277049052479476
Loss in iteration 105 : 0.3627716219916261
Loss in iteration 106 : 0.36278135953562846
Loss in iteration 107 : 0.3627546298581786
Loss in iteration 108 : 0.3627613194501059
Loss in iteration 109 : 0.36276499982527044
Loss in iteration 110 : 0.3627406696145261
Loss in iteration 111 : 0.36274896329474887
Loss in iteration 112 : 0.3627382286901025
Loss in iteration 113 : 0.36273315801384354
Loss in iteration 114 : 0.3627461633577948
Loss in iteration 115 : 0.3627266491352291
Loss in iteration 116 : 0.36272807659081296
Loss in iteration 117 : 0.36273504342921387
Loss in iteration 118 : 0.3627213822471022
Loss in iteration 119 : 0.3627211981987242
Loss in iteration 120 : 0.3627292030204735
Loss in iteration 121 : 0.3627205244502042
Loss in iteration 122 : 0.36271958548873723
Testing accuracy  of updater 7 on alg 1 with rate 0.08 = 0.7905, training accuracy 0.8423438005827129, time elapsed: 1285 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6104076752764618
Loss in iteration 3 : 0.7343827110465045
Loss in iteration 4 : 0.7369700854604435
Loss in iteration 5 : 0.6282240998706078
Loss in iteration 6 : 0.4548114373945263
Loss in iteration 7 : 0.4460075725806596
Loss in iteration 8 : 0.5471168401626001
Loss in iteration 9 : 0.5053436157300328
Loss in iteration 10 : 0.4011616010175401
Loss in iteration 11 : 0.37762257841425656
Loss in iteration 12 : 0.42365775641325276
Loss in iteration 13 : 0.4509081574954865
Loss in iteration 14 : 0.4300066482627292
Loss in iteration 15 : 0.39323006335735805
Loss in iteration 16 : 0.37975708640402006
Loss in iteration 17 : 0.39448896522903343
Loss in iteration 18 : 0.4143649991344304
Loss in iteration 19 : 0.42124147401952633
Loss in iteration 20 : 0.41233590374999446
Loss in iteration 21 : 0.3976402333171652
Loss in iteration 22 : 0.39038771574953113
Loss in iteration 23 : 0.39425016826054043
Loss in iteration 24 : 0.40405255496007253
Loss in iteration 25 : 0.4098232288380747
Loss in iteration 26 : 0.40681784843321045
Loss in iteration 27 : 0.3988628266061278
Loss in iteration 28 : 0.39340072410371385
Loss in iteration 29 : 0.39296994212379144
Loss in iteration 30 : 0.39630705459128557
Loss in iteration 31 : 0.39961379456428553
Loss in iteration 32 : 0.3991859889069809
Loss in iteration 33 : 0.39502767422550533
Loss in iteration 34 : 0.3904591772844602
Loss in iteration 35 : 0.38820295772017177
Loss in iteration 36 : 0.38852703979551767
Loss in iteration 37 : 0.3897620058280056
Loss in iteration 38 : 0.38983356729790664
Loss in iteration 39 : 0.3879485503478461
Loss in iteration 40 : 0.38476033464237863
Loss in iteration 41 : 0.38181261033255615
Loss in iteration 42 : 0.3805729312043995
Loss in iteration 43 : 0.3805074021578507
Loss in iteration 44 : 0.38059414932982727
Loss in iteration 45 : 0.37928292125669144
Loss in iteration 46 : 0.3768252848713634
Loss in iteration 47 : 0.3746509614853619
Loss in iteration 48 : 0.37368581966196335
Loss in iteration 49 : 0.37372062196928074
Loss in iteration 50 : 0.37327026754856524
Loss in iteration 51 : 0.3717314274727888
Loss in iteration 52 : 0.36982655834533984
Loss in iteration 53 : 0.3689225407746505
Loss in iteration 54 : 0.3690222620440405
Loss in iteration 55 : 0.36874625161930974
Loss in iteration 56 : 0.36765830541163086
Loss in iteration 57 : 0.3665327475600128
Loss in iteration 58 : 0.3660053072859908
Loss in iteration 59 : 0.366081351962785
Loss in iteration 60 : 0.3659202193624654
Loss in iteration 61 : 0.3652194966458193
Loss in iteration 62 : 0.36470619504442287
Loss in iteration 63 : 0.3646297549158596
Loss in iteration 64 : 0.3647567188933292
Loss in iteration 65 : 0.3645946611139521
Loss in iteration 66 : 0.36422712670803126
Loss in iteration 67 : 0.36401239884386233
Loss in iteration 68 : 0.3641111161628122
Loss in iteration 69 : 0.364196100991619
Loss in iteration 70 : 0.36408087938245454
Loss in iteration 71 : 0.3638909707516273
Loss in iteration 72 : 0.3638733915798516
Loss in iteration 73 : 0.36398503784251873
Loss in iteration 74 : 0.36400147898145746
Loss in iteration 75 : 0.3638812753462679
Loss in iteration 76 : 0.3637894262402173
Loss in iteration 77 : 0.3638173425008682
Loss in iteration 78 : 0.3638521955927966
Loss in iteration 79 : 0.36381094159746474
Loss in iteration 80 : 0.36371558051189445
Loss in iteration 81 : 0.3636619558022121
Loss in iteration 82 : 0.3636552251148458
Loss in iteration 83 : 0.36364376626702893
Loss in iteration 84 : 0.36357402038812664
Loss in iteration 85 : 0.3635096780277218
Loss in iteration 86 : 0.36347516584855644
Loss in iteration 87 : 0.3634516892697981
Loss in iteration 88 : 0.36341080972397255
Loss in iteration 89 : 0.36335315973338933
Loss in iteration 90 : 0.36331719800200474
Loss in iteration 91 : 0.3632987963743148
Loss in iteration 92 : 0.3632685165495887
Loss in iteration 93 : 0.36322823458837655
Loss in iteration 94 : 0.3631993452462092
Loss in iteration 95 : 0.36318474085944913
Loss in iteration 96 : 0.36317062077430756
Loss in iteration 97 : 0.3631438253729789
Loss in iteration 98 : 0.3631149931161116
Loss in iteration 99 : 0.3631039170335344
Loss in iteration 100 : 0.36309694183848434
Loss in iteration 101 : 0.3630770384523762
Loss in iteration 102 : 0.36305518040055945
Loss in iteration 103 : 0.36304468827477787
Loss in iteration 104 : 0.36303675496022325
Loss in iteration 105 : 0.3630195293714272
Loss in iteration 106 : 0.36300757278936535
Loss in iteration 107 : 0.3629995060143834
Loss in iteration 108 : 0.362989220413334
Loss in iteration 109 : 0.3629744035818247
Loss in iteration 110 : 0.36296429002527897
Loss in iteration 111 : 0.36295664163120583
Loss in iteration 112 : 0.36294349029015066
Loss in iteration 113 : 0.362933252040611
Loss in iteration 114 : 0.36292591004722813
Loss in iteration 115 : 0.36291454155204916
Loss in iteration 116 : 0.3629063815552413
Loss in iteration 117 : 0.36289902815922076
Loss in iteration 118 : 0.36288974235517
Loss in iteration 119 : 0.36288174801205675
Loss in iteration 120 : 0.3628758173623574
Loss in iteration 121 : 0.3628673950104379
Loss in iteration 122 : 0.3628595412444067
Loss in iteration 123 : 0.3628541460597778
Loss in iteration 124 : 0.3628464387289887
Loss in iteration 125 : 0.36283967311969423
Loss in iteration 126 : 0.3628335750462261
Loss in iteration 127 : 0.36282728907587025
Loss in iteration 128 : 0.362822637980546
Loss in iteration 129 : 0.36281581940884666
Loss in iteration 130 : 0.3628117454522691
Loss in iteration 131 : 0.36280695662077683
Loss in iteration 132 : 0.3628015847693403
Loss in iteration 133 : 0.3627993195922515
Loss in iteration 134 : 0.3627950218939073
Loss in iteration 135 : 0.36278995164250655
Loss in iteration 136 : 0.36278740531115694
Loss in iteration 137 : 0.3627846763120232
Loss in iteration 138 : 0.3627809192090732
Loss in iteration 139 : 0.362776648931118
Loss in iteration 140 : 0.3627748121194725
Loss in iteration 141 : 0.36277262628718315
Loss in iteration 142 : 0.36276792228707494
Loss in iteration 143 : 0.3627665482996602
Loss in iteration 144 : 0.3627645867789471
Loss in iteration 145 : 0.3627609954022517
Loss in iteration 146 : 0.3627591246807271
Loss in iteration 147 : 0.3627571432831132
Loss in iteration 148 : 0.36275504213889254
Loss in iteration 149 : 0.3627534519986886
Loss in iteration 150 : 0.3627515882179679
Loss in iteration 151 : 0.36274928064193457
Loss in iteration 152 : 0.3627478786248063
Loss in iteration 153 : 0.3627473993141006
Loss in iteration 154 : 0.3627447003479599
Loss in iteration 155 : 0.36274423845582715
Loss in iteration 156 : 0.36274226919739727
Loss in iteration 157 : 0.36274029789399204
Loss in iteration 158 : 0.36274066321841103
Loss in iteration 159 : 0.3627379054656589
Loss in iteration 160 : 0.36273778746147944
Loss in iteration 161 : 0.3627371463519681
Loss in iteration 162 : 0.362734735134607
Testing accuracy  of updater 7 on alg 1 with rate 0.056 = 0.79, training accuracy 0.8433149886694723, time elapsed: 1713 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5740047982630829
Loss in iteration 3 : 0.6645809744450872
Loss in iteration 4 : 0.7495149649773698
Loss in iteration 5 : 0.7628319520512306
Loss in iteration 6 : 0.7110474194241181
Loss in iteration 7 : 0.6016052288819553
Loss in iteration 8 : 0.46977962471405305
Loss in iteration 9 : 0.42978400213395807
Loss in iteration 10 : 0.5104516821721674
Loss in iteration 11 : 0.5486154724874472
Loss in iteration 12 : 0.49451014150871164
Loss in iteration 13 : 0.4125222262497706
Loss in iteration 14 : 0.3774082711945589
Loss in iteration 15 : 0.3988987653602876
Loss in iteration 16 : 0.4308995461024215
Loss in iteration 17 : 0.4368130328087041
Loss in iteration 18 : 0.4153977267907943
Loss in iteration 19 : 0.3883151918585265
Loss in iteration 20 : 0.37362151348595923
Loss in iteration 21 : 0.3768778595647103
Loss in iteration 22 : 0.38961556098666805
Loss in iteration 23 : 0.39980155167546616
Loss in iteration 24 : 0.4007410913812303
Loss in iteration 25 : 0.39366406064926923
Loss in iteration 26 : 0.38406690369444585
Loss in iteration 27 : 0.377630834561657
Loss in iteration 28 : 0.3778231655415619
Loss in iteration 29 : 0.38228064056611044
Loss in iteration 30 : 0.38722165974006995
Loss in iteration 31 : 0.3895518776045739
Loss in iteration 32 : 0.3881917245594739
Loss in iteration 33 : 0.3844846749998419
Loss in iteration 34 : 0.38051969777903416
Loss in iteration 35 : 0.378354181003875
Loss in iteration 36 : 0.37866331651531615
Loss in iteration 37 : 0.3805423962118016
Loss in iteration 38 : 0.38237666932734693
Loss in iteration 39 : 0.3825954620881231
Loss in iteration 40 : 0.3811319393405199
Loss in iteration 41 : 0.37879610514028894
Loss in iteration 42 : 0.3769630553972105
Loss in iteration 43 : 0.37633541329760867
Loss in iteration 44 : 0.37667027985651885
Loss in iteration 45 : 0.3773080844747321
Loss in iteration 46 : 0.37748522933572976
Loss in iteration 47 : 0.37672550921490267
Loss in iteration 48 : 0.3753651037357108
Loss in iteration 49 : 0.3742173485334046
Loss in iteration 50 : 0.3733137862037404
Loss in iteration 51 : 0.3731479854039266
Loss in iteration 52 : 0.3732979914072607
Loss in iteration 53 : 0.37323108955696654
Loss in iteration 54 : 0.3726172856309076
Loss in iteration 55 : 0.3716346270745811
Loss in iteration 56 : 0.3708011975168322
Loss in iteration 57 : 0.3704546322741979
Loss in iteration 58 : 0.37033730543336724
Loss in iteration 59 : 0.3702203464539494
Loss in iteration 60 : 0.36986985943625306
Loss in iteration 61 : 0.36929572200468613
Loss in iteration 62 : 0.36868361901936847
Loss in iteration 63 : 0.368312279323117
Loss in iteration 64 : 0.36816747544719447
Loss in iteration 65 : 0.3680660746027984
Loss in iteration 66 : 0.3678140154842613
Loss in iteration 67 : 0.3674363413494721
Loss in iteration 68 : 0.3670892776075019
Loss in iteration 69 : 0.36688190549002
Loss in iteration 70 : 0.36675277540749035
Loss in iteration 71 : 0.3666452869935542
Loss in iteration 72 : 0.3665063826243173
Loss in iteration 73 : 0.36630429442279266
Loss in iteration 74 : 0.36606818594049884
Loss in iteration 75 : 0.365894875878083
Loss in iteration 76 : 0.36580097675761497
Loss in iteration 77 : 0.3657348449468273
Loss in iteration 78 : 0.3656411644155984
Loss in iteration 79 : 0.3655113358841299
Loss in iteration 80 : 0.36537143852456927
Loss in iteration 81 : 0.36527379691527895
Loss in iteration 82 : 0.36520429664847814
Loss in iteration 83 : 0.36513995664143273
Loss in iteration 84 : 0.36507384781930363
Loss in iteration 85 : 0.3649916766268102
Loss in iteration 86 : 0.36491177335327013
Loss in iteration 87 : 0.36484997892802545
Loss in iteration 88 : 0.36480040429432053
Loss in iteration 89 : 0.3647605431770881
Loss in iteration 90 : 0.3647133804038865
Loss in iteration 91 : 0.3646609616172335
Loss in iteration 92 : 0.3646144509136744
Loss in iteration 93 : 0.36457258981782786
Loss in iteration 94 : 0.3645401368416672
Loss in iteration 95 : 0.36450902904054716
Loss in iteration 96 : 0.36446864885375235
Loss in iteration 97 : 0.36442377227039285
Loss in iteration 98 : 0.36438485171073115
Loss in iteration 99 : 0.36435721357727807
Loss in iteration 100 : 0.36432892712737097
Loss in iteration 101 : 0.3642961333909355
Loss in iteration 102 : 0.36425758000921155
Loss in iteration 103 : 0.3642173219269664
Loss in iteration 104 : 0.36418676940603484
Loss in iteration 105 : 0.36416186989714333
Loss in iteration 106 : 0.36412992740339617
Loss in iteration 107 : 0.3640930307321916
Loss in iteration 108 : 0.3640637431036543
Loss in iteration 109 : 0.3640363522273667
Loss in iteration 110 : 0.36400987327578616
Loss in iteration 111 : 0.3639814391825329
Loss in iteration 112 : 0.3639530178092364
Loss in iteration 113 : 0.36392603908377147
Loss in iteration 114 : 0.36390021120009997
Loss in iteration 115 : 0.36387457571439985
Loss in iteration 116 : 0.36385000606989976
Loss in iteration 117 : 0.3638254704430808
Loss in iteration 118 : 0.3638015694048345
Loss in iteration 119 : 0.36377916691085377
Loss in iteration 120 : 0.3637577448652047
Loss in iteration 121 : 0.3637377354994902
Loss in iteration 122 : 0.3637178132332249
Loss in iteration 123 : 0.36369803287801133
Loss in iteration 124 : 0.3636790547342317
Loss in iteration 125 : 0.36366119933844626
Loss in iteration 126 : 0.3636440029972855
Loss in iteration 127 : 0.36362673697757686
Loss in iteration 128 : 0.3636095649451384
Loss in iteration 129 : 0.36359234003084906
Loss in iteration 130 : 0.363575101531501
Loss in iteration 131 : 0.3635584913519533
Loss in iteration 132 : 0.3635423624133473
Loss in iteration 133 : 0.3635263125398759
Loss in iteration 134 : 0.3635114129598265
Loss in iteration 135 : 0.3634966657458677
Loss in iteration 136 : 0.3634824389994659
Loss in iteration 137 : 0.36346827937779413
Loss in iteration 138 : 0.36345439320549017
Loss in iteration 139 : 0.36344064434116863
Loss in iteration 140 : 0.3634284738287534
Loss in iteration 141 : 0.36341580682519264
Loss in iteration 142 : 0.3634022722542199
Loss in iteration 143 : 0.3633892783745608
Loss in iteration 144 : 0.3633775313697571
Loss in iteration 145 : 0.3633654143393333
Loss in iteration 146 : 0.3633529561298719
Loss in iteration 147 : 0.3633405117937352
Loss in iteration 148 : 0.36332797550686846
Loss in iteration 149 : 0.3633162188728048
Loss in iteration 150 : 0.3633045174251989
Loss in iteration 151 : 0.36329279829533656
Loss in iteration 152 : 0.363281900614827
Loss in iteration 153 : 0.3632711304579889
Loss in iteration 154 : 0.3632604252078004
Loss in iteration 155 : 0.36324999893562443
Loss in iteration 156 : 0.3632395624842525
Loss in iteration 157 : 0.3632291188350712
Loss in iteration 158 : 0.3632187191281826
Loss in iteration 159 : 0.36320853148404936
Loss in iteration 160 : 0.3631983282590743
Loss in iteration 161 : 0.36318825420204764
Loss in iteration 162 : 0.3631783416774701
Loss in iteration 163 : 0.3631691161855928
Loss in iteration 164 : 0.3631600699508071
Loss in iteration 165 : 0.3631511125506198
Loss in iteration 166 : 0.3631422201731517
Loss in iteration 167 : 0.36313340286192675
Loss in iteration 168 : 0.3631248365443967
Loss in iteration 169 : 0.3631163054574039
Loss in iteration 170 : 0.3631079029500394
Loss in iteration 171 : 0.3630997673153241
Loss in iteration 172 : 0.36309157248013685
Loss in iteration 173 : 0.3630834154870554
Loss in iteration 174 : 0.36307541155011536
Loss in iteration 175 : 0.36306744899880783
Loss in iteration 176 : 0.36305970282771066
Loss in iteration 177 : 0.3630522015751748
Loss in iteration 178 : 0.3630449571393221
Loss in iteration 179 : 0.36303794226070196
Loss in iteration 180 : 0.3630311331866065
Loss in iteration 181 : 0.3630242552156295
Loss in iteration 182 : 0.36301729002373523
Loss in iteration 183 : 0.3630102454848478
Loss in iteration 184 : 0.363003646178858
Loss in iteration 185 : 0.36299707314201834
Loss in iteration 186 : 0.3629903998316952
Loss in iteration 187 : 0.36298363543249157
Loss in iteration 188 : 0.36297687008374857
Loss in iteration 189 : 0.3629702392579641
Loss in iteration 190 : 0.36296362862658965
Loss in iteration 191 : 0.3629572034089387
Loss in iteration 192 : 0.36295074190295207
Loss in iteration 193 : 0.36294483904006203
Loss in iteration 194 : 0.36293885088832806
Loss in iteration 195 : 0.3629336280153857
Loss in iteration 196 : 0.3629283346391329
Loss in iteration 197 : 0.36292330448423243
Loss in iteration 198 : 0.3629183919584739
Loss in iteration 199 : 0.36291383203415767
Loss in iteration 200 : 0.3629092944670656
Testing accuracy  of updater 7 on alg 1 with rate 0.032 = 0.788, training accuracy 0.8423438005827129, time elapsed: 3961 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8641252882622729
Loss in iteration 3 : 0.6384256189253433
Loss in iteration 4 : 0.5660276198229839
Loss in iteration 5 : 0.6168975553812723
Loss in iteration 6 : 0.6638439846773861
Loss in iteration 7 : 0.691278584475682
Loss in iteration 8 : 0.700273435070177
Loss in iteration 9 : 0.6925597611568458
Loss in iteration 10 : 0.6697613833544797
Loss in iteration 11 : 0.6336010452471793
Loss in iteration 12 : 0.586191787783376
Loss in iteration 13 : 0.5340294958919333
Loss in iteration 14 : 0.49152129502076225
Loss in iteration 15 : 0.4702061104375842
Loss in iteration 16 : 0.46822960236041566
Loss in iteration 17 : 0.4764653604276982
Loss in iteration 18 : 0.48758933666892873
Loss in iteration 19 : 0.49468313869552705
Loss in iteration 20 : 0.4906902441538433
Loss in iteration 21 : 0.47520113370535355
Loss in iteration 22 : 0.453296687623371
Loss in iteration 23 : 0.43176121035430143
Loss in iteration 24 : 0.41666783105574884
Loss in iteration 25 : 0.4088439859157898
Loss in iteration 26 : 0.4077794643406772
Loss in iteration 27 : 0.4095786195146392
Loss in iteration 28 : 0.41132894942421333
Loss in iteration 29 : 0.4112245491789049
Loss in iteration 30 : 0.40839980040937385
Loss in iteration 31 : 0.4032098199122406
Loss in iteration 32 : 0.39657525342908057
Loss in iteration 33 : 0.3900647774107715
Loss in iteration 34 : 0.3854990186379057
Loss in iteration 35 : 0.38329305011852205
Loss in iteration 36 : 0.3826481540367231
Loss in iteration 37 : 0.38309833760311934
Loss in iteration 38 : 0.3836672948608298
Loss in iteration 39 : 0.38391038320674886
Loss in iteration 40 : 0.38358197944812616
Loss in iteration 41 : 0.38261401687790353
Loss in iteration 42 : 0.38116087398434517
Loss in iteration 43 : 0.37949931961657307
Loss in iteration 44 : 0.3779381265582547
Loss in iteration 45 : 0.3766781089632863
Loss in iteration 46 : 0.3757660173956262
Loss in iteration 47 : 0.375275536853449
Loss in iteration 48 : 0.37511857262500975
Loss in iteration 49 : 0.3752053564177496
Loss in iteration 50 : 0.37532869195134017
Loss in iteration 51 : 0.375340592755879
Loss in iteration 52 : 0.3751923264552982
Loss in iteration 53 : 0.3748879152324236
Loss in iteration 54 : 0.3744701530325028
Loss in iteration 55 : 0.37400235503190915
Loss in iteration 56 : 0.3735381752612607
Loss in iteration 57 : 0.37310571568494133
Loss in iteration 58 : 0.37275810084910893
Loss in iteration 59 : 0.3724992853288879
Loss in iteration 60 : 0.3723889132183013
Loss in iteration 61 : 0.37236200988951673
Loss in iteration 62 : 0.3723559097017613
Loss in iteration 63 : 0.3723367556166649
Loss in iteration 64 : 0.37226817621429203
Loss in iteration 65 : 0.3721398847408251
Loss in iteration 66 : 0.3719624382782523
Loss in iteration 67 : 0.37175113911142504
Loss in iteration 68 : 0.3715314452492245
Loss in iteration 69 : 0.3713549213264059
Loss in iteration 70 : 0.37123095191830496
Loss in iteration 71 : 0.3711583643541223
Loss in iteration 72 : 0.3711084552864402
Loss in iteration 73 : 0.3710617211842527
Loss in iteration 74 : 0.3710123686236216
Loss in iteration 75 : 0.37095346739550134
Loss in iteration 76 : 0.37088354881360036
Loss in iteration 77 : 0.370802575184319
Loss in iteration 78 : 0.37071256145387094
Loss in iteration 79 : 0.3706223123150276
Loss in iteration 80 : 0.3705298179452205
Loss in iteration 81 : 0.3704418154163396
Loss in iteration 82 : 0.370368669830662
Loss in iteration 83 : 0.37030334911828866
Loss in iteration 84 : 0.3702486567784895
Loss in iteration 85 : 0.37020264278934245
Loss in iteration 86 : 0.37015754931962935
Loss in iteration 87 : 0.37010965019672126
Loss in iteration 88 : 0.3700571321591822
Loss in iteration 89 : 0.37000188554691055
Loss in iteration 90 : 0.3699450814177729
Loss in iteration 91 : 0.3698885120087566
Loss in iteration 92 : 0.36983416868998775
Loss in iteration 93 : 0.3697831279419769
Loss in iteration 94 : 0.3697325606828132
Loss in iteration 95 : 0.36968241540098856
Loss in iteration 96 : 0.3696343989528669
Loss in iteration 97 : 0.36958895126227786
Loss in iteration 98 : 0.36954490227744063
Loss in iteration 99 : 0.36950089075917875
Loss in iteration 100 : 0.36945710409647325
Loss in iteration 101 : 0.36941332390799847
Loss in iteration 102 : 0.3693698485969363
Loss in iteration 103 : 0.369326313820857
Loss in iteration 104 : 0.369282860550769
Loss in iteration 105 : 0.3692397934407261
Loss in iteration 106 : 0.36919666551894653
Loss in iteration 107 : 0.36915486860222535
Loss in iteration 108 : 0.36911374086704185
Loss in iteration 109 : 0.3690728087154583
Loss in iteration 110 : 0.3690320739851454
Loss in iteration 111 : 0.368992296267639
Loss in iteration 112 : 0.368952684104314
Loss in iteration 113 : 0.368913217308739
Loss in iteration 114 : 0.36887387769115326
Loss in iteration 115 : 0.36883464886060024
Loss in iteration 116 : 0.36879585222096206
Loss in iteration 117 : 0.3687576287278231
Loss in iteration 118 : 0.36871975506883325
Loss in iteration 119 : 0.3686821485702187
Loss in iteration 120 : 0.3686447673585934
Loss in iteration 121 : 0.368607878848869
Loss in iteration 122 : 0.3685711902186363
Loss in iteration 123 : 0.36853481065024435
Loss in iteration 124 : 0.3684992772751806
Loss in iteration 125 : 0.36846450710790524
Loss in iteration 126 : 0.3684301114433595
Loss in iteration 127 : 0.36839577045996585
Loss in iteration 128 : 0.3683614753816162
Loss in iteration 129 : 0.36832732371835875
Loss in iteration 130 : 0.3682935315462792
Loss in iteration 131 : 0.3682598232656495
Loss in iteration 132 : 0.3682262933299214
Loss in iteration 133 : 0.36819352946807876
Loss in iteration 134 : 0.368160915581556
Loss in iteration 135 : 0.3681283515838595
Loss in iteration 136 : 0.3680958318095525
Loss in iteration 137 : 0.3680634690195667
Loss in iteration 138 : 0.368031209742929
Loss in iteration 139 : 0.36799919001333287
Loss in iteration 140 : 0.3679677561133606
Loss in iteration 141 : 0.36793746077335804
Loss in iteration 142 : 0.3679073234949033
Loss in iteration 143 : 0.3678772800217985
Loss in iteration 144 : 0.3678475283675406
Loss in iteration 145 : 0.3678178386622114
Loss in iteration 146 : 0.36778818370406785
Loss in iteration 147 : 0.3677589162428203
Loss in iteration 148 : 0.3677297359071344
Loss in iteration 149 : 0.3677007105273388
Loss in iteration 150 : 0.3676722251878173
Loss in iteration 151 : 0.36764391328658563
Loss in iteration 152 : 0.3676156597412164
Loss in iteration 153 : 0.36758745601861714
Loss in iteration 154 : 0.3675593942518682
Loss in iteration 155 : 0.36753132937748406
Loss in iteration 156 : 0.36750355121819994
Loss in iteration 157 : 0.3674759298284182
Loss in iteration 158 : 0.3674483619504335
Loss in iteration 159 : 0.3674208395948457
Loss in iteration 160 : 0.36739335556130587
Loss in iteration 161 : 0.36736590336032604
Loss in iteration 162 : 0.367338477142821
Loss in iteration 163 : 0.36731128762866155
Loss in iteration 164 : 0.36728421099098113
Loss in iteration 165 : 0.3672571830534049
Loss in iteration 166 : 0.36723027956891435
Loss in iteration 167 : 0.367203328880781
Loss in iteration 168 : 0.3671764825456006
Loss in iteration 169 : 0.3671498477740489
Loss in iteration 170 : 0.36712324258442414
Loss in iteration 171 : 0.3670967674102923
Loss in iteration 172 : 0.3670704612442225
Loss in iteration 173 : 0.367044477137957
Loss in iteration 174 : 0.36701865658560184
Loss in iteration 175 : 0.36699287567456873
Loss in iteration 176 : 0.36696712795476427
Loss in iteration 177 : 0.3669414076127699
Loss in iteration 178 : 0.36691570940874363
Loss in iteration 179 : 0.3668900286195805
Loss in iteration 180 : 0.3668643609876958
Loss in iteration 181 : 0.36683870267489604
Loss in iteration 182 : 0.3668130502208107
Loss in iteration 183 : 0.3667875339948001
Loss in iteration 184 : 0.3667624008284464
Loss in iteration 185 : 0.3667372721241874
Loss in iteration 186 : 0.3667121449511341
Loss in iteration 187 : 0.3666870166663516
Loss in iteration 188 : 0.366661884886327
Loss in iteration 189 : 0.36663679682385414
Loss in iteration 190 : 0.36661176223150804
Loss in iteration 191 : 0.36658664834795673
Loss in iteration 192 : 0.36656146046812865
Loss in iteration 193 : 0.36653650864643356
Loss in iteration 194 : 0.3665126839720198
Loss in iteration 195 : 0.3664897748616773
Loss in iteration 196 : 0.3664674027422572
Loss in iteration 197 : 0.36644457935847113
Loss in iteration 198 : 0.36642166275834304
Loss in iteration 199 : 0.3663992530739197
Loss in iteration 200 : 0.366376986072285
Testing accuracy  of updater 7 on alg 1 with rate 0.007999999999999993 = 0.78475, training accuracy 0.8394302363224344, time elapsed: 2745 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.45595856897186
Loss in iteration 3 : 3.3346000372421325
Loss in iteration 4 : 2.8496158580761484
Loss in iteration 5 : 2.0924113344009534
Loss in iteration 6 : 1.1355196614157554
Loss in iteration 7 : 0.5359421778347093
Loss in iteration 8 : 0.6730934158860896
Loss in iteration 9 : 0.8500451902826742
Loss in iteration 10 : 0.8121922769735487
Loss in iteration 11 : 0.7134353093291759
Loss in iteration 12 : 0.649686238546608
Loss in iteration 13 : 0.6333192785621158
Loss in iteration 14 : 0.6459883054507559
Loss in iteration 15 : 0.662309515030324
Loss in iteration 16 : 0.6697857286241911
Loss in iteration 17 : 0.668041196918638
Loss in iteration 18 : 0.660726524436888
Loss in iteration 19 : 0.6533203148701741
Loss in iteration 20 : 0.6482573301019343
Loss in iteration 21 : 0.6439551683670373
Loss in iteration 22 : 0.6388341634178436
Loss in iteration 23 : 0.6315697327925924
Loss in iteration 24 : 0.6222713054569703
Loss in iteration 25 : 0.6117626699381516
Loss in iteration 26 : 0.6006298136373712
Loss in iteration 27 : 0.589001831763688
Loss in iteration 28 : 0.5767242903125913
Loss in iteration 29 : 0.5636812199115964
Loss in iteration 30 : 0.5499027042564519
Loss in iteration 31 : 0.535630917292609
Loss in iteration 32 : 0.5209392508183314
Loss in iteration 33 : 0.5059851092982771
Loss in iteration 34 : 0.49074228695600136
Loss in iteration 35 : 0.4753164205924832
Loss in iteration 36 : 0.4597574498840829
Loss in iteration 37 : 0.4443429426960708
Loss in iteration 38 : 0.42945999259938156
Loss in iteration 39 : 0.41505261646326924
Loss in iteration 40 : 0.401329041510077
Loss in iteration 41 : 0.3885389321162517
Loss in iteration 42 : 0.3774341325855557
Loss in iteration 43 : 0.3690668174728598
Loss in iteration 44 : 0.36453665163937204
Loss in iteration 45 : 0.3648614548363297
Loss in iteration 46 : 0.3729722339395212
Loss in iteration 47 : 0.4054026404118312
Loss in iteration 48 : 0.594700147950927
Loss in iteration 49 : 0.640421942156092
Loss in iteration 50 : 0.9993890970579239
Loss in iteration 51 : 0.5847243316324079
Loss in iteration 52 : 0.4724390068174592
Loss in iteration 53 : 0.37727461320673683
Loss in iteration 54 : 0.3831915125143654
Loss in iteration 55 : 0.3875368412817259
Loss in iteration 56 : 0.3909189730969007
Loss in iteration 57 : 0.39305137336582013
Loss in iteration 58 : 0.3938240858480847
Loss in iteration 59 : 0.39333492995379954
Loss in iteration 60 : 0.3917459931027933
Loss in iteration 61 : 0.3891986048427551
Loss in iteration 62 : 0.38591842846487034
Loss in iteration 63 : 0.38208799428162227
Loss in iteration 64 : 0.37795635254694854
Loss in iteration 65 : 0.3738826258841573
Loss in iteration 66 : 0.37017789094635445
Loss in iteration 67 : 0.36705535236897163
Loss in iteration 68 : 0.364836597678368
Loss in iteration 69 : 0.3639481566391865
Loss in iteration 70 : 0.3643905943422389
Loss in iteration 71 : 0.36567141676642967
Loss in iteration 72 : 0.37155272006796175
Loss in iteration 73 : 0.4015975980934347
Loss in iteration 74 : 0.5285139359618729
Loss in iteration 75 : 0.5359119685751657
Loss in iteration 76 : 0.6785838406708414
Loss in iteration 77 : 0.3694569533032397
Loss in iteration 78 : 0.37465698904067635
Loss in iteration 79 : 0.3729662547865466
Loss in iteration 80 : 0.3751393495762212
Loss in iteration 81 : 0.37598581636960565
Loss in iteration 82 : 0.37624273176563927
Loss in iteration 83 : 0.37578932229287604
Loss in iteration 84 : 0.37466399466343525
Loss in iteration 85 : 0.3730683161194063
Loss in iteration 86 : 0.3711982799505293
Loss in iteration 87 : 0.3692161341646411
Loss in iteration 88 : 0.36725238833442814
Loss in iteration 89 : 0.36565646048617373
Loss in iteration 90 : 0.36450733439524813
Loss in iteration 91 : 0.36398601927316265
Loss in iteration 92 : 0.3644374340923803
Loss in iteration 93 : 0.36739789200271333
Loss in iteration 94 : 0.38052187713427593
Loss in iteration 95 : 0.43395874420840663
Loss in iteration 96 : 0.5066951976414691
Loss in iteration 97 : 0.7275765329974
Loss in iteration 98 : 0.3680850536935799
Loss in iteration 99 : 0.3737612691756753
Loss in iteration 100 : 0.3716917645413904
Loss in iteration 101 : 0.37419016389522863
Loss in iteration 102 : 0.3737184269893005
Loss in iteration 103 : 0.3730443501432223
Loss in iteration 104 : 0.3719260096583485
Loss in iteration 105 : 0.37064694303282286
Loss in iteration 106 : 0.36897781620855596
Loss in iteration 107 : 0.3673251270972908
Loss in iteration 108 : 0.36585072346477066
Loss in iteration 109 : 0.36477699552637
Loss in iteration 110 : 0.36462009265271017
Loss in iteration 111 : 0.3659770336248914
Loss in iteration 112 : 0.3761727278287625
Loss in iteration 113 : 0.4079320344914764
Loss in iteration 114 : 0.5404067723478297
Loss in iteration 115 : 0.5363716290795651
Loss in iteration 116 : 0.6846049746242894
Loss in iteration 117 : 0.3779336571141174
Loss in iteration 118 : 0.37802804404550294
Loss in iteration 119 : 0.3791567690812515
Loss in iteration 120 : 0.3809719333594376
Loss in iteration 121 : 0.3812791405458572
Loss in iteration 122 : 0.38044061942866947
Loss in iteration 123 : 0.3786245603820214
Loss in iteration 124 : 0.3761689514054129
Loss in iteration 125 : 0.37339909499822815
Loss in iteration 126 : 0.370606115113696
Loss in iteration 127 : 0.3679590043363197
Loss in iteration 128 : 0.3658070964906333
Loss in iteration 129 : 0.36438651204924544
Loss in iteration 130 : 0.36406605003010406
Loss in iteration 131 : 0.36554841244223907
Loss in iteration 132 : 0.37531475875783893
Loss in iteration 133 : 0.424784678342887
Loss in iteration 134 : 0.6409417922496226
Loss in iteration 135 : 0.484399143021031
Loss in iteration 136 : 0.6078886604214379
Loss in iteration 137 : 0.41790714801354784
Loss in iteration 138 : 0.39435265060729424
Loss in iteration 139 : 0.39037736730810985
Loss in iteration 140 : 0.3844588212801385
Loss in iteration 141 : 0.38423553275162514
Loss in iteration 142 : 0.3839076739811259
Loss in iteration 143 : 0.3824315260228651
Loss in iteration 144 : 0.3800037381124703
Loss in iteration 145 : 0.3770289779978209
Loss in iteration 146 : 0.3736619942920467
Loss in iteration 147 : 0.37047019251006175
Loss in iteration 148 : 0.36761157611198675
Loss in iteration 149 : 0.3655036385270807
Loss in iteration 150 : 0.36418095407430034
Loss in iteration 151 : 0.3644036060393203
Loss in iteration 152 : 0.3670393533179774
Loss in iteration 153 : 0.38128815612769823
Loss in iteration 154 : 0.45500381217768143
Loss in iteration 155 : 0.7530218340603705
Loss in iteration 156 : 0.3858085914103139
Loss in iteration 157 : 0.3878745162495797
Loss in iteration 158 : 0.422624599588708
Loss in iteration 159 : 0.45790429682197903
Loss in iteration 160 : 0.4090444775467353
Loss in iteration 161 : 0.3993614765325618
Loss in iteration 162 : 0.38940186109734415
Loss in iteration 163 : 0.3824344799121876
Loss in iteration 164 : 0.37974429079131666
Loss in iteration 165 : 0.37737637066158985
Loss in iteration 166 : 0.37497384012574936
Loss in iteration 167 : 0.3725168100750258
Loss in iteration 168 : 0.37117026634842376
Loss in iteration 169 : 0.3715430336563595
Loss in iteration 170 : 0.38039731552153744
Loss in iteration 171 : 0.4177501389219868
Loss in iteration 172 : 0.4629096886361291
Loss in iteration 173 : 0.6481264805974885
Loss in iteration 174 : 0.4303572777228131
Loss in iteration 175 : 0.45078472562855715
Loss in iteration 176 : 0.42512701834654
Loss in iteration 177 : 0.4223542133253567
Loss in iteration 178 : 0.3970653005130339
Loss in iteration 179 : 0.3872228787572123
Loss in iteration 180 : 0.38354297387213954
Loss in iteration 181 : 0.38015649051003236
Loss in iteration 182 : 0.3772907335678186
Loss in iteration 183 : 0.37484743368721124
Loss in iteration 184 : 0.3733176652580984
Loss in iteration 185 : 0.37410141080802595
Loss in iteration 186 : 0.38043241894150776
Loss in iteration 187 : 0.40744664020275234
Loss in iteration 188 : 0.4419850962625606
Loss in iteration 189 : 0.5877642024279904
Loss in iteration 190 : 0.47015145801600916
Loss in iteration 191 : 0.5286507166616694
Loss in iteration 192 : 0.41650728407811677
Loss in iteration 193 : 0.4046209961740219
Loss in iteration 194 : 0.3937026493711323
Loss in iteration 195 : 0.3847621836650452
Loss in iteration 196 : 0.38197576667390337
Loss in iteration 197 : 0.3794335079905487
Loss in iteration 198 : 0.3761623475897243
Loss in iteration 199 : 0.3734369115910783
Loss in iteration 200 : 0.37112319591679055
Testing accuracy  of updater 8 on alg 1 with rate 0.056 = 0.7895, training accuracy 0.8420200712204597, time elapsed: 2189 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.1933939331141468
Loss in iteration 3 : 2.3343199961770225
Loss in iteration 4 : 2.125213635371097
Loss in iteration 5 : 1.6067342094739079
Loss in iteration 6 : 0.8765402341626846
Loss in iteration 7 : 0.5285100702795995
Loss in iteration 8 : 0.6551447957404577
Loss in iteration 9 : 0.7814798042370734
Loss in iteration 10 : 0.7524541176774436
Loss in iteration 11 : 0.6849884647139458
Loss in iteration 12 : 0.6462007691697775
Loss in iteration 13 : 0.6433435271005699
Loss in iteration 14 : 0.6571069509636542
Loss in iteration 15 : 0.6692749292400902
Loss in iteration 16 : 0.6732781691642858
Loss in iteration 17 : 0.6695069759554692
Loss in iteration 18 : 0.6619806579363428
Loss in iteration 19 : 0.6551474197167606
Loss in iteration 20 : 0.6491712962855831
Loss in iteration 21 : 0.6431647790035391
Loss in iteration 22 : 0.6356559837761852
Loss in iteration 23 : 0.626142377124667
Loss in iteration 24 : 0.6150351146836428
Loss in iteration 25 : 0.6030622524819408
Loss in iteration 26 : 0.5906926360039948
Loss in iteration 27 : 0.5777745238701889
Loss in iteration 28 : 0.5642572088106774
Loss in iteration 29 : 0.5502271102131567
Loss in iteration 30 : 0.5356913081654272
Loss in iteration 31 : 0.5208198491863801
Loss in iteration 32 : 0.5058260762785719
Loss in iteration 33 : 0.49067779112131077
Loss in iteration 34 : 0.47544819150662504
Loss in iteration 35 : 0.46034781547330833
Loss in iteration 36 : 0.4454059791080255
Loss in iteration 37 : 0.43097186659020836
Loss in iteration 38 : 0.4171759625170851
Loss in iteration 39 : 0.40427097150999275
Loss in iteration 40 : 0.39231935394631573
Loss in iteration 41 : 0.3817164135326591
Loss in iteration 42 : 0.37338550216228816
Loss in iteration 43 : 0.36738992132836534
Loss in iteration 44 : 0.3650379674199267
Loss in iteration 45 : 0.36612986259422237
Loss in iteration 46 : 0.3693248548429438
Loss in iteration 47 : 0.3733295166638389
Loss in iteration 48 : 0.376338524469486
Loss in iteration 49 : 0.3803125222128525
Loss in iteration 50 : 0.3948235970876319
Loss in iteration 51 : 0.4656543958740084
Loss in iteration 52 : 0.6342889434087684
Loss in iteration 53 : 0.3661607384077697
Loss in iteration 54 : 0.3657995847820878
Loss in iteration 55 : 0.3649057727929085
Loss in iteration 56 : 0.36658334263722153
Loss in iteration 57 : 0.3677035526778779
Loss in iteration 58 : 0.36854196783398185
Loss in iteration 59 : 0.3689584159298289
Loss in iteration 60 : 0.36896353851922253
Loss in iteration 61 : 0.3685820608388668
Loss in iteration 62 : 0.36784837949677285
Loss in iteration 63 : 0.36693027103909626
Loss in iteration 64 : 0.3659750167353257
Loss in iteration 65 : 0.3651054846738154
Loss in iteration 66 : 0.36427685354233413
Loss in iteration 67 : 0.36364597287784234
Loss in iteration 68 : 0.3632063811058016
Loss in iteration 69 : 0.3629258381137503
Loss in iteration 70 : 0.36282953275039354
Loss in iteration 71 : 0.3628869305904313
Loss in iteration 72 : 0.36304231309997986
Loss in iteration 73 : 0.36333200576162106
Loss in iteration 74 : 0.3639272781994388
Loss in iteration 75 : 0.3647746211248628
Loss in iteration 76 : 0.3666916760464802
Loss in iteration 77 : 0.3691282237981268
Loss in iteration 78 : 0.3733188911736799
Loss in iteration 79 : 0.38293060586714367
Loss in iteration 80 : 0.38787500753076143
Loss in iteration 81 : 0.39663226450257266
Loss in iteration 82 : 0.3810578352424579
Loss in iteration 83 : 0.3806383036226031
Loss in iteration 84 : 0.3747837172376432
Loss in iteration 85 : 0.3712152026453932
Loss in iteration 86 : 0.3688713029750283
Loss in iteration 87 : 0.3665762258956568
Loss in iteration 88 : 0.3656638874295647
Loss in iteration 89 : 0.3654389017245672
Loss in iteration 90 : 0.365314247551296
Loss in iteration 91 : 0.3647987692571428
Loss in iteration 92 : 0.36411382763106576
Loss in iteration 93 : 0.3637974955326417
Loss in iteration 94 : 0.36345949377035885
Loss in iteration 95 : 0.36320816024045577
Loss in iteration 96 : 0.36345404705786266
Loss in iteration 97 : 0.36352453736474066
Loss in iteration 98 : 0.36383165110461263
Loss in iteration 99 : 0.36429530050042036
Loss in iteration 100 : 0.364600077774737
Loss in iteration 101 : 0.3668567721714291
Loss in iteration 102 : 0.3714838060210659
Loss in iteration 103 : 0.38096014714491927
Loss in iteration 104 : 0.3901318676832096
Loss in iteration 105 : 0.4065839742051098
Loss in iteration 106 : 0.3925425379885191
Loss in iteration 107 : 0.3947008432093406
Loss in iteration 108 : 0.3821048809873384
Loss in iteration 109 : 0.37818019220803095
Loss in iteration 110 : 0.37261876235849617
Loss in iteration 111 : 0.36814190328927926
Loss in iteration 112 : 0.3676407555195596
Loss in iteration 113 : 0.36638386109521903
Loss in iteration 114 : 0.3657928265833822
Loss in iteration 115 : 0.3662649854518919
Loss in iteration 116 : 0.36542143035366775
Loss in iteration 117 : 0.36518446611165495
Loss in iteration 118 : 0.3648341223958007
Loss in iteration 119 : 0.3645831834730335
Loss in iteration 120 : 0.36431859963698415
Loss in iteration 121 : 0.36442245080012586
Loss in iteration 122 : 0.3642256123469173
Loss in iteration 123 : 0.3651403813395286
Loss in iteration 124 : 0.36693346726116677
Loss in iteration 125 : 0.37034044263089383
Loss in iteration 126 : 0.37884035180572456
Loss in iteration 127 : 0.39787292147749753
Loss in iteration 128 : 0.39503600512197456
Loss in iteration 129 : 0.409983515601946
Loss in iteration 130 : 0.3955535921613538
Loss in iteration 131 : 0.40008006677986935
Loss in iteration 132 : 0.3856375692716841
Loss in iteration 133 : 0.38042901724825307
Loss in iteration 134 : 0.37318072206925
Loss in iteration 135 : 0.36838856707635903
Loss in iteration 136 : 0.36852284154149095
Loss in iteration 137 : 0.36784031582068094
Loss in iteration 138 : 0.3679519161433746
Loss in iteration 139 : 0.3672635042918162
Loss in iteration 140 : 0.3670229836699091
Loss in iteration 141 : 0.367131120332309
Loss in iteration 142 : 0.3674280438032675
Loss in iteration 143 : 0.37060430289470464
Loss in iteration 144 : 0.3741514466383097
Loss in iteration 145 : 0.38227396996618773
Loss in iteration 146 : 0.3870517872712639
Loss in iteration 147 : 0.4032121471521443
Loss in iteration 148 : 0.39312820729586856
Loss in iteration 149 : 0.40536429339992586
Loss in iteration 150 : 0.3910526138382608
Loss in iteration 151 : 0.3940339133248578
Loss in iteration 152 : 0.382954233705461
Loss in iteration 153 : 0.3811752536182985
Loss in iteration 154 : 0.37559670698321507
Loss in iteration 155 : 0.37327387045339727
Loss in iteration 156 : 0.3721954524181255
Loss in iteration 157 : 0.3699050380010376
Loss in iteration 158 : 0.3680904860484469
Loss in iteration 159 : 0.3674606600490375
Loss in iteration 160 : 0.3673369485206617
Loss in iteration 161 : 0.36794155756228575
Loss in iteration 162 : 0.3686007425116536
Loss in iteration 163 : 0.37171201593050074
Loss in iteration 164 : 0.3778672260658337
Loss in iteration 165 : 0.3930176751014571
Loss in iteration 166 : 0.39326627304577755
Loss in iteration 167 : 0.4098464149539391
Loss in iteration 168 : 0.3979353260727304
Loss in iteration 169 : 0.40877116335532676
Loss in iteration 170 : 0.39086087924316454
Loss in iteration 171 : 0.39041500305361154
Loss in iteration 172 : 0.3784671461939084
Loss in iteration 173 : 0.37516836114596164
Loss in iteration 174 : 0.3739343063683107
Loss in iteration 175 : 0.37034275729136434
Loss in iteration 176 : 0.36882655924872
Loss in iteration 177 : 0.3687564493519323
Loss in iteration 178 : 0.36802650279339066
Loss in iteration 179 : 0.3676611221537515
Loss in iteration 180 : 0.36738991794096965
Loss in iteration 181 : 0.3700676265413497
Loss in iteration 182 : 0.3725122594724
Loss in iteration 183 : 0.3787767530042484
Loss in iteration 184 : 0.39107534754670886
Loss in iteration 185 : 0.414335686147396
Loss in iteration 186 : 0.4046074992374267
Loss in iteration 187 : 0.42473961698133145
Loss in iteration 188 : 0.3970025953037824
Loss in iteration 189 : 0.398627900739976
Loss in iteration 190 : 0.38517472974426276
Loss in iteration 191 : 0.3824679120479219
Loss in iteration 192 : 0.37528895836699416
Loss in iteration 193 : 0.37018655535424805
Loss in iteration 194 : 0.3687418461729338
Loss in iteration 195 : 0.36815949195368175
Loss in iteration 196 : 0.3681120971812289
Loss in iteration 197 : 0.3679196931272156
Loss in iteration 198 : 0.3676203706359481
Loss in iteration 199 : 0.36797219204212656
Loss in iteration 200 : 0.3695941067811525
Testing accuracy  of updater 8 on alg 1 with rate 0.0392 = 0.7725, training accuracy 0.8368404014244092, time elapsed: 2300 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.5851474390345432
Loss in iteration 3 : 1.689447101125188
Loss in iteration 4 : 1.5595016730829097
Loss in iteration 5 : 1.2218370516038946
Loss in iteration 6 : 0.7209726952982981
Loss in iteration 7 : 0.43927829021243475
Loss in iteration 8 : 0.5298578077244825
Loss in iteration 9 : 0.6121849542038574
Loss in iteration 10 : 0.5928472211307704
Loss in iteration 11 : 0.5470047414642852
Loss in iteration 12 : 0.5189699880301601
Loss in iteration 13 : 0.514529006511097
Loss in iteration 14 : 0.5247394592892103
Loss in iteration 15 : 0.5338821785963987
Loss in iteration 16 : 0.5374128088853797
Loss in iteration 17 : 0.5358754734150025
Loss in iteration 18 : 0.5317893235911334
Loss in iteration 19 : 0.527549409264734
Loss in iteration 20 : 0.5244794263612893
Loss in iteration 21 : 0.5218119193105716
Loss in iteration 22 : 0.517920572916568
Loss in iteration 23 : 0.5125236721367635
Loss in iteration 24 : 0.5064557854423721
Loss in iteration 25 : 0.5001527256238767
Loss in iteration 26 : 0.49352255935750755
Loss in iteration 27 : 0.48645164027708426
Loss in iteration 28 : 0.47896757724118616
Loss in iteration 29 : 0.47122970468425324
Loss in iteration 30 : 0.46330529825100325
Loss in iteration 31 : 0.4552023247983534
Loss in iteration 32 : 0.4469417377824484
Loss in iteration 33 : 0.4386400765263134
Loss in iteration 34 : 0.4303843409249486
Loss in iteration 35 : 0.4222554681174321
Loss in iteration 36 : 0.4142780363305337
Loss in iteration 37 : 0.4065487383391352
Loss in iteration 38 : 0.39923095596784025
Loss in iteration 39 : 0.3924573943936034
Loss in iteration 40 : 0.3861460847119593
Loss in iteration 41 : 0.3803224796853246
Loss in iteration 42 : 0.37523144536181613
Loss in iteration 43 : 0.37111999393034784
Loss in iteration 44 : 0.3680244458593867
Loss in iteration 45 : 0.36603674168955874
Loss in iteration 46 : 0.36508029747677834
Loss in iteration 47 : 0.36524838072042143
Loss in iteration 48 : 0.3661799776794761
Loss in iteration 49 : 0.3673333604200814
Loss in iteration 50 : 0.3683806579686142
Loss in iteration 51 : 0.36912486733051725
Loss in iteration 52 : 0.36943984582229383
Loss in iteration 53 : 0.36932399483552686
Loss in iteration 54 : 0.3686842221602854
Loss in iteration 55 : 0.36778726509091897
Loss in iteration 56 : 0.3669050634895493
Loss in iteration 57 : 0.3660449687707831
Loss in iteration 58 : 0.3653739653270178
Loss in iteration 59 : 0.36487625944238367
Loss in iteration 60 : 0.36446129044453435
Loss in iteration 61 : 0.36415332307169523
Loss in iteration 62 : 0.36394360347895943
Loss in iteration 63 : 0.36379961495312824
Loss in iteration 64 : 0.36370251362607764
Loss in iteration 65 : 0.36364199907714473
Loss in iteration 66 : 0.363599627019701
Loss in iteration 67 : 0.36358257336748134
Loss in iteration 68 : 0.3635710044506544
Loss in iteration 69 : 0.3635551371083929
Loss in iteration 70 : 0.3635267119887411
Loss in iteration 71 : 0.3634887025227926
Loss in iteration 72 : 0.3634358206149011
Loss in iteration 73 : 0.3633737231147834
Loss in iteration 74 : 0.36330800476470954
Loss in iteration 75 : 0.3632436409048391
Loss in iteration 76 : 0.36318703244348444
Loss in iteration 77 : 0.36313597440892276
Loss in iteration 78 : 0.3630869609966574
Loss in iteration 79 : 0.36304250857901016
Loss in iteration 80 : 0.3629989039366355
Loss in iteration 81 : 0.36296011566508685
Loss in iteration 82 : 0.36292365500398965
Loss in iteration 83 : 0.3628946044349552
Loss in iteration 84 : 0.3628812087467703
Loss in iteration 85 : 0.3628730054349831
Loss in iteration 86 : 0.3628664596267057
Loss in iteration 87 : 0.36286004844062675
Loss in iteration 88 : 0.3628546566221604
Loss in iteration 89 : 0.36284921599748715
Loss in iteration 90 : 0.3628439208530632
Loss in iteration 91 : 0.3628375931281592
Loss in iteration 92 : 0.3628310902435374
Loss in iteration 93 : 0.36281946214031585
Loss in iteration 94 : 0.3628098650694382
Loss in iteration 95 : 0.3627969602758775
Loss in iteration 96 : 0.3627927531294604
Loss in iteration 97 : 0.3628027346585046
Loss in iteration 98 : 0.3628413001826836
Loss in iteration 99 : 0.3627682680751026
Loss in iteration 100 : 0.3627593045109889
Loss in iteration 101 : 0.36275477660882793
Loss in iteration 102 : 0.36275076343549273
Loss in iteration 103 : 0.36275131861904314
Loss in iteration 104 : 0.3627760161709445
Loss in iteration 105 : 0.36274682658410395
Loss in iteration 106 : 0.3627480741176688
Loss in iteration 107 : 0.3627580451336196
Loss in iteration 108 : 0.3627826342822208
Loss in iteration 109 : 0.36273835418028005
Loss in iteration 110 : 0.3627378421788735
Loss in iteration 111 : 0.36274702352607174
Loss in iteration 112 : 0.36275787810967086
Loss in iteration 113 : 0.3627402757744356
Loss in iteration 114 : 0.36272953824881793
Loss in iteration 115 : 0.36273646083944394
Loss in iteration 116 : 0.3627327392667859
Loss in iteration 117 : 0.36275771318653866
Loss in iteration 118 : 0.3627670492556856
Loss in iteration 119 : 0.3627279424759343
Loss in iteration 120 : 0.3627239269636742
Loss in iteration 121 : 0.36272306389495784
Loss in iteration 122 : 0.3627220435720718
Loss in iteration 123 : 0.3627214578249094
Testing accuracy  of updater 8 on alg 1 with rate 0.0224 = 0.79025, training accuracy 0.842667529944966, time elapsed: 2253 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6704590563664917
Loss in iteration 3 : 0.6911882774306528
Loss in iteration 4 : 0.6503165555880582
Loss in iteration 5 : 0.5578895655999819
Loss in iteration 6 : 0.4541708411031974
Loss in iteration 7 : 0.4092279347893006
Loss in iteration 8 : 0.41727916310920893
Loss in iteration 9 : 0.4106890785655724
Loss in iteration 10 : 0.3931662138830549
Loss in iteration 11 : 0.38049221343593126
Loss in iteration 12 : 0.3756751189001272
Loss in iteration 13 : 0.3748251065013844
Loss in iteration 14 : 0.3751266148394696
Loss in iteration 15 : 0.3753845160735755
Loss in iteration 16 : 0.3754978750783724
Loss in iteration 17 : 0.3755502903805442
Loss in iteration 18 : 0.3757206124191603
Loss in iteration 19 : 0.37610523279216207
Loss in iteration 20 : 0.37663363258661575
Loss in iteration 21 : 0.37712508738926076
Loss in iteration 22 : 0.37754359098616
Loss in iteration 23 : 0.3778521638585468
Loss in iteration 24 : 0.37803677505833183
Loss in iteration 25 : 0.3781314637460909
Loss in iteration 26 : 0.37814927401199955
Loss in iteration 27 : 0.3781044819663001
Loss in iteration 28 : 0.3780117908599626
Loss in iteration 29 : 0.37788384105119005
Loss in iteration 30 : 0.3776976628449571
Loss in iteration 31 : 0.37745660604660536
Loss in iteration 32 : 0.377173591565211
Loss in iteration 33 : 0.3768479811444364
Loss in iteration 34 : 0.37649695764475016
Loss in iteration 35 : 0.3761224431061931
Loss in iteration 36 : 0.37572737621115604
Loss in iteration 37 : 0.3753190999321313
Loss in iteration 38 : 0.37489716897394926
Loss in iteration 39 : 0.3744699594011119
Loss in iteration 40 : 0.37403554976650594
Loss in iteration 41 : 0.37360389476184536
Loss in iteration 42 : 0.3731908113387825
Loss in iteration 43 : 0.372779346241045
Loss in iteration 44 : 0.3723849673255176
Loss in iteration 45 : 0.37200208955695724
Loss in iteration 46 : 0.3716298639235193
Loss in iteration 47 : 0.371286566777539
Loss in iteration 48 : 0.3709888461826017
Loss in iteration 49 : 0.37071191706692325
Loss in iteration 50 : 0.37045458163667166
Loss in iteration 51 : 0.3702119738360714
Loss in iteration 52 : 0.36998539056592034
Loss in iteration 53 : 0.36978475199154603
Loss in iteration 54 : 0.36959410762216477
Loss in iteration 55 : 0.36940923817582455
Loss in iteration 56 : 0.36924105358920484
Loss in iteration 57 : 0.36910076974492423
Loss in iteration 58 : 0.3689746500745387
Loss in iteration 59 : 0.3688552253076073
Loss in iteration 60 : 0.3687449341215165
Loss in iteration 61 : 0.36864595047208787
Loss in iteration 62 : 0.3685542351428934
Loss in iteration 63 : 0.36846340759228496
Loss in iteration 64 : 0.3683822697287328
Loss in iteration 65 : 0.36831013619677594
Loss in iteration 66 : 0.3682435925657145
Loss in iteration 67 : 0.3681747239062465
Loss in iteration 68 : 0.36810749254753933
Loss in iteration 69 : 0.368040074218659
Loss in iteration 70 : 0.36797468552152923
Loss in iteration 71 : 0.36791167373074174
Loss in iteration 72 : 0.3678474719642289
Loss in iteration 73 : 0.36778271580928307
Loss in iteration 74 : 0.3677185048598265
Loss in iteration 75 : 0.36765433994790414
Loss in iteration 76 : 0.36758993423090597
Loss in iteration 77 : 0.3675264150540548
Loss in iteration 78 : 0.36746266497176666
Loss in iteration 79 : 0.36739900919271307
Loss in iteration 80 : 0.36733685290661
Loss in iteration 81 : 0.36727579036971425
Loss in iteration 82 : 0.36721530982499306
Loss in iteration 83 : 0.3671555458875246
Loss in iteration 84 : 0.3670961133862128
Loss in iteration 85 : 0.3670370362925662
Loss in iteration 86 : 0.3669785986420097
Loss in iteration 87 : 0.3669213273799875
Loss in iteration 88 : 0.36686431992175134
Loss in iteration 89 : 0.3668088852154011
Loss in iteration 90 : 0.3667564859828531
Loss in iteration 91 : 0.36670533793951027
Loss in iteration 92 : 0.3666551312344417
Loss in iteration 93 : 0.36660489347578246
Loss in iteration 94 : 0.366557243168018
Loss in iteration 95 : 0.36650808185809636
Loss in iteration 96 : 0.36645953599529535
Loss in iteration 97 : 0.36641233561901626
Loss in iteration 98 : 0.36636531428206887
Loss in iteration 99 : 0.3663193842592581
Loss in iteration 100 : 0.36627366315080734
Loss in iteration 101 : 0.36622838575721606
Loss in iteration 102 : 0.3661836389354894
Loss in iteration 103 : 0.3661392694259952
Loss in iteration 104 : 0.3660956344112283
Loss in iteration 105 : 0.3660521490700502
Loss in iteration 106 : 0.36600960842700536
Loss in iteration 107 : 0.36596760173370485
Loss in iteration 108 : 0.36592563634227365
Loss in iteration 109 : 0.36588370575027834
Loss in iteration 110 : 0.36584186394618357
Loss in iteration 111 : 0.36580061617026244
Loss in iteration 112 : 0.36576006656753957
Loss in iteration 113 : 0.36572017963177406
Loss in iteration 114 : 0.3656801400741865
Loss in iteration 115 : 0.36564048149994693
Loss in iteration 116 : 0.3656006343926743
Loss in iteration 117 : 0.36556200327422794
Loss in iteration 118 : 0.36552481263652276
Loss in iteration 119 : 0.36548788934638127
Loss in iteration 120 : 0.36545102568560633
Loss in iteration 121 : 0.36541421358483284
Loss in iteration 122 : 0.3653774457847137
Loss in iteration 123 : 0.36534144251556294
Loss in iteration 124 : 0.3653054744186797
Loss in iteration 125 : 0.36527007192767547
Loss in iteration 126 : 0.36523560016592066
Loss in iteration 127 : 0.365202084984336
Loss in iteration 128 : 0.3651686822332134
Loss in iteration 129 : 0.36513611254413003
Loss in iteration 130 : 0.36510393268919195
Loss in iteration 131 : 0.36507255997192706
Loss in iteration 132 : 0.3650416281218998
Loss in iteration 133 : 0.36501117248960363
Loss in iteration 134 : 0.3649808857069247
Loss in iteration 135 : 0.3649513127997144
Loss in iteration 136 : 0.3649221911751886
Loss in iteration 137 : 0.3648929295539316
Loss in iteration 138 : 0.36486383996371125
Loss in iteration 139 : 0.36483649400713675
Loss in iteration 140 : 0.3648090683781404
Loss in iteration 141 : 0.36478205965027094
Loss in iteration 142 : 0.36475558276629777
Loss in iteration 143 : 0.36472965518641304
Loss in iteration 144 : 0.3647039406017912
Loss in iteration 145 : 0.364678250096205
Loss in iteration 146 : 0.36465374142224216
Loss in iteration 147 : 0.36462927352361124
Loss in iteration 148 : 0.36460542063909906
Loss in iteration 149 : 0.36458192323058985
Loss in iteration 150 : 0.3645587407346954
Loss in iteration 151 : 0.3645356852602304
Loss in iteration 152 : 0.3645128959529776
Loss in iteration 153 : 0.3644902224731485
Loss in iteration 154 : 0.3644675999995786
Loss in iteration 155 : 0.36444516079868144
Loss in iteration 156 : 0.36442386790957354
Loss in iteration 157 : 0.3644016068144108
Loss in iteration 158 : 0.36438031044189917
Loss in iteration 159 : 0.3643586786441433
Loss in iteration 160 : 0.3643382390529412
Loss in iteration 161 : 0.36431845007771746
Loss in iteration 162 : 0.3642989781989581
Loss in iteration 163 : 0.3642799419173274
Loss in iteration 164 : 0.3642612538572325
Loss in iteration 165 : 0.36424297783647674
Loss in iteration 166 : 0.3642248162228442
Loss in iteration 167 : 0.3642069593013813
Loss in iteration 168 : 0.36418896807568274
Loss in iteration 169 : 0.36417124647797533
Loss in iteration 170 : 0.3641537812975121
Loss in iteration 171 : 0.3641364653279443
Loss in iteration 172 : 0.36411921690228155
Loss in iteration 173 : 0.3641020280402002
Loss in iteration 174 : 0.3640848915548175
Loss in iteration 175 : 0.36406787711904653
Loss in iteration 176 : 0.3640511780179195
Loss in iteration 177 : 0.3640347283563581
Loss in iteration 178 : 0.3640186778424732
Loss in iteration 179 : 0.36400250633051967
Loss in iteration 180 : 0.3639863727644662
Loss in iteration 181 : 0.3639707762973664
Loss in iteration 182 : 0.3639552753197046
Loss in iteration 183 : 0.36393953427962605
Loss in iteration 184 : 0.36392428786012143
Loss in iteration 185 : 0.3639092674024347
Loss in iteration 186 : 0.36389471375120636
Loss in iteration 187 : 0.3638805376896697
Loss in iteration 188 : 0.3638663717448482
Loss in iteration 189 : 0.363852586199778
Loss in iteration 190 : 0.363839343255305
Loss in iteration 191 : 0.3638262401418809
Loss in iteration 192 : 0.3638121206398462
Loss in iteration 193 : 0.3637992347774088
Loss in iteration 194 : 0.3637859793032771
Loss in iteration 195 : 0.363773233741893
Loss in iteration 196 : 0.3637623529638067
Loss in iteration 197 : 0.3637499223057509
Loss in iteration 198 : 0.3637379577906186
Loss in iteration 199 : 0.3637268283697362
Loss in iteration 200 : 0.36371535194048243
Testing accuracy  of updater 8 on alg 1 with rate 0.0056 = 0.78825, training accuracy 0.8423438005827129, time elapsed: 2856 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6031366861413295
Loss in iteration 3 : 0.6210322194998246
Loss in iteration 4 : 0.5907009807771919
Loss in iteration 5 : 0.5266170003614215
Loss in iteration 6 : 0.46829996576116134
Loss in iteration 7 : 0.44097967229959684
Loss in iteration 8 : 0.428333746299494
Loss in iteration 9 : 0.4190309700640595
Loss in iteration 10 : 0.4056136115567691
Loss in iteration 11 : 0.3921840112341054
Loss in iteration 12 : 0.3840029732325143
Loss in iteration 13 : 0.3797997148088253
Loss in iteration 14 : 0.37774138712622735
Loss in iteration 15 : 0.37652261721369185
Loss in iteration 16 : 0.37547413457701684
Loss in iteration 17 : 0.37449242528043664
Loss in iteration 18 : 0.37383992245367126
Loss in iteration 19 : 0.3737033429991163
Loss in iteration 20 : 0.3737392923935972
Loss in iteration 21 : 0.3738921436139455
Loss in iteration 22 : 0.3740669334076946
Loss in iteration 23 : 0.3742605950661973
Loss in iteration 24 : 0.3744321793338369
Loss in iteration 25 : 0.3745812555913708
Loss in iteration 26 : 0.3747351017255656
Loss in iteration 27 : 0.37487942510470673
Loss in iteration 28 : 0.37498527484652466
Loss in iteration 29 : 0.37505216734735947
Loss in iteration 30 : 0.37507829720698976
Loss in iteration 31 : 0.3750673953260971
Loss in iteration 32 : 0.37502528664560847
Loss in iteration 33 : 0.37495071885906617
Loss in iteration 34 : 0.3748464064848865
Loss in iteration 35 : 0.37471526925587906
Loss in iteration 36 : 0.3745599375046241
Loss in iteration 37 : 0.3743848367559046
Loss in iteration 38 : 0.37419329546911617
Loss in iteration 39 : 0.3739894897540163
Loss in iteration 40 : 0.37377846602465326
Loss in iteration 41 : 0.3735628572967511
Loss in iteration 42 : 0.37333959004219924
Loss in iteration 43 : 0.37311238696457105
Loss in iteration 44 : 0.3728831349485534
Loss in iteration 45 : 0.3726589283468088
Loss in iteration 46 : 0.37243797894131647
Loss in iteration 47 : 0.37221783664870556
Loss in iteration 48 : 0.37200353627175325
Loss in iteration 49 : 0.37180056053195754
Loss in iteration 50 : 0.37160899250812285
Loss in iteration 51 : 0.3714214529256276
Loss in iteration 52 : 0.37124466124240796
Loss in iteration 53 : 0.37107799345382225
Loss in iteration 54 : 0.37092058803491135
Loss in iteration 55 : 0.37076792571372713
Loss in iteration 56 : 0.3706218736944416
Loss in iteration 57 : 0.3704814993231514
Loss in iteration 58 : 0.3703467753707928
Loss in iteration 59 : 0.37021753495761983
Loss in iteration 60 : 0.37009663048856395
Loss in iteration 61 : 0.3699855894973151
Loss in iteration 62 : 0.3698812939772074
Loss in iteration 63 : 0.36978331841817075
Loss in iteration 64 : 0.36969472997343233
Loss in iteration 65 : 0.36961017978108274
Loss in iteration 66 : 0.3695302578679639
Loss in iteration 67 : 0.36945302051938833
Loss in iteration 68 : 0.36937781160066724
Loss in iteration 69 : 0.369305674795272
Loss in iteration 70 : 0.3692363531572578
Loss in iteration 71 : 0.36916937023358
Loss in iteration 72 : 0.3691036815357947
Loss in iteration 73 : 0.3690409243136572
Loss in iteration 74 : 0.36898195051145
Loss in iteration 75 : 0.36892412399622754
Loss in iteration 76 : 0.3688667346340146
Loss in iteration 77 : 0.3688095573948815
Loss in iteration 78 : 0.368753223461268
Loss in iteration 79 : 0.3686979320810291
Loss in iteration 80 : 0.3686431380400605
Loss in iteration 81 : 0.3685914331022281
Loss in iteration 82 : 0.36853794962225855
Loss in iteration 83 : 0.3684886368234635
Loss in iteration 84 : 0.3684385880933277
Loss in iteration 85 : 0.36838981312974217
Loss in iteration 86 : 0.3683410562799297
Loss in iteration 87 : 0.3682922999373006
Loss in iteration 88 : 0.36824354329249004
Loss in iteration 89 : 0.36819478562932084
Loss in iteration 90 : 0.3681460263143818
Loss in iteration 91 : 0.3680974210483955
Loss in iteration 92 : 0.36804875769241363
Loss in iteration 93 : 0.3680001325346719
Loss in iteration 94 : 0.36795281767419036
Loss in iteration 95 : 0.36790481203513525
Loss in iteration 96 : 0.36785760067848433
Loss in iteration 97 : 0.36781093485455635
Loss in iteration 98 : 0.3677648394426762
Loss in iteration 99 : 0.36772029970119074
Loss in iteration 100 : 0.3676759350342215
Loss in iteration 101 : 0.3676328733300461
Loss in iteration 102 : 0.36759016271411143
Loss in iteration 103 : 0.36754763660146633
Loss in iteration 104 : 0.3675052755298077
Loss in iteration 105 : 0.367463061982891
Loss in iteration 106 : 0.3674210848578444
Loss in iteration 107 : 0.36737983148777026
Loss in iteration 108 : 0.36733909039835805
Loss in iteration 109 : 0.367298663577104
Loss in iteration 110 : 0.3672583707137385
Loss in iteration 111 : 0.3672183506999819
Loss in iteration 112 : 0.3671788237270435
Loss in iteration 113 : 0.36713942647217956
Loss in iteration 114 : 0.36710052022697554
Loss in iteration 115 : 0.36706227512589934
Loss in iteration 116 : 0.36702467066441774
Loss in iteration 117 : 0.36698733442670656
Loss in iteration 118 : 0.36695021542753326
Loss in iteration 119 : 0.3669135171755148
Loss in iteration 120 : 0.3668776542896345
Loss in iteration 121 : 0.36684195014363846
Loss in iteration 122 : 0.3668064409016427
Loss in iteration 123 : 0.36677121830945636
Loss in iteration 124 : 0.36673699017804157
Loss in iteration 125 : 0.36670303595334663
Loss in iteration 126 : 0.3666694698038768
Loss in iteration 127 : 0.3666360085688296
Loss in iteration 128 : 0.36660287271336656
Loss in iteration 129 : 0.3665696530753122
Loss in iteration 130 : 0.36653696633525423
Loss in iteration 131 : 0.36650452626028496
Loss in iteration 132 : 0.3664721670544053
Loss in iteration 133 : 0.3664398794085907
Loss in iteration 134 : 0.3664077746742835
Loss in iteration 135 : 0.3663759366437089
Loss in iteration 136 : 0.3663443669346417
Loss in iteration 137 : 0.36631287399004897
Loss in iteration 138 : 0.3662814511101581
Loss in iteration 139 : 0.3662504972467745
Loss in iteration 140 : 0.36621986320530064
Loss in iteration 141 : 0.3661893105150635
Loss in iteration 142 : 0.3661589605949471
Loss in iteration 143 : 0.36612852650751465
Loss in iteration 144 : 0.36609821407032966
Loss in iteration 145 : 0.3660683651537542
Loss in iteration 146 : 0.3660389540839909
Loss in iteration 147 : 0.3660100553309818
Loss in iteration 148 : 0.36598140118500605
Loss in iteration 149 : 0.36595284059170335
Loss in iteration 150 : 0.36592436304273607
Loss in iteration 151 : 0.3658959590779368
Loss in iteration 152 : 0.36586762018089547
Loss in iteration 153 : 0.36583933868493806
Loss in iteration 154 : 0.3658113312010586
Loss in iteration 155 : 0.365783629013549
Loss in iteration 156 : 0.3657560726160252
Loss in iteration 157 : 0.3657283607631994
Loss in iteration 158 : 0.3657005074839277
Loss in iteration 159 : 0.36567257497110034
Loss in iteration 160 : 0.3656450548937694
Loss in iteration 161 : 0.3656182614091766
Loss in iteration 162 : 0.36559185676289735
Loss in iteration 163 : 0.36556595912773737
Loss in iteration 164 : 0.36554048850861726
Loss in iteration 165 : 0.3655151026530976
Loss in iteration 166 : 0.36548979195226605
Loss in iteration 167 : 0.36546454775588777
Loss in iteration 168 : 0.36543936227687007
Loss in iteration 169 : 0.365414228505248
Loss in iteration 170 : 0.3653891529873705
Loss in iteration 171 : 0.36536502079760413
Loss in iteration 172 : 0.36534088845098217
Loss in iteration 173 : 0.3653166849625741
Loss in iteration 174 : 0.3652924161190707
Loss in iteration 175 : 0.3652684400217261
Loss in iteration 176 : 0.3652452520521814
Loss in iteration 177 : 0.3652221179266101
Loss in iteration 178 : 0.36519914166250106
Loss in iteration 179 : 0.36517630505272164
Loss in iteration 180 : 0.36515352380545407
Loss in iteration 181 : 0.36513095341433854
Loss in iteration 182 : 0.36510840140253437
Loss in iteration 183 : 0.3650865343264456
Loss in iteration 184 : 0.3650651552946554
Loss in iteration 185 : 0.3650441297148803
Loss in iteration 186 : 0.36502331783899283
Loss in iteration 187 : 0.36500257720046014
Loss in iteration 188 : 0.3649818996255649
Loss in iteration 189 : 0.36496127775581994
Loss in iteration 190 : 0.36494070496673414
Loss in iteration 191 : 0.36492109421436675
Loss in iteration 192 : 0.3649011481879489
Loss in iteration 193 : 0.36488102773051123
Loss in iteration 194 : 0.36486091990050973
Loss in iteration 195 : 0.3648419781525094
Loss in iteration 196 : 0.364823016085549
Loss in iteration 197 : 0.36480426946742217
Loss in iteration 198 : 0.3647860429043589
Loss in iteration 199 : 0.3647679126272348
Loss in iteration 200 : 0.3647499256354205
Testing accuracy  of updater 8 on alg 1 with rate 0.00392 = 0.78775, training accuracy 0.8416963418582065, time elapsed: 3041 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.549550702577325
Loss in iteration 3 : 0.5746394092870603
Loss in iteration 4 : 0.5818890507212893
Loss in iteration 5 : 0.563232810735035
Loss in iteration 6 : 0.5286168509230116
Loss in iteration 7 : 0.49608868874159884
Loss in iteration 8 : 0.4762723535826767
Loss in iteration 9 : 0.4646004467507655
Loss in iteration 10 : 0.4540636939857723
Loss in iteration 11 : 0.44231286160193295
Loss in iteration 12 : 0.4301747464535083
Loss in iteration 13 : 0.41847805564508456
Loss in iteration 14 : 0.4078140264485977
Loss in iteration 15 : 0.39905070151321714
Loss in iteration 16 : 0.3927258494551196
Loss in iteration 17 : 0.3880865465152572
Loss in iteration 18 : 0.3847244411836877
Loss in iteration 19 : 0.3820881130397492
Loss in iteration 20 : 0.38001962678674495
Loss in iteration 21 : 0.3784594507671553
Loss in iteration 22 : 0.37720542565628057
Loss in iteration 23 : 0.3762278894463116
Loss in iteration 24 : 0.3754088888665441
Loss in iteration 25 : 0.3747612523814373
Loss in iteration 26 : 0.37423032933285755
Loss in iteration 27 : 0.3738506360326785
Loss in iteration 28 : 0.3736167464235911
Loss in iteration 29 : 0.37345287362979585
Loss in iteration 30 : 0.37333288435625966
Loss in iteration 31 : 0.3732363743480473
Loss in iteration 32 : 0.37316777670478235
Loss in iteration 33 : 0.37311596139156394
Loss in iteration 34 : 0.37308740670630397
Loss in iteration 35 : 0.37306435933633325
Loss in iteration 36 : 0.3730497184199067
Loss in iteration 37 : 0.3730319607318426
Loss in iteration 38 : 0.3730117073490112
Loss in iteration 39 : 0.372991288160105
Loss in iteration 40 : 0.37296947662569363
Loss in iteration 41 : 0.37294252472273914
Loss in iteration 42 : 0.37290987290810895
Loss in iteration 43 : 0.3728713530041688
Loss in iteration 44 : 0.37282815839897654
Loss in iteration 45 : 0.3727811447178379
Loss in iteration 46 : 0.37273253893807223
Loss in iteration 47 : 0.37268264137565876
Loss in iteration 48 : 0.3726301334876352
Loss in iteration 49 : 0.37257519952716234
Loss in iteration 50 : 0.37251838626265427
Loss in iteration 51 : 0.3724599014107554
Loss in iteration 52 : 0.3723990081300071
Loss in iteration 53 : 0.3723380412799743
Loss in iteration 54 : 0.37227592623057737
Loss in iteration 55 : 0.37221342553377956
Loss in iteration 56 : 0.37215024308821887
Loss in iteration 57 : 0.3720861704110705
Loss in iteration 58 : 0.3720215310432691
Loss in iteration 59 : 0.3719567093124051
Loss in iteration 60 : 0.3718914297420733
Loss in iteration 61 : 0.3718262289868183
Loss in iteration 62 : 0.3717622096671242
Loss in iteration 63 : 0.37170048213661755
Loss in iteration 64 : 0.3716391080323426
Loss in iteration 65 : 0.37157894748879894
Loss in iteration 66 : 0.3715188848586468
Loss in iteration 67 : 0.37145985634376777
Loss in iteration 68 : 0.3714024634285306
Loss in iteration 69 : 0.37134620460978957
Loss in iteration 70 : 0.3712901427219928
Loss in iteration 71 : 0.3712348440530596
Loss in iteration 72 : 0.37117985329790154
Loss in iteration 73 : 0.37112546983406897
Loss in iteration 74 : 0.3710715719684257
Loss in iteration 75 : 0.37101765015626537
Loss in iteration 76 : 0.37096408822570653
Loss in iteration 77 : 0.3709108820174394
Loss in iteration 78 : 0.37085970203541674
Loss in iteration 79 : 0.3708110008049313
Loss in iteration 80 : 0.3707627659171138
Loss in iteration 81 : 0.3707148453758491
Loss in iteration 82 : 0.37066750040280216
Loss in iteration 83 : 0.3706216827817481
Loss in iteration 84 : 0.37057687388830485
Loss in iteration 85 : 0.37053261525869063
Loss in iteration 86 : 0.37048958692535305
Loss in iteration 87 : 0.3704474227646829
Loss in iteration 88 : 0.37040628676572157
Loss in iteration 89 : 0.37036563112714727
Loss in iteration 90 : 0.3703252476316482
Loss in iteration 91 : 0.3702855192935309
Loss in iteration 92 : 0.3702463301868539
Loss in iteration 93 : 0.370207630262783
Loss in iteration 94 : 0.3701691455143764
Loss in iteration 95 : 0.37013107992559624
Loss in iteration 96 : 0.3700935239519056
Loss in iteration 97 : 0.37005633908723745
Loss in iteration 98 : 0.3700197050247659
Loss in iteration 99 : 0.3699835575564604
Loss in iteration 100 : 0.3699478173604555
Loss in iteration 101 : 0.36991259210007116
Loss in iteration 102 : 0.3698776147784934
Loss in iteration 103 : 0.36984321789343355
Loss in iteration 104 : 0.36980938275416575
Loss in iteration 105 : 0.36977569547292694
Loss in iteration 106 : 0.3697421409615626
Loss in iteration 107 : 0.36970870563524394
Loss in iteration 108 : 0.369675377262724
Loss in iteration 109 : 0.369642548027633
Loss in iteration 110 : 0.36960980298491514
Loss in iteration 111 : 0.36957740742833606
Loss in iteration 112 : 0.3695450608691815
Loss in iteration 113 : 0.3695126714167209
Loss in iteration 114 : 0.3694803815425389
Loss in iteration 115 : 0.3694482535952952
Loss in iteration 116 : 0.36941661449633667
Loss in iteration 117 : 0.36938508505592593
Loss in iteration 118 : 0.36935358796341483
Loss in iteration 119 : 0.36932290563766973
Loss in iteration 120 : 0.36929187509227596
Loss in iteration 121 : 0.36926093258609516
Loss in iteration 122 : 0.3692300326255373
Loss in iteration 123 : 0.3691992288152738
Loss in iteration 124 : 0.3691684604005093
Loss in iteration 125 : 0.3691377235416626
Loss in iteration 126 : 0.36910762038594086
Loss in iteration 127 : 0.36907761131944755
Loss in iteration 128 : 0.3690474331862943
Loss in iteration 129 : 0.3690176481475421
Loss in iteration 130 : 0.3689882035081336
Loss in iteration 131 : 0.3689589190916054
Loss in iteration 132 : 0.368929940014643
Loss in iteration 133 : 0.3689009108785308
Loss in iteration 134 : 0.3688722530337087
Loss in iteration 135 : 0.3688436365973026
Loss in iteration 136 : 0.3688150578482333
Loss in iteration 137 : 0.3687865126536436
Loss in iteration 138 : 0.3687579972941787
Loss in iteration 139 : 0.3687295084226744
Loss in iteration 140 : 0.36870106561843624
Loss in iteration 141 : 0.36867265555671663
Loss in iteration 142 : 0.3686442933402585
Loss in iteration 143 : 0.36861595731710123
Loss in iteration 144 : 0.3685878985927154
Loss in iteration 145 : 0.3685599479696545
Loss in iteration 146 : 0.3685320324472139
Loss in iteration 147 : 0.36850414812802823
Loss in iteration 148 : 0.36847679216915225
Loss in iteration 149 : 0.3684498782680504
Loss in iteration 150 : 0.3684233502530272
Loss in iteration 151 : 0.36839686893502
Loss in iteration 152 : 0.36837042915451707
Loss in iteration 153 : 0.36834402626967844
Loss in iteration 154 : 0.3683176561044571
Loss in iteration 155 : 0.36829148387899296
Loss in iteration 156 : 0.36826542894004033
Loss in iteration 157 : 0.36823942619847005
Loss in iteration 158 : 0.36821351819174314
Loss in iteration 159 : 0.36818782254113847
Loss in iteration 160 : 0.3681623306017392
Loss in iteration 161 : 0.36813699509812237
Loss in iteration 162 : 0.368111704136641
Loss in iteration 163 : 0.36808645275080953
Loss in iteration 164 : 0.368061416766636
Loss in iteration 165 : 0.36803657386665206
Loss in iteration 166 : 0.36801176527546187
Loss in iteration 167 : 0.36798720656605427
Loss in iteration 168 : 0.3679628608230612
Loss in iteration 169 : 0.36793871150230173
Loss in iteration 170 : 0.36791466617780766
Loss in iteration 171 : 0.36789068731759933
Loss in iteration 172 : 0.36786673702377304
Loss in iteration 173 : 0.36784281184850603
Loss in iteration 174 : 0.3678189086903138
Loss in iteration 175 : 0.3677950247593289
Loss in iteration 176 : 0.36777147539123717
Loss in iteration 177 : 0.3677478015840073
Loss in iteration 178 : 0.36772429756301717
Loss in iteration 179 : 0.36770080917896925
Loss in iteration 180 : 0.3676773342432666
Loss in iteration 181 : 0.3676538707879114
Loss in iteration 182 : 0.3676306452492968
Loss in iteration 183 : 0.36760783325650115
Loss in iteration 184 : 0.3675849358202756
Loss in iteration 185 : 0.3675620727200975
Loss in iteration 186 : 0.36753992827428755
Loss in iteration 187 : 0.3675174347017024
Loss in iteration 188 : 0.36749524357011515
Loss in iteration 189 : 0.36747319534506573
Loss in iteration 190 : 0.36745118695159945
Loss in iteration 191 : 0.3674292138173599
Loss in iteration 192 : 0.3674072718269468
Loss in iteration 193 : 0.3673857409406928
Loss in iteration 194 : 0.36736425829346786
Loss in iteration 195 : 0.3673428078763052
Loss in iteration 196 : 0.36732138588546853
Loss in iteration 197 : 0.3672999888974857
Loss in iteration 198 : 0.3672786138311941
Loss in iteration 199 : 0.3672572579135799
Loss in iteration 200 : 0.3672359734415415
Testing accuracy  of updater 8 on alg 1 with rate 0.0022400000000000002 = 0.7855, training accuracy 0.8397539656846876, time elapsed: 2767 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8171998733972538
Loss in iteration 3 : 0.6234473710565419
Loss in iteration 4 : 0.5540876546753569
Loss in iteration 5 : 0.5568704744432728
Loss in iteration 6 : 0.5729090992123194
Loss in iteration 7 : 0.5852776316324946
Loss in iteration 8 : 0.5907492269178933
Loss in iteration 9 : 0.5891323850524955
Loss in iteration 10 : 0.5813136943870032
Loss in iteration 11 : 0.5686388881430974
Loss in iteration 12 : 0.5525227591273095
Loss in iteration 13 : 0.5355501657754989
Loss in iteration 14 : 0.5197801079215089
Loss in iteration 15 : 0.5069238892726504
Loss in iteration 16 : 0.4981493593628822
Loss in iteration 17 : 0.49291760835180987
Loss in iteration 18 : 0.4895146108360898
Loss in iteration 19 : 0.4871022086452172
Loss in iteration 20 : 0.4848655390631606
Loss in iteration 21 : 0.48219533595674713
Loss in iteration 22 : 0.4788507483198215
Loss in iteration 23 : 0.47484768891218027
Loss in iteration 24 : 0.4703865192321032
Loss in iteration 25 : 0.4657100712385177
Loss in iteration 26 : 0.4610449070226686
Loss in iteration 27 : 0.4565240167629806
Loss in iteration 28 : 0.45221961314074904
Loss in iteration 29 : 0.4481157533352611
Loss in iteration 30 : 0.4442415983767025
Loss in iteration 31 : 0.440597962301518
Loss in iteration 32 : 0.4371818097479153
Loss in iteration 33 : 0.4338878393162293
Loss in iteration 34 : 0.4306583000355625
Loss in iteration 35 : 0.42750737402296124
Loss in iteration 36 : 0.424443933549829
Loss in iteration 37 : 0.42147186906216527
Loss in iteration 38 : 0.4185864746508092
Loss in iteration 39 : 0.4158346094967307
Loss in iteration 40 : 0.41319006785533235
Loss in iteration 41 : 0.41067597326648586
Loss in iteration 42 : 0.40830639948179076
Loss in iteration 43 : 0.40612818302745085
Loss in iteration 44 : 0.4041251750237354
Loss in iteration 45 : 0.40228963603873824
Loss in iteration 46 : 0.4005966339344751
Loss in iteration 47 : 0.39904146320481804
Loss in iteration 48 : 0.39760509511177694
Loss in iteration 49 : 0.3962693009745397
Loss in iteration 50 : 0.39503572950170546
Loss in iteration 51 : 0.3938940746125705
Loss in iteration 52 : 0.392818729724716
Loss in iteration 53 : 0.3917995025945842
Loss in iteration 54 : 0.39084620304415413
Loss in iteration 55 : 0.38997892815914703
Loss in iteration 56 : 0.389170356308655
Loss in iteration 57 : 0.38842331629434507
Loss in iteration 58 : 0.3877494203029366
Testing accuracy  of updater 8 on alg 1 with rate 5.599999999999997E-4 = 0.778, training accuracy 0.8374878601489155, time elapsed: 899 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 6.0127499554503085
Loss in iteration 3 : 8.469827164871228
Loss in iteration 4 : 8.614779239046067
Loss in iteration 5 : 7.002603761213542
Loss in iteration 6 : 4.036560691493228
Loss in iteration 7 : 1.6278919746157998
Loss in iteration 8 : 2.000931812117858
Loss in iteration 9 : 4.000575427920029
Loss in iteration 10 : 4.635423519304963
Loss in iteration 11 : 3.632890631021605
Loss in iteration 12 : 2.48173842907228
Loss in iteration 13 : 2.101244236416191
Loss in iteration 14 : 2.426775879195456
Loss in iteration 15 : 2.9325218190739255
Loss in iteration 16 : 3.3262436179789088
Loss in iteration 17 : 3.4587685786158735
Loss in iteration 18 : 3.3242400209087926
Loss in iteration 19 : 3.0202630985013337
Loss in iteration 20 : 2.686447578229973
Loss in iteration 21 : 2.4401194053814654
Loss in iteration 22 : 2.3815510758007967
Loss in iteration 23 : 2.5285948942391143
Loss in iteration 24 : 2.7306281919637296
Loss in iteration 25 : 2.808358482448515
Loss in iteration 26 : 2.6865821451004623
Loss in iteration 27 : 2.457118868246517
Loss in iteration 28 : 2.2652685545204885
Loss in iteration 29 : 2.1951589247765235
Loss in iteration 30 : 2.2218951060215897
Loss in iteration 31 : 2.2722855501391512
Loss in iteration 32 : 2.2829642864232773
Loss in iteration 33 : 2.224796633765024
Loss in iteration 34 : 2.1051332769858178
Loss in iteration 35 : 1.9601133641881072
Loss in iteration 36 : 1.839174808868905
Loss in iteration 37 : 1.785850226895402
Loss in iteration 38 : 1.7893789808978229
Loss in iteration 39 : 1.7781367835227944
Loss in iteration 40 : 1.7024467653496893
Loss in iteration 41 : 1.5763108526839096
Loss in iteration 42 : 1.4589782861214475
Loss in iteration 43 : 1.4009842467228488
Loss in iteration 44 : 1.375658403975358
Loss in iteration 45 : 1.3344267672671388
Loss in iteration 46 : 1.2526126816616066
Loss in iteration 47 : 1.1427890247765935
Loss in iteration 48 : 1.049492036291822
Loss in iteration 49 : 1.0077700436132504
Loss in iteration 50 : 0.9682425489749698
Loss in iteration 51 : 0.8774870812215745
Loss in iteration 52 : 0.7795114267591207
Loss in iteration 53 : 0.7337739936060268
Loss in iteration 54 : 0.6949827527398245
Loss in iteration 55 : 0.6093276221064957
Loss in iteration 56 : 0.5360815847348702
Loss in iteration 57 : 0.5414387996995789
Loss in iteration 58 : 0.45078216953739797
Loss in iteration 59 : 0.4953178007222019
Loss in iteration 60 : 0.4547690136912167
Loss in iteration 61 : 0.5502372746573704
Loss in iteration 62 : 1.1251724290162688
Loss in iteration 63 : 0.7104280169031335
Loss in iteration 64 : 2.5890351503623275
Loss in iteration 65 : 1.551106781802333
Loss in iteration 66 : 2.6900496608152853
Loss in iteration 67 : 2.760441032532175
Loss in iteration 68 : 1.990451442696072
Loss in iteration 69 : 1.1878096746565352
Loss in iteration 70 : 1.0084194767440542
Loss in iteration 71 : 1.4688577777305984
Loss in iteration 72 : 1.9293259004304595
Loss in iteration 73 : 1.933112125052817
Loss in iteration 74 : 1.6336771759186832
Loss in iteration 75 : 1.3869318727994218
Loss in iteration 76 : 1.3814663611175333
Loss in iteration 77 : 1.5358337016453079
Loss in iteration 78 : 1.6914692792051826
Loss in iteration 79 : 1.7722965451587371
Loss in iteration 80 : 1.7602536148827328
Loss in iteration 81 : 1.6730456601317316
Loss in iteration 82 : 1.551411851504964
Loss in iteration 83 : 1.4522475494869758
Loss in iteration 84 : 1.428872108025091
Loss in iteration 85 : 1.4838233190270307
Loss in iteration 86 : 1.5409977512681814
Loss in iteration 87 : 1.5340221854062086
Loss in iteration 88 : 1.4541144363586118
Loss in iteration 89 : 1.3508151164079958
Loss in iteration 90 : 1.2865526192831098
Loss in iteration 91 : 1.2792462222432541
Loss in iteration 92 : 1.2883663237928262
Loss in iteration 93 : 1.2791391738626054
Loss in iteration 94 : 1.2361856046497184
Loss in iteration 95 : 1.16492534321037
Loss in iteration 96 : 1.0876891682080787
Loss in iteration 97 : 1.0391045251582565
Loss in iteration 98 : 1.0264232655855958
Loss in iteration 99 : 1.012526708151151
Loss in iteration 100 : 0.96466628236343
Loss in iteration 101 : 0.8915396492081256
Loss in iteration 102 : 0.8319271299989783
Loss in iteration 103 : 0.805129154859482
Loss in iteration 104 : 0.7872494285073295
Loss in iteration 105 : 0.7431275355097289
Loss in iteration 106 : 0.6764337539226539
Loss in iteration 107 : 0.6280274235171971
Loss in iteration 108 : 0.6137740941246003
Loss in iteration 109 : 0.5824705944786581
Loss in iteration 110 : 0.5214599980943762
Loss in iteration 111 : 0.5029009497385385
Loss in iteration 112 : 0.49871757929502875
Loss in iteration 113 : 0.4556923026307887
Loss in iteration 114 : 0.46940098703482597
Loss in iteration 115 : 0.46399489626129126
Loss in iteration 116 : 0.4729667577306632
Loss in iteration 117 : 0.47986090629629746
Loss in iteration 118 : 0.539412089717804
Loss in iteration 119 : 0.5435678812882963
Loss in iteration 120 : 0.4401104293482035
Loss in iteration 121 : 0.5057400817105202
Loss in iteration 122 : 0.4408907173683031
Loss in iteration 123 : 0.47070541674563243
Loss in iteration 124 : 0.4204318405271728
Loss in iteration 125 : 0.4661127075657053
Loss in iteration 126 : 0.42200363261794593
Loss in iteration 127 : 0.4468109028069782
Loss in iteration 128 : 0.4425184860359097
Loss in iteration 129 : 0.4241158247775256
Loss in iteration 130 : 0.4453702523996893
Loss in iteration 131 : 0.42078230056756927
Loss in iteration 132 : 0.4280672495320255
Loss in iteration 133 : 0.42507388263203305
Loss in iteration 134 : 0.40382920368276515
Loss in iteration 135 : 0.42040649876515207
Loss in iteration 136 : 0.39129879030290243
Loss in iteration 137 : 0.40498983967952273
Loss in iteration 138 : 0.3816400513403483
Loss in iteration 139 : 0.3997423566637171
Loss in iteration 140 : 0.3921379407581449
Loss in iteration 141 : 0.381111256613036
Loss in iteration 142 : 0.4114299031630826
Loss in iteration 143 : 0.41267279716326577
Loss in iteration 144 : 0.370533746146311
Loss in iteration 145 : 0.41420100365987456
Loss in iteration 146 : 0.4299231658238092
Loss in iteration 147 : 0.37190173265512777
Loss in iteration 148 : 0.445768306356632
Loss in iteration 149 : 0.39442290751314774
Loss in iteration 150 : 0.40732676629675274
Loss in iteration 151 : 0.39912779935764325
Loss in iteration 152 : 0.3942080224532747
Loss in iteration 153 : 0.3974977018936519
Loss in iteration 154 : 0.3918374856681672
Loss in iteration 155 : 0.393618742875823
Loss in iteration 156 : 0.3847815697971962
Loss in iteration 157 : 0.39235736341303024
Loss in iteration 158 : 0.3753664542562844
Loss in iteration 159 : 0.4060295354477202
Loss in iteration 160 : 0.3681036858442259
Loss in iteration 161 : 0.38192403070628117
Loss in iteration 162 : 0.39293991494660774
Loss in iteration 163 : 0.3697545950863149
Loss in iteration 164 : 0.3660894547648819
Loss in iteration 165 : 0.38296900394809125
Loss in iteration 166 : 0.38905521585958236
Loss in iteration 167 : 0.36481647768113673
Loss in iteration 168 : 0.3689794818846873
Loss in iteration 169 : 0.3835267988751931
Loss in iteration 170 : 0.36935472409093556
Loss in iteration 171 : 0.3653735519878474
Loss in iteration 172 : 0.3789941496685214
Loss in iteration 173 : 0.3680143896160417
Loss in iteration 174 : 0.3669387691665553
Loss in iteration 175 : 0.37626792364089395
Loss in iteration 176 : 0.36582117668358405
Loss in iteration 177 : 0.3654691927648662
Loss in iteration 178 : 0.37196165785137375
Loss in iteration 179 : 0.3647797779877647
Loss in iteration 180 : 0.36498853612865284
Loss in iteration 181 : 0.371836739058829
Loss in iteration 182 : 0.36850882825365583
Loss in iteration 183 : 0.3632115671455893
Testing accuracy  of updater 9 on alg 1 with rate 0.0392 = 0.78425, training accuracy 0.842667529944966, time elapsed: 3029 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.8480438984127883
Loss in iteration 3 : 2.6320618432269
Loss in iteration 4 : 2.7899971763145515
Loss in iteration 5 : 2.3971442168493238
Loss in iteration 6 : 1.5313993502127807
Loss in iteration 7 : 0.7469087035063992
Loss in iteration 8 : 0.7972920403838383
Loss in iteration 9 : 1.4053397128807572
Loss in iteration 10 : 1.6977496117014634
Loss in iteration 11 : 1.4403216243841332
Loss in iteration 12 : 1.0540983684531111
Loss in iteration 13 : 0.8976630578824498
Loss in iteration 14 : 0.9880136807282407
Loss in iteration 15 : 1.1563950995187429
Loss in iteration 16 : 1.289480849096929
Loss in iteration 17 : 1.3320735177065146
Loss in iteration 18 : 1.2816655746841963
Loss in iteration 19 : 1.1765893527744544
Loss in iteration 20 : 1.0668692296090143
Loss in iteration 21 : 0.9993696628201609
Loss in iteration 22 : 1.0058896836603082
Loss in iteration 23 : 1.0724242395698407
Loss in iteration 24 : 1.13010001445145
Loss in iteration 25 : 1.1249241017457576
Loss in iteration 26 : 1.0587461527134436
Loss in iteration 27 : 0.9787244199494313
Loss in iteration 28 : 0.9313417463314291
Loss in iteration 29 : 0.9303828125116241
Loss in iteration 30 : 0.948888487784494
Loss in iteration 31 : 0.9589495651185169
Loss in iteration 32 : 0.9429737308966853
Loss in iteration 33 : 0.9009341479058615
Loss in iteration 34 : 0.8466053342351323
Loss in iteration 35 : 0.8003487301834283
Loss in iteration 36 : 0.7830999393480127
Loss in iteration 37 : 0.7855241445041946
Loss in iteration 38 : 0.7788097330052232
Loss in iteration 39 : 0.7452122416733016
Loss in iteration 40 : 0.6962338257508401
Loss in iteration 41 : 0.658689473601204
Loss in iteration 42 : 0.644687362150241
Loss in iteration 43 : 0.6370283446641167
Loss in iteration 44 : 0.6144495441470322
Loss in iteration 45 : 0.574725473408803
Loss in iteration 46 : 0.5371167435069577
Loss in iteration 47 : 0.5208531097987434
Loss in iteration 48 : 0.5125910873361412
Loss in iteration 49 : 0.48135675770062286
Loss in iteration 50 : 0.4450607576652247
Loss in iteration 51 : 0.43517764708294465
Loss in iteration 52 : 0.4276600871836437
Loss in iteration 53 : 0.3937733842373565
Loss in iteration 54 : 0.38692700269556846
Loss in iteration 55 : 0.3872917730637529
Loss in iteration 56 : 0.37437403319808366
Loss in iteration 57 : 0.3901790093693331
Loss in iteration 58 : 0.39492580961918106
Loss in iteration 59 : 0.38943288754837274
Loss in iteration 60 : 0.4096887787365954
Loss in iteration 61 : 0.40785387589780225
Loss in iteration 62 : 0.3835888242844854
Loss in iteration 63 : 0.3779642110135067
Loss in iteration 64 : 0.3765852742923567
Loss in iteration 65 : 0.37000748603473
Loss in iteration 66 : 0.3714278603328894
Loss in iteration 67 : 0.3714596860071015
Loss in iteration 68 : 0.37266056996309743
Loss in iteration 69 : 0.3750966493145286
Loss in iteration 70 : 0.3731644929695449
Loss in iteration 71 : 0.3757204629590986
Loss in iteration 72 : 0.3741415347742479
Loss in iteration 73 : 0.37340019203814456
Loss in iteration 74 : 0.37319380683787307
Loss in iteration 75 : 0.3701109484130929
Loss in iteration 76 : 0.3699850106323823
Loss in iteration 77 : 0.3671621834082245
Loss in iteration 78 : 0.3666594327500897
Loss in iteration 79 : 0.3651685713544115
Loss in iteration 80 : 0.36524728928833133
Loss in iteration 81 : 0.3650159066799538
Loss in iteration 82 : 0.36522583206591136
Loss in iteration 83 : 0.3655541918129622
Loss in iteration 84 : 0.36582908623378135
Loss in iteration 85 : 0.3655542872528913
Loss in iteration 86 : 0.36541816484419665
Loss in iteration 87 : 0.3646221861397006
Loss in iteration 88 : 0.36438323549950413
Loss in iteration 89 : 0.3635872087260829
Loss in iteration 90 : 0.36337261960170303
Loss in iteration 91 : 0.36307613051369436
Loss in iteration 92 : 0.36319380920105143
Loss in iteration 93 : 0.3631465055664716
Loss in iteration 94 : 0.3634918408161252
Loss in iteration 95 : 0.36347977698997275
Loss in iteration 96 : 0.3636268018148154
Loss in iteration 97 : 0.36361727263265753
Loss in iteration 98 : 0.3635571766699186
Loss in iteration 99 : 0.36339022292312334
Loss in iteration 100 : 0.36325968533432307
Loss in iteration 101 : 0.3630157111936408
Loss in iteration 102 : 0.36299135285255174
Loss in iteration 103 : 0.3628486256434455
Loss in iteration 104 : 0.3628632810608395
Loss in iteration 105 : 0.36280340122850385
Loss in iteration 106 : 0.3628632153869805
Loss in iteration 107 : 0.3628360745030706
Loss in iteration 108 : 0.3629003333783596
Loss in iteration 109 : 0.36292036741609685
Loss in iteration 110 : 0.36290083803496853
Loss in iteration 111 : 0.362991243852067
Loss in iteration 112 : 0.3628584839191561
Loss in iteration 113 : 0.36297318176110355
Loss in iteration 114 : 0.3627939002828536
Loss in iteration 115 : 0.36284378549937674
Loss in iteration 116 : 0.3627346565559624
Loss in iteration 117 : 0.3627688320072676
Loss in iteration 118 : 0.36272079385635303
Loss in iteration 119 : 0.36273740661698745
Loss in iteration 120 : 0.3627418051946467
Loss in iteration 121 : 0.3627214778087848
Loss in iteration 122 : 0.3627521396427494
Testing accuracy  of updater 9 on alg 1 with rate 0.02744 = 0.79075, training accuracy 0.8436387180317255, time elapsed: 3283 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.110980785430372
Loss in iteration 3 : 1.5070565290957123
Loss in iteration 4 : 1.5827837575781467
Loss in iteration 5 : 1.3754053850608372
Loss in iteration 6 : 0.920225590900554
Loss in iteration 7 : 0.48129112528291446
Loss in iteration 8 : 0.5670746304813251
Loss in iteration 9 : 0.9053735406316712
Loss in iteration 10 : 1.0011589839750739
Loss in iteration 11 : 0.8174568235563835
Loss in iteration 12 : 0.6200498678124978
Loss in iteration 13 : 0.571198753174935
Loss in iteration 14 : 0.6440879841288106
Loss in iteration 15 : 0.742032291481509
Loss in iteration 16 : 0.8019924956578148
Loss in iteration 17 : 0.8014806529084122
Loss in iteration 18 : 0.7522701811000414
Loss in iteration 19 : 0.6874141458171426
Loss in iteration 20 : 0.6389478335112506
Loss in iteration 21 : 0.629960140430839
Loss in iteration 22 : 0.6609364606890841
Loss in iteration 23 : 0.6999589985335731
Loss in iteration 24 : 0.7131902869265528
Loss in iteration 25 : 0.691418869009681
Loss in iteration 26 : 0.6497036698364381
Loss in iteration 27 : 0.6159480098630179
Loss in iteration 28 : 0.6049040490859758
Loss in iteration 29 : 0.614276824036452
Loss in iteration 30 : 0.6257058295008237
Loss in iteration 31 : 0.6267162451904008
Loss in iteration 32 : 0.6119385373443613
Loss in iteration 33 : 0.5856471625509228
Loss in iteration 34 : 0.5587798855574484
Loss in iteration 35 : 0.544459733722273
Loss in iteration 36 : 0.5440113212270229
Loss in iteration 37 : 0.5463773841743412
Loss in iteration 38 : 0.5395356588606787
Loss in iteration 39 : 0.5193563475495085
Loss in iteration 40 : 0.49669383711508247
Loss in iteration 41 : 0.4815192665508391
Loss in iteration 42 : 0.4768839252563529
Loss in iteration 43 : 0.4755343908474287
Loss in iteration 44 : 0.4650318405591675
Loss in iteration 45 : 0.44559135476238965
Loss in iteration 46 : 0.429565796430968
Loss in iteration 47 : 0.42333343602703083
Loss in iteration 48 : 0.42198815733398354
Loss in iteration 49 : 0.4120058417092207
Loss in iteration 50 : 0.39552734676227774
Loss in iteration 51 : 0.38729086510767735
Loss in iteration 52 : 0.38828032786949485
Loss in iteration 53 : 0.3832511049519745
Loss in iteration 54 : 0.3706630786197095
Loss in iteration 55 : 0.36946112331118
Loss in iteration 56 : 0.37485364524038495
Loss in iteration 57 : 0.365694118992612
Loss in iteration 58 : 0.3705441056918895
Loss in iteration 59 : 0.3742003211886523
Loss in iteration 60 : 0.3697894145807686
Loss in iteration 61 : 0.3779510474324715
Loss in iteration 62 : 0.3736676071730462
Loss in iteration 63 : 0.3743933379407211
Loss in iteration 64 : 0.375695657260207
Loss in iteration 65 : 0.3695180064415198
Loss in iteration 66 : 0.37210517121730813
Loss in iteration 67 : 0.3675674081842458
Loss in iteration 68 : 0.36611975434422755
Loss in iteration 69 : 0.3669552014875337
Loss in iteration 70 : 0.36424635676427813
Loss in iteration 71 : 0.36461121736977326
Loss in iteration 72 : 0.3658297622737419
Loss in iteration 73 : 0.36428238493265197
Loss in iteration 74 : 0.3646864058995058
Loss in iteration 75 : 0.36574194740935506
Loss in iteration 76 : 0.3646923745871247
Loss in iteration 77 : 0.3649678446538993
Loss in iteration 78 : 0.36574108545725864
Loss in iteration 79 : 0.3646955566609251
Loss in iteration 80 : 0.3645815751155046
Loss in iteration 81 : 0.365058393767334
Loss in iteration 82 : 0.36419955067708354
Loss in iteration 83 : 0.36392282542270565
Loss in iteration 84 : 0.3641886240175363
Loss in iteration 85 : 0.36362561943910454
Loss in iteration 86 : 0.3633227495916886
Loss in iteration 87 : 0.3634425038750706
Loss in iteration 88 : 0.363221730380211
Loss in iteration 89 : 0.36293551780375616
Loss in iteration 90 : 0.36309508377031996
Loss in iteration 91 : 0.36310826436610644
Loss in iteration 92 : 0.36287294499632333
Loss in iteration 93 : 0.36313927564170023
Loss in iteration 94 : 0.36314674183332923
Loss in iteration 95 : 0.36298831453673563
Loss in iteration 96 : 0.36323830708086075
Loss in iteration 97 : 0.3631616795425679
Loss in iteration 98 : 0.36298803427421167
Loss in iteration 99 : 0.36315880989697874
Loss in iteration 100 : 0.36298967719690706
Loss in iteration 101 : 0.362967162676937
Loss in iteration 102 : 0.3630542149108142
Loss in iteration 103 : 0.3628446021572508
Loss in iteration 104 : 0.3628702624096087
Loss in iteration 105 : 0.3628679552987969
Loss in iteration 106 : 0.3627448354048834
Loss in iteration 107 : 0.3627966984321521
Loss in iteration 108 : 0.3627677062053899
Loss in iteration 109 : 0.3627345396350263
Testing accuracy  of updater 9 on alg 1 with rate 0.01568 = 0.78925, training accuracy 0.8429912593072192, time elapsed: 2070 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5505645038428336
Loss in iteration 3 : 0.6594198054900349
Loss in iteration 4 : 0.7127145788608513
Loss in iteration 5 : 0.6967886592680468
Loss in iteration 6 : 0.6200039094447659
Loss in iteration 7 : 0.4995718764442448
Loss in iteration 8 : 0.40528284122110286
Loss in iteration 9 : 0.4348379759164788
Loss in iteration 10 : 0.5021803125200359
Loss in iteration 11 : 0.5014192676478393
Loss in iteration 12 : 0.4443871612598494
Loss in iteration 13 : 0.3890785741749923
Loss in iteration 14 : 0.3751825773361539
Loss in iteration 15 : 0.39339097092607683
Loss in iteration 16 : 0.41763883281525643
Loss in iteration 17 : 0.4285502869059615
Loss in iteration 18 : 0.4223829344037219
Loss in iteration 19 : 0.4055673987407281
Loss in iteration 20 : 0.3896276578689835
Loss in iteration 21 : 0.38295063007647173
Loss in iteration 22 : 0.3864073861140334
Loss in iteration 23 : 0.3956426261180966
Loss in iteration 24 : 0.4035112585306688
Loss in iteration 25 : 0.40605967358382955
Loss in iteration 26 : 0.40298598472648367
Loss in iteration 27 : 0.3964935627775925
Loss in iteration 28 : 0.3903276954754055
Loss in iteration 29 : 0.38741569030743067
Loss in iteration 30 : 0.388216676469963
Loss in iteration 31 : 0.3914180230373912
Loss in iteration 32 : 0.39436772365799294
Loss in iteration 33 : 0.3952669840702681
Loss in iteration 34 : 0.39357872360993185
Loss in iteration 35 : 0.3902902096448861
Loss in iteration 36 : 0.3868974045012529
Loss in iteration 37 : 0.38488278779979646
Loss in iteration 38 : 0.38447631874268073
Loss in iteration 39 : 0.38522453441144305
Loss in iteration 40 : 0.38597293683225364
Loss in iteration 41 : 0.38592153873689333
Loss in iteration 42 : 0.3846635921436113
Loss in iteration 43 : 0.38263265704849203
Loss in iteration 44 : 0.3805361977551276
Loss in iteration 45 : 0.3792139698954818
Loss in iteration 46 : 0.37884474274381547
Loss in iteration 47 : 0.37880301094980917
Loss in iteration 48 : 0.3786652625779898
Loss in iteration 49 : 0.3781281659545956
Loss in iteration 50 : 0.37711630515778255
Loss in iteration 51 : 0.375824771449324
Loss in iteration 52 : 0.37464902266893985
Loss in iteration 53 : 0.37380508888239894
Loss in iteration 54 : 0.37353759132650316
Loss in iteration 55 : 0.37341848209219136
Loss in iteration 56 : 0.37308880717519516
Loss in iteration 57 : 0.3724073441780122
Loss in iteration 58 : 0.37149660536465473
Loss in iteration 59 : 0.3707995523531501
Loss in iteration 60 : 0.37051009380165734
Loss in iteration 61 : 0.37044312435954957
Loss in iteration 62 : 0.3702509207618313
Loss in iteration 63 : 0.3698792579851547
Loss in iteration 64 : 0.36940346524912293
Loss in iteration 65 : 0.36905553332233526
Loss in iteration 66 : 0.3688949439410772
Loss in iteration 67 : 0.3688500880834183
Loss in iteration 68 : 0.3687603965515609
Loss in iteration 69 : 0.3685778058146693
Loss in iteration 70 : 0.368348587593114
Loss in iteration 71 : 0.3681801985953615
Loss in iteration 72 : 0.368119759146229
Loss in iteration 73 : 0.3681064094710233
Loss in iteration 74 : 0.36805313737315015
Loss in iteration 75 : 0.36793561230203065
Loss in iteration 76 : 0.36782988495791974
Loss in iteration 77 : 0.3677501928280875
Loss in iteration 78 : 0.3677207968852911
Loss in iteration 79 : 0.36769621697892335
Loss in iteration 80 : 0.3676422276117771
Loss in iteration 81 : 0.36756181932479126
Loss in iteration 82 : 0.3674918873982282
Loss in iteration 83 : 0.3674304234891833
Loss in iteration 84 : 0.3673892839421311
Loss in iteration 85 : 0.3673456507285385
Loss in iteration 86 : 0.36728847977870216
Loss in iteration 87 : 0.36721898210369136
Loss in iteration 88 : 0.36714613548950936
Loss in iteration 89 : 0.36708248701115787
Loss in iteration 90 : 0.367031398790345
Loss in iteration 91 : 0.3669837639603364
Loss in iteration 92 : 0.36691751707878373
Loss in iteration 93 : 0.3668488249659577
Loss in iteration 94 : 0.36679145313202743
Loss in iteration 95 : 0.36674116897384434
Loss in iteration 96 : 0.36668966763865224
Loss in iteration 97 : 0.3666347027806055
Loss in iteration 98 : 0.36657575961697547
Loss in iteration 99 : 0.3665172684718406
Loss in iteration 100 : 0.36646965892532474
Loss in iteration 101 : 0.3664242238410599
Loss in iteration 102 : 0.36637633084840543
Loss in iteration 103 : 0.36632680899041997
Loss in iteration 104 : 0.36627584185353557
Loss in iteration 105 : 0.36622841112480065
Loss in iteration 106 : 0.3661843358060301
Loss in iteration 107 : 0.366142692762332
Loss in iteration 108 : 0.366099641420054
Loss in iteration 109 : 0.36605538890226136
Loss in iteration 110 : 0.366011297886941
Loss in iteration 111 : 0.36596906375312455
Loss in iteration 112 : 0.36592910030355097
Loss in iteration 113 : 0.36588912954261255
Loss in iteration 114 : 0.3658491518492692
Loss in iteration 115 : 0.36580916757889287
Loss in iteration 116 : 0.36576917706420226
Loss in iteration 117 : 0.3657291806162486
Loss in iteration 118 : 0.36568923714871043
Loss in iteration 119 : 0.36565105303896284
Loss in iteration 120 : 0.36561296108109387
Loss in iteration 121 : 0.3655749518181752
Loss in iteration 122 : 0.3655372343801186
Loss in iteration 123 : 0.3654999490324368
Loss in iteration 124 : 0.3654625315402368
Loss in iteration 125 : 0.3654250522694051
Loss in iteration 126 : 0.3653892117134674
Loss in iteration 127 : 0.3653551968175276
Loss in iteration 128 : 0.3653218070760844
Loss in iteration 129 : 0.3652892126476236
Loss in iteration 130 : 0.3652561726737573
Loss in iteration 131 : 0.36522243523766357
Loss in iteration 132 : 0.36518852091447224
Loss in iteration 133 : 0.3651567761191682
Loss in iteration 134 : 0.36512693642780436
Loss in iteration 135 : 0.36509681443342934
Loss in iteration 136 : 0.3650664660922313
Loss in iteration 137 : 0.36503676222588954
Loss in iteration 138 : 0.36500725508529047
Loss in iteration 139 : 0.3649778897804007
Loss in iteration 140 : 0.3649490569543996
Loss in iteration 141 : 0.36492086800226503
Loss in iteration 142 : 0.36489287633707795
Loss in iteration 143 : 0.3648649193363179
Loss in iteration 144 : 0.36483846568371003
Loss in iteration 145 : 0.3648127686735082
Loss in iteration 146 : 0.3647867556699039
Loss in iteration 147 : 0.36476100320569055
Loss in iteration 148 : 0.3647363408118077
Loss in iteration 149 : 0.36471246026516757
Loss in iteration 150 : 0.3646878016714489
Loss in iteration 151 : 0.3646633163410528
Loss in iteration 152 : 0.3646396656380923
Loss in iteration 153 : 0.3646167138503598
Loss in iteration 154 : 0.3645943191212191
Loss in iteration 155 : 0.3645718415316415
Loss in iteration 156 : 0.3645493719698152
Loss in iteration 157 : 0.3645273870891756
Loss in iteration 158 : 0.3645055813796974
Loss in iteration 159 : 0.3644839577921669
Loss in iteration 160 : 0.3644624096534093
Loss in iteration 161 : 0.36444186233785514
Loss in iteration 162 : 0.3644219994550254
Loss in iteration 163 : 0.3644015593319543
Loss in iteration 164 : 0.3643801753702834
Loss in iteration 165 : 0.3643600842861411
Loss in iteration 166 : 0.36434092218762887
Loss in iteration 167 : 0.3643213169894111
Loss in iteration 168 : 0.3643028278336743
Loss in iteration 169 : 0.3642851829273934
Loss in iteration 170 : 0.3642675524475654
Loss in iteration 171 : 0.36424977315842155
Loss in iteration 172 : 0.3642320545098651
Loss in iteration 173 : 0.3642148512196592
Loss in iteration 174 : 0.36419776433639994
Loss in iteration 175 : 0.36418235786302383
Loss in iteration 176 : 0.3641652544902736
Loss in iteration 177 : 0.3641481252064597
Loss in iteration 178 : 0.3641316950595864
Loss in iteration 179 : 0.364115449370792
Loss in iteration 180 : 0.3640993784082484
Loss in iteration 181 : 0.36408361199180533
Loss in iteration 182 : 0.3640674176557652
Loss in iteration 183 : 0.3640515011719609
Loss in iteration 184 : 0.3640357203656147
Loss in iteration 185 : 0.36402053312400356
Loss in iteration 186 : 0.3640051737715538
Loss in iteration 187 : 0.3639900015289186
Loss in iteration 188 : 0.3639753902527929
Loss in iteration 189 : 0.36396058067579784
Loss in iteration 190 : 0.3639458440184149
Loss in iteration 191 : 0.3639318001906899
Loss in iteration 192 : 0.36391773546993295
Loss in iteration 193 : 0.3639035440026159
Loss in iteration 194 : 0.36389012619704164
Loss in iteration 195 : 0.3638767815890349
Loss in iteration 196 : 0.3638637113435256
Loss in iteration 197 : 0.36385093037108673
Loss in iteration 198 : 0.3638382486750462
Loss in iteration 199 : 0.3638256386836678
Loss in iteration 200 : 0.36381308693137704
Testing accuracy  of updater 9 on alg 1 with rate 0.00392 = 0.7895, training accuracy 0.8423438005827129, time elapsed: 4774 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5728331552370896
Loss in iteration 3 : 0.6170836482312245
Loss in iteration 4 : 0.7009016375928215
Loss in iteration 5 : 0.7328931894921081
Loss in iteration 6 : 0.7179015673880301
Loss in iteration 7 : 0.6614637628150154
Loss in iteration 8 : 0.5702344930277015
Loss in iteration 9 : 0.46744877507440286
Loss in iteration 10 : 0.4069095939314141
Loss in iteration 11 : 0.435662900761542
Loss in iteration 12 : 0.4911742954224348
Loss in iteration 13 : 0.5040344049617144
Loss in iteration 14 : 0.4690363843224479
Loss in iteration 15 : 0.4166792986836097
Loss in iteration 16 : 0.38132403050265473
Loss in iteration 17 : 0.3766596128112419
Loss in iteration 18 : 0.39130790392488246
Loss in iteration 19 : 0.4092316896222204
Loss in iteration 20 : 0.41899226691555835
Loss in iteration 21 : 0.41661047190414213
Loss in iteration 22 : 0.40525385439614714
Loss in iteration 23 : 0.3910725213865467
Loss in iteration 24 : 0.3805113502087568
Loss in iteration 25 : 0.377238216377883
Loss in iteration 26 : 0.38006022045495985
Loss in iteration 27 : 0.38647297009532416
Loss in iteration 28 : 0.3922633376066597
Loss in iteration 29 : 0.39480900359581994
Loss in iteration 30 : 0.3936125651849263
Loss in iteration 31 : 0.3895986128792307
Loss in iteration 32 : 0.38457482052358827
Loss in iteration 33 : 0.3808636864307752
Loss in iteration 34 : 0.37970747712276537
Loss in iteration 35 : 0.38066332534215613
Loss in iteration 36 : 0.38259458865052753
Loss in iteration 37 : 0.38456746639970546
Loss in iteration 38 : 0.3854799553434939
Loss in iteration 39 : 0.3849902339569774
Loss in iteration 40 : 0.3833546803178787
Loss in iteration 41 : 0.381331888079493
Loss in iteration 42 : 0.379590607991671
Loss in iteration 43 : 0.378465262171296
Loss in iteration 44 : 0.3781904055287351
Loss in iteration 45 : 0.3785851150302878
Loss in iteration 46 : 0.37921138659975756
Loss in iteration 47 : 0.3795202667666803
Loss in iteration 48 : 0.37913520299984493
Loss in iteration 49 : 0.37814218270605227
Loss in iteration 50 : 0.37695801103134363
Loss in iteration 51 : 0.3760181467438442
Loss in iteration 52 : 0.3755169864447307
Loss in iteration 53 : 0.37537286987897095
Loss in iteration 54 : 0.3753977868437496
Loss in iteration 55 : 0.3753788601892365
Loss in iteration 56 : 0.37521190611330685
Loss in iteration 57 : 0.37477792946368854
Loss in iteration 58 : 0.3741319038446521
Loss in iteration 59 : 0.37346793543657825
Loss in iteration 60 : 0.3729611468397409
Loss in iteration 61 : 0.3727068727649851
Loss in iteration 62 : 0.3726182571615514
Loss in iteration 63 : 0.37254596073457963
Loss in iteration 64 : 0.3723860323606835
Loss in iteration 65 : 0.3720759547587678
Loss in iteration 66 : 0.3716692971657394
Loss in iteration 67 : 0.37126976922481236
Loss in iteration 68 : 0.37102898809350915
Loss in iteration 69 : 0.3709092758961776
Loss in iteration 70 : 0.37083314292195463
Loss in iteration 71 : 0.3707309400485765
Loss in iteration 72 : 0.3705714351333711
Loss in iteration 73 : 0.3703555325528893
Loss in iteration 74 : 0.3701437413787741
Loss in iteration 75 : 0.36997169993051304
Loss in iteration 76 : 0.3698551840841463
Loss in iteration 77 : 0.36979698603545624
Loss in iteration 78 : 0.36972187661345174
Loss in iteration 79 : 0.36961729654694
Loss in iteration 80 : 0.36949856658824576
Loss in iteration 81 : 0.3693855630932523
Loss in iteration 82 : 0.3692997982115656
Loss in iteration 83 : 0.36924059245789437
Loss in iteration 84 : 0.3691917835196555
Loss in iteration 85 : 0.36913420950659304
Loss in iteration 86 : 0.3690633038414639
Loss in iteration 87 : 0.3689866818084013
Loss in iteration 88 : 0.3689212468973508
Loss in iteration 89 : 0.3688631055729538
Loss in iteration 90 : 0.3688150006247573
Loss in iteration 91 : 0.36877246444629913
Loss in iteration 92 : 0.36872394080695886
Loss in iteration 93 : 0.36866617308281746
Loss in iteration 94 : 0.36860773838020894
Loss in iteration 95 : 0.36855673065043937
Loss in iteration 96 : 0.36851026690733973
Loss in iteration 97 : 0.3684710781856359
Loss in iteration 98 : 0.36843077082668696
Loss in iteration 99 : 0.36838200773024565
Loss in iteration 100 : 0.36833045844236084
Loss in iteration 101 : 0.3682871854089844
Loss in iteration 102 : 0.36824865318755784
Loss in iteration 103 : 0.36820831189349346
Loss in iteration 104 : 0.36816544789521866
Loss in iteration 105 : 0.36812056148030764
Loss in iteration 106 : 0.3680750585010988
Loss in iteration 107 : 0.36803291994145043
Loss in iteration 108 : 0.36799493602421
Loss in iteration 109 : 0.3679552737827855
Loss in iteration 110 : 0.36791316463541623
Loss in iteration 111 : 0.36786885134030234
Loss in iteration 112 : 0.3678236315818316
Loss in iteration 113 : 0.36778636287867644
Loss in iteration 114 : 0.36774933464301085
Loss in iteration 115 : 0.3677071823153658
Loss in iteration 116 : 0.3676642646524348
Loss in iteration 117 : 0.3676253943686861
Loss in iteration 118 : 0.3675882612923571
Loss in iteration 119 : 0.36755082635177744
Loss in iteration 120 : 0.36751283354499975
Loss in iteration 121 : 0.36747534267136933
Loss in iteration 122 : 0.3674384116370576
Loss in iteration 123 : 0.36740177611664054
Loss in iteration 124 : 0.3673653191605902
Loss in iteration 125 : 0.367329302894034
Loss in iteration 126 : 0.36729376043100526
Loss in iteration 127 : 0.3672585381805713
Loss in iteration 128 : 0.3672232899077342
Loss in iteration 129 : 0.3671880764194094
Loss in iteration 130 : 0.36715336934390846
Loss in iteration 131 : 0.36711897074649713
Loss in iteration 132 : 0.36708501895078266
Loss in iteration 133 : 0.36705127781302993
Loss in iteration 134 : 0.36701834750868645
Loss in iteration 135 : 0.36698557680032856
Loss in iteration 136 : 0.3669529497174835
Loss in iteration 137 : 0.36692090141666234
Loss in iteration 138 : 0.36688933731220696
Loss in iteration 139 : 0.3668580228067736
Loss in iteration 140 : 0.36682690243171795
Loss in iteration 141 : 0.3667960631856162
Loss in iteration 142 : 0.36676539099965977
Loss in iteration 143 : 0.3667351998011233
Loss in iteration 144 : 0.3667057102685247
Loss in iteration 145 : 0.3666761891275687
Loss in iteration 146 : 0.3666470705002893
Loss in iteration 147 : 0.36661810683634627
Loss in iteration 148 : 0.3665892108740065
Loss in iteration 149 : 0.3665603755861364
Loss in iteration 150 : 0.3665320825071576
Loss in iteration 151 : 0.3665039023167975
Loss in iteration 152 : 0.36647579180943046
Loss in iteration 153 : 0.36644774379139133
Loss in iteration 154 : 0.36641975178975084
Loss in iteration 155 : 0.36639195514933953
Loss in iteration 156 : 0.36636431813010656
Loss in iteration 157 : 0.36633677718942087
Loss in iteration 158 : 0.3663094150244457
Loss in iteration 159 : 0.3662821167309724
Loss in iteration 160 : 0.36625487567722004
Loss in iteration 161 : 0.366227783630975
Loss in iteration 162 : 0.3662011431958289
Loss in iteration 163 : 0.36617460653846223
Loss in iteration 164 : 0.366148148446786
Loss in iteration 165 : 0.36612180171054337
Loss in iteration 166 : 0.36609570064397007
Loss in iteration 167 : 0.3660696888119684
Loss in iteration 168 : 0.36604416444911253
Loss in iteration 169 : 0.36601904659348866
Loss in iteration 170 : 0.365993923504716
Loss in iteration 171 : 0.36596889386814657
Loss in iteration 172 : 0.3659441431268913
Loss in iteration 173 : 0.365919469962968
Loss in iteration 174 : 0.36589486633631285
Loss in iteration 175 : 0.36587032501011935
Loss in iteration 176 : 0.3658458394706063
Loss in iteration 177 : 0.36582171990932405
Loss in iteration 178 : 0.3657978121229957
Loss in iteration 179 : 0.3657742238161898
Loss in iteration 180 : 0.36575059580534924
Loss in iteration 181 : 0.36572682518377014
Loss in iteration 182 : 0.365703209793851
Loss in iteration 183 : 0.36567918424305706
Loss in iteration 184 : 0.3656547853574327
Loss in iteration 185 : 0.3656303145189797
Loss in iteration 186 : 0.3656081304587113
Loss in iteration 187 : 0.36558574260568405
Loss in iteration 188 : 0.36556307848253267
Loss in iteration 189 : 0.36554050913033725
Loss in iteration 190 : 0.36551862982594924
Loss in iteration 191 : 0.3654968466529884
Loss in iteration 192 : 0.36547512175828556
Loss in iteration 193 : 0.3654534489573317
Loss in iteration 194 : 0.3654318226842812
Loss in iteration 195 : 0.36541023793008226
Loss in iteration 196 : 0.36538869018679226
Loss in iteration 197 : 0.3653671753974654
Loss in iteration 198 : 0.36534573095888956
Loss in iteration 199 : 0.36532478776096616
Loss in iteration 200 : 0.3653038890637653
Testing accuracy  of updater 9 on alg 1 with rate 0.002744 = 0.78725, training accuracy 0.8413726124959534, time elapsed: 3737 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7070708473979239
Loss in iteration 3 : 0.5524513911275636
Loss in iteration 4 : 0.6216167659979548
Loss in iteration 5 : 0.6775694601674489
Loss in iteration 6 : 0.7045365098377353
Loss in iteration 7 : 0.7051134981707646
Loss in iteration 8 : 0.682186585466035
Loss in iteration 9 : 0.6387006428602012
Loss in iteration 10 : 0.5781153525278909
Loss in iteration 11 : 0.5092401878228808
Loss in iteration 12 : 0.45172390863923
Loss in iteration 13 : 0.4216979202668025
Loss in iteration 14 : 0.42794350905619055
Loss in iteration 15 : 0.45757726077062594
Loss in iteration 16 : 0.47869452136797014
Loss in iteration 17 : 0.47727477093938353
Loss in iteration 18 : 0.45542243939048477
Loss in iteration 19 : 0.42430695786196654
Loss in iteration 20 : 0.3974478751243706
Loss in iteration 21 : 0.38293979148275803
Loss in iteration 22 : 0.3812859899563967
Loss in iteration 23 : 0.38772869255721365
Loss in iteration 24 : 0.3962746695044225
Loss in iteration 25 : 0.40191178157003765
Loss in iteration 26 : 0.40266131652861425
Loss in iteration 27 : 0.39879262508868457
Loss in iteration 28 : 0.3920636914198747
Loss in iteration 29 : 0.38471547663060585
Loss in iteration 30 : 0.3786909344098294
Loss in iteration 31 : 0.3749817188375386
Loss in iteration 32 : 0.37412340576665254
Loss in iteration 33 : 0.3756648569891504
Loss in iteration 34 : 0.3780565641585246
Loss in iteration 35 : 0.38040269144976263
Loss in iteration 36 : 0.3817718487697595
Loss in iteration 37 : 0.38170587424860714
Loss in iteration 38 : 0.3804179363043655
Loss in iteration 39 : 0.3784729676853458
Loss in iteration 40 : 0.3764442593560904
Loss in iteration 41 : 0.37474453843240113
Loss in iteration 42 : 0.37388989650772153
Loss in iteration 43 : 0.37380993678458324
Loss in iteration 44 : 0.37432924782455035
Loss in iteration 45 : 0.3750775119467086
Loss in iteration 46 : 0.37571575444260163
Loss in iteration 47 : 0.37609512693395497
Loss in iteration 48 : 0.376100965913136
Loss in iteration 49 : 0.37576790554270895
Loss in iteration 50 : 0.3752166785477186
Loss in iteration 51 : 0.374562500779271
Loss in iteration 52 : 0.3739492810613436
Loss in iteration 53 : 0.3734987211437628
Loss in iteration 54 : 0.37329921834372837
Loss in iteration 55 : 0.37333493754943553
Loss in iteration 56 : 0.3735050373582384
Loss in iteration 57 : 0.3737097602488239
Loss in iteration 58 : 0.37382170990399965
Loss in iteration 59 : 0.3738005136973779
Loss in iteration 60 : 0.3736294313846275
Loss in iteration 61 : 0.3733709952772809
Loss in iteration 62 : 0.37308846848331506
Loss in iteration 63 : 0.3728576166242298
Loss in iteration 64 : 0.3727080954545022
Loss in iteration 65 : 0.3726330005881291
Loss in iteration 66 : 0.3726173850131236
Loss in iteration 67 : 0.3726293288738279
Loss in iteration 68 : 0.37264690703305947
Loss in iteration 69 : 0.3726249518143492
Loss in iteration 70 : 0.3725460290076762
Loss in iteration 71 : 0.3724269196444011
Loss in iteration 72 : 0.37229211073551094
Loss in iteration 73 : 0.3721759609421447
Loss in iteration 74 : 0.37207626017034684
Loss in iteration 75 : 0.37200110301537975
Loss in iteration 76 : 0.37195236443891827
Loss in iteration 77 : 0.3719219749536303
Loss in iteration 78 : 0.3718936659827647
Loss in iteration 79 : 0.37185670981578023
Loss in iteration 80 : 0.3718064171519776
Loss in iteration 81 : 0.37174410935868574
Loss in iteration 82 : 0.3716709762576455
Loss in iteration 83 : 0.3715916345079855
Loss in iteration 84 : 0.3715168053930836
Loss in iteration 85 : 0.37145703898580906
Loss in iteration 86 : 0.3714031597508556
Loss in iteration 87 : 0.3713629156657737
Loss in iteration 88 : 0.37132887750558385
Loss in iteration 89 : 0.3712913834049325
Loss in iteration 90 : 0.37124998601644327
Loss in iteration 91 : 0.3712040976933352
Loss in iteration 92 : 0.3711546210835714
Loss in iteration 93 : 0.37110336746993383
Loss in iteration 94 : 0.3710528792289455
Loss in iteration 95 : 0.37100527837629027
Loss in iteration 96 : 0.370959591209953
Loss in iteration 97 : 0.3709157875560638
Loss in iteration 98 : 0.37087604415773373
Loss in iteration 99 : 0.3708421271954608
Loss in iteration 100 : 0.3708066706370207
Loss in iteration 101 : 0.37076923366113457
Loss in iteration 102 : 0.37072944220877985
Loss in iteration 103 : 0.3706884951028058
Loss in iteration 104 : 0.3706469357872955
Loss in iteration 105 : 0.37060624795325314
Loss in iteration 106 : 0.3705684507126763
Loss in iteration 107 : 0.3705340407930397
Loss in iteration 108 : 0.3705001402895376
Loss in iteration 109 : 0.37046684298870974
Loss in iteration 110 : 0.37043376861659877
Loss in iteration 111 : 0.370400000466997
Loss in iteration 112 : 0.3703651789006095
Loss in iteration 113 : 0.37032944539388846
Loss in iteration 114 : 0.37029472309736333
Loss in iteration 115 : 0.370262171982641
Loss in iteration 116 : 0.370230651008902
Loss in iteration 117 : 0.37019930093124465
Loss in iteration 118 : 0.3701679239526698
Loss in iteration 119 : 0.37013697353461517
Loss in iteration 120 : 0.37010596695711767
Loss in iteration 121 : 0.37007527919116373
Loss in iteration 122 : 0.37004496297165296
Loss in iteration 123 : 0.3700150266082295
Loss in iteration 124 : 0.36998542046282273
Loss in iteration 125 : 0.3699562674875822
Loss in iteration 126 : 0.3699272464455749
Loss in iteration 127 : 0.36989834436775254
Loss in iteration 128 : 0.36986973967716563
Loss in iteration 129 : 0.3698413210013734
Loss in iteration 130 : 0.3698129490581145
Loss in iteration 131 : 0.3697847148522587
Loss in iteration 132 : 0.36975665755945575
Loss in iteration 133 : 0.3697286805237036
Loss in iteration 134 : 0.36970100711664333
Loss in iteration 135 : 0.36967351764595274
Loss in iteration 136 : 0.36964623143313624
Loss in iteration 137 : 0.3696190732416958
Loss in iteration 138 : 0.3695919947559438
Loss in iteration 139 : 0.369564988169184
Loss in iteration 140 : 0.36953804645456395
Loss in iteration 141 : 0.36951130830354
Loss in iteration 142 : 0.36948498166945376
Loss in iteration 143 : 0.36945871500098304
Loss in iteration 144 : 0.36943250247870235
Loss in iteration 145 : 0.3694064444269106
Loss in iteration 146 : 0.36938038967523473
Loss in iteration 147 : 0.3693542709724748
Loss in iteration 148 : 0.36932809475124867
Loss in iteration 149 : 0.3693019761663551
Loss in iteration 150 : 0.3692760206926045
Loss in iteration 151 : 0.3692505016726876
Loss in iteration 152 : 0.3692249941842827
Loss in iteration 153 : 0.3691994972146542
Loss in iteration 154 : 0.3691740098528877
Loss in iteration 155 : 0.36914853127962066
Loss in iteration 156 : 0.36912306075781054
Loss in iteration 157 : 0.3690977177745622
Loss in iteration 158 : 0.3690724900192272
Loss in iteration 159 : 0.36904743893567704
Loss in iteration 160 : 0.36902237346077876
Loss in iteration 161 : 0.36899730211421067
Loss in iteration 162 : 0.3689728952277311
Loss in iteration 163 : 0.3689486356748906
Loss in iteration 164 : 0.3689248396129042
Loss in iteration 165 : 0.3689007830888883
Loss in iteration 166 : 0.368876652033561
Loss in iteration 167 : 0.36885295802585205
Loss in iteration 168 : 0.36882961504479883
Loss in iteration 169 : 0.368806359347893
Loss in iteration 170 : 0.3687830066698216
Loss in iteration 171 : 0.36875971999768326
Loss in iteration 172 : 0.3687365979131132
Loss in iteration 173 : 0.36871352303418603
Loss in iteration 174 : 0.3686905461389495
Loss in iteration 175 : 0.3686675668539835
Loss in iteration 176 : 0.36864456715966
Loss in iteration 177 : 0.3686216145382854
Loss in iteration 178 : 0.36859884996789566
Loss in iteration 179 : 0.36857640601338854
Loss in iteration 180 : 0.36855391968726464
Loss in iteration 181 : 0.3685313299631498
Loss in iteration 182 : 0.36850887624638357
Loss in iteration 183 : 0.36848642988310104
Loss in iteration 184 : 0.3684647148877397
Loss in iteration 185 : 0.368442875702793
Loss in iteration 186 : 0.36842100258033533
Loss in iteration 187 : 0.36839903363963367
Loss in iteration 188 : 0.36837697836126687
Loss in iteration 189 : 0.3683551619487663
Loss in iteration 190 : 0.36833396245225447
Loss in iteration 191 : 0.3683130740713036
Loss in iteration 192 : 0.3682919525847833
Loss in iteration 193 : 0.36827031753079575
Loss in iteration 194 : 0.3682486395311042
Loss in iteration 195 : 0.36822745642934307
Loss in iteration 196 : 0.36820690738342077
Loss in iteration 197 : 0.3681862617981223
Loss in iteration 198 : 0.36816525138463735
Loss in iteration 199 : 0.3681443641108738
Loss in iteration 200 : 0.36812361941012417
Testing accuracy  of updater 9 on alg 1 with rate 0.001568 = 0.78625, training accuracy 0.8400776950469407, time elapsed: 3398 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9288083058723517
Loss in iteration 3 : 0.8016345253809394
Loss in iteration 4 : 0.6562761127814685
Loss in iteration 5 : 0.5679842036019951
Loss in iteration 6 : 0.5519656370790609
Loss in iteration 7 : 0.5697766382203713
Loss in iteration 8 : 0.5936997840705932
Loss in iteration 9 : 0.6135471306961063
Loss in iteration 10 : 0.6270380731230258
Loss in iteration 11 : 0.6340252533718408
Loss in iteration 12 : 0.6350967132756271
Loss in iteration 13 : 0.6308929917822875
Loss in iteration 14 : 0.6220225679694826
Loss in iteration 15 : 0.6090796281584159
Loss in iteration 16 : 0.592690457498638
Loss in iteration 17 : 0.5737678884930866
Loss in iteration 18 : 0.5535877254799987
Loss in iteration 19 : 0.5335829134196413
Loss in iteration 20 : 0.5158288551802658
Loss in iteration 21 : 0.5010137774963884
Loss in iteration 22 : 0.4905561367740043
Loss in iteration 23 : 0.48492699173018744
Loss in iteration 24 : 0.48239594414105913
Loss in iteration 25 : 0.4820411724516911
Loss in iteration 26 : 0.48280302821718707
Loss in iteration 27 : 0.48360614100822
Loss in iteration 28 : 0.4837268253366754
Loss in iteration 29 : 0.48260788855824827
Loss in iteration 30 : 0.48018805048558766
Loss in iteration 31 : 0.47661126721151537
Loss in iteration 32 : 0.47209155629456967
Loss in iteration 33 : 0.46695896268011894
Loss in iteration 34 : 0.46163079802857965
Loss in iteration 35 : 0.45649430835440946
Loss in iteration 36 : 0.4516952888806542
Loss in iteration 37 : 0.447407475536854
Loss in iteration 38 : 0.44364758921260394
Loss in iteration 39 : 0.44032379811692324
Loss in iteration 40 : 0.4374303733529914
Loss in iteration 41 : 0.4349903734340877
Loss in iteration 42 : 0.432801722579432
Loss in iteration 43 : 0.4308418618714267
Loss in iteration 44 : 0.428918913336679
Loss in iteration 45 : 0.4269672100062113
Loss in iteration 46 : 0.42498254470815566
Loss in iteration 47 : 0.42293911773542464
Loss in iteration 48 : 0.42085150316800884
Loss in iteration 49 : 0.41872963630980836
Loss in iteration 50 : 0.41659436109760206
Loss in iteration 51 : 0.414504137594549
Loss in iteration 52 : 0.4124583018781048
Loss in iteration 53 : 0.4104876116266925
Loss in iteration 54 : 0.40861939167451583
Loss in iteration 55 : 0.4068759628264914
Loss in iteration 56 : 0.4053030929885365
Loss in iteration 57 : 0.40388050642707035
Loss in iteration 58 : 0.4026089501102693
Loss in iteration 59 : 0.40144609151412974
Testing accuracy  of updater 9 on alg 1 with rate 3.92E-4 = 0.779, training accuracy 0.8387827775979282, time elapsed: 1769 millisecond.
