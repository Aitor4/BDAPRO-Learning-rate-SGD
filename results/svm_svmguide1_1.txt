objc[2865]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10470a4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10572f4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 21:26:38 INFO SparkContext: Running Spark version 2.0.0
18/02/26 21:26:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 21:26:39 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 21:26:39 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 21:26:39 INFO SecurityManager: Changing view acls groups to: 
18/02/26 21:26:39 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 21:26:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 21:26:40 INFO Utils: Successfully started service 'sparkDriver' on port 51586.
18/02/26 21:26:40 INFO SparkEnv: Registering MapOutputTracker
18/02/26 21:26:40 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 21:26:40 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-effe9ef8-5bc1-4a43-9296-15dcc97a22da
18/02/26 21:26:41 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 21:26:41 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 21:26:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 21:26:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 21:26:41 INFO Executor: Starting executor ID driver on host localhost
18/02/26 21:26:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51587.
18/02/26 21:26:41 INFO NettyBlockTransferService: Server created on 192.168.2.140:51587
18/02/26 21:26:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 51587)
18/02/26 21:26:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:51587 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 51587)
18/02/26 21:26:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 51587)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 27.859476456914802
Loss in iteration 3 : 16.91628840210134
Loss in iteration 4 : 6.040633277946814
Loss in iteration 5 : 2.32520704244008
Loss in iteration 6 : 2.0936371471629376
Loss in iteration 7 : 1.9529915404352411
Loss in iteration 8 : 1.8596065310651941
Loss in iteration 9 : 1.7788544915693876
Loss in iteration 10 : 1.700914061665732
Loss in iteration 11 : 1.6243131507641282
Loss in iteration 12 : 1.5479512247008174
Loss in iteration 13 : 1.4720716406045702
Loss in iteration 14 : 1.3973314473829748
Loss in iteration 15 : 1.3235452921394806
Loss in iteration 16 : 1.2503989217416178
Loss in iteration 17 : 1.1780309398227948
Loss in iteration 18 : 1.1088270687368156
Loss in iteration 19 : 1.042272027612083
Loss in iteration 20 : 0.9855996381886397
Loss in iteration 21 : 1.0080924252056718
Loss in iteration 22 : 1.8541226513510793
Loss in iteration 23 : 8.737711501595541
Loss in iteration 24 : 21.221592498899103
Loss in iteration 25 : 10.278404444085648
Loss in iteration 26 : 1.9851677741424234
Loss in iteration 27 : 1.8192777024486968
Loss in iteration 28 : 1.7199681569394853
Loss in iteration 29 : 1.6385872199769438
Loss in iteration 30 : 1.5585590613187872
Loss in iteration 31 : 1.4893712930434508
Loss in iteration 32 : 1.4255593898336432
Loss in iteration 33 : 1.3631971433687333
Loss in iteration 34 : 1.3311101432112846
Loss in iteration 35 : 1.3708479607838717
Loss in iteration 36 : 1.7498264163903363
Loss in iteration 37 : 2.61852573903297
Loss in iteration 38 : 6.684425312078444
Loss in iteration 39 : 2.3441849070342013
Loss in iteration 40 : 4.323312687445414
Loss in iteration 41 : 4.0033439502837025
Loss in iteration 42 : 6.500582616223399
Loss in iteration 43 : 2.2312279618538398
Loss in iteration 44 : 2.4844813473018865
Loss in iteration 45 : 1.997441514758625
Loss in iteration 46 : 2.2234217839134214
Loss in iteration 47 : 2.0191317089402014
Loss in iteration 48 : 2.5220044742642163
Loss in iteration 49 : 2.294079432659887
Loss in iteration 50 : 3.2132543101101274
Loss in iteration 51 : 3.0615260077335935
Loss in iteration 52 : 4.655452666508553
Loss in iteration 53 : 3.066786628177434
Loss in iteration 54 : 3.906452472533135
Loss in iteration 55 : 2.6501035068365284
Loss in iteration 56 : 2.9069707251102344
Loss in iteration 57 : 2.0922877696703828
Loss in iteration 58 : 2.212154097815452
Loss in iteration 59 : 1.922199777671843
Loss in iteration 60 : 2.1365241516761624
Loss in iteration 61 : 1.9873546565232694
Loss in iteration 62 : 2.5273930767279205
Loss in iteration 63 : 2.3148564343496654
Loss in iteration 64 : 3.4603620585446806
Loss in iteration 65 : 3.15812753291503
Loss in iteration 66 : 4.855255250072712
Loss in iteration 67 : 2.9271818391174826
Loss in iteration 68 : 3.509309945095923
Loss in iteration 69 : 2.4814636047837104
Loss in iteration 70 : 2.727456534460165
Loss in iteration 71 : 1.9989057355218456
Loss in iteration 72 : 2.106371666944234
Loss in iteration 73 : 1.8769733099410004
Loss in iteration 74 : 2.1240296091614765
Loss in iteration 75 : 2.0309949831918006
Loss in iteration 76 : 2.641719599860701
Loss in iteration 77 : 2.431044460881204
Loss in iteration 78 : 3.77910861560104
Loss in iteration 79 : 3.3100167749504457
Loss in iteration 80 : 5.03587768593557
Loss in iteration 81 : 2.825744719341923
Loss in iteration 82 : 3.3698659473184374
Loss in iteration 83 : 2.320117191878271
Loss in iteration 84 : 2.4749835720443816
Loss in iteration 85 : 1.9588453352577508
Loss in iteration 86 : 2.120834603888126
Loss in iteration 87 : 1.9032522919957602
Loss in iteration 88 : 2.158442082276249
Loss in iteration 89 : 2.066251522823033
Loss in iteration 90 : 2.768892277978783
Loss in iteration 91 : 2.621285111410198
Loss in iteration 92 : 4.051480861475741
Loss in iteration 93 : 3.2228542400238904
Loss in iteration 94 : 4.620954733889481
Loss in iteration 95 : 2.8565267039132354
Loss in iteration 96 : 3.395960900362165
Loss in iteration 97 : 2.3036034212780225
Loss in iteration 98 : 2.426126497473537
Loss in iteration 99 : 1.9377477637951446
Loss in iteration 100 : 2.0921522129652157
Loss in iteration 101 : 1.8595716062909586
Loss in iteration 102 : 2.137972708134624
Loss in iteration 103 : 2.038512989125885
Loss in iteration 104 : 2.6961577743902936
Loss in iteration 105 : 2.5268264887363605
Loss in iteration 106 : 3.9384992996302035
Loss in iteration 107 : 3.1920932268116164
Loss in iteration 108 : 4.696060986686117
Loss in iteration 109 : 2.8292911062002166
Loss in iteration 110 : 3.39192423678631
Loss in iteration 111 : 2.3398590791886003
Loss in iteration 112 : 2.4783877326494523
Loss in iteration 113 : 1.9527152889688777
Loss in iteration 114 : 2.125706414690688
Loss in iteration 115 : 1.8921528274580555
Loss in iteration 116 : 2.153242840438925
Loss in iteration 117 : 2.0565076125985837
Loss in iteration 118 : 2.758109936023893
Loss in iteration 119 : 2.5982367778549977
Loss in iteration 120 : 4.017443030572755
Loss in iteration 121 : 3.1437034210452084
Loss in iteration 122 : 4.493486896067007
Loss in iteration 123 : 2.826853303157599
Loss in iteration 124 : 3.3828343984040896
Loss in iteration 125 : 2.2926566877703722
Loss in iteration 126 : 2.4583574876357384
Loss in iteration 127 : 1.9543073272957698
Loss in iteration 128 : 2.117672679111925
Loss in iteration 129 : 1.8889947697003031
Loss in iteration 130 : 2.1658359500517728
Loss in iteration 131 : 2.061697398298219
Loss in iteration 132 : 2.781332798791975
Loss in iteration 133 : 2.5935403481309
Loss in iteration 134 : 3.9901612773239354
Loss in iteration 135 : 3.1320561448965214
Loss in iteration 136 : 4.472968985430133
Loss in iteration 137 : 2.8237423771335917
Loss in iteration 138 : 3.379238578440612
Loss in iteration 139 : 2.3036822532569667
Loss in iteration 140 : 2.464566158828396
Loss in iteration 141 : 1.9550220168822805
Loss in iteration 142 : 2.1197601445578087
Loss in iteration 143 : 1.8916303695554288
Loss in iteration 144 : 2.1631861431382777
Loss in iteration 145 : 2.044476931387823
Loss in iteration 146 : 2.729292294374096
Loss in iteration 147 : 2.56944275646938
Loss in iteration 148 : 3.9888077347106004
Loss in iteration 149 : 3.1382893123452056
Loss in iteration 150 : 4.486584211061587
Loss in iteration 151 : 2.835280598835598
Loss in iteration 152 : 3.3760994195896723
Loss in iteration 153 : 2.328331570818982
Loss in iteration 154 : 2.477917585997611
Loss in iteration 155 : 1.9521470976390778
Loss in iteration 156 : 2.11263651616959
Loss in iteration 157 : 1.8693868952112664
Loss in iteration 158 : 2.145303190059373
Loss in iteration 159 : 2.0414521689454825
Loss in iteration 160 : 2.7324775232750853
Loss in iteration 161 : 2.5662994596593247
Loss in iteration 162 : 4.009580868963258
Loss in iteration 163 : 3.153622267540245
Loss in iteration 164 : 4.519978388198115
Loss in iteration 165 : 2.831261264639076
Loss in iteration 166 : 3.354210903656053
Loss in iteration 167 : 2.294441524987498
Loss in iteration 168 : 2.4745534906066564
Loss in iteration 169 : 1.9533670183499432
Loss in iteration 170 : 2.1208980655664194
Loss in iteration 171 : 1.8954075039653568
Loss in iteration 172 : 2.1856636773882014
Loss in iteration 173 : 2.0738180641217503
Loss in iteration 174 : 2.83844697930195
Loss in iteration 175 : 2.603081557938259
Loss in iteration 176 : 3.984458103172872
Loss in iteration 177 : 3.110488756180957
Loss in iteration 178 : 4.430371065854753
Loss in iteration 179 : 2.777310405289594
Loss in iteration 180 : 3.2778420484391093
Loss in iteration 181 : 2.2410263831919934
Loss in iteration 182 : 2.3876986368460527
Loss in iteration 183 : 1.9715014412734833
Loss in iteration 184 : 2.1322945057431717
Loss in iteration 185 : 1.9251035794925406
Loss in iteration 186 : 2.2670073314477257
Loss in iteration 187 : 2.106183593399872
Loss in iteration 188 : 2.8809679085276305
Loss in iteration 189 : 2.6327002087696063
Loss in iteration 190 : 4.027143386402657
Loss in iteration 191 : 3.073068620213722
Loss in iteration 192 : 4.315969114886463
Loss in iteration 193 : 2.6998697106985015
Loss in iteration 194 : 3.195858513663327
Loss in iteration 195 : 2.208652086222549
Loss in iteration 196 : 2.391792179071465
Loss in iteration 197 : 1.9672583969586952
Loss in iteration 198 : 2.154723659697918
Loss in iteration 199 : 1.943041810899548
Loss in iteration 200 : 2.3186916546413374
Testing accuracy  of updater 0 on alg 1 with rate 0.010000000000000002 = 0.78275, training accuracy 0.7779216574943347, time elapsed: 6774 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 19.60739590248844
Loss in iteration 3 : 11.947164264119055
Loss in iteration 4 : 4.328228724457461
Loss in iteration 5 : 1.7028372223456547
Loss in iteration 6 : 1.5440136376566653
Loss in iteration 7 : 1.4344158352271128
Loss in iteration 8 : 1.3639430645674377
Loss in iteration 9 : 1.3065466125604184
Loss in iteration 10 : 1.2517781590721522
Loss in iteration 11 : 1.1982060930728027
Loss in iteration 12 : 1.145352262700613
Loss in iteration 13 : 1.092795525294661
Loss in iteration 14 : 1.0408907680987658
Loss in iteration 15 : 0.9905378597925824
Loss in iteration 16 : 0.9426324217129195
Loss in iteration 17 : 0.8945483154768571
Loss in iteration 18 : 0.8529873430321036
Loss in iteration 19 : 0.8480372974940668
Loss in iteration 20 : 1.1104676501661623
Loss in iteration 21 : 2.6873546546810028
Loss in iteration 22 : 8.002573435080993
Loss in iteration 23 : 1.1550417567212947
Loss in iteration 24 : 1.3883544009166076
Loss in iteration 25 : 2.7059577855512518
Loss in iteration 26 : 3.7275487133728635
Loss in iteration 27 : 6.743268602211401
Loss in iteration 28 : 1.1547790384018513
Loss in iteration 29 : 1.103889067017465
Loss in iteration 30 : 1.0571879873705539
Loss in iteration 31 : 1.0107090806196477
Loss in iteration 32 : 0.9698727423085252
Loss in iteration 33 : 0.9457600163456568
Loss in iteration 34 : 1.0194719725464025
Loss in iteration 35 : 1.2942880854055523
Loss in iteration 36 : 2.9942797478657135
Loss in iteration 37 : 4.059322008550907
Loss in iteration 38 : 7.796248095247148
Loss in iteration 39 : 1.3469388056809362
Loss in iteration 40 : 1.256061601604126
Loss in iteration 41 : 1.3582680827165938
Loss in iteration 42 : 1.3365394604487375
Loss in iteration 43 : 1.7138355906322436
Loss in iteration 44 : 1.8433082252979136
Loss in iteration 45 : 3.13747072420701
Loss in iteration 46 : 2.671685969816989
Loss in iteration 47 : 4.103643388393769
Loss in iteration 48 : 1.9098260464086276
Loss in iteration 49 : 2.1130481653473283
Loss in iteration 50 : 1.5650364077507408
Loss in iteration 51 : 1.7290981406168209
Loss in iteration 52 : 1.4653590149784474
Loss in iteration 53 : 1.6530730446182098
Loss in iteration 54 : 1.4975913325529993
Loss in iteration 55 : 1.8469202748938331
Loss in iteration 56 : 1.7245976917154464
Loss in iteration 57 : 2.537375397418899
Loss in iteration 58 : 2.3369057108294777
Loss in iteration 59 : 3.459385971250999
Loss in iteration 60 : 2.107658785846373
Loss in iteration 61 : 2.4894454945783737
Loss in iteration 62 : 1.7800385758782125
Loss in iteration 63 : 1.9424960772290647
Loss in iteration 64 : 1.4840614157885117
Loss in iteration 65 : 1.5822446967838881
Loss in iteration 66 : 1.4363519139538221
Loss in iteration 67 : 1.6287093626430127
Loss in iteration 68 : 1.524750266633379
Loss in iteration 69 : 1.925076885737396
Loss in iteration 70 : 1.7815719280624636
Loss in iteration 71 : 2.7189569635877375
Loss in iteration 72 : 2.3666249590669515
Loss in iteration 73 : 3.435994950565866
Loss in iteration 74 : 2.085837943261845
Loss in iteration 75 : 2.450573963463506
Loss in iteration 76 : 1.6988948288173376
Loss in iteration 77 : 1.7996000677378943
Loss in iteration 78 : 1.4640967161895215
Loss in iteration 79 : 1.568250199187021
Loss in iteration 80 : 1.4169790830872284
Loss in iteration 81 : 1.6448846503374284
Loss in iteration 82 : 1.5452279190043576
Loss in iteration 83 : 2.068921190735439
Loss in iteration 84 : 1.9700333562244725
Loss in iteration 85 : 2.912601765810341
Loss in iteration 86 : 2.258083557118419
Loss in iteration 87 : 3.0911403727855906
Loss in iteration 88 : 2.011310665345495
Loss in iteration 89 : 2.380725030153722
Loss in iteration 90 : 1.7004421577756945
Loss in iteration 91 : 1.8074906837281204
Loss in iteration 92 : 1.4646649645546157
Loss in iteration 93 : 1.5673067214959409
Loss in iteration 94 : 1.4182266319546821
Loss in iteration 95 : 1.6459051146700414
Loss in iteration 96 : 1.5582539752382758
Loss in iteration 97 : 2.1057398523973183
Loss in iteration 98 : 1.9997605318399487
Loss in iteration 99 : 2.9675839991060875
Loss in iteration 100 : 2.263913917835071
Loss in iteration 101 : 3.0658141047653746
Loss in iteration 102 : 1.9948689238063617
Loss in iteration 103 : 2.3062969392725643
Loss in iteration 104 : 1.642994214763189
Loss in iteration 105 : 1.7595453924637519
Loss in iteration 106 : 1.455499471082815
Loss in iteration 107 : 1.5621495424926226
Loss in iteration 108 : 1.4176201033090767
Loss in iteration 109 : 1.65602867547602
Loss in iteration 110 : 1.5643242772708605
Loss in iteration 111 : 2.136445540059837
Loss in iteration 112 : 2.061937248784511
Loss in iteration 113 : 3.137154289770446
Loss in iteration 114 : 2.296225065707078
Loss in iteration 115 : 3.025056572849211
Loss in iteration 116 : 1.9505369332320208
Loss in iteration 117 : 2.22786772210177
Loss in iteration 118 : 1.5505987403274963
Loss in iteration 119 : 1.6368311993258093
Loss in iteration 120 : 1.4113639373598499
Loss in iteration 121 : 1.5212888253067152
Loss in iteration 122 : 1.410156139196287
Loss in iteration 123 : 1.6771008753343046
Loss in iteration 124 : 1.5748576749688654
Loss in iteration 125 : 2.212458465307313
Loss in iteration 126 : 2.142028284417734
Loss in iteration 127 : 3.2995550679431083
Loss in iteration 128 : 2.185834217225254
Loss in iteration 129 : 2.7988254236740997
Loss in iteration 130 : 1.890754431970193
Loss in iteration 131 : 2.1570776773098004
Loss in iteration 132 : 1.5486623182075208
Loss in iteration 133 : 1.683768903490377
Loss in iteration 134 : 1.465157206000387
Loss in iteration 135 : 1.6337766267007094
Loss in iteration 136 : 1.5104970730226912
Loss in iteration 137 : 1.8616302943853755
Loss in iteration 138 : 1.66084165185779
Loss in iteration 139 : 2.3233678236656155
Loss in iteration 140 : 2.0795172914859106
Loss in iteration 141 : 2.9609067752019675
Loss in iteration 142 : 2.119882226776508
Loss in iteration 143 : 2.6525854720791044
Loss in iteration 144 : 1.8566465086221111
Loss in iteration 145 : 2.1224171131569536
Loss in iteration 146 : 1.538042380252148
Loss in iteration 147 : 1.6875503669066712
Loss in iteration 148 : 1.4871087949191995
Loss in iteration 149 : 1.7061171867701397
Loss in iteration 150 : 1.5292438858503796
Loss in iteration 151 : 1.9347148699789387
Loss in iteration 152 : 1.7212334188123843
Loss in iteration 153 : 2.4505633872700625
Loss in iteration 154 : 2.1176283289843627
Loss in iteration 155 : 3.009343095890386
Loss in iteration 156 : 2.061830632367828
Loss in iteration 157 : 2.525409939788763
Loss in iteration 158 : 1.8089646548185963
Loss in iteration 159 : 2.0598967696935584
Loss in iteration 160 : 1.5213676451928166
Loss in iteration 161 : 1.669065236408256
Loss in iteration 162 : 1.4739342676394764
Loss in iteration 163 : 1.6979857339946218
Loss in iteration 164 : 1.5321728710405276
Loss in iteration 165 : 1.9561937821570863
Loss in iteration 166 : 1.7347123791553583
Loss in iteration 167 : 2.476670492550476
Loss in iteration 168 : 2.1360873344737987
Loss in iteration 169 : 3.0154674549428084
Loss in iteration 170 : 2.061277153911864
Loss in iteration 171 : 2.5250514335467105
Loss in iteration 172 : 1.7941623080677478
Loss in iteration 173 : 2.040627277043884
Loss in iteration 174 : 1.51800925598376
Loss in iteration 175 : 1.6558398530764336
Loss in iteration 176 : 1.4641177824985467
Loss in iteration 177 : 1.681483091893721
Loss in iteration 178 : 1.5333533060816265
Loss in iteration 179 : 1.9610665218759622
Loss in iteration 180 : 1.7525129169869305
Loss in iteration 181 : 2.5140738467073853
Loss in iteration 182 : 2.149403220830659
Loss in iteration 183 : 3.0469274283069105
Loss in iteration 184 : 2.0307608009646105
Loss in iteration 185 : 2.4598219925909945
Loss in iteration 186 : 1.7796297629570161
Loss in iteration 187 : 2.0337362205286422
Loss in iteration 188 : 1.5257645287155603
Loss in iteration 189 : 1.686692744110727
Loss in iteration 190 : 1.4863497748280203
Loss in iteration 191 : 1.6972456785767642
Loss in iteration 192 : 1.5318997775269008
Loss in iteration 193 : 1.9629647936998305
Loss in iteration 194 : 1.7503500156845975
Loss in iteration 195 : 2.5089449909918606
Loss in iteration 196 : 2.1540600802616128
Loss in iteration 197 : 3.0546341838216016
Loss in iteration 198 : 2.0225885826284875
Loss in iteration 199 : 2.456359140153662
Loss in iteration 200 : 1.7824914686426385
Testing accuracy  of updater 0 on alg 1 with rate 0.007 = 0.6915, training accuracy 0.7730657170605374, time elapsed: 4817 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 11.355315348062131
Loss in iteration 3 : 6.978040126136736
Loss in iteration 4 : 2.6170298043616325
Loss in iteration 5 : 1.0852194563557118
Loss in iteration 6 : 0.9871481532773367
Loss in iteration 7 : 0.9116758286279951
Loss in iteration 8 : 0.8721419436765999
Loss in iteration 9 : 0.8399436699680379
Loss in iteration 10 : 0.8082423793568553
Loss in iteration 11 : 0.7778870814027321
Loss in iteration 12 : 0.7494679143893128
Loss in iteration 13 : 0.7211065895614791
Loss in iteration 14 : 0.6940132845920329
Loss in iteration 15 : 0.6692306358489809
Loss in iteration 16 : 0.6518492165676092
Loss in iteration 17 : 0.6615224407762587
Loss in iteration 18 : 0.802186309116701
Loss in iteration 19 : 1.4212745175151618
Loss in iteration 20 : 3.428185867263587
Loss in iteration 21 : 0.772557796712292
Loss in iteration 22 : 0.9864707918034494
Loss in iteration 23 : 1.4181075304221051
Loss in iteration 24 : 2.9077107342708786
Loss in iteration 25 : 1.0263802500372485
Loss in iteration 26 : 1.412138219925712
Loss in iteration 27 : 1.526305568985732
Loss in iteration 28 : 2.351970285026589
Loss in iteration 29 : 1.3093246298467283
Loss in iteration 30 : 1.6054806513992492
Loss in iteration 31 : 1.2321818491294065
Loss in iteration 32 : 1.3704976837148013
Loss in iteration 33 : 1.1144993703632986
Loss in iteration 34 : 1.1860028039204271
Loss in iteration 35 : 1.0123020657727244
Loss in iteration 36 : 1.1338576931401785
Loss in iteration 37 : 1.0237856672987766
Loss in iteration 38 : 1.18542836227343
Loss in iteration 39 : 1.109269129192406
Loss in iteration 40 : 1.41084596638967
Loss in iteration 41 : 1.3163269614741058
Loss in iteration 42 : 1.792235026351039
Loss in iteration 43 : 1.3471288947663052
Loss in iteration 44 : 1.6416477437148984
Loss in iteration 45 : 1.2043408312914794
Loss in iteration 46 : 1.2995467198879203
Loss in iteration 47 : 1.0212757126984335
Loss in iteration 48 : 1.1121357846112132
Loss in iteration 49 : 0.9820498734065106
Loss in iteration 50 : 1.1068482470131549
Loss in iteration 51 : 0.9994149458546031
Loss in iteration 52 : 1.1854872086323367
Loss in iteration 53 : 1.1131968103664591
Loss in iteration 54 : 1.540337294155232
Loss in iteration 55 : 1.3512781727829077
Loss in iteration 56 : 1.8245811905380411
Loss in iteration 57 : 1.3285091564585165
Loss in iteration 58 : 1.5868676296764082
Loss in iteration 59 : 1.174888135901474
Loss in iteration 60 : 1.2725212831717243
Loss in iteration 61 : 1.0098226169469675
Loss in iteration 62 : 1.0840506676418693
Loss in iteration 63 : 0.9573876284742504
Loss in iteration 64 : 1.0716784042821372
Loss in iteration 65 : 0.9963111377424818
Loss in iteration 66 : 1.201183299172508
Loss in iteration 67 : 1.133815838461617
Loss in iteration 68 : 1.5822537763027462
Loss in iteration 69 : 1.365976499767202
Loss in iteration 70 : 1.835369817948691
Loss in iteration 71 : 1.2909168543194176
Loss in iteration 72 : 1.5074726674938586
Loss in iteration 73 : 1.1346653290909223
Loss in iteration 74 : 1.2446762862290042
Loss in iteration 75 : 0.9961411815988609
Loss in iteration 76 : 1.0805946179746646
Loss in iteration 77 : 0.9640664271553547
Loss in iteration 78 : 1.0953131982722148
Loss in iteration 79 : 1.0039397099073573
Loss in iteration 80 : 1.2369992463141515
Loss in iteration 81 : 1.1628608327368322
Loss in iteration 82 : 1.5887769656528103
Loss in iteration 83 : 1.3491060230522587
Loss in iteration 84 : 1.805637252498155
Loss in iteration 85 : 1.2750362194745395
Loss in iteration 86 : 1.4929763521963024
Loss in iteration 87 : 1.1251660748081225
Loss in iteration 88 : 1.2390448876524045
Loss in iteration 89 : 0.9871372677724972
Loss in iteration 90 : 1.0594056478679286
Loss in iteration 91 : 0.9539479438524286
Loss in iteration 92 : 1.0598714930412076
Loss in iteration 93 : 0.9974791705659646
Loss in iteration 94 : 1.2333520879342112
Loss in iteration 95 : 1.1708002541168565
Loss in iteration 96 : 1.603364224007815
Loss in iteration 97 : 1.3639486774231935
Loss in iteration 98 : 1.81138731902718
Loss in iteration 99 : 1.277116602509789
Loss in iteration 100 : 1.4874982568242228
Loss in iteration 101 : 1.1182040101525295
Loss in iteration 102 : 1.232642993786262
Loss in iteration 103 : 0.9828411201742194
Loss in iteration 104 : 1.0439471763828803
Loss in iteration 105 : 0.947167287902766
Loss in iteration 106 : 1.0532013848030177
Loss in iteration 107 : 0.9876876039010611
Loss in iteration 108 : 1.2284531332351525
Loss in iteration 109 : 1.1768119266512833
Loss in iteration 110 : 1.6299043304069194
Loss in iteration 111 : 1.3688140695243358
Loss in iteration 112 : 1.800098821976772
Loss in iteration 113 : 1.2783998824830078
Loss in iteration 114 : 1.4831147206258624
Loss in iteration 115 : 1.1076658471510554
Loss in iteration 116 : 1.2081891790365016
Loss in iteration 117 : 0.9715208407325995
Loss in iteration 118 : 1.0436810227343127
Loss in iteration 119 : 0.95391110148832
Loss in iteration 120 : 1.0682418917648868
Loss in iteration 121 : 1.0031579746629329
Loss in iteration 122 : 1.2556799020875282
Loss in iteration 123 : 1.1807419902975116
Loss in iteration 124 : 1.639509937867286
Loss in iteration 125 : 1.3562067856954203
Loss in iteration 126 : 1.7920513832888867
Loss in iteration 127 : 1.2727858883941017
Loss in iteration 128 : 1.4769813081380043
Loss in iteration 129 : 1.1010146585234222
Loss in iteration 130 : 1.2141807145098127
Loss in iteration 131 : 0.9690012026382787
Loss in iteration 132 : 1.0429415888356568
Loss in iteration 133 : 0.9550002494232844
Loss in iteration 134 : 1.0670992622635136
Loss in iteration 135 : 1.006548721185328
Loss in iteration 136 : 1.2582327749344246
Loss in iteration 137 : 1.177481953488549
Loss in iteration 138 : 1.623413296763625
Loss in iteration 139 : 1.33951558226308
Loss in iteration 140 : 1.763837683160561
Loss in iteration 141 : 1.277299439171708
Loss in iteration 142 : 1.4893303176646433
Loss in iteration 143 : 1.122869579755239
Loss in iteration 144 : 1.2571712948962395
Loss in iteration 145 : 0.9919980926110372
Loss in iteration 146 : 1.0520537250835154
Loss in iteration 147 : 0.9573736224977333
Loss in iteration 148 : 1.0540687820176469
Loss in iteration 149 : 0.9887594048243853
Loss in iteration 150 : 1.2061976812337913
Loss in iteration 151 : 1.122728369403736
Loss in iteration 152 : 1.5559690556003791
Loss in iteration 153 : 1.3581071883422582
Loss in iteration 154 : 1.813814393765954
Loss in iteration 155 : 1.289682388667855
Loss in iteration 156 : 1.521468640756216
Loss in iteration 157 : 1.1334976871229643
Loss in iteration 158 : 1.267413387482387
Loss in iteration 159 : 0.9931140409785312
Loss in iteration 160 : 1.0473531322709835
Loss in iteration 161 : 0.9526828793412522
Loss in iteration 162 : 1.0502741294179951
Loss in iteration 163 : 0.9878390006787118
Loss in iteration 164 : 1.1997888688925584
Loss in iteration 165 : 1.1221506051149635
Loss in iteration 166 : 1.5481816354063
Loss in iteration 167 : 1.3423893310307777
Loss in iteration 168 : 1.8045890240853266
Loss in iteration 169 : 1.294144216535222
Loss in iteration 170 : 1.5245354483455038
Loss in iteration 171 : 1.136704780989568
Loss in iteration 172 : 1.2728606374858922
Loss in iteration 173 : 1.0040630692495058
Loss in iteration 174 : 1.0586185751516564
Loss in iteration 175 : 0.9546762600335512
Loss in iteration 176 : 1.0490563131640582
Loss in iteration 177 : 0.9899869243229165
Loss in iteration 178 : 1.200355456534208
Loss in iteration 179 : 1.1215871990863486
Loss in iteration 180 : 1.5359725795912205
Loss in iteration 181 : 1.331664114621436
Loss in iteration 182 : 1.7840638734378538
Loss in iteration 183 : 1.2891320558382682
Loss in iteration 184 : 1.5243503127585771
Loss in iteration 185 : 1.1368926044046206
Loss in iteration 186 : 1.2739014150853445
Loss in iteration 187 : 1.0081513639039434
Loss in iteration 188 : 1.0698251961344867
Loss in iteration 189 : 0.957675554288906
Loss in iteration 190 : 1.051685166343748
Loss in iteration 191 : 0.9896586933809907
Loss in iteration 192 : 1.1968739599263598
Loss in iteration 193 : 1.1182106502738793
Loss in iteration 194 : 1.5335492503132824
Loss in iteration 195 : 1.3293581113214787
Loss in iteration 196 : 1.7798620995261774
Loss in iteration 197 : 1.2930912936407104
Loss in iteration 198 : 1.5332597255132907
Loss in iteration 199 : 1.1454939826369481
Loss in iteration 200 : 1.2767234315171914
Testing accuracy  of updater 0 on alg 1 with rate 0.004 = 0.7885, training accuracy 0.7937843962447394, time elapsed: 4108 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.103234793635799
Loss in iteration 3 : 2.0089159881544503
Loss in iteration 4 : 0.9154522820453969
Loss in iteration 5 : 0.5043635816453064
Loss in iteration 6 : 0.4968265904861386
Loss in iteration 7 : 0.48998791179784273
Loss in iteration 8 : 0.49223146257972267
Loss in iteration 9 : 0.49481601062100933
Loss in iteration 10 : 0.5018267954156078
Loss in iteration 11 : 0.5080092579217867
Loss in iteration 12 : 0.5199395031305535
Loss in iteration 13 : 0.5241721030883124
Loss in iteration 14 : 0.5364314985980208
Loss in iteration 15 : 0.5326094319636595
Loss in iteration 16 : 0.5476081402019933
Loss in iteration 17 : 0.5344487449765954
Loss in iteration 18 : 0.5418031722366891
Loss in iteration 19 : 0.5219625469338516
Loss in iteration 20 : 0.5244799763989981
Loss in iteration 21 : 0.5106187821925596
Loss in iteration 22 : 0.5102040182242314
Loss in iteration 23 : 0.5028918051029461
Loss in iteration 24 : 0.5066604951442482
Loss in iteration 25 : 0.49894199567228037
Loss in iteration 26 : 0.5035133212929694
Loss in iteration 27 : 0.5016539209935416
Loss in iteration 28 : 0.5079575246697542
Loss in iteration 29 : 0.5066213005530612
Loss in iteration 30 : 0.5177641290012779
Loss in iteration 31 : 0.5145246103350315
Loss in iteration 32 : 0.5282892747231491
Loss in iteration 33 : 0.5153200526909606
Loss in iteration 34 : 0.5269123346387121
Loss in iteration 35 : 0.5148888489675685
Loss in iteration 36 : 0.5233043431199509
Loss in iteration 37 : 0.5103695745899798
Loss in iteration 38 : 0.5119016197683844
Loss in iteration 39 : 0.5012503356331613
Loss in iteration 40 : 0.5049012015921671
Loss in iteration 41 : 0.4980344277192055
Loss in iteration 42 : 0.505569954896435
Loss in iteration 43 : 0.49845090276923226
Loss in iteration 44 : 0.507109661553177
Loss in iteration 45 : 0.4999176089366098
Loss in iteration 46 : 0.50686219296524
Loss in iteration 47 : 0.4993246927548156
Loss in iteration 48 : 0.5096824523026081
Loss in iteration 49 : 0.5025555649021524
Loss in iteration 50 : 0.5142627641707692
Loss in iteration 51 : 0.5024357117769348
Loss in iteration 52 : 0.5113370172708139
Loss in iteration 53 : 0.5009441713998177
Loss in iteration 54 : 0.5103838839707538
Loss in iteration 55 : 0.49982235594791113
Loss in iteration 56 : 0.5062231937283396
Loss in iteration 57 : 0.4980636915892795
Loss in iteration 58 : 0.5044135722201093
Loss in iteration 59 : 0.49854418027666336
Loss in iteration 60 : 0.5073013893747087
Loss in iteration 61 : 0.49866509530242903
Loss in iteration 62 : 0.5099903381571663
Loss in iteration 63 : 0.5001489691652506
Loss in iteration 64 : 0.5095458430465575
Loss in iteration 65 : 0.49880024808951307
Loss in iteration 66 : 0.5106791434496516
Loss in iteration 67 : 0.49852660565566886
Loss in iteration 68 : 0.5091004181221761
Loss in iteration 69 : 0.4978148407980978
Loss in iteration 70 : 0.506078872495336
Loss in iteration 71 : 0.49614552655864336
Loss in iteration 72 : 0.5047374083978431
Loss in iteration 73 : 0.49565102245035214
Loss in iteration 74 : 0.504709973370935
Loss in iteration 75 : 0.49452995966585434
Loss in iteration 76 : 0.5037528597978814
Loss in iteration 77 : 0.49305608450148175
Loss in iteration 78 : 0.5033989413462275
Loss in iteration 79 : 0.4924047872964341
Loss in iteration 80 : 0.5039166707284127
Loss in iteration 81 : 0.49411334187194006
Loss in iteration 82 : 0.5049000578455566
Loss in iteration 83 : 0.49495836820791655
Loss in iteration 84 : 0.5037381355499574
Loss in iteration 85 : 0.49230895631043714
Loss in iteration 86 : 0.503923978450374
Loss in iteration 87 : 0.49203222389669254
Loss in iteration 88 : 0.5041101289230474
Loss in iteration 89 : 0.4928201006110945
Loss in iteration 90 : 0.503231933063715
Loss in iteration 91 : 0.4925456492205579
Loss in iteration 92 : 0.5035204041637357
Loss in iteration 93 : 0.49217565030976484
Loss in iteration 94 : 0.5038092403157728
Loss in iteration 95 : 0.4928739229963876
Loss in iteration 96 : 0.5056447838924583
Loss in iteration 97 : 0.494392842662052
Loss in iteration 98 : 0.5092903939327795
Loss in iteration 99 : 0.49620622793311286
Loss in iteration 100 : 0.5119326749198425
Loss in iteration 101 : 0.49727031833947494
Loss in iteration 102 : 0.511182532881503
Loss in iteration 103 : 0.4933855086118478
Loss in iteration 104 : 0.5018109484015848
Loss in iteration 105 : 0.49081170100925003
Loss in iteration 106 : 0.5012726938018309
Loss in iteration 107 : 0.49129494145503
Loss in iteration 108 : 0.5027918548591461
Loss in iteration 109 : 0.4904819320929526
Loss in iteration 110 : 0.5026857220293256
Loss in iteration 111 : 0.4905387124349601
Loss in iteration 112 : 0.5023295048090594
Loss in iteration 113 : 0.49085081060433444
Loss in iteration 114 : 0.5019732875887924
Loss in iteration 115 : 0.49116774699906024
Loss in iteration 116 : 0.5040121782275859
Loss in iteration 117 : 0.49242411957649534
Loss in iteration 118 : 0.5109470333288415
Loss in iteration 119 : 0.4962567464728913
Loss in iteration 120 : 0.5123265455768068
Loss in iteration 121 : 0.4948140441106263
Loss in iteration 122 : 0.5088173327229688
Loss in iteration 123 : 0.49294913972759224
Loss in iteration 124 : 0.5052379441639088
Loss in iteration 125 : 0.4888784063493245
Loss in iteration 126 : 0.49841655592765916
Loss in iteration 127 : 0.49295335199468954
Loss in iteration 128 : 0.5083902017122345
Loss in iteration 129 : 0.4939502200323138
Loss in iteration 130 : 0.5084363214734432
Loss in iteration 131 : 0.49237467077363195
Loss in iteration 132 : 0.5026699767151863
Loss in iteration 133 : 0.4895336248216995
Loss in iteration 134 : 0.49807118938757744
Loss in iteration 135 : 0.4932367454202102
Loss in iteration 136 : 0.5081955148060192
Loss in iteration 137 : 0.4920472892010426
Loss in iteration 138 : 0.5065216598872013
Loss in iteration 139 : 0.4891869571398238
Loss in iteration 140 : 0.49954128415036747
Loss in iteration 141 : 0.4916818128316006
Loss in iteration 142 : 0.5073389057772849
Loss in iteration 143 : 0.4908685625351427
Loss in iteration 144 : 0.5004657768671671
Loss in iteration 145 : 0.4907113728527624
Loss in iteration 146 : 0.501512466376393
Loss in iteration 147 : 0.4903762587923352
Loss in iteration 148 : 0.503907184935627
Loss in iteration 149 : 0.49131888756864495
Loss in iteration 150 : 0.5076487393125869
Loss in iteration 151 : 0.4921476267246979
Loss in iteration 152 : 0.507679924580289
Loss in iteration 153 : 0.49045338347052964
Loss in iteration 154 : 0.5017155942469713
Loss in iteration 155 : 0.49011847350293586
Loss in iteration 156 : 0.5015820310529101
Loss in iteration 157 : 0.4902358739166507
Loss in iteration 158 : 0.5045081239838681
Loss in iteration 159 : 0.49064157897024896
Loss in iteration 160 : 0.5054397250548894
Loss in iteration 161 : 0.4906940059948016
Loss in iteration 162 : 0.5053733612691017
Loss in iteration 163 : 0.4897503721377327
Loss in iteration 164 : 0.5019058538977573
Loss in iteration 165 : 0.48870391496281634
Loss in iteration 166 : 0.5009921874110888
Loss in iteration 167 : 0.48960349657368907
Loss in iteration 168 : 0.5050751703829158
Loss in iteration 169 : 0.4910129019413276
Loss in iteration 170 : 0.5070724821481398
Loss in iteration 171 : 0.4925309913826738
Loss in iteration 172 : 0.5086263183216132
Loss in iteration 173 : 0.4939242370155991
Loss in iteration 174 : 0.5089551030061615
Loss in iteration 175 : 0.4926789034183868
Loss in iteration 176 : 0.5058173194824601
Loss in iteration 177 : 0.4883411725530788
Loss in iteration 178 : 0.49868381282156415
Loss in iteration 179 : 0.4903901241115673
Loss in iteration 180 : 0.50449018417819
Loss in iteration 181 : 0.48772870583365624
Loss in iteration 182 : 0.4987229509595441
Loss in iteration 183 : 0.4903288703052705
Loss in iteration 184 : 0.5040234082845211
Loss in iteration 185 : 0.48816502827559327
Loss in iteration 186 : 0.500421336965755
Loss in iteration 187 : 0.4907162366509477
Loss in iteration 188 : 0.5058637428746072
Loss in iteration 189 : 0.4892124836992306
Loss in iteration 190 : 0.503225896561933
Loss in iteration 191 : 0.487885833955384
Loss in iteration 192 : 0.5005646356158097
Loss in iteration 193 : 0.49055009082074397
Loss in iteration 194 : 0.5060080272195911
Loss in iteration 195 : 0.4894514019967378
Loss in iteration 196 : 0.5043067061520274
Loss in iteration 197 : 0.48784010474688794
Loss in iteration 198 : 0.5005903444432753
Loss in iteration 199 : 0.49050391827197487
Loss in iteration 200 : 0.505005885852001
Testing accuracy  of updater 0 on alg 1 with rate 0.001 = 0.78825, training accuracy 0.8022013596633215, time elapsed: 3595 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.2780267381931654
Loss in iteration 3 : 1.512003574356223
Loss in iteration 4 : 0.7472654443221762
Loss in iteration 5 : 0.4580709815326404
Loss in iteration 6 : 0.4738467632568665
Loss in iteration 7 : 0.49106684704131354
Loss in iteration 8 : 0.5041138199845822
Loss in iteration 9 : 0.4976963195971572
Loss in iteration 10 : 0.49361198663836553
Loss in iteration 11 : 0.4788269480745761
Loss in iteration 12 : 0.46670774994715764
Loss in iteration 13 : 0.44866737378765986
Loss in iteration 14 : 0.440301367969547
Loss in iteration 15 : 0.43353055307547667
Loss in iteration 16 : 0.42924062721822426
Loss in iteration 17 : 0.42146661302189997
Loss in iteration 18 : 0.4199163222556593
Loss in iteration 19 : 0.41263695844277487
Loss in iteration 20 : 0.41215732275533273
Loss in iteration 21 : 0.41224440285731245
Loss in iteration 22 : 0.4119053729491308
Loss in iteration 23 : 0.41353448145811234
Loss in iteration 24 : 0.41867986866388157
Loss in iteration 25 : 0.4204218502920882
Loss in iteration 26 : 0.42798635223752424
Loss in iteration 27 : 0.4338812528575143
Loss in iteration 28 : 0.4406663760579737
Loss in iteration 29 : 0.44910600972611603
Loss in iteration 30 : 0.46246695677484845
Loss in iteration 31 : 0.46297461753619645
Loss in iteration 32 : 0.4719558370391283
Loss in iteration 33 : 0.4643181826502439
Loss in iteration 34 : 0.4654956741757817
Loss in iteration 35 : 0.4494174366465128
Loss in iteration 36 : 0.44226751305462686
Loss in iteration 37 : 0.43527098039629303
Loss in iteration 38 : 0.42831872479487154
Loss in iteration 39 : 0.4236563525290641
Loss in iteration 40 : 0.41841042802553613
Loss in iteration 41 : 0.41382441364044587
Loss in iteration 42 : 0.41256462424554247
Loss in iteration 43 : 0.4118540183082646
Loss in iteration 44 : 0.412185612407564
Loss in iteration 45 : 0.41286119780478503
Loss in iteration 46 : 0.4155116372859534
Loss in iteration 47 : 0.4198554694075728
Loss in iteration 48 : 0.4204193054320408
Loss in iteration 49 : 0.425064268768508
Loss in iteration 50 : 0.4302424454617012
Loss in iteration 51 : 0.4348098215755326
Loss in iteration 52 : 0.4418640958939409
Loss in iteration 53 : 0.4463757372836897
Loss in iteration 54 : 0.453918581483928
Loss in iteration 55 : 0.44837274811380906
Loss in iteration 56 : 0.45084031341497594
Loss in iteration 57 : 0.44680169948466303
Loss in iteration 58 : 0.44492113501455716
Loss in iteration 59 : 0.4370585863583325
Loss in iteration 60 : 0.43636549147051357
Loss in iteration 61 : 0.43062990031319526
Loss in iteration 62 : 0.42793558366877227
Loss in iteration 63 : 0.42557618433104455
Loss in iteration 64 : 0.42248512224621526
Loss in iteration 65 : 0.4222263194439814
Loss in iteration 66 : 0.4167725627511684
Loss in iteration 67 : 0.41502590516739524
Loss in iteration 68 : 0.4145575354624967
Loss in iteration 69 : 0.41412951935858333
Loss in iteration 70 : 0.41437244282691954
Loss in iteration 71 : 0.41582206817393624
Loss in iteration 72 : 0.4182270477320488
Loss in iteration 73 : 0.422868972208211
Loss in iteration 74 : 0.4274460552638554
Loss in iteration 75 : 0.4286375560458857
Loss in iteration 76 : 0.4361078772855878
Loss in iteration 77 : 0.43661368712898196
Loss in iteration 78 : 0.44476426753669496
Loss in iteration 79 : 0.44448053693896733
Loss in iteration 80 : 0.4504002033378269
Loss in iteration 81 : 0.44310022315701014
Loss in iteration 82 : 0.4473797181775586
Loss in iteration 83 : 0.44051897249090727
Loss in iteration 84 : 0.44230519859132583
Loss in iteration 85 : 0.4329995334350145
Loss in iteration 86 : 0.43078744402962466
Loss in iteration 87 : 0.42643185819126017
Loss in iteration 88 : 0.42139445151488814
Loss in iteration 89 : 0.42091699433801283
Loss in iteration 90 : 0.41460753621713964
Loss in iteration 91 : 0.41336385248135216
Loss in iteration 92 : 0.4119535419165377
Loss in iteration 93 : 0.40991285742403116
Loss in iteration 94 : 0.4108382663331458
Loss in iteration 95 : 0.4109260508876525
Loss in iteration 96 : 0.41386688400863914
Loss in iteration 97 : 0.4186724310055755
Loss in iteration 98 : 0.42221280516687615
Loss in iteration 99 : 0.42608772994817395
Loss in iteration 100 : 0.43166553082444276
Loss in iteration 101 : 0.4345735579305871
Loss in iteration 102 : 0.44133178877372503
Loss in iteration 103 : 0.43994790868333855
Loss in iteration 104 : 0.44635432541141434
Loss in iteration 105 : 0.44275259996580985
Loss in iteration 106 : 0.44742067732771695
Loss in iteration 107 : 0.43970671616140755
Loss in iteration 108 : 0.4417000916014448
Loss in iteration 109 : 0.43366471931551626
Loss in iteration 110 : 0.4293423269179622
Loss in iteration 111 : 0.4246352378679436
Loss in iteration 112 : 0.4215211598743132
Loss in iteration 113 : 0.42012897938872473
Loss in iteration 114 : 0.4162216049138171
Loss in iteration 115 : 0.4167709532685215
Loss in iteration 116 : 0.4158447673273735
Loss in iteration 117 : 0.4173658518981585
Loss in iteration 118 : 0.4171637285434393
Loss in iteration 119 : 0.4189605083785867
Loss in iteration 120 : 0.4203528959140059
Loss in iteration 121 : 0.4230104820080654
Loss in iteration 122 : 0.4249195200215105
Loss in iteration 123 : 0.42514868861410643
Loss in iteration 124 : 0.4279000636906609
Loss in iteration 125 : 0.4292378493257131
Loss in iteration 126 : 0.43264370834231053
Loss in iteration 127 : 0.4326601366714706
Loss in iteration 128 : 0.4355410060835468
Loss in iteration 129 : 0.43331757061390813
Loss in iteration 130 : 0.4353686028267349
Loss in iteration 131 : 0.4322667048910914
Loss in iteration 132 : 0.4335302568469702
Loss in iteration 133 : 0.43096184042222135
Loss in iteration 134 : 0.43117040314930705
Loss in iteration 135 : 0.4258012909147229
Loss in iteration 136 : 0.4250129289214321
Loss in iteration 137 : 0.4227804680915856
Loss in iteration 138 : 0.421678368246372
Loss in iteration 139 : 0.4202497813923849
Loss in iteration 140 : 0.4197549301886294
Loss in iteration 141 : 0.41995525846949533
Loss in iteration 142 : 0.4209584784519419
Loss in iteration 143 : 0.42108833269962154
Loss in iteration 144 : 0.4225882658873304
Loss in iteration 145 : 0.422467734592942
Loss in iteration 146 : 0.4251686147213769
Loss in iteration 147 : 0.4239208197503778
Loss in iteration 148 : 0.42682348347138627
Loss in iteration 149 : 0.4265539737381644
Loss in iteration 150 : 0.42695796678020675
Loss in iteration 151 : 0.42603286592140055
Loss in iteration 152 : 0.42633201305988977
Loss in iteration 153 : 0.4251229619606893
Loss in iteration 154 : 0.4272260965711953
Loss in iteration 155 : 0.42572955584310984
Loss in iteration 156 : 0.4265999750465607
Loss in iteration 157 : 0.42566380315568064
Loss in iteration 158 : 0.4266489547487752
Loss in iteration 159 : 0.42559805046825244
Loss in iteration 160 : 0.42669793445098975
Loss in iteration 161 : 0.42553229778082324
Loss in iteration 162 : 0.4267469141532052
Loss in iteration 163 : 0.4254667941792833
Loss in iteration 164 : 0.42625427276586464
Loss in iteration 165 : 0.4252678866364332
Loss in iteration 166 : 0.4264400706391671
Loss in iteration 167 : 0.42506897909358254
Loss in iteration 168 : 0.42662636821587474
Loss in iteration 169 : 0.4253106951640491
Loss in iteration 170 : 0.42637165705911406
Loss in iteration 171 : 0.4251117876211988
Loss in iteration 172 : 0.42655794796356106
Loss in iteration 173 : 0.4253535036916655
Loss in iteration 174 : 0.42630324347906184
Loss in iteration 175 : 0.42515459614881534
Loss in iteration 176 : 0.42648952771124854
Loss in iteration 177 : 0.4253964948111871
Loss in iteration 178 : 0.42648169444750933
Loss in iteration 179 : 0.4258289556174077
Loss in iteration 180 : 0.42624729438601305
Loss in iteration 181 : 0.42517362593023894
Loss in iteration 182 : 0.42668204078770944
Loss in iteration 183 : 0.4256066061660686
Loss in iteration 184 : 0.4269610135349064
Loss in iteration 185 : 0.4258461273956602
Loss in iteration 186 : 0.42671113727267135
Loss in iteration 187 : 0.42608582074788365
Loss in iteration 188 : 0.4269992618319592
Loss in iteration 189 : 0.4257877790446215
Loss in iteration 190 : 0.4267495661270101
Loss in iteration 191 : 0.42565804472172153
Loss in iteration 192 : 0.4268700108770276
Loss in iteration 193 : 0.4255283103988218
Loss in iteration 194 : 0.42699045562704524
Loss in iteration 195 : 0.4253985760759215
Loss in iteration 196 : 0.42711090037706273
Loss in iteration 197 : 0.4252688417530217
Loss in iteration 198 : 0.4272319063906637
Loss in iteration 199 : 0.4263760095264069
Loss in iteration 200 : 0.4276042625328949
Testing accuracy  of updater 0 on alg 1 with rate 7.000000000000001E-4 = 0.7885, training accuracy 0.8157979928779541, time elapsed: 3304 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.4528186827505332
Loss in iteration 3 : 1.0150911605579942
Loss in iteration 4 : 0.5815190272260544
Loss in iteration 5 : 0.43749722331685226
Loss in iteration 6 : 0.4580227466280155
Loss in iteration 7 : 0.44133939274321904
Loss in iteration 8 : 0.4549328263583529
Loss in iteration 9 : 0.4320877604209343
Loss in iteration 10 : 0.4329442096283662
Loss in iteration 11 : 0.41810978237453467
Loss in iteration 12 : 0.412556050045175
Loss in iteration 13 : 0.405652352525011
Loss in iteration 14 : 0.3996970294240689
Loss in iteration 15 : 0.3950196652509662
Loss in iteration 16 : 0.39170281573629506
Loss in iteration 17 : 0.38975502403161805
Loss in iteration 18 : 0.3862940509668942
Loss in iteration 19 : 0.3846800144201073
Loss in iteration 20 : 0.38408674849637997
Testing accuracy  of updater 0 on alg 1 with rate 4.000000000000001E-4 = 0.7795, training accuracy 0.8336031078018776, time elapsed: 348 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6289790209690836
Loss in iteration 3 : 0.54337596803687
Loss in iteration 4 : 0.5286968335930471
Testing accuracy  of updater 0 on alg 1 with rate 9.999999999999994E-5 = 0.5005, training accuracy 0.6474587245063127, time elapsed: 81 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.278026738193165
Loss in iteration 3 : 3.244940490785752
Loss in iteration 4 : 3.34913970428214
Loss in iteration 5 : 2.6768958325919536
Loss in iteration 6 : 1.3929907893275086
Loss in iteration 7 : 0.8112704926195141
Loss in iteration 8 : 1.4107403082360992
Loss in iteration 9 : 2.1101985413700595
Loss in iteration 10 : 2.062595115681331
Loss in iteration 11 : 1.5383657923647354
Loss in iteration 12 : 1.1704965316523188
Loss in iteration 13 : 1.1951423480351817
Loss in iteration 14 : 1.4343913436781008
Loss in iteration 15 : 1.662138711081698
Loss in iteration 16 : 1.7440187219521954
Loss in iteration 17 : 1.6644829863470476
Loss in iteration 18 : 1.4883762144456265
Loss in iteration 19 : 1.3258032080884923
Loss in iteration 20 : 1.2590328357957732
Loss in iteration 21 : 1.3071064805298043
Loss in iteration 22 : 1.412664215831433
Loss in iteration 23 : 1.4627296329008603
Loss in iteration 24 : 1.4102261467428896
Loss in iteration 25 : 1.2912640414093102
Loss in iteration 26 : 1.187889349326869
Loss in iteration 27 : 1.150850626103891
Loss in iteration 28 : 1.1698779610835914
Loss in iteration 29 : 1.1969888699523357
Loss in iteration 30 : 1.185581828041296
Loss in iteration 31 : 1.1237733877151561
Loss in iteration 32 : 1.0369375041812845
Loss in iteration 33 : 0.9696414033669549
Loss in iteration 34 : 0.9452700724245219
Loss in iteration 35 : 0.9557440670174161
Loss in iteration 36 : 0.9416583388890424
Loss in iteration 37 : 0.8756891314103024
Loss in iteration 38 : 0.8000158680219382
Loss in iteration 39 : 0.7649753799811857
Loss in iteration 40 : 0.7593816608361939
Loss in iteration 41 : 0.7424977572914585
Loss in iteration 42 : 0.688750135116535
Loss in iteration 43 : 0.6247174866695164
Loss in iteration 44 : 0.6009857782246936
Loss in iteration 45 : 0.604068539659697
Loss in iteration 46 : 0.5576429737218563
Loss in iteration 47 : 0.5029427075356112
Loss in iteration 48 : 0.5129462401672465
Loss in iteration 49 : 0.49423951447627135
Loss in iteration 50 : 0.44368856462624723
Loss in iteration 51 : 0.4917828767367927
Loss in iteration 52 : 0.43455888079445837
Loss in iteration 53 : 0.4918707263129547
Loss in iteration 54 : 0.49833772226923245
Loss in iteration 55 : 0.4934125631219915
Loss in iteration 56 : 0.4406014824423632
Loss in iteration 57 : 0.43636313982979774
Loss in iteration 58 : 0.45770238249178624
Loss in iteration 59 : 0.4051901727076079
Loss in iteration 60 : 0.42706186629423254
Loss in iteration 61 : 0.40608336896545993
Loss in iteration 62 : 0.4197278455365687
Loss in iteration 63 : 0.40316048702013396
Loss in iteration 64 : 0.4191722722612117
Loss in iteration 65 : 0.403709874365135
Loss in iteration 66 : 0.4119355588021026
Loss in iteration 67 : 0.40390442691938444
Loss in iteration 68 : 0.4009670761925126
Loss in iteration 69 : 0.39962725531272414
Loss in iteration 70 : 0.39040403154872155
Loss in iteration 71 : 0.3916851987159401
Loss in iteration 72 : 0.382109597961597
Loss in iteration 73 : 0.3817999473180907
Loss in iteration 74 : 0.382883062984003
Loss in iteration 75 : 0.37533612436084485
Loss in iteration 76 : 0.3873707992938942
Loss in iteration 77 : 0.3952275810276503
Loss in iteration 78 : 0.38447087108360817
Loss in iteration 79 : 0.37866007005879043
Loss in iteration 80 : 0.38206578302468247
Loss in iteration 81 : 0.38580433402067593
Loss in iteration 82 : 0.37919507786589945
Loss in iteration 83 : 0.3766872758664357
Loss in iteration 84 : 0.38056795972091123
Loss in iteration 85 : 0.37760337643812125
Loss in iteration 86 : 0.3792600515040933
Loss in iteration 87 : 0.37960312928260814
Testing accuracy  of updater 1 on alg 1 with rate 6.999999999999999E-4 = 0.78125, training accuracy 0.8326319197151182, time elapsed: 1328 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.700381099383317
Loss in iteration 3 : 2.377220726198133
Loss in iteration 4 : 2.450160175645608
Loss in iteration 5 : 1.979589465462469
Loss in iteration 6 : 1.0662401224670357
Loss in iteration 7 : 0.6347351760446875
Loss in iteration 8 : 1.084860427196099
Loss in iteration 9 : 1.5697839775249667
Loss in iteration 10 : 1.5057116702962425
Loss in iteration 11 : 1.1239652504309274
Loss in iteration 12 : 0.8771009398566286
Loss in iteration 13 : 0.9111910018232257
Loss in iteration 14 : 1.0932700416038377
Loss in iteration 15 : 1.2518362939709329
Loss in iteration 16 : 1.2963751522838267
Loss in iteration 17 : 1.2224704264771127
Loss in iteration 18 : 1.0906148481848095
Loss in iteration 19 : 0.9825069582343056
Loss in iteration 20 : 0.9541122310605066
Loss in iteration 21 : 1.0059580244893944
Loss in iteration 22 : 1.0807783129105295
Loss in iteration 23 : 1.1047878481060662
Loss in iteration 24 : 1.0555617601841036
Loss in iteration 25 : 0.9682683459854795
Loss in iteration 26 : 0.9012413355287331
Loss in iteration 27 : 0.8844665483852513
Loss in iteration 28 : 0.9030269433631147
Loss in iteration 29 : 0.9225191219390799
Loss in iteration 30 : 0.9108432896432644
Loss in iteration 31 : 0.863956972057853
Loss in iteration 32 : 0.8014625875668151
Loss in iteration 33 : 0.7572538657180188
Loss in iteration 34 : 0.7436829538052288
Loss in iteration 35 : 0.7509544372555946
Loss in iteration 36 : 0.7404819769078109
Loss in iteration 37 : 0.6958530062426126
Loss in iteration 38 : 0.6438943229000702
Loss in iteration 39 : 0.6186166321316416
Loss in iteration 40 : 0.6156538403620662
Loss in iteration 41 : 0.6055772605133448
Loss in iteration 42 : 0.5713394149525063
Loss in iteration 43 : 0.5268252393503727
Loss in iteration 44 : 0.5083723788536124
Loss in iteration 45 : 0.509854274189965
Loss in iteration 46 : 0.4871963991828865
Loss in iteration 47 : 0.4471312063357655
Loss in iteration 48 : 0.4440374111269658
Loss in iteration 49 : 0.4440800981552726
Loss in iteration 50 : 0.41070772774442443
Loss in iteration 51 : 0.4259347526433407
Loss in iteration 52 : 0.41208037953541354
Loss in iteration 53 : 0.4289839424010877
Loss in iteration 54 : 0.4169282381292371
Loss in iteration 55 : 0.45751219181372754
Loss in iteration 56 : 0.42376648162710767
Loss in iteration 57 : 0.4268070905790942
Loss in iteration 58 : 0.42569780245251465
Loss in iteration 59 : 0.3972503378032025
Loss in iteration 60 : 0.4046949727139121
Loss in iteration 61 : 0.38837012268781856
Loss in iteration 62 : 0.39504825168457686
Loss in iteration 63 : 0.386464414440757
Loss in iteration 64 : 0.39269609970479996
Loss in iteration 65 : 0.39028618876090737
Loss in iteration 66 : 0.38881076843626344
Loss in iteration 67 : 0.39257737559197586
Loss in iteration 68 : 0.38830898224402505
Loss in iteration 69 : 0.3894519654916603
Loss in iteration 70 : 0.38838455423554197
Loss in iteration 71 : 0.38467581730816197
Loss in iteration 72 : 0.3861259309314525
Loss in iteration 73 : 0.3810698255327843
Loss in iteration 74 : 0.3806279215922531
Loss in iteration 75 : 0.37900731328745124
Loss in iteration 76 : 0.3766510199094443
Loss in iteration 77 : 0.37738975142990394
Loss in iteration 78 : 0.37563148123781
Loss in iteration 79 : 0.37660057353088083
Loss in iteration 80 : 0.3772354895991506
Loss in iteration 81 : 0.37718829477914934
Loss in iteration 82 : 0.37855387364939413
Loss in iteration 83 : 0.37706618997922337
Loss in iteration 84 : 0.37805421791067645
Loss in iteration 85 : 0.3766408158076903
Loss in iteration 86 : 0.3773412805675669
Loss in iteration 87 : 0.3758784903032885
Loss in iteration 88 : 0.37634751791659055
Loss in iteration 89 : 0.3756051128407079
Loss in iteration 90 : 0.3756034830073855
Loss in iteration 91 : 0.37609203973863947
Testing accuracy  of updater 1 on alg 1 with rate 4.9E-4 = 0.78, training accuracy 0.8339268371641307, time elapsed: 1443 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.122735460573479
Loss in iteration 3 : 1.5095009616105135
Loss in iteration 4 : 1.5511806470090712
Loss in iteration 5 : 1.2822830983329931
Loss in iteration 6 : 0.7486017280424128
Loss in iteration 7 : 0.47225634452603077
Loss in iteration 8 : 0.7691451385517152
Loss in iteration 9 : 1.0319324214762915
Loss in iteration 10 : 0.9534576874528501
Loss in iteration 11 : 0.7197720930331377
Loss in iteration 12 : 0.5932908007002493
Loss in iteration 13 : 0.636125644370053
Loss in iteration 14 : 0.7533657248503023
Loss in iteration 15 : 0.8386989126179935
Loss in iteration 16 : 0.8436989045107285
Loss in iteration 17 : 0.7821718256533007
Loss in iteration 18 : 0.7014619338180278
Loss in iteration 19 : 0.6553532779074707
Loss in iteration 20 : 0.6615702707535551
Loss in iteration 21 : 0.7070148001840848
Loss in iteration 22 : 0.7432679507749015
Loss in iteration 23 : 0.7411664514777094
Loss in iteration 24 : 0.702112018562451
Loss in iteration 25 : 0.6534129589712694
Loss in iteration 26 : 0.6260187033001638
Loss in iteration 27 : 0.6269138016622893
Loss in iteration 28 : 0.6428049918296331
Loss in iteration 29 : 0.6507572748649784
Loss in iteration 30 : 0.6366878571664777
Loss in iteration 31 : 0.6049989847239537
Loss in iteration 32 : 0.5727146989053047
Loss in iteration 33 : 0.5549505103500694
Loss in iteration 34 : 0.5569616871082649
Loss in iteration 35 : 0.5620335046606195
Loss in iteration 36 : 0.5513568957921134
Loss in iteration 37 : 0.5243048326198548
Loss in iteration 38 : 0.4963547401334967
Loss in iteration 39 : 0.48648709612239566
Loss in iteration 40 : 0.48717925697415454
Loss in iteration 41 : 0.48236608789449203
Loss in iteration 42 : 0.46344862100989964
Loss in iteration 43 : 0.43970014118119527
Loss in iteration 44 : 0.43101052356311154
Loss in iteration 45 : 0.4329384148312882
Loss in iteration 46 : 0.4253068446136892
Loss in iteration 47 : 0.40587573802224136
Loss in iteration 48 : 0.3971680387307983
Loss in iteration 49 : 0.40179461214225926
Loss in iteration 50 : 0.39504758707247123
Loss in iteration 51 : 0.3833876323407881
Loss in iteration 52 : 0.39021337986254656
Loss in iteration 53 : 0.39083740875053485
Loss in iteration 54 : 0.38367618851047025
Loss in iteration 55 : 0.39591293152015866
Loss in iteration 56 : 0.389686752470074
Loss in iteration 57 : 0.39532197501331245
Testing accuracy  of updater 1 on alg 1 with rate 2.8E-4 = 0.79425, training accuracy 0.8459048235674975, time elapsed: 726 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5616831111248025
Loss in iteration 3 : 0.6741031612579684
Loss in iteration 4 : 0.7137399983415097
Loss in iteration 5 : 0.6738578017899848
Loss in iteration 6 : 0.5663033473585979
Loss in iteration 7 : 0.4432765482353352
Loss in iteration 8 : 0.42672848486576237
Loss in iteration 9 : 0.5047609512550936
Loss in iteration 10 : 0.5190941291120781
Loss in iteration 11 : 0.4615390854571238
Loss in iteration 12 : 0.40053916396490497
Loss in iteration 13 : 0.3893441231692431
Loss in iteration 14 : 0.4153630321036869
Loss in iteration 15 : 0.4410856084906356
Loss in iteration 16 : 0.4449020158653489
Loss in iteration 17 : 0.42844658926601875
Loss in iteration 18 : 0.4080659775937363
Loss in iteration 19 : 0.39662824775257227
Loss in iteration 20 : 0.40022426004143785
Loss in iteration 21 : 0.4112678290016612
Loss in iteration 22 : 0.420868901131106
Testing accuracy  of updater 1 on alg 1 with rate 7.0E-5 = 0.7825, training accuracy 0.8193590158627387, time elapsed: 283 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5646295346293848
Loss in iteration 3 : 0.6340997992166559
Loss in iteration 4 : 0.7115121810332772
Loss in iteration 5 : 0.7286429389162563
Loss in iteration 6 : 0.6909257201266977
Loss in iteration 7 : 0.6053481210957908
Loss in iteration 8 : 0.4944166661393719
Loss in iteration 9 : 0.4220034098139135
Loss in iteration 10 : 0.44932426285709653
Loss in iteration 11 : 0.5113637444851082
Loss in iteration 12 : 0.5173177904353481
Loss in iteration 13 : 0.4696162785451212
Loss in iteration 14 : 0.4129381903268319
Loss in iteration 15 : 0.38773517861147366
Loss in iteration 16 : 0.39709857446117097
Loss in iteration 17 : 0.4197316148060477
Loss in iteration 18 : 0.4339160709718013
Loss in iteration 19 : 0.43160412076564547
Loss in iteration 20 : 0.4176396992094269
Loss in iteration 21 : 0.4016866911446079
Loss in iteration 22 : 0.39137028350779074
Loss in iteration 23 : 0.39122868850413317
Loss in iteration 24 : 0.39812323752235956
Loss in iteration 25 : 0.40616693029843054
Testing accuracy  of updater 1 on alg 1 with rate 4.9E-5 = 0.78125, training accuracy 0.8213013920362577, time elapsed: 301 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6814280873591742
Loss in iteration 3 : 0.5700309243911778
Loss in iteration 4 : 0.6543230878641343
Loss in iteration 5 : 0.708653086695683
Loss in iteration 6 : 0.7275446379598489
Loss in iteration 7 : 0.714196193126691
Loss in iteration 8 : 0.6720047758956954
Loss in iteration 9 : 0.6050264060259383
Loss in iteration 10 : 0.5238292091160254
Loss in iteration 11 : 0.45445750361651377
Loss in iteration 12 : 0.42246006084509424
Loss in iteration 13 : 0.4441149071350898
Loss in iteration 14 : 0.4860121123782555
Loss in iteration 15 : 0.5053771113093563
Loss in iteration 16 : 0.4909773170735165
Loss in iteration 17 : 0.454752325230371
Loss in iteration 18 : 0.41665025871833244
Loss in iteration 19 : 0.3939462823336078
Loss in iteration 20 : 0.3901889239655218
Loss in iteration 21 : 0.4002750919112498
Loss in iteration 22 : 0.41384759062764653
Loss in iteration 23 : 0.4219619250511429
Loss in iteration 24 : 0.4214241595633352
Loss in iteration 25 : 0.4137436861913491
Loss in iteration 26 : 0.4029958181159461
Loss in iteration 27 : 0.39314664696265383
Loss in iteration 28 : 0.3874558232370014
Loss in iteration 29 : 0.3865216160750338
Loss in iteration 30 : 0.3889580588103577
Testing accuracy  of updater 1 on alg 1 with rate 2.7999999999999996E-5 = 0.77825, training accuracy 0.8264810618323082, time elapsed: 459 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9169308145482059
Loss in iteration 3 : 0.7601466564462986
Loss in iteration 4 : 0.6029799925248973
Loss in iteration 5 : 0.5541723650010673
Loss in iteration 6 : 0.5734544394849383
Loss in iteration 7 : 0.6050112880532392
Loss in iteration 8 : 0.6308671780120265
Loss in iteration 9 : 0.6475962685722142
Testing accuracy  of updater 1 on alg 1 with rate 6.999999999999994E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 143 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 5.578858959963702
Loss in iteration 3 : 5.727714979244242
Loss in iteration 4 : 4.767366591115413
Loss in iteration 5 : 2.8183538776274286
Loss in iteration 6 : 1.1784976578440318
Loss in iteration 7 : 1.3213116954037707
Loss in iteration 8 : 1.7177047598454556
Loss in iteration 9 : 1.7271292652130192
Loss in iteration 10 : 1.5699323190029775
Loss in iteration 11 : 1.4645315292241001
Loss in iteration 12 : 1.458682758298955
Loss in iteration 13 : 1.4862476264762148
Loss in iteration 14 : 1.5171203790670675
Loss in iteration 15 : 1.524860519239389
Loss in iteration 16 : 1.5095750357243714
Loss in iteration 17 : 1.4863810313900592
Loss in iteration 18 : 1.463276711668867
Loss in iteration 19 : 1.4393843721394899
Loss in iteration 20 : 1.4122893941046657
Loss in iteration 21 : 1.3814452221004156
Loss in iteration 22 : 1.3449353927158112
Loss in iteration 23 : 1.3038828639123894
Loss in iteration 24 : 1.2601088264138112
Loss in iteration 25 : 1.2149153469082097
Loss in iteration 26 : 1.1678417222901074
Loss in iteration 27 : 1.1195356749695706
Loss in iteration 28 : 1.069901681504244
Loss in iteration 29 : 1.0192214463289866
Loss in iteration 30 : 0.9675741450288841
Loss in iteration 31 : 0.9154812673813673
Loss in iteration 32 : 0.8633778731514361
Loss in iteration 33 : 0.8110623722452106
Loss in iteration 34 : 0.7590623675086359
Loss in iteration 35 : 0.7087012649994724
Loss in iteration 36 : 0.6607275465770165
Loss in iteration 37 : 0.6142261643502605
Loss in iteration 38 : 0.570380480163667
Loss in iteration 39 : 0.5311883388083015
Loss in iteration 40 : 0.49752004063584904
Loss in iteration 41 : 0.4729105662902123
Loss in iteration 42 : 0.4659321399756791
Loss in iteration 43 : 0.5750589892020423
Loss in iteration 44 : 1.7334183819778057
Loss in iteration 45 : 0.48801851958573017
Loss in iteration 46 : 2.1731502265517135
Loss in iteration 47 : 4.104544634223668
Loss in iteration 48 : 3.8259285239575838
Loss in iteration 49 : 2.481067701856106
Loss in iteration 50 : 0.8646000241497178
Loss in iteration 51 : 0.9895124942691409
Loss in iteration 52 : 1.1595556709662451
Loss in iteration 53 : 1.0236006275237277
Loss in iteration 54 : 0.9570698493134197
Loss in iteration 55 : 0.9731085691817898
Loss in iteration 56 : 0.995282547289177
Loss in iteration 57 : 1.003720166539786
Loss in iteration 58 : 0.9994913424633827
Loss in iteration 59 : 0.989395129088071
Loss in iteration 60 : 0.9751217163044859
Loss in iteration 61 : 0.9573294727435063
Loss in iteration 62 : 0.9342265335756046
Loss in iteration 63 : 0.9066448449560164
Loss in iteration 64 : 0.8753101807934285
Loss in iteration 65 : 0.840924339946963
Loss in iteration 66 : 0.8039042491629459
Loss in iteration 67 : 0.7644695760731484
Loss in iteration 68 : 0.7233866835315003
Loss in iteration 69 : 0.680992302078542
Loss in iteration 70 : 0.6378163505451718
Loss in iteration 71 : 0.5952899758062102
Loss in iteration 72 : 0.5537095020728519
Loss in iteration 73 : 0.5135992403212137
Loss in iteration 74 : 0.47672647411922403
Loss in iteration 75 : 0.44535675608257586
Loss in iteration 76 : 0.421783117891149
Loss in iteration 77 : 0.41524248347855747
Loss in iteration 78 : 0.5302317721504948
Loss in iteration 79 : 2.031868377383221
Loss in iteration 80 : 0.7767781940640609
Loss in iteration 81 : 3.9184553326312086
Loss in iteration 82 : 4.381237516556135
Loss in iteration 83 : 4.302222669791073
Loss in iteration 84 : 3.1371292293943704
Loss in iteration 85 : 1.438086384283718
Loss in iteration 86 : 1.0307218086472472
Loss in iteration 87 : 1.33325342065996
Loss in iteration 88 : 1.487417893094159
Loss in iteration 89 : 1.4184520804191167
Loss in iteration 90 : 1.332267505628717
Loss in iteration 91 : 1.3119776629086126
Loss in iteration 92 : 1.3333556834865639
Loss in iteration 93 : 1.3576011012610312
Loss in iteration 94 : 1.3661301118598226
Loss in iteration 95 : 1.3566614276870144
Loss in iteration 96 : 1.3370054469341686
Loss in iteration 97 : 1.3139358670801848
Loss in iteration 98 : 1.289738704460873
Loss in iteration 99 : 1.2631438322718616
Loss in iteration 100 : 1.233103462036021
Loss in iteration 101 : 1.1971983689820331
Loss in iteration 102 : 1.1576957374728782
Loss in iteration 103 : 1.1164742076221992
Loss in iteration 104 : 1.0737600859911554
Loss in iteration 105 : 1.0286943827394202
Loss in iteration 106 : 0.9821338455559662
Loss in iteration 107 : 0.9340577722251584
Loss in iteration 108 : 0.8853625472458968
Loss in iteration 109 : 0.8360004618242086
Loss in iteration 110 : 0.7861523149551448
Loss in iteration 111 : 0.736441987975219
Loss in iteration 112 : 0.6880830239773644
Loss in iteration 113 : 0.6417907826869766
Loss in iteration 114 : 0.5966918438577359
Loss in iteration 115 : 0.5542959669009864
Loss in iteration 116 : 0.5163402248055645
Loss in iteration 117 : 0.4838528854026215
Loss in iteration 118 : 0.45991331065978375
Loss in iteration 119 : 0.44933701359404826
Loss in iteration 120 : 0.46227317508379656
Loss in iteration 121 : 0.6181087330606482
Loss in iteration 122 : 3.5218412179092558
Loss in iteration 123 : 5.007088819621324
Loss in iteration 124 : 5.204023700420909
Loss in iteration 125 : 4.286946287659181
Loss in iteration 126 : 2.3826503374574353
Loss in iteration 127 : 0.9368443996916003
Loss in iteration 128 : 1.2657048349934978
Loss in iteration 129 : 1.5141799758162608
Loss in iteration 130 : 1.3578504137020562
Loss in iteration 131 : 1.2024209197000397
Loss in iteration 132 : 1.1718670713752894
Loss in iteration 133 : 1.2011155169437484
Loss in iteration 134 : 1.2285900567597348
Loss in iteration 135 : 1.2332072029975376
Loss in iteration 136 : 1.2203277311502514
Loss in iteration 137 : 1.201707198864865
Loss in iteration 138 : 1.1829061462960182
Loss in iteration 139 : 1.1632053782068756
Loss in iteration 140 : 1.1389467732004321
Loss in iteration 141 : 1.1087022799870485
Loss in iteration 142 : 1.0741741106039528
Loss in iteration 143 : 1.0372673926340263
Loss in iteration 144 : 0.9982225616003295
Loss in iteration 145 : 0.9570092478801437
Loss in iteration 146 : 0.9134299267282436
Loss in iteration 147 : 0.8686104991031907
Loss in iteration 148 : 0.8225995454479633
Loss in iteration 149 : 0.7757698990155725
Loss in iteration 150 : 0.7284199850092684
Loss in iteration 151 : 0.6815523076837522
Loss in iteration 152 : 0.6363480295769091
Loss in iteration 153 : 0.5922268712135785
Loss in iteration 154 : 0.5501365810154172
Loss in iteration 155 : 0.5118154056737112
Loss in iteration 156 : 0.4786322501605445
Loss in iteration 157 : 0.4532172978261102
Loss in iteration 158 : 0.44295072212038256
Loss in iteration 159 : 0.508207945603511
Loss in iteration 160 : 1.4525935201826785
Loss in iteration 161 : 0.6615081712982687
Loss in iteration 162 : 2.3520273478926823
Loss in iteration 163 : 1.1228344810463584
Loss in iteration 164 : 1.298276913994858
Loss in iteration 165 : 1.0964187882342222
Loss in iteration 166 : 0.5596766640777332
Loss in iteration 167 : 0.6119506420285867
Loss in iteration 168 : 0.6176971336067058
Loss in iteration 169 : 0.6327648549819392
Loss in iteration 170 : 0.6426452127106337
Loss in iteration 171 : 0.6454628589966437
Loss in iteration 172 : 0.6418893171169079
Loss in iteration 173 : 0.6325817489965622
Loss in iteration 174 : 0.6181957425107985
Loss in iteration 175 : 0.5992938344271633
Loss in iteration 176 : 0.5765853342722563
Loss in iteration 177 : 0.5509587869967632
Loss in iteration 178 : 0.5233939885059867
Loss in iteration 179 : 0.49477330642998024
Loss in iteration 180 : 0.4660056921281128
Loss in iteration 181 : 0.437918755281896
Loss in iteration 182 : 0.41351010070458827
Loss in iteration 183 : 0.39588733732316495
Loss in iteration 184 : 0.4153647640483799
Loss in iteration 185 : 1.3706929965398884
Loss in iteration 186 : 4.465723428979131
Loss in iteration 187 : 4.345499048244429
Loss in iteration 188 : 3.142978300101858
Loss in iteration 189 : 1.0783076324535075
Loss in iteration 190 : 1.0489308569039373
Loss in iteration 191 : 1.016590056796754
Loss in iteration 192 : 0.8094706234802138
Loss in iteration 193 : 0.7856814080969452
Loss in iteration 194 : 0.8069572349417613
Loss in iteration 195 : 0.8157238721488608
Loss in iteration 196 : 0.8124050397188661
Loss in iteration 197 : 0.803165379188806
Loss in iteration 198 : 0.7901035890998959
Loss in iteration 199 : 0.7718882975432013
Loss in iteration 200 : 0.7488035841821253
Testing accuracy  of updater 2 on alg 1 with rate 0.001 = 0.77925, training accuracy 0.8319844609906119, time elapsed: 2597 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.0109636546227
Loss in iteration 3 : 4.115162868119085
Loss in iteration 4 : 3.442918996428886
Loss in iteration 5 : 2.076297127869165
Loss in iteration 6 : 0.8816388107045461
Loss in iteration 7 : 1.0044929548296537
Loss in iteration 8 : 1.2846319534126185
Loss in iteration 9 : 1.27942233630499
Loss in iteration 10 : 1.1638551747137602
Loss in iteration 11 : 1.0855404800030348
Loss in iteration 12 : 1.0819737598057702
Loss in iteration 13 : 1.1044355332858322
Loss in iteration 14 : 1.1266360914744156
Loss in iteration 15 : 1.1305332644438681
Loss in iteration 16 : 1.1192528922961238
Loss in iteration 17 : 1.1042525692094083
Loss in iteration 18 : 1.0882984797844881
Loss in iteration 19 : 1.0718847506185716
Loss in iteration 20 : 1.0539784724994556
Loss in iteration 21 : 1.03279489965115
Loss in iteration 22 : 1.0072780595855209
Loss in iteration 23 : 0.9791059228193993
Loss in iteration 24 : 0.949655624167329
Loss in iteration 25 : 0.9190499822602285
Loss in iteration 26 : 0.8872995548058795
Loss in iteration 27 : 0.8543712349457485
Loss in iteration 28 : 0.8203912142278821
Loss in iteration 29 : 0.7856943388047682
Loss in iteration 30 : 0.7502980003060555
Loss in iteration 31 : 0.7146092422273322
Loss in iteration 32 : 0.6788968476117329
Loss in iteration 33 : 0.6433224061593976
Loss in iteration 34 : 0.6086970102140423
Loss in iteration 35 : 0.5750456976877951
Loss in iteration 36 : 0.5427830335054857
Loss in iteration 37 : 0.5118922966214686
Loss in iteration 38 : 0.4836483227760705
Loss in iteration 39 : 0.45881102407156416
Loss in iteration 40 : 0.43783475903046143
Loss in iteration 41 : 0.4229007643901942
Loss in iteration 42 : 0.41656360039853735
Loss in iteration 43 : 0.41992834746113755
Loss in iteration 44 : 0.4533287047017448
Loss in iteration 45 : 0.8198346993013731
Loss in iteration 46 : 1.3371287231551434
Loss in iteration 47 : 2.754617600616362
Loss in iteration 48 : 2.4448550318856306
Loss in iteration 49 : 1.4016292851989907
Loss in iteration 50 : 0.5532306505191052
Loss in iteration 51 : 0.8003789340699974
Loss in iteration 52 : 0.7186117709693906
Loss in iteration 53 : 0.66219614461996
Loss in iteration 54 : 0.6673764969154046
Loss in iteration 55 : 0.6842815210677242
Loss in iteration 56 : 0.6916761335646038
Loss in iteration 57 : 0.6912190555030627
Loss in iteration 58 : 0.6876473180452423
Loss in iteration 59 : 0.6810262201303363
Loss in iteration 60 : 0.6707819837184111
Loss in iteration 61 : 0.6571118502447775
Loss in iteration 62 : 0.640527907475794
Loss in iteration 63 : 0.6214308297971026
Loss in iteration 64 : 0.6002585388483113
Loss in iteration 65 : 0.5771952056428301
Loss in iteration 66 : 0.5530564845543368
Loss in iteration 67 : 0.5283688921899133
Loss in iteration 68 : 0.5035687939738713
Loss in iteration 69 : 0.47894407680089673
Loss in iteration 70 : 0.45501872084987793
Loss in iteration 71 : 0.4329054357770521
Loss in iteration 72 : 0.41376262288800647
Loss in iteration 73 : 0.3985899074853793
Loss in iteration 74 : 0.38991122021693986
Loss in iteration 75 : 0.39020151807336484
Loss in iteration 76 : 0.4022905686633869
Loss in iteration 77 : 0.529562465453742
Loss in iteration 78 : 2.3757315268175163
Loss in iteration 79 : 3.475324638579122
Loss in iteration 80 : 3.5741664105035578
Loss in iteration 81 : 2.8971008413986223
Loss in iteration 82 : 1.5523166840507174
Loss in iteration 83 : 0.7261002490544475
Loss in iteration 84 : 0.9871901067102543
Loss in iteration 85 : 1.1224664321166278
Loss in iteration 86 : 1.0138018187278257
Loss in iteration 87 : 0.9247000325912643
Loss in iteration 88 : 0.9094984357687615
Loss in iteration 89 : 0.9298708542111921
Loss in iteration 90 : 0.9478356485588841
Loss in iteration 91 : 0.9505311618535689
Loss in iteration 92 : 0.9407880595296577
Loss in iteration 93 : 0.9267381496597025
Loss in iteration 94 : 0.9123889493228184
Loss in iteration 95 : 0.897758712993116
Loss in iteration 96 : 0.8805817228767856
Loss in iteration 97 : 0.8596289542886818
Loss in iteration 98 : 0.8355376026311337
Loss in iteration 99 : 0.8095985238788244
Loss in iteration 100 : 0.7819366998531374
Loss in iteration 101 : 0.7529613733229747
Loss in iteration 102 : 0.7229147803183207
Loss in iteration 103 : 0.6916609108883034
Loss in iteration 104 : 0.6597809212778052
Loss in iteration 105 : 0.6273407847587171
Loss in iteration 106 : 0.5949740569290076
Loss in iteration 107 : 0.5636559872616682
Loss in iteration 108 : 0.5330516462751689
Loss in iteration 109 : 0.5033196651307394
Loss in iteration 110 : 0.4755929712275332
Loss in iteration 111 : 0.45106752882095424
Loss in iteration 112 : 0.43015625571775645
Loss in iteration 113 : 0.413990427874525
Loss in iteration 114 : 0.4077300682358186
Loss in iteration 115 : 0.4169100756224101
Loss in iteration 116 : 0.617628205912635
Loss in iteration 117 : 1.824957042867926
Loss in iteration 118 : 1.1068742232833366
Loss in iteration 119 : 0.6708239632138759
Loss in iteration 120 : 0.7500233671374898
Loss in iteration 121 : 0.6131282140839387
Loss in iteration 122 : 0.5051951495730023
Loss in iteration 123 : 0.46631889572779245
Loss in iteration 124 : 0.4636001985696388
Loss in iteration 125 : 0.468209831587438
Loss in iteration 126 : 0.4694925480208311
Loss in iteration 127 : 0.4676150409963897
Loss in iteration 128 : 0.4629760116605565
Loss in iteration 129 : 0.4559285328920118
Loss in iteration 130 : 0.44701955909692076
Loss in iteration 131 : 0.43656347552631797
Loss in iteration 132 : 0.42500207531615264
Loss in iteration 133 : 0.41306354258579037
Loss in iteration 134 : 0.4016656414631743
Loss in iteration 135 : 0.39149022687912394
Loss in iteration 136 : 0.38386424936316943
Loss in iteration 137 : 0.3792755180697648
Loss in iteration 138 : 0.38137896522736686
Loss in iteration 139 : 0.42116417411467016
Loss in iteration 140 : 1.113983557276793
Loss in iteration 141 : 2.7568811037527503
Loss in iteration 142 : 2.491481686650837
Loss in iteration 143 : 1.4865990474221775
Loss in iteration 144 : 0.5156354143362957
Loss in iteration 145 : 0.7669125590407393
Loss in iteration 146 : 0.6228633048023838
Loss in iteration 147 : 0.5911309729048584
Loss in iteration 148 : 0.602296749372433
Loss in iteration 149 : 0.6100733002554649
Loss in iteration 150 : 0.6102049792195432
Loss in iteration 151 : 0.6068775417163695
Loss in iteration 152 : 0.5998273459846674
Loss in iteration 153 : 0.5890908856652054
Loss in iteration 154 : 0.5751620286873546
Loss in iteration 155 : 0.5585881845503804
Loss in iteration 156 : 0.5398469665083822
Loss in iteration 157 : 0.5193087560562183
Loss in iteration 158 : 0.4977333423667276
Loss in iteration 159 : 0.4753585912002215
Loss in iteration 160 : 0.45322855600259715
Loss in iteration 161 : 0.4325118584641944
Loss in iteration 162 : 0.4132582867553814
Loss in iteration 163 : 0.3973294036970473
Loss in iteration 164 : 0.38619470832464775
Loss in iteration 165 : 0.38215087447224094
Loss in iteration 166 : 0.38781551235514483
Loss in iteration 167 : 0.44316245472394183
Loss in iteration 168 : 1.0320528623771812
Loss in iteration 169 : 0.6208097000595588
Loss in iteration 170 : 1.611681133524523
Loss in iteration 171 : 0.7319201465359362
Loss in iteration 172 : 1.1149290411651087
Loss in iteration 173 : 0.7988075124640154
Loss in iteration 174 : 0.4822494428151165
Loss in iteration 175 : 0.5132626484101441
Loss in iteration 176 : 0.5224823229572649
Loss in iteration 177 : 0.5340707414169491
Loss in iteration 178 : 0.5409866826198197
Loss in iteration 179 : 0.5431577854177109
Loss in iteration 180 : 0.5410267250568419
Testing accuracy  of updater 2 on alg 1 with rate 7.000000000000001E-4 = 0.78075, training accuracy 0.8316607316283587, time elapsed: 3032 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.44306834928169
Loss in iteration 3 : 2.502610756993913
Loss in iteration 4 : 2.118471401742374
Loss in iteration 5 : 1.3360168414567144
Loss in iteration 6 : 0.5974833706982885
Loss in iteration 7 : 0.6821541742052805
Loss in iteration 8 : 0.8399346220409438
Loss in iteration 9 : 0.8205482008665436
Loss in iteration 10 : 0.7507629694041499
Loss in iteration 11 : 0.7086073061696806
Loss in iteration 12 : 0.7109035025655547
Loss in iteration 13 : 0.7266192835205139
Loss in iteration 14 : 0.7381083956295353
Loss in iteration 15 : 0.7406188715156341
Loss in iteration 16 : 0.7356834937673126
Loss in iteration 17 : 0.7269293110898355
Loss in iteration 18 : 0.7175673061615256
Loss in iteration 19 : 0.7089637272404424
Loss in iteration 20 : 0.6997674918733833
Loss in iteration 21 : 0.6883364769383677
Loss in iteration 22 : 0.67477028136193
Loss in iteration 23 : 0.6596785621221847
Loss in iteration 24 : 0.6435148606890982
Loss in iteration 25 : 0.6272459705504038
Loss in iteration 26 : 0.6104260797707022
Loss in iteration 27 : 0.5930403383006723
Loss in iteration 28 : 0.574987956851317
Loss in iteration 29 : 0.5566043625494055
Loss in iteration 30 : 0.5380575496107248
Loss in iteration 31 : 0.5197045467309341
Loss in iteration 32 : 0.5014804563889008
Loss in iteration 33 : 0.4837498301747875
Loss in iteration 34 : 0.46666808884528355
Loss in iteration 35 : 0.45068754935388106
Loss in iteration 36 : 0.43548030163111584
Loss in iteration 37 : 0.4217568932279569
Loss in iteration 38 : 0.4097702339328271
Loss in iteration 39 : 0.3996095533490076
Loss in iteration 40 : 0.39185690204046375
Loss in iteration 41 : 0.38727944125186214
Loss in iteration 42 : 0.386366299583387
Loss in iteration 43 : 0.3880554177004722
Loss in iteration 44 : 0.39220206761568277
Loss in iteration 45 : 0.39682740690759694
Loss in iteration 46 : 0.3995923454416805
Loss in iteration 47 : 0.4019536053643864
Loss in iteration 48 : 0.41852683336501173
Loss in iteration 49 : 0.5072189530770633
Loss in iteration 50 : 0.6257382832355274
Loss in iteration 51 : 0.742446565696906
Loss in iteration 52 : 0.38597306647550406
Loss in iteration 53 : 0.40206122322809107
Loss in iteration 54 : 0.39496951615546966
Loss in iteration 55 : 0.3987379775838937
Loss in iteration 56 : 0.4014816187636496
Loss in iteration 57 : 0.40311706778262085
Loss in iteration 58 : 0.40363440242247245
Loss in iteration 59 : 0.4030881953998971
Testing accuracy  of updater 2 on alg 1 with rate 4.0E-4 = 0.78325, training accuracy 0.8378115895111686, time elapsed: 811 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8751730439406885
Loss in iteration 3 : 0.8900586458687437
Loss in iteration 4 : 0.7940371025880678
Loss in iteration 5 : 0.6014878677305182
Loss in iteration 6 : 0.4021887940029303
Loss in iteration 7 : 0.43158775972462476
Loss in iteration 8 : 0.44180374145060464
Loss in iteration 9 : 0.4158998599491235
Loss in iteration 10 : 0.39645163502769054
Loss in iteration 11 : 0.3933341360347758
Loss in iteration 12 : 0.3967137880715932
Loss in iteration 13 : 0.40004519052610543
Loss in iteration 14 : 0.40148618162098076
Loss in iteration 15 : 0.40196616137058655
Loss in iteration 16 : 0.40240251739433064
Loss in iteration 17 : 0.40302656552354615
Loss in iteration 18 : 0.40361205439965364
Testing accuracy  of updater 2 on alg 1 with rate 1.0E-4 = 0.77825, training accuracy 0.830042084817093, time elapsed: 205 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7185432341540061
Loss in iteration 3 : 0.7294257159278489
Loss in iteration 4 : 0.663317174656249
Loss in iteration 5 : 0.5371070841251006
Loss in iteration 6 : 0.43075942960665453
Loss in iteration 7 : 0.425181019397458
Loss in iteration 8 : 0.42755801175399694
Loss in iteration 9 : 0.40889987883834483
Loss in iteration 10 : 0.39398551094261586
Loss in iteration 11 : 0.3876458556597646
Loss in iteration 12 : 0.3873159154645382
Loss in iteration 13 : 0.38801276204925317
Loss in iteration 14 : 0.38842756805092676
Loss in iteration 15 : 0.38836868043645933
Loss in iteration 16 : 0.38825441998179505
Loss in iteration 17 : 0.38847847316995476
Loss in iteration 18 : 0.38898411781498676
Loss in iteration 19 : 0.38942884228281294
Loss in iteration 20 : 0.38970566499438036
Testing accuracy  of updater 2 on alg 1 with rate 7.000000000000001E-5 = 0.77875, training accuracy 0.8329556490773713, time elapsed: 247 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.571752001315296
Loss in iteration 3 : 0.5932429260980682
Loss in iteration 4 : 0.5749461952076702
Loss in iteration 5 : 0.5314582300659884
Loss in iteration 6 : 0.4923740732609385
Loss in iteration 7 : 0.4713398529851794
Loss in iteration 8 : 0.457670423187737
Loss in iteration 9 : 0.4446117279311575
Loss in iteration 10 : 0.4315825452053924
Loss in iteration 11 : 0.41921333609621736
Loss in iteration 12 : 0.40847084797271893
Loss in iteration 13 : 0.40040689698483045
Loss in iteration 14 : 0.39494361190146876
Loss in iteration 15 : 0.39153118513079055
Loss in iteration 16 : 0.38923161201537393
Loss in iteration 17 : 0.3877229158759711
Loss in iteration 18 : 0.38665038339806473
Loss in iteration 19 : 0.38577015833872885
Loss in iteration 20 : 0.3850887194882467
Loss in iteration 21 : 0.38458278743755003
Loss in iteration 22 : 0.38427504750491753
Testing accuracy  of updater 2 on alg 1 with rate 4.0E-5 = 0.77925, training accuracy 0.8319844609906119, time elapsed: 226 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7750278097674823
Loss in iteration 3 : 0.573824397402165
Loss in iteration 4 : 0.5571106822337761
Loss in iteration 5 : 0.5765763188831475
Loss in iteration 6 : 0.5918424393843938
Loss in iteration 7 : 0.5974290768765523
Testing accuracy  of updater 2 on alg 1 with rate 9.999999999999999E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 116 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 128.20362317072215
Loss in iteration 3 : 79.36239138588451
Loss in iteration 4 : 35.19230028310583
Loss in iteration 5 : 6.267415042072265
Loss in iteration 6 : 5.790864132542424
Loss in iteration 7 : 5.55942264992805
Loss in iteration 8 : 5.328245484769447
Loss in iteration 9 : 5.150848552817881
Loss in iteration 10 : 4.94079101370629
Loss in iteration 11 : 4.779588816644109
Loss in iteration 12 : 4.624266507123446
Loss in iteration 13 : 4.668502983688332
Loss in iteration 14 : 4.617824381256864
Loss in iteration 15 : 5.538748221003423
Loss in iteration 16 : 6.072861695700763
Loss in iteration 17 : 10.56697554074097
Loss in iteration 18 : 11.436491261114993
Loss in iteration 19 : 19.923188310228003
Loss in iteration 20 : 5.404598868734042
Loss in iteration 21 : 5.556256019659812
Loss in iteration 22 : 4.639938619509913
Loss in iteration 23 : 4.799265471215991
Loss in iteration 24 : 4.198109709806479
Loss in iteration 25 : 4.368312042843988
Loss in iteration 26 : 4.044001278691171
Loss in iteration 27 : 4.84480665118292
Loss in iteration 28 : 4.897834142656874
Loss in iteration 29 : 7.792862355183834
Loss in iteration 30 : 7.691169942189967
Loss in iteration 31 : 14.584477895661934
Loss in iteration 32 : 6.0640561041404775
Loss in iteration 33 : 7.689439040494489
Loss in iteration 34 : 4.664562399497344
Loss in iteration 35 : 5.22837219765338
Loss in iteration 36 : 4.232934945281025
Loss in iteration 37 : 4.945216197608756
Loss in iteration 38 : 4.263920639389626
Loss in iteration 39 : 5.408625062364744
Loss in iteration 40 : 4.577785821411318
Loss in iteration 41 : 6.657199907335999
Loss in iteration 42 : 5.2485780213074404
Loss in iteration 43 : 8.556221241184724
Loss in iteration 44 : 6.053041062699468
Loss in iteration 45 : 9.260988231608504
Loss in iteration 46 : 5.015251582946807
Loss in iteration 47 : 5.976517806682234
Loss in iteration 48 : 3.86938120036225
Loss in iteration 49 : 4.302834839460924
Loss in iteration 50 : 3.6225690148053444
Loss in iteration 51 : 4.309943685067762
Loss in iteration 52 : 3.795946037938766
Loss in iteration 53 : 5.1118957112530765
Loss in iteration 54 : 4.388304229736359
Loss in iteration 55 : 7.464052186726875
Loss in iteration 56 : 5.698979166356516
Loss in iteration 57 : 9.875466021764055
Loss in iteration 58 : 4.508971134224145
Loss in iteration 59 : 5.458303720773083
Loss in iteration 60 : 3.58820553993434
Loss in iteration 61 : 4.1425222800258
Loss in iteration 62 : 3.3680480891376443
Loss in iteration 63 : 4.155115688029625
Loss in iteration 64 : 3.593977791783281
Loss in iteration 65 : 5.292288888945211
Loss in iteration 66 : 4.289276528508519
Loss in iteration 67 : 7.521800349566275
Loss in iteration 68 : 4.927471966999309
Loss in iteration 69 : 8.039111771209418
Loss in iteration 70 : 4.186562886999429
Loss in iteration 71 : 5.129227579840979
Loss in iteration 72 : 3.378647058559419
Loss in iteration 73 : 3.970681276226714
Loss in iteration 74 : 3.1398713624409407
Loss in iteration 75 : 3.939260940381776
Loss in iteration 76 : 3.4795169430903132
Loss in iteration 77 : 5.140685130043926
Loss in iteration 78 : 4.101699766558297
Loss in iteration 79 : 7.152670652712438
Loss in iteration 80 : 4.523104961186781
Loss in iteration 81 : 7.236713915514523
Loss in iteration 82 : 3.832720746633911
Loss in iteration 83 : 4.728719106427595
Loss in iteration 84 : 3.1722999761344877
Loss in iteration 85 : 3.7638625394121243
Loss in iteration 86 : 3.04555597648878
Loss in iteration 87 : 3.9240309630468273
Loss in iteration 88 : 3.4320265660343994
Loss in iteration 89 : 5.273690236644233
Loss in iteration 90 : 3.83338112116041
Loss in iteration 91 : 6.56057061546938
Loss in iteration 92 : 3.9358093699528296
Loss in iteration 93 : 6.084999830263217
Loss in iteration 94 : 3.3957162804736765
Loss in iteration 95 : 4.332578361061513
Loss in iteration 96 : 3.0886755227332596
Loss in iteration 97 : 3.945924223516945
Loss in iteration 98 : 3.193342118297398
Loss in iteration 99 : 4.5459474540689415
Loss in iteration 100 : 3.4172615554055064
Loss in iteration 101 : 5.451003380780126
Loss in iteration 102 : 3.500158555613762
Loss in iteration 103 : 5.389348646805601
Loss in iteration 104 : 3.352453578106254
Loss in iteration 105 : 4.733118503577045
Loss in iteration 106 : 3.2064706530539526
Loss in iteration 107 : 4.4252084092702475
Loss in iteration 108 : 3.1627348167670606
Loss in iteration 109 : 4.528888598891125
Loss in iteration 110 : 3.224816563525289
Loss in iteration 111 : 4.804703916387005
Loss in iteration 112 : 3.26573834646701
Loss in iteration 113 : 4.969393837807047
Loss in iteration 114 : 3.2044075140561437
Loss in iteration 115 : 4.62531243064607
Loss in iteration 116 : 3.14341278767854
Loss in iteration 117 : 4.521019717717484
Loss in iteration 118 : 3.1126427813509
Loss in iteration 119 : 4.505547318289914
Loss in iteration 120 : 3.107256090066539
Loss in iteration 121 : 4.534602639058598
Loss in iteration 122 : 3.133906105575656
Loss in iteration 123 : 4.7730196880425115
Loss in iteration 124 : 3.100180498978417
Loss in iteration 125 : 4.546374668499028
Loss in iteration 126 : 3.0578240773910044
Loss in iteration 127 : 4.406987153443267
Loss in iteration 128 : 2.9766269776825256
Loss in iteration 129 : 4.280401814281749
Loss in iteration 130 : 2.9661937675032553
Loss in iteration 131 : 4.3253255539243325
Loss in iteration 132 : 2.9797869117052613
Loss in iteration 133 : 4.4548307176545325
Loss in iteration 134 : 3.041917636990287
Loss in iteration 135 : 4.653245127993105
Loss in iteration 136 : 2.9439564646851077
Loss in iteration 137 : 4.267105823550822
Loss in iteration 138 : 2.8846311567892404
Loss in iteration 139 : 4.1664864272443936
Loss in iteration 140 : 2.888666048394434
Loss in iteration 141 : 4.302170944683451
Loss in iteration 142 : 2.945627562713978
Loss in iteration 143 : 4.5013119465080695
Loss in iteration 144 : 2.909187839181602
Loss in iteration 145 : 4.3652308544580904
Loss in iteration 146 : 2.881633544568125
Loss in iteration 147 : 4.2419601974143095
Loss in iteration 148 : 2.7970639760494618
Loss in iteration 149 : 4.014006362928069
Loss in iteration 150 : 2.792588243155133
Loss in iteration 151 : 4.085683665989461
Loss in iteration 152 : 2.788879415569293
Loss in iteration 153 : 4.194192620574648
Loss in iteration 154 : 2.8550831052745513
Loss in iteration 155 : 4.376312397345946
Loss in iteration 156 : 2.796249463021149
Loss in iteration 157 : 4.165123688834896
Loss in iteration 158 : 2.7886331667693596
Loss in iteration 159 : 4.136600827733137
Loss in iteration 160 : 2.7409778843412154
Loss in iteration 161 : 3.9274791864993657
Loss in iteration 162 : 2.681080108851171
Loss in iteration 163 : 3.8851514028793184
Loss in iteration 164 : 2.70211893789589
Loss in iteration 165 : 4.058905107706892
Loss in iteration 166 : 2.7866538740096845
Loss in iteration 167 : 4.336047318594343
Loss in iteration 168 : 2.6745412781232507
Loss in iteration 169 : 3.8300143062619387
Loss in iteration 170 : 2.64525250022896
Loss in iteration 171 : 3.8327354709170214
Loss in iteration 172 : 2.662973988026472
Loss in iteration 173 : 4.045988078813613
Loss in iteration 174 : 2.7125802026073003
Loss in iteration 175 : 4.17811639976725
Loss in iteration 176 : 2.6466410733558954
Loss in iteration 177 : 3.879865336209804
Loss in iteration 178 : 2.5613770727783574
Loss in iteration 179 : 3.718387212796797
Loss in iteration 180 : 2.589263310785085
Loss in iteration 181 : 3.921013106285761
Loss in iteration 182 : 2.6629075316620483
Loss in iteration 183 : 4.11387351088204
Loss in iteration 184 : 2.6100295929904433
Loss in iteration 185 : 3.8528263227339417
Loss in iteration 186 : 2.5644509593989233
Loss in iteration 187 : 3.771151745461137
Loss in iteration 188 : 2.5595776725153097
Loss in iteration 189 : 3.806675690623581
Loss in iteration 190 : 2.578141308233062
Loss in iteration 191 : 3.907058740056264
Loss in iteration 192 : 2.5646675988946157
Loss in iteration 193 : 3.822358981981686
Loss in iteration 194 : 2.5486743371501674
Loss in iteration 195 : 3.763498579322626
Loss in iteration 196 : 2.4976120641964363
Loss in iteration 197 : 3.6018229250791536
Loss in iteration 198 : 2.474573328892943
Loss in iteration 199 : 3.64880591230329
Loss in iteration 200 : 2.51237819721508
Testing accuracy  of updater 3 on alg 1 with rate 2.8000000000000007 = 0.66275, training accuracy 0.7526707672385885, time elapsed: 3026 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 26.87968980155007
Loss in iteration 3 : 18.051741644005777
Loss in iteration 4 : 9.280562829297345
Loss in iteration 5 : 2.835348580555083
Loss in iteration 6 : 2.005418022439654
Loss in iteration 7 : 1.9335352477339274
Loss in iteration 8 : 1.8664693150138811
Loss in iteration 9 : 1.7997897198502535
Loss in iteration 10 : 1.7337907249441278
Loss in iteration 11 : 1.6691102531977842
Loss in iteration 12 : 1.6020103597171695
Loss in iteration 13 : 1.5379563302770203
Loss in iteration 14 : 1.4728442529945758
Loss in iteration 15 : 1.4110506697023468
Loss in iteration 16 : 1.3536292787506394
Loss in iteration 17 : 1.2946394746548457
Loss in iteration 18 : 1.244806404553605
Loss in iteration 19 : 1.2020183599557352
Loss in iteration 20 : 1.221926976250143
Loss in iteration 21 : 1.276454310463599
Loss in iteration 22 : 1.8138063872756596
Loss in iteration 23 : 2.326510118243539
Loss in iteration 24 : 5.843202151105579
Loss in iteration 25 : 1.409019523764566
Loss in iteration 26 : 1.7341011647912092
Loss in iteration 27 : 1.7313005308408664
Loss in iteration 28 : 3.009480576556463
Loss in iteration 29 : 2.4612952251580604
Loss in iteration 30 : 4.582818011982437
Loss in iteration 31 : 1.7342602154020732
Loss in iteration 32 : 2.0511589812989914
Loss in iteration 33 : 1.5525870382624807
Loss in iteration 34 : 1.842743981472036
Loss in iteration 35 : 1.5696483082429278
Loss in iteration 36 : 2.083545754115798
Loss in iteration 37 : 1.744547392652811
Loss in iteration 38 : 2.8298166535696723
Loss in iteration 39 : 2.100246251303471
Loss in iteration 40 : 3.5824515422711865
Loss in iteration 41 : 1.8847290777957646
Loss in iteration 42 : 2.5439156974729906
Loss in iteration 43 : 1.6157558219176036
Loss in iteration 44 : 1.9148524321402662
Loss in iteration 45 : 1.5369988371217937
Loss in iteration 46 : 1.9307919332245833
Loss in iteration 47 : 1.604412365801154
Loss in iteration 48 : 2.3747448917100495
Loss in iteration 49 : 1.8324606803671875
Loss in iteration 50 : 3.0315512350955216
Loss in iteration 51 : 1.9886140179967358
Loss in iteration 52 : 3.1384902025143897
Loss in iteration 53 : 1.7643492970876795
Loss in iteration 54 : 2.305854534515813
Loss in iteration 55 : 1.5436396387039155
Loss in iteration 56 : 1.8529530412046242
Loss in iteration 57 : 1.5257660454238744
Loss in iteration 58 : 2.0128176552236843
Loss in iteration 59 : 1.6393701126161724
Loss in iteration 60 : 2.539412366817351
Loss in iteration 61 : 1.8348806184621205
Loss in iteration 62 : 2.966235347684411
Loss in iteration 63 : 1.8517120193893315
Loss in iteration 64 : 2.68804098550544
Loss in iteration 65 : 1.6127694433397644
Loss in iteration 66 : 2.0177799074926734
Loss in iteration 67 : 1.507809788579974
Loss in iteration 68 : 1.907495112375779
Loss in iteration 69 : 1.557498568417913
Loss in iteration 70 : 2.303183735339358
Loss in iteration 71 : 1.7667405696353908
Loss in iteration 72 : 2.895868394281
Loss in iteration 73 : 1.8436120966137794
Loss in iteration 74 : 2.788076025690171
Loss in iteration 75 : 1.6357949564884466
Loss in iteration 76 : 2.0961373526298517
Loss in iteration 77 : 1.516628481703497
Loss in iteration 78 : 1.9328899007898683
Loss in iteration 79 : 1.5270471379054393
Loss in iteration 80 : 2.108110729559411
Loss in iteration 81 : 1.5867902402731218
Loss in iteration 82 : 2.4631987993503954
Loss in iteration 83 : 1.7151722423459486
Loss in iteration 84 : 2.721405171637157
Loss in iteration 85 : 1.6940048448312834
Loss in iteration 86 : 2.440964240053011
Loss in iteration 87 : 1.5719377551221112
Loss in iteration 88 : 2.090017980213945
Loss in iteration 89 : 1.5052043453432902
Loss in iteration 90 : 2.0124205587597
Loss in iteration 91 : 1.5404568381143886
Loss in iteration 92 : 2.2867408350573903
Loss in iteration 93 : 1.658190019371773
Loss in iteration 94 : 2.53582400341483
Loss in iteration 95 : 1.6288038297123841
Loss in iteration 96 : 2.404166818846058
Loss in iteration 97 : 1.567267113074087
Loss in iteration 98 : 2.157145750571218
Loss in iteration 99 : 1.4905752970762216
Loss in iteration 100 : 1.9835796634459593
Loss in iteration 101 : 1.5233552888062454
Loss in iteration 102 : 2.2520958963606623
Loss in iteration 103 : 1.6347735116895985
Loss in iteration 104 : 2.52548467901037
Loss in iteration 105 : 1.6143835278896448
Loss in iteration 106 : 2.3937339269374993
Loss in iteration 107 : 1.534616493981157
Loss in iteration 108 : 2.0907679349125665
Loss in iteration 109 : 1.4335563883121003
Loss in iteration 110 : 1.915368236108684
Loss in iteration 111 : 1.4714293173887645
Loss in iteration 112 : 2.1509390088483533
Loss in iteration 113 : 1.5992648342748632
Loss in iteration 114 : 2.5820414716526834
Loss in iteration 115 : 1.6110586561149813
Loss in iteration 116 : 2.4001039774872543
Loss in iteration 117 : 1.498601929806273
Loss in iteration 118 : 2.0364073018150592
Loss in iteration 119 : 1.4415990092936333
Loss in iteration 120 : 1.971561817004452
Loss in iteration 121 : 1.4787421266202019
Loss in iteration 122 : 2.1763999501726707
Loss in iteration 123 : 1.5653309568096263
Loss in iteration 124 : 2.4318682237671347
Loss in iteration 125 : 1.5614831660670563
Loss in iteration 126 : 2.3303044295354645
Loss in iteration 127 : 1.4886895288705788
Loss in iteration 128 : 2.038499812561539
Loss in iteration 129 : 1.4213248089304564
Loss in iteration 130 : 1.9558191722499099
Loss in iteration 131 : 1.454986945045023
Loss in iteration 132 : 2.132944803103487
Loss in iteration 133 : 1.540083694425053
Loss in iteration 134 : 2.4228770842275327
Loss in iteration 135 : 1.520564688066056
Loss in iteration 136 : 2.2451219218561183
Loss in iteration 137 : 1.4831866512526126
Loss in iteration 138 : 2.0881238817966237
Loss in iteration 139 : 1.438056504619444
Loss in iteration 140 : 1.9950950630259852
Loss in iteration 141 : 1.4303929393101609
Loss in iteration 142 : 2.0397944301041977
Loss in iteration 143 : 1.4840487256943173
Loss in iteration 144 : 2.2359039407089023
Loss in iteration 145 : 1.4965266020671986
Loss in iteration 146 : 2.213026909153701
Loss in iteration 147 : 1.4717035968021879
Loss in iteration 148 : 2.1022888168092435
Loss in iteration 149 : 1.4601522983142536
Loss in iteration 150 : 2.0818573906950184
Loss in iteration 151 : 1.4538365919801137
Loss in iteration 152 : 2.0773731225917893
Loss in iteration 153 : 1.4485967804534385
Loss in iteration 154 : 2.064403339370098
Loss in iteration 155 : 1.4547015951712072
Loss in iteration 156 : 2.1233068810117612
Loss in iteration 157 : 1.4610091927710538
Loss in iteration 158 : 2.117941567291121
Loss in iteration 159 : 1.451735350319074
Loss in iteration 160 : 2.0673451323821386
Loss in iteration 161 : 1.4252200240486554
Loss in iteration 162 : 1.9996387154824495
Loss in iteration 163 : 1.417964971415261
Loss in iteration 164 : 2.0048600663084652
Loss in iteration 165 : 1.4323749308793168
Loss in iteration 166 : 2.1237662025806867
Loss in iteration 167 : 1.4461033589705192
Loss in iteration 168 : 2.112842594385671
Loss in iteration 169 : 1.4250283701695439
Loss in iteration 170 : 2.0331690584494306
Loss in iteration 171 : 1.4219118689252588
Loss in iteration 172 : 2.045157059737091
Loss in iteration 173 : 1.420386073814267
Loss in iteration 174 : 2.0394088438916143
Loss in iteration 175 : 1.4188886482524345
Loss in iteration 176 : 2.033685325423548
Loss in iteration 177 : 1.4174385607426876
Loss in iteration 178 : 2.055215501816596
Loss in iteration 179 : 1.4238999056735648
Loss in iteration 180 : 2.0607024948670642
Loss in iteration 181 : 1.4160141828216486
Loss in iteration 182 : 2.0309514395445003
Loss in iteration 183 : 1.3956538721027425
Loss in iteration 184 : 1.9643711177851502
Loss in iteration 185 : 1.380650515796634
Loss in iteration 186 : 1.9586541147350098
Loss in iteration 187 : 1.3820862999381187
Loss in iteration 188 : 1.995622812875464
Loss in iteration 189 : 1.41084530883523
Loss in iteration 190 : 2.0686815548972395
Loss in iteration 191 : 1.4106535848520367
Loss in iteration 192 : 2.046183967249251
Loss in iteration 193 : 1.401629080324772
Loss in iteration 194 : 2.0199466401571113
Loss in iteration 195 : 1.3723122521964541
Loss in iteration 196 : 1.9342400836652218
Loss in iteration 197 : 1.369019480073783
Loss in iteration 198 : 1.9701945883317449
Loss in iteration 199 : 1.3856080277041607
Loss in iteration 200 : 2.0125539123189617
Testing accuracy  of updater 3 on alg 1 with rate 1.9600000000000002 = 0.78575, training accuracy 0.7818064098413726, time elapsed: 2775 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 11.421015795075178
Loss in iteration 3 : 7.806369072125068
Loss in iteration 4 : 4.203449302674665
Loss in iteration 5 : 1.3888077567624646
Loss in iteration 6 : 0.9612163143999038
Loss in iteration 7 : 0.9331840123997076
Loss in iteration 8 : 0.9053451527364279
Loss in iteration 9 : 0.8777362319851905
Loss in iteration 10 : 0.8505055003792626
Loss in iteration 11 : 0.823486097624908
Loss in iteration 12 : 0.7965245094199165
Loss in iteration 13 : 0.7696837692447086
Loss in iteration 14 : 0.7435363823632147
Loss in iteration 15 : 0.7174865394331641
Loss in iteration 16 : 0.6915712944267073
Loss in iteration 17 : 0.6664637824861894
Loss in iteration 18 : 0.6420505442972878
Loss in iteration 19 : 0.6180854064536248
Loss in iteration 20 : 0.5953857488883669
Loss in iteration 21 : 0.577291680348741
Loss in iteration 22 : 0.5710176923109069
Loss in iteration 23 : 0.6315363693843631
Loss in iteration 24 : 1.06005011246589
Loss in iteration 25 : 1.9767973626451594
Loss in iteration 26 : 4.786941683967801
Loss in iteration 27 : 1.4019680835890123
Loss in iteration 28 : 0.8618772208758989
Loss in iteration 29 : 0.9182499572439191
Loss in iteration 30 : 0.8068746205621701
Loss in iteration 31 : 0.889366275408724
Loss in iteration 32 : 0.8282964229352859
Loss in iteration 33 : 1.0334977384605735
Loss in iteration 34 : 0.918867785469809
Loss in iteration 35 : 1.3771501749057637
Loss in iteration 36 : 1.0214070124378303
Loss in iteration 37 : 1.5094581468354666
Loss in iteration 38 : 0.9421581511376923
Loss in iteration 39 : 1.217611977515998
Loss in iteration 40 : 0.8763560246548014
Loss in iteration 41 : 1.0702488892790605
Loss in iteration 42 : 0.852893496227198
Loss in iteration 43 : 1.0582724362127798
Loss in iteration 44 : 0.8687528568492339
Loss in iteration 45 : 1.1720195736139432
Loss in iteration 46 : 0.9240921880409108
Loss in iteration 47 : 1.291902418687883
Loss in iteration 48 : 0.9332822532804618
Loss in iteration 49 : 1.265032358653613
Loss in iteration 50 : 0.8924515806266051
Loss in iteration 51 : 1.171341442687318
Loss in iteration 52 : 0.8774104288093895
Loss in iteration 53 : 1.1407391958309105
Loss in iteration 54 : 0.8777940136613194
Loss in iteration 55 : 1.1647257895815917
Loss in iteration 56 : 0.8956410124016558
Loss in iteration 57 : 1.216675142099242
Loss in iteration 58 : 0.879805745616316
Loss in iteration 59 : 1.162020740035978
Loss in iteration 60 : 0.8766485257620114
Loss in iteration 61 : 1.164752884122676
Loss in iteration 62 : 0.8862773502506486
Loss in iteration 63 : 1.1889230991233333
Loss in iteration 64 : 0.8785080793164183
Loss in iteration 65 : 1.1529058411852797
Loss in iteration 66 : 0.8719162334325319
Loss in iteration 67 : 1.1500535500198907
Loss in iteration 68 : 0.8710672019670117
Loss in iteration 69 : 1.153341643357262
Loss in iteration 70 : 0.8765958018508431
Loss in iteration 71 : 1.1712361558102566
Loss in iteration 72 : 0.8811840195396833
Loss in iteration 73 : 1.17149035324219
Loss in iteration 74 : 0.8772141562511597
Loss in iteration 75 : 1.1631953659064096
Loss in iteration 76 : 0.8733408487805545
Loss in iteration 77 : 1.1634812184567858
Loss in iteration 78 : 0.8695028460625575
Loss in iteration 79 : 1.1368734149626538
Loss in iteration 80 : 0.8545109638876452
Loss in iteration 81 : 1.124966941203672
Loss in iteration 82 : 0.8590082436951336
Loss in iteration 83 : 1.152096366707965
Loss in iteration 84 : 0.8652370433354237
Loss in iteration 85 : 1.1563308989341377
Loss in iteration 86 : 0.8630828842024766
Loss in iteration 87 : 1.1540863594309243
Loss in iteration 88 : 0.8543800712515995
Loss in iteration 89 : 1.1261160429573822
Loss in iteration 90 : 0.85709675622977
Loss in iteration 91 : 1.141834560017298
Loss in iteration 92 : 0.8497474352570319
Loss in iteration 93 : 1.127730197422865
Loss in iteration 94 : 0.8514990758876231
Loss in iteration 95 : 1.138830250674985
Loss in iteration 96 : 0.8549705854161566
Loss in iteration 97 : 1.143991894449729
Loss in iteration 98 : 0.854638932307116
Loss in iteration 99 : 1.137031461066234
Loss in iteration 100 : 0.8449948262724976
Loss in iteration 101 : 1.1361289100106835
Loss in iteration 102 : 0.8414761121660892
Loss in iteration 103 : 1.1148103345223805
Loss in iteration 104 : 0.8438970628131279
Loss in iteration 105 : 1.1296154166101335
Loss in iteration 106 : 0.8447390020064487
Loss in iteration 107 : 1.125789278729662
Loss in iteration 108 : 0.8427766303942845
Loss in iteration 109 : 1.1203342517645558
Loss in iteration 110 : 0.843049160971675
Loss in iteration 111 : 1.125547543307731
Loss in iteration 112 : 0.8398564860643544
Loss in iteration 113 : 1.1136905561475126
Loss in iteration 114 : 0.8420138636322393
Loss in iteration 115 : 1.1197660350765055
Loss in iteration 116 : 0.8376249325011136
Loss in iteration 117 : 1.1141219154336075
Loss in iteration 118 : 0.8385341826739449
Loss in iteration 119 : 1.1150799194416021
Loss in iteration 120 : 0.8347557543981976
Loss in iteration 121 : 1.1095072476171055
Loss in iteration 122 : 0.8365191248396188
Loss in iteration 123 : 1.1127104802518992
Loss in iteration 124 : 0.8315671085943219
Loss in iteration 125 : 1.102424483947622
Loss in iteration 126 : 0.8358039550700602
Loss in iteration 127 : 1.110453462136152
Loss in iteration 128 : 0.8303595138727526
Loss in iteration 129 : 1.1042693638096193
Loss in iteration 130 : 0.8338354126481059
Loss in iteration 131 : 1.107672979749337
Loss in iteration 132 : 0.8277492352918014
Loss in iteration 133 : 1.0948976691057957
Loss in iteration 134 : 0.8287648421117308
Loss in iteration 135 : 1.096527243896659
Loss in iteration 136 : 0.8280756903177479
Loss in iteration 137 : 1.0931548263459157
Loss in iteration 138 : 0.8253042229424569
Loss in iteration 139 : 1.0895862440099644
Loss in iteration 140 : 0.8285519394664722
Loss in iteration 141 : 1.096865257420135
Loss in iteration 142 : 0.8245132396824222
Loss in iteration 143 : 1.0907848909113769
Loss in iteration 144 : 0.8222431179688723
Loss in iteration 145 : 1.091453411143764
Loss in iteration 146 : 0.8207363963943405
Loss in iteration 147 : 1.0880882537857637
Loss in iteration 148 : 0.8168739851981666
Loss in iteration 149 : 1.074280138927748
Loss in iteration 150 : 0.812668174831405
Loss in iteration 151 : 1.0664991945834752
Loss in iteration 152 : 0.8129981196792836
Loss in iteration 153 : 1.0760163705670205
Loss in iteration 154 : 0.8229077493174218
Loss in iteration 155 : 1.10396764592799
Loss in iteration 156 : 0.8165574061406236
Loss in iteration 157 : 1.0733001028289704
Loss in iteration 158 : 0.8153084011007177
Loss in iteration 159 : 1.0726411262451407
Loss in iteration 160 : 0.8136292422160871
Loss in iteration 161 : 1.0676771563701022
Loss in iteration 162 : 0.8119110582626707
Loss in iteration 163 : 1.067085791120516
Loss in iteration 164 : 0.8146005210340014
Loss in iteration 165 : 1.0939423065706113
Loss in iteration 166 : 0.8147397791745756
Loss in iteration 167 : 1.073566319793656
Loss in iteration 168 : 0.8035925738125924
Loss in iteration 169 : 1.0381356792678573
Loss in iteration 170 : 0.7939361495637844
Loss in iteration 171 : 1.0200355023543206
Loss in iteration 172 : 0.8013727803082884
Loss in iteration 173 : 1.079506356836464
Loss in iteration 174 : 0.8206960287373628
Loss in iteration 175 : 1.1165944245304458
Loss in iteration 176 : 0.7983423884827234
Loss in iteration 177 : 1.0344346052600235
Loss in iteration 178 : 0.7939905648473048
Loss in iteration 179 : 1.030352217198607
Loss in iteration 180 : 0.7983642650279721
Loss in iteration 181 : 1.0731177076473843
Loss in iteration 182 : 0.8149931425561696
Loss in iteration 183 : 1.1033387496876386
Loss in iteration 184 : 0.7961994105258666
Loss in iteration 185 : 1.0266894425958073
Loss in iteration 186 : 0.78784948931862
Loss in iteration 187 : 1.0067840789153357
Loss in iteration 188 : 0.7961904840921535
Loss in iteration 189 : 1.063732506172957
Loss in iteration 190 : 0.8129665861555465
Loss in iteration 191 : 1.106772254467463
Loss in iteration 192 : 0.78697988433834
Loss in iteration 193 : 0.9986066242942201
Loss in iteration 194 : 0.7911299909641131
Loss in iteration 195 : 1.0439735107619206
Loss in iteration 196 : 0.8045589340575598
Loss in iteration 197 : 1.077916467046846
Loss in iteration 198 : 0.7960406123115245
Loss in iteration 199 : 1.0421780521033346
Loss in iteration 200 : 0.7886518901989206
Testing accuracy  of updater 3 on alg 1 with rate 1.12 = 0.69575, training accuracy 0.7779216574943347, time elapsed: 2665 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.635594206813042
Loss in iteration 3 : 1.893933126084225
Loss in iteration 4 : 1.1536780670467952
Loss in iteration 5 : 0.49184487945479943
Loss in iteration 6 : 0.41873700136506836
Loss in iteration 7 : 0.403648003737787
Loss in iteration 8 : 0.3986881295242488
Loss in iteration 9 : 0.39427555689273597
Loss in iteration 10 : 0.39211629304799805
Loss in iteration 11 : 0.38943349339626715
Loss in iteration 12 : 0.3875054121293123
Loss in iteration 13 : 0.3854941387449545
Loss in iteration 14 : 0.3836048501827098
Loss in iteration 15 : 0.38196471177413854
Loss in iteration 16 : 0.380716164451908
Loss in iteration 17 : 0.37929145102498335
Loss in iteration 18 : 0.37815779904794933
Loss in iteration 19 : 0.3770734737326043
Loss in iteration 20 : 0.376092197077515
Loss in iteration 21 : 0.3751933850638304
Loss in iteration 22 : 0.37438793010637517
Loss in iteration 23 : 0.3737159507241344
Loss in iteration 24 : 0.37307401877918944
Loss in iteration 25 : 0.37255200295674806
Loss in iteration 26 : 0.3723928739817342
Loss in iteration 27 : 0.3732321617044123
Loss in iteration 28 : 0.37610952012778603
Loss in iteration 29 : 0.38407669847523973
Loss in iteration 30 : 0.39524635856174734
Loss in iteration 31 : 0.43741657484581364
Loss in iteration 32 : 0.49006113151789205
Loss in iteration 33 : 0.6094019903251738
Loss in iteration 34 : 0.4706864471767587
Loss in iteration 35 : 0.5062386289755821
Loss in iteration 36 : 0.4433509118688491
Loss in iteration 37 : 0.46047253227209345
Loss in iteration 38 : 0.42534861942099883
Loss in iteration 39 : 0.4317904029917831
Loss in iteration 40 : 0.4191330289387602
Loss in iteration 41 : 0.4282526751841595
Loss in iteration 42 : 0.416106502754987
Loss in iteration 43 : 0.4242281017191394
Loss in iteration 44 : 0.41420522716106367
Loss in iteration 45 : 0.4227013350338474
Loss in iteration 46 : 0.41417659198097345
Loss in iteration 47 : 0.4233379447432691
Loss in iteration 48 : 0.4152542150192238
Loss in iteration 49 : 0.42622203657275737
Loss in iteration 50 : 0.41825645247241383
Loss in iteration 51 : 0.42750910820875926
Loss in iteration 52 : 0.41855261987276776
Loss in iteration 53 : 0.4292101466356272
Loss in iteration 54 : 0.41872941667421304
Loss in iteration 55 : 0.42976122739575445
Loss in iteration 56 : 0.41804949254128854
Loss in iteration 57 : 0.42815642188827235
Loss in iteration 58 : 0.4179656272828303
Loss in iteration 59 : 0.4286175586624432
Loss in iteration 60 : 0.41832230222111705
Loss in iteration 61 : 0.42782933039292725
Loss in iteration 62 : 0.4179095873991823
Loss in iteration 63 : 0.4281914170927768
Loss in iteration 64 : 0.41764086057548994
Loss in iteration 65 : 0.4280382464741167
Loss in iteration 66 : 0.41707206291555216
Loss in iteration 67 : 0.4265740841920256
Loss in iteration 68 : 0.41558825458385845
Loss in iteration 69 : 0.42289230709983233
Loss in iteration 70 : 0.412743567921591
Loss in iteration 71 : 0.42204526886852345
Loss in iteration 72 : 0.4127690271457531
Loss in iteration 73 : 0.4224047812454048
Loss in iteration 74 : 0.4132328274575792
Loss in iteration 75 : 0.42387470530735893
Loss in iteration 76 : 0.41412936552144575
Loss in iteration 77 : 0.42382374227800845
Loss in iteration 78 : 0.4150048679085402
Loss in iteration 79 : 0.42538657883623854
Loss in iteration 80 : 0.41543168345642173
Loss in iteration 81 : 0.4259448827184622
Loss in iteration 82 : 0.4150931557259054
Loss in iteration 83 : 0.42520400599511826
Loss in iteration 84 : 0.41503860876778786
Loss in iteration 85 : 0.425181288644736
Loss in iteration 86 : 0.41469648709399964
Loss in iteration 87 : 0.4246947287358066
Loss in iteration 88 : 0.4145047170731486
Loss in iteration 89 : 0.42482698761442206
Loss in iteration 90 : 0.4140182472499172
Loss in iteration 91 : 0.42378230588999083
Loss in iteration 92 : 0.41255347711940843
Loss in iteration 93 : 0.4220260997727438
Loss in iteration 94 : 0.41167801197921994
Loss in iteration 95 : 0.4225636898079208
Loss in iteration 96 : 0.41175658264515724
Loss in iteration 97 : 0.42243809730178095
Loss in iteration 98 : 0.41154998889232447
Loss in iteration 99 : 0.42151093228546094
Loss in iteration 100 : 0.41096684831132146
Loss in iteration 101 : 0.42078138141401045
Loss in iteration 102 : 0.4109194566742717
Loss in iteration 103 : 0.4207512784052363
Loss in iteration 104 : 0.4107232657519913
Loss in iteration 105 : 0.4212726781000841
Loss in iteration 106 : 0.4106441581937399
Loss in iteration 107 : 0.4210430571143606
Loss in iteration 108 : 0.4094815665728443
Loss in iteration 109 : 0.41952357832545817
Loss in iteration 110 : 0.40792130154763134
Loss in iteration 111 : 0.41646334655024725
Loss in iteration 112 : 0.407424622365969
Loss in iteration 113 : 0.4163260396637384
Loss in iteration 114 : 0.4077487545617945
Loss in iteration 115 : 0.4177166417897917
Loss in iteration 116 : 0.40761930153863574
Loss in iteration 117 : 0.4189232706506398
Loss in iteration 118 : 0.407902559567732
Loss in iteration 119 : 0.4205800129047326
Loss in iteration 120 : 0.408812338813099
Loss in iteration 121 : 0.42168337508026477
Loss in iteration 122 : 0.4079232719725712
Loss in iteration 123 : 0.4193525928604253
Loss in iteration 124 : 0.40771943842843
Loss in iteration 125 : 0.41877046821300495
Loss in iteration 126 : 0.4071590286677561
Loss in iteration 127 : 0.4178062181145443
Loss in iteration 128 : 0.406450087389138
Loss in iteration 129 : 0.41590054538964716
Loss in iteration 130 : 0.4072531055613003
Loss in iteration 131 : 0.41957312549880743
Loss in iteration 132 : 0.40790968254166243
Loss in iteration 133 : 0.42058353392885645
Loss in iteration 134 : 0.40741032333449745
Loss in iteration 135 : 0.4189185311679309
Loss in iteration 136 : 0.40642330496657486
Loss in iteration 137 : 0.4161477030131178
Loss in iteration 138 : 0.4054052921626182
Loss in iteration 139 : 0.41203969331463275
Loss in iteration 140 : 0.4058224631272286
Loss in iteration 141 : 0.41712100818305714
Loss in iteration 142 : 0.40683857955849784
Loss in iteration 143 : 0.4177593813887993
Loss in iteration 144 : 0.40598033079665613
Loss in iteration 145 : 0.4165960783621318
Loss in iteration 146 : 0.40656665141810766
Loss in iteration 147 : 0.41792735539913345
Loss in iteration 148 : 0.40539054819314346
Loss in iteration 149 : 0.4133528035971688
Loss in iteration 150 : 0.40572223827345777
Loss in iteration 151 : 0.41730323463812047
Loss in iteration 152 : 0.4060125439071641
Loss in iteration 153 : 0.41766103560205947
Loss in iteration 154 : 0.4054524797347709
Loss in iteration 155 : 0.41416255242583017
Loss in iteration 156 : 0.4055829475413565
Loss in iteration 157 : 0.4155350325423532
Loss in iteration 158 : 0.4059224822110308
Loss in iteration 159 : 0.41678943545591385
Loss in iteration 160 : 0.40527333342375416
Loss in iteration 161 : 0.4141890527742349
Loss in iteration 162 : 0.4053962724880753
Loss in iteration 163 : 0.4150769352079597
Loss in iteration 164 : 0.40519460658286144
Loss in iteration 165 : 0.4144715597086499
Loss in iteration 166 : 0.4051287235208341
Loss in iteration 167 : 0.41434946169564607
Loss in iteration 168 : 0.40474887545012767
Loss in iteration 169 : 0.4135954579212517
Loss in iteration 170 : 0.40444698715487426
Loss in iteration 171 : 0.4135115675447725
Loss in iteration 172 : 0.40384935916624326
Loss in iteration 173 : 0.4109775742178495
Loss in iteration 174 : 0.4040981897003353
Loss in iteration 175 : 0.4126916978595033
Loss in iteration 176 : 0.40395259971294883
Loss in iteration 177 : 0.4119067636783545
Loss in iteration 178 : 0.403782654730345
Loss in iteration 179 : 0.4124731935659433
Loss in iteration 180 : 0.40385186200470574
Loss in iteration 181 : 0.4127024081659169
Loss in iteration 182 : 0.4040127351762943
Loss in iteration 183 : 0.41353222971129094
Loss in iteration 184 : 0.40303635428597034
Loss in iteration 185 : 0.4106426294333183
Loss in iteration 186 : 0.4033357578404418
Loss in iteration 187 : 0.4123649161854705
Loss in iteration 188 : 0.4038899485001971
Loss in iteration 189 : 0.4126050067500722
Loss in iteration 190 : 0.4034953409127628
Loss in iteration 191 : 0.4123704467244117
Loss in iteration 192 : 0.40357643960848694
Loss in iteration 193 : 0.41213654764386826
Loss in iteration 194 : 0.4032952208431266
Loss in iteration 195 : 0.4114860247545407
Loss in iteration 196 : 0.40302735649709265
Loss in iteration 197 : 0.4114132698233002
Loss in iteration 198 : 0.402638442931853
Loss in iteration 199 : 0.41014600693700826
Loss in iteration 200 : 0.4020348994034634
Testing accuracy  of updater 3 on alg 1 with rate 0.28 = 0.75975, training accuracy 0.8268047911945613, time elapsed: 3472 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.809766914307169
Loss in iteration 3 : 1.3342473440574412
Loss in iteration 4 : 0.8594878905334127
Loss in iteration 5 : 0.42712386144768505
Loss in iteration 6 : 0.39946148578671076
Loss in iteration 7 : 0.39231870304457234
Loss in iteration 8 : 0.382117553928081
Loss in iteration 9 : 0.37990384724955006
Loss in iteration 10 : 0.3767360065167098
Loss in iteration 11 : 0.37568400946042224
Loss in iteration 12 : 0.37485438815228256
Loss in iteration 13 : 0.3743224330653397
Loss in iteration 14 : 0.37393552566589594
Loss in iteration 15 : 0.3736480344418504
Loss in iteration 16 : 0.37339439094088345
Loss in iteration 17 : 0.3731743398830927
Loss in iteration 18 : 0.372961386524755
Loss in iteration 19 : 0.37276448029537745
Loss in iteration 20 : 0.372579622671469
Loss in iteration 21 : 0.3723980087358806
Loss in iteration 22 : 0.37222101963969256
Loss in iteration 23 : 0.37206323892776777
Loss in iteration 24 : 0.3719261323579438
Loss in iteration 25 : 0.3717960140339835
Loss in iteration 26 : 0.37167317676908657
Loss in iteration 27 : 0.37154973818575937
Loss in iteration 28 : 0.3714305396986016
Loss in iteration 29 : 0.37131354084369633
Loss in iteration 30 : 0.3711992608795597
Loss in iteration 31 : 0.37108495436331435
Loss in iteration 32 : 0.3709728151783667
Loss in iteration 33 : 0.37086250686675937
Loss in iteration 34 : 0.3707596883987966
Loss in iteration 35 : 0.37066076531168796
Loss in iteration 36 : 0.3705599325153158
Loss in iteration 37 : 0.3704648669771301
Loss in iteration 38 : 0.3703669464410399
Loss in iteration 39 : 0.37026938064064263
Loss in iteration 40 : 0.3701767878077501
Loss in iteration 41 : 0.37008810374027346
Loss in iteration 42 : 0.36999513524164074
Loss in iteration 43 : 0.3699072123942287
Loss in iteration 44 : 0.36981939582985573
Loss in iteration 45 : 0.369732821788635
Loss in iteration 46 : 0.3696481072558222
Loss in iteration 47 : 0.36956261953372443
Loss in iteration 48 : 0.369478722425018
Loss in iteration 49 : 0.3693971169379908
Loss in iteration 50 : 0.3693174958406562
Loss in iteration 51 : 0.3692356380639543
Loss in iteration 52 : 0.36915826934012086
Loss in iteration 53 : 0.36908669340839995
Loss in iteration 54 : 0.369008448788123
Loss in iteration 55 : 0.36894068680152325
Loss in iteration 56 : 0.3688706739713166
Loss in iteration 57 : 0.3688098649253118
Loss in iteration 58 : 0.36875192340246743
Loss in iteration 59 : 0.3687216906633423
Loss in iteration 60 : 0.36868762870471355
Loss in iteration 61 : 0.36874404413980977
Loss in iteration 62 : 0.3688330869069781
Loss in iteration 63 : 0.3688148128796175
Loss in iteration 64 : 0.3686748765905167
Loss in iteration 65 : 0.36871301639745885
Loss in iteration 66 : 0.3685177656400252
Loss in iteration 67 : 0.36852968547761794
Loss in iteration 68 : 0.3683958795394812
Loss in iteration 69 : 0.36839658965587313
Loss in iteration 70 : 0.3682459370937881
Loss in iteration 71 : 0.36826467504851346
Loss in iteration 72 : 0.3681421233240525
Loss in iteration 73 : 0.36815570370210193
Loss in iteration 74 : 0.3680046129827665
Loss in iteration 75 : 0.3680513872286946
Loss in iteration 76 : 0.36812570537188516
Loss in iteration 77 : 0.3681587183296082
Loss in iteration 78 : 0.3680530497275012
Loss in iteration 79 : 0.36804802767871714
Loss in iteration 80 : 0.36798857897558307
Loss in iteration 81 : 0.36795043504024594
Loss in iteration 82 : 0.3680002512275657
Loss in iteration 83 : 0.3677244628479865
Loss in iteration 84 : 0.3675529723497748
Loss in iteration 85 : 0.3675725788981868
Loss in iteration 86 : 0.36740478235278373
Loss in iteration 87 : 0.36750911377082845
Loss in iteration 88 : 0.3673546687228547
Loss in iteration 89 : 0.3673938354156571
Loss in iteration 90 : 0.367308914150402
Loss in iteration 91 : 0.3673380832842837
Loss in iteration 92 : 0.36748730069726465
Loss in iteration 93 : 0.3673751253206053
Loss in iteration 94 : 0.3674759859694795
Loss in iteration 95 : 0.3674231307029705
Loss in iteration 96 : 0.3674118471925212
Loss in iteration 97 : 0.36730794070010814
Loss in iteration 98 : 0.3673505260621507
Loss in iteration 99 : 0.3673349478526975
Loss in iteration 100 : 0.36734892749534803
Loss in iteration 101 : 0.3673065479267635
Loss in iteration 102 : 0.3672120051674877
Loss in iteration 103 : 0.36714191538722624
Loss in iteration 104 : 0.3672112111190012
Loss in iteration 105 : 0.36714305047458007
Loss in iteration 106 : 0.36706286672848404
Loss in iteration 107 : 0.36700700989578483
Loss in iteration 108 : 0.36703835658035994
Loss in iteration 109 : 0.36687415523913486
Loss in iteration 110 : 0.36679030370249494
Loss in iteration 111 : 0.36672828650220446
Loss in iteration 112 : 0.3665973643343366
Loss in iteration 113 : 0.36668425275367456
Loss in iteration 114 : 0.36654739798103875
Loss in iteration 115 : 0.36658470460428566
Loss in iteration 116 : 0.3663913720488986
Loss in iteration 117 : 0.36656964656405494
Loss in iteration 118 : 0.36630847404401706
Loss in iteration 119 : 0.36644381270962173
Loss in iteration 120 : 0.3661229779252298
Loss in iteration 121 : 0.36611408279253577
Loss in iteration 122 : 0.36602465139613605
Loss in iteration 123 : 0.3659271972191297
Loss in iteration 124 : 0.36598435141420665
Loss in iteration 125 : 0.3658555215600703
Loss in iteration 126 : 0.36593681918898857
Loss in iteration 127 : 0.36583471305382115
Loss in iteration 128 : 0.36586889262264016
Loss in iteration 129 : 0.3657965094291142
Loss in iteration 130 : 0.36581995148279994
Loss in iteration 131 : 0.36573131135230985
Loss in iteration 132 : 0.3657717563774291
Loss in iteration 133 : 0.3657091072330008
Loss in iteration 134 : 0.3657004443951848
Loss in iteration 135 : 0.3655355441559364
Loss in iteration 136 : 0.36550271689040137
Loss in iteration 137 : 0.36530160831279035
Loss in iteration 138 : 0.3652903099557722
Loss in iteration 139 : 0.3652351861911041
Loss in iteration 140 : 0.36525188005809117
Loss in iteration 141 : 0.3651699221387412
Loss in iteration 142 : 0.3651920238856183
Loss in iteration 143 : 0.36511686908935714
Loss in iteration 144 : 0.36511281287174746
Loss in iteration 145 : 0.36505839318533617
Loss in iteration 146 : 0.3650333720186861
Loss in iteration 147 : 0.3650196526068628
Loss in iteration 148 : 0.36498100371395104
Loss in iteration 149 : 0.36496852216282105
Loss in iteration 150 : 0.3649424853084233
Loss in iteration 151 : 0.3649329462847829
Loss in iteration 152 : 0.36493861976203695
Loss in iteration 153 : 0.36487562318169314
Loss in iteration 154 : 0.36487212523114687
Loss in iteration 155 : 0.3648606145022364
Loss in iteration 156 : 0.36482469399673434
Loss in iteration 157 : 0.3648277502658304
Loss in iteration 158 : 0.36477747779491587
Loss in iteration 159 : 0.3647927952621803
Loss in iteration 160 : 0.3647350036835283
Loss in iteration 161 : 0.36473116763325425
Loss in iteration 162 : 0.36471929698888905
Loss in iteration 163 : 0.36469631236714045
Loss in iteration 164 : 0.36469196629036404
Loss in iteration 165 : 0.36472305291364987
Loss in iteration 166 : 0.3646655089012891
Loss in iteration 167 : 0.3646775674424661
Loss in iteration 168 : 0.3646392264281076
Loss in iteration 169 : 0.36460948786845976
Loss in iteration 170 : 0.36463569162074067
Loss in iteration 171 : 0.36456501927236346
Loss in iteration 172 : 0.36454836070398583
Loss in iteration 173 : 0.3645605846362535
Loss in iteration 174 : 0.3645445768755798
Loss in iteration 175 : 0.36449416645269883
Loss in iteration 176 : 0.36450927035836006
Loss in iteration 177 : 0.3644602128055445
Loss in iteration 178 : 0.3644870545221848
Loss in iteration 179 : 0.3644424366592666
Loss in iteration 180 : 0.36446848933605935
Loss in iteration 181 : 0.3643956165510782
Loss in iteration 182 : 0.364354203524537
Loss in iteration 183 : 0.3643394699693146
Loss in iteration 184 : 0.3642791109137064
Loss in iteration 185 : 0.36428661155305764
Loss in iteration 186 : 0.3642401165448027
Loss in iteration 187 : 0.36425654976252986
Loss in iteration 188 : 0.364212568505044
Loss in iteration 189 : 0.36422028649701466
Loss in iteration 190 : 0.3641914515632162
Loss in iteration 191 : 0.36418503491276133
Loss in iteration 192 : 0.36416026060215445
Loss in iteration 193 : 0.3641609544973176
Loss in iteration 194 : 0.36414312212641575
Loss in iteration 195 : 0.364124208243878
Loss in iteration 196 : 0.3641133618671767
Loss in iteration 197 : 0.36408553454387155
Loss in iteration 198 : 0.36408248627790907
Loss in iteration 199 : 0.36406574509556056
Loss in iteration 200 : 0.3640601224757495
Testing accuracy  of updater 3 on alg 1 with rate 0.196 = 0.7885, training accuracy 0.8420200712204597, time elapsed: 2111 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.1614274394811497
Loss in iteration 3 : 0.8948432254488555
Loss in iteration 4 : 0.6295937674597818
Loss in iteration 5 : 0.4233442838823024
Loss in iteration 6 : 0.407363967043204
Loss in iteration 7 : 0.400993995796377
Loss in iteration 8 : 0.39487106591573695
Loss in iteration 9 : 0.39119909655427887
Loss in iteration 10 : 0.38808463927249626
Loss in iteration 11 : 0.3858605420349645
Loss in iteration 12 : 0.3842449336237564
Loss in iteration 13 : 0.3829031613287493
Loss in iteration 14 : 0.38173991098662396
Loss in iteration 15 : 0.3807394615541171
Loss in iteration 16 : 0.37989459707443357
Loss in iteration 17 : 0.37918324757568406
Loss in iteration 18 : 0.3785825345484892
Loss in iteration 19 : 0.3780278689490988
Loss in iteration 20 : 0.3775372519594165
Loss in iteration 21 : 0.3770963036235106
Loss in iteration 22 : 0.37669205644642323
Loss in iteration 23 : 0.37630179574931066
Loss in iteration 24 : 0.3759254863191931
Loss in iteration 25 : 0.375559961929138
Loss in iteration 26 : 0.3752369945806337
Loss in iteration 27 : 0.3749404375866197
Loss in iteration 28 : 0.3746665882125521
Loss in iteration 29 : 0.3744161925391518
Loss in iteration 30 : 0.3741722675686107
Loss in iteration 31 : 0.37394351611163795
Loss in iteration 32 : 0.37372426725012226
Loss in iteration 33 : 0.3735120770626092
Loss in iteration 34 : 0.373311996686371
Loss in iteration 35 : 0.3731270735382619
Loss in iteration 36 : 0.3729465133851006
Loss in iteration 37 : 0.3727734865106843
Loss in iteration 38 : 0.37262050389150836
Loss in iteration 39 : 0.37247745052228437
Loss in iteration 40 : 0.3723447554095122
Loss in iteration 41 : 0.37223423439020564
Loss in iteration 42 : 0.37212709289207785
Loss in iteration 43 : 0.37202610699992156
Loss in iteration 44 : 0.3719304800045432
Loss in iteration 45 : 0.3718398998613738
Loss in iteration 46 : 0.3717500368205082
Loss in iteration 47 : 0.371664217083768
Loss in iteration 48 : 0.3715854505131378
Loss in iteration 49 : 0.37151002798809396
Loss in iteration 50 : 0.3714382727291767
Loss in iteration 51 : 0.3713644998021429
Loss in iteration 52 : 0.37129568184025297
Loss in iteration 53 : 0.37122711736305525
Loss in iteration 54 : 0.37115977293135743
Loss in iteration 55 : 0.3710980940748675
Loss in iteration 56 : 0.3710370793970414
Loss in iteration 57 : 0.3709730740825462
Loss in iteration 58 : 0.3709123270428181
Loss in iteration 59 : 0.3708525558939202
Loss in iteration 60 : 0.3707928015677427
Loss in iteration 61 : 0.37073322784737117
Loss in iteration 62 : 0.3706745493511163
Loss in iteration 63 : 0.3706173199891599
Loss in iteration 64 : 0.370560107064186
Loss in iteration 65 : 0.37050299251558455
Loss in iteration 66 : 0.3704469321201623
Loss in iteration 67 : 0.370390887615805
Loss in iteration 68 : 0.3703348589858463
Loss in iteration 69 : 0.3702792286998529
Loss in iteration 70 : 0.3702284603934405
Loss in iteration 71 : 0.3701768617502751
Loss in iteration 72 : 0.3701276371644541
Loss in iteration 73 : 0.37007820165908706
Loss in iteration 74 : 0.3700271487710473
Loss in iteration 75 : 0.36997829166493634
Loss in iteration 76 : 0.36992940334028945
Loss in iteration 77 : 0.36988056729982755
Loss in iteration 78 : 0.3698323499209462
Loss in iteration 79 : 0.36978419975057636
Loss in iteration 80 : 0.3697370880758088
Loss in iteration 81 : 0.36968972898775354
Loss in iteration 82 : 0.36964256172312016
Loss in iteration 83 : 0.36959554319601723
Loss in iteration 84 : 0.36955018739306883
Loss in iteration 85 : 0.36950387177157434
Loss in iteration 86 : 0.3694584726020969
Loss in iteration 87 : 0.36941320196293415
Loss in iteration 88 : 0.3693691881812438
Loss in iteration 89 : 0.3693251531591602
Loss in iteration 90 : 0.3692810135000356
Loss in iteration 91 : 0.3692380668634335
Loss in iteration 92 : 0.3691959292971002
Loss in iteration 93 : 0.3691538019131501
Loss in iteration 94 : 0.36911184450452084
Loss in iteration 95 : 0.3690708217107785
Loss in iteration 96 : 0.36903008509917345
Loss in iteration 97 : 0.3689893591290839
Loss in iteration 98 : 0.3689486437914171
Loss in iteration 99 : 0.36890848688699795
Loss in iteration 100 : 0.36886854754453463
Loss in iteration 101 : 0.36882906475637484
Loss in iteration 102 : 0.36878959205879286
Loss in iteration 103 : 0.3687501294433976
Loss in iteration 104 : 0.36871067690180903
Loss in iteration 105 : 0.36867123442565863
Loss in iteration 106 : 0.36863180200658924
Loss in iteration 107 : 0.36859262378178237
Loss in iteration 108 : 0.36855561395361713
Loss in iteration 109 : 0.36851766278100306
Loss in iteration 110 : 0.3684796372377736
Loss in iteration 111 : 0.3684429357920415
Loss in iteration 112 : 0.3684066763133302
Loss in iteration 113 : 0.36837121657075006
Loss in iteration 114 : 0.36833633766465845
Loss in iteration 115 : 0.3683004267782658
Loss in iteration 116 : 0.3682643461975649
Loss in iteration 117 : 0.36823126472548123
Loss in iteration 118 : 0.36819972548222574
Loss in iteration 119 : 0.36816436512304374
Loss in iteration 120 : 0.3681311727077111
Loss in iteration 121 : 0.36809651232899615
Loss in iteration 122 : 0.3680653612033009
Loss in iteration 123 : 0.3680403437167396
Loss in iteration 124 : 0.36800511770949973
Loss in iteration 125 : 0.3679729418261952
Loss in iteration 126 : 0.367943287885598
Loss in iteration 127 : 0.36790350941519384
Loss in iteration 128 : 0.3678736501992147
Loss in iteration 129 : 0.36784287618767286
Loss in iteration 130 : 0.36781286791521767
Loss in iteration 131 : 0.3677797840307412
Loss in iteration 132 : 0.3677501553687784
Loss in iteration 133 : 0.36772064322882614
Loss in iteration 134 : 0.36769052244948963
Loss in iteration 135 : 0.3676613051824099
Loss in iteration 136 : 0.36763229044555135
Loss in iteration 137 : 0.36760318069193587
Loss in iteration 138 : 0.36757428154204
Loss in iteration 139 : 0.367545894502576
Loss in iteration 140 : 0.3675185897578836
Loss in iteration 141 : 0.3674906292411681
Loss in iteration 142 : 0.3674631346540707
Loss in iteration 143 : 0.3674354822292705
Loss in iteration 144 : 0.36740783547034445
Loss in iteration 145 : 0.36738044362963834
Loss in iteration 146 : 0.3673528098744908
Loss in iteration 147 : 0.36732518003027476
Loss in iteration 148 : 0.3672977153306651
Loss in iteration 149 : 0.36727102587193494
Loss in iteration 150 : 0.36724345219765364
Loss in iteration 151 : 0.36721680258718375
Loss in iteration 152 : 0.3671900817423092
Loss in iteration 153 : 0.3671638906502959
Loss in iteration 154 : 0.3671378385534132
Loss in iteration 155 : 0.3671121744046807
Loss in iteration 156 : 0.367089128700826
Loss in iteration 157 : 0.3670643903860798
Loss in iteration 158 : 0.3670395214784955
Loss in iteration 159 : 0.3670151488433127
Loss in iteration 160 : 0.36699159815548515
Loss in iteration 161 : 0.36696702615671256
Loss in iteration 162 : 0.3669454635113368
Loss in iteration 163 : 0.3669197661393941
Loss in iteration 164 : 0.3668973814887139
Loss in iteration 165 : 0.36687365592224164
Loss in iteration 166 : 0.36684969878519086
Loss in iteration 167 : 0.36682664141840854
Loss in iteration 168 : 0.3668038870823658
Loss in iteration 169 : 0.36678182593122277
Loss in iteration 170 : 0.3667620032823712
Loss in iteration 171 : 0.36673775376865453
Loss in iteration 172 : 0.3667170750855076
Loss in iteration 173 : 0.36669493837798256
Loss in iteration 174 : 0.3666738924028051
Loss in iteration 175 : 0.3666521362107278
Loss in iteration 176 : 0.3666307233868627
Loss in iteration 177 : 0.36660934725429367
Loss in iteration 178 : 0.36658756802466425
Loss in iteration 179 : 0.3665665714961032
Loss in iteration 180 : 0.36654450828108914
Loss in iteration 181 : 0.3665237541692064
Loss in iteration 182 : 0.36650326461181487
Loss in iteration 183 : 0.3664811072106336
Loss in iteration 184 : 0.36645995957411426
Loss in iteration 185 : 0.36643895290005196
Loss in iteration 186 : 0.3664182889612159
Loss in iteration 187 : 0.36639743826537535
Loss in iteration 188 : 0.3663770842094717
Loss in iteration 189 : 0.3663567145261218
Loss in iteration 190 : 0.36633742104668177
Loss in iteration 191 : 0.3663168729165225
Loss in iteration 192 : 0.3662961559970084
Loss in iteration 193 : 0.36627704335644395
Loss in iteration 194 : 0.36625562109983484
Loss in iteration 195 : 0.36623587199479857
Loss in iteration 196 : 0.36621607491641733
Loss in iteration 197 : 0.3661962806803329
Loss in iteration 198 : 0.3661764892853017
Loss in iteration 199 : 0.3661567007300821
Loss in iteration 200 : 0.366136915013433
Testing accuracy  of updater 3 on alg 1 with rate 0.11200000000000002 = 0.786, training accuracy 0.8397539656846876, time elapsed: 2290 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.558516795813421
Loss in iteration 3 : 0.53230870600486
Loss in iteration 4 : 0.5265868635935435
Loss in iteration 5 : 0.5225467721032263
Loss in iteration 6 : 0.518661910795709
Loss in iteration 7 : 0.514898043764901
Loss in iteration 8 : 0.5112138295300822
Loss in iteration 9 : 0.5075738682710812
Loss in iteration 10 : 0.5039609703134097
Loss in iteration 11 : 0.5003954685355092
Loss in iteration 12 : 0.4968619110526475
Loss in iteration 13 : 0.49334580002904777
Loss in iteration 14 : 0.4898519136904422
Loss in iteration 15 : 0.4863868712301442
Loss in iteration 16 : 0.4829301938473196
Loss in iteration 17 : 0.479491246005607
Loss in iteration 18 : 0.47606356713551223
Loss in iteration 19 : 0.47264477645919317
Loss in iteration 20 : 0.46923299144766634
Loss in iteration 21 : 0.4658320479945344
Loss in iteration 22 : 0.46245267910181165
Loss in iteration 23 : 0.45909913651424056
Loss in iteration 24 : 0.45574986639177173
Loss in iteration 25 : 0.4524135191871186
Loss in iteration 26 : 0.4490834268324937
Loss in iteration 27 : 0.44576281473187496
Loss in iteration 28 : 0.44245091882305015
Loss in iteration 29 : 0.4391505966982633
Loss in iteration 30 : 0.4358508671252754
Loss in iteration 31 : 0.43259452764441886
Loss in iteration 32 : 0.42942300161902014
Loss in iteration 33 : 0.4263575247760932
Loss in iteration 34 : 0.42347598138567083
Loss in iteration 35 : 0.42069832726525835
Loss in iteration 36 : 0.4180822950470676
Loss in iteration 37 : 0.415642225124129
Loss in iteration 38 : 0.4133089435001968
Loss in iteration 39 : 0.4110924576708785
Loss in iteration 40 : 0.4089946337587726
Loss in iteration 41 : 0.40700235657912226
Loss in iteration 42 : 0.4051762754096713
Loss in iteration 43 : 0.40346800928126253
Loss in iteration 44 : 0.4018919258575579
Loss in iteration 45 : 0.4004113902492867
Loss in iteration 46 : 0.3989866882779065
Loss in iteration 47 : 0.3976708782456085
Loss in iteration 48 : 0.39645497744972025
Loss in iteration 49 : 0.3953485398723446
Loss in iteration 50 : 0.394344975380426
Loss in iteration 51 : 0.3933939456195019
Loss in iteration 52 : 0.39251369069871833
Loss in iteration 53 : 0.39169315626009543
Loss in iteration 54 : 0.39094087070785977
Loss in iteration 55 : 0.3902553699515106
Loss in iteration 56 : 0.3896118405348315
Loss in iteration 57 : 0.3890027717827946
Loss in iteration 58 : 0.3884258808678837
Loss in iteration 59 : 0.38789331743137145
Loss in iteration 60 : 0.38740047616963036
Loss in iteration 61 : 0.3869367421410024
Loss in iteration 62 : 0.38648960981237884
Loss in iteration 63 : 0.3860609975291074
Loss in iteration 64 : 0.3856420139999307
Loss in iteration 65 : 0.38524811984181906
Loss in iteration 66 : 0.38486784461424617
Loss in iteration 67 : 0.3845030201015672
Loss in iteration 68 : 0.3841549320410661
Loss in iteration 69 : 0.38381888824431803
Loss in iteration 70 : 0.38348732318881334
Loss in iteration 71 : 0.38316304606601803
Loss in iteration 72 : 0.38284630973041556
Loss in iteration 73 : 0.38253794588566964
Loss in iteration 74 : 0.3822390135627084
Loss in iteration 75 : 0.38194665293497226
Loss in iteration 76 : 0.3816656680872682
Loss in iteration 77 : 0.38139067098154417
Loss in iteration 78 : 0.38112585681387795
Loss in iteration 79 : 0.3808734242218033
Testing accuracy  of updater 3 on alg 1 with rate 0.02799999999999997 = 0.7785, training accuracy 0.8378115895111686, time elapsed: 1273 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 71 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 45 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 70 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 43 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 70.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 35 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 40.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 49 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9463033087219395
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.5, training accuracy 0.6474587245063127, time elapsed: 123 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 10.460057176888386
Loss in iteration 3 : 6.440140587936527
Loss in iteration 4 : 2.6744990907658646
Loss in iteration 5 : 0.7264468158985058
Loss in iteration 6 : 0.7739891216750493
Loss in iteration 7 : 0.8396197588676942
Loss in iteration 8 : 1.1828139338149675
Loss in iteration 9 : 1.4277107263228195
Loss in iteration 10 : 2.3786536288231317
Loss in iteration 11 : 0.8231569491159207
Loss in iteration 12 : 0.8883271607170903
Loss in iteration 13 : 0.8649445573729726
Loss in iteration 14 : 1.1243417943205227
Loss in iteration 15 : 1.1878407822411312
Loss in iteration 16 : 1.9995314712469665
Loss in iteration 17 : 1.037342221010281
Loss in iteration 18 : 1.3747606643379255
Loss in iteration 19 : 1.001515508154958
Loss in iteration 20 : 1.3557902917447535
Loss in iteration 21 : 1.0123640688634197
Loss in iteration 22 : 1.4279166525610443
Loss in iteration 23 : 1.0642246651760061
Loss in iteration 24 : 1.5622654446669215
Loss in iteration 25 : 1.0687379362800395
Loss in iteration 26 : 1.5044942116332585
Loss in iteration 27 : 1.0264306283749998
Loss in iteration 28 : 1.4255529201629107
Loss in iteration 29 : 1.0246282949826635
Loss in iteration 30 : 1.475854788869082
Loss in iteration 31 : 1.0700499676138264
Loss in iteration 32 : 1.5975005029505447
Loss in iteration 33 : 1.1046996944124723
Loss in iteration 34 : 1.6162316960624086
Loss in iteration 35 : 1.0636651298292588
Loss in iteration 36 : 1.495858587217764
Loss in iteration 37 : 1.024954955246767
Loss in iteration 38 : 1.4201830678551934
Loss in iteration 39 : 1.0927680165777416
Loss in iteration 40 : 1.6562591065093668
Loss in iteration 41 : 1.116527344086162
Loss in iteration 42 : 1.6359389943951017
Loss in iteration 43 : 1.0568535876727456
Loss in iteration 44 : 1.5016959859313337
Loss in iteration 45 : 1.0717451829620928
Loss in iteration 46 : 1.5942187389032048
Loss in iteration 47 : 1.0587098139520048
Loss in iteration 48 : 1.5481083032899687
Loss in iteration 49 : 1.0807695810510345
Loss in iteration 50 : 1.627310131884326
Loss in iteration 51 : 1.091040212574433
Loss in iteration 52 : 1.6257890396832928
Loss in iteration 53 : 1.0769021899756315
Loss in iteration 54 : 1.569163459141466
Loss in iteration 55 : 1.0762752537229274
Loss in iteration 56 : 1.5821769801777132
Loss in iteration 57 : 1.0909180999260228
Loss in iteration 58 : 1.6433892001310113
Loss in iteration 59 : 1.0934749716032053
Loss in iteration 60 : 1.6244411683143265
Loss in iteration 61 : 1.0627760330748706
Loss in iteration 62 : 1.5433886002154371
Loss in iteration 63 : 1.1055089038498855
Loss in iteration 64 : 1.673536626957194
Loss in iteration 65 : 1.0803557106298767
Loss in iteration 66 : 1.571277173604745
Loss in iteration 67 : 1.0833199175082924
Loss in iteration 68 : 1.5854993274421894
Loss in iteration 69 : 1.0871835617558971
Loss in iteration 70 : 1.600629598144036
Loss in iteration 71 : 1.1117324905934038
Loss in iteration 72 : 1.6732852889025915
Loss in iteration 73 : 1.0726637233086862
Loss in iteration 74 : 1.5681345289060502
Loss in iteration 75 : 1.0865088987537395
Loss in iteration 76 : 1.5814771129844463
Loss in iteration 77 : 1.100351318683852
Loss in iteration 78 : 1.628080962469503
Loss in iteration 79 : 1.098266049509623
Loss in iteration 80 : 1.6070735724365692
Loss in iteration 81 : 1.0871751220327268
Loss in iteration 82 : 1.5749970692432658
Loss in iteration 83 : 1.0959727002396566
Loss in iteration 84 : 1.6243323568539705
Loss in iteration 85 : 1.1017560797682078
Loss in iteration 86 : 1.614999357798069
Loss in iteration 87 : 1.0960900434580791
Loss in iteration 88 : 1.5801404165558464
Loss in iteration 89 : 1.085065296672124
Loss in iteration 90 : 1.5798093500556765
Loss in iteration 91 : 1.0936411827149561
Loss in iteration 92 : 1.6173621081518832
Loss in iteration 93 : 1.0960285098254012
Loss in iteration 94 : 1.6159967484238325
Loss in iteration 95 : 1.0941982419618583
Loss in iteration 96 : 1.6103324915477537
Loss in iteration 97 : 1.0945050936750282
Loss in iteration 98 : 1.6083554253183283
Loss in iteration 99 : 1.084834662728032
Loss in iteration 100 : 1.568132230971076
Loss in iteration 101 : 1.084417319789248
Loss in iteration 102 : 1.5965997601500763
Loss in iteration 103 : 1.0931034652794571
Loss in iteration 104 : 1.6242665548164728
Loss in iteration 105 : 1.093109751002868
Loss in iteration 106 : 1.617596229614981
Loss in iteration 107 : 1.0909887590035314
Loss in iteration 108 : 1.6046819321104708
Loss in iteration 109 : 1.0770378239927958
Loss in iteration 110 : 1.548362534452157
Loss in iteration 111 : 1.0950457266560307
Loss in iteration 112 : 1.625880100190285
Loss in iteration 113 : 1.0985784554421203
Loss in iteration 114 : 1.6164216171633272
Loss in iteration 115 : 1.0943747644164818
Loss in iteration 116 : 1.604178946817629
Loss in iteration 117 : 1.0760957091041725
Loss in iteration 118 : 1.5571352046969962
Loss in iteration 119 : 1.0867623680786025
Loss in iteration 120 : 1.6177549308612449
Loss in iteration 121 : 1.1008087533496904
Loss in iteration 122 : 1.6240620677434658
Loss in iteration 123 : 1.0880471411519095
Loss in iteration 124 : 1.5818748953642763
Loss in iteration 125 : 1.077564273492619
Loss in iteration 126 : 1.5706562770847805
Loss in iteration 127 : 1.0914799828477175
Loss in iteration 128 : 1.6223171427072651
Loss in iteration 129 : 1.0971535734866757
Loss in iteration 130 : 1.614989326207421
Loss in iteration 131 : 1.0812854358636714
Loss in iteration 132 : 1.5849491916683087
Loss in iteration 133 : 1.0853164699385345
Loss in iteration 134 : 1.598450269369073
Loss in iteration 135 : 1.0897313262339405
Loss in iteration 136 : 1.5944900255780472
Loss in iteration 137 : 1.0886163325097855
Loss in iteration 138 : 1.5974919159069614
Loss in iteration 139 : 1.0917071182743627
Loss in iteration 140 : 1.5957159894497808
Loss in iteration 141 : 1.0878552878980206
Loss in iteration 142 : 1.5999302697921882
Loss in iteration 143 : 1.0877262770043088
Loss in iteration 144 : 1.5999175552873164
Loss in iteration 145 : 1.0905783514092906
Loss in iteration 146 : 1.6014449896782335
Loss in iteration 147 : 1.086619007096418
Loss in iteration 148 : 1.6003257590366404
Loss in iteration 149 : 1.0838464540027284
Loss in iteration 150 : 1.5861663702573503
Loss in iteration 151 : 1.0857293312560146
Loss in iteration 152 : 1.5976242869802473
Loss in iteration 153 : 1.088423520916675
Loss in iteration 154 : 1.6109733940222288
Loss in iteration 155 : 1.0913027148824734
Loss in iteration 156 : 1.6049790221644675
Loss in iteration 157 : 1.0878099509159294
Loss in iteration 158 : 1.5806352679321265
Loss in iteration 159 : 1.0779881309735575
Loss in iteration 160 : 1.5585430853332785
Loss in iteration 161 : 1.0888090233774241
Loss in iteration 162 : 1.6309499587450436
Loss in iteration 163 : 1.0937547189530685
Loss in iteration 164 : 1.6192155461079434
Loss in iteration 165 : 1.085456419507553
Loss in iteration 166 : 1.5629374164629286
Loss in iteration 167 : 1.0825254413088834
Loss in iteration 168 : 1.5795279146244192
Loss in iteration 169 : 1.0995225175868424
Loss in iteration 170 : 1.6470817292971784
Loss in iteration 171 : 1.0763419558637792
Loss in iteration 172 : 1.545675729831664
Loss in iteration 173 : 1.0887369636007769
Loss in iteration 174 : 1.6109451729469295
Loss in iteration 175 : 1.100019538102179
Loss in iteration 176 : 1.6405074625788112
Loss in iteration 177 : 1.0759493377966303
Loss in iteration 178 : 1.5263321858039836
Loss in iteration 179 : 1.092296431468904
Loss in iteration 180 : 1.6268375551627519
Loss in iteration 181 : 1.0953485389073843
Loss in iteration 182 : 1.6178683141094554
Loss in iteration 183 : 1.088500197350871
Loss in iteration 184 : 1.5680712989309378
Loss in iteration 185 : 1.0838450855091475
Loss in iteration 186 : 1.5723784321327112
Loss in iteration 187 : 1.085111004261923
Loss in iteration 188 : 1.5821243874663293
Loss in iteration 189 : 1.0964216731231338
Loss in iteration 190 : 1.6302430922136015
Loss in iteration 191 : 1.0913040997824033
Loss in iteration 192 : 1.574994434245874
Loss in iteration 193 : 1.080404752510745
Loss in iteration 194 : 1.5740851551362682
Loss in iteration 195 : 1.0892076113104998
Loss in iteration 196 : 1.6004605154242297
Loss in iteration 197 : 1.1058801895654258
Loss in iteration 198 : 1.6679916001112431
Loss in iteration 199 : 1.0636275064398684
Loss in iteration 200 : 1.493095463468359
Testing accuracy  of updater 5 on alg 1 with rate 0.07 = 0.78625, training accuracy 0.7853674328261573, time elapsed: 2974 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 5.907258793565452
Loss in iteration 3 : 3.7159208274591453
Loss in iteration 4 : 1.5629233530474547
Loss in iteration 5 : 0.597103458576059
Loss in iteration 6 : 0.6142371463298103
Loss in iteration 7 : 0.6242195558070588
Loss in iteration 8 : 0.7523837846852156
Loss in iteration 9 : 0.8677561381592965
Loss in iteration 10 : 1.3688923729640246
Loss in iteration 11 : 0.8019704550309974
Loss in iteration 12 : 1.0181186070551558
Loss in iteration 13 : 0.7900957296387743
Loss in iteration 14 : 0.9980063709341274
Loss in iteration 15 : 0.7880612123072886
Loss in iteration 16 : 1.0236627361528738
Loss in iteration 17 : 0.7928753207659175
Loss in iteration 18 : 1.03337174086154
Loss in iteration 19 : 0.804102123313195
Loss in iteration 20 : 1.0633804536801092
Loss in iteration 21 : 0.8022865981423866
Loss in iteration 22 : 1.0540184893005724
Loss in iteration 23 : 0.7974008973589003
Loss in iteration 24 : 1.0528745567380826
Loss in iteration 25 : 0.8114860653000382
Loss in iteration 26 : 1.0988690475134524
Loss in iteration 27 : 0.8245899138826597
Loss in iteration 28 : 1.1437529730574114
Loss in iteration 29 : 0.8312131216020765
Loss in iteration 30 : 1.1227269151325803
Loss in iteration 31 : 0.8078147634068815
Loss in iteration 32 : 1.0810195407171321
Loss in iteration 33 : 0.8135353746729017
Loss in iteration 34 : 1.1217212583501335
Loss in iteration 35 : 0.8457220012277871
Loss in iteration 36 : 1.2017228452380593
Loss in iteration 37 : 0.8491832756910187
Loss in iteration 38 : 1.1649160577995712
Loss in iteration 39 : 0.822490762580196
Loss in iteration 40 : 1.126328377066034
Loss in iteration 41 : 0.8279816331213979
Loss in iteration 42 : 1.1578925129806135
Loss in iteration 43 : 0.8401053729055346
Loss in iteration 44 : 1.180997449005313
Loss in iteration 45 : 0.845174807546348
Loss in iteration 46 : 1.187759426290946
Loss in iteration 47 : 0.8429081973274193
Loss in iteration 48 : 1.177194530429997
Loss in iteration 49 : 0.8343120314561681
Loss in iteration 50 : 1.167574826320885
Loss in iteration 51 : 0.845877613225956
Loss in iteration 52 : 1.1919157646392344
Loss in iteration 53 : 0.845091442434016
Loss in iteration 54 : 1.1954919015536378
Loss in iteration 55 : 0.843033207340453
Loss in iteration 56 : 1.1914746195659287
Loss in iteration 57 : 0.8453790084168284
Loss in iteration 58 : 1.1958925828412719
Loss in iteration 59 : 0.8448106367181186
Loss in iteration 60 : 1.187576464257625
Loss in iteration 61 : 0.8522157216708626
Loss in iteration 62 : 1.199530765751197
Loss in iteration 63 : 0.8475590935103461
Loss in iteration 64 : 1.1946072101781129
Loss in iteration 65 : 0.8496234652739859
Loss in iteration 66 : 1.196047162121117
Loss in iteration 67 : 0.845523026045915
Loss in iteration 68 : 1.1681397599574046
Loss in iteration 69 : 0.8594448892579455
Loss in iteration 70 : 1.2262835300495338
Loss in iteration 71 : 0.8400335729408009
Loss in iteration 72 : 1.1488919172289682
Loss in iteration 73 : 0.8674656192982353
Loss in iteration 74 : 1.2386858083085608
Loss in iteration 75 : 0.8433615508001596
Loss in iteration 76 : 1.1544372129045053
Loss in iteration 77 : 0.85554273098465
Loss in iteration 78 : 1.2083642428997492
Loss in iteration 79 : 0.8576368334057219
Loss in iteration 80 : 1.1994820432884352
Loss in iteration 81 : 0.8535433153822896
Loss in iteration 82 : 1.1840957332827144
Loss in iteration 83 : 0.8614134424498617
Loss in iteration 84 : 1.2005558096978217
Loss in iteration 85 : 0.8483261794614911
Loss in iteration 86 : 1.17223355457928
Loss in iteration 87 : 0.8668574986470222
Loss in iteration 88 : 1.219478620707017
Loss in iteration 89 : 0.8509925145158841
Loss in iteration 90 : 1.175915824906881
Loss in iteration 91 : 0.8582486762425554
Loss in iteration 92 : 1.1836420801902576
Loss in iteration 93 : 0.8580258804684283
Loss in iteration 94 : 1.1850497292813529
Loss in iteration 95 : 0.8602290372302291
Loss in iteration 96 : 1.1893103143407606
Loss in iteration 97 : 0.8558902964438527
Loss in iteration 98 : 1.1837518394561029
Loss in iteration 99 : 0.8594699412712079
Loss in iteration 100 : 1.1907963102716275
Loss in iteration 101 : 0.8550800031740673
Loss in iteration 102 : 1.1885089477051152
Loss in iteration 103 : 0.8545889561002691
Loss in iteration 104 : 1.182912884153064
Loss in iteration 105 : 0.8525423808379647
Loss in iteration 106 : 1.1888353924286619
Loss in iteration 107 : 0.851806318127302
Loss in iteration 108 : 1.1854007135041147
Loss in iteration 109 : 0.8523897573038272
Loss in iteration 110 : 1.1930503263554821
Loss in iteration 111 : 0.8568342476509542
Loss in iteration 112 : 1.2057065180142879
Loss in iteration 113 : 0.851595351005638
Loss in iteration 114 : 1.1811903785230653
Loss in iteration 115 : 0.8419701651702363
Loss in iteration 116 : 1.1784761238798116
Loss in iteration 117 : 0.8564569673170884
Loss in iteration 118 : 1.205096352830678
Loss in iteration 119 : 0.8581983326105614
Loss in iteration 120 : 1.1992207576639564
Loss in iteration 121 : 0.8472973857041661
Loss in iteration 122 : 1.1808899218222313
Loss in iteration 123 : 0.8468133215777405
Loss in iteration 124 : 1.1840669279238427
Loss in iteration 125 : 0.8502158404186476
Loss in iteration 126 : 1.1895656070689982
Loss in iteration 127 : 0.8542389531544862
Loss in iteration 128 : 1.2047115530150818
Loss in iteration 129 : 0.85373445266872
Loss in iteration 130 : 1.1905961808840702
Loss in iteration 131 : 0.8476891190364124
Loss in iteration 132 : 1.173248037005178
Loss in iteration 133 : 0.8539657176866802
Loss in iteration 134 : 1.1972430699509402
Loss in iteration 135 : 0.8505846501948883
Loss in iteration 136 : 1.1905953864615624
Loss in iteration 137 : 0.8465837357261828
Loss in iteration 138 : 1.1903585236395808
Loss in iteration 139 : 0.8455687755995223
Loss in iteration 140 : 1.191959552958967
Loss in iteration 141 : 0.8475009645413412
Loss in iteration 142 : 1.19167713729944
Loss in iteration 143 : 0.8474651101378298
Loss in iteration 144 : 1.191542703067562
Loss in iteration 145 : 0.8454820971792043
Loss in iteration 146 : 1.1844839274565422
Loss in iteration 147 : 0.8514150410156838
Loss in iteration 148 : 1.194195720071288
Loss in iteration 149 : 0.8484207484084408
Loss in iteration 150 : 1.1829552402208467
Loss in iteration 151 : 0.8516897285328009
Loss in iteration 152 : 1.1976291475533598
Loss in iteration 153 : 0.8513098111477337
Loss in iteration 154 : 1.188042106231485
Loss in iteration 155 : 0.8444638700201742
Loss in iteration 156 : 1.1787431786787708
Loss in iteration 157 : 0.853069249761641
Loss in iteration 158 : 1.1989798032606425
Loss in iteration 159 : 0.8495956894188903
Loss in iteration 160 : 1.1840697856863698
Loss in iteration 161 : 0.8459513755287292
Loss in iteration 162 : 1.179602206616952
Loss in iteration 163 : 0.8522073226077014
Loss in iteration 164 : 1.19201992722862
Loss in iteration 165 : 0.8441990368814029
Loss in iteration 166 : 1.178868214072814
Loss in iteration 167 : 0.8561281081857314
Loss in iteration 168 : 1.200042723721548
Loss in iteration 169 : 0.8450894525017482
Loss in iteration 170 : 1.1754426078927616
Loss in iteration 171 : 0.853068504125434
Loss in iteration 172 : 1.192360058428698
Loss in iteration 173 : 0.8435757538829625
Loss in iteration 174 : 1.1742006487538401
Loss in iteration 175 : 0.8547336266444676
Loss in iteration 176 : 1.2011742962441703
Loss in iteration 177 : 0.8455535997649083
Loss in iteration 178 : 1.1715125613139943
Loss in iteration 179 : 0.8466826628615687
Loss in iteration 180 : 1.172577483199252
Loss in iteration 181 : 0.8574123802176143
Loss in iteration 182 : 1.2150067942223166
Loss in iteration 183 : 0.8465345130816173
Loss in iteration 184 : 1.1642595739476247
Loss in iteration 185 : 0.8467881022466008
Loss in iteration 186 : 1.1810646892033017
Loss in iteration 187 : 0.8575806526745848
Loss in iteration 188 : 1.2121952004771126
Loss in iteration 189 : 0.8459914500685667
Loss in iteration 190 : 1.15872351566676
Loss in iteration 191 : 0.8431349554965337
Loss in iteration 192 : 1.1674762187594347
Loss in iteration 193 : 0.8584716447898237
Loss in iteration 194 : 1.2199169673535712
Loss in iteration 195 : 0.8455830213409714
Loss in iteration 196 : 1.1560808035883725
Loss in iteration 197 : 0.8456952425909207
Loss in iteration 198 : 1.186347609460742
Loss in iteration 199 : 0.8546006259716131
Loss in iteration 200 : 1.2147097425813647
Testing accuracy  of updater 5 on alg 1 with rate 0.049 = 0.78825, training accuracy 0.7869860796374231, time elapsed: 3152 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.5223749434089053
Loss in iteration 3 : 2.272021007307248
Loss in iteration 4 : 1.043107435988903
Loss in iteration 5 : 0.47084608275914497
Loss in iteration 6 : 0.5218065808226224
Loss in iteration 7 : 0.586370947707938
Loss in iteration 8 : 0.7646116384595106
Loss in iteration 9 : 0.6390407225647606
Loss in iteration 10 : 0.7810863248023469
Loss in iteration 11 : 0.5751347698907553
Loss in iteration 12 : 0.6382585918770187
Loss in iteration 13 : 0.5596457062622512
Loss in iteration 14 : 0.6302865143095325
Loss in iteration 15 : 0.5721532302158804
Loss in iteration 16 : 0.6558844856038942
Loss in iteration 17 : 0.5939828649201305
Loss in iteration 18 : 0.7080270352382331
Loss in iteration 19 : 0.6062670765945662
Loss in iteration 20 : 0.7201645211171699
Loss in iteration 21 : 0.5956060506609586
Loss in iteration 22 : 0.6952816971844391
Loss in iteration 23 : 0.5927746837824003
Loss in iteration 24 : 0.7103124350778236
Loss in iteration 25 : 0.5990700440438352
Loss in iteration 26 : 0.724268952402985
Loss in iteration 27 : 0.6039507474564507
Loss in iteration 28 : 0.7362796309907167
Loss in iteration 29 : 0.603880927001065
Loss in iteration 30 : 0.7331299546867472
Loss in iteration 31 : 0.6020797952108973
Loss in iteration 32 : 0.7360548454912198
Loss in iteration 33 : 0.6078429688875515
Loss in iteration 34 : 0.7483008652913357
Loss in iteration 35 : 0.6120849272770995
Loss in iteration 36 : 0.7580714749056879
Loss in iteration 37 : 0.6116630729231669
Loss in iteration 38 : 0.7486953162073368
Loss in iteration 39 : 0.6159738533922984
Loss in iteration 40 : 0.763342898365004
Loss in iteration 41 : 0.6179276704090028
Loss in iteration 42 : 0.7624094938318909
Loss in iteration 43 : 0.6152000236835907
Loss in iteration 44 : 0.7579398436138877
Loss in iteration 45 : 0.6149187303425033
Loss in iteration 46 : 0.7615250607026428
Loss in iteration 47 : 0.6207574856414442
Loss in iteration 48 : 0.7789296369404252
Loss in iteration 49 : 0.6249105793690082
Loss in iteration 50 : 0.7792624478134864
Loss in iteration 51 : 0.620809917905267
Loss in iteration 52 : 0.7701476657440878
Loss in iteration 53 : 0.6198945504826948
Loss in iteration 54 : 0.7728655595851684
Loss in iteration 55 : 0.6214856409088441
Loss in iteration 56 : 0.7755783225159436
Loss in iteration 57 : 0.6257462672887519
Loss in iteration 58 : 0.7869136601045091
Loss in iteration 59 : 0.6206451925999849
Loss in iteration 60 : 0.7693783487362302
Loss in iteration 61 : 0.6229499442539874
Loss in iteration 62 : 0.7800934537344281
Loss in iteration 63 : 0.6259444068159065
Loss in iteration 64 : 0.7897638767090245
Loss in iteration 65 : 0.6195218921607447
Loss in iteration 66 : 0.7748560066655179
Loss in iteration 67 : 0.6193696945693705
Loss in iteration 68 : 0.7716488942652621
Loss in iteration 69 : 0.6259827441403658
Loss in iteration 70 : 0.793462802248712
Loss in iteration 71 : 0.6191963742999894
Loss in iteration 72 : 0.7706408525585311
Loss in iteration 73 : 0.6239027104924936
Loss in iteration 74 : 0.7846969120745811
Loss in iteration 75 : 0.6257366404961309
Loss in iteration 76 : 0.7869770292620627
Loss in iteration 77 : 0.6227124391914168
Loss in iteration 78 : 0.7831166435450706
Loss in iteration 79 : 0.6200679697878598
Loss in iteration 80 : 0.7773073124663489
Loss in iteration 81 : 0.6224880397975492
Loss in iteration 82 : 0.7827070663875667
Loss in iteration 83 : 0.6204894925295125
Loss in iteration 84 : 0.7844705955700693
Loss in iteration 85 : 0.6191639364372379
Loss in iteration 86 : 0.783646265628877
Loss in iteration 87 : 0.6217257411639129
Loss in iteration 88 : 0.7878085957310287
Loss in iteration 89 : 0.620615201324634
Loss in iteration 90 : 0.782767110958309
Loss in iteration 91 : 0.6194976430312649
Loss in iteration 92 : 0.7847051822534171
Loss in iteration 93 : 0.6194873002238536
Loss in iteration 94 : 0.7809404691776564
Loss in iteration 95 : 0.620948930697274
Loss in iteration 96 : 0.7878407469540015
Loss in iteration 97 : 0.6215364331468107
Loss in iteration 98 : 0.7841370292078121
Loss in iteration 99 : 0.6167588288949195
Loss in iteration 100 : 0.77308827529745
Loss in iteration 101 : 0.6268459762005598
Loss in iteration 102 : 0.7915545588199083
Loss in iteration 103 : 0.6211415797588503
Loss in iteration 104 : 0.7782167505118891
Loss in iteration 105 : 0.6220733823239606
Loss in iteration 106 : 0.783067058714448
Loss in iteration 107 : 0.6186822179973963
Loss in iteration 108 : 0.7720987314630439
Loss in iteration 109 : 0.624392883109549
Loss in iteration 110 : 0.7884648650163913
Loss in iteration 111 : 0.6174306692603192
Loss in iteration 112 : 0.7734598442241555
Loss in iteration 113 : 0.6230454360238231
Loss in iteration 114 : 0.7833533318338429
Loss in iteration 115 : 0.6210211821779685
Loss in iteration 116 : 0.7781388409481123
Loss in iteration 117 : 0.6238434680661201
Loss in iteration 118 : 0.7862135870231541
Loss in iteration 119 : 0.6189538227530307
Loss in iteration 120 : 0.7762221495473218
Loss in iteration 121 : 0.619115619403905
Loss in iteration 122 : 0.7793929768567693
Loss in iteration 123 : 0.6221657652609578
Loss in iteration 124 : 0.791131489842515
Loss in iteration 125 : 0.615673518154732
Loss in iteration 126 : 0.7724221798671922
Loss in iteration 127 : 0.619688118249522
Loss in iteration 128 : 0.7843559651157925
Loss in iteration 129 : 0.6174272632080797
Loss in iteration 130 : 0.7764245034883511
Loss in iteration 131 : 0.6188698347810029
Loss in iteration 132 : 0.7853533801281519
Loss in iteration 133 : 0.6230538910856533
Loss in iteration 134 : 0.7910609381756668
Loss in iteration 135 : 0.6180947984879728
Loss in iteration 136 : 0.7772597276521084
Loss in iteration 137 : 0.6133794010567217
Loss in iteration 138 : 0.7701567493716828
Loss in iteration 139 : 0.6169043806017049
Loss in iteration 140 : 0.7833291524753022
Loss in iteration 141 : 0.6192523220909627
Loss in iteration 142 : 0.7862848192113758
Loss in iteration 143 : 0.6175354020201844
Loss in iteration 144 : 0.7798633532229569
Loss in iteration 145 : 0.6183944742406093
Loss in iteration 146 : 0.782713557853037
Loss in iteration 147 : 0.6162439186103565
Loss in iteration 148 : 0.7795444033577028
Loss in iteration 149 : 0.6169457725421049
Loss in iteration 150 : 0.7778876678741766
Loss in iteration 151 : 0.6155809811831432
Loss in iteration 152 : 0.777336761748142
Loss in iteration 153 : 0.6168299260915339
Loss in iteration 154 : 0.7829454312157352
Loss in iteration 155 : 0.6158045675997367
Loss in iteration 156 : 0.7722571163575036
Loss in iteration 157 : 0.6202446539245642
Loss in iteration 158 : 0.7880762242723496
Loss in iteration 159 : 0.6140871764545087
Loss in iteration 160 : 0.7697043904924984
Loss in iteration 161 : 0.6124445773730619
Loss in iteration 162 : 0.7714764281624982
Loss in iteration 163 : 0.6226688498723827
Loss in iteration 164 : 0.7993759475818223
Loss in iteration 165 : 0.6166199993967352
Loss in iteration 166 : 0.7726668404043038
Loss in iteration 167 : 0.6157517808322135
Loss in iteration 168 : 0.7774898220186293
Loss in iteration 169 : 0.6181651698252992
Loss in iteration 170 : 0.7799410267775699
Loss in iteration 171 : 0.6169958762163295
Loss in iteration 172 : 0.7783762783478454
Loss in iteration 173 : 0.6183662496131326
Loss in iteration 174 : 0.7775254784914798
Loss in iteration 175 : 0.6151110858353065
Loss in iteration 176 : 0.7782384181612538
Loss in iteration 177 : 0.6152470938312076
Loss in iteration 178 : 0.7790907404546168
Loss in iteration 179 : 0.6145687816663344
Loss in iteration 180 : 0.7775258941731729
Loss in iteration 181 : 0.6160197245901134
Loss in iteration 182 : 0.7807917483551002
Loss in iteration 183 : 0.6143177188337451
Loss in iteration 184 : 0.7765067192829925
Loss in iteration 185 : 0.6141058735201577
Loss in iteration 186 : 0.7759470893105028
Loss in iteration 187 : 0.6154443090965562
Loss in iteration 188 : 0.7813767155921248
Loss in iteration 189 : 0.6148007368027009
Loss in iteration 190 : 0.7771762351959174
Loss in iteration 191 : 0.6134717256004366
Loss in iteration 192 : 0.7770547927661287
Loss in iteration 193 : 0.6142512436062372
Loss in iteration 194 : 0.7785679421576092
Loss in iteration 195 : 0.6175811229862355
Loss in iteration 196 : 0.789607017101359
Loss in iteration 197 : 0.6140888160265479
Loss in iteration 198 : 0.7732038208233085
Loss in iteration 199 : 0.6084073872489845
Loss in iteration 200 : 0.7540455843101314
Testing accuracy  of updater 5 on alg 1 with rate 0.028 = 0.7905, training accuracy 0.7931369375202331, time elapsed: 2945 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.1542968756909182
Loss in iteration 3 : 0.8375076264757515
Loss in iteration 4 : 0.5329960697541308
Loss in iteration 5 : 0.4642500969450594
Loss in iteration 6 : 0.4496168667657899
Loss in iteration 7 : 0.4325097362498751
Loss in iteration 8 : 0.43596130462437394
Loss in iteration 9 : 0.4256749090747609
Loss in iteration 10 : 0.43427967657865957
Loss in iteration 11 : 0.42140669731056013
Loss in iteration 12 : 0.42917840135072804
Loss in iteration 13 : 0.41365463355895377
Loss in iteration 14 : 0.4155403904913052
Loss in iteration 15 : 0.40626963751843914
Loss in iteration 16 : 0.40762630010227774
Loss in iteration 17 : 0.40064131949330656
Loss in iteration 18 : 0.4028805836539067
Loss in iteration 19 : 0.394882265228852
Loss in iteration 20 : 0.4011383544909284
Loss in iteration 21 : 0.39343510524501407
Loss in iteration 22 : 0.40036206103895
Loss in iteration 23 : 0.39422196279887023
Loss in iteration 24 : 0.4026249375904168
Loss in iteration 25 : 0.39909788422857706
Loss in iteration 26 : 0.4121094551174141
Loss in iteration 27 : 0.4067702036330627
Loss in iteration 28 : 0.4184938574399032
Loss in iteration 29 : 0.4110098724795786
Loss in iteration 30 : 0.4201236675374565
Loss in iteration 31 : 0.40567248255636845
Loss in iteration 32 : 0.4141573133027898
Loss in iteration 33 : 0.40238951385092486
Loss in iteration 34 : 0.4065734338764511
Loss in iteration 35 : 0.39853556753750086
Loss in iteration 36 : 0.4045815780355399
Loss in iteration 37 : 0.4019306998935145
Loss in iteration 38 : 0.4127014950101178
Loss in iteration 39 : 0.40972800475379534
Loss in iteration 40 : 0.4254558370111178
Loss in iteration 41 : 0.41112854589821196
Loss in iteration 42 : 0.42367064672089894
Loss in iteration 43 : 0.40864986281410404
Loss in iteration 44 : 0.4147975648052042
Loss in iteration 45 : 0.40536426996767655
Loss in iteration 46 : 0.4114847562446912
Loss in iteration 47 : 0.4043580622641974
Loss in iteration 48 : 0.4131475122973406
Loss in iteration 49 : 0.40725105028495456
Loss in iteration 50 : 0.4175003943078208
Loss in iteration 51 : 0.41158099464738646
Loss in iteration 52 : 0.420085089771853
Loss in iteration 53 : 0.41043411265585955
Loss in iteration 54 : 0.4189011925675184
Loss in iteration 55 : 0.40921378951615334
Loss in iteration 56 : 0.4170615894717029
Loss in iteration 57 : 0.4061153040225115
Loss in iteration 58 : 0.41331229512581463
Loss in iteration 59 : 0.40490151021937015
Loss in iteration 60 : 0.41379961924400666
Loss in iteration 61 : 0.4064866687106329
Loss in iteration 62 : 0.4163805497523826
Loss in iteration 63 : 0.41011606713062126
Loss in iteration 64 : 0.4227708325634716
Loss in iteration 65 : 0.411741817128382
Loss in iteration 66 : 0.42320463408820125
Loss in iteration 67 : 0.4088578302748465
Loss in iteration 68 : 0.4172993668062069
Loss in iteration 69 : 0.4077910103764767
Loss in iteration 70 : 0.4156771346872798
Loss in iteration 71 : 0.4076863737437628
Loss in iteration 72 : 0.4169940526806159
Loss in iteration 73 : 0.4091515950694617
Loss in iteration 74 : 0.4182404372883238
Loss in iteration 75 : 0.40840640423957564
Loss in iteration 76 : 0.4189242040123541
Loss in iteration 77 : 0.40827042354504134
Loss in iteration 78 : 0.41772343350928826
Loss in iteration 79 : 0.4081768784543089
Loss in iteration 80 : 0.4168152632115121
Loss in iteration 81 : 0.4078003283017404
Loss in iteration 82 : 0.41670920764251473
Loss in iteration 83 : 0.40764997225774335
Loss in iteration 84 : 0.41665260766185763
Loss in iteration 85 : 0.4081753841186487
Loss in iteration 86 : 0.41834813073515337
Loss in iteration 87 : 0.40789795469550283
Loss in iteration 88 : 0.41606934467726264
Loss in iteration 89 : 0.40691008156606007
Loss in iteration 90 : 0.41415699999213895
Loss in iteration 91 : 0.4057878487639417
Loss in iteration 92 : 0.4150054901925894
Loss in iteration 93 : 0.4078577547465087
Loss in iteration 94 : 0.4201151218669815
Loss in iteration 95 : 0.40882615150913926
Loss in iteration 96 : 0.42069815004811084
Loss in iteration 97 : 0.40857996808174524
Loss in iteration 98 : 0.4189257043570379
Loss in iteration 99 : 0.40780548569308256
Loss in iteration 100 : 0.41677587259675314
Loss in iteration 101 : 0.4059786248705153
Loss in iteration 102 : 0.4147918058756902
Loss in iteration 103 : 0.4044416246671272
Loss in iteration 104 : 0.41267396546566965
Loss in iteration 105 : 0.4047546247341859
Loss in iteration 106 : 0.41448133059556763
Loss in iteration 107 : 0.40636942789255015
Loss in iteration 108 : 0.4185460207408426
Loss in iteration 109 : 0.40983446302573123
Loss in iteration 110 : 0.4221787027929913
Loss in iteration 111 : 0.4086318603564484
Loss in iteration 112 : 0.41839616250898554
Loss in iteration 113 : 0.4075486259122842
Loss in iteration 114 : 0.41617373853167977
Loss in iteration 115 : 0.403757533659346
Loss in iteration 116 : 0.41219118329435184
Loss in iteration 117 : 0.4043575374985278
Loss in iteration 118 : 0.41388025164883396
Loss in iteration 119 : 0.40453563028041806
Loss in iteration 120 : 0.4162403946623341
Loss in iteration 121 : 0.4068578142056314
Loss in iteration 122 : 0.4194324528784816
Loss in iteration 123 : 0.4077536114249517
Loss in iteration 124 : 0.4181070832074207
Loss in iteration 125 : 0.4063759472431109
Loss in iteration 126 : 0.4166378198761384
Loss in iteration 127 : 0.404256013486363
Loss in iteration 128 : 0.41338199031300493
Loss in iteration 129 : 0.4043545508139326
Loss in iteration 130 : 0.41427378649318836
Loss in iteration 131 : 0.4047910045601556
Loss in iteration 132 : 0.41546729293486984
Loss in iteration 133 : 0.4056957801487412
Loss in iteration 134 : 0.417210672680211
Loss in iteration 135 : 0.40470142159476696
Loss in iteration 136 : 0.4148342735189965
Loss in iteration 137 : 0.4046535470050411
Loss in iteration 138 : 0.4158695438698411
Loss in iteration 139 : 0.40559787959851357
Loss in iteration 140 : 0.4159653318733624
Loss in iteration 141 : 0.40474742033455974
Loss in iteration 142 : 0.41542909837888276
Loss in iteration 143 : 0.40446629405109213
Loss in iteration 144 : 0.41499010229579814
Loss in iteration 145 : 0.4045006459738915
Loss in iteration 146 : 0.4153069816928371
Loss in iteration 147 : 0.40474937379421877
Loss in iteration 148 : 0.4157290206918817
Loss in iteration 149 : 0.4049477033099393
Loss in iteration 150 : 0.41635174626271415
Loss in iteration 151 : 0.4042862670505511
Loss in iteration 152 : 0.41399531113617966
Loss in iteration 153 : 0.40286347040340625
Loss in iteration 154 : 0.4128980930444052
Loss in iteration 155 : 0.4044140099715826
Loss in iteration 156 : 0.41668793119022596
Loss in iteration 157 : 0.40446446034127986
Loss in iteration 158 : 0.416498438695642
Loss in iteration 159 : 0.4032867532831635
Loss in iteration 160 : 0.4130437628817131
Loss in iteration 161 : 0.4036070153142036
Loss in iteration 162 : 0.41596670381968615
Loss in iteration 163 : 0.4038424241946653
Loss in iteration 164 : 0.4159702252374281
Loss in iteration 165 : 0.4043450762631206
Loss in iteration 166 : 0.4174674298421531
Loss in iteration 167 : 0.4026081314426444
Loss in iteration 168 : 0.41094816900089315
Loss in iteration 169 : 0.402906667348844
Loss in iteration 170 : 0.41289447691551917
Loss in iteration 171 : 0.40317028277582656
Loss in iteration 172 : 0.4154723478964794
Loss in iteration 173 : 0.40473598765314095
Loss in iteration 174 : 0.4191832986532296
Loss in iteration 175 : 0.40287483318513645
Loss in iteration 176 : 0.41369873472139446
Loss in iteration 177 : 0.4025633450247259
Loss in iteration 178 : 0.41211268401707296
Loss in iteration 179 : 0.4025346679388124
Loss in iteration 180 : 0.4133130405661115
Loss in iteration 181 : 0.4041724241561117
Loss in iteration 182 : 0.41906928520885667
Loss in iteration 183 : 0.4022406145173607
Loss in iteration 184 : 0.40913424944436483
Loss in iteration 185 : 0.40183288634552344
Loss in iteration 186 : 0.4109249743143117
Loss in iteration 187 : 0.4036341731924883
Loss in iteration 188 : 0.41872611752646893
Loss in iteration 189 : 0.40408872566438275
Loss in iteration 190 : 0.41826330339042006
Loss in iteration 191 : 0.4019618193503236
Loss in iteration 192 : 0.40932462390774216
Loss in iteration 193 : 0.4011443536732574
Loss in iteration 194 : 0.40967319887996695
Loss in iteration 195 : 0.40325861309346944
Loss in iteration 196 : 0.4197731297286798
Loss in iteration 197 : 0.4041934221497716
Loss in iteration 198 : 0.41780015278232036
Loss in iteration 199 : 0.4026741449405798
Loss in iteration 200 : 0.410109186493271
Testing accuracy  of updater 5 on alg 1 with rate 0.007 = 0.7955, training accuracy 0.8258336031078019, time elapsed: 3048 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.96584946543888
Loss in iteration 3 : 0.7216164325014337
Loss in iteration 4 : 0.5079495838237105
Loss in iteration 5 : 0.48898356175059693
Loss in iteration 6 : 0.4732227933335036
Loss in iteration 7 : 0.45794167677319364
Loss in iteration 8 : 0.4433911555958291
Loss in iteration 9 : 0.43070429714896935
Loss in iteration 10 : 0.4194266886493664
Loss in iteration 11 : 0.41015366814034887
Loss in iteration 12 : 0.40315898522674914
Loss in iteration 13 : 0.3980540678527704
Loss in iteration 14 : 0.396467658432275
Loss in iteration 15 : 0.3959608175043326
Loss in iteration 16 : 0.401589915006769
Loss in iteration 17 : 0.4040987618855914
Loss in iteration 18 : 0.4082541279114443
Loss in iteration 19 : 0.40564963147719724
Loss in iteration 20 : 0.40571176715308166
Loss in iteration 21 : 0.3969491456206421
Loss in iteration 22 : 0.39517802663903434
Loss in iteration 23 : 0.38916259952258747
Loss in iteration 24 : 0.3875555928053267
Loss in iteration 25 : 0.38398791313368685
Loss in iteration 26 : 0.383469696342295
Loss in iteration 27 : 0.3817244510561869
Loss in iteration 28 : 0.382278209177817
Loss in iteration 29 : 0.3821234491614171
Loss in iteration 30 : 0.38527271486247966
Loss in iteration 31 : 0.3878452654372196
Loss in iteration 32 : 0.3941745301736099
Loss in iteration 33 : 0.3935686246355256
Loss in iteration 34 : 0.40495503465843835
Loss in iteration 35 : 0.4010155069615796
Loss in iteration 36 : 0.40894554876168454
Loss in iteration 37 : 0.39455940881092605
Loss in iteration 38 : 0.3945521598927015
Loss in iteration 39 : 0.3854269872933751
Loss in iteration 40 : 0.38819547968263485
Loss in iteration 41 : 0.3845071004142363
Loss in iteration 42 : 0.3891059533257946
Loss in iteration 43 : 0.3864223702110286
Loss in iteration 44 : 0.3951983856050068
Loss in iteration 45 : 0.3923394587265832
Loss in iteration 46 : 0.39958878276285953
Loss in iteration 47 : 0.39831711499101524
Loss in iteration 48 : 0.4051059998855648
Loss in iteration 49 : 0.39519908963320644
Loss in iteration 50 : 0.3969067363679789
Loss in iteration 51 : 0.3900813273807705
Loss in iteration 52 : 0.392703362784051
Loss in iteration 53 : 0.38484632997940127
Loss in iteration 54 : 0.38686774758559994
Loss in iteration 55 : 0.3844324469985823
Loss in iteration 56 : 0.3886579881639323
Loss in iteration 57 : 0.3869823414977497
Loss in iteration 58 : 0.39832486138601453
Loss in iteration 59 : 0.4000547100844132
Loss in iteration 60 : 0.4105745478698316
Loss in iteration 61 : 0.40002797256986394
Loss in iteration 62 : 0.404041999437534
Loss in iteration 63 : 0.3917173574311138
Loss in iteration 64 : 0.39372388944978676
Loss in iteration 65 : 0.3859477187417764
Loss in iteration 66 : 0.38624044846418204
Loss in iteration 67 : 0.38133527349396223
Loss in iteration 68 : 0.38314450988595755
Loss in iteration 69 : 0.3829366613529825
Loss in iteration 70 : 0.38825802362760825
Loss in iteration 71 : 0.39052484995562026
Loss in iteration 72 : 0.40599075795748424
Loss in iteration 73 : 0.4081940094194575
Loss in iteration 74 : 0.41721030486243393
Loss in iteration 75 : 0.39729901928196776
Loss in iteration 76 : 0.3969673561661513
Loss in iteration 77 : 0.38727011878540474
Loss in iteration 78 : 0.38693362166877926
Loss in iteration 79 : 0.3805898540481828
Loss in iteration 80 : 0.38105833462118294
Loss in iteration 81 : 0.38164385136948176
Loss in iteration 82 : 0.3847075362284717
Loss in iteration 83 : 0.3845574008282388
Loss in iteration 84 : 0.3946730283616526
Loss in iteration 85 : 0.3992889237072498
Loss in iteration 86 : 0.41787715749024495
Loss in iteration 87 : 0.4037262143401276
Loss in iteration 88 : 0.408512298706152
Loss in iteration 89 : 0.39360025575789276
Loss in iteration 90 : 0.3934825147648164
Loss in iteration 91 : 0.3851437957784543
Loss in iteration 92 : 0.38488675514138454
Loss in iteration 93 : 0.3792083388875682
Loss in iteration 94 : 0.38058486066311753
Loss in iteration 95 : 0.38092508043263196
Loss in iteration 96 : 0.3847466820263644
Loss in iteration 97 : 0.3853317802331656
Loss in iteration 98 : 0.39627565634690215
Loss in iteration 99 : 0.4016768332832469
Loss in iteration 100 : 0.4192801701169252
Loss in iteration 101 : 0.4015422486294636
Loss in iteration 102 : 0.4051968450226064
Loss in iteration 103 : 0.3907783358359506
Loss in iteration 104 : 0.39040141585170923
Loss in iteration 105 : 0.3817689415540613
Loss in iteration 106 : 0.3815737182454418
Loss in iteration 107 : 0.37857764373356567
Loss in iteration 108 : 0.3816542596450348
Loss in iteration 109 : 0.38165226311645223
Loss in iteration 110 : 0.3883806506329235
Loss in iteration 111 : 0.3895049935558502
Loss in iteration 112 : 0.4056867335854148
Loss in iteration 113 : 0.4049947619122283
Loss in iteration 114 : 0.41519947716719624
Loss in iteration 115 : 0.3947440961547116
Loss in iteration 116 : 0.39608249236817394
Loss in iteration 117 : 0.3859546976593835
Loss in iteration 118 : 0.385833425108017
Loss in iteration 119 : 0.3789855868019894
Loss in iteration 120 : 0.3812864069095629
Loss in iteration 121 : 0.3791457756147337
Loss in iteration 122 : 0.3838879995652816
Loss in iteration 123 : 0.3840102193595974
Loss in iteration 124 : 0.39553377539919177
Loss in iteration 125 : 0.39836383541782766
Loss in iteration 126 : 0.41493050815958116
Loss in iteration 127 : 0.3986437029455987
Loss in iteration 128 : 0.4029815426611845
Loss in iteration 129 : 0.3893479966200939
Loss in iteration 130 : 0.38878133647689406
Loss in iteration 131 : 0.3805914804358408
Loss in iteration 132 : 0.38185946566393
Loss in iteration 133 : 0.37851143769772916
Loss in iteration 134 : 0.3828301073368534
Loss in iteration 135 : 0.3814691321008684
Loss in iteration 136 : 0.38873362204747247
Loss in iteration 137 : 0.3911060211283545
Loss in iteration 138 : 0.40711766240561525
Loss in iteration 139 : 0.4016065181752509
Loss in iteration 140 : 0.41061421546120624
Loss in iteration 141 : 0.3928920511291937
Loss in iteration 142 : 0.3940104584664837
Loss in iteration 143 : 0.3843822618688912
Loss in iteration 144 : 0.383717430446318
Loss in iteration 145 : 0.37722109819762284
Loss in iteration 146 : 0.38044800376414745
Loss in iteration 147 : 0.37933346064205065
Loss in iteration 148 : 0.38590896283030546
Loss in iteration 149 : 0.38514223501705597
Loss in iteration 150 : 0.3940409652543007
Loss in iteration 151 : 0.39511745121442143
Loss in iteration 152 : 0.4103120853413228
Loss in iteration 153 : 0.3975270216439065
Loss in iteration 154 : 0.4025154694949766
Loss in iteration 155 : 0.38675815350942955
Loss in iteration 156 : 0.385858186924001
Loss in iteration 157 : 0.37845701931098225
Loss in iteration 158 : 0.3814087776645594
Loss in iteration 159 : 0.3784878291305629
Loss in iteration 160 : 0.38473091170137697
Loss in iteration 161 : 0.38382335711426246
Loss in iteration 162 : 0.3907044006526581
Loss in iteration 163 : 0.3923511031986273
Loss in iteration 164 : 0.4079015180989181
Loss in iteration 165 : 0.3973782216931434
Loss in iteration 166 : 0.40364417982616724
Loss in iteration 167 : 0.38772013867442895
Loss in iteration 168 : 0.3879842068674385
Loss in iteration 169 : 0.3802485087911322
Loss in iteration 170 : 0.38261312886073845
Loss in iteration 171 : 0.3782619643184372
Loss in iteration 172 : 0.3833886139703557
Loss in iteration 173 : 0.3828063218955603
Loss in iteration 174 : 0.3902908119755527
Loss in iteration 175 : 0.389480370562923
Loss in iteration 176 : 0.3994536283888373
Loss in iteration 177 : 0.39407334678625733
Loss in iteration 178 : 0.4040502805900156
Loss in iteration 179 : 0.3897757141413673
Loss in iteration 180 : 0.3913323036443283
Loss in iteration 181 : 0.3834926898279509
Loss in iteration 182 : 0.38511562184848136
Loss in iteration 183 : 0.379921592667028
Loss in iteration 184 : 0.38482797717551476
Loss in iteration 185 : 0.3818368631253334
Loss in iteration 186 : 0.3880602632108212
Loss in iteration 187 : 0.38601314730429265
Loss in iteration 188 : 0.39313712176218785
Loss in iteration 189 : 0.39122456774257564
Loss in iteration 190 : 0.4013462538264636
Loss in iteration 191 : 0.3907061780030083
Loss in iteration 192 : 0.3929154200447817
Loss in iteration 193 : 0.3859322849611245
Loss in iteration 194 : 0.38721060582351985
Loss in iteration 195 : 0.3813742134403712
Loss in iteration 196 : 0.38639564904616996
Loss in iteration 197 : 0.3823984967466273
Loss in iteration 198 : 0.3882708067797095
Loss in iteration 199 : 0.38540209339151554
Loss in iteration 200 : 0.3911450260467777
Testing accuracy  of updater 5 on alg 1 with rate 0.0049 = 0.79475, training accuracy 0.8306895435415992, time elapsed: 2373 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7214347164412229
Loss in iteration 3 : 0.5787628217674221
Loss in iteration 4 : 0.5255491667174647
Loss in iteration 5 : 0.5165080402306913
Loss in iteration 6 : 0.5088526713298313
Loss in iteration 7 : 0.5012854276918127
Loss in iteration 8 : 0.49377838221445264
Loss in iteration 9 : 0.48602481741092307
Loss in iteration 10 : 0.47799210872485176
Loss in iteration 11 : 0.46966177483134486
Loss in iteration 12 : 0.46099970671081497
Loss in iteration 13 : 0.45198011953977135
Loss in iteration 14 : 0.4426451423482933
Loss in iteration 15 : 0.4330100391072683
Loss in iteration 16 : 0.4237254449261782
Loss in iteration 17 : 0.4154677984158316
Loss in iteration 18 : 0.40819748995863525
Loss in iteration 19 : 0.4022706890542235
Loss in iteration 20 : 0.39746627746992574
Loss in iteration 21 : 0.39366983461535243
Loss in iteration 22 : 0.39068821580627616
Loss in iteration 23 : 0.3883343018229753
Loss in iteration 24 : 0.38654437827814814
Loss in iteration 25 : 0.3847888631690143
Loss in iteration 26 : 0.3832796090397776
Loss in iteration 27 : 0.38197963012477243
Loss in iteration 28 : 0.3808276980981273
Loss in iteration 29 : 0.37982203297690137
Loss in iteration 30 : 0.3789495790389998
Loss in iteration 31 : 0.3785122172962763
Loss in iteration 32 : 0.3785628192816973
Loss in iteration 33 : 0.37983744368948524
Loss in iteration 34 : 0.38493201438612806
Loss in iteration 35 : 0.3932935966174758
Loss in iteration 36 : 0.4023270536761482
Loss in iteration 37 : 0.39133023696736485
Loss in iteration 38 : 0.387351142382043
Loss in iteration 39 : 0.3805181614984321
Loss in iteration 40 : 0.3786669752957518
Loss in iteration 41 : 0.3770082065477282
Loss in iteration 42 : 0.3762001992383031
Loss in iteration 43 : 0.37627059760278475
Loss in iteration 44 : 0.3765027637491749
Loss in iteration 45 : 0.37722749649305576
Loss in iteration 46 : 0.37809137991394137
Loss in iteration 47 : 0.3788402897151237
Loss in iteration 48 : 0.38002575045355713
Loss in iteration 49 : 0.3802640444723657
Loss in iteration 50 : 0.3827808535558735
Loss in iteration 51 : 0.38311981524744326
Loss in iteration 52 : 0.3860059275617557
Loss in iteration 53 : 0.3828131818206824
Loss in iteration 54 : 0.3830393446809229
Loss in iteration 55 : 0.3796472980120866
Loss in iteration 56 : 0.3785223131984361
Loss in iteration 57 : 0.3777195219802195
Loss in iteration 58 : 0.37795751558954505
Loss in iteration 59 : 0.3779696760641895
Loss in iteration 60 : 0.37858855878356634
Loss in iteration 61 : 0.3787320781967886
Loss in iteration 62 : 0.3799143697204164
Loss in iteration 63 : 0.3803628625341332
Loss in iteration 64 : 0.3821733170677686
Loss in iteration 65 : 0.38187732560031473
Loss in iteration 66 : 0.38330414002274477
Loss in iteration 67 : 0.3809255756350426
Loss in iteration 68 : 0.3810707173554623
Loss in iteration 69 : 0.37979356965158784
Loss in iteration 70 : 0.37882199815759204
Loss in iteration 71 : 0.37798934482565216
Loss in iteration 72 : 0.3781908083782839
Loss in iteration 73 : 0.3782539326845119
Loss in iteration 74 : 0.37869367724739067
Loss in iteration 75 : 0.3792748221631684
Loss in iteration 76 : 0.3797101690963815
Loss in iteration 77 : 0.37996677791331324
Loss in iteration 78 : 0.3819783880669956
Loss in iteration 79 : 0.38128516794490447
Loss in iteration 80 : 0.3823120812133961
Loss in iteration 81 : 0.38003948156004286
Loss in iteration 82 : 0.3802311370485417
Loss in iteration 83 : 0.37941386400467975
Loss in iteration 84 : 0.3783483515069599
Loss in iteration 85 : 0.37710828070056757
Loss in iteration 86 : 0.37713762415089325
Loss in iteration 87 : 0.3773754438248313
Loss in iteration 88 : 0.3785944483369481
Loss in iteration 89 : 0.37890429097059697
Loss in iteration 90 : 0.37991843720626617
Loss in iteration 91 : 0.3811381904273114
Loss in iteration 92 : 0.38296099446876614
Loss in iteration 93 : 0.3803402653192331
Loss in iteration 94 : 0.38053910654263784
Loss in iteration 95 : 0.3793479862318725
Loss in iteration 96 : 0.3794325352131148
Loss in iteration 97 : 0.3786634640453782
Loss in iteration 98 : 0.3777515187658774
Loss in iteration 99 : 0.3767472433490328
Loss in iteration 100 : 0.37677284350895685
Loss in iteration 101 : 0.37696847900421776
Loss in iteration 102 : 0.37816943301710076
Loss in iteration 103 : 0.37947020512382273
Loss in iteration 104 : 0.3814740923420158
Loss in iteration 105 : 0.3803736676174764
Loss in iteration 106 : 0.38182191154195333
Loss in iteration 107 : 0.37902117208106767
Loss in iteration 108 : 0.3790627764383439
Loss in iteration 109 : 0.37779036077280237
Loss in iteration 110 : 0.3774392039391116
Loss in iteration 111 : 0.3773576913161274
Loss in iteration 112 : 0.3777411114812923
Loss in iteration 113 : 0.37805378310439053
Loss in iteration 114 : 0.37880751791248485
Loss in iteration 115 : 0.3788696790313727
Loss in iteration 116 : 0.3796285576288119
Loss in iteration 117 : 0.37873073887838193
Loss in iteration 118 : 0.3792453746575166
Loss in iteration 119 : 0.37806820885134906
Loss in iteration 120 : 0.378306208306233
Loss in iteration 121 : 0.3775771283599389
Loss in iteration 122 : 0.3779767693472276
Loss in iteration 123 : 0.3773132947070128
Loss in iteration 124 : 0.3782552389766615
Loss in iteration 125 : 0.3779426434345599
Loss in iteration 126 : 0.3787588005769617
Loss in iteration 127 : 0.3783814199432339
Loss in iteration 128 : 0.3785306771206876
Loss in iteration 129 : 0.3774551198729159
Loss in iteration 130 : 0.3779587316266366
Loss in iteration 131 : 0.3772767570084275
Loss in iteration 132 : 0.37785684272881936
Loss in iteration 133 : 0.3773221344834954
Loss in iteration 134 : 0.3784536268461821
Loss in iteration 135 : 0.3778798281508936
Loss in iteration 136 : 0.37815032453871433
Loss in iteration 137 : 0.3770022737609905
Loss in iteration 138 : 0.37771345321932537
Loss in iteration 139 : 0.3770716110879254
Loss in iteration 140 : 0.37816623937452815
Loss in iteration 141 : 0.37703994356540427
Loss in iteration 142 : 0.3780121978158501
Loss in iteration 143 : 0.37674426066351757
Loss in iteration 144 : 0.3774074106230465
Loss in iteration 145 : 0.37715244639892664
Loss in iteration 146 : 0.3785809775624408
Loss in iteration 147 : 0.3770218422413089
Loss in iteration 148 : 0.37834359728015093
Loss in iteration 149 : 0.3766885415636425
Loss in iteration 150 : 0.3777969973291347
Loss in iteration 151 : 0.376267367857414
Loss in iteration 152 : 0.3764290105061885
Loss in iteration 153 : 0.3766439642865656
Loss in iteration 154 : 0.3779588853857263
Loss in iteration 155 : 0.3766776931854075
Loss in iteration 156 : 0.37800392332003924
Loss in iteration 157 : 0.3765743238686153
Loss in iteration 158 : 0.3778274711563402
Loss in iteration 159 : 0.3764874292417411
Loss in iteration 160 : 0.3774567474937018
Loss in iteration 161 : 0.37580520299023096
Loss in iteration 162 : 0.37609149930562613
Loss in iteration 163 : 0.37654864974170027
Loss in iteration 164 : 0.37725654733213415
Loss in iteration 165 : 0.3759899516941043
Loss in iteration 166 : 0.37695591556229585
Loss in iteration 167 : 0.37592131397827333
Loss in iteration 168 : 0.37697114863938364
Loss in iteration 169 : 0.3758454096857072
Loss in iteration 170 : 0.3769798891565629
Loss in iteration 171 : 0.37597534611878186
Loss in iteration 172 : 0.3772339847552237
Loss in iteration 173 : 0.37585385595225956
Loss in iteration 174 : 0.3771080496323535
Loss in iteration 175 : 0.37553862613462496
Loss in iteration 176 : 0.3769113116173888
Loss in iteration 177 : 0.37564553195336503
Loss in iteration 178 : 0.37724718478901964
Loss in iteration 179 : 0.37611501687380366
Loss in iteration 180 : 0.3772307816590615
Loss in iteration 181 : 0.37567590082993374
Loss in iteration 182 : 0.37670923654163935
Loss in iteration 183 : 0.37522129596666703
Loss in iteration 184 : 0.3764782275231869
Loss in iteration 185 : 0.3751481758592027
Loss in iteration 186 : 0.3764220094749699
Loss in iteration 187 : 0.3751842771668784
Loss in iteration 188 : 0.3765817272422099
Loss in iteration 189 : 0.37548828209279655
Loss in iteration 190 : 0.3770387631995664
Loss in iteration 191 : 0.37523041046753985
Loss in iteration 192 : 0.3766315274597114
Loss in iteration 193 : 0.3750548017482963
Loss in iteration 194 : 0.3765544198711177
Loss in iteration 195 : 0.375013925647008
Loss in iteration 196 : 0.37665798972293496
Loss in iteration 197 : 0.3748902833053275
Loss in iteration 198 : 0.3762264820784777
Loss in iteration 199 : 0.3751024654858865
Loss in iteration 200 : 0.37668705489255067
Testing accuracy  of updater 5 on alg 1 with rate 0.0028000000000000004 = 0.791, training accuracy 0.8381353188734219, time elapsed: 2200 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6565937839712387
Loss in iteration 3 : 0.5665088346791681
Loss in iteration 4 : 0.5613177082629195
Loss in iteration 5 : 0.5589012316326525
Loss in iteration 6 : 0.5566075228382785
Loss in iteration 7 : 0.5542824667231651
Loss in iteration 8 : 0.5519133703755821
Testing accuracy  of updater 5 on alg 1 with rate 7.000000000000001E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 84 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.255315226924871
Loss in iteration 3 : 2.964394697481796
Loss in iteration 4 : 3.002448526420269
Loss in iteration 5 : 2.60516507256122
Loss in iteration 6 : 1.900306715729107
Loss in iteration 7 : 1.0095218402391004
Loss in iteration 8 : 0.5521297555312628
Loss in iteration 9 : 0.8492357708222547
Loss in iteration 10 : 1.368175910103327
Loss in iteration 11 : 1.4891324955089666
Loss in iteration 12 : 1.2212116941434097
Loss in iteration 13 : 0.8811017693887412
Loss in iteration 14 : 0.7135465545812254
Loss in iteration 15 : 0.7451835272389278
Loss in iteration 16 : 0.8738308186377741
Loss in iteration 17 : 1.0045947348727426
Loss in iteration 18 : 1.0801884544384073
Loss in iteration 19 : 1.078433378508401
Loss in iteration 20 : 1.0114190468679063
Loss in iteration 21 : 0.9146864689282802
Loss in iteration 22 : 0.8242245641586543
Loss in iteration 23 : 0.7721517164577775
Loss in iteration 24 : 0.7800974512868041
Loss in iteration 25 : 0.8359479098066217
Loss in iteration 26 : 0.8908085563749465
Loss in iteration 27 : 0.9029468182490549
Loss in iteration 28 : 0.8651783514272422
Loss in iteration 29 : 0.7987092730128097
Loss in iteration 30 : 0.7446003218535258
Loss in iteration 31 : 0.7269785152470144
Loss in iteration 32 : 0.7408254203822419
Loss in iteration 33 : 0.7610154786410676
Loss in iteration 34 : 0.7676478602112187
Loss in iteration 35 : 0.7510377790777044
Loss in iteration 36 : 0.7140550048346637
Loss in iteration 37 : 0.6708373290027322
Loss in iteration 38 : 0.6394542068462352
Loss in iteration 39 : 0.6339606996813779
Loss in iteration 40 : 0.6417975029274883
Loss in iteration 41 : 0.6388322665901197
Loss in iteration 42 : 0.6097851808527192
Loss in iteration 43 : 0.5710088648703509
Loss in iteration 44 : 0.5453990954661727
Loss in iteration 45 : 0.539850635875805
Loss in iteration 46 : 0.5388558318348029
Loss in iteration 47 : 0.5207122907216186
Loss in iteration 48 : 0.48634104960122587
Loss in iteration 49 : 0.46229408869200994
Loss in iteration 50 : 0.45803290758886955
Loss in iteration 51 : 0.45345509059809325
Loss in iteration 52 : 0.4248071502590181
Loss in iteration 53 : 0.40206082715636415
Loss in iteration 54 : 0.4050725341357943
Loss in iteration 55 : 0.39617388603215353
Loss in iteration 56 : 0.36978002828239975
Loss in iteration 57 : 0.3884399966257077
Loss in iteration 58 : 0.3707924322817941
Loss in iteration 59 : 0.38638707902410135
Loss in iteration 60 : 0.3822945204971191
Loss in iteration 61 : 0.40228818337271854
Loss in iteration 62 : 0.38137296515657937
Loss in iteration 63 : 0.39556511112049453
Loss in iteration 64 : 0.38073953889266093
Loss in iteration 65 : 0.37899840890504
Loss in iteration 66 : 0.3741657446803799
Loss in iteration 67 : 0.36996027273265336
Loss in iteration 68 : 0.3682448155015317
Loss in iteration 69 : 0.37018631228550986
Loss in iteration 70 : 0.3667392661045107
Loss in iteration 71 : 0.3728220918068055
Loss in iteration 72 : 0.36852887359907116
Loss in iteration 73 : 0.3718427035921303
Loss in iteration 74 : 0.3718625742751309
Loss in iteration 75 : 0.3692803364351071
Loss in iteration 76 : 0.3715551288984039
Loss in iteration 77 : 0.3680720326391685
Loss in iteration 78 : 0.3686041941398574
Loss in iteration 79 : 0.3671780996217001
Loss in iteration 80 : 0.365554896228241
Loss in iteration 81 : 0.3660371169155914
Loss in iteration 82 : 0.36358539031285103
Loss in iteration 83 : 0.36585220608188757
Loss in iteration 84 : 0.36364275629166753
Loss in iteration 85 : 0.3660083365996717
Loss in iteration 86 : 0.36400332770454535
Loss in iteration 87 : 0.36645781679514156
Loss in iteration 88 : 0.36421986940844747
Loss in iteration 89 : 0.3655755517139541
Loss in iteration 90 : 0.36401957269470664
Loss in iteration 91 : 0.36388905992379655
Loss in iteration 92 : 0.36366057511256344
Loss in iteration 93 : 0.3630426555838562
Loss in iteration 94 : 0.3632235632340774
Loss in iteration 95 : 0.36291242767692256
Loss in iteration 96 : 0.3631076418976748
Loss in iteration 97 : 0.3631571264395752
Loss in iteration 98 : 0.36328628992351275
Loss in iteration 99 : 0.3632115773940817
Loss in iteration 100 : 0.3632975620726735
Loss in iteration 101 : 0.36318726884832075
Loss in iteration 102 : 0.36309058636291375
Loss in iteration 103 : 0.36297245863371186
Loss in iteration 104 : 0.36288374339113416
Loss in iteration 105 : 0.3627914061501677
Loss in iteration 106 : 0.362754435129033
Loss in iteration 107 : 0.36272430532151684
Loss in iteration 108 : 0.36272745482778046
Loss in iteration 109 : 0.3627406977007464
Loss in iteration 110 : 0.3628102374095584
Loss in iteration 111 : 0.36279462325896095
Loss in iteration 112 : 0.36283914851284416
Loss in iteration 113 : 0.3628446521245181
Loss in iteration 114 : 0.3628248976981743
Loss in iteration 115 : 0.3628492076693293
Loss in iteration 116 : 0.36280417615916
Loss in iteration 117 : 0.3627823586516364
Loss in iteration 118 : 0.3627986148181028
Loss in iteration 119 : 0.3627279496913674
Loss in iteration 120 : 0.362798292842529
Loss in iteration 121 : 0.3627588736352711
Testing accuracy  of updater 6 on alg 1 with rate 0.056 = 0.78975, training accuracy 0.842667529944966, time elapsed: 1479 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3200684674055576
Loss in iteration 3 : 1.8131351591562785
Loss in iteration 4 : 1.8947502870117503
Loss in iteration 5 : 1.61904087485805
Loss in iteration 6 : 1.0373739660498313
Loss in iteration 7 : 0.5215431613339977
Loss in iteration 8 : 0.6506057125703619
Loss in iteration 9 : 1.067601377676463
Loss in iteration 10 : 1.1642532159100278
Loss in iteration 11 : 0.9259627842557068
Loss in iteration 12 : 0.6917994084787197
Loss in iteration 13 : 0.6502171275986113
Loss in iteration 14 : 0.7510771622542682
Loss in iteration 15 : 0.8729813950595011
Loss in iteration 16 : 0.9442982432521472
Loss in iteration 17 : 0.9383335144151436
Loss in iteration 18 : 0.872716495947172
Loss in iteration 19 : 0.7904623777241792
Loss in iteration 20 : 0.7295083194657532
Loss in iteration 21 : 0.7196004327753037
Loss in iteration 22 : 0.7623453094469326
Loss in iteration 23 : 0.8142905339589775
Loss in iteration 24 : 0.827551088796477
Loss in iteration 25 : 0.7939534937411278
Loss in iteration 26 : 0.7363164708448089
Loss in iteration 27 : 0.6943446322946917
Loss in iteration 28 : 0.6872148444975547
Loss in iteration 29 : 0.7021666362384469
Loss in iteration 30 : 0.7157455221182935
Loss in iteration 31 : 0.7120673401539596
Loss in iteration 32 : 0.6866696472964356
Loss in iteration 33 : 0.6491329914036955
Loss in iteration 34 : 0.6158439620200984
Loss in iteration 35 : 0.6042345017698151
Loss in iteration 36 : 0.6077539327898424
Loss in iteration 37 : 0.6066407905497182
Loss in iteration 38 : 0.5856891003993472
Loss in iteration 39 : 0.5535420083345051
Loss in iteration 40 : 0.5283302298829702
Loss in iteration 41 : 0.5190361299361875
Loss in iteration 42 : 0.5172438523242582
Loss in iteration 43 : 0.503674453549465
Loss in iteration 44 : 0.47636132266323394
Loss in iteration 45 : 0.4536394632703702
Loss in iteration 46 : 0.4458675243812162
Loss in iteration 47 : 0.44210883076814145
Loss in iteration 48 : 0.4233085461725868
Loss in iteration 49 : 0.40076467980911495
Loss in iteration 50 : 0.39605782453584754
Loss in iteration 51 : 0.39405503441228557
Loss in iteration 52 : 0.3751284307074867
Loss in iteration 53 : 0.37276607736923956
Loss in iteration 54 : 0.37734473748013897
Loss in iteration 55 : 0.36609354863213445
Loss in iteration 56 : 0.3821055195492537
Loss in iteration 57 : 0.37341592324210915
Loss in iteration 58 : 0.3894102451033781
Loss in iteration 59 : 0.3787340937623205
Loss in iteration 60 : 0.3877029209052846
Loss in iteration 61 : 0.3758888594123189
Loss in iteration 62 : 0.3790919593116258
Loss in iteration 63 : 0.36899464803777204
Loss in iteration 64 : 0.3708137196210859
Loss in iteration 65 : 0.36501739114111
Loss in iteration 66 : 0.36770157124613445
Loss in iteration 67 : 0.3661613663746369
Loss in iteration 68 : 0.3661071727531981
Loss in iteration 69 : 0.3688973752396509
Loss in iteration 70 : 0.36639582009826727
Loss in iteration 71 : 0.36829303612664566
Loss in iteration 72 : 0.3684218308735872
Loss in iteration 73 : 0.3669269426321414
Loss in iteration 74 : 0.3680590076832314
Testing accuracy  of updater 6 on alg 1 with rate 0.0392 = 0.791, training accuracy 0.8442861767562317, time elapsed: 700 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8914613487763373
Loss in iteration 3 : 1.1674721961371248
Loss in iteration 4 : 1.2121042977185794
Loss in iteration 5 : 1.0530862895340538
Loss in iteration 6 : 0.7162341816264022
Loss in iteration 7 : 0.40353609537780694
Loss in iteration 8 : 0.5246663847255526
Loss in iteration 9 : 0.7544624309380129
Loss in iteration 10 : 0.7627798529914273
Loss in iteration 11 : 0.6019606069553997
Loss in iteration 12 : 0.484069291031556
Loss in iteration 13 : 0.48365867072397584
Loss in iteration 14 : 0.5561478269325476
Loss in iteration 15 : 0.624442381594876
Loss in iteration 16 : 0.6489670095455349
Loss in iteration 17 : 0.6273155904255963
Loss in iteration 18 : 0.5788315115700062
Loss in iteration 19 : 0.534089343058735
Loss in iteration 20 : 0.5168768160117432
Loss in iteration 21 : 0.5353650629475899
Loss in iteration 22 : 0.5680550120416054
Loss in iteration 23 : 0.5869722334419495
Loss in iteration 24 : 0.5791857978741534
Loss in iteration 25 : 0.5513154298683557
Loss in iteration 26 : 0.5226560635191925
Loss in iteration 27 : 0.5095864474131471
Loss in iteration 28 : 0.5149939597966028
Loss in iteration 29 : 0.526119267227912
Loss in iteration 30 : 0.5305914779582539
Loss in iteration 31 : 0.5223446749463605
Loss in iteration 32 : 0.5040324170849545
Loss in iteration 33 : 0.4845185579240664
Loss in iteration 34 : 0.47465566772730494
Loss in iteration 35 : 0.47447546708825566
Loss in iteration 36 : 0.4766186193041341
Loss in iteration 37 : 0.4733405080351442
Loss in iteration 38 : 0.4607854414179851
Loss in iteration 39 : 0.4455714142927964
Loss in iteration 40 : 0.43519262749366916
Loss in iteration 41 : 0.4323960956526264
Loss in iteration 42 : 0.4324624214340887
Loss in iteration 43 : 0.4267581964389939
Loss in iteration 44 : 0.41449975471898476
Loss in iteration 45 : 0.4032521861495464
Loss in iteration 46 : 0.39826116247248916
Loss in iteration 47 : 0.3983673356755489
Loss in iteration 48 : 0.3947063247999658
Loss in iteration 49 : 0.3841122558705419
Loss in iteration 50 : 0.3770296676043693
Loss in iteration 51 : 0.37759567773916003
Loss in iteration 52 : 0.37779151850430165
Loss in iteration 53 : 0.37100534133988444
Loss in iteration 54 : 0.3658884939322095
Loss in iteration 55 : 0.369915605305471
Loss in iteration 56 : 0.3691389539791104
Loss in iteration 57 : 0.3648238425086657
Loss in iteration 58 : 0.36903822411181747
Loss in iteration 59 : 0.3706180350484543
Loss in iteration 60 : 0.36744418779131477
Loss in iteration 61 : 0.37028390097762137
Loss in iteration 62 : 0.37107140761869467
Loss in iteration 63 : 0.36810385595274353
Loss in iteration 64 : 0.36983274909091113
Loss in iteration 65 : 0.36897719105978305
Loss in iteration 66 : 0.3664053044341966
Loss in iteration 67 : 0.367508233563783
Loss in iteration 68 : 0.36613559594776923
Loss in iteration 69 : 0.36456345295572107
Loss in iteration 70 : 0.36501544497822225
Loss in iteration 71 : 0.36473572480483507
Loss in iteration 72 : 0.3637339897685736
Loss in iteration 73 : 0.3639543141412785
Loss in iteration 74 : 0.364274725500584
Loss in iteration 75 : 0.36378499345170046
Loss in iteration 76 : 0.36365962226212734
Loss in iteration 77 : 0.36410369496135336
Loss in iteration 78 : 0.3639875577032521
Loss in iteration 79 : 0.3636569750306732
Loss in iteration 80 : 0.3638682947078798
Loss in iteration 81 : 0.36392835803588874
Loss in iteration 82 : 0.3636626744098338
Loss in iteration 83 : 0.3636639589092671
Loss in iteration 84 : 0.3637472691838438
Loss in iteration 85 : 0.36356005973527716
Loss in iteration 86 : 0.3633692359896238
Loss in iteration 87 : 0.3634231690608733
Loss in iteration 88 : 0.3633237025969819
Loss in iteration 89 : 0.3631312205617311
Loss in iteration 90 : 0.36314335920394
Loss in iteration 91 : 0.3631268638776036
Loss in iteration 92 : 0.3629658371079252
Loss in iteration 93 : 0.3629422350525029
Loss in iteration 94 : 0.36297033097796355
Loss in iteration 95 : 0.3628627212845607
Loss in iteration 96 : 0.3628620848641968
Loss in iteration 97 : 0.36290988895957416
Loss in iteration 98 : 0.36283091053149497
Loss in iteration 99 : 0.3628791122042943
Loss in iteration 100 : 0.3628940918098435
Loss in iteration 101 : 0.3628460730388041
Loss in iteration 102 : 0.36290038008306974
Loss in iteration 103 : 0.3628810243087317
Loss in iteration 104 : 0.3628567411086228
Loss in iteration 105 : 0.3628790638215003
Loss in iteration 106 : 0.36283779333192046
Loss in iteration 107 : 0.36282326579844615
Loss in iteration 108 : 0.36282082292828693
Loss in iteration 109 : 0.36278422519302483
Loss in iteration 110 : 0.3627899604400356
Loss in iteration 111 : 0.36277136791576015
Loss in iteration 112 : 0.36275419780968543
Loss in iteration 113 : 0.36277164738344403
Loss in iteration 114 : 0.36274260198155067
Loss in iteration 115 : 0.36275794400822065
Loss in iteration 116 : 0.36275825509259124
Loss in iteration 117 : 0.3627359478687011
Loss in iteration 118 : 0.3627548345026313
Loss in iteration 119 : 0.3627386590858946
Loss in iteration 120 : 0.3627430932105304
Loss in iteration 121 : 0.36273999272510027
Loss in iteration 122 : 0.3627393537663302
Loss in iteration 123 : 0.36274781336326795
Loss in iteration 124 : 0.3627308764437462
Loss in iteration 125 : 0.36275194143025197
Loss in iteration 126 : 0.36273866233874197
Loss in iteration 127 : 0.3627372251547141
Loss in iteration 128 : 0.3627477029773276
Loss in iteration 129 : 0.3627252037221874
Loss in iteration 130 : 0.36275190169683785
Loss in iteration 131 : 0.36273466857835923
Loss in iteration 132 : 0.36273335619195596
Loss in iteration 133 : 0.36274006724867813
Loss in iteration 134 : 0.36272015308849187
Testing accuracy  of updater 6 on alg 1 with rate 0.0224 = 0.7905, training accuracy 0.8423438005827129, time elapsed: 1248 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5505277434314294
Loss in iteration 3 : 0.6417567820325407
Loss in iteration 4 : 0.7072058739461672
Loss in iteration 5 : 0.7100831568751239
Loss in iteration 6 : 0.6573360784730992
Loss in iteration 7 : 0.5581282919173358
Loss in iteration 8 : 0.44846166185391534
Loss in iteration 9 : 0.4077178086441538
Loss in iteration 10 : 0.4654154964612923
Loss in iteration 11 : 0.5051765207227493
Loss in iteration 12 : 0.47988622649372703
Loss in iteration 13 : 0.4214073172013871
Loss in iteration 14 : 0.3801898337678097
Loss in iteration 15 : 0.37767123386742285
Loss in iteration 16 : 0.39703016885817083
Loss in iteration 17 : 0.41625867511721404
Loss in iteration 18 : 0.4220125586977992
Loss in iteration 19 : 0.4132489968252526
Loss in iteration 20 : 0.39730490345693903
Loss in iteration 21 : 0.38368462975732254
Loss in iteration 22 : 0.37914255027648264
Loss in iteration 23 : 0.3830296165755795
Loss in iteration 24 : 0.3917049470158254
Loss in iteration 25 : 0.3985295656154455
Loss in iteration 26 : 0.4005445015343647
Loss in iteration 27 : 0.3975996124465539
Loss in iteration 28 : 0.39163715417150885
Loss in iteration 29 : 0.3859118241697174
Loss in iteration 30 : 0.3831851125659318
Loss in iteration 31 : 0.38395911161995133
Loss in iteration 32 : 0.38662590159165733
Loss in iteration 33 : 0.3894000292675379
Loss in iteration 34 : 0.3905367206852974
Loss in iteration 35 : 0.38964585270460106
Loss in iteration 36 : 0.38720850065302986
Loss in iteration 37 : 0.3842479060851624
Loss in iteration 38 : 0.3821618202700305
Loss in iteration 39 : 0.3812065604169311
Loss in iteration 40 : 0.3816428159748225
Loss in iteration 41 : 0.3825564300218766
Loss in iteration 42 : 0.3830894663295442
Loss in iteration 43 : 0.38253294324232784
Loss in iteration 44 : 0.3809675690391178
Loss in iteration 45 : 0.3791131312317051
Loss in iteration 46 : 0.37767410009866287
Loss in iteration 47 : 0.3771949959781009
Loss in iteration 48 : 0.3771882435895334
Loss in iteration 49 : 0.37725384109618315
Loss in iteration 50 : 0.3770537764105528
Loss in iteration 51 : 0.37640784472265315
Loss in iteration 52 : 0.3753915451231143
Loss in iteration 53 : 0.37428635568776036
Loss in iteration 54 : 0.37337588491671714
Loss in iteration 55 : 0.37293971667946013
Loss in iteration 56 : 0.3728448208706288
Loss in iteration 57 : 0.37276707517512137
Loss in iteration 58 : 0.3724152034370117
Loss in iteration 59 : 0.3717364630366527
Loss in iteration 60 : 0.3709622949611886
Loss in iteration 61 : 0.37047842019451727
Loss in iteration 62 : 0.37033562622000593
Loss in iteration 63 : 0.37028185064198443
Loss in iteration 64 : 0.37011423081125533
Loss in iteration 65 : 0.3697733698206464
Loss in iteration 66 : 0.3693584997130418
Loss in iteration 67 : 0.3690678757513274
Loss in iteration 68 : 0.3689421819513698
Loss in iteration 69 : 0.36890294158377046
Loss in iteration 70 : 0.36883001983998165
Loss in iteration 71 : 0.3686767442476523
Loss in iteration 72 : 0.3684649572983245
Loss in iteration 73 : 0.36828348593776483
Loss in iteration 74 : 0.3681996868072365
Loss in iteration 75 : 0.36817493547873703
Loss in iteration 76 : 0.3681456674465909
Loss in iteration 77 : 0.3680540789771809
Loss in iteration 78 : 0.36792920570818427
Loss in iteration 79 : 0.3678310593288047
Loss in iteration 80 : 0.3677694113677842
Loss in iteration 81 : 0.3677445254892163
Loss in iteration 82 : 0.3677030299420105
Loss in iteration 83 : 0.36763325032531435
Loss in iteration 84 : 0.3675532522162236
Loss in iteration 85 : 0.3674892235085586
Loss in iteration 86 : 0.36744688702001044
Loss in iteration 87 : 0.3674068941297892
Loss in iteration 88 : 0.3673590810052433
Loss in iteration 89 : 0.36730076607356477
Loss in iteration 90 : 0.36723444718038534
Loss in iteration 91 : 0.3671682926525649
Loss in iteration 92 : 0.3671065201135005
Loss in iteration 93 : 0.3670557330383554
Loss in iteration 94 : 0.3670138178942858
Loss in iteration 95 : 0.36695490044329715
Loss in iteration 96 : 0.36688775175976085
Loss in iteration 97 : 0.3668348504189131
Loss in iteration 98 : 0.36678485112871045
Loss in iteration 99 : 0.3667362102419649
Loss in iteration 100 : 0.3666847132822735
Loss in iteration 101 : 0.3666301852312211
Loss in iteration 102 : 0.36657485727967964
Loss in iteration 103 : 0.3665273412546564
Loss in iteration 104 : 0.36648388038054763
Loss in iteration 105 : 0.36643862903179225
Loss in iteration 106 : 0.3663893239111097
Loss in iteration 107 : 0.3663400315186048
Loss in iteration 108 : 0.3662951801233216
Loss in iteration 109 : 0.36625295514729334
Loss in iteration 110 : 0.36621286033742995
Loss in iteration 111 : 0.36617078247286644
Loss in iteration 112 : 0.36612585740690273
Loss in iteration 113 : 0.366080826283878
Loss in iteration 114 : 0.3660377204944338
Loss in iteration 115 : 0.36599936729647503
Loss in iteration 116 : 0.36596146953057923
Loss in iteration 117 : 0.3659209079136461
Loss in iteration 118 : 0.36587969162036527
Loss in iteration 119 : 0.36583950567005125
Loss in iteration 120 : 0.36579938286613617
Loss in iteration 121 : 0.36575957789644004
Loss in iteration 122 : 0.3657224675691249
Loss in iteration 123 : 0.3656851399005655
Loss in iteration 124 : 0.36564873314927565
Loss in iteration 125 : 0.36561075157531076
Loss in iteration 126 : 0.3655727685939697
Loss in iteration 127 : 0.36553473289180644
Loss in iteration 128 : 0.3654972579635405
Loss in iteration 129 : 0.36546259880285714
Loss in iteration 130 : 0.3654274873407137
Loss in iteration 131 : 0.36539354829262655
Loss in iteration 132 : 0.36535850963329825
Loss in iteration 133 : 0.3653237412978402
Loss in iteration 134 : 0.3652896194890415
Loss in iteration 135 : 0.3652556288333807
Loss in iteration 136 : 0.3652221993068897
Loss in iteration 137 : 0.36519174531835397
Loss in iteration 138 : 0.3651613915713027
Loss in iteration 139 : 0.36513028964357497
Loss in iteration 140 : 0.365098847367119
Loss in iteration 141 : 0.3650685201003808
Loss in iteration 142 : 0.36503908806915747
Loss in iteration 143 : 0.365010186413672
Loss in iteration 144 : 0.36498132606792205
Loss in iteration 145 : 0.36495234845569074
Loss in iteration 146 : 0.364923544700363
Loss in iteration 147 : 0.36489530907617523
Loss in iteration 148 : 0.36486800844171635
Loss in iteration 149 : 0.3648406876180072
Loss in iteration 150 : 0.3648142199345073
Loss in iteration 151 : 0.3647881698465246
Loss in iteration 152 : 0.36476314823410644
Loss in iteration 153 : 0.3647381280914947
Loss in iteration 154 : 0.3647129781210684
Loss in iteration 155 : 0.3646881145724705
Loss in iteration 156 : 0.3646640992421819
Loss in iteration 157 : 0.36464037035069125
Loss in iteration 158 : 0.36461684893539215
Loss in iteration 159 : 0.3645934453724819
Loss in iteration 160 : 0.3645711123731146
Loss in iteration 161 : 0.3645486453766508
Loss in iteration 162 : 0.3645263495523458
Loss in iteration 163 : 0.36450436524621954
Loss in iteration 164 : 0.3644824518092005
Loss in iteration 165 : 0.36446060019937015
Loss in iteration 166 : 0.36443909054686885
Loss in iteration 167 : 0.3644186718839014
Loss in iteration 168 : 0.36439826077766174
Loss in iteration 169 : 0.36437690991245697
Loss in iteration 170 : 0.36435623357141
Loss in iteration 171 : 0.36433743837130117
Loss in iteration 172 : 0.36431752101137815
Loss in iteration 173 : 0.36429818088389454
Loss in iteration 174 : 0.3642800071597013
Loss in iteration 175 : 0.3642621095099605
Loss in iteration 176 : 0.3642440399755632
Loss in iteration 177 : 0.36422601830485124
Loss in iteration 178 : 0.3642084386057735
Loss in iteration 179 : 0.36419096380476185
Loss in iteration 180 : 0.36417466165551293
Loss in iteration 181 : 0.3641570955412374
Loss in iteration 182 : 0.36413999925025564
Loss in iteration 183 : 0.3641231632475458
Loss in iteration 184 : 0.364106564600793
Loss in iteration 185 : 0.3640902165149269
Loss in iteration 186 : 0.36407372855196357
Loss in iteration 187 : 0.3640572099076541
Loss in iteration 188 : 0.36404085299681854
Loss in iteration 189 : 0.3640249267153613
Loss in iteration 190 : 0.36400935110759086
Loss in iteration 191 : 0.36399350506586137
Loss in iteration 192 : 0.3639782563623613
Loss in iteration 193 : 0.36396304078582226
Loss in iteration 194 : 0.3639480061706701
Loss in iteration 195 : 0.36393327245958673
Loss in iteration 196 : 0.3639184568700365
Loss in iteration 197 : 0.36390435418884465
Loss in iteration 198 : 0.3638902236334268
Loss in iteration 199 : 0.3638762929295948
Loss in iteration 200 : 0.36386272336222264
Testing accuracy  of updater 6 on alg 1 with rate 0.0056 = 0.78875, training accuracy 0.8420200712204597, time elapsed: 1867 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5718556000436639
Loss in iteration 3 : 0.6197559484275003
Loss in iteration 4 : 0.7031357496969074
Loss in iteration 5 : 0.7336524423478532
Loss in iteration 6 : 0.7164698391634382
Loss in iteration 7 : 0.6573285951306448
Loss in iteration 8 : 0.563352804633392
Loss in iteration 9 : 0.46092384001251935
Loss in iteration 10 : 0.4065801197660419
Loss in iteration 11 : 0.4433378708285453
Loss in iteration 12 : 0.4971373225633514
Loss in iteration 13 : 0.5034214077041368
Loss in iteration 14 : 0.4631401418768958
Loss in iteration 15 : 0.41004059154437295
Loss in iteration 16 : 0.37860431148825907
Loss in iteration 17 : 0.37843501457173
Loss in iteration 18 : 0.3954763617200087
Loss in iteration 19 : 0.4131234218398119
Loss in iteration 20 : 0.4207883517313141
Loss in iteration 21 : 0.4157081512091623
Loss in iteration 22 : 0.4026611082188054
Loss in iteration 23 : 0.3881776442838234
Loss in iteration 24 : 0.37899079217242004
Loss in iteration 25 : 0.37736090722448334
Loss in iteration 26 : 0.3820045765637481
Loss in iteration 27 : 0.38921028797020746
Loss in iteration 28 : 0.39456391358187104
Loss in iteration 29 : 0.395987169530691
Loss in iteration 30 : 0.3934554444868402
Loss in iteration 31 : 0.38844829561226
Loss in iteration 32 : 0.38319860706913633
Loss in iteration 33 : 0.38024508565038473
Loss in iteration 34 : 0.379973312143955
Loss in iteration 35 : 0.3815917018273426
Loss in iteration 36 : 0.38380696692493266
Loss in iteration 37 : 0.3855604724815781
Loss in iteration 38 : 0.3859230101560098
Loss in iteration 39 : 0.3848185763802756
Loss in iteration 40 : 0.3827627977387559
Loss in iteration 41 : 0.3807024977345867
Loss in iteration 42 : 0.3791380954862663
Loss in iteration 43 : 0.3784094673155801
Loss in iteration 44 : 0.3785176959818289
Loss in iteration 45 : 0.37915048244648497
Loss in iteration 46 : 0.3797490724365194
Loss in iteration 47 : 0.3797447811770187
Loss in iteration 48 : 0.3789644522751803
Loss in iteration 49 : 0.37773298188168686
Loss in iteration 50 : 0.3765676581516865
Loss in iteration 51 : 0.3758193231615782
Loss in iteration 52 : 0.37550525232643767
Loss in iteration 53 : 0.3755305743387573
Loss in iteration 54 : 0.37558486100099114
Loss in iteration 55 : 0.37551283552647535
Loss in iteration 56 : 0.3751453082574349
Loss in iteration 57 : 0.3745123361554404
Loss in iteration 58 : 0.3737600069494084
Loss in iteration 59 : 0.3731521226521856
Loss in iteration 60 : 0.37278632138167395
Loss in iteration 61 : 0.3726832908372449
Loss in iteration 62 : 0.3726400287484907
Loss in iteration 63 : 0.372509232353445
Loss in iteration 64 : 0.37220522939508016
Loss in iteration 65 : 0.3717711045857758
Loss in iteration 66 : 0.3713090344193716
Loss in iteration 67 : 0.3710177295876658
Loss in iteration 68 : 0.3708900685719527
Loss in iteration 69 : 0.3708222188800111
Loss in iteration 70 : 0.3707298505503962
Loss in iteration 71 : 0.3705548890828056
Loss in iteration 72 : 0.3703145848038525
Loss in iteration 73 : 0.37007553858504244
Loss in iteration 74 : 0.36989180832101304
Loss in iteration 75 : 0.36977996224934545
Loss in iteration 76 : 0.369727519051047
Loss in iteration 77 : 0.36964536097620004
Loss in iteration 78 : 0.36952873707773415
Loss in iteration 79 : 0.36940041365915716
Loss in iteration 80 : 0.36928106452244513
Loss in iteration 81 : 0.3691951043868505
Loss in iteration 82 : 0.3691411412873387
Loss in iteration 83 : 0.36909234281263764
Loss in iteration 84 : 0.3690283845569897
Loss in iteration 85 : 0.3689533196992189
Loss in iteration 86 : 0.3688725297810862
Loss in iteration 87 : 0.3688050674622148
Loss in iteration 88 : 0.3687485585325333
Loss in iteration 89 : 0.3687021874661779
Loss in iteration 90 : 0.3686590753850425
Loss in iteration 91 : 0.36860617819348546
Loss in iteration 92 : 0.3685482029616237
Loss in iteration 93 : 0.3684881466288944
Loss in iteration 94 : 0.3684369450288246
Loss in iteration 95 : 0.3683953813966302
Loss in iteration 96 : 0.3683573795996268
Loss in iteration 97 : 0.3683114434078299
Loss in iteration 98 : 0.36825847543874696
Loss in iteration 99 : 0.36820645898042753
Loss in iteration 100 : 0.3681631162638582
Loss in iteration 101 : 0.368121379868344
Loss in iteration 102 : 0.3680781790572216
Loss in iteration 103 : 0.368033257154476
Loss in iteration 104 : 0.3679867288183765
Loss in iteration 105 : 0.3679397327125858
Loss in iteration 106 : 0.367894744955515
Loss in iteration 107 : 0.3678516087481861
Loss in iteration 108 : 0.3678082601685504
Loss in iteration 109 : 0.3677626092698509
Loss in iteration 110 : 0.36771777326788335
Loss in iteration 111 : 0.367675535276836
Loss in iteration 112 : 0.36763303103164197
Loss in iteration 113 : 0.3675896000602803
Loss in iteration 114 : 0.36754825359621945
Loss in iteration 115 : 0.3675083920841369
Loss in iteration 116 : 0.3674689424625189
Loss in iteration 117 : 0.3674297209081212
Loss in iteration 118 : 0.3673906246906185
Loss in iteration 119 : 0.3673511910988723
Loss in iteration 120 : 0.367312121467217
Loss in iteration 121 : 0.3672731219731033
Loss in iteration 122 : 0.36723482626815057
Loss in iteration 123 : 0.3671972434120336
Loss in iteration 124 : 0.3671597895625043
Loss in iteration 125 : 0.3671223261176027
Loss in iteration 126 : 0.3670853184925866
Loss in iteration 127 : 0.36704920949960673
Loss in iteration 128 : 0.36701326088876796
Loss in iteration 129 : 0.3669778150539708
Loss in iteration 130 : 0.36694310046941786
Loss in iteration 131 : 0.3669077776456895
Loss in iteration 132 : 0.36687319484859476
Loss in iteration 133 : 0.36683943142527975
Loss in iteration 134 : 0.36680566065153525
Loss in iteration 135 : 0.36677239915263926
Loss in iteration 136 : 0.36673976174542805
Loss in iteration 137 : 0.3667072538883214
Loss in iteration 138 : 0.366675301098995
Loss in iteration 139 : 0.36664380361790416
Loss in iteration 140 : 0.3666124384109166
Loss in iteration 141 : 0.3665809910415642
Loss in iteration 142 : 0.3665495321500069
Loss in iteration 143 : 0.3665182585312175
Loss in iteration 144 : 0.36648768308218893
Loss in iteration 145 : 0.3664572039347975
Loss in iteration 146 : 0.3664268534641668
Loss in iteration 147 : 0.36639659135803626
Loss in iteration 148 : 0.36636621681617487
Loss in iteration 149 : 0.3663361234831033
Loss in iteration 150 : 0.36630629640695694
Loss in iteration 151 : 0.3662768200334664
Loss in iteration 152 : 0.3662475858639084
Loss in iteration 153 : 0.3662183265441262
Loss in iteration 154 : 0.3661890429658521
Loss in iteration 155 : 0.3661598820370017
Loss in iteration 156 : 0.36613101318559654
Loss in iteration 157 : 0.36610224620832016
Loss in iteration 158 : 0.36607393863833565
Loss in iteration 159 : 0.36604567801569765
Loss in iteration 160 : 0.3660178002285222
Loss in iteration 161 : 0.3659904811404282
Loss in iteration 162 : 0.3659632952538181
Loss in iteration 163 : 0.36593619629429364
Loss in iteration 164 : 0.3659091741805239
Loss in iteration 165 : 0.36588221983617086
Loss in iteration 166 : 0.36585532508989693
Loss in iteration 167 : 0.36582848258531264
Loss in iteration 168 : 0.36580199970187355
Loss in iteration 169 : 0.3657757054595581
Loss in iteration 170 : 0.3657497932182542
Loss in iteration 171 : 0.3657237496790595
Loss in iteration 172 : 0.3656979511855451
Loss in iteration 173 : 0.3656715794727648
Loss in iteration 174 : 0.3656445732086664
Loss in iteration 175 : 0.36561861831780496
Loss in iteration 176 : 0.36559452589651276
Loss in iteration 177 : 0.365569752514919
Loss in iteration 178 : 0.36554470526301736
Loss in iteration 179 : 0.36551993581123454
Loss in iteration 180 : 0.36549579412637706
Loss in iteration 181 : 0.3654717855678728
Loss in iteration 182 : 0.3654478318136418
Loss in iteration 183 : 0.36542392603922097
Loss in iteration 184 : 0.3654000621015441
Loss in iteration 185 : 0.3653762344710697
Loss in iteration 186 : 0.3653524381706543
Loss in iteration 187 : 0.3653291066686072
Loss in iteration 188 : 0.36530591505300103
Loss in iteration 189 : 0.3652830415889703
Loss in iteration 190 : 0.36526089656080624
Loss in iteration 191 : 0.3652388306606725
Loss in iteration 192 : 0.3652168770705732
Loss in iteration 193 : 0.3651948296844441
Loss in iteration 194 : 0.3651728122499752
Loss in iteration 195 : 0.3651510709251179
Loss in iteration 196 : 0.36512978264329027
Loss in iteration 197 : 0.3651085873688443
Loss in iteration 198 : 0.365087398608771
Loss in iteration 199 : 0.36506592682947886
Loss in iteration 200 : 0.36504557970677737
Testing accuracy  of updater 6 on alg 1 with rate 0.00392 = 0.788, training accuracy 0.8416963418582065, time elapsed: 2131 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6828202493552278
Loss in iteration 3 : 0.5595649649914104
Loss in iteration 4 : 0.6370670627241208
Loss in iteration 5 : 0.6906604843707115
Loss in iteration 6 : 0.7125520895288886
Loss in iteration 7 : 0.7058713094846987
Loss in iteration 8 : 0.6738468313608329
Loss in iteration 9 : 0.6198352504349727
Loss in iteration 10 : 0.54897519225275
Loss in iteration 11 : 0.47741802755470586
Loss in iteration 12 : 0.4284905820569069
Loss in iteration 13 : 0.419220644076224
Loss in iteration 14 : 0.4484785864419326
Loss in iteration 15 : 0.47855668578977467
Loss in iteration 16 : 0.4849000635955876
Loss in iteration 17 : 0.4651664530332244
Loss in iteration 18 : 0.43150547796875255
Loss in iteration 19 : 0.40001763793292877
Loss in iteration 20 : 0.38228752527360543
Loss in iteration 21 : 0.3800395579879366
Loss in iteration 22 : 0.38774042920288254
Loss in iteration 23 : 0.3978430808172309
Loss in iteration 24 : 0.4045902815158066
Loss in iteration 25 : 0.4054082078089357
Loss in iteration 26 : 0.4008054081454421
Loss in iteration 27 : 0.3928546004902096
Loss in iteration 28 : 0.3843523894978733
Loss in iteration 29 : 0.37785370030185184
Loss in iteration 30 : 0.374363896384049
Loss in iteration 31 : 0.37453559617884674
Loss in iteration 32 : 0.37704338575270624
Loss in iteration 33 : 0.38025456513373374
Loss in iteration 34 : 0.38283512993490676
Loss in iteration 35 : 0.3838003414442575
Loss in iteration 36 : 0.38293374797315494
Loss in iteration 37 : 0.3808411416681083
Loss in iteration 38 : 0.3782763444071821
Loss in iteration 39 : 0.3759419489700467
Loss in iteration 40 : 0.37452823030865495
Loss in iteration 41 : 0.3742400307170485
Loss in iteration 42 : 0.374841117390221
Loss in iteration 43 : 0.37572750674995703
Loss in iteration 44 : 0.37659510460282564
Loss in iteration 45 : 0.3771289064354408
Loss in iteration 46 : 0.3771767038221098
Loss in iteration 47 : 0.37675506913618717
Loss in iteration 48 : 0.3760150195562269
Loss in iteration 49 : 0.3752116152676657
Loss in iteration 50 : 0.3744874696569061
Loss in iteration 51 : 0.37395168949506713
Loss in iteration 52 : 0.3737120984209139
Loss in iteration 53 : 0.3738433071982287
Loss in iteration 54 : 0.3741233461164545
Loss in iteration 55 : 0.3743661676671809
Loss in iteration 56 : 0.37444938323742805
Loss in iteration 57 : 0.37434041335554996
Loss in iteration 58 : 0.37406034126258764
Loss in iteration 59 : 0.37369627134927563
Loss in iteration 60 : 0.37330613399995416
Loss in iteration 61 : 0.3730273362007132
Loss in iteration 62 : 0.37289150842282864
Loss in iteration 63 : 0.37286320340348034
Loss in iteration 64 : 0.3729165716191162
Loss in iteration 65 : 0.37295426348288435
Loss in iteration 66 : 0.3729253618106736
Loss in iteration 67 : 0.37282271583120763
Loss in iteration 68 : 0.37266090665954404
Loss in iteration 69 : 0.37245808393974406
Loss in iteration 70 : 0.3722784976611812
Loss in iteration 71 : 0.3721391138861847
Loss in iteration 72 : 0.3720447835143818
Loss in iteration 73 : 0.37199660140946744
Loss in iteration 74 : 0.3719782774227882
Loss in iteration 75 : 0.3719523788249272
Loss in iteration 76 : 0.37189212227641494
Loss in iteration 77 : 0.37180175252611913
Loss in iteration 78 : 0.3716969891521089
Loss in iteration 79 : 0.3715902896734683
Loss in iteration 80 : 0.3714969973559892
Loss in iteration 81 : 0.37141713918797553
Loss in iteration 82 : 0.3713567273679491
Loss in iteration 83 : 0.3713057082549959
Loss in iteration 84 : 0.3712636117733391
Loss in iteration 85 : 0.37121891651068234
Loss in iteration 86 : 0.3711591888922859
Loss in iteration 87 : 0.3710898487201516
Loss in iteration 88 : 0.3710210368733158
Loss in iteration 89 : 0.3709592047439651
Loss in iteration 90 : 0.3709021315186354
Loss in iteration 91 : 0.37084933935194114
Loss in iteration 92 : 0.37080333577676017
Loss in iteration 93 : 0.37076341094119597
Loss in iteration 94 : 0.3707193317879099
Loss in iteration 95 : 0.3706732787093231
Loss in iteration 96 : 0.370625342528249
Loss in iteration 97 : 0.37057624248523136
Loss in iteration 98 : 0.3705277601090264
Loss in iteration 99 : 0.3704794824050917
Loss in iteration 100 : 0.3704333214208922
Loss in iteration 101 : 0.3703910365530577
Loss in iteration 102 : 0.3703531315252897
Loss in iteration 103 : 0.37031560380620826
Loss in iteration 104 : 0.37027701346398706
Loss in iteration 105 : 0.37023679227186596
Loss in iteration 106 : 0.37019502250066577
Loss in iteration 107 : 0.3701554682768549
Loss in iteration 108 : 0.3701177846934479
Loss in iteration 109 : 0.3700810416126657
Loss in iteration 110 : 0.37004521780437793
Loss in iteration 111 : 0.370010005343517
Loss in iteration 112 : 0.36997475503789495
Loss in iteration 113 : 0.36993946960991214
Loss in iteration 114 : 0.3699041515183027
Loss in iteration 115 : 0.36986888085474734
Loss in iteration 116 : 0.3698342099249272
Loss in iteration 117 : 0.36979963660220355
Loss in iteration 118 : 0.3697653150342095
Loss in iteration 119 : 0.36973222605812445
Loss in iteration 120 : 0.3696993632911047
Loss in iteration 121 : 0.36966656007091914
Loss in iteration 122 : 0.369634586213575
Loss in iteration 123 : 0.36960275527031594
Loss in iteration 124 : 0.36957105860512623
Loss in iteration 125 : 0.3695399455937294
Loss in iteration 126 : 0.3695090537942085
Loss in iteration 127 : 0.3694782763391792
Loss in iteration 128 : 0.36944760119678377
Loss in iteration 129 : 0.36941701753338707
Loss in iteration 130 : 0.3693865155943439
Loss in iteration 131 : 0.36935608659663327
Loss in iteration 132 : 0.3693257226321733
Loss in iteration 133 : 0.36929541658075504
Loss in iteration 134 : 0.36926516203164306
Loss in iteration 135 : 0.36923533856547974
Loss in iteration 136 : 0.3692055707290885
Loss in iteration 137 : 0.36917583404061716
Loss in iteration 138 : 0.36914620895767297
Loss in iteration 139 : 0.3691167007884625
Loss in iteration 140 : 0.36908723142150157
Loss in iteration 141 : 0.3690577963137092
Loss in iteration 142 : 0.3690283913761527
Loss in iteration 143 : 0.36899919994601643
Loss in iteration 144 : 0.3689704993543705
Loss in iteration 145 : 0.36894204126537494
Loss in iteration 146 : 0.3689136917052436
Loss in iteration 147 : 0.3688859467770745
Loss in iteration 148 : 0.3688582889906877
Loss in iteration 149 : 0.36883067711376205
Loss in iteration 150 : 0.36880310590134213
Loss in iteration 151 : 0.36877573095224847
Loss in iteration 152 : 0.36874842764205745
Loss in iteration 153 : 0.3687212154083395
Loss in iteration 154 : 0.36869399838458317
Loss in iteration 155 : 0.36866709988803753
Loss in iteration 156 : 0.36864029887471716
Loss in iteration 157 : 0.36861347407136574
Loss in iteration 158 : 0.3685866582873024
Loss in iteration 159 : 0.36856011245885845
Loss in iteration 160 : 0.3685336941612165
Loss in iteration 161 : 0.3685075294567072
Loss in iteration 162 : 0.36848136725001673
Loss in iteration 163 : 0.36845520663718895
Loss in iteration 164 : 0.3684292100615406
Loss in iteration 165 : 0.3684037213227939
Loss in iteration 166 : 0.368378312067029
Loss in iteration 167 : 0.368353016510844
Loss in iteration 168 : 0.36832800862848647
Loss in iteration 169 : 0.36830338970272564
Loss in iteration 170 : 0.3682786583844569
Loss in iteration 171 : 0.36825369879609116
Loss in iteration 172 : 0.36822870721744894
Loss in iteration 173 : 0.36820409114633107
Loss in iteration 174 : 0.3681800857971828
Loss in iteration 175 : 0.3681558761703844
Loss in iteration 176 : 0.3681315436695638
Loss in iteration 177 : 0.3681074160666805
Loss in iteration 178 : 0.36808348458131834
Loss in iteration 179 : 0.36805958187129467
Loss in iteration 180 : 0.3680357043819695
Loss in iteration 181 : 0.3680119796069654
Loss in iteration 182 : 0.36798853833308026
Loss in iteration 183 : 0.3679651389232793
Loss in iteration 184 : 0.36794200193380416
Loss in iteration 185 : 0.3679190121904778
Loss in iteration 186 : 0.3678961269636087
Loss in iteration 187 : 0.36787325162555984
Loss in iteration 188 : 0.3678504286929147
Loss in iteration 189 : 0.3678277274404672
Loss in iteration 190 : 0.3678050458653389
Loss in iteration 191 : 0.36778238126803964
Loss in iteration 192 : 0.36775973122063504
Loss in iteration 193 : 0.36773733477894727
Loss in iteration 194 : 0.3677150145526564
Loss in iteration 195 : 0.367692711784749
Loss in iteration 196 : 0.3676704239704375
Loss in iteration 197 : 0.3676481488571991
Loss in iteration 198 : 0.3676261038318664
Loss in iteration 199 : 0.3676044622288777
Loss in iteration 200 : 0.36758274788974377
Testing accuracy  of updater 6 on alg 1 with rate 0.0022400000000000002 = 0.78625, training accuracy 0.8397539656846876, time elapsed: 2434 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9164507178862157
Loss in iteration 3 : 0.7688333933744828
Loss in iteration 4 : 0.6200317304950411
Loss in iteration 5 : 0.554352064019294
Loss in iteration 6 : 0.5600400401513229
Loss in iteration 7 : 0.5868897339811376
Loss in iteration 8 : 0.6119702052314164
Loss in iteration 9 : 0.6298700755958855
Loss in iteration 10 : 0.6398752853146026
Loss in iteration 11 : 0.6426755784595106
Loss in iteration 12 : 0.6390523937812342
Loss in iteration 13 : 0.6297250448302395
Loss in iteration 14 : 0.6153864251790718
Loss in iteration 15 : 0.5967765170807983
Loss in iteration 16 : 0.5749307603258659
Loss in iteration 17 : 0.5514794379931852
Loss in iteration 18 : 0.5284933465519549
Loss in iteration 19 : 0.5084797218137701
Loss in iteration 20 : 0.49261596531832585
Loss in iteration 21 : 0.4826701137590962
Loss in iteration 22 : 0.47804775836389063
Loss in iteration 23 : 0.4768635922794124
Loss in iteration 24 : 0.4777225238984017
Loss in iteration 25 : 0.4791145655833538
Loss in iteration 26 : 0.47982402092488946
Loss in iteration 27 : 0.4790031135525144
Loss in iteration 28 : 0.4764706161075332
Loss in iteration 29 : 0.47234222691983924
Loss in iteration 30 : 0.467000037879032
Loss in iteration 31 : 0.46077111594264847
Loss in iteration 32 : 0.45414535704976844
Loss in iteration 33 : 0.44784207192178754
Loss in iteration 34 : 0.4420712283197115
Loss in iteration 35 : 0.4370409158154799
Loss in iteration 36 : 0.4327475584689146
Loss in iteration 37 : 0.42912750794194515
Loss in iteration 38 : 0.4261936158821423
Loss in iteration 39 : 0.4238427214376635
Loss in iteration 40 : 0.4218194980330904
Loss in iteration 41 : 0.4199217895600568
Loss in iteration 42 : 0.4180378102632378
Loss in iteration 43 : 0.41609537725901014
Loss in iteration 44 : 0.41407650601539686
Loss in iteration 45 : 0.41194668938870976
Loss in iteration 46 : 0.40971333107683355
Loss in iteration 47 : 0.4074422006340914
Loss in iteration 48 : 0.40519506431621327
Loss in iteration 49 : 0.40308381513266944
Loss in iteration 50 : 0.40113615357046445
Loss in iteration 51 : 0.39939876581754097
Loss in iteration 52 : 0.39788948407915486
Loss in iteration 53 : 0.39664271190429223
Loss in iteration 54 : 0.3955879768493262
Loss in iteration 55 : 0.3947060320462059
Loss in iteration 56 : 0.39393858653083974
Loss in iteration 57 : 0.3932359369240174
Loss in iteration 58 : 0.3925629253377342
Loss in iteration 59 : 0.39186619200261835
Loss in iteration 60 : 0.3911502585534795
Testing accuracy  of updater 6 on alg 1 with rate 5.599999999999997E-4 = 0.78, training accuracy 0.8358692133376497, time elapsed: 701 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 5.158972904620201
Loss in iteration 3 : 6.727438119564748
Loss in iteration 4 : 6.682584433812567
Loss in iteration 5 : 5.674916099831245
Loss in iteration 6 : 4.029641791138713
Loss in iteration 7 : 2.0214771832338045
Loss in iteration 8 : 0.962088260404717
Loss in iteration 9 : 1.3268220139822584
Loss in iteration 10 : 2.3436672071483438
Loss in iteration 11 : 2.929610797280571
Loss in iteration 12 : 2.750156389644843
Loss in iteration 13 : 2.1039277490048307
Loss in iteration 14 : 1.4741288027996196
Loss in iteration 15 : 1.1612263783944432
Loss in iteration 16 : 1.1937851814415021
Loss in iteration 17 : 1.390227415104462
Loss in iteration 18 : 1.6121344460545757
Loss in iteration 19 : 1.7706059962015854
Loss in iteration 20 : 1.8167059386383775
Loss in iteration 21 : 1.753284728863797
Loss in iteration 22 : 1.6122956438987488
Loss in iteration 23 : 1.4486997266964352
Loss in iteration 24 : 1.3088361429359159
Loss in iteration 25 : 1.2176471260777868
Loss in iteration 26 : 1.1941238896833963
Loss in iteration 27 : 1.2420438720471945
Loss in iteration 28 : 1.3168627970516977
Loss in iteration 29 : 1.3744865611285038
Loss in iteration 30 : 1.3844277936765026
Loss in iteration 31 : 1.3405847074911876
Loss in iteration 32 : 1.2664566888302249
Loss in iteration 33 : 1.1922138341864195
Loss in iteration 34 : 1.140768832366096
Loss in iteration 35 : 1.1273743362051762
Loss in iteration 36 : 1.137649217865257
Loss in iteration 37 : 1.1556602682207617
Loss in iteration 38 : 1.1677190367531425
Loss in iteration 39 : 1.1645379151138808
Loss in iteration 40 : 1.144408992151771
Loss in iteration 41 : 1.1111219062415736
Loss in iteration 42 : 1.0725769267751541
Loss in iteration 43 : 1.0378633203835186
Loss in iteration 44 : 1.0128466183829923
Loss in iteration 45 : 1.0031055837572922
Loss in iteration 46 : 1.0041575884272915
Loss in iteration 47 : 1.0035599245235856
Loss in iteration 48 : 0.993593437307445
Loss in iteration 49 : 0.9729012915779507
Loss in iteration 50 : 0.944858346968197
Loss in iteration 51 : 0.9168200937532663
Loss in iteration 52 : 0.8966951414814562
Loss in iteration 53 : 0.8857944848040952
Loss in iteration 54 : 0.8786042329604682
Loss in iteration 55 : 0.8693289613075251
Loss in iteration 56 : 0.8550166644781522
Loss in iteration 57 : 0.8354199837599126
Loss in iteration 58 : 0.8125069676734605
Loss in iteration 59 : 0.7911812430748113
Loss in iteration 60 : 0.7757413644667328
Loss in iteration 61 : 0.7649351519727575
Loss in iteration 62 : 0.7539886597896902
Loss in iteration 63 : 0.7390139246496427
Loss in iteration 64 : 0.7199316799872559
Loss in iteration 65 : 0.6998559882347071
Loss in iteration 66 : 0.6832187959477852
Loss in iteration 67 : 0.6701177558160598
Loss in iteration 68 : 0.6583742280408001
Loss in iteration 69 : 0.6438892057577196
Loss in iteration 70 : 0.6263142092938514
Loss in iteration 71 : 0.6089018352317613
Loss in iteration 72 : 0.5938919606937871
Loss in iteration 73 : 0.5810759539498112
Loss in iteration 74 : 0.5684621259499398
Loss in iteration 75 : 0.5538516740400794
Loss in iteration 76 : 0.5379362340787994
Loss in iteration 77 : 0.5225820575719884
Loss in iteration 78 : 0.5100020368415112
Loss in iteration 79 : 0.4984823264443399
Loss in iteration 80 : 0.48535819647470024
Loss in iteration 81 : 0.4714734441683863
Loss in iteration 82 : 0.45857511886342445
Loss in iteration 83 : 0.44799766662659135
Loss in iteration 84 : 0.4378661856960853
Loss in iteration 85 : 0.4260056755152215
Loss in iteration 86 : 0.4151390603699819
Loss in iteration 87 : 0.40735389457437116
Loss in iteration 88 : 0.3994923636531183
Loss in iteration 89 : 0.39141663764193063
Loss in iteration 90 : 0.3856972620753857
Loss in iteration 91 : 0.38264248330752854
Loss in iteration 92 : 0.3786576059226828
Loss in iteration 93 : 0.3771440146045567
Loss in iteration 94 : 0.37732681678200714
Loss in iteration 95 : 0.3769521432040514
Loss in iteration 96 : 0.379019352187065
Loss in iteration 97 : 0.3794942865446133
Loss in iteration 98 : 0.38128935546919845
Loss in iteration 99 : 0.38146181992871747
Loss in iteration 100 : 0.3820272336410525
Loss in iteration 101 : 0.3805788053207384
Loss in iteration 102 : 0.37960185377766276
Loss in iteration 103 : 0.37664142765849773
Loss in iteration 104 : 0.3749386618922618
Loss in iteration 105 : 0.3717825873502658
Loss in iteration 106 : 0.37003582257075346
Loss in iteration 107 : 0.36785327503135196
Loss in iteration 108 : 0.3668814236352004
Loss in iteration 109 : 0.36575821408692955
Loss in iteration 110 : 0.3652993559781145
Loss in iteration 111 : 0.36502612636696125
Loss in iteration 112 : 0.3647962791968879
Loss in iteration 113 : 0.36489816254091395
Loss in iteration 114 : 0.36485092758345206
Loss in iteration 115 : 0.36484380263379557
Loss in iteration 116 : 0.36485124174973965
Loss in iteration 117 : 0.36474776171903805
Loss in iteration 118 : 0.3646262792606872
Loss in iteration 119 : 0.3644572058529269
Loss in iteration 120 : 0.36420736190947217
Loss in iteration 121 : 0.3639855968577737
Loss in iteration 122 : 0.36372685037069413
Loss in iteration 123 : 0.3635194072678021
Loss in iteration 124 : 0.3633144153104296
Loss in iteration 125 : 0.3631425064572857
Loss in iteration 126 : 0.36301438629629407
Loss in iteration 127 : 0.36291551059044086
Loss in iteration 128 : 0.36285721849506275
Loss in iteration 129 : 0.362837073683347
Loss in iteration 130 : 0.3628515603657065
Loss in iteration 131 : 0.3629135467196958
Loss in iteration 132 : 0.3629592189227389
Loss in iteration 133 : 0.36297305881133474
Loss in iteration 134 : 0.3630184668246058
Loss in iteration 135 : 0.3630250089780259
Loss in iteration 136 : 0.3630173924273272
Loss in iteration 137 : 0.3630179697456494
Loss in iteration 138 : 0.36297192603184086
Loss in iteration 139 : 0.3629528849931395
Testing accuracy  of updater 7 on alg 1 with rate 0.19999999999999998 = 0.79025, training accuracy 0.8413726124959534, time elapsed: 1988 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9901698222177595
Loss in iteration 3 : 1.286101104179674
Loss in iteration 4 : 1.2738560220388047
Loss in iteration 5 : 0.983696749400688
Loss in iteration 6 : 0.47393732678109113
Loss in iteration 7 : 0.5369761164729079
Loss in iteration 8 : 0.883806432035308
Loss in iteration 9 : 0.817027110054357
Loss in iteration 10 : 0.5324492965224221
Loss in iteration 11 : 0.4482531222437648
Loss in iteration 12 : 0.5683716908593912
Loss in iteration 13 : 0.6856514301214347
Loss in iteration 14 : 0.6908373685225448
Loss in iteration 15 : 0.6085770140828747
Loss in iteration 16 : 0.5242743272045566
Loss in iteration 17 : 0.507562538107543
Loss in iteration 18 : 0.5577843395308313
Loss in iteration 19 : 0.6107773370925167
Loss in iteration 20 : 0.6174736113582819
Loss in iteration 21 : 0.5793552899895608
Loss in iteration 22 : 0.5348812874251068
Loss in iteration 23 : 0.5212786077244717
Loss in iteration 24 : 0.5411737541988544
Loss in iteration 25 : 0.5654777963200708
Loss in iteration 26 : 0.5707826402865046
Loss in iteration 27 : 0.5521135460455483
Loss in iteration 28 : 0.5241874168271041
Loss in iteration 29 : 0.5061585047892408
Loss in iteration 30 : 0.509554134685587
Loss in iteration 31 : 0.5203331899651839
Loss in iteration 32 : 0.522560877356497
Loss in iteration 33 : 0.5076746902376148
Loss in iteration 34 : 0.4876574292876601
Loss in iteration 35 : 0.47484975995554474
Loss in iteration 36 : 0.47491171749238953
Loss in iteration 37 : 0.47896001431838636
Loss in iteration 38 : 0.47298586571457213
Loss in iteration 39 : 0.45653279926181767
Loss in iteration 40 : 0.44324790090473976
Loss in iteration 41 : 0.439512217658771
Loss in iteration 42 : 0.43957981829363707
Loss in iteration 43 : 0.434181161553759
Loss in iteration 44 : 0.4215452858580838
Loss in iteration 45 : 0.41130583219451333
Loss in iteration 46 : 0.4077335333520831
Loss in iteration 47 : 0.4066693328850609
Loss in iteration 48 : 0.399913501513034
Loss in iteration 49 : 0.3896510338687008
Loss in iteration 50 : 0.3836266483811124
Loss in iteration 51 : 0.3844596386774646
Loss in iteration 52 : 0.37976406833180465
Loss in iteration 53 : 0.3710739103051804
Loss in iteration 54 : 0.3704945878062397
Loss in iteration 55 : 0.37058047584258197
Loss in iteration 56 : 0.36513161092169916
Loss in iteration 57 : 0.36439177340406514
Loss in iteration 58 : 0.3672934021037336
Loss in iteration 59 : 0.3637138968973041
Loss in iteration 60 : 0.3658690876086794
Loss in iteration 61 : 0.3680379705772251
Loss in iteration 62 : 0.36536921486883606
Loss in iteration 63 : 0.36901582693201235
Testing accuracy  of updater 7 on alg 1 with rate 0.13999999999999999 = 0.7885, training accuracy 0.8429912593072192, time elapsed: 1009 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7236599347620467
Loss in iteration 3 : 0.8989063578266818
Loss in iteration 4 : 0.89769840789821
Loss in iteration 5 : 0.7371086778385083
Loss in iteration 6 : 0.453871429918482
Loss in iteration 7 : 0.4725637521614686
Loss in iteration 8 : 0.6530039678705976
Loss in iteration 9 : 0.5826491552690017
Loss in iteration 10 : 0.4175584476265711
Loss in iteration 11 : 0.3882898954096578
Loss in iteration 12 : 0.4731271855827713
Loss in iteration 13 : 0.522244059380694
Loss in iteration 14 : 0.49429270303195516
Loss in iteration 15 : 0.43301013139690414
Loss in iteration 16 : 0.4047624725248877
Loss in iteration 17 : 0.42636067616769563
Loss in iteration 18 : 0.4612774887456306
Loss in iteration 19 : 0.4759752600673077
Loss in iteration 20 : 0.4627497341237814
Loss in iteration 21 : 0.43706802210799645
Loss in iteration 22 : 0.4218660647243085
Loss in iteration 23 : 0.42580347359781806
Loss in iteration 24 : 0.44235756377626995
Loss in iteration 25 : 0.45220702161739057
Loss in iteration 26 : 0.4474150547779551
Loss in iteration 27 : 0.43275626441563025
Loss in iteration 28 : 0.42123143485188996
Loss in iteration 29 : 0.42096208258285506
Loss in iteration 30 : 0.4267135666938987
Loss in iteration 31 : 0.43136376930351505
Loss in iteration 32 : 0.42851987286029863
Loss in iteration 33 : 0.41959849610403543
Loss in iteration 34 : 0.41167097898768856
Loss in iteration 35 : 0.40873839689336205
Loss in iteration 36 : 0.4106095006613173
Loss in iteration 37 : 0.41257674109394427
Loss in iteration 38 : 0.4094004272720374
Loss in iteration 39 : 0.4025487042369161
Loss in iteration 40 : 0.39684529560388887
Loss in iteration 41 : 0.39449955363495587
Loss in iteration 42 : 0.3947112431000053
Loss in iteration 43 : 0.3941927813118236
Loss in iteration 44 : 0.39042451385811966
Loss in iteration 45 : 0.3851918611139203
Loss in iteration 46 : 0.3820466940776866
Loss in iteration 47 : 0.3811722901515824
Loss in iteration 48 : 0.38052828837411556
Loss in iteration 49 : 0.37811203696221585
Loss in iteration 50 : 0.3744457072810501
Loss in iteration 51 : 0.3718062181168916
Loss in iteration 52 : 0.37174964963661805
Loss in iteration 53 : 0.37094617267185975
Loss in iteration 54 : 0.36857805631214896
Loss in iteration 55 : 0.3664122905724204
Loss in iteration 56 : 0.366229266776442
Loss in iteration 57 : 0.36622175934248274
Loss in iteration 58 : 0.36463922228030826
Loss in iteration 59 : 0.36383123919077115
Loss in iteration 60 : 0.3641248035105735
Loss in iteration 61 : 0.3643096394958488
Loss in iteration 62 : 0.36368963503030566
Loss in iteration 63 : 0.36324944204984727
Loss in iteration 64 : 0.3637014594835235
Loss in iteration 65 : 0.36400916843905257
Loss in iteration 66 : 0.3636313116656915
Loss in iteration 67 : 0.36360871525222394
Loss in iteration 68 : 0.36405067431489924
Loss in iteration 69 : 0.36416581682859883
Loss in iteration 70 : 0.36389102262351347
Loss in iteration 71 : 0.36388033579959883
Loss in iteration 72 : 0.3641292339408233
Loss in iteration 73 : 0.3640729325872484
Loss in iteration 74 : 0.3638046421519797
Loss in iteration 75 : 0.36376844346504605
Loss in iteration 76 : 0.36380999581599716
Loss in iteration 77 : 0.36368373517499125
Loss in iteration 78 : 0.3634855636481049
Loss in iteration 79 : 0.36339945573792176
Loss in iteration 80 : 0.3634023899920921
Loss in iteration 81 : 0.36325496761517106
Loss in iteration 82 : 0.3631290401228537
Loss in iteration 83 : 0.3630937828898873
Loss in iteration 84 : 0.36305654172102003
Loss in iteration 85 : 0.36297979934769065
Loss in iteration 86 : 0.36291337058417134
Loss in iteration 87 : 0.3628904902247833
Loss in iteration 88 : 0.36289150910795737
Loss in iteration 89 : 0.36283385007678626
Loss in iteration 90 : 0.36281586978659214
Loss in iteration 91 : 0.36283259059789214
Loss in iteration 92 : 0.36281434030372456
Loss in iteration 93 : 0.3627841827438489
Loss in iteration 94 : 0.3628154673565403
Loss in iteration 95 : 0.36281923714785985
Loss in iteration 96 : 0.36278521294966376
Loss in iteration 97 : 0.36279172980856
Loss in iteration 98 : 0.3628091344996536
Loss in iteration 99 : 0.36279150956244505
Loss in iteration 100 : 0.3627765626029688
Loss in iteration 101 : 0.36279030211193114
Loss in iteration 102 : 0.3627858844797704
Loss in iteration 103 : 0.36276746417525635
Loss in iteration 104 : 0.3627768082236588
Loss in iteration 105 : 0.3627698956839894
Loss in iteration 106 : 0.36275359117304556
Loss in iteration 107 : 0.36276171132528323
Loss in iteration 108 : 0.3627543550733357
Loss in iteration 109 : 0.3627393205743306
Loss in iteration 110 : 0.3627425401488558
Loss in iteration 111 : 0.3627314826677297
Loss in iteration 112 : 0.36272971383739877
Loss in iteration 113 : 0.36272963685430776
Loss in iteration 114 : 0.3627238083461351
Loss in iteration 115 : 0.3627220602982181
Loss in iteration 116 : 0.3627218788972735
Loss in iteration 117 : 0.3627200479980231
Loss in iteration 118 : 0.36271956354754403
Loss in iteration 119 : 0.362719926914973
Loss in iteration 120 : 0.3627192207114597
Testing accuracy  of updater 7 on alg 1 with rate 0.08 = 0.79075, training accuracy 0.8423438005827129, time elapsed: 1786 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6760332610581355
Loss in iteration 3 : 0.6007379147904467
Loss in iteration 4 : 0.6996287533090693
Loss in iteration 5 : 0.7520688409341406
Loss in iteration 6 : 0.7596350945621709
Loss in iteration 7 : 0.7265472795294895
Loss in iteration 8 : 0.6569745537225096
Loss in iteration 9 : 0.5575193360277246
Loss in iteration 10 : 0.46304410401043067
Loss in iteration 11 : 0.42964414863921563
Loss in iteration 12 : 0.4722768339391517
Loss in iteration 13 : 0.5234551597889573
Loss in iteration 14 : 0.5237830844576921
Loss in iteration 15 : 0.47721111787478
Loss in iteration 16 : 0.418759531762193
Loss in iteration 17 : 0.3843752017548801
Loss in iteration 18 : 0.3853942732962132
Loss in iteration 19 : 0.40614711019388183
Loss in iteration 20 : 0.42260600815678395
Loss in iteration 21 : 0.4236863187487002
Loss in iteration 22 : 0.4101300189168824
Loss in iteration 23 : 0.39140426179263016
Loss in iteration 24 : 0.37729310569882607
Loss in iteration 25 : 0.3721168655473806
Loss in iteration 26 : 0.3758431960079897
Loss in iteration 27 : 0.38348299634458793
Loss in iteration 28 : 0.3895592440355432
Loss in iteration 29 : 0.39083717743361596
Loss in iteration 30 : 0.3872908158262593
Loss in iteration 31 : 0.3813041094709237
Loss in iteration 32 : 0.3756548436440155
Loss in iteration 33 : 0.3725709443077662
Loss in iteration 34 : 0.372968075452875
Loss in iteration 35 : 0.37537993779391426
Loss in iteration 36 : 0.3781739674366988
Loss in iteration 37 : 0.37986173015098385
Loss in iteration 38 : 0.37965702369628973
Loss in iteration 39 : 0.3778951088460197
Loss in iteration 40 : 0.37553742503709153
Loss in iteration 41 : 0.3735748776683989
Loss in iteration 42 : 0.37257329628884855
Loss in iteration 43 : 0.3727467389982946
Loss in iteration 44 : 0.3737406052652696
Loss in iteration 45 : 0.3748327363336037
Loss in iteration 46 : 0.3752218174399867
Loss in iteration 47 : 0.3747578488167642
Loss in iteration 48 : 0.3736615807543862
Loss in iteration 49 : 0.3725189230318119
Loss in iteration 50 : 0.37181198696528395
Loss in iteration 51 : 0.37164642047663593
Loss in iteration 52 : 0.37187909248288187
Loss in iteration 53 : 0.3722585621060618
Loss in iteration 54 : 0.37240950386493316
Loss in iteration 55 : 0.3722138522141439
Loss in iteration 56 : 0.3717023503271543
Loss in iteration 57 : 0.37108136695184585
Loss in iteration 58 : 0.3706116238878863
Loss in iteration 59 : 0.37036291099795293
Loss in iteration 60 : 0.3703239879466224
Loss in iteration 61 : 0.3703869208609809
Loss in iteration 62 : 0.37038507849580105
Loss in iteration 63 : 0.3702423871741573
Loss in iteration 64 : 0.3699530094439443
Loss in iteration 65 : 0.36960562155985466
Loss in iteration 66 : 0.3693036777217608
Loss in iteration 67 : 0.36914474587793916
Loss in iteration 68 : 0.36908786786206094
Loss in iteration 69 : 0.36905356514457793
Loss in iteration 70 : 0.36897511366108093
Loss in iteration 71 : 0.368832231478437
Loss in iteration 72 : 0.36864204829838637
Loss in iteration 73 : 0.3684352248274585
Loss in iteration 74 : 0.3682393054322606
Loss in iteration 75 : 0.3681195505406776
Loss in iteration 76 : 0.36806413803597493
Loss in iteration 77 : 0.3680193266177709
Loss in iteration 78 : 0.36792958223217154
Loss in iteration 79 : 0.3677931448108082
Loss in iteration 80 : 0.3676439059043819
Loss in iteration 81 : 0.3675206129763287
Loss in iteration 82 : 0.36743655873142245
Loss in iteration 83 : 0.3673703443010011
Loss in iteration 84 : 0.36730003951877055
Loss in iteration 85 : 0.36722047629489785
Loss in iteration 86 : 0.36712786026945227
Loss in iteration 87 : 0.3670258732288516
Loss in iteration 88 : 0.366932745431797
Loss in iteration 89 : 0.36686290732442794
Loss in iteration 90 : 0.3667989405807015
Loss in iteration 91 : 0.36673388188110767
Loss in iteration 92 : 0.36666485544909905
Loss in iteration 93 : 0.3665921922012544
Loss in iteration 94 : 0.36651832946283985
Loss in iteration 95 : 0.3664480036416814
Loss in iteration 96 : 0.36638262270126726
Loss in iteration 97 : 0.366324440311649
Loss in iteration 98 : 0.366275592700738
Loss in iteration 99 : 0.36622258755657755
Loss in iteration 100 : 0.36616462302155617
Loss in iteration 101 : 0.36610777905488895
Loss in iteration 102 : 0.3660540927805544
Loss in iteration 103 : 0.3660040519670193
Loss in iteration 104 : 0.3659553185314197
Loss in iteration 105 : 0.3659071764204075
Loss in iteration 106 : 0.3658605906493483
Loss in iteration 107 : 0.36581393749085483
Loss in iteration 108 : 0.3657668622241355
Loss in iteration 109 : 0.36571940162929206
Loss in iteration 110 : 0.36567158883649387
Loss in iteration 111 : 0.3656234536876801
Loss in iteration 112 : 0.3655768751127352
Loss in iteration 113 : 0.36553273196274716
Loss in iteration 114 : 0.36548902863540345
Loss in iteration 115 : 0.36544499331054503
Loss in iteration 116 : 0.36540070723364515
Loss in iteration 117 : 0.36535821207822156
Loss in iteration 118 : 0.36531718255068274
Loss in iteration 119 : 0.3652765766551907
Loss in iteration 120 : 0.36523609743099344
Loss in iteration 121 : 0.36519790714657363
Loss in iteration 122 : 0.36516125229596885
Loss in iteration 123 : 0.3651251067945683
Loss in iteration 124 : 0.36508943797597404
Loss in iteration 125 : 0.365054684735051
Loss in iteration 126 : 0.3650206501508053
Loss in iteration 127 : 0.3649871046670755
Loss in iteration 128 : 0.36495371718468345
Loss in iteration 129 : 0.36492069023216656
Loss in iteration 130 : 0.364889694655879
Loss in iteration 131 : 0.3648587167786553
Loss in iteration 132 : 0.3648268323717573
Loss in iteration 133 : 0.364799400738049
Loss in iteration 134 : 0.3647716691939414
Loss in iteration 135 : 0.3647429435397476
Loss in iteration 136 : 0.36471601115663715
Loss in iteration 137 : 0.3646899662774112
Loss in iteration 138 : 0.3646639882754443
Loss in iteration 139 : 0.36463782827752006
Loss in iteration 140 : 0.36461243461005494
Loss in iteration 141 : 0.3645874171898714
Loss in iteration 142 : 0.36456282145454466
Loss in iteration 143 : 0.36453835124561135
Loss in iteration 144 : 0.3645139918093162
Loss in iteration 145 : 0.36448972985167083
Loss in iteration 146 : 0.36446555339378967
Loss in iteration 147 : 0.3644414560201482
Loss in iteration 148 : 0.3644177399201426
Loss in iteration 149 : 0.3643941252597881
Loss in iteration 150 : 0.3643705997354513
Loss in iteration 151 : 0.36434741695743256
Loss in iteration 152 : 0.3643247056140153
Loss in iteration 153 : 0.36430312591805253
Loss in iteration 154 : 0.36428231654640525
Loss in iteration 155 : 0.36426174598555416
Loss in iteration 156 : 0.3642414669319012
Loss in iteration 157 : 0.3642211601064342
Loss in iteration 158 : 0.36420099871699385
Loss in iteration 159 : 0.36418103662881857
Loss in iteration 160 : 0.36416133032586495
Loss in iteration 161 : 0.3641421601445965
Loss in iteration 162 : 0.3641231158331373
Loss in iteration 163 : 0.3641041171067214
Loss in iteration 164 : 0.36408522897637013
Loss in iteration 165 : 0.364067132954068
Loss in iteration 166 : 0.3640492081215277
Loss in iteration 167 : 0.3640316542507105
Loss in iteration 168 : 0.36401445294860263
Loss in iteration 169 : 0.3639974097075104
Loss in iteration 170 : 0.36398062913155277
Loss in iteration 171 : 0.3639642878338444
Loss in iteration 172 : 0.36394811390078463
Loss in iteration 173 : 0.3639320324295828
Loss in iteration 174 : 0.36391616856642417
Loss in iteration 175 : 0.36390039427352155
Loss in iteration 176 : 0.3638846991905345
Loss in iteration 177 : 0.3638692274664357
Loss in iteration 178 : 0.36385380026927316
Loss in iteration 179 : 0.3638385341341141
Loss in iteration 180 : 0.3638240854091897
Loss in iteration 181 : 0.36380966381713975
Loss in iteration 182 : 0.36379556026387755
Loss in iteration 183 : 0.36378172487718924
Loss in iteration 184 : 0.36376805754039016
Loss in iteration 185 : 0.3637544718544742
Loss in iteration 186 : 0.3637424146321725
Loss in iteration 187 : 0.36373063207555617
Loss in iteration 188 : 0.36371897153826893
Loss in iteration 189 : 0.36370741988163907
Loss in iteration 190 : 0.36369596526797404
Loss in iteration 191 : 0.3636845970316397
Loss in iteration 192 : 0.36367330556290556
Loss in iteration 193 : 0.36366208220330665
Loss in iteration 194 : 0.36365091915137215
Loss in iteration 195 : 0.36363980937769663
Loss in iteration 196 : 0.3636287465484279
Loss in iteration 197 : 0.36361805068055164
Loss in iteration 198 : 0.36360750559409794
Loss in iteration 199 : 0.363596831381762
Loss in iteration 200 : 0.3635860396644929
Testing accuracy  of updater 7 on alg 1 with rate 0.02 = 0.7885, training accuracy 0.8429912593072192, time elapsed: 2447 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7634230770595654
Loss in iteration 3 : 0.5636277003452038
Loss in iteration 4 : 0.6356890725982173
Loss in iteration 5 : 0.6964301259578546
Loss in iteration 6 : 0.7239586296647124
Loss in iteration 7 : 0.7209878382983493
Loss in iteration 8 : 0.6904054722769701
Loss in iteration 9 : 0.6352587740120261
Loss in iteration 10 : 0.5603555118841013
Loss in iteration 11 : 0.4871056700048933
Loss in iteration 12 : 0.44819301620949753
Loss in iteration 13 : 0.44988143282393017
Loss in iteration 14 : 0.48141990718961997
Loss in iteration 15 : 0.5063951958940676
Loss in iteration 16 : 0.5004703754364861
Loss in iteration 17 : 0.46745690312809296
Loss in iteration 18 : 0.42703501624454293
Loss in iteration 19 : 0.39774088704217087
Loss in iteration 20 : 0.3890783009029617
Loss in iteration 21 : 0.39732870754331145
Loss in iteration 22 : 0.4090634284814982
Loss in iteration 23 : 0.4146424751263582
Loss in iteration 24 : 0.4113483689127948
Loss in iteration 25 : 0.4007392148901751
Loss in iteration 26 : 0.3878412742557842
Loss in iteration 27 : 0.37847074354391913
Loss in iteration 28 : 0.37436028825715684
Loss in iteration 29 : 0.3751139211543022
Loss in iteration 30 : 0.3787720178756948
Loss in iteration 31 : 0.38245465724689476
Loss in iteration 32 : 0.3841889996621385
Loss in iteration 33 : 0.3833591974650732
Loss in iteration 34 : 0.38039782665106053
Loss in iteration 35 : 0.37670440506306374
Loss in iteration 36 : 0.3734788958275004
Loss in iteration 37 : 0.37159247323596045
Loss in iteration 38 : 0.3712580392531936
Loss in iteration 39 : 0.3723128369474184
Loss in iteration 40 : 0.37377497751194066
Loss in iteration 41 : 0.37481467662421014
Loss in iteration 42 : 0.37502072873421916
Loss in iteration 43 : 0.3743868358706059
Loss in iteration 44 : 0.37321737248031156
Loss in iteration 45 : 0.37193480080902797
Loss in iteration 46 : 0.3709428617807477
Loss in iteration 47 : 0.37055714405888396
Loss in iteration 48 : 0.370681234011729
Loss in iteration 49 : 0.3711148050421867
Loss in iteration 50 : 0.3715590714426155
Loss in iteration 51 : 0.371795693670485
Loss in iteration 52 : 0.3717149816634561
Loss in iteration 53 : 0.371364026393548
Loss in iteration 54 : 0.3708691193320036
Loss in iteration 55 : 0.370392319371292
Loss in iteration 56 : 0.3700437002768826
Loss in iteration 57 : 0.3699646335122172
Loss in iteration 58 : 0.37003006069834554
Loss in iteration 59 : 0.3701517061957268
Loss in iteration 60 : 0.37023567478405006
Loss in iteration 61 : 0.37022668179479223
Loss in iteration 62 : 0.37010120768165083
Loss in iteration 63 : 0.3699007595440405
Loss in iteration 64 : 0.36967880402444914
Loss in iteration 65 : 0.36948202692910853
Loss in iteration 66 : 0.36934959316728055
Loss in iteration 67 : 0.3692907252301941
Loss in iteration 68 : 0.36930077032839703
Loss in iteration 69 : 0.36931969821674304
Loss in iteration 70 : 0.36929669832900547
Loss in iteration 71 : 0.3692288905829476
Loss in iteration 72 : 0.3691264705198135
Loss in iteration 73 : 0.3689989968518791
Loss in iteration 74 : 0.3688718655266333
Loss in iteration 75 : 0.3687754348285684
Loss in iteration 76 : 0.36870720705416005
Loss in iteration 77 : 0.3686583041116331
Loss in iteration 78 : 0.36862240452354716
Loss in iteration 79 : 0.3685854093312236
Loss in iteration 80 : 0.36853610653031177
Loss in iteration 81 : 0.36846976813896687
Loss in iteration 82 : 0.3683936806916224
Loss in iteration 83 : 0.3683125945537896
Loss in iteration 84 : 0.36823607895883204
Loss in iteration 85 : 0.3681735178766097
Loss in iteration 86 : 0.3681165264854263
Loss in iteration 87 : 0.3680682472339291
Loss in iteration 88 : 0.3680224103378979
Loss in iteration 89 : 0.36797343746789124
Loss in iteration 90 : 0.36791983909635156
Loss in iteration 91 : 0.36786275356194814
Loss in iteration 92 : 0.3678024527143907
Loss in iteration 93 : 0.367740742404917
Loss in iteration 94 : 0.36768125449177286
Loss in iteration 95 : 0.36762510626832307
Loss in iteration 96 : 0.3675780734305592
Loss in iteration 97 : 0.3675325175347682
Loss in iteration 98 : 0.367485241441945
Loss in iteration 99 : 0.3674352326717308
Loss in iteration 100 : 0.36738289069848784
Loss in iteration 101 : 0.36733108579424106
Loss in iteration 102 : 0.3672798164105748
Loss in iteration 103 : 0.3672290123753665
Loss in iteration 104 : 0.3671824852955668
Loss in iteration 105 : 0.36713935218074817
Loss in iteration 106 : 0.3670943563916091
Loss in iteration 107 : 0.3670482702483901
Loss in iteration 108 : 0.3670035526212501
Loss in iteration 109 : 0.36696137072589124
Loss in iteration 110 : 0.366920645382477
Loss in iteration 111 : 0.3668805192293533
Loss in iteration 112 : 0.3668403531089939
Loss in iteration 113 : 0.3668001469616617
Loss in iteration 114 : 0.36675990072951703
Loss in iteration 115 : 0.36672142442486094
Loss in iteration 116 : 0.36668386812831
Loss in iteration 117 : 0.366646587565304
Loss in iteration 118 : 0.36660955196138634
Loss in iteration 119 : 0.36657273358836734
Loss in iteration 120 : 0.3665362480252204
Loss in iteration 121 : 0.366500517315252
Loss in iteration 122 : 0.366464922544471
Loss in iteration 123 : 0.36642944683307677
Loss in iteration 124 : 0.3663940749706277
Loss in iteration 125 : 0.36635879325061116
Loss in iteration 126 : 0.36632358932139647
Loss in iteration 127 : 0.3662884520519681
Loss in iteration 128 : 0.36625337141095693
Loss in iteration 129 : 0.36621833835766665
Loss in iteration 130 : 0.3661833447438915
Loss in iteration 131 : 0.36614838322547194
Loss in iteration 132 : 0.36611415772820755
Loss in iteration 133 : 0.3660807186072913
Loss in iteration 134 : 0.3660473830063414
Loss in iteration 135 : 0.3660141230960249
Loss in iteration 136 : 0.36598057356013103
Loss in iteration 137 : 0.36594703767000397
Loss in iteration 138 : 0.36591342132494376
Loss in iteration 139 : 0.36587981358047184
Loss in iteration 140 : 0.3658467346865491
Loss in iteration 141 : 0.3658151583832019
Loss in iteration 142 : 0.3657843943985777
Loss in iteration 143 : 0.36575335444909496
Loss in iteration 144 : 0.36572206255464645
Loss in iteration 145 : 0.3656906361169
Loss in iteration 146 : 0.3656591431524341
Loss in iteration 147 : 0.36562803169304225
Loss in iteration 148 : 0.36559685924927493
Loss in iteration 149 : 0.3655656287173794
Loss in iteration 150 : 0.3655345391976122
Loss in iteration 151 : 0.3655041553867568
Loss in iteration 152 : 0.3654744524909214
Loss in iteration 153 : 0.3654455659476538
Loss in iteration 154 : 0.36541631259266344
Loss in iteration 155 : 0.3653868043028772
Loss in iteration 156 : 0.3653582436256497
Loss in iteration 157 : 0.3653304627418295
Loss in iteration 158 : 0.3653032371390044
Loss in iteration 159 : 0.36527671593827254
Loss in iteration 160 : 0.3652501917152793
Loss in iteration 161 : 0.3652237045152282
Loss in iteration 162 : 0.36519750370340004
Loss in iteration 163 : 0.3651713922231809
Loss in iteration 164 : 0.3651456214983504
Loss in iteration 165 : 0.36512007142331876
Loss in iteration 166 : 0.36509498491597764
Loss in iteration 167 : 0.3650701130867708
Loss in iteration 168 : 0.3650459151180395
Loss in iteration 169 : 0.3650221483022397
Loss in iteration 170 : 0.3649984101768751
Loss in iteration 171 : 0.36497488926196525
Loss in iteration 172 : 0.3649515992360842
Loss in iteration 173 : 0.3649289713467571
Loss in iteration 174 : 0.36490647627863626
Loss in iteration 175 : 0.36488427386691513
Loss in iteration 176 : 0.36486184616493744
Loss in iteration 177 : 0.36483991613701344
Loss in iteration 178 : 0.3648194737922023
Loss in iteration 179 : 0.36479879211045196
Loss in iteration 180 : 0.3647777602712773
Loss in iteration 181 : 0.36475826572143427
Loss in iteration 182 : 0.3647395196389751
Loss in iteration 183 : 0.36472057701325716
Loss in iteration 184 : 0.3647014977460476
Loss in iteration 185 : 0.36468220003385743
Loss in iteration 186 : 0.3646633517729494
Loss in iteration 187 : 0.36464550028985987
Loss in iteration 188 : 0.36462772965868806
Loss in iteration 189 : 0.3646099033287127
Loss in iteration 190 : 0.3645922155881727
Loss in iteration 191 : 0.3645745841755861
Loss in iteration 192 : 0.3645570018106961
Loss in iteration 193 : 0.3645394619330096
Loss in iteration 194 : 0.3645219586304723
Loss in iteration 195 : 0.3645044865752057
Loss in iteration 196 : 0.3644870409656144
Loss in iteration 197 : 0.36446972505469044
Loss in iteration 198 : 0.3644526078196449
Loss in iteration 199 : 0.364435533373436
Loss in iteration 200 : 0.36441849582055813
Testing accuracy  of updater 7 on alg 1 with rate 0.014 = 0.78775, training accuracy 0.8416963418582065, time elapsed: 2031 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.864348403105816
Loss in iteration 3 : 0.6386803603350336
Loss in iteration 4 : 0.5659675409851621
Loss in iteration 5 : 0.6168394451211993
Loss in iteration 6 : 0.6638128699424949
Loss in iteration 7 : 0.6912789935638043
Loss in iteration 8 : 0.700301171531648
Loss in iteration 9 : 0.692611962696288
Loss in iteration 10 : 0.6698345327748515
Loss in iteration 11 : 0.6336931403881789
Loss in iteration 12 : 0.5862944830730281
Loss in iteration 13 : 0.534101892543325
Loss in iteration 14 : 0.49149941531182084
Loss in iteration 15 : 0.4700927706874917
Loss in iteration 16 : 0.4680838118682268
Loss in iteration 17 : 0.47630803474607986
Loss in iteration 18 : 0.48743213633809934
Loss in iteration 19 : 0.49453903737941346
Loss in iteration 20 : 0.49055767607180384
Loss in iteration 21 : 0.47507854492169704
Loss in iteration 22 : 0.45317544463974524
Loss in iteration 23 : 0.4316411396093522
Loss in iteration 24 : 0.4165327025198629
Loss in iteration 25 : 0.40870974590971815
Loss in iteration 26 : 0.4076667771484437
Loss in iteration 27 : 0.4094800969743995
Loss in iteration 28 : 0.4112309849047375
Loss in iteration 29 : 0.4111196230672684
Loss in iteration 30 : 0.4082850825294116
Loss in iteration 31 : 0.40309408874398067
Loss in iteration 32 : 0.39644577760807
Loss in iteration 33 : 0.3899384856123637
Loss in iteration 34 : 0.3853826811614919
Loss in iteration 35 : 0.38318449334411575
Loss in iteration 36 : 0.3825455317485505
Loss in iteration 37 : 0.38299376184934614
Loss in iteration 38 : 0.3835676779582581
Loss in iteration 39 : 0.38381651676286077
Loss in iteration 40 : 0.3834958138152907
Loss in iteration 41 : 0.38252687406003555
Loss in iteration 42 : 0.3810682206745651
Loss in iteration 43 : 0.3794045171486274
Loss in iteration 44 : 0.37785056367734116
Loss in iteration 45 : 0.3765913722303858
Loss in iteration 46 : 0.3756778868908227
Loss in iteration 47 : 0.3751818252362409
Loss in iteration 48 : 0.3750182896002091
Loss in iteration 49 : 0.3751022921366105
Loss in iteration 50 : 0.3752205502612938
Loss in iteration 51 : 0.3752373890255726
Loss in iteration 52 : 0.3750951750729478
Loss in iteration 53 : 0.3747923301150998
Loss in iteration 54 : 0.3743743785347772
Loss in iteration 55 : 0.3739058823946427
Loss in iteration 56 : 0.3734430494579036
Loss in iteration 57 : 0.3730142965783859
Loss in iteration 58 : 0.37267000983411575
Loss in iteration 59 : 0.37241169568046684
Loss in iteration 60 : 0.37230369128108254
Loss in iteration 61 : 0.37227881513537847
Loss in iteration 62 : 0.3722699701648862
Loss in iteration 63 : 0.3722486329728851
Loss in iteration 64 : 0.37217704112668804
Loss in iteration 65 : 0.3720455525624698
Loss in iteration 66 : 0.3718663037489588
Loss in iteration 67 : 0.37165515394100107
Loss in iteration 68 : 0.3714398325016607
Loss in iteration 69 : 0.3712747003928641
Loss in iteration 70 : 0.37115311270803647
Loss in iteration 71 : 0.3710802033420184
Loss in iteration 72 : 0.3710280137249393
Loss in iteration 73 : 0.3709785291232516
Loss in iteration 74 : 0.3709263929272458
Loss in iteration 75 : 0.37086610172505624
Loss in iteration 76 : 0.3707947890500144
Loss in iteration 77 : 0.3707139011499286
Loss in iteration 78 : 0.37062810703806287
Loss in iteration 79 : 0.3705404933799452
Loss in iteration 80 : 0.37045043827025564
Loss in iteration 81 : 0.370366596457022
Loss in iteration 82 : 0.3702963574170497
Loss in iteration 83 : 0.3702332447246875
Loss in iteration 84 : 0.37017916674480067
Loss in iteration 85 : 0.37013152780929326
Loss in iteration 86 : 0.3700836292186548
Loss in iteration 87 : 0.37003459112728704
Loss in iteration 88 : 0.36998170709772105
Loss in iteration 89 : 0.36992728401079666
Loss in iteration 90 : 0.36987184616932206
Loss in iteration 91 : 0.3698183858338455
Loss in iteration 92 : 0.36976669826784486
Loss in iteration 93 : 0.3697162517631456
Loss in iteration 94 : 0.3696662198583189
Loss in iteration 95 : 0.3696165568776969
Loss in iteration 96 : 0.3695677965590739
Loss in iteration 97 : 0.36952255577640003
Loss in iteration 98 : 0.3694784824006061
Loss in iteration 99 : 0.36943444890850796
Loss in iteration 100 : 0.3693904470155875
Loss in iteration 101 : 0.3693466941469312
Loss in iteration 102 : 0.3693032127317062
Loss in iteration 103 : 0.3692598066679364
Loss in iteration 104 : 0.36921691755217223
Loss in iteration 105 : 0.3691741788784964
Loss in iteration 106 : 0.3691320177785364
Loss in iteration 107 : 0.3690909230113923
Loss in iteration 108 : 0.36905002444088103
Loss in iteration 109 : 0.36900929874395605
Loss in iteration 110 : 0.3689693235692061
Loss in iteration 111 : 0.36892962688404873
Loss in iteration 112 : 0.368890133385671
Loss in iteration 113 : 0.3688510062343093
Loss in iteration 114 : 0.36881203564861015
Loss in iteration 115 : 0.3687732023884321
Loss in iteration 116 : 0.36873448911658757
Loss in iteration 117 : 0.3686962725621536
Loss in iteration 118 : 0.3686586181792923
Loss in iteration 119 : 0.3686214353981397
Loss in iteration 120 : 0.3685846661922479
Loss in iteration 121 : 0.3685481967178122
Loss in iteration 122 : 0.3685117300253881
Loss in iteration 123 : 0.3684755040068467
Loss in iteration 124 : 0.3684403443217636
Loss in iteration 125 : 0.3684059647993465
Loss in iteration 126 : 0.3683716417123089
Loss in iteration 127 : 0.36833736611405854
Loss in iteration 128 : 0.36830319939530504
Loss in iteration 129 : 0.36826942321321227
Loss in iteration 130 : 0.3682359217131826
Loss in iteration 131 : 0.3682030354136753
Loss in iteration 132 : 0.3681703078610398
Loss in iteration 133 : 0.36813764782249425
Loss in iteration 134 : 0.3681050454285163
Loss in iteration 135 : 0.36807249178446383
Loss in iteration 136 : 0.36803997887395984
Loss in iteration 137 : 0.36800761959208167
Loss in iteration 138 : 0.3679757436414472
Loss in iteration 139 : 0.36794484243252973
Loss in iteration 140 : 0.3679145663984346
Loss in iteration 141 : 0.36788440370289854
Loss in iteration 142 : 0.36785434022429436
Loss in iteration 143 : 0.36782441486890305
Loss in iteration 144 : 0.3677950393905235
Loss in iteration 145 : 0.3677657902994661
Loss in iteration 146 : 0.3677367475886327
Loss in iteration 147 : 0.36770790502866896
Loss in iteration 148 : 0.36767916571772163
Loss in iteration 149 : 0.36765070846347553
Loss in iteration 150 : 0.3676224093879082
Loss in iteration 151 : 0.36759417014452
Loss in iteration 152 : 0.36756598205666685
Loss in iteration 153 : 0.3675378373048035
Loss in iteration 154 : 0.3675100941168121
Loss in iteration 155 : 0.36748244738178115
Loss in iteration 156 : 0.36745486555605755
Loss in iteration 157 : 0.3674273831372488
Loss in iteration 158 : 0.3673999695492004
Loss in iteration 159 : 0.3673725861935551
Loss in iteration 160 : 0.36734522738518965
Loss in iteration 161 : 0.36731788799968096
Loss in iteration 162 : 0.36729056341773575
Loss in iteration 163 : 0.3672634507357761
Loss in iteration 164 : 0.36723668022890094
Loss in iteration 165 : 0.3672099467733351
Loss in iteration 166 : 0.36718314714111444
Loss in iteration 167 : 0.3671562851721291
Loss in iteration 168 : 0.3671293997974349
Loss in iteration 169 : 0.3671028260783031
Loss in iteration 170 : 0.36707627589085756
Loss in iteration 171 : 0.3670498405272113
Loss in iteration 172 : 0.36702402362660885
Loss in iteration 173 : 0.36699825104122785
Loss in iteration 174 : 0.36697251586283774
Loss in iteration 175 : 0.3669468118652855
Loss in iteration 176 : 0.36692113343690624
Loss in iteration 177 : 0.3668954755196188
Loss in iteration 178 : 0.36686983355406744
Loss in iteration 179 : 0.3668442034301967
Loss in iteration 180 : 0.36681858144272006
Loss in iteration 181 : 0.3667930435429461
Loss in iteration 182 : 0.3667679333222136
Loss in iteration 183 : 0.36674283025962373
Loss in iteration 184 : 0.36671773116208556
Loss in iteration 185 : 0.3666926331504533
Loss in iteration 186 : 0.366667533628415
Loss in iteration 187 : 0.36664243025445714
Loss in iteration 188 : 0.36661732091661664
Loss in iteration 189 : 0.3665922714223321
Loss in iteration 190 : 0.3665672091481479
Loss in iteration 191 : 0.3665420703260043
Loss in iteration 192 : 0.36651711569268136
Loss in iteration 193 : 0.3664930518046701
Loss in iteration 194 : 0.3664697789556008
Loss in iteration 195 : 0.36644719157352024
Loss in iteration 196 : 0.36642446530191214
Loss in iteration 197 : 0.36640172484707184
Loss in iteration 198 : 0.3663794161781136
Loss in iteration 199 : 0.3663571692384899
Loss in iteration 200 : 0.36633497575968976
Testing accuracy  of updater 7 on alg 1 with rate 0.008 = 0.78475, training accuracy 0.8394302363224344, time elapsed: 2181 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9659152195122568
Testing accuracy  of updater 7 on alg 1 with rate 0.0019999999999999983 = 0.5, training accuracy 0.6474587245063127, time elapsed: 35 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.45595856897186
Loss in iteration 3 : 3.3346000372421325
Loss in iteration 4 : 2.8496158580761484
Loss in iteration 5 : 2.0924113344009534
Loss in iteration 6 : 1.1355196614157554
Loss in iteration 7 : 0.5359421778347093
Loss in iteration 8 : 0.6730934158860896
Loss in iteration 9 : 0.8500451902826742
Loss in iteration 10 : 0.8121922769735487
Loss in iteration 11 : 0.7134353093291759
Loss in iteration 12 : 0.649686238546608
Loss in iteration 13 : 0.6333192785621158
Loss in iteration 14 : 0.6459883054507559
Loss in iteration 15 : 0.662309515030324
Loss in iteration 16 : 0.6697857286241911
Loss in iteration 17 : 0.668041196918638
Loss in iteration 18 : 0.660726524436888
Loss in iteration 19 : 0.6533203148701741
Loss in iteration 20 : 0.6482573301019343
Loss in iteration 21 : 0.6439551683670373
Loss in iteration 22 : 0.6388341634178436
Loss in iteration 23 : 0.6315697327925924
Loss in iteration 24 : 0.6222713054569703
Loss in iteration 25 : 0.6117626699381516
Loss in iteration 26 : 0.6006298136373712
Loss in iteration 27 : 0.589001831763688
Loss in iteration 28 : 0.5767242903125913
Loss in iteration 29 : 0.5636812199115964
Loss in iteration 30 : 0.5499027042564519
Loss in iteration 31 : 0.535630917292609
Loss in iteration 32 : 0.5209392508183314
Loss in iteration 33 : 0.5059851092982771
Loss in iteration 34 : 0.49074228695600136
Loss in iteration 35 : 0.4753164205924832
Loss in iteration 36 : 0.4597574498840829
Loss in iteration 37 : 0.4443429426960708
Loss in iteration 38 : 0.42945999259938156
Loss in iteration 39 : 0.41505261646326924
Loss in iteration 40 : 0.401329041510077
Loss in iteration 41 : 0.3885389321162517
Loss in iteration 42 : 0.3774341325855557
Loss in iteration 43 : 0.3690668174728598
Loss in iteration 44 : 0.36453665163937204
Loss in iteration 45 : 0.3648614548363297
Loss in iteration 46 : 0.3729722339395212
Loss in iteration 47 : 0.4054026404118312
Loss in iteration 48 : 0.594700147950927
Loss in iteration 49 : 0.640421942156092
Loss in iteration 50 : 0.9993890970579239
Loss in iteration 51 : 0.5847243316324079
Loss in iteration 52 : 0.4724390068174592
Loss in iteration 53 : 0.37727461320673683
Loss in iteration 54 : 0.3831915125143654
Loss in iteration 55 : 0.3875368412817259
Loss in iteration 56 : 0.3909189730969007
Loss in iteration 57 : 0.39305137336582013
Loss in iteration 58 : 0.3938240858480847
Loss in iteration 59 : 0.39333492995379954
Loss in iteration 60 : 0.3917459931027933
Loss in iteration 61 : 0.3891986048427551
Loss in iteration 62 : 0.38591842846487034
Loss in iteration 63 : 0.38208799428162227
Loss in iteration 64 : 0.37795635254694854
Loss in iteration 65 : 0.3738826258841573
Loss in iteration 66 : 0.37017789094635445
Loss in iteration 67 : 0.36705535236897163
Loss in iteration 68 : 0.364836597678368
Loss in iteration 69 : 0.3639481566391865
Loss in iteration 70 : 0.3643905943422389
Loss in iteration 71 : 0.36567141676642967
Loss in iteration 72 : 0.37155272006796175
Loss in iteration 73 : 0.4015975980934347
Loss in iteration 74 : 0.5285139359618729
Loss in iteration 75 : 0.5359119685751657
Loss in iteration 76 : 0.6785838406708414
Loss in iteration 77 : 0.3694569533032397
Loss in iteration 78 : 0.37465698904067635
Loss in iteration 79 : 0.3729662547865466
Loss in iteration 80 : 0.3751393495762212
Loss in iteration 81 : 0.37598581636960565
Loss in iteration 82 : 0.37624273176563927
Loss in iteration 83 : 0.37578932229287604
Loss in iteration 84 : 0.37466399466343525
Loss in iteration 85 : 0.3730683161194063
Loss in iteration 86 : 0.3711982799505293
Loss in iteration 87 : 0.3692161341646411
Loss in iteration 88 : 0.36725238833442814
Loss in iteration 89 : 0.36565646048617373
Loss in iteration 90 : 0.36450733439524813
Loss in iteration 91 : 0.36398601927316265
Loss in iteration 92 : 0.3644374340923803
Loss in iteration 93 : 0.36739789200271333
Loss in iteration 94 : 0.38052187713427593
Loss in iteration 95 : 0.43395874420840663
Loss in iteration 96 : 0.5066951976414691
Loss in iteration 97 : 0.7275765329974
Loss in iteration 98 : 0.3680850536935799
Loss in iteration 99 : 0.3737612691756753
Loss in iteration 100 : 0.3716917645413904
Loss in iteration 101 : 0.37419016389522863
Loss in iteration 102 : 0.3737184269893005
Loss in iteration 103 : 0.3730443501432223
Loss in iteration 104 : 0.3719260096583485
Loss in iteration 105 : 0.37064694303282286
Loss in iteration 106 : 0.36897781620855596
Loss in iteration 107 : 0.3673251270972908
Loss in iteration 108 : 0.36585072346477066
Loss in iteration 109 : 0.36477699552637
Loss in iteration 110 : 0.36462009265271017
Loss in iteration 111 : 0.3659770336248914
Loss in iteration 112 : 0.3761727278287625
Loss in iteration 113 : 0.4079320344914764
Loss in iteration 114 : 0.5404067723478297
Loss in iteration 115 : 0.5363716290795651
Loss in iteration 116 : 0.6846049746242894
Loss in iteration 117 : 0.3779336571141174
Loss in iteration 118 : 0.37802804404550294
Loss in iteration 119 : 0.3791567690812515
Loss in iteration 120 : 0.3809719333594376
Loss in iteration 121 : 0.3812791405458572
Loss in iteration 122 : 0.38044061942866947
Loss in iteration 123 : 0.3786245603820214
Loss in iteration 124 : 0.3761689514054129
Loss in iteration 125 : 0.37339909499822815
Loss in iteration 126 : 0.370606115113696
Loss in iteration 127 : 0.3679590043363197
Loss in iteration 128 : 0.3658070964906333
Loss in iteration 129 : 0.36438651204924544
Loss in iteration 130 : 0.36406605003010406
Loss in iteration 131 : 0.36554841244223907
Loss in iteration 132 : 0.37531475875783893
Loss in iteration 133 : 0.424784678342887
Loss in iteration 134 : 0.6409417922496226
Loss in iteration 135 : 0.484399143021031
Loss in iteration 136 : 0.6078886604214379
Loss in iteration 137 : 0.41790714801354784
Loss in iteration 138 : 0.39435265060729424
Loss in iteration 139 : 0.39037736730810985
Loss in iteration 140 : 0.3844588212801385
Loss in iteration 141 : 0.38423553275162514
Loss in iteration 142 : 0.3839076739811259
Loss in iteration 143 : 0.3824315260228651
Loss in iteration 144 : 0.3800037381124703
Loss in iteration 145 : 0.3770289779978209
Loss in iteration 146 : 0.3736619942920467
Loss in iteration 147 : 0.37047019251006175
Loss in iteration 148 : 0.36761157611198675
Loss in iteration 149 : 0.3655036385270807
Loss in iteration 150 : 0.36418095407430034
Loss in iteration 151 : 0.3644036060393203
Loss in iteration 152 : 0.3670393533179774
Loss in iteration 153 : 0.38128815612769823
Loss in iteration 154 : 0.45500381217768143
Loss in iteration 155 : 0.7530218340603705
Loss in iteration 156 : 0.3858085914103139
Loss in iteration 157 : 0.3878745162495797
Loss in iteration 158 : 0.422624599588708
Loss in iteration 159 : 0.45790429682197903
Loss in iteration 160 : 0.4090444775467353
Loss in iteration 161 : 0.3993614765325618
Loss in iteration 162 : 0.38940186109734415
Loss in iteration 163 : 0.3824344799121876
Loss in iteration 164 : 0.37974429079131666
Loss in iteration 165 : 0.37737637066158985
Loss in iteration 166 : 0.37497384012574936
Loss in iteration 167 : 0.3725168100750258
Loss in iteration 168 : 0.37117026634842376
Loss in iteration 169 : 0.3715430336563595
Loss in iteration 170 : 0.38039731552153744
Loss in iteration 171 : 0.4177501389219868
Loss in iteration 172 : 0.4629096886361291
Loss in iteration 173 : 0.6481264805974885
Loss in iteration 174 : 0.4303572777228131
Loss in iteration 175 : 0.45078472562855715
Loss in iteration 176 : 0.42512701834654
Loss in iteration 177 : 0.4223542133253567
Loss in iteration 178 : 0.3970653005130339
Loss in iteration 179 : 0.3872228787572123
Loss in iteration 180 : 0.38354297387213954
Loss in iteration 181 : 0.38015649051003236
Loss in iteration 182 : 0.3772907335678186
Loss in iteration 183 : 0.37484743368721124
Loss in iteration 184 : 0.3733176652580984
Loss in iteration 185 : 0.37410141080802595
Loss in iteration 186 : 0.38043241894150776
Loss in iteration 187 : 0.40744664020275234
Loss in iteration 188 : 0.4419850962625606
Loss in iteration 189 : 0.5877642024279904
Loss in iteration 190 : 0.47015145801600916
Loss in iteration 191 : 0.5286507166616694
Loss in iteration 192 : 0.41650728407811677
Loss in iteration 193 : 0.4046209961740219
Loss in iteration 194 : 0.3937026493711323
Loss in iteration 195 : 0.3847621836650452
Loss in iteration 196 : 0.38197576667390337
Loss in iteration 197 : 0.3794335079905487
Loss in iteration 198 : 0.3761623475897243
Loss in iteration 199 : 0.3734369115910783
Loss in iteration 200 : 0.37112319591679055
Testing accuracy  of updater 8 on alg 1 with rate 0.056 = 0.7895, training accuracy 0.8420200712204597, time elapsed: 2301 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.1933939331141468
Loss in iteration 3 : 2.3343199961770225
Loss in iteration 4 : 2.125213635371097
Loss in iteration 5 : 1.6067342094739079
Loss in iteration 6 : 0.8765402341626846
Loss in iteration 7 : 0.5285100702795995
Loss in iteration 8 : 0.6551447957404577
Loss in iteration 9 : 0.7814798042370734
Loss in iteration 10 : 0.7524541176774436
Loss in iteration 11 : 0.6849884647139458
Loss in iteration 12 : 0.6462007691697775
Loss in iteration 13 : 0.6433435271005699
Loss in iteration 14 : 0.6571069509636542
Loss in iteration 15 : 0.6692749292400902
Loss in iteration 16 : 0.6732781691642858
Loss in iteration 17 : 0.6695069759554692
Loss in iteration 18 : 0.6619806579363428
Loss in iteration 19 : 0.6551474197167606
Loss in iteration 20 : 0.6491712962855831
Loss in iteration 21 : 0.6431647790035391
Loss in iteration 22 : 0.6356559837761852
Loss in iteration 23 : 0.626142377124667
Loss in iteration 24 : 0.6150351146836428
Loss in iteration 25 : 0.6030622524819408
Loss in iteration 26 : 0.5906926360039948
Loss in iteration 27 : 0.5777745238701889
Loss in iteration 28 : 0.5642572088106774
Loss in iteration 29 : 0.5502271102131567
Loss in iteration 30 : 0.5356913081654272
Loss in iteration 31 : 0.5208198491863801
Loss in iteration 32 : 0.5058260762785719
Loss in iteration 33 : 0.49067779112131077
Loss in iteration 34 : 0.47544819150662504
Loss in iteration 35 : 0.46034781547330833
Loss in iteration 36 : 0.4454059791080255
Loss in iteration 37 : 0.43097186659020836
Loss in iteration 38 : 0.4171759625170851
Loss in iteration 39 : 0.40427097150999275
Loss in iteration 40 : 0.39231935394631573
Loss in iteration 41 : 0.3817164135326591
Loss in iteration 42 : 0.37338550216228816
Loss in iteration 43 : 0.36738992132836534
Loss in iteration 44 : 0.3650379674199267
Loss in iteration 45 : 0.36612986259422237
Loss in iteration 46 : 0.3693248548429438
Loss in iteration 47 : 0.3733295166638389
Loss in iteration 48 : 0.376338524469486
Loss in iteration 49 : 0.3803125222128525
Loss in iteration 50 : 0.3948235970876319
Loss in iteration 51 : 0.4656543958740084
Loss in iteration 52 : 0.6342889434087684
Loss in iteration 53 : 0.3661607384077697
Loss in iteration 54 : 0.3657995847820878
Loss in iteration 55 : 0.3649057727929085
Loss in iteration 56 : 0.36658334263722153
Loss in iteration 57 : 0.3677035526778779
Loss in iteration 58 : 0.36854196783398185
Loss in iteration 59 : 0.3689584159298289
Loss in iteration 60 : 0.36896353851922253
Loss in iteration 61 : 0.3685820608388668
Loss in iteration 62 : 0.36784837949677285
Loss in iteration 63 : 0.36693027103909626
Loss in iteration 64 : 0.3659750167353257
Loss in iteration 65 : 0.3651054846738154
Loss in iteration 66 : 0.36427685354233413
Loss in iteration 67 : 0.36364597287784234
Loss in iteration 68 : 0.3632063811058016
Loss in iteration 69 : 0.3629258381137503
Loss in iteration 70 : 0.36282953275039354
Loss in iteration 71 : 0.3628869305904313
Loss in iteration 72 : 0.36304231309997986
Loss in iteration 73 : 0.36333200576162106
Loss in iteration 74 : 0.3639272781994388
Loss in iteration 75 : 0.3647746211248628
Loss in iteration 76 : 0.3666916760464802
Loss in iteration 77 : 0.3691282237981268
Loss in iteration 78 : 0.3733188911736799
Loss in iteration 79 : 0.38293060586714367
Loss in iteration 80 : 0.38787500753076143
Loss in iteration 81 : 0.39663226450257266
Loss in iteration 82 : 0.3810578352424579
Loss in iteration 83 : 0.3806383036226031
Loss in iteration 84 : 0.3747837172376432
Loss in iteration 85 : 0.3712152026453932
Loss in iteration 86 : 0.3688713029750283
Loss in iteration 87 : 0.3665762258956568
Loss in iteration 88 : 0.3656638874295647
Loss in iteration 89 : 0.3654389017245672
Loss in iteration 90 : 0.365314247551296
Loss in iteration 91 : 0.3647987692571428
Loss in iteration 92 : 0.36411382763106576
Loss in iteration 93 : 0.3637974955326417
Loss in iteration 94 : 0.36345949377035885
Loss in iteration 95 : 0.36320816024045577
Loss in iteration 96 : 0.36345404705786266
Loss in iteration 97 : 0.36352453736474066
Loss in iteration 98 : 0.36383165110461263
Loss in iteration 99 : 0.36429530050042036
Loss in iteration 100 : 0.364600077774737
Loss in iteration 101 : 0.3668567721714291
Loss in iteration 102 : 0.3714838060210659
Loss in iteration 103 : 0.38096014714491927
Loss in iteration 104 : 0.3901318676832096
Loss in iteration 105 : 0.4065839742051098
Loss in iteration 106 : 0.3925425379885191
Loss in iteration 107 : 0.3947008432093406
Loss in iteration 108 : 0.3821048809873384
Loss in iteration 109 : 0.37818019220803095
Loss in iteration 110 : 0.37261876235849617
Loss in iteration 111 : 0.36814190328927926
Loss in iteration 112 : 0.3676407555195596
Loss in iteration 113 : 0.36638386109521903
Loss in iteration 114 : 0.3657928265833822
Loss in iteration 115 : 0.3662649854518919
Loss in iteration 116 : 0.36542143035366775
Loss in iteration 117 : 0.36518446611165495
Loss in iteration 118 : 0.3648341223958007
Loss in iteration 119 : 0.3645831834730335
Loss in iteration 120 : 0.36431859963698415
Loss in iteration 121 : 0.36442245080012586
Loss in iteration 122 : 0.3642256123469173
Loss in iteration 123 : 0.3651403813395286
Loss in iteration 124 : 0.36693346726116677
Loss in iteration 125 : 0.37034044263089383
Loss in iteration 126 : 0.37884035180572456
Loss in iteration 127 : 0.39787292147749753
Loss in iteration 128 : 0.39503600512197456
Loss in iteration 129 : 0.409983515601946
Loss in iteration 130 : 0.3955535921613538
Loss in iteration 131 : 0.40008006677986935
Loss in iteration 132 : 0.3856375692716841
Loss in iteration 133 : 0.38042901724825307
Loss in iteration 134 : 0.37318072206925
Loss in iteration 135 : 0.36838856707635903
Loss in iteration 136 : 0.36852284154149095
Loss in iteration 137 : 0.36784031582068094
Loss in iteration 138 : 0.3679519161433746
Loss in iteration 139 : 0.3672635042918162
Loss in iteration 140 : 0.3670229836699091
Loss in iteration 141 : 0.367131120332309
Loss in iteration 142 : 0.3674280438032675
Loss in iteration 143 : 0.37060430289470464
Loss in iteration 144 : 0.3741514466383097
Loss in iteration 145 : 0.38227396996618773
Loss in iteration 146 : 0.3870517872712639
Loss in iteration 147 : 0.4032121471521443
Loss in iteration 148 : 0.39312820729586856
Loss in iteration 149 : 0.40536429339992586
Loss in iteration 150 : 0.3910526138382608
Loss in iteration 151 : 0.3940339133248578
Loss in iteration 152 : 0.382954233705461
Loss in iteration 153 : 0.3811752536182985
Loss in iteration 154 : 0.37559670698321507
Loss in iteration 155 : 0.37327387045339727
Loss in iteration 156 : 0.3721954524181255
Loss in iteration 157 : 0.3699050380010376
Loss in iteration 158 : 0.3680904860484469
Loss in iteration 159 : 0.3674606600490375
Loss in iteration 160 : 0.3673369485206617
Loss in iteration 161 : 0.36794155756228575
Loss in iteration 162 : 0.3686007425116536
Loss in iteration 163 : 0.37171201593050074
Loss in iteration 164 : 0.3778672260658337
Loss in iteration 165 : 0.3930176751014571
Loss in iteration 166 : 0.39326627304577755
Loss in iteration 167 : 0.4098464149539391
Loss in iteration 168 : 0.3979353260727304
Loss in iteration 169 : 0.40877116335532676
Loss in iteration 170 : 0.39086087924316454
Loss in iteration 171 : 0.39041500305361154
Loss in iteration 172 : 0.3784671461939084
Loss in iteration 173 : 0.37516836114596164
Loss in iteration 174 : 0.3739343063683107
Loss in iteration 175 : 0.37034275729136434
Loss in iteration 176 : 0.36882655924872
Loss in iteration 177 : 0.3687564493519323
Loss in iteration 178 : 0.36802650279339066
Loss in iteration 179 : 0.3676611221537515
Loss in iteration 180 : 0.36738991794096965
Loss in iteration 181 : 0.3700676265413497
Loss in iteration 182 : 0.3725122594724
Loss in iteration 183 : 0.3787767530042484
Loss in iteration 184 : 0.39107534754670886
Loss in iteration 185 : 0.414335686147396
Loss in iteration 186 : 0.4046074992374267
Loss in iteration 187 : 0.42473961698133145
Loss in iteration 188 : 0.3970025953037824
Loss in iteration 189 : 0.398627900739976
Loss in iteration 190 : 0.38517472974426276
Loss in iteration 191 : 0.3824679120479219
Loss in iteration 192 : 0.37528895836699416
Loss in iteration 193 : 0.37018655535424805
Loss in iteration 194 : 0.3687418461729338
Loss in iteration 195 : 0.36815949195368175
Loss in iteration 196 : 0.3681120971812289
Loss in iteration 197 : 0.3679196931272156
Loss in iteration 198 : 0.3676203706359481
Loss in iteration 199 : 0.36797219204212656
Loss in iteration 200 : 0.3695941067811525
Testing accuracy  of updater 8 on alg 1 with rate 0.0392 = 0.7725, training accuracy 0.8368404014244092, time elapsed: 2265 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.5851474390345432
Loss in iteration 3 : 1.689447101125188
Loss in iteration 4 : 1.5595016730829097
Loss in iteration 5 : 1.2218370516038946
Loss in iteration 6 : 0.7209726952982981
Loss in iteration 7 : 0.43927829021243475
Loss in iteration 8 : 0.5298578077244825
Loss in iteration 9 : 0.6121849542038574
Loss in iteration 10 : 0.5928472211307704
Loss in iteration 11 : 0.5470047414642852
Loss in iteration 12 : 0.5189699880301601
Loss in iteration 13 : 0.514529006511097
Loss in iteration 14 : 0.5247394592892103
Loss in iteration 15 : 0.5338821785963987
Loss in iteration 16 : 0.5374128088853797
Loss in iteration 17 : 0.5358754734150025
Loss in iteration 18 : 0.5317893235911334
Loss in iteration 19 : 0.527549409264734
Loss in iteration 20 : 0.5244794263612893
Loss in iteration 21 : 0.5218119193105716
Loss in iteration 22 : 0.517920572916568
Loss in iteration 23 : 0.5125236721367635
Loss in iteration 24 : 0.5064557854423721
Loss in iteration 25 : 0.5001527256238767
Loss in iteration 26 : 0.49352255935750755
Loss in iteration 27 : 0.48645164027708426
Loss in iteration 28 : 0.47896757724118616
Loss in iteration 29 : 0.47122970468425324
Loss in iteration 30 : 0.46330529825100325
Loss in iteration 31 : 0.4552023247983534
Loss in iteration 32 : 0.4469417377824484
Loss in iteration 33 : 0.4386400765263134
Loss in iteration 34 : 0.4303843409249486
Loss in iteration 35 : 0.4222554681174321
Loss in iteration 36 : 0.4142780363305337
Loss in iteration 37 : 0.4065487383391352
Loss in iteration 38 : 0.39923095596784025
Loss in iteration 39 : 0.3924573943936034
Loss in iteration 40 : 0.3861460847119593
Loss in iteration 41 : 0.3803224796853246
Loss in iteration 42 : 0.37523144536181613
Loss in iteration 43 : 0.37111999393034784
Loss in iteration 44 : 0.3680244458593867
Loss in iteration 45 : 0.36603674168955874
Loss in iteration 46 : 0.36508029747677834
Loss in iteration 47 : 0.36524838072042143
Loss in iteration 48 : 0.3661799776794761
Loss in iteration 49 : 0.3673333604200814
Loss in iteration 50 : 0.3683806579686142
Loss in iteration 51 : 0.36912486733051725
Loss in iteration 52 : 0.36943984582229383
Loss in iteration 53 : 0.36932399483552686
Loss in iteration 54 : 0.3686842221602854
Loss in iteration 55 : 0.36778726509091897
Loss in iteration 56 : 0.3669050634895493
Loss in iteration 57 : 0.3660449687707831
Loss in iteration 58 : 0.3653739653270178
Loss in iteration 59 : 0.36487625944238367
Loss in iteration 60 : 0.36446129044453435
Loss in iteration 61 : 0.36415332307169523
Loss in iteration 62 : 0.36394360347895943
Loss in iteration 63 : 0.36379961495312824
Loss in iteration 64 : 0.36370251362607764
Loss in iteration 65 : 0.36364199907714473
Loss in iteration 66 : 0.363599627019701
Loss in iteration 67 : 0.36358257336748134
Loss in iteration 68 : 0.3635710044506544
Loss in iteration 69 : 0.3635551371083929
Loss in iteration 70 : 0.3635267119887411
Loss in iteration 71 : 0.3634887025227926
Loss in iteration 72 : 0.3634358206149011
Loss in iteration 73 : 0.3633737231147834
Loss in iteration 74 : 0.36330800476470954
Loss in iteration 75 : 0.3632436409048391
Loss in iteration 76 : 0.36318703244348444
Loss in iteration 77 : 0.36313597440892276
Loss in iteration 78 : 0.3630869609966574
Loss in iteration 79 : 0.36304250857901016
Loss in iteration 80 : 0.3629989039366355
Loss in iteration 81 : 0.36296011566508685
Loss in iteration 82 : 0.36292365500398965
Loss in iteration 83 : 0.3628946044349552
Loss in iteration 84 : 0.3628812087467703
Loss in iteration 85 : 0.3628730054349831
Loss in iteration 86 : 0.3628664596267057
Loss in iteration 87 : 0.36286004844062675
Loss in iteration 88 : 0.3628546566221604
Loss in iteration 89 : 0.36284921599748715
Loss in iteration 90 : 0.3628439208530632
Loss in iteration 91 : 0.3628375931281592
Loss in iteration 92 : 0.3628310902435374
Loss in iteration 93 : 0.36281946214031585
Loss in iteration 94 : 0.3628098650694382
Loss in iteration 95 : 0.3627969602758775
Loss in iteration 96 : 0.3627927531294604
Loss in iteration 97 : 0.3628027346585046
Loss in iteration 98 : 0.3628413001826836
Loss in iteration 99 : 0.3627682680751026
Loss in iteration 100 : 0.3627593045109889
Loss in iteration 101 : 0.36275477660882793
Loss in iteration 102 : 0.36275076343549273
Loss in iteration 103 : 0.36275131861904314
Loss in iteration 104 : 0.3627760161709445
Loss in iteration 105 : 0.36274682658410395
Loss in iteration 106 : 0.3627480741176688
Loss in iteration 107 : 0.3627580451336196
Loss in iteration 108 : 0.3627826342822208
Loss in iteration 109 : 0.36273835418028005
Loss in iteration 110 : 0.3627378421788735
Loss in iteration 111 : 0.36274702352607174
Loss in iteration 112 : 0.36275787810967086
Loss in iteration 113 : 0.3627402757744356
Loss in iteration 114 : 0.36272953824881793
Loss in iteration 115 : 0.36273646083944394
Loss in iteration 116 : 0.3627327392667859
Loss in iteration 117 : 0.36275771318653866
Loss in iteration 118 : 0.3627670492556856
Loss in iteration 119 : 0.3627279424759343
Loss in iteration 120 : 0.3627239269636742
Loss in iteration 121 : 0.36272306389495784
Loss in iteration 122 : 0.3627220435720718
Loss in iteration 123 : 0.3627214578249094
Testing accuracy  of updater 8 on alg 1 with rate 0.0224 = 0.79025, training accuracy 0.842667529944966, time elapsed: 1355 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6704590563664917
Loss in iteration 3 : 0.6911882774306528
Loss in iteration 4 : 0.6503165555880582
Loss in iteration 5 : 0.5578895655999819
Loss in iteration 6 : 0.4541708411031974
Loss in iteration 7 : 0.4092279347893006
Loss in iteration 8 : 0.41727916310920893
Loss in iteration 9 : 0.4106890785655724
Loss in iteration 10 : 0.3931662138830549
Loss in iteration 11 : 0.38049221343593126
Loss in iteration 12 : 0.3756751189001272
Loss in iteration 13 : 0.3748251065013844
Loss in iteration 14 : 0.3751266148394696
Loss in iteration 15 : 0.3753845160735755
Loss in iteration 16 : 0.3754978750783724
Loss in iteration 17 : 0.3755502903805442
Loss in iteration 18 : 0.3757206124191603
Loss in iteration 19 : 0.37610523279216207
Loss in iteration 20 : 0.37663363258661575
Loss in iteration 21 : 0.37712508738926076
Loss in iteration 22 : 0.37754359098616
Loss in iteration 23 : 0.3778521638585468
Loss in iteration 24 : 0.37803677505833183
Loss in iteration 25 : 0.3781314637460909
Loss in iteration 26 : 0.37814927401199955
Loss in iteration 27 : 0.3781044819663001
Loss in iteration 28 : 0.3780117908599626
Loss in iteration 29 : 0.37788384105119005
Loss in iteration 30 : 0.3776976628449571
Loss in iteration 31 : 0.37745660604660536
Loss in iteration 32 : 0.377173591565211
Loss in iteration 33 : 0.3768479811444364
Loss in iteration 34 : 0.37649695764475016
Loss in iteration 35 : 0.3761224431061931
Loss in iteration 36 : 0.37572737621115604
Loss in iteration 37 : 0.3753190999321313
Loss in iteration 38 : 0.37489716897394926
Loss in iteration 39 : 0.3744699594011119
Loss in iteration 40 : 0.37403554976650594
Loss in iteration 41 : 0.37360389476184536
Loss in iteration 42 : 0.3731908113387825
Loss in iteration 43 : 0.372779346241045
Loss in iteration 44 : 0.3723849673255176
Loss in iteration 45 : 0.37200208955695724
Loss in iteration 46 : 0.3716298639235193
Loss in iteration 47 : 0.371286566777539
Loss in iteration 48 : 0.3709888461826017
Loss in iteration 49 : 0.37071191706692325
Loss in iteration 50 : 0.37045458163667166
Loss in iteration 51 : 0.3702119738360714
Loss in iteration 52 : 0.36998539056592034
Loss in iteration 53 : 0.36978475199154603
Loss in iteration 54 : 0.36959410762216477
Loss in iteration 55 : 0.36940923817582455
Loss in iteration 56 : 0.36924105358920484
Loss in iteration 57 : 0.36910076974492423
Loss in iteration 58 : 0.3689746500745387
Loss in iteration 59 : 0.3688552253076073
Loss in iteration 60 : 0.3687449341215165
Loss in iteration 61 : 0.36864595047208787
Loss in iteration 62 : 0.3685542351428934
Loss in iteration 63 : 0.36846340759228496
Loss in iteration 64 : 0.3683822697287328
Loss in iteration 65 : 0.36831013619677594
Loss in iteration 66 : 0.3682435925657145
Loss in iteration 67 : 0.3681747239062465
Loss in iteration 68 : 0.36810749254753933
Loss in iteration 69 : 0.368040074218659
Loss in iteration 70 : 0.36797468552152923
Loss in iteration 71 : 0.36791167373074174
Loss in iteration 72 : 0.3678474719642289
Loss in iteration 73 : 0.36778271580928307
Loss in iteration 74 : 0.3677185048598265
Loss in iteration 75 : 0.36765433994790414
Loss in iteration 76 : 0.36758993423090597
Loss in iteration 77 : 0.3675264150540548
Loss in iteration 78 : 0.36746266497176666
Loss in iteration 79 : 0.36739900919271307
Loss in iteration 80 : 0.36733685290661
Loss in iteration 81 : 0.36727579036971425
Loss in iteration 82 : 0.36721530982499306
Loss in iteration 83 : 0.3671555458875246
Loss in iteration 84 : 0.3670961133862128
Loss in iteration 85 : 0.3670370362925662
Loss in iteration 86 : 0.3669785986420097
Loss in iteration 87 : 0.3669213273799875
Loss in iteration 88 : 0.36686431992175134
Loss in iteration 89 : 0.3668088852154011
Loss in iteration 90 : 0.3667564859828531
Loss in iteration 91 : 0.36670533793951027
Loss in iteration 92 : 0.3666551312344417
Loss in iteration 93 : 0.36660489347578246
Loss in iteration 94 : 0.366557243168018
Loss in iteration 95 : 0.36650808185809636
Loss in iteration 96 : 0.36645953599529535
Loss in iteration 97 : 0.36641233561901626
Loss in iteration 98 : 0.36636531428206887
Loss in iteration 99 : 0.3663193842592581
Loss in iteration 100 : 0.36627366315080734
Loss in iteration 101 : 0.36622838575721606
Loss in iteration 102 : 0.3661836389354894
Loss in iteration 103 : 0.3661392694259952
Loss in iteration 104 : 0.3660956344112283
Loss in iteration 105 : 0.3660521490700502
Loss in iteration 106 : 0.36600960842700536
Loss in iteration 107 : 0.36596760173370485
Loss in iteration 108 : 0.36592563634227365
Loss in iteration 109 : 0.36588370575027834
Loss in iteration 110 : 0.36584186394618357
Loss in iteration 111 : 0.36580061617026244
Loss in iteration 112 : 0.36576006656753957
Loss in iteration 113 : 0.36572017963177406
Loss in iteration 114 : 0.3656801400741865
Loss in iteration 115 : 0.36564048149994693
Loss in iteration 116 : 0.3656006343926743
Loss in iteration 117 : 0.36556200327422794
Loss in iteration 118 : 0.36552481263652276
Loss in iteration 119 : 0.36548788934638127
Loss in iteration 120 : 0.36545102568560633
Loss in iteration 121 : 0.36541421358483284
Loss in iteration 122 : 0.3653774457847137
Loss in iteration 123 : 0.36534144251556294
Loss in iteration 124 : 0.3653054744186797
Loss in iteration 125 : 0.36527007192767547
Loss in iteration 126 : 0.36523560016592066
Loss in iteration 127 : 0.365202084984336
Loss in iteration 128 : 0.3651686822332134
Loss in iteration 129 : 0.36513611254413003
Loss in iteration 130 : 0.36510393268919195
Loss in iteration 131 : 0.36507255997192706
Loss in iteration 132 : 0.3650416281218998
Loss in iteration 133 : 0.36501117248960363
Loss in iteration 134 : 0.3649808857069247
Loss in iteration 135 : 0.3649513127997144
Loss in iteration 136 : 0.3649221911751886
Loss in iteration 137 : 0.3648929295539316
Loss in iteration 138 : 0.36486383996371125
Loss in iteration 139 : 0.36483649400713675
Loss in iteration 140 : 0.3648090683781404
Loss in iteration 141 : 0.36478205965027094
Loss in iteration 142 : 0.36475558276629777
Loss in iteration 143 : 0.36472965518641304
Loss in iteration 144 : 0.3647039406017912
Loss in iteration 145 : 0.364678250096205
Loss in iteration 146 : 0.36465374142224216
Loss in iteration 147 : 0.36462927352361124
Loss in iteration 148 : 0.36460542063909906
Loss in iteration 149 : 0.36458192323058985
Loss in iteration 150 : 0.3645587407346954
Loss in iteration 151 : 0.3645356852602304
Loss in iteration 152 : 0.3645128959529776
Loss in iteration 153 : 0.3644902224731485
Loss in iteration 154 : 0.3644675999995786
Loss in iteration 155 : 0.36444516079868144
Loss in iteration 156 : 0.36442386790957354
Loss in iteration 157 : 0.3644016068144108
Loss in iteration 158 : 0.36438031044189917
Loss in iteration 159 : 0.3643586786441433
Loss in iteration 160 : 0.3643382390529412
Loss in iteration 161 : 0.36431845007771746
Loss in iteration 162 : 0.3642989781989581
Loss in iteration 163 : 0.3642799419173274
Loss in iteration 164 : 0.3642612538572325
Loss in iteration 165 : 0.36424297783647674
Loss in iteration 166 : 0.3642248162228442
Loss in iteration 167 : 0.3642069593013813
Loss in iteration 168 : 0.36418896807568274
Loss in iteration 169 : 0.36417124647797533
Loss in iteration 170 : 0.3641537812975121
Loss in iteration 171 : 0.3641364653279443
Loss in iteration 172 : 0.36411921690228155
Loss in iteration 173 : 0.3641020280402002
Loss in iteration 174 : 0.3640848915548175
Loss in iteration 175 : 0.36406787711904653
Loss in iteration 176 : 0.3640511780179195
Loss in iteration 177 : 0.3640347283563581
Loss in iteration 178 : 0.3640186778424732
Loss in iteration 179 : 0.36400250633051967
Loss in iteration 180 : 0.3639863727644662
Loss in iteration 181 : 0.3639707762973664
Loss in iteration 182 : 0.3639552753197046
Loss in iteration 183 : 0.36393953427962605
Loss in iteration 184 : 0.36392428786012143
Loss in iteration 185 : 0.3639092674024347
Loss in iteration 186 : 0.36389471375120636
Loss in iteration 187 : 0.3638805376896697
Loss in iteration 188 : 0.3638663717448482
Loss in iteration 189 : 0.363852586199778
Loss in iteration 190 : 0.363839343255305
Loss in iteration 191 : 0.3638262401418809
Loss in iteration 192 : 0.3638121206398462
Loss in iteration 193 : 0.3637992347774088
Loss in iteration 194 : 0.3637859793032771
Loss in iteration 195 : 0.363773233741893
Loss in iteration 196 : 0.3637623529638067
Loss in iteration 197 : 0.3637499223057509
Loss in iteration 198 : 0.3637379577906186
Loss in iteration 199 : 0.3637268283697362
Loss in iteration 200 : 0.36371535194048243
Testing accuracy  of updater 8 on alg 1 with rate 0.0056 = 0.78825, training accuracy 0.8423438005827129, time elapsed: 2627 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6031366861413295
Loss in iteration 3 : 0.6210322194998246
Loss in iteration 4 : 0.5907009807771919
Loss in iteration 5 : 0.5266170003614215
Loss in iteration 6 : 0.46829996576116134
Loss in iteration 7 : 0.44097967229959684
Loss in iteration 8 : 0.428333746299494
Loss in iteration 9 : 0.4190309700640595
Loss in iteration 10 : 0.4056136115567691
Loss in iteration 11 : 0.3921840112341054
Loss in iteration 12 : 0.3840029732325143
Loss in iteration 13 : 0.3797997148088253
Loss in iteration 14 : 0.37774138712622735
Loss in iteration 15 : 0.37652261721369185
Loss in iteration 16 : 0.37547413457701684
Loss in iteration 17 : 0.37449242528043664
Loss in iteration 18 : 0.37383992245367126
Loss in iteration 19 : 0.3737033429991163
Loss in iteration 20 : 0.3737392923935972
Loss in iteration 21 : 0.3738921436139455
Loss in iteration 22 : 0.3740669334076946
Loss in iteration 23 : 0.3742605950661973
Loss in iteration 24 : 0.3744321793338369
Loss in iteration 25 : 0.3745812555913708
Loss in iteration 26 : 0.3747351017255656
Loss in iteration 27 : 0.37487942510470673
Loss in iteration 28 : 0.37498527484652466
Loss in iteration 29 : 0.37505216734735947
Loss in iteration 30 : 0.37507829720698976
Loss in iteration 31 : 0.3750673953260971
Loss in iteration 32 : 0.37502528664560847
Loss in iteration 33 : 0.37495071885906617
Loss in iteration 34 : 0.3748464064848865
Loss in iteration 35 : 0.37471526925587906
Loss in iteration 36 : 0.3745599375046241
Loss in iteration 37 : 0.3743848367559046
Loss in iteration 38 : 0.37419329546911617
Loss in iteration 39 : 0.3739894897540163
Loss in iteration 40 : 0.37377846602465326
Loss in iteration 41 : 0.3735628572967511
Loss in iteration 42 : 0.37333959004219924
Loss in iteration 43 : 0.37311238696457105
Loss in iteration 44 : 0.3728831349485534
Loss in iteration 45 : 0.3726589283468088
Loss in iteration 46 : 0.37243797894131647
Loss in iteration 47 : 0.37221783664870556
Loss in iteration 48 : 0.37200353627175325
Loss in iteration 49 : 0.37180056053195754
Loss in iteration 50 : 0.37160899250812285
Loss in iteration 51 : 0.3714214529256276
Loss in iteration 52 : 0.37124466124240796
Loss in iteration 53 : 0.37107799345382225
Loss in iteration 54 : 0.37092058803491135
Loss in iteration 55 : 0.37076792571372713
Loss in iteration 56 : 0.3706218736944416
Loss in iteration 57 : 0.3704814993231514
Loss in iteration 58 : 0.3703467753707928
Loss in iteration 59 : 0.37021753495761983
Loss in iteration 60 : 0.37009663048856395
Loss in iteration 61 : 0.3699855894973151
Loss in iteration 62 : 0.3698812939772074
Loss in iteration 63 : 0.36978331841817075
Loss in iteration 64 : 0.36969472997343233
Loss in iteration 65 : 0.36961017978108274
Loss in iteration 66 : 0.3695302578679639
Loss in iteration 67 : 0.36945302051938833
Loss in iteration 68 : 0.36937781160066724
Loss in iteration 69 : 0.369305674795272
Loss in iteration 70 : 0.3692363531572578
Loss in iteration 71 : 0.36916937023358
Loss in iteration 72 : 0.3691036815357947
Loss in iteration 73 : 0.3690409243136572
Loss in iteration 74 : 0.36898195051145
Loss in iteration 75 : 0.36892412399622754
Loss in iteration 76 : 0.3688667346340146
Loss in iteration 77 : 0.3688095573948815
Loss in iteration 78 : 0.368753223461268
Loss in iteration 79 : 0.3686979320810291
Loss in iteration 80 : 0.3686431380400605
Loss in iteration 81 : 0.3685914331022281
Loss in iteration 82 : 0.36853794962225855
Loss in iteration 83 : 0.3684886368234635
Loss in iteration 84 : 0.3684385880933277
Loss in iteration 85 : 0.36838981312974217
Loss in iteration 86 : 0.3683410562799297
Loss in iteration 87 : 0.3682922999373006
Loss in iteration 88 : 0.36824354329249004
Loss in iteration 89 : 0.36819478562932084
Loss in iteration 90 : 0.3681460263143818
Loss in iteration 91 : 0.3680974210483955
Loss in iteration 92 : 0.36804875769241363
Loss in iteration 93 : 0.3680001325346719
Loss in iteration 94 : 0.36795281767419036
Loss in iteration 95 : 0.36790481203513525
Loss in iteration 96 : 0.36785760067848433
Loss in iteration 97 : 0.36781093485455635
Loss in iteration 98 : 0.3677648394426762
Loss in iteration 99 : 0.36772029970119074
Loss in iteration 100 : 0.3676759350342215
Loss in iteration 101 : 0.3676328733300461
Loss in iteration 102 : 0.36759016271411143
Loss in iteration 103 : 0.36754763660146633
Loss in iteration 104 : 0.3675052755298077
Loss in iteration 105 : 0.367463061982891
Loss in iteration 106 : 0.3674210848578444
Loss in iteration 107 : 0.36737983148777026
Loss in iteration 108 : 0.36733909039835805
Loss in iteration 109 : 0.367298663577104
Loss in iteration 110 : 0.3672583707137385
Loss in iteration 111 : 0.3672183506999819
Loss in iteration 112 : 0.3671788237270435
Loss in iteration 113 : 0.36713942647217956
Loss in iteration 114 : 0.36710052022697554
Loss in iteration 115 : 0.36706227512589934
Loss in iteration 116 : 0.36702467066441774
Loss in iteration 117 : 0.36698733442670656
Loss in iteration 118 : 0.36695021542753326
Loss in iteration 119 : 0.3669135171755148
Loss in iteration 120 : 0.3668776542896345
Loss in iteration 121 : 0.36684195014363846
Loss in iteration 122 : 0.3668064409016427
Loss in iteration 123 : 0.36677121830945636
Loss in iteration 124 : 0.36673699017804157
Loss in iteration 125 : 0.36670303595334663
Loss in iteration 126 : 0.3666694698038768
Loss in iteration 127 : 0.3666360085688296
Loss in iteration 128 : 0.36660287271336656
Loss in iteration 129 : 0.3665696530753122
Loss in iteration 130 : 0.36653696633525423
Loss in iteration 131 : 0.36650452626028496
Loss in iteration 132 : 0.3664721670544053
Loss in iteration 133 : 0.3664398794085907
Loss in iteration 134 : 0.3664077746742835
Loss in iteration 135 : 0.3663759366437089
Loss in iteration 136 : 0.3663443669346417
Loss in iteration 137 : 0.36631287399004897
Loss in iteration 138 : 0.3662814511101581
Loss in iteration 139 : 0.3662504972467745
Loss in iteration 140 : 0.36621986320530064
Loss in iteration 141 : 0.3661893105150635
Loss in iteration 142 : 0.3661589605949471
Loss in iteration 143 : 0.36612852650751465
Loss in iteration 144 : 0.36609821407032966
Loss in iteration 145 : 0.3660683651537542
Loss in iteration 146 : 0.3660389540839909
Loss in iteration 147 : 0.3660100553309818
Loss in iteration 148 : 0.36598140118500605
Loss in iteration 149 : 0.36595284059170335
Loss in iteration 150 : 0.36592436304273607
Loss in iteration 151 : 0.3658959590779368
Loss in iteration 152 : 0.36586762018089547
Loss in iteration 153 : 0.36583933868493806
Loss in iteration 154 : 0.3658113312010586
Loss in iteration 155 : 0.365783629013549
Loss in iteration 156 : 0.3657560726160252
Loss in iteration 157 : 0.3657283607631994
Loss in iteration 158 : 0.3657005074839277
Loss in iteration 159 : 0.36567257497110034
Loss in iteration 160 : 0.3656450548937694
Loss in iteration 161 : 0.3656182614091766
Loss in iteration 162 : 0.36559185676289735
Loss in iteration 163 : 0.36556595912773737
Loss in iteration 164 : 0.36554048850861726
Loss in iteration 165 : 0.3655151026530976
Loss in iteration 166 : 0.36548979195226605
Loss in iteration 167 : 0.36546454775588777
Loss in iteration 168 : 0.36543936227687007
Loss in iteration 169 : 0.365414228505248
Loss in iteration 170 : 0.3653891529873705
Loss in iteration 171 : 0.36536502079760413
Loss in iteration 172 : 0.36534088845098217
Loss in iteration 173 : 0.3653166849625741
Loss in iteration 174 : 0.3652924161190707
Loss in iteration 175 : 0.3652684400217261
Loss in iteration 176 : 0.3652452520521814
Loss in iteration 177 : 0.3652221179266101
Loss in iteration 178 : 0.36519914166250106
Loss in iteration 179 : 0.36517630505272164
Loss in iteration 180 : 0.36515352380545407
Loss in iteration 181 : 0.36513095341433854
Loss in iteration 182 : 0.36510840140253437
Loss in iteration 183 : 0.3650865343264456
Loss in iteration 184 : 0.3650651552946554
Loss in iteration 185 : 0.3650441297148803
Loss in iteration 186 : 0.36502331783899283
Loss in iteration 187 : 0.36500257720046014
Loss in iteration 188 : 0.3649818996255649
Loss in iteration 189 : 0.36496127775581994
Loss in iteration 190 : 0.36494070496673414
Loss in iteration 191 : 0.36492109421436675
Loss in iteration 192 : 0.3649011481879489
Loss in iteration 193 : 0.36488102773051123
Loss in iteration 194 : 0.36486091990050973
Loss in iteration 195 : 0.3648419781525094
Loss in iteration 196 : 0.364823016085549
Loss in iteration 197 : 0.36480426946742217
Loss in iteration 198 : 0.3647860429043589
Loss in iteration 199 : 0.3647679126272348
Loss in iteration 200 : 0.3647499256354205
Testing accuracy  of updater 8 on alg 1 with rate 0.00392 = 0.78775, training accuracy 0.8416963418582065, time elapsed: 2641 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.549550702577325
Loss in iteration 3 : 0.5746394092870603
Loss in iteration 4 : 0.5818890507212893
Loss in iteration 5 : 0.563232810735035
Loss in iteration 6 : 0.5286168509230116
Loss in iteration 7 : 0.49608868874159884
Loss in iteration 8 : 0.4762723535826767
Loss in iteration 9 : 0.4646004467507655
Loss in iteration 10 : 0.4540636939857723
Loss in iteration 11 : 0.44231286160193295
Loss in iteration 12 : 0.4301747464535083
Loss in iteration 13 : 0.41847805564508456
Loss in iteration 14 : 0.4078140264485977
Loss in iteration 15 : 0.39905070151321714
Loss in iteration 16 : 0.3927258494551196
Loss in iteration 17 : 0.3880865465152572
Loss in iteration 18 : 0.3847244411836877
Loss in iteration 19 : 0.3820881130397492
Loss in iteration 20 : 0.38001962678674495
Loss in iteration 21 : 0.3784594507671553
Loss in iteration 22 : 0.37720542565628057
Loss in iteration 23 : 0.3762278894463116
Loss in iteration 24 : 0.3754088888665441
Loss in iteration 25 : 0.3747612523814373
Loss in iteration 26 : 0.37423032933285755
Loss in iteration 27 : 0.3738506360326785
Loss in iteration 28 : 0.3736167464235911
Loss in iteration 29 : 0.37345287362979585
Loss in iteration 30 : 0.37333288435625966
Loss in iteration 31 : 0.3732363743480473
Loss in iteration 32 : 0.37316777670478235
Loss in iteration 33 : 0.37311596139156394
Loss in iteration 34 : 0.37308740670630397
Loss in iteration 35 : 0.37306435933633325
Loss in iteration 36 : 0.3730497184199067
Loss in iteration 37 : 0.3730319607318426
Loss in iteration 38 : 0.3730117073490112
Loss in iteration 39 : 0.372991288160105
Loss in iteration 40 : 0.37296947662569363
Loss in iteration 41 : 0.37294252472273914
Loss in iteration 42 : 0.37290987290810895
Loss in iteration 43 : 0.3728713530041688
Loss in iteration 44 : 0.37282815839897654
Loss in iteration 45 : 0.3727811447178379
Loss in iteration 46 : 0.37273253893807223
Loss in iteration 47 : 0.37268264137565876
Loss in iteration 48 : 0.3726301334876352
Loss in iteration 49 : 0.37257519952716234
Loss in iteration 50 : 0.37251838626265427
Loss in iteration 51 : 0.3724599014107554
Loss in iteration 52 : 0.3723990081300071
Loss in iteration 53 : 0.3723380412799743
Loss in iteration 54 : 0.37227592623057737
Loss in iteration 55 : 0.37221342553377956
Loss in iteration 56 : 0.37215024308821887
Loss in iteration 57 : 0.3720861704110705
Loss in iteration 58 : 0.3720215310432691
Loss in iteration 59 : 0.3719567093124051
Loss in iteration 60 : 0.3718914297420733
Loss in iteration 61 : 0.3718262289868183
Loss in iteration 62 : 0.3717622096671242
Loss in iteration 63 : 0.37170048213661755
Loss in iteration 64 : 0.3716391080323426
Loss in iteration 65 : 0.37157894748879894
Loss in iteration 66 : 0.3715188848586468
Loss in iteration 67 : 0.37145985634376777
Loss in iteration 68 : 0.3714024634285306
Loss in iteration 69 : 0.37134620460978957
Loss in iteration 70 : 0.3712901427219928
Loss in iteration 71 : 0.3712348440530596
Loss in iteration 72 : 0.37117985329790154
Loss in iteration 73 : 0.37112546983406897
Loss in iteration 74 : 0.3710715719684257
Loss in iteration 75 : 0.37101765015626537
Loss in iteration 76 : 0.37096408822570653
Loss in iteration 77 : 0.3709108820174394
Loss in iteration 78 : 0.37085970203541674
Loss in iteration 79 : 0.3708110008049313
Loss in iteration 80 : 0.3707627659171138
Loss in iteration 81 : 0.3707148453758491
Loss in iteration 82 : 0.37066750040280216
Loss in iteration 83 : 0.3706216827817481
Loss in iteration 84 : 0.37057687388830485
Loss in iteration 85 : 0.37053261525869063
Loss in iteration 86 : 0.37048958692535305
Loss in iteration 87 : 0.3704474227646829
Loss in iteration 88 : 0.37040628676572157
Loss in iteration 89 : 0.37036563112714727
Loss in iteration 90 : 0.3703252476316482
Loss in iteration 91 : 0.3702855192935309
Loss in iteration 92 : 0.3702463301868539
Loss in iteration 93 : 0.370207630262783
Loss in iteration 94 : 0.3701691455143764
Loss in iteration 95 : 0.37013107992559624
Loss in iteration 96 : 0.3700935239519056
Loss in iteration 97 : 0.37005633908723745
Loss in iteration 98 : 0.3700197050247659
Loss in iteration 99 : 0.3699835575564604
Loss in iteration 100 : 0.3699478173604555
Loss in iteration 101 : 0.36991259210007116
Loss in iteration 102 : 0.3698776147784934
Loss in iteration 103 : 0.36984321789343355
Loss in iteration 104 : 0.36980938275416575
Loss in iteration 105 : 0.36977569547292694
Loss in iteration 106 : 0.3697421409615626
Loss in iteration 107 : 0.36970870563524394
Loss in iteration 108 : 0.369675377262724
Loss in iteration 109 : 0.369642548027633
Loss in iteration 110 : 0.36960980298491514
Loss in iteration 111 : 0.36957740742833606
Loss in iteration 112 : 0.3695450608691815
Loss in iteration 113 : 0.3695126714167209
Loss in iteration 114 : 0.3694803815425389
Loss in iteration 115 : 0.3694482535952952
Loss in iteration 116 : 0.36941661449633667
Loss in iteration 117 : 0.36938508505592593
Loss in iteration 118 : 0.36935358796341483
Loss in iteration 119 : 0.36932290563766973
Loss in iteration 120 : 0.36929187509227596
Loss in iteration 121 : 0.36926093258609516
Loss in iteration 122 : 0.3692300326255373
Loss in iteration 123 : 0.3691992288152738
Loss in iteration 124 : 0.3691684604005093
Loss in iteration 125 : 0.3691377235416626
Loss in iteration 126 : 0.36910762038594086
Loss in iteration 127 : 0.36907761131944755
Loss in iteration 128 : 0.3690474331862943
Loss in iteration 129 : 0.3690176481475421
Loss in iteration 130 : 0.3689882035081336
Loss in iteration 131 : 0.3689589190916054
Loss in iteration 132 : 0.368929940014643
Loss in iteration 133 : 0.3689009108785308
Loss in iteration 134 : 0.3688722530337087
Loss in iteration 135 : 0.3688436365973026
Loss in iteration 136 : 0.3688150578482333
Loss in iteration 137 : 0.3687865126536436
Loss in iteration 138 : 0.3687579972941787
Loss in iteration 139 : 0.3687295084226744
Loss in iteration 140 : 0.36870106561843624
Loss in iteration 141 : 0.36867265555671663
Loss in iteration 142 : 0.3686442933402585
Loss in iteration 143 : 0.36861595731710123
Loss in iteration 144 : 0.3685878985927154
Loss in iteration 145 : 0.3685599479696545
Loss in iteration 146 : 0.3685320324472139
Loss in iteration 147 : 0.36850414812802823
Loss in iteration 148 : 0.36847679216915225
Loss in iteration 149 : 0.3684498782680504
Loss in iteration 150 : 0.3684233502530272
Loss in iteration 151 : 0.36839686893502
Loss in iteration 152 : 0.36837042915451707
Loss in iteration 153 : 0.36834402626967844
Loss in iteration 154 : 0.3683176561044571
Loss in iteration 155 : 0.36829148387899296
Loss in iteration 156 : 0.36826542894004033
Loss in iteration 157 : 0.36823942619847005
Loss in iteration 158 : 0.36821351819174314
Loss in iteration 159 : 0.36818782254113847
Loss in iteration 160 : 0.3681623306017392
Loss in iteration 161 : 0.36813699509812237
Loss in iteration 162 : 0.368111704136641
Loss in iteration 163 : 0.36808645275080953
Loss in iteration 164 : 0.368061416766636
Loss in iteration 165 : 0.36803657386665206
Loss in iteration 166 : 0.36801176527546187
Loss in iteration 167 : 0.36798720656605427
Loss in iteration 168 : 0.3679628608230612
Loss in iteration 169 : 0.36793871150230173
Loss in iteration 170 : 0.36791466617780766
Loss in iteration 171 : 0.36789068731759933
Loss in iteration 172 : 0.36786673702377304
Loss in iteration 173 : 0.36784281184850603
Loss in iteration 174 : 0.3678189086903138
Loss in iteration 175 : 0.3677950247593289
Loss in iteration 176 : 0.36777147539123717
Loss in iteration 177 : 0.3677478015840073
Loss in iteration 178 : 0.36772429756301717
Loss in iteration 179 : 0.36770080917896925
Loss in iteration 180 : 0.3676773342432666
Loss in iteration 181 : 0.3676538707879114
Loss in iteration 182 : 0.3676306452492968
Loss in iteration 183 : 0.36760783325650115
Loss in iteration 184 : 0.3675849358202756
Loss in iteration 185 : 0.3675620727200975
Loss in iteration 186 : 0.36753992827428755
Loss in iteration 187 : 0.3675174347017024
Loss in iteration 188 : 0.36749524357011515
Loss in iteration 189 : 0.36747319534506573
Loss in iteration 190 : 0.36745118695159945
Loss in iteration 191 : 0.3674292138173599
Loss in iteration 192 : 0.3674072718269468
Loss in iteration 193 : 0.3673857409406928
Loss in iteration 194 : 0.36736425829346786
Loss in iteration 195 : 0.3673428078763052
Loss in iteration 196 : 0.36732138588546853
Loss in iteration 197 : 0.3672999888974857
Loss in iteration 198 : 0.3672786138311941
Loss in iteration 199 : 0.3672572579135799
Loss in iteration 200 : 0.3672359734415415
Testing accuracy  of updater 8 on alg 1 with rate 0.0022400000000000002 = 0.7855, training accuracy 0.8397539656846876, time elapsed: 2036 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8171998733972538
Loss in iteration 3 : 0.6234473710565419
Loss in iteration 4 : 0.5540876546753569
Loss in iteration 5 : 0.5568704744432728
Loss in iteration 6 : 0.5729090992123194
Loss in iteration 7 : 0.5852776316324946
Loss in iteration 8 : 0.5907492269178933
Loss in iteration 9 : 0.5891323850524955
Loss in iteration 10 : 0.5813136943870032
Loss in iteration 11 : 0.5686388881430974
Loss in iteration 12 : 0.5525227591273095
Loss in iteration 13 : 0.5355501657754989
Loss in iteration 14 : 0.5197801079215089
Loss in iteration 15 : 0.5069238892726504
Loss in iteration 16 : 0.4981493593628822
Loss in iteration 17 : 0.49291760835180987
Loss in iteration 18 : 0.4895146108360898
Loss in iteration 19 : 0.4871022086452172
Loss in iteration 20 : 0.4848655390631606
Loss in iteration 21 : 0.48219533595674713
Loss in iteration 22 : 0.4788507483198215
Loss in iteration 23 : 0.47484768891218027
Loss in iteration 24 : 0.4703865192321032
Loss in iteration 25 : 0.4657100712385177
Loss in iteration 26 : 0.4610449070226686
Loss in iteration 27 : 0.4565240167629806
Loss in iteration 28 : 0.45221961314074904
Loss in iteration 29 : 0.4481157533352611
Loss in iteration 30 : 0.4442415983767025
Loss in iteration 31 : 0.440597962301518
Loss in iteration 32 : 0.4371818097479153
Loss in iteration 33 : 0.4338878393162293
Loss in iteration 34 : 0.4306583000355625
Loss in iteration 35 : 0.42750737402296124
Loss in iteration 36 : 0.424443933549829
Loss in iteration 37 : 0.42147186906216527
Loss in iteration 38 : 0.4185864746508092
Loss in iteration 39 : 0.4158346094967307
Loss in iteration 40 : 0.41319006785533235
Loss in iteration 41 : 0.41067597326648586
Loss in iteration 42 : 0.40830639948179076
Loss in iteration 43 : 0.40612818302745085
Loss in iteration 44 : 0.4041251750237354
Loss in iteration 45 : 0.40228963603873824
Loss in iteration 46 : 0.4005966339344751
Loss in iteration 47 : 0.39904146320481804
Loss in iteration 48 : 0.39760509511177694
Loss in iteration 49 : 0.3962693009745397
Loss in iteration 50 : 0.39503572950170546
Loss in iteration 51 : 0.3938940746125705
Loss in iteration 52 : 0.392818729724716
Loss in iteration 53 : 0.3917995025945842
Loss in iteration 54 : 0.39084620304415413
Loss in iteration 55 : 0.38997892815914703
Loss in iteration 56 : 0.389170356308655
Loss in iteration 57 : 0.38842331629434507
Loss in iteration 58 : 0.3877494203029366
Testing accuracy  of updater 8 on alg 1 with rate 5.599999999999997E-4 = 0.778, training accuracy 0.8374878601489155, time elapsed: 652 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 8.438553675431711
Loss in iteration 3 : 11.948663974604436
Loss in iteration 4 : 12.155738366282797
Loss in iteration 5 : 9.852630540807768
Loss in iteration 6 : 5.621475302140359
Loss in iteration 7 : 2.2470224531822756
Loss in iteration 8 : 2.7614672618636424
Loss in iteration 9 : 5.600668865781958
Loss in iteration 10 : 6.554627358409297
Loss in iteration 11 : 5.14901655006051
Loss in iteration 12 : 3.497322968576607
Loss in iteration 13 : 2.924750255629303
Loss in iteration 14 : 3.370769242334717
Loss in iteration 15 : 4.089581377977081
Loss in iteration 16 : 4.659501410861346
Loss in iteration 17 : 4.8645327541981995
Loss in iteration 18 : 4.69210758252761
Loss in iteration 19 : 4.268344159485575
Loss in iteration 20 : 3.78717382596317
Loss in iteration 21 : 3.4195124787032345
Loss in iteration 22 : 3.317657038331903
Loss in iteration 23 : 3.5153477625985485
Loss in iteration 24 : 3.8116537178745613
Loss in iteration 25 : 3.9379519676734778
Loss in iteration 26 : 3.775546850355181
Loss in iteration 27 : 3.4458104445738407
Loss in iteration 28 : 3.162585284185492
Loss in iteration 29 : 3.0529403934713306
Loss in iteration 30 : 3.0866394152748935
Loss in iteration 31 : 3.159478298897747
Loss in iteration 32 : 3.1803170096080127
Loss in iteration 33 : 3.1036315373354095
Loss in iteration 34 : 2.9346383601084205
Loss in iteration 35 : 2.7266612134868007
Loss in iteration 36 : 2.5508959714083503
Loss in iteration 37 : 2.4635500257977028
Loss in iteration 38 : 2.4644103171940888
Loss in iteration 39 : 2.4517455469005562
Loss in iteration 40 : 2.3506524366961696
Loss in iteration 41 : 2.1730171114572423
Loss in iteration 42 : 2.003043361147006
Loss in iteration 43 : 1.9157672182201124
Loss in iteration 44 : 1.8786199784075448
Loss in iteration 45 : 1.8233866226292677
Loss in iteration 46 : 1.7102398260862615
Loss in iteration 47 : 1.5547985507561561
Loss in iteration 48 : 1.41247605261555
Loss in iteration 49 : 1.3475250534763572
Loss in iteration 50 : 1.2995371800762736
Loss in iteration 51 : 1.174384081049966
Loss in iteration 52 : 1.0238005645738244
Loss in iteration 53 : 0.9520889653123766
Loss in iteration 54 : 0.9023506269507793
Loss in iteration 55 : 0.7836225680252379
Loss in iteration 56 : 0.6587122603112973
Loss in iteration 57 : 0.6675025591640034
Loss in iteration 58 : 0.529546653658279
Loss in iteration 59 : 0.5861545684716661
Loss in iteration 60 : 0.5202032496430475
Loss in iteration 61 : 0.6886425746333095
Loss in iteration 62 : 1.9367592492178713
Loss in iteration 63 : 1.7147347491318212
Loss in iteration 64 : 0.6128790083983087
Loss in iteration 65 : 1.3461402642699127
Loss in iteration 66 : 1.0187197868414613
Loss in iteration 67 : 1.55148094216869
Loss in iteration 68 : 1.0025937195505612
Loss in iteration 69 : 0.7503817492805686
Loss in iteration 70 : 1.2554974478672383
Loss in iteration 71 : 1.1383003646448662
Loss in iteration 72 : 0.8681672142568383
Loss in iteration 73 : 1.0291080027090918
Loss in iteration 74 : 1.207132066581374
Loss in iteration 75 : 1.1693870400939141
Loss in iteration 76 : 0.9975892836336625
Loss in iteration 77 : 0.9331987794547687
Loss in iteration 78 : 1.0689397123392737
Loss in iteration 79 : 1.0652018372256442
Loss in iteration 80 : 0.8947486207594435
Loss in iteration 81 : 0.869258470880216
Loss in iteration 82 : 0.9309345972175179
Loss in iteration 83 : 0.8918228469358602
Loss in iteration 84 : 0.7575826023719129
Loss in iteration 85 : 0.7196150658209288
Loss in iteration 86 : 0.7685556998888868
Loss in iteration 87 : 0.620782763189241
Loss in iteration 88 : 0.6053131921364195
Loss in iteration 89 : 0.6273570978817519
Loss in iteration 90 : 0.49441253755341275
Loss in iteration 91 : 0.6284869363357641
Loss in iteration 92 : 0.4767692611417843
Loss in iteration 93 : 0.541733606507915
Loss in iteration 94 : 0.564082848108506
Loss in iteration 95 : 0.663383261710781
Loss in iteration 96 : 0.43709234828388027
Loss in iteration 97 : 0.5735098898652987
Loss in iteration 98 : 0.7764267625258995
Loss in iteration 99 : 0.5440088976580747
Loss in iteration 100 : 0.7108306094971154
Loss in iteration 101 : 0.4825206127054258
Loss in iteration 102 : 0.6037131368847897
Loss in iteration 103 : 0.600304755217453
Loss in iteration 104 : 0.5088234231844257
Loss in iteration 105 : 0.6257542604984205
Loss in iteration 106 : 0.5208251121441608
Loss in iteration 107 : 0.5564221963898027
Loss in iteration 108 : 0.5705611778487
Loss in iteration 109 : 0.48461994912247397
Loss in iteration 110 : 0.5554956238342741
Loss in iteration 111 : 0.4545802248571362
Loss in iteration 112 : 0.497069151974923
Loss in iteration 113 : 0.43634059115674595
Loss in iteration 114 : 0.509742288388496
Loss in iteration 115 : 0.44841563836014653
Loss in iteration 116 : 0.42063315927223327
Loss in iteration 117 : 0.6116036894682787
Loss in iteration 118 : 0.9946817627277786
Loss in iteration 119 : 0.7731637691509998
Loss in iteration 120 : 0.8493698607157155
Loss in iteration 121 : 0.4416390306155236
Loss in iteration 122 : 0.635692311185239
Loss in iteration 123 : 0.6277534307708513
Loss in iteration 124 : 0.5208150527182872
Loss in iteration 125 : 0.6689880328879373
Loss in iteration 126 : 0.5929268261330737
Loss in iteration 127 : 0.5688751557966053
Loss in iteration 128 : 0.639382762607801
Loss in iteration 129 : 0.5780069576410393
Loss in iteration 130 : 0.5288339319058863
Loss in iteration 131 : 0.5921048447670859
Loss in iteration 132 : 0.4855140648316097
Loss in iteration 133 : 0.512621604675478
Loss in iteration 134 : 0.4937156153210858
Loss in iteration 135 : 0.4335176808271255
Loss in iteration 136 : 0.45663970540673793
Loss in iteration 137 : 0.4836284151790642
Loss in iteration 138 : 0.3984156669959809
Loss in iteration 139 : 0.7673725794381051
Loss in iteration 140 : 1.5090648183910917
Loss in iteration 141 : 1.8885672010269203
Loss in iteration 142 : 1.0710520651789421
Loss in iteration 143 : 0.7035742639906396
Loss in iteration 144 : 1.358865177951177
Loss in iteration 145 : 0.5938722656028673
Loss in iteration 146 : 0.8452181443082825
Loss in iteration 147 : 1.1099741607607132
Loss in iteration 148 : 0.992234929968464
Loss in iteration 149 : 0.7632429256139294
Loss in iteration 150 : 0.8093111873354143
Loss in iteration 151 : 1.021411389664082
Loss in iteration 152 : 0.9313577888645459
Loss in iteration 153 : 0.7605551650958209
Loss in iteration 154 : 0.8206273465531113
Loss in iteration 155 : 0.9214882730733298
Loss in iteration 156 : 0.8867486407602634
Loss in iteration 157 : 0.7469647602871567
Loss in iteration 158 : 0.6912362794689844
Loss in iteration 159 : 0.7875663933109147
Loss in iteration 160 : 0.7247016820065635
Loss in iteration 161 : 0.5911612230295156
Loss in iteration 162 : 0.6437953842927842
Loss in iteration 163 : 0.6542236732066259
Loss in iteration 164 : 0.5178849329859044
Loss in iteration 165 : 0.5343469922991655
Loss in iteration 166 : 0.5092523828073588
Loss in iteration 167 : 0.4694954647970888
Loss in iteration 168 : 0.5163362440884542
Loss in iteration 169 : 0.4377932997996187
Loss in iteration 170 : 0.41229245546354143
Loss in iteration 171 : 0.5693014453693629
Loss in iteration 172 : 0.4207765811489529
Loss in iteration 173 : 0.3883433793013987
Loss in iteration 174 : 0.40547676262319443
Loss in iteration 175 : 0.41421067280770557
Loss in iteration 176 : 0.39973744402552563
Loss in iteration 177 : 0.3884360626084462
Loss in iteration 178 : 0.40111010353433824
Loss in iteration 179 : 0.3894585594595517
Loss in iteration 180 : 0.3856902179581325
Loss in iteration 181 : 0.3923468932008494
Loss in iteration 182 : 0.38015253129803683
Loss in iteration 183 : 0.3819858373321166
Loss in iteration 184 : 0.3858751974879193
Loss in iteration 185 : 0.3776112394083053
Loss in iteration 186 : 0.3721240706310613
Loss in iteration 187 : 0.3745391791147834
Loss in iteration 188 : 0.38108378274213583
Loss in iteration 189 : 0.3861411583714898
Loss in iteration 190 : 0.4036329750253608
Loss in iteration 191 : 0.3959662109507738
Loss in iteration 192 : 0.4055237364516653
Loss in iteration 193 : 0.3782525480297504
Loss in iteration 194 : 0.36641079934530674
Loss in iteration 195 : 0.38140483436012657
Loss in iteration 196 : 0.3921515752629913
Loss in iteration 197 : 0.3788776428045904
Loss in iteration 198 : 0.36750078767166583
Loss in iteration 199 : 0.3766448588031945
Loss in iteration 200 : 0.38294353078664506
Testing accuracy  of updater 9 on alg 1 with rate 0.056 = 0.79925, training accuracy 0.8413726124959534, time elapsed: 2850 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.2707282033487504
Loss in iteration 3 : 3.2806572882809877
Loss in iteration 4 : 3.4866554836628625
Loss in iteration 5 : 2.981325511744139
Loss in iteration 6 : 1.889418819243453
Loss in iteration 7 : 0.9799483039565694
Loss in iteration 8 : 0.9576151995329162
Loss in iteration 9 : 1.6382484277637965
Loss in iteration 10 : 2.0901079179843727
Loss in iteration 11 : 1.8898529899084524
Loss in iteration 12 : 1.433455321734917
Loss in iteration 13 : 1.1799651195354857
Loss in iteration 14 : 1.2344891184609355
Loss in iteration 15 : 1.427893006477946
Loss in iteration 16 : 1.608142570550292
Loss in iteration 17 : 1.6982963073902426
Loss in iteration 18 : 1.676801579499911
Loss in iteration 19 : 1.568161279289258
Loss in iteration 20 : 1.4283632802977253
Loss in iteration 21 : 1.3102839955284462
Loss in iteration 22 : 1.2680298001897983
Loss in iteration 23 : 1.3167464121756807
Loss in iteration 24 : 1.3990885821999524
Loss in iteration 25 : 1.4325451622977576
Loss in iteration 26 : 1.3776900507154004
Loss in iteration 27 : 1.2729834322538949
Loss in iteration 28 : 1.1837878919863314
Loss in iteration 29 : 1.152307353622514
Loss in iteration 30 : 1.162805451260293
Loss in iteration 31 : 1.1792958497970376
Loss in iteration 32 : 1.1709707350357494
Loss in iteration 33 : 1.127388621488223
Loss in iteration 34 : 1.0575303768051183
Loss in iteration 35 : 0.9864979991842845
Loss in iteration 36 : 0.9420825871154036
Loss in iteration 37 : 0.9360282546027157
Loss in iteration 38 : 0.9322447804309872
Loss in iteration 39 : 0.8971147862092215
Loss in iteration 40 : 0.8308250226804706
Loss in iteration 41 : 0.7693942540469951
Loss in iteration 42 : 0.7418708707267658
Loss in iteration 43 : 0.7301739244893903
Loss in iteration 44 : 0.7012426417403731
Loss in iteration 45 : 0.648090376588534
Loss in iteration 46 : 0.5908940115526669
Loss in iteration 47 : 0.5637660073038087
Loss in iteration 48 : 0.5520860744028133
Loss in iteration 49 : 0.5045451749713675
Loss in iteration 50 : 0.4554439802173575
Loss in iteration 51 : 0.4513104362664615
Loss in iteration 52 : 0.42266195114430194
Loss in iteration 53 : 0.3851032619510676
Loss in iteration 54 : 0.40923257381408873
Loss in iteration 55 : 0.4065149761692392
Loss in iteration 56 : 0.40244932416487184
Loss in iteration 57 : 0.5834713804316172
Loss in iteration 58 : 1.0264580408676969
Loss in iteration 59 : 1.1967946205570243
Loss in iteration 60 : 0.7052412205806387
Loss in iteration 61 : 0.6041490960614418
Loss in iteration 62 : 0.8688735331372855
Loss in iteration 63 : 0.4635857733641106
Loss in iteration 64 : 0.6084157257370381
Loss in iteration 65 : 0.7523473765294079
Loss in iteration 66 : 0.6814988888777246
Loss in iteration 67 : 0.5576176549326471
Loss in iteration 68 : 0.589352581914761
Loss in iteration 69 : 0.7009751882478482
Loss in iteration 70 : 0.6722996943441107
Loss in iteration 71 : 0.5775638819569742
Loss in iteration 72 : 0.5840130797990227
Loss in iteration 73 : 0.6361759139297137
Loss in iteration 74 : 0.6361809549868506
Loss in iteration 75 : 0.5790102307142178
Loss in iteration 76 : 0.5340290555220143
Loss in iteration 77 : 0.5577028916832475
Loss in iteration 78 : 0.5657673237019333
Loss in iteration 79 : 0.5068581134194334
Loss in iteration 80 : 0.4812007412851407
Loss in iteration 81 : 0.5007352964724208
Loss in iteration 82 : 0.4820097906752686
Loss in iteration 83 : 0.4300746653107132
Loss in iteration 84 : 0.4375922327503143
Loss in iteration 85 : 0.43323865537622025
Loss in iteration 86 : 0.390683151730184
Loss in iteration 87 : 0.423813133932924
Loss in iteration 88 : 0.38127392489103845
Loss in iteration 89 : 0.43745386561244
Loss in iteration 90 : 0.3879254243595464
Loss in iteration 91 : 0.41547416217138
Loss in iteration 92 : 0.3981682564107903
Loss in iteration 93 : 0.3790165158950829
Loss in iteration 94 : 0.3967796077886802
Loss in iteration 95 : 0.37297050050374947
Loss in iteration 96 : 0.38757003084625286
Loss in iteration 97 : 0.3779216596086747
Loss in iteration 98 : 0.3799143677157202
Loss in iteration 99 : 0.37887368591104414
Loss in iteration 100 : 0.3795518028444019
Loss in iteration 101 : 0.37497752287523095
Loss in iteration 102 : 0.3769288911241668
Loss in iteration 103 : 0.3731587827802363
Loss in iteration 104 : 0.37625232520830054
Loss in iteration 105 : 0.37006945029482563
Loss in iteration 106 : 0.3750230550400793
Loss in iteration 107 : 0.36771554514721616
Loss in iteration 108 : 0.3727711861896012
Loss in iteration 109 : 0.36847487172923815
Loss in iteration 110 : 0.3686348121509817
Loss in iteration 111 : 0.37108786844733915
Loss in iteration 112 : 0.3660411235785528
Loss in iteration 113 : 0.3713431044138569
Loss in iteration 114 : 0.36596198818782166
Loss in iteration 115 : 0.3682818280551906
Loss in iteration 116 : 0.3673943509702367
Loss in iteration 117 : 0.363862380561616
Loss in iteration 118 : 0.36636926878927406
Loss in iteration 119 : 0.3636943643575934
Loss in iteration 120 : 0.3642176820559691
Loss in iteration 121 : 0.364427342587707
Loss in iteration 122 : 0.36341597054538205
Loss in iteration 123 : 0.3646264415500416
Loss in iteration 124 : 0.3635529045736482
Loss in iteration 125 : 0.3645585040451922
Loss in iteration 126 : 0.3637666025721697
Loss in iteration 127 : 0.36370140480099583
Loss in iteration 128 : 0.3634334317907074
Loss in iteration 129 : 0.3629247282064148
Loss in iteration 130 : 0.36335262622786385
Loss in iteration 131 : 0.36275318697045844
Loss in iteration 132 : 0.36297883314740464
Loss in iteration 133 : 0.3629510066288005
Loss in iteration 134 : 0.36284962163579615
Loss in iteration 135 : 0.3632111152263227
Loss in iteration 136 : 0.36288586908610826
Loss in iteration 137 : 0.3630166100974686
Loss in iteration 138 : 0.36298488658742806
Loss in iteration 139 : 0.36280877332462746
Loss in iteration 140 : 0.3629720312339636
Loss in iteration 141 : 0.36282229611027955
Loss in iteration 142 : 0.3627882767925743
Loss in iteration 143 : 0.3629019756208458
Loss in iteration 144 : 0.36276193037451543
Loss in iteration 145 : 0.36315479093837677
Testing accuracy  of updater 9 on alg 1 with rate 0.0392 = 0.79, training accuracy 0.8423438005827129, time elapsed: 2138 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3471076066180474
Loss in iteration 3 : 1.8882497015360136
Loss in iteration 4 : 2.026305793514252
Loss in iteration 5 : 1.806153638423085
Loss in iteration 6 : 1.270062346819891
Loss in iteration 7 : 0.6719066633730167
Loss in iteration 8 : 0.5745105822535358
Loss in iteration 9 : 0.9358660741287833
Loss in iteration 10 : 1.2297128976451601
Loss in iteration 11 : 1.1489772465506798
Loss in iteration 12 : 0.8834749388460996
Loss in iteration 13 : 0.7173731356216604
Loss in iteration 14 : 0.7286204319992233
Loss in iteration 15 : 0.8351357206154202
Loss in iteration 16 : 0.9421202082646106
Loss in iteration 17 : 0.9995588426081911
Loss in iteration 18 : 0.9904540647550762
Loss in iteration 19 : 0.9306475657928542
Loss in iteration 20 : 0.8518807867458891
Loss in iteration 21 : 0.7875877444290453
Loss in iteration 22 : 0.7660881014981348
Loss in iteration 23 : 0.7939119886466969
Loss in iteration 24 : 0.841608115371964
Loss in iteration 25 : 0.8648082719589741
Loss in iteration 26 : 0.8434268900559769
Loss in iteration 27 : 0.7900254702611895
Loss in iteration 28 : 0.7398983571543706
Loss in iteration 29 : 0.7173012557560159
Loss in iteration 30 : 0.7232019240150686
Loss in iteration 31 : 0.7367830823762606
Loss in iteration 32 : 0.7399218834643536
Loss in iteration 33 : 0.7235393504865459
Loss in iteration 34 : 0.6903114101965857
Loss in iteration 35 : 0.6523278820872607
Loss in iteration 36 : 0.6247241070888407
Loss in iteration 37 : 0.618090871121063
Loss in iteration 38 : 0.6210994811514188
Loss in iteration 39 : 0.6146244622168828
Loss in iteration 40 : 0.5876792046095745
Loss in iteration 41 : 0.5544227391957645
Loss in iteration 42 : 0.5313780116509765
Loss in iteration 43 : 0.5238496876881498
Loss in iteration 44 : 0.5207587242229524
Loss in iteration 45 : 0.5052426332719361
Loss in iteration 46 : 0.4767263914913474
Loss in iteration 47 : 0.45351736523691594
Loss in iteration 48 : 0.44580313258148163
Loss in iteration 49 : 0.4424712124182058
Loss in iteration 50 : 0.423889226050513
Loss in iteration 51 : 0.40022935074849314
Loss in iteration 52 : 0.39430526673223826
Loss in iteration 53 : 0.3930625025115258
Loss in iteration 54 : 0.37607040267626696
Loss in iteration 55 : 0.3709062992369812
Loss in iteration 56 : 0.3782871403867419
Loss in iteration 57 : 0.36698286087095716
Loss in iteration 58 : 0.38264958426410156
Loss in iteration 59 : 0.37613388888576194
Loss in iteration 60 : 0.38773560198908663
Loss in iteration 61 : 0.3804725863405622
Loss in iteration 62 : 0.385559332369818
Loss in iteration 63 : 0.3762398610165466
Loss in iteration 64 : 0.3759510501218155
Loss in iteration 65 : 0.3701154459003153
Loss in iteration 66 : 0.3679541908060292
Loss in iteration 67 : 0.36768338759181274
Loss in iteration 68 : 0.36498642963410505
Loss in iteration 69 : 0.36768420578201116
Loss in iteration 70 : 0.3664668357408361
Loss in iteration 71 : 0.3667587934053268
Loss in iteration 72 : 0.36863462165271965
Loss in iteration 73 : 0.36714956397349063
Loss in iteration 74 : 0.36777568794088883
Loss in iteration 75 : 0.36815747032568785
Loss in iteration 76 : 0.3666256274323213
Loss in iteration 77 : 0.3671882804312438
Loss in iteration 78 : 0.3664710303415824
Loss in iteration 79 : 0.36499232788373764
Loss in iteration 80 : 0.36560453801320897
Loss in iteration 81 : 0.36411515417186885
Loss in iteration 82 : 0.3638950621279917
Loss in iteration 83 : 0.3639480158073262
Loss in iteration 84 : 0.3631624476061082
Testing accuracy  of updater 9 on alg 1 with rate 0.0224 = 0.794, training accuracy 0.8416963418582065, time elapsed: 1102 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5833874672654809
Loss in iteration 3 : 0.7114918132940531
Loss in iteration 4 : 0.748276397306426
Loss in iteration 5 : 0.7004905246648103
Loss in iteration 6 : 0.5796870677523048
Loss in iteration 7 : 0.427209944039127
Loss in iteration 8 : 0.40141213891834526
Loss in iteration 9 : 0.4945858035474872
Loss in iteration 10 : 0.5253408931143684
Loss in iteration 11 : 0.4716803044407042
Loss in iteration 12 : 0.40262761040779543
Loss in iteration 13 : 0.3785697932710636
Loss in iteration 14 : 0.397724807436333
Loss in iteration 15 : 0.4287358819283137
Loss in iteration 16 : 0.44526646650734925
Loss in iteration 17 : 0.43971774889967385
Loss in iteration 18 : 0.4210295912724368
Loss in iteration 19 : 0.4023930326288341
Loss in iteration 20 : 0.3951030517484105
Loss in iteration 21 : 0.40106642242156004
Loss in iteration 22 : 0.41257094581878945
Loss in iteration 23 : 0.4218134045615789
Loss in iteration 24 : 0.4234605111744629
Loss in iteration 25 : 0.4176135955907477
Loss in iteration 26 : 0.40869132710776856
Loss in iteration 27 : 0.4016894392753091
Loss in iteration 28 : 0.39955278047366655
Loss in iteration 29 : 0.4018145588461336
Loss in iteration 30 : 0.4060461913800231
Loss in iteration 31 : 0.4083425220431332
Loss in iteration 32 : 0.4066951078795645
Loss in iteration 33 : 0.4020243013391508
Loss in iteration 34 : 0.39700743717998926
Loss in iteration 35 : 0.3937685819626284
Loss in iteration 36 : 0.39282936008954694
Loss in iteration 37 : 0.3935313651062421
Loss in iteration 38 : 0.3943975428066107
Loss in iteration 39 : 0.39361476812131196
Loss in iteration 40 : 0.39093058490343424
Loss in iteration 41 : 0.38744771752979495
Loss in iteration 42 : 0.3847258510014629
Loss in iteration 43 : 0.38328180561639735
Loss in iteration 44 : 0.38288600662893746
Loss in iteration 45 : 0.38284163746236183
Loss in iteration 46 : 0.38192213932176877
Loss in iteration 47 : 0.37999250896640213
Loss in iteration 48 : 0.37782819866211426
Loss in iteration 49 : 0.3759858869455406
Loss in iteration 50 : 0.37495614527964927
Loss in iteration 51 : 0.3747771063943714
Loss in iteration 52 : 0.37442524718236014
Loss in iteration 53 : 0.3732595068486539
Loss in iteration 54 : 0.37176067320532935
Loss in iteration 55 : 0.3707253056578325
Loss in iteration 56 : 0.37048543977181464
Loss in iteration 57 : 0.37045194810585974
Loss in iteration 58 : 0.3700830853250337
Loss in iteration 59 : 0.36927046109432543
Loss in iteration 60 : 0.3685801309564023
Loss in iteration 61 : 0.3683458338468033
Loss in iteration 62 : 0.3684232486030476
Loss in iteration 63 : 0.3683906622403074
Loss in iteration 64 : 0.3680681799273157
Loss in iteration 65 : 0.36774052073718433
Loss in iteration 66 : 0.36764548330362634
Loss in iteration 67 : 0.3677376708002991
Loss in iteration 68 : 0.36780345767282824
Loss in iteration 69 : 0.367710841401847
Loss in iteration 70 : 0.36752989791386487
Loss in iteration 71 : 0.36740298180431347
Loss in iteration 72 : 0.36739171867645154
Loss in iteration 73 : 0.3674379550046069
Loss in iteration 74 : 0.36739704893906977
Loss in iteration 75 : 0.36725227023385887
Loss in iteration 76 : 0.3671102499787446
Loss in iteration 77 : 0.3670511615490655
Loss in iteration 78 : 0.36702806702417246
Loss in iteration 79 : 0.366952905052254
Loss in iteration 80 : 0.366833499220007
Loss in iteration 81 : 0.36669654222103926
Loss in iteration 82 : 0.3666004010095913
Loss in iteration 83 : 0.3665517338229513
Loss in iteration 84 : 0.3664814636805496
Loss in iteration 85 : 0.3663683259259987
Loss in iteration 86 : 0.3662522699075569
Loss in iteration 87 : 0.36618465799128747
Loss in iteration 88 : 0.36612174343543263
Loss in iteration 89 : 0.3660517798061873
Loss in iteration 90 : 0.3659693993156822
Loss in iteration 91 : 0.3659024618466589
Loss in iteration 92 : 0.36584633418567875
Loss in iteration 93 : 0.3657921781597122
Loss in iteration 94 : 0.3657352231926073
Loss in iteration 95 : 0.36567989974628395
Loss in iteration 96 : 0.3656264139313451
Loss in iteration 97 : 0.3655777757982319
Loss in iteration 98 : 0.36553039641933904
Loss in iteration 99 : 0.3654810237269651
Loss in iteration 100 : 0.3654298541749733
Loss in iteration 101 : 0.36537781703198435
Loss in iteration 102 : 0.36533519353791105
Loss in iteration 103 : 0.3652937510562096
Loss in iteration 104 : 0.36524874579223604
Loss in iteration 105 : 0.3652035368326919
Loss in iteration 106 : 0.3651609722920234
Loss in iteration 107 : 0.36511902940728386
Loss in iteration 108 : 0.3650791567538993
Loss in iteration 109 : 0.3650406966541361
Loss in iteration 110 : 0.3650014996381262
Loss in iteration 111 : 0.3649606037403047
Loss in iteration 112 : 0.3649196484552317
Loss in iteration 113 : 0.36488186383708504
Loss in iteration 114 : 0.36484613578181885
Loss in iteration 115 : 0.36481118119253636
Loss in iteration 116 : 0.3647728819159113
Loss in iteration 117 : 0.36473748743304185
Loss in iteration 118 : 0.3647051710008866
Loss in iteration 119 : 0.36467170716220143
Loss in iteration 120 : 0.3646389241357181
Loss in iteration 121 : 0.36460727636704815
Loss in iteration 122 : 0.364577628126212
Loss in iteration 123 : 0.3645479743196432
Loss in iteration 124 : 0.3645183567978673
Loss in iteration 125 : 0.36448909353987624
Loss in iteration 126 : 0.3644600174378858
Loss in iteration 127 : 0.36443128071666103
Loss in iteration 128 : 0.3644049264621922
Loss in iteration 129 : 0.36437794139537766
Loss in iteration 130 : 0.3643496474180681
Loss in iteration 131 : 0.36432412617680204
Loss in iteration 132 : 0.3643003371377717
Loss in iteration 133 : 0.3642752143686907
Loss in iteration 134 : 0.36425034068089696
Loss in iteration 135 : 0.36422746730298466
Loss in iteration 136 : 0.3642046771775247
Loss in iteration 137 : 0.3641817907233427
Loss in iteration 138 : 0.3641591818253854
Loss in iteration 139 : 0.36413680908410223
Loss in iteration 140 : 0.3641150383598873
Loss in iteration 141 : 0.36409340399854767
Loss in iteration 142 : 0.36407189172007126
Loss in iteration 143 : 0.3640505775774723
Loss in iteration 144 : 0.3640297812518069
Loss in iteration 145 : 0.3640093815690023
Loss in iteration 146 : 0.3639891535625387
Loss in iteration 147 : 0.3639692117934557
Loss in iteration 148 : 0.36394961273379556
Loss in iteration 149 : 0.36393016630896596
Loss in iteration 150 : 0.36391134558577515
Loss in iteration 151 : 0.36389322864945467
Loss in iteration 152 : 0.3638760083054847
Loss in iteration 153 : 0.3638577499591526
Loss in iteration 154 : 0.36384093464461853
Loss in iteration 155 : 0.3638247851514636
Loss in iteration 156 : 0.3638072966774995
Loss in iteration 157 : 0.3637922303613883
Loss in iteration 158 : 0.36377679683124753
Loss in iteration 159 : 0.36376021660984237
Loss in iteration 160 : 0.3637465911172154
Loss in iteration 161 : 0.36373201802191624
Loss in iteration 162 : 0.36371739021531224
Loss in iteration 163 : 0.3637046329827469
Loss in iteration 164 : 0.36369069823601263
Loss in iteration 165 : 0.3636771390849503
Loss in iteration 166 : 0.3636644113876951
Loss in iteration 167 : 0.3636513761705486
Loss in iteration 168 : 0.3636385208794499
Loss in iteration 169 : 0.363626008956751
Loss in iteration 170 : 0.36361356692062907
Loss in iteration 171 : 0.36360118732107755
Loss in iteration 172 : 0.3635888634505281
Loss in iteration 173 : 0.36357658926984965
Loss in iteration 174 : 0.36356435934172543
Loss in iteration 175 : 0.36355223965299194
Loss in iteration 176 : 0.3635407649596699
Loss in iteration 177 : 0.36352977625977895
Loss in iteration 178 : 0.36351902397491437
Loss in iteration 179 : 0.36350834865177317
Loss in iteration 180 : 0.3634977422078338
Loss in iteration 181 : 0.36348749049128215
Loss in iteration 182 : 0.36347698466904155
Loss in iteration 183 : 0.3634665507919553
Loss in iteration 184 : 0.36345632146094164
Loss in iteration 185 : 0.36344611449753744
Loss in iteration 186 : 0.3634364038362156
Loss in iteration 187 : 0.3634261443588508
Loss in iteration 188 : 0.36341628251669117
Loss in iteration 189 : 0.363407096807662
Loss in iteration 190 : 0.36339773420638866
Loss in iteration 191 : 0.3633880543961706
Loss in iteration 192 : 0.36337917730714314
Loss in iteration 193 : 0.3633701970353371
Loss in iteration 194 : 0.36336081339963533
Loss in iteration 195 : 0.36335126206081586
Loss in iteration 196 : 0.36334284428140884
Loss in iteration 197 : 0.3633338804990919
Loss in iteration 198 : 0.36332441164795376
Loss in iteration 199 : 0.3633159881587801
Loss in iteration 200 : 0.3633076532467469
Testing accuracy  of updater 9 on alg 1 with rate 0.0056 = 0.78925, training accuracy 0.8433149886694723, time elapsed: 2476 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5465121351489365
Loss in iteration 3 : 0.6452430722997348
Loss in iteration 4 : 0.7065139094360258
Loss in iteration 5 : 0.7047227079676157
Loss in iteration 6 : 0.6467275347433188
Loss in iteration 7 : 0.5426484443469569
Loss in iteration 8 : 0.4330574607459286
Loss in iteration 9 : 0.40618922085309844
Loss in iteration 10 : 0.470160286674965
Loss in iteration 11 : 0.5049139612144378
Loss in iteration 12 : 0.47484889814981784
Loss in iteration 13 : 0.4158726355195136
Loss in iteration 14 : 0.3782617238133554
Loss in iteration 15 : 0.37884469989120756
Loss in iteration 16 : 0.3995973185972195
Loss in iteration 17 : 0.4187177759456167
Loss in iteration 18 : 0.4242862209527825
Loss in iteration 19 : 0.4157716070561711
Loss in iteration 20 : 0.39990054275216413
Loss in iteration 21 : 0.38651200946262826
Loss in iteration 22 : 0.3816818590865827
Loss in iteration 23 : 0.38543607215401415
Loss in iteration 24 : 0.39394133862203784
Loss in iteration 25 : 0.4008975723348942
Loss in iteration 26 : 0.4030485902495023
Loss in iteration 27 : 0.40022574674420747
Loss in iteration 28 : 0.394323392428022
Loss in iteration 29 : 0.3886170958544596
Loss in iteration 30 : 0.38566365741945546
Loss in iteration 31 : 0.3860203105820359
Loss in iteration 32 : 0.3884948439848268
Loss in iteration 33 : 0.3911457390124265
Loss in iteration 34 : 0.3923407015473945
Loss in iteration 35 : 0.3914295060091802
Loss in iteration 36 : 0.3888915274780975
Loss in iteration 37 : 0.3859023299156664
Loss in iteration 38 : 0.3836505737933346
Loss in iteration 39 : 0.38264628002154677
Loss in iteration 40 : 0.38298797319985933
Loss in iteration 41 : 0.3837859950658235
Loss in iteration 42 : 0.38415635478129456
Loss in iteration 43 : 0.38353327358283507
Loss in iteration 44 : 0.38194400013685353
Loss in iteration 45 : 0.3800244873389788
Loss in iteration 46 : 0.37850237720951263
Loss in iteration 47 : 0.3778692012160747
Loss in iteration 48 : 0.3777860565914079
Loss in iteration 49 : 0.37781219075055356
Loss in iteration 50 : 0.3775902123583906
Loss in iteration 51 : 0.37693061678046635
Loss in iteration 52 : 0.37588675276717765
Loss in iteration 53 : 0.37475409571209606
Loss in iteration 54 : 0.37377698552740996
Loss in iteration 55 : 0.37323297807376676
Loss in iteration 56 : 0.3730982786895236
Loss in iteration 57 : 0.3729930219859089
Loss in iteration 58 : 0.3726517882115932
Loss in iteration 59 : 0.3720005724993906
Loss in iteration 60 : 0.3712496172901288
Loss in iteration 61 : 0.3707209664449327
Loss in iteration 62 : 0.37051584861442516
Loss in iteration 63 : 0.37046015756981593
Loss in iteration 64 : 0.3703208308634085
Loss in iteration 65 : 0.3700124206563151
Loss in iteration 66 : 0.369616278693198
Loss in iteration 67 : 0.3693215099388041
Loss in iteration 68 : 0.3691842085371013
Loss in iteration 69 : 0.3691430361456152
Loss in iteration 70 : 0.36907846817414103
Loss in iteration 71 : 0.36894364084046816
Loss in iteration 72 : 0.36876069306977116
Loss in iteration 73 : 0.3685917023884161
Loss in iteration 74 : 0.36851962716636605
Loss in iteration 75 : 0.3684996880040792
Loss in iteration 76 : 0.3684608995023808
Loss in iteration 77 : 0.36838441687615625
Loss in iteration 78 : 0.3682774622052881
Loss in iteration 79 : 0.36819709530660877
Loss in iteration 80 : 0.3681532999361179
Loss in iteration 81 : 0.36812164409934195
Loss in iteration 82 : 0.368082757019436
Loss in iteration 83 : 0.36802553097384516
Loss in iteration 84 : 0.36795967788415723
Loss in iteration 85 : 0.36789557315322735
Loss in iteration 86 : 0.3678361677590275
Loss in iteration 87 : 0.3677893326830136
Loss in iteration 88 : 0.36774879388472814
Loss in iteration 89 : 0.36769537926347207
Loss in iteration 90 : 0.36762994366227025
Loss in iteration 91 : 0.3675645525158492
Loss in iteration 92 : 0.36750999508012194
Loss in iteration 93 : 0.3674585521682173
Loss in iteration 94 : 0.36740493434866167
Loss in iteration 95 : 0.3673483426049016
Loss in iteration 96 : 0.36728899024136075
Loss in iteration 97 : 0.3672295747819282
Loss in iteration 98 : 0.3671736074226025
Loss in iteration 99 : 0.36712170533053273
Loss in iteration 100 : 0.3670697932469692
Loss in iteration 101 : 0.3670169435122335
Loss in iteration 102 : 0.3669637274890069
Loss in iteration 103 : 0.3669125824782433
Loss in iteration 104 : 0.36686382480197244
Loss in iteration 105 : 0.3668168507159164
Loss in iteration 106 : 0.3667689402141279
Loss in iteration 107 : 0.3667221993238313
Loss in iteration 108 : 0.3666797650054106
Loss in iteration 109 : 0.36663756632876826
Loss in iteration 110 : 0.3665952390251891
Loss in iteration 111 : 0.3665532252625806
Loss in iteration 112 : 0.3665123073094644
Loss in iteration 113 : 0.36647172090193536
Loss in iteration 114 : 0.3664315542896999
Loss in iteration 115 : 0.36639208791194466
Loss in iteration 116 : 0.36635278764800183
Loss in iteration 117 : 0.3663139686361121
Loss in iteration 118 : 0.3662756070902047
Loss in iteration 119 : 0.36623740806027044
Loss in iteration 120 : 0.36619906583024303
Loss in iteration 121 : 0.3661609616365667
Loss in iteration 122 : 0.36612385699582045
Loss in iteration 123 : 0.3660868777220976
Loss in iteration 124 : 0.36605014963157495
Loss in iteration 125 : 0.3660138882257233
Loss in iteration 126 : 0.36597875121104323
Loss in iteration 127 : 0.3659436336331844
Loss in iteration 128 : 0.3659085329195689
Loss in iteration 129 : 0.3658734467611288
Loss in iteration 130 : 0.3658383730853503
Loss in iteration 131 : 0.36580331003207944
Loss in iteration 132 : 0.3657682559318027
Loss in iteration 133 : 0.3657353446115639
Loss in iteration 134 : 0.3657027041892446
Loss in iteration 135 : 0.36567013514023905
Loss in iteration 136 : 0.3656378757020809
Loss in iteration 137 : 0.3656048441429343
Loss in iteration 138 : 0.3655711057456626
Loss in iteration 139 : 0.3655376335071359
Loss in iteration 140 : 0.3655054939637852
Loss in iteration 141 : 0.36547527306334193
Loss in iteration 142 : 0.36544516832477125
Loss in iteration 143 : 0.365415428334149
Loss in iteration 144 : 0.3653850712219071
Loss in iteration 145 : 0.36535448283178235
Loss in iteration 146 : 0.3653244544334183
Loss in iteration 147 : 0.3652948258239339
Loss in iteration 148 : 0.36526585859218286
Loss in iteration 149 : 0.3652376643419949
Loss in iteration 150 : 0.36520976925731624
Loss in iteration 151 : 0.3651824288414547
Loss in iteration 152 : 0.36515504428368917
Loss in iteration 153 : 0.36512804170619867
Loss in iteration 154 : 0.3651004122739477
Loss in iteration 155 : 0.36507359869386463
Loss in iteration 156 : 0.3650483808505087
Loss in iteration 157 : 0.36502307186277455
Loss in iteration 158 : 0.3649976698003328
Loss in iteration 159 : 0.3649724964725137
Loss in iteration 160 : 0.36494751281230703
Loss in iteration 161 : 0.36492308382808625
Loss in iteration 162 : 0.36489945440478144
Loss in iteration 163 : 0.3648756407190882
Loss in iteration 164 : 0.36485177515606726
Loss in iteration 165 : 0.3648282991360248
Loss in iteration 166 : 0.3648065221291538
Loss in iteration 167 : 0.36478485711183944
Loss in iteration 168 : 0.3647629919971226
Loss in iteration 169 : 0.36474087502270613
Loss in iteration 170 : 0.36471910428339405
Loss in iteration 171 : 0.36469855957010383
Loss in iteration 172 : 0.36467794897977396
Loss in iteration 173 : 0.36465716250478325
Loss in iteration 174 : 0.3646366891178507
Loss in iteration 175 : 0.3646164466976436
Loss in iteration 176 : 0.36459690157550495
Loss in iteration 177 : 0.36457783489616086
Loss in iteration 178 : 0.3645588047798397
Loss in iteration 179 : 0.3645399244710701
Loss in iteration 180 : 0.36452120402891824
Loss in iteration 181 : 0.3645025276712367
Loss in iteration 182 : 0.36448389041614737
Loss in iteration 183 : 0.3644652877797077
Loss in iteration 184 : 0.36444671572614634
Loss in iteration 185 : 0.36442944529604004
Loss in iteration 186 : 0.36441232779875526
Loss in iteration 187 : 0.3643945389847227
Loss in iteration 188 : 0.3643761448055028
Loss in iteration 189 : 0.3643582187946655
Loss in iteration 190 : 0.36434160422165124
Loss in iteration 191 : 0.36432484221051475
Loss in iteration 192 : 0.36430862332887937
Loss in iteration 193 : 0.3642930255153957
Loss in iteration 194 : 0.3642770720596505
Loss in iteration 195 : 0.3642621066751896
Loss in iteration 196 : 0.3642471577314155
Loss in iteration 197 : 0.3642318122170067
Loss in iteration 198 : 0.3642171018240054
Loss in iteration 199 : 0.3642024818175067
Loss in iteration 200 : 0.36418791696263925
Testing accuracy  of updater 9 on alg 1 with rate 0.00392 = 0.787, training accuracy 0.8410488831337002, time elapsed: 2310 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6241206560140167
Loss in iteration 3 : 0.5794656928634344
Loss in iteration 4 : 0.6649707892387309
Loss in iteration 5 : 0.7129284408450449
Loss in iteration 6 : 0.7238684200426114
Loss in iteration 7 : 0.7016010663819755
Loss in iteration 8 : 0.6501245517920301
Loss in iteration 9 : 0.5742020801902713
Loss in iteration 10 : 0.4878029620559113
Loss in iteration 11 : 0.42265462846637497
Loss in iteration 12 : 0.4111572721265906
Loss in iteration 13 : 0.45096023894714815
Loss in iteration 14 : 0.4865529191250601
Loss in iteration 15 : 0.48818817808688447
Loss in iteration 16 : 0.4588073982993412
Loss in iteration 17 : 0.4176453498872465
Loss in iteration 18 : 0.3866396160278347
Loss in iteration 19 : 0.37644546039599525
Loss in iteration 20 : 0.3824907277110909
Loss in iteration 21 : 0.3951788886396176
Loss in iteration 22 : 0.4061139054623233
Loss in iteration 23 : 0.41014204478447513
Loss in iteration 24 : 0.4066571706031882
Loss in iteration 25 : 0.3979912506702522
Loss in iteration 26 : 0.3878261143001041
Loss in iteration 27 : 0.3795158703508828
Loss in iteration 28 : 0.3756557099897481
Loss in iteration 29 : 0.3758213638827
Loss in iteration 30 : 0.3793400496339899
Loss in iteration 31 : 0.38368830441993024
Loss in iteration 32 : 0.3867927282892425
Loss in iteration 33 : 0.38773174031281715
Loss in iteration 34 : 0.38647526741041976
Loss in iteration 35 : 0.38366109192358516
Loss in iteration 36 : 0.3803269860069043
Loss in iteration 37 : 0.3775920433478546
Loss in iteration 38 : 0.37640542830537704
Loss in iteration 39 : 0.37658455306754607
Loss in iteration 40 : 0.3775568805797889
Loss in iteration 41 : 0.3788301258370757
Loss in iteration 42 : 0.37976359449701463
Loss in iteration 43 : 0.3800828738646358
Loss in iteration 44 : 0.37975546765537055
Loss in iteration 45 : 0.37888281048912076
Loss in iteration 46 : 0.3777164600096331
Loss in iteration 47 : 0.3765359945895041
Loss in iteration 48 : 0.37571926511505205
Loss in iteration 49 : 0.3753102398048372
Loss in iteration 50 : 0.375295429962445
Loss in iteration 51 : 0.3755997971811383
Loss in iteration 52 : 0.3759467593229809
Loss in iteration 53 : 0.37607306002393026
Loss in iteration 54 : 0.3758632794766464
Loss in iteration 55 : 0.3753727146104787
Loss in iteration 56 : 0.3747403087151429
Loss in iteration 57 : 0.37414833419477916
Loss in iteration 58 : 0.3737521960180187
Loss in iteration 59 : 0.37357391261763273
Loss in iteration 60 : 0.37357585045647307
Loss in iteration 61 : 0.3736252148462772
Loss in iteration 62 : 0.37361359174160075
Loss in iteration 63 : 0.3734824495415319
Loss in iteration 64 : 0.37321504229025654
Loss in iteration 65 : 0.37287716538529814
Loss in iteration 66 : 0.372540020988924
Loss in iteration 67 : 0.37226631060140775
Loss in iteration 68 : 0.37211243650272346
Loss in iteration 69 : 0.3720380259401249
Loss in iteration 70 : 0.37201361441607567
Loss in iteration 71 : 0.3719575217990064
Loss in iteration 72 : 0.37183882825114495
Loss in iteration 73 : 0.3716644513344166
Loss in iteration 74 : 0.37146737369509414
Loss in iteration 75 : 0.3712871374300662
Loss in iteration 76 : 0.37114588075138255
Loss in iteration 77 : 0.3710686418384779
Loss in iteration 78 : 0.3710214903697307
Loss in iteration 79 : 0.37096748308908745
Loss in iteration 80 : 0.37088707602177945
Loss in iteration 81 : 0.37077797967580756
Loss in iteration 82 : 0.3706628192046957
Loss in iteration 83 : 0.37056267928715636
Loss in iteration 84 : 0.3704727034844851
Loss in iteration 85 : 0.370397583487638
Loss in iteration 86 : 0.37034618742565967
Loss in iteration 87 : 0.3702949669806636
Loss in iteration 88 : 0.370236983294561
Loss in iteration 89 : 0.37016845092237616
Loss in iteration 90 : 0.37009922101641546
Loss in iteration 91 : 0.3700321285107352
Loss in iteration 92 : 0.36996975521754444
Loss in iteration 93 : 0.3699184292211738
Loss in iteration 94 : 0.36987231520565
Loss in iteration 95 : 0.36983011234270197
Loss in iteration 96 : 0.3697850706993403
Loss in iteration 97 : 0.3697366842188647
Loss in iteration 98 : 0.36968673222018217
Loss in iteration 99 : 0.3696385044276964
Loss in iteration 100 : 0.369590576344533
Loss in iteration 101 : 0.3695454698612828
Loss in iteration 102 : 0.36950370825090545
Loss in iteration 103 : 0.36946313741376774
Loss in iteration 104 : 0.3694225550905418
Loss in iteration 105 : 0.3693825267434608
Loss in iteration 106 : 0.3693430087465501
Loss in iteration 107 : 0.3693033635745042
Loss in iteration 108 : 0.3692636038753903
Loss in iteration 109 : 0.36922374104262695
Loss in iteration 110 : 0.36918433804001327
Loss in iteration 111 : 0.3691451841358092
Loss in iteration 112 : 0.36910694972439356
Loss in iteration 113 : 0.36906947878344687
Loss in iteration 114 : 0.3690328727842346
Loss in iteration 115 : 0.3689960983105033
Loss in iteration 116 : 0.3689591387621269
Loss in iteration 117 : 0.3689220127093271
Loss in iteration 118 : 0.3688860785351635
Loss in iteration 119 : 0.36885082033467426
Loss in iteration 120 : 0.3688164894648558
Loss in iteration 121 : 0.36878230965453485
Loss in iteration 122 : 0.36874799809758874
Loss in iteration 123 : 0.36871356796251836
Loss in iteration 124 : 0.3686790311111843
Loss in iteration 125 : 0.36864439822842254
Loss in iteration 126 : 0.36861004155634813
Loss in iteration 127 : 0.3685764019897467
Loss in iteration 128 : 0.3685436740975957
Loss in iteration 129 : 0.36851168053672584
Loss in iteration 130 : 0.36847896560225596
Loss in iteration 131 : 0.3684452957502349
Loss in iteration 132 : 0.36841290153595646
Loss in iteration 133 : 0.36838192487752863
Loss in iteration 134 : 0.3683511502515938
Loss in iteration 135 : 0.36831908257733
Loss in iteration 136 : 0.3682872004979035
Loss in iteration 137 : 0.36825611191113056
Loss in iteration 138 : 0.3682258360817363
Loss in iteration 139 : 0.36819557354401283
Loss in iteration 140 : 0.36816491763847525
Loss in iteration 141 : 0.36813390731633133
Loss in iteration 142 : 0.36810350160469874
Loss in iteration 143 : 0.36807329328339766
Loss in iteration 144 : 0.36804316261161846
Loss in iteration 145 : 0.3680134645136541
Loss in iteration 146 : 0.36798447246572996
Loss in iteration 147 : 0.3679555514365547
Loss in iteration 148 : 0.36792648129089134
Loss in iteration 149 : 0.36789727667524097
Loss in iteration 150 : 0.3678682586299948
Loss in iteration 151 : 0.3678394844561312
Loss in iteration 152 : 0.36781076392553774
Loss in iteration 153 : 0.3677822825020002
Loss in iteration 154 : 0.36775406506911823
Loss in iteration 155 : 0.36772587718294164
Loss in iteration 156 : 0.36769844151690817
Loss in iteration 157 : 0.3676709837033302
Loss in iteration 158 : 0.3676437864033607
Loss in iteration 159 : 0.3676165508938276
Loss in iteration 160 : 0.36758928071338887
Loss in iteration 161 : 0.36756197905316934
Loss in iteration 162 : 0.36753488395890793
Loss in iteration 163 : 0.36750857243076207
Loss in iteration 164 : 0.3674827717020291
Loss in iteration 165 : 0.3674570420905984
Loss in iteration 166 : 0.3674309553389106
Loss in iteration 167 : 0.36740453632627823
Loss in iteration 168 : 0.36737823038023326
Loss in iteration 169 : 0.36735227593655667
Loss in iteration 170 : 0.36732671266301875
Loss in iteration 171 : 0.3673013840902702
Loss in iteration 172 : 0.36727610295765695
Loss in iteration 173 : 0.3672508643478073
Loss in iteration 174 : 0.36722566383547317
Loss in iteration 175 : 0.3672004974382946
Loss in iteration 176 : 0.3671753615724859
Loss in iteration 177 : 0.36715055915905564
Loss in iteration 178 : 0.3671261449737931
Loss in iteration 179 : 0.36710188037150415
Loss in iteration 180 : 0.36707773262323645
Loss in iteration 181 : 0.36705366304491044
Loss in iteration 182 : 0.36703003175780174
Loss in iteration 183 : 0.36700648659922164
Loss in iteration 184 : 0.36698301872377226
Loss in iteration 185 : 0.3669598715216742
Loss in iteration 186 : 0.3669370496448585
Loss in iteration 187 : 0.3669142370813234
Loss in iteration 188 : 0.3668916021693298
Loss in iteration 189 : 0.36686911566243363
Loss in iteration 190 : 0.36684670226537136
Loss in iteration 191 : 0.3668246267843861
Loss in iteration 192 : 0.3668023433175621
Loss in iteration 193 : 0.3667802340909258
Loss in iteration 194 : 0.3667582138695848
Loss in iteration 195 : 0.3667369188113283
Loss in iteration 196 : 0.3667155779380177
Loss in iteration 197 : 0.36669450656003477
Loss in iteration 198 : 0.36667360884131556
Loss in iteration 199 : 0.3666527394454963
Loss in iteration 200 : 0.36663189517979217
Testing accuracy  of updater 9 on alg 1 with rate 0.0022400000000000002 = 0.785, training accuracy 0.8400776950469407, time elapsed: 2926 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8974843655401712
Loss in iteration 3 : 0.7209682943338637
Loss in iteration 4 : 0.5799023348619993
Loss in iteration 5 : 0.5501234956030955
Loss in iteration 6 : 0.5753909046301179
Loss in iteration 7 : 0.6069531805978202
Loss in iteration 8 : 0.6309695953889173
Loss in iteration 9 : 0.6454266975050327
Loss in iteration 10 : 0.6510692518163008
Loss in iteration 11 : 0.6488142987795936
Loss in iteration 12 : 0.6395192094075464
Loss in iteration 13 : 0.6240060549795592
Loss in iteration 14 : 0.6030977562557428
Loss in iteration 15 : 0.5779152351878746
Loss in iteration 16 : 0.5503152706613148
Loss in iteration 17 : 0.5231740561194641
Loss in iteration 18 : 0.49966160511539287
Loss in iteration 19 : 0.4814572205141448
Loss in iteration 20 : 0.4708008157829978
Loss in iteration 21 : 0.4665477524115612
Loss in iteration 22 : 0.46625053175954
Loss in iteration 23 : 0.46796124501995806
Loss in iteration 24 : 0.4698177923372074
Loss in iteration 25 : 0.47027601123806984
Loss in iteration 26 : 0.46875288659707953
Loss in iteration 27 : 0.46490056168662525
Loss in iteration 28 : 0.4590597062114298
Loss in iteration 29 : 0.4516601760981759
Loss in iteration 30 : 0.44343266732274333
Loss in iteration 31 : 0.43557083388448015
Loss in iteration 32 : 0.42848359278832543
Loss in iteration 33 : 0.4225325202312664
Loss in iteration 34 : 0.4177150324651398
Loss in iteration 35 : 0.4140593716254628
Loss in iteration 36 : 0.4115637308743121
Loss in iteration 37 : 0.4097235481666341
Loss in iteration 38 : 0.40818823451585945
Loss in iteration 39 : 0.4067614771945476
Loss in iteration 40 : 0.4051906372331128
Loss in iteration 41 : 0.4034120012222369
Loss in iteration 42 : 0.40143618199456393
Loss in iteration 43 : 0.3993061232855204
Loss in iteration 44 : 0.39711742200448186
Loss in iteration 45 : 0.3949779706549975
Loss in iteration 46 : 0.3930518215748954
Loss in iteration 47 : 0.39139173736136584
Loss in iteration 48 : 0.39003176778730997
Loss in iteration 49 : 0.3889917961590996
Loss in iteration 50 : 0.3882280954401407
Loss in iteration 51 : 0.38763637808382156
Loss in iteration 52 : 0.387101774513922
Loss in iteration 53 : 0.3865937091390819
Loss in iteration 54 : 0.3861027200689247
Loss in iteration 55 : 0.38560725015846226
Loss in iteration 56 : 0.38508609040444824
Loss in iteration 57 : 0.3845414581001256
Loss in iteration 58 : 0.3839767917122558
Loss in iteration 59 : 0.3834306998851953
Loss in iteration 60 : 0.38290688135535983
Loss in iteration 61 : 0.38241853075418414
Loss in iteration 62 : 0.3819732747615241
Loss in iteration 63 : 0.38157246429632474
Testing accuracy  of updater 9 on alg 1 with rate 5.599999999999997E-4 = 0.77975, training accuracy 0.8381353188734219, time elapsed: 1043 millisecond.
