objc[2267]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1078714c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x1078f54e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 20:35:11 INFO SparkContext: Running Spark version 2.0.0
18/02/26 20:35:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 20:35:12 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 20:35:12 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 20:35:12 INFO SecurityManager: Changing view acls groups to: 
18/02/26 20:35:12 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 20:35:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 20:35:13 INFO Utils: Successfully started service 'sparkDriver' on port 51128.
18/02/26 20:35:13 INFO SparkEnv: Registering MapOutputTracker
18/02/26 20:35:13 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 20:35:13 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-d98d0013-b454-429a-ae88-8a1152108043
18/02/26 20:35:13 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 20:35:13 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 20:35:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 20:35:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 20:35:13 INFO Executor: Starting executor ID driver on host localhost
18/02/26 20:35:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51129.
18/02/26 20:35:13 INFO NettyBlockTransferService: Server created on 192.168.2.140:51129
18/02/26 20:35:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 51129)
18/02/26 20:35:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:51129 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 51129)
18/02/26 20:35:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 51129)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.061600602025958
Loss in iteration 3 : 2.935446769222878
Loss in iteration 4 : 1.5141555336887036
Loss in iteration 5 : 2.667328781414312
Loss in iteration 6 : 1.5293072060501407
Loss in iteration 7 : 2.981625671978102
Loss in iteration 8 : 1.108984160408452
Loss in iteration 9 : 2.002526445224543
Loss in iteration 10 : 1.733316219461027
Loss in iteration 11 : 3.240038518946684
Loss in iteration 12 : 0.91593821876377
Loss in iteration 13 : 0.9901029907364393
Loss in iteration 14 : 1.142822587217551
Loss in iteration 15 : 2.5341922434635915
Loss in iteration 16 : 1.2585822752476654
Loss in iteration 17 : 2.6016681639339816
Loss in iteration 18 : 1.1173292719092764
Loss in iteration 19 : 1.9714070678060294
Loss in iteration 20 : 1.4473552417310105
Loss in iteration 21 : 2.7607688168959625
Loss in iteration 22 : 0.9831609023447154
Loss in iteration 23 : 1.1978515754698198
Loss in iteration 24 : 1.3574141614950859
Loss in iteration 25 : 2.6731649640203066
Loss in iteration 26 : 1.0002443796823408
Loss in iteration 27 : 1.3746129711770037
Loss in iteration 28 : 1.5753348665702787
Loss in iteration 29 : 2.8743039121878184
Loss in iteration 30 : 0.9121033216318837
Loss in iteration 31 : 1.0059660411472473
Loss in iteration 32 : 1.101742247538469
Loss in iteration 33 : 2.0085616838918434
Loss in iteration 34 : 1.3515703101890144
Loss in iteration 35 : 2.5487118564706095
Loss in iteration 36 : 1.0119806601609427
Loss in iteration 37 : 1.4179608973643085
Loss in iteration 38 : 1.51181621229225
Loss in iteration 39 : 2.664794629759309
Loss in iteration 40 : 0.9526465315969306
Loss in iteration 41 : 1.1224777617130215
Loss in iteration 42 : 1.2107308412667765
Loss in iteration 43 : 2.1303949729548677
Loss in iteration 44 : 1.1995351711299165
Loss in iteration 45 : 2.0026781402242086
Loss in iteration 46 : 1.2525770510990053
Loss in iteration 47 : 2.0389176813623955
Loss in iteration 48 : 1.2050385151736458
Loss in iteration 49 : 1.8598838790907284
Loss in iteration 50 : 1.293271050534564
Loss in iteration 51 : 2.0207429093293636
Loss in iteration 52 : 1.186719660245458
Loss in iteration 53 : 1.7129766762099399
Loss in iteration 54 : 1.3164405893636553
Loss in iteration 55 : 2.003164635765985
Loss in iteration 56 : 1.1798847733007756
Loss in iteration 57 : 1.6435600935562533
Loss in iteration 58 : 1.3005856132545333
Loss in iteration 59 : 1.9581225588443039
Loss in iteration 60 : 1.202332812905601
Loss in iteration 61 : 1.6799292008252371
Loss in iteration 62 : 1.2829699017199043
Loss in iteration 63 : 1.8892024419857625
Loss in iteration 64 : 1.221822035976672
Loss in iteration 65 : 1.7143289896860237
Loss in iteration 66 : 1.2678532903156643
Loss in iteration 67 : 1.8305785784248614
Loss in iteration 68 : 1.2299300874892105
Loss in iteration 69 : 1.707184732476503
Loss in iteration 70 : 1.256421314798157
Loss in iteration 71 : 1.7895326677190941
Loss in iteration 72 : 1.240169924433591
Loss in iteration 73 : 1.7368859663354406
Loss in iteration 74 : 1.245913120513855
Loss in iteration 75 : 1.7546546085397443
Loss in iteration 76 : 1.239688220650289
Loss in iteration 77 : 1.7257589277629217
Loss in iteration 78 : 1.2461365975647283
Loss in iteration 79 : 1.7490647958937233
Loss in iteration 80 : 1.2397508342187404
Loss in iteration 81 : 1.7142324470718207
Loss in iteration 82 : 1.2449652389993315
Loss in iteration 83 : 1.7410677660293772
Loss in iteration 84 : 1.2363656172086779
Loss in iteration 85 : 1.7039802817252114
Loss in iteration 86 : 1.245467864279291
Loss in iteration 87 : 1.733380644087803
Loss in iteration 88 : 1.2386111314435955
Loss in iteration 89 : 1.7053087967630327
Loss in iteration 90 : 1.242935250137339
Loss in iteration 91 : 1.7180115975647303
Loss in iteration 92 : 1.239745127498506
Loss in iteration 93 : 1.707376336787422
Loss in iteration 94 : 1.240910024736034
Loss in iteration 95 : 1.7118742038889443
Loss in iteration 96 : 1.2398989071017645
Loss in iteration 97 : 1.7020526836865881
Loss in iteration 98 : 1.2401909025258213
Loss in iteration 99 : 1.7072077102789633
Loss in iteration 100 : 1.2396379449166608
Loss in iteration 101 : 1.7040884830273675
Loss in iteration 102 : 1.2386420420588127
Loss in iteration 103 : 1.6999700703747063
Loss in iteration 104 : 1.239029533078982
Loss in iteration 105 : 1.7012223040133017
Loss in iteration 106 : 1.2383191265869407
Loss in iteration 107 : 1.696112204872953
Loss in iteration 108 : 1.2394511417968166
Loss in iteration 109 : 1.6992672665545807
Loss in iteration 110 : 1.237474409368605
Loss in iteration 111 : 1.6936902067775839
Loss in iteration 112 : 1.2353714122180013
Loss in iteration 113 : 1.6830578135092864
Loss in iteration 114 : 1.235472520208396
Loss in iteration 115 : 1.6907637817765269
Loss in iteration 116 : 1.235721078530508
Loss in iteration 117 : 1.6898598655892878
Loss in iteration 118 : 1.2342415476398885
Loss in iteration 119 : 1.6834402580909067
Loss in iteration 120 : 1.2338434732476469
Loss in iteration 121 : 1.687331533845661
Loss in iteration 122 : 1.2353179482671228
Loss in iteration 123 : 1.692469534603288
Loss in iteration 124 : 1.2329504611407267
Loss in iteration 125 : 1.677362167142572
Loss in iteration 126 : 1.2346976324940089
Loss in iteration 127 : 1.6828844141075407
Loss in iteration 128 : 1.2338429261571147
Loss in iteration 129 : 1.682773269835615
Loss in iteration 130 : 1.2336837794070608
Loss in iteration 131 : 1.681896104489013
Loss in iteration 132 : 1.2336081016939415
Loss in iteration 133 : 1.6814514047775706
Loss in iteration 134 : 1.2333276989296644
Loss in iteration 135 : 1.6776866484252821
Loss in iteration 136 : 1.2325601309093304
Loss in iteration 137 : 1.6746773675059932
Loss in iteration 138 : 1.2340028935429725
Loss in iteration 139 : 1.6778082817433166
Loss in iteration 140 : 1.2337722948825505
Loss in iteration 141 : 1.6750148374726082
Loss in iteration 142 : 1.233303277940105
Loss in iteration 143 : 1.6715839289702914
Loss in iteration 144 : 1.2347496909881746
Loss in iteration 145 : 1.67632411944835
Loss in iteration 146 : 1.232146398408684
Loss in iteration 147 : 1.6669567126876685
Loss in iteration 148 : 1.2348185677999866
Loss in iteration 149 : 1.6743818159934545
Loss in iteration 150 : 1.2331846347548128
Loss in iteration 151 : 1.6665194175636435
Loss in iteration 152 : 1.2339557777288122
Loss in iteration 153 : 1.6686912726608665
Loss in iteration 154 : 1.2352587870285952
Loss in iteration 155 : 1.673296256316067
Loss in iteration 156 : 1.2338810810056222
Loss in iteration 157 : 1.666242410533718
Loss in iteration 158 : 1.232402182098899
Loss in iteration 159 : 1.6621877999565342
Loss in iteration 160 : 1.234637377074417
Loss in iteration 161 : 1.6754529626650296
Loss in iteration 162 : 1.2357421415160994
Loss in iteration 163 : 1.671167819455597
Loss in iteration 164 : 1.230688939791368
Loss in iteration 165 : 1.651972034995684
Loss in iteration 166 : 1.2343887810219187
Loss in iteration 167 : 1.6700991818544018
Loss in iteration 168 : 1.2304981844140337
Loss in iteration 169 : 1.6541279584392292
Loss in iteration 170 : 1.2345832056788737
Loss in iteration 171 : 1.6760981427596893
Loss in iteration 172 : 1.2351005741054883
Loss in iteration 173 : 1.666674281915382
Loss in iteration 174 : 1.228868552632376
Loss in iteration 175 : 1.645314660064353
Loss in iteration 176 : 1.2370368501319058
Loss in iteration 177 : 1.6787762829839008
Loss in iteration 178 : 1.2311334980138733
Loss in iteration 179 : 1.6523532722050844
Loss in iteration 180 : 1.2347401734993848
Loss in iteration 181 : 1.6660016624006224
Loss in iteration 182 : 1.2297185332389573
Loss in iteration 183 : 1.6504234386413457
Loss in iteration 184 : 1.2356591724067163
Loss in iteration 185 : 1.6666530397104682
Loss in iteration 186 : 1.2324564289552011
Loss in iteration 187 : 1.6553579783155985
Loss in iteration 188 : 1.2339510048355247
Loss in iteration 189 : 1.6625506058744688
Loss in iteration 190 : 1.2298761236107696
Loss in iteration 191 : 1.6524314778537756
Loss in iteration 192 : 1.2352696533785306
Loss in iteration 193 : 1.6663489516990755
Loss in iteration 194 : 1.2351730730339445
Loss in iteration 195 : 1.662448073562168
Loss in iteration 196 : 1.2297829107027496
Loss in iteration 197 : 1.6484526298830644
Loss in iteration 198 : 1.2350164825172512
Loss in iteration 199 : 1.6637711365598369
Loss in iteration 200 : 1.2311034740626303
Testing accuracy  of updater 0 on alg 1 with rate 10.0 = 0.8119280142497389, training accuracy 0.807985257985258, time elapsed: 7234 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9228953181878563
Loss in iteration 3 : 0.5186005339226321
Loss in iteration 4 : 0.5205571937877698
Loss in iteration 5 : 0.533586530291461
Loss in iteration 6 : 0.45756471892374945
Loss in iteration 7 : 0.42655479639931426
Loss in iteration 8 : 0.4217564558569615
Loss in iteration 9 : 0.41063035299790585
Loss in iteration 10 : 0.4161213658096335
Loss in iteration 11 : 0.4212176801550269
Loss in iteration 12 : 0.42899869415148834
Loss in iteration 13 : 0.43661601960319857
Loss in iteration 14 : 0.42269560278812474
Loss in iteration 15 : 0.4325841661057419
Loss in iteration 16 : 0.4126988692959211
Loss in iteration 17 : 0.4247361203131318
Loss in iteration 18 : 0.4054098736258599
Loss in iteration 19 : 0.4163248882048791
Loss in iteration 20 : 0.4000434040878613
Loss in iteration 21 : 0.41039301286153224
Loss in iteration 22 : 0.3951581714196891
Loss in iteration 23 : 0.40435457277134257
Loss in iteration 24 : 0.39058158742280413
Loss in iteration 25 : 0.4001525750608217
Loss in iteration 26 : 0.38681662355341695
Loss in iteration 27 : 0.3962961263349004
Loss in iteration 28 : 0.3839974745168991
Loss in iteration 29 : 0.39257815565593523
Loss in iteration 30 : 0.38156850196047104
Loss in iteration 31 : 0.3897928526583316
Loss in iteration 32 : 0.3792036182304132
Loss in iteration 33 : 0.38711858545780586
Loss in iteration 34 : 0.37716810941358025
Loss in iteration 35 : 0.3839519971445631
Loss in iteration 36 : 0.3760939075243446
Loss in iteration 37 : 0.38193547897210384
Loss in iteration 38 : 0.3750817211770064
Loss in iteration 39 : 0.38055177381843497
Loss in iteration 40 : 0.3743071183270045
Loss in iteration 41 : 0.3789357617613139
Loss in iteration 42 : 0.3732454429244972
Loss in iteration 43 : 0.3773737682916897
Loss in iteration 44 : 0.37201098406872346
Loss in iteration 45 : 0.3756882568714571
Loss in iteration 46 : 0.3711043522372605
Loss in iteration 47 : 0.3745985713768276
Loss in iteration 48 : 0.37064465941539043
Loss in iteration 49 : 0.37400746646523625
Loss in iteration 50 : 0.36988531850478895
Loss in iteration 51 : 0.3730754232216924
Loss in iteration 52 : 0.3696902401689714
Loss in iteration 53 : 0.37273308603583477
Loss in iteration 54 : 0.3692768755395446
Loss in iteration 55 : 0.3722569465405767
Loss in iteration 56 : 0.3687691104383355
Loss in iteration 57 : 0.3711286892767238
Loss in iteration 58 : 0.3680344668923448
Loss in iteration 59 : 0.3695716026432402
Loss in iteration 60 : 0.36711858923084406
Loss in iteration 61 : 0.36860157339464783
Loss in iteration 62 : 0.36680602131615625
Loss in iteration 63 : 0.3682510430564029
Loss in iteration 64 : 0.3666386295344373
Loss in iteration 65 : 0.3678981809805676
Loss in iteration 66 : 0.3664195452432555
Loss in iteration 67 : 0.36742292626276063
Loss in iteration 68 : 0.36592957284680255
Loss in iteration 69 : 0.3670402769938245
Loss in iteration 70 : 0.365845028493984
Loss in iteration 71 : 0.36704812208344106
Loss in iteration 72 : 0.36574329323600013
Loss in iteration 73 : 0.36676065430518767
Loss in iteration 74 : 0.36548219522152253
Loss in iteration 75 : 0.3662473786817295
Loss in iteration 76 : 0.36484813144057443
Loss in iteration 77 : 0.3655370759482393
Loss in iteration 78 : 0.3645034653469084
Loss in iteration 79 : 0.3650633181833274
Loss in iteration 80 : 0.36420013665944195
Loss in iteration 81 : 0.3644443673067756
Loss in iteration 82 : 0.36374283405725255
Loss in iteration 83 : 0.363999608169986
Loss in iteration 84 : 0.3634198808549998
Loss in iteration 85 : 0.36357387174839606
Loss in iteration 86 : 0.3630487591609371
Loss in iteration 87 : 0.3631650838142704
Loss in iteration 88 : 0.36247220308453426
Loss in iteration 89 : 0.3623674088358514
Loss in iteration 90 : 0.3619420714130488
Loss in iteration 91 : 0.36177754336730156
Loss in iteration 92 : 0.361091004171472
Loss in iteration 93 : 0.3611571898638079
Loss in iteration 94 : 0.3607038272567291
Loss in iteration 95 : 0.3607522683505482
Loss in iteration 96 : 0.36010561488448356
Loss in iteration 97 : 0.3605989745259557
Loss in iteration 98 : 0.3599817262328767
Loss in iteration 99 : 0.36053378397243524
Loss in iteration 100 : 0.3599853115624
Loss in iteration 101 : 0.36054332881574835
Loss in iteration 102 : 0.36008472168561223
Loss in iteration 103 : 0.36058745449716045
Loss in iteration 104 : 0.36027039572378894
Loss in iteration 105 : 0.3606242840660067
Loss in iteration 106 : 0.36021091094724295
Loss in iteration 107 : 0.3605187116282022
Loss in iteration 108 : 0.3600411506408128
Loss in iteration 109 : 0.3603445038530267
Loss in iteration 110 : 0.35985454937850564
Loss in iteration 111 : 0.36002530954005063
Loss in iteration 112 : 0.35956123244782673
Loss in iteration 113 : 0.3597899728869472
Loss in iteration 114 : 0.35950799582249227
Loss in iteration 115 : 0.359803902943874
Loss in iteration 116 : 0.35944530679327996
Loss in iteration 117 : 0.35971919635041655
Loss in iteration 118 : 0.3593927455417784
Loss in iteration 119 : 0.35966522586915683
Loss in iteration 120 : 0.3593093104621814
Loss in iteration 121 : 0.3596550198009049
Loss in iteration 122 : 0.3593083096638066
Loss in iteration 123 : 0.3595644734876159
Loss in iteration 124 : 0.35913314730997464
Loss in iteration 125 : 0.35930250201480146
Loss in iteration 126 : 0.358818307083048
Loss in iteration 127 : 0.35888844597613084
Loss in iteration 128 : 0.35827321814499263
Loss in iteration 129 : 0.3580703098117098
Loss in iteration 130 : 0.35757948093559244
Loss in iteration 131 : 0.35752479640686086
Loss in iteration 132 : 0.3568582288453294
Loss in iteration 133 : 0.35682166338915494
Loss in iteration 134 : 0.3560823441056092
Loss in iteration 135 : 0.35618145238576826
Loss in iteration 136 : 0.35560943527428446
Loss in iteration 137 : 0.35589705774710434
Loss in iteration 138 : 0.35549133351997175
Loss in iteration 139 : 0.3558275329159852
Loss in iteration 140 : 0.3553872712406956
Loss in iteration 141 : 0.35575280317870944
Loss in iteration 142 : 0.3553610797529714
Loss in iteration 143 : 0.35570283117465235
Loss in iteration 144 : 0.35528018675784334
Loss in iteration 145 : 0.3555975652961992
Loss in iteration 146 : 0.35516433562080074
Loss in iteration 147 : 0.3555153196668859
Loss in iteration 148 : 0.35509493812972015
Loss in iteration 149 : 0.3554818556480872
Loss in iteration 150 : 0.3551982684395927
Loss in iteration 151 : 0.35558533499296735
Loss in iteration 152 : 0.355258096562611
Loss in iteration 153 : 0.3556399761317609
Loss in iteration 154 : 0.3554278427201492
Loss in iteration 155 : 0.3558702154102951
Loss in iteration 156 : 0.3556570019665069
Loss in iteration 157 : 0.35619470989864127
Loss in iteration 158 : 0.3563137313310075
Loss in iteration 159 : 0.3566953930072036
Loss in iteration 160 : 0.3565787966951207
Loss in iteration 161 : 0.35696963722992575
Loss in iteration 162 : 0.3569244220837428
Loss in iteration 163 : 0.35719958504126226
Loss in iteration 164 : 0.35703260357744465
Loss in iteration 165 : 0.3572112135449066
Loss in iteration 166 : 0.3569804375516899
Loss in iteration 167 : 0.35716190182252877
Loss in iteration 168 : 0.35696590663843375
Loss in iteration 169 : 0.35713003757191397
Loss in iteration 170 : 0.35693386316850656
Loss in iteration 171 : 0.3571097018394317
Loss in iteration 172 : 0.35683231467591103
Loss in iteration 173 : 0.3570047095062449
Loss in iteration 174 : 0.356536309455536
Loss in iteration 175 : 0.35667444226949707
Loss in iteration 176 : 0.3562888613499031
Loss in iteration 177 : 0.3564999826440239
Loss in iteration 178 : 0.3561904133514246
Loss in iteration 179 : 0.35632857351991243
Loss in iteration 180 : 0.3556127904484781
Loss in iteration 181 : 0.35567633501409635
Loss in iteration 182 : 0.35506018373186665
Loss in iteration 183 : 0.35514880768220675
Loss in iteration 184 : 0.35461280987962507
Loss in iteration 185 : 0.3547596027594484
Loss in iteration 186 : 0.354360525682315
Loss in iteration 187 : 0.35454347087063665
Loss in iteration 188 : 0.3542268827837776
Loss in iteration 189 : 0.35443896525937535
Loss in iteration 190 : 0.35420640650562446
Loss in iteration 191 : 0.35439526876105476
Loss in iteration 192 : 0.35417471392824573
Loss in iteration 193 : 0.3543783174438124
Loss in iteration 194 : 0.35413301431037936
Loss in iteration 195 : 0.35435619234797683
Loss in iteration 196 : 0.3541507560791194
Loss in iteration 197 : 0.35433521991530326
Loss in iteration 198 : 0.354095637650998
Loss in iteration 199 : 0.35426913892477524
Loss in iteration 200 : 0.35399883186738235
Testing accuracy  of updater 0 on alg 1 with rate 1.0 = 0.8493335790184878, training accuracy 0.8461302211302212, time elapsed: 4622 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.818419298486557
Loss in iteration 3 : 0.6368385969731185
Loss in iteration 4 : 0.49049345057169136
Loss in iteration 5 : 0.4836623891104091
Loss in iteration 6 : 0.48032932727861266
Loss in iteration 7 : 0.47777336840623763
Loss in iteration 8 : 0.4755645573433889
Loss in iteration 9 : 0.4736408209150074
Loss in iteration 10 : 0.4719577298044359
Loss in iteration 11 : 0.470429599824025
Loss in iteration 12 : 0.4690005338849005
Loss in iteration 13 : 0.467649555875677
Loss in iteration 14 : 0.4663728580462296
Loss in iteration 15 : 0.4651561651066404
Loss in iteration 16 : 0.46399955864886017
Loss in iteration 17 : 0.4629014510727501
Loss in iteration 18 : 0.46184967668835797
Loss in iteration 19 : 0.4608304661701865
Loss in iteration 20 : 0.45984045000648843
Loss in iteration 21 : 0.45888326295359544
Loss in iteration 22 : 0.45794869564183993
Loss in iteration 23 : 0.45703172964747807
Loss in iteration 24 : 0.45613292922836823
Loss in iteration 25 : 0.45525312454723554
Loss in iteration 26 : 0.45438907616783064
Loss in iteration 27 : 0.4535387986231421
Loss in iteration 28 : 0.4527021997567134
Loss in iteration 29 : 0.4518774508712708
Loss in iteration 30 : 0.451063288659304
Loss in iteration 31 : 0.45025646698742616
Loss in iteration 32 : 0.449459473985657
Loss in iteration 33 : 0.448672317388727
Loss in iteration 34 : 0.4478935194862335
Loss in iteration 35 : 0.44712352172364417
Loss in iteration 36 : 0.44636146714966773
Loss in iteration 37 : 0.44560888874970567
Loss in iteration 38 : 0.4448632537775646
Loss in iteration 39 : 0.4441231798863859
Loss in iteration 40 : 0.44338973220484246
Loss in iteration 41 : 0.4426621444288825
Loss in iteration 42 : 0.4419386367220756
Loss in iteration 43 : 0.44121892157891157
Loss in iteration 44 : 0.4405032332107329
Loss in iteration 45 : 0.4397912726985977
Loss in iteration 46 : 0.43908319473706514
Loss in iteration 47 : 0.4383783905464261
Loss in iteration 48 : 0.43767618880886877
Loss in iteration 49 : 0.4369763429563414
Loss in iteration 50 : 0.4362796460814735
Loss in iteration 51 : 0.4355852041628683
Loss in iteration 52 : 0.4348933604526736
Loss in iteration 53 : 0.43420352796047695
Loss in iteration 54 : 0.4335164514840109
Loss in iteration 55 : 0.4328312540748808
Loss in iteration 56 : 0.4321477694364277
Loss in iteration 57 : 0.43146572223119284
Loss in iteration 58 : 0.43078560087896706
Loss in iteration 59 : 0.43010736198980876
Loss in iteration 60 : 0.4294305831720983
Loss in iteration 61 : 0.4287551658552115
Loss in iteration 62 : 0.428081127395125
Loss in iteration 63 : 0.4274084155352587
Loss in iteration 64 : 0.4267369045391159
Loss in iteration 65 : 0.42606711074697007
Loss in iteration 66 : 0.42539952606867554
Loss in iteration 67 : 0.42473318073909255
Loss in iteration 68 : 0.42406794939903153
Loss in iteration 69 : 0.42340395731335356
Loss in iteration 70 : 0.4227408290647706
Loss in iteration 71 : 0.42207840307291433
Loss in iteration 72 : 0.42141646512354525
Loss in iteration 73 : 0.4207554560961741
Loss in iteration 74 : 0.4200952442174413
Loss in iteration 75 : 0.4194362633867381
Loss in iteration 76 : 0.4187779343483809
Loss in iteration 77 : 0.4181203503907361
Loss in iteration 78 : 0.41746364724885276
Loss in iteration 79 : 0.41680792104087566
Loss in iteration 80 : 0.4161530112240333
Loss in iteration 81 : 0.4154986587792557
Loss in iteration 82 : 0.41484516554959505
Loss in iteration 83 : 0.4141924459814125
Loss in iteration 84 : 0.4135406210005792
Loss in iteration 85 : 0.4128893588627167
Loss in iteration 86 : 0.4122384987420683
Loss in iteration 87 : 0.4115883074867645
Loss in iteration 88 : 0.4109389958209827
Loss in iteration 89 : 0.41028999948686595
Loss in iteration 90 : 0.40964154364046845
Loss in iteration 91 : 0.4089933924669031
Loss in iteration 92 : 0.408345483145085
Loss in iteration 93 : 0.4076980177212057
Loss in iteration 94 : 0.4070509291295147
Loss in iteration 95 : 0.40640526136967225
Loss in iteration 96 : 0.40576088059315296
Loss in iteration 97 : 0.4051192476260038
Loss in iteration 98 : 0.4044799340397464
Loss in iteration 99 : 0.40384220371463114
Loss in iteration 100 : 0.4032069770833212
Loss in iteration 101 : 0.40258181852139097
Loss in iteration 102 : 0.40196528531337455
Loss in iteration 103 : 0.401358315153578
Loss in iteration 104 : 0.4007655069982323
Loss in iteration 105 : 0.4001987133751187
Loss in iteration 106 : 0.3996395153494742
Loss in iteration 107 : 0.3990927497675802
Loss in iteration 108 : 0.3985648029681726
Loss in iteration 109 : 0.39805052475415076
Loss in iteration 110 : 0.39754664456169536
Loss in iteration 111 : 0.3970521776278447
Loss in iteration 112 : 0.39656573632349157
Loss in iteration 113 : 0.39608604017380267
Loss in iteration 114 : 0.3956157858634826
Loss in iteration 115 : 0.39515547897964937
Loss in iteration 116 : 0.3947040580723411
Loss in iteration 117 : 0.3942603620833822
Loss in iteration 118 : 0.3938226641875285
Loss in iteration 119 : 0.393394818165972
Loss in iteration 120 : 0.39297345592185917
Loss in iteration 121 : 0.39256080232147483
Loss in iteration 122 : 0.39215239404931485
Loss in iteration 123 : 0.3917495978884587
Loss in iteration 124 : 0.3913581371605016
Loss in iteration 125 : 0.3909763922888163
Loss in iteration 126 : 0.3906018724079226
Loss in iteration 127 : 0.39023160304318033
Loss in iteration 128 : 0.38986459518696126
Loss in iteration 129 : 0.389503394036487
Loss in iteration 130 : 0.38915045027437517
Loss in iteration 131 : 0.38880632334786214
Loss in iteration 132 : 0.3884672790810689
Loss in iteration 133 : 0.3881343519467377
Loss in iteration 134 : 0.38780577154102985
Loss in iteration 135 : 0.38748356662657263
Loss in iteration 136 : 0.3871686007574759
Loss in iteration 137 : 0.38686328632002126
Loss in iteration 138 : 0.38656715319365226
Loss in iteration 139 : 0.38627664868562434
Loss in iteration 140 : 0.3859936081903607
Loss in iteration 141 : 0.3857148903706022
Loss in iteration 142 : 0.38544056106586927
Loss in iteration 143 : 0.38517032220614134
Loss in iteration 144 : 0.384904755499581
Loss in iteration 145 : 0.38464530781199996
Loss in iteration 146 : 0.3843906290560161
Loss in iteration 147 : 0.3841397747458494
Loss in iteration 148 : 0.38389198753840825
Loss in iteration 149 : 0.3836507252345315
Loss in iteration 150 : 0.3834135629025824
Loss in iteration 151 : 0.38317931637453284
Loss in iteration 152 : 0.38294727358375236
Loss in iteration 153 : 0.38271781824822393
Loss in iteration 154 : 0.382491802131314
Loss in iteration 155 : 0.3822679509837059
Loss in iteration 156 : 0.38204500329009017
Loss in iteration 157 : 0.3818242048133407
Loss in iteration 158 : 0.3816068900770609
Loss in iteration 159 : 0.3813914958172082
Loss in iteration 160 : 0.38117848423096334
Loss in iteration 161 : 0.38096856087646125
Loss in iteration 162 : 0.3807617200941448
Loss in iteration 163 : 0.3805567009346558
Loss in iteration 164 : 0.38035533501937874
Loss in iteration 165 : 0.38015656023806405
Loss in iteration 166 : 0.37996031254339024
Loss in iteration 167 : 0.3797672330688692
Loss in iteration 168 : 0.37957772807638385
Loss in iteration 169 : 0.3793920207411445
Loss in iteration 170 : 0.3792082429376264
Loss in iteration 171 : 0.3790266855670729
Loss in iteration 172 : 0.37884691350385435
Loss in iteration 173 : 0.3786697188973366
Loss in iteration 174 : 0.3784952803065515
Loss in iteration 175 : 0.3783219402358296
Loss in iteration 176 : 0.37815012135977294
Loss in iteration 177 : 0.3779802111694003
Loss in iteration 178 : 0.3778107832676324
Loss in iteration 179 : 0.3776424354923663
Loss in iteration 180 : 0.37747548609937326
Loss in iteration 181 : 0.37731000564446454
Loss in iteration 182 : 0.37714657847692995
Loss in iteration 183 : 0.37698535504666425
Loss in iteration 184 : 0.37682558631126684
Loss in iteration 185 : 0.37666724001125795
Loss in iteration 186 : 0.3765099535576731
Loss in iteration 187 : 0.37635463342292974
Loss in iteration 188 : 0.37620157220613976
Loss in iteration 189 : 0.37605029507045085
Loss in iteration 190 : 0.37590262118621404
Loss in iteration 191 : 0.37575600894059025
Loss in iteration 192 : 0.3756114266838315
Loss in iteration 193 : 0.3754687068930381
Loss in iteration 194 : 0.37532721645240863
Loss in iteration 195 : 0.3751870056376734
Loss in iteration 196 : 0.3750483114899581
Loss in iteration 197 : 0.3749104281681455
Loss in iteration 198 : 0.3747738037205169
Loss in iteration 199 : 0.3746388512004306
Loss in iteration 200 : 0.3745053970481263
Testing accuracy  of updater 0 on alg 1 with rate 0.09999999999999998 = 0.8405503347460229, training accuracy 0.837960687960688, time elapsed: 3896 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.061600602025958
Loss in iteration 3 : 9.073808838643776
Loss in iteration 4 : 6.761257187260409
Loss in iteration 5 : 2.39021501628632
Loss in iteration 6 : 5.746630942580386
Loss in iteration 7 : 7.438857960845093
Loss in iteration 8 : 4.697200330623846
Loss in iteration 9 : 3.417084856550327
Loss in iteration 10 : 5.268808580321095
Loss in iteration 11 : 6.178764038333611
Loss in iteration 12 : 5.257663309346931
Loss in iteration 13 : 3.7885067249942637
Loss in iteration 14 : 3.460965027022448
Loss in iteration 15 : 4.705157266727901
Loss in iteration 16 : 4.679049090924297
Loss in iteration 17 : 3.24688541934719
Loss in iteration 18 : 3.005181677924434
Loss in iteration 19 : 3.641116159382471
Loss in iteration 20 : 3.757559292082905
Loss in iteration 21 : 2.9133170033361537
Loss in iteration 22 : 2.2398112541480044
Loss in iteration 23 : 3.0947925672213885
Loss in iteration 24 : 2.9046221122163627
Loss in iteration 25 : 1.9983527124396845
Loss in iteration 26 : 2.680287544707847
Loss in iteration 27 : 2.655569459516343
Loss in iteration 28 : 1.8579425746580112
Loss in iteration 29 : 2.48432018881918
Loss in iteration 30 : 2.064857046815124
Loss in iteration 31 : 1.809945831912766
Loss in iteration 32 : 2.069316774432727
Loss in iteration 33 : 1.4529991791044385
Loss in iteration 34 : 1.7989924040926455
Loss in iteration 35 : 1.5515608684549265
Loss in iteration 36 : 1.4631483842188169
Loss in iteration 37 : 1.6693647506284466
Loss in iteration 38 : 1.2640195182334781
Loss in iteration 39 : 1.5953814534586588
Loss in iteration 40 : 1.1584751064743959
Loss in iteration 41 : 1.4659987190785664
Loss in iteration 42 : 1.0071121093331694
Loss in iteration 43 : 1.453922971179244
Loss in iteration 44 : 0.9519626086627958
Loss in iteration 45 : 1.0043752422539782
Loss in iteration 46 : 1.5923752627501513
Loss in iteration 47 : 0.8933371977091566
Loss in iteration 48 : 0.7683091920172247
Loss in iteration 49 : 0.9817765701162288
Loss in iteration 50 : 1.0604110531591282
Loss in iteration 51 : 0.7716197630042563
Loss in iteration 52 : 0.7866652925705657
Loss in iteration 53 : 1.038682152403363
Loss in iteration 54 : 0.8182236557577227
Loss in iteration 55 : 0.7445223175958084
Loss in iteration 56 : 0.7277471656876715
Loss in iteration 57 : 1.0560773106579286
Loss in iteration 58 : 1.2148204255503443
Loss in iteration 59 : 2.0388616828006665
Loss in iteration 60 : 0.9638093024955572
Loss in iteration 61 : 3.679114321489365
Loss in iteration 62 : 1.1317752786142232
Loss in iteration 63 : 2.224156496586456
Loss in iteration 64 : 1.2516381254173026
Loss in iteration 65 : 2.149704822304853
Loss in iteration 66 : 1.37828464529918
Loss in iteration 67 : 1.617675098955309
Loss in iteration 68 : 1.6658806938140347
Loss in iteration 69 : 1.1375227895129914
Loss in iteration 70 : 1.662189109299458
Loss in iteration 71 : 0.9785153006563845
Loss in iteration 72 : 1.3892095549717887
Loss in iteration 73 : 0.8970052685469396
Loss in iteration 74 : 1.6492178944796196
Loss in iteration 75 : 1.3119441898272373
Loss in iteration 76 : 1.1580548498319765
Loss in iteration 77 : 1.7393607568831222
Loss in iteration 78 : 0.8061181332861133
Loss in iteration 79 : 1.127143086164786
Loss in iteration 80 : 1.019752076547003
Loss in iteration 81 : 0.8816703872406615
Loss in iteration 82 : 1.0871787849092884
Loss in iteration 83 : 0.7830640905477105
Loss in iteration 84 : 1.1338642692999104
Loss in iteration 85 : 1.1255294784542482
Loss in iteration 86 : 0.6008631275609345
Loss in iteration 87 : 1.380102614657122
Loss in iteration 88 : 2.66614141842958
Loss in iteration 89 : 2.0825151462271694
Loss in iteration 90 : 2.685444306337062
Loss in iteration 91 : 0.7520060883204891
Loss in iteration 92 : 1.75842401608967
Loss in iteration 93 : 1.0154930254019507
Loss in iteration 94 : 1.8079022272940386
Loss in iteration 95 : 1.05367094480979
Loss in iteration 96 : 1.4568631918201633
Loss in iteration 97 : 1.1384291343901365
Loss in iteration 98 : 1.2276807633708453
Loss in iteration 99 : 0.9261247021882771
Loss in iteration 100 : 1.0851239413756768
Loss in iteration 101 : 0.8026093811851865
Loss in iteration 102 : 1.3688376867586367
Loss in iteration 103 : 1.4345489468481363
Loss in iteration 104 : 0.9375318778765841
Loss in iteration 105 : 2.7803987835989172
Loss in iteration 106 : 1.8151444704392816
Loss in iteration 107 : 1.894403113789356
Loss in iteration 108 : 1.4024797590548552
Loss in iteration 109 : 1.6242308132413559
Loss in iteration 110 : 1.4341648412735013
Loss in iteration 111 : 1.8926798647959995
Loss in iteration 112 : 1.2851023249484346
Loss in iteration 113 : 1.6519542226234685
Loss in iteration 114 : 1.2415191876330696
Loss in iteration 115 : 1.2581841033035135
Loss in iteration 116 : 1.254949930325292
Loss in iteration 117 : 1.0098084611426523
Loss in iteration 118 : 0.9062663415704819
Loss in iteration 119 : 1.4471712418170617
Loss in iteration 120 : 0.8251154163821679
Loss in iteration 121 : 2.3036116624779033
Loss in iteration 122 : 2.100398445122116
Loss in iteration 123 : 1.944838866628354
Loss in iteration 124 : 1.4859026304097616
Loss in iteration 125 : 1.3960497259563933
Loss in iteration 126 : 1.5028232431134545
Loss in iteration 127 : 1.8017008144104272
Loss in iteration 128 : 1.1907715844740623
Loss in iteration 129 : 1.8672959303092616
Loss in iteration 130 : 1.108372355560032
Loss in iteration 131 : 1.4463439189639269
Loss in iteration 132 : 1.113640625842121
Loss in iteration 133 : 1.2555668490010912
Loss in iteration 134 : 0.7583320730777576
Loss in iteration 135 : 1.2776556108998376
Loss in iteration 136 : 0.7557393175728954
Loss in iteration 137 : 0.9716472784053242
Loss in iteration 138 : 1.7402810812892158
Loss in iteration 139 : 0.6967662750020657
Loss in iteration 140 : 3.08499697136008
Loss in iteration 141 : 2.144731924207051
Loss in iteration 142 : 2.58929258798809
Loss in iteration 143 : 1.1818555507771302
Loss in iteration 144 : 2.8737522046923405
Loss in iteration 145 : 1.4856856544252761
Loss in iteration 146 : 2.050945410787947
Loss in iteration 147 : 2.2743832475382217
Loss in iteration 148 : 1.4567052673033878
Loss in iteration 149 : 1.9662455078860843
Loss in iteration 150 : 1.3857091923261817
Loss in iteration 151 : 1.4030853575465134
Loss in iteration 152 : 1.5711476539233058
Loss in iteration 153 : 0.9322100806454067
Loss in iteration 154 : 1.6712382594443482
Loss in iteration 155 : 1.1673654902945303
Loss in iteration 156 : 1.2873952312878452
Loss in iteration 157 : 1.3265288191427456
Loss in iteration 158 : 0.8199861934783769
Loss in iteration 159 : 0.8505646081520338
Loss in iteration 160 : 0.9719736606329187
Loss in iteration 161 : 0.7702774275948707
Loss in iteration 162 : 0.7649661154742528
Loss in iteration 163 : 0.8970830477810865
Loss in iteration 164 : 0.8265378276654319
Loss in iteration 165 : 0.6470528295121696
Loss in iteration 166 : 0.5669836842926372
Loss in iteration 167 : 0.5819737151809093
Loss in iteration 168 : 1.6435356693304137
Loss in iteration 169 : 3.7947539035825875
Loss in iteration 170 : 4.042265004615771
Loss in iteration 171 : 0.7018213749414933
Loss in iteration 172 : 5.37527511039299
Loss in iteration 173 : 1.0234930859859723
Loss in iteration 174 : 3.2249257801704796
Loss in iteration 175 : 2.7311829864945123
Loss in iteration 176 : 1.6837670958481812
Loss in iteration 177 : 3.1879879121904113
Loss in iteration 178 : 1.9056683552368268
Loss in iteration 179 : 2.1477289245164215
Loss in iteration 180 : 2.6581510353263447
Loss in iteration 181 : 2.0496850423695188
Loss in iteration 182 : 1.6278209368712002
Loss in iteration 183 : 2.399624190183723
Loss in iteration 184 : 1.4442391586600298
Loss in iteration 185 : 1.7339724299832016
Loss in iteration 186 : 1.8285358520293726
Loss in iteration 187 : 1.1744144976770985
Loss in iteration 188 : 2.0136318337066323
Loss in iteration 189 : 1.0533977021995777
Loss in iteration 190 : 1.6318972361520303
Loss in iteration 191 : 1.0390087274072224
Loss in iteration 192 : 1.8906053577556035
Loss in iteration 193 : 0.8874184965978648
Loss in iteration 194 : 1.4654953389262793
Loss in iteration 195 : 0.8721348869079185
Loss in iteration 196 : 1.1975709888165769
Loss in iteration 197 : 1.0560570232339637
Loss in iteration 198 : 0.9681390487941613
Loss in iteration 199 : 1.174816216750972
Loss in iteration 200 : 0.799189783772166
Testing accuracy  of updater 1 on alg 1 with rate 10.0 = 0.8309686137215159, training accuracy 0.8301597051597052, time elapsed: 3813 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9228953181878563
Loss in iteration 3 : 1.124116251645044
Loss in iteration 4 : 0.8934454726843334
Loss in iteration 5 : 0.4598038313108597
Loss in iteration 6 : 0.7863018230269629
Loss in iteration 7 : 0.8921878329650065
Loss in iteration 8 : 0.5975582052958756
Loss in iteration 9 : 0.568318746472627
Loss in iteration 10 : 0.7502507815894734
Loss in iteration 11 : 0.7670481597507046
Loss in iteration 12 : 0.6302220793300619
Loss in iteration 13 : 0.5328163537995121
Loss in iteration 14 : 0.6134497680303688
Loss in iteration 15 : 0.6875785080117517
Loss in iteration 16 : 0.5649136062292041
Loss in iteration 17 : 0.5027838170832951
Loss in iteration 18 : 0.5512452498467216
Loss in iteration 19 : 0.5819051516750852
Loss in iteration 20 : 0.5186315037331464
Loss in iteration 21 : 0.44851374103053016
Loss in iteration 22 : 0.49618202269182365
Loss in iteration 23 : 0.5176750248118818
Loss in iteration 24 : 0.4410321893717481
Loss in iteration 25 : 0.4564474917598284
Loss in iteration 26 : 0.49741735626891503
Loss in iteration 27 : 0.4447989070341919
Loss in iteration 28 : 0.43565061089727697
Loss in iteration 29 : 0.475016883050182
Loss in iteration 30 : 0.4253561674444015
Loss in iteration 31 : 0.415876611135544
Loss in iteration 32 : 0.43885829018642564
Loss in iteration 33 : 0.39915078862503683
Loss in iteration 34 : 0.39879318117001405
Loss in iteration 35 : 0.415719935689984
Loss in iteration 36 : 0.38415879601422515
Loss in iteration 37 : 0.39767658385246185
Loss in iteration 38 : 0.40157028750651835
Loss in iteration 39 : 0.3793573794478339
Loss in iteration 40 : 0.39539845388702727
Loss in iteration 41 : 0.3856871221554202
Loss in iteration 42 : 0.3758601875824867
Loss in iteration 43 : 0.3873919305182204
Loss in iteration 44 : 0.37033965087709614
Loss in iteration 45 : 0.3782742627852155
Loss in iteration 46 : 0.3735295561526366
Loss in iteration 47 : 0.36784113569969906
Loss in iteration 48 : 0.37400601384752946
Loss in iteration 49 : 0.36209913532133553
Loss in iteration 50 : 0.37012494043145727
Loss in iteration 51 : 0.3611855943230945
Loss in iteration 52 : 0.36595526579836685
Loss in iteration 53 : 0.3623555814336951
Loss in iteration 54 : 0.3616956304089781
Loss in iteration 55 : 0.36255056526544616
Loss in iteration 56 : 0.3579487042861314
Loss in iteration 57 : 0.3622383465153564
Loss in iteration 58 : 0.35626824569480514
Loss in iteration 59 : 0.3624483503848219
Loss in iteration 60 : 0.35610451951079997
Loss in iteration 61 : 0.3611494674393612
Loss in iteration 62 : 0.3560236300376297
Loss in iteration 63 : 0.3591121407739177
Loss in iteration 64 : 0.3557154788611596
Loss in iteration 65 : 0.35732031289071325
Loss in iteration 66 : 0.35542719177119264
Loss in iteration 67 : 0.3560409228381729
Loss in iteration 68 : 0.35530880183360375
Loss in iteration 69 : 0.3553521784155715
Loss in iteration 70 : 0.3547802299843473
Loss in iteration 71 : 0.3545893372372422
Loss in iteration 72 : 0.3543716592485372
Loss in iteration 73 : 0.35421771719777134
Loss in iteration 74 : 0.35390817474144004
Loss in iteration 75 : 0.3538308283103409
Loss in iteration 76 : 0.3535220108850041
Loss in iteration 77 : 0.35357047572693806
Loss in iteration 78 : 0.35322534950670326
Loss in iteration 79 : 0.3532263042658507
Loss in iteration 80 : 0.3527954918380711
Loss in iteration 81 : 0.3530627202036337
Loss in iteration 82 : 0.3526407878352139
Loss in iteration 83 : 0.3528688579962066
Loss in iteration 84 : 0.3523306473718715
Loss in iteration 85 : 0.3529582119208665
Loss in iteration 86 : 0.35219659509070217
Loss in iteration 87 : 0.3529799182889709
Loss in iteration 88 : 0.3521267418691719
Loss in iteration 89 : 0.3528393048332931
Loss in iteration 90 : 0.3520969967995228
Loss in iteration 91 : 0.3525798675772129
Loss in iteration 92 : 0.3521226306607102
Loss in iteration 93 : 0.3524283354667848
Loss in iteration 94 : 0.35210447902265124
Loss in iteration 95 : 0.35221148324950763
Loss in iteration 96 : 0.35213798611222147
Loss in iteration 97 : 0.35211085967715106
Loss in iteration 98 : 0.352077697038593
Loss in iteration 99 : 0.3519415325433094
Loss in iteration 100 : 0.35204170946015545
Loss in iteration 101 : 0.3519033370339026
Loss in iteration 102 : 0.35208519209792194
Loss in iteration 103 : 0.3518538493199126
Loss in iteration 104 : 0.35194189754409483
Loss in iteration 105 : 0.3518389735610392
Loss in iteration 106 : 0.3518301903532427
Loss in iteration 107 : 0.35187904436955014
Loss in iteration 108 : 0.35178930137183345
Loss in iteration 109 : 0.35186780022807584
Loss in iteration 110 : 0.3518168963405014
Loss in iteration 111 : 0.3517938177707204
Loss in iteration 112 : 0.35186154971556266
Loss in iteration 113 : 0.35177360635076704
Loss in iteration 114 : 0.35176184492582135
Loss in iteration 115 : 0.35183890477486524
Loss in iteration 116 : 0.35176674332742974
Loss in iteration 117 : 0.35172227606838935
Loss in iteration 118 : 0.3517576081838775
Loss in iteration 119 : 0.35179216821764375
Loss in iteration 120 : 0.3517171214446219
Loss in iteration 121 : 0.3517040264858223
Loss in iteration 122 : 0.35174808529380586
Loss in iteration 123 : 0.35169595976085616
Loss in iteration 124 : 0.3516762480739403
Loss in iteration 125 : 0.35169515276538976
Loss in iteration 126 : 0.351716872264418
Loss in iteration 127 : 0.3517307466688787
Loss in iteration 128 : 0.35165493049626595
Loss in iteration 129 : 0.3516830766987263
Loss in iteration 130 : 0.3517800948369704
Loss in iteration 131 : 0.35164668754681006
Loss in iteration 132 : 0.35167984033685057
Loss in iteration 133 : 0.35185465458899395
Loss in iteration 134 : 0.35164349621462876
Loss in iteration 135 : 0.3517171906442546
Loss in iteration 136 : 0.3518810669414725
Loss in iteration 137 : 0.35162912498694454
Loss in iteration 138 : 0.35199727377658674
Loss in iteration 139 : 0.3517376764729677
Loss in iteration 140 : 0.3517774523640408
Loss in iteration 141 : 0.35186154640802986
Loss in iteration 142 : 0.3516101468555529
Loss in iteration 143 : 0.3518255662125281
Loss in iteration 144 : 0.35163697448658776
Loss in iteration 145 : 0.3516495904046787
Loss in iteration 146 : 0.35176761642970017
Loss in iteration 147 : 0.35161787025179886
Loss in iteration 148 : 0.3518666279195171
Loss in iteration 149 : 0.3516419060832571
Loss in iteration 150 : 0.3516782521980144
Loss in iteration 151 : 0.351735211758495
Loss in iteration 152 : 0.35156927606116406
Loss in iteration 153 : 0.35170236371866626
Loss in iteration 154 : 0.3516230367659534
Loss in iteration 155 : 0.3515542138464774
Loss in iteration 156 : 0.3515976511888353
Loss in iteration 157 : 0.3515655039522155
Loss in iteration 158 : 0.35154628911304936
Loss in iteration 159 : 0.35156288320871587
Loss in iteration 160 : 0.35157163573082506
Loss in iteration 161 : 0.35157680199752267
Loss in iteration 162 : 0.3515539967526528
Loss in iteration 163 : 0.35152704728632134
Loss in iteration 164 : 0.3515247724873828
Loss in iteration 165 : 0.3515383746388542
Loss in iteration 166 : 0.35153970507938936
Loss in iteration 167 : 0.3515574407630374
Loss in iteration 168 : 0.3515143119569117
Loss in iteration 169 : 0.3515083847193834
Loss in iteration 170 : 0.35151259494830966
Loss in iteration 171 : 0.3515549734313479
Loss in iteration 172 : 0.3515853904740902
Loss in iteration 173 : 0.3515442616247622
Loss in iteration 174 : 0.35152802465720295
Loss in iteration 175 : 0.35149589977227785
Loss in iteration 176 : 0.3514971892332358
Loss in iteration 177 : 0.351535258128225
Loss in iteration 178 : 0.35149322059873506
Loss in iteration 179 : 0.35147507018608604
Loss in iteration 180 : 0.3514813817042708
Loss in iteration 181 : 0.35157025519627455
Loss in iteration 182 : 0.35155393728282
Loss in iteration 183 : 0.3514693692857416
Loss in iteration 184 : 0.35146181492664547
Loss in iteration 185 : 0.35147077658594766
Loss in iteration 186 : 0.3514745878456915
Loss in iteration 187 : 0.35151973016573024
Loss in iteration 188 : 0.3514647865551833
Loss in iteration 189 : 0.35144891575554854
Loss in iteration 190 : 0.3514499641956308
Loss in iteration 191 : 0.3514624854948744
Loss in iteration 192 : 0.35151524539675394
Loss in iteration 193 : 0.3514768048718903
Loss in iteration 194 : 0.3514908499594429
Loss in iteration 195 : 0.3514439658460645
Loss in iteration 196 : 0.35144416315699
Loss in iteration 197 : 0.3514628877032309
Loss in iteration 198 : 0.3514335156734384
Loss in iteration 199 : 0.35142487482101586
Loss in iteration 200 : 0.35142074186539807
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.8493950003071065, training accuracy 0.8495085995085995, time elapsed: 3611 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.818419298486557
Loss in iteration 3 : 0.4962877182702581
Loss in iteration 4 : 0.5809905680837184
Loss in iteration 5 : 0.6676089985707724
Loss in iteration 6 : 0.7046963152472893
Loss in iteration 7 : 0.6969300935392077
Loss in iteration 8 : 0.649214744651422
Loss in iteration 9 : 0.5688885538348197
Loss in iteration 10 : 0.4785866868340889
Loss in iteration 11 : 0.4289081890705451
Loss in iteration 12 : 0.4377458104959909
Loss in iteration 13 : 0.47576345192607444
Loss in iteration 14 : 0.512212924776091
Loss in iteration 15 : 0.5194924132347668
Loss in iteration 16 : 0.49642831516548497
Loss in iteration 17 : 0.45657814532906815
Loss in iteration 18 : 0.4158846709353216
Loss in iteration 19 : 0.39393871480003084
Loss in iteration 20 : 0.4049020381305836
Loss in iteration 21 : 0.42610137302583256
Loss in iteration 22 : 0.4385359851576456
Loss in iteration 23 : 0.43655687532635756
Loss in iteration 24 : 0.4219793817148346
Loss in iteration 25 : 0.4005554486420683
Loss in iteration 26 : 0.38361118598408117
Loss in iteration 27 : 0.3786283915425615
Loss in iteration 28 : 0.38635722693348024
Loss in iteration 29 : 0.39660987484392674
Loss in iteration 30 : 0.39855670509453306
Loss in iteration 31 : 0.39073575059477234
Loss in iteration 32 : 0.37880918048644224
Loss in iteration 33 : 0.37144173394192637
Loss in iteration 34 : 0.37070188138042415
Loss in iteration 35 : 0.3739959357002224
Loss in iteration 36 : 0.37741968744115784
Loss in iteration 37 : 0.3778600266384372
Loss in iteration 38 : 0.374692418916126
Loss in iteration 39 : 0.3695048639985948
Loss in iteration 40 : 0.36495630249814304
Loss in iteration 41 : 0.3631795146876139
Loss in iteration 42 : 0.3643979542301337
Loss in iteration 43 : 0.36676162518768324
Loss in iteration 44 : 0.3671089774195423
Loss in iteration 45 : 0.36494488526556196
Loss in iteration 46 : 0.3618941432617835
Loss in iteration 47 : 0.3601716417223707
Loss in iteration 48 : 0.36042910101468667
Loss in iteration 49 : 0.36156331955425525
Loss in iteration 50 : 0.3623579188148908
Loss in iteration 51 : 0.3620263532661187
Loss in iteration 52 : 0.36079009916248245
Loss in iteration 53 : 0.35954641806532245
Loss in iteration 54 : 0.3590336291682706
Loss in iteration 55 : 0.3593792312844664
Loss in iteration 56 : 0.359965199090668
Loss in iteration 57 : 0.3600783707793862
Loss in iteration 58 : 0.35954119768092696
Loss in iteration 59 : 0.3587092750285861
Loss in iteration 60 : 0.35816379664231435
Loss in iteration 61 : 0.3580299814942353
Loss in iteration 62 : 0.358217525282122
Loss in iteration 63 : 0.35831138681500824
Loss in iteration 64 : 0.3580742067860366
Loss in iteration 65 : 0.3576104683299285
Loss in iteration 66 : 0.3572338682745684
Loss in iteration 67 : 0.35704953906483244
Loss in iteration 68 : 0.3570477750683502
Loss in iteration 69 : 0.35709733958191053
Loss in iteration 70 : 0.3570468763240506
Loss in iteration 71 : 0.35685318427425605
Loss in iteration 72 : 0.3566122575046622
Loss in iteration 73 : 0.3564542386395208
Loss in iteration 74 : 0.35641455963109003
Loss in iteration 75 : 0.3564121269841483
Loss in iteration 76 : 0.356387577945932
Loss in iteration 77 : 0.3563052339858032
Loss in iteration 78 : 0.3561764149194647
Loss in iteration 79 : 0.35604101352005907
Loss in iteration 80 : 0.35593442413807513
Loss in iteration 81 : 0.3558951463912474
Loss in iteration 82 : 0.3558817123039062
Loss in iteration 83 : 0.355826907366054
Loss in iteration 84 : 0.35573220562721275
Loss in iteration 85 : 0.35563540607563326
Loss in iteration 86 : 0.3555710427656902
Loss in iteration 87 : 0.3555389891713563
Loss in iteration 88 : 0.3555092887558077
Loss in iteration 89 : 0.35546262632672604
Loss in iteration 90 : 0.3553962194385856
Loss in iteration 91 : 0.3553309158591593
Loss in iteration 92 : 0.3552810315013821
Loss in iteration 93 : 0.35524394748595384
Loss in iteration 94 : 0.35520770261514334
Loss in iteration 95 : 0.3551638597600288
Loss in iteration 96 : 0.3551105783751728
Loss in iteration 97 : 0.35506107543291043
Loss in iteration 98 : 0.3550190919232744
Loss in iteration 99 : 0.3549827847724288
Loss in iteration 100 : 0.3549487076480332
Loss in iteration 101 : 0.3549104400073733
Loss in iteration 102 : 0.3548687367512455
Loss in iteration 103 : 0.35482839339709293
Loss in iteration 104 : 0.3547933060126772
Loss in iteration 105 : 0.35476207811918753
Loss in iteration 106 : 0.3547309487446091
Loss in iteration 107 : 0.3546970355978205
Loss in iteration 108 : 0.3546625386352207
Loss in iteration 109 : 0.35463000883150253
Loss in iteration 110 : 0.35459905654316365
Loss in iteration 111 : 0.3545699307568156
Loss in iteration 112 : 0.35454023696308595
Loss in iteration 113 : 0.354510340292668
Loss in iteration 114 : 0.3544804877905887
Loss in iteration 115 : 0.3544519330644985
Loss in iteration 116 : 0.35442456308212716
Loss in iteration 117 : 0.3543978843155994
Loss in iteration 118 : 0.35437166911421913
Loss in iteration 119 : 0.35434558422825363
Loss in iteration 120 : 0.354319956020029
Loss in iteration 121 : 0.3542949756740004
Loss in iteration 122 : 0.35427094310868945
Loss in iteration 123 : 0.35424732580062074
Loss in iteration 124 : 0.354223714440716
Loss in iteration 125 : 0.3542006383923689
Loss in iteration 126 : 0.35417833539868265
Loss in iteration 127 : 0.35415630969260287
Loss in iteration 128 : 0.3541347551310368
Loss in iteration 129 : 0.3541135984261189
Loss in iteration 130 : 0.3540930381094586
Loss in iteration 131 : 0.35407271116381345
Loss in iteration 132 : 0.35405281183901277
Loss in iteration 133 : 0.35403352616810463
Loss in iteration 134 : 0.3540145030584706
Loss in iteration 135 : 0.35399566372129987
Loss in iteration 136 : 0.35397714837876965
Loss in iteration 137 : 0.3539591030803211
Loss in iteration 138 : 0.3539412552298985
Loss in iteration 139 : 0.3539237301785553
Loss in iteration 140 : 0.3539065399472046
Loss in iteration 141 : 0.35388965732583516
Loss in iteration 142 : 0.35387310509421877
Loss in iteration 143 : 0.35385681273259906
Loss in iteration 144 : 0.3538410278528149
Loss in iteration 145 : 0.3538253000632681
Loss in iteration 146 : 0.3538095597833229
Loss in iteration 147 : 0.35379431576693227
Loss in iteration 148 : 0.35377919379924455
Loss in iteration 149 : 0.3537643108050668
Loss in iteration 150 : 0.35374986799059377
Loss in iteration 151 : 0.35373536885924084
Loss in iteration 152 : 0.35372135036105457
Loss in iteration 153 : 0.3537072226379534
Loss in iteration 154 : 0.3536933875749465
Loss in iteration 155 : 0.3536800318520268
Loss in iteration 156 : 0.3536666417998567
Loss in iteration 157 : 0.3536537181922507
Loss in iteration 158 : 0.3536406393793775
Loss in iteration 159 : 0.3536284065490841
Loss in iteration 160 : 0.353616176559632
Loss in iteration 161 : 0.353603680073227
Loss in iteration 162 : 0.3535910348126396
Loss in iteration 163 : 0.3535790601722689
Loss in iteration 164 : 0.3535671666491827
Loss in iteration 165 : 0.35355525383003106
Loss in iteration 166 : 0.35354372460743044
Loss in iteration 167 : 0.35353230019171716
Loss in iteration 168 : 0.3535207113237754
Loss in iteration 169 : 0.3535093998285941
Loss in iteration 170 : 0.35349826713162663
Loss in iteration 171 : 0.3534871635806166
Loss in iteration 172 : 0.35347623438081444
Loss in iteration 173 : 0.3534655537434418
Loss in iteration 174 : 0.3534547870888221
Loss in iteration 175 : 0.3534443244910901
Loss in iteration 176 : 0.3534339072829544
Loss in iteration 177 : 0.3534234105576486
Loss in iteration 178 : 0.35341307087256335
Loss in iteration 179 : 0.35340287606222753
Loss in iteration 180 : 0.3533926056748056
Loss in iteration 181 : 0.35338234559059906
Loss in iteration 182 : 0.35337226847255343
Loss in iteration 183 : 0.35336226158134604
Loss in iteration 184 : 0.35335240664841017
Loss in iteration 185 : 0.3533427369731409
Loss in iteration 186 : 0.35333313049272597
Loss in iteration 187 : 0.35332360568766297
Loss in iteration 188 : 0.3533141932784406
Loss in iteration 189 : 0.35330486051996357
Loss in iteration 190 : 0.35329571051826353
Loss in iteration 191 : 0.3532867351900119
Loss in iteration 192 : 0.35327771814691367
Loss in iteration 193 : 0.3532690832175735
Loss in iteration 194 : 0.35326032115311296
Loss in iteration 195 : 0.3532513834903736
Loss in iteration 196 : 0.35324286942671923
Loss in iteration 197 : 0.35323428873327856
Loss in iteration 198 : 0.35322587347701023
Loss in iteration 199 : 0.3532175478221828
Loss in iteration 200 : 0.3532093770301792
Testing accuracy  of updater 1 on alg 1 with rate 0.09999999999999998 = 0.8497635280388183, training accuracy 0.846529484029484, time elapsed: 3447 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 13.200305885864088
Loss in iteration 3 : 10.884796251599782
Loss in iteration 4 : 4.841717071582836
Loss in iteration 5 : 4.017886917624006
Loss in iteration 6 : 2.949427640763184
Loss in iteration 7 : 2.92947460140498
Loss in iteration 8 : 2.952025806011961
Loss in iteration 9 : 2.9194154881526533
Loss in iteration 10 : 2.8343691251571204
Loss in iteration 11 : 2.707031561919872
Loss in iteration 12 : 2.5486896491368913
Loss in iteration 13 : 2.366938960609016
Loss in iteration 14 : 2.172696595596636
Loss in iteration 15 : 1.9786953844332698
Loss in iteration 16 : 1.8072149945320737
Loss in iteration 17 : 1.674876058029152
Loss in iteration 18 : 1.5797291877898105
Loss in iteration 19 : 1.5115440955194812
Loss in iteration 20 : 1.4764081298202247
Loss in iteration 21 : 1.4894216415473567
Loss in iteration 22 : 1.8102070480452102
Loss in iteration 23 : 2.900569072316689
Loss in iteration 24 : 1.9736662038053463
Loss in iteration 25 : 2.9739443451873906
Loss in iteration 26 : 1.9791133029117802
Loss in iteration 27 : 2.4499453115928502
Loss in iteration 28 : 1.9402115472059762
Loss in iteration 29 : 1.9788604686931952
Loss in iteration 30 : 1.6573538526500273
Loss in iteration 31 : 1.5224938343557863
Loss in iteration 32 : 1.35185582110665
Loss in iteration 33 : 1.2802240097061333
Loss in iteration 34 : 1.3030062965451752
Loss in iteration 35 : 2.0022611405449893
Loss in iteration 36 : 2.7608790190428065
Loss in iteration 37 : 4.305489989753788
Loss in iteration 38 : 1.0149722505217018
Loss in iteration 39 : 0.9650130344029375
Loss in iteration 40 : 0.9480893524749767
Loss in iteration 41 : 1.0858541605009495
Loss in iteration 42 : 2.318051027295429
Loss in iteration 43 : 5.497322885695221
Loss in iteration 44 : 1.228720567615886
Loss in iteration 45 : 3.6769271713695515
Loss in iteration 46 : 4.764793374602459
Loss in iteration 47 : 1.2386895107547822
Loss in iteration 48 : 1.7512178445913333
Loss in iteration 49 : 1.8868610666020276
Loss in iteration 50 : 1.520962532270633
Loss in iteration 51 : 1.4582014580031146
Loss in iteration 52 : 1.3165336963867549
Loss in iteration 53 : 1.3845512390442496
Loss in iteration 54 : 1.5057596053024327
Loss in iteration 55 : 2.5870119520191848
Loss in iteration 56 : 1.878132678981386
Loss in iteration 57 : 3.493779574282381
Loss in iteration 58 : 1.2961678920579136
Loss in iteration 59 : 1.7305312263466748
Loss in iteration 60 : 2.5484529060694783
Loss in iteration 61 : 4.411663939040351
Loss in iteration 62 : 1.1190191434630172
Loss in iteration 63 : 1.4974885150520885
Loss in iteration 64 : 1.834206414445797
Loss in iteration 65 : 1.7258799618198977
Loss in iteration 66 : 2.4434095022356193
Loss in iteration 67 : 1.6300029200514852
Loss in iteration 68 : 2.109279804944262
Loss in iteration 69 : 1.7989759543988861
Loss in iteration 70 : 2.5694726730452255
Loss in iteration 71 : 1.569828014288363
Loss in iteration 72 : 1.899430942628623
Loss in iteration 73 : 1.7835994324819398
Loss in iteration 74 : 2.64188567467841
Loss in iteration 75 : 1.5688638639632477
Loss in iteration 76 : 2.002672415278203
Loss in iteration 77 : 1.859613372035974
Loss in iteration 78 : 2.8111252074269553
Loss in iteration 79 : 1.475624178882693
Loss in iteration 80 : 1.632291646527399
Loss in iteration 81 : 1.7202114379785431
Loss in iteration 82 : 2.7010102132805858
Loss in iteration 83 : 1.5804079150044534
Loss in iteration 84 : 2.2308411037911053
Loss in iteration 85 : 1.9676462549813372
Loss in iteration 86 : 3.1348818505322757
Loss in iteration 87 : 1.3378508454177258
Loss in iteration 88 : 1.2545740341873697
Loss in iteration 89 : 1.369627456072949
Loss in iteration 90 : 2.0969517600993886
Loss in iteration 91 : 2.1347155580312687
Loss in iteration 92 : 3.85620331014549
Loss in iteration 93 : 1.0769830449962838
Loss in iteration 94 : 0.9725886057654107
Loss in iteration 95 : 0.9811077899110159
Loss in iteration 96 : 1.1957735107063616
Loss in iteration 97 : 2.41855073203177
Loss in iteration 98 : 5.735062977903343
Loss in iteration 99 : 1.4716811808576813
Loss in iteration 100 : 4.280173522336814
Loss in iteration 101 : 5.032688697532756
Loss in iteration 102 : 1.427040933988564
Loss in iteration 103 : 2.032433577757492
Loss in iteration 104 : 1.9400208871575815
Loss in iteration 105 : 1.4531940895869242
Loss in iteration 106 : 1.3176180644121749
Loss in iteration 107 : 1.2229461625224658
Loss in iteration 108 : 1.1255307358140743
Loss in iteration 109 : 1.0746254477492228
Loss in iteration 110 : 1.2937609431309056
Loss in iteration 111 : 2.242549725086202
Loss in iteration 112 : 5.06779824433887
Loss in iteration 113 : 0.9913965325178389
Loss in iteration 114 : 2.8130945178580116
Loss in iteration 115 : 5.280882045871063
Loss in iteration 116 : 1.226313265461598
Loss in iteration 117 : 2.5734750780205973
Loss in iteration 118 : 3.1583659210448194
Loss in iteration 119 : 1.3533727816131815
Loss in iteration 120 : 1.2233637254889582
Loss in iteration 121 : 1.2061279746497149
Loss in iteration 122 : 1.1574837854810576
Loss in iteration 123 : 1.1702755625668946
Loss in iteration 124 : 1.6880403865371505
Loss in iteration 125 : 2.479617863556573
Loss in iteration 126 : 4.891040814287184
Loss in iteration 127 : 1.0074462212373954
Loss in iteration 128 : 2.128930555517119
Loss in iteration 129 : 3.7050010819557726
Loss in iteration 130 : 1.1662631274367257
Loss in iteration 131 : 1.1707581824679738
Loss in iteration 132 : 1.5899956427689712
Loss in iteration 133 : 2.7474555707807644
Loss in iteration 134 : 1.6241165769794639
Loss in iteration 135 : 2.488522201886957
Loss in iteration 136 : 1.8162755614271449
Loss in iteration 137 : 2.7411145331841085
Loss in iteration 138 : 1.5409673864737112
Loss in iteration 139 : 1.772042220106669
Loss in iteration 140 : 1.7112007237623081
Loss in iteration 141 : 2.4726290252883305
Loss in iteration 142 : 1.6192218805484415
Loss in iteration 143 : 2.1460009653181515
Loss in iteration 144 : 1.801999339061649
Loss in iteration 145 : 2.668297955674704
Loss in iteration 146 : 1.5425406604568435
Loss in iteration 147 : 1.8871586606951614
Loss in iteration 148 : 1.7868160262315727
Loss in iteration 149 : 2.7022753675524886
Loss in iteration 150 : 1.5212473589873237
Loss in iteration 151 : 1.862838329056442
Loss in iteration 152 : 1.8443727313861522
Loss in iteration 153 : 2.912678023547568
Loss in iteration 154 : 1.4198334201831389
Loss in iteration 155 : 1.5569879622949399
Loss in iteration 156 : 1.7655454254034644
Loss in iteration 157 : 2.979694499671557
Loss in iteration 158 : 1.424304975343571
Loss in iteration 159 : 1.848513766168225
Loss in iteration 160 : 2.1368817919626113
Loss in iteration 161 : 3.6777521953796386
Loss in iteration 162 : 1.1383225405765292
Loss in iteration 163 : 1.0297122237296086
Loss in iteration 164 : 0.9913331964610855
Loss in iteration 165 : 0.9682723982071485
Loss in iteration 166 : 1.3044126323560172
Loss in iteration 167 : 3.336587091031846
Loss in iteration 168 : 1.8537107719973955
Loss in iteration 169 : 4.601074421014826
Loss in iteration 170 : 0.8969107801503083
Loss in iteration 171 : 0.9578425542872449
Loss in iteration 172 : 1.1592564265432808
Loss in iteration 173 : 2.5124009615630905
Loss in iteration 174 : 5.842830610410414
Loss in iteration 175 : 1.6439986426769682
Loss in iteration 176 : 3.820426075190204
Loss in iteration 177 : 4.057980493449579
Loss in iteration 178 : 1.265532982307533
Loss in iteration 179 : 1.4575969880301023
Loss in iteration 180 : 1.3621360362101553
Loss in iteration 181 : 1.2714459820712158
Loss in iteration 182 : 1.2527977244314978
Loss in iteration 183 : 1.2650759664618316
Loss in iteration 184 : 1.878046086114851
Loss in iteration 185 : 2.227284438760469
Loss in iteration 186 : 4.1837913074264685
Loss in iteration 187 : 0.9578424450343058
Loss in iteration 188 : 0.9446276575141423
Loss in iteration 189 : 0.9731414133370592
Loss in iteration 190 : 1.5977645924348483
Loss in iteration 191 : 4.2443089286438775
Loss in iteration 192 : 0.9251522610777814
Loss in iteration 193 : 0.95170830699105
Loss in iteration 194 : 2.105401921304329
Loss in iteration 195 : 5.378430820212078
Loss in iteration 196 : 1.1544239776999594
Loss in iteration 197 : 3.2270986017597836
Loss in iteration 198 : 4.739956414166238
Loss in iteration 199 : 1.200689525971346
Loss in iteration 200 : 1.7446533993283444
Testing accuracy  of updater 2 on alg 1 with rate 10.0 = 0.8440513481972852, training accuracy 0.8438267813267813, time elapsed: 3329 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.5367658465716663
Loss in iteration 3 : 1.3052209952406877
Loss in iteration 4 : 0.7001803862902508
Loss in iteration 5 : 0.6662779575579437
Loss in iteration 6 : 0.5416840751894345
Loss in iteration 7 : 0.5018309433006266
Loss in iteration 8 : 0.50039202179943
Loss in iteration 9 : 0.4956551817453353
Loss in iteration 10 : 0.48724031374941895
Loss in iteration 11 : 0.4753620422560926
Loss in iteration 12 : 0.4613268298651334
Loss in iteration 13 : 0.44640801368372096
Loss in iteration 14 : 0.4314647404751176
Loss in iteration 15 : 0.41796477528273934
Loss in iteration 16 : 0.40745891501918735
Loss in iteration 17 : 0.40067168852828466
Loss in iteration 18 : 0.39789173769181707
Loss in iteration 19 : 0.397573036726638
Loss in iteration 20 : 0.3976736837831974
Loss in iteration 21 : 0.39629947485164424
Loss in iteration 22 : 0.3930556292102105
Loss in iteration 23 : 0.3881881919852257
Loss in iteration 24 : 0.38246338066664975
Loss in iteration 25 : 0.3770072214114962
Loss in iteration 26 : 0.3726918719345201
Loss in iteration 27 : 0.3695511119562059
Loss in iteration 28 : 0.3675618958682814
Loss in iteration 29 : 0.36642282881992877
Loss in iteration 30 : 0.3657288026459315
Loss in iteration 31 : 0.36512149790733367
Loss in iteration 32 : 0.36442211608094477
Loss in iteration 33 : 0.36370328668360863
Loss in iteration 34 : 0.3634141626045892
Loss in iteration 35 : 0.3634390163791024
Loss in iteration 36 : 0.3655130083438551
Loss in iteration 37 : 0.37049056597487795
Loss in iteration 38 : 0.3839232416566327
Loss in iteration 39 : 0.39502976173199517
Loss in iteration 40 : 0.42433108174105594
Loss in iteration 41 : 0.41425789308019034
Loss in iteration 42 : 0.44795121980546465
Loss in iteration 43 : 0.40644281456974046
Loss in iteration 44 : 0.41961715479472395
Loss in iteration 45 : 0.39881637141522114
Loss in iteration 46 : 0.40514725460873974
Loss in iteration 47 : 0.391411539943375
Loss in iteration 48 : 0.3929794610082012
Loss in iteration 49 : 0.38582282251030137
Loss in iteration 50 : 0.3866200681932886
Loss in iteration 51 : 0.3810155342920084
Loss in iteration 52 : 0.38277344985798156
Loss in iteration 53 : 0.3788250968983759
Loss in iteration 54 : 0.3815719100399092
Loss in iteration 55 : 0.3808758758140461
Loss in iteration 56 : 0.38799427818890087
Loss in iteration 57 : 0.38653678130717883
Loss in iteration 58 : 0.3993839106343889
Loss in iteration 59 : 0.3922800417686821
Loss in iteration 60 : 0.4048691386279197
Loss in iteration 61 : 0.39427368215517306
Loss in iteration 62 : 0.4067640700849414
Loss in iteration 63 : 0.39340019081476696
Loss in iteration 64 : 0.40301677469486885
Loss in iteration 65 : 0.3916229721419091
Loss in iteration 66 : 0.3997972566474395
Loss in iteration 67 : 0.38760577258929046
Loss in iteration 68 : 0.3936368882556763
Loss in iteration 69 : 0.38668147984780143
Loss in iteration 70 : 0.3930202356237739
Loss in iteration 71 : 0.3852885152368971
Loss in iteration 72 : 0.392059655797881
Loss in iteration 73 : 0.3847853071365314
Loss in iteration 74 : 0.391849072635758
Loss in iteration 75 : 0.3860434925081052
Loss in iteration 76 : 0.39622535252687613
Loss in iteration 77 : 0.38912262166702305
Loss in iteration 78 : 0.4007935879627193
Loss in iteration 79 : 0.39093207226097887
Loss in iteration 80 : 0.4009168604633106
Loss in iteration 81 : 0.390791101850111
Loss in iteration 82 : 0.4006054029223111
Loss in iteration 83 : 0.38958612498908857
Loss in iteration 84 : 0.3966218072612366
Loss in iteration 85 : 0.38605296736128586
Loss in iteration 86 : 0.39279963606923146
Loss in iteration 87 : 0.38420001603679316
Loss in iteration 88 : 0.39043284807353856
Loss in iteration 89 : 0.384677988529593
Loss in iteration 90 : 0.39252143838507414
Loss in iteration 91 : 0.38592893331207495
Loss in iteration 92 : 0.3964221973312553
Loss in iteration 93 : 0.38989233378050614
Loss in iteration 94 : 0.4021848560347344
Loss in iteration 95 : 0.3919508665162947
Loss in iteration 96 : 0.4029860225904877
Loss in iteration 97 : 0.3914045566381871
Loss in iteration 98 : 0.40129296398710784
Loss in iteration 99 : 0.3904235102642759
Loss in iteration 100 : 0.3989304361388281
Loss in iteration 101 : 0.38725061569013025
Loss in iteration 102 : 0.39386078888726916
Loss in iteration 103 : 0.38519027756367813
Loss in iteration 104 : 0.3914222969015767
Loss in iteration 105 : 0.3844938444675685
Loss in iteration 106 : 0.39076793495513024
Loss in iteration 107 : 0.38503689491182264
Loss in iteration 108 : 0.3938245569846483
Loss in iteration 109 : 0.38703669585319517
Loss in iteration 110 : 0.3976546517625939
Loss in iteration 111 : 0.3897592403339366
Loss in iteration 112 : 0.4014556246471403
Loss in iteration 113 : 0.39127500496040596
Loss in iteration 114 : 0.40330813381097375
Loss in iteration 115 : 0.39144736113399264
Loss in iteration 116 : 0.40082362046423786
Loss in iteration 117 : 0.3904529210385237
Loss in iteration 118 : 0.3985002789256787
Loss in iteration 119 : 0.38673474228374455
Loss in iteration 120 : 0.3922797194752515
Loss in iteration 121 : 0.3848444079278447
Loss in iteration 122 : 0.3916186379937717
Loss in iteration 123 : 0.3842590118806302
Loss in iteration 124 : 0.39082058188867297
Loss in iteration 125 : 0.3847073172585083
Loss in iteration 126 : 0.3937106618024683
Loss in iteration 127 : 0.3866535707701626
Loss in iteration 128 : 0.3970026044615865
Loss in iteration 129 : 0.3898145084358557
Loss in iteration 130 : 0.4016837680997841
Loss in iteration 131 : 0.3913318826541563
Loss in iteration 132 : 0.4031339459679
Loss in iteration 133 : 0.3909886458097138
Loss in iteration 134 : 0.400122095951602
Loss in iteration 135 : 0.38978004683624246
Loss in iteration 136 : 0.3973070265160431
Loss in iteration 137 : 0.38508616070663015
Loss in iteration 138 : 0.3913178192859762
Loss in iteration 139 : 0.3843813838818633
Loss in iteration 140 : 0.3911869612032406
Loss in iteration 141 : 0.3841400422747701
Loss in iteration 142 : 0.3910765091024948
Loss in iteration 143 : 0.38485071098537155
Loss in iteration 144 : 0.3943914670740173
Loss in iteration 145 : 0.3872627176200075
Loss in iteration 146 : 0.3989004033158411
Loss in iteration 147 : 0.39094782257405986
Loss in iteration 148 : 0.4037184273143244
Loss in iteration 149 : 0.3916672818733157
Loss in iteration 150 : 0.4028599857610453
Loss in iteration 151 : 0.39074350475938996
Loss in iteration 152 : 0.3988287728481685
Loss in iteration 153 : 0.3878262288394053
Loss in iteration 154 : 0.3945797073227723
Loss in iteration 155 : 0.38484290376677555
Loss in iteration 156 : 0.3914080236182409
Loss in iteration 157 : 0.38441507167881644
Loss in iteration 158 : 0.3915287140847159
Loss in iteration 159 : 0.38455310067831955
Loss in iteration 160 : 0.39203314553151547
Loss in iteration 161 : 0.3858115456802952
Loss in iteration 162 : 0.396019400509559
Loss in iteration 163 : 0.388739760049722
Loss in iteration 164 : 0.4005456084699823
Loss in iteration 165 : 0.3913852175928235
Loss in iteration 166 : 0.4033553226549049
Loss in iteration 167 : 0.3910352609234099
Loss in iteration 168 : 0.4006042854429464
Loss in iteration 169 : 0.3901151407216028
Loss in iteration 170 : 0.3986538290280693
Loss in iteration 171 : 0.38696969507164763
Loss in iteration 172 : 0.39286006632444576
Loss in iteration 173 : 0.3848008127094088
Loss in iteration 174 : 0.3915639596709888
Loss in iteration 175 : 0.38420061932517935
Loss in iteration 176 : 0.3907805099232211
Loss in iteration 177 : 0.3847245863821145
Loss in iteration 178 : 0.39340869830589953
Loss in iteration 179 : 0.38592410516908837
Loss in iteration 180 : 0.39589516787108575
Loss in iteration 181 : 0.3887868106937787
Loss in iteration 182 : 0.3999460036934893
Loss in iteration 183 : 0.390703288064944
Loss in iteration 184 : 0.40254568801600427
Loss in iteration 185 : 0.3909742935651439
Loss in iteration 186 : 0.4007560932059946
Loss in iteration 187 : 0.3897418154863734
Loss in iteration 188 : 0.397369928878786
Loss in iteration 189 : 0.3853606221411219
Loss in iteration 190 : 0.3911558030217329
Loss in iteration 191 : 0.3843831848707616
Loss in iteration 192 : 0.39138816873374743
Loss in iteration 193 : 0.38492863762980245
Loss in iteration 194 : 0.3921175537110019
Loss in iteration 195 : 0.38564956002834183
Loss in iteration 196 : 0.39528953247130655
Loss in iteration 197 : 0.3869623040389777
Loss in iteration 198 : 0.3974886243529656
Loss in iteration 199 : 0.3893819411511943
Loss in iteration 200 : 0.4007854779596879
Testing accuracy  of updater 2 on alg 1 with rate 1.0 = 0.8469381487623611, training accuracy 0.8493243243243244, time elapsed: 3966 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6549966671244584
Loss in iteration 3 : 0.5561764172418023
Loss in iteration 4 : 0.5844209549164371
Loss in iteration 5 : 0.5698267485349854
Loss in iteration 6 : 0.5212280733852646
Loss in iteration 7 : 0.4665887576763199
Loss in iteration 8 : 0.44280054279347786
Loss in iteration 9 : 0.43798398408551037
Loss in iteration 10 : 0.43386609011064947
Loss in iteration 11 : 0.428998678073541
Loss in iteration 12 : 0.42149252752869204
Loss in iteration 13 : 0.41148713554721833
Loss in iteration 14 : 0.40107029901055696
Loss in iteration 15 : 0.3939085360261752
Loss in iteration 16 : 0.3902804372748307
Loss in iteration 17 : 0.3880092360343824
Loss in iteration 18 : 0.38555737725386796
Loss in iteration 19 : 0.38306719389387445
Loss in iteration 20 : 0.38095804533147404
Loss in iteration 21 : 0.3792855395587801
Loss in iteration 22 : 0.37787834408894405
Loss in iteration 23 : 0.3765565644900449
Loss in iteration 24 : 0.37519284224883337
Loss in iteration 25 : 0.3737601139745703
Loss in iteration 26 : 0.3722828523210693
Loss in iteration 27 : 0.37082224060975133
Loss in iteration 28 : 0.3694257307032381
Loss in iteration 29 : 0.3681238482741767
Loss in iteration 30 : 0.36690604989112524
Loss in iteration 31 : 0.36580484283688547
Loss in iteration 32 : 0.36482722662566386
Loss in iteration 33 : 0.3639688258337554
Loss in iteration 34 : 0.3632180821760986
Loss in iteration 35 : 0.3625613715539966
Loss in iteration 36 : 0.362064998471041
Loss in iteration 37 : 0.36168066912424574
Loss in iteration 38 : 0.36134312823476755
Loss in iteration 39 : 0.3610546331579231
Loss in iteration 40 : 0.36082387680139205
Loss in iteration 41 : 0.3606220192997414
Loss in iteration 42 : 0.36043315661626
Loss in iteration 43 : 0.3602453025846249
Loss in iteration 44 : 0.3600558984527535
Loss in iteration 45 : 0.3598650572361106
Loss in iteration 46 : 0.35967123319938904
Loss in iteration 47 : 0.35947674075177716
Loss in iteration 48 : 0.35928527181943803
Loss in iteration 49 : 0.3591031586210933
Loss in iteration 50 : 0.35893048170188746
Loss in iteration 51 : 0.3587699628700221
Loss in iteration 52 : 0.3586201259099957
Loss in iteration 53 : 0.35847757870616753
Loss in iteration 54 : 0.35834150992737834
Loss in iteration 55 : 0.35821142562260355
Loss in iteration 56 : 0.35808861657084223
Loss in iteration 57 : 0.35796981540225076
Loss in iteration 58 : 0.35785531024677314
Loss in iteration 59 : 0.3577444981348473
Loss in iteration 60 : 0.3576365872904219
Loss in iteration 61 : 0.35753118969429576
Loss in iteration 62 : 0.35742808031960854
Loss in iteration 63 : 0.357328478355261
Loss in iteration 64 : 0.35723313906501014
Loss in iteration 65 : 0.35714043786417604
Loss in iteration 66 : 0.3570503918877715
Loss in iteration 67 : 0.35696292402569163
Loss in iteration 68 : 0.35687894900467443
Loss in iteration 69 : 0.3567971165341092
Loss in iteration 70 : 0.35671703422526774
Loss in iteration 71 : 0.3566389676041092
Loss in iteration 72 : 0.35656384695359006
Loss in iteration 73 : 0.35649185404419187
Loss in iteration 74 : 0.35642079019896655
Loss in iteration 75 : 0.35635145773595206
Loss in iteration 76 : 0.35628489214695597
Loss in iteration 77 : 0.35622033561358624
Loss in iteration 78 : 0.3561574783918616
Loss in iteration 79 : 0.3560961171817816
Loss in iteration 80 : 0.35603615860345317
Loss in iteration 81 : 0.3559776998968307
Loss in iteration 82 : 0.3559204960206295
Loss in iteration 83 : 0.35586491766295814
Loss in iteration 84 : 0.3558109542207269
Loss in iteration 85 : 0.3557580382960207
Loss in iteration 86 : 0.3557066795234808
Loss in iteration 87 : 0.3556566642335437
Loss in iteration 88 : 0.3556076087643847
Loss in iteration 89 : 0.3555600221707182
Loss in iteration 90 : 0.3555132355731325
Loss in iteration 91 : 0.3554676116663365
Loss in iteration 92 : 0.3554234875224365
Loss in iteration 93 : 0.35538013425936527
Loss in iteration 94 : 0.3553383901803476
Loss in iteration 95 : 0.3552972258804164
Loss in iteration 96 : 0.35525789045258105
Loss in iteration 97 : 0.3552189857045447
Loss in iteration 98 : 0.3551802890566306
Loss in iteration 99 : 0.35514317988317906
Loss in iteration 100 : 0.3551061521219509
Loss in iteration 101 : 0.35506957315631965
Loss in iteration 102 : 0.35503390355906445
Loss in iteration 103 : 0.35499861826068735
Loss in iteration 104 : 0.35496375847714934
Loss in iteration 105 : 0.35492968267918235
Loss in iteration 106 : 0.35489622044767677
Loss in iteration 107 : 0.3548629144858358
Loss in iteration 108 : 0.35482984975543147
Loss in iteration 109 : 0.3547976535771815
Loss in iteration 110 : 0.3547658646274836
Loss in iteration 111 : 0.35473461894318475
Loss in iteration 112 : 0.35470363929418414
Loss in iteration 113 : 0.35467333453954203
Loss in iteration 114 : 0.3546440907374719
Loss in iteration 115 : 0.35461534199367084
Loss in iteration 116 : 0.3545864552581509
Loss in iteration 117 : 0.354558685326571
Loss in iteration 118 : 0.35453118502227743
Loss in iteration 119 : 0.3545040317448068
Loss in iteration 120 : 0.35447767615190257
Loss in iteration 121 : 0.35445147583043013
Loss in iteration 122 : 0.3544256774639743
Loss in iteration 123 : 0.35440043429112295
Loss in iteration 124 : 0.3543756204932978
Loss in iteration 125 : 0.35435092970497556
Loss in iteration 126 : 0.35432618746328853
Loss in iteration 127 : 0.35430253463896877
Loss in iteration 128 : 0.3542785217330256
Loss in iteration 129 : 0.3542550950853551
Loss in iteration 130 : 0.35423226682630987
Loss in iteration 131 : 0.35420992744278756
Loss in iteration 132 : 0.3541880018942289
Loss in iteration 133 : 0.3541666422072593
Loss in iteration 134 : 0.3541453346437442
Loss in iteration 135 : 0.3541244816060018
Loss in iteration 136 : 0.3541040374952029
Loss in iteration 137 : 0.3540842357266726
Loss in iteration 138 : 0.35406514372402875
Loss in iteration 139 : 0.3540466468705044
Loss in iteration 140 : 0.35402750477248446
Loss in iteration 141 : 0.35400925468237837
Loss in iteration 142 : 0.3539913781800066
Loss in iteration 143 : 0.35397414520601594
Loss in iteration 144 : 0.3539570547584049
Loss in iteration 145 : 0.3539395009307845
Loss in iteration 146 : 0.35392294851596795
Loss in iteration 147 : 0.3539069755374499
Loss in iteration 148 : 0.35389044045972456
Loss in iteration 149 : 0.35387501062838184
Loss in iteration 150 : 0.3538592524033873
Loss in iteration 151 : 0.3538441400963939
Loss in iteration 152 : 0.3538293992754306
Loss in iteration 153 : 0.35381486297822456
Loss in iteration 154 : 0.35380010850828936
Loss in iteration 155 : 0.35378572861132507
Loss in iteration 156 : 0.35377149710351147
Loss in iteration 157 : 0.3537574422458454
Loss in iteration 158 : 0.35374353519344154
Loss in iteration 159 : 0.3537298326888877
Loss in iteration 160 : 0.3537169275025421
Loss in iteration 161 : 0.3537037147050529
Loss in iteration 162 : 0.3536901704300252
Loss in iteration 163 : 0.3536769187365291
Loss in iteration 164 : 0.35366427781762255
Loss in iteration 165 : 0.35365108378805354
Loss in iteration 166 : 0.3536389744688138
Loss in iteration 167 : 0.35362588069974316
Loss in iteration 168 : 0.35361329849272627
Loss in iteration 169 : 0.3536009769011624
Loss in iteration 170 : 0.3535887284607627
Loss in iteration 171 : 0.3535767324888532
Loss in iteration 172 : 0.3535646988079866
Loss in iteration 173 : 0.3535527662633834
Loss in iteration 174 : 0.3535409162446996
Loss in iteration 175 : 0.35352937039468474
Loss in iteration 176 : 0.35351785460768614
Loss in iteration 177 : 0.35350612682825855
Loss in iteration 178 : 0.35349471649720754
Loss in iteration 179 : 0.353483530353514
Loss in iteration 180 : 0.35347235918395503
Loss in iteration 181 : 0.3534612649112064
Loss in iteration 182 : 0.353450255603666
Loss in iteration 183 : 0.35343941916667837
Loss in iteration 184 : 0.35342872390117813
Loss in iteration 185 : 0.3534182146848508
Loss in iteration 186 : 0.35340785369246686
Loss in iteration 187 : 0.35339704516950876
Loss in iteration 188 : 0.3533869807005546
Loss in iteration 189 : 0.3533764936788756
Loss in iteration 190 : 0.35336645363359737
Loss in iteration 191 : 0.35335638536547237
Loss in iteration 192 : 0.3533464825929047
Loss in iteration 193 : 0.3533366813633099
Loss in iteration 194 : 0.3533269411857458
Loss in iteration 195 : 0.3533174832837291
Loss in iteration 196 : 0.35330772390843895
Loss in iteration 197 : 0.35329829536805724
Loss in iteration 198 : 0.3532890976542383
Loss in iteration 199 : 0.3532801566725357
Loss in iteration 200 : 0.35327154611644507
Testing accuracy  of updater 2 on alg 1 with rate 0.09999999999999998 = 0.8493950003071065, training accuracy 0.8466523341523342, time elapsed: 4435 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 30.86598923888144
Loss in iteration 3 : 11.507581994223433
Loss in iteration 4 : 3.0906953443737972
Loss in iteration 5 : 2.5697024908525328
Loss in iteration 6 : 2.238908326264294
Loss in iteration 7 : 2.1753021611177084
Loss in iteration 8 : 2.997934543515922
Loss in iteration 9 : 5.580084439376735
Loss in iteration 10 : 3.861731624636468
Loss in iteration 11 : 5.008985747500675
Loss in iteration 12 : 2.7129528873832376
Loss in iteration 13 : 2.8334923494185635
Loss in iteration 14 : 2.417676048748685
Loss in iteration 15 : 2.8578276638054936
Loss in iteration 16 : 2.571077473171058
Loss in iteration 17 : 3.299448705157477
Loss in iteration 18 : 2.6604221264162815
Loss in iteration 19 : 3.2762494591612783
Loss in iteration 20 : 2.4546900144131527
Loss in iteration 21 : 2.82688524140031
Loss in iteration 22 : 2.2356876229048117
Loss in iteration 23 : 2.594508050064673
Loss in iteration 24 : 2.1795448666854353
Loss in iteration 25 : 2.6654589267740434
Loss in iteration 26 : 2.2690296814743895
Loss in iteration 27 : 2.8630509714917753
Loss in iteration 28 : 2.253119769024434
Loss in iteration 29 : 2.674268031914993
Loss in iteration 30 : 2.0817352531213364
Loss in iteration 31 : 2.4157980996659525
Loss in iteration 32 : 1.9852202021354428
Loss in iteration 33 : 2.3983690647186875
Loss in iteration 34 : 2.025743143654805
Loss in iteration 35 : 2.5168642895217994
Loss in iteration 36 : 2.0294694740298853
Loss in iteration 37 : 2.46058950055332
Loss in iteration 38 : 1.9450386530889618
Loss in iteration 39 : 2.3080063919715252
Loss in iteration 40 : 1.8603477193204736
Loss in iteration 41 : 2.229148316746885
Loss in iteration 42 : 1.853661736098946
Loss in iteration 43 : 2.2843242525616065
Loss in iteration 44 : 1.8586329810881745
Loss in iteration 45 : 2.290457470331978
Loss in iteration 46 : 1.8140826952481588
Loss in iteration 47 : 2.1771238780704896
Loss in iteration 48 : 1.750817158309278
Loss in iteration 49 : 2.1002397116395075
Loss in iteration 50 : 1.7263987728003931
Loss in iteration 51 : 2.137779941583005
Loss in iteration 52 : 1.736732123212775
Loss in iteration 53 : 2.1415295415030977
Loss in iteration 54 : 1.7038169605637445
Loss in iteration 55 : 2.05746532917414
Loss in iteration 56 : 1.6518960243013596
Loss in iteration 57 : 1.993123774233357
Loss in iteration 58 : 1.6331991798033496
Loss in iteration 59 : 2.0260201843577073
Loss in iteration 60 : 1.6420929078077406
Loss in iteration 61 : 2.0416236900546325
Loss in iteration 62 : 1.613721136823431
Loss in iteration 63 : 1.9588905651588746
Loss in iteration 64 : 1.566127708081844
Loss in iteration 65 : 1.895099303445857
Loss in iteration 66 : 1.544912325363142
Loss in iteration 67 : 1.921632593249256
Loss in iteration 68 : 1.562046040622386
Loss in iteration 69 : 1.9549346735275284
Loss in iteration 70 : 1.5579505666493345
Loss in iteration 71 : 1.9083404732473614
Loss in iteration 72 : 1.491950400112465
Loss in iteration 73 : 1.782427818262157
Loss in iteration 74 : 1.4588265708600745
Loss in iteration 75 : 1.8053892821036983
Loss in iteration 76 : 1.493999435275899
Loss in iteration 77 : 1.9012036939297543
Loss in iteration 78 : 1.49903543345182
Loss in iteration 79 : 1.8511438641299793
Loss in iteration 80 : 1.4322274135897843
Loss in iteration 81 : 1.7081113700765667
Loss in iteration 82 : 1.3901613863042637
Loss in iteration 83 : 1.7150676325980656
Loss in iteration 84 : 1.4260304462078368
Loss in iteration 85 : 1.834244933229677
Loss in iteration 86 : 1.4465602040076966
Loss in iteration 87 : 1.7848625049501827
Loss in iteration 88 : 1.3771354424902655
Loss in iteration 89 : 1.6388060181210122
Loss in iteration 90 : 1.339557960528106
Loss in iteration 91 : 1.6613381219283654
Loss in iteration 92 : 1.3792412657034638
Loss in iteration 93 : 1.780837024896291
Loss in iteration 94 : 1.3900294506533972
Loss in iteration 95 : 1.7117739049160658
Loss in iteration 96 : 1.3205738558711433
Loss in iteration 97 : 1.5697533213998724
Loss in iteration 98 : 1.3019005144185205
Loss in iteration 99 : 1.6331517576479468
Loss in iteration 100 : 1.3351226983764044
Loss in iteration 101 : 1.70222349293225
Loss in iteration 102 : 1.3437243726098496
Loss in iteration 103 : 1.6678888620365324
Loss in iteration 104 : 1.2958657656385053
Loss in iteration 105 : 1.53730794398541
Loss in iteration 106 : 1.2654323688196367
Loss in iteration 107 : 1.573760017356632
Loss in iteration 108 : 1.2860221830202279
Loss in iteration 109 : 1.6358486249269568
Loss in iteration 110 : 1.2956094363354531
Loss in iteration 111 : 1.60970110293435
Loss in iteration 112 : 1.257690842203163
Loss in iteration 113 : 1.4986070764210027
Loss in iteration 114 : 1.2265221922329455
Loss in iteration 115 : 1.5340509012175574
Loss in iteration 116 : 1.2463263312242372
Loss in iteration 117 : 1.6026554941615327
Loss in iteration 118 : 1.2609176753625007
Loss in iteration 119 : 1.5747383310558651
Loss in iteration 120 : 1.2228083773487157
Loss in iteration 121 : 1.4532040743743313
Loss in iteration 122 : 1.1909740343106416
Loss in iteration 123 : 1.4822042965192643
Loss in iteration 124 : 1.2118585877575696
Loss in iteration 125 : 1.5690131310995514
Loss in iteration 126 : 1.2252897945546317
Loss in iteration 127 : 1.527312887940186
Loss in iteration 128 : 1.1833673782948804
Loss in iteration 129 : 1.414027439310208
Loss in iteration 130 : 1.1615135594463566
Loss in iteration 131 : 1.463735566486619
Loss in iteration 132 : 1.2031891779517023
Loss in iteration 133 : 1.5549785925917063
Loss in iteration 134 : 1.180918247214315
Loss in iteration 135 : 1.4132988593019513
Loss in iteration 136 : 1.1414038244587188
Loss in iteration 137 : 1.390798809681545
Loss in iteration 138 : 1.1589770045574819
Loss in iteration 139 : 1.499892407571041
Loss in iteration 140 : 1.1733402124381576
Loss in iteration 141 : 1.4827622393740363
Loss in iteration 142 : 1.1405593524683966
Loss in iteration 143 : 1.3696842859162441
Loss in iteration 144 : 1.1172386399474878
Loss in iteration 145 : 1.379142082063716
Loss in iteration 146 : 1.1362251646161168
Loss in iteration 147 : 1.4623306829806262
Loss in iteration 148 : 1.144123825569624
Loss in iteration 149 : 1.4405702288357018
Loss in iteration 150 : 1.110879358249559
Loss in iteration 151 : 1.3323881263671333
Loss in iteration 152 : 1.0968040928283236
Loss in iteration 153 : 1.3702108441480858
Loss in iteration 154 : 1.1230042494020707
Loss in iteration 155 : 1.4496348040838685
Loss in iteration 156 : 1.1151444654208333
Loss in iteration 157 : 1.357703784528441
Loss in iteration 158 : 1.0800411228162796
Loss in iteration 159 : 1.3156291950368249
Loss in iteration 160 : 1.0846140969208673
Loss in iteration 161 : 1.3934614907097642
Loss in iteration 162 : 1.1033993477706405
Loss in iteration 163 : 1.3895688880678085
Loss in iteration 164 : 1.0770118633145132
Loss in iteration 165 : 1.3068693262590574
Loss in iteration 166 : 1.0623351430794685
Loss in iteration 167 : 1.323126314335365
Loss in iteration 168 : 1.0807549709038042
Loss in iteration 169 : 1.3884030158138443
Loss in iteration 170 : 1.076912583609709
Loss in iteration 171 : 1.3185870737418843
Loss in iteration 172 : 1.0432515775611062
Loss in iteration 173 : 1.2734083441299178
Loss in iteration 174 : 1.0489388070848364
Loss in iteration 175 : 1.3395185222934503
Loss in iteration 176 : 1.0658630905230209
Loss in iteration 177 : 1.3554489803031362
Loss in iteration 178 : 1.04597960486866
Loss in iteration 179 : 1.2703928704375116
Loss in iteration 180 : 1.023782356279961
Loss in iteration 181 : 1.27192338836053
Loss in iteration 182 : 1.0382198013647854
Loss in iteration 183 : 1.3277421046387476
Loss in iteration 184 : 1.0443503324792727
Loss in iteration 185 : 1.2927841486868559
Loss in iteration 186 : 1.0238670933159824
Loss in iteration 187 : 1.2608128043172657
Loss in iteration 188 : 1.0141031880590936
Loss in iteration 189 : 1.2677949820973353
Loss in iteration 190 : 1.0320650265791655
Loss in iteration 191 : 1.3233381913219093
Loss in iteration 192 : 1.0194824070883466
Loss in iteration 193 : 1.240476374638852
Loss in iteration 194 : 0.9892315393358065
Loss in iteration 195 : 1.212341642233098
Loss in iteration 196 : 1.0032660927026216
Loss in iteration 197 : 1.2887882999764138
Loss in iteration 198 : 1.021057674541611
Loss in iteration 199 : 1.2924851403634134
Loss in iteration 200 : 0.9981624730330925
Testing accuracy  of updater 3 on alg 1 with rate 10.0 = 0.8200356243473989, training accuracy 0.8162776412776412, time elapsed: 5305 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7700070944242571
Loss in iteration 3 : 0.47174051327666777
Loss in iteration 4 : 0.38706620460759383
Loss in iteration 5 : 0.37612060736594016
Loss in iteration 6 : 0.369510332335574
Loss in iteration 7 : 0.36545678308115714
Loss in iteration 8 : 0.3626774231179409
Loss in iteration 9 : 0.36059423881105124
Loss in iteration 10 : 0.35910898455574786
Loss in iteration 11 : 0.35801103750984015
Loss in iteration 12 : 0.3570974333487142
Loss in iteration 13 : 0.3563422828502665
Loss in iteration 14 : 0.355722412157986
Loss in iteration 15 : 0.35521711768975117
Loss in iteration 16 : 0.3548160675669118
Loss in iteration 17 : 0.35448595529323806
Loss in iteration 18 : 0.3542054058076483
Loss in iteration 19 : 0.3539771459538243
Loss in iteration 20 : 0.35377854534245656
Loss in iteration 21 : 0.35363405270374687
Loss in iteration 22 : 0.35348925679895044
Loss in iteration 23 : 0.3533646966828731
Loss in iteration 24 : 0.3532612430002535
Loss in iteration 25 : 0.3531265782546681
Loss in iteration 26 : 0.3530263648715356
Loss in iteration 27 : 0.35294337928306113
Loss in iteration 28 : 0.3528668897012842
Loss in iteration 29 : 0.3527982490872108
Loss in iteration 30 : 0.35272529318736723
Loss in iteration 31 : 0.35267992221349626
Loss in iteration 32 : 0.3526626132801494
Loss in iteration 33 : 0.35258881715771984
Loss in iteration 34 : 0.3525478987440771
Loss in iteration 35 : 0.35250468060859014
Loss in iteration 36 : 0.35248653269560887
Loss in iteration 37 : 0.3524341473545398
Loss in iteration 38 : 0.3523974386152566
Loss in iteration 39 : 0.352351331431607
Loss in iteration 40 : 0.35231050537620257
Loss in iteration 41 : 0.35227686506027445
Loss in iteration 42 : 0.35224683805345997
Loss in iteration 43 : 0.3522203572238944
Loss in iteration 44 : 0.35219681196174224
Loss in iteration 45 : 0.3521754502166122
Loss in iteration 46 : 0.3521550405676872
Loss in iteration 47 : 0.3521390165216215
Loss in iteration 48 : 0.35213925303931976
Loss in iteration 49 : 0.35212238425917675
Loss in iteration 50 : 0.3521180216193798
Loss in iteration 51 : 0.3521453347311584
Loss in iteration 52 : 0.352117518652202
Loss in iteration 53 : 0.3521334584861149
Loss in iteration 54 : 0.35207649274099956
Loss in iteration 55 : 0.3520461807815678
Loss in iteration 56 : 0.3520479909145725
Loss in iteration 57 : 0.3520580281011725
Loss in iteration 58 : 0.3520222647760284
Loss in iteration 59 : 0.35199187513808317
Loss in iteration 60 : 0.35197906701220233
Loss in iteration 61 : 0.3519565251959324
Loss in iteration 62 : 0.35194093688239686
Loss in iteration 63 : 0.35193180561743537
Loss in iteration 64 : 0.35192494389940443
Loss in iteration 65 : 0.3519158433932512
Loss in iteration 66 : 0.35192079557729133
Loss in iteration 67 : 0.35193055141332225
Loss in iteration 68 : 0.35193578572661977
Loss in iteration 69 : 0.3519580800387995
Loss in iteration 70 : 0.3519045780421373
Loss in iteration 71 : 0.3519003504523154
Loss in iteration 72 : 0.35188185577918496
Loss in iteration 73 : 0.3518599209886326
Loss in iteration 74 : 0.3518445931181813
Loss in iteration 75 : 0.35183581605254
Loss in iteration 76 : 0.3518283581272126
Loss in iteration 77 : 0.3518204910026045
Loss in iteration 78 : 0.3518153535777457
Loss in iteration 79 : 0.3518188651507015
Loss in iteration 80 : 0.35183007381956105
Loss in iteration 81 : 0.35185100192205465
Loss in iteration 82 : 0.3518174683822802
Loss in iteration 83 : 0.3518414450052669
Loss in iteration 84 : 0.3518034432019523
Loss in iteration 85 : 0.35178160239291023
Loss in iteration 86 : 0.351758145186748
Loss in iteration 87 : 0.3517512790982508
Loss in iteration 88 : 0.3517349778816275
Loss in iteration 89 : 0.3517273933637571
Loss in iteration 90 : 0.351716901879765
Loss in iteration 91 : 0.3517150675306444
Loss in iteration 92 : 0.3517100441535696
Loss in iteration 93 : 0.3517057961005236
Loss in iteration 94 : 0.35170465137846024
Loss in iteration 95 : 0.35175417480110577
Loss in iteration 96 : 0.3517338073869586
Loss in iteration 97 : 0.3517771463000778
Loss in iteration 98 : 0.35172458187131855
Loss in iteration 99 : 0.3517186505068104
Loss in iteration 100 : 0.3516853725900401
Loss in iteration 101 : 0.3516721462620571
Loss in iteration 102 : 0.35165522508441716
Loss in iteration 103 : 0.351646525883584
Loss in iteration 104 : 0.35163531576762047
Loss in iteration 105 : 0.35163337355265195
Loss in iteration 106 : 0.3516292838548193
Loss in iteration 107 : 0.35163072732658013
Loss in iteration 108 : 0.35163600499170355
Loss in iteration 109 : 0.3516874141719268
Loss in iteration 110 : 0.35165349727894146
Loss in iteration 111 : 0.35169145154851966
Loss in iteration 112 : 0.3516413327294519
Loss in iteration 113 : 0.3516257039783754
Loss in iteration 114 : 0.35162259001940255
Loss in iteration 115 : 0.3516694315770926
Loss in iteration 116 : 0.35162629768690495
Loss in iteration 117 : 0.35161429304595704
Loss in iteration 118 : 0.35158571357602897
Loss in iteration 119 : 0.35158494666019047
Loss in iteration 120 : 0.35156377464688565
Loss in iteration 121 : 0.3515644297395958
Loss in iteration 122 : 0.35155314826237566
Loss in iteration 123 : 0.35155332123499927
Loss in iteration 124 : 0.3515494341308373
Loss in iteration 125 : 0.35156870755898983
Loss in iteration 126 : 0.35156121018583686
Loss in iteration 127 : 0.3515856014827075
Loss in iteration 128 : 0.35158195205528125
Loss in iteration 129 : 0.35162388335706
Loss in iteration 130 : 0.3515779021952193
Loss in iteration 131 : 0.3515768964108028
Loss in iteration 132 : 0.3515528721440949
Loss in iteration 133 : 0.35153811753396147
Loss in iteration 134 : 0.3515131043049138
Loss in iteration 135 : 0.3515132079436398
Loss in iteration 136 : 0.35150230726334175
Loss in iteration 137 : 0.3515043260825079
Loss in iteration 138 : 0.35149852399063014
Loss in iteration 139 : 0.3515170797446411
Loss in iteration 140 : 0.35150605483109415
Loss in iteration 141 : 0.35152431786745086
Loss in iteration 142 : 0.35151690229537985
Loss in iteration 143 : 0.3515650225647263
Loss in iteration 144 : 0.3515157903966555
Loss in iteration 145 : 0.3515133589506964
Loss in iteration 146 : 0.3514866436672076
Loss in iteration 147 : 0.35147872578334866
Loss in iteration 148 : 0.351460046329311
Loss in iteration 149 : 0.3514641001949976
Loss in iteration 150 : 0.3514517402357809
Loss in iteration 151 : 0.35145809508289266
Loss in iteration 152 : 0.3514569402426477
Loss in iteration 153 : 0.3514752264600916
Loss in iteration 154 : 0.35148421439836725
Loss in iteration 155 : 0.35151904754294977
Loss in iteration 156 : 0.35148503594385705
Loss in iteration 157 : 0.35153487995298055
Loss in iteration 158 : 0.35149319326621525
Loss in iteration 159 : 0.35152189277496626
Loss in iteration 160 : 0.3514593600426964
Loss in iteration 161 : 0.3514503354785712
Loss in iteration 162 : 0.35142067867240745
Loss in iteration 163 : 0.35141514182224126
Loss in iteration 164 : 0.35140841887273905
Loss in iteration 165 : 0.35140930533315246
Loss in iteration 166 : 0.35141552430675005
Loss in iteration 167 : 0.35141847690395456
Loss in iteration 168 : 0.3514169430865161
Loss in iteration 169 : 0.35142842161868015
Loss in iteration 170 : 0.3514312406267118
Loss in iteration 171 : 0.3514797852725829
Loss in iteration 172 : 0.3514485373179226
Loss in iteration 173 : 0.3514879422768146
Loss in iteration 174 : 0.3514337426518599
Loss in iteration 175 : 0.35142415755741113
Loss in iteration 176 : 0.3513985598145814
Loss in iteration 177 : 0.3513947996381913
Loss in iteration 178 : 0.3513805911440289
Loss in iteration 179 : 0.35137962146577034
Loss in iteration 180 : 0.3513697633134688
Loss in iteration 181 : 0.3513868198360705
Loss in iteration 182 : 0.35137502439705853
Loss in iteration 183 : 0.3513896636759982
Loss in iteration 184 : 0.3513693251629356
Loss in iteration 185 : 0.3513667762034413
Loss in iteration 186 : 0.3513570189746944
Loss in iteration 187 : 0.3513550259707189
Loss in iteration 188 : 0.35135448126818053
Loss in iteration 189 : 0.3513726885255072
Loss in iteration 190 : 0.3513599116328138
Loss in iteration 191 : 0.3513900991763488
Loss in iteration 192 : 0.35140210252178133
Loss in iteration 193 : 0.3514362664409742
Loss in iteration 194 : 0.35140723714307404
Loss in iteration 195 : 0.35143481919200864
Loss in iteration 196 : 0.3513758532508462
Loss in iteration 197 : 0.3513620237052339
Loss in iteration 198 : 0.3513313515699807
Loss in iteration 199 : 0.35132112529015685
Loss in iteration 200 : 0.35131966425775063
Testing accuracy  of updater 3 on alg 1 with rate 1.0 = 0.8497635280388183, training accuracy 0.8495085995085995, time elapsed: 5122 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7935638049087376
Loss in iteration 3 : 0.6008684912411216
Loss in iteration 4 : 0.4769797676686116
Loss in iteration 5 : 0.462211219638326
Loss in iteration 6 : 0.45781132746489756
Loss in iteration 7 : 0.45516701582979857
Loss in iteration 8 : 0.45313566536745004
Loss in iteration 9 : 0.45129103898613065
Loss in iteration 10 : 0.4495921078138108
Loss in iteration 11 : 0.44798360193895465
Loss in iteration 12 : 0.44642732846269645
Loss in iteration 13 : 0.4448991162512151
Loss in iteration 14 : 0.44339699726956905
Loss in iteration 15 : 0.4419231294368304
Loss in iteration 16 : 0.4404705861481061
Loss in iteration 17 : 0.4390378753842298
Loss in iteration 18 : 0.43762342214392996
Loss in iteration 19 : 0.43622320398088665
Loss in iteration 20 : 0.4348309841690485
Loss in iteration 21 : 0.4334477662804485
Loss in iteration 22 : 0.43207545514797085
Loss in iteration 23 : 0.4307104988057861
Loss in iteration 24 : 0.42935244014269264
Loss in iteration 25 : 0.4280009601946664
Loss in iteration 26 : 0.42665708771710753
Loss in iteration 27 : 0.4253182045491133
Loss in iteration 28 : 0.4239855182180246
Loss in iteration 29 : 0.4226576985312272
Loss in iteration 30 : 0.42133465466964637
Loss in iteration 31 : 0.4200153376959651
Loss in iteration 32 : 0.4186994434753063
Loss in iteration 33 : 0.41738846487561
Loss in iteration 34 : 0.41608121586648167
Loss in iteration 35 : 0.4147776895025981
Loss in iteration 36 : 0.41347755757281524
Loss in iteration 37 : 0.41218277666457687
Loss in iteration 38 : 0.4109026523459202
Loss in iteration 39 : 0.40964597910595174
Loss in iteration 40 : 0.4084117098608929
Loss in iteration 41 : 0.40718708682752947
Loss in iteration 42 : 0.40597334898196563
Loss in iteration 43 : 0.4047814212224926
Loss in iteration 44 : 0.40361230786557767
Loss in iteration 45 : 0.40245855275749687
Loss in iteration 46 : 0.40134155133198124
Loss in iteration 47 : 0.4002467436573916
Loss in iteration 48 : 0.3991745624571114
Loss in iteration 49 : 0.3981214958061725
Loss in iteration 50 : 0.3970882315768672
Loss in iteration 51 : 0.3960786435897056
Loss in iteration 52 : 0.39509278181109236
Loss in iteration 53 : 0.394135951158762
Loss in iteration 54 : 0.39319249696046227
Loss in iteration 55 : 0.39226659220590854
Loss in iteration 56 : 0.3913654416718115
Loss in iteration 57 : 0.39049696544304546
Loss in iteration 58 : 0.38964716141909517
Loss in iteration 59 : 0.38881833191508414
Loss in iteration 60 : 0.3880131495185766
Loss in iteration 61 : 0.387223049322902
Loss in iteration 62 : 0.38645978428299044
Loss in iteration 63 : 0.385715456090047
Loss in iteration 64 : 0.38498593853868757
Loss in iteration 65 : 0.38428212945871626
Loss in iteration 66 : 0.38360554133870284
Loss in iteration 67 : 0.38294334084142206
Loss in iteration 68 : 0.38230099935467476
Loss in iteration 69 : 0.3816771327350632
Loss in iteration 70 : 0.3810712917134112
Loss in iteration 71 : 0.3804816059085374
Loss in iteration 72 : 0.37990798132428755
Loss in iteration 73 : 0.37935107555374714
Loss in iteration 74 : 0.37881059073568046
Loss in iteration 75 : 0.3782891151739691
Loss in iteration 76 : 0.37778154011048104
Loss in iteration 77 : 0.37728741033880475
Loss in iteration 78 : 0.3768052580644871
Loss in iteration 79 : 0.37633650251084366
Loss in iteration 80 : 0.3758819449050309
Loss in iteration 81 : 0.3754433226874094
Loss in iteration 82 : 0.37501927751490577
Loss in iteration 83 : 0.37460655582166874
Loss in iteration 84 : 0.3742060321764852
Loss in iteration 85 : 0.3738195324362794
Loss in iteration 86 : 0.3734443652252328
Loss in iteration 87 : 0.37308048992372783
Loss in iteration 88 : 0.37272560066736504
Loss in iteration 89 : 0.3723827717755853
Loss in iteration 90 : 0.3720512069450505
Loss in iteration 91 : 0.3717310787382877
Loss in iteration 92 : 0.37141746934744535
Loss in iteration 93 : 0.3711093232302608
Loss in iteration 94 : 0.3708089131771116
Loss in iteration 95 : 0.37051454080989826
Loss in iteration 96 : 0.37022492761208614
Loss in iteration 97 : 0.36993848312076555
Loss in iteration 98 : 0.3696583385131869
Loss in iteration 99 : 0.369386487266265
Loss in iteration 100 : 0.3691229000433583
Loss in iteration 101 : 0.36886599877416104
Loss in iteration 102 : 0.36861562781286006
Loss in iteration 103 : 0.36836965854252623
Loss in iteration 104 : 0.368129099266871
Loss in iteration 105 : 0.3678924417230111
Loss in iteration 106 : 0.36765950906648004
Loss in iteration 107 : 0.36743092577272424
Loss in iteration 108 : 0.3672083110182929
Loss in iteration 109 : 0.3669901044096252
Loss in iteration 110 : 0.3667774202691943
Loss in iteration 111 : 0.36657253205111134
Loss in iteration 112 : 0.36637224034726784
Loss in iteration 113 : 0.3661748207341478
Loss in iteration 114 : 0.3659825068664084
Loss in iteration 115 : 0.36579321881438626
Loss in iteration 116 : 0.3656085379134714
Loss in iteration 117 : 0.36542667690291536
Loss in iteration 118 : 0.36524855954666735
Loss in iteration 119 : 0.3650759205505688
Loss in iteration 120 : 0.3649064235082843
Loss in iteration 121 : 0.36473930455933556
Loss in iteration 122 : 0.3645751087097916
Loss in iteration 123 : 0.36441496733511924
Loss in iteration 124 : 0.36425977161440554
Loss in iteration 125 : 0.3641061116202961
Loss in iteration 126 : 0.36395445802244575
Loss in iteration 127 : 0.36380503349597276
Loss in iteration 128 : 0.36365778131872495
Loss in iteration 129 : 0.3635132246818269
Loss in iteration 130 : 0.3633709461902502
Loss in iteration 131 : 0.36323069156440124
Loss in iteration 132 : 0.3630921149716501
Loss in iteration 133 : 0.36295433968008467
Loss in iteration 134 : 0.3628175569197414
Loss in iteration 135 : 0.3626830916541522
Loss in iteration 136 : 0.36255168101652063
Loss in iteration 137 : 0.36242355816578387
Loss in iteration 138 : 0.3622981001072762
Loss in iteration 139 : 0.36217594558727795
Loss in iteration 140 : 0.36205633221991473
Loss in iteration 141 : 0.3619390854845619
Loss in iteration 142 : 0.36182410553847194
Loss in iteration 143 : 0.3617108466878965
Loss in iteration 144 : 0.3615986286750156
Loss in iteration 145 : 0.3614876264316473
Loss in iteration 146 : 0.361378336182837
Loss in iteration 147 : 0.36127157922854736
Loss in iteration 148 : 0.3611658966464322
Loss in iteration 149 : 0.3610613177588447
Loss in iteration 150 : 0.36095797139041536
Loss in iteration 151 : 0.36085657119342246
Loss in iteration 152 : 0.36075722102238694
Loss in iteration 153 : 0.3606599436160916
Loss in iteration 154 : 0.360563148364962
Loss in iteration 155 : 0.3604673865402978
Loss in iteration 156 : 0.3603735593513445
Loss in iteration 157 : 0.3602811268261366
Loss in iteration 158 : 0.3601899340135105
Loss in iteration 159 : 0.3601006450451754
Loss in iteration 160 : 0.36001241141564233
Loss in iteration 161 : 0.3599259386067643
Loss in iteration 162 : 0.35984055591332276
Loss in iteration 163 : 0.35975624928091515
Loss in iteration 164 : 0.35967294441225994
Loss in iteration 165 : 0.35959023619998604
Loss in iteration 166 : 0.3595085481868941
Loss in iteration 167 : 0.3594278815989606
Loss in iteration 168 : 0.3593481565149737
Loss in iteration 169 : 0.35927000286392335
Loss in iteration 170 : 0.3591928728876483
Loss in iteration 171 : 0.35911691602619294
Loss in iteration 172 : 0.3590416226959698
Loss in iteration 173 : 0.3589670418004088
Loss in iteration 174 : 0.35889318582944313
Loss in iteration 175 : 0.35881990986319784
Loss in iteration 176 : 0.3587470426448922
Loss in iteration 177 : 0.35867522774536814
Loss in iteration 178 : 0.3586049756991302
Loss in iteration 179 : 0.35853575311576424
Loss in iteration 180 : 0.35846802795935145
Loss in iteration 181 : 0.3584010430896568
Loss in iteration 182 : 0.35833461971496383
Loss in iteration 183 : 0.3582689308152804
Loss in iteration 184 : 0.35820432486524334
Loss in iteration 185 : 0.35814170765635145
Loss in iteration 186 : 0.3580804592842994
Loss in iteration 187 : 0.358019908875252
Loss in iteration 188 : 0.3579604245572505
Loss in iteration 189 : 0.3579026032916512
Loss in iteration 190 : 0.3578455492095127
Loss in iteration 191 : 0.35778898105985674
Loss in iteration 192 : 0.3577331148960539
Loss in iteration 193 : 0.35767827209901093
Loss in iteration 194 : 0.35762398607735313
Loss in iteration 195 : 0.35757020567450015
Loss in iteration 196 : 0.3575167448835132
Loss in iteration 197 : 0.357463915286204
Loss in iteration 198 : 0.3574115821169586
Loss in iteration 199 : 0.3573598316164087
Loss in iteration 200 : 0.35730946883709763
Testing accuracy  of updater 3 on alg 1 with rate 0.09999999999999998 = 0.8481665745347338, training accuracy 0.847051597051597, time elapsed: 3657 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 4000 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 1.0 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 4344 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9977087497627372
Loss in iteration 3 : 0.9953579969035659
Loss in iteration 4 : 0.9929664067709228
Loss in iteration 5 : 0.9905431201094781
Loss in iteration 6 : 0.9880935317584683
Loss in iteration 7 : 0.985621185368393
Loss in iteration 8 : 0.9831285763315405
Loss in iteration 9 : 0.9806175498414772
Loss in iteration 10 : 0.9780895205383133
Loss in iteration 11 : 0.975545603559427
Loss in iteration 12 : 0.9729866975429976
Loss in iteration 13 : 0.9704135397413154
Loss in iteration 14 : 0.9678267440433482
Loss in iteration 15 : 0.9652268280511056
Loss in iteration 16 : 0.9626142328817667
Loss in iteration 17 : 0.9599893379828666
Loss in iteration 18 : 0.9573524724357892
Loss in iteration 19 : 0.9547039237281607
Loss in iteration 20 : 0.952043944664537
Loss in iteration 21 : 0.9493727588826766
Loss in iteration 22 : 0.9466905653086299
Loss in iteration 23 : 0.9439975417924332
Loss in iteration 24 : 0.9412938481029236
Loss in iteration 25 : 0.9385796284153707
Loss in iteration 26 : 0.9358550133933228
Loss in iteration 27 : 0.9331201219428202
Loss in iteration 28 : 0.9303750626994401
Loss in iteration 29 : 0.9276199352960015
Loss in iteration 30 : 0.9248548314485451
Loss in iteration 31 : 0.9220798358909987
Loss in iteration 32 : 0.9192950271827545
Loss in iteration 33 : 0.9165004784092066
Loss in iteration 34 : 0.9136962577913823
Loss in iteration 35 : 0.9108824292180975
Loss in iteration 36 : 0.908059052711875
Loss in iteration 37 : 0.9052261848377771
Loss in iteration 38 : 0.9023838790630794
Loss in iteration 39 : 0.8995321860743655
Loss in iteration 40 : 0.8966711540575149
Loss in iteration 41 : 0.8938008289455334
Loss in iteration 42 : 0.8909212546381692
Loss in iteration 43 : 0.8880324731968079
Loss in iteration 44 : 0.8851345250177647
Loss in iteration 45 : 0.8822274489864853
Loss in iteration 46 : 0.8793112826149487
Loss in iteration 47 : 0.8763860621642694
Loss in iteration 48 : 0.8734518227541774
Loss in iteration 49 : 0.8705085984608499
Loss in iteration 50 : 0.8675564224045526
Loss in iteration 51 : 0.8645953268280513
Loss in iteration 52 : 0.8616253431669262
Loss in iteration 53 : 0.8586465021127108
Loss in iteration 54 : 0.8556588336695472
Loss in iteration 55 : 0.8526623672051329
Loss in iteration 56 : 0.8496571314965649
Loss in iteration 57 : 0.8466431547716006
Loss in iteration 58 : 0.8436204647458604
Loss in iteration 59 : 0.8405890886563842
Loss in iteration 60 : 0.8375490532919295
Loss in iteration 61 : 0.8345003850203724
Loss in iteration 62 : 0.8314431098134553
Loss in iteration 63 : 0.828377253269256
Loss in iteration 64 : 0.8253028406325082
Loss in iteration 65 : 0.8222198968131035
Loss in iteration 66 : 0.8191284464028322
Loss in iteration 67 : 0.8160285136906957
Loss in iteration 68 : 0.8129201226767849
Loss in iteration 69 : 0.8098032970850041
Loss in iteration 70 : 0.806678060374646
Loss in iteration 71 : 0.8035444357510212
Loss in iteration 72 : 0.8004024461751766
Loss in iteration 73 : 0.7972521143728094
Loss in iteration 74 : 0.7940934628425066
Loss in iteration 75 : 0.7909265138632673
Loss in iteration 76 : 0.7877512895015191
Loss in iteration 77 : 0.7845678116175413
Loss in iteration 78 : 0.7813761018714369
Loss in iteration 79 : 0.7781761817287067
Loss in iteration 80 : 0.7749680724653699
Loss in iteration 81 : 0.7717517951727897
Loss in iteration 82 : 0.7685273707621609
Loss in iteration 83 : 0.7652948199686974
Loss in iteration 84 : 0.7620541633555835
Loss in iteration 85 : 0.7588054213176681
Loss in iteration 86 : 0.7555486140849521
Loss in iteration 87 : 0.7522837617258629
Loss in iteration 88 : 0.7490108841503947
Loss in iteration 89 : 0.7457300011130314
Loss in iteration 90 : 0.7424411322155554
Loss in iteration 91 : 0.7391442969097187
Loss in iteration 92 : 0.7358395144997578
Loss in iteration 93 : 0.7325268041448685
Loss in iteration 94 : 0.7292061848614665
Loss in iteration 95 : 0.7258776755254549
Loss in iteration 96 : 0.7225412948743655
Loss in iteration 97 : 0.7191970615093684
Loss in iteration 98 : 0.7158449938973126
Loss in iteration 99 : 0.712485110372605
Loss in iteration 100 : 0.7091174291390389
Loss in iteration 101 : 0.7057419682716305
Loss in iteration 102 : 0.7023587457183049
Loss in iteration 103 : 0.6989677793016047
Loss in iteration 104 : 0.6955690867202924
Loss in iteration 105 : 0.6921626855509708
Loss in iteration 106 : 0.6887485932495816
Loss in iteration 107 : 0.6853268271529536
Loss in iteration 108 : 0.6818974044802322
Loss in iteration 109 : 0.678460342334328
Loss in iteration 110 : 0.6750156577032986
Loss in iteration 111 : 0.6715633674617241
Loss in iteration 112 : 0.6681034883720306
Loss in iteration 113 : 0.6646360370858122
Loss in iteration 114 : 0.6611610301450661
Loss in iteration 115 : 0.6576784839835038
Loss in iteration 116 : 0.6541884149277297
Loss in iteration 117 : 0.6506908391984609
Loss in iteration 118 : 0.6471857729117172
Loss in iteration 119 : 0.643673232079948
Loss in iteration 120 : 0.6401532326132013
Loss in iteration 121 : 0.6366257903202119
Loss in iteration 122 : 0.6330909209095311
Loss in iteration 123 : 0.6295486399905743
Loss in iteration 124 : 0.625998963074682
Loss in iteration 125 : 0.6224419055761862
Loss in iteration 126 : 0.6188774828133994
Loss in iteration 127 : 0.6153057100096583
Loss in iteration 128 : 0.6117266022942881
Loss in iteration 129 : 0.6081401747035867
Loss in iteration 130 : 0.6045464421817941
Loss in iteration 131 : 0.6009454195820173
Loss in iteration 132 : 0.5973371216671889
Loss in iteration 133 : 0.5937215631109656
Loss in iteration 134 : 0.5900987584986261
Loss in iteration 135 : 0.5864687223279763
Loss in iteration 136 : 0.5828314690102302
Loss in iteration 137 : 0.5791870128708456
Loss in iteration 138 : 0.575535368150409
Loss in iteration 139 : 0.5718765490054537
Loss in iteration 140 : 0.5682105695092902
Loss in iteration 141 : 0.5645374436528346
Loss in iteration 142 : 0.5608571853453889
Loss in iteration 143 : 0.5571698084154612
Loss in iteration 144 : 0.553475326611528
Loss in iteration 145 : 0.5497737536028113
Loss in iteration 146 : 0.5460651029800441
Loss in iteration 147 : 0.542349388256213
Loss in iteration 148 : 0.5386266228672953
Loss in iteration 149 : 0.5348968201730112
Loss in iteration 150 : 0.531159993457507
Loss in iteration 151 : 0.5274161559300995
Loss in iteration 152 : 0.5236653207259719
Loss in iteration 153 : 0.5199075009068397
Loss in iteration 154 : 0.5161427094616552
Loss in iteration 155 : 0.5123709593072967
Loss in iteration 156 : 0.5085922632891916
Loss in iteration 157 : 0.5048066341820087
Loss in iteration 158 : 0.5010140846902981
Loss in iteration 159 : 0.4972146274491133
Loss in iteration 160 : 0.4934082750246682
Loss in iteration 161 : 0.4895950399149443
Loss in iteration 162 : 0.4857749345503077
Loss in iteration 163 : 0.48252415431212503
Loss in iteration 164 : 0.4824638140561152
Loss in iteration 165 : 0.4820813663726612
Loss in iteration 166 : 0.48182888927255657
Loss in iteration 167 : 0.48166260441360437
Loss in iteration 168 : 0.4814422189549062
Loss in iteration 169 : 0.48126452179695167
Loss in iteration 170 : 0.48108893579965945
Loss in iteration 171 : 0.48091943019781797
Loss in iteration 172 : 0.48075097927077126
Loss in iteration 173 : 0.48058247456008396
Loss in iteration 174 : 0.4804139420331518
Loss in iteration 175 : 0.480245713404757
Loss in iteration 176 : 0.4800784114318449
Loss in iteration 177 : 0.47991149350466383
Loss in iteration 178 : 0.4797445591302583
Loss in iteration 179 : 0.4795771951890519
Loss in iteration 180 : 0.47940960139540334
Loss in iteration 181 : 0.4792415055055576
Loss in iteration 182 : 0.47907266955791084
Loss in iteration 183 : 0.47890294306401093
Loss in iteration 184 : 0.4787323343365096
Loss in iteration 185 : 0.4785607435208494
Loss in iteration 186 : 0.47838811846453155
Loss in iteration 187 : 0.4782144289549595
Loss in iteration 188 : 0.47803967733343533
Loss in iteration 189 : 0.47786368466662843
Loss in iteration 190 : 0.47768635696316364
Loss in iteration 191 : 0.4775076505627495
Loss in iteration 192 : 0.47732761912286575
Loss in iteration 193 : 0.47714606337154863
Loss in iteration 194 : 0.4769630004550087
Loss in iteration 195 : 0.4767783560410737
Loss in iteration 196 : 0.476592104625517
Loss in iteration 197 : 0.47640432310049674
Loss in iteration 198 : 0.4762148648892651
Loss in iteration 199 : 0.47602370000718075
Loss in iteration 200 : 0.47583077403890645
Testing accuracy  of updater 4 on alg 1 with rate 0.09999999999999998 = 0.7637737239727289, training accuracy 0.7591830466830467, time elapsed: 4402 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 9.923344732223397
Loss in iteration 3 : 3.6672824314554613
Loss in iteration 4 : 1.18384757030332
Loss in iteration 5 : 1.133066046903402
Loss in iteration 6 : 1.2464241968354512
Loss in iteration 7 : 2.271479562497434
Loss in iteration 8 : 1.6147828230319419
Loss in iteration 9 : 2.4132610091633824
Loss in iteration 10 : 1.1534838153077454
Loss in iteration 11 : 1.2424126933091673
Loss in iteration 12 : 1.2492462076953141
Loss in iteration 13 : 1.9660658036194725
Loss in iteration 14 : 1.3908179942129715
Loss in iteration 15 : 1.9987510391582357
Loss in iteration 16 : 1.2282902336048611
Loss in iteration 17 : 1.5616645012500787
Loss in iteration 18 : 1.3043065780823975
Loss in iteration 19 : 1.8383928352930587
Loss in iteration 20 : 1.2697382980644913
Loss in iteration 21 : 1.6950764780449536
Loss in iteration 22 : 1.276084542197252
Loss in iteration 23 : 1.7415271425464836
Loss in iteration 24 : 1.2693566903381301
Loss in iteration 25 : 1.6884238436449301
Loss in iteration 26 : 1.272916708684231
Loss in iteration 27 : 1.7248699448955442
Loss in iteration 28 : 1.2722368354926306
Loss in iteration 29 : 1.6979961722404804
Loss in iteration 30 : 1.2748792092551728
Loss in iteration 31 : 1.7206435639447735
Loss in iteration 32 : 1.270064222564155
Loss in iteration 33 : 1.6885007060082762
Loss in iteration 34 : 1.2742531763603633
Loss in iteration 35 : 1.7195700279790171
Loss in iteration 36 : 1.281840574379294
Loss in iteration 37 : 1.7165166301207195
Loss in iteration 38 : 1.2740780251149053
Loss in iteration 39 : 1.6884632930759866
Loss in iteration 40 : 1.2676670948224296
Loss in iteration 41 : 1.6925923399176608
Loss in iteration 42 : 1.288801995702921
Loss in iteration 43 : 1.754881062176924
Loss in iteration 44 : 1.2681948597951904
Loss in iteration 45 : 1.638252716767029
Loss in iteration 46 : 1.2687399942656985
Loss in iteration 47 : 1.7341930087958497
Loss in iteration 48 : 1.2996377819160554
Loss in iteration 49 : 1.7534852865889878
Loss in iteration 50 : 1.2694408594290176
Loss in iteration 51 : 1.6357146924453931
Loss in iteration 52 : 1.2666084529992623
Loss in iteration 53 : 1.7279116311995375
Loss in iteration 54 : 1.307060769037373
Loss in iteration 55 : 1.7837853188023658
Loss in iteration 56 : 1.2584381106877327
Loss in iteration 57 : 1.5906626692965795
Loss in iteration 58 : 1.2747563501753103
Loss in iteration 59 : 1.7635048190397271
Loss in iteration 60 : 1.3074125630068123
Loss in iteration 61 : 1.755314779315909
Loss in iteration 62 : 1.2644544426320188
Loss in iteration 63 : 1.611644736073788
Loss in iteration 64 : 1.2769558715819103
Loss in iteration 65 : 1.7506654264929986
Loss in iteration 66 : 1.3052710074607452
Loss in iteration 67 : 1.760381288764467
Loss in iteration 68 : 1.2682782349614954
Loss in iteration 69 : 1.6158645645666871
Loss in iteration 70 : 1.277109200706376
Loss in iteration 71 : 1.7425764325470796
Loss in iteration 72 : 1.3096800564761832
Loss in iteration 73 : 1.7655207164780822
Loss in iteration 74 : 1.2700229582160658
Loss in iteration 75 : 1.6085380955975743
Loss in iteration 76 : 1.2765355915734184
Loss in iteration 77 : 1.7455301710029463
Loss in iteration 78 : 1.3083208788890037
Loss in iteration 79 : 1.7600473214820977
Loss in iteration 80 : 1.2734496847720667
Loss in iteration 81 : 1.6198023829937735
Loss in iteration 82 : 1.2746785150547792
Loss in iteration 83 : 1.733658316183991
Loss in iteration 84 : 1.3115222169073952
Loss in iteration 85 : 1.773619559784884
Loss in iteration 86 : 1.2742282512239618
Loss in iteration 87 : 1.607248039866599
Loss in iteration 88 : 1.2711504357320362
Loss in iteration 89 : 1.737097057942326
Loss in iteration 90 : 1.3128879778087201
Loss in iteration 91 : 1.7732229277776124
Loss in iteration 92 : 1.273268344459891
Loss in iteration 93 : 1.6062962518813797
Loss in iteration 94 : 1.278753467601333
Loss in iteration 95 : 1.7542196517327922
Loss in iteration 96 : 1.307500774009436
Loss in iteration 97 : 1.7423307070538212
Loss in iteration 98 : 1.2766160347391813
Loss in iteration 99 : 1.635941113549063
Loss in iteration 100 : 1.2690856874648933
Loss in iteration 101 : 1.7113195009082518
Loss in iteration 102 : 1.3202013914564599
Loss in iteration 103 : 1.8131095738219971
Loss in iteration 104 : 1.266092557572456
Loss in iteration 105 : 1.573726279683563
Loss in iteration 106 : 1.2731149916697861
Loss in iteration 107 : 1.756468694762674
Loss in iteration 108 : 1.3176585535690857
Loss in iteration 109 : 1.763277423712462
Loss in iteration 110 : 1.2722299184293933
Loss in iteration 111 : 1.6036087002548263
Loss in iteration 112 : 1.2840710362485206
Loss in iteration 113 : 1.7611250706288264
Loss in iteration 114 : 1.3054405834200737
Loss in iteration 115 : 1.7278984136091307
Loss in iteration 116 : 1.270796107256301
Loss in iteration 117 : 1.6283420756229878
Loss in iteration 118 : 1.2798250745017354
Loss in iteration 119 : 1.7543914689665259
Loss in iteration 120 : 1.3116484519522684
Loss in iteration 121 : 1.7492016831219621
Loss in iteration 122 : 1.2780738896511668
Loss in iteration 123 : 1.627473819096427
Loss in iteration 124 : 1.2712734770941616
Loss in iteration 125 : 1.7163346089979314
Loss in iteration 126 : 1.3188693170948589
Loss in iteration 127 : 1.7990571054228248
Loss in iteration 128 : 1.2688843619056647
Loss in iteration 129 : 1.5813691215835437
Loss in iteration 130 : 1.271746276984392
Loss in iteration 131 : 1.7404739230929733
Loss in iteration 132 : 1.3199752530061364
Loss in iteration 133 : 1.7803163980572607
Loss in iteration 134 : 1.2745562789080584
Loss in iteration 135 : 1.6003539101374187
Loss in iteration 136 : 1.2796550534896105
Loss in iteration 137 : 1.739836436585508
Loss in iteration 138 : 1.3097949618947493
Loss in iteration 139 : 1.7530076164054982
Loss in iteration 140 : 1.2792663266074915
Loss in iteration 141 : 1.639072511593736
Loss in iteration 142 : 1.2669685532866755
Loss in iteration 143 : 1.7010707418440105
Loss in iteration 144 : 1.319264004282121
Loss in iteration 145 : 1.8126652864362849
Loss in iteration 146 : 1.2704461819287762
Loss in iteration 147 : 1.5826007323255462
Loss in iteration 148 : 1.2710037165919563
Loss in iteration 149 : 1.7337915969326547
Loss in iteration 150 : 1.3160474883789222
Loss in iteration 151 : 1.783844296037869
Loss in iteration 152 : 1.2778707872159887
Loss in iteration 153 : 1.606344876912364
Loss in iteration 154 : 1.2806421039420552
Loss in iteration 155 : 1.7337920609102764
Loss in iteration 156 : 1.308186413160647
Loss in iteration 157 : 1.7464873720017746
Loss in iteration 158 : 1.282424145308147
Loss in iteration 159 : 1.649598003291521
Loss in iteration 160 : 1.263100988019849
Loss in iteration 161 : 1.6818011428178101
Loss in iteration 162 : 1.317746143396092
Loss in iteration 163 : 1.821466257073268
Loss in iteration 164 : 1.2756149293384862
Loss in iteration 165 : 1.5884561307725762
Loss in iteration 166 : 1.2708146341323738
Loss in iteration 167 : 1.7310843602712678
Loss in iteration 168 : 1.316706942312974
Loss in iteration 169 : 1.7884630233949605
Loss in iteration 170 : 1.2776003633846154
Loss in iteration 171 : 1.5998442009422804
Loss in iteration 172 : 1.2759267942815204
Loss in iteration 173 : 1.7254812475733046
Loss in iteration 174 : 1.3142931303455276
Loss in iteration 175 : 1.775294118277226
Loss in iteration 176 : 1.2715912845339323
Loss in iteration 177 : 1.5977438561053368
Loss in iteration 178 : 1.284553685675541
Loss in iteration 179 : 1.7667057538320328
Loss in iteration 180 : 1.3049430530329307
Loss in iteration 181 : 1.7151789939542494
Loss in iteration 182 : 1.274588253153864
Loss in iteration 183 : 1.6516735331890329
Loss in iteration 184 : 1.2796611232638353
Loss in iteration 185 : 1.7322967834467087
Loss in iteration 186 : 1.3113339367235406
Loss in iteration 187 : 1.7652078248115046
Loss in iteration 188 : 1.271109898269951
Loss in iteration 189 : 1.5946535955916323
Loss in iteration 190 : 1.2789912992654457
Loss in iteration 191 : 1.7582853886708096
Loss in iteration 192 : 1.3137485116595506
Loss in iteration 193 : 1.7481244207645839
Loss in iteration 194 : 1.2770332683960597
Loss in iteration 195 : 1.6357806101218517
Loss in iteration 196 : 1.2677691339678563
Loss in iteration 197 : 1.6971014642972664
Loss in iteration 198 : 1.3139384976786712
Loss in iteration 199 : 1.8085018719585968
Loss in iteration 200 : 1.2748246174783928
Testing accuracy  of updater 5 on alg 1 with rate 1.0 = 0.8193599901725939, training accuracy 0.8164312039312039, time elapsed: 3617 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7792904647546864
Loss in iteration 3 : 0.4307216849791589
Loss in iteration 4 : 0.4198282416211767
Loss in iteration 5 : 0.4140945147030017
Loss in iteration 6 : 0.4037270418789778
Loss in iteration 7 : 0.40673198512401326
Loss in iteration 8 : 0.392391123089628
Loss in iteration 9 : 0.396846232695768
Loss in iteration 10 : 0.3846980417989099
Loss in iteration 11 : 0.39048755902911325
Loss in iteration 12 : 0.38242752619642445
Loss in iteration 13 : 0.38993046161524597
Loss in iteration 14 : 0.382434171582933
Loss in iteration 15 : 0.39369132442010724
Loss in iteration 16 : 0.3860243495113036
Loss in iteration 17 : 0.39977094543559955
Loss in iteration 18 : 0.3888556221564111
Loss in iteration 19 : 0.40360319259313937
Loss in iteration 20 : 0.3887672929144377
Loss in iteration 21 : 0.4011595213200444
Loss in iteration 22 : 0.3885141429505342
Loss in iteration 23 : 0.40088980594308277
Loss in iteration 24 : 0.3886489623602078
Loss in iteration 25 : 0.40211093745925236
Loss in iteration 26 : 0.3903880854909984
Loss in iteration 27 : 0.4051390852109619
Loss in iteration 28 : 0.39135302573498665
Loss in iteration 29 : 0.4048740173154714
Loss in iteration 30 : 0.39118408720153436
Loss in iteration 31 : 0.404325686467275
Loss in iteration 32 : 0.3911074729178935
Loss in iteration 33 : 0.40482491208900206
Loss in iteration 34 : 0.3919951507851448
Loss in iteration 35 : 0.4060008598730732
Loss in iteration 36 : 0.392936651409968
Loss in iteration 37 : 0.40760181269316964
Loss in iteration 38 : 0.39335594060867674
Loss in iteration 39 : 0.40732089004961675
Loss in iteration 40 : 0.3924424547398343
Loss in iteration 41 : 0.40509143847675233
Loss in iteration 42 : 0.3924509703415675
Loss in iteration 43 : 0.4054668502912217
Loss in iteration 44 : 0.3941104778126505
Loss in iteration 45 : 0.4094153854994683
Loss in iteration 46 : 0.39454300953467836
Loss in iteration 47 : 0.4080582829403705
Loss in iteration 48 : 0.3938900905443601
Loss in iteration 49 : 0.40689863055154896
Loss in iteration 50 : 0.3938818555822682
Loss in iteration 51 : 0.40710968678189313
Loss in iteration 52 : 0.39431521920017687
Loss in iteration 53 : 0.40804195510968994
Loss in iteration 54 : 0.3946635241336708
Loss in iteration 55 : 0.40845162407656843
Loss in iteration 56 : 0.3949684878628919
Loss in iteration 57 : 0.40868642394946225
Loss in iteration 58 : 0.39475284603088195
Loss in iteration 59 : 0.40766942082987595
Loss in iteration 60 : 0.3944537130490515
Loss in iteration 61 : 0.407324624058917
Loss in iteration 62 : 0.39497753348830034
Loss in iteration 63 : 0.40925016346786053
Loss in iteration 64 : 0.39580225417220744
Loss in iteration 65 : 0.4096035371014235
Loss in iteration 66 : 0.3955104586943936
Loss in iteration 67 : 0.40794887411650793
Loss in iteration 68 : 0.394693433115665
Loss in iteration 69 : 0.4070013725472522
Loss in iteration 70 : 0.3950346754756379
Loss in iteration 71 : 0.40884527876132226
Loss in iteration 72 : 0.3956166403065267
Loss in iteration 73 : 0.4089994671690146
Loss in iteration 74 : 0.395378411236622
Loss in iteration 75 : 0.40812934828706304
Loss in iteration 76 : 0.3954347557504523
Loss in iteration 77 : 0.40820532150200195
Loss in iteration 78 : 0.39561137301196797
Loss in iteration 79 : 0.4090135229357982
Loss in iteration 80 : 0.3955820815276759
Loss in iteration 81 : 0.4080424115072416
Loss in iteration 82 : 0.3952350001805818
Loss in iteration 83 : 0.407632569780167
Loss in iteration 84 : 0.3953356292774682
Loss in iteration 85 : 0.40891297735727816
Loss in iteration 86 : 0.3960843519443279
Loss in iteration 87 : 0.40990822180096537
Loss in iteration 88 : 0.396195380524949
Loss in iteration 89 : 0.4093010725719738
Loss in iteration 90 : 0.3952067729383917
Loss in iteration 91 : 0.40720494567423976
Loss in iteration 92 : 0.39494062854485446
Loss in iteration 93 : 0.40756999713371506
Loss in iteration 94 : 0.3955563505220502
Loss in iteration 95 : 0.40968265929732556
Loss in iteration 96 : 0.39619650589224814
Loss in iteration 97 : 0.40960269727611376
Loss in iteration 98 : 0.39530510416370995
Loss in iteration 99 : 0.407239843001498
Loss in iteration 100 : 0.3950502565278465
Loss in iteration 101 : 0.40786088068919374
Loss in iteration 102 : 0.3957458962684715
Loss in iteration 103 : 0.41017900137236457
Loss in iteration 104 : 0.3959487756855851
Loss in iteration 105 : 0.4088208376416227
Loss in iteration 106 : 0.3950186021713702
Loss in iteration 107 : 0.4067810110539125
Loss in iteration 108 : 0.39502619517724663
Loss in iteration 109 : 0.4079493469452995
Loss in iteration 110 : 0.39582425035877167
Loss in iteration 111 : 0.41008052410000057
Loss in iteration 112 : 0.3958770823421526
Loss in iteration 113 : 0.4088584748797201
Loss in iteration 114 : 0.3950395150310575
Loss in iteration 115 : 0.40727130221515384
Loss in iteration 116 : 0.39528800192558255
Loss in iteration 117 : 0.40917601059844344
Loss in iteration 118 : 0.39573725356359707
Loss in iteration 119 : 0.4094239623426129
Loss in iteration 120 : 0.3954529985757954
Loss in iteration 121 : 0.40786177831429776
Loss in iteration 122 : 0.39501476574878513
Loss in iteration 123 : 0.40761364634145847
Loss in iteration 124 : 0.3954452293867577
Loss in iteration 125 : 0.40939142726121885
Loss in iteration 126 : 0.39571778751191405
Loss in iteration 127 : 0.40945200827735173
Loss in iteration 128 : 0.3950547056279188
Loss in iteration 129 : 0.40668481740390505
Loss in iteration 130 : 0.3949336599571128
Loss in iteration 131 : 0.40826813827492675
Loss in iteration 132 : 0.3955764160329916
Loss in iteration 133 : 0.4098139225271948
Loss in iteration 134 : 0.39587626506205115
Loss in iteration 135 : 0.40947279714616436
Loss in iteration 136 : 0.3947587706248993
Loss in iteration 137 : 0.40598591040444443
Loss in iteration 138 : 0.3946212082356414
Loss in iteration 139 : 0.40821288383594084
Loss in iteration 140 : 0.3958264355835083
Loss in iteration 141 : 0.410600113055548
Loss in iteration 142 : 0.39581010037779985
Loss in iteration 143 : 0.40864993338591127
Loss in iteration 144 : 0.3949027613262436
Loss in iteration 145 : 0.40650817075339263
Loss in iteration 146 : 0.3949843240161991
Loss in iteration 147 : 0.4090039545370208
Loss in iteration 148 : 0.3956060207051482
Loss in iteration 149 : 0.4099530126231345
Loss in iteration 150 : 0.3954135714218959
Loss in iteration 151 : 0.40803373194161086
Loss in iteration 152 : 0.39491992287721384
Loss in iteration 153 : 0.40704837685612
Loss in iteration 154 : 0.3954545174244355
Loss in iteration 155 : 0.4100463805014216
Loss in iteration 156 : 0.39554561859399856
Loss in iteration 157 : 0.4086174169491851
Loss in iteration 158 : 0.39498869546412224
Loss in iteration 159 : 0.40670094443936067
Loss in iteration 160 : 0.3950645902732471
Loss in iteration 161 : 0.409245636665226
Loss in iteration 162 : 0.39560537523046696
Loss in iteration 163 : 0.40932080197964993
Loss in iteration 164 : 0.3948531041807245
Loss in iteration 165 : 0.4067784894586979
Loss in iteration 166 : 0.39518340948206476
Loss in iteration 167 : 0.4095388206616069
Loss in iteration 168 : 0.3956330028574553
Loss in iteration 169 : 0.4091639197145055
Loss in iteration 170 : 0.39477027249128477
Loss in iteration 171 : 0.4066524057029443
Loss in iteration 172 : 0.39506938395420704
Loss in iteration 173 : 0.4091709829293208
Loss in iteration 174 : 0.39551029370272406
Loss in iteration 175 : 0.4092900443372443
Loss in iteration 176 : 0.3949975750080608
Loss in iteration 177 : 0.407490147159954
Loss in iteration 178 : 0.39507045458942913
Loss in iteration 179 : 0.4089318980310271
Loss in iteration 180 : 0.3952837214531272
Loss in iteration 181 : 0.4094149416157691
Loss in iteration 182 : 0.39515270882230086
Loss in iteration 183 : 0.40749285263699253
Loss in iteration 184 : 0.39490437283001206
Loss in iteration 185 : 0.4077888469718313
Loss in iteration 186 : 0.3952630326385438
Loss in iteration 187 : 0.40945153825578223
Loss in iteration 188 : 0.39544299142502365
Loss in iteration 189 : 0.4089319521067024
Loss in iteration 190 : 0.3948142258358632
Loss in iteration 191 : 0.40651096102075196
Loss in iteration 192 : 0.39440338080256715
Loss in iteration 193 : 0.4077517024490606
Loss in iteration 194 : 0.39573244433874155
Loss in iteration 195 : 0.41058873812697083
Loss in iteration 196 : 0.3958364570710216
Loss in iteration 197 : 0.40870938773889703
Loss in iteration 198 : 0.39476620011498603
Loss in iteration 199 : 0.4067033372510151
Loss in iteration 200 : 0.39482070586102974
Testing accuracy  of updater 5 on alg 1 with rate 0.1 = 0.8374792703150912, training accuracy 0.8358108108108108, time elapsed: 3926 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8049090371279479
Loss in iteration 3 : 0.6537995045110541
Loss in iteration 4 : 0.5238243241128702
Loss in iteration 5 : 0.4657924568846009
Loss in iteration 6 : 0.4634373947582882
Loss in iteration 7 : 0.4612699664308146
Loss in iteration 8 : 0.45917520545211754
Loss in iteration 9 : 0.45710495236169146
Loss in iteration 10 : 0.45503327938677235
Loss in iteration 11 : 0.4529512275025393
Loss in iteration 12 : 0.4508399595372018
Loss in iteration 13 : 0.44869647902612153
Loss in iteration 14 : 0.44650377448965883
Loss in iteration 15 : 0.4442688166148239
Loss in iteration 16 : 0.44198603993092606
Loss in iteration 17 : 0.4396520902572011
Loss in iteration 18 : 0.43726489609909114
Loss in iteration 19 : 0.4348232503389841
Loss in iteration 20 : 0.4323284353714924
Loss in iteration 21 : 0.42978126730146027
Loss in iteration 22 : 0.42717785736284386
Loss in iteration 23 : 0.42452073152963027
Loss in iteration 24 : 0.42180488066389993
Loss in iteration 25 : 0.41903397687555916
Loss in iteration 26 : 0.4162088076524154
Loss in iteration 27 : 0.41334704118173027
Loss in iteration 28 : 0.4105151506955088
Loss in iteration 29 : 0.4077069505316232
Loss in iteration 30 : 0.4048908867510116
Loss in iteration 31 : 0.40209549412733264
Loss in iteration 32 : 0.399333289345821
Loss in iteration 33 : 0.39666317199860956
Loss in iteration 34 : 0.3941234175573645
Loss in iteration 35 : 0.3917091097536249
Loss in iteration 36 : 0.38945022981631466
Loss in iteration 37 : 0.3873239032537778
Loss in iteration 38 : 0.3853044870930008
Loss in iteration 39 : 0.3834030963042804
Loss in iteration 40 : 0.3816038217448691
Loss in iteration 41 : 0.379881925223016
Loss in iteration 42 : 0.3782453035425266
Loss in iteration 43 : 0.37672241842381154
Loss in iteration 44 : 0.3752980126362612
Loss in iteration 45 : 0.37397038673132454
Loss in iteration 46 : 0.3727259422540999
Loss in iteration 47 : 0.3715528269051389
Loss in iteration 48 : 0.3704299425753913
Loss in iteration 49 : 0.36937785626207154
Loss in iteration 50 : 0.36839990918299725
Loss in iteration 51 : 0.3674773443263162
Loss in iteration 52 : 0.3666182326592311
Loss in iteration 53 : 0.3658183616330752
Loss in iteration 54 : 0.3650640316940556
Loss in iteration 55 : 0.3643477488211921
Loss in iteration 56 : 0.3636624310442166
Loss in iteration 57 : 0.36301287123653814
Loss in iteration 58 : 0.3624006546110511
Loss in iteration 59 : 0.3618431906080978
Loss in iteration 60 : 0.3613590021147114
Loss in iteration 61 : 0.36099981864071684
Loss in iteration 62 : 0.36102160020055246
Loss in iteration 63 : 0.3610938638556983
Loss in iteration 64 : 0.36121732327485795
Loss in iteration 65 : 0.360185179487331
Loss in iteration 66 : 0.35974612620408775
Loss in iteration 67 : 0.35885959090100233
Loss in iteration 68 : 0.3584821613899104
Loss in iteration 69 : 0.3578670568091075
Loss in iteration 70 : 0.35767224716637275
Loss in iteration 71 : 0.3572537083383034
Loss in iteration 72 : 0.3569681206645514
Loss in iteration 73 : 0.35669326268597734
Loss in iteration 74 : 0.35654947950946114
Loss in iteration 75 : 0.35635210780429977
Loss in iteration 76 : 0.3564040104737923
Loss in iteration 77 : 0.35630780485921015
Loss in iteration 78 : 0.35643335061055986
Loss in iteration 79 : 0.356203808770732
Loss in iteration 80 : 0.3561693430469718
Loss in iteration 81 : 0.3556825151577814
Loss in iteration 82 : 0.35537762493223995
Loss in iteration 83 : 0.35496933334255154
Loss in iteration 84 : 0.35486758399537954
Loss in iteration 85 : 0.3546304566567286
Loss in iteration 86 : 0.35456673475767314
Loss in iteration 87 : 0.3544509516396002
Loss in iteration 88 : 0.3544280915445296
Loss in iteration 89 : 0.35431171884182466
Loss in iteration 90 : 0.35432716642961715
Loss in iteration 91 : 0.3542622909795123
Loss in iteration 92 : 0.35424045417752176
Loss in iteration 93 : 0.3540595490225968
Loss in iteration 94 : 0.35397656169839786
Loss in iteration 95 : 0.3538218129352495
Loss in iteration 96 : 0.3537794884651235
Loss in iteration 97 : 0.3535981682265994
Loss in iteration 98 : 0.35352441617820507
Loss in iteration 99 : 0.3533552645147766
Loss in iteration 100 : 0.3532794574054816
Loss in iteration 101 : 0.3531746023696452
Loss in iteration 102 : 0.35322829770557845
Loss in iteration 103 : 0.3532471201835789
Loss in iteration 104 : 0.35347175348827525
Loss in iteration 105 : 0.3533247766966623
Loss in iteration 106 : 0.35352601602847405
Loss in iteration 107 : 0.3531263389550529
Loss in iteration 108 : 0.35313409923378036
Loss in iteration 109 : 0.3527982616280894
Loss in iteration 110 : 0.35279891555587967
Loss in iteration 111 : 0.35262466976059537
Loss in iteration 112 : 0.3526929962713806
Loss in iteration 113 : 0.3525997468747616
Loss in iteration 114 : 0.35276589969013883
Loss in iteration 115 : 0.35277707446291073
Loss in iteration 116 : 0.35304701594150106
Loss in iteration 117 : 0.3528766966306186
Loss in iteration 118 : 0.3530966009567989
Loss in iteration 119 : 0.3527977627845978
Loss in iteration 120 : 0.3528568100650622
Loss in iteration 121 : 0.35248800564874866
Loss in iteration 122 : 0.35243868794302496
Loss in iteration 123 : 0.3522664669985643
Loss in iteration 124 : 0.35229316377941655
Loss in iteration 125 : 0.35227524348852934
Loss in iteration 126 : 0.3524457742202949
Loss in iteration 127 : 0.3524822322911447
Loss in iteration 128 : 0.3527581727639675
Loss in iteration 129 : 0.3527804132781248
Loss in iteration 130 : 0.3530164490986701
Loss in iteration 131 : 0.35272436973944915
Loss in iteration 132 : 0.3527228762518615
Loss in iteration 133 : 0.352400634727502
Loss in iteration 134 : 0.352369246351335
Loss in iteration 135 : 0.35213730817423466
Loss in iteration 136 : 0.35215972556635616
Loss in iteration 137 : 0.3520669121052613
Loss in iteration 138 : 0.352163589381428
Loss in iteration 139 : 0.3521690651952225
Loss in iteration 140 : 0.3524003011940924
Loss in iteration 141 : 0.3525291260672065
Loss in iteration 142 : 0.3529110781158157
Loss in iteration 143 : 0.3528006800480847
Loss in iteration 144 : 0.3528458452946306
Loss in iteration 145 : 0.3523869478546917
Loss in iteration 146 : 0.352376407684858
Loss in iteration 147 : 0.3520868493013629
Loss in iteration 148 : 0.3520651346847481
Loss in iteration 149 : 0.3518884124299399
Loss in iteration 150 : 0.3519052460769476
Loss in iteration 151 : 0.35185798957583175
Loss in iteration 152 : 0.3520253708806581
Loss in iteration 153 : 0.3521888654459418
Loss in iteration 154 : 0.35273072488764945
Loss in iteration 155 : 0.35282008225874295
Loss in iteration 156 : 0.35301484219486234
Loss in iteration 157 : 0.3525304328335145
Loss in iteration 158 : 0.35248149784483435
Loss in iteration 159 : 0.35208838973753803
Loss in iteration 160 : 0.35205961202174685
Loss in iteration 161 : 0.35187875826756626
Loss in iteration 162 : 0.35194988356003293
Loss in iteration 163 : 0.35184811497452406
Loss in iteration 164 : 0.35199529416742353
Loss in iteration 165 : 0.35209467867665567
Loss in iteration 166 : 0.35239022893860245
Loss in iteration 167 : 0.3524532197287673
Loss in iteration 168 : 0.3527723067155581
Loss in iteration 169 : 0.35245410030021057
Loss in iteration 170 : 0.3525190155286634
Loss in iteration 171 : 0.3521611059769258
Loss in iteration 172 : 0.3521940951504642
Loss in iteration 173 : 0.35192836519227433
Loss in iteration 174 : 0.3518992961475326
Loss in iteration 175 : 0.35177062542724086
Loss in iteration 176 : 0.35187961457872896
Loss in iteration 177 : 0.35191055926880527
Loss in iteration 178 : 0.3521323971629749
Loss in iteration 179 : 0.35223001849766133
Loss in iteration 180 : 0.35264529378017884
Loss in iteration 181 : 0.3525270920880494
Loss in iteration 182 : 0.3527438177209929
Loss in iteration 183 : 0.3522511436171777
Loss in iteration 184 : 0.35224873891784575
Loss in iteration 185 : 0.351926033340185
Loss in iteration 186 : 0.3518823642746548
Loss in iteration 187 : 0.35171389588901864
Loss in iteration 188 : 0.3517478793364321
Loss in iteration 189 : 0.3516953155111566
Loss in iteration 190 : 0.35183157479842003
Loss in iteration 191 : 0.3520032638055026
Loss in iteration 192 : 0.35245025942497155
Loss in iteration 193 : 0.3526624659522456
Loss in iteration 194 : 0.35288777888404843
Loss in iteration 195 : 0.3523269440614738
Loss in iteration 196 : 0.35234465641241086
Loss in iteration 197 : 0.35195276687738625
Loss in iteration 198 : 0.3519261124823976
Loss in iteration 199 : 0.3517165614405174
Loss in iteration 200 : 0.35176736117317303
Testing accuracy  of updater 5 on alg 1 with rate 0.009999999999999995 = 0.8487193661323015, training accuracy 0.8496621621621622, time elapsed: 2665 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.7991563988378765
Loss in iteration 3 : 4.911638239607042
Loss in iteration 4 : 3.4013901839673886
Loss in iteration 5 : 1.734611529625915
Loss in iteration 6 : 1.5537453716454859
Loss in iteration 7 : 2.6110811422002738
Loss in iteration 8 : 3.0652336935215776
Loss in iteration 9 : 2.6077221552666625
Loss in iteration 10 : 2.0046222317381988
Loss in iteration 11 : 1.8365235842977923
Loss in iteration 12 : 1.9993459007581105
Loss in iteration 13 : 2.244900258073459
Loss in iteration 14 : 2.3696130132177413
Loss in iteration 15 : 2.282645214869475
Loss in iteration 16 : 2.023192032084982
Loss in iteration 17 : 1.7410926146024723
Loss in iteration 18 : 1.6091483430189963
Loss in iteration 19 : 1.704889097717567
Loss in iteration 20 : 1.8546197259446093
Loss in iteration 21 : 1.8232311934420284
Loss in iteration 22 : 1.5994132485891817
Loss in iteration 23 : 1.3883950851586824
Loss in iteration 24 : 1.3979213616424437
Loss in iteration 25 : 1.5102243550872398
Loss in iteration 26 : 1.4744621146171433
Loss in iteration 27 : 1.2553693665831038
Loss in iteration 28 : 1.0985997558954914
Loss in iteration 29 : 1.1787719320895176
Loss in iteration 30 : 1.1938130897809005
Loss in iteration 31 : 0.988853798613895
Loss in iteration 32 : 0.9031014813472031
Loss in iteration 33 : 1.0019363040986342
Loss in iteration 34 : 0.933551732566694
Loss in iteration 35 : 0.7709772289839169
Loss in iteration 36 : 0.8702109236863327
Loss in iteration 37 : 0.7646895878454228
Loss in iteration 38 : 0.681018802831187
Loss in iteration 39 : 0.744748560169892
Loss in iteration 40 : 0.5997873061966291
Loss in iteration 41 : 0.6536518760582595
Loss in iteration 42 : 0.5859690463882884
Loss in iteration 43 : 0.541980778218604
Loss in iteration 44 : 0.5855919569820032
Loss in iteration 45 : 0.4751986344513698
Loss in iteration 46 : 0.5212596712569406
Loss in iteration 47 : 0.4653071908801597
Loss in iteration 48 : 0.44408352268402285
Loss in iteration 49 : 0.4974776839175836
Loss in iteration 50 : 0.4988899966853378
Loss in iteration 51 : 0.4669710143575869
Loss in iteration 52 : 0.44775104719522424
Loss in iteration 53 : 0.4558983055659572
Loss in iteration 54 : 0.4422255183807363
Loss in iteration 55 : 0.41376756178253504
Loss in iteration 56 : 0.41740854971297553
Loss in iteration 57 : 0.44518334813474775
Loss in iteration 58 : 0.48847749699431076
Loss in iteration 59 : 0.5295242460418109
Loss in iteration 60 : 0.4269444704919101
Loss in iteration 61 : 0.42861641919357235
Loss in iteration 62 : 0.4612892933260936
Loss in iteration 63 : 0.40615035167455105
Loss in iteration 64 : 0.4218061601316877
Loss in iteration 65 : 0.4589772167420396
Loss in iteration 66 : 0.40524263914781355
Loss in iteration 67 : 0.38320066836260985
Loss in iteration 68 : 0.4102461359528358
Loss in iteration 69 : 0.46606504703781937
Loss in iteration 70 : 0.5829685516581374
Loss in iteration 71 : 0.3966155496840654
Loss in iteration 72 : 0.440322916388745
Loss in iteration 73 : 0.5752092677303555
Loss in iteration 74 : 0.3850404905465878
Loss in iteration 75 : 0.5588334887478132
Loss in iteration 76 : 0.6418361180670854
Loss in iteration 77 : 0.4874089657402113
Loss in iteration 78 : 0.7360827908185609
Loss in iteration 79 : 0.42721626734906815
Loss in iteration 80 : 0.6124356274271531
Loss in iteration 81 : 0.44968578331047465
Loss in iteration 82 : 0.5974533920640889
Loss in iteration 83 : 0.43660509058624164
Loss in iteration 84 : 0.5609882808520683
Loss in iteration 85 : 0.41667243976941615
Loss in iteration 86 : 0.6059036474246019
Loss in iteration 87 : 0.46535786612240926
Loss in iteration 88 : 0.4478204796308891
Loss in iteration 89 : 0.5989133846680497
Loss in iteration 90 : 0.4472940549366001
Loss in iteration 91 : 0.4337535526261124
Loss in iteration 92 : 0.5337647329135811
Loss in iteration 93 : 0.3960690584052172
Loss in iteration 94 : 0.457775019688179
Loss in iteration 95 : 0.46245795775007653
Loss in iteration 96 : 0.378116519287342
Loss in iteration 97 : 0.3897394120223482
Loss in iteration 98 : 0.4724512569147778
Loss in iteration 99 : 0.5872755158651973
Loss in iteration 100 : 0.3686581415852928
Loss in iteration 101 : 0.5922673903029604
Loss in iteration 102 : 0.6224302445400705
Loss in iteration 103 : 0.5190026406169953
Loss in iteration 104 : 0.6137333310025677
Loss in iteration 105 : 0.47567925153031393
Loss in iteration 106 : 0.5652516931250181
Loss in iteration 107 : 0.5266967552617007
Loss in iteration 108 : 0.4989876011935472
Loss in iteration 109 : 0.49836998010843325
Loss in iteration 110 : 0.492600182381491
Loss in iteration 111 : 0.4598941754551438
Loss in iteration 112 : 0.5351668519730512
Loss in iteration 113 : 0.3941299700102796
Loss in iteration 114 : 0.4371354785687774
Loss in iteration 115 : 0.49851555672331754
Loss in iteration 116 : 0.44308604273994956
Loss in iteration 117 : 0.3704871469629917
Loss in iteration 118 : 0.4425423787481652
Loss in iteration 119 : 0.5053896842917173
Loss in iteration 120 : 0.3712431928504368
Loss in iteration 121 : 0.4229932699810669
Loss in iteration 122 : 0.5523558336375485
Loss in iteration 123 : 0.3698935541106167
Loss in iteration 124 : 0.5709126796728948
Loss in iteration 125 : 0.6382937896942249
Loss in iteration 126 : 0.5333525392984649
Loss in iteration 127 : 0.6198472884879404
Loss in iteration 128 : 0.46738934151867106
Loss in iteration 129 : 0.5682485860462677
Loss in iteration 130 : 0.5272965307132957
Loss in iteration 131 : 0.4957563567175424
Loss in iteration 132 : 0.511613024457298
Loss in iteration 133 : 0.4833446074399941
Loss in iteration 134 : 0.47666098053997136
Loss in iteration 135 : 0.49806743157895805
Loss in iteration 136 : 0.3824540319002788
Loss in iteration 137 : 0.47406185602917583
Loss in iteration 138 : 0.4242237191894993
Loss in iteration 139 : 0.36804322615707324
Loss in iteration 140 : 0.3703505387157933
Loss in iteration 141 : 0.45111969358040166
Loss in iteration 142 : 0.6415074243788085
Loss in iteration 143 : 0.36817606935910424
Loss in iteration 144 : 0.6172427254005302
Loss in iteration 145 : 0.6369552055228896
Loss in iteration 146 : 0.5840798889562928
Loss in iteration 147 : 0.5397130044574809
Loss in iteration 148 : 0.5852986407808757
Loss in iteration 149 : 0.5049899225586264
Loss in iteration 150 : 0.6049684144744057
Loss in iteration 151 : 0.45446761806183306
Loss in iteration 152 : 0.5860238609235501
Loss in iteration 153 : 0.4295842958870207
Loss in iteration 154 : 0.545412863525395
Loss in iteration 155 : 0.40952060915924693
Loss in iteration 156 : 0.6036014898302835
Loss in iteration 157 : 0.5212619300744645
Loss in iteration 158 : 0.44762103878968434
Loss in iteration 159 : 0.6230506888999481
Loss in iteration 160 : 0.4112888040355848
Loss in iteration 161 : 0.5266931846630027
Loss in iteration 162 : 0.41246917485730333
Loss in iteration 163 : 0.4627541244132257
Loss in iteration 164 : 0.4698385686752101
Loss in iteration 165 : 0.38765703863171647
Loss in iteration 166 : 0.5821744121240991
Loss in iteration 167 : 0.6659336667674894
Loss in iteration 168 : 0.5322693565567771
Loss in iteration 169 : 0.6745184138915931
Loss in iteration 170 : 0.46527020544480646
Loss in iteration 171 : 0.5875442370056728
Loss in iteration 172 : 0.5504646081540284
Loss in iteration 173 : 0.48669303348508536
Loss in iteration 174 : 0.5641635556562892
Loss in iteration 175 : 0.4494532570824885
Loss in iteration 176 : 0.5340089502548088
Loss in iteration 177 : 0.4152605714862903
Loss in iteration 178 : 0.4957424022964716
Loss in iteration 179 : 0.5235888917168822
Loss in iteration 180 : 0.3969355485722355
Loss in iteration 181 : 0.6606754480368833
Loss in iteration 182 : 0.5546503076856143
Loss in iteration 183 : 0.5137471007683094
Loss in iteration 184 : 0.5657434431374775
Loss in iteration 185 : 0.4620453423003902
Loss in iteration 186 : 0.5370337214078519
Loss in iteration 187 : 0.46876879059193377
Loss in iteration 188 : 0.5211269723651669
Loss in iteration 189 : 0.4129924557834581
Loss in iteration 190 : 0.5206558778861075
Loss in iteration 191 : 0.38853679939600844
Loss in iteration 192 : 0.5745544475421879
Loss in iteration 193 : 0.5631345367039658
Loss in iteration 194 : 0.4709166907692846
Loss in iteration 195 : 0.6493355005164145
Loss in iteration 196 : 0.4096593709512425
Loss in iteration 197 : 0.5600074992674985
Loss in iteration 198 : 0.41604053595988566
Loss in iteration 199 : 0.556273113456793
Loss in iteration 200 : 0.40322585325108135
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.8286346047540077, training accuracy 0.8257985257985258, time elapsed: 2691 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4632541529226789
Loss in iteration 3 : 0.591925152119685
Loss in iteration 4 : 0.6169165038719274
Loss in iteration 5 : 0.5386928987399067
Loss in iteration 6 : 0.4116135114635868
Loss in iteration 7 : 0.3769715160300016
Loss in iteration 8 : 0.4423729104943723
Loss in iteration 9 : 0.4857933921686258
Loss in iteration 10 : 0.46518572312427064
Loss in iteration 11 : 0.4180789716032422
Loss in iteration 12 : 0.3925136789466224
Loss in iteration 13 : 0.40259018620306053
Loss in iteration 14 : 0.4270057991432807
Loss in iteration 15 : 0.44193232148394057
Loss in iteration 16 : 0.4376164234342105
Loss in iteration 17 : 0.41907356538137236
Loss in iteration 18 : 0.4004274099345951
Loss in iteration 19 : 0.394689481317706
Loss in iteration 20 : 0.4030571621947605
Loss in iteration 21 : 0.4138896426975581
Loss in iteration 22 : 0.4151832360330889
Loss in iteration 23 : 0.40520724537432257
Loss in iteration 24 : 0.39212867598573425
Loss in iteration 25 : 0.3864710464089529
Loss in iteration 26 : 0.38928625921130056
Loss in iteration 27 : 0.3943683334310307
Loss in iteration 28 : 0.3934124545775233
Loss in iteration 29 : 0.38506357986204526
Loss in iteration 30 : 0.37618961005124874
Loss in iteration 31 : 0.3731211057516918
Loss in iteration 32 : 0.3756306044374906
Loss in iteration 33 : 0.3770264799072053
Loss in iteration 34 : 0.372626723777114
Loss in iteration 35 : 0.36567638318825385
Loss in iteration 36 : 0.3626668262521241
Loss in iteration 37 : 0.3642254766744766
Loss in iteration 38 : 0.36548993341187846
Loss in iteration 39 : 0.36272554036001414
Loss in iteration 40 : 0.35859958446063667
Loss in iteration 41 : 0.3575876765138027
Loss in iteration 42 : 0.35968785413871235
Loss in iteration 43 : 0.35975054243921256
Loss in iteration 44 : 0.3568689372239454
Loss in iteration 45 : 0.35539535393622196
Loss in iteration 46 : 0.3568367885132618
Loss in iteration 47 : 0.35740868206251253
Loss in iteration 48 : 0.3556527187322962
Loss in iteration 49 : 0.3542451290813675
Loss in iteration 50 : 0.3550482952857676
Loss in iteration 51 : 0.3556216356868771
Loss in iteration 52 : 0.3544091791055543
Loss in iteration 53 : 0.3534968334063745
Loss in iteration 54 : 0.3540839564259059
Loss in iteration 55 : 0.35439444319326224
Loss in iteration 56 : 0.3536475876728628
Loss in iteration 57 : 0.35291789968250936
Loss in iteration 58 : 0.3532382659485892
Loss in iteration 59 : 0.3533748938996133
Loss in iteration 60 : 0.352743187636231
Loss in iteration 61 : 0.3523612759118975
Loss in iteration 62 : 0.35260837448956345
Loss in iteration 63 : 0.35264688857373105
Loss in iteration 64 : 0.35223323233608533
Loss in iteration 65 : 0.3520340745304624
Loss in iteration 66 : 0.3522040109350548
Loss in iteration 67 : 0.3521246592520345
Loss in iteration 68 : 0.351807948492083
Loss in iteration 69 : 0.3517561936205693
Loss in iteration 70 : 0.35181702353430294
Loss in iteration 71 : 0.3517433993666051
Loss in iteration 72 : 0.3515519830260121
Loss in iteration 73 : 0.35146838424380855
Loss in iteration 74 : 0.35153589670528473
Loss in iteration 75 : 0.3514945595429728
Loss in iteration 76 : 0.35136839424767646
Loss in iteration 77 : 0.3513767212242805
Loss in iteration 78 : 0.3514197251431687
Loss in iteration 79 : 0.3513417736935554
Loss in iteration 80 : 0.3512853084068795
Loss in iteration 81 : 0.35131986335254267
Loss in iteration 82 : 0.35128560946377746
Loss in iteration 83 : 0.35123464346466027
Loss in iteration 84 : 0.35125973301286345
Loss in iteration 85 : 0.3512657398404356
Loss in iteration 86 : 0.35122647934933154
Loss in iteration 87 : 0.3512181798848535
Loss in iteration 88 : 0.3512408989904223
Loss in iteration 89 : 0.35120243640553045
Loss in iteration 90 : 0.3511904512991927
Loss in iteration 91 : 0.3512045823618217
Loss in iteration 92 : 0.3511722273302104
Loss in iteration 93 : 0.3511703879006996
Loss in iteration 94 : 0.3511792662399871
Loss in iteration 95 : 0.35115028412706284
Loss in iteration 96 : 0.3511514038131012
Loss in iteration 97 : 0.3511539748331632
Loss in iteration 98 : 0.3511324640363175
Loss in iteration 99 : 0.3511218003572141
Loss in iteration 100 : 0.3511267407261807
Loss in iteration 101 : 0.35110822434934036
Loss in iteration 102 : 0.3511057736047785
Loss in iteration 103 : 0.3511106564531515
Loss in iteration 104 : 0.3510923741696163
Loss in iteration 105 : 0.3510900013591094
Loss in iteration 106 : 0.351088426910076
Loss in iteration 107 : 0.3510755578774917
Loss in iteration 108 : 0.35108117263554034
Loss in iteration 109 : 0.35107115547360135
Loss in iteration 110 : 0.35106260438757286
Loss in iteration 111 : 0.3510716379696232
Loss in iteration 112 : 0.3510537203827339
Loss in iteration 113 : 0.3510607223921308
Loss in iteration 114 : 0.3510516596287116
Loss in iteration 115 : 0.3510442147867424
Loss in iteration 116 : 0.35104467564564296
Loss in iteration 117 : 0.35103743155030637
Loss in iteration 118 : 0.3510428502997663
Loss in iteration 119 : 0.3510326225823658
Loss in iteration 120 : 0.351033439183416
Loss in iteration 121 : 0.3510300073528719
Loss in iteration 122 : 0.35102116765818175
Loss in iteration 123 : 0.35102697682683215
Loss in iteration 124 : 0.35101772000815284
Loss in iteration 125 : 0.35102303012219466
Loss in iteration 126 : 0.3510193531813975
Loss in iteration 127 : 0.35101274123646903
Loss in iteration 128 : 0.3510120289389211
Loss in iteration 129 : 0.3510072387417772
Loss in iteration 130 : 0.3510043881170772
Loss in iteration 131 : 0.3510022617318241
Loss in iteration 132 : 0.3510000659738124
Loss in iteration 133 : 0.3509979682668774
Loss in iteration 134 : 0.3509951422148953
Loss in iteration 135 : 0.3509940231604025
Loss in iteration 136 : 0.35099128170273636
Loss in iteration 137 : 0.35099010718549273
Loss in iteration 138 : 0.35098927756380766
Loss in iteration 139 : 0.3509868173759874
Loss in iteration 140 : 0.3509849207416138
Loss in iteration 141 : 0.3509830844167303
Loss in iteration 142 : 0.3509817220742198
Loss in iteration 143 : 0.35097934471541326
Loss in iteration 144 : 0.3509807719096116
Loss in iteration 145 : 0.3509765284885739
Loss in iteration 146 : 0.3509749894477028
Loss in iteration 147 : 0.35097497331622957
Loss in iteration 148 : 0.35097196273469305
Loss in iteration 149 : 0.35097104262271167
Loss in iteration 150 : 0.3509683847269194
Loss in iteration 151 : 0.3509669837446228
Loss in iteration 152 : 0.3509655267676356
Loss in iteration 153 : 0.3509651445626804
Loss in iteration 154 : 0.35096904512497495
Loss in iteration 155 : 0.35096841978406523
Loss in iteration 156 : 0.35096668228216843
Loss in iteration 157 : 0.3509611690184278
Loss in iteration 158 : 0.35096395584568046
Loss in iteration 159 : 0.3509592904135638
Loss in iteration 160 : 0.3509608016322419
Loss in iteration 161 : 0.3509573001367712
Loss in iteration 162 : 0.35095815167100197
Loss in iteration 163 : 0.35095318295943645
Loss in iteration 164 : 0.35095604181355766
Loss in iteration 165 : 0.35095198765837804
Loss in iteration 166 : 0.3509510361464984
Loss in iteration 167 : 0.3509512649942017
Loss in iteration 168 : 0.350948246108493
Loss in iteration 169 : 0.35094708868008295
Loss in iteration 170 : 0.35094682853577475
Loss in iteration 171 : 0.35094556230322127
Loss in iteration 172 : 0.35094600481045873
Loss in iteration 173 : 0.35095336364814056
Loss in iteration 174 : 0.35094505360106454
Loss in iteration 175 : 0.3509531371979048
Loss in iteration 176 : 0.35094491765680896
Loss in iteration 177 : 0.35094798721156806
Loss in iteration 178 : 0.3509423521411771
Loss in iteration 179 : 0.3509488746092944
Loss in iteration 180 : 0.35094666173911493
Loss in iteration 181 : 0.3509472657901466
Loss in iteration 182 : 0.3509423874909706
Loss in iteration 183 : 0.3509463053853463
Loss in iteration 184 : 0.35093978941404225
Loss in iteration 185 : 0.3509454645520063
Loss in iteration 186 : 0.35093760352435704
Loss in iteration 187 : 0.3509391554046688
Loss in iteration 188 : 0.3509352498095124
Loss in iteration 189 : 0.3509390920120131
Loss in iteration 190 : 0.3509327600320744
Loss in iteration 191 : 0.3509384155940643
Loss in iteration 192 : 0.35093125616476734
Loss in iteration 193 : 0.35094049414278655
Loss in iteration 194 : 0.35093069179621833
Loss in iteration 195 : 0.35094214711720406
Loss in iteration 196 : 0.35093469525287396
Loss in iteration 197 : 0.3509378449165895
Loss in iteration 198 : 0.3509293678515966
Loss in iteration 199 : 0.3509389895775892
Loss in iteration 200 : 0.35093887207579433
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.8495792641729624, training accuracy 0.8501228501228502, time elapsed: 2610 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9101261837490733
Loss in iteration 3 : 0.7522089177136414
Loss in iteration 4 : 0.5426750437802262
Loss in iteration 5 : 0.4612185885487434
Loss in iteration 6 : 0.5027572223110787
Loss in iteration 7 : 0.5417862361786931
Loss in iteration 8 : 0.5645505813189249
Loss in iteration 9 : 0.5715317940558429
Loss in iteration 10 : 0.5647357136792437
Loss in iteration 11 : 0.5465818277338619
Loss in iteration 12 : 0.5196988846000813
Loss in iteration 13 : 0.4867777729183392
Loss in iteration 14 : 0.45119363110076954
Loss in iteration 15 : 0.41773205691352877
Loss in iteration 16 : 0.3920668254280562
Loss in iteration 17 : 0.37825186595472793
Loss in iteration 18 : 0.3759803058201969
Loss in iteration 19 : 0.38205631881509755
Loss in iteration 20 : 0.39155618248739615
Loss in iteration 21 : 0.40025943155461097
Loss in iteration 22 : 0.4054029046973677
Loss in iteration 23 : 0.4058324111985047
Loss in iteration 24 : 0.40177484332083585
Loss in iteration 25 : 0.39435125109643676
Loss in iteration 26 : 0.38520306562138995
Loss in iteration 27 : 0.37630984052557204
Loss in iteration 28 : 0.368949453738781
Loss in iteration 29 : 0.36370430310322527
Loss in iteration 30 : 0.36120462545773974
Loss in iteration 31 : 0.3611443445413952
Loss in iteration 32 : 0.36241637340534116
Loss in iteration 33 : 0.36431256189432115
Loss in iteration 34 : 0.3660716839517672
Loss in iteration 35 : 0.36713726681268627
Loss in iteration 36 : 0.3672450502570619
Loss in iteration 37 : 0.36638744795553874
Loss in iteration 38 : 0.36473814871562804
Loss in iteration 39 : 0.36261062186710435
Loss in iteration 40 : 0.3603922452327285
Loss in iteration 41 : 0.3584038973781049
Loss in iteration 42 : 0.35691129231612273
Loss in iteration 43 : 0.35594828696010533
Loss in iteration 44 : 0.35554081511782926
Loss in iteration 45 : 0.35555598335206967
Loss in iteration 46 : 0.3559266752382473
Loss in iteration 47 : 0.35639309137754366
Loss in iteration 48 : 0.35673891033040167
Loss in iteration 49 : 0.35683639830630104
Loss in iteration 50 : 0.35665111546534217
Loss in iteration 51 : 0.3562242719257339
Loss in iteration 52 : 0.3556519221701664
Loss in iteration 53 : 0.35502890748496385
Loss in iteration 54 : 0.3544772235280575
Loss in iteration 55 : 0.3540462836445617
Loss in iteration 56 : 0.35378324745733614
Loss in iteration 57 : 0.3537272229061567
Loss in iteration 58 : 0.3537868720815922
Loss in iteration 59 : 0.3538841432250789
Loss in iteration 60 : 0.3539759306329882
Loss in iteration 61 : 0.3540243599002241
Loss in iteration 62 : 0.35400848762494175
Loss in iteration 63 : 0.3539204313490176
Loss in iteration 64 : 0.35377249371853065
Loss in iteration 65 : 0.3535871217956496
Loss in iteration 66 : 0.35339768004861655
Loss in iteration 67 : 0.35323599856673216
Loss in iteration 68 : 0.35311464156657213
Loss in iteration 69 : 0.3530459787076194
Loss in iteration 70 : 0.3530220458763096
Loss in iteration 71 : 0.35303097826003493
Loss in iteration 72 : 0.353042116958187
Loss in iteration 73 : 0.35304301693078155
Loss in iteration 74 : 0.35302524394887086
Loss in iteration 75 : 0.35298392741716245
Loss in iteration 76 : 0.3529237710447798
Loss in iteration 77 : 0.3528542215173651
Loss in iteration 78 : 0.3527798707185752
Loss in iteration 79 : 0.35271556823877637
Loss in iteration 80 : 0.352670367490982
Loss in iteration 81 : 0.3526373530847277
Loss in iteration 82 : 0.35262372282940163
Loss in iteration 83 : 0.3526175347209578
Loss in iteration 84 : 0.3526090491631495
Loss in iteration 85 : 0.35259608586950253
Loss in iteration 86 : 0.35257790554262697
Loss in iteration 87 : 0.35255353829743324
Loss in iteration 88 : 0.35252456831974716
Loss in iteration 89 : 0.35249332036590936
Loss in iteration 90 : 0.3524612363281144
Loss in iteration 91 : 0.35243574433328695
Loss in iteration 92 : 0.3524189166391114
Loss in iteration 93 : 0.35240472128568806
Loss in iteration 94 : 0.3523925541224058
Loss in iteration 95 : 0.352382843806358
Loss in iteration 96 : 0.3523718685612266
Loss in iteration 97 : 0.3523591477473972
Loss in iteration 98 : 0.3523446864277181
Loss in iteration 99 : 0.35232898789021905
Loss in iteration 100 : 0.3523131772447987
Loss in iteration 101 : 0.35229882787271966
Loss in iteration 102 : 0.3522860081028817
Loss in iteration 103 : 0.35227439179306247
Loss in iteration 104 : 0.35226433497689635
Loss in iteration 105 : 0.3522552338084413
Loss in iteration 106 : 0.3522465114533261
Loss in iteration 107 : 0.3522378810921461
Loss in iteration 108 : 0.35222919448556206
Loss in iteration 109 : 0.35222040507574504
Loss in iteration 110 : 0.3522112840983915
Loss in iteration 111 : 0.3522021628414732
Loss in iteration 112 : 0.3521933225505201
Loss in iteration 113 : 0.35218487195364234
Loss in iteration 114 : 0.35217688581508955
Loss in iteration 115 : 0.35216920144843095
Loss in iteration 116 : 0.35216181750518166
Loss in iteration 117 : 0.3521544911975943
Loss in iteration 118 : 0.35214723122569047
Loss in iteration 119 : 0.35214026153677364
Loss in iteration 120 : 0.3521333815399956
Loss in iteration 121 : 0.3521265588856049
Loss in iteration 122 : 0.3521197767234186
Loss in iteration 123 : 0.35211305244336294
Loss in iteration 124 : 0.35210632884518506
Loss in iteration 125 : 0.3520997702243334
Loss in iteration 126 : 0.3520935796079513
Loss in iteration 127 : 0.35208738241392856
Loss in iteration 128 : 0.35208137154096397
Loss in iteration 129 : 0.3520754916568026
Loss in iteration 130 : 0.3520696248623907
Loss in iteration 131 : 0.35206363464004975
Loss in iteration 132 : 0.3520575921342275
Loss in iteration 133 : 0.35205174985862436
Loss in iteration 134 : 0.35204595694711494
Loss in iteration 135 : 0.35204032100857363
Loss in iteration 136 : 0.3520349530729512
Loss in iteration 137 : 0.35202953988426333
Loss in iteration 138 : 0.35202415457252345
Loss in iteration 139 : 0.35201881854229766
Loss in iteration 140 : 0.35201353769802673
Loss in iteration 141 : 0.352008411265836
Loss in iteration 142 : 0.35200327660762043
Loss in iteration 143 : 0.3519981443590781
Loss in iteration 144 : 0.35199297048450384
Loss in iteration 145 : 0.3519878020804746
Loss in iteration 146 : 0.3519827464927786
Loss in iteration 147 : 0.3519778052672743
Loss in iteration 148 : 0.35197283001362106
Loss in iteration 149 : 0.3519678653332579
Loss in iteration 150 : 0.35196307249176695
Loss in iteration 151 : 0.3519583260192596
Loss in iteration 152 : 0.3519536791473309
Loss in iteration 153 : 0.3519490494663078
Loss in iteration 154 : 0.3519446755024973
Loss in iteration 155 : 0.3519400524517151
Loss in iteration 156 : 0.3519355476925195
Loss in iteration 157 : 0.35193127952858466
Loss in iteration 158 : 0.3519269571464216
Loss in iteration 159 : 0.35192264679609014
Loss in iteration 160 : 0.35191837185204566
Loss in iteration 161 : 0.35191416186298435
Loss in iteration 162 : 0.3519098849444346
Loss in iteration 163 : 0.35190566049731925
Loss in iteration 164 : 0.3519015162563013
Loss in iteration 165 : 0.35189724863008703
Loss in iteration 166 : 0.35189311836007303
Loss in iteration 167 : 0.3518890153820705
Loss in iteration 168 : 0.35188495279340215
Loss in iteration 169 : 0.3518809206038344
Loss in iteration 170 : 0.35187693395571085
Loss in iteration 171 : 0.35187296487080144
Loss in iteration 172 : 0.35186899848063896
Loss in iteration 173 : 0.3518650330532717
Loss in iteration 174 : 0.3518610931284984
Loss in iteration 175 : 0.35185721519883023
Loss in iteration 176 : 0.35185333917450884
Loss in iteration 177 : 0.3518495310660403
Loss in iteration 178 : 0.3518457202513918
Loss in iteration 179 : 0.35184201872395865
Loss in iteration 180 : 0.3518382664107254
Loss in iteration 181 : 0.35183436756573544
Loss in iteration 182 : 0.3518307661986569
Loss in iteration 183 : 0.35182719755067215
Loss in iteration 184 : 0.3518236138953435
Loss in iteration 185 : 0.35182000741971475
Loss in iteration 186 : 0.35181653521956513
Loss in iteration 187 : 0.3518130116175878
Loss in iteration 188 : 0.3518094847041848
Loss in iteration 189 : 0.3518060371974784
Loss in iteration 190 : 0.3518025554051618
Loss in iteration 191 : 0.3517990892906456
Loss in iteration 192 : 0.3517956608361515
Loss in iteration 193 : 0.3517922729691521
Loss in iteration 194 : 0.35178884980377256
Loss in iteration 195 : 0.35178545458487454
Loss in iteration 196 : 0.3517820565151945
Loss in iteration 197 : 0.35177873525955
Loss in iteration 198 : 0.3517754054296966
Loss in iteration 199 : 0.35177231889813504
Loss in iteration 200 : 0.35176893102170864
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.8499477919046742, training accuracy 0.8489557739557739, time elapsed: 2803 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 32.4812152463483
Loss in iteration 3 : 33.37488805071153
Loss in iteration 4 : 21.918073609270586
Loss in iteration 5 : 10.193805231373178
Loss in iteration 6 : 9.498024519271613
Loss in iteration 7 : 15.461121573323167
Loss in iteration 8 : 18.811157956244777
Loss in iteration 9 : 16.924342559551736
Loss in iteration 10 : 13.00678877506765
Loss in iteration 11 : 10.643236582843521
Loss in iteration 12 : 10.44948096570787
Loss in iteration 13 : 11.37183884780058
Loss in iteration 14 : 12.452386339628122
Loss in iteration 15 : 12.896746618856474
Loss in iteration 16 : 12.367007949039492
Loss in iteration 17 : 11.086543529518961
Loss in iteration 18 : 9.663998732723467
Loss in iteration 19 : 8.723105592899298
Loss in iteration 20 : 8.6189599431216
Loss in iteration 21 : 9.163624716185252
Loss in iteration 22 : 9.631344852554628
Loss in iteration 23 : 9.45739912664189
Loss in iteration 24 : 8.622398185692067
Loss in iteration 25 : 7.6429293964228435
Loss in iteration 26 : 7.1230876708056625
Loss in iteration 27 : 7.231859221966977
Loss in iteration 28 : 7.5163927446362955
Loss in iteration 29 : 7.405607829554511
Loss in iteration 30 : 6.743986142493588
Loss in iteration 31 : 5.961611198438764
Loss in iteration 32 : 5.662649643258578
Loss in iteration 33 : 5.787156569008326
Loss in iteration 34 : 5.767728021601233
Loss in iteration 35 : 5.3340538267494875
Loss in iteration 36 : 4.70208135271139
Loss in iteration 37 : 4.427785026849585
Loss in iteration 38 : 4.524748410028436
Loss in iteration 39 : 4.408507770332351
Loss in iteration 40 : 3.9351620980125244
Loss in iteration 41 : 3.540884528287817
Loss in iteration 42 : 3.604041602897866
Loss in iteration 43 : 3.5370108867413053
Loss in iteration 44 : 3.0484067467791
Loss in iteration 45 : 2.9074848105559865
Loss in iteration 46 : 3.0191185950602746
Loss in iteration 47 : 2.6050513677934326
Loss in iteration 48 : 2.6094313337868447
Loss in iteration 49 : 2.564537652864695
Loss in iteration 50 : 2.364237750882776
Loss in iteration 51 : 2.2799263162444383
Loss in iteration 52 : 2.212929758721187
Loss in iteration 53 : 2.065690246055784
Loss in iteration 54 : 2.0517968584388986
Loss in iteration 55 : 1.9627586077169379
Loss in iteration 56 : 1.8494948394425121
Loss in iteration 57 : 1.6355004565938887
Loss in iteration 58 : 1.7309102556662748
Loss in iteration 59 : 1.4312821350375478
Loss in iteration 60 : 1.491377187425164
Loss in iteration 61 : 1.5357654949873114
Loss in iteration 62 : 1.2103509600888305
Loss in iteration 63 : 1.2634328562697452
Loss in iteration 64 : 1.2681466776756292
Loss in iteration 65 : 1.0975214102565072
Loss in iteration 66 : 1.206533777342208
Loss in iteration 67 : 1.14171370378927
Loss in iteration 68 : 0.9712818841833416
Loss in iteration 69 : 0.8536801711642547
Loss in iteration 70 : 0.8140983901084681
Loss in iteration 71 : 0.9944321774631932
Loss in iteration 72 : 2.6764445233065666
Loss in iteration 73 : 1.8519833586959078
Loss in iteration 74 : 1.3434183751486402
Loss in iteration 75 : 1.6801925862759055
Loss in iteration 76 : 1.3482595925912026
Loss in iteration 77 : 1.4500982810495904
Loss in iteration 78 : 1.4394988126548596
Loss in iteration 79 : 1.316937126626725
Loss in iteration 80 : 1.3827935494760748
Loss in iteration 81 : 1.2685837617906028
Loss in iteration 82 : 1.1222085496340224
Loss in iteration 83 : 1.2492158539202842
Loss in iteration 84 : 0.9086412959820117
Loss in iteration 85 : 1.0796295337090733
Loss in iteration 86 : 0.9977314973758945
Loss in iteration 87 : 0.9581527577010915
Loss in iteration 88 : 1.6272725255717668
Loss in iteration 89 : 2.2942589364009525
Loss in iteration 90 : 1.1028947641311817
Loss in iteration 91 : 2.682098572746902
Loss in iteration 92 : 1.2796611345391302
Loss in iteration 93 : 1.9422167417573124
Loss in iteration 94 : 1.9787030550611073
Loss in iteration 95 : 1.3101008345282903
Loss in iteration 96 : 2.0353603828335856
Loss in iteration 97 : 1.439001514463468
Loss in iteration 98 : 1.4976854262763735
Loss in iteration 99 : 1.7231659769666652
Loss in iteration 100 : 1.138306016706374
Loss in iteration 101 : 1.634081558279248
Loss in iteration 102 : 1.0368241834457985
Loss in iteration 103 : 1.5659079167732588
Loss in iteration 104 : 0.9448276848863407
Loss in iteration 105 : 1.7618713358810036
Loss in iteration 106 : 0.8795739431211834
Loss in iteration 107 : 1.2810582590550696
Loss in iteration 108 : 1.3771855572364489
Loss in iteration 109 : 0.7731023478477413
Loss in iteration 110 : 1.1020051839022547
Loss in iteration 111 : 1.6601627227226314
Loss in iteration 112 : 0.781261636016605
Loss in iteration 113 : 1.1219567262146581
Loss in iteration 114 : 1.2614665220506083
Loss in iteration 115 : 0.7752165251442773
Loss in iteration 116 : 1.0114092189814137
Loss in iteration 117 : 0.9251090979253745
Loss in iteration 118 : 0.6832829695338153
Loss in iteration 119 : 0.8176855917922788
Loss in iteration 120 : 1.418889388366804
Loss in iteration 121 : 1.9887696798936358
Loss in iteration 122 : 0.8625567695762728
Loss in iteration 123 : 2.5476851319706815
Loss in iteration 124 : 0.8751320296436421
Loss in iteration 125 : 1.7800148648753475
Loss in iteration 126 : 1.0301441471314767
Loss in iteration 127 : 1.6228006426127928
Loss in iteration 128 : 1.137145286203322
Loss in iteration 129 : 1.4029158289859218
Loss in iteration 130 : 1.184061735927572
Loss in iteration 131 : 1.227953741514011
Loss in iteration 132 : 1.0483243607068418
Loss in iteration 133 : 1.2592156787352768
Loss in iteration 134 : 0.8999982654278555
Loss in iteration 135 : 1.4957399024265092
Loss in iteration 136 : 0.8155138850104723
Loss in iteration 137 : 0.778083019991845
Loss in iteration 138 : 1.7305051677941974
Loss in iteration 139 : 1.4078265301951396
Loss in iteration 140 : 0.709875993543332
Loss in iteration 141 : 1.2787733532365206
Loss in iteration 142 : 0.9927980062554586
Loss in iteration 143 : 1.001921824317119
Loss in iteration 144 : 1.0400457579766194
Loss in iteration 145 : 0.7072827278353425
Loss in iteration 146 : 1.0642573396837696
Loss in iteration 147 : 0.8494189391619582
Loss in iteration 148 : 0.6507733352799469
Loss in iteration 149 : 0.5728016614869147
Loss in iteration 150 : 0.6343600062747933
Loss in iteration 151 : 1.9180315567212802
Loss in iteration 152 : 3.4105727842756006
Loss in iteration 153 : 3.568821110134838
Loss in iteration 154 : 1.1176194316582784
Loss in iteration 155 : 3.5299151223486773
Loss in iteration 156 : 1.9319706172884459
Loss in iteration 157 : 1.8800285601985498
Loss in iteration 158 : 2.892481733499775
Loss in iteration 159 : 2.1299768469384905
Loss in iteration 160 : 1.6225427760673474
Loss in iteration 161 : 2.5299070968192168
Loss in iteration 162 : 2.006436542966778
Loss in iteration 163 : 1.5661864431645456
Loss in iteration 164 : 2.2064455644538303
Loss in iteration 165 : 1.766102575752126
Loss in iteration 166 : 1.3896002667106957
Loss in iteration 167 : 1.9063544312980754
Loss in iteration 168 : 1.1822789982997868
Loss in iteration 169 : 1.5103592137251973
Loss in iteration 170 : 1.313130895809357
Loss in iteration 171 : 1.1990271261385013
Loss in iteration 172 : 1.015621334449309
Loss in iteration 173 : 1.5761225656514255
Loss in iteration 174 : 0.7079867790709808
Loss in iteration 175 : 1.6882655528589547
Loss in iteration 176 : 1.5619364171723162
Loss in iteration 177 : 0.8472728726715133
Loss in iteration 178 : 1.8081975281388536
Loss in iteration 179 : 0.8132687945779977
Loss in iteration 180 : 1.422518126881415
Loss in iteration 181 : 0.8213711339794622
Loss in iteration 182 : 1.377473223828816
Loss in iteration 183 : 0.848691890489124
Loss in iteration 184 : 1.1158872693145603
Loss in iteration 185 : 0.8932654375100251
Loss in iteration 186 : 0.71712807471048
Loss in iteration 187 : 1.0966773260477611
Loss in iteration 188 : 0.8060370103747225
Loss in iteration 189 : 0.662735866903307
Loss in iteration 190 : 0.7228807296165839
Loss in iteration 191 : 1.662865028422192
Loss in iteration 192 : 0.712124464919348
Loss in iteration 193 : 0.6659898433606118
Loss in iteration 194 : 0.6138528924248686
Loss in iteration 195 : 0.5726844385118477
Loss in iteration 196 : 0.7940427455094428
Loss in iteration 197 : 1.9126161114907152
Loss in iteration 198 : 0.5648027704443358
Loss in iteration 199 : 2.490363048771947
Loss in iteration 200 : 2.6325638967318827
Testing accuracy  of updater 7 on alg 1 with rate 20.0 = 0.7798046803021927, training accuracy 0.7765663390663391, time elapsed: 2730 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7303972882228208
Loss in iteration 3 : 0.7896938328211652
Loss in iteration 4 : 0.5639016414360386
Loss in iteration 5 : 0.4920784950148083
Loss in iteration 6 : 0.6366640914635265
Loss in iteration 7 : 0.7239356071626829
Loss in iteration 8 : 0.6650417600910887
Loss in iteration 9 : 0.6030571179557775
Loss in iteration 10 : 0.6098663115793094
Loss in iteration 11 : 0.6511913835411786
Loss in iteration 12 : 0.6764928111912725
Loss in iteration 13 : 0.6581812660111162
Loss in iteration 14 : 0.6126385310363381
Loss in iteration 15 : 0.5842883757304196
Loss in iteration 16 : 0.600471192059663
Loss in iteration 17 : 0.6243445152530126
Loss in iteration 18 : 0.610079442810994
Loss in iteration 19 : 0.569747033306809
Loss in iteration 20 : 0.5529209699942635
Loss in iteration 21 : 0.5664947349451519
Loss in iteration 22 : 0.5627000726319558
Loss in iteration 23 : 0.5242011054505813
Loss in iteration 24 : 0.4954795630845719
Loss in iteration 25 : 0.49685837118140724
Loss in iteration 26 : 0.49308342625492413
Loss in iteration 27 : 0.46497810822463115
Loss in iteration 28 : 0.44249740244670815
Loss in iteration 29 : 0.4455627431244497
Loss in iteration 30 : 0.443258047306905
Loss in iteration 31 : 0.42159774906713277
Loss in iteration 32 : 0.4148039811762897
Loss in iteration 33 : 0.4200888967350474
Loss in iteration 34 : 0.40282555993704583
Loss in iteration 35 : 0.40150875092138155
Loss in iteration 36 : 0.4081671279032027
Loss in iteration 37 : 0.39140866445821093
Loss in iteration 38 : 0.39645729056721507
Loss in iteration 39 : 0.38839305184323303
Loss in iteration 40 : 0.3797652210493178
Loss in iteration 41 : 0.383467180938148
Loss in iteration 42 : 0.3744225150168155
Loss in iteration 43 : 0.3789014362303299
Loss in iteration 44 : 0.37164645311129185
Loss in iteration 45 : 0.36881671795681764
Loss in iteration 46 : 0.3683230006016573
Loss in iteration 47 : 0.36318735890055254
Loss in iteration 48 : 0.366441367315212
Loss in iteration 49 : 0.36158241357344617
Loss in iteration 50 : 0.36315522324188554
Loss in iteration 51 : 0.36156104028464353
Loss in iteration 52 : 0.3609950246984709
Loss in iteration 53 : 0.36166959977520796
Loss in iteration 54 : 0.35889880337054947
Loss in iteration 55 : 0.3597955632450847
Loss in iteration 56 : 0.35778372079700455
Loss in iteration 57 : 0.35875541149917434
Loss in iteration 58 : 0.35810267687169195
Loss in iteration 59 : 0.3576869940435883
Loss in iteration 60 : 0.3572545983564253
Loss in iteration 61 : 0.3563961573432849
Loss in iteration 62 : 0.35642950352911673
Loss in iteration 63 : 0.3553568039651412
Loss in iteration 64 : 0.3552511876100308
Loss in iteration 65 : 0.35406123702874076
Loss in iteration 66 : 0.35440587045058014
Loss in iteration 67 : 0.3535890607285548
Loss in iteration 68 : 0.3537255654861256
Loss in iteration 69 : 0.3530000153307251
Loss in iteration 70 : 0.353131694746595
Loss in iteration 71 : 0.3528143468227305
Loss in iteration 72 : 0.3527798178104171
Loss in iteration 73 : 0.35244482473560423
Loss in iteration 74 : 0.35236107976095415
Loss in iteration 75 : 0.35221176430556717
Loss in iteration 76 : 0.35205484931908937
Loss in iteration 77 : 0.35179900193446767
Loss in iteration 78 : 0.3517450981954071
Loss in iteration 79 : 0.3517252586692323
Loss in iteration 80 : 0.3517358667148065
Loss in iteration 81 : 0.3516869567994867
Loss in iteration 82 : 0.35163459575063843
Loss in iteration 83 : 0.3515738717190631
Loss in iteration 84 : 0.35145717980427077
Loss in iteration 85 : 0.3513736384131592
Loss in iteration 86 : 0.3513037892646757
Loss in iteration 87 : 0.3512635284608474
Loss in iteration 88 : 0.3512324476076824
Loss in iteration 89 : 0.3511584262793103
Loss in iteration 90 : 0.35111472652006703
Loss in iteration 91 : 0.35114429682472714
Loss in iteration 92 : 0.3511193912289379
Loss in iteration 93 : 0.3510957617404456
Loss in iteration 94 : 0.3510805417113669
Loss in iteration 95 : 0.35105908876729336
Loss in iteration 96 : 0.3510409028861609
Loss in iteration 97 : 0.3510304775981463
Loss in iteration 98 : 0.35100712554222707
Loss in iteration 99 : 0.351025572143946
Loss in iteration 100 : 0.3510105659128566
Loss in iteration 101 : 0.35100040023501666
Loss in iteration 102 : 0.35101193338036296
Loss in iteration 103 : 0.3509846097273649
Loss in iteration 104 : 0.350979298021315
Loss in iteration 105 : 0.3509783741828717
Loss in iteration 106 : 0.3509885274811455
Loss in iteration 107 : 0.3509820576307642
Loss in iteration 108 : 0.3509566537342298
Loss in iteration 109 : 0.3509712565961382
Loss in iteration 110 : 0.3509520899359847
Loss in iteration 111 : 0.3509709766001339
Loss in iteration 112 : 0.3510211438554566
Loss in iteration 113 : 0.3509552154462981
Loss in iteration 114 : 0.3509743436991323
Loss in iteration 115 : 0.35098912737684856
Loss in iteration 116 : 0.3509238306203806
Loss in iteration 117 : 0.3509642254824868
Loss in iteration 118 : 0.3509946526774587
Loss in iteration 119 : 0.3509206211076591
Loss in iteration 120 : 0.35091564488764826
Loss in iteration 121 : 0.3509638109991764
Loss in iteration 122 : 0.35095673130821503
Loss in iteration 123 : 0.3509038997515256
Loss in iteration 124 : 0.35094502113577514
Loss in iteration 125 : 0.35095490363387577
Loss in iteration 126 : 0.35088866314824263
Loss in iteration 127 : 0.3509511973904296
Loss in iteration 128 : 0.35096559308595754
Loss in iteration 129 : 0.35090206187433026
Loss in iteration 130 : 0.35102313711137256
Loss in iteration 131 : 0.35096574515514445
Loss in iteration 132 : 0.35091044317506087
Loss in iteration 133 : 0.3510383099859416
Loss in iteration 134 : 0.3509070980886052
Loss in iteration 135 : 0.35094291595223354
Loss in iteration 136 : 0.35098741222649643
Loss in iteration 137 : 0.3508922479618397
Loss in iteration 138 : 0.35095643181858227
Loss in iteration 139 : 0.3509200320117561
Loss in iteration 140 : 0.3508954672426125
Loss in iteration 141 : 0.35099315990136865
Loss in iteration 142 : 0.3508946940026587
Loss in iteration 143 : 0.35093301506694274
Loss in iteration 144 : 0.35106087142591497
Loss in iteration 145 : 0.35090514787739124
Loss in iteration 146 : 0.35116826475432933
Loss in iteration 147 : 0.35094247511468846
Loss in iteration 148 : 0.3509901825589756
Loss in iteration 149 : 0.3510785551782045
Loss in iteration 150 : 0.350894570752799
Loss in iteration 151 : 0.3510191176943824
Loss in iteration 152 : 0.35089323067365863
Loss in iteration 153 : 0.3509801576266684
Loss in iteration 154 : 0.3509948142353961
Loss in iteration 155 : 0.3508862740776644
Loss in iteration 156 : 0.3510817330244186
Loss in iteration 157 : 0.3508903191387825
Loss in iteration 158 : 0.3509348152066364
Loss in iteration 159 : 0.35101019872245454
Loss in iteration 160 : 0.35087216072616445
Loss in iteration 161 : 0.35097208672072394
Loss in iteration 162 : 0.3509351544753838
Loss in iteration 163 : 0.350877704890928
Loss in iteration 164 : 0.3509508114483907
Loss in iteration 165 : 0.35087996513081177
Loss in iteration 166 : 0.3508672261403983
Loss in iteration 167 : 0.35090774679961106
Loss in iteration 168 : 0.35086042070485024
Loss in iteration 169 : 0.3508708428131933
Loss in iteration 170 : 0.35088188380416946
Loss in iteration 171 : 0.3508556425918922
Loss in iteration 172 : 0.35088500824357827
Loss in iteration 173 : 0.3510432676376433
Loss in iteration 174 : 0.35089090764396785
Loss in iteration 175 : 0.3510016036608403
Loss in iteration 176 : 0.3511328116370325
Loss in iteration 177 : 0.35088707550712817
Loss in iteration 178 : 0.35124456407003607
Loss in iteration 179 : 0.350928262032657
Loss in iteration 180 : 0.35102747390152866
Loss in iteration 181 : 0.35101903482497343
Loss in iteration 182 : 0.35089799102374863
Loss in iteration 183 : 0.35099500097015573
Loss in iteration 184 : 0.35088346713343027
Loss in iteration 185 : 0.35094893613606953
Loss in iteration 186 : 0.3509964777680667
Loss in iteration 187 : 0.3508610321519189
Loss in iteration 188 : 0.351011493553739
Loss in iteration 189 : 0.351028302136076
Loss in iteration 190 : 0.35088062300055184
Loss in iteration 191 : 0.3511322030752039
Loss in iteration 192 : 0.35095356657940646
Loss in iteration 193 : 0.35093208056226916
Loss in iteration 194 : 0.3510418940544412
Loss in iteration 195 : 0.350870063216694
Loss in iteration 196 : 0.3509873248592928
Loss in iteration 197 : 0.3509264845421116
Loss in iteration 198 : 0.35089108875107805
Loss in iteration 199 : 0.3509953461047267
Loss in iteration 200 : 0.3508520609993182
Testing accuracy  of updater 7 on alg 1 with rate 2.0 = 0.8489036299981574, training accuracy 0.8496314496314497, time elapsed: 2867 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8599767268743608
Loss in iteration 3 : 0.5934470568694854
Loss in iteration 4 : 0.5162427669836809
Loss in iteration 5 : 0.594815410585755
Loss in iteration 6 : 0.6257611287324408
Loss in iteration 7 : 0.6128699648043825
Loss in iteration 8 : 0.5643718091874955
Loss in iteration 9 : 0.49299596939617657
Loss in iteration 10 : 0.4195233658197442
Loss in iteration 11 : 0.3792990218660801
Loss in iteration 12 : 0.3890907099443363
Loss in iteration 13 : 0.4229329913803822
Loss in iteration 14 : 0.450971031714431
Loss in iteration 15 : 0.4592858263277879
Loss in iteration 16 : 0.44681617427416825
Loss in iteration 17 : 0.4218821645692032
Loss in iteration 18 : 0.39682773107432573
Loss in iteration 19 : 0.38148405585065387
Loss in iteration 20 : 0.3786046312682483
Loss in iteration 21 : 0.38445884407004743
Loss in iteration 22 : 0.39338796185709446
Loss in iteration 23 : 0.40001856705730093
Loss in iteration 24 : 0.4016773561790847
Loss in iteration 25 : 0.39785819912980036
Loss in iteration 26 : 0.3901455173748435
Loss in iteration 27 : 0.3813993366874687
Loss in iteration 28 : 0.37444195032618954
Loss in iteration 29 : 0.37112592457881827
Loss in iteration 30 : 0.37160717673134785
Loss in iteration 31 : 0.37446369833400717
Loss in iteration 32 : 0.3776012409293003
Loss in iteration 33 : 0.37894998835856075
Loss in iteration 34 : 0.377810350018935
Loss in iteration 35 : 0.37458225878844925
Loss in iteration 36 : 0.3705239525138332
Loss in iteration 37 : 0.36720933322209104
Loss in iteration 38 : 0.3655784599620257
Loss in iteration 39 : 0.3654662794363245
Loss in iteration 40 : 0.36624318222100793
Loss in iteration 41 : 0.3668971176575767
Loss in iteration 42 : 0.3667116327134214
Loss in iteration 43 : 0.36542033541475005
Loss in iteration 44 : 0.3633552480367559
Loss in iteration 45 : 0.3612543986617968
Loss in iteration 46 : 0.35967466041819085
Loss in iteration 47 : 0.3589272897531116
Loss in iteration 48 : 0.3589813114929793
Loss in iteration 49 : 0.35924786617066223
Loss in iteration 50 : 0.35924019948621255
Loss in iteration 51 : 0.35871303526323595
Loss in iteration 52 : 0.3577541031945606
Loss in iteration 53 : 0.3566714874096984
Loss in iteration 54 : 0.35585030900823456
Loss in iteration 55 : 0.35547829774603645
Loss in iteration 56 : 0.3554685513552091
Loss in iteration 57 : 0.35555294706475854
Loss in iteration 58 : 0.35547724883895093
Loss in iteration 59 : 0.3551438509527252
Loss in iteration 60 : 0.3546199430439344
Loss in iteration 61 : 0.35410903293034884
Loss in iteration 62 : 0.35377154628080804
Loss in iteration 63 : 0.3536708517636447
Loss in iteration 64 : 0.3536840721117476
Loss in iteration 65 : 0.35370744676112564
Loss in iteration 66 : 0.35362262349530205
Loss in iteration 67 : 0.3534125956245385
Loss in iteration 68 : 0.3531643560510785
Loss in iteration 69 : 0.3529854841142558
Loss in iteration 70 : 0.35290020253975707
Loss in iteration 71 : 0.35290642905940894
Loss in iteration 72 : 0.3529176392092218
Loss in iteration 73 : 0.3528823706367663
Loss in iteration 74 : 0.35278656562378613
Loss in iteration 75 : 0.35266098541459207
Loss in iteration 76 : 0.3525686092736544
Loss in iteration 77 : 0.3525234449102866
Loss in iteration 78 : 0.3525264301949339
Loss in iteration 79 : 0.3525299629785292
Loss in iteration 80 : 0.3525034415753057
Loss in iteration 81 : 0.35244979810321575
Loss in iteration 82 : 0.35238836158735865
Loss in iteration 83 : 0.35233596062743294
Loss in iteration 84 : 0.3523128270269779
Loss in iteration 85 : 0.35230760800234545
Loss in iteration 86 : 0.3523005859596088
Loss in iteration 87 : 0.3522761163603524
Loss in iteration 88 : 0.35223341770067695
Loss in iteration 89 : 0.35219365229799154
Loss in iteration 90 : 0.3521663332770918
Loss in iteration 91 : 0.3521542488274532
Loss in iteration 92 : 0.3521496004174389
Loss in iteration 93 : 0.35213225225641015
Loss in iteration 94 : 0.3521047477651265
Loss in iteration 95 : 0.3520789422451717
Loss in iteration 96 : 0.3520632491260601
Loss in iteration 97 : 0.35205469761841873
Loss in iteration 98 : 0.35204651366472683
Loss in iteration 99 : 0.3520303129277665
Loss in iteration 100 : 0.35201163341255154
Loss in iteration 101 : 0.3519975166003747
Loss in iteration 102 : 0.3519862174326571
Loss in iteration 103 : 0.3519773960162015
Loss in iteration 104 : 0.35196752136592035
Loss in iteration 105 : 0.3519551167188196
Loss in iteration 106 : 0.3519415978060447
Loss in iteration 107 : 0.3519295117806631
Loss in iteration 108 : 0.3519204606218502
Loss in iteration 109 : 0.3519117695385741
Loss in iteration 110 : 0.3519021825987858
Loss in iteration 111 : 0.3518907944226193
Loss in iteration 112 : 0.3518808272078688
Loss in iteration 113 : 0.35187305287740744
Loss in iteration 114 : 0.3518653339112438
Loss in iteration 115 : 0.35185714343435365
Loss in iteration 116 : 0.3518485412374044
Loss in iteration 117 : 0.35184076105582357
Loss in iteration 118 : 0.3518340118175527
Loss in iteration 119 : 0.3518267950395475
Loss in iteration 120 : 0.35181881728195813
Loss in iteration 121 : 0.35181084575171423
Loss in iteration 122 : 0.3518036557046385
Loss in iteration 123 : 0.35179680595444496
Loss in iteration 124 : 0.35178980659148484
Loss in iteration 125 : 0.3517827839826899
Loss in iteration 126 : 0.3517760495292543
Loss in iteration 127 : 0.35176914003786597
Loss in iteration 128 : 0.35176257439459707
Loss in iteration 129 : 0.3517561383197403
Loss in iteration 130 : 0.351749817563459
Loss in iteration 131 : 0.3517436969160426
Loss in iteration 132 : 0.3517376994203165
Loss in iteration 133 : 0.3517312593424605
Loss in iteration 134 : 0.35172512372040643
Loss in iteration 135 : 0.3517192300074855
Loss in iteration 136 : 0.3517134902047276
Loss in iteration 137 : 0.35170815497366925
Loss in iteration 138 : 0.35170221695029424
Loss in iteration 139 : 0.35169680152725774
Loss in iteration 140 : 0.3516912263266562
Loss in iteration 141 : 0.3516860034798571
Loss in iteration 142 : 0.35168047544993986
Loss in iteration 143 : 0.3516753621985915
Loss in iteration 144 : 0.35167006883745144
Loss in iteration 145 : 0.3516646396907586
Loss in iteration 146 : 0.35165949000176594
Loss in iteration 147 : 0.35165424438587944
Loss in iteration 148 : 0.3516491878847429
Loss in iteration 149 : 0.35164415453160536
Loss in iteration 150 : 0.3516393275766672
Loss in iteration 151 : 0.35163451858111944
Loss in iteration 152 : 0.3516299881924185
Loss in iteration 153 : 0.3516253428177971
Loss in iteration 154 : 0.3516206309499489
Loss in iteration 155 : 0.3516160674254618
Loss in iteration 156 : 0.35161169083094296
Loss in iteration 157 : 0.35160719162377047
Loss in iteration 158 : 0.35160257914628246
Loss in iteration 159 : 0.3515982702451152
Loss in iteration 160 : 0.3515939123145996
Loss in iteration 161 : 0.3515897058953933
Loss in iteration 162 : 0.35158542031526235
Loss in iteration 163 : 0.35158116031441466
Loss in iteration 164 : 0.35157693313589383
Loss in iteration 165 : 0.35157270370867416
Loss in iteration 166 : 0.35156858818640097
Loss in iteration 167 : 0.35156436019785137
Loss in iteration 168 : 0.3515603079079011
Loss in iteration 169 : 0.35155606630419634
Loss in iteration 170 : 0.35155213372331895
Loss in iteration 171 : 0.3515477441201772
Loss in iteration 172 : 0.3515437846418866
Loss in iteration 173 : 0.3515397903934516
Loss in iteration 174 : 0.3515355132268939
Loss in iteration 175 : 0.3515316818168146
Loss in iteration 176 : 0.35152745789083256
Loss in iteration 177 : 0.35152324777155647
Loss in iteration 178 : 0.3515191312572984
Loss in iteration 179 : 0.3515151708895672
Loss in iteration 180 : 0.3515113308010959
Loss in iteration 181 : 0.35150718971328004
Loss in iteration 182 : 0.3515033493018602
Loss in iteration 183 : 0.3514993394444275
Loss in iteration 184 : 0.3514952923249145
Loss in iteration 185 : 0.35149135627310185
Loss in iteration 186 : 0.35148764238604585
Loss in iteration 187 : 0.35148399477312914
Loss in iteration 188 : 0.3514803042101915
Loss in iteration 189 : 0.35147657505418184
Loss in iteration 190 : 0.35147304549842984
Loss in iteration 191 : 0.35146948690649427
Loss in iteration 192 : 0.3514659263794279
Loss in iteration 193 : 0.35146241882483975
Loss in iteration 194 : 0.35145911274003233
Loss in iteration 195 : 0.35145565710404764
Loss in iteration 196 : 0.3514521996720192
Loss in iteration 197 : 0.35144866480436565
Loss in iteration 198 : 0.35144534647511405
Loss in iteration 199 : 0.3514420623400925
Loss in iteration 200 : 0.35143867289729835
Testing accuracy  of updater 7 on alg 1 with rate 0.19999999999999996 = 0.8498863706160555, training accuracy 0.8496314496314497, time elapsed: 2755 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.675451768520838
Loss in iteration 3 : 5.271777378676827
Loss in iteration 4 : 2.5471211438901515
Loss in iteration 5 : 1.3902852633527785
Loss in iteration 6 : 1.736632362273488
Loss in iteration 7 : 1.8665764586719065
Loss in iteration 8 : 1.700625126788882
Loss in iteration 9 : 1.5748219568388364
Loss in iteration 10 : 1.5211686214067253
Loss in iteration 11 : 1.4895115489222905
Loss in iteration 12 : 1.4460599872675628
Loss in iteration 13 : 1.3868872824975251
Loss in iteration 14 : 1.3259785431748166
Loss in iteration 15 : 1.2759383588970181
Loss in iteration 16 : 1.2321662750832911
Loss in iteration 17 : 1.1867588200670633
Loss in iteration 18 : 1.1418999383791242
Loss in iteration 19 : 1.0941604778049638
Loss in iteration 20 : 1.0393402122883266
Loss in iteration 21 : 0.9777959926665042
Loss in iteration 22 : 0.911485359092043
Loss in iteration 23 : 0.8457061459177034
Loss in iteration 24 : 0.7856328065833375
Loss in iteration 25 : 0.7393156934884935
Loss in iteration 26 : 0.707730031720856
Loss in iteration 27 : 0.6911509581977662
Loss in iteration 28 : 0.7459528307112628
Loss in iteration 29 : 0.9920125027422338
Loss in iteration 30 : 1.3433545584488789
Loss in iteration 31 : 0.7030671518485053
Loss in iteration 32 : 0.6643469223069774
Loss in iteration 33 : 0.6562043201582538
Loss in iteration 34 : 0.8166321285007327
Loss in iteration 35 : 0.8828738232782978
Loss in iteration 36 : 1.0512714099447305
Loss in iteration 37 : 0.6541045285359389
Loss in iteration 38 : 0.5885081430618563
Loss in iteration 39 : 0.5536779527154579
Loss in iteration 40 : 0.5403643989179494
Loss in iteration 41 : 0.5827334932946389
Loss in iteration 42 : 0.8465696004963821
Loss in iteration 43 : 0.9588610717815059
Loss in iteration 44 : 1.3879679934385734
Loss in iteration 45 : 0.5280465454217456
Loss in iteration 46 : 0.5842321907447158
Loss in iteration 47 : 0.5565960328203591
Loss in iteration 48 : 0.5496009913200617
Loss in iteration 49 : 0.609001054284514
Loss in iteration 50 : 0.693744476827289
Loss in iteration 51 : 0.9813843653626296
Loss in iteration 52 : 0.612507936717506
Loss in iteration 53 : 0.6886463709852635
Loss in iteration 54 : 0.6756042837594477
Loss in iteration 55 : 0.7882538295614827
Loss in iteration 56 : 0.6001823585581428
Loss in iteration 57 : 0.6021984188656682
Loss in iteration 58 : 0.5726879865621333
Loss in iteration 59 : 0.6475960770374082
Loss in iteration 60 : 0.6711674841657733
Loss in iteration 61 : 0.9052447724198582
Loss in iteration 62 : 0.593745888318871
Loss in iteration 63 : 0.616354030154819
Loss in iteration 64 : 0.6021009873560901
Loss in iteration 65 : 0.6984064082641446
Loss in iteration 66 : 0.6282193503973772
Loss in iteration 67 : 0.7549917805923334
Loss in iteration 68 : 0.6112064756339136
Loss in iteration 69 : 0.684096502450709
Loss in iteration 70 : 0.6096371593351242
Loss in iteration 71 : 0.6954656847178874
Loss in iteration 72 : 0.6039335912928732
Loss in iteration 73 : 0.685519601948259
Loss in iteration 74 : 0.6125473557388744
Loss in iteration 75 : 0.7240640289451215
Loss in iteration 76 : 0.6029121845057694
Loss in iteration 77 : 0.6893158619911469
Loss in iteration 78 : 0.6108575939027243
Loss in iteration 79 : 0.7201221515472299
Loss in iteration 80 : 0.5980285898875274
Loss in iteration 81 : 0.6705473877100602
Loss in iteration 82 : 0.6044336198064058
Loss in iteration 83 : 0.7081298189033088
Loss in iteration 84 : 0.5956840391243949
Loss in iteration 85 : 0.6770487705353626
Loss in iteration 86 : 0.6043135283429599
Loss in iteration 87 : 0.7005609170103134
Loss in iteration 88 : 0.5970780165168968
Loss in iteration 89 : 0.6749473902463119
Loss in iteration 90 : 0.598048185793954
Loss in iteration 91 : 0.6909186101146674
Loss in iteration 92 : 0.5946979094768455
Loss in iteration 93 : 0.6753853383849066
Loss in iteration 94 : 0.5981907452301957
Loss in iteration 95 : 0.6968333930060732
Loss in iteration 96 : 0.5930852934587024
Loss in iteration 97 : 0.6712117258710086
Loss in iteration 98 : 0.594428541299797
Loss in iteration 99 : 0.6875103144770481
Loss in iteration 100 : 0.5933869469458958
Loss in iteration 101 : 0.6809079104787299
Loss in iteration 102 : 0.5926938927412881
Loss in iteration 103 : 0.6816893091718214
Loss in iteration 104 : 0.5905611105616441
Loss in iteration 105 : 0.6752289127247497
Loss in iteration 106 : 0.5913879684731749
Loss in iteration 107 : 0.6875474056543553
Loss in iteration 108 : 0.5910630787545363
Loss in iteration 109 : 0.6757352783546495
Loss in iteration 110 : 0.58826653976151
Loss in iteration 111 : 0.6735062821295112
Loss in iteration 112 : 0.5893874352984891
Loss in iteration 113 : 0.6834452321762189
Loss in iteration 114 : 0.5906246658411728
Loss in iteration 115 : 0.6848524619261666
Loss in iteration 116 : 0.586887549983588
Loss in iteration 117 : 0.6664734121800776
Loss in iteration 118 : 0.586950833534043
Loss in iteration 119 : 0.6788925252902787
Loss in iteration 120 : 0.5879528085265268
Loss in iteration 121 : 0.6846460055913117
Loss in iteration 122 : 0.5859832213067571
Loss in iteration 123 : 0.6673086407648587
Loss in iteration 124 : 0.5856602800785676
Loss in iteration 125 : 0.6758061543782348
Loss in iteration 126 : 0.5860067166086448
Loss in iteration 127 : 0.6803364383862904
Loss in iteration 128 : 0.5860570711305336
Loss in iteration 129 : 0.673897481938661
Loss in iteration 130 : 0.5834703349219175
Loss in iteration 131 : 0.6642165270171965
Loss in iteration 132 : 0.5863962829974875
Loss in iteration 133 : 0.6889129416190996
Loss in iteration 134 : 0.5818021116286162
Loss in iteration 135 : 0.6587759149057875
Loss in iteration 136 : 0.5849358514322638
Loss in iteration 137 : 0.6831988910304968
Loss in iteration 138 : 0.5822713058802055
Loss in iteration 139 : 0.668349562143208
Loss in iteration 140 : 0.5875325510096432
Loss in iteration 141 : 0.688814235476297
Loss in iteration 142 : 0.5773597085756281
Loss in iteration 143 : 0.6430713785906332
Loss in iteration 144 : 0.5833609407924925
Loss in iteration 145 : 0.6873228418502945
Loss in iteration 146 : 0.5818899530661947
Loss in iteration 147 : 0.6721795541634855
Loss in iteration 148 : 0.5844965360215052
Loss in iteration 149 : 0.6797367571173123
Loss in iteration 150 : 0.5787033742654738
Loss in iteration 151 : 0.6533837809526175
Loss in iteration 152 : 0.5825240450909698
Loss in iteration 153 : 0.6851090109057891
Loss in iteration 154 : 0.5786065984737855
Loss in iteration 155 : 0.6602413874273466
Loss in iteration 156 : 0.584145270877433
Loss in iteration 157 : 0.6869548085567649
Loss in iteration 158 : 0.5752709058827318
Loss in iteration 159 : 0.6466850646836516
Loss in iteration 160 : 0.5834591474789831
Loss in iteration 161 : 0.6914647804049151
Loss in iteration 162 : 0.5771263531796741
Loss in iteration 163 : 0.6582714213766708
Loss in iteration 164 : 0.5840783240741164
Loss in iteration 165 : 0.6831698269105015
Loss in iteration 166 : 0.5729369031364063
Loss in iteration 167 : 0.6443139648225021
Loss in iteration 168 : 0.5841789263273
Loss in iteration 169 : 0.6996251327142243
Loss in iteration 170 : 0.5759511753924373
Loss in iteration 171 : 0.6520791894889801
Loss in iteration 172 : 0.5805724506241633
Loss in iteration 173 : 0.6741785678739591
Loss in iteration 174 : 0.5770322188367498
Loss in iteration 175 : 0.6582544098762296
Loss in iteration 176 : 0.5819970662525963
Loss in iteration 177 : 0.6825842942931201
Loss in iteration 178 : 0.5751507992081328
Loss in iteration 179 : 0.6497118046131956
Loss in iteration 180 : 0.5821753411394269
Loss in iteration 181 : 0.6907415582763214
Loss in iteration 182 : 0.5735795793154721
Loss in iteration 183 : 0.6458331722095249
Loss in iteration 184 : 0.5811392329901408
Loss in iteration 185 : 0.6866823491827423
Loss in iteration 186 : 0.5744643380533647
Loss in iteration 187 : 0.6523813648622411
Loss in iteration 188 : 0.5808561174422783
Loss in iteration 189 : 0.6833065593862727
Loss in iteration 190 : 0.5731769643233535
Loss in iteration 191 : 0.6487786487843386
Loss in iteration 192 : 0.5807353768980609
Loss in iteration 193 : 0.6883661403616603
Loss in iteration 194 : 0.574030675608884
Loss in iteration 195 : 0.6489913290786121
Loss in iteration 196 : 0.5783254007289284
Loss in iteration 197 : 0.679742065936793
Loss in iteration 198 : 0.5721992625468656
Loss in iteration 199 : 0.6539139318765762
Loss in iteration 200 : 0.582546738406937
Testing accuracy  of updater 8 on alg 1 with rate 2.0 = 0.8196056753270684, training accuracy 0.8163697788697789, time elapsed: 2986 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5679704608204689
Loss in iteration 3 : 0.5438228918918433
Loss in iteration 4 : 0.46096550235803724
Loss in iteration 5 : 0.3896849356236242
Loss in iteration 6 : 0.37826900768828375
Loss in iteration 7 : 0.38476842564318997
Loss in iteration 8 : 0.3844165526330729
Loss in iteration 9 : 0.3801267501669993
Loss in iteration 10 : 0.3765282513701588
Loss in iteration 11 : 0.37597109633671266
Loss in iteration 12 : 0.3763618880951795
Loss in iteration 13 : 0.37628800097027526
Loss in iteration 14 : 0.3754459517241533
Loss in iteration 15 : 0.37414954046665927
Loss in iteration 16 : 0.3728454846623673
Loss in iteration 17 : 0.3717686620258003
Loss in iteration 18 : 0.37082652194208027
Loss in iteration 19 : 0.369967933029953
Loss in iteration 20 : 0.36917732729427133
Loss in iteration 21 : 0.3684148115387495
Loss in iteration 22 : 0.3676597181650996
Loss in iteration 23 : 0.36681749852045675
Loss in iteration 24 : 0.3658749738701447
Loss in iteration 25 : 0.3648391233996123
Loss in iteration 26 : 0.36370683510647095
Loss in iteration 27 : 0.362518259333939
Loss in iteration 28 : 0.361302237962481
Loss in iteration 29 : 0.36011390706605845
Loss in iteration 30 : 0.35899505285631783
Loss in iteration 31 : 0.3579746227111715
Loss in iteration 32 : 0.3570725353150419
Loss in iteration 33 : 0.3563278549182835
Loss in iteration 34 : 0.35569730329707766
Loss in iteration 35 : 0.355175532809934
Loss in iteration 36 : 0.35477427078057405
Loss in iteration 37 : 0.3544482745554
Loss in iteration 38 : 0.3541874728417585
Loss in iteration 39 : 0.35397462753520736
Loss in iteration 40 : 0.35380426975425067
Loss in iteration 41 : 0.3536495782943882
Loss in iteration 42 : 0.3534984323075975
Loss in iteration 43 : 0.3533537404019248
Loss in iteration 44 : 0.35321056702372683
Loss in iteration 45 : 0.3530813128672787
Loss in iteration 46 : 0.35295865429804835
Loss in iteration 47 : 0.35285238273343056
Loss in iteration 48 : 0.3527507634698288
Loss in iteration 49 : 0.3526488704837006
Loss in iteration 50 : 0.35254548596548546
Loss in iteration 51 : 0.3524455639952048
Loss in iteration 52 : 0.35234814292258215
Loss in iteration 53 : 0.3522507557375898
Loss in iteration 54 : 0.35216421309577073
Loss in iteration 55 : 0.3520672334062823
Loss in iteration 56 : 0.3519796103180683
Loss in iteration 57 : 0.35189638313795263
Loss in iteration 58 : 0.35182039428742656
Loss in iteration 59 : 0.3517492076156437
Loss in iteration 60 : 0.3516812244974337
Loss in iteration 61 : 0.35162176611607726
Loss in iteration 62 : 0.35156922115548594
Loss in iteration 63 : 0.3515238441597302
Loss in iteration 64 : 0.3514819149240194
Loss in iteration 65 : 0.3514411198970247
Loss in iteration 66 : 0.3514066730895856
Loss in iteration 67 : 0.35137733372491814
Loss in iteration 68 : 0.351353242621027
Loss in iteration 69 : 0.3513338293520928
Loss in iteration 70 : 0.3513145533508307
Loss in iteration 71 : 0.35129730983223445
Loss in iteration 72 : 0.3512835445031724
Loss in iteration 73 : 0.3512720292736227
Loss in iteration 74 : 0.3512599347421023
Loss in iteration 75 : 0.3512502744556511
Loss in iteration 76 : 0.3512441891638159
Loss in iteration 77 : 0.35123639436038095
Loss in iteration 78 : 0.3512292526994122
Loss in iteration 79 : 0.35122108914352407
Loss in iteration 80 : 0.35121367668504816
Loss in iteration 81 : 0.35120731542820516
Loss in iteration 82 : 0.3512016632522623
Loss in iteration 83 : 0.35119656357833956
Loss in iteration 84 : 0.3511938763566185
Loss in iteration 85 : 0.35119347029028425
Loss in iteration 86 : 0.35119033844480374
Loss in iteration 87 : 0.35118613964510526
Loss in iteration 88 : 0.35119672864427715
Loss in iteration 89 : 0.3511700712471842
Loss in iteration 90 : 0.3511608439417552
Loss in iteration 91 : 0.3511557652365764
Loss in iteration 92 : 0.35115119186419286
Loss in iteration 93 : 0.3511486293414209
Loss in iteration 94 : 0.35114931694163315
Loss in iteration 95 : 0.35113864423736235
Loss in iteration 96 : 0.3511314016488409
Loss in iteration 97 : 0.35112623724651254
Loss in iteration 98 : 0.35112069512889604
Loss in iteration 99 : 0.351116679110291
Loss in iteration 100 : 0.35111536844988545
Loss in iteration 101 : 0.3511133503823636
Loss in iteration 102 : 0.3511163088129237
Loss in iteration 103 : 0.3511281369887689
Loss in iteration 104 : 0.3511008301984902
Loss in iteration 105 : 0.3510936135235256
Loss in iteration 106 : 0.35108984678916105
Loss in iteration 107 : 0.3510887143603546
Loss in iteration 108 : 0.3510831520411842
Loss in iteration 109 : 0.3510776311564913
Loss in iteration 110 : 0.35107352377360435
Loss in iteration 111 : 0.3510692508659174
Loss in iteration 112 : 0.35106847711670663
Loss in iteration 113 : 0.351067473716706
Loss in iteration 114 : 0.3510695463141688
Loss in iteration 115 : 0.3510812096334877
Loss in iteration 116 : 0.3510556529816982
Loss in iteration 117 : 0.35105048075462336
Loss in iteration 118 : 0.3510474698329444
Loss in iteration 119 : 0.3510453272110089
Loss in iteration 120 : 0.3510420293413296
Loss in iteration 121 : 0.3510425332829525
Loss in iteration 122 : 0.3510396388307969
Loss in iteration 123 : 0.3510334430557009
Loss in iteration 124 : 0.3510305990295237
Loss in iteration 125 : 0.351027183582235
Loss in iteration 126 : 0.351024410029618
Loss in iteration 127 : 0.35102149558674384
Loss in iteration 128 : 0.351019150205418
Loss in iteration 129 : 0.3510185929012072
Loss in iteration 130 : 0.3510260853954342
Loss in iteration 131 : 0.3510649279111117
Loss in iteration 132 : 0.35102803395611326
Loss in iteration 133 : 0.35102053573200787
Loss in iteration 134 : 0.35101705790887133
Loss in iteration 135 : 0.35101157010819306
Loss in iteration 136 : 0.35100607330804745
Loss in iteration 137 : 0.3510037393122575
Loss in iteration 138 : 0.35100249484602225
Loss in iteration 139 : 0.3510006192619834
Loss in iteration 140 : 0.3510011225448097
Loss in iteration 141 : 0.35100543626738095
Loss in iteration 142 : 0.35100529656151147
Loss in iteration 143 : 0.35101816017730786
Loss in iteration 144 : 0.3510002587949616
Loss in iteration 145 : 0.35099810963161454
Loss in iteration 146 : 0.3509926678800682
Loss in iteration 147 : 0.35099278404790685
Loss in iteration 148 : 0.3509902973088701
Loss in iteration 149 : 0.3509887167422081
Loss in iteration 150 : 0.3509841354331287
Loss in iteration 151 : 0.3509811526718989
Loss in iteration 152 : 0.3509786478599272
Loss in iteration 153 : 0.35097881559115435
Loss in iteration 154 : 0.35098266919558946
Loss in iteration 155 : 0.3510040889720829
Loss in iteration 156 : 0.35099566805603155
Loss in iteration 157 : 0.35102471641975813
Loss in iteration 158 : 0.35099086460006135
Loss in iteration 159 : 0.3509864579473948
Loss in iteration 160 : 0.3509776827604952
Loss in iteration 161 : 0.3509755780288712
Loss in iteration 162 : 0.35097586516538976
Loss in iteration 163 : 0.3509717285817037
Loss in iteration 164 : 0.3509687226013342
Loss in iteration 165 : 0.3509656221268467
Loss in iteration 166 : 0.35096368840912745
Loss in iteration 167 : 0.35096159948329775
Loss in iteration 168 : 0.3509607234558612
Loss in iteration 169 : 0.3509645327642906
Loss in iteration 170 : 0.3509727577669617
Loss in iteration 171 : 0.3509983660478462
Loss in iteration 172 : 0.35098744794316883
Loss in iteration 173 : 0.3510300185487475
Loss in iteration 174 : 0.35097304985347877
Loss in iteration 175 : 0.35096329534256876
Loss in iteration 176 : 0.3509618659639193
Loss in iteration 177 : 0.3509619050040436
Loss in iteration 178 : 0.35095701386112144
Loss in iteration 179 : 0.35095537621864525
Loss in iteration 180 : 0.3509526838503773
Loss in iteration 181 : 0.3509506439180081
Loss in iteration 182 : 0.3509475879919907
Loss in iteration 183 : 0.3509459785639978
Loss in iteration 184 : 0.35094509793607026
Loss in iteration 185 : 0.3509583912975093
Loss in iteration 186 : 0.35098245101250686
Loss in iteration 187 : 0.35102360934774374
Loss in iteration 188 : 0.3509598551302025
Loss in iteration 189 : 0.35094583097384807
Loss in iteration 190 : 0.35094408885016776
Loss in iteration 191 : 0.35094266740872493
Loss in iteration 192 : 0.35093982552244
Loss in iteration 193 : 0.3509386794236355
Loss in iteration 194 : 0.35093789637318135
Loss in iteration 195 : 0.3509363795364408
Loss in iteration 196 : 0.35093540855627103
Loss in iteration 197 : 0.35094196812339273
Loss in iteration 198 : 0.3509615856579384
Loss in iteration 199 : 0.3510105893274541
Loss in iteration 200 : 0.35094988977452296
Testing accuracy  of updater 8 on alg 1 with rate 0.2 = 0.849640685461581, training accuracy 0.8498464373464374, time elapsed: 2988 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8350469763223431
Loss in iteration 3 : 0.6177982854190054
Loss in iteration 4 : 0.46244686184723677
Loss in iteration 5 : 0.47016757785398566
Loss in iteration 6 : 0.49017005605221625
Loss in iteration 7 : 0.4992218613016885
Loss in iteration 8 : 0.4973985119297299
Loss in iteration 9 : 0.486620352596818
Loss in iteration 10 : 0.4695134586506386
Loss in iteration 11 : 0.4491203878148626
Loss in iteration 12 : 0.42902003483473394
Loss in iteration 13 : 0.41166409070893484
Loss in iteration 14 : 0.39867539520438783
Loss in iteration 15 : 0.3897694712301013
Loss in iteration 16 : 0.3846704433788964
Loss in iteration 17 : 0.3823652449733578
Loss in iteration 18 : 0.38150595941059273
Loss in iteration 19 : 0.38072911135869053
Loss in iteration 20 : 0.3793646167524172
Loss in iteration 21 : 0.37711530991182773
Loss in iteration 22 : 0.37421848885382397
Loss in iteration 23 : 0.3711076903579767
Loss in iteration 24 : 0.3681107129925057
Loss in iteration 25 : 0.3653687217765083
Loss in iteration 26 : 0.36313477698686414
Loss in iteration 27 : 0.36155686835537276
Loss in iteration 28 : 0.3605978897213512
Loss in iteration 29 : 0.36009857202420653
Loss in iteration 30 : 0.3597282352544991
Loss in iteration 31 : 0.35942783307638687
Loss in iteration 32 : 0.3590838876248616
Loss in iteration 33 : 0.3586736893025807
Loss in iteration 34 : 0.3581975640565786
Loss in iteration 35 : 0.3576719381652934
Loss in iteration 36 : 0.3571213185922887
Loss in iteration 37 : 0.35659336316690815
Loss in iteration 38 : 0.3561468291496527
Loss in iteration 39 : 0.35576430057361025
Loss in iteration 40 : 0.35544124448446773
Loss in iteration 41 : 0.35518537214903195
Loss in iteration 42 : 0.35498235952674617
Loss in iteration 43 : 0.354807736881352
Loss in iteration 44 : 0.3546508798073104
Loss in iteration 45 : 0.35450917893044587
Loss in iteration 46 : 0.35437680455688453
Loss in iteration 47 : 0.35424890826993594
Loss in iteration 48 : 0.3541245287965588
Loss in iteration 49 : 0.354011272000703
Loss in iteration 50 : 0.35391060820605136
Loss in iteration 51 : 0.35382279125621174
Loss in iteration 52 : 0.35374478221956696
Loss in iteration 53 : 0.35367707443751034
Loss in iteration 54 : 0.35361666635867195
Loss in iteration 55 : 0.3535630239276249
Loss in iteration 56 : 0.3535139569670075
Loss in iteration 57 : 0.35346743905825
Loss in iteration 58 : 0.35342245226900143
Loss in iteration 59 : 0.35337778002231107
Loss in iteration 60 : 0.35333294103464763
Loss in iteration 61 : 0.35328878995117774
Loss in iteration 62 : 0.35324446966034284
Loss in iteration 63 : 0.3532007386445453
Loss in iteration 64 : 0.35315922917502807
Loss in iteration 65 : 0.3531209395502939
Loss in iteration 66 : 0.3530839099927664
Loss in iteration 67 : 0.3530485191208733
Loss in iteration 68 : 0.35301518184196023
Loss in iteration 69 : 0.35298306785908695
Loss in iteration 70 : 0.3529523163292036
Loss in iteration 71 : 0.3529222171511092
Loss in iteration 72 : 0.35289299622755804
Loss in iteration 73 : 0.3528647802799615
Loss in iteration 74 : 0.3528374597105124
Loss in iteration 75 : 0.35281092910921547
Loss in iteration 76 : 0.3527847704939423
Loss in iteration 77 : 0.35275986867767795
Loss in iteration 78 : 0.352736096384505
Loss in iteration 79 : 0.35271309422803354
Loss in iteration 80 : 0.3526908405560842
Loss in iteration 81 : 0.35266978823613104
Loss in iteration 82 : 0.3526494639166072
Loss in iteration 83 : 0.35262966550450797
Loss in iteration 84 : 0.3526101411178724
Loss in iteration 85 : 0.35259115945492625
Loss in iteration 86 : 0.3525725435221488
Loss in iteration 87 : 0.352554567772614
Loss in iteration 88 : 0.35253735241942746
Loss in iteration 89 : 0.35252043054358617
Loss in iteration 90 : 0.3525040033021164
Loss in iteration 91 : 0.3524883699281311
Loss in iteration 92 : 0.3524736075918733
Loss in iteration 93 : 0.35245921754617426
Loss in iteration 94 : 0.35244522584829513
Loss in iteration 95 : 0.35243189607844766
Loss in iteration 96 : 0.3524191056647624
Loss in iteration 97 : 0.3524067370681841
Loss in iteration 98 : 0.3523947748777803
Loss in iteration 99 : 0.3523830791071546
Loss in iteration 100 : 0.3523718984143512
Loss in iteration 101 : 0.35236108975304126
Loss in iteration 102 : 0.35235024929945674
Loss in iteration 103 : 0.3523397953007154
Loss in iteration 104 : 0.35232943728705407
Loss in iteration 105 : 0.35231935798493047
Loss in iteration 106 : 0.3523094675515033
Loss in iteration 107 : 0.3522997633869112
Loss in iteration 108 : 0.3522904762825426
Loss in iteration 109 : 0.35228146398476873
Loss in iteration 110 : 0.3522727477788552
Loss in iteration 111 : 0.35226411964079957
Loss in iteration 112 : 0.3522556083927208
Loss in iteration 113 : 0.35224721972747797
Loss in iteration 114 : 0.3522389140692975
Loss in iteration 115 : 0.3522306803443916
Loss in iteration 116 : 0.35222259088881486
Loss in iteration 117 : 0.35221459403235833
Loss in iteration 118 : 0.3522067846126702
Loss in iteration 119 : 0.3521990185634097
Loss in iteration 120 : 0.3521915207779447
Loss in iteration 121 : 0.352184158030912
Loss in iteration 122 : 0.3521767083405093
Loss in iteration 123 : 0.35216955892831325
Loss in iteration 124 : 0.3521624973901246
Loss in iteration 125 : 0.3521555667943074
Loss in iteration 126 : 0.3521487350545503
Loss in iteration 127 : 0.35214202637927167
Loss in iteration 128 : 0.352135478995533
Loss in iteration 129 : 0.352128999408187
Loss in iteration 130 : 0.3521225704514153
Loss in iteration 131 : 0.3521161194516013
Loss in iteration 132 : 0.35210983788439754
Loss in iteration 133 : 0.3521035954422645
Loss in iteration 134 : 0.35209761275460927
Loss in iteration 135 : 0.3520915186889052
Loss in iteration 136 : 0.35208534139930214
Loss in iteration 137 : 0.3520793784661406
Loss in iteration 138 : 0.35207336389259775
Loss in iteration 139 : 0.3520673881280795
Loss in iteration 140 : 0.3520615620604908
Loss in iteration 141 : 0.3520556902512141
Loss in iteration 142 : 0.3520502662477392
Loss in iteration 143 : 0.3520443411372005
Loss in iteration 144 : 0.35203866969683134
Loss in iteration 145 : 0.35203309255206383
Loss in iteration 146 : 0.35202787064269264
Loss in iteration 147 : 0.35202204155423655
Loss in iteration 148 : 0.35201663697171004
Loss in iteration 149 : 0.35201168061889815
Loss in iteration 150 : 0.3520060410622828
Loss in iteration 151 : 0.3520008892108525
Loss in iteration 152 : 0.35199563950799545
Loss in iteration 153 : 0.3519903899825812
Loss in iteration 154 : 0.35198540376111936
Loss in iteration 155 : 0.3519803455140841
Loss in iteration 156 : 0.35197501289308464
Loss in iteration 157 : 0.35196992906119556
Loss in iteration 158 : 0.3519649747328386
Loss in iteration 159 : 0.35195985550521786
Loss in iteration 160 : 0.3519548297098814
Loss in iteration 161 : 0.35194977977152786
Loss in iteration 162 : 0.3519448758782141
Loss in iteration 163 : 0.3519399325262065
Loss in iteration 164 : 0.35193509085975494
Loss in iteration 165 : 0.35193028595692016
Loss in iteration 166 : 0.35192554823743677
Loss in iteration 167 : 0.3519208805708523
Loss in iteration 168 : 0.3519160929941795
Loss in iteration 169 : 0.35191149994165866
Loss in iteration 170 : 0.351906984961427
Loss in iteration 171 : 0.3519025133760552
Loss in iteration 172 : 0.3518980327180954
Loss in iteration 173 : 0.3518936438584131
Loss in iteration 174 : 0.35188922856120497
Loss in iteration 175 : 0.35188503500246165
Loss in iteration 176 : 0.35188058508929154
Loss in iteration 177 : 0.351876208013656
Loss in iteration 178 : 0.35187186054928205
Loss in iteration 179 : 0.35186752632626556
Loss in iteration 180 : 0.3518633101117298
Loss in iteration 181 : 0.3518590177774832
Loss in iteration 182 : 0.3518547919745555
Loss in iteration 183 : 0.35185057730549446
Loss in iteration 184 : 0.3518466517819506
Loss in iteration 185 : 0.3518423372784713
Loss in iteration 186 : 0.3518381151116265
Loss in iteration 187 : 0.3518339834989384
Loss in iteration 188 : 0.3518298643087381
Loss in iteration 189 : 0.35182579337645936
Loss in iteration 190 : 0.35182173056989524
Loss in iteration 191 : 0.3518179960071359
Loss in iteration 192 : 0.3518137659427699
Loss in iteration 193 : 0.35180984451993325
Loss in iteration 194 : 0.3518057322114912
Loss in iteration 195 : 0.3518018513901893
Loss in iteration 196 : 0.3517978946217205
Loss in iteration 197 : 0.35179411214586215
Loss in iteration 198 : 0.35179006540558805
Loss in iteration 199 : 0.3517862899140049
Loss in iteration 200 : 0.3517823564054164
Testing accuracy  of updater 8 on alg 1 with rate 0.01999999999999999 = 0.8498863706160555, training accuracy 0.8487100737100737, time elapsed: 2925 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 19.61177185817325
Loss in iteration 3 : 20.16859879904228
Loss in iteration 4 : 12.369523854281207
Loss in iteration 5 : 6.243057989201141
Loss in iteration 6 : 9.542436416297443
Loss in iteration 7 : 14.256594847030382
Loss in iteration 8 : 12.917978159361976
Loss in iteration 9 : 9.663404933066426
Loss in iteration 10 : 8.935888450082235
Loss in iteration 11 : 10.112820730251157
Loss in iteration 12 : 11.45239047480851
Loss in iteration 13 : 11.695045554357778
Loss in iteration 14 : 10.621974298385862
Loss in iteration 15 : 9.003443505462203
Loss in iteration 16 : 8.065924737848023
Loss in iteration 17 : 8.50931019792244
Loss in iteration 18 : 9.355534789440654
Loss in iteration 19 : 9.097990405860843
Loss in iteration 20 : 7.789192469161312
Loss in iteration 21 : 6.821889534775795
Loss in iteration 22 : 7.105474557336281
Loss in iteration 23 : 7.543155283589572
Loss in iteration 24 : 6.925832877591871
Loss in iteration 25 : 5.663747939051537
Loss in iteration 26 : 5.417928705237868
Loss in iteration 27 : 5.83512782896658
Loss in iteration 28 : 5.267191315299999
Loss in iteration 29 : 4.180181432346568
Loss in iteration 30 : 4.426621291915977
Loss in iteration 31 : 4.5541354838945
Loss in iteration 32 : 3.743426860792963
Loss in iteration 33 : 3.5373566385589643
Loss in iteration 34 : 3.8261494403035865
Loss in iteration 35 : 2.7823867220732974
Loss in iteration 36 : 3.1234496117434962
Loss in iteration 37 : 2.3455683059025105
Loss in iteration 38 : 3.1462816138128513
Loss in iteration 39 : 2.157048981954649
Loss in iteration 40 : 3.927807396480896
Loss in iteration 41 : 2.1468887819905422
Loss in iteration 42 : 3.3617025401061236
Loss in iteration 43 : 2.186399735851987
Loss in iteration 44 : 2.417854093499326
Loss in iteration 45 : 2.7139760393598853
Loss in iteration 46 : 1.750348063128537
Loss in iteration 47 : 2.5884958679412824
Loss in iteration 48 : 1.520455604014929
Loss in iteration 49 : 2.3424229322174246
Loss in iteration 50 : 1.6501821247819337
Loss in iteration 51 : 2.5764307698896878
Loss in iteration 52 : 1.5061186938395796
Loss in iteration 53 : 2.3463932474619438
Loss in iteration 54 : 1.5503448887332019
Loss in iteration 55 : 2.260026568202287
Loss in iteration 56 : 1.4090675951844214
Loss in iteration 57 : 1.8695600933270968
Loss in iteration 58 : 1.2625871994340645
Loss in iteration 59 : 2.2536407258685447
Loss in iteration 60 : 1.196871393513219
Loss in iteration 61 : 2.780552935239289
Loss in iteration 62 : 1.3560600398651674
Loss in iteration 63 : 3.244202127878674
Loss in iteration 64 : 1.26169261488639
Loss in iteration 65 : 2.164770951389091
Loss in iteration 66 : 2.0007982330596987
Loss in iteration 67 : 1.4258105845214883
Loss in iteration 68 : 2.199946073257914
Loss in iteration 69 : 1.4915792682550386
Loss in iteration 70 : 1.6373583696459304
Loss in iteration 71 : 1.792749270727392
Loss in iteration 72 : 1.1435402077727341
Loss in iteration 73 : 1.7054516956192471
Loss in iteration 74 : 0.9648795511619909
Loss in iteration 75 : 1.4086220383302144
Loss in iteration 76 : 0.8010142060030763
Loss in iteration 77 : 1.2371704681721738
Loss in iteration 78 : 2.2254378211865586
Loss in iteration 79 : 1.2362902702742316
Loss in iteration 80 : 3.1753484631718534
Loss in iteration 81 : 1.0487104061740304
Loss in iteration 82 : 2.0044571065478114
Loss in iteration 83 : 1.6668926585799118
Loss in iteration 84 : 1.3601010743494715
Loss in iteration 85 : 1.9143205735412974
Loss in iteration 86 : 1.2466069834371483
Loss in iteration 87 : 1.516686581061087
Loss in iteration 88 : 1.4794489673897528
Loss in iteration 89 : 1.076844082042698
Loss in iteration 90 : 1.5076941288466945
Loss in iteration 91 : 0.9564309286345153
Loss in iteration 92 : 1.1878056688859724
Loss in iteration 93 : 0.8346961384285433
Loss in iteration 94 : 0.7153973022040632
Loss in iteration 95 : 1.178378753150742
Loss in iteration 96 : 0.8528164216662968
Loss in iteration 97 : 0.7485555725904697
Loss in iteration 98 : 0.6250550024805555
Loss in iteration 99 : 0.5617973866927299
Loss in iteration 100 : 0.6510704954455774
Loss in iteration 101 : 1.423436834906351
Loss in iteration 102 : 1.0496718906119848
Loss in iteration 103 : 1.5138471197967578
Loss in iteration 104 : 1.1072496768899074
Loss in iteration 105 : 1.3413483709832785
Loss in iteration 106 : 1.2094617383390824
Loss in iteration 107 : 1.1397353396923033
Loss in iteration 108 : 1.3032649673168475
Loss in iteration 109 : 0.9735420380310267
Loss in iteration 110 : 1.268979529058391
Loss in iteration 111 : 0.9317492559432774
Loss in iteration 112 : 1.175666224081387
Loss in iteration 113 : 0.8115199071862883
Loss in iteration 114 : 1.3120104780460629
Loss in iteration 115 : 0.8618885783977084
Loss in iteration 116 : 0.8571609440877648
Loss in iteration 117 : 1.1942236539432063
Loss in iteration 118 : 0.8340289216453165
Loss in iteration 119 : 0.7281151558714611
Loss in iteration 120 : 1.114093887868538
Loss in iteration 121 : 0.8709779875581749
Loss in iteration 122 : 0.5911821200576146
Loss in iteration 123 : 0.9354459337716285
Loss in iteration 124 : 1.144081733759937
Loss in iteration 125 : 0.676262912388116
Loss in iteration 126 : 1.4662968141113928
Loss in iteration 127 : 0.8771668376804732
Loss in iteration 128 : 1.0247061316455397
Loss in iteration 129 : 0.8613166911614203
Loss in iteration 130 : 0.872242950160479
Loss in iteration 131 : 0.8633461549900671
Loss in iteration 132 : 0.8076757578350207
Loss in iteration 133 : 0.856648458001547
Loss in iteration 134 : 0.5525934029849041
Loss in iteration 135 : 0.6779760376503262
Loss in iteration 136 : 1.2427882788949272
Loss in iteration 137 : 1.8075635760545448
Loss in iteration 138 : 1.2528873109696037
Loss in iteration 139 : 1.5976762409850351
Loss in iteration 140 : 1.2168725557165974
Loss in iteration 141 : 1.2386494203196035
Loss in iteration 142 : 1.6308481748085701
Loss in iteration 143 : 1.145040433671625
Loss in iteration 144 : 1.1828079051403504
Loss in iteration 145 : 1.4304591997343323
Loss in iteration 146 : 0.9069676268566237
Loss in iteration 147 : 1.2915240882914139
Loss in iteration 148 : 0.9984790910667755
Loss in iteration 149 : 1.1102049915878145
Loss in iteration 150 : 0.8205198654608591
Loss in iteration 151 : 1.0986246400174597
Loss in iteration 152 : 0.797932082795071
Loss in iteration 153 : 1.3064035972905288
Loss in iteration 154 : 0.7527045029300902
Loss in iteration 155 : 0.9349377734875504
Loss in iteration 156 : 0.8359329220583989
Loss in iteration 157 : 0.6958731108031727
Loss in iteration 158 : 0.9662769469768976
Loss in iteration 159 : 0.5485769742229972
Loss in iteration 160 : 0.7832368356559791
Loss in iteration 161 : 1.3188866445772203
Loss in iteration 162 : 0.8129640390222307
Loss in iteration 163 : 1.7198265809817035
Loss in iteration 164 : 0.8861553619532534
Loss in iteration 165 : 1.33419949096478
Loss in iteration 166 : 0.8507107944786614
Loss in iteration 167 : 1.2580906242357133
Loss in iteration 168 : 1.108912652407671
Loss in iteration 169 : 0.933150436672673
Loss in iteration 170 : 1.2041175228028647
Loss in iteration 171 : 0.82249282269387
Loss in iteration 172 : 1.0056086742205173
Loss in iteration 173 : 0.7483629751021637
Loss in iteration 174 : 0.9283771262392938
Loss in iteration 175 : 0.6632749331595961
Loss in iteration 176 : 1.420409344728198
Loss in iteration 177 : 1.1143738928081217
Loss in iteration 178 : 0.9145626675368981
Loss in iteration 179 : 1.110452819300406
Loss in iteration 180 : 0.9036501257075795
Loss in iteration 181 : 0.9570351489673097
Loss in iteration 182 : 1.039231034434174
Loss in iteration 183 : 0.7267730814921693
Loss in iteration 184 : 1.0276659582822663
Loss in iteration 185 : 0.6597765386292594
Loss in iteration 186 : 0.8665991115589455
Loss in iteration 187 : 0.6862520491035012
Loss in iteration 188 : 0.5202277827963891
Loss in iteration 189 : 0.6824945877244865
Loss in iteration 190 : 0.6698168755864446
Loss in iteration 191 : 0.7390843248700739
Loss in iteration 192 : 0.5251680123167115
Loss in iteration 193 : 0.4852645772966668
Loss in iteration 194 : 0.7318001941418688
Loss in iteration 195 : 1.329461901305993
Loss in iteration 196 : 1.4087215621635751
Loss in iteration 197 : 1.1945231147833508
Loss in iteration 198 : 0.918341195791214
Loss in iteration 199 : 1.4207294783159925
Loss in iteration 200 : 1.0212984046881102
Testing accuracy  of updater 9 on alg 1 with rate 2.0 = 0.8457097229899884, training accuracy 0.8427825552825553, time elapsed: 3704 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6955514818788314
Loss in iteration 3 : 0.8646074577381136
Loss in iteration 4 : 0.7825127861847271
Loss in iteration 5 : 0.5458978181682287
Loss in iteration 6 : 0.4437835761617103
Loss in iteration 7 : 0.5607217778587739
Loss in iteration 8 : 0.6772315908304736
Loss in iteration 9 : 0.6771248825100671
Loss in iteration 10 : 0.5990677978789964
Loss in iteration 11 : 0.5396732855227924
Loss in iteration 12 : 0.5472783063003847
Loss in iteration 13 : 0.5887861577761446
Loss in iteration 14 : 0.6200515233917312
Loss in iteration 15 : 0.6186876807730438
Loss in iteration 16 : 0.5853120238428897
Loss in iteration 17 : 0.5418525956854796
Loss in iteration 18 : 0.5169379259623029
Loss in iteration 19 : 0.5256134304281739
Loss in iteration 20 : 0.5467869744249761
Loss in iteration 21 : 0.5475744078092933
Loss in iteration 22 : 0.5201418272481678
Loss in iteration 23 : 0.48805085821323646
Loss in iteration 24 : 0.4787223974921976
Loss in iteration 25 : 0.4908999790744006
Loss in iteration 26 : 0.4960549975966786
Loss in iteration 27 : 0.4773504236235366
Loss in iteration 28 : 0.44878469052909764
Loss in iteration 29 : 0.4384437076360227
Loss in iteration 30 : 0.445577870457202
Loss in iteration 31 : 0.4417213619787604
Loss in iteration 32 : 0.4183285491016631
Loss in iteration 33 : 0.4024909049210409
Loss in iteration 34 : 0.4064957822670304
Loss in iteration 35 : 0.4060594592024438
Loss in iteration 36 : 0.3901071950459929
Loss in iteration 37 : 0.38198599969900526
Loss in iteration 38 : 0.38967753276548345
Loss in iteration 39 : 0.3846035157783418
Loss in iteration 40 : 0.3721395243937133
Loss in iteration 41 : 0.3782364457429444
Loss in iteration 42 : 0.3784772120851964
Loss in iteration 43 : 0.3679122859250138
Loss in iteration 44 : 0.37387965453737654
Loss in iteration 45 : 0.37324412654454686
Loss in iteration 46 : 0.3657379117624374
Loss in iteration 47 : 0.37156739097628816
Loss in iteration 48 : 0.369243539085739
Loss in iteration 49 : 0.36450243858951187
Loss in iteration 50 : 0.36762653656656397
Loss in iteration 51 : 0.36398945487423334
Loss in iteration 52 : 0.3614037895054846
Loss in iteration 53 : 0.36340225334394805
Loss in iteration 54 : 0.36083319008243947
Loss in iteration 55 : 0.35949844920087165
Loss in iteration 56 : 0.36079311846117146
Loss in iteration 57 : 0.3589437705325161
Loss in iteration 58 : 0.3574403063692205
Loss in iteration 59 : 0.3579330892698565
Loss in iteration 60 : 0.3566138266843102
Loss in iteration 61 : 0.3554123101865675
Loss in iteration 62 : 0.35600662973352665
Loss in iteration 63 : 0.35496660324892393
Loss in iteration 64 : 0.35433762793047435
Loss in iteration 65 : 0.35478809668791944
Loss in iteration 66 : 0.35383149082459786
Loss in iteration 67 : 0.3539125527374675
Loss in iteration 68 : 0.35397863628400245
Loss in iteration 69 : 0.35310810088178224
Loss in iteration 70 : 0.3533414902891629
Loss in iteration 71 : 0.3530946172336665
Loss in iteration 72 : 0.3525978006476773
Loss in iteration 73 : 0.35292152931881227
Loss in iteration 74 : 0.3524218652931904
Loss in iteration 75 : 0.35226979412006654
Loss in iteration 76 : 0.3523223131644444
Loss in iteration 77 : 0.35178938413946836
Loss in iteration 78 : 0.35193593721716276
Loss in iteration 79 : 0.35178029960290147
Loss in iteration 80 : 0.3514940770086871
Loss in iteration 81 : 0.35164952202235716
Loss in iteration 82 : 0.35135866560448176
Loss in iteration 83 : 0.35143568642795786
Loss in iteration 84 : 0.3513741004844269
Loss in iteration 85 : 0.3512963133159961
Loss in iteration 86 : 0.3513914373895744
Loss in iteration 87 : 0.35122451471683497
Loss in iteration 88 : 0.35133875099155026
Loss in iteration 89 : 0.35127605071823587
Loss in iteration 90 : 0.3512633148720141
Loss in iteration 91 : 0.35127169232114774
Loss in iteration 92 : 0.3511529513120031
Loss in iteration 93 : 0.3512460818608602
Loss in iteration 94 : 0.3511210907342145
Loss in iteration 95 : 0.35121893071861965
Loss in iteration 96 : 0.35113194134008197
Loss in iteration 97 : 0.3511552896591309
Loss in iteration 98 : 0.3511016088171826
Loss in iteration 99 : 0.3511145274925479
Loss in iteration 100 : 0.3510776237099457
Loss in iteration 101 : 0.3510754077074553
Loss in iteration 102 : 0.3510553154643485
Loss in iteration 103 : 0.3510459787876246
Loss in iteration 104 : 0.3510355159601165
Loss in iteration 105 : 0.35101226992197626
Loss in iteration 106 : 0.3510223140920175
Loss in iteration 107 : 0.3510213773933801
Loss in iteration 108 : 0.3509942258203713
Loss in iteration 109 : 0.3510052050058678
Loss in iteration 110 : 0.3509764773923645
Loss in iteration 111 : 0.3509854290473947
Loss in iteration 112 : 0.35096297872106885
Loss in iteration 113 : 0.3509854007555713
Loss in iteration 114 : 0.3509541567862468
Loss in iteration 115 : 0.35095584131916324
Loss in iteration 116 : 0.35094383899097303
Loss in iteration 117 : 0.3509457274686595
Loss in iteration 118 : 0.35094054875328523
Loss in iteration 119 : 0.3509304280707198
Loss in iteration 120 : 0.350928399099797
Loss in iteration 121 : 0.35093132717950315
Loss in iteration 122 : 0.3509209884631651
Loss in iteration 123 : 0.35092423583644083
Loss in iteration 124 : 0.3509221345248169
Loss in iteration 125 : 0.3509143485891779
Loss in iteration 126 : 0.35091649664284136
Loss in iteration 127 : 0.3509241269684899
Loss in iteration 128 : 0.350908155510465
Loss in iteration 129 : 0.3509144465443186
Loss in iteration 130 : 0.3509415334481333
Loss in iteration 131 : 0.35090897862699255
Loss in iteration 132 : 0.3509418645554787
Loss in iteration 133 : 0.3509165912634453
Loss in iteration 134 : 0.3509274432766507
Loss in iteration 135 : 0.3509090940607148
Loss in iteration 136 : 0.35091964063756304
Loss in iteration 137 : 0.3509081508022736
Loss in iteration 138 : 0.35092834010043206
Loss in iteration 139 : 0.35090345884247737
Loss in iteration 140 : 0.35093429063654263
Loss in iteration 141 : 0.35090697290357
Loss in iteration 142 : 0.3509289469837875
Loss in iteration 143 : 0.3508959662785132
Loss in iteration 144 : 0.3509348361282487
Loss in iteration 145 : 0.3509024715584002
Loss in iteration 146 : 0.3509138301826241
Loss in iteration 147 : 0.3509008607953967
Loss in iteration 148 : 0.35089466283506904
Loss in iteration 149 : 0.3508924940506025
Loss in iteration 150 : 0.350891719479539
Loss in iteration 151 : 0.35088528477434033
Loss in iteration 152 : 0.3508979121532322
Loss in iteration 153 : 0.35091033262064725
Loss in iteration 154 : 0.3508969402075124
Loss in iteration 155 : 0.35091260648709965
Loss in iteration 156 : 0.3508801213383877
Loss in iteration 157 : 0.35089762459206153
Loss in iteration 158 : 0.3508830076451739
Loss in iteration 159 : 0.35090751313212776
Loss in iteration 160 : 0.3508992484073459
Loss in iteration 161 : 0.35089472832233715
Loss in iteration 162 : 0.35088684400208364
Loss in iteration 163 : 0.35089065913830675
Loss in iteration 164 : 0.3508997257026938
Loss in iteration 165 : 0.35089324192598664
Loss in iteration 166 : 0.3508840129091576
Loss in iteration 167 : 0.35087813172658644
Loss in iteration 168 : 0.3508740818224339
Loss in iteration 169 : 0.3508824485954919
Loss in iteration 170 : 0.350874129242692
Loss in iteration 171 : 0.3508752495095697
Loss in iteration 172 : 0.35087170692425146
Loss in iteration 173 : 0.3508642438529784
Loss in iteration 174 : 0.3508779576392012
Loss in iteration 175 : 0.3508803364249904
Loss in iteration 176 : 0.3508615120718732
Loss in iteration 177 : 0.3508824265800741
Loss in iteration 178 : 0.35089067612005986
Loss in iteration 179 : 0.350875506318431
Loss in iteration 180 : 0.3508931654264924
Loss in iteration 181 : 0.35086269810694815
Loss in iteration 182 : 0.3508787646847277
Loss in iteration 183 : 0.3508631724490748
Loss in iteration 184 : 0.3508849362024362
Loss in iteration 185 : 0.35086233816050444
Loss in iteration 186 : 0.3508853897591567
Loss in iteration 187 : 0.35086233699577607
Loss in iteration 188 : 0.3508923423223862
Loss in iteration 189 : 0.3509006525798485
Loss in iteration 190 : 0.350895208249843
Loss in iteration 191 : 0.3508784820001879
Loss in iteration 192 : 0.35085883371118465
Loss in iteration 193 : 0.35087067743243716
Loss in iteration 194 : 0.3508590752579979
Loss in iteration 195 : 0.3508591905663697
Loss in iteration 196 : 0.3508565871399823
Loss in iteration 197 : 0.3508635638720807
Loss in iteration 198 : 0.35085750627310525
Loss in iteration 199 : 0.35085835443337904
Loss in iteration 200 : 0.35084984059504126
Testing accuracy  of updater 9 on alg 1 with rate 0.2 = 0.8497021067501996, training accuracy 0.850061425061425, time elapsed: 3337 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.855443805084429
Loss in iteration 3 : 0.6000431829216969
Loss in iteration 4 : 0.4624538910504758
Loss in iteration 5 : 0.5214577588314476
Loss in iteration 6 : 0.5672276536903265
Loss in iteration 7 : 0.5866397088291493
Loss in iteration 8 : 0.5823199547411351
Loss in iteration 9 : 0.5583923077308209
Loss in iteration 10 : 0.5197179377671615
Loss in iteration 11 : 0.4718972793181668
Loss in iteration 12 : 0.42346771715515524
Loss in iteration 13 : 0.3876759991309911
Loss in iteration 14 : 0.3738130078051387
Loss in iteration 15 : 0.37941703928944537
Loss in iteration 16 : 0.395113542256095
Loss in iteration 17 : 0.4109117653821984
Loss in iteration 18 : 0.4209481519814963
Loss in iteration 19 : 0.42251478228830275
Loss in iteration 20 : 0.415935360216512
Loss in iteration 21 : 0.40390369510096535
Loss in iteration 22 : 0.39024518103700406
Loss in iteration 23 : 0.37804173724004203
Loss in iteration 24 : 0.3693619726429342
Loss in iteration 25 : 0.3658103639112805
Loss in iteration 26 : 0.3664715355418864
Loss in iteration 27 : 0.3697461608017036
Loss in iteration 28 : 0.37355989873302675
Loss in iteration 29 : 0.3764681849550629
Loss in iteration 30 : 0.37748795071689123
Loss in iteration 31 : 0.37629273633664745
Loss in iteration 32 : 0.3732492626616171
Loss in iteration 33 : 0.36921619240592163
Loss in iteration 34 : 0.3651759599178015
Loss in iteration 35 : 0.3617510312167443
Loss in iteration 36 : 0.3596277572135528
Loss in iteration 37 : 0.35895463411001394
Loss in iteration 38 : 0.35930991790220035
Loss in iteration 39 : 0.36032608725022497
Loss in iteration 40 : 0.36134413928440484
Loss in iteration 41 : 0.36191381191981126
Loss in iteration 42 : 0.3617942617510493
Loss in iteration 43 : 0.36101289963402755
Loss in iteration 44 : 0.359754389336156
Loss in iteration 45 : 0.3583167833549002
Loss in iteration 46 : 0.35705938141772275
Loss in iteration 47 : 0.35617092628815294
Loss in iteration 48 : 0.35579755895135884
Loss in iteration 49 : 0.3558724993086333
Loss in iteration 50 : 0.35615225788645216
Loss in iteration 51 : 0.3564206294999981
Loss in iteration 52 : 0.3565232959889367
Loss in iteration 53 : 0.3563930279629753
Loss in iteration 54 : 0.35602634438812747
Loss in iteration 55 : 0.35550358678194155
Loss in iteration 56 : 0.3549567196928454
Loss in iteration 57 : 0.354460811488236
Loss in iteration 58 : 0.35413989022437975
Loss in iteration 59 : 0.3540201052486536
Loss in iteration 60 : 0.3540460837713216
Loss in iteration 61 : 0.3541149313789451
Loss in iteration 62 : 0.3541434736076368
Loss in iteration 63 : 0.35408296756900526
Loss in iteration 64 : 0.3539259588742872
Loss in iteration 65 : 0.3537026492252241
Loss in iteration 66 : 0.35345818367740206
Loss in iteration 67 : 0.35323078129351504
Loss in iteration 68 : 0.3530883391613808
Loss in iteration 69 : 0.3530139202269004
Loss in iteration 70 : 0.3530050863709722
Loss in iteration 71 : 0.353007969084289
Loss in iteration 72 : 0.3529960993298631
Loss in iteration 73 : 0.35295553429207505
Loss in iteration 74 : 0.3528832023231503
Loss in iteration 75 : 0.35278548702798956
Loss in iteration 76 : 0.35267682803222283
Loss in iteration 77 : 0.3525813270882733
Loss in iteration 78 : 0.35252074168839775
Loss in iteration 79 : 0.3524882055190061
Loss in iteration 80 : 0.35247287057611715
Loss in iteration 81 : 0.3524744030864938
Loss in iteration 82 : 0.3524686384926622
Loss in iteration 83 : 0.35244049030060626
Loss in iteration 84 : 0.3523933730460731
Loss in iteration 85 : 0.35233851614220785
Loss in iteration 86 : 0.3522970987245136
Loss in iteration 87 : 0.3522671360834662
Loss in iteration 88 : 0.3522453828917213
Loss in iteration 89 : 0.3522317517236222
Loss in iteration 90 : 0.3522267035520063
Loss in iteration 91 : 0.3522174113474621
Loss in iteration 92 : 0.35220162125818016
Loss in iteration 93 : 0.35217951802669484
Loss in iteration 94 : 0.3521528506772803
Loss in iteration 95 : 0.35212805249177626
Loss in iteration 96 : 0.3521090610587283
Loss in iteration 97 : 0.3520940127980396
Loss in iteration 98 : 0.3520822285279157
Loss in iteration 99 : 0.3520743507413072
Loss in iteration 100 : 0.35206569136973687
Loss in iteration 101 : 0.3520544046015354
Loss in iteration 102 : 0.3520401932965577
Loss in iteration 103 : 0.3520237591603113
Loss in iteration 104 : 0.3520071136758869
Loss in iteration 105 : 0.35199309869265283
Loss in iteration 106 : 0.35198216760416445
Loss in iteration 107 : 0.3519725439477045
Loss in iteration 108 : 0.35196314159245956
Loss in iteration 109 : 0.351954365927779
Loss in iteration 110 : 0.3519437789621039
Loss in iteration 111 : 0.3519317941984768
Loss in iteration 112 : 0.35192112812786913
Loss in iteration 113 : 0.35191207239970956
Loss in iteration 114 : 0.3519037089696721
Loss in iteration 115 : 0.3518956082870241
Loss in iteration 116 : 0.35188781013065884
Loss in iteration 117 : 0.35187988998083736
Loss in iteration 118 : 0.35187165253391445
Loss in iteration 119 : 0.35186323516715273
Loss in iteration 120 : 0.35185497746466854
Loss in iteration 121 : 0.3518469650687728
Loss in iteration 122 : 0.3518398702929103
Loss in iteration 123 : 0.3518329334214189
Loss in iteration 124 : 0.35182585765151764
Loss in iteration 125 : 0.3518187424551344
Loss in iteration 126 : 0.35181204558002854
Loss in iteration 127 : 0.3518054770134647
Loss in iteration 128 : 0.3517990343052207
Loss in iteration 129 : 0.35179278291298455
Loss in iteration 130 : 0.3517866582368833
Loss in iteration 131 : 0.35178059225704184
Loss in iteration 132 : 0.3517749295217292
Loss in iteration 133 : 0.3517693595047663
Loss in iteration 134 : 0.3517636965463214
Loss in iteration 135 : 0.3517580704127539
Loss in iteration 136 : 0.3517527993727266
Loss in iteration 137 : 0.35174763021168876
Loss in iteration 138 : 0.35174257631925454
Loss in iteration 139 : 0.3517375686104595
Loss in iteration 140 : 0.3517324890144655
Loss in iteration 141 : 0.35172759491615846
Loss in iteration 142 : 0.35172271775776826
Loss in iteration 143 : 0.35171786983248615
Loss in iteration 144 : 0.3517134273303554
Loss in iteration 145 : 0.35170864160311893
Loss in iteration 146 : 0.35170429307556905
Loss in iteration 147 : 0.3516999842138377
Loss in iteration 148 : 0.351695571273561
Loss in iteration 149 : 0.3516910291532897
Loss in iteration 150 : 0.35168653839038566
Loss in iteration 151 : 0.3516823981929191
Loss in iteration 152 : 0.35167827587948974
Loss in iteration 153 : 0.35167401312970276
Loss in iteration 154 : 0.3516696411884603
Loss in iteration 155 : 0.3516655340700155
Loss in iteration 156 : 0.3516614404024356
Loss in iteration 157 : 0.3516572358312197
Loss in iteration 158 : 0.3516530279211201
Loss in iteration 159 : 0.35164892924302815
Loss in iteration 160 : 0.35164521794418563
Loss in iteration 161 : 0.3516409878650769
Loss in iteration 162 : 0.35163714874023255
Loss in iteration 163 : 0.3516332411860069
Loss in iteration 164 : 0.35162940041061735
Loss in iteration 165 : 0.351625581142559
Loss in iteration 166 : 0.3516218213788804
Loss in iteration 167 : 0.35161806994142875
Loss in iteration 168 : 0.3516143191611531
Loss in iteration 169 : 0.3516107659637228
Loss in iteration 170 : 0.3516069760526591
Loss in iteration 171 : 0.3516033157173423
Loss in iteration 172 : 0.35159969452949413
Loss in iteration 173 : 0.35159605650476683
Loss in iteration 174 : 0.3515924172699927
Loss in iteration 175 : 0.3515888293639729
Loss in iteration 176 : 0.3515852790837098
Loss in iteration 177 : 0.3515817662365434
Loss in iteration 178 : 0.3515782750243645
Loss in iteration 179 : 0.351574908661427
Loss in iteration 180 : 0.3515714596368769
Loss in iteration 181 : 0.3515681097525087
Loss in iteration 182 : 0.3515648108605694
Loss in iteration 183 : 0.3515615000768377
Loss in iteration 184 : 0.3515582582364727
Loss in iteration 185 : 0.3515550134837206
Loss in iteration 186 : 0.3515517652160425
Loss in iteration 187 : 0.3515485522447818
Loss in iteration 188 : 0.35154540025293335
Loss in iteration 189 : 0.35154227560911844
Loss in iteration 190 : 0.3515392779915204
Loss in iteration 191 : 0.3515363984800567
Loss in iteration 192 : 0.35153350329536365
Loss in iteration 193 : 0.3515306713896687
Testing accuracy  of updater 9 on alg 1 with rate 0.01999999999999999 = 0.8498249493274369, training accuracy 0.849539312039312, time elapsed: 4515 millisecond.
