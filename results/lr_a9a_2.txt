objc[1924]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1042764c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10529b4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/26 20:03:48 INFO SparkContext: Running Spark version 2.0.0
18/02/26 20:03:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/26 20:03:50 INFO SecurityManager: Changing view acls to: Aitor
18/02/26 20:03:50 INFO SecurityManager: Changing modify acls to: Aitor
18/02/26 20:03:50 INFO SecurityManager: Changing view acls groups to: 
18/02/26 20:03:50 INFO SecurityManager: Changing modify acls groups to: 
18/02/26 20:03:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/26 20:03:50 INFO Utils: Successfully started service 'sparkDriver' on port 50899.
18/02/26 20:03:50 INFO SparkEnv: Registering MapOutputTracker
18/02/26 20:03:50 INFO SparkEnv: Registering BlockManagerMaster
18/02/26 20:03:50 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-d710b558-c327-4fb4-8e97-09e931e9b53d
18/02/26 20:03:50 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/26 20:03:50 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/26 20:03:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/26 20:03:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/26 20:03:51 INFO Executor: Starting executor ID driver on host localhost
18/02/26 20:03:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50901.
18/02/26 20:03:51 INFO NettyBlockTransferService: Server created on 192.168.2.140:50901
18/02/26 20:03:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50901)
18/02/26 20:03:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50901 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50901)
18/02/26 20:03:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50901)
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 3.410397480702917
Loss in iteration 3 : 0.7155504772682814
Loss in iteration 4 : 2.692573049811884
Loss in iteration 5 : 1.3747120209177774
Loss in iteration 6 : 4.366951103798951
Loss in iteration 7 : 0.622510161331102
Loss in iteration 8 : 1.1685966941013477
Loss in iteration 9 : 3.3594077790565735
Loss in iteration 10 : 0.5733109265212704
Loss in iteration 11 : 0.8756798886232056
Loss in iteration 12 : 2.073853127230582
Loss in iteration 13 : 4.323027123112702
Loss in iteration 14 : 0.7092460222658635
Loss in iteration 15 : 0.9133102336602418
Loss in iteration 16 : 2.0382971020019527
Loss in iteration 17 : 1.3592063048527825
Loss in iteration 18 : 2.8100659536748007
Loss in iteration 19 : 0.7651677868655695
Loss in iteration 20 : 1.0904479667541802
Loss in iteration 21 : 1.386416441800596
Loss in iteration 22 : 2.731624445139238
Loss in iteration 23 : 0.7682985424465607
Loss in iteration 24 : 1.0368770258975517
Loss in iteration 25 : 1.2493185880244715
Loss in iteration 26 : 2.4307538522202594
Loss in iteration 27 : 0.8701769034377372
Loss in iteration 28 : 1.349615317424023
Loss in iteration 29 : 1.317213821915632
Loss in iteration 30 : 2.3730936465630297
Loss in iteration 31 : 0.8540801472746422
Loss in iteration 32 : 1.1797482878309768
Loss in iteration 33 : 1.190890313009302
Loss in iteration 34 : 2.0881429632543735
Loss in iteration 35 : 0.957592974244539
Loss in iteration 36 : 1.4574351233361145
Loss in iteration 37 : 1.1920784689523931
Loss in iteration 38 : 1.9761766278138715
Loss in iteration 39 : 0.9766780055965865
Loss in iteration 40 : 1.4262725518358421
Loss in iteration 41 : 1.1483493526030688
Loss in iteration 42 : 1.8246281469209031
Loss in iteration 43 : 1.0234339098733798
Loss in iteration 44 : 1.5013774891325027
Loss in iteration 45 : 1.1134167477256562
Loss in iteration 46 : 1.6979409676187012
Loss in iteration 47 : 1.05444265205198
Loss in iteration 48 : 1.5392941544343794
Loss in iteration 49 : 1.08779900784675
Loss in iteration 50 : 1.6059820061339225
Loss in iteration 51 : 1.0678378428002033
Loss in iteration 52 : 1.547181503759818
Loss in iteration 53 : 1.074484145077099
Loss in iteration 54 : 1.555561771679515
Loss in iteration 55 : 1.0692594813853902
Loss in iteration 56 : 1.53693601327062
Loss in iteration 57 : 1.0689828873871114
Loss in iteration 58 : 1.5317104434832334
Loss in iteration 59 : 1.0672253486422558
Loss in iteration 60 : 1.5236722008721373
Loss in iteration 61 : 1.0661525603541406
Loss in iteration 62 : 1.5178652655664278
Loss in iteration 63 : 1.0650634447519693
Loss in iteration 64 : 1.5123908812764566
Loss in iteration 65 : 1.0641197579305481
Loss in iteration 66 : 1.5075453044412517
Loss in iteration 67 : 1.06325801099192
Loss in iteration 68 : 1.5031068677396606
Loss in iteration 69 : 1.0624749834415204
Loss in iteration 70 : 1.4990254948771424
Loss in iteration 71 : 1.0617580279945464
Loss in iteration 72 : 1.4952411603515112
Loss in iteration 73 : 1.0610990649438101
Loss in iteration 74 : 1.491713405255795
Loss in iteration 75 : 1.0604911119422544
Loss in iteration 76 : 1.488409753319876
Loss in iteration 77 : 1.0599284132359257
Loss in iteration 78 : 1.4853042659755251
Loss in iteration 79 : 1.05940609430727
Loss in iteration 80 : 1.4823756052665933
Loss in iteration 81 : 1.0589199879560243
Loss in iteration 82 : 1.4796059193477833
Loss in iteration 83 : 1.0584664982927168
Loss in iteration 84 : 1.4769800572946719
Loss in iteration 85 : 1.0580425002963922
Loss in iteration 86 : 1.474485015761381
Loss in iteration 87 : 1.0576452623513228
Loss in iteration 88 : 1.4721095330244587
Loss in iteration 89 : 1.0572723851063106
Loss in iteration 90 : 1.4698437822744252
Loss in iteration 91 : 1.0569217521872438
Loss in iteration 92 : 1.4676791337733741
Loss in iteration 93 : 1.056591489800634
Loss in iteration 94 : 1.4656079664205652
Loss in iteration 95 : 1.056279933194655
Loss in iteration 96 : 1.4636235158568616
Loss in iteration 97 : 1.0559855985365645
Loss in iteration 98 : 1.4617197503234804
Loss in iteration 99 : 1.055707159153187
Loss in iteration 100 : 1.4598912680901301
Loss in iteration 101 : 1.0554434253396943
Loss in iteration 102 : 1.458133211962845
Loss in iteration 103 : 1.055193327121762
Loss in iteration 104 : 1.4564411975246274
Loss in iteration 105 : 1.0549558994840804
Loss in iteration 106 : 1.4548112525523138
Loss in iteration 107 : 1.0547302696726546
Loss in iteration 108 : 1.453239765617302
Loss in iteration 109 : 1.0545156462496537
Loss in iteration 110 : 1.451723442290835
Loss in iteration 111 : 1.0543113096353591
Loss in iteration 112 : 1.4502592676857076
Loss in iteration 113 : 1.0541166039161525
Loss in iteration 114 : 1.4488444743043607
Loss in iteration 115 : 1.0539309297327117
Loss in iteration 116 : 1.4474765143498767
Loss in iteration 117 : 1.0537537380926534
Loss in iteration 118 : 1.4461530358042354
Loss in iteration 119 : 1.053584524975114
Loss in iteration 120 : 1.4448718616970897
Loss in iteration 121 : 1.0534228266156807
Loss in iteration 122 : 1.4436309720844982
Loss in iteration 123 : 1.0532682153766668
Loss in iteration 124 : 1.442428488336044
Loss in iteration 125 : 1.0531202961217143
Loss in iteration 126 : 1.4412626593932056
Loss in iteration 127 : 1.0529787030258733
Loss in iteration 128 : 1.4401318497154227
Loss in iteration 129 : 1.0528430967625657
Loss in iteration 130 : 1.4390345286749404
Loss in iteration 131 : 1.0527131620168861
Loss in iteration 132 : 1.4379692611981334
Loss in iteration 133 : 1.0525886052824318
Loss in iteration 134 : 1.4369346994820982
Loss in iteration 135 : 1.052469152904667
Loss in iteration 136 : 1.4359295756411126
Loss in iteration 137 : 1.0523545493393842
Loss in iteration 138 : 1.4349526951592932
Loss in iteration 139 : 1.0522445555986002
Loss in iteration 140 : 1.4340029310436224
Loss in iteration 141 : 1.0521389478611385
Loss in iteration 142 : 1.4330792185879397
Loss in iteration 143 : 1.0520375162269657
Loss in iteration 144 : 1.4321805506699627
Loss in iteration 145 : 1.0519400635984246
Loss in iteration 146 : 1.4313059735156444
Loss in iteration 147 : 1.051846404672844
Loss in iteration 148 : 1.4304545828739277
Loss in iteration 149 : 1.0517563650337258
Loss in iteration 150 : 1.429625520552757
Loss in iteration 151 : 1.0516697803289807
Loss in iteration 152 : 1.428817971274254
Loss in iteration 153 : 1.0515864955262233
Loss in iteration 154 : 1.4280311598121456
Loss in iteration 155 : 1.0515063642369702
Loss in iteration 156 : 1.4272643483802605
Loss in iteration 157 : 1.0514292481015244
Loss in iteration 158 : 1.426516834243878
Loss in iteration 159 : 1.0513550162285452
Loss in iteration 160 : 1.4257879475303903
Loss in iteration 161 : 1.051283544683302
Loss in iteration 162 : 1.4250770492182907
Loss in iteration 163 : 1.0512147160194074
Loss in iteration 164 : 1.424383529285623
Loss in iteration 165 : 1.0511484188498954
Loss in iteration 166 : 1.4237068050027337
Loss in iteration 167 : 1.0510845474534036
Loss in iteration 168 : 1.4230463193542298
Loss in iteration 169 : 1.0510230014121078
Loss in iteration 170 : 1.4224015395782335
Loss in iteration 171 : 1.0509636852783912
Loss in iteration 172 : 1.4217719558118813
Loss in iteration 173 : 1.0509065082673898
Loss in iteration 174 : 1.4211570798328927
Loss in iteration 175 : 1.0508513839730445
Loss in iteration 176 : 1.4205564438893095
Loss in iteration 177 : 1.0507982301055723
Loss in iteration 178 : 1.4199695996087602
Loss in iteration 179 : 1.0507469682482249
Loss in iteration 180 : 1.4193961169811251
Loss in iteration 181 : 1.050697523631865
Loss in iteration 182 : 1.4188355834081663
Loss in iteration 183 : 1.0506498249255947
Loss in iteration 184 : 1.4182876028144942
Loss in iteration 185 : 1.050603804042173
Loss in iteration 186 : 1.4177517948149687
Loss in iteration 187 : 1.050559395956989
Loss in iteration 188 : 1.417227793934394
Loss in iteration 189 : 1.0505165385392996
Loss in iteration 190 : 1.416715248874951
Loss in iteration 191 : 1.0504751723949612
Loss in iteration 192 : 1.416213821828043
Loss in iteration 193 : 1.0504352407195139
Loss in iteration 194 : 1.4157231878272747
Loss in iteration 195 : 1.0503966891609575
Loss in iteration 196 : 1.415243034139336
Loss in iteration 197 : 1.050359465691411
Loss in iteration 198 : 1.414773059690328
Loss in iteration 199 : 1.0503235204868224
Loss in iteration 200 : 1.4143129745247585
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.7701615379890671, training accuracy 0.773433660933661, time elapsed: 8077 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 2.387363824767098
Loss in iteration 3 : 0.5826851374808334
Loss in iteration 4 : 1.803859496041806
Loss in iteration 5 : 1.1197164775191346
Loss in iteration 6 : 2.992647598054052
Loss in iteration 7 : 0.4725214087215873
Loss in iteration 8 : 0.6461034331631207
Loss in iteration 9 : 1.584377704253561
Loss in iteration 10 : 1.075460146076915
Loss in iteration 11 : 2.3330518819379975
Loss in iteration 12 : 0.47959746843201495
Loss in iteration 13 : 0.5421309491380646
Loss in iteration 14 : 0.79271512647426
Loss in iteration 15 : 1.7765517631804049
Loss in iteration 16 : 0.744823562061977
Loss in iteration 17 : 1.4807543331809545
Loss in iteration 18 : 0.8816460252472308
Loss in iteration 19 : 1.6525749503551999
Loss in iteration 20 : 0.7183051124640517
Loss in iteration 21 : 1.1588375816439644
Loss in iteration 22 : 0.9161080467742534
Loss in iteration 23 : 1.548568259399013
Loss in iteration 24 : 0.7136002812253808
Loss in iteration 25 : 1.0374129808704475
Loss in iteration 26 : 0.8693577136105449
Loss in iteration 27 : 1.3805266081431127
Loss in iteration 28 : 0.7550590926964599
Loss in iteration 29 : 1.0936624450406203
Loss in iteration 30 : 0.8280325463986582
Loss in iteration 31 : 1.2425146092201773
Loss in iteration 32 : 0.7814105893978556
Loss in iteration 33 : 1.1197120127329987
Loss in iteration 34 : 0.8014060565596559
Loss in iteration 35 : 1.153263943059162
Loss in iteration 36 : 0.788115543618959
Loss in iteration 37 : 1.1136133805839323
Loss in iteration 38 : 0.7890631046193296
Loss in iteration 39 : 1.1092147326592838
Loss in iteration 40 : 0.7853224340845824
Loss in iteration 41 : 1.0956102494157243
Loss in iteration 42 : 0.7834299744046072
Loss in iteration 43 : 1.0873469696768283
Loss in iteration 44 : 0.7814363275483357
Loss in iteration 45 : 1.079526572645912
Loss in iteration 46 : 0.779749651113572
Loss in iteration 47 : 1.0728797581303298
Loss in iteration 48 : 0.7782348682255305
Loss in iteration 49 : 1.066959681495578
Loss in iteration 50 : 0.7768763852299785
Loss in iteration 51 : 1.0616487458778139
Loss in iteration 52 : 0.775648203555749
Loss in iteration 53 : 1.056833769571368
Loss in iteration 54 : 0.7745320677763813
Loss in iteration 55 : 1.052436458577815
Loss in iteration 56 : 0.7735132330761557
Loss in iteration 57 : 1.0483968711288039
Loss in iteration 58 : 0.7725796877916887
Loss in iteration 59 : 1.0446678789507957
Loss in iteration 60 : 0.7717214311800638
Loss in iteration 61 : 1.0412113317022185
Loss in iteration 62 : 0.7709300062042922
Loss in iteration 63 : 1.0379956786943754
Loss in iteration 64 : 0.7701981769380424
Loss in iteration 65 : 1.0349944078371234
Loss in iteration 66 : 0.7695196980974243
Loss in iteration 67 : 1.0321849748585543
Loss in iteration 68 : 0.7688891437875995
Loss in iteration 69 : 1.029548035085504
Loss in iteration 70 : 0.7683017756188615
Loss in iteration 71 : 1.0270668698968428
Loss in iteration 72 : 0.7677534380990908
Loss in iteration 73 : 1.024726944301021
Loss in iteration 74 : 0.7672404737524399
Loss in iteration 75 : 1.022515556909556
Loss in iteration 76 : 0.7667596530767654
Loss in iteration 77 : 1.0204215577021918
Loss in iteration 78 : 0.7663081160359946
Loss in iteration 79 : 1.0184351172283042
Loss in iteration 80 : 0.7658833227525965
Loss in iteration 81 : 1.0165475358805656
Loss in iteration 82 : 0.7654830116793524
Loss in iteration 83 : 1.0147510850194281
Loss in iteration 84 : 0.7651051639375771
Loss in iteration 85 : 1.0130388737977338
Loss in iteration 86 : 0.7647479727909976
Loss in iteration 87 : 1.0114047369559616
Loss in iteration 88 : 0.764409817429109
Loss in iteration 89 : 1.0098431398761805
Loss in iteration 90 : 0.7640892403867267
Loss in iteration 91 : 1.0083490979328904
Loss in iteration 92 : 0.7637849280452156
Loss in iteration 93 : 1.0069181077506757
Loss in iteration 94 : 0.7634956937544561
Loss in iteration 95 : 1.0055460884207668
Loss in iteration 96 : 0.7632204631900342
Loss in iteration 97 : 1.0042293310788892
Loss in iteration 98 : 0.7629582616217337
Loss in iteration 99 : 1.0029644555259953
Loss in iteration 100 : 0.7627082028202016
Loss in iteration 101 : 1.0017483727994632
Loss in iteration 102 : 0.7624694793703346
Loss in iteration 103 : 1.000578252785587
Loss in iteration 104 : 0.7622413541956932
Loss in iteration 105 : 0.9994514961152078
Loss in iteration 106 : 0.7620231531269122
Loss in iteration 107 : 0.9983657097070789
Loss in iteration 108 : 0.7618142583721622
Loss in iteration 109 : 0.997318685426332
Loss in iteration 110 : 0.7616141027685671
Loss in iteration 111 : 0.9963083814094117
Loss in iteration 112 : 0.7614221647107806
Loss in iteration 113 : 0.9953329056774112
Loss in iteration 114 : 0.7612379636680552
Loss in iteration 115 : 0.9943905017181397
Loss in iteration 116 : 0.761061056213501
Loss in iteration 117 : 0.9934795357657912
Loss in iteration 118 : 0.7608910325002246
Loss in iteration 119 : 0.9925984855486707
Loss in iteration 120 : 0.760727513127816
Loss in iteration 121 : 0.9917459303088773
Loss in iteration 122 : 0.7605701463506834
Loss in iteration 123 : 0.9909205419271022
Loss in iteration 124 : 0.7604186055861722
Loss in iteration 125 : 0.990121077010154
Loss in iteration 126 : 0.7602725871860878
Loss in iteration 127 : 0.9893463698186467
Loss in iteration 128 : 0.7601318084402151
Loss in iteration 129 : 0.9885953259301686
Loss in iteration 130 : 0.7599960057843635
Loss in iteration 131 : 0.9878669165477704
Loss in iteration 132 : 0.7598649331891716
Loss in iteration 133 : 0.9871601733757417
Loss in iteration 134 : 0.7597383607089692
Loss in iteration 135 : 0.9864741839957437
Loss in iteration 136 : 0.7596160731724655
Loss in iteration 137 : 0.9858080876848567
Loss in iteration 138 : 0.7594978689995772
Loss in iteration 139 : 0.985161071625219
Loss in iteration 140 : 0.7593835591304168
Loss in iteration 141 : 0.9845323674611964
Loss in iteration 142 : 0.7592729660542876
Loss in iteration 143 : 0.983921248165876
Loss in iteration 144 : 0.7591659229280411
Loss in iteration 145 : 0.9833270251834705
Loss in iteration 146 : 0.7590622727742916
Loss in iteration 147 : 0.9827490458182404
Loss in iteration 148 : 0.7589618677512232
Loss in iteration 149 : 0.9821866908443236
Loss in iteration 150 : 0.7588645684866664
Loss in iteration 151 : 0.9816393723138861
Loss in iteration 152 : 0.7587702434699259
Loss in iteration 153 : 0.9811065315437878
Loss in iteration 154 : 0.7586787684955246
Loss in iteration 155 : 0.9805876372628247
Loss in iteration 156 : 0.7585900261539378
Loss in iteration 157 : 0.9800821839045837
Loss in iteration 158 : 0.7585039053645575
Loss in iteration 159 : 0.979589690031589
Loss in iteration 160 : 0.7584203009469592
Loss in iteration 161 : 0.9791096968786923
Loss in iteration 162 : 0.758339113226847
Loss in iteration 163 : 0.9786417670049117
Loss in iteration 164 : 0.7582602476734587
Loss in iteration 165 : 0.9781854830438376
Loss in iteration 166 : 0.7581836145653719
Loss in iteration 167 : 0.9777404465437602
Loss in iteration 168 : 0.7581091286824606
Loss in iteration 169 : 0.9773062768901883
Loss in iteration 170 : 0.7580367090213195
Loss in iteration 171 : 0.9768826103031519
Loss in iteration 172 : 0.7579662785323368
Loss in iteration 173 : 0.9764690989035507
Loss in iteration 174 : 0.757897763876301
Loss in iteration 175 : 0.97606540984242
Loss in iteration 176 : 0.7578310951990989
Loss in iteration 177 : 0.9756712244883888
Loss in iteration 178 : 0.7577662059227613
Loss in iteration 179 : 0.9752862376683809
Loss in iteration 180 : 0.7577030325516106
Loss in iteration 181 : 0.9749101569575682
Loss in iteration 182 : 0.7576415144921025
Loss in iteration 183 : 0.9745427020146281
Loss in iteration 184 : 0.7575815938854976
Loss in iteration 185 : 0.9741836039590643
Loss in iteration 186 : 0.7575232154520077
Loss in iteration 187 : 0.9738326047871787
Loss in iteration 188 : 0.7574663263457366
Loss in iteration 189 : 0.9734894568239553
Loss in iteration 190 : 0.757410876019482
Loss in iteration 191 : 0.9731539222084157
Loss in iteration 192 : 0.7573568160984807
Loss in iteration 193 : 0.9728257724095307
Loss in iteration 194 : 0.7573041002627228
Loss in iteration 195 : 0.9725047877712385
Loss in iteration 196 : 0.7572526841368505
Loss in iteration 197 : 0.9721907570838002
Loss in iteration 198 : 0.7572025251872637
Loss in iteration 199 : 0.9718834771802823
Loss in iteration 200 : 0.7571535826257991
Testing accuracy  of updater 0 on alg 0 with rate 7.0 = 0.8092868988391376, training accuracy 0.8060503685503686, time elapsed: 5110 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 1.3671281430965467
Loss in iteration 3 : 0.4980878568678827
Loss in iteration 4 : 0.96451893251099
Loss in iteration 5 : 0.7533249410207916
Loss in iteration 6 : 1.4128494327890573
Loss in iteration 7 : 0.38858461406852285
Loss in iteration 8 : 0.43158173198389665
Loss in iteration 9 : 0.5353919769081814
Loss in iteration 10 : 0.8622024480770503
Loss in iteration 11 : 0.6243982199132568
Loss in iteration 12 : 0.9546621540214928
Loss in iteration 13 : 0.5292097461184109
Loss in iteration 14 : 0.7183595756574688
Loss in iteration 15 : 0.5770778385914996
Loss in iteration 16 : 0.7779207175204461
Loss in iteration 17 : 0.5419737689797104
Loss in iteration 18 : 0.6914700804112541
Loss in iteration 19 : 0.5379116488624557
Loss in iteration 20 : 0.6734274875333779
Loss in iteration 21 : 0.528314308275652
Loss in iteration 22 : 0.6499549002841146
Loss in iteration 23 : 0.5210403142250903
Loss in iteration 24 : 0.6338761349951748
Loss in iteration 25 : 0.5154321641234082
Loss in iteration 26 : 0.6222829449999073
Loss in iteration 27 : 0.5110263186071906
Loss in iteration 28 : 0.6134806718881836
Loss in iteration 29 : 0.5074289910736298
Loss in iteration 30 : 0.6063676260083414
Loss in iteration 31 : 0.5043682995683648
Loss in iteration 32 : 0.6003019108400899
Loss in iteration 33 : 0.501681319032215
Loss in iteration 34 : 0.594944199357442
Loss in iteration 35 : 0.4992772867021578
Loss in iteration 36 : 0.5901233757947827
Loss in iteration 37 : 0.4971049786942036
Loss in iteration 38 : 0.585748903153295
Loss in iteration 39 : 0.49513218316708313
Loss in iteration 40 : 0.5817643229602367
Loss in iteration 41 : 0.4933352479442693
Loss in iteration 42 : 0.5781266549828967
Loss in iteration 43 : 0.4916946017493351
Loss in iteration 44 : 0.5747988592661636
Loss in iteration 45 : 0.49019313055666863
Loss in iteration 46 : 0.5717477021001125
Loss in iteration 47 : 0.48881566669691273
Loss in iteration 48 : 0.5689433586401442
Loss in iteration 49 : 0.487548803177402
Loss in iteration 50 : 0.5663593286742569
Loss in iteration 51 : 0.48638075153541355
Loss in iteration 52 : 0.563972265240331
Loss in iteration 53 : 0.48530117907013726
Loss in iteration 54 : 0.5617616890834091
Loss in iteration 55 : 0.48430103270472885
Loss in iteration 56 : 0.5597096515977404
Loss in iteration 57 : 0.4833723681027773
Loss in iteration 58 : 0.557800401424628
Loss in iteration 59 : 0.4825081960853461
Loss in iteration 60 : 0.5560200835337239
Loss in iteration 61 : 0.4817023504178184
Loss in iteration 62 : 0.5543564794410012
Loss in iteration 63 : 0.48094937610123073
Loss in iteration 64 : 0.5527987867082175
Loss in iteration 65 : 0.4802444352902899
Loss in iteration 66 : 0.5513374321363231
Loss in iteration 67 : 0.4795832276777199
Loss in iteration 68 : 0.5499639127641633
Loss in iteration 69 : 0.4789619226256559
Loss in iteration 70 : 0.5486706597160148
Loss in iteration 71 : 0.47837710091634245
Loss in iteration 72 : 0.5474509210331674
Loss in iteration 73 : 0.47782570450084433
Loss in iteration 74 : 0.5462986605021314
Loss in iteration 75 : 0.4773049929934473
Loss in iteration 76 : 0.545208470110639
Loss in iteration 77 : 0.4768125059135366
Loss in iteration 78 : 0.5441754941875794
Loss in iteration 79 : 0.4763460298540952
Loss in iteration 80 : 0.5431953635890935
Loss in iteration 81 : 0.4759035698864596
Loss in iteration 82 : 0.5422641385300588
Loss in iteration 83 : 0.47548332461423043
Loss in iteration 84 : 0.5413782588574556
Loss in iteration 85 : 0.4750836643748046
Loss in iteration 86 : 0.5405345007320875
Loss in iteration 87 : 0.4747031121595601
Loss in iteration 88 : 0.5397299388321319
Loss in iteration 89 : 0.47434032688597144
Loss in iteration 90 : 0.5389619133201117
Loss in iteration 91 : 0.47399408870768445
Loss in iteration 92 : 0.5382280009242022
Loss in iteration 93 : 0.47366328609376507
Loss in iteration 94 : 0.53752598957854
Loss in iteration 95 : 0.47334690444624394
Loss in iteration 96 : 0.5368538561466513
Loss in iteration 97 : 0.47304401605751634
Loss in iteration 98 : 0.5362097468199191
Loss in iteration 99 : 0.4727537712364406
Loss in iteration 100 : 0.5355919598401507
Loss in iteration 101 : 0.4724753904554849
Loss in iteration 102 : 0.5349989302446267
Loss in iteration 103 : 0.47220815739104127
Loss in iteration 104 : 0.534429216373523
Loss in iteration 105 : 0.4719514127461974
Loss in iteration 106 : 0.5338814879154342
Loss in iteration 107 : 0.4717045487599011
Loss in iteration 108 : 0.5333545152971079
Loss in iteration 109 : 0.47146700431902977
Loss in iteration 110 : 0.5328471602499266
Loss in iteration 111 : 0.4712382606007382
Loss in iteration 112 : 0.5323583674079284
Loss in iteration 113 : 0.47101783718183055
Loss in iteration 114 : 0.5318871568115011
Loss in iteration 115 : 0.47080528856002535
Loss in iteration 116 : 0.5314326172074989
Loss in iteration 117 : 0.4706002010388565
Loss in iteration 118 : 0.530993900050804
Loss in iteration 119 : 0.47040218993421057
Loss in iteration 120 : 0.5305702141246295
Loss in iteration 121 : 0.4702108970655512
Loss in iteration 122 : 0.5301608207074123
Loss in iteration 123 : 0.47002598849955635
Loss in iteration 124 : 0.5297650292234459
Loss in iteration 125 : 0.46984715251785375
Loss in iteration 126 : 0.5293821933221843
Loss in iteration 127 : 0.4696740977839033
Loss in iteration 128 : 0.5290117073381516
Loss in iteration 129 : 0.46950655168720457
Loss in iteration 130 : 0.5286530030891118
Loss in iteration 131 : 0.46934425884548214
Loss in iteration 132 : 0.5283055469756142
Loss in iteration 133 : 0.469186979747918
Loss in iteration 134 : 0.5279688373492002
Loss in iteration 135 : 0.46903448952435517
Loss in iteration 136 : 0.5276424021207274
Loss in iteration 137 : 0.4688865768273611
Loss in iteration 138 : 0.5273257965835179
Loss in iteration 139 : 0.46874304281524415
Loss in iteration 140 : 0.5270186014289162
Loss in iteration 141 : 0.4686037002257768
Loss in iteration 142 : 0.5267204209347058
Loss in iteration 143 : 0.4684683725313881
Loss in iteration 144 : 0.5264308813087984
Loss in iteration 145 : 0.4683368931676119
Loss in iteration 146 : 0.5261496291727629
Loss in iteration 147 : 0.4682091048275322
Loss in iteration 148 : 0.5258763301714593
Loss in iteration 149 : 0.4680848588158091
Loss in iteration 150 : 0.5256106676965321
Loss in iteration 151 : 0.46796401445645536
Loss in iteration 152 : 0.5253523417128768
Loss in iteration 153 : 0.4678464385492419
Loss in iteration 154 : 0.5251010676784228
Loss in iteration 155 : 0.4677320048701489
Loss in iteration 156 : 0.5248565755484803
Loss in iteration 157 : 0.4676205937117395
Loss in iteration 158 : 0.5246186088569607
Loss in iteration 159 : 0.4675120914597868
Loss in iteration 160 : 0.5243869238675227
Loss in iteration 161 : 0.4674063902028153
Loss in iteration 162 : 0.5241612887883875
Loss in iteration 163 : 0.4673033873716699
Loss in iteration 164 : 0.5239414830452189
Loss in iteration 165 : 0.46720298540635397
Loss in iteration 166 : 0.5237272966070868
Loss in iteration 167 : 0.46710509144780726
Loss in iteration 168 : 0.5235185293609217
Loss in iteration 169 : 0.46700961705244837
Loss in iteration 170 : 0.5233149905304249
Loss in iteration 171 : 0.466916477927531
Loss in iteration 172 : 0.5231164981356854
Loss in iteration 173 : 0.4668255936855098
Loss in iteration 174 : 0.5229228784901638
Loss in iteration 175 : 0.46673688761592075
Loss in iteration 176 : 0.5227339657321086
Loss in iteration 177 : 0.46665028647323803
Loss in iteration 178 : 0.5225496013875333
Loss in iteration 179 : 0.46656572027945525
Loss in iteration 180 : 0.5223696339622883
Loss in iteration 181 : 0.4664831221401918
Loss in iteration 182 : 0.5221939185610894
Loss in iteration 183 : 0.4664024280732373
Loss in iteration 184 : 0.5220223165312196
Loss in iteration 185 : 0.46632357684857517
Loss in iteration 186 : 0.5218546951292277
Loss in iteration 187 : 0.4662465098389855
Loss in iteration 188 : 0.5216909272087613
Loss in iteration 189 : 0.46617117088037247
Loss in iteration 190 : 0.5215308909280717
Loss in iteration 191 : 0.4660975061411762
Loss in iteration 192 : 0.5213744694756474
Loss in iteration 193 : 0.46602546400006817
Loss in iteration 194 : 0.5212215508127578
Loss in iteration 195 : 0.46595499493140646
Loss in iteration 196 : 0.5210720274316601
Loss in iteration 197 : 0.46588605139780814
Loss in iteration 198 : 0.5209257961283436
Loss in iteration 199 : 0.4658185877493969
Loss in iteration 200 : 0.5207827577888193
Testing accuracy  of updater 0 on alg 0 with rate 4.0 = 0.7964498495178429, training accuracy 0.7953624078624079, time elapsed: 4227 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5309044865682255
Loss in iteration 3 : 0.48006841651644
Loss in iteration 4 : 0.45713603284681786
Loss in iteration 5 : 0.44001759513598765
Loss in iteration 6 : 0.42690872191348705
Loss in iteration 7 : 0.4166211311753286
Loss in iteration 8 : 0.40836455681194495
Loss in iteration 9 : 0.40160354258794156
Loss in iteration 10 : 0.3959676332341301
Loss in iteration 11 : 0.3911950457719669
Loss in iteration 12 : 0.3870969932970904
Loss in iteration 13 : 0.3835347162171088
Loss in iteration 14 : 0.3804044160573927
Loss in iteration 15 : 0.37762718146127133
Loss in iteration 16 : 0.37514212458771645
Loss in iteration 17 : 0.3729016203087438
Loss in iteration 18 : 0.37086794783533406
Loss in iteration 19 : 0.3690108840421445
Loss in iteration 20 : 0.36730595333526656
Loss in iteration 21 : 0.36573313751186676
Loss in iteration 22 : 0.364275912611671
Loss in iteration 23 : 0.3629205213895037
Loss in iteration 24 : 0.36165541773439897
Loss in iteration 25 : 0.36047083806197394
Loss in iteration 26 : 0.3593584675122685
Loss in iteration 27 : 0.35831117767056986
Loss in iteration 28 : 0.35732281877117633
Loss in iteration 29 : 0.3563880537820448
Loss in iteration 30 : 0.3555022249581998
Loss in iteration 31 : 0.35466124576910435
Loss in iteration 32 : 0.35386151280516115
Loss in iteration 33 : 0.35309983352737745
Loss in iteration 34 : 0.3523733666646589
Loss in iteration 35 : 0.3516795727717562
Loss in iteration 36 : 0.3510161729987353
Loss in iteration 37 : 0.3503811145345544
Loss in iteration 38 : 0.34977254150436254
Loss in iteration 39 : 0.3491887703461915
Loss in iteration 40 : 0.34862826888473336
Loss in iteration 41 : 0.34808963847078156
Loss in iteration 42 : 0.34757159867414106
Loss in iteration 43 : 0.3470729741123783
Loss in iteration 44 : 0.34659268307363805
Loss in iteration 45 : 0.34612972765214217
Loss in iteration 46 : 0.3456831851642783
Loss in iteration 47 : 0.34525220065255496
Loss in iteration 48 : 0.34483598031714835
Loss in iteration 49 : 0.3444337857411108
Loss in iteration 50 : 0.3440449287968642
Loss in iteration 51 : 0.3436687671395277
Loss in iteration 52 : 0.34330470020726744
Loss in iteration 53 : 0.34295216566111725
Loss in iteration 54 : 0.34261063620689663
Loss in iteration 55 : 0.34227961675026664
Loss in iteration 56 : 0.3419586418431681
Loss in iteration 57 : 0.3416472733857935
Loss in iteration 58 : 0.3413450985533085
Loss in iteration 59 : 0.34105172792076965
Loss in iteration 60 : 0.34076679376332225
Loss in iteration 61 : 0.3404899485117659
Loss in iteration 62 : 0.34022086334625706
Loss in iteration 63 : 0.3399592269130999
Loss in iteration 64 : 0.3397047441514636
Loss in iteration 65 : 0.3394571352185613
Loss in iteration 66 : 0.3392161345031907
Loss in iteration 67 : 0.3389814897187571
Loss in iteration 68 : 0.33875296106795577
Loss in iteration 69 : 0.3385303204722137
Loss in iteration 70 : 0.33831335085971603
Loss in iteration 71 : 0.33810184550664896
Loss in iteration 72 : 0.3378956074267543
Loss in iteration 73 : 0.33769444880492505
Loss in iteration 74 : 0.33749819047095064
Loss in iteration 75 : 0.3373066614099796
Loss in iteration 76 : 0.33711969830659105
Loss in iteration 77 : 0.3369371451196871
Loss in iteration 78 : 0.3367588526856969
Loss in iteration 79 : 0.33658467834781597
Loss in iteration 80 : 0.33641448560924847
Loss in iteration 81 : 0.33624814380856377
Loss in iteration 82 : 0.3360855278155239
Loss in iteration 83 : 0.3359265177457852
Loss in iteration 84 : 0.33577099869316024
Loss in iteration 85 : 0.33561886047806855
Loss in iteration 86 : 0.3354699974111051
Loss in iteration 87 : 0.3353243080705922
Loss in iteration 88 : 0.3351816950931625
Loss in iteration 89 : 0.33504206497649003
Loss in iteration 90 : 0.3349053278933235
Loss in iteration 91 : 0.33477139751606716
Loss in iteration 92 : 0.3346401908512218
Loss in iteration 93 : 0.33451162808303303
Loss in iteration 94 : 0.33438563242575736
Loss in iteration 95 : 0.3342621299839687
Loss in iteration 96 : 0.33414104962044977
Loss in iteration 97 : 0.3340223228311344
Loss in iteration 98 : 0.3339058836267066
Loss in iteration 99 : 0.33379166842043706
Loss in iteration 100 : 0.33367961592186274
Loss in iteration 101 : 0.3335696670359757
Loss in iteration 102 : 0.3334617647676024
Loss in iteration 103 : 0.3333558541306328
Loss in iteration 104 : 0.3332518820618591
Loss in iteration 105 : 0.3331497973391056
Loss in iteration 106 : 0.3330495505034616
Loss in iteration 107 : 0.33295109378534926
Loss in iteration 108 : 0.33285438103419723
Loss in iteration 109 : 0.3327593676515509
Loss in iteration 110 : 0.33266601052742295
Loss in iteration 111 : 0.3325742679796633
Loss in iteration 112 : 0.3324840996962599
Loss in iteration 113 : 0.33239546668032327
Loss in iteration 114 : 0.3323083311976863
Loss in iteration 115 : 0.3322226567269368
Loss in iteration 116 : 0.33213840791176097
Loss in iteration 117 : 0.33205555051546776
Loss in iteration 118 : 0.3319740513776274
Loss in iteration 119 : 0.33189387837263046
Loss in iteration 120 : 0.33181500037015105
Loss in iteration 121 : 0.331737387197368
Loss in iteration 122 : 0.33166100960284955
Loss in iteration 123 : 0.33158583922207124
Loss in iteration 124 : 0.33151184854441756
Loss in iteration 125 : 0.33143901088162603
Loss in iteration 126 : 0.33136730033762857
Loss in iteration 127 : 0.3312966917796465
Loss in iteration 128 : 0.3312271608105542
Loss in iteration 129 : 0.3311586837424284
Loss in iteration 130 : 0.33109123757116404
Loss in iteration 131 : 0.33102479995222495
Loss in iteration 132 : 0.3309593491773531
Loss in iteration 133 : 0.3308948641522736
Loss in iteration 134 : 0.33083132437530055
Loss in iteration 135 : 0.3307687099168416
Loss in iteration 136 : 0.33070700139971865
Loss in iteration 137 : 0.3306461799802963
Loss in iteration 138 : 0.3305862273303626
Loss in iteration 139 : 0.3305271256197453
Loss in iteration 140 : 0.33046885749960225
Loss in iteration 141 : 0.3304114060863979
Loss in iteration 142 : 0.3303547549464861
Loss in iteration 143 : 0.33029888808130803
Loss in iteration 144 : 0.3302437899131634
Loss in iteration 145 : 0.3301894452715282
Loss in iteration 146 : 0.33013583937990054
Loss in iteration 147 : 0.3300829578431487
Loss in iteration 148 : 0.33003078663533825
Loss in iteration 149 : 0.3299793120880199
Loss in iteration 150 : 0.32992852087895647
Loss in iteration 151 : 0.3298784000212693
Loss in iteration 152 : 0.32982893685299147
Loss in iteration 153 : 0.32978011902700033
Loss in iteration 154 : 0.32973193450132526
Loss in iteration 155 : 0.32968437152980135
Loss in iteration 156 : 0.32963741865307433
Loss in iteration 157 : 0.3295910646899158
Loss in iteration 158 : 0.3295452987288623
Loss in iteration 159 : 0.32950011012014774
Loss in iteration 160 : 0.32945548846791706
Loss in iteration 161 : 0.32941142362271747
Loss in iteration 162 : 0.3293679056742707
Loss in iteration 163 : 0.32932492494445903
Loss in iteration 164 : 0.3292824719805937
Loss in iteration 165 : 0.3292405375488948
Loss in iteration 166 : 0.32919911262820434
Loss in iteration 167 : 0.32915818840389877
Loss in iteration 168 : 0.3291177562620303
Loss in iteration 169 : 0.32907780778364637
Loss in iteration 170 : 0.3290383347393201
Loss in iteration 171 : 0.3289993290838364
Loss in iteration 172 : 0.32896078295108344
Loss in iteration 173 : 0.32892268864909896
Loss in iteration 174 : 0.32888503865527546
Loss in iteration 175 : 0.3288478256117286
Loss in iteration 176 : 0.328811042320818
Loss in iteration 177 : 0.32877468174081503
Loss in iteration 178 : 0.3287387369816933
Loss in iteration 179 : 0.3287032013010813
Loss in iteration 180 : 0.32866806810032473
Loss in iteration 181 : 0.32863333092066854
Loss in iteration 182 : 0.3285989834395838
Loss in iteration 183 : 0.3285650194671956
Loss in iteration 184 : 0.3285314329428073
Loss in iteration 185 : 0.3284982179315706
Loss in iteration 186 : 0.32846536862121345
Loss in iteration 187 : 0.32843287931891346
Loss in iteration 188 : 0.3284007444482339
Loss in iteration 189 : 0.328368958546158
Loss in iteration 190 : 0.3283375162602398
Loss in iteration 191 : 0.32830641234580127
Loss in iteration 192 : 0.32827564166324974
Loss in iteration 193 : 0.32824519917544065
Loss in iteration 194 : 0.3282150799451595
Loss in iteration 195 : 0.3281852791326341
Loss in iteration 196 : 0.3281557919931617
Loss in iteration 197 : 0.32812661387477315
Loss in iteration 198 : 0.3280977402159827
Loss in iteration 199 : 0.32806916654360047
Loss in iteration 200 : 0.32804088847059953
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.8519746944290891, training accuracy 0.8463759213759213, time elapsed: 4186 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5255663182148818
Loss in iteration 3 : 0.5001426921541264
Loss in iteration 4 : 0.47981562506048786
Loss in iteration 5 : 0.4634152703292112
Loss in iteration 6 : 0.4500375527154957
Loss in iteration 7 : 0.43899724150960046
Loss in iteration 8 : 0.42977888776496986
Loss in iteration 9 : 0.4219944611329299
Loss in iteration 10 : 0.41535030269232853
Loss in iteration 11 : 0.40962244116565255
Loss in iteration 12 : 0.4046385042868514
Loss in iteration 13 : 0.40026455290672397
Loss in iteration 14 : 0.3963954973177354
Loss in iteration 15 : 0.39294809254872987
Loss in iteration 16 : 0.38985578563708045
Loss in iteration 17 : 0.3870648957836501
Loss in iteration 18 : 0.3845317586607231
Loss in iteration 19 : 0.3822205728577083
Loss in iteration 20 : 0.3801017615919438
Loss in iteration 21 : 0.3781507156436964
Loss in iteration 22 : 0.376346820705508
Loss in iteration 23 : 0.3746726986956929
Loss in iteration 24 : 0.37311361136216487
Loss in iteration 25 : 0.3716569879703177
Loss in iteration 26 : 0.3702920486004107
Loss in iteration 27 : 0.3690095016656847
Loss in iteration 28 : 0.36780129946170553
Loss in iteration 29 : 0.36666043940142923
Loss in iteration 30 : 0.3655808014541489
Loss in iteration 31 : 0.36455701445518396
Loss in iteration 32 : 0.36358434557738006
Loss in iteration 33 : 0.3626586084912098
Loss in iteration 34 : 0.36177608668718203
Loss in iteration 35 : 0.36093346916422436
Loss in iteration 36 : 0.3601277962540433
Loss in iteration 37 : 0.35935641379353583
Loss in iteration 38 : 0.35861693420419344
Loss in iteration 39 : 0.3579072033114026
Loss in iteration 40 : 0.35722527195362264
Loss in iteration 41 : 0.3565693716050297
Loss in iteration 42 : 0.35593789337374776
Loss in iteration 43 : 0.35532936984991
Loss in iteration 44 : 0.3547424593680663
Loss in iteration 45 : 0.35417593232208944
Loss in iteration 46 : 0.35362865923057063
Loss in iteration 47 : 0.35309960029992826
Loss in iteration 48 : 0.3525877962727639
Loss in iteration 49 : 0.35209236038241065
Loss in iteration 50 : 0.3516124712621937
Loss in iteration 51 : 0.35114736668093355
Loss in iteration 52 : 0.3506963379953565
Loss in iteration 53 : 0.35025872522613744
Loss in iteration 54 : 0.3498339126777332
Loss in iteration 55 : 0.34942132503351225
Loss in iteration 56 : 0.3490204238672877
Loss in iteration 57 : 0.3486307045204593
Loss in iteration 58 : 0.34825169330079814
Loss in iteration 59 : 0.34788294496487876
Loss in iteration 60 : 0.34752404045109286
Loss in iteration 61 : 0.3471745848345135
Loss in iteration 62 : 0.34683420547846816
Loss in iteration 63 : 0.3465025503610052
Loss in iteration 64 : 0.3461792865569498
Loss in iteration 65 : 0.34586409885883446
Loss in iteration 66 : 0.3455566885217771
Loss in iteration 67 : 0.34525677211936945
Loss in iteration 68 : 0.34496408049905497
Loss in iteration 69 : 0.34467835782679895
Loss in iteration 70 : 0.34439936071211236
Loss in iteration 71 : 0.3441268574054157
Loss in iteration 72 : 0.3438606270606745
Loss in iteration 73 : 0.343600459057031
Loss in iteration 74 : 0.34334615237376725
Loss in iteration 75 : 0.34309751501365027
Loss in iteration 76 : 0.3428543634701585
Loss in iteration 77 : 0.34261652223457895
Loss in iteration 78 : 0.3423838233394169
Loss in iteration 79 : 0.3421561059348756
Loss in iteration 80 : 0.3419332158955225
Loss in iteration 81 : 0.3417150054545569
Loss in iteration 82 : 0.3415013328632903
Loss in iteration 83 : 0.341292062073798
Loss in iteration 84 : 0.34108706244273723
Loss in iteration 85 : 0.340886208454699
Loss in iteration 86 : 0.3406893794634657
Loss in iteration 87 : 0.34049645944978363
Loss in iteration 88 : 0.3403073367943352
Loss in iteration 89 : 0.34012190406480003
Loss in iteration 90 : 0.3399400578158541
Loss in iteration 91 : 0.33976169840122467
Loss in iteration 92 : 0.3395867297968287
Loss in iteration 93 : 0.3394150594342769
Loss in iteration 94 : 0.33924659804392454
Loss in iteration 95 : 0.3390812595068315
Loss in iteration 96 : 0.3389189607150177
Loss in iteration 97 : 0.3387596214394203
Loss in iteration 98 : 0.3386031642050442
Loss in iteration 99 : 0.33844951417283115
Loss in iteration 100 : 0.33829859902778847
Loss in iteration 101 : 0.33815034887296863
Loss in iteration 102 : 0.33800469612897316
Loss in iteration 103 : 0.3378615754385423
Loss in iteration 104 : 0.33772092357600464
Loss in iteration 105 : 0.33758267936121517
Loss in iteration 106 : 0.33744678357776703
Loss in iteration 107 : 0.3373131788951585
Loss in iteration 108 : 0.3371818097947674
Loss in iteration 109 : 0.3370526224992834
Loss in iteration 110 : 0.33692556490554415
Loss in iteration 111 : 0.3368005865204714
Loss in iteration 112 : 0.3366776383999612
Loss in iteration 113 : 0.3365566730906013
Loss in iteration 114 : 0.3364376445740087
Loss in iteration 115 : 0.33632050821368686
Loss in iteration 116 : 0.3362052207041986
Loss in iteration 117 : 0.33609174002264597
Loss in iteration 118 : 0.33598002538222155
Loss in iteration 119 : 0.33587003718777897
Loss in iteration 120 : 0.33576173699331846
Loss in iteration 121 : 0.3356550874612775
Loss in iteration 122 : 0.3355500523235308
Loss in iteration 123 : 0.3354465963440325
Loss in iteration 124 : 0.33534468528301264
Loss in iteration 125 : 0.3352442858626227
Loss in iteration 126 : 0.33514536573401904
Loss in iteration 127 : 0.33504789344574704
Loss in iteration 128 : 0.3349518384134325
Loss in iteration 129 : 0.33485717089065853
Loss in iteration 130 : 0.334763861940997
Loss in iteration 131 : 0.3346718834111727
Loss in iteration 132 : 0.3345812079052181
Loss in iteration 133 : 0.33449180875971046
Loss in iteration 134 : 0.33440366001989036
Loss in iteration 135 : 0.3343167364167437
Loss in iteration 136 : 0.33423101334493327
Loss in iteration 137 : 0.33414646684157623
Loss in iteration 138 : 0.33406307356581405
Loss in iteration 139 : 0.3339808107791479
Loss in iteration 140 : 0.3338996563265063
Loss in iteration 141 : 0.33381958861800026
Loss in iteration 142 : 0.33374058661136674
Loss in iteration 143 : 0.33366262979503813
Loss in iteration 144 : 0.33358569817183065
Loss in iteration 145 : 0.3335097722432203
Loss in iteration 146 : 0.3334348329941787
Loss in iteration 147 : 0.3333608618785604
Loss in iteration 148 : 0.3332878408049845
Loss in iteration 149 : 0.33321575212325877
Loss in iteration 150 : 0.3331445786112205
Loss in iteration 151 : 0.33307430346208683
Loss in iteration 152 : 0.33300491027221396
Loss in iteration 153 : 0.3329363830292973
Loss in iteration 154 : 0.33286870610095753
Loss in iteration 155 : 0.33280186422373687
Loss in iteration 156 : 0.3327358424924507
Loss in iteration 157 : 0.3326706263499136
Loss in iteration 158 : 0.33260620157700416
Loss in iteration 159 : 0.3325425542830524
Loss in iteration 160 : 0.33247967089658226
Loss in iteration 161 : 0.33241753815629965
Loss in iteration 162 : 0.33235614310245315
Loss in iteration 163 : 0.33229547306840324
Loss in iteration 164 : 0.33223551567253135
Loss in iteration 165 : 0.3321762588103639
Loss in iteration 166 : 0.3321176906469755
Loss in iteration 167 : 0.3320597996096368
Loss in iteration 168 : 0.33200257438067665
Loss in iteration 169 : 0.3319460038906078
Loss in iteration 170 : 0.3318900773114353
Loss in iteration 171 : 0.33183478405018957
Loss in iteration 172 : 0.3317801137426743
Loss in iteration 173 : 0.3317260562473928
Loss in iteration 174 : 0.3316726016396669
Loss in iteration 175 : 0.3316197402059574
Loss in iteration 176 : 0.3315674624383197
Loss in iteration 177 : 0.33151575902908237
Loss in iteration 178 : 0.33146462086562783
Loss in iteration 179 : 0.33141403902539607
Loss in iteration 180 : 0.3313640047709882
Loss in iteration 181 : 0.33131450954544256
Loss in iteration 182 : 0.33126554496764715
Loss in iteration 183 : 0.3312171028279003
Loss in iteration 184 : 0.3311691750835715
Loss in iteration 185 : 0.33112175385493386
Loss in iteration 186 : 0.3310748314210908
Loss in iteration 187 : 0.33102840021601876
Loss in iteration 188 : 0.33098245282476285
Loss in iteration 189 : 0.3309369819796884
Loss in iteration 190 : 0.3308919805568928
Loss in iteration 191 : 0.33084744157268725
Loss in iteration 192 : 0.33080335818018936
Loss in iteration 193 : 0.33075972366602974
Loss in iteration 194 : 0.3307165314471228
Loss in iteration 195 : 0.3306737750675418
Loss in iteration 196 : 0.330631448195506
Loss in iteration 197 : 0.33058954462041196
Loss in iteration 198 : 0.3305480582499671
Loss in iteration 199 : 0.3305069831074235
Loss in iteration 200 : 0.3304663133288449
Testing accuracy  of updater 0 on alg 0 with rate 0.7 = 0.8505005835022419, training accuracy 0.8449324324324324, time elapsed: 4936 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5623723947519088
Loss in iteration 3 : 0.5280050092309497
Loss in iteration 4 : 0.5097231617292561
Loss in iteration 5 : 0.4959914299184839
Loss in iteration 6 : 0.48440337163662994
Loss in iteration 7 : 0.4742614442739055
Loss in iteration 8 : 0.46527119125180183
Loss in iteration 9 : 0.4572536180477686
Loss in iteration 10 : 0.45007352254360167
Loss in iteration 11 : 0.4436195807604925
Loss in iteration 12 : 0.43779758273110436
Loss in iteration 13 : 0.43252716557800186
Loss in iteration 14 : 0.42773959622249547
Loss in iteration 15 : 0.4233759895148485
Loss in iteration 16 : 0.41938580420927785
Loss in iteration 17 : 0.41572556521713144
Loss in iteration 18 : 0.4123577820228032
Loss in iteration 19 : 0.4092500370211853
Loss in iteration 20 : 0.40637421921976025
Loss in iteration 21 : 0.40370588093299137
Loss in iteration 22 : 0.4012236977539163
Loss in iteration 23 : 0.3989090148492068
Loss in iteration 24 : 0.39674546523097604
Loss in iteration 25 : 0.39471864798765616
Loss in iteration 26 : 0.39281585646951694
Loss in iteration 27 : 0.3910258481302023
Loss in iteration 28 : 0.3893386491529155
Loss in iteration 29 : 0.38774538817490456
Loss in iteration 30 : 0.3862381544032012
Loss in iteration 31 : 0.3848098762215475
Loss in iteration 32 : 0.3834542170527585
Loss in iteration 33 : 0.3821654857873132
Loss in iteration 34 : 0.3809385595388088
Loss in iteration 35 : 0.3797688168577804
Loss in iteration 36 : 0.37865207984100435
Loss in iteration 37 : 0.37758456382637773
Loss in iteration 38 : 0.3765628335725285
Loss in iteration 39 : 0.3755837649960321
Loss in iteration 40 : 0.37464451168337726
Loss in iteration 41 : 0.3737424755150097
Loss in iteration 42 : 0.37287528083928506
Loss in iteration 43 : 0.37204075171817286
Loss in iteration 44 : 0.3712368918369994
Loss in iteration 45 : 0.37046186672991716
Loss in iteration 46 : 0.36971398802269473
Loss in iteration 47 : 0.3689916994366318
Loss in iteration 48 : 0.36829356433324406
Loss in iteration 49 : 0.3676182546096175
Loss in iteration 50 : 0.3669645407801989
Loss in iteration 51 : 0.36633128310275964
Loss in iteration 52 : 0.36571742362512943
Loss in iteration 53 : 0.36512197904532423
Loss in iteration 54 : 0.3645440342915778
Loss in iteration 55 : 0.3639827367406195
Loss in iteration 56 : 0.36343729100276556
Loss in iteration 57 : 0.362906954211304
Loss in iteration 58 : 0.36239103176118864
Loss in iteration 59 : 0.3618888734487651
Loss in iteration 60 : 0.36139986996996576
Loss in iteration 61 : 0.3609234497394197
Loss in iteration 62 : 0.3604590759972885
Loss in iteration 63 : 0.3600062441744445
Loss in iteration 64 : 0.359564479489935
Loss in iteration 65 : 0.35913333475757736
Loss in iteration 66 : 0.358712388381132
Loss in iteration 67 : 0.35830124251970125
Loss in iteration 68 : 0.35789952140700215
Loss in iteration 69 : 0.357506869809904
Loss in iteration 70 : 0.3571229516131799
Loss in iteration 71 : 0.35674744851875334
Loss in iteration 72 : 0.356380058848924
Loss in iteration 73 : 0.3560204964442175
Loss in iteration 74 : 0.35566848964727243
Loss in iteration 75 : 0.35532378036525103
Loss in iteration 76 : 0.35498612320378414
Loss in iteration 77 : 0.3546552846663414
Loss in iteration 78 : 0.3543310424133561
Loss in iteration 79 : 0.35401318457603514
Loss in iteration 80 : 0.35370150912029835
Loss in iteration 81 : 0.3533958232566603
Loss in iteration 82 : 0.3530959428922513
Loss in iteration 83 : 0.35280169212159335
Loss in iteration 84 : 0.3525129027529864
Loss in iteration 85 : 0.3522294138676615
Loss in iteration 86 : 0.35195107140911175
Loss in iteration 87 : 0.35167772780026507
Loss in iteration 88 : 0.35140924158630715
Loss in iteration 89 : 0.3511454771011994
Loss in iteration 90 : 0.3508863041561403
Loss in iteration 91 : 0.35063159774822994
Loss in iteration 92 : 0.35038123778790425
Loss in iteration 93 : 0.35013510884372995
Loss in iteration 94 : 0.34989309990329526
Loss in iteration 95 : 0.3496551041490062
Loss in iteration 96 : 0.3494210187477585
Loss in iteration 97 : 0.349190744653474
Loss in iteration 98 : 0.3489641864215852
Loss in iteration 99 : 0.3487412520346667
Loss in iteration 100 : 0.3485218527384033
Loss in iteration 101 : 0.3483059028872185
Loss in iteration 102 : 0.3480933197988674
Loss in iteration 103 : 0.3478840236174556
Loss in iteration 104 : 0.3476779371842305
Loss in iteration 105 : 0.3474749859157083
Loss in iteration 106 : 0.3472750976886148
Loss in iteration 107 : 0.3470782027312026
Loss in iteration 108 : 0.3468842335205468
Loss in iteration 109 : 0.34669312468541064
Loss in iteration 110 : 0.346504812914352
Loss in iteration 111 : 0.3463192368687293
Loss in iteration 112 : 0.34613633710028924
Loss in iteration 113 : 0.3459560559730941
Loss in iteration 114 : 0.3457783375894643
Loss in iteration 115 : 0.3456031277197354
Loss in iteration 116 : 0.34543037373557867
Loss in iteration 117 : 0.34526002454668997
Loss in iteration 118 : 0.34509203054060383
Loss in iteration 119 : 0.3449263435255334
Loss in iteration 120 : 0.34476291667593795
Loss in iteration 121 : 0.34460170448078536
Loss in iteration 122 : 0.34444266269425355
Loss in iteration 123 : 0.3442857482888125
Loss in iteration 124 : 0.34413091941047624
Loss in iteration 125 : 0.3439781353361675
Loss in iteration 126 : 0.3438273564330378
Loss in iteration 127 : 0.34367854411966436
Loss in iteration 128 : 0.3435316608289716
Loss in iteration 129 : 0.3433866699728642
Loss in iteration 130 : 0.3432435359083709
Loss in iteration 131 : 0.34310222390535344
Loss in iteration 132 : 0.34296270011555735
Loss in iteration 133 : 0.3428249315430316
Loss in iteration 134 : 0.34268888601581493
Loss in iteration 135 : 0.34255453215881465
Loss in iteration 136 : 0.34242183936782755
Loss in iteration 137 : 0.34229077778463896
Loss in iteration 138 : 0.3421613182731582
Loss in iteration 139 : 0.3420334323965179
Loss in iteration 140 : 0.34190709239510186
Loss in iteration 141 : 0.34178227116546483
Loss in iteration 142 : 0.341658942240078
Loss in iteration 143 : 0.34153707976786746
Loss in iteration 144 : 0.3414166584955347
Loss in iteration 145 : 0.34129765374957927
Loss in iteration 146 : 0.34118004141901126
Loss in iteration 147 : 0.3410637979387381
Loss in iteration 148 : 0.34094890027354424
Loss in iteration 149 : 0.340835325902709
Loss in iteration 150 : 0.3407230528051524
Loss in iteration 151 : 0.3406120594451686
Loss in iteration 152 : 0.34050232475862574
Loss in iteration 153 : 0.34039382813971525
Loss in iteration 154 : 0.3402865494281365
Loss in iteration 155 : 0.34018046889676146
Loss in iteration 156 : 0.34007556723969806
Loss in iteration 157 : 0.3399718255608136
Loss in iteration 158 : 0.3398692253626202
Loss in iteration 159 : 0.33976774853555053
Loss in iteration 160 : 0.33966737734759933
Loss in iteration 161 : 0.3395680944343194
Loss in iteration 162 : 0.339469882789139
Loss in iteration 163 : 0.3393727257540001
Loss in iteration 164 : 0.3392766070103278
Loss in iteration 165 : 0.339181510570252
Loss in iteration 166 : 0.3390874207681595
Loss in iteration 167 : 0.3389943222524798
Loss in iteration 168 : 0.33890219997775556
Loss in iteration 169 : 0.3388110391969498
Loss in iteration 170 : 0.3387208254540007
Loss in iteration 171 : 0.3386315445766092
Loss in iteration 172 : 0.3385431826692499
Loss in iteration 173 : 0.33845572610637814
Loss in iteration 174 : 0.33836916152587854
Loss in iteration 175 : 0.33828347582267665
Loss in iteration 176 : 0.3381986561425664
Loss in iteration 177 : 0.3381146898762076
Loss in iteration 178 : 0.33803156465330514
Loss in iteration 179 : 0.3379492683369649
Loss in iteration 180 : 0.33786778901819486
Loss in iteration 181 : 0.33778711501059705
Loss in iteration 182 : 0.33770723484518117
Loss in iteration 183 : 0.33762813726534635
Loss in iteration 184 : 0.33754981122199496
Loss in iteration 185 : 0.33747224586878394
Loss in iteration 186 : 0.33739543055752325
Loss in iteration 187 : 0.3373193548336803
Loss in iteration 188 : 0.33724400843203844
Loss in iteration 189 : 0.3371693812724287
Loss in iteration 190 : 0.33709546345562746
Loss in iteration 191 : 0.33702224525934427
Loss in iteration 192 : 0.33694971713430033
Loss in iteration 193 : 0.3368778697004558
Loss in iteration 194 : 0.3368066937432826
Loss in iteration 195 : 0.33673618021019025
Loss in iteration 196 : 0.3366663202070034
Loss in iteration 197 : 0.3365971049945571
Loss in iteration 198 : 0.33652852598537125
Loss in iteration 199 : 0.3364605747404088
Loss in iteration 200 : 0.33639324296592704
Testing accuracy  of updater 0 on alg 0 with rate 0.4 = 0.8454026165468952, training accuracy 0.8421068796068796, time elapsed: 4481 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6509822051128222
Loss in iteration 3 : 0.619779051756544
Loss in iteration 4 : 0.5963224403342656
Loss in iteration 5 : 0.5783439049270137
Loss in iteration 6 : 0.5642637446520444
Loss in iteration 7 : 0.552984463900704
Loss in iteration 8 : 0.5437406715945391
Loss in iteration 9 : 0.535994765094726
Loss in iteration 10 : 0.5293658102006733
Loss in iteration 11 : 0.5235813245848358
Loss in iteration 12 : 0.5184445275875502
Loss in iteration 13 : 0.5138119618532951
Loss in iteration 14 : 0.509578077305628
Loss in iteration 15 : 0.5056645141372714
Loss in iteration 16 : 0.5020125821730204
Loss in iteration 17 : 0.4985779342142503
Loss in iteration 18 : 0.4953267597265628
Loss in iteration 19 : 0.49223304216735253
Loss in iteration 20 : 0.4892765673839211
Loss in iteration 21 : 0.4864414670852677
Loss in iteration 22 : 0.4837151466950824
Loss in iteration 23 : 0.48108749147805946
Loss in iteration 24 : 0.47855027556311913
Loss in iteration 25 : 0.4760967198711903
Loss in iteration 26 : 0.47372115997051106
Loss in iteration 27 : 0.4714187955189669
Loss in iteration 28 : 0.4691855005495949
Loss in iteration 29 : 0.4670176793239062
Loss in iteration 30 : 0.46491215644244016
Loss in iteration 31 : 0.462866092796256
Loss in iteration 32 : 0.46087692106892697
Loss in iteration 33 : 0.45894229606883413
Loss in iteration 34 : 0.4570600563376163
Loss in iteration 35 : 0.4552281943503443
Loss in iteration 36 : 0.4534448332745742
Loss in iteration 37 : 0.45170820874524453
Loss in iteration 38 : 0.4500166544819759
Loss in iteration 39 : 0.4483685908548501
Loss in iteration 40 : 0.44676251571672926
Loss in iteration 41 : 0.44519699698141446
Loss in iteration 42 : 0.4436706665494948
Loss in iteration 43 : 0.44218221527733864
Loss in iteration 44 : 0.44073038875616505
Loss in iteration 45 : 0.43931398372258534
Loss in iteration 46 : 0.43793184496383747
Loss in iteration 47 : 0.43658286261281376
Loss in iteration 48 : 0.43526596975243304
Loss in iteration 49 : 0.43398014026764586
Loss in iteration 50 : 0.43272438689756965
Loss in iteration 51 : 0.43149775945133956
Loss in iteration 52 : 0.43029934315950713
Loss in iteration 53 : 0.4291282571393015
Loss in iteration 54 : 0.42798365295692237
Loss in iteration 55 : 0.42686471327376097
Loss in iteration 56 : 0.4257706505663162
Loss in iteration 57 : 0.4247007059116982
Loss in iteration 58 : 0.4236541478323447
Loss in iteration 59 : 0.4226302711947122
Loss in iteration 60 : 0.42162839615780084
Loss in iteration 61 : 0.4206478671679502
Loss in iteration 62 : 0.41968805199707815
Loss in iteration 63 : 0.4187483408217634
Loss in iteration 64 : 0.4178281453410574
Loss in iteration 65 : 0.416926897931072
Loss in iteration 66 : 0.41604405083463414
Loss in iteration 67 : 0.4151790753843762
Loss in iteration 68 : 0.41433146125788617
Loss in iteration 69 : 0.41350071576346087
Loss in iteration 70 : 0.4126863631552747
Loss in iteration 71 : 0.4118879439766629
Loss in iteration 72 : 0.41110501443044034
Loss in iteration 73 : 0.41033714577509
Loss in iteration 74 : 0.4095839237458105
Loss in iteration 75 : 0.4088449479993623
Loss in iteration 76 : 0.40811983158178033
Loss in iteration 77 : 0.40740820041795955
Loss in iteration 78 : 0.40670969282226976
Loss in iteration 79 : 0.40602395902926525
Loss in iteration 80 : 0.405350660743692
Loss in iteration 81 : 0.4046894707089881
Loss in iteration 82 : 0.4040400722934434
Loss in iteration 83 : 0.4034021590933795
Loss in iteration 84 : 0.4027754345524956
Loss in iteration 85 : 0.40215961159681063
Loss in iteration 86 : 0.40155441228449684
Loss in iteration 87 : 0.4009595674699313
Loss in iteration 88 : 0.40037481648142037
Loss in iteration 89 : 0.3997999068119872
Loss in iteration 90 : 0.3992345938226375
Loss in iteration 91 : 0.398678640457636
Loss in iteration 92 : 0.39813181697118394
Loss in iteration 93 : 0.3975939006651001
Loss in iteration 94 : 0.3970646756369867
Loss in iteration 95 : 0.39654393253843007
Loss in iteration 96 : 0.396031468342834
Loss in iteration 97 : 0.3955270861224234
Loss in iteration 98 : 0.39503059483410685
Loss in iteration 99 : 0.39454180911373393
Loss in iteration 100 : 0.3940605490784329
Loss in iteration 101 : 0.3935866401367034
Loss in iteration 102 : 0.3931199128058791
Loss in iteration 103 : 0.39266020253671885
Loss in iteration 104 : 0.39220734954473885
Loss in iteration 105 : 0.39176119864809367
Loss in iteration 106 : 0.3913215991116536
Loss in iteration 107 : 0.3908884044970716
Loss in iteration 108 : 0.39046147251856017
Loss in iteration 109 : 0.39004066490412603
Loss in iteration 110 : 0.38962584726209337
Loss in iteration 111 : 0.3892168889526263
Loss in iteration 112 : 0.38881366296409964
Loss in iteration 113 : 0.3884160457940592
Loss in iteration 114 : 0.3880239173346572
Loss in iteration 115 : 0.3876371607623131
Loss in iteration 116 : 0.38725566243144877
Loss in iteration 117 : 0.38687931177215623
Loss in iteration 118 : 0.38650800119158774
Loss in iteration 119 : 0.38614162597897317
Loss in iteration 120 : 0.38578008421406684
Loss in iteration 121 : 0.38542327667894277
Loss in iteration 122 : 0.38507110677292566
Loss in iteration 123 : 0.38472348043061816
Loss in iteration 124 : 0.3843803060428215
Loss in iteration 125 : 0.38404149438032603
Loss in iteration 126 : 0.38370695852034586
Loss in iteration 127 : 0.3833766137755962
Loss in iteration 128 : 0.38305037762583766
Loss in iteration 129 : 0.3827281696518407
Loss in iteration 130 : 0.38240991147163833
Loss in iteration 131 : 0.3820955266790079
Loss in iteration 132 : 0.3817849407840531
Loss in iteration 133 : 0.38147808115587994
Loss in iteration 134 : 0.38117487696719665
Loss in iteration 135 : 0.3808752591408225
Loss in iteration 136 : 0.3805791602980273
Loss in iteration 137 : 0.3802865147086048
Loss in iteration 138 : 0.37999725824264075
Loss in iteration 139 : 0.37971132832390425
Loss in iteration 140 : 0.3794286638847767
Loss in iteration 141 : 0.3791492053227238
Loss in iteration 142 : 0.3788728944581797
Loss in iteration 143 : 0.37859967449382703
Loss in iteration 144 : 0.37832948997522464
Loss in iteration 145 : 0.37806228675273124
Loss in iteration 146 : 0.3777980119446655
Loss in iteration 147 : 0.3775366139016537
Loss in iteration 148 : 0.37727804217215155
Loss in iteration 149 : 0.37702224746906304
Loss in iteration 150 : 0.37676918163744477
Loss in iteration 151 : 0.37651879762321994
Loss in iteration 152 : 0.3762710494429126
Loss in iteration 153 : 0.3760258921543399
Loss in iteration 154 : 0.37578328182821263
Loss in iteration 155 : 0.37554317552063843
Loss in iteration 156 : 0.37530553124650295
Loss in iteration 157 : 0.37507030795365515
Loss in iteration 158 : 0.37483746549790004
Loss in iteration 159 : 0.37460696461880444
Loss in iteration 160 : 0.3743787669161717
Loss in iteration 161 : 0.3741528348273166
Loss in iteration 162 : 0.37392913160499547
Loss in iteration 163 : 0.3737076212960118
Loss in iteration 164 : 0.37348826872046587
Loss in iteration 165 : 0.3732710394516718
Loss in iteration 166 : 0.37305589979660436
Loss in iteration 167 : 0.3728428167770154
Loss in iteration 168 : 0.37263175811104454
Loss in iteration 169 : 0.3724226921954235
Loss in iteration 170 : 0.37221558808818056
Loss in iteration 171 : 0.37201041549187974
Loss in iteration 172 : 0.37180714473731996
Loss in iteration 173 : 0.37160574676773434
Loss in iteration 174 : 0.371406193123462
Loss in iteration 175 : 0.37120845592702423
Loss in iteration 176 : 0.37101250786867224
Loss in iteration 177 : 0.37081832219233135
Loss in iteration 178 : 0.37062587268195823
Loss in iteration 179 : 0.3704351336482718
Loss in iteration 180 : 0.3702460799158957
Loss in iteration 181 : 0.3700586868108164
Loss in iteration 182 : 0.3698729301482463
Loss in iteration 183 : 0.36968878622078993
Loss in iteration 184 : 0.3695062317869752
Loss in iteration 185 : 0.3693252440600675
Loss in iteration 186 : 0.36914580069723424
Loss in iteration 187 : 0.36896787978897805
Loss in iteration 188 : 0.36879145984888345
Loss in iteration 189 : 0.36861651980363674
Loss in iteration 190 : 0.36844303898331987
Loss in iteration 191 : 0.36827099711196
Loss in iteration 192 : 0.3681003742983748
Loss in iteration 193 : 0.3679311510271989
Loss in iteration 194 : 0.36776330815022645
Loss in iteration 195 : 0.3675968268779306
Loss in iteration 196 : 0.36743168877124
Loss in iteration 197 : 0.36726787573352915
Loss in iteration 198 : 0.36710537000281634
Loss in iteration 199 : 0.3669441541441645
Loss in iteration 200 : 0.3667842110423049
Testing accuracy  of updater 0 on alg 0 with rate 0.09999999999999998 = 0.832626988514219, training accuracy 0.8330773955773956, time elapsed: 4509 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 3.410397480702917
Loss in iteration 3 : 2.35749750786848
Loss in iteration 4 : 2.4140370631307553
Loss in iteration 5 : 2.3012833616280024
Loss in iteration 6 : 1.7585581698565382
Loss in iteration 7 : 2.8324751372690136
Loss in iteration 8 : 1.9702641996248624
Loss in iteration 9 : 1.6343390000489357
Loss in iteration 10 : 2.5487900893899584
Loss in iteration 11 : 1.5808755818019093
Loss in iteration 12 : 1.6834303558899075
Loss in iteration 13 : 2.0137815231542926
Loss in iteration 14 : 1.3947777038011684
Loss in iteration 15 : 1.3814995819044409
Loss in iteration 16 : 1.5537815988916608
Loss in iteration 17 : 1.1289249591696342
Loss in iteration 18 : 1.5337245534823842
Loss in iteration 19 : 0.9910922222157842
Loss in iteration 20 : 1.559292432187011
Loss in iteration 21 : 0.898281458861102
Loss in iteration 22 : 1.257489171756002
Loss in iteration 23 : 0.7632754767465736
Loss in iteration 24 : 1.137874783048966
Loss in iteration 25 : 0.7614865000185247
Loss in iteration 26 : 0.9436441556443479
Loss in iteration 27 : 0.786248169477711
Loss in iteration 28 : 0.7841427864619321
Loss in iteration 29 : 0.8930517516357499
Loss in iteration 30 : 0.591764474569031
Loss in iteration 31 : 0.8029247022372723
Loss in iteration 32 : 1.1195634240786982
Loss in iteration 33 : 0.6699977131991677
Loss in iteration 34 : 0.4881115639749518
Loss in iteration 35 : 0.4510635881174934
Loss in iteration 36 : 0.47183546378420466
Loss in iteration 37 : 0.6222941060270578
Loss in iteration 38 : 0.9771576459467266
Loss in iteration 39 : 1.874751640309498
Loss in iteration 40 : 0.8390010570623453
Loss in iteration 41 : 3.229938790402298
Loss in iteration 42 : 0.8500077855645708
Loss in iteration 43 : 1.9468806775959488
Loss in iteration 44 : 1.039878541094821
Loss in iteration 45 : 1.9306762642994508
Loss in iteration 46 : 1.119126718747172
Loss in iteration 47 : 1.4391255191116545
Loss in iteration 48 : 1.4134510471032977
Loss in iteration 49 : 0.9038867234184532
Loss in iteration 50 : 1.5099500678652802
Loss in iteration 51 : 0.8454055263982456
Loss in iteration 52 : 1.2882354909548648
Loss in iteration 53 : 0.6959052897577445
Loss in iteration 54 : 1.4362613101541006
Loss in iteration 55 : 1.3376766686417276
Loss in iteration 56 : 1.1478309920678873
Loss in iteration 57 : 1.550227266724923
Loss in iteration 58 : 0.591307688621817
Loss in iteration 59 : 1.2646506548465808
Loss in iteration 60 : 0.6416558943403358
Loss in iteration 61 : 1.2293223812499705
Loss in iteration 62 : 0.766015429194518
Loss in iteration 63 : 0.9553645772445547
Loss in iteration 64 : 0.7700007037002286
Loss in iteration 65 : 0.5883580356770745
Loss in iteration 66 : 0.8914570954849097
Loss in iteration 67 : 0.5391412287596944
Loss in iteration 68 : 0.4554566153670231
Loss in iteration 69 : 0.699911172547153
Loss in iteration 70 : 1.2360803192591525
Loss in iteration 71 : 2.100780596317637
Loss in iteration 72 : 1.2222567744133916
Loss in iteration 73 : 2.9699106684076204
Loss in iteration 74 : 0.7587919693776194
Loss in iteration 75 : 1.9783817738687859
Loss in iteration 76 : 1.485967353053186
Loss in iteration 77 : 1.3028430973864495
Loss in iteration 78 : 1.709021062052623
Loss in iteration 79 : 1.1624488310230114
Loss in iteration 80 : 1.6475897100204966
Loss in iteration 81 : 1.1186473947210676
Loss in iteration 82 : 1.2166050511167377
Loss in iteration 83 : 0.9483108528965299
Loss in iteration 84 : 1.0894950143859043
Loss in iteration 85 : 0.9764777191909757
Loss in iteration 86 : 1.1685767663982445
Loss in iteration 87 : 0.645783403262357
Loss in iteration 88 : 0.8440901239967176
Loss in iteration 89 : 0.7665410919121142
Loss in iteration 90 : 0.5492827103663648
Loss in iteration 91 : 0.5299019790812457
Loss in iteration 92 : 0.6609054826444185
Loss in iteration 93 : 0.734590044974411
Loss in iteration 94 : 0.5621200805864535
Loss in iteration 95 : 0.4731934142492234
Loss in iteration 96 : 0.4638855932940539
Loss in iteration 97 : 0.6572712605662713
Loss in iteration 98 : 1.621937358102884
Loss in iteration 99 : 0.40769001056078963
Loss in iteration 100 : 1.0233917904622674
Loss in iteration 101 : 2.8000857817313842
Loss in iteration 102 : 2.308821337015552
Loss in iteration 103 : 1.5022530583185751
Loss in iteration 104 : 0.968701294895525
Loss in iteration 105 : 1.3920029611929146
Loss in iteration 106 : 1.391944684459645
Loss in iteration 107 : 0.9538995626501421
Loss in iteration 108 : 1.5716144374659973
Loss in iteration 109 : 0.9888469303506111
Loss in iteration 110 : 1.4445338373717374
Loss in iteration 111 : 0.7610104696239168
Loss in iteration 112 : 1.6167018106068938
Loss in iteration 113 : 0.7420534806444302
Loss in iteration 114 : 1.1454041428467185
Loss in iteration 115 : 0.6726499393390558
Loss in iteration 116 : 0.7950199587260088
Loss in iteration 117 : 1.1366363346268356
Loss in iteration 118 : 0.5755302802862695
Loss in iteration 119 : 1.6395772225331577
Loss in iteration 120 : 1.552729110366546
Loss in iteration 121 : 1.2279145194933379
Loss in iteration 122 : 1.664549898966656
Loss in iteration 123 : 0.858183190926129
Loss in iteration 124 : 1.4086147505304119
Loss in iteration 125 : 1.1250977594012417
Loss in iteration 126 : 1.0966716929013487
Loss in iteration 127 : 0.9227221978322031
Loss in iteration 128 : 1.0300334374283957
Loss in iteration 129 : 0.855546924455047
Loss in iteration 130 : 1.0910245647011776
Loss in iteration 131 : 0.5622377112983736
Loss in iteration 132 : 0.7852875802327333
Loss in iteration 133 : 0.9692808889224821
Loss in iteration 134 : 0.9384299659910647
Loss in iteration 135 : 0.47486083808782
Loss in iteration 136 : 0.9421958410933086
Loss in iteration 137 : 1.2727489189082113
Loss in iteration 138 : 0.6065886793210846
Loss in iteration 139 : 1.7433907762647145
Loss in iteration 140 : 0.985687247116522
Loss in iteration 141 : 1.167357037454443
Loss in iteration 142 : 0.9533509805211456
Loss in iteration 143 : 0.7600346040014881
Loss in iteration 144 : 1.0146258174309504
Loss in iteration 145 : 0.6398021294160345
Loss in iteration 146 : 1.2987116080578198
Loss in iteration 147 : 0.9197527228716296
Loss in iteration 148 : 0.6393804855729875
Loss in iteration 149 : 1.7441741889076303
Loss in iteration 150 : 1.4432701365911418
Loss in iteration 151 : 1.116754138460555
Loss in iteration 152 : 1.7629796010776841
Loss in iteration 153 : 0.6914771710268012
Loss in iteration 154 : 1.405898894925372
Loss in iteration 155 : 0.9331780707256759
Loss in iteration 156 : 1.2333552743156468
Loss in iteration 157 : 0.8033251710989179
Loss in iteration 158 : 1.0497695411760326
Loss in iteration 159 : 0.7869449191791649
Loss in iteration 160 : 1.0671835814596706
Loss in iteration 161 : 0.5561476961253952
Loss in iteration 162 : 0.7967613223616761
Loss in iteration 163 : 0.8859064109135851
Loss in iteration 164 : 0.7198193750448174
Loss in iteration 165 : 0.46912565481630475
Loss in iteration 166 : 0.5127673468543817
Loss in iteration 167 : 0.8140958720494978
Loss in iteration 168 : 0.7484299443994256
Loss in iteration 169 : 0.7007506318036335
Loss in iteration 170 : 0.47963557046501687
Loss in iteration 171 : 0.5232112556950375
Loss in iteration 172 : 0.8480124653398634
Loss in iteration 173 : 0.7965182232701211
Loss in iteration 174 : 1.0192928964398638
Loss in iteration 175 : 0.5071525838646881
Loss in iteration 176 : 0.41421922228790997
Loss in iteration 177 : 0.3984782633968694
Loss in iteration 178 : 0.4297457638738254
Loss in iteration 179 : 0.7367909282526147
Loss in iteration 180 : 1.2677230187712947
Loss in iteration 181 : 2.6760925122795927
Loss in iteration 182 : 2.2131847480754097
Loss in iteration 183 : 1.6338568655460857
Loss in iteration 184 : 1.0536446911021202
Loss in iteration 185 : 1.569397386567615
Loss in iteration 186 : 1.6135820062611324
Loss in iteration 187 : 0.9975179431161726
Loss in iteration 188 : 1.8695473130032858
Loss in iteration 189 : 0.9614614215782549
Loss in iteration 190 : 1.5457020996373876
Loss in iteration 191 : 0.9677995541949569
Loss in iteration 192 : 1.34686946061329
Loss in iteration 193 : 0.7680287199580264
Loss in iteration 194 : 1.2853695911980312
Loss in iteration 195 : 0.8099578974723463
Loss in iteration 196 : 1.6186478173612342
Loss in iteration 197 : 0.7264248331907732
Loss in iteration 198 : 0.9867537865051947
Loss in iteration 199 : 0.8148201205396497
Loss in iteration 200 : 0.6056725587400316
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.8289417111971009, training accuracy 0.8252457002457002, time elapsed: 4601 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 2.387363824767098
Loss in iteration 3 : 1.65497790047504
Loss in iteration 4 : 1.6792282729416521
Loss in iteration 5 : 1.57919897634193
Loss in iteration 6 : 1.2415516603200878
Loss in iteration 7 : 1.9540233116925105
Loss in iteration 8 : 1.365233636158255
Loss in iteration 9 : 1.161247426986455
Loss in iteration 10 : 1.760798876333183
Loss in iteration 11 : 1.1033021591097372
Loss in iteration 12 : 1.188320259378726
Loss in iteration 13 : 1.3987489959825128
Loss in iteration 14 : 0.9687443025782992
Loss in iteration 15 : 0.9998368318634958
Loss in iteration 16 : 1.0801000410546036
Loss in iteration 17 : 0.8189483019107269
Loss in iteration 18 : 1.0895334752408705
Loss in iteration 19 : 0.7233977223607209
Loss in iteration 20 : 1.095042038893776
Loss in iteration 21 : 0.6610709877900459
Loss in iteration 22 : 0.9021003213908912
Loss in iteration 23 : 0.5757420124212161
Loss in iteration 24 : 0.8357175388857742
Loss in iteration 25 : 0.550430267105986
Loss in iteration 26 : 0.7269645976399117
Loss in iteration 27 : 0.548793593004284
Loss in iteration 28 : 0.675718777565886
Loss in iteration 29 : 0.5933206847276418
Loss in iteration 30 : 0.5206454953675475
Loss in iteration 31 : 0.7113906269476767
Loss in iteration 32 : 0.5494763640613066
Loss in iteration 33 : 0.4154388951242494
Loss in iteration 34 : 0.508276033972117
Loss in iteration 35 : 0.7149293553434509
Loss in iteration 36 : 0.4978835037768374
Loss in iteration 37 : 0.40269089913142125
Loss in iteration 38 : 0.4013635243855056
Loss in iteration 39 : 0.4518097011934216
Loss in iteration 40 : 0.49987867716012657
Loss in iteration 41 : 0.41502839658184226
Loss in iteration 42 : 0.37381262737600335
Loss in iteration 43 : 0.36538966568409886
Loss in iteration 44 : 0.38624407933774635
Loss in iteration 45 : 0.5003061830718333
Loss in iteration 46 : 0.684782513298355
Loss in iteration 47 : 1.2606182089774403
Loss in iteration 48 : 0.5148182059544302
Loss in iteration 49 : 2.226064260595725
Loss in iteration 50 : 1.0920726548256046
Loss in iteration 51 : 1.5656265019500144
Loss in iteration 52 : 0.7482016369163149
Loss in iteration 53 : 1.6573608007608769
Loss in iteration 54 : 0.9332339473952495
Loss in iteration 55 : 1.1670723514604084
Loss in iteration 56 : 1.3302934211376454
Loss in iteration 57 : 0.8493427607839902
Loss in iteration 58 : 1.1229822977845059
Loss in iteration 59 : 0.8647259062525283
Loss in iteration 60 : 0.8313884552130397
Loss in iteration 61 : 0.9784790205173571
Loss in iteration 62 : 0.6108976495867797
Loss in iteration 63 : 1.0411281321175085
Loss in iteration 64 : 0.6729709708281105
Loss in iteration 65 : 0.8284571094224021
Loss in iteration 66 : 0.6239482687332629
Loss in iteration 67 : 0.5744784270836459
Loss in iteration 68 : 0.7032228366396585
Loss in iteration 69 : 0.467479267890086
Loss in iteration 70 : 0.7275451753353153
Loss in iteration 71 : 0.5399317338868059
Loss in iteration 72 : 0.54843223460177
Loss in iteration 73 : 0.6527305327968459
Loss in iteration 74 : 0.4495789169015408
Loss in iteration 75 : 0.4699857462417936
Loss in iteration 76 : 0.6347152004412198
Loss in iteration 77 : 0.6896538299242977
Loss in iteration 78 : 0.3970979048502934
Loss in iteration 79 : 0.38715355504979937
Loss in iteration 80 : 0.590918710540912
Loss in iteration 81 : 0.5874688056198392
Loss in iteration 82 : 0.6192271993355898
Loss in iteration 83 : 0.38125579899420536
Loss in iteration 84 : 0.5216033055104636
Loss in iteration 85 : 0.742914008412817
Loss in iteration 86 : 0.38567500545634414
Loss in iteration 87 : 0.8068071779755077
Loss in iteration 88 : 1.0429274457891706
Loss in iteration 89 : 0.6138197885466803
Loss in iteration 90 : 1.6394219646640746
Loss in iteration 91 : 0.7294910917321923
Loss in iteration 92 : 1.008062017691385
Loss in iteration 93 : 0.6028797934583056
Loss in iteration 94 : 0.918728374331127
Loss in iteration 95 : 0.7171351524112949
Loss in iteration 96 : 0.8935354466185993
Loss in iteration 97 : 0.5486468917487926
Loss in iteration 98 : 0.8810042490541965
Loss in iteration 99 : 0.6231468573126021
Loss in iteration 100 : 0.7299731791319692
Loss in iteration 101 : 0.6355219703243373
Loss in iteration 102 : 0.4561295078878602
Loss in iteration 103 : 0.6948757947785168
Loss in iteration 104 : 0.4205537901308058
Loss in iteration 105 : 0.5416759338101271
Loss in iteration 106 : 0.7582158904272805
Loss in iteration 107 : 0.38152424965607784
Loss in iteration 108 : 0.709674913074342
Loss in iteration 109 : 0.9217477819569779
Loss in iteration 110 : 0.47197729322883064
Loss in iteration 111 : 1.2741529918815429
Loss in iteration 112 : 0.7363212002596298
Loss in iteration 113 : 0.8058198021720789
Loss in iteration 114 : 0.7845991984377936
Loss in iteration 115 : 0.5175713329588986
Loss in iteration 116 : 0.7545090462544185
Loss in iteration 117 : 0.4800750896911803
Loss in iteration 118 : 0.846242914368657
Loss in iteration 119 : 0.5647616264296151
Loss in iteration 120 : 0.542523973910439
Loss in iteration 121 : 0.8532063673885095
Loss in iteration 122 : 0.6390437220974611
Loss in iteration 123 : 0.4605069402138271
Loss in iteration 124 : 0.9364646278223617
Loss in iteration 125 : 0.6814404142279565
Loss in iteration 126 : 0.5589816968871646
Loss in iteration 127 : 0.8980034261278821
Loss in iteration 128 : 0.47627072434478146
Loss in iteration 129 : 0.6707086625294502
Loss in iteration 130 : 0.5032293121713612
Loss in iteration 131 : 0.4760025882935239
Loss in iteration 132 : 0.6298990802980357
Loss in iteration 133 : 0.3754227918012758
Loss in iteration 134 : 0.5615513999609317
Loss in iteration 135 : 0.949597296533328
Loss in iteration 136 : 0.36494775286343367
Loss in iteration 137 : 1.1754263413068138
Loss in iteration 138 : 1.518283240026125
Loss in iteration 139 : 1.32686912317645
Loss in iteration 140 : 0.967470331607098
Loss in iteration 141 : 0.9327098140828568
Loss in iteration 142 : 0.9677662609307977
Loss in iteration 143 : 1.2315878751991736
Loss in iteration 144 : 0.713666054516366
Loss in iteration 145 : 1.235426782549834
Loss in iteration 146 : 0.6716178971966911
Loss in iteration 147 : 0.9202518332000081
Loss in iteration 148 : 0.7614959817744255
Loss in iteration 149 : 0.7203676684204539
Loss in iteration 150 : 0.6040338151955007
Loss in iteration 151 : 0.8219811343203174
Loss in iteration 152 : 0.5613138913606184
Loss in iteration 153 : 1.1156618524044029
Loss in iteration 154 : 0.6824587380665119
Loss in iteration 155 : 0.6396852299330563
Loss in iteration 156 : 0.8606822513947596
Loss in iteration 157 : 0.454168703220262
Loss in iteration 158 : 0.7079992892895215
Loss in iteration 159 : 0.4821234845596113
Loss in iteration 160 : 0.6559612142801778
Loss in iteration 161 : 0.594526390676597
Loss in iteration 162 : 0.4658543566130129
Loss in iteration 163 : 0.7879392594801641
Loss in iteration 164 : 0.6830771058459338
Loss in iteration 165 : 0.3800143445985437
Loss in iteration 166 : 0.9358407006420532
Loss in iteration 167 : 1.2043193925847775
Loss in iteration 168 : 0.7277871737029085
Loss in iteration 169 : 1.7004771763262008
Loss in iteration 170 : 0.5090309037431578
Loss in iteration 171 : 1.114075580839179
Loss in iteration 172 : 0.6258120784496161
Loss in iteration 173 : 1.087393807314967
Loss in iteration 174 : 0.6254412189794252
Loss in iteration 175 : 0.8763829941243865
Loss in iteration 176 : 0.6954706548475599
Loss in iteration 177 : 0.7222579614982185
Loss in iteration 178 : 0.5479085589615706
Loss in iteration 179 : 0.7185437765250757
Loss in iteration 180 : 0.4877379638865023
Loss in iteration 181 : 0.9422019854345112
Loss in iteration 182 : 0.7853353568162375
Loss in iteration 183 : 0.5655867628887916
Loss in iteration 184 : 1.1919304410505973
Loss in iteration 185 : 0.601161352090682
Loss in iteration 186 : 0.6999185968727009
Loss in iteration 187 : 0.7232553350763438
Loss in iteration 188 : 0.5412305982063655
Loss in iteration 189 : 0.7175238127000775
Loss in iteration 190 : 0.510102095813285
Loss in iteration 191 : 0.7381440789055156
Loss in iteration 192 : 0.4694127194770558
Loss in iteration 193 : 0.5076451554841229
Loss in iteration 194 : 0.7083195701912237
Loss in iteration 195 : 0.6772944430473475
Loss in iteration 196 : 0.3764726898887587
Loss in iteration 197 : 0.7505606601024458
Loss in iteration 198 : 1.0972398994940564
Loss in iteration 199 : 0.5432795814791455
Loss in iteration 200 : 1.6201370178699315
Testing accuracy  of updater 1 on alg 0 with rate 7.0 = 0.8401203857256925, training accuracy 0.8410626535626535, time elapsed: 4220 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 1.3671281430965467
Loss in iteration 3 : 0.9778722318704596
Loss in iteration 4 : 0.9079127674884195
Loss in iteration 5 : 0.8950921198903751
Loss in iteration 6 : 0.6929852427408553
Loss in iteration 7 : 1.0348306452043938
Loss in iteration 8 : 0.7629201535121631
Loss in iteration 9 : 0.6820773420283399
Loss in iteration 10 : 0.9383605038107136
Loss in iteration 11 : 0.6299640485448104
Loss in iteration 12 : 0.6861362250404973
Loss in iteration 13 : 0.7644143430381454
Loss in iteration 14 : 0.5467582544877913
Loss in iteration 15 : 0.6257366170886927
Loss in iteration 16 : 0.6013547030148784
Loss in iteration 17 : 0.5290745577304128
Loss in iteration 18 : 0.6409723833219557
Loss in iteration 19 : 0.4732560640286935
Loss in iteration 20 : 0.6309963968811152
Loss in iteration 21 : 0.4435995616768247
Loss in iteration 22 : 0.5464715611346542
Loss in iteration 23 : 0.41328793116104307
Loss in iteration 24 : 0.5055809204577507
Loss in iteration 25 : 0.39728717173062256
Loss in iteration 26 : 0.47400101784652166
Loss in iteration 27 : 0.4098929626280843
Loss in iteration 28 : 0.4596965790226554
Loss in iteration 29 : 0.39236179644818475
Loss in iteration 30 : 0.43883826772282764
Loss in iteration 31 : 0.3675583252746539
Loss in iteration 32 : 0.4344531303255014
Loss in iteration 33 : 0.3649962651680525
Loss in iteration 34 : 0.3802547107615943
Loss in iteration 35 : 0.40168876158096456
Loss in iteration 36 : 0.3446362869480592
Loss in iteration 37 : 0.36406756723726863
Loss in iteration 38 : 0.36581780486488397
Loss in iteration 39 : 0.3375757382309091
Loss in iteration 40 : 0.35559643857214984
Loss in iteration 41 : 0.34323954339559415
Loss in iteration 42 : 0.339123232875426
Loss in iteration 43 : 0.3486384528179035
Loss in iteration 44 : 0.3336838755704525
Loss in iteration 45 : 0.3425490725607197
Loss in iteration 46 : 0.3424322669152239
Loss in iteration 47 : 0.32973011020823856
Loss in iteration 48 : 0.3397452900241334
Loss in iteration 49 : 0.33967165684772466
Loss in iteration 50 : 0.3274420850458683
Loss in iteration 51 : 0.3363767747102787
Loss in iteration 52 : 0.3392554663722767
Loss in iteration 53 : 0.32641437405052404
Loss in iteration 54 : 0.3330742351241824
Loss in iteration 55 : 0.33621333206552906
Loss in iteration 56 : 0.3257106983813163
Loss in iteration 57 : 0.3320264202825657
Loss in iteration 58 : 0.3336589871625277
Loss in iteration 59 : 0.32467924095206496
Loss in iteration 60 : 0.3307262343230707
Loss in iteration 61 : 0.3320534911535691
Loss in iteration 62 : 0.32474653760307554
Loss in iteration 63 : 0.3301961905196271
Loss in iteration 64 : 0.33125451213094254
Loss in iteration 65 : 0.3244909759620267
Loss in iteration 66 : 0.32892924066701035
Loss in iteration 67 : 0.33001605598228373
Loss in iteration 68 : 0.32433485944202106
Loss in iteration 69 : 0.32794954576799973
Loss in iteration 70 : 0.328697799381336
Loss in iteration 71 : 0.3238454862450308
Loss in iteration 72 : 0.32686978502465164
Loss in iteration 73 : 0.327859931699275
Loss in iteration 74 : 0.32385570266852826
Loss in iteration 75 : 0.32610174307515805
Loss in iteration 76 : 0.3273320543855981
Loss in iteration 77 : 0.323839243965561
Loss in iteration 78 : 0.32510779332632006
Loss in iteration 79 : 0.3265878465194813
Loss in iteration 80 : 0.32388356736003154
Loss in iteration 81 : 0.32443395535027636
Loss in iteration 82 : 0.32579970916733225
Loss in iteration 83 : 0.32381627306217814
Loss in iteration 84 : 0.3240178521475422
Loss in iteration 85 : 0.3251629238064993
Loss in iteration 86 : 0.32375974579951
Loss in iteration 87 : 0.32376917263576555
Loss in iteration 88 : 0.324683849662853
Loss in iteration 89 : 0.32371260036428595
Loss in iteration 90 : 0.3235714735421319
Loss in iteration 91 : 0.32428103901397765
Loss in iteration 92 : 0.32364637168533594
Loss in iteration 93 : 0.32343434272018023
Loss in iteration 94 : 0.32397272790638226
Loss in iteration 95 : 0.3235885165855012
Loss in iteration 96 : 0.323371166345896
Loss in iteration 97 : 0.32375547608893573
Loss in iteration 98 : 0.3235212537899269
Loss in iteration 99 : 0.32331100864830536
Loss in iteration 100 : 0.3235837762041444
Loss in iteration 101 : 0.32346955090635365
Loss in iteration 102 : 0.32328193092481655
Loss in iteration 103 : 0.32345939216631464
Loss in iteration 104 : 0.32341295316424645
Loss in iteration 105 : 0.32325177261264626
Loss in iteration 106 : 0.3233637867030022
Loss in iteration 107 : 0.3233646613970448
Loss in iteration 108 : 0.3232414926699436
Loss in iteration 109 : 0.3233061021541627
Loss in iteration 110 : 0.3233234035050598
Loss in iteration 111 : 0.3232281004311238
Loss in iteration 112 : 0.3232606660404762
Loss in iteration 113 : 0.3232882811875342
Loss in iteration 114 : 0.32321954078704757
Loss in iteration 115 : 0.3232295368127656
Loss in iteration 116 : 0.32325734135681666
Loss in iteration 117 : 0.32320848698362203
Loss in iteration 118 : 0.3232048048057027
Loss in iteration 119 : 0.3232302449114733
Loss in iteration 120 : 0.32319810724928666
Loss in iteration 121 : 0.32318717935766944
Loss in iteration 122 : 0.3232070881358175
Loss in iteration 123 : 0.3231864059716477
Loss in iteration 124 : 0.3231723346007677
Loss in iteration 125 : 0.323186731739061
Loss in iteration 126 : 0.3231743728268852
Loss in iteration 127 : 0.32316006594777014
Loss in iteration 128 : 0.3231696763413226
Loss in iteration 129 : 0.3231629208740307
Loss in iteration 130 : 0.32314975269822643
Loss in iteration 131 : 0.32315504246208465
Loss in iteration 132 : 0.3231512261712131
Loss in iteration 133 : 0.32313965584513815
Loss in iteration 134 : 0.32314183825890425
Loss in iteration 135 : 0.32313987635458735
Loss in iteration 136 : 0.3231302079764685
Loss in iteration 137 : 0.32313006819962586
Loss in iteration 138 : 0.3231288619018356
Loss in iteration 139 : 0.32312093448977414
Loss in iteration 140 : 0.3231193731737001
Loss in iteration 141 : 0.32311856905071257
Loss in iteration 142 : 0.3231121884070557
Loss in iteration 143 : 0.32310962695184803
Loss in iteration 144 : 0.32310869575378753
Loss in iteration 145 : 0.32310352551506427
Loss in iteration 146 : 0.3231004994032366
Loss in iteration 147 : 0.32309939961147255
Loss in iteration 148 : 0.3230951812845079
Loss in iteration 149 : 0.32309190622621176
Loss in iteration 150 : 0.32309048714583233
Loss in iteration 151 : 0.32308694759367274
Loss in iteration 152 : 0.3230836646185434
Loss in iteration 153 : 0.32308200237394186
Loss in iteration 154 : 0.3230789568810678
Loss in iteration 155 : 0.32307574632625835
Loss in iteration 156 : 0.3230738534644966
Loss in iteration 157 : 0.32307114944347803
Loss in iteration 158 : 0.3230680957155472
Loss in iteration 159 : 0.32306604981066184
Loss in iteration 160 : 0.32306357144184755
Loss in iteration 161 : 0.32306067602406463
Loss in iteration 162 : 0.323058518059207
Loss in iteration 163 : 0.32305619146624437
Loss in iteration 164 : 0.323053468474994
Loss in iteration 165 : 0.32305125675453034
Loss in iteration 166 : 0.3230490305139062
Loss in iteration 167 : 0.32304646104472595
Loss in iteration 168 : 0.3230442288766997
Loss in iteration 169 : 0.32304207099305793
Loss in iteration 170 : 0.3230396396215862
Loss in iteration 171 : 0.3230374165855562
Loss in iteration 172 : 0.3230353081631458
Loss in iteration 173 : 0.3230329969149782
Loss in iteration 174 : 0.32303080523986283
Loss in iteration 175 : 0.3230287378112312
Loss in iteration 176 : 0.32302652769883367
Loss in iteration 177 : 0.32302437754613456
Loss in iteration 178 : 0.3230223457825752
Loss in iteration 179 : 0.32302022314054724
Loss in iteration 180 : 0.3230181248082495
Loss in iteration 181 : 0.32301612920166134
Loss in iteration 182 : 0.32301408022411865
Loss in iteration 183 : 0.3230120349569646
Loss in iteration 184 : 0.32301007561157596
Loss in iteration 185 : 0.32300809233988614
Loss in iteration 186 : 0.32300610376845884
Loss in iteration 187 : 0.3230041832043957
Loss in iteration 188 : 0.3230022570527785
Loss in iteration 189 : 0.3230003224316102
Loss in iteration 190 : 0.32299844112578396
Loss in iteration 191 : 0.32299656747391814
Loss in iteration 192 : 0.32299468676274595
Loss in iteration 193 : 0.32299284669088096
Loss in iteration 194 : 0.3229910210770658
Loss in iteration 195 : 0.32298919097410045
Loss in iteration 196 : 0.32298739200352533
Loss in iteration 197 : 0.3229856115557054
Loss in iteration 198 : 0.3229838302798281
Loss in iteration 199 : 0.32298207295609876
Loss in iteration 200 : 0.32298033535986975
Testing accuracy  of updater 1 on alg 0 with rate 4.0 = 0.8498249493274369, training accuracy 0.8491093366093366, time elapsed: 5041 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5309044865682255
Loss in iteration 3 : 0.5809864492631956
Loss in iteration 4 : 0.47590219663978167
Loss in iteration 5 : 0.3792723652306178
Loss in iteration 6 : 0.46165039817009434
Loss in iteration 7 : 0.4403695827828505
Loss in iteration 8 : 0.37481651053464177
Loss in iteration 9 : 0.404808780678106
Loss in iteration 10 : 0.4370787634843275
Loss in iteration 11 : 0.4116743220583489
Loss in iteration 12 : 0.3783065818289199
Loss in iteration 13 : 0.3897792510724412
Loss in iteration 14 : 0.4078654747971792
Loss in iteration 15 : 0.3855731792911296
Loss in iteration 16 : 0.3606031327201985
Loss in iteration 17 : 0.36643626884891084
Loss in iteration 18 : 0.3747010511836947
Loss in iteration 19 : 0.36049514931362336
Loss in iteration 20 : 0.34381080369745376
Loss in iteration 21 : 0.3502859285610308
Loss in iteration 22 : 0.3586033275413586
Loss in iteration 23 : 0.3471709602031832
Loss in iteration 24 : 0.34147284306118125
Loss in iteration 25 : 0.3509705895481439
Loss in iteration 26 : 0.3519289959767535
Loss in iteration 27 : 0.34208449290700443
Loss in iteration 28 : 0.3414844241824773
Loss in iteration 29 : 0.34631793971565794
Loss in iteration 30 : 0.34023408446691916
Loss in iteration 31 : 0.3338522311110685
Loss in iteration 32 : 0.33620377383647965
Loss in iteration 33 : 0.3360663050076217
Loss in iteration 34 : 0.3306407856773908
Loss in iteration 35 : 0.32971710423085104
Loss in iteration 36 : 0.33253760646314556
Loss in iteration 37 : 0.3310309721818364
Loss in iteration 38 : 0.32856071764509015
Loss in iteration 39 : 0.3301370716024686
Loss in iteration 40 : 0.33127558006601115
Loss in iteration 41 : 0.329268136832325
Loss in iteration 42 : 0.3283670402107795
Loss in iteration 43 : 0.32955785670589405
Loss in iteration 44 : 0.3289173525321304
Loss in iteration 45 : 0.32710205992268726
Loss in iteration 46 : 0.3272085572585722
Loss in iteration 47 : 0.32759767026661285
Loss in iteration 48 : 0.3264420030735576
Loss in iteration 49 : 0.3258224672555753
Loss in iteration 50 : 0.3264609326343168
Loss in iteration 51 : 0.32625508140550613
Loss in iteration 52 : 0.3255179447196167
Loss in iteration 53 : 0.3257838587776452
Loss in iteration 54 : 0.3260334908471016
Loss in iteration 55 : 0.3254689784522358
Loss in iteration 56 : 0.3252571426708098
Loss in iteration 57 : 0.3255215581739473
Loss in iteration 58 : 0.3252612459980973
Loss in iteration 59 : 0.32486961055097924
Loss in iteration 60 : 0.3249833525228502
Loss in iteration 61 : 0.3250145180582727
Loss in iteration 62 : 0.32472360239131
Loss in iteration 63 : 0.3246796450255521
Loss in iteration 64 : 0.32482719416917727
Loss in iteration 65 : 0.3247176326813971
Loss in iteration 66 : 0.32457771650492456
Loss in iteration 67 : 0.3246611551783247
Loss in iteration 68 : 0.3246740473907465
Loss in iteration 69 : 0.3245301563728621
Loss in iteration 70 : 0.3245016155467783
Loss in iteration 71 : 0.32454992901835844
Loss in iteration 72 : 0.32447155095354396
Loss in iteration 73 : 0.3243875606895681
Loss in iteration 74 : 0.3244144864672791
Loss in iteration 75 : 0.3244064317696798
Loss in iteration 76 : 0.3243309839658381
Loss in iteration 77 : 0.3243198163663637
Loss in iteration 78 : 0.32434161820063295
Loss in iteration 79 : 0.3243005709249131
Loss in iteration 80 : 0.3242629705527324
Loss in iteration 81 : 0.32427601458492794
Loss in iteration 82 : 0.3242646047083125
Loss in iteration 83 : 0.3242232551818603
Loss in iteration 84 : 0.32421530291481526
Loss in iteration 85 : 0.32421777101384625
Loss in iteration 86 : 0.32418974604154266
Loss in iteration 87 : 0.324168304071519
Loss in iteration 88 : 0.32417033069250833
Loss in iteration 89 : 0.3241586125784038
Loss in iteration 90 : 0.324135562396593
Loss in iteration 91 : 0.32412996758866375
Loss in iteration 92 : 0.3241271415938183
Loss in iteration 93 : 0.32410980032085107
Loss in iteration 94 : 0.32409705885490037
Loss in iteration 95 : 0.32409442332276217
Loss in iteration 96 : 0.32408398566734503
Loss in iteration 97 : 0.3240689389278521
Loss in iteration 98 : 0.32406246677769673
Loss in iteration 99 : 0.3240564278892316
Loss in iteration 100 : 0.3240437407660581
Loss in iteration 101 : 0.32403417046037025
Loss in iteration 102 : 0.32402932179151417
Loss in iteration 103 : 0.324020640077286
Loss in iteration 104 : 0.3240104876471928
Loss in iteration 105 : 0.3240047288731836
Loss in iteration 106 : 0.3239988375063176
Loss in iteration 107 : 0.32398991076472095
Loss in iteration 108 : 0.32398277724950547
Loss in iteration 109 : 0.32397762104646904
Loss in iteration 110 : 0.3239704017553512
Loss in iteration 111 : 0.3239626823794512
Loss in iteration 112 : 0.3239570576837005
Loss in iteration 113 : 0.3239511531204496
Loss in iteration 114 : 0.32394390536579015
Loss in iteration 115 : 0.3239377083370865
Loss in iteration 116 : 0.3239323937988652
Loss in iteration 117 : 0.323926088981551
Loss in iteration 118 : 0.3239197155550345
Loss in iteration 119 : 0.32391436982187977
Loss in iteration 120 : 0.3239088089266006
Loss in iteration 121 : 0.3239026578062341
Loss in iteration 122 : 0.32389707155319675
Loss in iteration 123 : 0.3238918542485493
Loss in iteration 124 : 0.3238861599455521
Loss in iteration 125 : 0.32388052064948986
Loss in iteration 126 : 0.3238754060795433
Loss in iteration 127 : 0.323870194137309
Loss in iteration 128 : 0.32386477727424456
Loss in iteration 129 : 0.32385970543187087
Loss in iteration 130 : 0.323854822950268
Loss in iteration 131 : 0.32384974011886664
Loss in iteration 132 : 0.3238447365723034
Loss in iteration 133 : 0.3238399939984287
Loss in iteration 134 : 0.3238351957208933
Loss in iteration 135 : 0.323830322111697
Loss in iteration 136 : 0.32382563336556663
Loss in iteration 137 : 0.3238210320831273
Loss in iteration 138 : 0.3238163504705677
Loss in iteration 139 : 0.32381174165470494
Loss in iteration 140 : 0.32380727767612993
Loss in iteration 141 : 0.3238028043324496
Loss in iteration 142 : 0.32379832882164455
Loss in iteration 143 : 0.3237939705876817
Loss in iteration 144 : 0.3237896688737508
Loss in iteration 145 : 0.3237853488612276
Loss in iteration 146 : 0.32378108768774333
Loss in iteration 147 : 0.3237769091418659
Loss in iteration 148 : 0.32377273769968246
Loss in iteration 149 : 0.32376858466712594
Loss in iteration 150 : 0.3237645052965706
Loss in iteration 151 : 0.3237604661779507
Loss in iteration 152 : 0.3237564365951149
Loss in iteration 153 : 0.32375245637429945
Loss in iteration 154 : 0.32374853255560326
Loss in iteration 155 : 0.32374462869512044
Loss in iteration 156 : 0.32374075318452583
Loss in iteration 157 : 0.32373693009125815
Loss in iteration 158 : 0.32373314029685507
Loss in iteration 159 : 0.3237293705464598
Loss in iteration 160 : 0.3237256401906445
Loss in iteration 161 : 0.32372195006097426
Loss in iteration 162 : 0.3237182827826026
Loss in iteration 163 : 0.3237146440331947
Loss in iteration 164 : 0.3237110445155857
Loss in iteration 165 : 0.32370747420627094
Loss in iteration 166 : 0.3237039279380375
Loss in iteration 167 : 0.32370041521597437
Loss in iteration 168 : 0.3236969352500379
Loss in iteration 169 : 0.3236934797103014
Loss in iteration 170 : 0.32369005177918364
Loss in iteration 171 : 0.32368665576598227
Loss in iteration 172 : 0.3236832861627634
Loss in iteration 173 : 0.32367994078006956
Loss in iteration 174 : 0.32367662399691716
Loss in iteration 175 : 0.32367333474204185
Loss in iteration 176 : 0.32367006902330947
Loss in iteration 177 : 0.32366682857482215
Loss in iteration 178 : 0.3236636150885397
Loss in iteration 179 : 0.32366042567150927
Loss in iteration 180 : 0.3236572594518316
Loss in iteration 181 : 0.3236541184117311
Loss in iteration 182 : 0.3236510016709389
Loss in iteration 183 : 0.323647907259939
Loss in iteration 184 : 0.32364483599035787
Loss in iteration 185 : 0.32364178836220703
Loss in iteration 186 : 0.3236387627612154
Loss in iteration 187 : 0.32363575874330713
Loss in iteration 188 : 0.3236327770920114
Loss in iteration 189 : 0.3236298171382132
Loss in iteration 190 : 0.3236268778656638
Loss in iteration 191 : 0.32362395960808965
Loss in iteration 192 : 0.3236210624237415
Loss in iteration 193 : 0.32361818541395554
Loss in iteration 194 : 0.323615328327232
Loss in iteration 195 : 0.3236124914190979
Loss in iteration 196 : 0.32360967420984715
Loss in iteration 197 : 0.32360687612905337
Loss in iteration 198 : 0.32360409724531874
Loss in iteration 199 : 0.3236013374323519
Loss in iteration 200 : 0.32359859613699954
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.8504391622136233, training accuracy 0.8484643734643734, time elapsed: 4322 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5255663182148818
Loss in iteration 3 : 0.5484785380566798
Loss in iteration 4 : 0.5318625651280662
Loss in iteration 5 : 0.42778511921807844
Loss in iteration 6 : 0.3814911374541801
Loss in iteration 7 : 0.43515312134641015
Loss in iteration 8 : 0.43586379417016197
Loss in iteration 9 : 0.38382056648830587
Loss in iteration 10 : 0.37330960878392133
Loss in iteration 11 : 0.4017957997048363
Loss in iteration 12 : 0.4136694565874141
Loss in iteration 13 : 0.3941054665208622
Loss in iteration 14 : 0.3709408219600266
Loss in iteration 15 : 0.3713533348261921
Loss in iteration 16 : 0.3848919167681363
Loss in iteration 17 : 0.38250014326404846
Loss in iteration 18 : 0.36340240811060215
Loss in iteration 19 : 0.35137574427431884
Loss in iteration 20 : 0.35481683400140873
Loss in iteration 21 : 0.35937001955342907
Loss in iteration 22 : 0.3530069632549182
Loss in iteration 23 : 0.3414399219197076
Loss in iteration 24 : 0.33834032780088924
Loss in iteration 25 : 0.34430473498932024
Loss in iteration 26 : 0.34603278077321076
Loss in iteration 27 : 0.33963309390700963
Loss in iteration 28 : 0.3359241556931518
Loss in iteration 29 : 0.33959422266957096
Loss in iteration 30 : 0.34260993431784814
Loss in iteration 31 : 0.33945305563772943
Loss in iteration 32 : 0.3351590444180797
Loss in iteration 33 : 0.33560630629658367
Loss in iteration 34 : 0.33750043066475455
Loss in iteration 35 : 0.33534507768164595
Loss in iteration 36 : 0.3314975130862035
Loss in iteration 37 : 0.33071403505654723
Loss in iteration 38 : 0.33176503192008105
Loss in iteration 39 : 0.3309414132204022
Loss in iteration 40 : 0.32860189934423956
Loss in iteration 41 : 0.32778158574939736
Loss in iteration 42 : 0.32874998962327473
Loss in iteration 43 : 0.32899060128060087
Loss in iteration 44 : 0.32784297004829915
Loss in iteration 45 : 0.32718662380167646
Loss in iteration 46 : 0.3277767083022113
Loss in iteration 47 : 0.32822405005576333
Loss in iteration 48 : 0.3276360860924951
Loss in iteration 49 : 0.3269182914256371
Loss in iteration 50 : 0.3269859812329935
Loss in iteration 51 : 0.32726438298799765
Loss in iteration 52 : 0.3268956634829436
Loss in iteration 53 : 0.3262415798222769
Loss in iteration 54 : 0.32607749669339614
Loss in iteration 55 : 0.326237953718639
Loss in iteration 56 : 0.32608456849097506
Loss in iteration 57 : 0.3256609387220945
Loss in iteration 58 : 0.32550199586248185
Loss in iteration 59 : 0.32564592062225234
Loss in iteration 60 : 0.32564383815851294
Loss in iteration 61 : 0.3254052173496539
Loss in iteration 62 : 0.32527972568699964
Loss in iteration 63 : 0.32536601554503397
Loss in iteration 64 : 0.32538582382830405
Loss in iteration 65 : 0.32522507651794036
Loss in iteration 66 : 0.32509384139969283
Loss in iteration 67 : 0.3251093520015717
Loss in iteration 68 : 0.3251149651422114
Loss in iteration 69 : 0.3250020682369772
Loss in iteration 70 : 0.3248897254729751
Loss in iteration 71 : 0.3248797583230029
Loss in iteration 72 : 0.32488972350024947
Loss in iteration 73 : 0.32482954858882446
Loss in iteration 74 : 0.32475387280922036
Loss in iteration 75 : 0.32474115886320504
Loss in iteration 76 : 0.32475559749801125
Loss in iteration 77 : 0.3247273193759689
Loss in iteration 78 : 0.3246750220836097
Loss in iteration 79 : 0.3246559461202295
Loss in iteration 80 : 0.3246612396636623
Loss in iteration 81 : 0.3246431194335224
Loss in iteration 82 : 0.32460204907058526
Loss in iteration 83 : 0.3245768563729849
Loss in iteration 84 : 0.3245727530610375
Loss in iteration 85 : 0.32455920059009824
Loss in iteration 86 : 0.32452838771296666
Loss in iteration 87 : 0.32450445321424265
Loss in iteration 88 : 0.32449698538265964
Loss in iteration 89 : 0.32448774407605335
Loss in iteration 90 : 0.3244665644379556
Loss in iteration 91 : 0.32444677227629015
Loss in iteration 92 : 0.32443813400548077
Loss in iteration 93 : 0.3244308056910053
Loss in iteration 94 : 0.3244154027703425
Loss in iteration 95 : 0.324398597212931
Loss in iteration 96 : 0.32438866199108984
Loss in iteration 97 : 0.3243811142907643
Loss in iteration 98 : 0.32436867783928
Loss in iteration 99 : 0.32435407747923617
Loss in iteration 100 : 0.32434356453570934
Loss in iteration 101 : 0.3243357313686751
Loss in iteration 102 : 0.32432536422981434
Loss in iteration 103 : 0.3243129888099877
Loss in iteration 104 : 0.32430289449381117
Loss in iteration 105 : 0.3242951725655899
Loss in iteration 106 : 0.32428634771451886
Loss in iteration 107 : 0.32427588021593984
Loss in iteration 108 : 0.3242664759045235
Loss in iteration 109 : 0.3242588746445991
Loss in iteration 110 : 0.32425094031287527
Loss in iteration 111 : 0.32424177252339315
Loss in iteration 112 : 0.3242329617024639
Loss in iteration 113 : 0.3242254173049504
Loss in iteration 114 : 0.3242179556713926
Loss in iteration 115 : 0.3242096840057749
Loss in iteration 116 : 0.3242014442410315
Loss in iteration 117 : 0.32419406837393117
Loss in iteration 118 : 0.3241869828008042
Loss in iteration 119 : 0.3241794425903881
Loss in iteration 120 : 0.3241718195303583
Loss in iteration 121 : 0.3241647639343994
Loss in iteration 122 : 0.3241580551107042
Loss in iteration 123 : 0.32415113414354035
Loss in iteration 124 : 0.3241441021330851
Loss in iteration 125 : 0.32413741707189586
Loss in iteration 126 : 0.3241310470100589
Loss in iteration 127 : 0.324124608018716
Loss in iteration 128 : 0.3241180688659212
Loss in iteration 129 : 0.32411172902104224
Loss in iteration 130 : 0.32410564531739894
Loss in iteration 131 : 0.3240995771998819
Loss in iteration 132 : 0.32409344366534304
Loss in iteration 133 : 0.32408742570598675
Loss in iteration 134 : 0.3240816092297475
Loss in iteration 135 : 0.3240758560603523
Loss in iteration 136 : 0.3240700764591157
Loss in iteration 137 : 0.32406436822260987
Loss in iteration 138 : 0.3240588135936702
Loss in iteration 139 : 0.32405334071455744
Loss in iteration 140 : 0.32404787033094273
Loss in iteration 141 : 0.3240424468509477
Loss in iteration 142 : 0.3240371365909741
Loss in iteration 143 : 0.32403190869503534
Loss in iteration 144 : 0.32402670234858605
Loss in iteration 145 : 0.3240215319700193
Loss in iteration 146 : 0.3240164463136303
Loss in iteration 147 : 0.3240114380056185
Loss in iteration 148 : 0.3240064649553739
Loss in iteration 149 : 0.32400152625820794
Loss in iteration 150 : 0.3239966545473124
Loss in iteration 151 : 0.32399185343790066
Loss in iteration 152 : 0.3239870955666633
Loss in iteration 153 : 0.32398237296948107
Loss in iteration 154 : 0.32397770514878615
Loss in iteration 155 : 0.32397309931887297
Loss in iteration 156 : 0.32396853878185206
Loss in iteration 157 : 0.3239640136682036
Loss in iteration 158 : 0.3239595343702798
Loss in iteration 159 : 0.32395510842933084
Loss in iteration 160 : 0.32395072670488667
Loss in iteration 161 : 0.32394638033363526
Loss in iteration 162 : 0.3239420741290961
Loss in iteration 163 : 0.32393781456567233
Loss in iteration 164 : 0.32393359735540417
Loss in iteration 165 : 0.32392941554849164
Loss in iteration 166 : 0.32392527061315085
Loss in iteration 167 : 0.32392116728429166
Loss in iteration 168 : 0.3239171039061199
Loss in iteration 169 : 0.3239130753455463
Loss in iteration 170 : 0.32390908118939793
Loss in iteration 171 : 0.32390512444870223
Loss in iteration 172 : 0.32390120475996687
Loss in iteration 173 : 0.3238973185644611
Loss in iteration 174 : 0.323893464646767
Loss in iteration 175 : 0.32388964471509585
Loss in iteration 176 : 0.3238858589965118
Loss in iteration 177 : 0.323882105198425
Loss in iteration 178 : 0.32387838194116003
Loss in iteration 179 : 0.32387469004080144
Loss in iteration 180 : 0.3238710298902075
Loss in iteration 181 : 0.3238674000744154
Loss in iteration 182 : 0.32386379930618175
Loss in iteration 183 : 0.3238602277936424
Loss in iteration 184 : 0.32385668584691923
Loss in iteration 185 : 0.3238531725879863
Loss in iteration 186 : 0.32384968691165766
Loss in iteration 187 : 0.32384622865642076
Loss in iteration 188 : 0.3238427979796569
Loss in iteration 189 : 0.3238393943299133
Loss in iteration 190 : 0.32383601682085256
Loss in iteration 191 : 0.32383266511327125
Loss in iteration 192 : 0.32382933922740414
Loss in iteration 193 : 0.3238260388042545
Loss in iteration 194 : 0.32382276316121006
Loss in iteration 195 : 0.323819511896807
Loss in iteration 196 : 0.323816284923232
Loss in iteration 197 : 0.3238130819794584
Loss in iteration 198 : 0.3238099025403261
Loss in iteration 199 : 0.3238067461978771
Loss in iteration 200 : 0.32380361278373565
Testing accuracy  of updater 1 on alg 0 with rate 0.7 = 0.8506234260794792, training accuracy 0.8484643734643734, time elapsed: 4038 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5623723947519088
Loss in iteration 3 : 0.516953141976074
Loss in iteration 4 : 0.5336398018270148
Loss in iteration 5 : 0.519270328016294
Loss in iteration 6 : 0.457713156813377
Loss in iteration 7 : 0.39452088831750026
Loss in iteration 8 : 0.38475161577799705
Loss in iteration 9 : 0.41335808169635935
Loss in iteration 10 : 0.42271567774256563
Loss in iteration 11 : 0.3986639949883727
Loss in iteration 12 : 0.37144784017693205
Loss in iteration 13 : 0.3666170500574658
Loss in iteration 14 : 0.3792231146463455
Loss in iteration 15 : 0.39004239208839364
Loss in iteration 16 : 0.3877314846633879
Loss in iteration 17 : 0.37486478069513374
Loss in iteration 18 : 0.36216740258755814
Loss in iteration 19 : 0.3583390637826065
Loss in iteration 20 : 0.36261547877053385
Loss in iteration 21 : 0.366565685825722
Loss in iteration 22 : 0.3633902575214348
Loss in iteration 23 : 0.35441387876708824
Loss in iteration 24 : 0.34634320166104965
Loss in iteration 25 : 0.3437338017475974
Loss in iteration 26 : 0.3452633946860298
Loss in iteration 27 : 0.34638665584302875
Loss in iteration 28 : 0.3440966163082269
Loss in iteration 29 : 0.33924959129948334
Loss in iteration 30 : 0.33522017302486606
Loss in iteration 31 : 0.33446312075899315
Loss in iteration 32 : 0.3361380148255628
Loss in iteration 33 : 0.3372391309374251
Loss in iteration 34 : 0.3359929707456348
Loss in iteration 35 : 0.33355120148958123
Loss in iteration 36 : 0.3322348043103321
Loss in iteration 37 : 0.332821770344454
Loss in iteration 38 : 0.33402101151903546
Loss in iteration 39 : 0.3341890388819854
Loss in iteration 40 : 0.3330255755877299
Loss in iteration 41 : 0.3316138383724305
Loss in iteration 42 : 0.33108244683635873
Loss in iteration 43 : 0.331418491500658
Loss in iteration 44 : 0.3316603978347782
Loss in iteration 45 : 0.3311013071000892
Loss in iteration 46 : 0.3300207026521191
Loss in iteration 47 : 0.32920478327103087
Loss in iteration 48 : 0.32901823986283124
Loss in iteration 49 : 0.3291241222448522
Loss in iteration 50 : 0.3289905922461448
Loss in iteration 51 : 0.3284771405349974
Loss in iteration 52 : 0.3278985021644534
Loss in iteration 53 : 0.3276161432044741
Loss in iteration 54 : 0.32765607704662514
Loss in iteration 55 : 0.3277384680842868
Loss in iteration 56 : 0.3276198382626084
Loss in iteration 57 : 0.32733453422788383
Loss in iteration 58 : 0.3270997739605965
Loss in iteration 59 : 0.32705032420725616
Loss in iteration 60 : 0.3271135553211904
Loss in iteration 61 : 0.32712219881348925
Loss in iteration 62 : 0.3269995051950682
Loss in iteration 63 : 0.3268170720188804
Loss in iteration 64 : 0.32669349295883615
Loss in iteration 65 : 0.3266647252577511
Loss in iteration 66 : 0.3266621165514552
Loss in iteration 67 : 0.32660234279386974
Loss in iteration 68 : 0.32647699357184845
Loss in iteration 69 : 0.32634830244859403
Loss in iteration 70 : 0.3262723608715812
Loss in iteration 71 : 0.3262432420362359
Loss in iteration 72 : 0.3262116393110822
Loss in iteration 73 : 0.32614419715544074
Loss in iteration 74 : 0.32605497076532214
Loss in iteration 75 : 0.3259821885238116
Loss in iteration 76 : 0.3259441575279102
Loss in iteration 77 : 0.3259235372726365
Loss in iteration 78 : 0.32589144392665936
Loss in iteration 79 : 0.32583924798614095
Loss in iteration 80 : 0.32578332651283437
Loss in iteration 81 : 0.325742765152577
Loss in iteration 82 : 0.3257185315932628
Loss in iteration 83 : 0.3256956127648443
Loss in iteration 84 : 0.32566141529547377
Loss in iteration 85 : 0.3256181882254178
Loss in iteration 86 : 0.3255777704611561
Loss in iteration 87 : 0.32554745067845675
Loss in iteration 88 : 0.32552325723745673
Loss in iteration 89 : 0.32549626358850275
Loss in iteration 90 : 0.32546289505257786
Loss in iteration 91 : 0.3254277036111337
Loss in iteration 92 : 0.32539713491105765
Loss in iteration 93 : 0.32537251736591866
Loss in iteration 94 : 0.3253497064008393
Loss in iteration 95 : 0.32532444015375805
Loss in iteration 96 : 0.3252967519010218
Loss in iteration 97 : 0.3252700763887529
Loss in iteration 98 : 0.32524700145755864
Loss in iteration 99 : 0.32522669480137634
Loss in iteration 100 : 0.3252063515688278
Loss in iteration 101 : 0.32518440809782373
Loss in iteration 102 : 0.32516186211983195
Loss in iteration 103 : 0.32514069626551106
Loss in iteration 104 : 0.3251215942226057
Loss in iteration 105 : 0.325103446453047
Loss in iteration 106 : 0.32508480860634725
Loss in iteration 107 : 0.32506543205067584
Loss in iteration 108 : 0.3250462763338688
Loss in iteration 109 : 0.3250282797836453
Loss in iteration 110 : 0.3250113888099759
Loss in iteration 111 : 0.3249947962969422
Loss in iteration 112 : 0.3249779137837242
Loss in iteration 113 : 0.3249609337986257
Loss in iteration 114 : 0.32494447814870353
Loss in iteration 115 : 0.32492886869706067
Loss in iteration 116 : 0.32491384527966705
Loss in iteration 117 : 0.3248989363319217
Loss in iteration 118 : 0.3248839755409151
Loss in iteration 119 : 0.3248692045121378
Loss in iteration 120 : 0.3248549347693599
Loss in iteration 121 : 0.3248412014967898
Loss in iteration 122 : 0.32482776812681263
Loss in iteration 123 : 0.32481441103099734
Loss in iteration 124 : 0.3248011375637486
Loss in iteration 125 : 0.3247881249996677
Loss in iteration 126 : 0.3247754965042776
Loss in iteration 127 : 0.3247631964510656
Loss in iteration 128 : 0.3247510747218524
Loss in iteration 129 : 0.3247390530438155
Loss in iteration 130 : 0.3247271860969948
Loss in iteration 131 : 0.3247155743389036
Loss in iteration 132 : 0.32470424652364377
Loss in iteration 133 : 0.3246931387150815
Loss in iteration 134 : 0.32468217303625724
Loss in iteration 135 : 0.32467133592074726
Loss in iteration 136 : 0.32466067520297154
Loss in iteration 137 : 0.32465023448214797
Loss in iteration 138 : 0.32464000439265617
Loss in iteration 139 : 0.32462993778608107
Loss in iteration 140 : 0.3246200009900724
Loss in iteration 141 : 0.32461020147369923
Loss in iteration 142 : 0.32460056794677916
Loss in iteration 143 : 0.3245911124723
Loss in iteration 144 : 0.3245818170562256
Loss in iteration 145 : 0.32457265388617557
Loss in iteration 146 : 0.32456361198074385
Loss in iteration 147 : 0.32455470158247024
Loss in iteration 148 : 0.32454593600603443
Loss in iteration 149 : 0.32453731392566304
Loss in iteration 150 : 0.3245288202479617
Loss in iteration 151 : 0.3245204411189212
Loss in iteration 152 : 0.3245121749145757
Loss in iteration 153 : 0.3245040286594084
Loss in iteration 154 : 0.32449600632361664
Loss in iteration 155 : 0.3244881025802847
Loss in iteration 156 : 0.32448030740225736
Loss in iteration 157 : 0.32447261470033667
Loss in iteration 158 : 0.3244650253382684
Loss in iteration 159 : 0.32445754248667913
Loss in iteration 160 : 0.3244501655512329
Loss in iteration 161 : 0.32444288919090974
Loss in iteration 162 : 0.32443570746452643
Loss in iteration 163 : 0.32442861786145205
Loss in iteration 164 : 0.3244216210910518
Loss in iteration 165 : 0.3244147176538596
Loss in iteration 166 : 0.32440790533325126
Loss in iteration 167 : 0.3244011800124468
Loss in iteration 168 : 0.324394538343779
Loss in iteration 169 : 0.3243879791718442
Loss in iteration 170 : 0.32438150248289443
Loss in iteration 171 : 0.32437510743044873
Loss in iteration 172 : 0.3243687916417666
Loss in iteration 173 : 0.32436255227273536
Loss in iteration 174 : 0.3243563873918887
Loss in iteration 175 : 0.32435029620611033
Loss in iteration 176 : 0.32434427811469496
Loss in iteration 177 : 0.32433833178552185
Loss in iteration 178 : 0.32433245519732545
Loss in iteration 179 : 0.3243266464194659
Loss in iteration 180 : 0.3243209041859795
Loss in iteration 181 : 0.3243152277117086
Loss in iteration 182 : 0.3243096160824656
Loss in iteration 183 : 0.3243040679259121
Loss in iteration 184 : 0.32429858164688663
Loss in iteration 185 : 0.32429315587704494
Loss in iteration 186 : 0.32428778963515376
Loss in iteration 187 : 0.32428248208777877
Loss in iteration 188 : 0.32427723223166777
Loss in iteration 189 : 0.32427203883859596
Loss in iteration 190 : 0.32426690066889285
Loss in iteration 191 : 0.32426181668264714
Loss in iteration 192 : 0.32425678603145164
Loss in iteration 193 : 0.3242518078802705
Loss in iteration 194 : 0.32424688127398027
Testing accuracy  of updater 1 on alg 0 with rate 0.4 = 0.8508076899453351, training accuracy 0.8482493857493858, time elapsed: 4266 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6509822051128222
Loss in iteration 3 : 0.5921162171312899
Loss in iteration 4 : 0.5436944900346836
Loss in iteration 5 : 0.5179478066288226
Loss in iteration 6 : 0.5108111602911033
Loss in iteration 7 : 0.511396528560556
Loss in iteration 8 : 0.5101683404249143
Loss in iteration 9 : 0.5018403084314089
Loss in iteration 10 : 0.48525352349479073
Loss in iteration 11 : 0.4624246930690256
Loss in iteration 12 : 0.4374548818695912
Loss in iteration 13 : 0.4152051754347772
Loss in iteration 14 : 0.39964316562107394
Loss in iteration 15 : 0.39230042409218996
Loss in iteration 16 : 0.39172898285928887
Loss in iteration 17 : 0.39442073473881917
Loss in iteration 18 : 0.396602502316336
Loss in iteration 19 : 0.39579763072647917
Loss in iteration 20 : 0.39145411621135723
Loss in iteration 21 : 0.38463689382545246
Loss in iteration 22 : 0.3771929184879546
Loss in iteration 23 : 0.3708625468298775
Loss in iteration 24 : 0.36667898994730014
Loss in iteration 25 : 0.3647899947874746
Loss in iteration 26 : 0.36463791873252543
Loss in iteration 27 : 0.3653200604832927
Loss in iteration 28 : 0.36594833643712193
Loss in iteration 29 : 0.3658972110996331
Loss in iteration 30 : 0.36490794108030133
Loss in iteration 31 : 0.3630685821415001
Loss in iteration 32 : 0.3607104225834242
Loss in iteration 33 : 0.35826524430519857
Loss in iteration 34 : 0.35612452668288785
Loss in iteration 35 : 0.35453458052889775
Loss in iteration 36 : 0.3535496378454612
Loss in iteration 37 : 0.3530479776766016
Loss in iteration 38 : 0.35279775092507165
Loss in iteration 39 : 0.35254545468705645
Loss in iteration 40 : 0.3520961901580239
Loss in iteration 41 : 0.351361871688929
Loss in iteration 42 : 0.3503677247733893
Loss in iteration 43 : 0.34922259905460684
Loss in iteration 44 : 0.34806930768929084
Loss in iteration 45 : 0.34703449421525046
Loss in iteration 46 : 0.346193706971962
Loss in iteration 47 : 0.3455591902202388
Loss in iteration 48 : 0.34508912868995084
Loss in iteration 49 : 0.34471081030911266
Loss in iteration 50 : 0.3443478101965777
Loss in iteration 51 : 0.3439425024389873
Loss in iteration 52 : 0.34346857521069807
Loss in iteration 53 : 0.34293217918164753
Loss in iteration 54 : 0.34236368769963876
Loss in iteration 55 : 0.34180414491545613
Loss in iteration 56 : 0.3412911335819895
Loss in iteration 57 : 0.3408481326188127
Loss in iteration 58 : 0.34047981012478373
Loss in iteration 59 : 0.3401736364336559
Loss in iteration 60 : 0.33990630833903784
Loss in iteration 61 : 0.3396522787750407
Loss in iteration 62 : 0.3393914728891869
Loss in iteration 63 : 0.33911399075717547
Loss in iteration 64 : 0.3388208944846635
Loss in iteration 65 : 0.33852154759936076
Loss in iteration 66 : 0.3382289596634156
Loss in iteration 67 : 0.3379549352141669
Loss in iteration 68 : 0.33770653975985576
Loss in iteration 69 : 0.3374846927362719
Loss in iteration 70 : 0.3372848870803136
Loss in iteration 71 : 0.33709939415898915
Loss in iteration 72 : 0.3369200013187847
Loss in iteration 73 : 0.3367403677562223
Loss in iteration 74 : 0.3365573829861093
Loss in iteration 75 : 0.336371326644851
Loss in iteration 76 : 0.3361850155109021
Loss in iteration 77 : 0.336002380230225
Loss in iteration 78 : 0.3358269947167066
Loss in iteration 79 : 0.3356609967842447
Loss in iteration 80 : 0.33550464334715135
Loss in iteration 81 : 0.33535651285722706
Loss in iteration 82 : 0.3352141744360324
Loss in iteration 83 : 0.33507503772842195
Loss in iteration 84 : 0.3349370961531793
Loss in iteration 85 : 0.3347993612976352
Loss in iteration 86 : 0.3346619166291494
Loss in iteration 87 : 0.3345256465442025
Loss in iteration 88 : 0.33439178349044496
Loss in iteration 89 : 0.33426144282292863
Loss in iteration 90 : 0.3341352856108855
Loss in iteration 91 : 0.3340133839064677
Loss in iteration 92 : 0.3338952881529575
Loss in iteration 93 : 0.33378023673838914
Loss in iteration 94 : 0.33366741782454346
Loss in iteration 95 : 0.3335561961719608
Loss in iteration 96 : 0.3334462452857792
Loss in iteration 97 : 0.3333375646564091
Loss in iteration 98 : 0.33323039932792997
Loss in iteration 99 : 0.33312510408451845
Loss in iteration 100 : 0.3330220024446286
Loss in iteration 101 : 0.3329212824785749
Loss in iteration 102 : 0.3328229526845008
Loss in iteration 103 : 0.3327268592270883
Loss in iteration 104 : 0.3326327477178355
Loss in iteration 105 : 0.33254034294398566
Loss in iteration 106 : 0.3324494198063954
Loss in iteration 107 : 0.33235984642479893
Loss in iteration 108 : 0.3322715921653869
Loss in iteration 109 : 0.3321847049949556
Loss in iteration 110 : 0.33209927067581785
Loss in iteration 111 : 0.33201536919071234
Loss in iteration 112 : 0.3319330416039536
Loss in iteration 113 : 0.3318522749550175
Loss in iteration 114 : 0.33177300605746163
Loss in iteration 115 : 0.3316951394195567
Loss in iteration 116 : 0.3316185713456815
Loss in iteration 117 : 0.33154321203977943
Loss in iteration 118 : 0.33146899970451665
Loss in iteration 119 : 0.3313959041095728
Loss in iteration 120 : 0.3313239206304412
Loss in iteration 121 : 0.33125305831154184
Loss in iteration 122 : 0.3311833265415534
Loss in iteration 123 : 0.3311147244431962
Loss in iteration 124 : 0.33104723550775444
Loss in iteration 125 : 0.3309808280075127
Loss in iteration 126 : 0.3309154599528112
Loss in iteration 127 : 0.33085108628811566
Loss in iteration 128 : 0.33078766581894764
Loss in iteration 129 : 0.3307251659201223
Loss in iteration 130 : 0.330663564088579
Loss in iteration 131 : 0.33060284649210947
Loss in iteration 132 : 0.33054300450020485
Loss in iteration 133 : 0.33048403057373077
Loss in iteration 134 : 0.3304259148090697
Loss in iteration 135 : 0.3303686429948077
Loss in iteration 136 : 0.33031219643939086
Loss in iteration 137 : 0.33025655327063785
Loss in iteration 138 : 0.3302016905439864
Loss in iteration 139 : 0.33014758639296743
Loss in iteration 140 : 0.33009422159156004
Loss in iteration 141 : 0.3300415801882923
Loss in iteration 142 : 0.32998964920354223
Loss in iteration 143 : 0.32993841765144927
Loss in iteration 144 : 0.3298878752911197
Loss in iteration 145 : 0.3298380115118111
Loss in iteration 146 : 0.3297888146411232
Loss in iteration 147 : 0.32974027178922305
Loss in iteration 148 : 0.32969236916749
Loss in iteration 149 : 0.32964509269654946
Loss in iteration 150 : 0.32959842867152056
Loss in iteration 151 : 0.3295523642806534
Loss in iteration 152 : 0.32950688785448323
Loss in iteration 153 : 0.3294619888231942
Loss in iteration 154 : 0.32941765744743395
Loss in iteration 155 : 0.32937388444008625
Loss in iteration 156 : 0.32933066060505783
Loss in iteration 157 : 0.32928797659025905
Loss in iteration 158 : 0.32924582280084275
Loss in iteration 159 : 0.32920418946442764
Loss in iteration 160 : 0.32916306679843577
Loss in iteration 161 : 0.32912244521032574
Loss in iteration 162 : 0.329082315465512
Loss in iteration 163 : 0.3290426687796278
Loss in iteration 164 : 0.3290034968216563
Loss in iteration 165 : 0.3289647916425354
Loss in iteration 166 : 0.32892654556229634
Loss in iteration 167 : 0.3288887510542699
Loss in iteration 168 : 0.3288514006585177
Loss in iteration 169 : 0.32881448694210985
Loss in iteration 170 : 0.3287780025073609
Loss in iteration 171 : 0.32874194003523693
Loss in iteration 172 : 0.32870629234372495
Loss in iteration 173 : 0.3286710524405347
Loss in iteration 174 : 0.32863621355511374
Loss in iteration 175 : 0.32860176914369427
Loss in iteration 176 : 0.32856771286998815
Loss in iteration 177 : 0.3285340385705031
Loss in iteration 178 : 0.3285007402161957
Loss in iteration 179 : 0.3284678118809342
Loss in iteration 180 : 0.328435247723366
Loss in iteration 181 : 0.3284030419837301
Loss in iteration 182 : 0.32837118899271456
Loss in iteration 183 : 0.32833968318656626
Loss in iteration 184 : 0.3283085191220804
Loss in iteration 185 : 0.3282776914864037
Loss in iteration 186 : 0.32824719509899586
Loss in iteration 187 : 0.3282170249059271
Loss in iteration 188 : 0.3281871759687632
Loss in iteration 189 : 0.32815764345156123
Loss in iteration 190 : 0.3281284226091922
Loss in iteration 191 : 0.328099508779434
Loss in iteration 192 : 0.32807089737964007
Loss in iteration 193 : 0.32804258390740904
Loss in iteration 194 : 0.32801456394367545
Loss in iteration 195 : 0.3279868331562872
Loss in iteration 196 : 0.32795938730234225
Loss in iteration 197 : 0.32793222222830476
Loss in iteration 198 : 0.32790533386771265
Loss in iteration 199 : 0.32787871823702436
Loss in iteration 200 : 0.3278523714305965
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.8520361157177078, training accuracy 0.8463144963144963, time elapsed: 4535 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 6.479744479830772
Loss in iteration 3 : 1.5646484738486262
Loss in iteration 4 : 5.64887591831858
Loss in iteration 5 : 3.4143021304980223
Loss in iteration 6 : 1.662176909443032
Loss in iteration 7 : 1.6584874547242603
Loss in iteration 8 : 1.6505423003759494
Loss in iteration 9 : 1.5751100454474665
Loss in iteration 10 : 1.4698985181049256
Loss in iteration 11 : 1.340596797149592
Loss in iteration 12 : 1.2042528163097286
Loss in iteration 13 : 1.0801841355489632
Loss in iteration 14 : 0.9886737911755527
Loss in iteration 15 : 0.9274586420184495
Loss in iteration 16 : 0.8979451857001427
Loss in iteration 17 : 0.9469214445799766
Loss in iteration 18 : 1.7451630781962613
Loss in iteration 19 : 4.8477199724123325
Loss in iteration 20 : 0.7532198647982832
Loss in iteration 21 : 1.722165070563847
Loss in iteration 22 : 3.812709678217866
Loss in iteration 23 : 1.0594179646656328
Loss in iteration 24 : 1.091134717116371
Loss in iteration 25 : 1.5320468948749206
Loss in iteration 26 : 2.5665025786581785
Loss in iteration 27 : 1.5463341079371462
Loss in iteration 28 : 2.120731736454065
Loss in iteration 29 : 1.621607651502398
Loss in iteration 30 : 2.179359160722983
Loss in iteration 31 : 1.4583032768288917
Loss in iteration 32 : 1.7632795386324096
Loss in iteration 33 : 1.4248999726538807
Loss in iteration 34 : 1.9256331808385643
Loss in iteration 35 : 1.5735847273689616
Loss in iteration 36 : 2.367355521487375
Loss in iteration 37 : 1.4780103298648657
Loss in iteration 38 : 2.0414823019998996
Loss in iteration 39 : 1.6294632402716567
Loss in iteration 40 : 2.342930094606013
Loss in iteration 41 : 1.431464497062149
Loss in iteration 42 : 1.746678324609755
Loss in iteration 43 : 1.520045073945642
Loss in iteration 44 : 2.07746571730779
Loss in iteration 45 : 1.487469027863106
Loss in iteration 46 : 1.9794851119338124
Loss in iteration 47 : 1.5388454877284226
Loss in iteration 48 : 2.138493933880509
Loss in iteration 49 : 1.4983414756118874
Loss in iteration 50 : 2.0184816477219587
Loss in iteration 51 : 1.5372006905805176
Loss in iteration 52 : 2.135261763746294
Loss in iteration 53 : 1.4936789409091975
Loss in iteration 54 : 2.008145911734058
Loss in iteration 55 : 1.5315337307502133
Loss in iteration 56 : 2.1335787640974306
Loss in iteration 57 : 1.495426096202642
Loss in iteration 58 : 2.0245808369319658
Loss in iteration 59 : 1.5337882461166663
Loss in iteration 60 : 2.138402581532039
Loss in iteration 61 : 1.4980419711506123
Loss in iteration 62 : 2.020016306739665
Loss in iteration 63 : 1.5330853206594857
Loss in iteration 64 : 2.120808323802364
Loss in iteration 65 : 1.5005137912320907
Loss in iteration 66 : 2.014012725750457
Loss in iteration 67 : 1.5307607296276116
Loss in iteration 68 : 2.106868010624279
Loss in iteration 69 : 1.5055429784157695
Loss in iteration 70 : 2.025614261057572
Loss in iteration 71 : 1.5305514114116008
Loss in iteration 72 : 2.1019038552236124
Loss in iteration 73 : 1.5099982295632213
Loss in iteration 74 : 2.034658833005696
Loss in iteration 75 : 1.5293287728791831
Loss in iteration 76 : 2.0931876966149763
Loss in iteration 77 : 1.5128329475909852
Loss in iteration 78 : 2.039777104021526
Loss in iteration 79 : 1.5276183604628404
Loss in iteration 80 : 2.085672908553849
Loss in iteration 81 : 1.5154173257808394
Loss in iteration 82 : 2.0465467177151435
Loss in iteration 83 : 1.5266528626221005
Loss in iteration 84 : 2.0811533008713363
Loss in iteration 85 : 1.517429300539118
Loss in iteration 86 : 2.0512331121362672
Loss in iteration 87 : 1.5255880054958024
Loss in iteration 88 : 2.0761643072266636
Loss in iteration 89 : 1.5184915812043813
Loss in iteration 90 : 2.053453363794054
Loss in iteration 91 : 1.5243861366461005
Loss in iteration 92 : 2.0719189279975536
Loss in iteration 93 : 1.5191195241098128
Loss in iteration 94 : 2.055469619008904
Loss in iteration 95 : 1.52342811657522
Loss in iteration 96 : 2.0691483111708893
Loss in iteration 97 : 1.5195402185388769
Loss in iteration 98 : 2.0570514613061888
Loss in iteration 99 : 1.5226591486559982
Loss in iteration 100 : 2.066937089541857
Loss in iteration 101 : 1.519793722754242
Loss in iteration 102 : 2.058017321713615
Loss in iteration 103 : 1.5220763592216282
Loss in iteration 104 : 2.0652579655359653
Loss in iteration 105 : 1.5200028203718405
Loss in iteration 106 : 2.058787995285679
Loss in iteration 107 : 1.5216821547977468
Loss in iteration 108 : 2.0640812567507334
Loss in iteration 109 : 1.5201624965613474
Loss in iteration 110 : 2.0593071903143745
Loss in iteration 111 : 1.521367185092335
Loss in iteration 112 : 2.063101027044202
Loss in iteration 113 : 1.5202270347584623
Loss in iteration 114 : 2.0595489374115226
Loss in iteration 115 : 1.5210780670753303
Loss in iteration 116 : 2.0622791479783786
Loss in iteration 117 : 1.5202185140537179
Loss in iteration 118 : 2.059651306670445
Loss in iteration 119 : 1.520816478993815
Loss in iteration 120 : 2.061611072458752
Loss in iteration 121 : 1.5201656028958703
Loss in iteration 122 : 2.0596495210262753
Loss in iteration 123 : 1.5205827295792267
Loss in iteration 124 : 2.061043647465516
Loss in iteration 125 : 1.520091006351446
Loss in iteration 126 : 2.059583152278343
Loss in iteration 127 : 1.520384434599537
Loss in iteration 128 : 2.060587967053601
Loss in iteration 129 : 1.5200165951306803
Loss in iteration 130 : 2.0595141642232084
Loss in iteration 131 : 1.5202236766750108
Loss in iteration 132 : 2.0602436587204593
Loss in iteration 133 : 1.5199467249327383
Loss in iteration 134 : 2.059452304931375
Loss in iteration 135 : 1.5200889759021878
Loss in iteration 136 : 2.0599780349323322
Loss in iteration 137 : 1.5198763880257684
Loss in iteration 138 : 2.059391888360394
Loss in iteration 139 : 1.5199698619606379
Loss in iteration 140 : 2.059768984971235
Loss in iteration 141 : 1.5198033814892573
Loss in iteration 142 : 2.0593315314539056
Loss in iteration 143 : 1.5198604016806623
Loss in iteration 144 : 2.059596009866786
Loss in iteration 145 : 1.5197266956328879
Loss in iteration 146 : 2.059261040002964
Loss in iteration 147 : 1.519756282500996
Loss in iteration 148 : 2.059437129329955
Loss in iteration 149 : 1.5196460769690292
Loss in iteration 150 : 2.0591724275034893
Loss in iteration 151 : 1.5196555955672078
Loss in iteration 152 : 2.0592810620772726
Loss in iteration 153 : 1.5195628530451109
Loss in iteration 154 : 2.0590660158911382
Loss in iteration 155 : 1.519558099389705
Loss in iteration 156 : 2.0591241798730113
Loss in iteration 157 : 1.5194785725086022
Loss in iteration 158 : 2.0589447795989577
Loss in iteration 159 : 1.5194638617200298
Loss in iteration 160 : 2.0589659717680044
Loss in iteration 161 : 1.5193945985448167
Loss in iteration 162 : 2.058813395877103
Loss in iteration 163 : 1.5193732781257092
Loss in iteration 164 : 2.058808827432784
Loss in iteration 165 : 1.519312265887153
Loss in iteration 166 : 2.058677493000351
Loss in iteration 167 : 1.5192867965063954
Loss in iteration 168 : 2.058655586199212
Loss in iteration 169 : 1.5192324479315942
Loss in iteration 170 : 2.058541192946031
Loss in iteration 171 : 1.519204443302506
Loss in iteration 172 : 2.058507730908339
Loss in iteration 173 : 1.5191553966412394
Loss in iteration 174 : 2.0584067557033228
Loss in iteration 175 : 1.5191258574904072
Loss in iteration 176 : 2.058365623657755
Loss in iteration 177 : 1.5190809577048479
Loss in iteration 178 : 2.058275127501732
Loss in iteration 179 : 1.5190504864622125
Loss in iteration 180 : 2.058228819476383
Loss in iteration 181 : 1.519008788529712
Loss in iteration 182 : 2.0581463563645532
Loss in iteration 183 : 1.5189777716566577
Loss in iteration 184 : 2.058096531655069
Loss in iteration 185 : 1.5189385699009967
Loss in iteration 186 : 2.058020267029903
Loss in iteration 187 : 1.5189073110410172
Loss in iteration 188 : 2.0579681940719583
Loss in iteration 189 : 1.5188701219266167
Loss in iteration 190 : 2.0578968664608586
Loss in iteration 191 : 1.5188388774736257
Loss in iteration 192 : 2.0578435556753245
Loss in iteration 193 : 1.5188033695506304
Loss in iteration 194 : 2.0577762953177037
Loss in iteration 195 : 1.5187723399090471
Loss in iteration 196 : 2.0577225212228702
Loss in iteration 197 : 1.5187382714498607
Loss in iteration 198 : 2.0576586969139288
Loss in iteration 199 : 1.5187075975240336
Loss in iteration 200 : 2.057605009892775
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.8375406916037098, training accuracy 0.8390049140049141, time elapsed: 3752 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 4.535821616779211
Loss in iteration 3 : 1.114375883751499
Loss in iteration 4 : 3.8209090495984084
Loss in iteration 5 : 2.3756332285016764
Loss in iteration 6 : 1.1707305185697263
Loss in iteration 7 : 1.1613416521639286
Loss in iteration 8 : 1.1538960857676175
Loss in iteration 9 : 1.1008498556692192
Loss in iteration 10 : 1.0286989494761898
Loss in iteration 11 : 0.9412127318337729
Loss in iteration 12 : 0.8505216056766202
Loss in iteration 13 : 0.7699015544106682
Loss in iteration 14 : 0.7112179648735651
Loss in iteration 15 : 0.6746442801567413
Loss in iteration 16 : 0.6581060306657234
Loss in iteration 17 : 0.6595042749259656
Loss in iteration 18 : 0.8095607593092045
Loss in iteration 19 : 2.039892653025885
Loss in iteration 20 : 1.6851165243500568
Loss in iteration 21 : 3.5142637246506094
Loss in iteration 22 : 0.683599364819946
Loss in iteration 23 : 1.3017968321523454
Loss in iteration 24 : 1.6416741381099729
Loss in iteration 25 : 1.0910356121857587
Loss in iteration 26 : 1.0646537846415531
Loss in iteration 27 : 0.8990311333766424
Loss in iteration 28 : 0.8616360748591233
Loss in iteration 29 : 0.8010527494772129
Loss in iteration 30 : 0.9716249609144271
Loss in iteration 31 : 1.2158192024252057
Loss in iteration 32 : 2.2992104383917304
Loss in iteration 33 : 0.8306380758060785
Loss in iteration 34 : 1.2352499550323492
Loss in iteration 35 : 1.6957140084703552
Loss in iteration 36 : 2.946729268011614
Loss in iteration 37 : 0.6556308083740363
Loss in iteration 38 : 0.8604537295000446
Loss in iteration 39 : 0.988227672437567
Loss in iteration 40 : 1.0290560112036635
Loss in iteration 41 : 1.478354481467828
Loss in iteration 42 : 1.079467743329815
Loss in iteration 43 : 1.496768112236816
Loss in iteration 44 : 1.0882819226802303
Loss in iteration 45 : 1.4671522658526932
Loss in iteration 46 : 1.0738364320657983
Loss in iteration 47 : 1.4023511506339226
Loss in iteration 48 : 1.0662393705361826
Loss in iteration 49 : 1.390426138355117
Loss in iteration 50 : 1.0645530465862842
Loss in iteration 51 : 1.4137164703145748
Loss in iteration 52 : 1.0760071665091049
Loss in iteration 53 : 1.4621135205274245
Loss in iteration 54 : 1.0799674517351605
Loss in iteration 55 : 1.4688758687804124
Loss in iteration 56 : 1.0826808569210062
Loss in iteration 57 : 1.4621009318055689
Loss in iteration 58 : 1.0774575874393983
Loss in iteration 59 : 1.4346325132857667
Loss in iteration 60 : 1.075411447904242
Loss in iteration 61 : 1.42490668492078
Loss in iteration 62 : 1.0735530208220962
Loss in iteration 63 : 1.422202191382172
Loss in iteration 64 : 1.077050848099777
Loss in iteration 65 : 1.4347574244032693
Loss in iteration 66 : 1.0793071038512079
Loss in iteration 67 : 1.4393043466070135
Loss in iteration 68 : 1.081316137918793
Loss in iteration 69 : 1.4402586809506963
Loss in iteration 70 : 1.080500245235687
Loss in iteration 71 : 1.4334982476004938
Loss in iteration 72 : 1.0798802009344692
Loss in iteration 73 : 1.4300579532842683
Loss in iteration 74 : 1.0790500074974712
Loss in iteration 75 : 1.4285048469131718
Loss in iteration 76 : 1.079660329020169
Loss in iteration 77 : 1.4317397701074956
Loss in iteration 78 : 1.0803034311267412
Loss in iteration 79 : 1.4339871186175466
Loss in iteration 80 : 1.081013411189616
Loss in iteration 81 : 1.4351558997813707
Loss in iteration 82 : 1.0809855915960669
Loss in iteration 83 : 1.4337129672357327
Loss in iteration 84 : 1.0808224030987594
Loss in iteration 85 : 1.4322924883264887
Loss in iteration 86 : 1.0805434662674784
Loss in iteration 87 : 1.4312225948585402
Loss in iteration 88 : 1.0805728739296294
Loss in iteration 89 : 1.4314389192023123
Loss in iteration 90 : 1.0806634412366465
Loss in iteration 91 : 1.4317875505895594
Loss in iteration 92 : 1.0807761274412702
Loss in iteration 93 : 1.4319912708806606
Loss in iteration 94 : 1.0807081253980881
Loss in iteration 95 : 1.431579661951037
Loss in iteration 96 : 1.0805678912634415
Loss in iteration 97 : 1.4310666611156575
Loss in iteration 98 : 1.0803986018605176
Loss in iteration 99 : 1.4306485810642315
Loss in iteration 100 : 1.080316801680056
Loss in iteration 101 : 1.430581066726427
Loss in iteration 102 : 1.0802922174108152
Loss in iteration 103 : 1.430640063665399
Loss in iteration 104 : 1.0803032681863274
Loss in iteration 105 : 1.430693600349098
Loss in iteration 106 : 1.080290214809603
Loss in iteration 107 : 1.4305929093139595
Loss in iteration 108 : 1.0802570294057945
Loss in iteration 109 : 1.4304202019823606
Loss in iteration 110 : 1.080209421333252
Loss in iteration 111 : 1.4302417205158904
Loss in iteration 112 : 1.0801716645605126
Loss in iteration 113 : 1.4301319403932644
Loss in iteration 114 : 1.0801403547441442
Loss in iteration 115 : 1.4300568531975972
Loss in iteration 116 : 1.080109521719382
Loss in iteration 117 : 1.4299798658125928
Loss in iteration 118 : 1.0800658935652852
Loss in iteration 119 : 1.4298630328480888
Loss in iteration 120 : 1.080010475918753
Loss in iteration 121 : 1.4297226609605005
Loss in iteration 122 : 1.0799483836141681
Loss in iteration 123 : 1.4295834105190208
Loss in iteration 124 : 1.0798891701396942
Loss in iteration 125 : 1.4294709701918544
Loss in iteration 126 : 1.0798358431967658
Loss in iteration 127 : 1.429383694531965
Loss in iteration 128 : 1.07978810074249
Loss in iteration 129 : 1.4293112749748709
Loss in iteration 130 : 1.0797424939097935
Loss in iteration 131 : 1.4292408427710508
Loss in iteration 132 : 1.0796978033689941
Loss in iteration 133 : 1.4291716045289924
Loss in iteration 134 : 1.079654230643642
Loss in iteration 135 : 1.4291076872785733
Loss in iteration 136 : 1.0796133216879087
Loss in iteration 137 : 1.4290543544310073
Loss in iteration 138 : 1.0795754745233785
Loss in iteration 139 : 1.4290106348083973
Loss in iteration 140 : 1.0795401360015273
Loss in iteration 141 : 1.4289721415717778
Loss in iteration 142 : 1.079505967057879
Loss in iteration 143 : 1.4289335603311375
Loss in iteration 144 : 1.0794721375658174
Loss in iteration 145 : 1.428892749232802
Loss in iteration 146 : 1.0794384223703406
Loss in iteration 147 : 1.4288499969129336
Loss in iteration 148 : 1.0794051327332008
Loss in iteration 149 : 1.4288067413968721
Loss in iteration 150 : 1.0793724691600435
Loss in iteration 151 : 1.428763397701477
Loss in iteration 152 : 1.0793403831386543
Loss in iteration 153 : 1.42871940330837
Loss in iteration 154 : 1.0793085886346014
Loss in iteration 155 : 1.42867379921025
Loss in iteration 156 : 1.0792768757133118
Loss in iteration 157 : 1.4286263012722287
Loss in iteration 158 : 1.079245214325448
Loss in iteration 159 : 1.4285773502167154
Loss in iteration 160 : 1.079213755148204
Loss in iteration 161 : 1.4285277712814786
Loss in iteration 162 : 1.0791826674330158
Loss in iteration 163 : 1.428478154850333
Loss in iteration 164 : 1.0791520559183554
Loss in iteration 165 : 1.4284287104510591
Loss in iteration 166 : 1.079121937351638
Loss in iteration 167 : 1.4283793841569603
Loss in iteration 168 : 1.0790923046288912
Loss in iteration 169 : 1.428330156288514
Loss in iteration 170 : 1.0790631658736
Loss in iteration 171 : 1.4282811414383163
Loss in iteration 172 : 1.0790345545575186
Loss in iteration 173 : 1.4282325430726222
Loss in iteration 174 : 1.0790064971638165
Loss in iteration 175 : 1.42818450852343
Loss in iteration 176 : 1.0789789912022345
Loss in iteration 177 : 1.4281370717582924
Loss in iteration 178 : 1.0789520013127922
Loss in iteration 179 : 1.428090179885952
Loss in iteration 180 : 1.0789254796739043
Loss in iteration 181 : 1.428043778054307
Loss in iteration 182 : 1.078899384589332
Loss in iteration 183 : 1.427997856835721
Loss in iteration 184 : 1.0788736892504829
Loss in iteration 185 : 1.4279524512396606
Loss in iteration 186 : 1.078848376205708
Loss in iteration 187 : 1.427907602544666
Loss in iteration 188 : 1.0788234295986474
Loss in iteration 189 : 1.4278633315134575
Loss in iteration 190 : 1.0787988307768113
Loss in iteration 191 : 1.4278196350309416
Loss in iteration 192 : 1.0787745604278736
Loss in iteration 193 : 1.427776502765909
Loss in iteration 194 : 1.0787506020984543
Loss in iteration 195 : 1.4277339303748138
Loss in iteration 196 : 1.0787269440177703
Loss in iteration 197 : 1.4276919209806427
Loss in iteration 198 : 1.0787035775534306
Loss in iteration 199 : 1.427650476044747
Loss in iteration 200 : 1.0786804947470514
Testing accuracy  of updater 2 on alg 0 with rate 7.0 = 0.8420244456728703, training accuracy 0.8407248157248157, time elapsed: 3572 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 2.5919465816582847
Loss in iteration 3 : 0.6818832272519664
Loss in iteration 4 : 1.9406566065149098
Loss in iteration 5 : 1.3406593820897783
Loss in iteration 6 : 0.689988158845404
Loss in iteration 7 : 0.6748376213652656
Loss in iteration 8 : 0.6692464795874737
Loss in iteration 9 : 0.641701304021489
Loss in iteration 10 : 0.6050350313211194
Loss in iteration 11 : 0.5619903669428136
Loss in iteration 12 : 0.5192269754126262
Loss in iteration 13 : 0.4834600034604612
Loss in iteration 14 : 0.4596738850590509
Loss in iteration 15 : 0.4483727906997382
Loss in iteration 16 : 0.44519649447693416
Loss in iteration 17 : 0.44134789137754243
Loss in iteration 18 : 0.43013744972676354
Loss in iteration 19 : 0.41430535709920313
Loss in iteration 20 : 0.4184890117934548
Loss in iteration 21 : 0.5748558707829349
Loss in iteration 22 : 1.1433033850018655
Loss in iteration 23 : 2.1343791151352485
Loss in iteration 24 : 0.5294172511488613
Loss in iteration 25 : 0.9560798708908063
Loss in iteration 26 : 0.979110081442317
Loss in iteration 27 : 0.6269053273636374
Loss in iteration 28 : 0.5684064140766696
Loss in iteration 29 : 0.5210017530345855
Loss in iteration 30 : 0.4834426021687081
Loss in iteration 31 : 0.4542362024934783
Loss in iteration 32 : 0.46114387599737044
Loss in iteration 33 : 0.5392486426473372
Loss in iteration 34 : 0.9258980307424095
Loss in iteration 35 : 0.9439736256228405
Loss in iteration 36 : 1.7269298624541631
Loss in iteration 37 : 0.3944822740601726
Loss in iteration 38 : 0.4378295017173259
Loss in iteration 39 : 0.46890996268005525
Loss in iteration 40 : 0.5944235903307798
Loss in iteration 41 : 0.9391016698470438
Loss in iteration 42 : 0.6930973149027614
Loss in iteration 43 : 0.9805154170746789
Loss in iteration 44 : 0.6402353884348861
Loss in iteration 45 : 0.7559074006870626
Loss in iteration 46 : 0.6426183067010484
Loss in iteration 47 : 0.7443494029513534
Loss in iteration 48 : 0.6062637578857732
Loss in iteration 49 : 0.6867932163725426
Loss in iteration 50 : 0.6095114460397169
Loss in iteration 51 : 0.7476888072201403
Loss in iteration 52 : 0.6519913352212083
Loss in iteration 53 : 0.8680947361665117
Loss in iteration 54 : 0.6633423507549235
Loss in iteration 55 : 0.8734186814064181
Loss in iteration 56 : 0.6648461431055382
Loss in iteration 57 : 0.8452426619696204
Loss in iteration 58 : 0.6512290328153292
Loss in iteration 59 : 0.7892275700399806
Loss in iteration 60 : 0.6376968267507671
Loss in iteration 61 : 0.7561363941875816
Loss in iteration 62 : 0.6300150564162528
Loss in iteration 63 : 0.7524927307122851
Loss in iteration 64 : 0.6371251621515032
Loss in iteration 65 : 0.7833807026663515
Loss in iteration 66 : 0.6498971209711448
Loss in iteration 67 : 0.8144866083583842
Loss in iteration 68 : 0.6547702200763538
Loss in iteration 69 : 0.8169843662581084
Loss in iteration 70 : 0.6528756457866742
Loss in iteration 71 : 0.8027136337762766
Loss in iteration 72 : 0.6471284338187513
Loss in iteration 73 : 0.7850998852882691
Loss in iteration 74 : 0.6424074123863605
Loss in iteration 75 : 0.7772839109168169
Loss in iteration 76 : 0.6421831984265011
Loss in iteration 77 : 0.7829853744361794
Loss in iteration 78 : 0.6459429074624917
Loss in iteration 79 : 0.7952190209310109
Loss in iteration 80 : 0.6494772479137164
Loss in iteration 81 : 0.8021310701404191
Loss in iteration 82 : 0.6501680074479987
Loss in iteration 83 : 0.7999914647743652
Loss in iteration 84 : 0.6484523049905979
Loss in iteration 85 : 0.7932027733925375
Loss in iteration 86 : 0.6461375142162757
Loss in iteration 87 : 0.7877699849778094
Loss in iteration 88 : 0.6450631819335073
Loss in iteration 89 : 0.7873469329643117
Loss in iteration 90 : 0.6457669432617211
Loss in iteration 91 : 0.7910750232937855
Loss in iteration 92 : 0.6472211057157973
Loss in iteration 93 : 0.7949672491083843
Loss in iteration 94 : 0.6480487288155177
Loss in iteration 95 : 0.7958834338135988
Loss in iteration 96 : 0.6477770027879135
Loss in iteration 97 : 0.7939199950360482
Loss in iteration 98 : 0.6468700123618368
Loss in iteration 99 : 0.7912586685786508
Loss in iteration 100 : 0.6461301338634825
Loss in iteration 101 : 0.7899975017538805
Loss in iteration 102 : 0.6460527989064255
Loss in iteration 103 : 0.790688031173983
Loss in iteration 104 : 0.6465129613598806
Loss in iteration 105 : 0.7922608642259299
Loss in iteration 106 : 0.6470002637182889
Loss in iteration 107 : 0.7932222309182481
Loss in iteration 108 : 0.6471232916208401
Loss in iteration 109 : 0.7929445190168393
Loss in iteration 110 : 0.6468732399761704
Loss in iteration 111 : 0.7919097678616798
Loss in iteration 112 : 0.6465195710995136
Loss in iteration 113 : 0.7910360678288353
Loss in iteration 114 : 0.6463399117587695
Loss in iteration 115 : 0.790888435954049
Loss in iteration 116 : 0.6464105300224501
Loss in iteration 117 : 0.7913464713312067
Loss in iteration 118 : 0.6465956862963028
Loss in iteration 119 : 0.7918606943091536
Loss in iteration 120 : 0.6467055855344119
Loss in iteration 121 : 0.7919812045568166
Loss in iteration 122 : 0.6466563743176325
Loss in iteration 123 : 0.7916764154895364
Loss in iteration 124 : 0.6465050403954102
Loss in iteration 125 : 0.791244930947177
Loss in iteration 126 : 0.6463700336059199
Loss in iteration 127 : 0.791004744722015
Loss in iteration 128 : 0.6463273643113706
Loss in iteration 129 : 0.7910507395606504
Loss in iteration 130 : 0.6463641170661804
Loss in iteration 131 : 0.7912397745101597
Loss in iteration 132 : 0.646410956461725
Loss in iteration 133 : 0.7913595723610155
Loss in iteration 134 : 0.6464096506099722
Loss in iteration 135 : 0.7913079066804386
Loss in iteration 136 : 0.6463543517804577
Loss in iteration 137 : 0.7911411776313332
Loss in iteration 138 : 0.6462822818775906
Loss in iteration 139 : 0.7909900134866278
Loss in iteration 140 : 0.6462345552889412
Loss in iteration 141 : 0.7909413117455619
Loss in iteration 142 : 0.6462241105386851
Loss in iteration 143 : 0.7909835684306762
Loss in iteration 144 : 0.6462330077220234
Loss in iteration 145 : 0.7910403149955266
Loss in iteration 146 : 0.6462338409257805
Loss in iteration 147 : 0.7910443185688025
Loss in iteration 148 : 0.6462128184390797
Loss in iteration 149 : 0.7909857640682001
Loss in iteration 150 : 0.6461765263959153
Loss in iteration 151 : 0.7909045069695992
Loss in iteration 152 : 0.6461415820531061
Loss in iteration 153 : 0.7908469474798446
Loss in iteration 154 : 0.6461194408112141
Loss in iteration 155 : 0.7908292555236994
Loss in iteration 156 : 0.6461090289922048
Loss in iteration 157 : 0.790832943874165
Loss in iteration 158 : 0.6461007661997539
Loss in iteration 159 : 0.7908278242515203
Loss in iteration 160 : 0.6460860147129823
Loss in iteration 161 : 0.7907979462516296
Loss in iteration 162 : 0.6460633837451627
Loss in iteration 163 : 0.7907500710723062
Loss in iteration 164 : 0.6460379285799033
Loss in iteration 165 : 0.7907027801464458
Loss in iteration 166 : 0.646015691713449
Loss in iteration 167 : 0.7906694692756723
Loss in iteration 168 : 0.6459989032081491
Loss in iteration 169 : 0.7906496344392752
Loss in iteration 170 : 0.6459852906117715
Loss in iteration 171 : 0.790632885162309
Loss in iteration 172 : 0.6459709643729601
Loss in iteration 173 : 0.7906093815229139
Loss in iteration 174 : 0.6459537760693791
Loss in iteration 175 : 0.7905771503784262
Loss in iteration 176 : 0.64593448871163
Loss in iteration 177 : 0.7905415566989362
Loss in iteration 178 : 0.6459154428704019
Loss in iteration 179 : 0.790509362605669
Loss in iteration 180 : 0.645898374804749
Loss in iteration 181 : 0.7904832631641395
Loss in iteration 182 : 0.6458832537848311
Loss in iteration 183 : 0.7904609030585867
Loss in iteration 184 : 0.6458687550653119
Loss in iteration 185 : 0.7904379625009438
Loss in iteration 186 : 0.6458535899191091
Loss in iteration 187 : 0.7904119317876279
Loss in iteration 188 : 0.6458374661201
Loss in iteration 189 : 0.7903835433579933
Loss in iteration 190 : 0.6458210480925095
Loss in iteration 191 : 0.790355383135601
Loss in iteration 192 : 0.6458052048025431
Loss in iteration 193 : 0.7903294581403932
Loss in iteration 194 : 0.6457902970138303
Loss in iteration 195 : 0.7903058267659551
Loss in iteration 196 : 0.6457760320594933
Loss in iteration 197 : 0.7902830523057459
Loss in iteration 198 : 0.6457618496154374
Loss in iteration 199 : 0.7902596673547378
Loss in iteration 200 : 0.6457474101474127
Testing accuracy  of updater 2 on alg 0 with rate 4.0 = 0.8428229224249125, training accuracy 0.8420454545454545, time elapsed: 3745 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.700828587648649
Loss in iteration 3 : 0.4518223404846068
Loss in iteration 4 : 0.4023148707648162
Loss in iteration 5 : 0.3707650066665527
Loss in iteration 6 : 0.36509815488668884
Loss in iteration 7 : 0.3626482874825865
Loss in iteration 8 : 0.3610023354284626
Loss in iteration 9 : 0.35911776627546715
Loss in iteration 10 : 0.35664881257854564
Loss in iteration 11 : 0.353597044730058
Loss in iteration 12 : 0.35014240704131483
Loss in iteration 13 : 0.34654613460127975
Loss in iteration 14 : 0.343085915965556
Loss in iteration 15 : 0.3400079910629379
Loss in iteration 16 : 0.33749078028537655
Loss in iteration 17 : 0.33562066976135685
Loss in iteration 18 : 0.334383673522483
Loss in iteration 19 : 0.3336760572199383
Loss in iteration 20 : 0.33333285127563933
Loss in iteration 21 : 0.3331675428275468
Loss in iteration 22 : 0.3330122500152161
Loss in iteration 23 : 0.33274773046356587
Loss in iteration 24 : 0.332316748038003
Loss in iteration 25 : 0.3317204375492855
Loss in iteration 26 : 0.33100245349961455
Loss in iteration 27 : 0.33022799729494673
Loss in iteration 28 : 0.3294641916978814
Loss in iteration 29 : 0.32876588330671813
Loss in iteration 30 : 0.3281682408811847
Loss in iteration 31 : 0.3276854559572034
Loss in iteration 32 : 0.3273137710883638
Loss in iteration 33 : 0.3270368252446157
Loss in iteration 34 : 0.32683159495120623
Loss in iteration 35 : 0.3266737182775945
Loss in iteration 36 : 0.3265415115005488
Loss in iteration 37 : 0.3264184233542428
Loss in iteration 38 : 0.32629398890344685
Loss in iteration 39 : 0.3261635499812984
Loss in iteration 40 : 0.32602711908250503
Loss in iteration 41 : 0.325887795746218
Loss in iteration 42 : 0.3257501136408741
Loss in iteration 43 : 0.32561861764078603
Loss in iteration 44 : 0.3254968607332757
Loss in iteration 45 : 0.32538689137282906
Loss in iteration 46 : 0.3252891944296691
Loss in iteration 47 : 0.32520297113236657
Loss in iteration 48 : 0.32512660555091744
Loss in iteration 49 : 0.32505816739374527
Loss in iteration 50 : 0.3249958342551276
Loss in iteration 51 : 0.3249381664096227
Loss in iteration 52 : 0.3248842184188496
Loss in iteration 53 : 0.32483351215534884
Loss in iteration 54 : 0.3247859190340303
Loss in iteration 55 : 0.3247415048444004
Loss in iteration 56 : 0.3247003825368648
Loss in iteration 57 : 0.3246626026526023
Loss in iteration 58 : 0.324628093742193
Loss in iteration 59 : 0.3245966505210049
Loss in iteration 60 : 0.3245679580064588
Loss in iteration 61 : 0.32454163589326407
Loss in iteration 62 : 0.32451728800827745
Loss in iteration 63 : 0.32449454523428956
Loss in iteration 64 : 0.3244730951033689
Loss in iteration 65 : 0.3244526959441194
Loss in iteration 66 : 0.3244331771208355
Loss in iteration 67 : 0.3244144291276257
Loss in iteration 68 : 0.32439638811098004
Loss in iteration 69 : 0.32437901907558625
Loss in iteration 70 : 0.3243623009936798
Loss in iteration 71 : 0.32434621570891353
Loss in iteration 72 : 0.3243307412476972
Loss in iteration 73 : 0.32431584915344774
Loss in iteration 74 : 0.32430150484684594
Loss in iteration 75 : 0.32428766978605983
Loss in iteration 76 : 0.32427430427999304
Loss in iteration 77 : 0.3242613700821393
Loss in iteration 78 : 0.32424883224766365
Loss in iteration 79 : 0.32423666007540525
Loss in iteration 80 : 0.3242248272177656
Loss in iteration 81 : 0.3242133111971382
Loss in iteration 82 : 0.32420209262011884
Loss in iteration 83 : 0.3241911543530379
Loss in iteration 84 : 0.3241804808457618
Loss in iteration 85 : 0.324170057697147
Loss in iteration 86 : 0.32415987147015674
Loss in iteration 87 : 0.32414990970307483
Loss in iteration 88 : 0.3241401610316268
Loss in iteration 89 : 0.32413061533278503
Loss in iteration 90 : 0.3241212638180992
Loss in iteration 91 : 0.32411209903254656
Loss in iteration 92 : 0.3241031147459292
Loss in iteration 93 : 0.3240943057503199
Loss in iteration 94 : 0.32408566759504576
Loss in iteration 95 : 0.32407719629858384
Loss in iteration 96 : 0.32406888807570605
Loss in iteration 97 : 0.32406073911033734
Loss in iteration 98 : 0.32405274539302265
Loss in iteration 99 : 0.3240449026296474
Loss in iteration 100 : 0.3240372062173104
Loss in iteration 101 : 0.32402965127542954
Loss in iteration 102 : 0.32402223271605507
Loss in iteration 103 : 0.3240149453364097
Loss in iteration 104 : 0.3240077839184114
Loss in iteration 105 : 0.32400074332321877
Loss in iteration 106 : 0.3239938185727902
Loss in iteration 107 : 0.32398700491412946
Loss in iteration 108 : 0.3239802978650493
Loss in iteration 109 : 0.32397369324235503
Loss in iteration 110 : 0.32396718717456346
Loss in iteration 111 : 0.3239607761018269
Loss in iteration 112 : 0.3239544567656792
Loss in iteration 113 : 0.32394822619099733
Loss in iteration 114 : 0.32394208166227256
Loss in iteration 115 : 0.3239360206960205
Loss in iteration 116 : 0.3239300410109867
Loss in iteration 117 : 0.3239241404977291
Loss in iteration 118 : 0.3239183171890052
Loss in iteration 119 : 0.3239125692322901
Loss in iteration 120 : 0.32390689486549656
Loss in iteration 121 : 0.323901292396737
Loss in iteration 122 : 0.3238957601884924
Loss in iteration 123 : 0.32389029664627034
Loss in iteration 124 : 0.3238849002114863
Loss in iteration 125 : 0.3238795693578514
Loss in iteration 126 : 0.32387430259061417
Loss in iteration 127 : 0.32386909844771117
Loss in iteration 128 : 0.3238639555020152
Loss in iteration 129 : 0.3238588723639682
Loss in iteration 130 : 0.32385384768400355
Loss in iteration 131 : 0.32384888015444296
Loss in iteration 132 : 0.3238439685105937
Loss in iteration 133 : 0.32383911153109196
Loss in iteration 134 : 0.3238343080374553
Loss in iteration 135 : 0.32382955689308934
Loss in iteration 136 : 0.3238248570018023
Loss in iteration 137 : 0.32382020730606154
Loss in iteration 138 : 0.3238156067850968
Loss in iteration 139 : 0.3238110544529509
Loss in iteration 140 : 0.3238065493565662
Loss in iteration 141 : 0.32380209057394405
Loss in iteration 142 : 0.32379767721238656
Loss in iteration 143 : 0.32379330840687287
Loss in iteration 144 : 0.32378898331851164
Loss in iteration 145 : 0.3237847011331466
Loss in iteration 146 : 0.3237804610600579
Loss in iteration 147 : 0.3237762623307574
Loss in iteration 148 : 0.3237721041979453
Loss in iteration 149 : 0.3237679859345286
Loss in iteration 150 : 0.32376390683275846
Loss in iteration 151 : 0.32375986620345965
Loss in iteration 152 : 0.32375586337533574
Loss in iteration 153 : 0.3237518976943242
Loss in iteration 154 : 0.32374796852302296
Loss in iteration 155 : 0.3237440752401272
Loss in iteration 156 : 0.32374021723992424
Loss in iteration 157 : 0.3237363939317618
Loss in iteration 158 : 0.32373260473957527
Loss in iteration 159 : 0.3237288491014034
Loss in iteration 160 : 0.3237251264688983
Loss in iteration 161 : 0.32372143630689143
Loss in iteration 162 : 0.3237177780929389
Loss in iteration 163 : 0.32371415131689063
Loss in iteration 164 : 0.32371055548048694
Loss in iteration 165 : 0.32370699009696247
Loss in iteration 166 : 0.3237034546906827
Loss in iteration 167 : 0.32369994879677577
Loss in iteration 168 : 0.32369647196081786
Loss in iteration 169 : 0.3236930237384915
Loss in iteration 170 : 0.32368960369529604
Loss in iteration 171 : 0.32368621140624454
Loss in iteration 172 : 0.323682846455591
Loss in iteration 173 : 0.3236795084365636
Loss in iteration 174 : 0.32367619695109523
Loss in iteration 175 : 0.3236729116095867
Loss in iteration 176 : 0.32366965203066284
Loss in iteration 177 : 0.32366641784094324
Loss in iteration 178 : 0.3236632086748168
Loss in iteration 179 : 0.3236600241742465
Loss in iteration 180 : 0.3236568639885357
Loss in iteration 181 : 0.3236537277741519
Loss in iteration 182 : 0.32365061519452787
Loss in iteration 183 : 0.323647525919879
Loss in iteration 184 : 0.32364445962701743
Loss in iteration 185 : 0.32364141599919116
Loss in iteration 186 : 0.3236383947258986
Loss in iteration 187 : 0.3236353955027499
Loss in iteration 188 : 0.32363241803128445
Loss in iteration 189 : 0.3236294620188298
Loss in iteration 190 : 0.32362652717835677
Loss in iteration 191 : 0.32362361322832905
Loss in iteration 192 : 0.323620719892572
Loss in iteration 193 : 0.3236178469001251
Loss in iteration 194 : 0.32361499398512256
Loss in iteration 195 : 0.3236121608866686
Loss in iteration 196 : 0.3236093473487076
Loss in iteration 197 : 0.323606553119914
Loss in iteration 198 : 0.32360377795356887
Loss in iteration 199 : 0.32360102160745796
Loss in iteration 200 : 0.32359828384376255
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.8505005835022419, training accuracy 0.8484643734643734, time elapsed: 3899 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5727942874333048
Loss in iteration 3 : 0.48167328124511927
Loss in iteration 4 : 0.4041423242609121
Loss in iteration 5 : 0.3864657244601372
Loss in iteration 6 : 0.37351606257739967
Loss in iteration 7 : 0.3658463498869314
Loss in iteration 8 : 0.36232313909573444
Loss in iteration 9 : 0.3602955222025691
Loss in iteration 10 : 0.35855316073182875
Loss in iteration 11 : 0.35668018900285303
Loss in iteration 12 : 0.35452672143297564
Loss in iteration 13 : 0.3520706776728476
Loss in iteration 14 : 0.34938064629805443
Loss in iteration 15 : 0.3465771578362778
Loss in iteration 16 : 0.3437980103136305
Loss in iteration 17 : 0.3411753987649929
Loss in iteration 18 : 0.3388208536586651
Loss in iteration 19 : 0.3368143518799114
Loss in iteration 20 : 0.3351975041692847
Loss in iteration 21 : 0.3339716895643292
Loss in iteration 22 : 0.3331014000513081
Loss in iteration 23 : 0.3325223140495614
Loss in iteration 24 : 0.3321528520211346
Loss in iteration 25 : 0.33190720489691233
Loss in iteration 26 : 0.33170745349306585
Loss in iteration 27 : 0.3314926891738016
Loss in iteration 28 : 0.33122389807031766
Loss in iteration 29 : 0.33088444619239793
Loss in iteration 30 : 0.33047694587789284
Loss in iteration 31 : 0.33001785439099746
Loss in iteration 32 : 0.329531282370151
Loss in iteration 33 : 0.32904325541727786
Loss in iteration 34 : 0.32857724007007827
Loss in iteration 35 : 0.3281512812978161
Loss in iteration 36 : 0.3277767184073361
Loss in iteration 37 : 0.32745820119108165
Loss in iteration 38 : 0.3271946171007446
Loss in iteration 39 : 0.32698053238034785
Loss in iteration 40 : 0.3268078060434402
Loss in iteration 41 : 0.3266671206250077
Loss in iteration 42 : 0.326549263222522
Loss in iteration 43 : 0.3264460702344002
Loss in iteration 44 : 0.326351013145047
Loss in iteration 45 : 0.32625944945055835
Loss in iteration 46 : 0.3261685936695761
Loss in iteration 47 : 0.3260772805610697
Loss in iteration 48 : 0.3259855984972028
Loss in iteration 49 : 0.325894467597407
Loss in iteration 50 : 0.3258052268283419
Loss in iteration 51 : 0.32571927898659214
Loss in iteration 52 : 0.32563782460686636
Loss in iteration 53 : 0.32556169772172683
Loss in iteration 54 : 0.3254913002144649
Loss in iteration 55 : 0.32542661898972935
Loss in iteration 56 : 0.32536730239061734
Loss in iteration 57 : 0.3253127694630622
Loss in iteration 58 : 0.32526232727642146
Loss in iteration 59 : 0.32521527642148956
Loss in iteration 60 : 0.32517099157139007
Loss in iteration 61 : 0.3251289711386638
Loss in iteration 62 : 0.32508885637186774
Loss in iteration 63 : 0.3250504248983244
Loss in iteration 64 : 0.3250135663931598
Loss in iteration 65 : 0.3249782488140125
Loss in iteration 66 : 0.32494448286318234
Loss in iteration 67 : 0.3249122905598324
Loss in iteration 68 : 0.3248816815822431
Loss in iteration 69 : 0.3248526388517349
Loss in iteration 70 : 0.32482511301432304
Loss in iteration 71 : 0.32479902421533235
Loss in iteration 72 : 0.32477426889730454
Loss in iteration 73 : 0.3247507292169915
Loss in iteration 74 : 0.32472828294341943
Loss in iteration 75 : 0.32470681221201936
Loss in iteration 76 : 0.3246862101222679
Loss in iteration 77 : 0.3246663847585392
Loss in iteration 78 : 0.32464726070375444
Loss in iteration 79 : 0.324628778459663
Loss in iteration 80 : 0.324610892376469
Loss in iteration 81 : 0.3245935677441862
Loss in iteration 82 : 0.32457677763999754
Loss in iteration 83 : 0.32456049999871056
Loss in iteration 84 : 0.32454471521439304
Loss in iteration 85 : 0.32452940442264
Loss in iteration 86 : 0.32451454847694333
Loss in iteration 87 : 0.32450012753310664
Loss in iteration 88 : 0.32448612109623615
Loss in iteration 89 : 0.32447250836322833
Loss in iteration 90 : 0.32445926870229413
Loss in iteration 91 : 0.3244463821397105
Loss in iteration 92 : 0.32443382976351604
Loss in iteration 93 : 0.324421593994622
Loss in iteration 94 : 0.32440965871200533
Loss in iteration 95 : 0.3243980092459538
Loss in iteration 96 : 0.3243866322705418
Loss in iteration 97 : 0.3243755156336895
Loss in iteration 98 : 0.32436464816244825
Loss in iteration 99 : 0.3243540194747735
Loss in iteration 100 : 0.32434361981967375
Loss in iteration 101 : 0.3243334399574376
Loss in iteration 102 : 0.3243234710825878
Loss in iteration 103 : 0.32431370478510596
Loss in iteration 104 : 0.3243041330410398
Loss in iteration 105 : 0.32429474822156773
Loss in iteration 106 : 0.324285543109818
Loss in iteration 107 : 0.32427651091641674
Loss in iteration 108 : 0.3242676452872202
Loss in iteration 109 : 0.324258940299672
Loss in iteration 110 : 0.3242503904467412
Loss in iteration 111 : 0.32424199060962794
Loss in iteration 112 : 0.32423373602185823
Loss in iteration 113 : 0.32422562222816337
Loss in iteration 114 : 0.3242176450416464
Loss in iteration 115 : 0.32420980050240894
Loss in iteration 116 : 0.32420208484015284
Loss in iteration 117 : 0.3241944944423109
Loss in iteration 118 : 0.32418702582862663
Loss in iteration 119 : 0.324179675632125
Loss in iteration 120 : 0.3241724405860001
Loss in iteration 121 : 0.32416531751546485
Loss in iteration 122 : 0.32415830333345746
Loss in iteration 123 : 0.32415139503902995
Loss in iteration 124 : 0.32414458971742394
Loss in iteration 125 : 0.3241378845409098
Loss in iteration 126 : 0.3241312767697919
Loss in iteration 127 : 0.32412476375310584
Loss in iteration 128 : 0.3241183429288304
Loss in iteration 129 : 0.32411201182348837
Loss in iteration 130 : 0.3241057680511608
Loss in iteration 131 : 0.3240996093120258
Loss in iteration 132 : 0.324093533390492
Loss in iteration 133 : 0.32408753815302777
Loss in iteration 134 : 0.32408162154579245
Loss in iteration 135 : 0.3240757815920742
Loss in iteration 136 : 0.3240700163896096
Loss in iteration 137 : 0.3240643241077606
Loss in iteration 138 : 0.3240587029846051
Loss in iteration 139 : 0.32405315132391443
Loss in iteration 140 : 0.32404766749205155
Loss in iteration 141 : 0.32404224991483566
Loss in iteration 142 : 0.3240368970743979
Loss in iteration 143 : 0.32403160750607357
Loss in iteration 144 : 0.3240263797954187
Loss in iteration 145 : 0.3240212125753287
Loss in iteration 146 : 0.3240161045234016
Loss in iteration 147 : 0.324011054359463
Loss in iteration 148 : 0.3240060608433502
Loss in iteration 149 : 0.3240011227729301
Loss in iteration 150 : 0.3239962389823253
Loss in iteration 151 : 0.3239914083403652
Loss in iteration 152 : 0.3239866297491821
Loss in iteration 153 : 0.32398190214298367
Loss in iteration 154 : 0.3239772244869037
Loss in iteration 155 : 0.3239725957759824
Loss in iteration 156 : 0.32396801503415207
Loss in iteration 157 : 0.32396348131331026
Loss in iteration 158 : 0.32395899369240894
Loss in iteration 159 : 0.32395455127654793
Loss in iteration 160 : 0.323950153196109
Loss in iteration 161 : 0.323945798605897
Loss in iteration 162 : 0.3239414866842908
Loss in iteration 163 : 0.32393721663244407
Loss in iteration 164 : 0.3239329876734771
Loss in iteration 165 : 0.323928799051717
Loss in iteration 166 : 0.3239246500319568
Loss in iteration 167 : 0.3239205398987534
Loss in iteration 168 : 0.32391646795574286
Loss in iteration 169 : 0.3239124335249838
Loss in iteration 170 : 0.3239084359463442
Loss in iteration 171 : 0.3239044745768867
Loss in iteration 172 : 0.3239005487903015
Loss in iteration 173 : 0.3238966579763431
Loss in iteration 174 : 0.32389280154029587
Loss in iteration 175 : 0.32388897890246676
Loss in iteration 176 : 0.3238851894976824
Loss in iteration 177 : 0.3238814327748102
Loss in iteration 178 : 0.3238777081963108
Loss in iteration 179 : 0.32387401523778764
Loss in iteration 180 : 0.32387035338757225
Loss in iteration 181 : 0.3238667221463141
Loss in iteration 182 : 0.32386312102659337
Loss in iteration 183 : 0.32385954955254215
Loss in iteration 184 : 0.32385600725949837
Loss in iteration 185 : 0.32385249369365093
Loss in iteration 186 : 0.3238490084117087
Loss in iteration 187 : 0.32384555098059015
Loss in iteration 188 : 0.3238421209770993
Loss in iteration 189 : 0.32383871798765473
Loss in iteration 190 : 0.32383534160797955
Loss in iteration 191 : 0.3238319914428333
Loss in iteration 192 : 0.32382866710576336
Loss in iteration 193 : 0.32382536821881114
Loss in iteration 194 : 0.3238220944122991
Loss in iteration 195 : 0.3238188453245722
Loss in iteration 196 : 0.3238156206017742
Loss in iteration 197 : 0.32381241989761905
Loss in iteration 198 : 0.32380924287318014
Loss in iteration 199 : 0.32380608919667686
Loss in iteration 200 : 0.32380295854327973
Testing accuracy  of updater 2 on alg 0 with rate 0.7 = 0.8506234260794792, training accuracy 0.8484950859950859, time elapsed: 3316 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5236076265314752
Loss in iteration 3 : 0.5052769058651898
Loss in iteration 4 : 0.46818549701476314
Loss in iteration 5 : 0.4236352489244953
Loss in iteration 6 : 0.395837683592471
Loss in iteration 7 : 0.38393655987359127
Loss in iteration 8 : 0.37651402567877146
Loss in iteration 9 : 0.369995857324992
Loss in iteration 10 : 0.3651129592110584
Loss in iteration 11 : 0.3620455517790311
Loss in iteration 12 : 0.3600670674780973
Loss in iteration 13 : 0.3584310758418553
Loss in iteration 14 : 0.3567701804639788
Loss in iteration 15 : 0.3550077830781961
Loss in iteration 16 : 0.353164085256571
Loss in iteration 17 : 0.3512557066748221
Loss in iteration 18 : 0.3492899363650024
Loss in iteration 19 : 0.3472867251145315
Loss in iteration 20 : 0.3452857209900263
Loss in iteration 21 : 0.3433362250857198
Loss in iteration 22 : 0.34148443877911444
Loss in iteration 23 : 0.3397670258179007
Loss in iteration 24 : 0.338210436480401
Loss in iteration 25 : 0.33683187438517387
Loss in iteration 26 : 0.3356395723141695
Loss in iteration 27 : 0.33463262698521384
Loss in iteration 28 : 0.3338014442478829
Loss in iteration 29 : 0.3331291935415822
Loss in iteration 30 : 0.3325939134123237
Loss in iteration 31 : 0.33217074393496604
Loss in iteration 32 : 0.33183399616637366
Loss in iteration 33 : 0.3315589760073269
Loss in iteration 34 : 0.33132350736198085
Loss in iteration 35 : 0.33110905732616996
Loss in iteration 36 : 0.3309013785115886
Loss in iteration 37 : 0.3306906596325321
Loss in iteration 38 : 0.33047125626789003
Loss in iteration 39 : 0.3302411175007465
Loss in iteration 40 : 0.3300010310431127
Loss in iteration 41 : 0.32975379742881084
Loss in iteration 42 : 0.32950342505386987
Loss in iteration 43 : 0.32925441541723843
Loss in iteration 44 : 0.32901118294838444
Loss in iteration 45 : 0.32877762922456844
Loss in iteration 46 : 0.32855687073899736
Loss in iteration 47 : 0.32835110470907564
Loss in iteration 48 : 0.32816158876900875
Loss in iteration 49 : 0.32798870672794955
Loss in iteration 50 : 0.32783209261221125
Loss in iteration 51 : 0.3276907877751815
Loss in iteration 52 : 0.3275634099276004
Loss in iteration 53 : 0.3274483176535395
Loss in iteration 54 : 0.32734375868472027
Loss in iteration 55 : 0.3272479944931561
Loss in iteration 56 : 0.32715939741553074
Loss in iteration 57 : 0.327076519466197
Loss in iteration 58 : 0.3269981342335246
Loss in iteration 59 : 0.3269232548362614
Loss in iteration 60 : 0.32685113191677456
Loss in iteration 61 : 0.32678123615282206
Loss in iteration 62 : 0.3267132298680921
Loss in iteration 63 : 0.32664693209962486
Loss in iteration 64 : 0.3265822810185637
Loss in iteration 65 : 0.326519296973612
Loss in iteration 66 : 0.3264580487031662
Loss in iteration 67 : 0.32639862450500656
Loss in iteration 68 : 0.3263411094163639
Loss in iteration 69 : 0.32628556878816256
Loss in iteration 70 : 0.3262320380698878
Loss in iteration 71 : 0.3261805181791916
Loss in iteration 72 : 0.3261309755235888
Loss in iteration 73 : 0.32608334556960716
Loss in iteration 74 : 0.32603753880600045
Loss in iteration 75 : 0.3259934480033801
Loss in iteration 76 : 0.325950955807531
Loss in iteration 77 : 0.3259099418915196
Loss in iteration 78 : 0.32587028910543536
Loss in iteration 79 : 0.3258318882785924
Loss in iteration 80 : 0.32579464152801507
Loss in iteration 81 : 0.3257584640945973
Loss in iteration 82 : 0.3257232848568916
Loss in iteration 83 : 0.3256890457582825
Loss in iteration 84 : 0.3256557004287064
Loss in iteration 85 : 0.3256232122913548
Loss in iteration 86 : 0.32559155242586657
Loss in iteration 87 : 0.3255606974200456
Loss in iteration 88 : 0.32553062739073163
Loss in iteration 89 : 0.325501324298568
Loss in iteration 90 : 0.3254727706270851
Loss in iteration 91 : 0.3254449484485079
Loss in iteration 92 : 0.32541783885979136
Loss in iteration 93 : 0.3253914217440893
Loss in iteration 94 : 0.32536567579514586
Loss in iteration 95 : 0.3253405787341434
Loss in iteration 96 : 0.3253161076487513
Loss in iteration 97 : 0.3252922393903668
Loss in iteration 98 : 0.32526895097614755
Loss in iteration 99 : 0.3252462199550402
Loss in iteration 100 : 0.32522402471031914
Loss in iteration 101 : 0.32520234468355913
Loss in iteration 102 : 0.3251811605157927
Loss in iteration 103 : 0.32516045411014444
Loss in iteration 104 : 0.32514020862625853
Loss in iteration 105 : 0.32512040842058576
Loss in iteration 106 : 0.3251010389480488
Loss in iteration 107 : 0.3250820866404481
Loss in iteration 108 : 0.32506353877538335
Loss in iteration 109 : 0.32504538334707755
Loss in iteration 110 : 0.32502760894767607
Loss in iteration 111 : 0.3250102046647058
Loss in iteration 112 : 0.32499315999765727
Loss in iteration 113 : 0.3249764647943579
Loss in iteration 114 : 0.3249601092060373
Loss in iteration 115 : 0.32494408365864813
Loss in iteration 116 : 0.32492837883735826
Loss in iteration 117 : 0.32491298568077887
Loss in iteration 118 : 0.32489789538159125
Loss in iteration 119 : 0.324883099390595
Loss in iteration 120 : 0.3248685894217446
Loss in iteration 121 : 0.32485435745630575
Loss in iteration 122 : 0.3248403957449288
Loss in iteration 123 : 0.32482669680694587
Loss in iteration 124 : 0.32481325342672424
Loss in iteration 125 : 0.3248000586472519
Loss in iteration 126 : 0.3247871057614003
Loss in iteration 127 : 0.32477438830142435
Loss in iteration 128 : 0.32476190002737815
Loss in iteration 129 : 0.32474963491502495
Loss in iteration 130 : 0.32473758714381074
Loss in iteration 131 : 0.3247257510853441
Loss in iteration 132 : 0.32471412129267074
Loss in iteration 133 : 0.3247026924905714
Loss in iteration 134 : 0.32469145956694007
Loss in iteration 135 : 0.32468041756525945
Loss in iteration 136 : 0.3246695616780789
Loss in iteration 137 : 0.3246588872413671
Loss in iteration 138 : 0.3246483897296058
Loss in iteration 139 : 0.3246380647514377
Loss in iteration 140 : 0.32462790804574587
Loss in iteration 141 : 0.3246179154780041
Loss in iteration 142 : 0.3246080830367869
Loss in iteration 143 : 0.32459840683038566
Loss in iteration 144 : 0.3245888830834132
Loss in iteration 145 : 0.3245795081334223
Loss in iteration 146 : 0.3245702784274741
Loss in iteration 147 : 0.32456119051869914
Loss in iteration 148 : 0.3245522410628245
Loss in iteration 149 : 0.3245434268147173
Loss in iteration 150 : 0.3245347446249747
Loss in iteration 151 : 0.3245261914365343
Loss in iteration 152 : 0.324517764281394
Loss in iteration 153 : 0.3245094602774197
Loss in iteration 154 : 0.3245012766252736
Loss in iteration 155 : 0.32449321060544084
Loss in iteration 156 : 0.3244852595754295
Loss in iteration 157 : 0.32447742096704957
Loss in iteration 158 : 0.32446969228385925
Loss in iteration 159 : 0.3244620710987248
Loss in iteration 160 : 0.324454555051485
Loss in iteration 161 : 0.3244471418467346
Loss in iteration 162 : 0.324439829251713
Loss in iteration 163 : 0.3244326150942765
Loss in iteration 164 : 0.32442549726095293
Loss in iteration 165 : 0.32441847369509863
Loss in iteration 166 : 0.3244115423950828
Loss in iteration 167 : 0.32440470141258076
Loss in iteration 168 : 0.32439794885091416
Loss in iteration 169 : 0.3243912828634359
Loss in iteration 170 : 0.32438470165200006
Loss in iteration 171 : 0.3243782034654482
Loss in iteration 172 : 0.32437178659818694
Loss in iteration 173 : 0.3243654493887877
Loss in iteration 174 : 0.32435919021864246
Loss in iteration 175 : 0.3243530075106711
Loss in iteration 176 : 0.3243468997280739
Loss in iteration 177 : 0.3243408653731168
Loss in iteration 178 : 0.3243349029859828
Loss in iteration 179 : 0.32432901114363716
Loss in iteration 180 : 0.3243231884587524
Loss in iteration 181 : 0.32431743357866083
Loss in iteration 182 : 0.32431174518434613
Loss in iteration 183 : 0.3243061219894712
Loss in iteration 184 : 0.324300562739433
Loss in iteration 185 : 0.3242950662104533
Loss in iteration 186 : 0.3242896312086958
Loss in iteration 187 : 0.3242842565694198
Loss in iteration 188 : 0.32427894115614525
Loss in iteration 189 : 0.3242736838598638
Loss in iteration 190 : 0.32426848359824345
Loss in iteration 191 : 0.324263339314907
Loss in iteration 192 : 0.3242582499786841
Loss in iteration 193 : 0.3242532145829048
Loss in iteration 194 : 0.3242482321447333
Loss in iteration 195 : 0.3242433017044886
Loss in iteration 196 : 0.32423842232501526
Testing accuracy  of updater 2 on alg 0 with rate 0.4 = 0.8508076899453351, training accuracy 0.8482186732186732, time elapsed: 3724 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6185220919983576
Loss in iteration 3 : 0.5592156130991556
Loss in iteration 4 : 0.524546646478602
Loss in iteration 5 : 0.5077593135930132
Loss in iteration 6 : 0.49864672171222846
Loss in iteration 7 : 0.4898789414263737
Loss in iteration 8 : 0.4780242002895382
Loss in iteration 9 : 0.4627089135925387
Loss in iteration 10 : 0.44546397704297647
Loss in iteration 11 : 0.42860126566722945
Loss in iteration 12 : 0.4141975089898949
Loss in iteration 13 : 0.4033643806198135
Loss in iteration 14 : 0.3960310509669396
Loss in iteration 15 : 0.3912643531942372
Loss in iteration 16 : 0.3878597775725799
Loss in iteration 17 : 0.3848486715349713
Loss in iteration 18 : 0.38172636090887396
Loss in iteration 19 : 0.37841674222304456
Loss in iteration 20 : 0.37509807448030646
Loss in iteration 21 : 0.3720145109328198
Loss in iteration 22 : 0.36934638042061957
Loss in iteration 23 : 0.367158068310144
Loss in iteration 24 : 0.3654073419124766
Loss in iteration 25 : 0.36398691019466445
Loss in iteration 26 : 0.3627716255973805
Loss in iteration 27 : 0.36165456848266214
Loss in iteration 28 : 0.36056567636357717
Loss in iteration 29 : 0.35947416281655864
Loss in iteration 30 : 0.3583799336290035
Loss in iteration 31 : 0.35730018165534744
Loss in iteration 32 : 0.35625638014558664
Loss in iteration 33 : 0.35526498309764704
Loss in iteration 34 : 0.35433305513810115
Loss in iteration 35 : 0.3534583360217846
Loss in iteration 36 : 0.3526322015759221
Loss in iteration 37 : 0.35184368047124903
Loss in iteration 38 : 0.35108298657966036
Loss in iteration 39 : 0.35034366691606716
Loss in iteration 40 : 0.3496231604108894
Loss in iteration 41 : 0.3489220952220677
Loss in iteration 42 : 0.34824291777188066
Loss in iteration 43 : 0.3475884526278626
Loss in iteration 44 : 0.3469608211017871
Loss in iteration 45 : 0.3463609063066275
Loss in iteration 46 : 0.34578833630421557
Loss in iteration 47 : 0.3452418196757265
Loss in iteration 48 : 0.34471962271205914
Loss in iteration 49 : 0.34422000673541114
Loss in iteration 50 : 0.3437415148945263
Loss in iteration 51 : 0.3432830765709677
Loss in iteration 52 : 0.34284395968923154
Loss in iteration 53 : 0.34242363482495103
Loss in iteration 54 : 0.3420216198562806
Loss in iteration 55 : 0.34163735777358917
Loss in iteration 56 : 0.3412701542570296
Loss in iteration 57 : 0.34091917613990746
Loss in iteration 58 : 0.34058349394677073
Loss in iteration 59 : 0.34026214409713945
Loss in iteration 60 : 0.3399541880971228
Loss in iteration 61 : 0.3396587537433353
Loss in iteration 62 : 0.3393750529881245
Loss in iteration 63 : 0.33910237929808007
Loss in iteration 64 : 0.33884009216701355
Loss in iteration 65 : 0.3385875975708652
Loss in iteration 66 : 0.3383443314090707
Loss in iteration 67 : 0.3381097497696633
Loss in iteration 68 : 0.33788332657101966
Loss in iteration 69 : 0.33766455674306556
Loss in iteration 70 : 0.33745296201980607
Loss in iteration 71 : 0.33724809652464277
Loss in iteration 72 : 0.33704955022694083
Loss in iteration 73 : 0.33685694952473044
Loss in iteration 74 : 0.3366699552371354
Loss in iteration 75 : 0.3364882589225218
Loss in iteration 76 : 0.3363115786176233
Loss in iteration 77 : 0.3361396549105217
Loss in iteration 78 : 0.3359722478814815
Loss in iteration 79 : 0.33580913503995985
Loss in iteration 80 : 0.3356501100768354
Loss in iteration 81 : 0.3354949820949609
Loss in iteration 82 : 0.33534357497360295
Loss in iteration 83 : 0.33519572661738206
Loss in iteration 84 : 0.3350512879777425
Loss in iteration 85 : 0.3349101218613284
Loss in iteration 86 : 0.3347721016218356
Loss in iteration 87 : 0.3346371098602175
Loss in iteration 88 : 0.3345050372411224
Loss in iteration 89 : 0.3343757814901999
Loss in iteration 90 : 0.3342492465878413
Loss in iteration 91 : 0.3341253421356768
Loss in iteration 92 : 0.3340039828506776
Loss in iteration 93 : 0.33388508813836354
Loss in iteration 94 : 0.33376858170659984
Loss in iteration 95 : 0.333654391197694
Loss in iteration 96 : 0.33354244783241277
Loss in iteration 97 : 0.3334326860708425
Loss in iteration 98 : 0.3333250432999967
Loss in iteration 99 : 0.33321945955745463
Loss in iteration 100 : 0.3331158772961052
Loss in iteration 101 : 0.33301424118967116
Loss in iteration 102 : 0.3329144979741753
Loss in iteration 103 : 0.332816596317867
Loss in iteration 104 : 0.33272048671159604
Loss in iteration 105 : 0.33262612137288095
Loss in iteration 106 : 0.3325334541589531
Loss in iteration 107 : 0.33244244048631433
Loss in iteration 108 : 0.3323530372559507
Loss in iteration 109 : 0.3322652027844284
Loss in iteration 110 : 0.33217889674128154
Loss in iteration 111 : 0.33209408009277563
Loss in iteration 112 : 0.33201071505175705
Loss in iteration 113 : 0.3319287650327458
Loss in iteration 114 : 0.3318481946110933
Loss in iteration 115 : 0.3317689694851596
Loss in iteration 116 : 0.3316910564403869
Loss in iteration 117 : 0.3316144233145998
Loss in iteration 118 : 0.3315390389641179
Loss in iteration 119 : 0.3314648732305019
Loss in iteration 120 : 0.33139189690791443
Loss in iteration 121 : 0.3313200817112389
Loss in iteration 122 : 0.33124940024498467
Loss in iteration 123 : 0.3311798259730143
Loss in iteration 124 : 0.3311113331891302
Loss in iteration 125 : 0.3310438969884132
Loss in iteration 126 : 0.3309774932392574
Loss in iteration 127 : 0.3309120985560925
Loss in iteration 128 : 0.3308476902726983
Loss in iteration 129 : 0.3307842464161555
Loss in iteration 130 : 0.33072174568142576
Loss in iteration 131 : 0.3306601674066672
Loss in iteration 132 : 0.33059949154923524
Loss in iteration 133 : 0.33053969866249283
Loss in iteration 134 : 0.33048076987343306
Loss in iteration 135 : 0.3304226868610945
Loss in iteration 136 : 0.33036543183583333
Loss in iteration 137 : 0.3303089875193913
Loss in iteration 138 : 0.33025333712576505
Loss in iteration 139 : 0.33019846434287636
Loss in iteration 140 : 0.33014435331499237
Loss in iteration 141 : 0.33009098862591113
Loss in iteration 142 : 0.33003835528286696
Loss in iteration 143 : 0.32998643870114913
Loss in iteration 144 : 0.3299352246894309
Loss in iteration 145 : 0.32988469943572113
Loss in iteration 146 : 0.32983484949401504
Loss in iteration 147 : 0.3297856617714981
Loss in iteration 148 : 0.3297371235163737
Loss in iteration 149 : 0.32968922230621944
Loss in iteration 150 : 0.32964194603690533
Loss in iteration 151 : 0.32959528291195545
Loss in iteration 152 : 0.32954922143243
Loss in iteration 153 : 0.32950375038722096
Loss in iteration 154 : 0.3294588588437944
Loss in iteration 155 : 0.32941453613929167
Loss in iteration 156 : 0.3293707718720572
Loss in iteration 157 : 0.32932755589346735
Loss in iteration 158 : 0.32928487830013864
Loss in iteration 159 : 0.3292427294264154
Loss in iteration 160 : 0.32920109983717877
Loss in iteration 161 : 0.3291599803209277
Loss in iteration 162 : 0.3291193618831074
Loss in iteration 163 : 0.3290792357397326
Loss in iteration 164 : 0.3290395933111883
Loss in iteration 165 : 0.32900042621631426
Loss in iteration 166 : 0.32896172626665526
Loss in iteration 167 : 0.328923485460944
Loss in iteration 168 : 0.32888569597976575
Loss in iteration 169 : 0.32884835018040404
Loss in iteration 170 : 0.3288114405918678
Loss in iteration 171 : 0.3287749599100932
Loss in iteration 172 : 0.32873890099327624
Loss in iteration 173 : 0.3287032568574083
Loss in iteration 174 : 0.32866802067190404
Loss in iteration 175 : 0.3286331857554147
Loss in iteration 176 : 0.3285987455717578
Loss in iteration 177 : 0.32856469372597913
Loss in iteration 178 : 0.3285310239605425
Loss in iteration 179 : 0.328497730151644
Loss in iteration 180 : 0.32846480630563263
Loss in iteration 181 : 0.32843224655556724
Loss in iteration 182 : 0.32840004515784427
Loss in iteration 183 : 0.32836819648897353
Loss in iteration 184 : 0.3283366950424209
Loss in iteration 185 : 0.328305535425573
Loss in iteration 186 : 0.3282747123567752
Loss in iteration 187 : 0.3282442206624801
Loss in iteration 188 : 0.32821405527446956
Loss in iteration 189 : 0.3281842112271662
Loss in iteration 190 : 0.3281546836550426
Loss in iteration 191 : 0.32812546779007007
Loss in iteration 192 : 0.3280965589592917
Loss in iteration 193 : 0.328067952582434
Loss in iteration 194 : 0.3280396441696073
Loss in iteration 195 : 0.32801162931906425
Loss in iteration 196 : 0.32798390371504416
Loss in iteration 197 : 0.3279564631256474
Loss in iteration 198 : 0.32792930340081594
Loss in iteration 199 : 0.32790242047032836
Loss in iteration 200 : 0.3278758103418943
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.8519746944290891, training accuracy 0.8463452088452088, time elapsed: 3355 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 30.621201081645385
Loss in iteration 3 : 6.017710250263125
Loss in iteration 4 : 10.383262877188017
Loss in iteration 5 : 10.795089090590423
Loss in iteration 6 : 2.783209025947943
Loss in iteration 7 : 2.397427933185929
Loss in iteration 8 : 2.1292715933724176
Loss in iteration 9 : 2.005691457437177
Loss in iteration 10 : 2.3607210338027738
Loss in iteration 11 : 2.8786945500024985
Loss in iteration 12 : 4.439245846588998
Loss in iteration 13 : 2.9850932606247245
Loss in iteration 14 : 3.8699334424813814
Loss in iteration 15 : 2.4525998493801473
Loss in iteration 16 : 2.8523534183287027
Loss in iteration 17 : 2.265799295909092
Loss in iteration 18 : 2.788669109527586
Loss in iteration 19 : 2.292255479303191
Loss in iteration 20 : 2.970905641159883
Loss in iteration 21 : 2.2894093609454136
Loss in iteration 22 : 2.9004142385502134
Loss in iteration 23 : 2.157710621042467
Loss in iteration 24 : 2.634904810959594
Loss in iteration 25 : 2.038068051463638
Loss in iteration 26 : 2.5138730257741306
Loss in iteration 27 : 2.006887933938567
Loss in iteration 28 : 2.549247542878578
Loss in iteration 29 : 1.9999783350182063
Loss in iteration 30 : 2.53817761734194
Loss in iteration 31 : 1.934473831981241
Loss in iteration 32 : 2.401380349161876
Loss in iteration 33 : 1.8530296664239547
Loss in iteration 34 : 2.2981597799912405
Loss in iteration 35 : 1.8136683389940365
Loss in iteration 36 : 2.289892548014699
Loss in iteration 37 : 1.7999715281462487
Loss in iteration 38 : 2.2850857815385215
Loss in iteration 39 : 1.7609783659296272
Loss in iteration 40 : 2.2089132990638816
Loss in iteration 41 : 1.7054247221699375
Loss in iteration 42 : 2.1311209651686536
Loss in iteration 43 : 1.668833015882941
Loss in iteration 44 : 2.1064293397861955
Loss in iteration 45 : 1.650490062850581
Loss in iteration 46 : 2.0958948372293777
Loss in iteration 47 : 1.6228409276775702
Loss in iteration 48 : 2.049698912409664
Loss in iteration 49 : 1.5841216182590439
Loss in iteration 50 : 1.9942123827236147
Loss in iteration 51 : 1.5532929580642305
Loss in iteration 52 : 1.9656765440988861
Loss in iteration 53 : 1.5334237122601477
Loss in iteration 54 : 1.9496033795082635
Loss in iteration 55 : 1.5109994823375126
Loss in iteration 56 : 1.9176686381411754
Loss in iteration 57 : 1.4827217729439448
Loss in iteration 58 : 1.878254443794553
Loss in iteration 59 : 1.4575815243873391
Loss in iteration 60 : 1.8513632755556628
Loss in iteration 61 : 1.4383535947877433
Loss in iteration 62 : 1.8325865850357994
Loss in iteration 63 : 1.4189752969071943
Loss in iteration 64 : 1.8076276476911575
Loss in iteration 65 : 1.3970871968064036
Loss in iteration 66 : 1.7785177061573225
Loss in iteration 67 : 1.3765332366363947
Loss in iteration 68 : 1.7548954574979096
Loss in iteration 69 : 1.3589868449703766
Loss in iteration 70 : 1.7358168163668795
Loss in iteration 71 : 1.3419414998974055
Loss in iteration 72 : 1.7147699539177512
Loss in iteration 73 : 1.3241106588515705
Loss in iteration 74 : 1.6919581413846203
Loss in iteration 75 : 1.3069910042578547
Loss in iteration 76 : 1.671544540201839
Loss in iteration 77 : 1.2914031968809991
Loss in iteration 78 : 1.6534983465003048
Loss in iteration 79 : 1.2763813249811489
Loss in iteration 80 : 1.6351073376632508
Loss in iteration 81 : 1.2613201586431562
Loss in iteration 82 : 1.6162179790814222
Loss in iteration 83 : 1.2467511435913676
Loss in iteration 84 : 1.5984671799011634
Loss in iteration 85 : 1.233020933929631
Loss in iteration 86 : 1.581955778987743
Loss in iteration 87 : 1.2197760462326062
Loss in iteration 88 : 1.565635775774687
Loss in iteration 89 : 1.2067560528682666
Loss in iteration 90 : 1.5493659465495533
Loss in iteration 91 : 1.1941298194955898
Loss in iteration 92 : 1.5337358354691712
Loss in iteration 93 : 1.1820246323204384
Loss in iteration 94 : 1.518811598014127
Loss in iteration 95 : 1.1703132803146767
Loss in iteration 96 : 1.5042105465633322
Loss in iteration 97 : 1.1588879593467372
Loss in iteration 98 : 1.4898501867873732
Loss in iteration 99 : 1.1477926525216877
Loss in iteration 100 : 1.4759233328142825
Loss in iteration 101 : 1.137065661235886
Loss in iteration 102 : 1.4624564550348063
Loss in iteration 103 : 1.1266587947446864
Loss in iteration 104 : 1.449313430175126
Loss in iteration 105 : 1.1165262540714793
Loss in iteration 106 : 1.4364513601482094
Loss in iteration 107 : 1.1066730646224698
Loss in iteration 108 : 1.423924609177639
Loss in iteration 109 : 1.097105429430462
Loss in iteration 110 : 1.4117383506173606
Loss in iteration 111 : 1.0878020025509847
Loss in iteration 112 : 1.3998425744219012
Loss in iteration 113 : 1.07874108849926
Loss in iteration 114 : 1.3882145738538572
Loss in iteration 115 : 1.0699173428263569
Loss in iteration 116 : 1.3768643111284762
Loss in iteration 117 : 1.061326967506796
Loss in iteration 118 : 1.3657883954807033
Loss in iteration 119 : 1.0529580197242971
Loss in iteration 120 : 1.3549658343359365
Loss in iteration 121 : 1.0447982679262162
Loss in iteration 122 : 1.3443832779947014
Loss in iteration 123 : 1.036840667024364
Loss in iteration 124 : 1.334037983268308
Loss in iteration 125 : 1.0290791724138733
Loss in iteration 126 : 1.3239240402672832
Loss in iteration 127 : 1.021505605966987
Loss in iteration 128 : 1.3140303216844489
Loss in iteration 129 : 1.014111790134393
Loss in iteration 130 : 1.3043478389411105
Loss in iteration 131 : 1.0068912608773566
Loss in iteration 132 : 1.294871030230182
Loss in iteration 133 : 0.9998381412536556
Loss in iteration 134 : 1.2855938186848523
Loss in iteration 135 : 0.99294616026189
Loss in iteration 136 : 1.2765087681896916
Loss in iteration 137 : 0.9862091903988809
Loss in iteration 138 : 1.2676091242306866
Loss in iteration 139 : 0.9796217570692726
Loss in iteration 140 : 1.258889303357163
Loss in iteration 141 : 0.9731787465832558
Loss in iteration 142 : 1.2503438091080321
Loss in iteration 143 : 0.9668751042725177
Loss in iteration 144 : 1.2419669140564724
Loss in iteration 145 : 0.9607059539955545
Loss in iteration 146 : 1.2337531953112566
Loss in iteration 147 : 0.9546667390471971
Loss in iteration 148 : 1.2256977015062358
Loss in iteration 149 : 0.9487531458691892
Loss in iteration 150 : 1.2177956631894968
Loss in iteration 151 : 0.9429610098014733
Loss in iteration 152 : 1.2100423782805851
Loss in iteration 153 : 0.9372863335877647
Loss in iteration 154 : 1.2024333414974595
Loss in iteration 155 : 0.9317253207026249
Loss in iteration 156 : 1.1949642943575123
Loss in iteration 157 : 0.9262743523893395
Loss in iteration 158 : 1.1876311474643206
Loss in iteration 159 : 0.9209299556748028
Loss in iteration 160 : 1.1804299388095092
Loss in iteration 161 : 0.9156888005996577
Loss in iteration 162 : 1.17335685958073
Loss in iteration 163 : 0.9105477041634059
Loss in iteration 164 : 1.1664082653832264
Loss in iteration 165 : 0.9055036205122228
Loss in iteration 166 : 1.1595806531673638
Loss in iteration 167 : 0.9005536277828873
Loss in iteration 168 : 1.1528706444803782
Loss in iteration 169 : 0.8956949223831474
Loss in iteration 170 : 1.1462749868748137
Loss in iteration 171 : 0.8909248159394847
Loss in iteration 172 : 1.1397905535622308
Loss in iteration 173 : 0.8862407292185447
Loss in iteration 174 : 1.133414334340815
Loss in iteration 175 : 0.8816401850834152
Loss in iteration 176 : 1.1271434273799579
Loss in iteration 177 : 0.8771208034027386
Loss in iteration 178 : 1.1209750358238517
Loss in iteration 179 : 0.8726802970094801
Loss in iteration 180 : 1.1149064646288105
Loss in iteration 181 : 0.8683164671775231
Loss in iteration 182 : 1.108935115386799
Loss in iteration 183 : 0.864027198948823
Loss in iteration 184 : 1.1030584812582933
Loss in iteration 185 : 0.8598104570624444
Loss in iteration 186 : 1.0972741432314166
Loss in iteration 187 : 0.855664282341672
Loss in iteration 188 : 1.091579766713766
Loss in iteration 189 : 0.8515867881372566
Loss in iteration 190 : 1.0859730977793898
Loss in iteration 191 : 0.847576156851314
Loss in iteration 192 : 1.0804519595002826
Loss in iteration 193 : 0.8436306367211678
Loss in iteration 194 : 1.0750142487071987
Loss in iteration 195 : 0.8397485388447742
Loss in iteration 196 : 1.06965793297856
Loss in iteration 197 : 0.8359282343419736
Loss in iteration 198 : 1.0643810476621123
Loss in iteration 199 : 0.8321681516376919
Loss in iteration 200 : 1.0591816930015587
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.776057981696456, training accuracy 0.7792076167076167, time elapsed: 3531 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 2.025607837666207
Loss in iteration 3 : 0.48162319249294505
Loss in iteration 4 : 0.4385372649153145
Loss in iteration 5 : 0.4200410892143638
Loss in iteration 6 : 0.44911251630489574
Loss in iteration 7 : 0.5707312004072815
Loss in iteration 8 : 0.9844883996724572
Loss in iteration 9 : 0.7856229787837679
Loss in iteration 10 : 1.1117590191560902
Loss in iteration 11 : 0.5818851736000131
Loss in iteration 12 : 0.6234599422265927
Loss in iteration 13 : 0.5432301529276912
Loss in iteration 14 : 0.6259104986205138
Loss in iteration 15 : 0.581576297006167
Loss in iteration 16 : 0.7339849800336923
Loss in iteration 17 : 0.6183692924087337
Loss in iteration 18 : 0.7708331826310981
Loss in iteration 19 : 0.5976508366067841
Loss in iteration 20 : 0.7035118665796252
Loss in iteration 21 : 0.5757575254519309
Loss in iteration 22 : 0.6729543978217798
Loss in iteration 23 : 0.572588369831094
Loss in iteration 24 : 0.6812133075210838
Loss in iteration 25 : 0.5787356066262509
Loss in iteration 26 : 0.6953732361432913
Loss in iteration 27 : 0.5795557432129613
Loss in iteration 28 : 0.6921588773234462
Loss in iteration 29 : 0.5743762897854705
Loss in iteration 30 : 0.6809347617609196
Loss in iteration 31 : 0.5698039948023202
Loss in iteration 32 : 0.6751190209742598
Loss in iteration 33 : 0.5681661392158971
Loss in iteration 34 : 0.674806091161882
Loss in iteration 35 : 0.5674883984497574
Loss in iteration 36 : 0.6743639704505641
Loss in iteration 37 : 0.5658997687500429
Loss in iteration 38 : 0.6714034912053056
Loss in iteration 39 : 0.5635947087062478
Loss in iteration 40 : 0.667634850025912
Loss in iteration 41 : 0.5614810331492625
Loss in iteration 42 : 0.6647485499056094
Loss in iteration 43 : 0.5598084014391747
Loss in iteration 44 : 0.662614730562775
Loss in iteration 45 : 0.558262174028276
Loss in iteration 46 : 0.6604228317518981
Loss in iteration 47 : 0.556600646587105
Loss in iteration 48 : 0.6579051989802426
Loss in iteration 49 : 0.5548676160105609
Loss in iteration 50 : 0.6553202644976527
Loss in iteration 51 : 0.5531873651393834
Loss in iteration 52 : 0.6528896000560312
Loss in iteration 53 : 0.5515883433798964
Loss in iteration 54 : 0.6505849887203898
Loss in iteration 55 : 0.5500245001073981
Loss in iteration 56 : 0.648294218313752
Loss in iteration 57 : 0.5484627631101252
Loss in iteration 58 : 0.6459830439520546
Loss in iteration 59 : 0.5469092219904078
Loss in iteration 60 : 0.6436872592307291
Loss in iteration 61 : 0.5453804858823889
Loss in iteration 62 : 0.6414366623071799
Loss in iteration 63 : 0.5438799992367946
Loss in iteration 64 : 0.6392270948293575
Loss in iteration 65 : 0.5424009934163949
Loss in iteration 66 : 0.6370428040960283
Loss in iteration 67 : 0.5409383724894833
Loss in iteration 68 : 0.6348784065317661
Loss in iteration 69 : 0.5394924299586151
Loss in iteration 70 : 0.6327381927828012
Loss in iteration 71 : 0.5380650749887848
Loss in iteration 72 : 0.6306259604783104
Loss in iteration 73 : 0.5366565182902212
Loss in iteration 74 : 0.6285409341955014
Loss in iteration 75 : 0.5352655258206276
Loss in iteration 76 : 0.626480560125655
Loss in iteration 77 : 0.5338910217405403
Loss in iteration 78 : 0.6244435681758105
Loss in iteration 79 : 0.5325326791497275
Loss in iteration 80 : 0.6224300688027491
Loss in iteration 81 : 0.5311904559712344
Loss in iteration 82 : 0.6204402180730365
Loss in iteration 83 : 0.5298641173071523
Loss in iteration 84 : 0.6184735761327633
Loss in iteration 85 : 0.5285532277022001
Loss in iteration 86 : 0.6165294200146244
Loss in iteration 87 : 0.5272573623214457
Loss in iteration 88 : 0.6146071760649157
Loss in iteration 89 : 0.5259762059756213
Loss in iteration 90 : 0.6127064774522826
Loss in iteration 91 : 0.5247095020197079
Loss in iteration 92 : 0.6108269957685437
Loss in iteration 93 : 0.5234569827388046
Loss in iteration 94 : 0.6089683370156659
Loss in iteration 95 : 0.5222183591273472
Loss in iteration 96 : 0.6071300692118399
Loss in iteration 97 : 0.5209933469578895
Loss in iteration 98 : 0.6053117824956269
Loss in iteration 99 : 0.5197816831560957
Loss in iteration 100 : 0.6035131050947456
Loss in iteration 101 : 0.5185831212065583
Loss in iteration 102 : 0.6017336836194366
Loss in iteration 103 : 0.5173974209915002
Loss in iteration 104 : 0.5999731661987474
Loss in iteration 105 : 0.5162243455107182
Loss in iteration 106 : 0.5982312030667036
Loss in iteration 107 : 0.515063663501182
Loss in iteration 108 : 0.5965074542774147
Loss in iteration 109 : 0.5139151518433183
Loss in iteration 110 : 0.5948015929745615
Loss in iteration 111 : 0.5127785952145997
Loss in iteration 112 : 0.5931133032388003
Loss in iteration 113 : 0.5116537845096413
Loss in iteration 114 : 0.591442277264356
Loss in iteration 115 : 0.5105405159243132
Loss in iteration 116 : 0.5897882146548363
Loss in iteration 117 : 0.5094385909544835
Loss in iteration 118 : 0.5881508230795869
Loss in iteration 119 : 0.5083478165712814
Loss in iteration 120 : 0.5865298186526114
Loss in iteration 121 : 0.5072680050859153
Loss in iteration 122 : 0.58492492555648
Loss in iteration 123 : 0.5061989737913672
Loss in iteration 124 : 0.5833358754128349
Loss in iteration 125 : 0.5051405446451481
Loss in iteration 126 : 0.5817624068725525
Loss in iteration 127 : 0.5040925440873936
Loss in iteration 128 : 0.5802042654411621
Loss in iteration 129 : 0.5030548029214621
Loss in iteration 130 : 0.578661203333565
Loss in iteration 131 : 0.5020271561753548
Loss in iteration 132 : 0.5771329792450538
Loss in iteration 133 : 0.5010094429337477
Loss in iteration 134 : 0.5756193580732625
Loss in iteration 135 : 0.5000015061717226
Loss in iteration 136 : 0.5741201106609595
Loss in iteration 137 : 0.4990031926105477
Loss in iteration 138 : 0.5726350135806252
Loss in iteration 139 : 0.4980143525915392
Loss in iteration 140 : 0.5711638489401457
Loss in iteration 141 : 0.4970348399559795
Loss in iteration 142 : 0.569706404188834
Loss in iteration 143 : 0.4960645119260465
Loss in iteration 144 : 0.5682624719220019
Loss in iteration 145 : 0.49510322898906567
Loss in iteration 146 : 0.5668318496923321
Loss in iteration 147 : 0.4941508547882307
Loss in iteration 148 : 0.5654143398329635
Loss in iteration 149 : 0.4932072560201661
Loss in iteration 150 : 0.5640097492912158
Loss in iteration 151 : 0.4922723023376915
Loss in iteration 152 : 0.5626178894695268
Loss in iteration 153 : 0.49134586625650617
Loss in iteration 154 : 0.5612385760721968
Loss in iteration 155 : 0.49042782306550475
Loss in iteration 156 : 0.5598716289582447
Loss in iteration 157 : 0.48951805074092575
Loss in iteration 158 : 0.5585168720010584
Loss in iteration 159 : 0.48861642986447296
Loss in iteration 160 : 0.5571741329547173
Loss in iteration 161 : 0.4877228435449164
Loss in iteration 162 : 0.5558432433263747
Loss in iteration 163 : 0.48683717734305176
Loss in iteration 164 : 0.5545240382541697
Loss in iteration 165 : 0.4859593191995668
Loss in iteration 166 : 0.5532163563902973
Loss in iteration 167 : 0.4850891593658365
Loss in iteration 168 : 0.5519200397892561
Loss in iteration 169 : 0.4842265903374537
Loss in iteration 170 : 0.5506349338008834
Loss in iteration 171 : 0.4833715067904091
Loss in iteration 172 : 0.5493608869681026
Loss in iteration 173 : 0.4825238055197359
Loss in iteration 174 : 0.5480977509290124
Loss in iteration 175 : 0.48168338538049
Loss in iteration 176 : 0.5468453803230562
Loss in iteration 177 : 0.48085014723093644
Loss in iteration 178 : 0.5456036327012379
Loss in iteration 179 : 0.4800239938778873
Loss in iteration 180 : 0.5443723684398925
Loss in iteration 181 : 0.47920483002400055
Loss in iteration 182 : 0.5431514506582015
Loss in iteration 183 : 0.478392562217033
Loss in iteration 184 : 0.5419407451389556
Loss in iteration 185 : 0.4775870988008889
Loss in iteration 186 : 0.540740120252518
Loss in iteration 187 : 0.47678834986844526
Loss in iteration 188 : 0.5395494468839185
Loss in iteration 189 : 0.4759962272159809
Loss in iteration 190 : 0.538368598362723
Loss in iteration 191 : 0.475210644299229
Loss in iteration 192 : 0.5371974503956949
Loss in iteration 193 : 0.47443151619092455
Loss in iteration 194 : 0.5360358810020874
Loss in iteration 195 : 0.4736587595397864
Loss in iteration 196 : 0.5348837704513673
Loss in iteration 197 : 0.4728922925309056
Loss in iteration 198 : 0.5337410012033112
Loss in iteration 199 : 0.4721320348474265
Loss in iteration 200 : 0.5326074578503946
Testing accuracy  of updater 3 on alg 0 with rate 7.0 = 0.7957742153430379, training accuracy 0.7964987714987715, time elapsed: 3871 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.8911735966370973
Loss in iteration 3 : 0.3489067990957202
Loss in iteration 4 : 0.3411426052267588
Loss in iteration 5 : 0.3363020983421737
Loss in iteration 6 : 0.33310436624413453
Loss in iteration 7 : 0.3309130258444339
Loss in iteration 8 : 0.3293585183320104
Loss in iteration 9 : 0.3282274565637258
Loss in iteration 10 : 0.3273837906183244
Loss in iteration 11 : 0.3267412749958374
Loss in iteration 12 : 0.32624204690755976
Loss in iteration 13 : 0.32584752461084365
Loss in iteration 14 : 0.3255308002020774
Loss in iteration 15 : 0.3252731199499608
Loss in iteration 16 : 0.3250608758320839
Loss in iteration 17 : 0.32488417149423987
Loss in iteration 18 : 0.324735563013578
Loss in iteration 19 : 0.3246094375030792
Loss in iteration 20 : 0.3245014517831864
Loss in iteration 21 : 0.324408238441111
Loss in iteration 22 : 0.32432713687864223
Loss in iteration 23 : 0.3242560410273389
Loss in iteration 24 : 0.3241932609270479
Loss in iteration 25 : 0.3241374376955057
Loss in iteration 26 : 0.32408746765105706
Loss in iteration 27 : 0.32404245219135464
Loss in iteration 28 : 0.3240016540050894
Loss in iteration 29 : 0.3239644663979494
Loss in iteration 30 : 0.32393038698468424
Loss in iteration 31 : 0.32389899841276776
Loss in iteration 32 : 0.3238699520566514
Loss in iteration 33 : 0.32384295566239013
Loss in iteration 34 : 0.32381776299195336
Loss in iteration 35 : 0.3237941657780958
Loss in iteration 36 : 0.3237719870173653
Loss in iteration 37 : 0.32375107566081257
Loss in iteration 38 : 0.3237313021984988
Loss in iteration 39 : 0.3237125551133068
Loss in iteration 40 : 0.3236947379324169
Loss in iteration 41 : 0.3236777668319463
Loss in iteration 42 : 0.3236615686425508
Loss in iteration 43 : 0.32364607921374966
Loss in iteration 44 : 0.32363124204852334
Loss in iteration 45 : 0.32361700717430325
Loss in iteration 46 : 0.3236033301972133
Loss in iteration 47 : 0.3235901715142472
Loss in iteration 48 : 0.3235774956504978
Loss in iteration 49 : 0.3235652707032444
Loss in iteration 50 : 0.32355346787199
Loss in iteration 51 : 0.3235420610615861
Loss in iteration 52 : 0.3235310265449331
Loss in iteration 53 : 0.3235203426761819
Loss in iteration 54 : 0.3235099896455807
Loss in iteration 55 : 0.3234999492695789
Loss in iteration 56 : 0.3234902048103114
Loss in iteration 57 : 0.32348074081993455
Loss in iteration 58 : 0.32347154300589176
Loss in iteration 59 : 0.32346259811390105
Loss in iteration 60 : 0.32345389382596945
Loss in iteration 61 : 0.32344541867121673
Loss in iteration 62 : 0.3234371619475739
Loss in iteration 63 : 0.323429113652855
Loss in iteration 64 : 0.3234212644237984
Loss in iteration 65 : 0.3234136054820359
Loss in iteration 66 : 0.32340612858597817
Loss in iteration 67 : 0.3233988259878744
Loss in iteration 68 : 0.3233916903953122
Loss in iteration 69 : 0.32338471493666276
Loss in iteration 70 : 0.3233778931298824
Loss in iteration 71 : 0.32337121885432824
Loss in iteration 72 : 0.32336468632519577
Loss in iteration 73 : 0.323358290070262
Loss in iteration 74 : 0.3233520249087023
Loss in iteration 75 : 0.32334588593171404
Loss in iteration 76 : 0.3233398684847604
Loss in iteration 77 : 0.32333396815127174
Loss in iteration 78 : 0.3233281807376677
Loss in iteration 79 : 0.32332250225949694
Loss in iteration 80 : 0.323316928928682
Loss in iteration 81 : 0.32331145714167775
Loss in iteration 82 : 0.32330608346853335
Loss in iteration 83 : 0.3233008046426976
Loss in iteration 84 : 0.32329561755155783
Loss in iteration 85 : 0.3232905192276348
Loss in iteration 86 : 0.3232855068403749
Loss in iteration 87 : 0.3232805776884664
Loss in iteration 88 : 0.32327572919269454
Loss in iteration 89 : 0.3232709588892348
Loss in iteration 90 : 0.3232662644233749
Loss in iteration 91 : 0.3232616435436427
Loss in iteration 92 : 0.323257094096295
Loss in iteration 93 : 0.3232526140201343
Loss in iteration 94 : 0.32324820134164955
Loss in iteration 95 : 0.3232438541704605
Loss in iteration 96 : 0.32323957069499176
Loss in iteration 97 : 0.3232353491784578
Loss in iteration 98 : 0.32323118795503225
Loss in iteration 99 : 0.32322708542627626
Loss in iteration 100 : 0.32322304005772895
Loss in iteration 101 : 0.323219050375751
Loss in iteration 102 : 0.3232151149644801
Loss in iteration 103 : 0.32321123246300937
Loss in iteration 104 : 0.3232074015626983
Loss in iteration 105 : 0.3232036210046308
Loss in iteration 106 : 0.323199889577227
Loss in iteration 107 : 0.3231962061139673
Loss in iteration 108 : 0.3231925694912574
Loss in iteration 109 : 0.3231889786263994
Loss in iteration 110 : 0.32318543247566395
Loss in iteration 111 : 0.3231819300324864
Loss in iteration 112 : 0.3231784703257344
Loss in iteration 113 : 0.3231750524180925
Loss in iteration 114 : 0.32317167540449826
Loss in iteration 115 : 0.32316833841070036
Loss in iteration 116 : 0.32316504059185414
Loss in iteration 117 : 0.3231617811312165
Loss in iteration 118 : 0.3231585592388902
Loss in iteration 119 : 0.32315537415064743
Loss in iteration 120 : 0.323152225126804
Loss in iteration 121 : 0.323149111451147
Loss in iteration 122 : 0.32314603242992734
Loss in iteration 123 : 0.32314298739090686
Loss in iteration 124 : 0.3231399756824247
Loss in iteration 125 : 0.32313699667254014
Loss in iteration 126 : 0.323134049748222
Loss in iteration 127 : 0.3231311343145344
Loss in iteration 128 : 0.32312824979391536
Loss in iteration 129 : 0.3231253956254622
Loss in iteration 130 : 0.3231225712642527
Loss in iteration 131 : 0.32311977618070176
Loss in iteration 132 : 0.3231170098599629
Loss in iteration 133 : 0.3231142718013321
Loss in iteration 134 : 0.3231115615177069
Loss in iteration 135 : 0.32310887853504544
Loss in iteration 136 : 0.3231062223918811
Loss in iteration 137 : 0.32310359263882904
Loss in iteration 138 : 0.3231009888381386
Loss in iteration 139 : 0.32309841056326305
Loss in iteration 140 : 0.3230958573984266
Loss in iteration 141 : 0.3230933289382456
Loss in iteration 142 : 0.32309082478735035
Loss in iteration 143 : 0.32308834456001845
Loss in iteration 144 : 0.3230858878798339
Loss in iteration 145 : 0.32308345437935987
Loss in iteration 146 : 0.32308104369982765
Loss in iteration 147 : 0.323078655490825
Loss in iteration 148 : 0.32307628941003097
Loss in iteration 149 : 0.32307394512292875
Loss in iteration 150 : 0.323071622302545
Loss in iteration 151 : 0.32306932062919913
Loss in iteration 152 : 0.32306703979026835
Loss in iteration 153 : 0.3230647794799642
Loss in iteration 154 : 0.3230625393990987
Loss in iteration 155 : 0.32306031925488216
Loss in iteration 156 : 0.3230581187607331
Loss in iteration 157 : 0.3230559376360616
Loss in iteration 158 : 0.3230537756061103
Loss in iteration 159 : 0.3230516324017622
Loss in iteration 160 : 0.3230495077593718
Loss in iteration 161 : 0.3230474014206096
Loss in iteration 162 : 0.32304531313230206
Loss in iteration 163 : 0.32304324264628437
Loss in iteration 164 : 0.32304118971924756
Loss in iteration 165 : 0.3230391541126125
Loss in iteration 166 : 0.3230371355923952
Loss in iteration 167 : 0.32303513392906713
Loss in iteration 168 : 0.32303314889744633
Loss in iteration 169 : 0.3230311802765741
Loss in iteration 170 : 0.32302922784960736
Loss in iteration 171 : 0.32302729140369113
Testing accuracy  of updater 3 on alg 0 with rate 4.0 = 0.8501934770591487, training accuracy 0.8491400491400491, time elapsed: 2778 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.4468657609937045
Loss in iteration 3 : 0.42001526892978674
Loss in iteration 4 : 0.40178835377812017
Loss in iteration 5 : 0.38860074054721255
Loss in iteration 6 : 0.3786867278902342
Loss in iteration 7 : 0.3710136619781713
Loss in iteration 8 : 0.36493143607166023
Loss in iteration 9 : 0.36001248034972805
Loss in iteration 10 : 0.35596587441380745
Loss in iteration 11 : 0.3525878780764589
Loss in iteration 12 : 0.34973216876765334
Loss in iteration 13 : 0.3472913061143797
Loss in iteration 14 : 0.345184837522835
Loss in iteration 15 : 0.3433514589034079
Loss in iteration 16 : 0.341743722644807
Loss in iteration 17 : 0.3403243870937066
Loss in iteration 18 : 0.33906384869726114
Loss in iteration 19 : 0.3379383034565635
Loss in iteration 20 : 0.3369284092065643
Loss in iteration 21 : 0.33601829791385746
Loss in iteration 22 : 0.3351948365591812
Loss in iteration 23 : 0.3344470671888485
Loss in iteration 24 : 0.3337657778665482
Loss in iteration 25 : 0.33314317046418807
Loss in iteration 26 : 0.3325726009266039
Loss in iteration 27 : 0.3320483743598279
Loss in iteration 28 : 0.3315655820063979
Loss in iteration 29 : 0.3311199705221903
Loss in iteration 30 : 0.3307078363797661
Loss in iteration 31 : 0.33032593997629184
Loss in iteration 32 : 0.3299714353123153
Loss in iteration 33 : 0.3296418120633652
Loss in iteration 34 : 0.3293348475819261
Loss in iteration 35 : 0.32904856690756107
Loss in iteration 36 : 0.328781209274266
Loss in iteration 37 : 0.3285311999194992
Loss in iteration 38 : 0.32829712624298
Loss in iteration 39 : 0.32807771755284715
Loss in iteration 40 : 0.3278718277850688
Loss in iteration 41 : 0.3276784206988164
Loss in iteration 42 : 0.32749655714298803
Loss in iteration 43 : 0.32732538406284156
Loss in iteration 44 : 0.3271641249746053
Loss in iteration 45 : 0.32701207168353236
Loss in iteration 46 : 0.32686857705914835
Loss in iteration 47 : 0.32673304871272535
Loss in iteration 48 : 0.3266049434474574
Loss in iteration 49 : 0.32648376237261933
Loss in iteration 50 : 0.326369046590221
Loss in iteration 51 : 0.32626037337682284
Loss in iteration 52 : 0.3261573527949106
Loss in iteration 53 : 0.32605962467807037
Loss in iteration 54 : 0.3259668559423274
Loss in iteration 55 : 0.3258787381828715
Loss in iteration 56 : 0.3257949855211721
Loss in iteration 57 : 0.32571533267234354
Loss in iteration 58 : 0.3256395332066682
Loss in iteration 59 : 0.32556735798280634
Loss in iteration 60 : 0.32549859373300327
Loss in iteration 61 : 0.3254330417833664
Loss in iteration 62 : 0.3253705168942249
Loss in iteration 63 : 0.32531084620763046
Loss in iteration 64 : 0.32525386829053465
Loss in iteration 65 : 0.32519943226366105
Loss in iteration 66 : 0.32514739700716017
Loss in iteration 67 : 0.32509763043536194
Loss in iteration 68 : 0.32505000883358
Loss in iteration 69 : 0.32500441625099824
Loss in iteration 70 : 0.3249607439440958
Loss in iteration 71 : 0.32491888986583417
Loss in iteration 72 : 0.32487875819629264
Loss in iteration 73 : 0.32484025891088564
Loss in iteration 74 : 0.3248033073827409
Loss in iteration 75 : 0.32476782401613524
Loss in iteration 76 : 0.32473373390825544
Loss in iteration 77 : 0.3247009665367329
Loss in iteration 78 : 0.32466945547078213
Loss in iteration 79 : 0.3246391381038775
Loss in iteration 80 : 0.3246099554061585
Loss in iteration 81 : 0.32458185169491
Loss in iteration 82 : 0.3245547744216308
Loss in iteration 83 : 0.32452867397433677
Loss in iteration 84 : 0.3245035034938476
Loss in iteration 85 : 0.32447921870297464
Loss in iteration 86 : 0.32445577774756684
Loss in iteration 87 : 0.32443314104849197
Loss in iteration 88 : 0.324411271163716
Loss in iteration 89 : 0.3243901326597125
Loss in iteration 90 : 0.32436969199146864
Loss in iteration 91 : 0.3243499173904715
Loss in iteration 92 : 0.32433077876008903
Loss in iteration 93 : 0.3243122475777614
Loss in iteration 94 : 0.32429429680354427
Loss in iteration 95 : 0.32427690079453086
Loss in iteration 96 : 0.32426003522474406
Loss in iteration 97 : 0.32424367701008505
Loss in iteration 98 : 0.3242278042380148
Loss in iteration 99 : 0.3242123961016192
Loss in iteration 100 : 0.32419743283775165
Loss in iteration 101 : 0.3241828956689965
Loss in iteration 102 : 0.32416876674917944
Loss in iteration 103 : 0.3241550291121647
Loss in iteration 104 : 0.32414166662376515
Loss in iteration 105 : 0.3241286639365177
Loss in iteration 106 : 0.32411600644715904
Loss in iteration 107 : 0.3241036802566244
Loss in iteration 108 : 0.32409167213236345
Loss in iteration 109 : 0.32407996947291917
Loss in iteration 110 : 0.32406856027450726
Loss in iteration 111 : 0.32405743309955287
Loss in iteration 112 : 0.3240465770470403
Loss in iteration 113 : 0.32403598172453313
Loss in iteration 114 : 0.32402563722181804
Loss in iteration 115 : 0.3240155340859916
Loss in iteration 116 : 0.3240056632979964
Loss in iteration 117 : 0.3239960162504343
Loss in iteration 118 : 0.32398658472661906
Loss in iteration 119 : 0.32397736088080153
Loss in iteration 120 : 0.32396833721947565
Loss in iteration 121 : 0.32395950658369455
Loss in iteration 122 : 0.32395086213237945
Loss in iteration 123 : 0.32394239732651164
Loss in iteration 124 : 0.3239341059141772
Loss in iteration 125 : 0.323925981916435
Loss in iteration 126 : 0.32391801961391425
Loss in iteration 127 : 0.3239102135341553
Loss in iteration 128 : 0.32390255843958876
Loss in iteration 129 : 0.3238950493161823
Loss in iteration 130 : 0.32388768136264984
Loss in iteration 131 : 0.3238804499802568
Loss in iteration 132 : 0.32387335076314255
Loss in iteration 133 : 0.3238663794891286
Loss in iteration 134 : 0.32385953211103885
Loss in iteration 135 : 0.32385280474843137
Loss in iteration 136 : 0.3238461936797793
Loss in iteration 137 : 0.3238396953350157
Loss in iteration 138 : 0.3238333062885161
Loss in iteration 139 : 0.32382702325236884
Loss in iteration 140 : 0.32382084307003545
Loss in iteration 141 : 0.3238147627103041
Loss in iteration 142 : 0.32380877926155843
Loss in iteration 143 : 0.3238028899263136
Loss in iteration 144 : 0.3237970920160507
Loss in iteration 145 : 0.3237913829462892
Loss in iteration 146 : 0.3237857602318889
Loss in iteration 147 : 0.3237802214826216
Loss in iteration 148 : 0.3237747643989297
Loss in iteration 149 : 0.32376938676788963
Loss in iteration 150 : 0.3237640864593894
Loss in iteration 151 : 0.3237588614224782
Loss in iteration 152 : 0.32375370968190365
Loss in iteration 153 : 0.323748629334796
Loss in iteration 154 : 0.3237436185475294
Loss in iteration 155 : 0.32373867555272834
Loss in iteration 156 : 0.3237337986464086
Loss in iteration 157 : 0.32372898618526796
Loss in iteration 158 : 0.32372423658408805
Loss in iteration 159 : 0.32371954831327016
Loss in iteration 160 : 0.32371491989648754
Loss in iteration 161 : 0.3237103499084379
Loss in iteration 162 : 0.3237058369727044
Loss in iteration 163 : 0.32370137975972174
Loss in iteration 164 : 0.32369697698483546
Loss in iteration 165 : 0.3236926274064353
Loss in iteration 166 : 0.32368832982419543
Loss in iteration 167 : 0.32368408307738794
Loss in iteration 168 : 0.3236798860432612
Loss in iteration 169 : 0.32367573763551255
Loss in iteration 170 : 0.3236716368028131
Loss in iteration 171 : 0.3236675825274101
Loss in iteration 172 : 0.32366357382377614
Loss in iteration 173 : 0.323659609737349
Loss in iteration 174 : 0.3236556893433003
Loss in iteration 175 : 0.3236518117453586
Loss in iteration 176 : 0.32364797607471196
Loss in iteration 177 : 0.32364418148893065
Loss in iteration 178 : 0.3236404271709422
Loss in iteration 179 : 0.32363671232807384
Loss in iteration 180 : 0.32363303619110206
Loss in iteration 181 : 0.3236293980133748
Loss in iteration 182 : 0.3236257970699544
Loss in iteration 183 : 0.3236222326567915
Loss in iteration 184 : 0.32361870408996846
Loss in iteration 185 : 0.32361521070492233
Loss in iteration 186 : 0.3236117518557534
Loss in iteration 187 : 0.3236083269145334
Loss in iteration 188 : 0.3236049352706426
Loss in iteration 189 : 0.32360157633014364
Loss in iteration 190 : 0.32359824951519967
Loss in iteration 191 : 0.3235949542634667
Loss in iteration 192 : 0.32359169002756805
Loss in iteration 193 : 0.32358845627454563
Loss in iteration 194 : 0.3235852524853683
Loss in iteration 195 : 0.3235820781544399
Loss in iteration 196 : 0.3235789327891247
Loss in iteration 197 : 0.3235758159093175
Loss in iteration 198 : 0.3235727270469979
Loss in iteration 199 : 0.32356966574583124
Loss in iteration 200 : 0.323566631560768
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.8498863706160555, training accuracy 0.8493857493857494, time elapsed: 3191 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.47514378797843904
Loss in iteration 3 : 0.4445504966681495
Loss in iteration 4 : 0.42588965115498345
Loss in iteration 5 : 0.4118445393610018
Loss in iteration 6 : 0.4006997036889009
Loss in iteration 7 : 0.39165579239504344
Loss in iteration 8 : 0.38420376870086964
Loss in iteration 9 : 0.37798366496469926
Loss in iteration 10 : 0.3727315722510957
Loss in iteration 11 : 0.3682504383083156
Loss in iteration 12 : 0.3643910346715088
Loss in iteration 13 : 0.3610388582566909
Loss in iteration 14 : 0.35810494083184374
Loss in iteration 15 : 0.3555193246773395
Loss in iteration 16 : 0.35322637979469823
Loss in iteration 17 : 0.3511814041041106
Loss in iteration 18 : 0.34934812591119163
Loss in iteration 19 : 0.34769684760034
Loss in iteration 20 : 0.3462030501530762
Loss in iteration 21 : 0.3448463326230744
Loss in iteration 22 : 0.34360959781897576
Loss in iteration 23 : 0.34247842093159103
Loss in iteration 24 : 0.34144055551373054
Loss in iteration 25 : 0.34048554360329986
Loss in iteration 26 : 0.33960440554879184
Loss in iteration 27 : 0.3387893913712806
Loss in iteration 28 : 0.3380337800334635
Loss in iteration 29 : 0.337331716298093
Loss in iteration 30 : 0.3366780772990522
Loss in iteration 31 : 0.3360683627631961
Loss in iteration 32 : 0.3354986041823979
Loss in iteration 33 : 0.33496528926436275
Loss in iteration 34 : 0.3344652987751763
Loss in iteration 35 : 0.3339958534883777
Loss in iteration 36 : 0.33355446942078293
Loss in iteration 37 : 0.3331389198972982
Loss in iteration 38 : 0.332747203270546
Loss in iteration 39 : 0.3323775153444441
Loss in iteration 40 : 0.33202822572792157
Loss in iteration 41 : 0.33169785748590636
Loss in iteration 42 : 0.33138506956770225
Loss in iteration 43 : 0.33108864158374995
Loss in iteration 44 : 0.3308074605752819
Loss in iteration 45 : 0.3305405094811251
Loss in iteration 46 : 0.33028685705464483
Loss in iteration 47 : 0.3300456490237985
Loss in iteration 48 : 0.32981610032006803
Loss in iteration 49 : 0.32959748822929463
Loss in iteration 50 : 0.3293891463398699
Loss in iteration 51 : 0.32919045918248657
Loss in iteration 52 : 0.32900085747133034
Loss in iteration 53 : 0.3288198138696624
Loss in iteration 54 : 0.3286468392137294
Loss in iteration 55 : 0.32848147913831743
Loss in iteration 56 : 0.3283233110549674
Loss in iteration 57 : 0.32817194144071016
Loss in iteration 58 : 0.32802700340062374
Loss in iteration 59 : 0.32788815447256336
Loss in iteration 60 : 0.3277550746463535
Loss in iteration 61 : 0.3276274645733845
Loss in iteration 62 : 0.3275050439455369
Loss in iteration 63 : 0.3273875500249995
Loss in iteration 64 : 0.32727473630878673
Loss in iteration 65 : 0.32716637131370424
Loss in iteration 66 : 0.3270622374692677
Loss in iteration 67 : 0.32696213010742686
Loss in iteration 68 : 0.32686585653939104
Loss in iteration 69 : 0.32677323521079504
Loss in iteration 70 : 0.32668409492756895
Loss in iteration 71 : 0.32659827414562986
Loss in iteration 72 : 0.3265156203182974
Loss in iteration 73 : 0.3264359892960417
Loss in iteration 74 : 0.3263592447736294
Loss in iteration 75 : 0.3262852577803755
Loss in iteration 76 : 0.3262139062096003
Loss in iteration 77 : 0.3261450743837717
Loss in iteration 78 : 0.326078652652184
Loss in iteration 79 : 0.326014537018411
Loss in iteration 80 : 0.32595262879487547
Loss in iteration 81 : 0.3258928342823362
Loss in iteration 82 : 0.32583506447214594
Loss in iteration 83 : 0.3257792347694434
Loss in iteration 84 : 0.32572526473551944
Loss in iteration 85 : 0.3256730778478818
Loss in iteration 86 : 0.32562260127653747
Loss in iteration 87 : 0.32557376567530005
Loss in iteration 88 : 0.32552650498684993
Loss in iteration 89 : 0.32548075626061634
Loss in iteration 90 : 0.32543645948238775
Loss in iteration 91 : 0.3253935574148717
Loss in iteration 92 : 0.3253519954483073
Loss in iteration 93 : 0.32531172146047416
Loss in iteration 94 : 0.32527268568535045
Loss in iteration 95 : 0.32523484058982916
Loss in iteration 96 : 0.325198140757936
Loss in iteration 97 : 0.32516254278199
Loss in iteration 98 : 0.32512800516024043
Loss in iteration 99 : 0.32509448820054615
Loss in iteration 100 : 0.32506195392967435
Loss in iteration 101 : 0.3250303660078365
Loss in iteration 102 : 0.32499968964811743
Loss in iteration 103 : 0.3249698915404968
Loss in iteration 104 : 0.3249409397801218
Loss in iteration 105 : 0.32491280379958637
Loss in iteration 106 : 0.32488545430495525
Loss in iteration 107 : 0.32485886321529683
Loss in iteration 108 : 0.3248330036054855
Loss in iteration 109 : 0.32480784965209425
Loss in iteration 110 : 0.3247833765821864
Loss in iteration 111 : 0.3247595606248119
Loss in iteration 112 : 0.32473637896504604
Loss in iteration 113 : 0.3247138097004451
Loss in iteration 114 : 0.3246918317997404
Loss in iteration 115 : 0.32467042506365956
Loss in iteration 116 : 0.3246495700877418
Loss in iteration 117 : 0.3246292482270488
Loss in iteration 118 : 0.3246094415626249
Loss in iteration 119 : 0.3245901328696541
Loss in iteration 120 : 0.324571305587182
Loss in iteration 121 : 0.3245529437893169
Loss in iteration 122 : 0.3245350321578676
Loss in iteration 123 : 0.3245175559562771
Loss in iteration 124 : 0.32450050100483846
Loss in iteration 125 : 0.324483853657074
Loss in iteration 126 : 0.32446760077725934
Loss in iteration 127 : 0.3244517297189964
Loss in iteration 128 : 0.3244362283047981
Loss in iteration 129 : 0.3244210848066272
Loss in iteration 130 : 0.32440628792733905
Loss in iteration 131 : 0.32439182678297
Loss in iteration 132 : 0.3243776908858561
Loss in iteration 133 : 0.324363870128504
Loss in iteration 134 : 0.32435035476820717
Loss in iteration 135 : 0.32433713541234294
Loss in iteration 136 : 0.3243242030043359
Loss in iteration 137 : 0.32431154881025154
Loss in iteration 138 : 0.324299164405971
Loss in iteration 139 : 0.3242870416649433
Loss in iteration 140 : 0.3242751727464671
Loss in iteration 141 : 0.3242635500844872
Loss in iteration 142 : 0.32425216637687687
Loss in iteration 143 : 0.32424101457519283
Loss in iteration 144 : 0.32423008787483387
Loss in iteration 145 : 0.3242193797056863
Loss in iteration 146 : 0.3242088837230966
Loss in iteration 147 : 0.3241985937992645
Loss in iteration 148 : 0.3241885040150085
Loss in iteration 149 : 0.324178608651842
Loss in iteration 150 : 0.3241689021844023
Loss in iteration 151 : 0.32415937927320315
Loss in iteration 152 : 0.3241500347576558
Loss in iteration 153 : 0.32414086364941624
Loss in iteration 154 : 0.3241318611259692
Loss in iteration 155 : 0.32412302252449904
Loss in iteration 156 : 0.3241143433359976
Loss in iteration 157 : 0.3241058191996183
Loss in iteration 158 : 0.32409744589724476
Loss in iteration 159 : 0.3240892193483026
Loss in iteration 160 : 0.32408113560473906
Loss in iteration 161 : 0.32407319084624536
Loss in iteration 162 : 0.32406538137564184
Loss in iteration 163 : 0.3240577036144458
Loss in iteration 164 : 0.32405015409863375
Loss in iteration 165 : 0.3240427294745484
Loss in iteration 166 : 0.3240354264949639
Loss in iteration 167 : 0.32402824201533154
Loss in iteration 168 : 0.3240211729901446
Loss in iteration 169 : 0.32401421646944567
Loss in iteration 170 : 0.32400736959548343
Loss in iteration 171 : 0.3240006295994829
Loss in iteration 172 : 0.3239939937985425
Loss in iteration 173 : 0.3239874595926628
Loss in iteration 174 : 0.3239810244618603
Loss in iteration 175 : 0.32397468596341217
Loss in iteration 176 : 0.3239684417292065
Loss in iteration 177 : 0.3239622894631731
Loss in iteration 178 : 0.3239562269388266
Loss in iteration 179 : 0.32395025199689215
Loss in iteration 180 : 0.3239443625430289
Loss in iteration 181 : 0.3239385565456253
Loss in iteration 182 : 0.32393283203368556
Loss in iteration 183 : 0.3239271870947923
Loss in iteration 184 : 0.3239216198731364
Loss in iteration 185 : 0.3239161285676324
Loss in iteration 186 : 0.32391071143008465
Loss in iteration 187 : 0.3239053667634399
Loss in iteration 188 : 0.3239000929200876
Loss in iteration 189 : 0.32389488830022767
Loss in iteration 190 : 0.3238897513502983
Loss in iteration 191 : 0.3238846805614573
Loss in iteration 192 : 0.3238796744681205
Loss in iteration 193 : 0.3238747316465436
Loss in iteration 194 : 0.32386985071347585
Loss in iteration 195 : 0.32386503032482894
Loss in iteration 196 : 0.3238602691744293
Loss in iteration 197 : 0.32385556599278076
Loss in iteration 198 : 0.323850919545895
Loss in iteration 199 : 0.3238463286341548
Loss in iteration 200 : 0.32384179209120395
Testing accuracy  of updater 3 on alg 0 with rate 0.7 = 0.8498249493274369, training accuracy 0.849477886977887, time elapsed: 3369 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5412680596499643
Loss in iteration 3 : 0.48971292920688897
Loss in iteration 4 : 0.46434579615552685
Loss in iteration 5 : 0.4481665811416415
Loss in iteration 6 : 0.43614657970790865
Loss in iteration 7 : 0.4264357362697102
Loss in iteration 8 : 0.41822778805599725
Loss in iteration 9 : 0.41111465560056315
Loss in iteration 10 : 0.40485914131397216
Loss in iteration 11 : 0.39930573374912437
Loss in iteration 12 : 0.39434247794959976
Loss in iteration 13 : 0.38988334863653096
Loss in iteration 14 : 0.3858594045749252
Loss in iteration 15 : 0.38221392748192884
Loss in iteration 16 : 0.37889948141515123
Loss in iteration 17 : 0.37587596494772196
Loss in iteration 18 : 0.3731092203611535
Loss in iteration 19 : 0.3705699842834187
Loss in iteration 20 : 0.36823306601329053
Loss in iteration 21 : 0.36607668863758186
Loss in iteration 22 : 0.36408195265597526
Loss in iteration 23 : 0.3622323950241841
Loss in iteration 24 : 0.3605136241676746
Loss in iteration 25 : 0.35891301632451256
Loss in iteration 26 : 0.357419461841164
Loss in iteration 27 : 0.356023152404135
Loss in iteration 28 : 0.354715401971683
Loss in iteration 29 : 0.35348849555349326
Loss in iteration 30 : 0.35233556108055797
Loss in iteration 31 : 0.3512504604818395
Loss in iteration 32 : 0.3502276967878797
Loss in iteration 33 : 0.34926233464986145
Loss in iteration 34 : 0.34834993212332277
Loss in iteration 35 : 0.347486481940029
Loss in iteration 36 : 0.34666836079661034
Loss in iteration 37 : 0.34589228543749456
Loss in iteration 38 : 0.34515527451364564
Loss in iteration 39 : 0.34445461536596683
Loss in iteration 40 : 0.34378783501994975
Loss in iteration 41 : 0.34315267479187006
Loss in iteration 42 : 0.34254706800087925
Loss in iteration 43 : 0.3419691203594372
Loss in iteration 44 : 0.34141709267952924
Loss in iteration 45 : 0.3408893855863895
Loss in iteration 46 : 0.3403845259767799
Loss in iteration 47 : 0.33990115499714096
Loss in iteration 48 : 0.33943801734886403
Loss in iteration 49 : 0.338993951755148
Loss in iteration 50 : 0.3385678824467458
Loss in iteration 51 : 0.338158811543423
Loss in iteration 52 : 0.33776581222444674
Loss in iteration 53 : 0.3373880225955771
Loss in iteration 54 : 0.337024640172089
Loss in iteration 55 : 0.3366749169076593
Loss in iteration 56 : 0.336338154707946
Loss in iteration 57 : 0.3360137013751794
Loss in iteration 58 : 0.3357009469368336
Loss in iteration 59 : 0.33539932031703334
Loss in iteration 60 : 0.3351082863144273
Loss in iteration 61 : 0.3348273428544019
Loss in iteration 62 : 0.33455601848738503
Loss in iteration 63 : 0.33429387010815065
Loss in iteration 64 : 0.33404048087393584
Loss in iteration 65 : 0.3337954583016602
Loss in iteration 66 : 0.3335584325266868
Loss in iteration 67 : 0.33332905470753293
Loss in iteration 68 : 0.3331069955625732
Loss in iteration 69 : 0.33289194402634004
Loss in iteration 70 : 0.33268360601420105
Loss in iteration 71 : 0.3324817032855268
Loss in iteration 72 : 0.332285972396348
Loss in iteration 73 : 0.33209616373346046
Loss in iteration 74 : 0.33191204062277146
Loss in iteration 75 : 0.33173337850535406
Loss in iteration 76 : 0.33155996417534234
Loss in iteration 77 : 0.3313915950743535
Loss in iteration 78 : 0.33122807863766307
Loss in iteration 79 : 0.33106923168775515
Loss in iteration 80 : 0.33091487987137513
Loss in iteration 81 : 0.33076485713644244
Loss in iteration 82 : 0.33061900524566085
Loss in iteration 83 : 0.33047717332382875
Loss in iteration 84 : 0.3303392174361828
Loss in iteration 85 : 0.33020500019535814
Loss in iteration 86 : 0.33007439039470216
Loss in iteration 87 : 0.3299472626659477
Loss in iteration 88 : 0.32982349715935416
Loss in iteration 89 : 0.32970297924466585
Loss in iteration 90 : 0.32958559923126873
Loss in iteration 91 : 0.32947125210620376
Loss in iteration 92 : 0.32935983728865076
Loss in iteration 93 : 0.32925125839976166
Loss in iteration 94 : 0.3291454230466699
Loss in iteration 95 : 0.3290422426197364
Loss in iteration 96 : 0.3289416321020457
Loss in iteration 97 : 0.3288435098903259
Loss in iteration 98 : 0.3287477976264955
Loss in iteration 99 : 0.32865442003910056
Loss in iteration 100 : 0.3285633047940116
Loss in iteration 101 : 0.3284743823536751
Loss in iteration 102 : 0.32838758584445554
Loss in iteration 103 : 0.3283028509314423
Loss in iteration 104 : 0.3282201157002828
Loss in iteration 105 : 0.32813932054558914
Loss in iteration 106 : 0.328060408065455
Loss in iteration 107 : 0.32798332296174226
Loss in iteration 108 : 0.3279080119457493
Loss in iteration 109 : 0.3278344236488998
Loss in iteration 110 : 0.32776250853822175
Loss in iteration 111 : 0.3276922188362014
Loss in iteration 112 : 0.32762350844487786
Loss in iteration 113 : 0.32755633287379315
Loss in iteration 114 : 0.32749064917169185
Loss in iteration 115 : 0.3274264158616311
Loss in iteration 116 : 0.3273635928793895
Loss in iteration 117 : 0.32730214151493564
Loss in iteration 118 : 0.32724202435678923
Loss in iteration 119 : 0.3271832052391059
Loss in iteration 120 : 0.3271256491913553
Loss in iteration 121 : 0.3270693223903907
Loss in iteration 122 : 0.3270141921148441
Loss in iteration 123 : 0.32696022670166214
Loss in iteration 124 : 0.3269073955046925
Loss in iteration 125 : 0.32685566885520284
Loss in iteration 126 : 0.32680501802422696
Loss in iteration 127 : 0.326755415186639
Loss in iteration 128 : 0.3267068333868675
Loss in iteration 129 : 0.3266592465061408
Loss in iteration 130 : 0.32661262923122863
Loss in iteration 131 : 0.3265669570245519
Loss in iteration 132 : 0.32652220609562593
Loss in iteration 133 : 0.3264783533737406
Loss in iteration 134 : 0.3264353764818418
Loss in iteration 135 : 0.3263932537115302
Loss in iteration 136 : 0.3263519639991351
Loss in iteration 137 : 0.32631148690280004
Loss in iteration 138 : 0.32627180258054805
Loss in iteration 139 : 0.3262328917692498
Loss in iteration 140 : 0.3261947357644621
Loss in iteration 141 : 0.3261573164011371
Loss in iteration 142 : 0.32612061603506676
Loss in iteration 143 : 0.3260846175251153
Loss in iteration 144 : 0.3260493042161831
Loss in iteration 145 : 0.3260146599228022
Loss in iteration 146 : 0.32598066891344823
Loss in iteration 147 : 0.3259473158954202
Loss in iteration 148 : 0.3259145860003485
Loss in iteration 149 : 0.325882464770258
Loss in iteration 150 : 0.3258509381441659
Loss in iteration 151 : 0.32581999244519283
Loss in iteration 152 : 0.32578961436819504
Loss in iteration 153 : 0.32575979096782376
Loss in iteration 154 : 0.3257305096470709
Loss in iteration 155 : 0.3257017581462277
Loss in iteration 156 : 0.32567352453225357
Loss in iteration 157 : 0.32564579718855274
Loss in iteration 158 : 0.3256185648051068
Loss in iteration 159 : 0.3255918163689916
Loss in iteration 160 : 0.3255655411552147
Loss in iteration 161 : 0.32553972871789766
Loss in iteration 162 : 0.3255143688817824
Loss in iteration 163 : 0.3254894517340147
Loss in iteration 164 : 0.3254649676162502
Loss in iteration 165 : 0.3254409071170132
Loss in iteration 166 : 0.3254172610643418
Loss in iteration 167 : 0.3253940205186738
Loss in iteration 168 : 0.32537117676599103
Loss in iteration 169 : 0.3253487213111945
Loss in iteration 170 : 0.32532664587171056
Loss in iteration 171 : 0.32530494237130214
Loss in iteration 172 : 0.32528360293410863
Loss in iteration 173 : 0.32526261987887345
Loss in iteration 174 : 0.32524198571336504
Loss in iteration 175 : 0.32522169312899285
Loss in iteration 176 : 0.32520173499558896
Loss in iteration 177 : 0.3251821043563734
Loss in iteration 178 : 0.3251627944230774
Loss in iteration 179 : 0.32514379857122483
Loss in iteration 180 : 0.3251251103355726
Loss in iteration 181 : 0.3251067234056962
Loss in iteration 182 : 0.3250886316217096
Loss in iteration 183 : 0.32507082897013406
Loss in iteration 184 : 0.3250533095798829
Loss in iteration 185 : 0.32503606771839577
Loss in iteration 186 : 0.3250190977878676
Loss in iteration 187 : 0.3250023943216076
Loss in iteration 188 : 0.32498595198053565
Loss in iteration 189 : 0.3249697655497274
Loss in iteration 190 : 0.32495382993513144
Loss in iteration 191 : 0.32493814016035305
Loss in iteration 192 : 0.3249226913635286
Loss in iteration 193 : 0.3249074787943173
Loss in iteration 194 : 0.3248924978109836
Loss in iteration 195 : 0.32487774387753704
Loss in iteration 196 : 0.3248632125609956
Loss in iteration 197 : 0.32484889952871054
Loss in iteration 198 : 0.32483480054577013
Loss in iteration 199 : 0.3248209114724905
Loss in iteration 200 : 0.3248072282619661
Testing accuracy  of updater 3 on alg 0 with rate 0.4 = 0.8497635280388183, training accuracy 0.8492628992628992, time elapsed: 3799 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.64911163328236
Loss in iteration 3 : 0.6147689552179676
Loss in iteration 4 : 0.5875964300985204
Loss in iteration 5 : 0.5657805657026809
Loss in iteration 6 : 0.5480101487632495
Loss in iteration 7 : 0.5333296076730865
Loss in iteration 8 : 0.5210359966960235
Loss in iteration 9 : 0.5106069425961233
Loss in iteration 10 : 0.5016502136875369
Loss in iteration 11 : 0.49386826139527923
Loss in iteration 12 : 0.48703311084870765
Loss in iteration 13 : 0.4809684254050768
Loss in iteration 14 : 0.4755365737156931
Loss in iteration 15 : 0.4706292124868992
Loss in iteration 16 : 0.4661603619833962
Loss in iteration 17 : 0.462061265609905
Loss in iteration 18 : 0.4582765386038419
Loss in iteration 19 : 0.4547612570479139
Loss in iteration 20 : 0.45147873915123443
Loss in iteration 21 : 0.44839884075394776
Loss in iteration 22 : 0.4454966360876508
Loss in iteration 23 : 0.44275138953696697
Loss in iteration 24 : 0.440145748921007
Loss in iteration 25 : 0.437665108648298
Loss in iteration 26 : 0.43529710404945177
Loss in iteration 27 : 0.4330312076738196
Loss in iteration 28 : 0.43085840533477543
Loss in iteration 29 : 0.4287709348934307
Loss in iteration 30 : 0.42676207467071386
Loss in iteration 31 : 0.4248259713208875
Loss in iteration 32 : 0.4229574992353994
Loss in iteration 33 : 0.42115214525569145
Loss in iteration 34 : 0.41940591378900915
Loss in iteration 35 : 0.41771524843917474
Loss in iteration 36 : 0.41607696705656755
Loss in iteration 37 : 0.4144882077313301
Loss in iteration 38 : 0.41294638374134823
Loss in iteration 39 : 0.411449145851554
Loss in iteration 40 : 0.40999435066691553
Loss in iteration 41 : 0.4085800339851244
Loss in iteration 42 : 0.40720438829014316
Loss in iteration 43 : 0.4058657436844724
Loss in iteration 44 : 0.40456255168454536
Loss in iteration 45 : 0.40329337140593735
Loss in iteration 46 : 0.4020568577482413
Loss in iteration 47 : 0.4008517512571237
Loss in iteration 48 : 0.3996768693964489
Loss in iteration 49 : 0.39853109900863243
Loss in iteration 50 : 0.39741338977863644
Loss in iteration 51 : 0.39632274854770005
Loss in iteration 52 : 0.39525823434814944
Loss in iteration 53 : 0.3942189540516027
Loss in iteration 54 : 0.39320405854028345
Loss in iteration 55 : 0.3922127393254683
Loss in iteration 56 : 0.3912442255492224
Loss in iteration 57 : 0.39029778131543724
Loss in iteration 58 : 0.38937270330472823
Loss in iteration 59 : 0.3884683186345249
Loss in iteration 60 : 0.38758398293176005
Loss in iteration 61 : 0.3867190785902911
Loss in iteration 62 : 0.3858730131894754
Loss in iteration 63 : 0.3850452180536979
Loss in iteration 64 : 0.3842351469356378
Loss in iteration 65 : 0.3834422748084998
Loss in iteration 66 : 0.3826660967545755
Loss in iteration 67 : 0.3819061269392301
Loss in iteration 68 : 0.38116189766089653
Loss in iteration 69 : 0.38043295846904707
Loss in iteration 70 : 0.3797188753430292
Loss in iteration 71 : 0.3790192299256982
Loss in iteration 72 : 0.3783336188065628
Loss in iteration 73 : 0.37766165284973574
Loss in iteration 74 : 0.3770029565626734
Loss in iteration 75 : 0.3763571675021172
Loss in iteration 76 : 0.37572393571407303
Loss in iteration 77 : 0.37510292320506616
Loss in iteration 78 : 0.3744938034421797
Loss in iteration 79 : 0.37389626087969813
Loss in iteration 80 : 0.3733099905103612
Loss in iteration 81 : 0.3727346974395133
Loss in iteration 82 : 0.3721700964805186
Loss in iteration 83 : 0.3716159117700324
Loss in iteration 84 : 0.3710718764018493
Loss in iteration 85 : 0.3705377320781379
Loss in iteration 86 : 0.370013228776975
Loss in iteration 87 : 0.36949812443526125
Loss in iteration 88 : 0.36899218464603983
Loss in iteration 89 : 0.36849518236945655
Loss in iteration 90 : 0.36800689765657707
Loss in iteration 91 : 0.36752711738536287
Loss in iteration 92 : 0.3670556350081446
Loss in iteration 93 : 0.3665922503099986
Loss in iteration 94 : 0.36613676917746696
Loss in iteration 95 : 0.3656890033770871
Loss in iteration 96 : 0.36524877034323
Loss in iteration 97 : 0.36481589297480643
Loss in iteration 98 : 0.36439019944041534
Loss in iteration 99 : 0.3639715229914849
Loss in iteration 100 : 0.36355970178308283
Loss in iteration 101 : 0.3631545787019821
Loss in iteration 102 : 0.3627560012016973
Loss in iteration 103 : 0.3623638211441209
Loss in iteration 104 : 0.3619778946474811
Loss in iteration 105 : 0.3615980819403426
Loss in iteration 106 : 0.3612242472213625
Loss in iteration 107 : 0.3608562585245389
Loss in iteration 108 : 0.3604939875897461
Loss in iteration 109 : 0.3601373097382697
Loss in iteration 110 : 0.3597861037531576
Loss in iteration 111 : 0.359440251764188
Loss in iteration 112 : 0.3590996391372065
Loss in iteration 113 : 0.35876415436770054
Loss in iteration 114 : 0.35843368897839584
Loss in iteration 115 : 0.35810813742069697
Loss in iteration 116 : 0.357787396979851
Loss in iteration 117 : 0.35747136768363724
Loss in iteration 118 : 0.3571599522144373
Loss in iteration 119 : 0.3568530558245926
Loss in iteration 120 : 0.3565505862548278
Loss in iteration 121 : 0.3562524536556998
Loss in iteration 122 : 0.3559585705119074
Loss in iteration 123 : 0.3556688515693251
Loss in iteration 124 : 0.3553832137646879
Loss in iteration 125 : 0.3551015761578109
Loss in iteration 126 : 0.35482385986622306
Loss in iteration 127 : 0.35454998800212617
Loss in iteration 128 : 0.35427988561160817
Loss in iteration 129 : 0.35401347961598384
Loss in iteration 130 : 0.35375069875519893
Loss in iteration 131 : 0.3534914735332249
Loss in iteration 132 : 0.35323573616534043
Loss in iteration 133 : 0.3529834205272572
Loss in iteration 134 : 0.3527344621059791
Loss in iteration 135 : 0.35248879795236154
Loss in iteration 136 : 0.35224636663530223
Loss in iteration 137 : 0.35200710819747477
Loss in iteration 138 : 0.3517709641125534
Loss in iteration 139 : 0.35153787724391905
Loss in iteration 140 : 0.3513077918047056
Loss in iteration 141 : 0.35108065331920785
Loss in iteration 142 : 0.3508564085855756
Loss in iteration 143 : 0.3506350056397286
Loss in iteration 144 : 0.3504163937204588
Loss in iteration 145 : 0.3502005232357123
Loss in iteration 146 : 0.3499873457299182
Loss in iteration 147 : 0.3497768138524294
Loss in iteration 148 : 0.349568881326956
Loss in iteration 149 : 0.3493635029220054
Loss in iteration 150 : 0.34916063442223627
Loss in iteration 151 : 0.3489602326007806
Loss in iteration 152 : 0.3487622551923914
Loss in iteration 153 : 0.3485666608674773
Loss in iteration 154 : 0.3483734092069501
Loss in iteration 155 : 0.34818246067783337
Loss in iteration 156 : 0.34799377660968395
Loss in iteration 157 : 0.3478073191716997
Loss in iteration 158 : 0.34762305135055976
Loss in iteration 159 : 0.3474409369289575
Loss in iteration 160 : 0.34726094046475064
Loss in iteration 161 : 0.34708302727081625
Loss in iteration 162 : 0.3469071633954352
Loss in iteration 163 : 0.34673331560335946
Loss in iteration 164 : 0.34656145135738053
Loss in iteration 165 : 0.3463915388004886
Loss in iteration 166 : 0.3462235467385544
Loss in iteration 167 : 0.3460574446235218
Loss in iteration 168 : 0.3458932025371205
Loss in iteration 169 : 0.345730791175033
Loss in iteration 170 : 0.34557018183153354
Loss in iteration 171 : 0.34541134638460635
Loss in iteration 172 : 0.3452542572814436
Loss in iteration 173 : 0.34509888752442125
Loss in iteration 174 : 0.3449452106574374
Loss in iteration 175 : 0.34479320075266295
Loss in iteration 176 : 0.34464283239768473
Loss in iteration 177 : 0.3444940806829828
Loss in iteration 178 : 0.34434692118979005
Loss in iteration 179 : 0.3442013299782894
Loss in iteration 180 : 0.3440572835761424
Loss in iteration 181 : 0.3439147589673388
Loss in iteration 182 : 0.34377373358135127
Loss in iteration 183 : 0.3436341852826203
Loss in iteration 184 : 0.34349609236029605
Loss in iteration 185 : 0.3433594335182793
Loss in iteration 186 : 0.3432241878655383
Loss in iteration 187 : 0.3430903349066913
Loss in iteration 188 : 0.34295785453284394
Loss in iteration 189 : 0.342826727012666
Loss in iteration 190 : 0.3426969329837281
Loss in iteration 191 : 0.34256845344406045
Loss in iteration 192 : 0.3424412697439474
Loss in iteration 193 : 0.34231536357792486
Loss in iteration 194 : 0.34219071697699993
Loss in iteration 195 : 0.34206731230109705
Loss in iteration 196 : 0.34194513223166256
Loss in iteration 197 : 0.34182415976450636
Loss in iteration 198 : 0.34170437820279687
Loss in iteration 199 : 0.34158577115026684
Loss in iteration 200 : 0.3414683225045724
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.843744241754192, training accuracy 0.8433046683046683, time elapsed: 4005 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 5342 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 7.0 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 5371 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 4.0 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 6581 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 9215 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 0.7 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 4975 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 0.4 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 4777 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6920044749122338
Loss in iteration 3 : 0.6908389442411161
Loss in iteration 4 : 0.6896609571526451
Loss in iteration 5 : 0.6884756519129271
Loss in iteration 6 : 0.6872860809116603
Loss in iteration 7 : 0.6860942527576904
Loss in iteration 8 : 0.6849015780586383
Loss in iteration 9 : 0.6837090923707456
Loss in iteration 10 : 0.6825175802631033
Loss in iteration 11 : 0.6813276499489451
Loss in iteration 12 : 0.680139780933271
Loss in iteration 13 : 0.6789543558969313
Loss in iteration 14 : 0.6777716828619428
Loss in iteration 15 : 0.6765920110976974
Loss in iteration 16 : 0.6754155428517176
Loss in iteration 17 : 0.6742424422178139
Loss in iteration 18 : 0.6730728420042023
Loss in iteration 19 : 0.6719068491910072
Loss in iteration 20 : 0.6707445493895094
Loss in iteration 21 : 0.6695860105843084
Loss in iteration 22 : 0.668431286323087
Loss in iteration 23 : 0.6672804184135043
Loss in iteration 24 : 0.6661334391203165
Loss in iteration 25 : 0.6649903728603587
Loss in iteration 26 : 0.6638512374534761
Loss in iteration 27 : 0.6627160450432952
Loss in iteration 28 : 0.6615848028091574
Loss in iteration 29 : 0.6604575135582188
Loss in iteration 30 : 0.6593341762475324
Loss in iteration 31 : 0.6582147864581975
Loss in iteration 32 : 0.6570993368292505
Loss in iteration 33 : 0.6559878174524506
Loss in iteration 34 : 0.654880216226413
Loss in iteration 35 : 0.6537765191682473
Loss in iteration 36 : 0.6526767106824045
Loss in iteration 37 : 0.6515807737893057
Loss in iteration 38 : 0.6504886903183353
Loss in iteration 39 : 0.6494004410707997
Loss in iteration 40 : 0.6483160059580525
Loss in iteration 41 : 0.6472353641186898
Loss in iteration 42 : 0.6461584940178311
Loss in iteration 43 : 0.6450853735303794
Loss in iteration 44 : 0.6440159800096135
Loss in iteration 45 : 0.642950290342046
Loss in iteration 46 : 0.6418882809892223
Loss in iteration 47 : 0.6408299280168186
Loss in iteration 48 : 0.6397752071113075
Loss in iteration 49 : 0.6387240935841674
Loss in iteration 50 : 0.6376765623635855
Loss in iteration 51 : 0.6366325879733169
Loss in iteration 52 : 0.6355921444984665
Loss in iteration 53 : 0.6345552055378624
Loss in iteration 54 : 0.6335217441429143
Loss in iteration 55 : 0.6324917327434713
Loss in iteration 56 : 0.6314651430619954
Loss in iteration 57 : 0.6304419460189089
Loss in iteration 58 : 0.6294221116344763
Loss in iteration 59 : 0.6284056089355885
Loss in iteration 60 : 0.6273924058800184
Loss in iteration 61 : 0.6263824693144625
Loss in iteration 62 : 0.6253757649856996
Loss in iteration 63 : 0.6243722576233541
Loss in iteration 64 : 0.6233719111069737
Loss in iteration 65 : 0.6223746887184484
Loss in iteration 66 : 0.6213805534654893
Loss in iteration 67 : 0.6203894684471366
Loss in iteration 68 : 0.6194013972237408
Loss in iteration 69 : 0.6184163041541714
Loss in iteration 70 : 0.6174341546717994
Loss in iteration 71 : 0.6164549154851751
Loss in iteration 72 : 0.6154785547035507
Loss in iteration 73 : 0.6145050418975496
Loss in iteration 74 : 0.6135343481098231
Loss in iteration 75 : 0.6125664458302517
Loss in iteration 76 : 0.6116013089478357
Loss in iteration 77 : 0.6106389126884997
Loss in iteration 78 : 0.6096792335458747
Loss in iteration 79 : 0.6087222492099575
Loss in iteration 80 : 0.6077679384971209
Loss in iteration 81 : 0.606816281283499
Loss in iteration 82 : 0.6058672584428179
Loss in iteration 83 : 0.6049208517889685
Loss in iteration 84 : 0.6039770440231856
Loss in iteration 85 : 0.6030358186854408
Loss in iteration 86 : 0.6020971601096271
Loss in iteration 87 : 0.6011610533819803
Loss in iteration 88 : 0.6002274843023214
Loss in iteration 89 : 0.5992964393477673
Loss in iteration 90 : 0.5983679056384295
Loss in iteration 91 : 0.5974418709049751
Loss in iteration 92 : 0.5965183234576569
Loss in iteration 93 : 0.5955972521566846
Loss in iteration 94 : 0.5946786463836935
Loss in iteration 95 : 0.5937624960141452
Loss in iteration 96 : 0.5928487913905571
Loss in iteration 97 : 0.5919375232963259
Loss in iteration 98 : 0.5910286829300948
Loss in iteration 99 : 0.5901222618804254
Loss in iteration 100 : 0.5892182521007379
Loss in iteration 101 : 0.5883166458843548
Loss in iteration 102 : 0.5874174358394019
Loss in iteration 103 : 0.5865206148636659
Loss in iteration 104 : 0.5856261761190386
Loss in iteration 105 : 0.5847341130057089
Loss in iteration 106 : 0.5838444191358916
Loss in iteration 107 : 0.5829570883072933
Loss in iteration 108 : 0.5820721144765173
Loss in iteration 109 : 0.5811894917329004
Loss in iteration 110 : 0.5803092142735574
Loss in iteration 111 : 0.5794312763809286
Loss in iteration 112 : 0.5785556724045388
Loss in iteration 113 : 0.5776823967492463
Loss in iteration 114 : 0.5768114438723613
Loss in iteration 115 : 0.5759428082920378
Loss in iteration 116 : 0.5750764846080214
Loss in iteration 117 : 0.5742124675342717
Loss in iteration 118 : 0.5733507519404601
Loss in iteration 119 : 0.5724913328970135
Loss in iteration 120 : 0.5716342057176004
Loss in iteration 121 : 0.5707793659933522
Loss in iteration 122 : 0.5699268096154323
Loss in iteration 123 : 0.569076532785209
Loss in iteration 124 : 0.5682285320135033
Loss in iteration 125 : 0.5673828041116644
Loss in iteration 126 : 0.5665393461773153
Loss in iteration 127 : 0.5656981555773102
Loss in iteration 128 : 0.5648592299296376
Loss in iteration 129 : 0.5640225670854444
Loss in iteration 130 : 0.5631881651117736
Loss in iteration 131 : 0.5623560222753404
Loss in iteration 132 : 0.5615261370274102
Loss in iteration 133 : 0.5606985079897522
Loss in iteration 134 : 0.5598731339415587
Loss in iteration 135 : 0.5590500138073115
Loss in iteration 136 : 0.5582291466454027
Loss in iteration 137 : 0.5574105316374722
Loss in iteration 138 : 0.5565941680783917
Loss in iteration 139 : 0.5557800553667587
Loss in iteration 140 : 0.5549681929959523
Loss in iteration 141 : 0.5541585805455759
Loss in iteration 142 : 0.5533512176733577
Loss in iteration 143 : 0.5525461041073889
Loss in iteration 144 : 0.5517432396387117
Loss in iteration 145 : 0.5509426241142216
Loss in iteration 146 : 0.5501442574298517
Loss in iteration 147 : 0.549348139524017
Loss in iteration 148 : 0.5485542703713274
Loss in iteration 149 : 0.547762649976496
Loss in iteration 150 : 0.5469732783685051
Loss in iteration 151 : 0.5461861555949376
Loss in iteration 152 : 0.5454012817165461
Loss in iteration 153 : 0.5446186568019948
Loss in iteration 154 : 0.543838280922814
Loss in iteration 155 : 0.5430601541485744
Loss in iteration 156 : 0.5422842765422862
Loss in iteration 157 : 0.5415106481560711
Loss in iteration 158 : 0.5407392690271591
Loss in iteration 159 : 0.5399701391742363
Loss in iteration 160 : 0.5392032585942401
Loss in iteration 161 : 0.5384386272596354
Loss in iteration 162 : 0.5376762451162971
Loss in iteration 163 : 0.5369161120819791
Loss in iteration 164 : 0.5361582280453652
Loss in iteration 165 : 0.535402592865724
Loss in iteration 166 : 0.5346492063729482
Loss in iteration 167 : 0.5338980683679033
Loss in iteration 168 : 0.5331491786227651
Loss in iteration 169 : 0.5324025368812696
Loss in iteration 170 : 0.5316581428585718
Loss in iteration 171 : 0.5309159962407234
Loss in iteration 172 : 0.5301760966836945
Loss in iteration 173 : 0.5294384438119903
Loss in iteration 174 : 0.5287030372169462
Loss in iteration 175 : 0.5279698764548258
Loss in iteration 176 : 0.5272389610447541
Loss in iteration 177 : 0.5265102904666583
Loss in iteration 178 : 0.5257838641591901
Loss in iteration 179 : 0.5250596815177029
Loss in iteration 180 : 0.5243377418922975
Loss in iteration 181 : 0.5236180445859466
Loss in iteration 182 : 0.5229005888527296
Loss in iteration 183 : 0.5221853738960786
Loss in iteration 184 : 0.521472398867174
Loss in iteration 185 : 0.5207616628633334
Loss in iteration 186 : 0.5200531649264982
Loss in iteration 187 : 0.5193469040417347
Loss in iteration 188 : 0.5186428791357846
Loss in iteration 189 : 0.5179410890756367
Loss in iteration 190 : 0.5172415326670853
Loss in iteration 191 : 0.51654420865333
Loss in iteration 192 : 0.5158491157135419
Loss in iteration 193 : 0.5151562524614075
Loss in iteration 194 : 0.5144656174436308
Loss in iteration 195 : 0.5137772091384095
Loss in iteration 196 : 0.5130910259538194
Loss in iteration 197 : 0.5124070662261155
Loss in iteration 198 : 0.5117253282179425
Loss in iteration 199 : 0.5110458101164064
Loss in iteration 200 : 0.5103685100309776
Testing accuracy  of updater 4 on alg 0 with rate 0.09999999999999998 = 0.7985995946194951, training accuracy 0.7963452088452089, time elapsed: 4856 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 9.682506281916961
Loss in iteration 3 : 1.8061207798208114
Loss in iteration 4 : 3.754921483208247
Loss in iteration 5 : 3.926788294894692
Loss in iteration 6 : 0.9562615305698622
Loss in iteration 7 : 0.8383314594966591
Loss in iteration 8 : 0.7817432413206891
Loss in iteration 9 : 0.8196903382617693
Loss in iteration 10 : 1.2853440500515152
Loss in iteration 11 : 1.7092266319834253
Loss in iteration 12 : 2.6443856712940854
Loss in iteration 13 : 0.8989133273629202
Loss in iteration 14 : 1.0002743594777594
Loss in iteration 15 : 1.0671801492468667
Loss in iteration 16 : 1.7093931926103791
Loss in iteration 17 : 1.3060997861670345
Loss in iteration 18 : 1.9107196179680854
Loss in iteration 19 : 1.0395452295624676
Loss in iteration 20 : 1.2763452796499606
Loss in iteration 21 : 1.100396699550756
Loss in iteration 22 : 1.575908278429676
Loss in iteration 23 : 1.1718582170102654
Loss in iteration 24 : 1.63506046163849
Loss in iteration 25 : 1.1002272619294329
Loss in iteration 26 : 1.4386717440437908
Loss in iteration 27 : 1.1099230405700797
Loss in iteration 28 : 1.520095262717777
Loss in iteration 29 : 1.1323758951913947
Loss in iteration 30 : 1.5484868918583412
Loss in iteration 31 : 1.1151817833936066
Loss in iteration 32 : 1.4936502652138113
Loss in iteration 33 : 1.1149491999198307
Loss in iteration 34 : 1.5112636185678632
Loss in iteration 35 : 1.122374683837667
Loss in iteration 36 : 1.5242160450402968
Loss in iteration 37 : 1.1186848472731277
Loss in iteration 38 : 1.5091364498258653
Loss in iteration 39 : 1.117829133694196
Loss in iteration 40 : 1.5119238082262292
Loss in iteration 41 : 1.1204288858311136
Loss in iteration 42 : 1.5173318563570477
Loss in iteration 43 : 1.1199395277723636
Loss in iteration 44 : 1.5133149103627486
Loss in iteration 45 : 1.119597442676035
Loss in iteration 46 : 1.5133372071947104
Loss in iteration 47 : 1.1206162086732117
Loss in iteration 48 : 1.5154540793607834
Loss in iteration 49 : 1.1208097283711915
Loss in iteration 50 : 1.514474644479648
Loss in iteration 51 : 1.120806742812173
Loss in iteration 52 : 1.514206774849124
Loss in iteration 53 : 1.1212908741397911
Loss in iteration 54 : 1.5149838621461709
Loss in iteration 55 : 1.1215706432804495
Loss in iteration 56 : 1.5147864897748489
Loss in iteration 57 : 1.1217013067794466
Loss in iteration 58 : 1.514600938713917
Loss in iteration 59 : 1.1219873994318659
Loss in iteration 60 : 1.5148558268476569
Loss in iteration 61 : 1.1222268730025935
Loss in iteration 62 : 1.5148241498521808
Loss in iteration 63 : 1.1223842010173617
Loss in iteration 64 : 1.5147147086815393
Loss in iteration 65 : 1.1225804483386046
Loss in iteration 66 : 1.514772404333631
Loss in iteration 67 : 1.1227643523845454
Loss in iteration 68 : 1.5147550342095806
Loss in iteration 69 : 1.1229058651172383
Loss in iteration 70 : 1.5146828853367367
Loss in iteration 71 : 1.1230494905876531
Loss in iteration 72 : 1.5146687146387587
Loss in iteration 73 : 1.123186175936441
Loss in iteration 74 : 1.5146393288734683
Loss in iteration 75 : 1.1233003410475857
Loss in iteration 76 : 1.5145822556897384
Loss in iteration 77 : 1.123407351009667
Loss in iteration 78 : 1.5145435582653477
Loss in iteration 79 : 1.1235079462059936
Loss in iteration 80 : 1.5145042669857682
Loss in iteration 81 : 1.1235956068855115
Loss in iteration 82 : 1.5144532038141008
Loss in iteration 83 : 1.1236754157935456
Loss in iteration 84 : 1.5144075180117613
Loss in iteration 85 : 1.1237493371706453
Loss in iteration 86 : 1.514363734565048
Loss in iteration 87 : 1.1238151396268987
Loss in iteration 88 : 1.5143157657103987
Loss in iteration 89 : 1.1238745274688333
Loss in iteration 90 : 1.5142694380086847
Loss in iteration 91 : 1.1239289603597997
Loss in iteration 92 : 1.5142248708769481
Loss in iteration 93 : 1.1239779260721787
Loss in iteration 94 : 1.514179319627889
Loss in iteration 95 : 1.1240220865370263
Loss in iteration 96 : 1.5141345528169725
Loss in iteration 97 : 1.1240623360813329
Loss in iteration 98 : 1.5140911193604845
Loss in iteration 99 : 1.1240987420464688
Loss in iteration 100 : 1.514047916682429
Loss in iteration 101 : 1.1241316495077278
Loss in iteration 102 : 1.5140053900330366
Loss in iteration 103 : 1.1241615877766533
Loss in iteration 104 : 1.5139639004871694
Loss in iteration 105 : 1.1241887638434
Loss in iteration 106 : 1.5139230397284105
Loss in iteration 107 : 1.124213410984854
Loss in iteration 108 : 1.5138828716016484
Loss in iteration 109 : 1.1242358517462054
Loss in iteration 110 : 1.5138435658233387
Loss in iteration 111 : 1.1242562876365052
Loss in iteration 112 : 1.5138049769277446
Loss in iteration 113 : 1.124274894211924
Loss in iteration 114 : 1.5137670801424643
Loss in iteration 115 : 1.1242918785812797
Loss in iteration 116 : 1.5137299363581311
Loss in iteration 117 : 1.124307401464178
Loss in iteration 118 : 1.5136934902435895
Loss in iteration 119 : 1.1243215969971454
Loss in iteration 120 : 1.5136577070624673
Loss in iteration 121 : 1.1243346049755767
Loss in iteration 122 : 1.5136225979624858
Loss in iteration 123 : 1.124346545272073
Loss in iteration 124 : 1.5135881357606598
Loss in iteration 125 : 1.124357519178971
Loss in iteration 126 : 1.5135542915673144
Loss in iteration 127 : 1.1243676245429555
Loss in iteration 128 : 1.513521057295938
Loss in iteration 129 : 1.1243769487627324
Loss in iteration 130 : 1.513488414111505
Loss in iteration 131 : 1.1243855673182837
Loss in iteration 132 : 1.513456339476211
Loss in iteration 133 : 1.1243935502471518
Loss in iteration 134 : 1.5134248194849527
Loss in iteration 135 : 1.1244009609495944
Loss in iteration 136 : 1.5133938382090648
Loss in iteration 137 : 1.124407855179416
Loss in iteration 138 : 1.5133633776377513
Loss in iteration 139 : 1.1244142837061324
Loss in iteration 140 : 1.5133334232503493
Loss in iteration 141 : 1.1244202925962208
Loss in iteration 142 : 1.5133039608338994
Loss in iteration 143 : 1.1244259228532112
Loss in iteration 144 : 1.51327497547046
Loss in iteration 145 : 1.124431211533481
Loss in iteration 146 : 1.5132464537510197
Loss in iteration 147 : 1.1244361922246837
Loss in iteration 148 : 1.513218382985303
Loss in iteration 149 : 1.1244408950370803
Loss in iteration 150 : 1.5131907504880602
Loss in iteration 151 : 1.1244453471181308
Loss in iteration 152 : 1.5131635443604476
Loss in iteration 153 : 1.1244495730350228
Loss in iteration 154 : 1.5131367533587718
Loss in iteration 155 : 1.1244535948944945
Loss in iteration 156 : 1.5131103665217644
Loss in iteration 157 : 1.1244574326204877
Loss in iteration 158 : 1.5130843734063082
Loss in iteration 159 : 1.1244611042191737
Loss in iteration 160 : 1.513058764093426
Loss in iteration 161 : 1.124464625920037
Loss in iteration 162 : 1.5130335290152686
Loss in iteration 163 : 1.124468012350679
Loss in iteration 164 : 1.513008659004888
Loss in iteration 165 : 1.1244712767158047
Loss in iteration 166 : 1.5129841453102995
Loss in iteration 167 : 1.1244744309207795
Loss in iteration 168 : 1.512959979516195
Loss in iteration 169 : 1.1244774856931063
Loss in iteration 170 : 1.5129361535393064
Loss in iteration 171 : 1.1244804507052586
Loss in iteration 172 : 1.512912659630714
Loss in iteration 173 : 1.1244833346720458
Loss in iteration 174 : 1.5128894903380425
Loss in iteration 175 : 1.1244861454393282
Loss in iteration 176 : 1.5128666384894487
Loss in iteration 177 : 1.1244888900702599
Loss in iteration 178 : 1.5128440971876658
Loss in iteration 179 : 1.1244915749191122
Loss in iteration 180 : 1.5128218597888503
Loss in iteration 181 : 1.1244942056970388
Loss in iteration 182 : 1.5127999198871023
Loss in iteration 183 : 1.1244967875342355
Loss in iteration 184 : 1.5127782713054576
Loss in iteration 185 : 1.1244993250348916
Loss in iteration 186 : 1.5127569080816088
Loss in iteration 187 : 1.124501822326438
Loss in iteration 188 : 1.5127358244555769
Loss in iteration 189 : 1.1245042831049468
Loss in iteration 190 : 1.5127150148601272
Loss in iteration 191 : 1.1245067106761037
Loss in iteration 192 : 1.5126944739103647
Loss in iteration 193 : 1.124509107992
Loss in iteration 194 : 1.5126741963937613
Loss in iteration 195 : 1.1245114776848593
Loss in iteration 196 : 1.5126541772617037
Loss in iteration 197 : 1.1245138220974682
Loss in iteration 198 : 1.5126344116211823
Loss in iteration 199 : 1.124516143310887
Loss in iteration 200 : 1.5126148947268445
Testing accuracy  of updater 5 on alg 0 with rate 1.0 = 0.772188440513482, training accuracy 0.7750921375921376, time elapsed: 4614 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 2.511389834643855
Loss in iteration 3 : 0.49851801373427884
Loss in iteration 4 : 0.5925863941014985
Loss in iteration 5 : 1.0811739419794388
Loss in iteration 6 : 2.1486543159237406
Loss in iteration 7 : 0.521157113359942
Loss in iteration 8 : 0.562660781797698
Loss in iteration 9 : 0.7335106479543807
Loss in iteration 10 : 1.4072117428176005
Loss in iteration 11 : 0.9182336102912056
Loss in iteration 12 : 1.3947101398771886
Loss in iteration 13 : 0.7048456682390996
Loss in iteration 14 : 0.8515396687508929
Loss in iteration 15 : 0.7644100127365839
Loss in iteration 16 : 1.0813275502992743
Loss in iteration 17 : 0.8264679484274294
Loss in iteration 18 : 1.1355489933440892
Loss in iteration 19 : 0.7757655952176182
Loss in iteration 20 : 0.9922123237816739
Loss in iteration 21 : 0.7843498789087286
Loss in iteration 22 : 1.04315481946346
Loss in iteration 23 : 0.8054888964911594
Loss in iteration 24 : 1.0746214306216895
Loss in iteration 25 : 0.7990505952368191
Loss in iteration 26 : 1.0441081955861342
Loss in iteration 27 : 0.7995561028933089
Loss in iteration 28 : 1.0502642680411598
Loss in iteration 29 : 0.8068910545206343
Loss in iteration 30 : 1.0635025320058495
Loss in iteration 31 : 0.8084838866744785
Loss in iteration 32 : 1.0598426808709798
Loss in iteration 33 : 0.809235063854096
Loss in iteration 34 : 1.0600060004445357
Loss in iteration 35 : 0.8122132943359701
Loss in iteration 36 : 1.0650359290939435
Loss in iteration 37 : 0.8142445249964858
Loss in iteration 38 : 1.066152534701355
Loss in iteration 39 : 0.8153508675214798
Loss in iteration 40 : 1.0664975698831094
Loss in iteration 41 : 0.8169020055732135
Loss in iteration 42 : 1.068514647043678
Loss in iteration 43 : 0.8183480388160691
Loss in iteration 44 : 1.0698474935045004
Loss in iteration 45 : 0.8193538782977304
Loss in iteration 46 : 1.0704601556572968
Loss in iteration 47 : 0.8203325511724381
Loss in iteration 48 : 1.071430326461016
Loss in iteration 49 : 0.8212824669075779
Loss in iteration 50 : 1.072332769697933
Loss in iteration 51 : 0.8220452820671206
Loss in iteration 52 : 1.0728953226504838
Loss in iteration 53 : 0.8227138287268847
Loss in iteration 54 : 1.073450573625723
Loss in iteration 55 : 0.823340561141793
Loss in iteration 56 : 1.073996764749425
Loss in iteration 57 : 0.8238795808441784
Loss in iteration 58 : 1.0744029847583418
Loss in iteration 59 : 0.8243443906583535
Loss in iteration 60 : 1.0747456491780998
Loss in iteration 61 : 0.8247655022117882
Loss in iteration 62 : 1.0750671240423404
Loss in iteration 63 : 0.8251368823421166
Loss in iteration 64 : 1.0753279643318145
Loss in iteration 65 : 0.8254592406656047
Loss in iteration 66 : 1.0755391293954186
Loss in iteration 67 : 0.8257458032086535
Loss in iteration 68 : 1.0757255347455865
Loss in iteration 69 : 0.8259998654826942
Loss in iteration 70 : 1.0758811548386995
Loss in iteration 71 : 0.8262222214038616
Loss in iteration 72 : 1.0760060263567357
Loss in iteration 73 : 0.8264184322313849
Loss in iteration 74 : 1.0761107638474456
Loss in iteration 75 : 0.8265922360246266
Loss in iteration 76 : 1.0761974210730216
Loss in iteration 77 : 0.8267451878373219
Loss in iteration 78 : 1.0762660363642862
Loss in iteration 79 : 0.8268799892870763
Loss in iteration 80 : 1.0763207604354272
Loss in iteration 81 : 0.8269992467205773
Loss in iteration 82 : 1.0763641507445505
Loss in iteration 83 : 0.8271045325319528
Loss in iteration 84 : 1.0763969373350122
Loss in iteration 85 : 0.827197448610088
Loss in iteration 86 : 1.0764208831568405
Loss in iteration 87 : 0.8272796467027281
Loss in iteration 88 : 1.0764376854116857
Loss in iteration 89 : 0.8273523680880558
Loss in iteration 90 : 1.076448184628193
Loss in iteration 91 : 0.8274166835273236
Loss in iteration 92 : 1.0764532896947265
Loss in iteration 93 : 0.8274736418093485
Loss in iteration 94 : 1.0764539842764438
Loss in iteration 95 : 0.827524125035701
Loss in iteration 96 : 1.0764509298440723
Loss in iteration 97 : 0.8275688741555757
Loss in iteration 98 : 1.076444676113787
Loss in iteration 99 : 0.8276085731781859
Loss in iteration 100 : 1.076435785272111
Loss in iteration 101 : 0.8276438236969809
Loss in iteration 102 : 1.0764247040662054
Loss in iteration 103 : 0.8276751378890622
Loss in iteration 104 : 1.0764117854632773
Loss in iteration 105 : 0.8277029725016537
Loss in iteration 106 : 1.076397359233736
Loss in iteration 107 : 0.8277277335465658
Loss in iteration 108 : 1.0763817072849262
Loss in iteration 109 : 0.8277497727046581
Loss in iteration 110 : 1.0763650560061575
Loss in iteration 111 : 0.8277693994906725
Loss in iteration 112 : 1.0763476039239415
Loss in iteration 113 : 0.8277868887270935
Loss in iteration 114 : 1.0763295237701471
Loss in iteration 115 : 0.8278024812741392
Loss in iteration 116 : 1.0763109577453274
Loss in iteration 117 : 0.8278163887651806
Loss in iteration 118 : 1.0762920265919598
Loss in iteration 119 : 0.8278287986707373
Loss in iteration 120 : 1.0762728345918584
Loss in iteration 121 : 0.8278398763792544
Loss in iteration 122 : 1.0762534688807037
Loss in iteration 123 : 0.8278497676269435
Loss in iteration 124 : 1.0762340023917576
Loss in iteration 125 : 0.8278586014600784
Loss in iteration 126 : 1.0762144971557002
Loss in iteration 127 : 0.8278664921617654
Loss in iteration 128 : 1.0761950051127636
Loss in iteration 129 : 0.8278735408350417
Loss in iteration 130 : 1.0761755693408295
Loss in iteration 131 : 0.8278798371456125
Loss in iteration 132 : 1.0761562258251782
Loss in iteration 133 : 0.8278854607337993
Loss in iteration 134 : 1.0761370044045457
Loss in iteration 135 : 0.8278904823295111
Loss in iteration 136 : 1.0761179294968128
Loss in iteration 137 : 0.8278949648343065
Loss in iteration 138 : 1.076099021029181
Loss in iteration 139 : 0.8278989642777953
Loss in iteration 140 : 1.0760802951380926
Loss in iteration 141 : 0.8279025306005788
Loss in iteration 142 : 1.0760617646663255
Loss in iteration 143 : 0.8279057083600216
Loss in iteration 144 : 1.0760434396801168
Loss in iteration 145 : 0.8279085373660385
Loss in iteration 146 : 1.0760253279199155
Loss in iteration 147 : 0.8279110532217596
Loss in iteration 148 : 1.0760074351393538
Loss in iteration 149 : 0.8279132877976269
Loss in iteration 150 : 1.075989765412191
Loss in iteration 151 : 0.8279152696561458
Loss in iteration 152 : 1.0759723214096066
Loss in iteration 153 : 0.8279170244218641
Loss in iteration 154 : 1.0759551046224294
Loss in iteration 155 : 0.8279185751045492
Loss in iteration 156 : 1.0759381155506178
Loss in iteration 157 : 0.8279199423864735
Loss in iteration 158 : 1.0759213538720473
Loss in iteration 159 : 0.827921144875649
Loss in iteration 160 : 1.0759048185835256
Loss in iteration 161 : 0.8279221993280266
Loss in iteration 162 : 1.0758885081188658
Loss in iteration 163 : 0.8279231208443865
Loss in iteration 164 : 1.0758724204517616
Loss in iteration 165 : 0.8279239230447984
Loss in iteration 166 : 1.0758565531830522
Loss in iteration 167 : 0.8279246182226142
Loss in iteration 168 : 1.0758409036139112
Loss in iteration 169 : 0.827925217481084
Loss in iteration 170 : 1.0758254688082292
Loss in iteration 171 : 0.8279257308547405
Loss in iteration 172 : 1.0758102456457537
Loss in iteration 173 : 0.827926167417177
Loss in iteration 174 : 1.0757952308666312
Loss in iteration 175 : 0.8279265353768798
Loss in iteration 176 : 1.0757804211088648
Loss in iteration 177 : 0.8279268421627505
Loss in iteration 178 : 1.0757658129399983
Loss in iteration 179 : 0.8279270945002951
Loss in iteration 180 : 1.0757514028835855
Loss in iteration 181 : 0.8279272984797393
Loss in iteration 182 : 1.0757371874412012
Loss in iteration 183 : 0.827927459616897
Loss in iteration 184 : 1.0757231631107735
Loss in iteration 185 : 0.8279275829077997
Loss in iteration 186 : 1.075709326401746
Loss in iteration 187 : 0.8279276728775693
Loss in iteration 188 : 1.0756956738475665
Loss in iteration 189 : 0.8279277336244366
Loss in iteration 190 : 1.0756822020157737
Loss in iteration 191 : 0.8279277688592083
Loss in iteration 192 : 1.0756689075161923
Loss in iteration 193 : 0.8279277819409153
Loss in iteration 194 : 1.0756557870075392
Loss in iteration 195 : 0.8279277759089044
Loss in iteration 196 : 1.075642837202483
Loss in iteration 197 : 0.827927753511811
Loss in iteration 198 : 1.0756300548716764
Loss in iteration 199 : 0.8279277172337864
Loss in iteration 200 : 1.0756174368466953
Testing accuracy  of updater 5 on alg 0 with rate 0.7000000000000001 = 0.7756280326761256, training accuracy 0.7785626535626535, time elapsed: 4979 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 1.4649773151935617
Loss in iteration 3 : 0.3903355209490202
Loss in iteration 4 : 0.4367531573892701
Loss in iteration 5 : 0.569700081898828
Loss in iteration 6 : 0.9400923137893868
Loss in iteration 7 : 0.5586358234287743
Loss in iteration 8 : 0.7104810416643862
Loss in iteration 9 : 0.5285080350971636
Loss in iteration 10 : 0.6225182582387786
Loss in iteration 11 : 0.5160668771472363
Loss in iteration 12 : 0.6106896603201176
Loss in iteration 13 : 0.5245046836535322
Loss in iteration 14 : 0.6318560880799037
Loss in iteration 15 : 0.5318870561254069
Loss in iteration 16 : 0.6389888855333932
Loss in iteration 17 : 0.5309789012573674
Loss in iteration 18 : 0.6324417258479873
Loss in iteration 19 : 0.531017042337694
Loss in iteration 20 : 0.6328899267199369
Loss in iteration 21 : 0.5343308315088875
Loss in iteration 22 : 0.6387926125294962
Loss in iteration 23 : 0.5372166485410788
Loss in iteration 24 : 0.6418358230330184
Loss in iteration 25 : 0.538504842132321
Loss in iteration 26 : 0.6423452365423155
Loss in iteration 27 : 0.5397899335212255
Loss in iteration 28 : 0.64393442450713
Loss in iteration 29 : 0.5415323977893439
Loss in iteration 30 : 0.6463291080500795
Loss in iteration 31 : 0.5430288178682054
Loss in iteration 32 : 0.6479515534919089
Loss in iteration 33 : 0.5441027291948737
Loss in iteration 34 : 0.6489839968728855
Loss in iteration 35 : 0.5450918369496321
Loss in iteration 36 : 0.6501475495645908
Loss in iteration 37 : 0.5460772692500852
Loss in iteration 38 : 0.6513420678231207
Loss in iteration 39 : 0.5469194692229077
Loss in iteration 40 : 0.6522655285556799
Loss in iteration 41 : 0.5476069536102431
Loss in iteration 42 : 0.6530027124100413
Loss in iteration 43 : 0.5482216862648946
Loss in iteration 44 : 0.6537067573369943
Loss in iteration 45 : 0.5487796455341586
Loss in iteration 46 : 0.654345848780842
Loss in iteration 47 : 0.5492568478226014
Loss in iteration 48 : 0.6548680559717585
Loss in iteration 49 : 0.5496616393531051
Loss in iteration 50 : 0.655307625445187
Loss in iteration 51 : 0.5500173593863591
Loss in iteration 52 : 0.6557001246862372
Loss in iteration 53 : 0.5503298336480279
Loss in iteration 54 : 0.6560394071944842
Loss in iteration 55 : 0.5505980812447032
Loss in iteration 56 : 0.6563210388066482
Loss in iteration 57 : 0.5508287438549664
Loss in iteration 58 : 0.6565589461513531
Loss in iteration 59 : 0.551029884945205
Loss in iteration 60 : 0.6567635577048387
Loss in iteration 61 : 0.5512048049792833
Loss in iteration 62 : 0.6569359298150594
Loss in iteration 63 : 0.5513556169052579
Loss in iteration 64 : 0.6570786598552216
Loss in iteration 65 : 0.5514859549213631
Loss in iteration 66 : 0.6571977274048963
Loss in iteration 67 : 0.5515991412549452
Loss in iteration 68 : 0.6572972310363379
Loss in iteration 69 : 0.5516971747581149
Loss in iteration 70 : 0.6573790919042485
Loss in iteration 71 : 0.5517817850177552
Loss in iteration 72 : 0.6574456560987184
Loss in iteration 73 : 0.5518548889073078
Loss in iteration 74 : 0.657499660750754
Loss in iteration 75 : 0.551918111615737
Loss in iteration 76 : 0.6575430726248459
Loss in iteration 77 : 0.5519726710542033
Loss in iteration 78 : 0.6575772818512919
Loss in iteration 79 : 0.5520196631534812
Loss in iteration 80 : 0.6576036935085993
Loss in iteration 81 : 0.5520601285537065
Loss in iteration 82 : 0.6576236304293614
Loss in iteration 83 : 0.5520949443588231
Loss in iteration 84 : 0.6576381086068153
Loss in iteration 85 : 0.5521248324107855
Loss in iteration 86 : 0.6576479473075825
Loss in iteration 87 : 0.5521504355662873
Loss in iteration 88 : 0.6576539019350248
Loss in iteration 89 : 0.5521723285863687
Loss in iteration 90 : 0.6576566308322621
Loss in iteration 91 : 0.5521910002727569
Loss in iteration 92 : 0.6576566634637538
Loss in iteration 93 : 0.5522068671259822
Loss in iteration 94 : 0.6576544437944648
Loss in iteration 95 : 0.5522202954476633
Loss in iteration 96 : 0.6576503622868687
Loss in iteration 97 : 0.5522316056975528
Loss in iteration 98 : 0.6576447502712737
Loss in iteration 99 : 0.5522410729015791
Loss in iteration 100 : 0.657637880873024
Loss in iteration 101 : 0.5522489344335261
Loss in iteration 102 : 0.6576299849823782
Loss in iteration 103 : 0.5522553976586596
Loss in iteration 104 : 0.6576212606767635
Loss in iteration 105 : 0.5522606429387633
Loss in iteration 106 : 0.6576118744400061
Loss in iteration 107 : 0.5522648261645303
Loss in iteration 108 : 0.6576019648673477
Loss in iteration 109 : 0.5522680826783642
Loss in iteration 110 : 0.6575916488012922
Loss in iteration 111 : 0.5522705305298284
Loss in iteration 112 : 0.6575810249853613
Loss in iteration 113 : 0.5522722724754999
Loss in iteration 114 : 0.6575701758986827
Loss in iteration 115 : 0.5522733978496039
Loss in iteration 116 : 0.6575591701826352
Loss in iteration 117 : 0.5522739845518048
Loss in iteration 118 : 0.6575480652214252
Loss in iteration 119 : 0.5522741006571218
Loss in iteration 120 : 0.6575369088670077
Loss in iteration 121 : 0.5522738056368439
Loss in iteration 122 : 0.6575257406994901
Loss in iteration 123 : 0.5522731514782268
Loss in iteration 124 : 0.6575145933380672
Loss in iteration 125 : 0.552272183724019
Loss in iteration 126 : 0.6575034936260213
Loss in iteration 127 : 0.5522709423302666
Loss in iteration 128 : 0.6574924635129139
Loss in iteration 129 : 0.5522694623751436
Loss in iteration 130 : 0.6574815207793463
Loss in iteration 131 : 0.5522677746924783
Loss in iteration 132 : 0.6574706797119321
Loss in iteration 133 : 0.5522659064316348
Loss in iteration 134 : 0.6574599516802331
Loss in iteration 135 : 0.5522638815295514
Loss in iteration 136 : 0.657449345595905
Loss in iteration 137 : 0.5522617211137579
Loss in iteration 138 : 0.6574388683019766
Loss in iteration 139 : 0.5522594438569944
Loss in iteration 140 : 0.6574285249163127
Loss in iteration 141 : 0.5522570662859149
Loss in iteration 142 : 0.6574183191206058
Loss in iteration 143 : 0.5522546030454694
Loss in iteration 144 : 0.6574082533982363
Loss in iteration 145 : 0.5522520671275236
Loss in iteration 146 : 0.6573983292366739
Loss in iteration 147 : 0.5522494700708267
Loss in iteration 148 : 0.6573885473013872
Loss in iteration 149 : 0.5522468221348096
Loss in iteration 150 : 0.6573789075816555
Loss in iteration 151 : 0.5522441324500867
Loss in iteration 152 : 0.6573694095122224
Loss in iteration 153 : 0.552241409149454
Loss in iteration 154 : 0.6573600520762977
Loss in iteration 155 : 0.5522386594825412
Loss in iteration 156 : 0.6573508338928525
Loss in iteration 157 : 0.5522358899158555
Loss in iteration 158 : 0.6573417532896011
Loss in iteration 159 : 0.5522331062201423
Loss in iteration 160 : 0.6573328083640599
Loss in iteration 161 : 0.5522303135469103
Loss in iteration 162 : 0.657323997034757
Loss in iteration 163 : 0.5522275164956792
Loss in iteration 164 : 0.6573153170842538
Loss in iteration 165 : 0.5522247191730397
Loss in iteration 166 : 0.6573067661948654
Loss in iteration 167 : 0.5522219252446644
Loss in iteration 168 : 0.657298341978385
Loss in iteration 169 : 0.5522191379811215
Loss in iteration 170 : 0.6572900420006782
Loss in iteration 171 : 0.5522163602984786
Loss in iteration 172 : 0.6572818638021309
Loss in iteration 173 : 0.5522135947941721
Loss in iteration 174 : 0.6572738049143815
Loss in iteration 175 : 0.5522108437788654
Loss in iteration 176 : 0.6572658628739908
Loss in iteration 177 : 0.5522081093047629
Loss in iteration 178 : 0.6572580352336226
Loss in iteration 179 : 0.5522053931908397
Loss in iteration 180 : 0.6572503195709654
Loss in iteration 181 : 0.5522026970453634
Loss in iteration 182 : 0.6572427134959608
Loss in iteration 183 : 0.5522000222860385
Loss in iteration 184 : 0.6572352146564502
Loss in iteration 185 : 0.552197370158035
Loss in iteration 186 : 0.6572278207425345
Loss in iteration 187 : 0.5521947417502243
Loss in iteration 188 : 0.6572205294899166
Loss in iteration 189 : 0.5521921380097171
Loss in iteration 190 : 0.6572133386823685
Loss in iteration 191 : 0.5521895597550711
Loss in iteration 192 : 0.6572062461534295
Loss in iteration 193 : 0.5521870076880965
Loss in iteration 194 : 0.6571992497875373
Loss in iteration 195 : 0.5521844824046089
Loss in iteration 196 : 0.6571923475206266
Loss in iteration 197 : 0.5521819844041772
Loss in iteration 198 : 0.6571855373403417
Loss in iteration 199 : 0.5521795140989114
Loss in iteration 200 : 0.6571788172859372
Testing accuracy  of updater 5 on alg 0 with rate 0.4 = 0.7853940175664885, training accuracy 0.7887592137592138, time elapsed: 4807 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5114044074976687
Loss in iteration 3 : 0.40397775930957763
Loss in iteration 4 : 0.3826548765099336
Loss in iteration 5 : 0.36909548310400786
Loss in iteration 6 : 0.3597234995248938
Loss in iteration 7 : 0.3528914206277257
Loss in iteration 8 : 0.34771559931279816
Loss in iteration 9 : 0.34368219708581593
Loss in iteration 10 : 0.34047030582999477
Loss in iteration 11 : 0.3378696935909674
Loss in iteration 12 : 0.33573638256567967
Loss in iteration 13 : 0.3339683333755702
Loss in iteration 14 : 0.3324909289176725
Loss in iteration 15 : 0.33124820681411177
Loss in iteration 16 : 0.3301972252617796
Loss in iteration 17 : 0.3293044144406612
Loss in iteration 18 : 0.3285430882614344
Loss in iteration 19 : 0.3278917465825361
Loss in iteration 20 : 0.3273328553452798
Loss in iteration 21 : 0.3268519818988352
Loss in iteration 22 : 0.32643714040233524
Loss in iteration 23 : 0.3260783199488526
Loss in iteration 24 : 0.32576710251189506
Loss in iteration 25 : 0.32549640397288204
Loss in iteration 26 : 0.3252602537114289
Loss in iteration 27 : 0.3250538523909022
Loss in iteration 28 : 0.3248745041689749
Loss in iteration 29 : 0.3247298073992575
Loss in iteration 30 : 0.32470151655773793
Loss in iteration 31 : 0.32551448247704184
Loss in iteration 32 : 0.333965646097888
Loss in iteration 33 : 0.4016408166000759
Loss in iteration 34 : 0.5210809065050547
Loss in iteration 35 : 0.3692234973147125
Loss in iteration 36 : 0.35682022971248484
Loss in iteration 37 : 0.3380333822464733
Loss in iteration 38 : 0.33460338601016126
Loss in iteration 39 : 0.3312747012206271
Loss in iteration 40 : 0.33135347716947083
Loss in iteration 41 : 0.331380126422897
Loss in iteration 42 : 0.3342462933022837
Loss in iteration 43 : 0.33735645608253456
Loss in iteration 44 : 0.34638302464854537
Loss in iteration 45 : 0.3524391446748849
Loss in iteration 46 : 0.36687509399586915
Loss in iteration 47 : 0.35970339175101085
Loss in iteration 48 : 0.3638546917661188
Loss in iteration 49 : 0.34893560160193277
Loss in iteration 50 : 0.34827860188168164
Loss in iteration 51 : 0.3408310795723505
Loss in iteration 52 : 0.3416342066329261
Loss in iteration 53 : 0.33890636601604085
Loss in iteration 54 : 0.3420325482010223
Loss in iteration 55 : 0.3416094657450181
Loss in iteration 56 : 0.3474701121154199
Loss in iteration 57 : 0.3470890184930594
Loss in iteration 58 : 0.3542421119733356
Loss in iteration 59 : 0.3501616981743241
Loss in iteration 60 : 0.3550829941855654
Loss in iteration 61 : 0.3478775535683837
Loss in iteration 62 : 0.35049147380690715
Loss in iteration 63 : 0.344364307234166
Loss in iteration 64 : 0.3468170210175294
Loss in iteration 65 : 0.3428654036527973
Loss in iteration 66 : 0.3463083567754993
Loss in iteration 67 : 0.3436997230915938
Loss in iteration 68 : 0.34832737969297367
Loss in iteration 69 : 0.345724492335333
Loss in iteration 70 : 0.35084502738904777
Loss in iteration 71 : 0.3469962804484118
Loss in iteration 72 : 0.3515103948230112
Loss in iteration 73 : 0.34650571645147854
Loss in iteration 74 : 0.3501613555748292
Loss in iteration 75 : 0.34521384200912525
Loss in iteration 76 : 0.34861783510246036
Loss in iteration 77 : 0.344443106655113
Loss in iteration 78 : 0.3481700356611513
Loss in iteration 79 : 0.34462304186456005
Loss in iteration 80 : 0.34882766905365586
Loss in iteration 81 : 0.3453490884125944
Loss in iteration 82 : 0.34977686064066343
Loss in iteration 83 : 0.3458755773268613
Loss in iteration 84 : 0.350133774360205
Loss in iteration 85 : 0.34579118151749944
Loss in iteration 86 : 0.34973660879959834
Loss in iteration 87 : 0.3453334284089438
Loss in iteration 88 : 0.3491361939446695
Loss in iteration 89 : 0.3449873991916225
Loss in iteration 90 : 0.3488803253421412
Loss in iteration 91 : 0.34499415560372837
Loss in iteration 92 : 0.34906853216158407
Loss in iteration 93 : 0.3452426304740393
Loss in iteration 94 : 0.34941828350382814
Loss in iteration 95 : 0.34545461922520665
Loss in iteration 96 : 0.3495867695068895
Loss in iteration 97 : 0.3454540457839946
Loss in iteration 98 : 0.3494733225367778
Loss in iteration 99 : 0.3452935859726793
Loss in iteration 100 : 0.3492456795878876
Loss in iteration 101 : 0.34514584048923796
Loss in iteration 102 : 0.34911850366584
Loss in iteration 103 : 0.34512242323019887
Loss in iteration 104 : 0.3491609774335421
Loss in iteration 105 : 0.34520122212454235
Loss in iteration 106 : 0.3492839767060392
Loss in iteration 107 : 0.3452821512634229
Loss in iteration 108 : 0.3493562148719995
Loss in iteration 109 : 0.34529062528643134
Loss in iteration 110 : 0.34932459496797086
Loss in iteration 111 : 0.3452335041767825
Loss in iteration 112 : 0.3492381119510233
Loss in iteration 113 : 0.3451707649313158
Loss in iteration 114 : 0.3491778474237155
Loss in iteration 115 : 0.34515012128284334
Loss in iteration 116 : 0.34918017645895255
Loss in iteration 117 : 0.34517108944977326
Loss in iteration 118 : 0.34921960492890464
Loss in iteration 119 : 0.34519896728355837
Loss in iteration 120 : 0.3492470682237075
Loss in iteration 121 : 0.3452030194632754
Loss in iteration 122 : 0.3492371532098062
Loss in iteration 123 : 0.3451810373689116
Loss in iteration 124 : 0.349202848179567
Loss in iteration 125 : 0.3451532594709563
Loss in iteration 126 : 0.3491739476771212
Loss in iteration 127 : 0.3451391633023176
Loss in iteration 128 : 0.34916748101578027
Loss in iteration 129 : 0.3451414994366123
Loss in iteration 130 : 0.34917718995993213
Loss in iteration 131 : 0.3451487039694908
Loss in iteration 132 : 0.34918523158948156
Loss in iteration 133 : 0.3451484825274189
Loss in iteration 134 : 0.349180221812732
Loss in iteration 135 : 0.34513824668876036
Loss in iteration 136 : 0.34916492572207014
Loss in iteration 137 : 0.3451245648219309
Loss in iteration 138 : 0.34914999926251566
Loss in iteration 139 : 0.3451151232222861
Loss in iteration 140 : 0.3491429007141629
Loss in iteration 141 : 0.3451120174168753
Loss in iteration 142 : 0.34914258603641646
Loss in iteration 143 : 0.3451115741897926
Loss in iteration 144 : 0.34914275237747316
Loss in iteration 145 : 0.34510901295299634
Loss in iteration 146 : 0.34913854289172597
Loss in iteration 147 : 0.34510272989810054
Loss in iteration 148 : 0.349130183670115
Loss in iteration 149 : 0.345094744478034
Loss in iteration 150 : 0.3491213735538807
Loss in iteration 151 : 0.3450880031218578
Loss in iteration 152 : 0.3491152316181137
Loss in iteration 153 : 0.34508366650267464
Loss in iteration 154 : 0.34911186288923485
Loss in iteration 155 : 0.34508064181105985
Loss in iteration 156 : 0.34910911161766467
Loss in iteration 157 : 0.3450771192348848
Loss in iteration 158 : 0.34910498353436226
Loss in iteration 159 : 0.34507227594086304
Loss in iteration 160 : 0.3490992580367616
Loss in iteration 161 : 0.34506668337266766
Loss in iteration 162 : 0.34909317433490383
Loss in iteration 163 : 0.3450614403498452
Loss in iteration 164 : 0.349087990317733
Loss in iteration 165 : 0.34505710977433024
Loss in iteration 166 : 0.34908392906261865
Loss in iteration 167 : 0.34505339859776757
Loss in iteration 168 : 0.34908028077151865
Loss in iteration 169 : 0.3450496392071068
Loss in iteration 170 : 0.349076252147142
Loss in iteration 171 : 0.34504544796311837
Loss in iteration 172 : 0.3490716480688689
Loss in iteration 173 : 0.34504096362197173
Loss in iteration 174 : 0.3490668652395532
Loss in iteration 175 : 0.3450365845286823
Loss in iteration 176 : 0.3490623952159071
Loss in iteration 177 : 0.34503256331912346
Loss in iteration 178 : 0.34905838748893364
Loss in iteration 179 : 0.3450288361717312
Loss in iteration 180 : 0.34905461930308757
Loss in iteration 181 : 0.3450251630657337
Loss in iteration 182 : 0.3490507836070412
Loss in iteration 183 : 0.3450213754698214
Loss in iteration 184 : 0.34904676673002255
Loss in iteration 185 : 0.345017495403662
Loss in iteration 186 : 0.34904268764722757
Loss in iteration 187 : 0.34501366274009454
Loss in iteration 188 : 0.3490387327602847
Loss in iteration 189 : 0.34500998480633166
Loss in iteration 190 : 0.3490349807928459
Loss in iteration 191 : 0.34500645512218153
Loss in iteration 192 : 0.34903136623673237
Loss in iteration 193 : 0.34500298955351744
Loss in iteration 194 : 0.34902777303431953
Loss in iteration 195 : 0.34499951699674414
Loss in iteration 196 : 0.3490241441749596
Loss in iteration 197 : 0.34499603391862566
Loss in iteration 198 : 0.34902051158421654
Loss in iteration 199 : 0.3449925874740429
Loss in iteration 200 : 0.3490169437062803
Testing accuracy  of updater 5 on alg 0 with rate 0.1 = 0.8329340949573122, training accuracy 0.8372542997542998, time elapsed: 5901 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5445447469755056
Loss in iteration 3 : 0.4168181568863177
Loss in iteration 4 : 0.3915035838937695
Loss in iteration 5 : 0.3760810999891878
Loss in iteration 6 : 0.36563068038783486
Loss in iteration 7 : 0.3580681966816433
Loss in iteration 8 : 0.35233542671016904
Loss in iteration 9 : 0.3478566531673333
Loss in iteration 10 : 0.34427338864396034
Loss in iteration 11 : 0.3413560155168711
Loss in iteration 12 : 0.3389462431224801
Loss in iteration 13 : 0.3369330706152572
Loss in iteration 14 : 0.33523491552174306
Loss in iteration 15 : 0.33379119348732533
Loss in iteration 16 : 0.33255530534824673
Loss in iteration 17 : 0.3314912731800876
Loss in iteration 18 : 0.3305704479130538
Loss in iteration 19 : 0.329770155488079
Loss in iteration 20 : 0.3290718926319238
Loss in iteration 21 : 0.3284611954099055
Loss in iteration 22 : 0.3279272135666093
Loss in iteration 23 : 0.32746731183876904
Loss in iteration 24 : 0.3271054136299543
Loss in iteration 25 : 0.32699684051746764
Loss in iteration 26 : 0.3280275936674878
Loss in iteration 27 : 0.3350431824513913
Loss in iteration 28 : 0.3660792422561301
Loss in iteration 29 : 0.3897021492870133
Loss in iteration 30 : 0.37540384289054896
Loss in iteration 31 : 0.337224510478259
Loss in iteration 32 : 0.3309123965855085
Loss in iteration 33 : 0.3275656379157502
Loss in iteration 34 : 0.3265517967236087
Loss in iteration 35 : 0.3259733059383995
Loss in iteration 36 : 0.3258381816163038
Loss in iteration 37 : 0.32587192093512024
Loss in iteration 38 : 0.32633946433373356
Loss in iteration 39 : 0.3272941430924089
Loss in iteration 40 : 0.3298030030418897
Loss in iteration 41 : 0.3342462337178254
Loss in iteration 42 : 0.34368803377858104
Loss in iteration 43 : 0.349346890869681
Loss in iteration 44 : 0.35482835979216004
Loss in iteration 45 : 0.3429801856127799
Loss in iteration 46 : 0.33955928793077067
Loss in iteration 47 : 0.333062180291106
Loss in iteration 48 : 0.3315634802790985
Loss in iteration 49 : 0.32964887151140926
Loss in iteration 50 : 0.32976358456005705
Loss in iteration 51 : 0.32961520708238573
Loss in iteration 52 : 0.3311291434256942
Loss in iteration 53 : 0.332290226843646
Loss in iteration 54 : 0.3358112286260308
Loss in iteration 55 : 0.3374440078919439
Loss in iteration 56 : 0.34172307144262715
Loss in iteration 57 : 0.33991348859973464
Loss in iteration 58 : 0.3412705679698114
Loss in iteration 59 : 0.3367662080463776
Loss in iteration 60 : 0.33648590748117524
Loss in iteration 61 : 0.3333900868989881
Loss in iteration 62 : 0.3335508090065271
Loss in iteration 63 : 0.3321415634872388
Loss in iteration 64 : 0.3332021221595223
Loss in iteration 65 : 0.3328320366256445
Loss in iteration 66 : 0.33486670759787174
Loss in iteration 67 : 0.3348257838509348
Loss in iteration 68 : 0.3374157055422802
Loss in iteration 69 : 0.33652847420505816
Loss in iteration 70 : 0.3385463020367192
Loss in iteration 71 : 0.3362997015280035
Loss in iteration 72 : 0.3373367452771229
Loss in iteration 73 : 0.33483810101943745
Loss in iteration 74 : 0.3355821837258069
Loss in iteration 75 : 0.3337301012961765
Loss in iteration 76 : 0.33476840549527737
Loss in iteration 77 : 0.33359701508560163
Loss in iteration 78 : 0.33511840225889594
Loss in iteration 79 : 0.3342761847887029
Loss in iteration 80 : 0.3361513466362952
Loss in iteration 81 : 0.33513042285252703
Loss in iteration 82 : 0.33696123333432726
Loss in iteration 83 : 0.3354155493941744
Loss in iteration 84 : 0.33688367672864916
Loss in iteration 85 : 0.33499900773168334
Loss in iteration 86 : 0.33618854909512497
Loss in iteration 87 : 0.33440264410433845
Loss in iteration 88 : 0.3355952594775722
Loss in iteration 89 : 0.3341194843284924
Loss in iteration 90 : 0.3355012315494149
Loss in iteration 91 : 0.33426331849782454
Loss in iteration 92 : 0.33584277654160183
Loss in iteration 93 : 0.3346249337320597
Loss in iteration 94 : 0.33626397364589417
Loss in iteration 95 : 0.33487008406254065
Loss in iteration 96 : 0.3364054608494002
Loss in iteration 97 : 0.33481768723930155
Loss in iteration 98 : 0.33620719324285797
Loss in iteration 99 : 0.3345739706951009
Loss in iteration 100 : 0.3359047158749647
Loss in iteration 101 : 0.33437056306984997
Loss in iteration 102 : 0.3357499532326184
Loss in iteration 103 : 0.33434379107850287
Loss in iteration 104 : 0.3358149209885847
Loss in iteration 105 : 0.3344644191000507
Loss in iteration 106 : 0.3359921430308287
Loss in iteration 107 : 0.3345994383492756
Loss in iteration 108 : 0.3361120962995501
Loss in iteration 109 : 0.33463281108050447
Loss in iteration 110 : 0.33608627154086385
Loss in iteration 111 : 0.3345553323428072
Loss in iteration 112 : 0.3359631640700588
Loss in iteration 113 : 0.3344481770365686
Loss in iteration 114 : 0.33585602375361384
Loss in iteration 115 : 0.3343949933129661
Loss in iteration 116 : 0.3358368091222688
Loss in iteration 117 : 0.33441729623263317
Loss in iteration 118 : 0.3358929907648222
Loss in iteration 119 : 0.334474155336632
Loss in iteration 120 : 0.3359575786218223
Loss in iteration 121 : 0.33450751936696355
Loss in iteration 122 : 0.3359723825596929
Loss in iteration 123 : 0.33449108817831713
Loss in iteration 124 : 0.33593179529776607
Loss in iteration 125 : 0.33444395801896193
Loss in iteration 126 : 0.3358750901185395
Loss in iteration 127 : 0.33440461745361305
Loss in iteration 128 : 0.33584451669133686
Loss in iteration 129 : 0.3343961967669189
Loss in iteration 130 : 0.33585196909871184
Loss in iteration 131 : 0.33441258023475123
Loss in iteration 132 : 0.33587738290728136
Loss in iteration 133 : 0.3344300888107761
Loss in iteration 134 : 0.33589163763587665
Loss in iteration 135 : 0.3344298196746288
Loss in iteration 136 : 0.33588121238680835
Loss in iteration 137 : 0.33441127603022947
Loss in iteration 138 : 0.335855126986112
Loss in iteration 139 : 0.3343883978811247
Loss in iteration 140 : 0.33583249903339707
Loss in iteration 141 : 0.33437521196052056
Loss in iteration 142 : 0.33582524169297656
Loss in iteration 143 : 0.33437496767995695
Loss in iteration 144 : 0.3358306301627603
Loss in iteration 145 : 0.33438023035998266
Loss in iteration 146 : 0.33583690378964115
Loss in iteration 147 : 0.33438115066221374
Loss in iteration 148 : 0.33583448724927883
Loss in iteration 149 : 0.33437366767514826
Loss in iteration 150 : 0.3358229112770665
Loss in iteration 151 : 0.3343612949223355
Loss in iteration 152 : 0.33580901186266615
Loss in iteration 153 : 0.33435057511311883
Loss in iteration 154 : 0.335799864645097
Loss in iteration 155 : 0.3343452773591166
Loss in iteration 156 : 0.3357972473618852
Loss in iteration 157 : 0.33434414736103685
Loss in iteration 158 : 0.335797534890208
Loss in iteration 159 : 0.3343430746680013
Loss in iteration 160 : 0.3357957876377491
Loss in iteration 161 : 0.33433895210337367
Loss in iteration 162 : 0.335789889502201
Loss in iteration 163 : 0.33433184584756337
Loss in iteration 164 : 0.3357815169129031
Loss in iteration 165 : 0.33432418558630567
Loss in iteration 166 : 0.3357739300915197
Loss in iteration 167 : 0.3343183128834971
Loss in iteration 168 : 0.3357690648482899
Loss in iteration 169 : 0.334314687002414
Loss in iteration 170 : 0.3357663493323801
Loss in iteration 171 : 0.33431197776555366
Loss in iteration 172 : 0.33576374389150165
Loss in iteration 173 : 0.334308520661208
Loss in iteration 174 : 0.3357596690344557
Loss in iteration 175 : 0.3343036849021504
Loss in iteration 176 : 0.33575411870322325
Loss in iteration 177 : 0.33429811459707315
Loss in iteration 178 : 0.3357482880847707
Loss in iteration 179 : 0.3342929234613461
Loss in iteration 180 : 0.33574335858985077
Loss in iteration 181 : 0.3342887181278459
Loss in iteration 182 : 0.33573958443816704
Loss in iteration 183 : 0.3342852480951563
Loss in iteration 184 : 0.33573631591086434
Loss in iteration 185 : 0.33428180308136796
Loss in iteration 186 : 0.33573271703035884
Loss in iteration 187 : 0.33427787177089924
Loss in iteration 188 : 0.33572845651993327
Loss in iteration 189 : 0.3342734894425295
Loss in iteration 190 : 0.33572384294594804
Loss in iteration 191 : 0.334269079003366
Loss in iteration 192 : 0.33571943160621986
Loss in iteration 193 : 0.33426502954329906
Loss in iteration 194 : 0.3357155341422157
Loss in iteration 195 : 0.3342614018500729
Loss in iteration 196 : 0.33571203418172296
Loss in iteration 197 : 0.33425795864989627
Loss in iteration 198 : 0.33570857861031705
Loss in iteration 199 : 0.33425441856583216
Loss in iteration 200 : 0.33570490664222147
Testing accuracy  of updater 5 on alg 0 with rate 0.07 = 0.8387076960874639, training accuracy 0.8441339066339066, time elapsed: 4424 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5062565371738027
Loss in iteration 3 : 0.45331950346123395
Loss in iteration 4 : 0.4254576117079502
Loss in iteration 5 : 0.40642733110260937
Loss in iteration 6 : 0.3926389591138902
Loss in iteration 7 : 0.38220246826696874
Loss in iteration 8 : 0.3740345720202632
Loss in iteration 9 : 0.3674730702035136
Loss in iteration 10 : 0.3620915739542064
Loss in iteration 11 : 0.3576034489757265
Loss in iteration 12 : 0.35380886657065885
Loss in iteration 13 : 0.35056410939710425
Loss in iteration 14 : 0.3477630072865801
Loss in iteration 15 : 0.3453252918580402
Loss in iteration 16 : 0.3431890527690728
Loss in iteration 17 : 0.34130570807043265
Loss in iteration 18 : 0.33963656168290246
Loss in iteration 19 : 0.33815038982079104
Loss in iteration 20 : 0.33682171105368436
Loss in iteration 21 : 0.3356295211402879
Loss in iteration 22 : 0.3345563507763752
Loss in iteration 23 : 0.3335875523492225
Loss in iteration 24 : 0.3327107522524898
Loss in iteration 25 : 0.33191542503506277
Loss in iteration 26 : 0.3311925586596859
Loss in iteration 27 : 0.33053438888121145
Loss in iteration 28 : 0.3299341867336232
Loss in iteration 29 : 0.32938608728768376
Loss in iteration 30 : 0.3288849508076182
Loss in iteration 31 : 0.32842624957981786
Loss in iteration 32 : 0.32800597525836644
Loss in iteration 33 : 0.3276205627352404
Loss in iteration 34 : 0.3272668274098484
Loss in iteration 35 : 0.32694191338061857
Loss in iteration 36 : 0.3266432505690688
Loss in iteration 37 : 0.3263685191568736
Loss in iteration 38 : 0.3261156200016196
Loss in iteration 39 : 0.32588264992290716
Loss in iteration 40 : 0.3256678809311596
Loss in iteration 41 : 0.3254697426296316
Loss in iteration 42 : 0.3252868071316267
Loss in iteration 43 : 0.3251177760106794
Loss in iteration 44 : 0.32496146874877463
Loss in iteration 45 : 0.32481681445730565
Loss in iteration 46 : 0.3246828818246578
Loss in iteration 47 : 0.32456003747635875
Loss in iteration 48 : 0.324486727523791
Loss in iteration 49 : 0.3258306850325179
Loss in iteration 50 : 0.3569951870278538
Loss in iteration 51 : 0.35778709224490285
Loss in iteration 52 : 0.3364464041111462
Loss in iteration 53 : 0.3261319223894022
Loss in iteration 54 : 0.3246541815827971
Loss in iteration 55 : 0.3242117680335366
Loss in iteration 56 : 0.3240611299837823
Loss in iteration 57 : 0.3239807309193726
Loss in iteration 58 : 0.32392882214767843
Loss in iteration 59 : 0.3238894131140398
Loss in iteration 60 : 0.323860882627273
Loss in iteration 61 : 0.32384529871445644
Loss in iteration 62 : 0.32385673121296527
Loss in iteration 63 : 0.32393006155262405
Loss in iteration 64 : 0.32418655292981263
Loss in iteration 65 : 0.32497173057253365
Loss in iteration 66 : 0.3274459909564143
Loss in iteration 67 : 0.3329284151787698
Loss in iteration 68 : 0.33937596402543324
Loss in iteration 69 : 0.334774643616728
Loss in iteration 70 : 0.33109517472895555
Loss in iteration 71 : 0.32703083198509886
Loss in iteration 72 : 0.3257077641549544
Loss in iteration 73 : 0.3248718686550148
Loss in iteration 74 : 0.32461062210422004
Loss in iteration 75 : 0.3244741680210218
Loss in iteration 76 : 0.32457809153972195
Loss in iteration 77 : 0.3247984746625525
Loss in iteration 78 : 0.32539844725067785
Loss in iteration 79 : 0.32629920977178184
Loss in iteration 80 : 0.32805651953988624
Loss in iteration 81 : 0.32961607534256143
Loss in iteration 82 : 0.33140821788760544
Loss in iteration 83 : 0.3303403559549753
Loss in iteration 84 : 0.32967460596193165
Loss in iteration 85 : 0.327660563346632
Loss in iteration 86 : 0.3269051771400444
Loss in iteration 87 : 0.3260006020202704
Loss in iteration 88 : 0.32580605314507277
Loss in iteration 89 : 0.325573806053533
Loss in iteration 90 : 0.3258032839372669
Loss in iteration 91 : 0.3259900416265465
Loss in iteration 92 : 0.32667629921793434
Loss in iteration 93 : 0.3271752156821912
Loss in iteration 94 : 0.32821061278057495
Loss in iteration 95 : 0.32841350705550276
Loss in iteration 96 : 0.32900645261690875
Loss in iteration 97 : 0.3282705119075965
Loss in iteration 98 : 0.328153743210495
Loss in iteration 99 : 0.3272113144858134
Loss in iteration 100 : 0.32702487017644777
Loss in iteration 101 : 0.3264576941497735
Loss in iteration 102 : 0.3265090156717191
Loss in iteration 103 : 0.32630695104655705
Loss in iteration 104 : 0.32663470917469783
Loss in iteration 105 : 0.326673824789537
Loss in iteration 106 : 0.32723297663476625
Loss in iteration 107 : 0.32730414472759084
Loss in iteration 108 : 0.3278802296711451
Loss in iteration 109 : 0.32767456604249895
Loss in iteration 110 : 0.3279889126135046
Loss in iteration 111 : 0.3274649418789293
Loss in iteration 112 : 0.32754744652150647
Loss in iteration 113 : 0.3269965225164321
Loss in iteration 114 : 0.3270707396231583
Loss in iteration 115 : 0.32668976285145424
Loss in iteration 116 : 0.32688806907355744
Loss in iteration 117 : 0.32668678292615544
Loss in iteration 118 : 0.32703027259768375
Loss in iteration 119 : 0.32692288929715946
Loss in iteration 120 : 0.3273475939185634
Loss in iteration 121 : 0.32719647021197723
Loss in iteration 122 : 0.3275774593308415
Loss in iteration 123 : 0.32728025895048285
Loss in iteration 124 : 0.32753883806228096
Loss in iteration 125 : 0.32713478301224364
Loss in iteration 126 : 0.3273145957318963
Loss in iteration 127 : 0.3269238137501482
Loss in iteration 128 : 0.3271164096629517
Loss in iteration 129 : 0.32681118644193385
Loss in iteration 130 : 0.32707071749360545
Loss in iteration 131 : 0.3268459002420632
Loss in iteration 132 : 0.32717074552963943
Loss in iteration 133 : 0.32697091513056653
Loss in iteration 134 : 0.32731528241223024
Loss in iteration 135 : 0.32707473833707357
Loss in iteration 136 : 0.32738342479345367
Loss in iteration 137 : 0.32707660501262165
Loss in iteration 138 : 0.32733190692406855
Loss in iteration 139 : 0.326991377213671
Loss in iteration 140 : 0.3272216184382099
Loss in iteration 141 : 0.32689834188818356
Loss in iteration 142 : 0.32714305863580667
Loss in iteration 143 : 0.3268622329060921
Loss in iteration 144 : 0.32714093934915534
Loss in iteration 145 : 0.3268924268247286
Loss in iteration 146 : 0.32719699639023525
Loss in iteration 147 : 0.32695016224112106
Loss in iteration 148 : 0.3272552630355046
Loss in iteration 149 : 0.3269834758986743
Loss in iteration 150 : 0.3272677004322108
Loss in iteration 151 : 0.32696808892993745
Loss in iteration 152 : 0.32723021440989924
Loss in iteration 153 : 0.326921869887475
Loss in iteration 154 : 0.3271776372905407
Loss in iteration 155 : 0.3268818891405308
Loss in iteration 156 : 0.3271480444448649
Loss in iteration 157 : 0.32687220723564564
Loss in iteration 158 : 0.3271543412287433
Loss in iteration 159 : 0.32688996730761954
Loss in iteration 160 : 0.32718106721164164
Loss in iteration 161 : 0.3269131429024972
Loss in iteration 162 : 0.3272009663917098
Loss in iteration 163 : 0.32691998745521494
Loss in iteration 164 : 0.32719711233217663
Loss in iteration 165 : 0.3269052708415311
Loss in iteration 166 : 0.32717383284768986
Loss in iteration 167 : 0.32688103440740257
Loss in iteration 168 : 0.32714907345725897
Loss in iteration 169 : 0.32686381106539625
Loss in iteration 170 : 0.327138007489429
Loss in iteration 171 : 0.32686148017379874
Loss in iteration 172 : 0.32714264017841943
Loss in iteration 173 : 0.32686949019700434
Loss in iteration 174 : 0.3271531882125826
Loss in iteration 175 : 0.3268767005224207
Loss in iteration 176 : 0.3271575339087599
Loss in iteration 177 : 0.3268748356436901
Loss in iteration 178 : 0.32715063067001077
Loss in iteration 179 : 0.3268640834820947
Loss in iteration 180 : 0.32713686781970036
Loss in iteration 181 : 0.3268512242500235
Loss in iteration 182 : 0.32712482739754184
Loss in iteration 183 : 0.32684319957765334
Loss in iteration 184 : 0.3271200518849313
Loss in iteration 185 : 0.32684198983319773
Loss in iteration 186 : 0.3271216498822466
Loss in iteration 187 : 0.3268442184521677
Loss in iteration 188 : 0.3271242849925024
Loss in iteration 189 : 0.3268447148451282
Loss in iteration 190 : 0.32712298616241164
Loss in iteration 191 : 0.32684068225114654
Loss in iteration 192 : 0.32711674617544806
Loss in iteration 193 : 0.3268332870378837
Loss in iteration 194 : 0.3271084569767129
Loss in iteration 195 : 0.32682597301163796
Loss in iteration 196 : 0.32710195503783096
Loss in iteration 197 : 0.32682144191533247
Loss in iteration 198 : 0.3270989991227918
Loss in iteration 199 : 0.3268198264267663
Loss in iteration 200 : 0.3270984166351824
Testing accuracy  of updater 5 on alg 0 with rate 0.04000000000000001 = 0.8445427185062343, training accuracy 0.8483722358722359, time elapsed: 4153 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6020788105998478
Loss in iteration 3 : 0.5613440427775551
Loss in iteration 4 : 0.5351008872204719
Loss in iteration 5 : 0.5155978231026199
Loss in iteration 6 : 0.5000462855253847
Loss in iteration 7 : 0.4871148239051968
Loss in iteration 8 : 0.47605423042784517
Loss in iteration 9 : 0.46639861118985915
Loss in iteration 10 : 0.45783826193546956
Loss in iteration 11 : 0.45015706604837985
Loss in iteration 12 : 0.443198423470028
Loss in iteration 13 : 0.4368453294680804
Loss in iteration 14 : 0.43100808217093506
Loss in iteration 15 : 0.42561637218047177
Loss in iteration 16 : 0.42061401704943163
Loss in iteration 17 : 0.41595535623832647
Loss in iteration 18 : 0.4116027223768189
Loss in iteration 19 : 0.4075246289651146
Loss in iteration 20 : 0.40369444600137855
Loss in iteration 21 : 0.400089414746762
Loss in iteration 22 : 0.39668990266409176
Loss in iteration 23 : 0.393478831455096
Loss in iteration 24 : 0.3904412319393816
Loss in iteration 25 : 0.3875638933423046
Loss in iteration 26 : 0.38483508387906534
Loss in iteration 27 : 0.38224432589629687
Loss in iteration 28 : 0.37978221325237244
Loss in iteration 29 : 0.3774402617269536
Loss in iteration 30 : 0.37521078547160297
Loss in iteration 31 : 0.37308679412781537
Loss in iteration 32 : 0.37106190643649617
Loss in iteration 33 : 0.3691302770717608
Loss in iteration 34 : 0.36728653413870166
Loss in iteration 35 : 0.36552572533689764
Loss in iteration 36 : 0.3638432712456104
Loss in iteration 37 : 0.3622349245550228
Loss in iteration 38 : 0.36069673436315847
Loss in iteration 39 : 0.3592250148875269
Loss in iteration 40 : 0.3578163181096264
Loss in iteration 41 : 0.3564674099847532
Loss in iteration 42 : 0.35517524991710364
Loss in iteration 43 : 0.3539369732300567
Loss in iteration 44 : 0.3527498763647621
Loss in iteration 45 : 0.351611404526713
Loss in iteration 46 : 0.35051914147985735
Loss in iteration 47 : 0.3494708011681051
Loss in iteration 48 : 0.34846422083066225
Loss in iteration 49 : 0.3474973552742903
Loss in iteration 50 : 0.34656827197437
Loss in iteration 51 : 0.34567514669872745
Loss in iteration 52 : 0.34481625938317667
Loss in iteration 53 : 0.34398999003428254
Loss in iteration 54 : 0.3431948144901684
Loss in iteration 55 : 0.34242929993053167
Loss in iteration 56 : 0.3416921000879645
Loss in iteration 57 : 0.3409819501690597
Loss in iteration 58 : 0.3402976615420821
Loss in iteration 59 : 0.33963811628449186
Loss in iteration 60 : 0.3390022617073297
Loss in iteration 61 : 0.3383891049840348
Loss in iteration 62 : 0.3377977080101175
Loss in iteration 63 : 0.33722718260915596
Loss in iteration 64 : 0.3366766861821781
Loss in iteration 65 : 0.33614541787380975
Loss in iteration 66 : 0.3356326153017384
Loss in iteration 67 : 0.33513755186767047
Loss in iteration 68 : 0.33465953463959
Loss in iteration 69 : 0.3341979027680733
Loss in iteration 70 : 0.33375202637515544
Loss in iteration 71 : 0.3333213058339877
Loss in iteration 72 : 0.33290517134270564
Loss in iteration 73 : 0.332503082687825
Loss in iteration 74 : 0.3321145290915888
Loss in iteration 75 : 0.33173902904444447
Loss in iteration 76 : 0.3313761300370449
Loss in iteration 77 : 0.3310254081241331
Loss in iteration 78 : 0.3306864672728935
Loss in iteration 79 : 0.3303589384680134
Loss in iteration 80 : 0.3300424785630712
Loss in iteration 81 : 0.3297367688814349
Loss in iteration 82 : 0.3294415135803687
Loss in iteration 83 : 0.32915643780005177
Loss in iteration 84 : 0.3288812856262888
Loss in iteration 85 : 0.3286158179025462
Loss in iteration 86 : 0.3283598099339258
Loss in iteration 87 : 0.3281130491312634
Loss in iteration 88 : 0.32787533264887825
Loss in iteration 89 : 0.3276464650787302
Loss in iteration 90 : 0.32742625634732664
Loss in iteration 91 : 0.32721452494921227
Loss in iteration 92 : 0.3270112742198084
Loss in iteration 93 : 0.32682383162359846
Loss in iteration 94 : 0.3269357088742888
Loss in iteration 95 : 0.32927242063465323
Loss in iteration 96 : 0.32758554674035517
Loss in iteration 97 : 0.3265843024041529
Loss in iteration 98 : 0.3262253900208324
Loss in iteration 99 : 0.32605728238563714
Loss in iteration 100 : 0.3259312260988835
Loss in iteration 101 : 0.3258192940666972
Loss in iteration 102 : 0.32571374506416023
Loss in iteration 103 : 0.3256126206713835
Loss in iteration 104 : 0.32551517698007704
Loss in iteration 105 : 0.32542140476177933
Loss in iteration 106 : 0.3253316261480376
Loss in iteration 107 : 0.3252472709915451
Loss in iteration 108 : 0.3251720371874698
Loss in iteration 109 : 0.3251176647047958
Loss in iteration 110 : 0.3251162461447466
Loss in iteration 111 : 0.3252470593940259
Loss in iteration 112 : 0.3255236100061432
Loss in iteration 113 : 0.3256407726818226
Loss in iteration 114 : 0.3253210051922085
Loss in iteration 115 : 0.32501362583457283
Loss in iteration 116 : 0.32479315911420154
Loss in iteration 117 : 0.32466664944837365
Loss in iteration 118 : 0.3245776926588885
Loss in iteration 119 : 0.32451250185379443
Loss in iteration 120 : 0.3244588157047572
Loss in iteration 121 : 0.32441673748962874
Loss in iteration 122 : 0.3243855423233729
Loss in iteration 123 : 0.32437329844855795
Loss in iteration 124 : 0.3243856283710356
Loss in iteration 125 : 0.32443757584563404
Loss in iteration 126 : 0.324506276729499
Loss in iteration 127 : 0.3245604347106211
Loss in iteration 128 : 0.3245115523307265
Loss in iteration 129 : 0.3244146026172832
Loss in iteration 130 : 0.3242831075004874
Loss in iteration 131 : 0.3241857735192291
Loss in iteration 132 : 0.32410511825767213
Loss in iteration 133 : 0.32405214608226873
Loss in iteration 134 : 0.32401103448263596
Loss in iteration 135 : 0.3239870812289648
Loss in iteration 136 : 0.3239723604590934
Loss in iteration 137 : 0.32397479751214686
Loss in iteration 138 : 0.3239858512588225
Loss in iteration 139 : 0.3240125916573788
Loss in iteration 140 : 0.324029777170649
Loss in iteration 141 : 0.3240409399073751
Loss in iteration 142 : 0.3240122711075375
Loss in iteration 143 : 0.323973864355479
Loss in iteration 144 : 0.32391209327852694
Loss in iteration 145 : 0.32386328546692067
Loss in iteration 146 : 0.32381385120341977
Loss in iteration 147 : 0.323782442389253
Loss in iteration 148 : 0.32375512219266683
Loss in iteration 149 : 0.32374323185605924
Loss in iteration 150 : 0.32373458719501
Loss in iteration 151 : 0.3237394023075271
Loss in iteration 152 : 0.3237430672647128
Loss in iteration 153 : 0.323755574846257
Loss in iteration 154 : 0.3237557335879082
Loss in iteration 155 : 0.32375698284754534
Loss in iteration 156 : 0.3237373372127257
Loss in iteration 157 : 0.323719609769819
Loss in iteration 158 : 0.3236871606366522
Loss in iteration 159 : 0.3236642472750287
Loss in iteration 160 : 0.32363596611551343
Loss in iteration 161 : 0.32362060841191664
Loss in iteration 162 : 0.32360366805139057
Loss in iteration 163 : 0.3235992682327375
Loss in iteration 164 : 0.32359284155775736
Loss in iteration 165 : 0.32359716366875296
Loss in iteration 166 : 0.3235958230658161
Loss in iteration 167 : 0.32360208796035533
Loss in iteration 168 : 0.32359715048612936
Loss in iteration 169 : 0.32359710391419555
Loss in iteration 170 : 0.32358324622878554
Loss in iteration 171 : 0.3235752193327233
Loss in iteration 172 : 0.3235561722942282
Loss in iteration 173 : 0.3235459967956687
Loss in iteration 174 : 0.3235289395646502
Loss in iteration 175 : 0.3235224810199702
Loss in iteration 176 : 0.32351118161561465
Loss in iteration 177 : 0.3235104150640302
Loss in iteration 178 : 0.32350439243598056
Loss in iteration 179 : 0.3235077155355017
Loss in iteration 180 : 0.3235035574934871
Loss in iteration 181 : 0.3235070763873568
Loss in iteration 182 : 0.32350055733147753
Loss in iteration 183 : 0.3235007604297511
Loss in iteration 184 : 0.323490123422941
Loss in iteration 185 : 0.32348682134894996
Loss in iteration 186 : 0.3234740637089276
Loss in iteration 187 : 0.3234699880589482
Loss in iteration 188 : 0.3234583727661962
Loss in iteration 189 : 0.3234562556745666
Loss in iteration 190 : 0.3234475568760957
Loss in iteration 191 : 0.3234482908820269
Loss in iteration 192 : 0.3234421000203927
Loss in iteration 193 : 0.3234446558314787
Loss in iteration 194 : 0.3234390788638292
Loss in iteration 195 : 0.3234414409541103
Loss in iteration 196 : 0.32343452406353834
Loss in iteration 197 : 0.32343521471890335
Loss in iteration 198 : 0.32342640482478047
Loss in iteration 199 : 0.3234255574302401
Loss in iteration 200 : 0.32341592083845894
Testing accuracy  of updater 5 on alg 0 with rate 0.009999999999999995 = 0.8498863706160555, training accuracy 0.8487714987714988, time elapsed: 4141 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 4.558338093070672
Loss in iteration 3 : 3.420373956978546
Loss in iteration 4 : 1.0891996810329483
Loss in iteration 5 : 1.71917630573738
Loss in iteration 6 : 2.574567951644458
Loss in iteration 7 : 1.7759037379769917
Loss in iteration 8 : 1.1998181584694345
Loss in iteration 9 : 1.4489895617874364
Loss in iteration 10 : 1.8445920318056113
Loss in iteration 11 : 1.9472190787945591
Loss in iteration 12 : 1.697477125931318
Loss in iteration 13 : 1.3273248894498957
Loss in iteration 14 : 1.1761044166767676
Loss in iteration 15 : 1.3589217777721923
Loss in iteration 16 : 1.5377252094064267
Loss in iteration 17 : 1.3920210275461489
Loss in iteration 18 : 1.078927111540717
Loss in iteration 19 : 0.9904626102614347
Loss in iteration 20 : 1.1577729846735823
Loss in iteration 21 : 1.2012107734427795
Loss in iteration 22 : 0.9752721857607818
Loss in iteration 23 : 0.7754466340874143
Loss in iteration 24 : 0.9336925957764937
Loss in iteration 25 : 0.9368576892008489
Loss in iteration 26 : 0.6464656991082987
Loss in iteration 27 : 0.7135193061415162
Loss in iteration 28 : 0.7836267378370965
Loss in iteration 29 : 0.5605783588654455
Loss in iteration 30 : 0.5926881330160825
Loss in iteration 31 : 0.6357709434145304
Loss in iteration 32 : 0.44489169351677277
Loss in iteration 33 : 0.6002703548150616
Loss in iteration 34 : 0.43383488885662597
Loss in iteration 35 : 0.553665156367759
Loss in iteration 36 : 0.43699139222248745
Loss in iteration 37 : 0.5273419912353253
Loss in iteration 38 : 0.44581003753031284
Loss in iteration 39 : 0.5046846982579067
Loss in iteration 40 : 0.39836180875497124
Loss in iteration 41 : 0.4475483683198381
Loss in iteration 42 : 0.4136451879546083
Loss in iteration 43 : 0.4024145989285246
Loss in iteration 44 : 0.4177366905003852
Loss in iteration 45 : 0.3771690516343653
Loss in iteration 46 : 0.4093196316733064
Loss in iteration 47 : 0.353934384070767
Loss in iteration 48 : 0.40350631651304614
Loss in iteration 49 : 0.3492720280196072
Loss in iteration 50 : 0.3849864386467188
Loss in iteration 51 : 0.3407526149105748
Loss in iteration 52 : 0.37412367404924124
Loss in iteration 53 : 0.34613944934467106
Loss in iteration 54 : 0.3594969333034863
Loss in iteration 55 : 0.3518940222910812
Loss in iteration 56 : 0.34207590374236063
Loss in iteration 57 : 0.3553246851437186
Loss in iteration 58 : 0.3299972749166587
Loss in iteration 59 : 0.3540719996268564
Loss in iteration 60 : 0.33782530177071923
Loss in iteration 61 : 0.3350403069850248
Loss in iteration 62 : 0.3470763827119289
Loss in iteration 63 : 0.32837807413396863
Loss in iteration 64 : 0.34080553267225466
Loss in iteration 65 : 0.3295863752434683
Loss in iteration 66 : 0.3332713148172966
Loss in iteration 67 : 0.3332045701629659
Loss in iteration 68 : 0.3270551627838944
Loss in iteration 69 : 0.33612539421243176
Loss in iteration 70 : 0.32739723152203376
Loss in iteration 71 : 0.3297820311715747
Loss in iteration 72 : 0.33202570945262644
Loss in iteration 73 : 0.3249462053673752
Loss in iteration 74 : 0.3287454231686223
Loss in iteration 75 : 0.32697877845242873
Loss in iteration 76 : 0.32392572769602423
Loss in iteration 77 : 0.3272076675800412
Loss in iteration 78 : 0.3245005119447359
Loss in iteration 79 : 0.32425094366378493
Loss in iteration 80 : 0.32654873213591434
Loss in iteration 81 : 0.32410487457136516
Loss in iteration 82 : 0.3242806466078964
Loss in iteration 83 : 0.3256108073393792
Loss in iteration 84 : 0.3235687673447542
Loss in iteration 85 : 0.3238461909302086
Loss in iteration 86 : 0.3249005754173837
Loss in iteration 87 : 0.3234195996605998
Loss in iteration 88 : 0.32344429516509154
Loss in iteration 89 : 0.32433551626138674
Loss in iteration 90 : 0.32335410379249674
Loss in iteration 91 : 0.3230023017038565
Loss in iteration 92 : 0.32377740127559484
Loss in iteration 93 : 0.32337820635926107
Loss in iteration 94 : 0.3228310077829435
Loss in iteration 95 : 0.3233289903714961
Loss in iteration 96 : 0.3234133230509145
Loss in iteration 97 : 0.32289844982235666
Loss in iteration 98 : 0.32295219091664573
Loss in iteration 99 : 0.32324900526926337
Loss in iteration 100 : 0.3230171279651322
Loss in iteration 101 : 0.3227299966420158
Loss in iteration 102 : 0.3229073016097975
Loss in iteration 103 : 0.3230578977622268
Loss in iteration 104 : 0.3228371321558211
Loss in iteration 105 : 0.32269798993976073
Loss in iteration 106 : 0.3228475901656892
Loss in iteration 107 : 0.3229249047402488
Loss in iteration 108 : 0.32278510042654374
Loss in iteration 109 : 0.32269125610167243
Loss in iteration 110 : 0.3227762140280207
Loss in iteration 111 : 0.32284148696865494
Loss in iteration 112 : 0.3227584398867903
Loss in iteration 113 : 0.3226676679585492
Loss in iteration 114 : 0.3226882215637201
Loss in iteration 115 : 0.32275445051905294
Loss in iteration 116 : 0.3227554864004026
Loss in iteration 117 : 0.3226905357082838
Loss in iteration 118 : 0.3226508568553547
Loss in iteration 119 : 0.3226756388438501
Loss in iteration 120 : 0.3227168842584773
Loss in iteration 121 : 0.3227178677553256
Loss in iteration 122 : 0.3226780258856631
Loss in iteration 123 : 0.32264528360950545
Loss in iteration 124 : 0.32264853627305085
Loss in iteration 125 : 0.3226734303572546
Loss in iteration 126 : 0.3226880012556351
Loss in iteration 127 : 0.32267687548767593
Loss in iteration 128 : 0.3226536188954459
Loss in iteration 129 : 0.3226378673632785
Loss in iteration 130 : 0.32263873116464525
Loss in iteration 131 : 0.322650902559222
Loss in iteration 132 : 0.3226634756523932
Loss in iteration 133 : 0.32266872672065405
Loss in iteration 134 : 0.3226637440988803
Loss in iteration 135 : 0.3226527804731084
Loss in iteration 136 : 0.3226416130247775
Loss in iteration 137 : 0.32263479415920354
Loss in iteration 138 : 0.3226329356932018
Loss in iteration 139 : 0.322634975207346
Loss in iteration 140 : 0.3226397236513316
Loss in iteration 141 : 0.3226456734321664
Loss in iteration 142 : 0.32265152465534785
Loss in iteration 143 : 0.3226566639613022
Loss in iteration 144 : 0.3226622473178175
Loss in iteration 145 : 0.3226687012042982
Loss in iteration 146 : 0.32267786281288796
Loss in iteration 147 : 0.32269050480041345
Loss in iteration 148 : 0.3227115858374727
Loss in iteration 149 : 0.3227448645003518
Loss in iteration 150 : 0.32280547056974734
Loss in iteration 151 : 0.3229077431691018
Loss in iteration 152 : 0.32310850322975176
Loss in iteration 153 : 0.3234620485692865
Loss in iteration 154 : 0.32421460546646447
Loss in iteration 155 : 0.3255386310260224
Loss in iteration 156 : 0.32861091686590493
Loss in iteration 157 : 0.3334425360127653
Loss in iteration 158 : 0.3453733547965215
Loss in iteration 159 : 0.35604882173564445
Loss in iteration 160 : 0.38037060401387474
Loss in iteration 161 : 0.36538270001151557
Loss in iteration 162 : 0.3502439389310651
Loss in iteration 163 : 0.3283323549499604
Loss in iteration 164 : 0.32926548828460284
Loss in iteration 165 : 0.35032623139326946
Loss in iteration 166 : 0.37312770616126123
Loss in iteration 167 : 0.4282953338238471
Loss in iteration 168 : 0.39331805327891717
Loss in iteration 169 : 0.3642678529919618
Loss in iteration 170 : 0.33192931954018234
Loss in iteration 171 : 0.3561274168159134
Loss in iteration 172 : 0.39660172833957874
Loss in iteration 173 : 0.36597798823888483
Loss in iteration 174 : 0.34262511417200625
Loss in iteration 175 : 0.32565227850893336
Loss in iteration 176 : 0.3265955494352503
Loss in iteration 177 : 0.34237534626149724
Loss in iteration 178 : 0.3644194205277862
Loss in iteration 179 : 0.413885813167208
Loss in iteration 180 : 0.39572088285710344
Loss in iteration 181 : 0.3769582758720853
Loss in iteration 182 : 0.3327048202237228
Loss in iteration 183 : 0.3511781132346002
Loss in iteration 184 : 0.39424485676153975
Loss in iteration 185 : 0.36087028380098446
Loss in iteration 186 : 0.3336507845396202
Loss in iteration 187 : 0.325429454482833
Loss in iteration 188 : 0.34016574791372634
Loss in iteration 189 : 0.3766552941773187
Loss in iteration 190 : 0.39144301812883053
Loss in iteration 191 : 0.4212572128120691
Loss in iteration 192 : 0.3565784936026847
Loss in iteration 193 : 0.33107816066459006
Loss in iteration 194 : 0.3567992612921466
Loss in iteration 195 : 0.3691456068964903
Loss in iteration 196 : 0.3696660257325636
Loss in iteration 197 : 0.3385808220257871
Loss in iteration 198 : 0.3250833843810685
Loss in iteration 199 : 0.3281802830981227
Loss in iteration 200 : 0.3419916047107004
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.8306615072784227, training accuracy 0.8282555282555283, time elapsed: 3909 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 1.460212080656427
Loss in iteration 3 : 1.2327542897146997
Loss in iteration 4 : 0.5793137163503845
Loss in iteration 5 : 1.1675004274329266
Loss in iteration 6 : 1.2209833397716723
Loss in iteration 7 : 0.8610092527989183
Loss in iteration 8 : 0.9692761560840822
Loss in iteration 9 : 1.1690043773768748
Loss in iteration 10 : 1.1222222898503185
Loss in iteration 11 : 0.9032675809641333
Loss in iteration 12 : 0.8408475158744635
Loss in iteration 13 : 0.9851663841106552
Loss in iteration 14 : 0.9328776149526954
Loss in iteration 15 : 0.7374936888189874
Loss in iteration 16 : 0.8021673176720051
Loss in iteration 17 : 0.8630796963812595
Loss in iteration 18 : 0.6821970872995962
Loss in iteration 19 : 0.6858223040437604
Loss in iteration 20 : 0.7156586589757619
Loss in iteration 21 : 0.5184666195878957
Loss in iteration 22 : 0.614306866323657
Loss in iteration 23 : 0.5014813462438689
Loss in iteration 24 : 0.5057492987621278
Loss in iteration 25 : 0.5046101343339476
Loss in iteration 26 : 0.4542697135536054
Loss in iteration 27 : 0.487736311872565
Loss in iteration 28 : 0.4329952376161732
Loss in iteration 29 : 0.4475840817120338
Loss in iteration 30 : 0.4874833427552394
Loss in iteration 31 : 0.40170313735896246
Loss in iteration 32 : 0.4852952972205997
Loss in iteration 33 : 0.3757506345739907
Loss in iteration 34 : 0.42177266640650685
Loss in iteration 35 : 0.36990055472348765
Loss in iteration 36 : 0.4170447652315337
Loss in iteration 37 : 0.36535022060091293
Loss in iteration 38 : 0.39762449221681495
Loss in iteration 39 : 0.35437211143671776
Loss in iteration 40 : 0.38960747694855613
Loss in iteration 41 : 0.35051952407876064
Loss in iteration 42 : 0.3737416191472652
Loss in iteration 43 : 0.34323625345061665
Loss in iteration 44 : 0.35593540863426265
Loss in iteration 45 : 0.3479323229291062
Loss in iteration 46 : 0.3429400314130343
Loss in iteration 47 : 0.3575684088230001
Loss in iteration 48 : 0.33421662867139157
Loss in iteration 49 : 0.3457626900335575
Loss in iteration 50 : 0.3343925023819599
Loss in iteration 51 : 0.331839251518461
Loss in iteration 52 : 0.3403266967829403
Loss in iteration 53 : 0.3283592472435902
Loss in iteration 54 : 0.3369150064065704
Loss in iteration 55 : 0.3315508510319553
Loss in iteration 56 : 0.3301597881473843
Loss in iteration 57 : 0.3330858659991776
Loss in iteration 58 : 0.3265613915978379
Loss in iteration 59 : 0.33160403777691483
Loss in iteration 60 : 0.32609397014363156
Loss in iteration 61 : 0.3296145735695351
Loss in iteration 62 : 0.32871819385004564
Loss in iteration 63 : 0.3263143530687927
Loss in iteration 64 : 0.3293907941981762
Loss in iteration 65 : 0.3250100142609799
Loss in iteration 66 : 0.3260392729258689
Loss in iteration 67 : 0.3258750255647292
Loss in iteration 68 : 0.32368951746476693
Loss in iteration 69 : 0.32564425405970504
Loss in iteration 70 : 0.3237674309049823
Loss in iteration 71 : 0.3244293425621468
Loss in iteration 72 : 0.3250918331522433
Loss in iteration 73 : 0.32355450622849635
Loss in iteration 74 : 0.32470774949923564
Loss in iteration 75 : 0.3238589625759097
Loss in iteration 76 : 0.3234163637983426
Loss in iteration 77 : 0.32422509206835154
Loss in iteration 78 : 0.32320063082925954
Loss in iteration 79 : 0.3236593347454506
Loss in iteration 80 : 0.32363687539885355
Loss in iteration 81 : 0.3229911341440917
Loss in iteration 82 : 0.3235113480652077
Loss in iteration 83 : 0.3230946576117305
Loss in iteration 84 : 0.3229470365773106
Loss in iteration 85 : 0.3232482443989554
Loss in iteration 86 : 0.3228168660993874
Loss in iteration 87 : 0.32301865962239046
Loss in iteration 88 : 0.32307919909465765
Loss in iteration 89 : 0.32282360159472595
Loss in iteration 90 : 0.32305638580506413
Loss in iteration 91 : 0.3228859701509583
Loss in iteration 92 : 0.3227877249771361
Loss in iteration 93 : 0.3229506517601934
Loss in iteration 94 : 0.3227584254526927
Loss in iteration 95 : 0.3227667730713994
Loss in iteration 96 : 0.3228379852398259
Loss in iteration 97 : 0.3226909122335469
Loss in iteration 98 : 0.3227597349812752
Loss in iteration 99 : 0.322769215474859
Loss in iteration 100 : 0.32267376942961085
Loss in iteration 101 : 0.3227461186741208
Loss in iteration 102 : 0.32271987367026833
Loss in iteration 103 : 0.3226765701951613
Loss in iteration 104 : 0.3227319060667671
Loss in iteration 105 : 0.3226852867100663
Loss in iteration 106 : 0.32266736177169303
Loss in iteration 107 : 0.3227039039950679
Loss in iteration 108 : 0.32266276112936637
Loss in iteration 109 : 0.3226607874644538
Loss in iteration 110 : 0.32267954153686207
Loss in iteration 111 : 0.32264733573526777
Loss in iteration 112 : 0.32265708491801354
Loss in iteration 113 : 0.3226690174619071
Loss in iteration 114 : 0.32264302113843696
Loss in iteration 115 : 0.3226514400328084
Loss in iteration 116 : 0.32265791377961867
Loss in iteration 117 : 0.32264165082329166
Loss in iteration 118 : 0.3226501939488655
Loss in iteration 119 : 0.32265061120948657
Loss in iteration 120 : 0.3226367910486668
Loss in iteration 121 : 0.32264407889400964
Loss in iteration 122 : 0.3226447433783501
Loss in iteration 123 : 0.32263515535245074
Loss in iteration 124 : 0.32264025555804554
Loss in iteration 125 : 0.32264035588505413
Loss in iteration 126 : 0.3226340366682734
Loss in iteration 127 : 0.32263846417027914
Loss in iteration 128 : 0.3226386306432947
Loss in iteration 129 : 0.32263337728661784
Loss in iteration 130 : 0.3226362581258521
Testing accuracy  of updater 6 on alg 0 with rate 1.4000000000000001 = 0.8498249493274369, training accuracy 0.8488022113022113, time elapsed: 2181 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.7848532224676416
Loss in iteration 3 : 0.6483481398323424
Loss in iteration 4 : 0.4128548724897573
Loss in iteration 5 : 0.6769840200461175
Loss in iteration 6 : 0.6614495017506948
Loss in iteration 7 : 0.5315876716270277
Loss in iteration 8 : 0.6030270709660577
Loss in iteration 9 : 0.6793281705674981
Loss in iteration 10 : 0.6267524951606701
Loss in iteration 11 : 0.5352978086901918
Loss in iteration 12 : 0.5553132512723541
Loss in iteration 13 : 0.602946738323951
Loss in iteration 14 : 0.5352409048147464
Loss in iteration 15 : 0.4865420786053007
Loss in iteration 16 : 0.5381490167715232
Loss in iteration 17 : 0.5179802299743804
Loss in iteration 18 : 0.44534945654216657
Loss in iteration 19 : 0.48359714814445187
Loss in iteration 20 : 0.4471857621389024
Loss in iteration 21 : 0.3958634047786525
Loss in iteration 22 : 0.43059215742595697
Loss in iteration 23 : 0.38280805031573917
Loss in iteration 24 : 0.38646713472107397
Loss in iteration 25 : 0.39892765344597925
Loss in iteration 26 : 0.3596289986463134
Loss in iteration 27 : 0.3908119243745302
Loss in iteration 28 : 0.35601339230713275
Loss in iteration 29 : 0.3782823202085305
Loss in iteration 30 : 0.3629374187373014
Loss in iteration 31 : 0.3660785137231503
Loss in iteration 32 : 0.36422576546440844
Loss in iteration 33 : 0.34747570909922915
Loss in iteration 34 : 0.3561289008708279
Loss in iteration 35 : 0.33599110395581677
Loss in iteration 36 : 0.3481540671443025
Loss in iteration 37 : 0.33623609340096544
Loss in iteration 38 : 0.34102307893651007
Loss in iteration 39 : 0.3393648496230719
Loss in iteration 40 : 0.33299210002821633
Loss in iteration 41 : 0.3381053089554687
Loss in iteration 42 : 0.32894877218096114
Loss in iteration 43 : 0.3349939052629692
Loss in iteration 44 : 0.3296833577494545
Loss in iteration 45 : 0.3313690278363292
Loss in iteration 46 : 0.3306692015237595
Loss in iteration 47 : 0.32766546914505174
Loss in iteration 48 : 0.3301783343109162
Loss in iteration 49 : 0.3261689755214035
Loss in iteration 50 : 0.32971232951341867
Loss in iteration 51 : 0.3262203861411482
Loss in iteration 52 : 0.3284507824809909
Loss in iteration 53 : 0.32554125590274263
Loss in iteration 54 : 0.3260803528689298
Loss in iteration 55 : 0.3249949605857346
Loss in iteration 56 : 0.3247941310982569
Loss in iteration 57 : 0.3252384828117653
Loss in iteration 58 : 0.3243566621168695
Loss in iteration 59 : 0.3252527839356001
Loss in iteration 60 : 0.32388152472050524
Loss in iteration 61 : 0.3248410993051676
Loss in iteration 62 : 0.32357115149989063
Loss in iteration 63 : 0.3242472793602519
Loss in iteration 64 : 0.32351033314652045
Loss in iteration 65 : 0.32374993018852394
Loss in iteration 66 : 0.3235748293647907
Loss in iteration 67 : 0.3234224734362106
Loss in iteration 68 : 0.3236405909143111
Loss in iteration 69 : 0.323259428346339
Loss in iteration 70 : 0.3236270208239956
Loss in iteration 71 : 0.3231584393122285
Loss in iteration 72 : 0.32345399442818307
Loss in iteration 73 : 0.3230186957823098
Loss in iteration 74 : 0.3232072270156185
Loss in iteration 75 : 0.3229272724985556
Loss in iteration 76 : 0.32302470809534883
Loss in iteration 77 : 0.32291923933515887
Loss in iteration 78 : 0.3229049339531161
Loss in iteration 79 : 0.32291237082547963
Loss in iteration 80 : 0.32282775878798986
Loss in iteration 81 : 0.3229171751189577
Loss in iteration 82 : 0.3228121185076834
Loss in iteration 83 : 0.32291827431869313
Loss in iteration 84 : 0.32279254965180526
Loss in iteration 85 : 0.32286570384422253
Loss in iteration 86 : 0.3227545966289533
Loss in iteration 87 : 0.32281005357747905
Loss in iteration 88 : 0.32274494773518786
Loss in iteration 89 : 0.3227802253360463
Loss in iteration 90 : 0.322748638249126
Loss in iteration 91 : 0.3227514470269408
Loss in iteration 92 : 0.3227388088714191
Loss in iteration 93 : 0.32271967852742345
Loss in iteration 94 : 0.32272513640086903
Loss in iteration 95 : 0.32269957369467334
Loss in iteration 96 : 0.3227171960952571
Loss in iteration 97 : 0.3226891258527093
Loss in iteration 98 : 0.3227074561160264
Loss in iteration 99 : 0.3226799668512151
Loss in iteration 100 : 0.3226973468399776
Loss in iteration 101 : 0.3226774155919569
Loss in iteration 102 : 0.32269264088463673
Loss in iteration 103 : 0.3226785271377367
Loss in iteration 104 : 0.3226859582252861
Loss in iteration 105 : 0.32267485287530884
Loss in iteration 106 : 0.3226766092215891
Loss in iteration 107 : 0.3226712757287243
Loss in iteration 108 : 0.3226706893487421
Loss in iteration 109 : 0.3226691093772037
Loss in iteration 110 : 0.3226653913521382
Loss in iteration 111 : 0.3226653378141359
Loss in iteration 112 : 0.32266043369026165
Loss in iteration 113 : 0.3226623185887781
Loss in iteration 114 : 0.3226575522048733
Loss in iteration 115 : 0.3226598969219526
Loss in iteration 116 : 0.32265501843930416
Loss in iteration 117 : 0.3226572304190271
Loss in iteration 118 : 0.32265320941944164
Loss in iteration 119 : 0.3226555157227621
Loss in iteration 120 : 0.3226524300550161
Loss in iteration 121 : 0.322654051990054
Loss in iteration 122 : 0.3226512801335096
Loss in iteration 123 : 0.32265200378419556
Testing accuracy  of updater 6 on alg 0 with rate 0.8 = 0.8498249493274369, training accuracy 0.8492321867321867, time elapsed: 2067 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.47899431299534734
Loss in iteration 3 : 0.4795535905997327
Loss in iteration 4 : 0.462871128538398
Loss in iteration 5 : 0.38733544715303264
Loss in iteration 6 : 0.3426400772890248
Loss in iteration 7 : 0.368005827528909
Loss in iteration 8 : 0.40560510987981707
Loss in iteration 9 : 0.40616169024480664
Loss in iteration 10 : 0.3818687305971778
Loss in iteration 11 : 0.36750624699249107
Loss in iteration 12 : 0.3749090486009881
Loss in iteration 13 : 0.38999727534178596
Loss in iteration 14 : 0.39519838886222686
Loss in iteration 15 : 0.38549382475106464
Loss in iteration 16 : 0.36974967406447434
Loss in iteration 17 : 0.361110840488533
Loss in iteration 18 : 0.3641004057752955
Loss in iteration 19 : 0.3703960613128239
Loss in iteration 20 : 0.3690681429283192
Loss in iteration 21 : 0.3594190135804668
Loss in iteration 22 : 0.3507887083585603
Loss in iteration 23 : 0.3502401485573132
Loss in iteration 24 : 0.35427210159125616
Loss in iteration 25 : 0.3545360808080813
Loss in iteration 26 : 0.3484505614958202
Loss in iteration 27 : 0.34159062729354067
Loss in iteration 28 : 0.3399920468950419
Loss in iteration 29 : 0.3419494588255484
Loss in iteration 30 : 0.3409116648244547
Loss in iteration 31 : 0.3356472644998905
Loss in iteration 32 : 0.33152682490438967
Loss in iteration 33 : 0.3316881034560375
Loss in iteration 34 : 0.3330374025945925
Loss in iteration 35 : 0.3318441806966368
Loss in iteration 36 : 0.32895766548511274
Loss in iteration 37 : 0.3277693170438168
Loss in iteration 38 : 0.32887115320494686
Loss in iteration 39 : 0.3294561754329553
Loss in iteration 40 : 0.32799614980400144
Loss in iteration 41 : 0.3263424696335516
Loss in iteration 42 : 0.32632358067605416
Loss in iteration 43 : 0.32703452909807773
Loss in iteration 44 : 0.3267389024640433
Loss in iteration 45 : 0.3256033933333572
Loss in iteration 46 : 0.32512121946758676
Loss in iteration 47 : 0.3256245960007254
Loss in iteration 48 : 0.32592200773343466
Loss in iteration 49 : 0.325372275064132
Loss in iteration 50 : 0.3247676404094181
Loss in iteration 51 : 0.32481360345416477
Loss in iteration 52 : 0.32505478808074945
Loss in iteration 53 : 0.3248085118427877
Loss in iteration 54 : 0.3242494932702024
Loss in iteration 55 : 0.3239940949215143
Loss in iteration 56 : 0.3240975225319191
Loss in iteration 57 : 0.32408427314969157
Loss in iteration 58 : 0.323786789918006
Loss in iteration 59 : 0.3235467321818261
Loss in iteration 60 : 0.323589009141225
Loss in iteration 61 : 0.3236990582246145
Loss in iteration 62 : 0.323625695783223
Loss in iteration 63 : 0.32345418952486343
Loss in iteration 64 : 0.32340535142239657
Loss in iteration 65 : 0.32347085582662866
Loss in iteration 66 : 0.32346412185913836
Loss in iteration 67 : 0.32333854637723
Loss in iteration 68 : 0.32323625155424496
Loss in iteration 69 : 0.32323933391425747
Loss in iteration 70 : 0.323260303156689
Loss in iteration 71 : 0.3232105697294169
Loss in iteration 72 : 0.32313673322169983
Loss in iteration 73 : 0.3231240664030559
Loss in iteration 74 : 0.3231565384179781
Loss in iteration 75 : 0.32315685985795045
Loss in iteration 76 : 0.3231146121044477
Loss in iteration 77 : 0.3230879956659375
Loss in iteration 78 : 0.3230994826594777
Loss in iteration 79 : 0.32310746981490424
Loss in iteration 80 : 0.32308310094336584
Loss in iteration 81 : 0.32305329095726515
Loss in iteration 82 : 0.3230495365571023
Loss in iteration 83 : 0.323058664181052
Loss in iteration 84 : 0.32305171887259454
Loss in iteration 85 : 0.32303192522529506
Loss in iteration 86 : 0.3230231333419194
Loss in iteration 87 : 0.323027995135592
Loss in iteration 88 : 0.3230272288459045
Loss in iteration 89 : 0.3230134619722644
Loss in iteration 90 : 0.3230003819072504
Loss in iteration 91 : 0.3229974838185107
Loss in iteration 92 : 0.32299619910228167
Loss in iteration 93 : 0.32298709642698364
Loss in iteration 94 : 0.3229753110119148
Loss in iteration 95 : 0.32297002651055784
Loss in iteration 96 : 0.322969388021812
Loss in iteration 97 : 0.3229655433942108
Loss in iteration 98 : 0.3229580936701041
Loss in iteration 99 : 0.3229531731310302
Loss in iteration 100 : 0.32295231564695
Loss in iteration 101 : 0.32295067169454955
Loss in iteration 102 : 0.3229457376378934
Loss in iteration 103 : 0.32294078556568423
Loss in iteration 104 : 0.322938505252223
Loss in iteration 105 : 0.3229368120299772
Loss in iteration 106 : 0.32293307977581165
Loss in iteration 107 : 0.322928460499042
Loss in iteration 108 : 0.32292538134739734
Loss in iteration 109 : 0.32292346746034123
Loss in iteration 110 : 0.32292068720449724
Loss in iteration 111 : 0.32291692239704806
Loss in iteration 112 : 0.3229137952859365
Loss in iteration 113 : 0.3229117274077425
Loss in iteration 114 : 0.32290946939552384
Loss in iteration 115 : 0.322906400789141
Loss in iteration 116 : 0.3229033968230169
Loss in iteration 117 : 0.32290113667317055
Loss in iteration 118 : 0.32289904652938983
Loss in iteration 119 : 0.3228964628064648
Loss in iteration 120 : 0.32289372613097206
Loss in iteration 121 : 0.3228914458863585
Loss in iteration 122 : 0.32288946111031763
Loss in iteration 123 : 0.3228872368321425
Loss in iteration 124 : 0.3228847890640099
Loss in iteration 125 : 0.32288254285099444
Loss in iteration 126 : 0.3228805537683649
Loss in iteration 127 : 0.322878478311768
Loss in iteration 128 : 0.32287620455876104
Loss in iteration 129 : 0.32287399293218055
Loss in iteration 130 : 0.32287198653173454
Loss in iteration 131 : 0.32287000531345184
Loss in iteration 132 : 0.3228679055322614
Loss in iteration 133 : 0.3228658105521274
Loss in iteration 134 : 0.32286385889588876
Loss in iteration 135 : 0.32286197184240134
Loss in iteration 136 : 0.32286001887444765
Loss in iteration 137 : 0.3228580371497245
Loss in iteration 138 : 0.3228561355752729
Loss in iteration 139 : 0.32285430026746315
Loss in iteration 140 : 0.3228524398576877
Loss in iteration 141 : 0.32285055120887823
Loss in iteration 142 : 0.322848709509182
Loss in iteration 143 : 0.3228469308851614
Loss in iteration 144 : 0.32284515742623693
Loss in iteration 145 : 0.322843366502702
Loss in iteration 146 : 0.3228416005113947
Loss in iteration 147 : 0.3228398827434553
Loss in iteration 148 : 0.32283818034429307
Loss in iteration 149 : 0.32283646750057077
Loss in iteration 150 : 0.32283476562912394
Loss in iteration 151 : 0.3228330981480954
Loss in iteration 152 : 0.3228314508749975
Loss in iteration 153 : 0.32282980283432894
Loss in iteration 154 : 0.3228281628719532
Loss in iteration 155 : 0.3228265496801635
Loss in iteration 156 : 0.32282495891946333
Loss in iteration 157 : 0.322823374707003
Loss in iteration 158 : 0.32282179801159927
Loss in iteration 159 : 0.322820241319076
Loss in iteration 160 : 0.322818704848673
Loss in iteration 161 : 0.3228171777471678
Loss in iteration 162 : 0.32281565772371074
Loss in iteration 163 : 0.32281415276400444
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.8500706344819114, training accuracy 0.8491707616707617, time elapsed: 3031 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.49990755024450373
Loss in iteration 3 : 0.47054084081533354
Loss in iteration 4 : 0.4724671350820802
Loss in iteration 5 : 0.42224147329948336
Loss in iteration 6 : 0.35944558573542956
Loss in iteration 7 : 0.34317645625514054
Loss in iteration 8 : 0.3711317589246943
Loss in iteration 9 : 0.3967796776269449
Loss in iteration 10 : 0.3937668221756275
Loss in iteration 11 : 0.37366527595232313
Loss in iteration 12 : 0.360097841279962
Loss in iteration 13 : 0.3627500348372427
Loss in iteration 14 : 0.37410725948022394
Loss in iteration 15 : 0.3815482497404133
Loss in iteration 16 : 0.3783739602473529
Loss in iteration 17 : 0.36714312516482256
Loss in iteration 18 : 0.35619785343351207
Loss in iteration 19 : 0.35253075733057443
Loss in iteration 20 : 0.3559847649509396
Loss in iteration 21 : 0.35998173995008464
Loss in iteration 22 : 0.35838597696883934
Loss in iteration 23 : 0.35142617197238324
Loss in iteration 24 : 0.34472049636941143
Loss in iteration 25 : 0.34294892604265825
Loss in iteration 26 : 0.34523217513975235
Loss in iteration 27 : 0.3468562949463448
Loss in iteration 28 : 0.34456802902231237
Loss in iteration 29 : 0.339588470747796
Loss in iteration 30 : 0.3358323127132913
Loss in iteration 31 : 0.3354719417791482
Loss in iteration 32 : 0.3366161418014664
Loss in iteration 33 : 0.33590307341950965
Loss in iteration 34 : 0.3327471310328228
Loss in iteration 35 : 0.32970909559809736
Loss in iteration 36 : 0.3289334152381646
Loss in iteration 37 : 0.3297113186836688
Loss in iteration 38 : 0.3298765447756166
Loss in iteration 39 : 0.3285539279120729
Loss in iteration 40 : 0.32689302620290384
Loss in iteration 41 : 0.32636891881825775
Loss in iteration 42 : 0.32694131290034284
Loss in iteration 43 : 0.3272901436620893
Loss in iteration 44 : 0.32662179881763437
Loss in iteration 45 : 0.32555810703213045
Loss in iteration 46 : 0.32510986486251703
Loss in iteration 47 : 0.32536417236403714
Loss in iteration 48 : 0.32556634307239873
Loss in iteration 49 : 0.3252032490643342
Loss in iteration 50 : 0.32458341636184024
Loss in iteration 51 : 0.3243175325987123
Loss in iteration 52 : 0.3245095968617161
Loss in iteration 53 : 0.3247018837026453
Loss in iteration 54 : 0.32453888166635675
Loss in iteration 55 : 0.3241918191731192
Loss in iteration 56 : 0.32403640635588093
Loss in iteration 57 : 0.3241346877290096
Loss in iteration 58 : 0.32421685570104375
Loss in iteration 59 : 0.324080617611428
Loss in iteration 60 : 0.3238283958175494
Loss in iteration 61 : 0.3236853457804788
Loss in iteration 62 : 0.32370060360571706
Loss in iteration 63 : 0.3237219526322027
Loss in iteration 64 : 0.32362758039209727
Loss in iteration 65 : 0.3234760150655616
Loss in iteration 66 : 0.3233990269990771
Loss in iteration 67 : 0.3234225246260303
Loss in iteration 68 : 0.32345406562441004
Loss in iteration 69 : 0.3234181399168831
Loss in iteration 70 : 0.3233432002076058
Loss in iteration 71 : 0.32330402809057635
Loss in iteration 72 : 0.3233180045691566
Loss in iteration 73 : 0.3233322661661666
Loss in iteration 74 : 0.3233019398402288
Loss in iteration 75 : 0.32324500838058146
Loss in iteration 76 : 0.3232086206330689
Loss in iteration 77 : 0.3232054735263517
Loss in iteration 78 : 0.3232055757661139
Loss in iteration 79 : 0.3231827580947607
Loss in iteration 80 : 0.3231472852356399
Loss in iteration 81 : 0.323126497764355
Loss in iteration 82 : 0.32312721536041905
Loss in iteration 83 : 0.3231304746729675
Loss in iteration 84 : 0.32311966792295005
Loss in iteration 85 : 0.32310056617200733
Loss in iteration 86 : 0.32308931198324126
Loss in iteration 87 : 0.3230896221819258
Loss in iteration 88 : 0.3230901959925582
Loss in iteration 89 : 0.32308176581240167
Loss in iteration 90 : 0.3230684746152736
Loss in iteration 91 : 0.32306021257256096
Loss in iteration 92 : 0.3230590860186733
Loss in iteration 93 : 0.3230582410531053
Loss in iteration 94 : 0.3230522819123723
Loss in iteration 95 : 0.32304385187015994
Loss in iteration 96 : 0.32303866344040844
Loss in iteration 97 : 0.3230375056862913
Loss in iteration 98 : 0.32303604198134744
Loss in iteration 99 : 0.32303120751437475
Loss in iteration 100 : 0.32302481102826325
Loss in iteration 101 : 0.3230202699461085
Loss in iteration 102 : 0.3230179256672517
Loss in iteration 103 : 0.32301520107212706
Loss in iteration 104 : 0.32301050323271596
Loss in iteration 105 : 0.3230051461878824
Loss in iteration 106 : 0.32300116671953044
Loss in iteration 107 : 0.3229986051070711
Loss in iteration 108 : 0.32299585483212956
Loss in iteration 109 : 0.32299205493283983
Loss in iteration 110 : 0.3229880580858755
Loss in iteration 111 : 0.32298499210294185
Loss in iteration 112 : 0.32298272401417677
Loss in iteration 113 : 0.32298023890431515
Loss in iteration 114 : 0.32297709383425494
Loss in iteration 115 : 0.3229738622300208
Loss in iteration 116 : 0.32297117948105497
Loss in iteration 117 : 0.3229688926816411
Loss in iteration 118 : 0.322966401276415
Loss in iteration 119 : 0.3229635215520918
Loss in iteration 120 : 0.32296065026450604
Loss in iteration 121 : 0.32295814313331295
Loss in iteration 122 : 0.3229558628242175
Loss in iteration 123 : 0.32295345387342
Loss in iteration 124 : 0.3229508497030879
Loss in iteration 125 : 0.3229483049342698
Loss in iteration 126 : 0.3229459991554521
Loss in iteration 127 : 0.32294381176947873
Loss in iteration 128 : 0.3229415319657759
Loss in iteration 129 : 0.3229391481682251
Loss in iteration 130 : 0.3229368229893362
Loss in iteration 131 : 0.3229346453076172
Loss in iteration 132 : 0.32293252672503453
Loss in iteration 133 : 0.32293035170097834
Loss in iteration 134 : 0.32292813606360615
Loss in iteration 135 : 0.3229259826410736
Loss in iteration 136 : 0.32292392973135514
Loss in iteration 137 : 0.32292191225920563
Loss in iteration 138 : 0.32291986598843087
Loss in iteration 139 : 0.3229178104586349
Loss in iteration 140 : 0.322915805052489
Loss in iteration 141 : 0.3229138603180054
Loss in iteration 142 : 0.32291193085355435
Loss in iteration 143 : 0.3229099843556354
Loss in iteration 144 : 0.32290804102814163
Loss in iteration 145 : 0.3229061363861586
Loss in iteration 146 : 0.32290427108439446
Loss in iteration 147 : 0.3229024168408045
Loss in iteration 148 : 0.3229005599065096
Loss in iteration 149 : 0.32289871691176014
Loss in iteration 150 : 0.3228969069456796
Loss in iteration 151 : 0.32289512527480013
Loss in iteration 152 : 0.3228933535816523
Loss in iteration 153 : 0.32289158631434
Loss in iteration 154 : 0.3228898349485167
Loss in iteration 155 : 0.32288810864812423
Loss in iteration 156 : 0.3228864017406598
Loss in iteration 157 : 0.3228847035995469
Loss in iteration 158 : 0.3228830134878386
Loss in iteration 159 : 0.3228813394489244
Loss in iteration 160 : 0.3228796856567562
Loss in iteration 161 : 0.3228780473628484
Loss in iteration 162 : 0.3228764188019522
Loss in iteration 163 : 0.3228748009526797
Loss in iteration 164 : 0.3228731986853247
Loss in iteration 165 : 0.32287161312825857
Loss in iteration 166 : 0.32287004048520335
Loss in iteration 167 : 0.32286847772771504
Loss in iteration 168 : 0.3228669261935425
Loss in iteration 169 : 0.3228653886177004
Loss in iteration 170 : 0.3228638648568411
Loss in iteration 171 : 0.32286235236145105
Loss in iteration 172 : 0.3228608498693589
Loss in iteration 173 : 0.3228593586887725
Loss in iteration 174 : 0.3228578802609817
Loss in iteration 175 : 0.3228564140050372
Loss in iteration 176 : 0.32285495832247063
Loss in iteration 177 : 0.322853512796804
Loss in iteration 178 : 0.32285207835789953
Testing accuracy  of updater 6 on alg 0 with rate 0.14 = 0.8499477919046742, training accuracy 0.8490786240786241, time elapsed: 3599 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5480305695618778
Loss in iteration 3 : 0.4711603121099885
Loss in iteration 4 : 0.46826564463283094
Loss in iteration 5 : 0.46076686912683845
Loss in iteration 6 : 0.4236877634402769
Loss in iteration 7 : 0.3751100651817691
Loss in iteration 8 : 0.34484771204949854
Loss in iteration 9 : 0.3454155872481556
Loss in iteration 10 : 0.36395917968984204
Loss in iteration 11 : 0.37894007905427657
Loss in iteration 12 : 0.3789847783254386
Loss in iteration 13 : 0.3672870708658692
Loss in iteration 14 : 0.35419864636226667
Loss in iteration 15 : 0.3478708580303625
Loss in iteration 16 : 0.349733876543674
Loss in iteration 17 : 0.35579117151746587
Loss in iteration 18 : 0.3606096564846339
Loss in iteration 19 : 0.36069715457414825
Loss in iteration 20 : 0.35587376594089054
Loss in iteration 21 : 0.3487154248969727
Loss in iteration 22 : 0.34274112500010134
Loss in iteration 23 : 0.34029864656445047
Loss in iteration 24 : 0.34127876987715
Loss in iteration 25 : 0.34345630465157945
Loss in iteration 26 : 0.34420737410365915
Loss in iteration 27 : 0.3423285529295862
Loss in iteration 28 : 0.3386631863751559
Loss in iteration 29 : 0.33522006934088916
Loss in iteration 30 : 0.33359168260718575
Loss in iteration 31 : 0.33389977902336315
Loss in iteration 32 : 0.3349495223061759
Loss in iteration 33 : 0.3352763337663558
Loss in iteration 34 : 0.334193244825704
Loss in iteration 35 : 0.33213554767861597
Loss in iteration 36 : 0.33019011327046205
Loss in iteration 37 : 0.329226496663181
Loss in iteration 38 : 0.3292767696938448
Loss in iteration 39 : 0.32961438633281354
Loss in iteration 40 : 0.32942491717602557
Loss in iteration 41 : 0.32847014841571015
Loss in iteration 42 : 0.32719323104463305
Loss in iteration 43 : 0.32625298110639617
Loss in iteration 44 : 0.32596447340033124
Loss in iteration 45 : 0.3261179004501063
Loss in iteration 46 : 0.3262427830456387
Loss in iteration 47 : 0.3260179109509169
Loss in iteration 48 : 0.32548548701001473
Loss in iteration 49 : 0.32494811543168867
Loss in iteration 50 : 0.3246830464226338
Loss in iteration 51 : 0.3247221516896405
Loss in iteration 52 : 0.32486198652733167
Loss in iteration 53 : 0.324863828592079
Loss in iteration 54 : 0.3246515050692105
Loss in iteration 55 : 0.32434304420331933
Loss in iteration 56 : 0.324117889087692
Loss in iteration 57 : 0.32405924549914245
Loss in iteration 58 : 0.32410539551411444
Loss in iteration 59 : 0.3241276477523758
Loss in iteration 60 : 0.32404686557141515
Loss in iteration 61 : 0.32388808825839627
Loss in iteration 62 : 0.3237414981133416
Loss in iteration 63 : 0.3236776352847591
Loss in iteration 64 : 0.3236929639732116
Loss in iteration 65 : 0.3237252272437728
Loss in iteration 66 : 0.32371453021550495
Loss in iteration 67 : 0.3236516887197647
Loss in iteration 68 : 0.32357563883238744
Loss in iteration 69 : 0.32353131771043264
Loss in iteration 70 : 0.3235306366856279
Loss in iteration 71 : 0.3235483850834733
Loss in iteration 72 : 0.3235495923396885
Loss in iteration 73 : 0.3235197271423825
Loss in iteration 74 : 0.3234725435757397
Loss in iteration 75 : 0.3234334504177623
Loss in iteration 76 : 0.3234165293380249
Loss in iteration 77 : 0.3234151848261035
Loss in iteration 78 : 0.3234116526363021
Loss in iteration 79 : 0.3233940487709055
Loss in iteration 80 : 0.32336535256997195
Loss in iteration 81 : 0.3233382887878553
Loss in iteration 82 : 0.32332301164339156
Loss in iteration 83 : 0.3233192076190977
Loss in iteration 84 : 0.32331831209828427
Loss in iteration 85 : 0.3233120531280917
Loss in iteration 86 : 0.32329907385152434
Loss in iteration 87 : 0.3232845709310338
Loss in iteration 88 : 0.323274583711716
Loss in iteration 89 : 0.3232706578358234
Loss in iteration 90 : 0.323269246490669
Loss in iteration 91 : 0.32326549210323047
Loss in iteration 92 : 0.3232574411689584
Loss in iteration 93 : 0.32324708997148965
Loss in iteration 94 : 0.3232379673091326
Loss in iteration 95 : 0.32323191516959104
Loss in iteration 96 : 0.32322790078819397
Loss in iteration 97 : 0.3232234600057398
Loss in iteration 98 : 0.3232170636384099
Loss in iteration 99 : 0.32320925460508876
Loss in iteration 100 : 0.3232018335277287
Loss in iteration 101 : 0.32319613305777056
Loss in iteration 102 : 0.32319199246893715
Loss in iteration 103 : 0.3231881653002946
Loss in iteration 104 : 0.32318355723469006
Loss in iteration 105 : 0.32317810201967007
Loss in iteration 106 : 0.32317260601304293
Loss in iteration 107 : 0.3231678859150075
Loss in iteration 108 : 0.32316406099341405
Loss in iteration 109 : 0.3231605694238445
Loss in iteration 110 : 0.3231567597529843
Loss in iteration 111 : 0.32315245061564246
Loss in iteration 112 : 0.3231479917279046
Loss in iteration 113 : 0.3231438667491768
Loss in iteration 114 : 0.32314025796492263
Loss in iteration 115 : 0.323136947664357
Loss in iteration 116 : 0.3231335769993108
Loss in iteration 117 : 0.3231299759710811
Loss in iteration 118 : 0.32312627054861054
Loss in iteration 119 : 0.32312271706965656
Loss in iteration 120 : 0.3231194558075742
Loss in iteration 121 : 0.32311640892144533
Loss in iteration 122 : 0.3231133830437985
Loss in iteration 123 : 0.32311025114132313
Loss in iteration 124 : 0.32310704408379487
Loss in iteration 125 : 0.3231038908508118
Loss in iteration 126 : 0.32310088670880216
Loss in iteration 127 : 0.32309801544574906
Loss in iteration 128 : 0.32309518217041144
Loss in iteration 129 : 0.3230923080304084
Loss in iteration 130 : 0.3230893940463446
Loss in iteration 131 : 0.323086504878623
Loss in iteration 132 : 0.3230837012197456
Loss in iteration 133 : 0.3230809883590717
Loss in iteration 134 : 0.32307832203547837
Loss in iteration 135 : 0.3230756557852184
Loss in iteration 136 : 0.323072981234513
Loss in iteration 137 : 0.32307032790375506
Loss in iteration 138 : 0.3230677302080695
Loss in iteration 139 : 0.323065196231789
Loss in iteration 140 : 0.32306270494299527
Loss in iteration 141 : 0.3230602290132354
Loss in iteration 142 : 0.32305775877964993
Loss in iteration 143 : 0.3230553064201628
Loss in iteration 144 : 0.3230528904361552
Loss in iteration 145 : 0.3230505175977406
Loss in iteration 146 : 0.323048178518813
Loss in iteration 147 : 0.32304585817935383
Loss in iteration 148 : 0.32304354946863423
Loss in iteration 149 : 0.32304125740249856
Loss in iteration 150 : 0.32303899206794484
Loss in iteration 151 : 0.32303675852085956
Loss in iteration 152 : 0.3230345529914629
Loss in iteration 153 : 0.32303236751742487
Loss in iteration 154 : 0.32303019737704974
Loss in iteration 155 : 0.3230280442949368
Loss in iteration 156 : 0.32302591339682846
Loss in iteration 157 : 0.3230238077404418
Loss in iteration 158 : 0.323021725665812
Loss in iteration 159 : 0.3230196627677667
Loss in iteration 160 : 0.32301761590746586
Loss in iteration 161 : 0.3230155853710365
Loss in iteration 162 : 0.32301357363155037
Loss in iteration 163 : 0.32301158243968603
Loss in iteration 164 : 0.3230096111157176
Loss in iteration 165 : 0.3230076573233316
Loss in iteration 166 : 0.32300571918031407
Loss in iteration 167 : 0.32300379659311584
Loss in iteration 168 : 0.3230018907775726
Loss in iteration 169 : 0.3230000027215238
Loss in iteration 170 : 0.3229981321401602
Loss in iteration 171 : 0.3229962777525393
Loss in iteration 172 : 0.32299443838626396
Loss in iteration 173 : 0.3229926137768313
Loss in iteration 174 : 0.3229908044102355
Loss in iteration 175 : 0.32298901072749825
Loss in iteration 176 : 0.32298723251768235
Loss in iteration 177 : 0.32298546900559033
Loss in iteration 178 : 0.32298371942900334
Loss in iteration 179 : 0.32298198350259827
Loss in iteration 180 : 0.3229802613777441
Loss in iteration 181 : 0.32297855322841224
Loss in iteration 182 : 0.3229768589019996
Loss in iteration 183 : 0.3229751779340589
Loss in iteration 184 : 0.3229735098452551
Loss in iteration 185 : 0.3229718544016627
Loss in iteration 186 : 0.322970211611477
Loss in iteration 187 : 0.3229685815092114
Loss in iteration 188 : 0.3229669639594943
Loss in iteration 189 : 0.3229653586521476
Loss in iteration 190 : 0.32296376525714404
Loss in iteration 191 : 0.32296218357051576
Loss in iteration 192 : 0.3229606135219827
Loss in iteration 193 : 0.32295905506333633
Testing accuracy  of updater 6 on alg 0 with rate 0.08000000000000002 = 0.8501934770591487, training accuracy 0.8488943488943489, time elapsed: 3527 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6455674998359743
Loss in iteration 3 : 0.5796946123072197
Loss in iteration 4 : 0.5223722189883339
Loss in iteration 5 : 0.4860672283787021
Loss in iteration 6 : 0.46876164342684434
Loss in iteration 7 : 0.46171714721336937
Loss in iteration 8 : 0.4565730061593935
Loss in iteration 9 : 0.4482398911539495
Loss in iteration 10 : 0.4349808519951361
Loss in iteration 11 : 0.41754349526511586
Loss in iteration 12 : 0.3981650508296537
Loss in iteration 13 : 0.37962834655938427
Loss in iteration 14 : 0.3644090279874585
Loss in iteration 15 : 0.3540229024592618
Loss in iteration 16 : 0.348720945320691
Loss in iteration 17 : 0.3475969272696137
Loss in iteration 18 : 0.3490224757267497
Loss in iteration 19 : 0.35121355325966186
Loss in iteration 20 : 0.3527210471536772
Loss in iteration 21 : 0.35271286745457014
Loss in iteration 22 : 0.35101931529074
Loss in iteration 23 : 0.34799478193732575
Loss in iteration 24 : 0.344285158258178
Loss in iteration 25 : 0.34058757978912296
Loss in iteration 26 : 0.33746406792424666
Loss in iteration 27 : 0.3352385154231799
Loss in iteration 28 : 0.3339775234154893
Loss in iteration 29 : 0.3335355838025514
Loss in iteration 30 : 0.33363604479254083
Loss in iteration 31 : 0.3339599479678525
Loss in iteration 32 : 0.3342217210454131
Loss in iteration 33 : 0.3342198805859845
Loss in iteration 34 : 0.33385932333135565
Loss in iteration 35 : 0.3331479622887752
Loss in iteration 36 : 0.3321741596303279
Loss in iteration 37 : 0.3310730505408258
Loss in iteration 38 : 0.3299899916145117
Loss in iteration 39 : 0.32904840289261816
Loss in iteration 40 : 0.3283274372161798
Loss in iteration 41 : 0.3278524127916489
Loss in iteration 42 : 0.327598118934816
Loss in iteration 43 : 0.327502452996998
Loss in iteration 44 : 0.32748593351785465
Loss in iteration 45 : 0.327471891264765
Loss in iteration 46 : 0.32740268599811245
Loss in iteration 47 : 0.3272488867273615
Loss in iteration 48 : 0.32701046714254517
Loss in iteration 49 : 0.3267110984324865
Loss in iteration 50 : 0.3263880666024661
Loss in iteration 51 : 0.32608093169095687
Loss in iteration 52 : 0.32582178113242644
Loss in iteration 53 : 0.3256290283522869
Loss in iteration 54 : 0.3255055111813387
Loss in iteration 55 : 0.3254405021193789
Loss in iteration 56 : 0.3254144188029902
Loss in iteration 57 : 0.32540464783377665
Loss in iteration 58 : 0.3253909598615556
Loss in iteration 59 : 0.32535938729700503
Loss in iteration 60 : 0.3253039978271137
Loss in iteration 61 : 0.3252265679456194
Loss in iteration 62 : 0.32513461779518105
Loss in iteration 63 : 0.3250385399036136
Loss in iteration 64 : 0.324948619895184
Loss in iteration 65 : 0.3248726305871283
Loss in iteration 66 : 0.32481443686458716
Loss in iteration 67 : 0.324773749031177
Loss in iteration 68 : 0.32474688015823494
Loss in iteration 69 : 0.32472815828660456
Loss in iteration 70 : 0.3247115523048802
Loss in iteration 71 : 0.32469209559307477
Loss in iteration 72 : 0.32466681031395006
Loss in iteration 73 : 0.3246350045110455
Loss in iteration 74 : 0.32459798492329783
Loss in iteration 75 : 0.3245583590077457
Loss in iteration 76 : 0.32451916532774644
Loss in iteration 77 : 0.3244830679087267
Loss in iteration 78 : 0.324451791021637
Loss in iteration 79 : 0.3244258809122057
Loss in iteration 80 : 0.3244047877864664
Loss in iteration 81 : 0.32438718775845743
Loss in iteration 82 : 0.32437142400253766
Loss in iteration 83 : 0.3243559419271133
Loss in iteration 84 : 0.32433961864146416
Loss in iteration 85 : 0.3243219305072946
Loss in iteration 86 : 0.32430295061070435
Loss in iteration 87 : 0.3242832087213658
Loss in iteration 88 : 0.3242634718746586
Loss in iteration 89 : 0.3242445111844865
Loss in iteration 90 : 0.3242269115374006
Loss in iteration 91 : 0.3242109604779706
Loss in iteration 92 : 0.32419662757489365
Loss in iteration 93 : 0.3241836224433416
Loss in iteration 94 : 0.3241715033957235
Loss in iteration 95 : 0.32415980194811145
Loss in iteration 96 : 0.3241481310290233
Loss in iteration 97 : 0.3241362544456792
Loss in iteration 98 : 0.3241241084048709
Loss in iteration 99 : 0.32411177889055054
Loss in iteration 100 : 0.3240994484224307
Loss in iteration 101 : 0.32408733044669896
Loss in iteration 102 : 0.3240756091350597
Loss in iteration 103 : 0.32406439776929463
Loss in iteration 104 : 0.3240537220275365
Loss in iteration 105 : 0.3240435274196314
Loss in iteration 106 : 0.324033704556214
Loss in iteration 107 : 0.3240241229122476
Loss in iteration 108 : 0.32401466349467734
Loss in iteration 109 : 0.32400524288182325
Loss in iteration 110 : 0.3239958245489627
Loss in iteration 111 : 0.32398641715912424
Loss in iteration 112 : 0.3239770626357022
Loss in iteration 113 : 0.3239678187237303
Loss in iteration 114 : 0.32395874117631773
Loss in iteration 115 : 0.32394986984047625
Loss in iteration 116 : 0.32394122120308033
Loss in iteration 117 : 0.32393278795359565
Loss in iteration 118 : 0.32392454435267987
Loss in iteration 119 : 0.323916455040323
Loss in iteration 120 : 0.32390848453697285
Loss in iteration 121 : 0.32390060503495477
Loss in iteration 122 : 0.32389280093119205
Loss in iteration 123 : 0.32388506961888086
Loss in iteration 124 : 0.32387741904080747
Loss in iteration 125 : 0.3238698631898629
Loss in iteration 126 : 0.32386241701455626
Loss in iteration 127 : 0.3238550920597292
Loss in iteration 128 : 0.32384789374950407
Loss in iteration 129 : 0.32384082065718883
Loss in iteration 130 : 0.32383386556636956
Loss in iteration 131 : 0.323827017734174
Loss in iteration 132 : 0.3238202655889767
Loss in iteration 133 : 0.3238135991336827
Loss in iteration 134 : 0.32380701153226704
Loss in iteration 135 : 0.3238004996509509
Loss in iteration 136 : 0.32379406361932495
Loss in iteration 137 : 0.32378770570062443
Loss in iteration 138 : 0.3237814288726751
Loss in iteration 139 : 0.32377523551634346
Loss in iteration 140 : 0.32376912650830536
Loss in iteration 141 : 0.3237631008622815
Loss in iteration 142 : 0.3237571559033037
Loss in iteration 143 : 0.32375128783350526
Loss in iteration 144 : 0.3237454924791633
Loss in iteration 145 : 0.32373976600382676
Loss in iteration 146 : 0.323734105420381
Loss in iteration 147 : 0.32372850881458454
Loss in iteration 148 : 0.32372297527899774
Loss in iteration 149 : 0.3237175046266363
Loss in iteration 150 : 0.323712096994398
Loss in iteration 151 : 0.32370675245270647
Loss in iteration 152 : 0.32370147071454114
Loss in iteration 153 : 0.3236962509956506
Loss in iteration 154 : 0.3236910920311459
Loss in iteration 155 : 0.3236859922146442
Loss in iteration 156 : 0.3236809498025953
Loss in iteration 157 : 0.3236759631211054
Loss in iteration 158 : 0.32367103072379527
Loss in iteration 159 : 0.32366615147073935
Loss in iteration 160 : 0.3236613245234847
Loss in iteration 161 : 0.32365654927270393
Loss in iteration 162 : 0.32365182522846087
Loss in iteration 163 : 0.32364715190671955
Loss in iteration 164 : 0.32364252874046745
Loss in iteration 165 : 0.3236379550324619
Loss in iteration 166 : 0.323633429953323
Loss in iteration 167 : 0.32362895257684443
Loss in iteration 168 : 0.3236245219367164
Loss in iteration 169 : 0.3236201370866104
Loss in iteration 170 : 0.32361579714807504
Loss in iteration 171 : 0.323611501336537
Loss in iteration 172 : 0.3236072489630567
Loss in iteration 173 : 0.32360303941572566
Loss in iteration 174 : 0.3235988721290989
Loss in iteration 175 : 0.323594746551297
Loss in iteration 176 : 0.3235906621173116
Loss in iteration 177 : 0.3235866182338511
Loss in iteration 178 : 0.32358261427726404
Loss in iteration 179 : 0.32357864960254434
Loss in iteration 180 : 0.32357472355899225
Loss in iteration 181 : 0.3235708355073602
Loss in iteration 182 : 0.3235669848338522
Loss in iteration 183 : 0.3235631709579925
Loss in iteration 184 : 0.32355939333349604
Loss in iteration 185 : 0.3235556514431364
Loss in iteration 186 : 0.32355194478995736
Loss in iteration 187 : 0.32354827288762433
Loss in iteration 188 : 0.32354463525241706
Loss in iteration 189 : 0.3235410313985189
Loss in iteration 190 : 0.3235374608370724
Loss in iteration 191 : 0.32353392307849266
Loss in iteration 192 : 0.32353041763680296
Loss in iteration 193 : 0.32352694403446364
Loss in iteration 194 : 0.3235235018063279
Loss in iteration 195 : 0.3235200905018766
Loss in iteration 196 : 0.3235167096853882
Loss in iteration 197 : 0.3235133589343952
Loss in iteration 198 : 0.3235100378370307
Loss in iteration 199 : 0.32350674598913204
Loss in iteration 200 : 0.32350348299180803
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.8499477919046742, training accuracy 0.8492321867321867, time elapsed: 4459 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 32.24039829303136
Loss in iteration 3 : 25.461149513710687
Loss in iteration 4 : 7.055606148155897
Loss in iteration 5 : 12.753591897201826
Loss in iteration 6 : 18.261194902938946
Loss in iteration 7 : 10.936951907375047
Loss in iteration 8 : 7.43276280830362
Loss in iteration 9 : 9.810652066799923
Loss in iteration 10 : 12.408087782228723
Loss in iteration 11 : 12.655697031142102
Loss in iteration 12 : 10.686156225734681
Loss in iteration 13 : 8.221158206682745
Loss in iteration 14 : 7.27860426557539
Loss in iteration 15 : 8.274599293286549
Loss in iteration 16 : 9.529135671687797
Loss in iteration 17 : 9.170357400496849
Loss in iteration 18 : 7.449186221839996
Loss in iteration 19 : 6.159358900300887
Loss in iteration 20 : 6.372170382716482
Loss in iteration 21 : 7.197794880019199
Loss in iteration 22 : 7.187791627820735
Loss in iteration 23 : 6.049376447245913
Loss in iteration 24 : 4.884129246622061
Loss in iteration 25 : 5.124605022964602
Loss in iteration 26 : 5.698130998127907
Loss in iteration 27 : 5.124621836412529
Loss in iteration 28 : 3.9101653347084735
Loss in iteration 29 : 3.9494745703285465
Loss in iteration 30 : 4.3827867449820905
Loss in iteration 31 : 3.768380854798267
Loss in iteration 32 : 2.900915719790044
Loss in iteration 33 : 3.433455998044336
Loss in iteration 34 : 3.212901137277427
Loss in iteration 35 : 2.347511012612317
Loss in iteration 36 : 2.8934925189783165
Loss in iteration 37 : 2.335321324528548
Loss in iteration 38 : 2.1384558795387307
Loss in iteration 39 : 2.216449339696478
Loss in iteration 40 : 1.8876907090835577
Loss in iteration 41 : 1.7551248602827503
Loss in iteration 42 : 2.0211450486623566
Loss in iteration 43 : 1.3904786493049202
Loss in iteration 44 : 1.8943651718412382
Loss in iteration 45 : 1.1396133781671418
Loss in iteration 46 : 1.8399639485883437
Loss in iteration 47 : 1.576220064430402
Loss in iteration 48 : 1.0959841649685083
Loss in iteration 49 : 2.0988506938236315
Loss in iteration 50 : 1.192900557373038
Loss in iteration 51 : 1.4419987088681678
Loss in iteration 52 : 1.4591639261918117
Loss in iteration 53 : 1.1703164785376867
Loss in iteration 54 : 1.5223542182732952
Loss in iteration 55 : 0.9728177325438837
Loss in iteration 56 : 1.5580954265401303
Loss in iteration 57 : 1.2796365550373772
Loss in iteration 58 : 0.8371089202229969
Loss in iteration 59 : 1.8937356619205528
Loss in iteration 60 : 1.9127389017496388
Loss in iteration 61 : 1.0076309509533163
Loss in iteration 62 : 2.5262724373378154
Loss in iteration 63 : 0.9620896299214226
Loss in iteration 64 : 1.8649011404413167
Loss in iteration 65 : 0.9927900105346636
Loss in iteration 66 : 1.7835304118201967
Loss in iteration 67 : 0.9166155816823693
Loss in iteration 68 : 1.552008793259291
Loss in iteration 69 : 0.8450011103653149
Loss in iteration 70 : 1.6354140221626583
Loss in iteration 71 : 1.0942024700343398
Loss in iteration 72 : 0.966787414252553
Loss in iteration 73 : 1.5596788307384932
Loss in iteration 74 : 0.8512168799181019
Loss in iteration 75 : 0.9391491016196353
Loss in iteration 76 : 1.2185988286544698
Loss in iteration 77 : 0.7389521904036598
Loss in iteration 78 : 0.6984776791464178
Loss in iteration 79 : 1.4618730854528286
Loss in iteration 80 : 2.4520460668925117
Loss in iteration 81 : 1.0628907931802314
Loss in iteration 82 : 4.0237158445293915
Loss in iteration 83 : 0.876960041114169
Loss in iteration 84 : 2.294295813097507
Loss in iteration 85 : 2.1879134477666633
Loss in iteration 86 : 1.3082468418924427
Loss in iteration 87 : 2.152050791363518
Loss in iteration 88 : 2.051463615808681
Loss in iteration 89 : 1.3867241224893891
Loss in iteration 90 : 1.8911967042212523
Loss in iteration 91 : 1.8380168711032931
Loss in iteration 92 : 1.1991348333040552
Loss in iteration 93 : 1.6402558933554425
Loss in iteration 94 : 1.3388321308148936
Loss in iteration 95 : 1.0823814460420325
Loss in iteration 96 : 1.3216909737167828
Loss in iteration 97 : 0.7208359727206566
Loss in iteration 98 : 1.253793327118517
Loss in iteration 99 : 1.0861477768956762
Loss in iteration 100 : 0.6805940687152482
Loss in iteration 101 : 1.944702841732066
Loss in iteration 102 : 1.2878400802775427
Loss in iteration 103 : 0.9282878840181177
Loss in iteration 104 : 1.2922926298291812
Loss in iteration 105 : 0.9211799675844715
Loss in iteration 106 : 1.0947833515110974
Loss in iteration 107 : 1.0067592657466728
Loss in iteration 108 : 0.8716777831369277
Loss in iteration 109 : 0.8892515811500799
Loss in iteration 110 : 0.9450573695468004
Loss in iteration 111 : 0.6721145163256836
Loss in iteration 112 : 1.3992916623761702
Loss in iteration 113 : 1.2463565538490413
Loss in iteration 114 : 0.8424431376919572
Loss in iteration 115 : 1.608735985470115
Loss in iteration 116 : 0.6718704712619741
Loss in iteration 117 : 1.2109899511211324
Loss in iteration 118 : 0.6765204491746111
Loss in iteration 119 : 1.1117915776692218
Loss in iteration 120 : 0.694425666043893
Loss in iteration 121 : 0.8162146781701803
Loss in iteration 122 : 0.919365110003813
Loss in iteration 123 : 0.5131057194756878
Loss in iteration 124 : 0.6005355947438271
Loss in iteration 125 : 1.0514272201958035
Loss in iteration 126 : 1.6333582471457475
Loss in iteration 127 : 0.6101817191828374
Loss in iteration 128 : 2.1609900009298064
Loss in iteration 129 : 0.9230259239971462
Loss in iteration 130 : 1.543137741454157
Loss in iteration 131 : 0.8494240016585395
Loss in iteration 132 : 1.5139912976476235
Loss in iteration 133 : 0.8954010401387218
Loss in iteration 134 : 1.2647229959949304
Loss in iteration 135 : 0.9686738268102809
Loss in iteration 136 : 1.0726517723223679
Loss in iteration 137 : 0.8078095623988867
Loss in iteration 138 : 1.130374712442687
Loss in iteration 139 : 0.60990553759321
Loss in iteration 140 : 1.6939860273911873
Loss in iteration 141 : 1.6081127748498096
Loss in iteration 142 : 1.062720702907158
Loss in iteration 143 : 1.7434183376916117
Loss in iteration 144 : 0.8762178841380314
Loss in iteration 145 : 1.3642002411486736
Loss in iteration 146 : 1.1091305270938359
Loss in iteration 147 : 1.0839158024645645
Loss in iteration 148 : 1.08225811091619
Loss in iteration 149 : 0.9776541809319647
Loss in iteration 150 : 0.9415627216094384
Loss in iteration 151 : 0.9611030207487669
Loss in iteration 152 : 0.5959132975569507
Loss in iteration 153 : 1.0115587725729807
Loss in iteration 154 : 0.554506950892047
Loss in iteration 155 : 0.444979399076877
Loss in iteration 156 : 0.8984872370874233
Loss in iteration 157 : 2.0172471858934413
Loss in iteration 158 : 2.383072232988747
Loss in iteration 159 : 2.0617340152226733
Loss in iteration 160 : 1.2431158831578972
Loss in iteration 161 : 2.5583689141442947
Loss in iteration 162 : 1.5401287854387469
Loss in iteration 163 : 1.7801811453178922
Loss in iteration 164 : 2.1525761227797218
Loss in iteration 165 : 1.6520135703234344
Loss in iteration 166 : 1.453240239279791
Loss in iteration 167 : 1.8768571104532965
Loss in iteration 168 : 1.3221873175014203
Loss in iteration 169 : 1.5163692843347087
Loss in iteration 170 : 1.4658801498669574
Loss in iteration 171 : 1.0270132602158972
Loss in iteration 172 : 1.3600627590518357
Loss in iteration 173 : 0.825803880642457
Loss in iteration 174 : 1.1306656766414498
Loss in iteration 175 : 0.682544511300129
Loss in iteration 176 : 1.0543171999389895
Loss in iteration 177 : 1.590545938626831
Loss in iteration 178 : 0.5585283112897858
Loss in iteration 179 : 1.7387834435830252
Loss in iteration 180 : 1.154904989796408
Loss in iteration 181 : 0.8771277677235658
Loss in iteration 182 : 1.1706738616801466
Loss in iteration 183 : 0.7782480739665829
Loss in iteration 184 : 1.0187993617970044
Loss in iteration 185 : 0.7089306876298169
Loss in iteration 186 : 1.0788800158994512
Loss in iteration 187 : 0.6314078410575507
Loss in iteration 188 : 0.7743863596195858
Loss in iteration 189 : 0.8641855549463342
Loss in iteration 190 : 0.673979245532241
Loss in iteration 191 : 0.4732456572945576
Loss in iteration 192 : 0.6882287119215517
Loss in iteration 193 : 1.5442459411559692
Loss in iteration 194 : 0.5079117940603372
Loss in iteration 195 : 0.6505636010733307
Loss in iteration 196 : 1.3759355985627448
Loss in iteration 197 : 0.5724735939145672
Loss in iteration 198 : 1.319367573999449
Loss in iteration 199 : 1.397533332052672
Loss in iteration 200 : 1.098032506946162
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.7459001289847061, training accuracy 0.7475737100737101, time elapsed: 3879 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5352861331537823
Loss in iteration 3 : 0.6588877090207251
Loss in iteration 4 : 0.7667333918616605
Loss in iteration 5 : 0.8343498994503548
Loss in iteration 6 : 0.9160344553020606
Loss in iteration 7 : 0.8798927247686547
Loss in iteration 8 : 0.9003995206091386
Loss in iteration 9 : 0.8829029503591345
Loss in iteration 10 : 0.8374892976050595
Loss in iteration 11 : 0.8343723089640251
Loss in iteration 12 : 0.7490975127187739
Loss in iteration 13 : 0.7336333179351135
Loss in iteration 14 : 0.6477658160714459
Loss in iteration 15 : 0.6273608386249676
Loss in iteration 16 : 0.6198252881676305
Loss in iteration 17 : 0.5515695516859168
Loss in iteration 18 : 0.5203263208085913
Loss in iteration 19 : 0.5711182189327777
Loss in iteration 20 : 0.7480906380993785
Loss in iteration 21 : 0.8093297467251486
Loss in iteration 22 : 0.6234456869253548
Loss in iteration 23 : 0.552338307856355
Loss in iteration 24 : 0.6692419486961642
Loss in iteration 25 : 0.5023125321643126
Loss in iteration 26 : 0.5757697861815455
Loss in iteration 27 : 0.632436927970579
Loss in iteration 28 : 0.4970669795305137
Loss in iteration 29 : 0.4949515017766113
Loss in iteration 30 : 0.5764831564562855
Loss in iteration 31 : 0.5725367436250681
Loss in iteration 32 : 0.4087864907707678
Loss in iteration 33 : 0.42223119147900956
Loss in iteration 34 : 0.6563671867288138
Loss in iteration 35 : 0.7893631441129578
Loss in iteration 36 : 0.6831377117544846
Loss in iteration 37 : 0.43302089422897894
Loss in iteration 38 : 0.657620623745441
Loss in iteration 39 : 0.5736825107370619
Loss in iteration 40 : 0.48284338120612885
Loss in iteration 41 : 0.7230425519618544
Loss in iteration 42 : 0.51538751678455
Loss in iteration 43 : 0.44133963288447486
Loss in iteration 44 : 0.6604666756841211
Loss in iteration 45 : 0.6169389711071248
Loss in iteration 46 : 0.40137461745426894
Loss in iteration 47 : 0.6747601437503193
Loss in iteration 48 : 0.8925085080783314
Loss in iteration 49 : 0.3780806523414911
Loss in iteration 50 : 0.9046127760060236
Loss in iteration 51 : 1.1238265966107672
Loss in iteration 52 : 0.6165394372826094
Loss in iteration 53 : 1.3294941775998135
Loss in iteration 54 : 0.5335249054554324
Loss in iteration 55 : 1.0221327491317147
Loss in iteration 56 : 0.585478708085646
Loss in iteration 57 : 0.980228398016651
Loss in iteration 58 : 0.5657594736118179
Loss in iteration 59 : 0.8990911514662333
Loss in iteration 60 : 0.5228402883762369
Loss in iteration 61 : 0.9508788997651646
Loss in iteration 62 : 0.4793302672186027
Loss in iteration 63 : 0.6688540368616024
Loss in iteration 64 : 0.7197209530680855
Loss in iteration 65 : 0.4421155540847454
Loss in iteration 66 : 0.4128708016477611
Loss in iteration 67 : 0.6966532645468599
Loss in iteration 68 : 0.9919631334251189
Loss in iteration 69 : 0.3998865752744623
Loss in iteration 70 : 0.7310509698342889
Loss in iteration 71 : 0.95039478265011
Loss in iteration 72 : 0.531968120202852
Loss in iteration 73 : 1.144271803389263
Loss in iteration 74 : 0.5409849322264284
Loss in iteration 75 : 0.8398401368995446
Loss in iteration 76 : 0.5585410787956702
Loss in iteration 77 : 0.6897383686599134
Loss in iteration 78 : 0.6497606971339203
Loss in iteration 79 : 0.5034167241117156
Loss in iteration 80 : 0.8142720399071475
Loss in iteration 81 : 0.585549189180416
Loss in iteration 82 : 0.3981208018624679
Loss in iteration 83 : 0.8484112543693558
Loss in iteration 84 : 1.1864163828379737
Loss in iteration 85 : 0.4587753175996216
Loss in iteration 86 : 1.4705923362123159
Loss in iteration 87 : 0.7103635164176527
Loss in iteration 88 : 1.0483617326868027
Loss in iteration 89 : 0.6240783185062888
Loss in iteration 90 : 1.0397607418325647
Loss in iteration 91 : 0.605043121662634
Loss in iteration 92 : 0.9376496197593417
Loss in iteration 93 : 0.5401184564804388
Loss in iteration 94 : 0.9139648747353617
Loss in iteration 95 : 0.49938606268021757
Loss in iteration 96 : 0.6829498848957695
Loss in iteration 97 : 0.6599289778405777
Loss in iteration 98 : 0.42482196597579197
Loss in iteration 99 : 0.4060217369191809
Loss in iteration 100 : 0.695743017868002
Loss in iteration 101 : 1.3670395088868563
Loss in iteration 102 : 0.4230685110434511
Loss in iteration 103 : 1.780050892989356
Loss in iteration 104 : 0.9456578192027989
Loss in iteration 105 : 1.2641574544511291
Loss in iteration 106 : 0.7106522463891346
Loss in iteration 107 : 1.338536413983464
Loss in iteration 108 : 0.8000966205151532
Loss in iteration 109 : 1.0434247276060258
Loss in iteration 110 : 1.0372350780248893
Loss in iteration 111 : 0.7199533094510111
Loss in iteration 112 : 1.0380839746871933
Loss in iteration 113 : 0.6094730470940766
Loss in iteration 114 : 0.9138624640688255
Loss in iteration 115 : 0.5143358898028078
Loss in iteration 116 : 0.9908145659034997
Loss in iteration 117 : 0.7742893471687808
Loss in iteration 118 : 0.5481393805892262
Loss in iteration 119 : 1.1735622738095335
Loss in iteration 120 : 0.7338814997540346
Loss in iteration 121 : 0.6738693072710886
Loss in iteration 122 : 0.917921454285638
Loss in iteration 123 : 0.5066243696724195
Loss in iteration 124 : 0.8179532246173776
Loss in iteration 125 : 0.4738580791051283
Loss in iteration 126 : 0.6904475946477494
Loss in iteration 127 : 0.7639746258120185
Loss in iteration 128 : 0.46777459555523887
Loss in iteration 129 : 1.1105825333764008
Loss in iteration 130 : 1.1497664852408722
Loss in iteration 131 : 0.8424113587099147
Loss in iteration 132 : 1.1689743899296896
Loss in iteration 133 : 0.6804016488112267
Loss in iteration 134 : 0.967775890172547
Loss in iteration 135 : 0.8301616299560804
Loss in iteration 136 : 0.7730146455091792
Loss in iteration 137 : 0.7913574939595429
Loss in iteration 138 : 0.7293928973512775
Loss in iteration 139 : 0.6575957908420782
Loss in iteration 140 : 0.8395176043092392
Loss in iteration 141 : 0.4367263055266044
Loss in iteration 142 : 0.6105786849349057
Loss in iteration 143 : 0.7488067562131704
Loss in iteration 144 : 0.6995477009622859
Loss in iteration 145 : 0.4026118486663582
Loss in iteration 146 : 0.5849777400573454
Loss in iteration 147 : 0.9301741776804199
Loss in iteration 148 : 0.41184520415472053
Loss in iteration 149 : 0.6849799946081471
Loss in iteration 150 : 1.0577619759861983
Loss in iteration 151 : 0.48803891899800106
Loss in iteration 152 : 1.3629257023325658
Loss in iteration 153 : 0.8020009971372426
Loss in iteration 154 : 0.9692996421201764
Loss in iteration 155 : 0.7179384645653266
Loss in iteration 156 : 0.905140730425962
Loss in iteration 157 : 0.6929585307675723
Loss in iteration 158 : 0.8496966974169343
Loss in iteration 159 : 0.584737589137056
Loss in iteration 160 : 0.7215609220900738
Loss in iteration 161 : 0.7341290983600699
Loss in iteration 162 : 0.46393338628812886
Loss in iteration 163 : 1.130509717798352
Loss in iteration 164 : 1.2842163007356404
Loss in iteration 165 : 0.7154800470195808
Loss in iteration 166 : 1.5137960046909313
Loss in iteration 167 : 0.6144596230264493
Loss in iteration 168 : 1.1293731665781308
Loss in iteration 169 : 0.7339774489846566
Loss in iteration 170 : 0.9806767806200991
Loss in iteration 171 : 0.724565661215153
Loss in iteration 172 : 0.9012261089447509
Loss in iteration 173 : 0.6901705315597055
Loss in iteration 174 : 0.9311866253801431
Loss in iteration 175 : 0.4779682357263178
Loss in iteration 176 : 0.791192358033433
Loss in iteration 177 : 0.4807438589101456
Loss in iteration 178 : 0.47480831658803874
Loss in iteration 179 : 0.9015234929649795
Loss in iteration 180 : 0.6281859153339338
Loss in iteration 181 : 0.5752709940888189
Loss in iteration 182 : 0.43426745412113776
Loss in iteration 183 : 0.39065255355243617
Loss in iteration 184 : 0.46733331596132527
Loss in iteration 185 : 0.6669854670116901
Loss in iteration 186 : 1.04130025587382
Loss in iteration 187 : 0.4176185765841212
Loss in iteration 188 : 0.730566076975445
Loss in iteration 189 : 1.0906284714494312
Loss in iteration 190 : 0.5867996365315802
Loss in iteration 191 : 1.3665511338015672
Loss in iteration 192 : 0.5988153818896602
Loss in iteration 193 : 0.9958454035504597
Loss in iteration 194 : 0.5968327445768217
Loss in iteration 195 : 0.9010167537808826
Loss in iteration 196 : 0.6040644738401887
Loss in iteration 197 : 0.7336401722760566
Loss in iteration 198 : 0.6124796146384368
Loss in iteration 199 : 0.4593891136367496
Loss in iteration 200 : 0.7675287215831118
Testing accuracy  of updater 7 on alg 0 with rate 14.0 = 0.7392666298138935, training accuracy 0.7412162162162163, time elapsed: 4133 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.7753106365592976
Loss in iteration 3 : 0.4432408753982494
Loss in iteration 4 : 0.6962520197908886
Loss in iteration 5 : 0.5517398606447993
Loss in iteration 6 : 0.6656458465334101
Loss in iteration 7 : 0.6224593730451509
Loss in iteration 8 : 0.5766712958372643
Loss in iteration 9 : 0.6516256508233877
Loss in iteration 10 : 0.5621676496901512
Loss in iteration 11 : 0.6224658292362945
Loss in iteration 12 : 0.555394424422985
Loss in iteration 13 : 0.5533841727713696
Loss in iteration 14 : 0.5117639306579267
Loss in iteration 15 : 0.503409259652431
Loss in iteration 16 : 0.48135922228828254
Loss in iteration 17 : 0.4850130926818315
Loss in iteration 18 : 0.4196111105856099
Loss in iteration 19 : 0.46661565899905966
Loss in iteration 20 : 0.4097855551002396
Loss in iteration 21 : 0.39638052498386805
Loss in iteration 22 : 0.4400569725830122
Loss in iteration 23 : 0.4082151329164079
Loss in iteration 24 : 0.3722286136059575
Loss in iteration 25 : 0.39083075213202134
Loss in iteration 26 : 0.3726317879616297
Loss in iteration 27 : 0.35764490304953706
Loss in iteration 28 : 0.3818152086683609
Loss in iteration 29 : 0.382624632204041
Loss in iteration 30 : 0.36011531367775346
Loss in iteration 31 : 0.3555828873951628
Loss in iteration 32 : 0.35755859927471356
Loss in iteration 33 : 0.3463661184303142
Loss in iteration 34 : 0.33955914333771997
Loss in iteration 35 : 0.34565798660301933
Loss in iteration 36 : 0.3477603813635569
Loss in iteration 37 : 0.3388226957380408
Loss in iteration 38 : 0.33305783453365867
Loss in iteration 39 : 0.3363587055063769
Loss in iteration 40 : 0.33795179467279046
Loss in iteration 41 : 0.33270679173568124
Loss in iteration 42 : 0.32863341175325134
Loss in iteration 43 : 0.33101456076722735
Loss in iteration 44 : 0.3331818195841034
Loss in iteration 45 : 0.3300240513075526
Loss in iteration 46 : 0.3273010157560439
Loss in iteration 47 : 0.32870252384618914
Loss in iteration 48 : 0.3295249964427643
Loss in iteration 49 : 0.3275010477906866
Loss in iteration 50 : 0.32584116334262336
Loss in iteration 51 : 0.3261140948360108
Loss in iteration 52 : 0.32684773101940245
Loss in iteration 53 : 0.32741616613826996
Loss in iteration 54 : 0.32754372926220726
Loss in iteration 55 : 0.3258862054368704
Loss in iteration 56 : 0.32411439240840434
Loss in iteration 57 : 0.3241948541032401
Loss in iteration 58 : 0.325299474637161
Loss in iteration 59 : 0.325333754622959
Loss in iteration 60 : 0.32424746049537817
Loss in iteration 61 : 0.32376527893176466
Loss in iteration 62 : 0.32421330488606875
Loss in iteration 63 : 0.3245437137904019
Loss in iteration 64 : 0.324300509424836
Loss in iteration 65 : 0.3237645431452407
Loss in iteration 66 : 0.3233818759566637
Loss in iteration 67 : 0.32339964233676416
Loss in iteration 68 : 0.3237164088295039
Loss in iteration 69 : 0.3238321548402858
Loss in iteration 70 : 0.3234556847950815
Loss in iteration 71 : 0.3230297880262002
Loss in iteration 72 : 0.3229679651889084
Loss in iteration 73 : 0.323142118783068
Loss in iteration 74 : 0.32322564170364665
Loss in iteration 75 : 0.3231126491431034
Loss in iteration 76 : 0.32292227563547243
Loss in iteration 77 : 0.32279399170655126
Loss in iteration 78 : 0.3228320631324976
Loss in iteration 79 : 0.3229731146073468
Loss in iteration 80 : 0.3230241801196635
Loss in iteration 81 : 0.32292966412939345
Loss in iteration 82 : 0.3228107467740092
Loss in iteration 83 : 0.3227748510544709
Loss in iteration 84 : 0.3227980741242364
Loss in iteration 85 : 0.32283190460334715
Loss in iteration 86 : 0.3228496672991496
Loss in iteration 87 : 0.32281413565141187
Loss in iteration 88 : 0.32273446098074815
Loss in iteration 89 : 0.3226791814020826
Loss in iteration 90 : 0.3226881743511404
Loss in iteration 91 : 0.32272173109505775
Loss in iteration 92 : 0.32273300805026794
Loss in iteration 93 : 0.3227188868988037
Loss in iteration 94 : 0.3226906056088866
Loss in iteration 95 : 0.3226634180155094
Loss in iteration 96 : 0.32265588498452746
Loss in iteration 97 : 0.3226695619947122
Loss in iteration 98 : 0.32268355707017954
Loss in iteration 99 : 0.3226843622978203
Loss in iteration 100 : 0.3226766751551115
Loss in iteration 101 : 0.32266459430725947
Loss in iteration 102 : 0.3226520530062348
Loss in iteration 103 : 0.3226482258292382
Loss in iteration 104 : 0.3226550973443253
Loss in iteration 105 : 0.3226615611408848
Loss in iteration 106 : 0.3226600122383928
Loss in iteration 107 : 0.32265417349550146
Loss in iteration 108 : 0.3226475195076947
Loss in iteration 109 : 0.3226406183306702
Loss in iteration 110 : 0.322636534408462
Loss in iteration 111 : 0.322637774717413
Loss in iteration 112 : 0.32264127426348493
Loss in iteration 113 : 0.3226431710495976
Loss in iteration 114 : 0.3226431051720788
Loss in iteration 115 : 0.3226413701530812
Loss in iteration 116 : 0.32263829738797384
Loss in iteration 117 : 0.32263573950134555
Loss in iteration 118 : 0.3226350976440899
Loss in iteration 119 : 0.32263543822614216
Loss in iteration 120 : 0.3226359046206585
Loss in iteration 121 : 0.32263675104581285
Loss in iteration 122 : 0.32263736976044693
Loss in iteration 123 : 0.32263672939455024
Loss in iteration 124 : 0.3226352812431277
Loss in iteration 125 : 0.32263408576736896
Testing accuracy  of updater 7 on alg 0 with rate 8.0 = 0.8498249493274369, training accuracy 0.8491707616707617, time elapsed: 2467 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5264419527798136
Loss in iteration 3 : 0.5219589181372043
Loss in iteration 4 : 0.3647328910009002
Loss in iteration 5 : 0.3964459859050866
Loss in iteration 6 : 0.4602760560186273
Loss in iteration 7 : 0.4069467806050885
Loss in iteration 8 : 0.38668672945920085
Loss in iteration 9 : 0.42684815167210444
Loss in iteration 10 : 0.44192956664304434
Loss in iteration 11 : 0.41122852164791684
Loss in iteration 12 : 0.393409497694883
Loss in iteration 13 : 0.41591168515415444
Loss in iteration 14 : 0.4253349670687273
Loss in iteration 15 : 0.3994200763953926
Loss in iteration 16 : 0.3859110311169039
Loss in iteration 17 : 0.39891178419945916
Loss in iteration 18 : 0.3948566102928415
Loss in iteration 19 : 0.36953685354403565
Loss in iteration 20 : 0.3640837944466791
Loss in iteration 21 : 0.373182205258903
Loss in iteration 22 : 0.35980749257910755
Loss in iteration 23 : 0.3466513313854783
Loss in iteration 24 : 0.35517908451030417
Loss in iteration 25 : 0.35343052042649986
Loss in iteration 26 : 0.3401405082851667
Loss in iteration 27 : 0.3440237578766115
Loss in iteration 28 : 0.3457119156197462
Loss in iteration 29 : 0.33500691870485555
Loss in iteration 30 : 0.33847486193684906
Loss in iteration 31 : 0.3405511267287048
Loss in iteration 32 : 0.33299871935319475
Loss in iteration 33 : 0.3356884703976949
Loss in iteration 34 : 0.336134057435494
Loss in iteration 35 : 0.3300189812048655
Loss in iteration 36 : 0.3318577513793725
Loss in iteration 37 : 0.33183890694008195
Loss in iteration 38 : 0.3275966433289381
Loss in iteration 39 : 0.3287418253328136
Loss in iteration 40 : 0.32880409485011297
Loss in iteration 41 : 0.3256665009089426
Loss in iteration 42 : 0.32663201472488584
Loss in iteration 43 : 0.32730449366376363
Loss in iteration 44 : 0.3255889231536247
Loss in iteration 45 : 0.32662706877700765
Loss in iteration 46 : 0.3273117025747113
Loss in iteration 47 : 0.3257802831977453
Loss in iteration 48 : 0.3260935754301755
Loss in iteration 49 : 0.3261473563968316
Loss in iteration 50 : 0.3247512006453522
Loss in iteration 51 : 0.32488805510750574
Loss in iteration 52 : 0.32498289094554333
Loss in iteration 53 : 0.3240438426728893
Loss in iteration 54 : 0.32422352921740233
Loss in iteration 55 : 0.32428637165207086
Loss in iteration 56 : 0.3236073473294409
Loss in iteration 57 : 0.32379273129126
Loss in iteration 58 : 0.3238651151157694
Loss in iteration 59 : 0.32342005066488144
Loss in iteration 60 : 0.32361483101319516
Loss in iteration 61 : 0.3236201743058219
Loss in iteration 62 : 0.3232753594592402
Loss in iteration 63 : 0.32341271287493023
Loss in iteration 64 : 0.32338023268404437
Loss in iteration 65 : 0.32315364512206257
Loss in iteration 66 : 0.32328684983661443
Loss in iteration 67 : 0.3232531851009804
Loss in iteration 68 : 0.32309286016313576
Loss in iteration 69 : 0.3231834069697364
Loss in iteration 70 : 0.32313434242826655
Loss in iteration 71 : 0.32302559319755664
Loss in iteration 72 : 0.3231055125737453
Loss in iteration 73 : 0.32307074940110164
Loss in iteration 74 : 0.3230031279222607
Loss in iteration 75 : 0.32305941373552516
Loss in iteration 76 : 0.3230166326084414
Loss in iteration 77 : 0.3229649681702337
Loss in iteration 78 : 0.323004222619869
Loss in iteration 79 : 0.3229702146102209
Loss in iteration 80 : 0.3229401559762916
Loss in iteration 81 : 0.32296860293885415
Loss in iteration 82 : 0.3229346747535561
Loss in iteration 83 : 0.3229095709508169
Loss in iteration 84 : 0.3229243447381833
Loss in iteration 85 : 0.3228953229330001
Loss in iteration 86 : 0.3228837994165148
Loss in iteration 87 : 0.32289827237616975
Loss in iteration 88 : 0.32287790561944973
Loss in iteration 89 : 0.32287316539117783
Loss in iteration 90 : 0.32287990364380975
Loss in iteration 91 : 0.32286057640401794
Loss in iteration 92 : 0.3228573611030456
Loss in iteration 93 : 0.32285950095416505
Loss in iteration 94 : 0.32284436578952846
Loss in iteration 95 : 0.3228433976340571
Loss in iteration 96 : 0.32284254426880377
Loss in iteration 97 : 0.32283028073980063
Loss in iteration 98 : 0.32282991843156245
Loss in iteration 99 : 0.32282776030186555
Loss in iteration 100 : 0.32281913967082665
Loss in iteration 101 : 0.32281981352812233
Loss in iteration 102 : 0.32281707293257716
Loss in iteration 103 : 0.32281060357184016
Loss in iteration 104 : 0.322810856391624
Loss in iteration 105 : 0.32280753261665324
Loss in iteration 106 : 0.32280291631312125
Loss in iteration 107 : 0.32280307808095793
Loss in iteration 108 : 0.32279971482723624
Loss in iteration 109 : 0.3227960852143051
Loss in iteration 110 : 0.32279531316099697
Loss in iteration 111 : 0.3227915193827348
Loss in iteration 112 : 0.3227884790695662
Loss in iteration 113 : 0.3227873454805914
Loss in iteration 114 : 0.32278403596724653
Loss in iteration 115 : 0.3227818568888134
Loss in iteration 116 : 0.32278054486168123
Loss in iteration 117 : 0.32277755750856635
Loss in iteration 118 : 0.322775728649176
Loss in iteration 119 : 0.3227741950180655
Loss in iteration 120 : 0.3227716230776832
Loss in iteration 121 : 0.32277011788676363
Loss in iteration 122 : 0.3227684711316447
Loss in iteration 123 : 0.3227661444428739
Loss in iteration 124 : 0.3227646198652294
Loss in iteration 125 : 0.322762772265577
Loss in iteration 126 : 0.32276066056461816
Loss in iteration 127 : 0.3227592176236758
Loss in iteration 128 : 0.3227574496635259
Loss in iteration 129 : 0.32275565310271664
Loss in iteration 130 : 0.32275427731929124
Loss in iteration 131 : 0.3227525567082483
Loss in iteration 132 : 0.3227509218610604
Loss in iteration 133 : 0.322749529347334
Loss in iteration 134 : 0.3227478676008891
Loss in iteration 135 : 0.3227463627279082
Loss in iteration 136 : 0.3227449657271653
Loss in iteration 137 : 0.32274337774340295
Loss in iteration 138 : 0.32274196254004445
Loss in iteration 139 : 0.3227405774310251
Loss in iteration 140 : 0.322739093088978
Loss in iteration 141 : 0.32273777162368544
Loss in iteration 142 : 0.32273642906283523
Loss in iteration 143 : 0.3227350437743114
Loss in iteration 144 : 0.32273377975228595
Loss in iteration 145 : 0.3227324714316832
Loss in iteration 146 : 0.3227311688328217
Loss in iteration 147 : 0.3227299541820304
Loss in iteration 148 : 0.32272869604759646
Loss in iteration 149 : 0.3227274678710729
Loss in iteration 150 : 0.32272628907580636
Loss in iteration 151 : 0.32272507443217385
Loss in iteration 152 : 0.3227239024427255
Loss in iteration 153 : 0.32272275818896207
Loss in iteration 154 : 0.32272159749577944
Loss in iteration 155 : 0.3227204821817686
Loss in iteration 156 : 0.3227193770437801
Loss in iteration 157 : 0.3227182678326047
Loss in iteration 158 : 0.32271719663476467
Loss in iteration 159 : 0.3227161273908759
Loss in iteration 160 : 0.32271506715366394
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.8501320557705301, training accuracy 0.8491707616707617, time elapsed: 3200 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5057389828724304
Loss in iteration 3 : 0.5151697469829599
Loss in iteration 4 : 0.403540780406414
Loss in iteration 5 : 0.3527359599589471
Loss in iteration 6 : 0.41538300278254814
Loss in iteration 7 : 0.42019620330046287
Loss in iteration 8 : 0.37820678496462307
Loss in iteration 9 : 0.37705751117688446
Loss in iteration 10 : 0.4058194078736937
Loss in iteration 11 : 0.4129981066038832
Loss in iteration 12 : 0.39163792089286387
Loss in iteration 13 : 0.37800712261423297
Loss in iteration 14 : 0.3907814320577545
Loss in iteration 15 : 0.4019910052610153
Loss in iteration 16 : 0.38955148997497474
Loss in iteration 17 : 0.3733078348998265
Loss in iteration 18 : 0.3753417578540346
Loss in iteration 19 : 0.3811133358311135
Loss in iteration 20 : 0.3706527156053083
Loss in iteration 21 : 0.3550273076625768
Loss in iteration 22 : 0.35387546346061327
Loss in iteration 23 : 0.35790785675183107
Loss in iteration 24 : 0.34967315061879584
Loss in iteration 25 : 0.34008023133134796
Loss in iteration 26 : 0.34280252431116853
Loss in iteration 27 : 0.345328115564621
Loss in iteration 28 : 0.33830587514984267
Loss in iteration 29 : 0.3344981363440696
Loss in iteration 30 : 0.33845458924021443
Loss in iteration 31 : 0.33664455690958933
Loss in iteration 32 : 0.3313675189782351
Loss in iteration 33 : 0.3332670027768802
Loss in iteration 34 : 0.33467015163758956
Loss in iteration 35 : 0.3306563215749844
Loss in iteration 36 : 0.3299950248951836
Loss in iteration 37 : 0.3318430015061256
Loss in iteration 38 : 0.32945591793722023
Loss in iteration 39 : 0.3275060397937189
Loss in iteration 40 : 0.3288163273541873
Loss in iteration 41 : 0.3282907012895846
Loss in iteration 42 : 0.3262154054166779
Loss in iteration 43 : 0.3263723829754448
Loss in iteration 44 : 0.3267064377912538
Loss in iteration 45 : 0.32517663044485595
Loss in iteration 46 : 0.3245443244522337
Loss in iteration 47 : 0.32523917862196433
Loss in iteration 48 : 0.32491023237513994
Loss in iteration 49 : 0.3242793070771954
Loss in iteration 50 : 0.32483778503997446
Loss in iteration 51 : 0.3251293288701717
Loss in iteration 52 : 0.3245225914696063
Loss in iteration 53 : 0.3244652631544993
Loss in iteration 54 : 0.3247249046762213
Loss in iteration 55 : 0.3242946004221426
Loss in iteration 56 : 0.3239265102470777
Loss in iteration 57 : 0.3241080901341991
Loss in iteration 58 : 0.32398989481239676
Loss in iteration 59 : 0.3236190908420661
Loss in iteration 60 : 0.323655066914722
Loss in iteration 61 : 0.3237021505800765
Loss in iteration 62 : 0.3234387671444489
Loss in iteration 63 : 0.3233681066163394
Loss in iteration 64 : 0.3234858158743934
Loss in iteration 65 : 0.32337463269833955
Loss in iteration 66 : 0.3232603758714715
Loss in iteration 67 : 0.32333965894201616
Loss in iteration 68 : 0.3233065842925793
Loss in iteration 69 : 0.32317173448646597
Loss in iteration 70 : 0.3231818944780388
Loss in iteration 71 : 0.32319667069800656
Loss in iteration 72 : 0.32310255767436286
Loss in iteration 73 : 0.3230807831709261
Loss in iteration 74 : 0.32311759208793084
Loss in iteration 75 : 0.32307030451553337
Loss in iteration 76 : 0.3230267766833259
Loss in iteration 77 : 0.3230518636917832
Loss in iteration 78 : 0.32303812755227346
Loss in iteration 79 : 0.3229958060909549
Loss in iteration 80 : 0.3230067079822907
Loss in iteration 81 : 0.32301415805150213
Loss in iteration 82 : 0.32298189230540797
Loss in iteration 83 : 0.3229743002126811
Loss in iteration 84 : 0.3229827753626308
Loss in iteration 85 : 0.3229607962175721
Loss in iteration 86 : 0.3229443470782061
Loss in iteration 87 : 0.32295217913389435
Loss in iteration 88 : 0.3229451644426503
Loss in iteration 89 : 0.3229301623466579
Loss in iteration 90 : 0.3229333853791399
Loss in iteration 91 : 0.32293232999358545
Loss in iteration 92 : 0.3229178088988317
Loss in iteration 93 : 0.3229135364274527
Loss in iteration 94 : 0.3229137637934499
Loss in iteration 95 : 0.3229036084627573
Loss in iteration 96 : 0.3228973024662506
Loss in iteration 97 : 0.32289856388406796
Loss in iteration 98 : 0.3228931661551488
Loss in iteration 99 : 0.3228861828178033
Loss in iteration 100 : 0.3228857053858196
Loss in iteration 101 : 0.32288256304049684
Loss in iteration 102 : 0.32287595989733664
Loss in iteration 103 : 0.3228740425874543
Loss in iteration 104 : 0.32287252422076673
Loss in iteration 105 : 0.32286736723528
Loss in iteration 106 : 0.3228643779882888
Loss in iteration 107 : 0.3228630452911074
Loss in iteration 108 : 0.32285889876331203
Loss in iteration 109 : 0.3228551778894538
Loss in iteration 110 : 0.32285358819896265
Loss in iteration 111 : 0.32285051479023447
Loss in iteration 112 : 0.3228468704095027
Loss in iteration 113 : 0.3228449805501854
Loss in iteration 114 : 0.32284258027287904
Loss in iteration 115 : 0.3228391877001603
Loss in iteration 116 : 0.3228369672806424
Loss in iteration 117 : 0.32283498381267856
Loss in iteration 118 : 0.32283212116387605
Loss in iteration 119 : 0.3228298612118111
Loss in iteration 120 : 0.32282812038463693
Loss in iteration 121 : 0.3228256816316228
Loss in iteration 122 : 0.32282333385921025
Loss in iteration 123 : 0.3228215225217092
Loss in iteration 124 : 0.32281932105177713
Loss in iteration 125 : 0.3228169975257083
Loss in iteration 126 : 0.322815157897343
Loss in iteration 127 : 0.3228131986827346
Loss in iteration 128 : 0.3228110135936605
Loss in iteration 129 : 0.322809141582694
Loss in iteration 130 : 0.322807295431133
Loss in iteration 131 : 0.32280522334312367
Loss in iteration 132 : 0.32280333632374
Loss in iteration 133 : 0.32280158221251903
Loss in iteration 134 : 0.32279967432540485
Loss in iteration 135 : 0.3227978476839694
Loss in iteration 136 : 0.32279616761040125
Loss in iteration 137 : 0.3227943843327548
Loss in iteration 138 : 0.32279260084293654
Loss in iteration 139 : 0.32279094558898996
Loss in iteration 140 : 0.3227892503009717
Loss in iteration 141 : 0.32278753071404376
Loss in iteration 142 : 0.3227859175235353
Loss in iteration 143 : 0.3227843096205717
Loss in iteration 144 : 0.3227826673219763
Loss in iteration 145 : 0.32278109596781274
Loss in iteration 146 : 0.3227795523450875
Loss in iteration 147 : 0.3227779798827717
Loss in iteration 148 : 0.3227764535923523
Loss in iteration 149 : 0.32277496831642405
Loss in iteration 150 : 0.32277346641474364
Loss in iteration 151 : 0.3227719905251012
Loss in iteration 152 : 0.32277055523017795
Loss in iteration 153 : 0.3227691135491862
Loss in iteration 154 : 0.32276768548984125
Loss in iteration 155 : 0.32276629493663045
Loss in iteration 156 : 0.32276490889019294
Loss in iteration 157 : 0.32276353105012656
Loss in iteration 158 : 0.32276218471397183
Loss in iteration 159 : 0.32276084902221763
Loss in iteration 160 : 0.3227595187719926
Loss in iteration 161 : 0.32275821404076555
Loss in iteration 162 : 0.32275692469717765
Loss in iteration 163 : 0.32275564202776996
Loss in iteration 164 : 0.3227543806732645
Loss in iteration 165 : 0.32275313676903494
Loss in iteration 166 : 0.32275190032894435
Loss in iteration 167 : 0.3227506806028559
Loss in iteration 168 : 0.32274947795757974
Loss in iteration 169 : 0.32274828393582655
Loss in iteration 170 : 0.32274710408681273
Loss in iteration 171 : 0.3227459410380577
Loss in iteration 172 : 0.32274478810176976
Loss in iteration 173 : 0.32274364761404506
Loss in iteration 174 : 0.3227425226527324
Loss in iteration 175 : 0.3227414083651609
Loss in iteration 176 : 0.3227403052646771
Loss in iteration 177 : 0.3227392164825456
Loss in iteration 178 : 0.3227381389579381
Loss in iteration 179 : 0.3227370721459061
Loss in iteration 180 : 0.3227360185900005
Loss in iteration 181 : 0.3227349763814652
Loss in iteration 182 : 0.3227339443899895
Loss in iteration 183 : 0.3227329245245909
Loss in iteration 184 : 0.32273191582234706
Loss in iteration 185 : 0.32273091708949553
Loss in iteration 186 : 0.32272992968228
Loss in iteration 187 : 0.3227289532085662
Loss in iteration 188 : 0.3227279865160564
Testing accuracy  of updater 7 on alg 0 with rate 1.4 = 0.8501320557705301, training accuracy 0.8490479115479116, time elapsed: 3689 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.523034708829783
Loss in iteration 3 : 0.4964026852799207
Loss in iteration 4 : 0.46851651064484384
Loss in iteration 5 : 0.38871797041760886
Loss in iteration 6 : 0.349812223930044
Loss in iteration 7 : 0.37686156206270843
Loss in iteration 8 : 0.4001049267405678
Loss in iteration 9 : 0.3859714796090924
Loss in iteration 10 : 0.3629551815684579
Loss in iteration 11 : 0.3606503778969153
Loss in iteration 12 : 0.3748199732856725
Loss in iteration 13 : 0.38446917462383695
Loss in iteration 14 : 0.3789031518019027
Loss in iteration 15 : 0.3659915074197905
Loss in iteration 16 : 0.36043640347618694
Loss in iteration 17 : 0.3661045235994309
Loss in iteration 18 : 0.3726685525227262
Loss in iteration 19 : 0.3700703265172753
Loss in iteration 20 : 0.3606092462969059
Loss in iteration 21 : 0.3542277168017075
Loss in iteration 22 : 0.3550119545267042
Loss in iteration 23 : 0.35695461198701806
Loss in iteration 24 : 0.35332150190719286
Loss in iteration 25 : 0.3454851806701092
Loss in iteration 26 : 0.3402723511632545
Loss in iteration 27 : 0.3404604111620062
Loss in iteration 28 : 0.3414601483918623
Loss in iteration 29 : 0.33860322930599385
Loss in iteration 30 : 0.3338523543419202
Loss in iteration 31 : 0.33206995113189713
Loss in iteration 32 : 0.3334175781626204
Loss in iteration 33 : 0.3337501164996653
Loss in iteration 34 : 0.33129390311202733
Loss in iteration 35 : 0.3289089618879854
Loss in iteration 36 : 0.32911203097169167
Loss in iteration 37 : 0.3301157019764011
Loss in iteration 38 : 0.3292387940071105
Loss in iteration 39 : 0.32741888632397187
Loss in iteration 40 : 0.3270884239169372
Loss in iteration 41 : 0.3278811965790374
Loss in iteration 42 : 0.3277031452983298
Loss in iteration 43 : 0.32645633832327436
Loss in iteration 44 : 0.32583742588499665
Loss in iteration 45 : 0.32623470997186355
Loss in iteration 46 : 0.3263349054132833
Loss in iteration 47 : 0.3256239931764872
Loss in iteration 48 : 0.32507643868873903
Loss in iteration 49 : 0.3252476996459424
Loss in iteration 50 : 0.3254433863970624
Loss in iteration 51 : 0.32507211987901974
Loss in iteration 52 : 0.324549953811259
Loss in iteration 53 : 0.32443277938743875
Loss in iteration 54 : 0.32450638022336054
Loss in iteration 55 : 0.3242917241798297
Loss in iteration 56 : 0.323888821891508
Loss in iteration 57 : 0.32372463960869957
Loss in iteration 58 : 0.3238139883052324
Loss in iteration 59 : 0.3238169937828063
Loss in iteration 60 : 0.3236459031569181
Loss in iteration 61 : 0.32354672489089786
Loss in iteration 62 : 0.32362602607419405
Loss in iteration 63 : 0.32368958307980567
Loss in iteration 64 : 0.3236039808591583
Loss in iteration 65 : 0.3234985678908824
Loss in iteration 66 : 0.3235046965593329
Loss in iteration 67 : 0.32354343117723905
Loss in iteration 68 : 0.32349659566664335
Loss in iteration 69 : 0.32340824479587704
Loss in iteration 70 : 0.32338348136431516
Loss in iteration 71 : 0.3234053488779196
Loss in iteration 72 : 0.32338449765249827
Loss in iteration 73 : 0.32331924557651265
Loss in iteration 74 : 0.32328263981137306
Loss in iteration 75 : 0.3232892176594603
Loss in iteration 76 : 0.3232831441981696
Loss in iteration 77 : 0.3232442542427825
Loss in iteration 78 : 0.32321506244857423
Loss in iteration 79 : 0.3232184560745353
Loss in iteration 80 : 0.3232222446129857
Loss in iteration 81 : 0.3232015363831727
Loss in iteration 82 : 0.3231777164708904
Loss in iteration 83 : 0.32317341961948637
Loss in iteration 84 : 0.32317431130971
Loss in iteration 85 : 0.3231600606244791
Loss in iteration 86 : 0.32313968513843955
Loss in iteration 87 : 0.32313138585658213
Loss in iteration 88 : 0.3231306861147239
Loss in iteration 89 : 0.32312219866715614
Loss in iteration 90 : 0.3231073265720426
Loss in iteration 91 : 0.32309862969549513
Loss in iteration 92 : 0.32309659736155555
Loss in iteration 93 : 0.3230910907569087
Loss in iteration 94 : 0.32308039876996913
Loss in iteration 95 : 0.32307263315600737
Loss in iteration 96 : 0.32307033492630843
Loss in iteration 97 : 0.3230672033992799
Loss in iteration 98 : 0.32306021590478473
Loss in iteration 99 : 0.3230538919251903
Loss in iteration 100 : 0.3230510111175636
Loss in iteration 101 : 0.3230480463717095
Loss in iteration 102 : 0.32304218969128407
Loss in iteration 103 : 0.3230359516415874
Loss in iteration 104 : 0.3230320024006644
Loss in iteration 105 : 0.3230287556961306
Loss in iteration 106 : 0.3230239827912634
Loss in iteration 107 : 0.3230187997531284
Loss in iteration 108 : 0.32301517963739285
Loss in iteration 109 : 0.32301242241233136
Loss in iteration 110 : 0.3230087826323778
Loss in iteration 111 : 0.3230045522023843
Loss in iteration 112 : 0.3230011176112375
Loss in iteration 113 : 0.3229983557977107
Loss in iteration 114 : 0.32299510740956827
Loss in iteration 115 : 0.32299134512231603
Loss in iteration 116 : 0.3229880154128727
Loss in iteration 117 : 0.3229852450556194
Loss in iteration 118 : 0.3229822744197497
Loss in iteration 119 : 0.32297891205147544
Loss in iteration 120 : 0.32297574419811287
Loss in iteration 121 : 0.3229729757959588
Loss in iteration 122 : 0.32297014659495527
Loss in iteration 123 : 0.3229670488006806
Loss in iteration 124 : 0.32296404482660307
Loss in iteration 125 : 0.32296134581264546
Loss in iteration 126 : 0.3229586821264538
Loss in iteration 127 : 0.322955858637805
Loss in iteration 128 : 0.3229530733385263
Loss in iteration 129 : 0.3229504953532697
Loss in iteration 130 : 0.3229479707325257
Loss in iteration 131 : 0.322945342829119
Loss in iteration 132 : 0.3229427200250713
Loss in iteration 133 : 0.3229402367685086
Loss in iteration 134 : 0.32293781502263874
Loss in iteration 135 : 0.32293533846533046
Loss in iteration 136 : 0.3229328596308993
Loss in iteration 137 : 0.32293047404340847
Loss in iteration 138 : 0.32292814249454227
Loss in iteration 139 : 0.3229257811757094
Loss in iteration 140 : 0.32292341380096606
Loss in iteration 141 : 0.32292110942754443
Loss in iteration 142 : 0.32291885403949666
Loss in iteration 143 : 0.32291659174715764
Loss in iteration 144 : 0.3229143313558795
Loss in iteration 145 : 0.32291211926790353
Loss in iteration 146 : 0.32290995071408546
Loss in iteration 147 : 0.3229077863469114
Loss in iteration 148 : 0.3229056259667985
Loss in iteration 149 : 0.32290349993501966
Loss in iteration 150 : 0.3229014089976057
Loss in iteration 151 : 0.3228993279544497
Loss in iteration 152 : 0.3228972547446004
Loss in iteration 153 : 0.32289520985852493
Loss in iteration 154 : 0.32289319602863903
Loss in iteration 155 : 0.32289119641352765
Loss in iteration 156 : 0.32288920727751813
Loss in iteration 157 : 0.3228872411431732
Loss in iteration 158 : 0.3228853005050342
Loss in iteration 159 : 0.32288337425788166
Loss in iteration 160 : 0.32288145907717813
Loss in iteration 161 : 0.3228795632792656
Loss in iteration 162 : 0.32287768952633883
Loss in iteration 163 : 0.3228758309049681
Loss in iteration 164 : 0.32287398476519125
Loss in iteration 165 : 0.3228721563572167
Loss in iteration 166 : 0.32287034767136336
Loss in iteration 167 : 0.32286855411950793
Loss in iteration 168 : 0.3228667735508099
Loss in iteration 169 : 0.3228650092622168
Loss in iteration 170 : 0.32286326275634253
Loss in iteration 171 : 0.3228615310771577
Loss in iteration 172 : 0.32285981261406344
Loss in iteration 173 : 0.32285810939431936
Loss in iteration 174 : 0.3228564224005599
Loss in iteration 175 : 0.3228547496076813
Loss in iteration 176 : 0.3228530897745939
Loss in iteration 177 : 0.32285144413813716
Loss in iteration 178 : 0.3228498133732931
Loss in iteration 179 : 0.32284819617182126
Loss in iteration 180 : 0.32284659169154545
Loss in iteration 181 : 0.32284500073234335
Loss in iteration 182 : 0.32284342373999386
Loss in iteration 183 : 0.3228418598172061
Loss in iteration 184 : 0.3228403083363646
Loss in iteration 185 : 0.32283876975901
Loss in iteration 186 : 0.32283724433363964
Loss in iteration 187 : 0.32283573142920013
Loss in iteration 188 : 0.3228342305967582
Loss in iteration 189 : 0.3228327421123526
Loss in iteration 190 : 0.3228312661080469
Loss in iteration 191 : 0.32282980212655077
Loss in iteration 192 : 0.3228283498269603
Loss in iteration 193 : 0.3228269093437176
Loss in iteration 194 : 0.3228254807123195
Loss in iteration 195 : 0.32282406358577376
Loss in iteration 196 : 0.32282265770897783
Loss in iteration 197 : 0.3228212631467255
Loss in iteration 198 : 0.32281987989450484
Loss in iteration 199 : 0.3228185076960742
Loss in iteration 200 : 0.32281714636004694
Testing accuracy  of updater 7 on alg 0 with rate 0.8 = 0.8501320557705301, training accuracy 0.8491400491400491, time elapsed: 4009 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6301859981884634
Loss in iteration 3 : 0.554456963957116
Loss in iteration 4 : 0.5059821791724339
Loss in iteration 5 : 0.4850259067952633
Loss in iteration 6 : 0.47184009858214115
Loss in iteration 7 : 0.45215932796432884
Loss in iteration 8 : 0.422868122383688
Loss in iteration 9 : 0.3909446900278649
Loss in iteration 10 : 0.36623242477436085
Loss in iteration 11 : 0.35445835983215185
Loss in iteration 12 : 0.35431224415744433
Loss in iteration 13 : 0.35971456590054746
Loss in iteration 14 : 0.3641837815617233
Loss in iteration 15 : 0.36401457298299994
Loss in iteration 16 : 0.3590703112086857
Loss in iteration 17 : 0.35169393618977296
Loss in iteration 18 : 0.34487426414365197
Loss in iteration 19 : 0.34069928874528016
Loss in iteration 20 : 0.3396666046357425
Loss in iteration 21 : 0.3408774332114441
Loss in iteration 22 : 0.3427589717690928
Loss in iteration 23 : 0.34386351229368806
Loss in iteration 24 : 0.3434153992861639
Loss in iteration 25 : 0.3414725634066848
Loss in iteration 26 : 0.33873559918011364
Loss in iteration 27 : 0.3361425947879491
Loss in iteration 28 : 0.3344357659666802
Loss in iteration 29 : 0.3338754249555867
Loss in iteration 30 : 0.334202434137504
Loss in iteration 31 : 0.3348330253046904
Loss in iteration 32 : 0.33516619454243207
Loss in iteration 33 : 0.3348485858245436
Loss in iteration 34 : 0.3338854202877286
Loss in iteration 35 : 0.3325738027122303
Loss in iteration 36 : 0.3313178836908775
Loss in iteration 37 : 0.33042841399516526
Loss in iteration 38 : 0.3299998286910939
Loss in iteration 39 : 0.32990740080827025
Loss in iteration 40 : 0.3299044254258188
Loss in iteration 41 : 0.32975698451763713
Loss in iteration 42 : 0.3293485228458279
Loss in iteration 43 : 0.3287129269319685
Loss in iteration 44 : 0.32799399652553646
Loss in iteration 45 : 0.32736178611288047
Loss in iteration 46 : 0.3269303744299569
Loss in iteration 47 : 0.32671434444238534
Loss in iteration 48 : 0.32663826377631383
Loss in iteration 49 : 0.32658670412380025
Loss in iteration 50 : 0.3264648810311677
Loss in iteration 51 : 0.32623931618250424
Loss in iteration 52 : 0.32594225221729417
Loss in iteration 53 : 0.32564370556832317
Loss in iteration 54 : 0.32541016278565943
Loss in iteration 55 : 0.32527226884437094
Loss in iteration 56 : 0.32521563816573407
Loss in iteration 57 : 0.3251951648605329
Loss in iteration 58 : 0.32516171573733244
Loss in iteration 59 : 0.32508609260489885
Loss in iteration 60 : 0.3249691693511157
Loss in iteration 61 : 0.3248357094071467
Loss in iteration 62 : 0.3247176554912504
Loss in iteration 63 : 0.3246368147811966
Loss in iteration 64 : 0.3245955434499979
Loss in iteration 65 : 0.32457870874762584
Loss in iteration 66 : 0.3245640634468786
Loss in iteration 67 : 0.32453440359420044
Loss in iteration 68 : 0.3244850688301692
Loss in iteration 69 : 0.32442379343868777
Loss in iteration 70 : 0.3243643298800572
Loss in iteration 71 : 0.3243181995879977
Loss in iteration 72 : 0.32428910884822665
Loss in iteration 73 : 0.3242723868310152
Loss in iteration 74 : 0.3242588092564946
Loss in iteration 75 : 0.3242400537774274
Loss in iteration 76 : 0.3242127116747536
Loss in iteration 77 : 0.32417908059652834
Loss in iteration 78 : 0.3241449222564401
Loss in iteration 79 : 0.32411587587007124
Loss in iteration 80 : 0.32409459455175205
Loss in iteration 81 : 0.3240799300239516
Loss in iteration 82 : 0.32406820566121125
Loss in iteration 83 : 0.32405554155261107
Loss in iteration 84 : 0.3240398442724653
Loss in iteration 85 : 0.32402150115118294
Loss in iteration 86 : 0.3240026637845908
Loss in iteration 87 : 0.3239857546829304
Loss in iteration 88 : 0.3239721135798843
Loss in iteration 89 : 0.323961456317486
Loss in iteration 90 : 0.3239522741878048
Loss in iteration 91 : 0.32394279343530796
Loss in iteration 92 : 0.323931897873517
Loss in iteration 93 : 0.3239195481412837
Loss in iteration 94 : 0.32390657635396447
Loss in iteration 95 : 0.32389407779660473
Loss in iteration 96 : 0.3238827869019245
Loss in iteration 97 : 0.323872760991547
Loss in iteration 98 : 0.3238634768225207
Loss in iteration 99 : 0.32385421238826945
Loss in iteration 100 : 0.3238444623002922
Loss in iteration 101 : 0.32383416407620175
Loss in iteration 102 : 0.32382365200768076
Loss in iteration 103 : 0.32381341245792405
Loss in iteration 104 : 0.32380380431975947
Loss in iteration 105 : 0.32379489641629833
Loss in iteration 106 : 0.323786484791829
Loss in iteration 107 : 0.3237782476112489
Loss in iteration 108 : 0.3237699316273159
Loss in iteration 109 : 0.3237614674276266
Loss in iteration 110 : 0.3237529668539065
Loss in iteration 111 : 0.32374462606779797
Loss in iteration 112 : 0.32373660264008314
Loss in iteration 113 : 0.3237289359577206
Loss in iteration 114 : 0.32372154473812476
Loss in iteration 115 : 0.3237142887374825
Loss in iteration 116 : 0.32370705036777175
Loss in iteration 117 : 0.32369978958930573
Loss in iteration 118 : 0.32369254807707
Loss in iteration 119 : 0.32368540979584015
Loss in iteration 120 : 0.3236784467855159
Loss in iteration 121 : 0.32367168144989406
Loss in iteration 122 : 0.32366508215803863
Loss in iteration 123 : 0.323658588276746
Loss in iteration 124 : 0.32365214592946073
Loss in iteration 125 : 0.3236457335754327
Loss in iteration 126 : 0.3236393657641517
Loss in iteration 127 : 0.3236330771361004
Loss in iteration 128 : 0.3236268988321827
Loss in iteration 129 : 0.32362084125696283
Loss in iteration 130 : 0.323614891189946
Loss in iteration 131 : 0.3236090221287155
Loss in iteration 132 : 0.32360320990107805
Loss in iteration 133 : 0.3235974442351194
Loss in iteration 134 : 0.3235917308434653
Loss in iteration 135 : 0.32358608464535726
Loss in iteration 136 : 0.3235805193731626
Loss in iteration 137 : 0.3235750397788017
Loss in iteration 138 : 0.3235696401195168
Loss in iteration 139 : 0.32356430856097407
Loss in iteration 140 : 0.32355903403145925
Loss in iteration 141 : 0.3235538113876099
Loss in iteration 142 : 0.3235486424262737
Loss in iteration 143 : 0.32354353296855404
Loss in iteration 144 : 0.32353848831733245
Loss in iteration 145 : 0.3235335098430876
Loss in iteration 146 : 0.32352859433680525
Loss in iteration 147 : 0.3235237359719915
Loss in iteration 148 : 0.32351892933519655
Loss in iteration 149 : 0.32351417168876584
Loss in iteration 150 : 0.32350946338239145
Loss in iteration 151 : 0.3235048065322767
Loss in iteration 152 : 0.3235002030042993
Loss in iteration 153 : 0.32349565292406063
Loss in iteration 154 : 0.3234911544245691
Loss in iteration 155 : 0.3234867045378734
Loss in iteration 156 : 0.32348230053141336
Loss in iteration 157 : 0.32347794087688553
Loss in iteration 158 : 0.3234736253901958
Loss in iteration 159 : 0.3234693546194035
Loss in iteration 160 : 0.32346512895494467
Loss in iteration 161 : 0.32346094800125985
Loss in iteration 162 : 0.3234568105066358
Loss in iteration 163 : 0.32345271478727317
Loss in iteration 164 : 0.32344865932205996
Loss in iteration 165 : 0.3234446431600878
Loss in iteration 166 : 0.32344066595156507
Loss in iteration 167 : 0.32343672765461773
Loss in iteration 168 : 0.3234328281390205
Loss in iteration 169 : 0.323428966923738
Loss in iteration 170 : 0.3234251431675227
Loss in iteration 171 : 0.3234213558698351
Loss in iteration 172 : 0.32341760413109594
Loss in iteration 173 : 0.32341388731610526
Loss in iteration 174 : 0.32341020504720097
Loss in iteration 175 : 0.32340655706173366
Loss in iteration 176 : 0.3234029430375472
Loss in iteration 177 : 0.3233993624890381
Loss in iteration 178 : 0.3233958147783911
Loss in iteration 179 : 0.3233922992143239
Loss in iteration 180 : 0.3233888151672131
Loss in iteration 181 : 0.32338536213338354
Loss in iteration 182 : 0.32338193972223966
Loss in iteration 183 : 0.32337854758775786
Loss in iteration 184 : 0.3233751853531053
Loss in iteration 185 : 0.3233718525718503
Loss in iteration 186 : 0.32336854874077936
Loss in iteration 187 : 0.32336527334749304
Loss in iteration 188 : 0.3233620259195094
Loss in iteration 189 : 0.32335880604690875
Loss in iteration 190 : 0.3233556133704704
Loss in iteration 191 : 0.32335244754823483
Loss in iteration 192 : 0.3233493082231623
Loss in iteration 193 : 0.32334619500977274
Loss in iteration 194 : 0.32334310750364526
Loss in iteration 195 : 0.3233400453040727
Loss in iteration 196 : 0.32333700803446375
Loss in iteration 197 : 0.3233339953492521
Loss in iteration 198 : 0.32333100692583655
Loss in iteration 199 : 0.323328042448741
Loss in iteration 200 : 0.323325101596398
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.8503777409250046, training accuracy 0.849017199017199, time elapsed: 3890 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 7.434632541737809
Loss in iteration 3 : 3.4293777692983247
Loss in iteration 4 : 0.843095364075567
Loss in iteration 5 : 1.5572374051871414
Loss in iteration 6 : 0.9957749837758284
Loss in iteration 7 : 0.9732734920910787
Loss in iteration 8 : 0.9540648747848232
Loss in iteration 9 : 0.919120455123847
Loss in iteration 10 : 0.8768635421362201
Loss in iteration 11 : 0.8340752742016637
Loss in iteration 12 : 0.7918099695777105
Loss in iteration 13 : 0.7501836445704618
Loss in iteration 14 : 0.7077694907683769
Loss in iteration 15 : 0.6625450537381578
Loss in iteration 16 : 0.6146116914048152
Loss in iteration 17 : 0.568233518925555
Loss in iteration 18 : 0.5291097243139521
Loss in iteration 19 : 0.49713161227633773
Loss in iteration 20 : 0.46737305167202453
Loss in iteration 21 : 0.4391157193888731
Loss in iteration 22 : 0.4149682339921083
Loss in iteration 23 : 0.39632281712622863
Loss in iteration 24 : 0.38450667511854375
Loss in iteration 25 : 0.4184583511522844
Loss in iteration 26 : 1.0274605974579636
Loss in iteration 27 : 1.8099463304721022
Loss in iteration 28 : 1.863933187897511
Loss in iteration 29 : 0.8591600233672467
Loss in iteration 30 : 0.6510896048973653
Loss in iteration 31 : 0.696766460113187
Loss in iteration 32 : 0.6518190696888404
Loss in iteration 33 : 0.6251139761349255
Loss in iteration 34 : 0.5930439339012371
Loss in iteration 35 : 0.5567906871008059
Loss in iteration 36 : 0.5210997932489461
Loss in iteration 37 : 0.4870964363192364
Loss in iteration 38 : 0.45459657666076114
Loss in iteration 39 : 0.4239684010434744
Loss in iteration 40 : 0.39762359060168023
Loss in iteration 41 : 0.3784681683529053
Loss in iteration 42 : 0.3651461119884303
Loss in iteration 43 : 0.35347763057501275
Loss in iteration 44 : 0.34410046839101827
Loss in iteration 45 : 0.33950791902955296
Loss in iteration 46 : 0.34154043335485246
Loss in iteration 47 : 0.38733867503694314
Loss in iteration 48 : 0.9076912599680053
Loss in iteration 49 : 2.3221169368670784
Loss in iteration 50 : 0.9557740682865956
Loss in iteration 51 : 0.736144762501751
Loss in iteration 52 : 0.5329236512472694
Loss in iteration 53 : 0.5276910880249291
Loss in iteration 54 : 0.5029667615715885
Loss in iteration 55 : 0.4739582175580983
Loss in iteration 56 : 0.4451817882445111
Loss in iteration 57 : 0.42032516102182665
Loss in iteration 58 : 0.3996483321791606
Loss in iteration 59 : 0.3818396490224425
Loss in iteration 60 : 0.36674058951600824
Loss in iteration 61 : 0.3546698926867317
Loss in iteration 62 : 0.34493040232580985
Loss in iteration 63 : 0.33750322353360046
Loss in iteration 64 : 0.334206703370556
Loss in iteration 65 : 0.34543864672585634
Loss in iteration 66 : 0.5078790789338953
Loss in iteration 67 : 1.5272369287811014
Loss in iteration 68 : 0.40012865849584256
Loss in iteration 69 : 0.37649889355649696
Loss in iteration 70 : 0.41141839472649583
Loss in iteration 71 : 0.49080775922821535
Loss in iteration 72 : 0.6436818853996796
Loss in iteration 73 : 1.0707840394703674
Loss in iteration 74 : 0.46122045838170384
Loss in iteration 75 : 0.4267073040029695
Loss in iteration 76 : 0.43559244959512344
Loss in iteration 77 : 0.4473806498958046
Loss in iteration 78 : 0.4617718373628205
Loss in iteration 79 : 0.5809945822029534
Loss in iteration 80 : 0.6097032991631269
Loss in iteration 81 : 0.9100692081724354
Loss in iteration 82 : 0.4918789926287348
Loss in iteration 83 : 0.48374451213263664
Loss in iteration 84 : 0.47436741591432624
Loss in iteration 85 : 0.5100832304203068
Loss in iteration 86 : 0.4986298031966374
Loss in iteration 87 : 0.6159094743882477
Loss in iteration 88 : 0.5737011643743383
Loss in iteration 89 : 0.7469350582295713
Loss in iteration 90 : 0.5251210522583583
Loss in iteration 91 : 0.5492857061803054
Loss in iteration 92 : 0.4837175109402753
Loss in iteration 93 : 0.5067128121362732
Loss in iteration 94 : 0.47952251084080266
Loss in iteration 95 : 0.5601388251342396
Loss in iteration 96 : 0.5425790309072341
Loss in iteration 97 : 0.7141715286007833
Loss in iteration 98 : 0.5306476391757772
Loss in iteration 99 : 0.5914420798463706
Loss in iteration 100 : 0.4942035517437783
Loss in iteration 101 : 0.5159891720651443
Loss in iteration 102 : 0.4692809695869222
Loss in iteration 103 : 0.5126760616203839
Loss in iteration 104 : 0.4965977919311918
Loss in iteration 105 : 0.6167156500461679
Loss in iteration 106 : 0.543105393429546
Loss in iteration 107 : 0.6735205823464088
Loss in iteration 108 : 0.5053874823451167
Loss in iteration 109 : 0.5286667982290949
Loss in iteration 110 : 0.46784953526529505
Loss in iteration 111 : 0.4900446870218632
Loss in iteration 112 : 0.46679804841844436
Loss in iteration 113 : 0.5437424752048108
Loss in iteration 114 : 0.5269960565679578
Loss in iteration 115 : 0.6884706485326862
Loss in iteration 116 : 0.5193926469593386
Loss in iteration 117 : 0.5759474814210267
Loss in iteration 118 : 0.4818352382321202
Loss in iteration 119 : 0.49810365768301235
Loss in iteration 120 : 0.45594399163101246
Loss in iteration 121 : 0.4939690339562062
Loss in iteration 122 : 0.48343041527372865
Loss in iteration 123 : 0.6022553809749226
Loss in iteration 124 : 0.5374538292680853
Loss in iteration 125 : 0.675019691636333
Loss in iteration 126 : 0.4957721812308084
Loss in iteration 127 : 0.5111448137789145
Loss in iteration 128 : 0.455611401134427
Loss in iteration 129 : 0.47081143149160676
Loss in iteration 130 : 0.4518151549403625
Loss in iteration 131 : 0.5219206699443811
Loss in iteration 132 : 0.5187758480786329
Loss in iteration 133 : 0.6934938511760977
Loss in iteration 134 : 0.5156430598545291
Loss in iteration 135 : 0.5735280862751807
Loss in iteration 136 : 0.4760161186018992
Loss in iteration 137 : 0.4855261619546988
Loss in iteration 138 : 0.4443687160072684
Loss in iteration 139 : 0.47217181068400244
Loss in iteration 140 : 0.4668633314070357
Loss in iteration 141 : 0.5787896072976596
Loss in iteration 142 : 0.5389880015108506
Loss in iteration 143 : 0.7045027713808604
Loss in iteration 144 : 0.4898070199035759
Loss in iteration 145 : 0.49764102136698307
Loss in iteration 146 : 0.4475531345999014
Loss in iteration 147 : 0.4563586281734032
Loss in iteration 148 : 0.4391009474046082
Loss in iteration 149 : 0.5001519990667334
Loss in iteration 150 : 0.5097229554461695
Loss in iteration 151 : 0.6963943518161603
Loss in iteration 152 : 0.5178195282111165
Loss in iteration 153 : 0.5864977317512603
Loss in iteration 154 : 0.475011858238935
Loss in iteration 155 : 0.47805148226119276
Loss in iteration 156 : 0.4357841455872953
Loss in iteration 157 : 0.452520639125504
Loss in iteration 158 : 0.448990030326445
Loss in iteration 159 : 0.5471699073207578
Loss in iteration 160 : 0.5397142293601448
Loss in iteration 161 : 0.7429188610155651
Loss in iteration 162 : 0.48536450585897833
Loss in iteration 163 : 0.4903432117426649
Loss in iteration 164 : 0.44516563386492025
Loss in iteration 165 : 0.45140152829489
Loss in iteration 166 : 0.43243290713944
Loss in iteration 167 : 0.4846710055881992
Loss in iteration 168 : 0.49751771188432703
Loss in iteration 169 : 0.6788196257575663
Loss in iteration 170 : 0.5237493417218356
Loss in iteration 171 : 0.6124412556446237
Loss in iteration 172 : 0.47540494640811853
Loss in iteration 173 : 0.4726066008603672
Loss in iteration 174 : 0.4300606772849849
Loss in iteration 175 : 0.4384019156139723
Loss in iteration 176 : 0.43443321790363576
Loss in iteration 177 : 0.5182996511417468
Loss in iteration 178 : 0.5356326486868066
Loss in iteration 179 : 0.7688451017383315
Loss in iteration 180 : 0.484148418236935
Loss in iteration 181 : 0.4928410361471698
Loss in iteration 182 : 0.4479999524958731
Loss in iteration 183 : 0.4540826998729291
Loss in iteration 184 : 0.43037381989831974
Loss in iteration 185 : 0.473429176123869
Loss in iteration 186 : 0.4827819915575846
Loss in iteration 187 : 0.6438333225629113
Loss in iteration 188 : 0.530414216903277
Loss in iteration 189 : 0.6470683993815648
Loss in iteration 190 : 0.4755384492787565
Loss in iteration 191 : 0.4686280251726216
Loss in iteration 192 : 0.42720387360008544
Loss in iteration 193 : 0.4306336264640975
Loss in iteration 194 : 0.4251331490937028
Loss in iteration 195 : 0.49754111729506884
Loss in iteration 196 : 0.5274173549361955
Loss in iteration 197 : 0.7736339407419279
Loss in iteration 198 : 0.48804878476204355
Loss in iteration 199 : 0.5070994191270758
Loss in iteration 200 : 0.4541684349399916
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.834653891038634, training accuracy 0.8328316953316953, time elapsed: 4070 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 1.4828524800461282
Loss in iteration 3 : 0.6994023777821139
Loss in iteration 4 : 0.521847505726615
Loss in iteration 5 : 0.549960477044243
Loss in iteration 6 : 0.5379358370318476
Loss in iteration 7 : 0.5392694349481432
Loss in iteration 8 : 0.5348477400122792
Loss in iteration 9 : 0.522616342712798
Loss in iteration 10 : 0.5064501678516734
Loss in iteration 11 : 0.4890465451585978
Loss in iteration 12 : 0.4722326286736777
Loss in iteration 13 : 0.45737916069043505
Loss in iteration 14 : 0.4447795692877869
Loss in iteration 15 : 0.4332742762785125
Loss in iteration 16 : 0.42071059052540194
Loss in iteration 17 : 0.4057824495470627
Loss in iteration 18 : 0.3895866305009908
Loss in iteration 19 : 0.37499795212735426
Loss in iteration 20 : 0.36443020623211414
Loss in iteration 21 : 0.35833233436770756
Loss in iteration 22 : 0.355613177757053
Loss in iteration 23 : 0.35483996128858053
Loss in iteration 24 : 0.35479952131710635
Loss in iteration 25 : 0.3544252346863534
Loss in iteration 26 : 0.35286752552969775
Loss in iteration 27 : 0.3498631559060339
Loss in iteration 28 : 0.345841938700004
Loss in iteration 29 : 0.34155121372394637
Loss in iteration 30 : 0.3376216496641723
Loss in iteration 31 : 0.334387292033632
Loss in iteration 32 : 0.3319316161114112
Loss in iteration 33 : 0.3301872994094434
Loss in iteration 34 : 0.32902764901937565
Loss in iteration 35 : 0.32829261576471963
Loss in iteration 36 : 0.3278243343078644
Loss in iteration 37 : 0.3274607311319404
Loss in iteration 38 : 0.32708455629646827
Loss in iteration 39 : 0.326618330846626
Loss in iteration 40 : 0.3260620941872602
Loss in iteration 41 : 0.3254549521988006
Loss in iteration 42 : 0.3248860938282465
Loss in iteration 43 : 0.324434056402973
Loss in iteration 44 : 0.32421559126016913
Loss in iteration 45 : 0.3243380993029029
Loss in iteration 46 : 0.3250640856812774
Loss in iteration 47 : 0.32708430659758336
Loss in iteration 48 : 0.3313948872988515
Loss in iteration 49 : 0.3429909361265415
Loss in iteration 50 : 0.3606835014724084
Loss in iteration 51 : 0.4106515686566335
Loss in iteration 52 : 0.42054149251198286
Loss in iteration 53 : 0.486107630808178
Loss in iteration 54 : 0.4026066635381738
Loss in iteration 55 : 0.39206394030169606
Loss in iteration 56 : 0.36348281411919464
Loss in iteration 57 : 0.3522784460909461
Loss in iteration 58 : 0.34368927013764766
Loss in iteration 59 : 0.34050561354942016
Loss in iteration 60 : 0.3392543140246137
Loss in iteration 61 : 0.3445517339986121
Loss in iteration 62 : 0.3526519447691682
Loss in iteration 63 : 0.3835329936229995
Loss in iteration 64 : 0.4061761198812868
Loss in iteration 65 : 0.494617324633037
Loss in iteration 66 : 0.43259958339250404
Loss in iteration 67 : 0.4578858019087015
Loss in iteration 68 : 0.3901965242149252
Loss in iteration 69 : 0.3741634241092215
Loss in iteration 70 : 0.3558931149407307
Loss in iteration 71 : 0.34843649666324944
Loss in iteration 72 : 0.3435086044307185
Loss in iteration 73 : 0.3467942377831938
Loss in iteration 74 : 0.35290879265999336
Loss in iteration 75 : 0.3811255046677532
Loss in iteration 76 : 0.4036522395692953
Loss in iteration 77 : 0.4956245218821577
Loss in iteration 78 : 0.4381680319931999
Loss in iteration 79 : 0.4762883004797596
Loss in iteration 80 : 0.3954138620822041
Loss in iteration 81 : 0.3784511554423101
Loss in iteration 82 : 0.3583229976134034
Loss in iteration 83 : 0.349556467779793
Loss in iteration 84 : 0.3436248416095086
Loss in iteration 85 : 0.34534183795916185
Loss in iteration 86 : 0.35031349514146426
Loss in iteration 87 : 0.3751783967436459
Loss in iteration 88 : 0.39975885438175895
Loss in iteration 89 : 0.4947075776258713
Loss in iteration 90 : 0.447834356757418
Loss in iteration 91 : 0.5015394897861256
Loss in iteration 92 : 0.4009338138847178
Loss in iteration 93 : 0.38137888033290757
Loss in iteration 94 : 0.3599649513354668
Loss in iteration 95 : 0.34967945485366886
Loss in iteration 96 : 0.3430113345532297
Loss in iteration 97 : 0.3432908998375073
Loss in iteration 98 : 0.34743918131833973
Loss in iteration 99 : 0.36940665134361905
Loss in iteration 100 : 0.3957275327252282
Loss in iteration 101 : 0.4940979666523285
Loss in iteration 102 : 0.4572148999063949
Loss in iteration 103 : 0.5279398858112304
Loss in iteration 104 : 0.40448017924067886
Loss in iteration 105 : 0.38187360399301645
Loss in iteration 106 : 0.3606244195956499
Loss in iteration 107 : 0.34913297024790946
Loss in iteration 108 : 0.34200385655352744
Loss in iteration 109 : 0.34094905480916177
Loss in iteration 110 : 0.3442061118543959
Loss in iteration 111 : 0.36282671402526645
Loss in iteration 112 : 0.3898275774171823
Loss in iteration 113 : 0.4887028324170121
Loss in iteration 114 : 0.4677280164402631
Loss in iteration 115 : 0.5613274776307479
Loss in iteration 116 : 0.4068229462596723
Loss in iteration 117 : 0.38076959222821577
Loss in iteration 118 : 0.3609212151173733
Loss in iteration 119 : 0.3484746371074487
Loss in iteration 120 : 0.34097228214611897
Loss in iteration 121 : 0.33868790290532597
Loss in iteration 122 : 0.34092647609050264
Loss in iteration 123 : 0.3558569610370594
Loss in iteration 124 : 0.3818049072532006
Loss in iteration 125 : 0.4764816051905528
Loss in iteration 126 : 0.47785525227901604
Loss in iteration 127 : 0.6014696975558411
Loss in iteration 128 : 0.4074523027257415
Loss in iteration 129 : 0.37841432398421554
Loss in iteration 130 : 0.3611422880613205
Loss in iteration 131 : 0.34811950605112935
Loss in iteration 132 : 0.3403073812781312
Loss in iteration 133 : 0.33706850245503395
Loss in iteration 134 : 0.33839222491915066
Loss in iteration 135 : 0.35022809326936993
Loss in iteration 136 : 0.3740590168942879
Loss in iteration 137 : 0.46170143078490306
Loss in iteration 138 : 0.4847990824837412
Loss in iteration 139 : 0.6386961516198122
Loss in iteration 140 : 0.40686881848282846
Loss in iteration 141 : 0.37620285423444483
Loss in iteration 142 : 0.3615536562453683
Loss in iteration 143 : 0.3482814916796109
Loss in iteration 144 : 0.34022941525816247
Loss in iteration 145 : 0.33645400739905673
Loss in iteration 146 : 0.33728019792327013
Loss in iteration 147 : 0.3476437732144079
Loss in iteration 148 : 0.3703102935397088
Loss in iteration 149 : 0.45451621106075074
Loss in iteration 150 : 0.4891151757669355
Loss in iteration 151 : 0.6620092757449848
Loss in iteration 152 : 0.4059994686345344
Loss in iteration 153 : 0.3745747375572075
Loss in iteration 154 : 0.3617610003257932
Loss in iteration 155 : 0.3484483628667425
Loss in iteration 156 : 0.34029022105919104
Loss in iteration 157 : 0.3363105953622296
Loss in iteration 158 : 0.33703066127488773
Loss in iteration 159 : 0.34719058799687497
Loss in iteration 160 : 0.3702530058462559
Loss in iteration 161 : 0.45663131323859857
Loss in iteration 162 : 0.4943916949747259
Loss in iteration 163 : 0.676577726013442
Loss in iteration 164 : 0.40439131279124924
Loss in iteration 165 : 0.37218523832690414
Loss in iteration 166 : 0.36112342722287905
Loss in iteration 167 : 0.347989626383858
Loss in iteration 168 : 0.3398929309948555
Loss in iteration 169 : 0.3358006729280822
Loss in iteration 170 : 0.3364679428413148
Loss in iteration 171 : 0.346329888907133
Loss in iteration 172 : 0.3697888451781628
Loss in iteration 173 : 0.45845531653163996
Loss in iteration 174 : 0.5005594387464767
Loss in iteration 175 : 0.6938289191097036
Loss in iteration 176 : 0.4021538415820338
Loss in iteration 177 : 0.369359504614636
Loss in iteration 178 : 0.36024278969498663
Loss in iteration 179 : 0.3473670338310156
Loss in iteration 180 : 0.3393293189997727
Loss in iteration 181 : 0.3350474624514717
Loss in iteration 182 : 0.33549121221987027
Loss in iteration 183 : 0.3444105680387116
Loss in iteration 184 : 0.36709955562128865
Loss in iteration 185 : 0.4539210690462415
Loss in iteration 186 : 0.5053349546785795
Loss in iteration 187 : 0.7155519092431133
Loss in iteration 188 : 0.39995171361970455
Loss in iteration 189 : 0.36713719774227704
Loss in iteration 190 : 0.35978441687863133
Loss in iteration 191 : 0.34711383132485146
Loss in iteration 192 : 0.3390585110991248
Loss in iteration 193 : 0.3345872718366605
Loss in iteration 194 : 0.3348085243144579
Loss in iteration 195 : 0.3429294799284261
Loss in iteration 196 : 0.3647405815011678
Loss in iteration 197 : 0.44920714045571064
Loss in iteration 198 : 0.5085962328461657
Loss in iteration 199 : 0.7337358324024159
Loss in iteration 200 : 0.3982576557507618
Testing accuracy  of updater 8 on alg 0 with rate 1.4000000000000001 = 0.8481665745347338, training accuracy 0.8471744471744471, time elapsed: 4038 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.9298657575342991
Loss in iteration 3 : 0.47187219693214905
Loss in iteration 4 : 0.4079174083412785
Loss in iteration 5 : 0.43725442121094255
Loss in iteration 6 : 0.43561998987202977
Loss in iteration 7 : 0.437427020158286
Loss in iteration 8 : 0.4384202073631635
Loss in iteration 9 : 0.43468808943096404
Loss in iteration 10 : 0.42746567210625824
Loss in iteration 11 : 0.41879722927795504
Loss in iteration 12 : 0.40987958458972973
Loss in iteration 13 : 0.40147112755412756
Loss in iteration 14 : 0.39406485783886197
Loss in iteration 15 : 0.38766302030700334
Loss in iteration 16 : 0.38178012336172545
Loss in iteration 17 : 0.37571772910524087
Loss in iteration 18 : 0.3689907497158624
Loss in iteration 19 : 0.3616923967864407
Loss in iteration 20 : 0.35449178573585616
Loss in iteration 21 : 0.3482513651514609
Loss in iteration 22 : 0.34355733389715787
Loss in iteration 23 : 0.34048755945523695
Loss in iteration 24 : 0.3387066417055995
Loss in iteration 25 : 0.33772341253763566
Loss in iteration 26 : 0.3371013644995335
Loss in iteration 27 : 0.33653625893187844
Loss in iteration 28 : 0.3358402289689817
Loss in iteration 29 : 0.3349078285606464
Loss in iteration 30 : 0.33370432709293674
Loss in iteration 31 : 0.33226876088828533
Loss in iteration 32 : 0.33070584746606996
Loss in iteration 33 : 0.3291552808485825
Loss in iteration 34 : 0.3277497085853977
Loss in iteration 35 : 0.3265810464647754
Loss in iteration 36 : 0.3256867354127638
Loss in iteration 37 : 0.32505504320295964
Loss in iteration 38 : 0.32464129894772054
Loss in iteration 39 : 0.3243864389122257
Loss in iteration 40 : 0.3242322216234684
Loss in iteration 41 : 0.324130828545436
Loss in iteration 42 : 0.3240489309415673
Loss in iteration 43 : 0.3239675214989808
Loss in iteration 44 : 0.32387916840215264
Loss in iteration 45 : 0.3237841969853983
Loss in iteration 46 : 0.323686922032209
Loss in iteration 47 : 0.3235926236117227
Loss in iteration 48 : 0.32350557375167827
Loss in iteration 49 : 0.3234281195701926
Loss in iteration 50 : 0.3233606249435478
Loss in iteration 51 : 0.32330196776392733
Loss in iteration 52 : 0.3232502725994821
Loss in iteration 53 : 0.32320360955894895
Loss in iteration 54 : 0.3231604839554923
Loss in iteration 55 : 0.32312004940826844
Loss in iteration 56 : 0.3230820716431155
Loss in iteration 57 : 0.32304673079973256
Loss in iteration 58 : 0.3230143685907359
Loss in iteration 59 : 0.3229852692555913
Loss in iteration 60 : 0.32295952552460344
Loss in iteration 61 : 0.3229370003454597
Loss in iteration 62 : 0.32291736492053097
Loss in iteration 63 : 0.32290017911304686
Loss in iteration 64 : 0.322884980205387
Loss in iteration 65 : 0.322871355162486
Loss in iteration 66 : 0.3228589840197764
Loss in iteration 67 : 0.3228476531717285
Loss in iteration 68 : 0.3228372447440342
Loss in iteration 69 : 0.3228277114433265
Loss in iteration 70 : 0.3228190461454824
Loss in iteration 71 : 0.32281125338266586
Loss in iteration 72 : 0.3228043271189888
Loss in iteration 73 : 0.3227982366659112
Loss in iteration 74 : 0.3227929207154269
Loss in iteration 75 : 0.32278828832669654
Loss in iteration 76 : 0.3227842251485321
Loss in iteration 77 : 0.3227806029943067
Loss in iteration 78 : 0.32277729094162416
Loss in iteration 79 : 0.322774166308141
Loss in iteration 80 : 0.32277112411942266
Loss in iteration 81 : 0.3227680840262397
Loss in iteration 82 : 0.3227649940396309
Loss in iteration 83 : 0.3227618309057149
Loss in iteration 84 : 0.3227585973899176
Loss in iteration 85 : 0.3227553171207588
Loss in iteration 86 : 0.3227520278998392
Loss in iteration 87 : 0.32274877448249234
Loss in iteration 88 : 0.3227456017704949
Loss in iteration 89 : 0.32274254916391176
Loss in iteration 90 : 0.3227396465462858
Loss in iteration 91 : 0.32273691208491884
Loss in iteration 92 : 0.32273435176766624
Loss in iteration 93 : 0.32273196040259666
Loss in iteration 94 : 0.3227297236899825
Loss in iteration 95 : 0.3227276209334231
Loss in iteration 96 : 0.3227256279733348
Loss in iteration 97 : 0.32272371998425886
Loss in iteration 98 : 0.32272187386018875
Loss in iteration 99 : 0.32272007000623465
Loss in iteration 100 : 0.3227182934498229
Loss in iteration 101 : 0.32271653427247854
Loss in iteration 102 : 0.32271478743664467
Testing accuracy  of updater 8 on alg 0 with rate 0.8 = 0.8500706344819114, training accuracy 0.8492628992628992, time elapsed: 2091 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.4577850796816444
Loss in iteration 3 : 0.4317104833504113
Loss in iteration 4 : 0.39014996229685356
Loss in iteration 5 : 0.3548575007978861
Loss in iteration 6 : 0.3399185926776311
Loss in iteration 7 : 0.3379516996501227
Loss in iteration 8 : 0.33889775849386855
Loss in iteration 9 : 0.33900986698565005
Loss in iteration 10 : 0.3385170082364432
Loss in iteration 11 : 0.33825207495846227
Loss in iteration 12 : 0.3383979813555599
Loss in iteration 13 : 0.33863842003365086
Loss in iteration 14 : 0.3386196175267189
Loss in iteration 15 : 0.3382036129647766
Loss in iteration 16 : 0.33746426393733614
Loss in iteration 17 : 0.33656184634397734
Loss in iteration 18 : 0.33563017000207007
Loss in iteration 19 : 0.3347356344300987
Loss in iteration 20 : 0.33389571575320726
Loss in iteration 21 : 0.3331132822263545
Loss in iteration 22 : 0.33239434623968744
Loss in iteration 23 : 0.3317451381782633
Loss in iteration 24 : 0.3311625602828692
Loss in iteration 25 : 0.33063113411338524
Loss in iteration 26 : 0.330128607917476
Loss in iteration 27 : 0.3296344207952312
Loss in iteration 28 : 0.3291352143271839
Loss in iteration 29 : 0.3286259637079308
Loss in iteration 30 : 0.32810859804512665
Loss in iteration 31 : 0.32759015999539576
Loss in iteration 32 : 0.32708113284122653
Loss in iteration 33 : 0.3265936477546965
Loss in iteration 34 : 0.3261394491407823
Loss in iteration 35 : 0.3257280043901763
Loss in iteration 36 : 0.32536526806986144
Loss in iteration 37 : 0.32505331492032863
Loss in iteration 38 : 0.3247907067339112
Loss in iteration 39 : 0.3245733036840169
Loss in iteration 40 : 0.3243952529208192
Loss in iteration 41 : 0.3242499631827972
Loss in iteration 42 : 0.32413093437854096
Loss in iteration 43 : 0.32403235767442845
Loss in iteration 44 : 0.32394945217150933
Loss in iteration 45 : 0.32387855674942395
Loss in iteration 46 : 0.32381703504872134
Loss in iteration 47 : 0.32376306834966356
Loss in iteration 48 : 0.3237154076071192
Loss in iteration 49 : 0.3236731405087138
Loss in iteration 50 : 0.323635509661521
Loss in iteration 51 : 0.3236017983419143
Loss in iteration 52 : 0.323571283210443
Loss in iteration 53 : 0.3235432408283605
Loss in iteration 54 : 0.3235169878551355
Loss in iteration 55 : 0.3234919334812899
Loss in iteration 56 : 0.3234676258441486
Loss in iteration 57 : 0.32344378011055874
Loss in iteration 58 : 0.3234202827457196
Loss in iteration 59 : 0.323397172710608
Loss in iteration 60 : 0.3233746049396382
Loss in iteration 61 : 0.32335280397968963
Loss in iteration 62 : 0.3233320161445709
Loss in iteration 63 : 0.3233124673548126
Loss in iteration 64 : 0.32329433160598825
Loss in iteration 65 : 0.32327771239911013
Loss in iteration 66 : 0.3232626370354328
Loss in iteration 67 : 0.3232490618072714
Loss in iteration 68 : 0.32323688498577635
Loss in iteration 69 : 0.3232259641182217
Loss in iteration 70 : 0.32321613437618885
Loss in iteration 71 : 0.3232072253481286
Loss in iteration 72 : 0.3231990745354395
Loss in iteration 73 : 0.3231915367040789
Loss in iteration 74 : 0.3231844890241836
Loss in iteration 75 : 0.32317783251426857
Loss in iteration 76 : 0.3231714906620126
Loss in iteration 77 : 0.3231654062309107
Loss in iteration 78 : 0.3231595372198446
Loss in iteration 79 : 0.32315385277527436
Loss in iteration 80 : 0.3231483296207551
Loss in iteration 81 : 0.3231429493180581
Loss in iteration 82 : 0.32313769644867024
Loss in iteration 83 : 0.3231325576307332
Loss in iteration 84 : 0.32312752117636895
Loss in iteration 85 : 0.32312257714753617
Loss in iteration 86 : 0.3231177175755469
Loss in iteration 87 : 0.3231129366543539
Loss in iteration 88 : 0.3231082307835439
Loss in iteration 89 : 0.32310359840730896
Loss in iteration 90 : 0.32309903965731723
Loss in iteration 91 : 0.323094555852279
Loss in iteration 92 : 0.32309014893125426
Loss in iteration 93 : 0.323085820902167
Loss in iteration 94 : 0.32308157337535026
Loss in iteration 95 : 0.32307740722979034
Loss in iteration 96 : 0.323073322433357
Loss in iteration 97 : 0.32306931801278105
Loss in iteration 98 : 0.32306539214882574
Loss in iteration 99 : 0.3230615423592648
Loss in iteration 100 : 0.32305776572727146
Loss in iteration 101 : 0.3230540591352644
Loss in iteration 102 : 0.3230504194717857
Loss in iteration 103 : 0.32304684378990783
Loss in iteration 104 : 0.3230433294070799
Loss in iteration 105 : 0.323039873947053
Loss in iteration 106 : 0.3230364753324127
Loss in iteration 107 : 0.3230331317413651
Loss in iteration 108 : 0.3230298415443454
Loss in iteration 109 : 0.3230266032350876
Loss in iteration 110 : 0.323023415368179
Loss in iteration 111 : 0.32302027651114495
Loss in iteration 112 : 0.3230171852150799
Loss in iteration 113 : 0.3230141400040544
Loss in iteration 114 : 0.32301113938073345
Loss in iteration 115 : 0.3230081818437202
Loss in iteration 116 : 0.32300526591155354
Loss in iteration 117 : 0.32300239014837084
Loss in iteration 118 : 0.32299955318712775
Loss in iteration 119 : 0.32299675374753384
Loss in iteration 120 : 0.3229939906471634
Loss in iteration 121 : 0.32299126280541407
Loss in iteration 122 : 0.3229885692410815
Loss in iteration 123 : 0.322985909064765
Loss in iteration 124 : 0.3229832814678608
Loss in iteration 125 : 0.3229806857097133
Loss in iteration 126 : 0.3229781211045031
Loss in iteration 127 : 0.32297558700894347
Loss in iteration 128 : 0.3229730828116193
Loss in iteration 129 : 0.32297060792435084
Loss in iteration 130 : 0.3229681617756908
Loss in iteration 131 : 0.3229657438064155
Loss in iteration 132 : 0.32296335346671406
Loss in iteration 133 : 0.32296099021472385
Loss in iteration 134 : 0.32295865351607095
Loss in iteration 135 : 0.32295634284402125
Loss in iteration 136 : 0.32295405768004043
Loss in iteration 137 : 0.3229517975145073
Loss in iteration 138 : 0.3229495618474299
Loss in iteration 139 : 0.32294735018913673
Loss in iteration 140 : 0.3229451620608032
Loss in iteration 141 : 0.3229429969948781
Loss in iteration 142 : 0.32294085453534177
Loss in iteration 143 : 0.32293873423786773
Loss in iteration 144 : 0.322936635669824
Loss in iteration 145 : 0.32293455841022356
Loss in iteration 146 : 0.3229325020495397
Loss in iteration 147 : 0.3229304661894952
Loss in iteration 148 : 0.32292845044277896
Loss in iteration 149 : 0.3229264544327188
Loss in iteration 150 : 0.32292447779294664
Loss in iteration 151 : 0.3229225201670471
Loss in iteration 152 : 0.32292058120819117
Loss in iteration 153 : 0.32291866057880636
Loss in iteration 154 : 0.32291675795023206
Loss in iteration 155 : 0.32291487300241556
Loss in iteration 156 : 0.3229130054236083
Loss in iteration 157 : 0.3229111549100947
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.8500092131932928, training accuracy 0.8488943488943489, time elapsed: 3220 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.46656188205874044
Loss in iteration 3 : 0.4383117905514225
Loss in iteration 4 : 0.4081492647888065
Loss in iteration 5 : 0.3733432706199779
Loss in iteration 6 : 0.349056016514497
Loss in iteration 7 : 0.3392295323708163
Loss in iteration 8 : 0.33772631138010767
Loss in iteration 9 : 0.3381922850183331
Loss in iteration 10 : 0.33803714347111785
Loss in iteration 11 : 0.33728359546002507
Loss in iteration 12 : 0.33660314632042937
Loss in iteration 13 : 0.3363181756637612
Loss in iteration 14 : 0.3363078876086575
Loss in iteration 15 : 0.3362799193702022
Loss in iteration 16 : 0.33601894387058484
Loss in iteration 17 : 0.3354777797243047
Loss in iteration 18 : 0.334738181292955
Loss in iteration 19 : 0.3339224757148347
Loss in iteration 20 : 0.3331239356319262
Loss in iteration 21 : 0.33238374267191617
Loss in iteration 22 : 0.3317054575326439
Loss in iteration 23 : 0.3310810544071331
Loss in iteration 24 : 0.3305073505313108
Loss in iteration 25 : 0.3299871479186203
Loss in iteration 26 : 0.32952198208751043
Loss in iteration 27 : 0.32910614775171687
Loss in iteration 28 : 0.32872674836880444
Loss in iteration 29 : 0.32836833968068696
Loss in iteration 30 : 0.32801803886831477
Loss in iteration 31 : 0.3276681711264344
Loss in iteration 32 : 0.32731617751402153
Loss in iteration 33 : 0.3269631827830832
Loss in iteration 34 : 0.3266126076939959
Loss in iteration 35 : 0.3262692973979911
Loss in iteration 36 : 0.32593891060033786
Loss in iteration 37 : 0.32562722659794474
Loss in iteration 38 : 0.32533933450234614
Loss in iteration 39 : 0.3250789262793155
Loss in iteration 40 : 0.3248479159134069
Loss in iteration 41 : 0.3246464391550885
Loss in iteration 42 : 0.3244731335502841
Loss in iteration 43 : 0.3243255502777303
Loss in iteration 44 : 0.32420058504923743
Loss in iteration 45 : 0.3240948694142419
Loss in iteration 46 : 0.32400509624367035
Loss in iteration 47 : 0.3239282636732386
Loss in iteration 48 : 0.32386182703700306
Loss in iteration 49 : 0.32380375866454314
Loss in iteration 50 : 0.32375252909753405
Loss in iteration 51 : 0.3237070335339879
Loss in iteration 52 : 0.323666490184064
Loss in iteration 53 : 0.32363033368677563
Loss in iteration 54 : 0.3235981201943134
Loss in iteration 55 : 0.32356945402705073
Loss in iteration 56 : 0.3235439401281024
Loss in iteration 57 : 0.3235211620025547
Loss in iteration 58 : 0.32350068133412935
Loss in iteration 59 : 0.323482053152449
Loss in iteration 60 : 0.32346484944796533
Loss in iteration 61 : 0.3234486844642436
Loss in iteration 62 : 0.32343323622781356
Loss in iteration 63 : 0.3234182607440233
Loss in iteration 64 : 0.3234035972526134
Loss in iteration 65 : 0.32338916467653567
Loss in iteration 66 : 0.3233749507224914
Loss in iteration 67 : 0.32336099591818634
Loss in iteration 68 : 0.32334737519691165
Loss in iteration 69 : 0.3233341795258058
Loss in iteration 70 : 0.32332149962912954
Loss in iteration 71 : 0.3233094132178252
Loss in iteration 72 : 0.3232979764363189
Loss in iteration 73 : 0.3232872195898264
Loss in iteration 74 : 0.3232771466978404
Loss in iteration 75 : 0.3232677380717364
Loss in iteration 76 : 0.3232589549424135
Loss in iteration 77 : 0.3232507451491464
Loss in iteration 78 : 0.3232430490081144
Loss in iteration 79 : 0.32323580466647944
Loss in iteration 80 : 0.32322895247308664
Loss in iteration 81 : 0.32322243812352547
Loss in iteration 82 : 0.323216214537457
Loss in iteration 83 : 0.32321024258232506
Loss in iteration 84 : 0.3232044908618848
Loss in iteration 85 : 0.3231989348408092
Loss in iteration 86 : 0.323193555584193
Loss in iteration 87 : 0.3231883383635366
Loss in iteration 88 : 0.323183271330254
Loss in iteration 89 : 0.3231783443958787
Loss in iteration 90 : 0.32317354839515505
Loss in iteration 91 : 0.3231688745516601
Loss in iteration 92 : 0.3231643142213822
Loss in iteration 93 : 0.3231598588595561
Loss in iteration 94 : 0.3231555001408689
Loss in iteration 95 : 0.3231512301607659
Loss in iteration 96 : 0.32314704165343916
Loss in iteration 97 : 0.32314292817660045
Loss in iteration 98 : 0.32313888423078113
Loss in iteration 99 : 0.32313490529875305
Loss in iteration 100 : 0.3231309878062173
Loss in iteration 101 : 0.32312712901665236
Loss in iteration 102 : 0.3231233268805051
Loss in iteration 103 : 0.3231195798618039
Loss in iteration 104 : 0.3231158867642105
Loss in iteration 105 : 0.3231122465748636
Loss in iteration 106 : 0.3231086583387818
Loss in iteration 107 : 0.3231051210705298
Loss in iteration 108 : 0.3231016337041457
Loss in iteration 109 : 0.32309819507755244
Loss in iteration 110 : 0.32309480394441353
Loss in iteration 111 : 0.3230914590046478
Loss in iteration 112 : 0.3230881589444953
Loss in iteration 113 : 0.3230849024779783
Loss in iteration 114 : 0.3230816883832394
Loss in iteration 115 : 0.3230785155294153
Loss in iteration 116 : 0.32307538289190074
Loss in iteration 117 : 0.32307228955584366
Loss in iteration 118 : 0.3230692347092767
Loss in iteration 119 : 0.3230662176283655
Loss in iteration 120 : 0.3230632376577288
Loss in iteration 121 : 0.32306029418884363
Loss in iteration 122 : 0.3230573866392163
Loss in iteration 123 : 0.3230545144343496
Loss in iteration 124 : 0.32305167699391146
Loss in iteration 125 : 0.3230488737226437
Loss in iteration 126 : 0.3230461040060692
Loss in iteration 127 : 0.3230433672104044
Loss in iteration 128 : 0.3230406626857922
Loss in iteration 129 : 0.3230379897718801
Loss in iteration 130 : 0.3230353478046275
Loss in iteration 131 : 0.32303273612340433
Loss in iteration 132 : 0.3230301540776196
Loss in iteration 133 : 0.3230276010323113
Loss in iteration 134 : 0.3230250763724152
Loss in iteration 135 : 0.32302257950554986
Loss in iteration 136 : 0.3230201098634219
Loss in iteration 137 : 0.3230176669020019
Loss in iteration 138 : 0.3230152501007418
Loss in iteration 139 : 0.3230128589610842
Loss in iteration 140 : 0.32301049300459184
Loss in iteration 141 : 0.32300815177085757
Loss in iteration 142 : 0.3230058348154733
Loss in iteration 143 : 0.32300354170810414
Loss in iteration 144 : 0.3230012720308459
Loss in iteration 145 : 0.32299902537681885
Loss in iteration 146 : 0.322996801349058
Loss in iteration 147 : 0.3229945995596578
Loss in iteration 148 : 0.3229924196291287
Loss in iteration 149 : 0.32299026118593194
Loss in iteration 150 : 0.3229881238661505
Loss in iteration 151 : 0.32298600731324395
Loss in iteration 152 : 0.32298391117786934
Loss in iteration 153 : 0.3229818351177382
Loss in iteration 154 : 0.3229797787974964
Loss in iteration 155 : 0.322977741888608
Loss in iteration 156 : 0.3229757240692358
Loss in iteration 157 : 0.3229737250241506
Loss in iteration 158 : 0.3229717444445941
Loss in iteration 159 : 0.3229697820281881
Loss in iteration 160 : 0.3229678374788041
Loss in iteration 161 : 0.3229659105064595
Loss in iteration 162 : 0.3229640008271979
Loss in iteration 163 : 0.3229621081629777
Loss in iteration 164 : 0.3229602322415444
Loss in iteration 165 : 0.3229583727963285
Loss in iteration 166 : 0.3229565295663059
Loss in iteration 167 : 0.322954702295896
Loss in iteration 168 : 0.322952890734825
Loss in iteration 169 : 0.3229510946380187
Loss in iteration 170 : 0.32294931376546515
Testing accuracy  of updater 8 on alg 0 with rate 0.14 = 0.8501320557705301, training accuracy 0.8488636363636364, time elapsed: 3472 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5058272973536231
Loss in iteration 3 : 0.45276488841559914
Loss in iteration 4 : 0.43245018339826186
Loss in iteration 5 : 0.4107297185382953
Loss in iteration 6 : 0.3851595768490643
Loss in iteration 7 : 0.3622578036656962
Loss in iteration 8 : 0.34714435135458627
Loss in iteration 9 : 0.3401038720058184
Loss in iteration 10 : 0.3381750748872446
Loss in iteration 11 : 0.3380821138137322
Loss in iteration 12 : 0.33787295639320547
Loss in iteration 13 : 0.3370191695494569
Loss in iteration 14 : 0.33579458372103854
Loss in iteration 15 : 0.334641676624415
Loss in iteration 16 : 0.3338217700850757
Loss in iteration 17 : 0.3333464948147179
Loss in iteration 18 : 0.3330678654197002
Loss in iteration 19 : 0.33280466100437495
Loss in iteration 20 : 0.3324322524957831
Loss in iteration 21 : 0.331914014844459
Loss in iteration 22 : 0.33128550664989975
Loss in iteration 23 : 0.33061599821122045
Loss in iteration 24 : 0.32997089753038766
Loss in iteration 25 : 0.32938957065264873
Loss in iteration 26 : 0.32888183369829943
Loss in iteration 27 : 0.3284376580378941
Loss in iteration 28 : 0.32804098607753046
Loss in iteration 29 : 0.327679998085905
Loss in iteration 30 : 0.32735053355283883
Loss in iteration 31 : 0.32705376032210465
Loss in iteration 32 : 0.32679153908466735
Loss in iteration 33 : 0.32656280184299
Loss in iteration 34 : 0.3263625599460202
Loss in iteration 35 : 0.3261832618315201
Loss in iteration 36 : 0.32601713491677825
Loss in iteration 37 : 0.3258581188581354
Loss in iteration 38 : 0.3257026639442355
Loss in iteration 39 : 0.32554945124163004
Loss in iteration 40 : 0.3253985629900367
Loss in iteration 41 : 0.325250673358265
Loss in iteration 42 : 0.32510657914635216
Loss in iteration 43 : 0.3249670823282928
Loss in iteration 44 : 0.32483304966282245
Loss in iteration 45 : 0.3247054586902116
Loss in iteration 46 : 0.32458533573942583
Loss in iteration 47 : 0.32447360394714203
Loss in iteration 48 : 0.32437091962783987
Loss in iteration 49 : 0.3242775698450444
Loss in iteration 50 : 0.3241934599427333
Loss in iteration 51 : 0.32411817470272847
Loss in iteration 52 : 0.3240510743618346
Loss in iteration 53 : 0.3239913895712148
Loss in iteration 54 : 0.3239382961196191
Loss in iteration 55 : 0.3238909670965437
Loss in iteration 56 : 0.3238486090940742
Loss in iteration 57 : 0.32381048948197877
Loss in iteration 58 : 0.3237759577955379
Loss in iteration 59 : 0.32374446042830557
Loss in iteration 60 : 0.32371554657318874
Loss in iteration 61 : 0.32368886460920115
Loss in iteration 62 : 0.3236641503567533
Loss in iteration 63 : 0.32364121027767967
Loss in iteration 64 : 0.32361990306905675
Loss in iteration 65 : 0.3236001223799639
Loss in iteration 66 : 0.32358178221113404
Loss in iteration 67 : 0.3235648055359964
Loss in iteration 68 : 0.3235491160832089
Loss in iteration 69 : 0.3235346330036565
Loss in iteration 70 : 0.32352126811460863
Loss in iteration 71 : 0.32350892539509996
Loss in iteration 72 : 0.3234975023313522
Loss in iteration 73 : 0.3234868926057697
Loss in iteration 74 : 0.3234769895524844
Loss in iteration 75 : 0.3234676898110666
Loss in iteration 76 : 0.32345889670008104
Loss in iteration 77 : 0.32345052297250215
Loss in iteration 78 : 0.3234424927643374
Loss in iteration 79 : 0.32343474267628025
Loss in iteration 80 : 0.32342722202312957
Loss in iteration 81 : 0.3234198923494826
Loss in iteration 82 : 0.3234127263499119
Loss in iteration 83 : 0.32340570635443805
Loss in iteration 84 : 0.3233988225481289
Loss in iteration 85 : 0.32339207108835877
Loss in iteration 86 : 0.3233854522651755
Loss in iteration 87 : 0.32337896882240375
Loss in iteration 88 : 0.3233726245231159
Loss in iteration 89 : 0.32336642300839014
Loss in iteration 90 : 0.3233603669660077
Loss in iteration 91 : 0.3233544575993169
Loss in iteration 92 : 0.32334869436630675
Loss in iteration 93 : 0.3233430749456101
Loss in iteration 94 : 0.32333759537883155
Loss in iteration 95 : 0.32333225033642193
Loss in iteration 96 : 0.32332703345675684
Loss in iteration 97 : 0.3233219377134635
Loss in iteration 98 : 0.3233169557740125
Loss in iteration 99 : 0.3233120803216122
Loss in iteration 100 : 0.3233073043216866
Loss in iteration 101 : 0.3233026212230212
Loss in iteration 102 : 0.32329802509109856
Loss in iteration 103 : 0.3232935106772798
Loss in iteration 104 : 0.32328907343182206
Loss in iteration 105 : 0.3232847094715272
Loss in iteration 106 : 0.3232804155140462
Loss in iteration 107 : 0.3232761887910247
Loss in iteration 108 : 0.3232720269511278
Loss in iteration 109 : 0.3232679279626329
Loss in iteration 110 : 0.3232638900228889
Loss in iteration 111 : 0.32325991148004407
Loss in iteration 112 : 0.32325599077006323
Loss in iteration 113 : 0.32325212637024936
Loss in iteration 114 : 0.3232483167688317
Loss in iteration 115 : 0.3232445604489769
Loss in iteration 116 : 0.3232408558847793
Loss in iteration 117 : 0.323237201546306
Loss in iteration 118 : 0.323233595910813
Loss in iteration 119 : 0.3232300374772132
Loss in iteration 120 : 0.3232265247814666
Loss in iteration 121 : 0.3232230564108979
Loss in iteration 122 : 0.3232196310160583
Loss in iteration 123 : 0.32321624731933724
Loss in iteration 124 : 0.3232129041198765
Loss in iteration 125 : 0.323209600294976
Loss in iteration 126 : 0.3232063347982642
Loss in iteration 127 : 0.32320310665532376
Loss in iteration 128 : 0.32319991495740835
Loss in iteration 129 : 0.3231967588540492
Loss in iteration 130 : 0.32319363754522795
Loss in iteration 131 : 0.3231905502736915
Loss in iteration 132 : 0.3231874963179199
Loss in iteration 133 : 0.32318447498604896
Loss in iteration 134 : 0.3231814856109303
Loss in iteration 135 : 0.32317852754641196
Loss in iteration 136 : 0.3231756001647604
Loss in iteration 137 : 0.3231727028551126
Loss in iteration 138 : 0.32316983502276797
Loss in iteration 139 : 0.3231669960890977
Loss in iteration 140 : 0.3231641854918816
Loss in iteration 141 : 0.32316140268584453
Loss in iteration 142 : 0.32315864714323184
Loss in iteration 143 : 0.3231559183543021
Loss in iteration 144 : 0.3231532158275913
Loss in iteration 145 : 0.32315053908996805
Loss in iteration 146 : 0.3231478876863872
Loss in iteration 147 : 0.32314526117938913
Loss in iteration 148 : 0.3231426591483673
Loss in iteration 149 : 0.3231400811886442
Loss in iteration 150 : 0.32313752691042646
Loss in iteration 151 : 0.3231349959376816
Loss in iteration 152 : 0.3231324879069863
Loss in iteration 153 : 0.32313000246642243
Loss in iteration 154 : 0.3231275392745266
Loss in iteration 155 : 0.32312509799932276
Loss in iteration 156 : 0.323122678317493
Loss in iteration 157 : 0.3231202799136429
Loss in iteration 158 : 0.32311790247969807
Loss in iteration 159 : 0.32311554571439777
Loss in iteration 160 : 0.32311320932290816
Loss in iteration 161 : 0.3231108930165028
Loss in iteration 162 : 0.3231085965123264
Loss in iteration 163 : 0.32310631953318375
Loss in iteration 164 : 0.3231040618074102
Loss in iteration 165 : 0.323101823068717
Loss in iteration 166 : 0.3230996030561055
Loss in iteration 167 : 0.3230974015137288
Loss in iteration 168 : 0.32309521819080245
Loss in iteration 169 : 0.3230930528414966
Loss in iteration 170 : 0.32309090522479933
Loss in iteration 171 : 0.3230887751044197
Loss in iteration 172 : 0.323086662248641
Loss in iteration 173 : 0.3230845664302038
Loss in iteration 174 : 0.32308248742617085
Loss in iteration 175 : 0.3230804250177884
Loss in iteration 176 : 0.32307837899037123
Loss in iteration 177 : 0.32307634913315764
Loss in iteration 178 : 0.32307433523920104
Loss in iteration 179 : 0.3230723371052475
Loss in iteration 180 : 0.32307035453161714
Loss in iteration 181 : 0.3230683873221104
Loss in iteration 182 : 0.3230664352838934
Loss in iteration 183 : 0.32306449822742567
Testing accuracy  of updater 8 on alg 0 with rate 0.08000000000000002 = 0.8500706344819114, training accuracy 0.8490786240786241, time elapsed: 3741 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6239946612272862
Loss in iteration 3 : 0.5590642009317136
Loss in iteration 4 : 0.5100442637642914
Loss in iteration 5 : 0.47799724631798834
Loss in iteration 6 : 0.45850800917378326
Loss in iteration 7 : 0.44616724724037027
Loss in iteration 8 : 0.43673257829904033
Loss in iteration 9 : 0.4276175005535858
Loss in iteration 10 : 0.4176631786060133
Loss in iteration 11 : 0.40672394539663465
Loss in iteration 12 : 0.39526316375180387
Loss in iteration 13 : 0.3840120361754077
Loss in iteration 14 : 0.37370434267592484
Loss in iteration 15 : 0.36489541120888414
Loss in iteration 16 : 0.3578702855418031
Loss in iteration 17 : 0.35263507710754244
Loss in iteration 18 : 0.34897148793725574
Loss in iteration 19 : 0.3465253862111642
Loss in iteration 20 : 0.3449001509133946
Loss in iteration 21 : 0.34373292348571616
Loss in iteration 22 : 0.34274264789616654
Loss in iteration 23 : 0.34174872077727975
Loss in iteration 24 : 0.3406658374878979
Loss in iteration 25 : 0.33948369638565806
Loss in iteration 26 : 0.3382403058214413
Loss in iteration 27 : 0.3369958773123811
Loss in iteration 28 : 0.33581177763589737
Loss in iteration 29 : 0.33473655097358385
Loss in iteration 30 : 0.3337990699788008
Loss in iteration 31 : 0.3330076072293233
Loss in iteration 32 : 0.33235301280781004
Loss in iteration 33 : 0.3318141034750859
Loss in iteration 34 : 0.3313636352363973
Loss in iteration 35 : 0.3309736725278225
Loss in iteration 36 : 0.3306196479647225
Loss in iteration 37 : 0.3302828364222043
Loss in iteration 38 : 0.329951298574243
Loss in iteration 39 : 0.32961956714985224
Loss in iteration 40 : 0.3292874608204409
Loss in iteration 41 : 0.32895843431352195
Loss in iteration 42 : 0.32863783233935834
Loss in iteration 43 : 0.3283313329499624
Loss in iteration 44 : 0.3280437647458406
Loss in iteration 45 : 0.3277783802299472
Loss in iteration 46 : 0.3275365786633289
Loss in iteration 47 : 0.3273180053196441
Loss in iteration 48 : 0.3271209144505668
Loss in iteration 49 : 0.32694267031312985
Loss in iteration 50 : 0.3267802702087664
Loss in iteration 51 : 0.3266307990186841
Loss in iteration 52 : 0.3264917584895434
Loss in iteration 53 : 0.3263612491861718
Loss in iteration 54 : 0.32623801273624875
Loss in iteration 55 : 0.32612136305738976
Loss in iteration 56 : 0.3260110463013061
Loss in iteration 57 : 0.32590707091343735
Loss in iteration 58 : 0.3258095435606441
Loss in iteration 59 : 0.3257185365207049
Loss in iteration 60 : 0.3256340003112986
Loss in iteration 61 : 0.3255557242700552
Loss in iteration 62 : 0.3254833391394271
Loss in iteration 63 : 0.32541635028080484
Loss in iteration 64 : 0.32535418800456284
Loss in iteration 65 : 0.3252962621531435
Loss in iteration 66 : 0.3252420106776901
Loss in iteration 67 : 0.32519093556714007
Loss in iteration 68 : 0.32514262326350535
Loss in iteration 69 : 0.3250967499691472
Loss in iteration 70 : 0.3250530746163457
Loss in iteration 71 : 0.3250114235747572
Loss in iteration 72 : 0.32497167147793093
Loss in iteration 73 : 0.32493372206272336
Loss in iteration 74 : 0.32489749191751355
Loss in iteration 75 : 0.3248628988200205
Loss in iteration 76 : 0.32482985516747415
Loss in iteration 77 : 0.3247982660442235
Loss in iteration 78 : 0.32476803083594963
Loss in iteration 79 : 0.32473904700781747
Loss in iteration 80 : 0.32471121467643077
Loss in iteration 81 : 0.32468444084233833
Loss in iteration 82 : 0.3246586425141378
Loss in iteration 83 : 0.32463374835340175
Loss in iteration 84 : 0.324609698826545
Loss in iteration 85 : 0.3245864451162643
Loss in iteration 86 : 0.3245639471996876
Loss in iteration 87 : 0.3245421715461337
Loss in iteration 88 : 0.32452108884445907
Loss in iteration 89 : 0.324500672068669
Loss in iteration 90 : 0.32448089506273586
Loss in iteration 91 : 0.32446173169973286
Loss in iteration 92 : 0.32444315556708136
Loss in iteration 93 : 0.3244251400603245
Loss in iteration 94 : 0.32440765873543
Loss in iteration 95 : 0.32439068576983127
Loss in iteration 96 : 0.3243741964070558
Loss in iteration 97 : 0.32435816729822536
Loss in iteration 98 : 0.3243425766962045
Loss in iteration 99 : 0.324327404496512
Loss in iteration 100 : 0.32431263214802425
Loss in iteration 101 : 0.32429824247375916
Loss in iteration 102 : 0.3242842194477292
Loss in iteration 103 : 0.3242705479701376
Loss in iteration 104 : 0.32425721367309845
Loss in iteration 105 : 0.3242442027761581
Loss in iteration 106 : 0.3242315019978153
Loss in iteration 107 : 0.3242190985185394
Loss in iteration 108 : 0.3242069799834837
Loss in iteration 109 : 0.32419513452958787
Loss in iteration 110 : 0.3241835508217837
Loss in iteration 111 : 0.3241722180855173
Loss in iteration 112 : 0.32416112612676323
Loss in iteration 113 : 0.3241502653352386
Loss in iteration 114 : 0.3241396266704503
Loss in iteration 115 : 0.324129201633366
Loss in iteration 116 : 0.32411898222815577
Loss in iteration 117 : 0.3241089609191836
Loss in iteration 118 : 0.32409913058789236
Loss in iteration 119 : 0.3240894844932076
Loss in iteration 120 : 0.3240800162376293
Loss in iteration 121 : 0.32407071973984236
Loss in iteration 122 : 0.32406158921342926
Loss in iteration 123 : 0.3240526191505482
Loss in iteration 124 : 0.3240438043090338
Loss in iteration 125 : 0.32403513970129627
Loss in iteration 126 : 0.324026620583803
Loss in iteration 127 : 0.32401824244610755
Loss in iteration 128 : 0.3240100009990656
Loss in iteration 129 : 0.3240018921621638
Loss in iteration 130 : 0.32399391205021877
Loss in iteration 131 : 0.3239860569598935
Loss in iteration 132 : 0.3239783233565263
Loss in iteration 133 : 0.32397070786173116
Loss in iteration 134 : 0.3239632072420444
Loss in iteration 135 : 0.3239558183988229
Loss in iteration 136 : 0.3239485383594309
Loss in iteration 137 : 0.3239413642695551
Loss in iteration 138 : 0.32393429338655083
Loss in iteration 139 : 0.323927323073543
Loss in iteration 140 : 0.32392045079411497
Loss in iteration 141 : 0.3239136741073885
Loss in iteration 142 : 0.32390699066337686
Loss in iteration 143 : 0.32390039819852723
Loss in iteration 144 : 0.3238938945314236
Loss in iteration 145 : 0.3238874775586462
Loss in iteration 146 : 0.3238811452508452
Loss in iteration 147 : 0.3238748956489973
Loss in iteration 148 : 0.3238687268609162
Loss in iteration 149 : 0.32386263705803375
Loss in iteration 150 : 0.3238566244724265
Loss in iteration 151 : 0.323850687394069
Loss in iteration 152 : 0.32384482416835314
Loss in iteration 153 : 0.323839033193743
Loss in iteration 154 : 0.32383331291962625
Loss in iteration 155 : 0.3238276618442822
Loss in iteration 156 : 0.3238220785129471
Loss in iteration 157 : 0.3238165615159792
Loss in iteration 158 : 0.3238111094871057
Loss in iteration 159 : 0.3238057211016976
Loss in iteration 160 : 0.32380039507515995
Loss in iteration 161 : 0.3237951301613648
Loss in iteration 162 : 0.32378992515113675
Loss in iteration 163 : 0.32378477887084056
Loss in iteration 164 : 0.32377969018100616
Loss in iteration 165 : 0.32377465797502986
Loss in iteration 166 : 0.3237696811779341
Loss in iteration 167 : 0.32376475874518884
Loss in iteration 168 : 0.32375988966159047
Loss in iteration 169 : 0.3237550729401662
Loss in iteration 170 : 0.3237503076211697
Loss in iteration 171 : 0.32374559277106507
Loss in iteration 172 : 0.3237409274815908
Loss in iteration 173 : 0.3237363108688355
Loss in iteration 174 : 0.3237317420723778
Loss in iteration 175 : 0.3237272202544223
Loss in iteration 176 : 0.3237227445990021
Loss in iteration 177 : 0.3237183143112049
Loss in iteration 178 : 0.32371392861642534
Loss in iteration 179 : 0.32370958675965483
Loss in iteration 180 : 0.3237052880048046
Loss in iteration 181 : 0.3237010316340413
Loss in iteration 182 : 0.3236968169471786
Loss in iteration 183 : 0.3236926432610686
Loss in iteration 184 : 0.3236885099090315
Loss in iteration 185 : 0.3236844162403024
Loss in iteration 186 : 0.32368036161952063
Loss in iteration 187 : 0.32367634542620066
Loss in iteration 188 : 0.32367236705426405
Loss in iteration 189 : 0.3236684259115762
Loss in iteration 190 : 0.3236645214194845
Loss in iteration 191 : 0.3236606530124055
Loss in iteration 192 : 0.32365682013741415
Loss in iteration 193 : 0.3236530222538357
Loss in iteration 194 : 0.32364925883287937
Loss in iteration 195 : 0.32364552935727176
Loss in iteration 196 : 0.3236418333209044
Loss in iteration 197 : 0.323638170228502
Loss in iteration 198 : 0.3236345395952986
Loss in iteration 199 : 0.32363094094672534
Loss in iteration 200 : 0.32362737381811274
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.8498863706160555, training accuracy 0.8492321867321867, time elapsed: 3819 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 19.370822584528653
Loss in iteration 3 : 13.741824437046205
Loss in iteration 4 : 3.5982748803758895
Loss in iteration 5 : 12.279619196380287
Loss in iteration 6 : 11.510445664214062
Loss in iteration 7 : 6.068500112873534
Loss in iteration 8 : 7.0991080788984835
Loss in iteration 9 : 9.914449891531314
Loss in iteration 10 : 10.369821537127784
Loss in iteration 11 : 8.371464732113738
Loss in iteration 12 : 6.3880441735299955
Loss in iteration 13 : 6.8053391423911
Loss in iteration 14 : 8.40094547912981
Loss in iteration 15 : 8.012728416643872
Loss in iteration 16 : 6.056005138710548
Loss in iteration 17 : 5.412710536453648
Loss in iteration 18 : 6.485414954767418
Loss in iteration 19 : 6.6616511353501755
Loss in iteration 20 : 5.156236036980289
Loss in iteration 21 : 4.216842960571525
Loss in iteration 22 : 5.342104519261457
Loss in iteration 23 : 4.740943641370825
Loss in iteration 24 : 3.2325643370774784
Loss in iteration 25 : 4.060265958264196
Loss in iteration 26 : 3.734260241685279
Loss in iteration 27 : 2.378023970230913
Loss in iteration 28 : 3.596947857538603
Loss in iteration 29 : 2.0987732722639554
Loss in iteration 30 : 2.816308965373309
Loss in iteration 31 : 2.2679706212080464
Loss in iteration 32 : 2.296405389432482
Loss in iteration 33 : 1.72525740982227
Loss in iteration 34 : 2.3439398829832565
Loss in iteration 35 : 1.301786665010121
Loss in iteration 36 : 3.04448844027972
Loss in iteration 37 : 2.2149124814043804
Loss in iteration 38 : 2.139160921114832
Loss in iteration 39 : 1.6623640935503738
Loss in iteration 40 : 2.1693046871143156
Loss in iteration 41 : 1.4942745781137994
Loss in iteration 42 : 1.9905235362958151
Loss in iteration 43 : 1.4991041552469078
Loss in iteration 44 : 1.5192980118975699
Loss in iteration 45 : 1.4698430145389365
Loss in iteration 46 : 1.3534049492489892
Loss in iteration 47 : 1.2959613921879876
Loss in iteration 48 : 1.4027101515260005
Loss in iteration 49 : 0.7712513199152435
Loss in iteration 50 : 0.9359242827309284
Loss in iteration 51 : 1.0284033515962399
Loss in iteration 52 : 0.8454301335876145
Loss in iteration 53 : 0.6318538902520395
Loss in iteration 54 : 1.0153949244167393
Loss in iteration 55 : 3.0676995153112365
Loss in iteration 56 : 1.7622483663042061
Loss in iteration 57 : 4.363350692994798
Loss in iteration 58 : 1.1393375402492079
Loss in iteration 59 : 2.575736882810778
Loss in iteration 60 : 2.5647527187880286
Loss in iteration 61 : 1.9864305862474272
Loss in iteration 62 : 2.4556386268401327
Loss in iteration 63 : 2.835638048673168
Loss in iteration 64 : 2.30055191016773
Loss in iteration 65 : 2.160766935831657
Loss in iteration 66 : 2.4346790272066494
Loss in iteration 67 : 2.304975868616549
Loss in iteration 68 : 1.8349417262322847
Loss in iteration 69 : 1.8421498348957528
Loss in iteration 70 : 1.9451202548762274
Loss in iteration 71 : 1.4315675955322889
Loss in iteration 72 : 1.5912762362947417
Loss in iteration 73 : 1.4470347567945367
Loss in iteration 74 : 1.2034253399293053
Loss in iteration 75 : 1.113082589817701
Loss in iteration 76 : 1.1955190214789886
Loss in iteration 77 : 0.7718661351590564
Loss in iteration 78 : 1.4674055191672106
Loss in iteration 79 : 1.0115325828782062
Loss in iteration 80 : 0.9582117213994911
Loss in iteration 81 : 1.2733631806440715
Loss in iteration 82 : 0.731928186467755
Loss in iteration 83 : 1.015617527237584
Loss in iteration 84 : 0.8444207212734257
Loss in iteration 85 : 0.5821831935476492
Loss in iteration 86 : 0.7250352016026352
Loss in iteration 87 : 0.6880948822302468
Loss in iteration 88 : 0.5745877437366151
Loss in iteration 89 : 0.5245444206345878
Loss in iteration 90 : 0.5778048295016625
Loss in iteration 91 : 0.7919446249686124
Loss in iteration 92 : 0.6922638977728445
Loss in iteration 93 : 0.60489383696699
Loss in iteration 94 : 0.4983217892865238
Loss in iteration 95 : 0.5976826298793747
Loss in iteration 96 : 0.8776460349146292
Loss in iteration 97 : 0.5798409634383778
Loss in iteration 98 : 0.4675457538304897
Loss in iteration 99 : 0.4060410689567976
Loss in iteration 100 : 0.4655258612369058
Loss in iteration 101 : 0.8543346766883728
Loss in iteration 102 : 0.8237770603320363
Loss in iteration 103 : 1.1401134623985942
Loss in iteration 104 : 0.5780317930922748
Loss in iteration 105 : 1.210021312583576
Loss in iteration 106 : 0.6535863767833787
Loss in iteration 107 : 0.9906004750513567
Loss in iteration 108 : 0.6103045991736468
Loss in iteration 109 : 0.9640473402024743
Loss in iteration 110 : 0.559878479381982
Loss in iteration 111 : 0.7988133788795976
Loss in iteration 112 : 0.6169011873477551
Loss in iteration 113 : 0.4464862288597038
Loss in iteration 114 : 0.528440236304386
Loss in iteration 115 : 0.7323185894799403
Loss in iteration 116 : 1.2431557754895426
Loss in iteration 117 : 0.4687163423396638
Loss in iteration 118 : 1.3086070757891786
Loss in iteration 119 : 0.9433839108992582
Loss in iteration 120 : 1.009940493026005
Loss in iteration 121 : 0.7897346619531623
Loss in iteration 122 : 0.9886493395044956
Loss in iteration 123 : 0.7329862328670166
Loss in iteration 124 : 0.9471452810792979
Loss in iteration 125 : 0.5747776438610172
Loss in iteration 126 : 0.9163668489622607
Loss in iteration 127 : 0.8075600943424467
Loss in iteration 128 : 0.6821620566874934
Loss in iteration 129 : 1.0817989846540539
Loss in iteration 130 : 0.5920004424630785
Loss in iteration 131 : 0.6876897085178385
Loss in iteration 132 : 0.7200332799353149
Loss in iteration 133 : 0.507336998203824
Loss in iteration 134 : 0.7281439523524772
Loss in iteration 135 : 0.4175947590113034
Loss in iteration 136 : 0.5243989694018899
Loss in iteration 137 : 1.1813500072195147
Loss in iteration 138 : 0.3832928802104286
Loss in iteration 139 : 1.4403704661938548
Loss in iteration 140 : 1.9479996034489113
Loss in iteration 141 : 2.1959883487170098
Loss in iteration 142 : 0.9520956027382983
Loss in iteration 143 : 1.7833822687447347
Loss in iteration 144 : 1.9241750026165503
Loss in iteration 145 : 1.229542970831823
Loss in iteration 146 : 1.5492415576896408
Loss in iteration 147 : 1.813812779536341
Loss in iteration 148 : 1.3726368216782505
Loss in iteration 149 : 1.1233025677738262
Loss in iteration 150 : 1.5231738221505542
Loss in iteration 151 : 1.170597078961037
Loss in iteration 152 : 0.9870554713064853
Loss in iteration 153 : 1.277762439100854
Loss in iteration 154 : 0.8005384755427807
Loss in iteration 155 : 1.1658472237378663
Loss in iteration 156 : 0.5904449544580782
Loss in iteration 157 : 0.9453288418933352
Loss in iteration 158 : 0.5475014381948616
Loss in iteration 159 : 1.131490817660152
Loss in iteration 160 : 0.6084857555906407
Loss in iteration 161 : 0.7531627643131437
Loss in iteration 162 : 0.8625205996237862
Loss in iteration 163 : 0.4932448015141634
Loss in iteration 164 : 0.8009061760600944
Loss in iteration 165 : 0.4593246527335201
Loss in iteration 166 : 0.6399780937293421
Loss in iteration 167 : 0.9140175674186954
Loss in iteration 168 : 0.5567999763617861
Loss in iteration 169 : 1.1924473029410225
Loss in iteration 170 : 0.5779415032730001
Loss in iteration 171 : 0.8603998407246386
Loss in iteration 172 : 0.5554273862539861
Loss in iteration 173 : 0.85282762240883
Loss in iteration 174 : 0.5490026075606091
Loss in iteration 175 : 0.7661895618586102
Loss in iteration 176 : 0.49614971489812065
Loss in iteration 177 : 0.7867793300341249
Loss in iteration 178 : 0.5192961028680934
Loss in iteration 179 : 0.4835995539417265
Loss in iteration 180 : 0.8201927412428442
Loss in iteration 181 : 0.7030630824434673
Loss in iteration 182 : 0.4689909127006339
Loss in iteration 183 : 0.8686536397028354
Loss in iteration 184 : 0.5471336744535695
Loss in iteration 185 : 0.6309280608260471
Loss in iteration 186 : 0.6061935142717028
Loss in iteration 187 : 0.457785416597872
Loss in iteration 188 : 0.6438664972528368
Loss in iteration 189 : 0.3872455929464147
Loss in iteration 190 : 0.6927086914625847
Loss in iteration 191 : 0.9619233033075257
Loss in iteration 192 : 0.5632460160602977
Loss in iteration 193 : 1.194657906822457
Loss in iteration 194 : 0.5044691139582901
Loss in iteration 195 : 0.8977152755919966
Loss in iteration 196 : 0.5563315935734473
Loss in iteration 197 : 0.8274822147141684
Loss in iteration 198 : 0.5626596057714811
Loss in iteration 199 : 0.7214559245970359
Loss in iteration 200 : 0.5541586954431673
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.7407407407407407, training accuracy 0.7462837837837838, time elapsed: 3343 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.8000933783379516
Loss in iteration 3 : 0.4741704909258731
Loss in iteration 4 : 0.7665616003716095
Loss in iteration 5 : 0.7090218294246483
Loss in iteration 6 : 0.7154606441559928
Loss in iteration 7 : 0.8210783004081387
Loss in iteration 8 : 0.7540649926150321
Loss in iteration 9 : 0.6964451767629763
Loss in iteration 10 : 0.7611723030893753
Loss in iteration 11 : 0.6889988821410905
Loss in iteration 12 : 0.6702780966392652
Loss in iteration 13 : 0.7089258857671351
Loss in iteration 14 : 0.6056285780411975
Loss in iteration 15 : 0.6565471943583905
Loss in iteration 16 : 0.5162229609662569
Loss in iteration 17 : 0.5446051945582945
Loss in iteration 18 : 0.45685046621807696
Loss in iteration 19 : 0.5265340027160533
Loss in iteration 20 : 0.4575686320872397
Loss in iteration 21 : 0.48234830299033316
Loss in iteration 22 : 0.4953438216250916
Loss in iteration 23 : 0.4412959216052406
Loss in iteration 24 : 0.44903886534660525
Loss in iteration 25 : 0.4600981518597182
Loss in iteration 26 : 0.4136641993763074
Loss in iteration 27 : 0.3963588354356178
Loss in iteration 28 : 0.41535440593746703
Loss in iteration 29 : 0.3909636277262623
Loss in iteration 30 : 0.40538488503860387
Loss in iteration 31 : 0.3835389443149286
Loss in iteration 32 : 0.38681585324917067
Loss in iteration 33 : 0.37848439775329135
Loss in iteration 34 : 0.36227787427165026
Loss in iteration 35 : 0.3704340917168737
Loss in iteration 36 : 0.3552015130567684
Loss in iteration 37 : 0.34678739929932106
Loss in iteration 38 : 0.3614044419592025
Loss in iteration 39 : 0.35764531349484485
Loss in iteration 40 : 0.34362183516744516
Loss in iteration 41 : 0.3500980521561132
Loss in iteration 42 : 0.3530770945720177
Loss in iteration 43 : 0.33447316145206757
Loss in iteration 44 : 0.33364815414990884
Loss in iteration 45 : 0.3402005512728587
Loss in iteration 46 : 0.3300598831975713
Loss in iteration 47 : 0.3327781885191148
Loss in iteration 48 : 0.33833990594958574
Loss in iteration 49 : 0.3312658185579642
Loss in iteration 50 : 0.3356356008787726
Loss in iteration 51 : 0.33854598235744926
Loss in iteration 52 : 0.33068729599589397
Loss in iteration 53 : 0.33188650261414415
Loss in iteration 54 : 0.33380613618557486
Loss in iteration 55 : 0.3268236484776651
Loss in iteration 56 : 0.3263226994453455
Loss in iteration 57 : 0.3294790825722438
Loss in iteration 58 : 0.32593300563743116
Loss in iteration 59 : 0.32516606490748634
Loss in iteration 60 : 0.3283713208034568
Loss in iteration 61 : 0.3269739614473072
Loss in iteration 62 : 0.3248430188531339
Loss in iteration 63 : 0.3262741724430854
Loss in iteration 64 : 0.32656799828615457
Loss in iteration 65 : 0.3246613657504041
Loss in iteration 66 : 0.3245229745642379
Loss in iteration 67 : 0.3252040743955361
Loss in iteration 68 : 0.3240742390935623
Loss in iteration 69 : 0.32329432126646673
Loss in iteration 70 : 0.3240780012131361
Loss in iteration 71 : 0.3240288681226996
Loss in iteration 72 : 0.3231917892003861
Loss in iteration 73 : 0.3235569983578562
Loss in iteration 74 : 0.32403062285943846
Loss in iteration 75 : 0.32344063976197485
Loss in iteration 76 : 0.3232861828464702
Loss in iteration 77 : 0.32367686779758925
Loss in iteration 78 : 0.32334643015518794
Loss in iteration 79 : 0.3228919060192461
Loss in iteration 80 : 0.323065538656223
Loss in iteration 81 : 0.3231311205935183
Loss in iteration 82 : 0.32283801411784546
Loss in iteration 83 : 0.32283836456927034
Loss in iteration 84 : 0.323050918390658
Loss in iteration 85 : 0.32295774737191585
Loss in iteration 86 : 0.32281286963928635
Loss in iteration 87 : 0.32293401816831585
Loss in iteration 88 : 0.3229604570653049
Loss in iteration 89 : 0.3227852814946752
Loss in iteration 90 : 0.3227672225594651
Loss in iteration 91 : 0.32284426576824204
Loss in iteration 92 : 0.3227615629923618
Loss in iteration 93 : 0.3226793354884721
Loss in iteration 94 : 0.32273518352470637
Loss in iteration 95 : 0.3227512895431906
Loss in iteration 96 : 0.322683308886017
Loss in iteration 97 : 0.32268627287397017
Loss in iteration 98 : 0.32273879382816334
Loss in iteration 99 : 0.3227183234493631
Loss in iteration 100 : 0.3226807326014272
Loss in iteration 101 : 0.32270285335903415
Loss in iteration 102 : 0.32270904304784237
Loss in iteration 103 : 0.3226665958150088
Loss in iteration 104 : 0.32265284082654966
Loss in iteration 105 : 0.32266895540399426
Loss in iteration 106 : 0.32265671274502494
Loss in iteration 107 : 0.3226390835412428
Loss in iteration 108 : 0.32265366048316063
Loss in iteration 109 : 0.32266537075470875
Loss in iteration 110 : 0.32265149994057973
Loss in iteration 111 : 0.32264551028206623
Loss in iteration 112 : 0.3226556561638027
Loss in iteration 113 : 0.32265316093358254
Testing accuracy  of updater 9 on alg 0 with rate 1.4000000000000001 = 0.849640685461581, training accuracy 0.8493857493857494, time elapsed: 2155 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.8574572954435861
Loss in iteration 3 : 0.6869153355300075
Loss in iteration 4 : 0.45953104634073993
Loss in iteration 5 : 0.758874897064655
Loss in iteration 6 : 0.7416905389088133
Loss in iteration 7 : 0.6031718900075789
Loss in iteration 8 : 0.6826439265894146
Loss in iteration 9 : 0.767318227386506
Loss in iteration 10 : 0.7130721888786518
Loss in iteration 11 : 0.609138921274109
Loss in iteration 12 : 0.6212555080829635
Loss in iteration 13 : 0.6756089341108673
Loss in iteration 14 : 0.6038350253780347
Loss in iteration 15 : 0.5417200109901503
Loss in iteration 16 : 0.5979697468814522
Loss in iteration 17 : 0.5740215374575808
Loss in iteration 18 : 0.48734013023294886
Loss in iteration 19 : 0.5311710325887752
Loss in iteration 20 : 0.4720699995769479
Loss in iteration 21 : 0.424659422164206
Loss in iteration 22 : 0.4573269232642808
Loss in iteration 23 : 0.3910299313724099
Loss in iteration 24 : 0.42708290243927144
Loss in iteration 25 : 0.40148775330747444
Loss in iteration 26 : 0.3938787451768277
Loss in iteration 27 : 0.4063640852409723
Loss in iteration 28 : 0.37754315534074034
Loss in iteration 29 : 0.40731246804132615
Loss in iteration 30 : 0.3735316441488918
Loss in iteration 31 : 0.39004220164930936
Loss in iteration 32 : 0.3538974910714701
Loss in iteration 33 : 0.37106794377926583
Loss in iteration 34 : 0.3426391064813205
Loss in iteration 35 : 0.3575289678001949
Loss in iteration 36 : 0.34424896638996016
Loss in iteration 37 : 0.3490256103185118
Loss in iteration 38 : 0.34704029710673656
Loss in iteration 39 : 0.3400488996457218
Loss in iteration 40 : 0.34531755193007035
Loss in iteration 41 : 0.33411782064114315
Loss in iteration 42 : 0.34249118166990244
Loss in iteration 43 : 0.33222989093454175
Loss in iteration 44 : 0.33800607741723615
Loss in iteration 45 : 0.3304812348977924
Loss in iteration 46 : 0.33341610413621725
Loss in iteration 47 : 0.32902045024238896
Loss in iteration 48 : 0.3308872198914933
Loss in iteration 49 : 0.329278398763104
Loss in iteration 50 : 0.3293050378781561
Loss in iteration 51 : 0.3282796222390582
Loss in iteration 52 : 0.32725384712133815
Loss in iteration 53 : 0.3272676035170683
Loss in iteration 54 : 0.32616987829340577
Loss in iteration 55 : 0.3269533563859475
Loss in iteration 56 : 0.32548947696310876
Loss in iteration 57 : 0.32640798674812976
Loss in iteration 58 : 0.3247466201745631
Loss in iteration 59 : 0.3259562583584515
Loss in iteration 60 : 0.3242856496746488
Loss in iteration 61 : 0.3253629259557505
Loss in iteration 62 : 0.3239769086062469
Loss in iteration 63 : 0.3248827188179311
Loss in iteration 64 : 0.3239028928018921
Loss in iteration 65 : 0.3244825411114143
Loss in iteration 66 : 0.3238062405745627
Loss in iteration 67 : 0.32401222030257576
Loss in iteration 68 : 0.3235266533051533
Loss in iteration 69 : 0.32353300407966984
Loss in iteration 70 : 0.32329717199541136
Loss in iteration 71 : 0.32323974450386017
Loss in iteration 72 : 0.32322914104524775
Loss in iteration 73 : 0.3231361675818809
Loss in iteration 74 : 0.32324784779574134
Loss in iteration 75 : 0.3230822835666746
Loss in iteration 76 : 0.3232222774435108
Loss in iteration 77 : 0.3230075706771484
Loss in iteration 78 : 0.32314611246293096
Loss in iteration 79 : 0.3229201698073779
Loss in iteration 80 : 0.323037024215878
Loss in iteration 81 : 0.3228153221569164
Loss in iteration 82 : 0.32291678132057083
Loss in iteration 83 : 0.32274799184972197
Loss in iteration 84 : 0.3228598181469849
Loss in iteration 85 : 0.32274528060261437
Loss in iteration 86 : 0.3228414949339622
Loss in iteration 87 : 0.32275676638952727
Loss in iteration 88 : 0.32281981437067137
Loss in iteration 89 : 0.32275051190642434
Loss in iteration 90 : 0.3227829550615358
Loss in iteration 91 : 0.32273276983675336
Loss in iteration 92 : 0.3227501066401464
Loss in iteration 93 : 0.32272175278098286
Loss in iteration 94 : 0.32272744327360914
Loss in iteration 95 : 0.32271041379092663
Loss in iteration 96 : 0.32270351091537997
Loss in iteration 97 : 0.32269655900159777
Loss in iteration 98 : 0.3226866711249707
Loss in iteration 99 : 0.32268927371482004
Loss in iteration 100 : 0.3226759605385111
Loss in iteration 101 : 0.32268056316063215
Loss in iteration 102 : 0.32266501087961863
Loss in iteration 103 : 0.32267307475542356
Loss in iteration 104 : 0.32266084666167355
Loss in iteration 105 : 0.32267157578275696
Loss in iteration 106 : 0.32266044082888434
Loss in iteration 107 : 0.32266937670319706
Loss in iteration 108 : 0.32265838304933175
Loss in iteration 109 : 0.32266551263323756
Loss in iteration 110 : 0.32265549872371124
Loss in iteration 111 : 0.32266060262295576
Loss in iteration 112 : 0.3226514440866554
Loss in iteration 113 : 0.32265482207350576
Loss in iteration 114 : 0.3226473394426535
Loss in iteration 115 : 0.32265014111505375
Loss in iteration 116 : 0.3226449579652929
Loss in iteration 117 : 0.3226475476194974
Loss in iteration 118 : 0.3226441550897401
Loss in iteration 119 : 0.32264616514461786
Testing accuracy  of updater 9 on alg 0 with rate 0.8 = 0.8498249493274369, training accuracy 0.8492014742014742, time elapsed: 3526 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.46658988504656673
Loss in iteration 3 : 0.4749160187880532
Loss in iteration 4 : 0.4522784666476683
Loss in iteration 5 : 0.37638291076880287
Loss in iteration 6 : 0.34338898237908116
Loss in iteration 7 : 0.37670727252503106
Loss in iteration 8 : 0.4118364837155876
Loss in iteration 9 : 0.40801994156227817
Loss in iteration 10 : 0.38391582048707823
Loss in iteration 11 : 0.37313155378961144
Loss in iteration 12 : 0.38313158200909264
Loss in iteration 13 : 0.3977420667828704
Loss in iteration 14 : 0.40045425457990513
Loss in iteration 15 : 0.3886195589020736
Loss in iteration 16 : 0.3728411145521991
Loss in iteration 17 : 0.36594963448995277
Loss in iteration 18 : 0.3702416971520132
Loss in iteration 19 : 0.37563353618877193
Loss in iteration 20 : 0.3720404012925637
Loss in iteration 21 : 0.3612680291272943
Loss in iteration 22 : 0.35373448699569504
Loss in iteration 23 : 0.35469894563776616
Loss in iteration 24 : 0.35833198355762286
Loss in iteration 25 : 0.3564202500699684
Loss in iteration 26 : 0.3487026016948575
Loss in iteration 27 : 0.3423678871298729
Loss in iteration 28 : 0.3420702613318899
Loss in iteration 29 : 0.343468431751275
Loss in iteration 30 : 0.3403326393127492
Loss in iteration 31 : 0.334361486866972
Loss in iteration 32 : 0.3316569748537658
Loss in iteration 33 : 0.3329301840332323
Loss in iteration 34 : 0.3335635841125778
Loss in iteration 35 : 0.33121966911771855
Loss in iteration 36 : 0.3286299515617214
Loss in iteration 37 : 0.32871431515631333
Loss in iteration 38 : 0.3300556053445297
Loss in iteration 39 : 0.3295817782336508
Loss in iteration 40 : 0.327567542644848
Loss in iteration 41 : 0.32663937208126254
Loss in iteration 42 : 0.3273043559736166
Loss in iteration 43 : 0.3276253639261146
Loss in iteration 44 : 0.32664576586112937
Loss in iteration 45 : 0.3256558513521723
Loss in iteration 46 : 0.3258139901796751
Loss in iteration 47 : 0.3263213523543516
Loss in iteration 48 : 0.32596797854583925
Loss in iteration 49 : 0.32510863886908886
Loss in iteration 50 : 0.3247966266876851
Loss in iteration 51 : 0.32499806055289243
Loss in iteration 52 : 0.3248908969348995
Loss in iteration 53 : 0.32430440572346614
Loss in iteration 54 : 0.32386522703482
Loss in iteration 55 : 0.323904582860279
Loss in iteration 56 : 0.3240133294020194
Loss in iteration 57 : 0.3238119781700098
Loss in iteration 58 : 0.32352408251993303
Loss in iteration 59 : 0.32350180160526926
Loss in iteration 60 : 0.3236524404377459
Loss in iteration 61 : 0.32365833431089275
Loss in iteration 62 : 0.3234872924498258
Loss in iteration 63 : 0.3233776736827241
Loss in iteration 64 : 0.3234252300785899
Loss in iteration 65 : 0.32346321981751197
Loss in iteration 66 : 0.3233671154011628
Loss in iteration 67 : 0.3232419840399496
Loss in iteration 68 : 0.32322294427765075
Loss in iteration 69 : 0.3232666144562599
Loss in iteration 70 : 0.32325292278085893
Loss in iteration 71 : 0.32318150743611335
Loss in iteration 72 : 0.32314782177142226
Loss in iteration 73 : 0.3231766572769176
Loss in iteration 74 : 0.3231936142540797
Loss in iteration 75 : 0.32315548018582524
Loss in iteration 76 : 0.3231099926822192
Loss in iteration 77 : 0.32310490989917834
Loss in iteration 78 : 0.32311528393981565
Loss in iteration 79 : 0.32309707512053276
Loss in iteration 80 : 0.3230597662141439
Loss in iteration 81 : 0.32304276431485046
Loss in iteration 82 : 0.3230495803429205
Loss in iteration 83 : 0.3230488929458815
Loss in iteration 84 : 0.32302896342546583
Loss in iteration 85 : 0.3230114146634391
Loss in iteration 86 : 0.3230107034781103
Loss in iteration 87 : 0.3230126132327716
Loss in iteration 88 : 0.3230014580687545
Loss in iteration 89 : 0.3229846471318201
Loss in iteration 90 : 0.32297715119040565
Loss in iteration 91 : 0.3229772376271706
Loss in iteration 92 : 0.32297257961615167
Loss in iteration 93 : 0.32296161120790456
Loss in iteration 94 : 0.3229540067811913
Loss in iteration 95 : 0.322953435823421
Loss in iteration 96 : 0.3229528291669216
Loss in iteration 97 : 0.322947169890302
Loss in iteration 98 : 0.32294066541277155
Loss in iteration 99 : 0.32293828052262014
Loss in iteration 100 : 0.32293773635345735
Loss in iteration 101 : 0.3229343401654881
Loss in iteration 102 : 0.32292872100444386
Loss in iteration 103 : 0.3229248452088513
Loss in iteration 104 : 0.32292322814171576
Loss in iteration 105 : 0.3229208200898785
Loss in iteration 106 : 0.32291645987815376
Loss in iteration 107 : 0.32291235167700133
Loss in iteration 108 : 0.3229100206600176
Loss in iteration 109 : 0.32290807952652323
Loss in iteration 110 : 0.32290494831879296
Loss in iteration 111 : 0.32290135698123157
Loss in iteration 112 : 0.32289876179340066
Loss in iteration 113 : 0.32289691936830056
Loss in iteration 114 : 0.32289456935013927
Loss in iteration 115 : 0.32289157585456163
Loss in iteration 116 : 0.32288891099629097
Loss in iteration 117 : 0.32288691260019575
Loss in iteration 118 : 0.3228848655197274
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.8500706344819114, training accuracy 0.8491400491400491, time elapsed: 5219 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.4940276027720122
Loss in iteration 3 : 0.4599033039889472
Loss in iteration 4 : 0.46430033774089774
Loss in iteration 5 : 0.4220029942692332
Loss in iteration 6 : 0.36301018788870487
Loss in iteration 7 : 0.3412128773485771
Loss in iteration 8 : 0.3627799912103406
Loss in iteration 9 : 0.39086189166857715
Loss in iteration 10 : 0.3963637590480553
Loss in iteration 11 : 0.3813181447638086
Loss in iteration 12 : 0.3650856428524367
Loss in iteration 13 : 0.3615087325469871
Loss in iteration 14 : 0.36944875000635286
Loss in iteration 15 : 0.3788738567154949
Loss in iteration 16 : 0.3809965820084164
Loss in iteration 17 : 0.37396263610321945
Loss in iteration 18 : 0.3627039277219265
Loss in iteration 19 : 0.3545475708958516
Loss in iteration 20 : 0.3534729256683552
Loss in iteration 21 : 0.35714200111254646
Loss in iteration 22 : 0.35946742090456074
Loss in iteration 23 : 0.35646478938440057
Loss in iteration 24 : 0.34971066987609123
Loss in iteration 25 : 0.3442128136701693
Loss in iteration 26 : 0.3431736988402845
Loss in iteration 27 : 0.3451398399659832
Loss in iteration 28 : 0.3460854453287858
Loss in iteration 29 : 0.34361324733270543
Loss in iteration 30 : 0.338990318841166
Loss in iteration 31 : 0.3354868408474497
Loss in iteration 32 : 0.3348621803414356
Loss in iteration 33 : 0.33561839190877146
Loss in iteration 34 : 0.3349984961834243
Loss in iteration 35 : 0.3323091101450536
Loss in iteration 36 : 0.32946382170946925
Loss in iteration 37 : 0.32839724512711815
Loss in iteration 38 : 0.32890058924030224
Loss in iteration 39 : 0.3292747207525911
Loss in iteration 40 : 0.3284499649447895
Loss in iteration 41 : 0.3269899951173584
Loss in iteration 42 : 0.32616280564701466
Loss in iteration 43 : 0.3263867032497032
Loss in iteration 44 : 0.3268544237358183
Loss in iteration 45 : 0.3266377458427096
Loss in iteration 46 : 0.3257824528624243
Loss in iteration 47 : 0.3250830962513348
Loss in iteration 48 : 0.325024656079681
Loss in iteration 49 : 0.32529028001033206
Loss in iteration 50 : 0.32528520016951395
Loss in iteration 51 : 0.32486807307328364
Loss in iteration 52 : 0.324426962987118
Loss in iteration 53 : 0.3243357476447078
Loss in iteration 54 : 0.32451421470285136
Loss in iteration 55 : 0.3245911087174531
Loss in iteration 56 : 0.32438743842626927
Loss in iteration 57 : 0.32408880333595214
Loss in iteration 58 : 0.32395387920829605
Loss in iteration 59 : 0.32399609154675313
Loss in iteration 60 : 0.3240189372077158
Loss in iteration 61 : 0.3238876383267326
Loss in iteration 62 : 0.3236747743354846
Loss in iteration 63 : 0.32353990663856375
Loss in iteration 64 : 0.3235318413797339
Loss in iteration 65 : 0.32355452571913784
Loss in iteration 66 : 0.3235066020617208
Loss in iteration 67 : 0.3234001371926935
Loss in iteration 68 : 0.32332347455351873
Loss in iteration 69 : 0.32332226540314996
Loss in iteration 70 : 0.3233534137186217
Loss in iteration 71 : 0.32334994397840544
Loss in iteration 72 : 0.32330020605555976
Loss in iteration 73 : 0.3232489759885191
Loss in iteration 74 : 0.323234520116365
Loss in iteration 75 : 0.3232452962052247
Loss in iteration 76 : 0.3232424319404283
Loss in iteration 77 : 0.3232101557596649
Loss in iteration 78 : 0.32317036930371446
Loss in iteration 79 : 0.3231508030125849
Loss in iteration 80 : 0.3231523647991009
Loss in iteration 81 : 0.32315355574113136
Loss in iteration 82 : 0.32313963403021323
Loss in iteration 83 : 0.3231182079937745
Loss in iteration 84 : 0.3231059376905666
Loss in iteration 85 : 0.32310711023990163
Loss in iteration 86 : 0.32311046821585815
Loss in iteration 87 : 0.3231047740749124
Loss in iteration 88 : 0.3230915983163454
Loss in iteration 89 : 0.3230808948748133
Loss in iteration 90 : 0.3230777427973545
Loss in iteration 91 : 0.323077390213863
Loss in iteration 92 : 0.3230727528675261
Loss in iteration 93 : 0.32306322319690867
Loss in iteration 94 : 0.32305427162261774
Loss in iteration 95 : 0.32305001076368883
Loss in iteration 96 : 0.3230486796734349
Loss in iteration 97 : 0.32304578449237953
Loss in iteration 98 : 0.3230398197699263
Loss in iteration 99 : 0.32303339898541217
Loss in iteration 100 : 0.32302934032491176
Loss in iteration 101 : 0.32302726207077953
Loss in iteration 102 : 0.3230245889144516
Loss in iteration 103 : 0.32301995753860796
Loss in iteration 104 : 0.32301458304431807
Loss in iteration 105 : 0.3230103670080912
Loss in iteration 106 : 0.3230075648612163
Loss in iteration 107 : 0.3230048235480046
Loss in iteration 108 : 0.3230010998494897
Loss in iteration 109 : 0.32299684831811415
Loss in iteration 110 : 0.32299321342171433
Loss in iteration 111 : 0.3229905517320017
Loss in iteration 112 : 0.3229881524750135
Loss in iteration 113 : 0.3229852614114177
Loss in iteration 114 : 0.3229819595900715
Loss in iteration 115 : 0.32297889262290796
Loss in iteration 116 : 0.3229763870107901
Loss in iteration 117 : 0.32297411096470496
Loss in iteration 118 : 0.32297157879504107
Loss in iteration 119 : 0.3229687455236574
Loss in iteration 120 : 0.3229659716746285
Loss in iteration 121 : 0.32296351769161485
Loss in iteration 122 : 0.32296125380798596
Loss in iteration 123 : 0.32295888706335985
Loss in iteration 124 : 0.32295633899042864
Loss in iteration 125 : 0.32295379880075803
Loss in iteration 126 : 0.3229514499849647
Loss in iteration 127 : 0.32294925483368486
Loss in iteration 128 : 0.3229470409557261
Loss in iteration 129 : 0.32294473063533014
Loss in iteration 130 : 0.3229424143176137
Loss in iteration 131 : 0.32294021036237885
Loss in iteration 132 : 0.322938117770268
Testing accuracy  of updater 9 on alg 0 with rate 0.14 = 0.8501320557705301, training accuracy 0.8489250614250614, time elapsed: 3733 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.5514909226450285
Loss in iteration 3 : 0.4655048091125989
Loss in iteration 4 : 0.4566389858750882
Loss in iteration 5 : 0.4546177272886502
Loss in iteration 6 : 0.4287389298845607
Loss in iteration 7 : 0.3866537498303044
Loss in iteration 8 : 0.35145978080913864
Loss in iteration 9 : 0.3397734379073293
Loss in iteration 10 : 0.3497813759104794
Loss in iteration 11 : 0.36669105678907027
Loss in iteration 12 : 0.37633356472347806
Loss in iteration 13 : 0.37382988852427484
Loss in iteration 14 : 0.36334607688769915
Loss in iteration 15 : 0.35259247267001775
Loss in iteration 16 : 0.3471968902373344
Loss in iteration 17 : 0.34816832382718743
Loss in iteration 18 : 0.35276342628255286
Loss in iteration 19 : 0.357007158847031
Loss in iteration 20 : 0.35797892940104004
Loss in iteration 21 : 0.35494700800751056
Loss in iteration 22 : 0.3492786280654182
Loss in iteration 23 : 0.3434535092137804
Loss in iteration 24 : 0.33969310842112577
Loss in iteration 25 : 0.33883799534005726
Loss in iteration 26 : 0.34004506468251616
Loss in iteration 27 : 0.34146554089414843
Loss in iteration 28 : 0.34145753177297344
Loss in iteration 29 : 0.3395449693547429
Loss in iteration 30 : 0.3365440377404688
Loss in iteration 31 : 0.33386596690143644
Loss in iteration 32 : 0.33254050889172954
Loss in iteration 33 : 0.33262713948893124
Loss in iteration 34 : 0.3333342528829409
Loss in iteration 35 : 0.33364905082705837
Loss in iteration 36 : 0.3329934201376297
Loss in iteration 37 : 0.3315056848572242
Loss in iteration 38 : 0.32984364820768913
Loss in iteration 39 : 0.3286925472923395
Loss in iteration 40 : 0.3283145378267963
Loss in iteration 41 : 0.3284303128722806
Loss in iteration 42 : 0.32848706723774646
Loss in iteration 43 : 0.3280868150147093
Loss in iteration 44 : 0.3272457468964153
Loss in iteration 45 : 0.32631636011173165
Loss in iteration 46 : 0.3256810904965107
Loss in iteration 47 : 0.3254795126587872
Loss in iteration 48 : 0.3255574376212758
Loss in iteration 49 : 0.32562797350626493
Loss in iteration 50 : 0.3254895922464628
Loss in iteration 51 : 0.3251430416916891
Loss in iteration 52 : 0.324749843864936
Loss in iteration 53 : 0.3244894276591587
Loss in iteration 54 : 0.3244285201849452
Loss in iteration 55 : 0.3244908809174918
Loss in iteration 56 : 0.3245334996976378
Loss in iteration 57 : 0.3244585075891349
Loss in iteration 58 : 0.32427420196047535
Loss in iteration 59 : 0.32406937128549573
Loss in iteration 60 : 0.32393485650873316
Loss in iteration 61 : 0.3238986801277462
Loss in iteration 62 : 0.32391843566214273
Loss in iteration 63 : 0.32392490350797515
Loss in iteration 64 : 0.32387597239404936
Loss in iteration 65 : 0.32378111141700294
Loss in iteration 66 : 0.32368489768829173
Loss in iteration 67 : 0.3236286313938221
Loss in iteration 68 : 0.32362099450731924
Loss in iteration 69 : 0.32363712470133515
Loss in iteration 70 : 0.32364197875339085
Loss in iteration 71 : 0.3236169371204783
Loss in iteration 72 : 0.3235697463199377
Loss in iteration 73 : 0.32352357529746295
Loss in iteration 74 : 0.32349705465812345
Loss in iteration 75 : 0.32349151677813787
Loss in iteration 76 : 0.32349324678103225
Loss in iteration 77 : 0.323486160120235
Loss in iteration 78 : 0.3234637755290513
Loss in iteration 79 : 0.32343207626659326
Loss in iteration 80 : 0.3234029613803318
Loss in iteration 81 : 0.3233847141092958
Loss in iteration 82 : 0.32337683561022207
Loss in iteration 83 : 0.3233721319665605
Loss in iteration 84 : 0.32336321134022555
Loss in iteration 85 : 0.3233478815209514
Loss in iteration 86 : 0.32332974680124904
Loss in iteration 87 : 0.3233144787037315
Loss in iteration 88 : 0.3233052849550405
Loss in iteration 89 : 0.3233010040036413
Loss in iteration 90 : 0.323297693102369
Loss in iteration 91 : 0.3232919003277384
Loss in iteration 92 : 0.3232829599709461
Loss in iteration 93 : 0.3232728629398741
Loss in iteration 94 : 0.32326425925553764
Loss in iteration 95 : 0.3232583765945132
Loss in iteration 96 : 0.32325436542788893
Loss in iteration 97 : 0.32325027728501354
Loss in iteration 98 : 0.32324466684263903
Loss in iteration 99 : 0.32323754183998027
Loss in iteration 100 : 0.3232300917411891
Loss in iteration 101 : 0.32322361692780105
Loss in iteration 102 : 0.3232185809055533
Loss in iteration 103 : 0.32321444529956533
Loss in iteration 104 : 0.3232102567982036
Loss in iteration 105 : 0.32320541873524966
Loss in iteration 106 : 0.3232000551017231
Loss in iteration 107 : 0.32319478112679073
Loss in iteration 108 : 0.32319015804025875
Loss in iteration 109 : 0.3231862862993961
Loss in iteration 110 : 0.32318280730015625
Loss in iteration 111 : 0.32317923754318784
Loss in iteration 112 : 0.32317533163040646
Loss in iteration 113 : 0.32317120541837163
Loss in iteration 114 : 0.3231671749368779
Loss in iteration 115 : 0.3231634818017295
Loss in iteration 116 : 0.32316012633251184
Loss in iteration 117 : 0.3231569093563894
Loss in iteration 118 : 0.32315361261436676
Loss in iteration 119 : 0.32315016000151203
Loss in iteration 120 : 0.3231466455319211
Loss in iteration 121 : 0.323143232364318
Loss in iteration 122 : 0.32314002034107425
Loss in iteration 123 : 0.3231369843204586
Loss in iteration 124 : 0.32313401497988514
Loss in iteration 125 : 0.32313101312665293
Loss in iteration 126 : 0.32312795861445487
Loss in iteration 127 : 0.3231249082876684
Loss in iteration 128 : 0.3231219378633751
Loss in iteration 129 : 0.32311908092927827
Loss in iteration 130 : 0.32311631036479627
Loss in iteration 131 : 0.32311356789209317
Loss in iteration 132 : 0.32311081111492945
Loss in iteration 133 : 0.32310804032291013
Loss in iteration 134 : 0.3231052892163121
Loss in iteration 135 : 0.323102593098771
Loss in iteration 136 : 0.32309996206165753
Loss in iteration 137 : 0.32309737787230025
Loss in iteration 138 : 0.3230948123734219
Loss in iteration 139 : 0.32309224979870926
Loss in iteration 140 : 0.32308969581682834
Loss in iteration 141 : 0.3230871691031511
Loss in iteration 142 : 0.32308468490101544
Loss in iteration 143 : 0.3230822441739716
Loss in iteration 144 : 0.32307983528895695
Loss in iteration 145 : 0.3230774446373276
Loss in iteration 146 : 0.3230750666872379
Loss in iteration 147 : 0.3230727061642262
Loss in iteration 148 : 0.323070372234683
Loss in iteration 149 : 0.323068070494817
Loss in iteration 150 : 0.32306579910330807
Loss in iteration 151 : 0.32306355114710855
Loss in iteration 152 : 0.3230613203278377
Loss in iteration 153 : 0.32305910514145336
Loss in iteration 154 : 0.3230569087409022
Loss in iteration 155 : 0.3230547353942615
Loss in iteration 156 : 0.323052586802599
Loss in iteration 157 : 0.32305046103344026
Loss in iteration 158 : 0.3230483543685948
Loss in iteration 159 : 0.32304626414612386
Loss in iteration 160 : 0.32304419029781095
Testing accuracy  of updater 9 on alg 0 with rate 0.08000000000000002 = 0.8501934770591487, training accuracy 0.8488943488943489, time elapsed: 4330 millisecond.
Loss in iteration 1 : 0.693147180560079
Loss in iteration 2 : 0.6494862051078086
Loss in iteration 3 : 0.5861390410377301
Loss in iteration 4 : 0.5273054988198288
Loss in iteration 5 : 0.48643211546041315
Loss in iteration 6 : 0.4642956270257228
Loss in iteration 7 : 0.454469075701725
Loss in iteration 8 : 0.4494763537757781
Loss in iteration 9 : 0.4439095585290453
Loss in iteration 10 : 0.43502275633575516
Loss in iteration 11 : 0.4222718083388213
Loss in iteration 12 : 0.4066234420781973
Loss in iteration 13 : 0.38988524078317466
Loss in iteration 14 : 0.37409433106974255
Loss in iteration 15 : 0.360990422714527
Loss in iteration 16 : 0.35163748644742937
Loss in iteration 17 : 0.34626140605554434
Loss in iteration 18 : 0.3443204650501844
Loss in iteration 19 : 0.3447554861175838
Loss in iteration 20 : 0.34631990064353135
Loss in iteration 21 : 0.3478861191124353
Loss in iteration 22 : 0.3486555027481995
Loss in iteration 23 : 0.3482441707350732
Loss in iteration 24 : 0.3466564411282833
Loss in iteration 25 : 0.3441816832260571
Loss in iteration 26 : 0.341257759211508
Loss in iteration 27 : 0.33833925078131283
Loss in iteration 28 : 0.3357967981327288
Loss in iteration 29 : 0.333859918545881
Loss in iteration 30 : 0.3326031586685023
Loss in iteration 31 : 0.33196657818799813
Loss in iteration 32 : 0.33179721155099634
Loss in iteration 33 : 0.3318979017003473
Loss in iteration 34 : 0.3320724814854315
Loss in iteration 35 : 0.33216016753212674
Loss in iteration 36 : 0.33205595688357137
Loss in iteration 37 : 0.33171701478968013
Loss in iteration 38 : 0.3311572376676482
Loss in iteration 39 : 0.3304334152269129
Loss in iteration 40 : 0.32962688219152186
Loss in iteration 41 : 0.328824432109007
Loss in iteration 42 : 0.32810170894760765
Loss in iteration 43 : 0.3275114079954596
Loss in iteration 44 : 0.3270775192165225
Loss in iteration 45 : 0.32679567761922
Loss in iteration 46 : 0.3266386216545745
Loss in iteration 47 : 0.32656497839273624
Loss in iteration 48 : 0.32652922322262723
Loss in iteration 49 : 0.3264907451299941
Loss in iteration 50 : 0.3264204274390681
Loss in iteration 51 : 0.32630388502723195
Loss in iteration 52 : 0.3261412967410053
Loss in iteration 53 : 0.32594445808650313
Loss in iteration 54 : 0.3257321259668328
Loss in iteration 55 : 0.32552487879263786
Loss in iteration 56 : 0.32534059027036066
Loss in iteration 57 : 0.3251912893657301
Loss in iteration 58 : 0.3250817578665137
Loss in iteration 59 : 0.32500980644863314
Loss in iteration 60 : 0.32496785251837595
Loss in iteration 61 : 0.3249452438491301
Loss in iteration 62 : 0.3249307379852208
Loss in iteration 63 : 0.3249146342549394
Loss in iteration 64 : 0.32489022024702685
Loss in iteration 65 : 0.32485438977562303
Loss in iteration 66 : 0.32480747209763894
Loss in iteration 67 : 0.32475245171698536
Loss in iteration 68 : 0.32469383806110597
Loss in iteration 69 : 0.3246364618679267
Loss in iteration 70 : 0.3245844386716109
Loss in iteration 71 : 0.3245404654160894
Loss in iteration 72 : 0.3245055238846997
Loss in iteration 73 : 0.3244789742464943
Loss in iteration 74 : 0.3244589501378784
Loss in iteration 75 : 0.32444292398435703
Loss in iteration 76 : 0.32442830126089933
Loss in iteration 77 : 0.3244129217387415
Loss in iteration 78 : 0.32439538581431693
Loss in iteration 79 : 0.32437517330190846
Loss in iteration 80 : 0.3243525691127573
Loss in iteration 81 : 0.32432844593842913
Loss in iteration 82 : 0.3243039732079914
Loss in iteration 83 : 0.32428032333402773
Loss in iteration 84 : 0.32425843348036754
Loss in iteration 85 : 0.3242388590984938
Loss in iteration 86 : 0.324221730476717
Loss in iteration 87 : 0.3242068011204381
Loss in iteration 88 : 0.32419356089948365
Loss in iteration 89 : 0.324181379420607
Loss in iteration 90 : 0.3241696458262283
Loss in iteration 91 : 0.32415787841705235
Loss in iteration 92 : 0.32414578847555
Loss in iteration 93 : 0.32413329456081963
Loss in iteration 94 : 0.32412049387920794
Loss in iteration 95 : 0.324107604420949
Loss in iteration 96 : 0.324094894648823
Loss in iteration 97 : 0.3240826167908899
Loss in iteration 98 : 0.32407095608382236
Loss in iteration 99 : 0.3240600029024385
Loss in iteration 100 : 0.3240497489540218
Loss in iteration 101 : 0.3240401037921728
Loss in iteration 102 : 0.3240309246194179
Loss in iteration 103 : 0.32402205104339377
Loss in iteration 104 : 0.3240133370228491
Loss in iteration 105 : 0.3240046742326514
Loss in iteration 106 : 0.3239960038390615
Loss in iteration 107 : 0.3239873165213224
Loss in iteration 108 : 0.32397864292501916
Loss in iteration 109 : 0.32397003821268555
Loss in iteration 110 : 0.3239615648561958
Loss in iteration 111 : 0.32395327738466106
Loss in iteration 112 : 0.3239452117173248
Loss in iteration 113 : 0.32393738031052216
Loss in iteration 114 : 0.3239297729684422
Loss in iteration 115 : 0.32392236207801944
Loss in iteration 116 : 0.3239151103894507
Loss in iteration 117 : 0.32390797931639165
Loss in iteration 118 : 0.3239009360099328
Loss in iteration 119 : 0.3238939580320413
Loss in iteration 120 : 0.3238870351495202
Loss in iteration 121 : 0.32388016842864464
Loss in iteration 122 : 0.32387336731024297
Loss in iteration 123 : 0.32386664561713085
Loss in iteration 124 : 0.3238600174788604
Loss in iteration 125 : 0.3238534939915664
Loss in iteration 126 : 0.32384708113267124
Loss in iteration 127 : 0.32384077910537123
Loss in iteration 128 : 0.32383458297222784
Loss in iteration 129 : 0.32382848420834237
Loss in iteration 130 : 0.32382247269150066
Loss in iteration 131 : 0.32381653865029386
Loss in iteration 132 : 0.3238106741893693
Loss in iteration 133 : 0.3238048741664438
Loss in iteration 134 : 0.32379913636704133
Loss in iteration 135 : 0.323793461073076
Loss in iteration 136 : 0.32378785022423295
Loss in iteration 137 : 0.3237823064152915
Loss in iteration 138 : 0.32377683195966156
Loss in iteration 139 : 0.32377142819326826
Loss in iteration 140 : 0.32376609511228766
Loss in iteration 141 : 0.32376083135419903
Loss in iteration 142 : 0.3237556344612687
Loss in iteration 143 : 0.32375050132064037
Loss in iteration 144 : 0.32374542865981415
Loss in iteration 145 : 0.32374041348786986
Loss in iteration 146 : 0.3237354534043183
Loss in iteration 147 : 0.32373054673847423
Loss in iteration 148 : 0.32372569252315087
Loss in iteration 149 : 0.32372089033890167
Loss in iteration 150 : 0.3237161400841753
Loss in iteration 151 : 0.3237114417311766
Loss in iteration 152 : 0.32370679511894956
Loss in iteration 153 : 0.3237021998180199
Loss in iteration 154 : 0.3236976550801809
Loss in iteration 155 : 0.32369315986737646
Loss in iteration 156 : 0.32368871293882834
Loss in iteration 157 : 0.32368431296778344
Loss in iteration 158 : 0.3236799586586697
Loss in iteration 159 : 0.32367564884076905
Loss in iteration 160 : 0.32367138252376865
Loss in iteration 161 : 0.323667158910959
Loss in iteration 162 : 0.32366297737515015
Loss in iteration 163 : 0.32365883740895784
Loss in iteration 164 : 0.3236547385641333
Loss in iteration 165 : 0.323650680394026
Loss in iteration 166 : 0.32364666241006396
Loss in iteration 167 : 0.3236426840582171
Loss in iteration 168 : 0.3236387447163238
Loss in iteration 169 : 0.3236348437087443
Loss in iteration 170 : 0.3236309803319971
Loss in iteration 171 : 0.3236271538839942
Loss in iteration 172 : 0.3236233636901827
Loss in iteration 173 : 0.3236196091217451
Loss in iteration 174 : 0.3236158896035884
Loss in iteration 175 : 0.3236122046123053
Loss in iteration 176 : 0.32360855366631963
Loss in iteration 177 : 0.3236049363116404
Loss in iteration 178 : 0.3236013521067802
Loss in iteration 179 : 0.32359780061010684
Loss in iteration 180 : 0.32359428137158597
Loss in iteration 181 : 0.3235907939297623
Loss in iteration 182 : 0.323587337813532
Loss in iteration 183 : 0.32358391254740465
Loss in iteration 184 : 0.3235805176584976
Loss in iteration 185 : 0.3235771526834195
Loss in iteration 186 : 0.32357381717368006
Loss in iteration 187 : 0.32357051069867926
Loss in iteration 188 : 0.323567232846171
Loss in iteration 189 : 0.32356398322043034
Loss in iteration 190 : 0.32356076143899387
Loss in iteration 191 : 0.3235575671287993
Loss in iteration 192 : 0.32355439992259916
Loss in iteration 193 : 0.3235512594563204
Loss in iteration 194 : 0.32354814536766763
Loss in iteration 195 : 0.3235450572960028
Loss in iteration 196 : 0.3235419948832422
Loss in iteration 197 : 0.3235389577753716
Loss in iteration 198 : 0.32353594562412924
Loss in iteration 199 : 0.323532958088417
Loss in iteration 200 : 0.32352999483521694
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.8499477919046742, training accuracy 0.8492321867321867, time elapsed: 6605 millisecond.
