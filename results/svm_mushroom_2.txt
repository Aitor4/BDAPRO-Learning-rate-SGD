objc[2606]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1077be4c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x108fef4e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 08:51:14 INFO SparkContext: Running Spark version 2.0.0
18/02/27 08:51:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 08:51:15 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 08:51:15 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 08:51:15 INFO SecurityManager: Changing view acls groups to: 
18/02/27 08:51:15 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 08:51:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 08:51:16 INFO Utils: Successfully started service 'sparkDriver' on port 50206.
18/02/27 08:51:16 INFO SparkEnv: Registering MapOutputTracker
18/02/27 08:51:16 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 08:51:16 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-a3f0c3b8-8d31-4ab7-826c-75221560fb28
18/02/27 08:51:16 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 08:51:16 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 08:51:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 08:51:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 08:51:16 INFO Executor: Starting executor ID driver on host localhost
18/02/27 08:51:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50207.
18/02/27 08:51:17 INFO NettyBlockTransferService: Server created on 192.168.2.140:50207
18/02/27 08:51:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50207)
18/02/27 08:51:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50207 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50207)
18/02/27 08:51:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50207)
Loss in iteration 1 : 1.0
Loss in iteration 2 : 17.507658171391146
Loss in iteration 3 : 2.1470878341021127
Loss in iteration 4 : 1.6624074032175602
Loss in iteration 5 : 1.4006894633105065
Loss in iteration 6 : 1.1662349370060492
Loss in iteration 7 : 0.963366865689221
Loss in iteration 8 : 0.776954767871047
Loss in iteration 9 : 0.60833046570853
Loss in iteration 10 : 0.4622969891123916
Loss in iteration 11 : 0.3706725989812462
Loss in iteration 12 : 0.320461543701581
Loss in iteration 13 : 0.2906339303574343
Loss in iteration 14 : 0.27679532983783944
Loss in iteration 15 : 0.25514467408766844
Loss in iteration 16 : 0.23427548523438
Loss in iteration 17 : 0.20420409995882538
Loss in iteration 18 : 0.18463120275821257
Loss in iteration 19 : 0.1705833398200326
Loss in iteration 20 : 0.14754352303001514
Loss in iteration 21 : 0.13443901540153413
Loss in iteration 22 : 0.12109800530115111
Loss in iteration 23 : 0.11291896488450812
Loss in iteration 24 : 0.1029858202249594
Loss in iteration 25 : 0.10100824136173789
Loss in iteration 26 : 0.08895780536161019
Loss in iteration 27 : 0.08577741682778582
Loss in iteration 28 : 0.07792954521526205
Loss in iteration 29 : 0.07524923831991835
Loss in iteration 30 : 0.0678836014143916
Loss in iteration 31 : 0.06548467977744989
Loss in iteration 32 : 0.06270064014533683
Loss in iteration 33 : 0.06432895823499471
Loss in iteration 34 : 0.054803352544037114
Loss in iteration 35 : 0.05241631524819764
Loss in iteration 36 : 0.05011500039900339
Loss in iteration 37 : 0.04800367164880564
Loss in iteration 38 : 0.045971816877250854
Loss in iteration 39 : 0.04425479504575708
Loss in iteration 40 : 0.042547166336096745
Loss in iteration 41 : 0.04264158763035296
Loss in iteration 42 : 0.038910966355389424
Loss in iteration 43 : 0.03730396656832741
Loss in iteration 44 : 0.03579894341264849
Loss in iteration 45 : 0.03431458512500317
Loss in iteration 46 : 0.03309927851839586
Loss in iteration 47 : 0.03225426511032702
Loss in iteration 48 : 0.03076333163739831
Loss in iteration 49 : 0.03088009222575404
Loss in iteration 50 : 0.02836632946448334
Loss in iteration 51 : 0.027222818980411664
Loss in iteration 52 : 0.02627341940101116
Loss in iteration 53 : 0.025713548500077635
Loss in iteration 54 : 0.02497271706503977
Loss in iteration 55 : 0.02311659535109072
Loss in iteration 56 : 0.022138485490607988
Loss in iteration 57 : 0.02116037563012525
Loss in iteration 58 : 0.0201822657696425
Loss in iteration 59 : 0.01923482649392899
Loss in iteration 60 : 0.018336231451749372
Loss in iteration 61 : 0.017456218454935935
Loss in iteration 62 : 0.016626928316023092
Loss in iteration 63 : 0.015902351065723117
Loss in iteration 64 : 0.014836272577287138
Loss in iteration 65 : 0.013819487558646695
Loss in iteration 66 : 0.012916849390112685
Loss in iteration 67 : 0.012045045164988484
Loss in iteration 68 : 0.011117862360707078
Loss in iteration 69 : 0.01023176425453197
Loss in iteration 70 : 0.009363594759160668
Loss in iteration 71 : 0.008914726054502418
Loss in iteration 72 : 0.007683246942477502
Loss in iteration 73 : 0.006799190819309671
Loss in iteration 74 : 0.0062559008203952
Loss in iteration 75 : 0.0060764921933763915
Loss in iteration 76 : 0.00520656659261885
Loss in iteration 77 : 0.004798210830824834
Loss in iteration 78 : 0.004424527940494268
Loss in iteration 79 : 0.0040508450501637
Loss in iteration 80 : 0.0037160823559517324
Loss in iteration 81 : 0.0034873394194772423
Loss in iteration 82 : 0.003156619851619666
Loss in iteration 83 : 0.0030340600315232587
Loss in iteration 84 : 0.0028361101987984436
Loss in iteration 85 : 0.0026135340510059094
Loss in iteration 86 : 0.0024644692914751395
Loss in iteration 87 : 0.0023338232186699607
Loss in iteration 88 : 0.0022193904909425023
Loss in iteration 89 : 0.0020887852577974584
Loss in iteration 90 : 0.0019458464472884996
Loss in iteration 91 : 0.0018130767121557363
Loss in iteration 92 : 0.0016783466733360172
Loss in iteration 93 : 0.0015597891399338605
Loss in iteration 94 : 0.0015046964383977012
Loss in iteration 95 : 0.0012924118849618272
Loss in iteration 96 : 0.0011901902156178374
Loss in iteration 97 : 0.0010657517711547618
Loss in iteration 98 : 9.228129606458001E-4
Loss in iteration 99 : 7.901249048333461E-4
Loss in iteration 100 : 6.778158394334498E-4
Loss in iteration 101 : 6.328513736133519E-4
Loss in iteration 102 : 6.10348720873226E-4
Loss in iteration 103 : 5.512128929826659E-4
Testing accuracy  of updater 0 on alg 1 with rate 100.0 = 0.9911111111111112, training accuracy 0.9997142040583024, time elapsed: 6305 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 12.298271355201228
Loss in iteration 3 : 1.5310192737875297
Loss in iteration 4 : 1.1826057598133008
Loss in iteration 5 : 0.9997318059518253
Loss in iteration 6 : 0.827667415856619
Loss in iteration 7 : 0.6840493192804606
Loss in iteration 8 : 0.5535651757277484
Loss in iteration 9 : 0.434850899709965
Loss in iteration 10 : 0.3314855565232741
Loss in iteration 11 : 0.2644273246404211
Loss in iteration 12 : 0.2310603829894321
Loss in iteration 13 : 0.20883519490033536
Loss in iteration 14 : 0.19834479307810227
Loss in iteration 15 : 0.18397119301060258
Loss in iteration 16 : 0.1715754168442851
Loss in iteration 17 : 0.14603172049075228
Loss in iteration 18 : 0.13610997009638398
Loss in iteration 19 : 0.12006900922412735
Loss in iteration 20 : 0.10748528894182069
Loss in iteration 21 : 0.10294971796539101
Loss in iteration 22 : 0.08762622824256894
Loss in iteration 23 : 0.08041602708355572
Loss in iteration 24 : 0.07313394812268657
Loss in iteration 25 : 0.06995029241605058
Loss in iteration 26 : 0.06355296385257174
Loss in iteration 27 : 0.06269786304844703
Loss in iteration 28 : 0.05655096328097989
Loss in iteration 29 : 0.05424478851222836
Loss in iteration 30 : 0.04895254031262432
Loss in iteration 31 : 0.049067748993894565
Loss in iteration 32 : 0.044541652818618556
Loss in iteration 33 : 0.044931181497085554
Loss in iteration 34 : 0.03952964636693241
Loss in iteration 35 : 0.037751569243521244
Loss in iteration 36 : 0.0360664023469409
Loss in iteration 37 : 0.034523970062568805
Loss in iteration 38 : 0.033283587904632206
Loss in iteration 39 : 0.03197075618959764
Loss in iteration 40 : 0.03160830420580705
Loss in iteration 41 : 0.029121209742611287
Loss in iteration 42 : 0.027983416810959984
Loss in iteration 43 : 0.026928650908384286
Loss in iteration 44 : 0.02586212318368672
Loss in iteration 45 : 0.025059133785907534
Loss in iteration 46 : 0.023968183944443014
Loss in iteration 47 : 0.02321673419776736
Loss in iteration 48 : 0.022243116699900617
Loss in iteration 49 : 0.021510044800290375
Loss in iteration 50 : 0.020850729326902757
Loss in iteration 51 : 0.02082965606226772
Loss in iteration 52 : 0.019030097277620065
Loss in iteration 53 : 0.018275380358132764
Loss in iteration 54 : 0.017650533557907883
Loss in iteration 55 : 0.017293390729936218
Loss in iteration 56 : 0.016167114582445904
Loss in iteration 57 : 0.01548243768010799
Loss in iteration 58 : 0.014797760777770072
Loss in iteration 59 : 0.014124437300952579
Loss in iteration 60 : 0.013489829821952945
Loss in iteration 61 : 0.012870741413808569
Loss in iteration 62 : 0.012327818971835417
Loss in iteration 63 : 0.011799925524795725
Loss in iteration 64 : 0.011438739570469685
Loss in iteration 65 : 0.010343705762990613
Loss in iteration 66 : 0.009623416677005905
Loss in iteration 67 : 0.00900008094420639
Loss in iteration 68 : 0.008411336403550079
Loss in iteration 69 : 0.007723800725001987
Loss in iteration 70 : 0.007194600408837657
Loss in iteration 71 : 0.006736216063365523
Loss in iteration 72 : 0.0059569953477909615
Loss in iteration 73 : 0.005363880963499074
Loss in iteration 74 : 0.004804826855768437
Loss in iteration 75 : 0.004423425269670417
Loss in iteration 76 : 0.003934615377389902
Loss in iteration 77 : 0.003643061043611774
Loss in iteration 78 : 0.003381483020380377
Loss in iteration 79 : 0.003119945836809126
Loss in iteration 80 : 0.0028726208549684808
Loss in iteration 81 : 0.002611042831737082
Loss in iteration 82 : 0.0024010861389294957
Loss in iteration 83 : 0.0022709301420460377
Loss in iteration 84 : 0.002106673028941181
Loss in iteration 85 : 0.0019665929946423967
Loss in iteration 86 : 0.0018750713162565093
Loss in iteration 87 : 0.0017808133806408975
Loss in iteration 88 : 0.0016464508987624684
Loss in iteration 89 : 0.0015464345710663443
Loss in iteration 90 : 0.001463407541990704
Loss in iteration 91 : 0.0014077022455523563
Loss in iteration 92 : 0.0012776687676493515
Loss in iteration 93 : 0.0011890875447939447
Loss in iteration 94 : 0.0011003838029580897
Loss in iteration 95 : 0.0010175609721831903
Loss in iteration 96 : 9.175038048269179E-4
Loss in iteration 97 : 8.216939621257692E-4
Loss in iteration 98 : 7.3025396306018E-4
Loss in iteration 99 : 6.48738001409918E-4
Loss in iteration 100 : 6.129624591225392E-4
Loss in iteration 101 : 5.058400305611102E-4
Loss in iteration 102 : 4.458874094676516E-4
Testing accuracy  of updater 0 on alg 1 with rate 70.0 = 0.9911111111111112, training accuracy 0.9997142040583024, time elapsed: 3183 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 7.089270269601382
Loss in iteration 3 : 0.9099013387322272
Loss in iteration 4 : 0.6992553133010444
Loss in iteration 5 : 0.5925365439404929
Loss in iteration 6 : 0.49202952772771974
Loss in iteration 7 : 0.40921784618003376
Loss in iteration 8 : 0.3327009039695414
Loss in iteration 9 : 0.26291245912664707
Loss in iteration 10 : 0.20191422021447192
Loss in iteration 11 : 0.15994530262637405
Loss in iteration 12 : 0.1388718746326983
Loss in iteration 13 : 0.12246143735510599
Loss in iteration 14 : 0.11721623644398951
Loss in iteration 15 : 0.11072718200382406
Loss in iteration 16 : 0.10324425359435972
Loss in iteration 17 : 0.09180665753437411
Loss in iteration 18 : 0.08464885365932745
Loss in iteration 19 : 0.07296323834327875
Loss in iteration 20 : 0.06475144451919917
Loss in iteration 21 : 0.062051534586985674
Loss in iteration 22 : 0.053973368130901894
Loss in iteration 23 : 0.05090859667494921
Loss in iteration 24 : 0.04485554644649625
Loss in iteration 25 : 0.04166837852908778
Loss in iteration 26 : 0.03819668069944616
Loss in iteration 27 : 0.03665118544056316
Loss in iteration 28 : 0.033865920618645705
Loss in iteration 29 : 0.03384578666619405
Loss in iteration 30 : 0.030550311970079864
Loss in iteration 31 : 0.029927057916600684
Loss in iteration 32 : 0.02743926917918246
Loss in iteration 33 : 0.027317526152288975
Loss in iteration 34 : 0.024006123988718115
Loss in iteration 35 : 0.023402963048030447
Loss in iteration 36 : 0.021958586787667496
Loss in iteration 37 : 0.021322794958523645
Loss in iteration 38 : 0.020097319276540116
Loss in iteration 39 : 0.019346114567825326
Loss in iteration 40 : 0.018681571617939083
Loss in iteration 41 : 0.01838809782013413
Loss in iteration 42 : 0.01697422878589979
Loss in iteration 43 : 0.016364002584006956
Loss in iteration 44 : 0.015683042090742286
Loss in iteration 45 : 0.015110184177882515
Loss in iteration 46 : 0.014599239189803215
Loss in iteration 47 : 0.014016702277488984
Loss in iteration 48 : 0.013537530545002814
Loss in iteration 49 : 0.013003102752339887
Loss in iteration 50 : 0.012489421507030831
Loss in iteration 51 : 0.012007595196635204
Loss in iteration 52 : 0.0117108542260186
Loss in iteration 53 : 0.011634402382226383
Loss in iteration 54 : 0.010728578720201035
Loss in iteration 55 : 0.010302294347603175
Loss in iteration 56 : 0.009901208045315033
Loss in iteration 57 : 0.00951653928640535
Loss in iteration 58 : 0.009107366731408417
Loss in iteration 59 : 0.008730049111324906
Loss in iteration 60 : 0.00837213032981047
Loss in iteration 61 : 0.00806097295916254
Loss in iteration 62 : 0.007662337036483128
Loss in iteration 63 : 0.007285999568243101
Loss in iteration 64 : 0.006910233855245129
Loss in iteration 65 : 0.006553172706593743
Loss in iteration 66 : 0.0062021149879837355
Loss in iteration 67 : 0.005823735536736454
Loss in iteration 68 : 0.005514293431814627
Loss in iteration 69 : 0.005136036499547773
Loss in iteration 70 : 0.004776851688568829
Loss in iteration 71 : 0.00442942869971177
Loss in iteration 72 : 0.0041577224407643104
Loss in iteration 73 : 0.0037705216229256128
Loss in iteration 74 : 0.003441884877735445
Loss in iteration 75 : 0.003241402986081588
Loss in iteration 76 : 0.002826962114925908
Loss in iteration 77 : 0.0025804947659483087
Loss in iteration 78 : 0.0022546351176480272
Loss in iteration 79 : 0.002075839085531394
Loss in iteration 80 : 0.001926365929399165
Loss in iteration 81 : 0.0017810175789416277
Loss in iteration 82 : 0.0016355875491637988
Loss in iteration 83 : 0.0014919136247722225
Loss in iteration 84 : 0.0013849953945115183
Loss in iteration 85 : 0.0013195294192984229
Loss in iteration 86 : 0.0012289878927560281
Loss in iteration 87 : 0.0011276646959352517
Loss in iteration 88 : 0.0010624437586830174
Loss in iteration 89 : 9.962018299271556E-4
Loss in iteration 90 : 9.497262966816688E-4
Loss in iteration 91 : 8.786652880286451E-4
Loss in iteration 92 : 8.19039384216336E-4
Loss in iteration 93 : 7.676222520932531E-4
Loss in iteration 94 : 7.10446727889671E-4
Loss in iteration 95 : 6.662173759521897E-4
Loss in iteration 96 : 6.482887651483475E-4
Loss in iteration 97 : 5.584823524885769E-4
Loss in iteration 98 : 5.062484271625873E-4
Loss in iteration 99 : 4.5715915566779956E-4
Loss in iteration 100 : 4.082332428135878E-4
Loss in iteration 101 : 3.510577186100043E-4
Loss in iteration 102 : 2.979661604209613E-4
Testing accuracy  of updater 0 on alg 1 with rate 40.0 = 0.9911111111111112, training accuracy 1.0, time elapsed: 2749 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.882639477036716
Loss in iteration 3 : 0.278181427903175
Loss in iteration 4 : 0.22172063859179628
Loss in iteration 5 : 0.1934160995331128
Loss in iteration 6 : 0.15912757011128237
Loss in iteration 7 : 0.13717902988001066
Loss in iteration 8 : 0.11205181638064071
Loss in iteration 9 : 0.09268789572099193
Loss in iteration 10 : 0.07693256743179223
Loss in iteration 11 : 0.06837694450894839
Loss in iteration 12 : 0.08559559866081856
Loss in iteration 13 : 0.1399742595790034
Loss in iteration 14 : 0.8829616202759435
Loss in iteration 15 : 0.4459558567380659
Loss in iteration 16 : 1.2495978722863783
Loss in iteration 17 : 0.13553074119654152
Loss in iteration 18 : 0.06734116904834038
Loss in iteration 19 : 0.05704675675514524
Loss in iteration 20 : 0.04923649846961542
Loss in iteration 21 : 0.04330163821762328
Loss in iteration 22 : 0.03868344860871938
Loss in iteration 23 : 0.034856854132414114
Loss in iteration 24 : 0.031584290485641556
Loss in iteration 25 : 0.028826714953302714
Loss in iteration 26 : 0.026567219916097378
Loss in iteration 27 : 0.024420401501233244
Loss in iteration 28 : 0.02253136302120698
Loss in iteration 29 : 0.020776646183399
Loss in iteration 30 : 0.019228986422528273
Loss in iteration 31 : 0.01792252569447638
Loss in iteration 32 : 0.01689414221235465
Loss in iteration 33 : 0.01598015061830021
Loss in iteration 34 : 0.01523874742802031
Loss in iteration 35 : 0.01452278734601101
Loss in iteration 36 : 0.013849668067494244
Loss in iteration 37 : 0.013199623196959644
Loss in iteration 38 : 0.012587150813758828
Loss in iteration 39 : 0.012047413865276994
Loss in iteration 40 : 0.011634157344265513
Loss in iteration 41 : 0.011352649566883156
Loss in iteration 42 : 0.011088049408800997
Loss in iteration 43 : 0.0106897810430629
Loss in iteration 44 : 0.010367147727914097
Loss in iteration 45 : 0.009974964471537662
Loss in iteration 46 : 0.009695784554783588
Loss in iteration 47 : 0.009410723726968577
Loss in iteration 48 : 0.009149268222717617
Loss in iteration 49 : 0.008898267671463884
Loss in iteration 50 : 0.008652453757048623
Loss in iteration 51 : 0.008472840931729075
Loss in iteration 52 : 0.008335537994320194
Loss in iteration 53 : 0.008224209080763779
Loss in iteration 54 : 0.007886383412040895
Loss in iteration 55 : 0.007727353775434636
Loss in iteration 56 : 0.007514946703018327
Loss in iteration 57 : 0.007298823221528777
Loss in iteration 58 : 0.007142979078413871
Loss in iteration 59 : 0.006958424654216726
Loss in iteration 60 : 0.006744833231656201
Loss in iteration 61 : 0.006590990231888411
Loss in iteration 62 : 0.006487053296818329
Loss in iteration 63 : 0.0063422358619426725
Loss in iteration 64 : 0.006178509664419702
Loss in iteration 65 : 0.0060399406975463
Loss in iteration 66 : 0.005920157974339792
Loss in iteration 67 : 0.005799068382008632
Loss in iteration 68 : 0.005679244819141979
Loss in iteration 69 : 0.005559584614915907
Loss in iteration 70 : 0.005441884714376814
Loss in iteration 71 : 0.00532663519344645
Loss in iteration 72 : 0.005204728807912379
Loss in iteration 73 : 0.00508698806771314
Loss in iteration 74 : 0.004976394268039353
Loss in iteration 75 : 0.004851996663236413
Loss in iteration 76 : 0.00474311812928873
Loss in iteration 77 : 0.004638282721695448
Loss in iteration 78 : 0.004508167564472147
Loss in iteration 79 : 0.004394469950627308
Loss in iteration 80 : 0.004294984538513068
Loss in iteration 81 : 0.004221268951950595
Loss in iteration 82 : 0.004094094250257762
Loss in iteration 83 : 0.0039840722058260124
Loss in iteration 84 : 0.0038680058916927343
Loss in iteration 85 : 0.003772767804233626
Loss in iteration 86 : 0.003656415612479331
Loss in iteration 87 : 0.0035554599725998596
Loss in iteration 88 : 0.003457118070969697
Loss in iteration 89 : 0.0033532628152199003
Loss in iteration 90 : 0.003257207934557882
Loss in iteration 91 : 0.003146940852165256
Loss in iteration 92 : 0.0030470062037894186
Loss in iteration 93 : 0.002949358576381727
Loss in iteration 94 : 0.0028473411053384747
Loss in iteration 95 : 0.0027585965238424823
Loss in iteration 96 : 0.0026537202765890525
Loss in iteration 97 : 0.0025765741585743627
Loss in iteration 98 : 0.002496773462650214
Loss in iteration 99 : 0.00240129033723023
Loss in iteration 100 : 0.002280690820820814
Loss in iteration 101 : 0.0022058725634344074
Loss in iteration 102 : 0.002119129125285544
Loss in iteration 103 : 0.002011720819103097
Loss in iteration 104 : 0.001925140739594811
Loss in iteration 105 : 0.0018461568368735779
Loss in iteration 106 : 0.0017428324967056737
Loss in iteration 107 : 0.0016568241724394216
Loss in iteration 108 : 0.0015741647003051009
Loss in iteration 109 : 0.0014708403601371974
Loss in iteration 110 : 0.0013905904279514487
Loss in iteration 111 : 0.001315567972264323
Loss in iteration 112 : 0.0012450378791931816
Loss in iteration 113 : 0.0011620108501175564
Loss in iteration 114 : 0.0010824143524941273
Loss in iteration 115 : 0.0010721227581374922
Loss in iteration 116 : 9.44253782222188E-4
Loss in iteration 117 : 8.820141401605744E-4
Loss in iteration 118 : 8.248386159569903E-4
Loss in iteration 119 : 7.742791166969644E-4
Loss in iteration 120 : 7.316425115051437E-4
Loss in iteration 121 : 6.852486575799539E-4
Loss in iteration 122 : 6.403658710801404E-4
Loss in iteration 123 : 6.010372783601042E-4
Loss in iteration 124 : 5.98055983169485E-4
Loss in iteration 125 : 5.865391990084819E-4
Loss in iteration 126 : 4.942007274196925E-4
Loss in iteration 127 : 4.7243318856218564E-4
Loss in iteration 128 : 4.4645916470970086E-4
Loss in iteration 129 : 4.367393255950912E-4
Loss in iteration 130 : 4.0165805753017707E-4
Loss in iteration 131 : 3.80829830856014E-4
Loss in iteration 132 : 3.6171686990795984E-4
Loss in iteration 133 : 3.448909299280506E-4
Loss in iteration 134 : 3.296168970336633E-4
Loss in iteration 135 : 3.1658904544727364E-4
Loss in iteration 136 : 3.104630964254619E-4
Loss in iteration 137 : 3.028669196384128E-4
Loss in iteration 138 : 2.7044022948295146E-4
Loss in iteration 139 : 2.691742000184465E-4
Loss in iteration 140 : 2.8481578985413924E-4
Loss in iteration 141 : 2.3801353932749114E-4
Loss in iteration 142 : 2.2322958235485293E-4
Loss in iteration 143 : 2.094257772256988E-4
Loss in iteration 144 : 2.0166624179807142E-4
Loss in iteration 145 : 2.026463936415598E-4
Loss in iteration 146 : 1.8145061002608766E-4
Loss in iteration 147 : 1.6368535786283228E-4
Loss in iteration 148 : 1.4947315613222668E-4
Loss in iteration 149 : 1.4081514818139806E-4
Loss in iteration 150 : 1.273788999935601E-4
Loss in iteration 151 : 1.1937432660505451E-4
Loss in iteration 152 : 1.053254835150332E-4
Loss in iteration 153 : 9.695335318521983E-5
Testing accuracy  of updater 0 on alg 1 with rate 10.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 3687 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3637007789185018
Loss in iteration 3 : 0.22364906692403277
Loss in iteration 4 : 0.20580176788354423
Loss in iteration 5 : 0.2416655238368435
Loss in iteration 6 : 0.6448464718740119
Loss in iteration 7 : 0.22883227323104846
Loss in iteration 8 : 0.3404423270078962
Loss in iteration 9 : 0.24777915970112221
Loss in iteration 10 : 0.33426175494063926
Loss in iteration 11 : 0.18505132034212848
Loss in iteration 12 : 0.09376943202079295
Loss in iteration 13 : 0.07988178306936349
Loss in iteration 14 : 0.06747189680046584
Loss in iteration 15 : 0.0582790935000332
Loss in iteration 16 : 0.05018209996062241
Loss in iteration 17 : 0.042998403741043544
Loss in iteration 18 : 0.03743344871081854
Loss in iteration 19 : 0.03336818726062345
Loss in iteration 20 : 0.030268742933207308
Loss in iteration 21 : 0.027816995195867417
Loss in iteration 22 : 0.02573899160834831
Loss in iteration 23 : 0.024052987498734992
Loss in iteration 24 : 0.022515721431371274
Loss in iteration 25 : 0.02105135415736713
Loss in iteration 26 : 0.019705197279653898
Loss in iteration 27 : 0.01845342085653672
Loss in iteration 28 : 0.01725871785847277
Loss in iteration 29 : 0.016126131983489996
Loss in iteration 30 : 0.015027831003199295
Loss in iteration 31 : 0.013981090093842184
Loss in iteration 32 : 0.013081657838629591
Loss in iteration 33 : 0.012356692611558228
Loss in iteration 34 : 0.011841132741882475
Loss in iteration 35 : 0.011407681008929102
Loss in iteration 36 : 0.01092638561411535
Loss in iteration 37 : 0.010808542774765766
Loss in iteration 38 : 0.010049047615041436
Loss in iteration 39 : 0.009671566636317358
Loss in iteration 40 : 0.009196499289675784
Loss in iteration 41 : 0.008800313746605101
Loss in iteration 42 : 0.008478681003129868
Loss in iteration 43 : 0.008196744409315983
Loss in iteration 44 : 0.00795060377761956
Loss in iteration 45 : 0.007902841795079486
Loss in iteration 46 : 0.007904883778086775
Loss in iteration 47 : 0.007546617859461085
Loss in iteration 48 : 0.007306909474237573
Loss in iteration 49 : 0.006970901170391147
Loss in iteration 50 : 0.0067831408328725955
Loss in iteration 51 : 0.0065527643099923
Loss in iteration 52 : 0.006376418657484388
Loss in iteration 53 : 0.006200787699029023
Loss in iteration 54 : 0.006029342805738562
Loss in iteration 55 : 0.005867638171392785
Loss in iteration 56 : 0.005735072634560758
Loss in iteration 57 : 0.005580800818361449
Loss in iteration 58 : 0.005422261257676939
Loss in iteration 59 : 0.005280731415443001
Loss in iteration 60 : 0.005150371220258828
Loss in iteration 61 : 0.005022951480605125
Loss in iteration 62 : 0.004928468926858708
Loss in iteration 63 : 0.00487019073183119
Loss in iteration 64 : 0.004864105622469537
Loss in iteration 65 : 0.004729988178551973
Loss in iteration 66 : 0.004634239595341053
Loss in iteration 67 : 0.004461528672586077
Loss in iteration 68 : 0.004296801903389543
Loss in iteration 69 : 0.004207158849370352
Loss in iteration 70 : 0.004121844799326577
Loss in iteration 71 : 0.004040349257506395
Loss in iteration 72 : 0.0039521355915922984
Loss in iteration 73 : 0.003895960639062273
Loss in iteration 74 : 0.0038258593624226683
Loss in iteration 75 : 0.003723841891379414
Loss in iteration 76 : 0.003637853986943241
Loss in iteration 77 : 0.0035592580609933844
Loss in iteration 78 : 0.003474005270439828
Loss in iteration 79 : 0.0033935307201232836
Loss in iteration 80 : 0.0033110958661197606
Loss in iteration 81 : 0.003240565773048623
Loss in iteration 82 : 0.0031780606731960665
Loss in iteration 83 : 0.0031233967880914195
Loss in iteration 84 : 0.0030696926350002036
Loss in iteration 85 : 0.0029723513050435965
Loss in iteration 86 : 0.0029403738511497397
Loss in iteration 87 : 0.00282626784070344
Loss in iteration 88 : 0.0027939228298682747
Loss in iteration 89 : 0.0026646040460178073
Loss in iteration 90 : 0.0025797188124055638
Loss in iteration 91 : 0.002513660662120346
Loss in iteration 92 : 0.002444866254605399
Loss in iteration 93 : 0.002434227523137511
Loss in iteration 94 : 0.0023570814051228264
Loss in iteration 95 : 0.002335477224905891
Loss in iteration 96 : 0.0022459158502070014
Loss in iteration 97 : 0.0022009718042169603
Loss in iteration 98 : 0.0021143100453883956
Loss in iteration 99 : 0.0020493954055872485
Loss in iteration 100 : 0.0019652657056876935
Loss in iteration 101 : 0.0018878541298820526
Loss in iteration 102 : 0.0018247568549573851
Loss in iteration 103 : 0.001746303867818038
Loss in iteration 104 : 0.0016827369368016985
Loss in iteration 105 : 0.001622273819956406
Loss in iteration 106 : 0.0015769826368551407
Loss in iteration 107 : 0.0015091275415235288
Loss in iteration 108 : 0.001470595322176331
Loss in iteration 109 : 0.0014180346795691751
Loss in iteration 110 : 0.0013658211740732646
Loss in iteration 111 : 0.0013136893478976352
Loss in iteration 112 : 0.0012303968610310593
Loss in iteration 113 : 0.0011531282240359268
Loss in iteration 114 : 0.0010829656879061047
Loss in iteration 115 : 0.0010188882611379454
Loss in iteration 116 : 9.551783913110955E-4
Loss in iteration 117 : 9.011271011086352E-4
Loss in iteration 118 : 8.579595803349318E-4
Loss in iteration 119 : 8.08237294107882E-4
Loss in iteration 120 : 8.211017870536927E-4
Loss in iteration 121 : 7.374009035856534E-4
Loss in iteration 122 : 6.92558956745991E-4
Loss in iteration 123 : 6.580698637531809E-4
Loss in iteration 124 : 6.228660767078396E-4
Loss in iteration 125 : 5.711834867938119E-4
Loss in iteration 126 : 5.324879088060288E-4
Loss in iteration 127 : 4.979988158132231E-4
Loss in iteration 128 : 4.685330010183057E-4
Loss in iteration 129 : 4.507881686851182E-4
Loss in iteration 130 : 4.830923398601492E-4
Loss in iteration 131 : 4.3114429215516877E-4
Loss in iteration 132 : 4.4854198737712494E-4
Loss in iteration 133 : 4.0206645413163103E-4
Loss in iteration 134 : 4.120925906973403E-4
Loss in iteration 135 : 3.7870616852845375E-4
Loss in iteration 136 : 4.7621085712564755E-4
Loss in iteration 137 : 3.520378704534952E-4
Loss in iteration 138 : 3.8458707958939934E-4
Loss in iteration 139 : 2.967409706166049E-4
Loss in iteration 140 : 2.84529912233126E-4
Loss in iteration 141 : 2.8412151563167004E-4
Loss in iteration 142 : 2.8350892072949137E-4
Loss in iteration 143 : 2.588213461715851E-4
Loss in iteration 144 : 2.6163928272162035E-4
Loss in iteration 145 : 2.4252632177356262E-4
Loss in iteration 146 : 2.658457677165981E-4
Loss in iteration 147 : 2.245977109697227E-4
Loss in iteration 148 : 2.270072509183059E-4
Loss in iteration 149 : 2.1930897498089255E-4
Loss in iteration 150 : 3.2492033611694496E-4
Loss in iteration 151 : 1.8755613921783173E-4
Loss in iteration 152 : 1.771624457108237E-4
Loss in iteration 153 : 1.644000519153808E-4
Loss in iteration 154 : 1.5416971704895567E-4
Loss in iteration 155 : 1.537204807873554E-4
Loss in iteration 156 : 1.7671320944921996E-4
Loss in iteration 157 : 2.847953700240714E-4
Loss in iteration 158 : 1.2821611302654202E-4
Loss in iteration 159 : 1.40406751579944E-4
Loss in iteration 160 : 1.6721798846541566E-4
Loss in iteration 161 : 1.0640773450888648E-4
Loss in iteration 162 : 1.063464750186695E-4
Loss in iteration 163 : 1.0628521552844896E-4
Loss in iteration 164 : 1.391611419455134E-4
Loss in iteration 165 : 9.342072258264186E-5
Loss in iteration 166 : 1.2229436230545647E-4
Loss in iteration 167 : 8.055622963683543E-5
Loss in iteration 168 : 1.0550926198568912E-4
Loss in iteration 169 : 6.469002167034185E-5
Testing accuracy  of updater 0 on alg 1 with rate 7.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 4244 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8483775750733009
Loss in iteration 3 : 0.2160252212672367
Loss in iteration 4 : 0.6583437345141113
Loss in iteration 5 : 0.13049905002866533
Loss in iteration 6 : 0.11407170513177285
Loss in iteration 7 : 0.10849745907886472
Loss in iteration 8 : 0.10039017394509729
Loss in iteration 9 : 0.10557656574560435
Loss in iteration 10 : 0.21005732173018685
Loss in iteration 11 : 0.3043232619191978
Loss in iteration 12 : 0.5847737821797123
Loss in iteration 13 : 0.09282058377680438
Loss in iteration 14 : 0.06487584212400212
Loss in iteration 15 : 0.0578923419184561
Loss in iteration 16 : 0.05174511543370779
Loss in iteration 17 : 0.0462478521401738
Loss in iteration 18 : 0.04149701531511758
Loss in iteration 19 : 0.037419297768578136
Loss in iteration 20 : 0.03385971299030359
Loss in iteration 21 : 0.03073082326792263
Loss in iteration 22 : 0.028298168071700713
Loss in iteration 23 : 0.026294206788025257
Loss in iteration 24 : 0.024542307886767305
Loss in iteration 25 : 0.022977863025576813
Loss in iteration 26 : 0.021505674956654824
Loss in iteration 27 : 0.020212609637130632
Loss in iteration 28 : 0.019047740010802904
Loss in iteration 29 : 0.018000488605693973
Loss in iteration 30 : 0.01700935089362486
Loss in iteration 31 : 0.01606493375276209
Loss in iteration 32 : 0.01518451235934719
Loss in iteration 33 : 0.014338192082153716
Loss in iteration 34 : 0.013513312626536579
Loss in iteration 35 : 0.01271808276418502
Loss in iteration 36 : 0.012018295187593307
Loss in iteration 37 : 0.011527688350266409
Loss in iteration 38 : 0.011371762527831208
Loss in iteration 39 : 0.01138095145136392
Loss in iteration 40 : 0.011783589660737599
Loss in iteration 41 : 0.01201760091337083
Loss in iteration 42 : 0.012990115740413652
Loss in iteration 43 : 0.01589434649233468
Loss in iteration 44 : 0.018986970596506524
Loss in iteration 45 : 0.030673280186777757
Loss in iteration 46 : 0.04247234807871045
Loss in iteration 47 : 0.08758865575523517
Loss in iteration 48 : 0.17763099913166702
Loss in iteration 49 : 0.3683306895098678
Loss in iteration 50 : 0.10276769560012287
Loss in iteration 51 : 0.05094286114981123
Loss in iteration 52 : 0.019260718838461265
Loss in iteration 53 : 0.013775380725689733
Loss in iteration 54 : 0.011902351392440473
Loss in iteration 55 : 0.010625336059353422
Loss in iteration 56 : 0.00962554033933347
Loss in iteration 57 : 0.008818099418598433
Loss in iteration 58 : 0.008282242237830419
Loss in iteration 59 : 0.00789414294746852
Loss in iteration 60 : 0.007583026416480734
Loss in iteration 61 : 0.007310176647049203
Loss in iteration 62 : 0.007069590209132554
Loss in iteration 63 : 0.006849219402987884
Loss in iteration 64 : 0.006660254295495038
Loss in iteration 65 : 0.006491954056035775
Loss in iteration 66 : 0.006348729367905798
Loss in iteration 67 : 0.0062102012406925425
Loss in iteration 68 : 0.006078248298762703
Loss in iteration 69 : 0.0059554842803655784
Loss in iteration 70 : 0.0058448904806917895
Loss in iteration 71 : 0.005736869579607161
Loss in iteration 72 : 0.005628562800901516
Loss in iteration 73 : 0.005530261738931496
Loss in iteration 74 : 0.0054352278497731095
Loss in iteration 75 : 0.005337784420666148
Loss in iteration 76 : 0.005229518481620647
Loss in iteration 77 : 0.00513893611541811
Loss in iteration 78 : 0.005048925504457614
Loss in iteration 79 : 0.004960997716164529
Loss in iteration 80 : 0.004876010383401916
Loss in iteration 81 : 0.004792983354326281
Loss in iteration 82 : 0.004715388000049992
Loss in iteration 83 : 0.004641386535866494
Loss in iteration 84 : 0.0045687736201279455
Loss in iteration 85 : 0.004502082455110477
Loss in iteration 86 : 0.004444171817024279
Loss in iteration 87 : 0.00438344324238804
Loss in iteration 88 : 0.004320223448482938
Loss in iteration 89 : 0.0042571261735582675
Loss in iteration 90 : 0.004199746451053958
Loss in iteration 91 : 0.004144122833935899
Loss in iteration 92 : 0.004089806085942493
Loss in iteration 93 : 0.00404778207565286
Loss in iteration 94 : 0.003991423344652184
Loss in iteration 95 : 0.003938372626123288
Loss in iteration 96 : 0.003887853966523408
Loss in iteration 97 : 0.003840561640075015
Loss in iteration 98 : 0.003796740684738982
Loss in iteration 99 : 0.0037501834721732075
Loss in iteration 100 : 0.003703136183685687
Loss in iteration 101 : 0.0036576816419438374
Loss in iteration 102 : 0.0036096133619526816
Loss in iteration 103 : 0.003563382866668069
Loss in iteration 104 : 0.0035163355781805497
Loss in iteration 105 : 0.0034708810364387013
Loss in iteration 106 : 0.0034279177139657224
Loss in iteration 107 : 0.0033859753829963805
Loss in iteration 108 : 0.003334884968154462
Loss in iteration 109 : 0.0032981292740235876
Loss in iteration 110 : 0.0032545533566484285
Loss in iteration 111 : 0.0032074652285007626
Loss in iteration 112 : 0.0031735274709199213
Loss in iteration 113 : 0.003127787051557052
Loss in iteration 114 : 0.003091521433347924
Loss in iteration 115 : 0.003046026051945928
Loss in iteration 116 : 0.003003716164035277
Loss in iteration 117 : 0.002962999022870294
Loss in iteration 118 : 0.0029208933332603697
Loss in iteration 119 : 0.0028796452765134997
Loss in iteration 120 : 0.0028397857682115725
Loss in iteration 121 : 0.0027991094667067376
Loss in iteration 122 : 0.002758555684182338
Loss in iteration 123 : 0.0027207789985478296
Loss in iteration 124 : 0.002680143536703138
Loss in iteration 125 : 0.002638854640296122
Loss in iteration 126 : 0.002597647423209395
Loss in iteration 127 : 0.0025586863874306684
Loss in iteration 128 : 0.002522624967522264
Loss in iteration 129 : 0.0024849299612080484
Loss in iteration 130 : 0.0024460914444097534
Loss in iteration 131 : 0.002406558653388992
Loss in iteration 132 : 0.0023665766261066266
Loss in iteration 133 : 0.0023288816197924093
Loss in iteration 134 : 0.002293963710368076
Loss in iteration 135 : 0.0022574530541980757
Loss in iteration 136 : 0.002216245837111347
Loss in iteration 137 : 0.0021766722064304384
Loss in iteration 138 : 0.002138650482835055
Loss in iteration 139 : 0.0021029974595281084
Loss in iteration 140 : 0.0020657108498153397
Loss in iteration 141 : 0.0020315280542736306
Loss in iteration 142 : 0.0019930570944166432
Loss in iteration 143 : 0.0019574857504299882
Loss in iteration 144 : 0.0019162785333432604
Loss in iteration 145 : 0.0018785018477087505
Loss in iteration 146 : 0.001841582794937294
Loss in iteration 147 : 0.0018031118350803098
Loss in iteration 148 : 0.0017684389636168547
Loss in iteration 149 : 0.0017398103618549128
Loss in iteration 150 : 0.001698603144768192
Loss in iteration 151 : 0.0016584985985053883
Loss in iteration 152 : 0.001619006647144772
Loss in iteration 153 : 0.0015811074425298238
Loss in iteration 154 : 0.00154561777786346
Loss in iteration 155 : 0.00151866360216748
Loss in iteration 156 : 0.0014799884440097754
Loss in iteration 157 : 0.0014386587079426067
Loss in iteration 158 : 0.001407661405892243
Loss in iteration 159 : 0.0013851179134919652
Loss in iteration 160 : 0.0013374580301022726
Loss in iteration 161 : 0.0013034794328612772
Loss in iteration 162 : 0.0012683573251362264
Loss in iteration 163 : 0.0012466306259388564
Loss in iteration 164 : 0.001200645168615126
Loss in iteration 165 : 0.0011894959413954165
Loss in iteration 166 : 0.0011266437044316305
Loss in iteration 167 : 0.0011043860896523701
Loss in iteration 168 : 0.0010517437677249357
Loss in iteration 169 : 0.0010195212758701943
Loss in iteration 170 : 9.79090012326237E-4
Loss in iteration 171 : 9.457240099874264E-4
Loss in iteration 172 : 9.070896914898666E-4
Loss in iteration 173 : 8.718450647843672E-4
Loss in iteration 174 : 8.512210364109412E-4
Loss in iteration 175 : 8.435840199637329E-4
Loss in iteration 176 : 8.036428323415314E-4
Loss in iteration 177 : 7.767294963056867E-4
Loss in iteration 178 : 7.324593047080691E-4
Loss in iteration 179 : 7.1183527633462E-4
Loss in iteration 180 : 6.726292025950314E-4
Loss in iteration 181 : 6.619700512970643E-4
Loss in iteration 182 : 6.379154914714284E-4
Loss in iteration 183 : 6.195376444059771E-4
Loss in iteration 184 : 6.007922403992427E-4
Loss in iteration 185 : 5.646491411705375E-4
Loss in iteration 186 : 5.356938221274451E-4
Loss in iteration 187 : 5.125785744851336E-4
Loss in iteration 188 : 4.86808748933381E-4
Loss in iteration 189 : 4.62427471826563E-4
Loss in iteration 190 : 4.407416122893475E-4
Loss in iteration 191 : 4.195049890137313E-4
Loss in iteration 192 : 3.993710365620412E-4
Loss in iteration 193 : 3.8009471697340066E-4
Loss in iteration 194 : 3.667401481058553E-4
Loss in iteration 195 : 3.5481496734338433E-4
Loss in iteration 196 : 3.5142527555132426E-4
Loss in iteration 197 : 3.6367717359493576E-4
Loss in iteration 198 : 3.7899204614948645E-4
Loss in iteration 199 : 3.415829174562663E-4
Loss in iteration 200 : 3.3745811178159283E-4
Testing accuracy  of updater 0 on alg 1 with rate 4.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 4792 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38032923872178076
Loss in iteration 3 : 0.7167107353826075
Loss in iteration 4 : 1.2735218064590537
Loss in iteration 5 : 0.1622033682754743
Loss in iteration 6 : 0.15664131286111962
Loss in iteration 7 : 0.2192297460402069
Loss in iteration 8 : 0.11009134933502009
Loss in iteration 9 : 0.10928799238029954
Loss in iteration 10 : 0.13191867745498023
Loss in iteration 11 : 0.101416862581323
Loss in iteration 12 : 0.12214970822921606
Loss in iteration 13 : 0.09000354569929384
Loss in iteration 14 : 0.09156151747435129
Loss in iteration 15 : 0.08039822099173233
Loss in iteration 16 : 0.07939405542807684
Loss in iteration 17 : 0.0722108492844197
Loss in iteration 18 : 0.06890871772370191
Loss in iteration 19 : 0.06522152942730301
Loss in iteration 20 : 0.06276596318173952
Loss in iteration 21 : 0.060688694708103015
Loss in iteration 22 : 0.06012931388309124
Loss in iteration 23 : 0.05847473589195984
Loss in iteration 24 : 0.059086514000938166
Loss in iteration 25 : 0.05744633199000799
Loss in iteration 26 : 0.057871411592631594
Loss in iteration 27 : 0.05494986482480886
Loss in iteration 28 : 0.05350228265112452
Loss in iteration 29 : 0.049839945287924084
Loss in iteration 30 : 0.04726379994578128
Loss in iteration 31 : 0.04477791025255981
Loss in iteration 32 : 0.042754590969975415
Loss in iteration 33 : 0.04130331280704788
Loss in iteration 34 : 0.03994788532648162
Loss in iteration 35 : 0.038910190401846734
Loss in iteration 36 : 0.03791760288184244
Loss in iteration 37 : 0.03697763726393553
Loss in iteration 38 : 0.03607781703195155
Loss in iteration 39 : 0.03515408517895245
Loss in iteration 40 : 0.03426870176682988
Loss in iteration 41 : 0.033384278086720724
Loss in iteration 42 : 0.03251320897547913
Loss in iteration 43 : 0.03167691483485136
Loss in iteration 44 : 0.030890996415012962
Loss in iteration 45 : 0.030130990759536812
Loss in iteration 46 : 0.029475207916751782
Loss in iteration 47 : 0.029029504285754775
Loss in iteration 48 : 0.028621720279202793
Loss in iteration 49 : 0.028266721533388755
Loss in iteration 50 : 0.027730517215509513
Loss in iteration 51 : 0.02794198497574246
Loss in iteration 52 : 0.027681754661295882
Loss in iteration 53 : 0.02844272004878542
Loss in iteration 54 : 0.029640424761870104
Loss in iteration 55 : 0.029217366722423695
Loss in iteration 56 : 0.030797759470900982
Loss in iteration 57 : 0.02796350747663909
Loss in iteration 58 : 0.027050291836127456
Loss in iteration 59 : 0.024382236838827346
Loss in iteration 60 : 0.022553396017855436
Loss in iteration 61 : 0.021318077977776857
Loss in iteration 62 : 0.02068726858717076
Loss in iteration 63 : 0.02012065914231324
Loss in iteration 64 : 0.01959778897347147
Loss in iteration 65 : 0.01908604761201932
Loss in iteration 66 : 0.018586925705552107
Loss in iteration 67 : 0.01810638584445106
Loss in iteration 68 : 0.01764181429046687
Loss in iteration 69 : 0.017200929739367023
Loss in iteration 70 : 0.016789143446120786
Loss in iteration 71 : 0.016380726424836543
Loss in iteration 72 : 0.016003776361694345
Loss in iteration 73 : 0.015639629532007735
Loss in iteration 74 : 0.015289919522182527
Loss in iteration 75 : 0.014985582374778882
Loss in iteration 76 : 0.014652126549691549
Loss in iteration 77 : 0.014353445695218046
Loss in iteration 78 : 0.014090867100313084
Loss in iteration 79 : 0.013869965378586526
Loss in iteration 80 : 0.013687555036547018
Loss in iteration 81 : 0.013585966381935296
Loss in iteration 82 : 0.013500631912061451
Loss in iteration 83 : 0.013766763557399066
Loss in iteration 84 : 0.013392815209277546
Loss in iteration 85 : 0.014236399229241296
Loss in iteration 86 : 0.013477884221360445
Loss in iteration 87 : 0.014936186805833014
Loss in iteration 88 : 0.013113022697621289
Loss in iteration 89 : 0.013668972991180873
Loss in iteration 90 : 0.011913112642888794
Loss in iteration 91 : 0.011796311214872911
Loss in iteration 92 : 0.01122594452128201
Loss in iteration 93 : 0.011161295339271826
Loss in iteration 94 : 0.010897430295072274
Loss in iteration 95 : 0.01091062150529926
Loss in iteration 96 : 0.010557113407080512
Loss in iteration 97 : 0.010632707618009702
Loss in iteration 98 : 0.010309237089827903
Loss in iteration 99 : 0.010308093579343851
Loss in iteration 100 : 0.009956852082263174
Loss in iteration 101 : 0.009903495066283203
Loss in iteration 102 : 0.009686452692440369
Loss in iteration 103 : 0.009576287709198128
Loss in iteration 104 : 0.009387465540515772
Loss in iteration 105 : 0.00928626486267544
Loss in iteration 106 : 0.00906814023783876
Loss in iteration 107 : 0.008894877979671837
Loss in iteration 108 : 0.008692558303311435
Loss in iteration 109 : 0.008529342601540285
Loss in iteration 110 : 0.008377337386479031
Loss in iteration 111 : 0.00824863119753076
Loss in iteration 112 : 0.008117678827274473
Loss in iteration 113 : 0.008079105768267134
Loss in iteration 114 : 0.00799185183436644
Loss in iteration 115 : 0.007971513683614035
Loss in iteration 116 : 0.007881258034692652
Loss in iteration 117 : 0.007885342000707207
Loss in iteration 118 : 0.008039082901324605
Loss in iteration 119 : 0.00847226917648707
Loss in iteration 120 : 0.00798748199073087
Loss in iteration 121 : 0.008053111324584581
Loss in iteration 122 : 0.007536101646973658
Loss in iteration 123 : 0.007123886537295906
Loss in iteration 124 : 0.006919320679627503
Loss in iteration 125 : 0.006796720019870962
Loss in iteration 126 : 0.00668377793973881
Loss in iteration 127 : 0.006586334510631845
Loss in iteration 128 : 0.006491096423172733
Loss in iteration 129 : 0.006405292297207211
Loss in iteration 130 : 0.006326328814316048
Loss in iteration 131 : 0.006259474290658
Loss in iteration 132 : 0.00620160449223194
Loss in iteration 133 : 0.006158784108569475
Loss in iteration 134 : 0.006109061822342425
Loss in iteration 135 : 0.0060773906658996586
Loss in iteration 136 : 0.006077472345219946
Loss in iteration 137 : 0.006158089834347004
Loss in iteration 138 : 0.006027443761541807
Loss in iteration 139 : 0.00620885353190776
Loss in iteration 140 : 0.005976884262281781
Loss in iteration 141 : 0.006139262751019972
Loss in iteration 142 : 0.005774789204052188
Loss in iteration 143 : 0.005758187882203081
Loss in iteration 144 : 0.005585701577578911
Loss in iteration 145 : 0.005538429670960594
Loss in iteration 146 : 0.005494016540552448
Loss in iteration 147 : 0.005476557585840286
Loss in iteration 148 : 0.0054315727001901055
Loss in iteration 149 : 0.005456954548970486
Loss in iteration 150 : 0.0053321689673961606
Loss in iteration 151 : 0.005323715157746062
Loss in iteration 152 : 0.005269561768393234
Loss in iteration 153 : 0.005271277034119347
Loss in iteration 154 : 0.005192864886640143
Loss in iteration 155 : 0.005199113354642396
Loss in iteration 156 : 0.005108367629799275
Loss in iteration 157 : 0.005114575258141382
Loss in iteration 158 : 0.0050476390551630395
Loss in iteration 159 : 0.005054540957727623
Loss in iteration 160 : 0.004974005147920853
Loss in iteration 161 : 0.0049596500073797465
Loss in iteration 162 : 0.0049099277211526945
Loss in iteration 163 : 0.004910356537584229
Loss in iteration 164 : 0.00484239934310225
Loss in iteration 165 : 0.0048305558416600865
Loss in iteration 166 : 0.004776810848908712
Loss in iteration 167 : 0.004757820406941097
Loss in iteration 168 : 0.004704177513340089
Loss in iteration 169 : 0.004676059407329971
Loss in iteration 170 : 0.004642407527370146
Loss in iteration 171 : 0.004612553735803848
Loss in iteration 172 : 0.004581801471714351
Loss in iteration 173 : 0.004556378783273829
Loss in iteration 174 : 0.004524789306151348
Loss in iteration 175 : 0.004500428448874608
Loss in iteration 176 : 0.004471268931530778
Loss in iteration 177 : 0.004455974478806321
Loss in iteration 178 : 0.0044142363461377055
Loss in iteration 179 : 0.0043937552565747814
Loss in iteration 180 : 0.0043575304780257935
Loss in iteration 181 : 0.004349648423617732
Loss in iteration 182 : 0.004332883743128031
Loss in iteration 183 : 0.004384974729643519
Loss in iteration 184 : 0.0042714813140994
Loss in iteration 185 : 0.004290900572498551
Loss in iteration 186 : 0.004203891676558734
Loss in iteration 187 : 0.004188944360945514
Loss in iteration 188 : 0.004146634473034862
Loss in iteration 189 : 0.004128950900191898
Loss in iteration 190 : 0.004098464093893342
Loss in iteration 191 : 0.004110123816864861
Loss in iteration 192 : 0.004060891606559557
Loss in iteration 193 : 0.004102445960757524
Loss in iteration 194 : 0.004010270847809315
Loss in iteration 195 : 0.004018377520348183
Loss in iteration 196 : 0.003962406766118886
Loss in iteration 197 : 0.003971228132710298
Loss in iteration 198 : 0.003911683908218279
Loss in iteration 199 : 0.0039025562441757783
Loss in iteration 200 : 0.003858592350029238
Testing accuracy  of updater 0 on alg 1 with rate 1.0 = 0.9937777777777778, training accuracy 0.9987139182623607, time elapsed: 4782 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.8826394770367163
Loss in iteration 3 : 1.1824377862911224
Loss in iteration 4 : 0.5964445767830943
Loss in iteration 5 : 1.0803368524596215
Loss in iteration 6 : 1.0575533146652523
Loss in iteration 7 : 0.885816542588292
Loss in iteration 8 : 0.7708676135910224
Loss in iteration 9 : 0.7260956320408695
Loss in iteration 10 : 0.7773871189506336
Loss in iteration 11 : 0.8171065438033126
Loss in iteration 12 : 0.7544954756130123
Loss in iteration 13 : 0.626221351985912
Loss in iteration 14 : 0.5086298190882191
Loss in iteration 15 : 0.4236272190385008
Loss in iteration 16 : 0.361504903304012
Loss in iteration 17 : 0.328613060716517
Loss in iteration 18 : 0.3209732516239444
Loss in iteration 19 : 0.2977726364772756
Loss in iteration 20 : 0.25115079583094985
Loss in iteration 21 : 0.21366665370009866
Loss in iteration 22 : 0.19155267297242584
Loss in iteration 23 : 0.18111841055973152
Loss in iteration 24 : 0.17960860616631413
Loss in iteration 25 : 0.18067582298170654
Loss in iteration 26 : 0.17615378648388708
Loss in iteration 27 : 0.16511282468673333
Loss in iteration 28 : 0.15049897747860738
Loss in iteration 29 : 0.13662904467423564
Loss in iteration 30 : 0.12404493813657688
Loss in iteration 31 : 0.11609987110252934
Loss in iteration 32 : 0.11484833305353326
Loss in iteration 33 : 0.1127067372692075
Loss in iteration 34 : 0.10363397833372495
Loss in iteration 35 : 0.09422341938386784
Loss in iteration 36 : 0.08604099932307528
Loss in iteration 37 : 0.07783909915039314
Loss in iteration 38 : 0.07430911274700044
Loss in iteration 39 : 0.07263105114872839
Loss in iteration 40 : 0.07076457599483203
Loss in iteration 41 : 0.06860029062395176
Loss in iteration 42 : 0.06606282533733861
Loss in iteration 43 : 0.06344706380568296
Loss in iteration 44 : 0.060989312186342466
Loss in iteration 45 : 0.05867243900579189
Loss in iteration 46 : 0.05670426964757239
Loss in iteration 47 : 0.054971688960231586
Loss in iteration 48 : 0.05343736091053446
Loss in iteration 49 : 0.05217087678902586
Loss in iteration 50 : 0.050887128768418534
Loss in iteration 51 : 0.04947247731232566
Loss in iteration 52 : 0.047882760140798175
Loss in iteration 53 : 0.046121253183280926
Loss in iteration 54 : 0.044493736034812444
Loss in iteration 55 : 0.04287916576100503
Loss in iteration 56 : 0.041387903345015874
Loss in iteration 57 : 0.04004064263629802
Loss in iteration 58 : 0.03873410544116984
Loss in iteration 59 : 0.03744027485461391
Loss in iteration 60 : 0.036098174805081026
Loss in iteration 61 : 0.034712632238868835
Loss in iteration 62 : 0.0333008453127964
Loss in iteration 63 : 0.031856274798872465
Loss in iteration 64 : 0.030430541044908804
Loss in iteration 65 : 0.029090027028424108
Loss in iteration 66 : 0.027812745764906884
Loss in iteration 67 : 0.02657417726209432
Loss in iteration 68 : 0.02542556017100291
Loss in iteration 69 : 0.02427821704906467
Loss in iteration 70 : 0.023169541635504214
Loss in iteration 71 : 0.02205861847124165
Loss in iteration 72 : 0.02095932018399585
Loss in iteration 73 : 0.019912068958810272
Loss in iteration 74 : 0.01890003908993163
Loss in iteration 75 : 0.017906800606343556
Loss in iteration 76 : 0.016943267490579717
Loss in iteration 77 : 0.01600072537341061
Loss in iteration 78 : 0.015054013887007962
Loss in iteration 79 : 0.014103549968295112
Loss in iteration 80 : 0.013149708860503095
Loss in iteration 81 : 0.012192828282539831
Loss in iteration 82 : 0.011241000132098113
Loss in iteration 83 : 0.010298975575714606
Loss in iteration 84 : 0.00935130050591391
Loss in iteration 85 : 0.008412970547116268
Loss in iteration 86 : 0.007525255134683362
Loss in iteration 87 : 0.0066859707855328595
Loss in iteration 88 : 0.006048154324092071
Loss in iteration 89 : 0.005487596831382454
Loss in iteration 90 : 0.004987112403082613
Loss in iteration 91 : 0.004540543084371612
Loss in iteration 92 : 0.004119853642551963
Loss in iteration 93 : 0.0037561363316816294
Loss in iteration 94 : 0.0033949921901029836
Loss in iteration 95 : 0.003088225618508238
Loss in iteration 96 : 0.0028242873694960522
Loss in iteration 97 : 0.002607998462151948
Loss in iteration 98 : 0.0023986471032139403
Loss in iteration 99 : 0.002250042697453493
Loss in iteration 100 : 0.0021020048512181942
Loss in iteration 101 : 0.0019544769085555343
Loss in iteration 102 : 0.0018074078791082441
Loss in iteration 103 : 0.0016607518715547828
Loss in iteration 104 : 0.0015144675837057723
Loss in iteration 105 : 0.0013685178435907679
Loss in iteration 106 : 0.001222869196436368
Loss in iteration 107 : 0.0010831027381937993
Loss in iteration 108 : 9.776014329718754E-4
Loss in iteration 109 : 8.702276336764591E-4
Loss in iteration 110 : 7.933598156437043E-4
Loss in iteration 111 : 7.154409702863785E-4
Testing accuracy  of updater 1 on alg 1 with rate 10.0 = 0.9866666666666667, training accuracy 0.9997142040583024, time elapsed: 2557 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.3637007789185018
Loss in iteration 3 : 0.8333449270321874
Loss in iteration 4 : 0.4478587449678392
Loss in iteration 5 : 0.8616356159531474
Loss in iteration 6 : 0.7142367689792722
Loss in iteration 7 : 0.6138207749521544
Loss in iteration 8 : 0.5511195437184886
Loss in iteration 9 : 0.5453572984245244
Loss in iteration 10 : 0.5933797660354796
Loss in iteration 11 : 0.5973166085300776
Loss in iteration 12 : 0.5291337713071252
Loss in iteration 13 : 0.4373406094454642
Loss in iteration 14 : 0.36829199601811297
Loss in iteration 15 : 0.31494351087226297
Loss in iteration 16 : 0.27062906530108527
Loss in iteration 17 : 0.24128597404361316
Loss in iteration 18 : 0.22412618466069484
Loss in iteration 19 : 0.20344988606326087
Loss in iteration 20 : 0.17465997608669462
Loss in iteration 21 : 0.15207904507173486
Loss in iteration 22 : 0.13788581992797094
Loss in iteration 23 : 0.130991849092193
Loss in iteration 24 : 0.12875638536988798
Loss in iteration 25 : 0.1279620105328687
Loss in iteration 26 : 0.12471520077622197
Loss in iteration 27 : 0.1185027590615286
Loss in iteration 28 : 0.11040061251505469
Loss in iteration 29 : 0.10161352817738997
Loss in iteration 30 : 0.09259904970042422
Loss in iteration 31 : 0.08536627728401555
Loss in iteration 32 : 0.08191098292082774
Loss in iteration 33 : 0.07977317988955239
Loss in iteration 34 : 0.07449042325311006
Loss in iteration 35 : 0.06911582908062744
Loss in iteration 36 : 0.06385869347678365
Loss in iteration 37 : 0.058346894977606895
Loss in iteration 38 : 0.054147761996326364
Loss in iteration 39 : 0.05260689876705898
Loss in iteration 40 : 0.05106659266813838
Loss in iteration 41 : 0.049463046036720026
Loss in iteration 42 : 0.04775538624563537
Loss in iteration 43 : 0.04598511322713002
Loss in iteration 44 : 0.04434659287058521
Loss in iteration 45 : 0.04284785450265087
Loss in iteration 46 : 0.041493366045429206
Loss in iteration 47 : 0.040233363234248865
Loss in iteration 48 : 0.039049400301259744
Loss in iteration 49 : 0.03794058922982923
Loss in iteration 50 : 0.036918071349883085
Loss in iteration 51 : 0.035924316865037505
Loss in iteration 52 : 0.03487867905693008
Loss in iteration 53 : 0.033790447756025584
Loss in iteration 54 : 0.032713493783160026
Loss in iteration 55 : 0.03162931240393184
Loss in iteration 56 : 0.03053862635897725
Loss in iteration 57 : 0.029479345882508626
Loss in iteration 58 : 0.02851883317292414
Loss in iteration 59 : 0.027556249709482915
Loss in iteration 60 : 0.026637456801980864
Loss in iteration 61 : 0.02569177844857626
Loss in iteration 62 : 0.024716311165369338
Loss in iteration 63 : 0.023735601861228233
Loss in iteration 64 : 0.022771102807510588
Loss in iteration 65 : 0.02184169722759227
Loss in iteration 66 : 0.020932669616691057
Loss in iteration 67 : 0.02005216903476822
Loss in iteration 68 : 0.019215114095798853
Loss in iteration 69 : 0.018362850332282236
Loss in iteration 70 : 0.017494183450845396
Loss in iteration 71 : 0.016667948861260776
Loss in iteration 72 : 0.01585307007385046
Loss in iteration 73 : 0.015086573748584936
Loss in iteration 74 : 0.01431677946630367
Loss in iteration 75 : 0.013614154689642695
Loss in iteration 76 : 0.012920475831988019
Loss in iteration 77 : 0.012225553087707388
Loss in iteration 78 : 0.011540640531282655
Loss in iteration 79 : 0.010877466895359454
Loss in iteration 80 : 0.010218463125962121
Loss in iteration 81 : 0.009556463226839215
Loss in iteration 82 : 0.008901968702021685
Loss in iteration 83 : 0.008236935743407114
Loss in iteration 84 : 0.007570358830785742
Loss in iteration 85 : 0.0069164512575063735
Loss in iteration 86 : 0.0062598955677526805
Loss in iteration 87 : 0.005620560956746968
Loss in iteration 88 : 0.005024209787134513
Loss in iteration 89 : 0.004519573251642494
Loss in iteration 90 : 0.0041211608048949125
Loss in iteration 91 : 0.003742595220127609
Loss in iteration 92 : 0.003357276318147471
Loss in iteration 93 : 0.0030053408586489644
Loss in iteration 94 : 0.0027223076518242393
Loss in iteration 95 : 0.0024564805453526216
Loss in iteration 96 : 0.002232988178504578
Loss in iteration 97 : 0.0020595865374885476
Loss in iteration 98 : 0.0019034931087772468
Loss in iteration 99 : 0.001788829031767051
Loss in iteration 100 : 0.0016828285860858586
Loss in iteration 101 : 0.0015774224682371582
Loss in iteration 102 : 0.0014725512454376959
Loss in iteration 103 : 0.0013681614281825594
Loss in iteration 104 : 0.001264204875917307
Loss in iteration 105 : 0.0011606382621429509
Loss in iteration 106 : 0.0010574225930104068
Loss in iteration 107 : 9.545227740554819E-4
Loss in iteration 108 : 8.569637731439704E-4
Loss in iteration 109 : 7.905010537492581E-4
Loss in iteration 110 : 7.188674306567378E-4
Loss in iteration 111 : 6.392456559595224E-4
Loss in iteration 112 : 5.52434544818077E-4
Loss in iteration 113 : 4.92640008562533E-4
Testing accuracy  of updater 1 on alg 1 with rate 7.0 = 0.9884444444444445, training accuracy 0.9997142040583024, time elapsed: 2718 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8483775750733009
Loss in iteration 3 : 0.4890232631688914
Loss in iteration 4 : 0.2998168145212109
Loss in iteration 5 : 0.6580633991616922
Loss in iteration 6 : 0.3986043833207235
Loss in iteration 7 : 0.34366930051545364
Loss in iteration 8 : 0.3645827737423202
Loss in iteration 9 : 0.42200510533134006
Loss in iteration 10 : 0.41888611276433535
Loss in iteration 11 : 0.3560876031988426
Loss in iteration 12 : 0.3001536789830035
Loss in iteration 13 : 0.2724735606461237
Loss in iteration 14 : 0.25003162004206775
Loss in iteration 15 : 0.2247423224470302
Loss in iteration 16 : 0.19558091791890733
Loss in iteration 17 : 0.16701407008516847
Loss in iteration 18 : 0.14125799374753495
Loss in iteration 19 : 0.11957137025927794
Loss in iteration 20 : 0.10432701442005482
Loss in iteration 21 : 0.0939594711495371
Loss in iteration 22 : 0.08695087360742995
Loss in iteration 23 : 0.0815689009130572
Loss in iteration 24 : 0.07804108493618421
Loss in iteration 25 : 0.07591139223034538
Loss in iteration 26 : 0.07406179571070616
Loss in iteration 27 : 0.07194005312311542
Loss in iteration 28 : 0.06935326116933214
Loss in iteration 29 : 0.06620780990601893
Loss in iteration 30 : 0.0626332135895525
Loss in iteration 31 : 0.05887881824585526
Loss in iteration 32 : 0.0547749351574364
Loss in iteration 33 : 0.050941340882884205
Loss in iteration 34 : 0.047412989149040304
Loss in iteration 35 : 0.0441059738415656
Loss in iteration 36 : 0.04088127246649385
Loss in iteration 37 : 0.037836610541691726
Loss in iteration 38 : 0.03486808769423151
Loss in iteration 39 : 0.032397377649885054
Loss in iteration 40 : 0.03103847931339242
Loss in iteration 41 : 0.02981294393281318
Loss in iteration 42 : 0.02866067134589266
Loss in iteration 43 : 0.02767923873681654
Loss in iteration 44 : 0.026820230005753915
Loss in iteration 45 : 0.025994873525375974
Loss in iteration 46 : 0.02527421409252228
Loss in iteration 47 : 0.024595444823378505
Loss in iteration 48 : 0.02391896398695557
Loss in iteration 49 : 0.023280153265903582
Loss in iteration 50 : 0.022659149970105347
Loss in iteration 51 : 0.022035148585615137
Loss in iteration 52 : 0.021415089551089096
Loss in iteration 53 : 0.02081183658174257
Loss in iteration 54 : 0.020229347564996013
Loss in iteration 55 : 0.019655175248358987
Loss in iteration 56 : 0.01908091792190092
Loss in iteration 57 : 0.018506584086603915
Loss in iteration 58 : 0.01794144240036516
Loss in iteration 59 : 0.017391072000213312
Loss in iteration 60 : 0.016861700047918537
Loss in iteration 61 : 0.01634192707274385
Loss in iteration 62 : 0.01582234670602224
Loss in iteration 63 : 0.015308286099317014
Loss in iteration 64 : 0.014809653743865774
Loss in iteration 65 : 0.014315137688563625
Loss in iteration 66 : 0.013819432060211375
Loss in iteration 67 : 0.013345296218438248
Loss in iteration 68 : 0.01287093560254321
Loss in iteration 69 : 0.012392471397134154
Loss in iteration 70 : 0.011910313961162496
Loss in iteration 71 : 0.01142525104259819
Loss in iteration 72 : 0.010935542964247846
Loss in iteration 73 : 0.01045407293059008
Loss in iteration 74 : 0.009984877219626596
Loss in iteration 75 : 0.009518632295304117
Loss in iteration 76 : 0.009051633846884153
Loss in iteration 77 : 0.008583805226404485
Loss in iteration 78 : 0.008152146460365163
Loss in iteration 79 : 0.0077226297326739935
Loss in iteration 80 : 0.00732153238114232
Loss in iteration 81 : 0.006937251368146746
Loss in iteration 82 : 0.00654794505805601
Loss in iteration 83 : 0.006172014597814572
Loss in iteration 84 : 0.005811947806240692
Loss in iteration 85 : 0.005449008337365753
Loss in iteration 86 : 0.005083483458919883
Loss in iteration 87 : 0.004715631711860159
Loss in iteration 88 : 0.004345685783047965
Loss in iteration 89 : 0.003973855090658559
Loss in iteration 90 : 0.00360169184767562
Loss in iteration 91 : 0.0032287112615204377
Loss in iteration 92 : 0.002857300099820611
Loss in iteration 93 : 0.002565196022487961
Loss in iteration 94 : 0.002324383353531093
Loss in iteration 95 : 0.0021100283473809048
Loss in iteration 96 : 0.0019312230920844678
Loss in iteration 97 : 0.0017673828192002869
Loss in iteration 98 : 0.0015767998924909588
Loss in iteration 99 : 0.0014198576600575948
Loss in iteration 100 : 0.001270982243385302
Loss in iteration 101 : 0.0011590225117142957
Loss in iteration 102 : 0.0010719326407754584
Loss in iteration 103 : 0.0010030054837442783
Loss in iteration 104 : 9.452435253773177E-4
Loss in iteration 105 : 8.875402104266943E-4
Loss in iteration 106 : 8.298896745507749E-4
Loss in iteration 107 : 7.722866398420882E-4
Loss in iteration 108 : 7.14726356183913E-4
Loss in iteration 109 : 6.572045484711965E-4
Loss in iteration 110 : 5.997173691093948E-4
Loss in iteration 111 : 5.422613552634109E-4
Loss in iteration 112 : 4.848333903816686E-4
Loss in iteration 113 : 4.319751252936103E-4
Loss in iteration 114 : 3.912157404749305E-4
Loss in iteration 115 : 3.4971321424095877E-4
Testing accuracy  of updater 1 on alg 1 with rate 4.0 = 0.9902222222222222, training accuracy 0.9997142040583024, time elapsed: 2657 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38032923872178076
Loss in iteration 3 : 0.16258107387232926
Loss in iteration 4 : 0.326088268717776
Loss in iteration 5 : 0.14181329563315398
Loss in iteration 6 : 0.19715923835585736
Loss in iteration 7 : 0.19136511362389824
Loss in iteration 8 : 0.1488623986017807
Loss in iteration 9 : 0.13082066461996122
Loss in iteration 10 : 0.13425179828196596
Loss in iteration 11 : 0.13580389235362586
Loss in iteration 12 : 0.13217917907493548
Loss in iteration 13 : 0.12330805276847047
Loss in iteration 14 : 0.11108442122421905
Loss in iteration 15 : 0.09839329628916099
Loss in iteration 16 : 0.08755668495476158
Loss in iteration 17 : 0.08041847796279518
Loss in iteration 18 : 0.07655394981892984
Loss in iteration 19 : 0.07333336569772489
Loss in iteration 20 : 0.06899019835483589
Loss in iteration 21 : 0.06283928146199333
Loss in iteration 22 : 0.056387844296443786
Loss in iteration 23 : 0.051959421746334375
Loss in iteration 24 : 0.050832334470968056
Loss in iteration 25 : 0.05007064644593811
Loss in iteration 26 : 0.046978698802786024
Loss in iteration 27 : 0.041559933942462476
Loss in iteration 28 : 0.036169557942760125
Loss in iteration 29 : 0.03295513746741535
Loss in iteration 30 : 0.031137213550274618
Loss in iteration 31 : 0.029423240588280872
Loss in iteration 32 : 0.02743641016951555
Loss in iteration 33 : 0.025192560434794845
Loss in iteration 34 : 0.022840509318262345
Loss in iteration 35 : 0.020702366695046482
Loss in iteration 36 : 0.01895401980009804
Loss in iteration 37 : 0.017714614877576244
Loss in iteration 38 : 0.016655735035967043
Loss in iteration 39 : 0.015749950724734278
Loss in iteration 40 : 0.014924000736312759
Loss in iteration 41 : 0.01417312765283592
Loss in iteration 42 : 0.013554204574498983
Loss in iteration 43 : 0.012985286387194952
Loss in iteration 44 : 0.012458235618433714
Loss in iteration 45 : 0.011976063444632137
Loss in iteration 46 : 0.011555601637078486
Loss in iteration 47 : 0.011178569309616639
Loss in iteration 48 : 0.010817830882497145
Loss in iteration 49 : 0.010479862675132902
Loss in iteration 50 : 0.010174161576167524
Loss in iteration 51 : 0.009888514185775178
Loss in iteration 52 : 0.009615010849534977
Loss in iteration 53 : 0.009362913295045577
Loss in iteration 54 : 0.009138705998446565
Loss in iteration 55 : 0.008925574003149537
Loss in iteration 56 : 0.008711621398192074
Loss in iteration 57 : 0.008499950342299103
Loss in iteration 58 : 0.008287149536388032
Loss in iteration 59 : 0.00809020640483004
Loss in iteration 60 : 0.007910866779092815
Loss in iteration 61 : 0.007733714544217559
Loss in iteration 62 : 0.0075577732211896806
Loss in iteration 63 : 0.007375695561550129
Loss in iteration 64 : 0.007182286702781469
Loss in iteration 65 : 0.0069966484172230715
Loss in iteration 66 : 0.006832366522164714
Loss in iteration 67 : 0.006682242460558773
Loss in iteration 68 : 0.006533388603102084
Loss in iteration 69 : 0.006384807522359157
Loss in iteration 70 : 0.006239803737357144
Loss in iteration 71 : 0.006102352599248908
Loss in iteration 72 : 0.005974296158170026
Loss in iteration 73 : 0.005847858090912075
Loss in iteration 74 : 0.005725889915850967
Loss in iteration 75 : 0.005604571000940698
Loss in iteration 76 : 0.005484129441281638
Loss in iteration 77 : 0.005364015992688496
Loss in iteration 78 : 0.005244032053741268
Loss in iteration 79 : 0.005124419307830471
Loss in iteration 80 : 0.005005128533369308
Loss in iteration 81 : 0.004886127533212819
Loss in iteration 82 : 0.004767387329930533
Loss in iteration 83 : 0.004648881843835034
Loss in iteration 84 : 0.004531141774015458
Loss in iteration 85 : 0.004419818796336922
Loss in iteration 86 : 0.004310208037880121
Loss in iteration 87 : 0.0042087961151827755
Loss in iteration 88 : 0.0041092021100284926
Loss in iteration 89 : 0.00401753088182345
Loss in iteration 90 : 0.003925346501509381
Loss in iteration 91 : 0.0038308433552356365
Loss in iteration 92 : 0.003734253319598188
Loss in iteration 93 : 0.0036372671107786823
Loss in iteration 94 : 0.0035470150159325697
Loss in iteration 95 : 0.0034555782155347137
Loss in iteration 96 : 0.00336307518014029
Loss in iteration 97 : 0.003270525131009886
Loss in iteration 98 : 0.0031774716186440933
Loss in iteration 99 : 0.003082698029487455
Loss in iteration 100 : 0.0029859908077823644
Loss in iteration 101 : 0.0028879226262501557
Loss in iteration 102 : 0.002790656764925276
Loss in iteration 103 : 0.0026952039295677407
Loss in iteration 104 : 0.0026000280457717496
Loss in iteration 105 : 0.0025051609411288786
Loss in iteration 106 : 0.0024109218905284434
Loss in iteration 107 : 0.002316366486043369
Loss in iteration 108 : 0.002223012692046168
Loss in iteration 109 : 0.002129474787314035
Loss in iteration 110 : 0.0020354700917876905
Loss in iteration 111 : 0.0019429841939802533
Loss in iteration 112 : 0.001849787397037304
Loss in iteration 113 : 0.0017572206403106232
Loss in iteration 114 : 0.0016639661020112011
Loss in iteration 115 : 0.001570092560296312
Loss in iteration 116 : 0.001475797192824111
Loss in iteration 117 : 0.001383078591946978
Loss in iteration 118 : 0.0012902578872001414
Loss in iteration 119 : 0.0011972281749322974
Loss in iteration 120 : 0.0011037228367301614
Loss in iteration 121 : 0.0010196150876353217
Loss in iteration 122 : 9.39829514330749E-4
Loss in iteration 123 : 8.734444647352973E-4
Loss in iteration 124 : 8.139353224343702E-4
Loss in iteration 125 : 7.568956841342973E-4
Loss in iteration 126 : 7.139150980035243E-4
Loss in iteration 127 : 6.75806988882644E-4
Loss in iteration 128 : 6.401471498890737E-4
Loss in iteration 129 : 6.059494707723353E-4
Loss in iteration 130 : 5.741929353346014E-4
Loss in iteration 131 : 5.456665334246765E-4
Loss in iteration 132 : 5.177873516242744E-4
Loss in iteration 133 : 4.911535952133796E-4
Loss in iteration 134 : 4.6471241500477625E-4
Loss in iteration 135 : 4.4122426518001205E-4
Loss in iteration 136 : 4.2332557452018517E-4
Loss in iteration 137 : 4.0401024952612324E-4
Loss in iteration 138 : 3.809266202898356E-4
Loss in iteration 139 : 3.596120947765857E-4
Loss in iteration 140 : 3.39513009299406E-4
Testing accuracy  of updater 1 on alg 1 with rate 1.0 = 0.992, training accuracy 1.0, time elapsed: 2870 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.38217011907456366
Loss in iteration 3 : 0.40137956024834126
Loss in iteration 4 : 0.20990544681406134
Loss in iteration 5 : 0.2563474903640864
Loss in iteration 6 : 0.21622869753686202
Loss in iteration 7 : 0.15322357846235987
Loss in iteration 8 : 0.1465448018628827
Loss in iteration 9 : 0.16680981491697544
Loss in iteration 10 : 0.18009422792658736
Loss in iteration 11 : 0.1708930065362577
Loss in iteration 12 : 0.152886300136513
Loss in iteration 13 : 0.13858109229855617
Loss in iteration 14 : 0.12915726729703347
Loss in iteration 15 : 0.12311286658112619
Loss in iteration 16 : 0.12119206552280527
Loss in iteration 17 : 0.12074894376390884
Loss in iteration 18 : 0.11819165812631956
Loss in iteration 19 : 0.11226880355797879
Loss in iteration 20 : 0.10391327989858848
Loss in iteration 21 : 0.09432067945196985
Loss in iteration 22 : 0.08519666519838358
Loss in iteration 23 : 0.0772050364896727
Loss in iteration 24 : 0.07154544419224926
Loss in iteration 25 : 0.0685045019813374
Loss in iteration 26 : 0.0682353732526023
Loss in iteration 27 : 0.06626722123519331
Loss in iteration 28 : 0.0605148995875584
Loss in iteration 29 : 0.05323055044650192
Loss in iteration 30 : 0.04850543865571373
Loss in iteration 31 : 0.046181220207842884
Loss in iteration 32 : 0.044568388981290515
Loss in iteration 33 : 0.042988823409800536
Loss in iteration 34 : 0.040970100501921486
Loss in iteration 35 : 0.038321526879082746
Loss in iteration 36 : 0.03526924090503595
Loss in iteration 37 : 0.03207820382255435
Loss in iteration 38 : 0.029117630572639355
Loss in iteration 39 : 0.026994581736521063
Loss in iteration 40 : 0.02567063038363253
Loss in iteration 41 : 0.02477966743601765
Loss in iteration 42 : 0.023766943930405342
Loss in iteration 43 : 0.02255488886907692
Loss in iteration 44 : 0.021260338508679796
Loss in iteration 45 : 0.020136984044699734
Loss in iteration 46 : 0.019037804551885168
Loss in iteration 47 : 0.01799289889950467
Loss in iteration 48 : 0.017117855191617175
Loss in iteration 49 : 0.016616447937297572
Loss in iteration 50 : 0.016106950356511954
Loss in iteration 51 : 0.015582702109086504
Loss in iteration 52 : 0.015035477495801324
Loss in iteration 53 : 0.014470893397975835
Loss in iteration 54 : 0.013937332105552742
Loss in iteration 55 : 0.013497042393136828
Loss in iteration 56 : 0.013100294483716053
Loss in iteration 57 : 0.012721423704470698
Loss in iteration 58 : 0.012389444186197552
Loss in iteration 59 : 0.01210928184557321
Loss in iteration 60 : 0.011855218730290985
Loss in iteration 61 : 0.011604105880233972
Loss in iteration 62 : 0.011347936417405289
Loss in iteration 63 : 0.01107111374399391
Loss in iteration 64 : 0.010795763992895106
Loss in iteration 65 : 0.010513102091938153
Loss in iteration 66 : 0.010232928893223425
Loss in iteration 67 : 0.009973278603202754
Loss in iteration 68 : 0.009742671197896893
Loss in iteration 69 : 0.009547218662505476
Loss in iteration 70 : 0.009369015084419589
Loss in iteration 71 : 0.009191388321988042
Loss in iteration 72 : 0.009010284276683512
Loss in iteration 73 : 0.00882199153070178
Loss in iteration 74 : 0.00863814542084896
Loss in iteration 75 : 0.00846355936122519
Loss in iteration 76 : 0.008286506237378843
Loss in iteration 77 : 0.0081105855224887
Loss in iteration 78 : 0.007935900764540753
Loss in iteration 79 : 0.007771226371753199
Loss in iteration 80 : 0.007633007921035767
Loss in iteration 81 : 0.0074921362212764615
Loss in iteration 82 : 0.007358022157486287
Loss in iteration 83 : 0.0072275760141581385
Loss in iteration 84 : 0.0071029968313483785
Loss in iteration 85 : 0.006988335423502417
Loss in iteration 86 : 0.006876938663032496
Loss in iteration 87 : 0.00677282730428589
Loss in iteration 88 : 0.0066718807520747955
Loss in iteration 89 : 0.006570709946652719
Loss in iteration 90 : 0.0064693421365750285
Loss in iteration 91 : 0.006368734714736009
Loss in iteration 92 : 0.006269670559785376
Loss in iteration 93 : 0.0061737197530824
Loss in iteration 94 : 0.0060785155044614314
Loss in iteration 95 : 0.005983669302948937
Loss in iteration 96 : 0.005889145343834067
Loss in iteration 97 : 0.005797943150837872
Loss in iteration 98 : 0.005709652525267681
Loss in iteration 99 : 0.005624568138603439
Loss in iteration 100 : 0.005544077774034516
Loss in iteration 101 : 0.005467169345034288
Loss in iteration 102 : 0.00538881796894256
Loss in iteration 103 : 0.005309167940468479
Loss in iteration 104 : 0.0052290690568278
Loss in iteration 105 : 0.005153476906734765
Loss in iteration 106 : 0.005080307116533386
Loss in iteration 107 : 0.00500979780996692
Loss in iteration 108 : 0.004939775992184594
Loss in iteration 109 : 0.004869007539155047
Loss in iteration 110 : 0.004800113299343208
Loss in iteration 111 : 0.004732280323392523
Loss in iteration 112 : 0.004663854007105564
Loss in iteration 113 : 0.004595414338817965
Loss in iteration 114 : 0.004528728769730505
Loss in iteration 115 : 0.0044630730464602095
Loss in iteration 116 : 0.004397914768452194
Loss in iteration 117 : 0.004332443280635808
Loss in iteration 118 : 0.00426650064600507
Loss in iteration 119 : 0.004200133979241418
Loss in iteration 120 : 0.004133385683558137
Loss in iteration 121 : 0.004066418756125814
Loss in iteration 122 : 0.003999967165558541
Loss in iteration 123 : 0.003935464331596292
Loss in iteration 124 : 0.0038707670098595904
Loss in iteration 125 : 0.0038055068308195166
Loss in iteration 126 : 0.0037397400802064084
Loss in iteration 127 : 0.0036735174151775723
Loss in iteration 128 : 0.003607506483045636
Loss in iteration 129 : 0.0035422549593599464
Loss in iteration 130 : 0.0034776725011072787
Loss in iteration 131 : 0.003412529993083886
Loss in iteration 132 : 0.0033470849786814824
Loss in iteration 133 : 0.0032816842772096367
Loss in iteration 134 : 0.003217019462695926
Loss in iteration 135 : 0.003152358895127805
Loss in iteration 136 : 0.0030880092058203513
Loss in iteration 137 : 0.003023419242992872
Loss in iteration 138 : 0.0029586130339973736
Loss in iteration 139 : 0.002893612203450656
Loss in iteration 140 : 0.002828436213507843
Loss in iteration 141 : 0.0027631025801085414
Loss in iteration 142 : 0.0026990856610906513
Loss in iteration 143 : 0.002634726150615063
Loss in iteration 144 : 0.0025701417752053302
Loss in iteration 145 : 0.0025054048016154335
Loss in iteration 146 : 0.0024404234012906072
Loss in iteration 147 : 0.0023765741317542987
Loss in iteration 148 : 0.00231213405311964
Loss in iteration 149 : 0.002247105392871408
Loss in iteration 150 : 0.0021821089082301897
Loss in iteration 151 : 0.0021216755885893196
Loss in iteration 152 : 0.002062396941776853
Loss in iteration 153 : 0.0020013524493311596
Loss in iteration 154 : 0.0019385841045640075
Loss in iteration 155 : 0.001875750253170595
Loss in iteration 156 : 0.0018155089533310822
Loss in iteration 157 : 0.0017573836788894535
Loss in iteration 158 : 0.0016961515501162283
Loss in iteration 159 : 0.0016332138013146356
Loss in iteration 160 : 0.001572896686728536
Loss in iteration 161 : 0.0015127400657455417
Loss in iteration 162 : 0.0014517809255995696
Loss in iteration 163 : 0.0013900995182069172
Loss in iteration 164 : 0.0013305371952057805
Loss in iteration 165 : 0.0012703159359548303
Loss in iteration 166 : 0.001210123754217606
Loss in iteration 167 : 0.0011504931552954718
Loss in iteration 168 : 0.0010925347683545113
Loss in iteration 169 : 0.0010383804233018948
Loss in iteration 170 : 9.85429212115169E-4
Loss in iteration 171 : 9.326896376186566E-4
Loss in iteration 172 : 8.840434390539593E-4
Loss in iteration 173 : 8.391925507106436E-4
Loss in iteration 174 : 7.942494371879549E-4
Loss in iteration 175 : 7.52151942842071E-4
Loss in iteration 176 : 7.142975458146051E-4
Loss in iteration 177 : 6.80464795030621E-4
Loss in iteration 178 : 6.505649911446601E-4
Loss in iteration 179 : 6.235265966655919E-4
Loss in iteration 180 : 5.973272461427613E-4
Loss in iteration 181 : 5.71446515823022E-4
Loss in iteration 182 : 5.45852543686062E-4
Loss in iteration 183 : 5.21924428304194E-4
Loss in iteration 184 : 5.046723567111854E-4
Testing accuracy  of updater 1 on alg 1 with rate 0.7 = 0.992, training accuracy 0.9998571020291512, time elapsed: 4077 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4795865082425462
Loss in iteration 3 : 0.22605157183275576
Loss in iteration 4 : 0.17852513007227558
Loss in iteration 5 : 0.14639867173089346
Loss in iteration 6 : 0.14276776842295447
Loss in iteration 7 : 0.12359714733593503
Loss in iteration 8 : 0.12048702961001144
Loss in iteration 9 : 0.11986020006265538
Loss in iteration 10 : 0.11728853219709075
Loss in iteration 11 : 0.1131306093888618
Loss in iteration 12 : 0.10811200669876053
Loss in iteration 13 : 0.10320044222632983
Loss in iteration 14 : 0.09902258055586333
Loss in iteration 15 : 0.09509181871957718
Loss in iteration 16 : 0.09104580725205677
Loss in iteration 17 : 0.08649555662346582
Loss in iteration 18 : 0.08175574940430487
Loss in iteration 19 : 0.07712607111608272
Loss in iteration 20 : 0.07287317425103737
Loss in iteration 21 : 0.06903424556413715
Loss in iteration 22 : 0.06590275506038937
Loss in iteration 23 : 0.06285151397343672
Loss in iteration 24 : 0.059819191449383524
Loss in iteration 25 : 0.05692300502831121
Loss in iteration 26 : 0.054220614139272705
Loss in iteration 27 : 0.05184491326362647
Loss in iteration 28 : 0.04975178575178697
Loss in iteration 29 : 0.04782434832495757
Loss in iteration 30 : 0.04574817321027408
Loss in iteration 31 : 0.043391166313872194
Loss in iteration 32 : 0.0409116131508161
Loss in iteration 33 : 0.038306439088163426
Loss in iteration 34 : 0.03575999622421088
Loss in iteration 35 : 0.0333508832494628
Loss in iteration 36 : 0.0309385424765695
Loss in iteration 37 : 0.02859784527147267
Loss in iteration 38 : 0.026442506341333126
Loss in iteration 39 : 0.024502043040717795
Loss in iteration 40 : 0.022700134599358268
Loss in iteration 41 : 0.020991033120781745
Loss in iteration 42 : 0.019528296991852493
Loss in iteration 43 : 0.018371519617649105
Loss in iteration 44 : 0.017349852008017298
Loss in iteration 45 : 0.016486434737195573
Loss in iteration 46 : 0.015693283554513163
Loss in iteration 47 : 0.014908225361461296
Loss in iteration 48 : 0.014195225210587914
Loss in iteration 49 : 0.013535637217181876
Loss in iteration 50 : 0.013019257709994004
Loss in iteration 51 : 0.012671739316110308
Loss in iteration 52 : 0.012317160476464866
Loss in iteration 53 : 0.011976896455702992
Loss in iteration 54 : 0.011630309555161736
Loss in iteration 55 : 0.01127003714412215
Loss in iteration 56 : 0.010920072998790489
Loss in iteration 57 : 0.010596930459115122
Loss in iteration 58 : 0.01030428638556037
Loss in iteration 59 : 0.010045390765425044
Loss in iteration 60 : 0.009811615986977202
Loss in iteration 61 : 0.009603965078774195
Loss in iteration 62 : 0.009404518169725642
Loss in iteration 63 : 0.009210428907024629
Loss in iteration 64 : 0.009018409441465855
Loss in iteration 65 : 0.008821533434366929
Loss in iteration 66 : 0.00863214031665282
Loss in iteration 67 : 0.008457931479060017
Loss in iteration 68 : 0.008299479703878527
Loss in iteration 69 : 0.008148308715014934
Loss in iteration 70 : 0.008003386688722209
Loss in iteration 71 : 0.007864253052588954
Loss in iteration 72 : 0.007728393271914496
Loss in iteration 73 : 0.007595241717713019
Loss in iteration 74 : 0.007459804728487889
Loss in iteration 75 : 0.007325740150001637
Loss in iteration 76 : 0.007201454792405436
Loss in iteration 77 : 0.007085225657851549
Loss in iteration 78 : 0.006969569387834499
Loss in iteration 79 : 0.006854556398436758
Loss in iteration 80 : 0.006739655314620645
Loss in iteration 81 : 0.006623992441142515
Loss in iteration 82 : 0.006511081173857847
Loss in iteration 83 : 0.006409980073060145
Loss in iteration 84 : 0.006313077177001915
Loss in iteration 85 : 0.006218700372446968
Loss in iteration 86 : 0.0061264555776185114
Loss in iteration 87 : 0.0060430036729194146
Loss in iteration 88 : 0.005961770073794772
Loss in iteration 89 : 0.005882127009210348
Loss in iteration 90 : 0.005804064819451647
Loss in iteration 91 : 0.005727806974595489
Loss in iteration 92 : 0.005653002568519558
Loss in iteration 93 : 0.0055790222602301424
Loss in iteration 94 : 0.0055108567901059734
Loss in iteration 95 : 0.005444899029042001
Loss in iteration 96 : 0.005380491715195546
Loss in iteration 97 : 0.005317859069796133
Loss in iteration 98 : 0.005255306575837739
Loss in iteration 99 : 0.005192999763453359
Loss in iteration 100 : 0.005130658828441107
Loss in iteration 101 : 0.005068287183063774
Loss in iteration 102 : 0.0050068389925236245
Loss in iteration 103 : 0.0049468885376285745
Loss in iteration 104 : 0.004886807179201215
Loss in iteration 105 : 0.004828104307971384
Loss in iteration 106 : 0.00477494816251753
Loss in iteration 107 : 0.0047215119573857135
Loss in iteration 108 : 0.0046677139545974865
Loss in iteration 109 : 0.00461433300307612
Loss in iteration 110 : 0.004561599831334443
Loss in iteration 111 : 0.0045107960668543026
Loss in iteration 112 : 0.0044600268089142504
Loss in iteration 113 : 0.004411731315761749
Loss in iteration 114 : 0.004362941554896726
Loss in iteration 115 : 0.00431301100421277
Loss in iteration 116 : 0.004265906949980407
Loss in iteration 117 : 0.004223463988694629
Loss in iteration 118 : 0.004183311474241323
Loss in iteration 119 : 0.0041442307636530605
Loss in iteration 120 : 0.0041052099273104405
Loss in iteration 121 : 0.004065995886669954
Loss in iteration 122 : 0.004026463163389966
Loss in iteration 123 : 0.0039865282670321625
Loss in iteration 124 : 0.003947158214209726
Loss in iteration 125 : 0.003908258056570377
Loss in iteration 126 : 0.0038692863428961793
Loss in iteration 127 : 0.003832476877105888
Loss in iteration 128 : 0.0037948818916922855
Loss in iteration 129 : 0.0037571716389154077
Loss in iteration 130 : 0.003722624797033155
Loss in iteration 131 : 0.003686730010185538
Loss in iteration 132 : 0.0036506640603656115
Loss in iteration 133 : 0.0036154105817314086
Loss in iteration 134 : 0.003579418790441445
Loss in iteration 135 : 0.003542762517761297
Loss in iteration 136 : 0.0035108528067480946
Loss in iteration 137 : 0.003474420771629665
Loss in iteration 138 : 0.003439285164104391
Loss in iteration 139 : 0.0034062771483130416
Loss in iteration 140 : 0.0033715600049702595
Loss in iteration 141 : 0.0033383689304547987
Loss in iteration 142 : 0.0033053202183463276
Loss in iteration 143 : 0.0032718763042395315
Loss in iteration 144 : 0.003238076708334239
Loss in iteration 145 : 0.003203956998810301
Loss in iteration 146 : 0.0031695592052163745
Loss in iteration 147 : 0.0031369003081023455
Loss in iteration 148 : 0.003103023208778436
Loss in iteration 149 : 0.003068811916227821
Loss in iteration 150 : 0.003035556447579331
Loss in iteration 151 : 0.003001526223917092
Loss in iteration 152 : 0.0029686816688471096
Loss in iteration 153 : 0.0029351353786820026
Loss in iteration 154 : 0.0029010832835278115
Loss in iteration 155 : 0.0028690495587324334
Loss in iteration 156 : 0.0028341337890883944
Loss in iteration 157 : 0.0028008173270008896
Loss in iteration 158 : 0.0027670966122919105
Loss in iteration 159 : 0.0027341962678802765
Loss in iteration 160 : 0.0027009751022375197
Loss in iteration 161 : 0.0026675926193245932
Loss in iteration 162 : 0.0026347642084638967
Loss in iteration 163 : 0.0026021578521189825
Loss in iteration 164 : 0.002569606836353726
Loss in iteration 165 : 0.0025376235893180446
Loss in iteration 166 : 0.002505106005258021
Loss in iteration 167 : 0.002472404567654591
Loss in iteration 168 : 0.002439376296283873
Loss in iteration 169 : 0.0024061515513796156
Loss in iteration 170 : 0.002372996035075868
Loss in iteration 171 : 0.0023395140285244966
Loss in iteration 172 : 0.0023070139530817816
Loss in iteration 173 : 0.0022746508682059303
Loss in iteration 174 : 0.002242080175178251
Loss in iteration 175 : 0.002209460317509889
Loss in iteration 176 : 0.002176558468343913
Loss in iteration 177 : 0.002144585977109557
Loss in iteration 178 : 0.0021112784719591686
Loss in iteration 179 : 0.002079893615149301
Loss in iteration 180 : 0.002046545857950445
Loss in iteration 181 : 0.0020148163017319146
Loss in iteration 182 : 0.001982656825605107
Loss in iteration 183 : 0.0019486313132197537
Loss in iteration 184 : 0.0019166949021246675
Loss in iteration 185 : 0.0018831564437897827
Loss in iteration 186 : 0.0018502526971762101
Loss in iteration 187 : 0.0018185876692229177
Loss in iteration 188 : 0.0017853514490174856
Loss in iteration 189 : 0.001753157807017805
Loss in iteration 190 : 0.0017201247450850188
Loss in iteration 191 : 0.0016869834509349929
Loss in iteration 192 : 0.001656839282836322
Loss in iteration 193 : 0.0016225837455931846
Loss in iteration 194 : 0.0015919726624174374
Loss in iteration 195 : 0.0015595825409272124
Loss in iteration 196 : 0.0015257086870786275
Loss in iteration 197 : 0.001494237043537587
Loss in iteration 198 : 0.0014611850351772132
Loss in iteration 199 : 0.0014279217727897326
Loss in iteration 200 : 0.001397303048036105
Testing accuracy  of updater 1 on alg 1 with rate 0.4 = 0.9911111111111112, training accuracy 0.9997142040583024, time elapsed: 4157 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8642561778766499
Loss in iteration 3 : 0.6063429158422842
Loss in iteration 4 : 0.4059050616552262
Loss in iteration 5 : 0.2617863986962837
Loss in iteration 6 : 0.20918030163172407
Loss in iteration 7 : 0.18624151727401242
Loss in iteration 8 : 0.17111295857390685
Loss in iteration 9 : 0.16033436855827457
Loss in iteration 10 : 0.14620481028971444
Loss in iteration 11 : 0.13018154793234904
Loss in iteration 12 : 0.12083721130120088
Loss in iteration 13 : 0.11562572415307916
Loss in iteration 14 : 0.11171603232471544
Loss in iteration 15 : 0.10843099073462258
Loss in iteration 16 : 0.10531626494123597
Loss in iteration 17 : 0.1023442542907585
Loss in iteration 18 : 0.09949210537593345
Loss in iteration 19 : 0.09677037703386146
Loss in iteration 20 : 0.09419722681325281
Loss in iteration 21 : 0.09169070599950761
Loss in iteration 22 : 0.08921294565228256
Loss in iteration 23 : 0.0867100788148634
Loss in iteration 24 : 0.0841694084741187
Loss in iteration 25 : 0.08162757754090255
Loss in iteration 26 : 0.07920069758406442
Loss in iteration 27 : 0.07688114878077898
Loss in iteration 28 : 0.0746894271041305
Loss in iteration 29 : 0.07260135813019747
Loss in iteration 30 : 0.07058397132959245
Loss in iteration 31 : 0.06859153996044014
Loss in iteration 32 : 0.06663963974816109
Loss in iteration 33 : 0.0646991359116289
Loss in iteration 34 : 0.06285192922141793
Loss in iteration 35 : 0.06103768872677061
Loss in iteration 36 : 0.059311111762704774
Loss in iteration 37 : 0.05767499318515044
Loss in iteration 38 : 0.05609978092629534
Loss in iteration 39 : 0.054628611018654405
Loss in iteration 40 : 0.05324688783244261
Loss in iteration 41 : 0.051982503635794916
Loss in iteration 42 : 0.050813322201305314
Loss in iteration 43 : 0.0497124538869808
Loss in iteration 44 : 0.04868087915375325
Loss in iteration 45 : 0.047686791552836116
Loss in iteration 46 : 0.04675172575680978
Loss in iteration 47 : 0.04584432534765278
Loss in iteration 48 : 0.04497514836247067
Loss in iteration 49 : 0.044124509289357405
Loss in iteration 50 : 0.043286294171716415
Loss in iteration 51 : 0.042440576555540704
Loss in iteration 52 : 0.04158425719848226
Loss in iteration 53 : 0.04071503613443036
Loss in iteration 54 : 0.03987014176908591
Loss in iteration 55 : 0.039031286094879034
Loss in iteration 56 : 0.038183856252870386
Loss in iteration 57 : 0.037328900295786255
Loss in iteration 58 : 0.03647117692742889
Loss in iteration 59 : 0.03561819738990076
Loss in iteration 60 : 0.0347723580027691
Loss in iteration 61 : 0.03394500589834645
Loss in iteration 62 : 0.03314016731625736
Loss in iteration 63 : 0.032355960529197716
Loss in iteration 64 : 0.031579195117284284
Loss in iteration 65 : 0.030820471486229902
Loss in iteration 66 : 0.0300893620239776
Loss in iteration 67 : 0.029397692575831885
Loss in iteration 68 : 0.02872517691206639
Loss in iteration 69 : 0.028068449586034053
Loss in iteration 70 : 0.027431970477274966
Loss in iteration 71 : 0.026828202981833384
Loss in iteration 72 : 0.02624907109783564
Loss in iteration 73 : 0.025675858199342136
Loss in iteration 74 : 0.025103361692422502
Loss in iteration 75 : 0.024532612295808248
Loss in iteration 76 : 0.023961302307837908
Loss in iteration 77 : 0.0233933360260768
Loss in iteration 78 : 0.022830419069357327
Loss in iteration 79 : 0.022270356079732434
Loss in iteration 80 : 0.02171159722416133
Loss in iteration 81 : 0.021152655821494712
Loss in iteration 82 : 0.02059625951517058
Loss in iteration 83 : 0.020047462649047425
Loss in iteration 84 : 0.01951439912351132
Loss in iteration 85 : 0.018999837980050928
Loss in iteration 86 : 0.01850980374407596
Loss in iteration 87 : 0.018047020252829296
Loss in iteration 88 : 0.017604064142348094
Loss in iteration 89 : 0.017180234032860852
Loss in iteration 90 : 0.016779079793215003
Loss in iteration 91 : 0.01640208055786216
Loss in iteration 92 : 0.01604870314744102
Loss in iteration 93 : 0.01573522309841142
Loss in iteration 94 : 0.015428770403479352
Loss in iteration 95 : 0.01515242343146793
Loss in iteration 96 : 0.014894176567302611
Loss in iteration 97 : 0.014650336203318107
Loss in iteration 98 : 0.014413261564811354
Loss in iteration 99 : 0.014183857981893836
Loss in iteration 100 : 0.013966405648652555
Loss in iteration 101 : 0.013757837089122609
Loss in iteration 102 : 0.013555183591921389
Loss in iteration 103 : 0.013360381389714392
Loss in iteration 104 : 0.013176029135712334
Loss in iteration 105 : 0.01299813852280319
Loss in iteration 106 : 0.012825833823099098
Loss in iteration 107 : 0.01265586163799484
Loss in iteration 108 : 0.012487722090654426
Loss in iteration 109 : 0.012319138472605184
Loss in iteration 110 : 0.012151582955248517
Loss in iteration 111 : 0.011987172516616637
Loss in iteration 112 : 0.011830460560861084
Loss in iteration 113 : 0.01167943257987926
Loss in iteration 114 : 0.011531141414032112
Loss in iteration 115 : 0.011383309800989385
Loss in iteration 116 : 0.011235915687469894
Loss in iteration 117 : 0.011090919086050016
Loss in iteration 118 : 0.010948372715103346
Loss in iteration 119 : 0.010807714238009122
Loss in iteration 120 : 0.010668049811869165
Loss in iteration 121 : 0.01052885276603875
Loss in iteration 122 : 0.010390655989758961
Loss in iteration 123 : 0.010255646526905337
Loss in iteration 124 : 0.010122220166125422
Loss in iteration 125 : 0.009990517892875573
Loss in iteration 126 : 0.009861203841908871
Loss in iteration 127 : 0.009734606522884536
Loss in iteration 128 : 0.009611703764318138
Loss in iteration 129 : 0.009490058039386443
Loss in iteration 130 : 0.00936906270108781
Loss in iteration 131 : 0.009249235322326698
Loss in iteration 132 : 0.00913077050354941
Loss in iteration 133 : 0.009018233587989382
Loss in iteration 134 : 0.00891298388348163
Loss in iteration 135 : 0.008808060358521426
Loss in iteration 136 : 0.008707547423897073
Loss in iteration 137 : 0.008609305504301843
Loss in iteration 138 : 0.008513825036317294
Loss in iteration 139 : 0.008419976763709095
Loss in iteration 140 : 0.008332036554344401
Loss in iteration 141 : 0.008247095110111284
Loss in iteration 142 : 0.008164999173649021
Loss in iteration 143 : 0.008087776745916953
Loss in iteration 144 : 0.008016114387880724
Loss in iteration 145 : 0.007945420556630212
Loss in iteration 146 : 0.007876182023468375
Loss in iteration 147 : 0.007806471882214724
Loss in iteration 148 : 0.007737191998096088
Loss in iteration 149 : 0.00767050491511085
Loss in iteration 150 : 0.007604238487899029
Loss in iteration 151 : 0.007538614792682964
Loss in iteration 152 : 0.007474807973396951
Loss in iteration 153 : 0.007413131320465432
Loss in iteration 154 : 0.0073532927034994245
Loss in iteration 155 : 0.007294308682732875
Loss in iteration 156 : 0.007236428253459643
Loss in iteration 157 : 0.007180302433383254
Loss in iteration 158 : 0.007124694646500706
Loss in iteration 159 : 0.007069574225602246
Loss in iteration 160 : 0.007014687320719835
Loss in iteration 161 : 0.006960224580598534
Loss in iteration 162 : 0.006908032229126692
Loss in iteration 163 : 0.0068581375514255895
Loss in iteration 164 : 0.006809633179699806
Loss in iteration 165 : 0.006761462017835993
Loss in iteration 166 : 0.0067136240511946445
Loss in iteration 167 : 0.006666411421870448
Loss in iteration 168 : 0.006619453832745061
Loss in iteration 169 : 0.00657402042850418
Loss in iteration 170 : 0.006528800807267543
Loss in iteration 171 : 0.0064838589159202835
Loss in iteration 172 : 0.0064403679471448545
Loss in iteration 173 : 0.006397329633113332
Loss in iteration 174 : 0.006353952961880634
Loss in iteration 175 : 0.006310349594384537
Loss in iteration 176 : 0.006266951410164002
Loss in iteration 177 : 0.006224059568551342
Loss in iteration 178 : 0.006181914704881536
Loss in iteration 179 : 0.006140019679593038
Loss in iteration 180 : 0.006097701484264317
Loss in iteration 181 : 0.006055004006114192
Loss in iteration 182 : 0.006013611258465876
Loss in iteration 183 : 0.005972370492534965
Loss in iteration 184 : 0.005931138870382226
Loss in iteration 185 : 0.005889666692758222
Loss in iteration 186 : 0.005848574210203467
Loss in iteration 187 : 0.005807844001130582
Loss in iteration 188 : 0.005766935404343849
Loss in iteration 189 : 0.0057258662586146475
Loss in iteration 190 : 0.005684733090499535
Loss in iteration 191 : 0.005644308077242625
Loss in iteration 192 : 0.005604090869113612
Loss in iteration 193 : 0.005565530959540387
Loss in iteration 194 : 0.005527599870723351
Loss in iteration 195 : 0.005489767438404656
Loss in iteration 196 : 0.005452023602247526
Loss in iteration 197 : 0.00541428768550419
Loss in iteration 198 : 0.005378583999373583
Loss in iteration 199 : 0.005343012579564708
Loss in iteration 200 : 0.005307754811460914
Testing accuracy  of updater 1 on alg 1 with rate 0.09999999999999998 = 0.9786666666666667, training accuracy 0.9985710202915119, time elapsed: 4116 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.443743919485099
Loss in iteration 3 : 0.8831775742728436
Loss in iteration 4 : 0.8273652125814578
Loss in iteration 5 : 0.9480216160237186
Loss in iteration 6 : 0.881446342941163
Loss in iteration 7 : 0.7848374331669175
Loss in iteration 8 : 0.7125773997293641
Loss in iteration 9 : 0.6432121477284963
Loss in iteration 10 : 0.5859097930158813
Loss in iteration 11 : 0.5444547408485861
Loss in iteration 12 : 0.49282691334067313
Loss in iteration 13 : 0.4283480375942148
Loss in iteration 14 : 0.3626709469155019
Loss in iteration 15 : 0.31141532259672094
Loss in iteration 16 : 0.28353028195902535
Loss in iteration 17 : 0.2707505974765086
Loss in iteration 18 : 0.26077768686227926
Loss in iteration 19 : 0.24453506943594683
Loss in iteration 20 : 0.22981016185416986
Loss in iteration 21 : 0.2149679181804924
Loss in iteration 22 : 0.198296577246585
Loss in iteration 23 : 0.18079947001140312
Loss in iteration 24 : 0.1684213471622158
Loss in iteration 25 : 0.15728511271049642
Loss in iteration 26 : 0.14693039908973202
Loss in iteration 27 : 0.1344987926420939
Loss in iteration 28 : 0.12333352312015185
Loss in iteration 29 : 0.11295217396002848
Loss in iteration 30 : 0.10365937405844124
Loss in iteration 31 : 0.09570981880545043
Loss in iteration 32 : 0.08903845681180887
Loss in iteration 33 : 0.0841369325518683
Loss in iteration 34 : 0.07991315544471948
Loss in iteration 35 : 0.07645406058915195
Loss in iteration 36 : 0.07340568151854411
Loss in iteration 37 : 0.0702961125743953
Loss in iteration 38 : 0.06734673583154986
Loss in iteration 39 : 0.06456740891600607
Loss in iteration 40 : 0.061944105906499465
Loss in iteration 41 : 0.05958287410471352
Loss in iteration 42 : 0.057342354981354045
Loss in iteration 43 : 0.055275676922061014
Loss in iteration 44 : 0.05327988426650352
Loss in iteration 45 : 0.05138557754808104
Loss in iteration 46 : 0.04955451536233321
Loss in iteration 47 : 0.047726466984851564
Loss in iteration 48 : 0.045875187359684454
Loss in iteration 49 : 0.04402825967561158
Loss in iteration 50 : 0.04228203985854601
Loss in iteration 51 : 0.04066659622374072
Loss in iteration 52 : 0.03923067008192779
Loss in iteration 53 : 0.03786294060903373
Loss in iteration 54 : 0.03654851236834937
Loss in iteration 55 : 0.03526445658860708
Loss in iteration 56 : 0.03402936906979717
Loss in iteration 57 : 0.032806298030671255
Loss in iteration 58 : 0.031610940455276154
Loss in iteration 59 : 0.0304918579461378
Loss in iteration 60 : 0.029403838175058768
Loss in iteration 61 : 0.02833227224585594
Loss in iteration 62 : 0.027260727553723337
Loss in iteration 63 : 0.026203086181648773
Loss in iteration 64 : 0.02516605377166873
Loss in iteration 65 : 0.024159785029112125
Loss in iteration 66 : 0.023132028253863922
Loss in iteration 67 : 0.022095146487342097
Loss in iteration 68 : 0.021075230180428933
Loss in iteration 69 : 0.02011104611988511
Loss in iteration 70 : 0.01919593060439347
Loss in iteration 71 : 0.018292812181177464
Loss in iteration 72 : 0.01738974995136169
Loss in iteration 73 : 0.01649032837063809
Loss in iteration 74 : 0.015606377430297979
Loss in iteration 75 : 0.014725067587893792
Loss in iteration 76 : 0.013850040078675772
Loss in iteration 77 : 0.012983676464206577
Loss in iteration 78 : 0.012112197617353932
Loss in iteration 79 : 0.011242499184207532
Loss in iteration 80 : 0.01038111689352831
Loss in iteration 81 : 0.009514033532932446
Loss in iteration 82 : 0.008659287544141686
Loss in iteration 83 : 0.007804600551117813
Loss in iteration 84 : 0.006959968798360557
Loss in iteration 85 : 0.006188896753380576
Loss in iteration 86 : 0.005518120863039
Loss in iteration 87 : 0.005000527695525649
Loss in iteration 88 : 0.004502124848242966
Loss in iteration 89 : 0.004016193996655485
Loss in iteration 90 : 0.0035414879411936944
Loss in iteration 91 : 0.0031209741403717805
Loss in iteration 92 : 0.0027276058026452273
Loss in iteration 93 : 0.002353394010240028
Loss in iteration 94 : 0.0020977037773530897
Loss in iteration 95 : 0.0018766482343403276
Loss in iteration 96 : 0.0016885354491523383
Loss in iteration 97 : 0.0015405754743674966
Loss in iteration 98 : 0.0013984267718291513
Loss in iteration 99 : 0.001261508214312648
Loss in iteration 100 : 0.001148181465950646
Loss in iteration 101 : 0.0010325138544245564
Loss in iteration 102 : 9.18667403790687E-4
Loss in iteration 103 : 8.53134076273383E-4
Loss in iteration 104 : 7.900701154932672E-4
Loss in iteration 105 : 7.292285847766229E-4
Loss in iteration 106 : 6.703872411171016E-4
Testing accuracy  of updater 2 on alg 1 with rate 10.0 = 0.9893333333333333, training accuracy 0.9997142040583024, time elapsed: 2055 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.4547303741133404
Loss in iteration 3 : 0.6327898298791285
Loss in iteration 4 : 0.6014347412452224
Loss in iteration 5 : 0.6688022696804484
Loss in iteration 6 : 0.625703609114914
Loss in iteration 7 : 0.5593163732401272
Loss in iteration 8 : 0.5111042946465106
Loss in iteration 9 : 0.4636660721168963
Loss in iteration 10 : 0.42522386860043965
Loss in iteration 11 : 0.39633072833608834
Loss in iteration 12 : 0.35792876984668126
Loss in iteration 13 : 0.3099213440806039
Loss in iteration 14 : 0.26207067200312734
Loss in iteration 15 : 0.22546239383138728
Loss in iteration 16 : 0.20502491038991105
Loss in iteration 17 : 0.1943735976891775
Loss in iteration 18 : 0.1865033548098105
Loss in iteration 19 : 0.17466724671251135
Loss in iteration 20 : 0.16388136757070468
Loss in iteration 21 : 0.1533116469846367
Loss in iteration 22 : 0.14147314107138423
Loss in iteration 23 : 0.12925766561534663
Loss in iteration 24 : 0.12052461976932431
Loss in iteration 25 : 0.11244745743760656
Loss in iteration 26 : 0.10492286478988769
Loss in iteration 27 : 0.09634338982995445
Loss in iteration 28 : 0.08828925223994996
Loss in iteration 29 : 0.08104822132841503
Loss in iteration 30 : 0.07438611027910284
Loss in iteration 31 : 0.0687124374663457
Loss in iteration 32 : 0.06392638503971855
Loss in iteration 33 : 0.06045548354274786
Loss in iteration 34 : 0.05745334436567393
Loss in iteration 35 : 0.054922213965896124
Loss in iteration 36 : 0.05266119445305774
Loss in iteration 37 : 0.05039196335777749
Loss in iteration 38 : 0.0482242474777131
Loss in iteration 39 : 0.04627735746478655
Loss in iteration 40 : 0.044479441012597376
Loss in iteration 41 : 0.04273181922601291
Loss in iteration 42 : 0.04122836077921423
Loss in iteration 43 : 0.03974489327942183
Loss in iteration 44 : 0.03829821382064248
Loss in iteration 45 : 0.03697712466868825
Loss in iteration 46 : 0.03571614599444126
Loss in iteration 47 : 0.03443049665792571
Loss in iteration 48 : 0.033102840009728886
Loss in iteration 49 : 0.03180291911184718
Loss in iteration 50 : 0.030577541677106905
Loss in iteration 51 : 0.02945317677467315
Loss in iteration 52 : 0.028413046470014518
Loss in iteration 53 : 0.02738172174043644
Loss in iteration 54 : 0.026458359906074057
Loss in iteration 55 : 0.025581536676308542
Loss in iteration 56 : 0.024712169692734116
Loss in iteration 57 : 0.023853686398807136
Loss in iteration 58 : 0.0230021492108719
Loss in iteration 59 : 0.02219041648264886
Loss in iteration 60 : 0.021420761234803787
Loss in iteration 61 : 0.020662742353203534
Loss in iteration 62 : 0.019913042349516998
Loss in iteration 63 : 0.019197868833246975
Loss in iteration 64 : 0.01845845798331735
Loss in iteration 65 : 0.0177305783846797
Loss in iteration 66 : 0.01700884774940317
Loss in iteration 67 : 0.01628950457903637
Loss in iteration 68 : 0.015570371627338795
Loss in iteration 69 : 0.014877159389519737
Loss in iteration 70 : 0.014196821069829578
Loss in iteration 71 : 0.013555282421072809
Loss in iteration 72 : 0.012920869077133674
Loss in iteration 73 : 0.01228834109803195
Loss in iteration 74 : 0.011667156179287144
Loss in iteration 75 : 0.011044733173929486
Loss in iteration 76 : 0.010430430805949981
Loss in iteration 77 : 0.00981626337438908
Loss in iteration 78 : 0.009203037143810386
Loss in iteration 79 : 0.0085974407394246
Loss in iteration 80 : 0.007987469984948507
Loss in iteration 81 : 0.007382820708254614
Loss in iteration 82 : 0.006775823667799061
Loss in iteration 83 : 0.006173917764470114
Loss in iteration 84 : 0.0055706051491751845
Loss in iteration 85 : 0.004967216999495532
Loss in iteration 86 : 0.004437710624617443
Loss in iteration 87 : 0.003913995781793607
Loss in iteration 88 : 0.003518327400254547
Loss in iteration 89 : 0.003163776780846416
Loss in iteration 90 : 0.002818523421055958
Loss in iteration 91 : 0.0024828918626120535
Loss in iteration 92 : 0.0022002851099306568
Loss in iteration 93 : 0.001922782945214942
Loss in iteration 94 : 0.0016737788635540721
Loss in iteration 95 : 0.001478844802457057
Loss in iteration 96 : 0.0013308447595480966
Loss in iteration 97 : 0.001219610500031109
Loss in iteration 98 : 0.0011161293962437988
Loss in iteration 99 : 0.001016707095172827
Loss in iteration 100 : 9.369750482218082E-4
Loss in iteration 101 : 8.546861024612469E-4
Loss in iteration 102 : 7.721926614567208E-4
Loss in iteration 103 : 6.895151747326107E-4
Loss in iteration 104 : 6.24592977907123E-4
Loss in iteration 105 : 5.792781694454536E-4
Loss in iteration 106 : 5.356360656197765E-4
Loss in iteration 107 : 4.934993959664862E-4
Loss in iteration 108 : 4.5271761706834856E-4
Testing accuracy  of updater 2 on alg 1 with rate 7.0 = 0.9893333333333333, training accuracy 0.9997142040583024, time elapsed: 2062 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.4673353289769404
Loss in iteration 3 : 0.38035239889304917
Loss in iteration 4 : 0.37942554852360133
Loss in iteration 5 : 0.393150776880603
Loss in iteration 6 : 0.36770966570287783
Loss in iteration 7 : 0.33336201507636887
Loss in iteration 8 : 0.31020862194407234
Loss in iteration 9 : 0.28640243629759804
Loss in iteration 10 : 0.2686791170525267
Loss in iteration 11 : 0.25016275281084344
Loss in iteration 12 : 0.2255273205644365
Loss in iteration 13 : 0.19547350158833315
Loss in iteration 14 : 0.16494579472547863
Loss in iteration 15 : 0.1411654552462159
Loss in iteration 16 : 0.12797858460468425
Loss in iteration 17 : 0.1192191149851336
Loss in iteration 18 : 0.11308506261315496
Loss in iteration 19 : 0.10539823489736029
Loss in iteration 20 : 0.09857285978350334
Loss in iteration 21 : 0.09236834643557469
Loss in iteration 22 : 0.08553388029627527
Loss in iteration 23 : 0.07837181175011125
Loss in iteration 24 : 0.07296200474294814
Loss in iteration 25 : 0.06790287972661778
Loss in iteration 26 : 0.06299990878082841
Loss in iteration 27 : 0.05819769379204627
Loss in iteration 28 : 0.05347168857699371
Loss in iteration 29 : 0.049242753850631774
Loss in iteration 30 : 0.04515017827388802
Loss in iteration 31 : 0.04159743546613671
Loss in iteration 32 : 0.0388800065173032
Loss in iteration 33 : 0.03677543468685893
Loss in iteration 34 : 0.035001749060035635
Loss in iteration 35 : 0.03340760232783624
Loss in iteration 36 : 0.031834827738441845
Loss in iteration 37 : 0.030381785365968222
Loss in iteration 38 : 0.029121065677649625
Loss in iteration 39 : 0.028020129755437127
Loss in iteration 40 : 0.026988185245950067
Loss in iteration 41 : 0.02597078012543645
Loss in iteration 42 : 0.025010967382242236
Loss in iteration 43 : 0.02412505603282676
Loss in iteration 44 : 0.02328199855673501
Loss in iteration 45 : 0.02250653009285103
Loss in iteration 46 : 0.021750008064807758
Loss in iteration 47 : 0.021016958571091638
Loss in iteration 48 : 0.02025454561129216
Loss in iteration 49 : 0.01951217099979847
Loss in iteration 50 : 0.018858433392445865
Loss in iteration 51 : 0.018226220596889053
Loss in iteration 52 : 0.01759249793670659
Loss in iteration 53 : 0.01696308777275527
Loss in iteration 54 : 0.016380722448152467
Loss in iteration 55 : 0.015845394538651747
Loss in iteration 56 : 0.015319124910402255
Loss in iteration 57 : 0.01481202175615508
Loss in iteration 58 : 0.014316067120335629
Loss in iteration 59 : 0.013841583791129888
Loss in iteration 60 : 0.013380499746487674
Loss in iteration 61 : 0.012949787991134137
Loss in iteration 62 : 0.01252422008453851
Loss in iteration 63 : 0.012099796519552784
Loss in iteration 64 : 0.011678851353505004
Loss in iteration 65 : 0.01125826257139339
Loss in iteration 66 : 0.01083927222253252
Loss in iteration 67 : 0.010429738787555216
Loss in iteration 68 : 0.010034455837873647
Loss in iteration 69 : 0.009649571830614786
Loss in iteration 70 : 0.009263235036459587
Loss in iteration 71 : 0.008875590734097698
Loss in iteration 72 : 0.008486769674349769
Loss in iteration 73 : 0.00810653565789878
Loss in iteration 74 : 0.007720365435694354
Loss in iteration 75 : 0.0073496962444076375
Loss in iteration 76 : 0.006989410237967004
Loss in iteration 77 : 0.006630859613550326
Loss in iteration 78 : 0.006272256986701224
Loss in iteration 79 : 0.005922253968957937
Loss in iteration 80 : 0.0055705260772221194
Loss in iteration 81 : 0.005224420109416542
Loss in iteration 82 : 0.004874712468822377
Loss in iteration 83 : 0.004529124869420956
Loss in iteration 84 : 0.0041836918742969
Loss in iteration 85 : 0.0038371316342685844
Loss in iteration 86 : 0.0034947247402709967
Loss in iteration 87 : 0.0031522376191017083
Loss in iteration 88 : 0.00281325910018211
Loss in iteration 89 : 0.0024988797136013454
Loss in iteration 90 : 0.0022057455436738623
Loss in iteration 91 : 0.001939887461501213
Loss in iteration 92 : 0.0017365769962554315
Loss in iteration 93 : 0.0015400286671183841
Loss in iteration 94 : 0.0013735848759280488
Loss in iteration 95 : 0.0012127599196156869
Loss in iteration 96 : 0.001054785409047446
Loss in iteration 97 : 9.215031312535483E-4
Loss in iteration 98 : 8.265306104122309E-4
Loss in iteration 99 : 7.63905318706318E-4
Loss in iteration 100 : 7.039486660782037E-4
Loss in iteration 101 : 6.463937886201018E-4
Loss in iteration 102 : 5.928933918270045E-4
Loss in iteration 103 : 5.464512257057454E-4
Loss in iteration 104 : 4.959227408778265E-4
Loss in iteration 105 : 4.41789096581874E-4
Loss in iteration 106 : 3.899958894211762E-4
Loss in iteration 107 : 3.596996004218034E-4
Loss in iteration 108 : 3.3204099857965736E-4
Loss in iteration 109 : 3.096658066064863E-4
Loss in iteration 110 : 2.857852700132271E-4
Testing accuracy  of updater 2 on alg 1 with rate 4.0 = 0.9884444444444445, training accuracy 0.9998571020291512, time elapsed: 1890 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5009066282033307
Loss in iteration 3 : 0.13945594895401872
Loss in iteration 4 : 0.20702487837745007
Loss in iteration 5 : 0.23068228073942
Loss in iteration 6 : 0.13686249024585556
Loss in iteration 7 : 0.12810766704887142
Loss in iteration 8 : 0.12619627081869064
Loss in iteration 9 : 0.12112996591329325
Loss in iteration 10 : 0.11438757412942752
Loss in iteration 11 : 0.10668682977842948
Loss in iteration 12 : 0.09835515054152012
Loss in iteration 13 : 0.09044711506451664
Loss in iteration 14 : 0.08308063192213236
Loss in iteration 15 : 0.07599451598614856
Loss in iteration 16 : 0.06870545694962397
Loss in iteration 17 : 0.061309002627908296
Loss in iteration 18 : 0.05437880185620423
Loss in iteration 19 : 0.04875428924478039
Loss in iteration 20 : 0.04510910227200869
Loss in iteration 21 : 0.04238709013505197
Loss in iteration 22 : 0.03900493394684469
Loss in iteration 23 : 0.03549849953192093
Loss in iteration 24 : 0.032320910827068665
Loss in iteration 25 : 0.0294678541266812
Loss in iteration 26 : 0.027073607933345647
Loss in iteration 27 : 0.02526298268752436
Loss in iteration 28 : 0.02369262254725149
Loss in iteration 29 : 0.02226835017055276
Loss in iteration 30 : 0.02092199168399479
Loss in iteration 31 : 0.019496619684169443
Loss in iteration 32 : 0.01813490963822116
Loss in iteration 33 : 0.017068834136367835
Loss in iteration 34 : 0.01607064801160084
Loss in iteration 35 : 0.01511671218571472
Loss in iteration 36 : 0.014172609820475374
Loss in iteration 37 : 0.013314302603063908
Loss in iteration 38 : 0.012499127948630267
Loss in iteration 39 : 0.01184573602484942
Loss in iteration 40 : 0.011316942420370093
Loss in iteration 41 : 0.010800386528056231
Loss in iteration 42 : 0.010395181846019862
Loss in iteration 43 : 0.010041575963475187
Loss in iteration 44 : 0.009703145238582124
Loss in iteration 45 : 0.009387931603073676
Loss in iteration 46 : 0.009092470736069677
Loss in iteration 47 : 0.008809317199840561
Loss in iteration 48 : 0.008540137444418432
Loss in iteration 49 : 0.008294725131438585
Loss in iteration 50 : 0.008051036283590515
Loss in iteration 51 : 0.007807779592769044
Loss in iteration 52 : 0.007596488340485405
Loss in iteration 53 : 0.007397914849946349
Loss in iteration 54 : 0.007203837197695826
Loss in iteration 55 : 0.0070043572171320165
Loss in iteration 56 : 0.00681386660093895
Loss in iteration 57 : 0.006641325150717084
Loss in iteration 58 : 0.006470623535471328
Loss in iteration 59 : 0.006308084933591683
Loss in iteration 60 : 0.006160095909374621
Loss in iteration 61 : 0.006017924624319082
Loss in iteration 62 : 0.005891537964035427
Loss in iteration 63 : 0.005769996121098057
Loss in iteration 64 : 0.005652694635042582
Loss in iteration 65 : 0.0055421076888174485
Loss in iteration 66 : 0.005432273133308682
Loss in iteration 67 : 0.005322623577902214
Loss in iteration 68 : 0.005209521242452516
Loss in iteration 69 : 0.0051016296088500415
Loss in iteration 70 : 0.004993253774448098
Loss in iteration 71 : 0.004890174546835247
Loss in iteration 72 : 0.004787978101151068
Loss in iteration 73 : 0.00468515283832333
Loss in iteration 74 : 0.0045825327503526
Loss in iteration 75 : 0.0044838058789328505
Loss in iteration 76 : 0.004388301224177016
Loss in iteration 77 : 0.004292830544431907
Loss in iteration 78 : 0.004197365808106671
Loss in iteration 79 : 0.004102085958879192
Loss in iteration 80 : 0.004009003880625097
Loss in iteration 81 : 0.003916538899861825
Loss in iteration 82 : 0.0038257778434223444
Loss in iteration 83 : 0.0037340131530266022
Loss in iteration 84 : 0.0036422242908920564
Loss in iteration 85 : 0.003550332967550534
Loss in iteration 86 : 0.003458771914981615
Loss in iteration 87 : 0.0033667030013721836
Loss in iteration 88 : 0.0032743049184797385
Loss in iteration 89 : 0.0031825621200196386
Loss in iteration 90 : 0.003089839877725521
Loss in iteration 91 : 0.002997447182861699
Loss in iteration 92 : 0.002906377185392917
Loss in iteration 93 : 0.002813725088673163
Loss in iteration 94 : 0.0027219370779563595
Loss in iteration 95 : 0.002629648594451294
Loss in iteration 96 : 0.0025378704107531813
Loss in iteration 97 : 0.002446983493329092
Loss in iteration 98 : 0.0023558861578527497
Loss in iteration 99 : 0.002265213680995413
Loss in iteration 100 : 0.0021751610574557256
Loss in iteration 101 : 0.0020868381980074465
Loss in iteration 102 : 0.001994206248129536
Loss in iteration 103 : 0.0019044611826006174
Loss in iteration 104 : 0.001815057988171582
Loss in iteration 105 : 0.0017265313940222024
Loss in iteration 106 : 0.001636342147168059
Loss in iteration 107 : 0.0015480509676840048
Loss in iteration 108 : 0.001458898378591482
Loss in iteration 109 : 0.0013696143516768878
Loss in iteration 110 : 0.0012803493938519292
Loss in iteration 111 : 0.0011912770652005588
Loss in iteration 112 : 0.0011023359005745564
Loss in iteration 113 : 0.0010145576821193262
Loss in iteration 114 : 9.303761008441811E-4
Loss in iteration 115 : 8.552266651167055E-4
Loss in iteration 116 : 7.792477086239621E-4
Loss in iteration 117 : 7.100435277091645E-4
Loss in iteration 118 : 6.482862017816814E-4
Loss in iteration 119 : 5.971266158541842E-4
Loss in iteration 120 : 5.490833261523458E-4
Loss in iteration 121 : 5.025008974375705E-4
Loss in iteration 122 : 4.6290682846727267E-4
Loss in iteration 123 : 4.33779336065967E-4
Loss in iteration 124 : 4.1007317281445925E-4
Loss in iteration 125 : 3.853734574161788E-4
Loss in iteration 126 : 3.602805277944315E-4
Loss in iteration 127 : 3.3608156585851164E-4
Loss in iteration 128 : 3.200907677014452E-4
Loss in iteration 129 : 3.04373676152242E-4
Loss in iteration 130 : 2.896698144447705E-4
Loss in iteration 131 : 2.7966191359382354E-4
Loss in iteration 132 : 2.698931409665628E-4
Loss in iteration 133 : 2.606854148972208E-4
Loss in iteration 134 : 2.517440993221095E-4
Testing accuracy  of updater 2 on alg 1 with rate 1.0 = 0.9946666666666667, training accuracy 1.0, time elapsed: 3123 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4189035227147329
Loss in iteration 3 : 0.1695518094787381
Loss in iteration 4 : 0.17310849327913966
Loss in iteration 5 : 0.11417672869952394
Loss in iteration 6 : 0.11344882619914835
Loss in iteration 7 : 0.10688132001916852
Loss in iteration 8 : 0.0998138920698565
Loss in iteration 9 : 0.0953781284214387
Loss in iteration 10 : 0.09102005077331048
Loss in iteration 11 : 0.08574518594881975
Loss in iteration 12 : 0.07979247027632318
Loss in iteration 13 : 0.0736138441741389
Loss in iteration 14 : 0.06755256218265578
Loss in iteration 15 : 0.061650178394292
Loss in iteration 16 : 0.05608468961275542
Loss in iteration 17 : 0.051113530885148806
Loss in iteration 18 : 0.046961721560036196
Loss in iteration 19 : 0.04362603442698725
Loss in iteration 20 : 0.04076605748570474
Loss in iteration 21 : 0.03763347277827726
Loss in iteration 22 : 0.03447577091703988
Loss in iteration 23 : 0.03151629306566758
Loss in iteration 24 : 0.028917421116811802
Loss in iteration 25 : 0.026389661283217354
Loss in iteration 26 : 0.023867098997730516
Loss in iteration 27 : 0.02161759844618401
Loss in iteration 28 : 0.01986477331837397
Loss in iteration 29 : 0.018354924835748205
Loss in iteration 30 : 0.016996537883852105
Loss in iteration 31 : 0.01580477075297635
Loss in iteration 32 : 0.014699226286746598
Loss in iteration 33 : 0.013779641349099635
Loss in iteration 34 : 0.012960400295420643
Loss in iteration 35 : 0.012182167632361728
Loss in iteration 36 : 0.01146027476270061
Loss in iteration 37 : 0.01089709239641285
Loss in iteration 38 : 0.010396988491452255
Loss in iteration 39 : 0.009913908126384515
Loss in iteration 40 : 0.009519070197363618
Loss in iteration 41 : 0.009185272471653762
Loss in iteration 42 : 0.008869787309178184
Loss in iteration 43 : 0.008626442612746843
Loss in iteration 44 : 0.008410588256581517
Loss in iteration 45 : 0.00822166754757774
Loss in iteration 46 : 0.008040485074208265
Loss in iteration 47 : 0.007842287307330224
Loss in iteration 48 : 0.007665734026919193
Loss in iteration 49 : 0.007493097134675829
Loss in iteration 50 : 0.007316204581149231
Loss in iteration 51 : 0.007137378030264789
Loss in iteration 52 : 0.00695852478635227
Loss in iteration 53 : 0.006788088618555935
Loss in iteration 54 : 0.006615055355962158
Loss in iteration 55 : 0.006445682532332648
Loss in iteration 56 : 0.00628037452075249
Loss in iteration 57 : 0.006117776628299055
Loss in iteration 58 : 0.005959502730693726
Loss in iteration 59 : 0.005800365008638778
Loss in iteration 60 : 0.005637941730117967
Loss in iteration 61 : 0.005504523023882619
Loss in iteration 62 : 0.005383382391797267
Loss in iteration 63 : 0.005275361931095862
Loss in iteration 64 : 0.005172172015647809
Loss in iteration 65 : 0.005072616096639487
Loss in iteration 66 : 0.004974548733840279
Loss in iteration 67 : 0.004877036243863717
Loss in iteration 68 : 0.004779978127850786
Loss in iteration 69 : 0.0046838194333687425
Loss in iteration 70 : 0.0045938447122480545
Loss in iteration 71 : 0.0045112038755311746
Loss in iteration 72 : 0.0044303148144314314
Loss in iteration 73 : 0.004350687461827292
Loss in iteration 74 : 0.004271197258917004
Loss in iteration 75 : 0.004199794404646457
Loss in iteration 76 : 0.004127506829937242
Loss in iteration 77 : 0.004055652458926161
Loss in iteration 78 : 0.003988019344650607
Loss in iteration 79 : 0.003922919318286402
Loss in iteration 80 : 0.003856397772129979
Loss in iteration 81 : 0.0037895966404362194
Loss in iteration 82 : 0.0037231947389869146
Loss in iteration 83 : 0.0036579682717577605
Loss in iteration 84 : 0.003592751384739463
Loss in iteration 85 : 0.00352842047389476
Loss in iteration 86 : 0.003463357911581847
Loss in iteration 87 : 0.00339831610218516
Loss in iteration 88 : 0.0033343970101398267
Loss in iteration 89 : 0.003268957102451344
Loss in iteration 90 : 0.003204254714377942
Loss in iteration 91 : 0.003140012800181414
Loss in iteration 92 : 0.003075599578103578
Loss in iteration 93 : 0.003011762635026672
Loss in iteration 94 : 0.0029525362278456095
Loss in iteration 95 : 0.002892017413160733
Loss in iteration 96 : 0.0028330380935982415
Loss in iteration 97 : 0.002777496161566624
Loss in iteration 98 : 0.0027194180036260733
Loss in iteration 99 : 0.0026603310794174133
Loss in iteration 100 : 0.0026025338617822183
Loss in iteration 101 : 0.0025456340148159503
Loss in iteration 102 : 0.002489286402999436
Loss in iteration 103 : 0.002430181182133068
Loss in iteration 104 : 0.0023727852622235004
Loss in iteration 105 : 0.0023164199477367494
Loss in iteration 106 : 0.002257760736534364
Loss in iteration 107 : 0.0021993337247080413
Loss in iteration 108 : 0.002141063947935867
Loss in iteration 109 : 0.002084242348229811
Loss in iteration 110 : 0.002030234738846927
Loss in iteration 111 : 0.001969006025110358
Loss in iteration 112 : 0.0019173095596519496
Loss in iteration 113 : 0.0018614777946560697
Loss in iteration 114 : 0.0017984046204336207
Loss in iteration 115 : 0.001747077020312572
Loss in iteration 116 : 0.0016971265290974197
Loss in iteration 117 : 0.0016368452308546566
Loss in iteration 118 : 0.0015861025834519554
Loss in iteration 119 : 0.0015345865436523656
Loss in iteration 120 : 0.00147789555382189
Loss in iteration 121 : 0.001423729806940158
Loss in iteration 122 : 0.0013693136431155788
Loss in iteration 123 : 0.0013181400428095937
Loss in iteration 124 : 0.0012698139264451908
Loss in iteration 125 : 0.0012195213739737312
Loss in iteration 126 : 0.0011678733556729963
Loss in iteration 127 : 0.0011164284655354642
Loss in iteration 128 : 0.0010750611471603143
Loss in iteration 129 : 0.0010093733955671364
Loss in iteration 130 : 9.566899450129304E-4
Loss in iteration 131 : 9.033151048549335E-4
Loss in iteration 132 : 8.569422574079822E-4
Loss in iteration 133 : 8.10712888271323E-4
Loss in iteration 134 : 7.754515271154505E-4
Loss in iteration 135 : 7.148069442407055E-4
Loss in iteration 136 : 6.79865547710708E-4
Loss in iteration 137 : 6.47649909462417E-4
Loss in iteration 138 : 6.131248403787506E-4
Loss in iteration 139 : 5.685486542083539E-4
Loss in iteration 140 : 5.348982514060228E-4
Loss in iteration 141 : 5.101409905127356E-4
Loss in iteration 142 : 4.823694487121064E-4
Loss in iteration 143 : 4.5921211908275404E-4
Loss in iteration 144 : 4.361009550420101E-4
Loss in iteration 145 : 4.125774540580071E-4
Loss in iteration 146 : 3.942979525466942E-4
Loss in iteration 147 : 3.7814886471305704E-4
Loss in iteration 148 : 3.6522756138865364E-4
Loss in iteration 149 : 3.542695831509937E-4
Loss in iteration 150 : 3.4460719745140725E-4
Loss in iteration 151 : 3.360545489146725E-4
Loss in iteration 152 : 3.243856719782247E-4
Loss in iteration 153 : 3.1421780373093276E-4
Loss in iteration 154 : 3.0591195501474005E-4
Testing accuracy  of updater 2 on alg 1 with rate 0.7 = 0.9964444444444445, training accuracy 1.0, time elapsed: 3436 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.37581481533816763
Loss in iteration 3 : 0.7500414899092142
Loss in iteration 4 : 0.37484870069846427
Loss in iteration 5 : 0.36012615738820897
Loss in iteration 6 : 0.2952235611588204
Loss in iteration 7 : 0.1861601206335522
Loss in iteration 8 : 0.11544500539704114
Loss in iteration 9 : 0.12378450077890156
Loss in iteration 10 : 0.13218580042858566
Loss in iteration 11 : 0.12188215733684864
Loss in iteration 12 : 0.11168281450445827
Loss in iteration 13 : 0.10335982127411406
Loss in iteration 14 : 0.09761617788136988
Loss in iteration 15 : 0.09349841295088838
Loss in iteration 16 : 0.08955708097071603
Loss in iteration 17 : 0.08550010595452671
Loss in iteration 18 : 0.08126799193574288
Loss in iteration 19 : 0.07663884161678422
Loss in iteration 20 : 0.071825940022867
Loss in iteration 21 : 0.06711288979990726
Loss in iteration 22 : 0.0627071917816569
Loss in iteration 23 : 0.05895615878748631
Loss in iteration 24 : 0.05585401957858701
Loss in iteration 25 : 0.05302015443070649
Loss in iteration 26 : 0.05053243869809699
Loss in iteration 27 : 0.04805407006852044
Loss in iteration 28 : 0.04571903427818879
Loss in iteration 29 : 0.043394504072232554
Loss in iteration 30 : 0.04118706429002005
Loss in iteration 31 : 0.03900276449856915
Loss in iteration 32 : 0.036824155096122003
Loss in iteration 33 : 0.03483249642798946
Loss in iteration 34 : 0.032893825309060286
Loss in iteration 35 : 0.031043412609081892
Loss in iteration 36 : 0.02925275715584438
Loss in iteration 37 : 0.027520067000339026
Loss in iteration 38 : 0.025859060849333846
Loss in iteration 39 : 0.02429446524223971
Loss in iteration 40 : 0.02283573281497788
Loss in iteration 41 : 0.02151905562615398
Loss in iteration 42 : 0.020364079473610883
Loss in iteration 43 : 0.01936528991963411
Loss in iteration 44 : 0.018474675781047042
Loss in iteration 45 : 0.01763003363997056
Loss in iteration 46 : 0.016808652148564127
Loss in iteration 47 : 0.016025474768016173
Loss in iteration 48 : 0.015268941287170726
Loss in iteration 49 : 0.014610599265288701
Loss in iteration 50 : 0.014045359097078181
Loss in iteration 51 : 0.013518680881384998
Loss in iteration 52 : 0.01300368889691056
Loss in iteration 53 : 0.012524332576723413
Loss in iteration 54 : 0.012102338516149749
Loss in iteration 55 : 0.01172110025042693
Loss in iteration 56 : 0.011363889144231601
Loss in iteration 57 : 0.01100855930907271
Loss in iteration 58 : 0.010661324653432839
Loss in iteration 59 : 0.010344483314976748
Loss in iteration 60 : 0.010053777509861056
Loss in iteration 61 : 0.0097973853042579
Loss in iteration 62 : 0.00955289217293673
Loss in iteration 63 : 0.009326418906334176
Loss in iteration 64 : 0.009110010903891533
Loss in iteration 65 : 0.008921397022240238
Loss in iteration 66 : 0.008734468791741758
Loss in iteration 67 : 0.008550475824669971
Loss in iteration 68 : 0.008366222869267454
Loss in iteration 69 : 0.008186891349305758
Loss in iteration 70 : 0.008027505630487626
Loss in iteration 71 : 0.007876588715087833
Loss in iteration 72 : 0.007729101440262741
Loss in iteration 73 : 0.007586505382019854
Loss in iteration 74 : 0.0074521439308602505
Loss in iteration 75 : 0.007322110407235885
Loss in iteration 76 : 0.0071917418336848084
Loss in iteration 77 : 0.007071501718961553
Loss in iteration 78 : 0.006950130160945445
Loss in iteration 79 : 0.006832347958225163
Loss in iteration 80 : 0.00672024917716937
Loss in iteration 81 : 0.00661239180196292
Loss in iteration 82 : 0.00650491602815708
Loss in iteration 83 : 0.006405949189834839
Loss in iteration 84 : 0.006310071888921644
Loss in iteration 85 : 0.006213998323262368
Loss in iteration 86 : 0.006118474579677121
Loss in iteration 87 : 0.006024824866916953
Loss in iteration 88 : 0.005931738368926793
Loss in iteration 89 : 0.005838822597846573
Loss in iteration 90 : 0.0057457971141089
Loss in iteration 91 : 0.005653308278265097
Loss in iteration 92 : 0.005563339659426398
Loss in iteration 93 : 0.005473572583338258
Loss in iteration 94 : 0.005384826284482885
Loss in iteration 95 : 0.005304880830442083
Loss in iteration 96 : 0.005227537892127151
Loss in iteration 97 : 0.005151677616008151
Loss in iteration 98 : 0.0050763313245404725
Loss in iteration 99 : 0.005006262298851805
Loss in iteration 100 : 0.004937877122072031
Loss in iteration 101 : 0.004869909843923437
Loss in iteration 102 : 0.004806767131866454
Loss in iteration 103 : 0.004742645517468158
Loss in iteration 104 : 0.00468338247995734
Loss in iteration 105 : 0.004624532929534976
Loss in iteration 106 : 0.004566968278640014
Loss in iteration 107 : 0.00451262868328788
Loss in iteration 108 : 0.004458324327397303
Loss in iteration 109 : 0.004405095581406853
Loss in iteration 110 : 0.004351643085501154
Loss in iteration 111 : 0.004299074536592544
Loss in iteration 112 : 0.00424687217472927
Loss in iteration 113 : 0.004195620787432446
Loss in iteration 114 : 0.004144562278531188
Loss in iteration 115 : 0.004096040154346802
Loss in iteration 116 : 0.004047644963447871
Loss in iteration 117 : 0.003998224716441951
Loss in iteration 118 : 0.003949461840862264
Loss in iteration 119 : 0.0039012140791443916
Loss in iteration 120 : 0.003855617839079735
Loss in iteration 121 : 0.003810095233474531
Loss in iteration 122 : 0.003765792818924349
Loss in iteration 123 : 0.003720583950441642
Loss in iteration 124 : 0.0036760471679879512
Loss in iteration 125 : 0.003632231325099128
Loss in iteration 126 : 0.003591778234569274
Loss in iteration 127 : 0.0035506744234132543
Loss in iteration 128 : 0.0035119231684546857
Loss in iteration 129 : 0.003472228848962103
Loss in iteration 130 : 0.0034336411149388907
Loss in iteration 131 : 0.0033958017804312643
Loss in iteration 132 : 0.0033580905048041054
Loss in iteration 133 : 0.003321327626426129
Loss in iteration 134 : 0.003283985959064268
Loss in iteration 135 : 0.003247115840167564
Loss in iteration 136 : 0.003211129015989898
Loss in iteration 137 : 0.0031745657581693414
Loss in iteration 138 : 0.0031405393564812923
Loss in iteration 139 : 0.0031030091433128393
Loss in iteration 140 : 0.0030683383370107057
Loss in iteration 141 : 0.0030321791900007073
Loss in iteration 142 : 0.0029959579911287243
Loss in iteration 143 : 0.0029600221668966416
Loss in iteration 144 : 0.002924395620966709
Loss in iteration 145 : 0.0028896723743388484
Loss in iteration 146 : 0.002854131077743357
Loss in iteration 147 : 0.0028192690691317707
Loss in iteration 148 : 0.0027827986926343045
Loss in iteration 149 : 0.0027479357845358243
Loss in iteration 150 : 0.0027127682752791597
Loss in iteration 151 : 0.0026768979478141324
Loss in iteration 152 : 0.002642657630484464
Loss in iteration 153 : 0.002606644302462925
Loss in iteration 154 : 0.002572103497828894
Loss in iteration 155 : 0.0025365118323469123
Loss in iteration 156 : 0.0025009862544141206
Loss in iteration 157 : 0.002467130683962761
Loss in iteration 158 : 0.0024310361580187735
Loss in iteration 159 : 0.0023969625885088596
Loss in iteration 160 : 0.0023616645315242923
Loss in iteration 161 : 0.002325725023317113
Loss in iteration 162 : 0.0022911436177557925
Loss in iteration 163 : 0.0022568009453069458
Loss in iteration 164 : 0.0022207916675964076
Loss in iteration 165 : 0.0021867556950492806
Loss in iteration 166 : 0.0021504123766550886
Loss in iteration 167 : 0.0021165259710823393
Loss in iteration 168 : 0.0020810491709262312
Loss in iteration 169 : 0.002047599371428761
Loss in iteration 170 : 0.0020138321273749813
Loss in iteration 171 : 0.0019817657224141284
Loss in iteration 172 : 0.0019485339918634305
Loss in iteration 173 : 0.0019142096302221848
Loss in iteration 174 : 0.0018797524148651784
Loss in iteration 175 : 0.0018465598945073293
Loss in iteration 176 : 0.001815467405635008
Loss in iteration 177 : 0.001779470137390401
Loss in iteration 178 : 0.001747363828750157
Loss in iteration 179 : 0.0017162467383255288
Loss in iteration 180 : 0.0016812426036949576
Loss in iteration 181 : 0.001647358391143672
Loss in iteration 182 : 0.00161463993091363
Loss in iteration 183 : 0.0015826176568476968
Loss in iteration 184 : 0.0015465402254703838
Loss in iteration 185 : 0.0015152558381998362
Loss in iteration 186 : 0.001481396379347027
Loss in iteration 187 : 0.0014486309047693781
Loss in iteration 188 : 0.0014150091832410019
Loss in iteration 189 : 0.0013822931291760977
Loss in iteration 190 : 0.001350170334834379
Loss in iteration 191 : 0.0013165000438489945
Loss in iteration 192 : 0.0012866988729338608
Loss in iteration 193 : 0.0012533325164025512
Loss in iteration 194 : 0.0012231489610936208
Loss in iteration 195 : 0.0011870263387139472
Loss in iteration 196 : 0.0011532979850186703
Loss in iteration 197 : 0.0011203121356684043
Loss in iteration 198 : 0.001087688264328616
Loss in iteration 199 : 0.00105498924209908
Loss in iteration 200 : 0.0010254558251185334
Testing accuracy  of updater 2 on alg 1 with rate 0.4 = 0.9946666666666667, training accuracy 1.0, time elapsed: 4243 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7420867379656345
Loss in iteration 3 : 0.44006208457387214
Loss in iteration 4 : 0.24176183539681162
Loss in iteration 5 : 0.20460925585871484
Loss in iteration 6 : 0.18553376078967496
Loss in iteration 7 : 0.1683459971488996
Loss in iteration 8 : 0.15023220885599314
Loss in iteration 9 : 0.1353935179368286
Loss in iteration 10 : 0.12385205717462919
Loss in iteration 11 : 0.11444974270526909
Loss in iteration 12 : 0.10696352770835756
Loss in iteration 13 : 0.10136303711364358
Loss in iteration 14 : 0.09714987330670392
Loss in iteration 15 : 0.0937821165273916
Loss in iteration 16 : 0.09076389340477078
Loss in iteration 17 : 0.08791025724222028
Loss in iteration 18 : 0.08514303083368804
Loss in iteration 19 : 0.08242375098676129
Loss in iteration 20 : 0.07974662373756054
Loss in iteration 21 : 0.07703343436981673
Loss in iteration 22 : 0.07438289938880246
Loss in iteration 23 : 0.07177822437127024
Loss in iteration 24 : 0.06928014057022099
Loss in iteration 25 : 0.06688850350699065
Loss in iteration 26 : 0.0646010566175226
Loss in iteration 27 : 0.06244257283184505
Loss in iteration 28 : 0.06040104512847664
Loss in iteration 29 : 0.05849726026777412
Loss in iteration 30 : 0.05669192132245924
Loss in iteration 31 : 0.05501865499783076
Loss in iteration 32 : 0.05352812242405151
Loss in iteration 33 : 0.052146259176314824
Loss in iteration 34 : 0.050884215806385184
Loss in iteration 35 : 0.049754620747272035
Loss in iteration 36 : 0.04868583809050654
Loss in iteration 37 : 0.047664555081910524
Loss in iteration 38 : 0.046689869827719416
Loss in iteration 39 : 0.0457510346001822
Loss in iteration 40 : 0.044817593655442425
Loss in iteration 41 : 0.043897863547115064
Loss in iteration 42 : 0.0429873291765169
Loss in iteration 43 : 0.04207679976961035
Loss in iteration 44 : 0.04116504458984792
Loss in iteration 45 : 0.04024802211692122
Loss in iteration 46 : 0.03932830878900816
Loss in iteration 47 : 0.03842515621796039
Loss in iteration 48 : 0.03752239854584722
Loss in iteration 49 : 0.03661572165823602
Loss in iteration 50 : 0.03571123525404955
Loss in iteration 51 : 0.03483184286717081
Loss in iteration 52 : 0.03398385966501436
Loss in iteration 53 : 0.033153289541343195
Loss in iteration 54 : 0.03233744710412786
Loss in iteration 55 : 0.0315338122295615
Loss in iteration 56 : 0.03074630107635769
Loss in iteration 57 : 0.029986855535886474
Loss in iteration 58 : 0.029255607842706307
Loss in iteration 59 : 0.028549365871837793
Loss in iteration 60 : 0.027863277721287855
Loss in iteration 61 : 0.027204369899271778
Loss in iteration 62 : 0.026563533737114692
Loss in iteration 63 : 0.02594294223011263
Loss in iteration 64 : 0.025339423817171587
Loss in iteration 65 : 0.02474205256593059
Loss in iteration 66 : 0.024155757284313486
Loss in iteration 67 : 0.023578785478530012
Loss in iteration 68 : 0.02301038819846323
Loss in iteration 69 : 0.022447703890063325
Loss in iteration 70 : 0.021892914974108564
Loss in iteration 71 : 0.021348885528045917
Loss in iteration 72 : 0.02081177609964266
Loss in iteration 73 : 0.020278693945208124
Loss in iteration 74 : 0.019747959690005045
Loss in iteration 75 : 0.01921973450788083
Loss in iteration 76 : 0.018706806040295048
Loss in iteration 77 : 0.01821401018584906
Loss in iteration 78 : 0.017732239167726288
Loss in iteration 79 : 0.01727977077294704
Loss in iteration 80 : 0.016845869617139506
Loss in iteration 81 : 0.016440071184087933
Loss in iteration 82 : 0.016077068146492464
Loss in iteration 83 : 0.015728266561095463
Loss in iteration 84 : 0.01538704818924704
Loss in iteration 85 : 0.015088149177054689
Loss in iteration 86 : 0.01481582091675188
Loss in iteration 87 : 0.014547223697004744
Loss in iteration 88 : 0.014282903537456744
Loss in iteration 89 : 0.014038022590740183
Loss in iteration 90 : 0.013808833991862996
Loss in iteration 91 : 0.013583031184528055
Loss in iteration 92 : 0.013361764132173482
Loss in iteration 93 : 0.013150296002282192
Loss in iteration 94 : 0.01294036910101598
Loss in iteration 95 : 0.012731132633484344
Loss in iteration 96 : 0.012525510891616498
Loss in iteration 97 : 0.012327737085048487
Loss in iteration 98 : 0.01213558225747541
Loss in iteration 99 : 0.011948359238001515
Loss in iteration 100 : 0.011769782770946475
Loss in iteration 101 : 0.011594291761991947
Loss in iteration 102 : 0.011425187337923547
Loss in iteration 103 : 0.011258603548512885
Loss in iteration 104 : 0.011092711872909439
Loss in iteration 105 : 0.010932187431326155
Loss in iteration 106 : 0.010776465172202881
Loss in iteration 107 : 0.010623670700806578
Loss in iteration 108 : 0.010474745065116841
Loss in iteration 109 : 0.010330406525084735
Loss in iteration 110 : 0.010192111581647036
Loss in iteration 111 : 0.010055881881735777
Loss in iteration 112 : 0.009921321118068647
Loss in iteration 113 : 0.009787662145323032
Loss in iteration 114 : 0.009655794577904492
Loss in iteration 115 : 0.009526930432206263
Loss in iteration 116 : 0.00940105989512161
Loss in iteration 117 : 0.009276936590051662
Loss in iteration 118 : 0.00915597942527984
Loss in iteration 119 : 0.009035571075734156
Loss in iteration 120 : 0.008918015175029171
Loss in iteration 121 : 0.008803983653145471
Loss in iteration 122 : 0.008691898968200178
Loss in iteration 123 : 0.008581225304450796
Loss in iteration 124 : 0.00847070256591948
Loss in iteration 125 : 0.008360315660084423
Loss in iteration 126 : 0.008250051003676004
Loss in iteration 127 : 0.008140323077352652
Loss in iteration 128 : 0.008035818500329622
Loss in iteration 129 : 0.00793582493761191
Loss in iteration 130 : 0.007839504944739699
Loss in iteration 131 : 0.007745143811216538
Loss in iteration 132 : 0.007654988117649742
Loss in iteration 133 : 0.007566288600224412
Loss in iteration 134 : 0.007479891303340693
Loss in iteration 135 : 0.007394933993447391
Loss in iteration 136 : 0.00731369453941737
Loss in iteration 137 : 0.007237453481278518
Loss in iteration 138 : 0.007164530501836118
Loss in iteration 139 : 0.007094091583299027
Loss in iteration 140 : 0.0070252583171746515
Loss in iteration 141 : 0.006957577651278868
Loss in iteration 142 : 0.0068901507783960675
Loss in iteration 143 : 0.006823429297063528
Loss in iteration 144 : 0.006757884487360597
Loss in iteration 145 : 0.006692576263203461
Loss in iteration 146 : 0.006627480966037546
Loss in iteration 147 : 0.006562777010742309
Loss in iteration 148 : 0.00650013240707528
Loss in iteration 149 : 0.006439261040545899
Loss in iteration 150 : 0.0063810029929073885
Loss in iteration 151 : 0.006324345405115342
Loss in iteration 152 : 0.006268498126858927
Loss in iteration 153 : 0.006213575558480577
Loss in iteration 154 : 0.006159741848350505
Loss in iteration 155 : 0.006107384257607432
Loss in iteration 156 : 0.006055564788048639
Loss in iteration 157 : 0.006005161948109247
Loss in iteration 158 : 0.005956036541854817
Loss in iteration 159 : 0.005907171841361712
Loss in iteration 160 : 0.005860444132471016
Loss in iteration 161 : 0.005813883590267062
Loss in iteration 162 : 0.0057676197297619036
Loss in iteration 163 : 0.005722743650354858
Loss in iteration 164 : 0.005677559546042563
Loss in iteration 165 : 0.005633832119758809
Loss in iteration 166 : 0.0055906021168324925
Loss in iteration 167 : 0.0055476661645549544
Loss in iteration 168 : 0.005505376956037937
Loss in iteration 169 : 0.0054626031335020635
Loss in iteration 170 : 0.005422352738587808
Loss in iteration 171 : 0.0053850267778344845
Loss in iteration 172 : 0.005347955051452127
Loss in iteration 173 : 0.005310584017952668
Loss in iteration 174 : 0.005275727944837336
Loss in iteration 175 : 0.005241144146324779
Loss in iteration 176 : 0.005207401672440702
Loss in iteration 177 : 0.0051750079133056105
Loss in iteration 178 : 0.005143055277673086
Loss in iteration 179 : 0.005111536281946084
Loss in iteration 180 : 0.005081493798206201
Loss in iteration 181 : 0.005052445331403622
Loss in iteration 182 : 0.005021873960091208
Loss in iteration 183 : 0.004992507257943416
Loss in iteration 184 : 0.004963646438199025
Loss in iteration 185 : 0.0049349317883335695
Loss in iteration 186 : 0.004906391228021577
Loss in iteration 187 : 0.00487807721093226
Loss in iteration 188 : 0.00485133535326685
Loss in iteration 189 : 0.004824243493034818
Loss in iteration 190 : 0.004797979693370201
Loss in iteration 191 : 0.004770962964576447
Loss in iteration 192 : 0.004745063795014154
Loss in iteration 193 : 0.004718935664636257
Loss in iteration 194 : 0.004693278537205697
Loss in iteration 195 : 0.004667659501282333
Loss in iteration 196 : 0.00464224665865296
Loss in iteration 197 : 0.004616955427095248
Loss in iteration 198 : 0.004591762913998534
Loss in iteration 199 : 0.0045665611032019255
Loss in iteration 200 : 0.004541768277653227
Testing accuracy  of updater 2 on alg 1 with rate 0.09999999999999998 = 0.9902222222222222, training accuracy 0.9987139182623607, time elapsed: 4230 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 12.19132066449727
Loss in iteration 3 : 1.524238975627231
Loss in iteration 4 : 0.8521449497021388
Loss in iteration 5 : 0.6222899706240723
Loss in iteration 6 : 0.2886561036142424
Loss in iteration 7 : 0.22040147692301784
Loss in iteration 8 : 0.17882326777300225
Loss in iteration 9 : 0.14770898226238685
Loss in iteration 10 : 0.12106209099250766
Loss in iteration 11 : 0.1013557589079036
Loss in iteration 12 : 0.08739482344860752
Loss in iteration 13 : 0.0771818228426325
Loss in iteration 14 : 0.06896821348951519
Loss in iteration 15 : 0.0619892414579938
Loss in iteration 16 : 0.056137692450405174
Loss in iteration 17 : 0.05158142141808706
Loss in iteration 18 : 0.04786620853054855
Loss in iteration 19 : 0.04490797851509921
Loss in iteration 20 : 0.04204480720392817
Loss in iteration 21 : 0.03963914508965118
Loss in iteration 22 : 0.0378447012653273
Loss in iteration 23 : 0.03624968510769803
Loss in iteration 24 : 0.034886020197377784
Loss in iteration 25 : 0.033527510257240774
Loss in iteration 26 : 0.03237511148393116
Loss in iteration 27 : 0.03143009821916349
Loss in iteration 28 : 0.030485187438791278
Loss in iteration 29 : 0.02954748569246493
Loss in iteration 30 : 0.028647984210952136
Loss in iteration 31 : 0.027818057166636413
Loss in iteration 32 : 0.026993366130226536
Loss in iteration 33 : 0.026162204180343457
Loss in iteration 34 : 0.025339037427231956
Loss in iteration 35 : 0.024514469569303535
Loss in iteration 36 : 0.02369539410488537
Loss in iteration 37 : 0.02285937595146411
Loss in iteration 38 : 0.02216279147075145
Loss in iteration 39 : 0.021579482149172286
Loss in iteration 40 : 0.021045200803849067
Loss in iteration 41 : 0.0205188433595718
Loss in iteration 42 : 0.019984764736762844
Loss in iteration 43 : 0.019479271399588174
Loss in iteration 44 : 0.01901742196203564
Loss in iteration 45 : 0.018596036056485184
Loss in iteration 46 : 0.018164301380707706
Loss in iteration 47 : 0.01788437028687145
Loss in iteration 48 : 0.01769156908587116
Loss in iteration 49 : 0.017338986258309836
Loss in iteration 50 : 0.01711760427117643
Loss in iteration 51 : 0.01688870862550375
Loss in iteration 52 : 0.01664932116178235
Loss in iteration 53 : 0.016450590675771405
Loss in iteration 54 : 0.01625322650741447
Loss in iteration 55 : 0.016036529945048558
Loss in iteration 56 : 0.015851383053329008
Loss in iteration 57 : 0.015697971186026775
Loss in iteration 58 : 0.015445534655430576
Loss in iteration 59 : 0.015246398935837312
Loss in iteration 60 : 0.01504092252078925
Loss in iteration 61 : 0.014834663009362169
Loss in iteration 62 : 0.014637711777123143
Loss in iteration 63 : 0.014453538246103334
Loss in iteration 64 : 0.014244143542348466
Loss in iteration 65 : 0.01403464674175117
Loss in iteration 66 : 0.013825245632413717
Loss in iteration 67 : 0.013639670054830362
Loss in iteration 68 : 0.013454895146456263
Loss in iteration 69 : 0.013238970462551713
Loss in iteration 70 : 0.013029505301726804
Loss in iteration 71 : 0.01282818262982046
Loss in iteration 72 : 0.012625399660541353
Loss in iteration 73 : 0.012430928454296527
Loss in iteration 74 : 0.012241648441249179
Loss in iteration 75 : 0.012038450987240145
Loss in iteration 76 : 0.011830520009540317
Loss in iteration 77 : 0.011626915628131024
Loss in iteration 78 : 0.01142613386205341
Loss in iteration 79 : 0.01121672549250357
Loss in iteration 80 : 0.01102838600234597
Loss in iteration 81 : 0.010847153673754866
Loss in iteration 82 : 0.01063709589967117
Loss in iteration 83 : 0.010431676037084859
Loss in iteration 84 : 0.010225648059212025
Loss in iteration 85 : 0.01002737754999571
Loss in iteration 86 : 0.009832976091760712
Loss in iteration 87 : 0.009676689469305068
Loss in iteration 88 : 0.009539396040309055
Loss in iteration 89 : 0.009429859600705984
Loss in iteration 90 : 0.009267013776648863
Loss in iteration 91 : 0.0090591417063044
Loss in iteration 92 : 0.008899200843243771
Loss in iteration 93 : 0.008794956831240108
Loss in iteration 94 : 0.00860571255145634
Loss in iteration 95 : 0.008419324904779896
Loss in iteration 96 : 0.008260915327904687
Loss in iteration 97 : 0.008118386285166413
Loss in iteration 98 : 0.007931078023000993
Loss in iteration 99 : 0.007777060979360808
Loss in iteration 100 : 0.007636681147410402
Loss in iteration 101 : 0.00746825039203857
Loss in iteration 102 : 0.007337236514214378
Loss in iteration 103 : 0.007161671547618327
Loss in iteration 104 : 0.00699748664155477
Loss in iteration 105 : 0.00684260525423104
Loss in iteration 106 : 0.006670009010581751
Loss in iteration 107 : 0.006519247592704774
Loss in iteration 108 : 0.006395677639307291
Loss in iteration 109 : 0.00619574969797077
Loss in iteration 110 : 0.006031249474147139
Loss in iteration 111 : 0.005886435639203546
Loss in iteration 112 : 0.00572503348470805
Loss in iteration 113 : 0.005589032612915948
Loss in iteration 114 : 0.005436375626597491
Loss in iteration 115 : 0.0052773206613282685
Loss in iteration 116 : 0.005108354182488529
Loss in iteration 117 : 0.0049471427309734865
Loss in iteration 118 : 0.004777305050109453
Loss in iteration 119 : 0.004627229565962221
Loss in iteration 120 : 0.004453928406108888
Loss in iteration 121 : 0.004300726568745993
Loss in iteration 122 : 0.004189996006730959
Loss in iteration 123 : 0.004061089335782835
Loss in iteration 124 : 0.003869171263668643
Loss in iteration 125 : 0.0036730877212477263
Loss in iteration 126 : 0.0035280564250425206
Loss in iteration 127 : 0.003387003363928538
Loss in iteration 128 : 0.003216495454108717
Loss in iteration 129 : 0.0030678560486130033
Loss in iteration 130 : 0.002894162626672141
Loss in iteration 131 : 0.002723477817115519
Loss in iteration 132 : 0.002579194721451823
Loss in iteration 133 : 0.0024443486897240607
Loss in iteration 134 : 0.002252928878126019
Loss in iteration 135 : 0.0020867559356844254
Loss in iteration 136 : 0.001962299837771554
Loss in iteration 137 : 0.001834494922651696
Loss in iteration 138 : 0.001720141240655849
Loss in iteration 139 : 0.0014949011847139998
Loss in iteration 140 : 0.0013059434736961532
Loss in iteration 141 : 0.001194871922257375
Loss in iteration 142 : 0.0010398033020899925
Loss in iteration 143 : 8.500747372649835E-4
Loss in iteration 144 : 6.81761170513374E-4
Loss in iteration 145 : 5.297723320295748E-4
Testing accuracy  of updater 3 on alg 1 with rate 10.0 = 0.9928888888888889, training accuracy 0.9998571020291512, time elapsed: 3617 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 4.088777997239091
Loss in iteration 3 : 0.6723279787870523
Loss in iteration 4 : 0.4653172597266012
Loss in iteration 5 : 0.3293218043823074
Loss in iteration 6 : 0.09754130701613989
Loss in iteration 7 : 0.07199590055861063
Loss in iteration 8 : 0.058508016437621393
Loss in iteration 9 : 0.04980229101013067
Loss in iteration 10 : 0.04290242455349131
Loss in iteration 11 : 0.03740386482524471
Loss in iteration 12 : 0.03292655017707575
Loss in iteration 13 : 0.029490505742283398
Loss in iteration 14 : 0.026390195592512808
Loss in iteration 15 : 0.023584010587827782
Loss in iteration 16 : 0.021486920537752176
Loss in iteration 17 : 0.020007488250087517
Loss in iteration 18 : 0.01878979129819571
Loss in iteration 19 : 0.017669680210863747
Loss in iteration 20 : 0.016617347850184448
Loss in iteration 21 : 0.015807725363898163
Loss in iteration 22 : 0.015112410048457104
Loss in iteration 23 : 0.014477387235912333
Loss in iteration 24 : 0.013879348869348274
Loss in iteration 25 : 0.01342123096394799
Loss in iteration 26 : 0.012942039148738432
Loss in iteration 27 : 0.012474286047663648
Loss in iteration 28 : 0.012019965640601522
Loss in iteration 29 : 0.011589055905453357
Loss in iteration 30 : 0.011176295972816318
Loss in iteration 31 : 0.010767923160552124
Loss in iteration 32 : 0.010360451604341087
Loss in iteration 33 : 0.009962425802255584
Loss in iteration 34 : 0.009565639841911966
Loss in iteration 35 : 0.009195087170422686
Loss in iteration 36 : 0.008904967939664363
Loss in iteration 37 : 0.008615026043083939
Loss in iteration 38 : 0.008341180929978808
Loss in iteration 39 : 0.008079278530538019
Loss in iteration 40 : 0.007804204658928217
Loss in iteration 41 : 0.007571404757516021
Loss in iteration 42 : 0.007312940468655164
Loss in iteration 43 : 0.007105101981271018
Loss in iteration 44 : 0.006886037125079943
Loss in iteration 45 : 0.00670234084282281
Loss in iteration 46 : 0.00650513796530253
Loss in iteration 47 : 0.006321975267552328
Loss in iteration 48 : 0.006154475686499685
Loss in iteration 49 : 0.005978917640817131
Loss in iteration 50 : 0.005793283722329488
Loss in iteration 51 : 0.005640221708578891
Loss in iteration 52 : 0.0055297785777230135
Loss in iteration 53 : 0.005362280865722165
Loss in iteration 54 : 0.005155318205319393
Loss in iteration 55 : 0.004998554679772578
Loss in iteration 56 : 0.004868169824607715
Loss in iteration 57 : 0.004764834820066329
Loss in iteration 58 : 0.0046578858946052714
Loss in iteration 59 : 0.004585490931217732
Loss in iteration 60 : 0.004494592404357382
Loss in iteration 61 : 0.004360418036764738
Loss in iteration 62 : 0.004267114342423961
Loss in iteration 63 : 0.004153911038274869
Loss in iteration 64 : 0.004060666450426594
Loss in iteration 65 : 0.0039919412233157085
Loss in iteration 66 : 0.003860168458200765
Loss in iteration 67 : 0.0037614354753938064
Loss in iteration 68 : 0.003670518292210287
Loss in iteration 69 : 0.003633833304690448
Loss in iteration 70 : 0.003522925654882252
Loss in iteration 71 : 0.003390786109800111
Loss in iteration 72 : 0.003279325714946477
Loss in iteration 73 : 0.0031771050096905356
Loss in iteration 74 : 0.0030842971575798096
Loss in iteration 75 : 0.002991044496954042
Loss in iteration 76 : 0.002878446348006526
Loss in iteration 77 : 0.002777024757649744
Loss in iteration 78 : 0.0026838530110312405
Loss in iteration 79 : 0.0025880017688916022
Loss in iteration 80 : 0.0024927502058379146
Loss in iteration 81 : 0.0024181625829019318
Loss in iteration 82 : 0.00232152253146419
Loss in iteration 83 : 0.0022444028320468866
Loss in iteration 84 : 0.0021092105805363107
Loss in iteration 85 : 0.0020186087507939087
Loss in iteration 86 : 0.001914259655136977
Loss in iteration 87 : 0.0018159529756376512
Loss in iteration 88 : 0.0017371443635345704
Loss in iteration 89 : 0.001643765313399531
Loss in iteration 90 : 0.0015571244538876584
Loss in iteration 91 : 0.0014480080495251652
Loss in iteration 92 : 0.0013674031419540122
Loss in iteration 93 : 0.0012863692441914783
Loss in iteration 94 : 0.0012090022078823208
Loss in iteration 95 : 0.001119453331452077
Loss in iteration 96 : 0.0010383114616712094
Loss in iteration 97 : 9.65312979029091E-4
Loss in iteration 98 : 8.72821906317536E-4
Loss in iteration 99 : 7.902684503671994E-4
Loss in iteration 100 : 7.233723647864582E-4
Loss in iteration 101 : 6.637205750470233E-4
Loss in iteration 102 : 6.098890751968409E-4
Loss in iteration 103 : 5.481513934795662E-4
Loss in iteration 104 : 4.854241350465515E-4
Loss in iteration 105 : 4.398549497893653E-4
Loss in iteration 106 : 3.9484740722104436E-4
Loss in iteration 107 : 3.5943078266111294E-4
Loss in iteration 108 : 3.3668536911972137E-4
Loss in iteration 109 : 2.788444322801356E-4
Loss in iteration 110 : 2.2736439497049652E-4
Loss in iteration 111 : 1.9849260952719527E-4
Loss in iteration 112 : 1.5972806971356265E-4
Loss in iteration 113 : 9.590019076217769E-5
Loss in iteration 114 : 9.865933251386402E-5
Loss in iteration 115 : 5.637125689958593E-5
Testing accuracy  of updater 3 on alg 1 with rate 7.0 = 0.9946666666666667, training accuracy 1.0, time elapsed: 3263 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.9364980604656086
Loss in iteration 3 : 0.4112229410697021
Loss in iteration 4 : 0.5551350144092774
Loss in iteration 5 : 0.43224696035326166
Loss in iteration 6 : 0.3759272627779347
Loss in iteration 7 : 0.18021192511305015
Loss in iteration 8 : 0.04898443249088427
Loss in iteration 9 : 0.03729860836691136
Loss in iteration 10 : 0.03193093102686213
Loss in iteration 11 : 0.0270634185755129
Loss in iteration 12 : 0.02366834882259083
Loss in iteration 13 : 0.02080834825500384
Loss in iteration 14 : 0.018099628333001896
Loss in iteration 15 : 0.01581871049458421
Loss in iteration 16 : 0.014160042613654703
Loss in iteration 17 : 0.013071098525414446
Loss in iteration 18 : 0.012097024889857907
Loss in iteration 19 : 0.011213108738362771
Loss in iteration 20 : 0.010419793178238509
Loss in iteration 21 : 0.009736263930196182
Loss in iteration 22 : 0.009216700358275251
Loss in iteration 23 : 0.008729601170126092
Loss in iteration 24 : 0.008358085367110558
Loss in iteration 25 : 0.007993070552968645
Loss in iteration 26 : 0.007636079186682303
Loss in iteration 27 : 0.007311994948000145
Loss in iteration 28 : 0.007018754953694832
Loss in iteration 29 : 0.006738582736290469
Loss in iteration 30 : 0.006489156171611568
Loss in iteration 31 : 0.0062606462892588035
Loss in iteration 32 : 0.006049586266784427
Loss in iteration 33 : 0.005854900942962714
Loss in iteration 34 : 0.005667056991184873
Loss in iteration 35 : 0.005482163759782094
Loss in iteration 36 : 0.005294432493056853
Loss in iteration 37 : 0.005105963114378193
Loss in iteration 38 : 0.004921829407864409
Loss in iteration 39 : 0.0047604113460288444
Loss in iteration 40 : 0.004590647667537982
Loss in iteration 41 : 0.004442689411394795
Loss in iteration 42 : 0.004299251092975425
Loss in iteration 43 : 0.004164502669449687
Loss in iteration 44 : 0.00403443564122914
Loss in iteration 45 : 0.0039017551087418474
Loss in iteration 46 : 0.003770959265159283
Loss in iteration 47 : 0.003669061459794749
Loss in iteration 48 : 0.0035686728876538703
Loss in iteration 49 : 0.003473756918100839
Loss in iteration 50 : 0.0033797811836162326
Loss in iteration 51 : 0.0032872261219948867
Loss in iteration 52 : 0.0031942746142876685
Loss in iteration 53 : 0.0031212516788964705
Loss in iteration 54 : 0.0030504081394461904
Loss in iteration 55 : 0.0029530217982175256
Loss in iteration 56 : 0.002871966552680989
Loss in iteration 57 : 0.0027935343656788542
Loss in iteration 58 : 0.0027220770485255094
Loss in iteration 59 : 0.0026706356428969754
Loss in iteration 60 : 0.0026205154715636145
Loss in iteration 61 : 0.002576274211196917
Loss in iteration 62 : 0.0025328903684097904
Loss in iteration 63 : 0.002480763808296802
Loss in iteration 64 : 0.002444107856241807
Loss in iteration 65 : 0.0023679792913067167
Loss in iteration 66 : 0.0023281110670600484
Loss in iteration 67 : 0.0023155374226839372
Loss in iteration 68 : 0.002276573677287485
Loss in iteration 69 : 0.002215862694679774
Loss in iteration 70 : 0.0021886209525625315
Loss in iteration 71 : 0.0021107893458165365
Loss in iteration 72 : 0.0020659881362418515
Loss in iteration 73 : 0.0020398436594302252
Loss in iteration 74 : 0.0019808035775281388
Loss in iteration 75 : 0.0019457072461790987
Loss in iteration 76 : 0.0019042134665156214
Loss in iteration 77 : 0.0018578419392768991
Loss in iteration 78 : 0.0018002680150528826
Loss in iteration 79 : 0.0017584293172493812
Loss in iteration 80 : 0.001713843440260432
Loss in iteration 81 : 0.0016719815056628852
Loss in iteration 82 : 0.0016454945671289668
Loss in iteration 83 : 0.0016239677951146434
Loss in iteration 84 : 0.0015415585366754304
Loss in iteration 85 : 0.001492746747147138
Loss in iteration 86 : 0.0014492860457783869
Loss in iteration 87 : 0.0014190290100025724
Loss in iteration 88 : 0.0013698008913334305
Loss in iteration 89 : 0.0013548748143929635
Loss in iteration 90 : 0.001310556882198215
Loss in iteration 91 : 0.0012867803274267573
Loss in iteration 92 : 0.0012048616457031643
Loss in iteration 93 : 0.0011544673670570817
Loss in iteration 94 : 0.0011081938160255429
Loss in iteration 95 : 0.001067720004487254
Loss in iteration 96 : 0.0010202929994692052
Loss in iteration 97 : 9.719252655841725E-4
Loss in iteration 98 : 9.363749390655471E-4
Loss in iteration 99 : 9.134728383870343E-4
Loss in iteration 100 : 8.821745384927007E-4
Loss in iteration 101 : 8.320860701720781E-4
Loss in iteration 102 : 7.97962612330837E-4
Loss in iteration 103 : 7.469721266614247E-4
Loss in iteration 104 : 6.898282515153804E-4
Loss in iteration 105 : 6.392473132853391E-4
Loss in iteration 106 : 5.928029790075688E-4
Loss in iteration 107 : 5.454145742248853E-4
Loss in iteration 108 : 4.97751380247489E-4
Loss in iteration 109 : 4.591842862588298E-4
Loss in iteration 110 : 4.118841170580845E-4
Loss in iteration 111 : 3.8580578989771784E-4
Loss in iteration 112 : 3.910997834265373E-4
Loss in iteration 113 : 3.0378357828604157E-4
Loss in iteration 114 : 2.5407400766410716E-4
Loss in iteration 115 : 2.0879476527670737E-4
Loss in iteration 116 : 1.9615365880727462E-4
Loss in iteration 117 : 2.1534735150423068E-4
Loss in iteration 118 : 1.7384254269737172E-4
Loss in iteration 119 : 1.1531305674434133E-4
Loss in iteration 120 : 8.799547954170352E-5
Loss in iteration 121 : 6.035006177313277E-5
Loss in iteration 122 : 4.2820193067541934E-5
Loss in iteration 123 : 2.0696969959262814E-5
Loss in iteration 124 : 1.5166641944874914E-5
Loss in iteration 125 : 3.082494526617018E-5
Loss in iteration 126 : 7.360239481019514E-6
Loss in iteration 127 : 0.0
Testing accuracy  of updater 3 on alg 1 with rate 4.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 3742 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4997008814609057
Loss in iteration 3 : 0.2397626413500873
Loss in iteration 4 : 1.0484703252649419
Loss in iteration 5 : 0.4290286807656434
Loss in iteration 6 : 0.5392672828243883
Loss in iteration 7 : 0.16259573550004353
Loss in iteration 8 : 0.06631490305261935
Loss in iteration 9 : 0.05251893227438028
Loss in iteration 10 : 0.0411778905471591
Loss in iteration 11 : 0.03355595869301216
Loss in iteration 12 : 0.02844047796567047
Loss in iteration 13 : 0.024810474086777872
Loss in iteration 14 : 0.021801104691404633
Loss in iteration 15 : 0.0195328098160529
Loss in iteration 16 : 0.01774539597807023
Loss in iteration 17 : 0.016170229624280146
Loss in iteration 18 : 0.014821532017714542
Loss in iteration 19 : 0.013637658262761441
Loss in iteration 20 : 0.012692658818899303
Loss in iteration 21 : 0.01189535346120175
Loss in iteration 22 : 0.011175205387559773
Loss in iteration 23 : 0.010505793015293129
Loss in iteration 24 : 0.009871163849045201
Loss in iteration 25 : 0.009262362906859833
Loss in iteration 26 : 0.00871621323355492
Loss in iteration 27 : 0.008197167803019326
Loss in iteration 28 : 0.007703230948515499
Loss in iteration 29 : 0.007246239326171781
Loss in iteration 30 : 0.006857749137564425
Loss in iteration 31 : 0.006501322068130391
Loss in iteration 32 : 0.00617686920890097
Loss in iteration 33 : 0.005876110587976076
Loss in iteration 34 : 0.005594456439619344
Loss in iteration 35 : 0.005327654847265428
Loss in iteration 36 : 0.0050952165220444975
Loss in iteration 37 : 0.004869799088690203
Loss in iteration 38 : 0.00466346359011987
Loss in iteration 39 : 0.004475414987579748
Loss in iteration 40 : 0.004303678806767008
Loss in iteration 41 : 0.0041348983221013175
Loss in iteration 42 : 0.0039742922009959195
Loss in iteration 43 : 0.0038225135402243167
Loss in iteration 44 : 0.0037002127845079374
Loss in iteration 45 : 0.0035810408612258933
Loss in iteration 46 : 0.003466979225755959
Loss in iteration 47 : 0.0033533295961297336
Loss in iteration 48 : 0.003239854033960615
Loss in iteration 49 : 0.003132737312195843
Loss in iteration 50 : 0.0030445872319520384
Loss in iteration 51 : 0.0029572338683939597
Loss in iteration 52 : 0.0028730113662719598
Loss in iteration 53 : 0.0027940874762037824
Loss in iteration 54 : 0.002716991434501074
Loss in iteration 55 : 0.002642835628206118
Loss in iteration 56 : 0.002577757338342279
Loss in iteration 57 : 0.002517389843618589
Loss in iteration 58 : 0.002462626281013798
Loss in iteration 59 : 0.0023971232466221113
Loss in iteration 60 : 0.00234988588289398
Loss in iteration 61 : 0.0023042122765372037
Loss in iteration 62 : 0.002271863391968643
Loss in iteration 63 : 0.0022265222477880382
Loss in iteration 64 : 0.0021894249141784033
Loss in iteration 65 : 0.0021613267649484656
Loss in iteration 66 : 0.0021422110388914536
Loss in iteration 67 : 0.002092612791168116
Loss in iteration 68 : 0.002061045547313154
Loss in iteration 69 : 0.0020501401413828436
Loss in iteration 70 : 0.002023118870678239
Loss in iteration 71 : 0.0019991486264712087
Loss in iteration 72 : 0.001975873509115819
Loss in iteration 73 : 0.001952940070687734
Loss in iteration 74 : 0.001928368943351694
Loss in iteration 75 : 0.0019069451499774093
Loss in iteration 76 : 0.0018890952901030173
Loss in iteration 77 : 0.001872693159456396
Loss in iteration 78 : 0.0018458489791380566
Loss in iteration 79 : 0.0018222621977911915
Loss in iteration 80 : 0.0018001987932738047
Loss in iteration 81 : 0.0017790191212515532
Loss in iteration 82 : 0.001764335192930073
Loss in iteration 83 : 0.0017433831830615384
Loss in iteration 84 : 0.001728560682253872
Loss in iteration 85 : 0.0017158033484838994
Loss in iteration 86 : 0.0017115909981426302
Loss in iteration 87 : 0.0016901519129106854
Loss in iteration 88 : 0.0016784497752026428
Loss in iteration 89 : 0.001663872527178143
Loss in iteration 90 : 0.0016530287524394057
Loss in iteration 91 : 0.0016400514129448047
Testing accuracy  of updater 3 on alg 1 with rate 1.0 = 0.9866666666666667, training accuracy 0.9994284081166047, time elapsed: 2024 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3187417340836254
Loss in iteration 3 : 0.26336440705322717
Loss in iteration 4 : 0.972532864875173
Loss in iteration 5 : 0.12680675544636733
Loss in iteration 6 : 0.12988461212380872
Loss in iteration 7 : 0.0670159831940276
Loss in iteration 8 : 0.05494594620527768
Loss in iteration 9 : 0.04540317874633647
Loss in iteration 10 : 0.03795326825910379
Loss in iteration 11 : 0.0321605383719819
Loss in iteration 12 : 0.02753259819905192
Loss in iteration 13 : 0.02387437379145064
Loss in iteration 14 : 0.02127613554328274
Loss in iteration 15 : 0.019385119894752025
Loss in iteration 16 : 0.017691123499458217
Loss in iteration 17 : 0.01624900490470666
Loss in iteration 18 : 0.015046597321054279
Loss in iteration 19 : 0.0140934068768339
Loss in iteration 20 : 0.013181277233706988
Loss in iteration 21 : 0.012366481526057344
Loss in iteration 22 : 0.01161221414481954
Loss in iteration 23 : 0.01093436625494242
Loss in iteration 24 : 0.010256223032987323
Loss in iteration 25 : 0.00960580080388477
Loss in iteration 26 : 0.00898834847107479
Loss in iteration 27 : 0.0084078303428385
Loss in iteration 28 : 0.007883844808899008
Loss in iteration 29 : 0.007384659204855913
Loss in iteration 30 : 0.006977244203325987
Loss in iteration 31 : 0.00660947053874684
Loss in iteration 32 : 0.006249802341932528
Loss in iteration 33 : 0.005910392248046321
Loss in iteration 34 : 0.00558429611136981
Loss in iteration 35 : 0.0052992369722024675
Loss in iteration 36 : 0.005022945061289368
Loss in iteration 37 : 0.004770295601475506
Loss in iteration 38 : 0.004584747416230318
Loss in iteration 39 : 0.004369336922582759
Loss in iteration 40 : 0.004174412033458997
Loss in iteration 41 : 0.004015573382694721
Loss in iteration 42 : 0.003881504775209844
Loss in iteration 43 : 0.003745159917481759
Loss in iteration 44 : 0.0036285792422108315
Loss in iteration 45 : 0.003465430153141938
Loss in iteration 46 : 0.0033581820560570482
Loss in iteration 47 : 0.00322180956439691
Loss in iteration 48 : 0.0031205895642818036
Loss in iteration 49 : 0.0030357446648377542
Loss in iteration 50 : 0.0030320576081851324
Loss in iteration 51 : 0.0029637166871200487
Loss in iteration 52 : 0.002853131093231815
Loss in iteration 53 : 0.002785007681714132
Loss in iteration 54 : 0.0026747243216495168
Loss in iteration 55 : 0.002627784184926238
Loss in iteration 56 : 0.0025915288801071517
Loss in iteration 57 : 0.0025660700942906896
Loss in iteration 58 : 0.0024908979452992063
Loss in iteration 59 : 0.0024406868096119687
Loss in iteration 60 : 0.002327625404040972
Loss in iteration 61 : 0.002273491352222382
Loss in iteration 62 : 0.002224716189003038
Loss in iteration 63 : 0.002177386030823481
Loss in iteration 64 : 0.0021746198350108864
Loss in iteration 65 : 0.0022018517019220885
Loss in iteration 66 : 0.002149660847660669
Loss in iteration 67 : 0.0020862896981009844
Loss in iteration 68 : 0.002031285368171654
Loss in iteration 69 : 0.001982353911546567
Loss in iteration 70 : 0.0019473053438370956
Loss in iteration 71 : 0.00191611912252438
Loss in iteration 72 : 0.0018983897681056335
Loss in iteration 73 : 0.0018648668823326236
Loss in iteration 74 : 0.0018359157074722611
Loss in iteration 75 : 0.0018138271327791558
Loss in iteration 76 : 0.0017841725113848634
Loss in iteration 77 : 0.0017654030248948273
Loss in iteration 78 : 0.0017488048324581008
Loss in iteration 79 : 0.001751307414913977
Loss in iteration 80 : 0.001712119187937014
Loss in iteration 81 : 0.0016828315877345952
Loss in iteration 82 : 0.0016698562149175182
Loss in iteration 83 : 0.001655974380806475
Loss in iteration 84 : 0.0016282140486870462
Loss in iteration 85 : 0.0016251902839584797
Loss in iteration 86 : 0.0015925553999364206
Loss in iteration 87 : 0.0015736234798408074
Loss in iteration 88 : 0.0015620265460364316
Loss in iteration 89 : 0.001569944724597782
Loss in iteration 90 : 0.0015374378106936054
Loss in iteration 91 : 0.0015219301072461205
Loss in iteration 92 : 0.0014990399808957092
Loss in iteration 93 : 0.001481396002135166
Loss in iteration 94 : 0.0014695353450131384
Loss in iteration 95 : 0.0014679335176513997
Loss in iteration 96 : 0.0014728263639807147
Loss in iteration 97 : 0.0014371733423097722
Loss in iteration 98 : 0.0014225917732910032
Loss in iteration 99 : 0.0014232923716607866
Loss in iteration 100 : 0.0014342316239795373
Loss in iteration 101 : 0.0014128402718730953
Loss in iteration 102 : 0.0014058791298297447
Loss in iteration 103 : 0.0013822243661333624
Testing accuracy  of updater 3 on alg 1 with rate 0.7 = 0.9955555555555555, training accuracy 0.9994284081166047, time elapsed: 2629 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.2962821403820689
Loss in iteration 3 : 0.7832726475477311
Loss in iteration 4 : 0.3941583697875532
Loss in iteration 5 : 0.1491805838092825
Loss in iteration 6 : 0.11789132276369403
Loss in iteration 7 : 0.09433492312493064
Loss in iteration 8 : 0.07842871029950294
Loss in iteration 9 : 0.07033469944941648
Loss in iteration 10 : 0.06133141595764684
Loss in iteration 11 : 0.05829878626248746
Loss in iteration 12 : 0.04636292756445272
Loss in iteration 13 : 0.04147981105444982
Loss in iteration 14 : 0.0362830970745453
Loss in iteration 15 : 0.03250666957039117
Loss in iteration 16 : 0.029378672035353003
Loss in iteration 17 : 0.026885537173548538
Loss in iteration 18 : 0.024874987939238924
Loss in iteration 19 : 0.02322669348610299
Loss in iteration 20 : 0.02164259085183198
Loss in iteration 21 : 0.020175132500206378
Loss in iteration 22 : 0.01892947962644966
Loss in iteration 23 : 0.017844800975637967
Loss in iteration 24 : 0.016871329473622833
Loss in iteration 25 : 0.01601511439707614
Loss in iteration 26 : 0.015211030325947781
Loss in iteration 27 : 0.01447732940082575
Loss in iteration 28 : 0.013845224657830025
Loss in iteration 29 : 0.013310108356181051
Loss in iteration 30 : 0.012819945499425849
Loss in iteration 31 : 0.012347901873960615
Loss in iteration 32 : 0.011899317832480981
Loss in iteration 33 : 0.011467415401199693
Loss in iteration 34 : 0.01106405575036319
Loss in iteration 35 : 0.010697134102147836
Loss in iteration 36 : 0.010352151888049168
Loss in iteration 37 : 0.010019528953035418
Loss in iteration 38 : 0.009701139900444312
Loss in iteration 39 : 0.009409006921193888
Loss in iteration 40 : 0.009141723483783847
Loss in iteration 41 : 0.008900929033508457
Loss in iteration 42 : 0.008676283094040688
Loss in iteration 43 : 0.008405626780386373
Loss in iteration 44 : 0.00815411095224141
Loss in iteration 45 : 0.007883172574380367
Loss in iteration 46 : 0.00765100979767143
Loss in iteration 47 : 0.007409769036224402
Loss in iteration 48 : 0.007164595470722331
Loss in iteration 49 : 0.00695204057109998
Loss in iteration 50 : 0.0066684314236273275
Loss in iteration 51 : 0.006494004068632284
Loss in iteration 52 : 0.0062208073490962095
Loss in iteration 53 : 0.006083420746668685
Loss in iteration 54 : 0.005828239244661995
Loss in iteration 55 : 0.00570222579822571
Loss in iteration 56 : 0.005500895573847032
Loss in iteration 57 : 0.005323912442549038
Loss in iteration 58 : 0.005135440789526774
Loss in iteration 59 : 0.005006383529000051
Loss in iteration 60 : 0.004832905607126328
Loss in iteration 61 : 0.004693168243872812
Loss in iteration 62 : 0.0045425407291423345
Loss in iteration 63 : 0.004422594041100538
Loss in iteration 64 : 0.004301178572506362
Loss in iteration 65 : 0.004188238535635406
Loss in iteration 66 : 0.004081756930621271
Loss in iteration 67 : 0.003997751277621229
Loss in iteration 68 : 0.003885699379615435
Loss in iteration 69 : 0.00378420569305883
Loss in iteration 70 : 0.003676734254200624
Loss in iteration 71 : 0.0035944045027317367
Loss in iteration 72 : 0.003473017553393238
Loss in iteration 73 : 0.003404885890231437
Loss in iteration 74 : 0.0032903295043487956
Loss in iteration 75 : 0.003219820945090928
Loss in iteration 76 : 0.0031566983907654793
Loss in iteration 77 : 0.00309939119099506
Loss in iteration 78 : 0.0030338610748282973
Loss in iteration 79 : 0.0029748439814459566
Loss in iteration 80 : 0.0029184251333120408
Loss in iteration 81 : 0.002867289528529175
Loss in iteration 82 : 0.0028227123760259207
Loss in iteration 83 : 0.0028162455475099516
Loss in iteration 84 : 0.0027587176637900723
Loss in iteration 85 : 0.002727978613973152
Loss in iteration 86 : 0.0026526197438526623
Loss in iteration 87 : 0.002610145929029322
Loss in iteration 88 : 0.0025732993623691297
Loss in iteration 89 : 0.002534823804305977
Loss in iteration 90 : 0.0024918543773997308
Loss in iteration 91 : 0.0024570418532140685
Loss in iteration 92 : 0.002419780236099867
Loss in iteration 93 : 0.0023876838405200835
Loss in iteration 94 : 0.0023480900657384196
Loss in iteration 95 : 0.002322665953222101
Loss in iteration 96 : 0.002277388683507769
Loss in iteration 97 : 0.002243820698523483
Loss in iteration 98 : 0.0022072374409179032
Loss in iteration 99 : 0.0021681886920187528
Loss in iteration 100 : 0.0021375491698280703
Loss in iteration 101 : 0.002104380020959274
Loss in iteration 102 : 0.0020741758309101455
Loss in iteration 103 : 0.0020638690784170032
Loss in iteration 104 : 0.002021917508147419
Loss in iteration 105 : 0.0020008620503968884
Loss in iteration 106 : 0.001952605111257972
Loss in iteration 107 : 0.0019104006739469736
Loss in iteration 108 : 0.001881546553670077
Loss in iteration 109 : 0.0018554028777627428
Loss in iteration 110 : 0.001840316625085272
Loss in iteration 111 : 0.00188312315936111
Loss in iteration 112 : 0.001847879463083959
Loss in iteration 113 : 0.0017703849913510235
Loss in iteration 114 : 0.001728663969350007
Loss in iteration 115 : 0.0016965659781538265
Loss in iteration 116 : 0.0016713427861092972
Loss in iteration 117 : 0.0016538083552816811
Loss in iteration 118 : 0.0016377942340048454
Loss in iteration 119 : 0.0016208710480748254
Loss in iteration 120 : 0.0016268583535204706
Loss in iteration 121 : 0.0016464735764677497
Loss in iteration 122 : 0.0016144821640614827
Loss in iteration 123 : 0.0015522464290543552
Loss in iteration 124 : 0.0015128820416497495
Loss in iteration 125 : 0.0014914990237920671
Loss in iteration 126 : 0.0014787736790373746
Loss in iteration 127 : 0.0014759264520661911
Loss in iteration 128 : 0.0014645314999131966
Loss in iteration 129 : 0.0014545878054647206
Loss in iteration 130 : 0.0014513137472769088
Loss in iteration 131 : 0.0014303313718870188
Loss in iteration 132 : 0.0014188878371654424
Loss in iteration 133 : 0.00138442504258065
Loss in iteration 134 : 0.0013689055077502424
Loss in iteration 135 : 0.0013540444150989127
Testing accuracy  of updater 3 on alg 1 with rate 0.4 = 1.0, training accuracy 0.9995713060874536, time elapsed: 3349 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7945472260064578
Loss in iteration 3 : 0.5971743971271065
Loss in iteration 4 : 0.4133858308165937
Loss in iteration 5 : 0.2959887809662208
Loss in iteration 6 : 0.24960437348484052
Loss in iteration 7 : 0.23024056172301322
Loss in iteration 8 : 0.2121763964834902
Loss in iteration 9 : 0.1993594182972987
Loss in iteration 10 : 0.18593891468694138
Loss in iteration 11 : 0.17777275311911186
Loss in iteration 12 : 0.1703277126032488
Loss in iteration 13 : 0.16397380291712035
Loss in iteration 14 : 0.15782811657810947
Loss in iteration 15 : 0.1519088925713191
Loss in iteration 16 : 0.14619502285178657
Loss in iteration 17 : 0.1406294687689867
Loss in iteration 18 : 0.135261780212793
Loss in iteration 19 : 0.13003488677856578
Loss in iteration 20 : 0.12491539975978452
Loss in iteration 21 : 0.11992337782935544
Loss in iteration 22 : 0.11501536543440012
Loss in iteration 23 : 0.11026115808997869
Loss in iteration 24 : 0.1056503205829505
Loss in iteration 25 : 0.10123880132689297
Loss in iteration 26 : 0.09705974249585214
Loss in iteration 27 : 0.09321134192853388
Loss in iteration 28 : 0.08954565254770726
Loss in iteration 29 : 0.08622026336538557
Loss in iteration 30 : 0.08305173582625601
Loss in iteration 31 : 0.08010565687192735
Loss in iteration 32 : 0.07738078772966378
Loss in iteration 33 : 0.07485834292222271
Loss in iteration 34 : 0.07247307117649496
Loss in iteration 35 : 0.07021798534722917
Loss in iteration 36 : 0.06809444299849117
Loss in iteration 37 : 0.06607056034182021
Loss in iteration 38 : 0.06416173491524722
Loss in iteration 39 : 0.06238087247626235
Loss in iteration 40 : 0.0607039961175195
Loss in iteration 41 : 0.05913329059253611
Loss in iteration 42 : 0.05764092349682689
Loss in iteration 43 : 0.05623618743258044
Loss in iteration 44 : 0.05489093023960679
Loss in iteration 45 : 0.053597256247732136
Loss in iteration 46 : 0.05235711915635412
Loss in iteration 47 : 0.051183040065457354
Loss in iteration 48 : 0.05004220338461722
Loss in iteration 49 : 0.0489504672561895
Loss in iteration 50 : 0.04788931774881783
Loss in iteration 51 : 0.04685925572240612
Loss in iteration 52 : 0.045859689356602326
Loss in iteration 53 : 0.04489117778997611
Loss in iteration 54 : 0.04396682877057338
Loss in iteration 55 : 0.043076027399310324
Loss in iteration 56 : 0.04221538334137683
Loss in iteration 57 : 0.04138273617054605
Loss in iteration 58 : 0.04057210431597011
Loss in iteration 59 : 0.039799598654506704
Loss in iteration 60 : 0.03904166470277996
Loss in iteration 61 : 0.038304420818303146
Loss in iteration 62 : 0.03758408949293616
Loss in iteration 63 : 0.03689033521063447
Loss in iteration 64 : 0.036225758645406544
Loss in iteration 65 : 0.03558658309266851
Loss in iteration 66 : 0.03497580631267853
Loss in iteration 67 : 0.034385262678750916
Loss in iteration 68 : 0.033804140322677596
Loss in iteration 69 : 0.033236123204966266
Loss in iteration 70 : 0.03267784012389352
Loss in iteration 71 : 0.032152027005393206
Loss in iteration 72 : 0.031649314280011026
Loss in iteration 73 : 0.031154165791079103
Loss in iteration 74 : 0.030671146396241647
Loss in iteration 75 : 0.0301989445704979
Loss in iteration 76 : 0.02973938500231417
Loss in iteration 77 : 0.029290611543725226
Loss in iteration 78 : 0.0288566732230567
Loss in iteration 79 : 0.028443202288254324
Loss in iteration 80 : 0.028038589393009485
Loss in iteration 81 : 0.027641212739661827
Loss in iteration 82 : 0.02725448609289727
Loss in iteration 83 : 0.02687745320306394
Loss in iteration 84 : 0.02650731189763012
Loss in iteration 85 : 0.026143750330436692
Loss in iteration 86 : 0.025793327420740333
Loss in iteration 87 : 0.025455209277393996
Loss in iteration 88 : 0.025126053231540598
Loss in iteration 89 : 0.024802855286870516
Loss in iteration 90 : 0.024487186771702498
Loss in iteration 91 : 0.02417917350241911
Loss in iteration 92 : 0.023879933941132078
Loss in iteration 93 : 0.02359189443460226
Loss in iteration 94 : 0.02331600649746962
Loss in iteration 95 : 0.023050229400506637
Loss in iteration 96 : 0.022788999887836593
Loss in iteration 97 : 0.022531272496743946
Loss in iteration 98 : 0.02227672321725328
Loss in iteration 99 : 0.022027910896168894
Loss in iteration 100 : 0.021786126998611405
Loss in iteration 101 : 0.02155317360491664
Loss in iteration 102 : 0.021329268844695635
Loss in iteration 103 : 0.02111047025602241
Loss in iteration 104 : 0.02089572715835531
Loss in iteration 105 : 0.020682916503680995
Loss in iteration 106 : 0.0204733468068707
Loss in iteration 107 : 0.020266389690702064
Loss in iteration 108 : 0.02006067383339034
Loss in iteration 109 : 0.019857085138162683
Loss in iteration 110 : 0.019654101597726816
Loss in iteration 111 : 0.019452029763774758
Loss in iteration 112 : 0.01925399066725226
Loss in iteration 113 : 0.019057335041919782
Loss in iteration 114 : 0.018862762400946777
Loss in iteration 115 : 0.018671274255390002
Loss in iteration 116 : 0.01848231246975006
Loss in iteration 117 : 0.01829643796170894
Loss in iteration 118 : 0.01811503545684242
Loss in iteration 119 : 0.01793566507193681
Loss in iteration 120 : 0.017757635635289575
Loss in iteration 121 : 0.017580809795650222
Loss in iteration 122 : 0.017406930008888363
Loss in iteration 123 : 0.017236518822841527
Loss in iteration 124 : 0.01706818442819283
Loss in iteration 125 : 0.01690259617097053
Loss in iteration 126 : 0.016740319334310427
Loss in iteration 127 : 0.01657984372234496
Loss in iteration 128 : 0.016422707929457984
Loss in iteration 129 : 0.016266400886797948
Loss in iteration 130 : 0.016111615874957647
Loss in iteration 131 : 0.01595854565711753
Loss in iteration 132 : 0.015807132535066985
Loss in iteration 133 : 0.015656477805982793
Loss in iteration 134 : 0.01550652547122893
Loss in iteration 135 : 0.015358309261290205
Loss in iteration 136 : 0.015212482315801029
Loss in iteration 137 : 0.015068718720594931
Loss in iteration 138 : 0.014925765327421037
Loss in iteration 139 : 0.014784429201233382
Loss in iteration 140 : 0.014645042529527915
Loss in iteration 141 : 0.014506914049658346
Loss in iteration 142 : 0.014369243342864687
Loss in iteration 143 : 0.014234801470613031
Loss in iteration 144 : 0.014103753314947167
Loss in iteration 145 : 0.013977446510463218
Loss in iteration 146 : 0.01385149111149828
Loss in iteration 147 : 0.013726152040823498
Loss in iteration 148 : 0.013601844980892824
Loss in iteration 149 : 0.013479721858369912
Loss in iteration 150 : 0.01335994675607831
Loss in iteration 151 : 0.013242102231392853
Loss in iteration 152 : 0.013125114089651292
Loss in iteration 153 : 0.013008770220784604
Loss in iteration 154 : 0.012893298380711076
Loss in iteration 155 : 0.012778868802744297
Loss in iteration 156 : 0.012669858338812677
Loss in iteration 157 : 0.012565974067657972
Loss in iteration 158 : 0.01246550738487855
Loss in iteration 159 : 0.012366484734225759
Loss in iteration 160 : 0.01227059482150164
Loss in iteration 161 : 0.012178870317842748
Loss in iteration 162 : 0.012087129541117117
Loss in iteration 163 : 0.01199825665225881
Loss in iteration 164 : 0.011910116531609621
Loss in iteration 165 : 0.011823013977608852
Loss in iteration 166 : 0.01173720614692395
Loss in iteration 167 : 0.011652066845170716
Loss in iteration 168 : 0.011568578468493504
Loss in iteration 169 : 0.011489082230443697
Loss in iteration 170 : 0.011411211922027755
Loss in iteration 171 : 0.01133734113549426
Loss in iteration 172 : 0.011259679433209727
Loss in iteration 173 : 0.011186628485153024
Loss in iteration 174 : 0.01111518400788805
Loss in iteration 175 : 0.011045923313365983
Loss in iteration 176 : 0.01097941584000352
Loss in iteration 177 : 0.010914428679574064
Loss in iteration 178 : 0.01085076916817021
Loss in iteration 179 : 0.010787334147487898
Loss in iteration 180 : 0.010725026379904523
Loss in iteration 181 : 0.01066364597208897
Loss in iteration 182 : 0.010603358560660024
Loss in iteration 183 : 0.01054359924058411
Loss in iteration 184 : 0.010484963588696933
Loss in iteration 185 : 0.010428503711777849
Loss in iteration 186 : 0.01037139840718704
Loss in iteration 187 : 0.01031452098119342
Loss in iteration 188 : 0.010258196587624235
Loss in iteration 189 : 0.010202955746081356
Loss in iteration 190 : 0.010147060934397623
Loss in iteration 191 : 0.010091998637162092
Loss in iteration 192 : 0.010037386846611889
Loss in iteration 193 : 0.009984024797144156
Loss in iteration 194 : 0.009931433918961336
Loss in iteration 195 : 0.009879617714267869
Loss in iteration 196 : 0.009828469819011035
Loss in iteration 197 : 0.009778740824409212
Loss in iteration 198 : 0.009729492929899775
Loss in iteration 199 : 0.00968352982607011
Loss in iteration 200 : 0.00963397872350228
Testing accuracy  of updater 3 on alg 1 with rate 0.09999999999999998 = 0.9964444444444445, training accuracy 0.9995713060874536, time elapsed: 4422 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923705
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433591
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.937065804358194
Loss in iteration 25 : 0.9341560838258172
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543957
Loss in iteration 32 : 0.9134823998512602
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830087
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160542
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142405
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743325
Loss in iteration 61 : 0.822579033385705
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249811
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652766
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048011
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088157
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955006
Loss in iteration 82 : 0.7518522153940244
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702151
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626255
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134936
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166519
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753548
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256584
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407875
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146481
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296424
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234752
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.4705657780895095
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.4450968060893727
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.4094159654317907
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 5693 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614217
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311625
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487191
Loss in iteration 31 : 0.9164678049543957
Loss in iteration 32 : 0.9134823998512602
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826744
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209122
Loss in iteration 51 : 0.8548422260830087
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743325
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.819301480225692
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652766
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048011
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940244
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505614
Loss in iteration 86 : 0.7379381485702151
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454701
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134936
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166519
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256584
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407875
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748412
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256972
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036678
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234752
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.5018535958418039
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.4610645411786028
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614924984
Loss in iteration 179 : 0.4224809016575409
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.41596146823413105
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.4094159654317907
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.3995523073557874
Loss in iteration 187 : 0.3962532440560076
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 5187 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614217
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.9312352176477111
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310218
Loss in iteration 30 : 0.9194426884487191
Loss in iteration 31 : 0.9164678049543961
Loss in iteration 32 : 0.9134823998512602
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694353
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793263
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223987
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048011
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088157
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955006
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016564
Loss in iteration 90 : 0.7238860463626255
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001563
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955337
Loss in iteration 110 : 0.6516008788407875
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934181
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249592
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954033
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748412
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036678
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234753
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.49874963817009316
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786574
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550484
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.4610645411786028
Loss in iteration 168 : 0.4578840198263232
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.4515028637989074
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.4257311461492499
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.41596146823413105
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.3736917204558962
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 7154 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302796
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610751
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468237
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311626
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.937065804358194
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310216
Loss in iteration 30 : 0.9194426884487193
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694353
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651215
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149773
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209122
Loss in iteration 51 : 0.8548422260830087
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243425
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.822579033385705
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249812
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.8027762269894211
Loss in iteration 68 : 0.7994438633952063
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.662655282008631
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407875
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146481
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249592
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892694
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748412
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423256
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036678
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234753
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472434
Loss in iteration 162 : 0.4768664044786574
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.4705657780895095
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.454696771363955
Loss in iteration 170 : 0.4515028637989074
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.4224809016575409
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.4094159654317907
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 5215 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486288
Loss in iteration 10 : 0.9765116492610753
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.937065804358194
Loss in iteration 25 : 0.9341560838258172
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310218
Loss in iteration 30 : 0.9194426884487191
Loss in iteration 31 : 0.9164678049543958
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784296
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651215
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318255
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987295
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743325
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223987
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.8027762269894211
Loss in iteration 68 : 0.7994438633952063
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955006
Loss in iteration 82 : 0.7518522153940244
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505614
Loss in iteration 86 : 0.7379381485702151
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454701
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001563
Loss in iteration 97 : 0.6989667568166521
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753548
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.673635788482536
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372487
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.6404730705088589
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294274
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986777
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234753
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.4987496381700931
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550484
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.4610645411786028
Loss in iteration 168 : 0.4578840198263232
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.4450968060893727
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.41596146823413105
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.3995523073557874
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.3832241331850308
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 70.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 5599 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486289
Loss in iteration 10 : 0.9765116492610751
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614217
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468237
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540818
Loss in iteration 20 : 0.9485901976311623
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807445
Loss in iteration 24 : 0.937065804358194
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487191
Loss in iteration 31 : 0.9164678049543957
Loss in iteration 32 : 0.9134823998512602
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826744
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793264
Loss in iteration 43 : 0.8799678200651214
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209122
Loss in iteration 51 : 0.8548422260830087
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160541
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142406
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987295
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249811
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299426
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427348
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626257
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166519
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451954
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.6330139892879431
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146482
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954033
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154834
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748412
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562492
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265787
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234753
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.49874963817009316
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472434
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.47056577808950945
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.454696771363955
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.44509680608937274
Loss in iteration 173 : 0.4418848318575792
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.4224809016575409
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.415961468234131
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649183
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.39955230735578745
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.39295876891020853
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.38322413318503085
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589616
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 40.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 5498 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9975437678308283
Loss in iteration 3 : 0.9950237374607399
Loss in iteration 4 : 0.9924599230480619
Loss in iteration 5 : 0.9898621253845753
Loss in iteration 6 : 0.9872361286991949
Loss in iteration 7 : 0.9845857323428036
Loss in iteration 8 : 0.9819136117302798
Loss in iteration 9 : 0.9792217451486289
Loss in iteration 10 : 0.9765116492610751
Loss in iteration 11 : 0.9737845196178069
Loss in iteration 12 : 0.9710413196502277
Loss in iteration 13 : 0.9682828397614216
Loss in iteration 14 : 0.9655097380923706
Loss in iteration 15 : 0.9627225695517719
Loss in iteration 16 : 0.9599218070468234
Loss in iteration 17 : 0.9571078573671776
Loss in iteration 18 : 0.9542810733037802
Loss in iteration 19 : 0.9514417630540819
Loss in iteration 20 : 0.9485901976311626
Loss in iteration 21 : 0.9457266167778982
Loss in iteration 22 : 0.9428512337433592
Loss in iteration 23 : 0.9399642391807446
Loss in iteration 24 : 0.9370658043581941
Loss in iteration 25 : 0.934156083825817
Loss in iteration 26 : 0.931235217647711
Loss in iteration 27 : 0.9283033332826579
Loss in iteration 28 : 0.9253605471784562
Loss in iteration 29 : 0.9224069661310217
Loss in iteration 30 : 0.9194426884487195
Loss in iteration 31 : 0.9164678049543957
Loss in iteration 32 : 0.9134823998512603
Loss in iteration 33 : 0.9104865514738996
Loss in iteration 34 : 0.9074803329418899
Loss in iteration 35 : 0.9044638127303521
Loss in iteration 36 : 0.9014370551694352
Loss in iteration 37 : 0.8984001208826745
Loss in iteration 38 : 0.8953530671725949
Loss in iteration 39 : 0.892295948360647
Loss in iteration 40 : 0.8892288160874448
Loss in iteration 41 : 0.8861517195784298
Loss in iteration 42 : 0.8830647058793263
Loss in iteration 43 : 0.8799678200651215
Loss in iteration 44 : 0.8768611054258293
Loss in iteration 45 : 0.8737446036318254
Loss in iteration 46 : 0.8706183548811587
Loss in iteration 47 : 0.8674823980309961
Loss in iteration 48 : 0.8643367707149775
Loss in iteration 49 : 0.8611815094481611
Loss in iteration 50 : 0.8580166497209123
Loss in iteration 51 : 0.8548422260830089
Loss in iteration 52 : 0.8516582722190371
Loss in iteration 53 : 0.8484648210160542
Loss in iteration 54 : 0.8452619046243424
Loss in iteration 55 : 0.8420495545120319
Loss in iteration 56 : 0.8388278015142405
Loss in iteration 57 : 0.8355966758773187
Loss in iteration 58 : 0.8323562072987296
Loss in iteration 59 : 0.8291064249630268
Loss in iteration 60 : 0.8258473575743321
Loss in iteration 61 : 0.8225790333857053
Loss in iteration 62 : 0.8193014802256919
Loss in iteration 63 : 0.8160147255223986
Loss in iteration 64 : 0.8127187963252868
Loss in iteration 65 : 0.8094137193249814
Loss in iteration 66 : 0.8060995208712589
Loss in iteration 67 : 0.802776226989421
Loss in iteration 68 : 0.7994438633952065
Loss in iteration 69 : 0.7961024555084038
Loss in iteration 70 : 0.7927520284652767
Loss in iteration 71 : 0.7893926071299427
Loss in iteration 72 : 0.7860242161048009
Loss in iteration 73 : 0.7826468797401014
Loss in iteration 74 : 0.7792606221427347
Loss in iteration 75 : 0.7758654671843481
Loss in iteration 76 : 0.7724614385088158
Loss in iteration 77 : 0.769048559539151
Loss in iteration 78 : 0.7656268534839158
Loss in iteration 79 : 0.7621963433431599
Loss in iteration 80 : 0.7587570519139462
Loss in iteration 81 : 0.7553090017955005
Loss in iteration 82 : 0.7518522153940242
Loss in iteration 83 : 0.7483867149271876
Loss in iteration 84 : 0.7449125224283593
Loss in iteration 85 : 0.7414296597505615
Loss in iteration 86 : 0.7379381485702152
Loss in iteration 87 : 0.7344380103906546
Loss in iteration 88 : 0.7309292665454702
Loss in iteration 89 : 0.7274119382016563
Loss in iteration 90 : 0.7238860463626255
Loss in iteration 91 : 0.7203516118710507
Loss in iteration 92 : 0.7168086554115903
Loss in iteration 93 : 0.7132571975134937
Loss in iteration 94 : 0.7096972585530794
Loss in iteration 95 : 0.7061288587561244
Loss in iteration 96 : 0.7025520182001562
Loss in iteration 97 : 0.6989667568166519
Loss in iteration 98 : 0.695373094393161
Loss in iteration 99 : 0.6917710505753549
Loss in iteration 100 : 0.6881606448690022
Loss in iteration 101 : 0.6845418966418866
Loss in iteration 102 : 0.6809148251256583
Loss in iteration 103 : 0.6772794494176313
Loss in iteration 104 : 0.6736357884825359
Loss in iteration 105 : 0.6699838611542083
Loss in iteration 106 : 0.6663236861372488
Loss in iteration 107 : 0.6626552820086309
Loss in iteration 108 : 0.6589786672192649
Loss in iteration 109 : 0.6552938600955336
Loss in iteration 110 : 0.6516008788407874
Loss in iteration 111 : 0.6478997415367975
Loss in iteration 112 : 0.6441904661451955
Loss in iteration 113 : 0.640473070508859
Loss in iteration 114 : 0.6367475723532878
Loss in iteration 115 : 0.633013989287943
Loss in iteration 116 : 0.6292723388075547
Loss in iteration 117 : 0.6255226382934183
Loss in iteration 118 : 0.6217649050146481
Loss in iteration 119 : 0.6179991561294272
Loss in iteration 120 : 0.6142254086862182
Loss in iteration 121 : 0.6104436796249593
Loss in iteration 122 : 0.6066539857782389
Loss in iteration 123 : 0.6028563438724529
Loss in iteration 124 : 0.5990507705289341
Loss in iteration 125 : 0.5952372822650713
Loss in iteration 126 : 0.5914158954954035
Loss in iteration 127 : 0.5880702948777138
Loss in iteration 128 : 0.5849089541154833
Loss in iteration 129 : 0.5817411120892693
Loss in iteration 130 : 0.5785667360539714
Loss in iteration 131 : 0.5753857882155737
Loss in iteration 132 : 0.5721982289748413
Loss in iteration 133 : 0.5690040186979854
Loss in iteration 134 : 0.5658031187562493
Loss in iteration 135 : 0.5625954921608142
Loss in iteration 136 : 0.5593811039538276
Loss in iteration 137 : 0.5561599214423255
Loss in iteration 138 : 0.5529319143256971
Loss in iteration 139 : 0.5496970547483521
Loss in iteration 140 : 0.5464553172986776
Loss in iteration 141 : 0.5432066789690807
Loss in iteration 142 : 0.539951119088028
Loss in iteration 143 : 0.5366886192323865
Loss in iteration 144 : 0.5334191631265786
Loss in iteration 145 : 0.5301447715558826
Loss in iteration 146 : 0.5269028933101405
Loss in iteration 147 : 0.5237076516137509
Loss in iteration 148 : 0.5205310080296425
Loss in iteration 149 : 0.5173760647036677
Loss in iteration 150 : 0.51424398890042
Loss in iteration 151 : 0.5111414489234753
Loss in iteration 152 : 0.50804613058329
Loss in iteration 153 : 0.5049515399596364
Loss in iteration 154 : 0.501853595841804
Loss in iteration 155 : 0.49874963817009316
Loss in iteration 156 : 0.4956393785875285
Loss in iteration 157 : 0.4925227254481818
Loss in iteration 158 : 0.4893999825280502
Loss in iteration 159 : 0.4862720905036924
Loss in iteration 160 : 0.48314120231889446
Loss in iteration 161 : 0.4800067167472433
Loss in iteration 162 : 0.4768664044786573
Loss in iteration 163 : 0.47371944185222303
Loss in iteration 164 : 0.4705657780895095
Loss in iteration 165 : 0.46740541871550473
Loss in iteration 166 : 0.4642383391638135
Loss in iteration 167 : 0.46106454117860274
Loss in iteration 168 : 0.45788401982632315
Loss in iteration 169 : 0.4546967713639549
Loss in iteration 170 : 0.45150286379890747
Loss in iteration 171 : 0.44830275272055414
Loss in iteration 172 : 0.4450968060893727
Loss in iteration 173 : 0.4418848318575793
Loss in iteration 174 : 0.4386669028021297
Loss in iteration 175 : 0.43544253268918615
Loss in iteration 176 : 0.4322119414813661
Loss in iteration 177 : 0.428974844298762
Loss in iteration 178 : 0.42573114614925
Loss in iteration 179 : 0.42248090165754093
Loss in iteration 180 : 0.41922434662506775
Loss in iteration 181 : 0.41596146823413105
Loss in iteration 182 : 0.4126919947676466
Loss in iteration 183 : 0.40941596543179076
Loss in iteration 184 : 0.4061336134649182
Loss in iteration 185 : 0.40284483649037695
Loss in iteration 186 : 0.3995523073557874
Loss in iteration 187 : 0.39625324405600765
Loss in iteration 188 : 0.3929587689102086
Loss in iteration 189 : 0.38971124646707356
Loss in iteration 190 : 0.3864670708651266
Loss in iteration 191 : 0.3832241331850308
Loss in iteration 192 : 0.380035067747719
Loss in iteration 193 : 0.376864365427017
Loss in iteration 194 : 0.37369172045589605
Loss in iteration 195 : 0.3705288575712078
Loss in iteration 196 : 0.36740877274747125
Loss in iteration 197 : 0.36428997497403637
Loss in iteration 198 : 0.36118273186487243
Loss in iteration 199 : 0.35813500196989234
Loss in iteration 200 : 0.35514368963835324
Testing accuracy  of updater 4 on alg 1 with rate 10.0 = 0.9991111111111111, training accuracy 0.8796799085452986, time elapsed: 5609 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 38.18326792354314
Loss in iteration 3 : 5.079728485450062
Loss in iteration 4 : 2.946146465549762
Loss in iteration 5 : 2.1044577315653963
Loss in iteration 6 : 0.7799164036211272
Loss in iteration 7 : 0.5699182088368433
Loss in iteration 8 : 0.4462818695204205
Loss in iteration 9 : 0.34939422068906806
Loss in iteration 10 : 0.28324517804888016
Loss in iteration 11 : 0.23778927929851915
Loss in iteration 12 : 0.19593680183615045
Loss in iteration 13 : 0.1620469155359943
Loss in iteration 14 : 0.13976373967471814
Loss in iteration 15 : 0.12082333416826771
Loss in iteration 16 : 0.10792992261350602
Loss in iteration 17 : 0.09903413973216679
Loss in iteration 18 : 0.09189422450647988
Loss in iteration 19 : 0.0844948211586036
Loss in iteration 20 : 0.07798961012246121
Loss in iteration 21 : 0.07165202603698709
Loss in iteration 22 : 0.06578740824873971
Loss in iteration 23 : 0.06097753180105627
Loss in iteration 24 : 0.05681741528144133
Loss in iteration 25 : 0.053625675070608045
Loss in iteration 26 : 0.05255785908309639
Loss in iteration 27 : 0.04936480314128857
Loss in iteration 28 : 0.046888910548757046
Loss in iteration 29 : 0.043973378831628204
Loss in iteration 30 : 0.0413800254902825
Loss in iteration 31 : 0.038492479914813436
Loss in iteration 32 : 0.0356262260555147
Loss in iteration 33 : 0.03297528522552146
Loss in iteration 34 : 0.03634297769035232
Loss in iteration 35 : 0.034950899830041654
Loss in iteration 36 : 0.03403793504265499
Loss in iteration 37 : 0.0234332141286683
Loss in iteration 38 : 0.018538098435349565
Loss in iteration 39 : 0.013962566448182034
Loss in iteration 40 : 0.010994041001918383
Loss in iteration 41 : 0.006905532478647497
Loss in iteration 42 : 0.009349829850456926
Loss in iteration 43 : 0.0214477665251521
Loss in iteration 44 : 0.09168863711633769
Loss in iteration 45 : 8.527944443018558
Loss in iteration 46 : 115.76986114266849
Loss in iteration 47 : 5.483335576848177
Loss in iteration 48 : 0.08326299540770908
Loss in iteration 49 : 0.06535402598090259
Loss in iteration 50 : 0.050881131278821906
Loss in iteration 51 : 0.044579484170048096
Loss in iteration 52 : 0.0396694374670199
Loss in iteration 53 : 0.03480829385038508
Loss in iteration 54 : 0.031595658435527266
Loss in iteration 55 : 0.02959746178941851
Loss in iteration 56 : 0.02749297490801513
Loss in iteration 57 : 0.02527674373295737
Loss in iteration 58 : 0.022943066242160845
Loss in iteration 59 : 0.020485986105300173
Loss in iteration 60 : 0.017899286938754092
Loss in iteration 61 : 0.015176487285471916
Loss in iteration 62 : 0.01231083645209262
Loss in iteration 63 : 0.009410273140446361
Loss in iteration 64 : 0.006352902190668533
Loss in iteration 65 : 0.0039253048008166406
Loss in iteration 66 : 0.002809207131388662
Loss in iteration 67 : 0.0018814736840005513
Loss in iteration 68 : 0.0013913883601622673
Loss in iteration 69 : 9.332803942859048E-4
Loss in iteration 70 : 4.895292675555268E-4
Loss in iteration 71 : 2.2662881067823308E-5
Loss in iteration 72 : 4.0487985198968363E-4
Loss in iteration 73 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 10.0 = 0.9982222222222222, training accuracy 1.0, time elapsed: 2364 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 16.71848101410153
Loss in iteration 3 : 2.6739213552059486
Loss in iteration 4 : 1.1116830280758372
Loss in iteration 5 : 0.725915870609879
Loss in iteration 6 : 0.35836122193286063
Loss in iteration 7 : 0.26104145137186013
Loss in iteration 8 : 0.22044569331566136
Loss in iteration 9 : 0.16898919451053662
Loss in iteration 10 : 0.14049750774175812
Loss in iteration 11 : 0.11738038728522907
Loss in iteration 12 : 0.0986292542848462
Loss in iteration 13 : 0.08749236246328762
Loss in iteration 14 : 0.07491035680223215
Loss in iteration 15 : 0.06600844693420767
Loss in iteration 16 : 0.05833171128555792
Loss in iteration 17 : 0.05389749669259043
Loss in iteration 18 : 0.04946759526188593
Loss in iteration 19 : 0.04500745524506963
Loss in iteration 20 : 0.04043390372829141
Loss in iteration 21 : 0.036196448022094194
Loss in iteration 22 : 0.03377241949871111
Loss in iteration 23 : 0.032175499886199437
Loss in iteration 24 : 0.028780110243650802
Loss in iteration 25 : 0.026465235018919932
Loss in iteration 26 : 0.022819146786146112
Loss in iteration 27 : 0.021724815895027998
Loss in iteration 28 : 0.018512564468931087
Loss in iteration 29 : 0.017031167373388365
Loss in iteration 30 : 0.014850155701497754
Loss in iteration 31 : 0.01342130528093432
Loss in iteration 32 : 0.011463935811928134
Loss in iteration 33 : 0.012415009729134554
Loss in iteration 34 : 0.010369763094607727
Loss in iteration 35 : 0.008351710673354452
Loss in iteration 36 : 0.004143484732247786
Loss in iteration 37 : 0.002431858862372854
Loss in iteration 38 : 3.7333786847482364E-4
Loss in iteration 39 : 1.8462938098598883E-4
Loss in iteration 40 : 8.642744652923627E-5
Loss in iteration 41 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 7.0 = 0.9946666666666667, training accuracy 1.0, time elapsed: 1134 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 14.557901744277252
Loss in iteration 3 : 2.2375362058576544
Loss in iteration 4 : 1.3512024578528294
Loss in iteration 5 : 0.838520923260213
Loss in iteration 6 : 0.3200492007782789
Loss in iteration 7 : 0.23215795342779902
Loss in iteration 8 : 0.17902750657484817
Loss in iteration 9 : 0.14124827901921894
Loss in iteration 10 : 0.11068562887496242
Loss in iteration 11 : 0.09144938952292772
Loss in iteration 12 : 0.07612886646324785
Loss in iteration 13 : 0.0650637279995616
Loss in iteration 14 : 0.05495310856625868
Loss in iteration 15 : 0.04812140955539139
Loss in iteration 16 : 0.042974119769640676
Loss in iteration 17 : 0.039960378828906566
Loss in iteration 18 : 0.03643552744773446
Loss in iteration 19 : 0.033345485078869114
Loss in iteration 20 : 0.030766849562931547
Loss in iteration 21 : 0.02810857742526691
Loss in iteration 22 : 0.025661335304885975
Loss in iteration 23 : 0.023889906105080516
Loss in iteration 24 : 0.022601294319380613
Loss in iteration 25 : 0.022425587667137866
Loss in iteration 26 : 0.020771765395325027
Loss in iteration 27 : 0.019962204484891017
Loss in iteration 28 : 0.018299106732299005
Loss in iteration 29 : 0.01731765828903991
Loss in iteration 30 : 0.01627827009899637
Loss in iteration 31 : 0.015154094388664377
Loss in iteration 32 : 0.014066065088053683
Loss in iteration 33 : 0.012929558294798475
Loss in iteration 34 : 0.012349530425049163
Loss in iteration 35 : 0.014500892806658196
Loss in iteration 36 : 0.010559251472671498
Loss in iteration 37 : 0.00877764462964128
Loss in iteration 38 : 0.007215365218675581
Loss in iteration 39 : 0.00678912432921158
Loss in iteration 40 : 0.00430763033567467
Loss in iteration 41 : 0.003187803661549332
Loss in iteration 42 : 0.0028365567142175628
Loss in iteration 43 : 0.007657982977030116
Loss in iteration 44 : 0.1086567984528294
Loss in iteration 45 : 10.240541469279894
Loss in iteration 46 : 28.60243121517533
Loss in iteration 47 : 6.125937411804408
Loss in iteration 48 : 0.24384184985726348
Loss in iteration 49 : 0.08552441873041601
Loss in iteration 50 : 0.0498245506883571
Loss in iteration 51 : 0.035947639898229605
Loss in iteration 52 : 0.02768541127346142
Loss in iteration 53 : 0.023220055268547517
Loss in iteration 54 : 0.01958669676894124
Loss in iteration 55 : 0.016855605881739705
Loss in iteration 56 : 0.01465532835434759
Loss in iteration 57 : 0.01321327446844238
Loss in iteration 58 : 0.011999131774649181
Loss in iteration 59 : 0.011281980776567299
Loss in iteration 60 : 0.010526310113666606
Loss in iteration 61 : 0.009791130385711319
Loss in iteration 62 : 0.009060980277474808
Loss in iteration 63 : 0.008490180491946674
Loss in iteration 64 : 0.008138728901805278
Loss in iteration 65 : 0.007307891190846822
Loss in iteration 66 : 0.0066182534158940294
Loss in iteration 67 : 0.0060107468018406164
Loss in iteration 68 : 0.005129989731595785
Loss in iteration 69 : 0.004392958043158931
Loss in iteration 70 : 0.0036711613821219687
Loss in iteration 71 : 0.0027970989416967293
Loss in iteration 72 : 0.0018283363612111919
Loss in iteration 73 : 0.001037397049986193
Loss in iteration 74 : 4.557072536440677E-4
Loss in iteration 75 : 2.6694676678998996E-4
Loss in iteration 76 : 2.20264227660426E-4
Loss in iteration 77 : 2.1242245170591462E-4
Loss in iteration 78 : 6.957390941450989E-5
Loss in iteration 79 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 4.0 = 0.9982222222222222, training accuracy 1.0, time elapsed: 2428 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.784503774528121
Loss in iteration 3 : 0.5128662758472587
Loss in iteration 4 : 0.44209953931843876
Loss in iteration 5 : 0.369001704121684
Loss in iteration 6 : 0.13087731436615352
Loss in iteration 7 : 0.07889886932577164
Loss in iteration 8 : 0.042174239694169606
Loss in iteration 9 : 0.034571286146669745
Loss in iteration 10 : 0.027871604704527667
Loss in iteration 11 : 0.022487091441155446
Loss in iteration 12 : 0.018756878774459914
Loss in iteration 13 : 0.016039508921216562
Loss in iteration 14 : 0.013904157885952322
Loss in iteration 15 : 0.0120929922958354
Loss in iteration 16 : 0.01087935679029899
Loss in iteration 17 : 0.009975650400076044
Loss in iteration 18 : 0.009187249460618002
Loss in iteration 19 : 0.008414844352298197
Loss in iteration 20 : 0.00778389315434828
Loss in iteration 21 : 0.007131006003617971
Loss in iteration 22 : 0.006493325848258482
Loss in iteration 23 : 0.00598094584158807
Loss in iteration 24 : 0.005654740574569661
Loss in iteration 25 : 0.005462682014943747
Loss in iteration 26 : 0.005276681059626577
Loss in iteration 27 : 0.00461136565079044
Loss in iteration 28 : 0.0041181809042753524
Loss in iteration 29 : 0.003971604025793232
Loss in iteration 30 : 0.003478064949499562
Loss in iteration 31 : 0.003190929788404077
Loss in iteration 32 : 0.0030471973113893654
Loss in iteration 33 : 0.0027287684493030634
Loss in iteration 34 : 0.0028495591067506797
Loss in iteration 35 : 0.002625506614076324
Loss in iteration 36 : 0.002236319898929346
Loss in iteration 37 : 0.0016507052053644359
Loss in iteration 38 : 0.0011791582257938671
Loss in iteration 39 : 6.369953466065018E-4
Loss in iteration 40 : 3.7614890848410785E-4
Loss in iteration 41 : 5.614043359144933E-4
Loss in iteration 42 : 0.002678124725490617
Loss in iteration 43 : 0.005245518788206031
Loss in iteration 44 : 0.03180624130060208
Loss in iteration 45 : 4.12190876443267
Loss in iteration 46 : 6.213005032390876
Loss in iteration 47 : 0.013603061554839077
Loss in iteration 48 : 0.009411635538237716
Loss in iteration 49 : 0.007096661459623227
Loss in iteration 50 : 0.005617894194158814
Loss in iteration 51 : 0.00474860940847451
Loss in iteration 52 : 0.0038526097379132673
Loss in iteration 53 : 0.0032080560487308847
Loss in iteration 54 : 0.0030355764054854286
Loss in iteration 55 : 0.002912380533708071
Loss in iteration 56 : 0.0027887368381566487
Loss in iteration 57 : 0.002664816043826227
Loss in iteration 58 : 0.0025802182269677493
Loss in iteration 59 : 0.0024150037602980196
Loss in iteration 60 : 0.0022980092113817968
Loss in iteration 61 : 0.002176944488960968
Loss in iteration 62 : 0.0020037846301371558
Loss in iteration 63 : 0.0018644633848836896
Loss in iteration 64 : 0.0017061184667233262
Loss in iteration 65 : 0.001553478301348732
Loss in iteration 66 : 0.0013838796800600297
Loss in iteration 67 : 0.0011696437691757805
Loss in iteration 68 : 9.835176472486786E-4
Loss in iteration 69 : 7.836293495869118E-4
Loss in iteration 70 : 6.590827597772039E-4
Loss in iteration 71 : 5.928910088565758E-4
Loss in iteration 72 : 4.444059447874978E-4
Loss in iteration 73 : 3.0958464857114607E-4
Loss in iteration 74 : 1.959065565963602E-4
Loss in iteration 75 : 1.0790789565961033E-4
Loss in iteration 76 : 8.213640478793275E-5
Loss in iteration 77 : 4.2549588592833403E-5
Loss in iteration 78 : 1.015335305491955E-5
Loss in iteration 79 : 5.104559103309041E-5
Loss in iteration 80 : 3.781607230210678E-5
Loss in iteration 81 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 1.0 = 0.9955555555555555, training accuracy 1.0, time elapsed: 3897 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 2.109648500027224
Loss in iteration 3 : 0.38375036996587975
Loss in iteration 4 : 0.38484642830561605
Loss in iteration 5 : 0.33850205988165016
Loss in iteration 6 : 0.22299589881548787
Loss in iteration 7 : 0.10733882600124524
Loss in iteration 8 : 0.03581935456558171
Loss in iteration 9 : 0.028840950663031745
Loss in iteration 10 : 0.02375158068198665
Loss in iteration 11 : 0.01991236488093648
Loss in iteration 12 : 0.016392257427260134
Loss in iteration 13 : 0.01369164237116958
Loss in iteration 14 : 0.011953048887711582
Loss in iteration 15 : 0.01046601251858338
Loss in iteration 16 : 0.009151138120445981
Loss in iteration 17 : 0.00826153790366871
Loss in iteration 18 : 0.007526805744760487
Loss in iteration 19 : 0.006855194814161344
Loss in iteration 20 : 0.006262795822519666
Loss in iteration 21 : 0.005708474016355197
Loss in iteration 22 : 0.005218500987340487
Loss in iteration 23 : 0.004726770529554434
Loss in iteration 24 : 0.00429076268827314
Loss in iteration 25 : 0.003935214234127772
Loss in iteration 26 : 0.003606627314539323
Loss in iteration 27 : 0.003338142553386224
Loss in iteration 28 : 0.0033657065934064798
Loss in iteration 29 : 0.0035835644165556385
Loss in iteration 30 : 0.003994242512686394
Loss in iteration 31 : 0.0029824337977574574
Loss in iteration 32 : 0.002432414510081665
Loss in iteration 33 : 0.0022512702721034362
Loss in iteration 34 : 0.002047118212574895
Loss in iteration 35 : 0.0018673709178445783
Loss in iteration 36 : 0.0015371769334704517
Loss in iteration 37 : 0.0012892698511893716
Loss in iteration 38 : 0.0011028650140508503
Loss in iteration 39 : 0.0014032780831525213
Loss in iteration 40 : 0.002106468619892081
Loss in iteration 41 : 0.0036260390760305913
Loss in iteration 42 : 0.011968492325214733
Loss in iteration 43 : 0.36104145590621844
Loss in iteration 44 : 8.57956507908462
Loss in iteration 45 : 0.34166396409823574
Loss in iteration 46 : 0.00310489585233256
Loss in iteration 47 : 0.002825247137854357
Loss in iteration 48 : 0.002530524527322598
Loss in iteration 49 : 0.002248470805087658
Loss in iteration 50 : 0.0020502767703081113
Loss in iteration 51 : 0.0019028429931721746
Loss in iteration 52 : 0.0017699186057356562
Loss in iteration 53 : 0.0016448796529702672
Loss in iteration 54 : 0.0015131400486646397
Loss in iteration 55 : 0.0013743478823367028
Loss in iteration 56 : 0.0012281339411729358
Loss in iteration 57 : 0.001074111050069309
Loss in iteration 58 : 9.309217201426046E-4
Loss in iteration 59 : 8.278789770855309E-4
Loss in iteration 60 : 7.178023761726374E-4
Loss in iteration 61 : 5.989881940223175E-4
Loss in iteration 62 : 4.7646907700661445E-4
Loss in iteration 63 : 3.551934592078459E-4
Loss in iteration 64 : 2.1975433868928133E-4
Loss in iteration 65 : 1.6803488469109925E-4
Loss in iteration 66 : 1.0753799515776141E-4
Loss in iteration 67 : 8.520517973310009E-5
Loss in iteration 68 : 5.261891828602347E-5
Loss in iteration 69 : 3.845829171351756E-5
Loss in iteration 70 : 3.601012721961912E-5
Loss in iteration 71 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 0.7 = 0.9982222222222222, training accuracy 1.0, time elapsed: 2382 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.0723147999984632
Loss in iteration 3 : 0.2516573654367923
Loss in iteration 4 : 0.36453679215492657
Loss in iteration 5 : 0.21209150077735797
Loss in iteration 6 : 0.14560762007079192
Loss in iteration 7 : 0.07222511293889422
Loss in iteration 8 : 0.027409806589561873
Loss in iteration 9 : 0.02264481032628627
Loss in iteration 10 : 0.018937486690162108
Loss in iteration 11 : 0.01565869581450486
Loss in iteration 12 : 0.013348954964184865
Loss in iteration 13 : 0.011313850112436852
Loss in iteration 14 : 0.00947756238269625
Loss in iteration 15 : 0.00796558574315181
Loss in iteration 16 : 0.0069016042778957875
Loss in iteration 17 : 0.0060326010427437585
Loss in iteration 18 : 0.005461605574788646
Loss in iteration 19 : 0.0049676901823451315
Loss in iteration 20 : 0.004507202939693368
Loss in iteration 21 : 0.004082375259952501
Loss in iteration 22 : 0.0037329392015910978
Loss in iteration 23 : 0.003401540574086191
Loss in iteration 24 : 0.003110098701237366
Loss in iteration 25 : 0.0028462099171592675
Loss in iteration 26 : 0.002631223068830091
Loss in iteration 27 : 0.002432640217483522
Loss in iteration 28 : 0.0024928303264873616
Loss in iteration 29 : 0.002378610179169633
Loss in iteration 30 : 0.002515582740248963
Loss in iteration 31 : 0.0020089072396807443
Loss in iteration 32 : 0.0017961262276818995
Loss in iteration 33 : 0.0016627572563420092
Loss in iteration 34 : 0.0015514997522644198
Loss in iteration 35 : 0.0014576312216338374
Loss in iteration 36 : 0.0012789638368684358
Loss in iteration 37 : 0.0012888701630374539
Loss in iteration 38 : 0.0012566641022594641
Loss in iteration 39 : 0.0010396215668750746
Loss in iteration 40 : 7.549688098050422E-4
Loss in iteration 41 : 5.969068220506553E-4
Loss in iteration 42 : 4.935371000142836E-4
Loss in iteration 43 : 3.772882035590682E-4
Loss in iteration 44 : 6.620378235920985E-4
Loss in iteration 45 : 0.0028820084584860774
Loss in iteration 46 : 0.08505640112125001
Loss in iteration 47 : 4.982980971661962
Loss in iteration 48 : 0.38208078948360763
Loss in iteration 49 : 0.009848030591326405
Loss in iteration 50 : 0.005835923341687853
Loss in iteration 51 : 0.0028695914494556995
Loss in iteration 52 : 0.0019539676036795283
Loss in iteration 53 : 0.0015222371754875965
Loss in iteration 54 : 0.0012061071285073468
Loss in iteration 55 : 0.0010933150847077988
Loss in iteration 56 : 0.0010116477029117228
Loss in iteration 57 : 9.282799883123541E-4
Loss in iteration 58 : 8.658975249658521E-4
Loss in iteration 59 : 8.161979708852032E-4
Loss in iteration 60 : 7.610543606861489E-4
Loss in iteration 61 : 7.106509202760368E-4
Loss in iteration 62 : 6.460152714073353E-4
Loss in iteration 63 : 5.855333539574902E-4
Loss in iteration 64 : 5.277827252666978E-4
Loss in iteration 65 : 4.5606018698629607E-4
Loss in iteration 66 : 3.894166822281336E-4
Loss in iteration 67 : 3.225640138356435E-4
Loss in iteration 68 : 2.5695763625829934E-4
Loss in iteration 69 : 1.9958096600789616E-4
Loss in iteration 70 : 1.4456497488887058E-4
Loss in iteration 71 : 1.1648396793463723E-4
Loss in iteration 72 : 6.890489859482335E-5
Loss in iteration 73 : 5.090707679597175E-5
Loss in iteration 74 : 4.6102520708817935E-5
Loss in iteration 75 : 1.8641495608797204E-6
Loss in iteration 76 : 8.660446738028857E-6
Loss in iteration 77 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 0.4 = 0.9964444444444445, training accuracy 1.0, time elapsed: 2141 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.40261017165946744
Loss in iteration 3 : 0.4080668758037745
Loss in iteration 4 : 0.5190255162521059
Loss in iteration 5 : 0.13864286131091277
Loss in iteration 6 : 0.06965577833412763
Loss in iteration 7 : 0.054739519965307314
Loss in iteration 8 : 0.04403035174190017
Loss in iteration 9 : 0.03567191693257242
Loss in iteration 10 : 0.029112260086336772
Loss in iteration 11 : 0.024512577649103397
Loss in iteration 12 : 0.020946347685996822
Loss in iteration 13 : 0.018285692838701934
Loss in iteration 14 : 0.016198648173745542
Loss in iteration 15 : 0.01447832244297897
Loss in iteration 16 : 0.012957154498745436
Loss in iteration 17 : 0.011530553283605917
Loss in iteration 18 : 0.01023595926943119
Loss in iteration 19 : 0.009101256131742466
Loss in iteration 20 : 0.008298923421065262
Loss in iteration 21 : 0.007715550845065612
Loss in iteration 22 : 0.0083812049574512
Loss in iteration 23 : 0.011122999947345228
Loss in iteration 24 : 0.021587958719818695
Loss in iteration 25 : 0.044789006982792444
Loss in iteration 26 : 0.11063577212537758
Loss in iteration 27 : 0.040995601871070875
Loss in iteration 28 : 0.010905309620247315
Loss in iteration 29 : 0.0051808447745189425
Loss in iteration 30 : 0.0042533267191740155
Loss in iteration 31 : 0.003665813697047356
Loss in iteration 32 : 0.003172405013348573
Loss in iteration 33 : 0.0027548103016533936
Loss in iteration 34 : 0.002477040440674686
Loss in iteration 35 : 0.002237206871046196
Loss in iteration 36 : 0.0020535876174377645
Loss in iteration 37 : 0.001882620248167528
Loss in iteration 38 : 0.0017301956047941266
Loss in iteration 39 : 0.0016110100131387236
Loss in iteration 40 : 0.0015167623122690763
Loss in iteration 41 : 0.0014505957892268588
Loss in iteration 42 : 0.0013841431973946258
Loss in iteration 43 : 0.0013351349981976569
Loss in iteration 44 : 0.001290267802235639
Loss in iteration 45 : 0.0012675111577899788
Loss in iteration 46 : 0.0012590998025994637
Loss in iteration 47 : 0.001255990470444385
Loss in iteration 48 : 0.0012688442232599263
Loss in iteration 49 : 0.0011370065095533274
Loss in iteration 50 : 0.0010914599165579397
Loss in iteration 51 : 0.00101571919158471
Loss in iteration 52 : 9.990196869691425E-4
Loss in iteration 53 : 9.860343094412278E-4
Loss in iteration 54 : 9.61000801740202E-4
Loss in iteration 55 : 0.0010489008765478208
Loss in iteration 56 : 0.0010032084213240837
Loss in iteration 57 : 0.0010113360748855462
Loss in iteration 58 : 8.108224209126851E-4
Loss in iteration 59 : 7.738809688123804E-4
Loss in iteration 60 : 6.932473002729512E-4
Loss in iteration 61 : 6.08882247920899E-4
Loss in iteration 62 : 5.560254394668654E-4
Loss in iteration 63 : 5.144089127871384E-4
Loss in iteration 64 : 5.887986721680526E-4
Loss in iteration 65 : 0.0013479253576083607
Loss in iteration 66 : 0.00196814419347904
Loss in iteration 67 : 0.03932624815206856
Loss in iteration 68 : 1.0367226439261346
Loss in iteration 69 : 0.031616858939714436
Loss in iteration 70 : 0.0020667441107805142
Loss in iteration 71 : 0.0014097175831963142
Loss in iteration 72 : 0.0011215154313515777
Loss in iteration 73 : 9.043476947610018E-4
Loss in iteration 74 : 8.148031382738276E-4
Loss in iteration 75 : 7.877485178767573E-4
Loss in iteration 76 : 7.680629547693184E-4
Loss in iteration 77 : 7.473270778706445E-4
Loss in iteration 78 : 7.25486477929341E-4
Loss in iteration 79 : 7.024841904134804E-4
Loss in iteration 80 : 6.782606169156032E-4
Loss in iteration 81 : 6.527534506491544E-4
Loss in iteration 82 : 6.258976071961689E-4
Loss in iteration 83 : 6.003547258402355E-4
Loss in iteration 84 : 5.722383461031444E-4
Loss in iteration 85 : 5.428748052187618E-4
Loss in iteration 86 : 5.131147488059555E-4
Loss in iteration 87 : 4.8036399181139303E-4
Loss in iteration 88 : 4.5182962961389124E-4
Loss in iteration 89 : 4.3678691551331993E-4
Loss in iteration 90 : 4.1087733589353605E-4
Loss in iteration 91 : 4.103403916291761E-4
Loss in iteration 92 : 3.679241831505832E-4
Loss in iteration 93 : 3.469537369536598E-4
Loss in iteration 94 : 3.5241987154626455E-4
Loss in iteration 95 : 3.006179271826324E-4
Loss in iteration 96 : 2.549694419822101E-4
Loss in iteration 97 : 2.3932486188358375E-4
Loss in iteration 98 : 2.0030890332290094E-4
Loss in iteration 99 : 2.873954551715544E-4
Loss in iteration 100 : 3.369992560949118E-4
Loss in iteration 101 : 2.760478232031907E-4
Loss in iteration 102 : 1.653468258005147E-4
Loss in iteration 103 : 8.141177656236884E-5
Loss in iteration 104 : 4.002493368719998E-5
Loss in iteration 105 : 3.951360267445404E-5
Loss in iteration 106 : 3.791432453851471E-6
Loss in iteration 107 : 0.0
Testing accuracy  of updater 5 on alg 1 with rate 0.09999999999999998 = 1.0, training accuracy 1.0, time elapsed: 2864 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.9628308837434816
Loss in iteration 3 : 0.7066223149120525
Loss in iteration 4 : 0.37764325050534925
Loss in iteration 5 : 0.7882389292438781
Loss in iteration 6 : 0.6919547874511416
Loss in iteration 7 : 0.446185328544377
Loss in iteration 8 : 0.29450815941292907
Loss in iteration 9 : 0.20870746305396282
Loss in iteration 10 : 0.19659015327009036
Loss in iteration 11 : 0.23180605846614669
Loss in iteration 12 : 0.25325973093035153
Loss in iteration 13 : 0.21796950998857537
Loss in iteration 14 : 0.15746336121050017
Loss in iteration 15 : 0.11000925013648195
Loss in iteration 16 : 0.07803920441453485
Loss in iteration 17 : 0.06270984293640805
Loss in iteration 18 : 0.05634847463578715
Loss in iteration 19 : 0.05605244247645271
Loss in iteration 20 : 0.05748484215100547
Loss in iteration 21 : 0.058057324298663095
Loss in iteration 22 : 0.05702520619949462
Loss in iteration 23 : 0.05359288624583691
Loss in iteration 24 : 0.04795031607335441
Loss in iteration 25 : 0.042037135565603526
Loss in iteration 26 : 0.0365268396027232
Loss in iteration 27 : 0.03227196957157908
Loss in iteration 28 : 0.029366928018886705
Loss in iteration 29 : 0.0280810439441369
Loss in iteration 30 : 0.027676196333675934
Loss in iteration 31 : 0.027720393090367924
Loss in iteration 32 : 0.027785179112732847
Loss in iteration 33 : 0.02766724491793696
Loss in iteration 34 : 0.02728690308616776
Loss in iteration 35 : 0.02666894231681894
Loss in iteration 36 : 0.025745696503981744
Loss in iteration 37 : 0.024567796846463794
Loss in iteration 38 : 0.0232780836353143
Loss in iteration 39 : 0.022199672904732723
Loss in iteration 40 : 0.021302381750812414
Loss in iteration 41 : 0.020486084705939257
Loss in iteration 42 : 0.019819188185919854
Loss in iteration 43 : 0.01928991043625233
Loss in iteration 44 : 0.0187583978154885
Loss in iteration 45 : 0.018224396920983234
Loss in iteration 46 : 0.01768767984946969
Loss in iteration 47 : 0.017148041924470794
Loss in iteration 48 : 0.016605299622593644
Loss in iteration 49 : 0.016059288680512998
Loss in iteration 50 : 0.015564559532404933
Loss in iteration 51 : 0.015098161129638759
Loss in iteration 52 : 0.014666155391181719
Loss in iteration 53 : 0.014289525127297063
Loss in iteration 54 : 0.013959895355835129
Loss in iteration 55 : 0.013660602308756799
Loss in iteration 56 : 0.01332685041104676
Loss in iteration 57 : 0.012961298381650325
Loss in iteration 58 : 0.012566370310537844
Loss in iteration 59 : 0.012144276172288044
Loss in iteration 60 : 0.011697030580574532
Loss in iteration 61 : 0.011226469931713678
Loss in iteration 62 : 0.010734268072942761
Loss in iteration 63 : 0.010221950619727069
Loss in iteration 64 : 0.009690908036024452
Loss in iteration 65 : 0.009298216177077987
Loss in iteration 66 : 0.008952548733753679
Loss in iteration 67 : 0.00861792520667556
Loss in iteration 68 : 0.008281717179623917
Loss in iteration 69 : 0.007943858033208054
Loss in iteration 70 : 0.007616843477195142
Loss in iteration 71 : 0.007301104329016634
Loss in iteration 72 : 0.007033611688373932
Loss in iteration 73 : 0.006747910057655717
Loss in iteration 74 : 0.006445424119061135
Loss in iteration 75 : 0.006127449032104986
Loss in iteration 76 : 0.005801858998940648
Loss in iteration 77 : 0.005489736737056971
Loss in iteration 78 : 0.00517423300508288
Loss in iteration 79 : 0.004855473087675238
Loss in iteration 80 : 0.004561031136711989
Loss in iteration 81 : 0.004267912055994588
Loss in iteration 82 : 0.003970746609742676
Loss in iteration 83 : 0.003669737010104151
Loss in iteration 84 : 0.003365067500474135
Loss in iteration 85 : 0.003056905994305263
Loss in iteration 86 : 0.0027454055655859433
Loss in iteration 87 : 0.0024307058042001266
Loss in iteration 88 : 0.0021429454163818815
Loss in iteration 89 : 0.0019143998865996208
Loss in iteration 90 : 0.0016838426170744261
Loss in iteration 91 : 0.0014513403224388744
Loss in iteration 92 : 0.0012481611520814262
Loss in iteration 93 : 0.0010700268285293985
Loss in iteration 94 : 8.815670703590406E-4
Loss in iteration 95 : 6.981327344914842E-4
Loss in iteration 96 : 5.808776957893771E-4
Loss in iteration 97 : 4.7108965719176054E-4
Loss in iteration 98 : 3.6684085086821817E-4
Loss in iteration 99 : 3.0131046376621754E-4
Loss in iteration 100 : 2.3296604569764438E-4
Loss in iteration 101 : 1.6203138097259934E-4
Loss in iteration 102 : 8.870931142680228E-5
Loss in iteration 103 : 1.3183696180290847E-5
Loss in iteration 104 : 0.0
Loss in iteration 105 : 0.0
Loss in iteration 106 : 0.0
Loss in iteration 107 : 0.0
Loss in iteration 108 : 0.0
Testing accuracy  of updater 6 on alg 1 with rate 2.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 2272 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8908649502498314
Loss in iteration 3 : 0.37985982661039747
Loss in iteration 4 : 0.2991135515767085
Loss in iteration 5 : 0.5287045748343187
Loss in iteration 6 : 0.3822547820449064
Loss in iteration 7 : 0.2332394322837988
Loss in iteration 8 : 0.1396391351751624
Loss in iteration 9 : 0.1268981886641862
Loss in iteration 10 : 0.16704762806165788
Loss in iteration 11 : 0.1800997155348684
Loss in iteration 12 : 0.1392540674518572
Loss in iteration 13 : 0.09582177359668945
Loss in iteration 14 : 0.06409970467589375
Loss in iteration 15 : 0.04941130638053317
Loss in iteration 16 : 0.04443319847444567
Loss in iteration 17 : 0.04672934840313725
Loss in iteration 18 : 0.04967477503345356
Loss in iteration 19 : 0.05070931975582507
Loss in iteration 20 : 0.04905430093597463
Loss in iteration 21 : 0.044236762075595744
Loss in iteration 22 : 0.0377543423330282
Loss in iteration 23 : 0.03182681200385685
Loss in iteration 24 : 0.02684508164323868
Loss in iteration 25 : 0.024344255964009583
Loss in iteration 26 : 0.023414849702752573
Loss in iteration 27 : 0.023018529667340342
Loss in iteration 28 : 0.022874208020937245
Loss in iteration 29 : 0.02262142508498053
Loss in iteration 30 : 0.022148112511810397
Loss in iteration 31 : 0.021404119295172906
Loss in iteration 32 : 0.020447903501298676
Loss in iteration 33 : 0.019480600248423102
Loss in iteration 34 : 0.018357352558997207
Loss in iteration 35 : 0.017293434738519617
Loss in iteration 36 : 0.016201993000468068
Loss in iteration 37 : 0.015356140429758695
Loss in iteration 38 : 0.014772471177800605
Loss in iteration 39 : 0.014270506762184559
Loss in iteration 40 : 0.013769245541221742
Loss in iteration 41 : 0.013268458056394542
Loss in iteration 42 : 0.012849396373790551
Loss in iteration 43 : 0.012507780910768453
Loss in iteration 44 : 0.012159945341161946
Loss in iteration 45 : 0.011806361115371604
Loss in iteration 46 : 0.011461925484102081
Loss in iteration 47 : 0.011161390655746468
Loss in iteration 48 : 0.010844981949408597
Loss in iteration 49 : 0.010497239618824107
Loss in iteration 50 : 0.010120995823333367
Loss in iteration 51 : 0.009718809709919982
Loss in iteration 52 : 0.009292993711660499
Loss in iteration 53 : 0.008850240090005316
Loss in iteration 54 : 0.008440583594768904
Loss in iteration 55 : 0.008015203460068425
Loss in iteration 56 : 0.0075978799982699555
Loss in iteration 57 : 0.007190551095106709
Loss in iteration 58 : 0.006778554611997346
Loss in iteration 59 : 0.006412383695545723
Loss in iteration 60 : 0.006145094048482479
Loss in iteration 61 : 0.005871696325435798
Loss in iteration 62 : 0.005592686214490177
Loss in iteration 63 : 0.005313376851031279
Loss in iteration 64 : 0.005042656545320254
Loss in iteration 65 : 0.004764597178635622
Loss in iteration 66 : 0.0044791265131619495
Loss in iteration 67 : 0.004198981642743674
Loss in iteration 68 : 0.003918671392130297
Loss in iteration 69 : 0.003637900584620563
Loss in iteration 70 : 0.0033566336947807736
Loss in iteration 71 : 0.003092606143440733
Loss in iteration 72 : 0.0028488149016567825
Loss in iteration 73 : 0.002602370199676506
Loss in iteration 74 : 0.002353455053094578
Loss in iteration 75 : 0.0021022348606690856
Loss in iteration 76 : 0.0018934538637161316
Loss in iteration 77 : 0.0017377088406028882
Loss in iteration 78 : 0.0015788930364898917
Loss in iteration 79 : 0.0014172542658472675
Loss in iteration 80 : 0.001253016372687507
Loss in iteration 81 : 0.0010863815537960372
Loss in iteration 82 : 9.646923549221849E-4
Loss in iteration 83 : 8.345768978197155E-4
Loss in iteration 84 : 7.180899892161276E-4
Loss in iteration 85 : 6.043258308425966E-4
Loss in iteration 86 : 4.915443125645882E-4
Loss in iteration 87 : 3.796226299027874E-4
Loss in iteration 88 : 2.9411137552140813E-4
Loss in iteration 89 : 2.490808814271961E-4
Loss in iteration 90 : 2.0152474498198565E-4
Loss in iteration 91 : 1.557904722763795E-4
Loss in iteration 92 : 1.0605667111628113E-4
Loss in iteration 93 : 4.72125986730454E-5
Loss in iteration 94 : 0.0
Loss in iteration 95 : 0.0
Loss in iteration 96 : 0.0
Testing accuracy  of updater 6 on alg 1 with rate 1.4000000000000001 = 0.9928888888888889, training accuracy 1.0, time elapsed: 1730 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.6080790375361607
Loss in iteration 3 : 0.2590277483850883
Loss in iteration 4 : 0.2320006217583124
Loss in iteration 5 : 0.3465268790562211
Loss in iteration 6 : 0.21786986873583555
Loss in iteration 7 : 0.13475339494504296
Loss in iteration 8 : 0.09444016480042387
Loss in iteration 9 : 0.10430633180801434
Loss in iteration 10 : 0.1257296961952808
Loss in iteration 11 : 0.11128559553711541
Loss in iteration 12 : 0.07735556987665362
Loss in iteration 13 : 0.052617473034578095
Loss in iteration 14 : 0.038108761644074535
Loss in iteration 15 : 0.034129194094919935
Loss in iteration 16 : 0.03417876264857973
Loss in iteration 17 : 0.03579532365588843
Loss in iteration 18 : 0.03660493255997976
Loss in iteration 19 : 0.035487258374199075
Loss in iteration 20 : 0.0328350275205149
Loss in iteration 21 : 0.028643741494031216
Loss in iteration 22 : 0.024368918142441853
Loss in iteration 23 : 0.02103420212947928
Loss in iteration 24 : 0.01861377910286588
Loss in iteration 25 : 0.01755643154149412
Loss in iteration 26 : 0.017001821082780512
Loss in iteration 27 : 0.01666150090549525
Loss in iteration 28 : 0.016424817050888624
Loss in iteration 29 : 0.016069865948876355
Loss in iteration 30 : 0.015587009797511656
Loss in iteration 31 : 0.014930760892200152
Loss in iteration 32 : 0.014115383341640081
Loss in iteration 33 : 0.013298608034909986
Loss in iteration 34 : 0.01244959791489894
Loss in iteration 35 : 0.011649010799684764
Loss in iteration 36 : 0.0108860752798611
Loss in iteration 37 : 0.0102423946626796
Loss in iteration 38 : 0.009905854345621513
Loss in iteration 39 : 0.009579274608913084
Loss in iteration 40 : 0.009287373499541673
Loss in iteration 41 : 0.009078592111278228
Loss in iteration 42 : 0.008866560384664712
Loss in iteration 43 : 0.008651544680100218
Loss in iteration 44 : 0.008459472233376559
Loss in iteration 45 : 0.008292660960204127
Loss in iteration 46 : 0.008110283291286725
Loss in iteration 47 : 0.007908560043268368
Loss in iteration 48 : 0.007689305503522267
Loss in iteration 49 : 0.007463881228461651
Loss in iteration 50 : 0.007235417826466936
Loss in iteration 51 : 0.006983155599384954
Loss in iteration 52 : 0.0067093277141374585
Loss in iteration 53 : 0.006445043596252945
Loss in iteration 54 : 0.006174453156627758
Loss in iteration 55 : 0.005915844140641515
Loss in iteration 56 : 0.005666234525067667
Loss in iteration 57 : 0.005424973248573653
Loss in iteration 58 : 0.00520782968475388
Loss in iteration 59 : 0.0049973323874106515
Loss in iteration 60 : 0.004786802161962721
Loss in iteration 61 : 0.0046391832459403565
Loss in iteration 62 : 0.004481301119244947
Loss in iteration 63 : 0.004312067311353346
Loss in iteration 64 : 0.004132539873512368
Loss in iteration 65 : 0.003956040957138302
Loss in iteration 66 : 0.0037792144601434207
Loss in iteration 67 : 0.003599316371483749
Loss in iteration 68 : 0.0034245259738237975
Loss in iteration 69 : 0.003255768743537287
Loss in iteration 70 : 0.0030872052461878054
Loss in iteration 71 : 0.002918783806736767
Loss in iteration 72 : 0.00275045784998712
Loss in iteration 73 : 0.0025821854007536016
Loss in iteration 74 : 0.0024139286329669684
Loss in iteration 75 : 0.0022456534629252305
Loss in iteration 76 : 0.00209723674006164
Loss in iteration 77 : 0.001964142893320826
Loss in iteration 78 : 0.0018292136592008277
Loss in iteration 79 : 0.0016926001701315662
Loss in iteration 80 : 0.0015544387633435934
Loss in iteration 81 : 0.001419525617628977
Loss in iteration 82 : 0.0012824551569727439
Loss in iteration 83 : 0.001141645231831624
Loss in iteration 84 : 0.0010278283577319673
Loss in iteration 85 : 9.229895596752626E-4
Loss in iteration 86 : 8.185877649558273E-4
Loss in iteration 87 : 7.164183958949957E-4
Loss in iteration 88 : 6.13315111797563E-4
Loss in iteration 89 : 5.093487644173858E-4
Loss in iteration 90 : 4.282083345465143E-4
Loss in iteration 91 : 3.494924459602431E-4
Loss in iteration 92 : 2.863090032865599E-4
Loss in iteration 93 : 2.3952429494013652E-4
Loss in iteration 94 : 1.9218341820787744E-4
Loss in iteration 95 : 1.5856341844356774E-4
Loss in iteration 96 : 1.271253748156964E-4
Loss in iteration 97 : 9.464728971208679E-5
Loss in iteration 98 : 6.122368484386407E-5
Loss in iteration 99 : 2.6939808186427975E-5
Loss in iteration 100 : 0.0
Loss in iteration 101 : 0.0
Loss in iteration 102 : 0.0
Testing accuracy  of updater 6 on alg 1 with rate 0.8 = 0.9928888888888889, training accuracy 1.0, time elapsed: 1803 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.31835358202064956
Loss in iteration 3 : 0.218994956925075
Loss in iteration 4 : 0.11814320362924022
Loss in iteration 5 : 0.10170752263035579
Loss in iteration 6 : 0.08264041501707962
Loss in iteration 7 : 0.07140607092647468
Loss in iteration 8 : 0.062664391478046
Loss in iteration 9 : 0.05319418390323702
Loss in iteration 10 : 0.043570981810199735
Loss in iteration 11 : 0.03450581051572083
Loss in iteration 12 : 0.02776064648619773
Loss in iteration 13 : 0.023315812134715414
Loss in iteration 14 : 0.020976324051966158
Loss in iteration 15 : 0.019112461715160015
Loss in iteration 16 : 0.017468667483390057
Loss in iteration 17 : 0.016009628054565177
Loss in iteration 18 : 0.01477406774055579
Loss in iteration 19 : 0.013559581466256151
Loss in iteration 20 : 0.01230026824600182
Loss in iteration 21 : 0.01118767660555738
Loss in iteration 22 : 0.010374334838283652
Loss in iteration 23 : 0.00978632716066179
Loss in iteration 24 : 0.009364063390084316
Loss in iteration 25 : 0.008869224082167684
Loss in iteration 26 : 0.00834987822825486
Loss in iteration 27 : 0.007793692211656188
Loss in iteration 28 : 0.007287683055989601
Loss in iteration 29 : 0.006773545341188574
Loss in iteration 30 : 0.006331705422626296
Loss in iteration 31 : 0.005931528013814445
Loss in iteration 32 : 0.00556683294390417
Loss in iteration 33 : 0.005320209071519302
Loss in iteration 34 : 0.005107827155009229
Loss in iteration 35 : 0.004955392605828735
Loss in iteration 36 : 0.004846039584323858
Loss in iteration 37 : 0.004744206936929977
Loss in iteration 38 : 0.0046402115846446365
Loss in iteration 39 : 0.004533749788200569
Loss in iteration 40 : 0.004422733176708543
Loss in iteration 41 : 0.004323864281419707
Loss in iteration 42 : 0.004238260586205608
Loss in iteration 43 : 0.004158192989962855
Loss in iteration 44 : 0.004080312365719234
Loss in iteration 45 : 0.004002353502005969
Loss in iteration 46 : 0.0039243114583522405
Loss in iteration 47 : 0.0038461817895493497
Loss in iteration 48 : 0.0037739383722487064
Loss in iteration 49 : 0.0037143345743487913
Loss in iteration 50 : 0.0036595370900799635
Loss in iteration 51 : 0.0036036919738241632
Loss in iteration 52 : 0.0035468920242739364
Loss in iteration 53 : 0.0034892209115862955
Loss in iteration 54 : 0.0034307540756476464
Loss in iteration 55 : 0.0033715595359770393
Loss in iteration 56 : 0.003311698621956383
Loss in iteration 57 : 0.003251226631222316
Loss in iteration 58 : 0.0031901934232842688
Loss in iteration 59 : 0.003129653704548316
Loss in iteration 60 : 0.0030719916499802953
Loss in iteration 61 : 0.0030145200359548148
Loss in iteration 62 : 0.0029584618922731207
Loss in iteration 63 : 0.002903491027431623
Loss in iteration 64 : 0.0028481761318184806
Loss in iteration 65 : 0.002792542084501319
Loss in iteration 66 : 0.0027366113196488273
Loss in iteration 67 : 0.0026859151830980147
Loss in iteration 68 : 0.002640453388875956
Loss in iteration 69 : 0.0025947849254081097
Loss in iteration 70 : 0.0025514388140527406
Loss in iteration 71 : 0.0025088503795861724
Loss in iteration 72 : 0.0024666335406537455
Loss in iteration 73 : 0.0024247458014557536
Loss in iteration 74 : 0.0023831488563239088
Loss in iteration 75 : 0.0023418081770131636
Loss in iteration 76 : 0.0023006926406280294
Loss in iteration 77 : 0.002260089969909937
Loss in iteration 78 : 0.0022206685578090163
Loss in iteration 79 : 0.002180890576736171
Loss in iteration 80 : 0.0021407847476935696
Loss in iteration 81 : 0.002100376964915251
Loss in iteration 82 : 0.0020603428908362448
Loss in iteration 83 : 0.002021001535703841
Loss in iteration 84 : 0.0019816413735009412
Loss in iteration 85 : 0.001942258421388059
Loss in iteration 86 : 0.0019028490921626526
Loss in iteration 87 : 0.0018634101552637969
Loss in iteration 88 : 0.0018239387016174172
Loss in iteration 89 : 0.001784432111943974
Loss in iteration 90 : 0.0017448880281877015
Loss in iteration 91 : 0.0017053043277600358
Loss in iteration 92 : 0.0016675550808460534
Loss in iteration 93 : 0.001632781714569463
Loss in iteration 94 : 0.001597749788116553
Loss in iteration 95 : 0.0015649019133100797
Loss in iteration 96 : 0.0015318341086840996
Loss in iteration 97 : 0.0014996726901063703
Loss in iteration 98 : 0.0014676151010550378
Loss in iteration 99 : 0.0014346569759928877
Loss in iteration 100 : 0.0014011872062025595
Loss in iteration 101 : 0.001367890458923502
Loss in iteration 102 : 0.0013339358376027045
Loss in iteration 103 : 0.0012993825345361549
Loss in iteration 104 : 0.0012661146973913977
Loss in iteration 105 : 0.001232680143703025
Loss in iteration 106 : 0.0011990813386293288
Loss in iteration 107 : 0.0011657639326578535
Loss in iteration 108 : 0.0011318308629850683
Loss in iteration 109 : 0.0010977595699473628
Loss in iteration 110 : 0.0010656356183023398
Loss in iteration 111 : 0.0010329805429190114
Loss in iteration 112 : 9.9980710361903E-4
Loss in iteration 113 : 9.6616114236245E-4
Loss in iteration 114 : 9.334161760972072E-4
Loss in iteration 115 : 9.005158676023584E-4
Loss in iteration 116 : 8.662582089783331E-4
Loss in iteration 117 : 8.316015923713698E-4
Loss in iteration 118 : 7.96306953119246E-4
Loss in iteration 119 : 7.63356776947483E-4
Loss in iteration 120 : 7.307102201909976E-4
Loss in iteration 121 : 6.971594489864949E-4
Loss in iteration 122 : 6.637877123620675E-4
Loss in iteration 123 : 6.305555824564016E-4
Loss in iteration 124 : 5.978785738546404E-4
Loss in iteration 125 : 5.646241141529158E-4
Loss in iteration 126 : 5.312926938926857E-4
Loss in iteration 127 : 4.975622260686986E-4
Loss in iteration 128 : 4.6340480288778707E-4
Loss in iteration 129 : 4.297203387906479E-4
Loss in iteration 130 : 3.9630986586653853E-4
Loss in iteration 131 : 3.627998810211394E-4
Loss in iteration 132 : 3.365523448968682E-4
Loss in iteration 133 : 3.13641866763622E-4
Loss in iteration 134 : 2.9306556459251305E-4
Loss in iteration 135 : 2.7758060904802445E-4
Loss in iteration 136 : 2.6307608491523565E-4
Loss in iteration 137 : 2.503004492559508E-4
Loss in iteration 138 : 2.3768023948235158E-4
Loss in iteration 139 : 2.2491062975169875E-4
Testing accuracy  of updater 6 on alg 1 with rate 0.2 = 0.9937777777777778, training accuracy 1.0, time elapsed: 3154 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3281341262280514
Loss in iteration 3 : 0.3548489509429963
Loss in iteration 4 : 0.15682036591851775
Loss in iteration 5 : 0.1603452409107774
Loss in iteration 6 : 0.11766078968338023
Loss in iteration 7 : 0.07964322213404502
Loss in iteration 8 : 0.06921300558586352
Loss in iteration 9 : 0.06774648946211446
Loss in iteration 10 : 0.06530582444641292
Loss in iteration 11 : 0.058696369125390946
Loss in iteration 12 : 0.049881987158099515
Loss in iteration 13 : 0.040540619986514266
Loss in iteration 14 : 0.032155850551415475
Loss in iteration 15 : 0.02581821448938887
Loss in iteration 16 : 0.021898903754593083
Loss in iteration 17 : 0.01953783475866734
Loss in iteration 18 : 0.018658511981908404
Loss in iteration 19 : 0.018507141922646627
Loss in iteration 20 : 0.01820579313283361
Loss in iteration 21 : 0.017611453027052807
Loss in iteration 22 : 0.016604396539072946
Loss in iteration 23 : 0.015389687481514579
Loss in iteration 24 : 0.014028107636422317
Loss in iteration 25 : 0.012595666017713896
Loss in iteration 26 : 0.011339484309059091
Loss in iteration 27 : 0.010161023086038981
Loss in iteration 28 : 0.009368722284124359
Loss in iteration 29 : 0.008689767228736534
Loss in iteration 30 : 0.008191991370424027
Loss in iteration 31 : 0.007848383769948966
Loss in iteration 32 : 0.0075220003141575386
Loss in iteration 33 : 0.0072233328223496015
Loss in iteration 34 : 0.007001203374658143
Loss in iteration 35 : 0.006756019194845448
Loss in iteration 36 : 0.006477164381930197
Loss in iteration 37 : 0.006168893514390046
Loss in iteration 38 : 0.0058509232313046205
Loss in iteration 39 : 0.005520740536377273
Loss in iteration 40 : 0.005167062029317923
Loss in iteration 41 : 0.004850065111227753
Loss in iteration 42 : 0.004628004724449591
Loss in iteration 43 : 0.004465832350526214
Loss in iteration 44 : 0.00436594362140586
Loss in iteration 45 : 0.004274250225309222
Loss in iteration 46 : 0.00418849001717733
Loss in iteration 47 : 0.004106996558626235
Loss in iteration 48 : 0.004038625002542024
Loss in iteration 49 : 0.00397332251742672
Loss in iteration 50 : 0.003913455561257868
Loss in iteration 51 : 0.0038553157783974574
Loss in iteration 52 : 0.0038015069805582597
Loss in iteration 53 : 0.0037533522612825097
Loss in iteration 54 : 0.003707150112600419
Loss in iteration 55 : 0.0036602391287027143
Loss in iteration 56 : 0.003612682837821699
Loss in iteration 57 : 0.0035645384915944066
Loss in iteration 58 : 0.0035199069027436357
Loss in iteration 59 : 0.0034744638496265814
Loss in iteration 60 : 0.003428211211786655
Loss in iteration 61 : 0.0033830588209557517
Loss in iteration 62 : 0.0033402859695594264
Loss in iteration 63 : 0.0032987086173309676
Loss in iteration 64 : 0.0032571048749381153
Loss in iteration 65 : 0.003215472305968603
Loss in iteration 66 : 0.003173808715987396
Loss in iteration 67 : 0.0031321121286262176
Loss in iteration 68 : 0.0030909909472436143
Loss in iteration 69 : 0.00305310805132297
Loss in iteration 70 : 0.0030161044833251225
Loss in iteration 71 : 0.0029785576343200155
Loss in iteration 72 : 0.0029405161461425627
Loss in iteration 73 : 0.002903687782961992
Loss in iteration 74 : 0.0028663906176706086
Loss in iteration 75 : 0.0028283565665546233
Loss in iteration 76 : 0.0027896531792448343
Loss in iteration 77 : 0.002750881090243883
Loss in iteration 78 : 0.0027133204099422607
Loss in iteration 79 : 0.0026756975709577246
Loss in iteration 80 : 0.0026380142147395622
Loss in iteration 81 : 0.00260027182143683
Loss in iteration 82 : 0.002562471725840666
Loss in iteration 83 : 0.0025246151317504565
Loss in iteration 84 : 0.0024870084634477916
Loss in iteration 85 : 0.0024497110244806773
Loss in iteration 86 : 0.0024120906803158705
Loss in iteration 87 : 0.0023741746263969653
Loss in iteration 88 : 0.0023359873699438432
Loss in iteration 89 : 0.0022980543673077245
Loss in iteration 90 : 0.002260664561453035
Loss in iteration 91 : 0.002223147964412818
Loss in iteration 92 : 0.0021920280940576947
Loss in iteration 93 : 0.002161612544260104
Loss in iteration 94 : 0.0021338274693458704
Loss in iteration 95 : 0.0021060192180163543
Loss in iteration 96 : 0.002077938655439733
Loss in iteration 97 : 0.002049609188010591
Loss in iteration 98 : 0.0020210519078713695
Loss in iteration 99 : 0.0019922858217578613
Loss in iteration 100 : 0.0019633280572181433
Loss in iteration 101 : 0.0019346926272223214
Loss in iteration 102 : 0.0019073079071651791
Loss in iteration 103 : 0.0018799674808988112
Loss in iteration 104 : 0.0018526638895313614
Loss in iteration 105 : 0.0018253904125037628
Loss in iteration 106 : 0.001798140994568003
Loss in iteration 107 : 0.0017709101799857712
Loss in iteration 108 : 0.0017436930532336735
Loss in iteration 109 : 0.0017164851855717273
Loss in iteration 110 : 0.001689282586895434
Loss in iteration 111 : 0.0016625977117768116
Loss in iteration 112 : 0.0016361251463554263
Loss in iteration 113 : 0.0016092408486285124
Loss in iteration 114 : 0.0015819821010253047
Loss in iteration 115 : 0.0015543824982348394
Loss in iteration 116 : 0.0015271820720362981
Loss in iteration 117 : 0.001500701032055651
Loss in iteration 118 : 0.0014745119827381987
Loss in iteration 119 : 0.0014478092428827128
Loss in iteration 120 : 0.0014206401255510005
Loss in iteration 121 : 0.0013932689720569634
Loss in iteration 122 : 0.001366515839804467
Loss in iteration 123 : 0.0013396830381606428
Loss in iteration 124 : 0.0013127753837406094
Loss in iteration 125 : 0.0012857972170470537
Loss in iteration 126 : 0.0012591857102895372
Loss in iteration 127 : 0.0012322399003254242
Loss in iteration 128 : 0.0012048773034406249
Loss in iteration 129 : 0.0011781322763924726
Loss in iteration 130 : 0.0011521201892206278
Loss in iteration 131 : 0.001125697596391458
Loss in iteration 132 : 0.001098901796290466
Loss in iteration 133 : 0.0010717663969800233
Loss in iteration 134 : 0.001044321681318627
Loss in iteration 135 : 0.001018258203233492
Loss in iteration 136 : 9.945423081065032E-4
Loss in iteration 137 : 9.713431308946651E-4
Loss in iteration 138 : 9.484560812565998E-4
Loss in iteration 139 : 9.258930269889292E-4
Loss in iteration 140 : 9.030690979808729E-4
Loss in iteration 141 : 8.80261123933144E-4
Loss in iteration 142 : 8.57344543724001E-4
Loss in iteration 143 : 8.343275100073904E-4
Testing accuracy  of updater 6 on alg 1 with rate 0.14 = 0.9928888888888889, training accuracy 0.9995713060874536, time elapsed: 2811 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.5163612147913927
Loss in iteration 3 : 0.29178540717497764
Loss in iteration 4 : 0.23598532198533353
Loss in iteration 5 : 0.17482875868575143
Loss in iteration 6 : 0.10620110326367163
Loss in iteration 7 : 0.10984377908872449
Loss in iteration 8 : 0.12691779829365063
Loss in iteration 9 : 0.11120088508673445
Loss in iteration 10 : 0.08476609593424468
Loss in iteration 11 : 0.0681869374952725
Loss in iteration 12 : 0.057734560788808606
Loss in iteration 13 : 0.051944494004679174
Loss in iteration 14 : 0.04843297678875057
Loss in iteration 15 : 0.045451383235126146
Loss in iteration 16 : 0.041965674240668806
Loss in iteration 17 : 0.03721615452280781
Loss in iteration 18 : 0.031732381763786316
Loss in iteration 19 : 0.02672209030150295
Loss in iteration 20 : 0.022696209375346516
Loss in iteration 21 : 0.01940439266583205
Loss in iteration 22 : 0.01675480331408915
Loss in iteration 23 : 0.015323942878907933
Loss in iteration 24 : 0.014243033534383833
Loss in iteration 25 : 0.013322568616877483
Loss in iteration 26 : 0.012570595494890844
Loss in iteration 27 : 0.011975132139681753
Loss in iteration 28 : 0.011357328602694972
Loss in iteration 29 : 0.010684628968484827
Loss in iteration 30 : 0.009926772094425362
Loss in iteration 31 : 0.009220655831739987
Loss in iteration 32 : 0.008653606992386252
Loss in iteration 33 : 0.00816337102791854
Loss in iteration 34 : 0.007675329984642572
Loss in iteration 35 : 0.007222192966847884
Loss in iteration 36 : 0.006828552634912276
Loss in iteration 37 : 0.006510487636447609
Loss in iteration 38 : 0.0062215568990958885
Loss in iteration 39 : 0.005964030068573311
Loss in iteration 40 : 0.005704656674018729
Loss in iteration 41 : 0.005453556325273477
Loss in iteration 42 : 0.005240906084510179
Loss in iteration 43 : 0.005048303264103344
Loss in iteration 44 : 0.004864478377099938
Loss in iteration 45 : 0.004666758465999058
Loss in iteration 46 : 0.004463993632448462
Loss in iteration 47 : 0.004271444643147748
Loss in iteration 48 : 0.004116903679784074
Loss in iteration 49 : 0.003988832099405363
Loss in iteration 50 : 0.0038638245035617046
Loss in iteration 51 : 0.0037445490090709157
Loss in iteration 52 : 0.003633176402267427
Loss in iteration 53 : 0.0035242924075708426
Loss in iteration 54 : 0.0034313785165120698
Loss in iteration 55 : 0.0033594307214393423
Loss in iteration 56 : 0.003304714271771949
Loss in iteration 57 : 0.003250320979272388
Loss in iteration 58 : 0.003202566793215577
Loss in iteration 59 : 0.0031554801104877375
Loss in iteration 60 : 0.0031132039613437686
Loss in iteration 61 : 0.0030706869974471405
Loss in iteration 62 : 0.0030280589543761837
Loss in iteration 63 : 0.002984797345096562
Loss in iteration 64 : 0.0029421268394286184
Loss in iteration 65 : 0.002904864023334059
Loss in iteration 66 : 0.0028716709511920732
Loss in iteration 67 : 0.0028398604238440645
Loss in iteration 68 : 0.0028097739364306477
Loss in iteration 69 : 0.0027807718190042477
Loss in iteration 70 : 0.0027521816628264502
Loss in iteration 71 : 0.0027250723712305405
Loss in iteration 72 : 0.0027006735109909117
Loss in iteration 73 : 0.00267630973408426
Loss in iteration 74 : 0.0026513941669352083
Loss in iteration 75 : 0.002625978468291106
Loss in iteration 76 : 0.0026001091799160102
Loss in iteration 77 : 0.002573828233403812
Loss in iteration 78 : 0.0025471734067984953
Loss in iteration 79 : 0.0025214286357266563
Loss in iteration 80 : 0.002498869110031701
Loss in iteration 81 : 0.002476835514295394
Loss in iteration 82 : 0.0024547642486901274
Loss in iteration 83 : 0.0024329466319396873
Loss in iteration 84 : 0.0024118906758466467
Loss in iteration 85 : 0.0023914445448934025
Loss in iteration 86 : 0.0023708984280555664
Loss in iteration 87 : 0.0023501119757753782
Testing accuracy  of updater 6 on alg 1 with rate 0.08000000000000002 = 0.9848888888888889, training accuracy 0.999285510145756, time elapsed: 1696 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8872141204398888
Loss in iteration 3 : 0.6843247009145151
Loss in iteration 4 : 0.42087799551755345
Loss in iteration 5 : 0.28160766258846276
Loss in iteration 6 : 0.21687724582715415
Loss in iteration 7 : 0.19329797839183496
Loss in iteration 8 : 0.1733598275763135
Loss in iteration 9 : 0.1536398954282908
Loss in iteration 10 : 0.13738297320280238
Loss in iteration 11 : 0.12585038308282512
Loss in iteration 12 : 0.11656420480460462
Loss in iteration 13 : 0.10581027055702365
Loss in iteration 14 : 0.09334650109004838
Loss in iteration 15 : 0.08292752009405259
Loss in iteration 16 : 0.07507946865334608
Loss in iteration 17 : 0.06942561220716024
Loss in iteration 18 : 0.06481820177584252
Loss in iteration 19 : 0.060698836011695255
Loss in iteration 20 : 0.05679156615059299
Loss in iteration 21 : 0.052841746430534124
Loss in iteration 22 : 0.048929520182907964
Loss in iteration 23 : 0.04517847987379087
Loss in iteration 24 : 0.04165400215745604
Loss in iteration 25 : 0.038338937803801654
Loss in iteration 26 : 0.03535749763527473
Loss in iteration 27 : 0.03263865022777088
Loss in iteration 28 : 0.030182170318819
Loss in iteration 29 : 0.027962282494012433
Loss in iteration 30 : 0.026045454878441766
Loss in iteration 31 : 0.0243379329323651
Loss in iteration 32 : 0.02280935143766145
Loss in iteration 33 : 0.021453917290707468
Loss in iteration 34 : 0.020294016768809636
Loss in iteration 35 : 0.019305377497412953
Loss in iteration 36 : 0.018465173087194937
Loss in iteration 37 : 0.01768993613815446
Loss in iteration 38 : 0.016962271708045623
Loss in iteration 39 : 0.016298319998023853
Loss in iteration 40 : 0.015704314181180356
Loss in iteration 41 : 0.015139490686868772
Loss in iteration 42 : 0.014609042963602532
Loss in iteration 43 : 0.01411854474756284
Loss in iteration 44 : 0.013665752883244219
Loss in iteration 45 : 0.013243269902654172
Loss in iteration 46 : 0.012843702823384382
Loss in iteration 47 : 0.012469240150395982
Loss in iteration 48 : 0.012111720692751477
Loss in iteration 49 : 0.01176253360641573
Loss in iteration 50 : 0.011415405652477536
Loss in iteration 51 : 0.011076155790802495
Loss in iteration 52 : 0.010744188296999404
Loss in iteration 53 : 0.010418162804072064
Loss in iteration 54 : 0.01009989178700063
Loss in iteration 55 : 0.009788634648702797
Loss in iteration 56 : 0.009484142310857728
Loss in iteration 57 : 0.009186569152433538
Loss in iteration 58 : 0.008893732323637615
Loss in iteration 59 : 0.00861168724645932
Loss in iteration 60 : 0.0083379380056031
Loss in iteration 61 : 0.008069023710549692
Loss in iteration 62 : 0.007814860285561206
Loss in iteration 63 : 0.007566413358154521
Loss in iteration 64 : 0.0073268388703738654
Loss in iteration 65 : 0.0070903260482620195
Loss in iteration 66 : 0.006860382051874397
Loss in iteration 67 : 0.0066381098992335475
Loss in iteration 68 : 0.006424811257758107
Loss in iteration 69 : 0.006221220818682205
Loss in iteration 70 : 0.006026613206063186
Loss in iteration 71 : 0.0058406312049237965
Loss in iteration 72 : 0.005668172332007576
Loss in iteration 73 : 0.005516453713637222
Loss in iteration 74 : 0.005377542460659166
Loss in iteration 75 : 0.005241853759922793
Loss in iteration 76 : 0.0051083125630935035
Loss in iteration 77 : 0.004980166684674445
Loss in iteration 78 : 0.004860750269854934
Loss in iteration 79 : 0.004745224959303776
Loss in iteration 80 : 0.004635446362175743
Loss in iteration 81 : 0.004530761477298137
Loss in iteration 82 : 0.0044300193212671495
Loss in iteration 83 : 0.00433294143494902
Loss in iteration 84 : 0.0042403159038353734
Loss in iteration 85 : 0.004151372702514645
Loss in iteration 86 : 0.004064137875336231
Loss in iteration 87 : 0.0039795184082763455
Loss in iteration 88 : 0.003898562797565661
Loss in iteration 89 : 0.0038197032103959765
Loss in iteration 90 : 0.0037420706904628264
Loss in iteration 91 : 0.0036732651520345223
Loss in iteration 92 : 0.0036095721547135325
Loss in iteration 93 : 0.003546037294386384
Loss in iteration 94 : 0.003484946204141904
Loss in iteration 95 : 0.003426865098600845
Loss in iteration 96 : 0.0033699972774415303
Loss in iteration 97 : 0.0033137134962710024
Loss in iteration 98 : 0.0032586527122222663
Loss in iteration 99 : 0.0032044997632166166
Loss in iteration 100 : 0.0031519571650711516
Loss in iteration 101 : 0.003101567078009375
Loss in iteration 102 : 0.003052672615251018
Loss in iteration 103 : 0.0030066938061695464
Loss in iteration 104 : 0.002961978151312324
Loss in iteration 105 : 0.0029202246319480246
Loss in iteration 106 : 0.0028799169211086192
Loss in iteration 107 : 0.0028403934261623455
Loss in iteration 108 : 0.002802985478017265
Loss in iteration 109 : 0.002765991532866821
Loss in iteration 110 : 0.002728865070780099
Loss in iteration 111 : 0.0026916125046878496
Loss in iteration 112 : 0.0026565262450087283
Loss in iteration 113 : 0.0026227116275291096
Loss in iteration 114 : 0.00259046439458389
Loss in iteration 115 : 0.0025585109668273755
Loss in iteration 116 : 0.002526787177884982
Loss in iteration 117 : 0.002495813439832024
Loss in iteration 118 : 0.0024658230998472414
Loss in iteration 119 : 0.002437312922287806
Loss in iteration 120 : 0.0024114221186759943
Loss in iteration 121 : 0.0023866819361959063
Loss in iteration 122 : 0.002362060887060784
Loss in iteration 123 : 0.002337515633772359
Loss in iteration 124 : 0.0023130635136873437
Loss in iteration 125 : 0.002288832433213237
Loss in iteration 126 : 0.002264603434934359
Loss in iteration 127 : 0.002240374205296949
Loss in iteration 128 : 0.0022161426601718485
Loss in iteration 129 : 0.002192328176260932
Loss in iteration 130 : 0.0021691822000882996
Loss in iteration 131 : 0.0021467913677695244
Loss in iteration 132 : 0.0021249400735410297
Loss in iteration 133 : 0.002103179508132529
Loss in iteration 134 : 0.0020847730487949345
Loss in iteration 135 : 0.0020675487624848173
Loss in iteration 136 : 0.0020524172527776373
Testing accuracy  of updater 6 on alg 1 with rate 0.01999999999999999 = 0.9848888888888889, training accuracy 0.9994284081166047, time elapsed: 2727 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 12.82320512627665
Loss in iteration 3 : 4.575331828105403
Loss in iteration 4 : 2.0607134843027333
Loss in iteration 5 : 4.001585228518579
Loss in iteration 6 : 4.436831662214039
Loss in iteration 7 : 3.227517190728091
Loss in iteration 8 : 2.1327430909611516
Loss in iteration 9 : 1.361892312236094
Loss in iteration 10 : 0.9169045005347088
Loss in iteration 11 : 0.7803864058280233
Loss in iteration 12 : 0.8267135603507789
Loss in iteration 13 : 0.9184397414752796
Loss in iteration 14 : 0.9889612779074004
Loss in iteration 15 : 0.9494286270042726
Loss in iteration 16 : 0.824603165739307
Loss in iteration 17 : 0.6700727982027014
Loss in iteration 18 : 0.5180862145154211
Loss in iteration 19 : 0.3827191951253992
Loss in iteration 20 : 0.27565813147201595
Loss in iteration 21 : 0.23014951826287738
Loss in iteration 22 : 0.20824563074137564
Loss in iteration 23 : 0.20012911329009114
Loss in iteration 24 : 0.1956439862260766
Loss in iteration 25 : 0.19493986818994954
Loss in iteration 26 : 0.1933548422982237
Loss in iteration 27 : 0.19035417706098198
Loss in iteration 28 : 0.18629752839258737
Loss in iteration 29 : 0.18197970493915414
Loss in iteration 30 : 0.17745037468610872
Loss in iteration 31 : 0.1711889211825577
Loss in iteration 32 : 0.16454820815666302
Loss in iteration 33 : 0.15791166731584413
Loss in iteration 34 : 0.15091587372693405
Loss in iteration 35 : 0.14376966430683974
Loss in iteration 36 : 0.1377102202610208
Loss in iteration 37 : 0.1324350817384161
Loss in iteration 38 : 0.12745970924572098
Loss in iteration 39 : 0.12313275235218327
Loss in iteration 40 : 0.11903635218981527
Loss in iteration 41 : 0.11598029925982306
Loss in iteration 42 : 0.1139927052196788
Loss in iteration 43 : 0.11265245038119749
Loss in iteration 44 : 0.1112653230312593
Loss in iteration 45 : 0.1098674268581769
Loss in iteration 46 : 0.1090439066540181
Loss in iteration 47 : 0.10820215291886694
Loss in iteration 48 : 0.10756439833243121
Loss in iteration 49 : 0.10668082343036053
Loss in iteration 50 : 0.10557574786693535
Loss in iteration 51 : 0.10427103004179868
Loss in iteration 52 : 0.10278632073783436
Loss in iteration 53 : 0.1011392897551266
Loss in iteration 54 : 0.09934582857817888
Loss in iteration 55 : 0.09767289236962923
Loss in iteration 56 : 0.09623754869690053
Loss in iteration 57 : 0.09485698136683092
Loss in iteration 58 : 0.09383710635398596
Loss in iteration 59 : 0.09284995570562932
Loss in iteration 60 : 0.0918569193207505
Loss in iteration 61 : 0.09085847833182645
Loss in iteration 62 : 0.08985506577137034
Testing accuracy  of updater 7 on alg 1 with rate 20.0 = 0.9804444444444445, training accuracy 0.9989997142040583, time elapsed: 1528 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.2777443280196286
Loss in iteration 3 : 0.3903253449872603
Loss in iteration 4 : 0.4653998348662558
Loss in iteration 5 : 0.6642232016488244
Loss in iteration 6 : 0.3961771032554669
Loss in iteration 7 : 0.20239674320877785
Loss in iteration 8 : 0.12696903579679278
Loss in iteration 9 : 0.1523436264250078
Loss in iteration 10 : 0.19251934337870225
Loss in iteration 11 : 0.18168759086458144
Loss in iteration 12 : 0.13301630103337933
Loss in iteration 13 : 0.08159768378436626
Loss in iteration 14 : 0.05064624229467398
Loss in iteration 15 : 0.04182725895432238
Loss in iteration 16 : 0.04174913491160658
Loss in iteration 17 : 0.04298926941957505
Loss in iteration 18 : 0.04656728753948462
Loss in iteration 19 : 0.04892804733261273
Loss in iteration 20 : 0.048543968776805314
Loss in iteration 21 : 0.046480164214627
Loss in iteration 22 : 0.043914463317888515
Loss in iteration 23 : 0.040803519413127355
Loss in iteration 24 : 0.03800522559191147
Loss in iteration 25 : 0.035506624202347646
Loss in iteration 26 : 0.032825481665095586
Loss in iteration 27 : 0.031520369912085615
Loss in iteration 28 : 0.030816087358662505
Loss in iteration 29 : 0.03048769319679877
Loss in iteration 30 : 0.029908965292652794
Loss in iteration 31 : 0.029275421586301306
Loss in iteration 32 : 0.028506853913165497
Loss in iteration 33 : 0.027659269549683892
Loss in iteration 34 : 0.02687806893855884
Loss in iteration 35 : 0.026179695868149014
Loss in iteration 36 : 0.025456175603326536
Loss in iteration 37 : 0.02470990589584771
Loss in iteration 38 : 0.023943046762059238
Loss in iteration 39 : 0.02315754404705501
Loss in iteration 40 : 0.022355150652721506
Loss in iteration 41 : 0.021537445661353273
Loss in iteration 42 : 0.020825819792464794
Loss in iteration 43 : 0.02011595917574067
Loss in iteration 44 : 0.019403970535079924
Loss in iteration 45 : 0.0186899922120438
Loss in iteration 46 : 0.018023040951736742
Loss in iteration 47 : 0.01746713539787053
Loss in iteration 48 : 0.016925857408522826
Loss in iteration 49 : 0.016376993593062376
Loss in iteration 50 : 0.01586180643267284
Loss in iteration 51 : 0.01545041420162437
Loss in iteration 52 : 0.01505922434651622
Loss in iteration 53 : 0.014641198645891548
Loss in iteration 54 : 0.014214922323636197
Loss in iteration 55 : 0.013783872129372871
Loss in iteration 56 : 0.013311886281424285
Loss in iteration 57 : 0.012802937757756632
Loss in iteration 58 : 0.012295953738048976
Loss in iteration 59 : 0.011859275071490897
Loss in iteration 60 : 0.01142205658967813
Loss in iteration 61 : 0.011037121139703157
Loss in iteration 62 : 0.010716937792069836
Loss in iteration 63 : 0.01037641474443422
Loss in iteration 64 : 0.01007222940934655
Loss in iteration 65 : 0.009756842190613827
Loss in iteration 66 : 0.009424592040079675
Loss in iteration 67 : 0.009077101897344345
Loss in iteration 68 : 0.008715833821200968
Loss in iteration 69 : 0.008342104932870494
Loss in iteration 70 : 0.007957101779249904
Loss in iteration 71 : 0.007590448603027152
Loss in iteration 72 : 0.0072685581933934935
Loss in iteration 73 : 0.006975997022655817
Loss in iteration 74 : 0.00667804785964112
Loss in iteration 75 : 0.006342221050000365
Loss in iteration 76 : 0.00596761895461169
Loss in iteration 77 : 0.0056124688320833135
Loss in iteration 78 : 0.005264226259136731
Loss in iteration 79 : 0.004935114613809137
Loss in iteration 80 : 0.004616564073129552
Loss in iteration 81 : 0.004306156575939249
Loss in iteration 82 : 0.003977918247568953
Loss in iteration 83 : 0.0036332651317869235
Loss in iteration 84 : 0.0032881136013896205
Loss in iteration 85 : 0.00294517378043838
Loss in iteration 86 : 0.0026125412331528407
Loss in iteration 87 : 0.002274721034165943
Loss in iteration 88 : 0.0020337869677346473
Loss in iteration 89 : 0.001837526779237205
Loss in iteration 90 : 0.001634588156330678
Loss in iteration 91 : 0.0014317653255189451
Loss in iteration 92 : 0.0012294408105169696
Loss in iteration 93 : 0.0010420605109787379
Loss in iteration 94 : 8.632218113175542E-4
Loss in iteration 95 : 6.85991938260762E-4
Loss in iteration 96 : 5.101953284637462E-4
Loss in iteration 97 : 3.356738006087532E-4
Loss in iteration 98 : 1.7334498334480532E-4
Testing accuracy  of updater 7 on alg 1 with rate 14.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 2850 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8210553137429547
Loss in iteration 3 : 0.23772231582047687
Loss in iteration 4 : 0.3551001170117803
Loss in iteration 5 : 0.36649313208706663
Loss in iteration 6 : 0.19362671802945514
Loss in iteration 7 : 0.10409041048077523
Loss in iteration 8 : 0.10224566139265812
Loss in iteration 9 : 0.1319175708321406
Loss in iteration 10 : 0.12773151574287703
Loss in iteration 11 : 0.08620496242832656
Loss in iteration 12 : 0.05159500777171949
Loss in iteration 13 : 0.0350690172520406
Loss in iteration 14 : 0.031213455543347673
Loss in iteration 15 : 0.0329967275896354
Loss in iteration 16 : 0.03415936683528007
Loss in iteration 17 : 0.034376189844237605
Loss in iteration 18 : 0.03247651058898695
Loss in iteration 19 : 0.029129911830500607
Loss in iteration 20 : 0.026624712681972713
Loss in iteration 21 : 0.024975207998015773
Loss in iteration 22 : 0.023424926933388577
Loss in iteration 23 : 0.021996176326792648
Loss in iteration 24 : 0.021148904967831183
Loss in iteration 25 : 0.02105719696920054
Loss in iteration 26 : 0.021257962613489972
Loss in iteration 27 : 0.021062050855826776
Loss in iteration 28 : 0.020508394910680797
Loss in iteration 29 : 0.019691074768565145
Loss in iteration 30 : 0.018794071728436026
Loss in iteration 31 : 0.01792983332573226
Loss in iteration 32 : 0.01715654473354879
Loss in iteration 33 : 0.016654579037130135
Loss in iteration 34 : 0.01615771959993296
Loss in iteration 35 : 0.015655199869396078
Loss in iteration 36 : 0.015147525821921545
Loss in iteration 37 : 0.014699816611609124
Loss in iteration 38 : 0.01428777877637438
Loss in iteration 39 : 0.013874680872095087
Loss in iteration 40 : 0.013518094943335914
Loss in iteration 41 : 0.013186663515203317
Loss in iteration 42 : 0.012897245335445359
Loss in iteration 43 : 0.012606421389899472
Loss in iteration 44 : 0.012343154348205588
Loss in iteration 45 : 0.012070185479027494
Loss in iteration 46 : 0.011842675317529852
Loss in iteration 47 : 0.011585369643021816
Loss in iteration 48 : 0.011287546987560761
Loss in iteration 49 : 0.010953156465635907
Loss in iteration 50 : 0.010594863806064624
Loss in iteration 51 : 0.010232666382748416
Loss in iteration 52 : 0.009857295787581072
Loss in iteration 53 : 0.009507060867646834
Loss in iteration 54 : 0.009153838765940893
Loss in iteration 55 : 0.008822737459495349
Loss in iteration 56 : 0.00857375723569285
Loss in iteration 57 : 0.00837170859547481
Loss in iteration 58 : 0.008162520837526867
Loss in iteration 59 : 0.007946874252839442
Loss in iteration 60 : 0.007725381688962286
Loss in iteration 61 : 0.007498595233561468
Loss in iteration 62 : 0.007267012235641832
Loss in iteration 63 : 0.007031080730071728
Loss in iteration 64 : 0.006791204324542166
Loss in iteration 65 : 0.006547746602232737
Loss in iteration 66 : 0.006301035088177239
Loss in iteration 67 : 0.006051364822566056
Loss in iteration 68 : 0.005799001579937415
Loss in iteration 69 : 0.005544184769349705
Loss in iteration 70 : 0.005287238669855533
Loss in iteration 71 : 0.005100660300096707
Loss in iteration 72 : 0.004922413330372009
Loss in iteration 73 : 0.004754276282025996
Loss in iteration 74 : 0.004587309952143496
Loss in iteration 75 : 0.0043974581506281965
Loss in iteration 76 : 0.0041840693223664455
Loss in iteration 77 : 0.003963336607084546
Loss in iteration 78 : 0.0037595248892030777
Loss in iteration 79 : 0.003582926125122015
Loss in iteration 80 : 0.003404606469258252
Loss in iteration 81 : 0.0032152720656213806
Loss in iteration 82 : 0.003015985751012732
Loss in iteration 83 : 0.0028234960792235344
Loss in iteration 84 : 0.0026297014599415513
Loss in iteration 85 : 0.0024406745400928063
Loss in iteration 86 : 0.0022591222119530212
Loss in iteration 87 : 0.0020781212951929452
Loss in iteration 88 : 0.001888019168807778
Loss in iteration 89 : 0.0016896906891693687
Loss in iteration 90 : 0.001483923988078871
Loss in iteration 91 : 0.001314663737565212
Loss in iteration 92 : 0.001171098527735317
Loss in iteration 93 : 0.0010464674195039137
Loss in iteration 94 : 9.418948491298563E-4
Loss in iteration 95 : 8.378667464974009E-4
Loss in iteration 96 : 7.28710132973678E-4
Loss in iteration 97 : 6.26484000716579E-4
Loss in iteration 98 : 5.293270589065665E-4
Loss in iteration 99 : 4.308895785800696E-4
Loss in iteration 100 : 3.298001679239208E-4
Loss in iteration 101 : 2.263092114293032E-4
Loss in iteration 102 : 1.2064226827804656E-4
Loss in iteration 103 : 6.175782724727785E-5
Testing accuracy  of updater 7 on alg 1 with rate 8.0 = 0.9928888888888889, training accuracy 1.0, time elapsed: 2794 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.37075599031778544
Loss in iteration 3 : 0.1354240559864231
Loss in iteration 4 : 0.23856367643233042
Loss in iteration 5 : 0.0940106902788636
Loss in iteration 6 : 0.09412512694795978
Loss in iteration 7 : 0.09437505134950028
Loss in iteration 8 : 0.07382172006676671
Loss in iteration 9 : 0.048638752974292127
Loss in iteration 10 : 0.035150051106164044
Loss in iteration 11 : 0.029356883826126376
Loss in iteration 12 : 0.026759467811531767
Loss in iteration 13 : 0.024166507988775468
Loss in iteration 14 : 0.021662018030899947
Loss in iteration 15 : 0.01911923455610639
Loss in iteration 16 : 0.016124334363501355
Loss in iteration 17 : 0.01374501720075378
Loss in iteration 18 : 0.01220453930094894
Loss in iteration 19 : 0.011189786777008496
Loss in iteration 20 : 0.010346322742419421
Loss in iteration 21 : 0.010026636952577394
Loss in iteration 22 : 0.009972216542306548
Loss in iteration 23 : 0.010051582647259638
Loss in iteration 24 : 0.010130418225027141
Loss in iteration 25 : 0.010107078968478764
Loss in iteration 26 : 0.0099467551915881
Loss in iteration 27 : 0.009650192238302003
Loss in iteration 28 : 0.009230739008865618
Loss in iteration 29 : 0.008700421387104014
Loss in iteration 30 : 0.00811418564672497
Loss in iteration 31 : 0.007612970751813332
Loss in iteration 32 : 0.007247807055438801
Loss in iteration 33 : 0.006993074406889805
Loss in iteration 34 : 0.006763128007057472
Loss in iteration 35 : 0.006567502958665895
Loss in iteration 36 : 0.006374385241525319
Loss in iteration 37 : 0.006217918739335899
Loss in iteration 38 : 0.0061195947080306045
Loss in iteration 39 : 0.006026488785503742
Loss in iteration 40 : 0.005925228430357618
Loss in iteration 41 : 0.005855031963438815
Loss in iteration 42 : 0.005792201701129237
Loss in iteration 43 : 0.005724570209674983
Loss in iteration 44 : 0.005652602247868629
Loss in iteration 45 : 0.005576716506275374
Loss in iteration 46 : 0.0054972901725418
Loss in iteration 47 : 0.0054146630442866535
Loss in iteration 48 : 0.005329141234407825
Loss in iteration 49 : 0.0052410005091967585
Loss in iteration 50 : 0.005150489295648696
Loss in iteration 51 : 0.005057831390751187
Loss in iteration 52 : 0.004964268319146807
Loss in iteration 53 : 0.004876547869021723
Loss in iteration 54 : 0.004792098809143141
Loss in iteration 55 : 0.004713784136650074
Loss in iteration 56 : 0.0046355807682901574
Loss in iteration 57 : 0.004557470016657444
Loss in iteration 58 : 0.004482872330814107
Loss in iteration 59 : 0.004416562166602722
Loss in iteration 60 : 0.004347874203301255
Loss in iteration 61 : 0.0042767700138970684
Loss in iteration 62 : 0.0042034798118463996
Loss in iteration 63 : 0.004128210987417762
Loss in iteration 64 : 0.00405115036944003
Loss in iteration 65 : 0.003972466262913321
Loss in iteration 66 : 0.0038923102846944763
Loss in iteration 67 : 0.0038108190172677398
Loss in iteration 68 : 0.0037311263949711803
Loss in iteration 69 : 0.003653233067054881
Loss in iteration 70 : 0.0035764176078121055
Loss in iteration 71 : 0.003510226704002925
Loss in iteration 72 : 0.0034424776687290355
Loss in iteration 73 : 0.0033775106703668343
Loss in iteration 74 : 0.003323473305079408
Loss in iteration 75 : 0.003269493075616683
Loss in iteration 76 : 0.003215589244653396
Loss in iteration 77 : 0.003161312660154061
Loss in iteration 78 : 0.003108985376814649
Loss in iteration 79 : 0.0030576446593767936
Loss in iteration 80 : 0.003006460224692352
Loss in iteration 81 : 0.0029554116488086897
Loss in iteration 82 : 0.0029045252584973635
Loss in iteration 83 : 0.0028541539633732058
Loss in iteration 84 : 0.0028037604167203893
Loss in iteration 85 : 0.002753614750126622
Loss in iteration 86 : 0.0027037313503752033
Loss in iteration 87 : 0.002653844697305268
Loss in iteration 88 : 0.0026039306001024026
Loss in iteration 89 : 0.0025544875464102498
Loss in iteration 90 : 0.002504901230541232
Loss in iteration 91 : 0.0024551346825655625
Loss in iteration 92 : 0.0024056234636638226
Loss in iteration 93 : 0.002356267754406098
Loss in iteration 94 : 0.0023068020532894156
Loss in iteration 95 : 0.0022634596458421495
Loss in iteration 96 : 0.0022227779513403732
Loss in iteration 97 : 0.0021822004593343106
Loss in iteration 98 : 0.002141306979966627
Loss in iteration 99 : 0.002100124485778398
Loss in iteration 100 : 0.0020586772720103167
Loss in iteration 101 : 0.0020169872219163876
Loss in iteration 102 : 0.0019757656578158972
Loss in iteration 103 : 0.0019332141590770189
Loss in iteration 104 : 0.0018908124216845269
Loss in iteration 105 : 0.0018484883268990024
Loss in iteration 106 : 0.0018059864774112929
Loss in iteration 107 : 0.0017633201121414694
Loss in iteration 108 : 0.001720501153663761
Loss in iteration 109 : 0.0016775403386508512
Loss in iteration 110 : 0.0016344473353912684
Loss in iteration 111 : 0.0015912733188048517
Loss in iteration 112 : 0.0015482095661406064
Loss in iteration 113 : 0.0015050373188106687
Loss in iteration 114 : 0.0014618730259629588
Loss in iteration 115 : 0.0014187359763499158
Loss in iteration 116 : 0.0013754907463949394
Loss in iteration 117 : 0.001332110663216139
Loss in iteration 118 : 0.0012889610846127766
Loss in iteration 119 : 0.0012454641024503587
Loss in iteration 120 : 0.0012019189230005997
Loss in iteration 121 : 0.0011584774835874576
Loss in iteration 122 : 0.0011148984665869954
Loss in iteration 123 : 0.0010711910587820417
Loss in iteration 124 : 0.001027363532102042
Loss in iteration 125 : 9.84020852584037E-4
Loss in iteration 126 : 9.402728085320378E-4
Loss in iteration 127 : 8.961419689186376E-4
Loss in iteration 128 : 8.523016210320365E-4
Loss in iteration 129 : 8.085424211161732E-4
Loss in iteration 130 : 7.6464143224412E-4
Loss in iteration 131 : 7.206082228046831E-4
Loss in iteration 132 : 6.786989992257363E-4
Loss in iteration 133 : 6.360397456500104E-4
Loss in iteration 134 : 5.927752086120428E-4
Loss in iteration 135 : 5.491663880154999E-4
Loss in iteration 136 : 5.047328901225613E-4
Loss in iteration 137 : 4.598991740721693E-4
Loss in iteration 138 : 4.163267701860677E-4
Loss in iteration 139 : 3.7229717696609564E-4
Loss in iteration 140 : 3.295158971847203E-4
Loss in iteration 141 : 2.9940841313702885E-4
Loss in iteration 142 : 2.691864279203927E-4
Loss in iteration 143 : 2.4250636059973247E-4
Loss in iteration 144 : 2.1712985915585292E-4
Testing accuracy  of updater 7 on alg 1 with rate 2.0 = 0.9973333333333333, training accuracy 1.0, time elapsed: 3083 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.321194380215536
Loss in iteration 3 : 0.19127164394248916
Loss in iteration 4 : 0.1012235922045692
Loss in iteration 5 : 0.08024050390330661
Loss in iteration 6 : 0.06521173097427109
Loss in iteration 7 : 0.05481542329232874
Loss in iteration 8 : 0.04552557084328417
Loss in iteration 9 : 0.0368284864378542
Loss in iteration 10 : 0.03007061236129074
Loss in iteration 11 : 0.02461363196883655
Loss in iteration 12 : 0.02064798779043102
Loss in iteration 13 : 0.017600305900404893
Loss in iteration 14 : 0.015464546136967272
Loss in iteration 15 : 0.013542788204789071
Loss in iteration 16 : 0.012168030832020931
Loss in iteration 17 : 0.011062790073215643
Loss in iteration 18 : 0.010143931455790355
Loss in iteration 19 : 0.009319212390903574
Loss in iteration 20 : 0.00850434115130644
Loss in iteration 21 : 0.007787423201147132
Loss in iteration 22 : 0.007224538902955886
Loss in iteration 23 : 0.0068341350412129106
Loss in iteration 24 : 0.006468768835416519
Loss in iteration 25 : 0.006160196503201121
Loss in iteration 26 : 0.0058699121761458025
Loss in iteration 27 : 0.005610279876953628
Loss in iteration 28 : 0.005434685470742736
Loss in iteration 29 : 0.005337201887235195
Loss in iteration 30 : 0.005251483351335094
Loss in iteration 31 : 0.005154154867641632
Loss in iteration 32 : 0.005050907538012591
Loss in iteration 33 : 0.00493951063911198
Loss in iteration 34 : 0.0048150982819998665
Loss in iteration 35 : 0.004719128584456007
Loss in iteration 36 : 0.004636209745644197
Loss in iteration 37 : 0.0045523462238904544
Loss in iteration 38 : 0.004475881940375071
Loss in iteration 39 : 0.004405885298788024
Loss in iteration 40 : 0.004339983705814566
Loss in iteration 41 : 0.004274203433136408
Loss in iteration 42 : 0.004213791208377848
Loss in iteration 43 : 0.004158104820097621
Loss in iteration 44 : 0.004102707331702374
Loss in iteration 45 : 0.0040475648786679405
Loss in iteration 46 : 0.003992646947173951
Loss in iteration 47 : 0.003937926042046849
Loss in iteration 48 : 0.0038833773876094687
Loss in iteration 49 : 0.003832831736354884
Loss in iteration 50 : 0.003782186363630289
Loss in iteration 51 : 0.0037307574364465568
Loss in iteration 52 : 0.0036786167960711396
Loss in iteration 53 : 0.0036258291585525193
Loss in iteration 54 : 0.003572872143601443
Loss in iteration 55 : 0.003521146876552095
Loss in iteration 56 : 0.003469502453309253
Loss in iteration 57 : 0.0034179258121908316
Loss in iteration 58 : 0.003366405180860484
Loss in iteration 59 : 0.0033149299485495417
Loss in iteration 60 : 0.003263490550941636
Loss in iteration 61 : 0.003212078366465151
Loss in iteration 62 : 0.003160685622862987
Loss in iteration 63 : 0.003109305313021185
Loss in iteration 64 : 0.00305806517147269
Loss in iteration 65 : 0.0030071375704551
Loss in iteration 66 : 0.00295591089349677
Loss in iteration 67 : 0.0029089346746214873
Loss in iteration 68 : 0.002869182183167898
Loss in iteration 69 : 0.0028299946646994367
Loss in iteration 70 : 0.002791244397354807
Loss in iteration 71 : 0.0027528846002952585
Loss in iteration 72 : 0.0027148731251679173
Loss in iteration 73 : 0.002677171997027151
Loss in iteration 74 : 0.002639747000750213
Loss in iteration 75 : 0.0026025673084387126
Loss in iteration 76 : 0.0025656051437442785
Loss in iteration 77 : 0.0025288354794592133
Loss in iteration 78 : 0.0024922357650756705
Loss in iteration 79 : 0.0024557856813434494
Loss in iteration 80 : 0.002419466919150879
Loss in iteration 81 : 0.002383262980318441
Loss in iteration 82 : 0.0023471589981335123
Loss in iteration 83 : 0.00231114157566999
Loss in iteration 84 : 0.0022751986401302686
Loss in iteration 85 : 0.002239319311621808
Loss in iteration 86 : 0.002203493784937805
Loss in iteration 87 : 0.0021677132230533054
Loss in iteration 88 : 0.0021319696611757483
Loss in iteration 89 : 0.0020962559203040036
Loss in iteration 90 : 0.002060565529353636
Loss in iteration 91 : 0.0020248926549994786
Loss in iteration 92 : 0.0019892320384707445
Loss in iteration 93 : 0.0019535789386096714
Loss in iteration 94 : 0.0019179290805730166
Loss in iteration 95 : 0.0018840072059912985
Loss in iteration 96 : 0.001851562405622993
Loss in iteration 97 : 0.0018194844275946905
Loss in iteration 98 : 0.0017909570832940246
Loss in iteration 99 : 0.0017623054174442803
Loss in iteration 100 : 0.0017335387958889912
Loss in iteration 101 : 0.0017046656533677667
Loss in iteration 102 : 0.001675693585784893
Loss in iteration 103 : 0.0016466294333341927
Loss in iteration 104 : 0.0016174793553862875
Loss in iteration 105 : 0.0015882488979545922
Loss in iteration 106 : 0.0015589430544754928
Loss in iteration 107 : 0.001529566320565252
Loss in iteration 108 : 0.001500122743350562
Loss in iteration 109 : 0.0014706159659104725
Loss in iteration 110 : 0.0014410492673141663
Loss in iteration 111 : 0.0014114255986910186
Loss in iteration 112 : 0.0013817476157261528
Loss in iteration 113 : 0.0013520177079357
Loss in iteration 114 : 0.0013222380250409148
Loss in iteration 115 : 0.001292410500728628
Loss in iteration 116 : 0.0012625368740570699
Loss in iteration 117 : 0.0012326187087403728
Loss in iteration 118 : 0.001202657410522015
Loss in iteration 119 : 0.0011726542428265544
Loss in iteration 120 : 0.0011426103408602919
Loss in iteration 121 : 0.001112526724314568
Loss in iteration 122 : 0.0010824043088101742
Loss in iteration 123 : 0.0010522439162076213
Loss in iteration 124 : 0.0010220462838956793
Loss in iteration 125 : 9.918120731594179E-4
Loss in iteration 126 : 9.618011721277299E-4
Loss in iteration 127 : 9.334505340331549E-4
Testing accuracy  of updater 7 on alg 1 with rate 1.4 = 0.9991111111111111, training accuracy 0.9995713060874536, time elapsed: 2767 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.4345222646481675
Loss in iteration 3 : 0.22128156462014226
Loss in iteration 4 : 0.164191957522936
Loss in iteration 5 : 0.09655207168439447
Loss in iteration 6 : 0.10369065210591115
Loss in iteration 7 : 0.09162259897931241
Loss in iteration 8 : 0.06781075412371948
Loss in iteration 9 : 0.05291168412589211
Loss in iteration 10 : 0.04356684391035534
Loss in iteration 11 : 0.037391990498446624
Loss in iteration 12 : 0.034185656486670775
Loss in iteration 13 : 0.031241553925397077
Loss in iteration 14 : 0.02745454560185706
Loss in iteration 15 : 0.023513301677147685
Loss in iteration 16 : 0.01941462374222271
Loss in iteration 17 : 0.015767701138123258
Loss in iteration 18 : 0.01303749236501138
Loss in iteration 19 : 0.01092338769081688
Loss in iteration 20 : 0.009479728756479957
Loss in iteration 21 : 0.008587075782233972
Loss in iteration 22 : 0.007918143101804503
Loss in iteration 23 : 0.007324559761981168
Loss in iteration 24 : 0.006869646259449134
Loss in iteration 25 : 0.0065407261533828726
Loss in iteration 26 : 0.006241850636892022
Loss in iteration 27 : 0.005952230670067075
Loss in iteration 28 : 0.005706794255607593
Loss in iteration 29 : 0.005490596087961461
Loss in iteration 30 : 0.005303406392790282
Loss in iteration 31 : 0.005121607306944518
Loss in iteration 32 : 0.004941521711383012
Loss in iteration 33 : 0.004759064737797664
Loss in iteration 34 : 0.00459251134806627
Loss in iteration 35 : 0.004464897727683529
Loss in iteration 36 : 0.0043451844780044305
Loss in iteration 37 : 0.004258008175432686
Loss in iteration 38 : 0.004165344039505897
Loss in iteration 39 : 0.0040881284447295396
Loss in iteration 40 : 0.004013457854297071
Loss in iteration 41 : 0.003930714014740048
Loss in iteration 42 : 0.0038477524385353095
Loss in iteration 43 : 0.0037646364406181245
Loss in iteration 44 : 0.0036884700314682778
Loss in iteration 45 : 0.0036126957614009254
Loss in iteration 46 : 0.0035379864390740526
Loss in iteration 47 : 0.003465885174609826
Loss in iteration 48 : 0.0033947974762175035
Loss in iteration 49 : 0.0033298721195647346
Loss in iteration 50 : 0.0032915381454729927
Loss in iteration 51 : 0.0032530801706456705
Loss in iteration 52 : 0.0032142974112389726
Loss in iteration 53 : 0.0031752179130368017
Loss in iteration 54 : 0.0031358669383669264
Loss in iteration 55 : 0.0030962672419348187
Loss in iteration 56 : 0.003056439319322623
Loss in iteration 57 : 0.003016447042263134
Loss in iteration 58 : 0.002979895188632614
Loss in iteration 59 : 0.0029424687368141654
Loss in iteration 60 : 0.0029042498573449787
Loss in iteration 61 : 0.002866839211990199
Loss in iteration 62 : 0.002831102207133041
Loss in iteration 63 : 0.00279627175932685
Loss in iteration 64 : 0.0027641483779784733
Loss in iteration 65 : 0.0027341352500159333
Loss in iteration 66 : 0.002703951028761397
Loss in iteration 67 : 0.0026736095208902146
Loss in iteration 68 : 0.0026431231616969755
Loss in iteration 69 : 0.0026125031509949604
Loss in iteration 70 : 0.0025822515174901057
Loss in iteration 71 : 0.0025512466491162057
Loss in iteration 72 : 0.0025195366382399537
Loss in iteration 73 : 0.0024879073432340776
Loss in iteration 74 : 0.002456330832614222
Loss in iteration 75 : 0.002424709708148077
Loss in iteration 76 : 0.002393045214033021
Loss in iteration 77 : 0.002361338467987418
Loss in iteration 78 : 0.0023298203861776665
Loss in iteration 79 : 0.0023039203040338425
Testing accuracy  of updater 7 on alg 1 with rate 0.8 = 0.9911111111111112, training accuracy 0.9994284081166047, time elapsed: 1811 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8540086232315556
Loss in iteration 3 : 0.5764768801966608
Loss in iteration 4 : 0.3570445329965255
Loss in iteration 5 : 0.2395315521830271
Loss in iteration 6 : 0.197159511116034
Loss in iteration 7 : 0.16363650701923418
Loss in iteration 8 : 0.12726597734839437
Loss in iteration 9 : 0.10468623901459789
Loss in iteration 10 : 0.1023353720702355
Loss in iteration 11 : 0.09851679626164875
Loss in iteration 12 : 0.08346755613004245
Loss in iteration 13 : 0.06730841438947442
Loss in iteration 14 : 0.055484570444434035
Loss in iteration 15 : 0.048677828052225126
Loss in iteration 16 : 0.04457823890215353
Loss in iteration 17 : 0.04130957770898945
Loss in iteration 18 : 0.03796218916947951
Loss in iteration 19 : 0.03447980017124581
Loss in iteration 20 : 0.031017048439706173
Loss in iteration 21 : 0.027792807179297108
Loss in iteration 22 : 0.02495936478043381
Loss in iteration 23 : 0.022457886239660496
Loss in iteration 24 : 0.020176882807125478
Loss in iteration 25 : 0.018139658511806486
Loss in iteration 26 : 0.016386740631918948
Loss in iteration 27 : 0.014996443710218478
Loss in iteration 28 : 0.013748579956692673
Loss in iteration 29 : 0.012696791312599232
Loss in iteration 30 : 0.011783033042988353
Loss in iteration 31 : 0.01093778313243979
Loss in iteration 32 : 0.010199451721676868
Loss in iteration 33 : 0.009600153008109118
Loss in iteration 34 : 0.009027214558246658
Loss in iteration 35 : 0.008472604032151238
Loss in iteration 36 : 0.00793180771773364
Loss in iteration 37 : 0.007474485171127667
Loss in iteration 38 : 0.007128374326810347
Loss in iteration 39 : 0.006801824854706011
Loss in iteration 40 : 0.006508803220829719
Loss in iteration 41 : 0.00624905103915435
Loss in iteration 42 : 0.006017733179473919
Loss in iteration 43 : 0.0058214627127517
Loss in iteration 44 : 0.005629635594790205
Loss in iteration 45 : 0.005460336720383973
Loss in iteration 46 : 0.005319016006827929
Loss in iteration 47 : 0.00518692762001844
Loss in iteration 48 : 0.005053404903347931
Loss in iteration 49 : 0.00492149268165324
Loss in iteration 50 : 0.004790836673515064
Loss in iteration 51 : 0.0046656730206412825
Loss in iteration 52 : 0.004541037427011137
Loss in iteration 53 : 0.004417059292539156
Loss in iteration 54 : 0.004293896727589041
Loss in iteration 55 : 0.004172234110789787
Loss in iteration 56 : 0.004060629118219035
Loss in iteration 57 : 0.003951554174193545
Loss in iteration 58 : 0.0038455706489340775
Loss in iteration 59 : 0.003741116822290691
Loss in iteration 60 : 0.0036405821511988207
Loss in iteration 61 : 0.003551833576051748
Loss in iteration 62 : 0.003466145774553211
Loss in iteration 63 : 0.003385624705992865
Loss in iteration 64 : 0.0033113327788603653
Loss in iteration 65 : 0.003244679081991333
Loss in iteration 66 : 0.0031900708648876473
Loss in iteration 67 : 0.003135764380139466
Loss in iteration 68 : 0.0030816394805838177
Loss in iteration 69 : 0.0030276729659478485
Loss in iteration 70 : 0.0029738439298742894
Loss in iteration 71 : 0.0029201335325912655
Loss in iteration 72 : 0.002866524796110338
Loss in iteration 73 : 0.0028130024197202106
Loss in iteration 74 : 0.0027606075242767435
Loss in iteration 75 : 0.0027227921770643206
Loss in iteration 76 : 0.002689175287462142
Loss in iteration 77 : 0.002655634996966233
Loss in iteration 78 : 0.0026227466081642295
Loss in iteration 79 : 0.0025906895658819144
Loss in iteration 80 : 0.0025590927042824316
Loss in iteration 81 : 0.00252747616322198
Loss in iteration 82 : 0.0024957872225632124
Loss in iteration 83 : 0.0024640614808192347
Loss in iteration 84 : 0.002432755450083585
Loss in iteration 85 : 0.002401422730158929
Loss in iteration 86 : 0.002371081593758856
Loss in iteration 87 : 0.002341545544880152
Loss in iteration 88 : 0.002313108672400435
Loss in iteration 89 : 0.0022854620136499456
Loss in iteration 90 : 0.0022580808677077646
Loss in iteration 91 : 0.0022332149358123303
Loss in iteration 92 : 0.002209334055740517
Loss in iteration 93 : 0.0021869288431683735
Loss in iteration 94 : 0.002166312641397995
Loss in iteration 95 : 0.002146014355201232
Loss in iteration 96 : 0.0021267661393812314
Loss in iteration 97 : 0.0021107010206370927
Testing accuracy  of updater 7 on alg 1 with rate 0.19999999999999996 = 0.9911111111111112, training accuracy 0.9994284081166047, time elapsed: 2155 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 3.091274687760605
Loss in iteration 3 : 0.42826660979751713
Loss in iteration 4 : 0.4071873580487533
Loss in iteration 5 : 0.38116025367364764
Loss in iteration 6 : 0.3135679205221432
Loss in iteration 7 : 0.24932889828984867
Loss in iteration 8 : 0.1984320185698494
Loss in iteration 9 : 0.15705544828098184
Loss in iteration 10 : 0.13344594966752626
Loss in iteration 11 : 0.11767574191311127
Loss in iteration 12 : 0.10443538574350059
Loss in iteration 13 : 0.09157943450611931
Loss in iteration 14 : 0.07901934409058341
Loss in iteration 15 : 0.06852340091339322
Loss in iteration 16 : 0.060121978078172385
Loss in iteration 17 : 0.05289494710897352
Loss in iteration 18 : 0.04747483286044441
Loss in iteration 19 : 0.04341160971030872
Loss in iteration 20 : 0.03968237873712408
Loss in iteration 21 : 0.036160725798380075
Loss in iteration 22 : 0.033270337183662756
Loss in iteration 23 : 0.031137431778622375
Loss in iteration 24 : 0.029187141481932247
Loss in iteration 25 : 0.02754303543380148
Loss in iteration 26 : 0.02588788607572457
Loss in iteration 27 : 0.024956497582414677
Loss in iteration 28 : 0.024086681041314305
Loss in iteration 29 : 0.023258381252666764
Loss in iteration 30 : 0.022626834940223703
Loss in iteration 31 : 0.02213015520035737
Loss in iteration 32 : 0.021752872175031846
Loss in iteration 33 : 0.021376107995973028
Loss in iteration 34 : 0.02097525487886239
Loss in iteration 35 : 0.02055180206781351
Loss in iteration 36 : 0.020107114798887254
Loss in iteration 37 : 0.01964244511247526
Loss in iteration 38 : 0.019158941644320977
Loss in iteration 39 : 0.018720661665055736
Loss in iteration 40 : 0.018269206690080732
Loss in iteration 41 : 0.017812994487662243
Loss in iteration 42 : 0.017342481633488865
Loss in iteration 43 : 0.01685841836093174
Loss in iteration 44 : 0.016369894875868266
Loss in iteration 45 : 0.01590356708580519
Loss in iteration 46 : 0.01542117528474713
Loss in iteration 47 : 0.01493353559946099
Loss in iteration 48 : 0.014477328855515927
Loss in iteration 49 : 0.014013361760056943
Loss in iteration 50 : 0.01354191944589713
Loss in iteration 51 : 0.01306326477297131
Loss in iteration 52 : 0.01258097647283557
Loss in iteration 53 : 0.012087317873996853
Loss in iteration 54 : 0.011633545184252877
Loss in iteration 55 : 0.011177326534315257
Loss in iteration 56 : 0.010714978861237604
Loss in iteration 57 : 0.010254080448488004
Loss in iteration 58 : 0.00984268888921838
Loss in iteration 59 : 0.009497856297395496
Loss in iteration 60 : 0.009159956187370711
Loss in iteration 61 : 0.008812759463062137
Loss in iteration 62 : 0.008460374155563484
Loss in iteration 63 : 0.008108198389005202
Loss in iteration 64 : 0.007750070428920299
Loss in iteration 65 : 0.007386277396867169
Loss in iteration 66 : 0.007017081862141713
Loss in iteration 67 : 0.006642724007276343
Loss in iteration 68 : 0.006313697723752854
Loss in iteration 69 : 0.005996012724148811
Loss in iteration 70 : 0.005675343301338581
Loss in iteration 71 : 0.005351758726061913
Loss in iteration 72 : 0.005025323034656501
Loss in iteration 73 : 0.004696095481348354
Loss in iteration 74 : 0.0043641309510825425
Loss in iteration 75 : 0.0040294803362784844
Loss in iteration 76 : 0.0036921908806122872
Loss in iteration 77 : 0.003352306492669139
Loss in iteration 78 : 0.003009868032070534
Loss in iteration 79 : 0.0026654716937072128
Loss in iteration 80 : 0.0023246832607150087
Loss in iteration 81 : 0.002062025981474503
Loss in iteration 82 : 0.001795008386050731
Loss in iteration 83 : 0.0015293608371371782
Loss in iteration 84 : 0.0012630495804065557
Loss in iteration 85 : 0.0010072573689336635
Loss in iteration 86 : 7.835932299633016E-4
Loss in iteration 87 : 5.910793473100439E-4
Loss in iteration 88 : 3.8749912512566295E-4
Loss in iteration 89 : 2.1498376445524772E-4
Loss in iteration 90 : 1.1505627506547847E-4
Loss in iteration 91 : 2.7961652812919873E-5
Loss in iteration 92 : 1.335951678780276E-5
Loss in iteration 93 : 0.0
Loss in iteration 94 : 0.0
Loss in iteration 95 : 0.0
Loss in iteration 96 : 0.0
Loss in iteration 97 : 0.0
Loss in iteration 98 : 0.0
Testing accuracy  of updater 8 on alg 1 with rate 2.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1803 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.6497375376044408
Loss in iteration 3 : 0.24781781104111494
Loss in iteration 4 : 0.28162717845499485
Loss in iteration 5 : 0.2394816083731352
Loss in iteration 6 : 0.19122864408417894
Loss in iteration 7 : 0.14894913465016435
Loss in iteration 8 : 0.11656116700802185
Loss in iteration 9 : 0.09667880969442386
Loss in iteration 10 : 0.08349887000898822
Loss in iteration 11 : 0.07383716866721858
Loss in iteration 12 : 0.06440273379235771
Loss in iteration 13 : 0.05575637206546594
Loss in iteration 14 : 0.04970061288643275
Loss in iteration 15 : 0.04530913010941565
Loss in iteration 16 : 0.04137098752045449
Loss in iteration 17 : 0.037656387570512026
Loss in iteration 18 : 0.03413523479824503
Loss in iteration 19 : 0.031186245679616753
Loss in iteration 20 : 0.029079722884516587
Loss in iteration 21 : 0.02721389804116239
Loss in iteration 22 : 0.025525743770421758
Loss in iteration 23 : 0.0239776715134898
Loss in iteration 24 : 0.022831044597000638
Loss in iteration 25 : 0.02167535470019658
Loss in iteration 26 : 0.020741157112890905
Loss in iteration 27 : 0.01983880075744847
Loss in iteration 28 : 0.01910248042832833
Loss in iteration 29 : 0.0184717034958907
Loss in iteration 30 : 0.01784998902617491
Loss in iteration 31 : 0.017345492740070807
Loss in iteration 32 : 0.01688298899531326
Loss in iteration 33 : 0.01640829668097237
Loss in iteration 34 : 0.0159223751350277
Loss in iteration 35 : 0.015426092396551402
Loss in iteration 36 : 0.014920233915579593
Loss in iteration 37 : 0.014416098393797752
Loss in iteration 38 : 0.013927385464183727
Loss in iteration 39 : 0.013431788734487957
Loss in iteration 40 : 0.012960167506414601
Loss in iteration 41 : 0.012513623702000418
Loss in iteration 42 : 0.012063888687584224
Loss in iteration 43 : 0.011612080102062538
Loss in iteration 44 : 0.011174431206971161
Loss in iteration 45 : 0.010749143245116397
Loss in iteration 46 : 0.010351178432248403
Loss in iteration 47 : 0.009953597524000322
Loss in iteration 48 : 0.009555840579880433
Loss in iteration 49 : 0.00915208395648387
Loss in iteration 50 : 0.008752428696964387
Loss in iteration 51 : 0.00833621107408742
Loss in iteration 52 : 0.007909079420023309
Loss in iteration 53 : 0.0075672980763067214
Loss in iteration 54 : 0.007239834014430068
Loss in iteration 55 : 0.0069355757737663915
Loss in iteration 56 : 0.006653541652793252
Loss in iteration 57 : 0.0063689870681005395
Loss in iteration 58 : 0.006079483955599085
Loss in iteration 59 : 0.00578540887967185
Loss in iteration 60 : 0.0054871022375248485
Loss in iteration 61 : 0.0051848717470900614
Loss in iteration 62 : 0.004878995599222478
Loss in iteration 63 : 0.004569725306410532
Loss in iteration 64 : 0.004257288277137509
Loss in iteration 65 : 0.0039418901422433004
Loss in iteration 66 : 0.0036271686915915896
Loss in iteration 67 : 0.003385673688054759
Loss in iteration 68 : 0.0031322301167655223
Loss in iteration 69 : 0.002880117309060368
Loss in iteration 70 : 0.0026195411703606696
Loss in iteration 71 : 0.0023525073649103305
Loss in iteration 72 : 0.0021309206151643736
Loss in iteration 73 : 0.0019382578778603572
Loss in iteration 74 : 0.0017435286275661445
Loss in iteration 75 : 0.0015478345590068778
Loss in iteration 76 : 0.0013534420905116713
Loss in iteration 77 : 0.0011700008263514654
Loss in iteration 78 : 0.0010146379363132117
Loss in iteration 79 : 8.552663544411282E-4
Loss in iteration 80 : 6.921090054350923E-4
Loss in iteration 81 : 5.254778696225992E-4
Loss in iteration 82 : 3.9611001401706596E-4
Loss in iteration 83 : 2.825705362400453E-4
Loss in iteration 84 : 1.830931210520388E-4
Loss in iteration 85 : 1.1432300317570788E-4
Loss in iteration 86 : 5.107979690521236E-5
Loss in iteration 87 : 0.0
Loss in iteration 88 : 0.0
Loss in iteration 89 : 0.0
Loss in iteration 90 : 0.0
Loss in iteration 91 : 0.0
Testing accuracy  of updater 8 on alg 1 with rate 1.4000000000000001 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1974 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 1.1055796273540797
Loss in iteration 3 : 0.17857721556034595
Loss in iteration 4 : 0.2106846918362637
Loss in iteration 5 : 0.1619411741369199
Loss in iteration 6 : 0.13203522931499173
Loss in iteration 7 : 0.10458537734916538
Loss in iteration 8 : 0.08209300491916749
Loss in iteration 9 : 0.06704047490699355
Loss in iteration 10 : 0.05699694986984528
Loss in iteration 11 : 0.0499006267154405
Loss in iteration 12 : 0.043423311420658674
Loss in iteration 13 : 0.0374239135536252
Loss in iteration 14 : 0.03291669248996732
Loss in iteration 15 : 0.030242031728533132
Loss in iteration 16 : 0.027536604291371707
Loss in iteration 17 : 0.02486364450588111
Loss in iteration 18 : 0.02244954668523325
Loss in iteration 19 : 0.020347227467744183
Loss in iteration 20 : 0.01883480985657922
Loss in iteration 21 : 0.0175696265876119
Loss in iteration 22 : 0.016490308723988774
Loss in iteration 23 : 0.015567246466502033
Loss in iteration 24 : 0.01472969805665989
Loss in iteration 25 : 0.01400301561896574
Loss in iteration 26 : 0.013349014862537338
Loss in iteration 27 : 0.012745322162743858
Loss in iteration 28 : 0.012258922333173375
Loss in iteration 29 : 0.011862153848948561
Loss in iteration 30 : 0.011481143360717445
Loss in iteration 31 : 0.01118855928302162
Loss in iteration 32 : 0.010890819380283816
Loss in iteration 33 : 0.010587338526333508
Loss in iteration 34 : 0.010278594653605802
Loss in iteration 35 : 0.009965019192024416
Loss in iteration 36 : 0.00964700160000129
Loss in iteration 37 : 0.009327676573363714
Loss in iteration 38 : 0.009022236564191833
Loss in iteration 39 : 0.008749102134153038
Loss in iteration 40 : 0.008474604028935968
Loss in iteration 41 : 0.008198810500452545
Loss in iteration 42 : 0.00792178322571343
Loss in iteration 43 : 0.0076435779476247525
Loss in iteration 44 : 0.007370553431582505
Loss in iteration 45 : 0.0071148940901650266
Loss in iteration 46 : 0.006869661225138332
Loss in iteration 47 : 0.006621560319656747
Loss in iteration 48 : 0.006383256193437224
Loss in iteration 49 : 0.006142803968721909
Loss in iteration 50 : 0.005895180514478583
Loss in iteration 51 : 0.005641018720294309
Loss in iteration 52 : 0.005381838203668825
Loss in iteration 53 : 0.005178901189681534
Loss in iteration 54 : 0.004989123902924002
Loss in iteration 55 : 0.00480334531102914
Loss in iteration 56 : 0.004629963074070691
Loss in iteration 57 : 0.004455321338217418
Loss in iteration 58 : 0.00428241747849309
Loss in iteration 59 : 0.004106666812869424
Loss in iteration 60 : 0.00392830654054508
Loss in iteration 61 : 0.0037475506784634036
Loss in iteration 62 : 0.003564592330458077
Loss in iteration 63 : 0.0033796057344444
Loss in iteration 64 : 0.003192748109346734
Loss in iteration 65 : 0.0030053993800044482
Loss in iteration 66 : 0.002827872956940003
Loss in iteration 67 : 0.0026580561195638547
Loss in iteration 68 : 0.0024993570517645044
Loss in iteration 69 : 0.0023418984697189745
Loss in iteration 70 : 0.002185886006126194
Loss in iteration 71 : 0.0020318151034467036
Loss in iteration 72 : 0.001877108029374369
Loss in iteration 73 : 0.0017215182925197684
Loss in iteration 74 : 0.0015669717981306283
Loss in iteration 75 : 0.0014163901209112487
Loss in iteration 76 : 0.0012546692376949338
Loss in iteration 77 : 0.0011351496157401917
Loss in iteration 78 : 0.0010148598017464764
Loss in iteration 79 : 9.041668548658776E-4
Loss in iteration 80 : 8.109690868778523E-4
Loss in iteration 81 : 7.128165492216048E-4
Loss in iteration 82 : 6.149300164438497E-4
Loss in iteration 83 : 5.192175088300385E-4
Loss in iteration 84 : 4.25665501229128E-4
Loss in iteration 85 : 3.296439259654695E-4
Loss in iteration 86 : 2.3919515891976457E-4
Loss in iteration 87 : 1.782829625069246E-4
Loss in iteration 88 : 1.2820559849678503E-4
Loss in iteration 89 : 8.766720871290538E-5
Loss in iteration 90 : 4.899931947731607E-5
Loss in iteration 91 : 1.1919014327704456E-5
Loss in iteration 92 : 7.725318353445834E-6
Loss in iteration 93 : 0.0
Loss in iteration 94 : 0.0
Testing accuracy  of updater 8 on alg 1 with rate 0.8 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1825 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.41203413153646967
Loss in iteration 3 : 0.20140109643412293
Loss in iteration 4 : 0.27367301489638046
Loss in iteration 5 : 0.10882408729965778
Loss in iteration 6 : 0.06877312379655635
Loss in iteration 7 : 0.061087182569530024
Loss in iteration 8 : 0.050790134353569064
Loss in iteration 9 : 0.038300250548620976
Loss in iteration 10 : 0.029616701996735332
Loss in iteration 11 : 0.02438515489528517
Loss in iteration 12 : 0.02126970713200803
Loss in iteration 13 : 0.01919962211016938
Loss in iteration 14 : 0.017650581616380917
Loss in iteration 15 : 0.016086283050139472
Loss in iteration 16 : 0.014739407741925933
Loss in iteration 17 : 0.013518816764913075
Loss in iteration 18 : 0.012414967402980375
Loss in iteration 19 : 0.011435017316442428
Loss in iteration 20 : 0.01049309493411232
Loss in iteration 21 : 0.009626845496632229
Loss in iteration 22 : 0.008881975087867878
Loss in iteration 23 : 0.008224827537833185
Loss in iteration 24 : 0.007562368530391774
Loss in iteration 25 : 0.0069529742898579465
Loss in iteration 26 : 0.00651068629151488
Loss in iteration 27 : 0.006118985795778744
Loss in iteration 28 : 0.005737446400690668
Loss in iteration 29 : 0.005351928244716462
Loss in iteration 30 : 0.004993419124317233
Loss in iteration 31 : 0.00474280410866195
Loss in iteration 32 : 0.004517580745104226
Loss in iteration 33 : 0.004316614859822697
Loss in iteration 34 : 0.004157582355938734
Loss in iteration 35 : 0.004030107598152326
Loss in iteration 36 : 0.003926633745443697
Loss in iteration 37 : 0.0038326971503928856
Loss in iteration 38 : 0.0037606912654188853
Loss in iteration 39 : 0.003688860727999973
Loss in iteration 40 : 0.0036171762215095027
Loss in iteration 41 : 0.0035456113212152646
Loss in iteration 42 : 0.003474142210017393
Loss in iteration 43 : 0.0034098130168239405
Loss in iteration 44 : 0.003349312439442883
Loss in iteration 45 : 0.0032887581635021278
Loss in iteration 46 : 0.0032281451407368458
Loss in iteration 47 : 0.003167468827217351
Loss in iteration 48 : 0.0031067251337262587
Loss in iteration 49 : 0.003046532937527182
Loss in iteration 50 : 0.0029892882976521287
Loss in iteration 51 : 0.0029327773307348867
Loss in iteration 52 : 0.002875595113631472
Loss in iteration 53 : 0.0028169934508180472
Loss in iteration 54 : 0.0027578390757696013
Loss in iteration 55 : 0.0026991304830879053
Loss in iteration 56 : 0.0026440135740651203
Loss in iteration 57 : 0.002587873646450072
Loss in iteration 58 : 0.0025312174315981753
Loss in iteration 59 : 0.0024744044162918824
Loss in iteration 60 : 0.0024177793951112333
Loss in iteration 61 : 0.002360687803790065
Loss in iteration 62 : 0.00230316554939476
Loss in iteration 63 : 0.0022458975129076176
Loss in iteration 64 : 0.002189845825132546
Loss in iteration 65 : 0.0021383491168729327
Loss in iteration 66 : 0.002092213807460371
Loss in iteration 67 : 0.0020465633484707335
Loss in iteration 68 : 0.002001186553550213
Loss in iteration 69 : 0.001956384829215973
Loss in iteration 70 : 0.001911401503059019
Loss in iteration 71 : 0.0018702392823309558
Loss in iteration 72 : 0.001831706367801645
Loss in iteration 73 : 0.0017934773441348536
Loss in iteration 74 : 0.0017551542721064052
Loss in iteration 75 : 0.0017166478809065194
Loss in iteration 76 : 0.0016782246120411225
Loss in iteration 77 : 0.001639840645468604
Loss in iteration 78 : 0.001601629633814367
Loss in iteration 79 : 0.0015627297467993594
Loss in iteration 80 : 0.001524610782222061
Loss in iteration 81 : 0.0014860346177949773
Loss in iteration 82 : 0.0014473238817870684
Loss in iteration 83 : 0.001408847102688559
Loss in iteration 84 : 0.0013702363634772114
Loss in iteration 85 : 0.0013312120224120499
Loss in iteration 86 : 0.00129251716611197
Loss in iteration 87 : 0.0012543124691494043
Loss in iteration 88 : 0.0012159258892436536
Loss in iteration 89 : 0.0011769172503095643
Loss in iteration 90 : 0.0011379706448145332
Loss in iteration 91 : 0.001101238016086723
Loss in iteration 92 : 0.001063526522344459
Loss in iteration 93 : 0.0010255300636842464
Loss in iteration 94 : 9.88595695114931E-4
Loss in iteration 95 : 9.536561184622028E-4
Loss in iteration 96 : 9.191498793250222E-4
Loss in iteration 97 : 8.843943563562589E-4
Loss in iteration 98 : 8.494321181800929E-4
Loss in iteration 99 : 8.144633490847239E-4
Loss in iteration 100 : 7.793621551541905E-4
Loss in iteration 101 : 7.469415136374102E-4
Loss in iteration 102 : 7.134200031256547E-4
Loss in iteration 103 : 6.799308537788595E-4
Loss in iteration 104 : 6.457035058457732E-4
Loss in iteration 105 : 6.109424565381787E-4
Loss in iteration 106 : 5.756944540411189E-4
Loss in iteration 107 : 5.40576730198482E-4
Loss in iteration 108 : 5.055269370414083E-4
Loss in iteration 109 : 4.708140533797227E-4
Loss in iteration 110 : 4.364478759996074E-4
Loss in iteration 111 : 4.0177514502438524E-4
Loss in iteration 112 : 3.680381160866409E-4
Loss in iteration 113 : 3.328652809714203E-4
Loss in iteration 114 : 2.979013797200013E-4
Loss in iteration 115 : 2.6345023376740775E-4
Loss in iteration 116 : 2.284891815985047E-4
Loss in iteration 117 : 1.977316629070641E-4
Loss in iteration 118 : 1.736900633227913E-4
Loss in iteration 119 : 1.5019138412765038E-4
Loss in iteration 120 : 1.269968781935189E-4
Loss in iteration 121 : 1.1019087430056535E-4
Loss in iteration 122 : 9.632149034187029E-5
Loss in iteration 123 : 8.249742742764333E-5
Loss in iteration 124 : 6.87341970377797E-5
Loss in iteration 125 : 5.501694437783822E-5
Loss in iteration 126 : 4.260656115079019E-5
Testing accuracy  of updater 8 on alg 1 with rate 0.2 = 0.9955555555555555, training accuracy 1.0, time elapsed: 2487 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.33301673675663
Loss in iteration 3 : 0.2017707619411495
Loss in iteration 4 : 0.24933083336770664
Loss in iteration 5 : 0.12050208600960463
Loss in iteration 6 : 0.06047205443546034
Loss in iteration 7 : 0.05495843489458791
Loss in iteration 8 : 0.048219805235299076
Loss in iteration 9 : 0.03775525545544224
Loss in iteration 10 : 0.028405477092149637
Loss in iteration 11 : 0.0224836411074835
Loss in iteration 12 : 0.01916622126937832
Loss in iteration 13 : 0.017372720098621498
Loss in iteration 14 : 0.016068726511109074
Loss in iteration 15 : 0.014928982792791548
Loss in iteration 16 : 0.013830565857967577
Loss in iteration 17 : 0.012778213898300403
Loss in iteration 18 : 0.011664597120542596
Loss in iteration 19 : 0.010563215337292542
Loss in iteration 20 : 0.009614904466807035
Loss in iteration 21 : 0.008759248316899433
Loss in iteration 22 : 0.007979795985697485
Loss in iteration 23 : 0.007327889966833371
Loss in iteration 24 : 0.006777511807146611
Loss in iteration 25 : 0.006270001108323826
Loss in iteration 26 : 0.005827304615579928
Loss in iteration 27 : 0.005416582203368086
Loss in iteration 28 : 0.005041851766165488
Loss in iteration 29 : 0.004686402555888674
Loss in iteration 30 : 0.004340254327001412
Loss in iteration 31 : 0.004065272357582513
Loss in iteration 32 : 0.0038434677882107234
Loss in iteration 33 : 0.003642785754591895
Loss in iteration 34 : 0.0034647675191021737
Loss in iteration 35 : 0.0033228787145408003
Loss in iteration 36 : 0.003216172228650863
Loss in iteration 37 : 0.0031223696923962816
Loss in iteration 38 : 0.0030379245546249757
Loss in iteration 39 : 0.002968766845366633
Loss in iteration 40 : 0.0029073675205314076
Loss in iteration 41 : 0.0028530023759286657
Loss in iteration 42 : 0.0027991895403092753
Loss in iteration 43 : 0.0027478769359743204
Loss in iteration 44 : 0.002702769374556361
Loss in iteration 45 : 0.002657959844737168
Loss in iteration 46 : 0.002618758944266664
Loss in iteration 47 : 0.0025784703054307686
Loss in iteration 48 : 0.0025377309292175442
Loss in iteration 49 : 0.002496733868008044
Loss in iteration 50 : 0.0024548584372234756
Loss in iteration 51 : 0.0024139807145313343
Loss in iteration 52 : 0.0023733328492160934
Loss in iteration 53 : 0.0023326041854782134
Loss in iteration 54 : 0.0022918274924409207
Loss in iteration 55 : 0.0022522820044960394
Loss in iteration 56 : 0.002212516955862271
Loss in iteration 57 : 0.0021725486346235094
Loss in iteration 58 : 0.0021323917221235276
Loss in iteration 59 : 0.002092404255443158
Loss in iteration 60 : 0.0020535022997797696
Loss in iteration 61 : 0.0020143764479820625
Loss in iteration 62 : 0.001975043555539649
Loss in iteration 63 : 0.0019355607567910656
Loss in iteration 64 : 0.0018960873558282968
Loss in iteration 65 : 0.0018565052952446706
Loss in iteration 66 : 0.0018174386067369363
Loss in iteration 67 : 0.0017781186996609878
Loss in iteration 68 : 0.0017387280656492791
Loss in iteration 69 : 0.0016992766543470544
Loss in iteration 70 : 0.001664887304114714
Loss in iteration 71 : 0.0016332548261141948
Loss in iteration 72 : 0.0016015772953625082
Loss in iteration 73 : 0.0015702061292565943
Loss in iteration 74 : 0.0015389603468049394
Loss in iteration 75 : 0.0015082682689103327
Loss in iteration 76 : 0.0014780221499733146
Loss in iteration 77 : 0.001448207380097661
Loss in iteration 78 : 0.0014192453414961339
Loss in iteration 79 : 0.0013905129553167946
Loss in iteration 80 : 0.001364480811430293
Loss in iteration 81 : 0.0013384601892616945
Loss in iteration 82 : 0.0013124276205157014
Loss in iteration 83 : 0.0012864799305238757
Loss in iteration 84 : 0.0012604003436758144
Loss in iteration 85 : 0.0012343787396133451
Loss in iteration 86 : 0.0012079914723619153
Loss in iteration 87 : 0.0011820810969008129
Loss in iteration 88 : 0.001156369200596788
Loss in iteration 89 : 0.0011299700970282226
Loss in iteration 90 : 0.0011035640692284342
Loss in iteration 91 : 0.001077470073293501
Loss in iteration 92 : 0.0010508502127907398
Loss in iteration 93 : 0.0010248417155962711
Loss in iteration 94 : 9.98272171541551E-4
Loss in iteration 95 : 9.721484653612477E-4
Loss in iteration 96 : 9.461713552676151E-4
Loss in iteration 97 : 9.199003384325044E-4
Loss in iteration 98 : 8.935777822278731E-4
Loss in iteration 99 : 8.697749668883584E-4
Loss in iteration 100 : 8.435631872304181E-4
Loss in iteration 101 : 8.185699850797556E-4
Loss in iteration 102 : 7.928586957722222E-4
Loss in iteration 103 : 7.697116222686932E-4
Loss in iteration 104 : 7.46770410253035E-4
Loss in iteration 105 : 7.242545497939488E-4
Loss in iteration 106 : 7.012221627094868E-4
Loss in iteration 107 : 6.779927612966808E-4
Loss in iteration 108 : 6.550227295122592E-4
Loss in iteration 109 : 6.319253787579034E-4
Loss in iteration 110 : 6.08471897462818E-4
Loss in iteration 111 : 5.85293041818107E-4
Loss in iteration 112 : 5.618201421398816E-4
Loss in iteration 113 : 5.383219094350941E-4
Loss in iteration 114 : 5.150405383532421E-4
Loss in iteration 115 : 4.911255955685756E-4
Loss in iteration 116 : 4.6764792348204183E-4
Loss in iteration 117 : 4.440139733482731E-4
Loss in iteration 118 : 4.200910657002086E-4
Loss in iteration 119 : 3.96558985375373E-4
Loss in iteration 120 : 3.725265707867878E-4
Loss in iteration 121 : 3.485298578972043E-4
Loss in iteration 122 : 3.246875233630791E-4
Loss in iteration 123 : 3.009647803436766E-4
Loss in iteration 124 : 2.7760313877327425E-4
Loss in iteration 125 : 2.5332528563463535E-4
Loss in iteration 126 : 2.2979866129749427E-4
Loss in iteration 127 : 2.0584987382199802E-4
Loss in iteration 128 : 1.84574959812669E-4
Loss in iteration 129 : 1.7275383110674998E-4
Loss in iteration 130 : 1.621773798718017E-4
Loss in iteration 131 : 1.517206289011846E-4
Loss in iteration 132 : 1.4212325881503247E-4
Loss in iteration 133 : 1.329881836019752E-4
Loss in iteration 134 : 1.2412420312144672E-4
Loss in iteration 135 : 1.1500444921957484E-4
Loss in iteration 136 : 1.0601409628309631E-4
Testing accuracy  of updater 8 on alg 1 with rate 0.14 = 0.9946666666666667, training accuracy 1.0, time elapsed: 2655 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3128078045615786
Loss in iteration 3 : 0.6596511284391382
Loss in iteration 4 : 0.26115993876255833
Loss in iteration 5 : 0.2406829963165974
Loss in iteration 6 : 0.17505443316700592
Loss in iteration 7 : 0.09527700061378187
Loss in iteration 8 : 0.06020652983058454
Loss in iteration 9 : 0.05581543208395457
Loss in iteration 10 : 0.05390912657018089
Loss in iteration 11 : 0.048663759563735996
Loss in iteration 12 : 0.041178093245627184
Loss in iteration 13 : 0.034051012450561066
Loss in iteration 14 : 0.02778729531421165
Loss in iteration 15 : 0.02295060474233533
Loss in iteration 16 : 0.01973321996782384
Loss in iteration 17 : 0.017729378796650153
Loss in iteration 18 : 0.016441275340718393
Loss in iteration 19 : 0.015495218651691353
Loss in iteration 20 : 0.014616539173684613
Loss in iteration 21 : 0.013805187473915237
Loss in iteration 22 : 0.012975236502023355
Loss in iteration 23 : 0.012058236024375384
Loss in iteration 24 : 0.01114645311904763
Loss in iteration 25 : 0.01031721831559216
Loss in iteration 26 : 0.009579882917925402
Loss in iteration 27 : 0.008880825388434507
Loss in iteration 28 : 0.008229881664750145
Loss in iteration 29 : 0.007663201874271272
Loss in iteration 30 : 0.007185079115069782
Loss in iteration 31 : 0.006726057761998447
Loss in iteration 32 : 0.006300054348728414
Loss in iteration 33 : 0.005959208198239276
Loss in iteration 34 : 0.005651591574766632
Loss in iteration 35 : 0.005369069475028397
Loss in iteration 36 : 0.005106575352089729
Loss in iteration 37 : 0.004845776431296895
Loss in iteration 38 : 0.004584493169659127
Loss in iteration 39 : 0.004343720631995725
Loss in iteration 40 : 0.00414836810836369
Loss in iteration 41 : 0.00396537128196706
Loss in iteration 42 : 0.003792071382916352
Loss in iteration 43 : 0.0036485431345411193
Loss in iteration 44 : 0.00351454886954745
Loss in iteration 45 : 0.0033839061640673056
Loss in iteration 46 : 0.003272907114459065
Loss in iteration 47 : 0.0031763949776600067
Loss in iteration 48 : 0.0030861538541501685
Loss in iteration 49 : 0.0029999945506991484
Loss in iteration 50 : 0.002918968278759471
Loss in iteration 51 : 0.002843375791542056
Loss in iteration 52 : 0.002768773017436546
Loss in iteration 53 : 0.0026983333960925514
Loss in iteration 54 : 0.0026390697763758017
Loss in iteration 55 : 0.002588479287005835
Loss in iteration 56 : 0.0025440865633818695
Loss in iteration 57 : 0.002503423263327867
Loss in iteration 58 : 0.0024679961051198886
Loss in iteration 59 : 0.002432917855716814
Loss in iteration 60 : 0.002402591702321077
Loss in iteration 61 : 0.0023736912300615326
Loss in iteration 62 : 0.002345483516515649
Loss in iteration 63 : 0.002318766189890146
Loss in iteration 64 : 0.002293345945767925
Loss in iteration 65 : 0.0022677267196879796
Loss in iteration 66 : 0.002242059885093513
Loss in iteration 67 : 0.0022163473951944293
Loss in iteration 68 : 0.0021938665912101404
Loss in iteration 69 : 0.0021723627966723285
Loss in iteration 70 : 0.002150835450298351
Loss in iteration 71 : 0.002129284590937441
Loss in iteration 72 : 0.0021077102538463615
Loss in iteration 73 : 0.00208611247103687
Loss in iteration 74 : 0.0020644912715893195
Loss in iteration 75 : 0.0020428466819357163
Loss in iteration 76 : 0.0020211787261151973
Loss in iteration 77 : 0.0019994874260046143
Loss in iteration 78 : 0.001977772801526658
Loss in iteration 79 : 0.001956304080995798
Loss in iteration 80 : 0.0019351026226595988
Loss in iteration 81 : 0.0019139000845008224
Loss in iteration 82 : 0.001892533779708969
Loss in iteration 83 : 0.0018710700462693597
Loss in iteration 84 : 0.0018496323629123934
Loss in iteration 85 : 0.0018284246871122169
Loss in iteration 86 : 0.0018072396596024182
Loss in iteration 87 : 0.001785903196598618
Loss in iteration 88 : 0.0017647535226713582
Loss in iteration 89 : 0.0017436957337183343
Loss in iteration 90 : 0.001722514355923958
Loss in iteration 91 : 0.0017011756216719837
Loss in iteration 92 : 0.0016804457082170853
Loss in iteration 93 : 0.0016595582523265225
Loss in iteration 94 : 0.001638957160733152
Loss in iteration 95 : 0.0016184978902074195
Loss in iteration 96 : 0.001597958226323806
Loss in iteration 97 : 0.001577381457242642
Loss in iteration 98 : 0.0015568614023958743
Loss in iteration 99 : 0.0015363103031054855
Loss in iteration 100 : 0.001515747200903853
Loss in iteration 101 : 0.0014951309223123067
Loss in iteration 102 : 0.0014744435853027793
Loss in iteration 103 : 0.0014552144228226847
Loss in iteration 104 : 0.00143845256056625
Loss in iteration 105 : 0.0014219201566449391
Loss in iteration 106 : 0.0014055042529793542
Loss in iteration 107 : 0.0013891917562773033
Loss in iteration 108 : 0.0013731956175811087
Loss in iteration 109 : 0.0013569775473429625
Testing accuracy  of updater 8 on alg 1 with rate 0.08000000000000002 = 0.992, training accuracy 0.9994284081166047, time elapsed: 2308 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.7771847798733238
Loss in iteration 3 : 0.4788061725658793
Loss in iteration 4 : 0.2881382829460368
Loss in iteration 5 : 0.2017243115142957
Loss in iteration 6 : 0.18342774240384155
Loss in iteration 7 : 0.16590798697464945
Loss in iteration 8 : 0.146395844649831
Loss in iteration 9 : 0.12632212999481898
Loss in iteration 10 : 0.10858032404875141
Loss in iteration 11 : 0.09494271667521732
Loss in iteration 12 : 0.08474189492744812
Loss in iteration 13 : 0.07577696925369824
Loss in iteration 14 : 0.06804101537098846
Loss in iteration 15 : 0.06138455725237349
Loss in iteration 16 : 0.05582930985267061
Loss in iteration 17 : 0.05109495670493223
Loss in iteration 18 : 0.04693875551625024
Loss in iteration 19 : 0.043144139810438624
Loss in iteration 20 : 0.03966249976280375
Loss in iteration 21 : 0.03638448029830323
Loss in iteration 22 : 0.033275631777820044
Loss in iteration 23 : 0.030528184606772243
Loss in iteration 24 : 0.028002669811805023
Loss in iteration 25 : 0.025781525945972626
Loss in iteration 26 : 0.02382782123312035
Loss in iteration 27 : 0.02214767489253607
Loss in iteration 28 : 0.02078592699178492
Loss in iteration 29 : 0.01960525350667494
Loss in iteration 30 : 0.018541339355526057
Loss in iteration 31 : 0.0175655704841153
Loss in iteration 32 : 0.01672420710079848
Loss in iteration 33 : 0.015957150659932372
Loss in iteration 34 : 0.015267986685408533
Loss in iteration 35 : 0.014647305078944692
Loss in iteration 36 : 0.014068820162263953
Loss in iteration 37 : 0.01354791011744977
Loss in iteration 38 : 0.013063353870827887
Loss in iteration 39 : 0.012598990661509466
Loss in iteration 40 : 0.012163419044549355
Loss in iteration 41 : 0.011735048014165996
Loss in iteration 42 : 0.011312424225104867
Loss in iteration 43 : 0.01090877272548573
Loss in iteration 44 : 0.010518920798265038
Loss in iteration 45 : 0.010137686349105746
Loss in iteration 46 : 0.009766760230172467
Loss in iteration 47 : 0.009410296591276873
Loss in iteration 48 : 0.009067582854230151
Loss in iteration 49 : 0.008737988600442519
Loss in iteration 50 : 0.008419858979857649
Loss in iteration 51 : 0.008112017370449984
Loss in iteration 52 : 0.007814698058636832
Loss in iteration 53 : 0.0075264403922837364
Loss in iteration 54 : 0.007246803512604692
Loss in iteration 55 : 0.006974500572863031
Loss in iteration 56 : 0.0067060212079058075
Loss in iteration 57 : 0.006446934167200354
Loss in iteration 58 : 0.006207009291736163
Loss in iteration 59 : 0.005976306231197602
Loss in iteration 60 : 0.005757578424801856
Loss in iteration 61 : 0.005555046561912275
Loss in iteration 62 : 0.005361258205759716
Loss in iteration 63 : 0.0051790621446348994
Loss in iteration 64 : 0.005007077775942082
Loss in iteration 65 : 0.004847082434291291
Loss in iteration 66 : 0.004699866263500458
Loss in iteration 67 : 0.0045620480219680645
Loss in iteration 68 : 0.004430485330701445
Loss in iteration 69 : 0.0043028978993288405
Loss in iteration 70 : 0.004181202042670275
Loss in iteration 71 : 0.004065753424927627
Loss in iteration 72 : 0.003952847120366553
Loss in iteration 73 : 0.0038457472519486686
Loss in iteration 74 : 0.003740228484487998
Loss in iteration 75 : 0.0036413824634678313
Loss in iteration 76 : 0.0035508320755030603
Loss in iteration 77 : 0.0034628668540068594
Loss in iteration 78 : 0.0033794015565712446
Loss in iteration 79 : 0.003300978709912972
Loss in iteration 80 : 0.003225876953649775
Loss in iteration 81 : 0.0031545356068897593
Loss in iteration 82 : 0.003085298779792935
Loss in iteration 83 : 0.003019708709983819
Loss in iteration 84 : 0.002956935659042046
Loss in iteration 85 : 0.0028959332090620856
Loss in iteration 86 : 0.0028360034844252644
Loss in iteration 87 : 0.0027772057946913653
Loss in iteration 88 : 0.0027202788973223753
Loss in iteration 89 : 0.00266736406103733
Loss in iteration 90 : 0.0026195989415373423
Loss in iteration 91 : 0.0025736138024127476
Loss in iteration 92 : 0.002530027248626981
Loss in iteration 93 : 0.0024873696488381713
Loss in iteration 94 : 0.002446371970205093
Loss in iteration 95 : 0.0024090314711837707
Loss in iteration 96 : 0.002373645340673693
Loss in iteration 97 : 0.0023402899211648702
Loss in iteration 98 : 0.0023075452571783063
Loss in iteration 99 : 0.0022753999883571695
Loss in iteration 100 : 0.0022438028951146474
Loss in iteration 101 : 0.0022126505228759192
Loss in iteration 102 : 0.0021846259869538523
Loss in iteration 103 : 0.00215785116817062
Loss in iteration 104 : 0.002132531057417173
Loss in iteration 105 : 0.00210734130922664
Loss in iteration 106 : 0.002082468504227588
Loss in iteration 107 : 0.0020583598360883
Loss in iteration 108 : 0.002035045482922775
Loss in iteration 109 : 0.0020119528887971743
Loss in iteration 110 : 0.001989019658813053
Loss in iteration 111 : 0.0019662280750056714
Loss in iteration 112 : 0.0019449424905676634
Loss in iteration 113 : 0.0019237818877971063
Loss in iteration 114 : 0.0019035231020048875
Loss in iteration 115 : 0.0018837885851236901
Loss in iteration 116 : 0.0018643503972232908
Loss in iteration 117 : 0.0018452750361397181
Loss in iteration 118 : 0.0018264392617742453
Loss in iteration 119 : 0.0018077785362053845
Loss in iteration 120 : 0.0017896100350444373
Loss in iteration 121 : 0.0017734950504282394
Loss in iteration 122 : 0.0017579572441147572
Loss in iteration 123 : 0.0017428051218010532
Loss in iteration 124 : 0.0017286374077450492
Testing accuracy  of updater 8 on alg 1 with rate 0.01999999999999999 = 0.9866666666666667, training accuracy 0.9994284081166047, time elapsed: 2350 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9695865511411372
Loss in iteration 3 : 0.23603575151678863
Loss in iteration 4 : 0.4214475549241793
Loss in iteration 5 : 0.48743661345344347
Loss in iteration 6 : 0.27219378685913154
Loss in iteration 7 : 0.18583715428050965
Loss in iteration 8 : 0.15468013167172695
Loss in iteration 9 : 0.16888513396674684
Loss in iteration 10 : 0.1863912219438806
Loss in iteration 11 : 0.16461714775031175
Loss in iteration 12 : 0.11872338087814384
Loss in iteration 13 : 0.07984727454419357
Loss in iteration 14 : 0.057322689705981686
Loss in iteration 15 : 0.046873890534793974
Loss in iteration 16 : 0.04330936311425321
Loss in iteration 17 : 0.04175664321584845
Loss in iteration 18 : 0.04058146380143167
Loss in iteration 19 : 0.038691993174978144
Loss in iteration 20 : 0.03597030463197346
Loss in iteration 21 : 0.03258973782588651
Loss in iteration 22 : 0.02942898244386367
Loss in iteration 23 : 0.026703434976676713
Loss in iteration 24 : 0.023980233604462473
Loss in iteration 25 : 0.021438858862985095
Loss in iteration 26 : 0.0198086033763286
Loss in iteration 27 : 0.018787370218099175
Loss in iteration 28 : 0.01806603867012561
Loss in iteration 29 : 0.0174334959717738
Loss in iteration 30 : 0.017039993235908383
Loss in iteration 31 : 0.01683961643498228
Loss in iteration 32 : 0.01666789863067572
Loss in iteration 33 : 0.01657226416536076
Loss in iteration 34 : 0.016423805172865083
Loss in iteration 35 : 0.016188204619530744
Loss in iteration 36 : 0.015850753358664812
Loss in iteration 37 : 0.015421553778619156
Loss in iteration 38 : 0.01496764107141999
Loss in iteration 39 : 0.01462703349774777
Loss in iteration 40 : 0.014372619758236208
Loss in iteration 41 : 0.014121950835131456
Loss in iteration 42 : 0.013867095093497291
Loss in iteration 43 : 0.013608455913983672
Loss in iteration 44 : 0.013346396511124705
Loss in iteration 45 : 0.013081243932293167
Loss in iteration 46 : 0.012813292658517723
Loss in iteration 47 : 0.012542807846801457
Loss in iteration 48 : 0.012307687555637667
Loss in iteration 49 : 0.012089954298114236
Loss in iteration 50 : 0.011873057767888742
Loss in iteration 51 : 0.011656904822577325
Loss in iteration 52 : 0.011441411589426702
Loss in iteration 53 : 0.011226502542379805
Loss in iteration 54 : 0.01101210967103357
Loss in iteration 55 : 0.010818851007970072
Loss in iteration 56 : 0.01063507072361178
Loss in iteration 57 : 0.010452466168497866
Loss in iteration 58 : 0.010277724492551564
Loss in iteration 59 : 0.010120815004503465
Loss in iteration 60 : 0.009963261595743777
Loss in iteration 61 : 0.009805120728852933
Loss in iteration 62 : 0.009646443243086503
Loss in iteration 63 : 0.009487274914108921
Loss in iteration 64 : 0.009327656958012416
Loss in iteration 65 : 0.009167626485166822
Loss in iteration 66 : 0.009008103617226659
Loss in iteration 67 : 0.008849902994859238
Loss in iteration 68 : 0.008687975152666652
Loss in iteration 69 : 0.00852268193116456
Loss in iteration 70 : 0.008356841743399271
Loss in iteration 71 : 0.008193047686507296
Loss in iteration 72 : 0.00802920322549716
Loss in iteration 73 : 0.007865500911589574
Loss in iteration 74 : 0.007710022493440368
Loss in iteration 75 : 0.007552566327777156
Loss in iteration 76 : 0.007393321068472148
Loss in iteration 77 : 0.007232456584741316
Loss in iteration 78 : 0.007070125831223815
Loss in iteration 79 : 0.006913277403449145
Loss in iteration 80 : 0.006786992688669276
Loss in iteration 81 : 0.006661753371508862
Loss in iteration 82 : 0.00654102548974195
Loss in iteration 83 : 0.006423270938653766
Loss in iteration 84 : 0.0063059132474916
Loss in iteration 85 : 0.006188907613451191
Loss in iteration 86 : 0.00607221369112988
Loss in iteration 87 : 0.005955795148844726
Loss in iteration 88 : 0.005839619269113942
Loss in iteration 89 : 0.005723656588905659
Loss in iteration 90 : 0.00560790709666985
Loss in iteration 91 : 0.0054959432767131815
Loss in iteration 92 : 0.0053835392305831235
Loss in iteration 93 : 0.005270733333185529
Loss in iteration 94 : 0.0051575601369547
Loss in iteration 95 : 0.005050074824934951
Loss in iteration 96 : 0.0049430872552114575
Loss in iteration 97 : 0.004833403079357716
Loss in iteration 98 : 0.004723325656562479
Loss in iteration 99 : 0.004614622917942028
Loss in iteration 100 : 0.0045054339975521785
Loss in iteration 101 : 0.0044069520328727085
Loss in iteration 102 : 0.004312835841977577
Loss in iteration 103 : 0.004220400270219448
Loss in iteration 104 : 0.004127308548770374
Loss in iteration 105 : 0.004033621430950024
Loss in iteration 106 : 0.003939393618664243
Loss in iteration 107 : 0.003844674364978277
Loss in iteration 108 : 0.003749508016688503
Loss in iteration 109 : 0.0036539345028673074
Loss in iteration 110 : 0.003557989774760852
Loss in iteration 111 : 0.0034618147702114763
Loss in iteration 112 : 0.0033664432155853267
Loss in iteration 113 : 0.003270006410046605
Loss in iteration 114 : 0.0031741876885800846
Loss in iteration 115 : 0.0030781562172158793
Loss in iteration 116 : 0.0029817715368569628
Loss in iteration 117 : 0.002885064210812578
Loss in iteration 118 : 0.0027880617573824316
Loss in iteration 119 : 0.0026907889530442975
Loss in iteration 120 : 0.0025941051299841374
Loss in iteration 121 : 0.002497180592560411
Loss in iteration 122 : 0.0023993009562478093
Loss in iteration 123 : 0.0023025101461645217
Loss in iteration 124 : 0.002205528571409391
Loss in iteration 125 : 0.0021082468623978908
Loss in iteration 126 : 0.002010690279902682
Loss in iteration 127 : 0.0019128815677733853
Loss in iteration 128 : 0.0018225322002758356
Loss in iteration 129 : 0.0017280761284198293
Loss in iteration 130 : 0.0016299247974518457
Loss in iteration 131 : 0.0015315473844866037
Loss in iteration 132 : 0.0014347514939612928
Loss in iteration 133 : 0.0013400893808605885
Loss in iteration 134 : 0.0012457443519682035
Loss in iteration 135 : 0.0011507903687557814
Loss in iteration 136 : 0.0010663490220163233
Loss in iteration 137 : 9.991186004325978E-4
Loss in iteration 138 : 9.316485591236807E-4
Loss in iteration 139 : 8.641992515638176E-4
Loss in iteration 140 : 8.017272098500153E-4
Loss in iteration 141 : 7.40250207798533E-4
Loss in iteration 142 : 6.796666534121087E-4
Loss in iteration 143 : 6.223067259048067E-4
Loss in iteration 144 : 5.830652783790351E-4
Testing accuracy  of updater 9 on alg 1 with rate 0.19999999999999998 = 0.992, training accuracy 0.9997142040583024, time elapsed: 3073 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3640704242418464
Loss in iteration 3 : 0.16165142063138832
Loss in iteration 4 : 0.17616954828189524
Loss in iteration 5 : 0.15703239750768588
Loss in iteration 6 : 0.0982227785726699
Loss in iteration 7 : 0.07372392319233603
Loss in iteration 8 : 0.0666033449440896
Loss in iteration 9 : 0.0662400224213833
Loss in iteration 10 : 0.06037076517738106
Loss in iteration 11 : 0.04740804197294661
Loss in iteration 12 : 0.034266379128009845
Loss in iteration 13 : 0.02485637132269141
Loss in iteration 14 : 0.020185777262929368
Loss in iteration 15 : 0.0186110150953436
Loss in iteration 16 : 0.017463527289734555
Loss in iteration 17 : 0.01665887435307025
Loss in iteration 18 : 0.015815697749922957
Loss in iteration 19 : 0.014652106238307169
Loss in iteration 20 : 0.01346364912824897
Loss in iteration 21 : 0.012327897523208639
Loss in iteration 22 : 0.011068652668468452
Loss in iteration 23 : 0.010004244324524837
Loss in iteration 24 : 0.009456035720753273
Loss in iteration 25 : 0.0091357161548911
Loss in iteration 26 : 0.00884863667628494
Loss in iteration 27 : 0.008586565640950537
Loss in iteration 28 : 0.008388594712343739
Loss in iteration 29 : 0.008207046421048834
Loss in iteration 30 : 0.007976438293682709
Loss in iteration 31 : 0.007689529732445708
Loss in iteration 32 : 0.007366947151085449
Loss in iteration 33 : 0.007023807563105321
Loss in iteration 34 : 0.006660371443962972
Loss in iteration 35 : 0.006330594152318254
Loss in iteration 36 : 0.006088054485006446
Loss in iteration 37 : 0.005876178163105909
Loss in iteration 38 : 0.005700714209009322
Loss in iteration 39 : 0.005527656332532337
Loss in iteration 40 : 0.005397074006911227
Loss in iteration 41 : 0.005290702652455752
Loss in iteration 42 : 0.005196377420020113
Loss in iteration 43 : 0.005125716379890533
Loss in iteration 44 : 0.005052800891585854
Loss in iteration 45 : 0.004976575974757558
Loss in iteration 46 : 0.004897365970829729
Loss in iteration 47 : 0.004818819559222853
Loss in iteration 48 : 0.0047369380337464
Loss in iteration 49 : 0.004649016258497342
Loss in iteration 50 : 0.004555648664460347
Loss in iteration 51 : 0.004457370501530316
Loss in iteration 52 : 0.004357267970817827
Loss in iteration 53 : 0.0042592316741763055
Loss in iteration 54 : 0.004160340672487054
Loss in iteration 55 : 0.004060674915298396
Loss in iteration 56 : 0.003960306390795628
Loss in iteration 57 : 0.0038592999183594775
Loss in iteration 58 : 0.0037577138622248156
Loss in iteration 59 : 0.0036618567903962847
Loss in iteration 60 : 0.0035703493789136106
Loss in iteration 61 : 0.003479044894730288
Loss in iteration 62 : 0.0033879188485803824
Loss in iteration 63 : 0.003302076362408562
Loss in iteration 64 : 0.0032159381610729837
Loss in iteration 65 : 0.0031289492516281196
Loss in iteration 66 : 0.00304118978557067
Loss in iteration 67 : 0.002952979751719109
Loss in iteration 68 : 0.002870614603562073
Loss in iteration 69 : 0.0028065458833792176
Loss in iteration 70 : 0.0027427544539723654
Loss in iteration 71 : 0.0026792097658965763
Loss in iteration 72 : 0.002615884309848865
Loss in iteration 73 : 0.002552753313995398
Loss in iteration 74 : 0.0024897944714323606
Loss in iteration 75 : 0.0024269876947804144
Loss in iteration 76 : 0.0023643148952113757
Loss in iteration 77 : 0.0023017597834746988
Loss in iteration 78 : 0.0022393076907334534
Loss in iteration 79 : 0.002176945407237637
Loss in iteration 80 : 0.0021146610370589374
Loss in iteration 81 : 0.002053102411065245
Loss in iteration 82 : 0.001993218582078312
Loss in iteration 83 : 0.0019329874090873184
Loss in iteration 84 : 0.0018724404254662
Loss in iteration 85 : 0.0018116060240269411
Loss in iteration 86 : 0.0017505097696663432
Loss in iteration 87 : 0.001697313878823025
Loss in iteration 88 : 0.0016466529051867244
Loss in iteration 89 : 0.0015966837814840515
Loss in iteration 90 : 0.001546445368295886
Loss in iteration 91 : 0.0014954648507074502
Loss in iteration 92 : 0.001443813353102052
Loss in iteration 93 : 0.001391770129737883
Loss in iteration 94 : 0.001340490650899719
Loss in iteration 95 : 0.0012889334988468676
Loss in iteration 96 : 0.0012371237416697073
Loss in iteration 97 : 0.0011850839502909156
Loss in iteration 98 : 0.0011329810677636967
Loss in iteration 99 : 0.0010805043217262627
Loss in iteration 100 : 0.0010279882665169204
Loss in iteration 101 : 9.754673187561285E-4
Loss in iteration 102 : 9.228434035708263E-4
Loss in iteration 103 : 8.703551034396313E-4
Loss in iteration 104 : 8.17758201195827E-4
Loss in iteration 105 : 7.649889914064277E-4
Loss in iteration 106 : 7.120620381571976E-4
Loss in iteration 107 : 6.58990454381355E-4
Loss in iteration 108 : 6.063248694213517E-4
Loss in iteration 109 : 5.533430661797187E-4
Loss in iteration 110 : 4.999023647962119E-4
Loss in iteration 111 : 4.4687963397477607E-4
Loss in iteration 112 : 3.937088282092819E-4
Loss in iteration 113 : 3.4954949100904717E-4
Loss in iteration 114 : 3.1248736696769063E-4
Loss in iteration 115 : 2.7626562854932987E-4
Loss in iteration 116 : 2.4079921501708198E-4
Loss in iteration 117 : 2.1236011093528681E-4
Loss in iteration 118 : 1.918324397033338E-4
Loss in iteration 119 : 1.7128572751330388E-4
Loss in iteration 120 : 1.5154205827356724E-4
Loss in iteration 121 : 1.3196300593422235E-4
Loss in iteration 122 : 1.1253129663548969E-4
Loss in iteration 123 : 9.32313757555192E-5
Loss in iteration 124 : 7.404923675550625E-5
Loss in iteration 125 : 5.699868336170181E-5
Testing accuracy  of updater 9 on alg 1 with rate 0.13999999999999999 = 0.9937777777777778, training accuracy 1.0, time elapsed: 2770 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.3150303153125652
Loss in iteration 3 : 0.2352954557636053
Loss in iteration 4 : 0.12022108768772677
Loss in iteration 5 : 0.101659580163997
Loss in iteration 6 : 0.08130789702714612
Loss in iteration 7 : 0.0706525703105277
Loss in iteration 8 : 0.06313056265119753
Loss in iteration 9 : 0.05489986687193042
Loss in iteration 10 : 0.04640531566611579
Loss in iteration 11 : 0.03797101632908466
Loss in iteration 12 : 0.030416542953467244
Loss in iteration 13 : 0.024936231346922636
Loss in iteration 14 : 0.021033427233493128
Loss in iteration 15 : 0.01893648424759665
Loss in iteration 16 : 0.01734077325331871
Loss in iteration 17 : 0.015979295481797846
Loss in iteration 18 : 0.014912295457328605
Loss in iteration 19 : 0.013899876338317076
Loss in iteration 20 : 0.012921889753404312
Loss in iteration 21 : 0.011879187567505339
Loss in iteration 22 : 0.010842908674181462
Loss in iteration 23 : 0.00991297383340136
Loss in iteration 24 : 0.009238677668149595
Loss in iteration 25 : 0.008694522131776754
Loss in iteration 26 : 0.008313609812193319
Loss in iteration 27 : 0.007898533888270371
Loss in iteration 28 : 0.007507758988719582
Loss in iteration 29 : 0.007093489775488605
Loss in iteration 30 : 0.0066847841417146765
Loss in iteration 31 : 0.006327122929658932
Loss in iteration 32 : 0.0059381880898954405
Loss in iteration 33 : 0.005601325651676803
Loss in iteration 34 : 0.005259250478860608
Loss in iteration 35 : 0.004969424135942284
Loss in iteration 36 : 0.004792622715787834
Loss in iteration 37 : 0.004628407508917612
Loss in iteration 38 : 0.004501482718422898
Loss in iteration 39 : 0.004383300363359869
Loss in iteration 40 : 0.00429252349014513
Loss in iteration 41 : 0.004209176804807658
Loss in iteration 42 : 0.00412923952529383
Loss in iteration 43 : 0.004048649965170133
Loss in iteration 44 : 0.003977227825032554
Loss in iteration 45 : 0.0039138674799375954
Loss in iteration 46 : 0.0038505979635040416
Loss in iteration 47 : 0.0037915828968710743
Loss in iteration 48 : 0.003731677222602628
Loss in iteration 49 : 0.0036763461301210784
Loss in iteration 50 : 0.0036219435919729666
Loss in iteration 51 : 0.0035677942876000683
Loss in iteration 52 : 0.003514876411603332
Loss in iteration 53 : 0.0034673591311523855
Loss in iteration 54 : 0.003423625436316483
Loss in iteration 55 : 0.0033802081372145453
Loss in iteration 56 : 0.003337073765500065
Loss in iteration 57 : 0.003294826978984752
Loss in iteration 58 : 0.0032525014557541047
Loss in iteration 59 : 0.0032096025740926412
Loss in iteration 60 : 0.003166185069390864
Loss in iteration 61 : 0.003123427246199542
Loss in iteration 62 : 0.0030810203652615552
Loss in iteration 63 : 0.0030387820078995376
Loss in iteration 64 : 0.0029966934207190216
Loss in iteration 65 : 0.002954737716428721
Loss in iteration 66 : 0.0029128996880580845
Loss in iteration 67 : 0.00287116564167064
Loss in iteration 68 : 0.002830175008118648
Loss in iteration 69 : 0.002789977847490863
Loss in iteration 70 : 0.0027495224301704326
Loss in iteration 71 : 0.0027088323726655677
Loss in iteration 72 : 0.0026679289395448065
Loss in iteration 73 : 0.0026268312775648514
Loss in iteration 74 : 0.0025855566264917063
Loss in iteration 75 : 0.0025450833093430443
Loss in iteration 76 : 0.0025049352790866594
Loss in iteration 77 : 0.0024647030487351105
Loss in iteration 78 : 0.0024308083024879843
Loss in iteration 79 : 0.002398836842329985
Loss in iteration 80 : 0.002367267308996233
Loss in iteration 81 : 0.0023362270832124666
Loss in iteration 82 : 0.002305492417180532
Loss in iteration 83 : 0.0022750290630195054
Loss in iteration 84 : 0.002244473593219128
Loss in iteration 85 : 0.0022144361049840685
Loss in iteration 86 : 0.0021845871900868456
Loss in iteration 87 : 0.002154906708687472
Loss in iteration 88 : 0.0021253765252719995
Loss in iteration 89 : 0.0020959803091177473
Loss in iteration 90 : 0.002066703354622582
Loss in iteration 91 : 0.00203753241952078
Loss in iteration 92 : 0.0020084555792047954
Loss in iteration 93 : 0.001979462095549586
Loss in iteration 94 : 0.0019505422987957253
Loss in iteration 95 : 0.001921687481191274
Loss in iteration 96 : 0.0018928898012218155
Loss in iteration 97 : 0.001864142197374572
Loss in iteration 98 : 0.001835438310487467
Loss in iteration 99 : 0.001806875835773333
Loss in iteration 100 : 0.0017781734598300233
Loss in iteration 101 : 0.001750955208484402
Loss in iteration 102 : 0.0017234976749549973
Loss in iteration 103 : 0.0016956732044181178
Loss in iteration 104 : 0.0016675168085993816
Loss in iteration 105 : 0.0016390600130112296
Loss in iteration 106 : 0.0016103312040134546
Loss in iteration 107 : 0.0015830084283229157
Loss in iteration 108 : 0.001556605246327419
Loss in iteration 109 : 0.001529628631981229
Loss in iteration 110 : 0.001502134110345028
Loss in iteration 111 : 0.0014741716779434913
Loss in iteration 112 : 0.0014457863531593333
Loss in iteration 113 : 0.0014176624442897227
Loss in iteration 114 : 0.0013896523022832504
Loss in iteration 115 : 0.0013631848564588393
Loss in iteration 116 : 0.001338159442121155
Loss in iteration 117 : 0.0013129401476300812
Loss in iteration 118 : 0.0012874930105625363
Testing accuracy  of updater 9 on alg 1 with rate 0.08 = 0.992, training accuracy 0.9994284081166047, time elapsed: 2416 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.78378277322092
Loss in iteration 3 : 0.4246259014330463
Loss in iteration 4 : 0.25525446466884866
Loss in iteration 5 : 0.20313207692278307
Loss in iteration 6 : 0.17298568415208523
Loss in iteration 7 : 0.14082096828926244
Loss in iteration 8 : 0.12247934957296193
Loss in iteration 9 : 0.11650916085263863
Loss in iteration 10 : 0.10415777640901447
Loss in iteration 11 : 0.08692443742402746
Loss in iteration 12 : 0.07441224120466401
Loss in iteration 13 : 0.06671774258390183
Loss in iteration 14 : 0.06142340880499313
Loss in iteration 15 : 0.05682100072459515
Loss in iteration 16 : 0.05201651289629251
Loss in iteration 17 : 0.04705031739788872
Loss in iteration 18 : 0.042124953758226266
Loss in iteration 19 : 0.03757772141188084
Loss in iteration 20 : 0.03360931775190377
Loss in iteration 21 : 0.030191576141515818
Loss in iteration 22 : 0.027122783124094302
Loss in iteration 23 : 0.02453782204092811
Loss in iteration 24 : 0.022231089144441
Loss in iteration 25 : 0.020426821827983388
Loss in iteration 26 : 0.01892194577783515
Loss in iteration 27 : 0.017635602844780208
Loss in iteration 28 : 0.01645769393983113
Loss in iteration 29 : 0.015482179927024869
Loss in iteration 30 : 0.014641524678763057
Loss in iteration 31 : 0.013854676241406707
Loss in iteration 32 : 0.013150637877862591
Loss in iteration 33 : 0.012542319791527357
Loss in iteration 34 : 0.011952531495738274
Loss in iteration 35 : 0.011378255484273087
Loss in iteration 36 : 0.01087204346015436
Loss in iteration 37 : 0.010433414651274158
Loss in iteration 38 : 0.010035181966008087
Loss in iteration 39 : 0.009664113526971899
Loss in iteration 40 : 0.009303303731439633
Loss in iteration 41 : 0.008933682577814605
Loss in iteration 42 : 0.008545275554002918
Loss in iteration 43 : 0.00813731173888463
Loss in iteration 44 : 0.007725975139159563
Loss in iteration 45 : 0.007314637306055097
Loss in iteration 46 : 0.006957466098783698
Loss in iteration 47 : 0.006641050828283177
Loss in iteration 48 : 0.006336559335908738
Loss in iteration 49 : 0.006056036856517896
Loss in iteration 50 : 0.005791724581000741
Loss in iteration 51 : 0.005553365141854348
Loss in iteration 52 : 0.00534025413425106
Loss in iteration 53 : 0.005151634284930252
Loss in iteration 54 : 0.004970867319703334
Loss in iteration 55 : 0.00480379034428703
Loss in iteration 56 : 0.004647411265263995
Loss in iteration 57 : 0.004498442615753565
Loss in iteration 58 : 0.00435881078644581
Loss in iteration 59 : 0.004232031467404346
Loss in iteration 60 : 0.004109511262446448
Loss in iteration 61 : 0.003995146168088335
Loss in iteration 62 : 0.0038921755170519125
Loss in iteration 63 : 0.00379114404696767
Loss in iteration 64 : 0.0037034602746599497
Loss in iteration 65 : 0.003625830917512529
Loss in iteration 66 : 0.0035514697439808004
Loss in iteration 67 : 0.003481583344590963
Loss in iteration 68 : 0.003415477216520141
Loss in iteration 69 : 0.003348016473245403
Loss in iteration 70 : 0.00328010361881503
Loss in iteration 71 : 0.0032172230591203405
Loss in iteration 72 : 0.0031565043102062527
Loss in iteration 73 : 0.0030969805659831003
Loss in iteration 74 : 0.003037799718233346
Loss in iteration 75 : 0.0029793463398321398
Loss in iteration 76 : 0.002922601320808744
Loss in iteration 77 : 0.0028676996640283783
Loss in iteration 78 : 0.002813584121856316
Loss in iteration 79 : 0.002762269340552531
Loss in iteration 80 : 0.002712376700956148
Loss in iteration 81 : 0.0026670223409551224
Loss in iteration 82 : 0.002627735822088783
Loss in iteration 83 : 0.0025933490048701898
Loss in iteration 84 : 0.0025612961466921274
Loss in iteration 85 : 0.0025332980794268837
Loss in iteration 86 : 0.002508533662992373
Loss in iteration 87 : 0.0024834105510721893
Loss in iteration 88 : 0.002459410011129457
Loss in iteration 89 : 0.0024383409576476636
Loss in iteration 90 : 0.0024181974132872824
Loss in iteration 91 : 0.00239815779346457
Loss in iteration 92 : 0.0023782108198996737
Loss in iteration 93 : 0.002358471417621953
Loss in iteration 94 : 0.0023396099297341775
Loss in iteration 95 : 0.0023209704792862525
Loss in iteration 96 : 0.0023027554467405947
Loss in iteration 97 : 0.0022844587107680456
Loss in iteration 98 : 0.0022660874761296426
Loss in iteration 99 : 0.0022476482299687156
Loss in iteration 100 : 0.0022291468132443737
Loss in iteration 101 : 0.002210588485054298
Loss in iteration 102 : 0.002191977980554672
Testing accuracy  of updater 9 on alg 1 with rate 0.02 = 0.9848888888888889, training accuracy 0.999285510145756, time elapsed: 1940 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.8778079119035382
Loss in iteration 3 : 0.6599538273762374
Loss in iteration 4 : 0.41059877823554486
Loss in iteration 5 : 0.2633028657000145
Loss in iteration 6 : 0.2044302362161535
Loss in iteration 7 : 0.18305746941474763
Loss in iteration 8 : 0.1657783773107882
Loss in iteration 9 : 0.14908958284116108
Loss in iteration 10 : 0.1336081382917573
Loss in iteration 11 : 0.11955921958602048
Loss in iteration 12 : 0.10587955673748776
Loss in iteration 13 : 0.09360581225399449
Loss in iteration 14 : 0.08394784744633267
Loss in iteration 15 : 0.076463250177402
Loss in iteration 16 : 0.07074500615127344
Loss in iteration 17 : 0.06572076653789072
Loss in iteration 18 : 0.06106294714534351
Loss in iteration 19 : 0.05672657347947725
Loss in iteration 20 : 0.052653360069812825
Loss in iteration 21 : 0.04875394105040302
Loss in iteration 22 : 0.0450941575856554
Loss in iteration 23 : 0.041584787735819743
Loss in iteration 24 : 0.038258723616494614
Loss in iteration 25 : 0.03513417236152683
Loss in iteration 26 : 0.03226895650216994
Loss in iteration 27 : 0.029718273828857646
Loss in iteration 28 : 0.02741041175254218
Loss in iteration 29 : 0.025466946502207342
Loss in iteration 30 : 0.02378075880608875
Loss in iteration 31 : 0.02229254673470281
Loss in iteration 32 : 0.020993364994482233
Loss in iteration 33 : 0.019928864771506224
Loss in iteration 34 : 0.019023851750976335
Loss in iteration 35 : 0.018197179537321594
Loss in iteration 36 : 0.017422365009263457
Loss in iteration 37 : 0.016736958985430928
Loss in iteration 38 : 0.016104682539211275
Loss in iteration 39 : 0.015513554179681314
Loss in iteration 40 : 0.01496577376124857
Loss in iteration 41 : 0.014478104392952969
Loss in iteration 42 : 0.014016046625082995
Loss in iteration 43 : 0.01357151814172489
Loss in iteration 44 : 0.013143330814913303
Loss in iteration 45 : 0.012736377632624966
Loss in iteration 46 : 0.012354169931369865
Loss in iteration 47 : 0.011984273822920536
Loss in iteration 48 : 0.011620196450045535
Loss in iteration 49 : 0.011260654845262714
Loss in iteration 50 : 0.010906643284060524
Loss in iteration 51 : 0.01056170183364497
Loss in iteration 52 : 0.010223954969093558
Loss in iteration 53 : 0.009892021077420269
Loss in iteration 54 : 0.009563563419984133
Loss in iteration 55 : 0.00924317456055779
Loss in iteration 56 : 0.008941273568050823
Loss in iteration 57 : 0.008651716583787382
Loss in iteration 58 : 0.008370272596772619
Loss in iteration 59 : 0.008096288722026876
Loss in iteration 60 : 0.007838133132442252
Loss in iteration 61 : 0.00758670212591889
Loss in iteration 62 : 0.007339303586799123
Loss in iteration 63 : 0.00709750091249904
Loss in iteration 64 : 0.006865711227935401
Loss in iteration 65 : 0.006641370697196385
Loss in iteration 66 : 0.006420743299237771
Loss in iteration 67 : 0.006213702940912979
Loss in iteration 68 : 0.006017934861379102
Loss in iteration 69 : 0.005831020150057271
Loss in iteration 70 : 0.005664289962846859
Loss in iteration 71 : 0.005510238551247972
Loss in iteration 72 : 0.005364422977941408
Loss in iteration 73 : 0.005225681663817944
Loss in iteration 74 : 0.005096012313671229
Loss in iteration 75 : 0.0049736074471720265
Loss in iteration 76 : 0.004857789634989944
Loss in iteration 77 : 0.004749503454632653
Loss in iteration 78 : 0.00464381892284104
Loss in iteration 79 : 0.004539683941795447
Loss in iteration 80 : 0.004437931664969725
Loss in iteration 81 : 0.004339599936972065
Loss in iteration 82 : 0.004242932829548204
Loss in iteration 83 : 0.004151561384951652
Loss in iteration 84 : 0.004064095762507502
Loss in iteration 85 : 0.003984007596780647
Loss in iteration 86 : 0.0039104333741537675
Loss in iteration 87 : 0.0038371291553983794
Loss in iteration 88 : 0.0037640210653215487
Loss in iteration 89 : 0.003691871050039809
Loss in iteration 90 : 0.0036213242323084593
Loss in iteration 91 : 0.0035537708895343727
Loss in iteration 92 : 0.0034887645391716337
Loss in iteration 93 : 0.0034258418054913574
Loss in iteration 94 : 0.0033676092281428403
Loss in iteration 95 : 0.003311494078691998
Loss in iteration 96 : 0.0032571740940419147
Loss in iteration 97 : 0.003206982950482992
Loss in iteration 98 : 0.0031586679275850886
Loss in iteration 99 : 0.003110444391181092
Loss in iteration 100 : 0.0030629616263603207
Loss in iteration 101 : 0.0030159490521902226
Loss in iteration 102 : 0.0029699304372316545
Loss in iteration 103 : 0.0029240580551788582
Loss in iteration 104 : 0.0028808258903618962
Loss in iteration 105 : 0.0028394432483553047
Loss in iteration 106 : 0.002798601973630642
Loss in iteration 107 : 0.002759195478701681
Loss in iteration 108 : 0.002720862371463478
Loss in iteration 109 : 0.00268390619298939
Loss in iteration 110 : 0.0026479439426291174
Loss in iteration 111 : 0.0026139079231369644
Loss in iteration 112 : 0.0025813628602628976
Loss in iteration 113 : 0.00254918385204903
Loss in iteration 114 : 0.002518056703264361
Loss in iteration 115 : 0.0024891956733392303
Loss in iteration 116 : 0.002460908627636106
Loss in iteration 117 : 0.0024321686267378493
Loss in iteration 118 : 0.00240574598510651
Loss in iteration 119 : 0.0023792635101144077
Loss in iteration 120 : 0.0023539299474547985
Loss in iteration 121 : 0.0023290856994348168
Loss in iteration 122 : 0.0023047902254350948
Loss in iteration 123 : 0.00228028061178511
Loss in iteration 124 : 0.0022554400857891276
Loss in iteration 125 : 0.0022310084647331843
Loss in iteration 126 : 0.0022070021535699853
Loss in iteration 127 : 0.0021849528558280833
Loss in iteration 128 : 0.002165143552628494
Loss in iteration 129 : 0.002149918611146162
Testing accuracy  of updater 9 on alg 1 with rate 0.014 = 0.9848888888888889, training accuracy 0.9994284081166047, time elapsed: 2629 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9409352292001077
Loss in iteration 3 : 0.8337339720911147
Loss in iteration 4 : 0.6872060968406216
Loss in iteration 5 : 0.5098548805254943
Loss in iteration 6 : 0.3958272077205637
Loss in iteration 7 : 0.29666728049076146
Loss in iteration 8 : 0.22856074982428617
Loss in iteration 9 : 0.20122466534777358
Loss in iteration 10 : 0.1878784764154693
Loss in iteration 11 : 0.1782932663303699
Loss in iteration 12 : 0.1693058065362713
Loss in iteration 13 : 0.1594220464361538
Loss in iteration 14 : 0.14798761726038825
Loss in iteration 15 : 0.13614419226026345
Loss in iteration 16 : 0.12477833737260402
Loss in iteration 17 : 0.1142835134485882
Loss in iteration 18 : 0.1050828452578398
Loss in iteration 19 : 0.0973527054458376
Loss in iteration 20 : 0.09059937650401215
Loss in iteration 21 : 0.08485762023574933
Loss in iteration 22 : 0.07978857093631243
Loss in iteration 23 : 0.07540249806115233
Loss in iteration 24 : 0.07136282792453551
Loss in iteration 25 : 0.06759470475648739
Loss in iteration 26 : 0.06426824396522457
Loss in iteration 27 : 0.06112613910094801
Loss in iteration 28 : 0.0581928662229834
Loss in iteration 29 : 0.05539472751027054
Loss in iteration 30 : 0.05272069972170891
Loss in iteration 31 : 0.050261304419468536
Loss in iteration 32 : 0.047901416526992875
Loss in iteration 33 : 0.04559646771111789
Loss in iteration 34 : 0.04344450087378834
Loss in iteration 35 : 0.04136260874329203
Loss in iteration 36 : 0.03936623799978585
Loss in iteration 37 : 0.037461558932094206
Loss in iteration 38 : 0.03568740828385738
Loss in iteration 39 : 0.034004692265146436
Loss in iteration 40 : 0.03242796988254389
Loss in iteration 41 : 0.030927345564813406
Loss in iteration 42 : 0.029556790154613707
Loss in iteration 43 : 0.028333532247401812
Loss in iteration 44 : 0.027183005894591133
Loss in iteration 45 : 0.02609206417881808
Loss in iteration 46 : 0.025077733257219726
Loss in iteration 47 : 0.024110865613394435
Loss in iteration 48 : 0.023243519106207103
Loss in iteration 49 : 0.022470563143882086
Loss in iteration 50 : 0.02174575301453792
Loss in iteration 51 : 0.021078618547422934
Loss in iteration 52 : 0.020465686474660478
Loss in iteration 53 : 0.019890081337874465
Loss in iteration 54 : 0.019351647438405267
Loss in iteration 55 : 0.018856851650931587
Loss in iteration 56 : 0.018396993785962205
Loss in iteration 57 : 0.01795442924597117
Loss in iteration 58 : 0.017529966140343346
Loss in iteration 59 : 0.017124491482069397
Loss in iteration 60 : 0.016749206892404805
Loss in iteration 61 : 0.016405448825368596
Loss in iteration 62 : 0.016074664348119726
Loss in iteration 63 : 0.015751924523330112
Loss in iteration 64 : 0.015446904607157112
Loss in iteration 65 : 0.01515740811476535
Loss in iteration 66 : 0.01487654685082408
Loss in iteration 67 : 0.014612372252285911
Loss in iteration 68 : 0.014354507541805136
Loss in iteration 69 : 0.01410633758887232
Loss in iteration 70 : 0.01386695364583107
Loss in iteration 71 : 0.013634213939456645
Loss in iteration 72 : 0.013407048123030207
Loss in iteration 73 : 0.013183475669439329
Loss in iteration 74 : 0.012963720389246768
Loss in iteration 75 : 0.012748879702834523
Loss in iteration 76 : 0.012542113186173477
Loss in iteration 77 : 0.012341187103010453
Loss in iteration 78 : 0.012144053453968663
Loss in iteration 79 : 0.01195096281882187
Loss in iteration 80 : 0.011760436943378267
Loss in iteration 81 : 0.011571666191634595
Loss in iteration 82 : 0.011384723443038751
Loss in iteration 83 : 0.011200072535838304
Loss in iteration 84 : 0.011019419030686487
Loss in iteration 85 : 0.010842304677976589
Loss in iteration 86 : 0.010672594344357765
Loss in iteration 87 : 0.010507861047255668
Loss in iteration 88 : 0.010344121331978824
Loss in iteration 89 : 0.010181270923041316
Loss in iteration 90 : 0.010019672740556847
Loss in iteration 91 : 0.009862164812111742
Loss in iteration 92 : 0.009707176201471964
Loss in iteration 93 : 0.009553433791265043
Loss in iteration 94 : 0.009400776957801237
Loss in iteration 95 : 0.009249359598638044
Loss in iteration 96 : 0.009099643592419637
Loss in iteration 97 : 0.008951445472828244
Loss in iteration 98 : 0.0088036247078427
Loss in iteration 99 : 0.00865840651618246
Loss in iteration 100 : 0.00851413861726831
Loss in iteration 101 : 0.008370781357348065
Loss in iteration 102 : 0.00823171524757855
Loss in iteration 103 : 0.008100424655740764
Loss in iteration 104 : 0.007969992612865044
Loss in iteration 105 : 0.007840726684890055
Loss in iteration 106 : 0.007714685524274116
Loss in iteration 107 : 0.007590922451645169
Loss in iteration 108 : 0.007467969655240616
Loss in iteration 109 : 0.007346350828391932
Loss in iteration 110 : 0.007226187003263371
Loss in iteration 111 : 0.007106990770224959
Loss in iteration 112 : 0.0069895434527041745
Loss in iteration 113 : 0.006875382947373441
Loss in iteration 114 : 0.006765770982157422
Loss in iteration 115 : 0.006662928981873881
Loss in iteration 116 : 0.006561862249756815
Loss in iteration 117 : 0.006461177747392632
Loss in iteration 118 : 0.006362903322348994
Loss in iteration 119 : 0.006267812945722076
Loss in iteration 120 : 0.006175294186434557
Loss in iteration 121 : 0.006083466434468668
Loss in iteration 122 : 0.005993747543202747
Loss in iteration 123 : 0.005907667771541771
Loss in iteration 124 : 0.0058251422662949194
Loss in iteration 125 : 0.005743634181668131
Loss in iteration 126 : 0.005662361364884666
Loss in iteration 127 : 0.005581225736530157
Loss in iteration 128 : 0.0055006815901000004
Loss in iteration 129 : 0.005422172711167527
Loss in iteration 130 : 0.005348279651843852
Loss in iteration 131 : 0.005279184958778166
Loss in iteration 132 : 0.005211746662412244
Loss in iteration 133 : 0.005145395761640673
Loss in iteration 134 : 0.005080182013921568
Loss in iteration 135 : 0.005016367905847223
Loss in iteration 136 : 0.004953403395981629
Loss in iteration 137 : 0.004891095752524249
Loss in iteration 138 : 0.0048327459368522575
Loss in iteration 139 : 0.004775450034592956
Loss in iteration 140 : 0.00472056916873161
Loss in iteration 141 : 0.0046669952225235935
Loss in iteration 142 : 0.00461608511953029
Loss in iteration 143 : 0.004565661398502527
Loss in iteration 144 : 0.004514996302018685
Loss in iteration 145 : 0.004466745443864384
Loss in iteration 146 : 0.004420417896616979
Loss in iteration 147 : 0.0043749015040185435
Loss in iteration 148 : 0.004330008081456898
Loss in iteration 149 : 0.004285573407330401
Loss in iteration 150 : 0.004241786156775163
Loss in iteration 151 : 0.004198967132269735
Loss in iteration 152 : 0.004156654376269093
Loss in iteration 153 : 0.004114663747803294
Loss in iteration 154 : 0.004072926807496841
Loss in iteration 155 : 0.004031410080811347
Loss in iteration 156 : 0.003990078667624121
Loss in iteration 157 : 0.003949528111774643
Loss in iteration 158 : 0.003909440579615488
Loss in iteration 159 : 0.0038695022307245306
Loss in iteration 160 : 0.0038298980076992323
Loss in iteration 161 : 0.003790666819126723
Loss in iteration 162 : 0.0037516096682257446
Loss in iteration 163 : 0.0037130201175762556
Loss in iteration 164 : 0.003674601843835236
Loss in iteration 165 : 0.003636237739477586
Loss in iteration 166 : 0.0035980890761980167
Loss in iteration 167 : 0.003561206856179113
Loss in iteration 168 : 0.003524871212164184
Loss in iteration 169 : 0.00348862965123628
Loss in iteration 170 : 0.0034524083903284103
Loss in iteration 171 : 0.0034160440559678506
Loss in iteration 172 : 0.003379550352070791
Loss in iteration 173 : 0.003343013826559137
Loss in iteration 174 : 0.003308692733104429
Loss in iteration 175 : 0.00327414442399697
Loss in iteration 176 : 0.003239189082632759
Loss in iteration 177 : 0.0032063643038331186
Loss in iteration 178 : 0.003174364384967033
Loss in iteration 179 : 0.0031417223495337657
Loss in iteration 180 : 0.0031085338328267847
Loss in iteration 181 : 0.003075696717113571
Loss in iteration 182 : 0.0030425802251702674
Loss in iteration 183 : 0.003012883602811537
Loss in iteration 184 : 0.0029836303137140943
Loss in iteration 185 : 0.002954451980594325
Loss in iteration 186 : 0.002925452143110354
Loss in iteration 187 : 0.0028966522290568664
Loss in iteration 188 : 0.002868454806567365
Loss in iteration 189 : 0.0028428844702461553
Loss in iteration 190 : 0.002817616404682898
Loss in iteration 191 : 0.0027928034248764205
Loss in iteration 192 : 0.002769713842893234
Loss in iteration 193 : 0.002747994622506304
Loss in iteration 194 : 0.002726117787769895
Loss in iteration 195 : 0.002704382016197745
Loss in iteration 196 : 0.002684073264460851
Loss in iteration 197 : 0.002662962060258196
Loss in iteration 198 : 0.0026411215098384308
Loss in iteration 199 : 0.0026190188718241874
Loss in iteration 200 : 0.0025976247234455394
Testing accuracy  of updater 9 on alg 1 with rate 0.008 = 0.9866666666666667, training accuracy 0.9994284081166047, time elapsed: 4006 millisecond.
Loss in iteration 1 : 1.0
Loss in iteration 2 : 0.9873858782994347
Loss in iteration 3 : 0.964196481734681
Loss in iteration 4 : 0.9321618086892955
Loss in iteration 5 : 0.8927177030847332
Loss in iteration 6 : 0.8470640522191389
Loss in iteration 7 : 0.7962091671179217
Loss in iteration 8 : 0.7410042477279338
Loss in iteration 9 : 0.6821705609326929
Loss in iteration 10 : 0.6203211521207415
Loss in iteration 11 : 0.5559795652425087
Loss in iteration 12 : 0.4935174174247965
Loss in iteration 13 : 0.4463416111027804
Loss in iteration 14 : 0.4117218835734161
Loss in iteration 15 : 0.3765171372307679
Loss in iteration 16 : 0.33606503742277094
Loss in iteration 17 : 0.29471968409113747
Loss in iteration 18 : 0.2606777744402828
Loss in iteration 19 : 0.23763375290950775
Loss in iteration 20 : 0.22453454240628562
Loss in iteration 21 : 0.2162844033246643
Loss in iteration 22 : 0.21181865357561297
Loss in iteration 23 : 0.20814011071487232
Loss in iteration 24 : 0.2042629241866832
Loss in iteration 25 : 0.20017477464420383
Loss in iteration 26 : 0.19559146332771582
Loss in iteration 27 : 0.19084017193470937
Loss in iteration 28 : 0.18627492382098657
Loss in iteration 29 : 0.18198442306231505
Loss in iteration 30 : 0.17808726454795287
Loss in iteration 31 : 0.17460920628527385
Loss in iteration 32 : 0.17138465300296668
Loss in iteration 33 : 0.16829691077717962
Loss in iteration 34 : 0.16534337160808776
Loss in iteration 35 : 0.162348457010192
Loss in iteration 36 : 0.1592919612831809
Loss in iteration 37 : 0.15619563370196168
Loss in iteration 38 : 0.15306051635452567
Loss in iteration 39 : 0.14987533132741146
Loss in iteration 40 : 0.14665324337005287
Loss in iteration 41 : 0.14343553068623216
Loss in iteration 42 : 0.14022980122660142
Loss in iteration 43 : 0.137082387975573
Loss in iteration 44 : 0.1339870989238463
Loss in iteration 45 : 0.13100857521883297
Loss in iteration 46 : 0.12816200264898445
Loss in iteration 47 : 0.12538834106403146
Loss in iteration 48 : 0.12272924432768136
Loss in iteration 49 : 0.1201487580763025
Loss in iteration 50 : 0.11763581628972994
Loss in iteration 51 : 0.11519604931635127
Loss in iteration 52 : 0.11283672025267094
Loss in iteration 53 : 0.11050534196652254
Loss in iteration 54 : 0.10819992124769175
Loss in iteration 55 : 0.10593267923551078
Loss in iteration 56 : 0.10375495604641882
Loss in iteration 57 : 0.10165240265173286
Loss in iteration 58 : 0.0996024489043424
Loss in iteration 59 : 0.09760517317571249
Loss in iteration 60 : 0.09566963303557854
Loss in iteration 61 : 0.09380606516709955
Loss in iteration 62 : 0.09202729102293365
Loss in iteration 63 : 0.090335197610176
Loss in iteration 64 : 0.08872016599294195
Loss in iteration 65 : 0.08716229762099893
Loss in iteration 66 : 0.0856735816831995
Loss in iteration 67 : 0.08425413220997859
Loss in iteration 68 : 0.08286749859998963
Loss in iteration 69 : 0.08150865834771814
Loss in iteration 70 : 0.08017489580655997
Loss in iteration 71 : 0.07889555487073992
Loss in iteration 72 : 0.0776578454989484
Loss in iteration 73 : 0.07646716932395102
Loss in iteration 74 : 0.0753065890645026
Loss in iteration 75 : 0.07418561172255586
Loss in iteration 76 : 0.07310349702743987
Loss in iteration 77 : 0.07206429720676648
Loss in iteration 78 : 0.07105060261805914
Loss in iteration 79 : 0.07006155913811891
Loss in iteration 80 : 0.06910041830949525
Loss in iteration 81 : 0.06816575174890092
Loss in iteration 82 : 0.0672550797099468
Loss in iteration 83 : 0.06636697172979943
Loss in iteration 84 : 0.06550058912347433
Loss in iteration 85 : 0.0646494651396735
Loss in iteration 86 : 0.06381349929173749
Loss in iteration 87 : 0.06299243262597777
Loss in iteration 88 : 0.06218574689196109
Loss in iteration 89 : 0.061392427877506796
Loss in iteration 90 : 0.06061346817577639
Loss in iteration 91 : 0.059848671118552174
Loss in iteration 92 : 0.05910360249559363
Loss in iteration 93 : 0.05837758150941119
Loss in iteration 94 : 0.05766881703524155
Loss in iteration 95 : 0.05697493634656286
Loss in iteration 96 : 0.05629616661504895
Loss in iteration 97 : 0.055631478658500204
Loss in iteration 98 : 0.05498059573104895
Loss in iteration 99 : 0.05434185892454318
Loss in iteration 100 : 0.05371703897443106
Loss in iteration 101 : 0.053110951004609726
Loss in iteration 102 : 0.05252300066158295
Loss in iteration 103 : 0.05195406047049701
Loss in iteration 104 : 0.05139526903184646
Loss in iteration 105 : 0.05084406663890124
Loss in iteration 106 : 0.05030049096437112
Loss in iteration 107 : 0.04976337336064567
Loss in iteration 108 : 0.04923438173879612
Loss in iteration 109 : 0.048713226311597545
Loss in iteration 110 : 0.04820307084801612
Loss in iteration 111 : 0.04770414337864158
Loss in iteration 112 : 0.04721700279516284
Loss in iteration 113 : 0.046737824245048194
Loss in iteration 114 : 0.046269325256692174
Loss in iteration 115 : 0.045807726141140565
Loss in iteration 116 : 0.04535212290256672
Loss in iteration 117 : 0.044902101635204944
Loss in iteration 118 : 0.04445764298095079
Loss in iteration 119 : 0.04402055840757124
Loss in iteration 120 : 0.04359142469947061
Loss in iteration 121 : 0.043169507901849286
Loss in iteration 122 : 0.042753567050712746
Loss in iteration 123 : 0.04234373744822824
Loss in iteration 124 : 0.04193927039634809
Loss in iteration 125 : 0.04154086497310147
Loss in iteration 126 : 0.04114786647373696
Loss in iteration 127 : 0.040761179975541864
Loss in iteration 128 : 0.040384878882365374
Loss in iteration 129 : 0.04001613029146809
Loss in iteration 130 : 0.03965447827764495
Loss in iteration 131 : 0.039298574414972476
Loss in iteration 132 : 0.038946789772153115
Loss in iteration 133 : 0.03859860620584128
Loss in iteration 134 : 0.0382552046827338
Loss in iteration 135 : 0.03791629996123517
Loss in iteration 136 : 0.037582041729412395
Loss in iteration 137 : 0.03725315077467458
Loss in iteration 138 : 0.03692772475950564
Loss in iteration 139 : 0.03660529104901027
Loss in iteration 140 : 0.03628795661159722
Loss in iteration 141 : 0.03597583108496364
Loss in iteration 142 : 0.0356678447902265
Loss in iteration 143 : 0.035363506136502604
Loss in iteration 144 : 0.03506230873254431
Loss in iteration 145 : 0.03476489067925359
Loss in iteration 146 : 0.034470655450947206
Loss in iteration 147 : 0.03418016010665618
Loss in iteration 148 : 0.033892688849200825
Loss in iteration 149 : 0.033607939139365876
Loss in iteration 150 : 0.03332556610998709
Loss in iteration 151 : 0.033045640222925596
Loss in iteration 152 : 0.03276783936569211
Loss in iteration 153 : 0.03249210605203427
Loss in iteration 154 : 0.03221986850130387
Loss in iteration 155 : 0.03194998279622501
Loss in iteration 156 : 0.031682372667087975
Loss in iteration 157 : 0.0314170808354862
Loss in iteration 158 : 0.031154554772939554
Loss in iteration 159 : 0.030895080513439428
Loss in iteration 160 : 0.0306391627119263
Loss in iteration 161 : 0.030384461407511693
Loss in iteration 162 : 0.03013323579165176
Loss in iteration 163 : 0.029884640250902206
Loss in iteration 164 : 0.029638439441653775
Loss in iteration 165 : 0.02939517040233727
Loss in iteration 166 : 0.02915522168221416
Loss in iteration 167 : 0.028917877325296267
Loss in iteration 168 : 0.02868184645025122
Loss in iteration 169 : 0.028448142064074592
Loss in iteration 170 : 0.028217058690905093
Loss in iteration 171 : 0.027989431198874704
Loss in iteration 172 : 0.027765584220754584
Loss in iteration 173 : 0.027544585758667937
Loss in iteration 174 : 0.027326608225050123
Loss in iteration 175 : 0.027112514780681204
Loss in iteration 176 : 0.026900602515576626
Loss in iteration 177 : 0.02669248964278222
Loss in iteration 178 : 0.026487483700825515
Loss in iteration 179 : 0.026286228506190508
Loss in iteration 180 : 0.02608984726174295
Loss in iteration 181 : 0.02589594679403832
Loss in iteration 182 : 0.025705476156023272
Loss in iteration 183 : 0.02551721135320231
Loss in iteration 184 : 0.025332347262888896
Loss in iteration 185 : 0.025149565740056813
Loss in iteration 186 : 0.02496858181211393
Loss in iteration 187 : 0.02478931389368504
Loss in iteration 188 : 0.02461265372947357
Loss in iteration 189 : 0.024438076822252432
Loss in iteration 190 : 0.02426646994776307
Loss in iteration 191 : 0.024096851260296176
Loss in iteration 192 : 0.023929939444628106
Loss in iteration 193 : 0.023766311646231098
Loss in iteration 194 : 0.023604946378007287
Loss in iteration 195 : 0.02344648641374557
Loss in iteration 196 : 0.023289360106341476
Loss in iteration 197 : 0.023134687943942713
Loss in iteration 198 : 0.022981660016428946
Loss in iteration 199 : 0.02282996153317131
Loss in iteration 200 : 0.022679602773317122
Testing accuracy  of updater 9 on alg 1 with rate 0.0019999999999999983 = 0.9857777777777778, training accuracy 0.996856244641326, time elapsed: 4138 millisecond.
