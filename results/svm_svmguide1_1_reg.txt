objc[3186]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x10f5b44c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x1105d94e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 09:41:57 INFO SparkContext: Running Spark version 2.0.0
18/02/27 09:41:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 09:41:57 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 09:41:57 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 09:41:57 INFO SecurityManager: Changing view acls groups to: 
18/02/27 09:41:57 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 09:41:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 09:41:58 INFO Utils: Successfully started service 'sparkDriver' on port 50619.
18/02/27 09:41:58 INFO SparkEnv: Registering MapOutputTracker
18/02/27 09:41:58 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 09:41:58 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-804bf3f0-4397-4e12-aa2d-613af96bca16
18/02/27 09:41:58 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 09:41:58 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 09:41:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 09:41:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 09:41:59 INFO Executor: Starting executor ID driver on host localhost
18/02/27 09:41:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50620.
18/02/27 09:41:59 INFO NettyBlockTransferService: Server created on 192.168.2.140:50620
18/02/27 09:41:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50620)
18/02/27 09:41:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50620 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50620)
18/02/27 09:41:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50620)
Loss in iteration 1 : 1.009303748770601
Loss in iteration 2 : 8.060323731617226
Loss in iteration 3 : 4.9901760679866
Loss in iteration 4 : 1.9310663204348293
Loss in iteration 5 : 0.8570529120554204
Loss in iteration 6 : 0.7709029614809826
Loss in iteration 7 : 0.7191062784432787
Loss in iteration 8 : 0.6994602499027376
Loss in iteration 9 : 0.672073347893521
Loss in iteration 10 : 0.6566520112426997
Loss in iteration 11 : 0.6378444856223358
Loss in iteration 12 : 0.6287048753588127
Loss in iteration 13 : 0.6312509905495932
Loss in iteration 14 : 0.6597564244372724
Loss in iteration 15 : 0.727886305185163
Loss in iteration 16 : 1.0127616191643511
Loss in iteration 17 : 1.3633226923334045
Loss in iteration 18 : 2.173551236980077
Loss in iteration 19 : 0.7669497413343014
Loss in iteration 20 : 0.8540599752318215
Loss in iteration 21 : 0.8405557454094947
Loss in iteration 22 : 1.0434316966055381
Loss in iteration 23 : 1.0578608759147108
Loss in iteration 24 : 1.4161773057124623
Loss in iteration 25 : 1.0754798382401298
Loss in iteration 26 : 1.2788899209704252
Loss in iteration 27 : 0.9723878000540733
Loss in iteration 28 : 1.047409963801018
Loss in iteration 29 : 0.8598374453995138
Loss in iteration 30 : 0.8837208912015578
Loss in iteration 31 : 0.7855314074430519
Loss in iteration 32 : 0.8461314787620416
Loss in iteration 33 : 0.7971179520868266
Loss in iteration 34 : 0.8914275917013745
Loss in iteration 35 : 0.8538546809159577
Loss in iteration 36 : 1.0507607819815914
Loss in iteration 37 : 1.0052095250530577
Loss in iteration 38 : 1.3066102197376728
Loss in iteration 39 : 1.0340613133898755
Loss in iteration 40 : 1.2244102592952297
Loss in iteration 41 : 0.9462916034939531
Loss in iteration 42 : 1.0439538076631174
Loss in iteration 43 : 0.8544855384168524
Loss in iteration 44 : 0.8946592627893537
Loss in iteration 45 : 0.7865808270016195
Loss in iteration 46 : 0.8487755835373759
Loss in iteration 47 : 0.7801876305017358
Loss in iteration 48 : 0.8687397754867219
Loss in iteration 49 : 0.8412021682335037
Loss in iteration 50 : 1.051235017221771
Loss in iteration 51 : 0.9931521142037233
Loss in iteration 52 : 1.3047052719391563
Loss in iteration 53 : 1.0329923929352227
Loss in iteration 54 : 1.2474447617200581
Loss in iteration 55 : 0.9388803244961896
Loss in iteration 56 : 1.0583284255441805
Loss in iteration 57 : 0.8571168886769532
Loss in iteration 58 : 0.9032460589707626
Loss in iteration 59 : 0.7764783273077083
Loss in iteration 60 : 0.8350503300145137
Loss in iteration 61 : 0.7689824538650433
Loss in iteration 62 : 0.859049267510408
Loss in iteration 63 : 0.8235148625237452
Loss in iteration 64 : 1.009593362660442
Loss in iteration 65 : 0.9728623904789614
Loss in iteration 66 : 1.2955375065898675
Loss in iteration 67 : 1.0417009065323226
Loss in iteration 68 : 1.2737398248595946
Loss in iteration 69 : 0.9560585162366412
Loss in iteration 70 : 1.082312643795158
Loss in iteration 71 : 0.8679557153482754
Loss in iteration 72 : 0.9161720168785645
Loss in iteration 73 : 0.7801207068869109
Loss in iteration 74 : 0.8242244933859066
Loss in iteration 75 : 0.7674209258527575
Loss in iteration 76 : 0.8378010590853656
Loss in iteration 77 : 0.7872902205609179
Loss in iteration 78 : 0.933506885422697
Loss in iteration 79 : 0.9148625674812292
Loss in iteration 80 : 1.1970645037110799
Loss in iteration 81 : 1.042480360748264
Loss in iteration 82 : 1.3458923019828521
Loss in iteration 83 : 0.9851615587723132
Loss in iteration 84 : 1.1025738129429543
Loss in iteration 85 : 0.8749187955862785
Loss in iteration 86 : 0.9367063057230554
Loss in iteration 87 : 0.7943873316092279
Loss in iteration 88 : 0.8346510012800433
Loss in iteration 89 : 0.7642584902889242
Loss in iteration 90 : 0.8399588459741417
Loss in iteration 91 : 0.7866383230567783
Loss in iteration 92 : 0.9405765991956782
Loss in iteration 93 : 0.903688452787654
Loss in iteration 94 : 1.1787971288456398
Loss in iteration 95 : 1.012476691284353
Loss in iteration 96 : 1.30503934375703
Loss in iteration 97 : 0.9978821679664761
Loss in iteration 98 : 1.1401097823859818
Loss in iteration 99 : 0.89523241571691
Loss in iteration 100 : 0.9614706623790589
Loss in iteration 101 : 0.8100536721357292
Loss in iteration 102 : 0.8555912094981263
Loss in iteration 103 : 0.7597460078671826
Loss in iteration 104 : 0.8116594423732111
Loss in iteration 105 : 0.7725715389542984
Loss in iteration 106 : 0.9019785422794467
Loss in iteration 107 : 0.8797389051553393
Loss in iteration 108 : 1.1575967038538306
Loss in iteration 109 : 1.0229234053101026
Loss in iteration 110 : 1.3259389195743878
Loss in iteration 111 : 1.0019443658767464
Loss in iteration 112 : 1.1577910230932267
Loss in iteration 113 : 0.8861642635915205
Loss in iteration 114 : 0.9554304166161932
Loss in iteration 115 : 0.8058614675599756
Loss in iteration 116 : 0.8463994675636611
Loss in iteration 117 : 0.7630865152778905
Loss in iteration 118 : 0.8173438243305696
Loss in iteration 119 : 0.7773986347923201
Loss in iteration 120 : 0.9033549733034136
Loss in iteration 121 : 0.8806416228163286
Loss in iteration 122 : 1.1558537472869967
Loss in iteration 123 : 1.0119108404647883
Loss in iteration 124 : 1.3081235119749965
Loss in iteration 125 : 0.9980637717219462
Loss in iteration 126 : 1.1665131313832453
Loss in iteration 127 : 0.8966489199055151
Loss in iteration 128 : 0.9581384872822317
Loss in iteration 129 : 0.8052973221447386
Loss in iteration 130 : 0.8506291644637672
Loss in iteration 131 : 0.7606445282560659
Loss in iteration 132 : 0.8104577171980609
Loss in iteration 133 : 0.7708550153909187
Loss in iteration 134 : 0.8910059025765907
Loss in iteration 135 : 0.8683372879002133
Loss in iteration 136 : 1.1304102731794057
Loss in iteration 137 : 1.0141858610585404
Loss in iteration 138 : 1.320661978196
Loss in iteration 139 : 1.0081900279491454
Loss in iteration 140 : 1.1692221736413124
Loss in iteration 141 : 0.8999532467055701
Loss in iteration 142 : 0.9773254770056776
Loss in iteration 143 : 0.8098632292523126
Loss in iteration 144 : 0.8509623298691916
Loss in iteration 145 : 0.7609351481955161
Loss in iteration 146 : 0.8108253463079694
Loss in iteration 147 : 0.7692367050280035
Loss in iteration 148 : 0.8850251827819988
Loss in iteration 149 : 0.8560529984290862
Loss in iteration 150 : 1.1221262593865866
Loss in iteration 151 : 1.0193565096003134
Loss in iteration 152 : 1.3370460129847386
Loss in iteration 153 : 0.999195911387745
Loss in iteration 154 : 1.1657045150382452
Loss in iteration 155 : 0.891894617879976
Loss in iteration 156 : 0.9612874863671692
Loss in iteration 157 : 0.8073638819556053
Loss in iteration 158 : 0.846863246042402
Loss in iteration 159 : 0.7637055322933455
Loss in iteration 160 : 0.8268514073274584
Loss in iteration 161 : 0.7767430023223817
Loss in iteration 162 : 0.8969188016629698
Loss in iteration 163 : 0.8726748148782023
Loss in iteration 164 : 1.1360196562285223
Loss in iteration 165 : 1.0066862211428484
Loss in iteration 166 : 1.3140863399326401
Loss in iteration 167 : 0.9990133868505295
Loss in iteration 168 : 1.1636890020426065
Loss in iteration 169 : 0.8861330815604662
Loss in iteration 170 : 0.9569389638278654
Loss in iteration 171 : 0.8046291767048154
Loss in iteration 172 : 0.8468978851769154
Loss in iteration 173 : 0.7629792300006574
Loss in iteration 174 : 0.8273426752112572
Loss in iteration 175 : 0.7760166794965316
Loss in iteration 176 : 0.9014896664085198
Loss in iteration 177 : 0.875725672397484
Loss in iteration 178 : 1.1351650847810482
Loss in iteration 179 : 1.0075648923665028
Loss in iteration 180 : 1.314071967113113
Loss in iteration 181 : 0.9959469033679262
Loss in iteration 182 : 1.166547452801536
Loss in iteration 183 : 0.8909563319946716
Loss in iteration 184 : 0.958862174726153
Loss in iteration 185 : 0.8030705357527365
Loss in iteration 186 : 0.843908394838188
Loss in iteration 187 : 0.7634217137639976
Loss in iteration 188 : 0.8270751704165431
Loss in iteration 189 : 0.7738656099511522
Loss in iteration 190 : 0.8892034558649834
Loss in iteration 191 : 0.8618697283593888
Loss in iteration 192 : 1.1234802949619611
Loss in iteration 193 : 1.0097288279374983
Loss in iteration 194 : 1.3271638889509594
Loss in iteration 195 : 1.0050833621942583
Loss in iteration 196 : 1.1756576141971107
Loss in iteration 197 : 0.8939655495592127
Loss in iteration 198 : 0.9657187083295596
Loss in iteration 199 : 0.8085287946358299
Loss in iteration 200 : 0.846336572450715
Testing accuracy  of updater 0 on alg 1 with rate 0.0027999999999999995 = 0.79075, training accuracy 0.7999352541275494, time elapsed: 7908 millisecond.
Loss in iteration 1 : 1.0045588368975944
Loss in iteration 2 : 5.7467636436484995
Loss in iteration 3 : 3.5989322225477234
Loss in iteration 4 : 1.4559306716518028
Loss in iteration 5 : 0.6882405288205464
Loss in iteration 6 : 0.6391673389753063
Loss in iteration 7 : 0.5974909186762222
Loss in iteration 8 : 0.5816891184924888
Loss in iteration 9 : 0.5658457613166531
Loss in iteration 10 : 0.5582423130654471
Loss in iteration 11 : 0.5564353515910436
Loss in iteration 12 : 0.5562578467232929
Loss in iteration 13 : 0.5714654537578966
Loss in iteration 14 : 0.6355612703761137
Loss in iteration 15 : 0.7291959044833178
Loss in iteration 16 : 1.030368424071394
Loss in iteration 17 : 0.9734498193729049
Loss in iteration 18 : 1.2935092306919098
Loss in iteration 19 : 0.7733157341580263
Loss in iteration 20 : 0.8272385591022653
Loss in iteration 21 : 0.7220263336686062
Loss in iteration 22 : 0.7627476392585403
Loss in iteration 23 : 0.676156630998231
Loss in iteration 24 : 0.6936084798854736
Loss in iteration 25 : 0.6492871338651046
Loss in iteration 26 : 0.6857745741635239
Loss in iteration 27 : 0.6674679841030592
Loss in iteration 28 : 0.7392161061046211
Loss in iteration 29 : 0.7488108241635612
Loss in iteration 30 : 0.9175629504904343
Loss in iteration 31 : 0.8281182050470365
Loss in iteration 32 : 0.9893288649550497
Loss in iteration 33 : 0.7927126025087088
Loss in iteration 34 : 0.8623343243001669
Loss in iteration 35 : 0.7200503088081162
Loss in iteration 36 : 0.7407826965064797
Loss in iteration 37 : 0.6555653688853774
Loss in iteration 38 : 0.6658073289660829
Loss in iteration 39 : 0.6251969934284796
Loss in iteration 40 : 0.6556535682633443
Loss in iteration 41 : 0.6364767132207911
Loss in iteration 42 : 0.6889788326071777
Loss in iteration 43 : 0.685652215843532
Loss in iteration 44 : 0.8269618007760218
Loss in iteration 45 : 0.8121416271519749
Loss in iteration 46 : 1.0101641591856678
Loss in iteration 47 : 0.8112153013464433
Loss in iteration 48 : 0.928104345638017
Loss in iteration 49 : 0.7412962661430718
Loss in iteration 50 : 0.7830108374564925
Loss in iteration 51 : 0.67494960044862
Loss in iteration 52 : 0.690098444568479
Loss in iteration 53 : 0.6248554702156663
Loss in iteration 54 : 0.6477380041189718
Loss in iteration 55 : 0.6242540841324248
Loss in iteration 56 : 0.6642543465991085
Loss in iteration 57 : 0.6499049524600972
Loss in iteration 58 : 0.7507147010906933
Loss in iteration 59 : 0.7467598972182211
Loss in iteration 60 : 0.9279769989186195
Loss in iteration 61 : 0.8177090985465834
Loss in iteration 62 : 0.9899304384079769
Loss in iteration 63 : 0.783091421050842
Loss in iteration 64 : 0.8465573217410456
Loss in iteration 65 : 0.6998062948317757
Loss in iteration 66 : 0.7263454714746548
Loss in iteration 67 : 0.6389030811927738
Loss in iteration 68 : 0.6495883514426218
Loss in iteration 69 : 0.6174368127754859
Loss in iteration 70 : 0.6416287366856868
Loss in iteration 71 : 0.6297977647444627
Loss in iteration 72 : 0.6999174918020911
Loss in iteration 73 : 0.6969224038818379
Loss in iteration 74 : 0.8621409626196043
Loss in iteration 75 : 0.8057958184296458
Loss in iteration 76 : 0.9984519527518483
Loss in iteration 77 : 0.7924578162867909
Loss in iteration 78 : 0.8789993865573362
Loss in iteration 79 : 0.7276772904590132
Loss in iteration 80 : 0.7844868816310276
Loss in iteration 81 : 0.6717950444760756
Loss in iteration 82 : 0.690991554243513
Loss in iteration 83 : 0.623677726025956
Loss in iteration 84 : 0.6386363906504017
Loss in iteration 85 : 0.6189601326916256
Loss in iteration 86 : 0.6598716679377585
Loss in iteration 87 : 0.645065003625834
Loss in iteration 88 : 0.7431228769406951
Loss in iteration 89 : 0.7430417356466829
Loss in iteration 90 : 0.9153491668361606
Loss in iteration 91 : 0.7971507782089478
Loss in iteration 92 : 0.9631273718134207
Loss in iteration 93 : 0.7540875206294716
Loss in iteration 94 : 0.8261025706277713
Loss in iteration 95 : 0.7025522332999082
Loss in iteration 96 : 0.738278762040499
Loss in iteration 97 : 0.6512623224953196
Loss in iteration 98 : 0.6834235854796064
Loss in iteration 99 : 0.6249530557787484
Loss in iteration 100 : 0.6529362207311662
Loss in iteration 101 : 0.6334979865696994
Loss in iteration 102 : 0.7048195201600502
Loss in iteration 103 : 0.689956976862198
Loss in iteration 104 : 0.8446828130157344
Loss in iteration 105 : 0.7905736381644745
Loss in iteration 106 : 0.9681599106000924
Loss in iteration 107 : 0.7725066157106587
Loss in iteration 108 : 0.8481881607511879
Loss in iteration 109 : 0.7134153324859109
Loss in iteration 110 : 0.7698437188782994
Loss in iteration 111 : 0.6692670484060022
Loss in iteration 112 : 0.6924388638216084
Loss in iteration 113 : 0.623794574895184
Loss in iteration 114 : 0.6475545801138362
Loss in iteration 115 : 0.6245359760825948
Loss in iteration 116 : 0.6835730529807449
Loss in iteration 117 : 0.6549000362162881
Loss in iteration 118 : 0.7510930312472054
Loss in iteration 119 : 0.7427554883528573
Loss in iteration 120 : 0.9069291262735765
Loss in iteration 121 : 0.789902930723351
Loss in iteration 122 : 0.9512666842511996
Loss in iteration 123 : 0.7494700511346163
Loss in iteration 124 : 0.8209258374804833
Loss in iteration 125 : 0.6932008397591353
Loss in iteration 126 : 0.7241447645680322
Loss in iteration 127 : 0.6350123279081848
Loss in iteration 128 : 0.6606405495971029
Loss in iteration 129 : 0.627956588969706
Loss in iteration 130 : 0.6736960978996867
Loss in iteration 131 : 0.6414997917168519
Loss in iteration 132 : 0.7276952472632835
Loss in iteration 133 : 0.7238395220612514
Loss in iteration 134 : 0.8626403360757824
Loss in iteration 135 : 0.7855091661123355
Loss in iteration 136 : 0.9442110348535359
Loss in iteration 137 : 0.7565933573181916
Loss in iteration 138 : 0.8337090977116473
Loss in iteration 139 : 0.701454835802975
Loss in iteration 140 : 0.7517327324035026
Loss in iteration 141 : 0.6605089706191303
Loss in iteration 142 : 0.6921673117405952
Loss in iteration 143 : 0.6282589103161489
Loss in iteration 144 : 0.6573349000958942
Loss in iteration 145 : 0.6316285158389577
Loss in iteration 146 : 0.6985003540649635
Loss in iteration 147 : 0.6817512303193006
Loss in iteration 148 : 0.8195705125640923
Loss in iteration 149 : 0.7762256689529929
Loss in iteration 150 : 0.9417104790922171
Loss in iteration 151 : 0.7654184004268579
Loss in iteration 152 : 0.8452531434262465
Loss in iteration 153 : 0.7178825560275753
Loss in iteration 154 : 0.7884248895187358
Loss in iteration 155 : 0.684641635897378
Loss in iteration 156 : 0.7167267085434124
Loss in iteration 157 : 0.636552668450256
Loss in iteration 158 : 0.6630851077245097
Loss in iteration 159 : 0.6256828294356784
Loss in iteration 160 : 0.672950469310433
Loss in iteration 161 : 0.6398410364169665
Loss in iteration 162 : 0.7293563524868205
Loss in iteration 163 : 0.7235503028185198
Loss in iteration 164 : 0.8687636638066073
Loss in iteration 165 : 0.7814962803309357
Loss in iteration 166 : 0.939801276537289
Loss in iteration 167 : 0.7572269297820736
Loss in iteration 168 : 0.8365888934766893
Loss in iteration 169 : 0.7164059187978798
Loss in iteration 170 : 0.7797383494162728
Loss in iteration 171 : 0.6784491666987491
Loss in iteration 172 : 0.7111799088328152
Loss in iteration 173 : 0.6300146146276171
Loss in iteration 174 : 0.6514588197419008
Loss in iteration 175 : 0.6219180718835231
Loss in iteration 176 : 0.6693630038837376
Loss in iteration 177 : 0.6432454199114082
Loss in iteration 178 : 0.7360964810103272
Loss in iteration 179 : 0.7331840814670302
Loss in iteration 180 : 0.8898399578912741
Loss in iteration 181 : 0.7793675114948103
Loss in iteration 182 : 0.9298910107942281
Loss in iteration 183 : 0.7525933850910924
Loss in iteration 184 : 0.8221092673063235
Loss in iteration 185 : 0.6955083470042317
Loss in iteration 186 : 0.7321151365274047
Loss in iteration 187 : 0.6492575381096218
Loss in iteration 188 : 0.6899093089058586
Loss in iteration 189 : 0.6296699890139512
Loss in iteration 190 : 0.6704078959773927
Loss in iteration 191 : 0.6352564994334653
Loss in iteration 192 : 0.705035275463329
Loss in iteration 193 : 0.6896534103689188
Loss in iteration 194 : 0.8255593235190867
Loss in iteration 195 : 0.7875858991873822
Loss in iteration 196 : 0.9556708553524579
Loss in iteration 197 : 0.7664754900593335
Loss in iteration 198 : 0.8459424244146367
Loss in iteration 199 : 0.7148582548901259
Loss in iteration 200 : 0.7744983231110628
Testing accuracy  of updater 0 on alg 1 with rate 0.00196 = 0.7875, training accuracy 0.79281320815798, time elapsed: 4588 millisecond.
Loss in iteration 1 : 1.001488599803296
Loss in iteration 2 : 3.434253280740304
Loss in iteration 3 : 2.207647673352569
Loss in iteration 4 : 0.9825178000152008
Loss in iteration 5 : 0.5238163193198445
Loss in iteration 6 : 0.515969112438957
Loss in iteration 7 : 0.5035602258414995
Loss in iteration 8 : 0.49819546835283235
Loss in iteration 9 : 0.49084154972052424
Loss in iteration 10 : 0.49317167820080654
Loss in iteration 11 : 0.49335472647268686
Loss in iteration 12 : 0.5129078158261101
Loss in iteration 13 : 0.5341674132204051
Loss in iteration 14 : 0.5730780921078892
Loss in iteration 15 : 0.5910355151415163
Loss in iteration 16 : 0.6426602339884216
Loss in iteration 17 : 0.6089864409237128
Loss in iteration 18 : 0.629339890855748
Loss in iteration 19 : 0.5791669833300319
Loss in iteration 20 : 0.5823052582084711
Loss in iteration 21 : 0.5353155144657694
Loss in iteration 22 : 0.5326187001374847
Loss in iteration 23 : 0.5101050104043281
Loss in iteration 24 : 0.5049459654262417
Loss in iteration 25 : 0.4984087111947553
Loss in iteration 26 : 0.49966346273576917
Loss in iteration 27 : 0.503697329694266
Loss in iteration 28 : 0.5218475911710718
Loss in iteration 29 : 0.5279196117391759
Loss in iteration 30 : 0.5577351150478497
Loss in iteration 31 : 0.555757687267684
Loss in iteration 32 : 0.5978308925139736
Loss in iteration 33 : 0.5722657297642331
Loss in iteration 34 : 0.5998920597683394
Loss in iteration 35 : 0.5540212006032619
Loss in iteration 36 : 0.5704942707397243
Loss in iteration 37 : 0.5348689488267275
Loss in iteration 38 : 0.5338388201161673
Loss in iteration 39 : 0.5157700001503982
Loss in iteration 40 : 0.5197019278905355
Loss in iteration 41 : 0.506922364302239
Loss in iteration 42 : 0.5140927391300788
Loss in iteration 43 : 0.5086803213210489
Loss in iteration 44 : 0.5238916661070405
Loss in iteration 45 : 0.5263496880897544
Loss in iteration 46 : 0.5519249295647266
Loss in iteration 47 : 0.5429507355679514
Loss in iteration 48 : 0.5691342695224473
Loss in iteration 49 : 0.5458881089892721
Loss in iteration 50 : 0.5676241205034592
Loss in iteration 51 : 0.5367265130568712
Loss in iteration 52 : 0.5490922509144586
Loss in iteration 53 : 0.5269465604196515
Loss in iteration 54 : 0.5408995357813162
Loss in iteration 55 : 0.5233969621441408
Loss in iteration 56 : 0.5301460249794144
Loss in iteration 57 : 0.516437831900014
Loss in iteration 58 : 0.5286389090417855
Loss in iteration 59 : 0.5170052212317797
Loss in iteration 60 : 0.5320426075705038
Loss in iteration 61 : 0.5236550511643122
Loss in iteration 62 : 0.5432265317480357
Loss in iteration 63 : 0.527990314639114
Loss in iteration 64 : 0.5437492621157216
Loss in iteration 65 : 0.5285528492485897
Loss in iteration 66 : 0.5436282867123545
Loss in iteration 67 : 0.5284606780153794
Loss in iteration 68 : 0.543507413216743
Loss in iteration 69 : 0.5283698898202983
Loss in iteration 70 : 0.5427200438422234
Loss in iteration 71 : 0.5257750628688685
Loss in iteration 72 : 0.540274327912464
Loss in iteration 73 : 0.5226535164627777
Loss in iteration 74 : 0.5371694104392447
Loss in iteration 75 : 0.5191033294484045
Loss in iteration 76 : 0.5358946174962548
Loss in iteration 77 : 0.5202219583393536
Loss in iteration 78 : 0.5386149645822653
Loss in iteration 79 : 0.5213775476868165
Loss in iteration 80 : 0.5387097311645752
Loss in iteration 81 : 0.5202353331228349
Loss in iteration 82 : 0.5359075360335229
Loss in iteration 83 : 0.5197944428704394
Loss in iteration 84 : 0.535736185045375
Loss in iteration 85 : 0.519878619980134
Loss in iteration 86 : 0.534526299938367
Loss in iteration 87 : 0.5200033147163877
Loss in iteration 88 : 0.5358795277581233
Loss in iteration 89 : 0.5195740656581224
Loss in iteration 90 : 0.5362300922378604
Loss in iteration 91 : 0.52011544955301
Loss in iteration 92 : 0.5372025989309334
Loss in iteration 93 : 0.5212749244364584
Loss in iteration 94 : 0.5407617191500345
Loss in iteration 95 : 0.5230540050638233
Loss in iteration 96 : 0.5428005678651314
Loss in iteration 97 : 0.5224032972769845
Loss in iteration 98 : 0.5418802326638612
Loss in iteration 99 : 0.5217898347692772
Loss in iteration 100 : 0.5387633283807379
Loss in iteration 101 : 0.5207307448855713
Loss in iteration 102 : 0.5382270398349241
Loss in iteration 103 : 0.5203949991832967
Loss in iteration 104 : 0.537315120061336
Loss in iteration 105 : 0.5168448928534198
Loss in iteration 106 : 0.5304883345435004
Loss in iteration 107 : 0.5193566892069739
Loss in iteration 108 : 0.537062657830053
Loss in iteration 109 : 0.5204563186147309
Loss in iteration 110 : 0.5401223261059495
Loss in iteration 111 : 0.5212786000222808
Loss in iteration 112 : 0.5407500447218522
Loss in iteration 113 : 0.520616584452474
Loss in iteration 114 : 0.5398920405056313
Loss in iteration 115 : 0.5195875785893243
Loss in iteration 116 : 0.5361462675831843
Loss in iteration 117 : 0.5188382097389227
Loss in iteration 118 : 0.535653351642989
Loss in iteration 119 : 0.5184175760050856
Loss in iteration 120 : 0.5350369086010193
Loss in iteration 121 : 0.5190062173769735
Loss in iteration 122 : 0.5378530797853742
Loss in iteration 123 : 0.5194721719539499
Loss in iteration 124 : 0.5384283250579358
Loss in iteration 125 : 0.5188784887952372
Loss in iteration 126 : 0.5360739848525905
Loss in iteration 127 : 0.5200573994359219
Loss in iteration 128 : 0.5382714352021387
Loss in iteration 129 : 0.5189986397466373
Loss in iteration 130 : 0.5375247861950715
Loss in iteration 131 : 0.5197287704675726
Loss in iteration 132 : 0.538003171762418
Loss in iteration 133 : 0.5192323171245328
Loss in iteration 134 : 0.5384813471062309
Loss in iteration 135 : 0.5187378689897261
Loss in iteration 136 : 0.5355829048602017
Loss in iteration 137 : 0.5190588422530215
Loss in iteration 138 : 0.5376081182108456
Loss in iteration 139 : 0.5195781432724643
Loss in iteration 140 : 0.5395719235722723
Loss in iteration 141 : 0.5205064536977734
Loss in iteration 142 : 0.540108572359478
Loss in iteration 143 : 0.5209789811984877
Loss in iteration 144 : 0.5401514458646005
Loss in iteration 145 : 0.5198797485888333
Loss in iteration 146 : 0.5346311420808626
Loss in iteration 147 : 0.5165725260379448
Loss in iteration 148 : 0.5321663074827057
Loss in iteration 149 : 0.5135101806079475
Loss in iteration 150 : 0.5293525742848104
Loss in iteration 151 : 0.5133952616606066
Loss in iteration 152 : 0.5294121471657455
Loss in iteration 153 : 0.5142355509131022
Loss in iteration 154 : 0.5340782990201687
Loss in iteration 155 : 0.5193328715226976
Loss in iteration 156 : 0.5400492199698838
Loss in iteration 157 : 0.5251588952026304
Loss in iteration 158 : 0.5411182837976333
Loss in iteration 159 : 0.5240540830973651
Loss in iteration 160 : 0.5387475384131745
Loss in iteration 161 : 0.5202069362925996
Loss in iteration 162 : 0.539101413982988
Loss in iteration 163 : 0.5198391823816502
Loss in iteration 164 : 0.5384378635326665
Loss in iteration 165 : 0.5204899475087655
Loss in iteration 166 : 0.5387918927882315
Loss in iteration 167 : 0.5187165212376823
Loss in iteration 168 : 0.5368488195164965
Loss in iteration 169 : 0.5174821956805096
Loss in iteration 170 : 0.5358694570791629
Loss in iteration 171 : 0.5154347919008576
Loss in iteration 172 : 0.5311504190833126
Loss in iteration 173 : 0.5132414135899043
Loss in iteration 174 : 0.5313812147388819
Loss in iteration 175 : 0.5129694723142204
Loss in iteration 176 : 0.5316170459928531
Loss in iteration 177 : 0.5154654209541656
Loss in iteration 178 : 0.5395749466745235
Loss in iteration 179 : 0.523058546908332
Loss in iteration 180 : 0.5392464889509067
Loss in iteration 181 : 0.5205524330858781
Loss in iteration 182 : 0.5386993498359574
Loss in iteration 183 : 0.5190198769142353
Loss in iteration 184 : 0.5392011492335627
Loss in iteration 185 : 0.5195306155726986
Loss in iteration 186 : 0.5386793612479631
Loss in iteration 187 : 0.5184984731990719
Loss in iteration 188 : 0.5397014243399144
Loss in iteration 189 : 0.5199757952498584
Loss in iteration 190 : 0.5382173012866984
Loss in iteration 191 : 0.5180561929004193
Loss in iteration 192 : 0.5374324769377634
Loss in iteration 193 : 0.5143630358868122
Loss in iteration 194 : 0.5306278057388373
Loss in iteration 195 : 0.5129849861811597
Loss in iteration 196 : 0.5311465538148284
Loss in iteration 197 : 0.5131367428123951
Loss in iteration 198 : 0.5328955193291582
Loss in iteration 199 : 0.5188911901229617
Loss in iteration 200 : 0.539309464144751
Testing accuracy  of updater 0 on alg 1 with rate 0.00112 = 0.7895, training accuracy 0.7983166073162836, time elapsed: 4096 millisecond.
Loss in iteration 1 : 1.000093037487706
Loss in iteration 2 : 1.1227939386416752
Loss in iteration 3 : 0.8163241470567377
Loss in iteration 4 : 0.5225370733465501
Loss in iteration 5 : 0.4487278508692215
Loss in iteration 6 : 0.43438645058029857
Loss in iteration 7 : 0.42330181234244146
Loss in iteration 8 : 0.4157360694975528
Loss in iteration 9 : 0.4100183848443889
Loss in iteration 10 : 0.4054544250269632
Loss in iteration 11 : 0.40226955759311683
Loss in iteration 12 : 0.39961935475906335
Loss in iteration 13 : 0.39742370723437387
Loss in iteration 14 : 0.3954103344683335
Testing accuracy  of updater 0 on alg 1 with rate 2.8E-4 = 0.77925, training accuracy 0.8332793784396245, time elapsed: 357 millisecond.
Loss in iteration 1 : 1.0000455883689758
Loss in iteration 2 : 0.8917058604796
Loss in iteration 3 : 0.6775589790129721
Loss in iteration 4 : 0.49984551173848446
Loss in iteration 5 : 0.48047556465032726
Loss in iteration 6 : 0.4693417434400873
Loss in iteration 7 : 0.4607069848979287
Loss in iteration 8 : 0.4523718665935209
Loss in iteration 9 : 0.4441378243597688
Loss in iteration 10 : 0.43627870522118056
Loss in iteration 11 : 0.42932252600605875
Loss in iteration 12 : 0.4231513654219325
Loss in iteration 13 : 0.41776089340350864
Testing accuracy  of updater 0 on alg 1 with rate 1.96E-4 = 0.77575, training accuracy 0.8336031078018776, time elapsed: 267 millisecond.
Loss in iteration 1 : 1.000014885998033
Loss in iteration 2 : 0.66137535061153
Loss in iteration 3 : 0.5528462917524295
Loss in iteration 4 : 0.5233627123468951
Testing accuracy  of updater 0 on alg 1 with rate 1.1199999999999998E-4 = 0.50225, training accuracy 0.6484299125930721, time elapsed: 100 millisecond.
Loss in iteration 1 : 1.0000009303748771
Loss in iteration 2 : 0.6814309572183064
Loss in iteration 3 : 0.5644245426231445
Testing accuracy  of updater 0 on alg 1 with rate 2.7999999999999976E-5 = 0.5, training accuracy 0.6474587245063127, time elapsed: 144 millisecond.
Loss in iteration 1 : 1.0001453710745407
Loss in iteration 2 : 1.315694123094059
Loss in iteration 3 : 1.7993241037347505
Loss in iteration 4 : 1.8515126265648398
Loss in iteration 5 : 1.5154829465140118
Loss in iteration 6 : 0.8538500828476684
Loss in iteration 7 : 0.5249130519040287
Loss in iteration 8 : 0.8797201987701664
Loss in iteration 9 : 1.2189771528920759
Loss in iteration 10 : 1.1429154655690652
Loss in iteration 11 : 0.8568569105157776
Loss in iteration 12 : 0.6893337868826171
Loss in iteration 13 : 0.730121562493608
Loss in iteration 14 : 0.8713182222585704
Loss in iteration 15 : 0.9821129716686959
Loss in iteration 16 : 1.0012184068649839
Loss in iteration 17 : 0.9361112951731259
Loss in iteration 18 : 0.8371983792698215
Loss in iteration 19 : 0.7673852993525478
Loss in iteration 20 : 0.7611746128161962
Loss in iteration 21 : 0.8087259238372713
Loss in iteration 22 : 0.8607725585653934
Loss in iteration 23 : 0.870008026548909
Loss in iteration 24 : 0.827554119167769
Loss in iteration 25 : 0.7643575461723471
Loss in iteration 26 : 0.7211124421193674
Loss in iteration 27 : 0.7138375698425129
Loss in iteration 28 : 0.7299178722042602
Loss in iteration 29 : 0.7420700551457038
Loss in iteration 30 : 0.7294295965493667
Loss in iteration 31 : 0.6925679916286405
Loss in iteration 32 : 0.6504094281981738
Loss in iteration 33 : 0.6236032116321284
Loss in iteration 34 : 0.6197447140191229
Loss in iteration 35 : 0.6243567389258521
Loss in iteration 36 : 0.6132686183798765
Loss in iteration 37 : 0.5807937420592396
Loss in iteration 38 : 0.5455031420838548
Loss in iteration 39 : 0.5308964327270183
Loss in iteration 40 : 0.5298358891865219
Loss in iteration 41 : 0.522846151369686
Loss in iteration 42 : 0.4984145190210536
Loss in iteration 43 : 0.46794179202182384
Loss in iteration 44 : 0.4562462128424257
Loss in iteration 45 : 0.4573425096712227
Loss in iteration 46 : 0.4433000676049157
Loss in iteration 47 : 0.4177098324253321
Loss in iteration 48 : 0.4123485641909581
Loss in iteration 49 : 0.41559060946550275
Loss in iteration 50 : 0.39823509232956533
Loss in iteration 51 : 0.3929818275341255
Loss in iteration 52 : 0.40432309186302345
Loss in iteration 53 : 0.38961956283732746
Loss in iteration 54 : 0.4060044045408208
Loss in iteration 55 : 0.39939185964061996
Loss in iteration 56 : 0.40996905788568205
Loss in iteration 57 : 0.40130287151722005
Loss in iteration 58 : 0.40745115381643865
Loss in iteration 59 : 0.39737774141155713
Loss in iteration 60 : 0.3980679987941639
Loss in iteration 61 : 0.3888744736297537
Loss in iteration 62 : 0.38761206794946057
Loss in iteration 63 : 0.38530097489565557
Loss in iteration 64 : 0.381452972485007
Loss in iteration 65 : 0.3842820923867895
Loss in iteration 66 : 0.381460915094172
Loss in iteration 67 : 0.382386237373725
Loss in iteration 68 : 0.3841215044915163
Loss in iteration 69 : 0.3818684391524614
Loss in iteration 70 : 0.38304946203976525
Loss in iteration 71 : 0.3835962985148082
Loss in iteration 72 : 0.3815620328436999
Loss in iteration 73 : 0.38230358184453755
Loss in iteration 74 : 0.38193759390757454
Loss in iteration 75 : 0.37986517948982335
Loss in iteration 76 : 0.3802110526770134
Loss in iteration 77 : 0.37907149986076216
Loss in iteration 78 : 0.3776650105495471
Loss in iteration 79 : 0.3777432125904129
Loss in iteration 80 : 0.37642174877222795
Loss in iteration 81 : 0.3764473000666242
Loss in iteration 82 : 0.37615570397148507
Loss in iteration 83 : 0.3757148084735901
Loss in iteration 84 : 0.37640629069588105
Loss in iteration 85 : 0.3757097336950436
Testing accuracy  of updater 1 on alg 1 with rate 3.4999999999999994E-4 = 0.78775, training accuracy 0.8378115895111686, time elapsed: 1746 millisecond.
Loss in iteration 1 : 1.000071231826525
Loss in iteration 2 : 1.0266621493919723
Loss in iteration 3 : 1.36516679553628
Loss in iteration 4 : 1.4016801044703988
Loss in iteration 5 : 1.166439971159402
Loss in iteration 6 : 0.697743878935981
Loss in iteration 7 : 0.4491108896995252
Loss in iteration 8 : 0.717159673682335
Loss in iteration 9 : 0.9467637962265113
Loss in iteration 10 : 0.8664561720273822
Loss in iteration 11 : 0.6563304747915313
Loss in iteration 12 : 0.5501707392706621
Loss in iteration 13 : 0.5953907294242637
Loss in iteration 14 : 0.7016805239192355
Loss in iteration 15 : 0.7721495283774868
Loss in iteration 16 : 0.7705619567920119
Loss in iteration 17 : 0.7118450273254576
Loss in iteration 18 : 0.6419718523146969
Loss in iteration 19 : 0.6065256962543422
Loss in iteration 20 : 0.6178435311928959
Loss in iteration 21 : 0.6581014603615494
Loss in iteration 22 : 0.68698911284284
Loss in iteration 23 : 0.6812581794105931
Loss in iteration 24 : 0.6456618390973546
Loss in iteration 25 : 0.605372786688251
Loss in iteration 26 : 0.5840898127351809
Loss in iteration 27 : 0.5869556680520568
Loss in iteration 28 : 0.6001451249866807
Loss in iteration 29 : 0.6050736454373915
Loss in iteration 30 : 0.5920729941493256
Loss in iteration 31 : 0.5655012652815692
Loss in iteration 32 : 0.5388037633470765
Loss in iteration 33 : 0.5241734861568658
Loss in iteration 34 : 0.5258061446778788
Loss in iteration 35 : 0.5291798409392898
Loss in iteration 36 : 0.51953194936138
Loss in iteration 37 : 0.49710390022880957
Loss in iteration 38 : 0.4743808106582437
Loss in iteration 39 : 0.4662867235125243
Loss in iteration 40 : 0.46624868848287254
Loss in iteration 41 : 0.461924479853994
Loss in iteration 42 : 0.44619547363481615
Loss in iteration 43 : 0.427042927017296
Loss in iteration 44 : 0.4194597999828925
Loss in iteration 45 : 0.42053531713905334
Loss in iteration 46 : 0.415158272660812
Loss in iteration 47 : 0.3999984314883802
Loss in iteration 48 : 0.39180468464640195
Loss in iteration 49 : 0.39429294389532293
Loss in iteration 50 : 0.3917342164077169
Loss in iteration 51 : 0.3822871025619935
Loss in iteration 52 : 0.38376536718601945
Loss in iteration 53 : 0.38934949185190226
Loss in iteration 54 : 0.38264413407539266
Loss in iteration 55 : 0.38641182656205286
Loss in iteration 56 : 0.39160756171400335
Loss in iteration 57 : 0.38569362072708846
Loss in iteration 58 : 0.39269907366186185
Testing accuracy  of updater 1 on alg 1 with rate 2.45E-4 = 0.79275, training accuracy 0.8452573648429913, time elapsed: 1169 millisecond.
Loss in iteration 1 : 1.0000232593719265
Loss in iteration 2 : 0.7377870323044141
Loss in iteration 3 : 0.9315308041066219
Loss in iteration 4 : 0.952759524316992
Loss in iteration 5 : 0.8187293989412615
Loss in iteration 6 : 0.5531919156144441
Loss in iteration 7 : 0.3955398694282754
Loss in iteration 8 : 0.5610504340059129
Loss in iteration 9 : 0.6718790249272591
Loss in iteration 10 : 0.6044941663341915
Loss in iteration 11 : 0.4800537973573124
Loss in iteration 12 : 0.4300195139509463
Loss in iteration 13 : 0.469590150508463
Loss in iteration 14 : 0.5333599084221117
Loss in iteration 15 : 0.5632843570214133
Loss in iteration 16 : 0.5461942706355711
Loss in iteration 17 : 0.5038545841448084
Loss in iteration 18 : 0.47034582756304205
Loss in iteration 19 : 0.4659072270077135
Loss in iteration 20 : 0.4863048991272726
Loss in iteration 21 : 0.5094170334298223
Loss in iteration 22 : 0.5174444256397557
Loss in iteration 23 : 0.5054152935424915
Loss in iteration 24 : 0.48320373602271094
Loss in iteration 25 : 0.46572292228564577
Loss in iteration 26 : 0.46216426581654374
Loss in iteration 27 : 0.468868777934279
Loss in iteration 28 : 0.47638431944989623
Loss in iteration 29 : 0.4766309467405696
Loss in iteration 30 : 0.46680788751059105
Loss in iteration 31 : 0.4525118054829628
Loss in iteration 32 : 0.4413221789319523
Loss in iteration 33 : 0.43817784066280796
Loss in iteration 34 : 0.44025643452906255
Loss in iteration 35 : 0.44153416755531927
Loss in iteration 36 : 0.43683730817506405
Loss in iteration 37 : 0.4268341808104422
Loss in iteration 38 : 0.41649962540966806
Loss in iteration 39 : 0.4108099596609089
Loss in iteration 40 : 0.4112447177729649
Loss in iteration 41 : 0.4113392091900138
Loss in iteration 42 : 0.406317730682813
Loss in iteration 43 : 0.397484496923394
Loss in iteration 44 : 0.3918055506962644
Loss in iteration 45 : 0.39112136759692295
Loss in iteration 46 : 0.3915865709687301
Loss in iteration 47 : 0.3885352185616498
Loss in iteration 48 : 0.38297856866427366
Loss in iteration 49 : 0.3800641885832917
Loss in iteration 50 : 0.38099870690738963
Loss in iteration 51 : 0.38143338466149
Loss in iteration 52 : 0.37911338862820076
Loss in iteration 53 : 0.3762744020893331
Loss in iteration 54 : 0.3768509267617793
Loss in iteration 55 : 0.3786123195151328
Loss in iteration 56 : 0.37789608387800677
Loss in iteration 57 : 0.37640133286370236
Loss in iteration 58 : 0.37722766224506166
Testing accuracy  of updater 1 on alg 1 with rate 1.4E-4 = 0.78125, training accuracy 0.8387827775979282, time elapsed: 1179 millisecond.
Loss in iteration 1 : 1.0000014537107453
Loss in iteration 2 : 0.6267573706131185
Loss in iteration 3 : 0.5953205374977644
Loss in iteration 4 : 0.6861346954010957
Loss in iteration 5 : 0.7329312672184009
Loss in iteration 6 : 0.7371116623844582
Loss in iteration 7 : 0.7028385885239985
Loss in iteration 8 : 0.6346971965754539
Loss in iteration 9 : 0.5420225094948727
Loss in iteration 10 : 0.45601714757377493
Loss in iteration 11 : 0.41809115061744967
Loss in iteration 12 : 0.4542879915851389
Loss in iteration 13 : 0.5048464845929899
Loss in iteration 14 : 0.5164847490928343
Loss in iteration 15 : 0.4856497389107294
Loss in iteration 16 : 0.4370393232995394
Loss in iteration 17 : 0.3996511102932486
Loss in iteration 18 : 0.38851083329084196
Loss in iteration 19 : 0.39859612550490986
Loss in iteration 20 : 0.4162240503447872
Loss in iteration 21 : 0.42773146713199306
Loss in iteration 22 : 0.427987810722676
Loss in iteration 23 : 0.41868991999495536
Loss in iteration 24 : 0.40543791167785626
Loss in iteration 25 : 0.3940894875697156
Loss in iteration 26 : 0.3883193130112748
Loss in iteration 27 : 0.38916481937360425
Loss in iteration 28 : 0.39412960917853357
Testing accuracy  of updater 1 on alg 1 with rate 3.5E-5 = 0.78, training accuracy 0.8225963094852703, time elapsed: 542 millisecond.
Loss in iteration 1 : 1.0000007123182653
Loss in iteration 2 : 0.7148655299351968
Loss in iteration 3 : 0.5592189501343692
Loss in iteration 4 : 0.634731751027267
Loss in iteration 5 : 0.6924514044371387
Loss in iteration 6 : 0.7184115308705638
Loss in iteration 7 : 0.7152457186155332
Loss in iteration 8 : 0.6858988207906219
Loss in iteration 9 : 0.6335083155929647
Loss in iteration 10 : 0.5634776128092
Loss in iteration 11 : 0.49167860327282514
Loss in iteration 12 : 0.4404150831505172
Loss in iteration 13 : 0.4260351173115996
Loss in iteration 14 : 0.45279409845750535
Loss in iteration 15 : 0.48601559924482307
Loss in iteration 16 : 0.49700969152599334
Loss in iteration 17 : 0.4806736695924831
Loss in iteration 18 : 0.4479597368156497
Loss in iteration 19 : 0.4153531459123723
Loss in iteration 20 : 0.39556995520623944
Loss in iteration 21 : 0.3910127440562441
Loss in iteration 22 : 0.3983000554238324
Loss in iteration 23 : 0.4092954495945888
Loss in iteration 24 : 0.4165827382004747
Testing accuracy  of updater 1 on alg 1 with rate 2.45E-5 = 0.749, training accuracy 0.8164454516024603, time elapsed: 505 millisecond.
Loss in iteration 1 : 1.0000002325937192
Loss in iteration 2 : 0.8338635852058144
Loss in iteration 3 : 0.5944339794360496
Loss in iteration 4 : 0.562516752820128
Loss in iteration 5 : 0.613508939838203
Loss in iteration 6 : 0.6562283453340698
Loss in iteration 7 : 0.6803367536744745
Loss in iteration 8 : 0.6871192996083186
Loss in iteration 9 : 0.6781840619452965
Loss in iteration 10 : 0.6551763930857192
Loss in iteration 11 : 0.6198311474397201
Loss in iteration 12 : 0.575064331899367
Loss in iteration 13 : 0.5273700340397042
Loss in iteration 14 : 0.48647713216797833
Loss in iteration 15 : 0.4593338111859131
Loss in iteration 16 : 0.4490203916841818
Loss in iteration 17 : 0.451779917111421
Loss in iteration 18 : 0.46442125041181115
Loss in iteration 19 : 0.4770832505286847
Loss in iteration 20 : 0.4803345388032734
Testing accuracy  of updater 1 on alg 1 with rate 1.3999999999999998E-5 = 0.7845, training accuracy 0.7944318549692457, time elapsed: 396 millisecond.
Loss in iteration 1 : 1.0000000145371075
Loss in iteration 2 : 0.9584655295311177
Testing accuracy  of updater 1 on alg 1 with rate 3.499999999999997E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 57 millisecond.
Loss in iteration 1 : 1.0005814842981626
Loss in iteration 2 : 4.0126039838287095
Loss in iteration 3 : 4.117495298872633
Loss in iteration 4 : 3.4456066744625216
Loss in iteration 5 : 2.07936494241425
Loss in iteration 6 : 0.8867405203175951
Loss in iteration 7 : 1.0112991484110088
Loss in iteration 8 : 1.293277455389644
Loss in iteration 9 : 1.2886897482111785
Loss in iteration 10 : 1.173614621568683
Loss in iteration 11 : 1.0956838402958895
Loss in iteration 12 : 1.0925387236538597
Loss in iteration 13 : 1.115344420223533
Loss in iteration 14 : 1.137757438574756
Loss in iteration 15 : 1.141627771715732
Loss in iteration 16 : 1.130253733535255
Loss in iteration 17 : 1.1152697335884036
Loss in iteration 18 : 1.098936672556803
Loss in iteration 19 : 1.0819854190395153
Loss in iteration 20 : 1.0634372486994685
Loss in iteration 21 : 1.0416135215091327
Loss in iteration 22 : 1.0156261142112522
Loss in iteration 23 : 0.9868613095813707
Loss in iteration 24 : 0.9567730792122213
Loss in iteration 25 : 0.9254614980609064
Loss in iteration 26 : 0.8930334072705691
Loss in iteration 27 : 0.859396190638688
Loss in iteration 28 : 0.8247289806578704
Loss in iteration 29 : 0.7893540852930024
Loss in iteration 30 : 0.7533224570724009
Loss in iteration 31 : 0.7170283060608312
Loss in iteration 32 : 0.6807551471212266
Loss in iteration 33 : 0.6446787668864373
Loss in iteration 34 : 0.6095744448406338
Loss in iteration 35 : 0.5756044972398688
Loss in iteration 36 : 0.5429974211219818
Loss in iteration 37 : 0.511893623130755
Loss in iteration 38 : 0.48352854122541766
Loss in iteration 39 : 0.4586725450451684
Loss in iteration 40 : 0.43769672814388844
Loss in iteration 41 : 0.42323974007315596
Loss in iteration 42 : 0.41779113044568156
Loss in iteration 43 : 0.42759509793063916
Loss in iteration 44 : 0.598825507952918
Loss in iteration 45 : 1.7885194313906032
Loss in iteration 46 : 1.042473439801286
Loss in iteration 47 : 0.8032646251493848
Loss in iteration 48 : 0.9640888817485671
Loss in iteration 49 : 0.471583433804283
Loss in iteration 50 : 0.4625141236131225
Loss in iteration 51 : 0.46806375803563705
Loss in iteration 52 : 0.47578135474871613
Loss in iteration 53 : 0.4799523254263278
Loss in iteration 54 : 0.48072485024934125
Loss in iteration 55 : 0.47841726333594814
Loss in iteration 56 : 0.47326851758389143
Loss in iteration 57 : 0.4656848449149758
Loss in iteration 58 : 0.4561806942218679
Loss in iteration 59 : 0.44513227653214854
Loss in iteration 60 : 0.4329230044559195
Loss in iteration 61 : 0.4203764821953601
Loss in iteration 62 : 0.40817202210619186
Loss in iteration 63 : 0.397075066480148
Loss in iteration 64 : 0.38827622326507666
Loss in iteration 65 : 0.38301347411589015
Loss in iteration 66 : 0.3860976516574495
Loss in iteration 67 : 0.46969409043543964
Loss in iteration 68 : 1.0907360101972132
Loss in iteration 69 : 0.44037719530489855
Loss in iteration 70 : 0.7227955788555097
Loss in iteration 71 : 1.1190457904766329
Loss in iteration 72 : 1.7228357419743678
Loss in iteration 73 : 1.0225278279110364
Loss in iteration 74 : 0.528607950890152
Loss in iteration 75 : 0.5749458979252483
Loss in iteration 76 : 0.543776900411777
Loss in iteration 77 : 0.552704987468288
Loss in iteration 78 : 0.5621357410964056
Loss in iteration 79 : 0.5657551319631953
Loss in iteration 80 : 0.5646807809847855
Testing accuracy  of updater 2 on alg 1 with rate 6.999999999999999E-4 = 0.78, training accuracy 0.8329556490773713, time elapsed: 1582 millisecond.
Loss in iteration 1 : 1.0002849273060996
Loss in iteration 2 : 2.9142407421187793
Loss in iteration 3 : 2.987519414175152
Loss in iteration 4 : 2.517122897761972
Loss in iteration 5 : 1.5593579441294025
Loss in iteration 6 : 0.6827918829929557
Loss in iteration 7 : 0.7834793508698423
Loss in iteration 8 : 0.9855905929141366
Loss in iteration 9 : 0.9663619612767096
Loss in iteration 10 : 0.8789530741298052
Loss in iteration 11 : 0.8256465128819568
Loss in iteration 12 : 0.826653609688158
Loss in iteration 13 : 0.8451997022405092
Loss in iteration 14 : 0.8596288723591831
Loss in iteration 15 : 0.8631569560588425
Loss in iteration 16 : 0.8563336536958682
Loss in iteration 17 : 0.8452571604937715
Loss in iteration 18 : 0.8338747338214798
Loss in iteration 19 : 0.8224649136972849
Loss in iteration 20 : 0.8106532480557623
Loss in iteration 21 : 0.7959986374841189
Loss in iteration 22 : 0.7786226329267104
Loss in iteration 23 : 0.7590728087061103
Loss in iteration 24 : 0.738597068528395
Loss in iteration 25 : 0.7177177387390832
Loss in iteration 26 : 0.6960930232676319
Loss in iteration 27 : 0.6735882921840408
Loss in iteration 28 : 0.6503892483413913
Loss in iteration 29 : 0.626479926701602
Loss in iteration 30 : 0.6024395364652135
Loss in iteration 31 : 0.5782512826391069
Loss in iteration 32 : 0.5543620246047504
Loss in iteration 33 : 0.5310932751103562
Loss in iteration 34 : 0.5085748926017217
Loss in iteration 35 : 0.48689781192459936
Loss in iteration 36 : 0.46622612437460786
Loss in iteration 37 : 0.4469898993310439
Loss in iteration 38 : 0.4300752216274913
Loss in iteration 39 : 0.41555695161585354
Loss in iteration 40 : 0.4038630922703444
Loss in iteration 41 : 0.39689694477084925
Loss in iteration 42 : 0.3952880108489836
Loss in iteration 43 : 0.40003172557257716
Loss in iteration 44 : 0.42462750010972
Loss in iteration 45 : 0.6213363597992756
Loss in iteration 46 : 1.3288069344482292
Loss in iteration 47 : 0.8195802144941575
Loss in iteration 48 : 0.5526365464918818
Loss in iteration 49 : 0.43172179090364476
Loss in iteration 50 : 0.4343588552527342
Loss in iteration 51 : 0.4112534022477201
Loss in iteration 52 : 0.4124636896742646
Loss in iteration 53 : 0.41462587651906324
Loss in iteration 54 : 0.41537649580216135
Loss in iteration 55 : 0.414693567567307
Loss in iteration 56 : 0.4127270239895396
Loss in iteration 57 : 0.409704692562799
Loss in iteration 58 : 0.40587236881685324
Loss in iteration 59 : 0.4014089820352431
Loss in iteration 60 : 0.39654731038769814
Loss in iteration 61 : 0.3917416592415889
Loss in iteration 62 : 0.3872842504757254
Loss in iteration 63 : 0.38354226937564984
Loss in iteration 64 : 0.38050848157004363
Loss in iteration 65 : 0.37861533188701146
Loss in iteration 66 : 0.37895953887572603
Loss in iteration 67 : 0.3849251621939727
Loss in iteration 68 : 0.4229398335589563
Loss in iteration 69 : 0.6211947788088451
Loss in iteration 70 : 1.019606345665973
Loss in iteration 71 : 0.44601049651170943
Loss in iteration 72 : 0.6445320862942557
Loss in iteration 73 : 0.535283001467596
Loss in iteration 74 : 0.4286476108931572
Loss in iteration 75 : 0.4081642748703591
Loss in iteration 76 : 0.4119228412925104
Loss in iteration 77 : 0.4139920372803054
Loss in iteration 78 : 0.41405732143151713
Testing accuracy  of updater 2 on alg 1 with rate 4.9E-4 = 0.782, training accuracy 0.8332793784396245, time elapsed: 1814 millisecond.
Loss in iteration 1 : 1.000093037487706
Loss in iteration 2 : 1.8161727058912913
Loss in iteration 3 : 1.8579631881779424
Loss in iteration 4 : 1.58912255914282
Loss in iteration 5 : 1.0412376287012957
Loss in iteration 6 : 0.49279558174870897
Loss in iteration 7 : 0.5646544893358975
Loss in iteration 8 : 0.6650861617416978
Loss in iteration 9 : 0.6435012639438723
Loss in iteration 10 : 0.5914997969578374
Loss in iteration 11 : 0.5664689704385042
Loss in iteration 12 : 0.5711153393907626
Loss in iteration 13 : 0.5829968549842501
Loss in iteration 14 : 0.5910919257149627
Loss in iteration 15 : 0.591828317133693
Loss in iteration 16 : 0.5880188733960195
Loss in iteration 17 : 0.5828842887301595
Loss in iteration 18 : 0.5779987371670545
Loss in iteration 19 : 0.5730770515721677
Loss in iteration 20 : 0.5672346608256611
Loss in iteration 21 : 0.5597247079777056
Loss in iteration 22 : 0.5506838460395767
Loss in iteration 23 : 0.5410384179493332
Loss in iteration 24 : 0.5311290835312282
Loss in iteration 25 : 0.5207775064986708
Loss in iteration 26 : 0.510017185521531
Loss in iteration 27 : 0.49893026629674847
Loss in iteration 28 : 0.48770522168097935
Loss in iteration 29 : 0.47648135214791654
Loss in iteration 30 : 0.4652245231649184
Loss in iteration 31 : 0.4540175716589017
Loss in iteration 32 : 0.442900171938045
Loss in iteration 33 : 0.43226530406824026
Loss in iteration 34 : 0.4223411938585126
Loss in iteration 35 : 0.4131889108935497
Loss in iteration 36 : 0.40475345255251094
Loss in iteration 37 : 0.3972441063341163
Loss in iteration 38 : 0.39093131868954495
Loss in iteration 39 : 0.38601370118697687
Loss in iteration 40 : 0.382469332972379
Loss in iteration 41 : 0.38016633579717213
Loss in iteration 42 : 0.37959294369061
Loss in iteration 43 : 0.3801688237777153
Loss in iteration 44 : 0.38170712249775457
Loss in iteration 45 : 0.3837102218309951
Loss in iteration 46 : 0.38557028959435213
Loss in iteration 47 : 0.3866456321114206
Testing accuracy  of updater 2 on alg 1 with rate 2.8E-4 = 0.789, training accuracy 0.8429912593072192, time elapsed: 999 millisecond.
Loss in iteration 1 : 1.0000058148429816
Loss in iteration 2 : 0.7185596606230653
Loss in iteration 3 : 0.7294491096930845
Loss in iteration 4 : 0.6633442589834635
Loss in iteration 5 : 0.5371398282268242
Loss in iteration 6 : 0.43085667660110843
Loss in iteration 7 : 0.42514940580299665
Loss in iteration 8 : 0.4275938371539605
Loss in iteration 9 : 0.4090865513892489
Loss in iteration 10 : 0.3940570062814308
Loss in iteration 11 : 0.3877884724265268
Loss in iteration 12 : 0.3874885451181502
Loss in iteration 13 : 0.38820195627578424
Loss in iteration 14 : 0.38862887489824155
Loss in iteration 15 : 0.3885811109537643
Loss in iteration 16 : 0.38847837137516306
Loss in iteration 17 : 0.38871408724645906
Loss in iteration 18 : 0.3892311675774005
Loss in iteration 19 : 0.3896809989528369
Loss in iteration 20 : 0.3899643791550152
Testing accuracy  of updater 2 on alg 1 with rate 7.0E-5 = 0.77875, training accuracy 0.8329556490773713, time elapsed: 410 millisecond.
Loss in iteration 1 : 1.000002849273061
Loss in iteration 2 : 0.6109546650540927
Loss in iteration 3 : 0.6240297333409555
Loss in iteration 4 : 0.5844617624266324
Loss in iteration 5 : 0.5181778348446814
Loss in iteration 6 : 0.4699883202798553
Loss in iteration 7 : 0.4493064163437121
Loss in iteration 8 : 0.4376328151935491
Loss in iteration 9 : 0.42692417250014386
Loss in iteration 10 : 0.4127134312963394
Loss in iteration 11 : 0.400981660673998
Loss in iteration 12 : 0.39384942371422244
Loss in iteration 13 : 0.3904223116369542
Loss in iteration 14 : 0.3887282964211289
Loss in iteration 15 : 0.3875781334067801
Loss in iteration 16 : 0.3866910995144499
Loss in iteration 17 : 0.3859303659705572
Loss in iteration 18 : 0.3854095306820263
Loss in iteration 19 : 0.3851714111195391
Loss in iteration 20 : 0.38507689003842094
Loss in iteration 21 : 0.38502196658653215
Testing accuracy  of updater 2 on alg 1 with rate 4.9E-5 = 0.7795, training accuracy 0.8329556490773713, time elapsed: 425 millisecond.
Loss in iteration 1 : 1.0000009303748771
Loss in iteration 2 : 0.5574532090785334
Loss in iteration 3 : 0.5782067974887509
Loss in iteration 4 : 0.5887472865073677
Loss in iteration 5 : 0.5735351323164241
Loss in iteration 6 : 0.5407225896876232
Loss in iteration 7 : 0.508262066945632
Loss in iteration 8 : 0.487891853690191
Loss in iteration 9 : 0.4766710753536235
Loss in iteration 10 : 0.4674070977203338
Loss in iteration 11 : 0.4570474438791963
Loss in iteration 12 : 0.4457473983434418
Loss in iteration 13 : 0.43425903007074157
Loss in iteration 14 : 0.4237405832037107
Loss in iteration 15 : 0.41483974453900985
Loss in iteration 16 : 0.4075843482454374
Loss in iteration 17 : 0.40188289103910546
Loss in iteration 18 : 0.39769765032741994
Loss in iteration 19 : 0.39462604643886146
Loss in iteration 20 : 0.3921466070010947
Loss in iteration 21 : 0.3901645804958102
Loss in iteration 22 : 0.3886819766506956
Loss in iteration 23 : 0.38754293980448823
Loss in iteration 24 : 0.38669156208059713
Testing accuracy  of updater 2 on alg 1 with rate 2.7999999999999996E-5 = 0.77875, training accuracy 0.8319844609906119, time elapsed: 461 millisecond.
Loss in iteration 1 : 1.0000000581484298
Loss in iteration 2 : 0.8421690366694147
Loss in iteration 3 : 0.6459650545677914
Loss in iteration 4 : 0.5599208940392779
Loss in iteration 5 : 0.5577796105914348
Loss in iteration 6 : 0.5734588183331588
Loss in iteration 7 : 0.5869610888356572
Testing accuracy  of updater 2 on alg 1 with rate 6.999999999999994E-6 = 0.5, training accuracy 0.6474587245063127, time elapsed: 148 millisecond.
Loss in iteration 1 : 2.536637061158183
Loss in iteration 2 : 90.81036172480268
Loss in iteration 3 : 21.522153940442294
Loss in iteration 4 : 24.37175503389434
Loss in iteration 5 : 49.800419714468255
Loss in iteration 6 : 9.357131660519366
Loss in iteration 7 : 24.881590443130253
Loss in iteration 8 : 45.143177056306804
Loss in iteration 9 : 11.659736652885746
Loss in iteration 10 : 8.633384488442097
Loss in iteration 11 : 24.096233380265687
Loss in iteration 12 : 1.3480766728971658
Loss in iteration 13 : 7.225754731368823
Loss in iteration 14 : 36.60444055053442
Loss in iteration 15 : 9.606022549897892
Loss in iteration 16 : 9.382642169739038
Loss in iteration 17 : 28.680924020204337
Loss in iteration 18 : 5.96343306138802
Loss in iteration 19 : 14.322348720343182
Loss in iteration 20 : 32.01090242228143
Loss in iteration 21 : 9.15221530139544
Loss in iteration 22 : 4.117249575857569
Loss in iteration 23 : 13.723357786283016
Loss in iteration 24 : 1.1401058384332758
Loss in iteration 25 : 6.746351461039408
Loss in iteration 26 : 16.668819543591574
Loss in iteration 27 : 30.62965845759243
Loss in iteration 28 : 9.56484465203785
Loss in iteration 29 : 2.3914908169366087
Loss in iteration 30 : 9.176390116537945
Loss in iteration 31 : 4.792304292892765
Loss in iteration 32 : 21.19896855073395
Loss in iteration 33 : 4.400948209189951
Loss in iteration 34 : 12.534078145618064
Loss in iteration 35 : 26.314744911384203
Loss in iteration 36 : 8.05138679685878
Loss in iteration 37 : 2.3170911514793846
Loss in iteration 38 : 8.208479775181392
Loss in iteration 39 : 3.918351130468852
Loss in iteration 40 : 17.58559765978488
Loss in iteration 41 : 3.1184955421347174
Loss in iteration 42 : 14.038904504026155
Loss in iteration 43 : 23.725664672328726
Loss in iteration 44 : 7.245945022290269
Loss in iteration 45 : 2.1841332268230973
Loss in iteration 46 : 7.689432081433393
Loss in iteration 47 : 3.21495247410387
Loss in iteration 48 : 15.053118130663911
Loss in iteration 49 : 2.2420706882289543
Loss in iteration 50 : 14.960215393635417
Loss in iteration 51 : 21.71359955627539
Loss in iteration 52 : 6.595127646297109
Loss in iteration 53 : 2.144287854475504
Loss in iteration 54 : 7.440522258507461
Loss in iteration 55 : 2.507290607153569
Loss in iteration 56 : 12.259515505938793
Loss in iteration 57 : 1.0837244057321995
Loss in iteration 58 : 13.449386936035273
Loss in iteration 59 : 20.751262394318115
Loss in iteration 60 : 6.441741170242316
Loss in iteration 61 : 1.8876125690781926
Loss in iteration 62 : 6.88569084396667
Loss in iteration 63 : 2.6238301835094293
Loss in iteration 64 : 12.666505700590319
Loss in iteration 65 : 1.6983690963703564
Loss in iteration 66 : 13.90971668969956
Loss in iteration 67 : 19.138028812873287
Loss in iteration 68 : 5.835065020816914
Loss in iteration 69 : 1.9494446951879685
Loss in iteration 70 : 6.74816783337832
Loss in iteration 71 : 2.066532102128564
Loss in iteration 72 : 10.391677224007156
Loss in iteration 73 : 0.7461957633870602
Loss in iteration 74 : 9.867943924381757
Loss in iteration 75 : 19.06688907695152
Loss in iteration 76 : 6.085306549138044
Loss in iteration 77 : 1.518766336128389
Loss in iteration 78 : 5.710065714492453
Loss in iteration 79 : 3.3984168360159317
Loss in iteration 80 : 14.623179360920908
Loss in iteration 81 : 3.5330687942741545
Loss in iteration 82 : 6.174396097699365
Loss in iteration 83 : 16.552390055056872
Loss in iteration 84 : 4.840257548034314
Loss in iteration 85 : 2.4096407137514193
Loss in iteration 86 : 8.113052881245617
Loss in iteration 87 : 0.5076014947002716
Loss in iteration 88 : 0.9965243208472031
Loss in iteration 89 : 16.16088325648055
Loss in iteration 90 : 16.48292431155098
Loss in iteration 91 : 4.990608427209461
Loss in iteration 92 : 1.9603424782245522
Loss in iteration 93 : 6.842041737841461
Loss in iteration 94 : 0.9097375543415955
Loss in iteration 95 : 5.283487282847612
Loss in iteration 96 : 4.222243662003404
Loss in iteration 97 : 17.03203274576411
Loss in iteration 98 : 5.497668640649974
Loss in iteration 99 : 1.3668574121961008
Loss in iteration 100 : 5.273482085470257
Loss in iteration 101 : 2.7809934459206316
Loss in iteration 102 : 12.539342171684618
Loss in iteration 103 : 2.8673427616125235
Loss in iteration 104 : 6.6399266777231505
Loss in iteration 105 : 15.794134014318324
Loss in iteration 106 : 4.949348872480092
Loss in iteration 107 : 1.4778304625609504
Loss in iteration 108 : 5.155005525520123
Loss in iteration 109 : 2.343461760985503
Loss in iteration 110 : 10.628234126156974
Loss in iteration 111 : 1.889956605684799
Loss in iteration 112 : 9.297599723099077
Loss in iteration 113 : 15.122149966927223
Loss in iteration 114 : 4.715102426992822
Loss in iteration 115 : 1.485936695959304
Loss in iteration 116 : 5.119039809506366
Loss in iteration 117 : 2.0292711814966577
Loss in iteration 118 : 9.497572066440396
Loss in iteration 119 : 1.3650656550215297
Loss in iteration 120 : 10.469956327544766
Loss in iteration 121 : 14.485408669899929
Loss in iteration 122 : 4.480711934262534
Loss in iteration 123 : 1.536530897507207
Loss in iteration 124 : 5.211014905925817
Loss in iteration 125 : 1.6089722459982774
Loss in iteration 126 : 7.891657354285331
Loss in iteration 127 : 0.6022113728140758
Loss in iteration 128 : 7.550564993584574
Loss in iteration 129 : 14.758018505129755
Loss in iteration 130 : 4.774629693276519
Loss in iteration 131 : 1.2256908180843404
Loss in iteration 132 : 4.560168364248516
Loss in iteration 133 : 2.557277021767085
Loss in iteration 134 : 11.150701377972567
Loss in iteration 135 : 2.6493771530344237
Loss in iteration 136 : 5.473317532237575
Loss in iteration 137 : 13.697015590752155
Loss in iteration 138 : 4.264856316448475
Loss in iteration 139 : 1.461549638323246
Loss in iteration 140 : 4.982200801304162
Loss in iteration 141 : 1.4784751725555623
Loss in iteration 142 : 7.340924447581619
Loss in iteration 143 : 0.5366384474403624
Loss in iteration 144 : 6.245216418152119
Loss in iteration 145 : 14.118838892739143
Loss in iteration 146 : 4.62751636926194
Loss in iteration 147 : 1.107976761327887
Loss in iteration 148 : 4.140173635130036
Loss in iteration 149 : 2.877041717754075
Loss in iteration 150 : 11.658810120095728
Loss in iteration 151 : 3.194388984028515
Loss in iteration 152 : 3.2219872987528073
Loss in iteration 153 : 10.01848961341249
Loss in iteration 154 : 2.238943578746561
Loss in iteration 155 : 5.387001468127641
Loss in iteration 156 : 12.728735997937505
Loss in iteration 157 : 3.941472725359973
Loss in iteration 158 : 1.4481383245919315
Loss in iteration 159 : 4.864877963450925
Loss in iteration 160 : 1.1834864362088444
Loss in iteration 161 : 6.02437157877452
Loss in iteration 162 : 0.45164079574161625
Loss in iteration 163 : 1.8874687789600582
Loss in iteration 164 : 9.66887580532757
Loss in iteration 165 : 12.771675694678478
Loss in iteration 166 : 4.057538999803613
Loss in iteration 167 : 1.3191888372036615
Loss in iteration 168 : 4.678313422410119
Loss in iteration 169 : 1.3198027006486406
Loss in iteration 170 : 6.681835946156616
Loss in iteration 171 : 0.4857669573592409
Loss in iteration 172 : 5.232091982705973
Loss in iteration 173 : 13.109612460874484
Loss in iteration 174 : 4.344478950459339
Loss in iteration 175 : 1.0034591606045948
Loss in iteration 176 : 3.7629991546961126
Loss in iteration 177 : 2.8711529280241916
Loss in iteration 178 : 11.169663279929216
Loss in iteration 179 : 3.2154949148567726
Loss in iteration 180 : 2.4443457813195884
Loss in iteration 181 : 8.006847970787629
Loss in iteration 182 : 1.3210589684293483
Loss in iteration 183 : 7.530299887539999
Loss in iteration 184 : 11.934804127937584
Loss in iteration 185 : 3.7532796611690724
Loss in iteration 186 : 1.2653321309165864
Loss in iteration 187 : 4.209772486756914
Loss in iteration 188 : 1.5391678100236632
Loss in iteration 189 : 7.09349863485633
Loss in iteration 190 : 0.8478028335896786
Loss in iteration 191 : 8.818169961973458
Loss in iteration 192 : 11.658444066114427
Loss in iteration 193 : 3.659569734469855
Loss in iteration 194 : 1.3052151387098085
Loss in iteration 195 : 4.394292240947814
Loss in iteration 196 : 1.1780235512287875
Loss in iteration 197 : 5.825244900219585
Loss in iteration 198 : 0.39413088080898534
Loss in iteration 199 : 0.38026973947755965
Loss in iteration 200 : 0.40605792366894133
Testing accuracy  of updater 3 on alg 1 with rate 1.96 = 0.64075, training accuracy 0.5302686953706701, time elapsed: 3836 millisecond.
Loss in iteration 1 : 1.0104509077981318
Loss in iteration 2 : 6.990888478672557
Loss in iteration 3 : 2.7047470132313
Loss in iteration 4 : 0.5661975900520302
Loss in iteration 5 : 1.2198289536557705
Loss in iteration 6 : 3.7132524568558813
Loss in iteration 7 : 7.792171950490889
Loss in iteration 8 : 3.31676507875392
Loss in iteration 9 : 0.4866677002001362
Loss in iteration 10 : 0.4744461547107864
Loss in iteration 11 : 1.1705625624108684
Loss in iteration 12 : 4.575090784528185
Loss in iteration 13 : 7.875146452787309
Loss in iteration 14 : 3.4071641999845284
Loss in iteration 15 : 0.49835877051352195
Loss in iteration 16 : 0.58995954375063
Loss in iteration 17 : 1.8621622525625126
Loss in iteration 18 : 2.3343916175260957
Loss in iteration 19 : 6.977692261429347
Loss in iteration 20 : 2.7817093982905057
Loss in iteration 21 : 0.5047346369169329
Loss in iteration 22 : 1.001224677899456
Loss in iteration 23 : 4.371555640038603
Loss in iteration 24 : 7.616155319358887
Loss in iteration 25 : 3.2727069046572086
Loss in iteration 26 : 0.4901346627143696
Loss in iteration 27 : 0.5131927404737943
Loss in iteration 28 : 1.4376932019673387
Loss in iteration 29 : 3.433225982142167
Loss in iteration 30 : 7.694083895341498
Loss in iteration 31 : 3.356435816234144
Loss in iteration 32 : 0.4977088083268118
Loss in iteration 33 : 0.6647924740624377
Loss in iteration 34 : 2.2011225206462397
Loss in iteration 35 : 1.336981393583711
Loss in iteration 36 : 4.8733641790371784
Loss in iteration 37 : 1.3276460109480794
Loss in iteration 38 : 2.7706889192275375
Loss in iteration 39 : 6.536514616240294
Loss in iteration 40 : 2.5529343520295327
Loss in iteration 41 : 0.5400397418845275
Loss in iteration 42 : 1.113004133515002
Loss in iteration 43 : 3.593043507899828
Loss in iteration 44 : 7.270352868872917
Loss in iteration 45 : 3.109589790846827
Loss in iteration 46 : 0.470723243242052
Loss in iteration 47 : 0.461879914616259
Loss in iteration 48 : 1.1125562927147463
Loss in iteration 49 : 4.263223152006925
Loss in iteration 50 : 7.346665679432933
Loss in iteration 51 : 3.189178153715251
Loss in iteration 52 : 0.47888822038041634
Loss in iteration 53 : 0.5710004823150634
Loss in iteration 54 : 1.7741212819992502
Loss in iteration 55 : 2.160928911929082
Loss in iteration 56 : 6.481839965178684
Loss in iteration 57 : 2.5823572474534853
Loss in iteration 58 : 0.504597708318737
Loss in iteration 59 : 1.0680849808110477
Loss in iteration 60 : 3.7803234304079627
Loss in iteration 61 : 7.144985506553159
Loss in iteration 62 : 3.0859471312427025
Loss in iteration 63 : 0.47190441877656686
Loss in iteration 64 : 0.5073356782995484
Loss in iteration 65 : 1.4381482889043788
Loss in iteration 66 : 3.0277531474928177
Loss in iteration 67 : 7.1772522924449955
Loss in iteration 68 : 3.1313678503439317
Loss in iteration 69 : 0.4714925332196473
Loss in iteration 70 : 0.5924170042484693
Loss in iteration 71 : 1.9349557245216593
Loss in iteration 72 : 1.6169366267825966
Loss in iteration 73 : 5.469422570756541
Loss in iteration 74 : 1.9088849397302612
Loss in iteration 75 : 1.088167472710262
Loss in iteration 76 : 3.17728334168657
Loss in iteration 77 : 0.4017385052772386
Loss in iteration 78 : 0.7866569631472616
Loss in iteration 79 : 3.4612282417790636
Loss in iteration 80 : 0.4861434504514246
Loss in iteration 81 : 4.000108107174231
Loss in iteration 82 : 7.057505293728695
Loss in iteration 83 : 3.0909199303834165
Loss in iteration 84 : 0.4718267704642493
Loss in iteration 85 : 0.6284747546000437
Loss in iteration 86 : 2.0376625673323474
Loss in iteration 87 : 1.2851210996991673
Loss in iteration 88 : 4.596583521391995
Loss in iteration 89 : 1.3193129922766953
Loss in iteration 90 : 2.3430290872485595
Loss in iteration 91 : 5.729608491425421
Loss in iteration 92 : 2.1547989382984962
Loss in iteration 93 : 0.6586527369552637
Loss in iteration 94 : 1.7605548018390638
Loss in iteration 95 : 1.5251628450499852
Loss in iteration 96 : 4.629210479034418
Loss in iteration 97 : 1.3695083466094369
Loss in iteration 98 : 1.9807201655730453
Loss in iteration 99 : 4.9701366683729615
Loss in iteration 100 : 1.628007485079352
Loss in iteration 101 : 1.2632724564501105
Loss in iteration 102 : 3.2876443643734063
Loss in iteration 103 : 0.4804751065886246
Loss in iteration 104 : 1.994575236720818
Loss in iteration 105 : 5.813702275236216
Loss in iteration 106 : 2.25857809220035
Loss in iteration 107 : 0.5766379306208514
Loss in iteration 108 : 1.5066012065618484
Loss in iteration 109 : 2.116093360334013
Loss in iteration 110 : 5.761067045514495
Loss in iteration 111 : 2.2355461696244046
Loss in iteration 112 : 0.573115103246312
Loss in iteration 113 : 1.4597631471672945
Loss in iteration 114 : 2.1634220145292544
Loss in iteration 115 : 5.7430164056803905
Loss in iteration 116 : 2.237317194791396
Loss in iteration 117 : 0.5594732205002814
Loss in iteration 118 : 1.3888261173160448
Loss in iteration 119 : 2.3039737602940527
Loss in iteration 120 : 5.890818460294569
Loss in iteration 121 : 2.359583900226594
Loss in iteration 122 : 0.4867182699531444
Loss in iteration 123 : 0.9294676131604236
Loss in iteration 124 : 3.56767982124459
Loss in iteration 125 : 6.498964567696613
Loss in iteration 126 : 2.8175161033053064
Loss in iteration 127 : 0.4517723751138608
Loss in iteration 128 : 0.46847841640556903
Loss in iteration 129 : 1.2152291162061717
Loss in iteration 130 : 3.1624865106641207
Loss in iteration 131 : 6.597546999886339
Loss in iteration 132 : 2.9056937516115036
Loss in iteration 133 : 0.461975905153039
Loss in iteration 134 : 0.6445329388797092
Loss in iteration 135 : 2.0103159227755905
Loss in iteration 136 : 1.041997417791619
Loss in iteration 137 : 3.7143182062281497
Loss in iteration 138 : 0.8242297985255154
Loss in iteration 139 : 3.6902838598671632
Loss in iteration 140 : 6.344751765385808
Loss in iteration 141 : 2.7460755566268458
Loss in iteration 142 : 0.4463103183478643
Loss in iteration 143 : 0.45094732163879625
Loss in iteration 144 : 1.0802649406392828
Loss in iteration 145 : 3.4681637384145723
Loss in iteration 146 : 6.443401167705884
Loss in iteration 147 : 2.8333214292230005
Loss in iteration 148 : 0.45452734841265163
Loss in iteration 149 : 0.5892118138964142
Loss in iteration 150 : 1.8215031569211388
Loss in iteration 151 : 1.3290040131018261
Loss in iteration 152 : 4.47271154237111
Loss in iteration 153 : 1.4145166146116148
Loss in iteration 154 : 1.6541436444883242
Loss in iteration 155 : 4.300573484184248
Loss in iteration 156 : 1.2977314859396638
Loss in iteration 157 : 1.7026056612603337
Loss in iteration 158 : 4.14794885495939
Loss in iteration 159 : 1.1948081566764366
Loss in iteration 160 : 1.8604663696902004
Loss in iteration 161 : 4.326506487457535
Loss in iteration 162 : 1.3324906880799285
Loss in iteration 163 : 1.5005596723722174
Loss in iteration 164 : 3.5695581132342546
Loss in iteration 165 : 0.7922652467837049
Loss in iteration 166 : 2.8186917888431524
Loss in iteration 167 : 5.622044572804943
Loss in iteration 168 : 2.289856669999892
Loss in iteration 169 : 0.4540644959787735
Loss in iteration 170 : 0.6122338762195149
Loss in iteration 171 : 3.429348115200783
Loss in iteration 172 : 6.105473598877226
Loss in iteration 173 : 2.6540319424119727
Loss in iteration 174 : 0.43916495053028404
Loss in iteration 175 : 0.45251491587003956
Loss in iteration 176 : 1.1224538685564085
Loss in iteration 177 : 3.112988578411898
Loss in iteration 178 : 6.215323306127889
Loss in iteration 179 : 2.7475459109344573
Loss in iteration 180 : 0.45010746871978774
Loss in iteration 181 : 0.6177018988534199
Loss in iteration 182 : 1.8896395924140534
Loss in iteration 183 : 1.038209789706914
Loss in iteration 184 : 3.581005177771218
Loss in iteration 185 : 0.8449834297673188
Loss in iteration 186 : 3.3048991438839836
Loss in iteration 187 : 5.990818103135589
Loss in iteration 188 : 2.604480791586236
Loss in iteration 189 : 0.4352679871412344
Loss in iteration 190 : 0.4467191735598767
Loss in iteration 191 : 1.0963414389873776
Loss in iteration 192 : 3.103043503130035
Loss in iteration 193 : 6.108800406127163
Loss in iteration 194 : 2.7031407642248237
Loss in iteration 195 : 0.4465928710948578
Loss in iteration 196 : 0.608612944122409
Loss in iteration 197 : 1.8428139380929605
Loss in iteration 198 : 1.0624905193945862
Loss in iteration 199 : 3.6332730901322123
Loss in iteration 200 : 0.9153883355779067
Testing accuracy  of updater 3 on alg 1 with rate 1.372 = 0.51525, training accuracy 0.3761735189381677, time elapsed: 3395 millisecond.
Loss in iteration 1 : 1.0018437311820416
Loss in iteration 2 : 3.1398447170505333
Loss in iteration 3 : 1.6671885135427154
Loss in iteration 4 : 0.46471429380486245
Loss in iteration 5 : 0.7467761884842146
Loss in iteration 6 : 1.3748386591513668
Loss in iteration 7 : 0.3775986551499429
Loss in iteration 8 : 0.39163965210032653
Loss in iteration 9 : 0.6054620912549228
Loss in iteration 10 : 1.4301623010766598
Loss in iteration 11 : 0.380122260037534
Loss in iteration 12 : 0.39826367238745913
Loss in iteration 13 : 0.7030727847888966
Loss in iteration 14 : 1.808734737711197
Loss in iteration 15 : 0.5551027564293625
Loss in iteration 16 : 1.4499813549592335
Loss in iteration 17 : 2.6250447565798023
Loss in iteration 18 : 1.2439228129235687
Loss in iteration 19 : 0.3953849929418859
Loss in iteration 20 : 0.4322513908077937
Loss in iteration 21 : 0.7745848029630055
Loss in iteration 22 : 1.5793868251415102
Loss in iteration 23 : 0.41345231088487505
Loss in iteration 24 : 0.7153310410998425
Loss in iteration 25 : 1.5604607582361252
Loss in iteration 26 : 0.4042397983922462
Loss in iteration 27 : 0.6738215645692813
Loss in iteration 28 : 1.5226220599756455
Loss in iteration 29 : 0.3910049034516501
Loss in iteration 30 : 0.570259497063678
Loss in iteration 31 : 1.3254619561592025
Loss in iteration 32 : 0.4036693086861121
Loss in iteration 33 : 0.6810419601861872
Loss in iteration 34 : 1.4817961681683105
Loss in iteration 35 : 2.8481340978383365
Loss in iteration 36 : 1.444101602794391
Loss in iteration 37 : 0.39487917597616395
Loss in iteration 38 : 0.4264166471533088
Loss in iteration 39 : 0.6752469968542537
Loss in iteration 40 : 1.167990684390767
Loss in iteration 41 : 2.180543503956321
Loss in iteration 42 : 0.8854425989835657
Loss in iteration 43 : 0.6409515256500514
Loss in iteration 44 : 1.1116416352962075
Loss in iteration 45 : 0.4771008710919238
Loss in iteration 46 : 0.886998574399613
Loss in iteration 47 : 0.7958275754836804
Loss in iteration 48 : 1.6532115799272344
Loss in iteration 49 : 0.46523556819906764
Loss in iteration 50 : 1.046611528206355
Loss in iteration 51 : 1.9949331911979429
Loss in iteration 52 : 0.736613862072193
Loss in iteration 53 : 0.8474850344194997
Loss in iteration 54 : 1.4777987930515342
Loss in iteration 55 : 0.3924630963646765
Loss in iteration 56 : 0.49185928195392015
Loss in iteration 57 : 0.9906626154055679
Loss in iteration 58 : 0.6722862177247437
Loss in iteration 59 : 1.4735696620150291
Loss in iteration 60 : 0.38512893593426517
Loss in iteration 61 : 0.4897628630891541
Loss in iteration 62 : 1.0822873999135325
Loss in iteration 63 : 0.6046859480754422
Loss in iteration 64 : 1.3916456256062735
Loss in iteration 65 : 0.38073488423878166
Loss in iteration 66 : 0.3972395126107259
Loss in iteration 67 : 0.6578136709361799
Loss in iteration 68 : 1.6601822336342817
Loss in iteration 69 : 0.4753521018102061
Loss in iteration 70 : 1.316997338522405
Loss in iteration 71 : 2.4994987007043616
Loss in iteration 72 : 1.1734531573924918
Loss in iteration 73 : 0.40638989514429463
Loss in iteration 74 : 0.5055125341913077
Loss in iteration 75 : 1.0914246901486886
Loss in iteration 76 : 1.982157640828431
Loss in iteration 77 : 0.7412536672950194
Loss in iteration 78 : 0.8009291710267173
Loss in iteration 79 : 1.3698974487433888
Loss in iteration 80 : 0.3769522658839457
Loss in iteration 81 : 0.38204984124102054
Loss in iteration 82 : 0.4451097852088502
Loss in iteration 83 : 1.2238906454427885
Loss in iteration 84 : 2.5035057269414924
Loss in iteration 85 : 1.1855603804991588
Loss in iteration 86 : 0.4017205756391129
Loss in iteration 87 : 0.5046957562881478
Loss in iteration 88 : 1.1376031361184988
Loss in iteration 89 : 2.0582519264004158
Loss in iteration 90 : 0.813785174416726
Loss in iteration 91 : 0.6959637971060578
Loss in iteration 92 : 1.1941713354361136
Loss in iteration 93 : 0.4075112597822505
Loss in iteration 94 : 0.6096455470936009
Loss in iteration 95 : 1.2796364397539324
Loss in iteration 96 : 2.3275423581078933
Loss in iteration 97 : 1.0458420493292004
Loss in iteration 98 : 0.4592609428547271
Loss in iteration 99 : 0.695999852336606
Loss in iteration 100 : 0.9519595120734271
Loss in iteration 101 : 1.682046216936211
Loss in iteration 102 : 0.5167723392881796
Loss in iteration 103 : 0.9627509680080424
Loss in iteration 104 : 1.6663480506197774
Loss in iteration 105 : 0.507952079078286
Loss in iteration 106 : 0.9293613043830833
Loss in iteration 107 : 1.6186641106039437
Loss in iteration 108 : 0.4769655716766283
Loss in iteration 109 : 0.8996163366733703
Loss in iteration 110 : 1.6076132680901785
Loss in iteration 111 : 0.4702266795315029
Loss in iteration 112 : 0.9061664211916537
Loss in iteration 113 : 1.6467299966436404
Loss in iteration 114 : 0.4978487332504877
Loss in iteration 115 : 0.9599042906417242
Loss in iteration 116 : 1.6828495171486564
Loss in iteration 117 : 0.5261703947969623
Loss in iteration 118 : 0.9501156772080112
Loss in iteration 119 : 1.6306923303667882
Loss in iteration 120 : 0.4915543744145974
Loss in iteration 121 : 0.9011536806353309
Loss in iteration 122 : 1.5879171104573722
Loss in iteration 123 : 0.4641189353013232
Loss in iteration 124 : 0.8607848603372394
Loss in iteration 125 : 1.5768066170012804
Loss in iteration 126 : 0.4572116857503477
Loss in iteration 127 : 0.884953433300623
Loss in iteration 128 : 1.6394148448579722
Loss in iteration 129 : 0.5002027911991189
Loss in iteration 130 : 0.9698005576431259
Loss in iteration 131 : 1.701590948313321
Loss in iteration 132 : 0.5487457058465051
Loss in iteration 133 : 0.9488387293008818
Loss in iteration 134 : 1.6100040012041383
Loss in iteration 135 : 0.48555246554789366
Loss in iteration 136 : 0.878697403401242
Loss in iteration 137 : 1.5493937621431881
Loss in iteration 138 : 0.4473165168074031
Loss in iteration 139 : 0.8071916338685552
Loss in iteration 140 : 1.5136074033431755
Loss in iteration 141 : 0.4264391889866168
Loss in iteration 142 : 0.746369522510078
Loss in iteration 143 : 1.4804616924541996
Loss in iteration 144 : 0.40957975087387943
Loss in iteration 145 : 0.6714926993188954
Loss in iteration 146 : 1.4071689265758476
Loss in iteration 147 : 0.3849885531678581
Loss in iteration 148 : 0.4795252951616909
Loss in iteration 149 : 0.9983799180568117
Loss in iteration 150 : 0.6421578913004808
Loss in iteration 151 : 1.4578375568537054
Loss in iteration 152 : 0.3998807205203939
Loss in iteration 153 : 0.6632210939103876
Loss in iteration 154 : 1.492608611825393
Loss in iteration 155 : 0.41527695416295607
Loss in iteration 156 : 0.8150647491040843
Loss in iteration 157 : 1.7227107584254788
Loss in iteration 158 : 0.5755357830734958
Loss in iteration 159 : 1.1236011522020317
Loss in iteration 160 : 1.957754981428854
Loss in iteration 161 : 0.7734842602696672
Loss in iteration 162 : 0.691691770377013
Loss in iteration 163 : 1.1580892519311041
Loss in iteration 164 : 0.4032320855149565
Loss in iteration 165 : 0.5815970852599099
Loss in iteration 166 : 1.2342838463153314
Loss in iteration 167 : 2.2203371273875856
Loss in iteration 168 : 0.9989985271752393
Loss in iteration 169 : 0.4643058057646934
Loss in iteration 170 : 0.7054370876799774
Loss in iteration 171 : 0.8840514146093555
Loss in iteration 172 : 1.5680451566726097
Loss in iteration 173 : 0.47172004005627205
Loss in iteration 174 : 0.8996736610505996
Loss in iteration 175 : 1.6019324347598942
Loss in iteration 176 : 0.49626982963859384
Loss in iteration 177 : 0.9313809976953749
Loss in iteration 178 : 1.6105634254383172
Loss in iteration 179 : 0.5044260001197187
Loss in iteration 180 : 0.9082014863601422
Loss in iteration 181 : 1.5654826802205375
Loss in iteration 182 : 0.475011399274205
Loss in iteration 183 : 0.8815007239454806
Loss in iteration 184 : 1.5476529711253464
Loss in iteration 185 : 0.46403308456251463
Loss in iteration 186 : 0.8640398111239109
Loss in iteration 187 : 1.5517661732277999
Loss in iteration 188 : 0.46731094933107614
Loss in iteration 189 : 0.8803808909379708
Loss in iteration 190 : 1.5842352962308133
Loss in iteration 191 : 0.49065191389452156
Loss in iteration 192 : 0.9309330057676937
Loss in iteration 193 : 1.6147001221558073
Loss in iteration 194 : 0.5144488516509159
Loss in iteration 195 : 0.9101863410108401
Loss in iteration 196 : 1.5623323648309113
Loss in iteration 197 : 0.4796984210753133
Loss in iteration 198 : 0.8769136215094548
Loss in iteration 199 : 1.5298650541379064
Loss in iteration 200 : 0.4591991537360609
Testing accuracy  of updater 3 on alg 1 with rate 0.784 = 0.71975, training accuracy 0.6633214632567174, time elapsed: 4008 millisecond.
Loss in iteration 1 : 1.0000936916461869
Loss in iteration 2 : 0.9725929230681525
Loss in iteration 3 : 0.7211866990970789
Loss in iteration 4 : 0.4993234720304098
Loss in iteration 5 : 0.4697761224777385
Loss in iteration 6 : 0.4550518236606523
Loss in iteration 7 : 0.44820141777721667
Loss in iteration 8 : 0.4408189896979388
Loss in iteration 9 : 0.4352640189875381
Loss in iteration 10 : 0.42969939478402724
Loss in iteration 11 : 0.4252205451222138
Loss in iteration 12 : 0.42119521583165914
Loss in iteration 13 : 0.41798921966701796
Loss in iteration 14 : 0.41511051608499133
Loss in iteration 15 : 0.41270926879785985
Loss in iteration 16 : 0.41074700411724174
Loss in iteration 17 : 0.4091832477532947
Loss in iteration 18 : 0.40781714862961577
Loss in iteration 19 : 0.40679357359445134
Loss in iteration 20 : 0.40581059546178244
Loss in iteration 21 : 0.4050160428175502
Loss in iteration 22 : 0.40434205318585137
Loss in iteration 23 : 0.4037524267475649
Loss in iteration 24 : 0.4033255074189556
Loss in iteration 25 : 0.40286533530959523
Loss in iteration 26 : 0.40260755020760364
Loss in iteration 27 : 0.40227534698166345
Loss in iteration 28 : 0.40212458821201313
Loss in iteration 29 : 0.40182266850311876
Loss in iteration 30 : 0.4017101947326596
Loss in iteration 31 : 0.4014915284403207
Loss in iteration 32 : 0.4014228294403949
Loss in iteration 33 : 0.4013135675896151
Loss in iteration 34 : 0.4012902087676209
Loss in iteration 35 : 0.4011812357135597
Loss in iteration 36 : 0.40119437125374025
Testing accuracy  of updater 3 on alg 1 with rate 0.196 = 0.777, training accuracy 0.8355454839753965, time elapsed: 851 millisecond.
Loss in iteration 1 : 1.000045321600705
Loss in iteration 2 : 0.7847550981548492
Loss in iteration 3 : 0.6152905735978567
Loss in iteration 4 : 0.49790118396724337
Loss in iteration 5 : 0.4908648784399268
Loss in iteration 6 : 0.48438331787465533
Loss in iteration 7 : 0.47892302653887836
Loss in iteration 8 : 0.4735494703431206
Loss in iteration 9 : 0.46836877548891565
Loss in iteration 10 : 0.46335920349744486
Loss in iteration 11 : 0.4585240035075811
Loss in iteration 12 : 0.4538132444093119
Loss in iteration 13 : 0.4493413186743983
Loss in iteration 14 : 0.4450155406889092
Loss in iteration 15 : 0.4407950443625836
Loss in iteration 16 : 0.4367273222770039
Loss in iteration 17 : 0.4329374732048943
Loss in iteration 18 : 0.4294636101352576
Loss in iteration 19 : 0.4263502611594497
Loss in iteration 20 : 0.423517247127666
Loss in iteration 21 : 0.42095507787103426
Loss in iteration 22 : 0.41872533332616796
Loss in iteration 23 : 0.41669559500997466
Loss in iteration 24 : 0.41486295917274363
Loss in iteration 25 : 0.4132717488766351
Loss in iteration 26 : 0.41184505557944534
Loss in iteration 27 : 0.4106150233081202
Loss in iteration 28 : 0.4095173755929808
Loss in iteration 29 : 0.4085676996322609
Loss in iteration 30 : 0.4076910161854416
Loss in iteration 31 : 0.4069814376586171
Loss in iteration 32 : 0.40631711164504025
Loss in iteration 33 : 0.4057355235950781
Loss in iteration 34 : 0.4051965502083604
Loss in iteration 35 : 0.4047086968410881
Loss in iteration 36 : 0.4042682527022234
Loss in iteration 37 : 0.4038940327471359
Loss in iteration 38 : 0.40353404391736747
Loss in iteration 39 : 0.4032654302808959
Loss in iteration 40 : 0.4029729747695688
Loss in iteration 41 : 0.40279133949392826
Loss in iteration 42 : 0.40252731949252146
Testing accuracy  of updater 3 on alg 1 with rate 0.13720000000000002 = 0.7775, training accuracy 0.8358692133376497, time elapsed: 1039 millisecond.
Loss in iteration 1 : 1.0000146042423317
Loss in iteration 2 : 0.6004806993609829
Loss in iteration 3 : 0.5350356700920532
Loss in iteration 4 : 0.5260295862161055
Loss in iteration 5 : 0.5214985239359532
Loss in iteration 6 : 0.5173338663605579
Loss in iteration 7 : 0.5133057177031342
Loss in iteration 8 : 0.5094242803804014
Loss in iteration 9 : 0.5056602783900379
Loss in iteration 10 : 0.5019811301283712
Loss in iteration 11 : 0.4984165918080668
Loss in iteration 12 : 0.49497273390749835
Loss in iteration 13 : 0.4915937866086431
Loss in iteration 14 : 0.48829517133360373
Loss in iteration 15 : 0.48506463747518286
Loss in iteration 16 : 0.48190653276404144
Loss in iteration 17 : 0.4787992242938043
Loss in iteration 18 : 0.4757451152029054
Loss in iteration 19 : 0.47278087931902585
Loss in iteration 20 : 0.4698738700448617
Loss in iteration 21 : 0.46701373534002427
Loss in iteration 22 : 0.46420100395771857
Loss in iteration 23 : 0.4614460717556775
Loss in iteration 24 : 0.45873373658742994
Loss in iteration 25 : 0.45606764551007484
Loss in iteration 26 : 0.4534703481467317
Loss in iteration 27 : 0.45095929501218796
Loss in iteration 28 : 0.4484926747983627
Loss in iteration 29 : 0.4460515791184878
Loss in iteration 30 : 0.44365474861245885
Loss in iteration 31 : 0.4412970409830085
Loss in iteration 32 : 0.4389801959194493
Loss in iteration 33 : 0.43674787532048714
Loss in iteration 34 : 0.4346016435790198
Loss in iteration 35 : 0.432546885943351
Loss in iteration 36 : 0.43058401510930966
Loss in iteration 37 : 0.4287327662601448
Loss in iteration 38 : 0.4270080577941306
Loss in iteration 39 : 0.4253506503178846
Loss in iteration 40 : 0.4237661669648337
Loss in iteration 41 : 0.4222837873543484
Loss in iteration 42 : 0.4209180161672925
Loss in iteration 43 : 0.419650369767015
Loss in iteration 44 : 0.4184509836823004
Loss in iteration 45 : 0.41731918768179377
Loss in iteration 46 : 0.41625293313505196
Loss in iteration 47 : 0.4152483341611295
Loss in iteration 48 : 0.4143210959642132
Loss in iteration 49 : 0.41344350292474474
Loss in iteration 50 : 0.4126147757725868
Loss in iteration 51 : 0.41186252342121993
Loss in iteration 52 : 0.4111638660669737
Testing accuracy  of updater 3 on alg 1 with rate 0.07840000000000001 = 0.773, training accuracy 0.8348980252508903, time elapsed: 1050 millisecond.
Loss in iteration 1 : 1.000000900059842
Loss in iteration 2 : 0.7216620395601605
Loss in iteration 3 : 0.5785007966709623
Loss in iteration 4 : 0.5556968642645724
Testing accuracy  of updater 3 on alg 1 with rate 0.019600000000000006 = 0.5, training accuracy 0.6474587245063127, time elapsed: 94 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 1.1059762627984824
Loss in iteration 3 : 638190.3069600448
Loss in iteration 4 : 2.5496625837403066E12
Loss in iteration 5 : 1.0188454004383472E19
Loss in iteration 6 : 4.07130723877968E25
Loss in iteration 7 : 1.6268947797470748E32
Loss in iteration 8 : 6.501073166764091E38
Loss in iteration 9 : 2.597829487546248E45
Loss in iteration 10 : 1.0380929230064294E52
Loss in iteration 11 : 4.148220358426614E58
Loss in iteration 12 : 1.657629270049311E65
Loss in iteration 13 : 6.623888220746315E71
Loss in iteration 14 : 2.64690639539905E78
Loss in iteration 15 : 1.0577040602920996E85
Loss in iteration 16 : 4.22658648263129E91
Loss in iteration 17 : 1.6889443811181123E98
Loss in iteration 18 : 6.749023435892357E104
Loss in iteration 19 : 2.6969104398849295E111
Loss in iteration 20 : 1.0776856814690618E118
Loss in iteration 21 : 4.306433060836052E124
Loss in iteration 22 : 1.7208510817533927E131
Loss in iteration 23 : 6.876522643537641E137
Loss in iteration 24 : 2.7478591360099048E144
Loss in iteration 25 : 1.0980447855354717E151
Loss in iteration 26 : 4.38778806104453E157
Loss in iteration 27 : 1.7533605479722E164
Loss in iteration 28 : 7.00643050305746E170
Loss in iteration 29 : 2.7997703296648117E177
Loss in iteration 30 : 1.1187885037110914E184
Loss in iteration 31 : 4.4706799796180255E190
Loss in iteration 32 : 1.786484166923361E197
Loss in iteration 33 : 7.138792517509918E203
Loss in iteration 34 : 2.852662203876215E210
Loss in iteration 35 : 1.139924101935156E217
Loss in iteration 36 : 4.555137851256984E223
Loss in iteration 37 : 1.8202335408760769E230
Loss in iteration 38 : 7.273655049574341E236
Loss in iteration 39 : 2.9065532851754124E243
Loss in iteration 40 : 1.161458983411423E250
Loss in iteration 41 : 4.641191259171031E256
Loss in iteration 42 : 1.8546204912838696E263
Loss in iteration 43 : 7.411065337790835E269
Loss in iteration 44 : 2.961462450087751E276
Loss in iteration 45 : 1.1834006912013103E283
Loss in iteration 46 : 4.728870345441127E289
Loss in iteration 47 : 1.889657062925309E296
Loss in iteration 48 : 7.551071513106598E302
Loss in iteration 49 : Infinity
Loss in iteration 50 : Infinity
Loss in iteration 51 : Infinity
Loss in iteration 52 : Infinity
Loss in iteration 53 : Infinity
Loss in iteration 54 : Infinity
Loss in iteration 55 : Infinity
Loss in iteration 56 : Infinity
Loss in iteration 57 : Infinity
Loss in iteration 58 : Infinity
Loss in iteration 59 : Infinity
Loss in iteration 60 : Infinity
Loss in iteration 61 : Infinity
Loss in iteration 62 : Infinity
Loss in iteration 63 : Infinity
Loss in iteration 64 : Infinity
Loss in iteration 65 : Infinity
Loss in iteration 66 : Infinity
Loss in iteration 67 : Infinity
Loss in iteration 68 : Infinity
Loss in iteration 69 : Infinity
Loss in iteration 70 : Infinity
Loss in iteration 71 : Infinity
Loss in iteration 72 : Infinity
Loss in iteration 73 : Infinity
Loss in iteration 74 : Infinity
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 10000.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 4091 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 1.02447506838928
Loss in iteration 3 : 153093.86420897176
Loss in iteration 4 : 2.994477971280785E11
Loss in iteration 5 : 5.8607947290164685E17
Loss in iteration 6 : 1.1470753300777539E24
Loss in iteration 7 : 2.245056983103402E30
Loss in iteration 8 : 4.394027772386961E36
Loss in iteration 9 : 8.599995550143531E42
Loss in iteration 10 : 1.683191989073647E49
Loss in iteration 11 : 3.294345044206932E55
Loss in iteration 12 : 6.447695414866853E61
Loss in iteration 13 : 1.2619435913672815E68
Loss in iteration 14 : 2.469877258967633E74
Loss in iteration 15 : 4.834046241128713E80
Loss in iteration 16 : 9.461200137183356E86
Loss in iteration 17 : 1.8517470369695403E93
Loss in iteration 18 : 3.6242411525038206E99
Loss in iteration 19 : 7.093368407921632E105
Loss in iteration 20 : 1.3883147741352626E112
Loss in iteration 21 : 2.7172110642523107E118
Loss in iteration 22 : 5.318128212165688E124
Loss in iteration 23 : 1.0408645854978892E131
Loss in iteration 24 : 2.037181207601055E137
Loss in iteration 25 : 3.987173096697991E143
Loss in iteration 26 : 7.803699172030405E149
Loss in iteration 27 : 1.5273407823197082E156
Loss in iteration 28 : 2.989312906496916E162
Loss in iteration 29 : 5.85068620990867E168
Loss in iteration 30 : 1.1450968900719456E175
Loss in iteration 31 : 2.241184778345702E181
Loss in iteration 32 : 4.386449089362985E187
Loss in iteration 33 : 8.585162544150324E193
Loss in iteration 34 : 1.680288871657356E200
Loss in iteration 35 : 3.2886630598966494E206
Loss in iteration 36 : 6.436574629492781E212
Loss in iteration 37 : 1.2597670301417899E219
Loss in iteration 38 : 2.4656172911605422E225
Loss in iteration 39 : 4.825708627876704E231
Loss in iteration 40 : 9.444881752188913E237
Loss in iteration 41 : 1.8485532010265896E244
Loss in iteration 42 : 3.617990173602441E250
Loss in iteration 43 : 7.081133985764874E256
Loss in iteration 44 : 1.3859202518072996E263
Loss in iteration 45 : 2.7125245027574974E269
Loss in iteration 46 : 5.3089556693214775E275
Loss in iteration 47 : 1.0390693344951665E282
Loss in iteration 48 : 2.033667540543274E288
Loss in iteration 49 : 3.980296144018838E294
Loss in iteration 50 : 7.790239593369813E300
Loss in iteration 51 : 1.5247064722382992E307
Loss in iteration 52 : Infinity
Loss in iteration 53 : Infinity
Loss in iteration 54 : Infinity
Loss in iteration 55 : Infinity
Loss in iteration 56 : Infinity
Loss in iteration 57 : Infinity
Loss in iteration 58 : Infinity
Loss in iteration 59 : Infinity
Loss in iteration 60 : Infinity
Loss in iteration 61 : Infinity
Loss in iteration 62 : Infinity
Loss in iteration 63 : Infinity
Loss in iteration 64 : Infinity
Loss in iteration 65 : Infinity
Loss in iteration 66 : Infinity
Loss in iteration 67 : Infinity
Loss in iteration 68 : Infinity
Loss in iteration 69 : Infinity
Loss in iteration 70 : Infinity
Loss in iteration 71 : Infinity
Loss in iteration 72 : Infinity
Loss in iteration 73 : Infinity
Loss in iteration 74 : Infinity
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 7000.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 4181 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.971773323214659
Loss in iteration 3 : 16315.13947193968
Loss in iteration 4 : 1.0380486872594873E10
Loss in iteration 5 : 6.626907357182758E15
Loss in iteration 6 : 4.230624261579543E21
Loss in iteration 7 : 2.700834759212911E27
Loss in iteration 8 : 1.724215611116268E33
Loss in iteration 9 : 1.1007409703522364E39
Loss in iteration 10 : 7.027141362138383E44
Loss in iteration 11 : 4.486134072730505E50
Loss in iteration 12 : 2.863952478165227E56
Loss in iteration 13 : 1.8283501260131594E62
Loss in iteration 14 : 1.167220548796927E68
Loss in iteration 15 : 7.451547655725071E73
Loss in iteration 16 : 4.75707547496254E79
Loss in iteration 17 : 3.0369217402915604E85
Loss in iteration 18 : 1.9387738759238724E91
Loss in iteration 19 : 1.2377151811636762E97
Loss in iteration 20 : 7.901586093700722E102
Loss in iteration 21 : 5.044380463804633E108
Loss in iteration 22 : 3.220337532473342E114
Loss in iteration 23 : 2.0558667010685136E120
Loss in iteration 24 : 1.31246735782884E126
Loss in iteration 25 : 8.378804737052893E131
Loss in iteration 26 : 5.349037322939304E137
Loss in iteration 27 : 3.4148307760017733E143
Loss in iteration 28 : 2.1800313822303085E149
Loss in iteration 29 : 1.391734214447211E155
Loss in iteration 30 : 8.884845142373142E160
Loss in iteration 31 : 5.672094023736156E166
Loss in iteration 32 : 3.6210704968471864E172
Loss in iteration 33 : 2.3116950262577406E178
Loss in iteration 34 : 1.4757884164579681E184
Loss in iteration 35 : 9.421448008551833E189
Loss in iteration 36 : 6.014661830107498E195
Loss in iteration 37 : 3.839766127002456E201
Loss in iteration 38 : 2.4513105352444954E207
Loss in iteration 39 : 1.564919097010621E213
Loss in iteration 40 : 9.990459164506776E218
Loss in iteration 41 : 6.3779191210802894E224
Loss in iteration 42 : 4.071669944816778E230
Loss in iteration 43 : 2.599358164440976E236
Loss in iteration 44 : 1.6594328515372834E242
Loss in iteration 45 : 1.0593835918542532E248
Loss in iteration 46 : 6.763115444233472E253
Loss in iteration 47 : 4.317579662714092E259
Loss in iteration 48 : 2.7563471742563392E265
Loss in iteration 49 : 1.7596547923924216E271
Loss in iteration 50 : 1.1233653791181142E277
Loss in iteration 51 : 7.171575813943831E282
Loss in iteration 52 : 4.578341171197557E288
Loss in iteration 53 : 2.922817582033691E294
Loss in iteration 54 : 1.8659296671878902E300
Loss in iteration 55 : 1.1912113654624163E306
Loss in iteration 56 : Infinity
Loss in iteration 57 : Infinity
Loss in iteration 58 : Infinity
Loss in iteration 59 : Infinity
Loss in iteration 60 : Infinity
Loss in iteration 61 : Infinity
Loss in iteration 62 : Infinity
Loss in iteration 63 : Infinity
Loss in iteration 64 : Infinity
Loss in iteration 65 : Infinity
Loss in iteration 66 : Infinity
Loss in iteration 67 : Infinity
Loss in iteration 68 : Infinity
Loss in iteration 69 : Infinity
Loss in iteration 70 : Infinity
Loss in iteration 71 : Infinity
Loss in iteration 72 : Infinity
Loss in iteration 73 : Infinity
Loss in iteration 74 : Infinity
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 4000.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 2948 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.9478710272746188
Loss in iteration 3 : 76.22222007029666
Loss in iteration 4 : 2459221.1604211377
Loss in iteration 5 : 9.736560924754233E10
Loss in iteration 6 : 3.855754359440113E15
Loss in iteration 7 : 1.5269172751720484E20
Loss in iteration 8 : 6.046745100571968E24
Loss in iteration 9 : 2.394571527277159E29
Loss in iteration 10 : 9.482742705170248E33
Loss in iteration 11 : 3.75526093867447E38
Loss in iteration 12 : 1.4871208843244765E43
Loss in iteration 13 : 5.889147414013361E47
Loss in iteration 14 : 2.3321612674234308E52
Loss in iteration 15 : 9.235591835123528E56
Loss in iteration 16 : 3.657386722627269E61
Loss in iteration 17 : 1.4483617160276246E66
Loss in iteration 18 : 5.735657231640997E70
Loss in iteration 19 : 2.271377620302151E75
Loss in iteration 20 : 8.994882514158546E79
Loss in iteration 21 : 3.562063424431926E84
Loss in iteration 22 : 1.4106127367092875E89
Loss in iteration 23 : 5.586167498642447E93
Loss in iteration 24 : 2.2121781911373958E98
Loss in iteration 25 : 8.760446854723203E102
Loss in iteration 26 : 3.469224558938935E107
Loss in iteration 27 : 1.3738476175854075E112
Loss in iteration 28 : 5.4405739503999715E116
Loss in iteration 29 : 2.1545216900978933E121
Loss in iteration 30 : 8.532121344956667E125
Loss in iteration 31 : 3.3788053738162894E130
Loss in iteration 32 : 1.3380407160849885E135
Loss in iteration 33 : 5.298775039768164E139
Loss in iteration 34 : 2.09836790349859E144
Loss in iteration 35 : 8.309746734644767E148
Loss in iteration 36 : 3.2907428043866743E153
Loss in iteration 37 : 1.3031670579651668E158
Loss in iteration 38 : 5.1606718662478585E162
Loss in iteration 39 : 2.0436776657528143E167
Loss in iteration 40 : 8.093167924147717E171
Loss in iteration 41 : 3.2049754296417376E176
Loss in iteration 42 : 1.2692023198924247E181
Loss in iteration 43 : 5.026168107005992E185
Loss in iteration 44 : 1.9904128320554426E190
Loss in iteration 45 : 7.882233856222756E194
Loss in iteration 46 : 3.1214434294027744E199
Loss in iteration 47 : 1.2361228124777927E204
Loss in iteration 48 : 4.895169949693306E208
Loss in iteration 49 : 1.9385362517780464E213
Loss in iteration 50 : 7.676797410666239E217
Loss in iteration 51 : 3.0400885425979375E222
Loss in iteration 52 : 1.2039054637542094E227
Loss in iteration 53 : 4.7675860270130456E231
Loss in iteration 54 : 1.888011742557436E236
Loss in iteration 55 : 7.4767153017017E240
Loss in iteration 56 : 2.960854026626891E245
Loss in iteration 57 : 1.1725278030845151E250
Loss in iteration 58 : 4.643327352994988E254
Loss in iteration 59 : 1.838804065059545E259
Loss in iteration 60 : 7.281847978042303E263
Loss in iteration 61 : 2.8836846177845324E268
Loss in iteration 62 : 1.141967945488853E273
Loss in iteration 63 : 4.5223072609304065E277
Loss in iteration 64 : 1.7908788984010503E282
Loss in iteration 65 : 7.0920595255579975E286
Loss in iteration 66 : 2.808526492716223E291
Loss in iteration 67 : 1.1122045763805516E296
Loss in iteration 68 : 4.404441342924622E300
Loss in iteration 69 : 1.7442028162115797E305
Loss in iteration 70 : Infinity
Loss in iteration 71 : Infinity
Loss in iteration 72 : Infinity
Loss in iteration 73 : Infinity
Loss in iteration 74 : Infinity
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 1000.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3176 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.9470647673885167
Loss in iteration 3 : 24.761825122972695
Loss in iteration 4 : 284552.95828025823
Loss in iteration 5 : 5.492671792662567E9
Loss in iteration 6 : 1.0612040742139303E14
Loss in iteration 7 : 2.05035229191646925E18
Loss in iteration 8 : 3.961485656441205E22
Loss in iteration 9 : 7.653986436790756E26
Loss in iteration 10 : 1.4788267194522117E31
Loss in iteration 11 : 2.857241104653617E35
Loss in iteration 12 : 5.520475538301253E39
Loss in iteration 13 : 1.0666110787551854E44
Loss in iteration 14 : 2.0607992652628936E48
Loss in iteration 15 : 3.981670260414437E52
Loss in iteration 16 : 7.6929851101467315E56
Loss in iteration 17 : 1.4863616531314505E61
Loss in iteration 18 : 2.871799350015275E65
Loss in iteration 19 : 5.5486035241645144E69
Loss in iteration 20 : 1.072045686903826E74
Loss in iteration 21 : 2.071299471666882E78
Loss in iteration 22 : 4.0019577092075824E82
Loss in iteration 23 : 7.732182489959972E86
Loss in iteration 24 : 1.4939349788851656E91
Loss in iteration 25 : 2.8864317727040286E95
Loss in iteration 26 : 5.576874828041453E99
Loss in iteration 27 : 1.0775079855258892E104
Loss in iteration 28 : 2.081853178834571E108
Loss in iteration 29 : 4.022348526826275E112
Loss in iteration 30 : 7.771579588681044E116
Loss in iteration 31 : 1.501546892329065E121
Loss in iteration 32 : 2.901138750668986E125
Loss in iteration 33 : 5.605290180167548E129
Loss in iteration 34 : 1.0829981157101714E134
Loss in iteration 35 : 2.092460659363623E138
Loss in iteration 36 : 4.0428432399564555E142
Loss in iteration 37 : 7.811177423919868E146
Loss in iteration 38 : 1.5091975900755576E151
Loss in iteration 39 : 2.9159206637849845E155
Loss in iteration 40 : 5.633850314498971E159
Loss in iteration 41 : 1.0885162192643456E164
Loss in iteration 42 : 2.1031221872406423E168
Loss in iteration 43 : 4.063442377967645E172
Loss in iteration 44 : 7.850977018471287E176
Loss in iteration 45 : 1.5168872697388378E181
Loss in iteration 46 : 2.930777893862408E185
Loss in iteration 47 : 5.662555968731558E189
Loss in iteration 48 : 1.0940624387186245E194
Loss in iteration 49 : 2.113838037848254E198
Loss in iteration 50 : 4.084146472926611E202
Loss in iteration 51 : 7.890979400341506E206
Loss in iteration 52 : 1.5246161299399822E211
Loss in iteration 53 : 2.9457108246570403E215
Loss in iteration 54 : 5.691407884319868E219
Loss in iteration 55 : 1.0996369173294414E224
Loss in iteration 56 : 2.1246084879722132E228
Loss in iteration 57 : 4.104956059611114E232
Loss in iteration 58 : 7.931185602774635E236
Loss in iteration 59 : 1.532384370312087E241
Loss in iteration 60 : 2.9607198418799835E245
Loss in iteration 61 : 5.720406806496315E249
Loss in iteration 62 : 1.1052397990831532E254
Loss in iteration 63 : 2.13543381580856E258
Loss in iteration 64 : 4.1258716755237194E262
Loss in iteration 65 : 7.971596664279379E266
Loss in iteration 66 : 1.540192191505419E271
Loss in iteration 67 : 2.975805333207619E275
Loss in iteration 68 : 5.749553484290442E279
Loss in iteration 69 : 1.1108712286997562E284
Loss in iteration 70 : 2.1463143009707986E288
Loss in iteration 71 : 4.146893860905681E292
Loss in iteration 72 : 8.012213628655866E296
Loss in iteration 73 : 1.5480397951926E301
Loss in iteration 74 : 2.9909676882916223E305
Loss in iteration 75 : Infinity
Loss in iteration 76 : Infinity
Loss in iteration 77 : Infinity
Loss in iteration 78 : Infinity
Loss in iteration 79 : Infinity
Loss in iteration 80 : Infinity
Loss in iteration 81 : Infinity
Loss in iteration 82 : Infinity
Loss in iteration 83 : Infinity
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 700.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3548 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.9465465019947604
Loss in iteration 3 : 7.478731131104393
Loss in iteration 4 : 9564.036508147976
Loss in iteration 5 : 5.916489684993966E7
Loss in iteration 6 : 3.69041696451435E11
Loss in iteration 7 : 2.3031859694000805E15
Loss in iteration 8 : 1.4374182346753954E19
Loss in iteration 9 : 8.970927200575731E22
Loss in iteration 10 : 5.598755665798914E26
Loss in iteration 11 : 3.494183411024975E30
Loss in iteration 12 : 2.1807198668206817E34
Loss in iteration 13 : 1.3609872688827876E38
Loss in iteration 14 : 8.493921545097477E41
Loss in iteration 15 : 5.301056436295336E45
Loss in iteration 16 : 3.308389321891919E49
Loss in iteration 17 : 2.0647657757927466E53
Loss in iteration 18 : 1.2886203206722532E57
Loss in iteration 19 : 8.042279421315532E60
Loss in iteration 20 : 5.019186586843024E64
Loss in iteration 21 : 3.132474348848731E68
Loss in iteration 22 : 1.9549772411164936E72
Loss in iteration 23 : 1.2201012961808033E76
Loss in iteration 24 : 7.614652189464395E79
Loss in iteration 25 : 4.752304431444728E83
Loss in iteration 26 : 2.9659131956646553E87
Loss in iteration 27 : 1.8510264254143112E91
Loss in iteration 28 : 1.1552255921010715E95
Loss in iteration 29 : 7.209762920302786E98
Loss in iteration 30 : 4.499613038560969E102
Loss in iteration 31 : 2.8082084973659013E106
Loss in iteration 32 : 1.7526029232060586E110
Loss in iteration 33 : 1.0937994843729012E114
Loss in iteration 34 : 6.826402581971276E117
Loss in iteration 35 : 4.260357851408274E121
Loss in iteration 36 : 2.6588893350639043E125
Loss in iteration 37 : 1.6594128340133827E129
Loss in iteration 38 : 1.035639549707752E133
Loss in iteration 39 : 6.463426429726081E136
Loss in iteration 40 : 4.033824434792047E140
Loss in iteration 41 : 2.5175098297537168E144
Loss in iteration 42 : 1.5711778847492947E148
Loss in iteration 43 : 9.805721178720349E151
Loss in iteration 44 : 6.119750587639369E155
Loss in iteration 45 : 3.81933634174573E159
Loss in iteration 46 : 2.3836478108835097E163
Loss in iteration 47 : 1.4876345987723986E167
Loss in iteration 48 : 9.28432753093854E170
Loss in iteration 49 : 5.794348812058742E174
Loss in iteration 50 : 3.6162530936058606E178
Loss in iteration 51 : 2.2569035557194178E182
Loss in iteration 52 : 1.4085335091244888E186
Loss in iteration 53 : 8.790657630445934E189
Loss in iteration 54 : 5.486249427161308E193
Loss in iteration 55 : 3.423968267491372E197
Loss in iteration 56 : 2.1368985957413644E201
Loss in iteration 57 : 1.3336384136021863E205
Loss in iteration 58 : 8.323237339291241E208
Loss in iteration 59 : 5.194532423451664E212
Loss in iteration 60 : 3.241907685476184E216
Loss in iteration 61 : 2.0232745865056863E220
Loss in iteration 62 : 1.2627256694381986E224
Loss in iteration 63 : 7.880670902963798E227
Loss in iteration 64 : 4.918326710539706E231
Loss in iteration 65 : 3.069527700047831E235
Loss in iteration 66 : 1.915692237599851E239
Loss in iteration 67 : 1.1955835254860672E243
Loss in iteration 68 : 7.461636782558544E246
Loss in iteration 69 : 4.6568075159947873E250
Loss in iteration 70 : 2.906313570732348E254
Loss in iteration 71 : 1.813830299494058E258
Loss in iteration 72 : 1.1320114899142416E262
Loss in iteration 73 : 7.06488370855478E265
Loss in iteration 74 : 4.409193922509039E269
Loss in iteration 75 : 2.751777927037892E273
Loss in iteration 76 : 1.717384604264348E277
Loss in iteration 77 : 1.0718197315213796E281
Loss in iteration 78 : 6.68922694442493E284
Loss in iteration 79 : 4.1747465360155977E288
Loss in iteration 80 : 2.6054593131273346E292
Loss in iteration 81 : 1.6260671573227702E296
Loss in iteration 82 : 1.0148285128851409E300
Loss in iteration 83 : 6.333544748916164E303
Loss in iteration 84 : Infinity
Loss in iteration 85 : Infinity
Loss in iteration 86 : Infinity
Loss in iteration 87 : Infinity
Loss in iteration 88 : Infinity
Loss in iteration 89 : Infinity
Loss in iteration 90 : Infinity
Loss in iteration 91 : Infinity
Loss in iteration 92 : Infinity
Loss in iteration 93 : Infinity
Loss in iteration 94 : Infinity
Loss in iteration 95 : Infinity
Loss in iteration 96 : Infinity
Loss in iteration 97 : Infinity
Loss in iteration 98 : Infinity
Loss in iteration 99 : Infinity
Loss in iteration 100 : Infinity
Loss in iteration 101 : Infinity
Loss in iteration 102 : Infinity
Loss in iteration 103 : Infinity
Loss in iteration 104 : Infinity
Loss in iteration 105 : Infinity
Loss in iteration 106 : Infinity
Loss in iteration 107 : Infinity
Loss in iteration 108 : Infinity
Loss in iteration 109 : Infinity
Loss in iteration 110 : Infinity
Loss in iteration 111 : Infinity
Loss in iteration 112 : Infinity
Loss in iteration 113 : Infinity
Loss in iteration 114 : Infinity
Loss in iteration 115 : Infinity
Loss in iteration 116 : Infinity
Loss in iteration 117 : Infinity
Loss in iteration 118 : Infinity
Loss in iteration 119 : Infinity
Loss in iteration 120 : Infinity
Loss in iteration 121 : Infinity
Loss in iteration 122 : Infinity
Loss in iteration 123 : Infinity
Loss in iteration 124 : Infinity
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 400.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3565 millisecond.
Loss in iteration 1 : 1.000000039999235
Loss in iteration 2 : 0.94631623109335
Loss in iteration 3 : 1.9782660851906644
Loss in iteration 4 : 6.993542582984772
Loss in iteration 5 : 1055.6431372970912
Loss in iteration 6 : 222518.0380439028
Loss in iteration 7 : 7.984422158775921E7
Loss in iteration 8 : 2.8766603961296635E10
Loss in iteration 9 : 1.0384569066327578E13
Loss in iteration 10 : 3.7488087982560485E15
Loss in iteration 11 : 1.3533199130085865E18
Loss in iteration 12 : 4.885484811469774E20
Loss in iteration 13 : 1.7636600167125736E23
Loss in iteration 14 : 6.366812660063477E25
Loss in iteration 15 : 2.298419370282093E28
Loss in iteration 16 : 8.297293926717383E30
Loss in iteration 17 : 2.9953231075449723E33
Loss in iteration 18 : 1.081311641823735E36
Loss in iteration 19 : 3.9035350269836825E38
Loss in iteration 20 : 1.40917614474111E41
Loss in iteration 21 : 5.087125882515406E43
Loss in iteration 22 : 1.8364524435880609E46
Loss in iteration 23 : 6.6295933213529E48
Loss in iteration 24 : 2.3932831890083972E51
Loss in iteration 25 : 8.639752312320312E53
Loss in iteration 26 : 3.118950584747633E56
Loss in iteration 27 : 1.1259411610938955E59
Loss in iteration 28 : 4.0646475915489623E61
Loss in iteration 29 : 1.4673377805491753E64
Loss in iteration 30 : 5.2970893877825225E66
Loss in iteration 31 : 1.9122492689894913E69
Loss in iteration 32 : 6.903219861052063E71
Loss in iteration 33 : 2.4920623698397946E74
Loss in iteration 34 : 8.996345155121659E76
Loss in iteration 35 : 3.247680600998919E79
Loss in iteration 36 : 1.1724126969606097E82
Loss in iteration 37 : 4.232409836027802E84
Loss in iteration 38 : 1.5278999508060363E87
Loss in iteration 39 : 5.515718822409792E89
Loss in iteration 40 : 1.9911744948899344E92
Loss in iteration 41 : 7.188139926552663E94
Loss in iteration 42 : 2.5949185134855117E97
Loss in iteration 43 : 9.367655833682696E99
Loss in iteration 44 : 3.3817237559594535E102
Loss in iteration 45 : 1.220802275901363E105
Loss in iteration 46 : 4.4070962160039205E107
Loss in iteration 47 : 1.5909617339774152E110
Loss in iteration 48 : 5.743371859658468E112
Loss in iteration 49 : 2.073357241336707E115
Loss in iteration 50 : 7.484819641225511E117
Loss in iteration 51 : 2.7020198904824096E120
Loss in iteration 52 : 9.754291804641499E122
Loss in iteration 53 : 3.521299341475581E125
Loss in iteration 54 : 1.2711890622726848E128
Loss in iteration 55 : 4.588992514804392E130
Loss in iteration 56 : 1.6566262978443855E133
Loss in iteration 57 : 5.980420935218234E135
Loss in iteration 58 : 2.1589319576137814E138
Loss in iteration 59 : 7.793744366985752E140
Loss in iteration 60 : 2.8135417164818563E143
Loss in iteration 61 : 1.0156885596499503E146
Loss in iteration 62 : 3.66663570033632E148
Loss in iteration 63 : 1.3236554878214114E151
Loss in iteration 64 : 4.778396311035295E153
Loss in iteration 65 : 1.725001068283741E156
Loss in iteration 66 : 6.227253856504307E158
Loss in iteration 67 : 2.248038642198055E161
Loss in iteration 68 : 8.115419498334977E163
Loss in iteration 69 : 2.929666438898926E166
Loss in iteration 70 : 1.0576095844425124E169
Loss in iteration 71 : 3.8179705998374704E171
Loss in iteration 72 : 1.378287386541327E174
Loss in iteration 73 : 4.9756174654141897E176
Loss in iteration 74 : 1.7961979050145224E179
Loss in iteration 75 : 6.4842744371024276E181
Loss in iteration 76 : 2.3408230717939758E184
Loss in iteration 77 : 8.450371289176253E186
Loss in iteration 78 : 3.050584035392627E189
Loss in iteration 79 : 1.1012608367767387E192
Loss in iteration 80 : 3.975551620764025E194
Loss in iteration 81 : 1.4351741350958136E197
Loss in iteration 82 : 5.180978627695887E199
Loss in iteration 83 : 1.870333284598215E202
Loss in iteration 84 : 6.751903157399557E204
Loss in iteration 85 : 2.4374370398212398E207
Loss in iteration 86 : 8.799147713754676E209
Loss in iteration 87 : 3.176492324665437E212
Loss in iteration 88 : 1.1467137292042231E215
Loss in iteration 89 : 4.1396365624272454E217
Loss in iteration 90 : 1.4944087990362352E220
Loss in iteration 91 : 5.3948157645208114E222
Loss in iteration 92 : 1.9475284909920123E225
Loss in iteration 93 : 7.030577852481165E227
Loss in iteration 94 : 2.5380386047457E230
Loss in iteration 95 : 9.162319363131978E232
Loss in iteration 96 : 3.307597290090644E235
Loss in iteration 97 : 1.1940426217227223E238
Loss in iteration 98 : 4.310493864419028E240
Loss in iteration 99 : 1.556088285055269E243
Loss in iteration 100 : 5.617478709049522E245
Loss in iteration 101 : 2.027909813966877E248
Loss in iteration 102 : 7.320754428420427E250
Loss in iteration 103 : 2.6427923486597746E253
Loss in iteration 104 : 9.540480378661784E255
Loss in iteration 105 : 3.4441134166969056E258
Loss in iteration 106 : 1.2433249434275826E261
Loss in iteration 107 : 4.488403045773573E263
Loss in iteration 108 : 1.62031349952426E266
Loss in iteration 109 : 5.849331733282578E268
Loss in iteration 110 : 2.111608755715011E271
Loss in iteration 111 : 7.62290760813119E273
Loss in iteration 112 : 2.751869646535359E276
Loss in iteration 113 : 9.934249423992645E278
Loss in iteration 114 : 3.586264042061346E281
Loss in iteration 115 : 1.294641319184146E284
Loss in iteration 116 : 4.673655162254766E286
Loss in iteration 117 : 1.6871895135739708E289
Loss in iteration 118 : 6.090754144002034E291
Loss in iteration 119 : 2.1987622459847344E294
Loss in iteration 120 : 7.93753170800489E296
Loss in iteration 121 : 2.865448946589765E299
Loss in iteration 122 : 1.034427069718905E302
Loss in iteration 123 : 3.7342817216852477E304
Loss in iteration 124 : 1.3480757015283744E307
Loss in iteration 125 : Infinity
Loss in iteration 126 : Infinity
Loss in iteration 127 : Infinity
Loss in iteration 128 : Infinity
Loss in iteration 129 : Infinity
Loss in iteration 130 : Infinity
Loss in iteration 131 : Infinity
Loss in iteration 132 : Infinity
Loss in iteration 133 : Infinity
Loss in iteration 134 : Infinity
Loss in iteration 135 : Infinity
Loss in iteration 136 : Infinity
Loss in iteration 137 : Infinity
Loss in iteration 138 : Infinity
Loss in iteration 139 : Infinity
Loss in iteration 140 : Infinity
Loss in iteration 141 : Infinity
Loss in iteration 142 : Infinity
Loss in iteration 143 : Infinity
Loss in iteration 144 : Infinity
Loss in iteration 145 : Infinity
Loss in iteration 146 : Infinity
Loss in iteration 147 : Infinity
Loss in iteration 148 : Infinity
Loss in iteration 149 : Infinity
Loss in iteration 150 : Infinity
Loss in iteration 151 : Infinity
Loss in iteration 152 : Infinity
Loss in iteration 153 : Infinity
Loss in iteration 154 : Infinity
Loss in iteration 155 : Infinity
Loss in iteration 156 : Infinity
Loss in iteration 157 : Infinity
Loss in iteration 158 : Infinity
Loss in iteration 159 : Infinity
Loss in iteration 160 : Infinity
Loss in iteration 161 : Infinity
Loss in iteration 162 : Infinity
Loss in iteration 163 : Infinity
Loss in iteration 164 : Infinity
Loss in iteration 165 : Infinity
Loss in iteration 166 : Infinity
Loss in iteration 167 : Infinity
Loss in iteration 168 : Infinity
Loss in iteration 169 : Infinity
Loss in iteration 170 : Infinity
Loss in iteration 171 : Infinity
Loss in iteration 172 : Infinity
Loss in iteration 173 : Infinity
Loss in iteration 174 : Infinity
Loss in iteration 175 : Infinity
Loss in iteration 176 : Infinity
Loss in iteration 177 : Infinity
Loss in iteration 178 : Infinity
Loss in iteration 179 : Infinity
Loss in iteration 180 : Infinity
Loss in iteration 181 : Infinity
Loss in iteration 182 : Infinity
Loss in iteration 183 : Infinity
Loss in iteration 184 : Infinity
Loss in iteration 185 : Infinity
Loss in iteration 186 : Infinity
Loss in iteration 187 : Infinity
Loss in iteration 188 : Infinity
Loss in iteration 189 : Infinity
Loss in iteration 190 : Infinity
Loss in iteration 191 : Infinity
Loss in iteration 192 : Infinity
Loss in iteration 193 : Infinity
Loss in iteration 194 : Infinity
Loss in iteration 195 : Infinity
Loss in iteration 196 : Infinity
Loss in iteration 197 : Infinity
Loss in iteration 198 : Infinity
Loss in iteration 199 : Infinity
Loss in iteration 200 : Infinity
Testing accuracy  of updater 4 on alg 1 with rate 100.0 = 0.5, training accuracy 0.3525412754936873, time elapsed: 3791 millisecond.
Loss in iteration 1 : 1.0096038163350318
Loss in iteration 2 : 7.439786315685599
Loss in iteration 3 : 4.561457651828125
Loss in iteration 4 : 1.8912307577104541
Loss in iteration 5 : 0.663057880829856
Loss in iteration 6 : 0.8101657422535691
Loss in iteration 7 : 0.9622902400567122
Loss in iteration 8 : 1.5184295156807728
Loss in iteration 9 : 0.8474395731236232
Loss in iteration 10 : 1.088152342812012
Loss in iteration 11 : 0.8792468211550998
Loss in iteration 12 : 1.1544137056270372
Loss in iteration 13 : 0.863443845234041
Loss in iteration 14 : 1.0915804775145856
Loss in iteration 15 : 0.8554836689637972
Loss in iteration 16 : 1.1086887948938118
Loss in iteration 17 : 0.8744240124952477
Loss in iteration 18 : 1.159975756125362
Loss in iteration 19 : 0.8827036503070025
Loss in iteration 20 : 1.183848652320055
Loss in iteration 21 : 0.8933102564441261
Loss in iteration 22 : 1.2055678984357567
Loss in iteration 23 : 0.8950776106547096
Loss in iteration 24 : 1.216819164940298
Loss in iteration 25 : 0.9083492523637097
Loss in iteration 26 : 1.2563795838320089
Loss in iteration 27 : 0.9170715636314132
Loss in iteration 28 : 1.2613402924884425
Loss in iteration 29 : 0.924793768456103
Loss in iteration 30 : 1.2741623327090865
Loss in iteration 31 : 0.9342877745320224
Loss in iteration 32 : 1.2850090599020443
Loss in iteration 33 : 0.9462919382472127
Loss in iteration 34 : 1.3068984750130712
Loss in iteration 35 : 0.9519214143548458
Loss in iteration 36 : 1.3141539408946108
Loss in iteration 37 : 0.9611167909330861
Loss in iteration 38 : 1.3242725776337878
Loss in iteration 39 : 0.9747128100238739
Loss in iteration 40 : 1.3536954870761284
Loss in iteration 41 : 0.9764234544950647
Loss in iteration 42 : 1.3256257289235198
Loss in iteration 43 : 0.9750850467241771
Loss in iteration 44 : 1.322578446336437
Loss in iteration 45 : 1.0025503516191707
Loss in iteration 46 : 1.422067405710995
Loss in iteration 47 : 1.0134010829988445
Loss in iteration 48 : 1.3996908668009633
Loss in iteration 49 : 1.004153703163254
Loss in iteration 50 : 1.3371768042049823
Loss in iteration 51 : 1.0162094926398288
Loss in iteration 52 : 1.4105757533945662
Loss in iteration 53 : 1.043299268561531
Loss in iteration 54 : 1.471123141091737
Loss in iteration 55 : 1.0277237338711864
Loss in iteration 56 : 1.3631621793214719
Loss in iteration 57 : 1.0292918204095063
Loss in iteration 58 : 1.3920970214024193
Loss in iteration 59 : 1.061637125777819
Loss in iteration 60 : 1.511870759327309
Loss in iteration 61 : 1.0618449039139812
Loss in iteration 62 : 1.4163516371258722
Loss in iteration 63 : 1.0510171636451018
Loss in iteration 64 : 1.4165855653275767
Loss in iteration 65 : 1.0741772378235939
Loss in iteration 66 : 1.499204751756305
Loss in iteration 67 : 1.0844600820549002
Loss in iteration 68 : 1.4784743062071646
Loss in iteration 69 : 1.0844784564521626
Loss in iteration 70 : 1.4431349264172386
Loss in iteration 71 : 1.0795455071526003
Loss in iteration 72 : 1.4528462651674974
Loss in iteration 73 : 1.1081632339098944
Loss in iteration 74 : 1.5600417565847104
Loss in iteration 75 : 1.1043260630665588
Loss in iteration 76 : 1.4601710762635642
Loss in iteration 77 : 1.0913216733915139
Loss in iteration 78 : 1.4551846267350685
Loss in iteration 79 : 1.115067957578595
Loss in iteration 80 : 1.5283561363403735
Loss in iteration 81 : 1.1433659907722453
Loss in iteration 82 : 1.5917729794861861
Loss in iteration 83 : 1.1162007307813984
Loss in iteration 84 : 1.4491747766294145
Loss in iteration 85 : 1.1078363616814808
Loss in iteration 86 : 1.471203957595903
Loss in iteration 87 : 1.1449899444397904
Loss in iteration 88 : 1.6116629088972476
Loss in iteration 89 : 1.1453671396146512
Loss in iteration 90 : 1.5212251920628574
Loss in iteration 91 : 1.1360239187049044
Loss in iteration 92 : 1.5051654113556845
Loss in iteration 93 : 1.1507545937756904
Loss in iteration 94 : 1.5495390984397663
Loss in iteration 95 : 1.1631328030194057
Loss in iteration 96 : 1.5738206882765384
Loss in iteration 97 : 1.1668376990899407
Loss in iteration 98 : 1.5509859519726026
Loss in iteration 99 : 1.1545105433152834
Loss in iteration 100 : 1.5186926169175925
Loss in iteration 101 : 1.1664293934229697
Loss in iteration 102 : 1.5641476475928506
Loss in iteration 103 : 1.180607848383142
Loss in iteration 104 : 1.5955353889313304
Loss in iteration 105 : 1.183007096721303
Loss in iteration 106 : 1.5699286223352122
Loss in iteration 107 : 1.1691945132498818
Loss in iteration 108 : 1.5295034539907664
Loss in iteration 109 : 1.1831823966953765
Loss in iteration 110 : 1.58963547733098
Loss in iteration 111 : 1.2006239813987538
Loss in iteration 112 : 1.6272576902862665
Loss in iteration 113 : 1.1881939033777522
Loss in iteration 114 : 1.547724761977887
Loss in iteration 115 : 1.179116786467353
Loss in iteration 116 : 1.5528953473961102
Loss in iteration 117 : 1.2033040845697873
Loss in iteration 118 : 1.6210719836772594
Loss in iteration 119 : 1.2117125150895283
Loss in iteration 120 : 1.610945011613674
Loss in iteration 121 : 1.199363208430365
Loss in iteration 122 : 1.5730688227584466
Loss in iteration 123 : 1.1959574192012952
Loss in iteration 124 : 1.581490107366713
Loss in iteration 125 : 1.204570514850474
Loss in iteration 126 : 1.609772004494412
Loss in iteration 127 : 1.228669169660743
Loss in iteration 128 : 1.6613251667031055
Loss in iteration 129 : 1.2125101316369404
Loss in iteration 130 : 1.5754360052168805
Loss in iteration 131 : 1.2024301648088023
Loss in iteration 132 : 1.5716018195071857
Loss in iteration 133 : 1.2256997963639364
Loss in iteration 134 : 1.6575157961164946
Loss in iteration 135 : 1.2367584096752609
Loss in iteration 136 : 1.6405727753893429
Loss in iteration 137 : 1.218424427840206
Loss in iteration 138 : 1.5764651462156176
Loss in iteration 139 : 1.2115017662535479
Loss in iteration 140 : 1.5916512498177857
Loss in iteration 141 : 1.2357676886033282
Loss in iteration 142 : 1.6681528092555546
Loss in iteration 143 : 1.2465264769975009
Loss in iteration 144 : 1.6507425364744523
Loss in iteration 145 : 1.2294526419622027
Loss in iteration 146 : 1.595184530884714
Loss in iteration 147 : 1.2231346922014519
Loss in iteration 148 : 1.6077036848741681
Loss in iteration 149 : 1.237095925237371
Loss in iteration 150 : 1.6526098258029998
Loss in iteration 151 : 1.2523115786338126
Loss in iteration 152 : 1.6640405053123133
Loss in iteration 153 : 1.2374844574529538
Loss in iteration 154 : 1.6131841934274465
Loss in iteration 155 : 1.230024849951973
Loss in iteration 156 : 1.611918409288821
Loss in iteration 157 : 1.2454494674805903
Loss in iteration 158 : 1.6658067744341252
Loss in iteration 159 : 1.2589115397925787
Loss in iteration 160 : 1.6713687512030722
Loss in iteration 161 : 1.243953972145643
Loss in iteration 162 : 1.613900166014664
Loss in iteration 163 : 1.2409549685061223
Loss in iteration 164 : 1.6277671774494902
Loss in iteration 165 : 1.2548475452595815
Loss in iteration 166 : 1.6745907511580786
Loss in iteration 167 : 1.2574504677597074
Loss in iteration 168 : 1.6557040439020825
Loss in iteration 169 : 1.2540659328404888
Loss in iteration 170 : 1.637719491724744
Loss in iteration 171 : 1.2467636524391437
Loss in iteration 172 : 1.6303431581756103
Loss in iteration 173 : 1.2568450611206754
Loss in iteration 174 : 1.668544913315313
Loss in iteration 175 : 1.2728950453013148
Loss in iteration 176 : 1.6827368738287996
Loss in iteration 177 : 1.2575430383133483
Loss in iteration 178 : 1.6267902558995986
Loss in iteration 179 : 1.2521683995174424
Loss in iteration 180 : 1.6340964038794064
Loss in iteration 181 : 1.261835124029869
Loss in iteration 182 : 1.6795010431407145
Loss in iteration 183 : 1.2777613838956836
Loss in iteration 184 : 1.6877636539751295
Loss in iteration 185 : 1.2622760331670404
Loss in iteration 186 : 1.628109607301747
Loss in iteration 187 : 1.253612580563695
Loss in iteration 188 : 1.6395243830583788
Loss in iteration 189 : 1.272268580929922
Loss in iteration 190 : 1.6994033079518938
Loss in iteration 191 : 1.2771600606102431
Loss in iteration 192 : 1.6812261911095423
Loss in iteration 193 : 1.2693605441798437
Loss in iteration 194 : 1.6397741923367186
Loss in iteration 195 : 1.2573335325584343
Loss in iteration 196 : 1.6393077037022552
Loss in iteration 197 : 1.271153162741892
Loss in iteration 198 : 1.6887155006417949
Loss in iteration 199 : 1.2866942342256578
Loss in iteration 200 : 1.7009606741836454
Testing accuracy  of updater 5 on alg 1 with rate 0.049 = 0.78725, training accuracy 0.7827775979281321, time elapsed: 3316 millisecond.
Loss in iteration 1 : 1.0029177586202107
Loss in iteration 2 : 4.112062142968089
Loss in iteration 3 : 2.6130046564891094
Loss in iteration 4 : 1.1423966335757372
Loss in iteration 5 : 0.5281154671456569
Loss in iteration 6 : 0.5855092244243209
Loss in iteration 7 : 0.6323920246802179
Loss in iteration 8 : 0.8526661032028915
Loss in iteration 9 : 0.726374875672819
Loss in iteration 10 : 0.952224916489972
Loss in iteration 11 : 0.6303819473204575
Loss in iteration 12 : 0.7246092207849588
Loss in iteration 13 : 0.6439698643911084
Loss in iteration 14 : 0.7872582669328292
Loss in iteration 15 : 0.6735646036117681
Loss in iteration 16 : 0.8633659778693074
Loss in iteration 17 : 0.6724818433444035
Loss in iteration 18 : 0.8212631260173145
Loss in iteration 19 : 0.6759544929950443
Loss in iteration 20 : 0.8372474117660862
Loss in iteration 21 : 0.6905195565548883
Loss in iteration 22 : 0.8713781882658689
Loss in iteration 23 : 0.692121098813057
Loss in iteration 24 : 0.8739446400017626
Loss in iteration 25 : 0.7003169093718008
Loss in iteration 26 : 0.8972609963516468
Loss in iteration 27 : 0.7045390109680265
Loss in iteration 28 : 0.9079638322470464
Loss in iteration 29 : 0.7073710286951264
Loss in iteration 30 : 0.9067989044286373
Loss in iteration 31 : 0.7176193725441189
Loss in iteration 32 : 0.938294888942862
Loss in iteration 33 : 0.7215565860670072
Loss in iteration 34 : 0.9370764669283218
Loss in iteration 35 : 0.7275178401032675
Loss in iteration 36 : 0.9411747482248134
Loss in iteration 37 : 0.7287314473606554
Loss in iteration 38 : 0.9449781430020034
Loss in iteration 39 : 0.7375373784270906
Loss in iteration 40 : 0.9663774682700308
Loss in iteration 41 : 0.7429284626442861
Loss in iteration 42 : 0.9688320017232832
Loss in iteration 43 : 0.750738249895822
Loss in iteration 44 : 0.9871573634390263
Loss in iteration 45 : 0.7548955431800024
Loss in iteration 46 : 0.9757706582233929
Loss in iteration 47 : 0.7626469675434937
Loss in iteration 48 : 0.9923735782379915
Loss in iteration 49 : 0.7714748829686562
Loss in iteration 50 : 1.010781045269616
Loss in iteration 51 : 0.7656678704047101
Loss in iteration 52 : 0.9743227803390518
Loss in iteration 53 : 0.780606284287132
Loss in iteration 54 : 1.029037399702291
Loss in iteration 55 : 0.7836294728558879
Loss in iteration 56 : 1.0116660540357392
Loss in iteration 57 : 0.7883557839888009
Loss in iteration 58 : 1.0135652957375698
Loss in iteration 59 : 0.7995463564939345
Loss in iteration 60 : 1.052516110346144
Loss in iteration 61 : 0.7959311075167572
Loss in iteration 62 : 1.0100245514858246
Loss in iteration 63 : 0.8044178067613699
Loss in iteration 64 : 1.0412111888744748
Loss in iteration 65 : 0.812384694422621
Loss in iteration 66 : 1.057087955696252
Loss in iteration 67 : 0.8153317986455898
Loss in iteration 68 : 1.04019577307694
Loss in iteration 69 : 0.8203082464393017
Loss in iteration 70 : 1.0528385749151516
Loss in iteration 71 : 0.8245648704276554
Loss in iteration 72 : 1.0593632672787254
Loss in iteration 73 : 0.8286329819862895
Loss in iteration 74 : 1.0589358947639393
Loss in iteration 75 : 0.8376638696795049
Loss in iteration 76 : 1.08750980682175
Loss in iteration 77 : 0.8367976889099363
Loss in iteration 78 : 1.0631866468038607
Loss in iteration 79 : 0.8428487147724841
Loss in iteration 80 : 1.0737984818026025
Loss in iteration 81 : 0.8468743942946187
Loss in iteration 82 : 1.0779604631295878
Loss in iteration 83 : 0.8533702843713673
Loss in iteration 84 : 1.096136098804061
Loss in iteration 85 : 0.8578308777807004
Loss in iteration 86 : 1.0916839274874879
Loss in iteration 87 : 0.85720724105814
Loss in iteration 88 : 1.0907875205327542
Loss in iteration 89 : 0.8687686316837161
Loss in iteration 90 : 1.1270416233720537
Loss in iteration 91 : 0.8675428916302543
Loss in iteration 92 : 1.0903247841820074
Loss in iteration 93 : 0.8661112518294117
Loss in iteration 94 : 1.0984979663321193
Loss in iteration 95 : 0.8778046902267158
Loss in iteration 96 : 1.132528506568019
Loss in iteration 97 : 0.8843278741552034
Loss in iteration 98 : 1.1355134407998853
Loss in iteration 99 : 0.8828487502012579
Loss in iteration 100 : 1.107141544817194
Loss in iteration 101 : 0.8836078829871281
Loss in iteration 102 : 1.1157786594110939
Loss in iteration 103 : 0.8936117655680856
Loss in iteration 104 : 1.1518233152457318
Loss in iteration 105 : 0.8987386245630553
Loss in iteration 106 : 1.1420368796981417
Loss in iteration 107 : 0.8989260201346144
Loss in iteration 108 : 1.1243915477085105
Loss in iteration 109 : 0.9003697090170244
Loss in iteration 110 : 1.135959172539899
Loss in iteration 111 : 0.9088170459310592
Loss in iteration 112 : 1.1679589441233973
Loss in iteration 113 : 0.9110545026659636
Loss in iteration 114 : 1.1436415753065203
Loss in iteration 115 : 0.9120678034606885
Loss in iteration 116 : 1.1515064155605375
Loss in iteration 117 : 0.9201694293500033
Loss in iteration 118 : 1.174761189049946
Loss in iteration 119 : 0.9184833210314999
Loss in iteration 120 : 1.1519632292843065
Loss in iteration 121 : 0.9239687061246331
Loss in iteration 122 : 1.1712329154714007
Loss in iteration 123 : 0.9283644230341686
Loss in iteration 124 : 1.179268438001282
Loss in iteration 125 : 0.9335212552329959
Loss in iteration 126 : 1.1816140528875443
Loss in iteration 127 : 0.9226800267800899
Loss in iteration 128 : 1.1550916235467978
Loss in iteration 129 : 0.9400250638571859
Loss in iteration 130 : 1.1982220641617904
Loss in iteration 131 : 0.9391528887529058
Loss in iteration 132 : 1.1835060455272186
Loss in iteration 133 : 0.937780231706698
Loss in iteration 134 : 1.17254921691534
Loss in iteration 135 : 0.9476972090108922
Loss in iteration 136 : 1.202534978773505
Loss in iteration 137 : 0.9531704517431878
Loss in iteration 138 : 1.2038809536732193
Loss in iteration 139 : 0.9424104585367798
Loss in iteration 140 : 1.1684041878774019
Loss in iteration 141 : 0.960924602001278
Loss in iteration 142 : 1.225092059812899
Loss in iteration 143 : 0.9615095224573651
Loss in iteration 144 : 1.2100232671488864
Loss in iteration 145 : 0.9459527033466915
Loss in iteration 146 : 1.1658969805420425
Loss in iteration 147 : 0.9678640727476062
Loss in iteration 148 : 1.2303468576845327
Loss in iteration 149 : 0.9705507600118259
Loss in iteration 150 : 1.219291946830489
Loss in iteration 151 : 0.9577050972412058
Loss in iteration 152 : 1.1833374460450423
Loss in iteration 153 : 0.9762745289746377
Loss in iteration 154 : 1.2425434565191633
Loss in iteration 155 : 0.9758159921827265
Loss in iteration 156 : 1.2177612746534503
Loss in iteration 157 : 0.9652966908183859
Loss in iteration 158 : 1.189479704819675
Loss in iteration 159 : 0.9791894685789871
Loss in iteration 160 : 1.2420541176756967
Loss in iteration 161 : 0.9845781862074253
Loss in iteration 162 : 1.2375940747634222
Loss in iteration 163 : 0.9730036880977617
Loss in iteration 164 : 1.1979973720310724
Loss in iteration 165 : 0.9864637477427368
Loss in iteration 166 : 1.2497228261093258
Loss in iteration 167 : 0.9911212657562436
Loss in iteration 168 : 1.244246752594352
Loss in iteration 169 : 0.9767508453102804
Loss in iteration 170 : 1.2004509735645066
Loss in iteration 171 : 0.9945523013825178
Loss in iteration 172 : 1.261686062005492
Loss in iteration 173 : 0.9987419059260343
Loss in iteration 174 : 1.2500330093472674
Loss in iteration 175 : 0.9821894875778123
Loss in iteration 176 : 1.2061362294946485
Loss in iteration 177 : 0.9949625361668801
Loss in iteration 178 : 1.2512428028260336
Loss in iteration 179 : 1.0003467788692821
Loss in iteration 180 : 1.2599067933408237
Loss in iteration 181 : 0.998135266337083
Loss in iteration 182 : 1.2382513666243113
Loss in iteration 183 : 0.9985152561679589
Loss in iteration 184 : 1.2390230091550567
Loss in iteration 185 : 1.0048871336062655
Loss in iteration 186 : 1.2596363224030258
Loss in iteration 187 : 1.00269831546631
Loss in iteration 188 : 1.2444858012164037
Loss in iteration 189 : 1.0051257774980626
Loss in iteration 190 : 1.2568764249845836
Loss in iteration 191 : 1.002937864551579
Loss in iteration 192 : 1.2407568390879764
Loss in iteration 193 : 1.0118550518322875
Loss in iteration 194 : 1.2765869109436037
Loss in iteration 195 : 1.008605220094538
Loss in iteration 196 : 1.2443043426234994
Loss in iteration 197 : 1.0112460805718178
Loss in iteration 198 : 1.2655974518833009
Loss in iteration 199 : 1.0106758977721848
Loss in iteration 200 : 1.2487995907976335
Testing accuracy  of updater 5 on alg 1 with rate 0.0343 = 0.78675, training accuracy 0.7863386209129168, time elapsed: 3063 millisecond.
Loss in iteration 1 : 1.0009799565263657
Loss in iteration 2 : 2.5333769864085407
Loss in iteration 3 : 1.6674913777678904
Loss in iteration 4 : 0.8175684162688348
Loss in iteration 5 : 0.44718347989717216
Loss in iteration 6 : 0.5379085798882621
Loss in iteration 7 : 0.6019472247009728
Loss in iteration 8 : 0.717246619017082
Loss in iteration 9 : 0.4820932121046966
Loss in iteration 10 : 0.5230542095404678
Loss in iteration 11 : 0.49903256848590327
Loss in iteration 12 : 0.5580525454653358
Loss in iteration 13 : 0.5171461842292606
Loss in iteration 14 : 0.5775317960344041
Loss in iteration 15 : 0.5175317935056893
Loss in iteration 16 : 0.570793017093949
Loss in iteration 17 : 0.5091319798334614
Loss in iteration 18 : 0.5669671021175432
Loss in iteration 19 : 0.5221874608261987
Loss in iteration 20 : 0.5902268741104585
Loss in iteration 21 : 0.523688021655894
Loss in iteration 22 : 0.5892116359166015
Loss in iteration 23 : 0.5283625334473729
Loss in iteration 24 : 0.5977924242880718
Loss in iteration 25 : 0.5303907673513251
Loss in iteration 26 : 0.6019858416537147
Loss in iteration 27 : 0.5330768512774658
Loss in iteration 28 : 0.6098016315182824
Loss in iteration 29 : 0.5345090173865117
Loss in iteration 30 : 0.610176729897277
Loss in iteration 31 : 0.5447280141905003
Loss in iteration 32 : 0.6316414932139518
Loss in iteration 33 : 0.5469537521024979
Loss in iteration 34 : 0.630579013008556
Loss in iteration 35 : 0.5473620753169495
Loss in iteration 36 : 0.6298606351526771
Loss in iteration 37 : 0.5451976611182288
Loss in iteration 38 : 0.6265921647971506
Loss in iteration 39 : 0.5523641187940852
Loss in iteration 40 : 0.6475765592000962
Loss in iteration 41 : 0.5632514851573127
Loss in iteration 42 : 0.6628652847953728
Loss in iteration 43 : 0.5615045952536698
Loss in iteration 44 : 0.6427533295785698
Loss in iteration 45 : 0.558879953737398
Loss in iteration 46 : 0.6451285922915349
Loss in iteration 47 : 0.5673591562252688
Loss in iteration 48 : 0.6632975199313049
Loss in iteration 49 : 0.5704764256922666
Loss in iteration 50 : 0.6679728428687824
Loss in iteration 51 : 0.5709867967524488
Loss in iteration 52 : 0.665428336276742
Loss in iteration 53 : 0.5747635641209388
Loss in iteration 54 : 0.6728564730204317
Loss in iteration 55 : 0.5766707603476826
Loss in iteration 56 : 0.6708911901063738
Loss in iteration 57 : 0.5805840467166425
Loss in iteration 58 : 0.6793288523720069
Loss in iteration 59 : 0.5858748026646772
Loss in iteration 60 : 0.6879547029405513
Loss in iteration 61 : 0.5854575795821121
Loss in iteration 62 : 0.6760744811715631
Loss in iteration 63 : 0.5894895631825873
Loss in iteration 64 : 0.6878580320320854
Loss in iteration 65 : 0.592894365286905
Loss in iteration 66 : 0.6884232090199756
Loss in iteration 67 : 0.5969743041233216
Loss in iteration 68 : 0.6967895501825587
Loss in iteration 69 : 0.5956697462983714
Loss in iteration 70 : 0.6872481208621
Loss in iteration 71 : 0.6030015934895608
Loss in iteration 72 : 0.7070433172382738
Loss in iteration 73 : 0.6009317864176115
Loss in iteration 74 : 0.6969035160144221
Loss in iteration 75 : 0.6088575088758302
Loss in iteration 76 : 0.7094161054010154
Loss in iteration 77 : 0.6074676745914496
Loss in iteration 78 : 0.7043469418801963
Loss in iteration 79 : 0.6133785649826258
Loss in iteration 80 : 0.7086468574870101
Loss in iteration 81 : 0.6151587095287219
Loss in iteration 82 : 0.7115133507061902
Loss in iteration 83 : 0.6184470086724292
Loss in iteration 84 : 0.7168969532006663
Loss in iteration 85 : 0.6194604948486944
Loss in iteration 86 : 0.7177032330975279
Loss in iteration 87 : 0.6236003107648608
Loss in iteration 88 : 0.7236999180043368
Loss in iteration 89 : 0.6241406357047835
Loss in iteration 90 : 0.720521118040755
Loss in iteration 91 : 0.6267842184915028
Loss in iteration 92 : 0.7262491200862933
Loss in iteration 93 : 0.6330049951981126
Loss in iteration 94 : 0.736379295217493
Loss in iteration 95 : 0.6306789672610555
Loss in iteration 96 : 0.7268854871437569
Loss in iteration 97 : 0.6358741636004444
Loss in iteration 98 : 0.7401199122639733
Loss in iteration 99 : 0.6385248435940402
Loss in iteration 100 : 0.7420377910023551
Loss in iteration 101 : 0.6402630328079449
Loss in iteration 102 : 0.7410604310042989
Loss in iteration 103 : 0.6392282319746746
Loss in iteration 104 : 0.7328901282596748
Loss in iteration 105 : 0.6474475695505023
Loss in iteration 106 : 0.7555641886946636
Loss in iteration 107 : 0.6465340364356034
Loss in iteration 108 : 0.7469012822290143
Loss in iteration 109 : 0.6517916188163396
Loss in iteration 110 : 0.7606755329300279
Loss in iteration 111 : 0.6508712023191804
Loss in iteration 112 : 0.7487901975314557
Loss in iteration 113 : 0.6522530155021292
Loss in iteration 114 : 0.7566955120429788
Loss in iteration 115 : 0.6589174421470876
Loss in iteration 116 : 0.7674736060041
Loss in iteration 117 : 0.6586635679347513
Loss in iteration 118 : 0.7618703918390601
Loss in iteration 119 : 0.6622838585704924
Loss in iteration 120 : 0.7667287128601046
Loss in iteration 121 : 0.666076902584172
Loss in iteration 122 : 0.767442161044259
Loss in iteration 123 : 0.6661052358500851
Loss in iteration 124 : 0.7689874253135788
Loss in iteration 125 : 0.6697365613009728
Loss in iteration 126 : 0.7737726905984954
Loss in iteration 127 : 0.670394937055343
Loss in iteration 128 : 0.7722967742376341
Loss in iteration 129 : 0.6700638454475489
Loss in iteration 130 : 0.7754124907210326
Loss in iteration 131 : 0.6776571248941161
Loss in iteration 132 : 0.7850979420310008
Loss in iteration 133 : 0.6782126983570225
Loss in iteration 134 : 0.7798352891134612
Loss in iteration 135 : 0.6762821567028791
Loss in iteration 136 : 0.7814017919087478
Loss in iteration 137 : 0.6809827814351428
Loss in iteration 138 : 0.7881538380056832
Loss in iteration 139 : 0.6883757157846458
Loss in iteration 140 : 0.8051347323577753
Loss in iteration 141 : 0.6829409041935495
Loss in iteration 142 : 0.7840149245154191
Loss in iteration 143 : 0.6861315106775996
Loss in iteration 144 : 0.7901829500353236
Loss in iteration 145 : 0.6862672107503255
Loss in iteration 146 : 0.7923441664523351
Loss in iteration 147 : 0.6915442130998277
Loss in iteration 148 : 0.800887031410733
Loss in iteration 149 : 0.6967737363185502
Loss in iteration 150 : 0.8136853867394875
Loss in iteration 151 : 0.6953276779329343
Loss in iteration 152 : 0.7982059414610831
Loss in iteration 153 : 0.6936024363391168
Loss in iteration 154 : 0.7974182907363102
Loss in iteration 155 : 0.7002056707320552
Loss in iteration 156 : 0.8099232843119215
Loss in iteration 157 : 0.7066446150319053
Loss in iteration 158 : 0.824003206884691
Loss in iteration 159 : 0.6984412226793767
Loss in iteration 160 : 0.7961591060878748
Loss in iteration 161 : 0.7024634553226005
Loss in iteration 162 : 0.8079980813387422
Loss in iteration 163 : 0.7053411722929981
Loss in iteration 164 : 0.8145969191182026
Loss in iteration 165 : 0.7126895232461273
Loss in iteration 166 : 0.831072228428632
Loss in iteration 167 : 0.7084656797427463
Loss in iteration 168 : 0.8080684438634351
Loss in iteration 169 : 0.7104008850157957
Loss in iteration 170 : 0.8153956957992806
Loss in iteration 171 : 0.7127745403571629
Loss in iteration 172 : 0.8222792129000033
Loss in iteration 173 : 0.7206034625739819
Loss in iteration 174 : 0.8411523375509062
Loss in iteration 175 : 0.7135030287718267
Loss in iteration 176 : 0.811974940914694
Loss in iteration 177 : 0.7144841292500284
Loss in iteration 178 : 0.818554077751931
Loss in iteration 179 : 0.7224504429882248
Loss in iteration 180 : 0.8427289600369408
Loss in iteration 181 : 0.7240786893594628
Loss in iteration 182 : 0.8315670050144743
Loss in iteration 183 : 0.7220856369915749
Loss in iteration 184 : 0.8213915441947317
Loss in iteration 185 : 0.7239061793993982
Loss in iteration 186 : 0.8312229282658332
Loss in iteration 187 : 0.7303323644196252
Loss in iteration 188 : 0.8485246513730583
Loss in iteration 189 : 0.729311942327545
Loss in iteration 190 : 0.8325783260987046
Loss in iteration 191 : 0.7289450105494905
Loss in iteration 192 : 0.8310670559252752
Loss in iteration 193 : 0.7294465189144173
Loss in iteration 194 : 0.8317288198633594
Loss in iteration 195 : 0.7366753653190858
Loss in iteration 196 : 0.854391324175792
Loss in iteration 197 : 0.7349626485168411
Loss in iteration 198 : 0.8410513722698214
Loss in iteration 199 : 0.7345905864782932
Loss in iteration 200 : 0.8378881543305392
Testing accuracy  of updater 5 on alg 1 with rate 0.0196 = 0.7895, training accuracy 0.7983166073162836, time elapsed: 3119 millisecond.
Loss in iteration 1 : 1.0000640025595073
Loss in iteration 2 : 0.9111328727797463
Loss in iteration 3 : 0.6901065859158994
Loss in iteration 4 : 0.5018925306236659
Loss in iteration 5 : 0.48618532161920214
Loss in iteration 6 : 0.47146202184047636
Loss in iteration 7 : 0.4590998447739486
Loss in iteration 8 : 0.4466150160944358
Loss in iteration 9 : 0.4349388052809335
Loss in iteration 10 : 0.4236654437577704
Loss in iteration 11 : 0.41400865148016985
Loss in iteration 12 : 0.4058573381376505
Loss in iteration 13 : 0.399905342375388
Loss in iteration 14 : 0.3956295555218151
Loss in iteration 15 : 0.3931466034948817
Loss in iteration 16 : 0.3915716058214477
Loss in iteration 17 : 0.3915679455526733
Loss in iteration 18 : 0.3945445606009321
Loss in iteration 19 : 0.4001926766491717
Loss in iteration 20 : 0.40869817812101605
Loss in iteration 21 : 0.41008720210561833
Loss in iteration 22 : 0.4106468314945392
Loss in iteration 23 : 0.40105495022819043
Loss in iteration 24 : 0.3981867141854055
Loss in iteration 25 : 0.3901837653511977
Loss in iteration 26 : 0.3886328943842813
Loss in iteration 27 : 0.3850854156205783
Loss in iteration 28 : 0.38459363163974175
Loss in iteration 29 : 0.38365108350167776
Loss in iteration 30 : 0.3851723245346069
Loss in iteration 31 : 0.3862371596718262
Loss in iteration 32 : 0.3915086735165251
Loss in iteration 33 : 0.3947842544797368
Loss in iteration 34 : 0.4047747690440994
Loss in iteration 35 : 0.40371873619948623
Loss in iteration 36 : 0.4142798097495647
Loss in iteration 37 : 0.4011961574247127
Loss in iteration 38 : 0.40497937962618413
Loss in iteration 39 : 0.39435826327205364
Loss in iteration 40 : 0.39452810816210077
Loss in iteration 41 : 0.38766518766633984
Loss in iteration 42 : 0.3908338724552212
Loss in iteration 43 : 0.3872330507340542
Loss in iteration 44 : 0.39192762619818405
Loss in iteration 45 : 0.3905026351249588
Loss in iteration 46 : 0.40104739289852354
Loss in iteration 47 : 0.4020060430485213
Loss in iteration 48 : 0.41273002969647565
Loss in iteration 49 : 0.40416652915953405
Loss in iteration 50 : 0.4097172984376121
Loss in iteration 51 : 0.39896436365288035
Loss in iteration 52 : 0.3999597294339355
Loss in iteration 53 : 0.39216377362237587
Loss in iteration 54 : 0.39624767893631063
Loss in iteration 55 : 0.3903824794979894
Loss in iteration 56 : 0.3938779623717597
Loss in iteration 57 : 0.3919826032178488
Loss in iteration 58 : 0.4003030708008274
Loss in iteration 59 : 0.39942131765037475
Loss in iteration 60 : 0.4103756349698301
Loss in iteration 61 : 0.4076788303903875
Loss in iteration 62 : 0.41595095624948036
Loss in iteration 63 : 0.40436663366298214
Loss in iteration 64 : 0.4057255634389615
Loss in iteration 65 : 0.3971599451244556
Loss in iteration 66 : 0.3999116584641326
Loss in iteration 67 : 0.39340451578131974
Loss in iteration 68 : 0.3955875725393306
Loss in iteration 69 : 0.3916059838147791
Loss in iteration 70 : 0.3945943435753046
Loss in iteration 71 : 0.393373365934574
Loss in iteration 72 : 0.4006063264521277
Loss in iteration 73 : 0.40186847834618555
Loss in iteration 74 : 0.41885019908754506
Loss in iteration 75 : 0.4164143574723691
Loss in iteration 76 : 0.4265697721090477
Loss in iteration 77 : 0.4071918494732686
Loss in iteration 78 : 0.40753066795893145
Loss in iteration 79 : 0.3980607755646304
Loss in iteration 80 : 0.3990969003131184
Loss in iteration 81 : 0.3923730831536544
Loss in iteration 82 : 0.3935870534129327
Loss in iteration 83 : 0.3942015643165609
Loss in iteration 84 : 0.39884098429395276
Loss in iteration 85 : 0.39919492878328056
Loss in iteration 86 : 0.4111524883090334
Loss in iteration 87 : 0.4148126760248377
Loss in iteration 88 : 0.4308859357663977
Loss in iteration 89 : 0.4155598624937036
Loss in iteration 90 : 0.4190374116705817
Loss in iteration 91 : 0.40592806294988126
Loss in iteration 92 : 0.40693017218107563
Loss in iteration 93 : 0.3992160915276039
Loss in iteration 94 : 0.3996030259885208
Loss in iteration 95 : 0.3948717091336834
Loss in iteration 96 : 0.39678243542341
Loss in iteration 97 : 0.39788033989979926
Loss in iteration 98 : 0.4052741241635427
Loss in iteration 99 : 0.4064268143544024
Loss in iteration 100 : 0.4236447534381596
Loss in iteration 101 : 0.42128731191834623
Loss in iteration 102 : 0.43235020089455867
Loss in iteration 103 : 0.4129484169561869
Loss in iteration 104 : 0.41579891980240835
Loss in iteration 105 : 0.4054938487546742
Loss in iteration 106 : 0.4053546793179647
Loss in iteration 107 : 0.39817519505911203
Loss in iteration 108 : 0.39974402982678836
Loss in iteration 109 : 0.3989857369954529
Loss in iteration 110 : 0.40272607065990473
Loss in iteration 111 : 0.4034094922184962
Loss in iteration 112 : 0.4150201005992606
Loss in iteration 113 : 0.4193994537732757
Loss in iteration 114 : 0.4385773184308169
Loss in iteration 115 : 0.42362700148420057
Loss in iteration 116 : 0.43048376139094846
Loss in iteration 117 : 0.4139206293272256
Loss in iteration 118 : 0.41459337879387287
Loss in iteration 119 : 0.40461374329590805
Loss in iteration 120 : 0.4043240473037169
Loss in iteration 121 : 0.4012227666196095
Loss in iteration 122 : 0.40480808621285463
Loss in iteration 123 : 0.40438437900667856
Loss in iteration 124 : 0.41178628849859283
Loss in iteration 125 : 0.4121411140693797
Loss in iteration 126 : 0.4263836581775124
Loss in iteration 127 : 0.42855963530663854
Loss in iteration 128 : 0.443164747795596
Loss in iteration 129 : 0.4238386633575268
Loss in iteration 130 : 0.42730847833474356
Loss in iteration 131 : 0.41364207228766675
Loss in iteration 132 : 0.4142669158010961
Loss in iteration 133 : 0.40599112483572697
Loss in iteration 134 : 0.4083950823039201
Loss in iteration 135 : 0.40662369769959883
Loss in iteration 136 : 0.4108913645098923
Loss in iteration 137 : 0.41074810030872294
Loss in iteration 138 : 0.41930919575176456
Loss in iteration 139 : 0.4225557067886186
Loss in iteration 140 : 0.44176432271333277
Loss in iteration 141 : 0.4326255035294796
Loss in iteration 142 : 0.4402921636936071
Loss in iteration 143 : 0.42281452190202895
Loss in iteration 144 : 0.424518032732876
Loss in iteration 145 : 0.41522230978751845
Loss in iteration 146 : 0.4153140114947819
Loss in iteration 147 : 0.41026456053797383
Loss in iteration 148 : 0.41441721823335453
Loss in iteration 149 : 0.4125414393872924
Loss in iteration 150 : 0.41947729656215865
Loss in iteration 151 : 0.41986201669398804
Loss in iteration 152 : 0.4329619014730724
Loss in iteration 153 : 0.43314227410928074
Loss in iteration 154 : 0.4485442405911637
Loss in iteration 155 : 0.4322342242348617
Loss in iteration 156 : 0.43715556601128175
Loss in iteration 157 : 0.42354873052280956
Loss in iteration 158 : 0.42257126969959136
Loss in iteration 159 : 0.4148320293326284
Loss in iteration 160 : 0.4173793691562353
Loss in iteration 161 : 0.4160024265303729
Loss in iteration 162 : 0.4225964389227283
Loss in iteration 163 : 0.4220264978363372
Loss in iteration 164 : 0.43271477220460264
Loss in iteration 165 : 0.43356021748429435
Loss in iteration 166 : 0.44947139856735996
Loss in iteration 167 : 0.43844873750686686
Loss in iteration 168 : 0.4452470020520708
Loss in iteration 169 : 0.4297450831679558
Loss in iteration 170 : 0.4290680079856276
Loss in iteration 171 : 0.42058681408043064
Loss in iteration 172 : 0.42336155436528666
Loss in iteration 173 : 0.4205238286440327
Loss in iteration 174 : 0.42642260243081087
Loss in iteration 175 : 0.42447982237649107
Loss in iteration 176 : 0.43411658239263695
Loss in iteration 177 : 0.43670978906307284
Loss in iteration 178 : 0.4531538625999613
Loss in iteration 179 : 0.4441039250933349
Loss in iteration 180 : 0.4523445918148598
Loss in iteration 181 : 0.43430205044986525
Loss in iteration 182 : 0.4330960580238288
Loss in iteration 183 : 0.42420282342483956
Loss in iteration 184 : 0.4269516156564757
Loss in iteration 185 : 0.4254595554050479
Loss in iteration 186 : 0.43249928289338135
Loss in iteration 187 : 0.43109493769023427
Loss in iteration 188 : 0.4388574354045714
Loss in iteration 189 : 0.43937642827200246
Loss in iteration 190 : 0.45359029630365477
Loss in iteration 191 : 0.4449784629123363
Loss in iteration 192 : 0.453940012776305
Loss in iteration 193 : 0.43899573671199754
Loss in iteration 194 : 0.4395718629273729
Loss in iteration 195 : 0.4322457302982952
Loss in iteration 196 : 0.436416495249175
Loss in iteration 197 : 0.4318462355714208
Loss in iteration 198 : 0.43837979784155284
Loss in iteration 199 : 0.4355045388028879
Loss in iteration 200 : 0.442656838313482
Testing accuracy  of updater 5 on alg 1 with rate 0.0049 = 0.79525, training accuracy 0.8313370022661055, time elapsed: 3841 millisecond.
Loss in iteration 1 : 1.000038148838701
Loss in iteration 2 : 0.8041444663218748
Loss in iteration 3 : 0.6247692430503083
Loss in iteration 4 : 0.512541667960391
Loss in iteration 5 : 0.5043996566747486
Loss in iteration 6 : 0.49611381868067655
Loss in iteration 7 : 0.4876138437335467
Loss in iteration 8 : 0.47881249156242217
Loss in iteration 9 : 0.4696898548920308
Loss in iteration 10 : 0.4602066172069476
Loss in iteration 11 : 0.4503415726633128
Loss in iteration 12 : 0.4401333508732883
Loss in iteration 13 : 0.4297207257002396
Loss in iteration 14 : 0.42017764900412635
Loss in iteration 15 : 0.41185200036690695
Loss in iteration 16 : 0.4048928810672671
Loss in iteration 17 : 0.3993816204434842
Loss in iteration 18 : 0.3951761737050915
Loss in iteration 19 : 0.39188096316606136
Loss in iteration 20 : 0.38943017043170935
Loss in iteration 21 : 0.38747321101098947
Loss in iteration 22 : 0.3857735550311992
Loss in iteration 23 : 0.3843422440731987
Loss in iteration 24 : 0.3831026731312981
Loss in iteration 25 : 0.38224165870593096
Loss in iteration 26 : 0.38229304613005793
Loss in iteration 27 : 0.3833001543879231
Loss in iteration 28 : 0.38722864190816697
Loss in iteration 29 : 0.3966770894236492
Loss in iteration 30 : 0.4084947136554937
Loss in iteration 31 : 0.4142910315185472
Loss in iteration 32 : 0.39556029009426547
Loss in iteration 33 : 0.38950076678716805
Loss in iteration 34 : 0.3816902491968071
Loss in iteration 35 : 0.38040642138807834
Loss in iteration 36 : 0.37894475752212364
Loss in iteration 37 : 0.37846491112763636
Loss in iteration 38 : 0.37838793616582533
Loss in iteration 39 : 0.3787820472105866
Loss in iteration 40 : 0.37997976559217245
Loss in iteration 41 : 0.3813022769268971
Loss in iteration 42 : 0.38244497157563173
Loss in iteration 43 : 0.386613434336998
Loss in iteration 44 : 0.3893507835844197
Loss in iteration 45 : 0.39563999334604977
Loss in iteration 46 : 0.3909833377393712
Loss in iteration 47 : 0.3933701165352451
Loss in iteration 48 : 0.38690663005446174
Loss in iteration 49 : 0.38921986349769805
Loss in iteration 50 : 0.3855876423422005
Loss in iteration 51 : 0.3869644718516707
Loss in iteration 52 : 0.3840739328773593
Loss in iteration 53 : 0.38453692093313036
Loss in iteration 54 : 0.3841650850237267
Loss in iteration 55 : 0.3856984017506326
Loss in iteration 56 : 0.38548171064304454
Loss in iteration 57 : 0.38872897317879707
Loss in iteration 58 : 0.3872973794761334
Loss in iteration 59 : 0.39126270058000107
Loss in iteration 60 : 0.3865356683440829
Loss in iteration 61 : 0.3888898274806766
Loss in iteration 62 : 0.3862071884268123
Loss in iteration 63 : 0.38828769045291717
Loss in iteration 64 : 0.38673980420318543
Loss in iteration 65 : 0.3894973271305811
Loss in iteration 66 : 0.3868427812422601
Loss in iteration 67 : 0.3895370521252723
Loss in iteration 68 : 0.3870086766542761
Loss in iteration 69 : 0.38944906783648486
Loss in iteration 70 : 0.38728483622417065
Loss in iteration 71 : 0.3897765210773594
Loss in iteration 72 : 0.3876042916363623
Loss in iteration 73 : 0.39003064024581263
Loss in iteration 74 : 0.3877727397517424
Loss in iteration 75 : 0.39002660830231584
Loss in iteration 76 : 0.38785552475397406
Loss in iteration 77 : 0.38968760059972335
Loss in iteration 78 : 0.388168207300037
Loss in iteration 79 : 0.389841315984103
Loss in iteration 80 : 0.3883582309712195
Loss in iteration 81 : 0.390573749241414
Loss in iteration 82 : 0.3889453951442298
Loss in iteration 83 : 0.391750842587156
Loss in iteration 84 : 0.38924824789316453
Loss in iteration 85 : 0.3917863328723178
Loss in iteration 86 : 0.38910412830090096
Loss in iteration 87 : 0.3915313760026785
Loss in iteration 88 : 0.38926505800113814
Loss in iteration 89 : 0.39130682428731184
Loss in iteration 90 : 0.38951900241992893
Loss in iteration 91 : 0.3920577787717323
Loss in iteration 92 : 0.3897870944143766
Loss in iteration 93 : 0.3918395147284298
Loss in iteration 94 : 0.3901271496028281
Loss in iteration 95 : 0.392523905799801
Loss in iteration 96 : 0.3906485710885701
Loss in iteration 97 : 0.393465655609342
Loss in iteration 98 : 0.39051316466085456
Loss in iteration 99 : 0.393077498037157
Loss in iteration 100 : 0.3909459791935139
Loss in iteration 101 : 0.3941233408454066
Loss in iteration 102 : 0.39084282289756916
Loss in iteration 103 : 0.39321704311882844
Loss in iteration 104 : 0.3912977359908958
Loss in iteration 105 : 0.39401965191630023
Loss in iteration 106 : 0.3915204328011746
Loss in iteration 107 : 0.3947077320483277
Loss in iteration 108 : 0.39167992581044914
Loss in iteration 109 : 0.3944219060798881
Loss in iteration 110 : 0.3921599356316324
Loss in iteration 111 : 0.3955416121503036
Loss in iteration 112 : 0.3929240678541752
Loss in iteration 113 : 0.39686158791697124
Loss in iteration 114 : 0.3926033768342496
Loss in iteration 115 : 0.39527320242930014
Loss in iteration 116 : 0.3921236476813243
Loss in iteration 117 : 0.39361255454753546
Loss in iteration 118 : 0.39241211980586005
Loss in iteration 119 : 0.39505056768890423
Loss in iteration 120 : 0.39392177965769953
Loss in iteration 121 : 0.39824726567914737
Loss in iteration 122 : 0.39490181734030716
Loss in iteration 123 : 0.39939048377558445
Loss in iteration 124 : 0.3942373740729233
Loss in iteration 125 : 0.3967373324335681
Loss in iteration 126 : 0.39396255620285625
Loss in iteration 127 : 0.3967354885831096
Loss in iteration 128 : 0.39484981592021445
Loss in iteration 129 : 0.39874810175372655
Loss in iteration 130 : 0.3954959809691116
Loss in iteration 131 : 0.39976216665922193
Loss in iteration 132 : 0.3953121358327752
Loss in iteration 133 : 0.398586940362642
Loss in iteration 134 : 0.39543089102187695
Loss in iteration 135 : 0.3988951191123485
Loss in iteration 136 : 0.3960658047172955
Loss in iteration 137 : 0.3999823763976508
Loss in iteration 138 : 0.3970009569785255
Loss in iteration 139 : 0.40134711257720485
Loss in iteration 140 : 0.39708725686772545
Loss in iteration 141 : 0.4009913825665277
Loss in iteration 142 : 0.3970925199236708
Loss in iteration 143 : 0.4004760067241754
Loss in iteration 144 : 0.39747093401503736
Loss in iteration 145 : 0.4010804432774878
Loss in iteration 146 : 0.3978372615465128
Loss in iteration 147 : 0.40225083778898546
Loss in iteration 148 : 0.3984485848116195
Loss in iteration 149 : 0.40319607718877265
Loss in iteration 150 : 0.39944251933126174
Loss in iteration 151 : 0.40331387810396774
Loss in iteration 152 : 0.39923777504699726
Loss in iteration 153 : 0.4036448525101039
Loss in iteration 154 : 0.3995067592136541
Loss in iteration 155 : 0.4032005786404409
Loss in iteration 156 : 0.3998361330247027
Loss in iteration 157 : 0.40347465843151004
Loss in iteration 158 : 0.4002070101727761
Loss in iteration 159 : 0.4047304537168812
Loss in iteration 160 : 0.4016661568436594
Loss in iteration 161 : 0.4055012981419227
Loss in iteration 162 : 0.40174956814401913
Loss in iteration 163 : 0.4057226252451895
Loss in iteration 164 : 0.40214816975989487
Loss in iteration 165 : 0.405810041135742
Loss in iteration 166 : 0.4025168045945341
Loss in iteration 167 : 0.40634313169810365
Loss in iteration 168 : 0.40309039016373177
Loss in iteration 169 : 0.40666435585491095
Loss in iteration 170 : 0.40321505807210256
Loss in iteration 171 : 0.4072945265808704
Loss in iteration 172 : 0.4035618993271855
Loss in iteration 173 : 0.40762703221184976
Loss in iteration 174 : 0.40402317206853616
Loss in iteration 175 : 0.4085013411390814
Loss in iteration 176 : 0.4048232505831073
Loss in iteration 177 : 0.4087400248968456
Loss in iteration 178 : 0.40484091648846615
Loss in iteration 179 : 0.4091484636988966
Loss in iteration 180 : 0.40548788389684387
Loss in iteration 181 : 0.4096587138169344
Loss in iteration 182 : 0.4060181637980457
Loss in iteration 183 : 0.4101694667577295
Loss in iteration 184 : 0.4065496145515826
Loss in iteration 185 : 0.41068079672871344
Loss in iteration 186 : 0.4077062577170923
Loss in iteration 187 : 0.41184123946835277
Loss in iteration 188 : 0.40836244345664585
Loss in iteration 189 : 0.41193998764366524
Loss in iteration 190 : 0.4085842903841475
Loss in iteration 191 : 0.41217938507286905
Loss in iteration 192 : 0.4087296938421176
Loss in iteration 193 : 0.41211747623653655
Loss in iteration 194 : 0.4088999439781309
Loss in iteration 195 : 0.41283041359332223
Loss in iteration 196 : 0.4095365213284523
Loss in iteration 197 : 0.4133569473136313
Loss in iteration 198 : 0.41076185980356844
Loss in iteration 199 : 0.4149522750883842
Loss in iteration 200 : 0.4110506043503547
Testing accuracy  of updater 5 on alg 1 with rate 0.00343 = 0.77075, training accuracy 0.8352217546131434, time elapsed: 3643 millisecond.
Loss in iteration 1 : 1.0000127765837035
Loss in iteration 2 : 0.6207310954011231
Loss in iteration 3 : 0.5464449529436792
Loss in iteration 4 : 0.5389841370152811
Loss in iteration 5 : 0.5330369297157315
Loss in iteration 6 : 0.5272772507512801
Loss in iteration 7 : 0.5214544616215495
Loss in iteration 8 : 0.5155306578352191
Loss in iteration 9 : 0.509466619945574
Loss in iteration 10 : 0.5032749882571161
Loss in iteration 11 : 0.496896641590505
Loss in iteration 12 : 0.4903149802113702
Loss in iteration 13 : 0.4835192452504964
Loss in iteration 14 : 0.4764648624995179
Loss in iteration 15 : 0.46916457638634573
Loss in iteration 16 : 0.46157478446720623
Loss in iteration 17 : 0.4536771541466655
Loss in iteration 18 : 0.4455079711616834
Loss in iteration 19 : 0.4370539215099827
Loss in iteration 20 : 0.428640947756189
Loss in iteration 21 : 0.4209354803672457
Loss in iteration 22 : 0.4139731224263127
Loss in iteration 23 : 0.4078221146359374
Loss in iteration 24 : 0.4027432006081027
Loss in iteration 25 : 0.3985353706730685
Loss in iteration 26 : 0.39510334551632886
Loss in iteration 27 : 0.3923099918346477
Loss in iteration 28 : 0.39008238932463546
Loss in iteration 29 : 0.38825562045546597
Loss in iteration 30 : 0.38671046035865514
Loss in iteration 31 : 0.38530970884071664
Loss in iteration 32 : 0.38401642584966755
Loss in iteration 33 : 0.38284366972640643
Loss in iteration 34 : 0.38180184615176327
Loss in iteration 35 : 0.38091897274264774
Loss in iteration 36 : 0.3800875122122023
Loss in iteration 37 : 0.3793347707011643
Loss in iteration 38 : 0.3787083491795112
Loss in iteration 39 : 0.378267055020394
Loss in iteration 40 : 0.37809584785546807
Loss in iteration 41 : 0.37853522539514345
Loss in iteration 42 : 0.3816785874911375
Loss in iteration 43 : 0.3910993242182212
Loss in iteration 44 : 0.3901650625457908
Loss in iteration 45 : 0.3859485677825767
Loss in iteration 46 : 0.37982754415012615
Loss in iteration 47 : 0.37806513979256035
Loss in iteration 48 : 0.3770258361297864
Loss in iteration 49 : 0.3765624595798828
Loss in iteration 50 : 0.37627620133028816
Loss in iteration 51 : 0.3759192630441977
Loss in iteration 52 : 0.3763227634793816
Loss in iteration 53 : 0.37656314691647536
Loss in iteration 54 : 0.3772619932629334
Loss in iteration 55 : 0.37800094081021174
Loss in iteration 56 : 0.3798198386740357
Loss in iteration 57 : 0.38082045982922047
Loss in iteration 58 : 0.38084801203385293
Loss in iteration 59 : 0.3807408182045747
Loss in iteration 60 : 0.37979808471556353
Loss in iteration 61 : 0.3796910879737706
Loss in iteration 62 : 0.3786734165259813
Loss in iteration 63 : 0.3783044659339703
Loss in iteration 64 : 0.3785326978667458
Loss in iteration 65 : 0.3783382577767527
Loss in iteration 66 : 0.37884501510229096
Loss in iteration 67 : 0.3790302761673059
Loss in iteration 68 : 0.37881648280164926
Loss in iteration 69 : 0.378667568886277
Loss in iteration 70 : 0.37907008983189966
Loss in iteration 71 : 0.37904987506921867
Loss in iteration 72 : 0.379178901972933
Loss in iteration 73 : 0.37906762789882886
Loss in iteration 74 : 0.37916448889767007
Loss in iteration 75 : 0.37870733568307474
Loss in iteration 76 : 0.37911777143055553
Loss in iteration 77 : 0.3785276636853742
Loss in iteration 78 : 0.3788521357867374
Loss in iteration 79 : 0.37842556784417025
Loss in iteration 80 : 0.3789715447081064
Loss in iteration 81 : 0.3786901078200947
Loss in iteration 82 : 0.3791084697998231
Loss in iteration 83 : 0.37879182463771327
Loss in iteration 84 : 0.3795102551603286
Loss in iteration 85 : 0.37973240881720804
Loss in iteration 86 : 0.3801889273598612
Loss in iteration 87 : 0.3802039359031153
Loss in iteration 88 : 0.3797506988895578
Loss in iteration 89 : 0.37940415523391846
Loss in iteration 90 : 0.3793372853846081
Loss in iteration 91 : 0.37878038215662363
Loss in iteration 92 : 0.3789561056369994
Loss in iteration 93 : 0.37915822924291886
Loss in iteration 94 : 0.37943460302712995
Loss in iteration 95 : 0.37940512104065516
Loss in iteration 96 : 0.37989387369888067
Loss in iteration 97 : 0.37979216986646
Loss in iteration 98 : 0.38002445384515027
Loss in iteration 99 : 0.3798148880863811
Loss in iteration 100 : 0.3799337334903202
Loss in iteration 101 : 0.37955102048240064
Loss in iteration 102 : 0.3796836684764083
Loss in iteration 103 : 0.37985240074382065
Loss in iteration 104 : 0.3802383663226713
Loss in iteration 105 : 0.38026239821789437
Loss in iteration 106 : 0.3801275302451714
Loss in iteration 107 : 0.3799770203948271
Loss in iteration 108 : 0.3802594245320787
Loss in iteration 109 : 0.3801840194218479
Loss in iteration 110 : 0.38032963717485774
Loss in iteration 111 : 0.380323698522946
Loss in iteration 112 : 0.3804781999942836
Loss in iteration 113 : 0.38054750061254505
Loss in iteration 114 : 0.3805179796738068
Loss in iteration 115 : 0.3804251101415995
Loss in iteration 116 : 0.3805753594909135
Loss in iteration 117 : 0.3804112428619611
Loss in iteration 118 : 0.3805260688529736
Loss in iteration 119 : 0.3805729655419299
Loss in iteration 120 : 0.38073916469951175
Loss in iteration 121 : 0.38085775895138246
Loss in iteration 122 : 0.3810532098567005
Loss in iteration 123 : 0.38139194492642375
Loss in iteration 124 : 0.3810488803838441
Loss in iteration 125 : 0.381093688921015
Loss in iteration 126 : 0.3809581754631095
Loss in iteration 127 : 0.38096786432343543
Loss in iteration 128 : 0.3810714583876929
Loss in iteration 129 : 0.3811293064995047
Loss in iteration 130 : 0.3811824347161632
Loss in iteration 131 : 0.38127835290861367
Loss in iteration 132 : 0.381304516441394
Loss in iteration 133 : 0.3815656540080751
Loss in iteration 134 : 0.38176317414254074
Loss in iteration 135 : 0.38225001306608064
Loss in iteration 136 : 0.38173330828161167
Loss in iteration 137 : 0.3822332231295269
Loss in iteration 138 : 0.3818510559954258
Loss in iteration 139 : 0.38234754034769736
Loss in iteration 140 : 0.3818649837396549
Loss in iteration 141 : 0.38189681791191077
Loss in iteration 142 : 0.3815389371254386
Loss in iteration 143 : 0.38143605923831897
Loss in iteration 144 : 0.3815565595226305
Loss in iteration 145 : 0.38151265561486747
Loss in iteration 146 : 0.38197748602912396
Loss in iteration 147 : 0.38244471797245594
Loss in iteration 148 : 0.38258717144944043
Loss in iteration 149 : 0.38369585038180415
Loss in iteration 150 : 0.3831080772731521
Loss in iteration 151 : 0.3835180415645593
Loss in iteration 152 : 0.38286314807423777
Loss in iteration 153 : 0.38323312179973706
Loss in iteration 154 : 0.38292861551398016
Loss in iteration 155 : 0.383170230595082
Loss in iteration 156 : 0.3829322510719196
Loss in iteration 157 : 0.38313797962194385
Loss in iteration 158 : 0.38283223128764926
Loss in iteration 159 : 0.3826861449617594
Loss in iteration 160 : 0.382625935680149
Loss in iteration 161 : 0.3824337489334472
Loss in iteration 162 : 0.3831646287450163
Loss in iteration 163 : 0.38334312973684415
Loss in iteration 164 : 0.38376372189073205
Loss in iteration 165 : 0.38450305167838444
Loss in iteration 166 : 0.38426852095615505
Loss in iteration 167 : 0.3849421165300899
Loss in iteration 168 : 0.3840785163287623
Loss in iteration 169 : 0.38419242890862304
Loss in iteration 170 : 0.38396390452347
Loss in iteration 171 : 0.38387207063913104
Loss in iteration 172 : 0.3838762450355074
Loss in iteration 173 : 0.38394861343551534
Loss in iteration 174 : 0.3842935191365316
Loss in iteration 175 : 0.38449013995384645
Loss in iteration 176 : 0.384734652103043
Loss in iteration 177 : 0.38526229429824554
Loss in iteration 178 : 0.3850582365453011
Loss in iteration 179 : 0.38536352172042654
Loss in iteration 180 : 0.3851578726436239
Loss in iteration 181 : 0.3854847820884938
Loss in iteration 182 : 0.3848613142241226
Loss in iteration 183 : 0.3850023487806744
Loss in iteration 184 : 0.38508967961325585
Loss in iteration 185 : 0.38531931507810663
Loss in iteration 186 : 0.38550663623237896
Loss in iteration 187 : 0.38610871247644735
Loss in iteration 188 : 0.38553227149607433
Loss in iteration 189 : 0.3858869356861054
Loss in iteration 190 : 0.3856618777838332
Loss in iteration 191 : 0.38594382942806155
Loss in iteration 192 : 0.3860504621929839
Loss in iteration 193 : 0.3865874556956693
Loss in iteration 194 : 0.3862481768129914
Loss in iteration 195 : 0.3868190659291247
Loss in iteration 196 : 0.3864681445452197
Loss in iteration 197 : 0.3869290197919174
Loss in iteration 198 : 0.3864374361777724
Loss in iteration 199 : 0.3866603475982132
Loss in iteration 200 : 0.3868437194175226
Testing accuracy  of updater 5 on alg 1 with rate 0.00196 = 0.77875, training accuracy 0.8339268371641307, time elapsed: 3390 millisecond.
Loss in iteration 1 : 1.0000008052757863
Loss in iteration 2 : 0.7426655704972345
Loss in iteration 3 : 0.6120686286125201
Loss in iteration 4 : 0.5754667616324666
Testing accuracy  of updater 5 on alg 1 with rate 4.9E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 89 millisecond.
Loss in iteration 1 : 1.0003403593083502
Loss in iteration 2 : 1.6854590226269421
Loss in iteration 3 : 2.1721542260058886
Loss in iteration 4 : 2.18548859427568
Loss in iteration 5 : 1.8942091364071088
Loss in iteration 6 : 1.3901947398981436
Loss in iteration 7 : 0.7555172327522983
Loss in iteration 8 : 0.4599087732975045
Loss in iteration 9 : 0.7362997452300988
Loss in iteration 10 : 1.0705146680746853
Loss in iteration 11 : 1.0794812156073053
Loss in iteration 12 : 0.8489248140755145
Loss in iteration 13 : 0.6290270997489037
Loss in iteration 14 : 0.5586221816131312
Loss in iteration 15 : 0.6178326858861437
Loss in iteration 16 : 0.7193570089885001
Loss in iteration 17 : 0.7953351599909637
Loss in iteration 18 : 0.8158471815694693
Loss in iteration 19 : 0.7817447408158219
Loss in iteration 20 : 0.7131862915626571
Loss in iteration 21 : 0.6434899184556027
Loss in iteration 22 : 0.6032552110040341
Loss in iteration 23 : 0.6104433949356363
Loss in iteration 24 : 0.6513305077546331
Loss in iteration 25 : 0.6881787227476615
Loss in iteration 26 : 0.6914231003460874
Loss in iteration 27 : 0.6583994883909373
Loss in iteration 28 : 0.6118874794334306
Loss in iteration 29 : 0.5807660891349748
Loss in iteration 30 : 0.5781242805179934
Loss in iteration 31 : 0.5938530840551195
Loss in iteration 32 : 0.6069538238932757
Loss in iteration 33 : 0.6023400297852013
Loss in iteration 34 : 0.5777459754582012
Loss in iteration 35 : 0.5462474327538676
Loss in iteration 36 : 0.5278793565447816
Loss in iteration 37 : 0.5305068672174489
Loss in iteration 38 : 0.5402031249409058
Loss in iteration 39 : 0.5349506599413256
Loss in iteration 40 : 0.5098404193901513
Loss in iteration 41 : 0.48636899901043656
Loss in iteration 42 : 0.48112206617728526
Loss in iteration 43 : 0.48890379830478453
Loss in iteration 44 : 0.4834027310746197
Loss in iteration 45 : 0.4613991228551321
Loss in iteration 46 : 0.4485979186204313
Loss in iteration 47 : 0.4567210246836473
Loss in iteration 48 : 0.4573027668284152
Loss in iteration 49 : 0.43631687039118416
Loss in iteration 50 : 0.43823264081007013
Loss in iteration 51 : 0.4490512335306719
Loss in iteration 52 : 0.4360545321983187
Loss in iteration 53 : 0.4441156876525977
Loss in iteration 54 : 0.4522218883847413
Loss in iteration 55 : 0.4427690640435889
Loss in iteration 56 : 0.45985869607691376
Loss in iteration 57 : 0.4492850712121794
Loss in iteration 58 : 0.45848591127587246
Loss in iteration 59 : 0.4537890978610311
Loss in iteration 60 : 0.45357972467508817
Loss in iteration 61 : 0.45673314773324364
Loss in iteration 62 : 0.4509607936275465
Loss in iteration 63 : 0.4565252339505524
Loss in iteration 64 : 0.45426957425655345
Loss in iteration 65 : 0.45483162290042667
Loss in iteration 66 : 0.45897646691973215
Loss in iteration 67 : 0.45787361365636925
Loss in iteration 68 : 0.4599822575959303
Loss in iteration 69 : 0.4624711176351638
Loss in iteration 70 : 0.4617700972163345
Loss in iteration 71 : 0.4643142067513336
Loss in iteration 72 : 0.4650428786502605
Loss in iteration 73 : 0.4653140183855207
Loss in iteration 74 : 0.46726947952459325
Loss in iteration 75 : 0.4668674920076495
Loss in iteration 76 : 0.46762608551477475
Loss in iteration 77 : 0.4687489354859706
Loss in iteration 78 : 0.468665918478699
Loss in iteration 79 : 0.46958534251705836
Loss in iteration 80 : 0.47018746933351774
Loss in iteration 81 : 0.47019389557142954
Loss in iteration 82 : 0.4713306824291288
Loss in iteration 83 : 0.47162574923838396
Loss in iteration 84 : 0.4723076604539571
Loss in iteration 85 : 0.472852507458444
Loss in iteration 86 : 0.47316828758035573
Loss in iteration 87 : 0.47385737858280075
Loss in iteration 88 : 0.474135505946526
Loss in iteration 89 : 0.47468009535318134
Loss in iteration 90 : 0.4748890581204517
Loss in iteration 91 : 0.47528179099421397
Loss in iteration 92 : 0.47563689588274494
Loss in iteration 93 : 0.47590141199668134
Loss in iteration 94 : 0.4761880129523825
Loss in iteration 95 : 0.47639077033010513
Loss in iteration 96 : 0.47676864843314504
Loss in iteration 97 : 0.4770195188988605
Loss in iteration 98 : 0.47730324226565213
Loss in iteration 99 : 0.4775240144317896
Testing accuracy  of updater 6 on alg 1 with rate 0.0392 = 0.7895, training accuracy 0.8452573648429913, time elapsed: 1823 millisecond.
Loss in iteration 1 : 1.0000917620069665
Loss in iteration 2 : 0.9710406986205121
Loss in iteration 3 : 1.2842920173122496
Loss in iteration 4 : 1.333700675217711
Loss in iteration 5 : 1.1548529854848515
Loss in iteration 6 : 0.7797490449580263
Loss in iteration 7 : 0.42523901592036983
Loss in iteration 8 : 0.5409041399064399
Loss in iteration 9 : 0.7996095888966159
Loss in iteration 10 : 0.8198876524138001
Loss in iteration 11 : 0.648778913102634
Loss in iteration 12 : 0.5179512487236333
Loss in iteration 13 : 0.5156860971569038
Loss in iteration 14 : 0.5942125009360564
Loss in iteration 15 : 0.6692603894400104
Loss in iteration 16 : 0.6959565669545137
Loss in iteration 17 : 0.6711742454841982
Loss in iteration 18 : 0.6174810121716557
Loss in iteration 19 : 0.5699828812790203
Loss in iteration 20 : 0.5546353074401663
Loss in iteration 21 : 0.5789826925818639
Loss in iteration 22 : 0.614926833129074
Loss in iteration 23 : 0.6310925044029014
Loss in iteration 24 : 0.6155710524857164
Loss in iteration 25 : 0.5811082917682386
Loss in iteration 26 : 0.5528128964913571
Loss in iteration 27 : 0.5461887185106531
Loss in iteration 28 : 0.5580479513242746
Loss in iteration 29 : 0.5698628482414942
Loss in iteration 30 : 0.5679951819659863
Loss in iteration 31 : 0.5497113308898285
Loss in iteration 32 : 0.5252200234507948
Loss in iteration 33 : 0.5108922379779178
Loss in iteration 34 : 0.5118363783969895
Loss in iteration 35 : 0.5178123163544776
Loss in iteration 36 : 0.5138157026300842
Loss in iteration 37 : 0.4962709470770875
Loss in iteration 38 : 0.4782189537092485
Loss in iteration 39 : 0.4716469750124469
Loss in iteration 40 : 0.47531638089807726
Loss in iteration 41 : 0.47388899317635563
Loss in iteration 42 : 0.45998292139150115
Loss in iteration 43 : 0.4464088981214608
Loss in iteration 44 : 0.44509448217775477
Loss in iteration 45 : 0.4490800407550762
Loss in iteration 46 : 0.4407810340202044
Loss in iteration 47 : 0.4297673224640668
Loss in iteration 48 : 0.4333020725510832
Loss in iteration 49 : 0.43777985189881885
Loss in iteration 50 : 0.4300422257115974
Loss in iteration 51 : 0.4332189944115301
Loss in iteration 52 : 0.4411772362414551
Loss in iteration 53 : 0.4350421592056607
Loss in iteration 54 : 0.4423533475133546
Loss in iteration 55 : 0.4465153866560292
Loss in iteration 56 : 0.44227386667149154
Loss in iteration 57 : 0.4499605125198564
Loss in iteration 58 : 0.4466483899976921
Loss in iteration 59 : 0.4467660334186668
Loss in iteration 60 : 0.45059723752207836
Loss in iteration 61 : 0.4467527818281152
Loss in iteration 62 : 0.4485551090430464
Loss in iteration 63 : 0.45025318507962075
Loss in iteration 64 : 0.44848621536814726
Loss in iteration 65 : 0.45097566422689905
Loss in iteration 66 : 0.4526807597402269
Loss in iteration 67 : 0.4523886681078356
Loss in iteration 68 : 0.45432221578680765
Loss in iteration 69 : 0.45599128366932073
Loss in iteration 70 : 0.4561913514989613
Loss in iteration 71 : 0.45746046161450915
Loss in iteration 72 : 0.4591183571766758
Loss in iteration 73 : 0.45937157818658153
Loss in iteration 74 : 0.4603630504041223
Loss in iteration 75 : 0.46163450654383487
Loss in iteration 76 : 0.4618852032457962
Loss in iteration 77 : 0.46247981845855624
Loss in iteration 78 : 0.4634881733382445
Loss in iteration 79 : 0.4639229481022251
Loss in iteration 80 : 0.4643790230341548
Loss in iteration 81 : 0.46517575068146116
Loss in iteration 82 : 0.46569428485538195
Loss in iteration 83 : 0.4660359232919807
Loss in iteration 84 : 0.4666987823219523
Loss in iteration 85 : 0.4673340023185596
Loss in iteration 86 : 0.46774925416827207
Loss in iteration 87 : 0.468442846951576
Loss in iteration 88 : 0.46885948400759847
Loss in iteration 89 : 0.46924154909295585
Loss in iteration 90 : 0.46985663238109826
Loss in iteration 91 : 0.47024204270998243
Loss in iteration 92 : 0.4707418988105767
Loss in iteration 93 : 0.47117594836228305
Loss in iteration 94 : 0.4714387095774368
Loss in iteration 95 : 0.47194877022473436
Loss in iteration 96 : 0.4722655359476185
Loss in iteration 97 : 0.4726082359492007
Loss in iteration 98 : 0.4729178144515011
Loss in iteration 99 : 0.4731605576411446
Loss in iteration 100 : 0.47353713861557295
Loss in iteration 101 : 0.4738082792438826
Loss in iteration 102 : 0.47410104453387925
Loss in iteration 103 : 0.4743745487050216
Loss in iteration 104 : 0.4746152591733485
Loss in iteration 105 : 0.47488897746025915
Loss in iteration 106 : 0.4751374018864908
Loss in iteration 107 : 0.4753857725348576
Loss in iteration 108 : 0.4756213130865683
Loss in iteration 109 : 0.475839190694084
Loss in iteration 110 : 0.4760703677159071
Loss in iteration 111 : 0.47628117663625447
Testing accuracy  of updater 6 on alg 1 with rate 0.02744 = 0.78925, training accuracy 0.8442861767562317, time elapsed: 1875 millisecond.
Loss in iteration 1 : 1.0000388955573787
Loss in iteration 2 : 0.7523334928498424
Loss in iteration 3 : 0.9574099541593315
Loss in iteration 4 : 0.9917951446876629
Loss in iteration 5 : 0.8766017502781694
Loss in iteration 6 : 0.6319241522534728
Loss in iteration 7 : 0.38678053693854986
Loss in iteration 8 : 0.4756339420318095
Loss in iteration 9 : 0.6395793014593326
Loss in iteration 10 : 0.6394131544993777
Loss in iteration 11 : 0.5202983786223031
Loss in iteration 12 : 0.43272812548330186
Loss in iteration 13 : 0.43243821784106645
Loss in iteration 14 : 0.4882913075256569
Loss in iteration 15 : 0.537990772734318
Loss in iteration 16 : 0.5515985512845647
Loss in iteration 17 : 0.530359291557599
Loss in iteration 18 : 0.49252906306960925
Loss in iteration 19 : 0.4642620489900911
Loss in iteration 20 : 0.4619141408374608
Loss in iteration 21 : 0.480920441413368
Loss in iteration 22 : 0.5031070230074143
Loss in iteration 23 : 0.5116038275037701
Loss in iteration 24 : 0.5018780554173431
Loss in iteration 25 : 0.4820517679365372
Loss in iteration 26 : 0.4657866685694
Loss in iteration 27 : 0.46110514593784785
Loss in iteration 28 : 0.4675326608537252
Loss in iteration 29 : 0.4763756401184668
Loss in iteration 30 : 0.4780592646358942
Loss in iteration 31 : 0.4700576759771482
Loss in iteration 32 : 0.4566313180631856
Loss in iteration 33 : 0.44690856173755117
Loss in iteration 34 : 0.4449924264931067
Loss in iteration 35 : 0.4474705110326976
Loss in iteration 36 : 0.4492795895842332
Loss in iteration 37 : 0.4448763223639599
Loss in iteration 38 : 0.4357128906818675
Loss in iteration 39 : 0.4277260313194196
Loss in iteration 40 : 0.42512267743992066
Loss in iteration 41 : 0.4259170966747752
Loss in iteration 42 : 0.42597343446207697
Loss in iteration 43 : 0.4220305333216111
Loss in iteration 44 : 0.4157450753334072
Loss in iteration 45 : 0.4109205069599738
Loss in iteration 46 : 0.4104148536920855
Loss in iteration 47 : 0.41190750417539806
Loss in iteration 48 : 0.4099027029888744
Loss in iteration 49 : 0.4053242414335338
Loss in iteration 50 : 0.40382168762780063
Loss in iteration 51 : 0.4060662933384568
Loss in iteration 52 : 0.40684696472778287
Loss in iteration 53 : 0.4047056738879231
Loss in iteration 54 : 0.40487547953890113
Loss in iteration 55 : 0.4081605756150777
Loss in iteration 56 : 0.40925881195278846
Loss in iteration 57 : 0.4083019020903705
Loss in iteration 58 : 0.41055526477474225
Loss in iteration 59 : 0.4131768958545138
Loss in iteration 60 : 0.4137115668938649
Loss in iteration 61 : 0.41389348135565096
Loss in iteration 62 : 0.4161973631263873
Loss in iteration 63 : 0.417513762199564
Loss in iteration 64 : 0.4171821315491262
Loss in iteration 65 : 0.4183847029323294
Loss in iteration 66 : 0.41995324492421904
Loss in iteration 67 : 0.4203105936825019
Loss in iteration 68 : 0.42057427476918485
Loss in iteration 69 : 0.4218109861415377
Loss in iteration 70 : 0.4229443605355382
Loss in iteration 71 : 0.42333523108183946
Loss in iteration 72 : 0.4241573462204591
Loss in iteration 73 : 0.4254248563332591
Loss in iteration 74 : 0.42645470378710015
Loss in iteration 75 : 0.4271354393954965
Loss in iteration 76 : 0.42812135580669475
Loss in iteration 77 : 0.42930172276674444
Loss in iteration 78 : 0.4302731756689221
Loss in iteration 79 : 0.43102915430861954
Loss in iteration 80 : 0.4319367737272904
Loss in iteration 81 : 0.4329604884593693
Loss in iteration 82 : 0.4338774994607832
Loss in iteration 83 : 0.43466808159106834
Loss in iteration 84 : 0.43547100931487703
Loss in iteration 85 : 0.43638618629237497
Loss in iteration 86 : 0.43717115864725953
Loss in iteration 87 : 0.43784889675844446
Loss in iteration 88 : 0.43861628483270304
Loss in iteration 89 : 0.4394076330162651
Loss in iteration 90 : 0.4400965210111258
Loss in iteration 91 : 0.4407790674487862
Loss in iteration 92 : 0.4415102651430578
Loss in iteration 93 : 0.44218372620286317
Loss in iteration 94 : 0.4427998450805153
Loss in iteration 95 : 0.44346786947952077
Loss in iteration 96 : 0.44414756079348405
Loss in iteration 97 : 0.44478949979164023
Loss in iteration 98 : 0.4454329398667539
Loss in iteration 99 : 0.44610365880691877
Loss in iteration 100 : 0.44674335773367496
Loss in iteration 101 : 0.44735117344265535
Loss in iteration 102 : 0.44798859359180876
Loss in iteration 103 : 0.44863091820642725
Loss in iteration 104 : 0.44924508404638025
Loss in iteration 105 : 0.4498579312348679
Loss in iteration 106 : 0.45047506991880104
Loss in iteration 107 : 0.45106472366804184
Loss in iteration 108 : 0.4516579739786846
Loss in iteration 109 : 0.4522571605299243
Loss in iteration 110 : 0.452843341161857
Loss in iteration 111 : 0.45341719497258237
Loss in iteration 112 : 0.4539829125345006
Loss in iteration 113 : 0.4545326566868726
Loss in iteration 114 : 0.45506971487432496
Loss in iteration 115 : 0.45560163115818797
Loss in iteration 116 : 0.4561287012879492
Loss in iteration 117 : 0.45664817864248886
Loss in iteration 118 : 0.45715421703615444
Loss in iteration 119 : 0.457645651701327
Loss in iteration 120 : 0.4581234906661624
Loss in iteration 121 : 0.4585897440788371
Loss in iteration 122 : 0.45904714244843164
Loss in iteration 123 : 0.4594946640515142
Loss in iteration 124 : 0.4599334687320226
Loss in iteration 125 : 0.4603627147032877
Loss in iteration 126 : 0.4607832326390266
Loss in iteration 127 : 0.46119576240482785
Loss in iteration 128 : 0.46160116672468543
Loss in iteration 129 : 0.46199830900209216
Loss in iteration 130 : 0.4623865954030017
Loss in iteration 131 : 0.46276364182032537
Loss in iteration 132 : 0.4631294843305284
Loss in iteration 133 : 0.46348166131737406
Loss in iteration 134 : 0.4638259605159391
Loss in iteration 135 : 0.46416485808045127
Loss in iteration 136 : 0.4644979033249366
Loss in iteration 137 : 0.4648257500057989
Loss in iteration 138 : 0.46514898889728173
Loss in iteration 139 : 0.46546544754755736
Loss in iteration 140 : 0.4657756021932744
Loss in iteration 141 : 0.4660810630511022
Loss in iteration 142 : 0.46638323198156023
Loss in iteration 143 : 0.4666822206148179
Loss in iteration 144 : 0.46697842291930414
Loss in iteration 145 : 0.46726771058646155
Loss in iteration 146 : 0.4675506646674281
Loss in iteration 147 : 0.46782789710240763
Loss in iteration 148 : 0.4681000460296292
Loss in iteration 149 : 0.4683675760335193
Loss in iteration 150 : 0.46862921050237294
Loss in iteration 151 : 0.4688875318829653
Loss in iteration 152 : 0.4691412726740034
Loss in iteration 153 : 0.4693923120360307
Loss in iteration 154 : 0.46964145218064174
Loss in iteration 155 : 0.46988853791899055
Loss in iteration 156 : 0.4701326359512556
Loss in iteration 157 : 0.47036922967357353
Loss in iteration 158 : 0.47059942807369215
Loss in iteration 159 : 0.4708237288151233
Loss in iteration 160 : 0.4710427875187373
Loss in iteration 161 : 0.47125719614642003
Loss in iteration 162 : 0.4714674891025846
Testing accuracy  of updater 6 on alg 1 with rate 0.01568 = 0.7885, training accuracy 0.8429912593072192, time elapsed: 2586 millisecond.
Loss in iteration 1 : 1.0000031754700225
Loss in iteration 2 : 0.582294522378948
Loss in iteration 3 : 0.6123626630817766
Loss in iteration 4 : 0.6984280804365212
Loss in iteration 5 : 0.7343249959482633
Loss in iteration 6 : 0.7247755554774524
Loss in iteration 7 : 0.6752122499555269
Loss in iteration 8 : 0.5916712322157535
Loss in iteration 9 : 0.4901716771999773
Loss in iteration 10 : 0.4165680784120239
Loss in iteration 11 : 0.42157571005472194
Loss in iteration 12 : 0.4771738582032737
Loss in iteration 13 : 0.5050159776041666
Loss in iteration 14 : 0.48398990535200825
Loss in iteration 15 : 0.435381791937168
Loss in iteration 16 : 0.3923369654991748
Loss in iteration 17 : 0.37674080728278386
Loss in iteration 18 : 0.3849115683009044
Loss in iteration 19 : 0.4022321664614622
Loss in iteration 20 : 0.4156330278724289
Loss in iteration 21 : 0.41849094689849636
Loss in iteration 22 : 0.41111867245352807
Loss in iteration 23 : 0.39836037483896136
Loss in iteration 24 : 0.3862011778338465
Loss in iteration 25 : 0.37957758295581145
Loss in iteration 26 : 0.3793961635992937
Loss in iteration 27 : 0.3843955288240438
Loss in iteration 28 : 0.3907212608728433
Loss in iteration 29 : 0.39509836819355043
Loss in iteration 30 : 0.3959366381776881
Loss in iteration 31 : 0.39342981759207113
Loss in iteration 32 : 0.3889829105406181
Loss in iteration 33 : 0.3845972076539116
Loss in iteration 34 : 0.3822333774702395
Loss in iteration 35 : 0.3822612196125786
Loss in iteration 36 : 0.383786589448527
Loss in iteration 37 : 0.38580146088862943
Loss in iteration 38 : 0.38742302701916354
Loss in iteration 39 : 0.3878477875990747
Loss in iteration 40 : 0.38699169130158745
Loss in iteration 41 : 0.3853754531926846
Loss in iteration 42 : 0.38373930732935846
Loss in iteration 43 : 0.38247601468434356
Loss in iteration 44 : 0.38200293717130807
Loss in iteration 45 : 0.3821740768761204
Loss in iteration 46 : 0.3828580243766388
Loss in iteration 47 : 0.38353148324744457
Loss in iteration 48 : 0.38364632630358425
Loss in iteration 49 : 0.38312182493871205
Loss in iteration 50 : 0.38219553357229097
Loss in iteration 51 : 0.381395137904524
Loss in iteration 52 : 0.3808957158915797
Loss in iteration 53 : 0.38079422475460145
Loss in iteration 54 : 0.38096517469013685
Loss in iteration 55 : 0.38121396050332684
Loss in iteration 56 : 0.3812812144178757
Loss in iteration 57 : 0.3810982911157604
Loss in iteration 58 : 0.38073302182866064
Loss in iteration 59 : 0.3803259718035621
Loss in iteration 60 : 0.38002686444746825
Loss in iteration 61 : 0.3799653660155106
Loss in iteration 62 : 0.380087119966354
Loss in iteration 63 : 0.3802597004419335
Loss in iteration 64 : 0.3803359354379208
Loss in iteration 65 : 0.38026311015032543
Loss in iteration 66 : 0.3801095680653291
Loss in iteration 67 : 0.3800059916555261
Loss in iteration 68 : 0.37999463962343355
Loss in iteration 69 : 0.38009077054405055
Loss in iteration 70 : 0.38025350541547237
Loss in iteration 71 : 0.3803931239759782
Loss in iteration 72 : 0.3804680412309553
Loss in iteration 73 : 0.3805091453446211
Loss in iteration 74 : 0.38054412041669605
Loss in iteration 75 : 0.3806409845264032
Loss in iteration 76 : 0.3808128332680819
Loss in iteration 77 : 0.3809916622890525
Loss in iteration 78 : 0.3811639427310446
Loss in iteration 79 : 0.38131446587881923
Loss in iteration 80 : 0.38143948550855655
Loss in iteration 81 : 0.38157321796713584
Loss in iteration 82 : 0.3817257568429407
Loss in iteration 83 : 0.38189317619021557
Loss in iteration 84 : 0.38207975877783784
Loss in iteration 85 : 0.3822793708473704
Loss in iteration 86 : 0.3824755697365644
Loss in iteration 87 : 0.3826595536361433
Loss in iteration 88 : 0.3828401874935698
Loss in iteration 89 : 0.38302127698900135
Loss in iteration 90 : 0.38321808957913756
Loss in iteration 91 : 0.38343679606957415
Loss in iteration 92 : 0.383652764280927
Loss in iteration 93 : 0.3838623370719201
Loss in iteration 94 : 0.384065752534585
Loss in iteration 95 : 0.38426864914674935
Loss in iteration 96 : 0.3844727826824754
Loss in iteration 97 : 0.3846845541281921
Loss in iteration 98 : 0.38490277564247766
Loss in iteration 99 : 0.38512630324213826
Loss in iteration 100 : 0.3853501748861399
Loss in iteration 101 : 0.38556771594184375
Loss in iteration 102 : 0.38578430121561075
Loss in iteration 103 : 0.3860029517428336
Loss in iteration 104 : 0.38622274267352125
Loss in iteration 105 : 0.38644402181278215
Loss in iteration 106 : 0.38667256848734416
Loss in iteration 107 : 0.3869025636994426
Loss in iteration 108 : 0.3871309554254672
Loss in iteration 109 : 0.387358067507814
Loss in iteration 110 : 0.38758456013809306
Loss in iteration 111 : 0.38781352061729557
Loss in iteration 112 : 0.38804507033351887
Loss in iteration 113 : 0.3882788191320272
Loss in iteration 114 : 0.3885150235278231
Loss in iteration 115 : 0.38875108491639093
Loss in iteration 116 : 0.38898616706932765
Loss in iteration 117 : 0.3892238283253982
Loss in iteration 118 : 0.38946307440708944
Loss in iteration 119 : 0.3897043878810199
Loss in iteration 120 : 0.38994568148423425
Loss in iteration 121 : 0.39018710482607694
Loss in iteration 122 : 0.3904289899564539
Loss in iteration 123 : 0.39067228311651
Loss in iteration 124 : 0.39091716292973133
Loss in iteration 125 : 0.3911622940676655
Loss in iteration 126 : 0.3914069602948291
Loss in iteration 127 : 0.3916532684382295
Loss in iteration 128 : 0.3919002484989193
Loss in iteration 129 : 0.3921481384019367
Loss in iteration 130 : 0.3923964372691895
Loss in iteration 131 : 0.3926454953837401
Loss in iteration 132 : 0.3928949310813385
Loss in iteration 133 : 0.3931447740730084
Loss in iteration 134 : 0.3933953124842533
Loss in iteration 135 : 0.3936463017753937
Loss in iteration 136 : 0.39389747139515385
Loss in iteration 137 : 0.39414916987936555
Loss in iteration 138 : 0.3944013537416818
Loss in iteration 139 : 0.39465368017961117
Loss in iteration 140 : 0.3949062034707583
Loss in iteration 141 : 0.39515892604017544
Loss in iteration 142 : 0.3954118469064468
Loss in iteration 143 : 0.3956649630859186
Loss in iteration 144 : 0.3959182400976862
Loss in iteration 145 : 0.3961713712595061
Loss in iteration 146 : 0.3964244865566148
Loss in iteration 147 : 0.3966774214680361
Loss in iteration 148 : 0.3969300186261699
Loss in iteration 149 : 0.39718320206332136
Loss in iteration 150 : 0.3974365291186116
Loss in iteration 151 : 0.39768985529826084
Loss in iteration 152 : 0.39794265233863596
Loss in iteration 153 : 0.3981959669060593
Loss in iteration 154 : 0.3984476994955294
Loss in iteration 155 : 0.39869893971518017
Loss in iteration 156 : 0.39895190879685344
Loss in iteration 157 : 0.39920508806811283
Loss in iteration 158 : 0.3994583902891589
Loss in iteration 159 : 0.39971154293822814
Loss in iteration 160 : 0.39996454595123015
Loss in iteration 161 : 0.4002169636593058
Loss in iteration 162 : 0.4004687883589475
Loss in iteration 163 : 0.40072065234699494
Loss in iteration 164 : 0.4009725661624633
Loss in iteration 165 : 0.40122459497679325
Loss in iteration 166 : 0.4014768240665606
Loss in iteration 167 : 0.4017306768482351
Loss in iteration 168 : 0.4019853330213331
Loss in iteration 169 : 0.4022407741846274
Loss in iteration 170 : 0.4024968942120547
Loss in iteration 171 : 0.40275374991024104
Loss in iteration 172 : 0.40301104550765443
Loss in iteration 173 : 0.4032687158040065
Loss in iteration 174 : 0.40352682124938444
Loss in iteration 175 : 0.40378521125101063
Loss in iteration 176 : 0.40404407341363124
Loss in iteration 177 : 0.40430280481326625
Loss in iteration 178 : 0.40456179884620724
Loss in iteration 179 : 0.4048206395891646
Loss in iteration 180 : 0.40507915416641177
Loss in iteration 181 : 0.40533746490359673
Loss in iteration 182 : 0.4055950203514718
Loss in iteration 183 : 0.4058519294727826
Loss in iteration 184 : 0.40610833809897395
Loss in iteration 185 : 0.4063643781440151
Loss in iteration 186 : 0.406619983262226
Loss in iteration 187 : 0.4068751537188438
Loss in iteration 188 : 0.40712866667992215
Loss in iteration 189 : 0.40738138475508817
Loss in iteration 190 : 0.4076349553723562
Loss in iteration 191 : 0.40788822393915614
Loss in iteration 192 : 0.40814127996353594
Loss in iteration 193 : 0.408394035381203
Loss in iteration 194 : 0.40864656919179665
Loss in iteration 195 : 0.40889897971387146
Loss in iteration 196 : 0.40915102355176697
Loss in iteration 197 : 0.4094029473672755
Loss in iteration 198 : 0.4096543803809797
Loss in iteration 199 : 0.4099049935714387
Loss in iteration 200 : 0.4101552488317596
Testing accuracy  of updater 6 on alg 1 with rate 0.00392 = 0.787, training accuracy 0.8413726124959534, time elapsed: 3196 millisecond.
Loss in iteration 1 : 1.0000020936689338
Loss in iteration 2 : 0.6405410645766048
Loss in iteration 3 : 0.576898342709624
Loss in iteration 4 : 0.662529593212654
Loss in iteration 5 : 0.7118438704268947
Loss in iteration 6 : 0.724652327677542
Loss in iteration 7 : 0.7048423262155905
Loss in iteration 8 : 0.6563957118942907
Loss in iteration 9 : 0.5838128348473041
Loss in iteration 10 : 0.4993032377861181
Loss in iteration 11 : 0.4329029599027246
Loss in iteration 12 : 0.4130291347748254
Loss in iteration 13 : 0.4470101515649717
Loss in iteration 14 : 0.4846645376665391
Loss in iteration 15 : 0.49182329035631134
Loss in iteration 16 : 0.4670393107656517
Loss in iteration 17 : 0.42684257146354393
Loss in iteration 18 : 0.39323281556469303
Loss in iteration 19 : 0.3788238232823788
Loss in iteration 20 : 0.38212221193380447
Loss in iteration 21 : 0.39423336953100035
Loss in iteration 22 : 0.40577129867416034
Loss in iteration 23 : 0.4111389179323092
Loss in iteration 24 : 0.40883155463716664
Loss in iteration 25 : 0.40069741574980927
Loss in iteration 26 : 0.3904984118490679
Loss in iteration 27 : 0.381763719858476
Loss in iteration 28 : 0.37700011422940594
Loss in iteration 29 : 0.3766122288741192
Loss in iteration 30 : 0.37983025389639796
Loss in iteration 31 : 0.3843025020049935
Loss in iteration 32 : 0.3877827363843983
Loss in iteration 33 : 0.38910167874245
Loss in iteration 34 : 0.38804279452002305
Loss in iteration 35 : 0.3854582394488464
Loss in iteration 36 : 0.38219668633404175
Loss in iteration 37 : 0.37938695311530507
Loss in iteration 38 : 0.37805218150745595
Loss in iteration 39 : 0.3782297278257797
Loss in iteration 40 : 0.37929754608219257
Loss in iteration 41 : 0.38069214494358344
Loss in iteration 42 : 0.3817866905997548
Loss in iteration 43 : 0.3822641763754508
Loss in iteration 44 : 0.3820143516251935
Loss in iteration 45 : 0.3812083215969168
Loss in iteration 46 : 0.38010105069369743
Loss in iteration 47 : 0.3791134134918236
Loss in iteration 48 : 0.3784214992469881
Loss in iteration 49 : 0.3781370892488626
Loss in iteration 50 : 0.37835421657492296
Loss in iteration 51 : 0.37883373588232083
Loss in iteration 52 : 0.37928289563025774
Loss in iteration 53 : 0.37946288748645557
Loss in iteration 54 : 0.3793283961429422
Loss in iteration 55 : 0.37892743885070856
Loss in iteration 56 : 0.3784198073568233
Loss in iteration 57 : 0.3779955052653891
Loss in iteration 58 : 0.3777322394853848
Loss in iteration 59 : 0.37771664832937435
Loss in iteration 60 : 0.37786321446812543
Loss in iteration 61 : 0.37801529771039005
Loss in iteration 62 : 0.3781155404599827
Loss in iteration 63 : 0.3780883771555741
Loss in iteration 64 : 0.37795329840528
Loss in iteration 65 : 0.37775886872326514
Loss in iteration 66 : 0.37757250539821025
Loss in iteration 67 : 0.3774775352813229
Loss in iteration 68 : 0.37747869296140607
Loss in iteration 69 : 0.3775730683679774
Loss in iteration 70 : 0.3776750873846978
Loss in iteration 71 : 0.3777362625778269
Loss in iteration 72 : 0.3777451768266964
Loss in iteration 73 : 0.37771757030293795
Loss in iteration 74 : 0.3776640140938761
Loss in iteration 75 : 0.3776217258734118
Loss in iteration 76 : 0.3776308390621566
Loss in iteration 77 : 0.37767482893609194
Loss in iteration 78 : 0.3777569048831058
Loss in iteration 79 : 0.3778480142436017
Loss in iteration 80 : 0.3779149470480126
Loss in iteration 81 : 0.37795819835931427
Loss in iteration 82 : 0.37799273743627015
Loss in iteration 83 : 0.3780312361510467
Loss in iteration 84 : 0.3780761336203066
Loss in iteration 85 : 0.3781415292661594
Loss in iteration 86 : 0.3782258719271812
Loss in iteration 87 : 0.37831998037026643
Loss in iteration 88 : 0.37841100930333604
Loss in iteration 89 : 0.37849530394978514
Loss in iteration 90 : 0.37857223769967835
Loss in iteration 91 : 0.3786469840596679
Loss in iteration 92 : 0.37872601719547894
Loss in iteration 93 : 0.37880880114680815
Loss in iteration 94 : 0.37890342882348566
Loss in iteration 95 : 0.3790074467526714
Loss in iteration 96 : 0.37911312666423524
Loss in iteration 97 : 0.379216003126064
Loss in iteration 98 : 0.3793143942662061
Loss in iteration 99 : 0.3794126202713698
Loss in iteration 100 : 0.37951278439667724
Loss in iteration 101 : 0.3796174491290692
Loss in iteration 102 : 0.3797249731261805
Loss in iteration 103 : 0.37983599377806476
Loss in iteration 104 : 0.37994811323804817
Loss in iteration 105 : 0.3800605763706307
Loss in iteration 106 : 0.38017280926265745
Loss in iteration 107 : 0.3802846153399684
Loss in iteration 108 : 0.38039733829411887
Loss in iteration 109 : 0.38051138003394763
Loss in iteration 110 : 0.3806275597592188
Loss in iteration 111 : 0.3807458576516576
Loss in iteration 112 : 0.3808651425973124
Loss in iteration 113 : 0.38098591163109297
Loss in iteration 114 : 0.3811078477507849
Loss in iteration 115 : 0.38123091690072136
Loss in iteration 116 : 0.3813551282500836
Loss in iteration 117 : 0.381480668297415
Loss in iteration 118 : 0.3816072118090692
Loss in iteration 119 : 0.3817340350459708
Loss in iteration 120 : 0.3818617850072489
Loss in iteration 121 : 0.38199050747237995
Loss in iteration 122 : 0.382119876474629
Loss in iteration 123 : 0.3822498085578745
Loss in iteration 124 : 0.3823803352509079
Loss in iteration 125 : 0.38251139432694575
Loss in iteration 126 : 0.38264319215542325
Loss in iteration 127 : 0.38277582935200016
Loss in iteration 128 : 0.3829088753887232
Loss in iteration 129 : 0.3830425645490059
Loss in iteration 130 : 0.3831773156342525
Loss in iteration 131 : 0.38331237962947656
Loss in iteration 132 : 0.38344780681662083
Loss in iteration 133 : 0.38358364273463386
Loss in iteration 134 : 0.38371992861456017
Loss in iteration 135 : 0.38385670177568887
Loss in iteration 136 : 0.3839939959860734
Loss in iteration 137 : 0.3841318417904807
Loss in iteration 138 : 0.38427026680860293
Loss in iteration 139 : 0.3844093402917559
Loss in iteration 140 : 0.3845496202145168
Loss in iteration 141 : 0.38469024101275023
Loss in iteration 142 : 0.3848312479159281
Loss in iteration 143 : 0.38497268181606487
Loss in iteration 144 : 0.3851145796744292
Loss in iteration 145 : 0.38525697489063004
Loss in iteration 146 : 0.38539989763747334
Loss in iteration 147 : 0.38554337516468284
Loss in iteration 148 : 0.38568743207432343
Loss in iteration 149 : 0.38583208045026385
Loss in iteration 150 : 0.385977716533059
Loss in iteration 151 : 0.38612395270859395
Loss in iteration 152 : 0.3862708077973253
Loss in iteration 153 : 0.38641853709980334
Loss in iteration 154 : 0.38656677756798485
Loss in iteration 155 : 0.38671553821890237
Loss in iteration 156 : 0.38686487564183925
Loss in iteration 157 : 0.38701468175427667
Loss in iteration 158 : 0.3871650185115107
Loss in iteration 159 : 0.38731594136321484
Loss in iteration 160 : 0.3874674380642892
Loss in iteration 161 : 0.38761942739233757
Loss in iteration 162 : 0.3877719343234175
Loss in iteration 163 : 0.38792498144523074
Loss in iteration 164 : 0.38807834683972287
Loss in iteration 165 : 0.38823203012857355
Loss in iteration 166 : 0.38838607200910513
Loss in iteration 167 : 0.3885408828701772
Loss in iteration 168 : 0.38869684641526736
Loss in iteration 169 : 0.3888531349355604
Loss in iteration 170 : 0.3890097746922968
Loss in iteration 171 : 0.3891667638656077
Loss in iteration 172 : 0.3893242111299883
Loss in iteration 173 : 0.3894823874833784
Loss in iteration 174 : 0.38964100933177476
Loss in iteration 175 : 0.38979962996185147
Loss in iteration 176 : 0.38995868664980543
Loss in iteration 177 : 0.39011790590359746
Loss in iteration 178 : 0.39027731572189844
Loss in iteration 179 : 0.39043679800019493
Loss in iteration 180 : 0.390596027869548
Loss in iteration 181 : 0.3907551823513543
Loss in iteration 182 : 0.3909143030222999
Loss in iteration 183 : 0.39107368860836206
Loss in iteration 184 : 0.39123317807210706
Loss in iteration 185 : 0.39139297535214146
Loss in iteration 186 : 0.3915529780480053
Loss in iteration 187 : 0.39171322135564535
Loss in iteration 188 : 0.3918736199364571
Loss in iteration 189 : 0.39203420917748477
Loss in iteration 190 : 0.3921951457155562
Loss in iteration 191 : 0.3923565020495228
Loss in iteration 192 : 0.3925182522211302
Loss in iteration 193 : 0.39268039480827055
Loss in iteration 194 : 0.3928428354908953
Loss in iteration 195 : 0.393005491496402
Loss in iteration 196 : 0.3931683387712776
Loss in iteration 197 : 0.39333138901685216
Loss in iteration 198 : 0.39349472332078395
Loss in iteration 199 : 0.3936579920329802
Loss in iteration 200 : 0.3938213874829071
Testing accuracy  of updater 6 on alg 1 with rate 0.002744 = 0.78525, training accuracy 0.8394302363224344, time elapsed: 3382 millisecond.
Loss in iteration 1 : 1.0000007471622976
Loss in iteration 2 : 0.7683675727752703
Loss in iteration 3 : 0.5546926807100246
Loss in iteration 4 : 0.5852630985756279
Loss in iteration 5 : 0.6426523986528748
Loss in iteration 6 : 0.6781850900011535
Loss in iteration 7 : 0.6920633454187971
Loss in iteration 8 : 0.6863899577934587
Loss in iteration 9 : 0.6634284206687553
Loss in iteration 10 : 0.6254014865375531
Loss in iteration 11 : 0.5750099759835501
Loss in iteration 12 : 0.51900169884836
Loss in iteration 13 : 0.47077838633374686
Loss in iteration 14 : 0.4407970134639665
Loss in iteration 15 : 0.4332563133399129
Loss in iteration 16 : 0.4444614350971221
Loss in iteration 17 : 0.46374448240695176
Loss in iteration 18 : 0.47385530687015537
Loss in iteration 19 : 0.4685451844735388
Loss in iteration 20 : 0.4504052862127735
Loss in iteration 21 : 0.42668511157773814
Loss in iteration 22 : 0.4055404429264546
Loss in iteration 23 : 0.3916165004021523
Loss in iteration 24 : 0.3871330183933125
Loss in iteration 25 : 0.38908061830023094
Loss in iteration 26 : 0.3940414190866091
Loss in iteration 27 : 0.39834615445791277
Loss in iteration 28 : 0.3999686553063081
Loss in iteration 29 : 0.398304464514246
Loss in iteration 30 : 0.39404911602619047
Loss in iteration 31 : 0.3886440564478938
Loss in iteration 32 : 0.3835011505827568
Loss in iteration 33 : 0.37951864593786344
Loss in iteration 34 : 0.3773070823148986
Loss in iteration 35 : 0.37681013617294623
Loss in iteration 36 : 0.37746907283623177
Loss in iteration 37 : 0.3789494462126648
Loss in iteration 38 : 0.38028289688676
Loss in iteration 39 : 0.38101203793301897
Loss in iteration 40 : 0.3809936903704643
Loss in iteration 41 : 0.38026232124607684
Loss in iteration 42 : 0.37910931565740164
Loss in iteration 43 : 0.3777764138580522
Loss in iteration 44 : 0.37659600469024385
Loss in iteration 45 : 0.37566761153292483
Loss in iteration 46 : 0.3751965372745335
Loss in iteration 47 : 0.37522415262573966
Loss in iteration 48 : 0.3755971632967168
Loss in iteration 49 : 0.37608930644619853
Loss in iteration 50 : 0.37647169394434277
Loss in iteration 51 : 0.376631207736352
Loss in iteration 52 : 0.37654967676450796
Loss in iteration 53 : 0.3762610342626925
Loss in iteration 54 : 0.3758756911885545
Loss in iteration 55 : 0.37548909634359523
Loss in iteration 56 : 0.37521386807150875
Loss in iteration 57 : 0.3750896341702109
Loss in iteration 58 : 0.37510258654487055
Loss in iteration 59 : 0.37520874216725997
Loss in iteration 60 : 0.37535107136936496
Loss in iteration 61 : 0.3754812557487115
Loss in iteration 62 : 0.37555609921391603
Loss in iteration 63 : 0.3755614792596419
Loss in iteration 64 : 0.3755031732365631
Loss in iteration 65 : 0.37540024749234197
Loss in iteration 66 : 0.3752908027132672
Loss in iteration 67 : 0.37521718175826607
Loss in iteration 68 : 0.37517275527858873
Loss in iteration 69 : 0.37517419043105266
Loss in iteration 70 : 0.3752081467135765
Loss in iteration 71 : 0.37524882613356897
Loss in iteration 72 : 0.3752946972188919
Loss in iteration 73 : 0.37533130354330685
Loss in iteration 74 : 0.375359200455538
Loss in iteration 75 : 0.37537215199867363
Loss in iteration 76 : 0.37537259387246535
Loss in iteration 77 : 0.3753639017461382
Loss in iteration 78 : 0.3753525356028423
Loss in iteration 79 : 0.37534502395788955
Loss in iteration 80 : 0.3753449877091074
Loss in iteration 81 : 0.3753558006481244
Loss in iteration 82 : 0.3753767828324368
Loss in iteration 83 : 0.37540356705697897
Loss in iteration 84 : 0.37543173303647603
Loss in iteration 85 : 0.3754602459598878
Loss in iteration 86 : 0.3754868055317968
Loss in iteration 87 : 0.37551121605242854
Loss in iteration 88 : 0.375531960903302
Loss in iteration 89 : 0.3755497009768847
Loss in iteration 90 : 0.3755656478117323
Loss in iteration 91 : 0.3755809332089502
Loss in iteration 92 : 0.375599720600695
Loss in iteration 93 : 0.3756189063475937
Loss in iteration 94 : 0.3756386700866687
Loss in iteration 95 : 0.37566077766213163
Loss in iteration 96 : 0.3756865269352597
Loss in iteration 97 : 0.3757143326303484
Loss in iteration 98 : 0.37574424134328976
Loss in iteration 99 : 0.37577525180431137
Loss in iteration 100 : 0.3758041341140214
Loss in iteration 101 : 0.3758314614868756
Loss in iteration 102 : 0.37585900131267086
Loss in iteration 103 : 0.37588646246106516
Loss in iteration 104 : 0.37591622863726576
Loss in iteration 105 : 0.37594732176331214
Loss in iteration 106 : 0.37597932543460516
Loss in iteration 107 : 0.37601158400986484
Loss in iteration 108 : 0.3760440369799944
Loss in iteration 109 : 0.37607673589895013
Loss in iteration 110 : 0.3761096915194663
Loss in iteration 111 : 0.37614291365577046
Loss in iteration 112 : 0.3761764111860539
Loss in iteration 113 : 0.37621019213924495
Loss in iteration 114 : 0.37624436434916203
Loss in iteration 115 : 0.3762790831691765
Loss in iteration 116 : 0.3763142052866326
Loss in iteration 117 : 0.3763499336235673
Loss in iteration 118 : 0.37638610107133474
Loss in iteration 119 : 0.37642265735237057
Loss in iteration 120 : 0.376459624945629
Loss in iteration 121 : 0.3764968848664417
Loss in iteration 122 : 0.376534442502117
Loss in iteration 123 : 0.376572448935163
Loss in iteration 124 : 0.37661107318762
Loss in iteration 125 : 0.37665006579887456
Loss in iteration 126 : 0.37668942506649866
Loss in iteration 127 : 0.37672914946362845
Loss in iteration 128 : 0.37676923761896997
Loss in iteration 129 : 0.37680968829906064
Loss in iteration 130 : 0.37685050039253076
Loss in iteration 131 : 0.37689167289614345
Loss in iteration 132 : 0.3769332952251111
Loss in iteration 133 : 0.3769752752805377
Loss in iteration 134 : 0.37701767028507577
Loss in iteration 135 : 0.37706056763870066
Loss in iteration 136 : 0.37710376525533185
Loss in iteration 137 : 0.3771472670504512
Loss in iteration 138 : 0.377191076572723
Loss in iteration 139 : 0.3772351970363782
Loss in iteration 140 : 0.37727963135081843
Loss in iteration 141 : 0.377324382147671
Loss in iteration 142 : 0.3773694518054995
Loss in iteration 143 : 0.37741484247236584
Loss in iteration 144 : 0.3774605560864204
Loss in iteration 145 : 0.3775066687831738
Loss in iteration 146 : 0.37755310914465706
Loss in iteration 147 : 0.37760012293980033
Loss in iteration 148 : 0.3776473599437094
Loss in iteration 149 : 0.3776948310537626
Loss in iteration 150 : 0.3777425448495748
Loss in iteration 151 : 0.3777906425565705
Loss in iteration 152 : 0.3778389898374844
Loss in iteration 153 : 0.37788819247065597
Loss in iteration 154 : 0.3779376964924761
Loss in iteration 155 : 0.37798724463913375
Loss in iteration 156 : 0.37803683352651474
Loss in iteration 157 : 0.3780868276430913
Loss in iteration 158 : 0.37813729565814147
Loss in iteration 159 : 0.3781880962184788
Loss in iteration 160 : 0.378239323934179
Loss in iteration 161 : 0.37829121302822777
Loss in iteration 162 : 0.3783433604069747
Loss in iteration 163 : 0.3783955885977405
Loss in iteration 164 : 0.3784481215945787
Loss in iteration 165 : 0.37850106016789964
Loss in iteration 166 : 0.3785543137558771
Loss in iteration 167 : 0.37860790974824265
Loss in iteration 168 : 0.37866198009923807
Loss in iteration 169 : 0.3787164355910598
Loss in iteration 170 : 0.37877112502904337
Loss in iteration 171 : 0.37882596096051996
Loss in iteration 172 : 0.3788813960727774
Loss in iteration 173 : 0.3789371335581871
Loss in iteration 174 : 0.3789931549112374
Loss in iteration 175 : 0.3790494451838282
Loss in iteration 176 : 0.37910600054569715
Loss in iteration 177 : 0.379162939151696
Loss in iteration 178 : 0.3792201168058262
Loss in iteration 179 : 0.3792775100319214
Loss in iteration 180 : 0.37933522650200635
Loss in iteration 181 : 0.37939322208737164
Loss in iteration 182 : 0.37945150033221003
Loss in iteration 183 : 0.37951006443582563
Loss in iteration 184 : 0.3795689172848302
Loss in iteration 185 : 0.3796280614823566
Loss in iteration 186 : 0.37968753288572893
Loss in iteration 187 : 0.3797474469516761
Loss in iteration 188 : 0.37980772008331015
Loss in iteration 189 : 0.3798682252979305
Loss in iteration 190 : 0.37992909604244884
Loss in iteration 191 : 0.37999034700989376
Loss in iteration 192 : 0.3800518134491392
Loss in iteration 193 : 0.3801135037530405
Loss in iteration 194 : 0.3801754255167799
Loss in iteration 195 : 0.3802375406566889
Loss in iteration 196 : 0.38030017908459807
Loss in iteration 197 : 0.38036292671491995
Loss in iteration 198 : 0.38042579932521514
Loss in iteration 199 : 0.380488813286342
Loss in iteration 200 : 0.3805521525232372
Testing accuracy  of updater 6 on alg 1 with rate 0.001568 = 0.78625, training accuracy 0.8400776950469407, time elapsed: 2766 millisecond.
Loss in iteration 1 : 1.0000000474112534
Loss in iteration 2 : 0.9426622412766195
Loss in iteration 3 : 0.8403799135200918
Loss in iteration 4 : 0.7102309368728249
Loss in iteration 5 : 0.6054379556728887
Loss in iteration 6 : 0.5566575911576412
Loss in iteration 7 : 0.5550053255093957
Loss in iteration 8 : 0.5719212810153287
Loss in iteration 9 : 0.5914312835395495
Loss in iteration 10 : 0.6075406974831417
Loss in iteration 11 : 0.6186470522458629
Testing accuracy  of updater 6 on alg 1 with rate 3.92E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 210 millisecond.
Loss in iteration 1 : 1.0347479224376732
Loss in iteration 2 : 13.892123017241707
Loss in iteration 3 : 16.830179793010128
Loss in iteration 4 : 14.942412267039574
Loss in iteration 5 : 10.59621994417665
Loss in iteration 6 : 5.02214056491137
Loss in iteration 7 : 1.9814317049581058
Loss in iteration 8 : 3.9221268276890595
Loss in iteration 9 : 5.832768092500863
Loss in iteration 10 : 4.644866562066402
Loss in iteration 11 : 2.4355647523595163
Loss in iteration 12 : 1.8206743147989266
Loss in iteration 13 : 2.332262723655431
Loss in iteration 14 : 2.7625474910867562
Loss in iteration 15 : 2.5777205892628023
Loss in iteration 16 : 1.9515076597949872
Loss in iteration 17 : 1.430381820895084
Loss in iteration 18 : 1.4304969377391872
Loss in iteration 19 : 1.675390985734078
Loss in iteration 20 : 1.4858591462432402
Loss in iteration 21 : 1.0730661632068044
Loss in iteration 22 : 1.0235206644496668
Loss in iteration 23 : 1.1252666466897872
Loss in iteration 24 : 0.9925018574479375
Loss in iteration 25 : 0.7609482978033955
Loss in iteration 26 : 0.8280791942759552
Loss in iteration 27 : 0.7673053329531376
Loss in iteration 28 : 0.6038019845509031
Loss in iteration 29 : 0.6850812775379884
Loss in iteration 30 : 0.5192816801706296
Loss in iteration 31 : 0.6308750688283831
Loss in iteration 32 : 0.612725907469848
Loss in iteration 33 : 0.4856704505517879
Loss in iteration 34 : 0.4662680031589172
Loss in iteration 35 : 0.5177625892651053
Loss in iteration 36 : 0.9223849920776593
Loss in iteration 37 : 1.5660691568048117
Loss in iteration 38 : 1.7315022845805939
Loss in iteration 39 : 0.7084122278361173
Loss in iteration 40 : 1.7660824442525362
Loss in iteration 41 : 0.6920320654146244
Loss in iteration 42 : 0.9704491843732777
Loss in iteration 43 : 1.074319793685694
Loss in iteration 44 : 0.6833417403324891
Loss in iteration 45 : 0.8735789111572789
Loss in iteration 46 : 0.8579745666208011
Loss in iteration 47 : 0.6679698300329
Loss in iteration 48 : 0.7783760249448282
Loss in iteration 49 : 0.7274248672574251
Loss in iteration 50 : 0.625853034174399
Loss in iteration 51 : 0.6893706112806841
Loss in iteration 52 : 0.5844835575876949
Loss in iteration 53 : 0.5912615352882241
Loss in iteration 54 : 0.550849778465537
Loss in iteration 55 : 0.544149754730953
Loss in iteration 56 : 0.48586004311803316
Loss in iteration 57 : 0.5512867547863868
Loss in iteration 58 : 0.5286415328496746
Loss in iteration 59 : 0.5148278313989804
Loss in iteration 60 : 0.5724281245858441
Loss in iteration 61 : 0.8149058819358003
Loss in iteration 62 : 0.4651732585024603
Loss in iteration 63 : 0.5494009439279557
Loss in iteration 64 : 0.7915297260915227
Loss in iteration 65 : 0.4765154435438659
Loss in iteration 66 : 0.6087527570286695
Loss in iteration 67 : 0.6418323468270218
Loss in iteration 68 : 0.49201617293834565
Loss in iteration 69 : 0.6015789975030722
Loss in iteration 70 : 0.533925828114164
Loss in iteration 71 : 0.5039693841424344
Loss in iteration 72 : 0.5515286310867665
Loss in iteration 73 : 0.48799152805321855
Loss in iteration 74 : 0.48460450655444653
Loss in iteration 75 : 0.5155102059739818
Loss in iteration 76 : 0.5063219823510366
Loss in iteration 77 : 0.47313012766479456
Loss in iteration 78 : 0.4535203448760462
Loss in iteration 79 : 0.44442330893357074
Loss in iteration 80 : 0.4412109361313656
Loss in iteration 81 : 0.43936027715104886
Loss in iteration 82 : 0.43973283992107637
Loss in iteration 83 : 0.4497416935134606
Loss in iteration 84 : 0.5781036179426005
Loss in iteration 85 : 0.9395210740131693
Loss in iteration 86 : 1.2402791227154415
Loss in iteration 87 : 1.2089355085952695
Loss in iteration 88 : 0.5399216797567483
Loss in iteration 89 : 0.9813294109916261
Loss in iteration 90 : 0.5549431705470316
Loss in iteration 91 : 0.7978766514280778
Loss in iteration 92 : 0.623183749063396
Loss in iteration 93 : 0.6646467077247247
Loss in iteration 94 : 0.6643863476828399
Loss in iteration 95 : 0.574805763148137
Loss in iteration 96 : 0.6285069156114484
Loss in iteration 97 : 0.5340298319750052
Loss in iteration 98 : 0.5822727444141027
Loss in iteration 99 : 0.49907946647291423
Loss in iteration 100 : 0.5401847087172309
Loss in iteration 101 : 0.5140683839090706
Loss in iteration 102 : 0.45660943492509787
Loss in iteration 103 : 0.46823030929733
Loss in iteration 104 : 0.5625590195956213
Loss in iteration 105 : 0.8942358227196773
Loss in iteration 106 : 0.45369566700189246
Loss in iteration 107 : 0.9492763427693395
Loss in iteration 108 : 1.3780176053647046
Loss in iteration 109 : 1.4417592182570718
Loss in iteration 110 : 0.5228633101017048
Loss in iteration 111 : 1.2318628477977906
Loss in iteration 112 : 0.5621839983562865
Loss in iteration 113 : 0.8877850715866987
Loss in iteration 114 : 0.7107181322005528
Loss in iteration 115 : 0.669149792741369
Loss in iteration 116 : 0.7528883109118458
Loss in iteration 117 : 0.590848457222957
Loss in iteration 118 : 0.6951839390654367
Loss in iteration 119 : 0.5745505513101429
Loss in iteration 120 : 0.6263969719399559
Loss in iteration 121 : 0.5382807516776279
Loss in iteration 122 : 0.5849517241469768
Loss in iteration 123 : 0.4921150373943879
Loss in iteration 124 : 0.5500201702669113
Loss in iteration 125 : 0.5560704985095889
Loss in iteration 126 : 0.48194968954061496
Loss in iteration 127 : 0.4562366695322859
Loss in iteration 128 : 0.4510267888367097
Loss in iteration 129 : 0.45655784318938003
Loss in iteration 130 : 0.5244710925777621
Loss in iteration 131 : 0.9689809384216864
Loss in iteration 132 : 0.4591757731392746
Loss in iteration 133 : 1.2576748935753634
Loss in iteration 134 : 1.2918283994916833
Loss in iteration 135 : 1.599507085354291
Loss in iteration 136 : 0.8139552647985517
Loss in iteration 137 : 0.9251025976989102
Loss in iteration 138 : 1.0630028604146766
Loss in iteration 139 : 0.6405553015363739
Loss in iteration 140 : 0.8814880264499114
Loss in iteration 141 : 0.8574588407059865
Loss in iteration 142 : 0.6613827537615871
Loss in iteration 143 : 0.7685414099553152
Loss in iteration 144 : 0.7361048979995817
Loss in iteration 145 : 0.6106984306253875
Loss in iteration 146 : 0.6781485893312696
Loss in iteration 147 : 0.5985894713702408
Loss in iteration 148 : 0.5785716510834746
Loss in iteration 149 : 0.5655183527265579
Loss in iteration 150 : 0.520302779373475
Loss in iteration 151 : 0.49662450085605914
Loss in iteration 152 : 0.5355381196315008
Loss in iteration 153 : 0.5121487360541692
Loss in iteration 154 : 0.47849767857180503
Loss in iteration 155 : 0.49506272734856116
Loss in iteration 156 : 0.6049998105655792
Loss in iteration 157 : 0.9036005129645507
Loss in iteration 158 : 0.5179395645007658
Loss in iteration 159 : 1.3299179542084707
Loss in iteration 160 : 0.9311307252811293
Loss in iteration 161 : 1.0239166251676395
Loss in iteration 162 : 0.5796228703053259
Loss in iteration 163 : 0.8989496116351665
Loss in iteration 164 : 0.6823376853035542
Loss in iteration 165 : 0.6840988866690362
Loss in iteration 166 : 0.7650646437762322
Loss in iteration 167 : 0.6444149494934521
Loss in iteration 168 : 0.6570329620290346
Loss in iteration 169 : 0.6614842200130243
Loss in iteration 170 : 0.5702527235888731
Loss in iteration 171 : 0.6108686832840615
Loss in iteration 172 : 0.5311095472374392
Loss in iteration 173 : 0.5492179955074876
Loss in iteration 174 : 0.483363601155989
Loss in iteration 175 : 0.5185324102348995
Loss in iteration 176 : 0.49278849442569556
Loss in iteration 177 : 0.47368557969310837
Loss in iteration 178 : 0.48179446746212873
Loss in iteration 179 : 0.6238864179696146
Loss in iteration 180 : 0.8293643807034896
Loss in iteration 181 : 1.2105979702601382
Loss in iteration 182 : 1.1266886881423936
Loss in iteration 183 : 0.5768869125839553
Loss in iteration 184 : 0.8549829451834468
Loss in iteration 185 : 0.5915049115897084
Loss in iteration 186 : 0.7658303230402257
Loss in iteration 187 : 0.5794006312443113
Loss in iteration 188 : 0.689367681771669
Loss in iteration 189 : 0.5951866524898114
Loss in iteration 190 : 0.601564650440496
Loss in iteration 191 : 0.5970494243672251
Loss in iteration 192 : 0.5441034524669208
Loss in iteration 193 : 0.5511882979107893
Loss in iteration 194 : 0.5092710371785039
Loss in iteration 195 : 0.4894131842716064
Loss in iteration 196 : 0.5143313867193096
Loss in iteration 197 : 0.5066422818289215
Loss in iteration 198 : 0.48305770790959063
Loss in iteration 199 : 0.5221523704975183
Loss in iteration 200 : 0.6990179419007777
Testing accuracy  of updater 7 on alg 1 with rate 0.56 = 0.502, training accuracy 0.6487536419553254, time elapsed: 3214 millisecond.
Loss in iteration 1 : 1.0013033613304745
Loss in iteration 2 : 2.683798826744388
Loss in iteration 3 : 3.8236918076225686
Loss in iteration 4 : 3.9673506935831533
Loss in iteration 5 : 3.283887842964989
Loss in iteration 6 : 1.9204272707639989
Loss in iteration 7 : 0.6831992017629899
Loss in iteration 8 : 1.4291370559358483
Loss in iteration 9 : 2.0779432919913488
Loss in iteration 10 : 1.4811915358106933
Loss in iteration 11 : 0.8466498709564289
Loss in iteration 12 : 0.9670627659152613
Loss in iteration 13 : 1.2887195506786457
Loss in iteration 14 : 1.3335327882027075
Loss in iteration 15 : 1.09130014794967
Loss in iteration 16 : 0.8603118030340077
Loss in iteration 17 : 0.8888626413297858
Loss in iteration 18 : 1.0368156031348728
Loss in iteration 19 : 0.9731417718618189
Loss in iteration 20 : 0.7855953900996568
Loss in iteration 21 : 0.7536703167834777
Loss in iteration 22 : 0.8186511275067972
Loss in iteration 23 : 0.7849578562871095
Loss in iteration 24 : 0.6632361698461946
Loss in iteration 25 : 0.646029480893487
Loss in iteration 26 : 0.6779037183589194
Loss in iteration 27 : 0.5796430379984688
Loss in iteration 28 : 0.56012430322537
Loss in iteration 29 : 0.5719753037026855
Loss in iteration 30 : 0.49384396128524516
Loss in iteration 31 : 0.5401287472975667
Loss in iteration 32 : 0.4658154143858533
Loss in iteration 33 : 0.4994112431817873
Loss in iteration 34 : 0.5119920279852646
Loss in iteration 35 : 0.4859434728411238
Loss in iteration 36 : 0.47320922345084093
Loss in iteration 37 : 0.4732390856094527
Loss in iteration 38 : 0.4833919165593069
Loss in iteration 39 : 0.5154503659920566
Loss in iteration 40 : 0.5218274004311942
Loss in iteration 41 : 0.4841435282159672
Loss in iteration 42 : 0.45055021395415745
Loss in iteration 43 : 0.4742280920930001
Loss in iteration 44 : 0.4568462642502299
Loss in iteration 45 : 0.4548007153834924
Loss in iteration 46 : 0.4606632118477312
Loss in iteration 47 : 0.44504213290980815
Loss in iteration 48 : 0.45435876975361666
Loss in iteration 49 : 0.44354714569853765
Loss in iteration 50 : 0.43972289752785454
Loss in iteration 51 : 0.44238712319559853
Loss in iteration 52 : 0.43655021995054827
Loss in iteration 53 : 0.433794254955409
Loss in iteration 54 : 0.4358963959180612
Loss in iteration 55 : 0.4365361719757065
Loss in iteration 56 : 0.43679644981107313
Loss in iteration 57 : 0.43330508469855755
Loss in iteration 58 : 0.43192858190929684
Loss in iteration 59 : 0.4308774792304411
Loss in iteration 60 : 0.43016105333533444
Loss in iteration 61 : 0.4301291622186974
Loss in iteration 62 : 0.43023190063553757
Loss in iteration 63 : 0.4299234582982325
Loss in iteration 64 : 0.42930528796444856
Testing accuracy  of updater 7 on alg 1 with rate 0.392 = 0.78725, training accuracy 0.8420200712204597, time elapsed: 864 millisecond.
Loss in iteration 1 : 1.0003646086895157
Loss in iteration 2 : 1.3752930120382223
Loss in iteration 3 : 1.806215935421444
Loss in iteration 4 : 1.7263215135785264
Loss in iteration 5 : 1.207539404854947
Loss in iteration 6 : 0.48477455651768464
Loss in iteration 7 : 0.7448706437587453
Loss in iteration 8 : 1.124034178597202
Loss in iteration 9 : 0.8819295925418896
Loss in iteration 10 : 0.5665993970746217
Loss in iteration 11 : 0.6147046176995171
Loss in iteration 12 : 0.7929298997010026
Loss in iteration 13 : 0.8363611468342175
Loss in iteration 14 : 0.7265981450705457
Loss in iteration 15 : 0.6114820095217183
Loss in iteration 16 : 0.6161034512229414
Loss in iteration 17 : 0.6993367791907732
Loss in iteration 18 : 0.7161123222677318
Loss in iteration 19 : 0.6414699035561132
Loss in iteration 20 : 0.5780003969036986
Loss in iteration 21 : 0.5958054990972197
Loss in iteration 22 : 0.6297813767827298
Loss in iteration 23 : 0.6066236021917305
Loss in iteration 24 : 0.5474751441218852
Loss in iteration 25 : 0.533575568910134
Loss in iteration 26 : 0.558419653599283
Loss in iteration 27 : 0.5402769052166478
Loss in iteration 28 : 0.49223659583201607
Loss in iteration 29 : 0.491390289176975
Loss in iteration 30 : 0.5015428197535207
Loss in iteration 31 : 0.4638141774569731
Loss in iteration 32 : 0.4634267667485294
Loss in iteration 33 : 0.4690197079597864
Loss in iteration 34 : 0.4396973241516808
Loss in iteration 35 : 0.46285921519004597
Loss in iteration 36 : 0.4407364685187754
Loss in iteration 37 : 0.46218866331325115
Loss in iteration 38 : 0.44169898080641573
Loss in iteration 39 : 0.46368225584596345
Loss in iteration 40 : 0.4487868916024127
Loss in iteration 41 : 0.45806229670887866
Loss in iteration 42 : 0.456084570996948
Loss in iteration 43 : 0.4478642441350297
Loss in iteration 44 : 0.45422561589052357
Loss in iteration 45 : 0.4414653343454795
Loss in iteration 46 : 0.4460946495149387
Loss in iteration 47 : 0.4397154231940107
Loss in iteration 48 : 0.4425790002182808
Loss in iteration 49 : 0.4383003818044882
Loss in iteration 50 : 0.4400371147436831
Loss in iteration 51 : 0.4378198539238712
Loss in iteration 52 : 0.4382654350423216
Loss in iteration 53 : 0.43681455406708247
Loss in iteration 54 : 0.43591402210692887
Loss in iteration 55 : 0.43572078144918786
Loss in iteration 56 : 0.4343760960692724
Loss in iteration 57 : 0.43409715975786767
Loss in iteration 58 : 0.4328156638390689
Loss in iteration 59 : 0.4326839016370198
Loss in iteration 60 : 0.43195962911798647
Loss in iteration 61 : 0.43159376702753083
Loss in iteration 62 : 0.4310843670951515
Loss in iteration 63 : 0.43074697289009284
Loss in iteration 64 : 0.43063344181319485
Loss in iteration 65 : 0.4303144006850765
Loss in iteration 66 : 0.43020797808564204
Loss in iteration 67 : 0.4299259709232487
Loss in iteration 68 : 0.42986079952644657
Testing accuracy  of updater 7 on alg 1 with rate 0.224 = 0.78775, training accuracy 0.842667529944966, time elapsed: 921 millisecond.
Loss in iteration 1 : 1.0000222183438285
Loss in iteration 2 : 0.6079359072933146
Loss in iteration 3 : 0.726902343162569
Loss in iteration 4 : 0.7234929522494153
Loss in iteration 5 : 0.6094942249833415
Loss in iteration 6 : 0.4423180665819303
Loss in iteration 7 : 0.45232273619334057
Loss in iteration 8 : 0.5411792545209092
Loss in iteration 9 : 0.49323172651120506
Loss in iteration 10 : 0.3973229910195428
Loss in iteration 11 : 0.38452279231831327
Loss in iteration 12 : 0.43016339067612425
Loss in iteration 13 : 0.45068015660470273
Loss in iteration 14 : 0.4266386735740289
Loss in iteration 15 : 0.39414433963702533
Loss in iteration 16 : 0.38816633428564623
Loss in iteration 17 : 0.4064620819750182
Loss in iteration 18 : 0.4236014316954212
Loss in iteration 19 : 0.42416543059544093
Loss in iteration 20 : 0.41180861955819087
Loss in iteration 21 : 0.39972397303414037
Loss in iteration 22 : 0.39933155730884845
Loss in iteration 23 : 0.40776116790862843
Loss in iteration 24 : 0.41531155704827655
Loss in iteration 25 : 0.4147954890153288
Loss in iteration 26 : 0.4083505940185108
Loss in iteration 27 : 0.4028646852100742
Loss in iteration 28 : 0.40282521395354126
Loss in iteration 29 : 0.40718729703192974
Loss in iteration 30 : 0.41044381222212856
Loss in iteration 31 : 0.4086377151352439
Loss in iteration 32 : 0.4038979026523058
Loss in iteration 33 : 0.4010496499483358
Loss in iteration 34 : 0.4021070824820115
Loss in iteration 35 : 0.4045998314744942
Loss in iteration 36 : 0.4048421879214618
Loss in iteration 37 : 0.4023973362012171
Loss in iteration 38 : 0.40000114867922265
Loss in iteration 39 : 0.4002131716448637
Loss in iteration 40 : 0.4020319658372259
Loss in iteration 41 : 0.40230809682193763
Loss in iteration 42 : 0.40088997875573407
Loss in iteration 43 : 0.39980869679818976
Loss in iteration 44 : 0.400523895069165
Loss in iteration 45 : 0.4020084072677908
Loss in iteration 46 : 0.4022798302029848
Loss in iteration 47 : 0.4017534182245291
Loss in iteration 48 : 0.4020475810284958
Loss in iteration 49 : 0.40335059345455426
Loss in iteration 50 : 0.40443001803502315
Loss in iteration 51 : 0.4046663793848919
Loss in iteration 52 : 0.4048699334707012
Loss in iteration 53 : 0.40573803795760577
Loss in iteration 54 : 0.40693268082880124
Loss in iteration 55 : 0.4077281255943048
Loss in iteration 56 : 0.40816779292544614
Loss in iteration 57 : 0.40884894104917285
Loss in iteration 58 : 0.4098959810983222
Loss in iteration 59 : 0.41070677270304085
Loss in iteration 60 : 0.41114617689324745
Loss in iteration 61 : 0.41167215846895566
Loss in iteration 62 : 0.41246560745067884
Loss in iteration 63 : 0.41318681788044637
Loss in iteration 64 : 0.4137111673608531
Loss in iteration 65 : 0.41421278202593204
Loss in iteration 66 : 0.4148364718591361
Loss in iteration 67 : 0.41548733091586854
Loss in iteration 68 : 0.4159445863357212
Loss in iteration 69 : 0.41637699351516505
Loss in iteration 70 : 0.41692438284674455
Loss in iteration 71 : 0.41747176042856443
Loss in iteration 72 : 0.4179239752223484
Loss in iteration 73 : 0.4183903514616307
Loss in iteration 74 : 0.4188698284327741
Loss in iteration 75 : 0.41933579100649965
Loss in iteration 76 : 0.41975883557468024
Loss in iteration 77 : 0.4201525782899839
Loss in iteration 78 : 0.42054344789854314
Loss in iteration 79 : 0.4209436602647266
Loss in iteration 80 : 0.42133309573674005
Loss in iteration 81 : 0.4216858348164558
Loss in iteration 82 : 0.4220292327068803
Loss in iteration 83 : 0.42237741334922785
Loss in iteration 84 : 0.42270827987611514
Loss in iteration 85 : 0.4230108691500507
Loss in iteration 86 : 0.4232904623354539
Loss in iteration 87 : 0.42357530814373706
Loss in iteration 88 : 0.42385709734092686
Loss in iteration 89 : 0.42412131704867134
Loss in iteration 90 : 0.4243703921174838
Loss in iteration 91 : 0.42462406271777453
Loss in iteration 92 : 0.4248626083822715
Loss in iteration 93 : 0.4250801403655695
Loss in iteration 94 : 0.42528594513729817
Loss in iteration 95 : 0.4254904484657686
Loss in iteration 96 : 0.42569202105871445
Loss in iteration 97 : 0.4258795699285775
Loss in iteration 98 : 0.42605838526819917
Loss in iteration 99 : 0.426234935724265
Loss in iteration 100 : 0.4263999309324823
Loss in iteration 101 : 0.4265539795391197
Loss in iteration 102 : 0.42670515538884557
Loss in iteration 103 : 0.4268583301681911
Loss in iteration 104 : 0.42700847374173756
Testing accuracy  of updater 7 on alg 1 with rate 0.056 = 0.78725, training accuracy 0.8413726124959534, time elapsed: 1932 millisecond.
Loss in iteration 1 : 1.0000120826799765
Loss in iteration 2 : 0.5611741060853074
Loss in iteration 3 : 0.6698602322912647
Loss in iteration 4 : 0.7158153148715066
Loss in iteration 5 : 0.6790729471771542
Loss in iteration 6 : 0.570371793100706
Loss in iteration 7 : 0.4504082076843226
Loss in iteration 8 : 0.449472108327383
Loss in iteration 9 : 0.5197772101724655
Loss in iteration 10 : 0.5033651008617483
Loss in iteration 11 : 0.4261349438279596
Loss in iteration 12 : 0.38227815309879
Loss in iteration 13 : 0.40015558239592336
Loss in iteration 14 : 0.4291727156707487
Loss in iteration 15 : 0.42912749473244477
Loss in iteration 16 : 0.40504155396665537
Loss in iteration 17 : 0.38417585000159404
Loss in iteration 18 : 0.3813412516459775
Loss in iteration 19 : 0.3938223559454748
Loss in iteration 20 : 0.4061299925240287
Loss in iteration 21 : 0.40780070079124786
Loss in iteration 22 : 0.39981372403524224
Loss in iteration 23 : 0.39062640797723003
Loss in iteration 24 : 0.3874317363758291
Loss in iteration 25 : 0.3915960835314273
Loss in iteration 26 : 0.39784346928514414
Loss in iteration 27 : 0.4012711985663774
Loss in iteration 28 : 0.39997525756123514
Loss in iteration 29 : 0.3960054372182301
Loss in iteration 30 : 0.39260082377785854
Loss in iteration 31 : 0.392176978222454
Loss in iteration 32 : 0.39485956016358514
Loss in iteration 33 : 0.3976023895628436
Loss in iteration 34 : 0.398183704686225
Loss in iteration 35 : 0.39640001143897013
Loss in iteration 36 : 0.39386674384391995
Loss in iteration 37 : 0.39301099527800404
Loss in iteration 38 : 0.39408354957279496
Loss in iteration 39 : 0.39550839377812014
Loss in iteration 40 : 0.39611920196917294
Loss in iteration 41 : 0.3955904644206731
Loss in iteration 42 : 0.39440127594364843
Loss in iteration 43 : 0.3936537670475193
Loss in iteration 44 : 0.39428015291284246
Loss in iteration 45 : 0.3953428818683073
Loss in iteration 46 : 0.395885741883049
Loss in iteration 47 : 0.3956379559966604
Loss in iteration 48 : 0.395084362654642
Loss in iteration 49 : 0.394994544890865
Loss in iteration 50 : 0.39559638324944063
Loss in iteration 51 : 0.3964757711756734
Loss in iteration 52 : 0.396959268705616
Loss in iteration 53 : 0.39703951984277513
Loss in iteration 54 : 0.39710587177439854
Loss in iteration 55 : 0.3975515832541126
Loss in iteration 56 : 0.39834095662176927
Loss in iteration 57 : 0.3990645380146427
Loss in iteration 58 : 0.39950092587466857
Loss in iteration 59 : 0.39977317030966364
Loss in iteration 60 : 0.4001425111979429
Loss in iteration 61 : 0.4007574422334369
Loss in iteration 62 : 0.401473561498196
Loss in iteration 63 : 0.4020908977974214
Loss in iteration 64 : 0.4025267334412593
Loss in iteration 65 : 0.402954829112845
Loss in iteration 66 : 0.40351853814352084
Loss in iteration 67 : 0.40414445278610517
Loss in iteration 68 : 0.4047330151749923
Loss in iteration 69 : 0.4052134875845992
Loss in iteration 70 : 0.4056408125042741
Loss in iteration 71 : 0.4061265646058382
Loss in iteration 72 : 0.40665325164408483
Loss in iteration 73 : 0.4071901387217919
Loss in iteration 74 : 0.4076930336080846
Loss in iteration 75 : 0.4081538310881569
Loss in iteration 76 : 0.40860188879671316
Loss in iteration 77 : 0.4090717085217091
Loss in iteration 78 : 0.40955743060915994
Loss in iteration 79 : 0.410021258076328
Loss in iteration 80 : 0.41044323767528823
Loss in iteration 81 : 0.41083568265695347
Loss in iteration 82 : 0.41122458676084916
Loss in iteration 83 : 0.4116370352346433
Loss in iteration 84 : 0.4120545069951468
Loss in iteration 85 : 0.41246094997698374
Loss in iteration 86 : 0.41285457469458176
Loss in iteration 87 : 0.4132403399718214
Loss in iteration 88 : 0.41361736983892633
Loss in iteration 89 : 0.4140000159514503
Loss in iteration 90 : 0.4143702530919626
Loss in iteration 91 : 0.41472203998141605
Loss in iteration 92 : 0.41505332537057593
Loss in iteration 93 : 0.41538352331167055
Loss in iteration 94 : 0.4157181883817196
Loss in iteration 95 : 0.4160474831667811
Loss in iteration 96 : 0.4163705463492175
Loss in iteration 97 : 0.41668729632263546
Loss in iteration 98 : 0.41699828561977315
Loss in iteration 99 : 0.4173040324761067
Loss in iteration 100 : 0.41760716407888415
Loss in iteration 101 : 0.417905894785505
Loss in iteration 102 : 0.41819880822087047
Loss in iteration 103 : 0.41848110489703677
Loss in iteration 104 : 0.41875338464713724
Loss in iteration 105 : 0.4190165984225761
Loss in iteration 106 : 0.41927364224829033
Loss in iteration 107 : 0.4195280850661036
Loss in iteration 108 : 0.4197837498336236
Loss in iteration 109 : 0.4200387139576619
Loss in iteration 110 : 0.42029186930647505
Loss in iteration 111 : 0.42054179306536016
Loss in iteration 112 : 0.420789655090115
Loss in iteration 113 : 0.4210353196392961
Loss in iteration 114 : 0.42127623011094945
Loss in iteration 115 : 0.42151121295771504
Loss in iteration 116 : 0.4217378132984114
Loss in iteration 117 : 0.4219608697721864
Loss in iteration 118 : 0.422182766781547
Loss in iteration 119 : 0.4224035243657364
Loss in iteration 120 : 0.42262115687811935
Loss in iteration 121 : 0.42283468456761836
Loss in iteration 122 : 0.4230443004242704
Loss in iteration 123 : 0.423251324685609
Loss in iteration 124 : 0.42345118232801826
Loss in iteration 125 : 0.42364794942393613
Loss in iteration 126 : 0.42384038571853877
Loss in iteration 127 : 0.42402737088582165
Loss in iteration 128 : 0.42421121102737475
Loss in iteration 129 : 0.4243898569006855
Loss in iteration 130 : 0.42456134845316706
Loss in iteration 131 : 0.42472892717811667
Loss in iteration 132 : 0.42489080511906024
Loss in iteration 133 : 0.425047174121659
Loss in iteration 134 : 0.42519794945286293
Loss in iteration 135 : 0.4253432454408229
Testing accuracy  of updater 7 on alg 1 with rate 0.0392 = 0.78725, training accuracy 0.8413726124959534, time elapsed: 1855 millisecond.
Loss in iteration 1 : 1.0000041015033094
Loss in iteration 2 : 0.6424883609081641
Loss in iteration 3 : 0.6158068029818442
Loss in iteration 4 : 0.7144531287613854
Loss in iteration 5 : 0.7598777258579903
Loss in iteration 6 : 0.755935759557589
Loss in iteration 7 : 0.7075696608190135
Loss in iteration 8 : 0.619982209630559
Loss in iteration 9 : 0.5081112132432501
Loss in iteration 10 : 0.4334205758070226
Loss in iteration 11 : 0.4460622736237232
Loss in iteration 12 : 0.508689061235464
Loss in iteration 13 : 0.5292939585000103
Loss in iteration 14 : 0.4916280031206717
Loss in iteration 15 : 0.43020397672030375
Loss in iteration 16 : 0.38835488184082706
Loss in iteration 17 : 0.38654030299718406
Loss in iteration 18 : 0.4087657822194469
Loss in iteration 19 : 0.4270740873102815
Loss in iteration 20 : 0.42796753576955043
Loss in iteration 21 : 0.4126486796932543
Loss in iteration 22 : 0.3933245996571805
Loss in iteration 23 : 0.38042187414345713
Loss in iteration 24 : 0.37862752230577407
Loss in iteration 25 : 0.38558817070512597
Loss in iteration 26 : 0.39452723944264495
Loss in iteration 27 : 0.3992200606183652
Loss in iteration 28 : 0.397749338191086
Loss in iteration 29 : 0.39182815617398475
Loss in iteration 30 : 0.38517800144892234
Loss in iteration 31 : 0.38123767862178126
Loss in iteration 32 : 0.3815033523579203
Loss in iteration 33 : 0.38456428992909425
Loss in iteration 34 : 0.38817862864385955
Loss in iteration 35 : 0.39018388221538397
Loss in iteration 36 : 0.38983991553634856
Loss in iteration 37 : 0.38769475531006475
Loss in iteration 38 : 0.3851688699641506
Loss in iteration 39 : 0.38361756662117524
Loss in iteration 40 : 0.3836333803724267
Loss in iteration 41 : 0.3850486794398209
Loss in iteration 42 : 0.3867570532436091
Loss in iteration 43 : 0.38773705091522565
Loss in iteration 44 : 0.38758332202822887
Loss in iteration 45 : 0.38659932124550034
Loss in iteration 46 : 0.3855250104467577
Loss in iteration 47 : 0.3848973773634749
Loss in iteration 48 : 0.3851308147403546
Loss in iteration 49 : 0.3859132967421995
Loss in iteration 50 : 0.38667763642201886
Loss in iteration 51 : 0.38710130372465024
Loss in iteration 52 : 0.38701012648031646
Loss in iteration 53 : 0.3866665127736669
Loss in iteration 54 : 0.38638755469227665
Loss in iteration 55 : 0.38645960714108973
Loss in iteration 56 : 0.3868664803135908
Loss in iteration 57 : 0.3874145056252595
Loss in iteration 58 : 0.38786002353075855
Loss in iteration 59 : 0.3881139453503431
Loss in iteration 60 : 0.3881945213896204
Loss in iteration 61 : 0.3881686021937575
Loss in iteration 62 : 0.38822996987993486
Loss in iteration 63 : 0.388456877417211
Loss in iteration 64 : 0.3888270519572297
Loss in iteration 65 : 0.38924494152945
Loss in iteration 66 : 0.3896109854579148
Loss in iteration 67 : 0.3898860321058998
Loss in iteration 68 : 0.3900930797351677
Loss in iteration 69 : 0.3903042788607385
Loss in iteration 70 : 0.3905467299131795
Loss in iteration 71 : 0.3908834623683301
Loss in iteration 72 : 0.39127472399736035
Loss in iteration 73 : 0.3916574373268365
Loss in iteration 74 : 0.39199111025590117
Loss in iteration 75 : 0.392273912917588
Loss in iteration 76 : 0.39254044978397107
Loss in iteration 77 : 0.39283515063032715
Loss in iteration 78 : 0.39316585393884035
Loss in iteration 79 : 0.3935241029789255
Loss in iteration 80 : 0.39388219048086626
Loss in iteration 81 : 0.3942265160044487
Loss in iteration 82 : 0.39455045520363974
Loss in iteration 83 : 0.3948642170717577
Loss in iteration 84 : 0.39517386820085837
Loss in iteration 85 : 0.3955090042857373
Loss in iteration 86 : 0.3958555267938569
Loss in iteration 87 : 0.39619752621487514
Loss in iteration 88 : 0.39652914725880106
Loss in iteration 89 : 0.3968490026134609
Loss in iteration 90 : 0.3971661779108315
Loss in iteration 91 : 0.39748074515855736
Loss in iteration 92 : 0.3977981614398666
Loss in iteration 93 : 0.3981217082164274
Loss in iteration 94 : 0.39844390980359573
Loss in iteration 95 : 0.39876072692165304
Loss in iteration 96 : 0.39907161210982295
Loss in iteration 97 : 0.39937897723775934
Loss in iteration 98 : 0.39968785404122104
Loss in iteration 99 : 0.3999985040243746
Loss in iteration 100 : 0.40030956883703667
Loss in iteration 101 : 0.40061829540280014
Loss in iteration 102 : 0.4009235200470516
Loss in iteration 103 : 0.4012268552671673
Loss in iteration 104 : 0.4015288763526677
Loss in iteration 105 : 0.4018308961439581
Loss in iteration 106 : 0.40213208246280563
Loss in iteration 107 : 0.4024323866759864
Loss in iteration 108 : 0.4027320451103978
Loss in iteration 109 : 0.4030299638509456
Loss in iteration 110 : 0.40332673943596087
Loss in iteration 111 : 0.40362275609962434
Loss in iteration 112 : 0.40391828039346894
Loss in iteration 113 : 0.4042130338352714
Loss in iteration 114 : 0.40450658103262177
Loss in iteration 115 : 0.40479906195164883
Loss in iteration 116 : 0.40509042052038685
Loss in iteration 117 : 0.40538030050553714
Loss in iteration 118 : 0.4056687672849196
Loss in iteration 119 : 0.4059558661355662
Loss in iteration 120 : 0.4062409558698593
Loss in iteration 121 : 0.4065245155105382
Loss in iteration 122 : 0.4068065300785584
Loss in iteration 123 : 0.4070871380640503
Loss in iteration 124 : 0.40736624512497716
Loss in iteration 125 : 0.40764301744307874
Loss in iteration 126 : 0.4079168338002428
Loss in iteration 127 : 0.4081880158832757
Loss in iteration 128 : 0.4084566157553832
Loss in iteration 129 : 0.4087229050478506
Loss in iteration 130 : 0.40898712945672394
Loss in iteration 131 : 0.40924951117080205
Loss in iteration 132 : 0.40950929324485635
Loss in iteration 133 : 0.40976609833055905
Loss in iteration 134 : 0.410020601397995
Loss in iteration 135 : 0.41027261362344863
Loss in iteration 136 : 0.41052154443759337
Loss in iteration 137 : 0.41076727000925684
Loss in iteration 138 : 0.4110098649471494
Loss in iteration 139 : 0.41124945992103845
Loss in iteration 140 : 0.4114863133966645
Loss in iteration 141 : 0.41172069926901167
Loss in iteration 142 : 0.41195231148996114
Loss in iteration 143 : 0.41218131332959707
Loss in iteration 144 : 0.41240741093070077
Loss in iteration 145 : 0.41263110815723936
Loss in iteration 146 : 0.4128523014449195
Loss in iteration 147 : 0.41307089456370577
Loss in iteration 148 : 0.4132871447827668
Loss in iteration 149 : 0.413501287148762
Loss in iteration 150 : 0.4137135342935465
Loss in iteration 151 : 0.41392407854664215
Loss in iteration 152 : 0.4141319904245723
Loss in iteration 153 : 0.41433752787239886
Loss in iteration 154 : 0.41454051472531284
Loss in iteration 155 : 0.414741159458218
Loss in iteration 156 : 0.4149396745804471
Loss in iteration 157 : 0.4151362748292013
Loss in iteration 158 : 0.415331154358031
Loss in iteration 159 : 0.4155244886950999
Loss in iteration 160 : 0.4157164365188729
Loss in iteration 161 : 0.41590714126755346
Loss in iteration 162 : 0.41609673259728
Loss in iteration 163 : 0.41628532770283233
Loss in iteration 164 : 0.41647303251344775
Loss in iteration 165 : 0.41665974874709283
Loss in iteration 166 : 0.4168458146991501
Loss in iteration 167 : 0.4170316031526592
Loss in iteration 168 : 0.4172163941467018
Loss in iteration 169 : 0.4174005903965047
Loss in iteration 170 : 0.4175842218535668
Loss in iteration 171 : 0.4177673524702947
Loss in iteration 172 : 0.417950040086139
Loss in iteration 173 : 0.41813233701576275
Loss in iteration 174 : 0.4183142905810841
Loss in iteration 175 : 0.41849530578494065
Loss in iteration 176 : 0.4186755823941751
Loss in iteration 177 : 0.4188550918431559
Loss in iteration 178 : 0.41903391772064863
Loss in iteration 179 : 0.4192121356160692
Loss in iteration 180 : 0.4193887501498246
Loss in iteration 181 : 0.4195639226596415
Loss in iteration 182 : 0.419737730167289
Loss in iteration 183 : 0.419911555634617
Loss in iteration 184 : 0.42008539708256576
Loss in iteration 185 : 0.42025887763625314
Loss in iteration 186 : 0.4204327019679975
Loss in iteration 187 : 0.4206063403030267
Loss in iteration 188 : 0.4207795812817571
Loss in iteration 189 : 0.42095251331189215
Loss in iteration 190 : 0.42112462010479584
Loss in iteration 191 : 0.4212958351561689
Loss in iteration 192 : 0.4214653921213182
Loss in iteration 193 : 0.4216323657118179
Loss in iteration 194 : 0.42179670729269325
Loss in iteration 195 : 0.42196058336873665
Loss in iteration 196 : 0.42212380521680587
Loss in iteration 197 : 0.4222863738192997
Loss in iteration 198 : 0.4224466316397625
Loss in iteration 199 : 0.4226057522820221
Loss in iteration 200 : 0.4227634801039274
Testing accuracy  of updater 7 on alg 1 with rate 0.022400000000000003 = 0.78725, training accuracy 0.8407251537714471, time elapsed: 2930 millisecond.
Loss in iteration 1 : 1.0000002886439627
Loss in iteration 2 : 0.9031077281671793
Loss in iteration 3 : 0.72512481373826
Loss in iteration 4 : 0.578633361296791
Loss in iteration 5 : 0.5728101991816629
Loss in iteration 6 : 0.6150486663585633
Loss in iteration 7 : 0.6509767847115381
Loss in iteration 8 : 0.6731252429295512
Loss in iteration 9 : 0.6820503656540485
Loss in iteration 10 : 0.6790107897720554
Loss in iteration 11 : 0.6652181525756173
Loss in iteration 12 : 0.6417943600191248
Loss in iteration 13 : 0.609965276415515
Loss in iteration 14 : 0.5718443710138719
Loss in iteration 15 : 0.5329048411296966
Loss in iteration 16 : 0.5014717614559525
Loss in iteration 17 : 0.48358328875858386
Loss in iteration 18 : 0.4788069795411666
Loss in iteration 19 : 0.482018083596783
Loss in iteration 20 : 0.48782414878909164
Loss in iteration 21 : 0.4922710585785425
Loss in iteration 22 : 0.49298310681599294
Loss in iteration 23 : 0.4887259613529846
Loss in iteration 24 : 0.4798413223450118
Loss in iteration 25 : 0.4676432987805408
Loss in iteration 26 : 0.45457500790517064
Loss in iteration 27 : 0.44233835203410543
Loss in iteration 28 : 0.4327639848363707
Loss in iteration 29 : 0.42609904886753425
Loss in iteration 30 : 0.4218750642940142
Loss in iteration 31 : 0.41992834767431453
Loss in iteration 32 : 0.41894253905831846
Loss in iteration 33 : 0.41800846118773904
Loss in iteration 34 : 0.4163998388084616
Loss in iteration 35 : 0.413925474055053
Loss in iteration 36 : 0.41066034490278214
Loss in iteration 37 : 0.40691850399924895
Loss in iteration 38 : 0.4030197949970942
Loss in iteration 39 : 0.3994025950733922
Loss in iteration 40 : 0.39632525323786527
Loss in iteration 41 : 0.39429681179068055
Loss in iteration 42 : 0.3933505206266998
Loss in iteration 43 : 0.3928990559977233
Loss in iteration 44 : 0.3926439100582512
Loss in iteration 45 : 0.3923719953213335
Loss in iteration 46 : 0.39186374631495646
Loss in iteration 47 : 0.39107354142422124
Loss in iteration 48 : 0.3900695862056701
Loss in iteration 49 : 0.38899242029624315
Loss in iteration 50 : 0.38790409412559035
Loss in iteration 51 : 0.3869313219622049
Loss in iteration 52 : 0.38615022317862013
Loss in iteration 53 : 0.3855996017226306
Loss in iteration 54 : 0.38521296356159523
Loss in iteration 55 : 0.38495218184435315
Loss in iteration 56 : 0.3847761843343503
Loss in iteration 57 : 0.3846320980285357
Loss in iteration 58 : 0.3844860242414772
Loss in iteration 59 : 0.38430287803771557
Loss in iteration 60 : 0.3840823731583536
Loss in iteration 61 : 0.38382131539182873
Loss in iteration 62 : 0.38354703107472166
Loss in iteration 63 : 0.38328068271487975
Loss in iteration 64 : 0.38304622690486595
Loss in iteration 65 : 0.38284692928069136
Loss in iteration 66 : 0.3826896447375294
Loss in iteration 67 : 0.38256301653273883
Loss in iteration 68 : 0.38247096451188795
Loss in iteration 69 : 0.382410624744385
Loss in iteration 70 : 0.3823607343453
Loss in iteration 71 : 0.3823129095361429
Loss in iteration 72 : 0.38226354189781536
Loss in iteration 73 : 0.38220789888806117
Loss in iteration 74 : 0.3821452028982553
Loss in iteration 75 : 0.3820801717169556
Loss in iteration 76 : 0.38201528262486434
Loss in iteration 77 : 0.3819585935846981
Loss in iteration 78 : 0.3819094586474454
Loss in iteration 79 : 0.3818677348353716
Loss in iteration 80 : 0.38183627812151544
Loss in iteration 81 : 0.3818133252067245
Loss in iteration 82 : 0.38179633219904074
Loss in iteration 83 : 0.3817846460413133
Loss in iteration 84 : 0.38177639151564585
Loss in iteration 85 : 0.38177272684302427
Loss in iteration 86 : 0.38177177350925307
Loss in iteration 87 : 0.38177282080862646
Loss in iteration 88 : 0.38177542830760774
Loss in iteration 89 : 0.38178097847647346
Loss in iteration 90 : 0.3817878340450361
Loss in iteration 91 : 0.38179577656210906
Loss in iteration 92 : 0.38180568462719555
Loss in iteration 93 : 0.38181719760576116
Loss in iteration 94 : 0.3818307584458821
Loss in iteration 95 : 0.3818463160464424
Loss in iteration 96 : 0.38186521225698766
Loss in iteration 97 : 0.3818864658022714
Loss in iteration 98 : 0.3819096528365702
Loss in iteration 99 : 0.3819339113246165
Loss in iteration 100 : 0.38195968818702286
Loss in iteration 101 : 0.381986171426884
Loss in iteration 102 : 0.3820140184953441
Loss in iteration 103 : 0.3820435270322766
Loss in iteration 104 : 0.38207439921614395
Loss in iteration 105 : 0.3821074759780276
Loss in iteration 106 : 0.38214212718193397
Loss in iteration 107 : 0.3821787956658942
Loss in iteration 108 : 0.3822175964728084
Loss in iteration 109 : 0.38225890586006206
Loss in iteration 110 : 0.3823019170972263
Loss in iteration 111 : 0.38234597100928164
Loss in iteration 112 : 0.382391874820319
Loss in iteration 113 : 0.3824391670733861
Loss in iteration 114 : 0.38248744146878
Loss in iteration 115 : 0.38253665372661516
Loss in iteration 116 : 0.38258676419379783
Loss in iteration 117 : 0.3826382502860143
Loss in iteration 118 : 0.38269121310093446
Loss in iteration 119 : 0.3827451598329059
Loss in iteration 120 : 0.3827998757448423
Loss in iteration 121 : 0.3828553035657888
Loss in iteration 122 : 0.3829114230067919
Loss in iteration 123 : 0.38296821602489245
Loss in iteration 124 : 0.38302579362738487
Loss in iteration 125 : 0.3830845233692851
Loss in iteration 126 : 0.3831439974618029
Loss in iteration 127 : 0.3832042823247074
Loss in iteration 128 : 0.3832652185798767
Loss in iteration 129 : 0.38332768352635255
Loss in iteration 130 : 0.3833914116326131
Loss in iteration 131 : 0.38345683529610425
Loss in iteration 132 : 0.38352370746778514
Loss in iteration 133 : 0.38359103270257405
Loss in iteration 134 : 0.3836585940756858
Loss in iteration 135 : 0.38372679992980213
Loss in iteration 136 : 0.3837965105899918
Loss in iteration 137 : 0.3838667217963527
Loss in iteration 138 : 0.3839374268864564
Loss in iteration 139 : 0.38400897755125435
Loss in iteration 140 : 0.38408122349615126
Loss in iteration 141 : 0.3841540064814318
Loss in iteration 142 : 0.38422731652446074
Loss in iteration 143 : 0.3843016025556273
Loss in iteration 144 : 0.3843764639219969
Loss in iteration 145 : 0.3844521069776585
Loss in iteration 146 : 0.3845286892017074
Loss in iteration 147 : 0.38460585795162744
Loss in iteration 148 : 0.38468331044854215
Loss in iteration 149 : 0.38476106183212777
Loss in iteration 150 : 0.3848391258855112
Loss in iteration 151 : 0.38491787151591805
Loss in iteration 152 : 0.3849970903396628
Loss in iteration 153 : 0.3850766268158301
Loss in iteration 154 : 0.38515685072793426
Loss in iteration 155 : 0.38523849296626683
Loss in iteration 156 : 0.38532052583458254
Loss in iteration 157 : 0.38540258832056495
Loss in iteration 158 : 0.3854849149825981
Loss in iteration 159 : 0.385567469423123
Loss in iteration 160 : 0.38565025986543844
Loss in iteration 161 : 0.38573328653397165
Loss in iteration 162 : 0.38581642860927173
Loss in iteration 163 : 0.38589968405874936
Loss in iteration 164 : 0.38598316725660553
Loss in iteration 165 : 0.38606688822708135
Loss in iteration 166 : 0.38615107659125647
Loss in iteration 167 : 0.386236278813641
Loss in iteration 168 : 0.3863220272811815
Loss in iteration 169 : 0.38640840193365306
Loss in iteration 170 : 0.386495151659979
Loss in iteration 171 : 0.38658227798874967
Loss in iteration 172 : 0.3866697824055557
Loss in iteration 173 : 0.3867576663497441
Loss in iteration 174 : 0.3868459312123361
Loss in iteration 175 : 0.3869345783348998
Loss in iteration 176 : 0.38702353332437633
Loss in iteration 177 : 0.3871130759224916
Loss in iteration 178 : 0.38720295324806697
Loss in iteration 179 : 0.387293133017395
Loss in iteration 180 : 0.3873835929349632
Loss in iteration 181 : 0.3874747048408735
Loss in iteration 182 : 0.387566089520688
Loss in iteration 183 : 0.3876577566906792
Loss in iteration 184 : 0.3877497152348109
Loss in iteration 185 : 0.38784193572719045
Loss in iteration 186 : 0.38793456888228256
Loss in iteration 187 : 0.3880274231782993
Loss in iteration 188 : 0.3881204863631495
Loss in iteration 189 : 0.3882139610906527
Loss in iteration 190 : 0.3883076648825486
Loss in iteration 191 : 0.38840161032579557
Loss in iteration 192 : 0.38849580889455765
Loss in iteration 193 : 0.38859027105029775
Loss in iteration 194 : 0.3886850063331245
Loss in iteration 195 : 0.3887800234451099
Loss in iteration 196 : 0.3888753303262507
Loss in iteration 197 : 0.38897093422370155
Loss in iteration 198 : 0.3890666092004809
Loss in iteration 199 : 0.3891626254900024
Loss in iteration 200 : 0.38925882019320207
Testing accuracy  of updater 7 on alg 1 with rate 0.005600000000000001 = 0.784, training accuracy 0.8400776950469407, time elapsed: 3471 millisecond.
Loss in iteration 1 : 1.0018477682881508
Loss in iteration 2 : 3.4593076068511026
Loss in iteration 3 : 3.3051106703393103
Loss in iteration 4 : 2.7895247962588177
Loss in iteration 5 : 2.0080981022668203
Loss in iteration 6 : 1.0432970267245247
Loss in iteration 7 : 0.5292002571607408
Loss in iteration 8 : 0.7138525122747603
Loss in iteration 9 : 0.8485044583089528
Loss in iteration 10 : 0.7738374386150244
Loss in iteration 11 : 0.6826043318912244
Loss in iteration 12 : 0.6352035681200292
Loss in iteration 13 : 0.6316508560205822
Loss in iteration 14 : 0.646355367813826
Loss in iteration 15 : 0.6567889345951919
Loss in iteration 16 : 0.6569141475659734
Loss in iteration 17 : 0.650685844761276
Loss in iteration 18 : 0.6431338228289344
Loss in iteration 19 : 0.6374887477038348
Loss in iteration 20 : 0.6325598535112765
Loss in iteration 21 : 0.6266046625046843
Loss in iteration 22 : 0.618324114893479
Loss in iteration 23 : 0.6084505706787088
Loss in iteration 24 : 0.5977520508877462
Loss in iteration 25 : 0.5866796645257251
Loss in iteration 26 : 0.5752402653844884
Loss in iteration 27 : 0.5631638624919258
Loss in iteration 28 : 0.5505365799913895
Loss in iteration 29 : 0.5376305211779013
Loss in iteration 30 : 0.5246292500491448
Loss in iteration 31 : 0.5115610041460489
Loss in iteration 32 : 0.49863570839149074
Loss in iteration 33 : 0.48621158366705364
Loss in iteration 34 : 0.474384072972544
Loss in iteration 35 : 0.46367001842492267
Loss in iteration 36 : 0.4546183898153213
Loss in iteration 37 : 0.4473540634818238
Loss in iteration 38 : 0.4434010154188412
Loss in iteration 39 : 0.44294138961606566
Loss in iteration 40 : 0.4467204734353308
Loss in iteration 41 : 0.4529462332361846
Loss in iteration 42 : 0.4593877515948123
Loss in iteration 43 : 0.46449659595963466
Loss in iteration 44 : 0.4675611777585008
Loss in iteration 45 : 0.4771685725930973
Loss in iteration 46 : 0.5724903814054239
Loss in iteration 47 : 0.8105198478595403
Loss in iteration 48 : 1.0753256347551787
Loss in iteration 49 : 0.6904143889929819
Loss in iteration 50 : 0.5380559295123002
Loss in iteration 51 : 0.5220990527011266
Loss in iteration 52 : 0.5212764149178429
Loss in iteration 53 : 0.5291676199553113
Loss in iteration 54 : 0.5353117382420458
Loss in iteration 55 : 0.5392900115523307
Loss in iteration 56 : 0.5413062289754049
Loss in iteration 57 : 0.5415964929490853
Loss in iteration 58 : 0.540280554664325
Loss in iteration 59 : 0.5376242515578997
Loss in iteration 60 : 0.5338363091018274
Loss in iteration 61 : 0.5291368212934442
Loss in iteration 62 : 0.5238828106199549
Loss in iteration 63 : 0.5183031472541777
Loss in iteration 64 : 0.5130950829046811
Loss in iteration 65 : 0.5097962026050656
Loss in iteration 66 : 0.5080627120937491
Loss in iteration 67 : 0.509344319700165
Loss in iteration 68 : 0.5161855224746061
Loss in iteration 69 : 0.5574569340415441
Loss in iteration 70 : 0.6564431769066393
Loss in iteration 71 : 0.9148701641903387
Loss in iteration 72 : 0.5117660337255926
Loss in iteration 73 : 0.5376757352759667
Loss in iteration 74 : 0.5455003263868076
Loss in iteration 75 : 0.5515690021611512
Loss in iteration 76 : 0.5589651004886997
Loss in iteration 77 : 0.5465255811255164
Loss in iteration 78 : 0.5415405336074752
Loss in iteration 79 : 0.5311653157894134
Loss in iteration 80 : 0.5262022822874423
Loss in iteration 81 : 0.5246513176275482
Loss in iteration 82 : 0.5228214340926916
Loss in iteration 83 : 0.5226035532151425
Loss in iteration 84 : 0.5230010021339508
Loss in iteration 85 : 0.5248974149147769
Loss in iteration 86 : 0.5377393015065872
Loss in iteration 87 : 0.5520944505588803
Loss in iteration 88 : 0.6172248567033704
Loss in iteration 89 : 0.6112346906731804
Loss in iteration 90 : 0.6738180425196671
Loss in iteration 91 : 0.5758400601623886
Loss in iteration 92 : 0.5687111473874714
Loss in iteration 93 : 0.5500603485455089
Loss in iteration 94 : 0.5401325709970948
Loss in iteration 95 : 0.5331377110838695
Loss in iteration 96 : 0.5291556731127272
Loss in iteration 97 : 0.5267160538435234
Loss in iteration 98 : 0.5254005940393052
Loss in iteration 99 : 0.5238495535438012
Loss in iteration 100 : 0.5221537486944067
Loss in iteration 101 : 0.5221530473107201
Loss in iteration 102 : 0.527614490544076
Loss in iteration 103 : 0.5421696857088725
Loss in iteration 104 : 0.6025521669539624
Loss in iteration 105 : 0.6403482962852189
Loss in iteration 106 : 0.7929401570633631
Loss in iteration 107 : 0.5559298476072251
Loss in iteration 108 : 0.5433676081134811
Loss in iteration 109 : 0.5446550538662748
Loss in iteration 110 : 0.5392773648659794
Loss in iteration 111 : 0.5361694109300098
Loss in iteration 112 : 0.5326867387086323
Loss in iteration 113 : 0.5304929290774454
Loss in iteration 114 : 0.5283999017013675
Loss in iteration 115 : 0.5269649539383974
Loss in iteration 116 : 0.5271735704520941
Loss in iteration 117 : 0.5296002381165769
Loss in iteration 118 : 0.5468737158731075
Loss in iteration 119 : 0.574888199853202
Loss in iteration 120 : 0.6836866593254606
Loss in iteration 121 : 0.6454625745956862
Loss in iteration 122 : 0.7273244128664234
Loss in iteration 123 : 0.5676872799951338
Loss in iteration 124 : 0.5489508899158366
Loss in iteration 125 : 0.5483127518233887
Loss in iteration 126 : 0.5433824122644219
Loss in iteration 127 : 0.5411971961980807
Loss in iteration 128 : 0.5399962630072176
Loss in iteration 129 : 0.5370455206016823
Loss in iteration 130 : 0.5354296523249746
Loss in iteration 131 : 0.5332953632314003
Loss in iteration 132 : 0.5316018292933717
Loss in iteration 133 : 0.53200885463506
Loss in iteration 134 : 0.5482558258513246
Loss in iteration 135 : 0.5814201914388033
Loss in iteration 136 : 0.7232090414924656
Loss in iteration 137 : 0.6654669914293672
Loss in iteration 138 : 0.7810556017411486
Loss in iteration 139 : 0.5740483183994582
Loss in iteration 140 : 0.5548918923025786
Loss in iteration 141 : 0.5561118831521037
Loss in iteration 142 : 0.5521121721261547
Loss in iteration 143 : 0.5507590743683841
Loss in iteration 144 : 0.5498212835991091
Loss in iteration 145 : 0.5469481877979976
Loss in iteration 146 : 0.5447878070981781
Loss in iteration 147 : 0.5414982591361445
Loss in iteration 148 : 0.5390583735981127
Loss in iteration 149 : 0.539422964659632
Loss in iteration 150 : 0.5483504127653281
Loss in iteration 151 : 0.5725942183648582
Loss in iteration 152 : 0.6818747398256098
Loss in iteration 153 : 0.7024957762512424
Loss in iteration 154 : 0.8773096019352831
Loss in iteration 155 : 0.5514401154082982
Loss in iteration 156 : 0.5533826901316502
Loss in iteration 157 : 0.5531500331973573
Loss in iteration 158 : 0.5543942607979726
Loss in iteration 159 : 0.5544789797109071
Loss in iteration 160 : 0.552197693412227
Loss in iteration 161 : 0.5505267504047208
Loss in iteration 162 : 0.5477795523789146
Loss in iteration 163 : 0.5455296736596387
Loss in iteration 164 : 0.5448485607348464
Loss in iteration 165 : 0.5527620027921352
Loss in iteration 166 : 0.5724466070146164
Loss in iteration 167 : 0.6635025768157028
Loss in iteration 168 : 0.6894307989143796
Loss in iteration 169 : 0.875133380817622
Loss in iteration 170 : 0.5578196610889341
Loss in iteration 171 : 0.5521588908663274
Loss in iteration 172 : 0.5548476386488435
Loss in iteration 173 : 0.555389252948056
Loss in iteration 174 : 0.5543095794145123
Loss in iteration 175 : 0.5533607567184009
Loss in iteration 176 : 0.5512359411482932
Loss in iteration 177 : 0.548538719356706
Loss in iteration 178 : 0.5479954862188212
Loss in iteration 179 : 0.5525115131801173
Loss in iteration 180 : 0.569746214345219
Loss in iteration 181 : 0.6369677678181711
Loss in iteration 182 : 0.6654595005083733
Loss in iteration 183 : 0.8452402751087182
Loss in iteration 184 : 0.5719705232965187
Loss in iteration 185 : 0.558727574948584
Loss in iteration 186 : 0.5653978520678347
Loss in iteration 187 : 0.5606033535869588
Loss in iteration 188 : 0.5570993231625531
Loss in iteration 189 : 0.5542800382852486
Loss in iteration 190 : 0.5517642205732558
Loss in iteration 191 : 0.5495120950249727
Loss in iteration 192 : 0.5483231408944989
Loss in iteration 193 : 0.5540733952470674
Loss in iteration 194 : 0.5658867234602769
Loss in iteration 195 : 0.6282622496218264
Loss in iteration 196 : 0.6514743727462289
Loss in iteration 197 : 0.8159674920278263
Loss in iteration 198 : 0.581763045693259
Loss in iteration 199 : 0.569969879322712
Loss in iteration 200 : 0.5705984770680215
Testing accuracy  of updater 8 on alg 1 with rate 0.056 = 0.77225, training accuracy 0.8355454839753965, time elapsed: 3416 millisecond.
Loss in iteration 1 : 1.0008529651698048
Loss in iteration 2 : 2.1257112435905454
Loss in iteration 3 : 2.267993512933001
Loss in iteration 4 : 2.0764154857346258
Loss in iteration 5 : 1.591644035509106
Loss in iteration 6 : 0.9095209221300199
Loss in iteration 7 : 0.5542678383868317
Loss in iteration 8 : 0.6380188458627298
Loss in iteration 9 : 0.7742337397957078
Loss in iteration 10 : 0.7817678759300144
Loss in iteration 11 : 0.7215176877226605
Loss in iteration 12 : 0.6763783946889449
Loss in iteration 13 : 0.6647330414732202
Loss in iteration 14 : 0.6746537180800156
Loss in iteration 15 : 0.6879986788651099
Loss in iteration 16 : 0.6937535383329292
Loss in iteration 17 : 0.6914154955294661
Loss in iteration 18 : 0.6843110261969143
Loss in iteration 19 : 0.6764583372545008
Loss in iteration 20 : 0.6696977164116882
Loss in iteration 21 : 0.6632085539044489
Loss in iteration 22 : 0.6558325360185899
Loss in iteration 23 : 0.6465361260609686
Loss in iteration 24 : 0.6354382342667397
Loss in iteration 25 : 0.6235095643828389
Loss in iteration 26 : 0.6112640980090912
Loss in iteration 27 : 0.5986361286953825
Loss in iteration 28 : 0.5857613594689977
Loss in iteration 29 : 0.572434823290406
Loss in iteration 30 : 0.5587115141493076
Loss in iteration 31 : 0.5449000683871523
Loss in iteration 32 : 0.5311734147067559
Loss in iteration 33 : 0.5176137044942883
Loss in iteration 34 : 0.5043090320168886
Loss in iteration 35 : 0.49167796155628296
Loss in iteration 36 : 0.479806886295689
Loss in iteration 37 : 0.4690223183265343
Loss in iteration 38 : 0.45966482080922555
Loss in iteration 39 : 0.4520877500843165
Loss in iteration 40 : 0.447102750546125
Loss in iteration 41 : 0.44521346382336563
Loss in iteration 42 : 0.44681936498251157
Loss in iteration 43 : 0.4512532913313135
Loss in iteration 44 : 0.45696210478004307
Loss in iteration 45 : 0.46215282194587193
Loss in iteration 46 : 0.46752231255702015
Loss in iteration 47 : 0.4776297089875872
Loss in iteration 48 : 0.5182647353285464
Loss in iteration 49 : 0.591920974032676
Loss in iteration 50 : 0.6970327774959436
Loss in iteration 51 : 0.46948272262218527
Loss in iteration 52 : 0.47364786516747304
Loss in iteration 53 : 0.4753553276029314
Loss in iteration 54 : 0.4793231125253321
Loss in iteration 55 : 0.48255851992730603
Loss in iteration 56 : 0.4848574342420728
Loss in iteration 57 : 0.4864351092570095
Loss in iteration 58 : 0.4872728150122744
Loss in iteration 59 : 0.4874742970555156
Loss in iteration 60 : 0.48724859178001745
Loss in iteration 61 : 0.48672539636284956
Loss in iteration 62 : 0.4861861215283714
Loss in iteration 63 : 0.485672676874434
Loss in iteration 64 : 0.4852545608412066
Loss in iteration 65 : 0.4849551068283767
Loss in iteration 66 : 0.48484901721729373
Loss in iteration 67 : 0.4848674289919833
Loss in iteration 68 : 0.48500150862432795
Loss in iteration 69 : 0.4853384145216963
Testing accuracy  of updater 8 on alg 1 with rate 0.0392 = 0.79025, training accuracy 0.840401424409194, time elapsed: 1049 millisecond.
Loss in iteration 1 : 1.0002607513946002
Loss in iteration 2 : 1.3605550327801217
Loss in iteration 3 : 1.4246737629756563
Loss in iteration 4 : 1.2943928613477085
Loss in iteration 5 : 0.9924022205955726
Loss in iteration 6 : 0.5686263131845275
Loss in iteration 7 : 0.41522234718611234
Loss in iteration 8 : 0.5078924899267802
Loss in iteration 9 : 0.5412798287032334
Loss in iteration 10 : 0.5102264473009642
Loss in iteration 11 : 0.47909927250207696
Loss in iteration 12 : 0.46820154854311463
Loss in iteration 13 : 0.47299965667739063
Loss in iteration 14 : 0.48179753240957546
Loss in iteration 15 : 0.4874304248230113
Loss in iteration 16 : 0.4885616434287554
Loss in iteration 17 : 0.4870300404638908
Loss in iteration 18 : 0.4848517061045355
Loss in iteration 19 : 0.4833777492200649
Loss in iteration 20 : 0.48235877455526993
Loss in iteration 21 : 0.48032845872625374
Loss in iteration 22 : 0.47730536143735025
Loss in iteration 23 : 0.4734563441890809
Loss in iteration 24 : 0.46929119560562255
Loss in iteration 25 : 0.46496068489784
Loss in iteration 26 : 0.46042705813452545
Loss in iteration 27 : 0.45564284157767837
Loss in iteration 28 : 0.450630450848406
Loss in iteration 29 : 0.4455354802606219
Loss in iteration 30 : 0.4403740807044174
Loss in iteration 31 : 0.43524896609264624
Loss in iteration 32 : 0.430285372405952
Loss in iteration 33 : 0.4255845930389391
Loss in iteration 34 : 0.4212798258191145
Loss in iteration 35 : 0.41725789642987166
Loss in iteration 36 : 0.4134822768399263
Loss in iteration 37 : 0.4101160265132105
Loss in iteration 38 : 0.40735715831434643
Loss in iteration 39 : 0.40541700565226113
Loss in iteration 40 : 0.4041312916969983
Loss in iteration 41 : 0.4037927792856215
Loss in iteration 42 : 0.40413539847532975
Loss in iteration 43 : 0.4052967017139313
Loss in iteration 44 : 0.4068922636434154
Loss in iteration 45 : 0.4090259844952946
Loss in iteration 46 : 0.41102910696757344
Loss in iteration 47 : 0.4130544500500627
Loss in iteration 48 : 0.4148788719586228
Loss in iteration 49 : 0.41648514373150436
Loss in iteration 50 : 0.4178380063404835
Loss in iteration 51 : 0.4190025383538819
Loss in iteration 52 : 0.4200100877446427
Loss in iteration 53 : 0.4209413988639163
Loss in iteration 54 : 0.42185374157924893
Loss in iteration 55 : 0.4228068298600434
Loss in iteration 56 : 0.42385293698559223
Loss in iteration 57 : 0.4249194595722705
Loss in iteration 58 : 0.4260215571512032
Loss in iteration 59 : 0.42713729686533397
Loss in iteration 60 : 0.4283165972131757
Loss in iteration 61 : 0.4295243859661434
Loss in iteration 62 : 0.4307298011713947
Loss in iteration 63 : 0.4319237928312017
Loss in iteration 64 : 0.433100658923422
Loss in iteration 65 : 0.43424852306521244
Loss in iteration 66 : 0.4353535940114597
Loss in iteration 67 : 0.43643769813685
Loss in iteration 68 : 0.4374757566677777
Loss in iteration 69 : 0.4384679369459569
Loss in iteration 70 : 0.43942813396807007
Loss in iteration 71 : 0.44035877515324195
Loss in iteration 72 : 0.44126380099951823
Loss in iteration 73 : 0.44213153842159336
Loss in iteration 74 : 0.44295551262723243
Loss in iteration 75 : 0.4437677042588837
Loss in iteration 76 : 0.44454707352668166
Loss in iteration 77 : 0.445298963301757
Loss in iteration 78 : 0.44602576754675594
Loss in iteration 79 : 0.4467657957776562
Loss in iteration 80 : 0.4474917943376938
Loss in iteration 81 : 0.44821173726889496
Loss in iteration 82 : 0.44891341712866883
Loss in iteration 83 : 0.44960611999859823
Loss in iteration 84 : 0.4502936460627439
Loss in iteration 85 : 0.4509771688021833
Loss in iteration 86 : 0.45164442355044315
Loss in iteration 87 : 0.4523291639162669
Loss in iteration 88 : 0.45297584455436024
Loss in iteration 89 : 0.45362675591611235
Loss in iteration 90 : 0.45425144994296224
Loss in iteration 91 : 0.4548730392006217
Loss in iteration 92 : 0.4554920100894138
Loss in iteration 93 : 0.45608892980633853
Loss in iteration 94 : 0.4566665702338435
Loss in iteration 95 : 0.4572423978331626
Loss in iteration 96 : 0.457806069888156
Loss in iteration 97 : 0.4583514883384055
Loss in iteration 98 : 0.45888863379616296
Loss in iteration 99 : 0.4594050859964826
Loss in iteration 100 : 0.4599050446210222
Loss in iteration 101 : 0.4603876380712727
Loss in iteration 102 : 0.4608537151803668
Loss in iteration 103 : 0.46130072706788294
Loss in iteration 104 : 0.4617380758766241
Loss in iteration 105 : 0.4621534663552543
Loss in iteration 106 : 0.4625596733463593
Loss in iteration 107 : 0.46294421809878183
Loss in iteration 108 : 0.46331420048723815
Loss in iteration 109 : 0.46367501600803035
Loss in iteration 110 : 0.4640183095136526
Loss in iteration 111 : 0.46435665727875614
Loss in iteration 112 : 0.4646871921561731
Loss in iteration 113 : 0.46500361652712935
Loss in iteration 114 : 0.4653175395578823
Loss in iteration 115 : 0.46562264125186925
Loss in iteration 116 : 0.465921575996193
Loss in iteration 117 : 0.4662075661644196
Loss in iteration 118 : 0.46648942308136243
Loss in iteration 119 : 0.466766207707752
Loss in iteration 120 : 0.4670342840039294
Loss in iteration 121 : 0.46729643223597406
Loss in iteration 122 : 0.46754894270956476
Loss in iteration 123 : 0.4677990001638721
Loss in iteration 124 : 0.46804127654804906
Loss in iteration 125 : 0.46827795966708935
Loss in iteration 126 : 0.4685050462379884
Loss in iteration 127 : 0.46873005133581214
Loss in iteration 128 : 0.4689475835053063
Loss in iteration 129 : 0.46916040896091576
Loss in iteration 130 : 0.4693688342126796
Loss in iteration 131 : 0.4695730086423362
Testing accuracy  of updater 8 on alg 1 with rate 0.0224 = 0.7885, training accuracy 0.8433149886694723, time elapsed: 2217 millisecond.
Loss in iteration 1 : 1.0000200586924028
Loss in iteration 2 : 0.6368073189998712
Loss in iteration 3 : 0.6548810116686454
Loss in iteration 4 : 0.6184062550434822
Loss in iteration 5 : 0.538564856223774
Loss in iteration 6 : 0.45844575618182676
Loss in iteration 7 : 0.42282930164847465
Loss in iteration 8 : 0.4197091457281157
Loss in iteration 9 : 0.41286133673710235
Loss in iteration 10 : 0.3975509512546114
Loss in iteration 11 : 0.38513244268681707
Loss in iteration 12 : 0.37941538675710057
Loss in iteration 13 : 0.3774385311941799
Loss in iteration 14 : 0.3769965087389947
Loss in iteration 15 : 0.3766353369707302
Loss in iteration 16 : 0.37620040884941564
Loss in iteration 17 : 0.37610941708171053
Loss in iteration 18 : 0.37625156720624103
Loss in iteration 19 : 0.3766031829512919
Loss in iteration 20 : 0.3771052242679074
Loss in iteration 21 : 0.37766355455762296
Loss in iteration 22 : 0.37815317387009295
Loss in iteration 23 : 0.37857529114218247
Loss in iteration 24 : 0.3789628637904578
Loss in iteration 25 : 0.3793322412764987
Loss in iteration 26 : 0.37963974962998276
Loss in iteration 27 : 0.3798909068025229
Loss in iteration 28 : 0.3800900722183076
Loss in iteration 29 : 0.3802406244605766
Loss in iteration 30 : 0.3803476054958481
Loss in iteration 31 : 0.38041445084196257
Loss in iteration 32 : 0.380449886750993
Loss in iteration 33 : 0.38045688612526213
Loss in iteration 34 : 0.3804393158635636
Loss in iteration 35 : 0.3803971205615224
Loss in iteration 36 : 0.3803355188209956
Loss in iteration 37 : 0.3802617609820634
Loss in iteration 38 : 0.38018331327673643
Loss in iteration 39 : 0.3801073770690818
Loss in iteration 40 : 0.3800387757571934
Loss in iteration 41 : 0.3799731167006833
Loss in iteration 42 : 0.37990982857708755
Loss in iteration 43 : 0.37986591747043175
Loss in iteration 44 : 0.3798365940916292
Loss in iteration 45 : 0.3798345314755036
Loss in iteration 46 : 0.3798490090216999
Loss in iteration 47 : 0.3798824702460914
Loss in iteration 48 : 0.3799365256119616
Loss in iteration 49 : 0.38000877255411936
Loss in iteration 50 : 0.38009307755458704
Loss in iteration 51 : 0.3801898015524187
Loss in iteration 52 : 0.3802991489780873
Loss in iteration 53 : 0.38042783725431284
Loss in iteration 54 : 0.3805795172945775
Loss in iteration 55 : 0.38075424521449536
Loss in iteration 56 : 0.38094355760729787
Loss in iteration 57 : 0.3811432787581403
Loss in iteration 58 : 0.3813527904127873
Loss in iteration 59 : 0.3815744926589903
Loss in iteration 60 : 0.38180531676749735
Loss in iteration 61 : 0.3820463155220658
Loss in iteration 62 : 0.38229673973901546
Loss in iteration 63 : 0.3825530186167921
Loss in iteration 64 : 0.38281628063252
Loss in iteration 65 : 0.38308726268407167
Loss in iteration 66 : 0.38336271920755516
Loss in iteration 67 : 0.3836434792620289
Loss in iteration 68 : 0.38392969522701226
Loss in iteration 69 : 0.38421872530836065
Loss in iteration 70 : 0.38451273265920605
Loss in iteration 71 : 0.3848096479023953
Loss in iteration 72 : 0.3851063027419133
Loss in iteration 73 : 0.3854060729050657
Loss in iteration 74 : 0.3857067531498911
Loss in iteration 75 : 0.38601070691598294
Loss in iteration 76 : 0.3863180458131419
Loss in iteration 77 : 0.3866276363733798
Loss in iteration 78 : 0.38693815315146446
Loss in iteration 79 : 0.3872501634733754
Loss in iteration 80 : 0.3875640531504191
Loss in iteration 81 : 0.3878797904456941
Loss in iteration 82 : 0.3881976397120258
Loss in iteration 83 : 0.3885185192579377
Loss in iteration 84 : 0.3888406302165917
Loss in iteration 85 : 0.3891642549154502
Loss in iteration 86 : 0.3894896469793807
Loss in iteration 87 : 0.38981673974567926
Loss in iteration 88 : 0.3901454976951084
Loss in iteration 89 : 0.39047602779026414
Loss in iteration 90 : 0.3908084408689788
Loss in iteration 91 : 0.3911426761485631
Loss in iteration 92 : 0.39147854043544683
Loss in iteration 93 : 0.39181589418415463
Loss in iteration 94 : 0.39215470985840917
Loss in iteration 95 : 0.3924948383912238
Loss in iteration 96 : 0.39283659126034665
Loss in iteration 97 : 0.3931801596302789
Loss in iteration 98 : 0.3935248306966601
Loss in iteration 99 : 0.3938701373543445
Loss in iteration 100 : 0.39421676264716626
Loss in iteration 101 : 0.3945641696055403
Loss in iteration 102 : 0.3949115588069916
Loss in iteration 103 : 0.39525982844439544
Loss in iteration 104 : 0.39560861142201176
Loss in iteration 105 : 0.39595727701619554
Loss in iteration 106 : 0.3963049571505234
Loss in iteration 107 : 0.39665240684040276
Loss in iteration 108 : 0.39700067773335773
Loss in iteration 109 : 0.39734950189490414
Loss in iteration 110 : 0.39769844390192055
Loss in iteration 111 : 0.3980454207132885
Loss in iteration 112 : 0.3983929973839678
Loss in iteration 113 : 0.3987396321533085
Loss in iteration 114 : 0.3990852193873404
Loss in iteration 115 : 0.3994307491270593
Loss in iteration 116 : 0.3997758965300532
Loss in iteration 117 : 0.40012372602160384
Loss in iteration 118 : 0.4004686401178485
Loss in iteration 119 : 0.4008142356257036
Loss in iteration 120 : 0.40115990097792387
Loss in iteration 121 : 0.4015057262740963
Loss in iteration 122 : 0.40185439957216795
Loss in iteration 123 : 0.40220161831462226
Loss in iteration 124 : 0.40255211250898343
Loss in iteration 125 : 0.4029016527321886
Loss in iteration 126 : 0.4032520136160128
Loss in iteration 127 : 0.4036027010645915
Loss in iteration 128 : 0.40395321118768596
Loss in iteration 129 : 0.40430424143165533
Loss in iteration 130 : 0.4046547035681509
Loss in iteration 131 : 0.4050049062583874
Loss in iteration 132 : 0.40535332367368715
Loss in iteration 133 : 0.4057011860861405
Loss in iteration 134 : 0.40604844254123523
Loss in iteration 135 : 0.4063955598788734
Loss in iteration 136 : 0.40674150611999027
Loss in iteration 137 : 0.40708782975811136
Loss in iteration 138 : 0.40743153613263283
Loss in iteration 139 : 0.4077749058948575
Loss in iteration 140 : 0.4081178510343763
Loss in iteration 141 : 0.4084603315717135
Loss in iteration 142 : 0.40880208164927956
Loss in iteration 143 : 0.40914228678607895
Loss in iteration 144 : 0.40948222478646396
Loss in iteration 145 : 0.4098208346738585
Loss in iteration 146 : 0.41015694742874054
Loss in iteration 147 : 0.41049127240815064
Loss in iteration 148 : 0.4108243284199578
Loss in iteration 149 : 0.4111561639290173
Loss in iteration 150 : 0.4114861485105968
Loss in iteration 151 : 0.4118152187373829
Loss in iteration 152 : 0.41214330079039774
Loss in iteration 153 : 0.4124705975012986
Loss in iteration 154 : 0.4127972924388601
Loss in iteration 155 : 0.41312355171014725
Loss in iteration 156 : 0.4134494313011584
Loss in iteration 157 : 0.41377509334671214
Loss in iteration 158 : 0.414100552053532
Loss in iteration 159 : 0.41442528600578094
Loss in iteration 160 : 0.41474964169300405
Loss in iteration 161 : 0.41507370724964865
Loss in iteration 162 : 0.41539908857543467
Loss in iteration 163 : 0.41572264726011765
Loss in iteration 164 : 0.41604611278005077
Loss in iteration 165 : 0.4163695230721531
Loss in iteration 166 : 0.4166920933677816
Loss in iteration 167 : 0.4170131847849067
Loss in iteration 168 : 0.4173337695874017
Loss in iteration 169 : 0.41765199593500885
Loss in iteration 170 : 0.41796938813870355
Loss in iteration 171 : 0.4182857494974467
Loss in iteration 172 : 0.4186022424434521
Loss in iteration 173 : 0.41891780229791653
Loss in iteration 174 : 0.41923381488896116
Loss in iteration 175 : 0.4195486981727113
Loss in iteration 176 : 0.4198616606114113
Loss in iteration 177 : 0.42017284920413217
Loss in iteration 178 : 0.4204827200482618
Loss in iteration 179 : 0.4207907735101633
Loss in iteration 180 : 0.4210974762985202
Loss in iteration 181 : 0.4214029863127435
Loss in iteration 182 : 0.42170699097833514
Loss in iteration 183 : 0.42200974933054985
Loss in iteration 184 : 0.42231235673543926
Loss in iteration 185 : 0.4226144524303338
Loss in iteration 186 : 0.422915652056443
Loss in iteration 187 : 0.42321629005429245
Loss in iteration 188 : 0.4235154274094256
Loss in iteration 189 : 0.42381330913196197
Loss in iteration 190 : 0.4241110295501821
Loss in iteration 191 : 0.42440835609752225
Loss in iteration 192 : 0.4247053534376103
Loss in iteration 193 : 0.42500097420998517
Loss in iteration 194 : 0.4252949455852251
Loss in iteration 195 : 0.4255880289675738
Loss in iteration 196 : 0.4258802248828429
Loss in iteration 197 : 0.4261716892684767
Loss in iteration 198 : 0.4264625630082387
Loss in iteration 199 : 0.4267529733762789
Loss in iteration 200 : 0.42704246581878846
Testing accuracy  of updater 8 on alg 1 with rate 0.0056 = 0.7865, training accuracy 0.8413726124959534, time elapsed: 3270 millisecond.
Loss in iteration 1 : 1.0000131135934796
Loss in iteration 2 : 0.587615494068765
Loss in iteration 3 : 0.6070773474603666
Loss in iteration 4 : 0.5811441091340517
Loss in iteration 5 : 0.5253154424932944
Loss in iteration 6 : 0.4747403288111597
Loss in iteration 7 : 0.4491477595332279
Loss in iteration 8 : 0.43498869946762003
Loss in iteration 9 : 0.4234987841814382
Loss in iteration 10 : 0.41142785862617
Loss in iteration 11 : 0.39831175630608384
Loss in iteration 12 : 0.38881321935908547
Loss in iteration 13 : 0.3834712053124223
Loss in iteration 14 : 0.38043561113116614
Loss in iteration 15 : 0.37867331382401637
Loss in iteration 16 : 0.3775232714916049
Loss in iteration 17 : 0.37658407647876135
Loss in iteration 18 : 0.37579566745445125
Loss in iteration 19 : 0.37532119465974695
Loss in iteration 20 : 0.37532141961657617
Loss in iteration 21 : 0.37546593783497045
Loss in iteration 22 : 0.37568378130172186
Loss in iteration 23 : 0.37591976405918237
Loss in iteration 24 : 0.37615230112009007
Loss in iteration 25 : 0.37637913052156097
Loss in iteration 26 : 0.376619600419988
Loss in iteration 27 : 0.3768660175686027
Loss in iteration 28 : 0.37709684073976957
Loss in iteration 29 : 0.3773095989121235
Loss in iteration 30 : 0.3775001831374619
Loss in iteration 31 : 0.377661191036155
Loss in iteration 32 : 0.3777923802807932
Loss in iteration 33 : 0.37789496810122514
Loss in iteration 34 : 0.3779760429050693
Loss in iteration 35 : 0.3780349413593384
Loss in iteration 36 : 0.3780758561308268
Loss in iteration 37 : 0.37810241227812436
Loss in iteration 38 : 0.37811462035865284
Loss in iteration 39 : 0.37811679650310737
Loss in iteration 40 : 0.378109579308945
Loss in iteration 41 : 0.3780960055716742
Loss in iteration 42 : 0.3780798186029112
Loss in iteration 43 : 0.3780628397982979
Loss in iteration 44 : 0.378045606722822
Loss in iteration 45 : 0.3780329976277186
Loss in iteration 46 : 0.37802795950587864
Loss in iteration 47 : 0.3780323280170013
Loss in iteration 48 : 0.37803925001933963
Loss in iteration 49 : 0.37804964008286995
Loss in iteration 50 : 0.3780631844600377
Loss in iteration 51 : 0.3780851003087932
Loss in iteration 52 : 0.378117845164341
Loss in iteration 53 : 0.37816356428243236
Loss in iteration 54 : 0.3782222057176152
Loss in iteration 55 : 0.37828965411627596
Loss in iteration 56 : 0.3783622043521132
Loss in iteration 57 : 0.3784399444029508
Loss in iteration 58 : 0.3785241126747192
Loss in iteration 59 : 0.3786196174768293
Loss in iteration 60 : 0.3787181303089666
Loss in iteration 61 : 0.3788231884696863
Loss in iteration 62 : 0.37893951905902745
Loss in iteration 63 : 0.3790629459074717
Loss in iteration 64 : 0.37919358333865916
Loss in iteration 65 : 0.37933212783949766
Loss in iteration 66 : 0.37947174286084034
Loss in iteration 67 : 0.3796157420069047
Loss in iteration 68 : 0.37976429915685267
Loss in iteration 69 : 0.3799169764892472
Loss in iteration 70 : 0.38007414016005353
Loss in iteration 71 : 0.3802341924858411
Loss in iteration 72 : 0.38039762962631113
Loss in iteration 73 : 0.38056347957402525
Loss in iteration 74 : 0.3807335576055757
Loss in iteration 75 : 0.380906130560677
Loss in iteration 76 : 0.3810815518774279
Loss in iteration 77 : 0.38126030208374756
Loss in iteration 78 : 0.3814415374549823
Loss in iteration 79 : 0.3816260497475792
Loss in iteration 80 : 0.3818119839906801
Loss in iteration 81 : 0.38199971588607495
Loss in iteration 82 : 0.3821900588084045
Loss in iteration 83 : 0.38238268554181315
Loss in iteration 84 : 0.38257636088288444
Loss in iteration 85 : 0.3827710977737023
Loss in iteration 86 : 0.3829678549559859
Loss in iteration 87 : 0.38316564813657567
Loss in iteration 88 : 0.38336443995266994
Loss in iteration 89 : 0.38356430199776037
Loss in iteration 90 : 0.3837652991741607
Loss in iteration 91 : 0.3839667534627574
Loss in iteration 92 : 0.3841707154686869
Loss in iteration 93 : 0.3843755639401674
Loss in iteration 94 : 0.3845813754319269
Loss in iteration 95 : 0.3847882192598316
Loss in iteration 96 : 0.38499615814924826
Loss in iteration 97 : 0.3852052488273619
Loss in iteration 98 : 0.385415542563941
Loss in iteration 99 : 0.3856270419433587
Loss in iteration 100 : 0.3858402299442139
Loss in iteration 101 : 0.3860548508974562
Loss in iteration 102 : 0.3862703755859126
Loss in iteration 103 : 0.3864866111798541
Loss in iteration 104 : 0.38670434020544886
Loss in iteration 105 : 0.38692318262957714
Loss in iteration 106 : 0.38714318525514446
Loss in iteration 107 : 0.38736439036802905
Loss in iteration 108 : 0.38758695636446083
Loss in iteration 109 : 0.3878104430287286
Loss in iteration 110 : 0.3880352920234115
Loss in iteration 111 : 0.38826078408285325
Loss in iteration 112 : 0.38848764058479535
Loss in iteration 113 : 0.3887154426945798
Loss in iteration 114 : 0.3889442421802058
Loss in iteration 115 : 0.3891737989451634
Loss in iteration 116 : 0.38940414669779033
Loss in iteration 117 : 0.3896353286477113
Loss in iteration 118 : 0.3898682490480151
Loss in iteration 119 : 0.3901020088064986
Loss in iteration 120 : 0.39033553603295656
Loss in iteration 121 : 0.39056970621110576
Loss in iteration 122 : 0.39080475356204486
Loss in iteration 123 : 0.3910400506097378
Loss in iteration 124 : 0.3912754472073256
Loss in iteration 125 : 0.39151169387604684
Loss in iteration 126 : 0.39174815454231476
Loss in iteration 127 : 0.391985028441601
Loss in iteration 128 : 0.3922222974355393
Loss in iteration 129 : 0.3924605605723321
Loss in iteration 130 : 0.3926993189783773
Loss in iteration 131 : 0.3929386058319964
Loss in iteration 132 : 0.3931786586665553
Loss in iteration 133 : 0.39341835527390956
Loss in iteration 134 : 0.39365869933298575
Loss in iteration 135 : 0.39389947102055134
Loss in iteration 136 : 0.3941405742554366
Loss in iteration 137 : 0.3943812847028398
Loss in iteration 138 : 0.3946224340781974
Loss in iteration 139 : 0.39486377771469705
Loss in iteration 140 : 0.39510520980147135
Loss in iteration 141 : 0.3953472399052519
Loss in iteration 142 : 0.3955896247882612
Loss in iteration 143 : 0.39583195229010004
Loss in iteration 144 : 0.3960738031487193
Loss in iteration 145 : 0.3963157310737634
Loss in iteration 146 : 0.3965580289206345
Loss in iteration 147 : 0.39680021930622345
Loss in iteration 148 : 0.3970428348103703
Loss in iteration 149 : 0.39728556604407633
Loss in iteration 150 : 0.39752715020076523
Loss in iteration 151 : 0.39776887161940905
Loss in iteration 152 : 0.3980103704859653
Loss in iteration 153 : 0.39825171858876185
Loss in iteration 154 : 0.39849284253263906
Loss in iteration 155 : 0.3987355821718927
Loss in iteration 156 : 0.3989771833965529
Loss in iteration 157 : 0.3992178184313157
Loss in iteration 158 : 0.399458297948876
Loss in iteration 159 : 0.39969873062021916
Loss in iteration 160 : 0.3999407694908428
Loss in iteration 161 : 0.40018146292292234
Loss in iteration 162 : 0.4004238751878032
Loss in iteration 163 : 0.4006655597812391
Loss in iteration 164 : 0.400907293860053
Loss in iteration 165 : 0.40114918654539466
Loss in iteration 166 : 0.40139130972681863
Loss in iteration 167 : 0.40163528077799326
Loss in iteration 168 : 0.40187879685378675
Loss in iteration 169 : 0.402122622956798
Loss in iteration 170 : 0.4023668436506038
Loss in iteration 171 : 0.402613224795624
Loss in iteration 172 : 0.4028586520800817
Loss in iteration 173 : 0.4031048216083736
Loss in iteration 174 : 0.4033507252951039
Loss in iteration 175 : 0.4035973120100362
Loss in iteration 176 : 0.40384437025105646
Loss in iteration 177 : 0.4040914828865973
Loss in iteration 178 : 0.4043392646469668
Loss in iteration 179 : 0.4045862053958836
Loss in iteration 180 : 0.4048334950549595
Loss in iteration 181 : 0.40508096155735684
Loss in iteration 182 : 0.40532739616378893
Loss in iteration 183 : 0.40557312113185
Loss in iteration 184 : 0.40581834856960164
Loss in iteration 185 : 0.4060632011508546
Loss in iteration 186 : 0.40630850954543896
Loss in iteration 187 : 0.4065528283541615
Loss in iteration 188 : 0.4067969611518954
Loss in iteration 189 : 0.4070411210529136
Loss in iteration 190 : 0.4072852227535037
Loss in iteration 191 : 0.407528089082737
Loss in iteration 192 : 0.4077706436658932
Loss in iteration 193 : 0.4080124019236524
Loss in iteration 194 : 0.408253755691377
Loss in iteration 195 : 0.4084951493200483
Loss in iteration 196 : 0.4087360735808328
Loss in iteration 197 : 0.40897678267681026
Loss in iteration 198 : 0.40921787944856014
Loss in iteration 199 : 0.4094583632125643
Loss in iteration 200 : 0.4096977855735693
Testing accuracy  of updater 8 on alg 1 with rate 0.00392 = 0.78675, training accuracy 0.8416963418582065, time elapsed: 3177 millisecond.
Loss in iteration 1 : 1.000005090744604
Loss in iteration 2 : 0.55154136401789
Loss in iteration 3 : 0.5728654569559782
Loss in iteration 4 : 0.5829536148088486
Loss in iteration 5 : 0.5676230833062338
Loss in iteration 6 : 0.5351383657501978
Loss in iteration 7 : 0.5024148230806881
Loss in iteration 8 : 0.4804351633825201
Loss in iteration 9 : 0.4683403837399937
Loss in iteration 10 : 0.4584777270313846
Loss in iteration 11 : 0.4476008782734105
Loss in iteration 12 : 0.4358349281441067
Loss in iteration 13 : 0.42409867364777726
Loss in iteration 14 : 0.4132874057789244
Loss in iteration 15 : 0.4038755102289786
Loss in iteration 16 : 0.3966970230404096
Loss in iteration 17 : 0.3915014784543927
Loss in iteration 18 : 0.3876865688244364
Loss in iteration 19 : 0.38483232816412344
Loss in iteration 20 : 0.3825095500485715
Loss in iteration 21 : 0.380713950965568
Loss in iteration 22 : 0.3793420299952952
Loss in iteration 23 : 0.378255607376762
Loss in iteration 24 : 0.37741061031094125
Loss in iteration 25 : 0.3767213392794145
Loss in iteration 26 : 0.37618976874112314
Loss in iteration 27 : 0.37576519151764215
Loss in iteration 28 : 0.37545036108995106
Loss in iteration 29 : 0.375247574261104
Loss in iteration 30 : 0.37514648218020724
Loss in iteration 31 : 0.37507861211904797
Loss in iteration 32 : 0.3750326940933176
Loss in iteration 33 : 0.3750158359950485
Loss in iteration 34 : 0.37502219451037266
Loss in iteration 35 : 0.37504937159847745
Loss in iteration 36 : 0.37508778359141
Loss in iteration 37 : 0.3751324001558641
Loss in iteration 38 : 0.37517505081370545
Loss in iteration 39 : 0.37521914908114695
Loss in iteration 40 : 0.3752641525369829
Loss in iteration 41 : 0.3753066615948241
Loss in iteration 42 : 0.37534774992160885
Loss in iteration 43 : 0.3753884555831568
Loss in iteration 44 : 0.37542705015190597
Loss in iteration 45 : 0.3754627650709265
Loss in iteration 46 : 0.37549544050429606
Loss in iteration 47 : 0.3755254570714473
Loss in iteration 48 : 0.3755547508995875
Loss in iteration 49 : 0.37558332432237684
Loss in iteration 50 : 0.3756098590380125
Loss in iteration 51 : 0.37563450986642505
Loss in iteration 52 : 0.3756582171839352
Loss in iteration 53 : 0.3756810142944038
Loss in iteration 54 : 0.3757029756764266
Loss in iteration 55 : 0.37572468348580557
Loss in iteration 56 : 0.3757477012302986
Loss in iteration 57 : 0.3757718089854569
Loss in iteration 58 : 0.37579695067818025
Loss in iteration 59 : 0.37582214851942336
Loss in iteration 60 : 0.3758485202398899
Loss in iteration 61 : 0.3758758792208636
Loss in iteration 62 : 0.3759038049346377
Loss in iteration 63 : 0.37593243156895695
Loss in iteration 64 : 0.3759627575990526
Loss in iteration 65 : 0.37599424959168226
Loss in iteration 66 : 0.3760278961250782
Loss in iteration 67 : 0.3760633477778631
Loss in iteration 68 : 0.3760998464579348
Loss in iteration 69 : 0.37613818974796814
Loss in iteration 70 : 0.3761780754968601
Loss in iteration 71 : 0.37621927966600976
Loss in iteration 72 : 0.3762619012495033
Loss in iteration 73 : 0.37630617536397476
Loss in iteration 74 : 0.3763517113204181
Loss in iteration 75 : 0.37639890333000026
Loss in iteration 76 : 0.3764481307780097
Loss in iteration 77 : 0.37649944169791844
Loss in iteration 78 : 0.376551864464913
Loss in iteration 79 : 0.37660578748102647
Loss in iteration 80 : 0.37666072143154805
Loss in iteration 81 : 0.3767167007327181
Loss in iteration 82 : 0.3767741610102697
Loss in iteration 83 : 0.3768325097775912
Loss in iteration 84 : 0.3768919482387577
Loss in iteration 85 : 0.3769535771344789
Loss in iteration 86 : 0.377015921024604
Loss in iteration 87 : 0.3770812630707996
Loss in iteration 88 : 0.3771477425468463
Loss in iteration 89 : 0.3772150937127378
Loss in iteration 90 : 0.37728324375713923
Loss in iteration 91 : 0.3773522918857912
Loss in iteration 92 : 0.3774225888344352
Loss in iteration 93 : 0.37749352880152814
Loss in iteration 94 : 0.3775653506722299
Loss in iteration 95 : 0.3776385799955112
Loss in iteration 96 : 0.37771252496999935
Loss in iteration 97 : 0.37778787755397863
Loss in iteration 98 : 0.37786335677014926
Loss in iteration 99 : 0.3779394872961494
Loss in iteration 100 : 0.37801614879182166
Loss in iteration 101 : 0.3780941559532405
Loss in iteration 102 : 0.3781727743794179
Loss in iteration 103 : 0.3782518264995648
Loss in iteration 104 : 0.37833154042770306
Loss in iteration 105 : 0.3784117937386423
Loss in iteration 106 : 0.3784926024751974
Loss in iteration 107 : 0.3785739811850684
Loss in iteration 108 : 0.37865623675790766
Loss in iteration 109 : 0.37873894228731286
Loss in iteration 110 : 0.37882234914257085
Loss in iteration 111 : 0.3789063683877145
Loss in iteration 112 : 0.37899130558698496
Loss in iteration 113 : 0.3790766415241416
Loss in iteration 114 : 0.3791629998162999
Loss in iteration 115 : 0.3792498888556887
Loss in iteration 116 : 0.3793376492787013
Loss in iteration 117 : 0.3794260484433952
Loss in iteration 118 : 0.3795152444370937
Loss in iteration 119 : 0.3796048898496493
Loss in iteration 120 : 0.3796953827343478
Loss in iteration 121 : 0.3797862790951316
Loss in iteration 122 : 0.379878091825863
Loss in iteration 123 : 0.37997040816464933
Loss in iteration 124 : 0.38006324286843546
Loss in iteration 125 : 0.38015656885880494
Loss in iteration 126 : 0.38025064058819413
Loss in iteration 127 : 0.3803451384616838
Loss in iteration 128 : 0.38044047602574144
Loss in iteration 129 : 0.3805362610052122
Loss in iteration 130 : 0.3806320903399072
Loss in iteration 131 : 0.38072821360324366
Loss in iteration 132 : 0.38082537085508866
Loss in iteration 133 : 0.3809230232218716
Loss in iteration 134 : 0.3810214596797736
Loss in iteration 135 : 0.3811202540036329
Loss in iteration 136 : 0.3812198095277826
Loss in iteration 137 : 0.3813198742460443
Loss in iteration 138 : 0.3814202540350566
Loss in iteration 139 : 0.3815213019092178
Loss in iteration 140 : 0.3816226570713893
Loss in iteration 141 : 0.38172428365538597
Loss in iteration 142 : 0.38182663496560537
Loss in iteration 143 : 0.3819293209863089
Loss in iteration 144 : 0.382032613887505
Loss in iteration 145 : 0.38213630760134426
Loss in iteration 146 : 0.3822404204531348
Loss in iteration 147 : 0.3823450212292955
Loss in iteration 148 : 0.38245000059275147
Loss in iteration 149 : 0.3825551094300078
Loss in iteration 150 : 0.382660381745861
Loss in iteration 151 : 0.38276638872971486
Loss in iteration 152 : 0.3828726153831405
Loss in iteration 153 : 0.38297890110821436
Loss in iteration 154 : 0.38308540807572317
Loss in iteration 155 : 0.3831923362191249
Loss in iteration 156 : 0.3832998049445844
Loss in iteration 157 : 0.3834078266287655
Loss in iteration 158 : 0.38351629178372193
Loss in iteration 159 : 0.38362504518053525
Loss in iteration 160 : 0.3837340855554014
Loss in iteration 161 : 0.3838440856921841
Loss in iteration 162 : 0.3839542005668306
Loss in iteration 163 : 0.3840648516518517
Loss in iteration 164 : 0.3841759522542198
Loss in iteration 165 : 0.384287512541204
Loss in iteration 166 : 0.38439935915745294
Loss in iteration 167 : 0.3845118019277705
Loss in iteration 168 : 0.3846247426458832
Loss in iteration 169 : 0.3847381788542888
Loss in iteration 170 : 0.3848520048505285
Loss in iteration 171 : 0.38496623333083596
Loss in iteration 172 : 0.3850813065619692
Loss in iteration 173 : 0.3851962674038677
Loss in iteration 174 : 0.38531203287469873
Loss in iteration 175 : 0.38542786999454254
Loss in iteration 176 : 0.3855445900446701
Loss in iteration 177 : 0.38566120776017
Loss in iteration 178 : 0.38577825691284406
Loss in iteration 179 : 0.38589556797633967
Loss in iteration 180 : 0.3860133308147786
Loss in iteration 181 : 0.38613124473169236
Loss in iteration 182 : 0.3862495162593616
Loss in iteration 183 : 0.38636805513335853
Loss in iteration 184 : 0.38648688416713006
Loss in iteration 185 : 0.3866059979378691
Loss in iteration 186 : 0.3867253743454516
Loss in iteration 187 : 0.3868451535132627
Loss in iteration 188 : 0.3869652530822125
Loss in iteration 189 : 0.38708578585756187
Loss in iteration 190 : 0.38720657796188623
Loss in iteration 191 : 0.3873276184664195
Loss in iteration 192 : 0.3874489607853968
Loss in iteration 193 : 0.3875709218918102
Loss in iteration 194 : 0.3876930908469914
Loss in iteration 195 : 0.3878161320475974
Loss in iteration 196 : 0.38793921122810565
Loss in iteration 197 : 0.38806262557263776
Loss in iteration 198 : 0.38818644470009783
Loss in iteration 199 : 0.38831064074092303
Loss in iteration 200 : 0.38843515343999885
Testing accuracy  of updater 8 on alg 1 with rate 0.0022400000000000002 = 0.78675, training accuracy 0.8397539656846876, time elapsed: 3268 millisecond.
Loss in iteration 1 : 1.0000003600637362
Loss in iteration 2 : 0.8237448240754376
Loss in iteration 3 : 0.6317455147671677
Loss in iteration 4 : 0.555863529731465
Loss in iteration 5 : 0.5556878320326176
Loss in iteration 6 : 0.5710189758166222
Loss in iteration 7 : 0.5836360127535858
Loss in iteration 8 : 0.5898538983201639
Loss in iteration 9 : 0.5892303363767201
Loss in iteration 10 : 0.5824486197927
Loss in iteration 11 : 0.5708040561496077
Loss in iteration 12 : 0.5556266547199022
Loss in iteration 13 : 0.5391479401791635
Loss in iteration 14 : 0.5235396001644587
Loss in iteration 15 : 0.5102974189914372
Loss in iteration 16 : 0.500833671848564
Loss in iteration 17 : 0.49508634094607457
Loss in iteration 18 : 0.4913963475049637
Loss in iteration 19 : 0.48892760212078856
Loss in iteration 20 : 0.4868056347442156
Loss in iteration 21 : 0.4844330703303755
Loss in iteration 22 : 0.48144338439399653
Loss in iteration 23 : 0.4777954599837781
Loss in iteration 24 : 0.47361843977854245
Loss in iteration 25 : 0.4691630322971412
Loss in iteration 26 : 0.4646107760493367
Loss in iteration 27 : 0.4601805808209989
Loss in iteration 28 : 0.4559580169661496
Loss in iteration 29 : 0.45192517338942073
Loss in iteration 30 : 0.4481121511086755
Loss in iteration 31 : 0.44453449362114744
Loss in iteration 32 : 0.4411735391112346
Loss in iteration 33 : 0.43796634835957193
Loss in iteration 34 : 0.43482118953222215
Loss in iteration 35 : 0.43172949787358184
Loss in iteration 36 : 0.4286972239372693
Loss in iteration 37 : 0.42571924496208646
Loss in iteration 38 : 0.42283100962531434
Loss in iteration 39 : 0.4200319721974735
Loss in iteration 40 : 0.4173530248588244
Loss in iteration 41 : 0.41478701136333873
Loss in iteration 42 : 0.4123450228608724
Loss in iteration 43 : 0.4100266571354714
Loss in iteration 44 : 0.4078819379634787
Loss in iteration 45 : 0.40590559902713974
Loss in iteration 46 : 0.4040911944688377
Loss in iteration 47 : 0.40241312817087593
Loss in iteration 48 : 0.40086228351434927
Loss in iteration 49 : 0.39942468063971
Loss in iteration 50 : 0.3980731980754601
Loss in iteration 51 : 0.39680108081807813
Loss in iteration 52 : 0.39562522098868425
Loss in iteration 53 : 0.3945353625912314
Loss in iteration 54 : 0.3935106737831257
Loss in iteration 55 : 0.39254092096496107
Loss in iteration 56 : 0.3916326499209859
Loss in iteration 57 : 0.3908088686097495
Testing accuracy  of updater 8 on alg 1 with rate 5.599999999999997E-4 = 0.77875, training accuracy 0.8381353188734219, time elapsed: 1016 millisecond.
Loss in iteration 1 : 1.0030118011953153
Loss in iteration 2 : 4.324603682857374
Loss in iteration 3 : 6.0328734674397175
Loss in iteration 4 : 6.117215180506061
Loss in iteration 5 : 4.9762670268999365
Loss in iteration 6 : 2.8998203609306397
Loss in iteration 7 : 1.2541198534986615
Loss in iteration 8 : 1.6635818581122928
Loss in iteration 9 : 3.081878528010057
Loss in iteration 10 : 3.400805974121486
Loss in iteration 11 : 2.640124789631692
Loss in iteration 12 : 1.9012321030237966
Loss in iteration 13 : 1.7693113415775148
Loss in iteration 14 : 2.0760815290071886
Loss in iteration 15 : 2.458542454632707
Loss in iteration 16 : 2.7220751053989654
Loss in iteration 17 : 2.773772040081834
Loss in iteration 18 : 2.6395752941120065
Loss in iteration 19 : 2.418929011929367
Loss in iteration 20 : 2.219323335580655
Loss in iteration 21 : 2.1259106619278674
Loss in iteration 22 : 2.195488964044472
Loss in iteration 23 : 2.3688095042663173
Loss in iteration 24 : 2.49203107010211
Loss in iteration 25 : 2.4613186401232827
Loss in iteration 26 : 2.312296613188453
Loss in iteration 27 : 2.165828474505564
Loss in iteration 28 : 2.107704638582769
Loss in iteration 29 : 2.1364750938778867
Loss in iteration 30 : 2.1897385260850717
Loss in iteration 31 : 2.2119442963572324
Loss in iteration 32 : 2.1798878401756365
Loss in iteration 33 : 2.1004249066992813
Loss in iteration 34 : 2.004129070715489
Loss in iteration 35 : 1.9369968868559027
Loss in iteration 36 : 1.9335651166674668
Loss in iteration 37 : 1.9525123997480673
Loss in iteration 38 : 1.9340056738338969
Loss in iteration 39 : 1.8548771340658496
Loss in iteration 40 : 1.7620491690010773
Loss in iteration 41 : 1.7135655062767174
Loss in iteration 42 : 1.7092021111340645
Loss in iteration 43 : 1.6931797531496129
Loss in iteration 44 : 1.6374604274077549
Loss in iteration 45 : 1.5556595519042826
Loss in iteration 46 : 1.50430343462663
Loss in iteration 47 : 1.4981232319639959
Loss in iteration 48 : 1.4546656400439515
Loss in iteration 49 : 1.3619582081836148
Loss in iteration 50 : 1.3331739002408902
Loss in iteration 51 : 1.3194302382452663
Loss in iteration 52 : 1.2373316894527449
Loss in iteration 53 : 1.23903234063967
Loss in iteration 54 : 1.1636811909145557
Loss in iteration 55 : 1.2276627392778332
Loss in iteration 56 : 1.1499904605981446
Loss in iteration 57 : 1.137401764855579
Loss in iteration 58 : 1.2164647119239769
Loss in iteration 59 : 1.25266182604215
Loss in iteration 60 : 1.4768128621091812
Loss in iteration 61 : 1.3848691071181318
Loss in iteration 62 : 1.0922827282295824
Loss in iteration 63 : 1.3770468995029037
Loss in iteration 64 : 1.1426318614719628
Loss in iteration 65 : 1.1818575172840107
Loss in iteration 66 : 1.294232841881429
Loss in iteration 67 : 1.2394959228222655
Loss in iteration 68 : 1.1636183544449918
Loss in iteration 69 : 1.2160147598029272
Loss in iteration 70 : 1.2516316234728389
Loss in iteration 71 : 1.1747251892923145
Loss in iteration 72 : 1.1458194329834892
Loss in iteration 73 : 1.1781584531800462
Loss in iteration 74 : 1.1625540759786288
Loss in iteration 75 : 1.1002715068721325
Loss in iteration 76 : 1.0854808510439167
Loss in iteration 77 : 1.0953417434752921
Loss in iteration 78 : 1.0299538400608215
Loss in iteration 79 : 1.008270225406318
Loss in iteration 80 : 1.0148963172232632
Loss in iteration 81 : 0.9515136627030976
Loss in iteration 82 : 0.958051288418783
Loss in iteration 83 : 0.9057333327352417
Loss in iteration 84 : 0.9204811875405126
Loss in iteration 85 : 0.8743715727189918
Loss in iteration 86 : 0.9459511570080834
Loss in iteration 87 : 0.9418402536801096
Loss in iteration 88 : 0.8463232288862841
Loss in iteration 89 : 0.9982166482909416
Loss in iteration 90 : 0.8940245021903803
Loss in iteration 91 : 0.9076063181608991
Loss in iteration 92 : 0.8096193202312675
Loss in iteration 93 : 0.8931703419548136
Loss in iteration 94 : 0.7939511620010302
Loss in iteration 95 : 0.8537620221502762
Loss in iteration 96 : 0.8215573173736994
Loss in iteration 97 : 0.7942954953675316
Loss in iteration 98 : 0.8250445511661748
Loss in iteration 99 : 0.771243573067566
Loss in iteration 100 : 0.7773690492055232
Loss in iteration 101 : 0.7719535508078367
Loss in iteration 102 : 0.7334825955037034
Loss in iteration 103 : 0.7497092612251518
Loss in iteration 104 : 0.7054190048295846
Loss in iteration 105 : 0.7157540127222727
Loss in iteration 106 : 0.6879096767859798
Loss in iteration 107 : 0.6984649365337978
Loss in iteration 108 : 0.6574665146022116
Loss in iteration 109 : 0.6791225944675737
Loss in iteration 110 : 0.6447814775637215
Loss in iteration 111 : 0.6479494116807283
Loss in iteration 112 : 0.6523159534356648
Loss in iteration 113 : 0.6181498605064367
Loss in iteration 114 : 0.6250320597291771
Loss in iteration 115 : 0.6217218729699614
Loss in iteration 116 : 0.5935052148495868
Loss in iteration 117 : 0.6065693410034985
Loss in iteration 118 : 0.5845942624628913
Loss in iteration 119 : 0.5842279651419341
Loss in iteration 120 : 0.5789576961172939
Loss in iteration 121 : 0.5701409467891227
Loss in iteration 122 : 0.569077302280317
Loss in iteration 123 : 0.5572403180025841
Loss in iteration 124 : 0.561933834315943
Loss in iteration 125 : 0.5446593138546967
Loss in iteration 126 : 0.5516010455128899
Loss in iteration 127 : 0.536152192517068
Loss in iteration 128 : 0.5402081538810768
Loss in iteration 129 : 0.5352359292570087
Loss in iteration 130 : 0.5268531301579504
Loss in iteration 131 : 0.5347208843963684
Loss in iteration 132 : 0.5246385091680943
Loss in iteration 133 : 0.5200423866349246
Loss in iteration 134 : 0.5262373651773758
Loss in iteration 135 : 0.5180593851707866
Loss in iteration 136 : 0.5142582442290814
Loss in iteration 137 : 0.5180098641458462
Loss in iteration 138 : 0.511754508563968
Loss in iteration 139 : 0.5118088776230363
Loss in iteration 140 : 0.5130845177411436
Loss in iteration 141 : 0.506865743648076
Loss in iteration 142 : 0.5089840201014696
Loss in iteration 143 : 0.5063661104581184
Loss in iteration 144 : 0.5054032954256302
Loss in iteration 145 : 0.5057623666729221
Loss in iteration 146 : 0.5036132064889577
Loss in iteration 147 : 0.5052755750644263
Loss in iteration 148 : 0.5031149649623878
Loss in iteration 149 : 0.503572343466731
Loss in iteration 150 : 0.5042500711335547
Loss in iteration 151 : 0.5018446199367714
Loss in iteration 152 : 0.5030329479638432
Loss in iteration 153 : 0.5026938429498125
Testing accuracy  of updater 9 on alg 1 with rate 0.02744 = 0.791, training accuracy 0.842667529944966, time elapsed: 2301 millisecond.
Loss in iteration 1 : 1.000345670858201
Loss in iteration 2 : 1.5463676493770229
Loss in iteration 3 : 2.1723642702170523
Loss in iteration 4 : 2.3035462560437203
Loss in iteration 5 : 2.003671379917316
Loss in iteration 6 : 1.3345889644091817
Loss in iteration 7 : 0.6826861933482617
Loss in iteration 8 : 0.6805456977722266
Loss in iteration 9 : 1.1553051107557433
Loss in iteration 10 : 1.417585836189029
Loss in iteration 11 : 1.23920885720564
Loss in iteration 12 : 0.9343735368486337
Loss in iteration 13 : 0.79879242989186
Loss in iteration 14 : 0.862417069934464
Loss in iteration 15 : 0.9977766696866684
Loss in iteration 16 : 1.1091842904112226
Loss in iteration 17 : 1.1499553181123763
Loss in iteration 18 : 1.1153849811797687
Loss in iteration 19 : 1.035662324535418
Loss in iteration 20 : 0.95186231374269
Loss in iteration 21 : 0.9020236239636253
Loss in iteration 22 : 0.9123693524361134
Loss in iteration 23 : 0.9683575855481574
Loss in iteration 24 : 1.014808859246059
Loss in iteration 25 : 1.009866613284252
Loss in iteration 26 : 0.9574053317924115
Loss in iteration 27 : 0.8963606778298785
Loss in iteration 28 : 0.8640986249711322
Loss in iteration 29 : 0.8684281041689739
Loss in iteration 30 : 0.8860341228771245
Loss in iteration 31 : 0.8935209882322305
Loss in iteration 32 : 0.878125449632215
Loss in iteration 33 : 0.8423210694851353
Loss in iteration 34 : 0.801387803701171
Loss in iteration 35 : 0.7754834307156158
Loss in iteration 36 : 0.7748037744076782
Loss in iteration 37 : 0.7801293108189172
Loss in iteration 38 : 0.76725468433349
Loss in iteration 39 : 0.731590494156604
Loss in iteration 40 : 0.6978509383952795
Loss in iteration 41 : 0.6832798955765044
Loss in iteration 42 : 0.6835663235668953
Loss in iteration 43 : 0.6747089949382364
Loss in iteration 44 : 0.6457206938963862
Loss in iteration 45 : 0.6172451673829816
Loss in iteration 46 : 0.6107074778132868
Loss in iteration 47 : 0.6113595871922413
Loss in iteration 48 : 0.5871518473880848
Loss in iteration 49 : 0.5632749745952169
Loss in iteration 50 : 0.5686320539505101
Loss in iteration 51 : 0.5631112059527524
Loss in iteration 52 : 0.5417130084553368
Loss in iteration 53 : 0.5634691179678823
Loss in iteration 54 : 0.5462030116053391
Loss in iteration 55 : 0.5686090851182082
Loss in iteration 56 : 0.5594482629536557
Loss in iteration 57 : 0.5834996844466529
Loss in iteration 58 : 0.5609661737207269
Loss in iteration 59 : 0.5731390976519029
Loss in iteration 60 : 0.5580856774945434
Loss in iteration 61 : 0.5612690454670528
Loss in iteration 62 : 0.5519709763183287
Loss in iteration 63 : 0.5565301328371337
Loss in iteration 64 : 0.5492082794018518
Loss in iteration 65 : 0.5565390850878041
Loss in iteration 66 : 0.55219094340953
Loss in iteration 67 : 0.5525554182565672
Loss in iteration 68 : 0.5559930316142678
Loss in iteration 69 : 0.5507384039892383
Loss in iteration 70 : 0.553382857774782
Loss in iteration 71 : 0.5518962697391511
Loss in iteration 72 : 0.5475649455190956
Loss in iteration 73 : 0.549044137080628
Loss in iteration 74 : 0.5451204271354985
Loss in iteration 75 : 0.5435178485060753
Loss in iteration 76 : 0.5424549984453694
Loss in iteration 77 : 0.5383919052921433
Loss in iteration 78 : 0.5384852329207791
Loss in iteration 79 : 0.5359335332067134
Loss in iteration 80 : 0.534937663190608
Loss in iteration 81 : 0.533945425809601
Loss in iteration 82 : 0.5317280150138133
Loss in iteration 83 : 0.5317772749559978
Loss in iteration 84 : 0.5291097956638435
Loss in iteration 85 : 0.5289633807041448
Loss in iteration 86 : 0.5259685979366993
Loss in iteration 87 : 0.5254797409086918
Loss in iteration 88 : 0.522944842183371
Loss in iteration 89 : 0.52191060201183
Loss in iteration 90 : 0.5200497817713398
Loss in iteration 91 : 0.5189782311387692
Loss in iteration 92 : 0.5178722448342724
Loss in iteration 93 : 0.5167292447260892
Loss in iteration 94 : 0.5159507895217521
Loss in iteration 95 : 0.514910002035011
Loss in iteration 96 : 0.5143312221844837
Loss in iteration 97 : 0.5135376090745559
Loss in iteration 98 : 0.5130347066429879
Loss in iteration 99 : 0.5121344398567508
Loss in iteration 100 : 0.5116837437042798
Loss in iteration 101 : 0.511070078665687
Loss in iteration 102 : 0.5105203131259732
Loss in iteration 103 : 0.5099956943515178
Loss in iteration 104 : 0.5094477535832911
Loss in iteration 105 : 0.50909166449319
Loss in iteration 106 : 0.5086545589216063
Loss in iteration 107 : 0.5083747335567094
Loss in iteration 108 : 0.5079960499937071
Loss in iteration 109 : 0.5078545039890945
Testing accuracy  of updater 9 on alg 1 with rate 0.019208 = 0.79, training accuracy 0.8429912593072192, time elapsed: 1521 millisecond.
Loss in iteration 1 : 1.0000805293980564
Loss in iteration 2 : 0.9280859664482058
Loss in iteration 3 : 1.2289603610549968
Loss in iteration 4 : 1.2882080414146193
Loss in iteration 5 : 1.1349977251365408
Loss in iteration 6 : 0.7959927211738632
Loss in iteration 7 : 0.44104223260256037
Loss in iteration 8 : 0.49768707937868606
Loss in iteration 9 : 0.7539469072538036
Loss in iteration 10 : 0.8271316378115761
Loss in iteration 11 : 0.6861397404028782
Loss in iteration 12 : 0.5387940581784535
Loss in iteration 13 : 0.5022935854358836
Loss in iteration 14 : 0.5585996742017632
Loss in iteration 15 : 0.6356476260807025
Loss in iteration 16 : 0.680245364124085
Loss in iteration 17 : 0.6779237491622949
Loss in iteration 18 : 0.6392273499161618
Loss in iteration 19 : 0.5897909787174334
Loss in iteration 20 : 0.5579600938446776
Loss in iteration 21 : 0.5592802572367858
Loss in iteration 22 : 0.5871793301280567
Loss in iteration 23 : 0.6150973734239489
Loss in iteration 24 : 0.6220863143776081
Loss in iteration 25 : 0.6042843661521143
Loss in iteration 26 : 0.5740418976894338
Loss in iteration 27 : 0.5523512043155074
Loss in iteration 28 : 0.5471775073882289
Loss in iteration 29 : 0.5560582246462356
Loss in iteration 30 : 0.5654772717659173
Loss in iteration 31 : 0.5657454477649764
Loss in iteration 32 : 0.5536354975623066
Loss in iteration 33 : 0.5345236467986365
Loss in iteration 34 : 0.518470595980665
Loss in iteration 35 : 0.5132411066118824
Loss in iteration 36 : 0.5155914391040785
Loss in iteration 37 : 0.5173801091203215
Loss in iteration 38 : 0.5107240813139245
Loss in iteration 39 : 0.4963441246459835
Loss in iteration 40 : 0.48317907613293143
Loss in iteration 41 : 0.4765066508335852
Loss in iteration 42 : 0.4764453770066249
Loss in iteration 43 : 0.4757499028175531
Loss in iteration 44 : 0.4680534630735571
Loss in iteration 45 : 0.4570143993802602
Loss in iteration 46 : 0.4499953149465705
Loss in iteration 47 : 0.44947975085541053
Loss in iteration 48 : 0.4494876051503828
Loss in iteration 49 : 0.4427251845486855
Loss in iteration 50 : 0.43547755947796013
Loss in iteration 51 : 0.43578390869331435
Loss in iteration 52 : 0.43858309585608246
Loss in iteration 53 : 0.4356599907788663
Loss in iteration 54 : 0.4320143694202264
Loss in iteration 55 : 0.43738362542394227
Loss in iteration 56 : 0.4394980709240287
Loss in iteration 57 : 0.4368186428347632
Loss in iteration 58 : 0.4416575435301879
Loss in iteration 59 : 0.445232992057521
Loss in iteration 60 : 0.4439977911904116
Loss in iteration 61 : 0.4472767570098408
Loss in iteration 62 : 0.4493790545452808
Loss in iteration 63 : 0.4479358328680468
Loss in iteration 64 : 0.4503030125065795
Loss in iteration 65 : 0.45130328780085116
Loss in iteration 66 : 0.4505783229259462
Loss in iteration 67 : 0.4521873669530946
Loss in iteration 68 : 0.4535520078910361
Loss in iteration 69 : 0.45344252229266896
Loss in iteration 70 : 0.4546208090225094
Loss in iteration 71 : 0.45654604082369077
Loss in iteration 72 : 0.4574495668921659
Loss in iteration 73 : 0.4582693750665774
Loss in iteration 74 : 0.45997541526148705
Loss in iteration 75 : 0.46143031853684763
Loss in iteration 76 : 0.46212241884831234
Loss in iteration 77 : 0.4631761879160981
Loss in iteration 78 : 0.4646456419983265
Loss in iteration 79 : 0.4656626110174016
Loss in iteration 80 : 0.4664428919468753
Loss in iteration 81 : 0.467568458390655
Loss in iteration 82 : 0.46866321220687857
Loss in iteration 83 : 0.4694005945916891
Loss in iteration 84 : 0.4700758893144731
Loss in iteration 85 : 0.4709877448480918
Loss in iteration 86 : 0.47189196648679527
Loss in iteration 87 : 0.47253362806637556
Loss in iteration 88 : 0.47323698664859004
Loss in iteration 89 : 0.47407363717555007
Loss in iteration 90 : 0.47475371536553523
Loss in iteration 91 : 0.47532637311808357
Loss in iteration 92 : 0.47600519581303224
Loss in iteration 93 : 0.4767253315210008
Loss in iteration 94 : 0.47735925128357826
Loss in iteration 95 : 0.4779257973910544
Loss in iteration 96 : 0.47860315767657213
Loss in iteration 97 : 0.4792059759909131
Loss in iteration 98 : 0.47966268985143506
Loss in iteration 99 : 0.4802767034123299
Loss in iteration 100 : 0.48088058401391465
Loss in iteration 101 : 0.48137446471635137
Loss in iteration 102 : 0.4819250082281522
Loss in iteration 103 : 0.4824396920020247
Loss in iteration 104 : 0.4828580751612343
Loss in iteration 105 : 0.48334724519184247
Loss in iteration 106 : 0.4838051872337491
Loss in iteration 107 : 0.484226879240918
Loss in iteration 108 : 0.4846589740955818
Loss in iteration 109 : 0.4850651563826111
Loss in iteration 110 : 0.48546161169339763
Loss in iteration 111 : 0.48586197603065906
Loss in iteration 112 : 0.48625105263891777
Loss in iteration 113 : 0.4866230180205274
Loss in iteration 114 : 0.4870036305857991
Loss in iteration 115 : 0.48737335877720067
Loss in iteration 116 : 0.48773734185138373
Loss in iteration 117 : 0.48810201505756134
Loss in iteration 118 : 0.4884586966393419
Loss in iteration 119 : 0.48881043957568077
Loss in iteration 120 : 0.48916046033982397
Loss in iteration 121 : 0.4895038731903774
Loss in iteration 122 : 0.4898402361079758
Loss in iteration 123 : 0.49016991658574693
Loss in iteration 124 : 0.4904858082319666
Loss in iteration 125 : 0.49079306752274865
Loss in iteration 126 : 0.49109727508679196
Loss in iteration 127 : 0.49139288241182777
Loss in iteration 128 : 0.4916857962365887
Loss in iteration 129 : 0.49196873289067217
Loss in iteration 130 : 0.49224765533529957
Loss in iteration 131 : 0.4925244123770417
Loss in iteration 132 : 0.4928005311932604
Loss in iteration 133 : 0.49307519133528444
Loss in iteration 134 : 0.49334461708503685
Loss in iteration 135 : 0.49360926082567647
Loss in iteration 136 : 0.49386884365624334
Testing accuracy  of updater 9 on alg 1 with rate 0.010976 = 0.7875, training accuracy 0.842667529944966, time elapsed: 2136 millisecond.
Loss in iteration 1 : 1.00000429507411
Loss in iteration 2 : 0.5582737711294434
Loss in iteration 3 : 0.6256585833347273
Loss in iteration 4 : 0.7024687300197112
Loss in iteration 5 : 0.7240382295649187
Loss in iteration 6 : 0.6960015569657195
Loss in iteration 7 : 0.6247746812846072
Loss in iteration 8 : 0.5214090738571466
Loss in iteration 9 : 0.4286121199885293
Loss in iteration 10 : 0.4120326739111952
Loss in iteration 11 : 0.4688572309511799
Loss in iteration 12 : 0.5034018499415233
Loss in iteration 13 : 0.48264020689962833
Loss in iteration 14 : 0.43060831974085356
Loss in iteration 15 : 0.3876717327645267
Loss in iteration 16 : 0.3764681079611674
Loss in iteration 17 : 0.38889640558075034
Loss in iteration 18 : 0.40746930413657834
Loss in iteration 19 : 0.41888956828087026
Loss in iteration 20 : 0.4179140989294016
Loss in iteration 21 : 0.40729777434120984
Loss in iteration 22 : 0.39337628665385166
Loss in iteration 23 : 0.3832822449453161
Loss in iteration 24 : 0.380600505272623
Loss in iteration 25 : 0.3841604845729422
Loss in iteration 26 : 0.39114876777903035
Loss in iteration 27 : 0.3969755827665661
Loss in iteration 28 : 0.3990928844598671
Loss in iteration 29 : 0.3973136934996274
Loss in iteration 30 : 0.3928240689440497
Loss in iteration 31 : 0.38791582772170197
Loss in iteration 32 : 0.3851067970045824
Loss in iteration 33 : 0.3849466410909472
Loss in iteration 34 : 0.38667630981925133
Loss in iteration 35 : 0.3889802256891153
Loss in iteration 36 : 0.390797520360432
Loss in iteration 37 : 0.39120206567231697
Loss in iteration 38 : 0.39015279344296094
Loss in iteration 39 : 0.3881927545456187
Loss in iteration 40 : 0.38632897505165187
Loss in iteration 41 : 0.38501315347982124
Loss in iteration 42 : 0.3845921608226727
Loss in iteration 43 : 0.3850010962811624
Loss in iteration 44 : 0.3857805233999747
Loss in iteration 45 : 0.3863519150092184
Loss in iteration 46 : 0.3862187371002226
Loss in iteration 47 : 0.38538309331208553
Loss in iteration 48 : 0.38429975288472806
Loss in iteration 49 : 0.38343080046853584
Loss in iteration 50 : 0.38305771796352994
Loss in iteration 51 : 0.38308759253314145
Loss in iteration 52 : 0.38332097054243447
Loss in iteration 53 : 0.3835193190574892
Loss in iteration 54 : 0.38347818281429635
Loss in iteration 55 : 0.3831295209937179
Loss in iteration 56 : 0.38259341429312477
Loss in iteration 57 : 0.38207677385847216
Loss in iteration 58 : 0.3818586568227533
Loss in iteration 59 : 0.3819568044657155
Loss in iteration 60 : 0.3821802889160919
Loss in iteration 61 : 0.3823362046846791
Loss in iteration 62 : 0.38227074449092985
Loss in iteration 63 : 0.3820361632516976
Loss in iteration 64 : 0.3818032342637338
Loss in iteration 65 : 0.3817853942784673
Loss in iteration 66 : 0.38190915599872166
Loss in iteration 67 : 0.3821286068379896
Loss in iteration 68 : 0.38231882388462257
Loss in iteration 69 : 0.38239580468174866
Loss in iteration 70 : 0.3824022144812381
Loss in iteration 71 : 0.38245046543083194
Loss in iteration 72 : 0.38258171500513966
Loss in iteration 73 : 0.38281631511156594
Loss in iteration 74 : 0.38306138367597004
Loss in iteration 75 : 0.38327602083919543
Loss in iteration 76 : 0.38344784314193686
Loss in iteration 77 : 0.3835826027188677
Loss in iteration 78 : 0.38374331957520164
Loss in iteration 79 : 0.38394950871403033
Loss in iteration 80 : 0.3842033102262977
Loss in iteration 81 : 0.38446879588017313
Loss in iteration 82 : 0.3847199655623277
Loss in iteration 83 : 0.38494600807121876
Loss in iteration 84 : 0.38515983781323015
Loss in iteration 85 : 0.38539431525599477
Loss in iteration 86 : 0.38566299385975866
Loss in iteration 87 : 0.3859380522375051
Loss in iteration 88 : 0.3862160200054582
Loss in iteration 89 : 0.38648071254812394
Loss in iteration 90 : 0.386736337155641
Loss in iteration 91 : 0.38699620106280375
Loss in iteration 92 : 0.38726846051355407
Loss in iteration 93 : 0.3875492961530243
Loss in iteration 94 : 0.3878364496371343
Loss in iteration 95 : 0.38811725985438794
Loss in iteration 96 : 0.3883908259596642
Loss in iteration 97 : 0.3886642950980343
Loss in iteration 98 : 0.388946338762588
Loss in iteration 99 : 0.389235556543953
Loss in iteration 100 : 0.38952518853848384
Loss in iteration 101 : 0.3898137381791936
Loss in iteration 102 : 0.39010115906902537
Loss in iteration 103 : 0.390388058199465
Loss in iteration 104 : 0.3906767613205522
Loss in iteration 105 : 0.3909664952575215
Loss in iteration 106 : 0.39126236547989807
Loss in iteration 107 : 0.3915582835594078
Loss in iteration 108 : 0.3918517374894175
Loss in iteration 109 : 0.3921431473677864
Loss in iteration 110 : 0.392437978060888
Loss in iteration 111 : 0.39273675156565496
Loss in iteration 112 : 0.39303663320999144
Loss in iteration 113 : 0.3933364145280452
Loss in iteration 114 : 0.3936347920763099
Loss in iteration 115 : 0.39393249211390463
Loss in iteration 116 : 0.3942333946864321
Loss in iteration 117 : 0.39453633474027827
Loss in iteration 118 : 0.3948397097533053
Loss in iteration 119 : 0.39514251705676384
Loss in iteration 120 : 0.3954445376839931
Loss in iteration 121 : 0.39574705634790713
Loss in iteration 122 : 0.3960497930404393
Loss in iteration 123 : 0.3963532977479115
Loss in iteration 124 : 0.39665567902524773
Loss in iteration 125 : 0.3969547304728532
Loss in iteration 126 : 0.39725316712416076
Loss in iteration 127 : 0.397554109249505
Loss in iteration 128 : 0.39785590771021057
Loss in iteration 129 : 0.3981577217675518
Loss in iteration 130 : 0.3984589597329084
Loss in iteration 131 : 0.39876010374655857
Loss in iteration 132 : 0.3990611205241463
Loss in iteration 133 : 0.39936125346297735
Loss in iteration 134 : 0.3996609910415761
Loss in iteration 135 : 0.39996052289734063
Loss in iteration 136 : 0.40025988865802337
Loss in iteration 137 : 0.400559242918867
Loss in iteration 138 : 0.4008589297354695
Loss in iteration 139 : 0.40115891950512017
Loss in iteration 140 : 0.40145943529234246
Loss in iteration 141 : 0.40176001030031044
Loss in iteration 142 : 0.40206110614900714
Loss in iteration 143 : 0.4023626209882191
Loss in iteration 144 : 0.40266361013511565
Loss in iteration 145 : 0.40296429866927286
Loss in iteration 146 : 0.4032656347512579
Loss in iteration 147 : 0.40356773758021136
Loss in iteration 148 : 0.4038697181042396
Loss in iteration 149 : 0.4041713854176417
Loss in iteration 150 : 0.4044733095307153
Loss in iteration 151 : 0.4047753368287952
Loss in iteration 152 : 0.4050771187946493
Loss in iteration 153 : 0.4053789354063825
Loss in iteration 154 : 0.40568031390113624
Loss in iteration 155 : 0.4059808272394283
Loss in iteration 156 : 0.40628060724832243
Loss in iteration 157 : 0.40658121349277354
Loss in iteration 158 : 0.40688155172989204
Loss in iteration 159 : 0.40718134016591734
Loss in iteration 160 : 0.40748094525353173
Loss in iteration 161 : 0.4077801416044016
Loss in iteration 162 : 0.40807868884516096
Loss in iteration 163 : 0.40837688910006903
Loss in iteration 164 : 0.40867451547773603
Loss in iteration 165 : 0.4089711998303286
Loss in iteration 166 : 0.40926741045157083
Loss in iteration 167 : 0.4095630584862973
Loss in iteration 168 : 0.4098582833965863
Loss in iteration 169 : 0.41015284207806846
Loss in iteration 170 : 0.41044718670475755
Loss in iteration 171 : 0.41074110464405783
Loss in iteration 172 : 0.4110347199508927
Loss in iteration 173 : 0.4113281446567968
Loss in iteration 174 : 0.4116214799129623
Loss in iteration 175 : 0.41191442585822124
Loss in iteration 176 : 0.4122065843420424
Loss in iteration 177 : 0.4124978956304778
Loss in iteration 178 : 0.41278850209621293
Loss in iteration 179 : 0.413078161227292
Loss in iteration 180 : 0.4133677218522788
Loss in iteration 181 : 0.4136567627257689
Loss in iteration 182 : 0.4139453475368699
Loss in iteration 183 : 0.414233356405613
Loss in iteration 184 : 0.4145212827210724
Loss in iteration 185 : 0.41480923749629484
Loss in iteration 186 : 0.4150972150652981
Loss in iteration 187 : 0.41538396408190276
Loss in iteration 188 : 0.41566986570091113
Loss in iteration 189 : 0.41595454403141996
Loss in iteration 190 : 0.4162373666897904
Loss in iteration 191 : 0.4165190224608686
Loss in iteration 192 : 0.4167994989614679
Loss in iteration 193 : 0.41707889377349766
Loss in iteration 194 : 0.4173573736689144
Loss in iteration 195 : 0.41763471840242256
Loss in iteration 196 : 0.4179109692884447
Loss in iteration 197 : 0.4181866421836501
Loss in iteration 198 : 0.4184623615588592
Loss in iteration 199 : 0.4187379983664331
Loss in iteration 200 : 0.41901285396379884
Testing accuracy  of updater 9 on alg 1 with rate 0.002744 = 0.7875, training accuracy 0.8410488831337002, time elapsed: 3737 millisecond.
Loss in iteration 1 : 1.0000021583700454
Loss in iteration 2 : 0.635404713147822
Loss in iteration 3 : 0.5766569198802262
Loss in iteration 4 : 0.6618998927910416
Loss in iteration 5 : 0.710873250938262
Loss in iteration 6 : 0.7234823100581426
Loss in iteration 7 : 0.703565789348346
Loss in iteration 8 : 0.6550823688017686
Loss in iteration 9 : 0.5825205625057458
Loss in iteration 10 : 0.4980645289181718
Loss in iteration 11 : 0.4313233012246586
Loss in iteration 12 : 0.4113108253987922
Loss in iteration 13 : 0.44523120318512155
Loss in iteration 14 : 0.4826615431029031
Loss in iteration 15 : 0.4900339512502142
Loss in iteration 16 : 0.4658365987538017
Loss in iteration 17 : 0.4262693469444913
Loss in iteration 18 : 0.39295712022665713
Loss in iteration 19 : 0.3785261542083275
Loss in iteration 20 : 0.3815406832739481
Loss in iteration 21 : 0.3933090533901276
Loss in iteration 22 : 0.40487521584290775
Loss in iteration 23 : 0.41063328220579876
Loss in iteration 24 : 0.40886156288097886
Loss in iteration 25 : 0.40129693886103607
Loss in iteration 26 : 0.3913651008763558
Loss in iteration 27 : 0.382524175719565
Loss in iteration 28 : 0.37749432028366453
Loss in iteration 29 : 0.3767172750862333
Loss in iteration 30 : 0.3795407439274079
Loss in iteration 31 : 0.38390166408185644
Loss in iteration 32 : 0.38746906260141134
Loss in iteration 33 : 0.3891357123346696
Loss in iteration 34 : 0.3884558000203508
Loss in iteration 35 : 0.3861274352392343
Loss in iteration 36 : 0.3829932855935607
Loss in iteration 37 : 0.3800664205455251
Loss in iteration 38 : 0.37854853906714053
Loss in iteration 39 : 0.3784873339195124
Loss in iteration 40 : 0.3793977309023284
Loss in iteration 41 : 0.38075154991792987
Loss in iteration 42 : 0.38190374724828957
Loss in iteration 43 : 0.3825203971413788
Loss in iteration 44 : 0.3824230261689713
Loss in iteration 45 : 0.3817452324967738
Loss in iteration 46 : 0.3807235186603381
Loss in iteration 47 : 0.3796825808498738
Loss in iteration 48 : 0.3789350481383552
Loss in iteration 49 : 0.3785553415222632
Loss in iteration 50 : 0.3786166358905402
Loss in iteration 51 : 0.3790272215530883
Loss in iteration 52 : 0.3794999267754255
Loss in iteration 53 : 0.3797655971572434
Loss in iteration 54 : 0.3797394515835082
Loss in iteration 55 : 0.379436136448064
Loss in iteration 56 : 0.3789598047025364
Loss in iteration 57 : 0.378513840910196
Loss in iteration 58 : 0.37820238461036193
Loss in iteration 59 : 0.37808397566710134
Loss in iteration 60 : 0.3781894434268553
Loss in iteration 61 : 0.3783324928495405
Loss in iteration 62 : 0.37846151880533363
Loss in iteration 63 : 0.37849565180073697
Loss in iteration 64 : 0.3784188991442559
Loss in iteration 65 : 0.37825826673847074
Loss in iteration 66 : 0.3780805902167275
Loss in iteration 67 : 0.37794301580242967
Loss in iteration 68 : 0.37790197968574324
Loss in iteration 69 : 0.3779474625481645
Loss in iteration 70 : 0.37804308622608085
Loss in iteration 71 : 0.3781288049398089
Loss in iteration 72 : 0.3781699967471617
Loss in iteration 73 : 0.3781725809505465
Loss in iteration 74 : 0.3781440969252322
Loss in iteration 75 : 0.3781000082464782
Loss in iteration 76 : 0.3780885246627666
Loss in iteration 77 : 0.37812666271155515
Loss in iteration 78 : 0.3781926709052569
Loss in iteration 79 : 0.37828097335542954
Loss in iteration 80 : 0.37836720982166105
Loss in iteration 81 : 0.3784307872546003
Loss in iteration 82 : 0.3784774828877074
Loss in iteration 83 : 0.37852326483540866
Loss in iteration 84 : 0.37857276437367615
Loss in iteration 85 : 0.3786319651749172
Loss in iteration 86 : 0.3787172168019876
Loss in iteration 87 : 0.3788173118920614
Loss in iteration 88 : 0.37891874677403603
Loss in iteration 89 : 0.3790145692137492
Loss in iteration 90 : 0.3791023910340628
Loss in iteration 91 : 0.37918491250035724
Loss in iteration 92 : 0.37927060179774913
Loss in iteration 93 : 0.3793593280182569
Loss in iteration 94 : 0.37945371172248993
Loss in iteration 95 : 0.3795558105655168
Loss in iteration 96 : 0.3796660010193229
Loss in iteration 97 : 0.37977786240892336
Loss in iteration 98 : 0.3798877395511502
Loss in iteration 99 : 0.3799947606446002
Loss in iteration 100 : 0.38010067920410956
Loss in iteration 101 : 0.38020728333156306
Loss in iteration 102 : 0.3803187220182716
Loss in iteration 103 : 0.3804341743529032
Loss in iteration 104 : 0.38055068590893054
Loss in iteration 105 : 0.38066957810373686
Loss in iteration 106 : 0.380789222149285
Loss in iteration 107 : 0.38090935911393575
Loss in iteration 108 : 0.3810300343965481
Loss in iteration 109 : 0.3811512888086805
Loss in iteration 110 : 0.3812731590155658
Loss in iteration 111 : 0.38139567793515466
Loss in iteration 112 : 0.3815189831278693
Loss in iteration 113 : 0.38164374571243087
Loss in iteration 114 : 0.3817705469655815
Loss in iteration 115 : 0.38189941184957377
Loss in iteration 116 : 0.3820291194277645
Loss in iteration 117 : 0.382160084305989
Loss in iteration 118 : 0.3822920568657435
Loss in iteration 119 : 0.3824247819959658
Loss in iteration 120 : 0.3825585518921869
Loss in iteration 121 : 0.3826934537640382
Loss in iteration 122 : 0.3828296925848033
Loss in iteration 123 : 0.38296672708930274
Loss in iteration 124 : 0.3831041358894318
Loss in iteration 125 : 0.3832419659411926
Loss in iteration 126 : 0.38338025956397626
Loss in iteration 127 : 0.3835192861335807
Loss in iteration 128 : 0.383659756373105
Loss in iteration 129 : 0.38380064568644384
Loss in iteration 130 : 0.38394166190035683
Loss in iteration 131 : 0.38408287167593713
Loss in iteration 132 : 0.3842243351417334
Loss in iteration 133 : 0.38436610651515873
Loss in iteration 134 : 0.38450861225273586
Loss in iteration 135 : 0.3846523292234784
Loss in iteration 136 : 0.38479772001197465
Loss in iteration 137 : 0.38494311498024103
Loss in iteration 138 : 0.3850888842048941
Loss in iteration 139 : 0.38523487919296806
Loss in iteration 140 : 0.38538174386454094
Loss in iteration 141 : 0.3855296681194777
Loss in iteration 142 : 0.3856785834281595
Loss in iteration 143 : 0.38582792638037433
Loss in iteration 144 : 0.38597779458762926
Loss in iteration 145 : 0.3861279435246218
Loss in iteration 146 : 0.38627872373081834
Loss in iteration 147 : 0.38642978704151565
Loss in iteration 148 : 0.386581164376455
Loss in iteration 149 : 0.38673300345472544
Loss in iteration 150 : 0.38688534310404094
Loss in iteration 151 : 0.38703815943124487
Loss in iteration 152 : 0.3871913370634403
Loss in iteration 153 : 0.3873449390469168
Loss in iteration 154 : 0.3874991063830726
Loss in iteration 155 : 0.38765379829245306
Loss in iteration 156 : 0.387809006389748
Loss in iteration 157 : 0.387964750574207
Loss in iteration 158 : 0.3881210487447761
Loss in iteration 159 : 0.38827780920809635
Loss in iteration 160 : 0.38843525890904307
Loss in iteration 161 : 0.3885931544295523
Loss in iteration 162 : 0.3887515363561366
Loss in iteration 163 : 0.3889102804922871
Loss in iteration 164 : 0.3890693333628299
Loss in iteration 165 : 0.38922871614038185
Loss in iteration 166 : 0.3893881456289025
Loss in iteration 167 : 0.38954783946294247
Loss in iteration 168 : 0.38970759898076035
Loss in iteration 169 : 0.3898673976814333
Loss in iteration 170 : 0.3900273535748878
Loss in iteration 171 : 0.3901873540125832
Loss in iteration 172 : 0.39034745390580644
Loss in iteration 173 : 0.3905077028872714
Loss in iteration 174 : 0.39066830990862955
Loss in iteration 175 : 0.3908291012085506
Loss in iteration 176 : 0.39098962769331047
Loss in iteration 177 : 0.3911502319590612
Loss in iteration 178 : 0.39131148401838695
Loss in iteration 179 : 0.3914728855925218
Loss in iteration 180 : 0.39163447931656675
Loss in iteration 181 : 0.391796462106955
Loss in iteration 182 : 0.39195853600518726
Loss in iteration 183 : 0.39212064432727106
Loss in iteration 184 : 0.39228297333718226
Loss in iteration 185 : 0.39244539157322866
Loss in iteration 186 : 0.3926078400918683
Loss in iteration 187 : 0.39277050598362634
Loss in iteration 188 : 0.39293332197992303
Loss in iteration 189 : 0.3930962758114387
Loss in iteration 190 : 0.3932595032138708
Loss in iteration 191 : 0.39342296426419004
Loss in iteration 192 : 0.39358664723112474
Loss in iteration 193 : 0.3937506002124834
Loss in iteration 194 : 0.39391484831362616
Loss in iteration 195 : 0.3940794380237903
Loss in iteration 196 : 0.39424434657370794
Loss in iteration 197 : 0.39440928253297003
Loss in iteration 198 : 0.39457439389057747
Loss in iteration 199 : 0.39473968534635684
Loss in iteration 200 : 0.3949049902048308
Testing accuracy  of updater 9 on alg 1 with rate 0.0019207999999999999 = 0.78575, training accuracy 0.8397539656846876, time elapsed: 3383 millisecond.
Loss in iteration 1 : 1.000000651401228
Loss in iteration 2 : 0.7841661486295581
Loss in iteration 3 : 0.5607226871377907
Loss in iteration 4 : 0.5750015054412767
Loss in iteration 5 : 0.6315014628309522
Loss in iteration 6 : 0.6694572306960838
Loss in iteration 7 : 0.6871981426543762
Loss in iteration 8 : 0.6866095116932824
Loss in iteration 9 : 0.669716296304563
Loss in iteration 10 : 0.6385293778549862
Loss in iteration 11 : 0.5951931504824688
Loss in iteration 12 : 0.5435118229274112
Loss in iteration 13 : 0.49351325691947306
Loss in iteration 14 : 0.4557305820754394
Loss in iteration 15 : 0.43702911256245774
Loss in iteration 16 : 0.43665470863364025
Loss in iteration 17 : 0.450171056346493
Loss in iteration 18 : 0.4657954650022578
Loss in iteration 19 : 0.47130261498475845
Loss in iteration 20 : 0.4634081016494436
Loss in iteration 21 : 0.44565522112269734
Loss in iteration 22 : 0.424022088583179
Loss in iteration 23 : 0.4050866521125995
Loss in iteration 24 : 0.3927150637221071
Loss in iteration 25 : 0.3884872264651156
Loss in iteration 26 : 0.3898810976645889
Loss in iteration 27 : 0.3939036885083381
Loss in iteration 28 : 0.39749169701857745
Loss in iteration 29 : 0.39892073404201167
Loss in iteration 30 : 0.39760164162874545
Loss in iteration 31 : 0.39398888943766397
Loss in iteration 32 : 0.3892645306850232
Loss in iteration 33 : 0.38459944421580367
Loss in iteration 34 : 0.3807992008473666
Loss in iteration 35 : 0.3783450721834848
Loss in iteration 36 : 0.37742280811247986
Loss in iteration 37 : 0.37754443374395713
Loss in iteration 38 : 0.37837917397362875
Loss in iteration 39 : 0.37947799803407944
Loss in iteration 40 : 0.38029394029817726
Loss in iteration 41 : 0.38055877462240734
Loss in iteration 42 : 0.38021650095795556
Loss in iteration 43 : 0.379382568030149
Loss in iteration 44 : 0.37829331321000437
Loss in iteration 45 : 0.3771752421576386
Loss in iteration 46 : 0.37625487392160667
Loss in iteration 47 : 0.37557863892682763
Loss in iteration 48 : 0.37534210805236823
Loss in iteration 49 : 0.3754617338594501
Loss in iteration 50 : 0.37576965535196677
Loss in iteration 51 : 0.37610856143911087
Loss in iteration 52 : 0.3763521723801675
Loss in iteration 53 : 0.3764384653386827
Loss in iteration 54 : 0.37635099010235756
Loss in iteration 55 : 0.3761153076008525
Loss in iteration 56 : 0.3757856482046372
Loss in iteration 57 : 0.37546399402555924
Loss in iteration 58 : 0.3752495750765854
Loss in iteration 59 : 0.3751286528929074
Loss in iteration 60 : 0.375100435646678
Loss in iteration 61 : 0.3751774054481913
Loss in iteration 62 : 0.37528606299881273
Loss in iteration 63 : 0.3753764777546861
Loss in iteration 64 : 0.37543889829012084
Loss in iteration 65 : 0.3754555571349233
Loss in iteration 66 : 0.3754263698178686
Loss in iteration 67 : 0.3753636174878602
Loss in iteration 68 : 0.3752866830274084
Loss in iteration 69 : 0.3752168345545199
Loss in iteration 70 : 0.37517296489394836
Loss in iteration 71 : 0.37515444363202527
Loss in iteration 72 : 0.3751645972734677
Loss in iteration 73 : 0.3751952129341574
Loss in iteration 74 : 0.37523708395553823
Loss in iteration 75 : 0.3752744308318671
Loss in iteration 76 : 0.37530154259367343
Loss in iteration 77 : 0.37531744233757913
Loss in iteration 78 : 0.37532258856982087
Loss in iteration 79 : 0.37532203752750143
Loss in iteration 80 : 0.3753168004827596
Loss in iteration 81 : 0.3753091172405039
Loss in iteration 82 : 0.3753080103641469
Loss in iteration 83 : 0.3753224547113326
Loss in iteration 84 : 0.3753417039069849
Loss in iteration 85 : 0.37536387290372497
Loss in iteration 86 : 0.37538724434427334
Loss in iteration 87 : 0.37541161246847066
Loss in iteration 88 : 0.3754356822196011
Loss in iteration 89 : 0.3754591191192351
Loss in iteration 90 : 0.37547916162483597
Loss in iteration 91 : 0.3754967361214034
Loss in iteration 92 : 0.37551418395792785
Loss in iteration 93 : 0.37553076073155917
Loss in iteration 94 : 0.37554757767091174
Loss in iteration 95 : 0.3755661399067182
Loss in iteration 96 : 0.3755871581760099
Loss in iteration 97 : 0.37560899836601136
Loss in iteration 98 : 0.3756324175555348
Loss in iteration 99 : 0.3756569767332945
Loss in iteration 100 : 0.37568302195716624
Loss in iteration 101 : 0.3757096916977191
Loss in iteration 102 : 0.37573621463308027
Loss in iteration 103 : 0.3757621866570844
Loss in iteration 104 : 0.37578784966983286
Loss in iteration 105 : 0.3758139510013235
Loss in iteration 106 : 0.375840226266971
Loss in iteration 107 : 0.3758666257390639
Loss in iteration 108 : 0.3758937268358052
Loss in iteration 109 : 0.3759216426450288
Loss in iteration 110 : 0.37595039098809324
Loss in iteration 111 : 0.3759794793486813
Loss in iteration 112 : 0.3760088683280137
Loss in iteration 113 : 0.37603846393563956
Loss in iteration 114 : 0.3760682748879004
Loss in iteration 115 : 0.3760983090502058
Loss in iteration 116 : 0.3761285735157518
Loss in iteration 117 : 0.37615907467709353
Loss in iteration 118 : 0.3761899844414561
Loss in iteration 119 : 0.3762214229562353
Loss in iteration 120 : 0.376253171603488
Loss in iteration 121 : 0.3762853506903739
Loss in iteration 122 : 0.3763178416209823
Loss in iteration 123 : 0.3763506165571255
Loss in iteration 124 : 0.37638367643976056
Loss in iteration 125 : 0.3764170221008702
Loss in iteration 126 : 0.3764506542719586
Loss in iteration 127 : 0.37648457359195864
Loss in iteration 128 : 0.37651878061458144
Loss in iteration 129 : 0.376553275815139
Loss in iteration 130 : 0.37658805959686714
Loss in iteration 131 : 0.37662313229678357
Loss in iteration 132 : 0.3766585652456489
Loss in iteration 133 : 0.37669433497307964
Loss in iteration 134 : 0.3767303456672212
Loss in iteration 135 : 0.3767666014243414
Loss in iteration 136 : 0.37680310593019417
Loss in iteration 137 : 0.37683997955837467
Loss in iteration 138 : 0.3768774549867857
Loss in iteration 139 : 0.3769152364935447
Loss in iteration 140 : 0.376953321912983
Loss in iteration 141 : 0.37699175745509356
Loss in iteration 142 : 0.3770305514819493
Loss in iteration 143 : 0.3770696976491264
Loss in iteration 144 : 0.37710904386709615
Loss in iteration 145 : 0.3771488122318891
Loss in iteration 146 : 0.37718885861049106
Loss in iteration 147 : 0.37722915739478124
Loss in iteration 148 : 0.37726971063856013
Loss in iteration 149 : 0.3773105201880305
Loss in iteration 150 : 0.37735161817756463
Loss in iteration 151 : 0.3773930545328414
Loss in iteration 152 : 0.3774347592724663
Loss in iteration 153 : 0.3774767329878273
Loss in iteration 154 : 0.3775189761903175
Loss in iteration 155 : 0.3775615203070439
Loss in iteration 156 : 0.37760439170946947
Loss in iteration 157 : 0.3776474363758454
Loss in iteration 158 : 0.3776906664192677
Loss in iteration 159 : 0.3777342156392342
Loss in iteration 160 : 0.3777779984371271
Loss in iteration 161 : 0.37782201783672076
Loss in iteration 162 : 0.37786632537593434
Loss in iteration 163 : 0.37791098053266214
Loss in iteration 164 : 0.37795585205907195
Loss in iteration 165 : 0.3780010153963715
Loss in iteration 166 : 0.37804633218271216
Loss in iteration 167 : 0.3780918125027879
Loss in iteration 168 : 0.3781374654459712
Loss in iteration 169 : 0.3781832992014416
Loss in iteration 170 : 0.3782293211442971
Loss in iteration 171 : 0.37827553791348184
Loss in iteration 172 : 0.37832200806418864
Loss in iteration 173 : 0.37836891263488187
Loss in iteration 174 : 0.37841615026778874
Loss in iteration 175 : 0.37846362321215027
Loss in iteration 176 : 0.3785113657207228
Loss in iteration 177 : 0.37855943040376266
Loss in iteration 178 : 0.37860758238099096
Loss in iteration 179 : 0.3786561115830075
Loss in iteration 180 : 0.3787050050078299
Loss in iteration 181 : 0.37875437039217474
Loss in iteration 182 : 0.3788039392956209
Loss in iteration 183 : 0.3788535155974036
Loss in iteration 184 : 0.37890312304335694
Loss in iteration 185 : 0.3789529491032009
Loss in iteration 186 : 0.37900353821791033
Loss in iteration 187 : 0.3790543175799292
Loss in iteration 188 : 0.3791052364424037
Loss in iteration 189 : 0.3791563051426354
Loss in iteration 190 : 0.3792075329858227
Loss in iteration 191 : 0.37925904450890646
Loss in iteration 192 : 0.37931088735055374
Loss in iteration 193 : 0.37936297866697843
Loss in iteration 194 : 0.3794153096328399
Loss in iteration 195 : 0.3794678721817284
Loss in iteration 196 : 0.3795206886443122
Loss in iteration 197 : 0.3795736830580992
Loss in iteration 198 : 0.37962686114996314
Loss in iteration 199 : 0.3796802616052922
Loss in iteration 200 : 0.379733873602296
Testing accuracy  of updater 9 on alg 1 with rate 0.0010976 = 0.7865, training accuracy 0.840401424409194, time elapsed: 2550 millisecond.
Loss in iteration 1 : 1.000000036336629
Loss in iteration 2 : 0.9503098610121973
Testing accuracy  of updater 9 on alg 1 with rate 2.7439999999999973E-4 = 0.5, training accuracy 0.6474587245063127, time elapsed: 42 millisecond.
