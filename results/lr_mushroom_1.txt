objc[1063]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java (0x1022a54c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x1023294e0). One of the two will be used. Which one is undefined.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
18/02/27 08:23:27 INFO SparkContext: Running Spark version 2.0.0
18/02/27 08:23:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/02/27 08:23:29 INFO SecurityManager: Changing view acls to: Aitor
18/02/27 08:23:29 INFO SecurityManager: Changing modify acls to: Aitor
18/02/27 08:23:29 INFO SecurityManager: Changing view acls groups to: 
18/02/27 08:23:29 INFO SecurityManager: Changing modify acls groups to: 
18/02/27 08:23:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Aitor); groups with view permissions: Set(); users  with modify permissions: Set(Aitor); groups with modify permissions: Set()
18/02/27 08:23:31 INFO Utils: Successfully started service 'sparkDriver' on port 50017.
18/02/27 08:23:31 INFO SparkEnv: Registering MapOutputTracker
18/02/27 08:23:31 INFO SparkEnv: Registering BlockManagerMaster
18/02/27 08:23:31 INFO DiskBlockManager: Created local directory at /private/var/folders/04/1m8vd7011dn7rc671175zs0h0000gn/T/blockmgr-e7d78d85-c6c8-4217-8194-c2c7b8c0bac4
18/02/27 08:23:31 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
18/02/27 08:23:31 INFO SparkEnv: Registering OutputCommitCoordinator
18/02/27 08:23:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
18/02/27 08:23:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.2.140:4040
18/02/27 08:23:33 INFO Executor: Starting executor ID driver on host localhost
18/02/27 08:23:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50018.
18/02/27 08:23:33 INFO NettyBlockTransferService: Server created on 192.168.2.140:50018
18/02/27 08:23:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.2.140, 50018)
18/02/27 08:23:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.2.140:50018 with 912.3 MB RAM, BlockManagerId(driver, 192.168.2.140, 50018)
18/02/27 08:23:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.2.140, 50018)
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.8982803999400666
Loss in iteration 3 : 2.6397664659256392
Loss in iteration 4 : 14.296968541226018
Loss in iteration 5 : 0.37614184775707127
Loss in iteration 6 : 0.2863767556424867
Loss in iteration 7 : 0.24138929855851085
Loss in iteration 8 : 0.2157530360515301
Loss in iteration 9 : 0.19241135504897794
Loss in iteration 10 : 0.17083126707562674
Loss in iteration 11 : 0.15085813296660014
Loss in iteration 12 : 0.13258222792112836
Loss in iteration 13 : 0.116284870318561
Loss in iteration 14 : 0.10227179438113242
Loss in iteration 15 : 0.09068742074392491
Loss in iteration 16 : 0.08139732432145991
Loss in iteration 17 : 0.07402195244955276
Loss in iteration 18 : 0.06810128485381256
Loss in iteration 19 : 0.06323427712274501
Loss in iteration 20 : 0.05912510060927892
Loss in iteration 21 : 0.05557218769162033
Loss in iteration 22 : 0.05244193929182018
Loss in iteration 23 : 0.049645478401115405
Loss in iteration 24 : 0.04712232983723579
Loss in iteration 25 : 0.044829927013082395
Loss in iteration 26 : 0.04273706775458136
Loss in iteration 27 : 0.040819852502801984
Loss in iteration 28 : 0.03905915641268188
Loss in iteration 29 : 0.037439057110186705
Loss in iteration 30 : 0.03594586467678787
Loss in iteration 31 : 0.03456753092504161
Loss in iteration 32 : 0.033293293079866185
Loss in iteration 33 : 0.03211345708928563
Loss in iteration 34 : 0.031019259945525502
Loss in iteration 35 : 0.03000277403180893
Loss in iteration 36 : 0.029056832404265126
Loss in iteration 37 : 0.028174964019419005
Loss in iteration 38 : 0.02735133387756726
Loss in iteration 39 : 0.02658068625813939
Loss in iteration 40 : 0.02585829074142023
Loss in iteration 41 : 0.025179891284621156
Loss in iteration 42 : 0.0245416587204907
Loss in iteration 43 : 0.023940146946643384
Loss in iteration 44 : 0.023372252917245637
Loss in iteration 45 : 0.022835180402094115
Loss in iteration 46 : 0.022326407365790325
Loss in iteration 47 : 0.021843656745223212
Loss in iteration 48 : 0.02138487036173102
Loss in iteration 49 : 0.020948185687335297
Loss in iteration 50 : 0.020531915184792415
Loss in iteration 51 : 0.020134527952679337
Loss in iteration 52 : 0.019754633424655395
Loss in iteration 53 : 0.019390966893122835
Loss in iteration 54 : 0.019042376649559985
Loss in iteration 55 : 0.018707812555442304
Loss in iteration 56 : 0.01838631587810799
Loss in iteration 57 : 0.01807701024474841
Loss in iteration 58 : 0.017779093584758408
Loss in iteration 59 : 0.01749183094595627
Loss in iteration 60 : 0.01721454808376373
Loss in iteration 61 : 0.01694662573444886
Loss in iteration 62 : 0.016687494494128277
Loss in iteration 63 : 0.016436630234544114
Loss in iteration 64 : 0.016193549994826115
Loss in iteration 65 : 0.01595780829564345
Loss in iteration 66 : 0.015728993828473346
Loss in iteration 67 : 0.015506726478263521
Loss in iteration 68 : 0.015290654642642067
Loss in iteration 69 : 0.015080452815114505
Loss in iteration 70 : 0.014875819403452497
Loss in iteration 71 : 0.014676474757793882
Loss in iteration 72 : 0.014482159385885705
Loss in iteration 73 : 0.014292632335468097
Loss in iteration 74 : 0.014107669726056142
Loss in iteration 75 : 0.013927063414365873
Loss in iteration 76 : 0.013750619779385935
Loss in iteration 77 : 0.013578158614644098
Loss in iteration 78 : 0.013409512116584083
Loss in iteration 79 : 0.013244523959175502
Loss in iteration 80 : 0.01308304844594899
Loss in iteration 81 : 0.012924949731593081
Loss in iteration 82 : 0.012770101106091187
Loss in iteration 83 : 0.012618384335121144
Loss in iteration 84 : 0.012469689051106752
Loss in iteration 85 : 0.01232391218990278
Loss in iteration 86 : 0.01218095746862783
Loss in iteration 87 : 0.012040734900636698
Loss in iteration 88 : 0.011903160344054805
Loss in iteration 89 : 0.011768155080685508
Loss in iteration 90 : 0.011635645422456044
Loss in iteration 91 : 0.011505562342887335
Loss in iteration 92 : 0.011377841131367002
Loss in iteration 93 : 0.011252421068272601
Loss in iteration 94 : 0.011129245119236603
Loss in iteration 95 : 0.011008259647069061
Loss in iteration 96 : 0.01088941414005754
Loss in iteration 97 : 0.010772660955550628
Loss in iteration 98 : 0.010657955077898329
Loss in iteration 99 : 0.010545253889973958
Loss in iteration 100 : 0.010434516957635868
Loss in iteration 101 : 0.010325705826603325
Loss in iteration 102 : 0.010218783831322287
Loss in iteration 103 : 0.010113715915479474
Loss in iteration 104 : 0.010010468463890622
Loss in iteration 105 : 0.009909009145541046
Loss in iteration 106 : 0.009809306767591905
Loss in iteration 107 : 0.00971133114018788
Loss in iteration 108 : 0.009615052951909947
Loss in iteration 109 : 0.009520443655712304
Loss in iteration 110 : 0.00942747536516798
Loss in iteration 111 : 0.009336120760822809
Loss in iteration 112 : 0.009246353006425399
Loss in iteration 113 : 0.00915814567476332
Loss in iteration 114 : 0.009071472682794068
Loss in iteration 115 : 0.008986308235716172
Loss in iteration 116 : 0.008902626779582753
Loss in iteration 117 : 0.00882040296201785
Loss in iteration 118 : 0.008739611600558148
Loss in iteration 119 : 0.00866022765810886
Loss in iteration 120 : 0.00858222622497428
Loss in iteration 121 : 0.008505582506903078
Loss in iteration 122 : 0.008430271818573146
Loss in iteration 123 : 0.008356269581934368
Loss in iteration 124 : 0.008283551328828265
Loss in iteration 125 : 0.008212092707310577
Loss in iteration 126 : 0.00814186949111768
Loss in iteration 127 : 0.008072857591737568
Loss in iteration 128 : 0.00800503307257272
Loss in iteration 129 : 0.007938372164711494
Loss in iteration 130 : 0.007872851283859625
Loss in iteration 131 : 0.007808447048019599
Loss in iteration 132 : 0.007745136295544511
Loss in iteration 133 : 0.007682896103232672
Loss in iteration 134 : 0.007621703804169102
Loss in iteration 135 : 0.0075615370050593185
Loss in iteration 136 : 0.007502373602839454
Loss in iteration 137 : 0.007444191800383111
Loss in iteration 138 : 0.007386970121160088
Loss in iteration 139 : 0.007330687422734562
Loss in iteration 140 : 0.00727532290901975
Loss in iteration 141 : 0.007220856141233145
Loss in iteration 142 : 0.007167267047520379
Loss in iteration 143 : 0.0071145359312372
Loss in iteration 144 : 0.007062643477897287
Loss in iteration 145 : 0.007011570760809777
Loss in iteration 146 : 0.006961299245443278
Loss in iteration 147 : 0.006911810792564297
Loss in iteration 148 : 0.006863087660206454
Loss in iteration 149 : 0.006815112504534074
Loss in iteration 150 : 0.006767868379667837
Loss in iteration 151 : 0.00672133873654418
Loss in iteration 152 : 0.00667550742088186
Loss in iteration 153 : 0.006630358670329432
Loss in iteration 154 : 0.006585877110867762
Loss in iteration 155 : 0.006542047752539762
Loss in iteration 156 : 0.006498855984578124
Loss in iteration 157 : 0.0064562875699993435
Loss in iteration 158 : 0.006414328639729007
Loss in iteration 159 : 0.006372965686320521
Loss in iteration 160 : 0.006332185557325848
Loss in iteration 161 : 0.006291975448372967
Loss in iteration 162 : 0.00625232289600132
Loss in iteration 163 : 0.0062132157703025884
Loss in iteration 164 : 0.006174642267410625
Loss in iteration 165 : 0.006136590901880557
Loss in iteration 166 : 0.006099050498993455
Loss in iteration 167 : 0.00606201018702005
Loss in iteration 168 : 0.006025459389472996
Loss in iteration 169 : 0.005989387817374826
Loss in iteration 170 : 0.005953785461565334
Loss in iteration 171 : 0.005918642585069568
Loss in iteration 172 : 0.005883949715545077
Loss in iteration 173 : 0.005849697637824757
Loss in iteration 174 : 0.0058158773865692424
Loss in iteration 175 : 0.005782480239041043
Loss in iteration 176 : 0.00574949770801039
Loss in iteration 177 : 0.005716921534801815
Loss in iteration 178 : 0.005684743682487874
Loss in iteration 179 : 0.005652956329236067
Loss in iteration 180 : 0.005621551861813074
Loss in iteration 181 : 0.0055905228692498
Loss in iteration 182 : 0.005559862136669162
Loss in iteration 183 : 0.005529562639278074
Loss in iteration 184 : 0.005499617536524376
Loss in iteration 185 : 0.005470020166418239
Loss in iteration 186 : 0.005440764040017504
Loss in iteration 187 : 0.005411842836075487
Loss in iteration 188 : 0.005383250395849775
Loss in iteration 189 : 0.0053549807180693415
Loss in iteration 190 : 0.005327027954058123
Testing accuracy  of updater 0 on alg 0 with rate 10.0 = 0.9848888888888889, training accuracy 0.9994284081166047, time elapsed: 10319 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.45859828352491205
Loss in iteration 3 : 0.36856470775773775
Loss in iteration 4 : 0.3163188041385286
Loss in iteration 5 : 0.2739225763498299
Loss in iteration 6 : 0.2469247483641216
Loss in iteration 7 : 0.22753696129570877
Loss in iteration 8 : 0.21301289123602263
Loss in iteration 9 : 0.2011217263498658
Loss in iteration 10 : 0.19103474225540862
Loss in iteration 11 : 0.18231552050529734
Loss in iteration 12 : 0.17468006333275374
Loss in iteration 13 : 0.16792270155413686
Loss in iteration 14 : 0.16188880622580817
Loss in iteration 15 : 0.1564592619195598
Loss in iteration 16 : 0.15154053125365094
Loss in iteration 17 : 0.14705796080665032
Loss in iteration 18 : 0.14295113547694227
Loss in iteration 19 : 0.13917056972347516
Loss in iteration 20 : 0.13567529867282746
Loss in iteration 21 : 0.13243109073792816
Loss in iteration 22 : 0.12940909922501828
Loss in iteration 23 : 0.12658483013995636
Loss in iteration 24 : 0.12393734171460241
Loss in iteration 25 : 0.12144861636716095
Loss in iteration 26 : 0.11910306275050088
Loss in iteration 27 : 0.11688711716137593
Loss in iteration 28 : 0.11478892169552664
Loss in iteration 29 : 0.11279806228872319
Loss in iteration 30 : 0.11090535392637373
Loss in iteration 31 : 0.10910266332578682
Loss in iteration 32 : 0.10738276162569634
Loss in iteration 33 : 0.10573920128262307
Loss in iteration 34 : 0.10416621262922353
Loss in iteration 35 : 0.10265861650561268
Loss in iteration 36 : 0.1012117501087221
Loss in iteration 37 : 0.09982140377315064
Loss in iteration 38 : 0.09848376684044045
Loss in iteration 39 : 0.09719538112223265
Loss in iteration 40 : 0.0959531007384668
Loss in iteration 41 : 0.09475405733133264
Loss in iteration 42 : 0.09359562983149619
Loss in iteration 43 : 0.09247541809476947
Loss in iteration 44 : 0.09139121984207667
Loss in iteration 45 : 0.0903410104289287
Loss in iteration 46 : 0.08932292504697563
Loss in iteration 47 : 0.08833524302293244
Loss in iteration 48 : 0.08737637393195745
Loss in iteration 49 : 0.08644484528545887
Loss in iteration 50 : 0.08553929158900829
Loss in iteration 51 : 0.08465844459585303
Loss in iteration 52 : 0.08380112460651747
Loss in iteration 53 : 0.0829662326860032
Loss in iteration 54 : 0.08215274368785351
Loss in iteration 55 : 0.08135969998937076
Loss in iteration 56 : 0.0805862058550455
Loss in iteration 57 : 0.07983142235613216
Loss in iteration 58 : 0.0790945627836036
Loss in iteration 59 : 0.07837488849967691
Loss in iteration 60 : 0.07767170517995091
Loss in iteration 61 : 0.07698435940408761
Loss in iteration 62 : 0.07631223555806001
Loss in iteration 63 : 0.07565475301539948
Loss in iteration 64 : 0.0750113635686932
Loss in iteration 65 : 0.07438154908591561
Loss in iteration 66 : 0.07376481936906991
Loss in iteration 67 : 0.07316071019515168
Loss in iteration 68 : 0.07256878152165916
Loss in iteration 69 : 0.07198861584082393
Loss in iteration 70 : 0.07141981666843616
Loss in iteration 71 : 0.07086200715464765
Loss in iteration 72 : 0.07031482880546017
Loss in iteration 73 : 0.06977794030477824
Loss in iteration 74 : 0.0692510164279416
Loss in iteration 75 : 0.06873374703857288
Loss in iteration 76 : 0.06822583616139354
Loss in iteration 77 : 0.06772700112438325
Loss in iteration 78 : 0.06723697176430844
Loss in iteration 79 : 0.06675548969022058
Loss in iteration 80 : 0.06628230760004027
Loss in iteration 81 : 0.06581718864580392
Loss in iteration 82 : 0.06535990584356453
Loss in iteration 83 : 0.06491024152430272
Loss in iteration 84 : 0.06446798682254733
Loss in iteration 85 : 0.06403294119969057
Loss in iteration 86 : 0.06360491199926305
Loss in iteration 87 : 0.06318371403167103
Loss in iteration 88 : 0.062769169186118
Loss in iteration 89 : 0.06236110606763167
Loss in iteration 90 : 0.06195935965729487
Loss in iteration 91 : 0.061563770993941766
Loss in iteration 92 : 0.061174186875727135
Loss in iteration 93 : 0.06079045958010921
Loss in iteration 94 : 0.06041244660090799
Loss in iteration 95 : 0.06004001040121004
Loss in iteration 96 : 0.05967301818099195
Loss in iteration 97 : 0.05931134165842341
Loss in iteration 98 : 0.05895485686389533
Loss in iteration 99 : 0.05860344394589453
Loss in iteration 100 : 0.05825698698791282
Loss in iteration 101 : 0.05791537383564528
Loss in iteration 102 : 0.05757849593378445
Loss in iteration 103 : 0.05724624817177785
Loss in iteration 104 : 0.05691852873795519
Loss in iteration 105 : 0.05659523898148404
Loss in iteration 106 : 0.056276283281647646
Loss in iteration 107 : 0.05596156892397901
Loss in iteration 108 : 0.055651005982818164
Loss in iteration 109 : 0.05534450720989042
Loss in iteration 110 : 0.055041987928533764
Loss in iteration 111 : 0.05474336593322973
Loss in iteration 112 : 0.05444856139411424
Loss in iteration 113 : 0.054157496766171966
Loss in iteration 114 : 0.05387009670283357
Loss in iteration 115 : 0.0535862879737179
Loss in iteration 116 : 0.05330599938627758
Loss in iteration 117 : 0.05302916171112251
Loss in iteration 118 : 0.052755707610811275
Loss in iteration 119 : 0.05248557157191498
Loss in iteration 120 : 0.05221868984016918
Loss in iteration 121 : 0.051955000358544556
Loss in iteration 122 : 0.05169444270807412
Loss in iteration 123 : 0.051436958051288846
Loss in iteration 124 : 0.05118248907812084
Loss in iteration 125 : 0.050930979954142315
Loss in iteration 126 : 0.050682376271017716
Loss in iteration 127 : 0.05043662499905329
Loss in iteration 128 : 0.050193674441735336
Loss in iteration 129 : 0.04995347419215574
Loss in iteration 130 : 0.04971597509122904
Loss in iteration 131 : 0.04948112918761028
Loss in iteration 132 : 0.04924888969923056
Loss in iteration 133 : 0.04901921097636889
Loss in iteration 134 : 0.048792048466186635
Loss in iteration 135 : 0.04856735867865314
Loss in iteration 136 : 0.04834509915379646
Loss in iteration 137 : 0.04812522843021574
Loss in iteration 138 : 0.04790770601479618
Loss in iteration 139 : 0.04769249235357123
Loss in iteration 140 : 0.04747954880367719
Loss in iteration 141 : 0.04726883760635344
Loss in iteration 142 : 0.04706032186093695
Loss in iteration 143 : 0.04685396549981013
Loss in iteration 144 : 0.04664973326425682
Loss in iteration 145 : 0.04644759068118817
Loss in iteration 146 : 0.04624750404069922
Loss in iteration 147 : 0.04604944037442075
Loss in iteration 148 : 0.0458533674346318
Loss in iteration 149 : 0.04565925367410106
Loss in iteration 150 : 0.045467068226625565
Loss in iteration 151 : 0.04527678088823815
Loss in iteration 152 : 0.04508836209905471
Loss in iteration 153 : 0.04490178292573702
Loss in iteration 154 : 0.044717015044543416
Loss in iteration 155 : 0.044534030724945134
Loss in iteration 156 : 0.044352802813785355
Loss in iteration 157 : 0.044173304719958256
Loss in iteration 158 : 0.04399551039958901
Loss in iteration 159 : 0.04381939434169365
Loss in iteration 160 : 0.043644931554300756
Loss in iteration 161 : 0.04347209755101724
Loss in iteration 162 : 0.043300868338020305
Loss in iteration 163 : 0.043131220401460627
Loss in iteration 164 : 0.04296313069525959
Loss in iteration 165 : 0.04279657662928717
Loss in iteration 166 : 0.04263153605790564
Loss in iteration 167 : 0.04246798726886508
Loss in iteration 168 : 0.04230590897253876
Loss in iteration 169 : 0.04214528029148516
Loss in iteration 170 : 0.04198608075032505
Loss in iteration 171 : 0.041828290265922004
Loss in iteration 172 : 0.04167188913785631
Loss in iteration 173 : 0.041516858039180565
Loss in iteration 174 : 0.04136317800744789
Loss in iteration 175 : 0.04121083043600309
Loss in iteration 176 : 0.04105979706552743
Loss in iteration 177 : 0.04091005997582785
Loss in iteration 178 : 0.04076160157786296
Loss in iteration 179 : 0.04061440460599706
Loss in iteration 180 : 0.04046845211047521
Loss in iteration 181 : 0.040323727450110494
Loss in iteration 182 : 0.04018021428517774
Loss in iteration 183 : 0.040037896570506236
Loss in iteration 184 : 0.03989675854876473
Loss in iteration 185 : 0.039756784743932114
Loss in iteration 186 : 0.0396179599549486
Loss in iteration 187 : 0.0394802692495408
Loss in iteration 188 : 0.03934369795821457
Loss in iteration 189 : 0.03920823166841188
Loss in iteration 190 : 0.03907385621882441
Loss in iteration 191 : 0.038940557693860206
Loss in iteration 192 : 0.038808322418258365
Loss in iteration 193 : 0.03867713695184649
Loss in iteration 194 : 0.03854698808443715
Loss in iteration 195 : 0.038417862830858227
Loss in iteration 196 : 0.03828974842611391
Loss in iteration 197 : 0.03816263232067141
Loss in iteration 198 : 0.03803650217587015
Loss in iteration 199 : 0.03791134585944924
Loss in iteration 200 : 0.037787151441190076
Testing accuracy  of updater 0 on alg 0 with rate 1.0 = 0.9742222222222222, training accuracy 0.9958559588453844, time elapsed: 6251 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6603165680413241
Loss in iteration 3 : 0.6313466773629935
Loss in iteration 4 : 0.6053872821391052
Loss in iteration 5 : 0.5818895674413216
Loss in iteration 6 : 0.5604766905691023
Loss in iteration 7 : 0.540871706570805
Loss in iteration 8 : 0.5228582315472522
Loss in iteration 9 : 0.5062590097666047
Loss in iteration 10 : 0.4909240370566345
Loss in iteration 11 : 0.4767237228796188
Loss in iteration 12 : 0.4635447060041061
Loss in iteration 13 : 0.4512870851734738
Loss in iteration 14 : 0.4398624281546784
Loss in iteration 15 : 0.42919223313883864
Loss in iteration 16 : 0.41920667450325816
Loss in iteration 17 : 0.40984354447440985
Loss in iteration 18 : 0.40104734194445574
Loss in iteration 19 : 0.39276847948472743
Loss in iteration 20 : 0.38496258955497303
Loss in iteration 21 : 0.37758991607167647
Loss in iteration 22 : 0.37061478035875395
Loss in iteration 23 : 0.3640051122642144
Loss in iteration 24 : 0.35773203845842844
Loss in iteration 25 : 0.351769520897577
Loss in iteration 26 : 0.3460940392572911
Loss in iteration 27 : 0.34068431186710973
Loss in iteration 28 : 0.3355210503270963
Loss in iteration 29 : 0.330586743572712
Loss in iteration 30 : 0.3258654676778241
Loss in iteration 31 : 0.32134271815204274
Loss in iteration 32 : 0.31700526190160766
Loss in iteration 33 : 0.31284100638676743
Loss in iteration 34 : 0.30883888382759084
Loss in iteration 35 : 0.30498874858892
Loss in iteration 36 : 0.3012812861180294
Loss in iteration 37 : 0.2977079320197881
Loss in iteration 38 : 0.29426080003747557
Loss in iteration 39 : 0.290932617866445
Loss in iteration 40 : 0.28771666986567945
Loss in iteration 41 : 0.2846067458517574
Loss in iteration 42 : 0.2815970952632529
Loss in iteration 43 : 0.2786823860733476
Loss in iteration 44 : 0.2758576679062418
Loss in iteration 45 : 0.2731183388804989
Loss in iteration 46 : 0.2704601157610926
Loss in iteration 47 : 0.2678790070529091
Loss in iteration 48 : 0.26537128871281607
Loss in iteration 49 : 0.262933482196021
Loss in iteration 50 : 0.260562334586138
Loss in iteration 51 : 0.258254800587778
Loss in iteration 52 : 0.2560080261861555
Loss in iteration 53 : 0.2538193338007127
Loss in iteration 54 : 0.2516862087794328
Loss in iteration 55 : 0.24960628709779817
Loss in iteration 56 : 0.24757734414153035
Loss in iteration 57 : 0.2455972844655773
Loss in iteration 58 : 0.2436641324335673
Loss in iteration 59 : 0.2417760236523079
Loss in iteration 60 : 0.23993119712505157
Loss in iteration 61 : 0.23812798805531507
Loss in iteration 62 : 0.23636482124021546
Loss in iteration 63 : 0.23464020499859348
Loss in iteration 64 : 0.2329527255848352
Loss in iteration 65 : 0.23130104204428206
Loss in iteration 66 : 0.22968388147056273
Loss in iteration 67 : 0.2281000346291329
Loss in iteration 68 : 0.22654835191482534
Loss in iteration 69 : 0.22502773961436284
Loss in iteration 70 : 0.22353715644758865
Loss in iteration 71 : 0.22207561036368134
Loss in iteration 72 : 0.22064215557087447
Loss in iteration 73 : 0.2192358897802101
Loss in iteration 74 : 0.21785595164566485
Loss in iteration 75 : 0.2165015183846199
Loss in iteration 76 : 0.2151718035640933
Loss in iteration 77 : 0.21386605503947978
Loss in iteration 78 : 0.2125835530337228
Loss in iteration 79 : 0.2113236083459133
Loss in iteration 80 : 0.21008556067927214
Loss in iteration 81 : 0.20886877707934975
Loss in iteration 82 : 0.2076726504740609
Loss in iteration 83 : 0.2064965983078923
Loss in iteration 84 : 0.2053400612632563
Loss in iteration 85 : 0.2042025020625686
Loss in iteration 86 : 0.20308340434514263
Loss in iteration 87 : 0.20198227161349383
Loss in iteration 88 : 0.2008986262440737
Loss in iteration 89 : 0.1998320085578658
Loss in iteration 90 : 0.19878197594663152
Loss in iteration 91 : 0.19774810205093865
Loss in iteration 92 : 0.19672997598639427
Loss in iteration 93 : 0.1957272016147962
Loss in iteration 94 : 0.19473939685716093
Loss in iteration 95 : 0.19376619304582215
Loss in iteration 96 : 0.19280723431300995
Loss in iteration 97 : 0.19186217701350405
Loss in iteration 98 : 0.1909306891791484
Loss in iteration 99 : 0.19001245000316522
Loss in iteration 100 : 0.1891071493523666
Loss in iteration 101 : 0.18821448730548984
Loss in iteration 102 : 0.18733417371602545
Loss in iteration 103 : 0.1864659277980013
Loss in iteration 104 : 0.1856094777333187
Loss in iteration 105 : 0.1847645602993165
Loss in iteration 106 : 0.18393092051534102
Loss in iteration 107 : 0.18310831130717983
Loss in iteration 108 : 0.18229649318829652
Loss in iteration 109 : 0.18149523395687636
Loss in iteration 110 : 0.18070430840776058
Loss in iteration 111 : 0.17992349805839977
Loss in iteration 112 : 0.1791525908880322
Loss in iteration 113 : 0.17839138108932298
Loss in iteration 114 : 0.1776396688317674
Loss in iteration 115 : 0.17689726003620096
Loss in iteration 116 : 0.17616396615979213
Loss in iteration 117 : 0.17543960399095057
Loss in iteration 118 : 0.17472399545360512
Loss in iteration 119 : 0.17401696742034345
Loss in iteration 120 : 0.17331835153394176
Loss in iteration 121 : 0.17262798403683743
Loss in iteration 122 : 0.17194570560812103
Loss in iteration 123 : 0.17127136120766354
Loss in iteration 124 : 0.17060479992699898
Loss in iteration 125 : 0.169945874846621
Loss in iteration 126 : 0.16929444289936335
Loss in iteration 127 : 0.1686503647395581
Loss in iteration 128 : 0.16801350461767936
Loss in iteration 129 : 0.1673837302602004
Loss in iteration 130 : 0.16676091275440652
Loss in iteration 131 : 0.16614492643792209
Loss in iteration 132 : 0.1655356487927175
Loss in iteration 133 : 0.1649329603433858
Loss in iteration 134 : 0.16433674455948222
Loss in iteration 135 : 0.16374688776173169
Loss in iteration 136 : 0.16316327903192349
Loss in iteration 137 : 0.1625858101263231
Loss in iteration 138 : 0.16201437539243257
Loss in iteration 139 : 0.16144887168894914
Loss in iteration 140 : 0.16088919830877466
Loss in iteration 141 : 0.16033525690493777
Loss in iteration 142 : 0.15978695141929602
Loss in iteration 143 : 0.15924418801389564
Loss in iteration 144 : 0.1587068750048703
Loss in iteration 145 : 0.15817492279876516
Loss in iteration 146 : 0.157648243831184
Loss in iteration 147 : 0.15712675250765504
Loss in iteration 148 : 0.15661036514662055
Loss in iteration 149 : 0.1560989999244603
Loss in iteration 150 : 0.15559257682246333
Loss in iteration 151 : 0.15509101757566104
Loss in iteration 152 : 0.1545942456234503
Loss in iteration 153 : 0.15410218606192663
Loss in iteration 154 : 0.15361476559785978
Loss in iteration 155 : 0.153131912504243
Loss in iteration 156 : 0.15265355657735252
Loss in iteration 157 : 0.15217962909525518
Loss in iteration 158 : 0.15171006277770754
Loss in iteration 159 : 0.151244791747389
Loss in iteration 160 : 0.1507837514924186
Loss in iteration 161 : 0.15032687883009996
Loss in iteration 162 : 0.14987411187185448
Loss in iteration 163 : 0.14942538998928656
Loss in iteration 164 : 0.1489806537813462
Loss in iteration 165 : 0.14853984504254084
Loss in iteration 166 : 0.14810290673215928
Loss in iteration 167 : 0.14766978294446728
Loss in iteration 168 : 0.1472404188798426
Loss in iteration 169 : 0.146814760816807
Loss in iteration 170 : 0.14639275608492852
Loss in iteration 171 : 0.14597435303856093
Loss in iteration 172 : 0.1455595010313841
Loss in iteration 173 : 0.14514815039172302
Loss in iteration 174 : 0.14474025239861402
Loss in iteration 175 : 0.14433575925859082
Loss in iteration 176 : 0.1439346240831646
Loss in iteration 177 : 0.14353680086697712
Loss in iteration 178 : 0.14314224446659557
Loss in iteration 179 : 0.14275091057993491
Loss in iteration 180 : 0.14236275572628027
Loss in iteration 181 : 0.14197773722688903
Loss in iteration 182 : 0.14159581318615685
Loss in iteration 183 : 0.14121694247332084
Loss in iteration 184 : 0.14084108470469
Loss in iteration 185 : 0.14046820022637704
Loss in iteration 186 : 0.14009825009752067
Loss in iteration 187 : 0.13973119607397913
Loss in iteration 188 : 0.1393670005924787
Loss in iteration 189 : 0.13900562675520425
Loss in iteration 190 : 0.13864703831481515
Loss in iteration 191 : 0.13829119965987355
Loss in iteration 192 : 0.13793807580067172
Loss in iteration 193 : 0.13758763235544474
Loss in iteration 194 : 0.1372398355369576
Loss in iteration 195 : 0.1368946521394514
Loss in iteration 196 : 0.1365520495259413
Loss in iteration 197 : 0.1362119956158529
Loss in iteration 198 : 0.13587445887298555
Loss in iteration 199 : 0.13553940829379396
Loss in iteration 200 : 0.13520681339597707
Testing accuracy  of updater 0 on alg 0 with rate 0.09999999999999998 = 0.984, training accuracy 0.9719919977136324, time elapsed: 5537 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.8982803999400666
Loss in iteration 3 : 0.25662086716860866
Loss in iteration 4 : 0.5487744360808786
Loss in iteration 5 : 0.2910037485550582
Loss in iteration 6 : 0.5312696737303221
Loss in iteration 7 : 0.4233612288718494
Loss in iteration 8 : 0.27295466319696504
Loss in iteration 9 : 0.26044379283715785
Loss in iteration 10 : 0.2539042965028756
Loss in iteration 11 : 0.2249464025293236
Loss in iteration 12 : 0.15715971526066091
Loss in iteration 13 : 0.10384712269049588
Loss in iteration 14 : 0.09718466542060264
Loss in iteration 15 : 0.10158453458745989
Loss in iteration 16 : 0.09977026459950342
Loss in iteration 17 : 0.08969690911451185
Loss in iteration 18 : 0.0812774717791057
Loss in iteration 19 : 0.08477422516387981
Loss in iteration 20 : 0.0893203332933076
Loss in iteration 21 : 0.07791141666650557
Loss in iteration 22 : 0.06200878884367
Loss in iteration 23 : 0.053685963111639964
Loss in iteration 24 : 0.0490018384001112
Loss in iteration 25 : 0.04486607749572504
Loss in iteration 26 : 0.0403729166859781
Loss in iteration 27 : 0.03636532773904415
Loss in iteration 28 : 0.03349417917379552
Loss in iteration 29 : 0.03170431998394583
Loss in iteration 30 : 0.030554664095829787
Loss in iteration 31 : 0.029695664863173578
Loss in iteration 32 : 0.028887425481532086
Loss in iteration 33 : 0.027932230466222147
Loss in iteration 34 : 0.026725998813481446
Loss in iteration 35 : 0.025307667426824255
Loss in iteration 36 : 0.02382183113587692
Loss in iteration 37 : 0.022415845517599804
Loss in iteration 38 : 0.02115644963748457
Loss in iteration 39 : 0.020023331808166345
Loss in iteration 40 : 0.018956164135236542
Loss in iteration 41 : 0.017898031814052986
Loss in iteration 42 : 0.016812026885926996
Loss in iteration 43 : 0.015680743112052628
Loss in iteration 44 : 0.014501387158491297
Loss in iteration 45 : 0.013281841440729063
Loss in iteration 46 : 0.012038545190719698
Loss in iteration 47 : 0.010795589715667962
Loss in iteration 48 : 0.009583886615378201
Loss in iteration 49 : 0.00843884632292311
Loss in iteration 50 : 0.0073952508273350485
Loss in iteration 51 : 0.00648042487618223
Loss in iteration 52 : 0.005705778352069532
Loss in iteration 53 : 0.005048444478427859
Loss in iteration 54 : 0.0044466141893629175
Loss in iteration 55 : 0.003848578610957007
Loss in iteration 56 : 0.0032548552460680063
Loss in iteration 57 : 0.0027015662592729723
Loss in iteration 58 : 0.002225740042869016
Loss in iteration 59 : 0.0018477913892515376
Loss in iteration 60 : 0.0015699588577579402
Loss in iteration 61 : 0.001380502759397382
Loss in iteration 62 : 0.0012587078594561414
Loss in iteration 63 : 0.0011798508481100979
Loss in iteration 64 : 0.0011201184239979286
Loss in iteration 65 : 0.0010609952439677277
Loss in iteration 66 : 9.921020542821284E-4
Loss in iteration 67 : 9.11545165333514E-4
Loss in iteration 68 : 8.237540981006566E-4
Loss in iteration 69 : 7.359526240283817E-4
Loss in iteration 70 : 6.548928436135872E-4
Loss in iteration 71 : 5.849549135632494E-4
Loss in iteration 72 : 5.277429137093706E-4
Loss in iteration 73 : 4.826462853823816E-4
Loss in iteration 74 : 4.477329207849875E-4
Loss in iteration 75 : 4.2056127372014E-4
Loss in iteration 76 : 3.9875091913463306E-4
Loss in iteration 77 : 3.803058988236493E-4
Loss in iteration 78 : 3.6374606732100226E-4
Loss in iteration 79 : 3.481102146820707E-4
Testing accuracy  of updater 1 on alg 0 with rate 10.0 = 0.9893333333333333, training accuracy 1.0, time elapsed: 2727 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.45859828352491205
Loss in iteration 3 : 0.27029421044944957
Loss in iteration 4 : 0.19442998949271065
Loss in iteration 5 : 0.1586146713314852
Loss in iteration 6 : 0.13698566980742155
Loss in iteration 7 : 0.12289838066315058
Loss in iteration 8 : 0.11302625270262544
Loss in iteration 9 : 0.10528707288407149
Loss in iteration 10 : 0.09853401406914856
Loss in iteration 11 : 0.09223239841202353
Loss in iteration 12 : 0.08618761689509787
Loss in iteration 13 : 0.0803721606231997
Loss in iteration 14 : 0.07482475702474885
Loss in iteration 15 : 0.06959708373116444
Loss in iteration 16 : 0.06473414868198502
Loss in iteration 17 : 0.06027254085277619
Loss in iteration 18 : 0.05623904278654795
Loss in iteration 19 : 0.05264126653084795
Loss in iteration 20 : 0.04945667377138
Loss in iteration 21 : 0.046633094806205935
Loss in iteration 22 : 0.04410537514517343
Loss in iteration 23 : 0.04181744017012485
Loss in iteration 24 : 0.03973373274542833
Loss in iteration 25 : 0.037834179479020964
Loss in iteration 26 : 0.03610094024378791
Loss in iteration 27 : 0.0345095368036716
Loss in iteration 28 : 0.03303013422098589
Loss in iteration 29 : 0.03163551760672986
Loss in iteration 30 : 0.030308501633690647
Loss in iteration 31 : 0.029043943032812294
Loss in iteration 32 : 0.027845260832479762
Loss in iteration 33 : 0.02671856708553493
Loss in iteration 34 : 0.025667858385345663
Loss in iteration 35 : 0.024693076286354452
Loss in iteration 36 : 0.02379082838610786
Loss in iteration 37 : 0.02295636471229333
Loss in iteration 38 : 0.02218533809156214
Loss in iteration 39 : 0.021474551889767948
Loss in iteration 40 : 0.020821695668241148
Loss in iteration 41 : 0.020224553792567528
Loss in iteration 42 : 0.019680242192498823
Loss in iteration 43 : 0.019184825083551505
Loss in iteration 44 : 0.01873339036827823
Loss in iteration 45 : 0.01832046012201091
Loss in iteration 46 : 0.017940527811233134
Loss in iteration 47 : 0.017588529320284235
Loss in iteration 48 : 0.01726012834451384
Loss in iteration 49 : 0.0169517862092235
Loss in iteration 50 : 0.01666066008988489
Loss in iteration 51 : 0.01638441404990144
Loss in iteration 52 : 0.01612103052163178
Loss in iteration 53 : 0.01586868408700842
Loss in iteration 54 : 0.01562570005972738
Loss in iteration 55 : 0.015390583656563643
Loss in iteration 56 : 0.015162083058969286
Loss in iteration 57 : 0.014939245543812861
Loss in iteration 58 : 0.014721437096121115
Loss in iteration 59 : 0.0145083150109009
Loss in iteration 60 : 0.014299761543765667
Loss in iteration 61 : 0.014095798456614027
Loss in iteration 62 : 0.013896504902157516
Loss in iteration 63 : 0.013701955795773501
Loss in iteration 64 : 0.0135121882838443
Loss in iteration 65 : 0.01332719429004406
Loss in iteration 66 : 0.013146930526810808
Loss in iteration 67 : 0.01297133511564544
Loss in iteration 68 : 0.01280034163102539
Loss in iteration 69 : 0.012633885399404883
Loss in iteration 70 : 0.012471901359056015
Loss in iteration 71 : 0.01231431619729944
Loss in iteration 72 : 0.012161039049467111
Loss in iteration 73 : 0.012011954768183723
Loss in iteration 74 : 0.011866922218248463
Loss in iteration 75 : 0.011725778029714463
Loss in iteration 76 : 0.01158834449321402
Loss in iteration 77 : 0.011454439281808603
Loss in iteration 78 : 0.01132388456706221
Loss in iteration 79 : 0.01119651370332759
Loss in iteration 80 : 0.011072174647281833
Loss in iteration 81 : 0.010950730287794938
Loss in iteration 82 : 0.010832056593465195
Loss in iteration 83 : 0.010716039797250035
Loss in iteration 84 : 0.010602573739225669
Loss in iteration 85 : 0.010491558104464414
Loss in iteration 86 : 0.010382897800320201
Loss in iteration 87 : 0.010276503282114297
Loss in iteration 88 : 0.010172291368594984
Loss in iteration 89 : 0.010070186025023114
Loss in iteration 90 : 0.009970118702136425
Loss in iteration 91 : 0.009872028031144535
Loss in iteration 92 : 0.009775858903983994
Loss in iteration 93 : 0.009681561144517967
Loss in iteration 94 : 0.00958908806103688
Loss in iteration 95 : 0.009498395158616826
Loss in iteration 96 : 0.009409439204865934
Loss in iteration 97 : 0.00932217772278588
Loss in iteration 98 : 0.009236568869592107
Loss in iteration 99 : 0.009152571580156725
Loss in iteration 100 : 0.009070145822208582
Loss in iteration 101 : 0.008989252825343286
Loss in iteration 102 : 0.008909855193068998
Loss in iteration 103 : 0.0088319168668294
Loss in iteration 104 : 0.008755402964508803
Loss in iteration 105 : 0.008680279550448013
Loss in iteration 106 : 0.008606513404571314
Loss in iteration 107 : 0.008534071847426326
Loss in iteration 108 : 0.008462922653477223
Loss in iteration 109 : 0.008393034056483718
Loss in iteration 110 : 0.008324374826789606
Loss in iteration 111 : 0.008256914386279221
Loss in iteration 112 : 0.008190622924253996
Loss in iteration 113 : 0.008125471484751961
Loss in iteration 114 : 0.008061432008936796
Loss in iteration 115 : 0.007998477330532381
Loss in iteration 116 : 0.007936581133967687
Loss in iteration 117 : 0.007875717891569278
Loss in iteration 118 : 0.007815862797233347
Loss in iteration 119 : 0.007756991710509206
Loss in iteration 120 : 0.007699081118852561
Loss in iteration 121 : 0.007642108119127641
Loss in iteration 122 : 0.007586050414051488
Loss in iteration 123 : 0.007530886316252693
Loss in iteration 124 : 0.007476594752189995
Loss in iteration 125 : 0.007423155259868713
Loss in iteration 126 : 0.007370547977205338
Loss in iteration 127 : 0.007318753621016799
Loss in iteration 128 : 0.0072677534591172755
Loss in iteration 129 : 0.007217529279396614
Loss in iteration 130 : 0.007168063359919791
Loss in iteration 131 : 0.007119338443244471
Loss in iteration 132 : 0.007071337716716026
Loss in iteration 133 : 0.007024044798938708
Loss in iteration 134 : 0.006977443731331147
Loss in iteration 135 : 0.006931518972900986
Loss in iteration 136 : 0.006886255396183568
Loss in iteration 137 : 0.006841638282600392
Loss in iteration 138 : 0.00679765331611967
Loss in iteration 139 : 0.006754286574830218
Loss in iteration 140 : 0.006711524520678679
Loss in iteration 141 : 0.006669353988043528
Loss in iteration 142 : 0.006627762171979407
Loss in iteration 143 : 0.006586736616887068
Loss in iteration 144 : 0.00654626520611984
Loss in iteration 145 : 0.006506336152725
Loss in iteration 146 : 0.006466937991224034
Loss in iteration 147 : 0.0064280595701275405
Loss in iteration 148 : 0.006389690044785742
Loss in iteration 149 : 0.006351818870193753
Loss in iteration 150 : 0.006314435793473454
Loss in iteration 151 : 0.006277530845899684
Loss in iteration 152 : 0.006241094334485853
Loss in iteration 153 : 0.006205116833260836
Loss in iteration 154 : 0.006169589174435183
Loss in iteration 155 : 0.006134502439666346
Loss in iteration 156 : 0.0060998479516007715
Loss in iteration 157 : 0.006065617265808623
Loss in iteration 158 : 0.006031802163157433
Loss in iteration 159 : 0.005998394642607688
Loss in iteration 160 : 0.005965386914369814
Loss in iteration 161 : 0.005932771393342379
Loss in iteration 162 : 0.005900540692752998
Loss in iteration 163 : 0.005868687617943336
Loss in iteration 164 : 0.005837205160265968
Loss in iteration 165 : 0.005806086491089541
Loss in iteration 166 : 0.005775324955930523
Loss in iteration 167 : 0.005744914068742166
Loss in iteration 168 : 0.005714847506394666
Loss in iteration 169 : 0.005685119103373452
Loss in iteration 170 : 0.005655722846711819
Loss in iteration 171 : 0.005626652871159711
Loss in iteration 172 : 0.005597903454578605
Loss in iteration 173 : 0.005569469013542811
Loss in iteration 174 : 0.005541344099123945
Loss in iteration 175 : 0.005513523392834475
Loss in iteration 176 : 0.00548600170271043
Loss in iteration 177 : 0.005458773959518618
Loss in iteration 178 : 0.005431835213079835
Loss in iteration 179 : 0.005405180628705169
Loss in iteration 180 : 0.0053788054837457105
Loss in iteration 181 : 0.005352705164258006
Loss in iteration 182 : 0.005326875161787599
Loss in iteration 183 : 0.005301311070271096
Loss in iteration 184 : 0.005276008583055407
Loss in iteration 185 : 0.005250963490030347
Loss in iteration 186 : 0.005226171674868776
Loss in iteration 187 : 0.005201629112367516
Loss in iteration 188 : 0.005177331865881785
Loss in iteration 189 : 0.005153276084846521
Loss in iteration 190 : 0.005129458002378618
Loss in iteration 191 : 0.005105873932955693
Loss in iteration 192 : 0.005082520270168057
Loss in iteration 193 : 0.00505939348454143
Loss in iteration 194 : 0.005036490121429166
Loss in iteration 195 : 0.005013806798972303
Loss in iteration 196 : 0.004991340206126652
Loss in iteration 197 : 0.0049690871007550454
Loss in iteration 198 : 0.004947044307783234
Loss in iteration 199 : 0.004925208717417083
Loss in iteration 200 : 0.004903577283418608
Testing accuracy  of updater 1 on alg 0 with rate 1.0 = 0.9884444444444445, training accuracy 0.9997142040583024, time elapsed: 5863 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6603165680413241
Loss in iteration 3 : 0.6053888150136256
Loss in iteration 4 : 0.5406228435382018
Loss in iteration 5 : 0.4753068823098476
Loss in iteration 6 : 0.41493492086818357
Loss in iteration 7 : 0.36242736211252613
Loss in iteration 8 : 0.3189302089101973
Loss in iteration 9 : 0.2841089695944797
Loss in iteration 10 : 0.25666311387666957
Loss in iteration 11 : 0.23498617476105652
Loss in iteration 12 : 0.21761761806468527
Loss in iteration 13 : 0.20341114656005335
Loss in iteration 14 : 0.19153131464051193
Loss in iteration 15 : 0.18138965031308882
Loss in iteration 16 : 0.1725766615191262
Loss in iteration 17 : 0.16480736512662011
Loss in iteration 18 : 0.157881570174775
Loss in iteration 19 : 0.15165585718602204
Loss in iteration 20 : 0.14602418351380528
Loss in iteration 21 : 0.14090482852488975
Loss in iteration 22 : 0.1362319842229479
Loss in iteration 23 : 0.13195064741845494
Loss in iteration 24 : 0.12801371743818082
Loss in iteration 25 : 0.12438043606225278
Loss in iteration 26 : 0.12101554020232547
Loss in iteration 27 : 0.11788871511390682
Loss in iteration 28 : 0.1149741159464536
Loss in iteration 29 : 0.11224985745998961
Loss in iteration 30 : 0.10969745604184981
Loss in iteration 31 : 0.10730125226002396
Loss in iteration 32 : 0.10504785674074041
Loss in iteration 33 : 0.10292565796666182
Loss in iteration 34 : 0.10092441692402067
Loss in iteration 35 : 0.09903495735216822
Loss in iteration 36 : 0.09724894620805266
Loss in iteration 37 : 0.09555874920720171
Loss in iteration 38 : 0.09395734156812911
Loss in iteration 39 : 0.09243825380425869
Loss in iteration 40 : 0.09099553534837987
Loss in iteration 41 : 0.08962372351469326
Loss in iteration 42 : 0.08831781047098929
Loss in iteration 43 : 0.08707320549103437
Loss in iteration 44 : 0.08588569316573269
Loss in iteration 45 : 0.08475139024089469
Loss in iteration 46 : 0.08366670440539095
Loss in iteration 47 : 0.08262829797260249
Loss in iteration 48 : 0.08163305837593905
Loss in iteration 49 : 0.08067807613302024
Loss in iteration 50 : 0.07976062975138998
Loss in iteration 51 : 0.07887817617190171
Loss in iteration 52 : 0.0780283448772575
Loss in iteration 53 : 0.07720893373173568
Loss in iteration 54 : 0.07641790488714213
Loss in iteration 55 : 0.07565337957054584
Loss in iteration 56 : 0.074913631132464
Loss in iteration 57 : 0.0741970762662818
Loss in iteration 58 : 0.07350226472884924
Loss in iteration 59 : 0.07282786815456196
Loss in iteration 60 : 0.07217266865360722
Loss in iteration 61 : 0.07153554784136339
Loss in iteration 62 : 0.0709154768007521
Loss in iteration 63 : 0.07031150728065233
Loss in iteration 64 : 0.0697227642270475
Loss in iteration 65 : 0.06914843956574661
Loss in iteration 66 : 0.06858778702947657
Loss in iteration 67 : 0.06804011775705546
Loss in iteration 68 : 0.06750479638514306
Loss in iteration 69 : 0.06698123739171996
Loss in iteration 70 : 0.06646890151811649
Loss in iteration 71 : 0.0659672921752657
Loss in iteration 72 : 0.06547595181432782
Loss in iteration 73 : 0.0649944583005963
Loss in iteration 74 : 0.06452242136652626
Loss in iteration 75 : 0.06405947923367343
Loss in iteration 76 : 0.06360529548716491
Loss in iteration 77 : 0.06315955626556086
Loss in iteration 78 : 0.06272196780023316
Loss in iteration 79 : 0.06229225430811619
Loss in iteration 80 : 0.06187015621509005
Loss in iteration 81 : 0.061455428667810286
Loss in iteration 82 : 0.06104784028103883
Loss in iteration 83 : 0.06064717206530216
Loss in iteration 84 : 0.060253216484531136
Loss in iteration 85 : 0.059865776603000125
Loss in iteration 86 : 0.05948466529290302
Loss in iteration 87 : 0.05910970448601044
Loss in iteration 88 : 0.058740724463269334
Loss in iteration 89 : 0.05837756318383679
Loss in iteration 90 : 0.05802006565944043
Loss in iteration 91 : 0.057668083381296226
Loss in iteration 92 : 0.05732147380567631
Loss in iteration 93 : 0.05698009990142175
Loss in iteration 94 : 0.056643829759139376
Loss in iteration 95 : 0.056312536258286736
Loss in iteration 96 : 0.055986096785459724
Loss in iteration 97 : 0.05566439299530896
Loss in iteration 98 : 0.055347310604769114
Loss in iteration 99 : 0.05503473921160267
Loss in iteration 100 : 0.054726572129427525
Loss in iteration 101 : 0.05442270623310361
Loss in iteration 102 : 0.05412304181028525
Loss in iteration 103 : 0.05382748241681006
Loss in iteration 104 : 0.05353593473517132
Loss in iteration 105 : 0.05324830843648641
Loss in iteration 106 : 0.05296451604705868
Loss in iteration 107 : 0.05268447282088543
Loss in iteration 108 : 0.052408096619350565
Loss in iteration 109 : 0.05213530779897746
Loss in iteration 110 : 0.05186602910762318
Loss in iteration 111 : 0.05160018558897947
Loss in iteration 112 : 0.05133770449479337
Loss in iteration 113 : 0.05107851520389395
Loss in iteration 114 : 0.0508225491469254
Loss in iteration 115 : 0.050569739735658516
Loss in iteration 116 : 0.05032002229583781
Loss in iteration 117 : 0.05007333400270303
Loss in iteration 118 : 0.04982961381855257
Loss in iteration 119 : 0.049588802431954127
Loss in iteration 120 : 0.049350842198424935
Loss in iteration 121 : 0.04911567708257607
Loss in iteration 122 : 0.04888325260183524
Loss in iteration 123 : 0.04865351577191641
Loss in iteration 124 : 0.04842641505421889
Loss in iteration 125 : 0.04820190030530116
Loss in iteration 126 : 0.04797992272851984
Loss in iteration 127 : 0.04776043482785421
Loss in iteration 128 : 0.047543390363868974
Loss in iteration 129 : 0.04732874431170966
Loss in iteration 130 : 0.04711645282098683
Loss in iteration 131 : 0.04690647317738191
Loss in iteration 132 : 0.046698763765805895
Loss in iteration 133 : 0.04649328403495391
Loss in iteration 134 : 0.04628999446312138
Loss in iteration 135 : 0.046088856525178905
Loss in iteration 136 : 0.04588983266062795
Loss in iteration 137 : 0.045692886242694054
Loss in iteration 138 : 0.045497981548429316
Loss in iteration 139 : 0.04530508372981631
Loss in iteration 140 : 0.045114158785869975
Loss in iteration 141 : 0.04492517353573965
Loss in iteration 142 : 0.044738095592804394
Loss in iteration 143 : 0.04455289333975465
Loss in iteration 144 : 0.0443695359046376
Loss in iteration 145 : 0.04418799313784095
Loss in iteration 146 : 0.044008235589978344
Loss in iteration 147 : 0.04383023449063518
Loss in iteration 148 : 0.04365396172793149
Loss in iteration 149 : 0.043479389828855255
Loss in iteration 150 : 0.04330649194032365
Loss in iteration 151 : 0.04313524181092985
Loss in iteration 152 : 0.042965613773339
Loss in iteration 153 : 0.04279758272730115
Loss in iteration 154 : 0.042631124123251
Loss in iteration 155 : 0.042466213946472764
Loss in iteration 156 : 0.04230282870180681
Loss in iteration 157 : 0.04214094539888016
Loss in iteration 158 : 0.041980541537844245
Loss in iteration 159 : 0.04182159509560297
Loss in iteration 160 : 0.04166408451251497
Loss in iteration 161 : 0.04150798867955487
Loss in iteration 162 : 0.04135328692591613
Loss in iteration 163 : 0.04119995900703963
Loss in iteration 164 : 0.04104798509305105
Loss in iteration 165 : 0.040897345757589444
Loss in iteration 166 : 0.040748021967011966
Loss in iteration 167 : 0.04059999506995719
Loss in iteration 168 : 0.0404532467872525
Loss in iteration 169 : 0.04030775920215137
Loss in iteration 170 : 0.040163514750885365
Loss in iteration 171 : 0.04002049621352049
Loss in iteration 172 : 0.039878686705104006
Loss in iteration 173 : 0.03973806966709222
Loss in iteration 174 : 0.039598628859048725
Loss in iteration 175 : 0.03946034835060264
Loss in iteration 176 : 0.03932321251365974
Loss in iteration 177 : 0.0391872060148558
Loss in iteration 178 : 0.039052313808244636
Loss in iteration 179 : 0.03891852112821354
Loss in iteration 180 : 0.038785813482616394
Loss in iteration 181 : 0.03865417664611862
Loss in iteration 182 : 0.03852359665374476
Loss in iteration 183 : 0.038394059794623125
Loss in iteration 184 : 0.03826555260591857
Loss in iteration 185 : 0.038138061866947605
Loss in iteration 186 : 0.03801157459346944
Loss in iteration 187 : 0.03788607803214511
Loss in iteration 188 : 0.03776155965516045
Loss in iteration 189 : 0.03763800715500571
Loss in iteration 190 : 0.03751540843940667
Loss in iteration 191 : 0.03739375162640186
Loss in iteration 192 : 0.03727302503956156
Loss in iteration 193 : 0.03715321720334174
Loss in iteration 194 : 0.03703431683857138
Loss in iteration 195 : 0.036916312858065146
Loss in iteration 196 : 0.03679919436236013
Loss in iteration 197 : 0.03668295063557126
Loss in iteration 198 : 0.03656757114136072
Loss in iteration 199 : 0.036453045519019456
Loss in iteration 200 : 0.0363393635796552
Testing accuracy  of updater 1 on alg 0 with rate 0.09999999999999998 = 0.9715555555555555, training accuracy 0.9955701629036867, time elapsed: 4533 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 1.6660322018952403
Loss in iteration 3 : 1.2178879602564268
Loss in iteration 4 : 3.223712439428169
Loss in iteration 5 : 0.8616268489875164
Loss in iteration 6 : 0.5155683052177406
Loss in iteration 7 : 0.5724973651256394
Loss in iteration 8 : 0.5072359890021207
Loss in iteration 9 : 0.38961195426253137
Loss in iteration 10 : 0.29429982216425277
Loss in iteration 11 : 0.23783706339609442
Loss in iteration 12 : 0.20554507431017546
Loss in iteration 13 : 0.17940828088642535
Loss in iteration 14 : 0.15527211563696902
Loss in iteration 15 : 0.13492628394785386
Loss in iteration 16 : 0.12236027471795896
Loss in iteration 17 : 0.11553419625614553
Loss in iteration 18 : 0.10933800587802325
Loss in iteration 19 : 0.10165265885309895
Loss in iteration 20 : 0.09349312144704407
Loss in iteration 21 : 0.08717399954856778
Loss in iteration 22 : 0.08212313484382719
Loss in iteration 23 : 0.0772799115794987
Loss in iteration 24 : 0.07189074633512899
Loss in iteration 25 : 0.06607046856893858
Loss in iteration 26 : 0.06033268545928196
Loss in iteration 27 : 0.055044711868213456
Loss in iteration 28 : 0.050297781710411216
Loss in iteration 29 : 0.046209934033937715
Loss in iteration 30 : 0.04284653233229737
Loss in iteration 31 : 0.04017380338973691
Loss in iteration 32 : 0.038052251841869715
Loss in iteration 33 : 0.0363112655191091
Loss in iteration 34 : 0.03481485490168967
Loss in iteration 35 : 0.03346506962023668
Loss in iteration 36 : 0.03219367720124202
Loss in iteration 37 : 0.030958359464379175
Loss in iteration 38 : 0.029736234671262022
Loss in iteration 39 : 0.0285162384424536
Loss in iteration 40 : 0.027293689925842007
Loss in iteration 41 : 0.02606717762112733
Loss in iteration 42 : 0.024836876493147763
Loss in iteration 43 : 0.023603648740435447
Loss in iteration 44 : 0.022368581553263902
Loss in iteration 45 : 0.021132774599865883
Loss in iteration 46 : 0.019897264937042223
Loss in iteration 47 : 0.018663020266975354
Loss in iteration 48 : 0.017430967056965167
Loss in iteration 49 : 0.01620205826860486
Loss in iteration 50 : 0.014977436890992861
Loss in iteration 51 : 0.013758839944172572
Loss in iteration 52 : 0.012549558774536923
Loss in iteration 53 : 0.011356545578188942
Loss in iteration 54 : 0.010194261336440939
Loss in iteration 55 : 0.009088660423860893
Loss in iteration 56 : 0.008073038629623918
Loss in iteration 57 : 0.007169550301787305
Loss in iteration 58 : 0.00637794161234202
Loss in iteration 59 : 0.005687113070367899
Loss in iteration 60 : 0.005085285997329411
Loss in iteration 61 : 0.0045601909737903144
Loss in iteration 62 : 0.004099280336600508
Loss in iteration 63 : 0.003691892207054198
Loss in iteration 64 : 0.0033304749817994876
Loss in iteration 65 : 0.0030103524618730025
Loss in iteration 66 : 0.00272897282244417
Loss in iteration 67 : 0.002485018060335993
Loss in iteration 68 : 0.0022772683495483233
Loss in iteration 69 : 0.0021033618129206416
Loss in iteration 70 : 0.001959031930653416
Loss in iteration 71 : 0.0018383324674103346
Loss in iteration 72 : 0.0017347076223217823
Loss in iteration 73 : 0.0016422412866577208
Loss in iteration 74 : 0.001556496497031482
Loss in iteration 75 : 0.0014747572071718536
Loss in iteration 76 : 0.0013957967747354554
Loss in iteration 77 : 0.001319408265296315
Loss in iteration 78 : 0.0012459074100456642
Loss in iteration 79 : 0.0011757397869493285
Loss in iteration 80 : 0.0011092411254358678
Loss in iteration 81 : 0.0010465397708475904
Loss in iteration 82 : 9.87560180818181E-4
Loss in iteration 83 : 9.320805388968265E-4
Loss in iteration 84 : 8.798061871307057E-4
Loss in iteration 85 : 8.304344164672043E-4
Loss in iteration 86 : 7.836991979875215E-4
Loss in iteration 87 : 7.393939392286193E-4
Testing accuracy  of updater 2 on alg 0 with rate 10.0 = 0.9848888888888889, training accuracy 0.9998571020291512, time elapsed: 2155 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.38662499419280294
Loss in iteration 3 : 0.30548939868474273
Loss in iteration 4 : 0.23935035529819632
Loss in iteration 5 : 0.1428464391819673
Loss in iteration 6 : 0.11982297701420228
Loss in iteration 7 : 0.10927266284116101
Loss in iteration 8 : 0.09947178714949753
Loss in iteration 9 : 0.09111934606004508
Loss in iteration 10 : 0.08414481363502871
Loss in iteration 11 : 0.07811252860595264
Loss in iteration 12 : 0.07267524496979494
Loss in iteration 13 : 0.06765633784016269
Loss in iteration 14 : 0.06300502281949519
Loss in iteration 15 : 0.05873047510768985
Loss in iteration 16 : 0.054850521376380024
Loss in iteration 17 : 0.05136347583990937
Loss in iteration 18 : 0.04824238109914792
Loss in iteration 19 : 0.04544503354165413
Loss in iteration 20 : 0.042928129886655154
Loss in iteration 21 : 0.04065544868035632
Loss in iteration 22 : 0.03859796310332098
Loss in iteration 23 : 0.03673026692488725
Loss in iteration 24 : 0.035028069854409004
Loss in iteration 25 : 0.033468276579740254
Loss in iteration 26 : 0.032030497713665555
Loss in iteration 27 : 0.030698330805200524
Loss in iteration 28 : 0.029459598620364458
Loss in iteration 29 : 0.028305670456941354
Loss in iteration 30 : 0.027230399278815154
Loss in iteration 31 : 0.02622913356091951
Loss in iteration 32 : 0.025298008766113163
Loss in iteration 33 : 0.02443351769655278
Loss in iteration 34 : 0.023632272421614597
Loss in iteration 35 : 0.022890872353016838
Loss in iteration 36 : 0.02220582813928357
Loss in iteration 37 : 0.021573522262984403
Loss in iteration 38 : 0.02099020240228272
Loss in iteration 39 : 0.020452005820233203
Loss in iteration 40 : 0.019955009572873595
Loss in iteration 41 : 0.019495297712285916
Loss in iteration 42 : 0.0190690352748817
Loss in iteration 43 : 0.01867253978301684
Loss in iteration 44 : 0.01830234335604792
Loss in iteration 45 : 0.01795524132653233
Loss in iteration 46 : 0.017628325774666807
Loss in iteration 47 : 0.01731900427192126
Loss in iteration 48 : 0.01702500529290109
Loss in iteration 49 : 0.016744372322327644
Loss in iteration 50 : 0.01647544884010614
Loss in iteration 51 : 0.01621685630082469
Loss in iteration 52 : 0.01596746707564259
Loss in iteration 53 : 0.01572637416923154
Loss in iteration 54 : 0.015492859378880508
Loss in iteration 55 : 0.015266361408886524
Loss in iteration 56 : 0.015046445264435547
Loss in iteration 57 : 0.014832774010242722
Loss in iteration 58 : 0.014625083694917817
Loss in iteration 59 : 0.014423161934415918
Loss in iteration 60 : 0.014226830347798808
Loss in iteration 61 : 0.014035930775370277
Loss in iteration 62 : 0.01385031500385167
Loss in iteration 63 : 0.01366983758452386
Loss in iteration 64 : 0.013494351255947643
Loss in iteration 65 : 0.013323704462870591
Loss in iteration 66 : 0.013157740483324668
Loss in iteration 67 : 0.01299629772250786
Loss in iteration 68 : 0.012839210792656711
Loss in iteration 69 : 0.012686312063633638
Loss in iteration 70 : 0.012537433433296882
Loss in iteration 71 : 0.012392408126312367
Loss in iteration 72 : 0.012251072383053493
Loss in iteration 73 : 0.012113266945840676
Loss in iteration 74 : 0.011978838287825935
Loss in iteration 75 : 0.011847639560491285
Loss in iteration 76 : 0.011719531259395765
Loss in iteration 77 : 0.011594381625041818
Loss in iteration 78 : 0.011472066807271752
Loss in iteration 79 : 0.011352470828301134
Loss in iteration 80 : 0.011235485382276377
Loss in iteration 81 : 0.011121009509033008
Loss in iteration 82 : 0.01100894917739672
Loss in iteration 83 : 0.010899216809666198
Loss in iteration 84 : 0.010791730774461388
Loss in iteration 85 : 0.010686414870390857
Loss in iteration 86 : 0.010583197818326026
Loss in iteration 87 : 0.010482012775702544
Loss in iteration 88 : 0.010382796882340612
Loss in iteration 89 : 0.010285490843867174
Loss in iteration 90 : 0.010190038555961551
Loss in iteration 91 : 0.010096386770333247
Loss in iteration 92 : 0.010004484801546486
Loss in iteration 93 : 0.009914284272489237
Loss in iteration 94 : 0.009825738895389317
Loss in iteration 95 : 0.009738804284744416
Loss in iteration 96 : 0.009653437798291848
Loss in iteration 97 : 0.009569598402134793
Loss in iteration 98 : 0.009487246556304195
Loss in iteration 99 : 0.009406344117317058
Loss in iteration 100 : 0.00932685425464723
Loss in iteration 101 : 0.009248741378417996
Loss in iteration 102 : 0.009171971076027624
Loss in iteration 103 : 0.009096510055807942
Loss in iteration 104 : 0.00902232609617934
Loss in iteration 105 : 0.008949387999091148
Loss in iteration 106 : 0.008877665546821295
Loss in iteration 107 : 0.008807129461450099
Loss in iteration 108 : 0.008737751366521463
Loss in iteration 109 : 0.00866950375056185
Loss in iteration 110 : 0.008602359932248864
Loss in iteration 111 : 0.008536294027108772
Loss in iteration 112 : 0.008471280915682914
Loss in iteration 113 : 0.008407296213141098
Loss in iteration 114 : 0.00834431624033892
Loss in iteration 115 : 0.008282317996322891
Loss in iteration 116 : 0.00822127913228305
Loss in iteration 117 : 0.00816117792694319
Loss in iteration 118 : 0.00810199326336526
Loss in iteration 119 : 0.008043704607130086
Loss in iteration 120 : 0.00798629198584165
Loss in iteration 121 : 0.007929735969889943
Loss in iteration 122 : 0.00787401765439634
Loss in iteration 123 : 0.007819118642257783
Loss in iteration 124 : 0.007765021028201041
Loss in iteration 125 : 0.007711707383755832
Loss in iteration 126 : 0.007659160743056154
Loss in iteration 127 : 0.007607364589381445
Loss in iteration 128 : 0.0075563028423538525
Loss in iteration 129 : 0.0075059598457132455
Loss in iteration 130 : 0.007456320355599121
Loss in iteration 131 : 0.007407369529275428
Loss in iteration 132 : 0.007359092914242796
Loss in iteration 133 : 0.007311476437689815
Loss in iteration 134 : 0.007264506396243443
Loss in iteration 135 : 0.0072181694459850405
Loss in iteration 136 : 0.0071724525927056365
Loss in iteration 137 : 0.007127343182379833
Loss in iteration 138 : 0.007082828891842834
Loss in iteration 139 : 0.00703889771965968
Loss in iteration 140 : 0.006995537977179515
Loss in iteration 141 : 0.006952738279770576
Loss in iteration 142 : 0.006910487538233977
Loss in iteration 143 : 0.006868774950396216
Loss in iteration 144 : 0.006827589992881625
Loss in iteration 145 : 0.006786922413066233
Loss in iteration 146 : 0.00674676222121584
Loss in iteration 147 : 0.006707099682810149
Loss in iteration 148 : 0.0066679253110553905
Loss in iteration 149 : 0.00662922985958726
Loss in iteration 150 : 0.006591004315365344
Loss in iteration 151 : 0.006553239891759914
Loss in iteration 152 : 0.006515928021831263
Loss in iteration 153 : 0.00647906035180081
Loss in iteration 154 : 0.006442628734713196
Loss in iteration 155 : 0.006406625224287366
Loss in iteration 156 : 0.006371042068954465
Loss in iteration 157 : 0.0063358717060797
Loss in iteration 158 : 0.006301106756364897
Loss in iteration 159 : 0.006266740018428345
Loss in iteration 160 : 0.00623276446355749
Loss in iteration 161 : 0.006199173230631036
Loss in iteration 162 : 0.006165959621205293
Loss in iteration 163 : 0.006133117094760806
Loss in iteration 164 : 0.006100639264104292
Loss in iteration 165 : 0.006068519890921415
Loss in iteration 166 : 0.006036752881475639
Loss in iteration 167 : 0.006005332282448344
Loss in iteration 168 : 0.005974252276915922
Loss in iteration 169 : 0.005943507180458847
Loss in iteration 170 : 0.005913091437398666
Loss in iteration 171 : 0.0058829996171582445
Loss in iteration 172 : 0.0058532264107411655
Loss in iteration 173 : 0.005823766627326031
Loss in iteration 174 : 0.005794615190971766
Loss in iteration 175 : 0.005765767137429946
Loss in iteration 176 : 0.005737217611060415
Loss in iteration 177 : 0.005708961861846503
Loss in iteration 178 : 0.005680995242506499
Loss in iteration 179 : 0.0056533132056977315
Loss in iteration 180 : 0.005625911301310317
Loss in iteration 181 : 0.005598785173847011
Loss in iteration 182 : 0.005571930559886622
Loss in iteration 183 : 0.0055453432856276495
Loss in iteration 184 : 0.0055190192645095185
Loss in iteration 185 : 0.0054929544949087325
Loss in iteration 186 : 0.005467145057907197
Loss in iteration 187 : 0.005441587115130284
Loss in iteration 188 : 0.005416276906652237
Loss in iteration 189 : 0.005391210748966475
Loss in iteration 190 : 0.005366385033018635
Loss in iteration 191 : 0.00534179622230009
Loss in iteration 192 : 0.0053174408509999585
Loss in iteration 193 : 0.005293315522213507
Loss in iteration 194 : 0.005269416906205002
Loss in iteration 195 : 0.0052457417387232015
Loss in iteration 196 : 0.005222286819367574
Loss in iteration 197 : 0.005199049010003607
Loss in iteration 198 : 0.0051760252332254475
Loss in iteration 199 : 0.005153212470864373
Loss in iteration 200 : 0.0051306077625413145
Testing accuracy  of updater 2 on alg 0 with rate 1.0 = 0.9893333333333333, training accuracy 0.9998571020291512, time elapsed: 5759 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6326531461336025
Loss in iteration 3 : 0.5635345628267657
Loss in iteration 4 : 0.49475424193644124
Loss in iteration 5 : 0.431754206901754
Loss in iteration 6 : 0.37754449328065887
Loss in iteration 7 : 0.3329463937417748
Loss in iteration 8 : 0.2971533414226448
Loss in iteration 9 : 0.2686120931526105
Loss in iteration 10 : 0.24570082410543118
Loss in iteration 11 : 0.22704612408202415
Loss in iteration 12 : 0.21159478298953666
Loss in iteration 13 : 0.1985770410653671
Loss in iteration 14 : 0.18744055076689167
Loss in iteration 15 : 0.17778837933054426
Loss in iteration 16 : 0.16933090997493522
Loss in iteration 17 : 0.16185187854148003
Loss in iteration 18 : 0.15518567224050447
Loss in iteration 19 : 0.14920258776951656
Loss in iteration 20 : 0.14379928313813378
Loss in iteration 21 : 0.13889242911420668
Loss in iteration 22 : 0.13441426965918413
Loss in iteration 23 : 0.13030933096212108
Loss in iteration 24 : 0.12653186699520805
Loss in iteration 25 : 0.12304383051047654
Loss in iteration 26 : 0.1198132587679148
Loss in iteration 27 : 0.11681300484757591
Loss in iteration 28 : 0.11401975843359344
Loss in iteration 29 : 0.1114133029080394
Loss in iteration 30 : 0.10897595769367308
Loss in iteration 31 : 0.10669215933912757
Loss in iteration 32 : 0.10454814197007427
Loss in iteration 33 : 0.10253168626498134
Loss in iteration 34 : 0.10063191468281556
Loss in iteration 35 : 0.09883911821346304
Loss in iteration 36 : 0.09714460584762519
Loss in iteration 37 : 0.09554057211000717
Loss in iteration 38 : 0.09401998052002765
Loss in iteration 39 : 0.09257646207127591
Loss in iteration 40 : 0.09120422815304575
Loss in iteration 41 : 0.08989799715375607
Loss in iteration 42 : 0.08865293359575577
Loss in iteration 43 : 0.08746459827027776
Loss in iteration 44 : 0.08632890759437369
Loss in iteration 45 : 0.08524210034509201
Loss in iteration 46 : 0.08420071003035579
Loss in iteration 47 : 0.08320154138808118
Loss in iteration 48 : 0.08224164980882932
Loss in iteration 49 : 0.08131832279831262
Loss in iteration 50 : 0.08042906289123052
Loss in iteration 51 : 0.07957157166952268
Loss in iteration 52 : 0.07874373471411171
Loss in iteration 53 : 0.07794360743042399
Loss in iteration 54 : 0.07716940174417834
Loss in iteration 55 : 0.07641947367955162
Loss in iteration 56 : 0.07569231182220894
Loss in iteration 57 : 0.07498652664845973
Loss in iteration 58 : 0.07430084067926704
Loss in iteration 59 : 0.07363407940048812
Loss in iteration 60 : 0.07298516288150572
Loss in iteration 61 : 0.07235309802347603
Loss in iteration 62 : 0.07173697137410284
Loss in iteration 63 : 0.07113594245564839
Loss in iteration 64 : 0.07054923756416735
Loss in iteration 65 : 0.06997614400856307
Loss in iteration 66 : 0.0694160047665245
Loss in iteration 67 : 0.06886821354004395
Loss in iteration 68 : 0.06833221019600187
Loss in iteration 69 : 0.06780747657771503
Loss in iteration 70 : 0.06729353267208733
Loss in iteration 71 : 0.06678993311489304
Loss in iteration 72 : 0.06629626401445766
Loss in iteration 73 : 0.065812140072133
Loss in iteration 74 : 0.06533720197680734
Loss in iteration 75 : 0.06487111405038087
Loss in iteration 76 : 0.06441356212161925
Loss in iteration 77 : 0.06396425160693345
Loss in iteration 78 : 0.06352290577820294
Loss in iteration 79 : 0.06308926419955725
Loss in iteration 80 : 0.062663081316859
Loss in iteration 81 : 0.06224412518536065
Loss in iteration 82 : 0.061832176322540004
Loss in iteration 83 : 0.06142702667443005
Loss in iteration 84 : 0.06102847868484288
Loss in iteration 85 : 0.06063634445778584
Loss in iteration 86 : 0.06025044500411893
Loss in iteration 87 : 0.059870609564153965
Loss in iteration 88 : 0.059496674998497555
Loss in iteration 89 : 0.05912848524000719
Loss in iteration 90 : 0.058765890800296976
Loss in iteration 91 : 0.05840874832479412
Loss in iteration 92 : 0.058056920190905026
Loss in iteration 93 : 0.05771027414440721
Loss in iteration 94 : 0.05736868296971031
Loss in iteration 95 : 0.05703202419013103
Loss in iteration 96 : 0.05670017979478992
Loss in iteration 97 : 0.05637303598915021
Loss in iteration 98 : 0.056050482966592846
Loss in iteration 99 : 0.055732414698739456
Loss in iteration 100 : 0.0554187287425181
Loss in iteration 101 : 0.05510932606220512
Loss in iteration 102 : 0.05480411086488284
Loss in iteration 103 : 0.054502990447935815
Loss in iteration 104 : 0.054205875057359444
Loss in iteration 105 : 0.053912677755798576
Loss in iteration 106 : 0.053623314299350626
Loss in iteration 107 : 0.05333770302228378
Loss in iteration 108 : 0.053055764728913275
Loss in iteration 109 : 0.05277742259197259
Loss in iteration 110 : 0.05250260205689343
Loss in iteration 111 : 0.052231230751477134
Loss in iteration 112 : 0.05196323840050717
Loss in iteration 113 : 0.05169855674490217
Loss in iteration 114 : 0.05143711946505663
Loss in iteration 115 : 0.0511788621080604
Loss in iteration 116 : 0.05092372201851679
Loss in iteration 117 : 0.05067163827271385
Loss in iteration 118 : 0.05042255161592455
Loss in iteration 119 : 0.05017640440263471
Loss in iteration 120 : 0.049933140539515794
Loss in iteration 121 : 0.0496927054309742
Loss in iteration 122 : 0.049455045927125005
Loss in iteration 123 : 0.04922011027404786
Loss in iteration 124 : 0.048987848066195945
Loss in iteration 125 : 0.04875821020083664
Loss in iteration 126 : 0.04853114883441551
Loss in iteration 127 : 0.0483066173407369
Loss in iteration 128 : 0.0480845702708699
Loss in iteration 129 : 0.047864963314687876
Loss in iteration 130 : 0.04764775326396048
Loss in iteration 131 : 0.04743289797692135
Loss in iteration 132 : 0.04722035634423887
Loss in iteration 133 : 0.047010088256322635
Loss in iteration 134 : 0.046802054571903076
Loss in iteration 135 : 0.0465962170878233
Loss in iteration 136 : 0.046392538509988986
Loss in iteration 137 : 0.04619098242542156
Loss in iteration 138 : 0.0459915132753655
Loss in iteration 139 : 0.04579409632940157
Loss in iteration 140 : 0.04559869766052335
Loss in iteration 141 : 0.0454052841211312
Loss in iteration 142 : 0.045213823319906706
Loss in iteration 143 : 0.04502428359952697
Loss in iteration 144 : 0.044836634015183736
Loss in iteration 145 : 0.044650844313873075
Loss in iteration 146 : 0.04446688491442207
Loss in iteration 147 : 0.044284726888222506
Loss in iteration 148 : 0.04410434194064094
Loss in iteration 149 : 0.04392570239307858
Loss in iteration 150 : 0.04374878116565237
Loss in iteration 151 : 0.043573551760474144
Loss in iteration 152 : 0.043399988245501085
Loss in iteration 153 : 0.043228065238937144
Loss in iteration 154 : 0.04305775789416052
Loss in iteration 155 : 0.042889041885158616
Loss in iteration 156 : 0.04272189339244932
Loss in iteration 157 : 0.042556289089469415
Loss in iteration 158 : 0.04239220612941184
Loss in iteration 159 : 0.04222962213249532
Loss in iteration 160 : 0.04206851517364784
Loss in iteration 161 : 0.041908863770589944
Loss in iteration 162 : 0.04175064687230123
Loss in iteration 163 : 0.041593843847855944
Loss in iteration 164 : 0.0414384344756145
Loss in iteration 165 : 0.04128439893275551
Loss in iteration 166 : 0.04113171778513814
Loss in iteration 167 : 0.04098037197748068
Loss in iteration 168 : 0.04083034282384427
Loss in iteration 169 : 0.04068161199841077
Loss in iteration 170 : 0.0405341615265436
Loss in iteration 171 : 0.040387973776121994
Loss in iteration 172 : 0.04024303144913748
Loss in iteration 173 : 0.040099317573544964
Loss in iteration 174 : 0.03995681549535751
Loss in iteration 175 : 0.03981550887097774
Loss in iteration 176 : 0.03967538165975582
Loss in iteration 177 : 0.03953641811676841
Loss in iteration 178 : 0.03939860278580795
Loss in iteration 179 : 0.039261920492577464
Loss in iteration 180 : 0.039126356338082355
Loss in iteration 181 : 0.03899189569221313
Loss in iteration 182 : 0.0388585241875119
Loss in iteration 183 : 0.03872622771311691
Loss in iteration 184 : 0.03859499240887901
Loss in iteration 185 : 0.03846480465964375
Loss in iteration 186 : 0.03833565108969448
Loss in iteration 187 : 0.03820751855735022
Loss in iteration 188 : 0.038080394149713495
Loss in iteration 189 : 0.03795426517756329
Loss in iteration 190 : 0.03782911917038851
Loss in iteration 191 : 0.037704943871556464
Loss in iteration 192 : 0.037581727233613416
Loss in iteration 193 : 0.037459457413711435
Loss in iteration 194 : 0.03733812276915874
Loss in iteration 195 : 0.037217711853088954
Loss in iteration 196 : 0.03709821341024512
Loss in iteration 197 : 0.03697961637287586
Loss in iteration 198 : 0.0368619098567394
Loss in iteration 199 : 0.03674508315721194
Loss in iteration 200 : 0.036629125745498044
Testing accuracy  of updater 2 on alg 0 with rate 0.09999999999999998 = 0.9724444444444444, training accuracy 0.9955701629036867, time elapsed: 5817 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 12.017962449375089
Loss in iteration 3 : 4.297431790821698
Loss in iteration 4 : 5.6366148461851395
Loss in iteration 5 : 0.5583257759467926
Loss in iteration 6 : 0.31852801893412547
Loss in iteration 7 : 0.22073767509609318
Loss in iteration 8 : 0.16817251544164932
Loss in iteration 9 : 0.13391401935091962
Loss in iteration 10 : 0.11177970745978388
Loss in iteration 11 : 0.0964241308476049
Loss in iteration 12 : 0.08447624362922035
Loss in iteration 13 : 0.07499010463638744
Loss in iteration 14 : 0.06766351665721809
Loss in iteration 15 : 0.061719529082004336
Loss in iteration 16 : 0.056571881176764705
Loss in iteration 17 : 0.052001286900151725
Loss in iteration 18 : 0.04791508780847075
Loss in iteration 19 : 0.04426310687385436
Loss in iteration 20 : 0.04102278440961569
Loss in iteration 21 : 0.038190883465680295
Loss in iteration 22 : 0.035753617874757795
Loss in iteration 23 : 0.03366169678665837
Loss in iteration 24 : 0.03185381138472356
Loss in iteration 25 : 0.030272622993705506
Loss in iteration 26 : 0.02886912846894472
Loss in iteration 27 : 0.027606869186400897
Loss in iteration 28 : 0.026460385694923882
Loss in iteration 29 : 0.02541084440005049
Loss in iteration 30 : 0.02444362789161696
Loss in iteration 31 : 0.023548185110950144
Loss in iteration 32 : 0.022718085128983562
Loss in iteration 33 : 0.021950109010591284
Loss in iteration 34 : 0.021242633332441748
Loss in iteration 35 : 0.020594114260376172
Loss in iteration 36 : 0.02000208017467966
Loss in iteration 37 : 0.01946255448831475
Loss in iteration 38 : 0.018970049131354496
Loss in iteration 39 : 0.018518262999459772
Loss in iteration 40 : 0.01810100417509196
Loss in iteration 41 : 0.0177127739267187
Loss in iteration 42 : 0.017348933771504947
Loss in iteration 43 : 0.01700564664994537
Loss in iteration 44 : 0.01667975213928815
Loss in iteration 45 : 0.01636864497605186
Loss in iteration 46 : 0.016070174495191228
Loss in iteration 47 : 0.01578256421608126
Loss in iteration 48 : 0.015504346839636043
Loss in iteration 49 : 0.015234310446743012
Loss in iteration 50 : 0.01497145307382017
Loss in iteration 51 : 0.014714944005366392
Loss in iteration 52 : 0.014464090863605953
Loss in iteration 53 : 0.014218311959849394
Loss in iteration 54 : 0.013977113518004363
Loss in iteration 55 : 0.01374007138480065
Loss in iteration 56 : 0.013506816774118854
Loss in iteration 57 : 0.013277025506868248
Loss in iteration 58 : 0.013050410142294525
Loss in iteration 59 : 0.012826714375498361
Loss in iteration 60 : 0.012605709106111574
Loss in iteration 61 : 0.012387189655869517
Loss in iteration 62 : 0.012170973710357936
Loss in iteration 63 : 0.011956899663079492
Loss in iteration 64 : 0.011744825133162613
Loss in iteration 65 : 0.011534625503550645
Loss in iteration 66 : 0.011326192383106202
Loss in iteration 67 : 0.011119431937154929
Loss in iteration 68 : 0.010914263062198571
Loss in iteration 69 : 0.01071061540748901
Loss in iteration 70 : 0.010508427272911344
Loss in iteration 71 : 0.01030764344054175
Loss in iteration 72 : 0.010108213024579924
Loss in iteration 73 : 0.00991008744657863
Loss in iteration 74 : 0.00971321865393175
Loss in iteration 75 : 0.009517557693912902
Loss in iteration 76 : 0.009323053730578523
Loss in iteration 77 : 0.009129653549693294
Loss in iteration 78 : 0.008937301544597896
Loss in iteration 79 : 0.008745940124005508
Loss in iteration 80 : 0.00855551044163639
Loss in iteration 81 : 0.008365953324860783
Loss in iteration 82 : 0.008177210277502182
Loss in iteration 83 : 0.007989224447900489
Loss in iteration 84 : 0.007801941480997585
Loss in iteration 85 : 0.00761531020525656
Loss in iteration 86 : 0.007429283135362196
Loss in iteration 87 : 0.007243816795800988
Loss in iteration 88 : 0.0070588718869236235
Loss in iteration 89 : 0.006874413324204806
Loss in iteration 90 : 0.0066904101845256265
Loss in iteration 91 : 0.006506835592230232
Loss in iteration 92 : 0.006323666574189928
Loss in iteration 93 : 0.006140883908553555
Loss in iteration 94 : 0.005958471987274829
Loss in iteration 95 : 0.005776418708530797
Loss in iteration 96 : 0.005594715412163171
Loss in iteration 97 : 0.005413356869520171
Loss in iteration 98 : 0.005232341338745842
Loss in iteration 99 : 0.005051670697917729
Loss in iteration 100 : 0.004871350671907326
Loss in iteration 101 : 0.004691391175167726
Loss in iteration 102 : 0.004511806803041005
Loss in iteration 103 : 0.004332617520511129
Loss in iteration 104 : 0.004153849622484376
Loss in iteration 105 : 0.0039755370779148876
Loss in iteration 106 : 0.003797723427514604
Loss in iteration 107 : 0.003620464489779749
Loss in iteration 108 : 0.0034438322533935626
Loss in iteration 109 : 0.0032679205078443604
Loss in iteration 110 : 0.003092852997932299
Loss in iteration 111 : 0.002918795178399111
Loss in iteration 112 : 0.002745970951890295
Loss in iteration 113 : 0.0025746859698489465
Loss in iteration 114 : 0.0024053588577396374
Loss in iteration 115 : 0.0022385604799911893
Loss in iteration 116 : 0.0020750580612826303
Loss in iteration 117 : 0.0019158544035552186
Loss in iteration 118 : 0.0017622023412669269
Loss in iteration 119 : 0.0016155645941089475
Loss in iteration 120 : 0.0014774901663613224
Loss in iteration 121 : 0.0013494054138315553
Testing accuracy  of updater 3 on alg 0 with rate 10.0 = 0.9893333333333333, training accuracy 0.9995713060874536, time elapsed: 3452 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.37301374694446077
Loss in iteration 3 : 0.5188635524338108
Loss in iteration 4 : 1.2052109182037634
Loss in iteration 5 : 0.13380204631195589
Loss in iteration 6 : 0.06904169375666928
Loss in iteration 7 : 0.05730446189589049
Loss in iteration 8 : 0.04940220225002027
Loss in iteration 9 : 0.04378978181333539
Loss in iteration 10 : 0.039636039571433204
Loss in iteration 11 : 0.03642991005752274
Loss in iteration 12 : 0.03386686043304495
Loss in iteration 13 : 0.03175955526130816
Loss in iteration 14 : 0.029987410591964535
Loss in iteration 15 : 0.028469469508675963
Loss in iteration 16 : 0.027149386776284312
Loss in iteration 17 : 0.025986731577185167
Loss in iteration 18 : 0.02495172489491366
Loss in iteration 19 : 0.02402193092141583
Loss in iteration 20 : 0.023180107251679995
Loss in iteration 21 : 0.022412767621448774
Loss in iteration 22 : 0.021709196800705943
Loss in iteration 23 : 0.021060760413290268
Loss in iteration 24 : 0.02046041184571272
Loss in iteration 25 : 0.019902333727163598
Loss in iteration 26 : 0.01938167307731664
Loss in iteration 27 : 0.018894342789860782
Loss in iteration 28 : 0.018436870839046514
Loss in iteration 29 : 0.0180062843157075
Loss in iteration 30 : 0.017600019221232412
Loss in iteration 31 : 0.017215849545701274
Loss in iteration 32 : 0.016851830949407638
Loss in iteration 33 : 0.016506255622095484
Loss in iteration 34 : 0.016177615784272185
Loss in iteration 35 : 0.01586457393372838
Loss in iteration 36 : 0.015565938403992247
Loss in iteration 37 : 0.015280643141455464
Loss in iteration 38 : 0.015007730859755203
Loss in iteration 39 : 0.014746338918294855
Loss in iteration 40 : 0.014495687413827455
Loss in iteration 41 : 0.014255069082085004
Loss in iteration 42 : 0.014023840689310375
Loss in iteration 43 : 0.013801415657600194
Loss in iteration 44 : 0.013587257717837
Loss in iteration 45 : 0.013380875423094263
Loss in iteration 46 : 0.0131818173862702
Loss in iteration 47 : 0.012989668130235462
Loss in iteration 48 : 0.012804044458393934
Loss in iteration 49 : 0.012624592269329276
Loss in iteration 50 : 0.012450983751969805
Loss in iteration 51 : 0.012282914908080823
Loss in iteration 52 : 0.012120103357377388
Loss in iteration 53 : 0.011962286387519762
Loss in iteration 54 : 0.011809219217006776
Loss in iteration 55 : 0.011660673443755001
Loss in iteration 56 : 0.011516435656122925
Loss in iteration 57 : 0.011376306186464364
Loss in iteration 58 : 0.011240097990085181
Loss in iteration 59 : 0.011107635634828765
Loss in iteration 60 : 0.010978754388505908
Loss in iteration 61 : 0.010853299393073447
Loss in iteration 62 : 0.010731124915904233
Loss in iteration 63 : 0.010612093669720728
Loss in iteration 64 : 0.01049607619381749
Loss in iteration 65 : 0.010382950290103842
Loss in iteration 66 : 0.01027260050827878
Loss in iteration 67 : 0.010164917675124745
Loss in iteration 68 : 0.010059798463492538
Loss in iteration 69 : 0.009957144997057287
Loss in iteration 70 : 0.009856864487368689
Loss in iteration 71 : 0.009758868900105083
Loss in iteration 72 : 0.009663074647778916
Loss in iteration 73 : 0.009569402306439021
Loss in iteration 74 : 0.009477776354173885
Loss in iteration 75 : 0.009388124929452062
Loss in iteration 76 : 0.009300379607536347
Loss in iteration 77 : 0.009214475193389332
Loss in iteration 78 : 0.009130349529645107
Loss in iteration 79 : 0.009047943318363779
Loss in iteration 80 : 0.008967199955410166
Loss in iteration 81 : 0.008888065376409453
Loss in iteration 82 : 0.008810487913332095
Loss in iteration 83 : 0.008734418160849149
Loss in iteration 84 : 0.008659808851678408
Loss in iteration 85 : 0.00858661474021294
Loss in iteration 86 : 0.008514792493787851
Loss in iteration 87 : 0.008444300590997966
Loss in iteration 88 : 0.008375099226531072
Loss in iteration 89 : 0.008307150222027734
Loss in iteration 90 : 0.008240416942521206
Loss in iteration 91 : 0.008174864218047768
Loss in iteration 92 : 0.008110458270053638
Loss in iteration 93 : 0.008047166642254194
Loss in iteration 94 : 0.007984958135630525
Loss in iteration 95 : 0.007923802747273388
Loss in iteration 96 : 0.007863671612807837
Loss in iteration 97 : 0.007804536952153324
Loss in iteration 98 : 0.007746372018392894
Loss in iteration 99 : 0.007689151049543001
Loss in iteration 100 : 0.007632849223031205
Loss in iteration 101 : 0.007577442612704007
Loss in iteration 102 : 0.007522908148199999
Loss in iteration 103 : 0.007469223576536292
Loss in iteration 104 : 0.0074163674257668516
Loss in iteration 105 : 0.007364318970582286
Loss in iteration 106 : 0.00731305819972947
Loss in iteration 107 : 0.007262565785138491
Loss in iteration 108 : 0.007212823052652256
Loss in iteration 109 : 0.007163811954261483
Loss in iteration 110 : 0.007115515041754334
Loss in iteration 111 : 0.007067915441696589
Loss in iteration 112 : 0.007020996831663774
Loss in iteration 113 : 0.006974743417651823
Loss in iteration 114 : 0.006929139912598182
Loss in iteration 115 : 0.0068841715159493715
Loss in iteration 116 : 0.006839823894215465
Loss in iteration 117 : 0.006796083162455721
Loss in iteration 118 : 0.006752935866643505
Loss in iteration 119 : 0.006710368966861261
Loss in iteration 120 : 0.006668369821280326
Loss in iteration 121 : 0.006626926170882471
Loss in iteration 122 : 0.006586026124883313
Loss in iteration 123 : 0.006545658146819621
Loss in iteration 124 : 0.006505811041265632
Loss in iteration 125 : 0.006466473941144714
Loss in iteration 126 : 0.006427636295605648
Loss in iteration 127 : 0.006389287858433987
Loss in iteration 128 : 0.0063514186769709135
Loss in iteration 129 : 0.00631401908151389
Loss in iteration 130 : 0.006277079675174359
Loss in iteration 131 : 0.006240591324169844
Loss in iteration 132 : 0.006204545148528541
Loss in iteration 133 : 0.00616893251318601
Loss in iteration 134 : 0.006133745019454764
Loss in iteration 135 : 0.006098974496848543
Loss in iteration 136 : 0.006064612995244073
Loss in iteration 137 : 0.006030652777364084
Loss in iteration 138 : 0.005997086311566286
Loss in iteration 139 : 0.005963906264923871
Loss in iteration 140 : 0.005931105496583696
Loss in iteration 141 : 0.005898677051389287
Loss in iteration 142 : 0.005866614153756402
Loss in iteration 143 : 0.005834910201789475
Loss in iteration 144 : 0.005803558761627944
Loss in iteration 145 : 0.005772553562012044
Loss in iteration 146 : 0.005741888489058215
Loss in iteration 147 : 0.005711557581234607
Loss in iteration 148 : 0.005681555024528025
Loss in iteration 149 : 0.005651875147793584
Loss in iteration 150 : 0.00562251241827925
Loss in iteration 151 : 0.005593461437317569
Loss in iteration 152 : 0.005564716936177433
Loss in iteration 153 : 0.00553627377206884
Loss in iteration 154 : 0.00550812692429426
Loss in iteration 155 : 0.00548027149054033
Loss in iteration 156 : 0.0054527026833039414
Loss in iteration 157 : 0.0054254158264471445
Loss in iteration 158 : 0.005398406351875357
Loss in iteration 159 : 0.005371669796334025
Loss in iteration 160 : 0.00534520179831863
Loss in iteration 161 : 0.005318998095093437
Loss in iteration 162 : 0.005293054519814759
Loss in iteration 163 : 0.005267366998754279
Loss in iteration 164 : 0.005241931548618486
Loss in iteration 165 : 0.005216744273960504
Loss in iteration 166 : 0.005191801364680446
Loss in iteration 167 : 0.005167099093610986
Loss in iteration 168 : 0.005142633814184632
Loss in iteration 169 : 0.005118401958179713
Loss in iteration 170 : 0.005094400033541768
Loss in iteration 171 : 0.005070624622277744
Loss in iteration 172 : 0.005047072378419872
Loss in iteration 173 : 0.005023740026056847
Loss in iteration 174 : 0.005000624357429554
Loss in iteration 175 : 0.004977722231089073
Loss in iteration 176 : 0.004955030570114437
Loss in iteration 177 : 0.004932546360388078
Loss in iteration 178 : 0.004910266648926741
Loss in iteration 179 : 0.00488818854226578
Loss in iteration 180 : 0.004866309204895013
Loss in iteration 181 : 0.004844625857743968
Loss in iteration 182 : 0.004823135776715147
Loss in iteration 183 : 0.004801836291263145
Loss in iteration 184 : 0.004780724783018164
Loss in iteration 185 : 0.00475979868445243
Loss in iteration 186 : 0.004739055477587734
Loss in iteration 187 : 0.004718492692742868
Loss in iteration 188 : 0.004698107907319341
Loss in iteration 189 : 0.004677898744624246
Loss in iteration 190 : 0.004657862872728789
Loss in iteration 191 : 0.004637998003361268
Loss in iteration 192 : 0.004618301890833466
Loss in iteration 193 : 0.004598772330999071
Loss in iteration 194 : 0.004579407160243167
Loss in iteration 195 : 0.004560204254501685
Testing accuracy  of updater 3 on alg 0 with rate 1.0 = 0.9955555555555555, training accuracy 0.9995713060874536, time elapsed: 5804 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.5674182297983487
Loss in iteration 3 : 0.48886961723165706
Loss in iteration 4 : 0.4344646619107948
Loss in iteration 5 : 0.3943636519619819
Loss in iteration 6 : 0.3633764609082318
Loss in iteration 7 : 0.3385370769496156
Loss in iteration 8 : 0.31804260119795896
Loss in iteration 9 : 0.30073969354174357
Loss in iteration 10 : 0.285858127004759
Loss in iteration 11 : 0.2728643172063984
Loss in iteration 12 : 0.2613767453820476
Loss in iteration 13 : 0.25111501436497075
Loss in iteration 14 : 0.24186801868081564
Loss in iteration 15 : 0.2334734091201236
Loss in iteration 16 : 0.22580396302215036
Loss in iteration 17 : 0.21875830623844886
Loss in iteration 18 : 0.2122544520804188
Loss in iteration 19 : 0.2062252083699396
Loss in iteration 20 : 0.2006148507005776
Loss in iteration 21 : 0.19537667118756633
Loss in iteration 22 : 0.19047114368005916
Loss in iteration 23 : 0.18586453038343875
Loss in iteration 24 : 0.18152780948089392
Loss in iteration 25 : 0.17743583956564912
Loss in iteration 26 : 0.17356670112260372
Loss in iteration 27 : 0.16990117203530097
Loss in iteration 28 : 0.16642230573301237
Loss in iteration 29 : 0.16311508879882977
Loss in iteration 30 : 0.15996616072083583
Loss in iteration 31 : 0.15696358270595015
Loss in iteration 32 : 0.15409664557498978
Loss in iteration 33 : 0.15135570904838583
Loss in iteration 34 : 0.14873206644294207
Loss in iteration 35 : 0.1462178300902473
Loss in iteration 36 : 0.14380583376923856
Loss in iteration 37 : 0.1414895491992157
Loss in iteration 38 : 0.13926301422303525
Loss in iteration 39 : 0.13712077076537516
Loss in iteration 40 : 0.1350578110086759
Loss in iteration 41 : 0.13306953051249878
Loss in iteration 42 : 0.13115168722765141
Loss in iteration 43 : 0.12930036553735932
Loss in iteration 44 : 0.12751194460373177
Loss in iteration 45 : 0.12578307041623743
Loss in iteration 46 : 0.12411063103555586
Loss in iteration 47 : 0.12249173460546182
Loss in iteration 48 : 0.12092368977075461
Loss in iteration 49 : 0.11940398819338337
Loss in iteration 50 : 0.11793028890395915
Loss in iteration 51 : 0.1165004042634881
Loss in iteration 52 : 0.11511228734173731
Loss in iteration 53 : 0.1137640205452566
Loss in iteration 54 : 0.11245380535057169
Loss in iteration 55 : 0.11117995301715529
Loss in iteration 56 : 0.10994087617104091
Loss in iteration 57 : 0.10873508116383025
Loss in iteration 58 : 0.10756116112374674
Loss in iteration 59 : 0.10641778962562266
Loss in iteration 60 : 0.10530371491552157
Loss in iteration 61 : 0.10421775463333012
Loss in iteration 62 : 0.10315879098325507
Loss in iteration 63 : 0.10212576630791274
Loss in iteration 64 : 0.10111767902669391
Loss in iteration 65 : 0.10013357990346314
Loss in iteration 66 : 0.09917256861247203
Loss in iteration 67 : 0.09823379057472265
Loss in iteration 68 : 0.09731643403995963
Loss in iteration 69 : 0.09641972739207887
Loss in iteration 70 : 0.09554293665801941
Loss in iteration 71 : 0.09468536320223953
Loss in iteration 72 : 0.09384634159067247
Loss in iteration 73 : 0.09302523760965072
Loss in iteration 74 : 0.09222144642670119
Loss in iteration 75 : 0.09143439088138085
Loss in iteration 76 : 0.0906635198954409
Loss in iteration 77 : 0.08990830699262006
Loss in iteration 78 : 0.08916824891925977
Loss in iteration 79 : 0.08844286435774386
Loss in iteration 80 : 0.08773169272549049
Loss in iteration 81 : 0.08703429305286667
Loss in iteration 82 : 0.08635024293398937
Loss in iteration 83 : 0.08567913754489673
Loss in iteration 84 : 0.08502058872405334
Loss in iteration 85 : 0.0843742241105788
Loss in iteration 86 : 0.08373968633598053
Loss in iteration 87 : 0.08311663226552274
Loss in iteration 88 : 0.08250473228567912
Loss in iteration 89 : 0.08190366963441142
Loss in iteration 90 : 0.08131313977127423
Loss in iteration 91 : 0.0807328497845895
Loss in iteration 92 : 0.08016251783314712
Loss in iteration 93 : 0.07960187262009252
Loss in iteration 94 : 0.07905065289683742
Loss in iteration 95 : 0.07850860699499985
Loss in iteration 96 : 0.07797549238452753
Loss in iteration 97 : 0.07745107525629927
Loss in iteration 98 : 0.076935130127624
Loss in iteration 99 : 0.07642743946917571
Loss in iteration 100 : 0.07592779335200467
Loss in iteration 101 : 0.07543598911336981
Loss in iteration 102 : 0.07495183104021978
Loss in iteration 103 : 0.07447513006923959
Loss in iteration 104 : 0.07400570350245031
Loss in iteration 105 : 0.07354337473742417
Loss in iteration 106 : 0.07308797301123802
Loss in iteration 107 : 0.07263933315735238
Loss in iteration 108 : 0.07219729537465307
Loss in iteration 109 : 0.07176170500794903
Loss in iteration 110 : 0.071332412339261
Loss in iteration 111 : 0.07090927238928703
Loss in iteration 112 : 0.07049214472846187
Loss in iteration 113 : 0.07008089329707382
Loss in iteration 114 : 0.06967538623393058
Loss in iteration 115 : 0.06927549571310179
Loss in iteration 116 : 0.06888109778829336
Loss in iteration 117 : 0.06849207224443957
Loss in iteration 118 : 0.06810830245612053
Loss in iteration 119 : 0.06772967525244038
Loss in iteration 120 : 0.06735608078802326
Loss in iteration 121 : 0.06698741241980105
Loss in iteration 122 : 0.0666235665892948
Loss in iteration 123 : 0.06626444271009821
Loss in iteration 124 : 0.0659099430603008
Loss in iteration 125 : 0.06555997267959378
Loss in iteration 126 : 0.06521443927082325
Loss in iteration 127 : 0.06487325310576525
Loss in iteration 128 : 0.06453632693491308
Loss in iteration 129 : 0.06420357590107566
Loss in iteration 130 : 0.0638749174566005
Loss in iteration 131 : 0.06355027128404421
Loss in iteration 132 : 0.06322955922012186
Loss in iteration 133 : 0.06291270518277764
Loss in iteration 134 : 0.06259963510122818
Loss in iteration 135 : 0.062290276848836104
Loss in iteration 136 : 0.0619845601786803
Loss in iteration 137 : 0.06168241666169819
Loss in iteration 138 : 0.06138377962727688
Loss in iteration 139 : 0.06108858410618404
Loss in iteration 140 : 0.06079676677572783
Loss in iteration 141 : 0.06050826590704621
Loss in iteration 142 : 0.0602230213144281
Loss in iteration 143 : 0.05994097430657512
Loss in iteration 144 : 0.059662067639718534
Loss in iteration 145 : 0.05938624547250626
Loss in iteration 146 : 0.05911345332258497
Loss in iteration 147 : 0.058843638024801144
Loss in iteration 148 : 0.05857674769095023
Loss in iteration 149 : 0.05831273167100911
Loss in iteration 150 : 0.058051540515785725
Loss in iteration 151 : 0.057793125940927
Loss in iteration 152 : 0.05753744079222611
Loss in iteration 153 : 0.05728443901217595
Loss in iteration 154 : 0.057034075607714695
Loss in iteration 155 : 0.056786306619115615
Loss in iteration 156 : 0.05654108908997172
Loss in iteration 157 : 0.05629838103823272
Loss in iteration 158 : 0.056058141428249234
Loss in iteration 159 : 0.055820330143783785
Loss in iteration 160 : 0.05558490796195038
Loss in iteration 161 : 0.05535183652804444
Loss in iteration 162 : 0.055121078331227584
Loss in iteration 163 : 0.054892596681034335
Loss in iteration 164 : 0.054666355684667045
Loss in iteration 165 : 0.05444232022504835
Loss in iteration 166 : 0.05422045593960361
Loss in iteration 167 : 0.054000729199740846
Loss in iteration 168 : 0.05378310709100644
Loss in iteration 169 : 0.05356755739388602
Loss in iteration 170 : 0.05335404856522941
Loss in iteration 171 : 0.05314254972027353
Loss in iteration 172 : 0.05293303061524286
Loss in iteration 173 : 0.05272546163050375
Loss in iteration 174 : 0.05251981375425358
Loss in iteration 175 : 0.05231605856672397
Loss in iteration 176 : 0.05211416822487973
Loss in iteration 177 : 0.05191411544759461
Loss in iteration 178 : 0.05171587350128754
Loss in iteration 179 : 0.051519416186001595
Loss in iteration 180 : 0.051324717821910164
Loss in iteration 181 : 0.051131753236235485
Loss in iteration 182 : 0.050940497750563814
Loss in iteration 183 : 0.05075092716854391
Loss in iteration 184 : 0.05056301776395538
Loss in iteration 185 : 0.050376746269132834
Loss in iteration 186 : 0.05019208986373525
Loss in iteration 187 : 0.05000902616384642
Loss in iteration 188 : 0.04982753321139692
Loss in iteration 189 : 0.049647589463895
Loss in iteration 190 : 0.04946917378445731
Loss in iteration 191 : 0.049292265432127884
Loss in iteration 192 : 0.04911684405247611
Loss in iteration 193 : 0.04894288966846514
Loss in iteration 194 : 0.04877038267158009
Loss in iteration 195 : 0.048599303813208365
Loss in iteration 196 : 0.04842963419626415
Loss in iteration 197 : 0.04826135526704743
Loss in iteration 198 : 0.04809444880733134
Loss in iteration 199 : 0.04792889692666984
Loss in iteration 200 : 0.047764682054918385
Testing accuracy  of updater 3 on alg 0 with rate 0.09999999999999998 = 0.976, training accuracy 0.9955701629036867, time elapsed: 5697 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224036
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.6842504092141719
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632758
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439016
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043772
Loss in iteration 15 : 0.6751379721466284
Loss in iteration 16 : 0.6738340200990942
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694718
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873453
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050735
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663436
Loss in iteration 37 : 0.6465682744294412
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699549
Loss in iteration 45 : 0.6362669733680169
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889295
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153971
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658474
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426644
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535707
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033767
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098036
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088277
Loss in iteration 87 : 0.5832800218249878
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.573454069721608
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.5710135726721919
Loss in iteration 98 : 0.5697958068708515
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879716
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677673
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231537
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211117
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.54581110087663
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722849
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190974
Loss in iteration 131 : 0.5306307670254145
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961832
Loss in iteration 134 : 0.5271766840349436
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035023
Loss in iteration 137 : 0.5237415434680152
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552454
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609136
Loss in iteration 145 : 0.5146754836056485
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248353
Loss in iteration 148 : 0.5113116633540281
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333072
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303146
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.50024354407835
Loss in iteration 159 : 0.4991491138168719
Loss in iteration 160 : 0.4980569551084795
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474518
Loss in iteration 163 : 0.4947941542526308
Loss in iteration 164 : 0.49371112713244425
Loss in iteration 165 : 0.4926303936870237
Loss in iteration 166 : 0.4915519580073085
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.48833047684132436
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.48619438167066215
Loss in iteration 172 : 0.48512981241120995
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967204
Loss in iteration 175 : 0.4819500575854046
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.4787912964742022
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.4766971565057869
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.4746124009886094
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.47253704845175315
Loss in iteration 185 : 0.47150290363343866
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.4684146180856046
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.46636756901018045
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.46129135823604484
Loss in iteration 196 : 0.4602832245926189
Loss in iteration 197 : 0.45927746265434266
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.45627441406910757
Testing accuracy  of updater 4 on alg 0 with rate 10.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4678 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224036
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.6842504092141719
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632758
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439014
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043774
Loss in iteration 15 : 0.6751379721466284
Loss in iteration 16 : 0.6738340200990942
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057904
Loss in iteration 19 : 0.6699232506694718
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873454
Loss in iteration 25 : 0.662113861637729
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050735
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663437
Loss in iteration 37 : 0.6465682744294412
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680166
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889296
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153971
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658474
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426643
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085112
Loss in iteration 65 : 0.6107749764739226
Loss in iteration 66 : 0.6095116756535706
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033767
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895767
Loss in iteration 84 : 0.5869900323068191
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.583280021824988
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.579583566669571
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960757
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708514
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872166
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677673
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190326
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211118
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781853
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722849
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.538762559856184
Loss in iteration 125 : 0.5375947686906091
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190974
Loss in iteration 131 : 0.5306307670254145
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961832
Loss in iteration 134 : 0.5271766840349438
Loss in iteration 135 : 0.5260295208028113
Loss in iteration 136 : 0.5248844711035022
Loss in iteration 137 : 0.523741543468015
Loss in iteration 138 : 0.5226007462549448
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056484
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049848
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333072
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.501340241117268
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.4991491138168719
Loss in iteration 160 : 0.4980569551084796
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.4947941542526308
Loss in iteration 164 : 0.4937111271324443
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.4915519580073086
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.48940199576514015
Loss in iteration 169 : 0.4883304768413243
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.48619438167066215
Loss in iteration 172 : 0.48512981241120995
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967204
Loss in iteration 175 : 0.48195005758540466
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.4798418793763408
Loss in iteration 178 : 0.47879129647420227
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.47669715650578703
Loss in iteration 181 : 0.475653604493347
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.4725370484517532
Loss in iteration 185 : 0.4715029036334387
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536212
Loss in iteration 188 : 0.46841461808560464
Loss in iteration 189 : 0.4673899116905835
Loss in iteration 190 : 0.4663675690101804
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465956
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.46230186273931484
Loss in iteration 195 : 0.4612913582360449
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.45927746265434266
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.45627441406910757
Testing accuracy  of updater 4 on alg 0 with rate 1.0 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4283 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 0.6919212218028126
Loss in iteration 3 : 0.6906687955906831
Loss in iteration 4 : 0.6894006853224037
Loss in iteration 5 : 0.6881221821962341
Loss in iteration 6 : 0.6868363932257243
Loss in iteration 7 : 0.6855453364617526
Loss in iteration 8 : 0.684250409214172
Loss in iteration 9 : 0.6829526221097164
Loss in iteration 10 : 0.6816527292632758
Loss in iteration 11 : 0.680351306519378
Loss in iteration 12 : 0.6790488013439014
Loss in iteration 13 : 0.6777455661498849
Loss in iteration 14 : 0.6764418814043774
Loss in iteration 15 : 0.6751379721466284
Loss in iteration 16 : 0.6738340200990942
Loss in iteration 17 : 0.6725301727374494
Loss in iteration 18 : 0.6712265502057905
Loss in iteration 19 : 0.6699232506694718
Loss in iteration 20 : 0.6686203545123528
Loss in iteration 21 : 0.6673179276642099
Loss in iteration 22 : 0.6660160242634572
Loss in iteration 23 : 0.6647146888053064
Loss in iteration 24 : 0.6634139578873454
Loss in iteration 25 : 0.6621138616377291
Loss in iteration 26 : 0.6608144248921373
Loss in iteration 27 : 0.6595156681718605
Loss in iteration 28 : 0.6582176085050735
Loss in iteration 29 : 0.6569202601249513
Loss in iteration 30 : 0.65562363507053
Loss in iteration 31 : 0.6543277437085572
Loss in iteration 32 : 0.6530325951873028
Loss in iteration 33 : 0.6517381978275596
Loss in iteration 34 : 0.6504445594534879
Loss in iteration 35 : 0.6491516876663453
Loss in iteration 36 : 0.6478595900663436
Loss in iteration 37 : 0.6465682744294413
Loss in iteration 38 : 0.6452777488458947
Loss in iteration 39 : 0.6439880218260153
Loss in iteration 40 : 0.642699102376993
Loss in iteration 41 : 0.6414110000538785
Loss in iteration 42 : 0.6401237249877991
Loss in iteration 43 : 0.6388372878949181
Loss in iteration 44 : 0.6375517000699548
Loss in iteration 45 : 0.6362669733680166
Loss in iteration 46 : 0.6349831201780048
Loss in iteration 47 : 0.6337001533902564
Loss in iteration 48 : 0.6324180863603418
Loss in iteration 49 : 0.6311369328704027
Loss in iteration 50 : 0.6298567070889295
Loss in iteration 51 : 0.6285774235295389
Loss in iteration 52 : 0.6272990970091332
Loss in iteration 53 : 0.6260217426056246
Loss in iteration 54 : 0.6247453756153972
Loss in iteration 55 : 0.6234700115106354
Loss in iteration 56 : 0.6221956658966667
Loss in iteration 57 : 0.6209223544695961
Loss in iteration 58 : 0.6196500929746442
Loss in iteration 59 : 0.6183788971658474
Loss in iteration 60 : 0.6171087827680661
Loss in iteration 61 : 0.6158397654426642
Loss in iteration 62 : 0.6145718607585837
Loss in iteration 63 : 0.6133050841708372
Loss in iteration 64 : 0.6120394510085111
Loss in iteration 65 : 0.6107749764739225
Loss in iteration 66 : 0.6095116756535707
Loss in iteration 67 : 0.6082495635398484
Loss in iteration 68 : 0.6069886550605424
Loss in iteration 69 : 0.6057289651114672
Loss in iteration 70 : 0.6044705085869325
Loss in iteration 71 : 0.6032133004033767
Loss in iteration 72 : 0.6019573555134001
Loss in iteration 73 : 0.6007026889098037
Loss in iteration 74 : 0.5994493156213934
Loss in iteration 75 : 0.5981972507036714
Loss in iteration 76 : 0.5969465092279852
Loss in iteration 77 : 0.5956971062724232
Loss in iteration 78 : 0.5944490569169169
Loss in iteration 79 : 0.59320237624393
Loss in iteration 80 : 0.5919570793448852
Loss in iteration 81 : 0.5907131813312783
Loss in iteration 82 : 0.5894706973484711
Loss in iteration 83 : 0.5882296425895768
Loss in iteration 84 : 0.586990032306819
Loss in iteration 85 : 0.5857518818182816
Loss in iteration 86 : 0.5845152065088276
Loss in iteration 87 : 0.583280021824988
Loss in iteration 88 : 0.5820463432644791
Loss in iteration 89 : 0.5808141863615476
Loss in iteration 90 : 0.5795835666695709
Loss in iteration 91 : 0.5783544997422799
Loss in iteration 92 : 0.5771270011147259
Loss in iteration 93 : 0.5759010862848323
Loss in iteration 94 : 0.5746767706960756
Loss in iteration 95 : 0.5734540697216081
Loss in iteration 96 : 0.5722329986499388
Loss in iteration 97 : 0.571013572672192
Loss in iteration 98 : 0.5697958068708514
Loss in iteration 99 : 0.5685797162098932
Loss in iteration 100 : 0.5673653155261588
Loss in iteration 101 : 0.5661526195218409
Loss in iteration 102 : 0.5649416427579538
Loss in iteration 103 : 0.5637323996486493
Loss in iteration 104 : 0.5625249044562975
Loss in iteration 105 : 0.5613191712872168
Loss in iteration 106 : 0.5601152140879715
Loss in iteration 107 : 0.5589130466421821
Loss in iteration 108 : 0.5577126825677672
Loss in iteration 109 : 0.5565141353145749
Loss in iteration 110 : 0.5553174181623552
Loss in iteration 111 : 0.5541225442190327
Loss in iteration 112 : 0.5529295264192511
Loss in iteration 113 : 0.5517383775231538
Loss in iteration 114 : 0.5505491101153797
Loss in iteration 115 : 0.5493617366042473
Loss in iteration 116 : 0.5481762692211117
Loss in iteration 117 : 0.5469927200198745
Loss in iteration 118 : 0.5458111008766301
Loss in iteration 119 : 0.5446314234894338
Loss in iteration 120 : 0.5434536993781852
Loss in iteration 121 : 0.5422779398846018
Loss in iteration 122 : 0.5411041561722848
Loss in iteration 123 : 0.5399323592268603
Loss in iteration 124 : 0.5387625598561838
Loss in iteration 125 : 0.537594768690609
Loss in iteration 126 : 0.5364289961832983
Loss in iteration 127 : 0.5352652526105837
Loss in iteration 128 : 0.5341035480723463
Loss in iteration 129 : 0.5329438924924356
Loss in iteration 130 : 0.5317862956190974
Loss in iteration 131 : 0.5306307670254143
Loss in iteration 132 : 0.5294773161097485
Loss in iteration 133 : 0.5283259520961832
Loss in iteration 134 : 0.5271766840349437
Loss in iteration 135 : 0.5260295208028114
Loss in iteration 136 : 0.5248844711035022
Loss in iteration 137 : 0.523741543468015
Loss in iteration 138 : 0.5226007462549447
Loss in iteration 139 : 0.5214620876507402
Loss in iteration 140 : 0.5203255756699224
Loss in iteration 141 : 0.5191912181552455
Loss in iteration 142 : 0.5180590227777956
Loss in iteration 143 : 0.5169289970370566
Loss in iteration 144 : 0.5158011482609137
Loss in iteration 145 : 0.5146754836056484
Loss in iteration 146 : 0.5135520100559261
Loss in iteration 147 : 0.5124307344248352
Loss in iteration 148 : 0.5113116633540282
Loss in iteration 149 : 0.5101948033140532
Loss in iteration 150 : 0.5090801606049847
Loss in iteration 151 : 0.5079677413574816
Loss in iteration 152 : 0.506857551534427
Loss in iteration 153 : 0.5057495969333072
Loss in iteration 154 : 0.5046438831894593
Loss in iteration 155 : 0.5035404157802712
Loss in iteration 156 : 0.5024392000303147
Loss in iteration 157 : 0.5013402411172682
Loss in iteration 158 : 0.5002435440783501
Loss in iteration 159 : 0.4991491138168719
Loss in iteration 160 : 0.4980569551084795
Loss in iteration 161 : 0.4969670726066897
Loss in iteration 162 : 0.4958794708474519
Loss in iteration 163 : 0.4947941542526308
Loss in iteration 164 : 0.4937111271324443
Loss in iteration 165 : 0.49263039368702377
Loss in iteration 166 : 0.4915519580073084
Loss in iteration 167 : 0.4904758240754822
Loss in iteration 168 : 0.4894019957651401
Loss in iteration 169 : 0.4883304768413243
Loss in iteration 170 : 0.48726127096051786
Loss in iteration 171 : 0.48619438167066215
Loss in iteration 172 : 0.48512981241120995
Loss in iteration 173 : 0.48406756651324917
Loss in iteration 174 : 0.48300764719967215
Loss in iteration 175 : 0.4819500575854046
Loss in iteration 176 : 0.4808948006776774
Loss in iteration 177 : 0.47984187937634076
Loss in iteration 178 : 0.4787912964742021
Loss in iteration 179 : 0.47774305465739936
Loss in iteration 180 : 0.47669715650578703
Loss in iteration 181 : 0.4756536044933469
Loss in iteration 182 : 0.47461240098860946
Loss in iteration 183 : 0.4735735482550942
Loss in iteration 184 : 0.4725370484517532
Loss in iteration 185 : 0.47150290363343866
Loss in iteration 186 : 0.47047111575137085
Loss in iteration 187 : 0.4694416866536213
Loss in iteration 188 : 0.46841461808560453
Loss in iteration 189 : 0.4673899116905834
Loss in iteration 190 : 0.4663675690101804
Loss in iteration 191 : 0.46534759148489896
Loss in iteration 192 : 0.46432998045465945
Loss in iteration 193 : 0.46331473715933646
Loss in iteration 194 : 0.4623018627393148
Loss in iteration 195 : 0.4612913582360449
Loss in iteration 196 : 0.46028322459261894
Loss in iteration 197 : 0.45927746265434266
Loss in iteration 198 : 0.45827407316932645
Loss in iteration 199 : 0.4572730567890787
Loss in iteration 200 : 0.4562744140691076
Testing accuracy  of updater 4 on alg 0 with rate 0.09999999999999998 = 0.9973333333333333, training accuracy 0.8933981137467848, time elapsed: 4120 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 38.023107635229856
Loss in iteration 3 : 14.887005509999245
Loss in iteration 4 : 21.60647447430146
Loss in iteration 5 : 1.6576291684181068
Loss in iteration 6 : 0.9670692878857597
Loss in iteration 7 : 0.6311853425716033
Loss in iteration 8 : 0.4594492931513261
Loss in iteration 9 : 0.3601509001479762
Loss in iteration 10 : 0.2979979601956786
Loss in iteration 11 : 0.2472282303669268
Loss in iteration 12 : 0.20996148267621445
Loss in iteration 13 : 0.18188383804264158
Loss in iteration 14 : 0.15674265241482974
Loss in iteration 15 : 0.13522059507135256
Loss in iteration 16 : 0.11703418490144386
Loss in iteration 17 : 0.10268845948004186
Loss in iteration 18 : 0.09140809174597758
Loss in iteration 19 : 0.08268618271948032
Loss in iteration 20 : 0.07500659801805638
Loss in iteration 21 : 0.0683529293352162
Loss in iteration 22 : 0.06192145698401972
Loss in iteration 23 : 0.05704007080868371
Loss in iteration 24 : 0.05332973138542309
Loss in iteration 25 : 0.050083993861759574
Loss in iteration 26 : 0.047045725069897196
Loss in iteration 27 : 0.04464927638376722
Loss in iteration 28 : 0.04189738705053138
Loss in iteration 29 : 0.03973161882289195
Loss in iteration 30 : 0.03644216306777368
Loss in iteration 31 : 0.033976598653960854
Loss in iteration 32 : 0.031212357550845025
Loss in iteration 33 : 0.028612536928512667
Loss in iteration 34 : 0.02556727550846803
Loss in iteration 35 : 0.023078865921906926
Loss in iteration 36 : 0.019839577878759635
Loss in iteration 37 : 0.01809062922164954
Loss in iteration 38 : 0.012849488470588626
Loss in iteration 39 : 0.010324297211386677
Loss in iteration 40 : 0.007078175445368945
Loss in iteration 41 : 0.0067594685684240685
Loss in iteration 42 : 0.0017462077115355011
Loss in iteration 43 : 0.010331223001243478
Loss in iteration 44 : 0.009268050321060222
Loss in iteration 45 : 0.05948228673498197
Loss in iteration 46 : 0.15372785369046627
Loss in iteration 47 : 0.41856503370590276
Loss in iteration 48 : 10.769433207869502
Loss in iteration 49 : 66.06358604829859
Loss in iteration 50 : 0.510345237905967
Loss in iteration 51 : 0.1897347753488899
Loss in iteration 52 : 0.08594630169964154
Loss in iteration 53 : 0.05551248693783416
Loss in iteration 54 : 0.04183149066514939
Loss in iteration 55 : 0.031080130627949537
Loss in iteration 56 : 0.02404409525342197
Loss in iteration 57 : 0.02023045766262568
Loss in iteration 58 : 0.017450340220791827
Loss in iteration 59 : 0.014942154619915325
Loss in iteration 60 : 0.012813307416906623
Loss in iteration 61 : 0.01125623782404172
Loss in iteration 62 : 0.00969119784166143
Loss in iteration 63 : 0.00806733756570618
Loss in iteration 64 : 0.006530093822926827
Loss in iteration 65 : 0.005301325922510706
Loss in iteration 66 : 0.004124231282672788
Loss in iteration 67 : 0.002918352651360278
Loss in iteration 68 : 0.001692567463679671
Loss in iteration 69 : 8.451417674759507E-4
Loss in iteration 70 : 4.0997096363025264E-4
Loss in iteration 71 : 1.2756715221426404E-4
Loss in iteration 72 : 5.6430725397177046E-5
Testing accuracy  of updater 5 on alg 0 with rate 10.0 = 0.9937777777777778, training accuracy 1.0, time elapsed: 1578 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 2.2431847058559833
Loss in iteration 3 : 1.521413712044515
Loss in iteration 4 : 3.222448299592937
Loss in iteration 5 : 0.19494502293677152
Loss in iteration 6 : 0.09863306801537486
Loss in iteration 7 : 0.06513006083544153
Loss in iteration 8 : 0.04866532728802949
Loss in iteration 9 : 0.038029294670965046
Loss in iteration 10 : 0.030682703501510224
Loss in iteration 11 : 0.025306224233289758
Loss in iteration 12 : 0.02123411521293035
Loss in iteration 13 : 0.01808225428784699
Loss in iteration 14 : 0.015598931123379987
Loss in iteration 15 : 0.013614959390305572
Loss in iteration 16 : 0.012015136930993101
Loss in iteration 17 : 0.010715187229936174
Loss in iteration 18 : 0.009649189071380106
Loss in iteration 19 : 0.008764630784709413
Loss in iteration 20 : 0.008020022759986551
Loss in iteration 21 : 0.007382904798234608
Loss in iteration 22 : 0.0068280108028966335
Loss in iteration 23 : 0.006335722957966941
Loss in iteration 24 : 0.005890838717579043
Loss in iteration 25 : 0.005481598576777166
Loss in iteration 26 : 0.005098920108876765
Loss in iteration 27 : 0.00473580177166582
Loss in iteration 28 : 0.00438687479205343
Loss in iteration 29 : 0.004048092247516597
Loss in iteration 30 : 0.0037165564040206197
Loss in iteration 31 : 0.0033905019493009354
Loss in iteration 32 : 0.003069470423481433
Loss in iteration 33 : 0.002754705446466231
Loss in iteration 34 : 0.002449695899780539
Loss in iteration 35 : 0.0021604785219343603
Loss in iteration 36 : 0.0018948677304221593
Loss in iteration 37 : 0.0016600127095301217
Loss in iteration 38 : 0.0014594069068457492
Loss in iteration 39 : 0.0012918079255158814
Loss in iteration 40 : 0.001152736579743991
Loss in iteration 41 : 0.0010367588749434858
Loss in iteration 42 : 9.389580429702082E-4
Loss in iteration 43 : 8.554200120095849E-4
Loss in iteration 44 : 7.831867518553983E-4
Loss in iteration 45 : 7.200504727491047E-4
Loss in iteration 46 : 6.64352285072932E-4
Loss in iteration 47 : 6.148275533637849E-4
Loss in iteration 48 : 5.704960971292353E-4
Loss in iteration 49 : 5.305857918376462E-4
Loss in iteration 50 : 4.944791812604736E-4
Loss in iteration 51 : 4.6167565176705026E-4
Loss in iteration 52 : 4.3176419011080655E-4
Loss in iteration 53 : 4.0440344624680937E-4
Loss in iteration 54 : 3.793069247924021E-4
Loss in iteration 55 : 3.5623183620551676E-4
Loss in iteration 56 : 3.349705979058992E-4
Loss in iteration 57 : 3.153442792023874E-4
Loss in iteration 58 : 2.971974887795525E-4
Loss in iteration 59 : 2.803943443508004E-4
Loss in iteration 60 : 2.648152624907622E-4
Loss in iteration 61 : 2.5035437635959697E-4
Loss in iteration 62 : 2.369174389779608E-4
Loss in iteration 63 : 2.2442010587699705E-4
Loss in iteration 64 : 2.1278651737707748E-4
Loss in iteration 65 : 2.0194812022901508E-4
Loss in iteration 66 : 1.9184268281590604E-4
Loss in iteration 67 : 1.824134689206771E-4
Loss in iteration 68 : 1.73608543159874E-4
Loss in iteration 69 : 1.6538018724554689E-4
Loss in iteration 70 : 1.5768441073288098E-4
Loss in iteration 71 : 1.5048054318419902E-4
Loss in iteration 72 : 1.4373089697706517E-4
Loss in iteration 73 : 1.3740049149971338E-4
Loss in iteration 74 : 1.314568303769011E-4
Loss in iteration 75 : 1.2586972380593952E-4
Loss in iteration 76 : 1.2061114821614522E-4
Loss in iteration 77 : 1.1565513545546393E-4
Loss in iteration 78 : 1.10977683720652E-4
Loss in iteration 79 : 1.0655668263097693E-4
Loss in iteration 80 : 1.0237184531904137E-4
Loss in iteration 81 : 9.840464124484344E-5
Loss in iteration 82 : 9.46382246304632E-5
Loss in iteration 83 : 9.105735489732854E-5
Loss in iteration 84 : 8.764830713688824E-5
Loss in iteration 85 : 8.439877229411565E-5
Loss in iteration 86 : 8.129774822198017E-5
Loss in iteration 87 : 7.833542392939098E-5
Loss in iteration 88 : 7.550306010267483E-5
Loss in iteration 89 : 7.27928693055761E-5
Loss in iteration 90 : 7.01978991845479E-5
Loss in iteration 91 : 6.771192160969747E-5
Loss in iteration 92 : 6.532933007172658E-5
Loss in iteration 93 : 6.304504694238171E-5
Loss in iteration 94 : 6.085444149133833E-5
Loss in iteration 95 : 5.8753258905727666E-5
Loss in iteration 96 : 5.6737560028978286E-5
Loss in iteration 97 : 5.4803671140181424E-5
Loss in iteration 98 : 5.294814283445444E-5
Loss in iteration 99 : 5.116771692095354E-5
Loss in iteration 100 : 4.9459300209318534E-5
Loss in iteration 101 : 4.781994407856313E-5
Loss in iteration 102 : 4.624682879520242E-5
Loss in iteration 103 : 4.473725164715773E-5
Loss in iteration 104 : 4.3288618074498953E-5
Loss in iteration 105 : 4.189843509326015E-5
Loss in iteration 106 : 4.056430642115705E-5
Loss in iteration 107 : 3.9283928815945315E-5
Loss in iteration 108 : 3.805508922999609E-5
Loss in iteration 109 : 3.6875662466511296E-5
Loss in iteration 110 : 3.574360909231678E-5
Loss in iteration 111 : 3.465697342432189E-5
Loss in iteration 112 : 3.361388145781318E-5
Loss in iteration 113 : 3.2612538648952447E-5
Loss in iteration 114 : 3.165122750081363E-5
Loss in iteration 115 : 3.072830493214831E-5
Loss in iteration 116 : 2.9842199432978744E-5
Loss in iteration 117 : 2.8991408029892725E-5
Loss in iteration 118 : 2.817449309844909E-5
Loss in iteration 119 : 2.7390079069868004E-5
Loss in iteration 120 : 2.663684908483021E-5
Loss in iteration 121 : 2.591354164997919E-5
Loss in iteration 122 : 2.521894735112184E-5
Loss in iteration 123 : 2.4551905674068736E-5
Loss in iteration 124 : 2.391130197820888E-5
Loss in iteration 125 : 2.3296064660195592E-5
Loss in iteration 126 : 2.270516253682618E-5
Loss in iteration 127 : 2.2137602467116744E-5
Loss in iteration 128 : 2.159242722371668E-5
Loss in iteration 129 : 2.1068713615914698E-5
Loss in iteration 130 : 2.0565570857157782E-5
Loss in iteration 131 : 2.0082139164200365E-5
Loss in iteration 132 : 1.961758856833567E-5
Loss in iteration 133 : 1.9171117915370034E-5
Loss in iteration 134 : 1.874195402831815E-5
Loss in iteration 135 : 1.832935100499132E-5
Loss in iteration 136 : 1.7932589623214505E-5
Loss in iteration 137 : 1.7550976826630257E-5
Loss in iteration 138 : 1.718384526697897E-5
Loss in iteration 139 : 1.683055288037425E-5
Loss in iteration 140 : 1.6490482478594894E-5
Loss in iteration 141 : 1.6163041339804817E-5
Loss in iteration 142 : 1.5847660785685486E-5
Testing accuracy  of updater 5 on alg 0 with rate 1.0 = 0.9982222222222222, training accuracy 1.0, time elapsed: 3450 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.5268025432863558
Loss in iteration 3 : 0.43755634731655624
Loss in iteration 4 : 0.2798734153606953
Loss in iteration 5 : 0.11545318713816148
Loss in iteration 6 : 0.10294742239103624
Loss in iteration 7 : 0.09338709596991515
Loss in iteration 8 : 0.08555440971733598
Loss in iteration 9 : 0.07897165305179428
Loss in iteration 10 : 0.07332315648881844
Loss in iteration 11 : 0.06839178324982191
Loss in iteration 12 : 0.06402546583839185
Loss in iteration 13 : 0.06011543957161895
Loss in iteration 14 : 0.05658203227572718
Loss in iteration 15 : 0.05336541225132151
Loss in iteration 16 : 0.05041954047322196
Loss in iteration 17 : 0.0477081768961464
Loss in iteration 18 : 0.04520220439015128
Loss in iteration 19 : 0.042877803291086286
Loss in iteration 20 : 0.04071518103324444
Loss in iteration 21 : 0.038697669185770375
Loss in iteration 22 : 0.03681106779414923
Loss in iteration 23 : 0.035043159320049866
Loss in iteration 24 : 0.033383341200467875
Loss in iteration 25 : 0.031822343040851055
Loss in iteration 26 : 0.030352005384022298
Loss in iteration 27 : 0.028965104119292623
Loss in iteration 28 : 0.02765520931110852
Loss in iteration 29 : 0.026416570400357775
Loss in iteration 30 : 0.02524402190538602
Loss in iteration 31 : 0.024132905265298266
Loss in iteration 32 : 0.023079003543108198
Loss in iteration 33 : 0.0220784864816305
Loss in iteration 34 : 0.021127863973080935
Loss in iteration 35 : 0.020223946425692294
Loss in iteration 36 : 0.019363810828885924
Loss in iteration 37 : 0.018544771561268138
Loss in iteration 38 : 0.017764355172896127
Loss in iteration 39 : 0.017020278519030582
Loss in iteration 40 : 0.016310429737108927
Loss in iteration 41 : 0.015632851649337042
Loss in iteration 42 : 0.014985727245547418
Loss in iteration 43 : 0.01436736695886064
Loss in iteration 44 : 0.013776197493248142
Loss in iteration 45 : 0.013210751999622537
Loss in iteration 46 : 0.012669661427310552
Loss in iteration 47 : 0.012151646902020261
Loss in iteration 48 : 0.011655513000711581
Loss in iteration 49 : 0.011180141808906193
Loss in iteration 50 : 0.010724487657566116
Loss in iteration 51 : 0.010287572445267195
Loss in iteration 52 : 0.009868481457504956
Loss in iteration 53 : 0.009466359599123774
Loss in iteration 54 : 0.009080407958668732
Loss in iteration 55 : 0.008709880625655609
Loss in iteration 56 : 0.008354081684217581
Loss in iteration 57 : 0.008012362310339844
Loss in iteration 58 : 0.0076841179060197495
Loss in iteration 59 : 0.007368785213217486
Loss in iteration 60 : 0.007065839364142366
Loss in iteration 61 : 0.006774790842478682
Loss in iteration 62 : 0.00649518235203249
Loss in iteration 63 : 0.006226585613464892
Loss in iteration 64 : 0.0059685981338141675
Loss in iteration 65 : 0.005720840014278736
Loss in iteration 66 : 0.005482950875992228
Loss in iteration 67 : 0.005254586988630397
Loss in iteration 68 : 0.005035418681326711
Loss in iteration 69 : 0.00482512810000275
Loss in iteration 70 : 0.004623407352129526
Loss in iteration 71 : 0.00442995705277505
Loss in iteration 72 : 0.004244485258748291
Loss in iteration 73 : 0.004066706754512447
Loss in iteration 74 : 0.003896342636955218
Loss in iteration 75 : 0.003733120137194727
Loss in iteration 76 : 0.0035767726160317753
Loss in iteration 77 : 0.0034270396740196853
Loss in iteration 78 : 0.0032836673254572563
Loss in iteration 79 : 0.0031464081959214417
Loss in iteration 80 : 0.003015021713568424
Loss in iteration 81 : 0.002889274274156097
Loss in iteration 82 : 0.0027689393679055496
Loss in iteration 83 : 0.0026537976626741485
Loss in iteration 84 : 0.0025436370425134143
Loss in iteration 85 : 0.0024382526037591444
Loss in iteration 86 : 0.0023374466126519417
Loss in iteration 87 : 0.0022410284294208317
Loss in iteration 88 : 0.0021488144040538444
Loss in iteration 89 : 0.002060627748849267
Loss in iteration 90 : 0.001976298392457042
Loss in iteration 91 : 0.0018956628196050778
Loss in iteration 92 : 0.0018185639001366063
Loss in iteration 93 : 0.001744850710425714
Loss in iteration 94 : 0.0016743783497009603
Loss in iteration 95 : 0.0016070077533456937
Loss in iteration 96 : 0.0015426055047985273
Loss in iteration 97 : 0.001481043647375632
Loss in iteration 98 : 0.00142219949694306
Loss in iteration 99 : 0.0013659554563165298
Loss in iteration 100 : 0.0013121988317245305
Loss in iteration 101 : 0.0012608216522024406
Loss in iteration 102 : 0.001211720491428658
Loss in iteration 103 : 0.0011647962938351874
Loss in iteration 104 : 0.0011199542029144552
Loss in iteration 105 : 0.0010771033986246657
Loss in iteration 106 : 0.0010361569521016934
Loss in iteration 107 : 9.970317891213364E-4
Loss in iteration 108 : 9.596493208334146E-4
Loss in iteration 109 : 9.239401098798585E-4
Loss in iteration 110 : 8.898752701205774E-4
Loss in iteration 111 : 8.576691061938255E-4
Loss in iteration 112 : 8.291235325675842E-4
Loss in iteration 113 : 8.165663675281415E-4
Loss in iteration 114 : 9.043128031835073E-4
Loss in iteration 115 : 0.001686183839723306
Loss in iteration 116 : 0.006032733315219218
Loss in iteration 117 : 0.005547901167054053
Loss in iteration 118 : 0.001229890317894094
Loss in iteration 119 : 6.253095940128173E-4
Loss in iteration 120 : 5.64235517329591E-4
Loss in iteration 121 : 5.455351898974067E-4
Loss in iteration 122 : 5.356676179700275E-4
Loss in iteration 123 : 5.276415568481389E-4
Loss in iteration 124 : 5.199426076011899E-4
Loss in iteration 125 : 5.12266651372015E-4
Loss in iteration 126 : 5.045611290454415E-4
Loss in iteration 127 : 4.96821550758259E-4
Loss in iteration 128 : 4.890522820890271E-4
Loss in iteration 129 : 4.8125971551145883E-4
Loss in iteration 130 : 4.734511339217137E-4
Loss in iteration 131 : 4.6563445521731776E-4
Loss in iteration 132 : 4.5781809463456784E-4
Loss in iteration 133 : 4.500108406210042E-4
Loss in iteration 134 : 4.4222173350362505E-4
Loss in iteration 135 : 4.3445994754618225E-4
Loss in iteration 136 : 4.267346780496477E-4
Loss in iteration 137 : 4.1905503511107983E-4
Loss in iteration 138 : 4.1142994549802774E-4
Loss in iteration 139 : 4.0386806389424926E-4
Loss in iteration 140 : 3.9637769453026707E-4
Loss in iteration 141 : 3.8896672393393117E-4
Loss in iteration 142 : 3.8164256523245434E-4
Loss in iteration 143 : 3.744121141219747E-4
Loss in iteration 144 : 3.6728171631009973E-4
Loss in iteration 145 : 3.602571459469905E-4
Loss in iteration 146 : 3.533435943045152E-4
Loss in iteration 147 : 3.465456677529636E-4
Loss in iteration 148 : 3.3986739392757593E-4
Loss in iteration 149 : 3.3331223487701996E-4
Loss in iteration 150 : 3.268831059421261E-4
Loss in iteration 151 : 3.2058239912336595E-4
Loss in iteration 152 : 3.144120097519037E-4
Loss in iteration 153 : 3.0837336537456786E-4
Loss in iteration 154 : 3.0246745588739034E-4
Loss in iteration 155 : 2.9669486409571763E-4
Loss in iteration 156 : 2.9105579603118815E-4
Loss in iteration 157 : 2.8555011050917676E-4
Loss in iteration 158 : 2.80177347555239E-4
Loss in iteration 159 : 2.7493675546157187E-4
Loss in iteration 160 : 2.698273163484063E-4
Loss in iteration 161 : 2.6484777019922923E-4
Loss in iteration 162 : 2.599966374101999E-4
Loss in iteration 163 : 2.552722399450034E-4
Loss in iteration 164 : 2.5067272121663705E-4
Loss in iteration 165 : 2.4619606483066087E-4
Loss in iteration 166 : 2.418401123229386E-4
Loss in iteration 167 : 2.376025800123907E-4
Loss in iteration 168 : 2.3348107506941272E-4
Loss in iteration 169 : 2.2947311087628426E-4
Loss in iteration 170 : 2.255761217315061E-4
Loss in iteration 171 : 2.2178747692507103E-4
Loss in iteration 172 : 2.181044941922936E-4
Loss in iteration 173 : 2.1452445253627856E-4
Loss in iteration 174 : 2.1104460439821642E-4
Loss in iteration 175 : 2.076621871477597E-4
Loss in iteration 176 : 2.0437443386267897E-4
Loss in iteration 177 : 2.011785833692969E-4
Loss in iteration 178 : 1.9807188951833014E-4
Loss in iteration 179 : 1.9505162967800102E-4
Loss in iteration 180 : 1.9211511243364272E-4
Loss in iteration 181 : 1.89259684491539E-4
Loss in iteration 182 : 1.8648273679339772E-4
Loss in iteration 183 : 1.8378170985641098E-4
Loss in iteration 184 : 1.8115409836128894E-4
Loss in iteration 185 : 1.7859745501801297E-4
Loss in iteration 186 : 1.761093937448084E-4
Loss in iteration 187 : 1.736875922007388E-4
Loss in iteration 188 : 1.713297937163069E-4
Loss in iteration 189 : 1.6903380866895333E-4
Loss in iteration 190 : 1.667975153519439E-4
Loss in iteration 191 : 1.646188603860813E-4
Loss in iteration 192 : 1.6249585872283898E-4
Loss in iteration 193 : 1.6042659328697995E-4
Loss in iteration 194 : 1.5840921430461085E-4
Loss in iteration 195 : 1.5644193836067673E-4
Loss in iteration 196 : 1.5452304722668372E-4
Loss in iteration 197 : 1.526508864972239E-4
Loss in iteration 198 : 1.5082386406999702E-4
Loss in iteration 199 : 1.490404485012723E-4
Loss in iteration 200 : 1.472991672653024E-4
Testing accuracy  of updater 5 on alg 0 with rate 0.09999999999999998 = 1.0, training accuracy 1.0, time elapsed: 4441 millisecond.
Loss in iteration 1 : 0.6931471805599142
Loss in iteration 2 : 1.8184721435293538
Loss in iteration 3 : 0.2400081205325083
Loss in iteration 4 : 0.7362726309832313
Loss in iteration 5 : 0.5845545117052733
Loss in iteration 6 : 0.3045554518381629
Loss in iteration 7 : 0.1696494957563672
Loss in iteration 8 : 0.14012982791874848
Loss in iteration 9 : 0.19108270292949714
Loss in iteration 10 : 0.21470576651306017
Loss in iteration 11 : 0.16961833349640218
Loss in iteration 12 : 0.10486135104777991
Loss in iteration 13 : 0.06019098153660869
Loss in iteration 14 : 0.03878518288473785
Loss in iteration 15 : 0.032152472385260956
Loss in iteration 16 : 0.03322574431591283
Loss in iteration 17 : 0.037159543889967755
Loss in iteration 18 : 0.040365732873583325
Loss in iteration 19 : 0.041474385498560785
Loss in iteration 20 : 0.04023437554579093
Loss in iteration 21 : 0.03696052663429015
Loss in iteration 22 : 0.032548137187812964
Loss in iteration 23 : 0.028095301126597742
Loss in iteration 24 : 0.024423583404554517
Loss in iteration 25 : 0.02185705198008546
Loss in iteration 26 : 0.02029872580128942
Loss in iteration 27 : 0.019541153210996292
Loss in iteration 28 : 0.019273356296834587
Loss in iteration 29 : 0.019207287259594207
Loss in iteration 30 : 0.019145590706312977
Loss in iteration 31 : 0.018978482886231385
Loss in iteration 32 : 0.018662985795222648
Loss in iteration 33 : 0.01819756931540014
Loss in iteration 34 : 0.01760740567183469
Loss in iteration 35 : 0.016931900807766704
Loss in iteration 36 : 0.01621025261017113
Loss in iteration 37 : 0.015468618968170013
Loss in iteration 38 : 0.014716895026483073
Loss in iteration 39 : 0.01395495753027233
Loss in iteration 40 : 0.013180259912761508
Loss in iteration 41 : 0.012392187580103861
Loss in iteration 42 : 0.011594397602618793
Loss in iteration 43 : 0.010797775448884337
Loss in iteration 44 : 0.010023045473413332
Loss in iteration 45 : 0.009293235543900429
Loss in iteration 46 : 0.008618245867414472
Loss in iteration 47 : 0.008000140437211313
Loss in iteration 48 : 0.007446203330258413
Loss in iteration 49 : 0.006965253211028796
Loss in iteration 50 : 0.006555398829345396
Loss in iteration 51 : 0.006199652058946646
Loss in iteration 52 : 0.0058758205383505945
Loss in iteration 53 : 0.005566569799030818
Loss in iteration 54 : 0.005262048746900589
Loss in iteration 55 : 0.004958827363039429
Loss in iteration 56 : 0.004657344323086498
Loss in iteration 57 : 0.004358676678153407
Loss in iteration 58 : 0.004062555698405393
Loss in iteration 59 : 0.003767998219718586
Loss in iteration 60 : 0.003475223924471321
Loss in iteration 61 : 0.0031865775226179107
Loss in iteration 62 : 0.0029058423142932738
Loss in iteration 63 : 0.002637019931959737
Loss in iteration 64 : 0.0023836960225852525
Loss in iteration 65 : 0.002148944301429195
Loss in iteration 66 : 0.0019349680479781302
Loss in iteration 67 : 0.0017422608265692827
Loss in iteration 68 : 0.0015690948872083268
Loss in iteration 69 : 0.001412079119785261
Loss in iteration 70 : 0.0012674322781087298
Loss in iteration 71 : 0.001132076752210819
Loss in iteration 72 : 0.001004122134261776
Loss in iteration 73 : 8.828387718604895E-4
Loss in iteration 74 : 7.683363774518446E-4
Loss in iteration 75 : 6.611019762239016E-4
Loss in iteration 76 : 5.615665545382973E-4
Loss in iteration 77 : 4.6992096173412287E-4
Loss in iteration 78 : 3.8629444975834046E-4
Loss in iteration 79 : 3.1113302061379925E-4
Loss in iteration 80 : 2.454094653677752E-4
Loss in iteration 81 : 1.9034705660572198E-4
Loss in iteration 82 : 1.4668416107743026E-4
Loss in iteration 83 : 1.1400102573716667E-4
Loss in iteration 84 : 9.070229813874005E-5
Loss in iteration 85 : 7.461763825185181E-5
Loss in iteration 86 : 6.366121377186463E-5
Loss in iteration 87 : 5.617503847432703E-5
Loss in iteration 88 : 5.097605270357175E-5
Loss in iteration 89 : 4.726744063646947E-5
Testing accuracy  of updater 6 on alg 0 with rate 2.0 = 0.9946666666666667, training accuracy 1.0, time elapsed: 1848 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.406007824071247
Loss in iteration 3 : 0.2318047094201581
Loss in iteration 4 : 0.15888685248166395
Loss in iteration 5 : 0.11953533109267758
Loss in iteration 6 : 0.09529375955094103
Loss in iteration 7 : 0.07781675148034778
Loss in iteration 8 : 0.06378179003027526
Loss in iteration 9 : 0.052418410600492314
Loss in iteration 10 : 0.04361124536908499
Loss in iteration 11 : 0.0370824551954817
Loss in iteration 12 : 0.03225369436375129
Loss in iteration 13 : 0.028529048066210972
Loss in iteration 14 : 0.025515451232012525
Loss in iteration 15 : 0.023019130775391174
Loss in iteration 16 : 0.020949824196879772
Loss in iteration 17 : 0.01924502805851089
Loss in iteration 18 : 0.01783869845649035
Loss in iteration 19 : 0.016659114389613032
Loss in iteration 20 : 0.01563761016723437
Loss in iteration 21 : 0.014717657329124985
Loss in iteration 22 : 0.013860170801853738
Loss in iteration 23 : 0.013044156798304693
Loss in iteration 24 : 0.012263463012746213
Loss in iteration 25 : 0.01152134699870564
Loss in iteration 26 : 0.010824828488806658
Loss in iteration 27 : 0.010180325516950452
Loss in iteration 28 : 0.009591229934430227
Loss in iteration 29 : 0.009057294262356425
Loss in iteration 30 : 0.008575247578511368
Loss in iteration 31 : 0.008139964525411976
Loss in iteration 32 : 0.007745656839718725
Loss in iteration 33 : 0.007386786218039712
Loss in iteration 34 : 0.0070586008996870305
Loss in iteration 35 : 0.0067573303308173524
Loss in iteration 36 : 0.006480133399645752
Loss in iteration 37 : 0.006224906806861866
Loss in iteration 38 : 0.005990043735802869
Loss in iteration 39 : 0.005774205969668131
Loss in iteration 40 : 0.00557614529264336
Loss in iteration 41 : 0.005394587680224442
Loss in iteration 42 : 0.00522817827188393
Loss in iteration 43 : 0.005075476141279077
Loss in iteration 44 : 0.0049349840742434386
Loss in iteration 45 : 0.004805198291833583
Loss in iteration 46 : 0.004684664877465027
Loss in iteration 47 : 0.0045720325313621255
Loss in iteration 48 : 0.004466094493639065
Loss in iteration 49 : 0.004365815621743636
Loss in iteration 50 : 0.004270343410681614
Loss in iteration 51 : 0.004179004030972275
Loss in iteration 52 : 0.004091286120291341
Loss in iteration 53 : 0.004006816051932591
Loss in iteration 54 : 0.003925328734003553
Loss in iteration 55 : 0.003846637751091994
Loss in iteration 56 : 0.00377060798343193
Loss in iteration 57 : 0.003697132898736491
Loss in iteration 58 : 0.003626117685304814
Loss in iteration 59 : 0.0035574684367635488
Loss in iteration 60 : 0.0034910868231780635
Loss in iteration 61 : 0.0034268691548202377
Loss in iteration 62 : 0.003364708479423067
Loss in iteration 63 : 0.003304498326923914
Loss in iteration 64 : 0.003246136875797927
Loss in iteration 65 : 0.0031895305968718175
Loss in iteration 66 : 0.0031345967675972084
Loss in iteration 67 : 0.0030812645840721047
Loss in iteration 68 : 0.0030294748858060863
Loss in iteration 69 : 0.002979178722274681
Loss in iteration 70 : 0.0029303351199914656
Loss in iteration 71 : 0.002882908457230364
Loss in iteration 72 : 0.0028368658337347527
Loss in iteration 73 : 0.0027921747536202657
Loss in iteration 74 : 0.0027488013419451774
Loss in iteration 75 : 0.0027067092085248903
Loss in iteration 76 : 0.0026658589724763834
Loss in iteration 77 : 0.00262620837904276
Loss in iteration 78 : 0.0025877128829123425
Loss in iteration 79 : 0.0025503265414427777
Loss in iteration 80 : 0.0025140030551824176
Loss in iteration 81 : 0.002478696807477122
Loss in iteration 82 : 0.0024443637838694725
Loss in iteration 83 : 0.002410962289054289
Loss in iteration 84 : 0.002378453418326371
Loss in iteration 85 : 0.0023468012767098594
Loss in iteration 86 : 0.002315972968644815
Loss in iteration 87 : 0.002285938402095981
Loss in iteration 88 : 0.0022566699625867436
Loss in iteration 89 : 0.002228142115529365
Loss in iteration 90 : 0.002200330990821132
Loss in iteration 91 : 0.0021732139940519105
Loss in iteration 92 : 0.002146769476049517
Loss in iteration 93 : 0.0021209764789876765
Loss in iteration 94 : 0.0020958145646548425
Loss in iteration 95 : 0.0020712637200103424
Loss in iteration 96 : 0.0020473043275839233
Loss in iteration 97 : 0.0020239171838474356
Loss in iteration 98 : 0.002001083547226799
Loss in iteration 99 : 0.0019787851984388377
Loss in iteration 100 : 0.0019570044986620115
Loss in iteration 101 : 0.0019357244349476111
Loss in iteration 102 : 0.0019149286465495869
Loss in iteration 103 : 0.0018946014299076823
Loss in iteration 104 : 0.0018747277234286953
Loss in iteration 105 : 0.0018552930757083895
Loss in iteration 106 : 0.0018362836023282589
Loss in iteration 107 : 0.0018176859368881828
Loss in iteration 108 : 0.0017994871816448897
Loss in iteration 109 : 0.0017816748622303674
Loss in iteration 110 : 0.0017642368896604679
Loss in iteration 111 : 0.001747161531440464
Loss in iteration 112 : 0.0017304373922285457
Loss in iteration 113 : 0.0017140534033780572
Loss in iteration 114 : 0.0016979988198336138
Loss in iteration 115 : 0.0016822632223467476
Loss in iteration 116 : 0.0016668365227911017
Loss in iteration 117 : 0.0016517089704520002
Loss in iteration 118 : 0.0016368711574734093
Loss in iteration 119 : 0.0016223140220885171
Loss in iteration 120 : 0.0016080288487621021
Loss in iteration 121 : 0.001594007264867859
Loss in iteration 122 : 0.0015802412339571855
Loss in iteration 123 : 0.001566723046017412
Loss in iteration 124 : 0.0015534453053452975
Loss in iteration 125 : 0.0015404009167804891
Loss in iteration 126 : 0.0015275830710593725
Loss in iteration 127 : 0.0015149852299869916
Loss in iteration 128 : 0.0015026011120036588
Loss in iteration 129 : 0.0014904246785713469
Loss in iteration 130 : 0.0014784501216441872
Loss in iteration 131 : 0.0014666718523364368
Loss in iteration 132 : 0.0014550844907742202
Loss in iteration 133 : 0.0014436828570216418
Loss in iteration 134 : 0.0014324619629098056
Testing accuracy  of updater 6 on alg 0 with rate 0.2 = 0.9946666666666667, training accuracy 1.0, time elapsed: 3125 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6415962468972009
Loss in iteration 3 : 0.5670786845159825
Loss in iteration 4 : 0.4887027417482753
Loss in iteration 5 : 0.4164837679809074
Loss in iteration 6 : 0.3548043741531693
Loss in iteration 7 : 0.30488209747456585
Loss in iteration 8 : 0.26573781027466864
Loss in iteration 9 : 0.23520138575781532
Loss in iteration 10 : 0.2109839967442993
Loss in iteration 11 : 0.19126375687086283
Loss in iteration 12 : 0.17477839824980657
Loss in iteration 13 : 0.16070053447256868
Loss in iteration 14 : 0.14848794034141838
Loss in iteration 15 : 0.13777353916307772
Loss in iteration 16 : 0.12829731261255142
Loss in iteration 17 : 0.11986661505988783
Loss in iteration 18 : 0.11233297317179228
Loss in iteration 19 : 0.10557805722997032
Loss in iteration 20 : 0.09950489151623915
Loss in iteration 21 : 0.09403221385263337
Loss in iteration 22 : 0.089090791829806
Loss in iteration 23 : 0.08462095653941949
Loss in iteration 24 : 0.080570880689037
Loss in iteration 25 : 0.0768953052196051
Loss in iteration 26 : 0.07355453901394383
Loss in iteration 27 : 0.07051363406344961
Loss in iteration 28 : 0.06774168519393649
Loss in iteration 29 : 0.06521122969098199
Loss in iteration 30 : 0.06289773584406383
Loss in iteration 31 : 0.0607791754795724
Loss in iteration 32 : 0.0588356767837523
Loss in iteration 33 : 0.05704925195638403
Loss in iteration 34 : 0.05540359103507491
Loss in iteration 35 : 0.05388391000453351
Loss in iteration 36 : 0.05247683916896425
Loss in iteration 37 : 0.051170337339039085
Loss in iteration 38 : 0.049953618740181956
Loss in iteration 39 : 0.04881708231053445
Loss in iteration 40 : 0.047752236592676245
Loss in iteration 41 : 0.04675161704877692
Loss in iteration 42 : 0.045808695770043935
Loss in iteration 43 : 0.044917785836877826
Loss in iteration 44 : 0.044073943872755546
Loss in iteration 45 : 0.04327287467903231
Loss in iteration 46 : 0.042510841436295556
Loss in iteration 47 : 0.04178458407744099
Loss in iteration 48 : 0.041091247351102145
Loss in iteration 49 : 0.040428319032527055
Loss in iteration 50 : 0.03979357786451051
Loss in iteration 51 : 0.03918505020890402
Loss in iteration 52 : 0.03860097407585851
Loss in iteration 53 : 0.03803976913780981
Loss in iteration 54 : 0.037500011461986706
Loss in iteration 55 : 0.03698041193182903
Loss in iteration 56 : 0.03647979760227911
Loss in iteration 57 : 0.035997095490188674
Loss in iteration 58 : 0.03553131850314031
Loss in iteration 59 : 0.0350815533421982
Loss in iteration 60 : 0.034646950277748646
Loss in iteration 61 : 0.034226714705656164
Loss in iteration 62 : 0.033820100362458604
Loss in iteration 63 : 0.03342640403335129
Loss in iteration 64 : 0.033044961542626185
Loss in iteration 65 : 0.032675144785457744
Loss in iteration 66 : 0.03231635954914157
Loss in iteration 67 : 0.031968043882377464
Loss in iteration 68 : 0.031629666800025255
Loss in iteration 69 : 0.03130072715234274
Loss in iteration 70 : 0.03098075253533312
Loss in iteration 71 : 0.030669298166038493
Loss in iteration 72 : 0.030365945688196578
Loss in iteration 73 : 0.030070301906185565
Loss in iteration 74 : 0.029781997467081565
Loss in iteration 75 : 0.02950068552212546
Loss in iteration 76 : 0.029226040401435185
Loss in iteration 77 : 0.028957756331700144
Loss in iteration 78 : 0.028695546218455742
Loss in iteration 79 : 0.028439140504816027
Loss in iteration 80 : 0.02818828610926045
Loss in iteration 81 : 0.027942745437634028
Loss in iteration 82 : 0.027702295459665977
Loss in iteration 83 : 0.027466726838197168
Loss in iteration 84 : 0.027235843099604702
Loss in iteration 85 : 0.027009459836010358
Loss in iteration 86 : 0.026787403933005268
Loss in iteration 87 : 0.026569512820079404
Loss in iteration 88 : 0.026355633744095348
Loss in iteration 89 : 0.02614562306855938
Loss in iteration 90 : 0.02593934560289269
Loss in iteration 91 : 0.02573667396635978
Loss in iteration 92 : 0.025537487990884726
Loss in iteration 93 : 0.025341674165905805
Loss in iteration 94 : 0.025149125126949844
Loss in iteration 95 : 0.024959739188014606
Loss in iteration 96 : 0.024773419916369565
Loss in iteration 97 : 0.024590075747175415
Loss in iteration 98 : 0.02440961963449062
Loss in iteration 99 : 0.024231968734796872
Loss in iteration 100 : 0.02405704411911159
Loss in iteration 101 : 0.023884770509989005
Loss in iteration 102 : 0.023715076040155394
Loss in iteration 103 : 0.023547892030074357
Loss in iteration 104 : 0.02338315278231263
Loss in iteration 105 : 0.02322079539110059
Loss in iteration 106 : 0.023060759565909163
Loss in iteration 107 : 0.022902987468172688
Loss in iteration 108 : 0.022747423560474083
Loss in iteration 109 : 0.022594014467587766
Loss in iteration 110 : 0.022442708848773776
Loss in iteration 111 : 0.02229345728066164
Loss in iteration 112 : 0.022146212149988496
Loss in iteration 113 : 0.022000927555380665
Loss in iteration 114 : 0.02185755921731916
Loss in iteration 115 : 0.021716064395411076
Loss in iteration 116 : 0.02157640181210718
Loss in iteration 117 : 0.02143853158206109
Loss in iteration 118 : 0.021302415146403512
Loss in iteration 119 : 0.021168015211305697
Loss in iteration 120 : 0.02103529569030966
Loss in iteration 121 : 0.02090422165000741
Loss in iteration 122 : 0.020774759258744686
Loss in iteration 123 : 0.020646875738103296
Loss in iteration 124 : 0.020520539316975624
Loss in iteration 125 : 0.020395719188089807
Loss in iteration 126 : 0.020272385466866556
Loss in iteration 127 : 0.020150509152505127
Loss in iteration 128 : 0.020030062091194765
Loss in iteration 129 : 0.01991101694134713
Loss in iteration 130 : 0.01979334714073844
Loss in iteration 131 : 0.01967702687544352
Loss in iteration 132 : 0.019562031050442217
Loss in iteration 133 : 0.019448335261776438
Loss in iteration 134 : 0.019335915770140925
Loss in iteration 135 : 0.01922474947579759
Loss in iteration 136 : 0.019114813894711848
Loss in iteration 137 : 0.019006087135821286
Loss in iteration 138 : 0.01889854787935834
Loss in iteration 139 : 0.018792175356159922
Loss in iteration 140 : 0.018686949327907196
Loss in iteration 141 : 0.01858285006824676
Loss in iteration 142 : 0.018479858344752048
Loss in iteration 143 : 0.018377955401687334
Loss in iteration 144 : 0.018277122943540842
Loss in iteration 145 : 0.018177343119294132
Loss in iteration 146 : 0.018078598507396276
Loss in iteration 147 : 0.01798087210141078
Loss in iteration 148 : 0.017884147296303797
Loss in iteration 149 : 0.017788407875340407
Loss in iteration 150 : 0.017693637997558093
Loss in iteration 151 : 0.017599822185784805
Loss in iteration 152 : 0.017506945315171187
Loss in iteration 153 : 0.01741499260220792
Loss in iteration 154 : 0.017323949594199722
Loss in iteration 155 : 0.017233802159170735
Loss in iteration 156 : 0.01714453647617719
Loss in iteration 157 : 0.01705613902600516
Loss in iteration 158 : 0.0169685965822339
Loss in iteration 159 : 0.016881896202646053
Loss in iteration 160 : 0.016796025220968028
Loss in iteration 161 : 0.016710971238925194
Loss in iteration 162 : 0.016626722118596927
Loss in iteration 163 : 0.016543265975058823
Loss in iteration 164 : 0.016460591169298006
Loss in iteration 165 : 0.016378686301390374
Loss in iteration 166 : 0.01629754020392698
Loss in iteration 167 : 0.01621714193567899
Loss in iteration 168 : 0.016137480775489682
Loss in iteration 169 : 0.016058546216383467
Loss in iteration 170 : 0.015980327959881496
Loss in iteration 171 : 0.015902815910514602
Loss in iteration 172 : 0.015826000170524476
Loss in iteration 173 : 0.01574987103474426
Loss in iteration 174 : 0.01567441898565083
Loss in iteration 175 : 0.015599634688580982
Loss in iteration 176 : 0.015525508987104574
Loss in iteration 177 : 0.01545203289854774
Loss in iteration 178 : 0.015379197609660238
Loss in iteration 179 : 0.015306994472420891
Loss in iteration 180 : 0.01523541499997555
Loss in iteration 181 : 0.015164450862702471
Loss in iteration 182 : 0.015094093884400186
Loss in iteration 183 : 0.015024336038592977
Loss in iteration 184 : 0.014955169444949607
Loss in iteration 185 : 0.014886586365811003
Loss in iteration 186 : 0.014818579202822656
Loss in iteration 187 : 0.014751140493668049
Loss in iteration 188 : 0.01468426290889902
Loss in iteration 189 : 0.014617939248859712
Loss in iteration 190 : 0.014552162440700574
Loss in iteration 191 : 0.014486925535479027
Loss in iteration 192 : 0.014422221705343867
Loss in iteration 193 : 0.014358044240799901
Loss in iteration 194 : 0.014294386548050788
Loss in iteration 195 : 0.014231242146416363
Loss in iteration 196 : 0.01416860466582255
Loss in iteration 197 : 0.014106467844361046
Loss in iteration 198 : 0.01404482552591653
Loss in iteration 199 : 0.01398367165785881
Loss in iteration 200 : 0.013923000288798327
Testing accuracy  of updater 6 on alg 0 with rate 0.01999999999999999 = 0.9831111111111112, training accuracy 0.999285510145756, time elapsed: 4913 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 12.648730475199672
Loss in iteration 3 : 2.055630161094476
Loss in iteration 4 : 3.394670536850769
Loss in iteration 5 : 5.199504424964678
Loss in iteration 6 : 2.869943660810852
Loss in iteration 7 : 1.554424804863215
Loss in iteration 8 : 0.8698107688906964
Loss in iteration 9 : 0.7844340305229386
Loss in iteration 10 : 1.0406213991043964
Loss in iteration 11 : 1.211239702318837
Loss in iteration 12 : 1.0801962754530576
Loss in iteration 13 : 0.7738182304863925
Loss in iteration 14 : 0.48545488035143936
Loss in iteration 15 : 0.3039620970790456
Loss in iteration 16 : 0.2086491166348494
Loss in iteration 17 : 0.17632375443553402
Loss in iteration 18 : 0.17018014328225647
Loss in iteration 19 : 0.18174594368094685
Loss in iteration 20 : 0.19396103287038682
Loss in iteration 21 : 0.20150396519053557
Loss in iteration 22 : 0.2011499147178029
Loss in iteration 23 : 0.1929260707424275
Loss in iteration 24 : 0.17793084687033672
Loss in iteration 25 : 0.16050965383175858
Loss in iteration 26 : 0.1442342052543157
Loss in iteration 27 : 0.13012267385178353
Loss in iteration 28 : 0.11993041589699988
Loss in iteration 29 : 0.11166668197503538
Loss in iteration 30 : 0.10382985246044135
Loss in iteration 31 : 0.09782555829295998
Loss in iteration 32 : 0.09422207392671758
Loss in iteration 33 : 0.09259024869804686
Loss in iteration 34 : 0.09237404391606267
Loss in iteration 35 : 0.09216872037998254
Loss in iteration 36 : 0.0917867868983219
Loss in iteration 37 : 0.0912673236725135
Loss in iteration 38 : 0.09063279720719673
Loss in iteration 39 : 0.08985970943576198
Loss in iteration 40 : 0.08890668183196301
Loss in iteration 41 : 0.08775745786848936
Loss in iteration 42 : 0.08642679293144284
Loss in iteration 43 : 0.08494611827162736
Loss in iteration 44 : 0.08334665966289363
Loss in iteration 45 : 0.08165305470197921
Loss in iteration 46 : 0.07988854088920794
Loss in iteration 47 : 0.07807688366232463
Loss in iteration 48 : 0.07623682389376228
Loss in iteration 49 : 0.0743807228002477
Loss in iteration 50 : 0.07254095886493417
Loss in iteration 51 : 0.07081212935057805
Loss in iteration 52 : 0.06919226670641285
Loss in iteration 53 : 0.06760043848712477
Loss in iteration 54 : 0.066012738988934
Loss in iteration 55 : 0.06442531033497834
Loss in iteration 56 : 0.06283737211067844
Loss in iteration 57 : 0.06124866712488182
Loss in iteration 58 : 0.059659117939673836
Loss in iteration 59 : 0.058068924328285275
Loss in iteration 60 : 0.056479281381901766
Loss in iteration 61 : 0.05489531733047788
Loss in iteration 62 : 0.05333507450156026
Loss in iteration 63 : 0.05183696003869267
Loss in iteration 64 : 0.05042701842239396
Loss in iteration 65 : 0.04910793666305958
Loss in iteration 66 : 0.04789084427537683
Loss in iteration 67 : 0.046751705339526814
Loss in iteration 68 : 0.04564021810763955
Loss in iteration 69 : 0.04453225490277036
Loss in iteration 70 : 0.04344390975890585
Loss in iteration 71 : 0.04240901149857393
Loss in iteration 72 : 0.04141282113411758
Loss in iteration 73 : 0.040422138081739195
Loss in iteration 74 : 0.03942529662061032
Loss in iteration 75 : 0.03842279231131073
Loss in iteration 76 : 0.037419533361027144
Loss in iteration 77 : 0.036422928005590716
Loss in iteration 78 : 0.03544305164246473
Loss in iteration 79 : 0.03449354936268212
Loss in iteration 80 : 0.03359136186797675
Loss in iteration 81 : 0.03274882322893415
Loss in iteration 82 : 0.03195874765229197
Loss in iteration 83 : 0.031200060984756434
Loss in iteration 84 : 0.03045591052195407
Loss in iteration 85 : 0.02971744125003536
Loss in iteration 86 : 0.028980513224954096
Loss in iteration 87 : 0.028243268302230146
Loss in iteration 88 : 0.027504975070663892
Loss in iteration 89 : 0.02676546109060614
Loss in iteration 90 : 0.026024810927835245
Loss in iteration 91 : 0.025283214847372636
Loss in iteration 92 : 0.02454090159409185
Loss in iteration 93 : 0.02379810446113505
Loss in iteration 94 : 0.02305503310863517
Loss in iteration 95 : 0.022311846329166073
Loss in iteration 96 : 0.021568632138958533
Loss in iteration 97 : 0.020825401404615796
Loss in iteration 98 : 0.020082095861983715
Loss in iteration 99 : 0.019338606399931298
Loss in iteration 100 : 0.018594795410200794
Loss in iteration 101 : 0.017850517754604608
Loss in iteration 102 : 0.017105637118836652
Loss in iteration 103 : 0.016360036928990303
Loss in iteration 104 : 0.015613627346891187
Loss in iteration 105 : 0.014866354359214264
Loss in iteration 106 : 0.014118234834935987
Loss in iteration 107 : 0.013369523754424885
Loss in iteration 108 : 0.012621492295898287
Loss in iteration 109 : 0.01187977127814452
Loss in iteration 110 : 0.011164866226849632
Loss in iteration 111 : 0.010516769218752652
Loss in iteration 112 : 0.00994338598259555
Loss in iteration 113 : 0.009409429674772062
Loss in iteration 114 : 0.008892801505112782
Loss in iteration 115 : 0.008388617518180583
Loss in iteration 116 : 0.007899951159218233
Loss in iteration 117 : 0.007436241846741241
Loss in iteration 118 : 0.007012094600877052
Testing accuracy  of updater 7 on alg 0 with rate 20.0 = 0.992, training accuracy 0.9995713060874536, time elapsed: 2716 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.3391016711935832
Loss in iteration 3 : 0.1714582946972003
Loss in iteration 4 : 0.12975586191863905
Loss in iteration 5 : 0.09190285033766137
Loss in iteration 6 : 0.06837450994477323
Loss in iteration 7 : 0.05325659230132613
Loss in iteration 8 : 0.04200957849604965
Loss in iteration 9 : 0.033275233666342774
Loss in iteration 10 : 0.026840044176141217
Loss in iteration 11 : 0.022421972189396087
Loss in iteration 12 : 0.019476216432929027
Loss in iteration 13 : 0.017453621581875935
Loss in iteration 14 : 0.015951161430354632
Loss in iteration 15 : 0.0147248441033285
Loss in iteration 16 : 0.01365060286928944
Loss in iteration 17 : 0.012678149483999882
Loss in iteration 18 : 0.011793197115566166
Loss in iteration 19 : 0.010992444621287699
Loss in iteration 20 : 0.010271093159175432
Loss in iteration 21 : 0.009619750259447685
Loss in iteration 22 : 0.009026377369775407
Loss in iteration 23 : 0.008479546878217005
Loss in iteration 24 : 0.007970788571988716
Loss in iteration 25 : 0.007495302477643977
Loss in iteration 26 : 0.007051327913491501
Loss in iteration 27 : 0.00663887019185796
Loss in iteration 28 : 0.006258431140255002
Loss in iteration 29 : 0.005910116636218176
Loss in iteration 30 : 0.005593214764697037
Loss in iteration 31 : 0.0053061581907077465
Loss in iteration 32 : 0.0050467161974974755
Loss in iteration 33 : 0.004812270359334679
Loss in iteration 34 : 0.004600072612275094
Loss in iteration 35 : 0.004407436409039512
Loss in iteration 36 : 0.004231852922848554
Loss in iteration 37 : 0.004071047329038119
Loss in iteration 38 : 0.00392299652944874
Loss in iteration 39 : 0.003785925994800176
Loss in iteration 40 : 0.0036582966563274617
Loss in iteration 41 : 0.0035387872231112975
Loss in iteration 42 : 0.003426274182823249
Loss in iteration 43 : 0.0033198105507438467
Loss in iteration 44 : 0.003218604202551411
Loss in iteration 45 : 0.0031219966486223504
Loss in iteration 46 : 0.0030294430601763
Loss in iteration 47 : 0.0029404941852404685
Loss in iteration 48 : 0.0028547805491147653
Loss in iteration 49 : 0.0027719990860477148
Loss in iteration 50 : 0.0026919021385277252
Loss in iteration 51 : 0.0026142886045327227
Loss in iteration 52 : 0.002538996911760894
Loss in iteration 53 : 0.0024658994450645934
Loss in iteration 54 : 0.002394898041526594
Loss in iteration 55 : 0.0023259201898860487
Loss in iteration 56 : 0.0022589156207247804
Loss in iteration 57 : 0.002193853043891306
Loss in iteration 58 : 0.002130716872031128
Loss in iteration 59 : 0.0020695038548973585
Loss in iteration 60 : 0.0020102196292429288
Loss in iteration 61 : 0.0019528752553155805
Loss in iteration 62 : 0.0018974838570231054
Loss in iteration 63 : 0.001844057505281814
Loss in iteration 64 : 0.0017926044827561508
Loss in iteration 65 : 0.0017431270461556229
Loss in iteration 66 : 0.001695619765009837
Loss in iteration 67 : 0.0016500684703867233
Loss in iteration 68 : 0.001606449800571277
Loss in iteration 69 : 0.0015647312895563344
Loss in iteration 70 : 0.0015248719128033046
Loss in iteration 71 : 0.0014868229854036142
Loss in iteration 72 : 0.0014505293006966625
Loss in iteration 73 : 0.0014159304010807087
Loss in iteration 74 : 0.001382961884661792
Loss in iteration 75 : 0.0013515566685950516
Loss in iteration 76 : 0.0013216461496959103
Loss in iteration 77 : 0.001293161222804226
Loss in iteration 78 : 0.0012660331357456508
Loss in iteration 79 : 0.0012401941754264683
Loss in iteration 80 : 0.0012155781920117636
Loss in iteration 81 : 0.0011921209770859142
Loss in iteration 82 : 0.0011697605172903624
Loss in iteration 83 : 0.001148437147523313
Loss in iteration 84 : 0.0011280936278556314
Loss in iteration 85 : 0.0011086751664471847
Loss in iteration 86 : 0.0010901294075451083
Loss in iteration 87 : 0.0010724063996984314
Loss in iteration 88 : 0.0010554585551536724
Loss in iteration 89 : 0.0010392406074340175
Loss in iteration 90 : 0.0010237095706606277
Loss in iteration 91 : 0.0010088247014456233
Loss in iteration 92 : 9.945474622528848E-4
Loss in iteration 93 : 9.808414839730182E-4
Testing accuracy  of updater 7 on alg 0 with rate 2.0 = 0.9928888888888889, training accuracy 0.9998571020291512, time elapsed: 2230 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.6261979445554724
Loss in iteration 3 : 0.5250289790639677
Loss in iteration 4 : 0.4225174267320094
Loss in iteration 5 : 0.3372174632891785
Loss in iteration 6 : 0.2739271328916023
Loss in iteration 7 : 0.22879978729702125
Loss in iteration 8 : 0.1958076065440417
Loss in iteration 9 : 0.17046587097923083
Loss in iteration 10 : 0.150219468104865
Loss in iteration 11 : 0.13366646566237247
Loss in iteration 12 : 0.11990927002096231
Loss in iteration 13 : 0.10826967535731034
Loss in iteration 14 : 0.09822535845398571
Loss in iteration 15 : 0.08940729011724717
Loss in iteration 16 : 0.08158328490106959
Loss in iteration 17 : 0.07462040922148372
Loss in iteration 18 : 0.06844218249731462
Loss in iteration 19 : 0.06299392736552337
Loss in iteration 20 : 0.058221340668984496
Loss in iteration 21 : 0.05406158579758296
Loss in iteration 22 : 0.05044348142196035
Loss in iteration 23 : 0.04729255109299062
Loss in iteration 24 : 0.04453717870674414
Loss in iteration 25 : 0.042113418991143146
Loss in iteration 26 : 0.039967490278732545
Loss in iteration 27 : 0.03805608121982435
Loss in iteration 28 : 0.0363451500666853
Loss in iteration 29 : 0.03480799346526803
Loss in iteration 30 : 0.033423212037267024
Loss in iteration 31 : 0.03217296766215826
Loss in iteration 32 : 0.03104170968394246
Loss in iteration 33 : 0.030015386357074233
Loss in iteration 34 : 0.02908106146191175
Loss in iteration 35 : 0.028226814482247585
Loss in iteration 36 : 0.02744180006691644
Loss in iteration 37 : 0.026716362789827468
Loss in iteration 38 : 0.026042133617121505
Loss in iteration 39 : 0.025412065988589555
Loss in iteration 40 : 0.024820396575085658
Loss in iteration 41 : 0.024262535937038823
Loss in iteration 42 : 0.023734906836056443
Loss in iteration 43 : 0.023234753509547366
Loss in iteration 44 : 0.022759945234787773
Loss in iteration 45 : 0.02230879373654413
Loss in iteration 46 : 0.02187989817492853
Loss in iteration 47 : 0.021472025085868523
Loss in iteration 48 : 0.02108402485093299
Loss in iteration 49 : 0.020714781735212646
Loss in iteration 50 : 0.020363191545919875
Loss in iteration 51 : 0.020028159511383765
Loss in iteration 52 : 0.01970861082993961
Loss in iteration 53 : 0.019403507149087125
Loss in iteration 54 : 0.019111863636549906
Loss in iteration 55 : 0.01883276295601056
Loss in iteration 56 : 0.018565364086477257
Loss in iteration 57 : 0.018308905331768485
Loss in iteration 58 : 0.018062701943367773
Loss in iteration 59 : 0.01782613948574917
Loss in iteration 60 : 0.017598664424581214
Loss in iteration 61 : 0.017379773470218927
Loss in iteration 62 : 0.017169003038266854
Loss in iteration 63 : 0.016965919878760994
Loss in iteration 64 : 0.016770113553205065
Loss in iteration 65 : 0.016581191068442712
Loss in iteration 66 : 0.01639877365472391
Loss in iteration 67 : 0.016222495430437894
Loss in iteration 68 : 0.01605200353906525
Loss in iteration 69 : 0.015886959272485776
Loss in iteration 70 : 0.01572703969667762
Loss in iteration 71 : 0.015571939353168961
Loss in iteration 72 : 0.015421371702410489
Loss in iteration 73 : 0.015275070084369539
Loss in iteration 74 : 0.015132788080794796
Loss in iteration 75 : 0.014994299260429606
Loss in iteration 76 : 0.014859396365022782
Loss in iteration 77 : 0.014727890046553479
Loss in iteration 78 : 0.014599607294475115
Loss in iteration 79 : 0.014474389698539732
Loss in iteration 80 : 0.014352091682198835
Loss in iteration 81 : 0.014232578818871862
Loss in iteration 82 : 0.014115726313794147
Loss in iteration 83 : 0.014001417702469548
Loss in iteration 84 : 0.0138895437868185
Loss in iteration 85 : 0.013780001804695541
Loss in iteration 86 : 0.013672694809181025
Loss in iteration 87 : 0.013567531221524287
Loss in iteration 88 : 0.013464424515570372
Loss in iteration 89 : 0.013363292991065784
Loss in iteration 90 : 0.013264059597142623
Loss in iteration 91 : 0.013166651774117766
Loss in iteration 92 : 0.013071001290117384
Loss in iteration 93 : 0.012977044057731611
Loss in iteration 94 : 0.01288471992394787
Loss in iteration 95 : 0.012793972433334778
Loss in iteration 96 : 0.012704748569471323
Loss in iteration 97 : 0.012616998482821927
Loss in iteration 98 : 0.0125306752147492
Loss in iteration 99 : 0.012445734427378147
Loss in iteration 100 : 0.012362134147923129
Loss in iteration 101 : 0.012279834534237205
Loss in iteration 102 : 0.012198797666105704
Loss in iteration 103 : 0.012118987364502518
Loss in iteration 104 : 0.01204036903890908
Loss in iteration 105 : 0.01196290956103653
Loss in iteration 106 : 0.011886577161992339
Loss in iteration 107 : 0.011811341349121877
Loss in iteration 108 : 0.01173717283841063
Loss in iteration 109 : 0.011664043498388047
Loss in iteration 110 : 0.011591926301841356
Loss in iteration 111 : 0.011520795282227636
Loss in iteration 112 : 0.011450625492368003
Loss in iteration 113 : 0.01138139296373171
Loss in iteration 114 : 0.011313074665301572
Loss in iteration 115 : 0.011245648461602689
Loss in iteration 116 : 0.011179093069946505
Loss in iteration 117 : 0.011113388017274229
Loss in iteration 118 : 0.011048513597184312
Loss in iteration 119 : 0.010984450827810252
Loss in iteration 120 : 0.010921181411197621
Loss in iteration 121 : 0.01085868769474031
Loss in iteration 122 : 0.01079695263510064
Loss in iteration 123 : 0.01073595976488056
Loss in iteration 124 : 0.010675693162151181
Loss in iteration 125 : 0.01061613742280374
Loss in iteration 126 : 0.010557277635564367
Loss in iteration 127 : 0.010499099359427229
Loss in iteration 128 : 0.01044158860320455
Loss in iteration 129 : 0.010384731806868096
Loss in iteration 130 : 0.010328515824359884
Loss in iteration 131 : 0.010272927907574628
Loss in iteration 132 : 0.01021795569125578
Loss in iteration 133 : 0.01016358717859683
Loss in iteration 134 : 0.010109810727389618
Loss in iteration 135 : 0.010056615036613035
Loss in iteration 136 : 0.010003989133398963
Loss in iteration 137 : 0.0099519223603495
Loss in iteration 138 : 0.009900404363207788
Loss in iteration 139 : 0.009849425078903074
Loss in iteration 140 : 0.009798974724001193
Loss in iteration 141 : 0.009749043783594485
Loss in iteration 142 : 0.009699623000662027
Loss in iteration 143 : 0.009650703365924182
Loss in iteration 144 : 0.009602276108205498
Loss in iteration 145 : 0.009554332685309438
Loss in iteration 146 : 0.009506864775397656
Loss in iteration 147 : 0.009459864268857367
Loss in iteration 148 : 0.009413323260632002
Loss in iteration 149 : 0.0093672340429857
Loss in iteration 150 : 0.009321589098667605
Loss in iteration 151 : 0.009276381094441484
Loss in iteration 152 : 0.00923160287494611
Loss in iteration 153 : 0.009187247456853832
Loss in iteration 154 : 0.009143308023298017
Loss in iteration 155 : 0.00909977791854312
Loss in iteration 156 : 0.009056650642875455
Loss in iteration 157 : 0.009013919847696764
Loss in iteration 158 : 0.008971579330805463
Loss in iteration 159 : 0.008929623031854808
Loss in iteration 160 : 0.008888045027978717
Loss in iteration 161 : 0.008846839529578671
Loss in iteration 162 : 0.008806000876266472
Loss in iteration 163 : 0.008765523532958107
Loss in iteration 164 : 0.008725402086115011
Loss in iteration 165 : 0.008685631240128881
Loss in iteration 166 : 0.008646205813845799
Loss in iteration 167 : 0.008607120737225664
Loss in iteration 168 : 0.008568371048132042
Loss in iteration 169 : 0.00852995188924757
Loss in iteration 170 : 0.008491858505109392
Loss in iteration 171 : 0.008454086239259213
Loss in iteration 172 : 0.008416630531502276
Loss in iteration 173 : 0.008379486915269352
Loss in iteration 174 : 0.008342651015076469
Loss in iteration 175 : 0.008306118544076755
Loss in iteration 176 : 0.008269885301699427
Loss in iteration 177 : 0.008233947171371042
Loss in iteration 178 : 0.008198300118314843
Loss in iteration 179 : 0.008162940187423862
Loss in iteration 180 : 0.008127863501204381
Loss in iteration 181 : 0.008093066257786446
Loss in iteration 182 : 0.008058544728998297
Loss in iteration 183 : 0.008024295258502297
Loss in iteration 184 : 0.007990314259989712
Loss in iteration 185 : 0.0079565982154321
Loss in iteration 186 : 0.007923143673387546
Loss in iteration 187 : 0.007889947247359377
Loss in iteration 188 : 0.007857005614205855
Loss in iteration 189 : 0.007824315512598783
Loss in iteration 190 : 0.007791873741529777
Loss in iteration 191 : 0.007759677158861916
Loss in iteration 192 : 0.0077277226799257115
Loss in iteration 193 : 0.00769600727615749
Loss in iteration 194 : 0.00766452797377879
Loss in iteration 195 : 0.007633281852515085
Loss in iteration 196 : 0.0076022660443527176
Loss in iteration 197 : 0.007571477732332108
Loss in iteration 198 : 0.007540914149376326
Loss in iteration 199 : 0.007510572577153523
Loss in iteration 200 : 0.007480450344971851
Testing accuracy  of updater 7 on alg 0 with rate 0.19999999999999996 = 0.9911111111111112, training accuracy 0.9994284081166047, time elapsed: 4822 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 2.9287772595038857
Loss in iteration 3 : 0.34643942686339607
Loss in iteration 4 : 0.3352026258962373
Loss in iteration 5 : 0.2705934284097816
Loss in iteration 6 : 0.20105395042903842
Loss in iteration 7 : 0.14253050849489973
Loss in iteration 8 : 0.1036182340774732
Loss in iteration 9 : 0.08338654390508875
Loss in iteration 10 : 0.07129047688230172
Loss in iteration 11 : 0.06043280079325396
Loss in iteration 12 : 0.05031922431489825
Loss in iteration 13 : 0.041950261934336715
Loss in iteration 14 : 0.035576642282700986
Loss in iteration 15 : 0.030790889114767114
Loss in iteration 16 : 0.027312397669972555
Loss in iteration 17 : 0.024884608474464946
Loss in iteration 18 : 0.023102527748399902
Loss in iteration 19 : 0.021680636567146104
Loss in iteration 20 : 0.020526749206008405
Loss in iteration 21 : 0.019606332880948624
Loss in iteration 22 : 0.018858396052738625
Loss in iteration 23 : 0.018223909182948844
Loss in iteration 24 : 0.017659423692108673
Loss in iteration 25 : 0.017136272261404
Loss in iteration 26 : 0.016638648640513568
Loss in iteration 27 : 0.016159175992483267
Loss in iteration 28 : 0.015692644286069285
Loss in iteration 29 : 0.015232227058548696
Loss in iteration 30 : 0.014769720235534198
Loss in iteration 31 : 0.014297412608706712
Loss in iteration 32 : 0.013809784063043827
Loss in iteration 33 : 0.01330427885154209
Loss in iteration 34 : 0.012781234251511247
Loss in iteration 35 : 0.012243440205745526
Loss in iteration 36 : 0.011695420891143957
Loss in iteration 37 : 0.011142163629704742
Loss in iteration 38 : 0.010587447383281292
Loss in iteration 39 : 0.010032983607532023
Loss in iteration 40 : 0.00947956059910125
Loss in iteration 41 : 0.00892961439715868
Loss in iteration 42 : 0.008389389461270979
Loss in iteration 43 : 0.00786897799928455
Loss in iteration 44 : 0.007379117640135523
Loss in iteration 45 : 0.006925790995522383
Loss in iteration 46 : 0.0065069708511194275
Loss in iteration 47 : 0.006114577706745775
Loss in iteration 48 : 0.005738996102440526
Loss in iteration 49 : 0.005372293209599413
Loss in iteration 50 : 0.00500969986031754
Loss in iteration 51 : 0.004650644006619984
Loss in iteration 52 : 0.004299669302247836
Loss in iteration 53 : 0.00396531125807755
Loss in iteration 54 : 0.0036542820175025094
Loss in iteration 55 : 0.0033650661419709653
Loss in iteration 56 : 0.0030900329172271076
Loss in iteration 57 : 0.0028227073340108266
Loss in iteration 58 : 0.0025609365784671563
Loss in iteration 59 : 0.0023059364945678946
Loss in iteration 60 : 0.0020606523642684484
Loss in iteration 61 : 0.0018285914458400308
Loss in iteration 62 : 0.0016128621072156863
Loss in iteration 63 : 0.0014152831582957151
Loss in iteration 64 : 0.001235955319157279
Loss in iteration 65 : 0.0010736388280836024
Loss in iteration 66 : 9.266461763273537E-4
Loss in iteration 67 : 7.935687821037548E-4
Loss in iteration 68 : 6.734478391712105E-4
Loss in iteration 69 : 5.655971946807078E-4
Loss in iteration 70 : 4.6957469655814336E-4
Loss in iteration 71 : 3.8543604914567534E-4
Loss in iteration 72 : 3.138052179017594E-4
Loss in iteration 73 : 2.553042148096476E-4
Loss in iteration 74 : 2.0964969344215495E-4
Loss in iteration 75 : 1.7528898892485648E-4
Loss in iteration 76 : 1.498871969704303E-4
Loss in iteration 77 : 1.3108432136886095E-4
Loss in iteration 78 : 1.1695082924328055E-4
Loss in iteration 79 : 1.0608209668609402E-4
Loss in iteration 80 : 9.751242929524379E-5
Loss in iteration 81 : 9.059141103769291E-5
Loss in iteration 82 : 8.488028009698079E-5
Testing accuracy  of updater 8 on alg 0 with rate 2.0 = 0.9964444444444445, training accuracy 1.0, time elapsed: 2149 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.32850598268066994
Loss in iteration 3 : 0.2410522116007296
Loss in iteration 4 : 0.15396760531302198
Loss in iteration 5 : 0.09555892913553546
Loss in iteration 6 : 0.07190285018552829
Loss in iteration 7 : 0.058196282490188905
Loss in iteration 8 : 0.04774793393032508
Loss in iteration 9 : 0.03956364500616932
Loss in iteration 10 : 0.03336906607876807
Loss in iteration 11 : 0.028775657320161383
Loss in iteration 12 : 0.025340819819679467
Loss in iteration 13 : 0.022693668070352967
Loss in iteration 14 : 0.020572412249072553
Loss in iteration 15 : 0.018808726398995144
Loss in iteration 16 : 0.017299324862165132
Loss in iteration 17 : 0.01598148326135002
Loss in iteration 18 : 0.014816068926858739
Loss in iteration 19 : 0.013777070049823436
Loss in iteration 20 : 0.012845695725392084
Loss in iteration 21 : 0.012007339775134178
Loss in iteration 22 : 0.0112501465790463
Loss in iteration 23 : 0.010564344429002804
Loss in iteration 24 : 0.009941868135361657
Loss in iteration 25 : 0.009376052841008436
Loss in iteration 26 : 0.008861337242834829
Loss in iteration 27 : 0.008392984606957714
Loss in iteration 28 : 0.007966846068152582
Loss in iteration 29 : 0.007579182119917139
Loss in iteration 30 : 0.007226544042307975
Loss in iteration 31 : 0.0069057063297842825
Loss in iteration 32 : 0.006613636263543194
Loss in iteration 33 : 0.006347486567123076
Loss in iteration 34 : 0.006104599565922931
Loss in iteration 35 : 0.005882514676051889
Loss in iteration 36 : 0.005678974265932859
Loss in iteration 37 : 0.005491925461727251
Loss in iteration 38 : 0.005319517204639556
Loss in iteration 39 : 0.005160092918897663
Loss in iteration 40 : 0.005012179685658936
Loss in iteration 41 : 0.004874475006536803
Loss in iteration 42 : 0.0047458322134498986
Loss in iteration 43 : 0.004625245434683109
Loss in iteration 44 : 0.004511834825699991
Loss in iteration 45 : 0.004404832560538589
Loss in iteration 46 : 0.004303569883145585
Loss in iteration 47 : 0.004207465354055911
Loss in iteration 48 : 0.004116014304135922
Loss in iteration 49 : 0.004028779424784951
Loss in iteration 50 : 0.003945382379215879
Loss in iteration 51 : 0.003865496305154134
Loss in iteration 52 : 0.003788839086838306
Loss in iteration 53 : 0.003715167294758195
Loss in iteration 54 : 0.003644270717297971
Loss in iteration 55 : 0.00357596743327302
Loss in iteration 56 : 0.0035100993942299005
Loss in iteration 57 : 0.0034465284983252204
Loss in iteration 58 : 0.003385133143340765
Loss in iteration 59 : 0.003325805245923561
Loss in iteration 60 : 0.0032684477092059894
Loss in iteration 61 : 0.003212972313577791
Loss in iteration 62 : 0.0031592979974577524
Loss in iteration 63 : 0.0031073494879864873
Loss in iteration 64 : 0.0030570562366762626
Loss in iteration 65 : 0.0030083516127297726
Loss in iteration 66 : 0.0029611723070342246
Loss in iteration 67 : 0.0029154579024560107
Loss in iteration 68 : 0.0028711505704773804
Loss in iteration 69 : 0.0028281948598038744
Loss in iteration 70 : 0.002786537548697062
Loss in iteration 71 : 0.002746127538900304
Loss in iteration 72 : 0.002706915774691025
Loss in iteration 73 : 0.002668855175519038
Loss in iteration 74 : 0.002631900574723195
Loss in iteration 75 : 0.002596008659919768
Loss in iteration 76 : 0.0025611379128833373
Loss in iteration 77 : 0.0025272485482092434
Loss in iteration 78 : 0.0024943024509084667
Loss in iteration 79 : 0.0024622631134965435
Loss in iteration 80 : 0.002431095573247733
Loss in iteration 81 : 0.002400766350217801
Loss in iteration 82 : 0.0023712433864908405
Loss in iteration 83 : 0.0023424959869447317
Loss in iteration 84 : 0.002314494761694857
Loss in iteration 85 : 0.0022872115702856034
Loss in iteration 86 : 0.002260619467653828
Loss in iteration 87 : 0.002234692651880702
Loss in iteration 88 : 0.002209406413762708
Loss in iteration 89 : 0.0021847370882567053
Loss in iteration 90 : 0.0021606620078741326
Loss in iteration 91 : 0.0021371594581084123
Loss in iteration 92 : 0.0021142086349731732
Loss in iteration 93 : 0.002091789604706834
Loss in iteration 94 : 0.0020698832656639173
Loss in iteration 95 : 0.002048471312371181
Loss in iteration 96 : 0.002027536201680795
Loss in iteration 97 : 0.0020070611209112776
Loss in iteration 98 : 0.0019870299578306303
Loss in iteration 99 : 0.0019674272723120287
Loss in iteration 100 : 0.0019482382694780195
Loss in iteration 101 : 0.001929448774147456
Loss in iteration 102 : 0.0019110452064075366
Loss in iteration 103 : 0.0018930145581498318
Loss in iteration 104 : 0.0018753443704316705
Loss in iteration 105 : 0.0018580227115499645
Loss in iteration 106 : 0.0018410381557409728
Loss in iteration 107 : 0.0018243797624452798
Loss in iteration 108 : 0.0018080370560999957
Loss in iteration 109 : 0.0017920000064391064
Loss in iteration 110 : 0.0017762590092983217
Loss in iteration 111 : 0.001760804867931127
Loss in iteration 112 : 0.0017456287748500567
Loss in iteration 113 : 0.0017307222942101708
Loss in iteration 114 : 0.0017160773447529394
Loss in iteration 115 : 0.001701686183326668
Loss in iteration 116 : 0.0016875413889975857
Loss in iteration 117 : 0.0016736358477609215
Loss in iteration 118 : 0.0016599627378583778
Loss in iteration 119 : 0.0016465155157033495
Loss in iteration 120 : 0.001633287902411913
Loss in iteration 121 : 0.0016202738709335696
Loss in iteration 122 : 0.0016074676337734448
Loss in iteration 123 : 0.0015948636312939411
Loss in iteration 124 : 0.0015824565205827589
Loss in iteration 125 : 0.0015702411648715615
Loss in iteration 126 : 0.0015582126234886982
Loss in iteration 127 : 0.0015463661423279244
Loss in iteration 128 : 0.001534697144814275
Loss in iteration 129 : 0.0015232012233477278
Loss in iteration 130 : 0.001511874131204418
Loss in iteration 131 : 0.001500711774875379
Loss in iteration 132 : 0.0014897102068221844
Loss in iteration 133 : 0.0014788656186291256
Loss in iteration 134 : 0.0014681743345316726
Loss in iteration 135 : 0.0014576328053011764
Loss in iteration 136 : 0.0014472376024663287
Loss in iteration 137 : 0.0014369854128523617
Loss in iteration 138 : 0.001426873033419476
Loss in iteration 139 : 0.0014168973663829763
Loss in iteration 140 : 0.0014070554145982335
Loss in iteration 141 : 0.001397344277194234
Loss in iteration 142 : 0.0013877611454407835
Loss in iteration 143 : 0.0013783032988348523
Loss in iteration 144 : 0.0013689681013925397
Loss in iteration 145 : 0.0013597529981343223
Loss in iteration 146 : 0.0013506555117514484
Loss in iteration 147 : 0.001341673239442593
Loss in iteration 148 : 0.0013328038499105526
Testing accuracy  of updater 8 on alg 0 with rate 0.2 = 0.9964444444444445, training accuracy 1.0, time elapsed: 4250 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.5963147427856007
Loss in iteration 3 : 0.5037441952002432
Loss in iteration 4 : 0.4234813451641909
Loss in iteration 5 : 0.35816757605522265
Loss in iteration 6 : 0.30692908853711187
Loss in iteration 7 : 0.2671084071709024
Loss in iteration 8 : 0.2358174553621716
Loss in iteration 9 : 0.21069523842367285
Loss in iteration 10 : 0.1900512428562997
Loss in iteration 11 : 0.17274620632198223
Loss in iteration 12 : 0.15802010821344112
Loss in iteration 13 : 0.14535024076351233
Loss in iteration 14 : 0.1343572541482475
Loss in iteration 15 : 0.1247511543897075
Loss in iteration 16 : 0.11630284676093092
Loss in iteration 17 : 0.10882900775655813
Loss in iteration 18 : 0.1021827027349327
Loss in iteration 19 : 0.09624616194501234
Loss in iteration 20 : 0.09092453878543232
Loss in iteration 21 : 0.08614052351419793
Loss in iteration 22 : 0.08182993949134959
Loss in iteration 23 : 0.07793837680346812
Loss in iteration 24 : 0.07441878306037888
Loss in iteration 25 : 0.07122983717062029
Loss in iteration 26 : 0.0683348980161101
Loss in iteration 27 : 0.06570133248209202
Loss in iteration 28 : 0.06330006581828769
Loss in iteration 29 : 0.06110524440012745
Loss in iteration 30 : 0.05909394479138099
Loss in iteration 31 : 0.05724589722351457
Loss in iteration 32 : 0.05554321426394111
Loss in iteration 33 : 0.05397012760467182
Loss in iteration 34 : 0.052512740185180935
Loss in iteration 35 : 0.05115880027087006
Loss in iteration 36 : 0.04989750125431639
Loss in iteration 37 : 0.04871930769398317
Loss in iteration 38 : 0.04761580552263908
Loss in iteration 39 : 0.04657957285861221
Loss in iteration 40 : 0.04560406740720653
Loss in iteration 41 : 0.04468352678105756
Loss in iteration 42 : 0.04381287885690721
Loss in iteration 43 : 0.042987660213621014
Loss in iteration 44 : 0.04220394153796187
Loss in iteration 45 : 0.04145825951412905
Loss in iteration 46 : 0.040747555089942225
Loss in iteration 47 : 0.04006911815843699
Loss in iteration 48 : 0.03942053866562727
Loss in iteration 49 : 0.038799664022502824
Loss in iteration 50 : 0.03820456252640098
Loss in iteration 51 : 0.03763349233354889
Loss in iteration 52 : 0.03708487540225665
Loss in iteration 53 : 0.03655727575893488
Loss in iteration 54 : 0.036049381426819405
Loss in iteration 55 : 0.035559989390797714
Loss in iteration 56 : 0.035087993037194475
Loss in iteration 57 : 0.03463237159008851
Loss in iteration 58 : 0.03419218115278963
Loss in iteration 59 : 0.033766547044809234
Loss in iteration 60 : 0.03335465719498145
Loss in iteration 61 : 0.032955756407671385
Loss in iteration 62 : 0.03256914136124049
Loss in iteration 63 : 0.03219415622789255
Loss in iteration 64 : 0.03183018882437174
Loss in iteration 65 : 0.031476667216619326
Loss in iteration 66 : 0.03113305671100861
Loss in iteration 67 : 0.030798857172198747
Loss in iteration 68 : 0.03047360061433486
Loss in iteration 69 : 0.030156849019004665
Loss in iteration 70 : 0.029848192340271428
Loss in iteration 71 : 0.029547246664123294
Loss in iteration 72 : 0.029253652496515403
Loss in iteration 73 : 0.028967073160492036
Loss in iteration 74 : 0.028687193288358497
Loss in iteration 75 : 0.028413717399342134
Loss in iteration 76 : 0.028146368556556826
Loss in iteration 77 : 0.027884887099416143
Loss in iteration 78 : 0.027629029449049203
Loss in iteration 79 : 0.027378566984944844
Loss in iteration 80 : 0.027133284991194453
Loss in iteration 81 : 0.02689298167050573
Loss in iteration 82 : 0.026657467223802484
Loss in iteration 83 : 0.026426562992828098
Loss in iteration 84 : 0.02620010066282693
Loss in iteration 85 : 0.025977921522136543
Loss in iteration 86 : 0.025759875775396388
Loss in iteration 87 : 0.025545821907066706
Loss in iteration 88 : 0.025335626092022275
Loss in iteration 89 : 0.02512916165012345
Loss in iteration 90 : 0.024926308541833763
Loss in iteration 91 : 0.024726952902133358
Loss in iteration 92 : 0.02453098661014961
Loss in iteration 93 : 0.024338306892080493
Loss in iteration 94 : 0.024148815955123658
Loss in iteration 95 : 0.023962420650236876
Loss in iteration 96 : 0.02377903216165741
Loss in iteration 97 : 0.02359856572119691
Loss in iteration 98 : 0.023420940345413872
Loss in iteration 99 : 0.023246078593853248
Loss in iteration 100 : 0.02307390634663049
Loss in iteration 101 : 0.022904352599733894
Loss in iteration 102 : 0.022737349276519093
Loss in iteration 103 : 0.022572831053973997
Loss in iteration 104 : 0.022410735202439788
Loss in iteration 105 : 0.022251001437581554
Loss in iteration 106 : 0.02209357178350577
Loss in iteration 107 : 0.02193839044602389
Loss in iteration 108 : 0.021785403695156066
Loss in iteration 109 : 0.021634559756056097
Loss in iteration 110 : 0.021485808707619902
Loss in iteration 111 : 0.021339102388111993
Loss in iteration 112 : 0.02119439430720945
Loss in iteration 113 : 0.021051639563921542
Loss in iteration 114 : 0.020910794769895023
Loss in iteration 115 : 0.020771817977660544
Loss in iteration 116 : 0.020634668613418974
Loss in iteration 117 : 0.02049930741400211
Loss in iteration 118 : 0.020365696367677644
Loss in iteration 119 : 0.02023379865849684
Loss in iteration 120 : 0.020103578613913695
Loss in iteration 121 : 0.019975001655427364
Loss in iteration 122 : 0.019848034252023618
Loss in iteration 123 : 0.01972264387621204
Loss in iteration 124 : 0.019598798962473536
Loss in iteration 125 : 0.019476468867950892
Loss in iteration 126 : 0.019355623835228358
Loss in iteration 127 : 0.019236234957061688
Loss in iteration 128 : 0.019118274142930797
Loss in iteration 129 : 0.019001714087298553
Loss in iteration 130 : 0.018886528239467834
Loss in iteration 131 : 0.01877269077493871
Loss in iteration 132 : 0.018660176568174376
Loss in iteration 133 : 0.01854896116669091
Loss in iteration 134 : 0.018439020766393095
Loss in iteration 135 : 0.01833033218808309
Loss in iteration 136 : 0.018222872855073922
Loss in iteration 137 : 0.018116620771845254
Loss in iteration 138 : 0.018011554503681353
Loss in iteration 139 : 0.017907653157237118
Loss in iteration 140 : 0.017804896361979614
Loss in iteration 141 : 0.017703264252457773
Loss in iteration 142 : 0.017602737451354183
Loss in iteration 143 : 0.01750329705327734
Loss in iteration 144 : 0.01740492460925412
Loss in iteration 145 : 0.017307602111885822
Loss in iteration 146 : 0.017211311981132182
Loss in iteration 147 : 0.017116037050690994
Loss in iteration 148 : 0.01702176055494254
Loss in iteration 149 : 0.016928466116429425
Loss in iteration 150 : 0.016836137733845093
Loss in iteration 151 : 0.016744759770504852
Loss in iteration 152 : 0.016654316943275623
Loss in iteration 153 : 0.016564794311941484
Loss in iteration 154 : 0.016476177268983587
Loss in iteration 155 : 0.01638845152975416
Loss in iteration 156 : 0.016301603123025803
Loss in iteration 157 : 0.016215618381897803
Loss in iteration 158 : 0.0161304839350427
Loss in iteration 159 : 0.016046186698276923
Loss in iteration 160 : 0.015962713866440836
Loss in iteration 161 : 0.015880052905573233
Loss in iteration 162 : 0.0157981915453677
Loss in iteration 163 : 0.01571711777189716
Loss in iteration 164 : 0.015636819820595508
Loss in iteration 165 : 0.015557286169484298
Loss in iteration 166 : 0.015478505532634067
Loss in iteration 167 : 0.015400466853849947
Loss in iteration 168 : 0.015323159300572174
Loss in iteration 169 : 0.015246572257981901
Loss in iteration 170 : 0.015170695323304298
Loss in iteration 171 : 0.015095518300300197
Loss in iteration 172 : 0.015021031193938583
Loss in iteration 173 : 0.014947224205242925
Loss in iteration 174 : 0.014874087726303767
Loss in iteration 175 : 0.014801612335451195
Loss in iteration 176 : 0.014729788792581003
Loss in iteration 177 : 0.014658608034628106
Loss in iteration 178 : 0.014588061171181721
Loss in iteration 179 : 0.014518139480236796
Loss in iteration 180 : 0.014448834404076495
Loss in iteration 181 : 0.01438013754528053
Loss in iteration 182 : 0.014312040662855196
Loss in iteration 183 : 0.014244535668479654
Loss in iteration 184 : 0.014177614622865141
Loss in iteration 185 : 0.014111269732222232
Loss in iteration 186 : 0.014045493344832545
Loss in iteration 187 : 0.013980277947721127
Loss in iteration 188 : 0.013915616163425832
Loss in iteration 189 : 0.013851500746860222
Loss in iteration 190 : 0.01378792458226675
Loss in iteration 191 : 0.013724880680257139
Loss in iteration 192 : 0.013662362174936701
Loss in iteration 193 : 0.013600362321109914
Loss in iteration 194 : 0.013538874491564426
Loss in iteration 195 : 0.013477892174430753
Loss in iteration 196 : 0.013417408970615084
Loss in iteration 197 : 0.013357418591303028
Loss in iteration 198 : 0.013297914855531431
Loss in iteration 199 : 0.013238891687826517
Loss in iteration 200 : 0.013180343115905886
Testing accuracy  of updater 8 on alg 0 with rate 0.01999999999999999 = 0.9848888888888889, training accuracy 0.999285510145756, time elapsed: 7292 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 7.599951025738028
Loss in iteration 3 : 0.8718756950006897
Loss in iteration 4 : 3.5092015466089292
Loss in iteration 5 : 1.840710350217541
Loss in iteration 6 : 0.9655536547669193
Loss in iteration 7 : 0.6994582692550784
Loss in iteration 8 : 0.9795323013910051
Loss in iteration 9 : 1.0218900944682658
Loss in iteration 10 : 0.6459168838248486
Loss in iteration 11 : 0.34944706628983807
Loss in iteration 12 : 0.21063737121657927
Loss in iteration 13 : 0.17346741671856397
Loss in iteration 14 : 0.1818045425662323
Loss in iteration 15 : 0.20285624052936463
Loss in iteration 16 : 0.2149487935723823
Loss in iteration 17 : 0.2116574864251263
Loss in iteration 18 : 0.1961495455277728
Loss in iteration 19 : 0.17392394851457152
Loss in iteration 20 : 0.15099961317168792
Loss in iteration 21 : 0.13144045543860017
Loss in iteration 22 : 0.11896960084913225
Loss in iteration 23 : 0.11100880116019149
Loss in iteration 24 : 0.10746852438650867
Loss in iteration 25 : 0.10691578400300873
Loss in iteration 26 : 0.1071851138383063
Loss in iteration 27 : 0.1073335300445461
Loss in iteration 28 : 0.10685619802856323
Loss in iteration 29 : 0.1054450589584096
Loss in iteration 30 : 0.10306571233365226
Loss in iteration 31 : 0.100022011834188
Loss in iteration 32 : 0.0968184062738999
Loss in iteration 33 : 0.09364853167153206
Loss in iteration 34 : 0.09038399646387575
Loss in iteration 35 : 0.0869727198145357
Loss in iteration 36 : 0.08342392193017448
Loss in iteration 37 : 0.079750830557921
Loss in iteration 38 : 0.07596562043757402
Loss in iteration 39 : 0.07207926531404238
Loss in iteration 40 : 0.06810164622530492
Loss in iteration 41 : 0.06404166349005612
Loss in iteration 42 : 0.05990795073659252
Loss in iteration 43 : 0.055795105678768483
Loss in iteration 44 : 0.05221442504296344
Loss in iteration 45 : 0.04875183344590102
Loss in iteration 46 : 0.04542224088873032
Loss in iteration 47 : 0.04246566284871335
Loss in iteration 48 : 0.03995511047129542
Loss in iteration 49 : 0.03777209576684478
Loss in iteration 50 : 0.035757814588322306
Loss in iteration 51 : 0.03407139409621475
Loss in iteration 52 : 0.032495262266396124
Loss in iteration 53 : 0.030962202667304775
Loss in iteration 54 : 0.029487334985368215
Loss in iteration 55 : 0.028049178029467086
Loss in iteration 56 : 0.02657154116559745
Loss in iteration 57 : 0.025022248552349635
Loss in iteration 58 : 0.023412120471245
Loss in iteration 59 : 0.021782466796382408
Loss in iteration 60 : 0.0202578005854662
Loss in iteration 61 : 0.019046540064642615
Loss in iteration 62 : 0.018031158105531086
Loss in iteration 63 : 0.017038620415776973
Loss in iteration 64 : 0.01602823176694054
Loss in iteration 65 : 0.014994512492604015
Loss in iteration 66 : 0.013938370289163171
Loss in iteration 67 : 0.012862660313867466
Loss in iteration 68 : 0.011777737446271773
Loss in iteration 69 : 0.010747822727150497
Loss in iteration 70 : 0.009954675402159215
Loss in iteration 71 : 0.009411090456802478
Loss in iteration 72 : 0.008942330274959463
Loss in iteration 73 : 0.008480964715216132
Loss in iteration 74 : 0.008016914541742814
Loss in iteration 75 : 0.007548902542328332
Loss in iteration 76 : 0.007076783562457969
Loss in iteration 77 : 0.0066006084204102665
Loss in iteration 78 : 0.00612056554993786
Loss in iteration 79 : 0.005636996223934325
Loss in iteration 80 : 0.0051503754571990785
Loss in iteration 81 : 0.004661357881634507
Loss in iteration 82 : 0.004171398290202301
Loss in iteration 83 : 0.003686295224678004
Loss in iteration 84 : 0.00322958244859455
Loss in iteration 85 : 0.002847859334710778
Loss in iteration 86 : 0.002536349133625245
Loss in iteration 87 : 0.0022449815466806106
Loss in iteration 88 : 0.0019504183487767176
Loss in iteration 89 : 0.0016473335694351127
Loss in iteration 90 : 0.0013351650981547284
Loss in iteration 91 : 0.0010144995429837286
Loss in iteration 92 : 6.869395586413332E-4
Loss in iteration 93 : 3.6215324735164957E-4
Loss in iteration 94 : 1.0507410385521599E-4
Loss in iteration 95 : 1.5408571328703506E-5
Testing accuracy  of updater 9 on alg 0 with rate 2.0 = 0.9928888888888889, training accuracy 1.0, time elapsed: 2136 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.3290784259227644
Loss in iteration 3 : 0.12415824136222954
Loss in iteration 4 : 0.15074074673936702
Loss in iteration 5 : 0.09044424505024082
Loss in iteration 6 : 0.05780968107645769
Loss in iteration 7 : 0.053020766835635114
Loss in iteration 8 : 0.051253823458692345
Loss in iteration 9 : 0.039644889977249506
Loss in iteration 10 : 0.027312135642481212
Loss in iteration 11 : 0.020240782118284298
Loss in iteration 12 : 0.017611541568726418
Loss in iteration 13 : 0.017241582382649286
Loss in iteration 14 : 0.017431978368339478
Loss in iteration 15 : 0.01715131517910781
Loss in iteration 16 : 0.016079661811357535
Loss in iteration 17 : 0.014420481305111659
Loss in iteration 18 : 0.012608964725789772
Loss in iteration 19 : 0.011031182945120855
Loss in iteration 20 : 0.009871022398729714
Loss in iteration 21 : 0.009114600649040051
Loss in iteration 22 : 0.008636397099323291
Loss in iteration 23 : 0.008293131017831037
Loss in iteration 24 : 0.007980193065055268
Loss in iteration 25 : 0.007642180262099931
Loss in iteration 26 : 0.007261763472293477
Loss in iteration 27 : 0.006845325244532908
Loss in iteration 28 : 0.006410759197047472
Loss in iteration 29 : 0.00597823843386168
Loss in iteration 30 : 0.005564235001858127
Loss in iteration 31 : 0.005178930237525999
Loss in iteration 32 : 0.004826548344460436
Loss in iteration 33 : 0.004507382947083066
Loss in iteration 34 : 0.004219995528198557
Loss in iteration 35 : 0.0039625616644283325
Loss in iteration 36 : 0.0037332071440603874
Loss in iteration 37 : 0.003529760294102249
Loss in iteration 38 : 0.0033494566460467857
Loss in iteration 39 : 0.0031889192572857623
Loss in iteration 40 : 0.003044426883825941
Loss in iteration 41 : 0.002912291419777904
Loss in iteration 42 : 0.002789173075531739
Loss in iteration 43 : 0.0026722676473720044
Loss in iteration 44 : 0.0025593839083441505
Loss in iteration 45 : 0.0024489536897405193
Loss in iteration 46 : 0.0023400050511470662
Loss in iteration 47 : 0.0022321099252001733
Loss in iteration 48 : 0.0021253072943607106
Loss in iteration 49 : 0.0020200028543611915
Loss in iteration 50 : 0.0019168511268011807
Loss in iteration 51 : 0.001816630854131397
Loss in iteration 52 : 0.0017201266731405544
Loss in iteration 53 : 0.001628029269337467
Loss in iteration 54 : 0.0015408631742227727
Loss in iteration 55 : 0.0014589469179039911
Loss in iteration 56 : 0.0013823852539124723
Loss in iteration 57 : 0.0013110886094062628
Loss in iteration 58 : 0.0012448117279386202
Loss in iteration 59 : 0.0011832022324237558
Loss in iteration 60 : 0.0011258505012535214
Loss in iteration 61 : 0.0010723342463125809
Loss in iteration 62 : 0.0010222537151198872
Loss in iteration 63 : 9.752558280163126E-4
Loss in iteration 64 : 9.310474095738401E-4
Loss in iteration 65 : 8.89398873706427E-4
Loss in iteration 66 : 8.501403553183311E-4
Loss in iteration 67 : 8.131525029176678E-4
Loss in iteration 68 : 7.783541056114112E-4
Loss in iteration 69 : 7.456885306307098E-4
Loss in iteration 70 : 7.151106533014031E-4
Loss in iteration 71 : 6.865755966304491E-4
Loss in iteration 72 : 6.600301774268791E-4
Loss in iteration 73 : 6.35407503084452E-4
Loss in iteration 74 : 6.126247183254756E-4
Loss in iteration 75 : 5.915835179270874E-4
Loss in iteration 76 : 5.721727705073508E-4
Loss in iteration 77 : 5.542724695199816E-4
Loss in iteration 78 : 5.377582388224448E-4
Loss in iteration 79 : 5.225057405995649E-4
Loss in iteration 80 : 5.083945158613284E-4
Loss in iteration 81 : 4.953109845855338E-4
Testing accuracy  of updater 9 on alg 0 with rate 0.2 = 0.992, training accuracy 1.0, time elapsed: 1768 millisecond.
Loss in iteration 1 : 0.6931471805599143
Loss in iteration 2 : 0.5845451149762634
Loss in iteration 3 : 0.45349224860846876
Loss in iteration 4 : 0.3444193961619395
Loss in iteration 5 : 0.2676710011313913
Loss in iteration 6 : 0.21671425474286896
Loss in iteration 7 : 0.18144674830914367
Loss in iteration 8 : 0.15533017392936746
Loss in iteration 9 : 0.13514266382294665
Loss in iteration 10 : 0.11912947749683192
Loss in iteration 11 : 0.10607859404604111
Loss in iteration 12 : 0.09512189840036431
Loss in iteration 13 : 0.08570533780649496
Loss in iteration 14 : 0.07751587285568734
Loss in iteration 15 : 0.07038144618354901
Loss in iteration 16 : 0.0641908365879631
Loss in iteration 17 : 0.05884770207200347
Loss in iteration 18 : 0.054252195491081054
Loss in iteration 19 : 0.05029959407421137
Loss in iteration 20 : 0.046887086584372836
Loss in iteration 21 : 0.0439220032107553
Loss in iteration 22 : 0.041327234918512167
Loss in iteration 23 : 0.039042394833621354
Loss in iteration 24 : 0.03702151475619137
Loss in iteration 25 : 0.03522906890609441
Loss in iteration 26 : 0.03363600283797828
Loss in iteration 27 : 0.032216781169472
Loss in iteration 28 : 0.03094776113543356
Loss in iteration 29 : 0.029806709024528293
Loss in iteration 30 : 0.028773050588035188
Loss in iteration 31 : 0.027828423913176803
Loss in iteration 32 : 0.026957196792335162
Loss in iteration 33 : 0.026146746911254324
Loss in iteration 34 : 0.025387433897990894
Loss in iteration 35 : 0.024672291729660553
Loss in iteration 36 : 0.023996529136743657
Loss in iteration 37 : 0.023356946518199754
Loss in iteration 38 : 0.022751369132614864
Loss in iteration 39 : 0.022178169470770396
Loss in iteration 40 : 0.021635918045498075
Loss in iteration 41 : 0.021123170294566992
Loss in iteration 42 : 0.020638373424557294
Loss in iteration 43 : 0.020179862993734213
Loss in iteration 44 : 0.019745914364634257
Loss in iteration 45 : 0.019334816765493718
Loss in iteration 46 : 0.01894494485901001
Loss in iteration 47 : 0.018574811792018402
Loss in iteration 48 : 0.01822309657851346
Loss in iteration 49 : 0.017888645942587867
Loss in iteration 50 : 0.01757045571172065
Loss in iteration 51 : 0.01726763939059126
Loss in iteration 52 : 0.016979391973584692
Loss in iteration 53 : 0.016704955930162024
Loss in iteration 54 : 0.01644359426581054
Loss in iteration 55 : 0.01619457322392293
Loss in iteration 56 : 0.015957155026032684
Loss in iteration 57 : 0.01573059935980665
Loss in iteration 58 : 0.015514171260724994
Loss in iteration 59 : 0.015307152599027608
Loss in iteration 60 : 0.015108854482759629
Loss in iteration 61 : 0.014918628368593501
Loss in iteration 62 : 0.014735874365239545
Loss in iteration 63 : 0.01456004596336563
Loss in iteration 64 : 0.014390651107268507
Loss in iteration 65 : 0.014227250055168996
Loss in iteration 66 : 0.01406945081757125
Loss in iteration 67 : 0.013916903114292308
Loss in iteration 68 : 0.013769291775939246
Loss in iteration 69 : 0.013626330377246573
Loss in iteration 70 : 0.01348775567699878
Loss in iteration 71 : 0.01335332319963571
Loss in iteration 72 : 0.013222804066505113
Loss in iteration 73 : 0.013095982998104827
Loss in iteration 74 : 0.012972657278127573
Loss in iteration 75 : 0.012852636399788375
Loss in iteration 76 : 0.012735742099676747
Loss in iteration 77 : 0.012621808512893939
Loss in iteration 78 : 0.012510682241011156
Loss in iteration 79 : 0.012402222196486071
Loss in iteration 80 : 0.012296299160372812
Loss in iteration 81 : 0.012192795054282984
Loss in iteration 82 : 0.012091601976111121
Loss in iteration 83 : 0.011992621079194904
Loss in iteration 84 : 0.011895761386640764
Loss in iteration 85 : 0.011800938629155284
Loss in iteration 86 : 0.011708074179939163
Loss in iteration 87 : 0.011617094138619135
Loss in iteration 88 : 0.011527928592208117
Loss in iteration 89 : 0.011440511058366023
Loss in iteration 90 : 0.011354778097418305
Loss in iteration 91 : 0.011270669066198957
Loss in iteration 92 : 0.01118812597928835
Loss in iteration 93 : 0.011107093441230498
Loss in iteration 94 : 0.01102751861585729
Loss in iteration 95 : 0.010949351204594124
Loss in iteration 96 : 0.010872543413172068
Loss in iteration 97 : 0.010797049894216308
Loss in iteration 98 : 0.010722827660666014
Loss in iteration 99 : 0.010649835971155817
Loss in iteration 100 : 0.010578036192940682
Loss in iteration 101 : 0.010507391650559594
Loss in iteration 102 : 0.010437867469341153
Loss in iteration 103 : 0.010369430422356945
Loss in iteration 104 : 0.010302048787924949
Loss in iteration 105 : 0.010235692222675888
Loss in iteration 106 : 0.010170331652912108
Loss in iteration 107 : 0.010105939184834435
Loss in iteration 108 : 0.010042488032415696
Loss in iteration 109 : 0.009979952460398123
Loss in iteration 110 : 0.009918307739128683
Loss in iteration 111 : 0.009857530107692903
Loss in iteration 112 : 0.009797596741984889
Loss in iteration 113 : 0.00973848572484295
Loss in iteration 114 : 0.009680176016065651
Loss in iteration 115 : 0.009622647420884058
Loss in iteration 116 : 0.009565880556200048
Loss in iteration 117 : 0.009509856814537834
Loss in iteration 118 : 0.009454558326144318
Loss in iteration 119 : 0.009399967919996626
Loss in iteration 120 : 0.00934606908463399
Loss in iteration 121 : 0.009292845929744564
Loss in iteration 122 : 0.00924028314933953
Loss in iteration 123 : 0.00918836598717027
Loss in iteration 124 : 0.00913708020482843
Loss in iteration 125 : 0.009086412052744557
Loss in iteration 126 : 0.0090363482440944
Loss in iteration 127 : 0.008986875931452708
Loss in iteration 128 : 0.00893798268591047
Loss in iteration 129 : 0.008889656478298258
Loss in iteration 130 : 0.008841885662130355
Loss in iteration 131 : 0.008794658957897002
Loss in iteration 132 : 0.008747965438373643
Loss in iteration 133 : 0.008701794514676495
Loss in iteration 134 : 0.008656135922864211
Loss in iteration 135 : 0.00861097971095404
Loss in iteration 136 : 0.008566316226284572
Loss in iteration 137 : 0.008522136103207581
Loss in iteration 138 : 0.008478430251129512
Loss in iteration 139 : 0.008435189842945551
Loss in iteration 140 : 0.00839240630391895
Loss in iteration 141 : 0.008350071301057227
Loss in iteration 142 : 0.0083081767330266
Loss in iteration 143 : 0.008266714720631297
Loss in iteration 144 : 0.008225677597866993
Loss in iteration 145 : 0.00818505790354016
Loss in iteration 146 : 0.00814484837342984
Loss in iteration 147 : 0.0081050419329561
Loss in iteration 148 : 0.008065631690311032
Loss in iteration 149 : 0.008026610930004134
Loss in iteration 150 : 0.007987973106773004
Loss in iteration 151 : 0.007949711839812855
Loss in iteration 152 : 0.007911820907283217
Loss in iteration 153 : 0.00787429424105634
Loss in iteration 154 : 0.007837125921678708
Loss in iteration 155 : 0.007800310173523797
Loss in iteration 156 : 0.007763841360120871
Loss in iteration 157 : 0.007727713979649032
Loss in iteration 158 : 0.007691922660590941
Loss in iteration 159 : 0.007656462157542316
Loss in iteration 160 : 0.007621327347175832
Loss in iteration 161 : 0.00758651322435827
Loss in iteration 162 : 0.007552014898419654
Loss in iteration 163 : 0.007517827589572519
Loss in iteration 164 : 0.007483946625478324
Loss in iteration 165 : 0.007450367437956811
Loss in iteration 166 : 0.007417085559833165
Loss in iteration 167 : 0.00738409662191665
Loss in iteration 168 : 0.007351396350103901
Loss in iteration 169 : 0.007318980562599849
Loss in iteration 170 : 0.007286845167248481
Loss in iteration 171 : 0.007254986158966728
Loss in iteration 172 : 0.007223399617274596
Loss in iteration 173 : 0.007192081703915073
Loss in iteration 174 : 0.0071610286605586475
Loss in iteration 175 : 0.007130236806587333
Loss in iteration 176 : 0.00709970253695382
Loss in iteration 177 : 0.007069422320112474
Loss in iteration 178 : 0.007039392696018802
Loss in iteration 179 : 0.007009610274194896
Loss in iteration 180 : 0.00698007173185825
Loss in iteration 181 : 0.006950773812112282
Loss in iteration 182 : 0.006921713322196175
Loss in iteration 183 : 0.00689288713179241
Loss in iteration 184 : 0.006864292171390066
Loss in iteration 185 : 0.006835925430702192
Loss in iteration 186 : 0.006807783957135075
Loss in iteration 187 : 0.0067798648543079595
Loss in iteration 188 : 0.0067521652806209675
Loss in iteration 189 : 0.006724682447869497
Loss in iteration 190 : 0.006697413619903234
Loss in iteration 191 : 0.00667035611132783
Loss in iteration 192 : 0.006643507286247634
Loss in iteration 193 : 0.006616864557047577
Loss in iteration 194 : 0.006590425383212716
Loss in iteration 195 : 0.006564187270183909
Loss in iteration 196 : 0.00653814776824799
Loss in iteration 197 : 0.006512304471461358
Loss in iteration 198 : 0.006486655016605552
Loss in iteration 199 : 0.00646119708217354
Loss in iteration 200 : 0.006435928387385955
Testing accuracy  of updater 9 on alg 0 with rate 0.01999999999999999 = 0.9884444444444445, training accuracy 0.9994284081166047, time elapsed: 4277 millisecond.
